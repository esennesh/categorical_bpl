{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='omniglot_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 25,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [128/17352 (1%)] Loss: 8625.625000\n",
      "Train Epoch: 1 [1536/17352 (9%)] Loss: -29233.093750\n",
      "Train Epoch: 1 [2944/17352 (17%)] Loss: -110716.046875\n",
      "Train Epoch: 1 [4352/17352 (25%)] Loss: -174722.265625\n",
      "Train Epoch: 1 [5760/17352 (33%)] Loss: -171997.593750\n",
      "Train Epoch: 1 [7168/17352 (41%)] Loss: -175392.343750\n",
      "Train Epoch: 1 [8576/17352 (49%)] Loss: -193932.250000\n",
      "Train Epoch: 1 [9984/17352 (58%)] Loss: -199739.906250\n",
      "Train Epoch: 1 [11392/17352 (66%)] Loss: -184556.093750\n",
      "Train Epoch: 1 [12800/17352 (74%)] Loss: -190507.468750\n",
      "Train Epoch: 1 [14208/17352 (82%)] Loss: -168949.656250\n",
      "Train Epoch: 1 [15502/17352 (89%)] Loss: -149918.234375\n",
      "Train Epoch: 1 [16184/17352 (93%)] Loss: -181477.531250\n",
      "Train Epoch: 1 [16918/17352 (97%)] Loss: -55983.863281\n",
      "    epoch          : 1\n",
      "    loss           : -143606.82831834626\n",
      "    val_loss       : -93203.58955891927\n",
      "Train Epoch: 2 [128/17352 (1%)] Loss: -197086.640625\n",
      "Train Epoch: 2 [1536/17352 (9%)] Loss: -178444.156250\n",
      "Train Epoch: 2 [2944/17352 (17%)] Loss: -161547.125000\n",
      "Train Epoch: 2 [4352/17352 (25%)] Loss: -177030.125000\n",
      "Train Epoch: 2 [5760/17352 (33%)] Loss: -179495.906250\n",
      "Train Epoch: 2 [7168/17352 (41%)] Loss: -186073.437500\n",
      "Train Epoch: 2 [8576/17352 (49%)] Loss: -195342.250000\n",
      "Train Epoch: 2 [9984/17352 (58%)] Loss: -194047.000000\n",
      "Train Epoch: 2 [11392/17352 (66%)] Loss: -183656.171875\n",
      "Train Epoch: 2 [12800/17352 (74%)] Loss: -173006.328125\n",
      "Train Epoch: 2 [14208/17352 (82%)] Loss: -185217.500000\n",
      "Train Epoch: 2 [15491/17352 (89%)] Loss: -121623.054688\n",
      "Train Epoch: 2 [16428/17352 (95%)] Loss: -120669.156250\n",
      "Train Epoch: 2 [17042/17352 (98%)] Loss: -4434.215820\n",
      "    epoch          : 2\n",
      "    loss           : -169962.04836278313\n",
      "    val_loss       : -94061.51396484375\n",
      "Train Epoch: 3 [128/17352 (1%)] Loss: -196436.000000\n",
      "Train Epoch: 3 [1536/17352 (9%)] Loss: -184736.812500\n",
      "Train Epoch: 3 [2944/17352 (17%)] Loss: -180251.906250\n",
      "Train Epoch: 3 [4352/17352 (25%)] Loss: -180801.625000\n",
      "Train Epoch: 3 [5760/17352 (33%)] Loss: -188069.421875\n",
      "Train Epoch: 3 [7168/17352 (41%)] Loss: -184801.406250\n",
      "Train Epoch: 3 [8576/17352 (49%)] Loss: -195769.437500\n",
      "Train Epoch: 3 [9984/17352 (58%)] Loss: -178782.718750\n",
      "Train Epoch: 3 [11392/17352 (66%)] Loss: -192576.937500\n",
      "Train Epoch: 3 [12800/17352 (74%)] Loss: -179636.890625\n",
      "Train Epoch: 3 [14208/17352 (82%)] Loss: -215228.453125\n",
      "Train Epoch: 3 [15427/17352 (89%)] Loss: -70750.984375\n",
      "Train Epoch: 3 [16359/17352 (94%)] Loss: -162703.250000\n",
      "Train Epoch: 3 [16952/17352 (98%)] Loss: -67955.820312\n",
      "    epoch          : 3\n",
      "    loss           : -171203.63949113883\n",
      "    val_loss       : -94891.29322916667\n",
      "Train Epoch: 4 [128/17352 (1%)] Loss: -195597.875000\n",
      "Train Epoch: 4 [1536/17352 (9%)] Loss: -195288.468750\n",
      "Train Epoch: 4 [2944/17352 (17%)] Loss: -182545.937500\n",
      "Train Epoch: 4 [4352/17352 (25%)] Loss: -175979.078125\n",
      "Train Epoch: 4 [5760/17352 (33%)] Loss: -214375.062500\n",
      "Train Epoch: 4 [7168/17352 (41%)] Loss: -177767.437500\n",
      "Train Epoch: 4 [8576/17352 (49%)] Loss: -183629.421875\n",
      "Train Epoch: 4 [9984/17352 (58%)] Loss: -182158.656250\n",
      "Train Epoch: 4 [11392/17352 (66%)] Loss: -182440.953125\n",
      "Train Epoch: 4 [12800/17352 (74%)] Loss: -178834.703125\n",
      "Train Epoch: 4 [14208/17352 (82%)] Loss: -181927.468750\n",
      "Train Epoch: 4 [15503/17352 (89%)] Loss: -166088.437500\n",
      "Train Epoch: 4 [16222/17352 (93%)] Loss: -113238.382812\n",
      "Train Epoch: 4 [16899/17352 (97%)] Loss: -23020.914062\n",
      "    epoch          : 4\n",
      "    loss           : -172892.83720244336\n",
      "    val_loss       : -95683.08924153647\n",
      "Train Epoch: 5 [128/17352 (1%)] Loss: -197590.812500\n",
      "Train Epoch: 5 [1536/17352 (9%)] Loss: -189381.437500\n",
      "Train Epoch: 5 [2944/17352 (17%)] Loss: -165212.093750\n",
      "Train Epoch: 5 [4352/17352 (25%)] Loss: -197390.453125\n",
      "Train Epoch: 5 [5760/17352 (33%)] Loss: -185761.750000\n",
      "Train Epoch: 5 [7168/17352 (41%)] Loss: -196722.156250\n",
      "Train Epoch: 5 [8576/17352 (49%)] Loss: -185313.343750\n",
      "Train Epoch: 5 [9984/17352 (58%)] Loss: -201950.531250\n",
      "Train Epoch: 5 [11392/17352 (66%)] Loss: -187295.750000\n",
      "Train Epoch: 5 [12800/17352 (74%)] Loss: -182313.546875\n",
      "Train Epoch: 5 [14208/17352 (82%)] Loss: -217899.421875\n",
      "Train Epoch: 5 [15532/17352 (90%)] Loss: -143914.812500\n",
      "Train Epoch: 5 [16324/17352 (94%)] Loss: -135947.734375\n",
      "Train Epoch: 5 [16924/17352 (98%)] Loss: -189749.906250\n",
      "    epoch          : 5\n",
      "    loss           : -174402.80129247063\n",
      "    val_loss       : -96857.78275553386\n",
      "Train Epoch: 6 [128/17352 (1%)] Loss: -200092.859375\n",
      "Train Epoch: 6 [1536/17352 (9%)] Loss: -219934.312500\n",
      "Train Epoch: 6 [2944/17352 (17%)] Loss: -167158.718750\n",
      "Train Epoch: 6 [4352/17352 (25%)] Loss: -202983.968750\n",
      "Train Epoch: 6 [5760/17352 (33%)] Loss: -198017.281250\n",
      "Train Epoch: 6 [7168/17352 (41%)] Loss: -195111.562500\n",
      "Train Epoch: 6 [8576/17352 (49%)] Loss: -220402.656250\n",
      "Train Epoch: 6 [9984/17352 (58%)] Loss: -217430.250000\n",
      "Train Epoch: 6 [11392/17352 (66%)] Loss: -199440.625000\n",
      "Train Epoch: 6 [12800/17352 (74%)] Loss: -182346.000000\n",
      "Train Epoch: 6 [14208/17352 (82%)] Loss: -220564.796875\n",
      "Train Epoch: 6 [15538/17352 (90%)] Loss: -157475.843750\n",
      "Train Epoch: 6 [16191/17352 (93%)] Loss: -53810.570312\n",
      "Train Epoch: 6 [17030/17352 (98%)] Loss: -127014.765625\n",
      "    epoch          : 6\n",
      "    loss           : -176198.56519701658\n",
      "    val_loss       : -97569.03841145833\n",
      "Train Epoch: 7 [128/17352 (1%)] Loss: -204304.531250\n",
      "Train Epoch: 7 [1536/17352 (9%)] Loss: -185049.218750\n",
      "Train Epoch: 7 [2944/17352 (17%)] Loss: -207329.593750\n",
      "Train Epoch: 7 [4352/17352 (25%)] Loss: -209242.140625\n",
      "Train Epoch: 7 [5760/17352 (33%)] Loss: -202720.625000\n",
      "Train Epoch: 7 [7168/17352 (41%)] Loss: -197862.625000\n",
      "Train Epoch: 7 [8576/17352 (49%)] Loss: -218931.859375\n",
      "Train Epoch: 7 [9984/17352 (58%)] Loss: -200347.843750\n",
      "Train Epoch: 7 [11392/17352 (66%)] Loss: -202420.500000\n",
      "Train Epoch: 7 [12800/17352 (74%)] Loss: -197213.296875\n",
      "Train Epoch: 7 [14208/17352 (82%)] Loss: -185214.328125\n",
      "Train Epoch: 7 [15471/17352 (89%)] Loss: -59187.285156\n",
      "Train Epoch: 7 [16095/17352 (93%)] Loss: -130170.062500\n",
      "Train Epoch: 7 [16896/17352 (97%)] Loss: -125978.828125\n",
      "    epoch          : 7\n",
      "    loss           : -177755.55076814178\n",
      "    val_loss       : -98425.46808268229\n",
      "Train Epoch: 8 [128/17352 (1%)] Loss: -173598.687500\n",
      "Train Epoch: 8 [1536/17352 (9%)] Loss: -184624.000000\n",
      "Train Epoch: 8 [2944/17352 (17%)] Loss: -190656.531250\n",
      "Train Epoch: 8 [4352/17352 (25%)] Loss: -225185.187500\n",
      "Train Epoch: 8 [5760/17352 (33%)] Loss: -170147.828125\n",
      "Train Epoch: 8 [7168/17352 (41%)] Loss: -191979.265625\n",
      "Train Epoch: 8 [8576/17352 (49%)] Loss: -190759.562500\n",
      "Train Epoch: 8 [9984/17352 (58%)] Loss: -200167.281250\n",
      "Train Epoch: 8 [11392/17352 (66%)] Loss: -204027.750000\n",
      "Train Epoch: 8 [12800/17352 (74%)] Loss: -197341.750000\n",
      "Train Epoch: 8 [14208/17352 (82%)] Loss: -191009.437500\n",
      "Train Epoch: 8 [15478/17352 (89%)] Loss: -140464.281250\n",
      "Train Epoch: 8 [16126/17352 (93%)] Loss: -8198.062500\n",
      "Train Epoch: 8 [16996/17352 (98%)] Loss: -68288.390625\n",
      "    epoch          : 8\n",
      "    loss           : -179412.7204901164\n",
      "    val_loss       : -99236.90661621094\n",
      "Train Epoch: 9 [128/17352 (1%)] Loss: -209900.796875\n",
      "Train Epoch: 9 [1536/17352 (9%)] Loss: -188832.593750\n",
      "Train Epoch: 9 [2944/17352 (17%)] Loss: -191712.656250\n",
      "Train Epoch: 9 [4352/17352 (25%)] Loss: -209641.281250\n",
      "Train Epoch: 9 [5760/17352 (33%)] Loss: -188593.000000\n",
      "Train Epoch: 9 [7168/17352 (41%)] Loss: -209791.140625\n",
      "Train Epoch: 9 [8576/17352 (49%)] Loss: -206155.406250\n",
      "Train Epoch: 9 [9984/17352 (58%)] Loss: -188602.578125\n",
      "Train Epoch: 9 [11392/17352 (66%)] Loss: -177776.406250\n",
      "Train Epoch: 9 [12800/17352 (74%)] Loss: -207316.437500\n",
      "Train Epoch: 9 [14208/17352 (82%)] Loss: -189110.937500\n",
      "Train Epoch: 9 [15442/17352 (89%)] Loss: -128805.406250\n",
      "Train Epoch: 9 [16241/17352 (94%)] Loss: -124484.843750\n",
      "Train Epoch: 9 [17079/17352 (98%)] Loss: -201693.546875\n",
      "    epoch          : 9\n",
      "    loss           : -181154.7123925126\n",
      "    val_loss       : -100485.69255371093\n",
      "Train Epoch: 10 [128/17352 (1%)] Loss: -203821.187500\n",
      "Train Epoch: 10 [1536/17352 (9%)] Loss: -212543.906250\n",
      "Train Epoch: 10 [2944/17352 (17%)] Loss: -238257.062500\n",
      "Train Epoch: 10 [4352/17352 (25%)] Loss: -205695.640625\n",
      "Train Epoch: 10 [5760/17352 (33%)] Loss: -155774.156250\n",
      "Train Epoch: 10 [7168/17352 (41%)] Loss: -208677.234375\n",
      "Train Epoch: 10 [8576/17352 (49%)] Loss: -223327.593750\n",
      "Train Epoch: 10 [9984/17352 (58%)] Loss: -193767.781250\n",
      "Train Epoch: 10 [11392/17352 (66%)] Loss: -206627.421875\n",
      "Train Epoch: 10 [12800/17352 (74%)] Loss: -190172.937500\n",
      "Train Epoch: 10 [14208/17352 (82%)] Loss: -216651.000000\n",
      "Train Epoch: 10 [15471/17352 (89%)] Loss: -129282.421875\n",
      "Train Epoch: 10 [16335/17352 (94%)] Loss: -75735.851562\n",
      "Train Epoch: 10 [17034/17352 (98%)] Loss: -130875.656250\n",
      "    epoch          : 10\n",
      "    loss           : -183082.6944014786\n",
      "    val_loss       : -101621.64033203125\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0821_122628/checkpoint-epoch10.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 11 [128/17352 (1%)] Loss: -193569.437500\n",
      "Train Epoch: 11 [1536/17352 (9%)] Loss: -189256.312500\n",
      "Train Epoch: 11 [2944/17352 (17%)] Loss: -196294.750000\n",
      "Train Epoch: 11 [4352/17352 (25%)] Loss: -217574.921875\n",
      "Train Epoch: 11 [5760/17352 (33%)] Loss: -207874.718750\n",
      "Train Epoch: 11 [7168/17352 (41%)] Loss: -174313.515625\n",
      "Train Epoch: 11 [8576/17352 (49%)] Loss: -214293.625000\n",
      "Train Epoch: 11 [9984/17352 (58%)] Loss: -209785.015625\n",
      "Train Epoch: 11 [11392/17352 (66%)] Loss: -182877.015625\n",
      "Train Epoch: 11 [12800/17352 (74%)] Loss: -208680.453125\n",
      "Train Epoch: 11 [14208/17352 (82%)] Loss: -216460.953125\n",
      "Train Epoch: 11 [15511/17352 (89%)] Loss: -121170.210938\n",
      "Train Epoch: 11 [16280/17352 (94%)] Loss: -178269.437500\n",
      "Train Epoch: 11 [16874/17352 (97%)] Loss: -125533.546875\n",
      "    epoch          : 11\n",
      "    loss           : -185345.3821505348\n",
      "    val_loss       : -102516.61370442709\n",
      "Train Epoch: 12 [128/17352 (1%)] Loss: -215581.093750\n",
      "Train Epoch: 12 [1536/17352 (9%)] Loss: -196772.296875\n",
      "Train Epoch: 12 [2944/17352 (17%)] Loss: -224601.250000\n",
      "Train Epoch: 12 [4352/17352 (25%)] Loss: -200893.781250\n",
      "Train Epoch: 12 [5760/17352 (33%)] Loss: -208163.140625\n",
      "Train Epoch: 12 [7168/17352 (41%)] Loss: -177448.781250\n",
      "Train Epoch: 12 [8576/17352 (49%)] Loss: -199110.750000\n",
      "Train Epoch: 12 [9984/17352 (58%)] Loss: -218325.421875\n",
      "Train Epoch: 12 [11392/17352 (66%)] Loss: -200050.437500\n",
      "Train Epoch: 12 [12800/17352 (74%)] Loss: -209111.468750\n",
      "Train Epoch: 12 [14208/17352 (82%)] Loss: -213243.750000\n",
      "Train Epoch: 12 [15571/17352 (90%)] Loss: -177407.343750\n",
      "Train Epoch: 12 [16289/17352 (94%)] Loss: -120597.820312\n",
      "Train Epoch: 12 [16879/17352 (97%)] Loss: -8099.364746\n",
      "    epoch          : 12\n",
      "    loss           : -186491.3452410602\n",
      "    val_loss       : -103066.987109375\n",
      "Train Epoch: 13 [128/17352 (1%)] Loss: -210175.859375\n",
      "Train Epoch: 13 [1536/17352 (9%)] Loss: -218583.109375\n",
      "Train Epoch: 13 [2944/17352 (17%)] Loss: -182728.062500\n",
      "Train Epoch: 13 [4352/17352 (25%)] Loss: -203682.125000\n",
      "Train Epoch: 13 [5760/17352 (33%)] Loss: -211756.750000\n",
      "Train Epoch: 13 [7168/17352 (41%)] Loss: -211573.937500\n",
      "Train Epoch: 13 [8576/17352 (49%)] Loss: -213628.734375\n",
      "Train Epoch: 13 [9984/17352 (58%)] Loss: -212512.531250\n",
      "Train Epoch: 13 [11392/17352 (66%)] Loss: -243665.500000\n",
      "Train Epoch: 13 [12800/17352 (74%)] Loss: -213154.812500\n",
      "Train Epoch: 13 [14208/17352 (82%)] Loss: -196819.218750\n",
      "Train Epoch: 13 [15455/17352 (89%)] Loss: -8422.244141\n",
      "Train Epoch: 13 [16439/17352 (95%)] Loss: -181257.890625\n",
      "Train Epoch: 13 [17130/17352 (99%)] Loss: -60356.957031\n",
      "    epoch          : 13\n",
      "    loss           : -187863.50904467283\n",
      "    val_loss       : -103755.30074055989\n",
      "Train Epoch: 14 [128/17352 (1%)] Loss: -217148.656250\n",
      "Train Epoch: 14 [1536/17352 (9%)] Loss: -199153.796875\n",
      "Train Epoch: 14 [2944/17352 (17%)] Loss: -186011.750000\n",
      "Train Epoch: 14 [4352/17352 (25%)] Loss: -203547.234375\n",
      "Train Epoch: 14 [5760/17352 (33%)] Loss: -179650.562500\n",
      "Train Epoch: 14 [7168/17352 (41%)] Loss: -206454.984375\n",
      "Train Epoch: 14 [8576/17352 (49%)] Loss: -198514.453125\n",
      "Train Epoch: 14 [9984/17352 (58%)] Loss: -197290.984375\n",
      "Train Epoch: 14 [11392/17352 (66%)] Loss: -187433.406250\n",
      "Train Epoch: 14 [12800/17352 (74%)] Loss: -198291.625000\n",
      "Train Epoch: 14 [14208/17352 (82%)] Loss: -199380.218750\n",
      "Train Epoch: 14 [15584/17352 (90%)] Loss: -180080.640625\n",
      "Train Epoch: 14 [16239/17352 (94%)] Loss: -5013.749023\n",
      "Train Epoch: 14 [17000/17352 (98%)] Loss: -25677.375000\n",
      "    epoch          : 14\n",
      "    loss           : -189166.33486917996\n",
      "    val_loss       : -104420.87860514323\n",
      "Train Epoch: 15 [128/17352 (1%)] Loss: -215040.531250\n",
      "Train Epoch: 15 [1536/17352 (9%)] Loss: -204371.296875\n",
      "Train Epoch: 15 [2944/17352 (17%)] Loss: -180468.562500\n",
      "Train Epoch: 15 [4352/17352 (25%)] Loss: -200193.000000\n",
      "Train Epoch: 15 [5760/17352 (33%)] Loss: -230989.281250\n",
      "Train Epoch: 15 [7168/17352 (41%)] Loss: -183554.781250\n",
      "Train Epoch: 15 [8576/17352 (49%)] Loss: -231065.265625\n",
      "Train Epoch: 15 [9984/17352 (58%)] Loss: -213785.031250\n",
      "Train Epoch: 15 [11392/17352 (66%)] Loss: -200246.125000\n",
      "Train Epoch: 15 [12800/17352 (74%)] Loss: -215273.937500\n",
      "Train Epoch: 15 [14208/17352 (82%)] Loss: -198745.250000\n",
      "Train Epoch: 15 [15491/17352 (89%)] Loss: -85993.468750\n",
      "Train Epoch: 15 [16480/17352 (95%)] Loss: -125665.406250\n",
      "Train Epoch: 15 [17119/17352 (99%)] Loss: -58977.566406\n",
      "    epoch          : 15\n",
      "    loss           : -190224.4364054635\n",
      "    val_loss       : -104797.9286295573\n",
      "Train Epoch: 16 [128/17352 (1%)] Loss: -183452.093750\n",
      "Train Epoch: 16 [1536/17352 (9%)] Loss: -202629.781250\n",
      "Train Epoch: 16 [2944/17352 (17%)] Loss: -196669.796875\n",
      "Train Epoch: 16 [4352/17352 (25%)] Loss: -210041.265625\n",
      "Train Epoch: 16 [5760/17352 (33%)] Loss: -213994.296875\n",
      "Train Epoch: 16 [7168/17352 (41%)] Loss: -209309.484375\n",
      "Train Epoch: 16 [8576/17352 (49%)] Loss: -189978.406250\n",
      "Train Epoch: 16 [9984/17352 (58%)] Loss: -185880.218750\n",
      "Train Epoch: 16 [11392/17352 (66%)] Loss: -190210.109375\n",
      "Train Epoch: 16 [12800/17352 (74%)] Loss: -217007.250000\n",
      "Train Epoch: 16 [14208/17352 (82%)] Loss: -217240.656250\n",
      "Train Epoch: 16 [15448/17352 (89%)] Loss: -77318.843750\n",
      "Train Epoch: 16 [16299/17352 (94%)] Loss: -152065.703125\n",
      "Train Epoch: 16 [17048/17352 (98%)] Loss: -128920.007812\n",
      "    epoch          : 16\n",
      "    loss           : -191252.34460531146\n",
      "    val_loss       : -105407.35393880209\n",
      "Train Epoch: 17 [128/17352 (1%)] Loss: -221104.593750\n",
      "Train Epoch: 17 [1536/17352 (9%)] Loss: -212837.125000\n",
      "Train Epoch: 17 [2944/17352 (17%)] Loss: -199566.000000\n",
      "Train Epoch: 17 [4352/17352 (25%)] Loss: -233824.796875\n",
      "Train Epoch: 17 [5760/17352 (33%)] Loss: -233109.546875\n",
      "Train Epoch: 17 [7168/17352 (41%)] Loss: -215817.781250\n",
      "Train Epoch: 17 [8576/17352 (49%)] Loss: -202620.937500\n",
      "Train Epoch: 17 [9984/17352 (58%)] Loss: -219462.828125\n",
      "Train Epoch: 17 [11392/17352 (66%)] Loss: -194530.750000\n",
      "Train Epoch: 17 [12800/17352 (74%)] Loss: -215313.906250\n",
      "Train Epoch: 17 [14208/17352 (82%)] Loss: -217136.109375\n",
      "Train Epoch: 17 [15484/17352 (89%)] Loss: -129057.515625\n",
      "Train Epoch: 17 [16248/17352 (94%)] Loss: -62394.617188\n",
      "Train Epoch: 17 [17009/17352 (98%)] Loss: -136231.062500\n",
      "    epoch          : 17\n",
      "    loss           : -192187.02238884228\n",
      "    val_loss       : -106008.2760172526\n",
      "Train Epoch: 18 [128/17352 (1%)] Loss: -218469.578125\n",
      "Train Epoch: 18 [1536/17352 (9%)] Loss: -223553.750000\n",
      "Train Epoch: 18 [2944/17352 (17%)] Loss: -184400.640625\n",
      "Train Epoch: 18 [4352/17352 (25%)] Loss: -204820.875000\n",
      "Train Epoch: 18 [5760/17352 (33%)] Loss: -202164.109375\n",
      "Train Epoch: 18 [7168/17352 (41%)] Loss: -236449.515625\n",
      "Train Epoch: 18 [8576/17352 (49%)] Loss: -200834.125000\n",
      "Train Epoch: 18 [9984/17352 (58%)] Loss: -186701.031250\n",
      "Train Epoch: 18 [11392/17352 (66%)] Loss: -205228.046875\n",
      "Train Epoch: 18 [12800/17352 (74%)] Loss: -203455.625000\n",
      "Train Epoch: 18 [14208/17352 (82%)] Loss: -223051.000000\n",
      "Train Epoch: 18 [15531/17352 (90%)] Loss: -143428.046875\n",
      "Train Epoch: 18 [16478/17352 (95%)] Loss: -124330.914062\n",
      "Train Epoch: 18 [17065/17352 (98%)] Loss: -130477.062500\n",
      "    epoch          : 18\n",
      "    loss           : -193321.6915963192\n",
      "    val_loss       : -106780.48219401042\n",
      "Train Epoch: 19 [128/17352 (1%)] Loss: -192585.796875\n",
      "Train Epoch: 19 [1536/17352 (9%)] Loss: -203600.906250\n",
      "Train Epoch: 19 [2944/17352 (17%)] Loss: -193750.406250\n",
      "Train Epoch: 19 [4352/17352 (25%)] Loss: -205396.812500\n",
      "Train Epoch: 19 [5760/17352 (33%)] Loss: -196611.000000\n",
      "Train Epoch: 19 [7168/17352 (41%)] Loss: -202069.031250\n",
      "Train Epoch: 19 [8576/17352 (49%)] Loss: -193223.375000\n",
      "Train Epoch: 19 [9984/17352 (58%)] Loss: -220621.234375\n",
      "Train Epoch: 19 [11392/17352 (66%)] Loss: -202203.593750\n",
      "Train Epoch: 19 [12800/17352 (74%)] Loss: -205871.843750\n",
      "Train Epoch: 19 [14208/17352 (82%)] Loss: -200626.000000\n",
      "Train Epoch: 19 [15528/17352 (89%)] Loss: -130779.039062\n",
      "Train Epoch: 19 [16215/17352 (93%)] Loss: -176591.750000\n",
      "Train Epoch: 19 [16967/17352 (98%)] Loss: -123693.484375\n",
      "    epoch          : 19\n",
      "    loss           : -194484.0511482802\n",
      "    val_loss       : -107098.04959309896\n",
      "Train Epoch: 20 [128/17352 (1%)] Loss: -220184.281250\n",
      "Train Epoch: 20 [1536/17352 (9%)] Loss: -207927.156250\n",
      "Train Epoch: 20 [2944/17352 (17%)] Loss: -208249.359375\n",
      "Train Epoch: 20 [4352/17352 (25%)] Loss: -225763.562500\n",
      "Train Epoch: 20 [5760/17352 (33%)] Loss: -200873.937500\n",
      "Train Epoch: 20 [7168/17352 (41%)] Loss: -233761.562500\n",
      "Train Epoch: 20 [8576/17352 (49%)] Loss: -205559.125000\n",
      "Train Epoch: 20 [9984/17352 (58%)] Loss: -204811.734375\n",
      "Train Epoch: 20 [11392/17352 (66%)] Loss: -222201.484375\n",
      "Train Epoch: 20 [12800/17352 (74%)] Loss: -202722.312500\n",
      "Train Epoch: 20 [14208/17352 (82%)] Loss: -203404.609375\n",
      "Train Epoch: 20 [15458/17352 (89%)] Loss: -155941.765625\n",
      "Train Epoch: 20 [16240/17352 (94%)] Loss: -120678.375000\n",
      "Train Epoch: 20 [16992/17352 (98%)] Loss: -86330.023438\n",
      "    epoch          : 20\n",
      "    loss           : -194839.12001232174\n",
      "    val_loss       : -107141.31356608073\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0821_122628/checkpoint-epoch20.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 21 [128/17352 (1%)] Loss: -219706.750000\n",
      "Train Epoch: 21 [1536/17352 (9%)] Loss: -206746.500000\n",
      "Train Epoch: 21 [2944/17352 (17%)] Loss: -187667.156250\n",
      "Train Epoch: 21 [4352/17352 (25%)] Loss: -217737.156250\n",
      "Train Epoch: 21 [5760/17352 (33%)] Loss: -191556.781250\n",
      "Train Epoch: 21 [7168/17352 (41%)] Loss: -207517.187500\n",
      "Train Epoch: 21 [8576/17352 (49%)] Loss: -203934.375000\n",
      "Train Epoch: 21 [9984/17352 (58%)] Loss: -207545.468750\n",
      "Train Epoch: 21 [11392/17352 (66%)] Loss: -206646.765625\n",
      "Train Epoch: 21 [12800/17352 (74%)] Loss: -204750.421875\n",
      "Train Epoch: 21 [14208/17352 (82%)] Loss: -223122.406250\n",
      "Train Epoch: 21 [15535/17352 (90%)] Loss: -168583.281250\n",
      "Train Epoch: 21 [16280/17352 (94%)] Loss: -135890.031250\n",
      "Train Epoch: 21 [16952/17352 (98%)] Loss: -27421.550781\n",
      "    epoch          : 21\n",
      "    loss           : -195622.57494297923\n",
      "    val_loss       : -107705.557421875\n",
      "Train Epoch: 22 [128/17352 (1%)] Loss: -191467.500000\n",
      "Train Epoch: 22 [1536/17352 (9%)] Loss: -205376.656250\n",
      "Train Epoch: 22 [2944/17352 (17%)] Loss: -236844.796875\n",
      "Train Epoch: 22 [4352/17352 (25%)] Loss: -225334.062500\n",
      "Train Epoch: 22 [5760/17352 (33%)] Loss: -209217.750000\n",
      "Train Epoch: 22 [7168/17352 (41%)] Loss: -221573.046875\n",
      "Train Epoch: 22 [8576/17352 (49%)] Loss: -208845.125000\n",
      "Train Epoch: 22 [9984/17352 (58%)] Loss: -206744.187500\n",
      "Train Epoch: 22 [11392/17352 (66%)] Loss: -221417.812500\n",
      "Train Epoch: 22 [12800/17352 (74%)] Loss: -204263.218750\n",
      "Train Epoch: 22 [14208/17352 (82%)] Loss: -218255.375000\n",
      "Train Epoch: 22 [15565/17352 (90%)] Loss: -128933.484375\n",
      "Train Epoch: 22 [16411/17352 (95%)] Loss: -155767.468750\n",
      "Train Epoch: 22 [16977/17352 (98%)] Loss: -159187.109375\n",
      "    epoch          : 22\n",
      "    loss           : -196170.17038721687\n",
      "    val_loss       : -107965.2509765625\n",
      "Train Epoch: 23 [128/17352 (1%)] Loss: -222377.343750\n",
      "Train Epoch: 23 [1536/17352 (9%)] Loss: -219552.906250\n",
      "Train Epoch: 23 [2944/17352 (17%)] Loss: -188563.156250\n",
      "Train Epoch: 23 [4352/17352 (25%)] Loss: -207562.000000\n",
      "Train Epoch: 23 [5760/17352 (33%)] Loss: -224989.343750\n",
      "Train Epoch: 23 [7168/17352 (41%)] Loss: -223548.781250\n",
      "Train Epoch: 23 [8576/17352 (49%)] Loss: -206008.687500\n",
      "Train Epoch: 23 [9984/17352 (58%)] Loss: -222552.687500\n",
      "Train Epoch: 23 [11392/17352 (66%)] Loss: -210004.546875\n",
      "Train Epoch: 23 [12800/17352 (74%)] Loss: -216088.156250\n",
      "Train Epoch: 23 [14208/17352 (82%)] Loss: -224239.390625\n",
      "Train Epoch: 23 [15521/17352 (89%)] Loss: -142994.859375\n",
      "Train Epoch: 23 [16365/17352 (94%)] Loss: -137574.515625\n",
      "Train Epoch: 23 [17015/17352 (98%)] Loss: -134579.531250\n",
      "    epoch          : 23\n",
      "    loss           : -196908.08277186452\n",
      "    val_loss       : -108498.0110921224\n",
      "Train Epoch: 24 [128/17352 (1%)] Loss: -207773.796875\n",
      "Train Epoch: 24 [1536/17352 (9%)] Loss: -223452.125000\n",
      "Train Epoch: 24 [2944/17352 (17%)] Loss: -193753.093750\n",
      "Train Epoch: 24 [4352/17352 (25%)] Loss: -208314.781250\n",
      "Train Epoch: 24 [5760/17352 (33%)] Loss: -225230.500000\n",
      "Train Epoch: 24 [7168/17352 (41%)] Loss: -222010.218750\n",
      "Train Epoch: 24 [8576/17352 (49%)] Loss: -207018.937500\n",
      "Train Epoch: 24 [9984/17352 (58%)] Loss: -221276.437500\n",
      "Train Epoch: 24 [11392/17352 (66%)] Loss: -198773.890625\n",
      "Train Epoch: 24 [12800/17352 (74%)] Loss: -209687.718750\n",
      "Train Epoch: 24 [14208/17352 (82%)] Loss: -213542.750000\n",
      "Train Epoch: 24 [15499/17352 (89%)] Loss: -62093.609375\n",
      "Train Epoch: 24 [16368/17352 (94%)] Loss: -144496.843750\n",
      "Train Epoch: 24 [17162/17352 (99%)] Loss: -223732.125000\n",
      "    epoch          : 24\n",
      "    loss           : -197823.87710386954\n",
      "    val_loss       : -108886.15969238282\n",
      "Train Epoch: 25 [128/17352 (1%)] Loss: -226182.312500\n",
      "Train Epoch: 25 [1536/17352 (9%)] Loss: -219840.765625\n",
      "Train Epoch: 25 [2944/17352 (17%)] Loss: -189220.062500\n",
      "Train Epoch: 25 [4352/17352 (25%)] Loss: -210538.656250\n",
      "Train Epoch: 25 [5760/17352 (33%)] Loss: -222615.906250\n",
      "Train Epoch: 25 [7168/17352 (41%)] Loss: -192433.625000\n",
      "Train Epoch: 25 [8576/17352 (49%)] Loss: -207877.625000\n",
      "Train Epoch: 25 [9984/17352 (58%)] Loss: -206793.828125\n",
      "Train Epoch: 25 [11392/17352 (66%)] Loss: -213534.109375\n",
      "Train Epoch: 25 [12800/17352 (74%)] Loss: -238929.078125\n",
      "Train Epoch: 25 [14208/17352 (82%)] Loss: -212109.562500\n",
      "Train Epoch: 25 [15463/17352 (89%)] Loss: -143716.156250\n",
      "Train Epoch: 25 [16242/17352 (94%)] Loss: -130704.906250\n",
      "Train Epoch: 25 [17084/17352 (98%)] Loss: -153091.718750\n",
      "    epoch          : 25\n",
      "    loss           : -198311.12872601196\n",
      "    val_loss       : -109218.06295572917\n",
      "Train Epoch: 26 [128/17352 (1%)] Loss: -226571.531250\n",
      "Train Epoch: 26 [1536/17352 (9%)] Loss: -211050.281250\n",
      "Train Epoch: 26 [2944/17352 (17%)] Loss: -215690.625000\n",
      "Train Epoch: 26 [4352/17352 (25%)] Loss: -222292.937500\n",
      "Train Epoch: 26 [5760/17352 (33%)] Loss: -226994.656250\n",
      "Train Epoch: 26 [7168/17352 (41%)] Loss: -225385.062500\n",
      "Train Epoch: 26 [8576/17352 (49%)] Loss: -211937.140625\n",
      "Train Epoch: 26 [9984/17352 (58%)] Loss: -197541.437500\n",
      "Train Epoch: 26 [11392/17352 (66%)] Loss: -197179.359375\n",
      "Train Epoch: 26 [12800/17352 (74%)] Loss: -209501.187500\n",
      "Train Epoch: 26 [14208/17352 (82%)] Loss: -210047.968750\n",
      "Train Epoch: 26 [15544/17352 (90%)] Loss: -122064.609375\n",
      "Train Epoch: 26 [16249/17352 (94%)] Loss: -127753.359375\n",
      "Train Epoch: 26 [16958/17352 (98%)] Loss: -143102.468750\n",
      "    epoch          : 26\n",
      "    loss           : -198969.47272506816\n",
      "    val_loss       : -109536.5512125651\n",
      "Train Epoch: 27 [128/17352 (1%)] Loss: -222178.015625\n",
      "Train Epoch: 27 [1536/17352 (9%)] Loss: -233439.156250\n",
      "Train Epoch: 27 [2944/17352 (17%)] Loss: -208289.218750\n",
      "Train Epoch: 27 [4352/17352 (25%)] Loss: -209018.718750\n",
      "Train Epoch: 27 [5760/17352 (33%)] Loss: -208574.359375\n",
      "Train Epoch: 27 [7168/17352 (41%)] Loss: -224447.218750\n",
      "Train Epoch: 27 [8576/17352 (49%)] Loss: -217200.593750\n",
      "Train Epoch: 27 [9984/17352 (58%)] Loss: -226975.281250\n",
      "Train Epoch: 27 [11392/17352 (66%)] Loss: -226077.906250\n",
      "Train Epoch: 27 [12800/17352 (74%)] Loss: -224035.531250\n",
      "Train Epoch: 27 [14208/17352 (82%)] Loss: -211368.875000\n",
      "Train Epoch: 27 [15561/17352 (90%)] Loss: -160768.062500\n",
      "Train Epoch: 27 [16328/17352 (94%)] Loss: -27816.761719\n",
      "Train Epoch: 27 [16963/17352 (98%)] Loss: -128576.984375\n",
      "    epoch          : 27\n",
      "    loss           : -199636.08329619336\n",
      "    val_loss       : -109809.06123046875\n",
      "Train Epoch: 28 [128/17352 (1%)] Loss: -225055.718750\n",
      "Train Epoch: 28 [1536/17352 (9%)] Loss: -226683.921875\n",
      "Train Epoch: 28 [2944/17352 (17%)] Loss: -256315.484375\n",
      "Train Epoch: 28 [4352/17352 (25%)] Loss: -215422.281250\n",
      "Train Epoch: 28 [5760/17352 (33%)] Loss: -210183.984375\n",
      "Train Epoch: 28 [7168/17352 (41%)] Loss: -228410.937500\n",
      "Train Epoch: 28 [8576/17352 (49%)] Loss: -210119.218750\n",
      "Train Epoch: 28 [9984/17352 (58%)] Loss: -220204.984375\n",
      "Train Epoch: 28 [11392/17352 (66%)] Loss: -218507.593750\n",
      "Train Epoch: 28 [12800/17352 (74%)] Loss: -227416.531250\n",
      "Train Epoch: 28 [14208/17352 (82%)] Loss: -214951.718750\n",
      "Train Epoch: 28 [15521/17352 (89%)] Loss: -225999.750000\n",
      "Train Epoch: 28 [16343/17352 (94%)] Loss: -137960.875000\n",
      "Train Epoch: 28 [17095/17352 (99%)] Loss: -148832.125000\n",
      "    epoch          : 28\n",
      "    loss           : -199996.565046272\n",
      "    val_loss       : -109899.81926269531\n",
      "Train Epoch: 29 [128/17352 (1%)] Loss: -226460.828125\n",
      "Train Epoch: 29 [1536/17352 (9%)] Loss: -212229.109375\n",
      "Train Epoch: 29 [2944/17352 (17%)] Loss: -197410.953125\n",
      "Train Epoch: 29 [4352/17352 (25%)] Loss: -224026.062500\n",
      "Train Epoch: 29 [5760/17352 (33%)] Loss: -225273.656250\n",
      "Train Epoch: 29 [7168/17352 (41%)] Loss: -214207.484375\n",
      "Train Epoch: 29 [8576/17352 (49%)] Loss: -210201.531250\n",
      "Train Epoch: 29 [9984/17352 (58%)] Loss: -224327.406250\n",
      "Train Epoch: 29 [11392/17352 (66%)] Loss: -207883.812500\n",
      "Train Epoch: 29 [12800/17352 (74%)] Loss: -211485.468750\n",
      "Train Epoch: 29 [14208/17352 (82%)] Loss: -228427.343750\n",
      "Train Epoch: 29 [15534/17352 (90%)] Loss: -129594.523438\n",
      "Train Epoch: 29 [16377/17352 (94%)] Loss: -63012.421875\n",
      "Train Epoch: 29 [17128/17352 (99%)] Loss: -226821.875000\n",
      "    epoch          : 29\n",
      "    loss           : -200622.4385355495\n",
      "    val_loss       : -110031.6342203776\n",
      "Train Epoch: 30 [128/17352 (1%)] Loss: -198435.281250\n",
      "Train Epoch: 30 [1536/17352 (9%)] Loss: -216057.718750\n",
      "Train Epoch: 30 [2944/17352 (17%)] Loss: -240717.812500\n",
      "Train Epoch: 30 [4352/17352 (25%)] Loss: -235110.125000\n",
      "Train Epoch: 30 [5760/17352 (33%)] Loss: -216197.937500\n",
      "Train Epoch: 30 [7168/17352 (41%)] Loss: -215182.031250\n",
      "Train Epoch: 30 [8576/17352 (49%)] Loss: -228505.703125\n",
      "Train Epoch: 30 [9984/17352 (58%)] Loss: -229033.734375\n",
      "Train Epoch: 30 [11392/17352 (66%)] Loss: -256363.218750\n",
      "Train Epoch: 30 [12800/17352 (74%)] Loss: -229317.562500\n",
      "Train Epoch: 30 [14208/17352 (82%)] Loss: -241203.656250\n",
      "Train Epoch: 30 [15489/17352 (89%)] Loss: -164900.921875\n",
      "Train Epoch: 30 [16303/17352 (94%)] Loss: -187131.875000\n",
      "Train Epoch: 30 [17004/17352 (98%)] Loss: -190103.937500\n",
      "    epoch          : 30\n",
      "    loss           : -201084.26799431103\n",
      "    val_loss       : -110404.30431315103\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0821_122628/checkpoint-epoch30.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 31 [128/17352 (1%)] Loss: -227061.656250\n",
      "Train Epoch: 31 [1536/17352 (9%)] Loss: -228831.296875\n",
      "Train Epoch: 31 [2944/17352 (17%)] Loss: -256011.953125\n",
      "Train Epoch: 31 [4352/17352 (25%)] Loss: -213451.109375\n",
      "Train Epoch: 31 [5760/17352 (33%)] Loss: -230428.203125\n",
      "Train Epoch: 31 [7168/17352 (41%)] Loss: -195724.796875\n",
      "Train Epoch: 31 [8576/17352 (49%)] Loss: -230455.515625\n",
      "Train Epoch: 31 [9984/17352 (58%)] Loss: -229292.843750\n",
      "Train Epoch: 31 [11392/17352 (66%)] Loss: -226748.531250\n",
      "Train Epoch: 31 [12800/17352 (74%)] Loss: -210480.875000\n",
      "Train Epoch: 31 [14208/17352 (82%)] Loss: -241765.765625\n",
      "Train Epoch: 31 [15534/17352 (90%)] Loss: -136416.828125\n",
      "Train Epoch: 31 [16012/17352 (92%)] Loss: -63846.667969\n",
      "Train Epoch: 31 [17011/17352 (98%)] Loss: -140828.343750\n",
      "    epoch          : 31\n",
      "    loss           : -201677.96986092176\n",
      "    val_loss       : -110823.97408854167\n",
      "Train Epoch: 32 [128/17352 (1%)] Loss: -225366.515625\n",
      "Train Epoch: 32 [1536/17352 (9%)] Loss: -210632.312500\n",
      "Train Epoch: 32 [2944/17352 (17%)] Loss: -217083.687500\n",
      "Train Epoch: 32 [4352/17352 (25%)] Loss: -236795.812500\n",
      "Train Epoch: 32 [5760/17352 (33%)] Loss: -228307.875000\n",
      "Train Epoch: 32 [7168/17352 (41%)] Loss: -214270.781250\n",
      "Train Epoch: 32 [8576/17352 (49%)] Loss: -229619.093750\n",
      "Train Epoch: 32 [9984/17352 (58%)] Loss: -228705.843750\n",
      "Train Epoch: 32 [11392/17352 (66%)] Loss: -229174.875000\n",
      "Train Epoch: 32 [12800/17352 (74%)] Loss: -210196.687500\n",
      "Train Epoch: 32 [14208/17352 (82%)] Loss: -234818.093750\n",
      "Train Epoch: 32 [15488/17352 (89%)] Loss: -83587.304688\n",
      "Train Epoch: 32 [16220/17352 (93%)] Loss: -138195.031250\n",
      "Train Epoch: 32 [17033/17352 (98%)] Loss: -5264.251953\n",
      "    epoch          : 32\n",
      "    loss           : -201921.7117239933\n",
      "    val_loss       : -110907.83236490886\n",
      "Train Epoch: 33 [128/17352 (1%)] Loss: -210338.531250\n",
      "Train Epoch: 33 [1536/17352 (9%)] Loss: -224822.000000\n",
      "Train Epoch: 33 [2944/17352 (17%)] Loss: -231417.750000\n",
      "Train Epoch: 33 [4352/17352 (25%)] Loss: -222701.781250\n",
      "Train Epoch: 33 [5760/17352 (33%)] Loss: -234365.718750\n",
      "Train Epoch: 33 [7168/17352 (41%)] Loss: -229805.343750\n",
      "Train Epoch: 33 [8576/17352 (49%)] Loss: -230345.156250\n",
      "Train Epoch: 33 [9984/17352 (58%)] Loss: -232423.562500\n",
      "Train Epoch: 33 [11392/17352 (66%)] Loss: -217377.734375\n",
      "Train Epoch: 33 [12800/17352 (74%)] Loss: -215211.187500\n",
      "Train Epoch: 33 [14208/17352 (82%)] Loss: -243436.515625\n",
      "Train Epoch: 33 [15489/17352 (89%)] Loss: -126139.500000\n",
      "Train Epoch: 33 [16279/17352 (94%)] Loss: -129565.937500\n",
      "Train Epoch: 33 [17041/17352 (98%)] Loss: -84701.601562\n",
      "    epoch          : 33\n",
      "    loss           : -202316.3020363622\n",
      "    val_loss       : -111037.71417643229\n",
      "Train Epoch: 34 [128/17352 (1%)] Loss: -225302.125000\n",
      "Train Epoch: 34 [1536/17352 (9%)] Loss: -216487.156250\n",
      "Train Epoch: 34 [2944/17352 (17%)] Loss: -256028.468750\n",
      "Train Epoch: 34 [4352/17352 (25%)] Loss: -212103.843750\n",
      "Train Epoch: 34 [5760/17352 (33%)] Loss: -216549.265625\n",
      "Train Epoch: 34 [7168/17352 (41%)] Loss: -217326.781250\n",
      "Train Epoch: 34 [8576/17352 (49%)] Loss: -211750.843750\n",
      "Train Epoch: 34 [9984/17352 (58%)] Loss: -198277.781250\n",
      "Train Epoch: 34 [11392/17352 (66%)] Loss: -228979.265625\n",
      "Train Epoch: 34 [12800/17352 (74%)] Loss: -218873.203125\n",
      "Train Epoch: 34 [14208/17352 (82%)] Loss: -228088.296875\n",
      "Train Epoch: 34 [15555/17352 (90%)] Loss: -151607.312500\n",
      "Train Epoch: 34 [16321/17352 (94%)] Loss: -63485.023438\n",
      "Train Epoch: 34 [16898/17352 (97%)] Loss: -8569.144531\n",
      "    epoch          : 34\n",
      "    loss           : -202502.10490509646\n",
      "    val_loss       : -111033.84565429688\n",
      "Train Epoch: 35 [128/17352 (1%)] Loss: -226500.906250\n",
      "Train Epoch: 35 [1536/17352 (9%)] Loss: -216083.906250\n",
      "Train Epoch: 35 [2944/17352 (17%)] Loss: -210377.125000\n",
      "Train Epoch: 35 [4352/17352 (25%)] Loss: -227401.718750\n",
      "Train Epoch: 35 [5760/17352 (33%)] Loss: -215660.812500\n",
      "Train Epoch: 35 [7168/17352 (41%)] Loss: -255888.531250\n",
      "Train Epoch: 35 [8576/17352 (49%)] Loss: -224434.062500\n",
      "Train Epoch: 35 [9984/17352 (58%)] Loss: -229021.218750\n",
      "Train Epoch: 35 [11392/17352 (66%)] Loss: -211993.671875\n",
      "Train Epoch: 35 [12800/17352 (74%)] Loss: -227699.046875\n",
      "Train Epoch: 35 [14208/17352 (82%)] Loss: -213186.765625\n",
      "Train Epoch: 35 [15474/17352 (89%)] Loss: -23597.162109\n",
      "Train Epoch: 35 [16271/17352 (94%)] Loss: -9014.864258\n",
      "Train Epoch: 35 [17075/17352 (98%)] Loss: -88270.476562\n",
      "    epoch          : 35\n",
      "    loss           : -202401.85415792785\n",
      "    val_loss       : -111007.20549316406\n",
      "Train Epoch: 36 [128/17352 (1%)] Loss: -227161.968750\n",
      "Train Epoch: 36 [1536/17352 (9%)] Loss: -216317.078125\n",
      "Train Epoch: 36 [2944/17352 (17%)] Loss: -215884.984375\n",
      "Train Epoch: 36 [4352/17352 (25%)] Loss: -226204.265625\n",
      "Train Epoch: 36 [5760/17352 (33%)] Loss: -228957.312500\n",
      "Train Epoch: 36 [7168/17352 (41%)] Loss: -234670.875000\n",
      "Train Epoch: 36 [8576/17352 (49%)] Loss: -218538.500000\n",
      "Train Epoch: 36 [9984/17352 (58%)] Loss: -229710.437500\n",
      "Train Epoch: 36 [11392/17352 (66%)] Loss: -230832.906250\n",
      "Train Epoch: 36 [12800/17352 (74%)] Loss: -212540.187500\n",
      "Train Epoch: 36 [14208/17352 (82%)] Loss: -230572.687500\n",
      "Train Epoch: 36 [15485/17352 (89%)] Loss: -125886.156250\n",
      "Train Epoch: 36 [16227/17352 (94%)] Loss: -62765.484375\n",
      "Train Epoch: 36 [17058/17352 (98%)] Loss: -130696.546875\n",
      "    epoch          : 36\n",
      "    loss           : -202975.91238464764\n",
      "    val_loss       : -111411.3567952474\n",
      "Train Epoch: 37 [128/17352 (1%)] Loss: -232235.437500\n",
      "Train Epoch: 37 [1536/17352 (9%)] Loss: -218094.828125\n",
      "Train Epoch: 37 [2944/17352 (17%)] Loss: -217190.312500\n",
      "Train Epoch: 37 [4352/17352 (25%)] Loss: -218471.375000\n",
      "Train Epoch: 37 [5760/17352 (33%)] Loss: -243692.734375\n",
      "Train Epoch: 37 [7168/17352 (41%)] Loss: -256181.421875\n",
      "Train Epoch: 37 [8576/17352 (49%)] Loss: -210373.796875\n",
      "Train Epoch: 37 [9984/17352 (58%)] Loss: -195497.531250\n",
      "Train Epoch: 37 [11392/17352 (66%)] Loss: -227362.093750\n",
      "Train Epoch: 37 [12800/17352 (74%)] Loss: -222648.859375\n",
      "Train Epoch: 37 [14208/17352 (82%)] Loss: -230280.156250\n",
      "Train Epoch: 37 [15450/17352 (89%)] Loss: -169558.406250\n",
      "Train Epoch: 37 [16218/17352 (93%)] Loss: -164456.015625\n",
      "Train Epoch: 37 [16938/17352 (98%)] Loss: -5667.722168\n",
      "    epoch          : 37\n",
      "    loss           : -203345.3974248899\n",
      "    val_loss       : -111592.23264973958\n",
      "Train Epoch: 38 [128/17352 (1%)] Loss: -229625.437500\n",
      "Train Epoch: 38 [1536/17352 (9%)] Loss: -231937.640625\n",
      "Train Epoch: 38 [2944/17352 (17%)] Loss: -201982.828125\n",
      "Train Epoch: 38 [4352/17352 (25%)] Loss: -236613.265625\n",
      "Train Epoch: 38 [5760/17352 (33%)] Loss: -213651.250000\n",
      "Train Epoch: 38 [7168/17352 (41%)] Loss: -232203.062500\n",
      "Train Epoch: 38 [8576/17352 (49%)] Loss: -241994.906250\n",
      "Train Epoch: 38 [9984/17352 (58%)] Loss: -228067.312500\n",
      "Train Epoch: 38 [11392/17352 (66%)] Loss: -222743.828125\n",
      "Train Epoch: 38 [12800/17352 (74%)] Loss: -228122.906250\n",
      "Train Epoch: 38 [14208/17352 (82%)] Loss: -232830.984375\n",
      "Train Epoch: 38 [15542/17352 (90%)] Loss: -139923.625000\n",
      "Train Epoch: 38 [16290/17352 (94%)] Loss: -131009.734375\n",
      "Train Epoch: 38 [17094/17352 (99%)] Loss: -180929.406250\n",
      "    epoch          : 38\n",
      "    loss           : -203628.93813574873\n",
      "    val_loss       : -111618.75021972656\n",
      "Train Epoch: 39 [128/17352 (1%)] Loss: -200423.421875\n",
      "Train Epoch: 39 [1536/17352 (9%)] Loss: -228602.484375\n",
      "Train Epoch: 39 [2944/17352 (17%)] Loss: -242437.375000\n",
      "Train Epoch: 39 [4352/17352 (25%)] Loss: -235264.343750\n",
      "Train Epoch: 39 [5760/17352 (33%)] Loss: -231102.875000\n",
      "Train Epoch: 39 [7168/17352 (41%)] Loss: -200456.312500\n",
      "Train Epoch: 39 [8576/17352 (49%)] Loss: -214167.750000\n",
      "Train Epoch: 39 [9984/17352 (58%)] Loss: -233478.343750\n",
      "Train Epoch: 39 [11392/17352 (66%)] Loss: -216470.078125\n",
      "Train Epoch: 39 [12800/17352 (74%)] Loss: -229011.500000\n",
      "Train Epoch: 39 [14208/17352 (82%)] Loss: -212757.343750\n",
      "Train Epoch: 39 [15448/17352 (89%)] Loss: -23879.951172\n",
      "Train Epoch: 39 [16232/17352 (94%)] Loss: -134184.812500\n",
      "Train Epoch: 39 [17038/17352 (98%)] Loss: -132384.921875\n",
      "    epoch          : 39\n",
      "    loss           : -203863.20888606334\n",
      "    val_loss       : -111831.64951985677\n",
      "Train Epoch: 40 [128/17352 (1%)] Loss: -229349.312500\n",
      "Train Epoch: 40 [1536/17352 (9%)] Loss: -218937.656250\n",
      "Train Epoch: 40 [2944/17352 (17%)] Loss: -215942.828125\n",
      "Train Epoch: 40 [4352/17352 (25%)] Loss: -231040.875000\n",
      "Train Epoch: 40 [5760/17352 (33%)] Loss: -243337.375000\n",
      "Train Epoch: 40 [7168/17352 (41%)] Loss: -233480.812500\n",
      "Train Epoch: 40 [8576/17352 (49%)] Loss: -203552.875000\n",
      "Train Epoch: 40 [9984/17352 (58%)] Loss: -216057.593750\n",
      "Train Epoch: 40 [11392/17352 (66%)] Loss: -199723.546875\n",
      "Train Epoch: 40 [12800/17352 (74%)] Loss: -215153.156250\n",
      "Train Epoch: 40 [14208/17352 (82%)] Loss: -217462.187500\n",
      "Train Epoch: 40 [15576/17352 (90%)] Loss: -191928.187500\n",
      "Train Epoch: 40 [16449/17352 (95%)] Loss: -229306.468750\n",
      "Train Epoch: 40 [17075/17352 (98%)] Loss: -5503.376465\n",
      "    epoch          : 40\n",
      "    loss           : -204156.97657233116\n",
      "    val_loss       : -111934.7063232422\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0821_122628/checkpoint-epoch40.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 41 [128/17352 (1%)] Loss: -231698.156250\n",
      "Train Epoch: 41 [1536/17352 (9%)] Loss: -226725.125000\n",
      "Train Epoch: 41 [2944/17352 (17%)] Loss: -257618.343750\n",
      "Train Epoch: 41 [4352/17352 (25%)] Loss: -239235.531250\n",
      "Train Epoch: 41 [5760/17352 (33%)] Loss: -210067.421875\n",
      "Train Epoch: 41 [7168/17352 (41%)] Loss: -200387.812500\n",
      "Train Epoch: 41 [8576/17352 (49%)] Loss: -241674.921875\n",
      "Train Epoch: 41 [9984/17352 (58%)] Loss: -235306.750000\n",
      "Train Epoch: 41 [11392/17352 (66%)] Loss: -228284.093750\n",
      "Train Epoch: 41 [12800/17352 (74%)] Loss: -231154.531250\n",
      "Train Epoch: 41 [14208/17352 (82%)] Loss: -244627.187500\n",
      "Train Epoch: 41 [15418/17352 (89%)] Loss: -77219.492188\n",
      "Train Epoch: 41 [16300/17352 (94%)] Loss: -195856.984375\n",
      "Train Epoch: 41 [16982/17352 (98%)] Loss: -140374.656250\n",
      "    epoch          : 41\n",
      "    loss           : -204316.7318647756\n",
      "    val_loss       : -112081.96105957031\n",
      "Train Epoch: 42 [128/17352 (1%)] Loss: -201344.312500\n",
      "Train Epoch: 42 [1536/17352 (9%)] Loss: -230873.140625\n",
      "Train Epoch: 42 [2944/17352 (17%)] Loss: -243375.250000\n",
      "Train Epoch: 42 [4352/17352 (25%)] Loss: -229802.062500\n",
      "Train Epoch: 42 [5760/17352 (33%)] Loss: -216428.218750\n",
      "Train Epoch: 42 [7168/17352 (41%)] Loss: -256671.250000\n",
      "Train Epoch: 42 [8576/17352 (49%)] Loss: -210776.390625\n",
      "Train Epoch: 42 [9984/17352 (58%)] Loss: -232911.843750\n",
      "Train Epoch: 42 [11392/17352 (66%)] Loss: -206129.781250\n",
      "Train Epoch: 42 [12800/17352 (74%)] Loss: -228290.656250\n",
      "Train Epoch: 42 [14208/17352 (82%)] Loss: -232414.156250\n",
      "Train Epoch: 42 [15476/17352 (89%)] Loss: -27406.433594\n",
      "Train Epoch: 42 [16178/17352 (93%)] Loss: -23744.339844\n",
      "Train Epoch: 42 [17031/17352 (98%)] Loss: -130479.218750\n",
      "    epoch          : 42\n",
      "    loss           : -204325.38971070157\n",
      "    val_loss       : -111911.269921875\n",
      "Train Epoch: 43 [128/17352 (1%)] Loss: -229498.750000\n",
      "Train Epoch: 43 [1536/17352 (9%)] Loss: -237870.140625\n",
      "Train Epoch: 43 [2944/17352 (17%)] Loss: -231652.765625\n",
      "Train Epoch: 43 [4352/17352 (25%)] Loss: -224584.187500\n",
      "Train Epoch: 43 [5760/17352 (33%)] Loss: -231658.906250\n",
      "Train Epoch: 43 [7168/17352 (41%)] Loss: -216825.625000\n",
      "Train Epoch: 43 [8576/17352 (49%)] Loss: -222558.625000\n",
      "Train Epoch: 43 [9984/17352 (58%)] Loss: -202356.125000\n",
      "Train Epoch: 43 [11392/17352 (66%)] Loss: -208113.406250\n",
      "Train Epoch: 43 [12800/17352 (74%)] Loss: -231978.765625\n",
      "Train Epoch: 43 [14208/17352 (82%)] Loss: -245526.203125\n",
      "Train Epoch: 43 [15519/17352 (89%)] Loss: -183977.718750\n",
      "Train Epoch: 43 [16217/17352 (93%)] Loss: -5486.034180\n",
      "Train Epoch: 43 [16998/17352 (98%)] Loss: -165894.906250\n",
      "    epoch          : 43\n",
      "    loss           : -204613.17546009857\n",
      "    val_loss       : -112158.18203125\n",
      "Train Epoch: 44 [128/17352 (1%)] Loss: -200250.562500\n",
      "Train Epoch: 44 [1536/17352 (9%)] Loss: -231007.640625\n",
      "Train Epoch: 44 [2944/17352 (17%)] Loss: -231655.296875\n",
      "Train Epoch: 44 [4352/17352 (25%)] Loss: -213708.734375\n",
      "Train Epoch: 44 [5760/17352 (33%)] Loss: -214527.468750\n",
      "Train Epoch: 44 [7168/17352 (41%)] Loss: -256843.187500\n",
      "Train Epoch: 44 [8576/17352 (49%)] Loss: -212092.453125\n",
      "Train Epoch: 44 [9984/17352 (58%)] Loss: -204588.406250\n",
      "Train Epoch: 44 [11392/17352 (66%)] Loss: -208528.015625\n",
      "Train Epoch: 44 [12800/17352 (74%)] Loss: -212679.140625\n",
      "Train Epoch: 44 [14208/17352 (82%)] Loss: -243424.171875\n",
      "Train Epoch: 44 [15530/17352 (89%)] Loss: -232858.312500\n",
      "Train Epoch: 44 [16380/17352 (94%)] Loss: -141475.828125\n",
      "Train Epoch: 44 [17061/17352 (98%)] Loss: -168624.656250\n",
      "    epoch          : 44\n",
      "    loss           : -204976.5009765625\n",
      "    val_loss       : -112394.10686035156\n",
      "Train Epoch: 45 [128/17352 (1%)] Loss: -233601.843750\n",
      "Train Epoch: 45 [1536/17352 (9%)] Loss: -217688.312500\n",
      "Train Epoch: 45 [2944/17352 (17%)] Loss: -220429.843750\n",
      "Train Epoch: 45 [4352/17352 (25%)] Loss: -229273.125000\n",
      "Train Epoch: 45 [5760/17352 (33%)] Loss: -244469.062500\n",
      "Train Epoch: 45 [7168/17352 (41%)] Loss: -205581.265625\n",
      "Train Epoch: 45 [8576/17352 (49%)] Loss: -213700.250000\n",
      "Train Epoch: 45 [9984/17352 (58%)] Loss: -243065.468750\n",
      "Train Epoch: 45 [11392/17352 (66%)] Loss: -231800.875000\n",
      "Train Epoch: 45 [12800/17352 (74%)] Loss: -215242.593750\n",
      "Train Epoch: 45 [14208/17352 (82%)] Loss: -215781.109375\n",
      "Train Epoch: 45 [15489/17352 (89%)] Loss: -166907.562500\n",
      "Train Epoch: 45 [16199/17352 (93%)] Loss: -193258.203125\n",
      "Train Epoch: 45 [16944/17352 (98%)] Loss: -138255.500000\n",
      "    epoch          : 45\n",
      "    loss           : -205212.47567114094\n",
      "    val_loss       : -112450.2121826172\n",
      "Train Epoch: 46 [128/17352 (1%)] Loss: -202901.640625\n",
      "Train Epoch: 46 [1536/17352 (9%)] Loss: -236554.531250\n",
      "Train Epoch: 46 [2944/17352 (17%)] Loss: -229205.062500\n",
      "Train Epoch: 46 [4352/17352 (25%)] Loss: -222071.843750\n",
      "Train Epoch: 46 [5760/17352 (33%)] Loss: -222290.796875\n",
      "Train Epoch: 46 [7168/17352 (41%)] Loss: -204147.656250\n",
      "Train Epoch: 46 [8576/17352 (49%)] Loss: -205111.375000\n",
      "Train Epoch: 46 [9984/17352 (58%)] Loss: -232569.468750\n",
      "Train Epoch: 46 [11392/17352 (66%)] Loss: -221430.312500\n",
      "Train Epoch: 46 [12800/17352 (74%)] Loss: -225456.593750\n",
      "Train Epoch: 46 [14208/17352 (82%)] Loss: -219464.031250\n",
      "Train Epoch: 46 [15448/17352 (89%)] Loss: -87987.929688\n",
      "Train Epoch: 46 [16147/17352 (93%)] Loss: -146578.187500\n",
      "Train Epoch: 46 [16854/17352 (97%)] Loss: -145194.796875\n",
      "    epoch          : 46\n",
      "    loss           : -204949.4978633599\n",
      "    val_loss       : -112426.53575846353\n",
      "Train Epoch: 47 [128/17352 (1%)] Loss: -232427.906250\n",
      "Train Epoch: 47 [1536/17352 (9%)] Loss: -238390.609375\n",
      "Train Epoch: 47 [2944/17352 (17%)] Loss: -219998.234375\n",
      "Train Epoch: 47 [4352/17352 (25%)] Loss: -235370.656250\n",
      "Train Epoch: 47 [5760/17352 (33%)] Loss: -213252.812500\n",
      "Train Epoch: 47 [7168/17352 (41%)] Loss: -219816.250000\n",
      "Train Epoch: 47 [8576/17352 (49%)] Loss: -225435.062500\n",
      "Train Epoch: 47 [9984/17352 (58%)] Loss: -236096.671875\n",
      "Train Epoch: 47 [11392/17352 (66%)] Loss: -221214.593750\n",
      "Train Epoch: 47 [12800/17352 (74%)] Loss: -216639.906250\n",
      "Train Epoch: 47 [14208/17352 (82%)] Loss: -221128.156250\n",
      "Train Epoch: 47 [15523/17352 (89%)] Loss: -142247.187500\n",
      "Train Epoch: 47 [16477/17352 (95%)] Loss: -151986.890625\n",
      "Train Epoch: 47 [17113/17352 (99%)] Loss: -135581.656250\n",
      "    epoch          : 47\n",
      "    loss           : -205373.839191616\n",
      "    val_loss       : -111970.35633138022\n",
      "Train Epoch: 48 [128/17352 (1%)] Loss: -216821.296875\n",
      "Train Epoch: 48 [1536/17352 (9%)] Loss: -228317.937500\n",
      "Train Epoch: 48 [2944/17352 (17%)] Loss: -220403.656250\n",
      "Train Epoch: 48 [4352/17352 (25%)] Loss: -233552.125000\n",
      "Train Epoch: 48 [5760/17352 (33%)] Loss: -221388.937500\n",
      "Train Epoch: 48 [7168/17352 (41%)] Loss: -234130.906250\n",
      "Train Epoch: 48 [8576/17352 (49%)] Loss: -234625.812500\n",
      "Train Epoch: 48 [9984/17352 (58%)] Loss: -219030.828125\n",
      "Train Epoch: 48 [11392/17352 (66%)] Loss: -231599.015625\n",
      "Train Epoch: 48 [12800/17352 (74%)] Loss: -213878.468750\n",
      "Train Epoch: 48 [14208/17352 (82%)] Loss: -230552.437500\n",
      "Train Epoch: 48 [15428/17352 (89%)] Loss: -64790.925781\n",
      "Train Epoch: 48 [16287/17352 (94%)] Loss: -156167.437500\n",
      "Train Epoch: 48 [16975/17352 (98%)] Loss: -169082.406250\n",
      "    epoch          : 48\n",
      "    loss           : -205417.06337497378\n",
      "    val_loss       : -112600.82276204428\n",
      "Train Epoch: 49 [128/17352 (1%)] Loss: -207327.562500\n",
      "Train Epoch: 49 [1536/17352 (9%)] Loss: -219224.078125\n",
      "Train Epoch: 49 [2944/17352 (17%)] Loss: -203013.093750\n",
      "Train Epoch: 49 [4352/17352 (25%)] Loss: -229822.765625\n",
      "Train Epoch: 49 [5760/17352 (33%)] Loss: -245430.875000\n",
      "Train Epoch: 49 [7168/17352 (41%)] Loss: -230821.531250\n",
      "Train Epoch: 49 [8576/17352 (49%)] Loss: -234912.812500\n",
      "Train Epoch: 49 [9984/17352 (58%)] Loss: -216260.437500\n",
      "Train Epoch: 49 [11392/17352 (66%)] Loss: -207116.062500\n",
      "Train Epoch: 49 [12800/17352 (74%)] Loss: -229865.718750\n",
      "Train Epoch: 49 [14208/17352 (82%)] Loss: -221339.359375\n",
      "Train Epoch: 49 [15483/17352 (89%)] Loss: -133624.437500\n",
      "Train Epoch: 49 [16314/17352 (94%)] Loss: -169219.500000\n",
      "Train Epoch: 49 [17107/17352 (99%)] Loss: -232845.218750\n",
      "    epoch          : 49\n",
      "    loss           : -205836.5626900692\n",
      "    val_loss       : -112669.66232910156\n",
      "Train Epoch: 50 [128/17352 (1%)] Loss: -231507.625000\n",
      "Train Epoch: 50 [1536/17352 (9%)] Loss: -236055.843750\n",
      "Train Epoch: 50 [2944/17352 (17%)] Loss: -202182.359375\n",
      "Train Epoch: 50 [4352/17352 (25%)] Loss: -227366.984375\n",
      "Train Epoch: 50 [5760/17352 (33%)] Loss: -234540.515625\n",
      "Train Epoch: 50 [7168/17352 (41%)] Loss: -229825.312500\n",
      "Train Epoch: 50 [8576/17352 (49%)] Loss: -231822.109375\n",
      "Train Epoch: 50 [9984/17352 (58%)] Loss: -217563.906250\n",
      "Train Epoch: 50 [11392/17352 (66%)] Loss: -206715.218750\n",
      "Train Epoch: 50 [12800/17352 (74%)] Loss: -215670.687500\n",
      "Train Epoch: 50 [14208/17352 (82%)] Loss: -233517.187500\n",
      "Train Epoch: 50 [15480/17352 (89%)] Loss: -65294.453125\n",
      "Train Epoch: 50 [16131/17352 (93%)] Loss: -129705.703125\n",
      "Train Epoch: 50 [17009/17352 (98%)] Loss: -24191.781250\n",
      "    epoch          : 50\n",
      "    loss           : -205934.751065043\n",
      "    val_loss       : -112749.93348795573\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0821_122628/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [128/17352 (1%)] Loss: -230293.640625\n",
      "Train Epoch: 51 [1536/17352 (9%)] Loss: -237311.218750\n",
      "Train Epoch: 51 [2944/17352 (17%)] Loss: -216057.921875\n",
      "Train Epoch: 51 [4352/17352 (25%)] Loss: -230635.281250\n",
      "Train Epoch: 51 [5760/17352 (33%)] Loss: -219184.265625\n",
      "Train Epoch: 51 [7168/17352 (41%)] Loss: -231325.812500\n",
      "Train Epoch: 51 [8576/17352 (49%)] Loss: -209122.281250\n",
      "Train Epoch: 51 [9984/17352 (58%)] Loss: -219254.218750\n",
      "Train Epoch: 51 [11392/17352 (66%)] Loss: -232501.187500\n",
      "Train Epoch: 51 [12800/17352 (74%)] Loss: -228967.406250\n",
      "Train Epoch: 51 [14208/17352 (82%)] Loss: -245893.937500\n",
      "Train Epoch: 51 [15512/17352 (89%)] Loss: -140739.031250\n",
      "Train Epoch: 51 [16297/17352 (94%)] Loss: -192795.671875\n",
      "Train Epoch: 51 [17074/17352 (98%)] Loss: -64766.593750\n",
      "    epoch          : 51\n",
      "    loss           : -206015.11935035654\n",
      "    val_loss       : -112689.00244954428\n",
      "Train Epoch: 52 [128/17352 (1%)] Loss: -223287.203125\n",
      "Train Epoch: 52 [1536/17352 (9%)] Loss: -218806.656250\n",
      "Train Epoch: 52 [2944/17352 (17%)] Loss: -221087.031250\n",
      "Train Epoch: 52 [4352/17352 (25%)] Loss: -215951.078125\n",
      "Train Epoch: 52 [5760/17352 (33%)] Loss: -232819.015625\n",
      "Train Epoch: 52 [7168/17352 (41%)] Loss: -258407.937500\n",
      "Train Epoch: 52 [8576/17352 (49%)] Loss: -215358.656250\n",
      "Train Epoch: 52 [9984/17352 (58%)] Loss: -230899.640625\n",
      "Train Epoch: 52 [11392/17352 (66%)] Loss: -207092.203125\n",
      "Train Epoch: 52 [12800/17352 (74%)] Loss: -219662.421875\n",
      "Train Epoch: 52 [14208/17352 (82%)] Loss: -216732.953125\n",
      "Train Epoch: 52 [15530/17352 (89%)] Loss: -157225.046875\n",
      "Train Epoch: 52 [16530/17352 (95%)] Loss: -168472.609375\n",
      "Train Epoch: 52 [17084/17352 (98%)] Loss: -88184.523438\n",
      "    epoch          : 52\n",
      "    loss           : -206254.3597780778\n",
      "    val_loss       : -112753.70410970053\n",
      "Train Epoch: 53 [128/17352 (1%)] Loss: -232692.125000\n",
      "Train Epoch: 53 [1536/17352 (9%)] Loss: -232049.343750\n",
      "Train Epoch: 53 [2944/17352 (17%)] Loss: -215805.140625\n",
      "Train Epoch: 53 [4352/17352 (25%)] Loss: -237361.671875\n",
      "Train Epoch: 53 [5760/17352 (33%)] Loss: -235806.953125\n",
      "Train Epoch: 53 [7168/17352 (41%)] Loss: -256859.968750\n",
      "Train Epoch: 53 [8576/17352 (49%)] Loss: -214896.875000\n",
      "Train Epoch: 53 [9984/17352 (58%)] Loss: -235268.203125\n",
      "Train Epoch: 53 [11392/17352 (66%)] Loss: -205473.984375\n",
      "Train Epoch: 53 [12800/17352 (74%)] Loss: -234509.625000\n",
      "Train Epoch: 53 [14208/17352 (82%)] Loss: -223446.625000\n",
      "Train Epoch: 53 [15453/17352 (89%)] Loss: -78150.507812\n",
      "Train Epoch: 53 [16137/17352 (93%)] Loss: -5323.648438\n",
      "Train Epoch: 53 [16924/17352 (98%)] Loss: -167578.984375\n",
      "    epoch          : 53\n",
      "    loss           : -206421.9674064073\n",
      "    val_loss       : -112733.3177327474\n",
      "Train Epoch: 54 [128/17352 (1%)] Loss: -233823.875000\n",
      "Train Epoch: 54 [1536/17352 (9%)] Loss: -219650.203125\n",
      "Train Epoch: 54 [2944/17352 (17%)] Loss: -225982.062500\n",
      "Train Epoch: 54 [4352/17352 (25%)] Loss: -232056.828125\n",
      "Train Epoch: 54 [5760/17352 (33%)] Loss: -232462.906250\n",
      "Train Epoch: 54 [7168/17352 (41%)] Loss: -230457.343750\n",
      "Train Epoch: 54 [8576/17352 (49%)] Loss: -213300.593750\n",
      "Train Epoch: 54 [9984/17352 (58%)] Loss: -233837.187500\n",
      "Train Epoch: 54 [11392/17352 (66%)] Loss: -205197.531250\n",
      "Train Epoch: 54 [12800/17352 (74%)] Loss: -224629.171875\n",
      "Train Epoch: 54 [14208/17352 (82%)] Loss: -221877.609375\n",
      "Train Epoch: 54 [15489/17352 (89%)] Loss: -86925.171875\n",
      "Train Epoch: 54 [16169/17352 (93%)] Loss: -142018.343750\n",
      "Train Epoch: 54 [16987/17352 (98%)] Loss: -146699.296875\n",
      "    epoch          : 54\n",
      "    loss           : -206347.89217832425\n",
      "    val_loss       : -112999.20610351562\n",
      "Train Epoch: 55 [128/17352 (1%)] Loss: -230091.531250\n",
      "Train Epoch: 55 [1536/17352 (9%)] Loss: -226731.093750\n",
      "Train Epoch: 55 [2944/17352 (17%)] Loss: -207102.640625\n",
      "Train Epoch: 55 [4352/17352 (25%)] Loss: -231885.546875\n",
      "Train Epoch: 55 [5760/17352 (33%)] Loss: -222778.656250\n",
      "Train Epoch: 55 [7168/17352 (41%)] Loss: -234092.500000\n",
      "Train Epoch: 55 [8576/17352 (49%)] Loss: -224321.093750\n",
      "Train Epoch: 55 [9984/17352 (58%)] Loss: -236338.515625\n",
      "Train Epoch: 55 [11392/17352 (66%)] Loss: -233922.406250\n",
      "Train Epoch: 55 [12800/17352 (74%)] Loss: -229943.078125\n",
      "Train Epoch: 55 [14208/17352 (82%)] Loss: -242814.875000\n",
      "Train Epoch: 55 [15464/17352 (89%)] Loss: -85963.500000\n",
      "Train Epoch: 55 [16333/17352 (94%)] Loss: -233040.718750\n",
      "Train Epoch: 55 [16930/17352 (98%)] Loss: -66034.406250\n",
      "    epoch          : 55\n",
      "    loss           : -206658.2959639786\n",
      "    val_loss       : -113070.69117838542\n",
      "Train Epoch: 56 [128/17352 (1%)] Loss: -205719.171875\n",
      "Train Epoch: 56 [1536/17352 (9%)] Loss: -223522.906250\n",
      "Train Epoch: 56 [2944/17352 (17%)] Loss: -235513.281250\n",
      "Train Epoch: 56 [4352/17352 (25%)] Loss: -241004.812500\n",
      "Train Epoch: 56 [5760/17352 (33%)] Loss: -223560.515625\n",
      "Train Epoch: 56 [7168/17352 (41%)] Loss: -208122.593750\n",
      "Train Epoch: 56 [8576/17352 (49%)] Loss: -217388.468750\n",
      "Train Epoch: 56 [9984/17352 (58%)] Loss: -238473.765625\n",
      "Train Epoch: 56 [11392/17352 (66%)] Loss: -225800.765625\n",
      "Train Epoch: 56 [12800/17352 (74%)] Loss: -220012.625000\n",
      "Train Epoch: 56 [14208/17352 (82%)] Loss: -237614.015625\n",
      "Train Epoch: 56 [15543/17352 (90%)] Loss: -168474.609375\n",
      "Train Epoch: 56 [16296/17352 (94%)] Loss: -197621.359375\n",
      "Train Epoch: 56 [17017/17352 (98%)] Loss: -196939.593750\n",
      "    epoch          : 56\n",
      "    loss           : -206873.41081166107\n",
      "    val_loss       : -113141.09200032552\n",
      "Train Epoch: 57 [128/17352 (1%)] Loss: -232339.062500\n",
      "Train Epoch: 57 [1536/17352 (9%)] Loss: -229642.968750\n",
      "Train Epoch: 57 [2944/17352 (17%)] Loss: -243880.156250\n",
      "Train Epoch: 57 [4352/17352 (25%)] Loss: -239511.500000\n",
      "Train Epoch: 57 [5760/17352 (33%)] Loss: -236207.343750\n",
      "Train Epoch: 57 [7168/17352 (41%)] Loss: -231447.406250\n",
      "Train Epoch: 57 [8576/17352 (49%)] Loss: -244887.000000\n",
      "Train Epoch: 57 [9984/17352 (58%)] Loss: -231463.875000\n",
      "Train Epoch: 57 [11392/17352 (66%)] Loss: -232073.453125\n",
      "Train Epoch: 57 [12800/17352 (74%)] Loss: -236696.031250\n",
      "Train Epoch: 57 [14208/17352 (82%)] Loss: -215605.421875\n",
      "Train Epoch: 57 [15466/17352 (89%)] Loss: -9417.322266\n",
      "Train Epoch: 57 [16357/17352 (94%)] Loss: -156675.906250\n",
      "Train Epoch: 57 [17113/17352 (99%)] Loss: -169244.578125\n",
      "    epoch          : 57\n",
      "    loss           : -207012.76118131293\n",
      "    val_loss       : -113208.82622884115\n",
      "Train Epoch: 58 [128/17352 (1%)] Loss: -206116.156250\n",
      "Train Epoch: 58 [1536/17352 (9%)] Loss: -237549.562500\n",
      "Train Epoch: 58 [2944/17352 (17%)] Loss: -225094.609375\n",
      "Train Epoch: 58 [4352/17352 (25%)] Loss: -228512.125000\n",
      "Train Epoch: 58 [5760/17352 (33%)] Loss: -244962.375000\n",
      "Train Epoch: 58 [7168/17352 (41%)] Loss: -221662.500000\n",
      "Train Epoch: 58 [8576/17352 (49%)] Loss: -236601.656250\n",
      "Train Epoch: 58 [9984/17352 (58%)] Loss: -237049.375000\n",
      "Train Epoch: 58 [11392/17352 (66%)] Loss: -256882.296875\n",
      "Train Epoch: 58 [12800/17352 (74%)] Loss: -219373.406250\n",
      "Train Epoch: 58 [14208/17352 (82%)] Loss: -224310.031250\n",
      "Train Epoch: 58 [15520/17352 (89%)] Loss: -128895.562500\n",
      "Train Epoch: 58 [16245/17352 (94%)] Loss: -164349.937500\n",
      "Train Epoch: 58 [17002/17352 (98%)] Loss: -135656.109375\n",
      "    epoch          : 58\n",
      "    loss           : -207210.9828544463\n",
      "    val_loss       : -113242.52755533854\n",
      "Train Epoch: 59 [128/17352 (1%)] Loss: -224357.171875\n",
      "Train Epoch: 59 [1536/17352 (9%)] Loss: -219920.500000\n",
      "Train Epoch: 59 [2944/17352 (17%)] Loss: -223799.125000\n",
      "Train Epoch: 59 [4352/17352 (25%)] Loss: -236749.015625\n",
      "Train Epoch: 59 [5760/17352 (33%)] Loss: -230999.625000\n",
      "Train Epoch: 59 [7168/17352 (41%)] Loss: -232391.593750\n",
      "Train Epoch: 59 [8576/17352 (49%)] Loss: -224353.218750\n",
      "Train Epoch: 59 [9984/17352 (58%)] Loss: -239568.875000\n",
      "Train Epoch: 59 [11392/17352 (66%)] Loss: -235852.843750\n",
      "Train Epoch: 59 [12800/17352 (74%)] Loss: -235548.765625\n",
      "Train Epoch: 59 [14208/17352 (82%)] Loss: -223247.406250\n",
      "Train Epoch: 59 [15423/17352 (89%)] Loss: -86615.250000\n",
      "Train Epoch: 59 [16187/17352 (93%)] Loss: -26607.437500\n",
      "Train Epoch: 59 [17010/17352 (98%)] Loss: -183781.812500\n",
      "    epoch          : 59\n",
      "    loss           : -207199.51215787543\n",
      "    val_loss       : -113022.87121582031\n",
      "Train Epoch: 60 [128/17352 (1%)] Loss: -230511.062500\n",
      "Train Epoch: 60 [1536/17352 (9%)] Loss: -222944.328125\n",
      "Train Epoch: 60 [2944/17352 (17%)] Loss: -221340.718750\n",
      "Train Epoch: 60 [4352/17352 (25%)] Loss: -216890.328125\n",
      "Train Epoch: 60 [5760/17352 (33%)] Loss: -224073.859375\n",
      "Train Epoch: 60 [7168/17352 (41%)] Loss: -239061.109375\n",
      "Train Epoch: 60 [8576/17352 (49%)] Loss: -234581.953125\n",
      "Train Epoch: 60 [9984/17352 (58%)] Loss: -217632.546875\n",
      "Train Epoch: 60 [11392/17352 (66%)] Loss: -206252.937500\n",
      "Train Epoch: 60 [12800/17352 (74%)] Loss: -238876.687500\n",
      "Train Epoch: 60 [14208/17352 (82%)] Loss: -237661.250000\n",
      "Train Epoch: 60 [15397/17352 (89%)] Loss: -63389.679688\n",
      "Train Epoch: 60 [16138/17352 (93%)] Loss: -9130.269531\n",
      "Train Epoch: 60 [16943/17352 (98%)] Loss: -234120.390625\n",
      "    epoch          : 60\n",
      "    loss           : -206977.86440200295\n",
      "    val_loss       : -113006.30110677083\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0821_122628/checkpoint-epoch60.pth ...\n",
      "Train Epoch: 61 [128/17352 (1%)] Loss: -208192.859375\n",
      "Train Epoch: 61 [1536/17352 (9%)] Loss: -231827.890625\n",
      "Train Epoch: 61 [2944/17352 (17%)] Loss: -208223.062500\n",
      "Train Epoch: 61 [4352/17352 (25%)] Loss: -237409.843750\n",
      "Train Epoch: 61 [5760/17352 (33%)] Loss: -224213.312500\n",
      "Train Epoch: 61 [7168/17352 (41%)] Loss: -237337.437500\n",
      "Train Epoch: 61 [8576/17352 (49%)] Loss: -245923.437500\n",
      "Train Epoch: 61 [9984/17352 (58%)] Loss: -229913.125000\n",
      "Train Epoch: 61 [11392/17352 (66%)] Loss: -210440.921875\n",
      "Train Epoch: 61 [12800/17352 (74%)] Loss: -216096.031250\n",
      "Train Epoch: 61 [14208/17352 (82%)] Loss: -218954.500000\n",
      "Train Epoch: 61 [15511/17352 (89%)] Loss: -182090.796875\n",
      "Train Epoch: 61 [16280/17352 (94%)] Loss: -5458.350586\n",
      "Train Epoch: 61 [17032/17352 (98%)] Loss: -141418.250000\n",
      "    epoch          : 61\n",
      "    loss           : -207405.9007642093\n",
      "    val_loss       : -113386.10455729166\n",
      "Train Epoch: 62 [128/17352 (1%)] Loss: -220074.234375\n",
      "Train Epoch: 62 [1536/17352 (9%)] Loss: -232342.234375\n",
      "Train Epoch: 62 [2944/17352 (17%)] Loss: -237520.781250\n",
      "Train Epoch: 62 [4352/17352 (25%)] Loss: -231129.640625\n",
      "Train Epoch: 62 [5760/17352 (33%)] Loss: -210829.812500\n",
      "Train Epoch: 62 [7168/17352 (41%)] Loss: -233703.375000\n",
      "Train Epoch: 62 [8576/17352 (49%)] Loss: -235578.125000\n",
      "Train Epoch: 62 [9984/17352 (58%)] Loss: -217863.937500\n",
      "Train Epoch: 62 [11392/17352 (66%)] Loss: -226591.359375\n",
      "Train Epoch: 62 [12800/17352 (74%)] Loss: -217051.781250\n",
      "Train Epoch: 62 [14208/17352 (82%)] Loss: -231005.921875\n",
      "Train Epoch: 62 [15378/17352 (89%)] Loss: -28327.519531\n",
      "Train Epoch: 62 [16103/17352 (93%)] Loss: -86426.578125\n",
      "Train Epoch: 62 [16876/17352 (97%)] Loss: -156468.500000\n",
      "    epoch          : 62\n",
      "    loss           : -207620.97287908976\n",
      "    val_loss       : -113458.1861735026\n",
      "Train Epoch: 63 [128/17352 (1%)] Loss: -207610.343750\n",
      "Train Epoch: 63 [1536/17352 (9%)] Loss: -224957.578125\n",
      "Train Epoch: 63 [2944/17352 (17%)] Loss: -210600.781250\n",
      "Train Epoch: 63 [4352/17352 (25%)] Loss: -236996.093750\n",
      "Train Epoch: 63 [5760/17352 (33%)] Loss: -245922.265625\n",
      "Train Epoch: 63 [7168/17352 (41%)] Loss: -232429.843750\n",
      "Train Epoch: 63 [8576/17352 (49%)] Loss: -226423.890625\n",
      "Train Epoch: 63 [9984/17352 (58%)] Loss: -208873.468750\n",
      "Train Epoch: 63 [11392/17352 (66%)] Loss: -209553.125000\n",
      "Train Epoch: 63 [12800/17352 (74%)] Loss: -231916.093750\n",
      "Train Epoch: 63 [14208/17352 (82%)] Loss: -234524.625000\n",
      "Train Epoch: 63 [15478/17352 (89%)] Loss: -195006.640625\n",
      "Train Epoch: 63 [16239/17352 (94%)] Loss: -136480.828125\n",
      "Train Epoch: 63 [17123/17352 (99%)] Loss: -164147.406250\n",
      "    epoch          : 63\n",
      "    loss           : -207729.62854905098\n",
      "    val_loss       : -113473.29471028646\n",
      "Train Epoch: 64 [128/17352 (1%)] Loss: -233238.906250\n",
      "Train Epoch: 64 [1536/17352 (9%)] Loss: -224075.843750\n",
      "Train Epoch: 64 [2944/17352 (17%)] Loss: -241527.515625\n",
      "Train Epoch: 64 [4352/17352 (25%)] Loss: -232249.781250\n",
      "Train Epoch: 64 [5760/17352 (33%)] Loss: -232506.359375\n",
      "Train Epoch: 64 [7168/17352 (41%)] Loss: -232555.750000\n",
      "Train Epoch: 64 [8576/17352 (49%)] Loss: -224227.421875\n",
      "Train Epoch: 64 [9984/17352 (58%)] Loss: -208244.015625\n",
      "Train Epoch: 64 [11392/17352 (66%)] Loss: -232723.671875\n",
      "Train Epoch: 64 [12800/17352 (74%)] Loss: -232649.421875\n",
      "Train Epoch: 64 [14208/17352 (82%)] Loss: -225938.171875\n",
      "Train Epoch: 64 [15540/17352 (90%)] Loss: -130154.750000\n",
      "Train Epoch: 64 [16177/17352 (93%)] Loss: -27881.964844\n",
      "Train Epoch: 64 [16905/17352 (97%)] Loss: -24479.304688\n",
      "    epoch          : 64\n",
      "    loss           : -207902.26752569212\n",
      "    val_loss       : -113464.2016031901\n",
      "Train Epoch: 65 [128/17352 (1%)] Loss: -234325.312500\n",
      "Train Epoch: 65 [1536/17352 (9%)] Loss: -220680.281250\n",
      "Train Epoch: 65 [2944/17352 (17%)] Loss: -216457.390625\n",
      "Train Epoch: 65 [4352/17352 (25%)] Loss: -222880.562500\n",
      "Train Epoch: 65 [5760/17352 (33%)] Loss: -234353.765625\n",
      "Train Epoch: 65 [7168/17352 (41%)] Loss: -236508.890625\n",
      "Train Epoch: 65 [8576/17352 (49%)] Loss: -218180.890625\n",
      "Train Epoch: 65 [9984/17352 (58%)] Loss: -231325.171875\n",
      "Train Epoch: 65 [11392/17352 (66%)] Loss: -233643.078125\n",
      "Train Epoch: 65 [12800/17352 (74%)] Loss: -231176.000000\n",
      "Train Epoch: 65 [14208/17352 (82%)] Loss: -221123.312500\n",
      "Train Epoch: 65 [15546/17352 (90%)] Loss: -182290.765625\n",
      "Train Epoch: 65 [16313/17352 (94%)] Loss: -146460.234375\n",
      "Train Epoch: 65 [17002/17352 (98%)] Loss: -87845.015625\n",
      "    epoch          : 65\n",
      "    loss           : -207880.3365470323\n",
      "    val_loss       : -113318.2159749349\n",
      "Train Epoch: 66 [128/17352 (1%)] Loss: -206365.656250\n",
      "Train Epoch: 66 [1536/17352 (9%)] Loss: -237252.000000\n",
      "Train Epoch: 66 [2944/17352 (17%)] Loss: -207944.171875\n",
      "Train Epoch: 66 [4352/17352 (25%)] Loss: -225930.609375\n",
      "Train Epoch: 66 [5760/17352 (33%)] Loss: -232537.656250\n",
      "Train Epoch: 66 [7168/17352 (41%)] Loss: -208849.406250\n",
      "Train Epoch: 66 [8576/17352 (49%)] Loss: -217141.828125\n",
      "Train Epoch: 66 [9984/17352 (58%)] Loss: -238576.109375\n",
      "Train Epoch: 66 [11392/17352 (66%)] Loss: -231984.265625\n",
      "Train Epoch: 66 [12800/17352 (74%)] Loss: -216347.375000\n",
      "Train Epoch: 66 [14208/17352 (82%)] Loss: -222734.281250\n",
      "Train Epoch: 66 [15547/17352 (90%)] Loss: -196650.406250\n",
      "Train Epoch: 66 [16176/17352 (93%)] Loss: -65992.453125\n",
      "Train Epoch: 66 [16968/17352 (98%)] Loss: -143749.312500\n",
      "    epoch          : 66\n",
      "    loss           : -207719.43806365354\n",
      "    val_loss       : -113487.1081298828\n",
      "Train Epoch: 67 [128/17352 (1%)] Loss: -234729.062500\n",
      "Train Epoch: 67 [1536/17352 (9%)] Loss: -233042.062500\n",
      "Train Epoch: 67 [2944/17352 (17%)] Loss: -219457.343750\n",
      "Train Epoch: 67 [4352/17352 (25%)] Loss: -238946.781250\n",
      "Train Epoch: 67 [5760/17352 (33%)] Loss: -234312.546875\n",
      "Train Epoch: 67 [7168/17352 (41%)] Loss: -238208.875000\n",
      "Train Epoch: 67 [8576/17352 (49%)] Loss: -217678.437500\n",
      "Train Epoch: 67 [9984/17352 (58%)] Loss: -227007.828125\n",
      "Train Epoch: 67 [11392/17352 (66%)] Loss: -210989.468750\n",
      "Train Epoch: 67 [12800/17352 (74%)] Loss: -228151.031250\n",
      "Train Epoch: 67 [14208/17352 (82%)] Loss: -223702.000000\n",
      "Train Epoch: 67 [15520/17352 (89%)] Loss: -132291.968750\n",
      "Train Epoch: 67 [16198/17352 (93%)] Loss: -28362.957031\n",
      "Train Epoch: 67 [17062/17352 (98%)] Loss: -88844.164062\n",
      "    epoch          : 67\n",
      "    loss           : -208125.8589784763\n",
      "    val_loss       : -113461.88598632812\n",
      "Train Epoch: 68 [128/17352 (1%)] Loss: -210448.468750\n",
      "Train Epoch: 68 [1536/17352 (9%)] Loss: -241533.312500\n",
      "Train Epoch: 68 [2944/17352 (17%)] Loss: -237627.968750\n",
      "Train Epoch: 68 [4352/17352 (25%)] Loss: -232442.296875\n",
      "Train Epoch: 68 [5760/17352 (33%)] Loss: -225188.156250\n",
      "Train Epoch: 68 [7168/17352 (41%)] Loss: -220587.281250\n",
      "Train Epoch: 68 [8576/17352 (49%)] Loss: -218378.656250\n",
      "Train Epoch: 68 [9984/17352 (58%)] Loss: -219470.562500\n",
      "Train Epoch: 68 [11392/17352 (66%)] Loss: -212610.671875\n",
      "Train Epoch: 68 [12800/17352 (74%)] Loss: -233186.328125\n",
      "Train Epoch: 68 [14208/17352 (82%)] Loss: -233536.171875\n",
      "Train Epoch: 68 [15450/17352 (89%)] Loss: -141555.250000\n",
      "Train Epoch: 68 [16341/17352 (94%)] Loss: -187495.328125\n",
      "Train Epoch: 68 [16931/17352 (98%)] Loss: -167439.343750\n",
      "    epoch          : 68\n",
      "    loss           : -208136.0455445155\n",
      "    val_loss       : -113478.62861328125\n",
      "Train Epoch: 69 [128/17352 (1%)] Loss: -208277.765625\n",
      "Train Epoch: 69 [1536/17352 (9%)] Loss: -220742.687500\n",
      "Train Epoch: 69 [2944/17352 (17%)] Loss: -240924.718750\n",
      "Train Epoch: 69 [4352/17352 (25%)] Loss: -221933.796875\n",
      "Train Epoch: 69 [5760/17352 (33%)] Loss: -234394.718750\n",
      "Train Epoch: 69 [7168/17352 (41%)] Loss: -209444.968750\n",
      "Train Epoch: 69 [8576/17352 (49%)] Loss: -216596.968750\n",
      "Train Epoch: 69 [9984/17352 (58%)] Loss: -231543.234375\n",
      "Train Epoch: 69 [11392/17352 (66%)] Loss: -232041.531250\n",
      "Train Epoch: 69 [12800/17352 (74%)] Loss: -245873.750000\n",
      "Train Epoch: 69 [14208/17352 (82%)] Loss: -234194.500000\n",
      "Train Epoch: 69 [15542/17352 (90%)] Loss: -186435.156250\n",
      "Train Epoch: 69 [16318/17352 (94%)] Loss: -197405.562500\n",
      "Train Epoch: 69 [17106/17352 (99%)] Loss: -65284.960938\n",
      "    epoch          : 69\n",
      "    loss           : -208205.73576119443\n",
      "    val_loss       : -113477.4950358073\n",
      "Train Epoch: 70 [128/17352 (1%)] Loss: -209914.718750\n",
      "Train Epoch: 70 [1536/17352 (9%)] Loss: -236818.484375\n",
      "Train Epoch: 70 [2944/17352 (17%)] Loss: -260424.062500\n",
      "Train Epoch: 70 [4352/17352 (25%)] Loss: -217710.343750\n",
      "Train Epoch: 70 [5760/17352 (33%)] Loss: -219694.968750\n",
      "Train Epoch: 70 [7168/17352 (41%)] Loss: -226384.875000\n",
      "Train Epoch: 70 [8576/17352 (49%)] Loss: -220110.656250\n",
      "Train Epoch: 70 [9984/17352 (58%)] Loss: -232956.843750\n",
      "Train Epoch: 70 [11392/17352 (66%)] Loss: -219331.500000\n",
      "Train Epoch: 70 [12800/17352 (74%)] Loss: -222823.937500\n",
      "Train Epoch: 70 [14208/17352 (82%)] Loss: -247437.500000\n",
      "Train Epoch: 70 [15499/17352 (89%)] Loss: -79471.265625\n",
      "Train Epoch: 70 [16318/17352 (94%)] Loss: -131618.625000\n",
      "Train Epoch: 70 [17024/17352 (98%)] Loss: -64651.691406\n",
      "    epoch          : 70\n",
      "    loss           : -208236.85265375944\n",
      "    val_loss       : -113611.4857014974\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0821_122628/checkpoint-epoch70.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 71 [128/17352 (1%)] Loss: -234276.812500\n",
      "Train Epoch: 71 [1536/17352 (9%)] Loss: -225752.937500\n",
      "Train Epoch: 71 [2944/17352 (17%)] Loss: -237618.562500\n",
      "Train Epoch: 71 [4352/17352 (25%)] Loss: -233208.015625\n",
      "Train Epoch: 71 [5760/17352 (33%)] Loss: -224523.296875\n",
      "Train Epoch: 71 [7168/17352 (41%)] Loss: -232553.281250\n",
      "Train Epoch: 71 [8576/17352 (49%)] Loss: -216245.093750\n",
      "Train Epoch: 71 [9984/17352 (58%)] Loss: -228528.812500\n",
      "Train Epoch: 71 [11392/17352 (66%)] Loss: -233471.265625\n",
      "Train Epoch: 71 [12800/17352 (74%)] Loss: -219565.781250\n",
      "Train Epoch: 71 [14208/17352 (82%)] Loss: -245243.156250\n",
      "Train Epoch: 71 [15474/17352 (89%)] Loss: -197869.375000\n",
      "Train Epoch: 71 [16244/17352 (94%)] Loss: -148938.906250\n",
      "Train Epoch: 71 [17028/17352 (98%)] Loss: -87732.812500\n",
      "    epoch          : 71\n",
      "    loss           : -208489.19210098573\n",
      "    val_loss       : -113662.39458821615\n",
      "Train Epoch: 72 [128/17352 (1%)] Loss: -235341.515625\n",
      "Train Epoch: 72 [1536/17352 (9%)] Loss: -238966.937500\n",
      "Train Epoch: 72 [2944/17352 (17%)] Loss: -236238.468750\n",
      "Train Epoch: 72 [4352/17352 (25%)] Loss: -239982.015625\n",
      "Train Epoch: 72 [5760/17352 (33%)] Loss: -225693.156250\n",
      "Train Epoch: 72 [7168/17352 (41%)] Loss: -227696.468750\n",
      "Train Epoch: 72 [8576/17352 (49%)] Loss: -235603.937500\n",
      "Train Epoch: 72 [9984/17352 (58%)] Loss: -237800.828125\n",
      "Train Epoch: 72 [11392/17352 (66%)] Loss: -234940.046875\n",
      "Train Epoch: 72 [12800/17352 (74%)] Loss: -231399.312500\n",
      "Train Epoch: 72 [14208/17352 (82%)] Loss: -233083.656250\n",
      "Train Epoch: 72 [15471/17352 (89%)] Loss: -142798.937500\n",
      "Train Epoch: 72 [16333/17352 (94%)] Loss: -87140.312500\n",
      "Train Epoch: 72 [17093/17352 (99%)] Loss: -170282.796875\n",
      "    epoch          : 72\n",
      "    loss           : -208557.28916736576\n",
      "    val_loss       : -113587.56767578125\n",
      "Train Epoch: 73 [128/17352 (1%)] Loss: -232449.046875\n",
      "Train Epoch: 73 [1536/17352 (9%)] Loss: -234083.234375\n",
      "Train Epoch: 73 [2944/17352 (17%)] Loss: -211138.250000\n",
      "Train Epoch: 73 [4352/17352 (25%)] Loss: -233394.250000\n",
      "Train Epoch: 73 [5760/17352 (33%)] Loss: -219561.187500\n",
      "Train Epoch: 73 [7168/17352 (41%)] Loss: -234288.500000\n",
      "Train Epoch: 73 [8576/17352 (49%)] Loss: -228123.328125\n",
      "Train Epoch: 73 [9984/17352 (58%)] Loss: -236540.421875\n",
      "Train Epoch: 73 [11392/17352 (66%)] Loss: -234737.609375\n",
      "Train Epoch: 73 [12800/17352 (74%)] Loss: -219804.750000\n",
      "Train Epoch: 73 [14208/17352 (82%)] Loss: -222943.531250\n",
      "Train Epoch: 73 [15497/17352 (89%)] Loss: -168201.531250\n",
      "Train Epoch: 73 [16331/17352 (94%)] Loss: -142153.750000\n",
      "Train Epoch: 73 [16955/17352 (98%)] Loss: -79775.945312\n",
      "    epoch          : 73\n",
      "    loss           : -208624.00994586304\n",
      "    val_loss       : -113712.28169759114\n",
      "Train Epoch: 74 [128/17352 (1%)] Loss: -233465.437500\n",
      "Train Epoch: 74 [1536/17352 (9%)] Loss: -232299.968750\n",
      "Train Epoch: 74 [2944/17352 (17%)] Loss: -213933.625000\n",
      "Train Epoch: 74 [4352/17352 (25%)] Loss: -217547.406250\n",
      "Train Epoch: 74 [5760/17352 (33%)] Loss: -231055.046875\n",
      "Train Epoch: 74 [7168/17352 (41%)] Loss: -218986.875000\n",
      "Train Epoch: 74 [8576/17352 (49%)] Loss: -236343.625000\n",
      "Train Epoch: 74 [9984/17352 (58%)] Loss: -219322.734375\n",
      "Train Epoch: 74 [11392/17352 (66%)] Loss: -236440.531250\n",
      "Train Epoch: 74 [12800/17352 (74%)] Loss: -237424.375000\n",
      "Train Epoch: 74 [14208/17352 (82%)] Loss: -225885.578125\n",
      "Train Epoch: 74 [15504/17352 (89%)] Loss: -90127.570312\n",
      "Train Epoch: 74 [16206/17352 (93%)] Loss: -141950.671875\n",
      "Train Epoch: 74 [16933/17352 (98%)] Loss: -24452.269531\n",
      "    epoch          : 74\n",
      "    loss           : -208750.52151059144\n",
      "    val_loss       : -113738.69374186198\n",
      "Train Epoch: 75 [128/17352 (1%)] Loss: -212767.062500\n",
      "Train Epoch: 75 [1536/17352 (9%)] Loss: -240346.515625\n",
      "Train Epoch: 75 [2944/17352 (17%)] Loss: -221468.468750\n",
      "Train Epoch: 75 [4352/17352 (25%)] Loss: -238029.250000\n",
      "Train Epoch: 75 [5760/17352 (33%)] Loss: -227603.968750\n",
      "Train Epoch: 75 [7168/17352 (41%)] Loss: -222387.140625\n",
      "Train Epoch: 75 [8576/17352 (49%)] Loss: -225212.343750\n",
      "Train Epoch: 75 [9984/17352 (58%)] Loss: -207507.343750\n",
      "Train Epoch: 75 [11392/17352 (66%)] Loss: -236649.671875\n",
      "Train Epoch: 75 [12800/17352 (74%)] Loss: -245340.062500\n",
      "Train Epoch: 75 [14208/17352 (82%)] Loss: -226163.859375\n",
      "Train Epoch: 75 [15380/17352 (89%)] Loss: -27545.431641\n",
      "Train Epoch: 75 [16142/17352 (93%)] Loss: -129361.187500\n",
      "Train Epoch: 75 [16877/17352 (97%)] Loss: -148608.812500\n",
      "    epoch          : 75\n",
      "    loss           : -208621.35443975462\n",
      "    val_loss       : -113714.86354166667\n",
      "Train Epoch: 76 [128/17352 (1%)] Loss: -207545.875000\n",
      "Train Epoch: 76 [1536/17352 (9%)] Loss: -240572.906250\n",
      "Train Epoch: 76 [2944/17352 (17%)] Loss: -224730.343750\n",
      "Train Epoch: 76 [4352/17352 (25%)] Loss: -222731.718750\n",
      "Train Epoch: 76 [5760/17352 (33%)] Loss: -213074.375000\n",
      "Train Epoch: 76 [7168/17352 (41%)] Loss: -236950.375000\n",
      "Train Epoch: 76 [8576/17352 (49%)] Loss: -210602.531250\n",
      "Train Epoch: 76 [9984/17352 (58%)] Loss: -207658.625000\n",
      "Train Epoch: 76 [11392/17352 (66%)] Loss: -236017.906250\n",
      "Train Epoch: 76 [12800/17352 (74%)] Loss: -223805.109375\n",
      "Train Epoch: 76 [14208/17352 (82%)] Loss: -234749.234375\n",
      "Train Epoch: 76 [15514/17352 (89%)] Loss: -141059.218750\n",
      "Train Epoch: 76 [16423/17352 (95%)] Loss: -130148.039062\n",
      "Train Epoch: 76 [17105/17352 (99%)] Loss: -88032.515625\n",
      "    epoch          : 76\n",
      "    loss           : -208601.6458617345\n",
      "    val_loss       : -113707.43308919271\n",
      "Train Epoch: 77 [128/17352 (1%)] Loss: -236403.593750\n",
      "Train Epoch: 77 [1536/17352 (9%)] Loss: -238619.843750\n",
      "Train Epoch: 77 [2944/17352 (17%)] Loss: -236206.875000\n",
      "Train Epoch: 77 [4352/17352 (25%)] Loss: -225102.718750\n",
      "Train Epoch: 77 [5760/17352 (33%)] Loss: -220596.703125\n",
      "Train Epoch: 77 [7168/17352 (41%)] Loss: -237460.718750\n",
      "Train Epoch: 77 [8576/17352 (49%)] Loss: -226086.687500\n",
      "Train Epoch: 77 [9984/17352 (58%)] Loss: -238671.546875\n",
      "Train Epoch: 77 [11392/17352 (66%)] Loss: -211710.593750\n",
      "Train Epoch: 77 [12800/17352 (74%)] Loss: -230120.343750\n",
      "Train Epoch: 77 [14208/17352 (82%)] Loss: -223495.843750\n",
      "Train Epoch: 77 [15543/17352 (90%)] Loss: -170732.578125\n",
      "Train Epoch: 77 [16280/17352 (94%)] Loss: -24245.890625\n",
      "Train Epoch: 77 [17080/17352 (98%)] Loss: -170968.656250\n",
      "    epoch          : 77\n",
      "    loss           : -208864.9029303429\n",
      "    val_loss       : -113736.0654296875\n",
      "Train Epoch: 78 [128/17352 (1%)] Loss: -236481.125000\n",
      "Train Epoch: 78 [1536/17352 (9%)] Loss: -235162.390625\n",
      "Train Epoch: 78 [2944/17352 (17%)] Loss: -209559.562500\n",
      "Train Epoch: 78 [4352/17352 (25%)] Loss: -232974.218750\n",
      "Train Epoch: 78 [5760/17352 (33%)] Loss: -236279.453125\n",
      "Train Epoch: 78 [7168/17352 (41%)] Loss: -235252.000000\n",
      "Train Epoch: 78 [8576/17352 (49%)] Loss: -246654.734375\n",
      "Train Epoch: 78 [9984/17352 (58%)] Loss: -228759.734375\n",
      "Train Epoch: 78 [11392/17352 (66%)] Loss: -233510.031250\n",
      "Train Epoch: 78 [12800/17352 (74%)] Loss: -236149.250000\n",
      "Train Epoch: 78 [14208/17352 (82%)] Loss: -233706.250000\n",
      "Train Epoch: 78 [15512/17352 (89%)] Loss: -197487.921875\n",
      "Train Epoch: 78 [16267/17352 (94%)] Loss: -91763.476562\n",
      "Train Epoch: 78 [16991/17352 (98%)] Loss: -24467.726562\n",
      "    epoch          : 78\n",
      "    loss           : -209080.40841941064\n",
      "    val_loss       : -113813.27789713541\n",
      "Train Epoch: 79 [128/17352 (1%)] Loss: -235331.796875\n",
      "Train Epoch: 79 [1536/17352 (9%)] Loss: -238688.531250\n",
      "Train Epoch: 79 [2944/17352 (17%)] Loss: -221321.296875\n",
      "Train Epoch: 79 [4352/17352 (25%)] Loss: -224228.031250\n",
      "Train Epoch: 79 [5760/17352 (33%)] Loss: -235339.953125\n",
      "Train Epoch: 79 [7168/17352 (41%)] Loss: -225205.000000\n",
      "Train Epoch: 79 [8576/17352 (49%)] Loss: -212082.187500\n",
      "Train Epoch: 79 [9984/17352 (58%)] Loss: -211355.000000\n",
      "Train Epoch: 79 [11392/17352 (66%)] Loss: -212228.062500\n",
      "Train Epoch: 79 [12800/17352 (74%)] Loss: -230842.031250\n",
      "Train Epoch: 79 [14208/17352 (82%)] Loss: -217626.781250\n",
      "Train Epoch: 79 [15487/17352 (89%)] Loss: -87524.578125\n",
      "Train Epoch: 79 [16233/17352 (94%)] Loss: -157801.203125\n",
      "Train Epoch: 79 [16927/17352 (98%)] Loss: -131889.531250\n",
      "    epoch          : 79\n",
      "    loss           : -209113.84129220847\n",
      "    val_loss       : -113806.20042317708\n",
      "Train Epoch: 80 [128/17352 (1%)] Loss: -235348.265625\n",
      "Train Epoch: 80 [1536/17352 (9%)] Loss: -236875.468750\n",
      "Train Epoch: 80 [2944/17352 (17%)] Loss: -224971.125000\n",
      "Train Epoch: 80 [4352/17352 (25%)] Loss: -225456.062500\n",
      "Train Epoch: 80 [5760/17352 (33%)] Loss: -233954.296875\n",
      "Train Epoch: 80 [7168/17352 (41%)] Loss: -226223.453125\n",
      "Train Epoch: 80 [8576/17352 (49%)] Loss: -245517.250000\n",
      "Train Epoch: 80 [9984/17352 (58%)] Loss: -237523.468750\n",
      "Train Epoch: 80 [11392/17352 (66%)] Loss: -225821.109375\n",
      "Train Epoch: 80 [12800/17352 (74%)] Loss: -221465.640625\n",
      "Train Epoch: 80 [14208/17352 (82%)] Loss: -225447.953125\n",
      "Train Epoch: 80 [15368/17352 (89%)] Loss: -9386.242188\n",
      "Train Epoch: 80 [16311/17352 (94%)] Loss: -133713.218750\n",
      "Train Epoch: 80 [16957/17352 (98%)] Loss: -142541.328125\n",
      "    epoch          : 80\n",
      "    loss           : -209166.05840695783\n",
      "    val_loss       : -113808.89852701823\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0821_122628/checkpoint-epoch80.pth ...\n",
      "Train Epoch: 81 [128/17352 (1%)] Loss: -224078.234375\n",
      "Train Epoch: 81 [1536/17352 (9%)] Loss: -234441.218750\n",
      "Train Epoch: 81 [2944/17352 (17%)] Loss: -236800.750000\n",
      "Train Epoch: 81 [4352/17352 (25%)] Loss: -233517.906250\n",
      "Train Epoch: 81 [5760/17352 (33%)] Loss: -246462.843750\n",
      "Train Epoch: 81 [7168/17352 (41%)] Loss: -234137.718750\n",
      "Train Epoch: 81 [8576/17352 (49%)] Loss: -225184.937500\n",
      "Train Epoch: 81 [9984/17352 (58%)] Loss: -232596.421875\n",
      "Train Epoch: 81 [11392/17352 (66%)] Loss: -213069.625000\n",
      "Train Epoch: 81 [12800/17352 (74%)] Loss: -227777.562500\n",
      "Train Epoch: 81 [14208/17352 (82%)] Loss: -233807.687500\n",
      "Train Epoch: 81 [15529/17352 (89%)] Loss: -149926.015625\n",
      "Train Epoch: 81 [16424/17352 (95%)] Loss: -168357.859375\n",
      "Train Epoch: 81 [16993/17352 (98%)] Loss: -5622.200195\n",
      "    epoch          : 81\n",
      "    loss           : -209285.85311582425\n",
      "    val_loss       : -113823.03496907552\n",
      "Train Epoch: 82 [128/17352 (1%)] Loss: -235934.046875\n",
      "Train Epoch: 82 [1536/17352 (9%)] Loss: -224745.062500\n",
      "Train Epoch: 82 [2944/17352 (17%)] Loss: -213810.453125\n",
      "Train Epoch: 82 [4352/17352 (25%)] Loss: -232471.437500\n",
      "Train Epoch: 82 [5760/17352 (33%)] Loss: -235081.140625\n",
      "Train Epoch: 82 [7168/17352 (41%)] Loss: -239998.250000\n",
      "Train Epoch: 82 [8576/17352 (49%)] Loss: -236251.203125\n",
      "Train Epoch: 82 [9984/17352 (58%)] Loss: -240357.062500\n",
      "Train Epoch: 82 [11392/17352 (66%)] Loss: -211797.296875\n",
      "Train Epoch: 82 [12800/17352 (74%)] Loss: -238317.968750\n",
      "Train Epoch: 82 [14208/17352 (82%)] Loss: -222097.203125\n",
      "Train Epoch: 82 [15541/17352 (90%)] Loss: -155855.250000\n",
      "Train Epoch: 82 [16332/17352 (94%)] Loss: -5365.268555\n",
      "Train Epoch: 82 [17046/17352 (98%)] Loss: -91051.468750\n",
      "    epoch          : 82\n",
      "    loss           : -209316.9791251573\n",
      "    val_loss       : -113822.32346191406\n",
      "Train Epoch: 83 [128/17352 (1%)] Loss: -231790.062500\n",
      "Train Epoch: 83 [1536/17352 (9%)] Loss: -233962.031250\n",
      "Train Epoch: 83 [2944/17352 (17%)] Loss: -211245.750000\n",
      "Train Epoch: 83 [4352/17352 (25%)] Loss: -223609.750000\n",
      "Train Epoch: 83 [5760/17352 (33%)] Loss: -222687.234375\n",
      "Train Epoch: 83 [7168/17352 (41%)] Loss: -233514.000000\n",
      "Train Epoch: 83 [8576/17352 (49%)] Loss: -222450.250000\n",
      "Train Epoch: 83 [9984/17352 (58%)] Loss: -237717.453125\n",
      "Train Epoch: 83 [11392/17352 (66%)] Loss: -236354.343750\n",
      "Train Epoch: 83 [12800/17352 (74%)] Loss: -232577.203125\n",
      "Train Epoch: 83 [14208/17352 (82%)] Loss: -235398.468750\n",
      "Train Epoch: 83 [15453/17352 (89%)] Loss: -137439.468750\n",
      "Train Epoch: 83 [16295/17352 (94%)] Loss: -170943.250000\n",
      "Train Epoch: 83 [17045/17352 (98%)] Loss: -5528.724609\n",
      "    epoch          : 83\n",
      "    loss           : -209388.35886377937\n",
      "    val_loss       : -113802.28826497396\n",
      "Train Epoch: 84 [128/17352 (1%)] Loss: -236991.046875\n",
      "Train Epoch: 84 [1536/17352 (9%)] Loss: -239368.781250\n",
      "Train Epoch: 84 [2944/17352 (17%)] Loss: -259000.296875\n",
      "Train Epoch: 84 [4352/17352 (25%)] Loss: -225751.484375\n",
      "Train Epoch: 84 [5760/17352 (33%)] Loss: -231152.781250\n",
      "Train Epoch: 84 [7168/17352 (41%)] Loss: -223288.375000\n",
      "Train Epoch: 84 [8576/17352 (49%)] Loss: -221408.203125\n",
      "Train Epoch: 84 [9984/17352 (58%)] Loss: -238846.171875\n",
      "Train Epoch: 84 [11392/17352 (66%)] Loss: -233792.734375\n",
      "Train Epoch: 84 [12800/17352 (74%)] Loss: -218487.421875\n",
      "Train Epoch: 84 [14208/17352 (82%)] Loss: -245606.312500\n",
      "Train Epoch: 84 [15522/17352 (89%)] Loss: -171035.531250\n",
      "Train Epoch: 84 [16203/17352 (93%)] Loss: -64777.644531\n",
      "Train Epoch: 84 [17081/17352 (98%)] Loss: -157736.406250\n",
      "    epoch          : 84\n",
      "    loss           : -209407.158203125\n",
      "    val_loss       : -113825.64946289062\n",
      "Train Epoch: 85 [128/17352 (1%)] Loss: -236653.125000\n",
      "Train Epoch: 85 [1536/17352 (9%)] Loss: -224399.593750\n",
      "Train Epoch: 85 [2944/17352 (17%)] Loss: -241430.265625\n",
      "Train Epoch: 85 [4352/17352 (25%)] Loss: -224810.156250\n",
      "Train Epoch: 85 [5760/17352 (33%)] Loss: -234852.281250\n",
      "Train Epoch: 85 [7168/17352 (41%)] Loss: -220583.078125\n",
      "Train Epoch: 85 [8576/17352 (49%)] Loss: -219170.859375\n",
      "Train Epoch: 85 [9984/17352 (58%)] Loss: -221221.218750\n",
      "Train Epoch: 85 [11392/17352 (66%)] Loss: -236289.562500\n",
      "Train Epoch: 85 [12800/17352 (74%)] Loss: -220390.812500\n",
      "Train Epoch: 85 [14208/17352 (82%)] Loss: -235510.937500\n",
      "Train Epoch: 85 [15550/17352 (90%)] Loss: -171623.187500\n",
      "Train Epoch: 85 [16347/17352 (94%)] Loss: -157338.906250\n",
      "Train Epoch: 85 [17026/17352 (98%)] Loss: -169828.187500\n",
      "    epoch          : 85\n",
      "    loss           : -209508.910749397\n",
      "    val_loss       : -113890.56899414063\n",
      "Train Epoch: 86 [128/17352 (1%)] Loss: -222170.843750\n",
      "Train Epoch: 86 [1536/17352 (9%)] Loss: -240477.593750\n",
      "Train Epoch: 86 [2944/17352 (17%)] Loss: -241512.906250\n",
      "Train Epoch: 86 [4352/17352 (25%)] Loss: -234294.140625\n",
      "Train Epoch: 86 [5760/17352 (33%)] Loss: -236626.531250\n",
      "Train Epoch: 86 [7168/17352 (41%)] Loss: -238092.031250\n",
      "Train Epoch: 86 [8576/17352 (49%)] Loss: -222092.500000\n",
      "Train Epoch: 86 [9984/17352 (58%)] Loss: -232443.937500\n",
      "Train Epoch: 86 [11392/17352 (66%)] Loss: -235972.031250\n",
      "Train Epoch: 86 [12800/17352 (74%)] Loss: -234052.500000\n",
      "Train Epoch: 86 [14208/17352 (82%)] Loss: -229246.328125\n",
      "Train Epoch: 86 [15533/17352 (90%)] Loss: -151969.265625\n",
      "Train Epoch: 86 [16375/17352 (94%)] Loss: -168130.875000\n",
      "Train Epoch: 86 [17021/17352 (98%)] Loss: -188457.078125\n",
      "    epoch          : 86\n",
      "    loss           : -209600.93792274015\n",
      "    val_loss       : -113881.70909830728\n",
      "Train Epoch: 87 [128/17352 (1%)] Loss: -212850.062500\n",
      "Train Epoch: 87 [1536/17352 (9%)] Loss: -227272.781250\n",
      "Train Epoch: 87 [2944/17352 (17%)] Loss: -225743.484375\n",
      "Train Epoch: 87 [4352/17352 (25%)] Loss: -232638.937500\n",
      "Train Epoch: 87 [5760/17352 (33%)] Loss: -242494.593750\n",
      "Train Epoch: 87 [7168/17352 (41%)] Loss: -220124.562500\n",
      "Train Epoch: 87 [8576/17352 (49%)] Loss: -213565.968750\n",
      "Train Epoch: 87 [9984/17352 (58%)] Loss: -219890.968750\n",
      "Train Epoch: 87 [11392/17352 (66%)] Loss: -234783.687500\n",
      "Train Epoch: 87 [12800/17352 (74%)] Loss: -232740.890625\n",
      "Train Epoch: 87 [14208/17352 (82%)] Loss: -237252.250000\n",
      "Train Epoch: 87 [15584/17352 (90%)] Loss: -199322.437500\n",
      "Train Epoch: 87 [16230/17352 (94%)] Loss: -27966.996094\n",
      "Train Epoch: 87 [17042/17352 (98%)] Loss: -236341.703125\n",
      "    epoch          : 87\n",
      "    loss           : -209586.92428691275\n",
      "    val_loss       : -113908.62911783854\n",
      "Train Epoch: 88 [128/17352 (1%)] Loss: -235322.015625\n",
      "Train Epoch: 88 [1536/17352 (9%)] Loss: -234364.187500\n",
      "Train Epoch: 88 [2944/17352 (17%)] Loss: -211260.390625\n",
      "Train Epoch: 88 [4352/17352 (25%)] Loss: -241379.750000\n",
      "Train Epoch: 88 [5760/17352 (33%)] Loss: -225469.843750\n",
      "Train Epoch: 88 [7168/17352 (41%)] Loss: -223509.687500\n",
      "Train Epoch: 88 [8576/17352 (49%)] Loss: -223182.921875\n",
      "Train Epoch: 88 [9984/17352 (58%)] Loss: -210050.718750\n",
      "Train Epoch: 88 [11392/17352 (66%)] Loss: -221956.812500\n",
      "Train Epoch: 88 [12800/17352 (74%)] Loss: -232483.062500\n",
      "Train Epoch: 88 [14208/17352 (82%)] Loss: -217697.281250\n",
      "Train Epoch: 88 [15526/17352 (89%)] Loss: -138960.437500\n",
      "Train Epoch: 88 [16228/17352 (94%)] Loss: -65017.226562\n",
      "Train Epoch: 88 [17007/17352 (98%)] Loss: -169880.281250\n",
      "    epoch          : 88\n",
      "    loss           : -209768.05185612416\n",
      "    val_loss       : -113955.69975585937\n",
      "Train Epoch: 89 [128/17352 (1%)] Loss: -234997.031250\n",
      "Train Epoch: 89 [1536/17352 (9%)] Loss: -227303.375000\n",
      "Train Epoch: 89 [2944/17352 (17%)] Loss: -226542.515625\n",
      "Train Epoch: 89 [4352/17352 (25%)] Loss: -232249.750000\n",
      "Train Epoch: 89 [5760/17352 (33%)] Loss: -225758.921875\n",
      "Train Epoch: 89 [7168/17352 (41%)] Loss: -237058.812500\n",
      "Train Epoch: 89 [8576/17352 (49%)] Loss: -236685.734375\n",
      "Train Epoch: 89 [9984/17352 (58%)] Loss: -209035.609375\n",
      "Train Epoch: 89 [11392/17352 (66%)] Loss: -233904.296875\n",
      "Train Epoch: 89 [12800/17352 (74%)] Loss: -222015.875000\n",
      "Train Epoch: 89 [14208/17352 (82%)] Loss: -223521.843750\n",
      "Train Epoch: 89 [15450/17352 (89%)] Loss: -24265.679688\n",
      "Train Epoch: 89 [16297/17352 (94%)] Loss: -144682.984375\n",
      "Train Epoch: 89 [17159/17352 (99%)] Loss: -89686.796875\n",
      "    epoch          : 89\n",
      "    loss           : -209754.92473586934\n",
      "    val_loss       : -113883.21495768228\n",
      "Train Epoch: 90 [128/17352 (1%)] Loss: -227545.468750\n",
      "Train Epoch: 90 [1536/17352 (9%)] Loss: -234474.093750\n",
      "Train Epoch: 90 [2944/17352 (17%)] Loss: -212714.828125\n",
      "Train Epoch: 90 [4352/17352 (25%)] Loss: -240014.781250\n",
      "Train Epoch: 90 [5760/17352 (33%)] Loss: -230868.218750\n",
      "Train Epoch: 90 [7168/17352 (41%)] Loss: -212201.468750\n",
      "Train Epoch: 90 [8576/17352 (49%)] Loss: -219884.375000\n",
      "Train Epoch: 90 [9984/17352 (58%)] Loss: -242374.828125\n",
      "Train Epoch: 90 [11392/17352 (66%)] Loss: -213166.921875\n",
      "Train Epoch: 90 [12800/17352 (74%)] Loss: -225691.125000\n",
      "Train Epoch: 90 [14208/17352 (82%)] Loss: -220464.796875\n",
      "Train Epoch: 90 [15499/17352 (89%)] Loss: -189134.203125\n",
      "Train Epoch: 90 [16308/17352 (94%)] Loss: -148979.281250\n",
      "Train Epoch: 90 [16894/17352 (97%)] Loss: -142966.593750\n",
      "    epoch          : 90\n",
      "    loss           : -209813.7664770344\n",
      "    val_loss       : -113903.25616048177\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0821_122628/checkpoint-epoch90.pth ...\n",
      "Train Epoch: 91 [128/17352 (1%)] Loss: -212866.796875\n",
      "Train Epoch: 91 [1536/17352 (9%)] Loss: -230909.937500\n",
      "Train Epoch: 91 [2944/17352 (17%)] Loss: -220039.125000\n",
      "Train Epoch: 91 [4352/17352 (25%)] Loss: -238361.421875\n",
      "Train Epoch: 91 [5760/17352 (33%)] Loss: -248029.171875\n",
      "Train Epoch: 91 [7168/17352 (41%)] Loss: -234504.015625\n",
      "Train Epoch: 91 [8576/17352 (49%)] Loss: -246839.781250\n",
      "Train Epoch: 91 [9984/17352 (58%)] Loss: -242907.281250\n",
      "Train Epoch: 91 [11392/17352 (66%)] Loss: -235964.406250\n",
      "Train Epoch: 91 [12800/17352 (74%)] Loss: -246341.593750\n",
      "Train Epoch: 91 [14208/17352 (82%)] Loss: -228731.968750\n",
      "Train Epoch: 91 [15509/17352 (89%)] Loss: -91149.421875\n",
      "Train Epoch: 91 [16263/17352 (94%)] Loss: -24098.619141\n",
      "Train Epoch: 91 [17031/17352 (98%)] Loss: -65020.703125\n",
      "    epoch          : 91\n",
      "    loss           : -209907.38471646918\n",
      "    val_loss       : -113943.18944498697\n",
      "Train Epoch: 92 [128/17352 (1%)] Loss: -238289.531250\n",
      "Train Epoch: 92 [1536/17352 (9%)] Loss: -232280.203125\n",
      "Train Epoch: 92 [2944/17352 (17%)] Loss: -212067.031250\n",
      "Train Epoch: 92 [4352/17352 (25%)] Loss: -227985.796875\n",
      "Train Epoch: 92 [5760/17352 (33%)] Loss: -237466.562500\n",
      "Train Epoch: 92 [7168/17352 (41%)] Loss: -233605.562500\n",
      "Train Epoch: 92 [8576/17352 (49%)] Loss: -221163.906250\n",
      "Train Epoch: 92 [9984/17352 (58%)] Loss: -236020.718750\n",
      "Train Epoch: 92 [11392/17352 (66%)] Loss: -213913.171875\n",
      "Train Epoch: 92 [12800/17352 (74%)] Loss: -224791.390625\n",
      "Train Epoch: 92 [14208/17352 (82%)] Loss: -223812.718750\n",
      "Train Epoch: 92 [15447/17352 (89%)] Loss: -5489.597656\n",
      "Train Epoch: 92 [15980/17352 (92%)] Loss: -65412.464844\n",
      "Train Epoch: 92 [16898/17352 (97%)] Loss: -140943.515625\n",
      "    epoch          : 92\n",
      "    loss           : -210047.25319840605\n",
      "    val_loss       : -113965.80512695313\n",
      "Train Epoch: 93 [128/17352 (1%)] Loss: -235808.984375\n",
      "Train Epoch: 93 [1536/17352 (9%)] Loss: -234933.031250\n",
      "Train Epoch: 93 [2944/17352 (17%)] Loss: -259864.125000\n",
      "Train Epoch: 93 [4352/17352 (25%)] Loss: -220122.093750\n",
      "Train Epoch: 93 [5760/17352 (33%)] Loss: -246585.890625\n",
      "Train Epoch: 93 [7168/17352 (41%)] Loss: -236576.343750\n",
      "Train Epoch: 93 [8576/17352 (49%)] Loss: -217676.828125\n",
      "Train Epoch: 93 [9984/17352 (58%)] Loss: -232715.750000\n",
      "Train Epoch: 93 [11392/17352 (66%)] Loss: -233126.562500\n",
      "Train Epoch: 93 [12800/17352 (74%)] Loss: -220259.562500\n",
      "Train Epoch: 93 [14208/17352 (82%)] Loss: -220419.937500\n",
      "Train Epoch: 93 [15528/17352 (89%)] Loss: -151301.468750\n",
      "Train Epoch: 93 [16179/17352 (93%)] Loss: -141429.171875\n",
      "Train Epoch: 93 [16955/17352 (98%)] Loss: -158624.796875\n",
      "    epoch          : 93\n",
      "    loss           : -210010.72630361264\n",
      "    val_loss       : -113967.79041341147\n",
      "Train Epoch: 94 [128/17352 (1%)] Loss: -237075.593750\n",
      "Train Epoch: 94 [1536/17352 (9%)] Loss: -234291.734375\n",
      "Train Epoch: 94 [2944/17352 (17%)] Loss: -228790.750000\n",
      "Train Epoch: 94 [4352/17352 (25%)] Loss: -230510.343750\n",
      "Train Epoch: 94 [5760/17352 (33%)] Loss: -229673.062500\n",
      "Train Epoch: 94 [7168/17352 (41%)] Loss: -233141.781250\n",
      "Train Epoch: 94 [8576/17352 (49%)] Loss: -236906.187500\n",
      "Train Epoch: 94 [9984/17352 (58%)] Loss: -233917.875000\n",
      "Train Epoch: 94 [11392/17352 (66%)] Loss: -233949.812500\n",
      "Train Epoch: 94 [12800/17352 (74%)] Loss: -223687.296875\n",
      "Train Epoch: 94 [14208/17352 (82%)] Loss: -228573.125000\n",
      "Train Epoch: 94 [15557/17352 (90%)] Loss: -169192.031250\n",
      "Train Epoch: 94 [16268/17352 (94%)] Loss: -90218.843750\n",
      "Train Epoch: 94 [16987/17352 (98%)] Loss: -24674.460938\n",
      "    epoch          : 94\n",
      "    loss           : -210117.4217504719\n",
      "    val_loss       : -113982.94987792968\n",
      "Train Epoch: 95 [128/17352 (1%)] Loss: -237268.125000\n",
      "Train Epoch: 95 [1536/17352 (9%)] Loss: -227581.578125\n",
      "Train Epoch: 95 [2944/17352 (17%)] Loss: -229821.468750\n",
      "Train Epoch: 95 [4352/17352 (25%)] Loss: -239577.687500\n",
      "Train Epoch: 95 [5760/17352 (33%)] Loss: -224454.781250\n",
      "Train Epoch: 95 [7168/17352 (41%)] Loss: -235558.218750\n",
      "Train Epoch: 95 [8576/17352 (49%)] Loss: -238037.500000\n",
      "Train Epoch: 95 [9984/17352 (58%)] Loss: -221370.640625\n",
      "Train Epoch: 95 [11392/17352 (66%)] Loss: -233294.281250\n",
      "Train Epoch: 95 [12800/17352 (74%)] Loss: -238104.687500\n",
      "Train Epoch: 95 [14208/17352 (82%)] Loss: -223637.750000\n",
      "Train Epoch: 95 [15490/17352 (89%)] Loss: -27933.818359\n",
      "Train Epoch: 95 [16389/17352 (94%)] Loss: -155708.500000\n",
      "Train Epoch: 95 [17003/17352 (98%)] Loss: -171115.453125\n",
      "    epoch          : 95\n",
      "    loss           : -210189.52756003564\n",
      "    val_loss       : -113880.577734375\n",
      "Train Epoch: 96 [128/17352 (1%)] Loss: -237600.625000\n",
      "Train Epoch: 96 [1536/17352 (9%)] Loss: -221772.562500\n",
      "Train Epoch: 96 [2944/17352 (17%)] Loss: -219046.390625\n",
      "Train Epoch: 96 [4352/17352 (25%)] Loss: -239764.437500\n",
      "Train Epoch: 96 [5760/17352 (33%)] Loss: -235821.375000\n",
      "Train Epoch: 96 [7168/17352 (41%)] Loss: -238688.734375\n",
      "Train Epoch: 96 [8576/17352 (49%)] Loss: -247603.312500\n",
      "Train Epoch: 96 [9984/17352 (58%)] Loss: -215548.937500\n",
      "Train Epoch: 96 [11392/17352 (66%)] Loss: -223101.406250\n",
      "Train Epoch: 96 [12800/17352 (74%)] Loss: -222185.453125\n",
      "Train Epoch: 96 [14208/17352 (82%)] Loss: -222450.968750\n",
      "Train Epoch: 96 [15574/17352 (90%)] Loss: -188630.390625\n",
      "Train Epoch: 96 [16240/17352 (94%)] Loss: -9641.160156\n",
      "Train Epoch: 96 [17066/17352 (98%)] Loss: -67686.906250\n",
      "    epoch          : 96\n",
      "    loss           : -210125.45626114198\n",
      "    val_loss       : -113581.41893717447\n",
      "Train Epoch: 97 [128/17352 (1%)] Loss: -235319.359375\n",
      "Train Epoch: 97 [1536/17352 (9%)] Loss: -216159.343750\n",
      "Train Epoch: 97 [2944/17352 (17%)] Loss: -230896.468750\n",
      "Train Epoch: 97 [4352/17352 (25%)] Loss: -234754.281250\n",
      "Train Epoch: 97 [5760/17352 (33%)] Loss: -229190.531250\n",
      "Train Epoch: 97 [7168/17352 (41%)] Loss: -222833.812500\n",
      "Train Epoch: 97 [8576/17352 (49%)] Loss: -221753.859375\n",
      "Train Epoch: 97 [9984/17352 (58%)] Loss: -236827.328125\n",
      "Train Epoch: 97 [11392/17352 (66%)] Loss: -221585.906250\n",
      "Train Epoch: 97 [12800/17352 (74%)] Loss: -219703.718750\n",
      "Train Epoch: 97 [14208/17352 (82%)] Loss: -247914.218750\n",
      "Train Epoch: 97 [15570/17352 (90%)] Loss: -199175.453125\n",
      "Train Epoch: 97 [16323/17352 (94%)] Loss: -189227.562500\n",
      "Train Epoch: 97 [17101/17352 (99%)] Loss: -142133.531250\n",
      "    epoch          : 97\n",
      "    loss           : -210176.1946079331\n",
      "    val_loss       : -113901.0523030599\n",
      "Train Epoch: 98 [128/17352 (1%)] Loss: -237112.343750\n",
      "Train Epoch: 98 [1536/17352 (9%)] Loss: -225225.218750\n",
      "Train Epoch: 98 [2944/17352 (17%)] Loss: -229577.375000\n",
      "Train Epoch: 98 [4352/17352 (25%)] Loss: -240321.671875\n",
      "Train Epoch: 98 [5760/17352 (33%)] Loss: -227813.156250\n",
      "Train Epoch: 98 [7168/17352 (41%)] Loss: -212442.968750\n",
      "Train Epoch: 98 [8576/17352 (49%)] Loss: -220474.453125\n",
      "Train Epoch: 98 [9984/17352 (58%)] Loss: -234359.546875\n",
      "Train Epoch: 98 [11392/17352 (66%)] Loss: -235580.500000\n",
      "Train Epoch: 98 [12800/17352 (74%)] Loss: -235108.312500\n",
      "Train Epoch: 98 [14208/17352 (82%)] Loss: -237394.703125\n",
      "Train Epoch: 98 [15512/17352 (89%)] Loss: -142359.156250\n",
      "Train Epoch: 98 [16293/17352 (94%)] Loss: -169093.343750\n",
      "Train Epoch: 98 [17129/17352 (99%)] Loss: -237246.828125\n",
      "    epoch          : 98\n",
      "    loss           : -210314.2838061032\n",
      "    val_loss       : -113879.32277018229\n",
      "Train Epoch: 99 [128/17352 (1%)] Loss: -237404.734375\n",
      "Train Epoch: 99 [1536/17352 (9%)] Loss: -227152.156250\n",
      "Train Epoch: 99 [2944/17352 (17%)] Loss: -220611.734375\n",
      "Train Epoch: 99 [4352/17352 (25%)] Loss: -224824.718750\n",
      "Train Epoch: 99 [5760/17352 (33%)] Loss: -224153.531250\n",
      "Train Epoch: 99 [7168/17352 (41%)] Loss: -220362.328125\n",
      "Train Epoch: 99 [8576/17352 (49%)] Loss: -228621.375000\n",
      "Train Epoch: 99 [9984/17352 (58%)] Loss: -240317.328125\n",
      "Train Epoch: 99 [11392/17352 (66%)] Loss: -215830.812500\n",
      "Train Epoch: 99 [12800/17352 (74%)] Loss: -226592.937500\n",
      "Train Epoch: 99 [14208/17352 (82%)] Loss: -236623.296875\n",
      "Train Epoch: 99 [15538/17352 (90%)] Loss: -143450.328125\n",
      "Train Epoch: 99 [16295/17352 (94%)] Loss: -24689.632812\n",
      "Train Epoch: 99 [17071/17352 (98%)] Loss: -5753.228027\n",
      "    epoch          : 99\n",
      "    loss           : -210199.03270173553\n",
      "    val_loss       : -113889.3723388672\n",
      "Train Epoch: 100 [128/17352 (1%)] Loss: -229381.875000\n",
      "Train Epoch: 100 [1536/17352 (9%)] Loss: -226247.343750\n",
      "Train Epoch: 100 [2944/17352 (17%)] Loss: -215007.609375\n",
      "Train Epoch: 100 [4352/17352 (25%)] Loss: -233541.562500\n",
      "Train Epoch: 100 [5760/17352 (33%)] Loss: -240638.187500\n",
      "Train Epoch: 100 [7168/17352 (41%)] Loss: -259704.968750\n",
      "Train Epoch: 100 [8576/17352 (49%)] Loss: -239145.875000\n",
      "Train Epoch: 100 [9984/17352 (58%)] Loss: -225640.859375\n",
      "Train Epoch: 100 [11392/17352 (66%)] Loss: -229948.343750\n",
      "Train Epoch: 100 [12800/17352 (74%)] Loss: -225178.281250\n",
      "Train Epoch: 100 [14208/17352 (82%)] Loss: -247602.359375\n",
      "Train Epoch: 100 [15487/17352 (89%)] Loss: -158324.156250\n",
      "Train Epoch: 100 [16280/17352 (94%)] Loss: -143417.421875\n",
      "Train Epoch: 100 [17055/17352 (98%)] Loss: -184410.718750\n",
      "    epoch          : 100\n",
      "    loss           : -210137.42701342283\n",
      "    val_loss       : -113923.69921061197\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0821_122628/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [128/17352 (1%)] Loss: -237378.437500\n",
      "Train Epoch: 101 [1536/17352 (9%)] Loss: -234934.281250\n",
      "Train Epoch: 101 [2944/17352 (17%)] Loss: -238697.578125\n",
      "Train Epoch: 101 [4352/17352 (25%)] Loss: -220438.625000\n",
      "Train Epoch: 101 [5760/17352 (33%)] Loss: -214776.750000\n",
      "Train Epoch: 101 [7168/17352 (41%)] Loss: -234882.796875\n",
      "Train Epoch: 101 [8576/17352 (49%)] Loss: -225667.375000\n",
      "Train Epoch: 101 [9984/17352 (58%)] Loss: -231221.093750\n",
      "Train Epoch: 101 [11392/17352 (66%)] Loss: -212988.140625\n",
      "Train Epoch: 101 [12800/17352 (74%)] Loss: -231145.156250\n",
      "Train Epoch: 101 [14208/17352 (82%)] Loss: -219398.734375\n",
      "Train Epoch: 101 [15446/17352 (89%)] Loss: -134422.703125\n",
      "Train Epoch: 101 [16184/17352 (93%)] Loss: -92240.335938\n",
      "Train Epoch: 101 [16943/17352 (98%)] Loss: -156794.375000\n",
      "    epoch          : 101\n",
      "    loss           : -210441.9878912804\n",
      "    val_loss       : -113986.26446940104\n",
      "Train Epoch: 102 [128/17352 (1%)] Loss: -236213.281250\n",
      "Train Epoch: 102 [1536/17352 (9%)] Loss: -239547.421875\n",
      "Train Epoch: 102 [2944/17352 (17%)] Loss: -220337.437500\n",
      "Train Epoch: 102 [4352/17352 (25%)] Loss: -225001.515625\n",
      "Train Epoch: 102 [5760/17352 (33%)] Loss: -230289.906250\n",
      "Train Epoch: 102 [7168/17352 (41%)] Loss: -239219.515625\n",
      "Train Epoch: 102 [8576/17352 (49%)] Loss: -229283.593750\n",
      "Train Epoch: 102 [9984/17352 (58%)] Loss: -232779.187500\n",
      "Train Epoch: 102 [11392/17352 (66%)] Loss: -229457.593750\n",
      "Train Epoch: 102 [12800/17352 (74%)] Loss: -225431.125000\n",
      "Train Epoch: 102 [14208/17352 (82%)] Loss: -222418.859375\n",
      "Train Epoch: 102 [15451/17352 (89%)] Loss: -88805.500000\n",
      "Train Epoch: 102 [16112/17352 (93%)] Loss: -79054.742188\n",
      "Train Epoch: 102 [16946/17352 (98%)] Loss: -149831.968750\n",
      "    epoch          : 102\n",
      "    loss           : -210466.3053560193\n",
      "    val_loss       : -113906.98004557291\n",
      "Train Epoch: 103 [128/17352 (1%)] Loss: -236747.656250\n",
      "Train Epoch: 103 [1536/17352 (9%)] Loss: -227104.250000\n",
      "Train Epoch: 103 [2944/17352 (17%)] Loss: -244730.234375\n",
      "Train Epoch: 103 [4352/17352 (25%)] Loss: -224825.125000\n",
      "Train Epoch: 103 [5760/17352 (33%)] Loss: -248190.500000\n",
      "Train Epoch: 103 [7168/17352 (41%)] Loss: -242037.937500\n",
      "Train Epoch: 103 [8576/17352 (49%)] Loss: -214230.453125\n",
      "Train Epoch: 103 [9984/17352 (58%)] Loss: -213987.156250\n",
      "Train Epoch: 103 [11392/17352 (66%)] Loss: -229018.656250\n",
      "Train Epoch: 103 [12800/17352 (74%)] Loss: -234070.437500\n",
      "Train Epoch: 103 [14208/17352 (82%)] Loss: -237767.968750\n",
      "Train Epoch: 103 [15487/17352 (89%)] Loss: -88689.617188\n",
      "Train Epoch: 103 [16111/17352 (93%)] Loss: -92002.929688\n",
      "Train Epoch: 103 [16883/17352 (97%)] Loss: -79728.351562\n",
      "    epoch          : 103\n",
      "    loss           : -210483.38905856753\n",
      "    val_loss       : -114002.1594563802\n",
      "Train Epoch: 104 [128/17352 (1%)] Loss: -239168.250000\n",
      "Train Epoch: 104 [1536/17352 (9%)] Loss: -227074.531250\n",
      "Train Epoch: 104 [2944/17352 (17%)] Loss: -259165.531250\n",
      "Train Epoch: 104 [4352/17352 (25%)] Loss: -240225.765625\n",
      "Train Epoch: 104 [5760/17352 (33%)] Loss: -228255.234375\n",
      "Train Epoch: 104 [7168/17352 (41%)] Loss: -225908.843750\n",
      "Train Epoch: 104 [8576/17352 (49%)] Loss: -222588.890625\n",
      "Train Epoch: 104 [9984/17352 (58%)] Loss: -221377.906250\n",
      "Train Epoch: 104 [11392/17352 (66%)] Loss: -237382.093750\n",
      "Train Epoch: 104 [12800/17352 (74%)] Loss: -233078.937500\n",
      "Train Epoch: 104 [14208/17352 (82%)] Loss: -228835.062500\n",
      "Train Epoch: 104 [15534/17352 (90%)] Loss: -151293.937500\n",
      "Train Epoch: 104 [16155/17352 (93%)] Loss: -24598.105469\n",
      "Train Epoch: 104 [16972/17352 (98%)] Loss: -184491.218750\n",
      "    epoch          : 104\n",
      "    loss           : -210589.2956690436\n",
      "    val_loss       : -113902.325390625\n",
      "Train Epoch: 105 [128/17352 (1%)] Loss: -216155.406250\n",
      "Train Epoch: 105 [1536/17352 (9%)] Loss: -234354.828125\n",
      "Train Epoch: 105 [2944/17352 (17%)] Loss: -215967.937500\n",
      "Train Epoch: 105 [4352/17352 (25%)] Loss: -242295.500000\n",
      "Train Epoch: 105 [5760/17352 (33%)] Loss: -237976.750000\n",
      "Train Epoch: 105 [7168/17352 (41%)] Loss: -225392.968750\n",
      "Train Epoch: 105 [8576/17352 (49%)] Loss: -220393.687500\n",
      "Train Epoch: 105 [9984/17352 (58%)] Loss: -234949.859375\n",
      "Train Epoch: 105 [11392/17352 (66%)] Loss: -226601.640625\n",
      "Train Epoch: 105 [12800/17352 (74%)] Loss: -237641.765625\n",
      "Train Epoch: 105 [14208/17352 (82%)] Loss: -234822.421875\n",
      "Train Epoch: 105 [15413/17352 (89%)] Loss: -5759.597656\n",
      "Train Epoch: 105 [16121/17352 (93%)] Loss: -143105.140625\n",
      "Train Epoch: 105 [16938/17352 (98%)] Loss: -80512.398438\n",
      "    epoch          : 105\n",
      "    loss           : -210600.49355075503\n",
      "    val_loss       : -113994.89454752604\n",
      "Train Epoch: 106 [128/17352 (1%)] Loss: -236655.765625\n",
      "Train Epoch: 106 [1536/17352 (9%)] Loss: -227317.859375\n",
      "Train Epoch: 106 [2944/17352 (17%)] Loss: -246544.078125\n",
      "Train Epoch: 106 [4352/17352 (25%)] Loss: -234217.156250\n",
      "Train Epoch: 106 [5760/17352 (33%)] Loss: -222811.093750\n",
      "Train Epoch: 106 [7168/17352 (41%)] Loss: -235966.781250\n",
      "Train Epoch: 106 [8576/17352 (49%)] Loss: -231230.828125\n",
      "Train Epoch: 106 [9984/17352 (58%)] Loss: -214119.046875\n",
      "Train Epoch: 106 [11392/17352 (66%)] Loss: -212762.875000\n",
      "Train Epoch: 106 [12800/17352 (74%)] Loss: -231554.093750\n",
      "Train Epoch: 106 [14208/17352 (82%)] Loss: -235777.515625\n",
      "Train Epoch: 106 [15534/17352 (90%)] Loss: -155554.437500\n",
      "Train Epoch: 106 [16420/17352 (95%)] Loss: -138525.875000\n",
      "Train Epoch: 106 [16944/17352 (98%)] Loss: -27986.001953\n",
      "    epoch          : 106\n",
      "    loss           : -210699.14475408976\n",
      "    val_loss       : -113885.83341471355\n",
      "Train Epoch: 107 [128/17352 (1%)] Loss: -238894.796875\n",
      "Train Epoch: 107 [1536/17352 (9%)] Loss: -240521.312500\n",
      "Train Epoch: 107 [2944/17352 (17%)] Loss: -224736.984375\n",
      "Train Epoch: 107 [4352/17352 (25%)] Loss: -236023.937500\n",
      "Train Epoch: 107 [5760/17352 (33%)] Loss: -230196.109375\n",
      "Train Epoch: 107 [7168/17352 (41%)] Loss: -226927.750000\n",
      "Train Epoch: 107 [8576/17352 (49%)] Loss: -230055.500000\n",
      "Train Epoch: 107 [9984/17352 (58%)] Loss: -234999.312500\n",
      "Train Epoch: 107 [11392/17352 (66%)] Loss: -216739.343750\n",
      "Train Epoch: 107 [12800/17352 (74%)] Loss: -234877.031250\n",
      "Train Epoch: 107 [14208/17352 (82%)] Loss: -222342.984375\n",
      "Train Epoch: 107 [15451/17352 (89%)] Loss: -153065.625000\n",
      "Train Epoch: 107 [16287/17352 (94%)] Loss: -143746.375000\n",
      "Train Epoch: 107 [17087/17352 (98%)] Loss: -185190.375000\n",
      "    epoch          : 107\n",
      "    loss           : -210727.06547228922\n",
      "    val_loss       : -113830.23330891927\n",
      "Train Epoch: 108 [128/17352 (1%)] Loss: -234841.781250\n",
      "Train Epoch: 108 [1536/17352 (9%)] Loss: -239148.968750\n",
      "Train Epoch: 108 [2944/17352 (17%)] Loss: -228905.015625\n",
      "Train Epoch: 108 [4352/17352 (25%)] Loss: -240139.703125\n",
      "Train Epoch: 108 [5760/17352 (33%)] Loss: -224318.062500\n",
      "Train Epoch: 108 [7168/17352 (41%)] Loss: -230765.968750\n",
      "Train Epoch: 108 [8576/17352 (49%)] Loss: -233358.421875\n",
      "Train Epoch: 108 [9984/17352 (58%)] Loss: -235634.562500\n",
      "Train Epoch: 108 [11392/17352 (66%)] Loss: -232433.968750\n",
      "Train Epoch: 108 [12800/17352 (74%)] Loss: -221238.921875\n",
      "Train Epoch: 108 [14208/17352 (82%)] Loss: -222870.750000\n",
      "Train Epoch: 108 [15491/17352 (89%)] Loss: -142068.343750\n",
      "Train Epoch: 108 [16152/17352 (93%)] Loss: -185232.171875\n",
      "Train Epoch: 108 [17010/17352 (98%)] Loss: -171957.156250\n",
      "    epoch          : 108\n",
      "    loss           : -210767.9565986787\n",
      "    val_loss       : -113970.75017903646\n",
      "Train Epoch: 109 [128/17352 (1%)] Loss: -234331.906250\n",
      "Train Epoch: 109 [1536/17352 (9%)] Loss: -234521.687500\n",
      "Train Epoch: 109 [2944/17352 (17%)] Loss: -258237.437500\n",
      "Train Epoch: 109 [4352/17352 (25%)] Loss: -234390.859375\n",
      "Train Epoch: 109 [5760/17352 (33%)] Loss: -211171.062500\n",
      "Train Epoch: 109 [7168/17352 (41%)] Loss: -236628.312500\n",
      "Train Epoch: 109 [8576/17352 (49%)] Loss: -216046.406250\n",
      "Train Epoch: 109 [9984/17352 (58%)] Loss: -230862.218750\n",
      "Train Epoch: 109 [11392/17352 (66%)] Loss: -215244.718750\n",
      "Train Epoch: 109 [12800/17352 (74%)] Loss: -217494.296875\n",
      "Train Epoch: 109 [14208/17352 (82%)] Loss: -229375.218750\n",
      "Train Epoch: 109 [15446/17352 (89%)] Loss: -24548.052734\n",
      "Train Epoch: 109 [16179/17352 (93%)] Loss: -28730.703125\n",
      "Train Epoch: 109 [17081/17352 (98%)] Loss: -65451.023438\n",
      "    epoch          : 109\n",
      "    loss           : -210868.1246165845\n",
      "    val_loss       : -114004.36295572917\n",
      "Train Epoch: 110 [128/17352 (1%)] Loss: -235746.609375\n",
      "Train Epoch: 110 [1536/17352 (9%)] Loss: -235986.781250\n",
      "Train Epoch: 110 [2944/17352 (17%)] Loss: -242286.265625\n",
      "Train Epoch: 110 [4352/17352 (25%)] Loss: -226703.750000\n",
      "Train Epoch: 110 [5760/17352 (33%)] Loss: -239618.656250\n",
      "Train Epoch: 110 [7168/17352 (41%)] Loss: -227185.687500\n",
      "Train Epoch: 110 [8576/17352 (49%)] Loss: -223561.562500\n",
      "Train Epoch: 110 [9984/17352 (58%)] Loss: -236390.734375\n",
      "Train Epoch: 110 [11392/17352 (66%)] Loss: -234892.484375\n",
      "Train Epoch: 110 [12800/17352 (74%)] Loss: -234846.718750\n",
      "Train Epoch: 110 [14208/17352 (82%)] Loss: -223200.375000\n",
      "Train Epoch: 110 [15559/17352 (90%)] Loss: -198455.156250\n",
      "Train Epoch: 110 [16270/17352 (94%)] Loss: -145710.890625\n",
      "Train Epoch: 110 [17007/17352 (98%)] Loss: -5505.633789\n",
      "    epoch          : 110\n",
      "    loss           : -210932.41269269085\n",
      "    val_loss       : -113976.0016764323\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0821_122628/checkpoint-epoch110.pth ...\n",
      "Train Epoch: 111 [128/17352 (1%)] Loss: -215930.203125\n",
      "Train Epoch: 111 [1536/17352 (9%)] Loss: -223927.000000\n",
      "Train Epoch: 111 [2944/17352 (17%)] Loss: -230868.562500\n",
      "Train Epoch: 111 [4352/17352 (25%)] Loss: -234565.359375\n",
      "Train Epoch: 111 [5760/17352 (33%)] Loss: -228849.484375\n",
      "Train Epoch: 111 [7168/17352 (41%)] Loss: -224981.937500\n",
      "Train Epoch: 111 [8576/17352 (49%)] Loss: -219134.125000\n",
      "Train Epoch: 111 [9984/17352 (58%)] Loss: -229859.890625\n",
      "Train Epoch: 111 [11392/17352 (66%)] Loss: -236629.125000\n",
      "Train Epoch: 111 [12800/17352 (74%)] Loss: -236142.406250\n",
      "Train Epoch: 111 [14208/17352 (82%)] Loss: -240141.609375\n",
      "Train Epoch: 111 [15544/17352 (90%)] Loss: -199775.437500\n",
      "Train Epoch: 111 [16250/17352 (94%)] Loss: -5562.151367\n",
      "Train Epoch: 111 [17091/17352 (98%)] Loss: -134923.750000\n",
      "    epoch          : 111\n",
      "    loss           : -210895.03854472525\n",
      "    val_loss       : -113983.14662272135\n",
      "Train Epoch: 112 [128/17352 (1%)] Loss: -237602.312500\n",
      "Train Epoch: 112 [1536/17352 (9%)] Loss: -229943.765625\n",
      "Train Epoch: 112 [2944/17352 (17%)] Loss: -224652.468750\n",
      "Train Epoch: 112 [4352/17352 (25%)] Loss: -239966.968750\n",
      "Train Epoch: 112 [5760/17352 (33%)] Loss: -216122.046875\n",
      "Train Epoch: 112 [7168/17352 (41%)] Loss: -239392.296875\n",
      "Train Epoch: 112 [8576/17352 (49%)] Loss: -221438.203125\n",
      "Train Epoch: 112 [9984/17352 (58%)] Loss: -236113.625000\n",
      "Train Epoch: 112 [11392/17352 (66%)] Loss: -236654.218750\n",
      "Train Epoch: 112 [12800/17352 (74%)] Loss: -226584.531250\n",
      "Train Epoch: 112 [14208/17352 (82%)] Loss: -226994.718750\n",
      "Train Epoch: 112 [15550/17352 (90%)] Loss: -133831.656250\n",
      "Train Epoch: 112 [16372/17352 (94%)] Loss: -24009.906250\n",
      "Train Epoch: 112 [17027/17352 (98%)] Loss: -147827.484375\n",
      "    epoch          : 112\n",
      "    loss           : -210997.25589869966\n",
      "    val_loss       : -113910.21007486978\n",
      "Train Epoch: 113 [128/17352 (1%)] Loss: -234647.437500\n",
      "Train Epoch: 113 [1536/17352 (9%)] Loss: -238127.187500\n",
      "Train Epoch: 113 [2944/17352 (17%)] Loss: -239939.781250\n",
      "Train Epoch: 113 [4352/17352 (25%)] Loss: -235315.281250\n",
      "Train Epoch: 113 [5760/17352 (33%)] Loss: -237913.687500\n",
      "Train Epoch: 113 [7168/17352 (41%)] Loss: -217311.750000\n",
      "Train Epoch: 113 [8576/17352 (49%)] Loss: -228221.968750\n",
      "Train Epoch: 113 [9984/17352 (58%)] Loss: -227156.968750\n",
      "Train Epoch: 113 [11392/17352 (66%)] Loss: -214296.796875\n",
      "Train Epoch: 113 [12800/17352 (74%)] Loss: -223163.296875\n",
      "Train Epoch: 113 [14208/17352 (82%)] Loss: -226480.015625\n",
      "Train Epoch: 113 [15528/17352 (89%)] Loss: -141836.156250\n",
      "Train Epoch: 113 [16187/17352 (93%)] Loss: -5715.215332\n",
      "Train Epoch: 113 [16985/17352 (98%)] Loss: -199291.500000\n",
      "    epoch          : 113\n",
      "    loss           : -211054.31073366717\n",
      "    val_loss       : -113990.1849609375\n",
      "Train Epoch: 114 [128/17352 (1%)] Loss: -215306.656250\n",
      "Train Epoch: 114 [1536/17352 (9%)] Loss: -223309.812500\n",
      "Train Epoch: 114 [2944/17352 (17%)] Loss: -239391.890625\n",
      "Train Epoch: 114 [4352/17352 (25%)] Loss: -228757.281250\n",
      "Train Epoch: 114 [5760/17352 (33%)] Loss: -229771.484375\n",
      "Train Epoch: 114 [7168/17352 (41%)] Loss: -214766.484375\n",
      "Train Epoch: 114 [8576/17352 (49%)] Loss: -222618.515625\n",
      "Train Epoch: 114 [9984/17352 (58%)] Loss: -216008.203125\n",
      "Train Epoch: 114 [11392/17352 (66%)] Loss: -226285.968750\n",
      "Train Epoch: 114 [12800/17352 (74%)] Loss: -247328.406250\n",
      "Train Epoch: 114 [14208/17352 (82%)] Loss: -238476.562500\n",
      "Train Epoch: 114 [15455/17352 (89%)] Loss: -150631.921875\n",
      "Train Epoch: 114 [16205/17352 (93%)] Loss: -185710.218750\n",
      "Train Epoch: 114 [16982/17352 (98%)] Loss: -65659.906250\n",
      "    epoch          : 114\n",
      "    loss           : -211126.16376428795\n",
      "    val_loss       : -113963.87672526042\n",
      "Train Epoch: 115 [128/17352 (1%)] Loss: -213261.890625\n",
      "Train Epoch: 115 [1536/17352 (9%)] Loss: -226373.250000\n",
      "Train Epoch: 115 [2944/17352 (17%)] Loss: -214755.203125\n",
      "Train Epoch: 115 [4352/17352 (25%)] Loss: -218614.718750\n",
      "Train Epoch: 115 [5760/17352 (33%)] Loss: -223774.062500\n",
      "Train Epoch: 115 [7168/17352 (41%)] Loss: -229292.000000\n",
      "Train Epoch: 115 [8576/17352 (49%)] Loss: -238173.281250\n",
      "Train Epoch: 115 [9984/17352 (58%)] Loss: -235936.937500\n",
      "Train Epoch: 115 [11392/17352 (66%)] Loss: -237047.750000\n",
      "Train Epoch: 115 [12800/17352 (74%)] Loss: -234595.031250\n",
      "Train Epoch: 115 [14208/17352 (82%)] Loss: -238337.890625\n",
      "Train Epoch: 115 [15534/17352 (90%)] Loss: -157205.281250\n",
      "Train Epoch: 115 [16218/17352 (93%)] Loss: -199550.812500\n",
      "Train Epoch: 115 [17039/17352 (98%)] Loss: -68516.664062\n",
      "    epoch          : 115\n",
      "    loss           : -211104.15692507342\n",
      "    val_loss       : -113940.53362630209\n",
      "Train Epoch: 116 [128/17352 (1%)] Loss: -217319.281250\n",
      "Train Epoch: 116 [1536/17352 (9%)] Loss: -234270.937500\n",
      "Train Epoch: 116 [2944/17352 (17%)] Loss: -225240.125000\n",
      "Train Epoch: 116 [4352/17352 (25%)] Loss: -233228.140625\n",
      "Train Epoch: 116 [5760/17352 (33%)] Loss: -218085.031250\n",
      "Train Epoch: 116 [7168/17352 (41%)] Loss: -257942.078125\n",
      "Train Epoch: 116 [8576/17352 (49%)] Loss: -238828.640625\n",
      "Train Epoch: 116 [9984/17352 (58%)] Loss: -236028.250000\n",
      "Train Epoch: 116 [11392/17352 (66%)] Loss: -237376.718750\n",
      "Train Epoch: 116 [12800/17352 (74%)] Loss: -232239.906250\n",
      "Train Epoch: 116 [14208/17352 (82%)] Loss: -248364.500000\n",
      "Train Epoch: 116 [15478/17352 (89%)] Loss: -188357.500000\n",
      "Train Epoch: 116 [16147/17352 (93%)] Loss: -153291.625000\n",
      "Train Epoch: 116 [16903/17352 (97%)] Loss: -158255.234375\n",
      "    epoch          : 116\n",
      "    loss           : -211163.53706677328\n",
      "    val_loss       : -114001.88426106771\n",
      "Train Epoch: 117 [128/17352 (1%)] Loss: -236544.031250\n",
      "Train Epoch: 117 [1536/17352 (9%)] Loss: -240500.984375\n",
      "Train Epoch: 117 [2944/17352 (17%)] Loss: -225421.968750\n",
      "Train Epoch: 117 [4352/17352 (25%)] Loss: -227344.125000\n",
      "Train Epoch: 117 [5760/17352 (33%)] Loss: -221901.343750\n",
      "Train Epoch: 117 [7168/17352 (41%)] Loss: -227898.781250\n",
      "Train Epoch: 117 [8576/17352 (49%)] Loss: -240153.281250\n",
      "Train Epoch: 117 [9984/17352 (58%)] Loss: -237266.875000\n",
      "Train Epoch: 117 [11392/17352 (66%)] Loss: -218149.968750\n",
      "Train Epoch: 117 [12800/17352 (74%)] Loss: -229699.968750\n",
      "Train Epoch: 117 [14208/17352 (82%)] Loss: -229753.140625\n",
      "Train Epoch: 117 [15541/17352 (90%)] Loss: -150680.593750\n",
      "Train Epoch: 117 [16391/17352 (94%)] Loss: -91663.640625\n",
      "Train Epoch: 117 [17154/17352 (99%)] Loss: -140285.296875\n",
      "    epoch          : 117\n",
      "    loss           : -211214.70962012373\n",
      "    val_loss       : -113863.55891113282\n",
      "Train Epoch: 118 [128/17352 (1%)] Loss: -237527.625000\n",
      "Train Epoch: 118 [1536/17352 (9%)] Loss: -221059.625000\n",
      "Train Epoch: 118 [2944/17352 (17%)] Loss: -238111.640625\n",
      "Train Epoch: 118 [4352/17352 (25%)] Loss: -239108.640625\n",
      "Train Epoch: 118 [5760/17352 (33%)] Loss: -236224.218750\n",
      "Train Epoch: 118 [7168/17352 (41%)] Loss: -242423.765625\n",
      "Train Epoch: 118 [8576/17352 (49%)] Loss: -238470.484375\n",
      "Train Epoch: 118 [9984/17352 (58%)] Loss: -216371.140625\n",
      "Train Epoch: 118 [11392/17352 (66%)] Loss: -236833.437500\n",
      "Train Epoch: 118 [12800/17352 (74%)] Loss: -232334.953125\n",
      "Train Epoch: 118 [14208/17352 (82%)] Loss: -234648.500000\n",
      "Train Epoch: 118 [15542/17352 (90%)] Loss: -156823.859375\n",
      "Train Epoch: 118 [16228/17352 (94%)] Loss: -140293.421875\n",
      "Train Epoch: 118 [17047/17352 (98%)] Loss: -198913.468750\n",
      "    epoch          : 118\n",
      "    loss           : -211265.29269347736\n",
      "    val_loss       : -113992.93015136718\n",
      "Train Epoch: 119 [128/17352 (1%)] Loss: -237294.953125\n",
      "Train Epoch: 119 [1536/17352 (9%)] Loss: -229986.968750\n",
      "Train Epoch: 119 [2944/17352 (17%)] Loss: -248980.671875\n",
      "Train Epoch: 119 [4352/17352 (25%)] Loss: -235667.687500\n",
      "Train Epoch: 119 [5760/17352 (33%)] Loss: -224999.750000\n",
      "Train Epoch: 119 [7168/17352 (41%)] Loss: -215761.828125\n",
      "Train Epoch: 119 [8576/17352 (49%)] Loss: -226166.468750\n",
      "Train Epoch: 119 [9984/17352 (58%)] Loss: -241274.218750\n",
      "Train Epoch: 119 [11392/17352 (66%)] Loss: -224339.875000\n",
      "Train Epoch: 119 [12800/17352 (74%)] Loss: -234802.906250\n",
      "Train Epoch: 119 [14208/17352 (82%)] Loss: -246031.921875\n",
      "Train Epoch: 119 [15470/17352 (89%)] Loss: -27975.861328\n",
      "Train Epoch: 119 [16124/17352 (93%)] Loss: -70041.234375\n",
      "Train Epoch: 119 [16928/17352 (98%)] Loss: -79748.437500\n",
      "    epoch          : 119\n",
      "    loss           : -211364.39548159606\n",
      "    val_loss       : -113958.70674641927\n",
      "Train Epoch: 120 [128/17352 (1%)] Loss: -235568.187500\n",
      "Train Epoch: 120 [1536/17352 (9%)] Loss: -227041.406250\n",
      "Train Epoch: 120 [2944/17352 (17%)] Loss: -229801.562500\n",
      "Train Epoch: 120 [4352/17352 (25%)] Loss: -234179.468750\n",
      "Train Epoch: 120 [5760/17352 (33%)] Loss: -231843.015625\n",
      "Train Epoch: 120 [7168/17352 (41%)] Loss: -258974.281250\n",
      "Train Epoch: 120 [8576/17352 (49%)] Loss: -246435.875000\n",
      "Train Epoch: 120 [9984/17352 (58%)] Loss: -235868.171875\n",
      "Train Epoch: 120 [11392/17352 (66%)] Loss: -237807.171875\n",
      "Train Epoch: 120 [12800/17352 (74%)] Loss: -222280.218750\n",
      "Train Epoch: 120 [14208/17352 (82%)] Loss: -231345.328125\n",
      "Train Epoch: 120 [15498/17352 (89%)] Loss: -171497.281250\n",
      "Train Epoch: 120 [16328/17352 (94%)] Loss: -71467.664062\n",
      "Train Epoch: 120 [17070/17352 (98%)] Loss: -5559.080566\n",
      "    epoch          : 120\n",
      "    loss           : -211412.65968107697\n",
      "    val_loss       : -113931.52722981772\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0821_122628/checkpoint-epoch120.pth ...\n",
      "Train Epoch: 121 [128/17352 (1%)] Loss: -230283.015625\n",
      "Train Epoch: 121 [1536/17352 (9%)] Loss: -239727.531250\n",
      "Train Epoch: 121 [2944/17352 (17%)] Loss: -214391.937500\n",
      "Train Epoch: 121 [4352/17352 (25%)] Loss: -228872.343750\n",
      "Train Epoch: 121 [5760/17352 (33%)] Loss: -224843.437500\n",
      "Train Epoch: 121 [7168/17352 (41%)] Loss: -237904.687500\n",
      "Train Epoch: 121 [8576/17352 (49%)] Loss: -221740.328125\n",
      "Train Epoch: 121 [9984/17352 (58%)] Loss: -228090.031250\n",
      "Train Epoch: 121 [11392/17352 (66%)] Loss: -217229.390625\n",
      "Train Epoch: 121 [12800/17352 (74%)] Loss: -238340.609375\n",
      "Train Epoch: 121 [14208/17352 (82%)] Loss: -236683.046875\n",
      "Train Epoch: 121 [15542/17352 (90%)] Loss: -200753.796875\n",
      "Train Epoch: 121 [16321/17352 (94%)] Loss: -169636.953125\n",
      "Train Epoch: 121 [17086/17352 (98%)] Loss: -134742.421875\n",
      "    epoch          : 121\n",
      "    loss           : -211372.4822612993\n",
      "    val_loss       : -113983.97370605469\n",
      "Train Epoch: 122 [128/17352 (1%)] Loss: -234115.062500\n",
      "Train Epoch: 122 [1536/17352 (9%)] Loss: -235283.031250\n",
      "Train Epoch: 122 [2944/17352 (17%)] Loss: -226364.875000\n",
      "Train Epoch: 122 [4352/17352 (25%)] Loss: -223561.250000\n",
      "Train Epoch: 122 [5760/17352 (33%)] Loss: -230638.187500\n",
      "Train Epoch: 122 [7168/17352 (41%)] Loss: -222015.843750\n",
      "Train Epoch: 122 [8576/17352 (49%)] Loss: -228945.140625\n",
      "Train Epoch: 122 [9984/17352 (58%)] Loss: -219033.125000\n",
      "Train Epoch: 122 [11392/17352 (66%)] Loss: -242643.406250\n",
      "Train Epoch: 122 [12800/17352 (74%)] Loss: -223442.515625\n",
      "Train Epoch: 122 [14208/17352 (82%)] Loss: -240334.375000\n",
      "Train Epoch: 122 [15554/17352 (90%)] Loss: -172291.234375\n",
      "Train Epoch: 122 [16284/17352 (94%)] Loss: -169790.406250\n",
      "Train Epoch: 122 [16978/17352 (98%)] Loss: -143283.968750\n",
      "    epoch          : 122\n",
      "    loss           : -211390.3385230967\n",
      "    val_loss       : -113925.26465657553\n",
      "Train Epoch: 123 [128/17352 (1%)] Loss: -216970.140625\n",
      "Train Epoch: 123 [1536/17352 (9%)] Loss: -228679.750000\n",
      "Train Epoch: 123 [2944/17352 (17%)] Loss: -224932.781250\n",
      "Train Epoch: 123 [4352/17352 (25%)] Loss: -234000.734375\n",
      "Train Epoch: 123 [5760/17352 (33%)] Loss: -229110.359375\n",
      "Train Epoch: 123 [7168/17352 (41%)] Loss: -241041.812500\n",
      "Train Epoch: 123 [8576/17352 (49%)] Loss: -248053.171875\n",
      "Train Epoch: 123 [9984/17352 (58%)] Loss: -239870.562500\n",
      "Train Epoch: 123 [11392/17352 (66%)] Loss: -231579.312500\n",
      "Train Epoch: 123 [12800/17352 (74%)] Loss: -226254.968750\n",
      "Train Epoch: 123 [14208/17352 (82%)] Loss: -230552.187500\n",
      "Train Epoch: 123 [15540/17352 (90%)] Loss: -134140.937500\n",
      "Train Epoch: 123 [16301/17352 (94%)] Loss: -201480.843750\n",
      "Train Epoch: 123 [16988/17352 (98%)] Loss: -91019.156250\n",
      "    epoch          : 123\n",
      "    loss           : -211492.599255453\n",
      "    val_loss       : -113910.73673502605\n",
      "Train Epoch: 124 [128/17352 (1%)] Loss: -215514.015625\n",
      "Train Epoch: 124 [1536/17352 (9%)] Loss: -231853.906250\n",
      "Train Epoch: 124 [2944/17352 (17%)] Loss: -221082.312500\n",
      "Train Epoch: 124 [4352/17352 (25%)] Loss: -240974.406250\n",
      "Train Epoch: 124 [5760/17352 (33%)] Loss: -236497.593750\n",
      "Train Epoch: 124 [7168/17352 (41%)] Loss: -234528.140625\n",
      "Train Epoch: 124 [8576/17352 (49%)] Loss: -238767.125000\n",
      "Train Epoch: 124 [9984/17352 (58%)] Loss: -222337.562500\n",
      "Train Epoch: 124 [11392/17352 (66%)] Loss: -237957.015625\n",
      "Train Epoch: 124 [12800/17352 (74%)] Loss: -235968.750000\n",
      "Train Epoch: 124 [14208/17352 (82%)] Loss: -241113.859375\n",
      "Train Epoch: 124 [15523/17352 (89%)] Loss: -157821.234375\n",
      "Train Epoch: 124 [16223/17352 (93%)] Loss: -9443.556641\n",
      "Train Epoch: 124 [16873/17352 (97%)] Loss: -65828.796875\n",
      "    epoch          : 124\n",
      "    loss           : -211566.992482435\n",
      "    val_loss       : -113922.6575032552\n",
      "Train Epoch: 125 [128/17352 (1%)] Loss: -218738.968750\n",
      "Train Epoch: 125 [1536/17352 (9%)] Loss: -237703.812500\n",
      "Train Epoch: 125 [2944/17352 (17%)] Loss: -217217.546875\n",
      "Train Epoch: 125 [4352/17352 (25%)] Loss: -247597.718750\n",
      "Train Epoch: 125 [5760/17352 (33%)] Loss: -231798.656250\n",
      "Train Epoch: 125 [7168/17352 (41%)] Loss: -222554.453125\n",
      "Train Epoch: 125 [8576/17352 (49%)] Loss: -220741.843750\n",
      "Train Epoch: 125 [9984/17352 (58%)] Loss: -239465.187500\n",
      "Train Epoch: 125 [11392/17352 (66%)] Loss: -230544.796875\n",
      "Train Epoch: 125 [12800/17352 (74%)] Loss: -226436.578125\n",
      "Train Epoch: 125 [14208/17352 (82%)] Loss: -236335.343750\n",
      "Train Epoch: 125 [15510/17352 (89%)] Loss: -170268.109375\n",
      "Train Epoch: 125 [16323/17352 (94%)] Loss: -24388.617188\n",
      "Train Epoch: 125 [17092/17352 (99%)] Loss: -188937.671875\n",
      "    epoch          : 125\n",
      "    loss           : -211608.2767702653\n",
      "    val_loss       : -113824.77231445312\n",
      "Train Epoch: 126 [128/17352 (1%)] Loss: -237217.187500\n",
      "Train Epoch: 126 [1536/17352 (9%)] Loss: -229577.500000\n",
      "Train Epoch: 126 [2944/17352 (17%)] Loss: -224466.500000\n",
      "Train Epoch: 126 [4352/17352 (25%)] Loss: -236714.562500\n",
      "Train Epoch: 126 [5760/17352 (33%)] Loss: -236722.046875\n",
      "Train Epoch: 126 [7168/17352 (41%)] Loss: -259316.156250\n",
      "Train Epoch: 126 [8576/17352 (49%)] Loss: -217708.843750\n",
      "Train Epoch: 126 [9984/17352 (58%)] Loss: -221190.031250\n",
      "Train Epoch: 126 [11392/17352 (66%)] Loss: -216196.312500\n",
      "Train Epoch: 126 [12800/17352 (74%)] Loss: -238451.250000\n",
      "Train Epoch: 126 [14208/17352 (82%)] Loss: -248377.218750\n",
      "Train Epoch: 126 [15495/17352 (89%)] Loss: -88901.820312\n",
      "Train Epoch: 126 [16364/17352 (94%)] Loss: -171369.562500\n",
      "Train Epoch: 126 [16944/17352 (98%)] Loss: -159494.875000\n",
      "    epoch          : 126\n",
      "    loss           : -211647.08023542367\n",
      "    val_loss       : -113948.4036702474\n",
      "Train Epoch: 127 [128/17352 (1%)] Loss: -216044.843750\n",
      "Train Epoch: 127 [1536/17352 (9%)] Loss: -226534.046875\n",
      "Train Epoch: 127 [2944/17352 (17%)] Loss: -231777.703125\n",
      "Train Epoch: 127 [4352/17352 (25%)] Loss: -236208.046875\n",
      "Train Epoch: 127 [5760/17352 (33%)] Loss: -243216.109375\n",
      "Train Epoch: 127 [7168/17352 (41%)] Loss: -236996.640625\n",
      "Train Epoch: 127 [8576/17352 (49%)] Loss: -231921.093750\n",
      "Train Epoch: 127 [9984/17352 (58%)] Loss: -236396.281250\n",
      "Train Epoch: 127 [11392/17352 (66%)] Loss: -259351.328125\n",
      "Train Epoch: 127 [12800/17352 (74%)] Loss: -223041.718750\n",
      "Train Epoch: 127 [14208/17352 (82%)] Loss: -240154.921875\n",
      "Train Epoch: 127 [15438/17352 (89%)] Loss: -135694.968750\n",
      "Train Epoch: 127 [16228/17352 (94%)] Loss: -238170.281250\n",
      "Train Epoch: 127 [16954/17352 (98%)] Loss: -70014.203125\n",
      "    epoch          : 127\n",
      "    loss           : -211703.65261246855\n",
      "    val_loss       : -113942.07422688803\n",
      "Train Epoch: 128 [128/17352 (1%)] Loss: -216766.625000\n",
      "Train Epoch: 128 [1536/17352 (9%)] Loss: -227969.390625\n",
      "Train Epoch: 128 [2944/17352 (17%)] Loss: -228495.500000\n",
      "Train Epoch: 128 [4352/17352 (25%)] Loss: -234985.546875\n",
      "Train Epoch: 128 [5760/17352 (33%)] Loss: -238275.546875\n",
      "Train Epoch: 128 [7168/17352 (41%)] Loss: -241382.921875\n",
      "Train Epoch: 128 [8576/17352 (49%)] Loss: -237850.375000\n",
      "Train Epoch: 128 [9984/17352 (58%)] Loss: -221792.359375\n",
      "Train Epoch: 128 [11392/17352 (66%)] Loss: -237518.406250\n",
      "Train Epoch: 128 [12800/17352 (74%)] Loss: -227350.625000\n",
      "Train Epoch: 128 [14208/17352 (82%)] Loss: -230616.125000\n",
      "Train Epoch: 128 [15416/17352 (89%)] Loss: -79822.171875\n",
      "Train Epoch: 128 [16303/17352 (94%)] Loss: -157629.687500\n",
      "Train Epoch: 128 [17074/17352 (98%)] Loss: -171470.312500\n",
      "    epoch          : 128\n",
      "    loss           : -211719.3095703125\n",
      "    val_loss       : -113916.82333984374\n",
      "Train Epoch: 129 [128/17352 (1%)] Loss: -237473.578125\n",
      "Train Epoch: 129 [1536/17352 (9%)] Loss: -236232.109375\n",
      "Train Epoch: 129 [2944/17352 (17%)] Loss: -217678.250000\n",
      "Train Epoch: 129 [4352/17352 (25%)] Loss: -226814.937500\n",
      "Train Epoch: 129 [5760/17352 (33%)] Loss: -239077.468750\n",
      "Train Epoch: 129 [7168/17352 (41%)] Loss: -234112.906250\n",
      "Train Epoch: 129 [8576/17352 (49%)] Loss: -246426.906250\n",
      "Train Epoch: 129 [9984/17352 (58%)] Loss: -241521.562500\n",
      "Train Epoch: 129 [11392/17352 (66%)] Loss: -229946.609375\n",
      "Train Epoch: 129 [12800/17352 (74%)] Loss: -225461.156250\n",
      "Train Epoch: 129 [14208/17352 (82%)] Loss: -238748.843750\n",
      "Train Epoch: 129 [15455/17352 (89%)] Loss: -68717.265625\n",
      "Train Epoch: 129 [16107/17352 (93%)] Loss: -158977.500000\n",
      "Train Epoch: 129 [16855/17352 (97%)] Loss: -27759.595703\n",
      "    epoch          : 129\n",
      "    loss           : -211819.4605049287\n",
      "    val_loss       : -113943.48976236979\n",
      "Train Epoch: 130 [128/17352 (1%)] Loss: -236047.687500\n",
      "Train Epoch: 130 [1536/17352 (9%)] Loss: -240293.109375\n",
      "Train Epoch: 130 [2944/17352 (17%)] Loss: -225835.500000\n",
      "Train Epoch: 130 [4352/17352 (25%)] Loss: -240591.218750\n",
      "Train Epoch: 130 [5760/17352 (33%)] Loss: -230982.171875\n",
      "Train Epoch: 130 [7168/17352 (41%)] Loss: -226128.265625\n",
      "Train Epoch: 130 [8576/17352 (49%)] Loss: -233101.625000\n",
      "Train Epoch: 130 [9984/17352 (58%)] Loss: -232114.484375\n",
      "Train Epoch: 130 [11392/17352 (66%)] Loss: -238020.078125\n",
      "Train Epoch: 130 [12800/17352 (74%)] Loss: -220743.375000\n",
      "Train Epoch: 130 [14208/17352 (82%)] Loss: -248556.093750\n",
      "Train Epoch: 130 [15448/17352 (89%)] Loss: -92754.203125\n",
      "Train Epoch: 130 [16250/17352 (94%)] Loss: -152881.234375\n",
      "Train Epoch: 130 [16959/17352 (98%)] Loss: -135001.531250\n",
      "    epoch          : 130\n",
      "    loss           : -211815.22563181628\n",
      "    val_loss       : -113927.86171061198\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0821_122628/checkpoint-epoch130.pth ...\n",
      "Train Epoch: 131 [128/17352 (1%)] Loss: -235862.187500\n",
      "Train Epoch: 131 [1536/17352 (9%)] Loss: -235594.593750\n",
      "Train Epoch: 131 [2944/17352 (17%)] Loss: -229523.437500\n",
      "Train Epoch: 131 [4352/17352 (25%)] Loss: -242149.265625\n",
      "Train Epoch: 131 [5760/17352 (33%)] Loss: -238574.656250\n",
      "Train Epoch: 131 [7168/17352 (41%)] Loss: -243028.437500\n",
      "Train Epoch: 131 [8576/17352 (49%)] Loss: -246571.796875\n",
      "Train Epoch: 131 [9984/17352 (58%)] Loss: -237036.828125\n",
      "Train Epoch: 131 [11392/17352 (66%)] Loss: -224214.859375\n",
      "Train Epoch: 131 [12800/17352 (74%)] Loss: -238074.625000\n",
      "Train Epoch: 131 [14208/17352 (82%)] Loss: -239079.375000\n",
      "Train Epoch: 131 [15380/17352 (89%)] Loss: -9247.312500\n",
      "Train Epoch: 131 [16048/17352 (92%)] Loss: -154483.781250\n",
      "Train Epoch: 131 [16918/17352 (97%)] Loss: -132011.484375\n",
      "    epoch          : 131\n",
      "    loss           : -211814.37322055895\n",
      "    val_loss       : -113924.14793294271\n",
      "Train Epoch: 132 [128/17352 (1%)] Loss: -237980.359375\n",
      "Train Epoch: 132 [1536/17352 (9%)] Loss: -228024.453125\n",
      "Train Epoch: 132 [2944/17352 (17%)] Loss: -216380.046875\n",
      "Train Epoch: 132 [4352/17352 (25%)] Loss: -231683.687500\n",
      "Train Epoch: 132 [5760/17352 (33%)] Loss: -218442.031250\n",
      "Train Epoch: 132 [7168/17352 (41%)] Loss: -235213.656250\n",
      "Train Epoch: 132 [8576/17352 (49%)] Loss: -221246.125000\n",
      "Train Epoch: 132 [9984/17352 (58%)] Loss: -239492.968750\n",
      "Train Epoch: 132 [11392/17352 (66%)] Loss: -238435.515625\n",
      "Train Epoch: 132 [12800/17352 (74%)] Loss: -231663.718750\n",
      "Train Epoch: 132 [14208/17352 (82%)] Loss: -229698.156250\n",
      "Train Epoch: 132 [15472/17352 (89%)] Loss: -66135.820312\n",
      "Train Epoch: 132 [16234/17352 (94%)] Loss: -9223.513672\n",
      "Train Epoch: 132 [17001/17352 (98%)] Loss: -155767.390625\n",
      "    epoch          : 132\n",
      "    loss           : -211883.31836265206\n",
      "    val_loss       : -113856.82135416666\n",
      "Train Epoch: 133 [128/17352 (1%)] Loss: -219564.578125\n",
      "Train Epoch: 133 [1536/17352 (9%)] Loss: -227315.093750\n",
      "Train Epoch: 133 [2944/17352 (17%)] Loss: -228749.062500\n",
      "Train Epoch: 133 [4352/17352 (25%)] Loss: -223083.562500\n",
      "Train Epoch: 133 [5760/17352 (33%)] Loss: -240111.421875\n",
      "Train Epoch: 133 [7168/17352 (41%)] Loss: -239926.359375\n",
      "Train Epoch: 133 [8576/17352 (49%)] Loss: -226330.125000\n",
      "Train Epoch: 133 [9984/17352 (58%)] Loss: -236833.671875\n",
      "Train Epoch: 133 [11392/17352 (66%)] Loss: -236259.265625\n",
      "Train Epoch: 133 [12800/17352 (74%)] Loss: -237265.859375\n",
      "Train Epoch: 133 [14208/17352 (82%)] Loss: -234236.000000\n",
      "Train Epoch: 133 [15449/17352 (89%)] Loss: -5732.014160\n",
      "Train Epoch: 133 [16349/17352 (94%)] Loss: -132112.875000\n",
      "Train Epoch: 133 [17066/17352 (98%)] Loss: -68405.960938\n",
      "    epoch          : 133\n",
      "    loss           : -211888.0431293257\n",
      "    val_loss       : -113885.02444661458\n",
      "Train Epoch: 134 [128/17352 (1%)] Loss: -237525.343750\n",
      "Train Epoch: 134 [1536/17352 (9%)] Loss: -229134.515625\n",
      "Train Epoch: 134 [2944/17352 (17%)] Loss: -221986.609375\n",
      "Train Epoch: 134 [4352/17352 (25%)] Loss: -232106.734375\n",
      "Train Epoch: 134 [5760/17352 (33%)] Loss: -230282.781250\n",
      "Train Epoch: 134 [7168/17352 (41%)] Loss: -236384.984375\n",
      "Train Epoch: 134 [8576/17352 (49%)] Loss: -229908.343750\n",
      "Train Epoch: 134 [9984/17352 (58%)] Loss: -237254.750000\n",
      "Train Epoch: 134 [11392/17352 (66%)] Loss: -238072.953125\n",
      "Train Epoch: 134 [12800/17352 (74%)] Loss: -235297.843750\n",
      "Train Epoch: 134 [14208/17352 (82%)] Loss: -239947.531250\n",
      "Train Epoch: 134 [15575/17352 (90%)] Loss: -156481.734375\n",
      "Train Epoch: 134 [16371/17352 (94%)] Loss: -190271.953125\n",
      "Train Epoch: 134 [17024/17352 (98%)] Loss: -172624.796875\n",
      "    epoch          : 134\n",
      "    loss           : -211950.71231386325\n",
      "    val_loss       : -113806.58662109375\n",
      "Train Epoch: 135 [128/17352 (1%)] Loss: -235372.937500\n",
      "Train Epoch: 135 [1536/17352 (9%)] Loss: -236979.625000\n",
      "Train Epoch: 135 [2944/17352 (17%)] Loss: -228464.375000\n",
      "Train Epoch: 135 [4352/17352 (25%)] Loss: -222419.203125\n",
      "Train Epoch: 135 [5760/17352 (33%)] Loss: -239544.890625\n",
      "Train Epoch: 135 [7168/17352 (41%)] Loss: -222602.140625\n",
      "Train Epoch: 135 [8576/17352 (49%)] Loss: -223954.437500\n",
      "Train Epoch: 135 [9984/17352 (58%)] Loss: -232357.765625\n",
      "Train Epoch: 135 [11392/17352 (66%)] Loss: -237261.093750\n",
      "Train Epoch: 135 [12800/17352 (74%)] Loss: -231111.625000\n",
      "Train Epoch: 135 [14208/17352 (82%)] Loss: -247507.078125\n",
      "Train Epoch: 135 [15447/17352 (89%)] Loss: -159990.218750\n",
      "Train Epoch: 135 [16166/17352 (93%)] Loss: -69026.351562\n",
      "Train Epoch: 135 [17000/17352 (98%)] Loss: -132194.718750\n",
      "    epoch          : 135\n",
      "    loss           : -211886.3207090237\n",
      "    val_loss       : -113887.75482584635\n",
      "Train Epoch: 136 [128/17352 (1%)] Loss: -238016.750000\n",
      "Train Epoch: 136 [1536/17352 (9%)] Loss: -232468.312500\n",
      "Train Epoch: 136 [2944/17352 (17%)] Loss: -226328.359375\n",
      "Train Epoch: 136 [4352/17352 (25%)] Loss: -228810.500000\n",
      "Train Epoch: 136 [5760/17352 (33%)] Loss: -241915.125000\n",
      "Train Epoch: 136 [7168/17352 (41%)] Loss: -227186.468750\n",
      "Train Epoch: 136 [8576/17352 (49%)] Loss: -222599.000000\n",
      "Train Epoch: 136 [9984/17352 (58%)] Loss: -239902.062500\n",
      "Train Epoch: 136 [11392/17352 (66%)] Loss: -217727.187500\n",
      "Train Epoch: 136 [12800/17352 (74%)] Loss: -233696.875000\n",
      "Train Epoch: 136 [14208/17352 (82%)] Loss: -247332.984375\n",
      "Train Epoch: 136 [15504/17352 (89%)] Loss: -92901.328125\n",
      "Train Epoch: 136 [16249/17352 (94%)] Loss: -69529.734375\n",
      "Train Epoch: 136 [16978/17352 (98%)] Loss: -28597.226562\n",
      "    epoch          : 136\n",
      "    loss           : -211943.28139091338\n",
      "    val_loss       : -113885.18948567708\n",
      "Train Epoch: 137 [128/17352 (1%)] Loss: -236343.750000\n",
      "Train Epoch: 137 [1536/17352 (9%)] Loss: -236502.515625\n",
      "Train Epoch: 137 [2944/17352 (17%)] Loss: -239612.875000\n",
      "Train Epoch: 137 [4352/17352 (25%)] Loss: -228208.406250\n",
      "Train Epoch: 137 [5760/17352 (33%)] Loss: -231724.265625\n",
      "Train Epoch: 137 [7168/17352 (41%)] Loss: -231666.906250\n",
      "Train Epoch: 137 [8576/17352 (49%)] Loss: -222925.906250\n",
      "Train Epoch: 137 [9984/17352 (58%)] Loss: -235720.437500\n",
      "Train Epoch: 137 [11392/17352 (66%)] Loss: -216713.781250\n",
      "Train Epoch: 137 [12800/17352 (74%)] Loss: -227270.109375\n",
      "Train Epoch: 137 [14208/17352 (82%)] Loss: -238608.125000\n",
      "Train Epoch: 137 [15479/17352 (89%)] Loss: -65795.171875\n",
      "Train Epoch: 137 [16361/17352 (94%)] Loss: -172719.515625\n",
      "Train Epoch: 137 [16989/17352 (98%)] Loss: -24319.308594\n",
      "    epoch          : 137\n",
      "    loss           : -211983.8734302905\n",
      "    val_loss       : -113888.70154622397\n",
      "Train Epoch: 138 [128/17352 (1%)] Loss: -235543.531250\n",
      "Train Epoch: 138 [1536/17352 (9%)] Loss: -237303.328125\n",
      "Train Epoch: 138 [2944/17352 (17%)] Loss: -240877.265625\n",
      "Train Epoch: 138 [4352/17352 (25%)] Loss: -222113.656250\n",
      "Train Epoch: 138 [5760/17352 (33%)] Loss: -233497.703125\n",
      "Train Epoch: 138 [7168/17352 (41%)] Loss: -223073.828125\n",
      "Train Epoch: 138 [8576/17352 (49%)] Loss: -238003.625000\n",
      "Train Epoch: 138 [9984/17352 (58%)] Loss: -235681.578125\n",
      "Train Epoch: 138 [11392/17352 (66%)] Loss: -231167.265625\n",
      "Train Epoch: 138 [12800/17352 (74%)] Loss: -224813.781250\n",
      "Train Epoch: 138 [14208/17352 (82%)] Loss: -239120.343750\n",
      "Train Epoch: 138 [15514/17352 (89%)] Loss: -143570.531250\n",
      "Train Epoch: 138 [16240/17352 (94%)] Loss: -151798.906250\n",
      "Train Epoch: 138 [17099/17352 (99%)] Loss: -171482.500000\n",
      "    epoch          : 138\n",
      "    loss           : -212013.9404296875\n",
      "    val_loss       : -113861.78872070313\n",
      "Train Epoch: 139 [128/17352 (1%)] Loss: -237981.031250\n",
      "Train Epoch: 139 [1536/17352 (9%)] Loss: -228504.812500\n",
      "Train Epoch: 139 [2944/17352 (17%)] Loss: -223664.281250\n",
      "Train Epoch: 139 [4352/17352 (25%)] Loss: -233365.000000\n",
      "Train Epoch: 139 [5760/17352 (33%)] Loss: -240069.359375\n",
      "Train Epoch: 139 [7168/17352 (41%)] Loss: -258669.968750\n",
      "Train Epoch: 139 [8576/17352 (49%)] Loss: -232802.593750\n",
      "Train Epoch: 139 [9984/17352 (58%)] Loss: -237386.093750\n",
      "Train Epoch: 139 [11392/17352 (66%)] Loss: -232264.296875\n",
      "Train Epoch: 139 [12800/17352 (74%)] Loss: -227813.359375\n",
      "Train Epoch: 139 [14208/17352 (82%)] Loss: -238308.781250\n",
      "Train Epoch: 139 [15419/17352 (89%)] Loss: -9610.098633\n",
      "Train Epoch: 139 [16232/17352 (94%)] Loss: -93312.429688\n",
      "Train Epoch: 139 [16957/17352 (98%)] Loss: -146331.937500\n",
      "    epoch          : 139\n",
      "    loss           : -212145.2224268561\n",
      "    val_loss       : -113867.74944661459\n",
      "Train Epoch: 140 [128/17352 (1%)] Loss: -239505.968750\n",
      "Train Epoch: 140 [1536/17352 (9%)] Loss: -241636.703125\n",
      "Train Epoch: 140 [2944/17352 (17%)] Loss: -244565.937500\n",
      "Train Epoch: 140 [4352/17352 (25%)] Loss: -241378.718750\n",
      "Train Epoch: 140 [5760/17352 (33%)] Loss: -239793.296875\n",
      "Train Epoch: 140 [7168/17352 (41%)] Loss: -223189.187500\n",
      "Train Epoch: 140 [8576/17352 (49%)] Loss: -222824.218750\n",
      "Train Epoch: 140 [9984/17352 (58%)] Loss: -241183.281250\n",
      "Train Epoch: 140 [11392/17352 (66%)] Loss: -216385.656250\n",
      "Train Epoch: 140 [12800/17352 (74%)] Loss: -221573.187500\n",
      "Train Epoch: 140 [14208/17352 (82%)] Loss: -230241.109375\n",
      "Train Epoch: 140 [15512/17352 (89%)] Loss: -143871.375000\n",
      "Train Epoch: 140 [16159/17352 (93%)] Loss: -199484.406250\n",
      "Train Epoch: 140 [17061/17352 (98%)] Loss: -65509.253906\n",
      "    epoch          : 140\n",
      "    loss           : -212158.08998466338\n",
      "    val_loss       : -113883.36073404948\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0821_122628/checkpoint-epoch140.pth ...\n",
      "Train Epoch: 141 [128/17352 (1%)] Loss: -218785.468750\n",
      "Train Epoch: 141 [1536/17352 (9%)] Loss: -222054.750000\n",
      "Train Epoch: 141 [2944/17352 (17%)] Loss: -236109.218750\n",
      "Train Epoch: 141 [4352/17352 (25%)] Loss: -228615.031250\n",
      "Train Epoch: 141 [5760/17352 (33%)] Loss: -239138.625000\n",
      "Train Epoch: 141 [7168/17352 (41%)] Loss: -259301.890625\n",
      "Train Epoch: 141 [8576/17352 (49%)] Loss: -228725.843750\n",
      "Train Epoch: 141 [9984/17352 (58%)] Loss: -242559.687500\n",
      "Train Epoch: 141 [11392/17352 (66%)] Loss: -219136.437500\n",
      "Train Epoch: 141 [12800/17352 (74%)] Loss: -237746.140625\n",
      "Train Epoch: 141 [14208/17352 (82%)] Loss: -225462.218750\n",
      "Train Epoch: 141 [15517/17352 (89%)] Loss: -160084.093750\n",
      "Train Epoch: 141 [16290/17352 (94%)] Loss: -68347.507812\n",
      "Train Epoch: 141 [16995/17352 (98%)] Loss: -238876.812500\n",
      "    epoch          : 141\n",
      "    loss           : -212151.87428560192\n",
      "    val_loss       : -113774.23745117188\n",
      "Train Epoch: 142 [128/17352 (1%)] Loss: -237217.406250\n",
      "Train Epoch: 142 [1536/17352 (9%)] Loss: -226606.500000\n",
      "Train Epoch: 142 [2944/17352 (17%)] Loss: -217835.578125\n",
      "Train Epoch: 142 [4352/17352 (25%)] Loss: -242479.062500\n",
      "Train Epoch: 142 [5760/17352 (33%)] Loss: -231849.625000\n",
      "Train Epoch: 142 [7168/17352 (41%)] Loss: -223061.890625\n",
      "Train Epoch: 142 [8576/17352 (49%)] Loss: -231912.968750\n",
      "Train Epoch: 142 [9984/17352 (58%)] Loss: -241167.656250\n",
      "Train Epoch: 142 [11392/17352 (66%)] Loss: -236442.656250\n",
      "Train Epoch: 142 [12800/17352 (74%)] Loss: -241711.218750\n",
      "Train Epoch: 142 [14208/17352 (82%)] Loss: -231725.921875\n",
      "Train Epoch: 142 [15552/17352 (90%)] Loss: -199845.062500\n",
      "Train Epoch: 142 [16332/17352 (94%)] Loss: -91744.421875\n",
      "Train Epoch: 142 [17106/17352 (99%)] Loss: -238310.531250\n",
      "    epoch          : 142\n",
      "    loss           : -212182.8947803062\n",
      "    val_loss       : -113877.78939615886\n",
      "Train Epoch: 143 [128/17352 (1%)] Loss: -225451.406250\n",
      "Train Epoch: 143 [1536/17352 (9%)] Loss: -234977.171875\n",
      "Train Epoch: 143 [2944/17352 (17%)] Loss: -226847.703125\n",
      "Train Epoch: 143 [4352/17352 (25%)] Loss: -236149.875000\n",
      "Train Epoch: 143 [5760/17352 (33%)] Loss: -238741.968750\n",
      "Train Epoch: 143 [7168/17352 (41%)] Loss: -240967.187500\n",
      "Train Epoch: 143 [8576/17352 (49%)] Loss: -227496.218750\n",
      "Train Epoch: 143 [9984/17352 (58%)] Loss: -224368.531250\n",
      "Train Epoch: 143 [11392/17352 (66%)] Loss: -237268.468750\n",
      "Train Epoch: 143 [12800/17352 (74%)] Loss: -233215.015625\n",
      "Train Epoch: 143 [14208/17352 (82%)] Loss: -226512.265625\n",
      "Train Epoch: 143 [15452/17352 (89%)] Loss: -28288.906250\n",
      "Train Epoch: 143 [16291/17352 (94%)] Loss: -91130.085938\n",
      "Train Epoch: 143 [17070/17352 (98%)] Loss: -151075.218750\n",
      "    epoch          : 143\n",
      "    loss           : -212263.19156354867\n",
      "    val_loss       : -113836.57841796875\n",
      "Train Epoch: 144 [128/17352 (1%)] Loss: -238401.218750\n",
      "Train Epoch: 144 [1536/17352 (9%)] Loss: -241900.093750\n",
      "Train Epoch: 144 [2944/17352 (17%)] Loss: -222720.156250\n",
      "Train Epoch: 144 [4352/17352 (25%)] Loss: -242529.937500\n",
      "Train Epoch: 144 [5760/17352 (33%)] Loss: -227435.796875\n",
      "Train Epoch: 144 [7168/17352 (41%)] Loss: -227095.578125\n",
      "Train Epoch: 144 [8576/17352 (49%)] Loss: -240019.687500\n",
      "Train Epoch: 144 [9984/17352 (58%)] Loss: -236627.546875\n",
      "Train Epoch: 144 [11392/17352 (66%)] Loss: -238571.734375\n",
      "Train Epoch: 144 [12800/17352 (74%)] Loss: -238909.031250\n",
      "Train Epoch: 144 [14208/17352 (82%)] Loss: -247669.328125\n",
      "Train Epoch: 144 [15546/17352 (90%)] Loss: -171063.031250\n",
      "Train Epoch: 144 [16185/17352 (93%)] Loss: -173510.984375\n",
      "Train Epoch: 144 [16978/17352 (98%)] Loss: -201277.062500\n",
      "    epoch          : 144\n",
      "    loss           : -212285.69636771182\n",
      "    val_loss       : -113828.89044596354\n",
      "Train Epoch: 145 [128/17352 (1%)] Loss: -219307.156250\n",
      "Train Epoch: 145 [1536/17352 (9%)] Loss: -237688.656250\n",
      "Train Epoch: 145 [2944/17352 (17%)] Loss: -218598.078125\n",
      "Train Epoch: 145 [4352/17352 (25%)] Loss: -236570.796875\n",
      "Train Epoch: 145 [5760/17352 (33%)] Loss: -238761.234375\n",
      "Train Epoch: 145 [7168/17352 (41%)] Loss: -242171.312500\n",
      "Train Epoch: 145 [8576/17352 (49%)] Loss: -226365.859375\n",
      "Train Epoch: 145 [9984/17352 (58%)] Loss: -241926.218750\n",
      "Train Epoch: 145 [11392/17352 (66%)] Loss: -235914.625000\n",
      "Train Epoch: 145 [12800/17352 (74%)] Loss: -232930.234375\n",
      "Train Epoch: 145 [14208/17352 (82%)] Loss: -237185.093750\n",
      "Train Epoch: 145 [15504/17352 (89%)] Loss: -187174.312500\n",
      "Train Epoch: 145 [16302/17352 (94%)] Loss: -143596.093750\n",
      "Train Epoch: 145 [16988/17352 (98%)] Loss: -171273.500000\n",
      "    epoch          : 145\n",
      "    loss           : -212341.29652435507\n",
      "    val_loss       : -113871.77583007813\n",
      "Train Epoch: 146 [128/17352 (1%)] Loss: -238388.281250\n",
      "Train Epoch: 146 [1536/17352 (9%)] Loss: -229177.125000\n",
      "Train Epoch: 146 [2944/17352 (17%)] Loss: -217991.562500\n",
      "Train Epoch: 146 [4352/17352 (25%)] Loss: -233565.750000\n",
      "Train Epoch: 146 [5760/17352 (33%)] Loss: -233927.375000\n",
      "Train Epoch: 146 [7168/17352 (41%)] Loss: -226506.140625\n",
      "Train Epoch: 146 [8576/17352 (49%)] Loss: -239460.250000\n",
      "Train Epoch: 146 [9984/17352 (58%)] Loss: -240521.375000\n",
      "Train Epoch: 146 [11392/17352 (66%)] Loss: -231045.390625\n",
      "Train Epoch: 146 [12800/17352 (74%)] Loss: -233820.750000\n",
      "Train Epoch: 146 [14208/17352 (82%)] Loss: -238636.156250\n",
      "Train Epoch: 146 [15519/17352 (89%)] Loss: -157744.015625\n",
      "Train Epoch: 146 [16284/17352 (94%)] Loss: -142031.437500\n",
      "Train Epoch: 146 [16957/17352 (98%)] Loss: -94110.890625\n",
      "    epoch          : 146\n",
      "    loss           : -212334.78834482486\n",
      "    val_loss       : -113815.58409830728\n",
      "Train Epoch: 147 [128/17352 (1%)] Loss: -237129.203125\n",
      "Train Epoch: 147 [1536/17352 (9%)] Loss: -235240.843750\n",
      "Train Epoch: 147 [2944/17352 (17%)] Loss: -225322.593750\n",
      "Train Epoch: 147 [4352/17352 (25%)] Loss: -235251.343750\n",
      "Train Epoch: 147 [5760/17352 (33%)] Loss: -237915.562500\n",
      "Train Epoch: 147 [7168/17352 (41%)] Loss: -235966.781250\n",
      "Train Epoch: 147 [8576/17352 (49%)] Loss: -248503.187500\n",
      "Train Epoch: 147 [9984/17352 (58%)] Loss: -227314.203125\n",
      "Train Epoch: 147 [11392/17352 (66%)] Loss: -235832.406250\n",
      "Train Epoch: 147 [12800/17352 (74%)] Loss: -227193.953125\n",
      "Train Epoch: 147 [14208/17352 (82%)] Loss: -236092.296875\n",
      "Train Epoch: 147 [15470/17352 (89%)] Loss: -8935.675781\n",
      "Train Epoch: 147 [16177/17352 (93%)] Loss: -144246.687500\n",
      "Train Epoch: 147 [16975/17352 (98%)] Loss: -134108.328125\n",
      "    epoch          : 147\n",
      "    loss           : -212255.87215551594\n",
      "    val_loss       : -113786.52665201823\n",
      "Train Epoch: 148 [128/17352 (1%)] Loss: -216945.750000\n",
      "Train Epoch: 148 [1536/17352 (9%)] Loss: -228844.515625\n",
      "Train Epoch: 148 [2944/17352 (17%)] Loss: -219830.406250\n",
      "Train Epoch: 148 [4352/17352 (25%)] Loss: -233837.265625\n",
      "Train Epoch: 148 [5760/17352 (33%)] Loss: -231270.890625\n",
      "Train Epoch: 148 [7168/17352 (41%)] Loss: -258002.890625\n",
      "Train Epoch: 148 [8576/17352 (49%)] Loss: -229212.906250\n",
      "Train Epoch: 148 [9984/17352 (58%)] Loss: -238773.062500\n",
      "Train Epoch: 148 [11392/17352 (66%)] Loss: -238972.296875\n",
      "Train Epoch: 148 [12800/17352 (74%)] Loss: -238202.546875\n",
      "Train Epoch: 148 [14208/17352 (82%)] Loss: -229080.781250\n",
      "Train Epoch: 148 [15514/17352 (89%)] Loss: -143629.078125\n",
      "Train Epoch: 148 [16383/17352 (94%)] Loss: -202296.656250\n",
      "Train Epoch: 148 [16958/17352 (98%)] Loss: -5739.121094\n",
      "    epoch          : 148\n",
      "    loss           : -212261.11426436662\n",
      "    val_loss       : -113775.10928548177\n",
      "Train Epoch: 149 [128/17352 (1%)] Loss: -217264.562500\n",
      "Train Epoch: 149 [1536/17352 (9%)] Loss: -236638.781250\n",
      "Train Epoch: 149 [2944/17352 (17%)] Loss: -223277.828125\n",
      "Train Epoch: 149 [4352/17352 (25%)] Loss: -234979.921875\n",
      "Train Epoch: 149 [5760/17352 (33%)] Loss: -248457.156250\n",
      "Train Epoch: 149 [7168/17352 (41%)] Loss: -240307.671875\n",
      "Train Epoch: 149 [8576/17352 (49%)] Loss: -233596.375000\n",
      "Train Epoch: 149 [9984/17352 (58%)] Loss: -233920.937500\n",
      "Train Epoch: 149 [11392/17352 (66%)] Loss: -231621.937500\n",
      "Train Epoch: 149 [12800/17352 (74%)] Loss: -228368.125000\n",
      "Train Epoch: 149 [14208/17352 (82%)] Loss: -229091.906250\n",
      "Train Epoch: 149 [15460/17352 (89%)] Loss: -27873.375000\n",
      "Train Epoch: 149 [16064/17352 (93%)] Loss: -199259.718750\n",
      "Train Epoch: 149 [16892/17352 (97%)] Loss: -151743.843750\n",
      "    epoch          : 149\n",
      "    loss           : -212394.92829802854\n",
      "    val_loss       : -113796.74414876303\n",
      "Train Epoch: 150 [128/17352 (1%)] Loss: -236949.156250\n",
      "Train Epoch: 150 [1536/17352 (9%)] Loss: -238580.718750\n",
      "Train Epoch: 150 [2944/17352 (17%)] Loss: -226411.515625\n",
      "Train Epoch: 150 [4352/17352 (25%)] Loss: -228919.578125\n",
      "Train Epoch: 150 [5760/17352 (33%)] Loss: -227928.312500\n",
      "Train Epoch: 150 [7168/17352 (41%)] Loss: -218107.109375\n",
      "Train Epoch: 150 [8576/17352 (49%)] Loss: -227398.031250\n",
      "Train Epoch: 150 [9984/17352 (58%)] Loss: -233486.687500\n",
      "Train Epoch: 150 [11392/17352 (66%)] Loss: -218095.500000\n",
      "Train Epoch: 150 [12800/17352 (74%)] Loss: -232781.640625\n",
      "Train Epoch: 150 [14208/17352 (82%)] Loss: -224488.312500\n",
      "Train Epoch: 150 [15448/17352 (89%)] Loss: -172574.625000\n",
      "Train Epoch: 150 [16090/17352 (93%)] Loss: -24408.263672\n",
      "Train Epoch: 150 [17040/17352 (98%)] Loss: -187467.343750\n",
      "    epoch          : 150\n",
      "    loss           : -212473.46643312185\n",
      "    val_loss       : -113813.91304524739\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0821_122628/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [128/17352 (1%)] Loss: -238208.390625\n",
      "Train Epoch: 151 [1536/17352 (9%)] Loss: -229613.656250\n",
      "Train Epoch: 151 [2944/17352 (17%)] Loss: -239062.843750\n",
      "Train Epoch: 151 [4352/17352 (25%)] Loss: -223831.671875\n",
      "Train Epoch: 151 [5760/17352 (33%)] Loss: -248129.562500\n",
      "Train Epoch: 151 [7168/17352 (41%)] Loss: -227337.187500\n",
      "Train Epoch: 151 [8576/17352 (49%)] Loss: -239816.375000\n",
      "Train Epoch: 151 [9984/17352 (58%)] Loss: -240157.906250\n",
      "Train Epoch: 151 [11392/17352 (66%)] Loss: -240192.640625\n",
      "Train Epoch: 151 [12800/17352 (74%)] Loss: -234436.687500\n",
      "Train Epoch: 151 [14208/17352 (82%)] Loss: -224508.640625\n",
      "Train Epoch: 151 [15451/17352 (89%)] Loss: -88829.195312\n",
      "Train Epoch: 151 [16242/17352 (94%)] Loss: -155183.625000\n",
      "Train Epoch: 151 [17042/17352 (98%)] Loss: -65283.671875\n",
      "    epoch          : 151\n",
      "    loss           : -212529.93501271497\n",
      "    val_loss       : -113814.63089192708\n",
      "Train Epoch: 152 [128/17352 (1%)] Loss: -219455.312500\n",
      "Train Epoch: 152 [1536/17352 (9%)] Loss: -241501.640625\n",
      "Train Epoch: 152 [2944/17352 (17%)] Loss: -233112.000000\n",
      "Train Epoch: 152 [4352/17352 (25%)] Loss: -231650.531250\n",
      "Train Epoch: 152 [5760/17352 (33%)] Loss: -233145.218750\n",
      "Train Epoch: 152 [7168/17352 (41%)] Loss: -232513.000000\n",
      "Train Epoch: 152 [8576/17352 (49%)] Loss: -220181.046875\n",
      "Train Epoch: 152 [9984/17352 (58%)] Loss: -238199.375000\n",
      "Train Epoch: 152 [11392/17352 (66%)] Loss: -231921.015625\n",
      "Train Epoch: 152 [12800/17352 (74%)] Loss: -248002.468750\n",
      "Train Epoch: 152 [14208/17352 (82%)] Loss: -225324.500000\n",
      "Train Epoch: 152 [15438/17352 (89%)] Loss: -136548.828125\n",
      "Train Epoch: 152 [16148/17352 (93%)] Loss: -133980.593750\n",
      "Train Epoch: 152 [16942/17352 (98%)] Loss: -201081.953125\n",
      "    epoch          : 152\n",
      "    loss           : -212555.26117803587\n",
      "    val_loss       : -113766.20574544271\n",
      "Train Epoch: 153 [128/17352 (1%)] Loss: -217426.703125\n",
      "Train Epoch: 153 [1536/17352 (9%)] Loss: -235063.687500\n",
      "Train Epoch: 153 [2944/17352 (17%)] Loss: -242852.046875\n",
      "Train Epoch: 153 [4352/17352 (25%)] Loss: -229001.218750\n",
      "Train Epoch: 153 [5760/17352 (33%)] Loss: -237050.921875\n",
      "Train Epoch: 153 [7168/17352 (41%)] Loss: -240674.937500\n",
      "Train Epoch: 153 [8576/17352 (49%)] Loss: -232148.750000\n",
      "Train Epoch: 153 [9984/17352 (58%)] Loss: -225293.734375\n",
      "Train Epoch: 153 [11392/17352 (66%)] Loss: -230025.484375\n",
      "Train Epoch: 153 [12800/17352 (74%)] Loss: -235500.312500\n",
      "Train Epoch: 153 [14208/17352 (82%)] Loss: -241625.312500\n",
      "Train Epoch: 153 [15565/17352 (90%)] Loss: -141602.953125\n",
      "Train Epoch: 153 [16344/17352 (94%)] Loss: -151269.875000\n",
      "Train Epoch: 153 [17084/17352 (98%)] Loss: -88699.656250\n",
      "    epoch          : 153\n",
      "    loss           : -212554.77723233012\n",
      "    val_loss       : -113744.5546875\n",
      "Train Epoch: 154 [128/17352 (1%)] Loss: -234894.500000\n",
      "Train Epoch: 154 [1536/17352 (9%)] Loss: -229949.671875\n",
      "Train Epoch: 154 [2944/17352 (17%)] Loss: -229092.593750\n",
      "Train Epoch: 154 [4352/17352 (25%)] Loss: -235635.093750\n",
      "Train Epoch: 154 [5760/17352 (33%)] Loss: -228390.609375\n",
      "Train Epoch: 154 [7168/17352 (41%)] Loss: -223092.671875\n",
      "Train Epoch: 154 [8576/17352 (49%)] Loss: -230654.359375\n",
      "Train Epoch: 154 [9984/17352 (58%)] Loss: -224726.062500\n",
      "Train Epoch: 154 [11392/17352 (66%)] Loss: -232791.000000\n",
      "Train Epoch: 154 [12800/17352 (74%)] Loss: -240942.078125\n",
      "Train Epoch: 154 [14208/17352 (82%)] Loss: -229821.250000\n",
      "Train Epoch: 154 [15531/17352 (90%)] Loss: -171323.546875\n",
      "Train Epoch: 154 [16330/17352 (94%)] Loss: -157591.687500\n",
      "Train Epoch: 154 [17027/17352 (98%)] Loss: -143794.859375\n",
      "    epoch          : 154\n",
      "    loss           : -212585.30927210045\n",
      "    val_loss       : -113734.46975097657\n",
      "Train Epoch: 155 [128/17352 (1%)] Loss: -221019.312500\n",
      "Train Epoch: 155 [1536/17352 (9%)] Loss: -237777.546875\n",
      "Train Epoch: 155 [2944/17352 (17%)] Loss: -239341.984375\n",
      "Train Epoch: 155 [4352/17352 (25%)] Loss: -239862.437500\n",
      "Train Epoch: 155 [5760/17352 (33%)] Loss: -231282.734375\n",
      "Train Epoch: 155 [7168/17352 (41%)] Loss: -228359.140625\n",
      "Train Epoch: 155 [8576/17352 (49%)] Loss: -241195.406250\n",
      "Train Epoch: 155 [9984/17352 (58%)] Loss: -219472.125000\n",
      "Train Epoch: 155 [11392/17352 (66%)] Loss: -231867.875000\n",
      "Train Epoch: 155 [12800/17352 (74%)] Loss: -231651.250000\n",
      "Train Epoch: 155 [14208/17352 (82%)] Loss: -224712.937500\n",
      "Train Epoch: 155 [15376/17352 (89%)] Loss: -5514.223633\n",
      "Train Epoch: 155 [16196/17352 (93%)] Loss: -157902.640625\n",
      "Train Epoch: 155 [16924/17352 (98%)] Loss: -143968.046875\n",
      "    epoch          : 155\n",
      "    loss           : -212546.34572278734\n",
      "    val_loss       : -113794.01809082032\n",
      "Train Epoch: 156 [128/17352 (1%)] Loss: -220121.328125\n",
      "Train Epoch: 156 [1536/17352 (9%)] Loss: -235965.484375\n",
      "Train Epoch: 156 [2944/17352 (17%)] Loss: -231053.171875\n",
      "Train Epoch: 156 [4352/17352 (25%)] Loss: -233465.343750\n",
      "Train Epoch: 156 [5760/17352 (33%)] Loss: -230541.218750\n",
      "Train Epoch: 156 [7168/17352 (41%)] Loss: -228894.593750\n",
      "Train Epoch: 156 [8576/17352 (49%)] Loss: -219275.140625\n",
      "Train Epoch: 156 [9984/17352 (58%)] Loss: -224793.562500\n",
      "Train Epoch: 156 [11392/17352 (66%)] Loss: -230825.718750\n",
      "Train Epoch: 156 [12800/17352 (74%)] Loss: -227144.609375\n",
      "Train Epoch: 156 [14208/17352 (82%)] Loss: -235860.765625\n",
      "Train Epoch: 156 [15378/17352 (89%)] Loss: -28366.503906\n",
      "Train Epoch: 156 [16112/17352 (93%)] Loss: -156498.828125\n",
      "Train Epoch: 156 [16912/17352 (97%)] Loss: -159701.203125\n",
      "    epoch          : 156\n",
      "    loss           : -212649.68921717702\n",
      "    val_loss       : -113738.91081542968\n",
      "Train Epoch: 157 [128/17352 (1%)] Loss: -237476.062500\n",
      "Train Epoch: 157 [1536/17352 (9%)] Loss: -236950.078125\n",
      "Train Epoch: 157 [2944/17352 (17%)] Loss: -259795.203125\n",
      "Train Epoch: 157 [4352/17352 (25%)] Loss: -236416.593750\n",
      "Train Epoch: 157 [5760/17352 (33%)] Loss: -227819.218750\n",
      "Train Epoch: 157 [7168/17352 (41%)] Loss: -225609.843750\n",
      "Train Epoch: 157 [8576/17352 (49%)] Loss: -248824.906250\n",
      "Train Epoch: 157 [9984/17352 (58%)] Loss: -238414.843750\n",
      "Train Epoch: 157 [11392/17352 (66%)] Loss: -232561.593750\n",
      "Train Epoch: 157 [12800/17352 (74%)] Loss: -240436.875000\n",
      "Train Epoch: 157 [14208/17352 (82%)] Loss: -230778.625000\n",
      "Train Epoch: 157 [15468/17352 (89%)] Loss: -173256.343750\n",
      "Train Epoch: 157 [16230/17352 (94%)] Loss: -159868.046875\n",
      "Train Epoch: 157 [16913/17352 (97%)] Loss: -89800.937500\n",
      "    epoch          : 157\n",
      "    loss           : -212671.3175433882\n",
      "    val_loss       : -113755.6263671875\n",
      "Train Epoch: 158 [128/17352 (1%)] Loss: -239345.062500\n",
      "Train Epoch: 158 [1536/17352 (9%)] Loss: -241911.671875\n",
      "Train Epoch: 158 [2944/17352 (17%)] Loss: -230247.031250\n",
      "Train Epoch: 158 [4352/17352 (25%)] Loss: -241646.187500\n",
      "Train Epoch: 158 [5760/17352 (33%)] Loss: -237883.640625\n",
      "Train Epoch: 158 [7168/17352 (41%)] Loss: -219000.125000\n",
      "Train Epoch: 158 [8576/17352 (49%)] Loss: -228996.843750\n",
      "Train Epoch: 158 [9984/17352 (58%)] Loss: -225354.500000\n",
      "Train Epoch: 158 [11392/17352 (66%)] Loss: -236678.375000\n",
      "Train Epoch: 158 [12800/17352 (74%)] Loss: -237652.531250\n",
      "Train Epoch: 158 [14208/17352 (82%)] Loss: -239821.390625\n",
      "Train Epoch: 158 [15522/17352 (89%)] Loss: -143120.953125\n",
      "Train Epoch: 158 [16298/17352 (94%)] Loss: -172256.250000\n",
      "Train Epoch: 158 [16991/17352 (98%)] Loss: -71711.148438\n",
      "    epoch          : 158\n",
      "    loss           : -212656.4507524119\n",
      "    val_loss       : -113703.56305338541\n",
      "Train Epoch: 159 [128/17352 (1%)] Loss: -215647.937500\n",
      "Train Epoch: 159 [1536/17352 (9%)] Loss: -244135.609375\n",
      "Train Epoch: 159 [2944/17352 (17%)] Loss: -218086.218750\n",
      "Train Epoch: 159 [4352/17352 (25%)] Loss: -239924.437500\n",
      "Train Epoch: 159 [5760/17352 (33%)] Loss: -236976.171875\n",
      "Train Epoch: 159 [7168/17352 (41%)] Loss: -259958.468750\n",
      "Train Epoch: 159 [8576/17352 (49%)] Loss: -224463.937500\n",
      "Train Epoch: 159 [9984/17352 (58%)] Loss: -239549.375000\n",
      "Train Epoch: 159 [11392/17352 (66%)] Loss: -231892.953125\n",
      "Train Epoch: 159 [12800/17352 (74%)] Loss: -228172.687500\n",
      "Train Epoch: 159 [14208/17352 (82%)] Loss: -229192.843750\n",
      "Train Epoch: 159 [15429/17352 (89%)] Loss: -95052.531250\n",
      "Train Epoch: 159 [16174/17352 (93%)] Loss: -24846.953125\n",
      "Train Epoch: 159 [17019/17352 (98%)] Loss: -158777.218750\n",
      "    epoch          : 159\n",
      "    loss           : -212754.18814557992\n",
      "    val_loss       : -113766.18700358072\n",
      "Train Epoch: 160 [128/17352 (1%)] Loss: -228486.359375\n",
      "Train Epoch: 160 [1536/17352 (9%)] Loss: -242456.031250\n",
      "Train Epoch: 160 [2944/17352 (17%)] Loss: -218227.578125\n",
      "Train Epoch: 160 [4352/17352 (25%)] Loss: -222337.093750\n",
      "Train Epoch: 160 [5760/17352 (33%)] Loss: -237016.125000\n",
      "Train Epoch: 160 [7168/17352 (41%)] Loss: -220156.875000\n",
      "Train Epoch: 160 [8576/17352 (49%)] Loss: -233215.781250\n",
      "Train Epoch: 160 [9984/17352 (58%)] Loss: -241160.390625\n",
      "Train Epoch: 160 [11392/17352 (66%)] Loss: -226234.125000\n",
      "Train Epoch: 160 [12800/17352 (74%)] Loss: -224485.156250\n",
      "Train Epoch: 160 [14208/17352 (82%)] Loss: -232358.781250\n",
      "Train Epoch: 160 [15536/17352 (90%)] Loss: -186189.406250\n",
      "Train Epoch: 160 [16372/17352 (94%)] Loss: -150804.640625\n",
      "Train Epoch: 160 [17029/17352 (98%)] Loss: -201112.046875\n",
      "    epoch          : 160\n",
      "    loss           : -212820.57729918204\n",
      "    val_loss       : -113767.44630533854\n",
      "Validation performance didn't improve for 50 epochs. Training stops.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
