{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='omniglot_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [128/17352 (1%)] Loss: 12556.362305\n",
      "Train Epoch: 1 [1536/17352 (9%)] Loss: 11330.934570\n",
      "Train Epoch: 1 [2944/17352 (17%)] Loss: 4539.952148\n",
      "Train Epoch: 1 [4352/17352 (25%)] Loss: 4938.904297\n",
      "Train Epoch: 1 [5760/17352 (33%)] Loss: -5405.991699\n",
      "Train Epoch: 1 [7168/17352 (41%)] Loss: -6055.822266\n",
      "Train Epoch: 1 [8576/17352 (49%)] Loss: -3515.976807\n",
      "Train Epoch: 1 [9984/17352 (58%)] Loss: -3699.166992\n",
      "Train Epoch: 1 [11392/17352 (66%)] Loss: -11949.865234\n",
      "Train Epoch: 1 [12800/17352 (74%)] Loss: -12269.368164\n",
      "Train Epoch: 1 [14208/17352 (82%)] Loss: -12171.238281\n",
      "Train Epoch: 1 [15616/17352 (90%)] Loss: -11543.471680\n",
      "Train Epoch: 1 [17024/17352 (98%)] Loss: -12704.007812\n",
      "    epoch          : 1\n",
      "    loss           : -4358.008477603688\n",
      "    val_loss       : -10328.759169578552\n",
      "Train Epoch: 2 [128/17352 (1%)] Loss: -13932.857422\n",
      "Train Epoch: 2 [1536/17352 (9%)] Loss: -14605.006836\n",
      "Train Epoch: 2 [2944/17352 (17%)] Loss: -12624.768555\n",
      "Train Epoch: 2 [4352/17352 (25%)] Loss: -13381.583984\n",
      "Train Epoch: 2 [5760/17352 (33%)] Loss: -14613.110352\n",
      "Train Epoch: 2 [7168/17352 (41%)] Loss: -15348.222656\n",
      "Train Epoch: 2 [8576/17352 (49%)] Loss: -14548.965820\n",
      "Train Epoch: 2 [9984/17352 (58%)] Loss: -16329.625000\n",
      "Train Epoch: 2 [11392/17352 (66%)] Loss: -16348.876953\n",
      "Train Epoch: 2 [12800/17352 (74%)] Loss: -16215.726562\n",
      "Train Epoch: 2 [14208/17352 (82%)] Loss: -13464.106445\n",
      "Train Epoch: 2 [15616/17352 (90%)] Loss: -19963.337891\n",
      "Train Epoch: 2 [17024/17352 (98%)] Loss: -16821.394531\n",
      "    epoch          : 2\n",
      "    loss           : -14432.437313304228\n",
      "    val_loss       : -15707.20268535614\n",
      "Train Epoch: 3 [128/17352 (1%)] Loss: -18632.753906\n",
      "Train Epoch: 3 [1536/17352 (9%)] Loss: -18638.832031\n",
      "Train Epoch: 3 [2944/17352 (17%)] Loss: -18333.132812\n",
      "Train Epoch: 3 [4352/17352 (25%)] Loss: -18582.812500\n",
      "Train Epoch: 3 [5760/17352 (33%)] Loss: -19563.105469\n",
      "Train Epoch: 3 [7168/17352 (41%)] Loss: -17700.433594\n",
      "Train Epoch: 3 [8576/17352 (49%)] Loss: -20073.628906\n",
      "Train Epoch: 3 [9984/17352 (58%)] Loss: -16040.038086\n",
      "Train Epoch: 3 [11392/17352 (66%)] Loss: -20948.767578\n",
      "Train Epoch: 3 [12800/17352 (74%)] Loss: -19365.015625\n",
      "Train Epoch: 3 [14208/17352 (82%)] Loss: -19516.609375\n",
      "Train Epoch: 3 [15616/17352 (90%)] Loss: -18628.187500\n",
      "Train Epoch: 3 [17024/17352 (98%)] Loss: -21971.275391\n",
      "    epoch          : 3\n",
      "    loss           : -19120.075733857997\n",
      "    val_loss       : -20420.565328598022\n",
      "Train Epoch: 4 [128/17352 (1%)] Loss: -18169.755859\n",
      "Train Epoch: 4 [1536/17352 (9%)] Loss: -20369.304688\n",
      "Train Epoch: 4 [2944/17352 (17%)] Loss: -23220.390625\n",
      "Train Epoch: 4 [4352/17352 (25%)] Loss: -21636.972656\n",
      "Train Epoch: 4 [5760/17352 (33%)] Loss: -22729.275391\n",
      "Train Epoch: 4 [7168/17352 (41%)] Loss: -23797.525391\n",
      "Train Epoch: 4 [8576/17352 (49%)] Loss: -23155.939453\n",
      "Train Epoch: 4 [9984/17352 (58%)] Loss: -22781.751953\n",
      "Train Epoch: 4 [11392/17352 (66%)] Loss: -23724.605469\n",
      "Train Epoch: 4 [12800/17352 (74%)] Loss: -24358.416016\n",
      "Train Epoch: 4 [14208/17352 (82%)] Loss: -21110.785156\n",
      "Train Epoch: 4 [15616/17352 (90%)] Loss: -24739.339844\n",
      "Train Epoch: 4 [17024/17352 (98%)] Loss: -25982.664062\n",
      "    epoch          : 4\n",
      "    loss           : -22901.081930721506\n",
      "    val_loss       : -23704.562244415283\n",
      "Train Epoch: 5 [128/17352 (1%)] Loss: -20986.968750\n",
      "Train Epoch: 5 [1536/17352 (9%)] Loss: -20483.701172\n",
      "Train Epoch: 5 [2944/17352 (17%)] Loss: -22186.697266\n",
      "Train Epoch: 5 [4352/17352 (25%)] Loss: -26846.289062\n",
      "Train Epoch: 5 [5760/17352 (33%)] Loss: -25785.585938\n",
      "Train Epoch: 5 [7168/17352 (41%)] Loss: -25997.267578\n",
      "Train Epoch: 5 [8576/17352 (49%)] Loss: -26148.703125\n",
      "Train Epoch: 5 [9984/17352 (58%)] Loss: -25350.777344\n",
      "Train Epoch: 5 [11392/17352 (66%)] Loss: -28585.623047\n",
      "Train Epoch: 5 [12800/17352 (74%)] Loss: -26572.765625\n",
      "Train Epoch: 5 [14208/17352 (82%)] Loss: -29290.265625\n",
      "Train Epoch: 5 [15616/17352 (90%)] Loss: -28219.001953\n",
      "Train Epoch: 5 [17024/17352 (98%)] Loss: -28412.490234\n",
      "    epoch          : 5\n",
      "    loss           : -26019.51314051011\n",
      "    val_loss       : -26174.15549468994\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0703_171824/checkpoint-epoch5.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 6 [128/17352 (1%)] Loss: -29492.921875\n",
      "Train Epoch: 6 [1536/17352 (9%)] Loss: -29342.513672\n",
      "Train Epoch: 6 [2944/17352 (17%)] Loss: -29721.664062\n",
      "Train Epoch: 6 [4352/17352 (25%)] Loss: -28966.876953\n",
      "Train Epoch: 6 [5760/17352 (33%)] Loss: -26757.535156\n",
      "Train Epoch: 6 [7168/17352 (41%)] Loss: -27983.519531\n",
      "Train Epoch: 6 [8576/17352 (49%)] Loss: -30229.878906\n",
      "Train Epoch: 6 [9984/17352 (58%)] Loss: -27765.345703\n",
      "Train Epoch: 6 [11392/17352 (66%)] Loss: -29851.808594\n",
      "Train Epoch: 6 [12800/17352 (74%)] Loss: -29416.718750\n",
      "Train Epoch: 6 [14208/17352 (82%)] Loss: -30914.082031\n",
      "Train Epoch: 6 [15616/17352 (90%)] Loss: -30952.392578\n",
      "Train Epoch: 6 [17024/17352 (98%)] Loss: -29264.121094\n",
      "    epoch          : 6\n",
      "    loss           : -29082.77800436581\n",
      "    val_loss       : -28575.445755958557\n",
      "Train Epoch: 7 [128/17352 (1%)] Loss: -31062.644531\n",
      "Train Epoch: 7 [1536/17352 (9%)] Loss: -31305.023438\n",
      "Train Epoch: 7 [2944/17352 (17%)] Loss: -30943.236328\n",
      "Train Epoch: 7 [4352/17352 (25%)] Loss: -30723.556641\n",
      "Train Epoch: 7 [5760/17352 (33%)] Loss: -30908.203125\n",
      "Train Epoch: 7 [7168/17352 (41%)] Loss: -29121.492188\n",
      "Train Epoch: 7 [8576/17352 (49%)] Loss: -28479.943359\n",
      "Train Epoch: 7 [9984/17352 (58%)] Loss: -31321.271484\n",
      "Train Epoch: 7 [11392/17352 (66%)] Loss: -31822.794922\n",
      "Train Epoch: 7 [12800/17352 (74%)] Loss: -32939.617188\n",
      "Train Epoch: 7 [14208/17352 (82%)] Loss: -32392.503906\n",
      "Train Epoch: 7 [15616/17352 (90%)] Loss: -32872.343750\n",
      "Train Epoch: 7 [17024/17352 (98%)] Loss: -32236.648438\n",
      "    epoch          : 7\n",
      "    loss           : -31362.384219898897\n",
      "    val_loss       : -30562.99524497986\n",
      "Train Epoch: 8 [128/17352 (1%)] Loss: -30752.695312\n",
      "Train Epoch: 8 [1536/17352 (9%)] Loss: -32903.421875\n",
      "Train Epoch: 8 [2944/17352 (17%)] Loss: -33019.996094\n",
      "Train Epoch: 8 [4352/17352 (25%)] Loss: -33414.550781\n",
      "Train Epoch: 8 [5760/17352 (33%)] Loss: -32488.701172\n",
      "Train Epoch: 8 [7168/17352 (41%)] Loss: -34040.250000\n",
      "Train Epoch: 8 [8576/17352 (49%)] Loss: -33226.789062\n",
      "Train Epoch: 8 [9984/17352 (58%)] Loss: -34001.988281\n",
      "Train Epoch: 8 [11392/17352 (66%)] Loss: -34366.496094\n",
      "Train Epoch: 8 [12800/17352 (74%)] Loss: -32864.210938\n",
      "Train Epoch: 8 [14208/17352 (82%)] Loss: -34597.550781\n",
      "Train Epoch: 8 [15616/17352 (90%)] Loss: -34589.007812\n",
      "Train Epoch: 8 [17024/17352 (98%)] Loss: -35150.320312\n",
      "    epoch          : 8\n",
      "    loss           : -33683.4116785386\n",
      "    val_loss       : -32886.971952438354\n",
      "Train Epoch: 9 [128/17352 (1%)] Loss: -36538.894531\n",
      "Train Epoch: 9 [1536/17352 (9%)] Loss: -35371.414062\n",
      "Train Epoch: 9 [2944/17352 (17%)] Loss: -35135.601562\n",
      "Train Epoch: 9 [4352/17352 (25%)] Loss: -34206.585938\n",
      "Train Epoch: 9 [5760/17352 (33%)] Loss: -35638.414062\n",
      "Train Epoch: 9 [7168/17352 (41%)] Loss: -34850.562500\n",
      "Train Epoch: 9 [8576/17352 (49%)] Loss: -35954.292969\n",
      "Train Epoch: 9 [9984/17352 (58%)] Loss: -34749.609375\n",
      "Train Epoch: 9 [11392/17352 (66%)] Loss: -36758.308594\n",
      "Train Epoch: 9 [12800/17352 (74%)] Loss: -37437.042969\n",
      "Train Epoch: 9 [14208/17352 (82%)] Loss: -36833.199219\n",
      "Train Epoch: 9 [15616/17352 (90%)] Loss: -37834.074219\n",
      "Train Epoch: 9 [17024/17352 (98%)] Loss: -35556.074219\n",
      "    epoch          : 9\n",
      "    loss           : -35784.16607306985\n",
      "    val_loss       : -34825.66396713257\n",
      "Train Epoch: 10 [128/17352 (1%)] Loss: -38701.707031\n",
      "Train Epoch: 10 [1536/17352 (9%)] Loss: -37328.492188\n",
      "Train Epoch: 10 [2944/17352 (17%)] Loss: -37736.460938\n",
      "Train Epoch: 10 [4352/17352 (25%)] Loss: -36959.105469\n",
      "Train Epoch: 10 [5760/17352 (33%)] Loss: -36074.156250\n",
      "Train Epoch: 10 [7168/17352 (41%)] Loss: -37414.957031\n",
      "Train Epoch: 10 [8576/17352 (49%)] Loss: -38044.023438\n",
      "Train Epoch: 10 [9984/17352 (58%)] Loss: -39473.359375\n",
      "Train Epoch: 10 [11392/17352 (66%)] Loss: -37226.046875\n",
      "Train Epoch: 10 [12800/17352 (74%)] Loss: -39551.585938\n",
      "Train Epoch: 10 [14208/17352 (82%)] Loss: -38740.023438\n",
      "Train Epoch: 10 [15616/17352 (90%)] Loss: -39443.625000\n",
      "Train Epoch: 10 [17024/17352 (98%)] Loss: -38201.691406\n",
      "    epoch          : 10\n",
      "    loss           : -38009.07332835478\n",
      "    val_loss       : -36824.87534236908\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0703_171824/checkpoint-epoch10.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 11 [128/17352 (1%)] Loss: -39674.621094\n",
      "Train Epoch: 11 [1536/17352 (9%)] Loss: -38786.222656\n",
      "Train Epoch: 11 [2944/17352 (17%)] Loss: -39505.886719\n",
      "Train Epoch: 11 [4352/17352 (25%)] Loss: -38653.023438\n",
      "Train Epoch: 11 [5760/17352 (33%)] Loss: -36898.726562\n",
      "Train Epoch: 11 [7168/17352 (41%)] Loss: -39304.441406\n",
      "Train Epoch: 11 [8576/17352 (49%)] Loss: -39351.871094\n",
      "Train Epoch: 11 [9984/17352 (58%)] Loss: -41444.359375\n",
      "Train Epoch: 11 [11392/17352 (66%)] Loss: -40292.359375\n",
      "Train Epoch: 11 [12800/17352 (74%)] Loss: -39256.800781\n",
      "Train Epoch: 11 [14208/17352 (82%)] Loss: -41991.445312\n",
      "Train Epoch: 11 [15616/17352 (90%)] Loss: -40280.238281\n",
      "Train Epoch: 11 [17024/17352 (98%)] Loss: -41010.210938\n",
      "    epoch          : 11\n",
      "    loss           : -39993.446174172794\n",
      "    val_loss       : -38846.27278137207\n",
      "Train Epoch: 12 [128/17352 (1%)] Loss: -41238.976562\n",
      "Train Epoch: 12 [1536/17352 (9%)] Loss: -41863.761719\n",
      "Train Epoch: 12 [2944/17352 (17%)] Loss: -41184.285156\n",
      "Train Epoch: 12 [4352/17352 (25%)] Loss: -41275.656250\n",
      "Train Epoch: 12 [5760/17352 (33%)] Loss: -42438.171875\n",
      "Train Epoch: 12 [7168/17352 (41%)] Loss: -42096.535156\n",
      "Train Epoch: 12 [8576/17352 (49%)] Loss: -42895.320312\n",
      "Train Epoch: 12 [9984/17352 (58%)] Loss: -37610.093750\n",
      "Train Epoch: 12 [11392/17352 (66%)] Loss: -41301.257812\n",
      "Train Epoch: 12 [12800/17352 (74%)] Loss: -44242.074219\n",
      "Train Epoch: 12 [14208/17352 (82%)] Loss: -41078.226562\n",
      "Train Epoch: 12 [15616/17352 (90%)] Loss: -42933.316406\n",
      "Train Epoch: 12 [17024/17352 (98%)] Loss: -43861.429688\n",
      "    epoch          : 12\n",
      "    loss           : -42013.01742015166\n",
      "    val_loss       : -40920.24461841583\n",
      "Train Epoch: 13 [128/17352 (1%)] Loss: -42933.031250\n",
      "Train Epoch: 13 [1536/17352 (9%)] Loss: -42107.730469\n",
      "Train Epoch: 13 [2944/17352 (17%)] Loss: -43799.375000\n",
      "Train Epoch: 13 [4352/17352 (25%)] Loss: -44070.281250\n",
      "Train Epoch: 13 [5760/17352 (33%)] Loss: -43882.406250\n",
      "Train Epoch: 13 [7168/17352 (41%)] Loss: -44233.476562\n",
      "Train Epoch: 13 [8576/17352 (49%)] Loss: -44219.597656\n",
      "Train Epoch: 13 [9984/17352 (58%)] Loss: -45231.765625\n",
      "Train Epoch: 13 [11392/17352 (66%)] Loss: -43857.015625\n",
      "Train Epoch: 13 [12800/17352 (74%)] Loss: -45411.218750\n",
      "Train Epoch: 13 [14208/17352 (82%)] Loss: -45201.710938\n",
      "Train Epoch: 13 [15616/17352 (90%)] Loss: -44876.296875\n",
      "Train Epoch: 13 [17024/17352 (98%)] Loss: -46641.636719\n",
      "    epoch          : 13\n",
      "    loss           : -44092.39405732996\n",
      "    val_loss       : -42667.2607793808\n",
      "Train Epoch: 14 [128/17352 (1%)] Loss: -45601.796875\n",
      "Train Epoch: 14 [1536/17352 (9%)] Loss: -45553.562500\n",
      "Train Epoch: 14 [2944/17352 (17%)] Loss: -45755.804688\n",
      "Train Epoch: 14 [4352/17352 (25%)] Loss: -45497.226562\n",
      "Train Epoch: 14 [5760/17352 (33%)] Loss: -44514.843750\n",
      "Train Epoch: 14 [7168/17352 (41%)] Loss: -45449.453125\n",
      "Train Epoch: 14 [8576/17352 (49%)] Loss: -45617.394531\n",
      "Train Epoch: 14 [9984/17352 (58%)] Loss: -46312.500000\n",
      "Train Epoch: 14 [11392/17352 (66%)] Loss: -45786.554688\n",
      "Train Epoch: 14 [12800/17352 (74%)] Loss: -46329.363281\n",
      "Train Epoch: 14 [14208/17352 (82%)] Loss: -47643.402344\n",
      "Train Epoch: 14 [15616/17352 (90%)] Loss: -47034.359375\n",
      "Train Epoch: 14 [17024/17352 (98%)] Loss: -47108.429688\n",
      "    epoch          : 14\n",
      "    loss           : -46097.21044921875\n",
      "    val_loss       : -44700.294818878174\n",
      "Train Epoch: 15 [128/17352 (1%)] Loss: -46788.046875\n",
      "Train Epoch: 15 [1536/17352 (9%)] Loss: -47722.949219\n",
      "Train Epoch: 15 [2944/17352 (17%)] Loss: -47745.917969\n",
      "Train Epoch: 15 [4352/17352 (25%)] Loss: -48872.699219\n",
      "Train Epoch: 15 [5760/17352 (33%)] Loss: -48097.386719\n",
      "Train Epoch: 15 [7168/17352 (41%)] Loss: -48023.726562\n",
      "Train Epoch: 15 [8576/17352 (49%)] Loss: -48292.007812\n",
      "Train Epoch: 15 [9984/17352 (58%)] Loss: -48827.960938\n",
      "Train Epoch: 15 [11392/17352 (66%)] Loss: -48850.046875\n",
      "Train Epoch: 15 [12800/17352 (74%)] Loss: -48371.187500\n",
      "Train Epoch: 15 [14208/17352 (82%)] Loss: -48169.656250\n",
      "Train Epoch: 15 [15616/17352 (90%)] Loss: -48515.667969\n",
      "Train Epoch: 15 [17024/17352 (98%)] Loss: -49665.484375\n",
      "    epoch          : 15\n",
      "    loss           : -48202.96436982996\n",
      "    val_loss       : -46656.55718803406\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0703_171824/checkpoint-epoch15.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 16 [128/17352 (1%)] Loss: -48676.214844\n",
      "Train Epoch: 16 [1536/17352 (9%)] Loss: -49901.015625\n",
      "Train Epoch: 16 [2944/17352 (17%)] Loss: -49045.035156\n",
      "Train Epoch: 16 [4352/17352 (25%)] Loss: -48413.554688\n",
      "Train Epoch: 16 [5760/17352 (33%)] Loss: -50397.234375\n",
      "Train Epoch: 16 [7168/17352 (41%)] Loss: -50470.519531\n",
      "Train Epoch: 16 [8576/17352 (49%)] Loss: -50947.726562\n",
      "Train Epoch: 16 [9984/17352 (58%)] Loss: -51197.996094\n",
      "Train Epoch: 16 [11392/17352 (66%)] Loss: -49590.351562\n",
      "Train Epoch: 16 [12800/17352 (74%)] Loss: -51606.753906\n",
      "Train Epoch: 16 [14208/17352 (82%)] Loss: -50792.277344\n",
      "Train Epoch: 16 [15616/17352 (90%)] Loss: -50895.136719\n",
      "Train Epoch: 16 [17024/17352 (98%)] Loss: -51258.656250\n",
      "    epoch          : 16\n",
      "    loss           : -50180.84932215074\n",
      "    val_loss       : -48459.97123813629\n",
      "Train Epoch: 17 [128/17352 (1%)] Loss: -49816.757812\n",
      "Train Epoch: 17 [1536/17352 (9%)] Loss: -51433.367188\n",
      "Train Epoch: 17 [2944/17352 (17%)] Loss: -52585.093750\n",
      "Train Epoch: 17 [4352/17352 (25%)] Loss: -51236.238281\n",
      "Train Epoch: 17 [5760/17352 (33%)] Loss: -52465.578125\n",
      "Train Epoch: 17 [7168/17352 (41%)] Loss: -52793.535156\n",
      "Train Epoch: 17 [8576/17352 (49%)] Loss: -51250.410156\n",
      "Train Epoch: 17 [9984/17352 (58%)] Loss: -53649.539062\n",
      "Train Epoch: 17 [11392/17352 (66%)] Loss: -52744.023438\n",
      "Train Epoch: 17 [12800/17352 (74%)] Loss: -53747.515625\n",
      "Train Epoch: 17 [14208/17352 (82%)] Loss: -53754.363281\n",
      "Train Epoch: 17 [15616/17352 (90%)] Loss: -53408.167969\n",
      "Train Epoch: 17 [17024/17352 (98%)] Loss: -53506.375000\n",
      "    epoch          : 17\n",
      "    loss           : -52246.57360121783\n",
      "    val_loss       : -50387.575320243835\n",
      "Train Epoch: 18 [128/17352 (1%)] Loss: -52901.492188\n",
      "Train Epoch: 18 [1536/17352 (9%)] Loss: -54374.058594\n",
      "Train Epoch: 18 [2944/17352 (17%)] Loss: -53927.554688\n",
      "Train Epoch: 18 [4352/17352 (25%)] Loss: -54738.695312\n",
      "Train Epoch: 18 [5760/17352 (33%)] Loss: -53787.960938\n",
      "Train Epoch: 18 [7168/17352 (41%)] Loss: -55481.585938\n",
      "Train Epoch: 18 [8576/17352 (49%)] Loss: -54683.671875\n",
      "Train Epoch: 18 [9984/17352 (58%)] Loss: -55705.273438\n",
      "Train Epoch: 18 [11392/17352 (66%)] Loss: -55407.941406\n",
      "Train Epoch: 18 [12800/17352 (74%)] Loss: -55143.261719\n",
      "Train Epoch: 18 [14208/17352 (82%)] Loss: -54640.343750\n",
      "Train Epoch: 18 [15616/17352 (90%)] Loss: -56347.097656\n",
      "Train Epoch: 18 [17024/17352 (98%)] Loss: -56311.898438\n",
      "    epoch          : 18\n",
      "    loss           : -54370.115550321694\n",
      "    val_loss       : -52346.22112560272\n",
      "Train Epoch: 19 [128/17352 (1%)] Loss: -55847.804688\n",
      "Train Epoch: 19 [1536/17352 (9%)] Loss: -55121.976562\n",
      "Train Epoch: 19 [2944/17352 (17%)] Loss: -55575.472656\n",
      "Train Epoch: 19 [4352/17352 (25%)] Loss: -56211.015625\n",
      "Train Epoch: 19 [5760/17352 (33%)] Loss: -56550.109375\n",
      "Train Epoch: 19 [7168/17352 (41%)] Loss: -56528.531250\n",
      "Train Epoch: 19 [8576/17352 (49%)] Loss: -56528.691406\n",
      "Train Epoch: 19 [9984/17352 (58%)] Loss: -56919.300781\n",
      "Train Epoch: 19 [11392/17352 (66%)] Loss: -57546.765625\n",
      "Train Epoch: 19 [12800/17352 (74%)] Loss: -57944.906250\n",
      "Train Epoch: 19 [14208/17352 (82%)] Loss: -57480.390625\n",
      "Train Epoch: 19 [15616/17352 (90%)] Loss: -56564.117188\n",
      "Train Epoch: 19 [17024/17352 (98%)] Loss: -57559.175781\n",
      "    epoch          : 19\n",
      "    loss           : -56461.24185719209\n",
      "    val_loss       : -54357.15491104126\n",
      "Train Epoch: 20 [128/17352 (1%)] Loss: -57000.406250\n",
      "Train Epoch: 20 [1536/17352 (9%)] Loss: -57292.890625\n",
      "Train Epoch: 20 [2944/17352 (17%)] Loss: -58200.933594\n",
      "Train Epoch: 20 [4352/17352 (25%)] Loss: -58115.414062\n",
      "Train Epoch: 20 [5760/17352 (33%)] Loss: -58863.824219\n",
      "Train Epoch: 20 [7168/17352 (41%)] Loss: -59150.437500\n",
      "Train Epoch: 20 [8576/17352 (49%)] Loss: -58378.476562\n",
      "Train Epoch: 20 [9984/17352 (58%)] Loss: -57241.585938\n",
      "Train Epoch: 20 [11392/17352 (66%)] Loss: -59009.800781\n",
      "Train Epoch: 20 [12800/17352 (74%)] Loss: -59158.046875\n",
      "Train Epoch: 20 [14208/17352 (82%)] Loss: -59291.218750\n",
      "Train Epoch: 20 [15616/17352 (90%)] Loss: -59415.160156\n",
      "Train Epoch: 20 [17024/17352 (98%)] Loss: -59188.203125\n",
      "    epoch          : 20\n",
      "    loss           : -58631.3081629136\n",
      "    val_loss       : -56213.188903808594\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0703_171824/checkpoint-epoch20.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 21 [128/17352 (1%)] Loss: -59958.359375\n",
      "Train Epoch: 21 [1536/17352 (9%)] Loss: -60707.566406\n",
      "Train Epoch: 21 [2944/17352 (17%)] Loss: -59947.531250\n",
      "Train Epoch: 21 [4352/17352 (25%)] Loss: -60294.648438\n",
      "Train Epoch: 21 [5760/17352 (33%)] Loss: -60585.953125\n",
      "Train Epoch: 21 [7168/17352 (41%)] Loss: -60768.550781\n",
      "Train Epoch: 21 [8576/17352 (49%)] Loss: -60165.050781\n",
      "Train Epoch: 21 [9984/17352 (58%)] Loss: -61121.574219\n",
      "Train Epoch: 21 [11392/17352 (66%)] Loss: -60696.660156\n",
      "Train Epoch: 21 [12800/17352 (74%)] Loss: -60240.410156\n",
      "Train Epoch: 21 [14208/17352 (82%)] Loss: -61467.269531\n",
      "Train Epoch: 21 [15616/17352 (90%)] Loss: -61458.878906\n",
      "Train Epoch: 21 [17024/17352 (98%)] Loss: -61947.031250\n",
      "    epoch          : 21\n",
      "    loss           : -60792.82838350184\n",
      "    val_loss       : -58142.91693210602\n",
      "Train Epoch: 22 [128/17352 (1%)] Loss: -61960.511719\n",
      "Train Epoch: 22 [1536/17352 (9%)] Loss: -62401.289062\n",
      "Train Epoch: 22 [2944/17352 (17%)] Loss: -62276.648438\n",
      "Train Epoch: 22 [4352/17352 (25%)] Loss: -62689.050781\n",
      "Train Epoch: 22 [5760/17352 (33%)] Loss: -63065.546875\n",
      "Train Epoch: 22 [7168/17352 (41%)] Loss: -62569.804688\n",
      "Train Epoch: 22 [8576/17352 (49%)] Loss: -62767.433594\n",
      "Train Epoch: 22 [9984/17352 (58%)] Loss: -64511.476562\n",
      "Train Epoch: 22 [11392/17352 (66%)] Loss: -63121.593750\n",
      "Train Epoch: 22 [12800/17352 (74%)] Loss: -64235.523438\n",
      "Train Epoch: 22 [14208/17352 (82%)] Loss: -62678.085938\n",
      "Train Epoch: 22 [15616/17352 (90%)] Loss: -65240.183594\n",
      "Train Epoch: 22 [17024/17352 (98%)] Loss: -63709.054688\n",
      "    epoch          : 22\n",
      "    loss           : -62929.63643152574\n",
      "    val_loss       : -60229.09977054596\n",
      "Train Epoch: 23 [128/17352 (1%)] Loss: -64153.867188\n",
      "Train Epoch: 23 [1536/17352 (9%)] Loss: -63861.214844\n",
      "Train Epoch: 23 [2944/17352 (17%)] Loss: -63575.281250\n",
      "Train Epoch: 23 [4352/17352 (25%)] Loss: -65114.570312\n",
      "Train Epoch: 23 [5760/17352 (33%)] Loss: -64059.171875\n",
      "Train Epoch: 23 [7168/17352 (41%)] Loss: -64151.250000\n",
      "Train Epoch: 23 [8576/17352 (49%)] Loss: -65154.890625\n",
      "Train Epoch: 23 [9984/17352 (58%)] Loss: -65110.617188\n",
      "Train Epoch: 23 [11392/17352 (66%)] Loss: -65308.773438\n",
      "Train Epoch: 23 [12800/17352 (74%)] Loss: -64913.000000\n",
      "Train Epoch: 23 [14208/17352 (82%)] Loss: -65420.996094\n",
      "Train Epoch: 23 [15616/17352 (90%)] Loss: -66855.382812\n",
      "Train Epoch: 23 [17024/17352 (98%)] Loss: -66246.000000\n",
      "    epoch          : 23\n",
      "    loss           : -65043.4838292739\n",
      "    val_loss       : -62491.12645530701\n",
      "Train Epoch: 24 [128/17352 (1%)] Loss: -67846.703125\n",
      "Train Epoch: 24 [1536/17352 (9%)] Loss: -66410.460938\n",
      "Train Epoch: 24 [2944/17352 (17%)] Loss: -66632.718750\n",
      "Train Epoch: 24 [4352/17352 (25%)] Loss: -67212.796875\n",
      "Train Epoch: 24 [5760/17352 (33%)] Loss: -66606.031250\n",
      "Train Epoch: 24 [7168/17352 (41%)] Loss: -66112.562500\n",
      "Train Epoch: 24 [8576/17352 (49%)] Loss: -67878.781250\n",
      "Train Epoch: 24 [9984/17352 (58%)] Loss: -67143.359375\n",
      "Train Epoch: 24 [11392/17352 (66%)] Loss: -67426.578125\n",
      "Train Epoch: 24 [12800/17352 (74%)] Loss: -67395.375000\n",
      "Train Epoch: 24 [14208/17352 (82%)] Loss: -68491.187500\n",
      "Train Epoch: 24 [15616/17352 (90%)] Loss: -68930.078125\n",
      "Train Epoch: 24 [17024/17352 (98%)] Loss: -69123.687500\n",
      "    epoch          : 24\n",
      "    loss           : -67222.62856158089\n",
      "    val_loss       : -64462.033761024475\n",
      "Train Epoch: 25 [128/17352 (1%)] Loss: -69756.398438\n",
      "Train Epoch: 25 [1536/17352 (9%)] Loss: -70340.703125\n",
      "Train Epoch: 25 [2944/17352 (17%)] Loss: -68871.539062\n",
      "Train Epoch: 25 [4352/17352 (25%)] Loss: -68786.242188\n",
      "Train Epoch: 25 [5760/17352 (33%)] Loss: -67593.898438\n",
      "Train Epoch: 25 [7168/17352 (41%)] Loss: -68933.414062\n",
      "Train Epoch: 25 [8576/17352 (49%)] Loss: -70370.031250\n",
      "Train Epoch: 25 [9984/17352 (58%)] Loss: -70534.835938\n",
      "Train Epoch: 25 [11392/17352 (66%)] Loss: -68977.953125\n",
      "Train Epoch: 25 [12800/17352 (74%)] Loss: -69579.515625\n",
      "Train Epoch: 25 [14208/17352 (82%)] Loss: -68921.031250\n",
      "Train Epoch: 25 [15616/17352 (90%)] Loss: -71957.437500\n",
      "Train Epoch: 25 [17024/17352 (98%)] Loss: -70120.171875\n",
      "    epoch          : 25\n",
      "    loss           : -69436.89912683824\n",
      "    val_loss       : -66483.35015106201\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0703_171824/checkpoint-epoch25.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 26 [128/17352 (1%)] Loss: -70635.804688\n",
      "Train Epoch: 26 [1536/17352 (9%)] Loss: -72981.625000\n",
      "Train Epoch: 26 [2944/17352 (17%)] Loss: -72146.210938\n",
      "Train Epoch: 26 [4352/17352 (25%)] Loss: -70637.742188\n",
      "Train Epoch: 26 [5760/17352 (33%)] Loss: -71524.953125\n",
      "Train Epoch: 26 [7168/17352 (41%)] Loss: -73639.000000\n",
      "Train Epoch: 26 [8576/17352 (49%)] Loss: -72328.687500\n",
      "Train Epoch: 26 [9984/17352 (58%)] Loss: -73905.117188\n",
      "Train Epoch: 26 [11392/17352 (66%)] Loss: -74199.578125\n",
      "Train Epoch: 26 [12800/17352 (74%)] Loss: -70318.468750\n",
      "Train Epoch: 26 [14208/17352 (82%)] Loss: -71728.937500\n",
      "Train Epoch: 26 [15616/17352 (90%)] Loss: -72638.429688\n",
      "Train Epoch: 26 [17024/17352 (98%)] Loss: -73135.453125\n",
      "    epoch          : 26\n",
      "    loss           : -71679.99609375\n",
      "    val_loss       : -69249.73500347137\n",
      "Train Epoch: 27 [128/17352 (1%)] Loss: -73552.164062\n",
      "Train Epoch: 27 [1536/17352 (9%)] Loss: -73943.070312\n",
      "Train Epoch: 27 [2944/17352 (17%)] Loss: -73689.796875\n",
      "Train Epoch: 27 [4352/17352 (25%)] Loss: -72639.125000\n",
      "Train Epoch: 27 [5760/17352 (33%)] Loss: -72969.531250\n",
      "Train Epoch: 27 [7168/17352 (41%)] Loss: -72298.945312\n",
      "Train Epoch: 27 [8576/17352 (49%)] Loss: -74634.289062\n",
      "Train Epoch: 27 [9984/17352 (58%)] Loss: -73751.437500\n",
      "Train Epoch: 27 [11392/17352 (66%)] Loss: -74499.593750\n",
      "Train Epoch: 27 [12800/17352 (74%)] Loss: -75711.289062\n",
      "Train Epoch: 27 [14208/17352 (82%)] Loss: -75027.492188\n",
      "Train Epoch: 27 [15616/17352 (90%)] Loss: -75691.843750\n",
      "Train Epoch: 27 [17024/17352 (98%)] Loss: -77364.960938\n",
      "    epoch          : 27\n",
      "    loss           : -73726.83211741727\n",
      "    val_loss       : -70411.19289207458\n",
      "Train Epoch: 28 [128/17352 (1%)] Loss: -75371.875000\n",
      "Train Epoch: 28 [1536/17352 (9%)] Loss: -75055.085938\n",
      "Train Epoch: 28 [2944/17352 (17%)] Loss: -74720.437500\n",
      "Train Epoch: 28 [4352/17352 (25%)] Loss: -74062.554688\n",
      "Train Epoch: 28 [5760/17352 (33%)] Loss: -76775.203125\n",
      "Train Epoch: 28 [7168/17352 (41%)] Loss: -75771.062500\n",
      "Train Epoch: 28 [8576/17352 (49%)] Loss: -75870.531250\n",
      "Train Epoch: 28 [9984/17352 (58%)] Loss: -76111.703125\n",
      "Train Epoch: 28 [11392/17352 (66%)] Loss: -78225.375000\n",
      "Train Epoch: 28 [12800/17352 (74%)] Loss: -75193.312500\n",
      "Train Epoch: 28 [14208/17352 (82%)] Loss: -76942.765625\n",
      "Train Epoch: 28 [15616/17352 (90%)] Loss: -77734.468750\n",
      "Train Epoch: 28 [17024/17352 (98%)] Loss: -78832.507812\n",
      "    epoch          : 28\n",
      "    loss           : -75902.97196691176\n",
      "    val_loss       : -73277.7565536499\n",
      "Train Epoch: 29 [128/17352 (1%)] Loss: -76765.781250\n",
      "Train Epoch: 29 [1536/17352 (9%)] Loss: -76309.734375\n",
      "Train Epoch: 29 [2944/17352 (17%)] Loss: -75236.687500\n",
      "Train Epoch: 29 [4352/17352 (25%)] Loss: -77579.617188\n",
      "Train Epoch: 29 [5760/17352 (33%)] Loss: -79942.273438\n",
      "Train Epoch: 29 [7168/17352 (41%)] Loss: -78840.164062\n",
      "Train Epoch: 29 [8576/17352 (49%)] Loss: -78483.976562\n",
      "Train Epoch: 29 [9984/17352 (58%)] Loss: -76249.523438\n",
      "Train Epoch: 29 [11392/17352 (66%)] Loss: -80693.921875\n",
      "Train Epoch: 29 [12800/17352 (74%)] Loss: -78530.226562\n",
      "Train Epoch: 29 [14208/17352 (82%)] Loss: -79445.531250\n",
      "Train Epoch: 29 [15616/17352 (90%)] Loss: -78934.351562\n",
      "Train Epoch: 29 [17024/17352 (98%)] Loss: -81521.882812\n",
      "    epoch          : 29\n",
      "    loss           : -78196.69536994485\n",
      "    val_loss       : -74757.58545017242\n",
      "Train Epoch: 30 [128/17352 (1%)] Loss: -81502.562500\n",
      "Train Epoch: 30 [1536/17352 (9%)] Loss: -81193.625000\n",
      "Train Epoch: 30 [2944/17352 (17%)] Loss: -79317.757812\n",
      "Train Epoch: 30 [4352/17352 (25%)] Loss: -79302.328125\n",
      "Train Epoch: 30 [5760/17352 (33%)] Loss: -78343.632812\n",
      "Train Epoch: 30 [7168/17352 (41%)] Loss: -80079.140625\n",
      "Train Epoch: 30 [8576/17352 (49%)] Loss: -81901.929688\n",
      "Train Epoch: 30 [9984/17352 (58%)] Loss: -80194.320312\n",
      "Train Epoch: 30 [11392/17352 (66%)] Loss: -81755.078125\n",
      "Train Epoch: 30 [12800/17352 (74%)] Loss: -80124.062500\n",
      "Train Epoch: 30 [14208/17352 (82%)] Loss: -79764.031250\n",
      "Train Epoch: 30 [15616/17352 (90%)] Loss: -82016.078125\n",
      "Train Epoch: 30 [17024/17352 (98%)] Loss: -77621.812500\n",
      "    epoch          : 30\n",
      "    loss           : -80207.88396139706\n",
      "    val_loss       : -76571.65277671814\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0703_171824/checkpoint-epoch30.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 31 [128/17352 (1%)] Loss: -82644.765625\n",
      "Train Epoch: 31 [1536/17352 (9%)] Loss: -80163.742188\n",
      "Train Epoch: 31 [2944/17352 (17%)] Loss: -79692.898438\n",
      "Train Epoch: 31 [4352/17352 (25%)] Loss: -81529.359375\n",
      "Train Epoch: 31 [5760/17352 (33%)] Loss: -81251.898438\n",
      "Train Epoch: 31 [7168/17352 (41%)] Loss: -82106.742188\n",
      "Train Epoch: 31 [8576/17352 (49%)] Loss: -82138.500000\n",
      "Train Epoch: 31 [9984/17352 (58%)] Loss: -84384.257812\n",
      "Train Epoch: 31 [11392/17352 (66%)] Loss: -81518.218750\n",
      "Train Epoch: 31 [12800/17352 (74%)] Loss: -82425.929688\n",
      "Train Epoch: 31 [14208/17352 (82%)] Loss: -86109.562500\n",
      "Train Epoch: 31 [15616/17352 (90%)] Loss: -83110.265625\n",
      "Train Epoch: 31 [17024/17352 (98%)] Loss: -86204.343750\n",
      "    epoch          : 31\n",
      "    loss           : -82481.61848000919\n",
      "    val_loss       : -78870.87617874146\n",
      "Train Epoch: 32 [128/17352 (1%)] Loss: -81585.875000\n",
      "Train Epoch: 32 [1536/17352 (9%)] Loss: -83216.937500\n",
      "Train Epoch: 32 [2944/17352 (17%)] Loss: -81092.484375\n",
      "Train Epoch: 32 [4352/17352 (25%)] Loss: -83708.937500\n",
      "Train Epoch: 32 [5760/17352 (33%)] Loss: -88921.070312\n",
      "Train Epoch: 32 [7168/17352 (41%)] Loss: -84849.937500\n",
      "Train Epoch: 32 [8576/17352 (49%)] Loss: -86403.890625\n",
      "Train Epoch: 32 [9984/17352 (58%)] Loss: -87618.328125\n",
      "Train Epoch: 32 [11392/17352 (66%)] Loss: -83048.687500\n",
      "Train Epoch: 32 [12800/17352 (74%)] Loss: -86505.039062\n",
      "Train Epoch: 32 [14208/17352 (82%)] Loss: -86194.398438\n",
      "Train Epoch: 32 [15616/17352 (90%)] Loss: -89689.695312\n",
      "Train Epoch: 32 [17024/17352 (98%)] Loss: -87694.250000\n",
      "    epoch          : 32\n",
      "    loss           : -84607.10943244485\n",
      "    val_loss       : -81061.13202857971\n",
      "Train Epoch: 33 [128/17352 (1%)] Loss: -84083.437500\n",
      "Train Epoch: 33 [1536/17352 (9%)] Loss: -86537.578125\n",
      "Train Epoch: 33 [2944/17352 (17%)] Loss: -83685.851562\n",
      "Train Epoch: 33 [4352/17352 (25%)] Loss: -87942.968750\n",
      "Train Epoch: 33 [5760/17352 (33%)] Loss: -86497.484375\n",
      "Train Epoch: 33 [7168/17352 (41%)] Loss: -85789.757812\n",
      "Train Epoch: 33 [8576/17352 (49%)] Loss: -91616.750000\n",
      "Train Epoch: 33 [9984/17352 (58%)] Loss: -85763.460938\n",
      "Train Epoch: 33 [11392/17352 (66%)] Loss: -84303.460938\n",
      "Train Epoch: 33 [12800/17352 (74%)] Loss: -89181.789062\n",
      "Train Epoch: 33 [14208/17352 (82%)] Loss: -84778.750000\n",
      "Train Epoch: 33 [15616/17352 (90%)] Loss: -88418.656250\n",
      "Train Epoch: 33 [17024/17352 (98%)] Loss: -85955.750000\n",
      "    epoch          : 33\n",
      "    loss           : -87358.5438304228\n",
      "    val_loss       : -84129.36471366882\n",
      "Train Epoch: 34 [128/17352 (1%)] Loss: -86928.429688\n",
      "Train Epoch: 34 [1536/17352 (9%)] Loss: -92627.000000\n",
      "Train Epoch: 34 [2944/17352 (17%)] Loss: -91259.734375\n",
      "Train Epoch: 34 [4352/17352 (25%)] Loss: -87791.921875\n",
      "Train Epoch: 34 [5760/17352 (33%)] Loss: -91927.890625\n",
      "Train Epoch: 34 [7168/17352 (41%)] Loss: -91451.609375\n",
      "Train Epoch: 34 [8576/17352 (49%)] Loss: -91164.093750\n",
      "Train Epoch: 34 [9984/17352 (58%)] Loss: -91695.812500\n",
      "Train Epoch: 34 [11392/17352 (66%)] Loss: -94420.140625\n",
      "Train Epoch: 34 [12800/17352 (74%)] Loss: -86962.507812\n",
      "Train Epoch: 34 [14208/17352 (82%)] Loss: -96083.101562\n",
      "Train Epoch: 34 [15616/17352 (90%)] Loss: -91148.828125\n",
      "Train Epoch: 34 [17024/17352 (98%)] Loss: -92317.382812\n",
      "    epoch          : 34\n",
      "    loss           : -90024.41420611214\n",
      "    val_loss       : -86620.36573505402\n",
      "Train Epoch: 35 [128/17352 (1%)] Loss: -89430.218750\n",
      "Train Epoch: 35 [1536/17352 (9%)] Loss: -90994.250000\n",
      "Train Epoch: 35 [2944/17352 (17%)] Loss: -88440.648438\n",
      "Train Epoch: 35 [4352/17352 (25%)] Loss: -94725.242188\n",
      "Train Epoch: 35 [5760/17352 (33%)] Loss: -96713.203125\n",
      "Train Epoch: 35 [7168/17352 (41%)] Loss: -91393.453125\n",
      "Train Epoch: 35 [8576/17352 (49%)] Loss: -93182.734375\n",
      "Train Epoch: 35 [9984/17352 (58%)] Loss: -92384.187500\n",
      "Train Epoch: 35 [11392/17352 (66%)] Loss: -92480.289062\n",
      "Train Epoch: 35 [12800/17352 (74%)] Loss: -95357.578125\n",
      "Train Epoch: 35 [14208/17352 (82%)] Loss: -92819.218750\n",
      "Train Epoch: 35 [15616/17352 (90%)] Loss: -100450.398438\n",
      "Train Epoch: 35 [17024/17352 (98%)] Loss: -89164.179688\n",
      "    epoch          : 35\n",
      "    loss           : -92678.1899988511\n",
      "    val_loss       : -90353.83448314667\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0703_171824/checkpoint-epoch35.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 36 [128/17352 (1%)] Loss: -96574.570312\n",
      "Train Epoch: 36 [1536/17352 (9%)] Loss: -94954.789062\n",
      "Train Epoch: 36 [2944/17352 (17%)] Loss: -93050.789062\n",
      "Train Epoch: 36 [4352/17352 (25%)] Loss: -95582.601562\n",
      "Train Epoch: 36 [5760/17352 (33%)] Loss: -99902.531250\n",
      "Train Epoch: 36 [7168/17352 (41%)] Loss: -93762.281250\n",
      "Train Epoch: 36 [8576/17352 (49%)] Loss: -97918.531250\n",
      "Train Epoch: 36 [9984/17352 (58%)] Loss: -100485.718750\n",
      "Train Epoch: 36 [11392/17352 (66%)] Loss: -94820.507812\n",
      "Train Epoch: 36 [12800/17352 (74%)] Loss: -98637.968750\n",
      "Train Epoch: 36 [14208/17352 (82%)] Loss: -99165.062500\n",
      "Train Epoch: 36 [15616/17352 (90%)] Loss: -103841.843750\n",
      "Train Epoch: 36 [17024/17352 (98%)] Loss: -101452.625000\n",
      "    epoch          : 36\n",
      "    loss           : -96440.20177504595\n",
      "    val_loss       : -93019.4336605072\n",
      "Train Epoch: 37 [128/17352 (1%)] Loss: -101453.335938\n",
      "Train Epoch: 37 [1536/17352 (9%)] Loss: -97334.953125\n",
      "Train Epoch: 37 [2944/17352 (17%)] Loss: -102250.562500\n",
      "Train Epoch: 37 [4352/17352 (25%)] Loss: -98247.296875\n",
      "Train Epoch: 37 [5760/17352 (33%)] Loss: -102439.468750\n",
      "Train Epoch: 37 [7168/17352 (41%)] Loss: -105227.484375\n",
      "Train Epoch: 37 [8576/17352 (49%)] Loss: -101366.710938\n",
      "Train Epoch: 37 [9984/17352 (58%)] Loss: -103772.726562\n",
      "Train Epoch: 37 [11392/17352 (66%)] Loss: -101813.179688\n",
      "Train Epoch: 37 [12800/17352 (74%)] Loss: -104048.781250\n",
      "Train Epoch: 37 [14208/17352 (82%)] Loss: -102375.570312\n",
      "Train Epoch: 37 [15616/17352 (90%)] Loss: -108337.398438\n",
      "Train Epoch: 37 [17024/17352 (98%)] Loss: -108333.968750\n",
      "    epoch          : 37\n",
      "    loss           : -102423.07637293199\n",
      "    val_loss       : -99849.64634227753\n",
      "Train Epoch: 38 [128/17352 (1%)] Loss: -108498.460938\n",
      "Train Epoch: 38 [1536/17352 (9%)] Loss: -109021.601562\n",
      "Train Epoch: 38 [2944/17352 (17%)] Loss: -104225.609375\n",
      "Train Epoch: 38 [4352/17352 (25%)] Loss: -101396.281250\n",
      "Train Epoch: 38 [5760/17352 (33%)] Loss: -109964.195312\n",
      "Train Epoch: 38 [7168/17352 (41%)] Loss: -110785.359375\n",
      "Train Epoch: 38 [8576/17352 (49%)] Loss: -104864.921875\n",
      "Train Epoch: 38 [9984/17352 (58%)] Loss: -111217.773438\n",
      "Train Epoch: 38 [11392/17352 (66%)] Loss: -111892.984375\n",
      "Train Epoch: 38 [12800/17352 (74%)] Loss: -112700.093750\n",
      "Train Epoch: 38 [14208/17352 (82%)] Loss: -109394.039062\n",
      "Train Epoch: 38 [15616/17352 (90%)] Loss: -113429.773438\n",
      "Train Epoch: 38 [17024/17352 (98%)] Loss: -107111.007812\n",
      "    epoch          : 38\n",
      "    loss           : -109470.69743795956\n",
      "    val_loss       : -106814.50460147858\n",
      "Train Epoch: 39 [128/17352 (1%)] Loss: -114338.937500\n",
      "Train Epoch: 39 [1536/17352 (9%)] Loss: -109984.023438\n",
      "Train Epoch: 39 [2944/17352 (17%)] Loss: -114811.984375\n",
      "Train Epoch: 39 [4352/17352 (25%)] Loss: -111046.906250\n",
      "Train Epoch: 39 [5760/17352 (33%)] Loss: -115432.835938\n",
      "Train Epoch: 39 [7168/17352 (41%)] Loss: -116170.523438\n",
      "Train Epoch: 39 [8576/17352 (49%)] Loss: -116625.960938\n",
      "Train Epoch: 39 [9984/17352 (58%)] Loss: -116871.859375\n",
      "Train Epoch: 39 [11392/17352 (66%)] Loss: -109986.171875\n",
      "Train Epoch: 39 [12800/17352 (74%)] Loss: -112918.539062\n",
      "Train Epoch: 39 [14208/17352 (82%)] Loss: -117538.914062\n",
      "Train Epoch: 39 [15616/17352 (90%)] Loss: -118801.070312\n",
      "Train Epoch: 39 [17024/17352 (98%)] Loss: -119464.062500\n",
      "    epoch          : 39\n",
      "    loss           : -114872.6572265625\n",
      "    val_loss       : -111741.85607814789\n",
      "Train Epoch: 40 [128/17352 (1%)] Loss: -111348.875000\n",
      "Train Epoch: 40 [1536/17352 (9%)] Loss: -119532.171875\n",
      "Train Epoch: 40 [2944/17352 (17%)] Loss: -120189.242188\n",
      "Train Epoch: 40 [4352/17352 (25%)] Loss: -119902.273438\n",
      "Train Epoch: 40 [5760/17352 (33%)] Loss: -120592.359375\n",
      "Train Epoch: 40 [7168/17352 (41%)] Loss: -121081.468750\n",
      "Train Epoch: 40 [8576/17352 (49%)] Loss: -121408.968750\n",
      "Train Epoch: 40 [9984/17352 (58%)] Loss: -122004.640625\n",
      "Train Epoch: 40 [11392/17352 (66%)] Loss: -122562.742188\n",
      "Train Epoch: 40 [12800/17352 (74%)] Loss: -122852.335938\n",
      "Train Epoch: 40 [14208/17352 (82%)] Loss: -122970.835938\n",
      "Train Epoch: 40 [15616/17352 (90%)] Loss: -123619.015625\n",
      "Train Epoch: 40 [17024/17352 (98%)] Loss: -123382.304688\n",
      "    epoch          : 40\n",
      "    loss           : -120316.96421185661\n",
      "    val_loss       : -116614.44931602478\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0703_171824/checkpoint-epoch40.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 41 [128/17352 (1%)] Loss: -123940.898438\n",
      "Train Epoch: 41 [1536/17352 (9%)] Loss: -124082.218750\n",
      "Train Epoch: 41 [2944/17352 (17%)] Loss: -124404.960938\n",
      "Train Epoch: 41 [4352/17352 (25%)] Loss: -125170.312500\n",
      "Train Epoch: 41 [5760/17352 (33%)] Loss: -124785.468750\n",
      "Train Epoch: 41 [7168/17352 (41%)] Loss: -125935.531250\n",
      "Train Epoch: 41 [8576/17352 (49%)] Loss: -125870.734375\n",
      "Train Epoch: 41 [9984/17352 (58%)] Loss: -126673.000000\n",
      "Train Epoch: 41 [11392/17352 (66%)] Loss: -126982.156250\n",
      "Train Epoch: 41 [12800/17352 (74%)] Loss: -127157.312500\n",
      "Train Epoch: 41 [14208/17352 (82%)] Loss: -127838.562500\n",
      "Train Epoch: 41 [15616/17352 (90%)] Loss: -127728.828125\n",
      "Train Epoch: 41 [17024/17352 (98%)] Loss: -128419.625000\n",
      "    epoch          : 41\n",
      "    loss           : -125133.06393612133\n",
      "    val_loss       : -120423.45415592194\n",
      "Train Epoch: 42 [128/17352 (1%)] Loss: -128604.453125\n",
      "Train Epoch: 42 [1536/17352 (9%)] Loss: -128646.195312\n",
      "Train Epoch: 42 [2944/17352 (17%)] Loss: -128671.750000\n",
      "Train Epoch: 42 [4352/17352 (25%)] Loss: -129468.781250\n",
      "Train Epoch: 42 [5760/17352 (33%)] Loss: -121944.625000\n",
      "Train Epoch: 42 [7168/17352 (41%)] Loss: -130387.148438\n",
      "Train Epoch: 42 [8576/17352 (49%)] Loss: -130271.000000\n",
      "Train Epoch: 42 [9984/17352 (58%)] Loss: -131038.984375\n",
      "Train Epoch: 42 [11392/17352 (66%)] Loss: -131515.062500\n",
      "Train Epoch: 42 [12800/17352 (74%)] Loss: -131704.703125\n",
      "Train Epoch: 42 [14208/17352 (82%)] Loss: -131986.468750\n",
      "Train Epoch: 42 [15616/17352 (90%)] Loss: -132304.375000\n",
      "Train Epoch: 42 [17024/17352 (98%)] Loss: -132524.515625\n",
      "    epoch          : 42\n",
      "    loss           : -129189.8407054228\n",
      "    val_loss       : -124903.34777355194\n",
      "Train Epoch: 43 [128/17352 (1%)] Loss: -132521.250000\n",
      "Train Epoch: 43 [1536/17352 (9%)] Loss: -133237.781250\n",
      "Train Epoch: 43 [2944/17352 (17%)] Loss: -133795.906250\n",
      "Train Epoch: 43 [4352/17352 (25%)] Loss: -133548.375000\n",
      "Train Epoch: 43 [5760/17352 (33%)] Loss: -133771.625000\n",
      "Train Epoch: 43 [7168/17352 (41%)] Loss: -134496.250000\n",
      "Train Epoch: 43 [8576/17352 (49%)] Loss: -135452.218750\n",
      "Train Epoch: 43 [9984/17352 (58%)] Loss: -135244.218750\n",
      "Train Epoch: 43 [11392/17352 (66%)] Loss: -135360.296875\n",
      "Train Epoch: 43 [12800/17352 (74%)] Loss: -123515.953125\n",
      "Train Epoch: 43 [14208/17352 (82%)] Loss: -136004.437500\n",
      "Train Epoch: 43 [15616/17352 (90%)] Loss: -136348.078125\n",
      "Train Epoch: 43 [17024/17352 (98%)] Loss: -136606.875000\n",
      "    epoch          : 43\n",
      "    loss           : -133771.36368336398\n",
      "    val_loss       : -128185.2784690857\n",
      "Train Epoch: 44 [128/17352 (1%)] Loss: -136728.765625\n",
      "Train Epoch: 44 [1536/17352 (9%)] Loss: -137321.937500\n",
      "Train Epoch: 44 [2944/17352 (17%)] Loss: -137691.281250\n",
      "Train Epoch: 44 [4352/17352 (25%)] Loss: -138123.187500\n",
      "Train Epoch: 44 [5760/17352 (33%)] Loss: -137947.750000\n",
      "Train Epoch: 44 [7168/17352 (41%)] Loss: -138410.187500\n",
      "Train Epoch: 44 [8576/17352 (49%)] Loss: -138673.203125\n",
      "Train Epoch: 44 [9984/17352 (58%)] Loss: -138740.031250\n",
      "Train Epoch: 44 [11392/17352 (66%)] Loss: -139132.453125\n",
      "Train Epoch: 44 [12800/17352 (74%)] Loss: -139671.562500\n",
      "Train Epoch: 44 [14208/17352 (82%)] Loss: -140366.375000\n",
      "Train Epoch: 44 [15616/17352 (90%)] Loss: -140844.312500\n",
      "Train Epoch: 44 [17024/17352 (98%)] Loss: -127466.890625\n",
      "    epoch          : 44\n",
      "    loss           : -137853.64022288602\n",
      "    val_loss       : -132605.04057216644\n",
      "Train Epoch: 45 [128/17352 (1%)] Loss: -140981.281250\n",
      "Train Epoch: 45 [1536/17352 (9%)] Loss: -141306.687500\n",
      "Train Epoch: 45 [2944/17352 (17%)] Loss: -141698.781250\n",
      "Train Epoch: 45 [4352/17352 (25%)] Loss: -142033.609375\n",
      "Train Epoch: 45 [5760/17352 (33%)] Loss: -142073.343750\n",
      "Train Epoch: 45 [7168/17352 (41%)] Loss: -142525.281250\n",
      "Train Epoch: 45 [8576/17352 (49%)] Loss: -142833.718750\n",
      "Train Epoch: 45 [9984/17352 (58%)] Loss: -142873.687500\n",
      "Train Epoch: 45 [11392/17352 (66%)] Loss: -143276.531250\n",
      "Train Epoch: 45 [12800/17352 (74%)] Loss: -143449.218750\n",
      "Train Epoch: 45 [14208/17352 (82%)] Loss: -144369.703125\n",
      "Train Epoch: 45 [15616/17352 (90%)] Loss: -144275.125000\n",
      "Train Epoch: 45 [17024/17352 (98%)] Loss: -144814.406250\n",
      "    epoch          : 45\n",
      "    loss           : -142266.8359375\n",
      "    val_loss       : -136271.70566368103\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0703_171824/checkpoint-epoch45.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 46 [128/17352 (1%)] Loss: -144933.000000\n",
      "Train Epoch: 46 [1536/17352 (9%)] Loss: -144511.406250\n",
      "Train Epoch: 46 [2944/17352 (17%)] Loss: -145937.750000\n",
      "Train Epoch: 46 [4352/17352 (25%)] Loss: -145699.937500\n",
      "Train Epoch: 46 [5760/17352 (33%)] Loss: -146500.875000\n",
      "Train Epoch: 46 [7168/17352 (41%)] Loss: -146040.453125\n",
      "Train Epoch: 46 [8576/17352 (49%)] Loss: -146577.781250\n",
      "Train Epoch: 46 [9984/17352 (58%)] Loss: -146770.765625\n",
      "Train Epoch: 46 [11392/17352 (66%)] Loss: -147509.203125\n",
      "Train Epoch: 46 [12800/17352 (74%)] Loss: -148045.187500\n",
      "Train Epoch: 46 [14208/17352 (82%)] Loss: -147175.312500\n",
      "Train Epoch: 46 [15616/17352 (90%)] Loss: -148084.906250\n",
      "Train Epoch: 46 [17024/17352 (98%)] Loss: -148337.781250\n",
      "    epoch          : 46\n",
      "    loss           : -146205.24052159928\n",
      "    val_loss       : -139862.72317790985\n",
      "Train Epoch: 47 [128/17352 (1%)] Loss: -148562.031250\n",
      "Train Epoch: 47 [1536/17352 (9%)] Loss: -148776.750000\n",
      "Train Epoch: 47 [2944/17352 (17%)] Loss: -149381.453125\n",
      "Train Epoch: 47 [4352/17352 (25%)] Loss: -149289.562500\n",
      "Train Epoch: 47 [5760/17352 (33%)] Loss: -149588.656250\n",
      "Train Epoch: 47 [7168/17352 (41%)] Loss: -150309.312500\n",
      "Train Epoch: 47 [8576/17352 (49%)] Loss: -150171.140625\n",
      "Train Epoch: 47 [9984/17352 (58%)] Loss: -150235.218750\n",
      "Train Epoch: 47 [11392/17352 (66%)] Loss: -151078.375000\n",
      "Train Epoch: 47 [12800/17352 (74%)] Loss: -151106.312500\n",
      "Train Epoch: 47 [14208/17352 (82%)] Loss: -151043.312500\n",
      "Train Epoch: 47 [15616/17352 (90%)] Loss: -152030.968750\n",
      "Train Epoch: 47 [17024/17352 (98%)] Loss: -151928.281250\n",
      "    epoch          : 47\n",
      "    loss           : -149982.88005514705\n",
      "    val_loss       : -143425.67412662506\n",
      "Train Epoch: 48 [128/17352 (1%)] Loss: -152616.484375\n",
      "Train Epoch: 48 [1536/17352 (9%)] Loss: -152579.281250\n",
      "Train Epoch: 48 [2944/17352 (17%)] Loss: -152706.484375\n",
      "Train Epoch: 48 [4352/17352 (25%)] Loss: -153178.000000\n",
      "Train Epoch: 48 [5760/17352 (33%)] Loss: -153579.109375\n",
      "Train Epoch: 48 [7168/17352 (41%)] Loss: -153690.656250\n",
      "Train Epoch: 48 [8576/17352 (49%)] Loss: -153985.000000\n",
      "Train Epoch: 48 [9984/17352 (58%)] Loss: -154777.656250\n",
      "Train Epoch: 48 [11392/17352 (66%)] Loss: -154748.656250\n",
      "Train Epoch: 48 [12800/17352 (74%)] Loss: -154792.718750\n",
      "Train Epoch: 48 [14208/17352 (82%)] Loss: -155362.140625\n",
      "Train Epoch: 48 [15616/17352 (90%)] Loss: -155895.421875\n",
      "Train Epoch: 48 [17024/17352 (98%)] Loss: -155764.953125\n",
      "    epoch          : 48\n",
      "    loss           : -153679.9228515625\n",
      "    val_loss       : -146839.5584936142\n",
      "Train Epoch: 49 [128/17352 (1%)] Loss: -156164.375000\n",
      "Train Epoch: 49 [1536/17352 (9%)] Loss: -155517.703125\n",
      "Train Epoch: 49 [2944/17352 (17%)] Loss: -156201.093750\n",
      "Train Epoch: 49 [4352/17352 (25%)] Loss: -156976.171875\n",
      "Train Epoch: 49 [5760/17352 (33%)] Loss: -158164.765625\n",
      "Train Epoch: 49 [7168/17352 (41%)] Loss: -156881.843750\n",
      "Train Epoch: 49 [8576/17352 (49%)] Loss: -157799.859375\n",
      "Train Epoch: 49 [9984/17352 (58%)] Loss: -158049.828125\n",
      "Train Epoch: 49 [11392/17352 (66%)] Loss: -158470.031250\n",
      "Train Epoch: 49 [12800/17352 (74%)] Loss: -158963.375000\n",
      "Train Epoch: 49 [14208/17352 (82%)] Loss: -158686.046875\n",
      "Train Epoch: 49 [15616/17352 (90%)] Loss: -159263.765625\n",
      "Train Epoch: 49 [17024/17352 (98%)] Loss: -159953.859375\n",
      "    epoch          : 49\n",
      "    loss           : -157321.56795726102\n",
      "    val_loss       : -150284.9342470169\n",
      "Train Epoch: 50 [128/17352 (1%)] Loss: -159647.765625\n",
      "Train Epoch: 50 [1536/17352 (9%)] Loss: -160013.687500\n",
      "Train Epoch: 50 [2944/17352 (17%)] Loss: -160113.296875\n",
      "Train Epoch: 50 [4352/17352 (25%)] Loss: -159697.218750\n",
      "Train Epoch: 50 [5760/17352 (33%)] Loss: -160658.093750\n",
      "Train Epoch: 50 [7168/17352 (41%)] Loss: -160704.312500\n",
      "Train Epoch: 50 [8576/17352 (49%)] Loss: -161579.562500\n",
      "Train Epoch: 50 [9984/17352 (58%)] Loss: -161565.531250\n",
      "Train Epoch: 50 [11392/17352 (66%)] Loss: -161988.281250\n",
      "Train Epoch: 50 [12800/17352 (74%)] Loss: -161740.078125\n",
      "Train Epoch: 50 [14208/17352 (82%)] Loss: -162661.921875\n",
      "Train Epoch: 50 [15616/17352 (90%)] Loss: -163362.156250\n",
      "Train Epoch: 50 [17024/17352 (98%)] Loss: -162820.796875\n",
      "    epoch          : 50\n",
      "    loss           : -160543.66704963235\n",
      "    val_loss       : -153617.95403766632\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0703_171824/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [128/17352 (1%)] Loss: -163255.593750\n",
      "Train Epoch: 51 [1536/17352 (9%)] Loss: -163324.593750\n",
      "Train Epoch: 51 [2944/17352 (17%)] Loss: -163849.984375\n",
      "Train Epoch: 51 [4352/17352 (25%)] Loss: -163823.078125\n",
      "Train Epoch: 51 [5760/17352 (33%)] Loss: -163994.781250\n",
      "Train Epoch: 51 [7168/17352 (41%)] Loss: -164277.953125\n",
      "Train Epoch: 51 [8576/17352 (49%)] Loss: -164974.437500\n",
      "Train Epoch: 51 [9984/17352 (58%)] Loss: -164907.906250\n",
      "Train Epoch: 51 [11392/17352 (66%)] Loss: -165429.687500\n",
      "Train Epoch: 51 [12800/17352 (74%)] Loss: -165729.484375\n",
      "Train Epoch: 51 [14208/17352 (82%)] Loss: -165891.968750\n",
      "Train Epoch: 51 [15616/17352 (90%)] Loss: -166643.843750\n",
      "Train Epoch: 51 [17024/17352 (98%)] Loss: -166502.312500\n",
      "    epoch          : 51\n",
      "    loss           : -164336.21392463235\n",
      "    val_loss       : -156914.29522800446\n",
      "Train Epoch: 52 [128/17352 (1%)] Loss: -166981.671875\n",
      "Train Epoch: 52 [1536/17352 (9%)] Loss: -167094.750000\n",
      "Train Epoch: 52 [2944/17352 (17%)] Loss: -167426.093750\n",
      "Train Epoch: 52 [4352/17352 (25%)] Loss: -167239.812500\n",
      "Train Epoch: 52 [5760/17352 (33%)] Loss: -167941.046875\n",
      "Train Epoch: 52 [7168/17352 (41%)] Loss: -168186.000000\n",
      "Train Epoch: 52 [8576/17352 (49%)] Loss: -167798.156250\n",
      "Train Epoch: 52 [9984/17352 (58%)] Loss: -168962.546875\n",
      "Train Epoch: 52 [11392/17352 (66%)] Loss: -168768.921875\n",
      "Train Epoch: 52 [12800/17352 (74%)] Loss: -169240.156250\n",
      "Train Epoch: 52 [14208/17352 (82%)] Loss: -169326.468750\n",
      "Train Epoch: 52 [15616/17352 (90%)] Loss: -170270.703125\n",
      "Train Epoch: 52 [17024/17352 (98%)] Loss: -169828.718750\n",
      "    epoch          : 52\n",
      "    loss           : -167819.68359375\n",
      "    val_loss       : -160221.39232444763\n",
      "Train Epoch: 53 [128/17352 (1%)] Loss: -169605.625000\n",
      "Train Epoch: 53 [1536/17352 (9%)] Loss: -171429.968750\n",
      "Train Epoch: 53 [2944/17352 (17%)] Loss: -170668.703125\n",
      "Train Epoch: 53 [4352/17352 (25%)] Loss: -169941.281250\n",
      "Train Epoch: 53 [5760/17352 (33%)] Loss: -171618.390625\n",
      "Train Epoch: 53 [7168/17352 (41%)] Loss: -172229.812500\n",
      "Train Epoch: 53 [8576/17352 (49%)] Loss: -171815.234375\n",
      "Train Epoch: 53 [9984/17352 (58%)] Loss: -172782.625000\n",
      "Train Epoch: 53 [11392/17352 (66%)] Loss: -172507.125000\n",
      "Train Epoch: 53 [12800/17352 (74%)] Loss: -172330.437500\n",
      "Train Epoch: 53 [14208/17352 (82%)] Loss: -172924.671875\n",
      "Train Epoch: 53 [15616/17352 (90%)] Loss: -173356.593750\n",
      "Train Epoch: 53 [17024/17352 (98%)] Loss: -173826.281250\n",
      "    epoch          : 53\n",
      "    loss           : -171272.13677619485\n",
      "    val_loss       : -162311.0436153412\n",
      "Train Epoch: 54 [128/17352 (1%)] Loss: -173356.500000\n",
      "Train Epoch: 54 [1536/17352 (9%)] Loss: -174019.984375\n",
      "Train Epoch: 54 [2944/17352 (17%)] Loss: -174103.312500\n",
      "Train Epoch: 54 [4352/17352 (25%)] Loss: -175050.093750\n",
      "Train Epoch: 54 [5760/17352 (33%)] Loss: -173739.781250\n",
      "Train Epoch: 54 [7168/17352 (41%)] Loss: -174935.125000\n",
      "Train Epoch: 54 [8576/17352 (49%)] Loss: -174838.906250\n",
      "Train Epoch: 54 [9984/17352 (58%)] Loss: -175765.562500\n",
      "Train Epoch: 54 [11392/17352 (66%)] Loss: -176048.937500\n",
      "Train Epoch: 54 [12800/17352 (74%)] Loss: -175907.921875\n",
      "Train Epoch: 54 [14208/17352 (82%)] Loss: -176221.609375\n",
      "Train Epoch: 54 [15616/17352 (90%)] Loss: -176805.328125\n",
      "Train Epoch: 54 [17024/17352 (98%)] Loss: -177346.781250\n",
      "    epoch          : 54\n",
      "    loss           : -174682.83363970587\n",
      "    val_loss       : -165457.9169588089\n",
      "Train Epoch: 55 [128/17352 (1%)] Loss: -177832.578125\n",
      "Train Epoch: 55 [1536/17352 (9%)] Loss: -177497.968750\n",
      "Train Epoch: 55 [2944/17352 (17%)] Loss: -177545.312500\n",
      "Train Epoch: 55 [4352/17352 (25%)] Loss: -177654.187500\n",
      "Train Epoch: 55 [5760/17352 (33%)] Loss: -177820.015625\n",
      "Train Epoch: 55 [7168/17352 (41%)] Loss: -178221.187500\n",
      "Train Epoch: 55 [8576/17352 (49%)] Loss: -178756.890625\n",
      "Train Epoch: 55 [9984/17352 (58%)] Loss: -179136.328125\n",
      "Train Epoch: 55 [11392/17352 (66%)] Loss: -179537.968750\n",
      "Train Epoch: 55 [12800/17352 (74%)] Loss: -179525.046875\n",
      "Train Epoch: 55 [14208/17352 (82%)] Loss: -179603.171875\n",
      "Train Epoch: 55 [15616/17352 (90%)] Loss: -179898.890625\n",
      "Train Epoch: 55 [17024/17352 (98%)] Loss: -181127.656250\n",
      "    epoch          : 55\n",
      "    loss           : -178095.80778952205\n",
      "    val_loss       : -169912.64090633392\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0703_171824/checkpoint-epoch55.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 56 [128/17352 (1%)] Loss: -181069.078125\n",
      "Train Epoch: 56 [1536/17352 (9%)] Loss: -180807.156250\n",
      "Train Epoch: 56 [2944/17352 (17%)] Loss: -180812.578125\n",
      "Train Epoch: 56 [4352/17352 (25%)] Loss: -181053.781250\n",
      "Train Epoch: 56 [5760/17352 (33%)] Loss: -180965.843750\n",
      "Train Epoch: 56 [7168/17352 (41%)] Loss: -181803.062500\n",
      "Train Epoch: 56 [8576/17352 (49%)] Loss: -181414.062500\n",
      "Train Epoch: 56 [9984/17352 (58%)] Loss: -182483.000000\n",
      "Train Epoch: 56 [11392/17352 (66%)] Loss: -182392.796875\n",
      "Train Epoch: 56 [12800/17352 (74%)] Loss: -182439.781250\n",
      "Train Epoch: 56 [14208/17352 (82%)] Loss: -183686.437500\n",
      "Train Epoch: 56 [15616/17352 (90%)] Loss: -183738.781250\n",
      "Train Epoch: 56 [17024/17352 (98%)] Loss: -183943.859375\n",
      "    epoch          : 56\n",
      "    loss           : -181463.6563648897\n",
      "    val_loss       : -173105.80631160736\n",
      "Train Epoch: 57 [128/17352 (1%)] Loss: -183841.203125\n",
      "Train Epoch: 57 [1536/17352 (9%)] Loss: -184248.125000\n",
      "Train Epoch: 57 [2944/17352 (17%)] Loss: -184537.375000\n",
      "Train Epoch: 57 [4352/17352 (25%)] Loss: -184807.406250\n",
      "Train Epoch: 57 [5760/17352 (33%)] Loss: -184634.937500\n",
      "Train Epoch: 57 [7168/17352 (41%)] Loss: -185189.546875\n",
      "Train Epoch: 57 [8576/17352 (49%)] Loss: -185843.781250\n",
      "Train Epoch: 57 [9984/17352 (58%)] Loss: -185630.906250\n",
      "Train Epoch: 57 [11392/17352 (66%)] Loss: -186633.218750\n",
      "Train Epoch: 57 [12800/17352 (74%)] Loss: -186758.437500\n",
      "Train Epoch: 57 [14208/17352 (82%)] Loss: -186903.359375\n",
      "Train Epoch: 57 [15616/17352 (90%)] Loss: -187272.000000\n",
      "Train Epoch: 57 [17024/17352 (98%)] Loss: -186162.421875\n",
      "    epoch          : 57\n",
      "    loss           : -184792.6336741728\n",
      "    val_loss       : -176247.17618656158\n",
      "Train Epoch: 58 [128/17352 (1%)] Loss: -187611.703125\n",
      "Train Epoch: 58 [1536/17352 (9%)] Loss: -187001.468750\n",
      "Train Epoch: 58 [2944/17352 (17%)] Loss: -188526.125000\n",
      "Train Epoch: 58 [4352/17352 (25%)] Loss: -188371.296875\n",
      "Train Epoch: 58 [5760/17352 (33%)] Loss: -187673.500000\n",
      "Train Epoch: 58 [7168/17352 (41%)] Loss: -188592.343750\n",
      "Train Epoch: 58 [8576/17352 (49%)] Loss: -188837.750000\n",
      "Train Epoch: 58 [9984/17352 (58%)] Loss: -189717.421875\n",
      "Train Epoch: 58 [11392/17352 (66%)] Loss: -189036.968750\n",
      "Train Epoch: 58 [12800/17352 (74%)] Loss: -190408.562500\n",
      "Train Epoch: 58 [14208/17352 (82%)] Loss: -190147.687500\n",
      "Train Epoch: 58 [15616/17352 (90%)] Loss: -190107.062500\n",
      "Train Epoch: 58 [17024/17352 (98%)] Loss: -190553.046875\n",
      "    epoch          : 58\n",
      "    loss           : -188134.79181985295\n",
      "    val_loss       : -177937.37861728668\n",
      "Train Epoch: 59 [128/17352 (1%)] Loss: -190841.046875\n",
      "Train Epoch: 59 [1536/17352 (9%)] Loss: -191320.500000\n",
      "Train Epoch: 59 [2944/17352 (17%)] Loss: -191342.453125\n",
      "Train Epoch: 59 [4352/17352 (25%)] Loss: -190989.078125\n",
      "Train Epoch: 59 [5760/17352 (33%)] Loss: -191920.609375\n",
      "Train Epoch: 59 [7168/17352 (41%)] Loss: -191585.421875\n",
      "Train Epoch: 59 [8576/17352 (49%)] Loss: -192102.968750\n",
      "Train Epoch: 59 [9984/17352 (58%)] Loss: -192076.859375\n",
      "Train Epoch: 59 [11392/17352 (66%)] Loss: -193079.593750\n",
      "Train Epoch: 59 [12800/17352 (74%)] Loss: -192754.000000\n",
      "Train Epoch: 59 [14208/17352 (82%)] Loss: -192796.046875\n",
      "Train Epoch: 59 [15616/17352 (90%)] Loss: -192985.156250\n",
      "Train Epoch: 59 [17024/17352 (98%)] Loss: -193223.484375\n",
      "    epoch          : 59\n",
      "    loss           : -191445.76177619485\n",
      "    val_loss       : -182451.52382659912\n",
      "Train Epoch: 60 [128/17352 (1%)] Loss: -193049.812500\n",
      "Train Epoch: 60 [1536/17352 (9%)] Loss: -194496.031250\n",
      "Train Epoch: 60 [2944/17352 (17%)] Loss: -194290.890625\n",
      "Train Epoch: 60 [4352/17352 (25%)] Loss: -194780.843750\n",
      "Train Epoch: 60 [5760/17352 (33%)] Loss: -194835.906250\n",
      "Train Epoch: 60 [7168/17352 (41%)] Loss: -195552.687500\n",
      "Train Epoch: 60 [8576/17352 (49%)] Loss: -195806.765625\n",
      "Train Epoch: 60 [9984/17352 (58%)] Loss: -196468.140625\n",
      "Train Epoch: 60 [11392/17352 (66%)] Loss: -195849.703125\n",
      "Train Epoch: 60 [12800/17352 (74%)] Loss: -195618.062500\n",
      "Train Epoch: 60 [14208/17352 (82%)] Loss: -196487.078125\n",
      "Train Epoch: 60 [15616/17352 (90%)] Loss: -197050.609375\n",
      "Train Epoch: 60 [17024/17352 (98%)] Loss: -197025.281250\n",
      "    epoch          : 60\n",
      "    loss           : -194910.59719669117\n",
      "    val_loss       : -185580.96176719666\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0703_171824/checkpoint-epoch60.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 61 [128/17352 (1%)] Loss: -197383.843750\n",
      "Train Epoch: 61 [1536/17352 (9%)] Loss: -197974.250000\n",
      "Train Epoch: 61 [2944/17352 (17%)] Loss: -196730.031250\n",
      "Train Epoch: 61 [4352/17352 (25%)] Loss: -197958.812500\n",
      "Train Epoch: 61 [5760/17352 (33%)] Loss: -197768.046875\n",
      "Train Epoch: 61 [7168/17352 (41%)] Loss: -197982.218750\n",
      "Train Epoch: 61 [8576/17352 (49%)] Loss: -198505.984375\n",
      "Train Epoch: 61 [9984/17352 (58%)] Loss: -198839.328125\n",
      "Train Epoch: 61 [11392/17352 (66%)] Loss: -199443.234375\n",
      "Train Epoch: 61 [12800/17352 (74%)] Loss: -200258.343750\n",
      "Train Epoch: 61 [14208/17352 (82%)] Loss: -200104.593750\n",
      "Train Epoch: 61 [15616/17352 (90%)] Loss: -200137.093750\n",
      "Train Epoch: 61 [17024/17352 (98%)] Loss: -200955.828125\n",
      "    epoch          : 61\n",
      "    loss           : -198188.73127297795\n",
      "    val_loss       : -188670.56842708588\n",
      "Train Epoch: 62 [128/17352 (1%)] Loss: -200430.437500\n",
      "Train Epoch: 62 [1536/17352 (9%)] Loss: -200365.343750\n",
      "Train Epoch: 62 [2944/17352 (17%)] Loss: -201369.875000\n",
      "Train Epoch: 62 [4352/17352 (25%)] Loss: -200602.875000\n",
      "Train Epoch: 62 [5760/17352 (33%)] Loss: -201462.093750\n",
      "Train Epoch: 62 [7168/17352 (41%)] Loss: -202289.296875\n",
      "Train Epoch: 62 [8576/17352 (49%)] Loss: -201482.203125\n",
      "Train Epoch: 62 [9984/17352 (58%)] Loss: -202742.156250\n",
      "Train Epoch: 62 [11392/17352 (66%)] Loss: -203027.703125\n",
      "Train Epoch: 62 [12800/17352 (74%)] Loss: -202119.218750\n",
      "Train Epoch: 62 [14208/17352 (82%)] Loss: -203190.562500\n",
      "Train Epoch: 62 [15616/17352 (90%)] Loss: -203561.218750\n",
      "Train Epoch: 62 [17024/17352 (98%)] Loss: -203602.109375\n",
      "    epoch          : 62\n",
      "    loss           : -201447.94680606617\n",
      "    val_loss       : -191758.28317642212\n",
      "Train Epoch: 63 [128/17352 (1%)] Loss: -203516.234375\n",
      "Train Epoch: 63 [1536/17352 (9%)] Loss: -203954.421875\n",
      "Train Epoch: 63 [2944/17352 (17%)] Loss: -204288.218750\n",
      "Train Epoch: 63 [4352/17352 (25%)] Loss: -204829.187500\n",
      "Train Epoch: 63 [5760/17352 (33%)] Loss: -205041.281250\n",
      "Train Epoch: 63 [7168/17352 (41%)] Loss: -205363.687500\n",
      "Train Epoch: 63 [8576/17352 (49%)] Loss: -205772.062500\n",
      "Train Epoch: 63 [9984/17352 (58%)] Loss: -206274.187500\n",
      "Train Epoch: 63 [11392/17352 (66%)] Loss: -205699.984375\n",
      "Train Epoch: 63 [12800/17352 (74%)] Loss: -207084.125000\n",
      "Train Epoch: 63 [14208/17352 (82%)] Loss: -205950.687500\n",
      "Train Epoch: 63 [15616/17352 (90%)] Loss: -206010.250000\n",
      "Train Epoch: 63 [17024/17352 (98%)] Loss: -206516.765625\n",
      "    epoch          : 63\n",
      "    loss           : -204714.73092830883\n",
      "    val_loss       : -194749.7196969986\n",
      "Train Epoch: 64 [128/17352 (1%)] Loss: -205981.031250\n",
      "Train Epoch: 64 [1536/17352 (9%)] Loss: -207187.546875\n",
      "Train Epoch: 64 [2944/17352 (17%)] Loss: -208033.000000\n",
      "Train Epoch: 64 [4352/17352 (25%)] Loss: -208166.828125\n",
      "Train Epoch: 64 [5760/17352 (33%)] Loss: -207731.234375\n",
      "Train Epoch: 64 [7168/17352 (41%)] Loss: -207483.343750\n",
      "Train Epoch: 64 [8576/17352 (49%)] Loss: -208685.656250\n",
      "Train Epoch: 64 [9984/17352 (58%)] Loss: -208998.187500\n",
      "Train Epoch: 64 [11392/17352 (66%)] Loss: -209408.093750\n",
      "Train Epoch: 64 [12800/17352 (74%)] Loss: -208510.390625\n",
      "Train Epoch: 64 [14208/17352 (82%)] Loss: -209353.156250\n",
      "Train Epoch: 64 [15616/17352 (90%)] Loss: -209358.828125\n",
      "Train Epoch: 64 [17024/17352 (98%)] Loss: -210856.265625\n",
      "    epoch          : 64\n",
      "    loss           : -207957.56548713235\n",
      "    val_loss       : -197871.7180519104\n",
      "Train Epoch: 65 [128/17352 (1%)] Loss: -209607.421875\n",
      "Train Epoch: 65 [1536/17352 (9%)] Loss: -209623.750000\n",
      "Train Epoch: 65 [2944/17352 (17%)] Loss: -210631.468750\n",
      "Train Epoch: 65 [4352/17352 (25%)] Loss: -210456.078125\n",
      "Train Epoch: 65 [5760/17352 (33%)] Loss: -211341.406250\n",
      "Train Epoch: 65 [7168/17352 (41%)] Loss: -211071.578125\n",
      "Train Epoch: 65 [8576/17352 (49%)] Loss: -210860.656250\n",
      "Train Epoch: 65 [9984/17352 (58%)] Loss: -212693.875000\n",
      "Train Epoch: 65 [11392/17352 (66%)] Loss: -213190.312500\n",
      "Train Epoch: 65 [12800/17352 (74%)] Loss: -212764.640625\n",
      "Train Epoch: 65 [14208/17352 (82%)] Loss: -213236.296875\n",
      "Train Epoch: 65 [15616/17352 (90%)] Loss: -213307.921875\n",
      "Train Epoch: 65 [17024/17352 (98%)] Loss: -213550.312500\n",
      "    epoch          : 65\n",
      "    loss           : -211192.04400275735\n",
      "    val_loss       : -200899.32501888275\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0703_171824/checkpoint-epoch65.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 66 [128/17352 (1%)] Loss: -212672.937500\n",
      "Train Epoch: 66 [1536/17352 (9%)] Loss: -214216.640625\n",
      "Train Epoch: 66 [2944/17352 (17%)] Loss: -214168.015625\n",
      "Train Epoch: 66 [4352/17352 (25%)] Loss: -214416.875000\n",
      "Train Epoch: 66 [5760/17352 (33%)] Loss: -214586.484375\n",
      "Train Epoch: 66 [7168/17352 (41%)] Loss: -214455.843750\n",
      "Train Epoch: 66 [8576/17352 (49%)] Loss: -215346.328125\n",
      "Train Epoch: 66 [9984/17352 (58%)] Loss: -215598.265625\n",
      "Train Epoch: 66 [11392/17352 (66%)] Loss: -215772.531250\n",
      "Train Epoch: 66 [12800/17352 (74%)] Loss: -215060.984375\n",
      "Train Epoch: 66 [14208/17352 (82%)] Loss: -215343.468750\n",
      "Train Epoch: 66 [15616/17352 (90%)] Loss: -215223.312500\n",
      "Train Epoch: 66 [17024/17352 (98%)] Loss: -215801.437500\n",
      "    epoch          : 66\n",
      "    loss           : -214423.44789751837\n",
      "    val_loss       : -203961.7140493393\n",
      "Train Epoch: 67 [128/17352 (1%)] Loss: -216572.781250\n",
      "Train Epoch: 67 [1536/17352 (9%)] Loss: -217818.812500\n",
      "Train Epoch: 67 [2944/17352 (17%)] Loss: -216843.609375\n",
      "Train Epoch: 67 [4352/17352 (25%)] Loss: -216853.156250\n",
      "Train Epoch: 67 [5760/17352 (33%)] Loss: -218140.968750\n",
      "Train Epoch: 67 [7168/17352 (41%)] Loss: -217596.875000\n",
      "Train Epoch: 67 [8576/17352 (49%)] Loss: -217857.265625\n",
      "Train Epoch: 67 [9984/17352 (58%)] Loss: -218316.796875\n",
      "Train Epoch: 67 [11392/17352 (66%)] Loss: -218905.750000\n",
      "Train Epoch: 67 [12800/17352 (74%)] Loss: -218830.156250\n",
      "Train Epoch: 67 [14208/17352 (82%)] Loss: -219746.875000\n",
      "Train Epoch: 67 [15616/17352 (90%)] Loss: -219787.000000\n",
      "Train Epoch: 67 [17024/17352 (98%)] Loss: -219549.312500\n",
      "    epoch          : 67\n",
      "    loss           : -217637.92767693015\n",
      "    val_loss       : -206989.53923130035\n",
      "Train Epoch: 68 [128/17352 (1%)] Loss: -219438.796875\n",
      "Train Epoch: 68 [1536/17352 (9%)] Loss: -219542.562500\n",
      "Train Epoch: 68 [2944/17352 (17%)] Loss: -220724.359375\n",
      "Train Epoch: 68 [4352/17352 (25%)] Loss: -221582.843750\n",
      "Train Epoch: 68 [5760/17352 (33%)] Loss: -220266.609375\n",
      "Train Epoch: 68 [7168/17352 (41%)] Loss: -219911.109375\n",
      "Train Epoch: 68 [8576/17352 (49%)] Loss: -222124.062500\n",
      "Train Epoch: 68 [9984/17352 (58%)] Loss: -222313.296875\n",
      "Train Epoch: 68 [11392/17352 (66%)] Loss: -220698.015625\n",
      "Train Epoch: 68 [12800/17352 (74%)] Loss: -222564.218750\n",
      "Train Epoch: 68 [14208/17352 (82%)] Loss: -221913.921875\n",
      "Train Epoch: 68 [15616/17352 (90%)] Loss: -222838.468750\n",
      "Train Epoch: 68 [17024/17352 (98%)] Loss: -223867.953125\n",
      "    epoch          : 68\n",
      "    loss           : -220833.1856617647\n",
      "    val_loss       : -210010.44784355164\n",
      "Train Epoch: 69 [128/17352 (1%)] Loss: -223204.468750\n",
      "Train Epoch: 69 [1536/17352 (9%)] Loss: -223928.437500\n",
      "Train Epoch: 69 [2944/17352 (17%)] Loss: -223583.328125\n",
      "Train Epoch: 69 [4352/17352 (25%)] Loss: -223787.000000\n",
      "Train Epoch: 69 [5760/17352 (33%)] Loss: -224669.140625\n",
      "Train Epoch: 69 [7168/17352 (41%)] Loss: -224461.500000\n",
      "Train Epoch: 69 [8576/17352 (49%)] Loss: -225091.968750\n",
      "Train Epoch: 69 [9984/17352 (58%)] Loss: -224976.531250\n",
      "Train Epoch: 69 [11392/17352 (66%)] Loss: -225271.968750\n",
      "Train Epoch: 69 [12800/17352 (74%)] Loss: -225613.625000\n",
      "Train Epoch: 69 [14208/17352 (82%)] Loss: -225907.093750\n",
      "Train Epoch: 69 [15616/17352 (90%)] Loss: -225773.250000\n",
      "Train Epoch: 69 [17024/17352 (98%)] Loss: -226378.093750\n",
      "    epoch          : 69\n",
      "    loss           : -224021.35271139705\n",
      "    val_loss       : -213041.5129070282\n",
      "Train Epoch: 70 [128/17352 (1%)] Loss: -226537.406250\n",
      "Train Epoch: 70 [1536/17352 (9%)] Loss: -226529.796875\n",
      "Train Epoch: 70 [2944/17352 (17%)] Loss: -228525.859375\n",
      "Train Epoch: 70 [4352/17352 (25%)] Loss: -226590.734375\n",
      "Train Epoch: 70 [5760/17352 (33%)] Loss: -227887.031250\n",
      "Train Epoch: 70 [7168/17352 (41%)] Loss: -227638.875000\n",
      "Train Epoch: 70 [8576/17352 (49%)] Loss: -228528.078125\n",
      "Train Epoch: 70 [9984/17352 (58%)] Loss: -227784.515625\n",
      "Train Epoch: 70 [11392/17352 (66%)] Loss: -228484.781250\n",
      "Train Epoch: 70 [12800/17352 (74%)] Loss: -228902.265625\n",
      "Train Epoch: 70 [14208/17352 (82%)] Loss: -228162.656250\n",
      "Train Epoch: 70 [15616/17352 (90%)] Loss: -228975.125000\n",
      "Train Epoch: 70 [17024/17352 (98%)] Loss: -229268.781250\n",
      "    epoch          : 70\n",
      "    loss           : -227201.00425091913\n",
      "    val_loss       : -215975.34011173248\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0703_171824/checkpoint-epoch70.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 71 [128/17352 (1%)] Loss: -230256.625000\n",
      "Train Epoch: 71 [1536/17352 (9%)] Loss: -229603.750000\n",
      "Train Epoch: 71 [2944/17352 (17%)] Loss: -230511.531250\n",
      "Train Epoch: 71 [4352/17352 (25%)] Loss: -230259.156250\n",
      "Train Epoch: 71 [5760/17352 (33%)] Loss: -230700.828125\n",
      "Train Epoch: 71 [7168/17352 (41%)] Loss: -230285.453125\n",
      "Train Epoch: 71 [8576/17352 (49%)] Loss: -231624.375000\n",
      "Train Epoch: 71 [9984/17352 (58%)] Loss: -231148.671875\n",
      "Train Epoch: 71 [11392/17352 (66%)] Loss: -230948.468750\n",
      "Train Epoch: 71 [12800/17352 (74%)] Loss: -232657.859375\n",
      "Train Epoch: 71 [14208/17352 (82%)] Loss: -232259.750000\n",
      "Train Epoch: 71 [15616/17352 (90%)] Loss: -232541.500000\n",
      "Train Epoch: 71 [17024/17352 (98%)] Loss: -232349.750000\n",
      "    epoch          : 71\n",
      "    loss           : -230388.13608685663\n",
      "    val_loss       : -219026.55543136597\n",
      "Train Epoch: 72 [128/17352 (1%)] Loss: -234053.281250\n",
      "Train Epoch: 72 [1536/17352 (9%)] Loss: -232993.218750\n",
      "Train Epoch: 72 [2944/17352 (17%)] Loss: -232540.781250\n",
      "Train Epoch: 72 [4352/17352 (25%)] Loss: -233615.343750\n",
      "Train Epoch: 72 [5760/17352 (33%)] Loss: -232739.656250\n",
      "Train Epoch: 72 [7168/17352 (41%)] Loss: -233705.984375\n",
      "Train Epoch: 72 [8576/17352 (49%)] Loss: -232994.750000\n",
      "Train Epoch: 72 [9984/17352 (58%)] Loss: -234370.375000\n",
      "Train Epoch: 72 [11392/17352 (66%)] Loss: -234448.125000\n",
      "Train Epoch: 72 [12800/17352 (74%)] Loss: -235308.328125\n",
      "Train Epoch: 72 [14208/17352 (82%)] Loss: -235310.484375\n",
      "Train Epoch: 72 [15616/17352 (90%)] Loss: -235879.343750\n",
      "Train Epoch: 72 [17024/17352 (98%)] Loss: -235803.390625\n",
      "    epoch          : 72\n",
      "    loss           : -233554.26206341913\n",
      "    val_loss       : -221995.94995689392\n",
      "Train Epoch: 73 [128/17352 (1%)] Loss: -237104.953125\n",
      "Train Epoch: 73 [1536/17352 (9%)] Loss: -236610.312500\n",
      "Train Epoch: 73 [2944/17352 (17%)] Loss: -236784.609375\n",
      "Train Epoch: 73 [4352/17352 (25%)] Loss: -236683.703125\n",
      "Train Epoch: 73 [5760/17352 (33%)] Loss: -236416.390625\n",
      "Train Epoch: 73 [7168/17352 (41%)] Loss: -236743.937500\n",
      "Train Epoch: 73 [8576/17352 (49%)] Loss: -236962.078125\n",
      "Train Epoch: 73 [9984/17352 (58%)] Loss: -238533.421875\n",
      "Train Epoch: 73 [11392/17352 (66%)] Loss: -237179.468750\n",
      "Train Epoch: 73 [12800/17352 (74%)] Loss: -238377.375000\n",
      "Train Epoch: 73 [14208/17352 (82%)] Loss: -239205.781250\n",
      "Train Epoch: 73 [15616/17352 (90%)] Loss: -239229.609375\n",
      "Train Epoch: 73 [17024/17352 (98%)] Loss: -238144.515625\n",
      "    epoch          : 73\n",
      "    loss           : -236704.13200827205\n",
      "    val_loss       : -224946.13359165192\n",
      "Train Epoch: 74 [128/17352 (1%)] Loss: -238643.453125\n",
      "Train Epoch: 74 [1536/17352 (9%)] Loss: -239232.062500\n",
      "Train Epoch: 74 [2944/17352 (17%)] Loss: -238700.609375\n",
      "Train Epoch: 74 [4352/17352 (25%)] Loss: -239923.906250\n",
      "Train Epoch: 74 [5760/17352 (33%)] Loss: -240308.156250\n",
      "Train Epoch: 74 [7168/17352 (41%)] Loss: -239440.812500\n",
      "Train Epoch: 74 [8576/17352 (49%)] Loss: -240840.359375\n",
      "Train Epoch: 74 [9984/17352 (58%)] Loss: -240496.656250\n",
      "Train Epoch: 74 [11392/17352 (66%)] Loss: -241375.937500\n",
      "Train Epoch: 74 [12800/17352 (74%)] Loss: -242218.546875\n",
      "Train Epoch: 74 [14208/17352 (82%)] Loss: -243236.906250\n",
      "Train Epoch: 74 [15616/17352 (90%)] Loss: -241863.875000\n",
      "Train Epoch: 74 [17024/17352 (98%)] Loss: -241440.140625\n",
      "    epoch          : 74\n",
      "    loss           : -239843.2575827206\n",
      "    val_loss       : -227919.90633106232\n",
      "Train Epoch: 75 [128/17352 (1%)] Loss: -242145.218750\n",
      "Train Epoch: 75 [1536/17352 (9%)] Loss: -241902.187500\n",
      "Train Epoch: 75 [2944/17352 (17%)] Loss: -243772.281250\n",
      "Train Epoch: 75 [4352/17352 (25%)] Loss: -242973.093750\n",
      "Train Epoch: 75 [5760/17352 (33%)] Loss: -242695.531250\n",
      "Train Epoch: 75 [7168/17352 (41%)] Loss: -244005.500000\n",
      "Train Epoch: 75 [8576/17352 (49%)] Loss: -243429.031250\n",
      "Train Epoch: 75 [9984/17352 (58%)] Loss: -245317.250000\n",
      "Train Epoch: 75 [11392/17352 (66%)] Loss: -244452.343750\n",
      "Train Epoch: 75 [12800/17352 (74%)] Loss: -245267.625000\n",
      "Train Epoch: 75 [14208/17352 (82%)] Loss: -244956.187500\n",
      "Train Epoch: 75 [15616/17352 (90%)] Loss: -245229.656250\n",
      "Train Epoch: 75 [17024/17352 (98%)] Loss: -246223.281250\n",
      "    epoch          : 75\n",
      "    loss           : -242975.88534007352\n",
      "    val_loss       : -230873.53048992157\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0703_171824/checkpoint-epoch75.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 76 [128/17352 (1%)] Loss: -245193.062500\n",
      "Train Epoch: 76 [1536/17352 (9%)] Loss: -244669.687500\n",
      "Train Epoch: 76 [2944/17352 (17%)] Loss: -246235.796875\n",
      "Train Epoch: 76 [4352/17352 (25%)] Loss: -245782.281250\n",
      "Train Epoch: 76 [5760/17352 (33%)] Loss: -247021.453125\n",
      "Train Epoch: 76 [7168/17352 (41%)] Loss: -246945.250000\n",
      "Train Epoch: 76 [8576/17352 (49%)] Loss: -246426.187500\n",
      "Train Epoch: 76 [9984/17352 (58%)] Loss: -247919.875000\n",
      "Train Epoch: 76 [11392/17352 (66%)] Loss: -245999.250000\n",
      "Train Epoch: 76 [12800/17352 (74%)] Loss: -247863.531250\n",
      "Train Epoch: 76 [14208/17352 (82%)] Loss: -247980.890625\n",
      "Train Epoch: 76 [15616/17352 (90%)] Loss: -248600.328125\n",
      "Train Epoch: 76 [17024/17352 (98%)] Loss: -247653.921875\n",
      "    epoch          : 76\n",
      "    loss           : -246099.80124080883\n",
      "    val_loss       : -233829.7929315567\n",
      "Train Epoch: 77 [128/17352 (1%)] Loss: -249296.125000\n",
      "Train Epoch: 77 [1536/17352 (9%)] Loss: -249198.578125\n",
      "Train Epoch: 77 [2944/17352 (17%)] Loss: -248458.390625\n",
      "Train Epoch: 77 [4352/17352 (25%)] Loss: -249502.968750\n",
      "Train Epoch: 77 [5760/17352 (33%)] Loss: -249940.109375\n",
      "Train Epoch: 77 [7168/17352 (41%)] Loss: -249200.906250\n",
      "Train Epoch: 77 [8576/17352 (49%)] Loss: -248515.703125\n",
      "Train Epoch: 77 [9984/17352 (58%)] Loss: -250955.531250\n",
      "Train Epoch: 77 [11392/17352 (66%)] Loss: -251391.281250\n",
      "Train Epoch: 77 [12800/17352 (74%)] Loss: -249969.953125\n",
      "Train Epoch: 77 [14208/17352 (82%)] Loss: -251334.531250\n",
      "Train Epoch: 77 [15616/17352 (90%)] Loss: -251959.218750\n",
      "Train Epoch: 77 [17024/17352 (98%)] Loss: -251751.000000\n",
      "    epoch          : 77\n",
      "    loss           : -249215.0916819853\n",
      "    val_loss       : -236731.44294166565\n",
      "Train Epoch: 78 [128/17352 (1%)] Loss: -251856.281250\n",
      "Train Epoch: 78 [1536/17352 (9%)] Loss: -252278.515625\n",
      "Train Epoch: 78 [2944/17352 (17%)] Loss: -252789.437500\n",
      "Train Epoch: 78 [4352/17352 (25%)] Loss: -253336.546875\n",
      "Train Epoch: 78 [5760/17352 (33%)] Loss: -252022.046875\n",
      "Train Epoch: 78 [7168/17352 (41%)] Loss: -252739.515625\n",
      "Train Epoch: 78 [8576/17352 (49%)] Loss: -252641.156250\n",
      "Train Epoch: 78 [9984/17352 (58%)] Loss: -253287.656250\n",
      "Train Epoch: 78 [11392/17352 (66%)] Loss: -253035.093750\n",
      "Train Epoch: 78 [12800/17352 (74%)] Loss: -253803.875000\n",
      "Train Epoch: 78 [14208/17352 (82%)] Loss: -255164.796875\n",
      "Train Epoch: 78 [15616/17352 (90%)] Loss: -253988.468750\n",
      "Train Epoch: 78 [17024/17352 (98%)] Loss: -255036.593750\n",
      "    epoch          : 78\n",
      "    loss           : -252326.2912454044\n",
      "    val_loss       : -239672.98495292664\n",
      "Train Epoch: 79 [128/17352 (1%)] Loss: -253989.390625\n",
      "Train Epoch: 79 [1536/17352 (9%)] Loss: -255476.359375\n",
      "Train Epoch: 79 [2944/17352 (17%)] Loss: -255377.531250\n",
      "Train Epoch: 79 [4352/17352 (25%)] Loss: -255127.296875\n",
      "Train Epoch: 79 [5760/17352 (33%)] Loss: -255809.406250\n",
      "Train Epoch: 79 [7168/17352 (41%)] Loss: -255897.828125\n",
      "Train Epoch: 79 [8576/17352 (49%)] Loss: -255396.328125\n",
      "Train Epoch: 79 [9984/17352 (58%)] Loss: -256915.625000\n",
      "Train Epoch: 79 [11392/17352 (66%)] Loss: -256897.312500\n",
      "Train Epoch: 79 [12800/17352 (74%)] Loss: -257673.500000\n",
      "Train Epoch: 79 [14208/17352 (82%)] Loss: -255590.687500\n",
      "Train Epoch: 79 [15616/17352 (90%)] Loss: -257676.578125\n",
      "Train Epoch: 79 [17024/17352 (98%)] Loss: -257832.734375\n",
      "    epoch          : 79\n",
      "    loss           : -255416.06950827205\n",
      "    val_loss       : -242577.59239006042\n",
      "Train Epoch: 80 [128/17352 (1%)] Loss: -256520.828125\n",
      "Train Epoch: 80 [1536/17352 (9%)] Loss: -257936.281250\n",
      "Train Epoch: 80 [2944/17352 (17%)] Loss: -258804.093750\n",
      "Train Epoch: 80 [4352/17352 (25%)] Loss: -259790.187500\n",
      "Train Epoch: 80 [5760/17352 (33%)] Loss: -258963.640625\n",
      "Train Epoch: 80 [7168/17352 (41%)] Loss: -258423.062500\n",
      "Train Epoch: 80 [8576/17352 (49%)] Loss: -259024.515625\n",
      "Train Epoch: 80 [9984/17352 (58%)] Loss: -260431.421875\n",
      "Train Epoch: 80 [11392/17352 (66%)] Loss: -260812.906250\n",
      "Train Epoch: 80 [12800/17352 (74%)] Loss: -259074.031250\n",
      "Train Epoch: 80 [14208/17352 (82%)] Loss: -260532.484375\n",
      "Train Epoch: 80 [15616/17352 (90%)] Loss: -261105.562500\n",
      "Train Epoch: 80 [17024/17352 (98%)] Loss: -261218.140625\n",
      "    epoch          : 80\n",
      "    loss           : -258499.88637408087\n",
      "    val_loss       : -245423.42257118225\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0703_171824/checkpoint-epoch80.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 81 [128/17352 (1%)] Loss: -259933.781250\n",
      "Train Epoch: 81 [1536/17352 (9%)] Loss: -261847.390625\n",
      "Train Epoch: 81 [2944/17352 (17%)] Loss: -261798.937500\n",
      "Train Epoch: 81 [4352/17352 (25%)] Loss: -261433.562500\n",
      "Train Epoch: 81 [5760/17352 (33%)] Loss: -263257.656250\n",
      "Train Epoch: 81 [7168/17352 (41%)] Loss: -262706.062500\n",
      "Train Epoch: 81 [8576/17352 (49%)] Loss: -262538.250000\n",
      "Train Epoch: 81 [9984/17352 (58%)] Loss: -262592.500000\n",
      "Train Epoch: 81 [11392/17352 (66%)] Loss: -262271.500000\n",
      "Train Epoch: 81 [12800/17352 (74%)] Loss: -263880.500000\n",
      "Train Epoch: 81 [14208/17352 (82%)] Loss: -264176.687500\n",
      "Train Epoch: 81 [15616/17352 (90%)] Loss: -264529.875000\n",
      "Train Epoch: 81 [17024/17352 (98%)] Loss: -264252.562500\n",
      "    epoch          : 81\n",
      "    loss           : -261572.32858455883\n",
      "    val_loss       : -248286.9020872116\n",
      "Train Epoch: 82 [128/17352 (1%)] Loss: -263791.968750\n",
      "Train Epoch: 82 [1536/17352 (9%)] Loss: -264083.968750\n",
      "Train Epoch: 82 [2944/17352 (17%)] Loss: -264333.968750\n",
      "Train Epoch: 82 [4352/17352 (25%)] Loss: -265868.562500\n",
      "Train Epoch: 82 [5760/17352 (33%)] Loss: -264216.593750\n",
      "Train Epoch: 82 [7168/17352 (41%)] Loss: -265613.500000\n",
      "Train Epoch: 82 [8576/17352 (49%)] Loss: -265653.812500\n",
      "Train Epoch: 82 [9984/17352 (58%)] Loss: -265906.125000\n",
      "Train Epoch: 82 [11392/17352 (66%)] Loss: -266505.218750\n",
      "Train Epoch: 82 [12800/17352 (74%)] Loss: -265856.343750\n",
      "Train Epoch: 82 [14208/17352 (82%)] Loss: -266317.343750\n",
      "Train Epoch: 82 [15616/17352 (90%)] Loss: -267272.062500\n",
      "Train Epoch: 82 [17024/17352 (98%)] Loss: -265431.343750\n",
      "    epoch          : 82\n",
      "    loss           : -264645.2184053309\n",
      "    val_loss       : -251293.7842092514\n",
      "Train Epoch: 83 [128/17352 (1%)] Loss: -266863.375000\n",
      "Train Epoch: 83 [1536/17352 (9%)] Loss: -268886.718750\n",
      "Train Epoch: 83 [2944/17352 (17%)] Loss: -266513.875000\n",
      "Train Epoch: 83 [4352/17352 (25%)] Loss: -268941.812500\n",
      "Train Epoch: 83 [5760/17352 (33%)] Loss: -269329.187500\n",
      "Train Epoch: 83 [7168/17352 (41%)] Loss: -268618.156250\n",
      "Train Epoch: 83 [8576/17352 (49%)] Loss: -268895.312500\n",
      "Train Epoch: 83 [9984/17352 (58%)] Loss: -268746.750000\n",
      "Train Epoch: 83 [11392/17352 (66%)] Loss: -267711.062500\n",
      "Train Epoch: 83 [12800/17352 (74%)] Loss: -268398.531250\n",
      "Train Epoch: 83 [14208/17352 (82%)] Loss: -269729.312500\n",
      "Train Epoch: 83 [15616/17352 (90%)] Loss: -271663.156250\n",
      "Train Epoch: 83 [17024/17352 (98%)] Loss: -269996.593750\n",
      "    epoch          : 83\n",
      "    loss           : -267704.9942555147\n",
      "    val_loss       : -254139.83547019958\n",
      "Train Epoch: 84 [128/17352 (1%)] Loss: -270119.906250\n",
      "Train Epoch: 84 [1536/17352 (9%)] Loss: -269205.656250\n",
      "Train Epoch: 84 [2944/17352 (17%)] Loss: -269329.062500\n",
      "Train Epoch: 84 [4352/17352 (25%)] Loss: -270504.437500\n",
      "Train Epoch: 84 [5760/17352 (33%)] Loss: -271442.625000\n",
      "Train Epoch: 84 [7168/17352 (41%)] Loss: -270860.312500\n",
      "Train Epoch: 84 [8576/17352 (49%)] Loss: -270655.687500\n",
      "Train Epoch: 84 [9984/17352 (58%)] Loss: -271635.875000\n",
      "Train Epoch: 84 [11392/17352 (66%)] Loss: -270512.500000\n",
      "Train Epoch: 84 [12800/17352 (74%)] Loss: -270955.125000\n",
      "Train Epoch: 84 [14208/17352 (82%)] Loss: -271837.375000\n",
      "Train Epoch: 84 [15616/17352 (90%)] Loss: -273348.562500\n",
      "Train Epoch: 84 [17024/17352 (98%)] Loss: -273137.718750\n",
      "    epoch          : 84\n",
      "    loss           : -270758.77596507355\n",
      "    val_loss       : -256984.84553909302\n",
      "Train Epoch: 85 [128/17352 (1%)] Loss: -274136.875000\n",
      "Train Epoch: 85 [1536/17352 (9%)] Loss: -272943.531250\n",
      "Train Epoch: 85 [2944/17352 (17%)] Loss: -272407.125000\n",
      "Train Epoch: 85 [4352/17352 (25%)] Loss: -272184.843750\n",
      "Train Epoch: 85 [5760/17352 (33%)] Loss: -273907.250000\n",
      "Train Epoch: 85 [7168/17352 (41%)] Loss: -275291.875000\n",
      "Train Epoch: 85 [8576/17352 (49%)] Loss: -274578.906250\n",
      "Train Epoch: 85 [9984/17352 (58%)] Loss: -274587.187500\n",
      "Train Epoch: 85 [11392/17352 (66%)] Loss: -275362.687500\n",
      "Train Epoch: 85 [12800/17352 (74%)] Loss: -275563.687500\n",
      "Train Epoch: 85 [14208/17352 (82%)] Loss: -275267.625000\n",
      "Train Epoch: 85 [15616/17352 (90%)] Loss: -275652.125000\n",
      "Train Epoch: 85 [17024/17352 (98%)] Loss: -275444.062500\n",
      "    epoch          : 85\n",
      "    loss           : -273801.8304227941\n",
      "    val_loss       : -259885.72013282776\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0703_171824/checkpoint-epoch85.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 86 [128/17352 (1%)] Loss: -277044.656250\n",
      "Train Epoch: 86 [1536/17352 (9%)] Loss: -277396.343750\n",
      "Train Epoch: 86 [2944/17352 (17%)] Loss: -276135.937500\n",
      "Train Epoch: 86 [4352/17352 (25%)] Loss: -277788.156250\n",
      "Train Epoch: 86 [5760/17352 (33%)] Loss: -276222.656250\n",
      "Train Epoch: 86 [7168/17352 (41%)] Loss: -277092.187500\n",
      "Train Epoch: 86 [8576/17352 (49%)] Loss: -278272.031250\n",
      "Train Epoch: 86 [9984/17352 (58%)] Loss: -277883.906250\n",
      "Train Epoch: 86 [11392/17352 (66%)] Loss: -278707.281250\n",
      "Train Epoch: 86 [12800/17352 (74%)] Loss: -279156.500000\n",
      "Train Epoch: 86 [14208/17352 (82%)] Loss: -278416.937500\n",
      "Train Epoch: 86 [15616/17352 (90%)] Loss: -279447.000000\n",
      "Train Epoch: 86 [17024/17352 (98%)] Loss: -278940.312500\n",
      "    epoch          : 86\n",
      "    loss           : -276831.45255055145\n",
      "    val_loss       : -262764.86797237396\n",
      "Train Epoch: 87 [128/17352 (1%)] Loss: -280650.781250\n",
      "Train Epoch: 87 [1536/17352 (9%)] Loss: -279739.812500\n",
      "Train Epoch: 87 [2944/17352 (17%)] Loss: -280039.437500\n",
      "Train Epoch: 87 [4352/17352 (25%)] Loss: -280750.468750\n",
      "Train Epoch: 87 [5760/17352 (33%)] Loss: -280800.593750\n",
      "Train Epoch: 87 [7168/17352 (41%)] Loss: -281528.218750\n",
      "Train Epoch: 87 [8576/17352 (49%)] Loss: -280623.125000\n",
      "Train Epoch: 87 [9984/17352 (58%)] Loss: -281504.375000\n",
      "Train Epoch: 87 [11392/17352 (66%)] Loss: -282353.937500\n",
      "Train Epoch: 87 [12800/17352 (74%)] Loss: -280375.062500\n",
      "Train Epoch: 87 [14208/17352 (82%)] Loss: -282865.500000\n",
      "Train Epoch: 87 [15616/17352 (90%)] Loss: -281857.156250\n",
      "Train Epoch: 87 [17024/17352 (98%)] Loss: -282474.437500\n",
      "    epoch          : 87\n",
      "    loss           : -279853.7889476103\n",
      "    val_loss       : -265534.2642297745\n",
      "Train Epoch: 88 [128/17352 (1%)] Loss: -283330.937500\n",
      "Train Epoch: 88 [1536/17352 (9%)] Loss: -281406.312500\n",
      "Train Epoch: 88 [2944/17352 (17%)] Loss: -283110.093750\n",
      "Train Epoch: 88 [4352/17352 (25%)] Loss: -282160.406250\n",
      "Train Epoch: 88 [5760/17352 (33%)] Loss: -282581.250000\n",
      "Train Epoch: 88 [7168/17352 (41%)] Loss: -282004.781250\n",
      "Train Epoch: 88 [8576/17352 (49%)] Loss: -284274.875000\n",
      "Train Epoch: 88 [9984/17352 (58%)] Loss: -283554.625000\n",
      "Train Epoch: 88 [11392/17352 (66%)] Loss: -285615.468750\n",
      "Train Epoch: 88 [12800/17352 (74%)] Loss: -284487.437500\n",
      "Train Epoch: 88 [14208/17352 (82%)] Loss: -285665.187500\n",
      "Train Epoch: 88 [15616/17352 (90%)] Loss: -283821.968750\n",
      "Train Epoch: 88 [17024/17352 (98%)] Loss: -286605.312500\n",
      "    epoch          : 88\n",
      "    loss           : -282870.87247242645\n",
      "    val_loss       : -268403.6315193176\n",
      "Train Epoch: 89 [128/17352 (1%)] Loss: -285024.812500\n",
      "Train Epoch: 89 [1536/17352 (9%)] Loss: -285503.687500\n",
      "Train Epoch: 89 [2944/17352 (17%)] Loss: -284266.843750\n",
      "Train Epoch: 89 [4352/17352 (25%)] Loss: -285768.531250\n",
      "Train Epoch: 89 [5760/17352 (33%)] Loss: -284733.406250\n",
      "Train Epoch: 89 [7168/17352 (41%)] Loss: -286320.000000\n",
      "Train Epoch: 89 [8576/17352 (49%)] Loss: -287069.187500\n",
      "Train Epoch: 89 [9984/17352 (58%)] Loss: -285948.687500\n",
      "Train Epoch: 89 [11392/17352 (66%)] Loss: -287333.562500\n",
      "Train Epoch: 89 [12800/17352 (74%)] Loss: -287505.718750\n",
      "Train Epoch: 89 [14208/17352 (82%)] Loss: -287159.687500\n",
      "Train Epoch: 89 [15616/17352 (90%)] Loss: -288679.156250\n",
      "Train Epoch: 89 [17024/17352 (98%)] Loss: -287342.968750\n",
      "    epoch          : 89\n",
      "    loss           : -285874.8903952206\n",
      "    val_loss       : -271237.3196258545\n",
      "Train Epoch: 90 [128/17352 (1%)] Loss: -288522.843750\n",
      "Train Epoch: 90 [1536/17352 (9%)] Loss: -286154.187500\n",
      "Train Epoch: 90 [2944/17352 (17%)] Loss: -287719.781250\n",
      "Train Epoch: 90 [4352/17352 (25%)] Loss: -288915.968750\n",
      "Train Epoch: 90 [5760/17352 (33%)] Loss: -290805.062500\n",
      "Train Epoch: 90 [7168/17352 (41%)] Loss: -289269.375000\n",
      "Train Epoch: 90 [8576/17352 (49%)] Loss: -289063.093750\n",
      "Train Epoch: 90 [9984/17352 (58%)] Loss: -290211.843750\n",
      "Train Epoch: 90 [11392/17352 (66%)] Loss: -292256.500000\n",
      "Train Epoch: 90 [12800/17352 (74%)] Loss: -291202.406250\n",
      "Train Epoch: 90 [14208/17352 (82%)] Loss: -290493.250000\n",
      "Train Epoch: 90 [15616/17352 (90%)] Loss: -292323.343750\n",
      "Train Epoch: 90 [17024/17352 (98%)] Loss: -290574.937500\n",
      "    epoch          : 90\n",
      "    loss           : -288864.63373161765\n",
      "    val_loss       : -274116.1682033539\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0703_171824/checkpoint-epoch90.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 91 [128/17352 (1%)] Loss: -291990.875000\n",
      "Train Epoch: 91 [1536/17352 (9%)] Loss: -291732.031250\n",
      "Train Epoch: 91 [2944/17352 (17%)] Loss: -292455.000000\n",
      "Train Epoch: 91 [4352/17352 (25%)] Loss: -291076.000000\n",
      "Train Epoch: 91 [5760/17352 (33%)] Loss: -292848.375000\n",
      "Train Epoch: 91 [7168/17352 (41%)] Loss: -293809.687500\n",
      "Train Epoch: 91 [8576/17352 (49%)] Loss: -292817.531250\n",
      "Train Epoch: 91 [9984/17352 (58%)] Loss: -293359.250000\n",
      "Train Epoch: 91 [11392/17352 (66%)] Loss: -294146.031250\n",
      "Train Epoch: 91 [12800/17352 (74%)] Loss: -293044.156250\n",
      "Train Epoch: 91 [14208/17352 (82%)] Loss: -293671.343750\n",
      "Train Epoch: 91 [15616/17352 (90%)] Loss: -293615.500000\n",
      "Train Epoch: 91 [17024/17352 (98%)] Loss: -294835.500000\n",
      "    epoch          : 91\n",
      "    loss           : -291500.12178308825\n",
      "    val_loss       : -276905.53983306885\n",
      "Train Epoch: 92 [128/17352 (1%)] Loss: -293725.500000\n",
      "Train Epoch: 92 [1536/17352 (9%)] Loss: -294484.468750\n",
      "Train Epoch: 92 [2944/17352 (17%)] Loss: -294180.312500\n",
      "Train Epoch: 92 [4352/17352 (25%)] Loss: -294267.812500\n",
      "Train Epoch: 92 [5760/17352 (33%)] Loss: -295737.375000\n",
      "Train Epoch: 92 [7168/17352 (41%)] Loss: -294551.906250\n",
      "Train Epoch: 92 [8576/17352 (49%)] Loss: -295998.093750\n",
      "Train Epoch: 92 [9984/17352 (58%)] Loss: -296749.125000\n",
      "Train Epoch: 92 [11392/17352 (66%)] Loss: -297369.375000\n",
      "Train Epoch: 92 [12800/17352 (74%)] Loss: -297858.875000\n",
      "Train Epoch: 92 [14208/17352 (82%)] Loss: -298085.031250\n",
      "Train Epoch: 92 [15616/17352 (90%)] Loss: -297797.562500\n",
      "Train Epoch: 92 [17024/17352 (98%)] Loss: -297625.281250\n",
      "    epoch          : 92\n",
      "    loss           : -294462.92715992645\n",
      "    val_loss       : -279695.39206790924\n",
      "Train Epoch: 93 [128/17352 (1%)] Loss: -297730.687500\n",
      "Train Epoch: 93 [1536/17352 (9%)] Loss: -297509.437500\n",
      "Train Epoch: 93 [2944/17352 (17%)] Loss: -299436.437500\n",
      "Train Epoch: 93 [4352/17352 (25%)] Loss: -299129.500000\n",
      "Train Epoch: 93 [5760/17352 (33%)] Loss: -297488.437500\n",
      "Train Epoch: 93 [7168/17352 (41%)] Loss: -297944.031250\n",
      "Train Epoch: 93 [8576/17352 (49%)] Loss: -299103.062500\n",
      "Train Epoch: 93 [9984/17352 (58%)] Loss: -299903.218750\n",
      "Train Epoch: 93 [11392/17352 (66%)] Loss: -298317.500000\n",
      "Train Epoch: 93 [12800/17352 (74%)] Loss: -299239.656250\n",
      "Train Epoch: 93 [14208/17352 (82%)] Loss: -298594.187500\n",
      "Train Epoch: 93 [15616/17352 (90%)] Loss: -298604.281250\n",
      "Train Epoch: 93 [17024/17352 (98%)] Loss: -300838.218750\n",
      "    epoch          : 93\n",
      "    loss           : -297808.8023897059\n",
      "    val_loss       : -282515.99179267883\n",
      "Train Epoch: 94 [128/17352 (1%)] Loss: -299246.156250\n",
      "Train Epoch: 94 [1536/17352 (9%)] Loss: -300068.281250\n",
      "Train Epoch: 94 [2944/17352 (17%)] Loss: -301571.812500\n",
      "Train Epoch: 94 [4352/17352 (25%)] Loss: -300794.812500\n",
      "Train Epoch: 94 [5760/17352 (33%)] Loss: -302427.437500\n",
      "Train Epoch: 94 [7168/17352 (41%)] Loss: -301724.875000\n",
      "Train Epoch: 94 [8576/17352 (49%)] Loss: -302006.531250\n",
      "Train Epoch: 94 [9984/17352 (58%)] Loss: -301844.781250\n",
      "Train Epoch: 94 [11392/17352 (66%)] Loss: -301903.812500\n",
      "Train Epoch: 94 [12800/17352 (74%)] Loss: -301934.812500\n",
      "Train Epoch: 94 [14208/17352 (82%)] Loss: -303308.718750\n",
      "Train Epoch: 94 [15616/17352 (90%)] Loss: -302046.000000\n",
      "Train Epoch: 94 [17024/17352 (98%)] Loss: -304181.218750\n",
      "    epoch          : 94\n",
      "    loss           : -300768.578125\n",
      "    val_loss       : -285325.99774742126\n",
      "Train Epoch: 95 [128/17352 (1%)] Loss: -303018.250000\n",
      "Train Epoch: 95 [1536/17352 (9%)] Loss: -305588.250000\n",
      "Train Epoch: 95 [2944/17352 (17%)] Loss: -303619.937500\n",
      "Train Epoch: 95 [4352/17352 (25%)] Loss: -303912.875000\n",
      "Train Epoch: 95 [5760/17352 (33%)] Loss: -303250.437500\n",
      "Train Epoch: 95 [7168/17352 (41%)] Loss: -304753.875000\n",
      "Train Epoch: 95 [8576/17352 (49%)] Loss: -303033.562500\n",
      "Train Epoch: 95 [9984/17352 (58%)] Loss: -305072.750000\n",
      "Train Epoch: 95 [11392/17352 (66%)] Loss: -306116.031250\n",
      "Train Epoch: 95 [12800/17352 (74%)] Loss: -305374.312500\n",
      "Train Epoch: 95 [14208/17352 (82%)] Loss: -304572.437500\n",
      "Train Epoch: 95 [15616/17352 (90%)] Loss: -305701.875000\n",
      "Train Epoch: 95 [17024/17352 (98%)] Loss: -307413.062500\n",
      "    epoch          : 95\n",
      "    loss           : -303725.2872242647\n",
      "    val_loss       : -288052.7716264725\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0703_171824/checkpoint-epoch95.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 96 [128/17352 (1%)] Loss: -305674.000000\n",
      "Train Epoch: 96 [1536/17352 (9%)] Loss: -307718.812500\n",
      "Train Epoch: 96 [2944/17352 (17%)] Loss: -306874.437500\n",
      "Train Epoch: 96 [4352/17352 (25%)] Loss: -306273.000000\n",
      "Train Epoch: 96 [5760/17352 (33%)] Loss: -307302.093750\n",
      "Train Epoch: 96 [7168/17352 (41%)] Loss: -306678.593750\n",
      "Train Epoch: 96 [8576/17352 (49%)] Loss: -308305.125000\n",
      "Train Epoch: 96 [9984/17352 (58%)] Loss: -307559.406250\n",
      "Train Epoch: 96 [11392/17352 (66%)] Loss: -309161.718750\n",
      "Train Epoch: 96 [12800/17352 (74%)] Loss: -309757.281250\n",
      "Train Epoch: 96 [14208/17352 (82%)] Loss: -308393.500000\n",
      "Train Epoch: 96 [15616/17352 (90%)] Loss: -308737.375000\n",
      "Train Epoch: 96 [17024/17352 (98%)] Loss: -307761.656250\n",
      "    epoch          : 96\n",
      "    loss           : -306665.54997702205\n",
      "    val_loss       : -290832.9069185257\n",
      "Train Epoch: 97 [128/17352 (1%)] Loss: -308428.781250\n",
      "Train Epoch: 97 [1536/17352 (9%)] Loss: -308619.718750\n",
      "Train Epoch: 97 [2944/17352 (17%)] Loss: -308971.812500\n",
      "Train Epoch: 97 [4352/17352 (25%)] Loss: -310437.156250\n",
      "Train Epoch: 97 [5760/17352 (33%)] Loss: -309053.375000\n",
      "Train Epoch: 97 [7168/17352 (41%)] Loss: -310247.562500\n",
      "Train Epoch: 97 [8576/17352 (49%)] Loss: -310955.375000\n",
      "Train Epoch: 97 [9984/17352 (58%)] Loss: -309693.250000\n",
      "Train Epoch: 97 [11392/17352 (66%)] Loss: -311001.750000\n",
      "Train Epoch: 97 [12800/17352 (74%)] Loss: -311553.437500\n",
      "Train Epoch: 97 [14208/17352 (82%)] Loss: -312625.750000\n",
      "Train Epoch: 97 [15616/17352 (90%)] Loss: -311450.656250\n",
      "Train Epoch: 97 [17024/17352 (98%)] Loss: -309738.375000\n",
      "    epoch          : 97\n",
      "    loss           : -309602.3941865809\n",
      "    val_loss       : -293598.83598327637\n",
      "Train Epoch: 98 [128/17352 (1%)] Loss: -312138.500000\n",
      "Train Epoch: 98 [1536/17352 (9%)] Loss: -313575.281250\n",
      "Train Epoch: 98 [2944/17352 (17%)] Loss: -312924.375000\n",
      "Train Epoch: 98 [4352/17352 (25%)] Loss: -314139.500000\n",
      "Train Epoch: 98 [5760/17352 (33%)] Loss: -313748.625000\n",
      "Train Epoch: 98 [7168/17352 (41%)] Loss: -313599.406250\n",
      "Train Epoch: 98 [8576/17352 (49%)] Loss: -313350.125000\n",
      "Train Epoch: 98 [9984/17352 (58%)] Loss: -312132.875000\n",
      "Train Epoch: 98 [11392/17352 (66%)] Loss: -314176.062500\n",
      "Train Epoch: 98 [12800/17352 (74%)] Loss: -314974.437500\n",
      "Train Epoch: 98 [14208/17352 (82%)] Loss: -316518.562500\n",
      "Train Epoch: 98 [15616/17352 (90%)] Loss: -313498.187500\n",
      "Train Epoch: 98 [17024/17352 (98%)] Loss: -314262.687500\n",
      "    epoch          : 98\n",
      "    loss           : -312529.306640625\n",
      "    val_loss       : -296376.5442867279\n",
      "Train Epoch: 99 [128/17352 (1%)] Loss: -314593.312500\n",
      "Train Epoch: 99 [1536/17352 (9%)] Loss: -315138.437500\n",
      "Train Epoch: 99 [2944/17352 (17%)] Loss: -315703.031250\n",
      "Train Epoch: 99 [4352/17352 (25%)] Loss: -316113.375000\n",
      "Train Epoch: 99 [5760/17352 (33%)] Loss: -316479.156250\n",
      "Train Epoch: 99 [7168/17352 (41%)] Loss: -317172.000000\n",
      "Train Epoch: 99 [8576/17352 (49%)] Loss: -316920.156250\n",
      "Train Epoch: 99 [9984/17352 (58%)] Loss: -316039.750000\n",
      "Train Epoch: 99 [11392/17352 (66%)] Loss: -317399.812500\n",
      "Train Epoch: 99 [12800/17352 (74%)] Loss: -318216.093750\n",
      "Train Epoch: 99 [14208/17352 (82%)] Loss: -317933.593750\n",
      "Train Epoch: 99 [15616/17352 (90%)] Loss: -318558.656250\n",
      "Train Epoch: 99 [17024/17352 (98%)] Loss: -318172.875000\n",
      "    epoch          : 99\n",
      "    loss           : -315452.96978400735\n",
      "    val_loss       : -299133.32746982574\n",
      "Train Epoch: 100 [128/17352 (1%)] Loss: -317112.968750\n",
      "Train Epoch: 100 [1536/17352 (9%)] Loss: -318494.875000\n",
      "Train Epoch: 100 [2944/17352 (17%)] Loss: -318734.687500\n",
      "Train Epoch: 100 [4352/17352 (25%)] Loss: -318543.750000\n",
      "Train Epoch: 100 [5760/17352 (33%)] Loss: -317891.312500\n",
      "Train Epoch: 100 [7168/17352 (41%)] Loss: -319788.625000\n",
      "Train Epoch: 100 [8576/17352 (49%)] Loss: -318594.125000\n",
      "Train Epoch: 100 [9984/17352 (58%)] Loss: -318875.687500\n",
      "Train Epoch: 100 [11392/17352 (66%)] Loss: -319999.031250\n",
      "Train Epoch: 100 [12800/17352 (74%)] Loss: -319878.125000\n",
      "Train Epoch: 100 [14208/17352 (82%)] Loss: -319968.500000\n",
      "Train Epoch: 100 [15616/17352 (90%)] Loss: -320524.781250\n",
      "Train Epoch: 100 [17024/17352 (98%)] Loss: -321183.437500\n",
      "    epoch          : 100\n",
      "    loss           : -318363.2088694853\n",
      "    val_loss       : -301888.2140493393\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0703_171824/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
