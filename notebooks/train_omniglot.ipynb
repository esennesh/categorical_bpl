{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f3284425fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='omniglot_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)\n",
    "model.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/17352 (0%)] Loss: 88777.539062\n",
      "Train Epoch: 1 [1408/17352 (8%)] Loss: 52848.246094\n",
      "Train Epoch: 1 [2816/17352 (16%)] Loss: 43475.164062\n",
      "Train Epoch: 1 [4224/17352 (24%)] Loss: 145167.000000\n",
      "Train Epoch: 1 [5632/17352 (32%)] Loss: 30136.761719\n",
      "Train Epoch: 1 [7040/17352 (41%)] Loss: 7834.488281\n",
      "Train Epoch: 1 [8448/17352 (49%)] Loss: -13428.801758\n",
      "Train Epoch: 1 [9856/17352 (57%)] Loss: -32056.523438\n",
      "Train Epoch: 1 [11264/17352 (65%)] Loss: -44461.578125\n",
      "Train Epoch: 1 [12672/17352 (73%)] Loss: -51174.507812\n",
      "Train Epoch: 1 [14080/17352 (81%)] Loss: -145481.968750\n",
      "Train Epoch: 1 [15488/17352 (89%)] Loss: -91262.218750\n",
      "Train Epoch: 1 [16896/17352 (97%)] Loss: -196676.750000\n",
      "    epoch          : 1\n",
      "    loss           : -22729.966394761028\n",
      "    val_loss       : -197305.2221069336\n",
      "Train Epoch: 2 [0/17352 (0%)] Loss: -218704.531250\n",
      "Train Epoch: 2 [1408/17352 (8%)] Loss: -249482.437500\n",
      "Train Epoch: 2 [2816/17352 (16%)] Loss: -300564.531250\n",
      "Train Epoch: 2 [4224/17352 (24%)] Loss: -347306.156250\n",
      "Train Epoch: 2 [5632/17352 (32%)] Loss: -395631.812500\n",
      "Train Epoch: 2 [7040/17352 (41%)] Loss: -445981.062500\n",
      "Train Epoch: 2 [8448/17352 (49%)] Loss: -499920.187500\n",
      "Train Epoch: 2 [9856/17352 (57%)] Loss: -547466.125000\n",
      "Train Epoch: 2 [11264/17352 (65%)] Loss: -588571.000000\n",
      "Train Epoch: 2 [12672/17352 (73%)] Loss: -637227.812500\n",
      "Train Epoch: 2 [14080/17352 (81%)] Loss: -688376.937500\n",
      "Train Epoch: 2 [15488/17352 (89%)] Loss: -732005.875000\n",
      "Train Epoch: 2 [16896/17352 (97%)] Loss: -765013.437500\n",
      "    epoch          : 2\n",
      "    loss           : -493809.5028147978\n",
      "    val_loss       : -707175.9903259277\n",
      "Train Epoch: 3 [0/17352 (0%)] Loss: -787464.500000\n",
      "Train Epoch: 3 [1408/17352 (8%)] Loss: -834177.062500\n",
      "Train Epoch: 3 [2816/17352 (16%)] Loss: -869946.000000\n",
      "Train Epoch: 3 [4224/17352 (24%)] Loss: -913815.750000\n",
      "Train Epoch: 3 [5632/17352 (32%)] Loss: -964331.000000\n",
      "Train Epoch: 3 [7040/17352 (41%)] Loss: -987981.375000\n",
      "Train Epoch: 3 [8448/17352 (49%)] Loss: -1044814.875000\n",
      "Train Epoch: 3 [9856/17352 (57%)] Loss: -1072546.250000\n",
      "Train Epoch: 3 [11264/17352 (65%)] Loss: -1120345.750000\n",
      "Train Epoch: 3 [12672/17352 (73%)] Loss: -1174553.125000\n",
      "Train Epoch: 3 [14080/17352 (81%)] Loss: -1215836.250000\n",
      "Train Epoch: 3 [15488/17352 (89%)] Loss: -1242599.250000\n",
      "Train Epoch: 3 [16896/17352 (97%)] Loss: -1262928.875000\n",
      "    epoch          : 3\n",
      "    loss           : -1043426.3915441176\n",
      "    val_loss       : -1134648.4322814941\n",
      "Train Epoch: 4 [0/17352 (0%)] Loss: -1280038.500000\n",
      "Train Epoch: 4 [1408/17352 (8%)] Loss: -1317276.875000\n",
      "Train Epoch: 4 [2816/17352 (16%)] Loss: -1312975.750000\n",
      "Train Epoch: 4 [4224/17352 (24%)] Loss: -1317461.375000\n",
      "Train Epoch: 4 [5632/17352 (32%)] Loss: -1337868.375000\n",
      "Train Epoch: 4 [7040/17352 (41%)] Loss: -1325871.500000\n",
      "Train Epoch: 4 [8448/17352 (49%)] Loss: -1326075.750000\n",
      "Train Epoch: 4 [9856/17352 (57%)] Loss: -1336435.250000\n",
      "Train Epoch: 4 [11264/17352 (65%)] Loss: -1351281.750000\n",
      "Train Epoch: 4 [12672/17352 (73%)] Loss: -1351503.250000\n",
      "Train Epoch: 4 [14080/17352 (81%)] Loss: -1348338.000000\n",
      "Train Epoch: 4 [15488/17352 (89%)] Loss: -1333692.625000\n",
      "Train Epoch: 4 [16896/17352 (97%)] Loss: -1342772.500000\n",
      "    epoch          : 4\n",
      "    loss           : -1322628.859375\n",
      "    val_loss       : -1259869.826538086\n",
      "Train Epoch: 5 [0/17352 (0%)] Loss: -1343651.500000\n",
      "Train Epoch: 5 [1408/17352 (8%)] Loss: -1323773.250000\n",
      "Train Epoch: 5 [2816/17352 (16%)] Loss: -1328642.500000\n",
      "Train Epoch: 5 [4224/17352 (24%)] Loss: -1336723.250000\n",
      "Train Epoch: 5 [5632/17352 (32%)] Loss: -1333975.000000\n",
      "Train Epoch: 5 [7040/17352 (41%)] Loss: -1333771.500000\n",
      "Train Epoch: 5 [8448/17352 (49%)] Loss: -1348567.750000\n",
      "Train Epoch: 5 [9856/17352 (57%)] Loss: -1340910.250000\n",
      "Train Epoch: 5 [11264/17352 (65%)] Loss: -1345030.375000\n",
      "Train Epoch: 5 [12672/17352 (73%)] Loss: -1347542.000000\n",
      "Train Epoch: 5 [14080/17352 (81%)] Loss: -1359829.750000\n",
      "Train Epoch: 5 [15488/17352 (89%)] Loss: -1346541.125000\n",
      "Train Epoch: 5 [16896/17352 (97%)] Loss: -1331385.125000\n",
      "    epoch          : 5\n",
      "    loss           : -1337056.0974264706\n",
      "    val_loss       : -1262220.5488891602\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0427_093817/checkpoint-epoch5.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 6 [0/17352 (0%)] Loss: -1331288.375000\n",
      "Train Epoch: 6 [1408/17352 (8%)] Loss: -1343279.375000\n",
      "Train Epoch: 6 [2816/17352 (16%)] Loss: -1335208.000000\n",
      "Train Epoch: 6 [4224/17352 (24%)] Loss: -1336769.625000\n",
      "Train Epoch: 6 [5632/17352 (32%)] Loss: -1347289.000000\n",
      "Train Epoch: 6 [7040/17352 (41%)] Loss: -1349982.125000\n",
      "Train Epoch: 6 [8448/17352 (49%)] Loss: -1346264.750000\n",
      "Train Epoch: 6 [9856/17352 (57%)] Loss: -1351072.375000\n",
      "Train Epoch: 6 [11264/17352 (65%)] Loss: -1330883.875000\n",
      "Train Epoch: 6 [12672/17352 (73%)] Loss: -1353103.000000\n",
      "Train Epoch: 6 [14080/17352 (81%)] Loss: -1331126.500000\n",
      "Train Epoch: 6 [15488/17352 (89%)] Loss: -1333314.750000\n",
      "Train Epoch: 6 [16896/17352 (97%)] Loss: -1343761.875000\n",
      "    epoch          : 6\n",
      "    loss           : -1338092.5523897058\n",
      "    val_loss       : -1262459.1826782227\n",
      "Train Epoch: 7 [0/17352 (0%)] Loss: -1339274.750000\n",
      "Train Epoch: 7 [1408/17352 (8%)] Loss: -1348487.375000\n",
      "Train Epoch: 7 [2816/17352 (16%)] Loss: -1342508.125000\n",
      "Train Epoch: 7 [4224/17352 (24%)] Loss: -1337421.625000\n",
      "Train Epoch: 7 [5632/17352 (32%)] Loss: -1337936.875000\n",
      "Train Epoch: 7 [7040/17352 (41%)] Loss: -1345436.250000\n",
      "Train Epoch: 7 [8448/17352 (49%)] Loss: -1349246.500000\n",
      "Train Epoch: 7 [9856/17352 (57%)] Loss: -1346784.000000\n",
      "Train Epoch: 7 [11264/17352 (65%)] Loss: -1357122.625000\n",
      "Train Epoch: 7 [12672/17352 (73%)] Loss: -1343496.375000\n",
      "Train Epoch: 7 [14080/17352 (81%)] Loss: -1334986.375000\n",
      "Train Epoch: 7 [15488/17352 (89%)] Loss: -1332708.125000\n",
      "Train Epoch: 7 [16896/17352 (97%)] Loss: -1345726.250000\n",
      "    epoch          : 7\n",
      "    loss           : -1338309.7403492648\n",
      "    val_loss       : -1262684.4746704102\n",
      "Train Epoch: 8 [0/17352 (0%)] Loss: -1341525.125000\n",
      "Train Epoch: 8 [1408/17352 (8%)] Loss: -1350670.625000\n",
      "Train Epoch: 8 [2816/17352 (16%)] Loss: -1350826.500000\n",
      "Train Epoch: 8 [4224/17352 (24%)] Loss: -1336722.750000\n",
      "Train Epoch: 8 [5632/17352 (32%)] Loss: -1358864.000000\n",
      "Train Epoch: 8 [7040/17352 (41%)] Loss: -1365294.125000\n",
      "Train Epoch: 8 [8448/17352 (49%)] Loss: -1350786.125000\n",
      "Train Epoch: 8 [9856/17352 (57%)] Loss: -1341609.000000\n",
      "Train Epoch: 8 [11264/17352 (65%)] Loss: -1348584.000000\n",
      "Train Epoch: 8 [12672/17352 (73%)] Loss: -1356902.000000\n",
      "Train Epoch: 8 [14080/17352 (81%)] Loss: -1330526.625000\n",
      "Train Epoch: 8 [15488/17352 (89%)] Loss: -1338114.250000\n",
      "Train Epoch: 8 [16896/17352 (97%)] Loss: -1336677.875000\n",
      "    epoch          : 8\n",
      "    loss           : -1338575.8318014706\n",
      "    val_loss       : -1262950.945098877\n",
      "Train Epoch: 9 [0/17352 (0%)] Loss: -1339891.750000\n",
      "Train Epoch: 9 [1408/17352 (8%)] Loss: -1348422.500000\n",
      "Train Epoch: 9 [2816/17352 (16%)] Loss: -1321977.750000\n",
      "Train Epoch: 9 [4224/17352 (24%)] Loss: -1342752.375000\n",
      "Train Epoch: 9 [5632/17352 (32%)] Loss: -1333417.250000\n",
      "Train Epoch: 9 [7040/17352 (41%)] Loss: -1350383.375000\n",
      "Train Epoch: 9 [8448/17352 (49%)] Loss: -1337197.250000\n",
      "Train Epoch: 9 [9856/17352 (57%)] Loss: -1338327.625000\n",
      "Train Epoch: 9 [11264/17352 (65%)] Loss: -1350833.500000\n",
      "Train Epoch: 9 [12672/17352 (73%)] Loss: -1331683.875000\n",
      "Train Epoch: 9 [14080/17352 (81%)] Loss: -1358298.500000\n",
      "Train Epoch: 9 [15488/17352 (89%)] Loss: -1351039.500000\n",
      "Train Epoch: 9 [16896/17352 (97%)] Loss: -1348453.625000\n",
      "    epoch          : 9\n",
      "    loss           : -1338694.0657169118\n",
      "    val_loss       : -1262948.8121032715\n",
      "Train Epoch: 10 [0/17352 (0%)] Loss: -1341421.625000\n",
      "Train Epoch: 10 [1408/17352 (8%)] Loss: -1348283.750000\n",
      "Train Epoch: 10 [2816/17352 (16%)] Loss: -1348593.125000\n",
      "Train Epoch: 10 [4224/17352 (24%)] Loss: -1340595.250000\n",
      "Train Epoch: 10 [5632/17352 (32%)] Loss: -1336021.750000\n",
      "Train Epoch: 10 [7040/17352 (41%)] Loss: -1339295.625000\n",
      "Train Epoch: 10 [8448/17352 (49%)] Loss: -1343732.625000\n",
      "Train Epoch: 10 [9856/17352 (57%)] Loss: -1345470.500000\n",
      "Train Epoch: 10 [11264/17352 (65%)] Loss: -1350882.125000\n",
      "Train Epoch: 10 [12672/17352 (73%)] Loss: -1348727.250000\n",
      "Train Epoch: 10 [14080/17352 (81%)] Loss: -1339501.750000\n",
      "Train Epoch: 10 [15488/17352 (89%)] Loss: -1344977.375000\n",
      "Train Epoch: 10 [16896/17352 (97%)] Loss: -1338292.250000\n",
      "    epoch          : 10\n",
      "    loss           : -1338736.752757353\n",
      "    val_loss       : -1263054.180267334\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0427_093817/checkpoint-epoch10.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 11 [0/17352 (0%)] Loss: -1344321.625000\n",
      "Train Epoch: 11 [1408/17352 (8%)] Loss: -1338791.125000\n",
      "Train Epoch: 11 [2816/17352 (16%)] Loss: -1349173.625000\n",
      "Train Epoch: 11 [4224/17352 (24%)] Loss: -1335601.375000\n",
      "Train Epoch: 11 [5632/17352 (32%)] Loss: -1351124.000000\n",
      "Train Epoch: 11 [7040/17352 (41%)] Loss: -1352802.875000\n",
      "Train Epoch: 11 [8448/17352 (49%)] Loss: -1360087.750000\n",
      "Train Epoch: 11 [9856/17352 (57%)] Loss: -1346721.500000\n",
      "Train Epoch: 11 [11264/17352 (65%)] Loss: -1342092.375000\n",
      "Train Epoch: 11 [12672/17352 (73%)] Loss: -1335505.125000\n",
      "Train Epoch: 11 [14080/17352 (81%)] Loss: -1342115.625000\n",
      "Train Epoch: 11 [15488/17352 (89%)] Loss: -1338716.375000\n",
      "Train Epoch: 11 [16896/17352 (97%)] Loss: -1335390.500000\n",
      "    epoch          : 11\n",
      "    loss           : -1338807.7637867648\n",
      "    val_loss       : -1263084.9219665527\n",
      "Train Epoch: 12 [0/17352 (0%)] Loss: -1334903.250000\n",
      "Train Epoch: 12 [1408/17352 (8%)] Loss: -1339964.000000\n",
      "Train Epoch: 12 [2816/17352 (16%)] Loss: -1353937.875000\n",
      "Train Epoch: 12 [4224/17352 (24%)] Loss: -1348956.750000\n",
      "Train Epoch: 12 [5632/17352 (32%)] Loss: -1341660.625000\n",
      "Train Epoch: 12 [7040/17352 (41%)] Loss: -1341728.125000\n",
      "Train Epoch: 12 [8448/17352 (49%)] Loss: -1329446.500000\n",
      "Train Epoch: 12 [9856/17352 (57%)] Loss: -1349377.500000\n",
      "Train Epoch: 12 [11264/17352 (65%)] Loss: -1344308.875000\n",
      "Train Epoch: 12 [12672/17352 (73%)] Loss: -1343902.500000\n",
      "Train Epoch: 12 [14080/17352 (81%)] Loss: -1332037.250000\n",
      "Train Epoch: 12 [15488/17352 (89%)] Loss: -1344875.250000\n",
      "Train Epoch: 12 [16896/17352 (97%)] Loss: -1331104.375000\n",
      "    epoch          : 12\n",
      "    loss           : -1338837.3083639706\n",
      "    val_loss       : -1263141.7333984375\n",
      "Train Epoch: 13 [0/17352 (0%)] Loss: -1339605.125000\n",
      "Train Epoch: 13 [1408/17352 (8%)] Loss: -1347213.375000\n",
      "Train Epoch: 13 [2816/17352 (16%)] Loss: -1353281.500000\n",
      "Train Epoch: 13 [4224/17352 (24%)] Loss: -1341113.750000\n",
      "Train Epoch: 13 [5632/17352 (32%)] Loss: -1341789.750000\n",
      "Train Epoch: 13 [7040/17352 (41%)] Loss: -1340397.375000\n",
      "Train Epoch: 13 [8448/17352 (49%)] Loss: -1346219.000000\n",
      "Train Epoch: 13 [9856/17352 (57%)] Loss: -1343029.125000\n",
      "Train Epoch: 13 [11264/17352 (65%)] Loss: -1341007.875000\n",
      "Train Epoch: 13 [12672/17352 (73%)] Loss: -1350014.500000\n",
      "Train Epoch: 13 [14080/17352 (81%)] Loss: -1336423.750000\n",
      "Train Epoch: 13 [15488/17352 (89%)] Loss: -1343295.500000\n",
      "Train Epoch: 13 [16896/17352 (97%)] Loss: -1338701.125000\n",
      "    epoch          : 13\n",
      "    loss           : -1338873.8304227942\n",
      "    val_loss       : -1263173.129272461\n",
      "Train Epoch: 14 [0/17352 (0%)] Loss: -1337650.875000\n",
      "Train Epoch: 14 [1408/17352 (8%)] Loss: -1343432.000000\n",
      "Train Epoch: 14 [2816/17352 (16%)] Loss: -1337981.750000\n",
      "Train Epoch: 14 [4224/17352 (24%)] Loss: -1337229.500000\n",
      "Train Epoch: 14 [5632/17352 (32%)] Loss: -1341392.750000\n",
      "Train Epoch: 14 [7040/17352 (41%)] Loss: -1326799.125000\n",
      "Train Epoch: 14 [8448/17352 (49%)] Loss: -1334015.125000\n",
      "Train Epoch: 14 [9856/17352 (57%)] Loss: -1347472.500000\n",
      "Train Epoch: 14 [11264/17352 (65%)] Loss: -1343117.250000\n",
      "Train Epoch: 14 [12672/17352 (73%)] Loss: -1344344.250000\n",
      "Train Epoch: 14 [14080/17352 (81%)] Loss: -1336569.125000\n",
      "Train Epoch: 14 [15488/17352 (89%)] Loss: -1331569.250000\n",
      "Train Epoch: 14 [16896/17352 (97%)] Loss: -1348328.750000\n",
      "    epoch          : 14\n",
      "    loss           : -1338918.801930147\n",
      "    val_loss       : -1263185.1058654785\n",
      "Train Epoch: 15 [0/17352 (0%)] Loss: -1344005.750000\n",
      "Train Epoch: 15 [1408/17352 (8%)] Loss: -1341721.000000\n",
      "Train Epoch: 15 [2816/17352 (16%)] Loss: -1339765.250000\n",
      "Train Epoch: 15 [4224/17352 (24%)] Loss: -1339801.500000\n",
      "Train Epoch: 15 [5632/17352 (32%)] Loss: -1317637.375000\n",
      "Train Epoch: 15 [7040/17352 (41%)] Loss: -1343019.250000\n",
      "Train Epoch: 15 [8448/17352 (49%)] Loss: -1344450.500000\n",
      "Train Epoch: 15 [9856/17352 (57%)] Loss: -1344607.125000\n",
      "Train Epoch: 15 [11264/17352 (65%)] Loss: -1347745.625000\n",
      "Train Epoch: 15 [12672/17352 (73%)] Loss: -1362272.875000\n",
      "Train Epoch: 15 [14080/17352 (81%)] Loss: -1326119.000000\n",
      "Train Epoch: 15 [15488/17352 (89%)] Loss: -1345075.500000\n",
      "Train Epoch: 15 [16896/17352 (97%)] Loss: -1348343.625000\n",
      "    epoch          : 15\n",
      "    loss           : -1338925.948069853\n",
      "    val_loss       : -1263212.2769470215\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0427_093817/checkpoint-epoch15.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 16 [0/17352 (0%)] Loss: -1356817.375000\n",
      "Train Epoch: 16 [1408/17352 (8%)] Loss: -1337864.500000\n",
      "Train Epoch: 16 [2816/17352 (16%)] Loss: -1333291.375000\n",
      "Train Epoch: 16 [4224/17352 (24%)] Loss: -1356317.125000\n",
      "Train Epoch: 16 [5632/17352 (32%)] Loss: -1315039.500000\n",
      "Train Epoch: 16 [7040/17352 (41%)] Loss: -1346302.500000\n",
      "Train Epoch: 16 [8448/17352 (49%)] Loss: -1343585.000000\n",
      "Train Epoch: 16 [9856/17352 (57%)] Loss: -1341502.625000\n",
      "Train Epoch: 16 [11264/17352 (65%)] Loss: -1333192.750000\n",
      "Train Epoch: 16 [12672/17352 (73%)] Loss: -1335950.750000\n",
      "Train Epoch: 16 [14080/17352 (81%)] Loss: -1342991.500000\n",
      "Train Epoch: 16 [15488/17352 (89%)] Loss: -1342364.500000\n",
      "Train Epoch: 16 [16896/17352 (97%)] Loss: -1358167.875000\n",
      "    epoch          : 16\n",
      "    loss           : -1338959.4067095588\n",
      "    val_loss       : -1263244.7541809082\n",
      "Train Epoch: 17 [0/17352 (0%)] Loss: -1345823.625000\n",
      "Train Epoch: 17 [1408/17352 (8%)] Loss: -1328076.625000\n",
      "Train Epoch: 17 [2816/17352 (16%)] Loss: -1350764.000000\n",
      "Train Epoch: 17 [4224/17352 (24%)] Loss: -1346338.875000\n",
      "Train Epoch: 17 [5632/17352 (32%)] Loss: -1326672.375000\n",
      "Train Epoch: 17 [7040/17352 (41%)] Loss: -1344933.875000\n",
      "Train Epoch: 17 [8448/17352 (49%)] Loss: -1357408.000000\n",
      "Train Epoch: 17 [9856/17352 (57%)] Loss: -1338495.250000\n",
      "Train Epoch: 17 [11264/17352 (65%)] Loss: -1349727.125000\n",
      "Train Epoch: 17 [12672/17352 (73%)] Loss: -1339297.375000\n",
      "Train Epoch: 17 [14080/17352 (81%)] Loss: -1348009.500000\n",
      "Train Epoch: 17 [15488/17352 (89%)] Loss: -1355105.750000\n",
      "Train Epoch: 17 [16896/17352 (97%)] Loss: -1332454.500000\n",
      "    epoch          : 17\n",
      "    loss           : -1338978.4908088236\n",
      "    val_loss       : -1263216.0167236328\n",
      "Train Epoch: 18 [0/17352 (0%)] Loss: -1344326.625000\n",
      "Train Epoch: 18 [1408/17352 (8%)] Loss: -1333902.000000\n",
      "Train Epoch: 18 [2816/17352 (16%)] Loss: -1338313.375000\n",
      "Train Epoch: 18 [4224/17352 (24%)] Loss: -1340463.125000\n",
      "Train Epoch: 18 [5632/17352 (32%)] Loss: -1334990.000000\n",
      "Train Epoch: 18 [7040/17352 (41%)] Loss: -1340711.250000\n",
      "Train Epoch: 18 [8448/17352 (49%)] Loss: -1333713.000000\n",
      "Train Epoch: 18 [9856/17352 (57%)] Loss: -1366465.375000\n",
      "Train Epoch: 18 [11264/17352 (65%)] Loss: -1342158.125000\n",
      "Train Epoch: 18 [12672/17352 (73%)] Loss: -1334707.375000\n",
      "Train Epoch: 18 [14080/17352 (81%)] Loss: -1342942.375000\n",
      "Train Epoch: 18 [15488/17352 (89%)] Loss: -1344158.750000\n",
      "Train Epoch: 18 [16896/17352 (97%)] Loss: -1345369.250000\n",
      "    epoch          : 18\n",
      "    loss           : -1338975.6438419118\n",
      "    val_loss       : -1263179.5633850098\n",
      "Train Epoch: 19 [0/17352 (0%)] Loss: -1339058.875000\n",
      "Train Epoch: 19 [1408/17352 (8%)] Loss: -1336125.875000\n",
      "Train Epoch: 19 [2816/17352 (16%)] Loss: -1338343.500000\n",
      "Train Epoch: 19 [4224/17352 (24%)] Loss: -1342543.000000\n",
      "Train Epoch: 19 [5632/17352 (32%)] Loss: -1332411.875000\n",
      "Train Epoch: 19 [7040/17352 (41%)] Loss: -1345271.750000\n",
      "Train Epoch: 19 [8448/17352 (49%)] Loss: -1353047.750000\n",
      "Train Epoch: 19 [9856/17352 (57%)] Loss: -1330968.375000\n",
      "Train Epoch: 19 [11264/17352 (65%)] Loss: -1352497.625000\n",
      "Train Epoch: 19 [12672/17352 (73%)] Loss: -1349696.250000\n",
      "Train Epoch: 19 [14080/17352 (81%)] Loss: -1353292.250000\n",
      "Train Epoch: 19 [15488/17352 (89%)] Loss: -1330237.750000\n",
      "Train Epoch: 19 [16896/17352 (97%)] Loss: -1341224.500000\n",
      "    epoch          : 19\n",
      "    loss           : -1338944.6116727942\n",
      "    val_loss       : -1263211.0379638672\n",
      "Train Epoch: 20 [0/17352 (0%)] Loss: -1334146.125000\n",
      "Train Epoch: 20 [1408/17352 (8%)] Loss: -1330803.625000\n",
      "Train Epoch: 20 [2816/17352 (16%)] Loss: -1340373.500000\n",
      "Train Epoch: 20 [4224/17352 (24%)] Loss: -1354252.875000\n",
      "Train Epoch: 20 [5632/17352 (32%)] Loss: -1344396.875000\n",
      "Train Epoch: 20 [7040/17352 (41%)] Loss: -1342503.125000\n",
      "Train Epoch: 20 [8448/17352 (49%)] Loss: -1335573.125000\n",
      "Train Epoch: 20 [9856/17352 (57%)] Loss: -1351947.375000\n",
      "Train Epoch: 20 [11264/17352 (65%)] Loss: -1339710.000000\n",
      "Train Epoch: 20 [12672/17352 (73%)] Loss: -1341472.000000\n",
      "Train Epoch: 20 [14080/17352 (81%)] Loss: -1332681.500000\n",
      "Train Epoch: 20 [15488/17352 (89%)] Loss: -1358277.875000\n",
      "Train Epoch: 20 [16896/17352 (97%)] Loss: -1338428.375000\n",
      "    epoch          : 20\n",
      "    loss           : -1338987.1989889706\n",
      "    val_loss       : -1263269.788269043\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0427_093817/checkpoint-epoch20.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 21 [0/17352 (0%)] Loss: -1342049.500000\n",
      "Train Epoch: 21 [1408/17352 (8%)] Loss: -1354167.875000\n",
      "Train Epoch: 21 [2816/17352 (16%)] Loss: -1339022.125000\n",
      "Train Epoch: 21 [4224/17352 (24%)] Loss: -1348840.750000\n",
      "Train Epoch: 21 [5632/17352 (32%)] Loss: -1340763.250000\n",
      "Train Epoch: 21 [7040/17352 (41%)] Loss: -1345066.000000\n",
      "Train Epoch: 21 [8448/17352 (49%)] Loss: -1345108.375000\n",
      "Train Epoch: 21 [9856/17352 (57%)] Loss: -1351354.000000\n",
      "Train Epoch: 21 [11264/17352 (65%)] Loss: -1351982.375000\n",
      "Train Epoch: 21 [12672/17352 (73%)] Loss: -1343583.500000\n",
      "Train Epoch: 21 [14080/17352 (81%)] Loss: -1351995.250000\n",
      "Train Epoch: 21 [15488/17352 (89%)] Loss: -1340616.750000\n",
      "Train Epoch: 21 [16896/17352 (97%)] Loss: -1334108.875000\n",
      "    epoch          : 21\n",
      "    loss           : -1339016.4163602942\n",
      "    val_loss       : -1263269.3258972168\n",
      "Train Epoch: 22 [0/17352 (0%)] Loss: -1346932.125000\n",
      "Train Epoch: 22 [1408/17352 (8%)] Loss: -1351734.625000\n",
      "Train Epoch: 22 [2816/17352 (16%)] Loss: -1326185.750000\n",
      "Train Epoch: 22 [4224/17352 (24%)] Loss: -1335524.875000\n",
      "Train Epoch: 22 [5632/17352 (32%)] Loss: -1341988.625000\n",
      "Train Epoch: 22 [7040/17352 (41%)] Loss: -1348654.875000\n",
      "Train Epoch: 22 [8448/17352 (49%)] Loss: -1346240.875000\n",
      "Train Epoch: 22 [9856/17352 (57%)] Loss: -1344558.125000\n",
      "Train Epoch: 22 [11264/17352 (65%)] Loss: -1342476.250000\n",
      "Train Epoch: 22 [12672/17352 (73%)] Loss: -1343636.625000\n",
      "Train Epoch: 22 [14080/17352 (81%)] Loss: -1338428.250000\n",
      "Train Epoch: 22 [15488/17352 (89%)] Loss: -1338857.875000\n",
      "Train Epoch: 22 [16896/17352 (97%)] Loss: -1329017.500000\n",
      "    epoch          : 22\n",
      "    loss           : -1339030.9944852942\n",
      "    val_loss       : -1263313.2664489746\n",
      "Train Epoch: 23 [0/17352 (0%)] Loss: -1330140.750000\n",
      "Train Epoch: 23 [1408/17352 (8%)] Loss: -1342392.875000\n",
      "Train Epoch: 23 [2816/17352 (16%)] Loss: -1348545.500000\n",
      "Train Epoch: 23 [4224/17352 (24%)] Loss: -1342674.375000\n",
      "Train Epoch: 23 [5632/17352 (32%)] Loss: -1356556.750000\n",
      "Train Epoch: 23 [7040/17352 (41%)] Loss: -1342967.250000\n",
      "Train Epoch: 23 [8448/17352 (49%)] Loss: -1330310.875000\n",
      "Train Epoch: 23 [9856/17352 (57%)] Loss: -1351311.000000\n",
      "Train Epoch: 23 [11264/17352 (65%)] Loss: -1332110.250000\n",
      "Train Epoch: 23 [12672/17352 (73%)] Loss: -1340661.375000\n",
      "Train Epoch: 23 [14080/17352 (81%)] Loss: -1336243.875000\n",
      "Train Epoch: 23 [15488/17352 (89%)] Loss: -1338890.250000\n",
      "Train Epoch: 23 [16896/17352 (97%)] Loss: -1343205.375000\n",
      "    epoch          : 23\n",
      "    loss           : -1339055.8056066176\n",
      "    val_loss       : -1263296.4150390625\n",
      "Train Epoch: 24 [0/17352 (0%)] Loss: -1340115.500000\n",
      "Train Epoch: 24 [1408/17352 (8%)] Loss: -1338817.500000\n",
      "Train Epoch: 24 [2816/17352 (16%)] Loss: -1340899.000000\n",
      "Train Epoch: 24 [4224/17352 (24%)] Loss: -1344379.875000\n",
      "Train Epoch: 24 [5632/17352 (32%)] Loss: -1345585.750000\n",
      "Train Epoch: 24 [7040/17352 (41%)] Loss: -1346325.875000\n",
      "Train Epoch: 24 [8448/17352 (49%)] Loss: -1345579.250000\n",
      "Train Epoch: 24 [9856/17352 (57%)] Loss: -1349998.375000\n",
      "Train Epoch: 24 [11264/17352 (65%)] Loss: -1345831.000000\n",
      "Train Epoch: 24 [12672/17352 (73%)] Loss: -1342760.500000\n",
      "Train Epoch: 24 [14080/17352 (81%)] Loss: -1339574.250000\n",
      "Train Epoch: 24 [15488/17352 (89%)] Loss: -1349117.875000\n",
      "Train Epoch: 24 [16896/17352 (97%)] Loss: -1344305.625000\n",
      "    epoch          : 24\n",
      "    loss           : -1339059.6332720588\n",
      "    val_loss       : -1263319.435333252\n",
      "Train Epoch: 25 [0/17352 (0%)] Loss: -1346475.000000\n",
      "Train Epoch: 25 [1408/17352 (8%)] Loss: -1343882.500000\n",
      "Train Epoch: 25 [2816/17352 (16%)] Loss: -1333454.625000\n",
      "Train Epoch: 25 [4224/17352 (24%)] Loss: -1345354.750000\n",
      "Train Epoch: 25 [5632/17352 (32%)] Loss: -1348138.625000\n",
      "Train Epoch: 25 [7040/17352 (41%)] Loss: -1349049.625000\n",
      "Train Epoch: 25 [8448/17352 (49%)] Loss: -1350228.375000\n",
      "Train Epoch: 25 [9856/17352 (57%)] Loss: -1336082.875000\n",
      "Train Epoch: 25 [11264/17352 (65%)] Loss: -1345016.750000\n",
      "Train Epoch: 25 [12672/17352 (73%)] Loss: -1352134.500000\n",
      "Train Epoch: 25 [14080/17352 (81%)] Loss: -1342402.750000\n",
      "Train Epoch: 25 [15488/17352 (89%)] Loss: -1347941.250000\n",
      "Train Epoch: 25 [16896/17352 (97%)] Loss: -1344424.375000\n",
      "    epoch          : 25\n",
      "    loss           : -1339073.6061580882\n",
      "    val_loss       : -1263343.1755981445\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0427_093817/checkpoint-epoch25.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 26 [0/17352 (0%)] Loss: -1345183.125000\n",
      "Train Epoch: 26 [1408/17352 (8%)] Loss: -1355562.625000\n",
      "Train Epoch: 26 [2816/17352 (16%)] Loss: -1331237.125000\n",
      "Train Epoch: 26 [4224/17352 (24%)] Loss: -1346162.750000\n",
      "Train Epoch: 26 [5632/17352 (32%)] Loss: -1338426.375000\n",
      "Train Epoch: 26 [7040/17352 (41%)] Loss: -1345917.500000\n",
      "Train Epoch: 26 [8448/17352 (49%)] Loss: -1355533.750000\n",
      "Train Epoch: 26 [9856/17352 (57%)] Loss: -1352027.875000\n",
      "Train Epoch: 26 [11264/17352 (65%)] Loss: -1335704.000000\n",
      "Train Epoch: 26 [12672/17352 (73%)] Loss: -1353188.625000\n",
      "Train Epoch: 26 [14080/17352 (81%)] Loss: -1342803.250000\n",
      "Train Epoch: 26 [15488/17352 (89%)] Loss: -1330819.000000\n",
      "Train Epoch: 26 [16896/17352 (97%)] Loss: -1337496.250000\n",
      "    epoch          : 26\n",
      "    loss           : -1339093.7201286764\n",
      "    val_loss       : -1263356.6752624512\n",
      "Train Epoch: 27 [0/17352 (0%)] Loss: -1344800.625000\n",
      "Train Epoch: 27 [1408/17352 (8%)] Loss: -1338130.250000\n",
      "Train Epoch: 27 [2816/17352 (16%)] Loss: -1342987.375000\n",
      "Train Epoch: 27 [4224/17352 (24%)] Loss: -1342965.750000\n",
      "Train Epoch: 27 [5632/17352 (32%)] Loss: -1348157.250000\n",
      "Train Epoch: 27 [7040/17352 (41%)] Loss: -1337719.625000\n",
      "Train Epoch: 27 [8448/17352 (49%)] Loss: -1348001.375000\n",
      "Train Epoch: 27 [9856/17352 (57%)] Loss: -1339021.750000\n",
      "Train Epoch: 27 [11264/17352 (65%)] Loss: -1350652.250000\n",
      "Train Epoch: 27 [12672/17352 (73%)] Loss: -1334445.625000\n",
      "Train Epoch: 27 [14080/17352 (81%)] Loss: -1350917.875000\n",
      "Train Epoch: 27 [15488/17352 (89%)] Loss: -1348755.000000\n",
      "Train Epoch: 27 [16896/17352 (97%)] Loss: -1339551.125000\n",
      "    epoch          : 27\n",
      "    loss           : -1339105.375\n",
      "    val_loss       : -1263382.4563598633\n",
      "Train Epoch: 28 [0/17352 (0%)] Loss: -1332491.875000\n",
      "Train Epoch: 28 [1408/17352 (8%)] Loss: -1338964.625000\n",
      "Train Epoch: 28 [2816/17352 (16%)] Loss: -1347562.250000\n",
      "Train Epoch: 28 [4224/17352 (24%)] Loss: -1355753.000000\n",
      "Train Epoch: 28 [5632/17352 (32%)] Loss: -1350404.625000\n",
      "Train Epoch: 28 [7040/17352 (41%)] Loss: -1333588.375000\n",
      "Train Epoch: 28 [8448/17352 (49%)] Loss: -1345892.000000\n",
      "Train Epoch: 28 [9856/17352 (57%)] Loss: -1337611.750000\n",
      "Train Epoch: 28 [11264/17352 (65%)] Loss: -1348751.750000\n",
      "Train Epoch: 28 [12672/17352 (73%)] Loss: -1347120.375000\n",
      "Train Epoch: 28 [14080/17352 (81%)] Loss: -1339035.750000\n",
      "Train Epoch: 28 [15488/17352 (89%)] Loss: -1336082.125000\n",
      "Train Epoch: 28 [16896/17352 (97%)] Loss: -1337949.125000\n",
      "    epoch          : 28\n",
      "    loss           : -1339119.8212316176\n",
      "    val_loss       : -1263403.8688354492\n",
      "Train Epoch: 29 [0/17352 (0%)] Loss: -1345532.375000\n",
      "Train Epoch: 29 [1408/17352 (8%)] Loss: -1348940.500000\n",
      "Train Epoch: 29 [2816/17352 (16%)] Loss: -1342865.250000\n",
      "Train Epoch: 29 [4224/17352 (24%)] Loss: -1338582.375000\n",
      "Train Epoch: 29 [5632/17352 (32%)] Loss: -1349441.625000\n",
      "Train Epoch: 29 [7040/17352 (41%)] Loss: -1354569.250000\n",
      "Train Epoch: 29 [8448/17352 (49%)] Loss: -1352460.000000\n",
      "Train Epoch: 29 [9856/17352 (57%)] Loss: -1350250.125000\n",
      "Train Epoch: 29 [11264/17352 (65%)] Loss: -1355430.500000\n",
      "Train Epoch: 29 [12672/17352 (73%)] Loss: -1338081.250000\n",
      "Train Epoch: 29 [14080/17352 (81%)] Loss: -1339686.000000\n",
      "Train Epoch: 29 [15488/17352 (89%)] Loss: -1344804.625000\n",
      "Train Epoch: 29 [16896/17352 (97%)] Loss: -1338735.625000\n",
      "    epoch          : 29\n",
      "    loss           : -1339124.9692095588\n",
      "    val_loss       : -1263388.123260498\n",
      "Train Epoch: 30 [0/17352 (0%)] Loss: -1349776.500000\n",
      "Train Epoch: 30 [1408/17352 (8%)] Loss: -1338800.500000\n",
      "Train Epoch: 30 [2816/17352 (16%)] Loss: -1342455.625000\n",
      "Train Epoch: 30 [4224/17352 (24%)] Loss: -1345812.000000\n",
      "Train Epoch: 30 [5632/17352 (32%)] Loss: -1338785.500000\n",
      "Train Epoch: 30 [7040/17352 (41%)] Loss: -1353081.000000\n",
      "Train Epoch: 30 [8448/17352 (49%)] Loss: -1346505.875000\n",
      "Train Epoch: 30 [9856/17352 (57%)] Loss: -1352628.875000\n",
      "Train Epoch: 30 [11264/17352 (65%)] Loss: -1347680.250000\n",
      "Train Epoch: 30 [12672/17352 (73%)] Loss: -1343024.000000\n",
      "Train Epoch: 30 [14080/17352 (81%)] Loss: -1349126.250000\n",
      "Train Epoch: 30 [15488/17352 (89%)] Loss: -1326480.250000\n",
      "Train Epoch: 30 [16896/17352 (97%)] Loss: -1341245.750000\n",
      "    epoch          : 30\n",
      "    loss           : -1339143.9039522058\n",
      "    val_loss       : -1263403.5274353027\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0427_093817/checkpoint-epoch30.pth ...\n",
      "Train Epoch: 31 [0/17352 (0%)] Loss: -1353576.375000\n",
      "Train Epoch: 31 [1408/17352 (8%)] Loss: -1345133.875000\n",
      "Train Epoch: 31 [2816/17352 (16%)] Loss: -1342005.250000\n",
      "Train Epoch: 31 [4224/17352 (24%)] Loss: -1342822.250000\n",
      "Train Epoch: 31 [5632/17352 (32%)] Loss: -1344950.625000\n",
      "Train Epoch: 31 [7040/17352 (41%)] Loss: -1339936.750000\n",
      "Train Epoch: 31 [8448/17352 (49%)] Loss: -1354835.875000\n",
      "Train Epoch: 31 [9856/17352 (57%)] Loss: -1337373.625000\n",
      "Train Epoch: 31 [11264/17352 (65%)] Loss: -1341201.875000\n",
      "Train Epoch: 31 [12672/17352 (73%)] Loss: -1350800.750000\n",
      "Train Epoch: 31 [14080/17352 (81%)] Loss: -1349354.875000\n",
      "Train Epoch: 31 [15488/17352 (89%)] Loss: -1341958.625000\n",
      "Train Epoch: 31 [16896/17352 (97%)] Loss: -1326453.875000\n",
      "    epoch          : 31\n",
      "    loss           : -1339148.7265625\n",
      "    val_loss       : -1263419.4864196777\n",
      "Train Epoch: 32 [0/17352 (0%)] Loss: -1338476.125000\n",
      "Train Epoch: 32 [1408/17352 (8%)] Loss: -1341168.750000\n",
      "Train Epoch: 32 [2816/17352 (16%)] Loss: -1351968.875000\n",
      "Train Epoch: 32 [4224/17352 (24%)] Loss: -1346444.125000\n",
      "Train Epoch: 32 [5632/17352 (32%)] Loss: -1360871.000000\n",
      "Train Epoch: 32 [7040/17352 (41%)] Loss: -1331876.750000\n",
      "Train Epoch: 32 [8448/17352 (49%)] Loss: -1335131.625000\n",
      "Train Epoch: 32 [9856/17352 (57%)] Loss: -1342364.000000\n",
      "Train Epoch: 32 [11264/17352 (65%)] Loss: -1346626.000000\n",
      "Train Epoch: 32 [12672/17352 (73%)] Loss: -1322655.000000\n",
      "Train Epoch: 32 [14080/17352 (81%)] Loss: -1324485.250000\n",
      "Train Epoch: 32 [15488/17352 (89%)] Loss: -1336853.500000\n",
      "Train Epoch: 32 [16896/17352 (97%)] Loss: -1346255.625000\n",
      "    epoch          : 32\n",
      "    loss           : -1339158.3221507352\n",
      "    val_loss       : -1263408.8170471191\n",
      "Train Epoch: 33 [0/17352 (0%)] Loss: -1342731.250000\n",
      "Train Epoch: 33 [1408/17352 (8%)] Loss: -1341145.250000\n",
      "Train Epoch: 33 [2816/17352 (16%)] Loss: -1366366.625000\n",
      "Train Epoch: 33 [4224/17352 (24%)] Loss: -1340482.875000\n",
      "Train Epoch: 33 [5632/17352 (32%)] Loss: -1346680.875000\n",
      "Train Epoch: 33 [7040/17352 (41%)] Loss: -1352251.250000\n",
      "Train Epoch: 33 [8448/17352 (49%)] Loss: -1347614.500000\n",
      "Train Epoch: 33 [9856/17352 (57%)] Loss: -1335021.250000\n",
      "Train Epoch: 33 [11264/17352 (65%)] Loss: -1342919.500000\n",
      "Train Epoch: 33 [12672/17352 (73%)] Loss: -1357268.000000\n",
      "Train Epoch: 33 [14080/17352 (81%)] Loss: -1342108.125000\n",
      "Train Epoch: 33 [15488/17352 (89%)] Loss: -1346980.500000\n",
      "Train Epoch: 33 [16896/17352 (97%)] Loss: -1332724.875000\n",
      "    epoch          : 33\n",
      "    loss           : -1339164.9241727942\n",
      "    val_loss       : -1263420.0127563477\n",
      "Train Epoch: 34 [0/17352 (0%)] Loss: -1358355.875000\n",
      "Train Epoch: 34 [1408/17352 (8%)] Loss: -1341054.500000\n",
      "Train Epoch: 34 [2816/17352 (16%)] Loss: -1320679.000000\n",
      "Train Epoch: 34 [4224/17352 (24%)] Loss: -1352281.750000\n",
      "Train Epoch: 34 [5632/17352 (32%)] Loss: -1341414.250000\n",
      "Train Epoch: 34 [7040/17352 (41%)] Loss: -1350118.750000\n",
      "Train Epoch: 34 [8448/17352 (49%)] Loss: -1349522.750000\n",
      "Train Epoch: 34 [9856/17352 (57%)] Loss: -1352302.250000\n",
      "Train Epoch: 34 [11264/17352 (65%)] Loss: -1345654.875000\n",
      "Train Epoch: 34 [12672/17352 (73%)] Loss: -1336823.500000\n",
      "Train Epoch: 34 [14080/17352 (81%)] Loss: -1337695.375000\n",
      "Train Epoch: 34 [15488/17352 (89%)] Loss: -1332177.625000\n",
      "Train Epoch: 34 [16896/17352 (97%)] Loss: -1338284.750000\n",
      "    epoch          : 34\n",
      "    loss           : -1339174.9572610294\n",
      "    val_loss       : -1263446.935546875\n",
      "Train Epoch: 35 [0/17352 (0%)] Loss: -1325311.500000\n",
      "Train Epoch: 35 [1408/17352 (8%)] Loss: -1354953.250000\n",
      "Train Epoch: 35 [2816/17352 (16%)] Loss: -1336624.250000\n",
      "Train Epoch: 35 [4224/17352 (24%)] Loss: -1348014.875000\n",
      "Train Epoch: 35 [5632/17352 (32%)] Loss: -1349404.750000\n",
      "Train Epoch: 35 [7040/17352 (41%)] Loss: -1345712.375000\n",
      "Train Epoch: 35 [8448/17352 (49%)] Loss: -1329984.750000\n",
      "Train Epoch: 35 [9856/17352 (57%)] Loss: -1338046.500000\n",
      "Train Epoch: 35 [11264/17352 (65%)] Loss: -1354791.500000\n",
      "Train Epoch: 35 [12672/17352 (73%)] Loss: -1325162.000000\n",
      "Train Epoch: 35 [14080/17352 (81%)] Loss: -1335364.125000\n",
      "Train Epoch: 35 [15488/17352 (89%)] Loss: -1335986.750000\n",
      "Train Epoch: 35 [16896/17352 (97%)] Loss: -1348752.375000\n",
      "    epoch          : 35\n",
      "    loss           : -1339181.5386029412\n",
      "    val_loss       : -1263410.9491577148\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0427_093817/checkpoint-epoch35.pth ...\n",
      "Train Epoch: 36 [0/17352 (0%)] Loss: -1356289.750000\n",
      "Train Epoch: 36 [1408/17352 (8%)] Loss: -1333780.000000\n",
      "Train Epoch: 36 [2816/17352 (16%)] Loss: -1343032.875000\n",
      "Train Epoch: 36 [4224/17352 (24%)] Loss: -1344872.000000\n",
      "Train Epoch: 36 [5632/17352 (32%)] Loss: -1343328.125000\n",
      "Train Epoch: 36 [7040/17352 (41%)] Loss: -1344502.250000\n",
      "Train Epoch: 36 [8448/17352 (49%)] Loss: -1346346.750000\n",
      "Train Epoch: 36 [9856/17352 (57%)] Loss: -1343935.625000\n",
      "Train Epoch: 36 [11264/17352 (65%)] Loss: -1336640.375000\n",
      "Train Epoch: 36 [12672/17352 (73%)] Loss: -1350411.250000\n",
      "Train Epoch: 36 [14080/17352 (81%)] Loss: -1343619.375000\n",
      "Train Epoch: 36 [15488/17352 (89%)] Loss: -1334937.000000\n",
      "Train Epoch: 36 [16896/17352 (97%)] Loss: -1340897.000000\n",
      "    epoch          : 36\n",
      "    loss           : -1339178.0349264706\n",
      "    val_loss       : -1263422.1949768066\n",
      "Train Epoch: 37 [0/17352 (0%)] Loss: -1330389.750000\n",
      "Train Epoch: 37 [1408/17352 (8%)] Loss: -1327708.250000\n",
      "Train Epoch: 37 [2816/17352 (16%)] Loss: -1335334.250000\n",
      "Train Epoch: 37 [4224/17352 (24%)] Loss: -1343106.125000\n",
      "Train Epoch: 37 [5632/17352 (32%)] Loss: -1348587.875000\n",
      "Train Epoch: 37 [7040/17352 (41%)] Loss: -1344187.875000\n",
      "Train Epoch: 37 [8448/17352 (49%)] Loss: -1340215.625000\n",
      "Train Epoch: 37 [9856/17352 (57%)] Loss: -1340911.500000\n",
      "Train Epoch: 37 [11264/17352 (65%)] Loss: -1358089.375000\n",
      "Train Epoch: 37 [12672/17352 (73%)] Loss: -1335958.250000\n",
      "Train Epoch: 37 [14080/17352 (81%)] Loss: -1349882.375000\n",
      "Train Epoch: 37 [15488/17352 (89%)] Loss: -1339546.000000\n",
      "Train Epoch: 37 [16896/17352 (97%)] Loss: -1355032.250000\n",
      "    epoch          : 37\n",
      "    loss           : -1339176.515625\n",
      "    val_loss       : -1263435.2188720703\n",
      "Train Epoch: 38 [0/17352 (0%)] Loss: -1343664.875000\n",
      "Train Epoch: 38 [1408/17352 (8%)] Loss: -1347853.500000\n",
      "Train Epoch: 38 [2816/17352 (16%)] Loss: -1355750.375000\n",
      "Train Epoch: 38 [4224/17352 (24%)] Loss: -1346440.125000\n",
      "Train Epoch: 38 [5632/17352 (32%)] Loss: -1330274.500000\n",
      "Train Epoch: 38 [7040/17352 (41%)] Loss: -1328528.875000\n",
      "Train Epoch: 38 [8448/17352 (49%)] Loss: -1345557.500000\n",
      "Train Epoch: 38 [9856/17352 (57%)] Loss: -1348668.625000\n",
      "Train Epoch: 38 [11264/17352 (65%)] Loss: -1347064.500000\n",
      "Train Epoch: 38 [12672/17352 (73%)] Loss: -1345785.250000\n",
      "Train Epoch: 38 [14080/17352 (81%)] Loss: -1333969.500000\n",
      "Train Epoch: 38 [15488/17352 (89%)] Loss: -1344302.500000\n",
      "Train Epoch: 38 [16896/17352 (97%)] Loss: -1342036.000000\n",
      "    epoch          : 38\n",
      "    loss           : -1339183.5579044118\n",
      "    val_loss       : -1263447.784576416\n",
      "Train Epoch: 39 [0/17352 (0%)] Loss: -1333550.625000\n",
      "Train Epoch: 39 [1408/17352 (8%)] Loss: -1354363.750000\n",
      "Train Epoch: 39 [2816/17352 (16%)] Loss: -1348478.250000\n",
      "Train Epoch: 39 [4224/17352 (24%)] Loss: -1353485.875000\n",
      "Train Epoch: 39 [5632/17352 (32%)] Loss: -1325638.375000\n",
      "Train Epoch: 39 [7040/17352 (41%)] Loss: -1332331.875000\n",
      "Train Epoch: 39 [8448/17352 (49%)] Loss: -1344702.500000\n",
      "Train Epoch: 39 [9856/17352 (57%)] Loss: -1361454.875000\n",
      "Train Epoch: 39 [11264/17352 (65%)] Loss: -1342894.750000\n",
      "Train Epoch: 39 [12672/17352 (73%)] Loss: -1362953.000000\n",
      "Train Epoch: 39 [14080/17352 (81%)] Loss: -1337116.625000\n",
      "Train Epoch: 39 [15488/17352 (89%)] Loss: -1340616.500000\n",
      "Train Epoch: 39 [16896/17352 (97%)] Loss: -1354004.750000\n",
      "    epoch          : 39\n",
      "    loss           : -1339191.8069852942\n",
      "    val_loss       : -1263462.111328125\n",
      "Train Epoch: 40 [0/17352 (0%)] Loss: -1364906.875000\n",
      "Train Epoch: 40 [1408/17352 (8%)] Loss: -1336856.375000\n",
      "Train Epoch: 40 [2816/17352 (16%)] Loss: -1343677.250000\n",
      "Train Epoch: 40 [4224/17352 (24%)] Loss: -1352881.125000\n",
      "Train Epoch: 40 [5632/17352 (32%)] Loss: -1343218.250000\n",
      "Train Epoch: 40 [7040/17352 (41%)] Loss: -1349872.875000\n",
      "Train Epoch: 40 [8448/17352 (49%)] Loss: -1339313.125000\n",
      "Train Epoch: 40 [9856/17352 (57%)] Loss: -1333983.375000\n",
      "Train Epoch: 40 [11264/17352 (65%)] Loss: -1349433.500000\n",
      "Train Epoch: 40 [12672/17352 (73%)] Loss: -1350258.000000\n",
      "Train Epoch: 40 [14080/17352 (81%)] Loss: -1347944.750000\n",
      "Train Epoch: 40 [15488/17352 (89%)] Loss: -1341568.250000\n",
      "Train Epoch: 40 [16896/17352 (97%)] Loss: -1342473.250000\n",
      "    epoch          : 40\n",
      "    loss           : -1339200.0505514706\n",
      "    val_loss       : -1263450.2235412598\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0427_093817/checkpoint-epoch40.pth ...\n",
      "Train Epoch: 41 [0/17352 (0%)] Loss: -1349481.000000\n",
      "Train Epoch: 41 [1408/17352 (8%)] Loss: -1361155.375000\n",
      "Train Epoch: 41 [2816/17352 (16%)] Loss: -1351870.500000\n",
      "Train Epoch: 41 [4224/17352 (24%)] Loss: -1340133.500000\n",
      "Train Epoch: 41 [5632/17352 (32%)] Loss: -1341561.750000\n",
      "Train Epoch: 41 [7040/17352 (41%)] Loss: -1346896.375000\n",
      "Train Epoch: 41 [8448/17352 (49%)] Loss: -1339194.750000\n",
      "Train Epoch: 41 [9856/17352 (57%)] Loss: -1338077.375000\n",
      "Train Epoch: 41 [11264/17352 (65%)] Loss: -1342484.500000\n",
      "Train Epoch: 41 [12672/17352 (73%)] Loss: -1339490.125000\n",
      "Train Epoch: 41 [14080/17352 (81%)] Loss: -1350800.750000\n",
      "Train Epoch: 41 [15488/17352 (89%)] Loss: -1355151.500000\n",
      "Train Epoch: 41 [16896/17352 (97%)] Loss: -1335422.625000\n",
      "    epoch          : 41\n",
      "    loss           : -1339201.205882353\n",
      "    val_loss       : -1263448.30758667\n",
      "Train Epoch: 42 [0/17352 (0%)] Loss: -1348583.750000\n",
      "Train Epoch: 42 [1408/17352 (8%)] Loss: -1337352.625000\n",
      "Train Epoch: 42 [2816/17352 (16%)] Loss: -1335607.250000\n",
      "Train Epoch: 42 [4224/17352 (24%)] Loss: -1349930.250000\n",
      "Train Epoch: 42 [5632/17352 (32%)] Loss: -1340071.125000\n",
      "Train Epoch: 42 [7040/17352 (41%)] Loss: -1338039.250000\n",
      "Train Epoch: 42 [8448/17352 (49%)] Loss: -1344743.000000\n",
      "Train Epoch: 42 [9856/17352 (57%)] Loss: -1351792.250000\n",
      "Train Epoch: 42 [11264/17352 (65%)] Loss: -1345262.875000\n",
      "Train Epoch: 42 [12672/17352 (73%)] Loss: -1342123.875000\n",
      "Train Epoch: 42 [14080/17352 (81%)] Loss: -1336216.625000\n",
      "Train Epoch: 42 [15488/17352 (89%)] Loss: -1344276.625000\n",
      "Train Epoch: 42 [16896/17352 (97%)] Loss: -1338508.250000\n",
      "    epoch          : 42\n",
      "    loss           : -1339218.0202205882\n",
      "    val_loss       : -1263456.7290039062\n",
      "Train Epoch: 43 [0/17352 (0%)] Loss: -1344496.750000\n",
      "Train Epoch: 43 [1408/17352 (8%)] Loss: -1343782.250000\n",
      "Train Epoch: 43 [2816/17352 (16%)] Loss: -1354916.750000\n",
      "Train Epoch: 43 [4224/17352 (24%)] Loss: -1335248.125000\n",
      "Train Epoch: 43 [5632/17352 (32%)] Loss: -1346187.125000\n",
      "Train Epoch: 43 [7040/17352 (41%)] Loss: -1351409.125000\n",
      "Train Epoch: 43 [8448/17352 (49%)] Loss: -1341464.500000\n",
      "Train Epoch: 43 [9856/17352 (57%)] Loss: -1342392.500000\n",
      "Train Epoch: 43 [11264/17352 (65%)] Loss: -1350909.625000\n",
      "Train Epoch: 43 [12672/17352 (73%)] Loss: -1342391.375000\n",
      "Train Epoch: 43 [14080/17352 (81%)] Loss: -1360369.000000\n",
      "Train Epoch: 43 [15488/17352 (89%)] Loss: -1346034.500000\n",
      "Train Epoch: 43 [16896/17352 (97%)] Loss: -1349304.375000\n",
      "    epoch          : 43\n",
      "    loss           : -1339217.1484375\n",
      "    val_loss       : -1263471.7644958496\n",
      "Train Epoch: 44 [0/17352 (0%)] Loss: -1346461.500000\n",
      "Train Epoch: 44 [1408/17352 (8%)] Loss: -1341183.875000\n",
      "Train Epoch: 44 [2816/17352 (16%)] Loss: -1334218.625000\n",
      "Train Epoch: 44 [4224/17352 (24%)] Loss: -1332465.375000\n",
      "Train Epoch: 44 [5632/17352 (32%)] Loss: -1336857.500000\n",
      "Train Epoch: 44 [7040/17352 (41%)] Loss: -1347914.500000\n",
      "Train Epoch: 44 [8448/17352 (49%)] Loss: -1331604.625000\n",
      "Train Epoch: 44 [9856/17352 (57%)] Loss: -1340619.000000\n",
      "Train Epoch: 44 [11264/17352 (65%)] Loss: -1346334.875000\n",
      "Train Epoch: 44 [12672/17352 (73%)] Loss: -1346999.625000\n",
      "Train Epoch: 44 [14080/17352 (81%)] Loss: -1341277.625000\n",
      "Train Epoch: 44 [15488/17352 (89%)] Loss: -1348580.250000\n",
      "Train Epoch: 44 [16896/17352 (97%)] Loss: -1353571.250000\n",
      "    epoch          : 44\n",
      "    loss           : -1339227.3690257352\n",
      "    val_loss       : -1263470.8217468262\n",
      "Train Epoch: 45 [0/17352 (0%)] Loss: -1359610.875000\n",
      "Train Epoch: 45 [1408/17352 (8%)] Loss: -1351516.750000\n",
      "Train Epoch: 45 [2816/17352 (16%)] Loss: -1332682.375000\n",
      "Train Epoch: 45 [4224/17352 (24%)] Loss: -1347419.750000\n",
      "Train Epoch: 45 [5632/17352 (32%)] Loss: -1347906.000000\n",
      "Train Epoch: 45 [7040/17352 (41%)] Loss: -1352925.750000\n",
      "Train Epoch: 45 [8448/17352 (49%)] Loss: -1349196.250000\n",
      "Train Epoch: 45 [9856/17352 (57%)] Loss: -1349524.875000\n",
      "Train Epoch: 45 [11264/17352 (65%)] Loss: -1327981.500000\n",
      "Train Epoch: 45 [12672/17352 (73%)] Loss: -1348194.750000\n",
      "Train Epoch: 45 [14080/17352 (81%)] Loss: -1352086.625000\n",
      "Train Epoch: 45 [15488/17352 (89%)] Loss: -1339276.750000\n",
      "Train Epoch: 45 [16896/17352 (97%)] Loss: -1356757.375000\n",
      "    epoch          : 45\n",
      "    loss           : -1339224.515625\n",
      "    val_loss       : -1263489.564300537\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0427_093817/checkpoint-epoch45.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 46 [0/17352 (0%)] Loss: -1356462.500000\n",
      "Train Epoch: 46 [1408/17352 (8%)] Loss: -1349455.750000\n",
      "Train Epoch: 46 [2816/17352 (16%)] Loss: -1337980.250000\n",
      "Train Epoch: 46 [4224/17352 (24%)] Loss: -1343563.250000\n",
      "Train Epoch: 46 [5632/17352 (32%)] Loss: -1339068.125000\n",
      "Train Epoch: 46 [7040/17352 (41%)] Loss: -1343188.875000\n",
      "Train Epoch: 46 [8448/17352 (49%)] Loss: -1333239.875000\n",
      "Train Epoch: 46 [9856/17352 (57%)] Loss: -1336140.125000\n",
      "Train Epoch: 46 [11264/17352 (65%)] Loss: -1340153.375000\n",
      "Train Epoch: 46 [12672/17352 (73%)] Loss: -1338691.625000\n",
      "Train Epoch: 46 [14080/17352 (81%)] Loss: -1337426.750000\n",
      "Train Epoch: 46 [15488/17352 (89%)] Loss: -1346161.125000\n",
      "Train Epoch: 46 [16896/17352 (97%)] Loss: -1354725.000000\n",
      "    epoch          : 46\n",
      "    loss           : -1339232.9646139706\n",
      "    val_loss       : -1263492.5365600586\n",
      "Train Epoch: 47 [0/17352 (0%)] Loss: -1350697.750000\n",
      "Train Epoch: 47 [1408/17352 (8%)] Loss: -1339920.875000\n",
      "Train Epoch: 47 [2816/17352 (16%)] Loss: -1328142.250000\n",
      "Train Epoch: 47 [4224/17352 (24%)] Loss: -1356180.625000\n",
      "Train Epoch: 47 [5632/17352 (32%)] Loss: -1358019.500000\n",
      "Train Epoch: 47 [7040/17352 (41%)] Loss: -1334102.000000\n",
      "Train Epoch: 47 [8448/17352 (49%)] Loss: -1343151.625000\n",
      "Train Epoch: 47 [9856/17352 (57%)] Loss: -1340772.500000\n",
      "Train Epoch: 47 [11264/17352 (65%)] Loss: -1341128.375000\n",
      "Train Epoch: 47 [12672/17352 (73%)] Loss: -1333739.375000\n",
      "Train Epoch: 47 [14080/17352 (81%)] Loss: -1342955.500000\n",
      "Train Epoch: 47 [15488/17352 (89%)] Loss: -1341850.250000\n",
      "Train Epoch: 47 [16896/17352 (97%)] Loss: -1352832.375000\n",
      "    epoch          : 47\n",
      "    loss           : -1339238.6585477942\n",
      "    val_loss       : -1263492.16696167\n",
      "Train Epoch: 48 [0/17352 (0%)] Loss: -1341482.000000\n",
      "Train Epoch: 48 [1408/17352 (8%)] Loss: -1336630.375000\n",
      "Train Epoch: 48 [2816/17352 (16%)] Loss: -1319983.000000\n",
      "Train Epoch: 48 [4224/17352 (24%)] Loss: -1337435.750000\n",
      "Train Epoch: 48 [5632/17352 (32%)] Loss: -1352387.750000\n",
      "Train Epoch: 48 [7040/17352 (41%)] Loss: -1331078.500000\n",
      "Train Epoch: 48 [8448/17352 (49%)] Loss: -1341739.500000\n",
      "Train Epoch: 48 [9856/17352 (57%)] Loss: -1340197.750000\n",
      "Train Epoch: 48 [11264/17352 (65%)] Loss: -1341547.500000\n",
      "Train Epoch: 48 [12672/17352 (73%)] Loss: -1341726.750000\n",
      "Train Epoch: 48 [14080/17352 (81%)] Loss: -1356032.250000\n",
      "Train Epoch: 48 [15488/17352 (89%)] Loss: -1339023.500000\n",
      "Train Epoch: 48 [16896/17352 (97%)] Loss: -1333999.500000\n",
      "    epoch          : 48\n",
      "    loss           : -1339246.3676470588\n",
      "    val_loss       : -1263498.05368042\n",
      "Train Epoch: 49 [0/17352 (0%)] Loss: -1357011.625000\n",
      "Train Epoch: 49 [1408/17352 (8%)] Loss: -1356318.625000\n",
      "Train Epoch: 49 [2816/17352 (16%)] Loss: -1363453.625000\n",
      "Train Epoch: 49 [4224/17352 (24%)] Loss: -1355393.250000\n",
      "Train Epoch: 49 [5632/17352 (32%)] Loss: -1350996.375000\n",
      "Train Epoch: 49 [7040/17352 (41%)] Loss: -1342509.500000\n",
      "Train Epoch: 49 [8448/17352 (49%)] Loss: -1337833.500000\n",
      "Train Epoch: 49 [9856/17352 (57%)] Loss: -1344675.500000\n",
      "Train Epoch: 49 [11264/17352 (65%)] Loss: -1351298.250000\n",
      "Train Epoch: 49 [12672/17352 (73%)] Loss: -1357973.375000\n",
      "Train Epoch: 49 [14080/17352 (81%)] Loss: -1335102.625000\n",
      "Train Epoch: 49 [15488/17352 (89%)] Loss: -1340628.750000\n",
      "Train Epoch: 49 [16896/17352 (97%)] Loss: -1327691.375000\n",
      "    epoch          : 49\n",
      "    loss           : -1339250.8851102942\n",
      "    val_loss       : -1263517.8672180176\n",
      "Train Epoch: 50 [0/17352 (0%)] Loss: -1344848.125000\n",
      "Train Epoch: 50 [1408/17352 (8%)] Loss: -1335553.375000\n",
      "Train Epoch: 50 [2816/17352 (16%)] Loss: -1348519.250000\n",
      "Train Epoch: 50 [4224/17352 (24%)] Loss: -1342823.500000\n",
      "Train Epoch: 50 [5632/17352 (32%)] Loss: -1335811.125000\n",
      "Train Epoch: 50 [7040/17352 (41%)] Loss: -1341161.625000\n",
      "Train Epoch: 50 [8448/17352 (49%)] Loss: -1348151.875000\n",
      "Train Epoch: 50 [9856/17352 (57%)] Loss: -1336090.875000\n",
      "Train Epoch: 50 [11264/17352 (65%)] Loss: -1350383.375000\n",
      "Train Epoch: 50 [12672/17352 (73%)] Loss: -1342597.625000\n",
      "Train Epoch: 50 [14080/17352 (81%)] Loss: -1339765.750000\n",
      "Train Epoch: 50 [15488/17352 (89%)] Loss: -1337419.250000\n",
      "Train Epoch: 50 [16896/17352 (97%)] Loss: -1340113.375000\n",
      "    epoch          : 50\n",
      "    loss           : -1339254.1488970588\n",
      "    val_loss       : -1263510.2299194336\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0427_093817/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [0/17352 (0%)] Loss: -1339489.000000\n",
      "Train Epoch: 51 [1408/17352 (8%)] Loss: -1347751.625000\n",
      "Train Epoch: 51 [2816/17352 (16%)] Loss: -1335657.375000\n",
      "Train Epoch: 51 [4224/17352 (24%)] Loss: -1348173.625000\n",
      "Train Epoch: 51 [5632/17352 (32%)] Loss: -1355832.125000\n",
      "Train Epoch: 51 [7040/17352 (41%)] Loss: -1334851.500000\n",
      "Train Epoch: 51 [8448/17352 (49%)] Loss: -1350714.000000\n",
      "Train Epoch: 51 [9856/17352 (57%)] Loss: -1345701.375000\n",
      "Train Epoch: 51 [11264/17352 (65%)] Loss: -1336487.625000\n",
      "Train Epoch: 51 [12672/17352 (73%)] Loss: -1335703.250000\n",
      "Train Epoch: 51 [14080/17352 (81%)] Loss: -1339877.750000\n",
      "Train Epoch: 51 [15488/17352 (89%)] Loss: -1344731.250000\n",
      "Train Epoch: 51 [16896/17352 (97%)] Loss: -1352094.500000\n",
      "    epoch          : 51\n",
      "    loss           : -1339247.7348345588\n",
      "    val_loss       : -1263506.804107666\n",
      "Train Epoch: 52 [0/17352 (0%)] Loss: -1337621.875000\n",
      "Train Epoch: 52 [1408/17352 (8%)] Loss: -1328185.375000\n",
      "Train Epoch: 52 [2816/17352 (16%)] Loss: -1342399.000000\n",
      "Train Epoch: 52 [4224/17352 (24%)] Loss: -1355095.500000\n",
      "Train Epoch: 52 [5632/17352 (32%)] Loss: -1339006.750000\n",
      "Train Epoch: 52 [7040/17352 (41%)] Loss: -1341264.625000\n",
      "Train Epoch: 52 [8448/17352 (49%)] Loss: -1338239.000000\n",
      "Train Epoch: 52 [9856/17352 (57%)] Loss: -1356192.625000\n",
      "Train Epoch: 52 [11264/17352 (65%)] Loss: -1337965.250000\n",
      "Train Epoch: 52 [12672/17352 (73%)] Loss: -1363589.750000\n",
      "Train Epoch: 52 [14080/17352 (81%)] Loss: -1335906.750000\n",
      "Train Epoch: 52 [15488/17352 (89%)] Loss: -1337845.875000\n",
      "Train Epoch: 52 [16896/17352 (97%)] Loss: -1339106.500000\n",
      "    epoch          : 52\n",
      "    loss           : -1339256.7614889706\n",
      "    val_loss       : -1263498.7053222656\n",
      "Train Epoch: 53 [0/17352 (0%)] Loss: -1340235.125000\n",
      "Train Epoch: 53 [1408/17352 (8%)] Loss: -1333903.500000\n",
      "Train Epoch: 53 [2816/17352 (16%)] Loss: -1333863.875000\n",
      "Train Epoch: 53 [4224/17352 (24%)] Loss: -1338931.875000\n",
      "Train Epoch: 53 [5632/17352 (32%)] Loss: -1343530.750000\n",
      "Train Epoch: 53 [7040/17352 (41%)] Loss: -1343206.250000\n",
      "Train Epoch: 53 [8448/17352 (49%)] Loss: -1352291.125000\n",
      "Train Epoch: 53 [9856/17352 (57%)] Loss: -1349051.250000\n",
      "Train Epoch: 53 [11264/17352 (65%)] Loss: -1340727.500000\n",
      "Train Epoch: 53 [12672/17352 (73%)] Loss: -1332737.875000\n",
      "Train Epoch: 53 [14080/17352 (81%)] Loss: -1338240.500000\n",
      "Train Epoch: 53 [15488/17352 (89%)] Loss: -1338836.250000\n",
      "Train Epoch: 53 [16896/17352 (97%)] Loss: -1345673.750000\n",
      "    epoch          : 53\n",
      "    loss           : -1339260.5735294118\n",
      "    val_loss       : -1263512.93258667\n",
      "Train Epoch: 54 [0/17352 (0%)] Loss: -1352693.000000\n",
      "Train Epoch: 54 [1408/17352 (8%)] Loss: -1323093.625000\n",
      "Train Epoch: 54 [2816/17352 (16%)] Loss: -1348264.500000\n",
      "Train Epoch: 54 [4224/17352 (24%)] Loss: -1342353.750000\n",
      "Train Epoch: 54 [5632/17352 (32%)] Loss: -1343822.125000\n",
      "Train Epoch: 54 [7040/17352 (41%)] Loss: -1341391.750000\n",
      "Train Epoch: 54 [8448/17352 (49%)] Loss: -1335921.500000\n",
      "Train Epoch: 54 [9856/17352 (57%)] Loss: -1346576.750000\n",
      "Train Epoch: 54 [11264/17352 (65%)] Loss: -1340733.125000\n",
      "Train Epoch: 54 [12672/17352 (73%)] Loss: -1353327.000000\n",
      "Train Epoch: 54 [14080/17352 (81%)] Loss: -1343851.000000\n",
      "Train Epoch: 54 [15488/17352 (89%)] Loss: -1341281.750000\n",
      "Train Epoch: 54 [16896/17352 (97%)] Loss: -1352588.125000\n",
      "    epoch          : 54\n",
      "    loss           : -1339257.3088235294\n",
      "    val_loss       : -1263521.1028747559\n",
      "Train Epoch: 55 [0/17352 (0%)] Loss: -1339741.000000\n",
      "Train Epoch: 55 [1408/17352 (8%)] Loss: -1340631.625000\n",
      "Train Epoch: 55 [2816/17352 (16%)] Loss: -1345715.500000\n",
      "Train Epoch: 55 [4224/17352 (24%)] Loss: -1345131.875000\n",
      "Train Epoch: 55 [5632/17352 (32%)] Loss: -1352591.625000\n",
      "Train Epoch: 55 [7040/17352 (41%)] Loss: -1350198.875000\n",
      "Train Epoch: 55 [8448/17352 (49%)] Loss: -1347279.750000\n",
      "Train Epoch: 55 [9856/17352 (57%)] Loss: -1340258.875000\n",
      "Train Epoch: 55 [11264/17352 (65%)] Loss: -1342385.250000\n",
      "Train Epoch: 55 [12672/17352 (73%)] Loss: -1349740.750000\n",
      "Train Epoch: 55 [14080/17352 (81%)] Loss: -1351958.875000\n",
      "Train Epoch: 55 [15488/17352 (89%)] Loss: -1343602.375000\n",
      "Train Epoch: 55 [16896/17352 (97%)] Loss: -1342034.750000\n",
      "    epoch          : 55\n",
      "    loss           : -1339268.8993566176\n",
      "    val_loss       : -1263524.564239502\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0427_093817/checkpoint-epoch55.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 56 [0/17352 (0%)] Loss: -1324869.625000\n",
      "Train Epoch: 56 [1408/17352 (8%)] Loss: -1347776.000000\n",
      "Train Epoch: 56 [2816/17352 (16%)] Loss: -1348026.250000\n",
      "Train Epoch: 56 [4224/17352 (24%)] Loss: -1354539.500000\n",
      "Train Epoch: 56 [5632/17352 (32%)] Loss: -1347033.750000\n",
      "Train Epoch: 56 [7040/17352 (41%)] Loss: -1338643.500000\n",
      "Train Epoch: 56 [8448/17352 (49%)] Loss: -1349597.125000\n",
      "Train Epoch: 56 [9856/17352 (57%)] Loss: -1337871.125000\n",
      "Train Epoch: 56 [11264/17352 (65%)] Loss: -1350208.500000\n",
      "Train Epoch: 56 [12672/17352 (73%)] Loss: -1348588.750000\n",
      "Train Epoch: 56 [14080/17352 (81%)] Loss: -1347377.500000\n",
      "Train Epoch: 56 [15488/17352 (89%)] Loss: -1337792.125000\n",
      "Train Epoch: 56 [16896/17352 (97%)] Loss: -1339312.750000\n",
      "    epoch          : 56\n",
      "    loss           : -1339274.3166360294\n",
      "    val_loss       : -1263505.1439208984\n",
      "Train Epoch: 57 [0/17352 (0%)] Loss: -1349790.875000\n",
      "Train Epoch: 57 [1408/17352 (8%)] Loss: -1348525.250000\n",
      "Train Epoch: 57 [2816/17352 (16%)] Loss: -1341314.000000\n",
      "Train Epoch: 57 [4224/17352 (24%)] Loss: -1334699.750000\n",
      "Train Epoch: 57 [5632/17352 (32%)] Loss: -1340132.500000\n",
      "Train Epoch: 57 [7040/17352 (41%)] Loss: -1327103.500000\n",
      "Train Epoch: 57 [8448/17352 (49%)] Loss: -1342207.625000\n",
      "Train Epoch: 57 [9856/17352 (57%)] Loss: -1357210.250000\n",
      "Train Epoch: 57 [11264/17352 (65%)] Loss: -1342308.125000\n",
      "Train Epoch: 57 [12672/17352 (73%)] Loss: -1335942.000000\n",
      "Train Epoch: 57 [14080/17352 (81%)] Loss: -1339289.875000\n",
      "Train Epoch: 57 [15488/17352 (89%)] Loss: -1337128.500000\n",
      "Train Epoch: 57 [16896/17352 (97%)] Loss: -1353244.000000\n",
      "    epoch          : 57\n",
      "    loss           : -1339274.3892463236\n",
      "    val_loss       : -1263514.5364074707\n",
      "Train Epoch: 58 [0/17352 (0%)] Loss: -1350887.125000\n",
      "Train Epoch: 58 [1408/17352 (8%)] Loss: -1342072.125000\n",
      "Train Epoch: 58 [2816/17352 (16%)] Loss: -1345382.875000\n",
      "Train Epoch: 58 [4224/17352 (24%)] Loss: -1341863.750000\n",
      "Train Epoch: 58 [5632/17352 (32%)] Loss: -1364801.500000\n",
      "Train Epoch: 58 [7040/17352 (41%)] Loss: -1353791.750000\n",
      "Train Epoch: 58 [8448/17352 (49%)] Loss: -1346129.250000\n",
      "Train Epoch: 58 [9856/17352 (57%)] Loss: -1346179.875000\n",
      "Train Epoch: 58 [11264/17352 (65%)] Loss: -1351800.000000\n",
      "Train Epoch: 58 [12672/17352 (73%)] Loss: -1355620.125000\n",
      "Train Epoch: 58 [14080/17352 (81%)] Loss: -1341867.625000\n",
      "Train Epoch: 58 [15488/17352 (89%)] Loss: -1341211.625000\n",
      "Train Epoch: 58 [16896/17352 (97%)] Loss: -1359137.375000\n",
      "    epoch          : 58\n",
      "    loss           : -1339277.231617647\n",
      "    val_loss       : -1263522.8824768066\n",
      "Train Epoch: 59 [0/17352 (0%)] Loss: -1355419.750000\n",
      "Train Epoch: 59 [1408/17352 (8%)] Loss: -1336804.875000\n",
      "Train Epoch: 59 [2816/17352 (16%)] Loss: -1353656.500000\n",
      "Train Epoch: 59 [4224/17352 (24%)] Loss: -1351891.125000\n",
      "Train Epoch: 59 [5632/17352 (32%)] Loss: -1360074.500000\n",
      "Train Epoch: 59 [7040/17352 (41%)] Loss: -1343849.375000\n",
      "Train Epoch: 59 [8448/17352 (49%)] Loss: -1331463.375000\n",
      "Train Epoch: 59 [9856/17352 (57%)] Loss: -1352619.375000\n",
      "Train Epoch: 59 [11264/17352 (65%)] Loss: -1347551.625000\n",
      "Train Epoch: 59 [12672/17352 (73%)] Loss: -1326868.500000\n",
      "Train Epoch: 59 [14080/17352 (81%)] Loss: -1353816.250000\n",
      "Train Epoch: 59 [15488/17352 (89%)] Loss: -1337843.125000\n",
      "Train Epoch: 59 [16896/17352 (97%)] Loss: -1344273.500000\n",
      "    epoch          : 59\n",
      "    loss           : -1339279.7228860294\n",
      "    val_loss       : -1263523.9700317383\n",
      "Train Epoch: 60 [0/17352 (0%)] Loss: -1326765.500000\n",
      "Train Epoch: 60 [1408/17352 (8%)] Loss: -1349806.625000\n",
      "Train Epoch: 60 [2816/17352 (16%)] Loss: -1343362.375000\n",
      "Train Epoch: 60 [4224/17352 (24%)] Loss: -1341258.500000\n",
      "Train Epoch: 60 [5632/17352 (32%)] Loss: -1340322.375000\n",
      "Train Epoch: 60 [7040/17352 (41%)] Loss: -1351190.750000\n",
      "Train Epoch: 60 [8448/17352 (49%)] Loss: -1339106.750000\n",
      "Train Epoch: 60 [9856/17352 (57%)] Loss: -1345445.375000\n",
      "Train Epoch: 60 [11264/17352 (65%)] Loss: -1344684.000000\n",
      "Train Epoch: 60 [12672/17352 (73%)] Loss: -1347385.125000\n",
      "Train Epoch: 60 [14080/17352 (81%)] Loss: -1356157.625000\n",
      "Train Epoch: 60 [15488/17352 (89%)] Loss: -1341431.375000\n",
      "Train Epoch: 60 [16896/17352 (97%)] Loss: -1349860.375000\n",
      "    epoch          : 60\n",
      "    loss           : -1339289.369944853\n",
      "    val_loss       : -1263534.432220459\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0427_093817/checkpoint-epoch60.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 61 [0/17352 (0%)] Loss: -1352458.875000\n",
      "Train Epoch: 61 [1408/17352 (8%)] Loss: -1335500.125000\n",
      "Train Epoch: 61 [2816/17352 (16%)] Loss: -1340044.000000\n",
      "Train Epoch: 61 [4224/17352 (24%)] Loss: -1353574.125000\n",
      "Train Epoch: 61 [5632/17352 (32%)] Loss: -1345752.375000\n",
      "Train Epoch: 61 [7040/17352 (41%)] Loss: -1343333.125000\n",
      "Train Epoch: 61 [8448/17352 (49%)] Loss: -1328151.625000\n",
      "Train Epoch: 61 [9856/17352 (57%)] Loss: -1343163.250000\n",
      "Train Epoch: 61 [11264/17352 (65%)] Loss: -1340787.500000\n",
      "Train Epoch: 61 [12672/17352 (73%)] Loss: -1337354.500000\n",
      "Train Epoch: 61 [14080/17352 (81%)] Loss: -1360760.500000\n",
      "Train Epoch: 61 [15488/17352 (89%)] Loss: -1342614.875000\n",
      "Train Epoch: 61 [16896/17352 (97%)] Loss: -1347090.625000\n",
      "    epoch          : 61\n",
      "    loss           : -1339286.9117647058\n",
      "    val_loss       : -1263538.2247314453\n",
      "Train Epoch: 62 [0/17352 (0%)] Loss: -1325993.000000\n",
      "Train Epoch: 62 [1408/17352 (8%)] Loss: -1359054.750000\n",
      "Train Epoch: 62 [2816/17352 (16%)] Loss: -1345254.375000\n",
      "Train Epoch: 62 [4224/17352 (24%)] Loss: -1346424.000000\n",
      "Train Epoch: 62 [5632/17352 (32%)] Loss: -1344333.625000\n",
      "Train Epoch: 62 [7040/17352 (41%)] Loss: -1333097.500000\n",
      "Train Epoch: 62 [8448/17352 (49%)] Loss: -1347641.000000\n",
      "Train Epoch: 62 [9856/17352 (57%)] Loss: -1348650.000000\n",
      "Train Epoch: 62 [11264/17352 (65%)] Loss: -1354820.000000\n",
      "Train Epoch: 62 [12672/17352 (73%)] Loss: -1355960.875000\n",
      "Train Epoch: 62 [14080/17352 (81%)] Loss: -1340059.000000\n",
      "Train Epoch: 62 [15488/17352 (89%)] Loss: -1348134.250000\n",
      "Train Epoch: 62 [16896/17352 (97%)] Loss: -1344892.750000\n",
      "    epoch          : 62\n",
      "    loss           : -1339288.2697610294\n",
      "    val_loss       : -1263551.5921020508\n",
      "Train Epoch: 63 [0/17352 (0%)] Loss: -1337835.000000\n",
      "Train Epoch: 63 [1408/17352 (8%)] Loss: -1340999.250000\n",
      "Train Epoch: 63 [2816/17352 (16%)] Loss: -1354438.375000\n",
      "Train Epoch: 63 [4224/17352 (24%)] Loss: -1343602.875000\n",
      "Train Epoch: 63 [5632/17352 (32%)] Loss: -1337699.375000\n",
      "Train Epoch: 63 [7040/17352 (41%)] Loss: -1331427.125000\n",
      "Train Epoch: 63 [8448/17352 (49%)] Loss: -1349252.750000\n",
      "Train Epoch: 63 [9856/17352 (57%)] Loss: -1342115.125000\n",
      "Train Epoch: 63 [11264/17352 (65%)] Loss: -1337719.125000\n",
      "Train Epoch: 63 [12672/17352 (73%)] Loss: -1330309.250000\n",
      "Train Epoch: 63 [14080/17352 (81%)] Loss: -1337590.250000\n",
      "Train Epoch: 63 [15488/17352 (89%)] Loss: -1349446.375000\n",
      "Train Epoch: 63 [16896/17352 (97%)] Loss: -1328754.375000\n",
      "    epoch          : 63\n",
      "    loss           : -1339296.4365808824\n",
      "    val_loss       : -1263550.1592407227\n",
      "Train Epoch: 64 [0/17352 (0%)] Loss: -1329985.250000\n",
      "Train Epoch: 64 [1408/17352 (8%)] Loss: -1339347.250000\n",
      "Train Epoch: 64 [2816/17352 (16%)] Loss: -1346145.750000\n",
      "Train Epoch: 64 [4224/17352 (24%)] Loss: -1361552.375000\n",
      "Train Epoch: 64 [5632/17352 (32%)] Loss: -1336476.250000\n",
      "Train Epoch: 64 [7040/17352 (41%)] Loss: -1337530.000000\n",
      "Train Epoch: 64 [8448/17352 (49%)] Loss: -1343166.875000\n",
      "Train Epoch: 64 [9856/17352 (57%)] Loss: -1339410.625000\n",
      "Train Epoch: 64 [11264/17352 (65%)] Loss: -1341037.125000\n",
      "Train Epoch: 64 [12672/17352 (73%)] Loss: -1342977.375000\n",
      "Train Epoch: 64 [14080/17352 (81%)] Loss: -1337329.000000\n",
      "Train Epoch: 64 [15488/17352 (89%)] Loss: -1346699.875000\n",
      "Train Epoch: 64 [16896/17352 (97%)] Loss: -1361442.875000\n",
      "    epoch          : 64\n",
      "    loss           : -1339297.4296875\n",
      "    val_loss       : -1263553.9242248535\n",
      "Train Epoch: 65 [0/17352 (0%)] Loss: -1346053.625000\n",
      "Train Epoch: 65 [1408/17352 (8%)] Loss: -1351149.000000\n",
      "Train Epoch: 65 [2816/17352 (16%)] Loss: -1352750.625000\n",
      "Train Epoch: 65 [4224/17352 (24%)] Loss: -1339960.750000\n",
      "Train Epoch: 65 [5632/17352 (32%)] Loss: -1351626.000000\n",
      "Train Epoch: 65 [7040/17352 (41%)] Loss: -1336685.250000\n",
      "Train Epoch: 65 [8448/17352 (49%)] Loss: -1338362.500000\n",
      "Train Epoch: 65 [9856/17352 (57%)] Loss: -1336180.875000\n",
      "Train Epoch: 65 [11264/17352 (65%)] Loss: -1360392.375000\n",
      "Train Epoch: 65 [12672/17352 (73%)] Loss: -1341415.875000\n",
      "Train Epoch: 65 [14080/17352 (81%)] Loss: -1352141.250000\n",
      "Train Epoch: 65 [15488/17352 (89%)] Loss: -1334471.375000\n",
      "Train Epoch: 65 [16896/17352 (97%)] Loss: -1356216.250000\n",
      "    epoch          : 65\n",
      "    loss           : -1339300.458180147\n",
      "    val_loss       : -1263534.4264831543\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0427_093817/checkpoint-epoch65.pth ...\n",
      "Train Epoch: 66 [0/17352 (0%)] Loss: -1338557.875000\n",
      "Train Epoch: 66 [1408/17352 (8%)] Loss: -1332511.750000\n",
      "Train Epoch: 66 [2816/17352 (16%)] Loss: -1338672.000000\n",
      "Train Epoch: 66 [4224/17352 (24%)] Loss: -1341877.875000\n",
      "Train Epoch: 66 [5632/17352 (32%)] Loss: -1339996.000000\n",
      "Train Epoch: 66 [7040/17352 (41%)] Loss: -1356552.875000\n",
      "Train Epoch: 66 [8448/17352 (49%)] Loss: -1335153.625000\n",
      "Train Epoch: 66 [9856/17352 (57%)] Loss: -1357312.375000\n",
      "Train Epoch: 66 [11264/17352 (65%)] Loss: -1345193.750000\n",
      "Train Epoch: 66 [12672/17352 (73%)] Loss: -1336482.125000\n",
      "Train Epoch: 66 [14080/17352 (81%)] Loss: -1348128.750000\n",
      "Train Epoch: 66 [15488/17352 (89%)] Loss: -1329474.375000\n",
      "Train Epoch: 66 [16896/17352 (97%)] Loss: -1349590.625000\n",
      "    epoch          : 66\n",
      "    loss           : -1339293.4977022058\n",
      "    val_loss       : -1263557.3195800781\n",
      "Train Epoch: 67 [0/17352 (0%)] Loss: -1332851.125000\n",
      "Train Epoch: 67 [1408/17352 (8%)] Loss: -1338054.125000\n",
      "Train Epoch: 67 [2816/17352 (16%)] Loss: -1348405.375000\n",
      "Train Epoch: 67 [4224/17352 (24%)] Loss: -1348987.875000\n",
      "Train Epoch: 67 [5632/17352 (32%)] Loss: -1338975.500000\n",
      "Train Epoch: 67 [7040/17352 (41%)] Loss: -1347007.000000\n",
      "Train Epoch: 67 [8448/17352 (49%)] Loss: -1350069.000000\n",
      "Train Epoch: 67 [9856/17352 (57%)] Loss: -1339834.625000\n",
      "Train Epoch: 67 [11264/17352 (65%)] Loss: -1338696.250000\n",
      "Train Epoch: 67 [12672/17352 (73%)] Loss: -1361623.750000\n",
      "Train Epoch: 67 [14080/17352 (81%)] Loss: -1350585.375000\n",
      "Train Epoch: 67 [15488/17352 (89%)] Loss: -1344403.750000\n",
      "Train Epoch: 67 [16896/17352 (97%)] Loss: -1345559.875000\n",
      "    epoch          : 67\n",
      "    loss           : -1339300.8134191176\n",
      "    val_loss       : -1263534.9609985352\n",
      "Train Epoch: 68 [0/17352 (0%)] Loss: -1343708.375000\n",
      "Train Epoch: 68 [1408/17352 (8%)] Loss: -1342450.000000\n",
      "Train Epoch: 68 [2816/17352 (16%)] Loss: -1342511.375000\n",
      "Train Epoch: 68 [4224/17352 (24%)] Loss: -1344301.500000\n",
      "Train Epoch: 68 [5632/17352 (32%)] Loss: -1338592.750000\n",
      "Train Epoch: 68 [7040/17352 (41%)] Loss: -1346037.000000\n",
      "Train Epoch: 68 [8448/17352 (49%)] Loss: -1345427.500000\n",
      "Train Epoch: 68 [9856/17352 (57%)] Loss: -1341332.500000\n",
      "Train Epoch: 68 [11264/17352 (65%)] Loss: -1338843.625000\n",
      "Train Epoch: 68 [12672/17352 (73%)] Loss: -1353198.375000\n",
      "Train Epoch: 68 [14080/17352 (81%)] Loss: -1347466.750000\n",
      "Train Epoch: 68 [15488/17352 (89%)] Loss: -1334850.875000\n",
      "Train Epoch: 68 [16896/17352 (97%)] Loss: -1340983.750000\n",
      "    epoch          : 68\n",
      "    loss           : -1339307.3671875\n",
      "    val_loss       : -1263560.1497497559\n",
      "Train Epoch: 69 [0/17352 (0%)] Loss: -1344068.875000\n",
      "Train Epoch: 69 [1408/17352 (8%)] Loss: -1345143.000000\n",
      "Train Epoch: 69 [2816/17352 (16%)] Loss: -1353637.000000\n",
      "Train Epoch: 69 [4224/17352 (24%)] Loss: -1342069.875000\n",
      "Train Epoch: 69 [5632/17352 (32%)] Loss: -1345380.375000\n",
      "Train Epoch: 69 [7040/17352 (41%)] Loss: -1343701.875000\n",
      "Train Epoch: 69 [8448/17352 (49%)] Loss: -1337621.500000\n",
      "Train Epoch: 69 [9856/17352 (57%)] Loss: -1357506.625000\n",
      "Train Epoch: 69 [11264/17352 (65%)] Loss: -1331811.625000\n",
      "Train Epoch: 69 [12672/17352 (73%)] Loss: -1344227.750000\n",
      "Train Epoch: 69 [14080/17352 (81%)] Loss: -1346229.625000\n",
      "Train Epoch: 69 [15488/17352 (89%)] Loss: -1334038.500000\n",
      "Train Epoch: 69 [16896/17352 (97%)] Loss: -1342525.625000\n",
      "    epoch          : 69\n",
      "    loss           : -1339304.7444852942\n",
      "    val_loss       : -1263564.9993896484\n",
      "Train Epoch: 70 [0/17352 (0%)] Loss: -1340154.250000\n",
      "Train Epoch: 70 [1408/17352 (8%)] Loss: -1334843.125000\n",
      "Train Epoch: 70 [2816/17352 (16%)] Loss: -1347885.750000\n",
      "Train Epoch: 70 [4224/17352 (24%)] Loss: -1340831.250000\n",
      "Train Epoch: 70 [5632/17352 (32%)] Loss: -1363088.500000\n",
      "Train Epoch: 70 [7040/17352 (41%)] Loss: -1345675.625000\n",
      "Train Epoch: 70 [8448/17352 (49%)] Loss: -1336819.875000\n",
      "Train Epoch: 70 [9856/17352 (57%)] Loss: -1327238.875000\n",
      "Train Epoch: 70 [11264/17352 (65%)] Loss: -1334800.500000\n",
      "Train Epoch: 70 [12672/17352 (73%)] Loss: -1344602.625000\n",
      "Train Epoch: 70 [14080/17352 (81%)] Loss: -1342059.250000\n",
      "Train Epoch: 70 [15488/17352 (89%)] Loss: -1350144.125000\n",
      "Train Epoch: 70 [16896/17352 (97%)] Loss: -1344963.875000\n",
      "    epoch          : 70\n",
      "    loss           : -1339309.6654411764\n",
      "    val_loss       : -1263562.4618530273\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0427_093817/checkpoint-epoch70.pth ...\n",
      "Train Epoch: 71 [0/17352 (0%)] Loss: -1342738.500000\n",
      "Train Epoch: 71 [1408/17352 (8%)] Loss: -1331110.125000\n",
      "Train Epoch: 71 [2816/17352 (16%)] Loss: -1328990.250000\n",
      "Train Epoch: 71 [4224/17352 (24%)] Loss: -1337668.375000\n",
      "Train Epoch: 71 [5632/17352 (32%)] Loss: -1342113.500000\n",
      "Train Epoch: 71 [7040/17352 (41%)] Loss: -1348512.625000\n",
      "Train Epoch: 71 [8448/17352 (49%)] Loss: -1352965.375000\n",
      "Train Epoch: 71 [9856/17352 (57%)] Loss: -1352742.000000\n",
      "Train Epoch: 71 [11264/17352 (65%)] Loss: -1355799.875000\n",
      "Train Epoch: 71 [12672/17352 (73%)] Loss: -1346583.250000\n",
      "Train Epoch: 71 [14080/17352 (81%)] Loss: -1351125.750000\n",
      "Train Epoch: 71 [15488/17352 (89%)] Loss: -1337738.375000\n",
      "Train Epoch: 71 [16896/17352 (97%)] Loss: -1353550.000000\n",
      "    epoch          : 71\n",
      "    loss           : -1339306.9921875\n",
      "    val_loss       : -1263563.3858642578\n",
      "Train Epoch: 72 [0/17352 (0%)] Loss: -1356871.250000\n",
      "Train Epoch: 72 [1408/17352 (8%)] Loss: -1333710.000000\n",
      "Train Epoch: 72 [2816/17352 (16%)] Loss: -1338784.375000\n",
      "Train Epoch: 72 [4224/17352 (24%)] Loss: -1348041.750000\n",
      "Train Epoch: 72 [5632/17352 (32%)] Loss: -1342800.500000\n",
      "Train Epoch: 72 [7040/17352 (41%)] Loss: -1357821.875000\n",
      "Train Epoch: 72 [8448/17352 (49%)] Loss: -1338744.250000\n",
      "Train Epoch: 72 [9856/17352 (57%)] Loss: -1347432.125000\n",
      "Train Epoch: 72 [11264/17352 (65%)] Loss: -1347642.750000\n",
      "Train Epoch: 72 [12672/17352 (73%)] Loss: -1347375.625000\n",
      "Train Epoch: 72 [14080/17352 (81%)] Loss: -1348218.250000\n",
      "Train Epoch: 72 [15488/17352 (89%)] Loss: -1362520.000000\n",
      "Train Epoch: 72 [16896/17352 (97%)] Loss: -1330027.000000\n",
      "    epoch          : 72\n",
      "    loss           : -1339313.3363970588\n",
      "    val_loss       : -1263563.7109375\n",
      "Train Epoch: 73 [0/17352 (0%)] Loss: -1342076.625000\n",
      "Train Epoch: 73 [1408/17352 (8%)] Loss: -1356376.625000\n",
      "Train Epoch: 73 [2816/17352 (16%)] Loss: -1335262.500000\n",
      "Train Epoch: 73 [4224/17352 (24%)] Loss: -1344323.500000\n",
      "Train Epoch: 73 [5632/17352 (32%)] Loss: -1346069.375000\n",
      "Train Epoch: 73 [7040/17352 (41%)] Loss: -1331089.000000\n",
      "Train Epoch: 73 [8448/17352 (49%)] Loss: -1346959.375000\n",
      "Train Epoch: 73 [9856/17352 (57%)] Loss: -1337140.750000\n",
      "Train Epoch: 73 [11264/17352 (65%)] Loss: -1349739.375000\n",
      "Train Epoch: 73 [12672/17352 (73%)] Loss: -1339234.500000\n",
      "Train Epoch: 73 [14080/17352 (81%)] Loss: -1348893.000000\n",
      "Train Epoch: 73 [15488/17352 (89%)] Loss: -1332137.500000\n",
      "Train Epoch: 73 [16896/17352 (97%)] Loss: -1348099.125000\n",
      "    epoch          : 73\n",
      "    loss           : -1339317.3671875\n",
      "    val_loss       : -1263561.5344543457\n",
      "Train Epoch: 74 [0/17352 (0%)] Loss: -1326063.500000\n",
      "Train Epoch: 74 [1408/17352 (8%)] Loss: -1348437.125000\n",
      "Train Epoch: 74 [2816/17352 (16%)] Loss: -1341381.625000\n",
      "Train Epoch: 74 [4224/17352 (24%)] Loss: -1343091.000000\n",
      "Train Epoch: 74 [5632/17352 (32%)] Loss: -1339363.000000\n",
      "Train Epoch: 74 [7040/17352 (41%)] Loss: -1354616.250000\n",
      "Train Epoch: 74 [8448/17352 (49%)] Loss: -1324516.875000\n",
      "Train Epoch: 74 [9856/17352 (57%)] Loss: -1341945.000000\n",
      "Train Epoch: 74 [11264/17352 (65%)] Loss: -1342840.875000\n",
      "Train Epoch: 74 [12672/17352 (73%)] Loss: -1344962.875000\n",
      "Train Epoch: 74 [14080/17352 (81%)] Loss: -1351646.750000\n",
      "Train Epoch: 74 [15488/17352 (89%)] Loss: -1343264.750000\n",
      "Train Epoch: 74 [16896/17352 (97%)] Loss: -1350291.125000\n",
      "    epoch          : 74\n",
      "    loss           : -1339320.2734375\n",
      "    val_loss       : -1263560.810974121\n",
      "Train Epoch: 75 [0/17352 (0%)] Loss: -1339335.000000\n",
      "Train Epoch: 75 [1408/17352 (8%)] Loss: -1343031.500000\n",
      "Train Epoch: 75 [2816/17352 (16%)] Loss: -1357053.000000\n",
      "Train Epoch: 75 [4224/17352 (24%)] Loss: -1325398.000000\n",
      "Train Epoch: 75 [5632/17352 (32%)] Loss: -1341484.500000\n",
      "Train Epoch: 75 [7040/17352 (41%)] Loss: -1334449.875000\n",
      "Train Epoch: 75 [8448/17352 (49%)] Loss: -1343664.750000\n",
      "Train Epoch: 75 [9856/17352 (57%)] Loss: -1340553.125000\n",
      "Train Epoch: 75 [11264/17352 (65%)] Loss: -1346635.125000\n",
      "Train Epoch: 75 [12672/17352 (73%)] Loss: -1338509.375000\n",
      "Train Epoch: 75 [14080/17352 (81%)] Loss: -1332532.000000\n",
      "Train Epoch: 75 [15488/17352 (89%)] Loss: -1349160.750000\n",
      "Train Epoch: 75 [16896/17352 (97%)] Loss: -1353645.500000\n",
      "    epoch          : 75\n",
      "    loss           : -1339320.5082720588\n",
      "    val_loss       : -1263571.8554077148\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0427_093817/checkpoint-epoch75.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 76 [0/17352 (0%)] Loss: -1345188.500000\n",
      "Train Epoch: 76 [1408/17352 (8%)] Loss: -1344605.625000\n",
      "Train Epoch: 76 [2816/17352 (16%)] Loss: -1354834.500000\n",
      "Train Epoch: 76 [4224/17352 (24%)] Loss: -1339214.750000\n",
      "Train Epoch: 76 [5632/17352 (32%)] Loss: -1341220.500000\n",
      "Train Epoch: 76 [7040/17352 (41%)] Loss: -1345399.625000\n",
      "Train Epoch: 76 [8448/17352 (49%)] Loss: -1350324.625000\n",
      "Train Epoch: 76 [9856/17352 (57%)] Loss: -1341987.125000\n",
      "Train Epoch: 76 [11264/17352 (65%)] Loss: -1348410.625000\n",
      "Train Epoch: 76 [12672/17352 (73%)] Loss: -1336057.625000\n",
      "Train Epoch: 76 [14080/17352 (81%)] Loss: -1344719.000000\n",
      "Train Epoch: 76 [15488/17352 (89%)] Loss: -1345827.625000\n",
      "Train Epoch: 76 [16896/17352 (97%)] Loss: -1340164.375000\n",
      "    epoch          : 76\n",
      "    loss           : -1339324.8998161764\n",
      "    val_loss       : -1263569.1054382324\n",
      "Train Epoch: 77 [0/17352 (0%)] Loss: -1337505.750000\n",
      "Train Epoch: 77 [1408/17352 (8%)] Loss: -1341240.000000\n",
      "Train Epoch: 77 [2816/17352 (16%)] Loss: -1352679.875000\n",
      "Train Epoch: 77 [4224/17352 (24%)] Loss: -1344401.000000\n",
      "Train Epoch: 77 [5632/17352 (32%)] Loss: -1350861.625000\n",
      "Train Epoch: 77 [7040/17352 (41%)] Loss: -1352571.625000\n",
      "Train Epoch: 77 [8448/17352 (49%)] Loss: -1340351.000000\n",
      "Train Epoch: 77 [9856/17352 (57%)] Loss: -1344568.625000\n",
      "Train Epoch: 77 [11264/17352 (65%)] Loss: -1333740.625000\n",
      "Train Epoch: 77 [12672/17352 (73%)] Loss: -1340405.500000\n",
      "Train Epoch: 77 [14080/17352 (81%)] Loss: -1347319.875000\n",
      "Train Epoch: 77 [15488/17352 (89%)] Loss: -1345769.000000\n",
      "Train Epoch: 77 [16896/17352 (97%)] Loss: -1340376.875000\n",
      "    epoch          : 77\n",
      "    loss           : -1339326.6245404412\n",
      "    val_loss       : -1263568.5545654297\n",
      "Train Epoch: 78 [0/17352 (0%)] Loss: -1326790.625000\n",
      "Train Epoch: 78 [1408/17352 (8%)] Loss: -1346286.375000\n",
      "Train Epoch: 78 [2816/17352 (16%)] Loss: -1345021.125000\n",
      "Train Epoch: 78 [4224/17352 (24%)] Loss: -1353571.500000\n",
      "Train Epoch: 78 [5632/17352 (32%)] Loss: -1335504.000000\n",
      "Train Epoch: 78 [7040/17352 (41%)] Loss: -1341500.000000\n",
      "Train Epoch: 78 [8448/17352 (49%)] Loss: -1352059.000000\n",
      "Train Epoch: 78 [9856/17352 (57%)] Loss: -1345630.375000\n",
      "Train Epoch: 78 [11264/17352 (65%)] Loss: -1347449.750000\n",
      "Train Epoch: 78 [12672/17352 (73%)] Loss: -1349158.625000\n",
      "Train Epoch: 78 [14080/17352 (81%)] Loss: -1346568.000000\n",
      "Train Epoch: 78 [15488/17352 (89%)] Loss: -1343939.000000\n",
      "Train Epoch: 78 [16896/17352 (97%)] Loss: -1320607.625000\n",
      "    epoch          : 78\n",
      "    loss           : -1339329.1732536764\n",
      "    val_loss       : -1263581.425567627\n",
      "Train Epoch: 79 [0/17352 (0%)] Loss: -1347931.750000\n",
      "Train Epoch: 79 [1408/17352 (8%)] Loss: -1365481.750000\n",
      "Train Epoch: 79 [2816/17352 (16%)] Loss: -1354478.125000\n",
      "Train Epoch: 79 [4224/17352 (24%)] Loss: -1354377.500000\n",
      "Train Epoch: 79 [5632/17352 (32%)] Loss: -1340923.125000\n",
      "Train Epoch: 79 [7040/17352 (41%)] Loss: -1353182.750000\n",
      "Train Epoch: 79 [8448/17352 (49%)] Loss: -1337432.000000\n",
      "Train Epoch: 79 [9856/17352 (57%)] Loss: -1340818.000000\n",
      "Train Epoch: 79 [11264/17352 (65%)] Loss: -1348334.125000\n",
      "Train Epoch: 79 [12672/17352 (73%)] Loss: -1343776.000000\n",
      "Train Epoch: 79 [14080/17352 (81%)] Loss: -1355950.000000\n",
      "Train Epoch: 79 [15488/17352 (89%)] Loss: -1353367.000000\n",
      "Train Epoch: 79 [16896/17352 (97%)] Loss: -1344373.125000\n",
      "    epoch          : 79\n",
      "    loss           : -1339331.3681066176\n",
      "    val_loss       : -1263587.1225280762\n",
      "Train Epoch: 80 [0/17352 (0%)] Loss: -1338932.500000\n",
      "Train Epoch: 80 [1408/17352 (8%)] Loss: -1351094.500000\n",
      "Train Epoch: 80 [2816/17352 (16%)] Loss: -1336078.500000\n",
      "Train Epoch: 80 [4224/17352 (24%)] Loss: -1360349.250000\n",
      "Train Epoch: 80 [5632/17352 (32%)] Loss: -1337679.750000\n",
      "Train Epoch: 80 [7040/17352 (41%)] Loss: -1346999.500000\n",
      "Train Epoch: 80 [8448/17352 (49%)] Loss: -1343173.500000\n",
      "Train Epoch: 80 [9856/17352 (57%)] Loss: -1355789.375000\n",
      "Train Epoch: 80 [11264/17352 (65%)] Loss: -1361989.375000\n",
      "Train Epoch: 80 [12672/17352 (73%)] Loss: -1345186.375000\n",
      "Train Epoch: 80 [14080/17352 (81%)] Loss: -1335504.625000\n",
      "Train Epoch: 80 [15488/17352 (89%)] Loss: -1338205.125000\n",
      "Train Epoch: 80 [16896/17352 (97%)] Loss: -1345302.125000\n",
      "    epoch          : 80\n",
      "    loss           : -1339334.0275735294\n",
      "    val_loss       : -1263576.6113586426\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0427_093817/checkpoint-epoch80.pth ...\n",
      "Train Epoch: 81 [0/17352 (0%)] Loss: -1336216.750000\n",
      "Train Epoch: 81 [1408/17352 (8%)] Loss: -1344233.250000\n",
      "Train Epoch: 81 [2816/17352 (16%)] Loss: -1356314.500000\n",
      "Train Epoch: 81 [4224/17352 (24%)] Loss: -1354666.625000\n",
      "Train Epoch: 81 [5632/17352 (32%)] Loss: -1338086.125000\n",
      "Train Epoch: 81 [7040/17352 (41%)] Loss: -1336944.500000\n",
      "Train Epoch: 81 [8448/17352 (49%)] Loss: -1339842.250000\n",
      "Train Epoch: 81 [9856/17352 (57%)] Loss: -1353886.500000\n",
      "Train Epoch: 81 [11264/17352 (65%)] Loss: -1342671.000000\n",
      "Train Epoch: 81 [12672/17352 (73%)] Loss: -1340217.250000\n",
      "Train Epoch: 81 [14080/17352 (81%)] Loss: -1342361.125000\n",
      "Train Epoch: 81 [15488/17352 (89%)] Loss: -1361557.750000\n",
      "Train Epoch: 81 [16896/17352 (97%)] Loss: -1348208.250000\n",
      "    epoch          : 81\n",
      "    loss           : -1339336.8671875\n",
      "    val_loss       : -1263590.9418029785\n",
      "Train Epoch: 82 [0/17352 (0%)] Loss: -1350636.875000\n",
      "Train Epoch: 82 [1408/17352 (8%)] Loss: -1344218.000000\n",
      "Train Epoch: 82 [2816/17352 (16%)] Loss: -1329363.250000\n",
      "Train Epoch: 82 [4224/17352 (24%)] Loss: -1350398.250000\n",
      "Train Epoch: 82 [5632/17352 (32%)] Loss: -1345232.250000\n",
      "Train Epoch: 82 [7040/17352 (41%)] Loss: -1336541.500000\n",
      "Train Epoch: 82 [8448/17352 (49%)] Loss: -1329786.625000\n",
      "Train Epoch: 82 [9856/17352 (57%)] Loss: -1336789.250000\n",
      "Train Epoch: 82 [11264/17352 (65%)] Loss: -1356272.750000\n",
      "Train Epoch: 82 [12672/17352 (73%)] Loss: -1347914.625000\n",
      "Train Epoch: 82 [14080/17352 (81%)] Loss: -1346134.250000\n",
      "Train Epoch: 82 [15488/17352 (89%)] Loss: -1342496.750000\n",
      "Train Epoch: 82 [16896/17352 (97%)] Loss: -1342229.250000\n",
      "    epoch          : 82\n",
      "    loss           : -1339340.2513786764\n",
      "    val_loss       : -1263565.8166503906\n",
      "Train Epoch: 83 [0/17352 (0%)] Loss: -1342287.250000\n",
      "Train Epoch: 83 [1408/17352 (8%)] Loss: -1349453.500000\n",
      "Train Epoch: 83 [2816/17352 (16%)] Loss: -1347801.500000\n",
      "Train Epoch: 83 [4224/17352 (24%)] Loss: -1337652.000000\n",
      "Train Epoch: 83 [5632/17352 (32%)] Loss: -1345338.000000\n",
      "Train Epoch: 83 [7040/17352 (41%)] Loss: -1351257.875000\n",
      "Train Epoch: 83 [8448/17352 (49%)] Loss: -1344162.750000\n",
      "Train Epoch: 83 [9856/17352 (57%)] Loss: -1339174.875000\n",
      "Train Epoch: 83 [11264/17352 (65%)] Loss: -1341440.250000\n",
      "Train Epoch: 83 [12672/17352 (73%)] Loss: -1339061.625000\n",
      "Train Epoch: 83 [14080/17352 (81%)] Loss: -1347000.125000\n",
      "Train Epoch: 83 [15488/17352 (89%)] Loss: -1329958.250000\n",
      "Train Epoch: 83 [16896/17352 (97%)] Loss: -1342689.375000\n",
      "    epoch          : 83\n",
      "    loss           : -1339340.671875\n",
      "    val_loss       : -1263585.1275939941\n",
      "Train Epoch: 84 [0/17352 (0%)] Loss: -1352305.125000\n",
      "Train Epoch: 84 [1408/17352 (8%)] Loss: -1352948.625000\n",
      "Train Epoch: 84 [2816/17352 (16%)] Loss: -1349079.250000\n",
      "Train Epoch: 84 [4224/17352 (24%)] Loss: -1350188.125000\n",
      "Train Epoch: 84 [5632/17352 (32%)] Loss: -1344523.750000\n",
      "Train Epoch: 84 [7040/17352 (41%)] Loss: -1344872.625000\n",
      "Train Epoch: 84 [8448/17352 (49%)] Loss: -1340957.500000\n",
      "Train Epoch: 84 [9856/17352 (57%)] Loss: -1351844.250000\n",
      "Train Epoch: 84 [11264/17352 (65%)] Loss: -1326824.875000\n",
      "Train Epoch: 84 [12672/17352 (73%)] Loss: -1343802.750000\n",
      "Train Epoch: 84 [14080/17352 (81%)] Loss: -1346594.250000\n",
      "Train Epoch: 84 [15488/17352 (89%)] Loss: -1340419.500000\n",
      "Train Epoch: 84 [16896/17352 (97%)] Loss: -1347758.750000\n",
      "    epoch          : 84\n",
      "    loss           : -1339344.6879595588\n",
      "    val_loss       : -1263584.2416992188\n",
      "Train Epoch: 85 [0/17352 (0%)] Loss: -1347911.625000\n",
      "Train Epoch: 85 [1408/17352 (8%)] Loss: -1349940.500000\n",
      "Train Epoch: 85 [2816/17352 (16%)] Loss: -1338012.625000\n",
      "Train Epoch: 85 [4224/17352 (24%)] Loss: -1349789.750000\n",
      "Train Epoch: 85 [5632/17352 (32%)] Loss: -1361814.625000\n",
      "Train Epoch: 85 [7040/17352 (41%)] Loss: -1343749.750000\n",
      "Train Epoch: 85 [8448/17352 (49%)] Loss: -1335854.500000\n",
      "Train Epoch: 85 [9856/17352 (57%)] Loss: -1339838.250000\n",
      "Train Epoch: 85 [11264/17352 (65%)] Loss: -1338757.000000\n",
      "Train Epoch: 85 [12672/17352 (73%)] Loss: -1336996.750000\n",
      "Train Epoch: 85 [14080/17352 (81%)] Loss: -1341664.250000\n",
      "Train Epoch: 85 [15488/17352 (89%)] Loss: -1341123.000000\n",
      "Train Epoch: 85 [16896/17352 (97%)] Loss: -1355624.250000\n",
      "    epoch          : 85\n",
      "    loss           : -1339347.2670036764\n",
      "    val_loss       : -1263593.6463012695\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0427_093817/checkpoint-epoch85.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 86 [0/17352 (0%)] Loss: -1350177.625000\n",
      "Train Epoch: 86 [1408/17352 (8%)] Loss: -1348552.125000\n",
      "Train Epoch: 86 [2816/17352 (16%)] Loss: -1335790.125000\n",
      "Train Epoch: 86 [4224/17352 (24%)] Loss: -1351162.375000\n",
      "Train Epoch: 86 [5632/17352 (32%)] Loss: -1354536.125000\n",
      "Train Epoch: 86 [7040/17352 (41%)] Loss: -1341106.500000\n",
      "Train Epoch: 86 [8448/17352 (49%)] Loss: -1337230.250000\n",
      "Train Epoch: 86 [9856/17352 (57%)] Loss: -1334517.250000\n",
      "Train Epoch: 86 [11264/17352 (65%)] Loss: -1339271.000000\n",
      "Train Epoch: 86 [12672/17352 (73%)] Loss: -1328877.625000\n",
      "Train Epoch: 86 [14080/17352 (81%)] Loss: -1354291.625000\n",
      "Train Epoch: 86 [15488/17352 (89%)] Loss: -1345236.875000\n",
      "Train Epoch: 86 [16896/17352 (97%)] Loss: -1342272.625000\n",
      "    epoch          : 86\n",
      "    loss           : -1339343.9241727942\n",
      "    val_loss       : -1263595.0144958496\n",
      "Train Epoch: 87 [0/17352 (0%)] Loss: -1342109.750000\n",
      "Train Epoch: 87 [1408/17352 (8%)] Loss: -1345005.875000\n",
      "Train Epoch: 87 [2816/17352 (16%)] Loss: -1341305.125000\n",
      "Train Epoch: 87 [4224/17352 (24%)] Loss: -1343198.250000\n",
      "Train Epoch: 87 [5632/17352 (32%)] Loss: -1348574.000000\n",
      "Train Epoch: 87 [7040/17352 (41%)] Loss: -1348546.250000\n",
      "Train Epoch: 87 [8448/17352 (49%)] Loss: -1347671.125000\n",
      "Train Epoch: 87 [9856/17352 (57%)] Loss: -1355547.875000\n",
      "Train Epoch: 87 [11264/17352 (65%)] Loss: -1338307.125000\n",
      "Train Epoch: 87 [12672/17352 (73%)] Loss: -1350531.625000\n",
      "Train Epoch: 87 [14080/17352 (81%)] Loss: -1354112.500000\n",
      "Train Epoch: 87 [15488/17352 (89%)] Loss: -1341303.500000\n",
      "Train Epoch: 87 [16896/17352 (97%)] Loss: -1337533.125000\n",
      "    epoch          : 87\n",
      "    loss           : -1339347.1263786764\n",
      "    val_loss       : -1263594.7079467773\n",
      "Train Epoch: 88 [0/17352 (0%)] Loss: -1347781.625000\n",
      "Train Epoch: 88 [1408/17352 (8%)] Loss: -1344022.875000\n",
      "Train Epoch: 88 [2816/17352 (16%)] Loss: -1345408.375000\n",
      "Train Epoch: 88 [4224/17352 (24%)] Loss: -1341979.500000\n",
      "Train Epoch: 88 [5632/17352 (32%)] Loss: -1351233.875000\n",
      "Train Epoch: 88 [7040/17352 (41%)] Loss: -1342246.625000\n",
      "Train Epoch: 88 [8448/17352 (49%)] Loss: -1339819.875000\n",
      "Train Epoch: 88 [9856/17352 (57%)] Loss: -1351430.625000\n",
      "Train Epoch: 88 [11264/17352 (65%)] Loss: -1345866.500000\n",
      "Train Epoch: 88 [12672/17352 (73%)] Loss: -1345158.375000\n",
      "Train Epoch: 88 [14080/17352 (81%)] Loss: -1331260.125000\n",
      "Train Epoch: 88 [15488/17352 (89%)] Loss: -1345679.625000\n",
      "Train Epoch: 88 [16896/17352 (97%)] Loss: -1344255.750000\n",
      "    epoch          : 88\n",
      "    loss           : -1339347.1783088236\n",
      "    val_loss       : -1263594.969909668\n",
      "Train Epoch: 89 [0/17352 (0%)] Loss: -1338979.750000\n",
      "Train Epoch: 89 [1408/17352 (8%)] Loss: -1338731.500000\n",
      "Train Epoch: 89 [2816/17352 (16%)] Loss: -1352772.125000\n",
      "Train Epoch: 89 [4224/17352 (24%)] Loss: -1347971.500000\n",
      "Train Epoch: 89 [5632/17352 (32%)] Loss: -1346599.625000\n",
      "Train Epoch: 89 [7040/17352 (41%)] Loss: -1358676.250000\n",
      "Train Epoch: 89 [8448/17352 (49%)] Loss: -1348795.500000\n",
      "Train Epoch: 89 [9856/17352 (57%)] Loss: -1352301.875000\n",
      "Train Epoch: 89 [11264/17352 (65%)] Loss: -1336861.750000\n",
      "Train Epoch: 89 [12672/17352 (73%)] Loss: -1340175.000000\n",
      "Train Epoch: 89 [14080/17352 (81%)] Loss: -1341626.125000\n",
      "Train Epoch: 89 [15488/17352 (89%)] Loss: -1349518.250000\n",
      "Train Epoch: 89 [16896/17352 (97%)] Loss: -1350339.875000\n",
      "    epoch          : 89\n",
      "    loss           : -1339350.5114889706\n",
      "    val_loss       : -1263599.4673461914\n",
      "Train Epoch: 90 [0/17352 (0%)] Loss: -1345243.875000\n",
      "Train Epoch: 90 [1408/17352 (8%)] Loss: -1350691.250000\n",
      "Train Epoch: 90 [2816/17352 (16%)] Loss: -1341519.000000\n",
      "Train Epoch: 90 [4224/17352 (24%)] Loss: -1352578.125000\n",
      "Train Epoch: 90 [5632/17352 (32%)] Loss: -1346448.750000\n",
      "Train Epoch: 90 [7040/17352 (41%)] Loss: -1345576.500000\n",
      "Train Epoch: 90 [8448/17352 (49%)] Loss: -1335919.000000\n",
      "Train Epoch: 90 [9856/17352 (57%)] Loss: -1342743.500000\n",
      "Train Epoch: 90 [11264/17352 (65%)] Loss: -1345888.125000\n",
      "Train Epoch: 90 [12672/17352 (73%)] Loss: -1332274.875000\n",
      "Train Epoch: 90 [14080/17352 (81%)] Loss: -1357216.125000\n",
      "Train Epoch: 90 [15488/17352 (89%)] Loss: -1346300.375000\n",
      "Train Epoch: 90 [16896/17352 (97%)] Loss: -1337764.375000\n",
      "    epoch          : 90\n",
      "    loss           : -1339352.6608455882\n",
      "    val_loss       : -1263605.5039672852\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0427_093817/checkpoint-epoch90.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 91 [0/17352 (0%)] Loss: -1356463.875000\n",
      "Train Epoch: 91 [1408/17352 (8%)] Loss: -1340783.500000\n",
      "Train Epoch: 91 [2816/17352 (16%)] Loss: -1342462.125000\n",
      "Train Epoch: 91 [4224/17352 (24%)] Loss: -1346553.000000\n",
      "Train Epoch: 91 [5632/17352 (32%)] Loss: -1341612.750000\n",
      "Train Epoch: 91 [7040/17352 (41%)] Loss: -1350386.125000\n",
      "Train Epoch: 91 [8448/17352 (49%)] Loss: -1337229.875000\n",
      "Train Epoch: 91 [9856/17352 (57%)] Loss: -1347100.875000\n",
      "Train Epoch: 91 [11264/17352 (65%)] Loss: -1348786.000000\n",
      "Train Epoch: 91 [12672/17352 (73%)] Loss: -1348048.375000\n",
      "Train Epoch: 91 [14080/17352 (81%)] Loss: -1344439.875000\n",
      "Train Epoch: 91 [15488/17352 (89%)] Loss: -1340562.625000\n",
      "Train Epoch: 91 [16896/17352 (97%)] Loss: -1345299.625000\n",
      "    epoch          : 91\n",
      "    loss           : -1339355.8170955882\n",
      "    val_loss       : -1263589.6589660645\n",
      "Train Epoch: 92 [0/17352 (0%)] Loss: -1344133.250000\n",
      "Train Epoch: 92 [1408/17352 (8%)] Loss: -1339799.000000\n",
      "Train Epoch: 92 [2816/17352 (16%)] Loss: -1354205.500000\n",
      "Train Epoch: 92 [4224/17352 (24%)] Loss: -1343099.625000\n",
      "Train Epoch: 92 [5632/17352 (32%)] Loss: -1344999.750000\n",
      "Train Epoch: 92 [7040/17352 (41%)] Loss: -1354335.875000\n",
      "Train Epoch: 92 [8448/17352 (49%)] Loss: -1326925.750000\n",
      "Train Epoch: 92 [9856/17352 (57%)] Loss: -1330519.875000\n",
      "Train Epoch: 92 [11264/17352 (65%)] Loss: -1331481.750000\n",
      "Train Epoch: 92 [12672/17352 (73%)] Loss: -1335173.750000\n",
      "Train Epoch: 92 [14080/17352 (81%)] Loss: -1330257.000000\n",
      "Train Epoch: 92 [15488/17352 (89%)] Loss: -1345748.750000\n",
      "Train Epoch: 92 [16896/17352 (97%)] Loss: -1348304.250000\n",
      "    epoch          : 92\n",
      "    loss           : -1339353.2366727942\n",
      "    val_loss       : -1263589.345916748\n",
      "Train Epoch: 93 [0/17352 (0%)] Loss: -1339635.125000\n",
      "Train Epoch: 93 [1408/17352 (8%)] Loss: -1340186.750000\n",
      "Train Epoch: 93 [2816/17352 (16%)] Loss: -1336954.750000\n",
      "Train Epoch: 93 [4224/17352 (24%)] Loss: -1330058.750000\n",
      "Train Epoch: 93 [5632/17352 (32%)] Loss: -1354433.500000\n",
      "Train Epoch: 93 [7040/17352 (41%)] Loss: -1330194.875000\n",
      "Train Epoch: 93 [8448/17352 (49%)] Loss: -1345334.750000\n",
      "Train Epoch: 93 [9856/17352 (57%)] Loss: -1356354.250000\n",
      "Train Epoch: 93 [11264/17352 (65%)] Loss: -1331050.375000\n",
      "Train Epoch: 93 [12672/17352 (73%)] Loss: -1343901.500000\n",
      "Train Epoch: 93 [14080/17352 (81%)] Loss: -1334721.500000\n",
      "Train Epoch: 93 [15488/17352 (89%)] Loss: -1341349.625000\n",
      "Train Epoch: 93 [16896/17352 (97%)] Loss: -1333327.625000\n",
      "    epoch          : 93\n",
      "    loss           : -1339352.059742647\n",
      "    val_loss       : -1263603.255645752\n",
      "Train Epoch: 94 [0/17352 (0%)] Loss: -1337539.625000\n",
      "Train Epoch: 94 [1408/17352 (8%)] Loss: -1326983.250000\n",
      "Train Epoch: 94 [2816/17352 (16%)] Loss: -1349352.875000\n",
      "Train Epoch: 94 [4224/17352 (24%)] Loss: -1350553.625000\n",
      "Train Epoch: 94 [5632/17352 (32%)] Loss: -1340847.000000\n",
      "Train Epoch: 94 [7040/17352 (41%)] Loss: -1327865.000000\n",
      "Train Epoch: 94 [8448/17352 (49%)] Loss: -1356808.375000\n",
      "Train Epoch: 94 [9856/17352 (57%)] Loss: -1346722.375000\n",
      "Train Epoch: 94 [11264/17352 (65%)] Loss: -1355638.250000\n",
      "Train Epoch: 94 [12672/17352 (73%)] Loss: -1348756.750000\n",
      "Train Epoch: 94 [14080/17352 (81%)] Loss: -1332349.750000\n",
      "Train Epoch: 94 [15488/17352 (89%)] Loss: -1341121.375000\n",
      "Train Epoch: 94 [16896/17352 (97%)] Loss: -1343001.750000\n",
      "    epoch          : 94\n",
      "    loss           : -1339351.692555147\n",
      "    val_loss       : -1263596.574005127\n",
      "Train Epoch: 95 [0/17352 (0%)] Loss: -1345025.375000\n",
      "Train Epoch: 95 [1408/17352 (8%)] Loss: -1342987.750000\n",
      "Train Epoch: 95 [2816/17352 (16%)] Loss: -1349843.875000\n",
      "Train Epoch: 95 [4224/17352 (24%)] Loss: -1348203.875000\n",
      "Train Epoch: 95 [5632/17352 (32%)] Loss: -1341432.250000\n",
      "Train Epoch: 95 [7040/17352 (41%)] Loss: -1336561.875000\n",
      "Train Epoch: 95 [8448/17352 (49%)] Loss: -1343916.000000\n",
      "Train Epoch: 95 [9856/17352 (57%)] Loss: -1350174.250000\n",
      "Train Epoch: 95 [11264/17352 (65%)] Loss: -1351501.875000\n",
      "Train Epoch: 95 [12672/17352 (73%)] Loss: -1333265.750000\n",
      "Train Epoch: 95 [14080/17352 (81%)] Loss: -1345826.500000\n",
      "Train Epoch: 95 [15488/17352 (89%)] Loss: -1358664.500000\n",
      "Train Epoch: 95 [16896/17352 (97%)] Loss: -1333835.875000\n",
      "    epoch          : 95\n",
      "    loss           : -1339355.4315257352\n",
      "    val_loss       : -1263603.0038757324\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0427_093817/checkpoint-epoch95.pth ...\n",
      "Train Epoch: 96 [0/17352 (0%)] Loss: -1341704.000000\n",
      "Train Epoch: 96 [1408/17352 (8%)] Loss: -1353623.250000\n",
      "Train Epoch: 96 [2816/17352 (16%)] Loss: -1345109.500000\n",
      "Train Epoch: 96 [4224/17352 (24%)] Loss: -1334859.875000\n",
      "Train Epoch: 96 [5632/17352 (32%)] Loss: -1360916.750000\n",
      "Train Epoch: 96 [7040/17352 (41%)] Loss: -1335392.000000\n",
      "Train Epoch: 96 [8448/17352 (49%)] Loss: -1355268.125000\n",
      "Train Epoch: 96 [9856/17352 (57%)] Loss: -1346975.625000\n",
      "Train Epoch: 96 [11264/17352 (65%)] Loss: -1333959.125000\n",
      "Train Epoch: 96 [12672/17352 (73%)] Loss: -1345746.500000\n",
      "Train Epoch: 96 [14080/17352 (81%)] Loss: -1343636.250000\n",
      "Train Epoch: 96 [15488/17352 (89%)] Loss: -1347710.750000\n",
      "Train Epoch: 96 [16896/17352 (97%)] Loss: -1361694.125000\n",
      "    epoch          : 96\n",
      "    loss           : -1339356.114430147\n",
      "    val_loss       : -1263596.4436340332\n",
      "Train Epoch: 97 [0/17352 (0%)] Loss: -1346411.125000\n",
      "Train Epoch: 97 [1408/17352 (8%)] Loss: -1329071.625000\n",
      "Train Epoch: 97 [2816/17352 (16%)] Loss: -1354268.250000\n",
      "Train Epoch: 97 [4224/17352 (24%)] Loss: -1347414.000000\n",
      "Train Epoch: 97 [5632/17352 (32%)] Loss: -1328106.750000\n",
      "Train Epoch: 97 [7040/17352 (41%)] Loss: -1349486.500000\n",
      "Train Epoch: 97 [8448/17352 (49%)] Loss: -1346984.000000\n",
      "Train Epoch: 97 [9856/17352 (57%)] Loss: -1356344.250000\n",
      "Train Epoch: 97 [11264/17352 (65%)] Loss: -1330185.375000\n",
      "Train Epoch: 97 [12672/17352 (73%)] Loss: -1358676.250000\n",
      "Train Epoch: 97 [14080/17352 (81%)] Loss: -1329728.375000\n",
      "Train Epoch: 97 [15488/17352 (89%)] Loss: -1340017.000000\n",
      "Train Epoch: 97 [16896/17352 (97%)] Loss: -1351563.375000\n",
      "    epoch          : 97\n",
      "    loss           : -1339359.8924632352\n",
      "    val_loss       : -1263605.7503967285\n",
      "Train Epoch: 98 [0/17352 (0%)] Loss: -1334984.750000\n",
      "Train Epoch: 98 [1408/17352 (8%)] Loss: -1344683.125000\n",
      "Train Epoch: 98 [2816/17352 (16%)] Loss: -1327219.125000\n",
      "Train Epoch: 98 [4224/17352 (24%)] Loss: -1340537.875000\n",
      "Train Epoch: 98 [5632/17352 (32%)] Loss: -1345068.375000\n",
      "Train Epoch: 98 [7040/17352 (41%)] Loss: -1329082.500000\n",
      "Train Epoch: 98 [8448/17352 (49%)] Loss: -1351347.000000\n",
      "Train Epoch: 98 [9856/17352 (57%)] Loss: -1342284.875000\n",
      "Train Epoch: 98 [11264/17352 (65%)] Loss: -1354981.875000\n",
      "Train Epoch: 98 [12672/17352 (73%)] Loss: -1337024.500000\n",
      "Train Epoch: 98 [14080/17352 (81%)] Loss: -1352964.250000\n",
      "Train Epoch: 98 [15488/17352 (89%)] Loss: -1342005.875000\n",
      "Train Epoch: 98 [16896/17352 (97%)] Loss: -1332540.625000\n",
      "    epoch          : 98\n",
      "    loss           : -1339360.036305147\n",
      "    val_loss       : -1263603.754486084\n",
      "Train Epoch: 99 [0/17352 (0%)] Loss: -1344532.625000\n",
      "Train Epoch: 99 [1408/17352 (8%)] Loss: -1338597.500000\n",
      "Train Epoch: 99 [2816/17352 (16%)] Loss: -1338640.750000\n",
      "Train Epoch: 99 [4224/17352 (24%)] Loss: -1333173.375000\n",
      "Train Epoch: 99 [5632/17352 (32%)] Loss: -1335085.250000\n",
      "Train Epoch: 99 [7040/17352 (41%)] Loss: -1336872.000000\n",
      "Train Epoch: 99 [8448/17352 (49%)] Loss: -1341069.625000\n",
      "Train Epoch: 99 [9856/17352 (57%)] Loss: -1349104.875000\n",
      "Train Epoch: 99 [11264/17352 (65%)] Loss: -1340063.500000\n",
      "Train Epoch: 99 [12672/17352 (73%)] Loss: -1341821.250000\n",
      "Train Epoch: 99 [14080/17352 (81%)] Loss: -1343843.875000\n",
      "Train Epoch: 99 [15488/17352 (89%)] Loss: -1349583.500000\n",
      "Train Epoch: 99 [16896/17352 (97%)] Loss: -1347981.625000\n",
      "    epoch          : 99\n",
      "    loss           : -1339362.198069853\n",
      "    val_loss       : -1263597.7350158691\n",
      "Train Epoch: 100 [0/17352 (0%)] Loss: -1344788.125000\n",
      "Train Epoch: 100 [1408/17352 (8%)] Loss: -1351469.000000\n",
      "Train Epoch: 100 [2816/17352 (16%)] Loss: -1322825.750000\n",
      "Train Epoch: 100 [4224/17352 (24%)] Loss: -1341471.000000\n",
      "Train Epoch: 100 [5632/17352 (32%)] Loss: -1334489.625000\n",
      "Train Epoch: 100 [7040/17352 (41%)] Loss: -1344433.500000\n",
      "Train Epoch: 100 [8448/17352 (49%)] Loss: -1345444.375000\n",
      "Train Epoch: 100 [9856/17352 (57%)] Loss: -1338218.750000\n",
      "Train Epoch: 100 [11264/17352 (65%)] Loss: -1344818.875000\n",
      "Train Epoch: 100 [12672/17352 (73%)] Loss: -1350581.500000\n",
      "Train Epoch: 100 [14080/17352 (81%)] Loss: -1338814.500000\n",
      "Train Epoch: 100 [15488/17352 (89%)] Loss: -1354303.875000\n",
      "Train Epoch: 100 [16896/17352 (97%)] Loss: -1343577.875000\n",
      "    epoch          : 100\n",
      "    loss           : -1339359.0174632352\n",
      "    val_loss       : -1263595.6145019531\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0427_093817/checkpoint-epoch100.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
