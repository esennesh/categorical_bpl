{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='omniglot_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 50,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [128/17352 (1%)] Loss: 107746.742188\n",
      "Train Epoch: 1 [1536/17352 (9%)] Loss: 77377.976562\n",
      "Train Epoch: 1 [2944/17352 (17%)] Loss: 40501.031250\n",
      "Train Epoch: 1 [4352/17352 (25%)] Loss: 61428.734375\n",
      "Train Epoch: 1 [5760/17352 (33%)] Loss: 40674.468750\n",
      "Train Epoch: 1 [7168/17352 (41%)] Loss: 11058.653320\n",
      "Train Epoch: 1 [8576/17352 (49%)] Loss: 40153.246094\n",
      "Train Epoch: 1 [9984/17352 (58%)] Loss: 23622.531250\n",
      "Train Epoch: 1 [11392/17352 (66%)] Loss: 10076.161133\n",
      "Train Epoch: 1 [12800/17352 (74%)] Loss: 19837.025391\n",
      "Train Epoch: 1 [14208/17352 (82%)] Loss: 25601.292969\n",
      "Train Epoch: 1 [15542/17352 (90%)] Loss: 11194.761719\n",
      "Train Epoch: 1 [16368/17352 (94%)] Loss: 589.217773\n",
      "Train Epoch: 1 [17044/17352 (98%)] Loss: 9856.757812\n",
      "    epoch          : 1\n",
      "    loss           : 28158.2657995032\n",
      "    val_loss       : 7453.896712239583\n",
      "Train Epoch: 2 [128/17352 (1%)] Loss: 10473.790039\n",
      "Train Epoch: 2 [1536/17352 (9%)] Loss: 7286.900879\n",
      "Train Epoch: 2 [2944/17352 (17%)] Loss: 17024.960938\n",
      "Train Epoch: 2 [4352/17352 (25%)] Loss: 2177.988281\n",
      "Train Epoch: 2 [5760/17352 (33%)] Loss: -15233.539062\n",
      "Train Epoch: 2 [7168/17352 (41%)] Loss: -2288.898682\n",
      "Train Epoch: 2 [8576/17352 (49%)] Loss: 10589.458984\n",
      "Train Epoch: 2 [9984/17352 (58%)] Loss: 17906.785156\n",
      "Train Epoch: 2 [11392/17352 (66%)] Loss: 10072.407227\n",
      "Train Epoch: 2 [12800/17352 (74%)] Loss: -13353.548828\n",
      "Train Epoch: 2 [14208/17352 (82%)] Loss: -11049.647461\n",
      "Train Epoch: 2 [15548/17352 (90%)] Loss: -9982.570312\n",
      "Train Epoch: 2 [16195/17352 (93%)] Loss: -25035.916016\n",
      "Train Epoch: 2 [16967/17352 (98%)] Loss: -6258.711426\n",
      "    epoch          : 2\n",
      "    loss           : 7836.04138019741\n",
      "    val_loss       : 1460.1988444010417\n",
      "Train Epoch: 3 [128/17352 (1%)] Loss: 618.574707\n",
      "Train Epoch: 3 [1536/17352 (9%)] Loss: -8387.833008\n",
      "Train Epoch: 3 [2944/17352 (17%)] Loss: -12373.976562\n",
      "Train Epoch: 3 [4352/17352 (25%)] Loss: -7622.496582\n",
      "Train Epoch: 3 [5760/17352 (33%)] Loss: -1491.422119\n",
      "Train Epoch: 3 [7168/17352 (41%)] Loss: 6414.086426\n",
      "Train Epoch: 3 [8576/17352 (49%)] Loss: -7432.080078\n",
      "Train Epoch: 3 [9984/17352 (58%)] Loss: -23659.728516\n",
      "Train Epoch: 3 [11392/17352 (66%)] Loss: 15903.496094\n",
      "Train Epoch: 3 [12800/17352 (74%)] Loss: -32305.117188\n",
      "Train Epoch: 3 [14208/17352 (82%)] Loss: -31808.714844\n",
      "Train Epoch: 3 [15530/17352 (89%)] Loss: -16106.707031\n",
      "Train Epoch: 3 [16279/17352 (94%)] Loss: 7156.048828\n",
      "Train Epoch: 3 [17082/17352 (98%)] Loss: -3110.959229\n",
      "    epoch          : 3\n",
      "    loss           : -4073.462082011588\n",
      "    val_loss       : -3575.190218098958\n",
      "Train Epoch: 4 [128/17352 (1%)] Loss: -15874.348633\n",
      "Train Epoch: 4 [1536/17352 (9%)] Loss: 10041.211914\n",
      "Train Epoch: 4 [2944/17352 (17%)] Loss: -7611.183594\n",
      "Train Epoch: 4 [4352/17352 (25%)] Loss: -9953.062500\n",
      "Train Epoch: 4 [5760/17352 (33%)] Loss: 1914.243164\n",
      "Train Epoch: 4 [7168/17352 (41%)] Loss: 7812.693848\n",
      "Train Epoch: 4 [8576/17352 (49%)] Loss: -40076.617188\n",
      "Train Epoch: 4 [9984/17352 (58%)] Loss: -14233.503906\n",
      "Train Epoch: 4 [11392/17352 (66%)] Loss: -12140.320312\n",
      "Train Epoch: 4 [12800/17352 (74%)] Loss: -11554.931641\n",
      "Train Epoch: 4 [14208/17352 (82%)] Loss: -34192.910156\n",
      "Train Epoch: 4 [15552/17352 (90%)] Loss: -9740.482422\n",
      "Train Epoch: 4 [16293/17352 (94%)] Loss: 1506.116699\n",
      "Train Epoch: 4 [16927/17352 (98%)] Loss: -1390.178711\n",
      "    epoch          : 4\n",
      "    loss           : -8774.400881364041\n",
      "    val_loss       : -6227.7671875\n",
      "Train Epoch: 5 [128/17352 (1%)] Loss: 16268.438477\n",
      "Train Epoch: 5 [1536/17352 (9%)] Loss: -5241.879883\n",
      "Train Epoch: 5 [2944/17352 (17%)] Loss: -6531.027344\n",
      "Train Epoch: 5 [4352/17352 (25%)] Loss: -12425.457031\n",
      "Train Epoch: 5 [5760/17352 (33%)] Loss: -34773.042969\n",
      "Train Epoch: 5 [7168/17352 (41%)] Loss: -11243.735352\n",
      "Train Epoch: 5 [8576/17352 (49%)] Loss: 19447.671875\n",
      "Train Epoch: 5 [9984/17352 (58%)] Loss: -14699.755859\n",
      "Train Epoch: 5 [11392/17352 (66%)] Loss: 15965.410156\n",
      "Train Epoch: 5 [12800/17352 (74%)] Loss: -652.370605\n",
      "Train Epoch: 5 [14208/17352 (82%)] Loss: -21869.886719\n",
      "Train Epoch: 5 [15451/17352 (89%)] Loss: -2594.047852\n",
      "Train Epoch: 5 [16082/17352 (93%)] Loss: -8300.536133\n",
      "Train Epoch: 5 [16963/17352 (98%)] Loss: -22058.652344\n",
      "    epoch          : 5\n",
      "    loss           : -14627.34360703846\n",
      "    val_loss       : -7910.87919921875\n",
      "Train Epoch: 6 [128/17352 (1%)] Loss: -934.890137\n",
      "Train Epoch: 6 [1536/17352 (9%)] Loss: -27658.267578\n",
      "Train Epoch: 6 [2944/17352 (17%)] Loss: -21848.664062\n",
      "Train Epoch: 6 [4352/17352 (25%)] Loss: -18648.992188\n",
      "Train Epoch: 6 [5760/17352 (33%)] Loss: 21251.226562\n",
      "Train Epoch: 6 [7168/17352 (41%)] Loss: -9400.611328\n",
      "Train Epoch: 6 [8576/17352 (49%)] Loss: -17876.744141\n",
      "Train Epoch: 6 [9984/17352 (58%)] Loss: 6699.518066\n",
      "Train Epoch: 6 [11392/17352 (66%)] Loss: -7522.099609\n",
      "Train Epoch: 6 [12800/17352 (74%)] Loss: -22615.029297\n",
      "Train Epoch: 6 [14208/17352 (82%)] Loss: -23355.224609\n",
      "Train Epoch: 6 [15496/17352 (89%)] Loss: -10607.508789\n",
      "Train Epoch: 6 [16180/17352 (93%)] Loss: -7389.322266\n",
      "Train Epoch: 6 [16983/17352 (98%)] Loss: -31873.626953\n",
      "    epoch          : 6\n",
      "    loss           : -19595.83147701801\n",
      "    val_loss       : -11009.575309244792\n",
      "Train Epoch: 7 [128/17352 (1%)] Loss: 7298.906250\n",
      "Train Epoch: 7 [1536/17352 (9%)] Loss: -7826.885254\n",
      "Train Epoch: 7 [2944/17352 (17%)] Loss: -30631.062500\n",
      "Train Epoch: 7 [4352/17352 (25%)] Loss: -45146.859375\n",
      "Train Epoch: 7 [5760/17352 (33%)] Loss: -23958.378906\n",
      "Train Epoch: 7 [7168/17352 (41%)] Loss: -46033.117188\n",
      "Train Epoch: 7 [8576/17352 (49%)] Loss: -24940.546875\n",
      "Train Epoch: 7 [9984/17352 (58%)] Loss: -29418.433594\n",
      "Train Epoch: 7 [11392/17352 (66%)] Loss: -25534.468750\n",
      "Train Epoch: 7 [12800/17352 (74%)] Loss: -34678.839844\n",
      "Train Epoch: 7 [14208/17352 (82%)] Loss: -10277.200195\n",
      "Train Epoch: 7 [15528/17352 (89%)] Loss: -13099.535156\n",
      "Train Epoch: 7 [16358/17352 (94%)] Loss: -18544.894531\n",
      "Train Epoch: 7 [17058/17352 (98%)] Loss: -12525.412109\n",
      "    epoch          : 7\n",
      "    loss           : -22769.855917706587\n",
      "    val_loss       : -13453.6076171875\n",
      "Train Epoch: 8 [128/17352 (1%)] Loss: -32837.359375\n",
      "Train Epoch: 8 [1536/17352 (9%)] Loss: -38022.179688\n",
      "Train Epoch: 8 [2944/17352 (17%)] Loss: -12778.331055\n",
      "Train Epoch: 8 [4352/17352 (25%)] Loss: -23756.257812\n",
      "Train Epoch: 8 [5760/17352 (33%)] Loss: -59509.671875\n",
      "Train Epoch: 8 [7168/17352 (41%)] Loss: -53070.480469\n",
      "Train Epoch: 8 [8576/17352 (49%)] Loss: -26925.464844\n",
      "Train Epoch: 8 [9984/17352 (58%)] Loss: 13467.304688\n",
      "Train Epoch: 8 [11392/17352 (66%)] Loss: -23250.121094\n",
      "Train Epoch: 8 [12800/17352 (74%)] Loss: -25252.089844\n",
      "Train Epoch: 8 [14208/17352 (82%)] Loss: -17712.976562\n",
      "Train Epoch: 8 [15487/17352 (89%)] Loss: -22982.720703\n",
      "Train Epoch: 8 [16283/17352 (94%)] Loss: -10023.118164\n",
      "Train Epoch: 8 [17099/17352 (99%)] Loss: -17097.214844\n",
      "    epoch          : 8\n",
      "    loss           : -24023.413710216548\n",
      "    val_loss       : -13559.037955729167\n",
      "Train Epoch: 9 [128/17352 (1%)] Loss: -24353.113281\n",
      "Train Epoch: 9 [1536/17352 (9%)] Loss: -35796.148438\n",
      "Train Epoch: 9 [2944/17352 (17%)] Loss: -49639.457031\n",
      "Train Epoch: 9 [4352/17352 (25%)] Loss: -9207.607422\n",
      "Train Epoch: 9 [5760/17352 (33%)] Loss: -25051.298828\n",
      "Train Epoch: 9 [7168/17352 (41%)] Loss: -26334.632812\n",
      "Train Epoch: 9 [8576/17352 (49%)] Loss: -1524.987427\n",
      "Train Epoch: 9 [9984/17352 (58%)] Loss: -33426.988281\n",
      "Train Epoch: 9 [11392/17352 (66%)] Loss: -44598.031250\n",
      "Train Epoch: 9 [12800/17352 (74%)] Loss: -7514.090820\n",
      "Train Epoch: 9 [14208/17352 (82%)] Loss: -39515.996094\n",
      "Train Epoch: 9 [15550/17352 (90%)] Loss: -24070.910156\n",
      "Train Epoch: 9 [16235/17352 (94%)] Loss: -30360.298828\n",
      "Train Epoch: 9 [17045/17352 (98%)] Loss: -43685.812500\n",
      "    epoch          : 9\n",
      "    loss           : -25947.245864356126\n",
      "    val_loss       : -16761.915763346355\n",
      "Train Epoch: 10 [128/17352 (1%)] Loss: -22120.347656\n",
      "Train Epoch: 10 [1536/17352 (9%)] Loss: -56347.546875\n",
      "Train Epoch: 10 [2944/17352 (17%)] Loss: -54023.058594\n",
      "Train Epoch: 10 [4352/17352 (25%)] Loss: -54683.718750\n",
      "Train Epoch: 10 [5760/17352 (33%)] Loss: -32479.955078\n",
      "Train Epoch: 10 [7168/17352 (41%)] Loss: -6496.188965\n",
      "Train Epoch: 10 [8576/17352 (49%)] Loss: -17893.099609\n",
      "Train Epoch: 10 [9984/17352 (58%)] Loss: 4412.734863\n",
      "Train Epoch: 10 [11392/17352 (66%)] Loss: -39255.390625\n",
      "Train Epoch: 10 [12800/17352 (74%)] Loss: -48342.449219\n",
      "Train Epoch: 10 [14208/17352 (82%)] Loss: -41412.042969\n",
      "Train Epoch: 10 [15560/17352 (90%)] Loss: -48701.984375\n",
      "Train Epoch: 10 [16218/17352 (93%)] Loss: -22600.337891\n",
      "Train Epoch: 10 [17033/17352 (98%)] Loss: -15432.893555\n",
      "    epoch          : 10\n",
      "    loss           : -25942.343822044015\n",
      "    val_loss       : -11916.97763671875\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0918_135442/checkpoint-epoch10.pth ...\n",
      "Train Epoch: 11 [128/17352 (1%)] Loss: 3652.996338\n",
      "Train Epoch: 11 [1536/17352 (9%)] Loss: -5391.492188\n",
      "Train Epoch: 11 [2944/17352 (17%)] Loss: -30242.347656\n",
      "Train Epoch: 11 [4352/17352 (25%)] Loss: -32326.814453\n",
      "Train Epoch: 11 [5760/17352 (33%)] Loss: -12352.710938\n",
      "Train Epoch: 11 [7168/17352 (41%)] Loss: -33856.531250\n",
      "Train Epoch: 11 [8576/17352 (49%)] Loss: -20091.089844\n",
      "Train Epoch: 11 [9984/17352 (58%)] Loss: -45136.171875\n",
      "Train Epoch: 11 [11392/17352 (66%)] Loss: -13441.027344\n",
      "Train Epoch: 11 [12800/17352 (74%)] Loss: -20115.308594\n",
      "Train Epoch: 11 [14208/17352 (82%)] Loss: -13037.102539\n",
      "Train Epoch: 11 [15458/17352 (89%)] Loss: -19137.294922\n",
      "Train Epoch: 11 [16345/17352 (94%)] Loss: -41951.574219\n",
      "Train Epoch: 11 [17025/17352 (98%)] Loss: -15905.048828\n",
      "    epoch          : 11\n",
      "    loss           : -25382.537228168258\n",
      "    val_loss       : -15215.327262369792\n",
      "Train Epoch: 12 [128/17352 (1%)] Loss: -24674.431641\n",
      "Train Epoch: 12 [1536/17352 (9%)] Loss: -24535.535156\n",
      "Train Epoch: 12 [2944/17352 (17%)] Loss: -48221.730469\n",
      "Train Epoch: 12 [4352/17352 (25%)] Loss: -36397.851562\n",
      "Train Epoch: 12 [5760/17352 (33%)] Loss: -6136.118164\n",
      "Train Epoch: 12 [7168/17352 (41%)] Loss: -36473.273438\n",
      "Train Epoch: 12 [8576/17352 (49%)] Loss: -57761.488281\n",
      "Train Epoch: 12 [9984/17352 (58%)] Loss: -18310.810547\n",
      "Train Epoch: 12 [11392/17352 (66%)] Loss: -43202.804688\n",
      "Train Epoch: 12 [12800/17352 (74%)] Loss: -46607.390625\n",
      "Train Epoch: 12 [14208/17352 (82%)] Loss: -44359.968750\n",
      "Train Epoch: 12 [15518/17352 (89%)] Loss: -44926.953125\n",
      "Train Epoch: 12 [16191/17352 (93%)] Loss: -36153.679688\n",
      "Train Epoch: 12 [16989/17352 (98%)] Loss: -7535.656250\n",
      "    epoch          : 12\n",
      "    loss           : -26184.073216790322\n",
      "    val_loss       : -17327.208951822915\n",
      "Train Epoch: 13 [128/17352 (1%)] Loss: -29293.332031\n",
      "Train Epoch: 13 [1536/17352 (9%)] Loss: -31209.121094\n",
      "Train Epoch: 13 [2944/17352 (17%)] Loss: -17895.767578\n",
      "Train Epoch: 13 [4352/17352 (25%)] Loss: -29274.208984\n",
      "Train Epoch: 13 [5760/17352 (33%)] Loss: -29677.677734\n",
      "Train Epoch: 13 [7168/17352 (41%)] Loss: -14701.732422\n",
      "Train Epoch: 13 [8576/17352 (49%)] Loss: -1489.834229\n",
      "Train Epoch: 13 [9984/17352 (58%)] Loss: -23353.611328\n",
      "Train Epoch: 13 [11392/17352 (66%)] Loss: -54371.152344\n",
      "Train Epoch: 13 [12800/17352 (74%)] Loss: -22948.160156\n",
      "Train Epoch: 13 [14208/17352 (82%)] Loss: -23002.035156\n",
      "Train Epoch: 13 [15523/17352 (89%)] Loss: -35005.304688\n",
      "Train Epoch: 13 [16230/17352 (94%)] Loss: -461.632385\n",
      "Train Epoch: 13 [17005/17352 (98%)] Loss: -7990.029297\n",
      "    epoch          : 13\n",
      "    loss           : -30538.62202607225\n",
      "    val_loss       : -14743.165283203125\n",
      "Train Epoch: 14 [128/17352 (1%)] Loss: -37239.050781\n",
      "Train Epoch: 14 [1536/17352 (9%)] Loss: -46397.320312\n",
      "Train Epoch: 14 [2944/17352 (17%)] Loss: -25271.896484\n",
      "Train Epoch: 14 [4352/17352 (25%)] Loss: -54010.648438\n",
      "Train Epoch: 14 [5760/17352 (33%)] Loss: -31000.761719\n",
      "Train Epoch: 14 [7168/17352 (41%)] Loss: -37525.566406\n",
      "Train Epoch: 14 [8576/17352 (49%)] Loss: -30523.792969\n",
      "Train Epoch: 14 [9984/17352 (58%)] Loss: -28901.419922\n",
      "Train Epoch: 14 [11392/17352 (66%)] Loss: -40744.085938\n",
      "Train Epoch: 14 [12800/17352 (74%)] Loss: -14894.123047\n",
      "Train Epoch: 14 [14208/17352 (82%)] Loss: -19737.308594\n",
      "Train Epoch: 14 [15584/17352 (90%)] Loss: -38694.476562\n",
      "Train Epoch: 14 [16336/17352 (94%)] Loss: -16519.578125\n",
      "Train Epoch: 14 [17071/17352 (98%)] Loss: 17826.226562\n",
      "    epoch          : 14\n",
      "    loss           : -30030.91471873034\n",
      "    val_loss       : -18593.595068359376\n",
      "Train Epoch: 15 [128/17352 (1%)] Loss: -25995.042969\n",
      "Train Epoch: 15 [1536/17352 (9%)] Loss: -48188.265625\n",
      "Train Epoch: 15 [2944/17352 (17%)] Loss: -58231.710938\n",
      "Train Epoch: 15 [4352/17352 (25%)] Loss: -30079.802734\n",
      "Train Epoch: 15 [5760/17352 (33%)] Loss: -46211.714844\n",
      "Train Epoch: 15 [7168/17352 (41%)] Loss: -8797.000000\n",
      "Train Epoch: 15 [8576/17352 (49%)] Loss: -4883.230469\n",
      "Train Epoch: 15 [9984/17352 (58%)] Loss: -40102.117188\n",
      "Train Epoch: 15 [11392/17352 (66%)] Loss: -49238.984375\n",
      "Train Epoch: 15 [12800/17352 (74%)] Loss: -52946.058594\n",
      "Train Epoch: 15 [14208/17352 (82%)] Loss: -49403.863281\n",
      "Train Epoch: 15 [15515/17352 (89%)] Loss: -26236.765625\n",
      "Train Epoch: 15 [16242/17352 (94%)] Loss: -21672.888672\n",
      "Train Epoch: 15 [17005/17352 (98%)] Loss: -15863.830078\n",
      "    epoch          : 15\n",
      "    loss           : -31950.32746221555\n",
      "    val_loss       : -15977.520751953125\n",
      "Train Epoch: 16 [128/17352 (1%)] Loss: -36470.281250\n",
      "Train Epoch: 16 [1536/17352 (9%)] Loss: -30677.644531\n",
      "Train Epoch: 16 [2944/17352 (17%)] Loss: -33249.058594\n",
      "Train Epoch: 16 [4352/17352 (25%)] Loss: -29043.855469\n",
      "Train Epoch: 16 [5760/17352 (33%)] Loss: -57710.503906\n",
      "Train Epoch: 16 [7168/17352 (41%)] Loss: -28646.031250\n",
      "Train Epoch: 16 [8576/17352 (49%)] Loss: -25660.250000\n",
      "Train Epoch: 16 [9984/17352 (58%)] Loss: -27135.867188\n",
      "Train Epoch: 16 [11392/17352 (66%)] Loss: -38322.378906\n",
      "Train Epoch: 16 [12800/17352 (74%)] Loss: -48401.414062\n",
      "Train Epoch: 16 [14208/17352 (82%)] Loss: -39139.941406\n",
      "Train Epoch: 16 [15452/17352 (89%)] Loss: -11498.373047\n",
      "Train Epoch: 16 [16166/17352 (93%)] Loss: -3120.825439\n",
      "Train Epoch: 16 [16900/17352 (97%)] Loss: -2395.433350\n",
      "    epoch          : 16\n",
      "    loss           : -33583.98674349177\n",
      "    val_loss       : -16743.712809244793\n",
      "Train Epoch: 17 [128/17352 (1%)] Loss: -16798.365234\n",
      "Train Epoch: 17 [1536/17352 (9%)] Loss: -37944.804688\n",
      "Train Epoch: 17 [2944/17352 (17%)] Loss: -53760.933594\n",
      "Train Epoch: 17 [4352/17352 (25%)] Loss: -36404.867188\n",
      "Train Epoch: 17 [5760/17352 (33%)] Loss: -57057.984375\n",
      "Train Epoch: 17 [7168/17352 (41%)] Loss: -29663.582031\n",
      "Train Epoch: 17 [8576/17352 (49%)] Loss: -41517.441406\n",
      "Train Epoch: 17 [9984/17352 (58%)] Loss: -33492.785156\n",
      "Train Epoch: 17 [11392/17352 (66%)] Loss: -36131.257812\n",
      "Train Epoch: 17 [12800/17352 (74%)] Loss: -38055.054688\n",
      "Train Epoch: 17 [14208/17352 (82%)] Loss: -63101.957031\n",
      "Train Epoch: 17 [15478/17352 (89%)] Loss: -30758.945312\n",
      "Train Epoch: 17 [16098/17352 (93%)] Loss: -49687.464844\n",
      "Train Epoch: 17 [17064/17352 (98%)] Loss: -33225.425781\n",
      "    epoch          : 17\n",
      "    loss           : -33555.26966888633\n",
      "    val_loss       : -18739.72628580729\n",
      "Train Epoch: 18 [128/17352 (1%)] Loss: -30576.642578\n",
      "Train Epoch: 18 [1536/17352 (9%)] Loss: -26898.636719\n",
      "Train Epoch: 18 [2944/17352 (17%)] Loss: -63132.242188\n",
      "Train Epoch: 18 [4352/17352 (25%)] Loss: -32568.841797\n",
      "Train Epoch: 18 [5760/17352 (33%)] Loss: -42899.472656\n",
      "Train Epoch: 18 [7168/17352 (41%)] Loss: -33692.675781\n",
      "Train Epoch: 18 [8576/17352 (49%)] Loss: -52043.656250\n",
      "Train Epoch: 18 [9984/17352 (58%)] Loss: -32262.386719\n",
      "Train Epoch: 18 [11392/17352 (66%)] Loss: -33019.070312\n",
      "Train Epoch: 18 [12800/17352 (74%)] Loss: -34182.308594\n",
      "Train Epoch: 18 [14208/17352 (82%)] Loss: -32751.535156\n",
      "Train Epoch: 18 [15555/17352 (90%)] Loss: -23342.761719\n",
      "Train Epoch: 18 [16212/17352 (93%)] Loss: -31302.837891\n",
      "Train Epoch: 18 [17015/17352 (98%)] Loss: -18467.902344\n",
      "    epoch          : 18\n",
      "    loss           : -35528.57687111669\n",
      "    val_loss       : -17701.78350423177\n",
      "Train Epoch: 19 [128/17352 (1%)] Loss: -41045.175781\n",
      "Train Epoch: 19 [1536/17352 (9%)] Loss: -35744.527344\n",
      "Train Epoch: 19 [2944/17352 (17%)] Loss: -27674.984375\n",
      "Train Epoch: 19 [4352/17352 (25%)] Loss: -14270.432617\n",
      "Train Epoch: 19 [5760/17352 (33%)] Loss: -31598.603516\n",
      "Train Epoch: 19 [7168/17352 (41%)] Loss: -29507.474609\n",
      "Train Epoch: 19 [8576/17352 (49%)] Loss: -44022.464844\n",
      "Train Epoch: 19 [9984/17352 (58%)] Loss: -41595.160156\n",
      "Train Epoch: 19 [11392/17352 (66%)] Loss: -61610.445312\n",
      "Train Epoch: 19 [12800/17352 (74%)] Loss: -45586.621094\n",
      "Train Epoch: 19 [14208/17352 (82%)] Loss: -60105.292969\n",
      "Train Epoch: 19 [15427/17352 (89%)] Loss: -6299.937988\n",
      "Train Epoch: 19 [16194/17352 (93%)] Loss: -22259.904297\n",
      "Train Epoch: 19 [16985/17352 (98%)] Loss: -29112.650391\n",
      "    epoch          : 19\n",
      "    loss           : -36440.10346892696\n",
      "    val_loss       : -22857.327701822916\n",
      "Train Epoch: 20 [128/17352 (1%)] Loss: -44971.906250\n",
      "Train Epoch: 20 [1536/17352 (9%)] Loss: -43802.546875\n",
      "Train Epoch: 20 [2944/17352 (17%)] Loss: -56998.179688\n",
      "Train Epoch: 20 [4352/17352 (25%)] Loss: -24407.339844\n",
      "Train Epoch: 20 [5760/17352 (33%)] Loss: -75224.984375\n",
      "Train Epoch: 20 [7168/17352 (41%)] Loss: -39133.601562\n",
      "Train Epoch: 20 [8576/17352 (49%)] Loss: -42244.328125\n",
      "Train Epoch: 20 [9984/17352 (58%)] Loss: -50027.156250\n",
      "Train Epoch: 20 [11392/17352 (66%)] Loss: -49190.121094\n",
      "Train Epoch: 20 [12800/17352 (74%)] Loss: -23643.906250\n",
      "Train Epoch: 20 [14208/17352 (82%)] Loss: -47784.898438\n",
      "Train Epoch: 20 [15487/17352 (89%)] Loss: -16285.803711\n",
      "Train Epoch: 20 [16287/17352 (94%)] Loss: -41955.718750\n",
      "Train Epoch: 20 [17063/17352 (98%)] Loss: -30391.636719\n",
      "    epoch          : 20\n",
      "    loss           : -39934.00499259386\n",
      "    val_loss       : -21946.972916666666\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0918_135442/checkpoint-epoch20.pth ...\n",
      "Train Epoch: 21 [128/17352 (1%)] Loss: -41912.468750\n",
      "Train Epoch: 21 [1536/17352 (9%)] Loss: -50878.335938\n",
      "Train Epoch: 21 [2944/17352 (17%)] Loss: -30429.937500\n",
      "Train Epoch: 21 [4352/17352 (25%)] Loss: -48281.679688\n",
      "Train Epoch: 21 [5760/17352 (33%)] Loss: -51291.476562\n",
      "Train Epoch: 21 [7168/17352 (41%)] Loss: -64035.550781\n",
      "Train Epoch: 21 [8576/17352 (49%)] Loss: -44580.914062\n",
      "Train Epoch: 21 [9984/17352 (58%)] Loss: -20239.082031\n",
      "Train Epoch: 21 [11392/17352 (66%)] Loss: -41192.542969\n",
      "Train Epoch: 21 [12800/17352 (74%)] Loss: -16527.285156\n",
      "Train Epoch: 21 [14208/17352 (82%)] Loss: -66943.882812\n",
      "Train Epoch: 21 [15534/17352 (90%)] Loss: -27926.476562\n",
      "Train Epoch: 21 [16324/17352 (94%)] Loss: -22033.589844\n",
      "Train Epoch: 21 [17042/17352 (98%)] Loss: -12788.273438\n",
      "    epoch          : 21\n",
      "    loss           : -39460.07134436281\n",
      "    val_loss       : -20192.649251302082\n",
      "Train Epoch: 22 [128/17352 (1%)] Loss: -40586.433594\n",
      "Train Epoch: 22 [1536/17352 (9%)] Loss: -32381.917969\n",
      "Train Epoch: 22 [2944/17352 (17%)] Loss: -30643.746094\n",
      "Train Epoch: 22 [4352/17352 (25%)] Loss: -30921.964844\n",
      "Train Epoch: 22 [5760/17352 (33%)] Loss: -24143.490234\n",
      "Train Epoch: 22 [7168/17352 (41%)] Loss: -41123.300781\n",
      "Train Epoch: 22 [8576/17352 (49%)] Loss: -44433.214844\n",
      "Train Epoch: 22 [9984/17352 (58%)] Loss: -65958.554688\n",
      "Train Epoch: 22 [11392/17352 (66%)] Loss: -44202.550781\n",
      "Train Epoch: 22 [12800/17352 (74%)] Loss: -37666.390625\n",
      "Train Epoch: 22 [14208/17352 (82%)] Loss: -28790.818359\n",
      "Train Epoch: 22 [15406/17352 (89%)] Loss: -1928.225586\n",
      "Train Epoch: 22 [16247/17352 (94%)] Loss: -18009.517578\n",
      "Train Epoch: 22 [17010/17352 (98%)] Loss: -48415.085938\n",
      "    epoch          : 22\n",
      "    loss           : -38099.95684445785\n",
      "    val_loss       : -20975.496158854166\n",
      "Train Epoch: 23 [128/17352 (1%)] Loss: -65113.132812\n",
      "Train Epoch: 23 [1536/17352 (9%)] Loss: -41130.289062\n",
      "Train Epoch: 23 [2944/17352 (17%)] Loss: -25641.181641\n",
      "Train Epoch: 23 [4352/17352 (25%)] Loss: -63854.851562\n",
      "Train Epoch: 23 [5760/17352 (33%)] Loss: -25186.132812\n",
      "Train Epoch: 23 [7168/17352 (41%)] Loss: -63690.839844\n",
      "Train Epoch: 23 [8576/17352 (49%)] Loss: -18999.417969\n",
      "Train Epoch: 23 [9984/17352 (58%)] Loss: -67239.523438\n",
      "Train Epoch: 23 [11392/17352 (66%)] Loss: -27429.992188\n",
      "Train Epoch: 23 [12800/17352 (74%)] Loss: -55419.421875\n",
      "Train Epoch: 23 [14208/17352 (82%)] Loss: -66694.929688\n",
      "Train Epoch: 23 [15404/17352 (89%)] Loss: -15035.615234\n",
      "Train Epoch: 23 [16160/17352 (93%)] Loss: -48382.164062\n",
      "Train Epoch: 23 [17024/17352 (98%)] Loss: -41486.484375\n",
      "    epoch          : 23\n",
      "    loss           : -40705.10792080668\n",
      "    val_loss       : -23498.754622395834\n",
      "Train Epoch: 24 [128/17352 (1%)] Loss: -57317.023438\n",
      "Train Epoch: 24 [1536/17352 (9%)] Loss: -30759.146484\n",
      "Train Epoch: 24 [2944/17352 (17%)] Loss: -49434.453125\n",
      "Train Epoch: 24 [4352/17352 (25%)] Loss: -56957.414062\n",
      "Train Epoch: 24 [5760/17352 (33%)] Loss: -44003.968750\n",
      "Train Epoch: 24 [7168/17352 (41%)] Loss: -44062.257812\n",
      "Train Epoch: 24 [8576/17352 (49%)] Loss: -27569.333984\n",
      "Train Epoch: 24 [9984/17352 (58%)] Loss: -50902.812500\n",
      "Train Epoch: 24 [11392/17352 (66%)] Loss: -43110.953125\n",
      "Train Epoch: 24 [12800/17352 (74%)] Loss: -43543.585938\n",
      "Train Epoch: 24 [14208/17352 (82%)] Loss: -61086.785156\n",
      "Train Epoch: 24 [15523/17352 (89%)] Loss: -30754.132812\n",
      "Train Epoch: 24 [16355/17352 (94%)] Loss: -18214.296875\n",
      "Train Epoch: 24 [16996/17352 (98%)] Loss: -3094.139160\n",
      "    epoch          : 24\n",
      "    loss           : -41290.75232179373\n",
      "    val_loss       : -24444.562255859375\n",
      "Train Epoch: 25 [128/17352 (1%)] Loss: -27834.953125\n",
      "Train Epoch: 25 [1536/17352 (9%)] Loss: -67425.734375\n",
      "Train Epoch: 25 [2944/17352 (17%)] Loss: -53840.769531\n",
      "Train Epoch: 25 [4352/17352 (25%)] Loss: -52528.320312\n",
      "Train Epoch: 25 [5760/17352 (33%)] Loss: -62133.605469\n",
      "Train Epoch: 25 [7168/17352 (41%)] Loss: -38890.453125\n",
      "Train Epoch: 25 [8576/17352 (49%)] Loss: -50191.199219\n",
      "Train Epoch: 25 [9984/17352 (58%)] Loss: -38918.562500\n",
      "Train Epoch: 25 [11392/17352 (66%)] Loss: -63263.937500\n",
      "Train Epoch: 25 [12800/17352 (74%)] Loss: -36367.812500\n",
      "Train Epoch: 25 [14208/17352 (82%)] Loss: -72221.132812\n",
      "Train Epoch: 25 [15428/17352 (89%)] Loss: -15771.556641\n",
      "Train Epoch: 25 [16058/17352 (93%)] Loss: -37732.257812\n",
      "Train Epoch: 25 [16942/17352 (98%)] Loss: -47680.996094\n",
      "    epoch          : 25\n",
      "    loss           : -44862.40660228345\n",
      "    val_loss       : -24720.45408528646\n",
      "Train Epoch: 26 [128/17352 (1%)] Loss: -41750.714844\n",
      "Train Epoch: 26 [1536/17352 (9%)] Loss: -45845.925781\n",
      "Train Epoch: 26 [2944/17352 (17%)] Loss: -57989.742188\n",
      "Train Epoch: 26 [4352/17352 (25%)] Loss: -60208.800781\n",
      "Train Epoch: 26 [5760/17352 (33%)] Loss: -43682.480469\n",
      "Train Epoch: 26 [7168/17352 (41%)] Loss: -56625.859375\n",
      "Train Epoch: 26 [8576/17352 (49%)] Loss: -48323.281250\n",
      "Train Epoch: 26 [9984/17352 (58%)] Loss: -56053.312500\n",
      "Train Epoch: 26 [11392/17352 (66%)] Loss: -67126.609375\n",
      "Train Epoch: 26 [12800/17352 (74%)] Loss: -46048.355469\n",
      "Train Epoch: 26 [14208/17352 (82%)] Loss: -64153.535156\n",
      "Train Epoch: 26 [15554/17352 (90%)] Loss: -39744.441406\n",
      "Train Epoch: 26 [16174/17352 (93%)] Loss: -42912.039062\n",
      "Train Epoch: 26 [17021/17352 (98%)] Loss: -42795.359375\n",
      "    epoch          : 26\n",
      "    loss           : -44333.890393967595\n",
      "    val_loss       : -26050.997591145835\n",
      "Train Epoch: 27 [128/17352 (1%)] Loss: -36902.062500\n",
      "Train Epoch: 27 [1536/17352 (9%)] Loss: -91084.781250\n",
      "Train Epoch: 27 [2944/17352 (17%)] Loss: -58971.300781\n",
      "Train Epoch: 27 [4352/17352 (25%)] Loss: -47514.867188\n",
      "Train Epoch: 27 [5760/17352 (33%)] Loss: -40481.117188\n",
      "Train Epoch: 27 [7168/17352 (41%)] Loss: -16690.636719\n",
      "Train Epoch: 27 [8576/17352 (49%)] Loss: -50109.113281\n",
      "Train Epoch: 27 [9984/17352 (58%)] Loss: -58121.656250\n",
      "Train Epoch: 27 [11392/17352 (66%)] Loss: -46297.777344\n",
      "Train Epoch: 27 [12800/17352 (74%)] Loss: -26678.482422\n",
      "Train Epoch: 27 [14208/17352 (82%)] Loss: -37729.312500\n",
      "Train Epoch: 27 [15471/17352 (89%)] Loss: -12231.072266\n",
      "Train Epoch: 27 [16292/17352 (94%)] Loss: -53458.187500\n",
      "Train Epoch: 27 [17102/17352 (99%)] Loss: -32827.687500\n",
      "    epoch          : 27\n",
      "    loss           : -44901.67158088428\n",
      "    val_loss       : -23951.9833984375\n",
      "Train Epoch: 28 [128/17352 (1%)] Loss: -30806.398438\n",
      "Train Epoch: 28 [1536/17352 (9%)] Loss: -58164.960938\n",
      "Train Epoch: 28 [2944/17352 (17%)] Loss: -89412.234375\n",
      "Train Epoch: 28 [4352/17352 (25%)] Loss: -45829.828125\n",
      "Train Epoch: 28 [5760/17352 (33%)] Loss: -54499.656250\n",
      "Train Epoch: 28 [7168/17352 (41%)] Loss: -47910.125000\n",
      "Train Epoch: 28 [8576/17352 (49%)] Loss: -40313.355469\n",
      "Train Epoch: 28 [9984/17352 (58%)] Loss: -63843.367188\n",
      "Train Epoch: 28 [11392/17352 (66%)] Loss: -67409.765625\n",
      "Train Epoch: 28 [12800/17352 (74%)] Loss: -65557.023438\n",
      "Train Epoch: 28 [14208/17352 (82%)] Loss: -57830.398438\n",
      "Train Epoch: 28 [15472/17352 (89%)] Loss: -15926.144531\n",
      "Train Epoch: 28 [16168/17352 (93%)] Loss: -30395.982422\n",
      "Train Epoch: 28 [17022/17352 (98%)] Loss: 4441.226562\n",
      "    epoch          : 28\n",
      "    loss           : -44324.1520037555\n",
      "    val_loss       : -25197.912947591147\n",
      "Train Epoch: 29 [128/17352 (1%)] Loss: -40885.363281\n",
      "Train Epoch: 29 [1536/17352 (9%)] Loss: -52843.910156\n",
      "Train Epoch: 29 [2944/17352 (17%)] Loss: -47288.792969\n",
      "Train Epoch: 29 [4352/17352 (25%)] Loss: -61785.363281\n",
      "Train Epoch: 29 [5760/17352 (33%)] Loss: -63557.375000\n",
      "Train Epoch: 29 [7168/17352 (41%)] Loss: -58757.226562\n",
      "Train Epoch: 29 [8576/17352 (49%)] Loss: -40641.167969\n",
      "Train Epoch: 29 [9984/17352 (58%)] Loss: -59829.824219\n",
      "Train Epoch: 29 [11392/17352 (66%)] Loss: -21088.826172\n",
      "Train Epoch: 29 [12800/17352 (74%)] Loss: -36968.500000\n",
      "Train Epoch: 29 [14208/17352 (82%)] Loss: -42339.917969\n",
      "Train Epoch: 29 [15511/17352 (89%)] Loss: -3806.956299\n",
      "Train Epoch: 29 [16106/17352 (93%)] Loss: -23050.773438\n",
      "Train Epoch: 29 [16963/17352 (98%)] Loss: -28115.283203\n",
      "    epoch          : 29\n",
      "    loss           : -44284.05080910497\n",
      "    val_loss       : -22274.233772786458\n",
      "Train Epoch: 30 [128/17352 (1%)] Loss: -24581.394531\n",
      "Train Epoch: 30 [1536/17352 (9%)] Loss: -55278.851562\n",
      "Train Epoch: 30 [2944/17352 (17%)] Loss: -38611.875000\n",
      "Train Epoch: 30 [4352/17352 (25%)] Loss: -49047.519531\n",
      "Train Epoch: 30 [5760/17352 (33%)] Loss: -42665.160156\n",
      "Train Epoch: 30 [7168/17352 (41%)] Loss: -43984.167969\n",
      "Train Epoch: 30 [8576/17352 (49%)] Loss: -47146.519531\n",
      "Train Epoch: 30 [9984/17352 (58%)] Loss: -53578.531250\n",
      "Train Epoch: 30 [11392/17352 (66%)] Loss: -66604.078125\n",
      "Train Epoch: 30 [12800/17352 (74%)] Loss: -52171.023438\n",
      "Train Epoch: 30 [14208/17352 (82%)] Loss: -55754.828125\n",
      "Train Epoch: 30 [15487/17352 (89%)] Loss: -28170.728516\n",
      "Train Epoch: 30 [16129/17352 (93%)] Loss: -4620.324707\n",
      "Train Epoch: 30 [17058/17352 (98%)] Loss: -42677.710938\n",
      "    epoch          : 30\n",
      "    loss           : -39992.16492436556\n",
      "    val_loss       : -27773.503531901042\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0918_135442/checkpoint-epoch30.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 31 [128/17352 (1%)] Loss: -51578.199219\n",
      "Train Epoch: 31 [1536/17352 (9%)] Loss: -31790.898438\n",
      "Train Epoch: 31 [2944/17352 (17%)] Loss: -47713.335938\n",
      "Train Epoch: 31 [4352/17352 (25%)] Loss: -52540.402344\n",
      "Train Epoch: 31 [5760/17352 (33%)] Loss: -80029.562500\n",
      "Train Epoch: 31 [7168/17352 (41%)] Loss: -52274.343750\n",
      "Train Epoch: 31 [8576/17352 (49%)] Loss: -36710.820312\n",
      "Train Epoch: 31 [9984/17352 (58%)] Loss: -49806.316406\n",
      "Train Epoch: 31 [11392/17352 (66%)] Loss: -22329.916016\n",
      "Train Epoch: 31 [12800/17352 (74%)] Loss: -34445.792969\n",
      "Train Epoch: 31 [14208/17352 (82%)] Loss: -41303.054688\n",
      "Train Epoch: 31 [15541/17352 (90%)] Loss: -5486.316895\n",
      "Train Epoch: 31 [16224/17352 (93%)] Loss: -25253.261719\n",
      "Train Epoch: 31 [16967/17352 (98%)] Loss: -52068.910156\n",
      "    epoch          : 31\n",
      "    loss           : -43568.04286798055\n",
      "    val_loss       : -26172.320735677084\n",
      "Train Epoch: 32 [128/17352 (1%)] Loss: -74979.125000\n",
      "Train Epoch: 32 [1536/17352 (9%)] Loss: -59410.761719\n",
      "Train Epoch: 32 [2944/17352 (17%)] Loss: -16742.347656\n",
      "Train Epoch: 32 [4352/17352 (25%)] Loss: -59604.386719\n",
      "Train Epoch: 32 [5760/17352 (33%)] Loss: -29470.437500\n",
      "Train Epoch: 32 [7168/17352 (41%)] Loss: -35616.910156\n",
      "Train Epoch: 32 [8576/17352 (49%)] Loss: -50192.621094\n",
      "Train Epoch: 32 [9984/17352 (58%)] Loss: -62362.843750\n",
      "Train Epoch: 32 [11392/17352 (66%)] Loss: -42041.363281\n",
      "Train Epoch: 32 [12800/17352 (74%)] Loss: -36934.699219\n",
      "Train Epoch: 32 [14208/17352 (82%)] Loss: -40420.468750\n",
      "Train Epoch: 32 [15542/17352 (90%)] Loss: -16445.964844\n",
      "Train Epoch: 32 [16367/17352 (94%)] Loss: -37378.515625\n",
      "Train Epoch: 32 [16996/17352 (98%)] Loss: -42033.539062\n",
      "    epoch          : 32\n",
      "    loss           : -43698.00577908714\n",
      "    val_loss       : -23562.280126953126\n",
      "Train Epoch: 33 [128/17352 (1%)] Loss: -46984.925781\n",
      "Train Epoch: 33 [1536/17352 (9%)] Loss: -63076.101562\n",
      "Train Epoch: 33 [2944/17352 (17%)] Loss: -29385.160156\n",
      "Train Epoch: 33 [4352/17352 (25%)] Loss: -18603.261719\n",
      "Train Epoch: 33 [5760/17352 (33%)] Loss: -62189.242188\n",
      "Train Epoch: 33 [7168/17352 (41%)] Loss: -42898.343750\n",
      "Train Epoch: 33 [8576/17352 (49%)] Loss: -41316.113281\n",
      "Train Epoch: 33 [9984/17352 (58%)] Loss: -29233.742188\n",
      "Train Epoch: 33 [11392/17352 (66%)] Loss: -53481.945312\n",
      "Train Epoch: 33 [12800/17352 (74%)] Loss: -56156.375000\n",
      "Train Epoch: 33 [14208/17352 (82%)] Loss: -47261.078125\n",
      "Train Epoch: 33 [15511/17352 (89%)] Loss: -30511.500000\n",
      "Train Epoch: 33 [16129/17352 (93%)] Loss: -30918.974609\n",
      "Train Epoch: 33 [16943/17352 (98%)] Loss: -21152.419922\n",
      "    epoch          : 33\n",
      "    loss           : -43134.096478148596\n",
      "    val_loss       : -22860.08465983073\n",
      "Train Epoch: 34 [128/17352 (1%)] Loss: -34521.468750\n",
      "Train Epoch: 34 [1536/17352 (9%)] Loss: -70951.812500\n",
      "Train Epoch: 34 [2944/17352 (17%)] Loss: -43456.375000\n",
      "Train Epoch: 34 [4352/17352 (25%)] Loss: -57101.734375\n",
      "Train Epoch: 34 [5760/17352 (33%)] Loss: -42520.535156\n",
      "Train Epoch: 34 [7168/17352 (41%)] Loss: -41357.941406\n",
      "Train Epoch: 34 [8576/17352 (49%)] Loss: -40497.410156\n",
      "Train Epoch: 34 [9984/17352 (58%)] Loss: -40538.148438\n",
      "Train Epoch: 34 [11392/17352 (66%)] Loss: -54169.093750\n",
      "Train Epoch: 34 [12800/17352 (74%)] Loss: -53059.921875\n",
      "Train Epoch: 34 [14208/17352 (82%)] Loss: -48361.949219\n",
      "Train Epoch: 34 [15482/17352 (89%)] Loss: -20414.246094\n",
      "Train Epoch: 34 [16318/17352 (94%)] Loss: -61397.359375\n",
      "Train Epoch: 34 [17009/17352 (98%)] Loss: -58322.765625\n",
      "    epoch          : 34\n",
      "    loss           : -42825.5089828184\n",
      "    val_loss       : -23560.300130208332\n",
      "Train Epoch: 35 [128/17352 (1%)] Loss: -55824.453125\n",
      "Train Epoch: 35 [1536/17352 (9%)] Loss: -36444.125000\n",
      "Train Epoch: 35 [2944/17352 (17%)] Loss: -51568.878906\n",
      "Train Epoch: 35 [4352/17352 (25%)] Loss: -60792.367188\n",
      "Train Epoch: 35 [5760/17352 (33%)] Loss: -40497.230469\n",
      "Train Epoch: 35 [7168/17352 (41%)] Loss: -27931.150391\n",
      "Train Epoch: 35 [8576/17352 (49%)] Loss: -18809.777344\n",
      "Train Epoch: 35 [9984/17352 (58%)] Loss: -42306.566406\n",
      "Train Epoch: 35 [11392/17352 (66%)] Loss: -38864.191406\n",
      "Train Epoch: 35 [12800/17352 (74%)] Loss: -51789.671875\n",
      "Train Epoch: 35 [14208/17352 (82%)] Loss: -65920.429688\n",
      "Train Epoch: 35 [15458/17352 (89%)] Loss: -26643.964844\n",
      "Train Epoch: 35 [16160/17352 (93%)] Loss: -35196.925781\n",
      "Train Epoch: 35 [17024/17352 (98%)] Loss: -46003.136719\n",
      "    epoch          : 35\n",
      "    loss           : -42774.89681535759\n",
      "    val_loss       : -22397.188924153645\n",
      "Train Epoch: 36 [128/17352 (1%)] Loss: -74750.328125\n",
      "Train Epoch: 36 [1536/17352 (9%)] Loss: -45619.796875\n",
      "Train Epoch: 36 [2944/17352 (17%)] Loss: -54344.187500\n",
      "Train Epoch: 36 [4352/17352 (25%)] Loss: -35647.613281\n",
      "Train Epoch: 36 [5760/17352 (33%)] Loss: -25635.150391\n",
      "Train Epoch: 36 [7168/17352 (41%)] Loss: -53365.125000\n",
      "Train Epoch: 36 [8576/17352 (49%)] Loss: -51342.507812\n",
      "Train Epoch: 36 [9984/17352 (58%)] Loss: -70832.648438\n",
      "Train Epoch: 36 [11392/17352 (66%)] Loss: -62895.816406\n",
      "Train Epoch: 36 [12800/17352 (74%)] Loss: -43332.464844\n",
      "Train Epoch: 36 [14208/17352 (82%)] Loss: -49144.859375\n",
      "Train Epoch: 36 [15521/17352 (89%)] Loss: -34699.699219\n",
      "Train Epoch: 36 [16168/17352 (93%)] Loss: -15402.678711\n",
      "Train Epoch: 36 [17103/17352 (99%)] Loss: -50657.636719\n",
      "    epoch          : 36\n",
      "    loss           : -44480.03159327155\n",
      "    val_loss       : -24961.7576171875\n",
      "Train Epoch: 37 [128/17352 (1%)] Loss: -30284.691406\n",
      "Train Epoch: 37 [1536/17352 (9%)] Loss: -41221.140625\n",
      "Train Epoch: 37 [2944/17352 (17%)] Loss: -41872.187500\n",
      "Train Epoch: 37 [4352/17352 (25%)] Loss: -35349.710938\n",
      "Train Epoch: 37 [5760/17352 (33%)] Loss: -56255.843750\n",
      "Train Epoch: 37 [7168/17352 (41%)] Loss: -32610.382812\n",
      "Train Epoch: 37 [8576/17352 (49%)] Loss: -38362.500000\n",
      "Train Epoch: 37 [9984/17352 (58%)] Loss: -60297.015625\n",
      "Train Epoch: 37 [11392/17352 (66%)] Loss: -47921.046875\n",
      "Train Epoch: 37 [12800/17352 (74%)] Loss: -41107.917969\n",
      "Train Epoch: 37 [14208/17352 (82%)] Loss: -46317.382812\n",
      "Train Epoch: 37 [15532/17352 (90%)] Loss: -37248.406250\n",
      "Train Epoch: 37 [16309/17352 (94%)] Loss: -55589.523438\n",
      "Train Epoch: 37 [17043/17352 (98%)] Loss: -2049.385498\n",
      "    epoch          : 37\n",
      "    loss           : -42992.921626353425\n",
      "    val_loss       : -22585.353930664063\n",
      "Train Epoch: 38 [128/17352 (1%)] Loss: -49224.640625\n",
      "Train Epoch: 38 [1536/17352 (9%)] Loss: -23113.328125\n",
      "Train Epoch: 38 [2944/17352 (17%)] Loss: -57534.738281\n",
      "Train Epoch: 38 [4352/17352 (25%)] Loss: -36637.398438\n",
      "Train Epoch: 38 [5760/17352 (33%)] Loss: -42910.281250\n",
      "Train Epoch: 38 [7168/17352 (41%)] Loss: -36541.933594\n",
      "Train Epoch: 38 [8576/17352 (49%)] Loss: -54557.035156\n",
      "Train Epoch: 38 [9984/17352 (58%)] Loss: -38731.769531\n",
      "Train Epoch: 38 [11392/17352 (66%)] Loss: -52365.414062\n",
      "Train Epoch: 38 [12800/17352 (74%)] Loss: -24190.908203\n",
      "Train Epoch: 38 [14208/17352 (82%)] Loss: -45208.703125\n",
      "Train Epoch: 38 [15530/17352 (89%)] Loss: -23624.433594\n",
      "Train Epoch: 38 [16438/17352 (95%)] Loss: 5072.490723\n",
      "Train Epoch: 38 [17123/17352 (99%)] Loss: -30908.839844\n",
      "    epoch          : 38\n",
      "    loss           : -39086.728088378906\n",
      "    val_loss       : -21139.25974934896\n",
      "Train Epoch: 39 [128/17352 (1%)] Loss: -31033.105469\n",
      "Train Epoch: 39 [1536/17352 (9%)] Loss: -46613.234375\n",
      "Train Epoch: 39 [2944/17352 (17%)] Loss: -50343.781250\n",
      "Train Epoch: 39 [4352/17352 (25%)] Loss: -34257.843750\n",
      "Train Epoch: 39 [5760/17352 (33%)] Loss: -34248.093750\n",
      "Train Epoch: 39 [7168/17352 (41%)] Loss: -55885.917969\n",
      "Train Epoch: 39 [8576/17352 (49%)] Loss: -47358.171875\n",
      "Train Epoch: 39 [9984/17352 (58%)] Loss: -60734.636719\n",
      "Train Epoch: 39 [11392/17352 (66%)] Loss: -34649.222656\n",
      "Train Epoch: 39 [12800/17352 (74%)] Loss: -25449.066406\n",
      "Train Epoch: 39 [14208/17352 (82%)] Loss: -69340.062500\n",
      "Train Epoch: 39 [15457/17352 (89%)] Loss: -17226.845703\n",
      "Train Epoch: 39 [16373/17352 (94%)] Loss: -33803.433594\n",
      "Train Epoch: 39 [17077/17352 (98%)] Loss: -23995.222656\n",
      "    epoch          : 39\n",
      "    loss           : -40280.63178678167\n",
      "    val_loss       : -18043.448014322916\n",
      "Train Epoch: 40 [128/17352 (1%)] Loss: -25528.964844\n",
      "Train Epoch: 40 [1536/17352 (9%)] Loss: -48928.312500\n",
      "Train Epoch: 40 [2944/17352 (17%)] Loss: -62595.148438\n",
      "Train Epoch: 40 [4352/17352 (25%)] Loss: -60054.734375\n",
      "Train Epoch: 40 [5760/17352 (33%)] Loss: -19682.781250\n",
      "Train Epoch: 40 [7168/17352 (41%)] Loss: -40993.406250\n",
      "Train Epoch: 40 [8576/17352 (49%)] Loss: -46621.687500\n",
      "Train Epoch: 40 [9984/17352 (58%)] Loss: -51957.789062\n",
      "Train Epoch: 40 [11392/17352 (66%)] Loss: -32408.056641\n",
      "Train Epoch: 40 [12800/17352 (74%)] Loss: -30273.718750\n",
      "Train Epoch: 40 [14208/17352 (82%)] Loss: -56379.031250\n",
      "Train Epoch: 40 [15518/17352 (89%)] Loss: -52436.140625\n",
      "Train Epoch: 40 [16349/17352 (94%)] Loss: -20218.527344\n",
      "Train Epoch: 40 [17171/17352 (99%)] Loss: -26585.931641\n",
      "    epoch          : 40\n",
      "    loss           : -40937.333925183186\n",
      "    val_loss       : -21380.56688639323\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0918_135442/checkpoint-epoch40.pth ...\n",
      "Train Epoch: 41 [128/17352 (1%)] Loss: -19600.865234\n",
      "Train Epoch: 41 [1536/17352 (9%)] Loss: -55453.867188\n",
      "Train Epoch: 41 [2944/17352 (17%)] Loss: -64890.468750\n",
      "Train Epoch: 41 [4352/17352 (25%)] Loss: -24588.501953\n",
      "Train Epoch: 41 [5760/17352 (33%)] Loss: -33019.257812\n",
      "Train Epoch: 41 [7168/17352 (41%)] Loss: -28309.123047\n",
      "Train Epoch: 41 [8576/17352 (49%)] Loss: -35458.617188\n",
      "Train Epoch: 41 [9984/17352 (58%)] Loss: -37266.472656\n",
      "Train Epoch: 41 [11392/17352 (66%)] Loss: -16068.404297\n",
      "Train Epoch: 41 [12800/17352 (74%)] Loss: -28919.234375\n",
      "Train Epoch: 41 [14208/17352 (82%)] Loss: -53776.937500\n",
      "Train Epoch: 41 [15492/17352 (89%)] Loss: -2316.179688\n",
      "Train Epoch: 41 [16323/17352 (94%)] Loss: -41260.160156\n",
      "Train Epoch: 41 [17068/17352 (98%)] Loss: -22626.769531\n",
      "    epoch          : 41\n",
      "    loss           : -37492.64817533557\n",
      "    val_loss       : -22324.23555501302\n",
      "Train Epoch: 42 [128/17352 (1%)] Loss: -31545.281250\n",
      "Train Epoch: 42 [1536/17352 (9%)] Loss: -19623.021484\n",
      "Train Epoch: 42 [2944/17352 (17%)] Loss: -25441.320312\n",
      "Train Epoch: 42 [4352/17352 (25%)] Loss: -43426.351562\n",
      "Train Epoch: 42 [5760/17352 (33%)] Loss: -18724.332031\n",
      "Train Epoch: 42 [7168/17352 (41%)] Loss: -36202.121094\n",
      "Train Epoch: 42 [8576/17352 (49%)] Loss: -32071.349609\n",
      "Train Epoch: 42 [9984/17352 (58%)] Loss: -20992.509766\n",
      "Train Epoch: 42 [11392/17352 (66%)] Loss: -34748.726562\n",
      "Train Epoch: 42 [12800/17352 (74%)] Loss: -32505.894531\n",
      "Train Epoch: 42 [14208/17352 (82%)] Loss: -19176.921875\n",
      "Train Epoch: 42 [15409/17352 (89%)] Loss: -12611.694336\n",
      "Train Epoch: 42 [16344/17352 (94%)] Loss: -36040.593750\n",
      "Train Epoch: 42 [17141/17352 (99%)] Loss: -30459.960938\n",
      "    epoch          : 42\n",
      "    loss           : -37377.625220381975\n",
      "    val_loss       : -19240.446354166666\n",
      "Train Epoch: 43 [128/17352 (1%)] Loss: -9931.176758\n",
      "Train Epoch: 43 [1536/17352 (9%)] Loss: -38929.765625\n",
      "Train Epoch: 43 [2944/17352 (17%)] Loss: -61564.378906\n",
      "Train Epoch: 43 [4352/17352 (25%)] Loss: -52596.531250\n",
      "Train Epoch: 43 [5760/17352 (33%)] Loss: -56078.335938\n",
      "Train Epoch: 43 [7168/17352 (41%)] Loss: -46636.054688\n",
      "Train Epoch: 43 [8576/17352 (49%)] Loss: -32413.605469\n",
      "Train Epoch: 43 [9984/17352 (58%)] Loss: -40079.617188\n",
      "Train Epoch: 43 [11392/17352 (66%)] Loss: -31670.685547\n",
      "Train Epoch: 43 [12800/17352 (74%)] Loss: -47835.972656\n",
      "Train Epoch: 43 [14208/17352 (82%)] Loss: -56506.425781\n",
      "Train Epoch: 43 [15467/17352 (89%)] Loss: -9884.093750\n",
      "Train Epoch: 43 [16228/17352 (94%)] Loss: -10687.810547\n",
      "Train Epoch: 43 [16982/17352 (98%)] Loss: -1077.958008\n",
      "    epoch          : 43\n",
      "    loss           : -40423.05586887846\n",
      "    val_loss       : -23245.344156901043\n",
      "Train Epoch: 44 [128/17352 (1%)] Loss: -47348.531250\n",
      "Train Epoch: 44 [1536/17352 (9%)] Loss: -56385.242188\n",
      "Train Epoch: 44 [2944/17352 (17%)] Loss: -49692.195312\n",
      "Train Epoch: 44 [4352/17352 (25%)] Loss: -42102.468750\n",
      "Train Epoch: 44 [5760/17352 (33%)] Loss: -52624.085938\n",
      "Train Epoch: 44 [7168/17352 (41%)] Loss: -43546.125000\n",
      "Train Epoch: 44 [8576/17352 (49%)] Loss: -47433.625000\n",
      "Train Epoch: 44 [9984/17352 (58%)] Loss: -51384.730469\n",
      "Train Epoch: 44 [11392/17352 (66%)] Loss: -41053.093750\n",
      "Train Epoch: 44 [12800/17352 (74%)] Loss: -35050.371094\n",
      "Train Epoch: 44 [14208/17352 (82%)] Loss: -13627.481445\n",
      "Train Epoch: 44 [15476/17352 (89%)] Loss: -27097.640625\n",
      "Train Epoch: 44 [16072/17352 (93%)] Loss: 1790.188232\n",
      "Train Epoch: 44 [16988/17352 (98%)] Loss: -35807.917969\n",
      "    epoch          : 44\n",
      "    loss           : -41878.57164871933\n",
      "    val_loss       : -19723.965405273437\n",
      "Train Epoch: 45 [128/17352 (1%)] Loss: -29923.210938\n",
      "Train Epoch: 45 [1536/17352 (9%)] Loss: -46167.500000\n",
      "Train Epoch: 45 [2944/17352 (17%)] Loss: -45366.464844\n",
      "Train Epoch: 45 [4352/17352 (25%)] Loss: -31816.552734\n",
      "Train Epoch: 45 [5760/17352 (33%)] Loss: -58727.851562\n",
      "Train Epoch: 45 [7168/17352 (41%)] Loss: -44884.753906\n",
      "Train Epoch: 45 [8576/17352 (49%)] Loss: -50419.726562\n",
      "Train Epoch: 45 [9984/17352 (58%)] Loss: -40751.593750\n",
      "Train Epoch: 45 [11392/17352 (66%)] Loss: -55553.011719\n",
      "Train Epoch: 45 [12800/17352 (74%)] Loss: -49374.441406\n",
      "Train Epoch: 45 [14208/17352 (82%)] Loss: -18446.107422\n",
      "Train Epoch: 45 [15538/17352 (90%)] Loss: -27866.107422\n",
      "Train Epoch: 45 [16155/17352 (93%)] Loss: -41834.683594\n",
      "Train Epoch: 45 [16854/17352 (97%)] Loss: -10289.115234\n",
      "    epoch          : 45\n",
      "    loss           : -42519.86927908059\n",
      "    val_loss       : -22821.98563639323\n",
      "Train Epoch: 46 [128/17352 (1%)] Loss: -48933.812500\n",
      "Train Epoch: 46 [1536/17352 (9%)] Loss: -49385.316406\n",
      "Train Epoch: 46 [2944/17352 (17%)] Loss: -36731.132812\n",
      "Train Epoch: 46 [4352/17352 (25%)] Loss: -27504.632812\n",
      "Train Epoch: 46 [5760/17352 (33%)] Loss: -40971.906250\n",
      "Train Epoch: 46 [7168/17352 (41%)] Loss: -39374.753906\n",
      "Train Epoch: 46 [8576/17352 (49%)] Loss: -51527.179688\n",
      "Train Epoch: 46 [9984/17352 (58%)] Loss: -46078.789062\n",
      "Train Epoch: 46 [11392/17352 (66%)] Loss: -42085.968750\n",
      "Train Epoch: 46 [12800/17352 (74%)] Loss: -35645.906250\n",
      "Train Epoch: 46 [14208/17352 (82%)] Loss: -37401.570312\n",
      "Train Epoch: 46 [15529/17352 (89%)] Loss: -36539.238281\n",
      "Train Epoch: 46 [16253/17352 (94%)] Loss: -18367.486328\n",
      "Train Epoch: 46 [16890/17352 (97%)] Loss: -21503.445312\n",
      "    epoch          : 46\n",
      "    loss           : -38248.03747845336\n",
      "    val_loss       : -20210.066031901042\n",
      "Train Epoch: 47 [128/17352 (1%)] Loss: -22632.841797\n",
      "Train Epoch: 47 [1536/17352 (9%)] Loss: -30264.742188\n",
      "Train Epoch: 47 [2944/17352 (17%)] Loss: -49250.648438\n",
      "Train Epoch: 47 [4352/17352 (25%)] Loss: -18745.609375\n",
      "Train Epoch: 47 [5760/17352 (33%)] Loss: -31220.912109\n",
      "Train Epoch: 47 [7168/17352 (41%)] Loss: -52648.609375\n",
      "Train Epoch: 47 [8576/17352 (49%)] Loss: -35495.152344\n",
      "Train Epoch: 47 [9984/17352 (58%)] Loss: -38097.750000\n",
      "Train Epoch: 47 [11392/17352 (66%)] Loss: -43145.472656\n",
      "Train Epoch: 47 [12800/17352 (74%)] Loss: -39190.328125\n",
      "Train Epoch: 47 [14208/17352 (82%)] Loss: -50208.898438\n",
      "Train Epoch: 47 [15525/17352 (89%)] Loss: -33855.570312\n",
      "Train Epoch: 47 [16239/17352 (94%)] Loss: -27704.539062\n",
      "Train Epoch: 47 [16903/17352 (97%)] Loss: -19320.466797\n",
      "    epoch          : 47\n",
      "    loss           : -38502.83453573957\n",
      "    val_loss       : -20328.314200846355\n",
      "Train Epoch: 48 [128/17352 (1%)] Loss: -42149.312500\n",
      "Train Epoch: 48 [1536/17352 (9%)] Loss: -26843.580078\n",
      "Train Epoch: 48 [2944/17352 (17%)] Loss: -20638.253906\n",
      "Train Epoch: 48 [4352/17352 (25%)] Loss: -38571.914062\n",
      "Train Epoch: 48 [5760/17352 (33%)] Loss: -44768.386719\n",
      "Train Epoch: 48 [7168/17352 (41%)] Loss: -39245.382812\n",
      "Train Epoch: 48 [8576/17352 (49%)] Loss: -53450.773438\n",
      "Train Epoch: 48 [9984/17352 (58%)] Loss: -33450.636719\n",
      "Train Epoch: 48 [11392/17352 (66%)] Loss: -24309.437500\n",
      "Train Epoch: 48 [12800/17352 (74%)] Loss: -42828.382812\n",
      "Train Epoch: 48 [14208/17352 (82%)] Loss: -35139.148438\n",
      "Train Epoch: 48 [15419/17352 (89%)] Loss: -22549.181641\n",
      "Train Epoch: 48 [16198/17352 (93%)] Loss: -15979.349609\n",
      "Train Epoch: 48 [16947/17352 (98%)] Loss: -19118.242188\n",
      "    epoch          : 48\n",
      "    loss           : -35612.34335921115\n",
      "    val_loss       : -18643.546614583334\n",
      "Train Epoch: 49 [128/17352 (1%)] Loss: -51459.210938\n",
      "Train Epoch: 49 [1536/17352 (9%)] Loss: -28293.648438\n",
      "Train Epoch: 49 [2944/17352 (17%)] Loss: -48017.992188\n",
      "Train Epoch: 49 [4352/17352 (25%)] Loss: -25118.791016\n",
      "Train Epoch: 49 [5760/17352 (33%)] Loss: -24748.644531\n",
      "Train Epoch: 49 [7168/17352 (41%)] Loss: -36525.917969\n",
      "Train Epoch: 49 [8576/17352 (49%)] Loss: -40972.898438\n",
      "Train Epoch: 49 [9984/17352 (58%)] Loss: -29287.835938\n",
      "Train Epoch: 49 [11392/17352 (66%)] Loss: -58788.917969\n",
      "Train Epoch: 49 [12800/17352 (74%)] Loss: -27718.089844\n",
      "Train Epoch: 49 [14208/17352 (82%)] Loss: -47800.921875\n",
      "Train Epoch: 49 [15399/17352 (89%)] Loss: -13069.569336\n",
      "Train Epoch: 49 [16191/17352 (93%)] Loss: -6683.247559\n",
      "Train Epoch: 49 [16860/17352 (97%)] Loss: -29284.175781\n",
      "    epoch          : 49\n",
      "    loss           : -35279.6895223528\n",
      "    val_loss       : -16121.529939778646\n",
      "Train Epoch: 50 [128/17352 (1%)] Loss: -40165.449219\n",
      "Train Epoch: 50 [1536/17352 (9%)] Loss: -27819.962891\n",
      "Train Epoch: 50 [2944/17352 (17%)] Loss: -47733.101562\n",
      "Train Epoch: 50 [4352/17352 (25%)] Loss: -25373.744141\n",
      "Train Epoch: 50 [5760/17352 (33%)] Loss: -44011.425781\n",
      "Train Epoch: 50 [7168/17352 (41%)] Loss: -52260.289062\n",
      "Train Epoch: 50 [8576/17352 (49%)] Loss: -48255.066406\n",
      "Train Epoch: 50 [9984/17352 (58%)] Loss: -29079.308594\n",
      "Train Epoch: 50 [11392/17352 (66%)] Loss: -11226.699219\n",
      "Train Epoch: 50 [12800/17352 (74%)] Loss: -39592.265625\n",
      "Train Epoch: 50 [14208/17352 (82%)] Loss: -53517.820312\n",
      "Train Epoch: 50 [15469/17352 (89%)] Loss: -13542.806641\n",
      "Train Epoch: 50 [16216/17352 (93%)] Loss: -20259.720703\n",
      "Train Epoch: 50 [16917/17352 (97%)] Loss: -42751.925781\n",
      "    epoch          : 50\n",
      "    loss           : -33935.967690691854\n",
      "    val_loss       : -15095.754207356771\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0918_135442/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [128/17352 (1%)] Loss: -38014.500000\n",
      "Train Epoch: 51 [1536/17352 (9%)] Loss: -43779.750000\n",
      "Train Epoch: 51 [2944/17352 (17%)] Loss: -20099.369141\n",
      "Train Epoch: 51 [4352/17352 (25%)] Loss: -25091.369141\n",
      "Train Epoch: 51 [5760/17352 (33%)] Loss: -48031.671875\n",
      "Train Epoch: 51 [7168/17352 (41%)] Loss: -42751.570312\n",
      "Train Epoch: 51 [8576/17352 (49%)] Loss: -39304.722656\n",
      "Train Epoch: 51 [9984/17352 (58%)] Loss: -29237.193359\n",
      "Train Epoch: 51 [11392/17352 (66%)] Loss: -52406.949219\n",
      "Train Epoch: 51 [12800/17352 (74%)] Loss: -24734.382812\n",
      "Train Epoch: 51 [14208/17352 (82%)] Loss: -51827.613281\n",
      "Train Epoch: 51 [15525/17352 (89%)] Loss: 2466.078369\n",
      "Train Epoch: 51 [16431/17352 (95%)] Loss: -22701.679688\n",
      "Train Epoch: 51 [17100/17352 (99%)] Loss: -10743.568359\n",
      "    epoch          : 51\n",
      "    loss           : -35915.05556544362\n",
      "    val_loss       : -18053.828255208333\n",
      "Train Epoch: 52 [128/17352 (1%)] Loss: -44996.933594\n",
      "Train Epoch: 52 [1536/17352 (9%)] Loss: -59075.171875\n",
      "Train Epoch: 52 [2944/17352 (17%)] Loss: -54834.527344\n",
      "Train Epoch: 52 [4352/17352 (25%)] Loss: -53087.468750\n",
      "Train Epoch: 52 [5760/17352 (33%)] Loss: -57545.804688\n",
      "Train Epoch: 52 [7168/17352 (41%)] Loss: -31156.527344\n",
      "Train Epoch: 52 [8576/17352 (49%)] Loss: -58778.882812\n",
      "Train Epoch: 52 [9984/17352 (58%)] Loss: -26317.279297\n",
      "Train Epoch: 52 [11392/17352 (66%)] Loss: -45134.574219\n",
      "Train Epoch: 52 [12800/17352 (74%)] Loss: -33970.554688\n",
      "Train Epoch: 52 [14208/17352 (82%)] Loss: -35352.890625\n",
      "Train Epoch: 52 [15546/17352 (90%)] Loss: -42526.980469\n",
      "Train Epoch: 52 [16098/17352 (93%)] Loss: -11415.880859\n",
      "Train Epoch: 52 [17005/17352 (98%)] Loss: -17403.009766\n",
      "    epoch          : 52\n",
      "    loss           : -35436.05183318477\n",
      "    val_loss       : -17542.515966796876\n",
      "Train Epoch: 53 [128/17352 (1%)] Loss: -22543.914062\n",
      "Train Epoch: 53 [1536/17352 (9%)] Loss: -47940.500000\n",
      "Train Epoch: 53 [2944/17352 (17%)] Loss: -38525.187500\n",
      "Train Epoch: 53 [4352/17352 (25%)] Loss: -20850.714844\n",
      "Train Epoch: 53 [5760/17352 (33%)] Loss: -49931.507812\n",
      "Train Epoch: 53 [7168/17352 (41%)] Loss: -42378.289062\n",
      "Train Epoch: 53 [8576/17352 (49%)] Loss: -24847.150391\n",
      "Train Epoch: 53 [9984/17352 (58%)] Loss: -27179.945312\n",
      "Train Epoch: 53 [11392/17352 (66%)] Loss: -49845.710938\n",
      "Train Epoch: 53 [12800/17352 (74%)] Loss: -38799.617188\n",
      "Train Epoch: 53 [14208/17352 (82%)] Loss: -33102.832031\n",
      "Train Epoch: 53 [15551/17352 (90%)] Loss: -32990.152344\n",
      "Train Epoch: 53 [16279/17352 (94%)] Loss: -36171.429688\n",
      "Train Epoch: 53 [16873/17352 (97%)] Loss: 5675.133301\n",
      "    epoch          : 53\n",
      "    loss           : -34110.28232323563\n",
      "    val_loss       : -16926.91142578125\n",
      "Train Epoch: 54 [128/17352 (1%)] Loss: -37876.789062\n",
      "Train Epoch: 54 [1536/17352 (9%)] Loss: -24670.333984\n",
      "Train Epoch: 54 [2944/17352 (17%)] Loss: -22023.144531\n",
      "Train Epoch: 54 [4352/17352 (25%)] Loss: -41810.375000\n",
      "Train Epoch: 54 [5760/17352 (33%)] Loss: -8649.862305\n",
      "Train Epoch: 54 [7168/17352 (41%)] Loss: -25080.460938\n",
      "Train Epoch: 54 [8576/17352 (49%)] Loss: -76988.796875\n",
      "Train Epoch: 54 [9984/17352 (58%)] Loss: -16454.226562\n",
      "Train Epoch: 54 [11392/17352 (66%)] Loss: -15482.144531\n",
      "Train Epoch: 54 [12800/17352 (74%)] Loss: -34557.781250\n",
      "Train Epoch: 54 [14208/17352 (82%)] Loss: -40681.074219\n",
      "Train Epoch: 54 [15528/17352 (89%)] Loss: -38593.617188\n",
      "Train Epoch: 54 [16327/17352 (94%)] Loss: -1408.887695\n",
      "Train Epoch: 54 [17039/17352 (98%)] Loss: 1080.967041\n",
      "    epoch          : 54\n",
      "    loss           : -35661.509291271235\n",
      "    val_loss       : -19999.878019205727\n",
      "Train Epoch: 55 [128/17352 (1%)] Loss: 12106.157227\n",
      "Train Epoch: 55 [1536/17352 (9%)] Loss: -26264.607422\n",
      "Train Epoch: 55 [2944/17352 (17%)] Loss: -32979.492188\n",
      "Train Epoch: 55 [4352/17352 (25%)] Loss: -45389.042969\n",
      "Train Epoch: 55 [5760/17352 (33%)] Loss: -13307.833984\n",
      "Train Epoch: 55 [7168/17352 (41%)] Loss: -15367.171875\n",
      "Train Epoch: 55 [8576/17352 (49%)] Loss: -27959.644531\n",
      "Train Epoch: 55 [9984/17352 (58%)] Loss: -36501.574219\n",
      "Train Epoch: 55 [11392/17352 (66%)] Loss: -42221.617188\n",
      "Train Epoch: 55 [12800/17352 (74%)] Loss: -38164.167969\n",
      "Train Epoch: 55 [14208/17352 (82%)] Loss: -61493.812500\n",
      "Train Epoch: 55 [15521/17352 (89%)] Loss: -18832.619141\n",
      "Train Epoch: 55 [16192/17352 (93%)] Loss: -5799.408203\n",
      "Train Epoch: 55 [17031/17352 (98%)] Loss: -56028.187500\n",
      "    epoch          : 55\n",
      "    loss           : -38899.07928917392\n",
      "    val_loss       : -23211.82569986979\n",
      "Train Epoch: 56 [128/17352 (1%)] Loss: -25113.029297\n",
      "Train Epoch: 56 [1536/17352 (9%)] Loss: -36621.863281\n",
      "Train Epoch: 56 [2944/17352 (17%)] Loss: -52055.242188\n",
      "Train Epoch: 56 [4352/17352 (25%)] Loss: -43836.945312\n",
      "Train Epoch: 56 [5760/17352 (33%)] Loss: -55485.089844\n",
      "Train Epoch: 56 [7168/17352 (41%)] Loss: -53069.703125\n",
      "Train Epoch: 56 [8576/17352 (49%)] Loss: -53648.937500\n",
      "Train Epoch: 56 [9984/17352 (58%)] Loss: -40364.351562\n",
      "Train Epoch: 56 [11392/17352 (66%)] Loss: -21689.539062\n",
      "Train Epoch: 56 [12800/17352 (74%)] Loss: -63866.144531\n",
      "Train Epoch: 56 [14208/17352 (82%)] Loss: -30106.644531\n",
      "Train Epoch: 56 [15487/17352 (89%)] Loss: -20453.320312\n",
      "Train Epoch: 56 [16314/17352 (94%)] Loss: -37825.171875\n",
      "Train Epoch: 56 [17080/17352 (98%)] Loss: -3393.558105\n",
      "    epoch          : 56\n",
      "    loss           : -41673.50776989828\n",
      "    val_loss       : -24726.111482747398\n",
      "Train Epoch: 57 [128/17352 (1%)] Loss: -33103.402344\n",
      "Train Epoch: 57 [1536/17352 (9%)] Loss: -65092.000000\n",
      "Train Epoch: 57 [2944/17352 (17%)] Loss: -41368.125000\n",
      "Train Epoch: 57 [4352/17352 (25%)] Loss: -61214.039062\n",
      "Train Epoch: 57 [5760/17352 (33%)] Loss: -23055.492188\n",
      "Train Epoch: 57 [7168/17352 (41%)] Loss: -38586.902344\n",
      "Train Epoch: 57 [8576/17352 (49%)] Loss: -46864.835938\n",
      "Train Epoch: 57 [9984/17352 (58%)] Loss: -61299.296875\n",
      "Train Epoch: 57 [11392/17352 (66%)] Loss: -44382.312500\n",
      "Train Epoch: 57 [12800/17352 (74%)] Loss: -25841.300781\n",
      "Train Epoch: 57 [14208/17352 (82%)] Loss: -67419.632812\n",
      "Train Epoch: 57 [15480/17352 (89%)] Loss: -3704.808350\n",
      "Train Epoch: 57 [16243/17352 (94%)] Loss: -18893.501953\n",
      "Train Epoch: 57 [16970/17352 (98%)] Loss: -12997.148438\n",
      "    epoch          : 57\n",
      "    loss           : -42595.238544643325\n",
      "    val_loss       : -26258.596028645832\n",
      "Train Epoch: 58 [128/17352 (1%)] Loss: -56441.753906\n",
      "Train Epoch: 58 [1536/17352 (9%)] Loss: -50970.480469\n",
      "Train Epoch: 58 [2944/17352 (17%)] Loss: -40587.500000\n",
      "Train Epoch: 58 [4352/17352 (25%)] Loss: -39645.187500\n",
      "Train Epoch: 58 [5760/17352 (33%)] Loss: -59850.398438\n",
      "Train Epoch: 58 [7168/17352 (41%)] Loss: -54447.835938\n",
      "Train Epoch: 58 [8576/17352 (49%)] Loss: -58993.519531\n",
      "Train Epoch: 58 [9984/17352 (58%)] Loss: -46818.179688\n",
      "Train Epoch: 58 [11392/17352 (66%)] Loss: -52942.996094\n",
      "Train Epoch: 58 [12800/17352 (74%)] Loss: -37694.582031\n",
      "Train Epoch: 58 [14208/17352 (82%)] Loss: -46955.902344\n",
      "Train Epoch: 58 [15541/17352 (90%)] Loss: -36009.859375\n",
      "Train Epoch: 58 [16323/17352 (94%)] Loss: -227.646729\n",
      "Train Epoch: 58 [17034/17352 (98%)] Loss: -1706.665405\n",
      "    epoch          : 58\n",
      "    loss           : -42537.09582150863\n",
      "    val_loss       : -27163.884358723957\n",
      "Train Epoch: 59 [128/17352 (1%)] Loss: -53984.484375\n",
      "Train Epoch: 59 [1536/17352 (9%)] Loss: -61327.300781\n",
      "Train Epoch: 59 [2944/17352 (17%)] Loss: -61970.152344\n",
      "Train Epoch: 59 [4352/17352 (25%)] Loss: -53882.539062\n",
      "Train Epoch: 59 [5760/17352 (33%)] Loss: -56029.500000\n",
      "Train Epoch: 59 [7168/17352 (41%)] Loss: -32246.285156\n",
      "Train Epoch: 59 [8576/17352 (49%)] Loss: -54491.863281\n",
      "Train Epoch: 59 [9984/17352 (58%)] Loss: -43929.503906\n",
      "Train Epoch: 59 [11392/17352 (66%)] Loss: -38437.511719\n",
      "Train Epoch: 59 [12800/17352 (74%)] Loss: -32618.683594\n",
      "Train Epoch: 59 [14208/17352 (82%)] Loss: -53365.031250\n",
      "Train Epoch: 59 [15479/17352 (89%)] Loss: -23348.451172\n",
      "Train Epoch: 59 [16214/17352 (93%)] Loss: 6557.023438\n",
      "Train Epoch: 59 [17056/17352 (98%)] Loss: -32870.121094\n",
      "    epoch          : 59\n",
      "    loss           : -40956.09491171613\n",
      "    val_loss       : -21476.884033203125\n",
      "Train Epoch: 60 [128/17352 (1%)] Loss: -56754.828125\n",
      "Train Epoch: 60 [1536/17352 (9%)] Loss: -46305.355469\n",
      "Train Epoch: 60 [2944/17352 (17%)] Loss: -51763.386719\n",
      "Train Epoch: 60 [4352/17352 (25%)] Loss: -39102.402344\n",
      "Train Epoch: 60 [5760/17352 (33%)] Loss: -49715.968750\n",
      "Train Epoch: 60 [7168/17352 (41%)] Loss: -61767.484375\n",
      "Train Epoch: 60 [8576/17352 (49%)] Loss: -39886.820312\n",
      "Train Epoch: 60 [9984/17352 (58%)] Loss: -49498.511719\n",
      "Train Epoch: 60 [11392/17352 (66%)] Loss: -32232.205078\n",
      "Train Epoch: 60 [12800/17352 (74%)] Loss: -42729.781250\n",
      "Train Epoch: 60 [14208/17352 (82%)] Loss: -65646.968750\n",
      "Train Epoch: 60 [15498/17352 (89%)] Loss: -14028.050781\n",
      "Train Epoch: 60 [16264/17352 (94%)] Loss: -33143.886719\n",
      "Train Epoch: 60 [16909/17352 (97%)] Loss: -49676.992188\n",
      "    epoch          : 60\n",
      "    loss           : -41309.68654770819\n",
      "    val_loss       : -22581.55547688802\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0918_135442/checkpoint-epoch60.pth ...\n",
      "Train Epoch: 61 [128/17352 (1%)] Loss: -33272.367188\n",
      "Train Epoch: 61 [1536/17352 (9%)] Loss: -60778.734375\n",
      "Train Epoch: 61 [2944/17352 (17%)] Loss: -26628.412109\n",
      "Train Epoch: 61 [4352/17352 (25%)] Loss: -25909.898438\n",
      "Train Epoch: 61 [5760/17352 (33%)] Loss: -18723.335938\n",
      "Train Epoch: 61 [7168/17352 (41%)] Loss: -39791.187500\n",
      "Train Epoch: 61 [8576/17352 (49%)] Loss: -45491.371094\n",
      "Train Epoch: 61 [9984/17352 (58%)] Loss: -39335.914062\n",
      "Train Epoch: 61 [11392/17352 (66%)] Loss: -60612.417969\n",
      "Train Epoch: 61 [12800/17352 (74%)] Loss: -54849.656250\n",
      "Train Epoch: 61 [14208/17352 (82%)] Loss: -56577.464844\n",
      "Train Epoch: 61 [15449/17352 (89%)] Loss: -38204.527344\n",
      "Train Epoch: 61 [16293/17352 (94%)] Loss: -2287.817139\n",
      "Train Epoch: 61 [16957/17352 (98%)] Loss: -29359.945312\n",
      "    epoch          : 61\n",
      "    loss           : -38580.29664069054\n",
      "    val_loss       : -20107.42089029948\n",
      "Train Epoch: 62 [128/17352 (1%)] Loss: -16051.537109\n",
      "Train Epoch: 62 [1536/17352 (9%)] Loss: -46561.851562\n",
      "Train Epoch: 62 [2944/17352 (17%)] Loss: -37759.093750\n",
      "Train Epoch: 62 [4352/17352 (25%)] Loss: -48893.695312\n",
      "Train Epoch: 62 [5760/17352 (33%)] Loss: -36512.765625\n",
      "Train Epoch: 62 [7168/17352 (41%)] Loss: -24719.242188\n",
      "Train Epoch: 62 [8576/17352 (49%)] Loss: -36047.402344\n",
      "Train Epoch: 62 [9984/17352 (58%)] Loss: -22282.714844\n",
      "Train Epoch: 62 [11392/17352 (66%)] Loss: -38538.425781\n",
      "Train Epoch: 62 [12800/17352 (74%)] Loss: -35523.156250\n",
      "Train Epoch: 62 [14208/17352 (82%)] Loss: -63240.773438\n",
      "Train Epoch: 62 [15540/17352 (90%)] Loss: -29178.820312\n",
      "Train Epoch: 62 [16204/17352 (93%)] Loss: -16742.142578\n",
      "Train Epoch: 62 [16891/17352 (97%)] Loss: -7073.745117\n",
      "    epoch          : 62\n",
      "    loss           : -40797.11784946518\n",
      "    val_loss       : -22599.57931315104\n",
      "Train Epoch: 63 [128/17352 (1%)] Loss: -39234.695312\n",
      "Train Epoch: 63 [1536/17352 (9%)] Loss: -62965.812500\n",
      "Train Epoch: 63 [2944/17352 (17%)] Loss: -55848.003906\n",
      "Train Epoch: 63 [4352/17352 (25%)] Loss: -55642.511719\n",
      "Train Epoch: 63 [5760/17352 (33%)] Loss: -54361.656250\n",
      "Train Epoch: 63 [7168/17352 (41%)] Loss: -38524.062500\n",
      "Train Epoch: 63 [8576/17352 (49%)] Loss: -31440.919922\n",
      "Train Epoch: 63 [9984/17352 (58%)] Loss: -54753.636719\n",
      "Train Epoch: 63 [11392/17352 (66%)] Loss: -45937.066406\n",
      "Train Epoch: 63 [12800/17352 (74%)] Loss: -45537.742188\n",
      "Train Epoch: 63 [14208/17352 (82%)] Loss: -50794.179688\n",
      "Train Epoch: 63 [15448/17352 (89%)] Loss: -30492.980469\n",
      "Train Epoch: 63 [16084/17352 (93%)] Loss: -30993.164062\n",
      "Train Epoch: 63 [16893/17352 (97%)] Loss: -33814.980469\n",
      "    epoch          : 63\n",
      "    loss           : -39502.522238097736\n",
      "    val_loss       : -19816.40545247396\n",
      "Train Epoch: 64 [128/17352 (1%)] Loss: -45455.125000\n",
      "Train Epoch: 64 [1536/17352 (9%)] Loss: -36093.203125\n",
      "Train Epoch: 64 [2944/17352 (17%)] Loss: -34851.203125\n",
      "Train Epoch: 64 [4352/17352 (25%)] Loss: -54278.875000\n",
      "Train Epoch: 64 [5760/17352 (33%)] Loss: -28114.289062\n",
      "Train Epoch: 64 [7168/17352 (41%)] Loss: -43711.792969\n",
      "Train Epoch: 64 [8576/17352 (49%)] Loss: -26809.289062\n",
      "Train Epoch: 64 [9984/17352 (58%)] Loss: -52850.292969\n",
      "Train Epoch: 64 [11392/17352 (66%)] Loss: -34472.929688\n",
      "Train Epoch: 64 [12800/17352 (74%)] Loss: -60137.023438\n",
      "Train Epoch: 64 [14208/17352 (82%)] Loss: -53671.726562\n",
      "Train Epoch: 64 [15535/17352 (90%)] Loss: -21407.781250\n",
      "Train Epoch: 64 [16228/17352 (94%)] Loss: -40739.796875\n",
      "Train Epoch: 64 [17007/17352 (98%)] Loss: -13518.589844\n",
      "    epoch          : 64\n",
      "    loss           : -39773.84071216967\n",
      "    val_loss       : -21716.010026041666\n",
      "Train Epoch: 65 [128/17352 (1%)] Loss: -32418.537109\n",
      "Train Epoch: 65 [1536/17352 (9%)] Loss: -48644.929688\n",
      "Train Epoch: 65 [2944/17352 (17%)] Loss: -23928.906250\n",
      "Train Epoch: 65 [4352/17352 (25%)] Loss: -50133.785156\n",
      "Train Epoch: 65 [5760/17352 (33%)] Loss: -33331.164062\n",
      "Train Epoch: 65 [7168/17352 (41%)] Loss: -32867.476562\n",
      "Train Epoch: 65 [8576/17352 (49%)] Loss: -50432.609375\n",
      "Train Epoch: 65 [9984/17352 (58%)] Loss: -18982.052734\n",
      "Train Epoch: 65 [11392/17352 (66%)] Loss: -39031.722656\n",
      "Train Epoch: 65 [12800/17352 (74%)] Loss: -53618.718750\n",
      "Train Epoch: 65 [14208/17352 (82%)] Loss: -28679.371094\n",
      "Train Epoch: 65 [15458/17352 (89%)] Loss: -32531.066406\n",
      "Train Epoch: 65 [16166/17352 (93%)] Loss: -17884.750000\n",
      "Train Epoch: 65 [17004/17352 (98%)] Loss: -40690.453125\n",
      "    epoch          : 65\n",
      "    loss           : -36762.60634208526\n",
      "    val_loss       : -22419.22401529948\n",
      "Train Epoch: 66 [128/17352 (1%)] Loss: -42894.453125\n",
      "Train Epoch: 66 [1536/17352 (9%)] Loss: -33922.144531\n",
      "Train Epoch: 66 [2944/17352 (17%)] Loss: -46346.632812\n",
      "Train Epoch: 66 [4352/17352 (25%)] Loss: -21924.625000\n",
      "Train Epoch: 66 [5760/17352 (33%)] Loss: -48151.125000\n",
      "Train Epoch: 66 [7168/17352 (41%)] Loss: -34493.457031\n",
      "Train Epoch: 66 [8576/17352 (49%)] Loss: -32606.511719\n",
      "Train Epoch: 66 [9984/17352 (58%)] Loss: -32115.990234\n",
      "Train Epoch: 66 [11392/17352 (66%)] Loss: -53636.523438\n",
      "Train Epoch: 66 [12800/17352 (74%)] Loss: -18094.714844\n",
      "Train Epoch: 66 [14208/17352 (82%)] Loss: -54291.050781\n",
      "Train Epoch: 66 [15487/17352 (89%)] Loss: -25948.445312\n",
      "Train Epoch: 66 [16157/17352 (93%)] Loss: -28615.695312\n",
      "Train Epoch: 66 [17043/17352 (98%)] Loss: -20011.167969\n",
      "    epoch          : 66\n",
      "    loss           : -39118.76221358536\n",
      "    val_loss       : -21675.190665690105\n",
      "Train Epoch: 67 [128/17352 (1%)] Loss: -23621.339844\n",
      "Train Epoch: 67 [1536/17352 (9%)] Loss: -44490.921875\n",
      "Train Epoch: 67 [2944/17352 (17%)] Loss: -27424.449219\n",
      "Train Epoch: 67 [4352/17352 (25%)] Loss: -34851.496094\n",
      "Train Epoch: 67 [5760/17352 (33%)] Loss: -37273.007812\n",
      "Train Epoch: 67 [7168/17352 (41%)] Loss: -40129.515625\n",
      "Train Epoch: 67 [8576/17352 (49%)] Loss: -60440.675781\n",
      "Train Epoch: 67 [9984/17352 (58%)] Loss: -59656.195312\n",
      "Train Epoch: 67 [11392/17352 (66%)] Loss: -29736.687500\n",
      "Train Epoch: 67 [12800/17352 (74%)] Loss: -85413.734375\n",
      "Train Epoch: 67 [14208/17352 (82%)] Loss: -36743.773438\n",
      "Train Epoch: 67 [15444/17352 (89%)] Loss: -21845.539062\n",
      "Train Epoch: 67 [16377/17352 (94%)] Loss: -33802.390625\n",
      "Train Epoch: 67 [17029/17352 (98%)] Loss: -30869.224609\n",
      "    epoch          : 67\n",
      "    loss           : -40878.086203351115\n",
      "    val_loss       : -21573.27236328125\n",
      "Train Epoch: 68 [128/17352 (1%)] Loss: -45262.539062\n",
      "Train Epoch: 68 [1536/17352 (9%)] Loss: -58116.496094\n",
      "Train Epoch: 68 [2944/17352 (17%)] Loss: -58801.070312\n",
      "Train Epoch: 68 [4352/17352 (25%)] Loss: -42541.378906\n",
      "Train Epoch: 68 [5760/17352 (33%)] Loss: -49858.117188\n",
      "Train Epoch: 68 [7168/17352 (41%)] Loss: -21578.138672\n",
      "Train Epoch: 68 [8576/17352 (49%)] Loss: -18121.597656\n",
      "Train Epoch: 68 [9984/17352 (58%)] Loss: -52181.750000\n",
      "Train Epoch: 68 [11392/17352 (66%)] Loss: -51404.843750\n",
      "Train Epoch: 68 [12800/17352 (74%)] Loss: -50996.511719\n",
      "Train Epoch: 68 [14208/17352 (82%)] Loss: -50673.546875\n",
      "Train Epoch: 68 [15534/17352 (90%)] Loss: -27929.863281\n",
      "Train Epoch: 68 [16176/17352 (93%)] Loss: -20684.046875\n",
      "Train Epoch: 68 [16922/17352 (98%)] Loss: 2129.164551\n",
      "    epoch          : 68\n",
      "    loss           : -41351.8704661939\n",
      "    val_loss       : -23745.446834309896\n",
      "Train Epoch: 69 [128/17352 (1%)] Loss: -27094.181641\n",
      "Train Epoch: 69 [1536/17352 (9%)] Loss: -62794.921875\n",
      "Train Epoch: 69 [2944/17352 (17%)] Loss: -42101.808594\n",
      "Train Epoch: 69 [4352/17352 (25%)] Loss: -39104.480469\n",
      "Train Epoch: 69 [5760/17352 (33%)] Loss: -46378.085938\n",
      "Train Epoch: 69 [7168/17352 (41%)] Loss: -27383.953125\n",
      "Train Epoch: 69 [8576/17352 (49%)] Loss: -49697.218750\n",
      "Train Epoch: 69 [9984/17352 (58%)] Loss: -30616.529297\n",
      "Train Epoch: 69 [11392/17352 (66%)] Loss: -33793.457031\n",
      "Train Epoch: 69 [12800/17352 (74%)] Loss: -43544.250000\n",
      "Train Epoch: 69 [14208/17352 (82%)] Loss: -42028.761719\n",
      "Train Epoch: 69 [15453/17352 (89%)] Loss: -6217.837402\n",
      "Train Epoch: 69 [16127/17352 (93%)] Loss: -14947.188477\n",
      "Train Epoch: 69 [16910/17352 (97%)] Loss: -4507.626465\n",
      "    epoch          : 69\n",
      "    loss           : -40795.939944683305\n",
      "    val_loss       : -23087.154231770834\n",
      "Train Epoch: 70 [128/17352 (1%)] Loss: -22469.515625\n",
      "Train Epoch: 70 [1536/17352 (9%)] Loss: -40876.371094\n",
      "Train Epoch: 70 [2944/17352 (17%)] Loss: -45084.992188\n",
      "Train Epoch: 70 [4352/17352 (25%)] Loss: -36661.796875\n",
      "Train Epoch: 70 [5760/17352 (33%)] Loss: -71213.484375\n",
      "Train Epoch: 70 [7168/17352 (41%)] Loss: -56381.750000\n",
      "Train Epoch: 70 [8576/17352 (49%)] Loss: -23479.605469\n",
      "Train Epoch: 70 [9984/17352 (58%)] Loss: -41834.617188\n",
      "Train Epoch: 70 [11392/17352 (66%)] Loss: -60464.367188\n",
      "Train Epoch: 70 [12800/17352 (74%)] Loss: -62217.445312\n",
      "Train Epoch: 70 [14208/17352 (82%)] Loss: -37245.585938\n",
      "Train Epoch: 70 [15489/17352 (89%)] Loss: -9699.731445\n",
      "Train Epoch: 70 [16225/17352 (94%)] Loss: -35057.621094\n",
      "Train Epoch: 70 [16984/17352 (98%)] Loss: -540.739746\n",
      "    epoch          : 70\n",
      "    loss           : -41512.16125570208\n",
      "    val_loss       : -23502.72822265625\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0918_135442/checkpoint-epoch70.pth ...\n",
      "Train Epoch: 71 [128/17352 (1%)] Loss: -47032.085938\n",
      "Train Epoch: 71 [1536/17352 (9%)] Loss: -52152.140625\n",
      "Train Epoch: 71 [2944/17352 (17%)] Loss: -65805.218750\n",
      "Train Epoch: 71 [4352/17352 (25%)] Loss: -63790.593750\n",
      "Train Epoch: 71 [5760/17352 (33%)] Loss: -24415.242188\n",
      "Train Epoch: 71 [7168/17352 (41%)] Loss: -47767.156250\n",
      "Train Epoch: 71 [8576/17352 (49%)] Loss: -63442.507812\n",
      "Train Epoch: 71 [9984/17352 (58%)] Loss: -28753.753906\n",
      "Train Epoch: 71 [11392/17352 (66%)] Loss: -36852.082031\n",
      "Train Epoch: 71 [12800/17352 (74%)] Loss: -23532.333984\n",
      "Train Epoch: 71 [14208/17352 (82%)] Loss: -38912.289062\n",
      "Train Epoch: 71 [15556/17352 (90%)] Loss: -34579.449219\n",
      "Train Epoch: 71 [16254/17352 (94%)] Loss: -21699.656250\n",
      "Train Epoch: 71 [16992/17352 (98%)] Loss: -11120.892578\n",
      "    epoch          : 71\n",
      "    loss           : -41861.9108427931\n",
      "    val_loss       : -21516.23928222656\n",
      "Train Epoch: 72 [128/17352 (1%)] Loss: -19479.955078\n",
      "Train Epoch: 72 [1536/17352 (9%)] Loss: -42090.570312\n",
      "Train Epoch: 72 [2944/17352 (17%)] Loss: -77415.781250\n",
      "Train Epoch: 72 [4352/17352 (25%)] Loss: -48144.738281\n",
      "Train Epoch: 72 [5760/17352 (33%)] Loss: -46650.929688\n",
      "Train Epoch: 72 [7168/17352 (41%)] Loss: -41801.707031\n",
      "Train Epoch: 72 [8576/17352 (49%)] Loss: -41903.542969\n",
      "Train Epoch: 72 [9984/17352 (58%)] Loss: -25875.503906\n",
      "Train Epoch: 72 [11392/17352 (66%)] Loss: -35661.433594\n",
      "Train Epoch: 72 [12800/17352 (74%)] Loss: -55975.765625\n",
      "Train Epoch: 72 [14208/17352 (82%)] Loss: -64597.128906\n",
      "Train Epoch: 72 [15509/17352 (89%)] Loss: -27565.058594\n",
      "Train Epoch: 72 [16342/17352 (94%)] Loss: -39268.328125\n",
      "Train Epoch: 72 [16953/17352 (98%)] Loss: -28250.691406\n",
      "    epoch          : 72\n",
      "    loss           : -40589.56407954069\n",
      "    val_loss       : -24344.669848632813\n",
      "Train Epoch: 73 [128/17352 (1%)] Loss: -38629.804688\n",
      "Train Epoch: 73 [1536/17352 (9%)] Loss: -41341.664062\n",
      "Train Epoch: 73 [2944/17352 (17%)] Loss: -50898.878906\n",
      "Train Epoch: 73 [4352/17352 (25%)] Loss: -63716.656250\n",
      "Train Epoch: 73 [5760/17352 (33%)] Loss: -38167.085938\n",
      "Train Epoch: 73 [7168/17352 (41%)] Loss: -56535.476562\n",
      "Train Epoch: 73 [8576/17352 (49%)] Loss: -32138.628906\n",
      "Train Epoch: 73 [9984/17352 (58%)] Loss: -67296.132812\n",
      "Train Epoch: 73 [11392/17352 (66%)] Loss: -24039.955078\n",
      "Train Epoch: 73 [12800/17352 (74%)] Loss: -47218.710938\n",
      "Train Epoch: 73 [14208/17352 (82%)] Loss: -52392.433594\n",
      "Train Epoch: 73 [15464/17352 (89%)] Loss: -1852.197021\n",
      "Train Epoch: 73 [16245/17352 (94%)] Loss: -37735.609375\n",
      "Train Epoch: 73 [17139/17352 (99%)] Loss: -31242.814453\n",
      "    epoch          : 73\n",
      "    loss           : -39563.60876055212\n",
      "    val_loss       : -22009.11805826823\n",
      "Train Epoch: 74 [128/17352 (1%)] Loss: -45947.390625\n",
      "Train Epoch: 74 [1536/17352 (9%)] Loss: -52125.546875\n",
      "Train Epoch: 74 [2944/17352 (17%)] Loss: -32478.187500\n",
      "Train Epoch: 74 [4352/17352 (25%)] Loss: -58979.058594\n",
      "Train Epoch: 74 [5760/17352 (33%)] Loss: -52414.355469\n",
      "Train Epoch: 74 [7168/17352 (41%)] Loss: -74752.015625\n",
      "Train Epoch: 74 [8576/17352 (49%)] Loss: -23108.679688\n",
      "Train Epoch: 74 [9984/17352 (58%)] Loss: -43229.375000\n",
      "Train Epoch: 74 [11392/17352 (66%)] Loss: -54723.425781\n",
      "Train Epoch: 74 [12800/17352 (74%)] Loss: -69521.109375\n",
      "Train Epoch: 74 [14208/17352 (82%)] Loss: -56158.031250\n",
      "Train Epoch: 74 [15515/17352 (89%)] Loss: -5985.602539\n",
      "Train Epoch: 74 [16465/17352 (95%)] Loss: -32151.007812\n",
      "Train Epoch: 74 [17066/17352 (98%)] Loss: -37043.671875\n",
      "    epoch          : 74\n",
      "    loss           : -38702.830253447464\n",
      "    val_loss       : -19816.600504557293\n",
      "Train Epoch: 75 [128/17352 (1%)] Loss: -36225.121094\n",
      "Train Epoch: 75 [1536/17352 (9%)] Loss: -26939.500000\n",
      "Train Epoch: 75 [2944/17352 (17%)] Loss: -34098.492188\n",
      "Train Epoch: 75 [4352/17352 (25%)] Loss: -49889.507812\n",
      "Train Epoch: 75 [5760/17352 (33%)] Loss: -51715.820312\n",
      "Train Epoch: 75 [7168/17352 (41%)] Loss: -58980.617188\n",
      "Train Epoch: 75 [8576/17352 (49%)] Loss: -43805.343750\n",
      "Train Epoch: 75 [9984/17352 (58%)] Loss: -59725.609375\n",
      "Train Epoch: 75 [11392/17352 (66%)] Loss: -41069.898438\n",
      "Train Epoch: 75 [12800/17352 (74%)] Loss: -27156.234375\n",
      "Train Epoch: 75 [14208/17352 (82%)] Loss: -30744.414062\n",
      "Train Epoch: 75 [15484/17352 (89%)] Loss: -13599.541016\n",
      "Train Epoch: 75 [16341/17352 (94%)] Loss: -4887.111328\n",
      "Train Epoch: 75 [17028/17352 (98%)] Loss: -852.074463\n",
      "    epoch          : 75\n",
      "    loss           : -36725.56631940803\n",
      "    val_loss       : -20744.071736653645\n",
      "Train Epoch: 76 [128/17352 (1%)] Loss: -21639.923828\n",
      "Train Epoch: 76 [1536/17352 (9%)] Loss: -56426.757812\n",
      "Train Epoch: 76 [2944/17352 (17%)] Loss: -49619.828125\n",
      "Train Epoch: 76 [4352/17352 (25%)] Loss: -56322.789062\n",
      "Train Epoch: 76 [5760/17352 (33%)] Loss: -39550.796875\n",
      "Train Epoch: 76 [7168/17352 (41%)] Loss: -51394.460938\n",
      "Train Epoch: 76 [8576/17352 (49%)] Loss: -1773.745117\n",
      "Train Epoch: 76 [9984/17352 (58%)] Loss: -46644.746094\n",
      "Train Epoch: 76 [11392/17352 (66%)] Loss: -59329.214844\n",
      "Train Epoch: 76 [12800/17352 (74%)] Loss: -50208.054688\n",
      "Train Epoch: 76 [14208/17352 (82%)] Loss: -29395.615234\n",
      "Train Epoch: 76 [15461/17352 (89%)] Loss: -40334.539062\n",
      "Train Epoch: 76 [16081/17352 (93%)] Loss: -10666.056641\n",
      "Train Epoch: 76 [16882/17352 (97%)] Loss: -16111.458984\n",
      "    epoch          : 76\n",
      "    loss           : -35015.28830058462\n",
      "    val_loss       : -18695.046427408855\n",
      "Train Epoch: 77 [128/17352 (1%)] Loss: -52160.281250\n",
      "Train Epoch: 77 [1536/17352 (9%)] Loss: -60995.148438\n",
      "Train Epoch: 77 [2944/17352 (17%)] Loss: -51569.191406\n",
      "Train Epoch: 77 [4352/17352 (25%)] Loss: -67166.195312\n",
      "Train Epoch: 77 [5760/17352 (33%)] Loss: -45172.609375\n",
      "Train Epoch: 77 [7168/17352 (41%)] Loss: -43483.984375\n",
      "Train Epoch: 77 [8576/17352 (49%)] Loss: -26946.789062\n",
      "Train Epoch: 77 [9984/17352 (58%)] Loss: -52828.917969\n",
      "Train Epoch: 77 [11392/17352 (66%)] Loss: -36795.613281\n",
      "Train Epoch: 77 [12800/17352 (74%)] Loss: -50424.523438\n",
      "Train Epoch: 77 [14208/17352 (82%)] Loss: -44677.324219\n",
      "Train Epoch: 77 [15525/17352 (89%)] Loss: -26162.992188\n",
      "Train Epoch: 77 [16405/17352 (95%)] Loss: -29749.556641\n",
      "Train Epoch: 77 [17032/17352 (98%)] Loss: -8693.749023\n",
      "    epoch          : 77\n",
      "    loss           : -34940.92330789246\n",
      "    val_loss       : -17737.473706054687\n",
      "Train Epoch: 78 [128/17352 (1%)] Loss: -25950.250000\n",
      "Train Epoch: 78 [1536/17352 (9%)] Loss: -33701.835938\n",
      "Train Epoch: 78 [2944/17352 (17%)] Loss: -31094.724609\n",
      "Train Epoch: 78 [4352/17352 (25%)] Loss: -5236.002930\n",
      "Train Epoch: 78 [5760/17352 (33%)] Loss: -20559.156250\n",
      "Train Epoch: 78 [7168/17352 (41%)] Loss: -48834.210938\n",
      "Train Epoch: 78 [8576/17352 (49%)] Loss: -20195.062500\n",
      "Train Epoch: 78 [9984/17352 (58%)] Loss: -39027.468750\n",
      "Train Epoch: 78 [11392/17352 (66%)] Loss: -24930.757812\n",
      "Train Epoch: 78 [12800/17352 (74%)] Loss: -37521.113281\n",
      "Train Epoch: 78 [14208/17352 (82%)] Loss: -32470.564453\n",
      "Train Epoch: 78 [15518/17352 (89%)] Loss: -17292.921875\n",
      "Train Epoch: 78 [16394/17352 (94%)] Loss: -1595.393799\n",
      "Train Epoch: 78 [17016/17352 (98%)] Loss: -28502.656250\n",
      "    epoch          : 78\n",
      "    loss           : -32121.140182597526\n",
      "    val_loss       : -16338.360774739584\n",
      "Train Epoch: 79 [128/17352 (1%)] Loss: -30767.972656\n",
      "Train Epoch: 79 [1536/17352 (9%)] Loss: -30208.414062\n",
      "Train Epoch: 79 [2944/17352 (17%)] Loss: -17595.855469\n",
      "Train Epoch: 79 [4352/17352 (25%)] Loss: -24359.076172\n",
      "Train Epoch: 79 [5760/17352 (33%)] Loss: -22664.056641\n",
      "Train Epoch: 79 [7168/17352 (41%)] Loss: -24413.640625\n",
      "Train Epoch: 79 [8576/17352 (49%)] Loss: -24021.347656\n",
      "Train Epoch: 79 [9984/17352 (58%)] Loss: -42448.859375\n",
      "Train Epoch: 79 [11392/17352 (66%)] Loss: -39745.390625\n",
      "Train Epoch: 79 [12800/17352 (74%)] Loss: -16233.413086\n",
      "Train Epoch: 79 [14208/17352 (82%)] Loss: -42350.312500\n",
      "Train Epoch: 79 [15496/17352 (89%)] Loss: -15663.226562\n",
      "Train Epoch: 79 [16266/17352 (94%)] Loss: -4347.974609\n",
      "Train Epoch: 79 [17055/17352 (98%)] Loss: -6265.333008\n",
      "    epoch          : 79\n",
      "    loss           : -30252.78480642434\n",
      "    val_loss       : -15708.746297200521\n",
      "Train Epoch: 80 [128/17352 (1%)] Loss: -22244.712891\n",
      "Train Epoch: 80 [1536/17352 (9%)] Loss: -20543.386719\n",
      "Train Epoch: 80 [2944/17352 (17%)] Loss: -8803.626953\n",
      "Train Epoch: 80 [4352/17352 (25%)] Loss: -36712.828125\n",
      "Train Epoch: 80 [5760/17352 (33%)] Loss: -34576.257812\n",
      "Train Epoch: 80 [7168/17352 (41%)] Loss: -27516.492188\n",
      "Train Epoch: 80 [8576/17352 (49%)] Loss: -17469.273438\n",
      "Train Epoch: 80 [9984/17352 (58%)] Loss: -11510.773438\n",
      "Train Epoch: 80 [11392/17352 (66%)] Loss: -26558.074219\n",
      "Train Epoch: 80 [12800/17352 (74%)] Loss: -18030.179688\n",
      "Train Epoch: 80 [14208/17352 (82%)] Loss: -29365.064453\n",
      "Train Epoch: 80 [15484/17352 (89%)] Loss: -3412.175293\n",
      "Train Epoch: 80 [16236/17352 (94%)] Loss: -13198.552734\n",
      "Train Epoch: 80 [17029/17352 (98%)] Loss: -25208.277344\n",
      "    epoch          : 80\n",
      "    loss           : -26214.085524591024\n",
      "    val_loss       : -13846.797534179688\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0918_135442/checkpoint-epoch80.pth ...\n",
      "Train Epoch: 81 [128/17352 (1%)] Loss: -19421.718750\n",
      "Train Epoch: 81 [1536/17352 (9%)] Loss: -5323.467773\n",
      "Train Epoch: 81 [2944/17352 (17%)] Loss: -41688.355469\n",
      "Train Epoch: 81 [4352/17352 (25%)] Loss: -41345.367188\n",
      "Train Epoch: 81 [5760/17352 (33%)] Loss: -28488.234375\n",
      "Train Epoch: 81 [7168/17352 (41%)] Loss: -63561.882812\n",
      "Train Epoch: 81 [8576/17352 (49%)] Loss: -41315.429688\n",
      "Train Epoch: 81 [9984/17352 (58%)] Loss: -40613.507812\n",
      "Train Epoch: 81 [11392/17352 (66%)] Loss: -20861.136719\n",
      "Train Epoch: 81 [12800/17352 (74%)] Loss: 725.163330\n",
      "Train Epoch: 81 [14208/17352 (82%)] Loss: -24788.236328\n",
      "Train Epoch: 81 [15474/17352 (89%)] Loss: -4494.163574\n",
      "Train Epoch: 81 [16293/17352 (94%)] Loss: -4306.747559\n",
      "Train Epoch: 81 [17094/17352 (99%)] Loss: -16635.984375\n",
      "    epoch          : 81\n",
      "    loss           : -21769.380991071663\n",
      "    val_loss       : -3147.109798177083\n",
      "Train Epoch: 82 [128/17352 (1%)] Loss: -10406.115234\n",
      "Train Epoch: 82 [1536/17352 (9%)] Loss: -2373.504883\n",
      "Train Epoch: 82 [2944/17352 (17%)] Loss: -10519.887695\n",
      "Train Epoch: 82 [4352/17352 (25%)] Loss: -1493.175049\n",
      "Train Epoch: 82 [5760/17352 (33%)] Loss: -14197.319336\n",
      "Train Epoch: 82 [7168/17352 (41%)] Loss: 7984.871094\n",
      "Train Epoch: 82 [8576/17352 (49%)] Loss: -8477.600586\n",
      "Train Epoch: 82 [9984/17352 (58%)] Loss: 16125.983398\n",
      "Train Epoch: 82 [11392/17352 (66%)] Loss: -14012.703125\n",
      "Train Epoch: 82 [12800/17352 (74%)] Loss: -18035.812500\n",
      "Train Epoch: 82 [14208/17352 (82%)] Loss: -11871.570312\n",
      "Train Epoch: 82 [15465/17352 (89%)] Loss: 9702.764648\n",
      "Train Epoch: 82 [16268/17352 (94%)] Loss: 5439.978516\n",
      "Train Epoch: 82 [16982/17352 (98%)] Loss: -10698.900391\n",
      "    epoch          : 82\n",
      "    loss           : -5286.554724981321\n",
      "    val_loss       : -2336.589046223958\n",
      "Train Epoch: 83 [128/17352 (1%)] Loss: 6865.221680\n",
      "Train Epoch: 83 [1536/17352 (9%)] Loss: 2940.193359\n",
      "Train Epoch: 83 [2944/17352 (17%)] Loss: 11496.909180\n",
      "Train Epoch: 83 [4352/17352 (25%)] Loss: 9225.217773\n",
      "Train Epoch: 83 [5760/17352 (33%)] Loss: 2029.973877\n",
      "Train Epoch: 83 [7168/17352 (41%)] Loss: 6039.996094\n",
      "Train Epoch: 83 [8576/17352 (49%)] Loss: -13973.804688\n",
      "Train Epoch: 83 [9984/17352 (58%)] Loss: -519.306519\n",
      "Train Epoch: 83 [11392/17352 (66%)] Loss: -17422.792969\n",
      "Train Epoch: 83 [12800/17352 (74%)] Loss: 829.352661\n",
      "Train Epoch: 83 [14208/17352 (82%)] Loss: -12848.652344\n",
      "Train Epoch: 83 [15479/17352 (89%)] Loss: -6430.198730\n",
      "Train Epoch: 83 [16198/17352 (93%)] Loss: -1198.819824\n",
      "Train Epoch: 83 [17045/17352 (98%)] Loss: 4259.451172\n",
      "    epoch          : 83\n",
      "    loss           : -3877.108080358313\n",
      "    val_loss       : -1817.9013020833333\n",
      "Train Epoch: 84 [128/17352 (1%)] Loss: -3529.470459\n",
      "Train Epoch: 84 [1536/17352 (9%)] Loss: 12121.578125\n",
      "Train Epoch: 84 [2944/17352 (17%)] Loss: -11918.605469\n",
      "Train Epoch: 84 [4352/17352 (25%)] Loss: -22072.228516\n",
      "Train Epoch: 84 [5760/17352 (33%)] Loss: -8278.726562\n",
      "Train Epoch: 84 [7168/17352 (41%)] Loss: -24371.732422\n",
      "Train Epoch: 84 [8576/17352 (49%)] Loss: 3808.100586\n",
      "Train Epoch: 84 [9984/17352 (58%)] Loss: -1886.465820\n",
      "Train Epoch: 84 [11392/17352 (66%)] Loss: 6993.911133\n",
      "Train Epoch: 84 [12800/17352 (74%)] Loss: -12187.009766\n",
      "Train Epoch: 84 [14208/17352 (82%)] Loss: 2943.538574\n",
      "Train Epoch: 84 [15515/17352 (89%)] Loss: -3279.616943\n",
      "Train Epoch: 84 [16269/17352 (94%)] Loss: -3815.485352\n",
      "Train Epoch: 84 [17030/17352 (98%)] Loss: -4993.532227\n",
      "    epoch          : 84\n",
      "    loss           : -4824.364339329252\n",
      "    val_loss       : -3517.512239583333\n",
      "Train Epoch: 85 [128/17352 (1%)] Loss: -8764.462891\n",
      "Train Epoch: 85 [1536/17352 (9%)] Loss: -13869.431641\n",
      "Train Epoch: 85 [2944/17352 (17%)] Loss: 26011.044922\n",
      "Train Epoch: 85 [4352/17352 (25%)] Loss: 11889.232422\n",
      "Train Epoch: 85 [5760/17352 (33%)] Loss: -36341.609375\n",
      "Train Epoch: 85 [7168/17352 (41%)] Loss: -15444.300781\n",
      "Train Epoch: 85 [8576/17352 (49%)] Loss: -10897.418945\n",
      "Train Epoch: 85 [9984/17352 (58%)] Loss: 10713.853516\n",
      "Train Epoch: 85 [11392/17352 (66%)] Loss: -17058.908203\n",
      "Train Epoch: 85 [12800/17352 (74%)] Loss: -16158.220703\n",
      "Train Epoch: 85 [14208/17352 (82%)] Loss: -17458.123047\n",
      "Train Epoch: 85 [15447/17352 (89%)] Loss: -1383.233398\n",
      "Train Epoch: 85 [16246/17352 (94%)] Loss: -595.794189\n",
      "Train Epoch: 85 [17067/17352 (98%)] Loss: 5315.769043\n",
      "    epoch          : 85\n",
      "    loss           : -5398.379839801149\n",
      "    val_loss       : -3403.826236979167\n",
      "Train Epoch: 86 [128/17352 (1%)] Loss: -14179.987305\n",
      "Train Epoch: 86 [1536/17352 (9%)] Loss: -9736.933594\n",
      "Train Epoch: 86 [2944/17352 (17%)] Loss: -21327.484375\n",
      "Train Epoch: 86 [4352/17352 (25%)] Loss: 5640.850586\n",
      "Train Epoch: 86 [5760/17352 (33%)] Loss: 5119.940918\n",
      "Train Epoch: 86 [7168/17352 (41%)] Loss: 10880.533203\n",
      "Train Epoch: 86 [8576/17352 (49%)] Loss: 3592.219727\n",
      "Train Epoch: 86 [9984/17352 (58%)] Loss: -18559.933594\n",
      "Train Epoch: 86 [11392/17352 (66%)] Loss: -11945.404297\n",
      "Train Epoch: 86 [12800/17352 (74%)] Loss: -9876.472656\n",
      "Train Epoch: 86 [14208/17352 (82%)] Loss: -15456.392578\n",
      "Train Epoch: 86 [15564/17352 (90%)] Loss: -15068.421875\n",
      "Train Epoch: 86 [16216/17352 (93%)] Loss: 3704.934082\n",
      "Train Epoch: 86 [17051/17352 (98%)] Loss: -12960.333984\n",
      "    epoch          : 86\n",
      "    loss           : -6537.334104397153\n",
      "    val_loss       : -2706.4887858072916\n",
      "Train Epoch: 87 [128/17352 (1%)] Loss: 8799.687500\n",
      "Train Epoch: 87 [1536/17352 (9%)] Loss: -19756.175781\n",
      "Train Epoch: 87 [2944/17352 (17%)] Loss: 12767.572266\n",
      "Train Epoch: 87 [4352/17352 (25%)] Loss: -25578.505859\n",
      "Train Epoch: 87 [5760/17352 (33%)] Loss: -8143.799805\n",
      "Train Epoch: 87 [7168/17352 (41%)] Loss: -52878.378906\n",
      "Train Epoch: 87 [8576/17352 (49%)] Loss: -1138.833862\n",
      "Train Epoch: 87 [9984/17352 (58%)] Loss: -7124.117188\n",
      "Train Epoch: 87 [11392/17352 (66%)] Loss: 21589.185547\n",
      "Train Epoch: 87 [12800/17352 (74%)] Loss: 13145.109375\n",
      "Train Epoch: 87 [14208/17352 (82%)] Loss: -13051.986328\n",
      "Train Epoch: 87 [15436/17352 (89%)] Loss: -1567.571289\n",
      "Train Epoch: 87 [16110/17352 (93%)] Loss: -14131.420898\n",
      "Train Epoch: 87 [16952/17352 (98%)] Loss: -11212.446289\n",
      "    epoch          : 87\n",
      "    loss           : -7134.723057074835\n",
      "    val_loss       : -3463.17197265625\n",
      "Train Epoch: 88 [128/17352 (1%)] Loss: 18311.392578\n",
      "Train Epoch: 88 [1536/17352 (9%)] Loss: 2400.931152\n",
      "Train Epoch: 88 [2944/17352 (17%)] Loss: 10184.162109\n",
      "Train Epoch: 88 [4352/17352 (25%)] Loss: -11323.285156\n",
      "Train Epoch: 88 [5760/17352 (33%)] Loss: -40938.117188\n",
      "Train Epoch: 88 [7168/17352 (41%)] Loss: -1002.401367\n",
      "Train Epoch: 88 [8576/17352 (49%)] Loss: 3130.125000\n",
      "Train Epoch: 88 [9984/17352 (58%)] Loss: -2975.789307\n",
      "Train Epoch: 88 [11392/17352 (66%)] Loss: -19904.283203\n",
      "Train Epoch: 88 [12800/17352 (74%)] Loss: -5351.593262\n",
      "Train Epoch: 88 [14208/17352 (82%)] Loss: -29179.716797\n",
      "Train Epoch: 88 [15531/17352 (90%)] Loss: -1133.147095\n",
      "Train Epoch: 88 [16293/17352 (94%)] Loss: -9865.462891\n",
      "Train Epoch: 88 [17094/17352 (99%)] Loss: -1801.435059\n",
      "    epoch          : 88\n",
      "    loss           : -9923.238374646078\n",
      "    val_loss       : -5835.222770182291\n",
      "Train Epoch: 89 [128/17352 (1%)] Loss: -5051.364258\n",
      "Train Epoch: 89 [1536/17352 (9%)] Loss: -31158.367188\n",
      "Train Epoch: 89 [2944/17352 (17%)] Loss: -8467.039062\n",
      "Train Epoch: 89 [4352/17352 (25%)] Loss: -22005.617188\n",
      "Train Epoch: 89 [5760/17352 (33%)] Loss: -7949.517578\n",
      "Train Epoch: 89 [7168/17352 (41%)] Loss: -6676.030762\n",
      "Train Epoch: 89 [8576/17352 (49%)] Loss: -30071.460938\n",
      "Train Epoch: 89 [9984/17352 (58%)] Loss: -13432.527344\n",
      "Train Epoch: 89 [11392/17352 (66%)] Loss: -25894.789062\n",
      "Train Epoch: 89 [12800/17352 (74%)] Loss: 672.552490\n",
      "Train Epoch: 89 [14208/17352 (82%)] Loss: -23222.972656\n",
      "Train Epoch: 89 [15504/17352 (89%)] Loss: -10459.863281\n",
      "Train Epoch: 89 [16303/17352 (94%)] Loss: 5679.641113\n",
      "Train Epoch: 89 [17017/17352 (98%)] Loss: 367.916809\n",
      "    epoch          : 89\n",
      "    loss           : -12239.619098842544\n",
      "    val_loss       : -7842.7494140625\n",
      "Train Epoch: 90 [128/17352 (1%)] Loss: -4192.221680\n",
      "Train Epoch: 90 [1536/17352 (9%)] Loss: -22450.570312\n",
      "Train Epoch: 90 [2944/17352 (17%)] Loss: 3349.047852\n",
      "Train Epoch: 90 [4352/17352 (25%)] Loss: -41122.937500\n",
      "Train Epoch: 90 [5760/17352 (33%)] Loss: -40058.710938\n",
      "Train Epoch: 90 [7168/17352 (41%)] Loss: -42023.277344\n",
      "Train Epoch: 90 [8576/17352 (49%)] Loss: -22347.224609\n",
      "Train Epoch: 90 [9984/17352 (58%)] Loss: -38524.925781\n",
      "Train Epoch: 90 [11392/17352 (66%)] Loss: -30083.109375\n",
      "Train Epoch: 90 [12800/17352 (74%)] Loss: -12266.939453\n",
      "Train Epoch: 90 [14208/17352 (82%)] Loss: -58792.812500\n",
      "Train Epoch: 90 [15465/17352 (89%)] Loss: -13020.400391\n",
      "Train Epoch: 90 [16123/17352 (93%)] Loss: -21430.062500\n",
      "Train Epoch: 90 [16994/17352 (98%)] Loss: -21295.355469\n",
      "    epoch          : 90\n",
      "    loss           : -22935.592838159344\n",
      "    val_loss       : -13096.17353515625\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0918_135442/checkpoint-epoch90.pth ...\n",
      "Train Epoch: 91 [128/17352 (1%)] Loss: -16206.398438\n",
      "Train Epoch: 91 [1536/17352 (9%)] Loss: -22327.009766\n",
      "Train Epoch: 91 [2944/17352 (17%)] Loss: -27943.277344\n",
      "Train Epoch: 91 [4352/17352 (25%)] Loss: -19382.542969\n",
      "Train Epoch: 91 [5760/17352 (33%)] Loss: -16791.261719\n",
      "Train Epoch: 91 [7168/17352 (41%)] Loss: -25367.167969\n",
      "Train Epoch: 91 [8576/17352 (49%)] Loss: -34503.953125\n",
      "Train Epoch: 91 [9984/17352 (58%)] Loss: -27597.681641\n",
      "Train Epoch: 91 [11392/17352 (66%)] Loss: -43942.167969\n",
      "Train Epoch: 91 [12800/17352 (74%)] Loss: 3273.607178\n",
      "Train Epoch: 91 [14208/17352 (82%)] Loss: -36426.113281\n",
      "Train Epoch: 91 [15435/17352 (89%)] Loss: -16443.513672\n",
      "Train Epoch: 91 [16235/17352 (94%)] Loss: -26632.039062\n",
      "Train Epoch: 91 [17056/17352 (98%)] Loss: -30546.324219\n",
      "    epoch          : 91\n",
      "    loss           : -25285.49808742856\n",
      "    val_loss       : -13840.367610677084\n",
      "Train Epoch: 92 [128/17352 (1%)] Loss: -6991.526855\n",
      "Train Epoch: 92 [1536/17352 (9%)] Loss: -42978.605469\n",
      "Train Epoch: 92 [2944/17352 (17%)] Loss: -28766.201172\n",
      "Train Epoch: 92 [4352/17352 (25%)] Loss: -11541.899414\n",
      "Train Epoch: 92 [5760/17352 (33%)] Loss: -6320.103516\n",
      "Train Epoch: 92 [7168/17352 (41%)] Loss: -10273.583984\n",
      "Train Epoch: 92 [8576/17352 (49%)] Loss: -42460.597656\n",
      "Train Epoch: 92 [9984/17352 (58%)] Loss: -21003.718750\n",
      "Train Epoch: 92 [11392/17352 (66%)] Loss: -52647.234375\n",
      "Train Epoch: 92 [12800/17352 (74%)] Loss: -4400.463867\n",
      "Train Epoch: 92 [14208/17352 (82%)] Loss: -24345.468750\n",
      "Train Epoch: 92 [15452/17352 (89%)] Loss: -19219.085938\n",
      "Train Epoch: 92 [16321/17352 (94%)] Loss: -9377.773438\n",
      "Train Epoch: 92 [17051/17352 (98%)] Loss: -23856.228516\n",
      "    epoch          : 92\n",
      "    loss           : -24152.968891323013\n",
      "    val_loss       : -14015.193676757812\n",
      "Train Epoch: 93 [128/17352 (1%)] Loss: -16741.828125\n",
      "Train Epoch: 93 [1536/17352 (9%)] Loss: -8232.071289\n",
      "Train Epoch: 93 [2944/17352 (17%)] Loss: -4474.081055\n",
      "Train Epoch: 93 [4352/17352 (25%)] Loss: 3762.002930\n",
      "Train Epoch: 93 [5760/17352 (33%)] Loss: -21665.546875\n",
      "Train Epoch: 93 [7168/17352 (41%)] Loss: -1961.796875\n",
      "Train Epoch: 93 [8576/17352 (49%)] Loss: -19317.416016\n",
      "Train Epoch: 93 [9984/17352 (58%)] Loss: -18542.011719\n",
      "Train Epoch: 93 [11392/17352 (66%)] Loss: -33770.398438\n",
      "Train Epoch: 93 [12800/17352 (74%)] Loss: -20645.257812\n",
      "Train Epoch: 93 [14208/17352 (82%)] Loss: -15244.075195\n",
      "Train Epoch: 93 [15510/17352 (89%)] Loss: -12149.229492\n",
      "Train Epoch: 93 [16213/17352 (93%)] Loss: -32919.695312\n",
      "Train Epoch: 93 [16952/17352 (98%)] Loss: -483.653564\n",
      "    epoch          : 93\n",
      "    loss           : -22294.6942294332\n",
      "    val_loss       : -12936.993180338543\n",
      "Train Epoch: 94 [128/17352 (1%)] Loss: -24955.402344\n",
      "Train Epoch: 94 [1536/17352 (9%)] Loss: -44319.500000\n",
      "Train Epoch: 94 [2944/17352 (17%)] Loss: -27649.597656\n",
      "Train Epoch: 94 [4352/17352 (25%)] Loss: 8797.851562\n",
      "Train Epoch: 94 [5760/17352 (33%)] Loss: -18980.378906\n",
      "Train Epoch: 94 [7168/17352 (41%)] Loss: -4063.131836\n",
      "Train Epoch: 94 [8576/17352 (49%)] Loss: -23273.017578\n",
      "Train Epoch: 94 [9984/17352 (58%)] Loss: 15838.530273\n",
      "Train Epoch: 94 [11392/17352 (66%)] Loss: -15824.173828\n",
      "Train Epoch: 94 [12800/17352 (74%)] Loss: -1492.439453\n",
      "Train Epoch: 94 [14208/17352 (82%)] Loss: -18653.972656\n",
      "Train Epoch: 94 [15448/17352 (89%)] Loss: -7888.432617\n",
      "Train Epoch: 94 [16284/17352 (94%)] Loss: -15432.636719\n",
      "Train Epoch: 94 [17087/17352 (98%)] Loss: -17555.044922\n",
      "    epoch          : 94\n",
      "    loss           : -21196.48502713402\n",
      "    val_loss       : -14394.58466796875\n",
      "Train Epoch: 95 [128/17352 (1%)] Loss: -8246.969727\n",
      "Train Epoch: 95 [1536/17352 (9%)] Loss: -22928.097656\n",
      "Train Epoch: 95 [2944/17352 (17%)] Loss: -19143.609375\n",
      "Train Epoch: 95 [4352/17352 (25%)] Loss: -24767.972656\n",
      "Train Epoch: 95 [5760/17352 (33%)] Loss: -31407.185547\n",
      "Train Epoch: 95 [7168/17352 (41%)] Loss: -23693.300781\n",
      "Train Epoch: 95 [8576/17352 (49%)] Loss: -28714.125000\n",
      "Train Epoch: 95 [9984/17352 (58%)] Loss: -22893.101562\n",
      "Train Epoch: 95 [11392/17352 (66%)] Loss: -29417.570312\n",
      "Train Epoch: 95 [12800/17352 (74%)] Loss: -60494.359375\n",
      "Train Epoch: 95 [14208/17352 (82%)] Loss: -27670.171875\n",
      "Train Epoch: 95 [15563/17352 (90%)] Loss: -11958.941406\n",
      "Train Epoch: 95 [16248/17352 (94%)] Loss: -44850.820312\n",
      "Train Epoch: 95 [16942/17352 (98%)] Loss: -29053.597656\n",
      "    epoch          : 95\n",
      "    loss           : -24278.553313594537\n",
      "    val_loss       : -11724.250423177084\n",
      "Train Epoch: 96 [128/17352 (1%)] Loss: -41889.453125\n",
      "Train Epoch: 96 [1536/17352 (9%)] Loss: -28811.337891\n",
      "Train Epoch: 96 [2944/17352 (17%)] Loss: -19440.414062\n",
      "Train Epoch: 96 [4352/17352 (25%)] Loss: -27617.261719\n",
      "Train Epoch: 96 [5760/17352 (33%)] Loss: -14169.285156\n",
      "Train Epoch: 96 [7168/17352 (41%)] Loss: -32351.433594\n",
      "Train Epoch: 96 [8576/17352 (49%)] Loss: -56611.039062\n",
      "Train Epoch: 96 [9984/17352 (58%)] Loss: -35518.457031\n",
      "Train Epoch: 96 [11392/17352 (66%)] Loss: 1471.331055\n",
      "Train Epoch: 96 [12800/17352 (74%)] Loss: -30688.343750\n",
      "Train Epoch: 96 [14208/17352 (82%)] Loss: -42696.312500\n",
      "Train Epoch: 96 [15539/17352 (90%)] Loss: -41559.609375\n",
      "Train Epoch: 96 [16337/17352 (94%)] Loss: -27645.291016\n",
      "Train Epoch: 96 [17037/17352 (98%)] Loss: -41587.214844\n",
      "    epoch          : 96\n",
      "    loss           : -26710.50478593455\n",
      "    val_loss       : -15638.313696289062\n",
      "Train Epoch: 97 [128/17352 (1%)] Loss: -42208.503906\n",
      "Train Epoch: 97 [1536/17352 (9%)] Loss: -50867.375000\n",
      "Train Epoch: 97 [2944/17352 (17%)] Loss: -48555.238281\n",
      "Train Epoch: 97 [4352/17352 (25%)] Loss: -30731.041016\n",
      "Train Epoch: 97 [5760/17352 (33%)] Loss: -14869.233398\n",
      "Train Epoch: 97 [7168/17352 (41%)] Loss: -33220.601562\n",
      "Train Epoch: 97 [8576/17352 (49%)] Loss: -40715.125000\n",
      "Train Epoch: 97 [9984/17352 (58%)] Loss: -31358.332031\n",
      "Train Epoch: 97 [11392/17352 (66%)] Loss: -35658.851562\n",
      "Train Epoch: 97 [12800/17352 (74%)] Loss: -27701.695312\n",
      "Train Epoch: 97 [14208/17352 (82%)] Loss: -21111.712891\n",
      "Train Epoch: 97 [15596/17352 (90%)] Loss: -32174.894531\n",
      "Train Epoch: 97 [16220/17352 (93%)] Loss: -22765.517578\n",
      "Train Epoch: 97 [17025/17352 (98%)] Loss: -23592.886719\n",
      "    epoch          : 97\n",
      "    loss           : -30951.329946223523\n",
      "    val_loss       : -16231.101285807292\n",
      "Train Epoch: 98 [128/17352 (1%)] Loss: -14477.685547\n",
      "Train Epoch: 98 [1536/17352 (9%)] Loss: -30878.628906\n",
      "Train Epoch: 98 [2944/17352 (17%)] Loss: -40609.589844\n",
      "Train Epoch: 98 [4352/17352 (25%)] Loss: -22675.355469\n",
      "Train Epoch: 98 [5760/17352 (33%)] Loss: -58149.507812\n",
      "Train Epoch: 98 [7168/17352 (41%)] Loss: -33969.125000\n",
      "Train Epoch: 98 [8576/17352 (49%)] Loss: -50080.093750\n",
      "Train Epoch: 98 [9984/17352 (58%)] Loss: -46909.367188\n",
      "Train Epoch: 98 [11392/17352 (66%)] Loss: -28832.876953\n",
      "Train Epoch: 98 [12800/17352 (74%)] Loss: -15359.309570\n",
      "Train Epoch: 98 [14208/17352 (82%)] Loss: -52526.101562\n",
      "Train Epoch: 98 [15521/17352 (89%)] Loss: -9740.821289\n",
      "Train Epoch: 98 [16432/17352 (95%)] Loss: -33445.058594\n",
      "Train Epoch: 98 [17054/17352 (98%)] Loss: -25214.742188\n",
      "    epoch          : 98\n",
      "    loss           : -32813.141852359644\n",
      "    val_loss       : -19227.929296875\n",
      "Train Epoch: 99 [128/17352 (1%)] Loss: -17193.902344\n",
      "Train Epoch: 99 [1536/17352 (9%)] Loss: -31621.621094\n",
      "Train Epoch: 99 [2944/17352 (17%)] Loss: -56993.046875\n",
      "Train Epoch: 99 [4352/17352 (25%)] Loss: -48025.562500\n",
      "Train Epoch: 99 [5760/17352 (33%)] Loss: -55510.160156\n",
      "Train Epoch: 99 [7168/17352 (41%)] Loss: -61929.218750\n",
      "Train Epoch: 99 [8576/17352 (49%)] Loss: -42531.402344\n",
      "Train Epoch: 99 [9984/17352 (58%)] Loss: -35021.054688\n",
      "Train Epoch: 99 [11392/17352 (66%)] Loss: -25078.945312\n",
      "Train Epoch: 99 [12800/17352 (74%)] Loss: -28270.640625\n",
      "Train Epoch: 99 [14208/17352 (82%)] Loss: -52781.929688\n",
      "Train Epoch: 99 [15428/17352 (89%)] Loss: -11302.000000\n",
      "Train Epoch: 99 [16189/17352 (93%)] Loss: -16535.382812\n",
      "Train Epoch: 99 [16988/17352 (98%)] Loss: -23819.246094\n",
      "    epoch          : 99\n",
      "    loss           : -31597.25398899565\n",
      "    val_loss       : -18363.92918294271\n",
      "Train Epoch: 100 [128/17352 (1%)] Loss: -21974.873047\n",
      "Train Epoch: 100 [1536/17352 (9%)] Loss: -32804.523438\n",
      "Train Epoch: 100 [2944/17352 (17%)] Loss: -16037.545898\n",
      "Train Epoch: 100 [4352/17352 (25%)] Loss: -48802.875000\n",
      "Train Epoch: 100 [5760/17352 (33%)] Loss: -37159.324219\n",
      "Train Epoch: 100 [7168/17352 (41%)] Loss: -37917.632812\n",
      "Train Epoch: 100 [8576/17352 (49%)] Loss: -36146.632812\n",
      "Train Epoch: 100 [9984/17352 (58%)] Loss: -61053.617188\n",
      "Train Epoch: 100 [11392/17352 (66%)] Loss: -33744.234375\n",
      "Train Epoch: 100 [12800/17352 (74%)] Loss: -39062.585938\n",
      "Train Epoch: 100 [14208/17352 (82%)] Loss: -48889.941406\n",
      "Train Epoch: 100 [15458/17352 (89%)] Loss: -11012.644531\n",
      "Train Epoch: 100 [16227/17352 (94%)] Loss: -1215.065308\n",
      "Train Epoch: 100 [16999/17352 (98%)] Loss: -780.820312\n",
      "    epoch          : 100\n",
      "    loss           : -33421.05469159632\n",
      "    val_loss       : -18698.519986979165\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0918_135442/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [128/17352 (1%)] Loss: -24629.068359\n",
      "Train Epoch: 101 [1536/17352 (9%)] Loss: -47807.054688\n",
      "Train Epoch: 101 [2944/17352 (17%)] Loss: -42595.343750\n",
      "Train Epoch: 101 [4352/17352 (25%)] Loss: -53808.628906\n",
      "Train Epoch: 101 [5760/17352 (33%)] Loss: -53744.738281\n",
      "Train Epoch: 101 [7168/17352 (41%)] Loss: -41630.742188\n",
      "Train Epoch: 101 [8576/17352 (49%)] Loss: -40222.117188\n",
      "Train Epoch: 101 [9984/17352 (58%)] Loss: -33534.832031\n",
      "Train Epoch: 101 [11392/17352 (66%)] Loss: -51819.152344\n",
      "Train Epoch: 101 [12800/17352 (74%)] Loss: -25841.765625\n",
      "Train Epoch: 101 [14208/17352 (82%)] Loss: -51970.378906\n",
      "Train Epoch: 101 [15533/17352 (90%)] Loss: -18407.638672\n",
      "Train Epoch: 101 [16330/17352 (94%)] Loss: -20508.755859\n",
      "Train Epoch: 101 [16918/17352 (97%)] Loss: -23843.146484\n",
      "    epoch          : 101\n",
      "    loss           : -37566.94943421639\n",
      "    val_loss       : -20041.200130208334\n",
      "Train Epoch: 102 [128/17352 (1%)] Loss: -28465.347656\n",
      "Train Epoch: 102 [1536/17352 (9%)] Loss: -40645.480469\n",
      "Train Epoch: 102 [2944/17352 (17%)] Loss: -44106.101562\n",
      "Train Epoch: 102 [4352/17352 (25%)] Loss: -19797.605469\n",
      "Train Epoch: 102 [5760/17352 (33%)] Loss: -36371.964844\n",
      "Train Epoch: 102 [7168/17352 (41%)] Loss: -48687.585938\n",
      "Train Epoch: 102 [8576/17352 (49%)] Loss: -62028.054688\n",
      "Train Epoch: 102 [9984/17352 (58%)] Loss: -47264.953125\n",
      "Train Epoch: 102 [11392/17352 (66%)] Loss: -49657.808594\n",
      "Train Epoch: 102 [12800/17352 (74%)] Loss: -53786.355469\n",
      "Train Epoch: 102 [14208/17352 (82%)] Loss: -49002.265625\n",
      "Train Epoch: 102 [15397/17352 (89%)] Loss: -9743.571289\n",
      "Train Epoch: 102 [16311/17352 (94%)] Loss: -17837.140625\n",
      "Train Epoch: 102 [17020/17352 (98%)] Loss: -15417.189453\n",
      "    epoch          : 102\n",
      "    loss           : -37516.53728756488\n",
      "    val_loss       : -17335.034448242186\n",
      "Train Epoch: 103 [128/17352 (1%)] Loss: -33755.847656\n",
      "Train Epoch: 103 [1536/17352 (9%)] Loss: -51345.445312\n",
      "Train Epoch: 103 [2944/17352 (17%)] Loss: -43682.089844\n",
      "Train Epoch: 103 [4352/17352 (25%)] Loss: -35975.355469\n",
      "Train Epoch: 103 [5760/17352 (33%)] Loss: -25525.595703\n",
      "Train Epoch: 103 [7168/17352 (41%)] Loss: -62979.500000\n",
      "Train Epoch: 103 [8576/17352 (49%)] Loss: -40837.156250\n",
      "Train Epoch: 103 [9984/17352 (58%)] Loss: -27162.570312\n",
      "Train Epoch: 103 [11392/17352 (66%)] Loss: -20451.355469\n",
      "Train Epoch: 103 [12800/17352 (74%)] Loss: -50302.507812\n",
      "Train Epoch: 103 [14208/17352 (82%)] Loss: -44772.785156\n",
      "Train Epoch: 103 [15572/17352 (90%)] Loss: -37630.398438\n",
      "Train Epoch: 103 [16206/17352 (93%)] Loss: -37374.222656\n",
      "Train Epoch: 103 [16997/17352 (98%)] Loss: -16344.449219\n",
      "    epoch          : 103\n",
      "    loss           : -35678.08045580563\n",
      "    val_loss       : -21384.111246744793\n",
      "Train Epoch: 104 [128/17352 (1%)] Loss: -46409.414062\n",
      "Train Epoch: 104 [1536/17352 (9%)] Loss: -36738.972656\n",
      "Train Epoch: 104 [2944/17352 (17%)] Loss: -20811.324219\n",
      "Train Epoch: 104 [4352/17352 (25%)] Loss: -59293.652344\n",
      "Train Epoch: 104 [5760/17352 (33%)] Loss: -37983.832031\n",
      "Train Epoch: 104 [7168/17352 (41%)] Loss: -44526.195312\n",
      "Train Epoch: 104 [8576/17352 (49%)] Loss: -67328.273438\n",
      "Train Epoch: 104 [9984/17352 (58%)] Loss: -47508.531250\n",
      "Train Epoch: 104 [11392/17352 (66%)] Loss: -39953.250000\n",
      "Train Epoch: 104 [12800/17352 (74%)] Loss: -50527.722656\n",
      "Train Epoch: 104 [14208/17352 (82%)] Loss: -59031.394531\n",
      "Train Epoch: 104 [15557/17352 (90%)] Loss: -30877.089844\n",
      "Train Epoch: 104 [16393/17352 (94%)] Loss: -25832.757812\n",
      "Train Epoch: 104 [17089/17352 (98%)] Loss: -4468.123535\n",
      "    epoch          : 104\n",
      "    loss           : -36595.15495023791\n",
      "    val_loss       : -20775.506575520834\n",
      "Train Epoch: 105 [128/17352 (1%)] Loss: -42591.078125\n",
      "Train Epoch: 105 [1536/17352 (9%)] Loss: -17433.738281\n",
      "Train Epoch: 105 [2944/17352 (17%)] Loss: -24610.302734\n",
      "Train Epoch: 105 [4352/17352 (25%)] Loss: -17490.460938\n",
      "Train Epoch: 105 [5760/17352 (33%)] Loss: -66773.992188\n",
      "Train Epoch: 105 [7168/17352 (41%)] Loss: -27278.042969\n",
      "Train Epoch: 105 [8576/17352 (49%)] Loss: -51522.195312\n",
      "Train Epoch: 105 [9984/17352 (58%)] Loss: -32610.531250\n",
      "Train Epoch: 105 [11392/17352 (66%)] Loss: -32815.292969\n",
      "Train Epoch: 105 [12800/17352 (74%)] Loss: -52004.546875\n",
      "Train Epoch: 105 [14208/17352 (82%)] Loss: -31987.609375\n",
      "Train Epoch: 105 [15540/17352 (90%)] Loss: -22636.000000\n",
      "Train Epoch: 105 [16228/17352 (94%)] Loss: -37342.585938\n",
      "Train Epoch: 105 [17072/17352 (98%)] Loss: -27791.851562\n",
      "    epoch          : 105\n",
      "    loss           : -35882.17668407875\n",
      "    val_loss       : -19574.30105794271\n",
      "Train Epoch: 106 [128/17352 (1%)] Loss: -28964.582031\n",
      "Train Epoch: 106 [1536/17352 (9%)] Loss: -54268.437500\n",
      "Train Epoch: 106 [2944/17352 (17%)] Loss: -20420.457031\n",
      "Train Epoch: 106 [4352/17352 (25%)] Loss: -29704.277344\n",
      "Train Epoch: 106 [5760/17352 (33%)] Loss: -72948.328125\n",
      "Train Epoch: 106 [7168/17352 (41%)] Loss: -24993.126953\n",
      "Train Epoch: 106 [8576/17352 (49%)] Loss: -38460.453125\n",
      "Train Epoch: 106 [9984/17352 (58%)] Loss: -36135.082031\n",
      "Train Epoch: 106 [11392/17352 (66%)] Loss: -48447.730469\n",
      "Train Epoch: 106 [12800/17352 (74%)] Loss: -19619.433594\n",
      "Train Epoch: 106 [14208/17352 (82%)] Loss: -39598.683594\n",
      "Train Epoch: 106 [15540/17352 (90%)] Loss: -31029.453125\n",
      "Train Epoch: 106 [16231/17352 (94%)] Loss: -12272.205078\n",
      "Train Epoch: 106 [17026/17352 (98%)] Loss: -20559.738281\n",
      "    epoch          : 106\n",
      "    loss           : -37724.53558882131\n",
      "    val_loss       : -19106.587931315105\n",
      "Validation performance didn't improve for 75 epochs. Training stops.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
