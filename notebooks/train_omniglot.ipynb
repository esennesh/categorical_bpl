{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='omniglot_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 50,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [128/17352 (1%)] Loss: 131152.906250\n",
      "Train Epoch: 1 [1536/17352 (9%)] Loss: -31911.925781\n",
      "Train Epoch: 1 [2944/17352 (17%)] Loss: -9360.845703\n",
      "Train Epoch: 1 [4352/17352 (25%)] Loss: -14636.622070\n",
      "Train Epoch: 1 [5760/17352 (33%)] Loss: -29958.679688\n",
      "Train Epoch: 1 [7168/17352 (41%)] Loss: 249087.093750\n",
      "Train Epoch: 1 [8576/17352 (49%)] Loss: -6460.961914\n",
      "Train Epoch: 1 [9984/17352 (58%)] Loss: -65506.703125\n",
      "Train Epoch: 1 [11392/17352 (66%)] Loss: -42389.761719\n",
      "Train Epoch: 1 [12800/17352 (74%)] Loss: -20819.212891\n",
      "Train Epoch: 1 [14208/17352 (82%)] Loss: -49952.730469\n",
      "Train Epoch: 1 [15409/17352 (89%)] Loss: -9599.809570\n",
      "Train Epoch: 1 [16288/17352 (94%)] Loss: -46485.160156\n",
      "Train Epoch: 1 [16992/17352 (98%)] Loss: 41712.664062\n",
      "    epoch          : 1\n",
      "    loss           : -8559.506116623847\n",
      "    val_loss       : -10346.917751057943\n",
      "Train Epoch: 2 [128/17352 (1%)] Loss: -45351.796875\n",
      "Train Epoch: 2 [1536/17352 (9%)] Loss: -90253.656250\n",
      "Train Epoch: 2 [2944/17352 (17%)] Loss: -66374.734375\n",
      "Train Epoch: 2 [4352/17352 (25%)] Loss: -94707.390625\n",
      "Train Epoch: 2 [5760/17352 (33%)] Loss: -94623.398438\n",
      "Train Epoch: 2 [7168/17352 (41%)] Loss: -59454.703125\n",
      "Train Epoch: 2 [8576/17352 (49%)] Loss: -99763.921875\n",
      "Train Epoch: 2 [9984/17352 (58%)] Loss: -99029.015625\n",
      "Train Epoch: 2 [11392/17352 (66%)] Loss: -96549.406250\n",
      "Train Epoch: 2 [12800/17352 (74%)] Loss: -63591.453125\n",
      "Train Epoch: 2 [14208/17352 (82%)] Loss: -77994.578125\n",
      "Train Epoch: 2 [15527/17352 (89%)] Loss: -49124.027344\n",
      "Train Epoch: 2 [16413/17352 (95%)] Loss: -31999.269531\n",
      "Train Epoch: 2 [17068/17352 (98%)] Loss: -83954.242188\n",
      "    epoch          : 2\n",
      "    loss           : -51163.041335137896\n",
      "    val_loss       : -28662.722952651977\n",
      "Train Epoch: 3 [128/17352 (1%)] Loss: -121890.289062\n",
      "Train Epoch: 3 [1536/17352 (9%)] Loss: -128311.445312\n",
      "Train Epoch: 3 [2944/17352 (17%)] Loss: -77871.242188\n",
      "Train Epoch: 3 [4352/17352 (25%)] Loss: -133281.531250\n",
      "Train Epoch: 3 [5760/17352 (33%)] Loss: -93115.656250\n",
      "Train Epoch: 3 [7168/17352 (41%)] Loss: -95949.632812\n",
      "Train Epoch: 3 [8576/17352 (49%)] Loss: -65486.390625\n",
      "Train Epoch: 3 [9984/17352 (58%)] Loss: -100267.617188\n",
      "Train Epoch: 3 [11392/17352 (66%)] Loss: -124138.375000\n",
      "Train Epoch: 3 [12800/17352 (74%)] Loss: 72676.570312\n",
      "Train Epoch: 3 [14208/17352 (82%)] Loss: -141284.703125\n",
      "Train Epoch: 3 [15486/17352 (89%)] Loss: -67618.125000\n",
      "Train Epoch: 3 [16121/17352 (93%)] Loss: -100046.882812\n",
      "Train Epoch: 3 [17070/17352 (98%)] Loss: -77833.273438\n",
      "    epoch          : 3\n",
      "    loss           : -71385.55160215237\n",
      "    val_loss       : -44383.4581509908\n",
      "Train Epoch: 4 [128/17352 (1%)] Loss: 27415.914062\n",
      "Train Epoch: 4 [1536/17352 (9%)] Loss: -95357.210938\n",
      "Train Epoch: 4 [2944/17352 (17%)] Loss: -130526.398438\n",
      "Train Epoch: 4 [4352/17352 (25%)] Loss: -87994.929688\n",
      "Train Epoch: 4 [5760/17352 (33%)] Loss: -147821.718750\n",
      "Train Epoch: 4 [7168/17352 (41%)] Loss: -10490.468750\n",
      "Train Epoch: 4 [8576/17352 (49%)] Loss: -152985.796875\n",
      "Train Epoch: 4 [9984/17352 (58%)] Loss: -90314.046875\n",
      "Train Epoch: 4 [11392/17352 (66%)] Loss: -161325.343750\n",
      "Train Epoch: 4 [12800/17352 (74%)] Loss: 45307.437500\n",
      "Train Epoch: 4 [14208/17352 (82%)] Loss: 4813.244141\n",
      "Train Epoch: 4 [15531/17352 (90%)] Loss: -76259.203125\n",
      "Train Epoch: 4 [16153/17352 (93%)] Loss: -75632.953125\n",
      "Train Epoch: 4 [16916/17352 (97%)] Loss: -86301.960938\n",
      "    epoch          : 4\n",
      "    loss           : -76858.99100612153\n",
      "    val_loss       : -41838.66355142593\n",
      "Train Epoch: 5 [128/17352 (1%)] Loss: -148260.515625\n",
      "Train Epoch: 5 [1536/17352 (9%)] Loss: 41461.242188\n",
      "Train Epoch: 5 [2944/17352 (17%)] Loss: 16081.279297\n",
      "Train Epoch: 5 [4352/17352 (25%)] Loss: 28273.332031\n",
      "Train Epoch: 5 [5760/17352 (33%)] Loss: -33077.445312\n",
      "Train Epoch: 5 [7168/17352 (41%)] Loss: -180637.500000\n",
      "Train Epoch: 5 [8576/17352 (49%)] Loss: -126337.828125\n",
      "Train Epoch: 5 [9984/17352 (58%)] Loss: -182804.453125\n",
      "Train Epoch: 5 [11392/17352 (66%)] Loss: -178686.546875\n",
      "Train Epoch: 5 [12800/17352 (74%)] Loss: -159346.343750\n",
      "Train Epoch: 5 [14208/17352 (82%)] Loss: -193650.468750\n",
      "Train Epoch: 5 [15553/17352 (90%)] Loss: 39920.898438\n",
      "Train Epoch: 5 [16059/17352 (93%)] Loss: 12278.016602\n",
      "Train Epoch: 5 [16895/17352 (97%)] Loss: -82989.375000\n",
      "    epoch          : 5\n",
      "    loss           : -81566.3032939322\n",
      "    val_loss       : -54003.96229199568\n",
      "Train Epoch: 6 [128/17352 (1%)] Loss: -180084.656250\n",
      "Train Epoch: 6 [1536/17352 (9%)] Loss: -152643.656250\n",
      "Train Epoch: 6 [2944/17352 (17%)] Loss: 45123.031250\n",
      "Train Epoch: 6 [4352/17352 (25%)] Loss: -133852.390625\n",
      "Train Epoch: 6 [5760/17352 (33%)] Loss: -119705.265625\n",
      "Train Epoch: 6 [7168/17352 (41%)] Loss: -178617.546875\n",
      "Train Epoch: 6 [8576/17352 (49%)] Loss: -157188.234375\n",
      "Train Epoch: 6 [9984/17352 (58%)] Loss: -9594.867188\n",
      "Train Epoch: 6 [11392/17352 (66%)] Loss: -125562.125000\n",
      "Train Epoch: 6 [12800/17352 (74%)] Loss: -137101.671875\n",
      "Train Epoch: 6 [14208/17352 (82%)] Loss: -202533.125000\n",
      "Train Epoch: 6 [15542/17352 (90%)] Loss: -87346.250000\n",
      "Train Epoch: 6 [16170/17352 (93%)] Loss: -107343.234375\n",
      "Train Epoch: 6 [16927/17352 (98%)] Loss: -20670.843750\n",
      "    epoch          : 6\n",
      "    loss           : -91011.69418519296\n",
      "    val_loss       : -60872.51834947268\n",
      "Train Epoch: 7 [128/17352 (1%)] Loss: -105923.539062\n",
      "Train Epoch: 7 [1536/17352 (9%)] Loss: -132173.500000\n",
      "Train Epoch: 7 [2944/17352 (17%)] Loss: -175471.171875\n",
      "Train Epoch: 7 [4352/17352 (25%)] Loss: -182335.906250\n",
      "Train Epoch: 7 [5760/17352 (33%)] Loss: -138361.734375\n",
      "Train Epoch: 7 [7168/17352 (41%)] Loss: -108956.203125\n",
      "Train Epoch: 7 [8576/17352 (49%)] Loss: -184961.125000\n",
      "Train Epoch: 7 [9984/17352 (58%)] Loss: -13455.359375\n",
      "Train Epoch: 7 [11392/17352 (66%)] Loss: -119277.617188\n",
      "Train Epoch: 7 [12800/17352 (74%)] Loss: -190631.984375\n",
      "Train Epoch: 7 [14208/17352 (82%)] Loss: -159360.421875\n",
      "Train Epoch: 7 [15478/17352 (89%)] Loss: -4812.109375\n",
      "Train Epoch: 7 [16359/17352 (94%)] Loss: -105814.843750\n",
      "Train Epoch: 7 [17143/17352 (99%)] Loss: -79896.968750\n",
      "    epoch          : 7\n",
      "    loss           : -86973.46594156355\n",
      "    val_loss       : -52400.853752628966\n",
      "Train Epoch: 8 [128/17352 (1%)] Loss: -164351.718750\n",
      "Train Epoch: 8 [1536/17352 (9%)] Loss: -79399.156250\n",
      "Train Epoch: 8 [2944/17352 (17%)] Loss: 57707.242188\n",
      "Train Epoch: 8 [4352/17352 (25%)] Loss: 123830.882812\n",
      "Train Epoch: 8 [5760/17352 (33%)] Loss: -200534.609375\n",
      "Train Epoch: 8 [7168/17352 (41%)] Loss: -203397.109375\n",
      "Train Epoch: 8 [8576/17352 (49%)] Loss: -186480.437500\n",
      "Train Epoch: 8 [9984/17352 (58%)] Loss: -73543.773438\n",
      "Train Epoch: 8 [11392/17352 (66%)] Loss: 80317.593750\n",
      "Train Epoch: 8 [12800/17352 (74%)] Loss: -190285.234375\n",
      "Train Epoch: 8 [14208/17352 (82%)] Loss: -183763.125000\n",
      "Train Epoch: 8 [15418/17352 (89%)] Loss: -13641.906250\n",
      "Train Epoch: 8 [16231/17352 (94%)] Loss: -55865.625000\n",
      "Train Epoch: 8 [17007/17352 (98%)] Loss: -71602.070312\n",
      "    epoch          : 8\n",
      "    loss           : -95483.38371041317\n",
      "    val_loss       : -56635.630278937024\n",
      "Train Epoch: 9 [128/17352 (1%)] Loss: 1979.234375\n",
      "Train Epoch: 9 [1536/17352 (9%)] Loss: -35030.320312\n",
      "Train Epoch: 9 [2944/17352 (17%)] Loss: -154871.250000\n",
      "Train Epoch: 9 [4352/17352 (25%)] Loss: -87037.054688\n",
      "Train Epoch: 9 [5760/17352 (33%)] Loss: 97658.773438\n",
      "Train Epoch: 9 [7168/17352 (41%)] Loss: -206074.406250\n",
      "Train Epoch: 9 [8576/17352 (49%)] Loss: -137528.421875\n",
      "Train Epoch: 9 [9984/17352 (58%)] Loss: -177229.765625\n",
      "Train Epoch: 9 [11392/17352 (66%)] Loss: -202776.343750\n",
      "Train Epoch: 9 [12800/17352 (74%)] Loss: -148748.781250\n",
      "Train Epoch: 9 [14208/17352 (82%)] Loss: -179321.281250\n",
      "Train Epoch: 9 [15552/17352 (90%)] Loss: -120327.718750\n",
      "Train Epoch: 9 [16335/17352 (94%)] Loss: -80052.203125\n",
      "Train Epoch: 9 [17117/17352 (99%)] Loss: -73481.609375\n",
      "    epoch          : 9\n",
      "    loss           : -104010.95857965866\n",
      "    val_loss       : -61753.06954429944\n",
      "Train Epoch: 10 [128/17352 (1%)] Loss: -54948.453125\n",
      "Train Epoch: 10 [1536/17352 (9%)] Loss: -150809.140625\n",
      "Train Epoch: 10 [2944/17352 (17%)] Loss: -25001.656250\n",
      "Train Epoch: 10 [4352/17352 (25%)] Loss: -207903.093750\n",
      "Train Epoch: 10 [5760/17352 (33%)] Loss: -121127.328125\n",
      "Train Epoch: 10 [7168/17352 (41%)] Loss: -190602.906250\n",
      "Train Epoch: 10 [8576/17352 (49%)] Loss: -126183.320312\n",
      "Train Epoch: 10 [9984/17352 (58%)] Loss: 39049.828125\n",
      "Train Epoch: 10 [11392/17352 (66%)] Loss: -201486.906250\n",
      "Train Epoch: 10 [12800/17352 (74%)] Loss: -178834.109375\n",
      "Train Epoch: 10 [14208/17352 (82%)] Loss: -190962.125000\n",
      "Train Epoch: 10 [15515/17352 (89%)] Loss: -54863.382812\n",
      "Train Epoch: 10 [16368/17352 (94%)] Loss: -32745.808594\n",
      "Train Epoch: 10 [17072/17352 (98%)] Loss: -27737.357422\n",
      "    epoch          : 10\n",
      "    loss           : -110567.59904572148\n",
      "    val_loss       : -61993.19234897296\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0913_161229/checkpoint-epoch10.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 11 [128/17352 (1%)] Loss: -9842.539062\n",
      "Train Epoch: 11 [1536/17352 (9%)] Loss: 287767.031250\n",
      "Train Epoch: 11 [2944/17352 (17%)] Loss: -6539.445312\n",
      "Train Epoch: 11 [4352/17352 (25%)] Loss: -140869.031250\n",
      "Train Epoch: 11 [5760/17352 (33%)] Loss: -211236.984375\n",
      "Train Epoch: 11 [7168/17352 (41%)] Loss: -139847.265625\n",
      "Train Epoch: 11 [8576/17352 (49%)] Loss: -132802.828125\n",
      "Train Epoch: 11 [9984/17352 (58%)] Loss: 115788.804688\n",
      "Train Epoch: 11 [11392/17352 (66%)] Loss: -177265.109375\n",
      "Train Epoch: 11 [12800/17352 (74%)] Loss: -189461.453125\n",
      "Train Epoch: 11 [14208/17352 (82%)] Loss: -163201.875000\n",
      "Train Epoch: 11 [15419/17352 (89%)] Loss: -77398.437500\n",
      "Train Epoch: 11 [16106/17352 (93%)] Loss: -24853.062500\n",
      "Train Epoch: 11 [16916/17352 (97%)] Loss: -17145.843750\n",
      "    epoch          : 11\n",
      "    loss           : -111945.48351641149\n",
      "    val_loss       : -76094.81185774803\n",
      "Train Epoch: 12 [128/17352 (1%)] Loss: -16226.908203\n",
      "Train Epoch: 12 [1536/17352 (9%)] Loss: -201727.406250\n",
      "Train Epoch: 12 [2944/17352 (17%)] Loss: -209246.765625\n",
      "Train Epoch: 12 [4352/17352 (25%)] Loss: -144895.046875\n",
      "Train Epoch: 12 [5760/17352 (33%)] Loss: -192073.953125\n",
      "Train Epoch: 12 [7168/17352 (41%)] Loss: -198860.218750\n",
      "Train Epoch: 12 [8576/17352 (49%)] Loss: 66057.445312\n",
      "Train Epoch: 12 [9984/17352 (58%)] Loss: -181607.718750\n",
      "Train Epoch: 12 [11392/17352 (66%)] Loss: 103725.132812\n",
      "Train Epoch: 12 [12800/17352 (74%)] Loss: -89249.226562\n",
      "Train Epoch: 12 [14208/17352 (82%)] Loss: -192556.593750\n",
      "Train Epoch: 12 [15534/17352 (90%)] Loss: 42384.359375\n",
      "Train Epoch: 12 [16242/17352 (94%)] Loss: -56114.070312\n",
      "Train Epoch: 12 [16907/17352 (97%)] Loss: -5246.554688\n",
      "    epoch          : 12\n",
      "    loss           : -108291.62891280411\n",
      "    val_loss       : -66495.21498277983\n",
      "Train Epoch: 13 [128/17352 (1%)] Loss: -59775.535156\n",
      "Train Epoch: 13 [1536/17352 (9%)] Loss: -163921.015625\n",
      "Train Epoch: 13 [2944/17352 (17%)] Loss: -183629.000000\n",
      "Train Epoch: 13 [4352/17352 (25%)] Loss: -66571.578125\n",
      "Train Epoch: 13 [5760/17352 (33%)] Loss: -121745.898438\n",
      "Train Epoch: 13 [7168/17352 (41%)] Loss: -74947.335938\n",
      "Train Epoch: 13 [8576/17352 (49%)] Loss: -213285.546875\n",
      "Train Epoch: 13 [9984/17352 (58%)] Loss: -16185.464844\n",
      "Train Epoch: 13 [11392/17352 (66%)] Loss: -124674.570312\n",
      "Train Epoch: 13 [12800/17352 (74%)] Loss: -142787.234375\n",
      "Train Epoch: 13 [14208/17352 (82%)] Loss: -164232.734375\n",
      "Train Epoch: 13 [15493/17352 (89%)] Loss: -126818.640625\n",
      "Train Epoch: 13 [16280/17352 (94%)] Loss: -476.453125\n",
      "Train Epoch: 13 [17048/17352 (98%)] Loss: -101344.226562\n",
      "    epoch          : 13\n",
      "    loss           : -115537.01494664953\n",
      "    val_loss       : -68048.74060235024\n",
      "Train Epoch: 14 [128/17352 (1%)] Loss: -131228.156250\n",
      "Train Epoch: 14 [1536/17352 (9%)] Loss: -209810.734375\n",
      "Train Epoch: 14 [2944/17352 (17%)] Loss: -128064.906250\n",
      "Train Epoch: 14 [4352/17352 (25%)] Loss: -187534.156250\n",
      "Train Epoch: 14 [5760/17352 (33%)] Loss: -46502.531250\n",
      "Train Epoch: 14 [7168/17352 (41%)] Loss: -208140.421875\n",
      "Train Epoch: 14 [8576/17352 (49%)] Loss: -144151.500000\n",
      "Train Epoch: 14 [9984/17352 (58%)] Loss: -223159.062500\n",
      "Train Epoch: 14 [11392/17352 (66%)] Loss: -206617.406250\n",
      "Train Epoch: 14 [12800/17352 (74%)] Loss: -135720.671875\n",
      "Train Epoch: 14 [14208/17352 (82%)] Loss: 91459.273438\n",
      "Train Epoch: 14 [15450/17352 (89%)] Loss: -10987.375977\n",
      "Train Epoch: 14 [16282/17352 (94%)] Loss: -77483.390625\n",
      "Train Epoch: 14 [17073/17352 (98%)] Loss: 17675.726562\n",
      "    epoch          : 14\n",
      "    loss           : -123008.82549024749\n",
      "    val_loss       : -63752.32779466311\n",
      "Train Epoch: 15 [128/17352 (1%)] Loss: -211204.312500\n",
      "Train Epoch: 15 [1536/17352 (9%)] Loss: -204260.500000\n",
      "Train Epoch: 15 [2944/17352 (17%)] Loss: -210298.156250\n",
      "Train Epoch: 15 [4352/17352 (25%)] Loss: -27635.904297\n",
      "Train Epoch: 15 [5760/17352 (33%)] Loss: -147148.812500\n",
      "Train Epoch: 15 [7168/17352 (41%)] Loss: -132754.515625\n",
      "Train Epoch: 15 [8576/17352 (49%)] Loss: -197925.781250\n",
      "Train Epoch: 15 [9984/17352 (58%)] Loss: -121080.984375\n",
      "Train Epoch: 15 [11392/17352 (66%)] Loss: -201655.078125\n",
      "Train Epoch: 15 [12800/17352 (74%)] Loss: -135264.687500\n",
      "Train Epoch: 15 [14208/17352 (82%)] Loss: -142886.906250\n",
      "Train Epoch: 15 [15530/17352 (89%)] Loss: 9361.534180\n",
      "Train Epoch: 15 [16177/17352 (93%)] Loss: -96235.921875\n",
      "Train Epoch: 15 [17018/17352 (98%)] Loss: 31829.148438\n",
      "    epoch          : 15\n",
      "    loss           : -118336.71561713507\n",
      "    val_loss       : -72339.92657302221\n",
      "Train Epoch: 16 [128/17352 (1%)] Loss: -153288.265625\n",
      "Train Epoch: 16 [1536/17352 (9%)] Loss: -225294.875000\n",
      "Train Epoch: 16 [2944/17352 (17%)] Loss: -68104.179688\n",
      "Train Epoch: 16 [4352/17352 (25%)] Loss: -198752.875000\n",
      "Train Epoch: 16 [5760/17352 (33%)] Loss: -115073.656250\n",
      "Train Epoch: 16 [7168/17352 (41%)] Loss: -185934.421875\n",
      "Train Epoch: 16 [8576/17352 (49%)] Loss: -212841.250000\n",
      "Train Epoch: 16 [9984/17352 (58%)] Loss: -83809.890625\n",
      "Train Epoch: 16 [11392/17352 (66%)] Loss: -130066.773438\n",
      "Train Epoch: 16 [12800/17352 (74%)] Loss: 27098.695312\n",
      "Train Epoch: 16 [14208/17352 (82%)] Loss: -153976.421875\n",
      "Train Epoch: 16 [15512/17352 (89%)] Loss: -77548.109375\n",
      "Train Epoch: 16 [16261/17352 (94%)] Loss: -139613.921875\n",
      "Train Epoch: 16 [17087/17352 (98%)] Loss: -105048.937500\n",
      "    epoch          : 16\n",
      "    loss           : -111930.25754378147\n",
      "    val_loss       : -61543.25561240514\n",
      "Train Epoch: 17 [128/17352 (1%)] Loss: -188348.265625\n",
      "Train Epoch: 17 [1536/17352 (9%)] Loss: 92309.234375\n",
      "Train Epoch: 17 [2944/17352 (17%)] Loss: -153164.687500\n",
      "Train Epoch: 17 [4352/17352 (25%)] Loss: -176438.265625\n",
      "Train Epoch: 17 [5760/17352 (33%)] Loss: -207844.187500\n",
      "Train Epoch: 17 [7168/17352 (41%)] Loss: 65958.109375\n",
      "Train Epoch: 17 [8576/17352 (49%)] Loss: -93496.500000\n",
      "Train Epoch: 17 [9984/17352 (58%)] Loss: -207338.250000\n",
      "Train Epoch: 17 [11392/17352 (66%)] Loss: -172875.859375\n",
      "Train Epoch: 17 [12800/17352 (74%)] Loss: -178854.968750\n",
      "Train Epoch: 17 [14208/17352 (82%)] Loss: -212773.421875\n",
      "Train Epoch: 17 [15489/17352 (89%)] Loss: -46664.718750\n",
      "Train Epoch: 17 [16170/17352 (93%)] Loss: -8273.017578\n",
      "Train Epoch: 17 [16939/17352 (98%)] Loss: -87794.429688\n",
      "    epoch          : 17\n",
      "    loss           : -114014.31043545512\n",
      "    val_loss       : -48854.57726910909\n",
      "Train Epoch: 18 [128/17352 (1%)] Loss: -190400.625000\n",
      "Train Epoch: 18 [1536/17352 (9%)] Loss: -174638.390625\n",
      "Train Epoch: 18 [2944/17352 (17%)] Loss: -206190.343750\n",
      "Train Epoch: 18 [4352/17352 (25%)] Loss: -208216.093750\n",
      "Train Epoch: 18 [5760/17352 (33%)] Loss: -112166.156250\n",
      "Train Epoch: 18 [7168/17352 (41%)] Loss: -151923.687500\n",
      "Train Epoch: 18 [8576/17352 (49%)] Loss: -46636.429688\n",
      "Train Epoch: 18 [9984/17352 (58%)] Loss: -211278.781250\n",
      "Train Epoch: 18 [11392/17352 (66%)] Loss: -116806.843750\n",
      "Train Epoch: 18 [12800/17352 (74%)] Loss: -215975.250000\n",
      "Train Epoch: 18 [14208/17352 (82%)] Loss: -157172.109375\n",
      "Train Epoch: 18 [15479/17352 (89%)] Loss: -162583.921875\n",
      "Train Epoch: 18 [16280/17352 (94%)] Loss: -179951.281250\n",
      "Train Epoch: 18 [17102/17352 (99%)] Loss: -134584.812500\n",
      "    epoch          : 18\n",
      "    loss           : -111907.20741056916\n",
      "    val_loss       : -62889.612286631265\n",
      "Train Epoch: 19 [128/17352 (1%)] Loss: -194817.203125\n",
      "Train Epoch: 19 [1536/17352 (9%)] Loss: 33470.648438\n",
      "Train Epoch: 19 [2944/17352 (17%)] Loss: 106607.437500\n",
      "Train Epoch: 19 [4352/17352 (25%)] Loss: -98571.867188\n",
      "Train Epoch: 19 [5760/17352 (33%)] Loss: -53850.273438\n",
      "Train Epoch: 19 [7168/17352 (41%)] Loss: -208446.531250\n",
      "Train Epoch: 19 [8576/17352 (49%)] Loss: -154026.953125\n",
      "Train Epoch: 19 [9984/17352 (58%)] Loss: -188119.640625\n",
      "Train Epoch: 19 [11392/17352 (66%)] Loss: -147085.000000\n",
      "Train Epoch: 19 [12800/17352 (74%)] Loss: -141003.125000\n",
      "Train Epoch: 19 [14208/17352 (82%)] Loss: -229774.171875\n",
      "Train Epoch: 19 [15447/17352 (89%)] Loss: -5115.753906\n",
      "Train Epoch: 19 [16203/17352 (93%)] Loss: -142123.500000\n",
      "Train Epoch: 19 [16841/17352 (97%)] Loss: -149583.843750\n",
      "    epoch          : 19\n",
      "    loss           : -121556.70865994653\n",
      "    val_loss       : -77396.5237273693\n",
      "Train Epoch: 20 [128/17352 (1%)] Loss: -33294.890625\n",
      "Train Epoch: 20 [1536/17352 (9%)] Loss: 141458.968750\n",
      "Train Epoch: 20 [2944/17352 (17%)] Loss: -151307.406250\n",
      "Train Epoch: 20 [4352/17352 (25%)] Loss: -65583.664062\n",
      "Train Epoch: 20 [5760/17352 (33%)] Loss: -133896.390625\n",
      "Train Epoch: 20 [7168/17352 (41%)] Loss: -108964.937500\n",
      "Train Epoch: 20 [8576/17352 (49%)] Loss: -199991.937500\n",
      "Train Epoch: 20 [9984/17352 (58%)] Loss: -198416.187500\n",
      "Train Epoch: 20 [11392/17352 (66%)] Loss: -166017.750000\n",
      "Train Epoch: 20 [12800/17352 (74%)] Loss: -132362.437500\n",
      "Train Epoch: 20 [14208/17352 (82%)] Loss: -37882.714844\n",
      "Train Epoch: 20 [15522/17352 (89%)] Loss: 5689.914062\n",
      "Train Epoch: 20 [16224/17352 (93%)] Loss: -142327.906250\n",
      "Train Epoch: 20 [17082/17352 (98%)] Loss: -132868.796875\n",
      "    epoch          : 20\n",
      "    loss           : -110905.7307997326\n",
      "    val_loss       : -69954.18649155299\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0913_161229/checkpoint-epoch20.pth ...\n",
      "Train Epoch: 21 [128/17352 (1%)] Loss: -115212.296875\n",
      "Train Epoch: 21 [1536/17352 (9%)] Loss: -182264.062500\n",
      "Train Epoch: 21 [2944/17352 (17%)] Loss: -59652.660156\n",
      "Train Epoch: 21 [4352/17352 (25%)] Loss: -125329.406250\n",
      "Train Epoch: 21 [5760/17352 (33%)] Loss: -216073.281250\n",
      "Train Epoch: 21 [7168/17352 (41%)] Loss: -230871.890625\n",
      "Train Epoch: 21 [8576/17352 (49%)] Loss: -227127.890625\n",
      "Train Epoch: 21 [9984/17352 (58%)] Loss: -185768.406250\n",
      "Train Epoch: 21 [11392/17352 (66%)] Loss: -35979.414062\n",
      "Train Epoch: 21 [12800/17352 (74%)] Loss: 57377.128906\n",
      "Train Epoch: 21 [14208/17352 (82%)] Loss: -230663.312500\n",
      "Train Epoch: 21 [15512/17352 (89%)] Loss: -95006.250000\n",
      "Train Epoch: 21 [16293/17352 (94%)] Loss: -60376.562500\n",
      "Train Epoch: 21 [17022/17352 (98%)] Loss: -54595.390625\n",
      "    epoch          : 21\n",
      "    loss           : -122414.60120898765\n",
      "    val_loss       : -65964.62685381572\n",
      "Train Epoch: 22 [128/17352 (1%)] Loss: -132941.515625\n",
      "Train Epoch: 22 [1536/17352 (9%)] Loss: -222925.812500\n",
      "Train Epoch: 22 [2944/17352 (17%)] Loss: -701.226562\n",
      "Train Epoch: 22 [4352/17352 (25%)] Loss: -140652.281250\n",
      "Train Epoch: 22 [5760/17352 (33%)] Loss: -201038.015625\n",
      "Train Epoch: 22 [7168/17352 (41%)] Loss: -208827.953125\n",
      "Train Epoch: 22 [8576/17352 (49%)] Loss: -182232.125000\n",
      "Train Epoch: 22 [9984/17352 (58%)] Loss: -203148.406250\n",
      "Train Epoch: 22 [11392/17352 (66%)] Loss: -131480.750000\n",
      "Train Epoch: 22 [12800/17352 (74%)] Loss: -157988.250000\n",
      "Train Epoch: 22 [14208/17352 (82%)] Loss: -148303.078125\n",
      "Train Epoch: 22 [15529/17352 (89%)] Loss: 69149.804688\n",
      "Train Epoch: 22 [16064/17352 (93%)] Loss: -159211.375000\n",
      "Train Epoch: 22 [16873/17352 (97%)] Loss: -75005.914062\n",
      "    epoch          : 22\n",
      "    loss           : -130610.98959534921\n",
      "    val_loss       : -59453.30300078392\n",
      "Train Epoch: 23 [128/17352 (1%)] Loss: -190651.718750\n",
      "Train Epoch: 23 [1536/17352 (9%)] Loss: -145558.921875\n",
      "Train Epoch: 23 [2944/17352 (17%)] Loss: -204804.140625\n",
      "Train Epoch: 23 [4352/17352 (25%)] Loss: -148980.312500\n",
      "Train Epoch: 23 [5760/17352 (33%)] Loss: -155156.578125\n",
      "Train Epoch: 23 [7168/17352 (41%)] Loss: 22642.085938\n",
      "Train Epoch: 23 [8576/17352 (49%)] Loss: -153842.328125\n",
      "Train Epoch: 23 [9984/17352 (58%)] Loss: 72195.546875\n",
      "Train Epoch: 23 [11392/17352 (66%)] Loss: -25657.312500\n",
      "Train Epoch: 23 [12800/17352 (74%)] Loss: -30549.652344\n",
      "Train Epoch: 23 [14208/17352 (82%)] Loss: -178225.546875\n",
      "Train Epoch: 23 [15538/17352 (90%)] Loss: -20974.421875\n",
      "Train Epoch: 23 [16191/17352 (93%)] Loss: -68618.679688\n",
      "Train Epoch: 23 [16999/17352 (98%)] Loss: -24970.472656\n",
      "    epoch          : 23\n",
      "    loss           : -125214.45458492817\n",
      "    val_loss       : -61587.07260834376\n",
      "Train Epoch: 24 [128/17352 (1%)] Loss: -222234.312500\n",
      "Train Epoch: 24 [1536/17352 (9%)] Loss: -26884.347656\n",
      "Train Epoch: 24 [2944/17352 (17%)] Loss: -225379.203125\n",
      "Train Epoch: 24 [4352/17352 (25%)] Loss: -86748.687500\n",
      "Train Epoch: 24 [5760/17352 (33%)] Loss: -131473.453125\n",
      "Train Epoch: 24 [7168/17352 (41%)] Loss: -195379.359375\n",
      "Train Epoch: 24 [8576/17352 (49%)] Loss: -125910.679688\n",
      "Train Epoch: 24 [9984/17352 (58%)] Loss: 112951.078125\n",
      "Train Epoch: 24 [11392/17352 (66%)] Loss: -135671.656250\n",
      "Train Epoch: 24 [12800/17352 (74%)] Loss: -120353.687500\n",
      "Train Epoch: 24 [14208/17352 (82%)] Loss: -221979.906250\n",
      "Train Epoch: 24 [15546/17352 (90%)] Loss: -168300.750000\n",
      "Train Epoch: 24 [16146/17352 (93%)] Loss: -76014.421875\n",
      "Train Epoch: 24 [17031/17352 (98%)] Loss: 13698.652344\n",
      "    epoch          : 24\n",
      "    loss           : -117019.25612809353\n",
      "    val_loss       : -80788.74547904333\n",
      "Train Epoch: 25 [128/17352 (1%)] Loss: -189462.750000\n",
      "Train Epoch: 25 [1536/17352 (9%)] Loss: -123174.703125\n",
      "Train Epoch: 25 [2944/17352 (17%)] Loss: -233916.656250\n",
      "Train Epoch: 25 [4352/17352 (25%)] Loss: 177842.875000\n",
      "Train Epoch: 25 [5760/17352 (33%)] Loss: -196971.234375\n",
      "Train Epoch: 25 [7168/17352 (41%)] Loss: -150438.640625\n",
      "Train Epoch: 25 [8576/17352 (49%)] Loss: -100904.468750\n",
      "Train Epoch: 25 [9984/17352 (58%)] Loss: -226043.062500\n",
      "Train Epoch: 25 [11392/17352 (66%)] Loss: -147594.218750\n",
      "Train Epoch: 25 [12800/17352 (74%)] Loss: -207350.421875\n",
      "Train Epoch: 25 [14208/17352 (82%)] Loss: -155908.250000\n",
      "Train Epoch: 25 [15459/17352 (89%)] Loss: 155875.890625\n",
      "Train Epoch: 25 [16398/17352 (95%)] Loss: -89565.390625\n",
      "Train Epoch: 25 [17048/17352 (98%)] Loss: -50261.007812\n",
      "    epoch          : 25\n",
      "    loss           : -123454.1766332844\n",
      "    val_loss       : -57085.51162341436\n",
      "Train Epoch: 26 [128/17352 (1%)] Loss: -131057.546875\n",
      "Train Epoch: 26 [1536/17352 (9%)] Loss: -163141.578125\n",
      "Train Epoch: 26 [2944/17352 (17%)] Loss: -227574.203125\n",
      "Train Epoch: 26 [4352/17352 (25%)] Loss: -86809.125000\n",
      "Train Epoch: 26 [5760/17352 (33%)] Loss: 243936.015625\n",
      "Train Epoch: 26 [7168/17352 (41%)] Loss: -170248.781250\n",
      "Train Epoch: 26 [8576/17352 (49%)] Loss: -208550.859375\n",
      "Train Epoch: 26 [9984/17352 (58%)] Loss: -218962.796875\n",
      "Train Epoch: 26 [11392/17352 (66%)] Loss: -211344.312500\n",
      "Train Epoch: 26 [12800/17352 (74%)] Loss: -169110.750000\n",
      "Train Epoch: 26 [14208/17352 (82%)] Loss: -175054.515625\n",
      "Train Epoch: 26 [15549/17352 (90%)] Loss: -73940.343750\n",
      "Train Epoch: 26 [16172/17352 (93%)] Loss: 79093.132812\n",
      "Train Epoch: 26 [17032/17352 (98%)] Loss: -61654.863281\n",
      "    epoch          : 26\n",
      "    loss           : -113835.77392905831\n",
      "    val_loss       : -68800.07649911245\n",
      "Train Epoch: 27 [128/17352 (1%)] Loss: -197111.625000\n",
      "Train Epoch: 27 [1536/17352 (9%)] Loss: -214251.156250\n",
      "Train Epoch: 27 [2944/17352 (17%)] Loss: -150979.515625\n",
      "Train Epoch: 27 [4352/17352 (25%)] Loss: -205015.953125\n",
      "Train Epoch: 27 [5760/17352 (33%)] Loss: -139159.484375\n",
      "Train Epoch: 27 [7168/17352 (41%)] Loss: -192756.500000\n",
      "Train Epoch: 27 [8576/17352 (49%)] Loss: 138573.937500\n",
      "Train Epoch: 27 [9984/17352 (58%)] Loss: -197652.687500\n",
      "Train Epoch: 27 [11392/17352 (66%)] Loss: -161594.750000\n",
      "Train Epoch: 27 [12800/17352 (74%)] Loss: -217626.109375\n",
      "Train Epoch: 27 [14208/17352 (82%)] Loss: -216725.531250\n",
      "Train Epoch: 27 [15419/17352 (89%)] Loss: -8737.149414\n",
      "Train Epoch: 27 [16264/17352 (94%)] Loss: -27712.707031\n",
      "Train Epoch: 27 [17026/17352 (98%)] Loss: -62923.117188\n",
      "    epoch          : 27\n",
      "    loss           : -115193.21337562919\n",
      "    val_loss       : -57852.69807162285\n",
      "Train Epoch: 28 [128/17352 (1%)] Loss: -26503.050781\n",
      "Train Epoch: 28 [1536/17352 (9%)] Loss: -106904.882812\n",
      "Train Epoch: 28 [2944/17352 (17%)] Loss: -231683.031250\n",
      "Train Epoch: 28 [4352/17352 (25%)] Loss: 183904.781250\n",
      "Train Epoch: 28 [5760/17352 (33%)] Loss: -118929.523438\n",
      "Train Epoch: 28 [7168/17352 (41%)] Loss: -155925.562500\n",
      "Train Epoch: 28 [8576/17352 (49%)] Loss: -218343.781250\n",
      "Train Epoch: 28 [9984/17352 (58%)] Loss: -187149.421875\n",
      "Train Epoch: 28 [11392/17352 (66%)] Loss: -152964.375000\n",
      "Train Epoch: 28 [12800/17352 (74%)] Loss: -225451.406250\n",
      "Train Epoch: 28 [14208/17352 (82%)] Loss: -219773.781250\n",
      "Train Epoch: 28 [15464/17352 (89%)] Loss: -65720.632812\n",
      "Train Epoch: 28 [16371/17352 (94%)] Loss: -216569.765625\n",
      "Train Epoch: 28 [17052/17352 (98%)] Loss: -62302.226562\n",
      "    epoch          : 28\n",
      "    loss           : -108008.80424673605\n",
      "    val_loss       : -54857.3894267718\n",
      "Train Epoch: 29 [128/17352 (1%)] Loss: -220440.062500\n",
      "Train Epoch: 29 [1536/17352 (9%)] Loss: -125913.937500\n",
      "Train Epoch: 29 [2944/17352 (17%)] Loss: -201796.046875\n",
      "Train Epoch: 29 [4352/17352 (25%)] Loss: -128070.242188\n",
      "Train Epoch: 29 [5760/17352 (33%)] Loss: 8744.335938\n",
      "Train Epoch: 29 [7168/17352 (41%)] Loss: -201906.734375\n",
      "Train Epoch: 29 [8576/17352 (49%)] Loss: -201706.312500\n",
      "Train Epoch: 29 [9984/17352 (58%)] Loss: -209274.765625\n",
      "Train Epoch: 29 [11392/17352 (66%)] Loss: 136941.625000\n",
      "Train Epoch: 29 [12800/17352 (74%)] Loss: -181278.312500\n",
      "Train Epoch: 29 [14208/17352 (82%)] Loss: -150469.375000\n",
      "Train Epoch: 29 [15416/17352 (89%)] Loss: -54036.304688\n",
      "Train Epoch: 29 [16211/17352 (93%)] Loss: -4057.240234\n",
      "Train Epoch: 29 [16977/17352 (98%)] Loss: -98169.132812\n",
      "    epoch          : 29\n",
      "    loss           : -117357.94256305054\n",
      "    val_loss       : -83724.26770407359\n",
      "Train Epoch: 30 [128/17352 (1%)] Loss: -183037.640625\n",
      "Train Epoch: 30 [1536/17352 (9%)] Loss: -151142.000000\n",
      "Train Epoch: 30 [2944/17352 (17%)] Loss: -160705.515625\n",
      "Train Epoch: 30 [4352/17352 (25%)] Loss: -206041.718750\n",
      "Train Epoch: 30 [5760/17352 (33%)] Loss: 29081.093750\n",
      "Train Epoch: 30 [7168/17352 (41%)] Loss: -157315.890625\n",
      "Train Epoch: 30 [8576/17352 (49%)] Loss: -59891.476562\n",
      "Train Epoch: 30 [9984/17352 (58%)] Loss: -165398.015625\n",
      "Train Epoch: 30 [11392/17352 (66%)] Loss: -219105.125000\n",
      "Train Epoch: 30 [12800/17352 (74%)] Loss: -225422.468750\n",
      "Train Epoch: 30 [14208/17352 (82%)] Loss: -219683.671875\n",
      "Train Epoch: 30 [15399/17352 (89%)] Loss: -6126.648926\n",
      "Train Epoch: 30 [16335/17352 (94%)] Loss: -100144.882812\n",
      "Train Epoch: 30 [16928/17352 (98%)] Loss: -62127.671875\n",
      "    epoch          : 30\n",
      "    loss           : -122286.93424260696\n",
      "    val_loss       : -76073.50174825986\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0913_161229/checkpoint-epoch30.pth ...\n",
      "Train Epoch: 31 [128/17352 (1%)] Loss: -200166.875000\n",
      "Train Epoch: 31 [1536/17352 (9%)] Loss: -183759.203125\n",
      "Train Epoch: 31 [2944/17352 (17%)] Loss: -186941.687500\n",
      "Train Epoch: 31 [4352/17352 (25%)] Loss: -212553.328125\n",
      "Train Epoch: 31 [5760/17352 (33%)] Loss: 89143.906250\n",
      "Train Epoch: 31 [7168/17352 (41%)] Loss: -179106.140625\n",
      "Train Epoch: 31 [8576/17352 (49%)] Loss: -166043.453125\n",
      "Train Epoch: 31 [9984/17352 (58%)] Loss: -227449.250000\n",
      "Train Epoch: 31 [11392/17352 (66%)] Loss: -206200.953125\n",
      "Train Epoch: 31 [12800/17352 (74%)] Loss: -204957.812500\n",
      "Train Epoch: 31 [14208/17352 (82%)] Loss: -186650.171875\n",
      "Train Epoch: 31 [15489/17352 (89%)] Loss: -162848.234375\n",
      "Train Epoch: 31 [16170/17352 (93%)] Loss: -166983.046875\n",
      "Train Epoch: 31 [16974/17352 (98%)] Loss: -14231.871094\n",
      "    epoch          : 31\n",
      "    loss           : -130954.0211107907\n",
      "    val_loss       : -69124.17002595266\n",
      "Train Epoch: 32 [128/17352 (1%)] Loss: -187193.625000\n",
      "Train Epoch: 32 [1536/17352 (9%)] Loss: -15932.550781\n",
      "Train Epoch: 32 [2944/17352 (17%)] Loss: -49883.492188\n",
      "Train Epoch: 32 [4352/17352 (25%)] Loss: -197531.609375\n",
      "Train Epoch: 32 [5760/17352 (33%)] Loss: -210312.937500\n",
      "Train Epoch: 32 [7168/17352 (41%)] Loss: -176589.421875\n",
      "Train Epoch: 32 [8576/17352 (49%)] Loss: -189537.921875\n",
      "Train Epoch: 32 [9984/17352 (58%)] Loss: -194012.359375\n",
      "Train Epoch: 32 [11392/17352 (66%)] Loss: -28794.531250\n",
      "Train Epoch: 32 [12800/17352 (74%)] Loss: 68848.273438\n",
      "Train Epoch: 32 [14208/17352 (82%)] Loss: -196985.593750\n",
      "Train Epoch: 32 [15460/17352 (89%)] Loss: -20672.496094\n",
      "Train Epoch: 32 [16096/17352 (93%)] Loss: -54501.417969\n",
      "Train Epoch: 32 [16854/17352 (97%)] Loss: -87866.171875\n",
      "    epoch          : 32\n",
      "    loss           : -122803.26219064597\n",
      "    val_loss       : -78946.4867444833\n",
      "Train Epoch: 33 [128/17352 (1%)] Loss: 186966.218750\n",
      "Train Epoch: 33 [1536/17352 (9%)] Loss: -38221.734375\n",
      "Train Epoch: 33 [2944/17352 (17%)] Loss: -186874.562500\n",
      "Train Epoch: 33 [4352/17352 (25%)] Loss: -62433.074219\n",
      "Train Epoch: 33 [5760/17352 (33%)] Loss: -22541.285156\n",
      "Train Epoch: 33 [7168/17352 (41%)] Loss: -188879.687500\n",
      "Train Epoch: 33 [8576/17352 (49%)] Loss: -91989.320312\n",
      "Train Epoch: 33 [9984/17352 (58%)] Loss: 185134.000000\n",
      "Train Epoch: 33 [11392/17352 (66%)] Loss: -176275.109375\n",
      "Train Epoch: 33 [12800/17352 (74%)] Loss: -56934.699219\n",
      "Train Epoch: 33 [14208/17352 (82%)] Loss: -29265.261719\n",
      "Train Epoch: 33 [15565/17352 (90%)] Loss: -102203.398438\n",
      "Train Epoch: 33 [16266/17352 (94%)] Loss: -74778.312500\n",
      "Train Epoch: 33 [17061/17352 (98%)] Loss: -46369.679688\n",
      "    epoch          : 33\n",
      "    loss           : -108384.07050420775\n",
      "    val_loss       : -70348.09375346501\n",
      "Train Epoch: 34 [128/17352 (1%)] Loss: -203037.015625\n",
      "Train Epoch: 34 [1536/17352 (9%)] Loss: -161620.171875\n",
      "Train Epoch: 34 [2944/17352 (17%)] Loss: -101727.242188\n",
      "Train Epoch: 34 [4352/17352 (25%)] Loss: -234427.156250\n",
      "Train Epoch: 34 [5760/17352 (33%)] Loss: 122154.968750\n",
      "Train Epoch: 34 [7168/17352 (41%)] Loss: 116781.273438\n",
      "Train Epoch: 34 [8576/17352 (49%)] Loss: -88904.773438\n",
      "Train Epoch: 34 [9984/17352 (58%)] Loss: -137436.859375\n",
      "Train Epoch: 34 [11392/17352 (66%)] Loss: -222034.093750\n",
      "Train Epoch: 34 [12800/17352 (74%)] Loss: -94502.796875\n",
      "Train Epoch: 34 [14208/17352 (82%)] Loss: -201131.484375\n",
      "Train Epoch: 34 [15448/17352 (89%)] Loss: -30329.265625\n",
      "Train Epoch: 34 [16272/17352 (94%)] Loss: 3956.660156\n",
      "Train Epoch: 34 [17007/17352 (98%)] Loss: -118223.648438\n",
      "    epoch          : 34\n",
      "    loss           : -109679.24950680317\n",
      "    val_loss       : -56707.18152952194\n",
      "Train Epoch: 35 [128/17352 (1%)] Loss: -198764.656250\n",
      "Train Epoch: 35 [1536/17352 (9%)] Loss: -236630.015625\n",
      "Train Epoch: 35 [2944/17352 (17%)] Loss: -150438.687500\n",
      "Train Epoch: 35 [4352/17352 (25%)] Loss: -97219.460938\n",
      "Train Epoch: 35 [5760/17352 (33%)] Loss: -157530.859375\n",
      "Train Epoch: 35 [7168/17352 (41%)] Loss: -91712.421875\n",
      "Train Epoch: 35 [8576/17352 (49%)] Loss: -157088.640625\n",
      "Train Epoch: 35 [9984/17352 (58%)] Loss: -171425.921875\n",
      "Train Epoch: 35 [11392/17352 (66%)] Loss: -179386.234375\n",
      "Train Epoch: 35 [12800/17352 (74%)] Loss: -34682.242188\n",
      "Train Epoch: 35 [14208/17352 (82%)] Loss: -190141.906250\n",
      "Train Epoch: 35 [15448/17352 (89%)] Loss: -81001.671875\n",
      "Train Epoch: 35 [16086/17352 (93%)] Loss: -43200.425781\n",
      "Train Epoch: 35 [16889/17352 (97%)] Loss: -22204.107422\n",
      "    epoch          : 35\n",
      "    loss           : -131672.77818267618\n",
      "    val_loss       : -56702.70160315831\n",
      "Train Epoch: 36 [128/17352 (1%)] Loss: -198464.656250\n",
      "Train Epoch: 36 [1536/17352 (9%)] Loss: -209462.484375\n",
      "Train Epoch: 36 [2944/17352 (17%)] Loss: -111615.421875\n",
      "Train Epoch: 36 [4352/17352 (25%)] Loss: -202082.015625\n",
      "Train Epoch: 36 [5760/17352 (33%)] Loss: -206280.500000\n",
      "Train Epoch: 36 [7168/17352 (41%)] Loss: -190118.812500\n",
      "Train Epoch: 36 [8576/17352 (49%)] Loss: -165701.875000\n",
      "Train Epoch: 36 [9984/17352 (58%)] Loss: -66251.187500\n",
      "Train Epoch: 36 [11392/17352 (66%)] Loss: 10233.257812\n",
      "Train Epoch: 36 [12800/17352 (74%)] Loss: -92527.125000\n",
      "Train Epoch: 36 [14208/17352 (82%)] Loss: -207999.312500\n",
      "Train Epoch: 36 [15468/17352 (89%)] Loss: -2874.799805\n",
      "Train Epoch: 36 [16357/17352 (94%)] Loss: -147015.078125\n",
      "Train Epoch: 36 [16957/17352 (98%)] Loss: -22869.689453\n",
      "    epoch          : 36\n",
      "    loss           : -136714.25439453125\n",
      "    val_loss       : -60885.11487846375\n",
      "Train Epoch: 37 [128/17352 (1%)] Loss: -197979.187500\n",
      "Train Epoch: 37 [1536/17352 (9%)] Loss: -224955.046875\n",
      "Train Epoch: 37 [2944/17352 (17%)] Loss: -205215.390625\n",
      "Train Epoch: 37 [4352/17352 (25%)] Loss: -135002.984375\n",
      "Train Epoch: 37 [5760/17352 (33%)] Loss: -91431.695312\n",
      "Train Epoch: 37 [7168/17352 (41%)] Loss: -186205.843750\n",
      "Train Epoch: 37 [8576/17352 (49%)] Loss: -155907.765625\n",
      "Train Epoch: 37 [9984/17352 (58%)] Loss: 19509.609375\n",
      "Train Epoch: 37 [11392/17352 (66%)] Loss: -147618.171875\n",
      "Train Epoch: 37 [12800/17352 (74%)] Loss: 100943.437500\n",
      "Train Epoch: 37 [14208/17352 (82%)] Loss: -161463.468750\n",
      "Train Epoch: 37 [15423/17352 (89%)] Loss: -72001.343750\n",
      "Train Epoch: 37 [16239/17352 (94%)] Loss: -140699.296875\n",
      "Train Epoch: 37 [16889/17352 (97%)] Loss: -128220.656250\n",
      "    epoch          : 37\n",
      "    loss           : -108766.78529060927\n",
      "    val_loss       : -72758.89712317784\n",
      "Train Epoch: 38 [128/17352 (1%)] Loss: -108821.523438\n",
      "Train Epoch: 38 [1536/17352 (9%)] Loss: -201611.656250\n",
      "Train Epoch: 38 [2944/17352 (17%)] Loss: -173409.562500\n",
      "Train Epoch: 38 [4352/17352 (25%)] Loss: -208523.625000\n",
      "Train Epoch: 38 [5760/17352 (33%)] Loss: -116327.335938\n",
      "Train Epoch: 38 [7168/17352 (41%)] Loss: -137413.578125\n",
      "Train Epoch: 38 [8576/17352 (49%)] Loss: -62658.378906\n",
      "Train Epoch: 38 [9984/17352 (58%)] Loss: -43609.640625\n",
      "Train Epoch: 38 [11392/17352 (66%)] Loss: -209326.218750\n",
      "Train Epoch: 38 [12800/17352 (74%)] Loss: -224575.015625\n",
      "Train Epoch: 38 [14208/17352 (82%)] Loss: -198067.296875\n",
      "Train Epoch: 38 [15409/17352 (89%)] Loss: -57256.851562\n",
      "Train Epoch: 38 [16309/17352 (94%)] Loss: -94347.578125\n",
      "Train Epoch: 38 [17016/17352 (98%)] Loss: -5821.726562\n",
      "    epoch          : 38\n",
      "    loss           : -130094.5305159396\n",
      "    val_loss       : -66167.30603100458\n",
      "Train Epoch: 39 [128/17352 (1%)] Loss: -189651.046875\n",
      "Train Epoch: 39 [1536/17352 (9%)] Loss: -214438.343750\n",
      "Train Epoch: 39 [2944/17352 (17%)] Loss: -137425.625000\n",
      "Train Epoch: 39 [4352/17352 (25%)] Loss: 39707.714844\n",
      "Train Epoch: 39 [5760/17352 (33%)] Loss: -237457.921875\n",
      "Train Epoch: 39 [7168/17352 (41%)] Loss: -32282.679688\n",
      "Train Epoch: 39 [8576/17352 (49%)] Loss: -208559.531250\n",
      "Train Epoch: 39 [9984/17352 (58%)] Loss: -225946.109375\n",
      "Train Epoch: 39 [11392/17352 (66%)] Loss: -180304.218750\n",
      "Train Epoch: 39 [12800/17352 (74%)] Loss: 22059.003906\n",
      "Train Epoch: 39 [14208/17352 (82%)] Loss: -161267.156250\n",
      "Train Epoch: 39 [15523/17352 (89%)] Loss: -105445.859375\n",
      "Train Epoch: 39 [16043/17352 (92%)] Loss: -61559.671875\n",
      "Train Epoch: 39 [17023/17352 (98%)] Loss: -122424.781250\n",
      "    epoch          : 39\n",
      "    loss           : -113086.03598042943\n",
      "    val_loss       : -80041.05045038859\n",
      "Train Epoch: 40 [128/17352 (1%)] Loss: -68232.015625\n",
      "Train Epoch: 40 [1536/17352 (9%)] Loss: -107468.382812\n",
      "Train Epoch: 40 [2944/17352 (17%)] Loss: -143188.312500\n",
      "Train Epoch: 40 [4352/17352 (25%)] Loss: -171595.328125\n",
      "Train Epoch: 40 [5760/17352 (33%)] Loss: -225480.656250\n",
      "Train Epoch: 40 [7168/17352 (41%)] Loss: -93039.734375\n",
      "Train Epoch: 40 [8576/17352 (49%)] Loss: -147714.390625\n",
      "Train Epoch: 40 [9984/17352 (58%)] Loss: -151646.703125\n",
      "Train Epoch: 40 [11392/17352 (66%)] Loss: 30191.820312\n",
      "Train Epoch: 40 [12800/17352 (74%)] Loss: -220431.734375\n",
      "Train Epoch: 40 [14208/17352 (82%)] Loss: -177994.156250\n",
      "Train Epoch: 40 [15479/17352 (89%)] Loss: -20585.835938\n",
      "Train Epoch: 40 [16352/17352 (94%)] Loss: -82916.296875\n",
      "Train Epoch: 40 [16959/17352 (98%)] Loss: -84950.718750\n",
      "    epoch          : 40\n",
      "    loss           : -127241.56526583472\n",
      "    val_loss       : -73185.46527112325\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0913_161229/checkpoint-epoch40.pth ...\n",
      "Train Epoch: 41 [128/17352 (1%)] Loss: -150848.718750\n",
      "Train Epoch: 41 [1536/17352 (9%)] Loss: -205373.078125\n",
      "Train Epoch: 41 [2944/17352 (17%)] Loss: -179927.312500\n",
      "Train Epoch: 41 [4352/17352 (25%)] Loss: -203244.687500\n",
      "Train Epoch: 41 [5760/17352 (33%)] Loss: -227715.750000\n",
      "Train Epoch: 41 [7168/17352 (41%)] Loss: -234765.453125\n",
      "Train Epoch: 41 [8576/17352 (49%)] Loss: -145808.406250\n",
      "Train Epoch: 41 [9984/17352 (58%)] Loss: -191320.296875\n",
      "Train Epoch: 41 [11392/17352 (66%)] Loss: -211195.187500\n",
      "Train Epoch: 41 [12800/17352 (74%)] Loss: -221463.859375\n",
      "Train Epoch: 41 [14208/17352 (82%)] Loss: -78655.062500\n",
      "Train Epoch: 41 [15368/17352 (89%)] Loss: -5263.875977\n",
      "Train Epoch: 41 [16286/17352 (94%)] Loss: -92172.921875\n",
      "Train Epoch: 41 [17095/17352 (99%)] Loss: -44818.921875\n",
      "    epoch          : 41\n",
      "    loss           : -127910.97640192429\n",
      "    val_loss       : -86063.63032251994\n",
      "Train Epoch: 42 [128/17352 (1%)] Loss: -156868.718750\n",
      "Train Epoch: 42 [1536/17352 (9%)] Loss: -182837.250000\n",
      "Train Epoch: 42 [2944/17352 (17%)] Loss: -163924.671875\n",
      "Train Epoch: 42 [4352/17352 (25%)] Loss: -188483.468750\n",
      "Train Epoch: 42 [5760/17352 (33%)] Loss: -67559.109375\n",
      "Train Epoch: 42 [7168/17352 (41%)] Loss: 18206.875000\n",
      "Train Epoch: 42 [8576/17352 (49%)] Loss: -204793.656250\n",
      "Train Epoch: 42 [9984/17352 (58%)] Loss: -234312.359375\n",
      "Train Epoch: 42 [11392/17352 (66%)] Loss: -44966.382812\n",
      "Train Epoch: 42 [12800/17352 (74%)] Loss: -223504.781250\n",
      "Train Epoch: 42 [14208/17352 (82%)] Loss: -226475.031250\n",
      "Train Epoch: 42 [15450/17352 (89%)] Loss: -148392.687500\n",
      "Train Epoch: 42 [16374/17352 (94%)] Loss: -64021.937500\n",
      "Train Epoch: 42 [17126/17352 (99%)] Loss: -164254.250000\n",
      "    epoch          : 42\n",
      "    loss           : -125116.45725408976\n",
      "    val_loss       : -66313.73714470863\n",
      "Train Epoch: 43 [128/17352 (1%)] Loss: -9698.503906\n",
      "Train Epoch: 43 [1536/17352 (9%)] Loss: -192472.078125\n",
      "Train Epoch: 43 [2944/17352 (17%)] Loss: -137373.843750\n",
      "Train Epoch: 43 [4352/17352 (25%)] Loss: -53238.390625\n",
      "Train Epoch: 43 [5760/17352 (33%)] Loss: -148387.562500\n",
      "Train Epoch: 43 [7168/17352 (41%)] Loss: -64403.281250\n",
      "Train Epoch: 43 [8576/17352 (49%)] Loss: -12221.769531\n",
      "Train Epoch: 43 [9984/17352 (58%)] Loss: -219549.640625\n",
      "Train Epoch: 43 [11392/17352 (66%)] Loss: -219431.937500\n",
      "Train Epoch: 43 [12800/17352 (74%)] Loss: -202402.062500\n",
      "Train Epoch: 43 [14208/17352 (82%)] Loss: -170321.578125\n",
      "Train Epoch: 43 [15376/17352 (89%)] Loss: -17234.535156\n",
      "Train Epoch: 43 [16192/17352 (93%)] Loss: -227958.937500\n",
      "Train Epoch: 43 [16969/17352 (98%)] Loss: -49104.152344\n",
      "    epoch          : 43\n",
      "    loss           : -127020.62459364513\n",
      "    val_loss       : -71720.65381072363\n",
      "Train Epoch: 44 [128/17352 (1%)] Loss: -131188.875000\n",
      "Train Epoch: 44 [1536/17352 (9%)] Loss: -86197.859375\n",
      "Train Epoch: 44 [2944/17352 (17%)] Loss: -208481.390625\n",
      "Train Epoch: 44 [4352/17352 (25%)] Loss: -8707.574219\n",
      "Train Epoch: 44 [5760/17352 (33%)] Loss: -224663.437500\n",
      "Train Epoch: 44 [7168/17352 (41%)] Loss: -205976.093750\n",
      "Train Epoch: 44 [8576/17352 (49%)] Loss: -150630.953125\n",
      "Train Epoch: 44 [9984/17352 (58%)] Loss: -208157.343750\n",
      "Train Epoch: 44 [11392/17352 (66%)] Loss: -172110.906250\n",
      "Train Epoch: 44 [12800/17352 (74%)] Loss: -209751.171875\n",
      "Train Epoch: 44 [14208/17352 (82%)] Loss: -59258.578125\n",
      "Train Epoch: 44 [15554/17352 (90%)] Loss: -192137.546875\n",
      "Train Epoch: 44 [16152/17352 (93%)] Loss: 2147.371094\n",
      "Train Epoch: 44 [17013/17352 (98%)] Loss: -141989.750000\n",
      "    epoch          : 44\n",
      "    loss           : -112757.41724697856\n",
      "    val_loss       : -72279.57101629575\n",
      "Train Epoch: 45 [128/17352 (1%)] Loss: -197773.109375\n",
      "Train Epoch: 45 [1536/17352 (9%)] Loss: -198797.250000\n",
      "Train Epoch: 45 [2944/17352 (17%)] Loss: -186450.750000\n",
      "Train Epoch: 45 [4352/17352 (25%)] Loss: -164931.593750\n",
      "Train Epoch: 45 [5760/17352 (33%)] Loss: -212001.421875\n",
      "Train Epoch: 45 [7168/17352 (41%)] Loss: -190206.390625\n",
      "Train Epoch: 45 [8576/17352 (49%)] Loss: -109288.562500\n",
      "Train Epoch: 45 [9984/17352 (58%)] Loss: -152106.046875\n",
      "Train Epoch: 45 [11392/17352 (66%)] Loss: -227149.203125\n",
      "Train Epoch: 45 [12800/17352 (74%)] Loss: -158438.203125\n",
      "Train Epoch: 45 [14208/17352 (82%)] Loss: -176253.656250\n",
      "Train Epoch: 45 [15553/17352 (90%)] Loss: -34564.011719\n",
      "Train Epoch: 45 [16315/17352 (94%)] Loss: -116069.773438\n",
      "Train Epoch: 45 [16914/17352 (97%)] Loss: -143396.984375\n",
      "    epoch          : 45\n",
      "    loss           : -128726.83336173448\n",
      "    val_loss       : -81514.25384661356\n",
      "Train Epoch: 46 [128/17352 (1%)] Loss: -219777.375000\n",
      "Train Epoch: 46 [1536/17352 (9%)] Loss: -148464.125000\n",
      "Train Epoch: 46 [2944/17352 (17%)] Loss: -199109.812500\n",
      "Train Epoch: 46 [4352/17352 (25%)] Loss: -214185.937500\n",
      "Train Epoch: 46 [5760/17352 (33%)] Loss: -146558.687500\n",
      "Train Epoch: 46 [7168/17352 (41%)] Loss: -139503.156250\n",
      "Train Epoch: 46 [8576/17352 (49%)] Loss: 24594.488281\n",
      "Train Epoch: 46 [9984/17352 (58%)] Loss: -153667.765625\n",
      "Train Epoch: 46 [11392/17352 (66%)] Loss: -115362.734375\n",
      "Train Epoch: 46 [12800/17352 (74%)] Loss: 57454.968750\n",
      "Train Epoch: 46 [14208/17352 (82%)] Loss: -140226.781250\n",
      "Train Epoch: 46 [15529/17352 (89%)] Loss: -193590.609375\n",
      "Train Epoch: 46 [16359/17352 (94%)] Loss: -229443.046875\n",
      "Train Epoch: 46 [17049/17352 (98%)] Loss: -39102.804688\n",
      "    epoch          : 46\n",
      "    loss           : -131079.55660130034\n",
      "    val_loss       : -65270.516383298236\n",
      "Train Epoch: 47 [128/17352 (1%)] Loss: 24506.265625\n",
      "Train Epoch: 47 [1536/17352 (9%)] Loss: -153041.375000\n",
      "Train Epoch: 47 [2944/17352 (17%)] Loss: -181348.875000\n",
      "Train Epoch: 47 [4352/17352 (25%)] Loss: -93887.375000\n",
      "Train Epoch: 47 [5760/17352 (33%)] Loss: -168468.437500\n",
      "Train Epoch: 47 [7168/17352 (41%)] Loss: -129941.671875\n",
      "Train Epoch: 47 [8576/17352 (49%)] Loss: -219061.171875\n",
      "Train Epoch: 47 [9984/17352 (58%)] Loss: 78705.078125\n",
      "Train Epoch: 47 [11392/17352 (66%)] Loss: -122151.015625\n",
      "Train Epoch: 47 [12800/17352 (74%)] Loss: -229412.734375\n",
      "Train Epoch: 47 [14208/17352 (82%)] Loss: -163602.031250\n",
      "Train Epoch: 47 [15550/17352 (90%)] Loss: -86030.156250\n",
      "Train Epoch: 47 [16323/17352 (94%)] Loss: -140331.890625\n",
      "Train Epoch: 47 [17066/17352 (98%)] Loss: -23720.035156\n",
      "    epoch          : 47\n",
      "    loss           : -131358.9446259569\n",
      "    val_loss       : -64687.15838743846\n",
      "Train Epoch: 48 [128/17352 (1%)] Loss: -227617.859375\n",
      "Train Epoch: 48 [1536/17352 (9%)] Loss: -53710.429688\n",
      "Train Epoch: 48 [2944/17352 (17%)] Loss: 92323.062500\n",
      "Train Epoch: 48 [4352/17352 (25%)] Loss: -182388.093750\n",
      "Train Epoch: 48 [5760/17352 (33%)] Loss: -230723.187500\n",
      "Train Epoch: 48 [7168/17352 (41%)] Loss: -203778.906250\n",
      "Train Epoch: 48 [8576/17352 (49%)] Loss: -155834.328125\n",
      "Train Epoch: 48 [9984/17352 (58%)] Loss: 211164.296875\n",
      "Train Epoch: 48 [11392/17352 (66%)] Loss: -230737.187500\n",
      "Train Epoch: 48 [12800/17352 (74%)] Loss: -80252.734375\n",
      "Train Epoch: 48 [14208/17352 (82%)] Loss: -213645.781250\n",
      "Train Epoch: 48 [15397/17352 (89%)] Loss: -63842.910156\n",
      "Train Epoch: 48 [16307/17352 (94%)] Loss: -165840.875000\n",
      "Train Epoch: 48 [17042/17352 (98%)] Loss: 8091.117188\n",
      "    epoch          : 48\n",
      "    loss           : -121933.5895357068\n",
      "    val_loss       : -69043.29467185338\n",
      "Train Epoch: 49 [128/17352 (1%)] Loss: -220708.125000\n",
      "Train Epoch: 49 [1536/17352 (9%)] Loss: -235963.171875\n",
      "Train Epoch: 49 [2944/17352 (17%)] Loss: -36559.347656\n",
      "Train Epoch: 49 [4352/17352 (25%)] Loss: -136364.140625\n",
      "Train Epoch: 49 [5760/17352 (33%)] Loss: -144912.171875\n",
      "Train Epoch: 49 [7168/17352 (41%)] Loss: -235238.593750\n",
      "Train Epoch: 49 [8576/17352 (49%)] Loss: -216918.156250\n",
      "Train Epoch: 49 [9984/17352 (58%)] Loss: 189176.687500\n",
      "Train Epoch: 49 [11392/17352 (66%)] Loss: 19167.906250\n",
      "Train Epoch: 49 [12800/17352 (74%)] Loss: -154586.734375\n",
      "Train Epoch: 49 [14208/17352 (82%)] Loss: -64112.593750\n",
      "Train Epoch: 49 [15427/17352 (89%)] Loss: -38775.796875\n",
      "Train Epoch: 49 [16100/17352 (93%)] Loss: -47443.609375\n",
      "Train Epoch: 49 [16897/17352 (97%)] Loss: -33574.925781\n",
      "    epoch          : 49\n",
      "    loss           : -119577.96338054478\n",
      "    val_loss       : -79353.69865849814\n",
      "Train Epoch: 50 [128/17352 (1%)] Loss: -178484.515625\n",
      "Train Epoch: 50 [1536/17352 (9%)] Loss: -221902.046875\n",
      "Train Epoch: 50 [2944/17352 (17%)] Loss: -14052.610352\n",
      "Train Epoch: 50 [4352/17352 (25%)] Loss: -223101.718750\n",
      "Train Epoch: 50 [5760/17352 (33%)] Loss: -7265.269531\n",
      "Train Epoch: 50 [7168/17352 (41%)] Loss: 89553.453125\n",
      "Train Epoch: 50 [8576/17352 (49%)] Loss: -197060.718750\n",
      "Train Epoch: 50 [9984/17352 (58%)] Loss: -138620.187500\n",
      "Train Epoch: 50 [11392/17352 (66%)] Loss: -71276.656250\n",
      "Train Epoch: 50 [12800/17352 (74%)] Loss: -101067.460938\n",
      "Train Epoch: 50 [14208/17352 (82%)] Loss: -220818.375000\n",
      "Train Epoch: 50 [15472/17352 (89%)] Loss: -116414.406250\n",
      "Train Epoch: 50 [16279/17352 (94%)] Loss: -56911.984375\n",
      "Train Epoch: 50 [17094/17352 (99%)] Loss: -17686.269531\n",
      "    epoch          : 50\n",
      "    loss           : -128584.97748990667\n",
      "    val_loss       : -68064.58653465907\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0913_161229/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [128/17352 (1%)] Loss: 45955.375000\n",
      "Train Epoch: 51 [1536/17352 (9%)] Loss: -225490.218750\n",
      "Train Epoch: 51 [2944/17352 (17%)] Loss: -236781.843750\n",
      "Train Epoch: 51 [4352/17352 (25%)] Loss: -39554.445312\n",
      "Train Epoch: 51 [5760/17352 (33%)] Loss: -198190.500000\n",
      "Train Epoch: 51 [7168/17352 (41%)] Loss: -127484.929688\n",
      "Train Epoch: 51 [8576/17352 (49%)] Loss: 88439.250000\n",
      "Train Epoch: 51 [9984/17352 (58%)] Loss: -172522.031250\n",
      "Train Epoch: 51 [11392/17352 (66%)] Loss: -198556.562500\n",
      "Train Epoch: 51 [12800/17352 (74%)] Loss: -222637.937500\n",
      "Train Epoch: 51 [14208/17352 (82%)] Loss: -206504.875000\n",
      "Train Epoch: 51 [15588/17352 (90%)] Loss: -57210.199219\n",
      "Train Epoch: 51 [16383/17352 (94%)] Loss: -122385.234375\n",
      "Train Epoch: 51 [17157/17352 (99%)] Loss: -30264.171875\n",
      "    epoch          : 51\n",
      "    loss           : -125063.29950647547\n",
      "    val_loss       : -63711.95833261808\n",
      "Train Epoch: 52 [128/17352 (1%)] Loss: -135555.218750\n",
      "Train Epoch: 52 [1536/17352 (9%)] Loss: -236238.390625\n",
      "Train Epoch: 52 [2944/17352 (17%)] Loss: -192482.937500\n",
      "Train Epoch: 52 [4352/17352 (25%)] Loss: -172643.062500\n",
      "Train Epoch: 52 [5760/17352 (33%)] Loss: -202949.375000\n",
      "Train Epoch: 52 [7168/17352 (41%)] Loss: -179527.093750\n",
      "Train Epoch: 52 [8576/17352 (49%)] Loss: -159680.500000\n",
      "Train Epoch: 52 [9984/17352 (58%)] Loss: 22498.226562\n",
      "Train Epoch: 52 [11392/17352 (66%)] Loss: -60355.796875\n",
      "Train Epoch: 52 [12800/17352 (74%)] Loss: 93935.804688\n",
      "Train Epoch: 52 [14208/17352 (82%)] Loss: -183699.406250\n",
      "Train Epoch: 52 [15508/17352 (89%)] Loss: -186337.406250\n",
      "Train Epoch: 52 [16492/17352 (95%)] Loss: -3153.681641\n",
      "Train Epoch: 52 [17001/17352 (98%)] Loss: -45532.593750\n",
      "    epoch          : 52\n",
      "    loss           : -121993.05096230732\n",
      "    val_loss       : -76350.42448860804\n",
      "Train Epoch: 53 [128/17352 (1%)] Loss: -228423.656250\n",
      "Train Epoch: 53 [1536/17352 (9%)] Loss: -147537.640625\n",
      "Train Epoch: 53 [2944/17352 (17%)] Loss: 90112.484375\n",
      "Train Epoch: 53 [4352/17352 (25%)] Loss: -153591.859375\n",
      "Train Epoch: 53 [5760/17352 (33%)] Loss: -215943.687500\n",
      "Train Epoch: 53 [7168/17352 (41%)] Loss: -219915.218750\n",
      "Train Epoch: 53 [8576/17352 (49%)] Loss: -35548.886719\n",
      "Train Epoch: 53 [9984/17352 (58%)] Loss: -170660.296875\n",
      "Train Epoch: 53 [11392/17352 (66%)] Loss: -204543.359375\n",
      "Train Epoch: 53 [12800/17352 (74%)] Loss: -77539.609375\n",
      "Train Epoch: 53 [14208/17352 (82%)] Loss: -37915.273438\n",
      "Train Epoch: 53 [15521/17352 (89%)] Loss: -228377.906250\n",
      "Train Epoch: 53 [16387/17352 (94%)] Loss: -140152.781250\n",
      "Train Epoch: 53 [17081/17352 (98%)] Loss: -64138.539062\n",
      "    epoch          : 53\n",
      "    loss           : -131763.45019367398\n",
      "    val_loss       : -69691.69056835174\n",
      "Train Epoch: 54 [128/17352 (1%)] Loss: -227148.265625\n",
      "Train Epoch: 54 [1536/17352 (9%)] Loss: -199999.484375\n",
      "Train Epoch: 54 [2944/17352 (17%)] Loss: -205549.250000\n",
      "Train Epoch: 54 [4352/17352 (25%)] Loss: -44801.207031\n",
      "Train Epoch: 54 [5760/17352 (33%)] Loss: -201273.218750\n",
      "Train Epoch: 54 [7168/17352 (41%)] Loss: -224863.937500\n",
      "Train Epoch: 54 [8576/17352 (49%)] Loss: -215158.546875\n",
      "Train Epoch: 54 [9984/17352 (58%)] Loss: -238180.015625\n",
      "Train Epoch: 54 [11392/17352 (66%)] Loss: -222881.578125\n",
      "Train Epoch: 54 [12800/17352 (74%)] Loss: -179470.515625\n",
      "Train Epoch: 54 [14208/17352 (82%)] Loss: -203078.187500\n",
      "Train Epoch: 54 [15550/17352 (90%)] Loss: -84151.492188\n",
      "Train Epoch: 54 [16323/17352 (94%)] Loss: -63937.031250\n",
      "Train Epoch: 54 [17111/17352 (99%)] Loss: -80342.156250\n",
      "    epoch          : 54\n",
      "    loss           : -140380.8247578256\n",
      "    val_loss       : -76662.46251813571\n",
      "Train Epoch: 55 [128/17352 (1%)] Loss: -181869.968750\n",
      "Train Epoch: 55 [1536/17352 (9%)] Loss: -213859.812500\n",
      "Train Epoch: 55 [2944/17352 (17%)] Loss: -214264.531250\n",
      "Train Epoch: 55 [4352/17352 (25%)] Loss: -29662.242188\n",
      "Train Epoch: 55 [5760/17352 (33%)] Loss: -170412.375000\n",
      "Train Epoch: 55 [7168/17352 (41%)] Loss: -209726.140625\n",
      "Train Epoch: 55 [8576/17352 (49%)] Loss: -52455.683594\n",
      "Train Epoch: 55 [9984/17352 (58%)] Loss: 24966.027344\n",
      "Train Epoch: 55 [11392/17352 (66%)] Loss: -227106.796875\n",
      "Train Epoch: 55 [12800/17352 (74%)] Loss: 83182.414062\n",
      "Train Epoch: 55 [14208/17352 (82%)] Loss: -234301.468750\n",
      "Train Epoch: 55 [15554/17352 (90%)] Loss: -190427.375000\n",
      "Train Epoch: 55 [16188/17352 (93%)] Loss: -42014.187500\n",
      "Train Epoch: 55 [17123/17352 (99%)] Loss: -68395.203125\n",
      "    epoch          : 55\n",
      "    loss           : -135109.82297838453\n",
      "    val_loss       : -77184.96634494464\n",
      "Train Epoch: 56 [128/17352 (1%)] Loss: -137213.296875\n",
      "Train Epoch: 56 [1536/17352 (9%)] Loss: -97575.406250\n",
      "Train Epoch: 56 [2944/17352 (17%)] Loss: -146849.859375\n",
      "Train Epoch: 56 [4352/17352 (25%)] Loss: -209320.937500\n",
      "Train Epoch: 56 [5760/17352 (33%)] Loss: -185617.296875\n",
      "Train Epoch: 56 [7168/17352 (41%)] Loss: -222266.421875\n",
      "Train Epoch: 56 [8576/17352 (49%)] Loss: -174726.968750\n",
      "Train Epoch: 56 [9984/17352 (58%)] Loss: -229948.828125\n",
      "Train Epoch: 56 [11392/17352 (66%)] Loss: -220787.578125\n",
      "Train Epoch: 56 [12800/17352 (74%)] Loss: -79197.117188\n",
      "Train Epoch: 56 [14208/17352 (82%)] Loss: -149075.812500\n",
      "Train Epoch: 56 [15466/17352 (89%)] Loss: -8264.328125\n",
      "Train Epoch: 56 [16375/17352 (94%)] Loss: -87592.195312\n",
      "Train Epoch: 56 [17054/17352 (98%)] Loss: -18798.562500\n",
      "    epoch          : 56\n",
      "    loss           : -129159.15537174916\n",
      "    val_loss       : -57382.486190636955\n",
      "Train Epoch: 57 [128/17352 (1%)] Loss: -40035.980469\n",
      "Train Epoch: 57 [1536/17352 (9%)] Loss: -201499.562500\n",
      "Train Epoch: 57 [2944/17352 (17%)] Loss: -121925.328125\n",
      "Train Epoch: 57 [4352/17352 (25%)] Loss: -220785.203125\n",
      "Train Epoch: 57 [5760/17352 (33%)] Loss: -206857.968750\n",
      "Train Epoch: 57 [7168/17352 (41%)] Loss: -184441.531250\n",
      "Train Epoch: 57 [8576/17352 (49%)] Loss: -144560.640625\n",
      "Train Epoch: 57 [9984/17352 (58%)] Loss: -222331.156250\n",
      "Train Epoch: 57 [11392/17352 (66%)] Loss: -196142.968750\n",
      "Train Epoch: 57 [12800/17352 (74%)] Loss: -216841.515625\n",
      "Train Epoch: 57 [14208/17352 (82%)] Loss: -116599.617188\n",
      "Train Epoch: 57 [15455/17352 (89%)] Loss: -7792.012695\n",
      "Train Epoch: 57 [16354/17352 (94%)] Loss: -120893.546875\n",
      "Train Epoch: 57 [17090/17352 (98%)] Loss: -92539.875000\n",
      "    epoch          : 57\n",
      "    loss           : -132180.54820220743\n",
      "    val_loss       : -73450.32621849378\n",
      "Train Epoch: 58 [128/17352 (1%)] Loss: -223903.625000\n",
      "Train Epoch: 58 [1536/17352 (9%)] Loss: -214868.468750\n",
      "Train Epoch: 58 [2944/17352 (17%)] Loss: -194030.218750\n",
      "Train Epoch: 58 [4352/17352 (25%)] Loss: 60559.976562\n",
      "Train Epoch: 58 [5760/17352 (33%)] Loss: -188150.906250\n",
      "Train Epoch: 58 [7168/17352 (41%)] Loss: -188509.515625\n",
      "Train Epoch: 58 [8576/17352 (49%)] Loss: -111725.406250\n",
      "Train Epoch: 58 [9984/17352 (58%)] Loss: -166580.765625\n",
      "Train Epoch: 58 [11392/17352 (66%)] Loss: -220130.218750\n",
      "Train Epoch: 58 [12800/17352 (74%)] Loss: -103976.820312\n",
      "Train Epoch: 58 [14208/17352 (82%)] Loss: -193055.218750\n",
      "Train Epoch: 58 [15541/17352 (90%)] Loss: -227893.312500\n",
      "Train Epoch: 58 [16207/17352 (93%)] Loss: -3290.986572\n",
      "Train Epoch: 58 [17010/17352 (98%)] Loss: -167328.578125\n",
      "    epoch          : 58\n",
      "    loss           : -135584.00314433462\n",
      "    val_loss       : -82361.02513322831\n",
      "Train Epoch: 59 [128/17352 (1%)] Loss: -171134.171875\n",
      "Train Epoch: 59 [1536/17352 (9%)] Loss: -201387.250000\n",
      "Train Epoch: 59 [2944/17352 (17%)] Loss: -197617.281250\n",
      "Train Epoch: 59 [4352/17352 (25%)] Loss: 81227.195312\n",
      "Train Epoch: 59 [5760/17352 (33%)] Loss: -210800.937500\n",
      "Train Epoch: 59 [7168/17352 (41%)] Loss: -162319.718750\n",
      "Train Epoch: 59 [8576/17352 (49%)] Loss: -163822.281250\n",
      "Train Epoch: 59 [9984/17352 (58%)] Loss: -179168.015625\n",
      "Train Epoch: 59 [11392/17352 (66%)] Loss: 61494.125000\n",
      "Train Epoch: 59 [12800/17352 (74%)] Loss: -210431.250000\n",
      "Train Epoch: 59 [14208/17352 (82%)] Loss: -31690.191406\n",
      "Train Epoch: 59 [15429/17352 (89%)] Loss: -26621.355469\n",
      "Train Epoch: 59 [16331/17352 (94%)] Loss: -161657.796875\n",
      "Train Epoch: 59 [17000/17352 (98%)] Loss: -21376.250000\n",
      "    epoch          : 59\n",
      "    loss           : -121027.79071331664\n",
      "    val_loss       : -65930.86554025015\n",
      "Train Epoch: 60 [128/17352 (1%)] Loss: -154962.843750\n",
      "Train Epoch: 60 [1536/17352 (9%)] Loss: -223989.390625\n",
      "Train Epoch: 60 [2944/17352 (17%)] Loss: -127122.906250\n",
      "Train Epoch: 60 [4352/17352 (25%)] Loss: -197803.796875\n",
      "Train Epoch: 60 [5760/17352 (33%)] Loss: -161138.250000\n",
      "Train Epoch: 60 [7168/17352 (41%)] Loss: -234270.734375\n",
      "Train Epoch: 60 [8576/17352 (49%)] Loss: -190098.406250\n",
      "Train Epoch: 60 [9984/17352 (58%)] Loss: -200744.671875\n",
      "Train Epoch: 60 [11392/17352 (66%)] Loss: 129148.414062\n",
      "Train Epoch: 60 [12800/17352 (74%)] Loss: -226615.359375\n",
      "Train Epoch: 60 [14208/17352 (82%)] Loss: -143034.765625\n",
      "Train Epoch: 60 [15548/17352 (90%)] Loss: -58439.691406\n",
      "Train Epoch: 60 [16138/17352 (93%)] Loss: -26416.640625\n",
      "Train Epoch: 60 [17043/17352 (98%)] Loss: -10530.799805\n",
      "    epoch          : 60\n",
      "    loss           : -136403.81227224466\n",
      "    val_loss       : -69636.85884308815\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0913_161229/checkpoint-epoch60.pth ...\n",
      "Train Epoch: 61 [128/17352 (1%)] Loss: -228119.812500\n",
      "Train Epoch: 61 [1536/17352 (9%)] Loss: -81451.335938\n",
      "Train Epoch: 61 [2944/17352 (17%)] Loss: -48391.304688\n",
      "Train Epoch: 61 [4352/17352 (25%)] Loss: -224967.125000\n",
      "Train Epoch: 61 [5760/17352 (33%)] Loss: -101332.804688\n",
      "Train Epoch: 61 [7168/17352 (41%)] Loss: -159365.703125\n",
      "Train Epoch: 61 [8576/17352 (49%)] Loss: -239985.625000\n",
      "Train Epoch: 61 [9984/17352 (58%)] Loss: 117882.234375\n",
      "Train Epoch: 61 [11392/17352 (66%)] Loss: -103586.898438\n",
      "Train Epoch: 61 [12800/17352 (74%)] Loss: -113819.625000\n",
      "Train Epoch: 61 [14208/17352 (82%)] Loss: -53715.859375\n",
      "Train Epoch: 61 [15508/17352 (89%)] Loss: -79766.929688\n",
      "Train Epoch: 61 [16483/17352 (95%)] Loss: -187628.843750\n",
      "Train Epoch: 61 [17137/17352 (99%)] Loss: 89763.312500\n",
      "    epoch          : 61\n",
      "    loss           : -122437.68994468331\n",
      "    val_loss       : -66010.39214779536\n",
      "Train Epoch: 62 [128/17352 (1%)] Loss: -38420.781250\n",
      "Train Epoch: 62 [1536/17352 (9%)] Loss: -120833.843750\n",
      "Train Epoch: 62 [2944/17352 (17%)] Loss: -151346.968750\n",
      "Train Epoch: 62 [4352/17352 (25%)] Loss: -186458.125000\n",
      "Train Epoch: 62 [5760/17352 (33%)] Loss: -182284.812500\n",
      "Train Epoch: 62 [7168/17352 (41%)] Loss: -156825.546875\n",
      "Train Epoch: 62 [8576/17352 (49%)] Loss: -204089.375000\n",
      "Train Epoch: 62 [9984/17352 (58%)] Loss: -167831.000000\n",
      "Train Epoch: 62 [11392/17352 (66%)] Loss: -166454.906250\n",
      "Train Epoch: 62 [12800/17352 (74%)] Loss: -152006.390625\n",
      "Train Epoch: 62 [14208/17352 (82%)] Loss: -225852.281250\n",
      "Train Epoch: 62 [15570/17352 (90%)] Loss: -108107.632812\n",
      "Train Epoch: 62 [16283/17352 (94%)] Loss: -62972.046875\n",
      "Train Epoch: 62 [17173/17352 (99%)] Loss: -108014.609375\n",
      "    epoch          : 62\n",
      "    loss           : -124284.95664947305\n",
      "    val_loss       : -63774.51414100329\n",
      "Train Epoch: 63 [128/17352 (1%)] Loss: -55532.714844\n",
      "Train Epoch: 63 [1536/17352 (9%)] Loss: -235040.234375\n",
      "Train Epoch: 63 [2944/17352 (17%)] Loss: -214443.500000\n",
      "Train Epoch: 63 [4352/17352 (25%)] Loss: -161094.562500\n",
      "Train Epoch: 63 [5760/17352 (33%)] Loss: -50422.921875\n",
      "Train Epoch: 63 [7168/17352 (41%)] Loss: -157811.437500\n",
      "Train Epoch: 63 [8576/17352 (49%)] Loss: -148044.968750\n",
      "Train Epoch: 63 [9984/17352 (58%)] Loss: 274706.406250\n",
      "Train Epoch: 63 [11392/17352 (66%)] Loss: -161538.906250\n",
      "Train Epoch: 63 [12800/17352 (74%)] Loss: -225554.781250\n",
      "Train Epoch: 63 [14208/17352 (82%)] Loss: -244081.625000\n",
      "Train Epoch: 63 [15451/17352 (89%)] Loss: -4496.336914\n",
      "Train Epoch: 63 [16193/17352 (93%)] Loss: -24344.777344\n",
      "Train Epoch: 63 [17032/17352 (98%)] Loss: -80412.851562\n",
      "    epoch          : 63\n",
      "    loss           : -124723.13768875839\n",
      "    val_loss       : -78982.81279913585\n",
      "Train Epoch: 64 [128/17352 (1%)] Loss: -195470.281250\n",
      "Train Epoch: 64 [1536/17352 (9%)] Loss: -151715.875000\n",
      "Train Epoch: 64 [2944/17352 (17%)] Loss: -209768.187500\n",
      "Train Epoch: 64 [4352/17352 (25%)] Loss: -152798.000000\n",
      "Train Epoch: 64 [5760/17352 (33%)] Loss: -213535.812500\n",
      "Train Epoch: 64 [7168/17352 (41%)] Loss: -77293.343750\n",
      "Train Epoch: 64 [8576/17352 (49%)] Loss: -231009.546875\n",
      "Train Epoch: 64 [9984/17352 (58%)] Loss: -97180.117188\n",
      "Train Epoch: 64 [11392/17352 (66%)] Loss: -227323.984375\n",
      "Train Epoch: 64 [12800/17352 (74%)] Loss: -215788.109375\n",
      "Train Epoch: 64 [14208/17352 (82%)] Loss: -202826.531250\n",
      "Train Epoch: 64 [15508/17352 (89%)] Loss: -188016.312500\n",
      "Train Epoch: 64 [16423/17352 (95%)] Loss: -145978.296875\n",
      "Train Epoch: 64 [17095/17352 (99%)] Loss: 16104.611328\n",
      "    epoch          : 64\n",
      "    loss           : -133161.49904309984\n",
      "    val_loss       : -75164.47979545593\n",
      "Train Epoch: 65 [128/17352 (1%)] Loss: 56144.171875\n",
      "Train Epoch: 65 [1536/17352 (9%)] Loss: -208132.843750\n",
      "Train Epoch: 65 [2944/17352 (17%)] Loss: -219992.921875\n",
      "Train Epoch: 65 [4352/17352 (25%)] Loss: -204024.562500\n",
      "Train Epoch: 65 [5760/17352 (33%)] Loss: 28776.937500\n",
      "Train Epoch: 65 [7168/17352 (41%)] Loss: -212395.250000\n",
      "Train Epoch: 65 [8576/17352 (49%)] Loss: -156459.265625\n",
      "Train Epoch: 65 [9984/17352 (58%)] Loss: -215485.109375\n",
      "Train Epoch: 65 [11392/17352 (66%)] Loss: -144398.468750\n",
      "Train Epoch: 65 [12800/17352 (74%)] Loss: -158701.218750\n",
      "Train Epoch: 65 [14208/17352 (82%)] Loss: -243283.125000\n",
      "Train Epoch: 65 [15523/17352 (89%)] Loss: -137478.843750\n",
      "Train Epoch: 65 [16223/17352 (93%)] Loss: -24332.324219\n",
      "Train Epoch: 65 [17021/17352 (98%)] Loss: -129629.148438\n",
      "    epoch          : 65\n",
      "    loss           : -127320.51563810822\n",
      "    val_loss       : -72234.79173606237\n",
      "Train Epoch: 66 [128/17352 (1%)] Loss: -57209.156250\n",
      "Train Epoch: 66 [1536/17352 (9%)] Loss: -22890.687500\n",
      "Train Epoch: 66 [2944/17352 (17%)] Loss: -183560.171875\n",
      "Train Epoch: 66 [4352/17352 (25%)] Loss: -158050.859375\n",
      "Train Epoch: 66 [5760/17352 (33%)] Loss: -178976.500000\n",
      "Train Epoch: 66 [7168/17352 (41%)] Loss: -108890.546875\n",
      "Train Epoch: 66 [8576/17352 (49%)] Loss: -236192.453125\n",
      "Train Epoch: 66 [9984/17352 (58%)] Loss: -211552.812500\n",
      "Train Epoch: 66 [11392/17352 (66%)] Loss: -213838.859375\n",
      "Train Epoch: 66 [12800/17352 (74%)] Loss: -218108.187500\n",
      "Train Epoch: 66 [14208/17352 (82%)] Loss: -150845.625000\n",
      "Train Epoch: 66 [15409/17352 (89%)] Loss: -25066.525391\n",
      "Train Epoch: 66 [16164/17352 (93%)] Loss: -86468.093750\n",
      "Train Epoch: 66 [16936/17352 (98%)] Loss: -64041.109375\n",
      "    epoch          : 66\n",
      "    loss           : -137862.3063325818\n",
      "    val_loss       : -69634.16816705068\n",
      "Train Epoch: 67 [128/17352 (1%)] Loss: -171594.437500\n",
      "Train Epoch: 67 [1536/17352 (9%)] Loss: -38578.753906\n",
      "Train Epoch: 67 [2944/17352 (17%)] Loss: 5025.750000\n",
      "Train Epoch: 67 [4352/17352 (25%)] Loss: -96049.257812\n",
      "Train Epoch: 67 [5760/17352 (33%)] Loss: -188899.140625\n",
      "Train Epoch: 67 [7168/17352 (41%)] Loss: -107240.875000\n",
      "Train Epoch: 67 [8576/17352 (49%)] Loss: -218679.078125\n",
      "Train Epoch: 67 [9984/17352 (58%)] Loss: -67713.062500\n",
      "Train Epoch: 67 [11392/17352 (66%)] Loss: -26256.460938\n",
      "Train Epoch: 67 [12800/17352 (74%)] Loss: -231307.343750\n",
      "Train Epoch: 67 [14208/17352 (82%)] Loss: -244012.734375\n",
      "Train Epoch: 67 [15542/17352 (90%)] Loss: 25264.154297\n",
      "Train Epoch: 67 [16549/17352 (95%)] Loss: -136974.531250\n",
      "Train Epoch: 67 [17145/17352 (99%)] Loss: -24319.609375\n",
      "    epoch          : 67\n",
      "    loss           : -119076.2816465237\n",
      "    val_loss       : -78550.41705007553\n",
      "Train Epoch: 68 [128/17352 (1%)] Loss: -148705.125000\n",
      "Train Epoch: 68 [1536/17352 (9%)] Loss: -221949.406250\n",
      "Train Epoch: 68 [2944/17352 (17%)] Loss: -214994.937500\n",
      "Train Epoch: 68 [4352/17352 (25%)] Loss: -124575.414062\n",
      "Train Epoch: 68 [5760/17352 (33%)] Loss: -148628.828125\n",
      "Train Epoch: 68 [7168/17352 (41%)] Loss: -154972.078125\n",
      "Train Epoch: 68 [8576/17352 (49%)] Loss: 9195.546875\n",
      "Train Epoch: 68 [9984/17352 (58%)] Loss: -43174.101562\n",
      "Train Epoch: 68 [11392/17352 (66%)] Loss: -210221.343750\n",
      "Train Epoch: 68 [12800/17352 (74%)] Loss: 20263.148438\n",
      "Train Epoch: 68 [14208/17352 (82%)] Loss: -233283.937500\n",
      "Train Epoch: 68 [15442/17352 (89%)] Loss: -7827.101562\n",
      "Train Epoch: 68 [16159/17352 (93%)] Loss: 48569.882812\n",
      "Train Epoch: 68 [16904/17352 (97%)] Loss: -130690.609375\n",
      "    epoch          : 68\n",
      "    loss           : -123084.6969100645\n",
      "    val_loss       : -74921.76565879186\n",
      "Train Epoch: 69 [128/17352 (1%)] Loss: -227590.250000\n",
      "Train Epoch: 69 [1536/17352 (9%)] Loss: -76785.625000\n",
      "Train Epoch: 69 [2944/17352 (17%)] Loss: -218146.343750\n",
      "Train Epoch: 69 [4352/17352 (25%)] Loss: -35196.460938\n",
      "Train Epoch: 69 [5760/17352 (33%)] Loss: -63300.882812\n",
      "Train Epoch: 69 [7168/17352 (41%)] Loss: -225947.765625\n",
      "Train Epoch: 69 [8576/17352 (49%)] Loss: -51690.617188\n",
      "Train Epoch: 69 [9984/17352 (58%)] Loss: -226441.906250\n",
      "Train Epoch: 69 [11392/17352 (66%)] Loss: -167107.781250\n",
      "Train Epoch: 69 [12800/17352 (74%)] Loss: -212974.968750\n",
      "Train Epoch: 69 [14208/17352 (82%)] Loss: -159495.656250\n",
      "Train Epoch: 69 [15542/17352 (90%)] Loss: -198549.343750\n",
      "Train Epoch: 69 [16213/17352 (93%)] Loss: -143383.687500\n",
      "Train Epoch: 69 [16954/17352 (98%)] Loss: -5488.978516\n",
      "    epoch          : 69\n",
      "    loss           : -121331.98626258389\n",
      "    val_loss       : -74468.20336758296\n",
      "Train Epoch: 70 [128/17352 (1%)] Loss: -224682.000000\n",
      "Train Epoch: 70 [1536/17352 (9%)] Loss: -226562.000000\n",
      "Train Epoch: 70 [2944/17352 (17%)] Loss: -206196.859375\n",
      "Train Epoch: 70 [4352/17352 (25%)] Loss: -152477.453125\n",
      "Train Epoch: 70 [5760/17352 (33%)] Loss: -72872.921875\n",
      "Train Epoch: 70 [7168/17352 (41%)] Loss: -92510.343750\n",
      "Train Epoch: 70 [8576/17352 (49%)] Loss: -66951.375000\n",
      "Train Epoch: 70 [9984/17352 (58%)] Loss: -33641.953125\n",
      "Train Epoch: 70 [11392/17352 (66%)] Loss: 3317.667969\n",
      "Train Epoch: 70 [12800/17352 (74%)] Loss: -130790.484375\n",
      "Train Epoch: 70 [14208/17352 (82%)] Loss: -165860.640625\n",
      "Train Epoch: 70 [15509/17352 (89%)] Loss: -77653.984375\n",
      "Train Epoch: 70 [16112/17352 (93%)] Loss: -8703.024414\n",
      "Train Epoch: 70 [16986/17352 (98%)] Loss: -33861.933594\n",
      "    epoch          : 70\n",
      "    loss           : -119327.5354053062\n",
      "    val_loss       : -66592.23256462415\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0913_161229/checkpoint-epoch70.pth ...\n",
      "Train Epoch: 71 [128/17352 (1%)] Loss: -196410.875000\n",
      "Train Epoch: 71 [1536/17352 (9%)] Loss: -79595.226562\n",
      "Train Epoch: 71 [2944/17352 (17%)] Loss: -138927.562500\n",
      "Train Epoch: 71 [4352/17352 (25%)] Loss: -212513.109375\n",
      "Train Epoch: 71 [5760/17352 (33%)] Loss: -20475.457031\n",
      "Train Epoch: 71 [7168/17352 (41%)] Loss: -228803.843750\n",
      "Train Epoch: 71 [8576/17352 (49%)] Loss: -119823.773438\n",
      "Train Epoch: 71 [9984/17352 (58%)] Loss: -165113.234375\n",
      "Train Epoch: 71 [11392/17352 (66%)] Loss: 125814.859375\n",
      "Train Epoch: 71 [12800/17352 (74%)] Loss: -231317.218750\n",
      "Train Epoch: 71 [14208/17352 (82%)] Loss: -201521.500000\n",
      "Train Epoch: 71 [15469/17352 (89%)] Loss: -4801.433594\n",
      "Train Epoch: 71 [16355/17352 (94%)] Loss: -136991.781250\n",
      "Train Epoch: 71 [16922/17352 (98%)] Loss: -4652.139160\n",
      "    epoch          : 71\n",
      "    loss           : -134802.4904539377\n",
      "    val_loss       : -70845.25584877332\n",
      "Train Epoch: 72 [128/17352 (1%)] Loss: -203912.562500\n",
      "Train Epoch: 72 [1536/17352 (9%)] Loss: -219046.390625\n",
      "Train Epoch: 72 [2944/17352 (17%)] Loss: -203455.359375\n",
      "Train Epoch: 72 [4352/17352 (25%)] Loss: -135791.593750\n",
      "Train Epoch: 72 [5760/17352 (33%)] Loss: -153927.578125\n",
      "Train Epoch: 72 [7168/17352 (41%)] Loss: -196971.406250\n",
      "Train Epoch: 72 [8576/17352 (49%)] Loss: -89272.687500\n",
      "Train Epoch: 72 [9984/17352 (58%)] Loss: -64409.007812\n",
      "Train Epoch: 72 [11392/17352 (66%)] Loss: -289.355469\n",
      "Train Epoch: 72 [12800/17352 (74%)] Loss: -218416.953125\n",
      "Train Epoch: 72 [14208/17352 (82%)] Loss: -211906.234375\n",
      "Train Epoch: 72 [15550/17352 (90%)] Loss: 58593.328125\n",
      "Train Epoch: 72 [16267/17352 (94%)] Loss: -85197.250000\n",
      "Train Epoch: 72 [17061/17352 (98%)] Loss: -120963.281250\n",
      "    epoch          : 72\n",
      "    loss           : -131414.5267112783\n",
      "    val_loss       : -84929.45925738016\n",
      "Train Epoch: 73 [128/17352 (1%)] Loss: -106050.710938\n",
      "Train Epoch: 73 [1536/17352 (9%)] Loss: -21826.484375\n",
      "Train Epoch: 73 [2944/17352 (17%)] Loss: -198259.218750\n",
      "Train Epoch: 73 [4352/17352 (25%)] Loss: -232387.125000\n",
      "Train Epoch: 73 [5760/17352 (33%)] Loss: -220428.750000\n",
      "Train Epoch: 73 [7168/17352 (41%)] Loss: -223206.906250\n",
      "Train Epoch: 73 [8576/17352 (49%)] Loss: -155634.812500\n",
      "Train Epoch: 73 [9984/17352 (58%)] Loss: -211922.515625\n",
      "Train Epoch: 73 [11392/17352 (66%)] Loss: -166183.015625\n",
      "Train Epoch: 73 [12800/17352 (74%)] Loss: -240808.281250\n",
      "Train Epoch: 73 [14208/17352 (82%)] Loss: 61313.441406\n",
      "Train Epoch: 73 [15535/17352 (90%)] Loss: -13038.750000\n",
      "Train Epoch: 73 [16429/17352 (95%)] Loss: -71034.023438\n",
      "Train Epoch: 73 [17136/17352 (99%)] Loss: -131570.906250\n",
      "    epoch          : 73\n",
      "    loss           : -129749.74081769085\n",
      "    val_loss       : -70515.34835062028\n",
      "Train Epoch: 74 [128/17352 (1%)] Loss: -172391.937500\n",
      "Train Epoch: 74 [1536/17352 (9%)] Loss: -174440.421875\n",
      "Train Epoch: 74 [2944/17352 (17%)] Loss: -158812.296875\n",
      "Train Epoch: 74 [4352/17352 (25%)] Loss: -83781.500000\n",
      "Train Epoch: 74 [5760/17352 (33%)] Loss: -151352.625000\n",
      "Train Epoch: 74 [7168/17352 (41%)] Loss: -255480.578125\n",
      "Train Epoch: 74 [8576/17352 (49%)] Loss: -10737.667969\n",
      "Train Epoch: 74 [9984/17352 (58%)] Loss: -162346.750000\n",
      "Train Epoch: 74 [11392/17352 (66%)] Loss: -195515.234375\n",
      "Train Epoch: 74 [12800/17352 (74%)] Loss: -219864.171875\n",
      "Train Epoch: 74 [14208/17352 (82%)] Loss: -229049.656250\n",
      "Train Epoch: 74 [15573/17352 (90%)] Loss: -219466.843750\n",
      "Train Epoch: 74 [16209/17352 (93%)] Loss: -1700.514160\n",
      "Train Epoch: 74 [17053/17352 (98%)] Loss: -38895.195312\n",
      "    epoch          : 74\n",
      "    loss           : -124899.07658478398\n",
      "    val_loss       : -71047.45112012228\n",
      "Train Epoch: 75 [128/17352 (1%)] Loss: -204544.140625\n",
      "Train Epoch: 75 [1536/17352 (9%)] Loss: -67435.648438\n",
      "Train Epoch: 75 [2944/17352 (17%)] Loss: -187288.437500\n",
      "Train Epoch: 75 [4352/17352 (25%)] Loss: -59532.835938\n",
      "Train Epoch: 75 [5760/17352 (33%)] Loss: 51082.449219\n",
      "Train Epoch: 75 [7168/17352 (41%)] Loss: -159859.609375\n",
      "Train Epoch: 75 [8576/17352 (49%)] Loss: -222269.156250\n",
      "Train Epoch: 75 [9984/17352 (58%)] Loss: -226633.015625\n",
      "Train Epoch: 75 [11392/17352 (66%)] Loss: -224548.515625\n",
      "Train Epoch: 75 [12800/17352 (74%)] Loss: -230818.125000\n",
      "Train Epoch: 75 [14208/17352 (82%)] Loss: -160361.375000\n",
      "Train Epoch: 75 [15529/17352 (89%)] Loss: 50852.039062\n",
      "Train Epoch: 75 [16296/17352 (94%)] Loss: -112553.257812\n",
      "Train Epoch: 75 [17019/17352 (98%)] Loss: -231217.625000\n",
      "    epoch          : 75\n",
      "    loss           : -127861.50031213953\n",
      "    val_loss       : -74911.23657725652\n",
      "Train Epoch: 76 [128/17352 (1%)] Loss: -210822.812500\n",
      "Train Epoch: 76 [1536/17352 (9%)] Loss: -221423.062500\n",
      "Train Epoch: 76 [2944/17352 (17%)] Loss: -10286.035156\n",
      "Train Epoch: 76 [4352/17352 (25%)] Loss: -215440.843750\n",
      "Train Epoch: 76 [5760/17352 (33%)] Loss: -164080.671875\n",
      "Train Epoch: 76 [7168/17352 (41%)] Loss: -209898.203125\n",
      "Train Epoch: 76 [8576/17352 (49%)] Loss: -224941.328125\n",
      "Train Epoch: 76 [9984/17352 (58%)] Loss: -180732.312500\n",
      "Train Epoch: 76 [11392/17352 (66%)] Loss: -227915.500000\n",
      "Train Epoch: 76 [12800/17352 (74%)] Loss: -221110.500000\n",
      "Train Epoch: 76 [14208/17352 (82%)] Loss: 31906.355469\n",
      "Train Epoch: 76 [15552/17352 (90%)] Loss: -53326.351562\n",
      "Train Epoch: 76 [16338/17352 (94%)] Loss: -133630.843750\n",
      "Train Epoch: 76 [17028/17352 (98%)] Loss: -4886.675293\n",
      "    epoch          : 76\n",
      "    loss           : -115402.41755747955\n",
      "    val_loss       : -79493.67321923574\n",
      "Train Epoch: 77 [128/17352 (1%)] Loss: -137199.312500\n",
      "Train Epoch: 77 [1536/17352 (9%)] Loss: -225913.187500\n",
      "Train Epoch: 77 [2944/17352 (17%)] Loss: -50915.808594\n",
      "Train Epoch: 77 [4352/17352 (25%)] Loss: -44276.523438\n",
      "Train Epoch: 77 [5760/17352 (33%)] Loss: -245845.265625\n",
      "Train Epoch: 77 [7168/17352 (41%)] Loss: -586.777344\n",
      "Train Epoch: 77 [8576/17352 (49%)] Loss: -148461.125000\n",
      "Train Epoch: 77 [9984/17352 (58%)] Loss: -236131.562500\n",
      "Train Epoch: 77 [11392/17352 (66%)] Loss: -183062.187500\n",
      "Train Epoch: 77 [12800/17352 (74%)] Loss: 86700.609375\n",
      "Train Epoch: 77 [14208/17352 (82%)] Loss: -41673.949219\n",
      "Train Epoch: 77 [15484/17352 (89%)] Loss: 16030.367188\n",
      "Train Epoch: 77 [16274/17352 (94%)] Loss: -183306.500000\n",
      "Train Epoch: 77 [17034/17352 (98%)] Loss: -137023.078125\n",
      "    epoch          : 77\n",
      "    loss           : -126099.13936333368\n",
      "    val_loss       : -60763.19288409551\n",
      "Train Epoch: 78 [128/17352 (1%)] Loss: -62923.808594\n",
      "Train Epoch: 78 [1536/17352 (9%)] Loss: 183186.953125\n",
      "Train Epoch: 78 [2944/17352 (17%)] Loss: -164920.843750\n",
      "Train Epoch: 78 [4352/17352 (25%)] Loss: -163626.953125\n",
      "Train Epoch: 78 [5760/17352 (33%)] Loss: -233706.000000\n",
      "Train Epoch: 78 [7168/17352 (41%)] Loss: -367.238281\n",
      "Train Epoch: 78 [8576/17352 (49%)] Loss: -25048.896484\n",
      "Train Epoch: 78 [9984/17352 (58%)] Loss: -124629.625000\n",
      "Train Epoch: 78 [11392/17352 (66%)] Loss: -229333.656250\n",
      "Train Epoch: 78 [12800/17352 (74%)] Loss: -43535.652344\n",
      "Train Epoch: 78 [14208/17352 (82%)] Loss: -152329.140625\n",
      "Train Epoch: 78 [15561/17352 (90%)] Loss: -138059.156250\n",
      "Train Epoch: 78 [16398/17352 (95%)] Loss: -144984.906250\n",
      "Train Epoch: 78 [16978/17352 (98%)] Loss: -9182.376953\n",
      "    epoch          : 78\n",
      "    loss           : -132624.6622928901\n",
      "    val_loss       : -60656.481894858676\n",
      "Train Epoch: 79 [128/17352 (1%)] Loss: -130743.875000\n",
      "Train Epoch: 79 [1536/17352 (9%)] Loss: 11771.688477\n",
      "Train Epoch: 79 [2944/17352 (17%)] Loss: -214716.375000\n",
      "Train Epoch: 79 [4352/17352 (25%)] Loss: -139897.468750\n",
      "Train Epoch: 79 [5760/17352 (33%)] Loss: -229393.265625\n",
      "Train Epoch: 79 [7168/17352 (41%)] Loss: -159508.203125\n",
      "Train Epoch: 79 [8576/17352 (49%)] Loss: -220956.578125\n",
      "Train Epoch: 79 [9984/17352 (58%)] Loss: 75222.359375\n",
      "Train Epoch: 79 [11392/17352 (66%)] Loss: -158579.187500\n",
      "Train Epoch: 79 [12800/17352 (74%)] Loss: -160522.562500\n",
      "Train Epoch: 79 [14208/17352 (82%)] Loss: -231996.734375\n",
      "Train Epoch: 79 [15519/17352 (89%)] Loss: -141642.562500\n",
      "Train Epoch: 79 [16299/17352 (94%)] Loss: -31571.683594\n",
      "Train Epoch: 79 [16944/17352 (98%)] Loss: -131719.750000\n",
      "    epoch          : 79\n",
      "    loss           : -135173.9751664744\n",
      "    val_loss       : -64756.61858215332\n",
      "Train Epoch: 80 [128/17352 (1%)] Loss: -150155.500000\n",
      "Train Epoch: 80 [1536/17352 (9%)] Loss: -158867.093750\n",
      "Train Epoch: 80 [2944/17352 (17%)] Loss: -170463.125000\n",
      "Train Epoch: 80 [4352/17352 (25%)] Loss: -167093.250000\n",
      "Train Epoch: 80 [5760/17352 (33%)] Loss: -212218.906250\n",
      "Train Epoch: 80 [7168/17352 (41%)] Loss: -153445.812500\n",
      "Train Epoch: 80 [8576/17352 (49%)] Loss: -166162.750000\n",
      "Train Epoch: 80 [9984/17352 (58%)] Loss: -198396.046875\n",
      "Train Epoch: 80 [11392/17352 (66%)] Loss: -155052.625000\n",
      "Train Epoch: 80 [12800/17352 (74%)] Loss: -28552.406250\n",
      "Train Epoch: 80 [14208/17352 (82%)] Loss: -201842.437500\n",
      "Train Epoch: 80 [15438/17352 (89%)] Loss: -135195.140625\n",
      "Train Epoch: 80 [16103/17352 (93%)] Loss: -88784.093750\n",
      "Train Epoch: 80 [16945/17352 (98%)] Loss: -52353.148438\n",
      "    epoch          : 80\n",
      "    loss           : -128642.47751940017\n",
      "    val_loss       : -54572.13019630114\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0913_161229/checkpoint-epoch80.pth ...\n",
      "Train Epoch: 81 [128/17352 (1%)] Loss: -220770.453125\n",
      "Train Epoch: 81 [1536/17352 (9%)] Loss: -205794.468750\n",
      "Train Epoch: 81 [2944/17352 (17%)] Loss: 215215.000000\n",
      "Train Epoch: 81 [4352/17352 (25%)] Loss: -191248.250000\n",
      "Train Epoch: 81 [5760/17352 (33%)] Loss: -34992.449219\n",
      "Train Epoch: 81 [7168/17352 (41%)] Loss: -224976.140625\n",
      "Train Epoch: 81 [8576/17352 (49%)] Loss: -173176.750000\n",
      "Train Epoch: 81 [9984/17352 (58%)] Loss: -217954.171875\n",
      "Train Epoch: 81 [11392/17352 (66%)] Loss: -148067.609375\n",
      "Train Epoch: 81 [12800/17352 (74%)] Loss: -159798.218750\n",
      "Train Epoch: 81 [14208/17352 (82%)] Loss: -156489.000000\n",
      "Train Epoch: 81 [15478/17352 (89%)] Loss: -37218.164062\n",
      "Train Epoch: 81 [16194/17352 (93%)] Loss: -103998.585938\n",
      "Train Epoch: 81 [16900/17352 (97%)] Loss: -41620.808594\n",
      "    epoch          : 81\n",
      "    loss           : -124336.27965407404\n",
      "    val_loss       : -61212.95104813576\n",
      "Train Epoch: 82 [128/17352 (1%)] Loss: -227572.890625\n",
      "Train Epoch: 82 [1536/17352 (9%)] Loss: -208078.453125\n",
      "Train Epoch: 82 [2944/17352 (17%)] Loss: -155222.156250\n",
      "Train Epoch: 82 [4352/17352 (25%)] Loss: -154756.093750\n",
      "Train Epoch: 82 [5760/17352 (33%)] Loss: -56635.074219\n",
      "Train Epoch: 82 [7168/17352 (41%)] Loss: -164582.218750\n",
      "Train Epoch: 82 [8576/17352 (49%)] Loss: -146004.125000\n",
      "Train Epoch: 82 [9984/17352 (58%)] Loss: -117653.328125\n",
      "Train Epoch: 82 [11392/17352 (66%)] Loss: -117200.757812\n",
      "Train Epoch: 82 [12800/17352 (74%)] Loss: -141928.968750\n",
      "Train Epoch: 82 [14208/17352 (82%)] Loss: -219078.203125\n",
      "Train Epoch: 82 [15522/17352 (89%)] Loss: -142965.343750\n",
      "Train Epoch: 82 [16363/17352 (94%)] Loss: -68213.078125\n",
      "Train Epoch: 82 [17127/17352 (99%)] Loss: -99366.125000\n",
      "    epoch          : 82\n",
      "    loss           : -134962.74111917996\n",
      "    val_loss       : -56012.40063028336\n",
      "Train Epoch: 83 [128/17352 (1%)] Loss: -172948.187500\n",
      "Train Epoch: 83 [1536/17352 (9%)] Loss: -78318.500000\n",
      "Train Epoch: 83 [2944/17352 (17%)] Loss: -185216.250000\n",
      "Train Epoch: 83 [4352/17352 (25%)] Loss: -165592.656250\n",
      "Train Epoch: 83 [5760/17352 (33%)] Loss: 18306.218750\n",
      "Train Epoch: 83 [7168/17352 (41%)] Loss: -191357.843750\n",
      "Train Epoch: 83 [8576/17352 (49%)] Loss: -214521.968750\n",
      "Train Epoch: 83 [9984/17352 (58%)] Loss: -134518.187500\n",
      "Train Epoch: 83 [11392/17352 (66%)] Loss: -204995.062500\n",
      "Train Epoch: 83 [12800/17352 (74%)] Loss: -232992.281250\n",
      "Train Epoch: 83 [14208/17352 (82%)] Loss: 16453.816406\n",
      "Train Epoch: 83 [15495/17352 (89%)] Loss: -37102.195312\n",
      "Train Epoch: 83 [16148/17352 (93%)] Loss: -57552.101562\n",
      "Train Epoch: 83 [17004/17352 (98%)] Loss: -167440.734375\n",
      "    epoch          : 83\n",
      "    loss           : -128876.3966203728\n",
      "    val_loss       : -82749.50885270437\n",
      "Train Epoch: 84 [128/17352 (1%)] Loss: -166335.843750\n",
      "Train Epoch: 84 [1536/17352 (9%)] Loss: -238394.890625\n",
      "Train Epoch: 84 [2944/17352 (17%)] Loss: -36682.894531\n",
      "Train Epoch: 84 [4352/17352 (25%)] Loss: -232660.281250\n",
      "Train Epoch: 84 [5760/17352 (33%)] Loss: -142886.031250\n",
      "Train Epoch: 84 [7168/17352 (41%)] Loss: -135918.062500\n",
      "Train Epoch: 84 [8576/17352 (49%)] Loss: 29606.968750\n",
      "Train Epoch: 84 [9984/17352 (58%)] Loss: -215117.953125\n",
      "Train Epoch: 84 [11392/17352 (66%)] Loss: -39351.851562\n",
      "Train Epoch: 84 [12800/17352 (74%)] Loss: -112154.062500\n",
      "Train Epoch: 84 [14208/17352 (82%)] Loss: -70728.773438\n",
      "Train Epoch: 84 [15519/17352 (89%)] Loss: -144610.140625\n",
      "Train Epoch: 84 [16176/17352 (93%)] Loss: 221.229248\n",
      "Train Epoch: 84 [16963/17352 (98%)] Loss: -169846.250000\n",
      "    epoch          : 84\n",
      "    loss           : -126349.40680218383\n",
      "    val_loss       : -55072.32100226085\n",
      "Train Epoch: 85 [128/17352 (1%)] Loss: -228126.640625\n",
      "Train Epoch: 85 [1536/17352 (9%)] Loss: -205850.156250\n",
      "Train Epoch: 85 [2944/17352 (17%)] Loss: -190836.031250\n",
      "Train Epoch: 85 [4352/17352 (25%)] Loss: 27302.609375\n",
      "Train Epoch: 85 [5760/17352 (33%)] Loss: -223293.968750\n",
      "Train Epoch: 85 [7168/17352 (41%)] Loss: -69672.390625\n",
      "Train Epoch: 85 [8576/17352 (49%)] Loss: -203080.718750\n",
      "Train Epoch: 85 [9984/17352 (58%)] Loss: -156604.421875\n",
      "Train Epoch: 85 [11392/17352 (66%)] Loss: -168541.734375\n",
      "Train Epoch: 85 [12800/17352 (74%)] Loss: -121217.875000\n",
      "Train Epoch: 85 [14208/17352 (82%)] Loss: -172335.796875\n",
      "Train Epoch: 85 [15490/17352 (89%)] Loss: -3574.069336\n",
      "Train Epoch: 85 [16216/17352 (93%)] Loss: -54511.292969\n",
      "Train Epoch: 85 [17073/17352 (98%)] Loss: -505.507812\n",
      "    epoch          : 85\n",
      "    loss           : -124228.88651557257\n",
      "    val_loss       : -69281.43616778056\n",
      "Train Epoch: 86 [128/17352 (1%)] Loss: -140221.937500\n",
      "Train Epoch: 86 [1536/17352 (9%)] Loss: -104979.500000\n",
      "Train Epoch: 86 [2944/17352 (17%)] Loss: -142032.218750\n",
      "Train Epoch: 86 [4352/17352 (25%)] Loss: -230705.359375\n",
      "Train Epoch: 86 [5760/17352 (33%)] Loss: -212215.921875\n",
      "Train Epoch: 86 [7168/17352 (41%)] Loss: -223034.203125\n",
      "Train Epoch: 86 [8576/17352 (49%)] Loss: -180108.843750\n",
      "Train Epoch: 86 [9984/17352 (58%)] Loss: -225087.593750\n",
      "Train Epoch: 86 [11392/17352 (66%)] Loss: -229028.750000\n",
      "Train Epoch: 86 [12800/17352 (74%)] Loss: -127322.687500\n",
      "Train Epoch: 86 [14208/17352 (82%)] Loss: -207074.812500\n",
      "Train Epoch: 86 [15521/17352 (89%)] Loss: -114054.531250\n",
      "Train Epoch: 86 [16409/17352 (95%)] Loss: -34096.039062\n",
      "Train Epoch: 86 [17211/17352 (99%)] Loss: -6484.100098\n",
      "    epoch          : 86\n",
      "    loss           : -131124.32453990143\n",
      "    val_loss       : -73355.26599245072\n",
      "Train Epoch: 87 [128/17352 (1%)] Loss: -168401.234375\n",
      "Train Epoch: 87 [1536/17352 (9%)] Loss: -203455.968750\n",
      "Train Epoch: 87 [2944/17352 (17%)] Loss: -182245.312500\n",
      "Train Epoch: 87 [4352/17352 (25%)] Loss: -43581.000000\n",
      "Train Epoch: 87 [5760/17352 (33%)] Loss: -218975.875000\n",
      "Train Epoch: 87 [7168/17352 (41%)] Loss: -234809.968750\n",
      "Train Epoch: 87 [8576/17352 (49%)] Loss: -191589.281250\n",
      "Train Epoch: 87 [9984/17352 (58%)] Loss: -157366.984375\n",
      "Train Epoch: 87 [11392/17352 (66%)] Loss: -228548.515625\n",
      "Train Epoch: 87 [12800/17352 (74%)] Loss: -228929.140625\n",
      "Train Epoch: 87 [14208/17352 (82%)] Loss: -241481.625000\n",
      "Train Epoch: 87 [15479/17352 (89%)] Loss: -64998.289062\n",
      "Train Epoch: 87 [16235/17352 (94%)] Loss: -88117.250000\n",
      "Train Epoch: 87 [16986/17352 (98%)] Loss: -139223.187500\n",
      "    epoch          : 87\n",
      "    loss           : -130225.46893515362\n",
      "    val_loss       : -69497.70109055837\n",
      "Train Epoch: 88 [128/17352 (1%)] Loss: 93058.546875\n",
      "Train Epoch: 88 [1536/17352 (9%)] Loss: -43968.847656\n",
      "Train Epoch: 88 [2944/17352 (17%)] Loss: -215876.468750\n",
      "Train Epoch: 88 [4352/17352 (25%)] Loss: -42841.042969\n",
      "Train Epoch: 88 [5760/17352 (33%)] Loss: -184003.062500\n",
      "Train Epoch: 88 [7168/17352 (41%)] Loss: -210301.593750\n",
      "Train Epoch: 88 [8576/17352 (49%)] Loss: -215907.031250\n",
      "Train Epoch: 88 [9984/17352 (58%)] Loss: -225101.000000\n",
      "Train Epoch: 88 [11392/17352 (66%)] Loss: -143325.875000\n",
      "Train Epoch: 88 [12800/17352 (74%)] Loss: -226170.031250\n",
      "Train Epoch: 88 [14208/17352 (82%)] Loss: -241346.875000\n",
      "Train Epoch: 88 [15427/17352 (89%)] Loss: -89618.648438\n",
      "Train Epoch: 88 [16206/17352 (93%)] Loss: -149299.796875\n",
      "Train Epoch: 88 [16920/17352 (98%)] Loss: -44902.996094\n",
      "    epoch          : 88\n",
      "    loss           : -120236.14446898595\n",
      "    val_loss       : -73284.83790127437\n",
      "Train Epoch: 89 [128/17352 (1%)] Loss: -164708.750000\n",
      "Train Epoch: 89 [1536/17352 (9%)] Loss: -84111.289062\n",
      "Train Epoch: 89 [2944/17352 (17%)] Loss: -189863.281250\n",
      "Train Epoch: 89 [4352/17352 (25%)] Loss: -208492.609375\n",
      "Train Epoch: 89 [5760/17352 (33%)] Loss: -196506.703125\n",
      "Train Epoch: 89 [7168/17352 (41%)] Loss: -170368.625000\n",
      "Train Epoch: 89 [8576/17352 (49%)] Loss: -25379.296875\n",
      "Train Epoch: 89 [9984/17352 (58%)] Loss: -124603.148438\n",
      "Train Epoch: 89 [11392/17352 (66%)] Loss: 21947.343750\n",
      "Train Epoch: 89 [12800/17352 (74%)] Loss: -215024.000000\n",
      "Train Epoch: 89 [14208/17352 (82%)] Loss: -145883.750000\n",
      "Train Epoch: 89 [15500/17352 (89%)] Loss: -197409.875000\n",
      "Train Epoch: 89 [16199/17352 (93%)] Loss: -60243.792969\n",
      "Train Epoch: 89 [17113/17352 (99%)] Loss: -140370.781250\n",
      "    epoch          : 89\n",
      "    loss           : -130727.18246152737\n",
      "    val_loss       : -66478.48107592265\n",
      "Train Epoch: 90 [128/17352 (1%)] Loss: -87701.507812\n",
      "Train Epoch: 90 [1536/17352 (9%)] Loss: -145363.734375\n",
      "Train Epoch: 90 [2944/17352 (17%)] Loss: -177390.125000\n",
      "Train Epoch: 90 [4352/17352 (25%)] Loss: -205925.546875\n",
      "Train Epoch: 90 [5760/17352 (33%)] Loss: -123700.773438\n",
      "Train Epoch: 90 [7168/17352 (41%)] Loss: -228183.765625\n",
      "Train Epoch: 90 [8576/17352 (49%)] Loss: -195681.843750\n",
      "Train Epoch: 90 [9984/17352 (58%)] Loss: 16924.546875\n",
      "Train Epoch: 90 [11392/17352 (66%)] Loss: -146960.187500\n",
      "Train Epoch: 90 [12800/17352 (74%)] Loss: -215459.406250\n",
      "Train Epoch: 90 [14208/17352 (82%)] Loss: -205649.359375\n",
      "Train Epoch: 90 [15498/17352 (89%)] Loss: -89842.203125\n",
      "Train Epoch: 90 [16365/17352 (94%)] Loss: -16554.234375\n",
      "Train Epoch: 90 [17062/17352 (98%)] Loss: -36633.906250\n",
      "    epoch          : 90\n",
      "    loss           : -119310.8483788407\n",
      "    val_loss       : -81010.20792361895\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0913_161229/checkpoint-epoch90.pth ...\n",
      "Train Epoch: 91 [128/17352 (1%)] Loss: -207463.218750\n",
      "Train Epoch: 91 [1536/17352 (9%)] Loss: -228080.140625\n",
      "Train Epoch: 91 [2944/17352 (17%)] Loss: -87256.773438\n",
      "Train Epoch: 91 [4352/17352 (25%)] Loss: -114476.171875\n",
      "Train Epoch: 91 [5760/17352 (33%)] Loss: -228222.984375\n",
      "Train Epoch: 91 [7168/17352 (41%)] Loss: -208368.656250\n",
      "Train Epoch: 91 [8576/17352 (49%)] Loss: -225572.625000\n",
      "Train Epoch: 91 [9984/17352 (58%)] Loss: -197788.265625\n",
      "Train Epoch: 91 [11392/17352 (66%)] Loss: -152215.875000\n",
      "Train Epoch: 91 [12800/17352 (74%)] Loss: -135652.812500\n",
      "Train Epoch: 91 [14208/17352 (82%)] Loss: -72524.421875\n",
      "Train Epoch: 91 [15455/17352 (89%)] Loss: -57092.500000\n",
      "Train Epoch: 91 [16172/17352 (93%)] Loss: -155750.546875\n",
      "Train Epoch: 91 [16945/17352 (98%)] Loss: -149198.234375\n",
      "    epoch          : 91\n",
      "    loss           : -128468.0650511876\n",
      "    val_loss       : -71736.7099108696\n",
      "Train Epoch: 92 [128/17352 (1%)] Loss: -228119.296875\n",
      "Train Epoch: 92 [1536/17352 (9%)] Loss: -236298.843750\n",
      "Train Epoch: 92 [2944/17352 (17%)] Loss: -106622.812500\n",
      "Train Epoch: 92 [4352/17352 (25%)] Loss: -209822.109375\n",
      "Train Epoch: 92 [5760/17352 (33%)] Loss: -131915.968750\n",
      "Train Epoch: 92 [7168/17352 (41%)] Loss: -155687.703125\n",
      "Train Epoch: 92 [8576/17352 (49%)] Loss: 35137.832031\n",
      "Train Epoch: 92 [9984/17352 (58%)] Loss: 25411.996094\n",
      "Train Epoch: 92 [11392/17352 (66%)] Loss: -217939.843750\n",
      "Train Epoch: 92 [12800/17352 (74%)] Loss: -72834.843750\n",
      "Train Epoch: 92 [14208/17352 (82%)] Loss: -97284.445312\n",
      "Train Epoch: 92 [15538/17352 (90%)] Loss: -183483.281250\n",
      "Train Epoch: 92 [16316/17352 (94%)] Loss: -99760.460938\n",
      "Train Epoch: 92 [17126/17352 (99%)] Loss: -164941.109375\n",
      "    epoch          : 92\n",
      "    loss           : -127855.90445417365\n",
      "    val_loss       : -91001.66761395136\n",
      "Train Epoch: 93 [128/17352 (1%)] Loss: -149443.218750\n",
      "Train Epoch: 93 [1536/17352 (9%)] Loss: -196986.453125\n",
      "Train Epoch: 93 [2944/17352 (17%)] Loss: -88596.671875\n",
      "Train Epoch: 93 [4352/17352 (25%)] Loss: -135465.500000\n",
      "Train Epoch: 93 [5760/17352 (33%)] Loss: -240342.656250\n",
      "Train Epoch: 93 [7168/17352 (41%)] Loss: -212196.515625\n",
      "Train Epoch: 93 [8576/17352 (49%)] Loss: -188791.718750\n",
      "Train Epoch: 93 [9984/17352 (58%)] Loss: -151507.031250\n",
      "Train Epoch: 93 [11392/17352 (66%)] Loss: -155637.671875\n",
      "Train Epoch: 93 [12800/17352 (74%)] Loss: -224911.296875\n",
      "Train Epoch: 93 [14208/17352 (82%)] Loss: -52799.015625\n",
      "Train Epoch: 93 [15469/17352 (89%)] Loss: -5749.479492\n",
      "Train Epoch: 93 [16272/17352 (94%)] Loss: -166409.343750\n",
      "Train Epoch: 93 [17125/17352 (99%)] Loss: -121724.687500\n",
      "    epoch          : 93\n",
      "    loss           : -139597.8155488904\n",
      "    val_loss       : -77461.14086360931\n",
      "Train Epoch: 94 [128/17352 (1%)] Loss: -145769.437500\n",
      "Train Epoch: 94 [1536/17352 (9%)] Loss: -198677.328125\n",
      "Train Epoch: 94 [2944/17352 (17%)] Loss: -154954.984375\n",
      "Train Epoch: 94 [4352/17352 (25%)] Loss: -107850.796875\n",
      "Train Epoch: 94 [5760/17352 (33%)] Loss: -64685.125000\n",
      "Train Epoch: 94 [7168/17352 (41%)] Loss: -230709.031250\n",
      "Train Epoch: 94 [8576/17352 (49%)] Loss: -162881.218750\n",
      "Train Epoch: 94 [9984/17352 (58%)] Loss: -13298.902344\n",
      "Train Epoch: 94 [11392/17352 (66%)] Loss: -147548.546875\n",
      "Train Epoch: 94 [12800/17352 (74%)] Loss: -153286.343750\n",
      "Train Epoch: 94 [14208/17352 (82%)] Loss: -148987.062500\n",
      "Train Epoch: 94 [15543/17352 (90%)] Loss: -154985.843750\n",
      "Train Epoch: 94 [16474/17352 (95%)] Loss: -119231.968750\n",
      "Train Epoch: 94 [17080/17352 (98%)] Loss: -14359.894531\n",
      "    epoch          : 94\n",
      "    loss           : -133082.07579009805\n",
      "    val_loss       : -72374.02473971048\n",
      "Train Epoch: 95 [128/17352 (1%)] Loss: -169131.593750\n",
      "Train Epoch: 95 [1536/17352 (9%)] Loss: -201892.312500\n",
      "Train Epoch: 95 [2944/17352 (17%)] Loss: -157853.031250\n",
      "Train Epoch: 95 [4352/17352 (25%)] Loss: -196710.125000\n",
      "Train Epoch: 95 [5760/17352 (33%)] Loss: -36152.113281\n",
      "Train Epoch: 95 [7168/17352 (41%)] Loss: -29978.734375\n",
      "Train Epoch: 95 [8576/17352 (49%)] Loss: -166395.906250\n",
      "Train Epoch: 95 [9984/17352 (58%)] Loss: -199057.578125\n",
      "Train Epoch: 95 [11392/17352 (66%)] Loss: -234985.406250\n",
      "Train Epoch: 95 [12800/17352 (74%)] Loss: -186509.703125\n",
      "Train Epoch: 95 [14208/17352 (82%)] Loss: -241310.312500\n",
      "Train Epoch: 95 [15466/17352 (89%)] Loss: -822.032593\n",
      "Train Epoch: 95 [16359/17352 (94%)] Loss: 36235.453125\n",
      "Train Epoch: 95 [16972/17352 (98%)] Loss: -20508.761719\n",
      "    epoch          : 95\n",
      "    loss           : -137943.17610485922\n",
      "    val_loss       : -71931.72633511225\n",
      "Train Epoch: 96 [128/17352 (1%)] Loss: -158908.328125\n",
      "Train Epoch: 96 [1536/17352 (9%)] Loss: 14237.773438\n",
      "Train Epoch: 96 [2944/17352 (17%)] Loss: -219263.328125\n",
      "Train Epoch: 96 [4352/17352 (25%)] Loss: -202967.718750\n",
      "Train Epoch: 96 [5760/17352 (33%)] Loss: -130856.328125\n",
      "Train Epoch: 96 [7168/17352 (41%)] Loss: 121010.335938\n",
      "Train Epoch: 96 [8576/17352 (49%)] Loss: -229466.250000\n",
      "Train Epoch: 96 [9984/17352 (58%)] Loss: -202687.968750\n",
      "Train Epoch: 96 [11392/17352 (66%)] Loss: -214869.406250\n",
      "Train Epoch: 96 [12800/17352 (74%)] Loss: -241795.421875\n",
      "Train Epoch: 96 [14208/17352 (82%)] Loss: -217340.437500\n",
      "Train Epoch: 96 [15442/17352 (89%)] Loss: -6376.720215\n",
      "Train Epoch: 96 [16229/17352 (94%)] Loss: -86186.851562\n",
      "Train Epoch: 96 [16978/17352 (98%)] Loss: -118550.609375\n",
      "    epoch          : 96\n",
      "    loss           : -116944.76325077338\n",
      "    val_loss       : -44930.76478764216\n",
      "Train Epoch: 97 [128/17352 (1%)] Loss: -87616.046875\n",
      "Train Epoch: 97 [1536/17352 (9%)] Loss: -10324.890625\n",
      "Train Epoch: 97 [2944/17352 (17%)] Loss: -118100.109375\n",
      "Train Epoch: 97 [4352/17352 (25%)] Loss: 8094.421875\n",
      "Train Epoch: 97 [5760/17352 (33%)] Loss: -49274.898438\n",
      "Train Epoch: 97 [7168/17352 (41%)] Loss: -209843.484375\n",
      "Train Epoch: 97 [8576/17352 (49%)] Loss: -137324.140625\n",
      "Train Epoch: 97 [9984/17352 (58%)] Loss: -226787.218750\n",
      "Train Epoch: 97 [11392/17352 (66%)] Loss: -156443.687500\n",
      "Train Epoch: 97 [12800/17352 (74%)] Loss: -221423.937500\n",
      "Train Epoch: 97 [14208/17352 (82%)] Loss: -47987.226562\n",
      "Train Epoch: 97 [15536/17352 (90%)] Loss: -127835.453125\n",
      "Train Epoch: 97 [16181/17352 (93%)] Loss: -94645.046875\n",
      "Train Epoch: 97 [17033/17352 (98%)] Loss: -81691.125000\n",
      "    epoch          : 97\n",
      "    loss           : -123273.84533445627\n",
      "    val_loss       : -58824.299644502\n",
      "Train Epoch: 98 [128/17352 (1%)] Loss: -153575.062500\n",
      "Train Epoch: 98 [1536/17352 (9%)] Loss: -233900.515625\n",
      "Train Epoch: 98 [2944/17352 (17%)] Loss: -248271.421875\n",
      "Train Epoch: 98 [4352/17352 (25%)] Loss: -209432.343750\n",
      "Train Epoch: 98 [5760/17352 (33%)] Loss: -201207.718750\n",
      "Train Epoch: 98 [7168/17352 (41%)] Loss: -169458.531250\n",
      "Train Epoch: 98 [8576/17352 (49%)] Loss: -150528.296875\n",
      "Train Epoch: 98 [9984/17352 (58%)] Loss: -229405.062500\n",
      "Train Epoch: 98 [11392/17352 (66%)] Loss: -47157.269531\n",
      "Train Epoch: 98 [12800/17352 (74%)] Loss: -243799.875000\n",
      "Train Epoch: 98 [14208/17352 (82%)] Loss: -209622.031250\n",
      "Train Epoch: 98 [15572/17352 (90%)] Loss: -172196.218750\n",
      "Train Epoch: 98 [16268/17352 (94%)] Loss: -136306.125000\n",
      "Train Epoch: 98 [16949/17352 (98%)] Loss: -66926.976562\n",
      "    epoch          : 98\n",
      "    loss           : -134818.89178999318\n",
      "    val_loss       : -69081.6444234848\n",
      "Train Epoch: 99 [128/17352 (1%)] Loss: 14842.984375\n",
      "Train Epoch: 99 [1536/17352 (9%)] Loss: -152669.500000\n",
      "Train Epoch: 99 [2944/17352 (17%)] Loss: -39868.503906\n",
      "Train Epoch: 99 [4352/17352 (25%)] Loss: -207341.796875\n",
      "Train Epoch: 99 [5760/17352 (33%)] Loss: -171662.781250\n",
      "Train Epoch: 99 [7168/17352 (41%)] Loss: -137212.703125\n",
      "Train Epoch: 99 [8576/17352 (49%)] Loss: -172842.750000\n",
      "Train Epoch: 99 [9984/17352 (58%)] Loss: -53925.937500\n",
      "Train Epoch: 99 [11392/17352 (66%)] Loss: -171534.218750\n",
      "Train Epoch: 99 [12800/17352 (74%)] Loss: -234326.812500\n",
      "Train Epoch: 99 [14208/17352 (82%)] Loss: -149471.984375\n",
      "Train Epoch: 99 [15502/17352 (89%)] Loss: -149542.968750\n",
      "Train Epoch: 99 [16201/17352 (93%)] Loss: -17672.976562\n",
      "Train Epoch: 99 [17067/17352 (98%)] Loss: -101619.656250\n",
      "    epoch          : 99\n",
      "    loss           : -122330.41432794149\n",
      "    val_loss       : -55560.41117054621\n",
      "Train Epoch: 100 [128/17352 (1%)] Loss: -207401.906250\n",
      "Train Epoch: 100 [1536/17352 (9%)] Loss: 50459.867188\n",
      "Train Epoch: 100 [2944/17352 (17%)] Loss: -93215.054688\n",
      "Train Epoch: 100 [4352/17352 (25%)] Loss: -201721.203125\n",
      "Train Epoch: 100 [5760/17352 (33%)] Loss: -213181.843750\n",
      "Train Epoch: 100 [7168/17352 (41%)] Loss: 121505.171875\n",
      "Train Epoch: 100 [8576/17352 (49%)] Loss: -200475.562500\n",
      "Train Epoch: 100 [9984/17352 (58%)] Loss: -158098.484375\n",
      "Train Epoch: 100 [11392/17352 (66%)] Loss: -143363.093750\n",
      "Train Epoch: 100 [12800/17352 (74%)] Loss: -227318.671875\n",
      "Train Epoch: 100 [14208/17352 (82%)] Loss: -218035.156250\n",
      "Train Epoch: 100 [15483/17352 (89%)] Loss: -83030.843750\n",
      "Train Epoch: 100 [16325/17352 (94%)] Loss: -69745.476562\n",
      "Train Epoch: 100 [17123/17352 (99%)] Loss: -101865.171875\n",
      "    epoch          : 100\n",
      "    loss           : -138476.28234125944\n",
      "    val_loss       : -66410.7605305036\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0913_161229/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [128/17352 (1%)] Loss: -232351.656250\n",
      "Train Epoch: 101 [1536/17352 (9%)] Loss: -234162.093750\n",
      "Train Epoch: 101 [2944/17352 (17%)] Loss: -83230.968750\n",
      "Train Epoch: 101 [4352/17352 (25%)] Loss: -230648.296875\n",
      "Train Epoch: 101 [5760/17352 (33%)] Loss: -122788.203125\n",
      "Train Epoch: 101 [7168/17352 (41%)] Loss: -193365.906250\n",
      "Train Epoch: 101 [8576/17352 (49%)] Loss: -133559.390625\n",
      "Train Epoch: 101 [9984/17352 (58%)] Loss: -78021.984375\n",
      "Train Epoch: 101 [11392/17352 (66%)] Loss: -222204.187500\n",
      "Train Epoch: 101 [12800/17352 (74%)] Loss: -164036.640625\n",
      "Train Epoch: 101 [14208/17352 (82%)] Loss: -226943.859375\n",
      "Train Epoch: 101 [15485/17352 (89%)] Loss: -33369.558594\n",
      "Train Epoch: 101 [16170/17352 (93%)] Loss: -74228.703125\n",
      "Train Epoch: 101 [17122/17352 (99%)] Loss: -166871.046875\n",
      "    epoch          : 101\n",
      "    loss           : -125453.44903933122\n",
      "    val_loss       : -81063.4374420166\n",
      "Train Epoch: 102 [128/17352 (1%)] Loss: -213788.468750\n",
      "Train Epoch: 102 [1536/17352 (9%)] Loss: -202908.000000\n",
      "Train Epoch: 102 [2944/17352 (17%)] Loss: -193865.328125\n",
      "Train Epoch: 102 [4352/17352 (25%)] Loss: -222761.765625\n",
      "Train Epoch: 102 [5760/17352 (33%)] Loss: 127809.554688\n",
      "Train Epoch: 102 [7168/17352 (41%)] Loss: -130063.812500\n",
      "Train Epoch: 102 [8576/17352 (49%)] Loss: -232134.187500\n",
      "Train Epoch: 102 [9984/17352 (58%)] Loss: -207157.812500\n",
      "Train Epoch: 102 [11392/17352 (66%)] Loss: -102997.148438\n",
      "Train Epoch: 102 [12800/17352 (74%)] Loss: -206062.031250\n",
      "Train Epoch: 102 [14208/17352 (82%)] Loss: -75730.328125\n",
      "Train Epoch: 102 [15459/17352 (89%)] Loss: -27989.060547\n",
      "Train Epoch: 102 [16262/17352 (94%)] Loss: -5575.436523\n",
      "Train Epoch: 102 [17093/17352 (99%)] Loss: -64617.429688\n",
      "    epoch          : 102\n",
      "    loss           : -132571.86313378252\n",
      "    val_loss       : -55971.44492255847\n",
      "Train Epoch: 103 [128/17352 (1%)] Loss: -23092.804688\n",
      "Train Epoch: 103 [1536/17352 (9%)] Loss: -99386.187500\n",
      "Train Epoch: 103 [2944/17352 (17%)] Loss: -180755.859375\n",
      "Train Epoch: 103 [4352/17352 (25%)] Loss: -221826.593750\n",
      "Train Epoch: 103 [5760/17352 (33%)] Loss: -173526.203125\n",
      "Train Epoch: 103 [7168/17352 (41%)] Loss: -229964.718750\n",
      "Train Epoch: 103 [8576/17352 (49%)] Loss: -208452.578125\n",
      "Train Epoch: 103 [9984/17352 (58%)] Loss: -145504.890625\n",
      "Train Epoch: 103 [11392/17352 (66%)] Loss: -200166.203125\n",
      "Train Epoch: 103 [12800/17352 (74%)] Loss: -225558.562500\n",
      "Train Epoch: 103 [14208/17352 (82%)] Loss: -44037.332031\n",
      "Train Epoch: 103 [15562/17352 (90%)] Loss: -202030.593750\n",
      "Train Epoch: 103 [16328/17352 (94%)] Loss: -57563.914062\n",
      "Train Epoch: 103 [16937/17352 (98%)] Loss: -65908.390625\n",
      "    epoch          : 103\n",
      "    loss           : -131932.34613651558\n",
      "    val_loss       : -69813.4296274821\n",
      "Train Epoch: 104 [128/17352 (1%)] Loss: -2466.285156\n",
      "Train Epoch: 104 [1536/17352 (9%)] Loss: -160016.250000\n",
      "Train Epoch: 104 [2944/17352 (17%)] Loss: 87017.062500\n",
      "Train Epoch: 104 [4352/17352 (25%)] Loss: -100979.531250\n",
      "Train Epoch: 104 [5760/17352 (33%)] Loss: -226967.031250\n",
      "Train Epoch: 104 [7168/17352 (41%)] Loss: -169559.812500\n",
      "Train Epoch: 104 [8576/17352 (49%)] Loss: -223723.453125\n",
      "Train Epoch: 104 [9984/17352 (58%)] Loss: 30993.980469\n",
      "Train Epoch: 104 [11392/17352 (66%)] Loss: -209537.359375\n",
      "Train Epoch: 104 [12800/17352 (74%)] Loss: -158441.031250\n",
      "Train Epoch: 104 [14208/17352 (82%)] Loss: -214466.312500\n",
      "Train Epoch: 104 [15480/17352 (89%)] Loss: -7942.411133\n",
      "Train Epoch: 104 [16337/17352 (94%)] Loss: 21773.580078\n",
      "Train Epoch: 104 [17042/17352 (98%)] Loss: -79404.046875\n",
      "    epoch          : 104\n",
      "    loss           : -128705.57482992082\n",
      "    val_loss       : -59787.67752335866\n",
      "Train Epoch: 105 [128/17352 (1%)] Loss: -216780.921875\n",
      "Train Epoch: 105 [1536/17352 (9%)] Loss: -170861.000000\n",
      "Train Epoch: 105 [2944/17352 (17%)] Loss: -145912.171875\n",
      "Train Epoch: 105 [4352/17352 (25%)] Loss: -158760.375000\n",
      "Train Epoch: 105 [5760/17352 (33%)] Loss: -91752.937500\n",
      "Train Epoch: 105 [7168/17352 (41%)] Loss: 100011.078125\n",
      "Train Epoch: 105 [8576/17352 (49%)] Loss: -191903.203125\n",
      "Train Epoch: 105 [9984/17352 (58%)] Loss: -190056.718750\n",
      "Train Epoch: 105 [11392/17352 (66%)] Loss: -113692.195312\n",
      "Train Epoch: 105 [12800/17352 (74%)] Loss: -134854.468750\n",
      "Train Epoch: 105 [14208/17352 (82%)] Loss: -235627.984375\n",
      "Train Epoch: 105 [15504/17352 (89%)] Loss: -2244.712891\n",
      "Train Epoch: 105 [16314/17352 (94%)] Loss: -186543.109375\n",
      "Train Epoch: 105 [16966/17352 (98%)] Loss: -3703.054688\n",
      "    epoch          : 105\n",
      "    loss           : -130994.03082725986\n",
      "    val_loss       : -66701.45670700073\n",
      "Train Epoch: 106 [128/17352 (1%)] Loss: -62676.187500\n",
      "Train Epoch: 106 [1536/17352 (9%)] Loss: -222323.796875\n",
      "Train Epoch: 106 [2944/17352 (17%)] Loss: -107800.328125\n",
      "Train Epoch: 106 [4352/17352 (25%)] Loss: -183957.625000\n",
      "Train Epoch: 106 [5760/17352 (33%)] Loss: -228653.468750\n",
      "Train Epoch: 106 [7168/17352 (41%)] Loss: -44583.820312\n",
      "Train Epoch: 106 [8576/17352 (49%)] Loss: 71754.640625\n",
      "Train Epoch: 106 [9984/17352 (58%)] Loss: 6022.140625\n",
      "Train Epoch: 106 [11392/17352 (66%)] Loss: -217676.500000\n",
      "Train Epoch: 106 [12800/17352 (74%)] Loss: -143553.437500\n",
      "Train Epoch: 106 [14208/17352 (82%)] Loss: -228992.093750\n",
      "Train Epoch: 106 [15546/17352 (90%)] Loss: -127230.546875\n",
      "Train Epoch: 106 [16226/17352 (94%)] Loss: -26884.998047\n",
      "Train Epoch: 106 [17059/17352 (98%)] Loss: 14580.787109\n",
      "    epoch          : 106\n",
      "    loss           : -131079.75496473888\n",
      "    val_loss       : -70627.21415042877\n",
      "Train Epoch: 107 [128/17352 (1%)] Loss: -130464.312500\n",
      "Train Epoch: 107 [1536/17352 (9%)] Loss: -40649.976562\n",
      "Train Epoch: 107 [2944/17352 (17%)] Loss: -43643.812500\n",
      "Train Epoch: 107 [4352/17352 (25%)] Loss: -232478.281250\n",
      "Train Epoch: 107 [5760/17352 (33%)] Loss: -222849.234375\n",
      "Train Epoch: 107 [7168/17352 (41%)] Loss: -231082.375000\n",
      "Train Epoch: 107 [8576/17352 (49%)] Loss: -7670.355469\n",
      "Train Epoch: 107 [9984/17352 (58%)] Loss: -160852.953125\n",
      "Train Epoch: 107 [11392/17352 (66%)] Loss: -74766.218750\n",
      "Train Epoch: 107 [12800/17352 (74%)] Loss: -76770.898438\n",
      "Train Epoch: 107 [14208/17352 (82%)] Loss: -229620.750000\n",
      "Train Epoch: 107 [15584/17352 (90%)] Loss: -108823.156250\n",
      "Train Epoch: 107 [16338/17352 (94%)] Loss: -45908.808594\n",
      "Train Epoch: 107 [16920/17352 (98%)] Loss: -5555.817871\n",
      "    epoch          : 107\n",
      "    loss           : -121478.50599045721\n",
      "    val_loss       : -54417.9536553065\n",
      "Train Epoch: 108 [128/17352 (1%)] Loss: -235192.593750\n",
      "Train Epoch: 108 [1536/17352 (9%)] Loss: -214717.734375\n",
      "Train Epoch: 108 [2944/17352 (17%)] Loss: -209388.843750\n",
      "Train Epoch: 108 [4352/17352 (25%)] Loss: -204270.937500\n",
      "Train Epoch: 108 [5760/17352 (33%)] Loss: -122357.312500\n",
      "Train Epoch: 108 [7168/17352 (41%)] Loss: -145972.921875\n",
      "Train Epoch: 108 [8576/17352 (49%)] Loss: -219772.187500\n",
      "Train Epoch: 108 [9984/17352 (58%)] Loss: 113815.515625\n",
      "Train Epoch: 108 [11392/17352 (66%)] Loss: -201332.843750\n",
      "Train Epoch: 108 [12800/17352 (74%)] Loss: -62868.628906\n",
      "Train Epoch: 108 [14208/17352 (82%)] Loss: -69730.804688\n",
      "Train Epoch: 108 [15487/17352 (89%)] Loss: 62518.515625\n",
      "Train Epoch: 108 [16257/17352 (94%)] Loss: -54811.835938\n",
      "Train Epoch: 108 [16905/17352 (97%)] Loss: 180.400879\n",
      "    epoch          : 108\n",
      "    loss           : -132385.1817381174\n",
      "    val_loss       : -64375.836573235196\n",
      "Train Epoch: 109 [128/17352 (1%)] Loss: -12372.484375\n",
      "Train Epoch: 109 [1536/17352 (9%)] Loss: -214066.843750\n",
      "Train Epoch: 109 [2944/17352 (17%)] Loss: -158575.578125\n",
      "Train Epoch: 109 [4352/17352 (25%)] Loss: -159141.687500\n",
      "Train Epoch: 109 [5760/17352 (33%)] Loss: 26046.648438\n",
      "Train Epoch: 109 [7168/17352 (41%)] Loss: -221213.890625\n",
      "Train Epoch: 109 [8576/17352 (49%)] Loss: -223400.687500\n",
      "Train Epoch: 109 [9984/17352 (58%)] Loss: -199493.937500\n",
      "Train Epoch: 109 [11392/17352 (66%)] Loss: -172605.203125\n",
      "Train Epoch: 109 [12800/17352 (74%)] Loss: -165247.609375\n",
      "Train Epoch: 109 [14208/17352 (82%)] Loss: -194245.750000\n",
      "Train Epoch: 109 [15515/17352 (89%)] Loss: -69186.054688\n",
      "Train Epoch: 109 [16238/17352 (94%)] Loss: -62457.410156\n",
      "Train Epoch: 109 [17010/17352 (98%)] Loss: -57469.277344\n",
      "    epoch          : 109\n",
      "    loss           : -130490.80109584732\n",
      "    val_loss       : -63054.9160197258\n",
      "Train Epoch: 110 [128/17352 (1%)] Loss: -42439.218750\n",
      "Train Epoch: 110 [1536/17352 (9%)] Loss: 2952.238281\n",
      "Train Epoch: 110 [2944/17352 (17%)] Loss: -157660.343750\n",
      "Train Epoch: 110 [4352/17352 (25%)] Loss: -238839.375000\n",
      "Train Epoch: 110 [5760/17352 (33%)] Loss: -218707.703125\n",
      "Train Epoch: 110 [7168/17352 (41%)] Loss: -232280.218750\n",
      "Train Epoch: 110 [8576/17352 (49%)] Loss: -179363.343750\n",
      "Train Epoch: 110 [9984/17352 (58%)] Loss: -160179.843750\n",
      "Train Epoch: 110 [11392/17352 (66%)] Loss: -160284.375000\n",
      "Train Epoch: 110 [12800/17352 (74%)] Loss: -108715.578125\n",
      "Train Epoch: 110 [14208/17352 (82%)] Loss: -229020.046875\n",
      "Train Epoch: 110 [15523/17352 (89%)] Loss: -191670.593750\n",
      "Train Epoch: 110 [16114/17352 (93%)] Loss: -122802.765625\n",
      "Train Epoch: 110 [16874/17352 (97%)] Loss: -24802.160156\n",
      "    epoch          : 110\n",
      "    loss           : -124476.93026098469\n",
      "    val_loss       : -76315.51250189146\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0913_161229/checkpoint-epoch110.pth ...\n",
      "Train Epoch: 111 [128/17352 (1%)] Loss: -228988.437500\n",
      "Train Epoch: 111 [1536/17352 (9%)] Loss: -170328.562500\n",
      "Train Epoch: 111 [2944/17352 (17%)] Loss: -88444.500000\n",
      "Train Epoch: 111 [4352/17352 (25%)] Loss: -205229.671875\n",
      "Train Epoch: 111 [5760/17352 (33%)] Loss: -13467.585938\n",
      "Train Epoch: 111 [7168/17352 (41%)] Loss: -178154.718750\n",
      "Train Epoch: 111 [8576/17352 (49%)] Loss: 144411.437500\n",
      "Train Epoch: 111 [9984/17352 (58%)] Loss: -104386.406250\n",
      "Train Epoch: 111 [11392/17352 (66%)] Loss: -179749.140625\n",
      "Train Epoch: 111 [12800/17352 (74%)] Loss: -207629.437500\n",
      "Train Epoch: 111 [14208/17352 (82%)] Loss: -209463.234375\n",
      "Train Epoch: 111 [15543/17352 (90%)] Loss: -110458.460938\n",
      "Train Epoch: 111 [16155/17352 (93%)] Loss: -64274.125000\n",
      "Train Epoch: 111 [17018/17352 (98%)] Loss: -88967.023438\n",
      "    epoch          : 111\n",
      "    loss           : -131073.57411060718\n",
      "    val_loss       : -65103.370721054074\n",
      "Train Epoch: 112 [128/17352 (1%)] Loss: -171082.140625\n",
      "Train Epoch: 112 [1536/17352 (9%)] Loss: -71878.835938\n",
      "Train Epoch: 112 [2944/17352 (17%)] Loss: -235198.750000\n",
      "Train Epoch: 112 [4352/17352 (25%)] Loss: -212208.593750\n",
      "Train Epoch: 112 [5760/17352 (33%)] Loss: -52441.992188\n",
      "Train Epoch: 112 [7168/17352 (41%)] Loss: -125986.640625\n",
      "Train Epoch: 112 [8576/17352 (49%)] Loss: -211288.015625\n",
      "Train Epoch: 112 [9984/17352 (58%)] Loss: -194485.328125\n",
      "Train Epoch: 112 [11392/17352 (66%)] Loss: 86153.750000\n",
      "Train Epoch: 112 [12800/17352 (74%)] Loss: -158201.281250\n",
      "Train Epoch: 112 [14208/17352 (82%)] Loss: -205705.093750\n",
      "Train Epoch: 112 [15532/17352 (90%)] Loss: -14904.308594\n",
      "Train Epoch: 112 [16305/17352 (94%)] Loss: -86658.562500\n",
      "Train Epoch: 112 [16965/17352 (98%)] Loss: -64300.406250\n",
      "    epoch          : 112\n",
      "    loss           : -139124.6504561661\n",
      "    val_loss       : -81443.86664759318\n",
      "Train Epoch: 113 [128/17352 (1%)] Loss: -233027.640625\n",
      "Train Epoch: 113 [1536/17352 (9%)] Loss: -195369.187500\n",
      "Train Epoch: 113 [2944/17352 (17%)] Loss: -131765.406250\n",
      "Train Epoch: 113 [4352/17352 (25%)] Loss: -232102.546875\n",
      "Train Epoch: 113 [5760/17352 (33%)] Loss: -183012.625000\n",
      "Train Epoch: 113 [7168/17352 (41%)] Loss: -66252.632812\n",
      "Train Epoch: 113 [8576/17352 (49%)] Loss: -164825.437500\n",
      "Train Epoch: 113 [9984/17352 (58%)] Loss: -242024.578125\n",
      "Train Epoch: 113 [11392/17352 (66%)] Loss: -169813.781250\n",
      "Train Epoch: 113 [12800/17352 (74%)] Loss: -91529.523438\n",
      "Train Epoch: 113 [14208/17352 (82%)] Loss: -237105.828125\n",
      "Train Epoch: 113 [15504/17352 (89%)] Loss: -36697.523438\n",
      "Train Epoch: 113 [16130/17352 (93%)] Loss: -94967.531250\n",
      "Train Epoch: 113 [17028/17352 (98%)] Loss: -206111.406250\n",
      "    epoch          : 113\n",
      "    loss           : -136884.51185310926\n",
      "    val_loss       : -66567.54396896363\n",
      "Train Epoch: 114 [128/17352 (1%)] Loss: -141514.218750\n",
      "Train Epoch: 114 [1536/17352 (9%)] Loss: -210136.734375\n",
      "Train Epoch: 114 [2944/17352 (17%)] Loss: -133639.843750\n",
      "Train Epoch: 114 [4352/17352 (25%)] Loss: -144623.812500\n",
      "Train Epoch: 114 [5760/17352 (33%)] Loss: -218554.562500\n",
      "Train Epoch: 114 [7168/17352 (41%)] Loss: -114093.593750\n",
      "Train Epoch: 114 [8576/17352 (49%)] Loss: -194043.312500\n",
      "Train Epoch: 114 [9984/17352 (58%)] Loss: -165295.296875\n",
      "Train Epoch: 114 [11392/17352 (66%)] Loss: -228968.890625\n",
      "Train Epoch: 114 [12800/17352 (74%)] Loss: -134189.062500\n",
      "Train Epoch: 114 [14208/17352 (82%)] Loss: -204309.140625\n",
      "Train Epoch: 114 [15482/17352 (89%)] Loss: -148857.187500\n",
      "Train Epoch: 114 [16222/17352 (93%)] Loss: -124179.484375\n",
      "Train Epoch: 114 [17066/17352 (98%)] Loss: -177196.890625\n",
      "    epoch          : 114\n",
      "    loss           : -133585.98903497274\n",
      "    val_loss       : -72469.23693615595\n",
      "Train Epoch: 115 [128/17352 (1%)] Loss: -202568.984375\n",
      "Train Epoch: 115 [1536/17352 (9%)] Loss: -184627.343750\n",
      "Train Epoch: 115 [2944/17352 (17%)] Loss: -212368.343750\n",
      "Train Epoch: 115 [4352/17352 (25%)] Loss: -215102.421875\n",
      "Train Epoch: 115 [5760/17352 (33%)] Loss: -214991.468750\n",
      "Train Epoch: 115 [7168/17352 (41%)] Loss: -70896.804688\n",
      "Train Epoch: 115 [8576/17352 (49%)] Loss: -28742.882812\n",
      "Train Epoch: 115 [9984/17352 (58%)] Loss: -98575.578125\n",
      "Train Epoch: 115 [11392/17352 (66%)] Loss: -199008.000000\n",
      "Train Epoch: 115 [12800/17352 (74%)] Loss: -232652.484375\n",
      "Train Epoch: 115 [14208/17352 (82%)] Loss: 197327.968750\n",
      "Train Epoch: 115 [15510/17352 (89%)] Loss: 162706.921875\n",
      "Train Epoch: 115 [16266/17352 (94%)] Loss: -983.609375\n",
      "Train Epoch: 115 [16890/17352 (97%)] Loss: -16539.406250\n",
      "    epoch          : 115\n",
      "    loss           : -128137.9110762833\n",
      "    val_loss       : -70567.64047282537\n",
      "Train Epoch: 116 [128/17352 (1%)] Loss: -110370.546875\n",
      "Train Epoch: 116 [1536/17352 (9%)] Loss: -225364.593750\n",
      "Train Epoch: 116 [2944/17352 (17%)] Loss: -171807.906250\n",
      "Train Epoch: 116 [4352/17352 (25%)] Loss: -161495.359375\n",
      "Train Epoch: 116 [5760/17352 (33%)] Loss: -215927.906250\n",
      "Train Epoch: 116 [7168/17352 (41%)] Loss: -219786.140625\n",
      "Train Epoch: 116 [8576/17352 (49%)] Loss: -235161.968750\n",
      "Train Epoch: 116 [9984/17352 (58%)] Loss: -203054.875000\n",
      "Train Epoch: 116 [11392/17352 (66%)] Loss: -230645.937500\n",
      "Train Epoch: 116 [12800/17352 (74%)] Loss: -215197.000000\n",
      "Train Epoch: 116 [14208/17352 (82%)] Loss: -238104.562500\n",
      "Train Epoch: 116 [15491/17352 (89%)] Loss: -12233.351562\n",
      "Train Epoch: 116 [16321/17352 (94%)] Loss: -143345.906250\n",
      "Train Epoch: 116 [16918/17352 (97%)] Loss: -94345.015625\n",
      "    epoch          : 116\n",
      "    loss           : -139542.98434878356\n",
      "    val_loss       : -56046.485787963866\n",
      "Train Epoch: 117 [128/17352 (1%)] Loss: -172438.687500\n",
      "Train Epoch: 117 [1536/17352 (9%)] Loss: -228738.062500\n",
      "Train Epoch: 117 [2944/17352 (17%)] Loss: -57279.285156\n",
      "Train Epoch: 117 [4352/17352 (25%)] Loss: -75513.875000\n",
      "Train Epoch: 117 [5760/17352 (33%)] Loss: -204983.000000\n",
      "Train Epoch: 117 [7168/17352 (41%)] Loss: -166946.468750\n",
      "Train Epoch: 117 [8576/17352 (49%)] Loss: -4063.156250\n",
      "Train Epoch: 117 [9984/17352 (58%)] Loss: 223752.890625\n",
      "Train Epoch: 117 [11392/17352 (66%)] Loss: -182933.125000\n",
      "Train Epoch: 117 [12800/17352 (74%)] Loss: -224188.546875\n",
      "Train Epoch: 117 [14208/17352 (82%)] Loss: -157304.906250\n",
      "Train Epoch: 117 [15488/17352 (89%)] Loss: -75475.468750\n",
      "Train Epoch: 117 [16372/17352 (94%)] Loss: -46377.570312\n",
      "Train Epoch: 117 [17156/17352 (99%)] Loss: -169163.406250\n",
      "    epoch          : 117\n",
      "    loss           : -121561.84695905646\n",
      "    val_loss       : -63796.49728075663\n",
      "Train Epoch: 118 [128/17352 (1%)] Loss: -174437.031250\n",
      "Train Epoch: 118 [1536/17352 (9%)] Loss: -154494.390625\n",
      "Train Epoch: 118 [2944/17352 (17%)] Loss: -232229.562500\n",
      "Train Epoch: 118 [4352/17352 (25%)] Loss: -102845.351562\n",
      "Train Epoch: 118 [5760/17352 (33%)] Loss: -156781.265625\n",
      "Train Epoch: 118 [7168/17352 (41%)] Loss: -133760.718750\n",
      "Train Epoch: 118 [8576/17352 (49%)] Loss: -172770.140625\n",
      "Train Epoch: 118 [9984/17352 (58%)] Loss: -171194.515625\n",
      "Train Epoch: 118 [11392/17352 (66%)] Loss: -228912.593750\n",
      "Train Epoch: 118 [12800/17352 (74%)] Loss: -210197.093750\n",
      "Train Epoch: 118 [14208/17352 (82%)] Loss: -215136.437500\n",
      "Train Epoch: 118 [15464/17352 (89%)] Loss: -89210.765625\n",
      "Train Epoch: 118 [16307/17352 (94%)] Loss: -80420.750000\n",
      "Train Epoch: 118 [17046/17352 (98%)] Loss: -141926.562500\n",
      "    epoch          : 118\n",
      "    loss           : -130036.75116663172\n",
      "    val_loss       : -75445.89550951323\n",
      "Train Epoch: 119 [128/17352 (1%)] Loss: -159153.453125\n",
      "Train Epoch: 119 [1536/17352 (9%)] Loss: -70053.117188\n",
      "Train Epoch: 119 [2944/17352 (17%)] Loss: -160112.250000\n",
      "Train Epoch: 119 [4352/17352 (25%)] Loss: -70444.117188\n",
      "Train Epoch: 119 [5760/17352 (33%)] Loss: -198709.375000\n",
      "Train Epoch: 119 [7168/17352 (41%)] Loss: -228287.312500\n",
      "Train Epoch: 119 [8576/17352 (49%)] Loss: -164957.234375\n",
      "Train Epoch: 119 [9984/17352 (58%)] Loss: 22711.617188\n",
      "Train Epoch: 119 [11392/17352 (66%)] Loss: -88244.390625\n",
      "Train Epoch: 119 [12800/17352 (74%)] Loss: -201612.625000\n",
      "Train Epoch: 119 [14208/17352 (82%)] Loss: -221479.500000\n",
      "Train Epoch: 119 [15519/17352 (89%)] Loss: -85164.265625\n",
      "Train Epoch: 119 [16261/17352 (94%)] Loss: -21122.191406\n",
      "Train Epoch: 119 [17058/17352 (98%)] Loss: -168559.968750\n",
      "    epoch          : 119\n",
      "    loss           : -147330.94854039955\n",
      "    val_loss       : -81274.56551551819\n",
      "Train Epoch: 120 [128/17352 (1%)] Loss: -123123.476562\n",
      "Train Epoch: 120 [1536/17352 (9%)] Loss: -162034.187500\n",
      "Train Epoch: 120 [2944/17352 (17%)] Loss: -151022.875000\n",
      "Train Epoch: 120 [4352/17352 (25%)] Loss: -160056.828125\n",
      "Train Epoch: 120 [5760/17352 (33%)] Loss: -2944.201172\n",
      "Train Epoch: 120 [7168/17352 (41%)] Loss: -26069.777344\n",
      "Train Epoch: 120 [8576/17352 (49%)] Loss: -202456.125000\n",
      "Train Epoch: 120 [9984/17352 (58%)] Loss: -217258.093750\n",
      "Train Epoch: 120 [11392/17352 (66%)] Loss: -159911.187500\n",
      "Train Epoch: 120 [12800/17352 (74%)] Loss: -141057.687500\n",
      "Train Epoch: 120 [14208/17352 (82%)] Loss: 158996.234375\n",
      "Train Epoch: 120 [15460/17352 (89%)] Loss: -13417.464844\n",
      "Train Epoch: 120 [16196/17352 (93%)] Loss: -139548.250000\n",
      "Train Epoch: 120 [16949/17352 (98%)] Loss: -43583.949219\n",
      "    epoch          : 120\n",
      "    loss           : -131025.13354983745\n",
      "    val_loss       : -65654.309289217\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0913_161229/checkpoint-epoch120.pth ...\n",
      "Train Epoch: 121 [128/17352 (1%)] Loss: -159835.296875\n",
      "Train Epoch: 121 [1536/17352 (9%)] Loss: -163004.953125\n",
      "Train Epoch: 121 [2944/17352 (17%)] Loss: -207191.343750\n",
      "Train Epoch: 121 [4352/17352 (25%)] Loss: -129007.750000\n",
      "Train Epoch: 121 [5760/17352 (33%)] Loss: 14097.839844\n",
      "Train Epoch: 121 [7168/17352 (41%)] Loss: -76391.843750\n",
      "Train Epoch: 121 [8576/17352 (49%)] Loss: 23762.832031\n",
      "Train Epoch: 121 [9984/17352 (58%)] Loss: -162139.437500\n",
      "Train Epoch: 121 [11392/17352 (66%)] Loss: -61524.277344\n",
      "Train Epoch: 121 [12800/17352 (74%)] Loss: -227250.593750\n",
      "Train Epoch: 121 [14208/17352 (82%)] Loss: -187549.828125\n",
      "Train Epoch: 121 [15478/17352 (89%)] Loss: -154641.703125\n",
      "Train Epoch: 121 [16196/17352 (93%)] Loss: -91648.921875\n",
      "Train Epoch: 121 [17032/17352 (98%)] Loss: -151604.625000\n",
      "    epoch          : 121\n",
      "    loss           : -132541.22608077287\n",
      "    val_loss       : -82912.77913424173\n",
      "Train Epoch: 122 [128/17352 (1%)] Loss: -133808.578125\n",
      "Train Epoch: 122 [1536/17352 (9%)] Loss: -216739.781250\n",
      "Train Epoch: 122 [2944/17352 (17%)] Loss: -60831.382812\n",
      "Train Epoch: 122 [4352/17352 (25%)] Loss: -223854.968750\n",
      "Train Epoch: 122 [5760/17352 (33%)] Loss: -166005.375000\n",
      "Train Epoch: 122 [7168/17352 (41%)] Loss: -232392.281250\n",
      "Train Epoch: 122 [8576/17352 (49%)] Loss: -151151.296875\n",
      "Train Epoch: 122 [9984/17352 (58%)] Loss: -129285.093750\n",
      "Train Epoch: 122 [11392/17352 (66%)] Loss: -220047.687500\n",
      "Train Epoch: 122 [12800/17352 (74%)] Loss: -169823.437500\n",
      "Train Epoch: 122 [14208/17352 (82%)] Loss: -134531.531250\n",
      "Train Epoch: 122 [15557/17352 (90%)] Loss: -153828.875000\n",
      "Train Epoch: 122 [16153/17352 (93%)] Loss: 12461.737305\n",
      "Train Epoch: 122 [16931/17352 (98%)] Loss: -126218.593750\n",
      "    epoch          : 122\n",
      "    loss           : -136707.19328891832\n",
      "    val_loss       : -80018.04144595464\n",
      "Train Epoch: 123 [128/17352 (1%)] Loss: -47759.792969\n",
      "Train Epoch: 123 [1536/17352 (9%)] Loss: -223474.281250\n",
      "Train Epoch: 123 [2944/17352 (17%)] Loss: -146956.156250\n",
      "Train Epoch: 123 [4352/17352 (25%)] Loss: -224720.875000\n",
      "Train Epoch: 123 [5760/17352 (33%)] Loss: 100494.187500\n",
      "Train Epoch: 123 [7168/17352 (41%)] Loss: -200887.953125\n",
      "Train Epoch: 123 [8576/17352 (49%)] Loss: -147156.218750\n",
      "Train Epoch: 123 [9984/17352 (58%)] Loss: -150585.984375\n",
      "Train Epoch: 123 [11392/17352 (66%)] Loss: -72396.406250\n",
      "Train Epoch: 123 [12800/17352 (74%)] Loss: 614.921875\n",
      "Train Epoch: 123 [14208/17352 (82%)] Loss: -216617.734375\n",
      "Train Epoch: 123 [15474/17352 (89%)] Loss: -118314.859375\n",
      "Train Epoch: 123 [16361/17352 (94%)] Loss: -141490.062500\n",
      "Train Epoch: 123 [17016/17352 (98%)] Loss: -44866.339844\n",
      "    epoch          : 123\n",
      "    loss           : -140584.11601759124\n",
      "    val_loss       : -80654.96774775187\n",
      "Train Epoch: 124 [128/17352 (1%)] Loss: 181011.328125\n",
      "Train Epoch: 124 [1536/17352 (9%)] Loss: 61197.132812\n",
      "Train Epoch: 124 [2944/17352 (17%)] Loss: -92806.601562\n",
      "Train Epoch: 124 [4352/17352 (25%)] Loss: -239346.531250\n",
      "Train Epoch: 124 [5760/17352 (33%)] Loss: -118212.976562\n",
      "Train Epoch: 124 [7168/17352 (41%)] Loss: -61922.175781\n",
      "Train Epoch: 124 [8576/17352 (49%)] Loss: -254377.625000\n",
      "Train Epoch: 124 [9984/17352 (58%)] Loss: 141752.937500\n",
      "Train Epoch: 124 [11392/17352 (66%)] Loss: -225898.906250\n",
      "Train Epoch: 124 [12800/17352 (74%)] Loss: -152669.203125\n",
      "Train Epoch: 124 [14208/17352 (82%)] Loss: -215563.437500\n",
      "Train Epoch: 124 [15602/17352 (90%)] Loss: -53876.640625\n",
      "Train Epoch: 124 [16455/17352 (95%)] Loss: -149743.828125\n",
      "Train Epoch: 124 [17113/17352 (99%)] Loss: -120549.765625\n",
      "    epoch          : 124\n",
      "    loss           : -126580.94671835675\n",
      "    val_loss       : -75773.49438994726\n",
      "Train Epoch: 125 [128/17352 (1%)] Loss: -227310.906250\n",
      "Train Epoch: 125 [1536/17352 (9%)] Loss: -159807.515625\n",
      "Train Epoch: 125 [2944/17352 (17%)] Loss: -235092.046875\n",
      "Train Epoch: 125 [4352/17352 (25%)] Loss: -209762.640625\n",
      "Train Epoch: 125 [5760/17352 (33%)] Loss: -220939.687500\n",
      "Train Epoch: 125 [7168/17352 (41%)] Loss: -233970.968750\n",
      "Train Epoch: 125 [8576/17352 (49%)] Loss: -214248.000000\n",
      "Train Epoch: 125 [9984/17352 (58%)] Loss: -160201.265625\n",
      "Train Epoch: 125 [11392/17352 (66%)] Loss: -8544.917969\n",
      "Train Epoch: 125 [12800/17352 (74%)] Loss: -221148.875000\n",
      "Train Epoch: 125 [14208/17352 (82%)] Loss: -169653.390625\n",
      "Train Epoch: 125 [15549/17352 (90%)] Loss: -56813.011719\n",
      "Train Epoch: 125 [16402/17352 (95%)] Loss: -121000.390625\n",
      "Train Epoch: 125 [17144/17352 (99%)] Loss: -139326.359375\n",
      "    epoch          : 125\n",
      "    loss           : -136825.1026521209\n",
      "    val_loss       : -74475.59478030205\n",
      "Train Epoch: 126 [128/17352 (1%)] Loss: -93870.468750\n",
      "Train Epoch: 126 [1536/17352 (9%)] Loss: -194956.953125\n",
      "Train Epoch: 126 [2944/17352 (17%)] Loss: -216271.578125\n",
      "Train Epoch: 126 [4352/17352 (25%)] Loss: -219628.218750\n",
      "Train Epoch: 126 [5760/17352 (33%)] Loss: -100371.242188\n",
      "Train Epoch: 126 [7168/17352 (41%)] Loss: -231780.078125\n",
      "Train Epoch: 126 [8576/17352 (49%)] Loss: -256342.968750\n",
      "Train Epoch: 126 [9984/17352 (58%)] Loss: -212799.406250\n",
      "Train Epoch: 126 [11392/17352 (66%)] Loss: -203554.187500\n",
      "Train Epoch: 126 [12800/17352 (74%)] Loss: -197760.140625\n",
      "Train Epoch: 126 [14208/17352 (82%)] Loss: -232157.375000\n",
      "Train Epoch: 126 [15547/17352 (90%)] Loss: -192716.531250\n",
      "Train Epoch: 126 [16150/17352 (93%)] Loss: -72397.585938\n",
      "Train Epoch: 126 [16994/17352 (98%)] Loss: -33661.902344\n",
      "    epoch          : 126\n",
      "    loss           : -138802.9345293493\n",
      "    val_loss       : -55609.454528999326\n",
      "Train Epoch: 127 [128/17352 (1%)] Loss: -233591.062500\n",
      "Train Epoch: 127 [1536/17352 (9%)] Loss: -217030.390625\n",
      "Train Epoch: 127 [2944/17352 (17%)] Loss: -197423.812500\n",
      "Train Epoch: 127 [4352/17352 (25%)] Loss: -225695.250000\n",
      "Train Epoch: 127 [5760/17352 (33%)] Loss: 66586.093750\n",
      "Train Epoch: 127 [7168/17352 (41%)] Loss: -165382.578125\n",
      "Train Epoch: 127 [8576/17352 (49%)] Loss: -211372.500000\n",
      "Train Epoch: 127 [9984/17352 (58%)] Loss: -114570.359375\n",
      "Train Epoch: 127 [11392/17352 (66%)] Loss: -152387.296875\n",
      "Train Epoch: 127 [12800/17352 (74%)] Loss: -124610.843750\n",
      "Train Epoch: 127 [14208/17352 (82%)] Loss: -27803.703125\n",
      "Train Epoch: 127 [15523/17352 (89%)] Loss: -98655.265625\n",
      "Train Epoch: 127 [16258/17352 (94%)] Loss: -4898.785645\n",
      "Train Epoch: 127 [16894/17352 (97%)] Loss: -5753.919922\n",
      "    epoch          : 127\n",
      "    loss           : -117333.6700169096\n",
      "    val_loss       : -73480.81880003611\n",
      "Train Epoch: 128 [128/17352 (1%)] Loss: -161447.046875\n",
      "Train Epoch: 128 [1536/17352 (9%)] Loss: -229166.093750\n",
      "Train Epoch: 128 [2944/17352 (17%)] Loss: -156337.828125\n",
      "Train Epoch: 128 [4352/17352 (25%)] Loss: 71976.117188\n",
      "Train Epoch: 128 [5760/17352 (33%)] Loss: -217516.828125\n",
      "Train Epoch: 128 [7168/17352 (41%)] Loss: -163658.421875\n",
      "Train Epoch: 128 [8576/17352 (49%)] Loss: -206853.562500\n",
      "Train Epoch: 128 [9984/17352 (58%)] Loss: -204303.515625\n",
      "Train Epoch: 128 [11392/17352 (66%)] Loss: -225545.562500\n",
      "Train Epoch: 128 [12800/17352 (74%)] Loss: -55977.386719\n",
      "Train Epoch: 128 [14208/17352 (82%)] Loss: -52333.101562\n",
      "Train Epoch: 128 [15470/17352 (89%)] Loss: -7011.983398\n",
      "Train Epoch: 128 [16192/17352 (93%)] Loss: -24265.218750\n",
      "Train Epoch: 128 [16997/17352 (98%)] Loss: -92147.000000\n",
      "    epoch          : 128\n",
      "    loss           : -140119.7136181313\n",
      "    val_loss       : -54153.32830781936\n",
      "Train Epoch: 129 [128/17352 (1%)] Loss: -234563.140625\n",
      "Train Epoch: 129 [1536/17352 (9%)] Loss: -115818.140625\n",
      "Train Epoch: 129 [2944/17352 (17%)] Loss: -133952.421875\n",
      "Train Epoch: 129 [4352/17352 (25%)] Loss: -211309.671875\n",
      "Train Epoch: 129 [5760/17352 (33%)] Loss: -137386.718750\n",
      "Train Epoch: 129 [7168/17352 (41%)] Loss: -168790.093750\n",
      "Train Epoch: 129 [8576/17352 (49%)] Loss: -168986.781250\n",
      "Train Epoch: 129 [9984/17352 (58%)] Loss: -36715.109375\n",
      "Train Epoch: 129 [11392/17352 (66%)] Loss: 52349.421875\n",
      "Train Epoch: 129 [12800/17352 (74%)] Loss: -228557.609375\n",
      "Train Epoch: 129 [14208/17352 (82%)] Loss: -217464.875000\n",
      "Train Epoch: 129 [15437/17352 (89%)] Loss: -25962.505859\n",
      "Train Epoch: 129 [16265/17352 (94%)] Loss: -5716.160645\n",
      "Train Epoch: 129 [16995/17352 (98%)] Loss: -14916.716797\n",
      "    epoch          : 129\n",
      "    loss           : -121825.25531374528\n",
      "    val_loss       : -50916.46105477015\n",
      "Train Epoch: 130 [128/17352 (1%)] Loss: -224971.406250\n",
      "Train Epoch: 130 [1536/17352 (9%)] Loss: 53068.652344\n",
      "Train Epoch: 130 [2944/17352 (17%)] Loss: -51381.359375\n",
      "Train Epoch: 130 [4352/17352 (25%)] Loss: -150420.453125\n",
      "Train Epoch: 130 [5760/17352 (33%)] Loss: -51192.734375\n",
      "Train Epoch: 130 [7168/17352 (41%)] Loss: 119008.015625\n",
      "Train Epoch: 130 [8576/17352 (49%)] Loss: -154729.781250\n",
      "Train Epoch: 130 [9984/17352 (58%)] Loss: -240998.406250\n",
      "Train Epoch: 130 [11392/17352 (66%)] Loss: -258969.031250\n",
      "Train Epoch: 130 [12800/17352 (74%)] Loss: 355770.625000\n",
      "Train Epoch: 130 [14208/17352 (82%)] Loss: -153981.937500\n",
      "Train Epoch: 130 [15499/17352 (89%)] Loss: -118775.648438\n",
      "Train Epoch: 130 [16259/17352 (94%)] Loss: -9291.343750\n",
      "Train Epoch: 130 [16957/17352 (98%)] Loss: -62874.949219\n",
      "    epoch          : 130\n",
      "    loss           : -124739.1062683515\n",
      "    val_loss       : -58492.38765584628\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0913_161229/checkpoint-epoch130.pth ...\n",
      "Train Epoch: 131 [128/17352 (1%)] Loss: -158776.125000\n",
      "Train Epoch: 131 [1536/17352 (9%)] Loss: -204716.671875\n",
      "Train Epoch: 131 [2944/17352 (17%)] Loss: -202855.718750\n",
      "Train Epoch: 131 [4352/17352 (25%)] Loss: -229116.656250\n",
      "Train Epoch: 131 [5760/17352 (33%)] Loss: -52784.238281\n",
      "Train Epoch: 131 [7168/17352 (41%)] Loss: -223708.515625\n",
      "Train Epoch: 131 [8576/17352 (49%)] Loss: -186878.781250\n",
      "Train Epoch: 131 [9984/17352 (58%)] Loss: 25590.037109\n",
      "Train Epoch: 131 [11392/17352 (66%)] Loss: -183296.750000\n",
      "Train Epoch: 131 [12800/17352 (74%)] Loss: -116607.562500\n",
      "Train Epoch: 131 [14208/17352 (82%)] Loss: -211598.156250\n",
      "Train Epoch: 131 [15399/17352 (89%)] Loss: -1200.305664\n",
      "Train Epoch: 131 [16217/17352 (93%)] Loss: -91880.914062\n",
      "Train Epoch: 131 [16945/17352 (98%)] Loss: -69728.125000\n",
      "    epoch          : 131\n",
      "    loss           : -129704.52733391883\n",
      "    val_loss       : -65203.75597082774\n",
      "Train Epoch: 132 [128/17352 (1%)] Loss: -136591.812500\n",
      "Train Epoch: 132 [1536/17352 (9%)] Loss: -159316.312500\n",
      "Train Epoch: 132 [2944/17352 (17%)] Loss: -21598.828125\n",
      "Train Epoch: 132 [4352/17352 (25%)] Loss: -121099.093750\n",
      "Train Epoch: 132 [5760/17352 (33%)] Loss: -159334.000000\n",
      "Train Epoch: 132 [7168/17352 (41%)] Loss: -162590.953125\n",
      "Train Epoch: 132 [8576/17352 (49%)] Loss: 103327.984375\n",
      "Train Epoch: 132 [9984/17352 (58%)] Loss: -224000.140625\n",
      "Train Epoch: 132 [11392/17352 (66%)] Loss: -78692.734375\n",
      "Train Epoch: 132 [12800/17352 (74%)] Loss: -216329.468750\n",
      "Train Epoch: 132 [14208/17352 (82%)] Loss: -153676.187500\n",
      "Train Epoch: 132 [15592/17352 (90%)] Loss: -174711.140625\n",
      "Train Epoch: 132 [16412/17352 (95%)] Loss: -158089.171875\n",
      "Train Epoch: 132 [17006/17352 (98%)] Loss: -127853.023438\n",
      "    epoch          : 132\n",
      "    loss           : -137175.1193601877\n",
      "    val_loss       : -79587.50869909923\n",
      "Train Epoch: 133 [128/17352 (1%)] Loss: 6388.773438\n",
      "Train Epoch: 133 [1536/17352 (9%)] Loss: -218073.609375\n",
      "Train Epoch: 133 [2944/17352 (17%)] Loss: -233491.468750\n",
      "Train Epoch: 133 [4352/17352 (25%)] Loss: -233825.171875\n",
      "Train Epoch: 133 [5760/17352 (33%)] Loss: -111812.562500\n",
      "Train Epoch: 133 [7168/17352 (41%)] Loss: -174629.921875\n",
      "Train Epoch: 133 [8576/17352 (49%)] Loss: -210165.046875\n",
      "Train Epoch: 133 [9984/17352 (58%)] Loss: -109787.531250\n",
      "Train Epoch: 133 [11392/17352 (66%)] Loss: -237319.703125\n",
      "Train Epoch: 133 [12800/17352 (74%)] Loss: -117559.921875\n",
      "Train Epoch: 133 [14208/17352 (82%)] Loss: -155608.328125\n",
      "Train Epoch: 133 [15510/17352 (89%)] Loss: -147713.375000\n",
      "Train Epoch: 133 [16403/17352 (95%)] Loss: -12670.174805\n",
      "Train Epoch: 133 [17164/17352 (99%)] Loss: -75782.101562\n",
      "    epoch          : 133\n",
      "    loss           : -134446.91881013397\n",
      "    val_loss       : -69394.07183396022\n",
      "Train Epoch: 134 [128/17352 (1%)] Loss: -7125.156250\n",
      "Train Epoch: 134 [1536/17352 (9%)] Loss: -64539.859375\n",
      "Train Epoch: 134 [2944/17352 (17%)] Loss: -216985.187500\n",
      "Train Epoch: 134 [4352/17352 (25%)] Loss: -56028.164062\n",
      "Train Epoch: 134 [5760/17352 (33%)] Loss: -157001.140625\n",
      "Train Epoch: 134 [7168/17352 (41%)] Loss: -207532.750000\n",
      "Train Epoch: 134 [8576/17352 (49%)] Loss: -197493.703125\n",
      "Train Epoch: 134 [9984/17352 (58%)] Loss: -232464.093750\n",
      "Train Epoch: 134 [11392/17352 (66%)] Loss: -197440.671875\n",
      "Train Epoch: 134 [12800/17352 (74%)] Loss: -226825.500000\n",
      "Train Epoch: 134 [14208/17352 (82%)] Loss: 74304.500000\n",
      "Train Epoch: 134 [15533/17352 (90%)] Loss: 16060.601562\n",
      "Train Epoch: 134 [16156/17352 (93%)] Loss: 2420.834961\n",
      "Train Epoch: 134 [16948/17352 (98%)] Loss: -152882.640625\n",
      "    epoch          : 134\n",
      "    loss           : -120746.6504856596\n",
      "    val_loss       : -88687.91764952341\n",
      "Train Epoch: 135 [128/17352 (1%)] Loss: 143276.609375\n",
      "Train Epoch: 135 [1536/17352 (9%)] Loss: -172553.125000\n",
      "Train Epoch: 135 [2944/17352 (17%)] Loss: 249635.906250\n",
      "Train Epoch: 135 [4352/17352 (25%)] Loss: -213388.375000\n",
      "Train Epoch: 135 [5760/17352 (33%)] Loss: -58635.933594\n",
      "Train Epoch: 135 [7168/17352 (41%)] Loss: -228163.218750\n",
      "Train Epoch: 135 [8576/17352 (49%)] Loss: -94865.531250\n",
      "Train Epoch: 135 [9984/17352 (58%)] Loss: -220091.109375\n",
      "Train Epoch: 135 [11392/17352 (66%)] Loss: -233274.562500\n",
      "Train Epoch: 135 [12800/17352 (74%)] Loss: -146734.531250\n",
      "Train Epoch: 135 [14208/17352 (82%)] Loss: -237684.703125\n",
      "Train Epoch: 135 [15546/17352 (90%)] Loss: -146403.000000\n",
      "Train Epoch: 135 [16189/17352 (93%)] Loss: -135742.671875\n",
      "Train Epoch: 135 [16933/17352 (98%)] Loss: -93150.515625\n",
      "    epoch          : 135\n",
      "    loss           : -131294.10087759543\n",
      "    val_loss       : -79881.86267077128\n",
      "Train Epoch: 136 [128/17352 (1%)] Loss: -168323.968750\n",
      "Train Epoch: 136 [1536/17352 (9%)] Loss: -32759.644531\n",
      "Train Epoch: 136 [2944/17352 (17%)] Loss: -204450.781250\n",
      "Train Epoch: 136 [4352/17352 (25%)] Loss: -223150.093750\n",
      "Train Epoch: 136 [5760/17352 (33%)] Loss: -51663.207031\n",
      "Train Epoch: 136 [7168/17352 (41%)] Loss: -171416.593750\n",
      "Train Epoch: 136 [8576/17352 (49%)] Loss: 343095.562500\n",
      "Train Epoch: 136 [9984/17352 (58%)] Loss: -40937.328125\n",
      "Train Epoch: 136 [11392/17352 (66%)] Loss: -136342.484375\n",
      "Train Epoch: 136 [12800/17352 (74%)] Loss: -53522.191406\n",
      "Train Epoch: 136 [14208/17352 (82%)] Loss: -217391.656250\n",
      "Train Epoch: 136 [15549/17352 (90%)] Loss: -97830.976562\n",
      "Train Epoch: 136 [16396/17352 (94%)] Loss: -67328.304688\n",
      "Train Epoch: 136 [17124/17352 (99%)] Loss: -40201.667969\n",
      "    epoch          : 136\n",
      "    loss           : -134724.5627425021\n",
      "    val_loss       : -63749.17861302694\n",
      "Train Epoch: 137 [128/17352 (1%)] Loss: -232346.031250\n",
      "Train Epoch: 137 [1536/17352 (9%)] Loss: -233510.343750\n",
      "Train Epoch: 137 [2944/17352 (17%)] Loss: -208659.546875\n",
      "Train Epoch: 137 [4352/17352 (25%)] Loss: -93420.046875\n",
      "Train Epoch: 137 [5760/17352 (33%)] Loss: -92978.492188\n",
      "Train Epoch: 137 [7168/17352 (41%)] Loss: -163747.656250\n",
      "Train Epoch: 137 [8576/17352 (49%)] Loss: -223670.187500\n",
      "Train Epoch: 137 [9984/17352 (58%)] Loss: -209882.859375\n",
      "Train Epoch: 137 [11392/17352 (66%)] Loss: -88080.148438\n",
      "Train Epoch: 137 [12800/17352 (74%)] Loss: -206453.687500\n",
      "Train Epoch: 137 [14208/17352 (82%)] Loss: -201838.468750\n",
      "Train Epoch: 137 [15505/17352 (89%)] Loss: -100931.703125\n",
      "Train Epoch: 137 [16247/17352 (94%)] Loss: -72596.453125\n",
      "Train Epoch: 137 [17130/17352 (99%)] Loss: -143926.421875\n",
      "    epoch          : 137\n",
      "    loss           : -129046.48248741611\n",
      "    val_loss       : -76866.579322052\n",
      "Train Epoch: 138 [128/17352 (1%)] Loss: -55102.792969\n",
      "Train Epoch: 138 [1536/17352 (9%)] Loss: -218604.781250\n",
      "Train Epoch: 138 [2944/17352 (17%)] Loss: -232729.468750\n",
      "Train Epoch: 138 [4352/17352 (25%)] Loss: -160110.687500\n",
      "Train Epoch: 138 [5760/17352 (33%)] Loss: -242477.062500\n",
      "Train Epoch: 138 [7168/17352 (41%)] Loss: -119173.851562\n",
      "Train Epoch: 138 [8576/17352 (49%)] Loss: -223123.312500\n",
      "Train Epoch: 138 [9984/17352 (58%)] Loss: -175734.718750\n",
      "Train Epoch: 138 [11392/17352 (66%)] Loss: -200724.187500\n",
      "Train Epoch: 138 [12800/17352 (74%)] Loss: -39095.113281\n",
      "Train Epoch: 138 [14208/17352 (82%)] Loss: -134049.109375\n",
      "Train Epoch: 138 [15547/17352 (90%)] Loss: -159816.718750\n",
      "Train Epoch: 138 [16324/17352 (94%)] Loss: -134065.843750\n",
      "Train Epoch: 138 [17028/17352 (98%)] Loss: -81091.093750\n",
      "    epoch          : 138\n",
      "    loss           : -138071.3722603817\n",
      "    val_loss       : -84334.41521816254\n",
      "Train Epoch: 139 [128/17352 (1%)] Loss: -117722.523438\n",
      "Train Epoch: 139 [1536/17352 (9%)] Loss: 63072.472656\n",
      "Train Epoch: 139 [2944/17352 (17%)] Loss: -162368.171875\n",
      "Train Epoch: 139 [4352/17352 (25%)] Loss: -67008.265625\n",
      "Train Epoch: 139 [5760/17352 (33%)] Loss: -238136.593750\n",
      "Train Epoch: 139 [7168/17352 (41%)] Loss: -227295.562500\n",
      "Train Epoch: 139 [8576/17352 (49%)] Loss: -138361.656250\n",
      "Train Epoch: 139 [9984/17352 (58%)] Loss: -233920.218750\n",
      "Train Epoch: 139 [11392/17352 (66%)] Loss: -94096.015625\n",
      "Train Epoch: 139 [12800/17352 (74%)] Loss: -72571.546875\n",
      "Train Epoch: 139 [14208/17352 (82%)] Loss: -243561.703125\n",
      "Train Epoch: 139 [15476/17352 (89%)] Loss: -66938.000000\n",
      "Train Epoch: 139 [16423/17352 (95%)] Loss: 39978.796875\n",
      "Train Epoch: 139 [17052/17352 (98%)] Loss: -122618.031250\n",
      "    epoch          : 139\n",
      "    loss           : -135176.84553271811\n",
      "    val_loss       : -89161.10224014918\n",
      "Train Epoch: 140 [128/17352 (1%)] Loss: -159411.750000\n",
      "Train Epoch: 140 [1536/17352 (9%)] Loss: -227676.015625\n",
      "Train Epoch: 140 [2944/17352 (17%)] Loss: -21402.152344\n",
      "Train Epoch: 140 [4352/17352 (25%)] Loss: -215076.343750\n",
      "Train Epoch: 140 [5760/17352 (33%)] Loss: -237452.234375\n",
      "Train Epoch: 140 [7168/17352 (41%)] Loss: -121759.234375\n",
      "Train Epoch: 140 [8576/17352 (49%)] Loss: -10436.554688\n",
      "Train Epoch: 140 [9984/17352 (58%)] Loss: -218885.234375\n",
      "Train Epoch: 140 [11392/17352 (66%)] Loss: -24702.714844\n",
      "Train Epoch: 140 [12800/17352 (74%)] Loss: -176243.343750\n",
      "Train Epoch: 140 [14208/17352 (82%)] Loss: -184658.515625\n",
      "Train Epoch: 140 [15461/17352 (89%)] Loss: -150078.812500\n",
      "Train Epoch: 140 [16174/17352 (93%)] Loss: -61527.453125\n",
      "Train Epoch: 140 [16947/17352 (98%)] Loss: -161948.062500\n",
      "    epoch          : 140\n",
      "    loss           : -125494.02473521393\n",
      "    val_loss       : -75874.71844444275\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0913_161229/checkpoint-epoch140.pth ...\n",
      "Train Epoch: 141 [128/17352 (1%)] Loss: -162476.968750\n",
      "Train Epoch: 141 [1536/17352 (9%)] Loss: -230145.468750\n",
      "Train Epoch: 141 [2944/17352 (17%)] Loss: -140476.359375\n",
      "Train Epoch: 141 [4352/17352 (25%)] Loss: -241177.406250\n",
      "Train Epoch: 141 [5760/17352 (33%)] Loss: -65646.726562\n",
      "Train Epoch: 141 [7168/17352 (41%)] Loss: -63628.160156\n",
      "Train Epoch: 141 [8576/17352 (49%)] Loss: -190085.203125\n",
      "Train Epoch: 141 [9984/17352 (58%)] Loss: -36180.402344\n",
      "Train Epoch: 141 [11392/17352 (66%)] Loss: -201941.593750\n",
      "Train Epoch: 141 [12800/17352 (74%)] Loss: -149989.187500\n",
      "Train Epoch: 141 [14208/17352 (82%)] Loss: -229902.687500\n",
      "Train Epoch: 141 [15406/17352 (89%)] Loss: -2512.563721\n",
      "Train Epoch: 141 [16285/17352 (94%)] Loss: -116476.796875\n",
      "Train Epoch: 141 [17037/17352 (98%)] Loss: -146126.828125\n",
      "    epoch          : 141\n",
      "    loss           : -124862.27102067166\n",
      "    val_loss       : -69456.4870018959\n",
      "Train Epoch: 142 [128/17352 (1%)] Loss: -19447.613281\n",
      "Train Epoch: 142 [1536/17352 (9%)] Loss: -223495.125000\n",
      "Train Epoch: 142 [2944/17352 (17%)] Loss: -170760.609375\n",
      "Train Epoch: 142 [4352/17352 (25%)] Loss: -226947.562500\n",
      "Train Epoch: 142 [5760/17352 (33%)] Loss: -49359.113281\n",
      "Train Epoch: 142 [7168/17352 (41%)] Loss: -174714.437500\n",
      "Train Epoch: 142 [8576/17352 (49%)] Loss: -180339.078125\n",
      "Train Epoch: 142 [9984/17352 (58%)] Loss: -168125.859375\n",
      "Train Epoch: 142 [11392/17352 (66%)] Loss: -162567.421875\n",
      "Train Epoch: 142 [12800/17352 (74%)] Loss: -229865.484375\n",
      "Train Epoch: 142 [14208/17352 (82%)] Loss: -203684.281250\n",
      "Train Epoch: 142 [15487/17352 (89%)] Loss: -63968.804688\n",
      "Train Epoch: 142 [16284/17352 (94%)] Loss: -36732.136719\n",
      "Train Epoch: 142 [16989/17352 (98%)] Loss: -24521.691406\n",
      "    epoch          : 142\n",
      "    loss           : -134493.01980160706\n",
      "    val_loss       : -80359.25478884378\n",
      "Train Epoch: 143 [128/17352 (1%)] Loss: -234291.125000\n",
      "Train Epoch: 143 [1536/17352 (9%)] Loss: -213387.906250\n",
      "Train Epoch: 143 [2944/17352 (17%)] Loss: 16035.734375\n",
      "Train Epoch: 143 [4352/17352 (25%)] Loss: -174973.968750\n",
      "Train Epoch: 143 [5760/17352 (33%)] Loss: -232548.093750\n",
      "Train Epoch: 143 [7168/17352 (41%)] Loss: -71964.164062\n",
      "Train Epoch: 143 [8576/17352 (49%)] Loss: -109325.046875\n",
      "Train Epoch: 143 [9984/17352 (58%)] Loss: -46806.046875\n",
      "Train Epoch: 143 [11392/17352 (66%)] Loss: -210638.187500\n",
      "Train Epoch: 143 [12800/17352 (74%)] Loss: -218109.406250\n",
      "Train Epoch: 143 [14208/17352 (82%)] Loss: -194680.468750\n",
      "Train Epoch: 143 [15553/17352 (90%)] Loss: -99999.640625\n",
      "Train Epoch: 143 [16164/17352 (93%)] Loss: -3911.583740\n",
      "Train Epoch: 143 [17023/17352 (98%)] Loss: -50782.109375\n",
      "    epoch          : 143\n",
      "    loss           : -132942.33921619388\n",
      "    val_loss       : -78966.15948851903\n",
      "Train Epoch: 144 [128/17352 (1%)] Loss: -166688.000000\n",
      "Train Epoch: 144 [1536/17352 (9%)] Loss: -148708.859375\n",
      "Train Epoch: 144 [2944/17352 (17%)] Loss: -241789.218750\n",
      "Train Epoch: 144 [4352/17352 (25%)] Loss: 25053.140625\n",
      "Train Epoch: 144 [5760/17352 (33%)] Loss: -157439.406250\n",
      "Train Epoch: 144 [7168/17352 (41%)] Loss: -174423.000000\n",
      "Train Epoch: 144 [8576/17352 (49%)] Loss: -221775.734375\n",
      "Train Epoch: 144 [9984/17352 (58%)] Loss: -152312.171875\n",
      "Train Epoch: 144 [11392/17352 (66%)] Loss: -225868.875000\n",
      "Train Epoch: 144 [12800/17352 (74%)] Loss: -210600.734375\n",
      "Train Epoch: 144 [14208/17352 (82%)] Loss: -41252.335938\n",
      "Train Epoch: 144 [15470/17352 (89%)] Loss: -173901.812500\n",
      "Train Epoch: 144 [16156/17352 (93%)] Loss: 58790.773438\n",
      "Train Epoch: 144 [17073/17352 (98%)] Loss: -147674.890625\n",
      "    epoch          : 144\n",
      "    loss           : -134730.21471266777\n",
      "    val_loss       : -48848.46752389272\n",
      "Train Epoch: 145 [128/17352 (1%)] Loss: -203436.468750\n",
      "Train Epoch: 145 [1536/17352 (9%)] Loss: -205241.968750\n",
      "Train Epoch: 145 [2944/17352 (17%)] Loss: -98590.296875\n",
      "Train Epoch: 145 [4352/17352 (25%)] Loss: -159135.093750\n",
      "Train Epoch: 145 [5760/17352 (33%)] Loss: -224725.703125\n",
      "Train Epoch: 145 [7168/17352 (41%)] Loss: -139736.781250\n",
      "Train Epoch: 145 [8576/17352 (49%)] Loss: -157299.671875\n",
      "Train Epoch: 145 [9984/17352 (58%)] Loss: -76661.140625\n",
      "Train Epoch: 145 [11392/17352 (66%)] Loss: -71949.671875\n",
      "Train Epoch: 145 [12800/17352 (74%)] Loss: 92192.781250\n",
      "Train Epoch: 145 [14208/17352 (82%)] Loss: -90554.398438\n",
      "Train Epoch: 145 [15571/17352 (90%)] Loss: -197224.078125\n",
      "Train Epoch: 145 [16288/17352 (94%)] Loss: -151886.859375\n",
      "Train Epoch: 145 [16998/17352 (98%)] Loss: 235.178711\n",
      "    epoch          : 145\n",
      "    loss           : -141351.88199323617\n",
      "    val_loss       : -65134.522364203134\n",
      "Train Epoch: 146 [128/17352 (1%)] Loss: -201697.484375\n",
      "Train Epoch: 146 [1536/17352 (9%)] Loss: -195649.796875\n",
      "Train Epoch: 146 [2944/17352 (17%)] Loss: -92272.148438\n",
      "Train Epoch: 146 [4352/17352 (25%)] Loss: 105083.531250\n",
      "Train Epoch: 146 [5760/17352 (33%)] Loss: -162631.921875\n",
      "Train Epoch: 146 [7168/17352 (41%)] Loss: 15539.320312\n",
      "Train Epoch: 146 [8576/17352 (49%)] Loss: -214379.640625\n",
      "Train Epoch: 146 [9984/17352 (58%)] Loss: -236237.406250\n",
      "Train Epoch: 146 [11392/17352 (66%)] Loss: -203914.437500\n",
      "Train Epoch: 146 [12800/17352 (74%)] Loss: -175216.625000\n",
      "Train Epoch: 146 [14208/17352 (82%)] Loss: -234421.875000\n",
      "Train Epoch: 146 [15448/17352 (89%)] Loss: -47506.277344\n",
      "Train Epoch: 146 [16139/17352 (93%)] Loss: -93922.054688\n",
      "Train Epoch: 146 [16943/17352 (98%)] Loss: -19768.964844\n",
      "    epoch          : 146\n",
      "    loss           : -141940.72826984586\n",
      "    val_loss       : -61625.60786002477\n",
      "Train Epoch: 147 [128/17352 (1%)] Loss: 5962.099609\n",
      "Train Epoch: 147 [1536/17352 (9%)] Loss: -169431.609375\n",
      "Train Epoch: 147 [2944/17352 (17%)] Loss: -130124.718750\n",
      "Train Epoch: 147 [4352/17352 (25%)] Loss: 116283.828125\n",
      "Train Epoch: 147 [5760/17352 (33%)] Loss: -233936.312500\n",
      "Train Epoch: 147 [7168/17352 (41%)] Loss: -196831.671875\n",
      "Train Epoch: 147 [8576/17352 (49%)] Loss: -159076.531250\n",
      "Train Epoch: 147 [9984/17352 (58%)] Loss: -145991.375000\n",
      "Train Epoch: 147 [11392/17352 (66%)] Loss: -228808.656250\n",
      "Train Epoch: 147 [12800/17352 (74%)] Loss: -171316.234375\n",
      "Train Epoch: 147 [14208/17352 (82%)] Loss: 36783.875000\n",
      "Train Epoch: 147 [15453/17352 (89%)] Loss: -9410.831055\n",
      "Train Epoch: 147 [16070/17352 (93%)] Loss: -19951.630859\n",
      "Train Epoch: 147 [16890/17352 (97%)] Loss: -96267.875000\n",
      "    epoch          : 147\n",
      "    loss           : -125860.8155247221\n",
      "    val_loss       : -63625.78015940984\n",
      "Train Epoch: 148 [128/17352 (1%)] Loss: -204202.421875\n",
      "Train Epoch: 148 [1536/17352 (9%)] Loss: -161108.671875\n",
      "Train Epoch: 148 [2944/17352 (17%)] Loss: -175967.812500\n",
      "Train Epoch: 148 [4352/17352 (25%)] Loss: -173898.031250\n",
      "Train Epoch: 148 [5760/17352 (33%)] Loss: -222700.406250\n",
      "Train Epoch: 148 [7168/17352 (41%)] Loss: 25180.289062\n",
      "Train Epoch: 148 [8576/17352 (49%)] Loss: -180621.031250\n",
      "Train Epoch: 148 [9984/17352 (58%)] Loss: 8203.347656\n",
      "Train Epoch: 148 [11392/17352 (66%)] Loss: -194926.703125\n",
      "Train Epoch: 148 [12800/17352 (74%)] Loss: -88776.406250\n",
      "Train Epoch: 148 [14208/17352 (82%)] Loss: -240416.093750\n",
      "Train Epoch: 148 [15451/17352 (89%)] Loss: 64302.609375\n",
      "Train Epoch: 148 [16202/17352 (93%)] Loss: -94106.632812\n",
      "Train Epoch: 148 [17031/17352 (98%)] Loss: -28321.097656\n",
      "    epoch          : 148\n",
      "    loss           : -133094.1013417084\n",
      "    val_loss       : -55012.646987342836\n",
      "Train Epoch: 149 [128/17352 (1%)] Loss: -149370.468750\n",
      "Train Epoch: 149 [1536/17352 (9%)] Loss: -197506.984375\n",
      "Train Epoch: 149 [2944/17352 (17%)] Loss: -239573.015625\n",
      "Train Epoch: 149 [4352/17352 (25%)] Loss: -207807.609375\n",
      "Train Epoch: 149 [5760/17352 (33%)] Loss: -172160.484375\n",
      "Train Epoch: 149 [7168/17352 (41%)] Loss: 1680.824219\n",
      "Train Epoch: 149 [8576/17352 (49%)] Loss: -206054.843750\n",
      "Train Epoch: 149 [9984/17352 (58%)] Loss: -222918.312500\n",
      "Train Epoch: 149 [11392/17352 (66%)] Loss: -135997.562500\n",
      "Train Epoch: 149 [12800/17352 (74%)] Loss: -228311.796875\n",
      "Train Epoch: 149 [14208/17352 (82%)] Loss: -172475.250000\n",
      "Train Epoch: 149 [15469/17352 (89%)] Loss: 6483.570312\n",
      "Train Epoch: 149 [16247/17352 (94%)] Loss: -79886.289062\n",
      "Train Epoch: 149 [16937/17352 (98%)] Loss: -120814.531250\n",
      "    epoch          : 149\n",
      "    loss           : -126001.1716914849\n",
      "    val_loss       : -72045.6380613327\n",
      "Train Epoch: 150 [128/17352 (1%)] Loss: -142916.171875\n",
      "Train Epoch: 150 [1536/17352 (9%)] Loss: -221981.109375\n",
      "Train Epoch: 150 [2944/17352 (17%)] Loss: -1699.836914\n",
      "Train Epoch: 150 [4352/17352 (25%)] Loss: 238290.296875\n",
      "Train Epoch: 150 [5760/17352 (33%)] Loss: -159742.781250\n",
      "Train Epoch: 150 [7168/17352 (41%)] Loss: -135665.765625\n",
      "Train Epoch: 150 [8576/17352 (49%)] Loss: -225114.687500\n",
      "Train Epoch: 150 [9984/17352 (58%)] Loss: -131052.539062\n",
      "Train Epoch: 150 [11392/17352 (66%)] Loss: -205611.906250\n",
      "Train Epoch: 150 [12800/17352 (74%)] Loss: -162308.312500\n",
      "Train Epoch: 150 [14208/17352 (82%)] Loss: -141353.218750\n",
      "Train Epoch: 150 [15552/17352 (90%)] Loss: -142915.968750\n",
      "Train Epoch: 150 [16238/17352 (94%)] Loss: -18161.013672\n",
      "Train Epoch: 150 [16979/17352 (98%)] Loss: -38390.460938\n",
      "    epoch          : 150\n",
      "    loss           : -115815.18462848023\n",
      "    val_loss       : -67604.95157016118\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0913_161229/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [128/17352 (1%)] Loss: -35179.984375\n",
      "Train Epoch: 151 [1536/17352 (9%)] Loss: 120359.250000\n",
      "Train Epoch: 151 [2944/17352 (17%)] Loss: -222520.125000\n",
      "Train Epoch: 151 [4352/17352 (25%)] Loss: -233409.984375\n",
      "Train Epoch: 151 [5760/17352 (33%)] Loss: -163915.375000\n",
      "Train Epoch: 151 [7168/17352 (41%)] Loss: -215385.500000\n",
      "Train Epoch: 151 [8576/17352 (49%)] Loss: 246125.531250\n",
      "Train Epoch: 151 [9984/17352 (58%)] Loss: -218601.843750\n",
      "Train Epoch: 151 [11392/17352 (66%)] Loss: -220916.937500\n",
      "Train Epoch: 151 [12800/17352 (74%)] Loss: -204237.437500\n",
      "Train Epoch: 151 [14208/17352 (82%)] Loss: -179940.921875\n",
      "Train Epoch: 151 [15500/17352 (89%)] Loss: -96694.195312\n",
      "Train Epoch: 151 [16266/17352 (94%)] Loss: -138425.328125\n",
      "Train Epoch: 151 [16923/17352 (98%)] Loss: -168439.500000\n",
      "    epoch          : 151\n",
      "    loss           : -124388.90564374476\n",
      "    val_loss       : -74796.66523501078\n",
      "Train Epoch: 152 [128/17352 (1%)] Loss: -102866.359375\n",
      "Train Epoch: 152 [1536/17352 (9%)] Loss: -109028.140625\n",
      "Train Epoch: 152 [2944/17352 (17%)] Loss: -155404.046875\n",
      "Train Epoch: 152 [4352/17352 (25%)] Loss: -163742.953125\n",
      "Train Epoch: 152 [5760/17352 (33%)] Loss: -167636.375000\n",
      "Train Epoch: 152 [7168/17352 (41%)] Loss: -223472.171875\n",
      "Train Epoch: 152 [8576/17352 (49%)] Loss: -238076.656250\n",
      "Train Epoch: 152 [9984/17352 (58%)] Loss: -220745.109375\n",
      "Train Epoch: 152 [11392/17352 (66%)] Loss: -231578.500000\n",
      "Train Epoch: 152 [12800/17352 (74%)] Loss: -9133.187500\n",
      "Train Epoch: 152 [14208/17352 (82%)] Loss: -23917.048828\n",
      "Train Epoch: 152 [15453/17352 (89%)] Loss: -39313.468750\n",
      "Train Epoch: 152 [16319/17352 (94%)] Loss: -4575.054688\n",
      "Train Epoch: 152 [16955/17352 (98%)] Loss: -14124.425781\n",
      "    epoch          : 152\n",
      "    loss           : -123479.22123728502\n",
      "    val_loss       : -64884.460871394476\n",
      "Train Epoch: 153 [128/17352 (1%)] Loss: -154107.875000\n",
      "Train Epoch: 153 [1536/17352 (9%)] Loss: -34866.203125\n",
      "Train Epoch: 153 [2944/17352 (17%)] Loss: -213622.281250\n",
      "Train Epoch: 153 [4352/17352 (25%)] Loss: -164727.296875\n",
      "Train Epoch: 153 [5760/17352 (33%)] Loss: -185972.750000\n",
      "Train Epoch: 153 [7168/17352 (41%)] Loss: -89004.328125\n",
      "Train Epoch: 153 [8576/17352 (49%)] Loss: -227357.500000\n",
      "Train Epoch: 153 [9984/17352 (58%)] Loss: -234788.765625\n",
      "Train Epoch: 153 [11392/17352 (66%)] Loss: -221016.437500\n",
      "Train Epoch: 153 [12800/17352 (74%)] Loss: -155299.781250\n",
      "Train Epoch: 153 [14208/17352 (82%)] Loss: -164572.796875\n",
      "Train Epoch: 153 [15486/17352 (89%)] Loss: -131528.875000\n",
      "Train Epoch: 153 [16403/17352 (95%)] Loss: -10510.808594\n",
      "Train Epoch: 153 [17110/17352 (99%)] Loss: -47066.460938\n",
      "    epoch          : 153\n",
      "    loss           : -122857.70485200817\n",
      "    val_loss       : -71333.55651067098\n",
      "Train Epoch: 154 [128/17352 (1%)] Loss: -226071.843750\n",
      "Train Epoch: 154 [1536/17352 (9%)] Loss: -165162.109375\n",
      "Train Epoch: 154 [2944/17352 (17%)] Loss: -183704.218750\n",
      "Train Epoch: 154 [4352/17352 (25%)] Loss: -163029.781250\n",
      "Train Epoch: 154 [5760/17352 (33%)] Loss: -49468.136719\n",
      "Train Epoch: 154 [7168/17352 (41%)] Loss: -65540.171875\n",
      "Train Epoch: 154 [8576/17352 (49%)] Loss: -189524.984375\n",
      "Train Epoch: 154 [9984/17352 (58%)] Loss: -209039.125000\n",
      "Train Epoch: 154 [11392/17352 (66%)] Loss: -222077.281250\n",
      "Train Epoch: 154 [12800/17352 (74%)] Loss: 76623.023438\n",
      "Train Epoch: 154 [14208/17352 (82%)] Loss: -229416.625000\n",
      "Train Epoch: 154 [15419/17352 (89%)] Loss: -9437.095703\n",
      "Train Epoch: 154 [16211/17352 (93%)] Loss: -26882.904297\n",
      "Train Epoch: 154 [17039/17352 (98%)] Loss: -150533.437500\n",
      "    epoch          : 154\n",
      "    loss           : -134713.24228581166\n",
      "    val_loss       : -87035.25910097758\n",
      "Train Epoch: 155 [128/17352 (1%)] Loss: -160181.656250\n",
      "Train Epoch: 155 [1536/17352 (9%)] Loss: -232398.437500\n",
      "Train Epoch: 155 [2944/17352 (17%)] Loss: -234184.156250\n",
      "Train Epoch: 155 [4352/17352 (25%)] Loss: -207391.421875\n",
      "Train Epoch: 155 [5760/17352 (33%)] Loss: -153527.640625\n",
      "Train Epoch: 155 [7168/17352 (41%)] Loss: -92859.984375\n",
      "Train Epoch: 155 [8576/17352 (49%)] Loss: -210756.281250\n",
      "Train Epoch: 155 [9984/17352 (58%)] Loss: -227001.734375\n",
      "Train Epoch: 155 [11392/17352 (66%)] Loss: -222326.875000\n",
      "Train Epoch: 155 [12800/17352 (74%)] Loss: -71140.765625\n",
      "Train Epoch: 155 [14208/17352 (82%)] Loss: -144723.343750\n",
      "Train Epoch: 155 [15592/17352 (90%)] Loss: -206296.437500\n",
      "Train Epoch: 155 [16336/17352 (94%)] Loss: -6734.925781\n",
      "Train Epoch: 155 [17170/17352 (99%)] Loss: -55200.597656\n",
      "    epoch          : 155\n",
      "    loss           : -135675.93026917733\n",
      "    val_loss       : -76452.79576953252\n",
      "Train Epoch: 156 [128/17352 (1%)] Loss: -174252.406250\n",
      "Train Epoch: 156 [1536/17352 (9%)] Loss: -234988.687500\n",
      "Train Epoch: 156 [2944/17352 (17%)] Loss: -254707.281250\n",
      "Train Epoch: 156 [4352/17352 (25%)] Loss: -224696.093750\n",
      "Train Epoch: 156 [5760/17352 (33%)] Loss: -228454.328125\n",
      "Train Epoch: 156 [7168/17352 (41%)] Loss: -226259.937500\n",
      "Train Epoch: 156 [8576/17352 (49%)] Loss: -218643.671875\n",
      "Train Epoch: 156 [9984/17352 (58%)] Loss: -222086.250000\n",
      "Train Epoch: 156 [11392/17352 (66%)] Loss: -36118.750000\n",
      "Train Epoch: 156 [12800/17352 (74%)] Loss: -64804.949219\n",
      "Train Epoch: 156 [14208/17352 (82%)] Loss: -217480.984375\n",
      "Train Epoch: 156 [15522/17352 (89%)] Loss: -75156.250000\n",
      "Train Epoch: 156 [16401/17352 (95%)] Loss: -169220.484375\n",
      "Train Epoch: 156 [17134/17352 (99%)] Loss: -39734.203125\n",
      "    epoch          : 156\n",
      "    loss           : -137397.02592478503\n",
      "    val_loss       : -69944.70021247864\n",
      "Train Epoch: 157 [128/17352 (1%)] Loss: -39435.578125\n",
      "Train Epoch: 157 [1536/17352 (9%)] Loss: -37189.632812\n",
      "Train Epoch: 157 [2944/17352 (17%)] Loss: -148521.953125\n",
      "Train Epoch: 157 [4352/17352 (25%)] Loss: -82952.078125\n",
      "Train Epoch: 157 [5760/17352 (33%)] Loss: -207719.687500\n",
      "Train Epoch: 157 [7168/17352 (41%)] Loss: -69776.796875\n",
      "Train Epoch: 157 [8576/17352 (49%)] Loss: -238418.125000\n",
      "Train Epoch: 157 [9984/17352 (58%)] Loss: -216565.781250\n",
      "Train Epoch: 157 [11392/17352 (66%)] Loss: -146240.125000\n",
      "Train Epoch: 157 [12800/17352 (74%)] Loss: -244694.843750\n",
      "Train Epoch: 157 [14208/17352 (82%)] Loss: 127410.750000\n",
      "Train Epoch: 157 [15563/17352 (90%)] Loss: -118378.937500\n",
      "Train Epoch: 157 [16293/17352 (94%)] Loss: -69920.914062\n",
      "Train Epoch: 157 [17066/17352 (98%)] Loss: -114613.406250\n",
      "    epoch          : 157\n",
      "    loss           : -135320.94258598992\n",
      "    val_loss       : -69346.14304564794\n",
      "Train Epoch: 158 [128/17352 (1%)] Loss: -5074.273438\n",
      "Train Epoch: 158 [1536/17352 (9%)] Loss: -85229.578125\n",
      "Train Epoch: 158 [2944/17352 (17%)] Loss: -156597.062500\n",
      "Train Epoch: 158 [4352/17352 (25%)] Loss: -229680.687500\n",
      "Train Epoch: 158 [5760/17352 (33%)] Loss: -209752.015625\n",
      "Train Epoch: 158 [7168/17352 (41%)] Loss: -225066.750000\n",
      "Train Epoch: 158 [8576/17352 (49%)] Loss: -173606.078125\n",
      "Train Epoch: 158 [9984/17352 (58%)] Loss: -229321.484375\n",
      "Train Epoch: 158 [11392/17352 (66%)] Loss: -233065.875000\n",
      "Train Epoch: 158 [12800/17352 (74%)] Loss: -106149.218750\n",
      "Train Epoch: 158 [14208/17352 (82%)] Loss: -193218.687500\n",
      "Train Epoch: 158 [15510/17352 (89%)] Loss: 15744.197266\n",
      "Train Epoch: 158 [16290/17352 (94%)] Loss: -136409.906250\n",
      "Train Epoch: 158 [17064/17352 (98%)] Loss: 4188.426270\n",
      "    epoch          : 158\n",
      "    loss           : -131303.13068896811\n",
      "    val_loss       : -69281.94138120016\n",
      "Train Epoch: 159 [128/17352 (1%)] Loss: -228345.296875\n",
      "Train Epoch: 159 [1536/17352 (9%)] Loss: -191184.875000\n",
      "Train Epoch: 159 [2944/17352 (17%)] Loss: -59138.785156\n",
      "Train Epoch: 159 [4352/17352 (25%)] Loss: -201133.546875\n",
      "Train Epoch: 159 [5760/17352 (33%)] Loss: -124165.640625\n",
      "Train Epoch: 159 [7168/17352 (41%)] Loss: -203917.531250\n",
      "Train Epoch: 159 [8576/17352 (49%)] Loss: -232979.750000\n",
      "Train Epoch: 159 [9984/17352 (58%)] Loss: -82946.812500\n",
      "Train Epoch: 159 [11392/17352 (66%)] Loss: -195642.218750\n",
      "Train Epoch: 159 [12800/17352 (74%)] Loss: 105651.335938\n",
      "Train Epoch: 159 [14208/17352 (82%)] Loss: -202660.046875\n",
      "Train Epoch: 159 [15453/17352 (89%)] Loss: -86530.554688\n",
      "Train Epoch: 159 [16327/17352 (94%)] Loss: -120993.039062\n",
      "Train Epoch: 159 [17016/17352 (98%)] Loss: -148458.031250\n",
      "    epoch          : 159\n",
      "    loss           : -132919.21123079644\n",
      "    val_loss       : -68097.2091178894\n",
      "Train Epoch: 160 [128/17352 (1%)] Loss: -196251.859375\n",
      "Train Epoch: 160 [1536/17352 (9%)] Loss: -149261.187500\n",
      "Train Epoch: 160 [2944/17352 (17%)] Loss: -90591.890625\n",
      "Train Epoch: 160 [4352/17352 (25%)] Loss: -226151.359375\n",
      "Train Epoch: 160 [5760/17352 (33%)] Loss: -224041.843750\n",
      "Train Epoch: 160 [7168/17352 (41%)] Loss: -235210.937500\n",
      "Train Epoch: 160 [8576/17352 (49%)] Loss: -178343.234375\n",
      "Train Epoch: 160 [9984/17352 (58%)] Loss: -233804.750000\n",
      "Train Epoch: 160 [11392/17352 (66%)] Loss: -239668.359375\n",
      "Train Epoch: 160 [12800/17352 (74%)] Loss: -152399.203125\n",
      "Train Epoch: 160 [14208/17352 (82%)] Loss: 129919.703125\n",
      "Train Epoch: 160 [15524/17352 (89%)] Loss: -155639.421875\n",
      "Train Epoch: 160 [16433/17352 (95%)] Loss: -68730.859375\n",
      "Train Epoch: 160 [17194/17352 (99%)] Loss: -102512.828125\n",
      "    epoch          : 160\n",
      "    loss           : -135178.26174496644\n",
      "    val_loss       : -78071.8506037712\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0913_161229/checkpoint-epoch160.pth ...\n",
      "Train Epoch: 161 [128/17352 (1%)] Loss: -213464.890625\n",
      "Train Epoch: 161 [1536/17352 (9%)] Loss: -168519.046875\n",
      "Train Epoch: 161 [2944/17352 (17%)] Loss: 178489.468750\n",
      "Train Epoch: 161 [4352/17352 (25%)] Loss: -223859.500000\n",
      "Train Epoch: 161 [5760/17352 (33%)] Loss: -206371.484375\n",
      "Train Epoch: 161 [7168/17352 (41%)] Loss: -27477.792969\n",
      "Train Epoch: 161 [8576/17352 (49%)] Loss: -155784.140625\n",
      "Train Epoch: 161 [9984/17352 (58%)] Loss: 8788.437500\n",
      "Train Epoch: 161 [11392/17352 (66%)] Loss: -216547.515625\n",
      "Train Epoch: 161 [12800/17352 (74%)] Loss: -71551.007812\n",
      "Train Epoch: 161 [14208/17352 (82%)] Loss: -239849.140625\n",
      "Train Epoch: 161 [15529/17352 (89%)] Loss: -92932.906250\n",
      "Train Epoch: 161 [16283/17352 (94%)] Loss: -47946.757812\n",
      "Train Epoch: 161 [16940/17352 (98%)] Loss: -82696.140625\n",
      "    epoch          : 161\n",
      "    loss           : -132633.51087490824\n",
      "    val_loss       : -67309.89800949096\n",
      "Train Epoch: 162 [128/17352 (1%)] Loss: -236496.140625\n",
      "Train Epoch: 162 [1536/17352 (9%)] Loss: 62881.378906\n",
      "Train Epoch: 162 [2944/17352 (17%)] Loss: -61679.812500\n",
      "Train Epoch: 162 [4352/17352 (25%)] Loss: -167884.625000\n",
      "Train Epoch: 162 [5760/17352 (33%)] Loss: -25292.234375\n",
      "Train Epoch: 162 [7168/17352 (41%)] Loss: -43895.769531\n",
      "Train Epoch: 162 [8576/17352 (49%)] Loss: 90360.140625\n",
      "Train Epoch: 162 [9984/17352 (58%)] Loss: -73098.132812\n",
      "Train Epoch: 162 [11392/17352 (66%)] Loss: -232655.500000\n",
      "Train Epoch: 162 [12800/17352 (74%)] Loss: -232124.015625\n",
      "Train Epoch: 162 [14208/17352 (82%)] Loss: 76624.828125\n",
      "Train Epoch: 162 [15495/17352 (89%)] Loss: -67510.562500\n",
      "Train Epoch: 162 [16276/17352 (94%)] Loss: -17918.742188\n",
      "Train Epoch: 162 [17023/17352 (98%)] Loss: -36218.687500\n",
      "    epoch          : 162\n",
      "    loss           : -122860.44104741243\n",
      "    val_loss       : -82560.05324199995\n",
      "Train Epoch: 163 [128/17352 (1%)] Loss: -209919.015625\n",
      "Train Epoch: 163 [1536/17352 (9%)] Loss: -157739.734375\n",
      "Train Epoch: 163 [2944/17352 (17%)] Loss: -236543.968750\n",
      "Train Epoch: 163 [4352/17352 (25%)] Loss: -216972.796875\n",
      "Train Epoch: 163 [5760/17352 (33%)] Loss: -165206.843750\n",
      "Train Epoch: 163 [7168/17352 (41%)] Loss: -225341.500000\n",
      "Train Epoch: 163 [8576/17352 (49%)] Loss: -225906.171875\n",
      "Train Epoch: 163 [9984/17352 (58%)] Loss: -198730.843750\n",
      "Train Epoch: 163 [11392/17352 (66%)] Loss: -141643.812500\n",
      "Train Epoch: 163 [12800/17352 (74%)] Loss: -226900.453125\n",
      "Train Epoch: 163 [14208/17352 (82%)] Loss: -224747.468750\n",
      "Train Epoch: 163 [15556/17352 (90%)] Loss: -121697.671875\n",
      "Train Epoch: 163 [16373/17352 (94%)] Loss: -206389.156250\n",
      "Train Epoch: 163 [16867/17352 (97%)] Loss: -13827.935547\n",
      "    epoch          : 163\n",
      "    loss           : -132718.3414069054\n",
      "    val_loss       : -81624.63012178738\n",
      "Train Epoch: 164 [128/17352 (1%)] Loss: -182820.406250\n",
      "Train Epoch: 164 [1536/17352 (9%)] Loss: -106554.289062\n",
      "Train Epoch: 164 [2944/17352 (17%)] Loss: -132967.390625\n",
      "Train Epoch: 164 [4352/17352 (25%)] Loss: -213151.328125\n",
      "Train Epoch: 164 [5760/17352 (33%)] Loss: 2105.124512\n",
      "Train Epoch: 164 [7168/17352 (41%)] Loss: -231872.750000\n",
      "Train Epoch: 164 [8576/17352 (49%)] Loss: -193872.218750\n",
      "Train Epoch: 164 [9984/17352 (58%)] Loss: -213481.343750\n",
      "Train Epoch: 164 [11392/17352 (66%)] Loss: -159122.812500\n",
      "Train Epoch: 164 [12800/17352 (74%)] Loss: -223473.437500\n",
      "Train Epoch: 164 [14208/17352 (82%)] Loss: -182283.812500\n",
      "Train Epoch: 164 [15509/17352 (89%)] Loss: -195131.218750\n",
      "Train Epoch: 164 [16245/17352 (94%)] Loss: -1580.609375\n",
      "Train Epoch: 164 [17159/17352 (99%)] Loss: -62510.488281\n",
      "    epoch          : 164\n",
      "    loss           : -124235.12720054269\n",
      "    val_loss       : -56274.23033123017\n",
      "Train Epoch: 165 [128/17352 (1%)] Loss: -61359.621094\n",
      "Train Epoch: 165 [1536/17352 (9%)] Loss: -68101.773438\n",
      "Train Epoch: 165 [2944/17352 (17%)] Loss: -226444.062500\n",
      "Train Epoch: 165 [4352/17352 (25%)] Loss: -56120.531250\n",
      "Train Epoch: 165 [5760/17352 (33%)] Loss: -145883.468750\n",
      "Train Epoch: 165 [7168/17352 (41%)] Loss: 12033.448242\n",
      "Train Epoch: 165 [8576/17352 (49%)] Loss: -162450.062500\n",
      "Train Epoch: 165 [9984/17352 (58%)] Loss: -219973.921875\n",
      "Train Epoch: 165 [11392/17352 (66%)] Loss: -219298.687500\n",
      "Train Epoch: 165 [12800/17352 (74%)] Loss: -141546.015625\n",
      "Train Epoch: 165 [14208/17352 (82%)] Loss: -190282.937500\n",
      "Train Epoch: 165 [15530/17352 (89%)] Loss: -155986.750000\n",
      "Train Epoch: 165 [16260/17352 (94%)] Loss: -27184.763672\n",
      "Train Epoch: 165 [17139/17352 (99%)] Loss: -142804.078125\n",
      "    epoch          : 165\n",
      "    loss           : -136498.6730039456\n",
      "    val_loss       : -62129.95364058812\n",
      "Train Epoch: 166 [128/17352 (1%)] Loss: -235578.968750\n",
      "Train Epoch: 166 [1536/17352 (9%)] Loss: 1686.855469\n",
      "Train Epoch: 166 [2944/17352 (17%)] Loss: 43217.257812\n",
      "Train Epoch: 166 [4352/17352 (25%)] Loss: -8263.226562\n",
      "Train Epoch: 166 [5760/17352 (33%)] Loss: -907.289062\n",
      "Train Epoch: 166 [7168/17352 (41%)] Loss: -186097.812500\n",
      "Train Epoch: 166 [8576/17352 (49%)] Loss: -209843.593750\n",
      "Train Epoch: 166 [9984/17352 (58%)] Loss: -173726.218750\n",
      "Train Epoch: 166 [11392/17352 (66%)] Loss: -215210.875000\n",
      "Train Epoch: 166 [12800/17352 (74%)] Loss: -154201.921875\n",
      "Train Epoch: 166 [14208/17352 (82%)] Loss: -212140.031250\n",
      "Train Epoch: 166 [15461/17352 (89%)] Loss: 650.072510\n",
      "Train Epoch: 166 [16283/17352 (94%)] Loss: -127863.515625\n",
      "Train Epoch: 166 [17082/17352 (98%)] Loss: -138959.000000\n",
      "    epoch          : 166\n",
      "    loss           : -128799.4200578728\n",
      "    val_loss       : -61624.622165584566\n",
      "Train Epoch: 167 [128/17352 (1%)] Loss: -234181.593750\n",
      "Train Epoch: 167 [1536/17352 (9%)] Loss: -224879.718750\n",
      "Train Epoch: 167 [2944/17352 (17%)] Loss: -203589.656250\n",
      "Train Epoch: 167 [4352/17352 (25%)] Loss: -90347.000000\n",
      "Train Epoch: 167 [5760/17352 (33%)] Loss: -211199.250000\n",
      "Train Epoch: 167 [7168/17352 (41%)] Loss: -232240.250000\n",
      "Train Epoch: 167 [8576/17352 (49%)] Loss: -152298.640625\n",
      "Train Epoch: 167 [9984/17352 (58%)] Loss: -236648.937500\n",
      "Train Epoch: 167 [11392/17352 (66%)] Loss: -220590.093750\n",
      "Train Epoch: 167 [12800/17352 (74%)] Loss: -259107.750000\n",
      "Train Epoch: 167 [14208/17352 (82%)] Loss: -226744.718750\n",
      "Train Epoch: 167 [15499/17352 (89%)] Loss: -45413.546875\n",
      "Train Epoch: 167 [16426/17352 (95%)] Loss: -151364.250000\n",
      "Train Epoch: 167 [17208/17352 (99%)] Loss: -126994.914062\n",
      "    epoch          : 167\n",
      "    loss           : -143712.76274610686\n",
      "    val_loss       : -59905.13688344955\n",
      "Train Epoch: 168 [128/17352 (1%)] Loss: -171125.531250\n",
      "Train Epoch: 168 [1536/17352 (9%)] Loss: -212433.984375\n",
      "Train Epoch: 168 [2944/17352 (17%)] Loss: -175949.656250\n",
      "Train Epoch: 168 [4352/17352 (25%)] Loss: -229130.500000\n",
      "Train Epoch: 168 [5760/17352 (33%)] Loss: -240658.093750\n",
      "Train Epoch: 168 [7168/17352 (41%)] Loss: -154384.125000\n",
      "Train Epoch: 168 [8576/17352 (49%)] Loss: -176740.578125\n",
      "Train Epoch: 168 [9984/17352 (58%)] Loss: -27137.539062\n",
      "Train Epoch: 168 [11392/17352 (66%)] Loss: -221284.718750\n",
      "Train Epoch: 168 [12800/17352 (74%)] Loss: -164054.062500\n",
      "Train Epoch: 168 [14208/17352 (82%)] Loss: -147480.796875\n",
      "Train Epoch: 168 [15565/17352 (90%)] Loss: -150115.593750\n",
      "Train Epoch: 168 [16141/17352 (93%)] Loss: -79320.296875\n",
      "Train Epoch: 168 [16958/17352 (98%)] Loss: -60647.500000\n",
      "    epoch          : 168\n",
      "    loss           : -143847.24364497038\n",
      "    val_loss       : -80650.96681184768\n",
      "Validation performance didn't improve for 75 epochs. Training stops.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
