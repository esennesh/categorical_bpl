{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='omniglot_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 50,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [128/17352 (1%)] Loss: -9358.930664\n",
      "Train Epoch: 1 [1536/17352 (9%)] Loss: -5579.917969\n",
      "Train Epoch: 1 [2944/17352 (17%)] Loss: -19323.892578\n",
      "Train Epoch: 1 [4352/17352 (25%)] Loss: -5641.958008\n",
      "Train Epoch: 1 [5760/17352 (33%)] Loss: 9557.472656\n",
      "Train Epoch: 1 [7168/17352 (41%)] Loss: -29750.332031\n",
      "Train Epoch: 1 [8576/17352 (49%)] Loss: -21824.986328\n",
      "Train Epoch: 1 [9984/17352 (58%)] Loss: -26377.087891\n",
      "Train Epoch: 1 [11392/17352 (66%)] Loss: -25295.343750\n",
      "Train Epoch: 1 [12800/17352 (74%)] Loss: -27293.730469\n",
      "Train Epoch: 1 [14208/17352 (82%)] Loss: -22941.111328\n",
      "Train Epoch: 1 [15557/17352 (90%)] Loss: -22319.845703\n",
      "Train Epoch: 1 [16187/17352 (93%)] Loss: -18930.851562\n",
      "Train Epoch: 1 [16999/17352 (98%)] Loss: -11383.324219\n",
      "    epoch          : 1\n",
      "    loss           : -15607.127456562632\n",
      "    val_loss       : -11188.9740234375\n",
      "Train Epoch: 2 [128/17352 (1%)] Loss: -6799.135254\n",
      "Train Epoch: 2 [1536/17352 (9%)] Loss: -29300.648438\n",
      "Train Epoch: 2 [2944/17352 (17%)] Loss: -12063.446289\n",
      "Train Epoch: 2 [4352/17352 (25%)] Loss: -19103.562500\n",
      "Train Epoch: 2 [5760/17352 (33%)] Loss: -17753.929688\n",
      "Train Epoch: 2 [7168/17352 (41%)] Loss: -31459.720703\n",
      "Train Epoch: 2 [8576/17352 (49%)] Loss: -58710.492188\n",
      "Train Epoch: 2 [9984/17352 (58%)] Loss: -39416.644531\n",
      "Train Epoch: 2 [11392/17352 (66%)] Loss: -32379.376953\n",
      "Train Epoch: 2 [12800/17352 (74%)] Loss: -41758.996094\n",
      "Train Epoch: 2 [14208/17352 (82%)] Loss: -34122.480469\n",
      "Train Epoch: 2 [15449/17352 (89%)] Loss: -25588.605469\n",
      "Train Epoch: 2 [16192/17352 (93%)] Loss: -24636.886719\n",
      "Train Epoch: 2 [16920/17352 (98%)] Loss: -23929.003906\n",
      "    epoch          : 2\n",
      "    loss           : -23646.58174614618\n",
      "    val_loss       : -15471.18809407552\n",
      "Train Epoch: 3 [128/17352 (1%)] Loss: 10715.547852\n",
      "Train Epoch: 3 [1536/17352 (9%)] Loss: -42129.187500\n",
      "Train Epoch: 3 [2944/17352 (17%)] Loss: -46273.992188\n",
      "Train Epoch: 3 [4352/17352 (25%)] Loss: -28237.617188\n",
      "Train Epoch: 3 [5760/17352 (33%)] Loss: -38172.964844\n",
      "Train Epoch: 3 [7168/17352 (41%)] Loss: -63533.125000\n",
      "Train Epoch: 3 [8576/17352 (49%)] Loss: -19014.460938\n",
      "Train Epoch: 3 [9984/17352 (58%)] Loss: -37785.242188\n",
      "Train Epoch: 3 [11392/17352 (66%)] Loss: -30243.871094\n",
      "Train Epoch: 3 [12800/17352 (74%)] Loss: -34775.476562\n",
      "Train Epoch: 3 [14208/17352 (82%)] Loss: -31180.312500\n",
      "Train Epoch: 3 [15554/17352 (90%)] Loss: -24791.998047\n",
      "Train Epoch: 3 [16439/17352 (95%)] Loss: -1512.092163\n",
      "Train Epoch: 3 [17049/17352 (98%)] Loss: -8045.500977\n",
      "    epoch          : 3\n",
      "    loss           : -31463.238144432937\n",
      "    val_loss       : -18503.00001627604\n",
      "Train Epoch: 4 [128/17352 (1%)] Loss: -37572.378906\n",
      "Train Epoch: 4 [1536/17352 (9%)] Loss: -42940.867188\n",
      "Train Epoch: 4 [2944/17352 (17%)] Loss: 21510.050781\n",
      "Train Epoch: 4 [4352/17352 (25%)] Loss: -28932.490234\n",
      "Train Epoch: 4 [5760/17352 (33%)] Loss: -56440.722656\n",
      "Train Epoch: 4 [7168/17352 (41%)] Loss: -38463.078125\n",
      "Train Epoch: 4 [8576/17352 (49%)] Loss: -10230.192383\n",
      "Train Epoch: 4 [9984/17352 (58%)] Loss: -29178.587891\n",
      "Train Epoch: 4 [11392/17352 (66%)] Loss: -25481.199219\n",
      "Train Epoch: 4 [12800/17352 (74%)] Loss: -18116.714844\n",
      "Train Epoch: 4 [14208/17352 (82%)] Loss: -38358.308594\n",
      "Train Epoch: 4 [15451/17352 (89%)] Loss: -1252.165283\n",
      "Train Epoch: 4 [16300/17352 (94%)] Loss: -27651.136719\n",
      "Train Epoch: 4 [17030/17352 (98%)] Loss: -7812.909668\n",
      "    epoch          : 4\n",
      "    loss           : -31601.004038151477\n",
      "    val_loss       : -19784.235546875\n",
      "Train Epoch: 5 [128/17352 (1%)] Loss: -45952.359375\n",
      "Train Epoch: 5 [1536/17352 (9%)] Loss: -45966.226562\n",
      "Train Epoch: 5 [2944/17352 (17%)] Loss: -47382.039062\n",
      "Train Epoch: 5 [4352/17352 (25%)] Loss: -17159.242188\n",
      "Train Epoch: 5 [5760/17352 (33%)] Loss: -48230.601562\n",
      "Train Epoch: 5 [7168/17352 (41%)] Loss: -56681.515625\n",
      "Train Epoch: 5 [8576/17352 (49%)] Loss: -44080.218750\n",
      "Train Epoch: 5 [9984/17352 (58%)] Loss: -49659.324219\n",
      "Train Epoch: 5 [11392/17352 (66%)] Loss: -51515.046875\n",
      "Train Epoch: 5 [12800/17352 (74%)] Loss: -42057.253906\n",
      "Train Epoch: 5 [14208/17352 (82%)] Loss: -67494.765625\n",
      "Train Epoch: 5 [15545/17352 (90%)] Loss: -33359.292969\n",
      "Train Epoch: 5 [16222/17352 (93%)] Loss: -35214.757812\n",
      "Train Epoch: 5 [17011/17352 (98%)] Loss: -28328.107422\n",
      "    epoch          : 5\n",
      "    loss           : -38723.422377208735\n",
      "    val_loss       : -20663.929264322916\n",
      "Train Epoch: 6 [128/17352 (1%)] Loss: -48886.820312\n",
      "Train Epoch: 6 [1536/17352 (9%)] Loss: -41430.339844\n",
      "Train Epoch: 6 [2944/17352 (17%)] Loss: -24166.058594\n",
      "Train Epoch: 6 [4352/17352 (25%)] Loss: -61211.828125\n",
      "Train Epoch: 6 [5760/17352 (33%)] Loss: -47214.570312\n",
      "Train Epoch: 6 [7168/17352 (41%)] Loss: -53817.824219\n",
      "Train Epoch: 6 [8576/17352 (49%)] Loss: -43527.468750\n",
      "Train Epoch: 6 [9984/17352 (58%)] Loss: -60561.035156\n",
      "Train Epoch: 6 [11392/17352 (66%)] Loss: -43136.429688\n",
      "Train Epoch: 6 [12800/17352 (74%)] Loss: -33454.011719\n",
      "Train Epoch: 6 [14208/17352 (82%)] Loss: -70821.195312\n",
      "Train Epoch: 6 [15479/17352 (89%)] Loss: -20102.408203\n",
      "Train Epoch: 6 [16293/17352 (94%)] Loss: -21001.812500\n",
      "Train Epoch: 6 [16983/17352 (98%)] Loss: -9253.785156\n",
      "    epoch          : 6\n",
      "    loss           : -42899.57026989828\n",
      "    val_loss       : -23720.9294921875\n",
      "Train Epoch: 7 [128/17352 (1%)] Loss: -61622.910156\n",
      "Train Epoch: 7 [1536/17352 (9%)] Loss: -54118.058594\n",
      "Train Epoch: 7 [2944/17352 (17%)] Loss: -34867.371094\n",
      "Train Epoch: 7 [4352/17352 (25%)] Loss: -51748.449219\n",
      "Train Epoch: 7 [5760/17352 (33%)] Loss: -47783.640625\n",
      "Train Epoch: 7 [7168/17352 (41%)] Loss: -51754.058594\n",
      "Train Epoch: 7 [8576/17352 (49%)] Loss: -73715.109375\n",
      "Train Epoch: 7 [9984/17352 (58%)] Loss: -28430.144531\n",
      "Train Epoch: 7 [11392/17352 (66%)] Loss: -52688.632812\n",
      "Train Epoch: 7 [12800/17352 (74%)] Loss: -65563.468750\n",
      "Train Epoch: 7 [14208/17352 (82%)] Loss: -33347.203125\n",
      "Train Epoch: 7 [15459/17352 (89%)] Loss: -2165.259766\n",
      "Train Epoch: 7 [16218/17352 (93%)] Loss: -39348.000000\n",
      "Train Epoch: 7 [16966/17352 (98%)] Loss: -45567.417969\n",
      "    epoch          : 7\n",
      "    loss           : -48436.36660582267\n",
      "    val_loss       : -27213.945149739582\n",
      "Train Epoch: 8 [128/17352 (1%)] Loss: -57254.597656\n",
      "Train Epoch: 8 [1536/17352 (9%)] Loss: -57648.617188\n",
      "Train Epoch: 8 [2944/17352 (17%)] Loss: -57240.847656\n",
      "Train Epoch: 8 [4352/17352 (25%)] Loss: -37849.832031\n",
      "Train Epoch: 8 [5760/17352 (33%)] Loss: -65292.742188\n",
      "Train Epoch: 8 [7168/17352 (41%)] Loss: -43426.511719\n",
      "Train Epoch: 8 [8576/17352 (49%)] Loss: -50569.910156\n",
      "Train Epoch: 8 [9984/17352 (58%)] Loss: -46429.285156\n",
      "Train Epoch: 8 [11392/17352 (66%)] Loss: -64428.621094\n",
      "Train Epoch: 8 [12800/17352 (74%)] Loss: -69074.187500\n",
      "Train Epoch: 8 [14208/17352 (82%)] Loss: -51397.304688\n",
      "Train Epoch: 8 [15471/17352 (89%)] Loss: -20350.119141\n",
      "Train Epoch: 8 [16277/17352 (94%)] Loss: -35613.960938\n",
      "Train Epoch: 8 [17075/17352 (98%)] Loss: -40369.687500\n",
      "    epoch          : 8\n",
      "    loss           : -52662.899933475775\n",
      "    val_loss       : -31122.897680664064\n",
      "Train Epoch: 9 [128/17352 (1%)] Loss: -69555.093750\n",
      "Train Epoch: 9 [1536/17352 (9%)] Loss: -67119.531250\n",
      "Train Epoch: 9 [2944/17352 (17%)] Loss: -69176.820312\n",
      "Train Epoch: 9 [4352/17352 (25%)] Loss: -70795.929688\n",
      "Train Epoch: 9 [5760/17352 (33%)] Loss: -57132.046875\n",
      "Train Epoch: 9 [7168/17352 (41%)] Loss: -60545.546875\n",
      "Train Epoch: 9 [8576/17352 (49%)] Loss: -63675.300781\n",
      "Train Epoch: 9 [9984/17352 (58%)] Loss: -22363.466797\n",
      "Train Epoch: 9 [11392/17352 (66%)] Loss: -26131.632812\n",
      "Train Epoch: 9 [12800/17352 (74%)] Loss: -73319.468750\n",
      "Train Epoch: 9 [14208/17352 (82%)] Loss: -50355.121094\n",
      "Train Epoch: 9 [15546/17352 (90%)] Loss: -36352.820312\n",
      "Train Epoch: 9 [16306/17352 (94%)] Loss: -20937.701172\n",
      "Train Epoch: 9 [17066/17352 (98%)] Loss: -42522.710938\n",
      "    epoch          : 9\n",
      "    loss           : -57558.32169214031\n",
      "    val_loss       : -32372.61923828125\n",
      "Train Epoch: 10 [128/17352 (1%)] Loss: -63957.601562\n",
      "Train Epoch: 10 [1536/17352 (9%)] Loss: -74940.218750\n",
      "Train Epoch: 10 [2944/17352 (17%)] Loss: -75624.953125\n",
      "Train Epoch: 10 [4352/17352 (25%)] Loss: -76151.492188\n",
      "Train Epoch: 10 [5760/17352 (33%)] Loss: -64433.007812\n",
      "Train Epoch: 10 [7168/17352 (41%)] Loss: -65844.171875\n",
      "Train Epoch: 10 [8576/17352 (49%)] Loss: -73295.421875\n",
      "Train Epoch: 10 [9984/17352 (58%)] Loss: -77083.429688\n",
      "Train Epoch: 10 [11392/17352 (66%)] Loss: -63398.785156\n",
      "Train Epoch: 10 [12800/17352 (74%)] Loss: -96317.156250\n",
      "Train Epoch: 10 [14208/17352 (82%)] Loss: -85118.335938\n",
      "Train Epoch: 10 [15570/17352 (90%)] Loss: -60259.996094\n",
      "Train Epoch: 10 [16452/17352 (95%)] Loss: -10988.933594\n",
      "Train Epoch: 10 [17122/17352 (99%)] Loss: -46718.339844\n",
      "    epoch          : 10\n",
      "    loss           : -66449.55306453833\n",
      "    val_loss       : -36304.13764648438\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch10.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 11 [128/17352 (1%)] Loss: -70373.812500\n",
      "Train Epoch: 11 [1536/17352 (9%)] Loss: -73318.273438\n",
      "Train Epoch: 11 [2944/17352 (17%)] Loss: -79453.812500\n",
      "Train Epoch: 11 [4352/17352 (25%)] Loss: -75069.664062\n",
      "Train Epoch: 11 [5760/17352 (33%)] Loss: -90248.187500\n",
      "Train Epoch: 11 [7168/17352 (41%)] Loss: -57748.074219\n",
      "Train Epoch: 11 [8576/17352 (49%)] Loss: -82714.906250\n",
      "Train Epoch: 11 [9984/17352 (58%)] Loss: -93423.648438\n",
      "Train Epoch: 11 [11392/17352 (66%)] Loss: -77865.828125\n",
      "Train Epoch: 11 [12800/17352 (74%)] Loss: -82853.203125\n",
      "Train Epoch: 11 [14208/17352 (82%)] Loss: -85547.929688\n",
      "Train Epoch: 11 [15441/17352 (89%)] Loss: -1437.665771\n",
      "Train Epoch: 11 [16300/17352 (94%)] Loss: -68833.867188\n",
      "Train Epoch: 11 [17033/17352 (98%)] Loss: -73326.734375\n",
      "    epoch          : 11\n",
      "    loss           : -72669.1567235345\n",
      "    val_loss       : -41130.8353515625\n",
      "Train Epoch: 12 [128/17352 (1%)] Loss: -95867.843750\n",
      "Train Epoch: 12 [1536/17352 (9%)] Loss: -86190.203125\n",
      "Train Epoch: 12 [2944/17352 (17%)] Loss: -73047.812500\n",
      "Train Epoch: 12 [4352/17352 (25%)] Loss: -89059.156250\n",
      "Train Epoch: 12 [5760/17352 (33%)] Loss: -95294.445312\n",
      "Train Epoch: 12 [7168/17352 (41%)] Loss: -86427.406250\n",
      "Train Epoch: 12 [8576/17352 (49%)] Loss: -65339.359375\n",
      "Train Epoch: 12 [9984/17352 (58%)] Loss: -91621.500000\n",
      "Train Epoch: 12 [11392/17352 (66%)] Loss: -82405.531250\n",
      "Train Epoch: 12 [12800/17352 (74%)] Loss: -78113.984375\n",
      "Train Epoch: 12 [14208/17352 (82%)] Loss: -63104.023438\n",
      "Train Epoch: 12 [15498/17352 (89%)] Loss: -30791.533203\n",
      "Train Epoch: 12 [16298/17352 (94%)] Loss: -19046.716797\n",
      "Train Epoch: 12 [17054/17352 (98%)] Loss: -58443.679688\n",
      "    epoch          : 12\n",
      "    loss           : -78096.94979223469\n",
      "    val_loss       : -42464.66737467448\n",
      "Train Epoch: 13 [128/17352 (1%)] Loss: -72382.859375\n",
      "Train Epoch: 13 [1536/17352 (9%)] Loss: -79489.726562\n",
      "Train Epoch: 13 [2944/17352 (17%)] Loss: -82828.429688\n",
      "Train Epoch: 13 [4352/17352 (25%)] Loss: -96245.781250\n",
      "Train Epoch: 13 [5760/17352 (33%)] Loss: -93656.796875\n",
      "Train Epoch: 13 [7168/17352 (41%)] Loss: -88337.898438\n",
      "Train Epoch: 13 [8576/17352 (49%)] Loss: -83801.929688\n",
      "Train Epoch: 13 [9984/17352 (58%)] Loss: -94697.429688\n",
      "Train Epoch: 13 [11392/17352 (66%)] Loss: -93694.562500\n",
      "Train Epoch: 13 [12800/17352 (74%)] Loss: -90888.476562\n",
      "Train Epoch: 13 [14208/17352 (82%)] Loss: -94450.828125\n",
      "Train Epoch: 13 [15521/17352 (89%)] Loss: -65923.968750\n",
      "Train Epoch: 13 [16364/17352 (94%)] Loss: -52695.656250\n",
      "Train Epoch: 13 [17005/17352 (98%)] Loss: -35843.828125\n",
      "    epoch          : 13\n",
      "    loss           : -80121.49378014891\n",
      "    val_loss       : -47181.19097493489\n",
      "Train Epoch: 14 [128/17352 (1%)] Loss: -103690.093750\n",
      "Train Epoch: 14 [1536/17352 (9%)] Loss: -116855.437500\n",
      "Train Epoch: 14 [2944/17352 (17%)] Loss: -93898.250000\n",
      "Train Epoch: 14 [4352/17352 (25%)] Loss: -95042.843750\n",
      "Train Epoch: 14 [5760/17352 (33%)] Loss: -114388.765625\n",
      "Train Epoch: 14 [7168/17352 (41%)] Loss: -93254.414062\n",
      "Train Epoch: 14 [8576/17352 (49%)] Loss: -107383.578125\n",
      "Train Epoch: 14 [9984/17352 (58%)] Loss: -102384.578125\n",
      "Train Epoch: 14 [11392/17352 (66%)] Loss: -87015.625000\n",
      "Train Epoch: 14 [12800/17352 (74%)] Loss: -97703.367188\n",
      "Train Epoch: 14 [14208/17352 (82%)] Loss: -75880.671875\n",
      "Train Epoch: 14 [15564/17352 (90%)] Loss: -110666.617188\n",
      "Train Epoch: 14 [16258/17352 (94%)] Loss: -66728.804688\n",
      "Train Epoch: 14 [16980/17352 (98%)] Loss: -73521.515625\n",
      "    epoch          : 14\n",
      "    loss           : -89327.16525534815\n",
      "    val_loss       : -50058.531404622394\n",
      "Train Epoch: 15 [128/17352 (1%)] Loss: -99009.484375\n",
      "Train Epoch: 15 [1536/17352 (9%)] Loss: -107225.531250\n",
      "Train Epoch: 15 [2944/17352 (17%)] Loss: -117988.679688\n",
      "Train Epoch: 15 [4352/17352 (25%)] Loss: -86648.015625\n",
      "Train Epoch: 15 [5760/17352 (33%)] Loss: -107577.570312\n",
      "Train Epoch: 15 [7168/17352 (41%)] Loss: -82166.421875\n",
      "Train Epoch: 15 [8576/17352 (49%)] Loss: -98169.851562\n",
      "Train Epoch: 15 [9984/17352 (58%)] Loss: -121219.820312\n",
      "Train Epoch: 15 [11392/17352 (66%)] Loss: -117674.640625\n",
      "Train Epoch: 15 [12800/17352 (74%)] Loss: -104652.429688\n",
      "Train Epoch: 15 [14208/17352 (82%)] Loss: -79678.039062\n",
      "Train Epoch: 15 [15541/17352 (90%)] Loss: -79448.265625\n",
      "Train Epoch: 15 [16239/17352 (94%)] Loss: -58790.843750\n",
      "Train Epoch: 15 [17049/17352 (98%)] Loss: -67024.257812\n",
      "    epoch          : 15\n",
      "    loss           : -97061.97809288486\n",
      "    val_loss       : -53912.745450846356\n",
      "Train Epoch: 16 [128/17352 (1%)] Loss: -90304.757812\n",
      "Train Epoch: 16 [1536/17352 (9%)] Loss: -118198.250000\n",
      "Train Epoch: 16 [2944/17352 (17%)] Loss: -98816.109375\n",
      "Train Epoch: 16 [4352/17352 (25%)] Loss: -107760.359375\n",
      "Train Epoch: 16 [5760/17352 (33%)] Loss: -127466.125000\n",
      "Train Epoch: 16 [7168/17352 (41%)] Loss: -118001.718750\n",
      "Train Epoch: 16 [8576/17352 (49%)] Loss: -111861.078125\n",
      "Train Epoch: 16 [9984/17352 (58%)] Loss: -106520.921875\n",
      "Train Epoch: 16 [11392/17352 (66%)] Loss: -102710.500000\n",
      "Train Epoch: 16 [12800/17352 (74%)] Loss: -113271.578125\n",
      "Train Epoch: 16 [14208/17352 (82%)] Loss: -99113.046875\n",
      "Train Epoch: 16 [15562/17352 (90%)] Loss: -113482.218750\n",
      "Train Epoch: 16 [16336/17352 (94%)] Loss: -84121.070312\n",
      "Train Epoch: 16 [17032/17352 (98%)] Loss: -95538.289062\n",
      "    epoch          : 16\n",
      "    loss           : -97022.02323432257\n",
      "    val_loss       : -55950.91123860677\n",
      "Train Epoch: 17 [128/17352 (1%)] Loss: -115585.578125\n",
      "Train Epoch: 17 [1536/17352 (9%)] Loss: -125521.929688\n",
      "Train Epoch: 17 [2944/17352 (17%)] Loss: -113581.351562\n",
      "Train Epoch: 17 [4352/17352 (25%)] Loss: -112361.789062\n",
      "Train Epoch: 17 [5760/17352 (33%)] Loss: -121317.953125\n",
      "Train Epoch: 17 [7168/17352 (41%)] Loss: -105297.328125\n",
      "Train Epoch: 17 [8576/17352 (49%)] Loss: -119091.796875\n",
      "Train Epoch: 17 [9984/17352 (58%)] Loss: -120233.609375\n",
      "Train Epoch: 17 [11392/17352 (66%)] Loss: -103806.984375\n",
      "Train Epoch: 17 [12800/17352 (74%)] Loss: -100468.000000\n",
      "Train Epoch: 17 [14208/17352 (82%)] Loss: -132157.687500\n",
      "Train Epoch: 17 [15399/17352 (89%)] Loss: -33078.605469\n",
      "Train Epoch: 17 [16108/17352 (93%)] Loss: -10937.356445\n",
      "Train Epoch: 17 [16922/17352 (98%)] Loss: -70744.757812\n",
      "    epoch          : 17\n",
      "    loss           : -103868.68747869914\n",
      "    val_loss       : -57557.64845377604\n",
      "Train Epoch: 18 [128/17352 (1%)] Loss: -123687.296875\n",
      "Train Epoch: 18 [1536/17352 (9%)] Loss: -122086.984375\n",
      "Train Epoch: 18 [2944/17352 (17%)] Loss: -110903.695312\n",
      "Train Epoch: 18 [4352/17352 (25%)] Loss: -98360.320312\n",
      "Train Epoch: 18 [5760/17352 (33%)] Loss: -114859.234375\n",
      "Train Epoch: 18 [7168/17352 (41%)] Loss: -108286.781250\n",
      "Train Epoch: 18 [8576/17352 (49%)] Loss: -115882.546875\n",
      "Train Epoch: 18 [9984/17352 (58%)] Loss: -106886.390625\n",
      "Train Epoch: 18 [11392/17352 (66%)] Loss: -110053.468750\n",
      "Train Epoch: 18 [12800/17352 (74%)] Loss: -111991.617188\n",
      "Train Epoch: 18 [14208/17352 (82%)] Loss: -118645.750000\n",
      "Train Epoch: 18 [15484/17352 (89%)] Loss: -18408.083984\n",
      "Train Epoch: 18 [16411/17352 (95%)] Loss: -42587.367188\n",
      "Train Epoch: 18 [16973/17352 (98%)] Loss: -29674.255859\n",
      "    epoch          : 18\n",
      "    loss           : -101606.7431640625\n",
      "    val_loss       : -56603.58614095052\n",
      "Train Epoch: 19 [128/17352 (1%)] Loss: -88657.046875\n",
      "Train Epoch: 19 [1536/17352 (9%)] Loss: -125900.734375\n",
      "Train Epoch: 19 [2944/17352 (17%)] Loss: -120735.515625\n",
      "Train Epoch: 19 [4352/17352 (25%)] Loss: -100683.835938\n",
      "Train Epoch: 19 [5760/17352 (33%)] Loss: -103960.796875\n",
      "Train Epoch: 19 [7168/17352 (41%)] Loss: -110181.000000\n",
      "Train Epoch: 19 [8576/17352 (49%)] Loss: -86246.140625\n",
      "Train Epoch: 19 [9984/17352 (58%)] Loss: -134771.781250\n",
      "Train Epoch: 19 [11392/17352 (66%)] Loss: -103180.906250\n",
      "Train Epoch: 19 [12800/17352 (74%)] Loss: -107088.281250\n",
      "Train Epoch: 19 [14208/17352 (82%)] Loss: -111294.515625\n",
      "Train Epoch: 19 [15560/17352 (90%)] Loss: -121530.703125\n",
      "Train Epoch: 19 [16349/17352 (94%)] Loss: -65157.281250\n",
      "Train Epoch: 19 [17016/17352 (98%)] Loss: -10522.175781\n",
      "    epoch          : 19\n",
      "    loss           : -99788.26644262532\n",
      "    val_loss       : -54201.71277669271\n",
      "Train Epoch: 20 [128/17352 (1%)] Loss: -103821.679688\n",
      "Train Epoch: 20 [1536/17352 (9%)] Loss: -108148.843750\n",
      "Train Epoch: 20 [2944/17352 (17%)] Loss: -121859.421875\n",
      "Train Epoch: 20 [4352/17352 (25%)] Loss: -109765.125000\n",
      "Train Epoch: 20 [5760/17352 (33%)] Loss: -119493.250000\n",
      "Train Epoch: 20 [7168/17352 (41%)] Loss: -120392.375000\n",
      "Train Epoch: 20 [8576/17352 (49%)] Loss: -124065.015625\n",
      "Train Epoch: 20 [9984/17352 (58%)] Loss: -121365.578125\n",
      "Train Epoch: 20 [11392/17352 (66%)] Loss: -130647.375000\n",
      "Train Epoch: 20 [12800/17352 (74%)] Loss: -116691.218750\n",
      "Train Epoch: 20 [14208/17352 (82%)] Loss: -122172.960938\n",
      "Train Epoch: 20 [15523/17352 (89%)] Loss: -41258.085938\n",
      "Train Epoch: 20 [16165/17352 (93%)] Loss: -105285.742188\n",
      "Train Epoch: 20 [17003/17352 (98%)] Loss: -100988.976562\n",
      "    epoch          : 20\n",
      "    loss           : -104215.7594936294\n",
      "    val_loss       : -58480.44680989583\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch20.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 21 [128/17352 (1%)] Loss: -97267.171875\n",
      "Train Epoch: 21 [1536/17352 (9%)] Loss: -125473.296875\n",
      "Train Epoch: 21 [2944/17352 (17%)] Loss: -129465.578125\n",
      "Train Epoch: 21 [4352/17352 (25%)] Loss: -131508.718750\n",
      "Train Epoch: 21 [5760/17352 (33%)] Loss: -125904.328125\n",
      "Train Epoch: 21 [7168/17352 (41%)] Loss: -123682.320312\n",
      "Train Epoch: 21 [8576/17352 (49%)] Loss: -131934.437500\n",
      "Train Epoch: 21 [9984/17352 (58%)] Loss: -105440.351562\n",
      "Train Epoch: 21 [11392/17352 (66%)] Loss: -115448.015625\n",
      "Train Epoch: 21 [12800/17352 (74%)] Loss: -120987.593750\n",
      "Train Epoch: 21 [14208/17352 (82%)] Loss: -127732.625000\n",
      "Train Epoch: 21 [15478/17352 (89%)] Loss: -68411.757812\n",
      "Train Epoch: 21 [16222/17352 (93%)] Loss: -4461.021973\n",
      "Train Epoch: 21 [17029/17352 (98%)] Loss: -11004.339844\n",
      "    epoch          : 21\n",
      "    loss           : -109804.00490247483\n",
      "    val_loss       : -60880.168766276045\n",
      "Train Epoch: 22 [128/17352 (1%)] Loss: -123007.429688\n",
      "Train Epoch: 22 [1536/17352 (9%)] Loss: -128894.906250\n",
      "Train Epoch: 22 [2944/17352 (17%)] Loss: -114862.476562\n",
      "Train Epoch: 22 [4352/17352 (25%)] Loss: -126336.328125\n",
      "Train Epoch: 22 [5760/17352 (33%)] Loss: -132030.968750\n",
      "Train Epoch: 22 [7168/17352 (41%)] Loss: -122112.093750\n",
      "Train Epoch: 22 [8576/17352 (49%)] Loss: -134162.500000\n",
      "Train Epoch: 22 [9984/17352 (58%)] Loss: -136201.250000\n",
      "Train Epoch: 22 [11392/17352 (66%)] Loss: -133289.062500\n",
      "Train Epoch: 22 [12800/17352 (74%)] Loss: -133041.375000\n",
      "Train Epoch: 22 [14208/17352 (82%)] Loss: -143396.406250\n",
      "Train Epoch: 22 [15553/17352 (90%)] Loss: -122434.132812\n",
      "Train Epoch: 22 [16293/17352 (94%)] Loss: -37168.664062\n",
      "Train Epoch: 22 [16937/17352 (98%)] Loss: -56824.968750\n",
      "    epoch          : 22\n",
      "    loss           : -114661.9980648988\n",
      "    val_loss       : -64520.50205891927\n",
      "Train Epoch: 23 [128/17352 (1%)] Loss: -126813.984375\n",
      "Train Epoch: 23 [1536/17352 (9%)] Loss: -119913.507812\n",
      "Train Epoch: 23 [2944/17352 (17%)] Loss: -132815.250000\n",
      "Train Epoch: 23 [4352/17352 (25%)] Loss: -137972.875000\n",
      "Train Epoch: 23 [5760/17352 (33%)] Loss: -134617.843750\n",
      "Train Epoch: 23 [7168/17352 (41%)] Loss: -129110.656250\n",
      "Train Epoch: 23 [8576/17352 (49%)] Loss: -116464.554688\n",
      "Train Epoch: 23 [9984/17352 (58%)] Loss: -137419.000000\n",
      "Train Epoch: 23 [11392/17352 (66%)] Loss: -131010.875000\n",
      "Train Epoch: 23 [12800/17352 (74%)] Loss: -131057.671875\n",
      "Train Epoch: 23 [14208/17352 (82%)] Loss: -135339.359375\n",
      "Train Epoch: 23 [15523/17352 (89%)] Loss: -93783.984375\n",
      "Train Epoch: 23 [16190/17352 (93%)] Loss: -81838.257812\n",
      "Train Epoch: 23 [17091/17352 (98%)] Loss: -107336.703125\n",
      "    epoch          : 23\n",
      "    loss           : -118227.74002464346\n",
      "    val_loss       : -65817.85698242187\n",
      "Train Epoch: 24 [128/17352 (1%)] Loss: -129298.585938\n",
      "Train Epoch: 24 [1536/17352 (9%)] Loss: -129675.265625\n",
      "Train Epoch: 24 [2944/17352 (17%)] Loss: -131109.265625\n",
      "Train Epoch: 24 [4352/17352 (25%)] Loss: -156426.921875\n",
      "Train Epoch: 24 [5760/17352 (33%)] Loss: -134108.390625\n",
      "Train Epoch: 24 [7168/17352 (41%)] Loss: -137662.218750\n",
      "Train Epoch: 24 [8576/17352 (49%)] Loss: -150270.953125\n",
      "Train Epoch: 24 [9984/17352 (58%)] Loss: -126068.437500\n",
      "Train Epoch: 24 [11392/17352 (66%)] Loss: -142395.890625\n",
      "Train Epoch: 24 [12800/17352 (74%)] Loss: -141747.875000\n",
      "Train Epoch: 24 [14208/17352 (82%)] Loss: -139383.281250\n",
      "Train Epoch: 24 [15521/17352 (89%)] Loss: -37209.859375\n",
      "Train Epoch: 24 [16185/17352 (93%)] Loss: -99972.015625\n",
      "Train Epoch: 24 [16976/17352 (98%)] Loss: -103750.390625\n",
      "    epoch          : 24\n",
      "    loss           : -121671.38067422138\n",
      "    val_loss       : -70132.83777669272\n",
      "Train Epoch: 25 [128/17352 (1%)] Loss: -150592.859375\n",
      "Train Epoch: 25 [1536/17352 (9%)] Loss: -136399.265625\n",
      "Train Epoch: 25 [2944/17352 (17%)] Loss: -155728.859375\n",
      "Train Epoch: 25 [4352/17352 (25%)] Loss: -129336.703125\n",
      "Train Epoch: 25 [5760/17352 (33%)] Loss: -150937.812500\n",
      "Train Epoch: 25 [7168/17352 (41%)] Loss: -136230.781250\n",
      "Train Epoch: 25 [8576/17352 (49%)] Loss: -147926.156250\n",
      "Train Epoch: 25 [9984/17352 (58%)] Loss: -139344.468750\n",
      "Train Epoch: 25 [11392/17352 (66%)] Loss: -149912.796875\n",
      "Train Epoch: 25 [12800/17352 (74%)] Loss: -160738.796875\n",
      "Train Epoch: 25 [14208/17352 (82%)] Loss: -140594.343750\n",
      "Train Epoch: 25 [15453/17352 (89%)] Loss: -35826.671875\n",
      "Train Epoch: 25 [16284/17352 (94%)] Loss: -57233.117188\n",
      "Train Epoch: 25 [16987/17352 (98%)] Loss: -85311.562500\n",
      "    epoch          : 25\n",
      "    loss           : -126803.12642879614\n",
      "    val_loss       : -74102.14811197917\n",
      "Train Epoch: 26 [128/17352 (1%)] Loss: -137955.812500\n",
      "Train Epoch: 26 [1536/17352 (9%)] Loss: -129853.500000\n",
      "Train Epoch: 26 [2944/17352 (17%)] Loss: -156428.781250\n",
      "Train Epoch: 26 [4352/17352 (25%)] Loss: -161849.093750\n",
      "Train Epoch: 26 [5760/17352 (33%)] Loss: -173310.562500\n",
      "Train Epoch: 26 [7168/17352 (41%)] Loss: -162276.218750\n",
      "Train Epoch: 26 [8576/17352 (49%)] Loss: -155461.343750\n",
      "Train Epoch: 26 [9984/17352 (58%)] Loss: -147370.375000\n",
      "Train Epoch: 26 [11392/17352 (66%)] Loss: -158873.640625\n",
      "Train Epoch: 26 [12800/17352 (74%)] Loss: -159716.546875\n",
      "Train Epoch: 26 [14208/17352 (82%)] Loss: -147921.875000\n",
      "Train Epoch: 26 [15572/17352 (90%)] Loss: -143184.375000\n",
      "Train Epoch: 26 [16237/17352 (94%)] Loss: -61760.511719\n",
      "Train Epoch: 26 [16958/17352 (98%)] Loss: -50834.289062\n",
      "    epoch          : 26\n",
      "    loss           : -137064.72388842283\n",
      "    val_loss       : -71552.02325846354\n",
      "Train Epoch: 27 [128/17352 (1%)] Loss: -165880.562500\n",
      "Train Epoch: 27 [1536/17352 (9%)] Loss: -162022.312500\n",
      "Train Epoch: 27 [2944/17352 (17%)] Loss: -164583.343750\n",
      "Train Epoch: 27 [4352/17352 (25%)] Loss: -164272.484375\n",
      "Train Epoch: 27 [5760/17352 (33%)] Loss: -166034.875000\n",
      "Train Epoch: 27 [7168/17352 (41%)] Loss: -163477.515625\n",
      "Train Epoch: 27 [8576/17352 (49%)] Loss: -158011.796875\n",
      "Train Epoch: 27 [9984/17352 (58%)] Loss: -39092.457031\n",
      "Train Epoch: 27 [11392/17352 (66%)] Loss: -106182.015625\n",
      "Train Epoch: 27 [12800/17352 (74%)] Loss: -168447.671875\n",
      "Train Epoch: 27 [14208/17352 (82%)] Loss: -158663.843750\n",
      "Train Epoch: 27 [15515/17352 (89%)] Loss: -82353.867188\n",
      "Train Epoch: 27 [16230/17352 (94%)] Loss: -101755.265625\n",
      "Train Epoch: 27 [17086/17352 (98%)] Loss: -92291.585938\n",
      "    epoch          : 27\n",
      "    loss           : -138338.21454553795\n",
      "    val_loss       : -79461.73272705078\n",
      "Train Epoch: 28 [128/17352 (1%)] Loss: -167096.421875\n",
      "Train Epoch: 28 [1536/17352 (9%)] Loss: -163546.421875\n",
      "Train Epoch: 28 [2944/17352 (17%)] Loss: -168739.062500\n",
      "Train Epoch: 28 [4352/17352 (25%)] Loss: -163769.984375\n",
      "Train Epoch: 28 [5760/17352 (33%)] Loss: -92868.468750\n",
      "Train Epoch: 28 [7168/17352 (41%)] Loss: -150842.968750\n",
      "Train Epoch: 28 [8576/17352 (49%)] Loss: -78327.640625\n",
      "Train Epoch: 28 [9984/17352 (58%)] Loss: -155656.937500\n",
      "Train Epoch: 28 [11392/17352 (66%)] Loss: -155378.218750\n",
      "Train Epoch: 28 [12800/17352 (74%)] Loss: -155812.984375\n",
      "Train Epoch: 28 [14208/17352 (82%)] Loss: -164055.546875\n",
      "Train Epoch: 28 [15500/17352 (89%)] Loss: -123844.625000\n",
      "Train Epoch: 28 [16135/17352 (93%)] Loss: -3738.368896\n",
      "Train Epoch: 28 [17041/17352 (98%)] Loss: -125668.195312\n",
      "    epoch          : 28\n",
      "    loss           : -142588.2727280175\n",
      "    val_loss       : -81744.01723225911\n",
      "Train Epoch: 29 [128/17352 (1%)] Loss: -178604.281250\n",
      "Train Epoch: 29 [1536/17352 (9%)] Loss: -173552.875000\n",
      "Train Epoch: 29 [2944/17352 (17%)] Loss: -62778.730469\n",
      "Train Epoch: 29 [4352/17352 (25%)] Loss: -161602.562500\n",
      "Train Epoch: 29 [5760/17352 (33%)] Loss: -102061.937500\n",
      "Train Epoch: 29 [7168/17352 (41%)] Loss: -166250.437500\n",
      "Train Epoch: 29 [8576/17352 (49%)] Loss: -103677.859375\n",
      "Train Epoch: 29 [9984/17352 (58%)] Loss: -164818.531250\n",
      "Train Epoch: 29 [11392/17352 (66%)] Loss: -158781.796875\n",
      "Train Epoch: 29 [12800/17352 (74%)] Loss: -39072.234375\n",
      "Train Epoch: 29 [14208/17352 (82%)] Loss: -143958.968750\n",
      "Train Epoch: 29 [15521/17352 (89%)] Loss: -42976.007812\n",
      "Train Epoch: 29 [16267/17352 (94%)] Loss: -119675.265625\n",
      "Train Epoch: 29 [17085/17352 (98%)] Loss: -72289.843750\n",
      "    epoch          : 29\n",
      "    loss           : -144641.9381980128\n",
      "    val_loss       : -82074.62979736328\n",
      "Train Epoch: 30 [128/17352 (1%)] Loss: -144335.062500\n",
      "Train Epoch: 30 [1536/17352 (9%)] Loss: -154098.984375\n",
      "Train Epoch: 30 [2944/17352 (17%)] Loss: -182575.890625\n",
      "Train Epoch: 30 [4352/17352 (25%)] Loss: -146587.593750\n",
      "Train Epoch: 30 [5760/17352 (33%)] Loss: -167664.437500\n",
      "Train Epoch: 30 [7168/17352 (41%)] Loss: -150883.312500\n",
      "Train Epoch: 30 [8576/17352 (49%)] Loss: -180471.468750\n",
      "Train Epoch: 30 [9984/17352 (58%)] Loss: -168805.015625\n",
      "Train Epoch: 30 [11392/17352 (66%)] Loss: -154771.718750\n",
      "Train Epoch: 30 [12800/17352 (74%)] Loss: -163578.937500\n",
      "Train Epoch: 30 [14208/17352 (82%)] Loss: -164923.687500\n",
      "Train Epoch: 30 [15521/17352 (89%)] Loss: -161840.109375\n",
      "Train Epoch: 30 [16068/17352 (93%)] Loss: -95701.593750\n",
      "Train Epoch: 30 [16929/17352 (98%)] Loss: -97471.234375\n",
      "    epoch          : 30\n",
      "    loss           : -147854.54140887165\n",
      "    val_loss       : -81984.96840820313\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch30.pth ...\n",
      "Train Epoch: 31 [128/17352 (1%)] Loss: -175421.125000\n",
      "Train Epoch: 31 [1536/17352 (9%)] Loss: -174680.281250\n",
      "Train Epoch: 31 [2944/17352 (17%)] Loss: -153197.125000\n",
      "Train Epoch: 31 [4352/17352 (25%)] Loss: -165970.656250\n",
      "Train Epoch: 31 [5760/17352 (33%)] Loss: -139109.718750\n",
      "Train Epoch: 31 [7168/17352 (41%)] Loss: -164625.375000\n",
      "Train Epoch: 31 [8576/17352 (49%)] Loss: -172782.343750\n",
      "Train Epoch: 31 [9984/17352 (58%)] Loss: -163432.140625\n",
      "Train Epoch: 31 [11392/17352 (66%)] Loss: -158839.125000\n",
      "Train Epoch: 31 [12800/17352 (74%)] Loss: -164394.687500\n",
      "Train Epoch: 31 [14208/17352 (82%)] Loss: -179219.859375\n",
      "Train Epoch: 31 [15533/17352 (90%)] Loss: -97389.445312\n",
      "Train Epoch: 31 [16248/17352 (94%)] Loss: -111902.062500\n",
      "Train Epoch: 31 [16956/17352 (98%)] Loss: -148017.359375\n",
      "    epoch          : 31\n",
      "    loss           : -152763.8032914744\n",
      "    val_loss       : -82528.1895345052\n",
      "Train Epoch: 32 [128/17352 (1%)] Loss: -180318.781250\n",
      "Train Epoch: 32 [1536/17352 (9%)] Loss: -179855.312500\n",
      "Train Epoch: 32 [2944/17352 (17%)] Loss: -160689.484375\n",
      "Train Epoch: 32 [4352/17352 (25%)] Loss: -175155.515625\n",
      "Train Epoch: 32 [5760/17352 (33%)] Loss: -159575.296875\n",
      "Train Epoch: 32 [7168/17352 (41%)] Loss: -164337.812500\n",
      "Train Epoch: 32 [8576/17352 (49%)] Loss: -179914.062500\n",
      "Train Epoch: 32 [9984/17352 (58%)] Loss: -171821.968750\n",
      "Train Epoch: 32 [11392/17352 (66%)] Loss: -158815.578125\n",
      "Train Epoch: 32 [12800/17352 (74%)] Loss: -161837.906250\n",
      "Train Epoch: 32 [14208/17352 (82%)] Loss: -168208.515625\n",
      "Train Epoch: 32 [15546/17352 (90%)] Loss: -132142.640625\n",
      "Train Epoch: 32 [16364/17352 (94%)] Loss: -56956.257812\n",
      "Train Epoch: 32 [17036/17352 (98%)] Loss: -46868.152344\n",
      "    epoch          : 32\n",
      "    loss           : -152527.53592308096\n",
      "    val_loss       : -87833.85399576822\n",
      "Train Epoch: 33 [128/17352 (1%)] Loss: -157728.000000\n",
      "Train Epoch: 33 [1536/17352 (9%)] Loss: -155921.765625\n",
      "Train Epoch: 33 [2944/17352 (17%)] Loss: -169154.312500\n",
      "Train Epoch: 33 [4352/17352 (25%)] Loss: -187638.812500\n",
      "Train Epoch: 33 [5760/17352 (33%)] Loss: -184019.453125\n",
      "Train Epoch: 33 [7168/17352 (41%)] Loss: -176828.031250\n",
      "Train Epoch: 33 [8576/17352 (49%)] Loss: -184876.718750\n",
      "Train Epoch: 33 [9984/17352 (58%)] Loss: -169257.968750\n",
      "Train Epoch: 33 [11392/17352 (66%)] Loss: -181472.421875\n",
      "Train Epoch: 33 [12800/17352 (74%)] Loss: -181956.015625\n",
      "Train Epoch: 33 [14208/17352 (82%)] Loss: -182099.328125\n",
      "Train Epoch: 33 [15461/17352 (89%)] Loss: -136035.281250\n",
      "Train Epoch: 33 [16368/17352 (94%)] Loss: -67222.000000\n",
      "Train Epoch: 33 [16984/17352 (98%)] Loss: -147490.484375\n",
      "    epoch          : 33\n",
      "    loss           : -158813.42685448565\n",
      "    val_loss       : -90308.85494588217\n",
      "Train Epoch: 34 [128/17352 (1%)] Loss: -188512.187500\n",
      "Train Epoch: 34 [1536/17352 (9%)] Loss: -196113.859375\n",
      "Train Epoch: 34 [2944/17352 (17%)] Loss: -189496.906250\n",
      "Train Epoch: 34 [4352/17352 (25%)] Loss: -171718.812500\n",
      "Train Epoch: 34 [5760/17352 (33%)] Loss: -197208.562500\n",
      "Train Epoch: 34 [7168/17352 (41%)] Loss: -173594.968750\n",
      "Train Epoch: 34 [8576/17352 (49%)] Loss: -196094.218750\n",
      "Train Epoch: 34 [9984/17352 (58%)] Loss: -191838.937500\n",
      "Train Epoch: 34 [11392/17352 (66%)] Loss: -191935.781250\n",
      "Train Epoch: 34 [12800/17352 (74%)] Loss: -179518.593750\n",
      "Train Epoch: 34 [14208/17352 (82%)] Loss: -118531.523438\n",
      "Train Epoch: 34 [15577/17352 (90%)] Loss: -126899.945312\n",
      "Train Epoch: 34 [16456/17352 (95%)] Loss: -171475.312500\n",
      "Train Epoch: 34 [17037/17352 (98%)] Loss: -106221.734375\n",
      "    epoch          : 34\n",
      "    loss           : -165181.1938574874\n",
      "    val_loss       : -89692.30182495117\n",
      "Train Epoch: 35 [128/17352 (1%)] Loss: -183560.734375\n",
      "Train Epoch: 35 [1536/17352 (9%)] Loss: -179629.937500\n",
      "Train Epoch: 35 [2944/17352 (17%)] Loss: -181729.875000\n",
      "Train Epoch: 35 [4352/17352 (25%)] Loss: -175872.484375\n",
      "Train Epoch: 35 [5760/17352 (33%)] Loss: -199136.015625\n",
      "Train Epoch: 35 [7168/17352 (41%)] Loss: -187414.640625\n",
      "Train Epoch: 35 [8576/17352 (49%)] Loss: -195567.062500\n",
      "Train Epoch: 35 [9984/17352 (58%)] Loss: -135669.843750\n",
      "Train Epoch: 35 [11392/17352 (66%)] Loss: -201819.390625\n",
      "Train Epoch: 35 [12800/17352 (74%)] Loss: -193034.031250\n",
      "Train Epoch: 35 [14208/17352 (82%)] Loss: -191943.203125\n",
      "Train Epoch: 35 [15493/17352 (89%)] Loss: -130802.570312\n",
      "Train Epoch: 35 [16426/17352 (95%)] Loss: -185897.906250\n",
      "Train Epoch: 35 [16974/17352 (98%)] Loss: -80153.382812\n",
      "    epoch          : 35\n",
      "    loss           : -166227.4196728188\n",
      "    val_loss       : -93168.75736083984\n",
      "Train Epoch: 36 [128/17352 (1%)] Loss: -177221.312500\n",
      "Train Epoch: 36 [1536/17352 (9%)] Loss: -187097.687500\n",
      "Train Epoch: 36 [2944/17352 (17%)] Loss: -186242.375000\n",
      "Train Epoch: 36 [4352/17352 (25%)] Loss: -187648.953125\n",
      "Train Epoch: 36 [5760/17352 (33%)] Loss: -191227.468750\n",
      "Train Epoch: 36 [7168/17352 (41%)] Loss: -161128.062500\n",
      "Train Epoch: 36 [8576/17352 (49%)] Loss: -169959.906250\n",
      "Train Epoch: 36 [9984/17352 (58%)] Loss: -180514.562500\n",
      "Train Epoch: 36 [11392/17352 (66%)] Loss: -179219.218750\n",
      "Train Epoch: 36 [12800/17352 (74%)] Loss: -178505.359375\n",
      "Train Epoch: 36 [14208/17352 (82%)] Loss: -171261.531250\n",
      "Train Epoch: 36 [15520/17352 (89%)] Loss: -111900.648438\n",
      "Train Epoch: 36 [16071/17352 (93%)] Loss: -17815.105469\n",
      "Train Epoch: 36 [16947/17352 (98%)] Loss: -25447.978516\n",
      "    epoch          : 36\n",
      "    loss           : -170838.85360738254\n",
      "    val_loss       : -96408.13474934896\n",
      "Train Epoch: 37 [128/17352 (1%)] Loss: -198203.640625\n",
      "Train Epoch: 37 [1536/17352 (9%)] Loss: -188563.578125\n",
      "Train Epoch: 37 [2944/17352 (17%)] Loss: -208497.046875\n",
      "Train Epoch: 37 [4352/17352 (25%)] Loss: -178665.000000\n",
      "Train Epoch: 37 [5760/17352 (33%)] Loss: -188902.796875\n",
      "Train Epoch: 37 [7168/17352 (41%)] Loss: -194461.828125\n",
      "Train Epoch: 37 [8576/17352 (49%)] Loss: -186446.140625\n",
      "Train Epoch: 37 [9984/17352 (58%)] Loss: -190396.000000\n",
      "Train Epoch: 37 [11392/17352 (66%)] Loss: -199024.968750\n",
      "Train Epoch: 37 [12800/17352 (74%)] Loss: -206734.734375\n",
      "Train Epoch: 37 [14208/17352 (82%)] Loss: -185419.000000\n",
      "Train Epoch: 37 [15536/17352 (90%)] Loss: -110773.039062\n",
      "Train Epoch: 37 [16232/17352 (94%)] Loss: -157238.328125\n",
      "Train Epoch: 37 [16949/17352 (98%)] Loss: -128305.796875\n",
      "    epoch          : 37\n",
      "    loss           : -171283.14043165374\n",
      "    val_loss       : -92338.99778696695\n",
      "Train Epoch: 38 [128/17352 (1%)] Loss: -179397.109375\n",
      "Train Epoch: 38 [1536/17352 (9%)] Loss: -195704.562500\n",
      "Train Epoch: 38 [2944/17352 (17%)] Loss: -198326.375000\n",
      "Train Epoch: 38 [4352/17352 (25%)] Loss: -138706.062500\n",
      "Train Epoch: 38 [5760/17352 (33%)] Loss: -201909.062500\n",
      "Train Epoch: 38 [7168/17352 (41%)] Loss: -197490.296875\n",
      "Train Epoch: 38 [8576/17352 (49%)] Loss: -155789.109375\n",
      "Train Epoch: 38 [9984/17352 (58%)] Loss: -157526.578125\n",
      "Train Epoch: 38 [11392/17352 (66%)] Loss: -212359.734375\n",
      "Train Epoch: 38 [12800/17352 (74%)] Loss: -188283.375000\n",
      "Train Epoch: 38 [14208/17352 (82%)] Loss: -208934.187500\n",
      "Train Epoch: 38 [15460/17352 (89%)] Loss: -25460.945312\n",
      "Train Epoch: 38 [16127/17352 (93%)] Loss: -143823.203125\n",
      "Train Epoch: 38 [17071/17352 (98%)] Loss: -173963.000000\n",
      "    epoch          : 38\n",
      "    loss           : -174290.06958663225\n",
      "    val_loss       : -98003.61109530131\n",
      "Train Epoch: 39 [128/17352 (1%)] Loss: -196480.687500\n",
      "Train Epoch: 39 [1536/17352 (9%)] Loss: -196038.734375\n",
      "Train Epoch: 39 [2944/17352 (17%)] Loss: -125322.718750\n",
      "Train Epoch: 39 [4352/17352 (25%)] Loss: -209743.281250\n",
      "Train Epoch: 39 [5760/17352 (33%)] Loss: -201315.218750\n",
      "Train Epoch: 39 [7168/17352 (41%)] Loss: -196665.671875\n",
      "Train Epoch: 39 [8576/17352 (49%)] Loss: -186783.437500\n",
      "Train Epoch: 39 [9984/17352 (58%)] Loss: -199197.390625\n",
      "Train Epoch: 39 [11392/17352 (66%)] Loss: -193676.031250\n",
      "Train Epoch: 39 [12800/17352 (74%)] Loss: -208686.156250\n",
      "Train Epoch: 39 [14208/17352 (82%)] Loss: -217899.437500\n",
      "Train Epoch: 39 [15450/17352 (89%)] Loss: -139381.187500\n",
      "Train Epoch: 39 [16226/17352 (94%)] Loss: -23048.929688\n",
      "Train Epoch: 39 [16965/17352 (98%)] Loss: -132558.953125\n",
      "    epoch          : 39\n",
      "    loss           : -183332.63209154783\n",
      "    val_loss       : -104171.2479153951\n",
      "Train Epoch: 40 [128/17352 (1%)] Loss: -211189.375000\n",
      "Train Epoch: 40 [1536/17352 (9%)] Loss: -212585.000000\n",
      "Train Epoch: 40 [2944/17352 (17%)] Loss: -212869.750000\n",
      "Train Epoch: 40 [4352/17352 (25%)] Loss: -227716.406250\n",
      "Train Epoch: 40 [5760/17352 (33%)] Loss: -209165.000000\n",
      "Train Epoch: 40 [7168/17352 (41%)] Loss: -193327.875000\n",
      "Train Epoch: 40 [8576/17352 (49%)] Loss: -209097.718750\n",
      "Train Epoch: 40 [9984/17352 (58%)] Loss: -231615.125000\n",
      "Train Epoch: 40 [11392/17352 (66%)] Loss: -213982.312500\n",
      "Train Epoch: 40 [12800/17352 (74%)] Loss: -216368.015625\n",
      "Train Epoch: 40 [14208/17352 (82%)] Loss: -203588.453125\n",
      "Train Epoch: 40 [15592/17352 (90%)] Loss: -186012.562500\n",
      "Train Epoch: 40 [16334/17352 (94%)] Loss: -117762.796875\n",
      "Train Epoch: 40 [17029/17352 (98%)] Loss: -163123.734375\n",
      "    epoch          : 40\n",
      "    loss           : -192508.23751114198\n",
      "    val_loss       : -106285.60845985412\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch40.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 41 [128/17352 (1%)] Loss: -134724.656250\n",
      "Train Epoch: 41 [1536/17352 (9%)] Loss: -211923.156250\n",
      "Train Epoch: 41 [2944/17352 (17%)] Loss: -192959.281250\n",
      "Train Epoch: 41 [4352/17352 (25%)] Loss: -230215.546875\n",
      "Train Epoch: 41 [5760/17352 (33%)] Loss: -200324.734375\n",
      "Train Epoch: 41 [7168/17352 (41%)] Loss: -220362.031250\n",
      "Train Epoch: 41 [8576/17352 (49%)] Loss: -223979.234375\n",
      "Train Epoch: 41 [9984/17352 (58%)] Loss: -229550.562500\n",
      "Train Epoch: 41 [11392/17352 (66%)] Loss: -225465.093750\n",
      "Train Epoch: 41 [12800/17352 (74%)] Loss: -217585.171875\n",
      "Train Epoch: 41 [14208/17352 (82%)] Loss: -219535.843750\n",
      "Train Epoch: 41 [15522/17352 (89%)] Loss: -155658.703125\n",
      "Train Epoch: 41 [16273/17352 (94%)] Loss: -173743.750000\n",
      "Train Epoch: 41 [16983/17352 (98%)] Loss: -25179.919922\n",
      "    epoch          : 41\n",
      "    loss           : -194107.0046927433\n",
      "    val_loss       : -110814.10245539347\n",
      "Train Epoch: 42 [128/17352 (1%)] Loss: -227962.984375\n",
      "Train Epoch: 42 [1536/17352 (9%)] Loss: -212833.656250\n",
      "Train Epoch: 42 [2944/17352 (17%)] Loss: -179687.156250\n",
      "Train Epoch: 42 [4352/17352 (25%)] Loss: -227224.890625\n",
      "Train Epoch: 42 [5760/17352 (33%)] Loss: -241190.421875\n",
      "Train Epoch: 42 [7168/17352 (41%)] Loss: -210326.281250\n",
      "Train Epoch: 42 [8576/17352 (49%)] Loss: -222081.171875\n",
      "Train Epoch: 42 [9984/17352 (58%)] Loss: -229940.703125\n",
      "Train Epoch: 42 [11392/17352 (66%)] Loss: -202627.531250\n",
      "Train Epoch: 42 [12800/17352 (74%)] Loss: -233220.609375\n",
      "Train Epoch: 42 [14208/17352 (82%)] Loss: -228021.109375\n",
      "Train Epoch: 42 [15522/17352 (89%)] Loss: -140463.906250\n",
      "Train Epoch: 42 [16101/17352 (93%)] Loss: -142932.656250\n",
      "Train Epoch: 42 [17023/17352 (98%)] Loss: -182184.812500\n",
      "    epoch          : 42\n",
      "    loss           : -197340.3611102008\n",
      "    val_loss       : -105421.71742477416\n",
      "Train Epoch: 43 [128/17352 (1%)] Loss: -229109.312500\n",
      "Train Epoch: 43 [1536/17352 (9%)] Loss: -211370.015625\n",
      "Train Epoch: 43 [2944/17352 (17%)] Loss: -215762.203125\n",
      "Train Epoch: 43 [4352/17352 (25%)] Loss: -217330.937500\n",
      "Train Epoch: 43 [5760/17352 (33%)] Loss: -228020.250000\n",
      "Train Epoch: 43 [7168/17352 (41%)] Loss: -218545.687500\n",
      "Train Epoch: 43 [8576/17352 (49%)] Loss: -215818.656250\n",
      "Train Epoch: 43 [9984/17352 (58%)] Loss: -238931.328125\n",
      "Train Epoch: 43 [11392/17352 (66%)] Loss: -222516.921875\n",
      "Train Epoch: 43 [12800/17352 (74%)] Loss: -220830.406250\n",
      "Train Epoch: 43 [14208/17352 (82%)] Loss: -243151.328125\n",
      "Train Epoch: 43 [15514/17352 (89%)] Loss: -138405.953125\n",
      "Train Epoch: 43 [16366/17352 (94%)] Loss: -50501.945312\n",
      "Train Epoch: 43 [17131/17352 (99%)] Loss: -231922.156250\n",
      "    epoch          : 43\n",
      "    loss           : -198523.94963165896\n",
      "    val_loss       : -112442.04158910115\n",
      "Train Epoch: 44 [128/17352 (1%)] Loss: -217535.187500\n",
      "Train Epoch: 44 [1536/17352 (9%)] Loss: -222471.046875\n",
      "Train Epoch: 44 [2944/17352 (17%)] Loss: -228844.781250\n",
      "Train Epoch: 44 [4352/17352 (25%)] Loss: -238683.093750\n",
      "Train Epoch: 44 [5760/17352 (33%)] Loss: -229330.000000\n",
      "Train Epoch: 44 [7168/17352 (41%)] Loss: -244387.953125\n",
      "Train Epoch: 44 [8576/17352 (49%)] Loss: -242406.562500\n",
      "Train Epoch: 44 [9984/17352 (58%)] Loss: -229320.093750\n",
      "Train Epoch: 44 [11392/17352 (66%)] Loss: -236682.593750\n",
      "Train Epoch: 44 [12800/17352 (74%)] Loss: -239358.062500\n",
      "Train Epoch: 44 [14208/17352 (82%)] Loss: -197660.093750\n",
      "Train Epoch: 44 [15522/17352 (89%)] Loss: -123395.078125\n",
      "Train Epoch: 44 [16192/17352 (93%)] Loss: -176993.062500\n",
      "Train Epoch: 44 [17100/17352 (99%)] Loss: -177663.656250\n",
      "    epoch          : 44\n",
      "    loss           : -204304.11337300754\n",
      "    val_loss       : -108890.56478627522\n",
      "Train Epoch: 45 [128/17352 (1%)] Loss: -232740.812500\n",
      "Train Epoch: 45 [1536/17352 (9%)] Loss: -240608.156250\n",
      "Train Epoch: 45 [2944/17352 (17%)] Loss: -174708.687500\n",
      "Train Epoch: 45 [4352/17352 (25%)] Loss: -212203.875000\n",
      "Train Epoch: 45 [5760/17352 (33%)] Loss: -240003.156250\n",
      "Train Epoch: 45 [7168/17352 (41%)] Loss: -237476.421875\n",
      "Train Epoch: 45 [8576/17352 (49%)] Loss: -194823.906250\n",
      "Train Epoch: 45 [9984/17352 (58%)] Loss: -226315.093750\n",
      "Train Epoch: 45 [11392/17352 (66%)] Loss: -233620.828125\n",
      "Train Epoch: 45 [12800/17352 (74%)] Loss: -236598.453125\n",
      "Train Epoch: 45 [14208/17352 (82%)] Loss: -207178.625000\n",
      "Train Epoch: 45 [15446/17352 (89%)] Loss: -23038.242188\n",
      "Train Epoch: 45 [16307/17352 (94%)] Loss: -165769.234375\n",
      "Train Epoch: 45 [16980/17352 (98%)] Loss: -132585.296875\n",
      "    epoch          : 45\n",
      "    loss           : -202122.38707594902\n",
      "    val_loss       : -110439.98733825683\n",
      "Train Epoch: 46 [128/17352 (1%)] Loss: -232449.781250\n",
      "Train Epoch: 46 [1536/17352 (9%)] Loss: -253613.875000\n",
      "Train Epoch: 46 [2944/17352 (17%)] Loss: -199833.187500\n",
      "Train Epoch: 46 [4352/17352 (25%)] Loss: -222094.562500\n",
      "Train Epoch: 46 [5760/17352 (33%)] Loss: -216095.812500\n",
      "Train Epoch: 46 [7168/17352 (41%)] Loss: -229710.890625\n",
      "Train Epoch: 46 [8576/17352 (49%)] Loss: -238206.750000\n",
      "Train Epoch: 46 [9984/17352 (58%)] Loss: -235826.234375\n",
      "Train Epoch: 46 [11392/17352 (66%)] Loss: -187054.750000\n",
      "Train Epoch: 46 [12800/17352 (74%)] Loss: -213507.734375\n",
      "Train Epoch: 46 [14208/17352 (82%)] Loss: -152664.984375\n",
      "Train Epoch: 46 [15502/17352 (89%)] Loss: -67200.343750\n",
      "Train Epoch: 46 [16225/17352 (94%)] Loss: -181792.875000\n",
      "Train Epoch: 46 [17079/17352 (98%)] Loss: -136260.031250\n",
      "    epoch          : 46\n",
      "    loss           : -208148.61972394085\n",
      "    val_loss       : -112309.57538935343\n",
      "Train Epoch: 47 [128/17352 (1%)] Loss: -216691.015625\n",
      "Train Epoch: 47 [1536/17352 (9%)] Loss: -235178.890625\n",
      "Train Epoch: 47 [2944/17352 (17%)] Loss: -223810.484375\n",
      "Train Epoch: 47 [4352/17352 (25%)] Loss: -237664.281250\n",
      "Train Epoch: 47 [5760/17352 (33%)] Loss: -202134.609375\n",
      "Train Epoch: 47 [7168/17352 (41%)] Loss: -226217.718750\n",
      "Train Epoch: 47 [8576/17352 (49%)] Loss: -246646.390625\n",
      "Train Epoch: 47 [9984/17352 (58%)] Loss: -245985.781250\n",
      "Train Epoch: 47 [11392/17352 (66%)] Loss: -180042.281250\n",
      "Train Epoch: 47 [12800/17352 (74%)] Loss: -230514.859375\n",
      "Train Epoch: 47 [14208/17352 (82%)] Loss: -224571.625000\n",
      "Train Epoch: 47 [15484/17352 (89%)] Loss: -66276.250000\n",
      "Train Epoch: 47 [16314/17352 (94%)] Loss: -30205.917969\n",
      "Train Epoch: 47 [17093/17352 (99%)] Loss: -94774.734375\n",
      "    epoch          : 47\n",
      "    loss           : -205682.8062441013\n",
      "    val_loss       : -114170.49476674398\n",
      "Train Epoch: 48 [128/17352 (1%)] Loss: -234371.609375\n",
      "Train Epoch: 48 [1536/17352 (9%)] Loss: -233016.015625\n",
      "Train Epoch: 48 [2944/17352 (17%)] Loss: -245656.046875\n",
      "Train Epoch: 48 [4352/17352 (25%)] Loss: -216816.500000\n",
      "Train Epoch: 48 [5760/17352 (33%)] Loss: -229653.218750\n",
      "Train Epoch: 48 [7168/17352 (41%)] Loss: -241901.437500\n",
      "Train Epoch: 48 [8576/17352 (49%)] Loss: -233719.328125\n",
      "Train Epoch: 48 [9984/17352 (58%)] Loss: -224447.859375\n",
      "Train Epoch: 48 [11392/17352 (66%)] Loss: -205343.640625\n",
      "Train Epoch: 48 [12800/17352 (74%)] Loss: -235032.765625\n",
      "Train Epoch: 48 [14208/17352 (82%)] Loss: -234946.750000\n",
      "Train Epoch: 48 [15450/17352 (89%)] Loss: -120854.687500\n",
      "Train Epoch: 48 [16350/17352 (94%)] Loss: -25689.636719\n",
      "Train Epoch: 48 [17099/17352 (99%)] Loss: -131790.218750\n",
      "    epoch          : 48\n",
      "    loss           : -203474.20028707004\n",
      "    val_loss       : -111894.72584495545\n",
      "Train Epoch: 49 [128/17352 (1%)] Loss: -230629.781250\n",
      "Train Epoch: 49 [1536/17352 (9%)] Loss: -216401.406250\n",
      "Train Epoch: 49 [2944/17352 (17%)] Loss: -213990.484375\n",
      "Train Epoch: 49 [4352/17352 (25%)] Loss: -226858.562500\n",
      "Train Epoch: 49 [5760/17352 (33%)] Loss: -242462.656250\n",
      "Train Epoch: 49 [7168/17352 (41%)] Loss: -246998.500000\n",
      "Train Epoch: 49 [8576/17352 (49%)] Loss: -208028.593750\n",
      "Train Epoch: 49 [9984/17352 (58%)] Loss: -212379.062500\n",
      "Train Epoch: 49 [11392/17352 (66%)] Loss: -225595.093750\n",
      "Train Epoch: 49 [12800/17352 (74%)] Loss: -237867.015625\n",
      "Train Epoch: 49 [14208/17352 (82%)] Loss: -199401.000000\n",
      "Train Epoch: 49 [15588/17352 (90%)] Loss: -202692.703125\n",
      "Train Epoch: 49 [16232/17352 (94%)] Loss: -132595.406250\n",
      "Train Epoch: 49 [17123/17352 (99%)] Loss: -155816.562500\n",
      "    epoch          : 49\n",
      "    loss           : -199279.67137033347\n",
      "    val_loss       : -113452.59730097452\n",
      "Train Epoch: 50 [128/17352 (1%)] Loss: -216952.906250\n",
      "Train Epoch: 50 [1536/17352 (9%)] Loss: -250332.312500\n",
      "Train Epoch: 50 [2944/17352 (17%)] Loss: -236815.406250\n",
      "Train Epoch: 50 [4352/17352 (25%)] Loss: -245015.609375\n",
      "Train Epoch: 50 [5760/17352 (33%)] Loss: -227655.500000\n",
      "Train Epoch: 50 [7168/17352 (41%)] Loss: -215818.156250\n",
      "Train Epoch: 50 [8576/17352 (49%)] Loss: -243630.375000\n",
      "Train Epoch: 50 [9984/17352 (58%)] Loss: -193756.203125\n",
      "Train Epoch: 50 [11392/17352 (66%)] Loss: -209238.750000\n",
      "Train Epoch: 50 [12800/17352 (74%)] Loss: -224054.750000\n",
      "Train Epoch: 50 [14208/17352 (82%)] Loss: -209597.843750\n",
      "Train Epoch: 50 [15554/17352 (90%)] Loss: -206445.812500\n",
      "Train Epoch: 50 [16330/17352 (94%)] Loss: -133716.968750\n",
      "Train Epoch: 50 [17057/17352 (98%)] Loss: -81764.484375\n",
      "    epoch          : 50\n",
      "    loss           : -202612.05901812867\n",
      "    val_loss       : -112473.83283843994\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [128/17352 (1%)] Loss: -209253.718750\n",
      "Train Epoch: 51 [1536/17352 (9%)] Loss: -230285.937500\n",
      "Train Epoch: 51 [2944/17352 (17%)] Loss: -238029.578125\n",
      "Train Epoch: 51 [4352/17352 (25%)] Loss: -210731.281250\n",
      "Train Epoch: 51 [5760/17352 (33%)] Loss: -231910.265625\n",
      "Train Epoch: 51 [7168/17352 (41%)] Loss: -172194.296875\n",
      "Train Epoch: 51 [8576/17352 (49%)] Loss: -224565.812500\n",
      "Train Epoch: 51 [9984/17352 (58%)] Loss: -243044.625000\n",
      "Train Epoch: 51 [11392/17352 (66%)] Loss: -234836.687500\n",
      "Train Epoch: 51 [12800/17352 (74%)] Loss: -251522.125000\n",
      "Train Epoch: 51 [14208/17352 (82%)] Loss: -238328.937500\n",
      "Train Epoch: 51 [15480/17352 (89%)] Loss: -205693.281250\n",
      "Train Epoch: 51 [16193/17352 (93%)] Loss: -172228.203125\n",
      "Train Epoch: 51 [16931/17352 (98%)] Loss: -143195.828125\n",
      "    epoch          : 51\n",
      "    loss           : -205183.49906276216\n",
      "    val_loss       : -106344.72999572754\n",
      "Train Epoch: 52 [128/17352 (1%)] Loss: -250241.703125\n",
      "Train Epoch: 52 [1536/17352 (9%)] Loss: -188143.468750\n",
      "Train Epoch: 52 [2944/17352 (17%)] Loss: -228087.000000\n",
      "Train Epoch: 52 [4352/17352 (25%)] Loss: -204673.468750\n",
      "Train Epoch: 52 [5760/17352 (33%)] Loss: -227548.703125\n",
      "Train Epoch: 52 [7168/17352 (41%)] Loss: -224215.734375\n",
      "Train Epoch: 52 [8576/17352 (49%)] Loss: -217443.406250\n",
      "Train Epoch: 52 [9984/17352 (58%)] Loss: -239747.609375\n",
      "Train Epoch: 52 [11392/17352 (66%)] Loss: -238891.625000\n",
      "Train Epoch: 52 [12800/17352 (74%)] Loss: -242074.093750\n",
      "Train Epoch: 52 [14208/17352 (82%)] Loss: -251199.968750\n",
      "Train Epoch: 52 [15482/17352 (89%)] Loss: -51167.851562\n",
      "Train Epoch: 52 [16312/17352 (94%)] Loss: -217937.734375\n",
      "Train Epoch: 52 [16880/17352 (97%)] Loss: -100489.531250\n",
      "    epoch          : 52\n",
      "    loss           : -206503.21723927747\n",
      "    val_loss       : -111737.24875590006\n",
      "Train Epoch: 53 [128/17352 (1%)] Loss: -241668.406250\n",
      "Train Epoch: 53 [1536/17352 (9%)] Loss: -78774.726562\n",
      "Train Epoch: 53 [2944/17352 (17%)] Loss: -227693.500000\n",
      "Train Epoch: 53 [4352/17352 (25%)] Loss: -210575.390625\n",
      "Train Epoch: 53 [5760/17352 (33%)] Loss: -222491.687500\n",
      "Train Epoch: 53 [7168/17352 (41%)] Loss: -229273.687500\n",
      "Train Epoch: 53 [8576/17352 (49%)] Loss: -237687.125000\n",
      "Train Epoch: 53 [9984/17352 (58%)] Loss: -247357.703125\n",
      "Train Epoch: 53 [11392/17352 (66%)] Loss: -222097.015625\n",
      "Train Epoch: 53 [12800/17352 (74%)] Loss: -223099.109375\n",
      "Train Epoch: 53 [14208/17352 (82%)] Loss: -246839.296875\n",
      "Train Epoch: 53 [15520/17352 (89%)] Loss: -124348.414062\n",
      "Train Epoch: 53 [16050/17352 (92%)] Loss: -4599.220215\n",
      "Train Epoch: 53 [16907/17352 (97%)] Loss: -23915.617188\n",
      "    epoch          : 53\n",
      "    loss           : -204263.51899053587\n",
      "    val_loss       : -115366.28459879557\n",
      "Train Epoch: 54 [128/17352 (1%)] Loss: -201690.359375\n",
      "Train Epoch: 54 [1536/17352 (9%)] Loss: -203911.296875\n",
      "Train Epoch: 54 [2944/17352 (17%)] Loss: -249381.031250\n",
      "Train Epoch: 54 [4352/17352 (25%)] Loss: -209403.015625\n",
      "Train Epoch: 54 [5760/17352 (33%)] Loss: -227572.968750\n",
      "Train Epoch: 54 [7168/17352 (41%)] Loss: -225286.359375\n",
      "Train Epoch: 54 [8576/17352 (49%)] Loss: -229891.312500\n",
      "Train Epoch: 54 [9984/17352 (58%)] Loss: -234845.359375\n",
      "Train Epoch: 54 [11392/17352 (66%)] Loss: -252517.312500\n",
      "Train Epoch: 54 [12800/17352 (74%)] Loss: -239945.093750\n",
      "Train Epoch: 54 [14208/17352 (82%)] Loss: -230157.968750\n",
      "Train Epoch: 54 [15577/17352 (90%)] Loss: -229297.078125\n",
      "Train Epoch: 54 [16317/17352 (94%)] Loss: -100042.437500\n",
      "Train Epoch: 54 [16930/17352 (98%)] Loss: -77647.882812\n",
      "    epoch          : 54\n",
      "    loss           : -206753.0059249161\n",
      "    val_loss       : -117555.42793184916\n",
      "Train Epoch: 55 [128/17352 (1%)] Loss: -232651.281250\n",
      "Train Epoch: 55 [1536/17352 (9%)] Loss: -244289.734375\n",
      "Train Epoch: 55 [2944/17352 (17%)] Loss: -252472.593750\n",
      "Train Epoch: 55 [4352/17352 (25%)] Loss: -240233.687500\n",
      "Train Epoch: 55 [5760/17352 (33%)] Loss: -234845.656250\n",
      "Train Epoch: 55 [7168/17352 (41%)] Loss: -243353.937500\n",
      "Train Epoch: 55 [8576/17352 (49%)] Loss: -247883.031250\n",
      "Train Epoch: 55 [9984/17352 (58%)] Loss: -259819.156250\n",
      "Train Epoch: 55 [11392/17352 (66%)] Loss: -240205.875000\n",
      "Train Epoch: 55 [12800/17352 (74%)] Loss: -247968.265625\n",
      "Train Epoch: 55 [14208/17352 (82%)] Loss: -258362.390625\n",
      "Train Epoch: 55 [15482/17352 (89%)] Loss: -164675.218750\n",
      "Train Epoch: 55 [16334/17352 (94%)] Loss: -185014.218750\n",
      "Train Epoch: 55 [17104/17352 (99%)] Loss: -126697.195312\n",
      "    epoch          : 55\n",
      "    loss           : -212148.29472852874\n",
      "    val_loss       : -114706.91507288614\n",
      "Train Epoch: 56 [128/17352 (1%)] Loss: -225247.921875\n",
      "Train Epoch: 56 [1536/17352 (9%)] Loss: -254434.093750\n",
      "Train Epoch: 56 [2944/17352 (17%)] Loss: -251755.421875\n",
      "Train Epoch: 56 [4352/17352 (25%)] Loss: -225916.765625\n",
      "Train Epoch: 56 [5760/17352 (33%)] Loss: -218004.265625\n",
      "Train Epoch: 56 [7168/17352 (41%)] Loss: -274449.218750\n",
      "Train Epoch: 56 [8576/17352 (49%)] Loss: -231394.000000\n",
      "Train Epoch: 56 [9984/17352 (58%)] Loss: -213788.828125\n",
      "Train Epoch: 56 [11392/17352 (66%)] Loss: -241757.375000\n",
      "Train Epoch: 56 [12800/17352 (74%)] Loss: -251115.609375\n",
      "Train Epoch: 56 [14208/17352 (82%)] Loss: -249649.687500\n",
      "Train Epoch: 56 [15499/17352 (89%)] Loss: -61083.546875\n",
      "Train Epoch: 56 [16383/17352 (94%)] Loss: -93211.359375\n",
      "Train Epoch: 56 [17095/17352 (99%)] Loss: -74709.320312\n",
      "    epoch          : 56\n",
      "    loss           : -214216.49335085467\n",
      "    val_loss       : -118905.5569925944\n",
      "Train Epoch: 57 [128/17352 (1%)] Loss: -259904.531250\n",
      "Train Epoch: 57 [1536/17352 (9%)] Loss: -254219.515625\n",
      "Train Epoch: 57 [2944/17352 (17%)] Loss: -222278.265625\n",
      "Train Epoch: 57 [4352/17352 (25%)] Loss: -253424.343750\n",
      "Train Epoch: 57 [5760/17352 (33%)] Loss: -223860.953125\n",
      "Train Epoch: 57 [7168/17352 (41%)] Loss: -249668.937500\n",
      "Train Epoch: 57 [8576/17352 (49%)] Loss: -253587.375000\n",
      "Train Epoch: 57 [9984/17352 (58%)] Loss: -244743.140625\n",
      "Train Epoch: 57 [11392/17352 (66%)] Loss: -253141.718750\n",
      "Train Epoch: 57 [12800/17352 (74%)] Loss: -260410.937500\n",
      "Train Epoch: 57 [14208/17352 (82%)] Loss: -242003.828125\n",
      "Train Epoch: 57 [15527/17352 (89%)] Loss: -143466.781250\n",
      "Train Epoch: 57 [16353/17352 (94%)] Loss: -203877.296875\n",
      "Train Epoch: 57 [17062/17352 (98%)] Loss: -135124.406250\n",
      "    epoch          : 57\n",
      "    loss           : -214811.93989552747\n",
      "    val_loss       : -120970.31684875488\n",
      "Train Epoch: 58 [128/17352 (1%)] Loss: -253535.796875\n",
      "Train Epoch: 58 [1536/17352 (9%)] Loss: -256291.375000\n",
      "Train Epoch: 58 [2944/17352 (17%)] Loss: -233395.687500\n",
      "Train Epoch: 58 [4352/17352 (25%)] Loss: -204150.359375\n",
      "Train Epoch: 58 [5760/17352 (33%)] Loss: -250702.250000\n",
      "Train Epoch: 58 [7168/17352 (41%)] Loss: -242070.750000\n",
      "Train Epoch: 58 [8576/17352 (49%)] Loss: -247649.531250\n",
      "Train Epoch: 58 [9984/17352 (58%)] Loss: -214879.125000\n",
      "Train Epoch: 58 [11392/17352 (66%)] Loss: -239301.531250\n",
      "Train Epoch: 58 [12800/17352 (74%)] Loss: -267112.562500\n",
      "Train Epoch: 58 [14208/17352 (82%)] Loss: -252896.203125\n",
      "Train Epoch: 58 [15499/17352 (89%)] Loss: -141071.625000\n",
      "Train Epoch: 58 [16278/17352 (94%)] Loss: -161415.937500\n",
      "Train Epoch: 58 [16946/17352 (98%)] Loss: -188449.843750\n",
      "    epoch          : 58\n",
      "    loss           : -222845.906308987\n",
      "    val_loss       : -124449.39489339193\n",
      "Train Epoch: 59 [128/17352 (1%)] Loss: -256323.406250\n",
      "Train Epoch: 59 [1536/17352 (9%)] Loss: -252345.234375\n",
      "Train Epoch: 59 [2944/17352 (17%)] Loss: -184735.187500\n",
      "Train Epoch: 59 [4352/17352 (25%)] Loss: -251955.546875\n",
      "Train Epoch: 59 [5760/17352 (33%)] Loss: -255318.156250\n",
      "Train Epoch: 59 [7168/17352 (41%)] Loss: -263379.593750\n",
      "Train Epoch: 59 [8576/17352 (49%)] Loss: -187692.062500\n",
      "Train Epoch: 59 [9984/17352 (58%)] Loss: -263949.875000\n",
      "Train Epoch: 59 [11392/17352 (66%)] Loss: -268144.187500\n",
      "Train Epoch: 59 [12800/17352 (74%)] Loss: -92500.031250\n",
      "Train Epoch: 59 [14208/17352 (82%)] Loss: -269772.906250\n",
      "Train Epoch: 59 [15437/17352 (89%)] Loss: -68579.898438\n",
      "Train Epoch: 59 [16176/17352 (93%)] Loss: -9214.881836\n",
      "Train Epoch: 59 [16912/17352 (97%)] Loss: -125748.578125\n",
      "    epoch          : 59\n",
      "    loss           : -226711.32672242032\n",
      "    val_loss       : -129241.60662027994\n",
      "Train Epoch: 60 [128/17352 (1%)] Loss: -272082.343750\n",
      "Train Epoch: 60 [1536/17352 (9%)] Loss: -226154.500000\n",
      "Train Epoch: 60 [2944/17352 (17%)] Loss: -246245.812500\n",
      "Train Epoch: 60 [4352/17352 (25%)] Loss: -267140.156250\n",
      "Train Epoch: 60 [5760/17352 (33%)] Loss: -251274.062500\n",
      "Train Epoch: 60 [7168/17352 (41%)] Loss: -244499.906250\n",
      "Train Epoch: 60 [8576/17352 (49%)] Loss: -248483.281250\n",
      "Train Epoch: 60 [9984/17352 (58%)] Loss: -241860.609375\n",
      "Train Epoch: 60 [11392/17352 (66%)] Loss: -227313.218750\n",
      "Train Epoch: 60 [12800/17352 (74%)] Loss: -219111.218750\n",
      "Train Epoch: 60 [14208/17352 (82%)] Loss: -255067.921875\n",
      "Train Epoch: 60 [15515/17352 (89%)] Loss: -211464.531250\n",
      "Train Epoch: 60 [16181/17352 (93%)] Loss: -5812.981445\n",
      "Train Epoch: 60 [16912/17352 (97%)] Loss: -165306.484375\n",
      "    epoch          : 60\n",
      "    loss           : -219948.55174470428\n",
      "    val_loss       : -125464.93073323568\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch60.pth ...\n",
      "Train Epoch: 61 [128/17352 (1%)] Loss: -243938.046875\n",
      "Train Epoch: 61 [1536/17352 (9%)] Loss: -237219.406250\n",
      "Train Epoch: 61 [2944/17352 (17%)] Loss: -265959.906250\n",
      "Train Epoch: 61 [4352/17352 (25%)] Loss: -238439.156250\n",
      "Train Epoch: 61 [5760/17352 (33%)] Loss: -232878.671875\n",
      "Train Epoch: 61 [7168/17352 (41%)] Loss: -257069.031250\n",
      "Train Epoch: 61 [8576/17352 (49%)] Loss: -251681.421875\n",
      "Train Epoch: 61 [9984/17352 (58%)] Loss: -230598.312500\n",
      "Train Epoch: 61 [11392/17352 (66%)] Loss: -247256.906250\n",
      "Train Epoch: 61 [12800/17352 (74%)] Loss: -246886.515625\n",
      "Train Epoch: 61 [14208/17352 (82%)] Loss: -243120.078125\n",
      "Train Epoch: 61 [15550/17352 (90%)] Loss: -140910.156250\n",
      "Train Epoch: 61 [16177/17352 (93%)] Loss: -184303.656250\n",
      "Train Epoch: 61 [16962/17352 (98%)] Loss: -132811.000000\n",
      "    epoch          : 61\n",
      "    loss           : -220583.49343278105\n",
      "    val_loss       : -122549.61958312988\n",
      "Train Epoch: 62 [128/17352 (1%)] Loss: -172581.375000\n",
      "Train Epoch: 62 [1536/17352 (9%)] Loss: -259076.765625\n",
      "Train Epoch: 62 [2944/17352 (17%)] Loss: -190118.906250\n",
      "Train Epoch: 62 [4352/17352 (25%)] Loss: -174388.421875\n",
      "Train Epoch: 62 [5760/17352 (33%)] Loss: -270143.531250\n",
      "Train Epoch: 62 [7168/17352 (41%)] Loss: -257766.906250\n",
      "Train Epoch: 62 [8576/17352 (49%)] Loss: -232959.937500\n",
      "Train Epoch: 62 [9984/17352 (58%)] Loss: -236833.625000\n",
      "Train Epoch: 62 [11392/17352 (66%)] Loss: -238712.140625\n",
      "Train Epoch: 62 [12800/17352 (74%)] Loss: -272723.687500\n",
      "Train Epoch: 62 [14208/17352 (82%)] Loss: -250170.765625\n",
      "Train Epoch: 62 [15552/17352 (90%)] Loss: -145204.656250\n",
      "Train Epoch: 62 [16306/17352 (94%)] Loss: -88045.015625\n",
      "Train Epoch: 62 [17133/17352 (99%)] Loss: -34790.039062\n",
      "    epoch          : 62\n",
      "    loss           : -223842.99024420616\n",
      "    val_loss       : -124071.83127441406\n",
      "Train Epoch: 63 [128/17352 (1%)] Loss: -259167.390625\n",
      "Train Epoch: 63 [1536/17352 (9%)] Loss: -267615.156250\n",
      "Train Epoch: 63 [2944/17352 (17%)] Loss: -256835.375000\n",
      "Train Epoch: 63 [4352/17352 (25%)] Loss: -242401.640625\n",
      "Train Epoch: 63 [5760/17352 (33%)] Loss: -245704.140625\n",
      "Train Epoch: 63 [7168/17352 (41%)] Loss: -255979.468750\n",
      "Train Epoch: 63 [8576/17352 (49%)] Loss: -268962.843750\n",
      "Train Epoch: 63 [9984/17352 (58%)] Loss: -262536.531250\n",
      "Train Epoch: 63 [11392/17352 (66%)] Loss: -130756.257812\n",
      "Train Epoch: 63 [12800/17352 (74%)] Loss: -211365.609375\n",
      "Train Epoch: 63 [14208/17352 (82%)] Loss: -265283.750000\n",
      "Train Epoch: 63 [15463/17352 (89%)] Loss: -181543.937500\n",
      "Train Epoch: 63 [16142/17352 (93%)] Loss: -230894.906250\n",
      "Train Epoch: 63 [16904/17352 (97%)] Loss: -79870.203125\n",
      "    epoch          : 63\n",
      "    loss           : -227215.6342970061\n",
      "    val_loss       : -119840.14637349447\n",
      "Train Epoch: 64 [128/17352 (1%)] Loss: -231977.750000\n",
      "Train Epoch: 64 [1536/17352 (9%)] Loss: -209764.656250\n",
      "Train Epoch: 64 [2944/17352 (17%)] Loss: -251274.500000\n",
      "Train Epoch: 64 [4352/17352 (25%)] Loss: -253511.812500\n",
      "Train Epoch: 64 [5760/17352 (33%)] Loss: -265149.687500\n",
      "Train Epoch: 64 [7168/17352 (41%)] Loss: -233383.671875\n",
      "Train Epoch: 64 [8576/17352 (49%)] Loss: -260933.171875\n",
      "Train Epoch: 64 [9984/17352 (58%)] Loss: -274008.312500\n",
      "Train Epoch: 64 [11392/17352 (66%)] Loss: -259016.125000\n",
      "Train Epoch: 64 [12800/17352 (74%)] Loss: -273558.843750\n",
      "Train Epoch: 64 [14208/17352 (82%)] Loss: -253031.281250\n",
      "Train Epoch: 64 [15436/17352 (89%)] Loss: -139301.156250\n",
      "Train Epoch: 64 [16182/17352 (93%)] Loss: -92178.304688\n",
      "Train Epoch: 64 [16987/17352 (98%)] Loss: -192993.859375\n",
      "    epoch          : 64\n",
      "    loss           : -227780.916133599\n",
      "    val_loss       : -127139.23647054036\n",
      "Train Epoch: 65 [128/17352 (1%)] Loss: -191953.984375\n",
      "Train Epoch: 65 [1536/17352 (9%)] Loss: -252224.125000\n",
      "Train Epoch: 65 [2944/17352 (17%)] Loss: -188872.843750\n",
      "Train Epoch: 65 [4352/17352 (25%)] Loss: -252482.875000\n",
      "Train Epoch: 65 [5760/17352 (33%)] Loss: -272436.812500\n",
      "Train Epoch: 65 [7168/17352 (41%)] Loss: -231696.812500\n",
      "Train Epoch: 65 [8576/17352 (49%)] Loss: -218849.203125\n",
      "Train Epoch: 65 [9984/17352 (58%)] Loss: -261524.562500\n",
      "Train Epoch: 65 [11392/17352 (66%)] Loss: -198928.625000\n",
      "Train Epoch: 65 [12800/17352 (74%)] Loss: -248477.750000\n",
      "Train Epoch: 65 [14208/17352 (82%)] Loss: -247192.531250\n",
      "Train Epoch: 65 [15570/17352 (90%)] Loss: -183441.281250\n",
      "Train Epoch: 65 [16260/17352 (94%)] Loss: -149595.500000\n",
      "Train Epoch: 65 [17026/17352 (98%)] Loss: -29780.050781\n",
      "    epoch          : 65\n",
      "    loss           : -226704.92593854867\n",
      "    val_loss       : -127687.91939697266\n",
      "Train Epoch: 66 [128/17352 (1%)] Loss: -244255.906250\n",
      "Train Epoch: 66 [1536/17352 (9%)] Loss: -246249.437500\n",
      "Train Epoch: 66 [2944/17352 (17%)] Loss: -285078.218750\n",
      "Train Epoch: 66 [4352/17352 (25%)] Loss: -267747.406250\n",
      "Train Epoch: 66 [5760/17352 (33%)] Loss: -263239.750000\n",
      "Train Epoch: 66 [7168/17352 (41%)] Loss: -246145.703125\n",
      "Train Epoch: 66 [8576/17352 (49%)] Loss: -262198.875000\n",
      "Train Epoch: 66 [9984/17352 (58%)] Loss: -250662.343750\n",
      "Train Epoch: 66 [11392/17352 (66%)] Loss: -251075.000000\n",
      "Train Epoch: 66 [12800/17352 (74%)] Loss: -254856.328125\n",
      "Train Epoch: 66 [14208/17352 (82%)] Loss: -246652.109375\n",
      "Train Epoch: 66 [15539/17352 (90%)] Loss: -173209.218750\n",
      "Train Epoch: 66 [16200/17352 (93%)] Loss: -66708.531250\n",
      "Train Epoch: 66 [17063/17352 (98%)] Loss: -257424.171875\n",
      "    epoch          : 66\n",
      "    loss           : -232777.72636915374\n",
      "    val_loss       : -128008.37673339844\n",
      "Train Epoch: 67 [128/17352 (1%)] Loss: -281947.875000\n",
      "Train Epoch: 67 [1536/17352 (9%)] Loss: -265397.093750\n",
      "Train Epoch: 67 [2944/17352 (17%)] Loss: -191120.015625\n",
      "Train Epoch: 67 [4352/17352 (25%)] Loss: -281165.062500\n",
      "Train Epoch: 67 [5760/17352 (33%)] Loss: -278646.437500\n",
      "Train Epoch: 67 [7168/17352 (41%)] Loss: -280542.125000\n",
      "Train Epoch: 67 [8576/17352 (49%)] Loss: -280181.375000\n",
      "Train Epoch: 67 [9984/17352 (58%)] Loss: -261911.250000\n",
      "Train Epoch: 67 [11392/17352 (66%)] Loss: -284553.343750\n",
      "Train Epoch: 67 [12800/17352 (74%)] Loss: -258014.343750\n",
      "Train Epoch: 67 [14208/17352 (82%)] Loss: -274544.531250\n",
      "Train Epoch: 67 [15437/17352 (89%)] Loss: -103826.148438\n",
      "Train Epoch: 67 [16168/17352 (93%)] Loss: -187881.281250\n",
      "Train Epoch: 67 [17015/17352 (98%)] Loss: -165955.859375\n",
      "    epoch          : 67\n",
      "    loss           : -236758.66653339975\n",
      "    val_loss       : -134942.36876424155\n",
      "Train Epoch: 68 [128/17352 (1%)] Loss: -254857.953125\n",
      "Train Epoch: 68 [1536/17352 (9%)] Loss: -231878.359375\n",
      "Train Epoch: 68 [2944/17352 (17%)] Loss: -265188.843750\n",
      "Train Epoch: 68 [4352/17352 (25%)] Loss: -267792.625000\n",
      "Train Epoch: 68 [5760/17352 (33%)] Loss: -273206.187500\n",
      "Train Epoch: 68 [7168/17352 (41%)] Loss: -270345.562500\n",
      "Train Epoch: 68 [8576/17352 (49%)] Loss: -225235.968750\n",
      "Train Epoch: 68 [9984/17352 (58%)] Loss: -286360.812500\n",
      "Train Epoch: 68 [11392/17352 (66%)] Loss: -271821.093750\n",
      "Train Epoch: 68 [12800/17352 (74%)] Loss: -261286.437500\n",
      "Train Epoch: 68 [14208/17352 (82%)] Loss: -276353.406250\n",
      "Train Epoch: 68 [15506/17352 (89%)] Loss: -86525.937500\n",
      "Train Epoch: 68 [16331/17352 (94%)] Loss: -65855.695312\n",
      "Train Epoch: 68 [17030/17352 (98%)] Loss: -110066.226562\n",
      "    epoch          : 68\n",
      "    loss           : -234961.9818123427\n",
      "    val_loss       : -133403.5874633789\n",
      "Train Epoch: 69 [128/17352 (1%)] Loss: -268323.312500\n",
      "Train Epoch: 69 [1536/17352 (9%)] Loss: -260183.156250\n",
      "Train Epoch: 69 [2944/17352 (17%)] Loss: -266637.062500\n",
      "Train Epoch: 69 [4352/17352 (25%)] Loss: -273408.062500\n",
      "Train Epoch: 69 [5760/17352 (33%)] Loss: -261968.406250\n",
      "Train Epoch: 69 [7168/17352 (41%)] Loss: -246239.093750\n",
      "Train Epoch: 69 [8576/17352 (49%)] Loss: -251153.093750\n",
      "Train Epoch: 69 [9984/17352 (58%)] Loss: -270876.156250\n",
      "Train Epoch: 69 [11392/17352 (66%)] Loss: -230072.265625\n",
      "Train Epoch: 69 [12800/17352 (74%)] Loss: -225791.343750\n",
      "Train Epoch: 69 [14208/17352 (82%)] Loss: -208837.906250\n",
      "Train Epoch: 69 [15522/17352 (89%)] Loss: -161192.281250\n",
      "Train Epoch: 69 [16368/17352 (94%)] Loss: -159061.187500\n",
      "Train Epoch: 69 [17114/17352 (99%)] Loss: -225789.843750\n",
      "    epoch          : 69\n",
      "    loss           : -229941.6110626835\n",
      "    val_loss       : -122530.83418986002\n",
      "Train Epoch: 70 [128/17352 (1%)] Loss: -268040.531250\n",
      "Train Epoch: 70 [1536/17352 (9%)] Loss: -285509.375000\n",
      "Train Epoch: 70 [2944/17352 (17%)] Loss: -200032.187500\n",
      "Train Epoch: 70 [4352/17352 (25%)] Loss: -257135.906250\n",
      "Train Epoch: 70 [5760/17352 (33%)] Loss: -255320.468750\n",
      "Train Epoch: 70 [7168/17352 (41%)] Loss: -251958.515625\n",
      "Train Epoch: 70 [8576/17352 (49%)] Loss: -272434.250000\n",
      "Train Epoch: 70 [9984/17352 (58%)] Loss: -274348.562500\n",
      "Train Epoch: 70 [11392/17352 (66%)] Loss: -246700.578125\n",
      "Train Epoch: 70 [12800/17352 (74%)] Loss: -269184.687500\n",
      "Train Epoch: 70 [14208/17352 (82%)] Loss: -251897.781250\n",
      "Train Epoch: 70 [15451/17352 (89%)] Loss: -14841.548828\n",
      "Train Epoch: 70 [16177/17352 (93%)] Loss: -160026.671875\n",
      "Train Epoch: 70 [16965/17352 (98%)] Loss: -237427.656250\n",
      "    epoch          : 70\n",
      "    loss           : -233549.34913747903\n",
      "    val_loss       : -131030.37384847006\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch70.pth ...\n",
      "Train Epoch: 71 [128/17352 (1%)] Loss: -268765.906250\n",
      "Train Epoch: 71 [1536/17352 (9%)] Loss: -253153.531250\n",
      "Train Epoch: 71 [2944/17352 (17%)] Loss: -274228.406250\n",
      "Train Epoch: 71 [4352/17352 (25%)] Loss: -279079.625000\n",
      "Train Epoch: 71 [5760/17352 (33%)] Loss: -272728.562500\n",
      "Train Epoch: 71 [7168/17352 (41%)] Loss: -219114.500000\n",
      "Train Epoch: 71 [8576/17352 (49%)] Loss: -276583.562500\n",
      "Train Epoch: 71 [9984/17352 (58%)] Loss: -289181.250000\n",
      "Train Epoch: 71 [11392/17352 (66%)] Loss: -271167.312500\n",
      "Train Epoch: 71 [12800/17352 (74%)] Loss: -277153.500000\n",
      "Train Epoch: 71 [14208/17352 (82%)] Loss: -275850.031250\n",
      "Train Epoch: 71 [15455/17352 (89%)] Loss: -114389.445312\n",
      "Train Epoch: 71 [16251/17352 (94%)] Loss: -147409.875000\n",
      "Train Epoch: 71 [16870/17352 (97%)] Loss: -61560.753906\n",
      "    epoch          : 71\n",
      "    loss           : -239509.8032914744\n",
      "    val_loss       : -133830.30845133463\n",
      "Train Epoch: 72 [128/17352 (1%)] Loss: -261402.125000\n",
      "Train Epoch: 72 [1536/17352 (9%)] Loss: -275057.750000\n",
      "Train Epoch: 72 [2944/17352 (17%)] Loss: -223522.406250\n",
      "Train Epoch: 72 [4352/17352 (25%)] Loss: -195145.937500\n",
      "Train Epoch: 72 [5760/17352 (33%)] Loss: -281458.625000\n",
      "Train Epoch: 72 [7168/17352 (41%)] Loss: -290307.562500\n",
      "Train Epoch: 72 [8576/17352 (49%)] Loss: -114155.367188\n",
      "Train Epoch: 72 [9984/17352 (58%)] Loss: -281754.156250\n",
      "Train Epoch: 72 [11392/17352 (66%)] Loss: -269928.312500\n",
      "Train Epoch: 72 [12800/17352 (74%)] Loss: -277786.156250\n",
      "Train Epoch: 72 [14208/17352 (82%)] Loss: -246805.656250\n",
      "Train Epoch: 72 [15453/17352 (89%)] Loss: -96577.851562\n",
      "Train Epoch: 72 [16177/17352 (93%)] Loss: -207165.906250\n",
      "Train Epoch: 72 [16969/17352 (98%)] Loss: -150268.890625\n",
      "    epoch          : 72\n",
      "    loss           : -239554.30355363883\n",
      "    val_loss       : -134905.27786458333\n",
      "Train Epoch: 73 [128/17352 (1%)] Loss: -263025.437500\n",
      "Train Epoch: 73 [1536/17352 (9%)] Loss: -301571.593750\n",
      "Train Epoch: 73 [2944/17352 (17%)] Loss: -165119.593750\n",
      "Train Epoch: 73 [4352/17352 (25%)] Loss: -230440.796875\n",
      "Train Epoch: 73 [5760/17352 (33%)] Loss: -244284.859375\n",
      "Train Epoch: 73 [7168/17352 (41%)] Loss: -271279.656250\n",
      "Train Epoch: 73 [8576/17352 (49%)] Loss: -280542.750000\n",
      "Train Epoch: 73 [9984/17352 (58%)] Loss: -292650.812500\n",
      "Train Epoch: 73 [11392/17352 (66%)] Loss: -286958.781250\n",
      "Train Epoch: 73 [12800/17352 (74%)] Loss: -269386.312500\n",
      "Train Epoch: 73 [14208/17352 (82%)] Loss: -257430.375000\n",
      "Train Epoch: 73 [15491/17352 (89%)] Loss: -109339.695312\n",
      "Train Epoch: 73 [16176/17352 (93%)] Loss: -4700.982910\n",
      "Train Epoch: 73 [17011/17352 (98%)] Loss: -76051.000000\n",
      "    epoch          : 73\n",
      "    loss           : -242585.02728148593\n",
      "    val_loss       : -137072.39763183595\n",
      "Train Epoch: 74 [128/17352 (1%)] Loss: -258841.687500\n",
      "Train Epoch: 74 [1536/17352 (9%)] Loss: -288687.812500\n",
      "Train Epoch: 74 [2944/17352 (17%)] Loss: -284025.656250\n",
      "Train Epoch: 74 [4352/17352 (25%)] Loss: -282440.812500\n",
      "Train Epoch: 74 [5760/17352 (33%)] Loss: -252250.593750\n",
      "Train Epoch: 74 [7168/17352 (41%)] Loss: -289453.937500\n",
      "Train Epoch: 74 [8576/17352 (49%)] Loss: -260677.875000\n",
      "Train Epoch: 74 [9984/17352 (58%)] Loss: -293225.281250\n",
      "Train Epoch: 74 [11392/17352 (66%)] Loss: -265797.562500\n",
      "Train Epoch: 74 [12800/17352 (74%)] Loss: -254520.328125\n",
      "Train Epoch: 74 [14208/17352 (82%)] Loss: -276032.062500\n",
      "Train Epoch: 74 [15418/17352 (89%)] Loss: -95780.343750\n",
      "Train Epoch: 74 [16196/17352 (93%)] Loss: -150279.468750\n",
      "Train Epoch: 74 [17039/17352 (98%)] Loss: -210464.328125\n",
      "    epoch          : 74\n",
      "    loss           : -252626.05322265625\n",
      "    val_loss       : -136828.55686442056\n",
      "Train Epoch: 75 [128/17352 (1%)] Loss: -285570.000000\n",
      "Train Epoch: 75 [1536/17352 (9%)] Loss: -272636.281250\n",
      "Train Epoch: 75 [2944/17352 (17%)] Loss: -298465.875000\n",
      "Train Epoch: 75 [4352/17352 (25%)] Loss: -287698.156250\n",
      "Train Epoch: 75 [5760/17352 (33%)] Loss: -306487.437500\n",
      "Train Epoch: 75 [7168/17352 (41%)] Loss: -267645.562500\n",
      "Train Epoch: 75 [8576/17352 (49%)] Loss: -292402.250000\n",
      "Train Epoch: 75 [9984/17352 (58%)] Loss: -275552.531250\n",
      "Train Epoch: 75 [11392/17352 (66%)] Loss: -268828.281250\n",
      "Train Epoch: 75 [12800/17352 (74%)] Loss: -283112.562500\n",
      "Train Epoch: 75 [14208/17352 (82%)] Loss: -251930.968750\n",
      "Train Epoch: 75 [15468/17352 (89%)] Loss: -215205.546875\n",
      "Train Epoch: 75 [16201/17352 (93%)] Loss: -10158.813477\n",
      "Train Epoch: 75 [16903/17352 (97%)] Loss: -112957.312500\n",
      "    epoch          : 75\n",
      "    loss           : -250425.19601378986\n",
      "    val_loss       : -142187.03818766275\n",
      "Train Epoch: 76 [128/17352 (1%)] Loss: -275410.687500\n",
      "Train Epoch: 76 [1536/17352 (9%)] Loss: -310105.406250\n",
      "Train Epoch: 76 [2944/17352 (17%)] Loss: -296175.250000\n",
      "Train Epoch: 76 [4352/17352 (25%)] Loss: -277177.375000\n",
      "Train Epoch: 76 [5760/17352 (33%)] Loss: -302758.312500\n",
      "Train Epoch: 76 [7168/17352 (41%)] Loss: -277513.468750\n",
      "Train Epoch: 76 [8576/17352 (49%)] Loss: -289235.187500\n",
      "Train Epoch: 76 [9984/17352 (58%)] Loss: -282005.781250\n",
      "Train Epoch: 76 [11392/17352 (66%)] Loss: -303457.937500\n",
      "Train Epoch: 76 [12800/17352 (74%)] Loss: -232881.406250\n",
      "Train Epoch: 76 [14208/17352 (82%)] Loss: -294772.625000\n",
      "Train Epoch: 76 [15474/17352 (89%)] Loss: -235812.593750\n",
      "Train Epoch: 76 [16154/17352 (93%)] Loss: -251919.625000\n",
      "Train Epoch: 76 [16987/17352 (98%)] Loss: -178363.984375\n",
      "    epoch          : 76\n",
      "    loss           : -251890.0230049287\n",
      "    val_loss       : -142386.39661458333\n",
      "Train Epoch: 77 [128/17352 (1%)] Loss: -299097.250000\n",
      "Train Epoch: 77 [1536/17352 (9%)] Loss: -283163.875000\n",
      "Train Epoch: 77 [2944/17352 (17%)] Loss: -260127.218750\n",
      "Train Epoch: 77 [4352/17352 (25%)] Loss: -248544.250000\n",
      "Train Epoch: 77 [5760/17352 (33%)] Loss: -321913.156250\n",
      "Train Epoch: 77 [7168/17352 (41%)] Loss: -292089.625000\n",
      "Train Epoch: 77 [8576/17352 (49%)] Loss: -295165.687500\n",
      "Train Epoch: 77 [9984/17352 (58%)] Loss: -294732.125000\n",
      "Train Epoch: 77 [11392/17352 (66%)] Loss: -256572.234375\n",
      "Train Epoch: 77 [12800/17352 (74%)] Loss: -282369.281250\n",
      "Train Epoch: 77 [14208/17352 (82%)] Loss: -266958.531250\n",
      "Train Epoch: 77 [15521/17352 (89%)] Loss: -154163.234375\n",
      "Train Epoch: 77 [16280/17352 (94%)] Loss: -210453.968750\n",
      "Train Epoch: 77 [17131/17352 (99%)] Loss: -105161.179688\n",
      "    epoch          : 77\n",
      "    loss           : -258486.54854629823\n",
      "    val_loss       : -146118.74632161457\n",
      "Train Epoch: 78 [128/17352 (1%)] Loss: -295576.375000\n",
      "Train Epoch: 78 [1536/17352 (9%)] Loss: -309848.656250\n",
      "Train Epoch: 78 [2944/17352 (17%)] Loss: -278213.875000\n",
      "Train Epoch: 78 [4352/17352 (25%)] Loss: -310828.500000\n",
      "Train Epoch: 78 [5760/17352 (33%)] Loss: -255647.187500\n",
      "Train Epoch: 78 [7168/17352 (41%)] Loss: -245960.437500\n",
      "Train Epoch: 78 [8576/17352 (49%)] Loss: -287636.906250\n",
      "Train Epoch: 78 [9984/17352 (58%)] Loss: -293451.406250\n",
      "Train Epoch: 78 [11392/17352 (66%)] Loss: -282111.031250\n",
      "Train Epoch: 78 [12800/17352 (74%)] Loss: -297420.437500\n",
      "Train Epoch: 78 [14208/17352 (82%)] Loss: -285323.843750\n",
      "Train Epoch: 78 [15526/17352 (89%)] Loss: -204184.031250\n",
      "Train Epoch: 78 [16317/17352 (94%)] Loss: -275821.656250\n",
      "Train Epoch: 78 [16957/17352 (98%)] Loss: -7856.967773\n",
      "    epoch          : 78\n",
      "    loss           : -255504.4469051489\n",
      "    val_loss       : -141430.98970540366\n",
      "Train Epoch: 79 [128/17352 (1%)] Loss: -296213.437500\n",
      "Train Epoch: 79 [1536/17352 (9%)] Loss: -255018.093750\n",
      "Train Epoch: 79 [2944/17352 (17%)] Loss: -282923.968750\n",
      "Train Epoch: 79 [4352/17352 (25%)] Loss: -285742.062500\n",
      "Train Epoch: 79 [5760/17352 (33%)] Loss: -279926.343750\n",
      "Train Epoch: 79 [7168/17352 (41%)] Loss: -287482.125000\n",
      "Train Epoch: 79 [8576/17352 (49%)] Loss: -291363.406250\n",
      "Train Epoch: 79 [9984/17352 (58%)] Loss: -291127.718750\n",
      "Train Epoch: 79 [11392/17352 (66%)] Loss: -292731.656250\n",
      "Train Epoch: 79 [12800/17352 (74%)] Loss: -281183.812500\n",
      "Train Epoch: 79 [14208/17352 (82%)] Loss: -315252.531250\n",
      "Train Epoch: 79 [15487/17352 (89%)] Loss: -122232.531250\n",
      "Train Epoch: 79 [16335/17352 (94%)] Loss: -96589.585938\n",
      "Train Epoch: 79 [17043/17352 (98%)] Loss: -227546.187500\n",
      "    epoch          : 79\n",
      "    loss           : -255209.4546095061\n",
      "    val_loss       : -138340.91610514323\n",
      "Train Epoch: 80 [128/17352 (1%)] Loss: -285255.843750\n",
      "Train Epoch: 80 [1536/17352 (9%)] Loss: -277050.218750\n",
      "Train Epoch: 80 [2944/17352 (17%)] Loss: -249729.375000\n",
      "Train Epoch: 80 [4352/17352 (25%)] Loss: -240363.609375\n",
      "Train Epoch: 80 [5760/17352 (33%)] Loss: -239863.656250\n",
      "Train Epoch: 80 [7168/17352 (41%)] Loss: -244994.296875\n",
      "Train Epoch: 80 [8576/17352 (49%)] Loss: -297616.687500\n",
      "Train Epoch: 80 [9984/17352 (58%)] Loss: -292776.625000\n",
      "Train Epoch: 80 [11392/17352 (66%)] Loss: -277529.468750\n",
      "Train Epoch: 80 [12800/17352 (74%)] Loss: -276604.500000\n",
      "Train Epoch: 80 [14208/17352 (82%)] Loss: -276751.406250\n",
      "Train Epoch: 80 [15489/17352 (89%)] Loss: -96710.718750\n",
      "Train Epoch: 80 [16064/17352 (93%)] Loss: -133788.890625\n",
      "Train Epoch: 80 [16949/17352 (98%)] Loss: -96924.804688\n",
      "    epoch          : 80\n",
      "    loss           : -255411.85188365143\n",
      "    val_loss       : -142309.480851237\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch80.pth ...\n",
      "Train Epoch: 81 [128/17352 (1%)] Loss: -307453.312500\n",
      "Train Epoch: 81 [1536/17352 (9%)] Loss: -310025.250000\n",
      "Train Epoch: 81 [2944/17352 (17%)] Loss: -241505.281250\n",
      "Train Epoch: 81 [4352/17352 (25%)] Loss: -297583.125000\n",
      "Train Epoch: 81 [5760/17352 (33%)] Loss: -311163.468750\n",
      "Train Epoch: 81 [7168/17352 (41%)] Loss: -275974.125000\n",
      "Train Epoch: 81 [8576/17352 (49%)] Loss: -242389.703125\n",
      "Train Epoch: 81 [9984/17352 (58%)] Loss: -301630.062500\n",
      "Train Epoch: 81 [11392/17352 (66%)] Loss: -305513.875000\n",
      "Train Epoch: 81 [12800/17352 (74%)] Loss: -299526.875000\n",
      "Train Epoch: 81 [14208/17352 (82%)] Loss: -284245.281250\n",
      "Train Epoch: 81 [15523/17352 (89%)] Loss: -190552.734375\n",
      "Train Epoch: 81 [16464/17352 (95%)] Loss: -169533.875000\n",
      "Train Epoch: 81 [17168/17352 (99%)] Loss: -236635.781250\n",
      "    epoch          : 81\n",
      "    loss           : -254454.63276989828\n",
      "    val_loss       : -149049.8433268229\n",
      "Train Epoch: 82 [128/17352 (1%)] Loss: -291742.625000\n",
      "Train Epoch: 82 [1536/17352 (9%)] Loss: -288285.937500\n",
      "Train Epoch: 82 [2944/17352 (17%)] Loss: -307375.375000\n",
      "Train Epoch: 82 [4352/17352 (25%)] Loss: -272450.093750\n",
      "Train Epoch: 82 [5760/17352 (33%)] Loss: -304945.843750\n",
      "Train Epoch: 82 [7168/17352 (41%)] Loss: -307199.437500\n",
      "Train Epoch: 82 [8576/17352 (49%)] Loss: -305973.750000\n",
      "Train Epoch: 82 [9984/17352 (58%)] Loss: -268005.406250\n",
      "Train Epoch: 82 [11392/17352 (66%)] Loss: -278875.312500\n",
      "Train Epoch: 82 [12800/17352 (74%)] Loss: -304273.625000\n",
      "Train Epoch: 82 [14208/17352 (82%)] Loss: -313026.218750\n",
      "Train Epoch: 82 [15505/17352 (89%)] Loss: -204620.406250\n",
      "Train Epoch: 82 [16261/17352 (94%)] Loss: -11678.703125\n",
      "Train Epoch: 82 [16958/17352 (98%)] Loss: -26546.867188\n",
      "    epoch          : 82\n",
      "    loss           : -261262.36838034814\n",
      "    val_loss       : -147095.9276204427\n",
      "Train Epoch: 83 [128/17352 (1%)] Loss: -314865.968750\n",
      "Train Epoch: 83 [1536/17352 (9%)] Loss: -276459.625000\n",
      "Train Epoch: 83 [2944/17352 (17%)] Loss: -295095.187500\n",
      "Train Epoch: 83 [4352/17352 (25%)] Loss: -292916.718750\n",
      "Train Epoch: 83 [5760/17352 (33%)] Loss: -299097.406250\n",
      "Train Epoch: 83 [7168/17352 (41%)] Loss: -288959.500000\n",
      "Train Epoch: 83 [8576/17352 (49%)] Loss: -184533.687500\n",
      "Train Epoch: 83 [9984/17352 (58%)] Loss: -308839.593750\n",
      "Train Epoch: 83 [11392/17352 (66%)] Loss: -277266.812500\n",
      "Train Epoch: 83 [12800/17352 (74%)] Loss: -311906.343750\n",
      "Train Epoch: 83 [14208/17352 (82%)] Loss: -273327.750000\n",
      "Train Epoch: 83 [15521/17352 (89%)] Loss: -209102.437500\n",
      "Train Epoch: 83 [16264/17352 (94%)] Loss: -164307.125000\n",
      "Train Epoch: 83 [17031/17352 (98%)] Loss: -203152.781250\n",
      "    epoch          : 83\n",
      "    loss           : -262372.59970768663\n",
      "    val_loss       : -139096.32211914062\n",
      "Train Epoch: 84 [128/17352 (1%)] Loss: -292999.031250\n",
      "Train Epoch: 84 [1536/17352 (9%)] Loss: -293629.187500\n",
      "Train Epoch: 84 [2944/17352 (17%)] Loss: -292699.906250\n",
      "Train Epoch: 84 [4352/17352 (25%)] Loss: -318650.062500\n",
      "Train Epoch: 84 [5760/17352 (33%)] Loss: -262947.468750\n",
      "Train Epoch: 84 [7168/17352 (41%)] Loss: -282938.593750\n",
      "Train Epoch: 84 [8576/17352 (49%)] Loss: -309090.812500\n",
      "Train Epoch: 84 [9984/17352 (58%)] Loss: -221344.640625\n",
      "Train Epoch: 84 [11392/17352 (66%)] Loss: -295565.781250\n",
      "Train Epoch: 84 [12800/17352 (74%)] Loss: -290374.281250\n",
      "Train Epoch: 84 [14208/17352 (82%)] Loss: -289437.812500\n",
      "Train Epoch: 84 [15464/17352 (89%)] Loss: -126278.265625\n",
      "Train Epoch: 84 [16234/17352 (94%)] Loss: -74980.703125\n",
      "Train Epoch: 84 [16980/17352 (98%)] Loss: -27656.500000\n",
      "    epoch          : 84\n",
      "    loss           : -263056.2484073511\n",
      "    val_loss       : -143387.1689046224\n",
      "Train Epoch: 85 [128/17352 (1%)] Loss: -284985.718750\n",
      "Train Epoch: 85 [1536/17352 (9%)] Loss: -298556.031250\n",
      "Train Epoch: 85 [2944/17352 (17%)] Loss: -265846.187500\n",
      "Train Epoch: 85 [4352/17352 (25%)] Loss: -269959.750000\n",
      "Train Epoch: 85 [5760/17352 (33%)] Loss: -304872.562500\n",
      "Train Epoch: 85 [7168/17352 (41%)] Loss: -289992.843750\n",
      "Train Epoch: 85 [8576/17352 (49%)] Loss: -293056.531250\n",
      "Train Epoch: 85 [9984/17352 (58%)] Loss: -298971.531250\n",
      "Train Epoch: 85 [11392/17352 (66%)] Loss: -292686.281250\n",
      "Train Epoch: 85 [12800/17352 (74%)] Loss: -270703.906250\n",
      "Train Epoch: 85 [14208/17352 (82%)] Loss: -289281.750000\n",
      "Train Epoch: 85 [15529/17352 (89%)] Loss: -223021.031250\n",
      "Train Epoch: 85 [16382/17352 (94%)] Loss: -187107.156250\n",
      "Train Epoch: 85 [16943/17352 (98%)] Loss: -182976.187500\n",
      "    epoch          : 85\n",
      "    loss           : -262655.20064754615\n",
      "    val_loss       : -145722.68053792318\n",
      "Train Epoch: 86 [128/17352 (1%)] Loss: -298933.812500\n",
      "Train Epoch: 86 [1536/17352 (9%)] Loss: -268871.375000\n",
      "Train Epoch: 86 [2944/17352 (17%)] Loss: -287551.218750\n",
      "Train Epoch: 86 [4352/17352 (25%)] Loss: -289239.875000\n",
      "Train Epoch: 86 [5760/17352 (33%)] Loss: -313932.812500\n",
      "Train Epoch: 86 [7168/17352 (41%)] Loss: -295453.250000\n",
      "Train Epoch: 86 [8576/17352 (49%)] Loss: -297094.312500\n",
      "Train Epoch: 86 [9984/17352 (58%)] Loss: -289192.562500\n",
      "Train Epoch: 86 [11392/17352 (66%)] Loss: -315905.218750\n",
      "Train Epoch: 86 [12800/17352 (74%)] Loss: -284056.281250\n",
      "Train Epoch: 86 [14208/17352 (82%)] Loss: -323624.437500\n",
      "Train Epoch: 86 [15423/17352 (89%)] Loss: -121542.875000\n",
      "Train Epoch: 86 [16234/17352 (94%)] Loss: -193995.796875\n",
      "Train Epoch: 86 [16910/17352 (97%)] Loss: -92880.671875\n",
      "    epoch          : 86\n",
      "    loss           : -263223.33395815856\n",
      "    val_loss       : -145562.63792317707\n",
      "Train Epoch: 87 [128/17352 (1%)] Loss: -306475.187500\n",
      "Train Epoch: 87 [1536/17352 (9%)] Loss: -333210.968750\n",
      "Train Epoch: 87 [2944/17352 (17%)] Loss: -211290.437500\n",
      "Train Epoch: 87 [4352/17352 (25%)] Loss: -321793.250000\n",
      "Train Epoch: 87 [5760/17352 (33%)] Loss: -313435.125000\n",
      "Train Epoch: 87 [7168/17352 (41%)] Loss: -212161.609375\n",
      "Train Epoch: 87 [8576/17352 (49%)] Loss: -283290.625000\n",
      "Train Epoch: 87 [9984/17352 (58%)] Loss: -246433.593750\n",
      "Train Epoch: 87 [11392/17352 (66%)] Loss: -312788.500000\n",
      "Train Epoch: 87 [12800/17352 (74%)] Loss: -303348.000000\n",
      "Train Epoch: 87 [14208/17352 (82%)] Loss: -224686.328125\n",
      "Train Epoch: 87 [15536/17352 (90%)] Loss: -224037.843750\n",
      "Train Epoch: 87 [16430/17352 (95%)] Loss: -242248.750000\n",
      "Train Epoch: 87 [17059/17352 (98%)] Loss: -77166.421875\n",
      "    epoch          : 87\n",
      "    loss           : -267285.540307781\n",
      "    val_loss       : -148077.00690917968\n",
      "Train Epoch: 88 [128/17352 (1%)] Loss: -305789.250000\n",
      "Train Epoch: 88 [1536/17352 (9%)] Loss: -306308.843750\n",
      "Train Epoch: 88 [2944/17352 (17%)] Loss: -341405.343750\n",
      "Train Epoch: 88 [4352/17352 (25%)] Loss: -299822.000000\n",
      "Train Epoch: 88 [5760/17352 (33%)] Loss: -286522.937500\n",
      "Train Epoch: 88 [7168/17352 (41%)] Loss: -318171.500000\n",
      "Train Epoch: 88 [8576/17352 (49%)] Loss: -313749.687500\n",
      "Train Epoch: 88 [9984/17352 (58%)] Loss: -300543.062500\n",
      "Train Epoch: 88 [11392/17352 (66%)] Loss: -274842.062500\n",
      "Train Epoch: 88 [12800/17352 (74%)] Loss: -316528.000000\n",
      "Train Epoch: 88 [14208/17352 (82%)] Loss: -328200.968750\n",
      "Train Epoch: 88 [15474/17352 (89%)] Loss: -18014.140625\n",
      "Train Epoch: 88 [16179/17352 (93%)] Loss: -195242.062500\n",
      "Train Epoch: 88 [16980/17352 (98%)] Loss: -114127.953125\n",
      "    epoch          : 88\n",
      "    loss           : -270181.8203190541\n",
      "    val_loss       : -147917.8057779948\n",
      "Train Epoch: 89 [128/17352 (1%)] Loss: -318275.750000\n",
      "Train Epoch: 89 [1536/17352 (9%)] Loss: -311396.375000\n",
      "Train Epoch: 89 [2944/17352 (17%)] Loss: -306530.531250\n",
      "Train Epoch: 89 [4352/17352 (25%)] Loss: -309102.062500\n",
      "Train Epoch: 89 [5760/17352 (33%)] Loss: -309374.500000\n",
      "Train Epoch: 89 [7168/17352 (41%)] Loss: -318977.687500\n",
      "Train Epoch: 89 [8576/17352 (49%)] Loss: -236584.656250\n",
      "Train Epoch: 89 [9984/17352 (58%)] Loss: -251711.187500\n",
      "Train Epoch: 89 [11392/17352 (66%)] Loss: -260309.562500\n",
      "Train Epoch: 89 [12800/17352 (74%)] Loss: -281707.593750\n",
      "Train Epoch: 89 [14208/17352 (82%)] Loss: -309494.343750\n",
      "Train Epoch: 89 [15521/17352 (89%)] Loss: -304728.812500\n",
      "Train Epoch: 89 [16165/17352 (93%)] Loss: -233740.656250\n",
      "Train Epoch: 89 [17002/17352 (98%)] Loss: -29473.798828\n",
      "    epoch          : 89\n",
      "    loss           : -267539.98185166734\n",
      "    val_loss       : -146372.64370117188\n",
      "Train Epoch: 90 [128/17352 (1%)] Loss: -314214.312500\n",
      "Train Epoch: 90 [1536/17352 (9%)] Loss: -298343.187500\n",
      "Train Epoch: 90 [2944/17352 (17%)] Loss: -312217.062500\n",
      "Train Epoch: 90 [4352/17352 (25%)] Loss: -274707.812500\n",
      "Train Epoch: 90 [5760/17352 (33%)] Loss: -213630.328125\n",
      "Train Epoch: 90 [7168/17352 (41%)] Loss: -288175.937500\n",
      "Train Epoch: 90 [8576/17352 (49%)] Loss: -305448.812500\n",
      "Train Epoch: 90 [9984/17352 (58%)] Loss: -295619.437500\n",
      "Train Epoch: 90 [11392/17352 (66%)] Loss: -294445.000000\n",
      "Train Epoch: 90 [12800/17352 (74%)] Loss: -300262.187500\n",
      "Train Epoch: 90 [14208/17352 (82%)] Loss: -267703.718750\n",
      "Train Epoch: 90 [15499/17352 (89%)] Loss: -204124.218750\n",
      "Train Epoch: 90 [16243/17352 (94%)] Loss: -5853.100098\n",
      "Train Epoch: 90 [17071/17352 (98%)] Loss: -236047.640625\n",
      "    epoch          : 90\n",
      "    loss           : -264338.5686870805\n",
      "    val_loss       : -148946.1831542969\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch90.pth ...\n",
      "Train Epoch: 91 [128/17352 (1%)] Loss: -252677.906250\n",
      "Train Epoch: 91 [1536/17352 (9%)] Loss: -278369.375000\n",
      "Train Epoch: 91 [2944/17352 (17%)] Loss: -311631.375000\n",
      "Train Epoch: 91 [4352/17352 (25%)] Loss: -297346.312500\n",
      "Train Epoch: 91 [5760/17352 (33%)] Loss: -296848.031250\n",
      "Train Epoch: 91 [7168/17352 (41%)] Loss: -312339.500000\n",
      "Train Epoch: 91 [8576/17352 (49%)] Loss: -254061.125000\n",
      "Train Epoch: 91 [9984/17352 (58%)] Loss: -308128.625000\n",
      "Train Epoch: 91 [11392/17352 (66%)] Loss: -279029.312500\n",
      "Train Epoch: 91 [12800/17352 (74%)] Loss: -286549.312500\n",
      "Train Epoch: 91 [14208/17352 (82%)] Loss: -301837.750000\n",
      "Train Epoch: 91 [15479/17352 (89%)] Loss: -67567.593750\n",
      "Train Epoch: 91 [16320/17352 (94%)] Loss: -287745.281250\n",
      "Train Epoch: 91 [17009/17352 (98%)] Loss: -128131.507812\n",
      "    epoch          : 91\n",
      "    loss           : -268043.85794620385\n",
      "    val_loss       : -151339.40165201822\n",
      "Train Epoch: 92 [128/17352 (1%)] Loss: -283665.593750\n",
      "Train Epoch: 92 [1536/17352 (9%)] Loss: -307725.187500\n",
      "Train Epoch: 92 [2944/17352 (17%)] Loss: -273249.125000\n",
      "Train Epoch: 92 [4352/17352 (25%)] Loss: -293957.718750\n",
      "Train Epoch: 92 [5760/17352 (33%)] Loss: -317500.000000\n",
      "Train Epoch: 92 [7168/17352 (41%)] Loss: -258716.343750\n",
      "Train Epoch: 92 [8576/17352 (49%)] Loss: -274817.281250\n",
      "Train Epoch: 92 [9984/17352 (58%)] Loss: -313915.375000\n",
      "Train Epoch: 92 [11392/17352 (66%)] Loss: -281703.375000\n",
      "Train Epoch: 92 [12800/17352 (74%)] Loss: -286057.000000\n",
      "Train Epoch: 92 [14208/17352 (82%)] Loss: -297034.031250\n",
      "Train Epoch: 92 [15376/17352 (89%)] Loss: -26653.136719\n",
      "Train Epoch: 92 [16147/17352 (93%)] Loss: -275496.687500\n",
      "Train Epoch: 92 [16921/17352 (98%)] Loss: -238973.000000\n",
      "    epoch          : 92\n",
      "    loss           : -261836.2633933253\n",
      "    val_loss       : -146393.8405843099\n",
      "Train Epoch: 93 [128/17352 (1%)] Loss: -303477.218750\n",
      "Train Epoch: 93 [1536/17352 (9%)] Loss: -223158.484375\n",
      "Train Epoch: 93 [2944/17352 (17%)] Loss: -310265.562500\n",
      "Train Epoch: 93 [4352/17352 (25%)] Loss: -295452.781250\n",
      "Train Epoch: 93 [5760/17352 (33%)] Loss: -314818.843750\n",
      "Train Epoch: 93 [7168/17352 (41%)] Loss: -311605.625000\n",
      "Train Epoch: 93 [8576/17352 (49%)] Loss: -298531.468750\n",
      "Train Epoch: 93 [9984/17352 (58%)] Loss: -298981.125000\n",
      "Train Epoch: 93 [11392/17352 (66%)] Loss: -322061.218750\n",
      "Train Epoch: 93 [12800/17352 (74%)] Loss: -288119.187500\n",
      "Train Epoch: 93 [14208/17352 (82%)] Loss: -301655.125000\n",
      "Train Epoch: 93 [15440/17352 (89%)] Loss: -167877.171875\n",
      "Train Epoch: 93 [16086/17352 (93%)] Loss: -123383.593750\n",
      "Train Epoch: 93 [16892/17352 (97%)] Loss: -130850.312500\n",
      "    epoch          : 93\n",
      "    loss           : -264483.8949113884\n",
      "    val_loss       : -148294.0411702474\n",
      "Train Epoch: 94 [128/17352 (1%)] Loss: -268885.375000\n",
      "Train Epoch: 94 [1536/17352 (9%)] Loss: -317455.281250\n",
      "Train Epoch: 94 [2944/17352 (17%)] Loss: -316588.593750\n",
      "Train Epoch: 94 [4352/17352 (25%)] Loss: -291745.718750\n",
      "Train Epoch: 94 [5760/17352 (33%)] Loss: -330498.843750\n",
      "Train Epoch: 94 [7168/17352 (41%)] Loss: -260984.125000\n",
      "Train Epoch: 94 [8576/17352 (49%)] Loss: -333205.312500\n",
      "Train Epoch: 94 [9984/17352 (58%)] Loss: -304471.250000\n",
      "Train Epoch: 94 [11392/17352 (66%)] Loss: -323937.718750\n",
      "Train Epoch: 94 [12800/17352 (74%)] Loss: -271682.500000\n",
      "Train Epoch: 94 [14208/17352 (82%)] Loss: -270066.281250\n",
      "Train Epoch: 94 [15523/17352 (89%)] Loss: -168615.859375\n",
      "Train Epoch: 94 [16265/17352 (94%)] Loss: -5411.611816\n",
      "Train Epoch: 94 [17053/17352 (98%)] Loss: -256669.812500\n",
      "    epoch          : 94\n",
      "    loss           : -271921.9085406617\n",
      "    val_loss       : -146155.6539876302\n",
      "Train Epoch: 95 [128/17352 (1%)] Loss: -258511.484375\n",
      "Train Epoch: 95 [1536/17352 (9%)] Loss: -295878.906250\n",
      "Train Epoch: 95 [2944/17352 (17%)] Loss: -333225.968750\n",
      "Train Epoch: 95 [4352/17352 (25%)] Loss: -315391.437500\n",
      "Train Epoch: 95 [5760/17352 (33%)] Loss: -318510.500000\n",
      "Train Epoch: 95 [7168/17352 (41%)] Loss: -312546.468750\n",
      "Train Epoch: 95 [8576/17352 (49%)] Loss: -317434.437500\n",
      "Train Epoch: 95 [9984/17352 (58%)] Loss: -285030.187500\n",
      "Train Epoch: 95 [11392/17352 (66%)] Loss: -287985.062500\n",
      "Train Epoch: 95 [12800/17352 (74%)] Loss: -320565.281250\n",
      "Train Epoch: 95 [14208/17352 (82%)] Loss: -292029.406250\n",
      "Train Epoch: 95 [15449/17352 (89%)] Loss: -5866.622070\n",
      "Train Epoch: 95 [16290/17352 (94%)] Loss: -212328.343750\n",
      "Train Epoch: 95 [16985/17352 (98%)] Loss: -128105.234375\n",
      "    epoch          : 95\n",
      "    loss           : -268713.3554294253\n",
      "    val_loss       : -151481.91539713542\n",
      "Train Epoch: 96 [128/17352 (1%)] Loss: -316065.875000\n",
      "Train Epoch: 96 [1536/17352 (9%)] Loss: -312010.218750\n",
      "Train Epoch: 96 [2944/17352 (17%)] Loss: -287364.312500\n",
      "Train Epoch: 96 [4352/17352 (25%)] Loss: -288411.343750\n",
      "Train Epoch: 96 [5760/17352 (33%)] Loss: -324136.843750\n",
      "Train Epoch: 96 [7168/17352 (41%)] Loss: -304776.750000\n",
      "Train Epoch: 96 [8576/17352 (49%)] Loss: -310397.781250\n",
      "Train Epoch: 96 [9984/17352 (58%)] Loss: -266418.875000\n",
      "Train Epoch: 96 [11392/17352 (66%)] Loss: -307536.531250\n",
      "Train Epoch: 96 [12800/17352 (74%)] Loss: -331248.406250\n",
      "Train Epoch: 96 [14208/17352 (82%)] Loss: -321775.156250\n",
      "Train Epoch: 96 [15528/17352 (89%)] Loss: -102579.820312\n",
      "Train Epoch: 96 [16240/17352 (94%)] Loss: -266732.468750\n",
      "Train Epoch: 96 [17106/17352 (99%)] Loss: -82449.750000\n",
      "    epoch          : 96\n",
      "    loss           : -270459.493193556\n",
      "    val_loss       : -146605.03323567708\n",
      "Train Epoch: 97 [128/17352 (1%)] Loss: -301763.250000\n",
      "Train Epoch: 97 [1536/17352 (9%)] Loss: -152595.812500\n",
      "Train Epoch: 97 [2944/17352 (17%)] Loss: -253560.609375\n",
      "Train Epoch: 97 [4352/17352 (25%)] Loss: -317472.562500\n",
      "Train Epoch: 97 [5760/17352 (33%)] Loss: -270020.312500\n",
      "Train Epoch: 97 [7168/17352 (41%)] Loss: -307163.625000\n",
      "Train Epoch: 97 [8576/17352 (49%)] Loss: -278318.875000\n",
      "Train Epoch: 97 [9984/17352 (58%)] Loss: -223988.718750\n",
      "Train Epoch: 97 [11392/17352 (66%)] Loss: -315688.281250\n",
      "Train Epoch: 97 [12800/17352 (74%)] Loss: -319078.687500\n",
      "Train Epoch: 97 [14208/17352 (82%)] Loss: -316671.562500\n",
      "Train Epoch: 97 [15487/17352 (89%)] Loss: -191972.062500\n",
      "Train Epoch: 97 [16286/17352 (94%)] Loss: -180474.953125\n",
      "Train Epoch: 97 [16953/17352 (98%)] Loss: -224277.578125\n",
      "    epoch          : 97\n",
      "    loss           : -271310.8299404887\n",
      "    val_loss       : -151288.73234049478\n",
      "Train Epoch: 98 [128/17352 (1%)] Loss: -319689.562500\n",
      "Train Epoch: 98 [1536/17352 (9%)] Loss: -305673.875000\n",
      "Train Epoch: 98 [2944/17352 (17%)] Loss: -302432.437500\n",
      "Train Epoch: 98 [4352/17352 (25%)] Loss: -327593.250000\n",
      "Train Epoch: 98 [5760/17352 (33%)] Loss: -272345.250000\n",
      "Train Epoch: 98 [7168/17352 (41%)] Loss: -306225.562500\n",
      "Train Epoch: 98 [8576/17352 (49%)] Loss: -331521.750000\n",
      "Train Epoch: 98 [9984/17352 (58%)] Loss: -308392.250000\n",
      "Train Epoch: 98 [11392/17352 (66%)] Loss: -311821.750000\n",
      "Train Epoch: 98 [12800/17352 (74%)] Loss: -285926.375000\n",
      "Train Epoch: 98 [14208/17352 (82%)] Loss: -283202.937500\n",
      "Train Epoch: 98 [15523/17352 (89%)] Loss: -223381.828125\n",
      "Train Epoch: 98 [16335/17352 (94%)] Loss: -223988.328125\n",
      "Train Epoch: 98 [16962/17352 (98%)] Loss: -189090.718750\n",
      "    epoch          : 98\n",
      "    loss           : -275266.1068844379\n",
      "    val_loss       : -151216.32347005207\n",
      "Train Epoch: 99 [128/17352 (1%)] Loss: -315347.312500\n",
      "Train Epoch: 99 [1536/17352 (9%)] Loss: -319833.531250\n",
      "Train Epoch: 99 [2944/17352 (17%)] Loss: -311069.062500\n",
      "Train Epoch: 99 [4352/17352 (25%)] Loss: -316755.500000\n",
      "Train Epoch: 99 [5760/17352 (33%)] Loss: -306268.406250\n",
      "Train Epoch: 99 [7168/17352 (41%)] Loss: -272537.437500\n",
      "Train Epoch: 99 [8576/17352 (49%)] Loss: -339907.437500\n",
      "Train Epoch: 99 [9984/17352 (58%)] Loss: -325469.906250\n",
      "Train Epoch: 99 [11392/17352 (66%)] Loss: -306897.781250\n",
      "Train Epoch: 99 [12800/17352 (74%)] Loss: -278872.593750\n",
      "Train Epoch: 99 [14208/17352 (82%)] Loss: -315511.812500\n",
      "Train Epoch: 99 [15546/17352 (90%)] Loss: -224978.531250\n",
      "Train Epoch: 99 [16252/17352 (94%)] Loss: -253646.906250\n",
      "Train Epoch: 99 [17012/17352 (98%)] Loss: -11753.843750\n",
      "    epoch          : 99\n",
      "    loss           : -273455.9966377412\n",
      "    val_loss       : -149467.16774088543\n",
      "Train Epoch: 100 [128/17352 (1%)] Loss: -276166.562500\n",
      "Train Epoch: 100 [1536/17352 (9%)] Loss: -301127.156250\n",
      "Train Epoch: 100 [2944/17352 (17%)] Loss: -314884.968750\n",
      "Train Epoch: 100 [4352/17352 (25%)] Loss: -317767.281250\n",
      "Train Epoch: 100 [5760/17352 (33%)] Loss: -315457.843750\n",
      "Train Epoch: 100 [7168/17352 (41%)] Loss: -234373.625000\n",
      "Train Epoch: 100 [8576/17352 (49%)] Loss: -316321.343750\n",
      "Train Epoch: 100 [9984/17352 (58%)] Loss: -325445.500000\n",
      "Train Epoch: 100 [11392/17352 (66%)] Loss: -285105.281250\n",
      "Train Epoch: 100 [12800/17352 (74%)] Loss: -321839.250000\n",
      "Train Epoch: 100 [14208/17352 (82%)] Loss: -287887.281250\n",
      "Train Epoch: 100 [15499/17352 (89%)] Loss: -72002.960938\n",
      "Train Epoch: 100 [16260/17352 (94%)] Loss: -180078.125000\n",
      "Train Epoch: 100 [16965/17352 (98%)] Loss: -5545.261230\n",
      "    epoch          : 100\n",
      "    loss           : -275759.1765644662\n",
      "    val_loss       : -154100.1167154948\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 101 [128/17352 (1%)] Loss: -329456.531250\n",
      "Train Epoch: 101 [1536/17352 (9%)] Loss: -292666.000000\n",
      "Train Epoch: 101 [2944/17352 (17%)] Loss: -150616.156250\n",
      "Train Epoch: 101 [4352/17352 (25%)] Loss: -305147.875000\n",
      "Train Epoch: 101 [5760/17352 (33%)] Loss: -318812.500000\n",
      "Train Epoch: 101 [7168/17352 (41%)] Loss: -291534.812500\n",
      "Train Epoch: 101 [8576/17352 (49%)] Loss: -313124.375000\n",
      "Train Epoch: 101 [9984/17352 (58%)] Loss: -287914.718750\n",
      "Train Epoch: 101 [11392/17352 (66%)] Loss: -317360.437500\n",
      "Train Epoch: 101 [12800/17352 (74%)] Loss: -321070.000000\n",
      "Train Epoch: 101 [14208/17352 (82%)] Loss: -290533.406250\n",
      "Train Epoch: 101 [15471/17352 (89%)] Loss: -83543.984375\n",
      "Train Epoch: 101 [16282/17352 (94%)] Loss: -243833.687500\n",
      "Train Epoch: 101 [16917/17352 (97%)] Loss: -93852.421875\n",
      "    epoch          : 101\n",
      "    loss           : -277357.95924653945\n",
      "    val_loss       : -151963.27897135416\n",
      "Train Epoch: 102 [128/17352 (1%)] Loss: -325365.093750\n",
      "Train Epoch: 102 [1536/17352 (9%)] Loss: -333804.625000\n",
      "Train Epoch: 102 [2944/17352 (17%)] Loss: -335164.500000\n",
      "Train Epoch: 102 [4352/17352 (25%)] Loss: -315069.906250\n",
      "Train Epoch: 102 [5760/17352 (33%)] Loss: -313866.906250\n",
      "Train Epoch: 102 [7168/17352 (41%)] Loss: -313462.875000\n",
      "Train Epoch: 102 [8576/17352 (49%)] Loss: -306421.812500\n",
      "Train Epoch: 102 [9984/17352 (58%)] Loss: -339509.312500\n",
      "Train Epoch: 102 [11392/17352 (66%)] Loss: -287902.625000\n",
      "Train Epoch: 102 [12800/17352 (74%)] Loss: -321631.312500\n",
      "Train Epoch: 102 [14208/17352 (82%)] Loss: -322385.281250\n",
      "Train Epoch: 102 [15555/17352 (90%)] Loss: -224152.203125\n",
      "Train Epoch: 102 [16368/17352 (94%)] Loss: -244474.656250\n",
      "Train Epoch: 102 [17005/17352 (98%)] Loss: -255504.890625\n",
      "    epoch          : 102\n",
      "    loss           : -279200.5962602244\n",
      "    val_loss       : -154159.86107584636\n",
      "Train Epoch: 103 [128/17352 (1%)] Loss: -305932.968750\n",
      "Train Epoch: 103 [1536/17352 (9%)] Loss: -304469.562500\n",
      "Train Epoch: 103 [2944/17352 (17%)] Loss: -291669.250000\n",
      "Train Epoch: 103 [4352/17352 (25%)] Loss: -324668.875000\n",
      "Train Epoch: 103 [5760/17352 (33%)] Loss: -318798.500000\n",
      "Train Epoch: 103 [7168/17352 (41%)] Loss: -332800.781250\n",
      "Train Epoch: 103 [8576/17352 (49%)] Loss: -298250.750000\n",
      "Train Epoch: 103 [9984/17352 (58%)] Loss: -321855.531250\n",
      "Train Epoch: 103 [11392/17352 (66%)] Loss: -322979.187500\n",
      "Train Epoch: 103 [12800/17352 (74%)] Loss: -309323.687500\n",
      "Train Epoch: 103 [14208/17352 (82%)] Loss: -291987.250000\n",
      "Train Epoch: 103 [15521/17352 (89%)] Loss: -322138.125000\n",
      "Train Epoch: 103 [16389/17352 (94%)] Loss: -222330.250000\n",
      "Train Epoch: 103 [17020/17352 (98%)] Loss: -12049.266602\n",
      "    epoch          : 103\n",
      "    loss           : -281296.76180395344\n",
      "    val_loss       : -155602.10039876302\n",
      "Train Epoch: 104 [128/17352 (1%)] Loss: -336689.718750\n",
      "Train Epoch: 104 [1536/17352 (9%)] Loss: -315427.625000\n",
      "Train Epoch: 104 [2944/17352 (17%)] Loss: -299072.750000\n",
      "Train Epoch: 104 [4352/17352 (25%)] Loss: -335039.750000\n",
      "Train Epoch: 104 [5760/17352 (33%)] Loss: -309172.687500\n",
      "Train Epoch: 104 [7168/17352 (41%)] Loss: -325149.500000\n",
      "Train Epoch: 104 [8576/17352 (49%)] Loss: -331985.812500\n",
      "Train Epoch: 104 [9984/17352 (58%)] Loss: -317192.781250\n",
      "Train Epoch: 104 [11392/17352 (66%)] Loss: -232828.328125\n",
      "Train Epoch: 104 [12800/17352 (74%)] Loss: -291208.187500\n",
      "Train Epoch: 104 [14208/17352 (82%)] Loss: -340313.718750\n",
      "Train Epoch: 104 [15461/17352 (89%)] Loss: -40927.437500\n",
      "Train Epoch: 104 [16177/17352 (93%)] Loss: -117251.195312\n",
      "Train Epoch: 104 [17006/17352 (98%)] Loss: -194141.265625\n",
      "    epoch          : 104\n",
      "    loss           : -277665.390683987\n",
      "    val_loss       : -156354.18444010417\n",
      "Train Epoch: 105 [128/17352 (1%)] Loss: -312878.093750\n",
      "Train Epoch: 105 [1536/17352 (9%)] Loss: -315672.500000\n",
      "Train Epoch: 105 [2944/17352 (17%)] Loss: -326348.750000\n",
      "Train Epoch: 105 [4352/17352 (25%)] Loss: -309434.187500\n",
      "Train Epoch: 105 [5760/17352 (33%)] Loss: -214243.125000\n",
      "Train Epoch: 105 [7168/17352 (41%)] Loss: -319048.593750\n",
      "Train Epoch: 105 [8576/17352 (49%)] Loss: -298816.062500\n",
      "Train Epoch: 105 [9984/17352 (58%)] Loss: -308019.875000\n",
      "Train Epoch: 105 [11392/17352 (66%)] Loss: -331600.156250\n",
      "Train Epoch: 105 [12800/17352 (74%)] Loss: -350755.875000\n",
      "Train Epoch: 105 [14208/17352 (82%)] Loss: -235117.390625\n",
      "Train Epoch: 105 [15397/17352 (89%)] Loss: -73611.007812\n",
      "Train Epoch: 105 [16225/17352 (94%)] Loss: -289008.968750\n",
      "Train Epoch: 105 [16999/17352 (98%)] Loss: -159350.062500\n",
      "    epoch          : 105\n",
      "    loss           : -282578.0211796088\n",
      "    val_loss       : -156019.49436035156\n",
      "Train Epoch: 106 [128/17352 (1%)] Loss: -316993.093750\n",
      "Train Epoch: 106 [1536/17352 (9%)] Loss: -335181.750000\n",
      "Train Epoch: 106 [2944/17352 (17%)] Loss: -326120.031250\n",
      "Train Epoch: 106 [4352/17352 (25%)] Loss: -290197.125000\n",
      "Train Epoch: 106 [5760/17352 (33%)] Loss: -318473.468750\n",
      "Train Epoch: 106 [7168/17352 (41%)] Loss: -273782.312500\n",
      "Train Epoch: 106 [8576/17352 (49%)] Loss: -322622.937500\n",
      "Train Epoch: 106 [9984/17352 (58%)] Loss: -316974.750000\n",
      "Train Epoch: 106 [11392/17352 (66%)] Loss: -300041.000000\n",
      "Train Epoch: 106 [12800/17352 (74%)] Loss: -324800.562500\n",
      "Train Epoch: 106 [14208/17352 (82%)] Loss: -311312.125000\n",
      "Train Epoch: 106 [15479/17352 (89%)] Loss: -195392.656250\n",
      "Train Epoch: 106 [16316/17352 (94%)] Loss: -293905.375000\n",
      "Train Epoch: 106 [17024/17352 (98%)] Loss: -207596.500000\n",
      "    epoch          : 106\n",
      "    loss           : -285917.6821616768\n",
      "    val_loss       : -160012.29204101564\n",
      "Train Epoch: 107 [128/17352 (1%)] Loss: -296951.000000\n",
      "Train Epoch: 107 [1536/17352 (9%)] Loss: -318561.625000\n",
      "Train Epoch: 107 [2944/17352 (17%)] Loss: -331478.187500\n",
      "Train Epoch: 107 [4352/17352 (25%)] Loss: -241886.187500\n",
      "Train Epoch: 107 [5760/17352 (33%)] Loss: -298396.468750\n",
      "Train Epoch: 107 [7168/17352 (41%)] Loss: -327496.531250\n",
      "Train Epoch: 107 [8576/17352 (49%)] Loss: -341811.875000\n",
      "Train Epoch: 107 [9984/17352 (58%)] Loss: -320818.968750\n",
      "Train Epoch: 107 [11392/17352 (66%)] Loss: -321668.875000\n",
      "Train Epoch: 107 [12800/17352 (74%)] Loss: -332918.562500\n",
      "Train Epoch: 107 [14208/17352 (82%)] Loss: -348532.406250\n",
      "Train Epoch: 107 [15489/17352 (89%)] Loss: -102121.109375\n",
      "Train Epoch: 107 [16346/17352 (94%)] Loss: -169701.000000\n",
      "Train Epoch: 107 [17038/17352 (98%)] Loss: -70537.343750\n",
      "    epoch          : 107\n",
      "    loss           : -285109.17102624266\n",
      "    val_loss       : -162181.49991861978\n",
      "Train Epoch: 108 [128/17352 (1%)] Loss: -243541.781250\n",
      "Train Epoch: 108 [1536/17352 (9%)] Loss: -315361.187500\n",
      "Train Epoch: 108 [2944/17352 (17%)] Loss: -280368.218750\n",
      "Train Epoch: 108 [4352/17352 (25%)] Loss: -348358.406250\n",
      "Train Epoch: 108 [5760/17352 (33%)] Loss: -315102.375000\n",
      "Train Epoch: 108 [7168/17352 (41%)] Loss: -327457.625000\n",
      "Train Epoch: 108 [8576/17352 (49%)] Loss: -335051.093750\n",
      "Train Epoch: 108 [9984/17352 (58%)] Loss: -337279.875000\n",
      "Train Epoch: 108 [11392/17352 (66%)] Loss: -323106.625000\n",
      "Train Epoch: 108 [12800/17352 (74%)] Loss: -329662.250000\n",
      "Train Epoch: 108 [14208/17352 (82%)] Loss: -313631.187500\n",
      "Train Epoch: 108 [15466/17352 (89%)] Loss: -14042.427734\n",
      "Train Epoch: 108 [16126/17352 (93%)] Loss: -129484.281250\n",
      "Train Epoch: 108 [16992/17352 (98%)] Loss: -215918.140625\n",
      "    epoch          : 108\n",
      "    loss           : -289308.75498112413\n",
      "    val_loss       : -157675.12405598958\n",
      "Train Epoch: 109 [128/17352 (1%)] Loss: -324003.750000\n",
      "Train Epoch: 109 [1536/17352 (9%)] Loss: -327496.062500\n",
      "Train Epoch: 109 [2944/17352 (17%)] Loss: -349428.062500\n",
      "Train Epoch: 109 [4352/17352 (25%)] Loss: -309808.375000\n",
      "Train Epoch: 109 [5760/17352 (33%)] Loss: -339449.843750\n",
      "Train Epoch: 109 [7168/17352 (41%)] Loss: -322947.593750\n",
      "Train Epoch: 109 [8576/17352 (49%)] Loss: -335297.312500\n",
      "Train Epoch: 109 [9984/17352 (58%)] Loss: -313429.531250\n",
      "Train Epoch: 109 [11392/17352 (66%)] Loss: -309259.500000\n",
      "Train Epoch: 109 [12800/17352 (74%)] Loss: -233070.687500\n",
      "Train Epoch: 109 [14208/17352 (82%)] Loss: -302423.781250\n",
      "Train Epoch: 109 [15525/17352 (89%)] Loss: -187308.078125\n",
      "Train Epoch: 109 [16176/17352 (93%)] Loss: -203542.484375\n",
      "Train Epoch: 109 [16966/17352 (98%)] Loss: -278602.375000\n",
      "    epoch          : 109\n",
      "    loss           : -285014.27247404575\n",
      "    val_loss       : -158523.39060058593\n",
      "Train Epoch: 110 [128/17352 (1%)] Loss: -328621.312500\n",
      "Train Epoch: 110 [1536/17352 (9%)] Loss: -328914.500000\n",
      "Train Epoch: 110 [2944/17352 (17%)] Loss: -327098.437500\n",
      "Train Epoch: 110 [4352/17352 (25%)] Loss: -330410.562500\n",
      "Train Epoch: 110 [5760/17352 (33%)] Loss: -324712.937500\n",
      "Train Epoch: 110 [7168/17352 (41%)] Loss: -307920.125000\n",
      "Train Epoch: 110 [8576/17352 (49%)] Loss: -281110.062500\n",
      "Train Epoch: 110 [9984/17352 (58%)] Loss: -327741.000000\n",
      "Train Epoch: 110 [11392/17352 (66%)] Loss: -233894.031250\n",
      "Train Epoch: 110 [12800/17352 (74%)] Loss: -334743.375000\n",
      "Train Epoch: 110 [14208/17352 (82%)] Loss: -343393.937500\n",
      "Train Epoch: 110 [15407/17352 (89%)] Loss: -74717.375000\n",
      "Train Epoch: 110 [16039/17352 (92%)] Loss: -154472.953125\n",
      "Train Epoch: 110 [16972/17352 (98%)] Loss: -229874.343750\n",
      "    epoch          : 110\n",
      "    loss           : -287960.57728935085\n",
      "    val_loss       : -158172.0162923177\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch110.pth ...\n",
      "Train Epoch: 111 [128/17352 (1%)] Loss: -346136.875000\n",
      "Train Epoch: 111 [1536/17352 (9%)] Loss: -325056.531250\n",
      "Train Epoch: 111 [2944/17352 (17%)] Loss: -316912.312500\n",
      "Train Epoch: 111 [4352/17352 (25%)] Loss: -329299.000000\n",
      "Train Epoch: 111 [5760/17352 (33%)] Loss: -317883.250000\n",
      "Train Epoch: 111 [7168/17352 (41%)] Loss: -344523.000000\n",
      "Train Epoch: 111 [8576/17352 (49%)] Loss: -267914.937500\n",
      "Train Epoch: 111 [9984/17352 (58%)] Loss: -347701.187500\n",
      "Train Epoch: 111 [11392/17352 (66%)] Loss: -275950.500000\n",
      "Train Epoch: 111 [12800/17352 (74%)] Loss: -323250.031250\n",
      "Train Epoch: 111 [14208/17352 (82%)] Loss: -336884.906250\n",
      "Train Epoch: 111 [15525/17352 (89%)] Loss: -173835.265625\n",
      "Train Epoch: 111 [16210/17352 (93%)] Loss: -11633.955078\n",
      "Train Epoch: 111 [16985/17352 (98%)] Loss: -226885.234375\n",
      "    epoch          : 111\n",
      "    loss           : -286748.3324539901\n",
      "    val_loss       : -157431.20585530598\n",
      "Train Epoch: 112 [128/17352 (1%)] Loss: -325245.500000\n",
      "Train Epoch: 112 [1536/17352 (9%)] Loss: -317946.812500\n",
      "Train Epoch: 112 [2944/17352 (17%)] Loss: -326881.156250\n",
      "Train Epoch: 112 [4352/17352 (25%)] Loss: -269369.281250\n",
      "Train Epoch: 112 [5760/17352 (33%)] Loss: -300218.000000\n",
      "Train Epoch: 112 [7168/17352 (41%)] Loss: -343777.031250\n",
      "Train Epoch: 112 [8576/17352 (49%)] Loss: -331868.125000\n",
      "Train Epoch: 112 [9984/17352 (58%)] Loss: -249209.437500\n",
      "Train Epoch: 112 [11392/17352 (66%)] Loss: -307219.562500\n",
      "Train Epoch: 112 [12800/17352 (74%)] Loss: -319562.687500\n",
      "Train Epoch: 112 [14208/17352 (82%)] Loss: -332575.437500\n",
      "Train Epoch: 112 [15565/17352 (90%)] Loss: -289495.531250\n",
      "Train Epoch: 112 [16172/17352 (93%)] Loss: -240380.000000\n",
      "Train Epoch: 112 [16964/17352 (98%)] Loss: -110690.414062\n",
      "    epoch          : 112\n",
      "    loss           : -286589.5892342177\n",
      "    val_loss       : -156208.58364257813\n",
      "Train Epoch: 113 [128/17352 (1%)] Loss: -210162.484375\n",
      "Train Epoch: 113 [1536/17352 (9%)] Loss: -328677.812500\n",
      "Train Epoch: 113 [2944/17352 (17%)] Loss: -286837.093750\n",
      "Train Epoch: 113 [4352/17352 (25%)] Loss: -353951.250000\n",
      "Train Epoch: 113 [5760/17352 (33%)] Loss: -328717.718750\n",
      "Train Epoch: 113 [7168/17352 (41%)] Loss: -319471.593750\n",
      "Train Epoch: 113 [8576/17352 (49%)] Loss: -348005.968750\n",
      "Train Epoch: 113 [9984/17352 (58%)] Loss: -312330.375000\n",
      "Train Epoch: 113 [11392/17352 (66%)] Loss: -327898.593750\n",
      "Train Epoch: 113 [12800/17352 (74%)] Loss: -303460.156250\n",
      "Train Epoch: 113 [14208/17352 (82%)] Loss: -338590.125000\n",
      "Train Epoch: 113 [15556/17352 (90%)] Loss: -244564.187500\n",
      "Train Epoch: 113 [16285/17352 (94%)] Loss: -159261.515625\n",
      "Train Epoch: 113 [16928/17352 (98%)] Loss: -141382.718750\n",
      "    epoch          : 113\n",
      "    loss           : -288715.1136646655\n",
      "    val_loss       : -161648.1588704427\n",
      "Train Epoch: 114 [128/17352 (1%)] Loss: -313808.625000\n",
      "Train Epoch: 114 [1536/17352 (9%)] Loss: -308616.750000\n",
      "Train Epoch: 114 [2944/17352 (17%)] Loss: -330783.343750\n",
      "Train Epoch: 114 [4352/17352 (25%)] Loss: -338650.968750\n",
      "Train Epoch: 114 [5760/17352 (33%)] Loss: -340020.187500\n",
      "Train Epoch: 114 [7168/17352 (41%)] Loss: -281916.562500\n",
      "Train Epoch: 114 [8576/17352 (49%)] Loss: -345643.625000\n",
      "Train Epoch: 114 [9984/17352 (58%)] Loss: -354976.093750\n",
      "Train Epoch: 114 [11392/17352 (66%)] Loss: -281618.812500\n",
      "Train Epoch: 114 [12800/17352 (74%)] Loss: -354032.187500\n",
      "Train Epoch: 114 [14208/17352 (82%)] Loss: -217766.640625\n",
      "Train Epoch: 114 [15399/17352 (89%)] Loss: -88506.390625\n",
      "Train Epoch: 114 [16262/17352 (94%)] Loss: -277571.312500\n",
      "Train Epoch: 114 [17052/17352 (98%)] Loss: -240587.734375\n",
      "    epoch          : 114\n",
      "    loss           : -289065.92247797817\n",
      "    val_loss       : -156488.19191894532\n",
      "Train Epoch: 115 [128/17352 (1%)] Loss: -314638.500000\n",
      "Train Epoch: 115 [1536/17352 (9%)] Loss: -324213.250000\n",
      "Train Epoch: 115 [2944/17352 (17%)] Loss: -346753.531250\n",
      "Train Epoch: 115 [4352/17352 (25%)] Loss: -324303.343750\n",
      "Train Epoch: 115 [5760/17352 (33%)] Loss: -331844.125000\n",
      "Train Epoch: 115 [7168/17352 (41%)] Loss: -307722.343750\n",
      "Train Epoch: 115 [8576/17352 (49%)] Loss: -284598.687500\n",
      "Train Epoch: 115 [9984/17352 (58%)] Loss: -321534.500000\n",
      "Train Epoch: 115 [11392/17352 (66%)] Loss: -337205.312500\n",
      "Train Epoch: 115 [12800/17352 (74%)] Loss: -284578.156250\n",
      "Train Epoch: 115 [14208/17352 (82%)] Loss: -362263.125000\n",
      "Train Epoch: 115 [15533/17352 (90%)] Loss: -246749.812500\n",
      "Train Epoch: 115 [16252/17352 (94%)] Loss: -5087.955566\n",
      "Train Epoch: 115 [17138/17352 (99%)] Loss: -114283.828125\n",
      "    epoch          : 115\n",
      "    loss           : -286975.69566969905\n",
      "    val_loss       : -163134.4225830078\n",
      "Train Epoch: 116 [128/17352 (1%)] Loss: -324716.187500\n",
      "Train Epoch: 116 [1536/17352 (9%)] Loss: -333204.312500\n",
      "Train Epoch: 116 [2944/17352 (17%)] Loss: -349927.187500\n",
      "Train Epoch: 116 [4352/17352 (25%)] Loss: -319691.031250\n",
      "Train Epoch: 116 [5760/17352 (33%)] Loss: -232198.843750\n",
      "Train Epoch: 116 [7168/17352 (41%)] Loss: -295706.062500\n",
      "Train Epoch: 116 [8576/17352 (49%)] Loss: -367471.687500\n",
      "Train Epoch: 116 [9984/17352 (58%)] Loss: -324110.687500\n",
      "Train Epoch: 116 [11392/17352 (66%)] Loss: -316733.375000\n",
      "Train Epoch: 116 [12800/17352 (74%)] Loss: -331574.968750\n",
      "Train Epoch: 116 [14208/17352 (82%)] Loss: -159948.625000\n",
      "Train Epoch: 116 [15450/17352 (89%)] Loss: -32742.949219\n",
      "Train Epoch: 116 [16248/17352 (94%)] Loss: -11927.895508\n",
      "Train Epoch: 116 [16897/17352 (97%)] Loss: -130893.031250\n",
      "    epoch          : 116\n",
      "    loss           : -292185.36363517196\n",
      "    val_loss       : -160967.22229817708\n",
      "Train Epoch: 117 [128/17352 (1%)] Loss: -244799.468750\n",
      "Train Epoch: 117 [1536/17352 (9%)] Loss: -340906.250000\n",
      "Train Epoch: 117 [2944/17352 (17%)] Loss: -315480.750000\n",
      "Train Epoch: 117 [4352/17352 (25%)] Loss: -310192.437500\n",
      "Train Epoch: 117 [5760/17352 (33%)] Loss: -281621.500000\n",
      "Train Epoch: 117 [7168/17352 (41%)] Loss: -334101.843750\n",
      "Train Epoch: 117 [8576/17352 (49%)] Loss: -309188.750000\n",
      "Train Epoch: 117 [9984/17352 (58%)] Loss: -343377.375000\n",
      "Train Epoch: 117 [11392/17352 (66%)] Loss: -302102.406250\n",
      "Train Epoch: 117 [12800/17352 (74%)] Loss: -318149.562500\n",
      "Train Epoch: 117 [14208/17352 (82%)] Loss: -293285.968750\n",
      "Train Epoch: 117 [15463/17352 (89%)] Loss: -245236.968750\n",
      "Train Epoch: 117 [16263/17352 (94%)] Loss: -117547.312500\n",
      "Train Epoch: 117 [16915/17352 (97%)] Loss: -26653.115234\n",
      "    epoch          : 117\n",
      "    loss           : -291075.870326919\n",
      "    val_loss       : -158371.8696777344\n",
      "Train Epoch: 118 [128/17352 (1%)] Loss: -328741.937500\n",
      "Train Epoch: 118 [1536/17352 (9%)] Loss: -295105.968750\n",
      "Train Epoch: 118 [2944/17352 (17%)] Loss: -196619.500000\n",
      "Train Epoch: 118 [4352/17352 (25%)] Loss: -312262.687500\n",
      "Train Epoch: 118 [5760/17352 (33%)] Loss: -302276.250000\n",
      "Train Epoch: 118 [7168/17352 (41%)] Loss: -349390.375000\n",
      "Train Epoch: 118 [8576/17352 (49%)] Loss: -349576.312500\n",
      "Train Epoch: 118 [9984/17352 (58%)] Loss: -355345.187500\n",
      "Train Epoch: 118 [11392/17352 (66%)] Loss: -347657.312500\n",
      "Train Epoch: 118 [12800/17352 (74%)] Loss: -328728.812500\n",
      "Train Epoch: 118 [14208/17352 (82%)] Loss: -308706.687500\n",
      "Train Epoch: 118 [15484/17352 (89%)] Loss: -219206.437500\n",
      "Train Epoch: 118 [16148/17352 (93%)] Loss: -270356.718750\n",
      "Train Epoch: 118 [16939/17352 (98%)] Loss: -6851.662109\n",
      "    epoch          : 118\n",
      "    loss           : -296009.23037699243\n",
      "    val_loss       : -161265.4178548177\n",
      "Train Epoch: 119 [128/17352 (1%)] Loss: -253567.125000\n",
      "Train Epoch: 119 [1536/17352 (9%)] Loss: -120887.890625\n",
      "Train Epoch: 119 [2944/17352 (17%)] Loss: -329259.125000\n",
      "Train Epoch: 119 [4352/17352 (25%)] Loss: -306575.843750\n",
      "Train Epoch: 119 [5760/17352 (33%)] Loss: -304873.625000\n",
      "Train Epoch: 119 [7168/17352 (41%)] Loss: -307449.687500\n",
      "Train Epoch: 119 [8576/17352 (49%)] Loss: -331566.531250\n",
      "Train Epoch: 119 [9984/17352 (58%)] Loss: -313037.187500\n",
      "Train Epoch: 119 [11392/17352 (66%)] Loss: -323382.031250\n",
      "Train Epoch: 119 [12800/17352 (74%)] Loss: -327361.437500\n",
      "Train Epoch: 119 [14208/17352 (82%)] Loss: -293922.687500\n",
      "Train Epoch: 119 [15476/17352 (89%)] Loss: -175282.593750\n",
      "Train Epoch: 119 [16114/17352 (93%)] Loss: -7719.347168\n",
      "Train Epoch: 119 [16940/17352 (98%)] Loss: -242353.953125\n",
      "    epoch          : 119\n",
      "    loss           : -284587.60639615665\n",
      "    val_loss       : -163432.6072998047\n",
      "Train Epoch: 120 [128/17352 (1%)] Loss: -335468.656250\n",
      "Train Epoch: 120 [1536/17352 (9%)] Loss: -325902.250000\n",
      "Train Epoch: 120 [2944/17352 (17%)] Loss: -342935.968750\n",
      "Train Epoch: 120 [4352/17352 (25%)] Loss: -328250.906250\n",
      "Train Epoch: 120 [5760/17352 (33%)] Loss: -323735.656250\n",
      "Train Epoch: 120 [7168/17352 (41%)] Loss: -319991.031250\n",
      "Train Epoch: 120 [8576/17352 (49%)] Loss: -354451.937500\n",
      "Train Epoch: 120 [9984/17352 (58%)] Loss: -347997.750000\n",
      "Train Epoch: 120 [11392/17352 (66%)] Loss: -337105.468750\n",
      "Train Epoch: 120 [12800/17352 (74%)] Loss: -331716.343750\n",
      "Train Epoch: 120 [14208/17352 (82%)] Loss: -341469.937500\n",
      "Train Epoch: 120 [15471/17352 (89%)] Loss: -79862.765625\n",
      "Train Epoch: 120 [16327/17352 (94%)] Loss: -314823.562500\n",
      "Train Epoch: 120 [17037/17352 (98%)] Loss: -13105.967773\n",
      "    epoch          : 120\n",
      "    loss           : -289558.17199625104\n",
      "    val_loss       : -165602.82369791667\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch120.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 121 [128/17352 (1%)] Loss: -278296.937500\n",
      "Train Epoch: 121 [1536/17352 (9%)] Loss: -317516.187500\n",
      "Train Epoch: 121 [2944/17352 (17%)] Loss: -293071.906250\n",
      "Train Epoch: 121 [4352/17352 (25%)] Loss: -323433.500000\n",
      "Train Epoch: 121 [5760/17352 (33%)] Loss: -297792.187500\n",
      "Train Epoch: 121 [7168/17352 (41%)] Loss: -343526.625000\n",
      "Train Epoch: 121 [8576/17352 (49%)] Loss: -333263.968750\n",
      "Train Epoch: 121 [9984/17352 (58%)] Loss: -334057.250000\n",
      "Train Epoch: 121 [11392/17352 (66%)] Loss: -328948.343750\n",
      "Train Epoch: 121 [12800/17352 (74%)] Loss: -313536.375000\n",
      "Train Epoch: 121 [14208/17352 (82%)] Loss: -299822.500000\n",
      "Train Epoch: 121 [15448/17352 (89%)] Loss: -191190.656250\n",
      "Train Epoch: 121 [16238/17352 (94%)] Loss: -167443.578125\n",
      "Train Epoch: 121 [17015/17352 (98%)] Loss: -321729.750000\n",
      "    epoch          : 121\n",
      "    loss           : -292078.51466154575\n",
      "    val_loss       : -165869.5169189453\n",
      "Train Epoch: 122 [128/17352 (1%)] Loss: -317882.687500\n",
      "Train Epoch: 122 [1536/17352 (9%)] Loss: -293175.781250\n",
      "Train Epoch: 122 [2944/17352 (17%)] Loss: -339393.000000\n",
      "Train Epoch: 122 [4352/17352 (25%)] Loss: -293756.500000\n",
      "Train Epoch: 122 [5760/17352 (33%)] Loss: -324019.156250\n",
      "Train Epoch: 122 [7168/17352 (41%)] Loss: -298358.781250\n",
      "Train Epoch: 122 [8576/17352 (49%)] Loss: -340108.812500\n",
      "Train Epoch: 122 [9984/17352 (58%)] Loss: -340913.875000\n",
      "Train Epoch: 122 [11392/17352 (66%)] Loss: -317953.906250\n",
      "Train Epoch: 122 [12800/17352 (74%)] Loss: -346136.312500\n",
      "Train Epoch: 122 [14208/17352 (82%)] Loss: -348937.656250\n",
      "Train Epoch: 122 [15530/17352 (89%)] Loss: -180071.156250\n",
      "Train Epoch: 122 [16333/17352 (94%)] Loss: -198927.062500\n",
      "Train Epoch: 122 [16967/17352 (98%)] Loss: -268953.968750\n",
      "    epoch          : 122\n",
      "    loss           : -286541.324100776\n",
      "    val_loss       : -156736.28831380207\n",
      "Train Epoch: 123 [128/17352 (1%)] Loss: -317998.750000\n",
      "Train Epoch: 123 [1536/17352 (9%)] Loss: -312497.312500\n",
      "Train Epoch: 123 [2944/17352 (17%)] Loss: -286636.781250\n",
      "Train Epoch: 123 [4352/17352 (25%)] Loss: -338029.000000\n",
      "Train Epoch: 123 [5760/17352 (33%)] Loss: -333487.156250\n",
      "Train Epoch: 123 [7168/17352 (41%)] Loss: -337512.000000\n",
      "Train Epoch: 123 [8576/17352 (49%)] Loss: -332833.312500\n",
      "Train Epoch: 123 [9984/17352 (58%)] Loss: -339045.625000\n",
      "Train Epoch: 123 [11392/17352 (66%)] Loss: -330695.375000\n",
      "Train Epoch: 123 [12800/17352 (74%)] Loss: -334025.968750\n",
      "Train Epoch: 123 [14208/17352 (82%)] Loss: -317128.937500\n",
      "Train Epoch: 123 [15470/17352 (89%)] Loss: -271277.500000\n",
      "Train Epoch: 123 [16191/17352 (93%)] Loss: -168762.031250\n",
      "Train Epoch: 123 [16965/17352 (98%)] Loss: -283688.812500\n",
      "    epoch          : 123\n",
      "    loss           : -286525.7004083211\n",
      "    val_loss       : -159531.9028808594\n",
      "Train Epoch: 124 [128/17352 (1%)] Loss: -319253.031250\n",
      "Train Epoch: 124 [1536/17352 (9%)] Loss: -313616.437500\n",
      "Train Epoch: 124 [2944/17352 (17%)] Loss: -359711.750000\n",
      "Train Epoch: 124 [4352/17352 (25%)] Loss: -334398.125000\n",
      "Train Epoch: 124 [5760/17352 (33%)] Loss: -338624.125000\n",
      "Train Epoch: 124 [7168/17352 (41%)] Loss: -284499.937500\n",
      "Train Epoch: 124 [8576/17352 (49%)] Loss: -295498.437500\n",
      "Train Epoch: 124 [9984/17352 (58%)] Loss: -319365.031250\n",
      "Train Epoch: 124 [11392/17352 (66%)] Loss: -342539.500000\n",
      "Train Epoch: 124 [12800/17352 (74%)] Loss: -321655.562500\n",
      "Train Epoch: 124 [14208/17352 (82%)] Loss: -355916.156250\n",
      "Train Epoch: 124 [15464/17352 (89%)] Loss: -135670.625000\n",
      "Train Epoch: 124 [16369/17352 (94%)] Loss: -226285.531250\n",
      "Train Epoch: 124 [16951/17352 (98%)] Loss: -22395.841797\n",
      "    epoch          : 124\n",
      "    loss           : -288410.85050073406\n",
      "    val_loss       : -161452.8966064453\n",
      "Train Epoch: 125 [128/17352 (1%)] Loss: -324513.250000\n",
      "Train Epoch: 125 [1536/17352 (9%)] Loss: -345808.656250\n",
      "Train Epoch: 125 [2944/17352 (17%)] Loss: -343943.875000\n",
      "Train Epoch: 125 [4352/17352 (25%)] Loss: -349973.312500\n",
      "Train Epoch: 125 [5760/17352 (33%)] Loss: -340389.812500\n",
      "Train Epoch: 125 [7168/17352 (41%)] Loss: -349314.062500\n",
      "Train Epoch: 125 [8576/17352 (49%)] Loss: -304862.468750\n",
      "Train Epoch: 125 [9984/17352 (58%)] Loss: -317214.781250\n",
      "Train Epoch: 125 [11392/17352 (66%)] Loss: -347777.625000\n",
      "Train Epoch: 125 [12800/17352 (74%)] Loss: -347080.531250\n",
      "Train Epoch: 125 [14208/17352 (82%)] Loss: -344927.718750\n",
      "Train Epoch: 125 [15407/17352 (89%)] Loss: -87886.914062\n",
      "Train Epoch: 125 [16286/17352 (94%)] Loss: -198835.250000\n",
      "Train Epoch: 125 [16870/17352 (97%)] Loss: -89715.765625\n",
      "    epoch          : 125\n",
      "    loss           : -296356.06624895136\n",
      "    val_loss       : -160635.2688232422\n",
      "Train Epoch: 126 [128/17352 (1%)] Loss: -353613.625000\n",
      "Train Epoch: 126 [1536/17352 (9%)] Loss: -307179.437500\n",
      "Train Epoch: 126 [2944/17352 (17%)] Loss: -347192.593750\n",
      "Train Epoch: 126 [4352/17352 (25%)] Loss: -347097.750000\n",
      "Train Epoch: 126 [5760/17352 (33%)] Loss: -322232.156250\n",
      "Train Epoch: 126 [7168/17352 (41%)] Loss: -300391.937500\n",
      "Train Epoch: 126 [8576/17352 (49%)] Loss: -285466.843750\n",
      "Train Epoch: 126 [9984/17352 (58%)] Loss: -331924.093750\n",
      "Train Epoch: 126 [11392/17352 (66%)] Loss: -336287.718750\n",
      "Train Epoch: 126 [12800/17352 (74%)] Loss: -351321.625000\n",
      "Train Epoch: 126 [14208/17352 (82%)] Loss: -324737.593750\n",
      "Train Epoch: 126 [15500/17352 (89%)] Loss: -141578.312500\n",
      "Train Epoch: 126 [16336/17352 (94%)] Loss: -109793.773438\n",
      "Train Epoch: 126 [17083/17352 (98%)] Loss: -220023.093750\n",
      "    epoch          : 126\n",
      "    loss           : -292354.85660916526\n",
      "    val_loss       : -163145.9753173828\n",
      "Train Epoch: 127 [128/17352 (1%)] Loss: -338676.156250\n",
      "Train Epoch: 127 [1536/17352 (9%)] Loss: -355606.812500\n",
      "Train Epoch: 127 [2944/17352 (17%)] Loss: -311994.031250\n",
      "Train Epoch: 127 [4352/17352 (25%)] Loss: -304464.468750\n",
      "Train Epoch: 127 [5760/17352 (33%)] Loss: -355502.625000\n",
      "Train Epoch: 127 [7168/17352 (41%)] Loss: -344137.156250\n",
      "Train Epoch: 127 [8576/17352 (49%)] Loss: -300028.937500\n",
      "Train Epoch: 127 [9984/17352 (58%)] Loss: -308310.000000\n",
      "Train Epoch: 127 [11392/17352 (66%)] Loss: -303163.812500\n",
      "Train Epoch: 127 [12800/17352 (74%)] Loss: -341803.562500\n",
      "Train Epoch: 127 [14208/17352 (82%)] Loss: -335555.062500\n",
      "Train Epoch: 127 [15515/17352 (89%)] Loss: -194134.734375\n",
      "Train Epoch: 127 [16257/17352 (94%)] Loss: -200871.156250\n",
      "Train Epoch: 127 [17038/17352 (98%)] Loss: -27979.109375\n",
      "    epoch          : 127\n",
      "    loss           : -294667.8906446623\n",
      "    val_loss       : -163623.60354003907\n",
      "Train Epoch: 128 [128/17352 (1%)] Loss: -342549.375000\n",
      "Train Epoch: 128 [1536/17352 (9%)] Loss: -298242.468750\n",
      "Train Epoch: 128 [2944/17352 (17%)] Loss: -324171.187500\n",
      "Train Epoch: 128 [4352/17352 (25%)] Loss: -360889.218750\n",
      "Train Epoch: 128 [5760/17352 (33%)] Loss: -351185.375000\n",
      "Train Epoch: 128 [7168/17352 (41%)] Loss: -331808.875000\n",
      "Train Epoch: 128 [8576/17352 (49%)] Loss: -336596.125000\n",
      "Train Epoch: 128 [9984/17352 (58%)] Loss: -344098.906250\n",
      "Train Epoch: 128 [11392/17352 (66%)] Loss: -343161.343750\n",
      "Train Epoch: 128 [12800/17352 (74%)] Loss: -345280.375000\n",
      "Train Epoch: 128 [14208/17352 (82%)] Loss: -323053.406250\n",
      "Train Epoch: 128 [15510/17352 (89%)] Loss: -180471.203125\n",
      "Train Epoch: 128 [16343/17352 (94%)] Loss: -133837.421875\n",
      "Train Epoch: 128 [17083/17352 (98%)] Loss: -245402.000000\n",
      "    epoch          : 128\n",
      "    loss           : -296354.56623912015\n",
      "    val_loss       : -162244.67000325522\n",
      "Train Epoch: 129 [128/17352 (1%)] Loss: -324211.187500\n",
      "Train Epoch: 129 [1536/17352 (9%)] Loss: -320781.125000\n",
      "Train Epoch: 129 [2944/17352 (17%)] Loss: -322408.500000\n",
      "Train Epoch: 129 [4352/17352 (25%)] Loss: -327215.687500\n",
      "Train Epoch: 129 [5760/17352 (33%)] Loss: -324161.906250\n",
      "Train Epoch: 129 [7168/17352 (41%)] Loss: -336527.656250\n",
      "Train Epoch: 129 [8576/17352 (49%)] Loss: -325522.062500\n",
      "Train Epoch: 129 [9984/17352 (58%)] Loss: -330695.625000\n",
      "Train Epoch: 129 [11392/17352 (66%)] Loss: -310446.625000\n",
      "Train Epoch: 129 [12800/17352 (74%)] Loss: -334313.406250\n",
      "Train Epoch: 129 [14208/17352 (82%)] Loss: -322523.750000\n",
      "Train Epoch: 129 [15476/17352 (89%)] Loss: -25415.699219\n",
      "Train Epoch: 129 [16190/17352 (93%)] Loss: -143357.546875\n",
      "Train Epoch: 129 [16929/17352 (98%)] Loss: -314380.031250\n",
      "    epoch          : 129\n",
      "    loss           : -286256.3476365877\n",
      "    val_loss       : -156749.6420735677\n",
      "Train Epoch: 130 [128/17352 (1%)] Loss: -325879.375000\n",
      "Train Epoch: 130 [1536/17352 (9%)] Loss: -321865.843750\n",
      "Train Epoch: 130 [2944/17352 (17%)] Loss: -316205.593750\n",
      "Train Epoch: 130 [4352/17352 (25%)] Loss: -241709.718750\n",
      "Train Epoch: 130 [5760/17352 (33%)] Loss: -344129.687500\n",
      "Train Epoch: 130 [7168/17352 (41%)] Loss: -348262.250000\n",
      "Train Epoch: 130 [8576/17352 (49%)] Loss: -347883.468750\n",
      "Train Epoch: 130 [9984/17352 (58%)] Loss: -354999.187500\n",
      "Train Epoch: 130 [11392/17352 (66%)] Loss: -329831.906250\n",
      "Train Epoch: 130 [12800/17352 (74%)] Loss: -330571.687500\n",
      "Train Epoch: 130 [14208/17352 (82%)] Loss: -312942.218750\n",
      "Train Epoch: 130 [15546/17352 (90%)] Loss: -184755.328125\n",
      "Train Epoch: 130 [16323/17352 (94%)] Loss: -244677.718750\n",
      "Train Epoch: 130 [17095/17352 (99%)] Loss: -7039.658203\n",
      "    epoch          : 130\n",
      "    loss           : -288050.98056050757\n",
      "    val_loss       : -158174.92806803386\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch130.pth ...\n",
      "Train Epoch: 131 [128/17352 (1%)] Loss: -306338.843750\n",
      "Train Epoch: 131 [1536/17352 (9%)] Loss: -331985.312500\n",
      "Train Epoch: 131 [2944/17352 (17%)] Loss: -341742.000000\n",
      "Train Epoch: 131 [4352/17352 (25%)] Loss: -348339.062500\n",
      "Train Epoch: 131 [5760/17352 (33%)] Loss: -333089.437500\n",
      "Train Epoch: 131 [7168/17352 (41%)] Loss: -356790.312500\n",
      "Train Epoch: 131 [8576/17352 (49%)] Loss: -297723.656250\n",
      "Train Epoch: 131 [9984/17352 (58%)] Loss: -327156.500000\n",
      "Train Epoch: 131 [11392/17352 (66%)] Loss: -310886.812500\n",
      "Train Epoch: 131 [12800/17352 (74%)] Loss: -345775.656250\n",
      "Train Epoch: 131 [14208/17352 (82%)] Loss: -309099.343750\n",
      "Train Epoch: 131 [15413/17352 (89%)] Loss: -128568.109375\n",
      "Train Epoch: 131 [16240/17352 (94%)] Loss: -253420.390625\n",
      "Train Epoch: 131 [17115/17352 (99%)] Loss: -181868.375000\n",
      "    epoch          : 131\n",
      "    loss           : -284490.58342072146\n",
      "    val_loss       : -160359.9330078125\n",
      "Train Epoch: 132 [128/17352 (1%)] Loss: -323965.187500\n",
      "Train Epoch: 132 [1536/17352 (9%)] Loss: -277389.500000\n",
      "Train Epoch: 132 [2944/17352 (17%)] Loss: -310883.500000\n",
      "Train Epoch: 132 [4352/17352 (25%)] Loss: -291474.562500\n",
      "Train Epoch: 132 [5760/17352 (33%)] Loss: -336916.156250\n",
      "Train Epoch: 132 [7168/17352 (41%)] Loss: -335298.281250\n",
      "Train Epoch: 132 [8576/17352 (49%)] Loss: -344449.125000\n",
      "Train Epoch: 132 [9984/17352 (58%)] Loss: -304660.593750\n",
      "Train Epoch: 132 [11392/17352 (66%)] Loss: -320566.125000\n",
      "Train Epoch: 132 [12800/17352 (74%)] Loss: -331083.937500\n",
      "Train Epoch: 132 [14208/17352 (82%)] Loss: -333362.500000\n",
      "Train Epoch: 132 [15547/17352 (90%)] Loss: -221814.703125\n",
      "Train Epoch: 132 [16407/17352 (95%)] Loss: -134202.656250\n",
      "Train Epoch: 132 [17076/17352 (98%)] Loss: -204835.031250\n",
      "    epoch          : 132\n",
      "    loss           : -290391.94053783035\n",
      "    val_loss       : -156187.13958333334\n",
      "Train Epoch: 133 [128/17352 (1%)] Loss: -327856.718750\n",
      "Train Epoch: 133 [1536/17352 (9%)] Loss: -316047.906250\n",
      "Train Epoch: 133 [2944/17352 (17%)] Loss: -305537.562500\n",
      "Train Epoch: 133 [4352/17352 (25%)] Loss: -333616.375000\n",
      "Train Epoch: 133 [5760/17352 (33%)] Loss: -342949.906250\n",
      "Train Epoch: 133 [7168/17352 (41%)] Loss: -302891.375000\n",
      "Train Epoch: 133 [8576/17352 (49%)] Loss: -365758.750000\n",
      "Train Epoch: 133 [9984/17352 (58%)] Loss: -328625.156250\n",
      "Train Epoch: 133 [11392/17352 (66%)] Loss: -329836.781250\n",
      "Train Epoch: 133 [12800/17352 (74%)] Loss: -339164.062500\n",
      "Train Epoch: 133 [14208/17352 (82%)] Loss: -300390.656250\n",
      "Train Epoch: 133 [15521/17352 (89%)] Loss: -222149.171875\n",
      "Train Epoch: 133 [16188/17352 (93%)] Loss: -129985.359375\n",
      "Train Epoch: 133 [16983/17352 (98%)] Loss: -179632.750000\n",
      "    epoch          : 133\n",
      "    loss           : -293811.5071570889\n",
      "    val_loss       : -160929.63619791667\n",
      "Train Epoch: 134 [128/17352 (1%)] Loss: -290830.625000\n",
      "Train Epoch: 134 [1536/17352 (9%)] Loss: -325308.031250\n",
      "Train Epoch: 134 [2944/17352 (17%)] Loss: -348371.375000\n",
      "Train Epoch: 134 [4352/17352 (25%)] Loss: -307577.906250\n",
      "Train Epoch: 134 [5760/17352 (33%)] Loss: -297695.468750\n",
      "Train Epoch: 134 [7168/17352 (41%)] Loss: -348429.500000\n",
      "Train Epoch: 134 [8576/17352 (49%)] Loss: -333403.625000\n",
      "Train Epoch: 134 [9984/17352 (58%)] Loss: -280397.937500\n",
      "Train Epoch: 134 [11392/17352 (66%)] Loss: -342097.875000\n",
      "Train Epoch: 134 [12800/17352 (74%)] Loss: -305998.031250\n",
      "Train Epoch: 134 [14208/17352 (82%)] Loss: -259995.656250\n",
      "Train Epoch: 134 [15491/17352 (89%)] Loss: -190123.468750\n",
      "Train Epoch: 134 [16230/17352 (94%)] Loss: -205356.406250\n",
      "Train Epoch: 134 [17011/17352 (98%)] Loss: -251887.234375\n",
      "    epoch          : 134\n",
      "    loss           : -294367.5447285287\n",
      "    val_loss       : -161193.4112874349\n",
      "Train Epoch: 135 [128/17352 (1%)] Loss: -318398.875000\n",
      "Train Epoch: 135 [1536/17352 (9%)] Loss: -348514.312500\n",
      "Train Epoch: 135 [2944/17352 (17%)] Loss: -315318.812500\n",
      "Train Epoch: 135 [4352/17352 (25%)] Loss: -348873.343750\n",
      "Train Epoch: 135 [5760/17352 (33%)] Loss: -322084.343750\n",
      "Train Epoch: 135 [7168/17352 (41%)] Loss: -348226.500000\n",
      "Train Epoch: 135 [8576/17352 (49%)] Loss: -339799.375000\n",
      "Train Epoch: 135 [9984/17352 (58%)] Loss: -366895.500000\n",
      "Train Epoch: 135 [11392/17352 (66%)] Loss: -283541.437500\n",
      "Train Epoch: 135 [12800/17352 (74%)] Loss: -331729.468750\n",
      "Train Epoch: 135 [14208/17352 (82%)] Loss: -314055.562500\n",
      "Train Epoch: 135 [15553/17352 (90%)] Loss: -279654.875000\n",
      "Train Epoch: 135 [16197/17352 (93%)] Loss: -94758.578125\n",
      "Train Epoch: 135 [17005/17352 (98%)] Loss: -135941.187500\n",
      "    epoch          : 135\n",
      "    loss           : -293086.1979406984\n",
      "    val_loss       : -165070.9835530599\n",
      "Train Epoch: 136 [128/17352 (1%)] Loss: -308998.687500\n",
      "Train Epoch: 136 [1536/17352 (9%)] Loss: -301188.218750\n",
      "Train Epoch: 136 [2944/17352 (17%)] Loss: -337106.687500\n",
      "Train Epoch: 136 [4352/17352 (25%)] Loss: -327355.250000\n",
      "Train Epoch: 136 [5760/17352 (33%)] Loss: -329995.687500\n",
      "Train Epoch: 136 [7168/17352 (41%)] Loss: -334187.281250\n",
      "Train Epoch: 136 [8576/17352 (49%)] Loss: -329990.437500\n",
      "Train Epoch: 136 [9984/17352 (58%)] Loss: -305917.187500\n",
      "Train Epoch: 136 [11392/17352 (66%)] Loss: -325745.000000\n",
      "Train Epoch: 136 [12800/17352 (74%)] Loss: -351002.468750\n",
      "Train Epoch: 136 [14208/17352 (82%)] Loss: -325921.468750\n",
      "Train Epoch: 136 [15540/17352 (90%)] Loss: -240716.718750\n",
      "Train Epoch: 136 [16306/17352 (94%)] Loss: -276957.937500\n",
      "Train Epoch: 136 [16978/17352 (98%)] Loss: -137982.968750\n",
      "    epoch          : 136\n",
      "    loss           : -287883.16190619755\n",
      "    val_loss       : -158995.06815592447\n",
      "Train Epoch: 137 [128/17352 (1%)] Loss: -296876.281250\n",
      "Train Epoch: 137 [1536/17352 (9%)] Loss: -326241.375000\n",
      "Train Epoch: 137 [2944/17352 (17%)] Loss: -307995.250000\n",
      "Train Epoch: 137 [4352/17352 (25%)] Loss: -345136.312500\n",
      "Train Epoch: 137 [5760/17352 (33%)] Loss: -271978.656250\n",
      "Train Epoch: 137 [7168/17352 (41%)] Loss: -320072.437500\n",
      "Train Epoch: 137 [8576/17352 (49%)] Loss: -332491.937500\n",
      "Train Epoch: 137 [9984/17352 (58%)] Loss: -322440.562500\n",
      "Train Epoch: 137 [11392/17352 (66%)] Loss: -334363.281250\n",
      "Train Epoch: 137 [12800/17352 (74%)] Loss: -283681.250000\n",
      "Train Epoch: 137 [14208/17352 (82%)] Loss: -339601.375000\n",
      "Train Epoch: 137 [15399/17352 (89%)] Loss: -11840.686523\n",
      "Train Epoch: 137 [16247/17352 (94%)] Loss: -32298.501953\n",
      "Train Epoch: 137 [16974/17352 (98%)] Loss: -93486.320312\n",
      "    epoch          : 137\n",
      "    loss           : -282665.09144295304\n",
      "    val_loss       : -155660.95872395832\n",
      "Train Epoch: 138 [128/17352 (1%)] Loss: -307846.468750\n",
      "Train Epoch: 138 [1536/17352 (9%)] Loss: -304528.093750\n",
      "Train Epoch: 138 [2944/17352 (17%)] Loss: -311761.531250\n",
      "Train Epoch: 138 [4352/17352 (25%)] Loss: -261323.234375\n",
      "Train Epoch: 138 [5760/17352 (33%)] Loss: -307731.937500\n",
      "Train Epoch: 138 [7168/17352 (41%)] Loss: -317421.437500\n",
      "Train Epoch: 138 [8576/17352 (49%)] Loss: -290738.406250\n",
      "Train Epoch: 138 [9984/17352 (58%)] Loss: -312516.250000\n",
      "Train Epoch: 138 [11392/17352 (66%)] Loss: -323478.625000\n",
      "Train Epoch: 138 [12800/17352 (74%)] Loss: -336513.468750\n",
      "Train Epoch: 138 [14208/17352 (82%)] Loss: -334912.906250\n",
      "Train Epoch: 138 [15440/17352 (89%)] Loss: -7215.946289\n",
      "Train Epoch: 138 [16210/17352 (93%)] Loss: -221169.718750\n",
      "Train Epoch: 138 [16920/17352 (98%)] Loss: -206846.468750\n",
      "    epoch          : 138\n",
      "    loss           : -278104.6229354551\n",
      "    val_loss       : -148958.24503580728\n",
      "Train Epoch: 139 [128/17352 (1%)] Loss: -315277.750000\n",
      "Train Epoch: 139 [1536/17352 (9%)] Loss: -344646.531250\n",
      "Train Epoch: 139 [2944/17352 (17%)] Loss: -294919.343750\n",
      "Train Epoch: 139 [4352/17352 (25%)] Loss: -328987.218750\n",
      "Train Epoch: 139 [5760/17352 (33%)] Loss: -311276.062500\n",
      "Train Epoch: 139 [7168/17352 (41%)] Loss: -315210.125000\n",
      "Train Epoch: 139 [8576/17352 (49%)] Loss: -292274.468750\n",
      "Train Epoch: 139 [9984/17352 (58%)] Loss: -318031.750000\n",
      "Train Epoch: 139 [11392/17352 (66%)] Loss: -305257.250000\n",
      "Train Epoch: 139 [12800/17352 (74%)] Loss: -318474.000000\n",
      "Train Epoch: 139 [14208/17352 (82%)] Loss: -320074.437500\n",
      "Train Epoch: 139 [15506/17352 (89%)] Loss: -243490.515625\n",
      "Train Epoch: 139 [16167/17352 (93%)] Loss: -206017.281250\n",
      "Train Epoch: 139 [16917/17352 (97%)] Loss: -110251.781250\n",
      "    epoch          : 139\n",
      "    loss           : -272460.78430421563\n",
      "    val_loss       : -155287.56383463542\n",
      "Train Epoch: 140 [128/17352 (1%)] Loss: -283081.437500\n",
      "Train Epoch: 140 [1536/17352 (9%)] Loss: -286395.343750\n",
      "Train Epoch: 140 [2944/17352 (17%)] Loss: -301506.500000\n",
      "Train Epoch: 140 [4352/17352 (25%)] Loss: -325296.125000\n",
      "Train Epoch: 140 [5760/17352 (33%)] Loss: -307470.687500\n",
      "Train Epoch: 140 [7168/17352 (41%)] Loss: -329347.437500\n",
      "Train Epoch: 140 [8576/17352 (49%)] Loss: -278518.187500\n",
      "Train Epoch: 140 [9984/17352 (58%)] Loss: -315925.062500\n",
      "Train Epoch: 140 [11392/17352 (66%)] Loss: -317930.500000\n",
      "Train Epoch: 140 [12800/17352 (74%)] Loss: -296624.687500\n",
      "Train Epoch: 140 [14208/17352 (82%)] Loss: -280327.281250\n",
      "Train Epoch: 140 [15472/17352 (89%)] Loss: -77074.226562\n",
      "Train Epoch: 140 [16330/17352 (94%)] Loss: -273968.187500\n",
      "Train Epoch: 140 [16936/17352 (98%)] Loss: -129976.796875\n",
      "    epoch          : 140\n",
      "    loss           : -270335.2010997798\n",
      "    val_loss       : -151102.826171875\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch140.pth ...\n",
      "Train Epoch: 141 [128/17352 (1%)] Loss: -310296.937500\n",
      "Train Epoch: 141 [1536/17352 (9%)] Loss: -303212.625000\n",
      "Train Epoch: 141 [2944/17352 (17%)] Loss: -325870.312500\n",
      "Train Epoch: 141 [4352/17352 (25%)] Loss: -317794.375000\n",
      "Train Epoch: 141 [5760/17352 (33%)] Loss: -275007.937500\n",
      "Train Epoch: 141 [7168/17352 (41%)] Loss: -271833.156250\n",
      "Train Epoch: 141 [8576/17352 (49%)] Loss: -323201.531250\n",
      "Train Epoch: 141 [9984/17352 (58%)] Loss: -316012.000000\n",
      "Train Epoch: 141 [11392/17352 (66%)] Loss: -322060.000000\n",
      "Train Epoch: 141 [12800/17352 (74%)] Loss: -327047.750000\n",
      "Train Epoch: 141 [14208/17352 (82%)] Loss: -307601.843750\n",
      "Train Epoch: 141 [15504/17352 (89%)] Loss: -227494.906250\n",
      "Train Epoch: 141 [16313/17352 (94%)] Loss: -210721.265625\n",
      "Train Epoch: 141 [16989/17352 (98%)] Loss: -198603.093750\n",
      "    epoch          : 141\n",
      "    loss           : -279539.4100776007\n",
      "    val_loss       : -155451.05869954426\n",
      "Train Epoch: 142 [128/17352 (1%)] Loss: -315824.187500\n",
      "Train Epoch: 142 [1536/17352 (9%)] Loss: -322749.781250\n",
      "Train Epoch: 142 [2944/17352 (17%)] Loss: -328899.218750\n",
      "Train Epoch: 142 [4352/17352 (25%)] Loss: -328214.000000\n",
      "Train Epoch: 142 [5760/17352 (33%)] Loss: -296068.500000\n",
      "Train Epoch: 142 [7168/17352 (41%)] Loss: -197939.718750\n",
      "Train Epoch: 142 [8576/17352 (49%)] Loss: -315793.937500\n",
      "Train Epoch: 142 [9984/17352 (58%)] Loss: -317978.562500\n",
      "Train Epoch: 142 [11392/17352 (66%)] Loss: -327295.906250\n",
      "Train Epoch: 142 [12800/17352 (74%)] Loss: -330346.062500\n",
      "Train Epoch: 142 [14208/17352 (82%)] Loss: -303059.562500\n",
      "Train Epoch: 142 [15458/17352 (89%)] Loss: -220873.296875\n",
      "Train Epoch: 142 [15991/17352 (92%)] Loss: -216067.156250\n",
      "Train Epoch: 142 [16886/17352 (97%)] Loss: -186722.281250\n",
      "    epoch          : 142\n",
      "    loss           : -282256.92874370806\n",
      "    val_loss       : -152418.74388020832\n",
      "Train Epoch: 143 [128/17352 (1%)] Loss: -310904.125000\n",
      "Train Epoch: 143 [1536/17352 (9%)] Loss: -313589.875000\n",
      "Train Epoch: 143 [2944/17352 (17%)] Loss: -326590.125000\n",
      "Train Epoch: 143 [4352/17352 (25%)] Loss: -341148.750000\n",
      "Train Epoch: 143 [5760/17352 (33%)] Loss: -321829.375000\n",
      "Train Epoch: 143 [7168/17352 (41%)] Loss: -324773.968750\n",
      "Train Epoch: 143 [8576/17352 (49%)] Loss: -315882.750000\n",
      "Train Epoch: 143 [9984/17352 (58%)] Loss: -318684.312500\n",
      "Train Epoch: 143 [11392/17352 (66%)] Loss: -327953.500000\n",
      "Train Epoch: 143 [12800/17352 (74%)] Loss: -328292.125000\n",
      "Train Epoch: 143 [14208/17352 (82%)] Loss: -333952.468750\n",
      "Train Epoch: 143 [15538/17352 (90%)] Loss: -220774.937500\n",
      "Train Epoch: 143 [16373/17352 (94%)] Loss: -228321.109375\n",
      "Train Epoch: 143 [17096/17352 (99%)] Loss: -192881.000000\n",
      "    epoch          : 143\n",
      "    loss           : -285718.40995634964\n",
      "    val_loss       : -159817.80965169272\n",
      "Train Epoch: 144 [128/17352 (1%)] Loss: -324666.937500\n",
      "Train Epoch: 144 [1536/17352 (9%)] Loss: -340301.156250\n",
      "Train Epoch: 144 [2944/17352 (17%)] Loss: -353699.468750\n",
      "Train Epoch: 144 [4352/17352 (25%)] Loss: -328028.437500\n",
      "Train Epoch: 144 [5760/17352 (33%)] Loss: -323637.500000\n",
      "Train Epoch: 144 [7168/17352 (41%)] Loss: -324616.562500\n",
      "Train Epoch: 144 [8576/17352 (49%)] Loss: -299382.812500\n",
      "Train Epoch: 144 [9984/17352 (58%)] Loss: -335653.062500\n",
      "Train Epoch: 144 [11392/17352 (66%)] Loss: -327665.468750\n",
      "Train Epoch: 144 [12800/17352 (74%)] Loss: -275740.375000\n",
      "Train Epoch: 144 [14208/17352 (82%)] Loss: -328578.375000\n",
      "Train Epoch: 144 [15523/17352 (89%)] Loss: -161333.718750\n",
      "Train Epoch: 144 [16027/17352 (92%)] Loss: -139336.843750\n",
      "Train Epoch: 144 [16933/17352 (98%)] Loss: -269812.312500\n",
      "    epoch          : 144\n",
      "    loss           : -284473.8803743708\n",
      "    val_loss       : -160955.6838704427\n",
      "Train Epoch: 145 [128/17352 (1%)] Loss: -334260.562500\n",
      "Train Epoch: 145 [1536/17352 (9%)] Loss: -345260.593750\n",
      "Train Epoch: 145 [2944/17352 (17%)] Loss: -316130.250000\n",
      "Train Epoch: 145 [4352/17352 (25%)] Loss: -316212.312500\n",
      "Train Epoch: 145 [5760/17352 (33%)] Loss: -315002.000000\n",
      "Train Epoch: 145 [7168/17352 (41%)] Loss: -340707.812500\n",
      "Train Epoch: 145 [8576/17352 (49%)] Loss: -321472.406250\n",
      "Train Epoch: 145 [9984/17352 (58%)] Loss: -313483.281250\n",
      "Train Epoch: 145 [11392/17352 (66%)] Loss: -356429.875000\n",
      "Train Epoch: 145 [12800/17352 (74%)] Loss: -259804.812500\n",
      "Train Epoch: 145 [14208/17352 (82%)] Loss: -334289.250000\n",
      "Train Epoch: 145 [15551/17352 (90%)] Loss: -226561.437500\n",
      "Train Epoch: 145 [16395/17352 (94%)] Loss: -15096.275391\n",
      "Train Epoch: 145 [17048/17352 (98%)] Loss: -215111.968750\n",
      "    epoch          : 145\n",
      "    loss           : -289263.39773621014\n",
      "    val_loss       : -162508.72041829428\n",
      "Train Epoch: 146 [128/17352 (1%)] Loss: -296124.687500\n",
      "Train Epoch: 146 [1536/17352 (9%)] Loss: -354072.562500\n",
      "Train Epoch: 146 [2944/17352 (17%)] Loss: -325470.687500\n",
      "Train Epoch: 146 [4352/17352 (25%)] Loss: -302760.875000\n",
      "Train Epoch: 146 [5760/17352 (33%)] Loss: -299823.156250\n",
      "Train Epoch: 146 [7168/17352 (41%)] Loss: -223649.671875\n",
      "Train Epoch: 146 [8576/17352 (49%)] Loss: -344468.187500\n",
      "Train Epoch: 146 [9984/17352 (58%)] Loss: -350255.187500\n",
      "Train Epoch: 146 [11392/17352 (66%)] Loss: -323813.125000\n",
      "Train Epoch: 146 [12800/17352 (74%)] Loss: -341554.281250\n",
      "Train Epoch: 146 [14208/17352 (82%)] Loss: -327107.500000\n",
      "Train Epoch: 146 [15519/17352 (89%)] Loss: -137757.921875\n",
      "Train Epoch: 146 [16300/17352 (94%)] Loss: -329359.500000\n",
      "Train Epoch: 146 [17000/17352 (98%)] Loss: -264028.437500\n",
      "    epoch          : 146\n",
      "    loss           : -292609.22048028524\n",
      "    val_loss       : -157208.46340332032\n",
      "Train Epoch: 147 [128/17352 (1%)] Loss: -287100.625000\n",
      "Train Epoch: 147 [1536/17352 (9%)] Loss: -315190.250000\n",
      "Train Epoch: 147 [2944/17352 (17%)] Loss: -343805.187500\n",
      "Train Epoch: 147 [4352/17352 (25%)] Loss: -313256.031250\n",
      "Train Epoch: 147 [5760/17352 (33%)] Loss: -321636.500000\n",
      "Train Epoch: 147 [7168/17352 (41%)] Loss: -302823.000000\n",
      "Train Epoch: 147 [8576/17352 (49%)] Loss: -341894.625000\n",
      "Train Epoch: 147 [9984/17352 (58%)] Loss: -287558.625000\n",
      "Train Epoch: 147 [11392/17352 (66%)] Loss: -350111.156250\n",
      "Train Epoch: 147 [12800/17352 (74%)] Loss: -326867.500000\n",
      "Train Epoch: 147 [14208/17352 (82%)] Loss: -333270.281250\n",
      "Train Epoch: 147 [15499/17352 (89%)] Loss: -139696.000000\n",
      "Train Epoch: 147 [16310/17352 (94%)] Loss: -235037.375000\n",
      "Train Epoch: 147 [17051/17352 (98%)] Loss: -224591.031250\n",
      "    epoch          : 147\n",
      "    loss           : -286790.07524119125\n",
      "    val_loss       : -164863.76013997395\n",
      "Train Epoch: 148 [128/17352 (1%)] Loss: -293210.500000\n",
      "Train Epoch: 148 [1536/17352 (9%)] Loss: -309324.531250\n",
      "Train Epoch: 148 [2944/17352 (17%)] Loss: -320173.375000\n",
      "Train Epoch: 148 [4352/17352 (25%)] Loss: -315165.375000\n",
      "Train Epoch: 148 [5760/17352 (33%)] Loss: -337083.437500\n",
      "Train Epoch: 148 [7168/17352 (41%)] Loss: -351288.625000\n",
      "Train Epoch: 148 [8576/17352 (49%)] Loss: -287959.625000\n",
      "Train Epoch: 148 [9984/17352 (58%)] Loss: -328908.281250\n",
      "Train Epoch: 148 [11392/17352 (66%)] Loss: -332819.812500\n",
      "Train Epoch: 148 [12800/17352 (74%)] Loss: -354239.781250\n",
      "Train Epoch: 148 [14208/17352 (82%)] Loss: -341398.375000\n",
      "Train Epoch: 148 [15480/17352 (89%)] Loss: -78606.671875\n",
      "Train Epoch: 148 [16177/17352 (93%)] Loss: -87234.828125\n",
      "Train Epoch: 148 [16990/17352 (98%)] Loss: -164282.031250\n",
      "    epoch          : 148\n",
      "    loss           : -291247.58646510594\n",
      "    val_loss       : -160645.88638509114\n",
      "Train Epoch: 149 [128/17352 (1%)] Loss: -319141.781250\n",
      "Train Epoch: 149 [1536/17352 (9%)] Loss: -316685.343750\n",
      "Train Epoch: 149 [2944/17352 (17%)] Loss: -283921.187500\n",
      "Train Epoch: 149 [4352/17352 (25%)] Loss: -335178.718750\n",
      "Train Epoch: 149 [5760/17352 (33%)] Loss: -334952.031250\n",
      "Train Epoch: 149 [7168/17352 (41%)] Loss: -316177.656250\n",
      "Train Epoch: 149 [8576/17352 (49%)] Loss: -303923.312500\n",
      "Train Epoch: 149 [9984/17352 (58%)] Loss: -326293.312500\n",
      "Train Epoch: 149 [11392/17352 (66%)] Loss: -346161.125000\n",
      "Train Epoch: 149 [12800/17352 (74%)] Loss: -343658.406250\n",
      "Train Epoch: 149 [14208/17352 (82%)] Loss: -351266.281250\n",
      "Train Epoch: 149 [15529/17352 (89%)] Loss: -262863.750000\n",
      "Train Epoch: 149 [16128/17352 (93%)] Loss: -213851.390625\n",
      "Train Epoch: 149 [16908/17352 (97%)] Loss: -83689.101562\n",
      "    epoch          : 149\n",
      "    loss           : -290961.8395422609\n",
      "    val_loss       : -161045.83304036458\n",
      "Train Epoch: 150 [128/17352 (1%)] Loss: -329711.437500\n",
      "Train Epoch: 150 [1536/17352 (9%)] Loss: -329117.687500\n",
      "Train Epoch: 150 [2944/17352 (17%)] Loss: -337623.812500\n",
      "Train Epoch: 150 [4352/17352 (25%)] Loss: -336354.562500\n",
      "Train Epoch: 150 [5760/17352 (33%)] Loss: -331336.875000\n",
      "Train Epoch: 150 [7168/17352 (41%)] Loss: -360328.312500\n",
      "Train Epoch: 150 [8576/17352 (49%)] Loss: -309331.531250\n",
      "Train Epoch: 150 [9984/17352 (58%)] Loss: -328820.937500\n",
      "Train Epoch: 150 [11392/17352 (66%)] Loss: -338435.593750\n",
      "Train Epoch: 150 [12800/17352 (74%)] Loss: -356273.937500\n",
      "Train Epoch: 150 [14208/17352 (82%)] Loss: -338365.281250\n",
      "Train Epoch: 150 [15489/17352 (89%)] Loss: -217857.843750\n",
      "Train Epoch: 150 [16289/17352 (94%)] Loss: -116299.812500\n",
      "Train Epoch: 150 [17004/17352 (98%)] Loss: -133431.187500\n",
      "    epoch          : 150\n",
      "    loss           : -290236.7002542995\n",
      "    val_loss       : -155458.90266927084\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [128/17352 (1%)] Loss: -335732.156250\n",
      "Train Epoch: 151 [1536/17352 (9%)] Loss: -335932.687500\n",
      "Train Epoch: 151 [2944/17352 (17%)] Loss: -336713.312500\n",
      "Train Epoch: 151 [4352/17352 (25%)] Loss: -165606.093750\n",
      "Train Epoch: 151 [5760/17352 (33%)] Loss: -341324.625000\n",
      "Train Epoch: 151 [7168/17352 (41%)] Loss: -357241.468750\n",
      "Train Epoch: 151 [8576/17352 (49%)] Loss: -293653.437500\n",
      "Train Epoch: 151 [9984/17352 (58%)] Loss: -350281.562500\n",
      "Train Epoch: 151 [11392/17352 (66%)] Loss: -334955.125000\n",
      "Train Epoch: 151 [12800/17352 (74%)] Loss: -333595.406250\n",
      "Train Epoch: 151 [14208/17352 (82%)] Loss: -349052.500000\n",
      "Train Epoch: 151 [15478/17352 (89%)] Loss: -264506.437500\n",
      "Train Epoch: 151 [16221/17352 (93%)] Loss: -130652.906250\n",
      "Train Epoch: 151 [17074/17352 (98%)] Loss: -227705.125000\n",
      "    epoch          : 151\n",
      "    loss           : -292550.63533255557\n",
      "    val_loss       : -163981.1353027344\n",
      "Train Epoch: 152 [128/17352 (1%)] Loss: -319479.531250\n",
      "Train Epoch: 152 [1536/17352 (9%)] Loss: -288326.968750\n",
      "Train Epoch: 152 [2944/17352 (17%)] Loss: -257916.750000\n",
      "Train Epoch: 152 [4352/17352 (25%)] Loss: -329678.187500\n",
      "Train Epoch: 152 [5760/17352 (33%)] Loss: -315385.156250\n",
      "Train Epoch: 152 [7168/17352 (41%)] Loss: -317226.125000\n",
      "Train Epoch: 152 [8576/17352 (49%)] Loss: -342104.906250\n",
      "Train Epoch: 152 [9984/17352 (58%)] Loss: -336065.468750\n",
      "Train Epoch: 152 [11392/17352 (66%)] Loss: -288005.781250\n",
      "Train Epoch: 152 [12800/17352 (74%)] Loss: -328389.843750\n",
      "Train Epoch: 152 [14208/17352 (82%)] Loss: -328138.250000\n",
      "Train Epoch: 152 [15451/17352 (89%)] Loss: -14514.443359\n",
      "Train Epoch: 152 [16337/17352 (94%)] Loss: -272665.656250\n",
      "Train Epoch: 152 [17055/17352 (98%)] Loss: -118334.835938\n",
      "    epoch          : 152\n",
      "    loss           : -293752.63883572776\n",
      "    val_loss       : -161278.23581542968\n",
      "Train Epoch: 153 [128/17352 (1%)] Loss: -259112.953125\n",
      "Train Epoch: 153 [1536/17352 (9%)] Loss: -355046.000000\n",
      "Train Epoch: 153 [2944/17352 (17%)] Loss: -280513.000000\n",
      "Train Epoch: 153 [4352/17352 (25%)] Loss: -350951.250000\n",
      "Train Epoch: 153 [5760/17352 (33%)] Loss: -348920.437500\n",
      "Train Epoch: 153 [7168/17352 (41%)] Loss: -322048.031250\n",
      "Train Epoch: 153 [8576/17352 (49%)] Loss: -233468.531250\n",
      "Train Epoch: 153 [9984/17352 (58%)] Loss: -333084.781250\n",
      "Train Epoch: 153 [11392/17352 (66%)] Loss: -315648.843750\n",
      "Train Epoch: 153 [12800/17352 (74%)] Loss: -327596.687500\n",
      "Train Epoch: 153 [14208/17352 (82%)] Loss: -343893.718750\n",
      "Train Epoch: 153 [15554/17352 (90%)] Loss: -268750.468750\n",
      "Train Epoch: 153 [16322/17352 (94%)] Loss: -305074.406250\n",
      "Train Epoch: 153 [17180/17352 (99%)] Loss: -255497.187500\n",
      "    epoch          : 153\n",
      "    loss           : -291497.4472000839\n",
      "    val_loss       : -170093.5184000651\n",
      "Train Epoch: 154 [128/17352 (1%)] Loss: -333794.906250\n",
      "Train Epoch: 154 [1536/17352 (9%)] Loss: -353706.375000\n",
      "Train Epoch: 154 [2944/17352 (17%)] Loss: -330262.437500\n",
      "Train Epoch: 154 [4352/17352 (25%)] Loss: -364883.156250\n",
      "Train Epoch: 154 [5760/17352 (33%)] Loss: -316448.062500\n",
      "Train Epoch: 154 [7168/17352 (41%)] Loss: -337723.468750\n",
      "Train Epoch: 154 [8576/17352 (49%)] Loss: -383377.843750\n",
      "Train Epoch: 154 [9984/17352 (58%)] Loss: -300418.687500\n",
      "Train Epoch: 154 [11392/17352 (66%)] Loss: -313580.781250\n",
      "Train Epoch: 154 [12800/17352 (74%)] Loss: -310569.687500\n",
      "Train Epoch: 154 [14208/17352 (82%)] Loss: -339539.250000\n",
      "Train Epoch: 154 [15495/17352 (89%)] Loss: -273856.812500\n",
      "Train Epoch: 154 [16283/17352 (94%)] Loss: -36178.160156\n",
      "Train Epoch: 154 [16949/17352 (98%)] Loss: -5980.057617\n",
      "    epoch          : 154\n",
      "    loss           : -300465.3104944421\n",
      "    val_loss       : -173744.17338053384\n",
      "Train Epoch: 155 [128/17352 (1%)] Loss: -347466.843750\n",
      "Train Epoch: 155 [1536/17352 (9%)] Loss: -302116.718750\n",
      "Train Epoch: 155 [2944/17352 (17%)] Loss: -351053.937500\n",
      "Train Epoch: 155 [4352/17352 (25%)] Loss: -360275.343750\n",
      "Train Epoch: 155 [5760/17352 (33%)] Loss: -385618.000000\n",
      "Train Epoch: 155 [7168/17352 (41%)] Loss: -347195.937500\n",
      "Train Epoch: 155 [8576/17352 (49%)] Loss: -288364.250000\n",
      "Train Epoch: 155 [9984/17352 (58%)] Loss: -366626.968750\n",
      "Train Epoch: 155 [11392/17352 (66%)] Loss: -340275.593750\n",
      "Train Epoch: 155 [12800/17352 (74%)] Loss: -357397.218750\n",
      "Train Epoch: 155 [14208/17352 (82%)] Loss: -325855.750000\n",
      "Train Epoch: 155 [15504/17352 (89%)] Loss: -248722.875000\n",
      "Train Epoch: 155 [16289/17352 (94%)] Loss: -187853.250000\n",
      "Train Epoch: 155 [17056/17352 (98%)] Loss: -330874.843750\n",
      "    epoch          : 155\n",
      "    loss           : -303988.59763003356\n",
      "    val_loss       : -163977.18870442707\n",
      "Train Epoch: 156 [128/17352 (1%)] Loss: -245154.312500\n",
      "Train Epoch: 156 [1536/17352 (9%)] Loss: -269393.812500\n",
      "Train Epoch: 156 [2944/17352 (17%)] Loss: -358212.750000\n",
      "Train Epoch: 156 [4352/17352 (25%)] Loss: -332604.125000\n",
      "Train Epoch: 156 [5760/17352 (33%)] Loss: -346739.625000\n",
      "Train Epoch: 156 [7168/17352 (41%)] Loss: -325884.781250\n",
      "Train Epoch: 156 [8576/17352 (49%)] Loss: -207796.718750\n",
      "Train Epoch: 156 [9984/17352 (58%)] Loss: -254045.531250\n",
      "Train Epoch: 156 [11392/17352 (66%)] Loss: -336794.062500\n",
      "Train Epoch: 156 [12800/17352 (74%)] Loss: -317739.468750\n",
      "Train Epoch: 156 [14208/17352 (82%)] Loss: -348249.375000\n",
      "Train Epoch: 156 [15482/17352 (89%)] Loss: -89399.218750\n",
      "Train Epoch: 156 [16228/17352 (94%)] Loss: -8771.616211\n",
      "Train Epoch: 156 [16956/17352 (98%)] Loss: -230474.265625\n",
      "    epoch          : 156\n",
      "    loss           : -304208.8672268247\n",
      "    val_loss       : -171357.04943847656\n",
      "Train Epoch: 157 [128/17352 (1%)] Loss: -339428.250000\n",
      "Train Epoch: 157 [1536/17352 (9%)] Loss: -354410.781250\n",
      "Train Epoch: 157 [2944/17352 (17%)] Loss: -273601.375000\n",
      "Train Epoch: 157 [4352/17352 (25%)] Loss: -358446.906250\n",
      "Train Epoch: 157 [5760/17352 (33%)] Loss: -360274.156250\n",
      "Train Epoch: 157 [7168/17352 (41%)] Loss: -348003.562500\n",
      "Train Epoch: 157 [8576/17352 (49%)] Loss: -347743.437500\n",
      "Train Epoch: 157 [9984/17352 (58%)] Loss: -354355.625000\n",
      "Train Epoch: 157 [11392/17352 (66%)] Loss: -396503.843750\n",
      "Train Epoch: 157 [12800/17352 (74%)] Loss: -377976.812500\n",
      "Train Epoch: 157 [14208/17352 (82%)] Loss: -322358.250000\n",
      "Train Epoch: 157 [15418/17352 (89%)] Loss: -111351.265625\n",
      "Train Epoch: 157 [16313/17352 (94%)] Loss: -235399.750000\n",
      "Train Epoch: 157 [17071/17352 (98%)] Loss: -283580.250000\n",
      "    epoch          : 157\n",
      "    loss           : -308779.41262059566\n",
      "    val_loss       : -173410.88478190106\n",
      "Train Epoch: 158 [128/17352 (1%)] Loss: -347418.687500\n",
      "Train Epoch: 158 [1536/17352 (9%)] Loss: -353698.531250\n",
      "Train Epoch: 158 [2944/17352 (17%)] Loss: -368393.187500\n",
      "Train Epoch: 158 [4352/17352 (25%)] Loss: -310815.406250\n",
      "Train Epoch: 158 [5760/17352 (33%)] Loss: -335856.531250\n",
      "Train Epoch: 158 [7168/17352 (41%)] Loss: -386427.625000\n",
      "Train Epoch: 158 [8576/17352 (49%)] Loss: -321136.062500\n",
      "Train Epoch: 158 [9984/17352 (58%)] Loss: -302347.531250\n",
      "Train Epoch: 158 [11392/17352 (66%)] Loss: -360752.937500\n",
      "Train Epoch: 158 [12800/17352 (74%)] Loss: -358271.656250\n",
      "Train Epoch: 158 [14208/17352 (82%)] Loss: -331969.625000\n",
      "Train Epoch: 158 [15525/17352 (89%)] Loss: -218220.140625\n",
      "Train Epoch: 158 [16177/17352 (93%)] Loss: -280239.750000\n",
      "Train Epoch: 158 [17060/17352 (98%)] Loss: -110290.968750\n",
      "    epoch          : 158\n",
      "    loss           : -309277.6981307676\n",
      "    val_loss       : -171240.230460612\n",
      "Train Epoch: 159 [128/17352 (1%)] Loss: -292560.000000\n",
      "Train Epoch: 159 [1536/17352 (9%)] Loss: -309515.000000\n",
      "Train Epoch: 159 [2944/17352 (17%)] Loss: -346823.281250\n",
      "Train Epoch: 159 [4352/17352 (25%)] Loss: -341205.843750\n",
      "Train Epoch: 159 [5760/17352 (33%)] Loss: -381938.437500\n",
      "Train Epoch: 159 [7168/17352 (41%)] Loss: -354937.937500\n",
      "Train Epoch: 159 [8576/17352 (49%)] Loss: -384012.437500\n",
      "Train Epoch: 159 [9984/17352 (58%)] Loss: -377921.250000\n",
      "Train Epoch: 159 [11392/17352 (66%)] Loss: -332497.000000\n",
      "Train Epoch: 159 [12800/17352 (74%)] Loss: -359269.812500\n",
      "Train Epoch: 159 [14208/17352 (82%)] Loss: -361648.187500\n",
      "Train Epoch: 159 [15495/17352 (89%)] Loss: -94109.085938\n",
      "Train Epoch: 159 [16329/17352 (94%)] Loss: -201576.234375\n",
      "Train Epoch: 159 [17031/17352 (98%)] Loss: -202068.125000\n",
      "    epoch          : 159\n",
      "    loss           : -312355.41648752097\n",
      "    val_loss       : -167842.19759114584\n",
      "Train Epoch: 160 [128/17352 (1%)] Loss: -358997.187500\n",
      "Train Epoch: 160 [1536/17352 (9%)] Loss: -347176.187500\n",
      "Train Epoch: 160 [2944/17352 (17%)] Loss: -365248.000000\n",
      "Train Epoch: 160 [4352/17352 (25%)] Loss: -347621.312500\n",
      "Train Epoch: 160 [5760/17352 (33%)] Loss: -367420.687500\n",
      "Train Epoch: 160 [7168/17352 (41%)] Loss: -283041.906250\n",
      "Train Epoch: 160 [8576/17352 (49%)] Loss: -356402.812500\n",
      "Train Epoch: 160 [9984/17352 (58%)] Loss: -365268.562500\n",
      "Train Epoch: 160 [11392/17352 (66%)] Loss: -353634.812500\n",
      "Train Epoch: 160 [12800/17352 (74%)] Loss: -370434.531250\n",
      "Train Epoch: 160 [14208/17352 (82%)] Loss: -376041.187500\n",
      "Train Epoch: 160 [15541/17352 (90%)] Loss: -260935.359375\n",
      "Train Epoch: 160 [16197/17352 (93%)] Loss: -216641.812500\n",
      "Train Epoch: 160 [16996/17352 (98%)] Loss: -178249.875000\n",
      "    epoch          : 160\n",
      "    loss           : -317840.49384569004\n",
      "    val_loss       : -167214.28164876302\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch160.pth ...\n",
      "Train Epoch: 161 [128/17352 (1%)] Loss: -355232.656250\n",
      "Train Epoch: 161 [1536/17352 (9%)] Loss: -348952.312500\n",
      "Train Epoch: 161 [2944/17352 (17%)] Loss: -346322.437500\n",
      "Train Epoch: 161 [4352/17352 (25%)] Loss: -370013.500000\n",
      "Train Epoch: 161 [5760/17352 (33%)] Loss: -273933.781250\n",
      "Train Epoch: 161 [7168/17352 (41%)] Loss: -358326.843750\n",
      "Train Epoch: 161 [8576/17352 (49%)] Loss: -301585.281250\n",
      "Train Epoch: 161 [9984/17352 (58%)] Loss: -276888.968750\n",
      "Train Epoch: 161 [11392/17352 (66%)] Loss: -346406.281250\n",
      "Train Epoch: 161 [12800/17352 (74%)] Loss: -361664.562500\n",
      "Train Epoch: 161 [14208/17352 (82%)] Loss: -359064.750000\n",
      "Train Epoch: 161 [15528/17352 (89%)] Loss: -251608.468750\n",
      "Train Epoch: 161 [16314/17352 (94%)] Loss: -94303.687500\n",
      "Train Epoch: 161 [16988/17352 (98%)] Loss: -246111.375000\n",
      "    epoch          : 161\n",
      "    loss           : -310397.5470060822\n",
      "    val_loss       : -168775.44138183593\n",
      "Train Epoch: 162 [128/17352 (1%)] Loss: -372295.250000\n",
      "Train Epoch: 162 [1536/17352 (9%)] Loss: -351731.812500\n",
      "Train Epoch: 162 [2944/17352 (17%)] Loss: -355268.875000\n",
      "Train Epoch: 162 [4352/17352 (25%)] Loss: -358185.281250\n",
      "Train Epoch: 162 [5760/17352 (33%)] Loss: -379821.218750\n",
      "Train Epoch: 162 [7168/17352 (41%)] Loss: -366005.718750\n",
      "Train Epoch: 162 [8576/17352 (49%)] Loss: -363716.343750\n",
      "Train Epoch: 162 [9984/17352 (58%)] Loss: -315608.468750\n",
      "Train Epoch: 162 [11392/17352 (66%)] Loss: -306044.250000\n",
      "Train Epoch: 162 [12800/17352 (74%)] Loss: -358750.812500\n",
      "Train Epoch: 162 [14208/17352 (82%)] Loss: -364280.875000\n",
      "Train Epoch: 162 [15457/17352 (89%)] Loss: -107892.484375\n",
      "Train Epoch: 162 [16010/17352 (92%)] Loss: -93940.289062\n",
      "Train Epoch: 162 [16870/17352 (97%)] Loss: -256512.015625\n",
      "    epoch          : 162\n",
      "    loss           : -313940.9897559249\n",
      "    val_loss       : -170024.42872721356\n",
      "Train Epoch: 163 [128/17352 (1%)] Loss: -345442.281250\n",
      "Train Epoch: 163 [1536/17352 (9%)] Loss: -385884.468750\n",
      "Train Epoch: 163 [2944/17352 (17%)] Loss: -346371.437500\n",
      "Train Epoch: 163 [4352/17352 (25%)] Loss: -358654.062500\n",
      "Train Epoch: 163 [5760/17352 (33%)] Loss: -286713.437500\n",
      "Train Epoch: 163 [7168/17352 (41%)] Loss: -295107.937500\n",
      "Train Epoch: 163 [8576/17352 (49%)] Loss: -373730.000000\n",
      "Train Epoch: 163 [9984/17352 (58%)] Loss: -361051.125000\n",
      "Train Epoch: 163 [11392/17352 (66%)] Loss: -317805.562500\n",
      "Train Epoch: 163 [12800/17352 (74%)] Loss: -351697.718750\n",
      "Train Epoch: 163 [14208/17352 (82%)] Loss: -384719.687500\n",
      "Train Epoch: 163 [15487/17352 (89%)] Loss: -178243.750000\n",
      "Train Epoch: 163 [16253/17352 (94%)] Loss: -122916.359375\n",
      "Train Epoch: 163 [17010/17352 (98%)] Loss: -168873.781250\n",
      "    epoch          : 163\n",
      "    loss           : -310017.51678835467\n",
      "    val_loss       : -175037.43922526043\n",
      "Train Epoch: 164 [128/17352 (1%)] Loss: -288903.000000\n",
      "Train Epoch: 164 [1536/17352 (9%)] Loss: -361643.343750\n",
      "Train Epoch: 164 [2944/17352 (17%)] Loss: -329052.406250\n",
      "Train Epoch: 164 [4352/17352 (25%)] Loss: -282993.406250\n",
      "Train Epoch: 164 [5760/17352 (33%)] Loss: -375508.562500\n",
      "Train Epoch: 164 [7168/17352 (41%)] Loss: -351459.156250\n",
      "Train Epoch: 164 [8576/17352 (49%)] Loss: -401162.625000\n",
      "Train Epoch: 164 [9984/17352 (58%)] Loss: -368309.687500\n",
      "Train Epoch: 164 [11392/17352 (66%)] Loss: -362397.250000\n",
      "Train Epoch: 164 [12800/17352 (74%)] Loss: -322246.906250\n",
      "Train Epoch: 164 [14208/17352 (82%)] Loss: -370359.031250\n",
      "Train Epoch: 164 [15529/17352 (89%)] Loss: -327090.562500\n",
      "Train Epoch: 164 [16383/17352 (94%)] Loss: -286235.843750\n",
      "Train Epoch: 164 [17183/17352 (99%)] Loss: -223924.968750\n",
      "    epoch          : 164\n",
      "    loss           : -313182.4369461776\n",
      "    val_loss       : -172278.2128499349\n",
      "Train Epoch: 165 [128/17352 (1%)] Loss: -349492.218750\n",
      "Train Epoch: 165 [1536/17352 (9%)] Loss: -339599.656250\n",
      "Train Epoch: 165 [2944/17352 (17%)] Loss: -368309.250000\n",
      "Train Epoch: 165 [4352/17352 (25%)] Loss: -363410.250000\n",
      "Train Epoch: 165 [5760/17352 (33%)] Loss: -366152.562500\n",
      "Train Epoch: 165 [7168/17352 (41%)] Loss: -348141.500000\n",
      "Train Epoch: 165 [8576/17352 (49%)] Loss: -346800.875000\n",
      "Train Epoch: 165 [9984/17352 (58%)] Loss: -364962.000000\n",
      "Train Epoch: 165 [11392/17352 (66%)] Loss: -268622.750000\n",
      "Train Epoch: 165 [12800/17352 (74%)] Loss: -379809.687500\n",
      "Train Epoch: 165 [14208/17352 (82%)] Loss: -363287.125000\n",
      "Train Epoch: 165 [15575/17352 (90%)] Loss: -239117.828125\n",
      "Train Epoch: 165 [16312/17352 (94%)] Loss: -274259.437500\n",
      "Train Epoch: 165 [17086/17352 (98%)] Loss: -241686.406250\n",
      "    epoch          : 165\n",
      "    loss           : -311363.1618439335\n",
      "    val_loss       : -170786.6450683594\n",
      "Train Epoch: 166 [128/17352 (1%)] Loss: -351358.875000\n",
      "Train Epoch: 166 [1536/17352 (9%)] Loss: -360184.093750\n",
      "Train Epoch: 166 [2944/17352 (17%)] Loss: -365699.250000\n",
      "Train Epoch: 166 [4352/17352 (25%)] Loss: -368549.062500\n",
      "Train Epoch: 166 [5760/17352 (33%)] Loss: -318218.343750\n",
      "Train Epoch: 166 [7168/17352 (41%)] Loss: -369003.625000\n",
      "Train Epoch: 166 [8576/17352 (49%)] Loss: -289634.562500\n",
      "Train Epoch: 166 [9984/17352 (58%)] Loss: -325240.843750\n",
      "Train Epoch: 166 [11392/17352 (66%)] Loss: -326942.343750\n",
      "Train Epoch: 166 [12800/17352 (74%)] Loss: -348601.875000\n",
      "Train Epoch: 166 [14208/17352 (82%)] Loss: -343862.687500\n",
      "Train Epoch: 166 [15487/17352 (89%)] Loss: -148828.156250\n",
      "Train Epoch: 166 [16171/17352 (93%)] Loss: -99625.718750\n",
      "Train Epoch: 166 [16950/17352 (98%)] Loss: -269660.500000\n",
      "    epoch          : 166\n",
      "    loss           : -313253.6396123899\n",
      "    val_loss       : -176240.9342610677\n",
      "Train Epoch: 167 [128/17352 (1%)] Loss: -329488.125000\n",
      "Train Epoch: 167 [1536/17352 (9%)] Loss: -366270.375000\n",
      "Train Epoch: 167 [2944/17352 (17%)] Loss: -360559.687500\n",
      "Train Epoch: 167 [4352/17352 (25%)] Loss: -339503.812500\n",
      "Train Epoch: 167 [5760/17352 (33%)] Loss: -351411.125000\n",
      "Train Epoch: 167 [7168/17352 (41%)] Loss: -382713.406250\n",
      "Train Epoch: 167 [8576/17352 (49%)] Loss: -307297.281250\n",
      "Train Epoch: 167 [9984/17352 (58%)] Loss: -352758.031250\n",
      "Train Epoch: 167 [11392/17352 (66%)] Loss: -365227.343750\n",
      "Train Epoch: 167 [12800/17352 (74%)] Loss: -360727.562500\n",
      "Train Epoch: 167 [14208/17352 (82%)] Loss: -333025.750000\n",
      "Train Epoch: 167 [15529/17352 (89%)] Loss: -333253.062500\n",
      "Train Epoch: 167 [16106/17352 (93%)] Loss: -249685.203125\n",
      "Train Epoch: 167 [17004/17352 (98%)] Loss: -210896.359375\n",
      "    epoch          : 167\n",
      "    loss           : -317058.26597892196\n",
      "    val_loss       : -175588.4292480469\n",
      "Train Epoch: 168 [128/17352 (1%)] Loss: -356093.281250\n",
      "Train Epoch: 168 [1536/17352 (9%)] Loss: -274455.625000\n",
      "Train Epoch: 168 [2944/17352 (17%)] Loss: -362772.406250\n",
      "Train Epoch: 168 [4352/17352 (25%)] Loss: -371756.593750\n",
      "Train Epoch: 168 [5760/17352 (33%)] Loss: -360293.281250\n",
      "Train Epoch: 168 [7168/17352 (41%)] Loss: -276365.468750\n",
      "Train Epoch: 168 [8576/17352 (49%)] Loss: -340894.500000\n",
      "Train Epoch: 168 [9984/17352 (58%)] Loss: -363310.312500\n",
      "Train Epoch: 168 [11392/17352 (66%)] Loss: -364968.375000\n",
      "Train Epoch: 168 [12800/17352 (74%)] Loss: -362142.906250\n",
      "Train Epoch: 168 [14208/17352 (82%)] Loss: -338023.093750\n",
      "Train Epoch: 168 [15602/17352 (90%)] Loss: -337637.312500\n",
      "Train Epoch: 168 [16305/17352 (94%)] Loss: -134373.500000\n",
      "Train Epoch: 168 [16980/17352 (98%)] Loss: -96265.640625\n",
      "    epoch          : 168\n",
      "    loss           : -308467.4211474937\n",
      "    val_loss       : -170599.8135904948\n",
      "Train Epoch: 169 [128/17352 (1%)] Loss: -352497.125000\n",
      "Train Epoch: 169 [1536/17352 (9%)] Loss: -345631.687500\n",
      "Train Epoch: 169 [2944/17352 (17%)] Loss: -340048.937500\n",
      "Train Epoch: 169 [4352/17352 (25%)] Loss: -359982.718750\n",
      "Train Epoch: 169 [5760/17352 (33%)] Loss: -349799.593750\n",
      "Train Epoch: 169 [7168/17352 (41%)] Loss: -382744.531250\n",
      "Train Epoch: 169 [8576/17352 (49%)] Loss: -365803.781250\n",
      "Train Epoch: 169 [9984/17352 (58%)] Loss: -336563.437500\n",
      "Train Epoch: 169 [11392/17352 (66%)] Loss: -330374.343750\n",
      "Train Epoch: 169 [12800/17352 (74%)] Loss: -395973.906250\n",
      "Train Epoch: 169 [14208/17352 (82%)] Loss: -328845.187500\n",
      "Train Epoch: 169 [15472/17352 (89%)] Loss: -91730.234375\n",
      "Train Epoch: 169 [16201/17352 (93%)] Loss: -46437.671875\n",
      "Train Epoch: 169 [17059/17352 (98%)] Loss: -152810.218750\n",
      "    epoch          : 169\n",
      "    loss           : -317527.2643010696\n",
      "    val_loss       : -171697.48775227866\n",
      "Train Epoch: 170 [128/17352 (1%)] Loss: -332282.593750\n",
      "Train Epoch: 170 [1536/17352 (9%)] Loss: -306281.062500\n",
      "Train Epoch: 170 [2944/17352 (17%)] Loss: -359768.781250\n",
      "Train Epoch: 170 [4352/17352 (25%)] Loss: -350340.812500\n",
      "Train Epoch: 170 [5760/17352 (33%)] Loss: -368356.625000\n",
      "Train Epoch: 170 [7168/17352 (41%)] Loss: -276221.781250\n",
      "Train Epoch: 170 [8576/17352 (49%)] Loss: -383324.281250\n",
      "Train Epoch: 170 [9984/17352 (58%)] Loss: -373311.875000\n",
      "Train Epoch: 170 [11392/17352 (66%)] Loss: -321772.375000\n",
      "Train Epoch: 170 [12800/17352 (74%)] Loss: -381092.812500\n",
      "Train Epoch: 170 [14208/17352 (82%)] Loss: -384136.843750\n",
      "Train Epoch: 170 [15514/17352 (89%)] Loss: -224451.250000\n",
      "Train Epoch: 170 [16180/17352 (93%)] Loss: -286560.968750\n",
      "Train Epoch: 170 [17017/17352 (98%)] Loss: -250995.750000\n",
      "    epoch          : 170\n",
      "    loss           : -324662.09938653524\n",
      "    val_loss       : -182929.52517903646\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch170.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 171 [128/17352 (1%)] Loss: -361574.500000\n",
      "Train Epoch: 171 [1536/17352 (9%)] Loss: -368972.093750\n",
      "Train Epoch: 171 [2944/17352 (17%)] Loss: -371532.312500\n",
      "Train Epoch: 171 [4352/17352 (25%)] Loss: -357898.968750\n",
      "Train Epoch: 171 [5760/17352 (33%)] Loss: -382210.375000\n",
      "Train Epoch: 171 [7168/17352 (41%)] Loss: -391929.312500\n",
      "Train Epoch: 171 [8576/17352 (49%)] Loss: -375301.750000\n",
      "Train Epoch: 171 [9984/17352 (58%)] Loss: -383190.687500\n",
      "Train Epoch: 171 [11392/17352 (66%)] Loss: -385275.625000\n",
      "Train Epoch: 171 [12800/17352 (74%)] Loss: -378781.093750\n",
      "Train Epoch: 171 [14208/17352 (82%)] Loss: -347798.875000\n",
      "Train Epoch: 171 [15518/17352 (89%)] Loss: -187099.718750\n",
      "Train Epoch: 171 [16269/17352 (94%)] Loss: -145475.796875\n",
      "Train Epoch: 171 [17062/17352 (98%)] Loss: -331062.875000\n",
      "    epoch          : 171\n",
      "    loss           : -322247.54472197464\n",
      "    val_loss       : -175713.19083658853\n",
      "Train Epoch: 172 [128/17352 (1%)] Loss: -353593.187500\n",
      "Train Epoch: 172 [1536/17352 (9%)] Loss: -343081.187500\n",
      "Train Epoch: 172 [2944/17352 (17%)] Loss: -367444.687500\n",
      "Train Epoch: 172 [4352/17352 (25%)] Loss: -387898.250000\n",
      "Train Epoch: 172 [5760/17352 (33%)] Loss: -399270.125000\n",
      "Train Epoch: 172 [7168/17352 (41%)] Loss: -364201.031250\n",
      "Train Epoch: 172 [8576/17352 (49%)] Loss: -317310.437500\n",
      "Train Epoch: 172 [9984/17352 (58%)] Loss: -376302.187500\n",
      "Train Epoch: 172 [11392/17352 (66%)] Loss: -337181.437500\n",
      "Train Epoch: 172 [12800/17352 (74%)] Loss: -337655.312500\n",
      "Train Epoch: 172 [14208/17352 (82%)] Loss: -316733.781250\n",
      "Train Epoch: 172 [15446/17352 (89%)] Loss: -32792.460938\n",
      "Train Epoch: 172 [16232/17352 (94%)] Loss: -240850.062500\n",
      "Train Epoch: 172 [16909/17352 (97%)] Loss: -254824.281250\n",
      "    epoch          : 172\n",
      "    loss           : -316799.5372437343\n",
      "    val_loss       : -177522.4076497396\n",
      "Train Epoch: 173 [128/17352 (1%)] Loss: -303691.187500\n",
      "Train Epoch: 173 [1536/17352 (9%)] Loss: -378811.406250\n",
      "Train Epoch: 173 [2944/17352 (17%)] Loss: -269825.968750\n",
      "Train Epoch: 173 [4352/17352 (25%)] Loss: -395622.625000\n",
      "Train Epoch: 173 [5760/17352 (33%)] Loss: -358414.718750\n",
      "Train Epoch: 173 [7168/17352 (41%)] Loss: -362519.406250\n",
      "Train Epoch: 173 [8576/17352 (49%)] Loss: -374647.312500\n",
      "Train Epoch: 173 [9984/17352 (58%)] Loss: -382750.812500\n",
      "Train Epoch: 173 [11392/17352 (66%)] Loss: -309932.437500\n",
      "Train Epoch: 173 [12800/17352 (74%)] Loss: -371766.687500\n",
      "Train Epoch: 173 [14208/17352 (82%)] Loss: -369356.625000\n",
      "Train Epoch: 173 [15388/17352 (89%)] Loss: -37105.183594\n",
      "Train Epoch: 173 [16278/17352 (94%)] Loss: -262824.593750\n",
      "Train Epoch: 173 [16973/17352 (98%)] Loss: -14530.832031\n",
      "    epoch          : 173\n",
      "    loss           : -320952.8382117764\n",
      "    val_loss       : -173403.31129557293\n",
      "Train Epoch: 174 [128/17352 (1%)] Loss: -296536.687500\n",
      "Train Epoch: 174 [1536/17352 (9%)] Loss: -366801.406250\n",
      "Train Epoch: 174 [2944/17352 (17%)] Loss: -376241.781250\n",
      "Train Epoch: 174 [4352/17352 (25%)] Loss: -365077.312500\n",
      "Train Epoch: 174 [5760/17352 (33%)] Loss: -406266.093750\n",
      "Train Epoch: 174 [7168/17352 (41%)] Loss: -328760.843750\n",
      "Train Epoch: 174 [8576/17352 (49%)] Loss: -334888.812500\n",
      "Train Epoch: 174 [9984/17352 (58%)] Loss: -383046.312500\n",
      "Train Epoch: 174 [11392/17352 (66%)] Loss: -387365.750000\n",
      "Train Epoch: 174 [12800/17352 (74%)] Loss: -239450.609375\n",
      "Train Epoch: 174 [14208/17352 (82%)] Loss: -360978.250000\n",
      "Train Epoch: 174 [15423/17352 (89%)] Loss: -141647.906250\n",
      "Train Epoch: 174 [16178/17352 (93%)] Loss: -253284.750000\n",
      "Train Epoch: 174 [16955/17352 (98%)] Loss: -168546.765625\n",
      "    epoch          : 174\n",
      "    loss           : -326672.7742797032\n",
      "    val_loss       : -186079.61396484374\n",
      "Train Epoch: 175 [128/17352 (1%)] Loss: -369621.500000\n",
      "Train Epoch: 175 [1536/17352 (9%)] Loss: -235369.234375\n",
      "Train Epoch: 175 [2944/17352 (17%)] Loss: -239507.703125\n",
      "Train Epoch: 175 [4352/17352 (25%)] Loss: -334929.593750\n",
      "Train Epoch: 175 [5760/17352 (33%)] Loss: -384779.718750\n",
      "Train Epoch: 175 [7168/17352 (41%)] Loss: -350302.906250\n",
      "Train Epoch: 175 [8576/17352 (49%)] Loss: -378021.656250\n",
      "Train Epoch: 175 [9984/17352 (58%)] Loss: -339881.468750\n",
      "Train Epoch: 175 [11392/17352 (66%)] Loss: -364456.500000\n",
      "Train Epoch: 175 [12800/17352 (74%)] Loss: -388020.281250\n",
      "Train Epoch: 175 [14208/17352 (82%)] Loss: -368233.031250\n",
      "Train Epoch: 175 [15419/17352 (89%)] Loss: -13413.313477\n",
      "Train Epoch: 175 [16197/17352 (93%)] Loss: -284677.906250\n",
      "Train Epoch: 175 [17007/17352 (98%)] Loss: -287159.687500\n",
      "    epoch          : 175\n",
      "    loss           : -327489.19143246644\n",
      "    val_loss       : -183008.5680419922\n",
      "Train Epoch: 176 [128/17352 (1%)] Loss: -303876.625000\n",
      "Train Epoch: 176 [1536/17352 (9%)] Loss: -383093.062500\n",
      "Train Epoch: 176 [2944/17352 (17%)] Loss: -384452.156250\n",
      "Train Epoch: 176 [4352/17352 (25%)] Loss: -277037.562500\n",
      "Train Epoch: 176 [5760/17352 (33%)] Loss: -397344.937500\n",
      "Train Epoch: 176 [7168/17352 (41%)] Loss: -346317.625000\n",
      "Train Epoch: 176 [8576/17352 (49%)] Loss: -384528.312500\n",
      "Train Epoch: 176 [9984/17352 (58%)] Loss: -395696.937500\n",
      "Train Epoch: 176 [11392/17352 (66%)] Loss: -387083.375000\n",
      "Train Epoch: 176 [12800/17352 (74%)] Loss: -368068.687500\n",
      "Train Epoch: 176 [14208/17352 (82%)] Loss: -284137.000000\n",
      "Train Epoch: 176 [15549/17352 (90%)] Loss: -273207.562500\n",
      "Train Epoch: 176 [16243/17352 (94%)] Loss: -227275.875000\n",
      "Train Epoch: 176 [16886/17352 (97%)] Loss: -33640.679688\n",
      "    epoch          : 176\n",
      "    loss           : -327883.7777075031\n",
      "    val_loss       : -183746.18239746094\n",
      "Train Epoch: 177 [128/17352 (1%)] Loss: -237726.500000\n",
      "Train Epoch: 177 [1536/17352 (9%)] Loss: -370082.062500\n",
      "Train Epoch: 177 [2944/17352 (17%)] Loss: -367879.250000\n",
      "Train Epoch: 177 [4352/17352 (25%)] Loss: -366379.312500\n",
      "Train Epoch: 177 [5760/17352 (33%)] Loss: -367055.468750\n",
      "Train Epoch: 177 [7168/17352 (41%)] Loss: -409079.531250\n",
      "Train Epoch: 177 [8576/17352 (49%)] Loss: -279571.375000\n",
      "Train Epoch: 177 [9984/17352 (58%)] Loss: -268528.781250\n",
      "Train Epoch: 177 [11392/17352 (66%)] Loss: -386803.312500\n",
      "Train Epoch: 177 [12800/17352 (74%)] Loss: -386694.937500\n",
      "Train Epoch: 177 [14208/17352 (82%)] Loss: -406654.875000\n",
      "Train Epoch: 177 [15512/17352 (89%)] Loss: -242963.562500\n",
      "Train Epoch: 177 [16242/17352 (94%)] Loss: -269346.812500\n",
      "Train Epoch: 177 [16961/17352 (98%)] Loss: -182159.703125\n",
      "    epoch          : 177\n",
      "    loss           : -331357.6526452391\n",
      "    val_loss       : -180503.8891113281\n",
      "Train Epoch: 178 [128/17352 (1%)] Loss: -369487.218750\n",
      "Train Epoch: 178 [1536/17352 (9%)] Loss: -387092.875000\n",
      "Train Epoch: 178 [2944/17352 (17%)] Loss: -372843.031250\n",
      "Train Epoch: 178 [4352/17352 (25%)] Loss: -357823.125000\n",
      "Train Epoch: 178 [5760/17352 (33%)] Loss: -355711.312500\n",
      "Train Epoch: 178 [7168/17352 (41%)] Loss: -362180.093750\n",
      "Train Epoch: 178 [8576/17352 (49%)] Loss: -346454.187500\n",
      "Train Epoch: 178 [9984/17352 (58%)] Loss: -360593.562500\n",
      "Train Epoch: 178 [11392/17352 (66%)] Loss: -322845.687500\n",
      "Train Epoch: 178 [12800/17352 (74%)] Loss: -401091.531250\n",
      "Train Epoch: 178 [14208/17352 (82%)] Loss: -360351.437500\n",
      "Train Epoch: 178 [15482/17352 (89%)] Loss: -99978.835938\n",
      "Train Epoch: 178 [16329/17352 (94%)] Loss: -270466.375000\n",
      "Train Epoch: 178 [17058/17352 (98%)] Loss: -216157.437500\n",
      "    epoch          : 178\n",
      "    loss           : -330055.6944899591\n",
      "    val_loss       : -182799.0169840495\n",
      "Train Epoch: 179 [128/17352 (1%)] Loss: -369899.718750\n",
      "Train Epoch: 179 [1536/17352 (9%)] Loss: -379854.562500\n",
      "Train Epoch: 179 [2944/17352 (17%)] Loss: -280071.906250\n",
      "Train Epoch: 179 [4352/17352 (25%)] Loss: -393813.750000\n",
      "Train Epoch: 179 [5760/17352 (33%)] Loss: -313476.375000\n",
      "Train Epoch: 179 [7168/17352 (41%)] Loss: -370105.312500\n",
      "Train Epoch: 179 [8576/17352 (49%)] Loss: -306055.843750\n",
      "Train Epoch: 179 [9984/17352 (58%)] Loss: -362562.125000\n",
      "Train Epoch: 179 [11392/17352 (66%)] Loss: -376923.156250\n",
      "Train Epoch: 179 [12800/17352 (74%)] Loss: -346052.781250\n",
      "Train Epoch: 179 [14208/17352 (82%)] Loss: -334906.656250\n",
      "Train Epoch: 179 [15467/17352 (89%)] Loss: -201375.609375\n",
      "Train Epoch: 179 [16197/17352 (93%)] Loss: -53463.437500\n",
      "Train Epoch: 179 [17026/17352 (98%)] Loss: -161868.796875\n",
      "    epoch          : 179\n",
      "    loss           : -329792.55104013736\n",
      "    val_loss       : -185672.1099121094\n",
      "Train Epoch: 180 [128/17352 (1%)] Loss: -344005.468750\n",
      "Train Epoch: 180 [1536/17352 (9%)] Loss: -398541.937500\n",
      "Train Epoch: 180 [2944/17352 (17%)] Loss: -381940.781250\n",
      "Train Epoch: 180 [4352/17352 (25%)] Loss: -306560.843750\n",
      "Train Epoch: 180 [5760/17352 (33%)] Loss: -386202.687500\n",
      "Train Epoch: 180 [7168/17352 (41%)] Loss: -390273.156250\n",
      "Train Epoch: 180 [8576/17352 (49%)] Loss: -395055.468750\n",
      "Train Epoch: 180 [9984/17352 (58%)] Loss: -350432.968750\n",
      "Train Epoch: 180 [11392/17352 (66%)] Loss: -392881.500000\n",
      "Train Epoch: 180 [12800/17352 (74%)] Loss: -343345.312500\n",
      "Train Epoch: 180 [14208/17352 (82%)] Loss: -326806.125000\n",
      "Train Epoch: 180 [15522/17352 (89%)] Loss: -132982.000000\n",
      "Train Epoch: 180 [16307/17352 (94%)] Loss: -109430.437500\n",
      "Train Epoch: 180 [17060/17352 (98%)] Loss: -102179.554688\n",
      "    epoch          : 180\n",
      "    loss           : -329059.1912948301\n",
      "    val_loss       : -180425.7442952474\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch180.pth ...\n",
      "Train Epoch: 181 [128/17352 (1%)] Loss: -257740.984375\n",
      "Train Epoch: 181 [1536/17352 (9%)] Loss: -343681.812500\n",
      "Train Epoch: 181 [2944/17352 (17%)] Loss: -398922.281250\n",
      "Train Epoch: 181 [4352/17352 (25%)] Loss: -388199.781250\n",
      "Train Epoch: 181 [5760/17352 (33%)] Loss: -393875.250000\n",
      "Train Epoch: 181 [7168/17352 (41%)] Loss: -331821.000000\n",
      "Train Epoch: 181 [8576/17352 (49%)] Loss: -399920.312500\n",
      "Train Epoch: 181 [9984/17352 (58%)] Loss: -332168.250000\n",
      "Train Epoch: 181 [11392/17352 (66%)] Loss: -363904.718750\n",
      "Train Epoch: 181 [12800/17352 (74%)] Loss: -397563.062500\n",
      "Train Epoch: 181 [14208/17352 (82%)] Loss: -368735.625000\n",
      "Train Epoch: 181 [15565/17352 (90%)] Loss: -332670.062500\n",
      "Train Epoch: 181 [16237/17352 (94%)] Loss: -6769.277344\n",
      "Train Epoch: 181 [17102/17352 (99%)] Loss: -88917.359375\n",
      "    epoch          : 181\n",
      "    loss           : -331954.6288669253\n",
      "    val_loss       : -184432.61673177083\n",
      "Train Epoch: 182 [128/17352 (1%)] Loss: -381533.656250\n",
      "Train Epoch: 182 [1536/17352 (9%)] Loss: -370179.343750\n",
      "Train Epoch: 182 [2944/17352 (17%)] Loss: -388175.312500\n",
      "Train Epoch: 182 [4352/17352 (25%)] Loss: -385570.500000\n",
      "Train Epoch: 182 [5760/17352 (33%)] Loss: -389831.968750\n",
      "Train Epoch: 182 [7168/17352 (41%)] Loss: -382351.750000\n",
      "Train Epoch: 182 [8576/17352 (49%)] Loss: -388910.375000\n",
      "Train Epoch: 182 [9984/17352 (58%)] Loss: -350385.937500\n",
      "Train Epoch: 182 [11392/17352 (66%)] Loss: -335486.531250\n",
      "Train Epoch: 182 [12800/17352 (74%)] Loss: -372706.906250\n",
      "Train Epoch: 182 [14208/17352 (82%)] Loss: -401389.375000\n",
      "Train Epoch: 182 [15497/17352 (89%)] Loss: -105811.359375\n",
      "Train Epoch: 182 [16352/17352 (94%)] Loss: -319106.437500\n",
      "Train Epoch: 182 [17006/17352 (98%)] Loss: -207432.140625\n",
      "    epoch          : 182\n",
      "    loss           : -332569.1470021497\n",
      "    val_loss       : -189432.3660481771\n",
      "Train Epoch: 183 [128/17352 (1%)] Loss: -345291.968750\n",
      "Train Epoch: 183 [1536/17352 (9%)] Loss: -363312.093750\n",
      "Train Epoch: 183 [2944/17352 (17%)] Loss: -339954.031250\n",
      "Train Epoch: 183 [4352/17352 (25%)] Loss: -377595.187500\n",
      "Train Epoch: 183 [5760/17352 (33%)] Loss: -356126.812500\n",
      "Train Epoch: 183 [7168/17352 (41%)] Loss: -399281.687500\n",
      "Train Epoch: 183 [8576/17352 (49%)] Loss: -337092.062500\n",
      "Train Epoch: 183 [9984/17352 (58%)] Loss: -378576.531250\n",
      "Train Epoch: 183 [11392/17352 (66%)] Loss: -386438.500000\n",
      "Train Epoch: 183 [12800/17352 (74%)] Loss: -384803.406250\n",
      "Train Epoch: 183 [14208/17352 (82%)] Loss: -389931.406250\n",
      "Train Epoch: 183 [15532/17352 (90%)] Loss: -260647.343750\n",
      "Train Epoch: 183 [16422/17352 (95%)] Loss: -204424.421875\n",
      "Train Epoch: 183 [17112/17352 (99%)] Loss: -9675.550781\n",
      "    epoch          : 183\n",
      "    loss           : -328500.4949992135\n",
      "    val_loss       : -183400.6271158854\n",
      "Train Epoch: 184 [128/17352 (1%)] Loss: -336274.625000\n",
      "Train Epoch: 184 [1536/17352 (9%)] Loss: -356071.437500\n",
      "Train Epoch: 184 [2944/17352 (17%)] Loss: -387707.656250\n",
      "Train Epoch: 184 [4352/17352 (25%)] Loss: -386945.937500\n",
      "Train Epoch: 184 [5760/17352 (33%)] Loss: -357941.781250\n",
      "Train Epoch: 184 [7168/17352 (41%)] Loss: -232187.953125\n",
      "Train Epoch: 184 [8576/17352 (49%)] Loss: -367558.500000\n",
      "Train Epoch: 184 [9984/17352 (58%)] Loss: -389944.062500\n",
      "Train Epoch: 184 [11392/17352 (66%)] Loss: -381954.187500\n",
      "Train Epoch: 184 [12800/17352 (74%)] Loss: -359145.968750\n",
      "Train Epoch: 184 [14208/17352 (82%)] Loss: -392022.437500\n",
      "Train Epoch: 184 [15541/17352 (90%)] Loss: -162812.031250\n",
      "Train Epoch: 184 [16261/17352 (94%)] Loss: -160967.750000\n",
      "Train Epoch: 184 [16955/17352 (98%)] Loss: -197136.953125\n",
      "    epoch          : 184\n",
      "    loss           : -327720.93636613886\n",
      "    val_loss       : -179760.7084309896\n",
      "Train Epoch: 185 [128/17352 (1%)] Loss: -388745.031250\n",
      "Train Epoch: 185 [1536/17352 (9%)] Loss: -385978.875000\n",
      "Train Epoch: 185 [2944/17352 (17%)] Loss: -385778.687500\n",
      "Train Epoch: 185 [4352/17352 (25%)] Loss: -378834.000000\n",
      "Train Epoch: 185 [5760/17352 (33%)] Loss: -376979.437500\n",
      "Train Epoch: 185 [7168/17352 (41%)] Loss: -343519.687500\n",
      "Train Epoch: 185 [8576/17352 (49%)] Loss: -395850.375000\n",
      "Train Epoch: 185 [9984/17352 (58%)] Loss: -376829.312500\n",
      "Train Epoch: 185 [11392/17352 (66%)] Loss: -387452.062500\n",
      "Train Epoch: 185 [12800/17352 (74%)] Loss: -386037.812500\n",
      "Train Epoch: 185 [14208/17352 (82%)] Loss: -383934.531250\n",
      "Train Epoch: 185 [15488/17352 (89%)] Loss: -232059.500000\n",
      "Train Epoch: 185 [16280/17352 (94%)] Loss: -198145.781250\n",
      "Train Epoch: 185 [16949/17352 (98%)] Loss: -92760.062500\n",
      "    epoch          : 185\n",
      "    loss           : -332511.5909579488\n",
      "    val_loss       : -188755.09095052083\n",
      "Train Epoch: 186 [128/17352 (1%)] Loss: -366748.250000\n",
      "Train Epoch: 186 [1536/17352 (9%)] Loss: -359280.687500\n",
      "Train Epoch: 186 [2944/17352 (17%)] Loss: -396267.125000\n",
      "Train Epoch: 186 [4352/17352 (25%)] Loss: -283561.468750\n",
      "Train Epoch: 186 [5760/17352 (33%)] Loss: -381328.031250\n",
      "Train Epoch: 186 [7168/17352 (41%)] Loss: -275857.468750\n",
      "Train Epoch: 186 [8576/17352 (49%)] Loss: -395044.093750\n",
      "Train Epoch: 186 [9984/17352 (58%)] Loss: -380035.093750\n",
      "Train Epoch: 186 [11392/17352 (66%)] Loss: -391580.000000\n",
      "Train Epoch: 186 [12800/17352 (74%)] Loss: -379068.625000\n",
      "Train Epoch: 186 [14208/17352 (82%)] Loss: -377260.125000\n",
      "Train Epoch: 186 [15530/17352 (89%)] Loss: -374864.187500\n",
      "Train Epoch: 186 [16317/17352 (94%)] Loss: -289385.625000\n",
      "Train Epoch: 186 [17208/17352 (99%)] Loss: -177525.687500\n",
      "    epoch          : 186\n",
      "    loss           : -332067.79058633075\n",
      "    val_loss       : -181635.54498697916\n",
      "Train Epoch: 187 [128/17352 (1%)] Loss: -399842.437500\n",
      "Train Epoch: 187 [1536/17352 (9%)] Loss: -382189.000000\n",
      "Train Epoch: 187 [2944/17352 (17%)] Loss: -364031.437500\n",
      "Train Epoch: 187 [4352/17352 (25%)] Loss: -385395.812500\n",
      "Train Epoch: 187 [5760/17352 (33%)] Loss: -391537.312500\n",
      "Train Epoch: 187 [7168/17352 (41%)] Loss: -342580.875000\n",
      "Train Epoch: 187 [8576/17352 (49%)] Loss: -324542.906250\n",
      "Train Epoch: 187 [9984/17352 (58%)] Loss: -359457.968750\n",
      "Train Epoch: 187 [11392/17352 (66%)] Loss: -377632.000000\n",
      "Train Epoch: 187 [12800/17352 (74%)] Loss: -378646.875000\n",
      "Train Epoch: 187 [14208/17352 (82%)] Loss: -369586.000000\n",
      "Train Epoch: 187 [15503/17352 (89%)] Loss: -98436.617188\n",
      "Train Epoch: 187 [16105/17352 (93%)] Loss: -230791.562500\n",
      "Train Epoch: 187 [16943/17352 (98%)] Loss: -257364.765625\n",
      "    epoch          : 187\n",
      "    loss           : -334994.2204475147\n",
      "    val_loss       : -190252.6069824219\n",
      "Train Epoch: 188 [128/17352 (1%)] Loss: -376855.343750\n",
      "Train Epoch: 188 [1536/17352 (9%)] Loss: -378385.406250\n",
      "Train Epoch: 188 [2944/17352 (17%)] Loss: -396579.156250\n",
      "Train Epoch: 188 [4352/17352 (25%)] Loss: -376574.000000\n",
      "Train Epoch: 188 [5760/17352 (33%)] Loss: -374321.750000\n",
      "Train Epoch: 188 [7168/17352 (41%)] Loss: -382234.875000\n",
      "Train Epoch: 188 [8576/17352 (49%)] Loss: -389721.687500\n",
      "Train Epoch: 188 [9984/17352 (58%)] Loss: -366912.750000\n",
      "Train Epoch: 188 [11392/17352 (66%)] Loss: -310415.656250\n",
      "Train Epoch: 188 [12800/17352 (74%)] Loss: -435071.968750\n",
      "Train Epoch: 188 [14208/17352 (82%)] Loss: -398911.875000\n",
      "Train Epoch: 188 [15491/17352 (89%)] Loss: -167578.937500\n",
      "Train Epoch: 188 [16219/17352 (93%)] Loss: -111116.296875\n",
      "Train Epoch: 188 [17051/17352 (98%)] Loss: -227999.781250\n",
      "    epoch          : 188\n",
      "    loss           : -337792.3826289849\n",
      "    val_loss       : -187603.73307291666\n",
      "Train Epoch: 189 [128/17352 (1%)] Loss: -394542.031250\n",
      "Train Epoch: 189 [1536/17352 (9%)] Loss: -374822.875000\n",
      "Train Epoch: 189 [2944/17352 (17%)] Loss: -372216.593750\n",
      "Train Epoch: 189 [4352/17352 (25%)] Loss: -308182.281250\n",
      "Train Epoch: 189 [5760/17352 (33%)] Loss: -367317.687500\n",
      "Train Epoch: 189 [7168/17352 (41%)] Loss: -360665.562500\n",
      "Train Epoch: 189 [8576/17352 (49%)] Loss: -410392.812500\n",
      "Train Epoch: 189 [9984/17352 (58%)] Loss: -366248.625000\n",
      "Train Epoch: 189 [11392/17352 (66%)] Loss: -378127.875000\n",
      "Train Epoch: 189 [12800/17352 (74%)] Loss: -379103.625000\n",
      "Train Epoch: 189 [14208/17352 (82%)] Loss: -359214.718750\n",
      "Train Epoch: 189 [15514/17352 (89%)] Loss: -222006.359375\n",
      "Train Epoch: 189 [16275/17352 (94%)] Loss: -230648.531250\n",
      "Train Epoch: 189 [17054/17352 (98%)] Loss: -172610.312500\n",
      "    epoch          : 189\n",
      "    loss           : -335991.0499750944\n",
      "    val_loss       : -189764.15123697917\n",
      "Train Epoch: 190 [128/17352 (1%)] Loss: -405136.937500\n",
      "Train Epoch: 190 [1536/17352 (9%)] Loss: -396755.625000\n",
      "Train Epoch: 190 [2944/17352 (17%)] Loss: -317884.312500\n",
      "Train Epoch: 190 [4352/17352 (25%)] Loss: -350950.875000\n",
      "Train Epoch: 190 [5760/17352 (33%)] Loss: -312220.562500\n",
      "Train Epoch: 190 [7168/17352 (41%)] Loss: -279072.281250\n",
      "Train Epoch: 190 [8576/17352 (49%)] Loss: -395425.468750\n",
      "Train Epoch: 190 [9984/17352 (58%)] Loss: -391931.750000\n",
      "Train Epoch: 190 [11392/17352 (66%)] Loss: -376738.187500\n",
      "Train Epoch: 190 [12800/17352 (74%)] Loss: -400717.562500\n",
      "Train Epoch: 190 [14208/17352 (82%)] Loss: -265054.593750\n",
      "Train Epoch: 190 [15482/17352 (89%)] Loss: -94521.625000\n",
      "Train Epoch: 190 [16234/17352 (94%)] Loss: -175974.156250\n",
      "Train Epoch: 190 [17010/17352 (98%)] Loss: -252436.296875\n",
      "    epoch          : 190\n",
      "    loss           : -341506.260355495\n",
      "    val_loss       : -187202.13697916668\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch190.pth ...\n",
      "Train Epoch: 191 [128/17352 (1%)] Loss: -351476.781250\n",
      "Train Epoch: 191 [1536/17352 (9%)] Loss: -366513.250000\n",
      "Train Epoch: 191 [2944/17352 (17%)] Loss: -403638.312500\n",
      "Train Epoch: 191 [4352/17352 (25%)] Loss: -294883.593750\n",
      "Train Epoch: 191 [5760/17352 (33%)] Loss: -368326.562500\n",
      "Train Epoch: 191 [7168/17352 (41%)] Loss: -390119.312500\n",
      "Train Epoch: 191 [8576/17352 (49%)] Loss: -398445.937500\n",
      "Train Epoch: 191 [9984/17352 (58%)] Loss: -391008.156250\n",
      "Train Epoch: 191 [11392/17352 (66%)] Loss: -383487.218750\n",
      "Train Epoch: 191 [12800/17352 (74%)] Loss: -339768.500000\n",
      "Train Epoch: 191 [14208/17352 (82%)] Loss: -362972.250000\n",
      "Train Epoch: 191 [15531/17352 (90%)] Loss: -82208.750000\n",
      "Train Epoch: 191 [16365/17352 (94%)] Loss: -302004.656250\n",
      "Train Epoch: 191 [16902/17352 (97%)] Loss: -129344.476562\n",
      "    epoch          : 191\n",
      "    loss           : -338135.2791002517\n",
      "    val_loss       : -182609.07752278645\n",
      "Train Epoch: 192 [128/17352 (1%)] Loss: -366189.750000\n",
      "Train Epoch: 192 [1536/17352 (9%)] Loss: -310297.281250\n",
      "Train Epoch: 192 [2944/17352 (17%)] Loss: -343155.812500\n",
      "Train Epoch: 192 [4352/17352 (25%)] Loss: -397889.187500\n",
      "Train Epoch: 192 [5760/17352 (33%)] Loss: -401555.437500\n",
      "Train Epoch: 192 [7168/17352 (41%)] Loss: -351745.562500\n",
      "Train Epoch: 192 [8576/17352 (49%)] Loss: -409921.500000\n",
      "Train Epoch: 192 [9984/17352 (58%)] Loss: -384076.156250\n",
      "Train Epoch: 192 [11392/17352 (66%)] Loss: -390045.656250\n",
      "Train Epoch: 192 [12800/17352 (74%)] Loss: -338119.937500\n",
      "Train Epoch: 192 [14208/17352 (82%)] Loss: -355293.187500\n",
      "Train Epoch: 192 [15561/17352 (90%)] Loss: -293276.906250\n",
      "Train Epoch: 192 [16384/17352 (94%)] Loss: -270027.000000\n",
      "Train Epoch: 192 [17163/17352 (99%)] Loss: -171030.406250\n",
      "    epoch          : 192\n",
      "    loss           : -338984.9298185822\n",
      "    val_loss       : -189171.08369140624\n",
      "Train Epoch: 193 [128/17352 (1%)] Loss: -378174.656250\n",
      "Train Epoch: 193 [1536/17352 (9%)] Loss: -387287.812500\n",
      "Train Epoch: 193 [2944/17352 (17%)] Loss: -359850.750000\n",
      "Train Epoch: 193 [4352/17352 (25%)] Loss: -390364.281250\n",
      "Train Epoch: 193 [5760/17352 (33%)] Loss: -391933.625000\n",
      "Train Epoch: 193 [7168/17352 (41%)] Loss: -395679.218750\n",
      "Train Epoch: 193 [8576/17352 (49%)] Loss: -386131.906250\n",
      "Train Epoch: 193 [9984/17352 (58%)] Loss: -387142.687500\n",
      "Train Epoch: 193 [11392/17352 (66%)] Loss: -354022.093750\n",
      "Train Epoch: 193 [12800/17352 (74%)] Loss: -399373.062500\n",
      "Train Epoch: 193 [14208/17352 (82%)] Loss: -405084.625000\n",
      "Train Epoch: 193 [15492/17352 (89%)] Loss: -165893.828125\n",
      "Train Epoch: 193 [16145/17352 (93%)] Loss: -129566.976562\n",
      "Train Epoch: 193 [16908/17352 (97%)] Loss: -192565.062500\n",
      "    epoch          : 193\n",
      "    loss           : -336002.2118485214\n",
      "    val_loss       : -188084.7787923177\n",
      "Train Epoch: 194 [128/17352 (1%)] Loss: -322890.906250\n",
      "Train Epoch: 194 [1536/17352 (9%)] Loss: -429847.875000\n",
      "Train Epoch: 194 [2944/17352 (17%)] Loss: -317229.281250\n",
      "Train Epoch: 194 [4352/17352 (25%)] Loss: -354322.281250\n",
      "Train Epoch: 194 [5760/17352 (33%)] Loss: -395636.937500\n",
      "Train Epoch: 194 [7168/17352 (41%)] Loss: -373385.531250\n",
      "Train Epoch: 194 [8576/17352 (49%)] Loss: -391914.968750\n",
      "Train Epoch: 194 [9984/17352 (58%)] Loss: -368210.125000\n",
      "Train Epoch: 194 [11392/17352 (66%)] Loss: -274319.625000\n",
      "Train Epoch: 194 [12800/17352 (74%)] Loss: -411289.593750\n",
      "Train Epoch: 194 [14208/17352 (82%)] Loss: -390217.406250\n",
      "Train Epoch: 194 [15506/17352 (89%)] Loss: -147656.500000\n",
      "Train Epoch: 194 [16182/17352 (93%)] Loss: -114110.023438\n",
      "Train Epoch: 194 [17009/17352 (98%)] Loss: -79783.867188\n",
      "    epoch          : 194\n",
      "    loss           : -341197.81526583474\n",
      "    val_loss       : -186454.92928059897\n",
      "Train Epoch: 195 [128/17352 (1%)] Loss: -386973.875000\n",
      "Train Epoch: 195 [1536/17352 (9%)] Loss: -416619.375000\n",
      "Train Epoch: 195 [2944/17352 (17%)] Loss: -411936.062500\n",
      "Train Epoch: 195 [4352/17352 (25%)] Loss: -347173.000000\n",
      "Train Epoch: 195 [5760/17352 (33%)] Loss: -407066.562500\n",
      "Train Epoch: 195 [7168/17352 (41%)] Loss: -404676.250000\n",
      "Train Epoch: 195 [8576/17352 (49%)] Loss: -397052.125000\n",
      "Train Epoch: 195 [9984/17352 (58%)] Loss: -399895.125000\n",
      "Train Epoch: 195 [11392/17352 (66%)] Loss: -352529.125000\n",
      "Train Epoch: 195 [12800/17352 (74%)] Loss: -348052.812500\n",
      "Train Epoch: 195 [14208/17352 (82%)] Loss: -369515.218750\n",
      "Train Epoch: 195 [15449/17352 (89%)] Loss: -9395.324219\n",
      "Train Epoch: 195 [16212/17352 (93%)] Loss: -396483.156250\n",
      "Train Epoch: 195 [17015/17352 (98%)] Loss: -129702.218750\n",
      "    epoch          : 195\n",
      "    loss           : -335687.9567625315\n",
      "    val_loss       : -188558.53693033854\n",
      "Train Epoch: 196 [128/17352 (1%)] Loss: -353249.406250\n",
      "Train Epoch: 196 [1536/17352 (9%)] Loss: -342281.500000\n",
      "Train Epoch: 196 [2944/17352 (17%)] Loss: -395010.406250\n",
      "Train Epoch: 196 [4352/17352 (25%)] Loss: -400733.406250\n",
      "Train Epoch: 196 [5760/17352 (33%)] Loss: -383190.437500\n",
      "Train Epoch: 196 [7168/17352 (41%)] Loss: -378459.031250\n",
      "Train Epoch: 196 [8576/17352 (49%)] Loss: -372224.187500\n",
      "Train Epoch: 196 [9984/17352 (58%)] Loss: -403202.031250\n",
      "Train Epoch: 196 [11392/17352 (66%)] Loss: -396562.312500\n",
      "Train Epoch: 196 [12800/17352 (74%)] Loss: -403252.593750\n",
      "Train Epoch: 196 [14208/17352 (82%)] Loss: -375282.500000\n",
      "Train Epoch: 196 [15448/17352 (89%)] Loss: -160382.312500\n",
      "Train Epoch: 196 [16229/17352 (94%)] Loss: -162357.187500\n",
      "Train Epoch: 196 [16941/17352 (98%)] Loss: -352066.500000\n",
      "    epoch          : 196\n",
      "    loss           : -340319.79627202183\n",
      "    val_loss       : -190262.82890625\n",
      "Train Epoch: 197 [128/17352 (1%)] Loss: -365093.250000\n",
      "Train Epoch: 197 [1536/17352 (9%)] Loss: -385636.937500\n",
      "Train Epoch: 197 [2944/17352 (17%)] Loss: -345867.312500\n",
      "Train Epoch: 197 [4352/17352 (25%)] Loss: -326942.218750\n",
      "Train Epoch: 197 [5760/17352 (33%)] Loss: -376840.437500\n",
      "Train Epoch: 197 [7168/17352 (41%)] Loss: -392909.437500\n",
      "Train Epoch: 197 [8576/17352 (49%)] Loss: -402129.156250\n",
      "Train Epoch: 197 [9984/17352 (58%)] Loss: -418621.875000\n",
      "Train Epoch: 197 [11392/17352 (66%)] Loss: -354852.875000\n",
      "Train Epoch: 197 [12800/17352 (74%)] Loss: -384819.843750\n",
      "Train Epoch: 197 [14208/17352 (82%)] Loss: -405125.187500\n",
      "Train Epoch: 197 [15541/17352 (90%)] Loss: -202204.328125\n",
      "Train Epoch: 197 [16044/17352 (92%)] Loss: -97362.882812\n",
      "Train Epoch: 197 [16964/17352 (98%)] Loss: -343493.062500\n",
      "    epoch          : 197\n",
      "    loss           : -340913.6958564912\n",
      "    val_loss       : -185252.42607421876\n",
      "Train Epoch: 198 [128/17352 (1%)] Loss: -311307.687500\n",
      "Train Epoch: 198 [1536/17352 (9%)] Loss: -386485.187500\n",
      "Train Epoch: 198 [2944/17352 (17%)] Loss: -377491.437500\n",
      "Train Epoch: 198 [4352/17352 (25%)] Loss: -396614.625000\n",
      "Train Epoch: 198 [5760/17352 (33%)] Loss: -389289.843750\n",
      "Train Epoch: 198 [7168/17352 (41%)] Loss: -376372.937500\n",
      "Train Epoch: 198 [8576/17352 (49%)] Loss: -261178.281250\n",
      "Train Epoch: 198 [9984/17352 (58%)] Loss: -350478.562500\n",
      "Train Epoch: 198 [11392/17352 (66%)] Loss: -380068.031250\n",
      "Train Epoch: 198 [12800/17352 (74%)] Loss: -353872.218750\n",
      "Train Epoch: 198 [14208/17352 (82%)] Loss: -378514.312500\n",
      "Train Epoch: 198 [15531/17352 (90%)] Loss: -260008.828125\n",
      "Train Epoch: 198 [16294/17352 (94%)] Loss: -52423.750000\n",
      "Train Epoch: 198 [17074/17352 (98%)] Loss: -161762.625000\n",
      "    epoch          : 198\n",
      "    loss           : -345780.2960164115\n",
      "    val_loss       : -188858.24314778644\n",
      "Train Epoch: 199 [128/17352 (1%)] Loss: -401011.062500\n",
      "Train Epoch: 199 [1536/17352 (9%)] Loss: -354186.562500\n",
      "Train Epoch: 199 [2944/17352 (17%)] Loss: -420742.500000\n",
      "Train Epoch: 199 [4352/17352 (25%)] Loss: -402772.468750\n",
      "Train Epoch: 199 [5760/17352 (33%)] Loss: -376254.781250\n",
      "Train Epoch: 199 [7168/17352 (41%)] Loss: -397900.375000\n",
      "Train Epoch: 199 [8576/17352 (49%)] Loss: -382805.687500\n",
      "Train Epoch: 199 [9984/17352 (58%)] Loss: -396774.218750\n",
      "Train Epoch: 199 [11392/17352 (66%)] Loss: -405158.937500\n",
      "Train Epoch: 199 [12800/17352 (74%)] Loss: -386876.937500\n",
      "Train Epoch: 199 [14208/17352 (82%)] Loss: -412262.000000\n",
      "Train Epoch: 199 [15484/17352 (89%)] Loss: -281551.000000\n",
      "Train Epoch: 199 [16220/17352 (93%)] Loss: -219022.562500\n",
      "Train Epoch: 199 [17018/17352 (98%)] Loss: -272636.750000\n",
      "    epoch          : 199\n",
      "    loss           : -348523.3586278314\n",
      "    val_loss       : -187561.95084635416\n",
      "Train Epoch: 200 [128/17352 (1%)] Loss: -383145.625000\n",
      "Train Epoch: 200 [1536/17352 (9%)] Loss: -410835.625000\n",
      "Train Epoch: 200 [2944/17352 (17%)] Loss: -350301.562500\n",
      "Train Epoch: 200 [4352/17352 (25%)] Loss: -387474.468750\n",
      "Train Epoch: 200 [5760/17352 (33%)] Loss: -329893.437500\n",
      "Train Epoch: 200 [7168/17352 (41%)] Loss: -395994.625000\n",
      "Train Epoch: 200 [8576/17352 (49%)] Loss: -399668.875000\n",
      "Train Epoch: 200 [9984/17352 (58%)] Loss: -382965.937500\n",
      "Train Epoch: 200 [11392/17352 (66%)] Loss: -403015.812500\n",
      "Train Epoch: 200 [12800/17352 (74%)] Loss: -393049.062500\n",
      "Train Epoch: 200 [14208/17352 (82%)] Loss: -398467.875000\n",
      "Train Epoch: 200 [15503/17352 (89%)] Loss: -106757.781250\n",
      "Train Epoch: 200 [16403/17352 (95%)] Loss: -239105.937500\n",
      "Train Epoch: 200 [17023/17352 (98%)] Loss: -124273.554688\n",
      "    epoch          : 200\n",
      "    loss           : -347918.3945246959\n",
      "    val_loss       : -190756.1432454427\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch200.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 201 [128/17352 (1%)] Loss: -400694.656250\n",
      "Train Epoch: 201 [1536/17352 (9%)] Loss: -349198.250000\n",
      "Train Epoch: 201 [2944/17352 (17%)] Loss: -425681.937500\n",
      "Train Epoch: 201 [4352/17352 (25%)] Loss: -375131.718750\n",
      "Train Epoch: 201 [5760/17352 (33%)] Loss: -391711.093750\n",
      "Train Epoch: 201 [7168/17352 (41%)] Loss: -392257.562500\n",
      "Train Epoch: 201 [8576/17352 (49%)] Loss: -408686.156250\n",
      "Train Epoch: 201 [9984/17352 (58%)] Loss: -414667.000000\n",
      "Train Epoch: 201 [11392/17352 (66%)] Loss: -404553.937500\n",
      "Train Epoch: 201 [12800/17352 (74%)] Loss: -404666.812500\n",
      "Train Epoch: 201 [14208/17352 (82%)] Loss: -334943.843750\n",
      "Train Epoch: 201 [15441/17352 (89%)] Loss: -251799.375000\n",
      "Train Epoch: 201 [16235/17352 (94%)] Loss: -260426.500000\n",
      "Train Epoch: 201 [16944/17352 (98%)] Loss: -81004.625000\n",
      "    epoch          : 201\n",
      "    loss           : -340715.5645481596\n",
      "    val_loss       : -193648.80776367188\n",
      "Train Epoch: 202 [128/17352 (1%)] Loss: -361657.312500\n",
      "Train Epoch: 202 [1536/17352 (9%)] Loss: -277604.593750\n",
      "Train Epoch: 202 [2944/17352 (17%)] Loss: -387832.218750\n",
      "Train Epoch: 202 [4352/17352 (25%)] Loss: -407930.875000\n",
      "Train Epoch: 202 [5760/17352 (33%)] Loss: -388226.781250\n",
      "Train Epoch: 202 [7168/17352 (41%)] Loss: -355928.375000\n",
      "Train Epoch: 202 [8576/17352 (49%)] Loss: -404915.562500\n",
      "Train Epoch: 202 [9984/17352 (58%)] Loss: -384352.656250\n",
      "Train Epoch: 202 [11392/17352 (66%)] Loss: -395881.875000\n",
      "Train Epoch: 202 [12800/17352 (74%)] Loss: -416946.812500\n",
      "Train Epoch: 202 [14208/17352 (82%)] Loss: -392818.187500\n",
      "Train Epoch: 202 [15538/17352 (90%)] Loss: -253946.265625\n",
      "Train Epoch: 202 [16217/17352 (93%)] Loss: -48502.281250\n",
      "Train Epoch: 202 [17020/17352 (98%)] Loss: -129651.421875\n",
      "    epoch          : 202\n",
      "    loss           : -344911.46299876785\n",
      "    val_loss       : -187935.11697591146\n",
      "Train Epoch: 203 [128/17352 (1%)] Loss: -343973.187500\n",
      "Train Epoch: 203 [1536/17352 (9%)] Loss: -392318.718750\n",
      "Train Epoch: 203 [2944/17352 (17%)] Loss: -381046.875000\n",
      "Train Epoch: 203 [4352/17352 (25%)] Loss: -325473.406250\n",
      "Train Epoch: 203 [5760/17352 (33%)] Loss: -363233.343750\n",
      "Train Epoch: 203 [7168/17352 (41%)] Loss: -360535.906250\n",
      "Train Epoch: 203 [8576/17352 (49%)] Loss: -413747.468750\n",
      "Train Epoch: 203 [9984/17352 (58%)] Loss: -403487.625000\n",
      "Train Epoch: 203 [11392/17352 (66%)] Loss: -389679.125000\n",
      "Train Epoch: 203 [12800/17352 (74%)] Loss: -406685.125000\n",
      "Train Epoch: 203 [14208/17352 (82%)] Loss: -392278.343750\n",
      "Train Epoch: 203 [15416/17352 (89%)] Loss: -50523.984375\n",
      "Train Epoch: 203 [16329/17352 (94%)] Loss: -185658.343750\n",
      "Train Epoch: 203 [16961/17352 (98%)] Loss: -215407.484375\n",
      "    epoch          : 203\n",
      "    loss           : -338164.11836723995\n",
      "    val_loss       : -188333.38680013022\n",
      "Train Epoch: 204 [128/17352 (1%)] Loss: -350610.156250\n",
      "Train Epoch: 204 [1536/17352 (9%)] Loss: -395223.125000\n",
      "Train Epoch: 204 [2944/17352 (17%)] Loss: -311313.125000\n",
      "Train Epoch: 204 [4352/17352 (25%)] Loss: -400723.625000\n",
      "Train Epoch: 204 [5760/17352 (33%)] Loss: -414002.781250\n",
      "Train Epoch: 204 [7168/17352 (41%)] Loss: -407084.187500\n",
      "Train Epoch: 204 [8576/17352 (49%)] Loss: -346248.250000\n",
      "Train Epoch: 204 [9984/17352 (58%)] Loss: -347117.968750\n",
      "Train Epoch: 204 [11392/17352 (66%)] Loss: -382837.187500\n",
      "Train Epoch: 204 [12800/17352 (74%)] Loss: -392425.093750\n",
      "Train Epoch: 204 [14208/17352 (82%)] Loss: -398322.312500\n",
      "Train Epoch: 204 [15523/17352 (89%)] Loss: -222551.468750\n",
      "Train Epoch: 204 [16344/17352 (94%)] Loss: -172427.140625\n",
      "Train Epoch: 204 [16986/17352 (98%)] Loss: -130913.734375\n",
      "    epoch          : 204\n",
      "    loss           : -347090.96294633497\n",
      "    val_loss       : -193055.9071126302\n",
      "Train Epoch: 205 [128/17352 (1%)] Loss: -400779.062500\n",
      "Train Epoch: 205 [1536/17352 (9%)] Loss: -398625.000000\n",
      "Train Epoch: 205 [2944/17352 (17%)] Loss: -416555.718750\n",
      "Train Epoch: 205 [4352/17352 (25%)] Loss: -398685.312500\n",
      "Train Epoch: 205 [5760/17352 (33%)] Loss: -395065.187500\n",
      "Train Epoch: 205 [7168/17352 (41%)] Loss: -404444.375000\n",
      "Train Epoch: 205 [8576/17352 (49%)] Loss: -417911.718750\n",
      "Train Epoch: 205 [9984/17352 (58%)] Loss: -378071.562500\n",
      "Train Epoch: 205 [11392/17352 (66%)] Loss: -398968.031250\n",
      "Train Epoch: 205 [12800/17352 (74%)] Loss: -401386.312500\n",
      "Train Epoch: 205 [14208/17352 (82%)] Loss: -374113.812500\n",
      "Train Epoch: 205 [15479/17352 (89%)] Loss: -93932.140625\n",
      "Train Epoch: 205 [16331/17352 (94%)] Loss: -266708.250000\n",
      "Train Epoch: 205 [17120/17352 (99%)] Loss: -147572.062500\n",
      "    epoch          : 205\n",
      "    loss           : -344820.62007130875\n",
      "    val_loss       : -190603.72454427084\n",
      "Train Epoch: 206 [128/17352 (1%)] Loss: -413843.031250\n",
      "Train Epoch: 206 [1536/17352 (9%)] Loss: -395621.656250\n",
      "Train Epoch: 206 [2944/17352 (17%)] Loss: -339351.000000\n",
      "Train Epoch: 206 [4352/17352 (25%)] Loss: -337035.000000\n",
      "Train Epoch: 206 [5760/17352 (33%)] Loss: -393512.906250\n",
      "Train Epoch: 206 [7168/17352 (41%)] Loss: -418083.562500\n",
      "Train Epoch: 206 [8576/17352 (49%)] Loss: -393815.343750\n",
      "Train Epoch: 206 [9984/17352 (58%)] Loss: -384348.343750\n",
      "Train Epoch: 206 [11392/17352 (66%)] Loss: -392230.718750\n",
      "Train Epoch: 206 [12800/17352 (74%)] Loss: -378187.218750\n",
      "Train Epoch: 206 [14208/17352 (82%)] Loss: -352914.718750\n",
      "Train Epoch: 206 [15508/17352 (89%)] Loss: -127714.984375\n",
      "Train Epoch: 206 [16429/17352 (95%)] Loss: -236770.125000\n",
      "Train Epoch: 206 [17102/17352 (99%)] Loss: -283508.156250\n",
      "    epoch          : 206\n",
      "    loss           : -341072.7129102873\n",
      "    val_loss       : -180527.15540364583\n",
      "Train Epoch: 207 [128/17352 (1%)] Loss: -385762.125000\n",
      "Train Epoch: 207 [1536/17352 (9%)] Loss: -401035.750000\n",
      "Train Epoch: 207 [2944/17352 (17%)] Loss: -404706.187500\n",
      "Train Epoch: 207 [4352/17352 (25%)] Loss: -405060.437500\n",
      "Train Epoch: 207 [5760/17352 (33%)] Loss: -297727.687500\n",
      "Train Epoch: 207 [7168/17352 (41%)] Loss: -403164.593750\n",
      "Train Epoch: 207 [8576/17352 (49%)] Loss: -388674.687500\n",
      "Train Epoch: 207 [9984/17352 (58%)] Loss: -409656.968750\n",
      "Train Epoch: 207 [11392/17352 (66%)] Loss: -402330.250000\n",
      "Train Epoch: 207 [12800/17352 (74%)] Loss: -364633.062500\n",
      "Train Epoch: 207 [14208/17352 (82%)] Loss: -388695.250000\n",
      "Train Epoch: 207 [15500/17352 (89%)] Loss: -390187.468750\n",
      "Train Epoch: 207 [16312/17352 (94%)] Loss: -94878.937500\n",
      "Train Epoch: 207 [17034/17352 (98%)] Loss: -40709.031250\n",
      "    epoch          : 207\n",
      "    loss           : -347066.6440527999\n",
      "    val_loss       : -187245.12669270832\n",
      "Train Epoch: 208 [128/17352 (1%)] Loss: -385397.750000\n",
      "Train Epoch: 208 [1536/17352 (9%)] Loss: -389449.437500\n",
      "Train Epoch: 208 [2944/17352 (17%)] Loss: -387057.218750\n",
      "Train Epoch: 208 [4352/17352 (25%)] Loss: -390610.468750\n",
      "Train Epoch: 208 [5760/17352 (33%)] Loss: -416067.250000\n",
      "Train Epoch: 208 [7168/17352 (41%)] Loss: -250553.531250\n",
      "Train Epoch: 208 [8576/17352 (49%)] Loss: -369389.625000\n",
      "Train Epoch: 208 [9984/17352 (58%)] Loss: -387695.062500\n",
      "Train Epoch: 208 [11392/17352 (66%)] Loss: -380916.312500\n",
      "Train Epoch: 208 [12800/17352 (74%)] Loss: -375163.250000\n",
      "Train Epoch: 208 [14208/17352 (82%)] Loss: -356479.718750\n",
      "Train Epoch: 208 [15460/17352 (89%)] Loss: -46690.910156\n",
      "Train Epoch: 208 [16233/17352 (94%)] Loss: -14941.447266\n",
      "Train Epoch: 208 [17089/17352 (98%)] Loss: -107852.546875\n",
      "    epoch          : 208\n",
      "    loss           : -343708.9857743026\n",
      "    val_loss       : -182372.87337239584\n",
      "Train Epoch: 209 [128/17352 (1%)] Loss: -388654.000000\n",
      "Train Epoch: 209 [1536/17352 (9%)] Loss: -323273.375000\n",
      "Train Epoch: 209 [2944/17352 (17%)] Loss: -360530.656250\n",
      "Train Epoch: 209 [4352/17352 (25%)] Loss: -401296.531250\n",
      "Train Epoch: 209 [5760/17352 (33%)] Loss: -395919.593750\n",
      "Train Epoch: 209 [7168/17352 (41%)] Loss: -414394.062500\n",
      "Train Epoch: 209 [8576/17352 (49%)] Loss: -349936.593750\n",
      "Train Epoch: 209 [9984/17352 (58%)] Loss: -396078.343750\n",
      "Train Epoch: 209 [11392/17352 (66%)] Loss: -385789.906250\n",
      "Train Epoch: 209 [12800/17352 (74%)] Loss: -364498.312500\n",
      "Train Epoch: 209 [14208/17352 (82%)] Loss: -385170.562500\n",
      "Train Epoch: 209 [15556/17352 (90%)] Loss: -315263.843750\n",
      "Train Epoch: 209 [16325/17352 (94%)] Loss: -277957.500000\n",
      "Train Epoch: 209 [16950/17352 (98%)] Loss: -300724.562500\n",
      "    epoch          : 209\n",
      "    loss           : -336107.34354682255\n",
      "    val_loss       : -190828.3676595052\n",
      "Train Epoch: 210 [128/17352 (1%)] Loss: -297459.687500\n",
      "Train Epoch: 210 [1536/17352 (9%)] Loss: -388606.656250\n",
      "Train Epoch: 210 [2944/17352 (17%)] Loss: -393008.312500\n",
      "Train Epoch: 210 [4352/17352 (25%)] Loss: -286227.562500\n",
      "Train Epoch: 210 [5760/17352 (33%)] Loss: -318661.531250\n",
      "Train Epoch: 210 [7168/17352 (41%)] Loss: -383417.625000\n",
      "Train Epoch: 210 [8576/17352 (49%)] Loss: -357070.000000\n",
      "Train Epoch: 210 [9984/17352 (58%)] Loss: -358476.937500\n",
      "Train Epoch: 210 [11392/17352 (66%)] Loss: -386502.343750\n",
      "Train Epoch: 210 [12800/17352 (74%)] Loss: -383397.968750\n",
      "Train Epoch: 210 [14208/17352 (82%)] Loss: -384946.187500\n",
      "Train Epoch: 210 [15570/17352 (90%)] Loss: -284749.125000\n",
      "Train Epoch: 210 [16337/17352 (94%)] Loss: -236684.062500\n",
      "Train Epoch: 210 [16974/17352 (98%)] Loss: -247824.921875\n",
      "    epoch          : 210\n",
      "    loss           : -335814.2377962458\n",
      "    val_loss       : -186935.45891927084\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch210.pth ...\n",
      "Train Epoch: 211 [128/17352 (1%)] Loss: -383611.093750\n",
      "Train Epoch: 211 [1536/17352 (9%)] Loss: -295347.750000\n",
      "Train Epoch: 211 [2944/17352 (17%)] Loss: -392066.875000\n",
      "Train Epoch: 211 [4352/17352 (25%)] Loss: -378903.218750\n",
      "Train Epoch: 211 [5760/17352 (33%)] Loss: -307687.625000\n",
      "Train Epoch: 211 [7168/17352 (41%)] Loss: -409524.250000\n",
      "Train Epoch: 211 [8576/17352 (49%)] Loss: -408056.937500\n",
      "Train Epoch: 211 [9984/17352 (58%)] Loss: -385133.343750\n",
      "Train Epoch: 211 [11392/17352 (66%)] Loss: -362123.500000\n",
      "Train Epoch: 211 [12800/17352 (74%)] Loss: -393612.625000\n",
      "Train Epoch: 211 [14208/17352 (82%)] Loss: -389257.875000\n",
      "Train Epoch: 211 [15484/17352 (89%)] Loss: -252134.468750\n",
      "Train Epoch: 211 [16295/17352 (94%)] Loss: -45925.468750\n",
      "Train Epoch: 211 [16970/17352 (98%)] Loss: -233562.015625\n",
      "    epoch          : 211\n",
      "    loss           : -339564.0224609375\n",
      "    val_loss       : -191552.07369791667\n",
      "Train Epoch: 212 [128/17352 (1%)] Loss: -347089.281250\n",
      "Train Epoch: 212 [1536/17352 (9%)] Loss: -407144.500000\n",
      "Train Epoch: 212 [2944/17352 (17%)] Loss: -378899.781250\n",
      "Train Epoch: 212 [4352/17352 (25%)] Loss: -399563.562500\n",
      "Train Epoch: 212 [5760/17352 (33%)] Loss: -400675.125000\n",
      "Train Epoch: 212 [7168/17352 (41%)] Loss: -385507.750000\n",
      "Train Epoch: 212 [8576/17352 (49%)] Loss: -367742.718750\n",
      "Train Epoch: 212 [9984/17352 (58%)] Loss: -433512.156250\n",
      "Train Epoch: 212 [11392/17352 (66%)] Loss: -374670.406250\n",
      "Train Epoch: 212 [12800/17352 (74%)] Loss: -398624.000000\n",
      "Train Epoch: 212 [14208/17352 (82%)] Loss: -362552.125000\n",
      "Train Epoch: 212 [15488/17352 (89%)] Loss: -200569.578125\n",
      "Train Epoch: 212 [16348/17352 (94%)] Loss: -240095.000000\n",
      "Train Epoch: 212 [17109/17352 (99%)] Loss: -128787.437500\n",
      "    epoch          : 212\n",
      "    loss           : -352050.3947278733\n",
      "    val_loss       : -187834.97010091145\n",
      "Train Epoch: 213 [128/17352 (1%)] Loss: -395333.156250\n",
      "Train Epoch: 213 [1536/17352 (9%)] Loss: -350923.062500\n",
      "Train Epoch: 213 [2944/17352 (17%)] Loss: -438560.187500\n",
      "Train Epoch: 213 [4352/17352 (25%)] Loss: -396719.562500\n",
      "Train Epoch: 213 [5760/17352 (33%)] Loss: -406507.468750\n",
      "Train Epoch: 213 [7168/17352 (41%)] Loss: -346102.406250\n",
      "Train Epoch: 213 [8576/17352 (49%)] Loss: -419270.437500\n",
      "Train Epoch: 213 [9984/17352 (58%)] Loss: -419505.031250\n",
      "Train Epoch: 213 [11392/17352 (66%)] Loss: -396661.906250\n",
      "Train Epoch: 213 [12800/17352 (74%)] Loss: -390842.312500\n",
      "Train Epoch: 213 [14208/17352 (82%)] Loss: -388075.125000\n",
      "Train Epoch: 213 [15531/17352 (90%)] Loss: -282750.375000\n",
      "Train Epoch: 213 [16174/17352 (93%)] Loss: -301089.218750\n",
      "Train Epoch: 213 [16997/17352 (98%)] Loss: -98964.218750\n",
      "    epoch          : 213\n",
      "    loss           : -353684.8380610319\n",
      "    val_loss       : -191027.6030436198\n",
      "Train Epoch: 214 [128/17352 (1%)] Loss: -364726.343750\n",
      "Train Epoch: 214 [1536/17352 (9%)] Loss: -371550.500000\n",
      "Train Epoch: 214 [2944/17352 (17%)] Loss: -422688.062500\n",
      "Train Epoch: 214 [4352/17352 (25%)] Loss: -375291.562500\n",
      "Train Epoch: 214 [5760/17352 (33%)] Loss: -324279.281250\n",
      "Train Epoch: 214 [7168/17352 (41%)] Loss: -394002.312500\n",
      "Train Epoch: 214 [8576/17352 (49%)] Loss: -430456.000000\n",
      "Train Epoch: 214 [9984/17352 (58%)] Loss: -412580.562500\n",
      "Train Epoch: 214 [11392/17352 (66%)] Loss: -392549.062500\n",
      "Train Epoch: 214 [12800/17352 (74%)] Loss: -353650.906250\n",
      "Train Epoch: 214 [14208/17352 (82%)] Loss: -251801.265625\n",
      "Train Epoch: 214 [15506/17352 (89%)] Loss: -98648.585938\n",
      "Train Epoch: 214 [16379/17352 (94%)] Loss: -166703.531250\n",
      "Train Epoch: 214 [17056/17352 (98%)] Loss: -230198.218750\n",
      "    epoch          : 214\n",
      "    loss           : -343491.93491112627\n",
      "    val_loss       : -188722.1157877604\n",
      "Train Epoch: 215 [128/17352 (1%)] Loss: -378239.312500\n",
      "Train Epoch: 215 [1536/17352 (9%)] Loss: -346709.187500\n",
      "Train Epoch: 215 [2944/17352 (17%)] Loss: -266276.000000\n",
      "Train Epoch: 215 [4352/17352 (25%)] Loss: -413646.718750\n",
      "Train Epoch: 215 [5760/17352 (33%)] Loss: -402029.937500\n",
      "Train Epoch: 215 [7168/17352 (41%)] Loss: -408045.437500\n",
      "Train Epoch: 215 [8576/17352 (49%)] Loss: -400368.437500\n",
      "Train Epoch: 215 [9984/17352 (58%)] Loss: -400495.000000\n",
      "Train Epoch: 215 [11392/17352 (66%)] Loss: -394257.718750\n",
      "Train Epoch: 215 [12800/17352 (74%)] Loss: -404084.125000\n",
      "Train Epoch: 215 [14208/17352 (82%)] Loss: -417523.968750\n",
      "Train Epoch: 215 [15554/17352 (90%)] Loss: -190092.093750\n",
      "Train Epoch: 215 [16238/17352 (94%)] Loss: -134936.906250\n",
      "Train Epoch: 215 [16938/17352 (98%)] Loss: -184868.593750\n",
      "    epoch          : 215\n",
      "    loss           : -340101.0943791946\n",
      "    val_loss       : -187188.96463216146\n",
      "Train Epoch: 216 [128/17352 (1%)] Loss: -402802.000000\n",
      "Train Epoch: 216 [1536/17352 (9%)] Loss: -390070.750000\n",
      "Train Epoch: 216 [2944/17352 (17%)] Loss: -361887.656250\n",
      "Train Epoch: 216 [4352/17352 (25%)] Loss: -386327.937500\n",
      "Train Epoch: 216 [5760/17352 (33%)] Loss: -403027.000000\n",
      "Train Epoch: 216 [7168/17352 (41%)] Loss: -325240.000000\n",
      "Train Epoch: 216 [8576/17352 (49%)] Loss: -415359.593750\n",
      "Train Epoch: 216 [9984/17352 (58%)] Loss: -370107.781250\n",
      "Train Epoch: 216 [11392/17352 (66%)] Loss: -288313.531250\n",
      "Train Epoch: 216 [12800/17352 (74%)] Loss: -397601.656250\n",
      "Train Epoch: 216 [14208/17352 (82%)] Loss: -373847.625000\n",
      "Train Epoch: 216 [15492/17352 (89%)] Loss: -358136.937500\n",
      "Train Epoch: 216 [16175/17352 (93%)] Loss: -204711.281250\n",
      "Train Epoch: 216 [17025/17352 (98%)] Loss: -239352.656250\n",
      "    epoch          : 216\n",
      "    loss           : -338135.4391647441\n",
      "    val_loss       : -185947.03556315103\n",
      "Train Epoch: 217 [128/17352 (1%)] Loss: -361737.375000\n",
      "Train Epoch: 217 [1536/17352 (9%)] Loss: -371484.625000\n",
      "Train Epoch: 217 [2944/17352 (17%)] Loss: -390398.281250\n",
      "Train Epoch: 217 [4352/17352 (25%)] Loss: -366064.562500\n",
      "Train Epoch: 217 [5760/17352 (33%)] Loss: -408364.687500\n",
      "Train Epoch: 217 [7168/17352 (41%)] Loss: -376313.125000\n",
      "Train Epoch: 217 [8576/17352 (49%)] Loss: -402217.687500\n",
      "Train Epoch: 217 [9984/17352 (58%)] Loss: -263124.156250\n",
      "Train Epoch: 217 [11392/17352 (66%)] Loss: -347880.000000\n",
      "Train Epoch: 217 [12800/17352 (74%)] Loss: -421637.562500\n",
      "Train Epoch: 217 [14208/17352 (82%)] Loss: -323675.968750\n",
      "Train Epoch: 217 [15553/17352 (90%)] Loss: -345879.218750\n",
      "Train Epoch: 217 [16214/17352 (93%)] Loss: -13180.532227\n",
      "Train Epoch: 217 [16984/17352 (98%)] Loss: -132765.515625\n",
      "    epoch          : 217\n",
      "    loss           : -340034.49188928795\n",
      "    val_loss       : -193096.04373372396\n",
      "Train Epoch: 218 [128/17352 (1%)] Loss: -391654.406250\n",
      "Train Epoch: 218 [1536/17352 (9%)] Loss: -387584.343750\n",
      "Train Epoch: 218 [2944/17352 (17%)] Loss: -395683.718750\n",
      "Train Epoch: 218 [4352/17352 (25%)] Loss: -405925.468750\n",
      "Train Epoch: 218 [5760/17352 (33%)] Loss: -430368.500000\n",
      "Train Epoch: 218 [7168/17352 (41%)] Loss: -371284.062500\n",
      "Train Epoch: 218 [8576/17352 (49%)] Loss: -389922.718750\n",
      "Train Epoch: 218 [9984/17352 (58%)] Loss: -415735.375000\n",
      "Train Epoch: 218 [11392/17352 (66%)] Loss: -294971.031250\n",
      "Train Epoch: 218 [12800/17352 (74%)] Loss: -399074.562500\n",
      "Train Epoch: 218 [14208/17352 (82%)] Loss: -407762.718750\n",
      "Train Epoch: 218 [15537/17352 (90%)] Loss: -164095.281250\n",
      "Train Epoch: 218 [16278/17352 (94%)] Loss: -215043.468750\n",
      "Train Epoch: 218 [17064/17352 (98%)] Loss: -213384.343750\n",
      "    epoch          : 218\n",
      "    loss           : -349762.92679058306\n",
      "    val_loss       : -190818.39737955728\n",
      "Train Epoch: 219 [128/17352 (1%)] Loss: -396431.843750\n",
      "Train Epoch: 219 [1536/17352 (9%)] Loss: -220161.078125\n",
      "Train Epoch: 219 [2944/17352 (17%)] Loss: -386834.562500\n",
      "Train Epoch: 219 [4352/17352 (25%)] Loss: -410798.843750\n",
      "Train Epoch: 219 [5760/17352 (33%)] Loss: -386588.000000\n",
      "Train Epoch: 219 [7168/17352 (41%)] Loss: -374257.500000\n",
      "Train Epoch: 219 [8576/17352 (49%)] Loss: -386266.718750\n",
      "Train Epoch: 219 [9984/17352 (58%)] Loss: -410210.750000\n",
      "Train Epoch: 219 [11392/17352 (66%)] Loss: -358362.031250\n",
      "Train Epoch: 219 [12800/17352 (74%)] Loss: -390159.312500\n",
      "Train Epoch: 219 [14208/17352 (82%)] Loss: -335842.000000\n",
      "Train Epoch: 219 [15529/17352 (89%)] Loss: -251315.265625\n",
      "Train Epoch: 219 [16422/17352 (95%)] Loss: -379228.656250\n",
      "Train Epoch: 219 [17006/17352 (98%)] Loss: -19035.257812\n",
      "    epoch          : 219\n",
      "    loss           : -345710.9488517198\n",
      "    val_loss       : -186255.22475585938\n",
      "Train Epoch: 220 [128/17352 (1%)] Loss: -404700.375000\n",
      "Train Epoch: 220 [1536/17352 (9%)] Loss: -344933.781250\n",
      "Train Epoch: 220 [2944/17352 (17%)] Loss: -197036.890625\n",
      "Train Epoch: 220 [4352/17352 (25%)] Loss: -406682.250000\n",
      "Train Epoch: 220 [5760/17352 (33%)] Loss: -347767.843750\n",
      "Train Epoch: 220 [7168/17352 (41%)] Loss: -406088.187500\n",
      "Train Epoch: 220 [8576/17352 (49%)] Loss: -390396.437500\n",
      "Train Epoch: 220 [9984/17352 (58%)] Loss: -287397.625000\n",
      "Train Epoch: 220 [11392/17352 (66%)] Loss: -388720.437500\n",
      "Train Epoch: 220 [12800/17352 (74%)] Loss: -306211.375000\n",
      "Train Epoch: 220 [14208/17352 (82%)] Loss: -413840.750000\n",
      "Train Epoch: 220 [15427/17352 (89%)] Loss: -183598.687500\n",
      "Train Epoch: 220 [16285/17352 (94%)] Loss: -157923.796875\n",
      "Train Epoch: 220 [16929/17352 (98%)] Loss: -315087.218750\n",
      "    epoch          : 220\n",
      "    loss           : -344648.8578085675\n",
      "    val_loss       : -190920.84908854167\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch220.pth ...\n",
      "Train Epoch: 221 [128/17352 (1%)] Loss: -392002.406250\n",
      "Train Epoch: 221 [1536/17352 (9%)] Loss: -402051.093750\n",
      "Train Epoch: 221 [2944/17352 (17%)] Loss: -369375.281250\n",
      "Train Epoch: 221 [4352/17352 (25%)] Loss: -401065.437500\n",
      "Train Epoch: 221 [5760/17352 (33%)] Loss: -381489.312500\n",
      "Train Epoch: 221 [7168/17352 (41%)] Loss: -287564.000000\n",
      "Train Epoch: 221 [8576/17352 (49%)] Loss: -387232.312500\n",
      "Train Epoch: 221 [9984/17352 (58%)] Loss: -401102.937500\n",
      "Train Epoch: 221 [11392/17352 (66%)] Loss: -331912.031250\n",
      "Train Epoch: 221 [12800/17352 (74%)] Loss: -372743.125000\n",
      "Train Epoch: 221 [14208/17352 (82%)] Loss: -397672.312500\n",
      "Train Epoch: 221 [15540/17352 (90%)] Loss: -186421.843750\n",
      "Train Epoch: 221 [16285/17352 (94%)] Loss: -170672.437500\n",
      "Train Epoch: 221 [17043/17352 (98%)] Loss: -233122.593750\n",
      "    epoch          : 221\n",
      "    loss           : -344205.6451866611\n",
      "    val_loss       : -191541.4846435547\n",
      "Train Epoch: 222 [128/17352 (1%)] Loss: -409682.906250\n",
      "Train Epoch: 222 [1536/17352 (9%)] Loss: -400178.375000\n",
      "Train Epoch: 222 [2944/17352 (17%)] Loss: -279061.906250\n",
      "Train Epoch: 222 [4352/17352 (25%)] Loss: -315066.781250\n",
      "Train Epoch: 222 [5760/17352 (33%)] Loss: -384647.843750\n",
      "Train Epoch: 222 [7168/17352 (41%)] Loss: -288537.312500\n",
      "Train Epoch: 222 [8576/17352 (49%)] Loss: -399515.156250\n",
      "Train Epoch: 222 [9984/17352 (58%)] Loss: -395461.062500\n",
      "Train Epoch: 222 [11392/17352 (66%)] Loss: -378458.812500\n",
      "Train Epoch: 222 [12800/17352 (74%)] Loss: -421727.250000\n",
      "Train Epoch: 222 [14208/17352 (82%)] Loss: -393539.562500\n",
      "Train Epoch: 222 [15478/17352 (89%)] Loss: -76513.687500\n",
      "Train Epoch: 222 [16311/17352 (94%)] Loss: -275202.781250\n",
      "Train Epoch: 222 [17013/17352 (98%)] Loss: -142972.046875\n",
      "    epoch          : 222\n",
      "    loss           : -348176.0984427433\n",
      "    val_loss       : -191278.064453125\n",
      "Train Epoch: 223 [128/17352 (1%)] Loss: -420812.062500\n",
      "Train Epoch: 223 [1536/17352 (9%)] Loss: -407603.375000\n",
      "Train Epoch: 223 [2944/17352 (17%)] Loss: -361067.437500\n",
      "Train Epoch: 223 [4352/17352 (25%)] Loss: -400207.875000\n",
      "Train Epoch: 223 [5760/17352 (33%)] Loss: -422021.500000\n",
      "Train Epoch: 223 [7168/17352 (41%)] Loss: -404556.656250\n",
      "Train Epoch: 223 [8576/17352 (49%)] Loss: -404991.437500\n",
      "Train Epoch: 223 [9984/17352 (58%)] Loss: -399876.687500\n",
      "Train Epoch: 223 [11392/17352 (66%)] Loss: -409909.375000\n",
      "Train Epoch: 223 [12800/17352 (74%)] Loss: -427761.437500\n",
      "Train Epoch: 223 [14208/17352 (82%)] Loss: -367037.375000\n",
      "Train Epoch: 223 [15440/17352 (89%)] Loss: -224107.921875\n",
      "Train Epoch: 223 [16372/17352 (94%)] Loss: -356236.750000\n",
      "Train Epoch: 223 [16994/17352 (98%)] Loss: -34893.476562\n",
      "    epoch          : 223\n",
      "    loss           : -345685.2422399329\n",
      "    val_loss       : -189013.19947916668\n",
      "Train Epoch: 224 [128/17352 (1%)] Loss: -340328.687500\n",
      "Train Epoch: 224 [1536/17352 (9%)] Loss: -374610.625000\n",
      "Train Epoch: 224 [2944/17352 (17%)] Loss: -393511.468750\n",
      "Train Epoch: 224 [4352/17352 (25%)] Loss: -340705.187500\n",
      "Train Epoch: 224 [5760/17352 (33%)] Loss: -408579.781250\n",
      "Train Epoch: 224 [7168/17352 (41%)] Loss: -386441.093750\n",
      "Train Epoch: 224 [8576/17352 (49%)] Loss: -350083.437500\n",
      "Train Epoch: 224 [9984/17352 (58%)] Loss: -400350.656250\n",
      "Train Epoch: 224 [11392/17352 (66%)] Loss: -383839.343750\n",
      "Train Epoch: 224 [12800/17352 (74%)] Loss: -391692.187500\n",
      "Train Epoch: 224 [14208/17352 (82%)] Loss: -394959.937500\n",
      "Train Epoch: 224 [15471/17352 (89%)] Loss: -225936.140625\n",
      "Train Epoch: 224 [16281/17352 (94%)] Loss: -101953.484375\n",
      "Train Epoch: 224 [17027/17352 (98%)] Loss: -131296.437500\n",
      "    epoch          : 224\n",
      "    loss           : -339748.271661336\n",
      "    val_loss       : -191121.01634114582\n",
      "Train Epoch: 225 [128/17352 (1%)] Loss: -374620.562500\n",
      "Train Epoch: 225 [1536/17352 (9%)] Loss: -381557.156250\n",
      "Train Epoch: 225 [2944/17352 (17%)] Loss: -402134.625000\n",
      "Train Epoch: 225 [4352/17352 (25%)] Loss: -410079.718750\n",
      "Train Epoch: 225 [5760/17352 (33%)] Loss: -300709.187500\n",
      "Train Epoch: 225 [7168/17352 (41%)] Loss: -358731.906250\n",
      "Train Epoch: 225 [8576/17352 (49%)] Loss: -402153.687500\n",
      "Train Epoch: 225 [9984/17352 (58%)] Loss: -351178.156250\n",
      "Train Epoch: 225 [11392/17352 (66%)] Loss: -407900.812500\n",
      "Train Epoch: 225 [12800/17352 (74%)] Loss: -403389.750000\n",
      "Train Epoch: 225 [14208/17352 (82%)] Loss: -384484.562500\n",
      "Train Epoch: 225 [15491/17352 (89%)] Loss: -122607.437500\n",
      "Train Epoch: 225 [16189/17352 (93%)] Loss: -94302.625000\n",
      "Train Epoch: 225 [17080/17352 (98%)] Loss: -327700.218750\n",
      "    epoch          : 225\n",
      "    loss           : -342852.514736918\n",
      "    val_loss       : -189049.3490559896\n",
      "Train Epoch: 226 [128/17352 (1%)] Loss: -409222.375000\n",
      "Train Epoch: 226 [1536/17352 (9%)] Loss: -339284.750000\n",
      "Train Epoch: 226 [2944/17352 (17%)] Loss: -377514.375000\n",
      "Train Epoch: 226 [4352/17352 (25%)] Loss: -356903.781250\n",
      "Train Epoch: 226 [5760/17352 (33%)] Loss: -383909.218750\n",
      "Train Epoch: 226 [7168/17352 (41%)] Loss: -400631.593750\n",
      "Train Epoch: 226 [8576/17352 (49%)] Loss: -347091.843750\n",
      "Train Epoch: 226 [9984/17352 (58%)] Loss: -389962.531250\n",
      "Train Epoch: 226 [11392/17352 (66%)] Loss: -419320.812500\n",
      "Train Epoch: 226 [12800/17352 (74%)] Loss: -399811.156250\n",
      "Train Epoch: 226 [14208/17352 (82%)] Loss: -393966.687500\n",
      "Train Epoch: 226 [15546/17352 (90%)] Loss: -259434.843750\n",
      "Train Epoch: 226 [16281/17352 (94%)] Loss: -253024.312500\n",
      "Train Epoch: 226 [16930/17352 (98%)] Loss: -240516.890625\n",
      "    epoch          : 226\n",
      "    loss           : -342542.889858169\n",
      "    val_loss       : -191018.3306152344\n",
      "Train Epoch: 227 [128/17352 (1%)] Loss: -395988.593750\n",
      "Train Epoch: 227 [1536/17352 (9%)] Loss: -341610.312500\n",
      "Train Epoch: 227 [2944/17352 (17%)] Loss: -284157.562500\n",
      "Train Epoch: 227 [4352/17352 (25%)] Loss: -338322.281250\n",
      "Train Epoch: 227 [5760/17352 (33%)] Loss: -391548.906250\n",
      "Train Epoch: 227 [7168/17352 (41%)] Loss: -384995.250000\n",
      "Train Epoch: 227 [8576/17352 (49%)] Loss: -421317.156250\n",
      "Train Epoch: 227 [9984/17352 (58%)] Loss: -354566.906250\n",
      "Train Epoch: 227 [11392/17352 (66%)] Loss: -385702.375000\n",
      "Train Epoch: 227 [12800/17352 (74%)] Loss: -406300.218750\n",
      "Train Epoch: 227 [14208/17352 (82%)] Loss: -231011.156250\n",
      "Train Epoch: 227 [15474/17352 (89%)] Loss: -332201.312500\n",
      "Train Epoch: 227 [16197/17352 (93%)] Loss: -337181.375000\n",
      "Train Epoch: 227 [17101/17352 (99%)] Loss: -57629.171875\n",
      "    epoch          : 227\n",
      "    loss           : -341365.47371801594\n",
      "    val_loss       : -184803.3626953125\n",
      "Train Epoch: 228 [128/17352 (1%)] Loss: -344970.031250\n",
      "Train Epoch: 228 [1536/17352 (9%)] Loss: -401321.593750\n",
      "Train Epoch: 228 [2944/17352 (17%)] Loss: -360010.562500\n",
      "Train Epoch: 228 [4352/17352 (25%)] Loss: -384812.437500\n",
      "Train Epoch: 228 [5760/17352 (33%)] Loss: -385584.343750\n",
      "Train Epoch: 228 [7168/17352 (41%)] Loss: -378720.218750\n",
      "Train Epoch: 228 [8576/17352 (49%)] Loss: -420801.937500\n",
      "Train Epoch: 228 [9984/17352 (58%)] Loss: -375738.812500\n",
      "Train Epoch: 228 [11392/17352 (66%)] Loss: -406584.250000\n",
      "Train Epoch: 228 [12800/17352 (74%)] Loss: -402245.968750\n",
      "Train Epoch: 228 [14208/17352 (82%)] Loss: -378037.625000\n",
      "Train Epoch: 228 [15491/17352 (89%)] Loss: -162309.109375\n",
      "Train Epoch: 228 [16340/17352 (94%)] Loss: -253822.593750\n",
      "Train Epoch: 228 [17143/17352 (99%)] Loss: -231613.875000\n",
      "    epoch          : 228\n",
      "    loss           : -338286.68797189597\n",
      "    val_loss       : -177131.14005533853\n",
      "Train Epoch: 229 [128/17352 (1%)] Loss: -356453.750000\n",
      "Train Epoch: 229 [1536/17352 (9%)] Loss: -372979.437500\n",
      "Train Epoch: 229 [2944/17352 (17%)] Loss: -364945.593750\n",
      "Train Epoch: 229 [4352/17352 (25%)] Loss: -348192.656250\n",
      "Train Epoch: 229 [5760/17352 (33%)] Loss: -376799.906250\n",
      "Train Epoch: 229 [7168/17352 (41%)] Loss: -380011.687500\n",
      "Train Epoch: 229 [8576/17352 (49%)] Loss: -395579.312500\n",
      "Train Epoch: 229 [9984/17352 (58%)] Loss: -370625.156250\n",
      "Train Epoch: 229 [11392/17352 (66%)] Loss: -338453.875000\n",
      "Train Epoch: 229 [12800/17352 (74%)] Loss: -400177.500000\n",
      "Train Epoch: 229 [14208/17352 (82%)] Loss: -391568.593750\n",
      "Train Epoch: 229 [15543/17352 (90%)] Loss: -281327.906250\n",
      "Train Epoch: 229 [16280/17352 (94%)] Loss: -256576.312500\n",
      "Train Epoch: 229 [16987/17352 (98%)] Loss: -308065.781250\n",
      "    epoch          : 229\n",
      "    loss           : -329959.6695712301\n",
      "    val_loss       : -194985.4364095052\n",
      "Train Epoch: 230 [128/17352 (1%)] Loss: -380967.968750\n",
      "Train Epoch: 230 [1536/17352 (9%)] Loss: -356993.500000\n",
      "Train Epoch: 230 [2944/17352 (17%)] Loss: -257588.390625\n",
      "Train Epoch: 230 [4352/17352 (25%)] Loss: -407180.562500\n",
      "Train Epoch: 230 [5760/17352 (33%)] Loss: -405593.218750\n",
      "Train Epoch: 230 [7168/17352 (41%)] Loss: -376548.000000\n",
      "Train Epoch: 230 [8576/17352 (49%)] Loss: -415956.250000\n",
      "Train Epoch: 230 [9984/17352 (58%)] Loss: -394619.250000\n",
      "Train Epoch: 230 [11392/17352 (66%)] Loss: -393078.062500\n",
      "Train Epoch: 230 [12800/17352 (74%)] Loss: -307664.750000\n",
      "Train Epoch: 230 [14208/17352 (82%)] Loss: -358717.906250\n",
      "Train Epoch: 230 [15571/17352 (90%)] Loss: -234485.656250\n",
      "Train Epoch: 230 [16265/17352 (94%)] Loss: -105710.000000\n",
      "Train Epoch: 230 [17045/17352 (98%)] Loss: -319096.125000\n",
      "    epoch          : 230\n",
      "    loss           : -341275.69972341653\n",
      "    val_loss       : -195336.8811360677\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch230.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 231 [128/17352 (1%)] Loss: -392315.687500\n",
      "Train Epoch: 231 [1536/17352 (9%)] Loss: -407146.562500\n",
      "Train Epoch: 231 [2944/17352 (17%)] Loss: -302227.562500\n",
      "Train Epoch: 231 [4352/17352 (25%)] Loss: -394501.937500\n",
      "Train Epoch: 231 [5760/17352 (33%)] Loss: -316297.718750\n",
      "Train Epoch: 231 [7168/17352 (41%)] Loss: -402817.343750\n",
      "Train Epoch: 231 [8576/17352 (49%)] Loss: -274844.125000\n",
      "Train Epoch: 231 [9984/17352 (58%)] Loss: -325066.375000\n",
      "Train Epoch: 231 [11392/17352 (66%)] Loss: -376749.062500\n",
      "Train Epoch: 231 [12800/17352 (74%)] Loss: -400165.250000\n",
      "Train Epoch: 231 [14208/17352 (82%)] Loss: -390952.312500\n",
      "Train Epoch: 231 [15474/17352 (89%)] Loss: -38261.070312\n",
      "Train Epoch: 231 [16196/17352 (93%)] Loss: -112020.890625\n",
      "Train Epoch: 231 [17014/17352 (98%)] Loss: -133919.937500\n",
      "    epoch          : 231\n",
      "    loss           : -345098.50098967075\n",
      "    val_loss       : -193051.8528157552\n",
      "Train Epoch: 232 [128/17352 (1%)] Loss: -313439.812500\n",
      "Train Epoch: 232 [1536/17352 (9%)] Loss: -363799.718750\n",
      "Train Epoch: 232 [2944/17352 (17%)] Loss: -390766.125000\n",
      "Train Epoch: 232 [4352/17352 (25%)] Loss: -388513.906250\n",
      "Train Epoch: 232 [5760/17352 (33%)] Loss: -380092.375000\n",
      "Train Epoch: 232 [7168/17352 (41%)] Loss: -360530.562500\n",
      "Train Epoch: 232 [8576/17352 (49%)] Loss: -408191.625000\n",
      "Train Epoch: 232 [9984/17352 (58%)] Loss: -407487.125000\n",
      "Train Epoch: 232 [11392/17352 (66%)] Loss: -414909.812500\n",
      "Train Epoch: 232 [12800/17352 (74%)] Loss: -363654.875000\n",
      "Train Epoch: 232 [14208/17352 (82%)] Loss: -404980.718750\n",
      "Train Epoch: 232 [15500/17352 (89%)] Loss: -38531.929688\n",
      "Train Epoch: 232 [16346/17352 (94%)] Loss: -276457.843750\n",
      "Train Epoch: 232 [17097/17352 (99%)] Loss: -342367.906250\n",
      "    epoch          : 232\n",
      "    loss           : -339796.5241846686\n",
      "    val_loss       : -191602.76131184897\n",
      "Train Epoch: 233 [128/17352 (1%)] Loss: -368314.000000\n",
      "Train Epoch: 233 [1536/17352 (9%)] Loss: -203758.437500\n",
      "Train Epoch: 233 [2944/17352 (17%)] Loss: -216468.921875\n",
      "Train Epoch: 233 [4352/17352 (25%)] Loss: -383722.625000\n",
      "Train Epoch: 233 [5760/17352 (33%)] Loss: -383512.375000\n",
      "Train Epoch: 233 [7168/17352 (41%)] Loss: -356462.437500\n",
      "Train Epoch: 233 [8576/17352 (49%)] Loss: -412632.343750\n",
      "Train Epoch: 233 [9984/17352 (58%)] Loss: -369544.812500\n",
      "Train Epoch: 233 [11392/17352 (66%)] Loss: -379974.125000\n",
      "Train Epoch: 233 [12800/17352 (74%)] Loss: -389510.750000\n",
      "Train Epoch: 233 [14208/17352 (82%)] Loss: -343254.593750\n",
      "Train Epoch: 233 [15532/17352 (90%)] Loss: -217361.046875\n",
      "Train Epoch: 233 [16356/17352 (94%)] Loss: -278752.218750\n",
      "Train Epoch: 233 [17115/17352 (99%)] Loss: -290248.562500\n",
      "    epoch          : 233\n",
      "    loss           : -339132.03688653524\n",
      "    val_loss       : -190574.60423177082\n",
      "Train Epoch: 234 [128/17352 (1%)] Loss: -408595.437500\n",
      "Train Epoch: 234 [1536/17352 (9%)] Loss: -391603.281250\n",
      "Train Epoch: 234 [2944/17352 (17%)] Loss: -380970.906250\n",
      "Train Epoch: 234 [4352/17352 (25%)] Loss: -389393.281250\n",
      "Train Epoch: 234 [5760/17352 (33%)] Loss: -403563.750000\n",
      "Train Epoch: 234 [7168/17352 (41%)] Loss: -404369.093750\n",
      "Train Epoch: 234 [8576/17352 (49%)] Loss: -363809.156250\n",
      "Train Epoch: 234 [9984/17352 (58%)] Loss: -411134.000000\n",
      "Train Epoch: 234 [11392/17352 (66%)] Loss: -362824.218750\n",
      "Train Epoch: 234 [12800/17352 (74%)] Loss: -392318.375000\n",
      "Train Epoch: 234 [14208/17352 (82%)] Loss: -407551.500000\n",
      "Train Epoch: 234 [15495/17352 (89%)] Loss: -132696.765625\n",
      "Train Epoch: 234 [16323/17352 (94%)] Loss: -265614.125000\n",
      "Train Epoch: 234 [16970/17352 (98%)] Loss: -41965.050781\n",
      "    epoch          : 234\n",
      "    loss           : -341321.7031446623\n",
      "    val_loss       : -187393.09773763022\n",
      "Train Epoch: 235 [128/17352 (1%)] Loss: -384245.687500\n",
      "Train Epoch: 235 [1536/17352 (9%)] Loss: -331824.125000\n",
      "Train Epoch: 235 [2944/17352 (17%)] Loss: -395036.750000\n",
      "Train Epoch: 235 [4352/17352 (25%)] Loss: -404838.250000\n",
      "Train Epoch: 235 [5760/17352 (33%)] Loss: -410594.375000\n",
      "Train Epoch: 235 [7168/17352 (41%)] Loss: -407575.062500\n",
      "Train Epoch: 235 [8576/17352 (49%)] Loss: -334165.812500\n",
      "Train Epoch: 235 [9984/17352 (58%)] Loss: -352307.875000\n",
      "Train Epoch: 235 [11392/17352 (66%)] Loss: -374004.937500\n",
      "Train Epoch: 235 [12800/17352 (74%)] Loss: -340224.250000\n",
      "Train Epoch: 235 [14208/17352 (82%)] Loss: -372198.281250\n",
      "Train Epoch: 235 [15498/17352 (89%)] Loss: -147071.484375\n",
      "Train Epoch: 235 [16519/17352 (95%)] Loss: -289885.343750\n",
      "Train Epoch: 235 [17095/17352 (99%)] Loss: -218475.625000\n",
      "    epoch          : 235\n",
      "    loss           : -331001.6057374685\n",
      "    val_loss       : -185433.9465983073\n",
      "Train Epoch: 236 [128/17352 (1%)] Loss: -370116.437500\n",
      "Train Epoch: 236 [1536/17352 (9%)] Loss: -386997.437500\n",
      "Train Epoch: 236 [2944/17352 (17%)] Loss: -249411.156250\n",
      "Train Epoch: 236 [4352/17352 (25%)] Loss: -377441.312500\n",
      "Train Epoch: 236 [5760/17352 (33%)] Loss: -393160.500000\n",
      "Train Epoch: 236 [7168/17352 (41%)] Loss: -220880.453125\n",
      "Train Epoch: 236 [8576/17352 (49%)] Loss: -399405.968750\n",
      "Train Epoch: 236 [9984/17352 (58%)] Loss: -389997.187500\n",
      "Train Epoch: 236 [11392/17352 (66%)] Loss: -387197.812500\n",
      "Train Epoch: 236 [12800/17352 (74%)] Loss: -369911.406250\n",
      "Train Epoch: 236 [14208/17352 (82%)] Loss: -383131.750000\n",
      "Train Epoch: 236 [15417/17352 (89%)] Loss: -6566.057129\n",
      "Train Epoch: 236 [16044/17352 (92%)] Loss: -165673.109375\n",
      "Train Epoch: 236 [16915/17352 (97%)] Loss: -158829.406250\n",
      "    epoch          : 236\n",
      "    loss           : -337890.60339437396\n",
      "    val_loss       : -184580.3205078125\n",
      "Train Epoch: 237 [128/17352 (1%)] Loss: -413154.968750\n",
      "Train Epoch: 237 [1536/17352 (9%)] Loss: -372618.687500\n",
      "Train Epoch: 237 [2944/17352 (17%)] Loss: -410076.500000\n",
      "Train Epoch: 237 [4352/17352 (25%)] Loss: -283578.250000\n",
      "Train Epoch: 237 [5760/17352 (33%)] Loss: -377349.250000\n",
      "Train Epoch: 237 [7168/17352 (41%)] Loss: -410519.750000\n",
      "Train Epoch: 237 [8576/17352 (49%)] Loss: -354546.968750\n",
      "Train Epoch: 237 [9984/17352 (58%)] Loss: -407895.312500\n",
      "Train Epoch: 237 [11392/17352 (66%)] Loss: -398238.000000\n",
      "Train Epoch: 237 [12800/17352 (74%)] Loss: -301080.062500\n",
      "Train Epoch: 237 [14208/17352 (82%)] Loss: -415495.843750\n",
      "Train Epoch: 237 [15504/17352 (89%)] Loss: -294759.250000\n",
      "Train Epoch: 237 [16320/17352 (94%)] Loss: -102036.351562\n",
      "Train Epoch: 237 [17188/17352 (99%)] Loss: -374686.406250\n",
      "    epoch          : 237\n",
      "    loss           : -346128.58321098995\n",
      "    val_loss       : -186610.7850423177\n",
      "Train Epoch: 238 [128/17352 (1%)] Loss: -343823.625000\n",
      "Train Epoch: 238 [1536/17352 (9%)] Loss: -395855.000000\n",
      "Train Epoch: 238 [2944/17352 (17%)] Loss: -419937.156250\n",
      "Train Epoch: 238 [4352/17352 (25%)] Loss: -337550.562500\n",
      "Train Epoch: 238 [5760/17352 (33%)] Loss: -310347.375000\n",
      "Train Epoch: 238 [7168/17352 (41%)] Loss: -384348.468750\n",
      "Train Epoch: 238 [8576/17352 (49%)] Loss: -427877.375000\n",
      "Train Epoch: 238 [9984/17352 (58%)] Loss: -418306.000000\n",
      "Train Epoch: 238 [11392/17352 (66%)] Loss: -358892.125000\n",
      "Train Epoch: 238 [12800/17352 (74%)] Loss: -396572.625000\n",
      "Train Epoch: 238 [14208/17352 (82%)] Loss: -381573.156250\n",
      "Train Epoch: 238 [15509/17352 (89%)] Loss: -173381.218750\n",
      "Train Epoch: 238 [16396/17352 (94%)] Loss: -79889.578125\n",
      "Train Epoch: 238 [17169/17352 (99%)] Loss: -265434.250000\n",
      "    epoch          : 238\n",
      "    loss           : -345721.0565881921\n",
      "    val_loss       : -176960.2263671875\n",
      "Train Epoch: 239 [128/17352 (1%)] Loss: -343827.343750\n",
      "Train Epoch: 239 [1536/17352 (9%)] Loss: -359545.812500\n",
      "Train Epoch: 239 [2944/17352 (17%)] Loss: -359303.625000\n",
      "Train Epoch: 239 [4352/17352 (25%)] Loss: -378853.312500\n",
      "Train Epoch: 239 [5760/17352 (33%)] Loss: -385009.875000\n",
      "Train Epoch: 239 [7168/17352 (41%)] Loss: -395065.937500\n",
      "Train Epoch: 239 [8576/17352 (49%)] Loss: -384534.875000\n",
      "Train Epoch: 239 [9984/17352 (58%)] Loss: -352172.875000\n",
      "Train Epoch: 239 [11392/17352 (66%)] Loss: -332269.812500\n",
      "Train Epoch: 239 [12800/17352 (74%)] Loss: -372506.187500\n",
      "Train Epoch: 239 [14208/17352 (82%)] Loss: -299191.250000\n",
      "Train Epoch: 239 [15499/17352 (89%)] Loss: -180820.609375\n",
      "Train Epoch: 239 [16259/17352 (94%)] Loss: -150974.859375\n",
      "Train Epoch: 239 [17044/17352 (98%)] Loss: -133426.375000\n",
      "    epoch          : 239\n",
      "    loss           : -332251.4005282613\n",
      "    val_loss       : -174194.1801920573\n",
      "Train Epoch: 240 [128/17352 (1%)] Loss: -348819.437500\n",
      "Train Epoch: 240 [1536/17352 (9%)] Loss: -389488.218750\n",
      "Train Epoch: 240 [2944/17352 (17%)] Loss: -403552.000000\n",
      "Train Epoch: 240 [4352/17352 (25%)] Loss: -386255.531250\n",
      "Train Epoch: 240 [5760/17352 (33%)] Loss: -367411.375000\n",
      "Train Epoch: 240 [7168/17352 (41%)] Loss: -354253.750000\n",
      "Train Epoch: 240 [8576/17352 (49%)] Loss: -403481.937500\n",
      "Train Epoch: 240 [9984/17352 (58%)] Loss: -387803.687500\n",
      "Train Epoch: 240 [11392/17352 (66%)] Loss: -377554.562500\n",
      "Train Epoch: 240 [12800/17352 (74%)] Loss: -392012.718750\n",
      "Train Epoch: 240 [14208/17352 (82%)] Loss: -382231.281250\n",
      "Train Epoch: 240 [15484/17352 (89%)] Loss: -100711.578125\n",
      "Train Epoch: 240 [16280/17352 (94%)] Loss: -264159.312500\n",
      "Train Epoch: 240 [16942/17352 (98%)] Loss: -366507.468750\n",
      "    epoch          : 240\n",
      "    loss           : -334135.220644138\n",
      "    val_loss       : -185515.33681640626\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch240.pth ...\n",
      "Train Epoch: 241 [128/17352 (1%)] Loss: -375367.750000\n",
      "Train Epoch: 241 [1536/17352 (9%)] Loss: -385798.906250\n",
      "Train Epoch: 241 [2944/17352 (17%)] Loss: -389849.562500\n",
      "Train Epoch: 241 [4352/17352 (25%)] Loss: -317404.500000\n",
      "Train Epoch: 241 [5760/17352 (33%)] Loss: -378032.375000\n",
      "Train Epoch: 241 [7168/17352 (41%)] Loss: -425395.250000\n",
      "Train Epoch: 241 [8576/17352 (49%)] Loss: -416059.875000\n",
      "Train Epoch: 241 [9984/17352 (58%)] Loss: -414257.437500\n",
      "Train Epoch: 241 [11392/17352 (66%)] Loss: -385572.593750\n",
      "Train Epoch: 241 [12800/17352 (74%)] Loss: -401398.343750\n",
      "Train Epoch: 241 [14208/17352 (82%)] Loss: -379166.406250\n",
      "Train Epoch: 241 [15491/17352 (89%)] Loss: -130743.375000\n",
      "Train Epoch: 241 [16152/17352 (93%)] Loss: -239090.093750\n",
      "Train Epoch: 241 [17005/17352 (98%)] Loss: -204520.031250\n",
      "    epoch          : 241\n",
      "    loss           : -344819.92483745806\n",
      "    val_loss       : -184585.96580403644\n",
      "Train Epoch: 242 [128/17352 (1%)] Loss: -394788.562500\n",
      "Train Epoch: 242 [1536/17352 (9%)] Loss: -397618.000000\n",
      "Train Epoch: 242 [2944/17352 (17%)] Loss: -396384.875000\n",
      "Train Epoch: 242 [4352/17352 (25%)] Loss: -397631.625000\n",
      "Train Epoch: 242 [5760/17352 (33%)] Loss: -313123.625000\n",
      "Train Epoch: 242 [7168/17352 (41%)] Loss: -279649.437500\n",
      "Train Epoch: 242 [8576/17352 (49%)] Loss: -403208.500000\n",
      "Train Epoch: 242 [9984/17352 (58%)] Loss: -303458.375000\n",
      "Train Epoch: 242 [11392/17352 (66%)] Loss: -379465.000000\n",
      "Train Epoch: 242 [12800/17352 (74%)] Loss: -380129.562500\n",
      "Train Epoch: 242 [14208/17352 (82%)] Loss: -408853.656250\n",
      "Train Epoch: 242 [15504/17352 (89%)] Loss: -294318.687500\n",
      "Train Epoch: 242 [16194/17352 (93%)] Loss: -105309.750000\n",
      "Train Epoch: 242 [16988/17352 (98%)] Loss: -64877.152344\n",
      "    epoch          : 242\n",
      "    loss           : -343725.5687395134\n",
      "    val_loss       : -195536.2593261719\n",
      "Train Epoch: 243 [128/17352 (1%)] Loss: -341194.750000\n",
      "Train Epoch: 243 [1536/17352 (9%)] Loss: -411519.625000\n",
      "Train Epoch: 243 [2944/17352 (17%)] Loss: -308854.968750\n",
      "Train Epoch: 243 [4352/17352 (25%)] Loss: -296561.250000\n",
      "Train Epoch: 243 [5760/17352 (33%)] Loss: -378270.968750\n",
      "Train Epoch: 243 [7168/17352 (41%)] Loss: -435435.375000\n",
      "Train Epoch: 243 [8576/17352 (49%)] Loss: -432309.343750\n",
      "Train Epoch: 243 [9984/17352 (58%)] Loss: -412919.000000\n",
      "Train Epoch: 243 [11392/17352 (66%)] Loss: -362275.125000\n",
      "Train Epoch: 243 [12800/17352 (74%)] Loss: -383518.062500\n",
      "Train Epoch: 243 [14208/17352 (82%)] Loss: -375991.093750\n",
      "Train Epoch: 243 [15534/17352 (90%)] Loss: -264699.843750\n",
      "Train Epoch: 243 [16356/17352 (94%)] Loss: -132417.781250\n",
      "Train Epoch: 243 [17094/17352 (99%)] Loss: -165279.187500\n",
      "    epoch          : 243\n",
      "    loss           : -346938.6699022127\n",
      "    val_loss       : -182290.6993815104\n",
      "Train Epoch: 244 [128/17352 (1%)] Loss: -306933.906250\n",
      "Train Epoch: 244 [1536/17352 (9%)] Loss: -389650.312500\n",
      "Train Epoch: 244 [2944/17352 (17%)] Loss: -409267.250000\n",
      "Train Epoch: 244 [4352/17352 (25%)] Loss: -399976.656250\n",
      "Train Epoch: 244 [5760/17352 (33%)] Loss: -391434.750000\n",
      "Train Epoch: 244 [7168/17352 (41%)] Loss: -409023.625000\n",
      "Train Epoch: 244 [8576/17352 (49%)] Loss: -413106.843750\n",
      "Train Epoch: 244 [9984/17352 (58%)] Loss: -381453.406250\n",
      "Train Epoch: 244 [11392/17352 (66%)] Loss: -367408.593750\n",
      "Train Epoch: 244 [12800/17352 (74%)] Loss: -352428.187500\n",
      "Train Epoch: 244 [14208/17352 (82%)] Loss: -242623.015625\n",
      "Train Epoch: 244 [15451/17352 (89%)] Loss: -281201.437500\n",
      "Train Epoch: 244 [16269/17352 (94%)] Loss: -290034.625000\n",
      "Train Epoch: 244 [17049/17352 (98%)] Loss: -257518.812500\n",
      "    epoch          : 244\n",
      "    loss           : -346396.3682361577\n",
      "    val_loss       : -183071.89155273436\n",
      "Train Epoch: 245 [128/17352 (1%)] Loss: -381799.031250\n",
      "Train Epoch: 245 [1536/17352 (9%)] Loss: -350951.375000\n",
      "Train Epoch: 245 [2944/17352 (17%)] Loss: -361885.281250\n",
      "Train Epoch: 245 [4352/17352 (25%)] Loss: -350897.750000\n",
      "Train Epoch: 245 [5760/17352 (33%)] Loss: -366816.750000\n",
      "Train Epoch: 245 [7168/17352 (41%)] Loss: -381007.312500\n",
      "Train Epoch: 245 [8576/17352 (49%)] Loss: -392921.875000\n",
      "Train Epoch: 245 [9984/17352 (58%)] Loss: -306024.562500\n",
      "Train Epoch: 245 [11392/17352 (66%)] Loss: -347172.781250\n",
      "Train Epoch: 245 [12800/17352 (74%)] Loss: -394475.343750\n",
      "Train Epoch: 245 [14208/17352 (82%)] Loss: -367995.843750\n",
      "Train Epoch: 245 [15417/17352 (89%)] Loss: -10618.361328\n",
      "Train Epoch: 245 [16320/17352 (94%)] Loss: -194801.531250\n",
      "Train Epoch: 245 [17133/17352 (99%)] Loss: -42072.503906\n",
      "    epoch          : 245\n",
      "    loss           : -335208.1112494757\n",
      "    val_loss       : -187763.79181315104\n",
      "Train Epoch: 246 [128/17352 (1%)] Loss: -396797.687500\n",
      "Train Epoch: 246 [1536/17352 (9%)] Loss: -387556.062500\n",
      "Train Epoch: 246 [2944/17352 (17%)] Loss: -203288.390625\n",
      "Train Epoch: 246 [4352/17352 (25%)] Loss: -367564.343750\n",
      "Train Epoch: 246 [5760/17352 (33%)] Loss: -364850.343750\n",
      "Train Epoch: 246 [7168/17352 (41%)] Loss: -399292.062500\n",
      "Train Epoch: 246 [8576/17352 (49%)] Loss: -378612.093750\n",
      "Train Epoch: 246 [9984/17352 (58%)] Loss: -392609.250000\n",
      "Train Epoch: 246 [11392/17352 (66%)] Loss: -388813.812500\n",
      "Train Epoch: 246 [12800/17352 (74%)] Loss: -377134.593750\n",
      "Train Epoch: 246 [14208/17352 (82%)] Loss: -379458.937500\n",
      "Train Epoch: 246 [15503/17352 (89%)] Loss: -96340.851562\n",
      "Train Epoch: 246 [16332/17352 (94%)] Loss: -120977.937500\n",
      "Train Epoch: 246 [17058/17352 (98%)] Loss: -217541.812500\n",
      "    epoch          : 246\n",
      "    loss           : -331609.8872365247\n",
      "    val_loss       : -184751.78645833334\n",
      "Train Epoch: 247 [128/17352 (1%)] Loss: -362298.312500\n",
      "Train Epoch: 247 [1536/17352 (9%)] Loss: -383704.250000\n",
      "Train Epoch: 247 [2944/17352 (17%)] Loss: -342228.343750\n",
      "Train Epoch: 247 [4352/17352 (25%)] Loss: -370899.656250\n",
      "Train Epoch: 247 [5760/17352 (33%)] Loss: -386046.843750\n",
      "Train Epoch: 247 [7168/17352 (41%)] Loss: -298599.656250\n",
      "Train Epoch: 247 [8576/17352 (49%)] Loss: -353725.562500\n",
      "Train Epoch: 247 [9984/17352 (58%)] Loss: -341275.906250\n",
      "Train Epoch: 247 [11392/17352 (66%)] Loss: -371610.625000\n",
      "Train Epoch: 247 [12800/17352 (74%)] Loss: -367936.218750\n",
      "Train Epoch: 247 [14208/17352 (82%)] Loss: -359441.312500\n",
      "Train Epoch: 247 [15451/17352 (89%)] Loss: -256571.296875\n",
      "Train Epoch: 247 [16376/17352 (94%)] Loss: -251275.171875\n",
      "Train Epoch: 247 [17045/17352 (98%)] Loss: -103982.414062\n",
      "    epoch          : 247\n",
      "    loss           : -322311.47495018877\n",
      "    val_loss       : -174152.39270833333\n",
      "Train Epoch: 248 [128/17352 (1%)] Loss: -353649.531250\n",
      "Train Epoch: 248 [1536/17352 (9%)] Loss: -392194.156250\n",
      "Train Epoch: 248 [2944/17352 (17%)] Loss: -382198.250000\n",
      "Train Epoch: 248 [4352/17352 (25%)] Loss: -384927.906250\n",
      "Train Epoch: 248 [5760/17352 (33%)] Loss: -390351.656250\n",
      "Train Epoch: 248 [7168/17352 (41%)] Loss: -387052.281250\n",
      "Train Epoch: 248 [8576/17352 (49%)] Loss: -349733.437500\n",
      "Train Epoch: 248 [9984/17352 (58%)] Loss: -387391.843750\n",
      "Train Epoch: 248 [11392/17352 (66%)] Loss: -380922.500000\n",
      "Train Epoch: 248 [12800/17352 (74%)] Loss: -364712.937500\n",
      "Train Epoch: 248 [14208/17352 (82%)] Loss: -367557.937500\n",
      "Train Epoch: 248 [15509/17352 (89%)] Loss: -281521.093750\n",
      "Train Epoch: 248 [16401/17352 (95%)] Loss: -257252.546875\n",
      "Train Epoch: 248 [16943/17352 (98%)] Loss: -94051.484375\n",
      "    epoch          : 248\n",
      "    loss           : -319030.5867239933\n",
      "    val_loss       : -174619.21284179686\n",
      "Train Epoch: 249 [128/17352 (1%)] Loss: -362908.312500\n",
      "Train Epoch: 249 [1536/17352 (9%)] Loss: -374981.062500\n",
      "Train Epoch: 249 [2944/17352 (17%)] Loss: -339920.062500\n",
      "Train Epoch: 249 [4352/17352 (25%)] Loss: -361409.406250\n",
      "Train Epoch: 249 [5760/17352 (33%)] Loss: -382811.093750\n",
      "Train Epoch: 249 [7168/17352 (41%)] Loss: -363381.062500\n",
      "Train Epoch: 249 [8576/17352 (49%)] Loss: -176916.812500\n",
      "Train Epoch: 249 [9984/17352 (58%)] Loss: -379095.375000\n",
      "Train Epoch: 249 [11392/17352 (66%)] Loss: -271829.562500\n",
      "Train Epoch: 249 [12800/17352 (74%)] Loss: -396727.062500\n",
      "Train Epoch: 249 [14208/17352 (82%)] Loss: -337710.812500\n",
      "Train Epoch: 249 [15541/17352 (90%)] Loss: -187804.296875\n",
      "Train Epoch: 249 [16263/17352 (94%)] Loss: -295393.437500\n",
      "Train Epoch: 249 [17032/17352 (98%)] Loss: -114702.843750\n",
      "    epoch          : 249\n",
      "    loss           : -324313.2063102978\n",
      "    val_loss       : -178307.5877766927\n",
      "Train Epoch: 250 [128/17352 (1%)] Loss: -237849.968750\n",
      "Train Epoch: 250 [1536/17352 (9%)] Loss: -365194.593750\n",
      "Train Epoch: 250 [2944/17352 (17%)] Loss: -296786.937500\n",
      "Train Epoch: 250 [4352/17352 (25%)] Loss: -374986.906250\n",
      "Train Epoch: 250 [5760/17352 (33%)] Loss: -368447.218750\n",
      "Train Epoch: 250 [7168/17352 (41%)] Loss: -375965.968750\n",
      "Train Epoch: 250 [8576/17352 (49%)] Loss: -289808.781250\n",
      "Train Epoch: 250 [9984/17352 (58%)] Loss: -357468.343750\n",
      "Train Epoch: 250 [11392/17352 (66%)] Loss: -300629.687500\n",
      "Train Epoch: 250 [12800/17352 (74%)] Loss: -334528.812500\n",
      "Train Epoch: 250 [14208/17352 (82%)] Loss: -355233.687500\n",
      "Train Epoch: 250 [15522/17352 (89%)] Loss: -246880.312500\n",
      "Train Epoch: 250 [16188/17352 (93%)] Loss: -242943.156250\n",
      "Train Epoch: 250 [16862/17352 (97%)] Loss: -137354.593750\n",
      "    epoch          : 250\n",
      "    loss           : -318198.8888422819\n",
      "    val_loss       : -174339.02170410156\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [128/17352 (1%)] Loss: -344270.562500\n",
      "Train Epoch: 251 [1536/17352 (9%)] Loss: -363297.156250\n",
      "Train Epoch: 251 [2944/17352 (17%)] Loss: -378760.406250\n",
      "Train Epoch: 251 [4352/17352 (25%)] Loss: -360514.406250\n",
      "Train Epoch: 251 [5760/17352 (33%)] Loss: -378237.000000\n",
      "Train Epoch: 251 [7168/17352 (41%)] Loss: -352442.000000\n",
      "Train Epoch: 251 [8576/17352 (49%)] Loss: -355799.531250\n",
      "Train Epoch: 251 [9984/17352 (58%)] Loss: -364536.500000\n",
      "Train Epoch: 251 [11392/17352 (66%)] Loss: -392696.687500\n",
      "Train Epoch: 251 [12800/17352 (74%)] Loss: -401841.062500\n",
      "Train Epoch: 251 [14208/17352 (82%)] Loss: -356100.437500\n",
      "Train Epoch: 251 [15380/17352 (89%)] Loss: -45356.015625\n",
      "Train Epoch: 251 [16243/17352 (94%)] Loss: -199151.156250\n",
      "Train Epoch: 251 [17044/17352 (98%)] Loss: -173273.718750\n",
      "    epoch          : 251\n",
      "    loss           : -316153.7107081061\n",
      "    val_loss       : -175652.5863769531\n",
      "Train Epoch: 252 [128/17352 (1%)] Loss: -370483.812500\n",
      "Train Epoch: 252 [1536/17352 (9%)] Loss: -364731.000000\n",
      "Train Epoch: 252 [2944/17352 (17%)] Loss: -355967.062500\n",
      "Train Epoch: 252 [4352/17352 (25%)] Loss: -321844.500000\n",
      "Train Epoch: 252 [5760/17352 (33%)] Loss: -340320.125000\n",
      "Train Epoch: 252 [7168/17352 (41%)] Loss: -293401.656250\n",
      "Train Epoch: 252 [8576/17352 (49%)] Loss: -374400.968750\n",
      "Train Epoch: 252 [9984/17352 (58%)] Loss: -307207.093750\n",
      "Train Epoch: 252 [11392/17352 (66%)] Loss: -357570.968750\n",
      "Train Epoch: 252 [12800/17352 (74%)] Loss: -341904.250000\n",
      "Train Epoch: 252 [14208/17352 (82%)] Loss: -358412.937500\n",
      "Train Epoch: 252 [15572/17352 (90%)] Loss: -245679.031250\n",
      "Train Epoch: 252 [16230/17352 (94%)] Loss: -303945.187500\n",
      "Train Epoch: 252 [16895/17352 (97%)] Loss: -139741.468750\n",
      "    epoch          : 252\n",
      "    loss           : -321828.3698877936\n",
      "    val_loss       : -181375.3870279948\n",
      "Train Epoch: 253 [128/17352 (1%)] Loss: -369556.875000\n",
      "Train Epoch: 253 [1536/17352 (9%)] Loss: -381666.250000\n",
      "Train Epoch: 253 [2944/17352 (17%)] Loss: -376234.531250\n",
      "Train Epoch: 253 [4352/17352 (25%)] Loss: -375739.718750\n",
      "Train Epoch: 253 [5760/17352 (33%)] Loss: -386359.625000\n",
      "Train Epoch: 253 [7168/17352 (41%)] Loss: -361303.875000\n",
      "Train Epoch: 253 [8576/17352 (49%)] Loss: -381838.562500\n",
      "Train Epoch: 253 [9984/17352 (58%)] Loss: -262658.906250\n",
      "Train Epoch: 253 [11392/17352 (66%)] Loss: -382903.125000\n",
      "Train Epoch: 253 [12800/17352 (74%)] Loss: -264985.375000\n",
      "Train Epoch: 253 [14208/17352 (82%)] Loss: -370170.531250\n",
      "Train Epoch: 253 [15541/17352 (90%)] Loss: -279793.375000\n",
      "Train Epoch: 253 [16356/17352 (94%)] Loss: -263075.687500\n",
      "Train Epoch: 253 [17123/17352 (99%)] Loss: -115737.921875\n",
      "    epoch          : 253\n",
      "    loss           : -328637.680516595\n",
      "    val_loss       : -189563.1069173177\n",
      "Train Epoch: 254 [128/17352 (1%)] Loss: -383076.875000\n",
      "Train Epoch: 254 [1536/17352 (9%)] Loss: -387471.343750\n",
      "Train Epoch: 254 [2944/17352 (17%)] Loss: -283483.156250\n",
      "Train Epoch: 254 [4352/17352 (25%)] Loss: -383257.687500\n",
      "Train Epoch: 254 [5760/17352 (33%)] Loss: -371239.187500\n",
      "Train Epoch: 254 [7168/17352 (41%)] Loss: -382754.656250\n",
      "Train Epoch: 254 [8576/17352 (49%)] Loss: -384794.906250\n",
      "Train Epoch: 254 [9984/17352 (58%)] Loss: -397866.500000\n",
      "Train Epoch: 254 [11392/17352 (66%)] Loss: -386812.312500\n",
      "Train Epoch: 254 [12800/17352 (74%)] Loss: -309453.968750\n",
      "Train Epoch: 254 [14208/17352 (82%)] Loss: -368333.781250\n",
      "Train Epoch: 254 [15530/17352 (89%)] Loss: -342052.718750\n",
      "Train Epoch: 254 [16259/17352 (94%)] Loss: -118200.523438\n",
      "Train Epoch: 254 [17003/17352 (98%)] Loss: -223504.531250\n",
      "    epoch          : 254\n",
      "    loss           : -333581.9474622483\n",
      "    val_loss       : -182985.70200195312\n",
      "Train Epoch: 255 [128/17352 (1%)] Loss: -343938.468750\n",
      "Train Epoch: 255 [1536/17352 (9%)] Loss: -340599.125000\n",
      "Train Epoch: 255 [2944/17352 (17%)] Loss: -336704.500000\n",
      "Train Epoch: 255 [4352/17352 (25%)] Loss: -384373.343750\n",
      "Train Epoch: 255 [5760/17352 (33%)] Loss: -282464.250000\n",
      "Train Epoch: 255 [7168/17352 (41%)] Loss: -369103.375000\n",
      "Train Epoch: 255 [8576/17352 (49%)] Loss: -358652.312500\n",
      "Train Epoch: 255 [9984/17352 (58%)] Loss: -361847.718750\n",
      "Train Epoch: 255 [11392/17352 (66%)] Loss: -291154.843750\n",
      "Train Epoch: 255 [12800/17352 (74%)] Loss: -337187.812500\n",
      "Train Epoch: 255 [14208/17352 (82%)] Loss: -350298.937500\n",
      "Train Epoch: 255 [15457/17352 (89%)] Loss: -137635.015625\n",
      "Train Epoch: 255 [16209/17352 (93%)] Loss: -46783.125000\n",
      "Train Epoch: 255 [16964/17352 (98%)] Loss: -20791.744141\n",
      "    epoch          : 255\n",
      "    loss           : -312554.335167392\n",
      "    val_loss       : -164027.51346842447\n",
      "Train Epoch: 256 [128/17352 (1%)] Loss: -358689.156250\n",
      "Train Epoch: 256 [1536/17352 (9%)] Loss: -387440.093750\n",
      "Train Epoch: 256 [2944/17352 (17%)] Loss: -365555.687500\n",
      "Train Epoch: 256 [4352/17352 (25%)] Loss: -345212.875000\n",
      "Train Epoch: 256 [5760/17352 (33%)] Loss: -358153.218750\n",
      "Train Epoch: 256 [7168/17352 (41%)] Loss: -295580.843750\n",
      "Train Epoch: 256 [8576/17352 (49%)] Loss: -366312.093750\n",
      "Train Epoch: 256 [9984/17352 (58%)] Loss: -383272.375000\n",
      "Train Epoch: 256 [11392/17352 (66%)] Loss: -359858.968750\n",
      "Train Epoch: 256 [12800/17352 (74%)] Loss: -327642.937500\n",
      "Train Epoch: 256 [14208/17352 (82%)] Loss: -375277.312500\n",
      "Train Epoch: 256 [15570/17352 (90%)] Loss: -227432.843750\n",
      "Train Epoch: 256 [16362/17352 (94%)] Loss: -317743.875000\n",
      "Train Epoch: 256 [17084/17352 (98%)] Loss: -127981.718750\n",
      "    epoch          : 256\n",
      "    loss           : -316975.45246631186\n",
      "    val_loss       : -167890.28849283853\n",
      "Train Epoch: 257 [128/17352 (1%)] Loss: -310737.718750\n",
      "Train Epoch: 257 [1536/17352 (9%)] Loss: -313045.593750\n",
      "Train Epoch: 257 [2944/17352 (17%)] Loss: -352088.812500\n",
      "Train Epoch: 257 [4352/17352 (25%)] Loss: -296041.562500\n",
      "Train Epoch: 257 [5760/17352 (33%)] Loss: -349580.500000\n",
      "Train Epoch: 257 [7168/17352 (41%)] Loss: -313919.750000\n",
      "Train Epoch: 257 [8576/17352 (49%)] Loss: -337411.125000\n",
      "Train Epoch: 257 [9984/17352 (58%)] Loss: -316295.062500\n",
      "Train Epoch: 257 [11392/17352 (66%)] Loss: -360434.250000\n",
      "Train Epoch: 257 [12800/17352 (74%)] Loss: -296004.812500\n",
      "Train Epoch: 257 [14208/17352 (82%)] Loss: -342126.156250\n",
      "Train Epoch: 257 [15561/17352 (90%)] Loss: -275418.875000\n",
      "Train Epoch: 257 [16244/17352 (94%)] Loss: -155936.171875\n",
      "Train Epoch: 257 [17007/17352 (98%)] Loss: -93166.000000\n",
      "    epoch          : 257\n",
      "    loss           : -307319.7276177118\n",
      "    val_loss       : -159480.74964192708\n",
      "Train Epoch: 258 [128/17352 (1%)] Loss: -308355.281250\n",
      "Train Epoch: 258 [1536/17352 (9%)] Loss: -318775.937500\n",
      "Train Epoch: 258 [2944/17352 (17%)] Loss: -297095.875000\n",
      "Train Epoch: 258 [4352/17352 (25%)] Loss: -332769.593750\n",
      "Train Epoch: 258 [5760/17352 (33%)] Loss: -174461.750000\n",
      "Train Epoch: 258 [7168/17352 (41%)] Loss: -335043.000000\n",
      "Train Epoch: 258 [8576/17352 (49%)] Loss: -344513.156250\n",
      "Train Epoch: 258 [9984/17352 (58%)] Loss: -348363.468750\n",
      "Train Epoch: 258 [11392/17352 (66%)] Loss: -338422.812500\n",
      "Train Epoch: 258 [12800/17352 (74%)] Loss: -352677.343750\n",
      "Train Epoch: 258 [14208/17352 (82%)] Loss: -330370.500000\n",
      "Train Epoch: 258 [15440/17352 (89%)] Loss: -211516.093750\n",
      "Train Epoch: 258 [16210/17352 (93%)] Loss: -228662.375000\n",
      "Train Epoch: 258 [16994/17352 (98%)] Loss: -246481.843750\n",
      "    epoch          : 258\n",
      "    loss           : -299702.388137715\n",
      "    val_loss       : -167787.42259928386\n",
      "Train Epoch: 259 [128/17352 (1%)] Loss: -329550.468750\n",
      "Train Epoch: 259 [1536/17352 (9%)] Loss: -319876.531250\n",
      "Train Epoch: 259 [2944/17352 (17%)] Loss: -352887.437500\n",
      "Train Epoch: 259 [4352/17352 (25%)] Loss: -270300.812500\n",
      "Train Epoch: 259 [5760/17352 (33%)] Loss: -339886.062500\n",
      "Train Epoch: 259 [7168/17352 (41%)] Loss: -311782.812500\n",
      "Train Epoch: 259 [8576/17352 (49%)] Loss: -336771.750000\n",
      "Train Epoch: 259 [9984/17352 (58%)] Loss: -346951.187500\n",
      "Train Epoch: 259 [11392/17352 (66%)] Loss: -334103.375000\n",
      "Train Epoch: 259 [12800/17352 (74%)] Loss: -355230.437500\n",
      "Train Epoch: 259 [14208/17352 (82%)] Loss: -343330.625000\n",
      "Train Epoch: 259 [15515/17352 (89%)] Loss: -137747.250000\n",
      "Train Epoch: 259 [16383/17352 (94%)] Loss: -332596.781250\n",
      "Train Epoch: 259 [17069/17352 (98%)] Loss: -201848.656250\n",
      "    epoch          : 259\n",
      "    loss           : -294518.44168807677\n",
      "    val_loss       : -165093.9211751302\n",
      "Train Epoch: 260 [128/17352 (1%)] Loss: -333118.000000\n",
      "Train Epoch: 260 [1536/17352 (9%)] Loss: -358748.125000\n",
      "Train Epoch: 260 [2944/17352 (17%)] Loss: -343519.062500\n",
      "Train Epoch: 260 [4352/17352 (25%)] Loss: -249384.500000\n",
      "Train Epoch: 260 [5760/17352 (33%)] Loss: -342798.250000\n",
      "Train Epoch: 260 [7168/17352 (41%)] Loss: -370386.812500\n",
      "Train Epoch: 260 [8576/17352 (49%)] Loss: -364359.281250\n",
      "Train Epoch: 260 [9984/17352 (58%)] Loss: -354691.750000\n",
      "Train Epoch: 260 [11392/17352 (66%)] Loss: -307810.812500\n",
      "Train Epoch: 260 [12800/17352 (74%)] Loss: -367196.406250\n",
      "Train Epoch: 260 [14208/17352 (82%)] Loss: -348323.562500\n",
      "Train Epoch: 260 [15491/17352 (89%)] Loss: -136369.250000\n",
      "Train Epoch: 260 [16265/17352 (94%)] Loss: -42252.730469\n",
      "Train Epoch: 260 [17030/17352 (98%)] Loss: -272786.968750\n",
      "    epoch          : 260\n",
      "    loss           : -308915.214191616\n",
      "    val_loss       : -174332.4430501302\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch260.pth ...\n",
      "Train Epoch: 261 [128/17352 (1%)] Loss: -356618.000000\n",
      "Train Epoch: 261 [1536/17352 (9%)] Loss: -350563.093750\n",
      "Train Epoch: 261 [2944/17352 (17%)] Loss: -255787.531250\n",
      "Train Epoch: 261 [4352/17352 (25%)] Loss: -346053.843750\n",
      "Train Epoch: 261 [5760/17352 (33%)] Loss: -350887.906250\n",
      "Train Epoch: 261 [7168/17352 (41%)] Loss: -343921.218750\n",
      "Train Epoch: 261 [8576/17352 (49%)] Loss: -345036.187500\n",
      "Train Epoch: 261 [9984/17352 (58%)] Loss: -376602.937500\n",
      "Train Epoch: 261 [11392/17352 (66%)] Loss: -361637.968750\n",
      "Train Epoch: 261 [12800/17352 (74%)] Loss: -356636.843750\n",
      "Train Epoch: 261 [14208/17352 (82%)] Loss: -393986.187500\n",
      "Train Epoch: 261 [15530/17352 (89%)] Loss: -244623.093750\n",
      "Train Epoch: 261 [16192/17352 (93%)] Loss: -35745.335938\n",
      "Train Epoch: 261 [17095/17352 (99%)] Loss: -314815.375000\n",
      "    epoch          : 261\n",
      "    loss           : -315142.20388527686\n",
      "    val_loss       : -172226.3853515625\n",
      "Train Epoch: 262 [128/17352 (1%)] Loss: -353003.000000\n",
      "Train Epoch: 262 [1536/17352 (9%)] Loss: -357525.218750\n",
      "Train Epoch: 262 [2944/17352 (17%)] Loss: -341960.531250\n",
      "Train Epoch: 262 [4352/17352 (25%)] Loss: -382000.062500\n",
      "Train Epoch: 262 [5760/17352 (33%)] Loss: -359507.062500\n",
      "Train Epoch: 262 [7168/17352 (41%)] Loss: -342688.343750\n",
      "Train Epoch: 262 [8576/17352 (49%)] Loss: -360949.937500\n",
      "Train Epoch: 262 [9984/17352 (58%)] Loss: -351660.562500\n",
      "Train Epoch: 262 [11392/17352 (66%)] Loss: -351343.062500\n",
      "Train Epoch: 262 [12800/17352 (74%)] Loss: -305883.625000\n",
      "Train Epoch: 262 [14208/17352 (82%)] Loss: -341667.687500\n",
      "Train Epoch: 262 [15592/17352 (90%)] Loss: -357281.187500\n",
      "Train Epoch: 262 [16229/17352 (94%)] Loss: -227097.187500\n",
      "Train Epoch: 262 [17007/17352 (98%)] Loss: -312714.843750\n",
      "    epoch          : 262\n",
      "    loss           : -311190.0343533714\n",
      "    val_loss       : -172438.3016438802\n",
      "Train Epoch: 263 [128/17352 (1%)] Loss: -339475.062500\n",
      "Train Epoch: 263 [1536/17352 (9%)] Loss: -369977.406250\n",
      "Train Epoch: 263 [2944/17352 (17%)] Loss: -261249.312500\n",
      "Train Epoch: 263 [4352/17352 (25%)] Loss: -250029.875000\n",
      "Train Epoch: 263 [5760/17352 (33%)] Loss: -369724.281250\n",
      "Train Epoch: 263 [7168/17352 (41%)] Loss: -323990.343750\n",
      "Train Epoch: 263 [8576/17352 (49%)] Loss: -351167.906250\n",
      "Train Epoch: 263 [9984/17352 (58%)] Loss: -308671.562500\n",
      "Train Epoch: 263 [11392/17352 (66%)] Loss: -329060.375000\n",
      "Train Epoch: 263 [12800/17352 (74%)] Loss: -355194.406250\n",
      "Train Epoch: 263 [14208/17352 (82%)] Loss: -290560.343750\n",
      "Train Epoch: 263 [15427/17352 (89%)] Loss: -33562.710938\n",
      "Train Epoch: 263 [16391/17352 (94%)] Loss: -320678.062500\n",
      "Train Epoch: 263 [17107/17352 (99%)] Loss: -214248.750000\n",
      "    epoch          : 263\n",
      "    loss           : -310957.56369940226\n",
      "    val_loss       : -168089.07001953124\n",
      "Train Epoch: 264 [128/17352 (1%)] Loss: -314103.031250\n",
      "Train Epoch: 264 [1536/17352 (9%)] Loss: -365509.437500\n",
      "Train Epoch: 264 [2944/17352 (17%)] Loss: -321676.062500\n",
      "Train Epoch: 264 [4352/17352 (25%)] Loss: -276512.343750\n",
      "Train Epoch: 264 [5760/17352 (33%)] Loss: -357159.593750\n",
      "Train Epoch: 264 [7168/17352 (41%)] Loss: -344951.125000\n",
      "Train Epoch: 264 [8576/17352 (49%)] Loss: -350043.281250\n",
      "Train Epoch: 264 [9984/17352 (58%)] Loss: -376732.625000\n",
      "Train Epoch: 264 [11392/17352 (66%)] Loss: -312522.125000\n",
      "Train Epoch: 264 [12800/17352 (74%)] Loss: -347695.156250\n",
      "Train Epoch: 264 [14208/17352 (82%)] Loss: -326968.656250\n",
      "Train Epoch: 264 [15485/17352 (89%)] Loss: -115644.867188\n",
      "Train Epoch: 264 [16059/17352 (93%)] Loss: -13791.935547\n",
      "Train Epoch: 264 [16946/17352 (98%)] Loss: -172163.296875\n",
      "    epoch          : 264\n",
      "    loss           : -309593.15366112627\n",
      "    val_loss       : -172772.4059733073\n",
      "Train Epoch: 265 [128/17352 (1%)] Loss: -346647.250000\n",
      "Train Epoch: 265 [1536/17352 (9%)] Loss: -351896.000000\n",
      "Train Epoch: 265 [2944/17352 (17%)] Loss: -351340.125000\n",
      "Train Epoch: 265 [4352/17352 (25%)] Loss: -319438.937500\n",
      "Train Epoch: 265 [5760/17352 (33%)] Loss: -339927.781250\n",
      "Train Epoch: 265 [7168/17352 (41%)] Loss: -357852.343750\n",
      "Train Epoch: 265 [8576/17352 (49%)] Loss: -334094.000000\n",
      "Train Epoch: 265 [9984/17352 (58%)] Loss: -378332.093750\n",
      "Train Epoch: 265 [11392/17352 (66%)] Loss: -283472.625000\n",
      "Train Epoch: 265 [12800/17352 (74%)] Loss: -355927.937500\n",
      "Train Epoch: 265 [14208/17352 (82%)] Loss: -333015.000000\n",
      "Train Epoch: 265 [15444/17352 (89%)] Loss: -132728.156250\n",
      "Train Epoch: 265 [16275/17352 (94%)] Loss: -239840.390625\n",
      "Train Epoch: 265 [16959/17352 (98%)] Loss: -309446.375000\n",
      "    epoch          : 265\n",
      "    loss           : -314665.78292785236\n",
      "    val_loss       : -178275.69055989583\n",
      "Train Epoch: 266 [128/17352 (1%)] Loss: -356457.062500\n",
      "Train Epoch: 266 [1536/17352 (9%)] Loss: -376775.593750\n",
      "Train Epoch: 266 [2944/17352 (17%)] Loss: -271675.593750\n",
      "Train Epoch: 266 [4352/17352 (25%)] Loss: -353800.187500\n",
      "Train Epoch: 266 [5760/17352 (33%)] Loss: -365929.531250\n",
      "Train Epoch: 266 [7168/17352 (41%)] Loss: -332467.000000\n",
      "Train Epoch: 266 [8576/17352 (49%)] Loss: -361665.343750\n",
      "Train Epoch: 266 [9984/17352 (58%)] Loss: -356753.781250\n",
      "Train Epoch: 266 [11392/17352 (66%)] Loss: -349040.218750\n",
      "Train Epoch: 266 [12800/17352 (74%)] Loss: -367321.500000\n",
      "Train Epoch: 266 [14208/17352 (82%)] Loss: -382789.062500\n",
      "Train Epoch: 266 [15452/17352 (89%)] Loss: -46702.996094\n",
      "Train Epoch: 266 [16111/17352 (93%)] Loss: -97027.453125\n",
      "Train Epoch: 266 [16978/17352 (98%)] Loss: -182282.453125\n",
      "    epoch          : 266\n",
      "    loss           : -316890.4624941013\n",
      "    val_loss       : -173797.823828125\n",
      "Train Epoch: 267 [128/17352 (1%)] Loss: -354258.468750\n",
      "Train Epoch: 267 [1536/17352 (9%)] Loss: -367054.718750\n",
      "Train Epoch: 267 [2944/17352 (17%)] Loss: -381054.562500\n",
      "Train Epoch: 267 [4352/17352 (25%)] Loss: -355714.625000\n",
      "Train Epoch: 267 [5760/17352 (33%)] Loss: -338446.156250\n",
      "Train Epoch: 267 [7168/17352 (41%)] Loss: -366699.843750\n",
      "Train Epoch: 267 [8576/17352 (49%)] Loss: -354721.812500\n",
      "Train Epoch: 267 [9984/17352 (58%)] Loss: -378649.656250\n",
      "Train Epoch: 267 [11392/17352 (66%)] Loss: -377991.500000\n",
      "Train Epoch: 267 [12800/17352 (74%)] Loss: -334316.093750\n",
      "Train Epoch: 267 [14208/17352 (82%)] Loss: -377633.750000\n",
      "Train Epoch: 267 [15440/17352 (89%)] Loss: -221158.828125\n",
      "Train Epoch: 267 [16250/17352 (94%)] Loss: -209872.546875\n",
      "Train Epoch: 267 [16988/17352 (98%)] Loss: -209973.765625\n",
      "    epoch          : 267\n",
      "    loss           : -324428.3162358431\n",
      "    val_loss       : -173831.87701009115\n",
      "Train Epoch: 268 [128/17352 (1%)] Loss: -380965.218750\n",
      "Train Epoch: 268 [1536/17352 (9%)] Loss: -369206.937500\n",
      "Train Epoch: 268 [2944/17352 (17%)] Loss: -369685.375000\n",
      "Train Epoch: 268 [4352/17352 (25%)] Loss: -357647.031250\n",
      "Train Epoch: 268 [5760/17352 (33%)] Loss: -355006.312500\n",
      "Train Epoch: 268 [7168/17352 (41%)] Loss: -359632.062500\n",
      "Train Epoch: 268 [8576/17352 (49%)] Loss: -326317.656250\n",
      "Train Epoch: 268 [9984/17352 (58%)] Loss: -351895.968750\n",
      "Train Epoch: 268 [11392/17352 (66%)] Loss: -353623.750000\n",
      "Train Epoch: 268 [12800/17352 (74%)] Loss: -368654.031250\n",
      "Train Epoch: 268 [14208/17352 (82%)] Loss: -344018.437500\n",
      "Train Epoch: 268 [15523/17352 (89%)] Loss: -205861.531250\n",
      "Train Epoch: 268 [16408/17352 (95%)] Loss: -200941.421875\n",
      "Train Epoch: 268 [17132/17352 (99%)] Loss: -130974.398438\n",
      "    epoch          : 268\n",
      "    loss           : -321882.55944578437\n",
      "    val_loss       : -178532.5424641927\n",
      "Train Epoch: 269 [128/17352 (1%)] Loss: -369964.375000\n",
      "Train Epoch: 269 [1536/17352 (9%)] Loss: -371830.343750\n",
      "Train Epoch: 269 [2944/17352 (17%)] Loss: -376574.656250\n",
      "Train Epoch: 269 [4352/17352 (25%)] Loss: -278484.281250\n",
      "Train Epoch: 269 [5760/17352 (33%)] Loss: -347151.406250\n",
      "Train Epoch: 269 [7168/17352 (41%)] Loss: -348989.000000\n",
      "Train Epoch: 269 [8576/17352 (49%)] Loss: -353249.125000\n",
      "Train Epoch: 269 [9984/17352 (58%)] Loss: -327725.406250\n",
      "Train Epoch: 269 [11392/17352 (66%)] Loss: -349650.250000\n",
      "Train Epoch: 269 [12800/17352 (74%)] Loss: -341634.687500\n",
      "Train Epoch: 269 [14208/17352 (82%)] Loss: -350290.937500\n",
      "Train Epoch: 269 [15496/17352 (89%)] Loss: -249287.531250\n",
      "Train Epoch: 269 [16016/17352 (92%)] Loss: -134745.171875\n",
      "Train Epoch: 269 [16900/17352 (97%)] Loss: -193525.875000\n",
      "    epoch          : 269\n",
      "    loss           : -307048.7373571204\n",
      "    val_loss       : -166926.5955078125\n",
      "Train Epoch: 270 [128/17352 (1%)] Loss: -352651.437500\n",
      "Train Epoch: 270 [1536/17352 (9%)] Loss: -321705.218750\n",
      "Train Epoch: 270 [2944/17352 (17%)] Loss: -265515.968750\n",
      "Train Epoch: 270 [4352/17352 (25%)] Loss: -371018.562500\n",
      "Train Epoch: 270 [5760/17352 (33%)] Loss: -360622.375000\n",
      "Train Epoch: 270 [7168/17352 (41%)] Loss: -359020.343750\n",
      "Train Epoch: 270 [8576/17352 (49%)] Loss: -349921.500000\n",
      "Train Epoch: 270 [9984/17352 (58%)] Loss: -343230.468750\n",
      "Train Epoch: 270 [11392/17352 (66%)] Loss: -324158.093750\n",
      "Train Epoch: 270 [12800/17352 (74%)] Loss: -343511.968750\n",
      "Train Epoch: 270 [14208/17352 (82%)] Loss: -326560.031250\n",
      "Train Epoch: 270 [15510/17352 (89%)] Loss: -221175.703125\n",
      "Train Epoch: 270 [16221/17352 (93%)] Loss: -36064.234375\n",
      "Train Epoch: 270 [17035/17352 (98%)] Loss: -132583.015625\n",
      "    epoch          : 270\n",
      "    loss           : -306699.10074323614\n",
      "    val_loss       : -179007.7862467448\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch270.pth ...\n",
      "Train Epoch: 271 [128/17352 (1%)] Loss: -369551.375000\n",
      "Train Epoch: 271 [1536/17352 (9%)] Loss: -389128.781250\n",
      "Train Epoch: 271 [2944/17352 (17%)] Loss: -328794.187500\n",
      "Train Epoch: 271 [4352/17352 (25%)] Loss: -387690.187500\n",
      "Train Epoch: 271 [5760/17352 (33%)] Loss: -360956.156250\n",
      "Train Epoch: 271 [7168/17352 (41%)] Loss: -399260.750000\n",
      "Train Epoch: 271 [8576/17352 (49%)] Loss: -382052.875000\n",
      "Train Epoch: 271 [9984/17352 (58%)] Loss: -378196.531250\n",
      "Train Epoch: 271 [11392/17352 (66%)] Loss: -381459.625000\n",
      "Train Epoch: 271 [12800/17352 (74%)] Loss: -393757.968750\n",
      "Train Epoch: 271 [14208/17352 (82%)] Loss: -356294.906250\n",
      "Train Epoch: 271 [15498/17352 (89%)] Loss: -221140.093750\n",
      "Train Epoch: 271 [16424/17352 (95%)] Loss: -164515.593750\n",
      "Train Epoch: 271 [17243/17352 (99%)] Loss: -263678.500000\n",
      "    epoch          : 271\n",
      "    loss           : -331535.0867829803\n",
      "    val_loss       : -185736.72467447916\n",
      "Train Epoch: 272 [128/17352 (1%)] Loss: -380990.687500\n",
      "Train Epoch: 272 [1536/17352 (9%)] Loss: -359515.531250\n",
      "Train Epoch: 272 [2944/17352 (17%)] Loss: -377988.875000\n",
      "Train Epoch: 272 [4352/17352 (25%)] Loss: -383109.000000\n",
      "Train Epoch: 272 [5760/17352 (33%)] Loss: -397308.531250\n",
      "Train Epoch: 272 [7168/17352 (41%)] Loss: -380327.562500\n",
      "Train Epoch: 272 [8576/17352 (49%)] Loss: -399148.468750\n",
      "Train Epoch: 272 [9984/17352 (58%)] Loss: -364305.406250\n",
      "Train Epoch: 272 [11392/17352 (66%)] Loss: -282691.406250\n",
      "Train Epoch: 272 [12800/17352 (74%)] Loss: -334375.875000\n",
      "Train Epoch: 272 [14208/17352 (82%)] Loss: -391642.593750\n",
      "Train Epoch: 272 [15500/17352 (89%)] Loss: -271694.406250\n",
      "Train Epoch: 272 [16340/17352 (94%)] Loss: -301061.625000\n",
      "Train Epoch: 272 [17055/17352 (98%)] Loss: -149514.609375\n",
      "    epoch          : 272\n",
      "    loss           : -338259.87663852767\n",
      "    val_loss       : -179879.69436848958\n",
      "Train Epoch: 273 [128/17352 (1%)] Loss: -379268.687500\n",
      "Train Epoch: 273 [1536/17352 (9%)] Loss: -339195.468750\n",
      "Train Epoch: 273 [2944/17352 (17%)] Loss: -360474.562500\n",
      "Train Epoch: 273 [4352/17352 (25%)] Loss: -312593.437500\n",
      "Train Epoch: 273 [5760/17352 (33%)] Loss: -378894.812500\n",
      "Train Epoch: 273 [7168/17352 (41%)] Loss: -366548.375000\n",
      "Train Epoch: 273 [8576/17352 (49%)] Loss: -415079.062500\n",
      "Train Epoch: 273 [9984/17352 (58%)] Loss: -409487.718750\n",
      "Train Epoch: 273 [11392/17352 (66%)] Loss: -354384.187500\n",
      "Train Epoch: 273 [12800/17352 (74%)] Loss: -396279.093750\n",
      "Train Epoch: 273 [14208/17352 (82%)] Loss: -339587.437500\n",
      "Train Epoch: 273 [15515/17352 (89%)] Loss: -225551.640625\n",
      "Train Epoch: 273 [16226/17352 (94%)] Loss: -222628.890625\n",
      "Train Epoch: 273 [17021/17352 (98%)] Loss: -224554.218750\n",
      "    epoch          : 273\n",
      "    loss           : -342005.28794174705\n",
      "    val_loss       : -184411.06087239584\n",
      "Train Epoch: 274 [128/17352 (1%)] Loss: -369009.125000\n",
      "Train Epoch: 274 [1536/17352 (9%)] Loss: -388856.062500\n",
      "Train Epoch: 274 [2944/17352 (17%)] Loss: -354237.937500\n",
      "Train Epoch: 274 [4352/17352 (25%)] Loss: -378058.156250\n",
      "Train Epoch: 274 [5760/17352 (33%)] Loss: -374544.375000\n",
      "Train Epoch: 274 [7168/17352 (41%)] Loss: -401155.343750\n",
      "Train Epoch: 274 [8576/17352 (49%)] Loss: -402341.062500\n",
      "Train Epoch: 274 [9984/17352 (58%)] Loss: -413181.625000\n",
      "Train Epoch: 274 [11392/17352 (66%)] Loss: -270965.750000\n",
      "Train Epoch: 274 [12800/17352 (74%)] Loss: -376369.875000\n",
      "Train Epoch: 274 [14208/17352 (82%)] Loss: -370200.187500\n",
      "Train Epoch: 274 [15533/17352 (90%)] Loss: -270120.125000\n",
      "Train Epoch: 274 [16251/17352 (94%)] Loss: -254745.156250\n",
      "Train Epoch: 274 [17051/17352 (98%)] Loss: -234032.468750\n",
      "    epoch          : 274\n",
      "    loss           : -337911.832919332\n",
      "    val_loss       : -189627.3748046875\n",
      "Train Epoch: 275 [128/17352 (1%)] Loss: -387523.968750\n",
      "Train Epoch: 275 [1536/17352 (9%)] Loss: -378287.593750\n",
      "Train Epoch: 275 [2944/17352 (17%)] Loss: -403115.125000\n",
      "Train Epoch: 275 [4352/17352 (25%)] Loss: -322420.500000\n",
      "Train Epoch: 275 [5760/17352 (33%)] Loss: -374868.656250\n",
      "Train Epoch: 275 [7168/17352 (41%)] Loss: -382185.812500\n",
      "Train Epoch: 275 [8576/17352 (49%)] Loss: -408347.125000\n",
      "Train Epoch: 275 [9984/17352 (58%)] Loss: -398737.312500\n",
      "Train Epoch: 275 [11392/17352 (66%)] Loss: -391635.593750\n",
      "Train Epoch: 275 [12800/17352 (74%)] Loss: -411254.031250\n",
      "Train Epoch: 275 [14208/17352 (82%)] Loss: -325973.281250\n",
      "Train Epoch: 275 [15499/17352 (89%)] Loss: -145376.296875\n",
      "Train Epoch: 275 [16291/17352 (94%)] Loss: -332912.625000\n",
      "Train Epoch: 275 [16923/17352 (98%)] Loss: -90343.093750\n",
      "    epoch          : 275\n",
      "    loss           : -342000.1594811766\n",
      "    val_loss       : -193089.0747233073\n",
      "Train Epoch: 276 [128/17352 (1%)] Loss: -416478.500000\n",
      "Train Epoch: 276 [1536/17352 (9%)] Loss: -393121.125000\n",
      "Train Epoch: 276 [2944/17352 (17%)] Loss: -327683.812500\n",
      "Train Epoch: 276 [4352/17352 (25%)] Loss: -419186.343750\n",
      "Train Epoch: 276 [5760/17352 (33%)] Loss: -414343.812500\n",
      "Train Epoch: 276 [7168/17352 (41%)] Loss: -401060.968750\n",
      "Train Epoch: 276 [8576/17352 (49%)] Loss: -398861.406250\n",
      "Train Epoch: 276 [9984/17352 (58%)] Loss: -364020.625000\n",
      "Train Epoch: 276 [11392/17352 (66%)] Loss: -392452.031250\n",
      "Train Epoch: 276 [12800/17352 (74%)] Loss: -391676.000000\n",
      "Train Epoch: 276 [14208/17352 (82%)] Loss: -374147.750000\n",
      "Train Epoch: 276 [15525/17352 (89%)] Loss: -210753.843750\n",
      "Train Epoch: 276 [16165/17352 (93%)] Loss: -162075.484375\n",
      "Train Epoch: 276 [17021/17352 (98%)] Loss: -303162.375000\n",
      "    epoch          : 276\n",
      "    loss           : -344926.95913511957\n",
      "    val_loss       : -189133.58699544272\n",
      "Train Epoch: 277 [128/17352 (1%)] Loss: -398555.875000\n",
      "Train Epoch: 277 [1536/17352 (9%)] Loss: -394406.031250\n",
      "Train Epoch: 277 [2944/17352 (17%)] Loss: -413191.375000\n",
      "Train Epoch: 277 [4352/17352 (25%)] Loss: -321746.500000\n",
      "Train Epoch: 277 [5760/17352 (33%)] Loss: -375768.875000\n",
      "Train Epoch: 277 [7168/17352 (41%)] Loss: -411016.812500\n",
      "Train Epoch: 277 [8576/17352 (49%)] Loss: -407058.687500\n",
      "Train Epoch: 277 [9984/17352 (58%)] Loss: -405641.250000\n",
      "Train Epoch: 277 [11392/17352 (66%)] Loss: -350540.250000\n",
      "Train Epoch: 277 [12800/17352 (74%)] Loss: -410785.875000\n",
      "Train Epoch: 277 [14208/17352 (82%)] Loss: -414621.812500\n",
      "Train Epoch: 277 [15428/17352 (89%)] Loss: -85623.398438\n",
      "Train Epoch: 277 [16314/17352 (94%)] Loss: -355081.593750\n",
      "Train Epoch: 277 [17047/17352 (98%)] Loss: -362597.812500\n",
      "    epoch          : 277\n",
      "    loss           : -349168.7821544673\n",
      "    val_loss       : -186354.82389322916\n",
      "Train Epoch: 278 [128/17352 (1%)] Loss: -331796.937500\n",
      "Train Epoch: 278 [1536/17352 (9%)] Loss: -379033.000000\n",
      "Train Epoch: 278 [2944/17352 (17%)] Loss: -360399.843750\n",
      "Train Epoch: 278 [4352/17352 (25%)] Loss: -380451.468750\n",
      "Train Epoch: 278 [5760/17352 (33%)] Loss: -384479.531250\n",
      "Train Epoch: 278 [7168/17352 (41%)] Loss: -396006.843750\n",
      "Train Epoch: 278 [8576/17352 (49%)] Loss: -325308.375000\n",
      "Train Epoch: 278 [9984/17352 (58%)] Loss: -342317.750000\n",
      "Train Epoch: 278 [11392/17352 (66%)] Loss: -397850.531250\n",
      "Train Epoch: 278 [12800/17352 (74%)] Loss: -386390.250000\n",
      "Train Epoch: 278 [14208/17352 (82%)] Loss: -394695.125000\n",
      "Train Epoch: 278 [15556/17352 (90%)] Loss: -286539.875000\n",
      "Train Epoch: 278 [16516/17352 (95%)] Loss: -249630.750000\n",
      "Train Epoch: 278 [17143/17352 (99%)] Loss: -9983.952148\n",
      "    epoch          : 278\n",
      "    loss           : -343295.22861393663\n",
      "    val_loss       : -188210.48946940104\n",
      "Train Epoch: 279 [128/17352 (1%)] Loss: -378308.437500\n",
      "Train Epoch: 279 [1536/17352 (9%)] Loss: -404126.750000\n",
      "Train Epoch: 279 [2944/17352 (17%)] Loss: -352291.625000\n",
      "Train Epoch: 279 [4352/17352 (25%)] Loss: -366889.250000\n",
      "Train Epoch: 279 [5760/17352 (33%)] Loss: -391949.281250\n",
      "Train Epoch: 279 [7168/17352 (41%)] Loss: -391829.468750\n",
      "Train Epoch: 279 [8576/17352 (49%)] Loss: -59591.816406\n",
      "Train Epoch: 279 [9984/17352 (58%)] Loss: -352035.000000\n",
      "Train Epoch: 279 [11392/17352 (66%)] Loss: -416950.312500\n",
      "Train Epoch: 279 [12800/17352 (74%)] Loss: -392168.843750\n",
      "Train Epoch: 279 [14208/17352 (82%)] Loss: -386528.750000\n",
      "Train Epoch: 279 [15540/17352 (90%)] Loss: -271308.000000\n",
      "Train Epoch: 279 [16269/17352 (94%)] Loss: -173225.421875\n",
      "Train Epoch: 279 [17141/17352 (99%)] Loss: -351309.312500\n",
      "    epoch          : 279\n",
      "    loss           : -339257.3665157037\n",
      "    val_loss       : -195104.26625976563\n",
      "Train Epoch: 280 [128/17352 (1%)] Loss: -372670.531250\n",
      "Train Epoch: 280 [1536/17352 (9%)] Loss: -392178.437500\n",
      "Train Epoch: 280 [2944/17352 (17%)] Loss: -202687.968750\n",
      "Train Epoch: 280 [4352/17352 (25%)] Loss: -418412.750000\n",
      "Train Epoch: 280 [5760/17352 (33%)] Loss: -401806.656250\n",
      "Train Epoch: 280 [7168/17352 (41%)] Loss: -360002.875000\n",
      "Train Epoch: 280 [8576/17352 (49%)] Loss: -416509.125000\n",
      "Train Epoch: 280 [9984/17352 (58%)] Loss: -428536.468750\n",
      "Train Epoch: 280 [11392/17352 (66%)] Loss: -415577.781250\n",
      "Train Epoch: 280 [12800/17352 (74%)] Loss: -358980.656250\n",
      "Train Epoch: 280 [14208/17352 (82%)] Loss: -413399.250000\n",
      "Train Epoch: 280 [15538/17352 (90%)] Loss: -184485.343750\n",
      "Train Epoch: 280 [16214/17352 (93%)] Loss: -52242.945312\n",
      "Train Epoch: 280 [17022/17352 (98%)] Loss: -174966.281250\n",
      "    epoch          : 280\n",
      "    loss           : -354241.1909474622\n",
      "    val_loss       : -198539.61111653646\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch280.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 281 [128/17352 (1%)] Loss: -344479.000000\n",
      "Train Epoch: 281 [1536/17352 (9%)] Loss: -374192.843750\n",
      "Train Epoch: 281 [2944/17352 (17%)] Loss: -425771.250000\n",
      "Train Epoch: 281 [4352/17352 (25%)] Loss: -413275.500000\n",
      "Train Epoch: 281 [5760/17352 (33%)] Loss: -287026.812500\n",
      "Train Epoch: 281 [7168/17352 (41%)] Loss: -320199.750000\n",
      "Train Epoch: 281 [8576/17352 (49%)] Loss: -384630.906250\n",
      "Train Epoch: 281 [9984/17352 (58%)] Loss: -409616.406250\n",
      "Train Epoch: 281 [11392/17352 (66%)] Loss: -373960.250000\n",
      "Train Epoch: 281 [12800/17352 (74%)] Loss: -435687.593750\n",
      "Train Epoch: 281 [14208/17352 (82%)] Loss: -395722.406250\n",
      "Train Epoch: 281 [15470/17352 (89%)] Loss: -352391.375000\n",
      "Train Epoch: 281 [16330/17352 (94%)] Loss: -299682.750000\n",
      "Train Epoch: 281 [17012/17352 (98%)] Loss: -266270.125000\n",
      "    epoch          : 281\n",
      "    loss           : -348781.60560966335\n",
      "    val_loss       : -192638.47010091145\n",
      "Train Epoch: 282 [128/17352 (1%)] Loss: -386556.562500\n",
      "Train Epoch: 282 [1536/17352 (9%)] Loss: -329947.562500\n",
      "Train Epoch: 282 [2944/17352 (17%)] Loss: -400762.031250\n",
      "Train Epoch: 282 [4352/17352 (25%)] Loss: -431670.093750\n",
      "Train Epoch: 282 [5760/17352 (33%)] Loss: -391462.468750\n",
      "Train Epoch: 282 [7168/17352 (41%)] Loss: -369104.500000\n",
      "Train Epoch: 282 [8576/17352 (49%)] Loss: -426653.125000\n",
      "Train Epoch: 282 [9984/17352 (58%)] Loss: -409861.500000\n",
      "Train Epoch: 282 [11392/17352 (66%)] Loss: -388928.250000\n",
      "Train Epoch: 282 [12800/17352 (74%)] Loss: -410516.437500\n",
      "Train Epoch: 282 [14208/17352 (82%)] Loss: -404278.000000\n",
      "Train Epoch: 282 [15518/17352 (89%)] Loss: -133847.406250\n",
      "Train Epoch: 282 [16259/17352 (94%)] Loss: -262053.671875\n",
      "Train Epoch: 282 [17025/17352 (98%)] Loss: -319704.812500\n",
      "    epoch          : 282\n",
      "    loss           : -355565.7051436661\n",
      "    val_loss       : -192964.81541341144\n",
      "Train Epoch: 283 [128/17352 (1%)] Loss: -434914.406250\n",
      "Train Epoch: 283 [1536/17352 (9%)] Loss: -395463.562500\n",
      "Train Epoch: 283 [2944/17352 (17%)] Loss: -383211.250000\n",
      "Train Epoch: 283 [4352/17352 (25%)] Loss: -399201.750000\n",
      "Train Epoch: 283 [5760/17352 (33%)] Loss: -354444.031250\n",
      "Train Epoch: 283 [7168/17352 (41%)] Loss: -286309.437500\n",
      "Train Epoch: 283 [8576/17352 (49%)] Loss: -401377.312500\n",
      "Train Epoch: 283 [9984/17352 (58%)] Loss: -367920.875000\n",
      "Train Epoch: 283 [11392/17352 (66%)] Loss: -336745.781250\n",
      "Train Epoch: 283 [12800/17352 (74%)] Loss: -422922.500000\n",
      "Train Epoch: 283 [14208/17352 (82%)] Loss: -404310.531250\n",
      "Train Epoch: 283 [15488/17352 (89%)] Loss: -160478.515625\n",
      "Train Epoch: 283 [16148/17352 (93%)] Loss: -375392.437500\n",
      "Train Epoch: 283 [16898/17352 (97%)] Loss: -201812.531250\n",
      "    epoch          : 283\n",
      "    loss           : -348774.6440855705\n",
      "    val_loss       : -190731.3549967448\n",
      "Train Epoch: 284 [128/17352 (1%)] Loss: -398195.531250\n",
      "Train Epoch: 284 [1536/17352 (9%)] Loss: -419819.687500\n",
      "Train Epoch: 284 [2944/17352 (17%)] Loss: -401312.093750\n",
      "Train Epoch: 284 [4352/17352 (25%)] Loss: -395958.937500\n",
      "Train Epoch: 284 [5760/17352 (33%)] Loss: -363459.250000\n",
      "Train Epoch: 284 [7168/17352 (41%)] Loss: -352473.187500\n",
      "Train Epoch: 284 [8576/17352 (49%)] Loss: -334543.437500\n",
      "Train Epoch: 284 [9984/17352 (58%)] Loss: -407392.687500\n",
      "Train Epoch: 284 [11392/17352 (66%)] Loss: -410207.062500\n",
      "Train Epoch: 284 [12800/17352 (74%)] Loss: -416969.375000\n",
      "Train Epoch: 284 [14208/17352 (82%)] Loss: -371332.906250\n",
      "Train Epoch: 284 [15404/17352 (89%)] Loss: -9277.708984\n",
      "Train Epoch: 284 [16225/17352 (94%)] Loss: -14243.533203\n",
      "Train Epoch: 284 [17109/17352 (99%)] Loss: -250017.500000\n",
      "    epoch          : 284\n",
      "    loss           : -350071.9567428691\n",
      "    val_loss       : -196550.65405273438\n",
      "Train Epoch: 285 [128/17352 (1%)] Loss: -436562.187500\n",
      "Train Epoch: 285 [1536/17352 (9%)] Loss: -412628.781250\n",
      "Train Epoch: 285 [2944/17352 (17%)] Loss: -393297.250000\n",
      "Train Epoch: 285 [4352/17352 (25%)] Loss: -421071.875000\n",
      "Train Epoch: 285 [5760/17352 (33%)] Loss: -391840.031250\n",
      "Train Epoch: 285 [7168/17352 (41%)] Loss: -291223.375000\n",
      "Train Epoch: 285 [8576/17352 (49%)] Loss: -410022.031250\n",
      "Train Epoch: 285 [9984/17352 (58%)] Loss: -421872.562500\n",
      "Train Epoch: 285 [11392/17352 (66%)] Loss: -371686.156250\n",
      "Train Epoch: 285 [12800/17352 (74%)] Loss: -382760.125000\n",
      "Train Epoch: 285 [14208/17352 (82%)] Loss: -415905.093750\n",
      "Train Epoch: 285 [15519/17352 (89%)] Loss: -306270.812500\n",
      "Train Epoch: 285 [16418/17352 (95%)] Loss: -292233.281250\n",
      "Train Epoch: 285 [17033/17352 (98%)] Loss: -302011.656250\n",
      "    epoch          : 285\n",
      "    loss           : -355292.6720650692\n",
      "    val_loss       : -195608.59803059895\n",
      "Train Epoch: 286 [128/17352 (1%)] Loss: -403716.750000\n",
      "Train Epoch: 286 [1536/17352 (9%)] Loss: -418678.375000\n",
      "Train Epoch: 286 [2944/17352 (17%)] Loss: -410675.031250\n",
      "Train Epoch: 286 [4352/17352 (25%)] Loss: -343641.125000\n",
      "Train Epoch: 286 [5760/17352 (33%)] Loss: -391035.656250\n",
      "Train Epoch: 286 [7168/17352 (41%)] Loss: -427896.625000\n",
      "Train Epoch: 286 [8576/17352 (49%)] Loss: -407122.656250\n",
      "Train Epoch: 286 [9984/17352 (58%)] Loss: -401741.875000\n",
      "Train Epoch: 286 [11392/17352 (66%)] Loss: -405978.281250\n",
      "Train Epoch: 286 [12800/17352 (74%)] Loss: -393097.500000\n",
      "Train Epoch: 286 [14208/17352 (82%)] Loss: -422004.625000\n",
      "Train Epoch: 286 [15444/17352 (89%)] Loss: -126722.039062\n",
      "Train Epoch: 286 [16410/17352 (95%)] Loss: -357607.656250\n",
      "Train Epoch: 286 [17138/17352 (99%)] Loss: -247862.250000\n",
      "    epoch          : 286\n",
      "    loss           : -349913.661339267\n",
      "    val_loss       : -194882.5239420573\n",
      "Train Epoch: 287 [128/17352 (1%)] Loss: -408591.250000\n",
      "Train Epoch: 287 [1536/17352 (9%)] Loss: -413890.343750\n",
      "Train Epoch: 287 [2944/17352 (17%)] Loss: -307066.062500\n",
      "Train Epoch: 287 [4352/17352 (25%)] Loss: -315215.375000\n",
      "Train Epoch: 287 [5760/17352 (33%)] Loss: -359131.531250\n",
      "Train Epoch: 287 [7168/17352 (41%)] Loss: -404704.656250\n",
      "Train Epoch: 287 [8576/17352 (49%)] Loss: -400205.968750\n",
      "Train Epoch: 287 [9984/17352 (58%)] Loss: -422062.218750\n",
      "Train Epoch: 287 [11392/17352 (66%)] Loss: -423950.656250\n",
      "Train Epoch: 287 [12800/17352 (74%)] Loss: -408918.000000\n",
      "Train Epoch: 287 [14208/17352 (82%)] Loss: -379907.656250\n",
      "Train Epoch: 287 [15512/17352 (89%)] Loss: -233832.687500\n",
      "Train Epoch: 287 [16418/17352 (95%)] Loss: -297155.718750\n",
      "Train Epoch: 287 [17012/17352 (98%)] Loss: -364032.406250\n",
      "    epoch          : 287\n",
      "    loss           : -348208.24495333474\n",
      "    val_loss       : -178443.12978515626\n",
      "Train Epoch: 288 [128/17352 (1%)] Loss: -391084.562500\n",
      "Train Epoch: 288 [1536/17352 (9%)] Loss: -394092.093750\n",
      "Train Epoch: 288 [2944/17352 (17%)] Loss: -402412.593750\n",
      "Train Epoch: 288 [4352/17352 (25%)] Loss: -382079.468750\n",
      "Train Epoch: 288 [5760/17352 (33%)] Loss: -285930.000000\n",
      "Train Epoch: 288 [7168/17352 (41%)] Loss: -399168.218750\n",
      "Train Epoch: 288 [8576/17352 (49%)] Loss: -314010.562500\n",
      "Train Epoch: 288 [9984/17352 (58%)] Loss: -409007.187500\n",
      "Train Epoch: 288 [11392/17352 (66%)] Loss: -415198.625000\n",
      "Train Epoch: 288 [12800/17352 (74%)] Loss: -347552.031250\n",
      "Train Epoch: 288 [14208/17352 (82%)] Loss: -379433.718750\n",
      "Train Epoch: 288 [15534/17352 (90%)] Loss: -288382.468750\n",
      "Train Epoch: 288 [16336/17352 (94%)] Loss: -225788.375000\n",
      "Train Epoch: 288 [17050/17352 (98%)] Loss: -272822.687500\n",
      "    epoch          : 288\n",
      "    loss           : -339171.7650744547\n",
      "    val_loss       : -186359.37475585938\n",
      "Train Epoch: 289 [128/17352 (1%)] Loss: -362187.687500\n",
      "Train Epoch: 289 [1536/17352 (9%)] Loss: -398105.625000\n",
      "Train Epoch: 289 [2944/17352 (17%)] Loss: -405168.750000\n",
      "Train Epoch: 289 [4352/17352 (25%)] Loss: -326856.062500\n",
      "Train Epoch: 289 [5760/17352 (33%)] Loss: -360129.312500\n",
      "Train Epoch: 289 [7168/17352 (41%)] Loss: -391614.437500\n",
      "Train Epoch: 289 [8576/17352 (49%)] Loss: -391469.062500\n",
      "Train Epoch: 289 [9984/17352 (58%)] Loss: -379781.562500\n",
      "Train Epoch: 289 [11392/17352 (66%)] Loss: -392146.031250\n",
      "Train Epoch: 289 [12800/17352 (74%)] Loss: -392880.687500\n",
      "Train Epoch: 289 [14208/17352 (82%)] Loss: -406512.906250\n",
      "Train Epoch: 289 [15450/17352 (89%)] Loss: -15387.357422\n",
      "Train Epoch: 289 [16369/17352 (94%)] Loss: -150604.000000\n",
      "Train Epoch: 289 [16992/17352 (98%)] Loss: -38828.984375\n",
      "    epoch          : 289\n",
      "    loss           : -340389.4653942953\n",
      "    val_loss       : -182593.95944010417\n",
      "Train Epoch: 290 [128/17352 (1%)] Loss: -375239.687500\n",
      "Train Epoch: 290 [1536/17352 (9%)] Loss: -363098.937500\n",
      "Train Epoch: 290 [2944/17352 (17%)] Loss: -385165.000000\n",
      "Train Epoch: 290 [4352/17352 (25%)] Loss: -397952.406250\n",
      "Train Epoch: 290 [5760/17352 (33%)] Loss: -388752.812500\n",
      "Train Epoch: 290 [7168/17352 (41%)] Loss: -390626.843750\n",
      "Train Epoch: 290 [8576/17352 (49%)] Loss: -336330.750000\n",
      "Train Epoch: 290 [9984/17352 (58%)] Loss: -383080.843750\n",
      "Train Epoch: 290 [11392/17352 (66%)] Loss: -366323.812500\n",
      "Train Epoch: 290 [12800/17352 (74%)] Loss: -351331.625000\n",
      "Train Epoch: 290 [14208/17352 (82%)] Loss: -382324.281250\n",
      "Train Epoch: 290 [15406/17352 (89%)] Loss: -15713.505859\n",
      "Train Epoch: 290 [16222/17352 (93%)] Loss: -154194.687500\n",
      "Train Epoch: 290 [17032/17352 (98%)] Loss: -299506.750000\n",
      "    epoch          : 290\n",
      "    loss           : -337994.4592858641\n",
      "    val_loss       : -185675.1936686198\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch290.pth ...\n",
      "Train Epoch: 291 [128/17352 (1%)] Loss: -392921.750000\n",
      "Train Epoch: 291 [1536/17352 (9%)] Loss: -141187.468750\n",
      "Train Epoch: 291 [2944/17352 (17%)] Loss: -369152.000000\n",
      "Train Epoch: 291 [4352/17352 (25%)] Loss: -390262.156250\n",
      "Train Epoch: 291 [5760/17352 (33%)] Loss: -395895.125000\n",
      "Train Epoch: 291 [7168/17352 (41%)] Loss: -400993.531250\n",
      "Train Epoch: 291 [8576/17352 (49%)] Loss: -405643.312500\n",
      "Train Epoch: 291 [9984/17352 (58%)] Loss: -411630.187500\n",
      "Train Epoch: 291 [11392/17352 (66%)] Loss: -397396.156250\n",
      "Train Epoch: 291 [12800/17352 (74%)] Loss: -303908.812500\n",
      "Train Epoch: 291 [14208/17352 (82%)] Loss: -402577.000000\n",
      "Train Epoch: 291 [15487/17352 (89%)] Loss: -159187.296875\n",
      "Train Epoch: 291 [16226/17352 (94%)] Loss: -40831.683594\n",
      "Train Epoch: 291 [17078/17352 (98%)] Loss: -292175.937500\n",
      "    epoch          : 291\n",
      "    loss           : -334835.42438850144\n",
      "    val_loss       : -191206.71010742188\n",
      "Train Epoch: 292 [128/17352 (1%)] Loss: -385148.625000\n",
      "Train Epoch: 292 [1536/17352 (9%)] Loss: -403324.593750\n",
      "Train Epoch: 292 [2944/17352 (17%)] Loss: -374150.093750\n",
      "Train Epoch: 292 [4352/17352 (25%)] Loss: -254684.281250\n",
      "Train Epoch: 292 [5760/17352 (33%)] Loss: -388177.875000\n",
      "Train Epoch: 292 [7168/17352 (41%)] Loss: -396686.250000\n",
      "Train Epoch: 292 [8576/17352 (49%)] Loss: -404707.531250\n",
      "Train Epoch: 292 [9984/17352 (58%)] Loss: -420634.812500\n",
      "Train Epoch: 292 [11392/17352 (66%)] Loss: -367403.406250\n",
      "Train Epoch: 292 [12800/17352 (74%)] Loss: -376761.687500\n",
      "Train Epoch: 292 [14208/17352 (82%)] Loss: -415602.437500\n",
      "Train Epoch: 292 [15552/17352 (90%)] Loss: -239195.187500\n",
      "Train Epoch: 292 [16325/17352 (94%)] Loss: -48289.113281\n",
      "Train Epoch: 292 [16965/17352 (98%)] Loss: -39018.250000\n",
      "    epoch          : 292\n",
      "    loss           : -344723.6748571204\n",
      "    val_loss       : -190408.88720703125\n",
      "Train Epoch: 293 [128/17352 (1%)] Loss: -406863.437500\n",
      "Train Epoch: 293 [1536/17352 (9%)] Loss: -345474.281250\n",
      "Train Epoch: 293 [2944/17352 (17%)] Loss: -317324.968750\n",
      "Train Epoch: 293 [4352/17352 (25%)] Loss: -389225.781250\n",
      "Train Epoch: 293 [5760/17352 (33%)] Loss: -403474.656250\n",
      "Train Epoch: 293 [7168/17352 (41%)] Loss: -370592.437500\n",
      "Train Epoch: 293 [8576/17352 (49%)] Loss: -392160.812500\n",
      "Train Epoch: 293 [9984/17352 (58%)] Loss: -348844.093750\n",
      "Train Epoch: 293 [11392/17352 (66%)] Loss: -387281.187500\n",
      "Train Epoch: 293 [12800/17352 (74%)] Loss: -402463.218750\n",
      "Train Epoch: 293 [14208/17352 (82%)] Loss: -331869.062500\n",
      "Train Epoch: 293 [15442/17352 (89%)] Loss: -227026.015625\n",
      "Train Epoch: 293 [16161/17352 (93%)] Loss: -249919.562500\n",
      "Train Epoch: 293 [16886/17352 (97%)] Loss: -282385.531250\n",
      "    epoch          : 293\n",
      "    loss           : -340204.09601772233\n",
      "    val_loss       : -184257.82106119793\n",
      "Train Epoch: 294 [128/17352 (1%)] Loss: -343941.281250\n",
      "Train Epoch: 294 [1536/17352 (9%)] Loss: -364066.718750\n",
      "Train Epoch: 294 [2944/17352 (17%)] Loss: -395368.156250\n",
      "Train Epoch: 294 [4352/17352 (25%)] Loss: -368101.937500\n",
      "Train Epoch: 294 [5760/17352 (33%)] Loss: -367867.437500\n",
      "Train Epoch: 294 [7168/17352 (41%)] Loss: -395983.656250\n",
      "Train Epoch: 294 [8576/17352 (49%)] Loss: -381140.593750\n",
      "Train Epoch: 294 [9984/17352 (58%)] Loss: -384199.187500\n",
      "Train Epoch: 294 [11392/17352 (66%)] Loss: -390429.562500\n",
      "Train Epoch: 294 [12800/17352 (74%)] Loss: -391041.875000\n",
      "Train Epoch: 294 [14208/17352 (82%)] Loss: -394835.375000\n",
      "Train Epoch: 294 [15540/17352 (90%)] Loss: -200735.984375\n",
      "Train Epoch: 294 [16260/17352 (94%)] Loss: -43582.191406\n",
      "Train Epoch: 294 [17045/17352 (98%)] Loss: -255617.578125\n",
      "    epoch          : 294\n",
      "    loss           : -340049.41694630875\n",
      "    val_loss       : -187984.03040364583\n",
      "Train Epoch: 295 [128/17352 (1%)] Loss: -387119.843750\n",
      "Train Epoch: 295 [1536/17352 (9%)] Loss: -309634.281250\n",
      "Train Epoch: 295 [2944/17352 (17%)] Loss: -237321.734375\n",
      "Train Epoch: 295 [4352/17352 (25%)] Loss: -341145.031250\n",
      "Train Epoch: 295 [5760/17352 (33%)] Loss: -379658.031250\n",
      "Train Epoch: 295 [7168/17352 (41%)] Loss: -376242.343750\n",
      "Train Epoch: 295 [8576/17352 (49%)] Loss: -393754.906250\n",
      "Train Epoch: 295 [9984/17352 (58%)] Loss: -173288.312500\n",
      "Train Epoch: 295 [11392/17352 (66%)] Loss: -410220.562500\n",
      "Train Epoch: 295 [12800/17352 (74%)] Loss: -328695.656250\n",
      "Train Epoch: 295 [14208/17352 (82%)] Loss: -393831.562500\n",
      "Train Epoch: 295 [15502/17352 (89%)] Loss: -272435.375000\n",
      "Train Epoch: 295 [16358/17352 (94%)] Loss: -237889.937500\n",
      "Train Epoch: 295 [17105/17352 (99%)] Loss: -181698.750000\n",
      "    epoch          : 295\n",
      "    loss           : -331204.80991112627\n",
      "    val_loss       : -186495.62189127604\n",
      "Train Epoch: 296 [128/17352 (1%)] Loss: -389709.531250\n",
      "Train Epoch: 296 [1536/17352 (9%)] Loss: -376483.125000\n",
      "Train Epoch: 296 [2944/17352 (17%)] Loss: -392083.156250\n",
      "Train Epoch: 296 [4352/17352 (25%)] Loss: -404314.031250\n",
      "Train Epoch: 296 [5760/17352 (33%)] Loss: -339827.312500\n",
      "Train Epoch: 296 [7168/17352 (41%)] Loss: -369918.937500\n",
      "Train Epoch: 296 [8576/17352 (49%)] Loss: -395824.593750\n",
      "Train Epoch: 296 [9984/17352 (58%)] Loss: -389273.937500\n",
      "Train Epoch: 296 [11392/17352 (66%)] Loss: -382532.000000\n",
      "Train Epoch: 296 [12800/17352 (74%)] Loss: -400089.843750\n",
      "Train Epoch: 296 [14208/17352 (82%)] Loss: -314685.750000\n",
      "Train Epoch: 296 [15419/17352 (89%)] Loss: -16942.562500\n",
      "Train Epoch: 296 [16187/17352 (93%)] Loss: -231801.421875\n",
      "Train Epoch: 296 [16976/17352 (98%)] Loss: -234968.015625\n",
      "    epoch          : 296\n",
      "    loss           : -335934.6620274486\n",
      "    val_loss       : -191443.50874023436\n",
      "Train Epoch: 297 [128/17352 (1%)] Loss: -397533.187500\n",
      "Train Epoch: 297 [1536/17352 (9%)] Loss: -391719.187500\n",
      "Train Epoch: 297 [2944/17352 (17%)] Loss: -388404.187500\n",
      "Train Epoch: 297 [4352/17352 (25%)] Loss: -398912.000000\n",
      "Train Epoch: 297 [5760/17352 (33%)] Loss: -406611.062500\n",
      "Train Epoch: 297 [7168/17352 (41%)] Loss: -278229.187500\n",
      "Train Epoch: 297 [8576/17352 (49%)] Loss: -403293.812500\n",
      "Train Epoch: 297 [9984/17352 (58%)] Loss: -398252.562500\n",
      "Train Epoch: 297 [11392/17352 (66%)] Loss: -397286.593750\n",
      "Train Epoch: 297 [12800/17352 (74%)] Loss: -301169.093750\n",
      "Train Epoch: 297 [14208/17352 (82%)] Loss: -361905.375000\n",
      "Train Epoch: 297 [15573/17352 (90%)] Loss: -363454.437500\n",
      "Train Epoch: 297 [16157/17352 (93%)] Loss: -194704.781250\n",
      "Train Epoch: 297 [17000/17352 (98%)] Loss: -322129.937500\n",
      "    epoch          : 297\n",
      "    loss           : -346530.9766018247\n",
      "    val_loss       : -200389.71791992188\n",
      "Train Epoch: 298 [128/17352 (1%)] Loss: -397979.625000\n",
      "Train Epoch: 298 [1536/17352 (9%)] Loss: -383972.125000\n",
      "Train Epoch: 298 [2944/17352 (17%)] Loss: -354205.218750\n",
      "Train Epoch: 298 [4352/17352 (25%)] Loss: -338968.875000\n",
      "Train Epoch: 298 [5760/17352 (33%)] Loss: -380160.062500\n",
      "Train Epoch: 298 [7168/17352 (41%)] Loss: -360654.312500\n",
      "Train Epoch: 298 [8576/17352 (49%)] Loss: -403742.906250\n",
      "Train Epoch: 298 [9984/17352 (58%)] Loss: -318133.843750\n",
      "Train Epoch: 298 [11392/17352 (66%)] Loss: -413475.562500\n",
      "Train Epoch: 298 [12800/17352 (74%)] Loss: -326986.343750\n",
      "Train Epoch: 298 [14208/17352 (82%)] Loss: -391178.906250\n",
      "Train Epoch: 298 [15448/17352 (89%)] Loss: -45245.109375\n",
      "Train Epoch: 298 [16285/17352 (94%)] Loss: -296794.437500\n",
      "Train Epoch: 298 [16947/17352 (98%)] Loss: -238578.296875\n",
      "    epoch          : 298\n",
      "    loss           : -346509.67450975254\n",
      "    val_loss       : -193055.1713216146\n",
      "Train Epoch: 299 [128/17352 (1%)] Loss: -407050.218750\n",
      "Train Epoch: 299 [1536/17352 (9%)] Loss: -160168.046875\n",
      "Train Epoch: 299 [2944/17352 (17%)] Loss: -364413.281250\n",
      "Train Epoch: 299 [4352/17352 (25%)] Loss: -369727.781250\n",
      "Train Epoch: 299 [5760/17352 (33%)] Loss: -394447.000000\n",
      "Train Epoch: 299 [7168/17352 (41%)] Loss: -356016.750000\n",
      "Train Epoch: 299 [8576/17352 (49%)] Loss: -249898.156250\n",
      "Train Epoch: 299 [9984/17352 (58%)] Loss: -405742.812500\n",
      "Train Epoch: 299 [11392/17352 (66%)] Loss: -423723.937500\n",
      "Train Epoch: 299 [12800/17352 (74%)] Loss: -413690.875000\n",
      "Train Epoch: 299 [14208/17352 (82%)] Loss: -401994.250000\n",
      "Train Epoch: 299 [15496/17352 (89%)] Loss: -122610.546875\n",
      "Train Epoch: 299 [16220/17352 (93%)] Loss: -293565.812500\n",
      "Train Epoch: 299 [16999/17352 (98%)] Loss: -90784.382812\n",
      "    epoch          : 299\n",
      "    loss           : -346693.32809878356\n",
      "    val_loss       : -199042.98759765626\n",
      "Train Epoch: 300 [128/17352 (1%)] Loss: -341091.750000\n",
      "Train Epoch: 300 [1536/17352 (9%)] Loss: -394572.500000\n",
      "Train Epoch: 300 [2944/17352 (17%)] Loss: -421475.375000\n",
      "Train Epoch: 300 [4352/17352 (25%)] Loss: -399477.656250\n",
      "Train Epoch: 300 [5760/17352 (33%)] Loss: -318476.781250\n",
      "Train Epoch: 300 [7168/17352 (41%)] Loss: -423979.812500\n",
      "Train Epoch: 300 [8576/17352 (49%)] Loss: -391432.562500\n",
      "Train Epoch: 300 [9984/17352 (58%)] Loss: -407118.218750\n",
      "Train Epoch: 300 [11392/17352 (66%)] Loss: -343422.531250\n",
      "Train Epoch: 300 [12800/17352 (74%)] Loss: -400703.468750\n",
      "Train Epoch: 300 [14208/17352 (82%)] Loss: -400694.500000\n",
      "Train Epoch: 300 [15529/17352 (89%)] Loss: -242243.000000\n",
      "Train Epoch: 300 [16296/17352 (94%)] Loss: -143272.921875\n",
      "Train Epoch: 300 [16925/17352 (98%)] Loss: -198988.843750\n",
      "    epoch          : 300\n",
      "    loss           : -343070.94436215394\n",
      "    val_loss       : -189651.6442220052\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [128/17352 (1%)] Loss: -373601.968750\n",
      "Train Epoch: 301 [1536/17352 (9%)] Loss: -398074.625000\n",
      "Train Epoch: 301 [2944/17352 (17%)] Loss: -381081.687500\n",
      "Train Epoch: 301 [4352/17352 (25%)] Loss: -379334.312500\n",
      "Train Epoch: 301 [5760/17352 (33%)] Loss: -430873.562500\n",
      "Train Epoch: 301 [7168/17352 (41%)] Loss: -379749.000000\n",
      "Train Epoch: 301 [8576/17352 (49%)] Loss: -426303.968750\n",
      "Train Epoch: 301 [9984/17352 (58%)] Loss: -359627.562500\n",
      "Train Epoch: 301 [11392/17352 (66%)] Loss: -393056.781250\n",
      "Train Epoch: 301 [12800/17352 (74%)] Loss: -378553.625000\n",
      "Train Epoch: 301 [14208/17352 (82%)] Loss: -384441.968750\n",
      "Train Epoch: 301 [15539/17352 (90%)] Loss: -292743.937500\n",
      "Train Epoch: 301 [16329/17352 (94%)] Loss: -151282.656250\n",
      "Train Epoch: 301 [17085/17352 (98%)] Loss: -42179.703125\n",
      "    epoch          : 301\n",
      "    loss           : -352983.6971312657\n",
      "    val_loss       : -188985.89165039064\n",
      "Train Epoch: 302 [128/17352 (1%)] Loss: -372644.250000\n",
      "Train Epoch: 302 [1536/17352 (9%)] Loss: -382537.906250\n",
      "Train Epoch: 302 [2944/17352 (17%)] Loss: -299660.218750\n",
      "Train Epoch: 302 [4352/17352 (25%)] Loss: -410754.625000\n",
      "Train Epoch: 302 [5760/17352 (33%)] Loss: -442598.125000\n",
      "Train Epoch: 302 [7168/17352 (41%)] Loss: -413019.500000\n",
      "Train Epoch: 302 [8576/17352 (49%)] Loss: -388847.031250\n",
      "Train Epoch: 302 [9984/17352 (58%)] Loss: -398165.843750\n",
      "Train Epoch: 302 [11392/17352 (66%)] Loss: -372625.843750\n",
      "Train Epoch: 302 [12800/17352 (74%)] Loss: -392917.187500\n",
      "Train Epoch: 302 [14208/17352 (82%)] Loss: -361566.000000\n",
      "Train Epoch: 302 [15449/17352 (89%)] Loss: -17878.093750\n",
      "Train Epoch: 302 [16407/17352 (95%)] Loss: -277533.593750\n",
      "Train Epoch: 302 [17086/17352 (98%)] Loss: -373553.781250\n",
      "    epoch          : 302\n",
      "    loss           : -348548.18832581793\n",
      "    val_loss       : -183832.89653320314\n",
      "Train Epoch: 303 [128/17352 (1%)] Loss: -412041.000000\n",
      "Train Epoch: 303 [1536/17352 (9%)] Loss: -366717.312500\n",
      "Train Epoch: 303 [2944/17352 (17%)] Loss: -381456.000000\n",
      "Train Epoch: 303 [4352/17352 (25%)] Loss: -275934.718750\n",
      "Train Epoch: 303 [5760/17352 (33%)] Loss: -405117.281250\n",
      "Train Epoch: 303 [7168/17352 (41%)] Loss: -401208.062500\n",
      "Train Epoch: 303 [8576/17352 (49%)] Loss: -279963.281250\n",
      "Train Epoch: 303 [9984/17352 (58%)] Loss: -378745.125000\n",
      "Train Epoch: 303 [11392/17352 (66%)] Loss: -370843.312500\n",
      "Train Epoch: 303 [12800/17352 (74%)] Loss: -381731.812500\n",
      "Train Epoch: 303 [14208/17352 (82%)] Loss: -414323.843750\n",
      "Train Epoch: 303 [15500/17352 (89%)] Loss: -263599.437500\n",
      "Train Epoch: 303 [16212/17352 (93%)] Loss: -114137.578125\n",
      "Train Epoch: 303 [17037/17352 (98%)] Loss: -102610.859375\n",
      "    epoch          : 303\n",
      "    loss           : -339101.8495897127\n",
      "    val_loss       : -188052.9647298177\n",
      "Train Epoch: 304 [128/17352 (1%)] Loss: -377029.968750\n",
      "Train Epoch: 304 [1536/17352 (9%)] Loss: -367140.375000\n",
      "Train Epoch: 304 [2944/17352 (17%)] Loss: -361789.687500\n",
      "Train Epoch: 304 [4352/17352 (25%)] Loss: -413870.156250\n",
      "Train Epoch: 304 [5760/17352 (33%)] Loss: -417194.343750\n",
      "Train Epoch: 304 [7168/17352 (41%)] Loss: -410794.187500\n",
      "Train Epoch: 304 [8576/17352 (49%)] Loss: -301247.718750\n",
      "Train Epoch: 304 [9984/17352 (58%)] Loss: -395737.218750\n",
      "Train Epoch: 304 [11392/17352 (66%)] Loss: -367386.437500\n",
      "Train Epoch: 304 [12800/17352 (74%)] Loss: -379088.468750\n",
      "Train Epoch: 304 [14208/17352 (82%)] Loss: -382621.406250\n",
      "Train Epoch: 304 [15495/17352 (89%)] Loss: -307235.093750\n",
      "Train Epoch: 304 [16192/17352 (93%)] Loss: -38353.511719\n",
      "Train Epoch: 304 [17049/17352 (98%)] Loss: -266383.312500\n",
      "    epoch          : 304\n",
      "    loss           : -348269.2348665583\n",
      "    val_loss       : -194874.3556640625\n",
      "Train Epoch: 305 [128/17352 (1%)] Loss: -396038.625000\n",
      "Train Epoch: 305 [1536/17352 (9%)] Loss: -275963.500000\n",
      "Train Epoch: 305 [2944/17352 (17%)] Loss: -426204.843750\n",
      "Train Epoch: 305 [4352/17352 (25%)] Loss: -395802.125000\n",
      "Train Epoch: 305 [5760/17352 (33%)] Loss: -366184.125000\n",
      "Train Epoch: 305 [7168/17352 (41%)] Loss: -416011.406250\n",
      "Train Epoch: 305 [8576/17352 (49%)] Loss: -416297.031250\n",
      "Train Epoch: 305 [9984/17352 (58%)] Loss: -409293.312500\n",
      "Train Epoch: 305 [11392/17352 (66%)] Loss: -364694.000000\n",
      "Train Epoch: 305 [12800/17352 (74%)] Loss: -360299.375000\n",
      "Train Epoch: 305 [14208/17352 (82%)] Loss: -350291.343750\n",
      "Train Epoch: 305 [15463/17352 (89%)] Loss: -275278.312500\n",
      "Train Epoch: 305 [16360/17352 (94%)] Loss: -7783.533203\n",
      "Train Epoch: 305 [17096/17352 (99%)] Loss: -327087.156250\n",
      "    epoch          : 305\n",
      "    loss           : -350660.6856124161\n",
      "    val_loss       : -194068.85483398437\n",
      "Train Epoch: 306 [128/17352 (1%)] Loss: -402189.562500\n",
      "Train Epoch: 306 [1536/17352 (9%)] Loss: -427819.250000\n",
      "Train Epoch: 306 [2944/17352 (17%)] Loss: -398472.531250\n",
      "Train Epoch: 306 [4352/17352 (25%)] Loss: -425783.750000\n",
      "Train Epoch: 306 [5760/17352 (33%)] Loss: -421234.625000\n",
      "Train Epoch: 306 [7168/17352 (41%)] Loss: -401130.750000\n",
      "Train Epoch: 306 [8576/17352 (49%)] Loss: -426613.093750\n",
      "Train Epoch: 306 [9984/17352 (58%)] Loss: -329464.968750\n",
      "Train Epoch: 306 [11392/17352 (66%)] Loss: -365121.375000\n",
      "Train Epoch: 306 [12800/17352 (74%)] Loss: -417817.468750\n",
      "Train Epoch: 306 [14208/17352 (82%)] Loss: -416657.093750\n",
      "Train Epoch: 306 [15575/17352 (90%)] Loss: -369696.812500\n",
      "Train Epoch: 306 [16271/17352 (94%)] Loss: -7210.366699\n",
      "Train Epoch: 306 [17049/17352 (98%)] Loss: -272323.375000\n",
      "    epoch          : 306\n",
      "    loss           : -354349.21522061137\n",
      "    val_loss       : -197672.46225585937\n",
      "Train Epoch: 307 [128/17352 (1%)] Loss: -371514.875000\n",
      "Train Epoch: 307 [1536/17352 (9%)] Loss: -393971.656250\n",
      "Train Epoch: 307 [2944/17352 (17%)] Loss: -408900.250000\n",
      "Train Epoch: 307 [4352/17352 (25%)] Loss: -416698.187500\n",
      "Train Epoch: 307 [5760/17352 (33%)] Loss: -411702.375000\n",
      "Train Epoch: 307 [7168/17352 (41%)] Loss: -408034.625000\n",
      "Train Epoch: 307 [8576/17352 (49%)] Loss: -325979.062500\n",
      "Train Epoch: 307 [9984/17352 (58%)] Loss: -416561.187500\n",
      "Train Epoch: 307 [11392/17352 (66%)] Loss: -399222.000000\n",
      "Train Epoch: 307 [12800/17352 (74%)] Loss: -423251.562500\n",
      "Train Epoch: 307 [14208/17352 (82%)] Loss: -397449.250000\n",
      "Train Epoch: 307 [15451/17352 (89%)] Loss: -15284.131836\n",
      "Train Epoch: 307 [16183/17352 (93%)] Loss: -153508.031250\n",
      "Train Epoch: 307 [17008/17352 (98%)] Loss: -281897.875000\n",
      "    epoch          : 307\n",
      "    loss           : -352655.1012544568\n",
      "    val_loss       : -196722.56735026042\n",
      "Train Epoch: 308 [128/17352 (1%)] Loss: -384633.062500\n",
      "Train Epoch: 308 [1536/17352 (9%)] Loss: -420557.375000\n",
      "Train Epoch: 308 [2944/17352 (17%)] Loss: -384310.312500\n",
      "Train Epoch: 308 [4352/17352 (25%)] Loss: -425073.250000\n",
      "Train Epoch: 308 [5760/17352 (33%)] Loss: -414525.000000\n",
      "Train Epoch: 308 [7168/17352 (41%)] Loss: -381337.250000\n",
      "Train Epoch: 308 [8576/17352 (49%)] Loss: -382848.437500\n",
      "Train Epoch: 308 [9984/17352 (58%)] Loss: -444795.781250\n",
      "Train Epoch: 308 [11392/17352 (66%)] Loss: -367576.000000\n",
      "Train Epoch: 308 [12800/17352 (74%)] Loss: -414107.093750\n",
      "Train Epoch: 308 [14208/17352 (82%)] Loss: -390673.375000\n",
      "Train Epoch: 308 [15450/17352 (89%)] Loss: -45384.320312\n",
      "Train Epoch: 308 [16168/17352 (93%)] Loss: -313664.031250\n",
      "Train Epoch: 308 [17094/17352 (99%)] Loss: -355001.281250\n",
      "    epoch          : 308\n",
      "    loss           : -351417.1130387479\n",
      "    val_loss       : -193171.49995117186\n",
      "Train Epoch: 309 [128/17352 (1%)] Loss: -380686.250000\n",
      "Train Epoch: 309 [1536/17352 (9%)] Loss: -398296.875000\n",
      "Train Epoch: 309 [2944/17352 (17%)] Loss: -429245.468750\n",
      "Train Epoch: 309 [4352/17352 (25%)] Loss: -389253.906250\n",
      "Train Epoch: 309 [5760/17352 (33%)] Loss: -393943.406250\n",
      "Train Epoch: 309 [7168/17352 (41%)] Loss: -388567.875000\n",
      "Train Epoch: 309 [8576/17352 (49%)] Loss: -420841.156250\n",
      "Train Epoch: 309 [9984/17352 (58%)] Loss: -408330.531250\n",
      "Train Epoch: 309 [11392/17352 (66%)] Loss: -382751.281250\n",
      "Train Epoch: 309 [12800/17352 (74%)] Loss: -327813.031250\n",
      "Train Epoch: 309 [14208/17352 (82%)] Loss: -380128.218750\n",
      "Train Epoch: 309 [15563/17352 (90%)] Loss: -301553.750000\n",
      "Train Epoch: 309 [16242/17352 (94%)] Loss: -7542.528809\n",
      "Train Epoch: 309 [17074/17352 (98%)] Loss: -105266.757812\n",
      "    epoch          : 309\n",
      "    loss           : -350735.7678107959\n",
      "    val_loss       : -197844.558984375\n",
      "Train Epoch: 310 [128/17352 (1%)] Loss: -394582.687500\n",
      "Train Epoch: 310 [1536/17352 (9%)] Loss: -400259.000000\n",
      "Train Epoch: 310 [2944/17352 (17%)] Loss: -367076.875000\n",
      "Train Epoch: 310 [4352/17352 (25%)] Loss: -394792.875000\n",
      "Train Epoch: 310 [5760/17352 (33%)] Loss: -440132.937500\n",
      "Train Epoch: 310 [7168/17352 (41%)] Loss: -390867.562500\n",
      "Train Epoch: 310 [8576/17352 (49%)] Loss: -372986.312500\n",
      "Train Epoch: 310 [9984/17352 (58%)] Loss: -412241.031250\n",
      "Train Epoch: 310 [11392/17352 (66%)] Loss: -441537.625000\n",
      "Train Epoch: 310 [12800/17352 (74%)] Loss: -418109.125000\n",
      "Train Epoch: 310 [14208/17352 (82%)] Loss: -349953.687500\n",
      "Train Epoch: 310 [15531/17352 (90%)] Loss: -262402.125000\n",
      "Train Epoch: 310 [16179/17352 (93%)] Loss: -333594.656250\n",
      "Train Epoch: 310 [17007/17352 (98%)] Loss: -226937.500000\n",
      "    epoch          : 310\n",
      "    loss           : -361487.71554995544\n",
      "    val_loss       : -199300.6276529948\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch310.pth ...\n",
      "Train Epoch: 311 [128/17352 (1%)] Loss: -389868.375000\n",
      "Train Epoch: 311 [1536/17352 (9%)] Loss: -325402.687500\n",
      "Train Epoch: 311 [2944/17352 (17%)] Loss: -428305.875000\n",
      "Train Epoch: 311 [4352/17352 (25%)] Loss: -417365.687500\n",
      "Train Epoch: 311 [5760/17352 (33%)] Loss: -415109.937500\n",
      "Train Epoch: 311 [7168/17352 (41%)] Loss: -400512.156250\n",
      "Train Epoch: 311 [8576/17352 (49%)] Loss: -417018.312500\n",
      "Train Epoch: 311 [9984/17352 (58%)] Loss: -425001.312500\n",
      "Train Epoch: 311 [11392/17352 (66%)] Loss: -427086.187500\n",
      "Train Epoch: 311 [12800/17352 (74%)] Loss: -45657.789062\n",
      "Train Epoch: 311 [14208/17352 (82%)] Loss: -415693.875000\n",
      "Train Epoch: 311 [15479/17352 (89%)] Loss: -196810.078125\n",
      "Train Epoch: 311 [16050/17352 (92%)] Loss: -180127.843750\n",
      "Train Epoch: 311 [16885/17352 (97%)] Loss: -278842.125000\n",
      "    epoch          : 311\n",
      "    loss           : -357022.3157770554\n",
      "    val_loss       : -190701.12799479166\n",
      "Train Epoch: 312 [128/17352 (1%)] Loss: -374631.593750\n",
      "Train Epoch: 312 [1536/17352 (9%)] Loss: -407460.406250\n",
      "Train Epoch: 312 [2944/17352 (17%)] Loss: -451893.625000\n",
      "Train Epoch: 312 [4352/17352 (25%)] Loss: -412025.468750\n",
      "Train Epoch: 312 [5760/17352 (33%)] Loss: -389053.500000\n",
      "Train Epoch: 312 [7168/17352 (41%)] Loss: -413145.375000\n",
      "Train Epoch: 312 [8576/17352 (49%)] Loss: -424286.968750\n",
      "Train Epoch: 312 [9984/17352 (58%)] Loss: -354056.718750\n",
      "Train Epoch: 312 [11392/17352 (66%)] Loss: -419744.343750\n",
      "Train Epoch: 312 [12800/17352 (74%)] Loss: -303094.187500\n",
      "Train Epoch: 312 [14208/17352 (82%)] Loss: -398507.156250\n",
      "Train Epoch: 312 [15536/17352 (90%)] Loss: -208448.765625\n",
      "Train Epoch: 312 [16127/17352 (93%)] Loss: -128111.843750\n",
      "Train Epoch: 312 [16909/17352 (97%)] Loss: -304064.062500\n",
      "    epoch          : 312\n",
      "    loss           : -360013.5585871959\n",
      "    val_loss       : -200206.46575520834\n",
      "Train Epoch: 313 [128/17352 (1%)] Loss: -408019.187500\n",
      "Train Epoch: 313 [1536/17352 (9%)] Loss: -370983.125000\n",
      "Train Epoch: 313 [2944/17352 (17%)] Loss: -421775.656250\n",
      "Train Epoch: 313 [4352/17352 (25%)] Loss: -429187.218750\n",
      "Train Epoch: 313 [5760/17352 (33%)] Loss: -432912.500000\n",
      "Train Epoch: 313 [7168/17352 (41%)] Loss: -429006.875000\n",
      "Train Epoch: 313 [8576/17352 (49%)] Loss: -430076.937500\n",
      "Train Epoch: 313 [9984/17352 (58%)] Loss: -358013.812500\n",
      "Train Epoch: 313 [11392/17352 (66%)] Loss: -310835.187500\n",
      "Train Epoch: 313 [12800/17352 (74%)] Loss: -467801.312500\n",
      "Train Epoch: 313 [14208/17352 (82%)] Loss: -340644.406250\n",
      "Train Epoch: 313 [15527/17352 (89%)] Loss: -254624.562500\n",
      "Train Epoch: 313 [16373/17352 (94%)] Loss: -273087.875000\n",
      "Train Epoch: 313 [17032/17352 (98%)] Loss: -163035.250000\n",
      "    epoch          : 313\n",
      "    loss           : -365606.23264471476\n",
      "    val_loss       : -203725.8600748698\n",
      "Train Epoch: 314 [128/17352 (1%)] Loss: -420778.187500\n",
      "Train Epoch: 314 [1536/17352 (9%)] Loss: -417357.500000\n",
      "Train Epoch: 314 [2944/17352 (17%)] Loss: -395220.656250\n",
      "Train Epoch: 314 [4352/17352 (25%)] Loss: -438066.375000\n",
      "Train Epoch: 314 [5760/17352 (33%)] Loss: -462547.437500\n",
      "Train Epoch: 314 [7168/17352 (41%)] Loss: -401462.062500\n",
      "Train Epoch: 314 [8576/17352 (49%)] Loss: -442108.156250\n",
      "Train Epoch: 314 [9984/17352 (58%)] Loss: -400311.343750\n",
      "Train Epoch: 314 [11392/17352 (66%)] Loss: -424173.250000\n",
      "Train Epoch: 314 [12800/17352 (74%)] Loss: -376961.593750\n",
      "Train Epoch: 314 [14208/17352 (82%)] Loss: -433516.437500\n",
      "Train Epoch: 314 [15468/17352 (89%)] Loss: -174658.593750\n",
      "Train Epoch: 314 [16218/17352 (93%)] Loss: -117913.492188\n",
      "Train Epoch: 314 [17002/17352 (98%)] Loss: -287403.875000\n",
      "    epoch          : 314\n",
      "    loss           : -367985.6515638108\n",
      "    val_loss       : -204354.28359375\n",
      "Train Epoch: 315 [128/17352 (1%)] Loss: -389125.000000\n",
      "Train Epoch: 315 [1536/17352 (9%)] Loss: -336587.562500\n",
      "Train Epoch: 315 [2944/17352 (17%)] Loss: -371049.031250\n",
      "Train Epoch: 315 [4352/17352 (25%)] Loss: -448139.187500\n",
      "Train Epoch: 315 [5760/17352 (33%)] Loss: -417266.156250\n",
      "Train Epoch: 315 [7168/17352 (41%)] Loss: -435165.437500\n",
      "Train Epoch: 315 [8576/17352 (49%)] Loss: -431418.812500\n",
      "Train Epoch: 315 [9984/17352 (58%)] Loss: -452331.562500\n",
      "Train Epoch: 315 [11392/17352 (66%)] Loss: -426091.062500\n",
      "Train Epoch: 315 [12800/17352 (74%)] Loss: -416291.937500\n",
      "Train Epoch: 315 [14208/17352 (82%)] Loss: -356262.406250\n",
      "Train Epoch: 315 [15479/17352 (89%)] Loss: -106425.968750\n",
      "Train Epoch: 315 [16280/17352 (94%)] Loss: -244525.890625\n",
      "Train Epoch: 315 [17097/17352 (99%)] Loss: -337713.937500\n",
      "    epoch          : 315\n",
      "    loss           : -369965.0762243079\n",
      "    val_loss       : -203526.3723795573\n",
      "Train Epoch: 316 [128/17352 (1%)] Loss: -426517.531250\n",
      "Train Epoch: 316 [1536/17352 (9%)] Loss: -403373.375000\n",
      "Train Epoch: 316 [2944/17352 (17%)] Loss: -435656.937500\n",
      "Train Epoch: 316 [4352/17352 (25%)] Loss: -414409.187500\n",
      "Train Epoch: 316 [5760/17352 (33%)] Loss: -301052.843750\n",
      "Train Epoch: 316 [7168/17352 (41%)] Loss: -387283.937500\n",
      "Train Epoch: 316 [8576/17352 (49%)] Loss: -431352.625000\n",
      "Train Epoch: 316 [9984/17352 (58%)] Loss: -391787.937500\n",
      "Train Epoch: 316 [11392/17352 (66%)] Loss: -410741.406250\n",
      "Train Epoch: 316 [12800/17352 (74%)] Loss: -390054.812500\n",
      "Train Epoch: 316 [14208/17352 (82%)] Loss: -436383.000000\n",
      "Train Epoch: 316 [15463/17352 (89%)] Loss: -304911.656250\n",
      "Train Epoch: 316 [16284/17352 (94%)] Loss: -13185.125000\n",
      "Train Epoch: 316 [17099/17352 (99%)] Loss: -184042.687500\n",
      "    epoch          : 316\n",
      "    loss           : -373021.7635407928\n",
      "    val_loss       : -213128.13076171876\n",
      "Train Epoch: 317 [128/17352 (1%)] Loss: -428159.281250\n",
      "Train Epoch: 317 [1536/17352 (9%)] Loss: -385759.750000\n",
      "Train Epoch: 317 [2944/17352 (17%)] Loss: -438476.875000\n",
      "Train Epoch: 317 [4352/17352 (25%)] Loss: -435655.375000\n",
      "Train Epoch: 317 [5760/17352 (33%)] Loss: -434203.000000\n",
      "Train Epoch: 317 [7168/17352 (41%)] Loss: -451843.843750\n",
      "Train Epoch: 317 [8576/17352 (49%)] Loss: -448364.937500\n",
      "Train Epoch: 317 [9984/17352 (58%)] Loss: -450631.531250\n",
      "Train Epoch: 317 [11392/17352 (66%)] Loss: -438996.093750\n",
      "Train Epoch: 317 [12800/17352 (74%)] Loss: -467537.562500\n",
      "Train Epoch: 317 [14208/17352 (82%)] Loss: -423173.156250\n",
      "Train Epoch: 317 [15534/17352 (90%)] Loss: -192433.656250\n",
      "Train Epoch: 317 [16253/17352 (94%)] Loss: -129752.843750\n",
      "Train Epoch: 317 [16990/17352 (98%)] Loss: -279790.750000\n",
      "    epoch          : 317\n",
      "    loss           : -378227.5030607697\n",
      "    val_loss       : -205378.15128580728\n",
      "Train Epoch: 318 [128/17352 (1%)] Loss: -428698.125000\n",
      "Train Epoch: 318 [1536/17352 (9%)] Loss: -372034.500000\n",
      "Train Epoch: 318 [2944/17352 (17%)] Loss: -469471.375000\n",
      "Train Epoch: 318 [4352/17352 (25%)] Loss: -437282.531250\n",
      "Train Epoch: 318 [5760/17352 (33%)] Loss: -446750.062500\n",
      "Train Epoch: 318 [7168/17352 (41%)] Loss: -461168.437500\n",
      "Train Epoch: 318 [8576/17352 (49%)] Loss: -448510.375000\n",
      "Train Epoch: 318 [9984/17352 (58%)] Loss: -437098.750000\n",
      "Train Epoch: 318 [11392/17352 (66%)] Loss: -453561.937500\n",
      "Train Epoch: 318 [12800/17352 (74%)] Loss: -459140.812500\n",
      "Train Epoch: 318 [14208/17352 (82%)] Loss: -355594.937500\n",
      "Train Epoch: 318 [15435/17352 (89%)] Loss: -112986.781250\n",
      "Train Epoch: 318 [16174/17352 (93%)] Loss: -9310.009766\n",
      "Train Epoch: 318 [16936/17352 (98%)] Loss: -338824.312500\n",
      "    epoch          : 318\n",
      "    loss           : -378605.2487153943\n",
      "    val_loss       : -206356.58575846354\n",
      "Train Epoch: 319 [128/17352 (1%)] Loss: -358388.250000\n",
      "Train Epoch: 319 [1536/17352 (9%)] Loss: -456193.781250\n",
      "Train Epoch: 319 [2944/17352 (17%)] Loss: -302824.625000\n",
      "Train Epoch: 319 [4352/17352 (25%)] Loss: -428823.281250\n",
      "Train Epoch: 319 [5760/17352 (33%)] Loss: -370889.562500\n",
      "Train Epoch: 319 [7168/17352 (41%)] Loss: -401729.562500\n",
      "Train Epoch: 319 [8576/17352 (49%)] Loss: -386128.562500\n",
      "Train Epoch: 319 [9984/17352 (58%)] Loss: -345119.375000\n",
      "Train Epoch: 319 [11392/17352 (66%)] Loss: -454096.250000\n",
      "Train Epoch: 319 [12800/17352 (74%)] Loss: -417403.781250\n",
      "Train Epoch: 319 [14208/17352 (82%)] Loss: -431754.187500\n",
      "Train Epoch: 319 [15541/17352 (90%)] Loss: -335034.062500\n",
      "Train Epoch: 319 [16466/17352 (95%)] Loss: -417464.062500\n",
      "Train Epoch: 319 [16984/17352 (98%)] Loss: -211923.281250\n",
      "    epoch          : 319\n",
      "    loss           : -372422.04325057677\n",
      "    val_loss       : -198284.40227864584\n",
      "Train Epoch: 320 [128/17352 (1%)] Loss: -460590.062500\n",
      "Train Epoch: 320 [1536/17352 (9%)] Loss: -364885.656250\n",
      "Train Epoch: 320 [2944/17352 (17%)] Loss: -406791.500000\n",
      "Train Epoch: 320 [4352/17352 (25%)] Loss: -414330.875000\n",
      "Train Epoch: 320 [5760/17352 (33%)] Loss: -399359.562500\n",
      "Train Epoch: 320 [7168/17352 (41%)] Loss: -435952.250000\n",
      "Train Epoch: 320 [8576/17352 (49%)] Loss: -443531.062500\n",
      "Train Epoch: 320 [9984/17352 (58%)] Loss: -422675.250000\n",
      "Train Epoch: 320 [11392/17352 (66%)] Loss: -426822.875000\n",
      "Train Epoch: 320 [12800/17352 (74%)] Loss: -429912.718750\n",
      "Train Epoch: 320 [14208/17352 (82%)] Loss: -395016.343750\n",
      "Train Epoch: 320 [15455/17352 (89%)] Loss: -143669.250000\n",
      "Train Epoch: 320 [16092/17352 (93%)] Loss: -298798.125000\n",
      "Train Epoch: 320 [17025/17352 (98%)] Loss: -15613.676758\n",
      "    epoch          : 320\n",
      "    loss           : -369902.0214450503\n",
      "    val_loss       : -210058.36878255208\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch320.pth ...\n",
      "Train Epoch: 321 [128/17352 (1%)] Loss: -355639.062500\n",
      "Train Epoch: 321 [1536/17352 (9%)] Loss: -440287.125000\n",
      "Train Epoch: 321 [2944/17352 (17%)] Loss: -438616.062500\n",
      "Train Epoch: 321 [4352/17352 (25%)] Loss: -441360.437500\n",
      "Train Epoch: 321 [5760/17352 (33%)] Loss: -380213.406250\n",
      "Train Epoch: 321 [7168/17352 (41%)] Loss: -406600.687500\n",
      "Train Epoch: 321 [8576/17352 (49%)] Loss: -436548.625000\n",
      "Train Epoch: 321 [9984/17352 (58%)] Loss: -363593.437500\n",
      "Train Epoch: 321 [11392/17352 (66%)] Loss: -440863.687500\n",
      "Train Epoch: 321 [12800/17352 (74%)] Loss: -448079.125000\n",
      "Train Epoch: 321 [14208/17352 (82%)] Loss: -435960.031250\n",
      "Train Epoch: 321 [15514/17352 (89%)] Loss: -244080.750000\n",
      "Train Epoch: 321 [16291/17352 (94%)] Loss: -310963.500000\n",
      "Train Epoch: 321 [16903/17352 (97%)] Loss: -38865.468750\n",
      "    epoch          : 321\n",
      "    loss           : -368329.34627333266\n",
      "    val_loss       : -204162.1297200521\n",
      "Train Epoch: 322 [128/17352 (1%)] Loss: -383038.875000\n",
      "Train Epoch: 322 [1536/17352 (9%)] Loss: -427647.375000\n",
      "Train Epoch: 322 [2944/17352 (17%)] Loss: -434395.625000\n",
      "Train Epoch: 322 [4352/17352 (25%)] Loss: -427625.687500\n",
      "Train Epoch: 322 [5760/17352 (33%)] Loss: -370292.500000\n",
      "Train Epoch: 322 [7168/17352 (41%)] Loss: -392651.000000\n",
      "Train Epoch: 322 [8576/17352 (49%)] Loss: -432264.000000\n",
      "Train Epoch: 322 [9984/17352 (58%)] Loss: -445030.531250\n",
      "Train Epoch: 322 [11392/17352 (66%)] Loss: -376604.125000\n",
      "Train Epoch: 322 [12800/17352 (74%)] Loss: -449600.593750\n",
      "Train Epoch: 322 [14208/17352 (82%)] Loss: -439371.750000\n",
      "Train Epoch: 322 [15522/17352 (89%)] Loss: -274596.281250\n",
      "Train Epoch: 322 [16442/17352 (95%)] Loss: -401834.000000\n",
      "Train Epoch: 322 [17117/17352 (99%)] Loss: -333163.625000\n",
      "    epoch          : 322\n",
      "    loss           : -376426.9514930264\n",
      "    val_loss       : -208145.58142903645\n",
      "Train Epoch: 323 [128/17352 (1%)] Loss: -440560.687500\n",
      "Train Epoch: 323 [1536/17352 (9%)] Loss: -413542.093750\n",
      "Train Epoch: 323 [2944/17352 (17%)] Loss: -263748.281250\n",
      "Train Epoch: 323 [4352/17352 (25%)] Loss: -410889.750000\n",
      "Train Epoch: 323 [5760/17352 (33%)] Loss: -428068.625000\n",
      "Train Epoch: 323 [7168/17352 (41%)] Loss: -433054.343750\n",
      "Train Epoch: 323 [8576/17352 (49%)] Loss: -433707.593750\n",
      "Train Epoch: 323 [9984/17352 (58%)] Loss: -459673.375000\n",
      "Train Epoch: 323 [11392/17352 (66%)] Loss: -412898.375000\n",
      "Train Epoch: 323 [12800/17352 (74%)] Loss: -468526.843750\n",
      "Train Epoch: 323 [14208/17352 (82%)] Loss: -461237.781250\n",
      "Train Epoch: 323 [15486/17352 (89%)] Loss: -293718.375000\n",
      "Train Epoch: 323 [16148/17352 (93%)] Loss: -132413.937500\n",
      "Train Epoch: 323 [16925/17352 (98%)] Loss: -158201.062500\n",
      "    epoch          : 323\n",
      "    loss           : -377275.18881737627\n",
      "    val_loss       : -204760.53696289062\n",
      "Train Epoch: 324 [128/17352 (1%)] Loss: -432598.812500\n",
      "Train Epoch: 324 [1536/17352 (9%)] Loss: -372026.687500\n",
      "Train Epoch: 324 [2944/17352 (17%)] Loss: -391473.187500\n",
      "Train Epoch: 324 [4352/17352 (25%)] Loss: -390135.062500\n",
      "Train Epoch: 324 [5760/17352 (33%)] Loss: -422729.625000\n",
      "Train Epoch: 324 [7168/17352 (41%)] Loss: -407601.562500\n",
      "Train Epoch: 324 [8576/17352 (49%)] Loss: -332758.593750\n",
      "Train Epoch: 324 [9984/17352 (58%)] Loss: -376739.406250\n",
      "Train Epoch: 324 [11392/17352 (66%)] Loss: -411896.062500\n",
      "Train Epoch: 324 [12800/17352 (74%)] Loss: -433017.000000\n",
      "Train Epoch: 324 [14208/17352 (82%)] Loss: -445493.531250\n",
      "Train Epoch: 324 [15467/17352 (89%)] Loss: -234764.234375\n",
      "Train Epoch: 324 [16369/17352 (94%)] Loss: -235743.078125\n",
      "Train Epoch: 324 [16913/17352 (97%)] Loss: -312830.843750\n",
      "    epoch          : 324\n",
      "    loss           : -380538.9425007865\n",
      "    val_loss       : -205148.8891113281\n",
      "Train Epoch: 325 [128/17352 (1%)] Loss: -410319.656250\n",
      "Train Epoch: 325 [1536/17352 (9%)] Loss: -390780.406250\n",
      "Train Epoch: 325 [2944/17352 (17%)] Loss: -450383.687500\n",
      "Train Epoch: 325 [4352/17352 (25%)] Loss: -433722.437500\n",
      "Train Epoch: 325 [5760/17352 (33%)] Loss: -227184.031250\n",
      "Train Epoch: 325 [7168/17352 (41%)] Loss: -427691.000000\n",
      "Train Epoch: 325 [8576/17352 (49%)] Loss: -438782.656250\n",
      "Train Epoch: 325 [9984/17352 (58%)] Loss: -367685.468750\n",
      "Train Epoch: 325 [11392/17352 (66%)] Loss: -407135.125000\n",
      "Train Epoch: 325 [12800/17352 (74%)] Loss: -402165.625000\n",
      "Train Epoch: 325 [14208/17352 (82%)] Loss: -442657.812500\n",
      "Train Epoch: 325 [15541/17352 (90%)] Loss: -201995.750000\n",
      "Train Epoch: 325 [16219/17352 (93%)] Loss: -149413.359375\n",
      "Train Epoch: 325 [17047/17352 (98%)] Loss: -337274.625000\n",
      "    epoch          : 325\n",
      "    loss           : -371867.8714607802\n",
      "    val_loss       : -210184.56814778646\n",
      "Train Epoch: 326 [128/17352 (1%)] Loss: -455149.375000\n",
      "Train Epoch: 326 [1536/17352 (9%)] Loss: -383790.937500\n",
      "Train Epoch: 326 [2944/17352 (17%)] Loss: -421495.750000\n",
      "Train Epoch: 326 [4352/17352 (25%)] Loss: -419796.218750\n",
      "Train Epoch: 326 [5760/17352 (33%)] Loss: -309540.468750\n",
      "Train Epoch: 326 [7168/17352 (41%)] Loss: -427384.031250\n",
      "Train Epoch: 326 [8576/17352 (49%)] Loss: -440335.062500\n",
      "Train Epoch: 326 [9984/17352 (58%)] Loss: -443763.000000\n",
      "Train Epoch: 326 [11392/17352 (66%)] Loss: -364527.531250\n",
      "Train Epoch: 326 [12800/17352 (74%)] Loss: -399125.593750\n",
      "Train Epoch: 326 [14208/17352 (82%)] Loss: -423861.531250\n",
      "Train Epoch: 326 [15470/17352 (89%)] Loss: -51135.941406\n",
      "Train Epoch: 326 [16143/17352 (93%)] Loss: -129043.203125\n",
      "Train Epoch: 326 [17075/17352 (98%)] Loss: -223417.656250\n",
      "    epoch          : 326\n",
      "    loss           : -369774.62863097736\n",
      "    val_loss       : -195910.4301269531\n",
      "Train Epoch: 327 [128/17352 (1%)] Loss: -444746.437500\n",
      "Train Epoch: 327 [1536/17352 (9%)] Loss: -357343.343750\n",
      "Train Epoch: 327 [2944/17352 (17%)] Loss: -383039.000000\n",
      "Train Epoch: 327 [4352/17352 (25%)] Loss: -423207.468750\n",
      "Train Epoch: 327 [5760/17352 (33%)] Loss: -422689.031250\n",
      "Train Epoch: 327 [7168/17352 (41%)] Loss: -445347.468750\n",
      "Train Epoch: 327 [8576/17352 (49%)] Loss: -442535.187500\n",
      "Train Epoch: 327 [9984/17352 (58%)] Loss: -419338.093750\n",
      "Train Epoch: 327 [11392/17352 (66%)] Loss: -461978.187500\n",
      "Train Epoch: 327 [12800/17352 (74%)] Loss: -429118.875000\n",
      "Train Epoch: 327 [14208/17352 (82%)] Loss: -413800.437500\n",
      "Train Epoch: 327 [15491/17352 (89%)] Loss: -143824.140625\n",
      "Train Epoch: 327 [16159/17352 (93%)] Loss: -304767.750000\n",
      "Train Epoch: 327 [17072/17352 (98%)] Loss: -266754.062500\n",
      "    epoch          : 327\n",
      "    loss           : -371444.4825529572\n",
      "    val_loss       : -213477.91150716145\n",
      "Train Epoch: 328 [128/17352 (1%)] Loss: -446772.312500\n",
      "Train Epoch: 328 [1536/17352 (9%)] Loss: -427130.187500\n",
      "Train Epoch: 328 [2944/17352 (17%)] Loss: -439154.687500\n",
      "Train Epoch: 328 [4352/17352 (25%)] Loss: -344602.062500\n",
      "Train Epoch: 328 [5760/17352 (33%)] Loss: -354059.468750\n",
      "Train Epoch: 328 [7168/17352 (41%)] Loss: -431872.343750\n",
      "Train Epoch: 328 [8576/17352 (49%)] Loss: -413257.812500\n",
      "Train Epoch: 328 [9984/17352 (58%)] Loss: -424673.562500\n",
      "Train Epoch: 328 [11392/17352 (66%)] Loss: -435342.437500\n",
      "Train Epoch: 328 [12800/17352 (74%)] Loss: -321597.125000\n",
      "Train Epoch: 328 [14208/17352 (82%)] Loss: -425962.125000\n",
      "Train Epoch: 328 [15476/17352 (89%)] Loss: -228065.359375\n",
      "Train Epoch: 328 [16266/17352 (94%)] Loss: -17094.953125\n",
      "Train Epoch: 328 [16912/17352 (97%)] Loss: -313206.812500\n",
      "    epoch          : 328\n",
      "    loss           : -374941.54598364094\n",
      "    val_loss       : -206406.1342610677\n",
      "Train Epoch: 329 [128/17352 (1%)] Loss: -413438.812500\n",
      "Train Epoch: 329 [1536/17352 (9%)] Loss: -390608.937500\n",
      "Train Epoch: 329 [2944/17352 (17%)] Loss: -310526.625000\n",
      "Train Epoch: 329 [4352/17352 (25%)] Loss: -377712.500000\n",
      "Train Epoch: 329 [5760/17352 (33%)] Loss: -351376.375000\n",
      "Train Epoch: 329 [7168/17352 (41%)] Loss: -421764.000000\n",
      "Train Epoch: 329 [8576/17352 (49%)] Loss: -372825.812500\n",
      "Train Epoch: 329 [9984/17352 (58%)] Loss: -433807.812500\n",
      "Train Epoch: 329 [11392/17352 (66%)] Loss: -432711.562500\n",
      "Train Epoch: 329 [12800/17352 (74%)] Loss: -439458.156250\n",
      "Train Epoch: 329 [14208/17352 (82%)] Loss: -411454.250000\n",
      "Train Epoch: 329 [15442/17352 (89%)] Loss: -16535.113281\n",
      "Train Epoch: 329 [16108/17352 (93%)] Loss: -312168.343750\n",
      "Train Epoch: 329 [16902/17352 (97%)] Loss: -263714.187500\n",
      "    epoch          : 329\n",
      "    loss           : -368632.71690174076\n",
      "    val_loss       : -205575.64236653646\n",
      "Train Epoch: 330 [128/17352 (1%)] Loss: -435202.593750\n",
      "Train Epoch: 330 [1536/17352 (9%)] Loss: -367327.593750\n",
      "Train Epoch: 330 [2944/17352 (17%)] Loss: -420290.562500\n",
      "Train Epoch: 330 [4352/17352 (25%)] Loss: -413903.375000\n",
      "Train Epoch: 330 [5760/17352 (33%)] Loss: -452943.156250\n",
      "Train Epoch: 330 [7168/17352 (41%)] Loss: -404628.625000\n",
      "Train Epoch: 330 [8576/17352 (49%)] Loss: -449700.343750\n",
      "Train Epoch: 330 [9984/17352 (58%)] Loss: -445204.625000\n",
      "Train Epoch: 330 [11392/17352 (66%)] Loss: -411308.718750\n",
      "Train Epoch: 330 [12800/17352 (74%)] Loss: -384899.281250\n",
      "Train Epoch: 330 [14208/17352 (82%)] Loss: -450268.468750\n",
      "Train Epoch: 330 [15399/17352 (89%)] Loss: -13341.285156\n",
      "Train Epoch: 330 [16023/17352 (92%)] Loss: -133273.171875\n",
      "Train Epoch: 330 [16979/17352 (98%)] Loss: -264640.843750\n",
      "    epoch          : 330\n",
      "    loss           : -376785.41817192745\n",
      "    val_loss       : -206964.39392903645\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch330.pth ...\n",
      "Train Epoch: 331 [128/17352 (1%)] Loss: -420742.062500\n",
      "Train Epoch: 331 [1536/17352 (9%)] Loss: -418750.218750\n",
      "Train Epoch: 331 [2944/17352 (17%)] Loss: -441152.125000\n",
      "Train Epoch: 331 [4352/17352 (25%)] Loss: -447010.375000\n",
      "Train Epoch: 331 [5760/17352 (33%)] Loss: -340038.093750\n",
      "Train Epoch: 331 [7168/17352 (41%)] Loss: -417215.250000\n",
      "Train Epoch: 331 [8576/17352 (49%)] Loss: -440653.750000\n",
      "Train Epoch: 331 [9984/17352 (58%)] Loss: -266106.218750\n",
      "Train Epoch: 331 [11392/17352 (66%)] Loss: -375221.875000\n",
      "Train Epoch: 331 [12800/17352 (74%)] Loss: -410854.937500\n",
      "Train Epoch: 331 [14208/17352 (82%)] Loss: -371584.000000\n",
      "Train Epoch: 331 [15553/17352 (90%)] Loss: -391777.625000\n",
      "Train Epoch: 331 [16142/17352 (93%)] Loss: -16782.539062\n",
      "Train Epoch: 331 [16946/17352 (98%)] Loss: -126761.976562\n",
      "    epoch          : 331\n",
      "    loss           : -374977.28336697776\n",
      "    val_loss       : -204600.72828776043\n",
      "Train Epoch: 332 [128/17352 (1%)] Loss: -339440.750000\n",
      "Train Epoch: 332 [1536/17352 (9%)] Loss: -424916.218750\n",
      "Train Epoch: 332 [2944/17352 (17%)] Loss: -415277.687500\n",
      "Train Epoch: 332 [4352/17352 (25%)] Loss: -387800.093750\n",
      "Train Epoch: 332 [5760/17352 (33%)] Loss: -451939.218750\n",
      "Train Epoch: 332 [7168/17352 (41%)] Loss: -432452.031250\n",
      "Train Epoch: 332 [8576/17352 (49%)] Loss: -454742.375000\n",
      "Train Epoch: 332 [9984/17352 (58%)] Loss: -442575.531250\n",
      "Train Epoch: 332 [11392/17352 (66%)] Loss: -388831.968750\n",
      "Train Epoch: 332 [12800/17352 (74%)] Loss: -443231.468750\n",
      "Train Epoch: 332 [14208/17352 (82%)] Loss: -437046.937500\n",
      "Train Epoch: 332 [15512/17352 (89%)] Loss: -253948.171875\n",
      "Train Epoch: 332 [16294/17352 (94%)] Loss: -257694.500000\n",
      "Train Epoch: 332 [17023/17352 (98%)] Loss: -327694.968750\n",
      "    epoch          : 332\n",
      "    loss           : -375933.2081388947\n",
      "    val_loss       : -213720.04733072917\n",
      "Train Epoch: 333 [128/17352 (1%)] Loss: -425214.906250\n",
      "Train Epoch: 333 [1536/17352 (9%)] Loss: -425805.250000\n",
      "Train Epoch: 333 [2944/17352 (17%)] Loss: -450343.906250\n",
      "Train Epoch: 333 [4352/17352 (25%)] Loss: -406926.437500\n",
      "Train Epoch: 333 [5760/17352 (33%)] Loss: -385533.750000\n",
      "Train Epoch: 333 [7168/17352 (41%)] Loss: -433321.812500\n",
      "Train Epoch: 333 [8576/17352 (49%)] Loss: -456341.750000\n",
      "Train Epoch: 333 [9984/17352 (58%)] Loss: -376070.125000\n",
      "Train Epoch: 333 [11392/17352 (66%)] Loss: -315875.343750\n",
      "Train Epoch: 333 [12800/17352 (74%)] Loss: -411994.750000\n",
      "Train Epoch: 333 [14208/17352 (82%)] Loss: -480338.250000\n",
      "Train Epoch: 333 [15551/17352 (90%)] Loss: -342100.312500\n",
      "Train Epoch: 333 [16219/17352 (93%)] Loss: -114115.921875\n",
      "Train Epoch: 333 [16948/17352 (98%)] Loss: -264568.406250\n",
      "    epoch          : 333\n",
      "    loss           : -382084.70608745806\n",
      "    val_loss       : -206765.0131998698\n",
      "Train Epoch: 334 [128/17352 (1%)] Loss: -433033.562500\n",
      "Train Epoch: 334 [1536/17352 (9%)] Loss: -420287.781250\n",
      "Train Epoch: 334 [2944/17352 (17%)] Loss: -396072.781250\n",
      "Train Epoch: 334 [4352/17352 (25%)] Loss: -361293.062500\n",
      "Train Epoch: 334 [5760/17352 (33%)] Loss: -436784.093750\n",
      "Train Epoch: 334 [7168/17352 (41%)] Loss: -433672.625000\n",
      "Train Epoch: 334 [8576/17352 (49%)] Loss: -395223.781250\n",
      "Train Epoch: 334 [9984/17352 (58%)] Loss: -460122.531250\n",
      "Train Epoch: 334 [11392/17352 (66%)] Loss: -345297.781250\n",
      "Train Epoch: 334 [12800/17352 (74%)] Loss: -455700.437500\n",
      "Train Epoch: 334 [14208/17352 (82%)] Loss: -427213.687500\n",
      "Train Epoch: 334 [15518/17352 (89%)] Loss: -138951.687500\n",
      "Train Epoch: 334 [16223/17352 (93%)] Loss: -3753.786621\n",
      "Train Epoch: 334 [17049/17352 (98%)] Loss: -311461.375000\n",
      "    epoch          : 334\n",
      "    loss           : -368740.08813640417\n",
      "    val_loss       : -206681.52509765624\n",
      "Train Epoch: 335 [128/17352 (1%)] Loss: -390892.375000\n",
      "Train Epoch: 335 [1536/17352 (9%)] Loss: -411645.875000\n",
      "Train Epoch: 335 [2944/17352 (17%)] Loss: -430302.937500\n",
      "Train Epoch: 335 [4352/17352 (25%)] Loss: -356212.843750\n",
      "Train Epoch: 335 [5760/17352 (33%)] Loss: -356908.531250\n",
      "Train Epoch: 335 [7168/17352 (41%)] Loss: -392494.125000\n",
      "Train Epoch: 335 [8576/17352 (49%)] Loss: -419362.468750\n",
      "Train Epoch: 335 [9984/17352 (58%)] Loss: -433243.156250\n",
      "Train Epoch: 335 [11392/17352 (66%)] Loss: -419901.562500\n",
      "Train Epoch: 335 [12800/17352 (74%)] Loss: -427825.843750\n",
      "Train Epoch: 335 [14208/17352 (82%)] Loss: -432840.062500\n",
      "Train Epoch: 335 [15525/17352 (89%)] Loss: -292770.562500\n",
      "Train Epoch: 335 [16300/17352 (94%)] Loss: -260898.828125\n",
      "Train Epoch: 335 [17016/17352 (98%)] Loss: -163433.296875\n",
      "    epoch          : 335\n",
      "    loss           : -371342.24136823614\n",
      "    val_loss       : -200824.3723795573\n",
      "Train Epoch: 336 [128/17352 (1%)] Loss: -411606.812500\n",
      "Train Epoch: 336 [1536/17352 (9%)] Loss: -396486.437500\n",
      "Train Epoch: 336 [2944/17352 (17%)] Loss: -384574.406250\n",
      "Train Epoch: 336 [4352/17352 (25%)] Loss: -414978.500000\n",
      "Train Epoch: 336 [5760/17352 (33%)] Loss: -433540.250000\n",
      "Train Epoch: 336 [7168/17352 (41%)] Loss: -299492.093750\n",
      "Train Epoch: 336 [8576/17352 (49%)] Loss: -405310.093750\n",
      "Train Epoch: 336 [9984/17352 (58%)] Loss: -461065.312500\n",
      "Train Epoch: 336 [11392/17352 (66%)] Loss: -420448.656250\n",
      "Train Epoch: 336 [12800/17352 (74%)] Loss: -442005.750000\n",
      "Train Epoch: 336 [14208/17352 (82%)] Loss: -319755.625000\n",
      "Train Epoch: 336 [15449/17352 (89%)] Loss: -288509.437500\n",
      "Train Epoch: 336 [16280/17352 (94%)] Loss: -301198.687500\n",
      "Train Epoch: 336 [17098/17352 (99%)] Loss: -39341.917969\n",
      "    epoch          : 336\n",
      "    loss           : -372994.4552092072\n",
      "    val_loss       : -212341.40730794272\n",
      "Train Epoch: 337 [128/17352 (1%)] Loss: -430783.562500\n",
      "Train Epoch: 337 [1536/17352 (9%)] Loss: -442264.843750\n",
      "Train Epoch: 337 [2944/17352 (17%)] Loss: -389702.187500\n",
      "Train Epoch: 337 [4352/17352 (25%)] Loss: -409650.656250\n",
      "Train Epoch: 337 [5760/17352 (33%)] Loss: -386203.875000\n",
      "Train Epoch: 337 [7168/17352 (41%)] Loss: -463334.593750\n",
      "Train Epoch: 337 [8576/17352 (49%)] Loss: -359923.468750\n",
      "Train Epoch: 337 [9984/17352 (58%)] Loss: -461602.062500\n",
      "Train Epoch: 337 [11392/17352 (66%)] Loss: -338926.031250\n",
      "Train Epoch: 337 [12800/17352 (74%)] Loss: -452040.656250\n",
      "Train Epoch: 337 [14208/17352 (82%)] Loss: -326198.156250\n",
      "Train Epoch: 337 [15523/17352 (89%)] Loss: -252125.578125\n",
      "Train Epoch: 337 [16278/17352 (94%)] Loss: -254094.515625\n",
      "Train Epoch: 337 [17139/17352 (99%)] Loss: -9063.967773\n",
      "    epoch          : 337\n",
      "    loss           : -379374.3080956376\n",
      "    val_loss       : -212878.16969401043\n",
      "Train Epoch: 338 [128/17352 (1%)] Loss: -420204.125000\n",
      "Train Epoch: 338 [1536/17352 (9%)] Loss: -458501.468750\n",
      "Train Epoch: 338 [2944/17352 (17%)] Loss: -456766.312500\n",
      "Train Epoch: 338 [4352/17352 (25%)] Loss: -426950.312500\n",
      "Train Epoch: 338 [5760/17352 (33%)] Loss: -393507.000000\n",
      "Train Epoch: 338 [7168/17352 (41%)] Loss: -459244.687500\n",
      "Train Epoch: 338 [8576/17352 (49%)] Loss: -387735.625000\n",
      "Train Epoch: 338 [9984/17352 (58%)] Loss: -442395.281250\n",
      "Train Epoch: 338 [11392/17352 (66%)] Loss: -441313.687500\n",
      "Train Epoch: 338 [12800/17352 (74%)] Loss: -390667.687500\n",
      "Train Epoch: 338 [14208/17352 (82%)] Loss: -415771.031250\n",
      "Train Epoch: 338 [15448/17352 (89%)] Loss: -304513.031250\n",
      "Train Epoch: 338 [16255/17352 (94%)] Loss: -340217.468750\n",
      "Train Epoch: 338 [17000/17352 (98%)] Loss: -149775.421875\n",
      "    epoch          : 338\n",
      "    loss           : -381378.4623466338\n",
      "    val_loss       : -213359.04895833333\n",
      "Train Epoch: 339 [128/17352 (1%)] Loss: -467990.750000\n",
      "Train Epoch: 339 [1536/17352 (9%)] Loss: -456061.562500\n",
      "Train Epoch: 339 [2944/17352 (17%)] Loss: -404180.500000\n",
      "Train Epoch: 339 [4352/17352 (25%)] Loss: -423279.500000\n",
      "Train Epoch: 339 [5760/17352 (33%)] Loss: -432890.312500\n",
      "Train Epoch: 339 [7168/17352 (41%)] Loss: -466958.218750\n",
      "Train Epoch: 339 [8576/17352 (49%)] Loss: -456159.718750\n",
      "Train Epoch: 339 [9984/17352 (58%)] Loss: -433575.312500\n",
      "Train Epoch: 339 [11392/17352 (66%)] Loss: -396763.593750\n",
      "Train Epoch: 339 [12800/17352 (74%)] Loss: -319674.500000\n",
      "Train Epoch: 339 [14208/17352 (82%)] Loss: -392631.375000\n",
      "Train Epoch: 339 [15469/17352 (89%)] Loss: -111039.773438\n",
      "Train Epoch: 339 [16205/17352 (93%)] Loss: -150863.203125\n",
      "Train Epoch: 339 [17036/17352 (98%)] Loss: -308698.687500\n",
      "    epoch          : 339\n",
      "    loss           : -386276.6209102349\n",
      "    val_loss       : -207057.07612304686\n",
      "Train Epoch: 340 [128/17352 (1%)] Loss: -337190.562500\n",
      "Train Epoch: 340 [1536/17352 (9%)] Loss: -397261.437500\n",
      "Train Epoch: 340 [2944/17352 (17%)] Loss: -461316.218750\n",
      "Train Epoch: 340 [4352/17352 (25%)] Loss: -445097.937500\n",
      "Train Epoch: 340 [5760/17352 (33%)] Loss: -439151.187500\n",
      "Train Epoch: 340 [7168/17352 (41%)] Loss: -438255.875000\n",
      "Train Epoch: 340 [8576/17352 (49%)] Loss: -459322.250000\n",
      "Train Epoch: 340 [9984/17352 (58%)] Loss: -444011.000000\n",
      "Train Epoch: 340 [11392/17352 (66%)] Loss: -435766.125000\n",
      "Train Epoch: 340 [12800/17352 (74%)] Loss: -447039.187500\n",
      "Train Epoch: 340 [14208/17352 (82%)] Loss: -425904.406250\n",
      "Train Epoch: 340 [15453/17352 (89%)] Loss: -54025.207031\n",
      "Train Epoch: 340 [16204/17352 (93%)] Loss: -205286.968750\n",
      "Train Epoch: 340 [17043/17352 (98%)] Loss: -385793.281250\n",
      "    epoch          : 340\n",
      "    loss           : -383478.51417654153\n",
      "    val_loss       : -213184.63880208333\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch340.pth ...\n",
      "Train Epoch: 341 [128/17352 (1%)] Loss: -398524.312500\n",
      "Train Epoch: 341 [1536/17352 (9%)] Loss: -376002.468750\n",
      "Train Epoch: 341 [2944/17352 (17%)] Loss: -390790.656250\n",
      "Train Epoch: 341 [4352/17352 (25%)] Loss: -442521.281250\n",
      "Train Epoch: 341 [5760/17352 (33%)] Loss: -403259.312500\n",
      "Train Epoch: 341 [7168/17352 (41%)] Loss: -469541.812500\n",
      "Train Epoch: 341 [8576/17352 (49%)] Loss: -445579.125000\n",
      "Train Epoch: 341 [9984/17352 (58%)] Loss: -434454.218750\n",
      "Train Epoch: 341 [11392/17352 (66%)] Loss: -436194.281250\n",
      "Train Epoch: 341 [12800/17352 (74%)] Loss: -452386.812500\n",
      "Train Epoch: 341 [14208/17352 (82%)] Loss: -468833.500000\n",
      "Train Epoch: 341 [15485/17352 (89%)] Loss: -257609.250000\n",
      "Train Epoch: 341 [16246/17352 (94%)] Loss: -295895.250000\n",
      "Train Epoch: 341 [16972/17352 (98%)] Loss: -108788.570312\n",
      "    epoch          : 341\n",
      "    loss           : -381891.85073012795\n",
      "    val_loss       : -210663.2724609375\n",
      "Train Epoch: 342 [128/17352 (1%)] Loss: -427999.875000\n",
      "Train Epoch: 342 [1536/17352 (9%)] Loss: -461748.656250\n",
      "Train Epoch: 342 [2944/17352 (17%)] Loss: -406906.187500\n",
      "Train Epoch: 342 [4352/17352 (25%)] Loss: -445841.500000\n",
      "Train Epoch: 342 [5760/17352 (33%)] Loss: -437027.218750\n",
      "Train Epoch: 342 [7168/17352 (41%)] Loss: -448967.500000\n",
      "Train Epoch: 342 [8576/17352 (49%)] Loss: -443447.062500\n",
      "Train Epoch: 342 [9984/17352 (58%)] Loss: -279575.750000\n",
      "Train Epoch: 342 [11392/17352 (66%)] Loss: -450296.531250\n",
      "Train Epoch: 342 [12800/17352 (74%)] Loss: -434502.000000\n",
      "Train Epoch: 342 [14208/17352 (82%)] Loss: -368348.062500\n",
      "Train Epoch: 342 [15582/17352 (90%)] Loss: -337398.125000\n",
      "Train Epoch: 342 [16342/17352 (94%)] Loss: -338888.093750\n",
      "Train Epoch: 342 [17119/17352 (99%)] Loss: -202629.937500\n",
      "    epoch          : 342\n",
      "    loss           : -384475.4984925545\n",
      "    val_loss       : -205379.15073242187\n",
      "Train Epoch: 343 [128/17352 (1%)] Loss: -442748.593750\n",
      "Train Epoch: 343 [1536/17352 (9%)] Loss: -459296.312500\n",
      "Train Epoch: 343 [2944/17352 (17%)] Loss: -388878.437500\n",
      "Train Epoch: 343 [4352/17352 (25%)] Loss: -481229.031250\n",
      "Train Epoch: 343 [5760/17352 (33%)] Loss: -463455.562500\n",
      "Train Epoch: 343 [7168/17352 (41%)] Loss: -446013.875000\n",
      "Train Epoch: 343 [8576/17352 (49%)] Loss: -455835.062500\n",
      "Train Epoch: 343 [9984/17352 (58%)] Loss: -454029.562500\n",
      "Train Epoch: 343 [11392/17352 (66%)] Loss: -452533.593750\n",
      "Train Epoch: 343 [12800/17352 (74%)] Loss: -440175.906250\n",
      "Train Epoch: 343 [14208/17352 (82%)] Loss: -379068.062500\n",
      "Train Epoch: 343 [15450/17352 (89%)] Loss: -239710.156250\n",
      "Train Epoch: 343 [16310/17352 (94%)] Loss: -7232.841797\n",
      "Train Epoch: 343 [16968/17352 (98%)] Loss: -134559.625000\n",
      "    epoch          : 343\n",
      "    loss           : -383596.97391463927\n",
      "    val_loss       : -215790.5396158854\n",
      "Train Epoch: 344 [128/17352 (1%)] Loss: -393747.343750\n",
      "Train Epoch: 344 [1536/17352 (9%)] Loss: -421567.656250\n",
      "Train Epoch: 344 [2944/17352 (17%)] Loss: -421317.500000\n",
      "Train Epoch: 344 [4352/17352 (25%)] Loss: -457991.437500\n",
      "Train Epoch: 344 [5760/17352 (33%)] Loss: -418458.500000\n",
      "Train Epoch: 344 [7168/17352 (41%)] Loss: -324080.468750\n",
      "Train Epoch: 344 [8576/17352 (49%)] Loss: -460514.812500\n",
      "Train Epoch: 344 [9984/17352 (58%)] Loss: -416654.687500\n",
      "Train Epoch: 344 [11392/17352 (66%)] Loss: -391135.000000\n",
      "Train Epoch: 344 [12800/17352 (74%)] Loss: -360350.687500\n",
      "Train Epoch: 344 [14208/17352 (82%)] Loss: -322201.625000\n",
      "Train Epoch: 344 [15533/17352 (90%)] Loss: -300202.656250\n",
      "Train Epoch: 344 [16442/17352 (95%)] Loss: -333222.375000\n",
      "Train Epoch: 344 [17022/17352 (98%)] Loss: -48681.769531\n",
      "    epoch          : 344\n",
      "    loss           : -375595.7359971424\n",
      "    val_loss       : -214344.12718098957\n",
      "Train Epoch: 345 [128/17352 (1%)] Loss: -456189.250000\n",
      "Train Epoch: 345 [1536/17352 (9%)] Loss: -403700.093750\n",
      "Train Epoch: 345 [2944/17352 (17%)] Loss: -413605.375000\n",
      "Train Epoch: 345 [4352/17352 (25%)] Loss: -455734.218750\n",
      "Train Epoch: 345 [5760/17352 (33%)] Loss: -453422.281250\n",
      "Train Epoch: 345 [7168/17352 (41%)] Loss: -447751.437500\n",
      "Train Epoch: 345 [8576/17352 (49%)] Loss: -425606.687500\n",
      "Train Epoch: 345 [9984/17352 (58%)] Loss: -431495.375000\n",
      "Train Epoch: 345 [11392/17352 (66%)] Loss: -407307.625000\n",
      "Train Epoch: 345 [12800/17352 (74%)] Loss: -455685.750000\n",
      "Train Epoch: 345 [14208/17352 (82%)] Loss: -429987.593750\n",
      "Train Epoch: 345 [15530/17352 (89%)] Loss: -285188.656250\n",
      "Train Epoch: 345 [16281/17352 (94%)] Loss: -259197.156250\n",
      "Train Epoch: 345 [17030/17352 (98%)] Loss: -343518.906250\n",
      "    epoch          : 345\n",
      "    loss           : -385057.63090853085\n",
      "    val_loss       : -217184.37522786457\n",
      "Train Epoch: 346 [128/17352 (1%)] Loss: -360056.343750\n",
      "Train Epoch: 346 [1536/17352 (9%)] Loss: -388032.375000\n",
      "Train Epoch: 346 [2944/17352 (17%)] Loss: -472394.625000\n",
      "Train Epoch: 346 [4352/17352 (25%)] Loss: -446112.968750\n",
      "Train Epoch: 346 [5760/17352 (33%)] Loss: -441162.906250\n",
      "Train Epoch: 346 [7168/17352 (41%)] Loss: -405446.625000\n",
      "Train Epoch: 346 [8576/17352 (49%)] Loss: -402017.875000\n",
      "Train Epoch: 346 [9984/17352 (58%)] Loss: -394335.656250\n",
      "Train Epoch: 346 [11392/17352 (66%)] Loss: -452073.750000\n",
      "Train Epoch: 346 [12800/17352 (74%)] Loss: -450474.250000\n",
      "Train Epoch: 346 [14208/17352 (82%)] Loss: -427183.031250\n",
      "Train Epoch: 346 [15399/17352 (89%)] Loss: -104453.937500\n",
      "Train Epoch: 346 [16237/17352 (94%)] Loss: -257137.218750\n",
      "Train Epoch: 346 [16912/17352 (97%)] Loss: -238597.812500\n",
      "    epoch          : 346\n",
      "    loss           : -386873.4241951552\n",
      "    val_loss       : -220799.92618815103\n",
      "Train Epoch: 347 [128/17352 (1%)] Loss: -443705.625000\n",
      "Train Epoch: 347 [1536/17352 (9%)] Loss: -423017.062500\n",
      "Train Epoch: 347 [2944/17352 (17%)] Loss: -427515.406250\n",
      "Train Epoch: 347 [4352/17352 (25%)] Loss: -446040.062500\n",
      "Train Epoch: 347 [5760/17352 (33%)] Loss: -409114.656250\n",
      "Train Epoch: 347 [7168/17352 (41%)] Loss: -424154.343750\n",
      "Train Epoch: 347 [8576/17352 (49%)] Loss: -462206.062500\n",
      "Train Epoch: 347 [9984/17352 (58%)] Loss: -460331.468750\n",
      "Train Epoch: 347 [11392/17352 (66%)] Loss: -441859.843750\n",
      "Train Epoch: 347 [12800/17352 (74%)] Loss: -463285.031250\n",
      "Train Epoch: 347 [14208/17352 (82%)] Loss: -421765.437500\n",
      "Train Epoch: 347 [15554/17352 (90%)] Loss: -273280.562500\n",
      "Train Epoch: 347 [16247/17352 (94%)] Loss: -316946.406250\n",
      "Train Epoch: 347 [16988/17352 (98%)] Loss: -151548.703125\n",
      "    epoch          : 347\n",
      "    loss           : -387441.8119035759\n",
      "    val_loss       : -215218.58932291667\n",
      "Train Epoch: 348 [128/17352 (1%)] Loss: -264500.312500\n",
      "Train Epoch: 348 [1536/17352 (9%)] Loss: -440785.656250\n",
      "Train Epoch: 348 [2944/17352 (17%)] Loss: -480375.000000\n",
      "Train Epoch: 348 [4352/17352 (25%)] Loss: -434925.125000\n",
      "Train Epoch: 348 [5760/17352 (33%)] Loss: -464207.343750\n",
      "Train Epoch: 348 [7168/17352 (41%)] Loss: -432375.656250\n",
      "Train Epoch: 348 [8576/17352 (49%)] Loss: -320933.468750\n",
      "Train Epoch: 348 [9984/17352 (58%)] Loss: -455558.312500\n",
      "Train Epoch: 348 [11392/17352 (66%)] Loss: -479175.437500\n",
      "Train Epoch: 348 [12800/17352 (74%)] Loss: -421059.281250\n",
      "Train Epoch: 348 [14208/17352 (82%)] Loss: -450547.187500\n",
      "Train Epoch: 348 [15450/17352 (89%)] Loss: -267214.375000\n",
      "Train Epoch: 348 [16249/17352 (94%)] Loss: -17835.121094\n",
      "Train Epoch: 348 [16914/17352 (97%)] Loss: -106545.328125\n",
      "    epoch          : 348\n",
      "    loss           : -389317.78464502934\n",
      "    val_loss       : -209864.02096354167\n",
      "Train Epoch: 349 [128/17352 (1%)] Loss: -430066.437500\n",
      "Train Epoch: 349 [1536/17352 (9%)] Loss: -341264.812500\n",
      "Train Epoch: 349 [2944/17352 (17%)] Loss: -422575.125000\n",
      "Train Epoch: 349 [4352/17352 (25%)] Loss: -467387.125000\n",
      "Train Epoch: 349 [5760/17352 (33%)] Loss: -476067.375000\n",
      "Train Epoch: 349 [7168/17352 (41%)] Loss: -425011.000000\n",
      "Train Epoch: 349 [8576/17352 (49%)] Loss: -433908.593750\n",
      "Train Epoch: 349 [9984/17352 (58%)] Loss: -442810.468750\n",
      "Train Epoch: 349 [11392/17352 (66%)] Loss: -429275.625000\n",
      "Train Epoch: 349 [12800/17352 (74%)] Loss: -396183.656250\n",
      "Train Epoch: 349 [14208/17352 (82%)] Loss: -416139.343750\n",
      "Train Epoch: 349 [15455/17352 (89%)] Loss: -16786.876953\n",
      "Train Epoch: 349 [16112/17352 (93%)] Loss: -182551.921875\n",
      "Train Epoch: 349 [16924/17352 (98%)] Loss: -142348.578125\n",
      "    epoch          : 349\n",
      "    loss           : -389423.9958971267\n",
      "    val_loss       : -207710.4919108073\n",
      "Train Epoch: 350 [128/17352 (1%)] Loss: -484711.750000\n",
      "Train Epoch: 350 [1536/17352 (9%)] Loss: -438158.187500\n",
      "Train Epoch: 350 [2944/17352 (17%)] Loss: -421181.656250\n",
      "Train Epoch: 350 [4352/17352 (25%)] Loss: -395022.062500\n",
      "Train Epoch: 350 [5760/17352 (33%)] Loss: -447585.250000\n",
      "Train Epoch: 350 [7168/17352 (41%)] Loss: -443955.093750\n",
      "Train Epoch: 350 [8576/17352 (49%)] Loss: -469102.187500\n",
      "Train Epoch: 350 [9984/17352 (58%)] Loss: -456297.125000\n",
      "Train Epoch: 350 [11392/17352 (66%)] Loss: -472523.250000\n",
      "Train Epoch: 350 [12800/17352 (74%)] Loss: -468522.218750\n",
      "Train Epoch: 350 [14208/17352 (82%)] Loss: -414319.062500\n",
      "Train Epoch: 350 [15448/17352 (89%)] Loss: -7249.812988\n",
      "Train Epoch: 350 [16248/17352 (94%)] Loss: -181317.953125\n",
      "Train Epoch: 350 [17034/17352 (98%)] Loss: -115744.593750\n",
      "    epoch          : 350\n",
      "    loss           : -394906.80446793727\n",
      "    val_loss       : -216202.92278645834\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [128/17352 (1%)] Loss: -412452.937500\n",
      "Train Epoch: 351 [1536/17352 (9%)] Loss: -395020.375000\n",
      "Train Epoch: 351 [2944/17352 (17%)] Loss: -443527.937500\n",
      "Train Epoch: 351 [4352/17352 (25%)] Loss: -441608.156250\n",
      "Train Epoch: 351 [5760/17352 (33%)] Loss: -455978.500000\n",
      "Train Epoch: 351 [7168/17352 (41%)] Loss: -429380.375000\n",
      "Train Epoch: 351 [8576/17352 (49%)] Loss: -469955.750000\n",
      "Train Epoch: 351 [9984/17352 (58%)] Loss: -385745.031250\n",
      "Train Epoch: 351 [11392/17352 (66%)] Loss: -468499.250000\n",
      "Train Epoch: 351 [12800/17352 (74%)] Loss: -456018.250000\n",
      "Train Epoch: 351 [14208/17352 (82%)] Loss: -446109.656250\n",
      "Train Epoch: 351 [15592/17352 (90%)] Loss: -363197.531250\n",
      "Train Epoch: 351 [16315/17352 (94%)] Loss: -260748.156250\n",
      "Train Epoch: 351 [17160/17352 (99%)] Loss: -290044.125000\n",
      "    epoch          : 351\n",
      "    loss           : -389512.55458918837\n",
      "    val_loss       : -204855.19108072916\n",
      "Train Epoch: 352 [128/17352 (1%)] Loss: -420888.031250\n",
      "Train Epoch: 352 [1536/17352 (9%)] Loss: -386970.968750\n",
      "Train Epoch: 352 [2944/17352 (17%)] Loss: -298617.875000\n",
      "Train Epoch: 352 [4352/17352 (25%)] Loss: -468089.312500\n",
      "Train Epoch: 352 [5760/17352 (33%)] Loss: -471528.281250\n",
      "Train Epoch: 352 [7168/17352 (41%)] Loss: -378159.312500\n",
      "Train Epoch: 352 [8576/17352 (49%)] Loss: -436644.656250\n",
      "Train Epoch: 352 [9984/17352 (58%)] Loss: -435634.156250\n",
      "Train Epoch: 352 [11392/17352 (66%)] Loss: -458618.968750\n",
      "Train Epoch: 352 [12800/17352 (74%)] Loss: -478945.531250\n",
      "Train Epoch: 352 [14208/17352 (82%)] Loss: -434969.750000\n",
      "Train Epoch: 352 [15474/17352 (89%)] Loss: -384494.375000\n",
      "Train Epoch: 352 [16201/17352 (93%)] Loss: -204576.687500\n",
      "Train Epoch: 352 [16871/17352 (97%)] Loss: -49841.339844\n",
      "    epoch          : 352\n",
      "    loss           : -388500.1067009228\n",
      "    val_loss       : -215906.37721354168\n",
      "Train Epoch: 353 [128/17352 (1%)] Loss: -376553.562500\n",
      "Train Epoch: 353 [1536/17352 (9%)] Loss: -425964.531250\n",
      "Train Epoch: 353 [2944/17352 (17%)] Loss: -385627.531250\n",
      "Train Epoch: 353 [4352/17352 (25%)] Loss: -462271.343750\n",
      "Train Epoch: 353 [5760/17352 (33%)] Loss: -466698.312500\n",
      "Train Epoch: 353 [7168/17352 (41%)] Loss: -365288.875000\n",
      "Train Epoch: 353 [8576/17352 (49%)] Loss: -393823.625000\n",
      "Train Epoch: 353 [9984/17352 (58%)] Loss: -447101.687500\n",
      "Train Epoch: 353 [11392/17352 (66%)] Loss: -435974.625000\n",
      "Train Epoch: 353 [12800/17352 (74%)] Loss: -445602.156250\n",
      "Train Epoch: 353 [14208/17352 (82%)] Loss: -462976.375000\n",
      "Train Epoch: 353 [15602/17352 (90%)] Loss: -406529.312500\n",
      "Train Epoch: 353 [16391/17352 (94%)] Loss: -267121.906250\n",
      "Train Epoch: 353 [16983/17352 (98%)] Loss: -127707.429688\n",
      "    epoch          : 353\n",
      "    loss           : -387924.029060927\n",
      "    val_loss       : -211801.4910481771\n",
      "Train Epoch: 354 [128/17352 (1%)] Loss: -428164.968750\n",
      "Train Epoch: 354 [1536/17352 (9%)] Loss: -326216.093750\n",
      "Train Epoch: 354 [2944/17352 (17%)] Loss: -356286.906250\n",
      "Train Epoch: 354 [4352/17352 (25%)] Loss: -433006.281250\n",
      "Train Epoch: 354 [5760/17352 (33%)] Loss: -435281.750000\n",
      "Train Epoch: 354 [7168/17352 (41%)] Loss: -446638.968750\n",
      "Train Epoch: 354 [8576/17352 (49%)] Loss: -419449.468750\n",
      "Train Epoch: 354 [9984/17352 (58%)] Loss: -417227.500000\n",
      "Train Epoch: 354 [11392/17352 (66%)] Loss: -439784.843750\n",
      "Train Epoch: 354 [12800/17352 (74%)] Loss: -421957.468750\n",
      "Train Epoch: 354 [14208/17352 (82%)] Loss: -407750.343750\n",
      "Train Epoch: 354 [15481/17352 (89%)] Loss: -154701.812500\n",
      "Train Epoch: 354 [16402/17352 (95%)] Loss: -10706.082031\n",
      "Train Epoch: 354 [17084/17352 (98%)] Loss: -281970.312500\n",
      "    epoch          : 354\n",
      "    loss           : -392112.3726011955\n",
      "    val_loss       : -219948.70646158853\n",
      "Train Epoch: 355 [128/17352 (1%)] Loss: -448964.562500\n",
      "Train Epoch: 355 [1536/17352 (9%)] Loss: -442935.968750\n",
      "Train Epoch: 355 [2944/17352 (17%)] Loss: -457191.343750\n",
      "Train Epoch: 355 [4352/17352 (25%)] Loss: -427905.218750\n",
      "Train Epoch: 355 [5760/17352 (33%)] Loss: -436660.875000\n",
      "Train Epoch: 355 [7168/17352 (41%)] Loss: -457781.781250\n",
      "Train Epoch: 355 [8576/17352 (49%)] Loss: -457901.250000\n",
      "Train Epoch: 355 [9984/17352 (58%)] Loss: -417731.437500\n",
      "Train Epoch: 355 [11392/17352 (66%)] Loss: -438114.093750\n",
      "Train Epoch: 355 [12800/17352 (74%)] Loss: -334678.875000\n",
      "Train Epoch: 355 [14208/17352 (82%)] Loss: -349589.906250\n",
      "Train Epoch: 355 [15540/17352 (90%)] Loss: -341073.750000\n",
      "Train Epoch: 355 [16413/17352 (95%)] Loss: -271973.562500\n",
      "Train Epoch: 355 [17032/17352 (98%)] Loss: -34659.734375\n",
      "    epoch          : 355\n",
      "    loss           : -388076.69069185195\n",
      "    val_loss       : -218138.8069173177\n",
      "Train Epoch: 356 [128/17352 (1%)] Loss: -416699.531250\n",
      "Train Epoch: 356 [1536/17352 (9%)] Loss: -409613.781250\n",
      "Train Epoch: 356 [2944/17352 (17%)] Loss: -452216.281250\n",
      "Train Epoch: 356 [4352/17352 (25%)] Loss: -403682.375000\n",
      "Train Epoch: 356 [5760/17352 (33%)] Loss: -434442.406250\n",
      "Train Epoch: 356 [7168/17352 (41%)] Loss: -448105.500000\n",
      "Train Epoch: 356 [8576/17352 (49%)] Loss: -458950.875000\n",
      "Train Epoch: 356 [9984/17352 (58%)] Loss: -412826.375000\n",
      "Train Epoch: 356 [11392/17352 (66%)] Loss: -429950.968750\n",
      "Train Epoch: 356 [12800/17352 (74%)] Loss: -409049.437500\n",
      "Train Epoch: 356 [14208/17352 (82%)] Loss: -435677.312500\n",
      "Train Epoch: 356 [15546/17352 (90%)] Loss: -198799.312500\n",
      "Train Epoch: 356 [16212/17352 (93%)] Loss: -249691.609375\n",
      "Train Epoch: 356 [17028/17352 (98%)] Loss: -344135.312500\n",
      "    epoch          : 356\n",
      "    loss           : -387635.37168362\n",
      "    val_loss       : -217597.89611002605\n",
      "Train Epoch: 357 [128/17352 (1%)] Loss: -469914.968750\n",
      "Train Epoch: 357 [1536/17352 (9%)] Loss: -450533.781250\n",
      "Train Epoch: 357 [2944/17352 (17%)] Loss: -392937.812500\n",
      "Train Epoch: 357 [4352/17352 (25%)] Loss: -397633.312500\n",
      "Train Epoch: 357 [5760/17352 (33%)] Loss: -450158.812500\n",
      "Train Epoch: 357 [7168/17352 (41%)] Loss: -442561.937500\n",
      "Train Epoch: 357 [8576/17352 (49%)] Loss: -462101.312500\n",
      "Train Epoch: 357 [9984/17352 (58%)] Loss: -452221.187500\n",
      "Train Epoch: 357 [11392/17352 (66%)] Loss: -451454.437500\n",
      "Train Epoch: 357 [12800/17352 (74%)] Loss: -466001.125000\n",
      "Train Epoch: 357 [14208/17352 (82%)] Loss: -321755.781250\n",
      "Train Epoch: 357 [15539/17352 (90%)] Loss: -288836.062500\n",
      "Train Epoch: 357 [16220/17352 (93%)] Loss: -149930.218750\n",
      "Train Epoch: 357 [17000/17352 (98%)] Loss: -196666.015625\n",
      "    epoch          : 357\n",
      "    loss           : -386978.31107120385\n",
      "    val_loss       : -214027.1870768229\n",
      "Train Epoch: 358 [128/17352 (1%)] Loss: -356825.312500\n",
      "Train Epoch: 358 [1536/17352 (9%)] Loss: -476398.750000\n",
      "Train Epoch: 358 [2944/17352 (17%)] Loss: -434598.531250\n",
      "Train Epoch: 358 [4352/17352 (25%)] Loss: -326895.687500\n",
      "Train Epoch: 358 [5760/17352 (33%)] Loss: -453389.906250\n",
      "Train Epoch: 358 [7168/17352 (41%)] Loss: -425423.187500\n",
      "Train Epoch: 358 [8576/17352 (49%)] Loss: -376949.593750\n",
      "Train Epoch: 358 [9984/17352 (58%)] Loss: -371923.843750\n",
      "Train Epoch: 358 [11392/17352 (66%)] Loss: -466028.187500\n",
      "Train Epoch: 358 [12800/17352 (74%)] Loss: -429649.937500\n",
      "Train Epoch: 358 [14208/17352 (82%)] Loss: -409533.875000\n",
      "Train Epoch: 358 [15428/17352 (89%)] Loss: -113337.390625\n",
      "Train Epoch: 358 [16183/17352 (93%)] Loss: -262784.406250\n",
      "Train Epoch: 358 [16977/17352 (98%)] Loss: -331257.875000\n",
      "    epoch          : 358\n",
      "    loss           : -389369.7049994757\n",
      "    val_loss       : -215206.98541666666\n",
      "Train Epoch: 359 [128/17352 (1%)] Loss: -439128.750000\n",
      "Train Epoch: 359 [1536/17352 (9%)] Loss: -458768.812500\n",
      "Train Epoch: 359 [2944/17352 (17%)] Loss: -427991.375000\n",
      "Train Epoch: 359 [4352/17352 (25%)] Loss: -464677.250000\n",
      "Train Epoch: 359 [5760/17352 (33%)] Loss: -405863.531250\n",
      "Train Epoch: 359 [7168/17352 (41%)] Loss: -465255.937500\n",
      "Train Epoch: 359 [8576/17352 (49%)] Loss: -452925.625000\n",
      "Train Epoch: 359 [9984/17352 (58%)] Loss: -482762.906250\n",
      "Train Epoch: 359 [11392/17352 (66%)] Loss: -474185.125000\n",
      "Train Epoch: 359 [12800/17352 (74%)] Loss: -485793.687500\n",
      "Train Epoch: 359 [14208/17352 (82%)] Loss: -302977.031250\n",
      "Train Epoch: 359 [15376/17352 (89%)] Loss: -53560.539062\n",
      "Train Epoch: 359 [16159/17352 (93%)] Loss: -61812.203125\n",
      "Train Epoch: 359 [16944/17352 (98%)] Loss: -327513.468750\n",
      "    epoch          : 359\n",
      "    loss           : -394859.0714594694\n",
      "    val_loss       : -211541.18193359376\n",
      "Train Epoch: 360 [128/17352 (1%)] Loss: -403883.437500\n",
      "Train Epoch: 360 [1536/17352 (9%)] Loss: -405399.625000\n",
      "Train Epoch: 360 [2944/17352 (17%)] Loss: -465169.625000\n",
      "Train Epoch: 360 [4352/17352 (25%)] Loss: -479666.843750\n",
      "Train Epoch: 360 [5760/17352 (33%)] Loss: -404321.343750\n",
      "Train Epoch: 360 [7168/17352 (41%)] Loss: -453067.906250\n",
      "Train Epoch: 360 [8576/17352 (49%)] Loss: -418440.843750\n",
      "Train Epoch: 360 [9984/17352 (58%)] Loss: -405799.937500\n",
      "Train Epoch: 360 [11392/17352 (66%)] Loss: -447585.843750\n",
      "Train Epoch: 360 [12800/17352 (74%)] Loss: -440836.781250\n",
      "Train Epoch: 360 [14208/17352 (82%)] Loss: -430396.843750\n",
      "Train Epoch: 360 [15588/17352 (90%)] Loss: -347836.500000\n",
      "Train Epoch: 360 [16423/17352 (95%)] Loss: -253894.015625\n",
      "Train Epoch: 360 [17099/17352 (99%)] Loss: -316130.062500\n",
      "    epoch          : 360\n",
      "    loss           : -390650.8892814073\n",
      "    val_loss       : -222217.79197591144\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch360.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 361 [128/17352 (1%)] Loss: -390573.625000\n",
      "Train Epoch: 361 [1536/17352 (9%)] Loss: -461415.593750\n",
      "Train Epoch: 361 [2944/17352 (17%)] Loss: -460055.750000\n",
      "Train Epoch: 361 [4352/17352 (25%)] Loss: -396379.000000\n",
      "Train Epoch: 361 [5760/17352 (33%)] Loss: -444974.187500\n",
      "Train Epoch: 361 [7168/17352 (41%)] Loss: -445701.437500\n",
      "Train Epoch: 361 [8576/17352 (49%)] Loss: -466261.843750\n",
      "Train Epoch: 361 [9984/17352 (58%)] Loss: -462055.781250\n",
      "Train Epoch: 361 [11392/17352 (66%)] Loss: -466474.875000\n",
      "Train Epoch: 361 [12800/17352 (74%)] Loss: -372896.968750\n",
      "Train Epoch: 361 [14208/17352 (82%)] Loss: -475633.625000\n",
      "Train Epoch: 361 [15560/17352 (90%)] Loss: -215501.171875\n",
      "Train Epoch: 361 [16331/17352 (94%)] Loss: -170465.625000\n",
      "Train Epoch: 361 [17004/17352 (98%)] Loss: -184478.859375\n",
      "    epoch          : 361\n",
      "    loss           : -390997.6221882865\n",
      "    val_loss       : -208819.9451985677\n",
      "Train Epoch: 362 [128/17352 (1%)] Loss: -409063.937500\n",
      "Train Epoch: 362 [1536/17352 (9%)] Loss: -405834.125000\n",
      "Train Epoch: 362 [2944/17352 (17%)] Loss: -362438.187500\n",
      "Train Epoch: 362 [4352/17352 (25%)] Loss: -448179.031250\n",
      "Train Epoch: 362 [5760/17352 (33%)] Loss: -395785.625000\n",
      "Train Epoch: 362 [7168/17352 (41%)] Loss: -479028.500000\n",
      "Train Epoch: 362 [8576/17352 (49%)] Loss: -350600.656250\n",
      "Train Epoch: 362 [9984/17352 (58%)] Loss: -390133.781250\n",
      "Train Epoch: 362 [11392/17352 (66%)] Loss: -427706.187500\n",
      "Train Epoch: 362 [12800/17352 (74%)] Loss: -401484.437500\n",
      "Train Epoch: 362 [14208/17352 (82%)] Loss: -453706.812500\n",
      "Train Epoch: 362 [15521/17352 (89%)] Loss: -319123.968750\n",
      "Train Epoch: 362 [16268/17352 (94%)] Loss: -11004.792969\n",
      "Train Epoch: 362 [17071/17352 (98%)] Loss: -357715.843750\n",
      "    epoch          : 362\n",
      "    loss           : -390949.1120752936\n",
      "    val_loss       : -218332.84654947917\n",
      "Train Epoch: 363 [128/17352 (1%)] Loss: -436231.031250\n",
      "Train Epoch: 363 [1536/17352 (9%)] Loss: -440148.062500\n",
      "Train Epoch: 363 [2944/17352 (17%)] Loss: -455395.875000\n",
      "Train Epoch: 363 [4352/17352 (25%)] Loss: -355529.687500\n",
      "Train Epoch: 363 [5760/17352 (33%)] Loss: -430895.562500\n",
      "Train Epoch: 363 [7168/17352 (41%)] Loss: -404810.562500\n",
      "Train Epoch: 363 [8576/17352 (49%)] Loss: -490943.125000\n",
      "Train Epoch: 363 [9984/17352 (58%)] Loss: -449706.906250\n",
      "Train Epoch: 363 [11392/17352 (66%)] Loss: -456703.562500\n",
      "Train Epoch: 363 [12800/17352 (74%)] Loss: -442144.656250\n",
      "Train Epoch: 363 [14208/17352 (82%)] Loss: -377248.968750\n",
      "Train Epoch: 363 [15575/17352 (90%)] Loss: -327807.500000\n",
      "Train Epoch: 363 [16293/17352 (94%)] Loss: -393290.718750\n",
      "Train Epoch: 363 [17017/17352 (98%)] Loss: -267435.750000\n",
      "    epoch          : 363\n",
      "    loss           : -392423.4482421875\n",
      "    val_loss       : -223773.86012369793\n",
      "Train Epoch: 364 [128/17352 (1%)] Loss: -452422.218750\n",
      "Train Epoch: 364 [1536/17352 (9%)] Loss: -382682.343750\n",
      "Train Epoch: 364 [2944/17352 (17%)] Loss: -411629.062500\n",
      "Train Epoch: 364 [4352/17352 (25%)] Loss: -438991.000000\n",
      "Train Epoch: 364 [5760/17352 (33%)] Loss: -431381.125000\n",
      "Train Epoch: 364 [7168/17352 (41%)] Loss: -456460.718750\n",
      "Train Epoch: 364 [8576/17352 (49%)] Loss: -416350.750000\n",
      "Train Epoch: 364 [9984/17352 (58%)] Loss: -423180.406250\n",
      "Train Epoch: 364 [11392/17352 (66%)] Loss: -473758.843750\n",
      "Train Epoch: 364 [12800/17352 (74%)] Loss: -449206.656250\n",
      "Train Epoch: 364 [14208/17352 (82%)] Loss: -453213.750000\n",
      "Train Epoch: 364 [15525/17352 (89%)] Loss: -248357.468750\n",
      "Train Epoch: 364 [16151/17352 (93%)] Loss: -36198.242188\n",
      "Train Epoch: 364 [16937/17352 (98%)] Loss: -172963.625000\n",
      "    epoch          : 364\n",
      "    loss           : -390900.84835753986\n",
      "    val_loss       : -213512.65849609376\n",
      "Train Epoch: 365 [128/17352 (1%)] Loss: -411633.687500\n",
      "Train Epoch: 365 [1536/17352 (9%)] Loss: -375056.750000\n",
      "Train Epoch: 365 [2944/17352 (17%)] Loss: -188958.593750\n",
      "Train Epoch: 365 [4352/17352 (25%)] Loss: -457323.531250\n",
      "Train Epoch: 365 [5760/17352 (33%)] Loss: -450162.000000\n",
      "Train Epoch: 365 [7168/17352 (41%)] Loss: -435573.875000\n",
      "Train Epoch: 365 [8576/17352 (49%)] Loss: -458856.812500\n",
      "Train Epoch: 365 [9984/17352 (58%)] Loss: -462948.468750\n",
      "Train Epoch: 365 [11392/17352 (66%)] Loss: -432563.062500\n",
      "Train Epoch: 365 [12800/17352 (74%)] Loss: -470882.593750\n",
      "Train Epoch: 365 [14208/17352 (82%)] Loss: -438844.625000\n",
      "Train Epoch: 365 [15437/17352 (89%)] Loss: -119084.992188\n",
      "Train Epoch: 365 [16156/17352 (93%)] Loss: -353858.843750\n",
      "Train Epoch: 365 [16969/17352 (98%)] Loss: -368813.906250\n",
      "    epoch          : 365\n",
      "    loss           : -387309.4161893089\n",
      "    val_loss       : -216697.86861979167\n",
      "Train Epoch: 366 [128/17352 (1%)] Loss: -400976.625000\n",
      "Train Epoch: 366 [1536/17352 (9%)] Loss: -407684.500000\n",
      "Train Epoch: 366 [2944/17352 (17%)] Loss: -452250.062500\n",
      "Train Epoch: 366 [4352/17352 (25%)] Loss: -428913.250000\n",
      "Train Epoch: 366 [5760/17352 (33%)] Loss: -413713.031250\n",
      "Train Epoch: 366 [7168/17352 (41%)] Loss: -422163.843750\n",
      "Train Epoch: 366 [8576/17352 (49%)] Loss: -419465.687500\n",
      "Train Epoch: 366 [9984/17352 (58%)] Loss: -355728.375000\n",
      "Train Epoch: 366 [11392/17352 (66%)] Loss: -452037.625000\n",
      "Train Epoch: 366 [12800/17352 (74%)] Loss: -465162.750000\n",
      "Train Epoch: 366 [14208/17352 (82%)] Loss: -446448.312500\n",
      "Train Epoch: 366 [15573/17352 (90%)] Loss: -303516.531250\n",
      "Train Epoch: 366 [16432/17352 (95%)] Loss: -247125.312500\n",
      "Train Epoch: 366 [17082/17352 (98%)] Loss: -216904.953125\n",
      "    epoch          : 366\n",
      "    loss           : -389853.0546088507\n",
      "    val_loss       : -220937.14205729167\n",
      "Train Epoch: 367 [128/17352 (1%)] Loss: -461562.062500\n",
      "Train Epoch: 367 [1536/17352 (9%)] Loss: -457124.625000\n",
      "Train Epoch: 367 [2944/17352 (17%)] Loss: -438848.312500\n",
      "Train Epoch: 367 [4352/17352 (25%)] Loss: -466018.687500\n",
      "Train Epoch: 367 [5760/17352 (33%)] Loss: -475749.687500\n",
      "Train Epoch: 367 [7168/17352 (41%)] Loss: -397152.281250\n",
      "Train Epoch: 367 [8576/17352 (49%)] Loss: -458232.312500\n",
      "Train Epoch: 367 [9984/17352 (58%)] Loss: -447714.281250\n",
      "Train Epoch: 367 [11392/17352 (66%)] Loss: -480233.343750\n",
      "Train Epoch: 367 [12800/17352 (74%)] Loss: -474490.500000\n",
      "Train Epoch: 367 [14208/17352 (82%)] Loss: -457641.312500\n",
      "Train Epoch: 367 [15472/17352 (89%)] Loss: -288446.062500\n",
      "Train Epoch: 367 [16210/17352 (93%)] Loss: -123498.593750\n",
      "Train Epoch: 367 [17153/17352 (99%)] Loss: -395603.812500\n",
      "    epoch          : 367\n",
      "    loss           : -393589.2685743498\n",
      "    val_loss       : -216985.26521809897\n",
      "Train Epoch: 368 [128/17352 (1%)] Loss: -387755.937500\n",
      "Train Epoch: 368 [1536/17352 (9%)] Loss: -444949.437500\n",
      "Train Epoch: 368 [2944/17352 (17%)] Loss: -192779.625000\n",
      "Train Epoch: 368 [4352/17352 (25%)] Loss: -371332.218750\n",
      "Train Epoch: 368 [5760/17352 (33%)] Loss: -451026.812500\n",
      "Train Epoch: 368 [7168/17352 (41%)] Loss: -388125.937500\n",
      "Train Epoch: 368 [8576/17352 (49%)] Loss: -414334.250000\n",
      "Train Epoch: 368 [9984/17352 (58%)] Loss: -444295.718750\n",
      "Train Epoch: 368 [11392/17352 (66%)] Loss: -354692.125000\n",
      "Train Epoch: 368 [12800/17352 (74%)] Loss: -439734.000000\n",
      "Train Epoch: 368 [14208/17352 (82%)] Loss: -483223.750000\n",
      "Train Epoch: 368 [15478/17352 (89%)] Loss: -211079.343750\n",
      "Train Epoch: 368 [16353/17352 (94%)] Loss: -398251.937500\n",
      "Train Epoch: 368 [16999/17352 (98%)] Loss: -335936.781250\n",
      "    epoch          : 368\n",
      "    loss           : -390224.07974386535\n",
      "    val_loss       : -214014.2822265625\n",
      "Train Epoch: 369 [128/17352 (1%)] Loss: -485659.125000\n",
      "Train Epoch: 369 [1536/17352 (9%)] Loss: -444183.562500\n",
      "Train Epoch: 369 [2944/17352 (17%)] Loss: -465589.875000\n",
      "Train Epoch: 369 [4352/17352 (25%)] Loss: -429491.468750\n",
      "Train Epoch: 369 [5760/17352 (33%)] Loss: -424377.281250\n",
      "Train Epoch: 369 [7168/17352 (41%)] Loss: -439596.500000\n",
      "Train Epoch: 369 [8576/17352 (49%)] Loss: -443421.687500\n",
      "Train Epoch: 369 [9984/17352 (58%)] Loss: -423363.031250\n",
      "Train Epoch: 369 [11392/17352 (66%)] Loss: -448447.531250\n",
      "Train Epoch: 369 [12800/17352 (74%)] Loss: -435867.312500\n",
      "Train Epoch: 369 [14208/17352 (82%)] Loss: -446863.500000\n",
      "Train Epoch: 369 [15499/17352 (89%)] Loss: -194759.484375\n",
      "Train Epoch: 369 [16260/17352 (94%)] Loss: -386334.500000\n",
      "Train Epoch: 369 [16910/17352 (97%)] Loss: -117171.335938\n",
      "    epoch          : 369\n",
      "    loss           : -393990.33732697146\n",
      "    val_loss       : -222214.57958984375\n",
      "Train Epoch: 370 [128/17352 (1%)] Loss: -476973.875000\n",
      "Train Epoch: 370 [1536/17352 (9%)] Loss: -430750.468750\n",
      "Train Epoch: 370 [2944/17352 (17%)] Loss: -457217.718750\n",
      "Train Epoch: 370 [4352/17352 (25%)] Loss: -434630.312500\n",
      "Train Epoch: 370 [5760/17352 (33%)] Loss: -474523.468750\n",
      "Train Epoch: 370 [7168/17352 (41%)] Loss: -442893.000000\n",
      "Train Epoch: 370 [8576/17352 (49%)] Loss: -412813.750000\n",
      "Train Epoch: 370 [9984/17352 (58%)] Loss: -435009.218750\n",
      "Train Epoch: 370 [11392/17352 (66%)] Loss: -423728.843750\n",
      "Train Epoch: 370 [12800/17352 (74%)] Loss: -481781.656250\n",
      "Train Epoch: 370 [14208/17352 (82%)] Loss: -371738.437500\n",
      "Train Epoch: 370 [15442/17352 (89%)] Loss: -262614.906250\n",
      "Train Epoch: 370 [16272/17352 (94%)] Loss: -327312.781250\n",
      "Train Epoch: 370 [17092/17352 (99%)] Loss: -373600.218750\n",
      "    epoch          : 370\n",
      "    loss           : -392484.22128644085\n",
      "    val_loss       : -214831.9326171875\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch370.pth ...\n",
      "Train Epoch: 371 [128/17352 (1%)] Loss: -469495.562500\n",
      "Train Epoch: 371 [1536/17352 (9%)] Loss: -393219.750000\n",
      "Train Epoch: 371 [2944/17352 (17%)] Loss: -432239.562500\n",
      "Train Epoch: 371 [4352/17352 (25%)] Loss: -469420.437500\n",
      "Train Epoch: 371 [5760/17352 (33%)] Loss: -466056.562500\n",
      "Train Epoch: 371 [7168/17352 (41%)] Loss: -411176.843750\n",
      "Train Epoch: 371 [8576/17352 (49%)] Loss: -429559.218750\n",
      "Train Epoch: 371 [9984/17352 (58%)] Loss: -478759.812500\n",
      "Train Epoch: 371 [11392/17352 (66%)] Loss: -367511.937500\n",
      "Train Epoch: 371 [12800/17352 (74%)] Loss: -466066.062500\n",
      "Train Epoch: 371 [14208/17352 (82%)] Loss: -370716.312500\n",
      "Train Epoch: 371 [15419/17352 (89%)] Loss: -146618.843750\n",
      "Train Epoch: 371 [16173/17352 (93%)] Loss: -416552.187500\n",
      "Train Epoch: 371 [17004/17352 (98%)] Loss: -214062.500000\n",
      "    epoch          : 371\n",
      "    loss           : -391914.6100435193\n",
      "    val_loss       : -214677.67283528644\n",
      "Train Epoch: 372 [128/17352 (1%)] Loss: -479788.343750\n",
      "Train Epoch: 372 [1536/17352 (9%)] Loss: -429706.250000\n",
      "Train Epoch: 372 [2944/17352 (17%)] Loss: -402165.468750\n",
      "Train Epoch: 372 [4352/17352 (25%)] Loss: -465886.625000\n",
      "Train Epoch: 372 [5760/17352 (33%)] Loss: -429845.750000\n",
      "Train Epoch: 372 [7168/17352 (41%)] Loss: -395633.468750\n",
      "Train Epoch: 372 [8576/17352 (49%)] Loss: -435192.812500\n",
      "Train Epoch: 372 [9984/17352 (58%)] Loss: -439177.343750\n",
      "Train Epoch: 372 [11392/17352 (66%)] Loss: -421899.531250\n",
      "Train Epoch: 372 [12800/17352 (74%)] Loss: -440681.093750\n",
      "Train Epoch: 372 [14208/17352 (82%)] Loss: -354613.625000\n",
      "Train Epoch: 372 [15510/17352 (89%)] Loss: -325562.218750\n",
      "Train Epoch: 372 [16324/17352 (94%)] Loss: -149746.562500\n",
      "Train Epoch: 372 [17048/17352 (98%)] Loss: -91494.609375\n",
      "    epoch          : 372\n",
      "    loss           : -385308.4394006921\n",
      "    val_loss       : -217719.86751302084\n",
      "Train Epoch: 373 [128/17352 (1%)] Loss: -468417.812500\n",
      "Train Epoch: 373 [1536/17352 (9%)] Loss: -362936.750000\n",
      "Train Epoch: 373 [2944/17352 (17%)] Loss: -406737.062500\n",
      "Train Epoch: 373 [4352/17352 (25%)] Loss: -379817.687500\n",
      "Train Epoch: 373 [5760/17352 (33%)] Loss: -448553.218750\n",
      "Train Epoch: 373 [7168/17352 (41%)] Loss: -465028.125000\n",
      "Train Epoch: 373 [8576/17352 (49%)] Loss: -412116.187500\n",
      "Train Epoch: 373 [9984/17352 (58%)] Loss: -421453.875000\n",
      "Train Epoch: 373 [11392/17352 (66%)] Loss: -476291.250000\n",
      "Train Epoch: 373 [12800/17352 (74%)] Loss: -471692.656250\n",
      "Train Epoch: 373 [14208/17352 (82%)] Loss: -423915.750000\n",
      "Train Epoch: 373 [15468/17352 (89%)] Loss: -206029.000000\n",
      "Train Epoch: 373 [16211/17352 (93%)] Loss: -334281.218750\n",
      "Train Epoch: 373 [17006/17352 (98%)] Loss: -287724.781250\n",
      "    epoch          : 373\n",
      "    loss           : -388245.8703727978\n",
      "    val_loss       : -219188.10305989583\n",
      "Train Epoch: 374 [128/17352 (1%)] Loss: -445854.875000\n",
      "Train Epoch: 374 [1536/17352 (9%)] Loss: -450581.000000\n",
      "Train Epoch: 374 [2944/17352 (17%)] Loss: -468946.187500\n",
      "Train Epoch: 374 [4352/17352 (25%)] Loss: -468415.562500\n",
      "Train Epoch: 374 [5760/17352 (33%)] Loss: -435350.750000\n",
      "Train Epoch: 374 [7168/17352 (41%)] Loss: -404616.812500\n",
      "Train Epoch: 374 [8576/17352 (49%)] Loss: -397419.531250\n",
      "Train Epoch: 374 [9984/17352 (58%)] Loss: -445310.156250\n",
      "Train Epoch: 374 [11392/17352 (66%)] Loss: -414417.312500\n",
      "Train Epoch: 374 [12800/17352 (74%)] Loss: -447892.468750\n",
      "Train Epoch: 374 [14208/17352 (82%)] Loss: -444433.593750\n",
      "Train Epoch: 374 [15530/17352 (89%)] Loss: -241848.312500\n",
      "Train Epoch: 374 [16430/17352 (95%)] Loss: -159272.062500\n",
      "Train Epoch: 374 [17009/17352 (98%)] Loss: -98486.734375\n",
      "    epoch          : 374\n",
      "    loss           : -391866.8441760172\n",
      "    val_loss       : -214151.57906901042\n",
      "Train Epoch: 375 [128/17352 (1%)] Loss: -450883.750000\n",
      "Train Epoch: 375 [1536/17352 (9%)] Loss: -431174.875000\n",
      "Train Epoch: 375 [2944/17352 (17%)] Loss: -349428.500000\n",
      "Train Epoch: 375 [4352/17352 (25%)] Loss: -448037.718750\n",
      "Train Epoch: 375 [5760/17352 (33%)] Loss: -456558.218750\n",
      "Train Epoch: 375 [7168/17352 (41%)] Loss: -239789.718750\n",
      "Train Epoch: 375 [8576/17352 (49%)] Loss: -385369.843750\n",
      "Train Epoch: 375 [9984/17352 (58%)] Loss: -451487.406250\n",
      "Train Epoch: 375 [11392/17352 (66%)] Loss: -466460.062500\n",
      "Train Epoch: 375 [12800/17352 (74%)] Loss: -460828.093750\n",
      "Train Epoch: 375 [14208/17352 (82%)] Loss: -452885.562500\n",
      "Train Epoch: 375 [15497/17352 (89%)] Loss: -359115.625000\n",
      "Train Epoch: 375 [16149/17352 (93%)] Loss: -453391.750000\n",
      "Train Epoch: 375 [17094/17352 (99%)] Loss: -185287.171875\n",
      "    epoch          : 375\n",
      "    loss           : -387207.1495910235\n",
      "    val_loss       : -218930.85538736978\n",
      "Train Epoch: 376 [128/17352 (1%)] Loss: -438370.187500\n",
      "Train Epoch: 376 [1536/17352 (9%)] Loss: -432100.250000\n",
      "Train Epoch: 376 [2944/17352 (17%)] Loss: -326653.812500\n",
      "Train Epoch: 376 [4352/17352 (25%)] Loss: -443351.093750\n",
      "Train Epoch: 376 [5760/17352 (33%)] Loss: -444700.937500\n",
      "Train Epoch: 376 [7168/17352 (41%)] Loss: -439435.250000\n",
      "Train Epoch: 376 [8576/17352 (49%)] Loss: -460576.250000\n",
      "Train Epoch: 376 [9984/17352 (58%)] Loss: -471950.937500\n",
      "Train Epoch: 376 [11392/17352 (66%)] Loss: -403928.812500\n",
      "Train Epoch: 376 [12800/17352 (74%)] Loss: -423704.750000\n",
      "Train Epoch: 376 [14208/17352 (82%)] Loss: -439726.843750\n",
      "Train Epoch: 376 [15529/17352 (89%)] Loss: -332377.968750\n",
      "Train Epoch: 376 [16195/17352 (93%)] Loss: -240076.421875\n",
      "Train Epoch: 376 [17079/17352 (98%)] Loss: -386364.500000\n",
      "    epoch          : 376\n",
      "    loss           : -389768.9990168834\n",
      "    val_loss       : -214908.31567382812\n",
      "Train Epoch: 377 [128/17352 (1%)] Loss: -422432.875000\n",
      "Train Epoch: 377 [1536/17352 (9%)] Loss: -425943.750000\n",
      "Train Epoch: 377 [2944/17352 (17%)] Loss: -480964.343750\n",
      "Train Epoch: 377 [4352/17352 (25%)] Loss: -374881.062500\n",
      "Train Epoch: 377 [5760/17352 (33%)] Loss: -435251.250000\n",
      "Train Epoch: 377 [7168/17352 (41%)] Loss: -391110.687500\n",
      "Train Epoch: 377 [8576/17352 (49%)] Loss: -474706.437500\n",
      "Train Epoch: 377 [9984/17352 (58%)] Loss: -384161.812500\n",
      "Train Epoch: 377 [11392/17352 (66%)] Loss: -445916.375000\n",
      "Train Epoch: 377 [12800/17352 (74%)] Loss: -439916.000000\n",
      "Train Epoch: 377 [14208/17352 (82%)] Loss: -457360.843750\n",
      "Train Epoch: 377 [15574/17352 (90%)] Loss: -393549.437500\n",
      "Train Epoch: 377 [16193/17352 (93%)] Loss: -243071.515625\n",
      "Train Epoch: 377 [17107/17352 (99%)] Loss: -308875.937500\n",
      "    epoch          : 377\n",
      "    loss           : -393998.3627044883\n",
      "    val_loss       : -218447.5601236979\n",
      "Train Epoch: 378 [128/17352 (1%)] Loss: -463030.187500\n",
      "Train Epoch: 378 [1536/17352 (9%)] Loss: -449889.062500\n",
      "Train Epoch: 378 [2944/17352 (17%)] Loss: -448405.031250\n",
      "Train Epoch: 378 [4352/17352 (25%)] Loss: -424292.093750\n",
      "Train Epoch: 378 [5760/17352 (33%)] Loss: -453206.375000\n",
      "Train Epoch: 378 [7168/17352 (41%)] Loss: -413847.968750\n",
      "Train Epoch: 378 [8576/17352 (49%)] Loss: -466796.875000\n",
      "Train Epoch: 378 [9984/17352 (58%)] Loss: -326630.093750\n",
      "Train Epoch: 378 [11392/17352 (66%)] Loss: -335049.343750\n",
      "Train Epoch: 378 [12800/17352 (74%)] Loss: -453867.187500\n",
      "Train Epoch: 378 [14208/17352 (82%)] Loss: -459852.843750\n",
      "Train Epoch: 378 [15427/17352 (89%)] Loss: -27959.517578\n",
      "Train Epoch: 378 [16325/17352 (94%)] Loss: -171270.578125\n",
      "Train Epoch: 378 [16976/17352 (98%)] Loss: -379490.093750\n",
      "    epoch          : 378\n",
      "    loss           : -389035.92119337246\n",
      "    val_loss       : -223077.54541015625\n",
      "Train Epoch: 379 [128/17352 (1%)] Loss: -362039.937500\n",
      "Train Epoch: 379 [1536/17352 (9%)] Loss: -454967.343750\n",
      "Train Epoch: 379 [2944/17352 (17%)] Loss: -470123.781250\n",
      "Train Epoch: 379 [4352/17352 (25%)] Loss: -468088.593750\n",
      "Train Epoch: 379 [5760/17352 (33%)] Loss: -465967.250000\n",
      "Train Epoch: 379 [7168/17352 (41%)] Loss: -436066.875000\n",
      "Train Epoch: 379 [8576/17352 (49%)] Loss: -392571.937500\n",
      "Train Epoch: 379 [9984/17352 (58%)] Loss: -388343.750000\n",
      "Train Epoch: 379 [11392/17352 (66%)] Loss: -428364.125000\n",
      "Train Epoch: 379 [12800/17352 (74%)] Loss: -436813.437500\n",
      "Train Epoch: 379 [14208/17352 (82%)] Loss: -415535.218750\n",
      "Train Epoch: 379 [15523/17352 (89%)] Loss: -261905.812500\n",
      "Train Epoch: 379 [16146/17352 (93%)] Loss: -281488.000000\n",
      "Train Epoch: 379 [16939/17352 (98%)] Loss: -121751.359375\n",
      "    epoch          : 379\n",
      "    loss           : -386749.9534527055\n",
      "    val_loss       : -215453.70485026043\n",
      "Train Epoch: 380 [128/17352 (1%)] Loss: -440172.375000\n",
      "Train Epoch: 380 [1536/17352 (9%)] Loss: -469446.062500\n",
      "Train Epoch: 380 [2944/17352 (17%)] Loss: -470477.875000\n",
      "Train Epoch: 380 [4352/17352 (25%)] Loss: -456940.656250\n",
      "Train Epoch: 380 [5760/17352 (33%)] Loss: -403834.093750\n",
      "Train Epoch: 380 [7168/17352 (41%)] Loss: -400463.906250\n",
      "Train Epoch: 380 [8576/17352 (49%)] Loss: -485627.437500\n",
      "Train Epoch: 380 [9984/17352 (58%)] Loss: -441422.500000\n",
      "Train Epoch: 380 [11392/17352 (66%)] Loss: -441046.000000\n",
      "Train Epoch: 380 [12800/17352 (74%)] Loss: -444637.687500\n",
      "Train Epoch: 380 [14208/17352 (82%)] Loss: -382660.000000\n",
      "Train Epoch: 380 [15474/17352 (89%)] Loss: -344091.593750\n",
      "Train Epoch: 380 [16375/17352 (94%)] Loss: -244750.125000\n",
      "Train Epoch: 380 [16978/17352 (98%)] Loss: -388584.125000\n",
      "    epoch          : 380\n",
      "    loss           : -383580.11633546563\n",
      "    val_loss       : -214503.5317220052\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch380.pth ...\n",
      "Train Epoch: 381 [128/17352 (1%)] Loss: -445108.750000\n",
      "Train Epoch: 381 [1536/17352 (9%)] Loss: -394100.437500\n",
      "Train Epoch: 381 [2944/17352 (17%)] Loss: -466331.781250\n",
      "Train Epoch: 381 [4352/17352 (25%)] Loss: -446662.312500\n",
      "Train Epoch: 381 [5760/17352 (33%)] Loss: -463285.843750\n",
      "Train Epoch: 381 [7168/17352 (41%)] Loss: -459687.031250\n",
      "Train Epoch: 381 [8576/17352 (49%)] Loss: -455645.125000\n",
      "Train Epoch: 381 [9984/17352 (58%)] Loss: -451630.406250\n",
      "Train Epoch: 381 [11392/17352 (66%)] Loss: -452877.000000\n",
      "Train Epoch: 381 [12800/17352 (74%)] Loss: -455237.531250\n",
      "Train Epoch: 381 [14208/17352 (82%)] Loss: -456437.468750\n",
      "Train Epoch: 381 [15533/17352 (90%)] Loss: -330101.718750\n",
      "Train Epoch: 381 [16350/17352 (94%)] Loss: -227008.687500\n",
      "Train Epoch: 381 [17039/17352 (98%)] Loss: -299791.468750\n",
      "    epoch          : 381\n",
      "    loss           : -387183.6129797609\n",
      "    val_loss       : -212309.15182291667\n",
      "Train Epoch: 382 [128/17352 (1%)] Loss: -410807.218750\n",
      "Train Epoch: 382 [1536/17352 (9%)] Loss: -311329.375000\n",
      "Train Epoch: 382 [2944/17352 (17%)] Loss: -468208.093750\n",
      "Train Epoch: 382 [4352/17352 (25%)] Loss: -463116.937500\n",
      "Train Epoch: 382 [5760/17352 (33%)] Loss: -442797.875000\n",
      "Train Epoch: 382 [7168/17352 (41%)] Loss: -463745.062500\n",
      "Train Epoch: 382 [8576/17352 (49%)] Loss: -457830.312500\n",
      "Train Epoch: 382 [9984/17352 (58%)] Loss: -425455.468750\n",
      "Train Epoch: 382 [11392/17352 (66%)] Loss: -457635.187500\n",
      "Train Epoch: 382 [12800/17352 (74%)] Loss: -473536.687500\n",
      "Train Epoch: 382 [14208/17352 (82%)] Loss: -471509.062500\n",
      "Train Epoch: 382 [15435/17352 (89%)] Loss: -102396.765625\n",
      "Train Epoch: 382 [16045/17352 (92%)] Loss: -183139.687500\n",
      "Train Epoch: 382 [16901/17352 (97%)] Loss: -441044.593750\n",
      "    epoch          : 382\n",
      "    loss           : -390572.75671140937\n",
      "    val_loss       : -218091.44643554688\n",
      "Train Epoch: 383 [128/17352 (1%)] Loss: -373213.718750\n",
      "Train Epoch: 383 [1536/17352 (9%)] Loss: -358677.625000\n",
      "Train Epoch: 383 [2944/17352 (17%)] Loss: -439478.781250\n",
      "Train Epoch: 383 [4352/17352 (25%)] Loss: -462579.250000\n",
      "Train Epoch: 383 [5760/17352 (33%)] Loss: -430837.781250\n",
      "Train Epoch: 383 [7168/17352 (41%)] Loss: -399862.875000\n",
      "Train Epoch: 383 [8576/17352 (49%)] Loss: -498328.500000\n",
      "Train Epoch: 383 [9984/17352 (58%)] Loss: -347546.937500\n",
      "Train Epoch: 383 [11392/17352 (66%)] Loss: -429820.750000\n",
      "Train Epoch: 383 [12800/17352 (74%)] Loss: -480563.062500\n",
      "Train Epoch: 383 [14208/17352 (82%)] Loss: -436630.218750\n",
      "Train Epoch: 383 [15480/17352 (89%)] Loss: -291765.187500\n",
      "Train Epoch: 383 [16343/17352 (94%)] Loss: -323233.656250\n",
      "Train Epoch: 383 [17046/17352 (98%)] Loss: -331584.812500\n",
      "    epoch          : 383\n",
      "    loss           : -391184.0063378251\n",
      "    val_loss       : -221635.36166992187\n",
      "Train Epoch: 384 [128/17352 (1%)] Loss: -272748.375000\n",
      "Train Epoch: 384 [1536/17352 (9%)] Loss: -392187.812500\n",
      "Train Epoch: 384 [2944/17352 (17%)] Loss: -461815.250000\n",
      "Train Epoch: 384 [4352/17352 (25%)] Loss: -358105.781250\n",
      "Train Epoch: 384 [5760/17352 (33%)] Loss: -442585.781250\n",
      "Train Epoch: 384 [7168/17352 (41%)] Loss: -449952.218750\n",
      "Train Epoch: 384 [8576/17352 (49%)] Loss: -435288.750000\n",
      "Train Epoch: 384 [9984/17352 (58%)] Loss: -443410.437500\n",
      "Train Epoch: 384 [11392/17352 (66%)] Loss: -473515.218750\n",
      "Train Epoch: 384 [12800/17352 (74%)] Loss: -470232.468750\n",
      "Train Epoch: 384 [14208/17352 (82%)] Loss: -413314.750000\n",
      "Train Epoch: 384 [15525/17352 (89%)] Loss: -261825.890625\n",
      "Train Epoch: 384 [16287/17352 (94%)] Loss: -228195.046875\n",
      "Train Epoch: 384 [16960/17352 (98%)] Loss: -194049.062500\n",
      "    epoch          : 384\n",
      "    loss           : -389914.24275115354\n",
      "    val_loss       : -211734.17408854168\n",
      "Train Epoch: 385 [128/17352 (1%)] Loss: -465374.812500\n",
      "Train Epoch: 385 [1536/17352 (9%)] Loss: -430298.000000\n",
      "Train Epoch: 385 [2944/17352 (17%)] Loss: -414499.625000\n",
      "Train Epoch: 385 [4352/17352 (25%)] Loss: -446578.312500\n",
      "Train Epoch: 385 [5760/17352 (33%)] Loss: -466125.312500\n",
      "Train Epoch: 385 [7168/17352 (41%)] Loss: -467685.312500\n",
      "Train Epoch: 385 [8576/17352 (49%)] Loss: -457004.750000\n",
      "Train Epoch: 385 [9984/17352 (58%)] Loss: -454274.000000\n",
      "Train Epoch: 385 [11392/17352 (66%)] Loss: -320036.687500\n",
      "Train Epoch: 385 [12800/17352 (74%)] Loss: -475906.375000\n",
      "Train Epoch: 385 [14208/17352 (82%)] Loss: -411066.250000\n",
      "Train Epoch: 385 [15468/17352 (89%)] Loss: -377431.718750\n",
      "Train Epoch: 385 [16189/17352 (93%)] Loss: -271953.781250\n",
      "Train Epoch: 385 [17008/17352 (98%)] Loss: -296300.875000\n",
      "    epoch          : 385\n",
      "    loss           : -385392.30900010484\n",
      "    val_loss       : -213040.13865559894\n",
      "Train Epoch: 386 [128/17352 (1%)] Loss: -461563.656250\n",
      "Train Epoch: 386 [1536/17352 (9%)] Loss: -459372.031250\n",
      "Train Epoch: 386 [2944/17352 (17%)] Loss: -468777.250000\n",
      "Train Epoch: 386 [4352/17352 (25%)] Loss: -442881.000000\n",
      "Train Epoch: 386 [5760/17352 (33%)] Loss: -389698.062500\n",
      "Train Epoch: 386 [7168/17352 (41%)] Loss: -390870.156250\n",
      "Train Epoch: 386 [8576/17352 (49%)] Loss: -445364.093750\n",
      "Train Epoch: 386 [9984/17352 (58%)] Loss: -436568.375000\n",
      "Train Epoch: 386 [11392/17352 (66%)] Loss: -436914.781250\n",
      "Train Epoch: 386 [12800/17352 (74%)] Loss: -440561.937500\n",
      "Train Epoch: 386 [14208/17352 (82%)] Loss: -423070.187500\n",
      "Train Epoch: 386 [15505/17352 (89%)] Loss: -191312.593750\n",
      "Train Epoch: 386 [16426/17352 (95%)] Loss: -264903.843750\n",
      "Train Epoch: 386 [17135/17352 (99%)] Loss: -11102.890625\n",
      "    epoch          : 386\n",
      "    loss           : -381279.37191301386\n",
      "    val_loss       : -215200.69490559897\n",
      "Train Epoch: 387 [128/17352 (1%)] Loss: -422463.625000\n",
      "Train Epoch: 387 [1536/17352 (9%)] Loss: -451158.312500\n",
      "Train Epoch: 387 [2944/17352 (17%)] Loss: -310511.093750\n",
      "Train Epoch: 387 [4352/17352 (25%)] Loss: -367815.812500\n",
      "Train Epoch: 387 [5760/17352 (33%)] Loss: -451523.593750\n",
      "Train Epoch: 387 [7168/17352 (41%)] Loss: -443259.500000\n",
      "Train Epoch: 387 [8576/17352 (49%)] Loss: -431009.718750\n",
      "Train Epoch: 387 [9984/17352 (58%)] Loss: -352161.812500\n",
      "Train Epoch: 387 [11392/17352 (66%)] Loss: -385197.656250\n",
      "Train Epoch: 387 [12800/17352 (74%)] Loss: -442872.125000\n",
      "Train Epoch: 387 [14208/17352 (82%)] Loss: -463279.375000\n",
      "Train Epoch: 387 [15460/17352 (89%)] Loss: -55304.304688\n",
      "Train Epoch: 387 [16208/17352 (93%)] Loss: -350633.312500\n",
      "Train Epoch: 387 [16983/17352 (98%)] Loss: -325152.125000\n",
      "    epoch          : 387\n",
      "    loss           : -380752.0369389681\n",
      "    val_loss       : -216784.32604166667\n",
      "Train Epoch: 388 [128/17352 (1%)] Loss: -415158.312500\n",
      "Train Epoch: 388 [1536/17352 (9%)] Loss: -400399.468750\n",
      "Train Epoch: 388 [2944/17352 (17%)] Loss: -450482.750000\n",
      "Train Epoch: 388 [4352/17352 (25%)] Loss: -341165.250000\n",
      "Train Epoch: 388 [5760/17352 (33%)] Loss: -425490.125000\n",
      "Train Epoch: 388 [7168/17352 (41%)] Loss: -370794.375000\n",
      "Train Epoch: 388 [8576/17352 (49%)] Loss: -384799.750000\n",
      "Train Epoch: 388 [9984/17352 (58%)] Loss: -450080.656250\n",
      "Train Epoch: 388 [11392/17352 (66%)] Loss: -430327.937500\n",
      "Train Epoch: 388 [12800/17352 (74%)] Loss: -445032.937500\n",
      "Train Epoch: 388 [14208/17352 (82%)] Loss: -453350.781250\n",
      "Train Epoch: 388 [15540/17352 (90%)] Loss: -376790.937500\n",
      "Train Epoch: 388 [16344/17352 (94%)] Loss: -187826.015625\n",
      "Train Epoch: 388 [17109/17352 (99%)] Loss: -302521.812500\n",
      "    epoch          : 388\n",
      "    loss           : -385175.2710583578\n",
      "    val_loss       : -205859.8477701823\n",
      "Train Epoch: 389 [128/17352 (1%)] Loss: -310600.343750\n",
      "Train Epoch: 389 [1536/17352 (9%)] Loss: -427150.125000\n",
      "Train Epoch: 389 [2944/17352 (17%)] Loss: -310538.937500\n",
      "Train Epoch: 389 [4352/17352 (25%)] Loss: -437499.812500\n",
      "Train Epoch: 389 [5760/17352 (33%)] Loss: -445726.093750\n",
      "Train Epoch: 389 [7168/17352 (41%)] Loss: -322841.500000\n",
      "Train Epoch: 389 [8576/17352 (49%)] Loss: -462605.218750\n",
      "Train Epoch: 389 [9984/17352 (58%)] Loss: -429684.125000\n",
      "Train Epoch: 389 [11392/17352 (66%)] Loss: -456793.625000\n",
      "Train Epoch: 389 [12800/17352 (74%)] Loss: -420295.500000\n",
      "Train Epoch: 389 [14208/17352 (82%)] Loss: -386446.125000\n",
      "Train Epoch: 389 [15497/17352 (89%)] Loss: -140634.406250\n",
      "Train Epoch: 389 [16242/17352 (94%)] Loss: -254552.468750\n",
      "Train Epoch: 389 [17083/17352 (98%)] Loss: -258298.421875\n",
      "    epoch          : 389\n",
      "    loss           : -382544.90995634964\n",
      "    val_loss       : -210369.58404947916\n",
      "Train Epoch: 390 [128/17352 (1%)] Loss: -453418.625000\n",
      "Train Epoch: 390 [1536/17352 (9%)] Loss: -434999.000000\n",
      "Train Epoch: 390 [2944/17352 (17%)] Loss: -452349.625000\n",
      "Train Epoch: 390 [4352/17352 (25%)] Loss: -443248.125000\n",
      "Train Epoch: 390 [5760/17352 (33%)] Loss: -458512.250000\n",
      "Train Epoch: 390 [7168/17352 (41%)] Loss: -455366.031250\n",
      "Train Epoch: 390 [8576/17352 (49%)] Loss: -398499.468750\n",
      "Train Epoch: 390 [9984/17352 (58%)] Loss: -386121.625000\n",
      "Train Epoch: 390 [11392/17352 (66%)] Loss: -446349.468750\n",
      "Train Epoch: 390 [12800/17352 (74%)] Loss: -394874.375000\n",
      "Train Epoch: 390 [14208/17352 (82%)] Loss: -440704.687500\n",
      "Train Epoch: 390 [15553/17352 (90%)] Loss: -431307.156250\n",
      "Train Epoch: 390 [16405/17352 (95%)] Loss: -185024.515625\n",
      "Train Epoch: 390 [17163/17352 (99%)] Loss: -316375.687500\n",
      "    epoch          : 390\n",
      "    loss           : -384766.2891935822\n",
      "    val_loss       : -207827.96411132812\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch390.pth ...\n",
      "Train Epoch: 391 [128/17352 (1%)] Loss: -264901.218750\n",
      "Train Epoch: 391 [1536/17352 (9%)] Loss: -436249.031250\n",
      "Train Epoch: 391 [2944/17352 (17%)] Loss: -312489.406250\n",
      "Train Epoch: 391 [4352/17352 (25%)] Loss: -476095.625000\n",
      "Train Epoch: 391 [5760/17352 (33%)] Loss: -437890.750000\n",
      "Train Epoch: 391 [7168/17352 (41%)] Loss: -447172.375000\n",
      "Train Epoch: 391 [8576/17352 (49%)] Loss: -405463.500000\n",
      "Train Epoch: 391 [9984/17352 (58%)] Loss: -458766.500000\n",
      "Train Epoch: 391 [11392/17352 (66%)] Loss: -419133.593750\n",
      "Train Epoch: 391 [12800/17352 (74%)] Loss: -465848.531250\n",
      "Train Epoch: 391 [14208/17352 (82%)] Loss: -376301.531250\n",
      "Train Epoch: 391 [15489/17352 (89%)] Loss: -241151.140625\n",
      "Train Epoch: 391 [16421/17352 (95%)] Loss: -281969.125000\n",
      "Train Epoch: 391 [17160/17352 (99%)] Loss: -251774.640625\n",
      "    epoch          : 391\n",
      "    loss           : -383182.361446099\n",
      "    val_loss       : -218863.53904622394\n",
      "Train Epoch: 392 [128/17352 (1%)] Loss: -380695.031250\n",
      "Train Epoch: 392 [1536/17352 (9%)] Loss: -447820.500000\n",
      "Train Epoch: 392 [2944/17352 (17%)] Loss: -409028.281250\n",
      "Train Epoch: 392 [4352/17352 (25%)] Loss: -396947.843750\n",
      "Train Epoch: 392 [5760/17352 (33%)] Loss: -380438.750000\n",
      "Train Epoch: 392 [7168/17352 (41%)] Loss: -163624.125000\n",
      "Train Epoch: 392 [8576/17352 (49%)] Loss: -392184.031250\n",
      "Train Epoch: 392 [9984/17352 (58%)] Loss: -427388.468750\n",
      "Train Epoch: 392 [11392/17352 (66%)] Loss: -437123.343750\n",
      "Train Epoch: 392 [12800/17352 (74%)] Loss: -370693.000000\n",
      "Train Epoch: 392 [14208/17352 (82%)] Loss: -382451.218750\n",
      "Train Epoch: 392 [15533/17352 (90%)] Loss: -235660.890625\n",
      "Train Epoch: 392 [16365/17352 (94%)] Loss: -62443.695312\n",
      "Train Epoch: 392 [17033/17352 (98%)] Loss: -59394.886719\n",
      "    epoch          : 392\n",
      "    loss           : -382204.07890166214\n",
      "    val_loss       : -215509.61905924478\n",
      "Train Epoch: 393 [128/17352 (1%)] Loss: -454767.218750\n",
      "Train Epoch: 393 [1536/17352 (9%)] Loss: -438649.906250\n",
      "Train Epoch: 393 [2944/17352 (17%)] Loss: -331578.500000\n",
      "Train Epoch: 393 [4352/17352 (25%)] Loss: -470523.031250\n",
      "Train Epoch: 393 [5760/17352 (33%)] Loss: -383088.250000\n",
      "Train Epoch: 393 [7168/17352 (41%)] Loss: -388110.187500\n",
      "Train Epoch: 393 [8576/17352 (49%)] Loss: -346222.250000\n",
      "Train Epoch: 393 [9984/17352 (58%)] Loss: -392878.375000\n",
      "Train Epoch: 393 [11392/17352 (66%)] Loss: -437833.437500\n",
      "Train Epoch: 393 [12800/17352 (74%)] Loss: -443369.562500\n",
      "Train Epoch: 393 [14208/17352 (82%)] Loss: -406078.531250\n",
      "Train Epoch: 393 [15560/17352 (90%)] Loss: -261771.218750\n",
      "Train Epoch: 393 [16308/17352 (94%)] Loss: -387529.812500\n",
      "Train Epoch: 393 [17009/17352 (98%)] Loss: -153107.328125\n",
      "    epoch          : 393\n",
      "    loss           : -378768.92966128356\n",
      "    val_loss       : -210796.66108398436\n",
      "Train Epoch: 394 [128/17352 (1%)] Loss: -404289.187500\n",
      "Train Epoch: 394 [1536/17352 (9%)] Loss: -431182.187500\n",
      "Train Epoch: 394 [2944/17352 (17%)] Loss: -433956.937500\n",
      "Train Epoch: 394 [4352/17352 (25%)] Loss: -441019.500000\n",
      "Train Epoch: 394 [5760/17352 (33%)] Loss: -309854.406250\n",
      "Train Epoch: 394 [7168/17352 (41%)] Loss: -426715.593750\n",
      "Train Epoch: 394 [8576/17352 (49%)] Loss: -464168.968750\n",
      "Train Epoch: 394 [9984/17352 (58%)] Loss: -442901.125000\n",
      "Train Epoch: 394 [11392/17352 (66%)] Loss: -471810.062500\n",
      "Train Epoch: 394 [12800/17352 (74%)] Loss: -467589.000000\n",
      "Train Epoch: 394 [14208/17352 (82%)] Loss: -445286.218750\n",
      "Train Epoch: 394 [15560/17352 (90%)] Loss: -218415.562500\n",
      "Train Epoch: 394 [16233/17352 (94%)] Loss: -15843.959961\n",
      "Train Epoch: 394 [17045/17352 (98%)] Loss: -282346.375000\n",
      "    epoch          : 394\n",
      "    loss           : -392969.1865299916\n",
      "    val_loss       : -216324.907421875\n",
      "Train Epoch: 395 [128/17352 (1%)] Loss: -448084.625000\n",
      "Train Epoch: 395 [1536/17352 (9%)] Loss: -469919.781250\n",
      "Train Epoch: 395 [2944/17352 (17%)] Loss: -473941.625000\n",
      "Train Epoch: 395 [4352/17352 (25%)] Loss: -462696.937500\n",
      "Train Epoch: 395 [5760/17352 (33%)] Loss: -479780.031250\n",
      "Train Epoch: 395 [7168/17352 (41%)] Loss: -439431.781250\n",
      "Train Epoch: 395 [8576/17352 (49%)] Loss: -460428.125000\n",
      "Train Epoch: 395 [9984/17352 (58%)] Loss: -417495.125000\n",
      "Train Epoch: 395 [11392/17352 (66%)] Loss: -425434.312500\n",
      "Train Epoch: 395 [12800/17352 (74%)] Loss: -355919.968750\n",
      "Train Epoch: 395 [14208/17352 (82%)] Loss: -435682.406250\n",
      "Train Epoch: 395 [15560/17352 (90%)] Loss: -423127.375000\n",
      "Train Epoch: 395 [16146/17352 (93%)] Loss: -118893.671875\n",
      "Train Epoch: 395 [16972/17352 (98%)] Loss: -270407.968750\n",
      "    epoch          : 395\n",
      "    loss           : -395142.6953649329\n",
      "    val_loss       : -218095.52351888022\n",
      "Train Epoch: 396 [128/17352 (1%)] Loss: -468312.968750\n",
      "Train Epoch: 396 [1536/17352 (9%)] Loss: -399838.375000\n",
      "Train Epoch: 396 [2944/17352 (17%)] Loss: -469087.312500\n",
      "Train Epoch: 396 [4352/17352 (25%)] Loss: -390552.500000\n",
      "Train Epoch: 396 [5760/17352 (33%)] Loss: -467532.625000\n",
      "Train Epoch: 396 [7168/17352 (41%)] Loss: -443979.250000\n",
      "Train Epoch: 396 [8576/17352 (49%)] Loss: -462656.125000\n",
      "Train Epoch: 396 [9984/17352 (58%)] Loss: -403460.812500\n",
      "Train Epoch: 396 [11392/17352 (66%)] Loss: -461996.875000\n",
      "Train Epoch: 396 [12800/17352 (74%)] Loss: -436297.937500\n",
      "Train Epoch: 396 [14208/17352 (82%)] Loss: -340217.875000\n",
      "Train Epoch: 396 [15414/17352 (89%)] Loss: -154150.921875\n",
      "Train Epoch: 396 [16319/17352 (94%)] Loss: -280199.937500\n",
      "Train Epoch: 396 [16994/17352 (98%)] Loss: -150957.015625\n",
      "    epoch          : 396\n",
      "    loss           : -393553.53427799913\n",
      "    val_loss       : -221665.21018880207\n",
      "Train Epoch: 397 [128/17352 (1%)] Loss: -440197.531250\n",
      "Train Epoch: 397 [1536/17352 (9%)] Loss: -378314.062500\n",
      "Train Epoch: 397 [2944/17352 (17%)] Loss: -479202.750000\n",
      "Train Epoch: 397 [4352/17352 (25%)] Loss: -481494.468750\n",
      "Train Epoch: 397 [5760/17352 (33%)] Loss: -448699.937500\n",
      "Train Epoch: 397 [7168/17352 (41%)] Loss: -349805.468750\n",
      "Train Epoch: 397 [8576/17352 (49%)] Loss: -464485.312500\n",
      "Train Epoch: 397 [9984/17352 (58%)] Loss: -471979.250000\n",
      "Train Epoch: 397 [11392/17352 (66%)] Loss: -218574.984375\n",
      "Train Epoch: 397 [12800/17352 (74%)] Loss: -414316.437500\n",
      "Train Epoch: 397 [14208/17352 (82%)] Loss: -293516.625000\n",
      "Train Epoch: 397 [15448/17352 (89%)] Loss: -260234.828125\n",
      "Train Epoch: 397 [16366/17352 (94%)] Loss: -326839.687500\n",
      "Train Epoch: 397 [17005/17352 (98%)] Loss: -12157.604492\n",
      "    epoch          : 397\n",
      "    loss           : -393590.1257996015\n",
      "    val_loss       : -221795.89848632814\n",
      "Train Epoch: 398 [128/17352 (1%)] Loss: -459583.281250\n",
      "Train Epoch: 398 [1536/17352 (9%)] Loss: -468258.125000\n",
      "Train Epoch: 398 [2944/17352 (17%)] Loss: -331682.843750\n",
      "Train Epoch: 398 [4352/17352 (25%)] Loss: -323639.031250\n",
      "Train Epoch: 398 [5760/17352 (33%)] Loss: -478887.375000\n",
      "Train Epoch: 398 [7168/17352 (41%)] Loss: -469109.406250\n",
      "Train Epoch: 398 [8576/17352 (49%)] Loss: -415222.562500\n",
      "Train Epoch: 398 [9984/17352 (58%)] Loss: -472246.000000\n",
      "Train Epoch: 398 [11392/17352 (66%)] Loss: -441241.062500\n",
      "Train Epoch: 398 [12800/17352 (74%)] Loss: -481347.187500\n",
      "Train Epoch: 398 [14208/17352 (82%)] Loss: -317480.312500\n",
      "Train Epoch: 398 [15551/17352 (90%)] Loss: -191418.125000\n",
      "Train Epoch: 398 [16137/17352 (93%)] Loss: -154737.281250\n",
      "Train Epoch: 398 [16984/17352 (98%)] Loss: -192342.421875\n",
      "    epoch          : 398\n",
      "    loss           : -395277.0666946309\n",
      "    val_loss       : -219939.3639485677\n",
      "Train Epoch: 399 [128/17352 (1%)] Loss: -381548.062500\n",
      "Train Epoch: 399 [1536/17352 (9%)] Loss: -412793.281250\n",
      "Train Epoch: 399 [2944/17352 (17%)] Loss: -472879.812500\n",
      "Train Epoch: 399 [4352/17352 (25%)] Loss: -465906.000000\n",
      "Train Epoch: 399 [5760/17352 (33%)] Loss: -465067.531250\n",
      "Train Epoch: 399 [7168/17352 (41%)] Loss: -289368.656250\n",
      "Train Epoch: 399 [8576/17352 (49%)] Loss: -344832.875000\n",
      "Train Epoch: 399 [9984/17352 (58%)] Loss: -446998.125000\n",
      "Train Epoch: 399 [11392/17352 (66%)] Loss: -454671.500000\n",
      "Train Epoch: 399 [12800/17352 (74%)] Loss: -368206.312500\n",
      "Train Epoch: 399 [14208/17352 (82%)] Loss: -449235.468750\n",
      "Train Epoch: 399 [15513/17352 (89%)] Loss: -243590.921875\n",
      "Train Epoch: 399 [16230/17352 (94%)] Loss: -51526.406250\n",
      "Train Epoch: 399 [16916/17352 (97%)] Loss: -9235.412109\n",
      "    epoch          : 399\n",
      "    loss           : -401969.73742921563\n",
      "    val_loss       : -212559.58595377606\n",
      "Train Epoch: 400 [128/17352 (1%)] Loss: -353076.062500\n",
      "Train Epoch: 400 [1536/17352 (9%)] Loss: -455959.062500\n",
      "Train Epoch: 400 [2944/17352 (17%)] Loss: -460599.187500\n",
      "Train Epoch: 400 [4352/17352 (25%)] Loss: -418746.312500\n",
      "Train Epoch: 400 [5760/17352 (33%)] Loss: -417170.031250\n",
      "Train Epoch: 400 [7168/17352 (41%)] Loss: -492763.468750\n",
      "Train Epoch: 400 [8576/17352 (49%)] Loss: -472603.593750\n",
      "Train Epoch: 400 [9984/17352 (58%)] Loss: -464227.250000\n",
      "Train Epoch: 400 [11392/17352 (66%)] Loss: -435897.125000\n",
      "Train Epoch: 400 [12800/17352 (74%)] Loss: -392712.375000\n",
      "Train Epoch: 400 [14208/17352 (82%)] Loss: -444212.843750\n",
      "Train Epoch: 400 [15572/17352 (90%)] Loss: -430317.281250\n",
      "Train Epoch: 400 [16386/17352 (94%)] Loss: -419997.906250\n",
      "Train Epoch: 400 [17050/17352 (98%)] Loss: -44743.273438\n",
      "    epoch          : 400\n",
      "    loss           : -404334.6724124371\n",
      "    val_loss       : -226059.65211588543\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch400.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 401 [128/17352 (1%)] Loss: -414691.250000\n",
      "Train Epoch: 401 [1536/17352 (9%)] Loss: -408458.937500\n",
      "Train Epoch: 401 [2944/17352 (17%)] Loss: -457880.000000\n",
      "Train Epoch: 401 [4352/17352 (25%)] Loss: -464388.687500\n",
      "Train Epoch: 401 [5760/17352 (33%)] Loss: -432364.750000\n",
      "Train Epoch: 401 [7168/17352 (41%)] Loss: -339397.718750\n",
      "Train Epoch: 401 [8576/17352 (49%)] Loss: -397192.187500\n",
      "Train Epoch: 401 [9984/17352 (58%)] Loss: -459153.031250\n",
      "Train Epoch: 401 [11392/17352 (66%)] Loss: -442333.593750\n",
      "Train Epoch: 401 [12800/17352 (74%)] Loss: -431954.531250\n",
      "Train Epoch: 401 [14208/17352 (82%)] Loss: -434110.625000\n",
      "Train Epoch: 401 [15576/17352 (90%)] Loss: -432107.031250\n",
      "Train Epoch: 401 [16272/17352 (94%)] Loss: -170188.218750\n",
      "Train Epoch: 401 [17092/17352 (99%)] Loss: -251400.078125\n",
      "    epoch          : 401\n",
      "    loss           : -395079.01101746014\n",
      "    val_loss       : -221842.92513020834\n",
      "Train Epoch: 402 [128/17352 (1%)] Loss: -426620.062500\n",
      "Train Epoch: 402 [1536/17352 (9%)] Loss: -441332.250000\n",
      "Train Epoch: 402 [2944/17352 (17%)] Loss: -466733.375000\n",
      "Train Epoch: 402 [4352/17352 (25%)] Loss: -483656.093750\n",
      "Train Epoch: 402 [5760/17352 (33%)] Loss: -485805.375000\n",
      "Train Epoch: 402 [7168/17352 (41%)] Loss: -445133.625000\n",
      "Train Epoch: 402 [8576/17352 (49%)] Loss: -483043.718750\n",
      "Train Epoch: 402 [9984/17352 (58%)] Loss: -481734.062500\n",
      "Train Epoch: 402 [11392/17352 (66%)] Loss: -440904.156250\n",
      "Train Epoch: 402 [12800/17352 (74%)] Loss: -465106.031250\n",
      "Train Epoch: 402 [14208/17352 (82%)] Loss: -415137.843750\n",
      "Train Epoch: 402 [15508/17352 (89%)] Loss: -249890.468750\n",
      "Train Epoch: 402 [16194/17352 (93%)] Loss: -225012.531250\n",
      "Train Epoch: 402 [16942/17352 (98%)] Loss: -101034.843750\n",
      "    epoch          : 402\n",
      "    loss           : -394036.07301934774\n",
      "    val_loss       : -214676.9349609375\n",
      "Train Epoch: 403 [128/17352 (1%)] Loss: -419032.156250\n",
      "Train Epoch: 403 [1536/17352 (9%)] Loss: -425905.312500\n",
      "Train Epoch: 403 [2944/17352 (17%)] Loss: -489793.968750\n",
      "Train Epoch: 403 [4352/17352 (25%)] Loss: -381112.937500\n",
      "Train Epoch: 403 [5760/17352 (33%)] Loss: -467892.906250\n",
      "Train Epoch: 403 [7168/17352 (41%)] Loss: -427117.125000\n",
      "Train Epoch: 403 [8576/17352 (49%)] Loss: -452687.375000\n",
      "Train Epoch: 403 [9984/17352 (58%)] Loss: -451409.531250\n",
      "Train Epoch: 403 [11392/17352 (66%)] Loss: -458614.750000\n",
      "Train Epoch: 403 [12800/17352 (74%)] Loss: -471547.187500\n",
      "Train Epoch: 403 [14208/17352 (82%)] Loss: -398370.125000\n",
      "Train Epoch: 403 [15570/17352 (90%)] Loss: -372154.750000\n",
      "Train Epoch: 403 [16184/17352 (93%)] Loss: -152492.796875\n",
      "Train Epoch: 403 [17097/17352 (99%)] Loss: -415931.812500\n",
      "    epoch          : 403\n",
      "    loss           : -395140.3913000734\n",
      "    val_loss       : -211724.5026204427\n",
      "Train Epoch: 404 [128/17352 (1%)] Loss: -417803.750000\n",
      "Train Epoch: 404 [1536/17352 (9%)] Loss: -431423.468750\n",
      "Train Epoch: 404 [2944/17352 (17%)] Loss: -463201.781250\n",
      "Train Epoch: 404 [4352/17352 (25%)] Loss: -442796.187500\n",
      "Train Epoch: 404 [5760/17352 (33%)] Loss: -441974.187500\n",
      "Train Epoch: 404 [7168/17352 (41%)] Loss: -477106.250000\n",
      "Train Epoch: 404 [8576/17352 (49%)] Loss: -433231.437500\n",
      "Train Epoch: 404 [9984/17352 (58%)] Loss: -466447.156250\n",
      "Train Epoch: 404 [11392/17352 (66%)] Loss: -331176.593750\n",
      "Train Epoch: 404 [12800/17352 (74%)] Loss: -440747.843750\n",
      "Train Epoch: 404 [14208/17352 (82%)] Loss: -426146.406250\n",
      "Train Epoch: 404 [15534/17352 (90%)] Loss: -325190.812500\n",
      "Train Epoch: 404 [16211/17352 (93%)] Loss: -230636.203125\n",
      "Train Epoch: 404 [17094/17352 (99%)] Loss: -181916.890625\n",
      "    epoch          : 404\n",
      "    loss           : -389667.7544240247\n",
      "    val_loss       : -209027.00366210938\n",
      "Train Epoch: 405 [128/17352 (1%)] Loss: -431523.375000\n",
      "Train Epoch: 405 [1536/17352 (9%)] Loss: -430248.500000\n",
      "Train Epoch: 405 [2944/17352 (17%)] Loss: -456173.281250\n",
      "Train Epoch: 405 [4352/17352 (25%)] Loss: -457443.687500\n",
      "Train Epoch: 405 [5760/17352 (33%)] Loss: -441232.062500\n",
      "Train Epoch: 405 [7168/17352 (41%)] Loss: -444327.593750\n",
      "Train Epoch: 405 [8576/17352 (49%)] Loss: -463004.562500\n",
      "Train Epoch: 405 [9984/17352 (58%)] Loss: -498690.687500\n",
      "Train Epoch: 405 [11392/17352 (66%)] Loss: -462135.687500\n",
      "Train Epoch: 405 [12800/17352 (74%)] Loss: -450053.156250\n",
      "Train Epoch: 405 [14208/17352 (82%)] Loss: -437330.437500\n",
      "Train Epoch: 405 [15541/17352 (90%)] Loss: -203310.546875\n",
      "Train Epoch: 405 [16310/17352 (94%)] Loss: -392353.812500\n",
      "Train Epoch: 405 [17095/17352 (99%)] Loss: -146974.906250\n",
      "    epoch          : 405\n",
      "    loss           : -394292.50163852767\n",
      "    val_loss       : -217779.21053059897\n",
      "Train Epoch: 406 [128/17352 (1%)] Loss: -469314.625000\n",
      "Train Epoch: 406 [1536/17352 (9%)] Loss: -462881.312500\n",
      "Train Epoch: 406 [2944/17352 (17%)] Loss: -462335.187500\n",
      "Train Epoch: 406 [4352/17352 (25%)] Loss: -449109.312500\n",
      "Train Epoch: 406 [5760/17352 (33%)] Loss: -474251.937500\n",
      "Train Epoch: 406 [7168/17352 (41%)] Loss: -459349.531250\n",
      "Train Epoch: 406 [8576/17352 (49%)] Loss: -430141.125000\n",
      "Train Epoch: 406 [9984/17352 (58%)] Loss: -432188.718750\n",
      "Train Epoch: 406 [11392/17352 (66%)] Loss: -441827.718750\n",
      "Train Epoch: 406 [12800/17352 (74%)] Loss: -345065.406250\n",
      "Train Epoch: 406 [14208/17352 (82%)] Loss: -462566.281250\n",
      "Train Epoch: 406 [15508/17352 (89%)] Loss: -155025.265625\n",
      "Train Epoch: 406 [16109/17352 (93%)] Loss: -8637.052734\n",
      "Train Epoch: 406 [17000/17352 (98%)] Loss: -308363.000000\n",
      "    epoch          : 406\n",
      "    loss           : -397473.8715787542\n",
      "    val_loss       : -220458.24825846354\n",
      "Train Epoch: 407 [128/17352 (1%)] Loss: -411653.500000\n",
      "Train Epoch: 407 [1536/17352 (9%)] Loss: -399777.843750\n",
      "Train Epoch: 407 [2944/17352 (17%)] Loss: -468308.250000\n",
      "Train Epoch: 407 [4352/17352 (25%)] Loss: -388015.531250\n",
      "Train Epoch: 407 [5760/17352 (33%)] Loss: -456944.500000\n",
      "Train Epoch: 407 [7168/17352 (41%)] Loss: -441373.718750\n",
      "Train Epoch: 407 [8576/17352 (49%)] Loss: -471457.562500\n",
      "Train Epoch: 407 [9984/17352 (58%)] Loss: -459716.812500\n",
      "Train Epoch: 407 [11392/17352 (66%)] Loss: -362284.625000\n",
      "Train Epoch: 407 [12800/17352 (74%)] Loss: -484781.406250\n",
      "Train Epoch: 407 [14208/17352 (82%)] Loss: -340981.406250\n",
      "Train Epoch: 407 [15455/17352 (89%)] Loss: -334534.218750\n",
      "Train Epoch: 407 [16291/17352 (94%)] Loss: -396573.125000\n",
      "Train Epoch: 407 [17173/17352 (99%)] Loss: -283255.406250\n",
      "    epoch          : 407\n",
      "    loss           : -399924.1080510696\n",
      "    val_loss       : -218283.8901204427\n",
      "Train Epoch: 408 [128/17352 (1%)] Loss: -378530.687500\n",
      "Train Epoch: 408 [1536/17352 (9%)] Loss: -425678.156250\n",
      "Train Epoch: 408 [2944/17352 (17%)] Loss: -483626.031250\n",
      "Train Epoch: 408 [4352/17352 (25%)] Loss: -425980.125000\n",
      "Train Epoch: 408 [5760/17352 (33%)] Loss: -486000.906250\n",
      "Train Epoch: 408 [7168/17352 (41%)] Loss: -384998.093750\n",
      "Train Epoch: 408 [8576/17352 (49%)] Loss: -449389.187500\n",
      "Train Epoch: 408 [9984/17352 (58%)] Loss: -446801.125000\n",
      "Train Epoch: 408 [11392/17352 (66%)] Loss: -424189.937500\n",
      "Train Epoch: 408 [12800/17352 (74%)] Loss: -482528.875000\n",
      "Train Epoch: 408 [14208/17352 (82%)] Loss: -439545.187500\n",
      "Train Epoch: 408 [15499/17352 (89%)] Loss: -124103.843750\n",
      "Train Epoch: 408 [16401/17352 (95%)] Loss: -425295.187500\n",
      "Train Epoch: 408 [17164/17352 (99%)] Loss: -257465.437500\n",
      "    epoch          : 408\n",
      "    loss           : -393768.6657075818\n",
      "    val_loss       : -218852.05864257814\n",
      "Train Epoch: 409 [128/17352 (1%)] Loss: -483548.062500\n",
      "Train Epoch: 409 [1536/17352 (9%)] Loss: -456350.781250\n",
      "Train Epoch: 409 [2944/17352 (17%)] Loss: -479768.312500\n",
      "Train Epoch: 409 [4352/17352 (25%)] Loss: -474714.468750\n",
      "Train Epoch: 409 [5760/17352 (33%)] Loss: -397044.343750\n",
      "Train Epoch: 409 [7168/17352 (41%)] Loss: -459292.218750\n",
      "Train Epoch: 409 [8576/17352 (49%)] Loss: -490556.187500\n",
      "Train Epoch: 409 [9984/17352 (58%)] Loss: -444942.250000\n",
      "Train Epoch: 409 [11392/17352 (66%)] Loss: -450315.562500\n",
      "Train Epoch: 409 [12800/17352 (74%)] Loss: -385295.437500\n",
      "Train Epoch: 409 [14208/17352 (82%)] Loss: -474664.218750\n",
      "Train Epoch: 409 [15523/17352 (89%)] Loss: -256401.109375\n",
      "Train Epoch: 409 [16250/17352 (94%)] Loss: -51498.660156\n",
      "Train Epoch: 409 [17022/17352 (98%)] Loss: -190350.875000\n",
      "    epoch          : 409\n",
      "    loss           : -395089.4527972945\n",
      "    val_loss       : -220944.14456380208\n",
      "Train Epoch: 410 [128/17352 (1%)] Loss: -479299.875000\n",
      "Train Epoch: 410 [1536/17352 (9%)] Loss: -432907.406250\n",
      "Train Epoch: 410 [2944/17352 (17%)] Loss: -440553.000000\n",
      "Train Epoch: 410 [4352/17352 (25%)] Loss: -459801.468750\n",
      "Train Epoch: 410 [5760/17352 (33%)] Loss: -464080.312500\n",
      "Train Epoch: 410 [7168/17352 (41%)] Loss: -471979.812500\n",
      "Train Epoch: 410 [8576/17352 (49%)] Loss: -451558.812500\n",
      "Train Epoch: 410 [9984/17352 (58%)] Loss: -479755.000000\n",
      "Train Epoch: 410 [11392/17352 (66%)] Loss: -439352.437500\n",
      "Train Epoch: 410 [12800/17352 (74%)] Loss: -482589.437500\n",
      "Train Epoch: 410 [14208/17352 (82%)] Loss: -482466.375000\n",
      "Train Epoch: 410 [15406/17352 (89%)] Loss: -11580.541016\n",
      "Train Epoch: 410 [16214/17352 (93%)] Loss: -257777.859375\n",
      "Train Epoch: 410 [16998/17352 (98%)] Loss: -338871.937500\n",
      "    epoch          : 410\n",
      "    loss           : -399209.4997116191\n",
      "    val_loss       : -226945.42972005208\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch410.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 411 [128/17352 (1%)] Loss: -448922.250000\n",
      "Train Epoch: 411 [1536/17352 (9%)] Loss: -406069.062500\n",
      "Train Epoch: 411 [2944/17352 (17%)] Loss: -470227.250000\n",
      "Train Epoch: 411 [4352/17352 (25%)] Loss: -417006.312500\n",
      "Train Epoch: 411 [5760/17352 (33%)] Loss: -468612.625000\n",
      "Train Epoch: 411 [7168/17352 (41%)] Loss: -447660.812500\n",
      "Train Epoch: 411 [8576/17352 (49%)] Loss: -470617.000000\n",
      "Train Epoch: 411 [9984/17352 (58%)] Loss: -473294.937500\n",
      "Train Epoch: 411 [11392/17352 (66%)] Loss: -285365.156250\n",
      "Train Epoch: 411 [12800/17352 (74%)] Loss: -423092.000000\n",
      "Train Epoch: 411 [14208/17352 (82%)] Loss: -450506.750000\n",
      "Train Epoch: 411 [15515/17352 (89%)] Loss: -207543.171875\n",
      "Train Epoch: 411 [16360/17352 (94%)] Loss: -44308.585938\n",
      "Train Epoch: 411 [17029/17352 (98%)] Loss: -149119.125000\n",
      "    epoch          : 411\n",
      "    loss           : -403449.988045302\n",
      "    val_loss       : -216285.25177408854\n",
      "Train Epoch: 412 [128/17352 (1%)] Loss: -448311.437500\n",
      "Train Epoch: 412 [1536/17352 (9%)] Loss: -349492.500000\n",
      "Train Epoch: 412 [2944/17352 (17%)] Loss: -426253.531250\n",
      "Train Epoch: 412 [4352/17352 (25%)] Loss: -448456.968750\n",
      "Train Epoch: 412 [5760/17352 (33%)] Loss: -439395.593750\n",
      "Train Epoch: 412 [7168/17352 (41%)] Loss: -449969.562500\n",
      "Train Epoch: 412 [8576/17352 (49%)] Loss: -465292.468750\n",
      "Train Epoch: 412 [9984/17352 (58%)] Loss: -309195.531250\n",
      "Train Epoch: 412 [11392/17352 (66%)] Loss: -448995.968750\n",
      "Train Epoch: 412 [12800/17352 (74%)] Loss: -482494.187500\n",
      "Train Epoch: 412 [14208/17352 (82%)] Loss: -471708.812500\n",
      "Train Epoch: 412 [15500/17352 (89%)] Loss: -204963.062500\n",
      "Train Epoch: 412 [16087/17352 (93%)] Loss: -7270.573730\n",
      "Train Epoch: 412 [16856/17352 (97%)] Loss: -309130.500000\n",
      "    epoch          : 412\n",
      "    loss           : -396488.8784835099\n",
      "    val_loss       : -221412.38805338542\n",
      "Train Epoch: 413 [128/17352 (1%)] Loss: -438589.750000\n",
      "Train Epoch: 413 [1536/17352 (9%)] Loss: -482897.218750\n",
      "Train Epoch: 413 [2944/17352 (17%)] Loss: -364232.000000\n",
      "Train Epoch: 413 [4352/17352 (25%)] Loss: -454934.937500\n",
      "Train Epoch: 413 [5760/17352 (33%)] Loss: -480399.625000\n",
      "Train Epoch: 413 [7168/17352 (41%)] Loss: -459678.062500\n",
      "Train Epoch: 413 [8576/17352 (49%)] Loss: -384697.531250\n",
      "Train Epoch: 413 [9984/17352 (58%)] Loss: -493299.843750\n",
      "Train Epoch: 413 [11392/17352 (66%)] Loss: -437189.312500\n",
      "Train Epoch: 413 [12800/17352 (74%)] Loss: -340104.437500\n",
      "Train Epoch: 413 [14208/17352 (82%)] Loss: -404594.875000\n",
      "Train Epoch: 413 [15510/17352 (89%)] Loss: -195433.609375\n",
      "Train Epoch: 413 [16197/17352 (93%)] Loss: -10829.085938\n",
      "Train Epoch: 413 [17012/17352 (98%)] Loss: -370194.000000\n",
      "    epoch          : 413\n",
      "    loss           : -402668.2150272651\n",
      "    val_loss       : -228454.4144856771\n",
      "Train Epoch: 414 [128/17352 (1%)] Loss: -435509.500000\n",
      "Train Epoch: 414 [1536/17352 (9%)] Loss: -443316.156250\n",
      "Train Epoch: 414 [2944/17352 (17%)] Loss: -452487.812500\n",
      "Train Epoch: 414 [4352/17352 (25%)] Loss: -454293.718750\n",
      "Train Epoch: 414 [5760/17352 (33%)] Loss: -485888.375000\n",
      "Train Epoch: 414 [7168/17352 (41%)] Loss: -287516.031250\n",
      "Train Epoch: 414 [8576/17352 (49%)] Loss: -471946.656250\n",
      "Train Epoch: 414 [9984/17352 (58%)] Loss: -399814.750000\n",
      "Train Epoch: 414 [11392/17352 (66%)] Loss: -454542.875000\n",
      "Train Epoch: 414 [12800/17352 (74%)] Loss: -473044.812500\n",
      "Train Epoch: 414 [14208/17352 (82%)] Loss: -393972.906250\n",
      "Train Epoch: 414 [15522/17352 (89%)] Loss: -328934.375000\n",
      "Train Epoch: 414 [16275/17352 (94%)] Loss: -321108.531250\n",
      "Train Epoch: 414 [17085/17352 (98%)] Loss: -348838.375000\n",
      "    epoch          : 414\n",
      "    loss           : -399637.8008205747\n",
      "    val_loss       : -218313.5435221354\n",
      "Train Epoch: 415 [128/17352 (1%)] Loss: -430441.125000\n",
      "Train Epoch: 415 [1536/17352 (9%)] Loss: -449374.687500\n",
      "Train Epoch: 415 [2944/17352 (17%)] Loss: -496778.125000\n",
      "Train Epoch: 415 [4352/17352 (25%)] Loss: -400426.812500\n",
      "Train Epoch: 415 [5760/17352 (33%)] Loss: -470256.125000\n",
      "Train Epoch: 415 [7168/17352 (41%)] Loss: -426921.406250\n",
      "Train Epoch: 415 [8576/17352 (49%)] Loss: -456032.656250\n",
      "Train Epoch: 415 [9984/17352 (58%)] Loss: -405384.468750\n",
      "Train Epoch: 415 [11392/17352 (66%)] Loss: -428283.000000\n",
      "Train Epoch: 415 [12800/17352 (74%)] Loss: -349833.062500\n",
      "Train Epoch: 415 [14208/17352 (82%)] Loss: -420351.968750\n",
      "Train Epoch: 415 [15499/17352 (89%)] Loss: -106236.031250\n",
      "Train Epoch: 415 [16239/17352 (94%)] Loss: -298904.000000\n",
      "Train Epoch: 415 [16973/17352 (98%)] Loss: -252909.812500\n",
      "    epoch          : 415\n",
      "    loss           : -400372.19697069004\n",
      "    val_loss       : -221981.17976888022\n",
      "Train Epoch: 416 [128/17352 (1%)] Loss: -451744.875000\n",
      "Train Epoch: 416 [1536/17352 (9%)] Loss: -488379.906250\n",
      "Train Epoch: 416 [2944/17352 (17%)] Loss: -300276.750000\n",
      "Train Epoch: 416 [4352/17352 (25%)] Loss: -448354.906250\n",
      "Train Epoch: 416 [5760/17352 (33%)] Loss: -487957.937500\n",
      "Train Epoch: 416 [7168/17352 (41%)] Loss: -471260.906250\n",
      "Train Epoch: 416 [8576/17352 (49%)] Loss: -370450.250000\n",
      "Train Epoch: 416 [9984/17352 (58%)] Loss: -475881.875000\n",
      "Train Epoch: 416 [11392/17352 (66%)] Loss: -476975.468750\n",
      "Train Epoch: 416 [12800/17352 (74%)] Loss: -461865.968750\n",
      "Train Epoch: 416 [14208/17352 (82%)] Loss: -372518.312500\n",
      "Train Epoch: 416 [15487/17352 (89%)] Loss: -284776.937500\n",
      "Train Epoch: 416 [16352/17352 (94%)] Loss: -288465.062500\n",
      "Train Epoch: 416 [17098/17352 (99%)] Loss: -351553.093750\n",
      "    epoch          : 416\n",
      "    loss           : -400620.02021287754\n",
      "    val_loss       : -219557.3898111979\n",
      "Train Epoch: 417 [128/17352 (1%)] Loss: -480565.687500\n",
      "Train Epoch: 417 [1536/17352 (9%)] Loss: -481891.187500\n",
      "Train Epoch: 417 [2944/17352 (17%)] Loss: -475393.593750\n",
      "Train Epoch: 417 [4352/17352 (25%)] Loss: -448587.093750\n",
      "Train Epoch: 417 [5760/17352 (33%)] Loss: -471657.750000\n",
      "Train Epoch: 417 [7168/17352 (41%)] Loss: -446659.218750\n",
      "Train Epoch: 417 [8576/17352 (49%)] Loss: -422438.500000\n",
      "Train Epoch: 417 [9984/17352 (58%)] Loss: -494218.125000\n",
      "Train Epoch: 417 [11392/17352 (66%)] Loss: -429054.437500\n",
      "Train Epoch: 417 [12800/17352 (74%)] Loss: -466327.312500\n",
      "Train Epoch: 417 [14208/17352 (82%)] Loss: -411856.656250\n",
      "Train Epoch: 417 [15540/17352 (90%)] Loss: -359352.718750\n",
      "Train Epoch: 417 [16256/17352 (94%)] Loss: -95737.117188\n",
      "Train Epoch: 417 [17027/17352 (98%)] Loss: -382686.937500\n",
      "    epoch          : 417\n",
      "    loss           : -401905.57252778945\n",
      "    val_loss       : -214335.8041829427\n",
      "Train Epoch: 418 [128/17352 (1%)] Loss: -256053.781250\n",
      "Train Epoch: 418 [1536/17352 (9%)] Loss: -465608.062500\n",
      "Train Epoch: 418 [2944/17352 (17%)] Loss: -481248.656250\n",
      "Train Epoch: 418 [4352/17352 (25%)] Loss: -358387.156250\n",
      "Train Epoch: 418 [5760/17352 (33%)] Loss: -437413.218750\n",
      "Train Epoch: 418 [7168/17352 (41%)] Loss: -397662.687500\n",
      "Train Epoch: 418 [8576/17352 (49%)] Loss: -373552.250000\n",
      "Train Epoch: 418 [9984/17352 (58%)] Loss: -452746.062500\n",
      "Train Epoch: 418 [11392/17352 (66%)] Loss: -456072.625000\n",
      "Train Epoch: 418 [12800/17352 (74%)] Loss: -448968.937500\n",
      "Train Epoch: 418 [14208/17352 (82%)] Loss: -467093.968750\n",
      "Train Epoch: 418 [15503/17352 (89%)] Loss: -329513.812500\n",
      "Train Epoch: 418 [16287/17352 (94%)] Loss: -175363.937500\n",
      "Train Epoch: 418 [17050/17352 (98%)] Loss: -315577.000000\n",
      "    epoch          : 418\n",
      "    loss           : -396019.65916657925\n",
      "    val_loss       : -225973.79567057293\n",
      "Train Epoch: 419 [128/17352 (1%)] Loss: -449162.937500\n",
      "Train Epoch: 419 [1536/17352 (9%)] Loss: -455497.750000\n",
      "Train Epoch: 419 [2944/17352 (17%)] Loss: -466111.375000\n",
      "Train Epoch: 419 [4352/17352 (25%)] Loss: -417335.875000\n",
      "Train Epoch: 419 [5760/17352 (33%)] Loss: -474832.625000\n",
      "Train Epoch: 419 [7168/17352 (41%)] Loss: -449639.625000\n",
      "Train Epoch: 419 [8576/17352 (49%)] Loss: -467759.218750\n",
      "Train Epoch: 419 [9984/17352 (58%)] Loss: -486034.937500\n",
      "Train Epoch: 419 [11392/17352 (66%)] Loss: -359849.718750\n",
      "Train Epoch: 419 [12800/17352 (74%)] Loss: -468599.062500\n",
      "Train Epoch: 419 [14208/17352 (82%)] Loss: -470723.750000\n",
      "Train Epoch: 419 [15488/17352 (89%)] Loss: -288724.250000\n",
      "Train Epoch: 419 [16336/17352 (94%)] Loss: -292866.687500\n",
      "Train Epoch: 419 [16955/17352 (98%)] Loss: -16818.628906\n",
      "    epoch          : 419\n",
      "    loss           : -400369.9482225252\n",
      "    val_loss       : -219593.22164713542\n",
      "Train Epoch: 420 [128/17352 (1%)] Loss: -471420.531250\n",
      "Train Epoch: 420 [1536/17352 (9%)] Loss: -453875.281250\n",
      "Train Epoch: 420 [2944/17352 (17%)] Loss: -433101.312500\n",
      "Train Epoch: 420 [4352/17352 (25%)] Loss: -376722.812500\n",
      "Train Epoch: 420 [5760/17352 (33%)] Loss: -464414.312500\n",
      "Train Epoch: 420 [7168/17352 (41%)] Loss: -455061.937500\n",
      "Train Epoch: 420 [8576/17352 (49%)] Loss: -471537.156250\n",
      "Train Epoch: 420 [9984/17352 (58%)] Loss: -351871.937500\n",
      "Train Epoch: 420 [11392/17352 (66%)] Loss: -471342.812500\n",
      "Train Epoch: 420 [12800/17352 (74%)] Loss: -203659.875000\n",
      "Train Epoch: 420 [14208/17352 (82%)] Loss: -437505.312500\n",
      "Train Epoch: 420 [15511/17352 (89%)] Loss: -253848.593750\n",
      "Train Epoch: 420 [16215/17352 (93%)] Loss: -256436.500000\n",
      "Train Epoch: 420 [16892/17352 (97%)] Loss: -329795.500000\n",
      "    epoch          : 420\n",
      "    loss           : -394622.83181496436\n",
      "    val_loss       : -224043.02644856772\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch420.pth ...\n",
      "Train Epoch: 421 [128/17352 (1%)] Loss: -377292.000000\n",
      "Train Epoch: 421 [1536/17352 (9%)] Loss: -482449.812500\n",
      "Train Epoch: 421 [2944/17352 (17%)] Loss: -469546.156250\n",
      "Train Epoch: 421 [4352/17352 (25%)] Loss: -416026.718750\n",
      "Train Epoch: 421 [5760/17352 (33%)] Loss: -489082.937500\n",
      "Train Epoch: 421 [7168/17352 (41%)] Loss: -490380.156250\n",
      "Train Epoch: 421 [8576/17352 (49%)] Loss: -500845.250000\n",
      "Train Epoch: 421 [9984/17352 (58%)] Loss: -465818.750000\n",
      "Train Epoch: 421 [11392/17352 (66%)] Loss: -458534.250000\n",
      "Train Epoch: 421 [12800/17352 (74%)] Loss: -451383.812500\n",
      "Train Epoch: 421 [14208/17352 (82%)] Loss: -485315.781250\n",
      "Train Epoch: 421 [15571/17352 (90%)] Loss: -288863.187500\n",
      "Train Epoch: 421 [16408/17352 (95%)] Loss: -386254.250000\n",
      "Train Epoch: 421 [17063/17352 (98%)] Loss: -319804.593750\n",
      "    epoch          : 421\n",
      "    loss           : -401686.63267158665\n",
      "    val_loss       : -221174.95628255207\n",
      "Train Epoch: 422 [128/17352 (1%)] Loss: -499190.937500\n",
      "Train Epoch: 422 [1536/17352 (9%)] Loss: -423536.906250\n",
      "Train Epoch: 422 [2944/17352 (17%)] Loss: -462318.343750\n",
      "Train Epoch: 422 [4352/17352 (25%)] Loss: -451689.250000\n",
      "Train Epoch: 422 [5760/17352 (33%)] Loss: -486874.062500\n",
      "Train Epoch: 422 [7168/17352 (41%)] Loss: -459683.000000\n",
      "Train Epoch: 422 [8576/17352 (49%)] Loss: -449119.125000\n",
      "Train Epoch: 422 [9984/17352 (58%)] Loss: -443948.062500\n",
      "Train Epoch: 422 [11392/17352 (66%)] Loss: -389039.593750\n",
      "Train Epoch: 422 [12800/17352 (74%)] Loss: -481146.093750\n",
      "Train Epoch: 422 [14208/17352 (82%)] Loss: -471349.875000\n",
      "Train Epoch: 422 [15468/17352 (89%)] Loss: -9365.094727\n",
      "Train Epoch: 422 [16311/17352 (94%)] Loss: -142907.515625\n",
      "Train Epoch: 422 [17054/17352 (98%)] Loss: -227011.546875\n",
      "    epoch          : 422\n",
      "    loss           : -397658.9177852349\n",
      "    val_loss       : -219136.0104329427\n",
      "Train Epoch: 423 [128/17352 (1%)] Loss: -452969.687500\n",
      "Train Epoch: 423 [1536/17352 (9%)] Loss: -488548.750000\n",
      "Train Epoch: 423 [2944/17352 (17%)] Loss: -486867.937500\n",
      "Train Epoch: 423 [4352/17352 (25%)] Loss: -422093.187500\n",
      "Train Epoch: 423 [5760/17352 (33%)] Loss: -410491.718750\n",
      "Train Epoch: 423 [7168/17352 (41%)] Loss: -243404.250000\n",
      "Train Epoch: 423 [8576/17352 (49%)] Loss: -474243.718750\n",
      "Train Epoch: 423 [9984/17352 (58%)] Loss: -429665.937500\n",
      "Train Epoch: 423 [11392/17352 (66%)] Loss: -482202.656250\n",
      "Train Epoch: 423 [12800/17352 (74%)] Loss: -443180.687500\n",
      "Train Epoch: 423 [14208/17352 (82%)] Loss: -456948.500000\n",
      "Train Epoch: 423 [15476/17352 (89%)] Loss: -231495.703125\n",
      "Train Epoch: 423 [16325/17352 (94%)] Loss: -296214.687500\n",
      "Train Epoch: 423 [17143/17352 (99%)] Loss: -386993.437500\n",
      "    epoch          : 423\n",
      "    loss           : -402125.623164849\n",
      "    val_loss       : -225717.6467936198\n",
      "Train Epoch: 424 [128/17352 (1%)] Loss: -451832.281250\n",
      "Train Epoch: 424 [1536/17352 (9%)] Loss: -462702.250000\n",
      "Train Epoch: 424 [2944/17352 (17%)] Loss: -350583.000000\n",
      "Train Epoch: 424 [4352/17352 (25%)] Loss: -505424.187500\n",
      "Train Epoch: 424 [5760/17352 (33%)] Loss: -383019.656250\n",
      "Train Epoch: 424 [7168/17352 (41%)] Loss: -362693.312500\n",
      "Train Epoch: 424 [8576/17352 (49%)] Loss: -422762.500000\n",
      "Train Epoch: 424 [9984/17352 (58%)] Loss: -475359.687500\n",
      "Train Epoch: 424 [11392/17352 (66%)] Loss: -463241.562500\n",
      "Train Epoch: 424 [12800/17352 (74%)] Loss: -431702.250000\n",
      "Train Epoch: 424 [14208/17352 (82%)] Loss: -398271.406250\n",
      "Train Epoch: 424 [15440/17352 (89%)] Loss: -8149.114258\n",
      "Train Epoch: 424 [16339/17352 (94%)] Loss: -236101.859375\n",
      "Train Epoch: 424 [17107/17352 (99%)] Loss: -342542.312500\n",
      "    epoch          : 424\n",
      "    loss           : -402824.3151806313\n",
      "    val_loss       : -227388.74028320314\n",
      "Train Epoch: 425 [128/17352 (1%)] Loss: -447746.656250\n",
      "Train Epoch: 425 [1536/17352 (9%)] Loss: -466674.843750\n",
      "Train Epoch: 425 [2944/17352 (17%)] Loss: -405131.531250\n",
      "Train Epoch: 425 [4352/17352 (25%)] Loss: -368303.406250\n",
      "Train Epoch: 425 [5760/17352 (33%)] Loss: -455465.562500\n",
      "Train Epoch: 425 [7168/17352 (41%)] Loss: -458309.062500\n",
      "Train Epoch: 425 [8576/17352 (49%)] Loss: -467700.750000\n",
      "Train Epoch: 425 [9984/17352 (58%)] Loss: -395098.937500\n",
      "Train Epoch: 425 [11392/17352 (66%)] Loss: -394130.312500\n",
      "Train Epoch: 425 [12800/17352 (74%)] Loss: -474137.062500\n",
      "Train Epoch: 425 [14208/17352 (82%)] Loss: -460879.468750\n",
      "Train Epoch: 425 [15482/17352 (89%)] Loss: -108936.843750\n",
      "Train Epoch: 425 [16250/17352 (94%)] Loss: -323340.343750\n",
      "Train Epoch: 425 [17008/17352 (98%)] Loss: -324118.031250\n",
      "    epoch          : 425\n",
      "    loss           : -400281.59827889054\n",
      "    val_loss       : -224143.71521809895\n",
      "Train Epoch: 426 [128/17352 (1%)] Loss: -386341.625000\n",
      "Train Epoch: 426 [1536/17352 (9%)] Loss: -458829.218750\n",
      "Train Epoch: 426 [2944/17352 (17%)] Loss: -482407.000000\n",
      "Train Epoch: 426 [4352/17352 (25%)] Loss: -454651.906250\n",
      "Train Epoch: 426 [5760/17352 (33%)] Loss: -443992.437500\n",
      "Train Epoch: 426 [7168/17352 (41%)] Loss: -413499.562500\n",
      "Train Epoch: 426 [8576/17352 (49%)] Loss: -374068.375000\n",
      "Train Epoch: 426 [9984/17352 (58%)] Loss: -377357.187500\n",
      "Train Epoch: 426 [11392/17352 (66%)] Loss: -400822.281250\n",
      "Train Epoch: 426 [12800/17352 (74%)] Loss: -391338.875000\n",
      "Train Epoch: 426 [14208/17352 (82%)] Loss: -455904.125000\n",
      "Train Epoch: 426 [15397/17352 (89%)] Loss: -117740.585938\n",
      "Train Epoch: 426 [16306/17352 (94%)] Loss: -155731.953125\n",
      "Train Epoch: 426 [16896/17352 (97%)] Loss: -209624.265625\n",
      "    epoch          : 426\n",
      "    loss           : -400202.8326866611\n",
      "    val_loss       : -223348.20314127606\n",
      "Train Epoch: 427 [128/17352 (1%)] Loss: -395519.687500\n",
      "Train Epoch: 427 [1536/17352 (9%)] Loss: -472139.625000\n",
      "Train Epoch: 427 [2944/17352 (17%)] Loss: -477893.687500\n",
      "Train Epoch: 427 [4352/17352 (25%)] Loss: -457305.750000\n",
      "Train Epoch: 427 [5760/17352 (33%)] Loss: -469488.187500\n",
      "Train Epoch: 427 [7168/17352 (41%)] Loss: -504907.156250\n",
      "Train Epoch: 427 [8576/17352 (49%)] Loss: -443797.406250\n",
      "Train Epoch: 427 [9984/17352 (58%)] Loss: -481274.062500\n",
      "Train Epoch: 427 [11392/17352 (66%)] Loss: -443446.343750\n",
      "Train Epoch: 427 [12800/17352 (74%)] Loss: -469441.437500\n",
      "Train Epoch: 427 [14208/17352 (82%)] Loss: -440549.250000\n",
      "Train Epoch: 427 [15479/17352 (89%)] Loss: -121930.335938\n",
      "Train Epoch: 427 [16191/17352 (93%)] Loss: -297286.625000\n",
      "Train Epoch: 427 [17080/17352 (98%)] Loss: -350287.718750\n",
      "    epoch          : 427\n",
      "    loss           : -404747.3765926489\n",
      "    val_loss       : -225888.8700032552\n",
      "Train Epoch: 428 [128/17352 (1%)] Loss: -484295.437500\n",
      "Train Epoch: 428 [1536/17352 (9%)] Loss: -473322.468750\n",
      "Train Epoch: 428 [2944/17352 (17%)] Loss: -444314.562500\n",
      "Train Epoch: 428 [4352/17352 (25%)] Loss: -458865.343750\n",
      "Train Epoch: 428 [5760/17352 (33%)] Loss: -435715.093750\n",
      "Train Epoch: 428 [7168/17352 (41%)] Loss: -402700.437500\n",
      "Train Epoch: 428 [8576/17352 (49%)] Loss: -487843.437500\n",
      "Train Epoch: 428 [9984/17352 (58%)] Loss: -474364.093750\n",
      "Train Epoch: 428 [11392/17352 (66%)] Loss: -468699.125000\n",
      "Train Epoch: 428 [12800/17352 (74%)] Loss: -451913.625000\n",
      "Train Epoch: 428 [14208/17352 (82%)] Loss: -499112.562500\n",
      "Train Epoch: 428 [15487/17352 (89%)] Loss: -252410.468750\n",
      "Train Epoch: 428 [16277/17352 (94%)] Loss: -341013.406250\n",
      "Train Epoch: 428 [16919/17352 (98%)] Loss: -299482.062500\n",
      "    epoch          : 428\n",
      "    loss           : -402730.2530673238\n",
      "    val_loss       : -221315.97506510417\n",
      "Train Epoch: 429 [128/17352 (1%)] Loss: -394811.625000\n",
      "Train Epoch: 429 [1536/17352 (9%)] Loss: -469988.187500\n",
      "Train Epoch: 429 [2944/17352 (17%)] Loss: -458917.000000\n",
      "Train Epoch: 429 [4352/17352 (25%)] Loss: -439701.250000\n",
      "Train Epoch: 429 [5760/17352 (33%)] Loss: -438492.562500\n",
      "Train Epoch: 429 [7168/17352 (41%)] Loss: -461597.687500\n",
      "Train Epoch: 429 [8576/17352 (49%)] Loss: -472809.437500\n",
      "Train Epoch: 429 [9984/17352 (58%)] Loss: -455259.562500\n",
      "Train Epoch: 429 [11392/17352 (66%)] Loss: -350254.968750\n",
      "Train Epoch: 429 [12800/17352 (74%)] Loss: -485562.875000\n",
      "Train Epoch: 429 [14208/17352 (82%)] Loss: -454834.718750\n",
      "Train Epoch: 429 [15534/17352 (90%)] Loss: -264491.781250\n",
      "Train Epoch: 429 [16361/17352 (94%)] Loss: -194209.046875\n",
      "Train Epoch: 429 [17022/17352 (98%)] Loss: -236067.562500\n",
      "    epoch          : 429\n",
      "    loss           : -407907.83225408976\n",
      "    val_loss       : -225540.4849609375\n",
      "Train Epoch: 430 [128/17352 (1%)] Loss: -381639.750000\n",
      "Train Epoch: 430 [1536/17352 (9%)] Loss: -486794.125000\n",
      "Train Epoch: 430 [2944/17352 (17%)] Loss: -492864.062500\n",
      "Train Epoch: 430 [4352/17352 (25%)] Loss: -492682.156250\n",
      "Train Epoch: 430 [5760/17352 (33%)] Loss: -460089.875000\n",
      "Train Epoch: 430 [7168/17352 (41%)] Loss: -344529.000000\n",
      "Train Epoch: 430 [8576/17352 (49%)] Loss: -416073.906250\n",
      "Train Epoch: 430 [9984/17352 (58%)] Loss: -487128.562500\n",
      "Train Epoch: 430 [11392/17352 (66%)] Loss: -480094.843750\n",
      "Train Epoch: 430 [12800/17352 (74%)] Loss: -431471.500000\n",
      "Train Epoch: 430 [14208/17352 (82%)] Loss: -402555.812500\n",
      "Train Epoch: 430 [15530/17352 (89%)] Loss: -274054.000000\n",
      "Train Epoch: 430 [16252/17352 (94%)] Loss: -304381.687500\n",
      "Train Epoch: 430 [17000/17352 (98%)] Loss: -162688.843750\n",
      "    epoch          : 430\n",
      "    loss           : -398842.38107566064\n",
      "    val_loss       : -217473.9051106771\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch430.pth ...\n",
      "Train Epoch: 431 [128/17352 (1%)] Loss: -469929.687500\n",
      "Train Epoch: 431 [1536/17352 (9%)] Loss: -469839.375000\n",
      "Train Epoch: 431 [2944/17352 (17%)] Loss: -456430.562500\n",
      "Train Epoch: 431 [4352/17352 (25%)] Loss: -427183.781250\n",
      "Train Epoch: 431 [5760/17352 (33%)] Loss: -489262.781250\n",
      "Train Epoch: 431 [7168/17352 (41%)] Loss: -458951.656250\n",
      "Train Epoch: 431 [8576/17352 (49%)] Loss: -469443.281250\n",
      "Train Epoch: 431 [9984/17352 (58%)] Loss: -421563.593750\n",
      "Train Epoch: 431 [11392/17352 (66%)] Loss: -481707.906250\n",
      "Train Epoch: 431 [12800/17352 (74%)] Loss: -329930.218750\n",
      "Train Epoch: 431 [14208/17352 (82%)] Loss: -455123.437500\n",
      "Train Epoch: 431 [15435/17352 (89%)] Loss: -146924.125000\n",
      "Train Epoch: 431 [16249/17352 (94%)] Loss: -347494.437500\n",
      "Train Epoch: 431 [16952/17352 (98%)] Loss: -397895.312500\n",
      "    epoch          : 431\n",
      "    loss           : -402094.78518246644\n",
      "    val_loss       : -218290.55861002606\n",
      "Train Epoch: 432 [128/17352 (1%)] Loss: -446977.781250\n",
      "Train Epoch: 432 [1536/17352 (9%)] Loss: -478649.750000\n",
      "Train Epoch: 432 [2944/17352 (17%)] Loss: -471052.437500\n",
      "Train Epoch: 432 [4352/17352 (25%)] Loss: -434171.406250\n",
      "Train Epoch: 432 [5760/17352 (33%)] Loss: -438022.062500\n",
      "Train Epoch: 432 [7168/17352 (41%)] Loss: -482828.375000\n",
      "Train Epoch: 432 [8576/17352 (49%)] Loss: -392872.000000\n",
      "Train Epoch: 432 [9984/17352 (58%)] Loss: -451220.875000\n",
      "Train Epoch: 432 [11392/17352 (66%)] Loss: -479323.812500\n",
      "Train Epoch: 432 [12800/17352 (74%)] Loss: -479934.937500\n",
      "Train Epoch: 432 [14208/17352 (82%)] Loss: -425873.718750\n",
      "Train Epoch: 432 [15550/17352 (90%)] Loss: -256683.343750\n",
      "Train Epoch: 432 [16239/17352 (94%)] Loss: -219889.156250\n",
      "Train Epoch: 432 [17065/17352 (98%)] Loss: -264485.281250\n",
      "    epoch          : 432\n",
      "    loss           : -400333.25889392826\n",
      "    val_loss       : -220219.09680989583\n",
      "Train Epoch: 433 [128/17352 (1%)] Loss: -478858.687500\n",
      "Train Epoch: 433 [1536/17352 (9%)] Loss: -478342.531250\n",
      "Train Epoch: 433 [2944/17352 (17%)] Loss: -479570.687500\n",
      "Train Epoch: 433 [4352/17352 (25%)] Loss: -465006.125000\n",
      "Train Epoch: 433 [5760/17352 (33%)] Loss: -453774.437500\n",
      "Train Epoch: 433 [7168/17352 (41%)] Loss: -417864.000000\n",
      "Train Epoch: 433 [8576/17352 (49%)] Loss: -468166.750000\n",
      "Train Epoch: 433 [9984/17352 (58%)] Loss: -429365.312500\n",
      "Train Epoch: 433 [11392/17352 (66%)] Loss: -458781.281250\n",
      "Train Epoch: 433 [12800/17352 (74%)] Loss: -446076.093750\n",
      "Train Epoch: 433 [14208/17352 (82%)] Loss: -461591.281250\n",
      "Train Epoch: 433 [15533/17352 (90%)] Loss: -342820.062500\n",
      "Train Epoch: 433 [16400/17352 (95%)] Loss: -111220.734375\n",
      "Train Epoch: 433 [17090/17352 (98%)] Loss: -87151.195312\n",
      "    epoch          : 433\n",
      "    loss           : -403065.6177314912\n",
      "    val_loss       : -222369.24819335938\n",
      "Train Epoch: 434 [128/17352 (1%)] Loss: -424296.968750\n",
      "Train Epoch: 434 [1536/17352 (9%)] Loss: -457801.687500\n",
      "Train Epoch: 434 [2944/17352 (17%)] Loss: -466397.437500\n",
      "Train Epoch: 434 [4352/17352 (25%)] Loss: -461938.750000\n",
      "Train Epoch: 434 [5760/17352 (33%)] Loss: -468517.875000\n",
      "Train Epoch: 434 [7168/17352 (41%)] Loss: -467075.843750\n",
      "Train Epoch: 434 [8576/17352 (49%)] Loss: -412530.187500\n",
      "Train Epoch: 434 [9984/17352 (58%)] Loss: -419960.937500\n",
      "Train Epoch: 434 [11392/17352 (66%)] Loss: -375819.750000\n",
      "Train Epoch: 434 [12800/17352 (74%)] Loss: -479320.031250\n",
      "Train Epoch: 434 [14208/17352 (82%)] Loss: -442570.687500\n",
      "Train Epoch: 434 [15518/17352 (89%)] Loss: -419692.875000\n",
      "Train Epoch: 434 [16330/17352 (94%)] Loss: -114276.375000\n",
      "Train Epoch: 434 [17071/17352 (98%)] Loss: -279228.093750\n",
      "    epoch          : 434\n",
      "    loss           : -395362.16878801386\n",
      "    val_loss       : -219228.77185872395\n",
      "Train Epoch: 435 [128/17352 (1%)] Loss: -415618.687500\n",
      "Train Epoch: 435 [1536/17352 (9%)] Loss: -431907.531250\n",
      "Train Epoch: 435 [2944/17352 (17%)] Loss: -456752.500000\n",
      "Train Epoch: 435 [4352/17352 (25%)] Loss: -409743.343750\n",
      "Train Epoch: 435 [5760/17352 (33%)] Loss: -419011.406250\n",
      "Train Epoch: 435 [7168/17352 (41%)] Loss: -360798.062500\n",
      "Train Epoch: 435 [8576/17352 (49%)] Loss: -405830.875000\n",
      "Train Epoch: 435 [9984/17352 (58%)] Loss: -369566.625000\n",
      "Train Epoch: 435 [11392/17352 (66%)] Loss: -479853.218750\n",
      "Train Epoch: 435 [12800/17352 (74%)] Loss: -486735.843750\n",
      "Train Epoch: 435 [14208/17352 (82%)] Loss: -467162.562500\n",
      "Train Epoch: 435 [15530/17352 (89%)] Loss: -225929.734375\n",
      "Train Epoch: 435 [16291/17352 (94%)] Loss: -269462.375000\n",
      "Train Epoch: 435 [16907/17352 (97%)] Loss: -292363.406250\n",
      "    epoch          : 435\n",
      "    loss           : -395794.0104046508\n",
      "    val_loss       : -219324.58157552083\n",
      "Train Epoch: 436 [128/17352 (1%)] Loss: -477581.062500\n",
      "Train Epoch: 436 [1536/17352 (9%)] Loss: -448860.468750\n",
      "Train Epoch: 436 [2944/17352 (17%)] Loss: -447165.468750\n",
      "Train Epoch: 436 [4352/17352 (25%)] Loss: -458258.906250\n",
      "Train Epoch: 436 [5760/17352 (33%)] Loss: -459748.187500\n",
      "Train Epoch: 436 [7168/17352 (41%)] Loss: -369511.156250\n",
      "Train Epoch: 436 [8576/17352 (49%)] Loss: -412848.343750\n",
      "Train Epoch: 436 [9984/17352 (58%)] Loss: -468297.062500\n",
      "Train Epoch: 436 [11392/17352 (66%)] Loss: -468012.375000\n",
      "Train Epoch: 436 [12800/17352 (74%)] Loss: -389547.125000\n",
      "Train Epoch: 436 [14208/17352 (82%)] Loss: -409724.687500\n",
      "Train Epoch: 436 [15495/17352 (89%)] Loss: -183151.625000\n",
      "Train Epoch: 436 [16207/17352 (93%)] Loss: -155476.328125\n",
      "Train Epoch: 436 [16984/17352 (98%)] Loss: -393943.437500\n",
      "    epoch          : 436\n",
      "    loss           : -398708.65510958474\n",
      "    val_loss       : -219361.29869791667\n",
      "Train Epoch: 437 [128/17352 (1%)] Loss: -415642.406250\n",
      "Train Epoch: 437 [1536/17352 (9%)] Loss: -388102.031250\n",
      "Train Epoch: 437 [2944/17352 (17%)] Loss: -399589.937500\n",
      "Train Epoch: 437 [4352/17352 (25%)] Loss: -425689.718750\n",
      "Train Epoch: 437 [5760/17352 (33%)] Loss: -446048.500000\n",
      "Train Epoch: 437 [7168/17352 (41%)] Loss: -433420.312500\n",
      "Train Epoch: 437 [8576/17352 (49%)] Loss: -473207.875000\n",
      "Train Epoch: 437 [9984/17352 (58%)] Loss: -489364.250000\n",
      "Train Epoch: 437 [11392/17352 (66%)] Loss: -462016.031250\n",
      "Train Epoch: 437 [12800/17352 (74%)] Loss: -376388.562500\n",
      "Train Epoch: 437 [14208/17352 (82%)] Loss: -441572.812500\n",
      "Train Epoch: 437 [15490/17352 (89%)] Loss: -440304.156250\n",
      "Train Epoch: 437 [16295/17352 (94%)] Loss: -271098.687500\n",
      "Train Epoch: 437 [17053/17352 (98%)] Loss: -332774.093750\n",
      "    epoch          : 437\n",
      "    loss           : -396883.7026399958\n",
      "    val_loss       : -220179.05776367188\n",
      "Train Epoch: 438 [128/17352 (1%)] Loss: -479572.937500\n",
      "Train Epoch: 438 [1536/17352 (9%)] Loss: -447683.406250\n",
      "Train Epoch: 438 [2944/17352 (17%)] Loss: -424144.875000\n",
      "Train Epoch: 438 [4352/17352 (25%)] Loss: -421316.781250\n",
      "Train Epoch: 438 [5760/17352 (33%)] Loss: -459841.593750\n",
      "Train Epoch: 438 [7168/17352 (41%)] Loss: -331888.187500\n",
      "Train Epoch: 438 [8576/17352 (49%)] Loss: -448151.812500\n",
      "Train Epoch: 438 [9984/17352 (58%)] Loss: -467757.781250\n",
      "Train Epoch: 438 [11392/17352 (66%)] Loss: -456495.781250\n",
      "Train Epoch: 438 [12800/17352 (74%)] Loss: -442441.687500\n",
      "Train Epoch: 438 [14208/17352 (82%)] Loss: -438358.000000\n",
      "Train Epoch: 438 [15484/17352 (89%)] Loss: -301581.562500\n",
      "Train Epoch: 438 [16223/17352 (93%)] Loss: -331968.875000\n",
      "Train Epoch: 438 [16966/17352 (98%)] Loss: -45856.164062\n",
      "    epoch          : 438\n",
      "    loss           : -405802.42380190856\n",
      "    val_loss       : -219878.06555989583\n",
      "Train Epoch: 439 [128/17352 (1%)] Loss: -451662.468750\n",
      "Train Epoch: 439 [1536/17352 (9%)] Loss: -411887.000000\n",
      "Train Epoch: 439 [2944/17352 (17%)] Loss: -478353.625000\n",
      "Train Epoch: 439 [4352/17352 (25%)] Loss: -418234.156250\n",
      "Train Epoch: 439 [5760/17352 (33%)] Loss: -427150.937500\n",
      "Train Epoch: 439 [7168/17352 (41%)] Loss: -443227.281250\n",
      "Train Epoch: 439 [8576/17352 (49%)] Loss: -354346.250000\n",
      "Train Epoch: 439 [9984/17352 (58%)] Loss: -448888.468750\n",
      "Train Epoch: 439 [11392/17352 (66%)] Loss: -297749.812500\n",
      "Train Epoch: 439 [12800/17352 (74%)] Loss: -374455.250000\n",
      "Train Epoch: 439 [14208/17352 (82%)] Loss: -421053.593750\n",
      "Train Epoch: 439 [15435/17352 (89%)] Loss: -124337.312500\n",
      "Train Epoch: 439 [16130/17352 (93%)] Loss: -257651.718750\n",
      "Train Epoch: 439 [16961/17352 (98%)] Loss: -401268.718750\n",
      "    epoch          : 439\n",
      "    loss           : -400157.84224910865\n",
      "    val_loss       : -215787.52571614584\n",
      "Train Epoch: 440 [128/17352 (1%)] Loss: -450065.562500\n",
      "Train Epoch: 440 [1536/17352 (9%)] Loss: -460531.750000\n",
      "Train Epoch: 440 [2944/17352 (17%)] Loss: -249113.203125\n",
      "Train Epoch: 440 [4352/17352 (25%)] Loss: -473098.812500\n",
      "Train Epoch: 440 [5760/17352 (33%)] Loss: -440748.812500\n",
      "Train Epoch: 440 [7168/17352 (41%)] Loss: -452564.687500\n",
      "Train Epoch: 440 [8576/17352 (49%)] Loss: -455049.750000\n",
      "Train Epoch: 440 [9984/17352 (58%)] Loss: -470450.562500\n",
      "Train Epoch: 440 [11392/17352 (66%)] Loss: -467407.156250\n",
      "Train Epoch: 440 [12800/17352 (74%)] Loss: -469855.062500\n",
      "Train Epoch: 440 [14208/17352 (82%)] Loss: -407519.718750\n",
      "Train Epoch: 440 [15553/17352 (90%)] Loss: -270383.375000\n",
      "Train Epoch: 440 [16331/17352 (94%)] Loss: -174048.156250\n",
      "Train Epoch: 440 [17006/17352 (98%)] Loss: -433601.812500\n",
      "    epoch          : 440\n",
      "    loss           : -401270.80470060825\n",
      "    val_loss       : -217610.2719563802\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch440.pth ...\n",
      "Train Epoch: 441 [128/17352 (1%)] Loss: -425161.062500\n",
      "Train Epoch: 441 [1536/17352 (9%)] Loss: -477217.156250\n",
      "Train Epoch: 441 [2944/17352 (17%)] Loss: -470521.562500\n",
      "Train Epoch: 441 [4352/17352 (25%)] Loss: -464446.375000\n",
      "Train Epoch: 441 [5760/17352 (33%)] Loss: -445593.500000\n",
      "Train Epoch: 441 [7168/17352 (41%)] Loss: -354315.625000\n",
      "Train Epoch: 441 [8576/17352 (49%)] Loss: -419337.156250\n",
      "Train Epoch: 441 [9984/17352 (58%)] Loss: -408804.375000\n",
      "Train Epoch: 441 [11392/17352 (66%)] Loss: -395865.187500\n",
      "Train Epoch: 441 [12800/17352 (74%)] Loss: -409347.875000\n",
      "Train Epoch: 441 [14208/17352 (82%)] Loss: -442510.593750\n",
      "Train Epoch: 441 [15488/17352 (89%)] Loss: -285070.312500\n",
      "Train Epoch: 441 [16094/17352 (93%)] Loss: -13613.232422\n",
      "Train Epoch: 441 [16981/17352 (98%)] Loss: -301813.187500\n",
      "    epoch          : 441\n",
      "    loss           : -386512.56132026005\n",
      "    val_loss       : -219416.24085286458\n",
      "Train Epoch: 442 [128/17352 (1%)] Loss: -472281.968750\n",
      "Train Epoch: 442 [1536/17352 (9%)] Loss: -432222.312500\n",
      "Train Epoch: 442 [2944/17352 (17%)] Loss: -350894.375000\n",
      "Train Epoch: 442 [4352/17352 (25%)] Loss: -404998.500000\n",
      "Train Epoch: 442 [5760/17352 (33%)] Loss: -456409.531250\n",
      "Train Epoch: 442 [7168/17352 (41%)] Loss: -402245.062500\n",
      "Train Epoch: 442 [8576/17352 (49%)] Loss: -480129.656250\n",
      "Train Epoch: 442 [9984/17352 (58%)] Loss: -447631.843750\n",
      "Train Epoch: 442 [11392/17352 (66%)] Loss: -461171.687500\n",
      "Train Epoch: 442 [12800/17352 (74%)] Loss: -453681.812500\n",
      "Train Epoch: 442 [14208/17352 (82%)] Loss: -466536.281250\n",
      "Train Epoch: 442 [15502/17352 (89%)] Loss: -47371.210938\n",
      "Train Epoch: 442 [16142/17352 (93%)] Loss: -12727.161133\n",
      "Train Epoch: 442 [16952/17352 (98%)] Loss: -88910.671875\n",
      "    epoch          : 442\n",
      "    loss           : -391305.7053665059\n",
      "    val_loss       : -208941.15673828125\n",
      "Train Epoch: 443 [128/17352 (1%)] Loss: -470740.906250\n",
      "Train Epoch: 443 [1536/17352 (9%)] Loss: -385004.812500\n",
      "Train Epoch: 443 [2944/17352 (17%)] Loss: -383344.093750\n",
      "Train Epoch: 443 [4352/17352 (25%)] Loss: -426234.406250\n",
      "Train Epoch: 443 [5760/17352 (33%)] Loss: -483938.906250\n",
      "Train Epoch: 443 [7168/17352 (41%)] Loss: -442007.187500\n",
      "Train Epoch: 443 [8576/17352 (49%)] Loss: -408906.500000\n",
      "Train Epoch: 443 [9984/17352 (58%)] Loss: -382521.062500\n",
      "Train Epoch: 443 [11392/17352 (66%)] Loss: -431589.312500\n",
      "Train Epoch: 443 [12800/17352 (74%)] Loss: -460502.875000\n",
      "Train Epoch: 443 [14208/17352 (82%)] Loss: -460485.000000\n",
      "Train Epoch: 443 [15523/17352 (89%)] Loss: -292576.312500\n",
      "Train Epoch: 443 [16364/17352 (94%)] Loss: -376171.875000\n",
      "Train Epoch: 443 [17102/17352 (99%)] Loss: -192116.968750\n",
      "    epoch          : 443\n",
      "    loss           : -394781.85727113043\n",
      "    val_loss       : -225857.22692057292\n",
      "Train Epoch: 444 [128/17352 (1%)] Loss: -439265.625000\n",
      "Train Epoch: 444 [1536/17352 (9%)] Loss: -396927.656250\n",
      "Train Epoch: 444 [2944/17352 (17%)] Loss: -428056.843750\n",
      "Train Epoch: 444 [4352/17352 (25%)] Loss: -446530.437500\n",
      "Train Epoch: 444 [5760/17352 (33%)] Loss: -325364.187500\n",
      "Train Epoch: 444 [7168/17352 (41%)] Loss: -402883.093750\n",
      "Train Epoch: 444 [8576/17352 (49%)] Loss: -478395.750000\n",
      "Train Epoch: 444 [9984/17352 (58%)] Loss: -427383.562500\n",
      "Train Epoch: 444 [11392/17352 (66%)] Loss: -361970.843750\n",
      "Train Epoch: 444 [12800/17352 (74%)] Loss: -455309.250000\n",
      "Train Epoch: 444 [14208/17352 (82%)] Loss: -418916.062500\n",
      "Train Epoch: 444 [15491/17352 (89%)] Loss: -185131.578125\n",
      "Train Epoch: 444 [16308/17352 (94%)] Loss: -371668.687500\n",
      "Train Epoch: 444 [17079/17352 (98%)] Loss: -99862.320312\n",
      "    epoch          : 444\n",
      "    loss           : -395146.3476759123\n",
      "    val_loss       : -216277.19272460937\n",
      "Train Epoch: 445 [128/17352 (1%)] Loss: -271215.812500\n",
      "Train Epoch: 445 [1536/17352 (9%)] Loss: -474159.125000\n",
      "Train Epoch: 445 [2944/17352 (17%)] Loss: -490468.000000\n",
      "Train Epoch: 445 [4352/17352 (25%)] Loss: -352769.000000\n",
      "Train Epoch: 445 [5760/17352 (33%)] Loss: -444479.812500\n",
      "Train Epoch: 445 [7168/17352 (41%)] Loss: -464829.625000\n",
      "Train Epoch: 445 [8576/17352 (49%)] Loss: -427890.312500\n",
      "Train Epoch: 445 [9984/17352 (58%)] Loss: -449145.750000\n",
      "Train Epoch: 445 [11392/17352 (66%)] Loss: -462473.781250\n",
      "Train Epoch: 445 [12800/17352 (74%)] Loss: -410596.375000\n",
      "Train Epoch: 445 [14208/17352 (82%)] Loss: -457536.187500\n",
      "Train Epoch: 445 [15479/17352 (89%)] Loss: -193793.093750\n",
      "Train Epoch: 445 [16229/17352 (94%)] Loss: -201047.968750\n",
      "Train Epoch: 445 [16968/17352 (98%)] Loss: -12006.535156\n",
      "    epoch          : 445\n",
      "    loss           : -391316.5358772022\n",
      "    val_loss       : -217725.82218424478\n",
      "Train Epoch: 446 [128/17352 (1%)] Loss: -390457.937500\n",
      "Train Epoch: 446 [1536/17352 (9%)] Loss: -422817.750000\n",
      "Train Epoch: 446 [2944/17352 (17%)] Loss: -440280.937500\n",
      "Train Epoch: 446 [4352/17352 (25%)] Loss: -484715.406250\n",
      "Train Epoch: 446 [5760/17352 (33%)] Loss: -429675.531250\n",
      "Train Epoch: 446 [7168/17352 (41%)] Loss: -354537.625000\n",
      "Train Epoch: 446 [8576/17352 (49%)] Loss: -434051.031250\n",
      "Train Epoch: 446 [9984/17352 (58%)] Loss: -408593.062500\n",
      "Train Epoch: 446 [11392/17352 (66%)] Loss: -461710.531250\n",
      "Train Epoch: 446 [12800/17352 (74%)] Loss: -444759.187500\n",
      "Train Epoch: 446 [14208/17352 (82%)] Loss: -438775.750000\n",
      "Train Epoch: 446 [15452/17352 (89%)] Loss: -271614.031250\n",
      "Train Epoch: 446 [16302/17352 (94%)] Loss: -309915.750000\n",
      "Train Epoch: 446 [16913/17352 (97%)] Loss: -218657.562500\n",
      "    epoch          : 446\n",
      "    loss           : -400296.2652579698\n",
      "    val_loss       : -227595.54287109376\n",
      "Train Epoch: 447 [128/17352 (1%)] Loss: -464742.531250\n",
      "Train Epoch: 447 [1536/17352 (9%)] Loss: -362898.187500\n",
      "Train Epoch: 447 [2944/17352 (17%)] Loss: -490550.812500\n",
      "Train Epoch: 447 [4352/17352 (25%)] Loss: -422537.406250\n",
      "Train Epoch: 447 [5760/17352 (33%)] Loss: -454966.062500\n",
      "Train Epoch: 447 [7168/17352 (41%)] Loss: -466678.375000\n",
      "Train Epoch: 447 [8576/17352 (49%)] Loss: -489161.250000\n",
      "Train Epoch: 447 [9984/17352 (58%)] Loss: -441422.562500\n",
      "Train Epoch: 447 [11392/17352 (66%)] Loss: -488709.593750\n",
      "Train Epoch: 447 [12800/17352 (74%)] Loss: -483050.718750\n",
      "Train Epoch: 447 [14208/17352 (82%)] Loss: -455666.906250\n",
      "Train Epoch: 447 [15476/17352 (89%)] Loss: -243801.265625\n",
      "Train Epoch: 447 [16277/17352 (94%)] Loss: -293895.875000\n",
      "Train Epoch: 447 [17030/17352 (98%)] Loss: -285499.906250\n",
      "    epoch          : 447\n",
      "    loss           : -402069.8152396183\n",
      "    val_loss       : -224655.95548502603\n",
      "Train Epoch: 448 [128/17352 (1%)] Loss: -473970.031250\n",
      "Train Epoch: 448 [1536/17352 (9%)] Loss: -437302.812500\n",
      "Train Epoch: 448 [2944/17352 (17%)] Loss: -423716.968750\n",
      "Train Epoch: 448 [4352/17352 (25%)] Loss: -511991.531250\n",
      "Train Epoch: 448 [5760/17352 (33%)] Loss: -445072.250000\n",
      "Train Epoch: 448 [7168/17352 (41%)] Loss: -482346.375000\n",
      "Train Epoch: 448 [8576/17352 (49%)] Loss: -453199.125000\n",
      "Train Epoch: 448 [9984/17352 (58%)] Loss: -502545.437500\n",
      "Train Epoch: 448 [11392/17352 (66%)] Loss: -489663.687500\n",
      "Train Epoch: 448 [12800/17352 (74%)] Loss: -479404.125000\n",
      "Train Epoch: 448 [14208/17352 (82%)] Loss: -444376.437500\n",
      "Train Epoch: 448 [15455/17352 (89%)] Loss: -203423.468750\n",
      "Train Epoch: 448 [16348/17352 (94%)] Loss: -237241.421875\n",
      "Train Epoch: 448 [17029/17352 (98%)] Loss: -116124.117188\n",
      "    epoch          : 448\n",
      "    loss           : -403219.95384922926\n",
      "    val_loss       : -221227.56857096354\n",
      "Train Epoch: 449 [128/17352 (1%)] Loss: -454792.125000\n",
      "Train Epoch: 449 [1536/17352 (9%)] Loss: -422306.406250\n",
      "Train Epoch: 449 [2944/17352 (17%)] Loss: -477448.500000\n",
      "Train Epoch: 449 [4352/17352 (25%)] Loss: -426233.250000\n",
      "Train Epoch: 449 [5760/17352 (33%)] Loss: -366909.968750\n",
      "Train Epoch: 449 [7168/17352 (41%)] Loss: -467209.125000\n",
      "Train Epoch: 449 [8576/17352 (49%)] Loss: -399779.656250\n",
      "Train Epoch: 449 [9984/17352 (58%)] Loss: -408311.687500\n",
      "Train Epoch: 449 [11392/17352 (66%)] Loss: -415869.500000\n",
      "Train Epoch: 449 [12800/17352 (74%)] Loss: -408930.750000\n",
      "Train Epoch: 449 [14208/17352 (82%)] Loss: -462360.250000\n",
      "Train Epoch: 449 [15457/17352 (89%)] Loss: -47781.019531\n",
      "Train Epoch: 449 [16240/17352 (94%)] Loss: -263086.562500\n",
      "Train Epoch: 449 [17075/17352 (98%)] Loss: -109416.421875\n",
      "    epoch          : 449\n",
      "    loss           : -405912.63717098365\n",
      "    val_loss       : -214489.8655110677\n",
      "Train Epoch: 450 [128/17352 (1%)] Loss: -443619.437500\n",
      "Train Epoch: 450 [1536/17352 (9%)] Loss: -473392.343750\n",
      "Train Epoch: 450 [2944/17352 (17%)] Loss: -378038.250000\n",
      "Train Epoch: 450 [4352/17352 (25%)] Loss: -467657.875000\n",
      "Train Epoch: 450 [5760/17352 (33%)] Loss: -439178.875000\n",
      "Train Epoch: 450 [7168/17352 (41%)] Loss: -428869.468750\n",
      "Train Epoch: 450 [8576/17352 (49%)] Loss: -495127.812500\n",
      "Train Epoch: 450 [9984/17352 (58%)] Loss: -458818.125000\n",
      "Train Epoch: 450 [11392/17352 (66%)] Loss: -460637.031250\n",
      "Train Epoch: 450 [12800/17352 (74%)] Loss: -439283.375000\n",
      "Train Epoch: 450 [14208/17352 (82%)] Loss: -394569.250000\n",
      "Train Epoch: 450 [15521/17352 (89%)] Loss: -261269.484375\n",
      "Train Epoch: 450 [16266/17352 (94%)] Loss: -363471.625000\n",
      "Train Epoch: 450 [17039/17352 (98%)] Loss: -365707.406250\n",
      "    epoch          : 450\n",
      "    loss           : -398883.52744861576\n",
      "    val_loss       : -228757.32054036457\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch450.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 451 [128/17352 (1%)] Loss: -380552.937500\n",
      "Train Epoch: 451 [1536/17352 (9%)] Loss: -478158.312500\n",
      "Train Epoch: 451 [2944/17352 (17%)] Loss: -468118.062500\n",
      "Train Epoch: 451 [4352/17352 (25%)] Loss: -374703.625000\n",
      "Train Epoch: 451 [5760/17352 (33%)] Loss: -349699.718750\n",
      "Train Epoch: 451 [7168/17352 (41%)] Loss: -500542.343750\n",
      "Train Epoch: 451 [8576/17352 (49%)] Loss: -469328.000000\n",
      "Train Epoch: 451 [9984/17352 (58%)] Loss: -452378.281250\n",
      "Train Epoch: 451 [11392/17352 (66%)] Loss: -456371.250000\n",
      "Train Epoch: 451 [12800/17352 (74%)] Loss: -476205.937500\n",
      "Train Epoch: 451 [14208/17352 (82%)] Loss: -390712.000000\n",
      "Train Epoch: 451 [15498/17352 (89%)] Loss: -203903.718750\n",
      "Train Epoch: 451 [16311/17352 (94%)] Loss: -198707.781250\n",
      "Train Epoch: 451 [16949/17352 (98%)] Loss: -273602.156250\n",
      "    epoch          : 451\n",
      "    loss           : -402620.40026609687\n",
      "    val_loss       : -228129.6573404948\n",
      "Train Epoch: 452 [128/17352 (1%)] Loss: -453852.562500\n",
      "Train Epoch: 452 [1536/17352 (9%)] Loss: -468401.500000\n",
      "Train Epoch: 452 [2944/17352 (17%)] Loss: -492373.312500\n",
      "Train Epoch: 452 [4352/17352 (25%)] Loss: -423279.437500\n",
      "Train Epoch: 452 [5760/17352 (33%)] Loss: -329588.875000\n",
      "Train Epoch: 452 [7168/17352 (41%)] Loss: -432413.156250\n",
      "Train Epoch: 452 [8576/17352 (49%)] Loss: -475964.875000\n",
      "Train Epoch: 452 [9984/17352 (58%)] Loss: -483511.218750\n",
      "Train Epoch: 452 [11392/17352 (66%)] Loss: -433944.906250\n",
      "Train Epoch: 452 [12800/17352 (74%)] Loss: -453580.031250\n",
      "Train Epoch: 452 [14208/17352 (82%)] Loss: -488133.250000\n",
      "Train Epoch: 452 [15472/17352 (89%)] Loss: -126095.109375\n",
      "Train Epoch: 452 [16171/17352 (93%)] Loss: -256251.218750\n",
      "Train Epoch: 452 [16981/17352 (98%)] Loss: -200773.937500\n",
      "    epoch          : 452\n",
      "    loss           : -404446.55080746644\n",
      "    val_loss       : -221712.979296875\n",
      "Train Epoch: 453 [128/17352 (1%)] Loss: -459551.687500\n",
      "Train Epoch: 453 [1536/17352 (9%)] Loss: -321111.125000\n",
      "Train Epoch: 453 [2944/17352 (17%)] Loss: -477999.156250\n",
      "Train Epoch: 453 [4352/17352 (25%)] Loss: -476797.468750\n",
      "Train Epoch: 453 [5760/17352 (33%)] Loss: -447589.625000\n",
      "Train Epoch: 453 [7168/17352 (41%)] Loss: -410209.718750\n",
      "Train Epoch: 453 [8576/17352 (49%)] Loss: -392524.718750\n",
      "Train Epoch: 453 [9984/17352 (58%)] Loss: -426917.593750\n",
      "Train Epoch: 453 [11392/17352 (66%)] Loss: -448624.937500\n",
      "Train Epoch: 453 [12800/17352 (74%)] Loss: -435187.531250\n",
      "Train Epoch: 453 [14208/17352 (82%)] Loss: -458882.062500\n",
      "Train Epoch: 453 [15527/17352 (89%)] Loss: -268225.468750\n",
      "Train Epoch: 453 [16246/17352 (94%)] Loss: -139053.718750\n",
      "Train Epoch: 453 [17060/17352 (98%)] Loss: -257348.687500\n",
      "    epoch          : 453\n",
      "    loss           : -405974.9872850252\n",
      "    val_loss       : -221858.14669596354\n",
      "Train Epoch: 454 [128/17352 (1%)] Loss: -512839.125000\n",
      "Train Epoch: 454 [1536/17352 (9%)] Loss: -450658.531250\n",
      "Train Epoch: 454 [2944/17352 (17%)] Loss: -474842.187500\n",
      "Train Epoch: 454 [4352/17352 (25%)] Loss: -466141.062500\n",
      "Train Epoch: 454 [5760/17352 (33%)] Loss: -489494.562500\n",
      "Train Epoch: 454 [7168/17352 (41%)] Loss: -462433.156250\n",
      "Train Epoch: 454 [8576/17352 (49%)] Loss: -427342.343750\n",
      "Train Epoch: 454 [9984/17352 (58%)] Loss: -454688.468750\n",
      "Train Epoch: 454 [11392/17352 (66%)] Loss: -440093.375000\n",
      "Train Epoch: 454 [12800/17352 (74%)] Loss: -455592.625000\n",
      "Train Epoch: 454 [14208/17352 (82%)] Loss: -500278.062500\n",
      "Train Epoch: 454 [15602/17352 (90%)] Loss: -433043.187500\n",
      "Train Epoch: 454 [16401/17352 (95%)] Loss: -223500.812500\n",
      "Train Epoch: 454 [17153/17352 (99%)] Loss: -164012.796875\n",
      "    epoch          : 454\n",
      "    loss           : -408616.3555211829\n",
      "    val_loss       : -221316.26486002604\n",
      "Train Epoch: 455 [128/17352 (1%)] Loss: -443404.500000\n",
      "Train Epoch: 455 [1536/17352 (9%)] Loss: -438977.187500\n",
      "Train Epoch: 455 [2944/17352 (17%)] Loss: -437576.625000\n",
      "Train Epoch: 455 [4352/17352 (25%)] Loss: -498404.125000\n",
      "Train Epoch: 455 [5760/17352 (33%)] Loss: -396892.625000\n",
      "Train Epoch: 455 [7168/17352 (41%)] Loss: -456934.718750\n",
      "Train Epoch: 455 [8576/17352 (49%)] Loss: -498934.687500\n",
      "Train Epoch: 455 [9984/17352 (58%)] Loss: -355714.406250\n",
      "Train Epoch: 455 [11392/17352 (66%)] Loss: -402378.906250\n",
      "Train Epoch: 455 [12800/17352 (74%)] Loss: -478188.406250\n",
      "Train Epoch: 455 [14208/17352 (82%)] Loss: -416711.875000\n",
      "Train Epoch: 455 [15529/17352 (89%)] Loss: -325925.312500\n",
      "Train Epoch: 455 [16257/17352 (94%)] Loss: -292968.531250\n",
      "Train Epoch: 455 [17032/17352 (98%)] Loss: -450645.000000\n",
      "    epoch          : 455\n",
      "    loss           : -407135.9965394295\n",
      "    val_loss       : -226174.1922200521\n",
      "Train Epoch: 456 [128/17352 (1%)] Loss: -514769.187500\n",
      "Train Epoch: 456 [1536/17352 (9%)] Loss: -405784.218750\n",
      "Train Epoch: 456 [2944/17352 (17%)] Loss: -454552.593750\n",
      "Train Epoch: 456 [4352/17352 (25%)] Loss: -433948.625000\n",
      "Train Epoch: 456 [5760/17352 (33%)] Loss: -453522.656250\n",
      "Train Epoch: 456 [7168/17352 (41%)] Loss: -354606.406250\n",
      "Train Epoch: 456 [8576/17352 (49%)] Loss: -450002.500000\n",
      "Train Epoch: 456 [9984/17352 (58%)] Loss: -464071.656250\n",
      "Train Epoch: 456 [11392/17352 (66%)] Loss: -498106.781250\n",
      "Train Epoch: 456 [12800/17352 (74%)] Loss: -461147.937500\n",
      "Train Epoch: 456 [14208/17352 (82%)] Loss: -442060.937500\n",
      "Train Epoch: 456 [15399/17352 (89%)] Loss: -20212.035156\n",
      "Train Epoch: 456 [16066/17352 (93%)] Loss: -375500.687500\n",
      "Train Epoch: 456 [17019/17352 (98%)] Loss: -296050.875000\n",
      "    epoch          : 456\n",
      "    loss           : -405295.74623794045\n",
      "    val_loss       : -217555.0255859375\n",
      "Train Epoch: 457 [128/17352 (1%)] Loss: -452897.687500\n",
      "Train Epoch: 457 [1536/17352 (9%)] Loss: -409039.062500\n",
      "Train Epoch: 457 [2944/17352 (17%)] Loss: -480901.406250\n",
      "Train Epoch: 457 [4352/17352 (25%)] Loss: -418164.343750\n",
      "Train Epoch: 457 [5760/17352 (33%)] Loss: -459496.187500\n",
      "Train Epoch: 457 [7168/17352 (41%)] Loss: -461322.406250\n",
      "Train Epoch: 457 [8576/17352 (49%)] Loss: -426140.312500\n",
      "Train Epoch: 457 [9984/17352 (58%)] Loss: -365594.062500\n",
      "Train Epoch: 457 [11392/17352 (66%)] Loss: -383779.750000\n",
      "Train Epoch: 457 [12800/17352 (74%)] Loss: -381499.937500\n",
      "Train Epoch: 457 [14208/17352 (82%)] Loss: -389837.468750\n",
      "Train Epoch: 457 [15528/17352 (89%)] Loss: -352405.437500\n",
      "Train Epoch: 457 [16303/17352 (94%)] Loss: -371907.406250\n",
      "Train Epoch: 457 [17029/17352 (98%)] Loss: -221868.468750\n",
      "    epoch          : 457\n",
      "    loss           : -405988.7246028209\n",
      "    val_loss       : -228560.49853515625\n",
      "Train Epoch: 458 [128/17352 (1%)] Loss: -452965.093750\n",
      "Train Epoch: 458 [1536/17352 (9%)] Loss: -434551.906250\n",
      "Train Epoch: 458 [2944/17352 (17%)] Loss: -334427.656250\n",
      "Train Epoch: 458 [4352/17352 (25%)] Loss: -439766.250000\n",
      "Train Epoch: 458 [5760/17352 (33%)] Loss: -414994.968750\n",
      "Train Epoch: 458 [7168/17352 (41%)] Loss: -484221.718750\n",
      "Train Epoch: 458 [8576/17352 (49%)] Loss: -406236.312500\n",
      "Train Epoch: 458 [9984/17352 (58%)] Loss: -487144.437500\n",
      "Train Epoch: 458 [11392/17352 (66%)] Loss: -472172.281250\n",
      "Train Epoch: 458 [12800/17352 (74%)] Loss: -448759.593750\n",
      "Train Epoch: 458 [14208/17352 (82%)] Loss: -430212.312500\n",
      "Train Epoch: 458 [15378/17352 (89%)] Loss: -9609.924805\n",
      "Train Epoch: 458 [16067/17352 (93%)] Loss: -95144.343750\n",
      "Train Epoch: 458 [16869/17352 (97%)] Loss: -346718.531250\n",
      "    epoch          : 458\n",
      "    loss           : -400967.0834272756\n",
      "    val_loss       : -227917.2229654948\n",
      "Train Epoch: 459 [128/17352 (1%)] Loss: -416418.687500\n",
      "Train Epoch: 459 [1536/17352 (9%)] Loss: -408923.531250\n",
      "Train Epoch: 459 [2944/17352 (17%)] Loss: -423527.437500\n",
      "Train Epoch: 459 [4352/17352 (25%)] Loss: -447595.281250\n",
      "Train Epoch: 459 [5760/17352 (33%)] Loss: -393778.968750\n",
      "Train Epoch: 459 [7168/17352 (41%)] Loss: -489987.031250\n",
      "Train Epoch: 459 [8576/17352 (49%)] Loss: -408232.562500\n",
      "Train Epoch: 459 [9984/17352 (58%)] Loss: -456533.375000\n",
      "Train Epoch: 459 [11392/17352 (66%)] Loss: -374632.750000\n",
      "Train Epoch: 459 [12800/17352 (74%)] Loss: -439423.312500\n",
      "Train Epoch: 459 [14208/17352 (82%)] Loss: -502143.625000\n",
      "Train Epoch: 459 [15530/17352 (89%)] Loss: -135956.656250\n",
      "Train Epoch: 459 [16266/17352 (94%)] Loss: -407881.000000\n",
      "Train Epoch: 459 [17058/17352 (98%)] Loss: -379161.843750\n",
      "    epoch          : 459\n",
      "    loss           : -402551.3921586619\n",
      "    val_loss       : -223811.03240559896\n",
      "Train Epoch: 460 [128/17352 (1%)] Loss: -469757.093750\n",
      "Train Epoch: 460 [1536/17352 (9%)] Loss: -438387.343750\n",
      "Train Epoch: 460 [2944/17352 (17%)] Loss: -460639.156250\n",
      "Train Epoch: 460 [4352/17352 (25%)] Loss: -451416.531250\n",
      "Train Epoch: 460 [5760/17352 (33%)] Loss: -476895.843750\n",
      "Train Epoch: 460 [7168/17352 (41%)] Loss: -470834.312500\n",
      "Train Epoch: 460 [8576/17352 (49%)] Loss: -380957.781250\n",
      "Train Epoch: 460 [9984/17352 (58%)] Loss: -435192.031250\n",
      "Train Epoch: 460 [11392/17352 (66%)] Loss: -474088.437500\n",
      "Train Epoch: 460 [12800/17352 (74%)] Loss: -439840.812500\n",
      "Train Epoch: 460 [14208/17352 (82%)] Loss: -312682.843750\n",
      "Train Epoch: 460 [15448/17352 (89%)] Loss: -114439.351562\n",
      "Train Epoch: 460 [16397/17352 (94%)] Loss: -358540.531250\n",
      "Train Epoch: 460 [17129/17352 (99%)] Loss: -347853.125000\n",
      "    epoch          : 460\n",
      "    loss           : -404274.3121526321\n",
      "    val_loss       : -224438.2395670573\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch460.pth ...\n",
      "Train Epoch: 461 [128/17352 (1%)] Loss: -485792.062500\n",
      "Train Epoch: 461 [1536/17352 (9%)] Loss: -462186.531250\n",
      "Train Epoch: 461 [2944/17352 (17%)] Loss: -409530.750000\n",
      "Train Epoch: 461 [4352/17352 (25%)] Loss: -462019.187500\n",
      "Train Epoch: 461 [5760/17352 (33%)] Loss: -498653.875000\n",
      "Train Epoch: 461 [7168/17352 (41%)] Loss: -427332.750000\n",
      "Train Epoch: 461 [8576/17352 (49%)] Loss: -478998.750000\n",
      "Train Epoch: 461 [9984/17352 (58%)] Loss: -478655.750000\n",
      "Train Epoch: 461 [11392/17352 (66%)] Loss: -447435.843750\n",
      "Train Epoch: 461 [12800/17352 (74%)] Loss: -498496.156250\n",
      "Train Epoch: 461 [14208/17352 (82%)] Loss: -431724.375000\n",
      "Train Epoch: 461 [15388/17352 (89%)] Loss: -44380.042969\n",
      "Train Epoch: 461 [16304/17352 (94%)] Loss: -437268.593750\n",
      "Train Epoch: 461 [17172/17352 (99%)] Loss: -268973.937500\n",
      "    epoch          : 461\n",
      "    loss           : -404890.029119914\n",
      "    val_loss       : -224130.02501627605\n",
      "Train Epoch: 462 [128/17352 (1%)] Loss: -481854.375000\n",
      "Train Epoch: 462 [1536/17352 (9%)] Loss: -478962.093750\n",
      "Train Epoch: 462 [2944/17352 (17%)] Loss: -444473.312500\n",
      "Train Epoch: 462 [4352/17352 (25%)] Loss: -421160.937500\n",
      "Train Epoch: 462 [5760/17352 (33%)] Loss: -446675.500000\n",
      "Train Epoch: 462 [7168/17352 (41%)] Loss: -456542.562500\n",
      "Train Epoch: 462 [8576/17352 (49%)] Loss: -452176.406250\n",
      "Train Epoch: 462 [9984/17352 (58%)] Loss: -447299.031250\n",
      "Train Epoch: 462 [11392/17352 (66%)] Loss: -422525.437500\n",
      "Train Epoch: 462 [12800/17352 (74%)] Loss: -422362.437500\n",
      "Train Epoch: 462 [14208/17352 (82%)] Loss: -469901.000000\n",
      "Train Epoch: 462 [15438/17352 (89%)] Loss: -234990.062500\n",
      "Train Epoch: 462 [16225/17352 (94%)] Loss: -358586.812500\n",
      "Train Epoch: 462 [17005/17352 (98%)] Loss: -435889.468750\n",
      "    epoch          : 462\n",
      "    loss           : -405376.10055316694\n",
      "    val_loss       : -233235.1939453125\n",
      "Train Epoch: 463 [128/17352 (1%)] Loss: -478780.437500\n",
      "Train Epoch: 463 [1536/17352 (9%)] Loss: -444342.937500\n",
      "Train Epoch: 463 [2944/17352 (17%)] Loss: -433289.812500\n",
      "Train Epoch: 463 [4352/17352 (25%)] Loss: -422222.718750\n",
      "Train Epoch: 463 [5760/17352 (33%)] Loss: -417537.218750\n",
      "Train Epoch: 463 [7168/17352 (41%)] Loss: -489290.031250\n",
      "Train Epoch: 463 [8576/17352 (49%)] Loss: -478507.718750\n",
      "Train Epoch: 463 [9984/17352 (58%)] Loss: -461873.187500\n",
      "Train Epoch: 463 [11392/17352 (66%)] Loss: -456460.906250\n",
      "Train Epoch: 463 [12800/17352 (74%)] Loss: -504110.500000\n",
      "Train Epoch: 463 [14208/17352 (82%)] Loss: -416289.937500\n",
      "Train Epoch: 463 [15547/17352 (90%)] Loss: -250079.546875\n",
      "Train Epoch: 463 [16208/17352 (93%)] Loss: -55135.839844\n",
      "Train Epoch: 463 [16949/17352 (98%)] Loss: -161514.734375\n",
      "    epoch          : 463\n",
      "    loss           : -411214.90556837246\n",
      "    val_loss       : -226974.7640950521\n",
      "Train Epoch: 464 [128/17352 (1%)] Loss: -465934.843750\n",
      "Train Epoch: 464 [1536/17352 (9%)] Loss: -475170.625000\n",
      "Train Epoch: 464 [2944/17352 (17%)] Loss: -486171.750000\n",
      "Train Epoch: 464 [4352/17352 (25%)] Loss: -471125.656250\n",
      "Train Epoch: 464 [5760/17352 (33%)] Loss: -476977.906250\n",
      "Train Epoch: 464 [7168/17352 (41%)] Loss: -473743.250000\n",
      "Train Epoch: 464 [8576/17352 (49%)] Loss: -488377.812500\n",
      "Train Epoch: 464 [9984/17352 (58%)] Loss: -460875.500000\n",
      "Train Epoch: 464 [11392/17352 (66%)] Loss: -402303.093750\n",
      "Train Epoch: 464 [12800/17352 (74%)] Loss: -498466.312500\n",
      "Train Epoch: 464 [14208/17352 (82%)] Loss: -447210.812500\n",
      "Train Epoch: 464 [15419/17352 (89%)] Loss: -14047.316406\n",
      "Train Epoch: 464 [16214/17352 (93%)] Loss: -190512.343750\n",
      "Train Epoch: 464 [17040/17352 (98%)] Loss: -263831.312500\n",
      "    epoch          : 464\n",
      "    loss           : -409104.78773201554\n",
      "    val_loss       : -225057.75122070312\n",
      "Train Epoch: 465 [128/17352 (1%)] Loss: -484152.750000\n",
      "Train Epoch: 465 [1536/17352 (9%)] Loss: -489171.843750\n",
      "Train Epoch: 465 [2944/17352 (17%)] Loss: -434158.687500\n",
      "Train Epoch: 465 [4352/17352 (25%)] Loss: -464750.062500\n",
      "Train Epoch: 465 [5760/17352 (33%)] Loss: -470470.625000\n",
      "Train Epoch: 465 [7168/17352 (41%)] Loss: -472653.156250\n",
      "Train Epoch: 465 [8576/17352 (49%)] Loss: -435992.125000\n",
      "Train Epoch: 465 [9984/17352 (58%)] Loss: -480258.375000\n",
      "Train Epoch: 465 [11392/17352 (66%)] Loss: -484068.250000\n",
      "Train Epoch: 465 [12800/17352 (74%)] Loss: -475429.500000\n",
      "Train Epoch: 465 [14208/17352 (82%)] Loss: -459088.812500\n",
      "Train Epoch: 465 [15479/17352 (89%)] Loss: -326819.812500\n",
      "Train Epoch: 465 [16310/17352 (94%)] Loss: -315255.437500\n",
      "Train Epoch: 465 [17000/17352 (98%)] Loss: -403488.250000\n",
      "    epoch          : 465\n",
      "    loss           : -407785.1466220113\n",
      "    val_loss       : -226798.18728841146\n",
      "Train Epoch: 466 [128/17352 (1%)] Loss: -396924.656250\n",
      "Train Epoch: 466 [1536/17352 (9%)] Loss: -249689.265625\n",
      "Train Epoch: 466 [2944/17352 (17%)] Loss: -480880.750000\n",
      "Train Epoch: 466 [4352/17352 (25%)] Loss: -453014.437500\n",
      "Train Epoch: 466 [5760/17352 (33%)] Loss: -471768.843750\n",
      "Train Epoch: 466 [7168/17352 (41%)] Loss: -392951.156250\n",
      "Train Epoch: 466 [8576/17352 (49%)] Loss: -482431.031250\n",
      "Train Epoch: 466 [9984/17352 (58%)] Loss: -355192.093750\n",
      "Train Epoch: 466 [11392/17352 (66%)] Loss: -403529.062500\n",
      "Train Epoch: 466 [12800/17352 (74%)] Loss: -490634.000000\n",
      "Train Epoch: 466 [14208/17352 (82%)] Loss: -456488.875000\n",
      "Train Epoch: 466 [15533/17352 (90%)] Loss: -327249.437500\n",
      "Train Epoch: 466 [16172/17352 (93%)] Loss: -122847.976562\n",
      "Train Epoch: 466 [16990/17352 (98%)] Loss: -6206.731445\n",
      "    epoch          : 466\n",
      "    loss           : -404179.1010840499\n",
      "    val_loss       : -226649.53131510416\n",
      "Train Epoch: 467 [128/17352 (1%)] Loss: -399888.125000\n",
      "Train Epoch: 467 [1536/17352 (9%)] Loss: -449336.562500\n",
      "Train Epoch: 467 [2944/17352 (17%)] Loss: -450165.375000\n",
      "Train Epoch: 467 [4352/17352 (25%)] Loss: -384079.750000\n",
      "Train Epoch: 467 [5760/17352 (33%)] Loss: -495758.375000\n",
      "Train Epoch: 467 [7168/17352 (41%)] Loss: -469437.968750\n",
      "Train Epoch: 467 [8576/17352 (49%)] Loss: -351524.531250\n",
      "Train Epoch: 467 [9984/17352 (58%)] Loss: -331439.156250\n",
      "Train Epoch: 467 [11392/17352 (66%)] Loss: -433912.468750\n",
      "Train Epoch: 467 [12800/17352 (74%)] Loss: -448852.375000\n",
      "Train Epoch: 467 [14208/17352 (82%)] Loss: -458232.875000\n",
      "Train Epoch: 467 [15463/17352 (89%)] Loss: -48971.933594\n",
      "Train Epoch: 467 [16091/17352 (93%)] Loss: -48991.253906\n",
      "Train Epoch: 467 [16897/17352 (97%)] Loss: -255804.828125\n",
      "    epoch          : 467\n",
      "    loss           : -407804.40034474625\n",
      "    val_loss       : -220907.70504557292\n",
      "Train Epoch: 468 [128/17352 (1%)] Loss: -467199.968750\n",
      "Train Epoch: 468 [1536/17352 (9%)] Loss: -444232.187500\n",
      "Train Epoch: 468 [2944/17352 (17%)] Loss: -486216.625000\n",
      "Train Epoch: 468 [4352/17352 (25%)] Loss: -466120.218750\n",
      "Train Epoch: 468 [5760/17352 (33%)] Loss: -415109.593750\n",
      "Train Epoch: 468 [7168/17352 (41%)] Loss: -383226.312500\n",
      "Train Epoch: 468 [8576/17352 (49%)] Loss: -444641.250000\n",
      "Train Epoch: 468 [9984/17352 (58%)] Loss: -457585.187500\n",
      "Train Epoch: 468 [11392/17352 (66%)] Loss: -499874.281250\n",
      "Train Epoch: 468 [12800/17352 (74%)] Loss: -487657.500000\n",
      "Train Epoch: 468 [14208/17352 (82%)] Loss: -469492.593750\n",
      "Train Epoch: 468 [15522/17352 (89%)] Loss: -289175.156250\n",
      "Train Epoch: 468 [16223/17352 (93%)] Loss: -276788.250000\n",
      "Train Epoch: 468 [17060/17352 (98%)] Loss: -210185.265625\n",
      "    epoch          : 468\n",
      "    loss           : -409807.0296639052\n",
      "    val_loss       : -224495.67807617187\n",
      "Train Epoch: 469 [128/17352 (1%)] Loss: -482659.562500\n",
      "Train Epoch: 469 [1536/17352 (9%)] Loss: -479725.156250\n",
      "Train Epoch: 469 [2944/17352 (17%)] Loss: -326891.250000\n",
      "Train Epoch: 469 [4352/17352 (25%)] Loss: -418851.187500\n",
      "Train Epoch: 469 [5760/17352 (33%)] Loss: -480134.281250\n",
      "Train Epoch: 469 [7168/17352 (41%)] Loss: -428445.781250\n",
      "Train Epoch: 469 [8576/17352 (49%)] Loss: -443809.281250\n",
      "Train Epoch: 469 [9984/17352 (58%)] Loss: -475498.562500\n",
      "Train Epoch: 469 [11392/17352 (66%)] Loss: -456503.625000\n",
      "Train Epoch: 469 [12800/17352 (74%)] Loss: -492992.281250\n",
      "Train Epoch: 469 [14208/17352 (82%)] Loss: -446335.250000\n",
      "Train Epoch: 469 [15525/17352 (89%)] Loss: -183395.125000\n",
      "Train Epoch: 469 [16257/17352 (94%)] Loss: -174756.593750\n",
      "Train Epoch: 469 [17005/17352 (98%)] Loss: -127190.601562\n",
      "    epoch          : 469\n",
      "    loss           : -414293.43571072776\n",
      "    val_loss       : -216497.8483561198\n",
      "Train Epoch: 470 [128/17352 (1%)] Loss: -485683.937500\n",
      "Train Epoch: 470 [1536/17352 (9%)] Loss: -371584.781250\n",
      "Train Epoch: 470 [2944/17352 (17%)] Loss: -332120.500000\n",
      "Train Epoch: 470 [4352/17352 (25%)] Loss: -478556.750000\n",
      "Train Epoch: 470 [5760/17352 (33%)] Loss: -478503.937500\n",
      "Train Epoch: 470 [7168/17352 (41%)] Loss: -478729.625000\n",
      "Train Epoch: 470 [8576/17352 (49%)] Loss: -458168.531250\n",
      "Train Epoch: 470 [9984/17352 (58%)] Loss: -469612.750000\n",
      "Train Epoch: 470 [11392/17352 (66%)] Loss: -491987.187500\n",
      "Train Epoch: 470 [12800/17352 (74%)] Loss: -480055.750000\n",
      "Train Epoch: 470 [14208/17352 (82%)] Loss: -473807.156250\n",
      "Train Epoch: 470 [15523/17352 (89%)] Loss: -309068.375000\n",
      "Train Epoch: 470 [16287/17352 (94%)] Loss: -380403.218750\n",
      "Train Epoch: 470 [17125/17352 (99%)] Loss: -41944.628906\n",
      "    epoch          : 470\n",
      "    loss           : -408748.6306627517\n",
      "    val_loss       : -231753.19314778646\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch470.pth ...\n",
      "Train Epoch: 471 [128/17352 (1%)] Loss: -487927.843750\n",
      "Train Epoch: 471 [1536/17352 (9%)] Loss: -408168.031250\n",
      "Train Epoch: 471 [2944/17352 (17%)] Loss: -478381.531250\n",
      "Train Epoch: 471 [4352/17352 (25%)] Loss: -477696.906250\n",
      "Train Epoch: 471 [5760/17352 (33%)] Loss: -454757.687500\n",
      "Train Epoch: 471 [7168/17352 (41%)] Loss: -454777.125000\n",
      "Train Epoch: 471 [8576/17352 (49%)] Loss: -471231.375000\n",
      "Train Epoch: 471 [9984/17352 (58%)] Loss: -440262.531250\n",
      "Train Epoch: 471 [11392/17352 (66%)] Loss: -480536.812500\n",
      "Train Epoch: 471 [12800/17352 (74%)] Loss: -498033.812500\n",
      "Train Epoch: 471 [14208/17352 (82%)] Loss: -473547.343750\n",
      "Train Epoch: 471 [15521/17352 (89%)] Loss: -304323.625000\n",
      "Train Epoch: 471 [16184/17352 (93%)] Loss: -56947.445312\n",
      "Train Epoch: 471 [16987/17352 (98%)] Loss: -298112.000000\n",
      "    epoch          : 471\n",
      "    loss           : -414475.9513684983\n",
      "    val_loss       : -227711.52856445312\n",
      "Train Epoch: 472 [128/17352 (1%)] Loss: -452425.500000\n",
      "Train Epoch: 472 [1536/17352 (9%)] Loss: -494209.000000\n",
      "Train Epoch: 472 [2944/17352 (17%)] Loss: -379396.406250\n",
      "Train Epoch: 472 [4352/17352 (25%)] Loss: -481558.031250\n",
      "Train Epoch: 472 [5760/17352 (33%)] Loss: -417481.687500\n",
      "Train Epoch: 472 [7168/17352 (41%)] Loss: -483696.406250\n",
      "Train Epoch: 472 [8576/17352 (49%)] Loss: -475975.375000\n",
      "Train Epoch: 472 [9984/17352 (58%)] Loss: -321475.781250\n",
      "Train Epoch: 472 [11392/17352 (66%)] Loss: -405925.281250\n",
      "Train Epoch: 472 [12800/17352 (74%)] Loss: -318462.156250\n",
      "Train Epoch: 472 [14208/17352 (82%)] Loss: -444024.500000\n",
      "Train Epoch: 472 [15444/17352 (89%)] Loss: -168958.546875\n",
      "Train Epoch: 472 [16148/17352 (93%)] Loss: -229898.328125\n",
      "Train Epoch: 472 [16999/17352 (98%)] Loss: -288274.750000\n",
      "    epoch          : 472\n",
      "    loss           : -406552.40493917785\n",
      "    val_loss       : -226887.7161295573\n",
      "Train Epoch: 473 [128/17352 (1%)] Loss: -461836.531250\n",
      "Train Epoch: 473 [1536/17352 (9%)] Loss: -473069.750000\n",
      "Train Epoch: 473 [2944/17352 (17%)] Loss: -508256.750000\n",
      "Train Epoch: 473 [4352/17352 (25%)] Loss: -449421.125000\n",
      "Train Epoch: 473 [5760/17352 (33%)] Loss: -480821.312500\n",
      "Train Epoch: 473 [7168/17352 (41%)] Loss: -509890.312500\n",
      "Train Epoch: 473 [8576/17352 (49%)] Loss: -479652.343750\n",
      "Train Epoch: 473 [9984/17352 (58%)] Loss: -447474.687500\n",
      "Train Epoch: 473 [11392/17352 (66%)] Loss: -448347.062500\n",
      "Train Epoch: 473 [12800/17352 (74%)] Loss: -488387.375000\n",
      "Train Epoch: 473 [14208/17352 (82%)] Loss: -399164.000000\n",
      "Train Epoch: 473 [15529/17352 (89%)] Loss: -277276.593750\n",
      "Train Epoch: 473 [16375/17352 (94%)] Loss: -148936.453125\n",
      "Train Epoch: 473 [17077/17352 (98%)] Loss: -395508.500000\n",
      "    epoch          : 473\n",
      "    loss           : -405899.616951552\n",
      "    val_loss       : -224228.2212402344\n",
      "Train Epoch: 474 [128/17352 (1%)] Loss: -447022.625000\n",
      "Train Epoch: 474 [1536/17352 (9%)] Loss: -451145.312500\n",
      "Train Epoch: 474 [2944/17352 (17%)] Loss: -484347.937500\n",
      "Train Epoch: 474 [4352/17352 (25%)] Loss: -483427.906250\n",
      "Train Epoch: 474 [5760/17352 (33%)] Loss: -502557.937500\n",
      "Train Epoch: 474 [7168/17352 (41%)] Loss: -492360.656250\n",
      "Train Epoch: 474 [8576/17352 (49%)] Loss: -489912.312500\n",
      "Train Epoch: 474 [9984/17352 (58%)] Loss: -428879.875000\n",
      "Train Epoch: 474 [11392/17352 (66%)] Loss: -489185.906250\n",
      "Train Epoch: 474 [12800/17352 (74%)] Loss: -339090.937500\n",
      "Train Epoch: 474 [14208/17352 (82%)] Loss: -484779.125000\n",
      "Train Epoch: 474 [15467/17352 (89%)] Loss: -267087.906250\n",
      "Train Epoch: 474 [16373/17352 (94%)] Loss: -216550.593750\n",
      "Train Epoch: 474 [17048/17352 (98%)] Loss: -17513.406250\n",
      "    epoch          : 474\n",
      "    loss           : -408337.1025226772\n",
      "    val_loss       : -227870.4694498698\n",
      "Train Epoch: 475 [128/17352 (1%)] Loss: -420223.468750\n",
      "Train Epoch: 475 [1536/17352 (9%)] Loss: -382364.812500\n",
      "Train Epoch: 475 [2944/17352 (17%)] Loss: -490371.062500\n",
      "Train Epoch: 475 [4352/17352 (25%)] Loss: -452687.687500\n",
      "Train Epoch: 475 [5760/17352 (33%)] Loss: -454693.375000\n",
      "Train Epoch: 475 [7168/17352 (41%)] Loss: -430839.812500\n",
      "Train Epoch: 475 [8576/17352 (49%)] Loss: -464257.000000\n",
      "Train Epoch: 475 [9984/17352 (58%)] Loss: -463309.937500\n",
      "Train Epoch: 475 [11392/17352 (66%)] Loss: -473541.500000\n",
      "Train Epoch: 475 [12800/17352 (74%)] Loss: -478525.437500\n",
      "Train Epoch: 475 [14208/17352 (82%)] Loss: -398388.781250\n",
      "Train Epoch: 475 [15574/17352 (90%)] Loss: -394185.562500\n",
      "Train Epoch: 475 [16258/17352 (94%)] Loss: -443319.125000\n",
      "Train Epoch: 475 [16991/17352 (98%)] Loss: -197334.781250\n",
      "    epoch          : 475\n",
      "    loss           : -400702.965374633\n",
      "    val_loss       : -224639.2328450521\n",
      "Train Epoch: 476 [128/17352 (1%)] Loss: -419191.156250\n",
      "Train Epoch: 476 [1536/17352 (9%)] Loss: -447105.500000\n",
      "Train Epoch: 476 [2944/17352 (17%)] Loss: -480218.250000\n",
      "Train Epoch: 476 [4352/17352 (25%)] Loss: -468634.812500\n",
      "Train Epoch: 476 [5760/17352 (33%)] Loss: -448594.312500\n",
      "Train Epoch: 476 [7168/17352 (41%)] Loss: -320910.281250\n",
      "Train Epoch: 476 [8576/17352 (49%)] Loss: -390643.093750\n",
      "Train Epoch: 476 [9984/17352 (58%)] Loss: -455544.843750\n",
      "Train Epoch: 476 [11392/17352 (66%)] Loss: -458842.812500\n",
      "Train Epoch: 476 [12800/17352 (74%)] Loss: -436017.437500\n",
      "Train Epoch: 476 [14208/17352 (82%)] Loss: -430034.750000\n",
      "Train Epoch: 476 [15491/17352 (89%)] Loss: -149646.718750\n",
      "Train Epoch: 476 [16393/17352 (94%)] Loss: -262274.187500\n",
      "Train Epoch: 476 [17045/17352 (98%)] Loss: -128869.718750\n",
      "    epoch          : 476\n",
      "    loss           : -398369.10963716445\n",
      "    val_loss       : -218346.9323079427\n",
      "Train Epoch: 477 [128/17352 (1%)] Loss: -209800.578125\n",
      "Train Epoch: 477 [1536/17352 (9%)] Loss: -428990.562500\n",
      "Train Epoch: 477 [2944/17352 (17%)] Loss: -470200.531250\n",
      "Train Epoch: 477 [4352/17352 (25%)] Loss: -426226.406250\n",
      "Train Epoch: 477 [5760/17352 (33%)] Loss: -480649.625000\n",
      "Train Epoch: 477 [7168/17352 (41%)] Loss: -334672.218750\n",
      "Train Epoch: 477 [8576/17352 (49%)] Loss: -468069.843750\n",
      "Train Epoch: 477 [9984/17352 (58%)] Loss: -458508.156250\n",
      "Train Epoch: 477 [11392/17352 (66%)] Loss: -450340.625000\n",
      "Train Epoch: 477 [12800/17352 (74%)] Loss: -462483.812500\n",
      "Train Epoch: 477 [14208/17352 (82%)] Loss: -455033.156250\n",
      "Train Epoch: 477 [15541/17352 (90%)] Loss: -327242.562500\n",
      "Train Epoch: 477 [16295/17352 (94%)] Loss: -332463.593750\n",
      "Train Epoch: 477 [17155/17352 (99%)] Loss: -329751.375000\n",
      "    epoch          : 477\n",
      "    loss           : -401121.99795511743\n",
      "    val_loss       : -224333.52080078126\n",
      "Train Epoch: 478 [128/17352 (1%)] Loss: -420208.562500\n",
      "Train Epoch: 478 [1536/17352 (9%)] Loss: -454562.250000\n",
      "Train Epoch: 478 [2944/17352 (17%)] Loss: -443652.093750\n",
      "Train Epoch: 478 [4352/17352 (25%)] Loss: -483165.937500\n",
      "Train Epoch: 478 [5760/17352 (33%)] Loss: -489731.593750\n",
      "Train Epoch: 478 [7168/17352 (41%)] Loss: -449256.656250\n",
      "Train Epoch: 478 [8576/17352 (49%)] Loss: -428922.531250\n",
      "Train Epoch: 478 [9984/17352 (58%)] Loss: -484581.187500\n",
      "Train Epoch: 478 [11392/17352 (66%)] Loss: -424765.718750\n",
      "Train Epoch: 478 [12800/17352 (74%)] Loss: -433913.062500\n",
      "Train Epoch: 478 [14208/17352 (82%)] Loss: -475451.437500\n",
      "Train Epoch: 478 [15478/17352 (89%)] Loss: -308293.500000\n",
      "Train Epoch: 478 [16253/17352 (94%)] Loss: -234200.562500\n",
      "Train Epoch: 478 [16937/17352 (98%)] Loss: -204111.062500\n",
      "    epoch          : 478\n",
      "    loss           : -401116.8694290059\n",
      "    val_loss       : -220877.86305338543\n",
      "Train Epoch: 479 [128/17352 (1%)] Loss: -447822.875000\n",
      "Train Epoch: 479 [1536/17352 (9%)] Loss: -446496.812500\n",
      "Train Epoch: 479 [2944/17352 (17%)] Loss: -468636.812500\n",
      "Train Epoch: 479 [4352/17352 (25%)] Loss: -366177.062500\n",
      "Train Epoch: 479 [5760/17352 (33%)] Loss: -443185.406250\n",
      "Train Epoch: 479 [7168/17352 (41%)] Loss: -437122.187500\n",
      "Train Epoch: 479 [8576/17352 (49%)] Loss: -470495.812500\n",
      "Train Epoch: 479 [9984/17352 (58%)] Loss: -435016.000000\n",
      "Train Epoch: 479 [11392/17352 (66%)] Loss: -470380.468750\n",
      "Train Epoch: 479 [12800/17352 (74%)] Loss: -446751.250000\n",
      "Train Epoch: 479 [14208/17352 (82%)] Loss: -511489.875000\n",
      "Train Epoch: 479 [15468/17352 (89%)] Loss: -307389.562500\n",
      "Train Epoch: 479 [16335/17352 (94%)] Loss: -244903.750000\n",
      "Train Epoch: 479 [17043/17352 (98%)] Loss: -267488.250000\n",
      "    epoch          : 479\n",
      "    loss           : -409688.187382026\n",
      "    val_loss       : -222053.240234375\n",
      "Train Epoch: 480 [128/17352 (1%)] Loss: -408839.625000\n",
      "Train Epoch: 480 [1536/17352 (9%)] Loss: -478892.562500\n",
      "Train Epoch: 480 [2944/17352 (17%)] Loss: -415820.625000\n",
      "Train Epoch: 480 [4352/17352 (25%)] Loss: -482796.812500\n",
      "Train Epoch: 480 [5760/17352 (33%)] Loss: -439239.875000\n",
      "Train Epoch: 480 [7168/17352 (41%)] Loss: -411543.843750\n",
      "Train Epoch: 480 [8576/17352 (49%)] Loss: -479828.000000\n",
      "Train Epoch: 480 [9984/17352 (58%)] Loss: -476325.562500\n",
      "Train Epoch: 480 [11392/17352 (66%)] Loss: -483178.312500\n",
      "Train Epoch: 480 [12800/17352 (74%)] Loss: -449682.000000\n",
      "Train Epoch: 480 [14208/17352 (82%)] Loss: -451315.187500\n",
      "Train Epoch: 480 [15522/17352 (89%)] Loss: -316208.562500\n",
      "Train Epoch: 480 [16208/17352 (93%)] Loss: -307963.000000\n",
      "Train Epoch: 480 [17080/17352 (98%)] Loss: -19217.226562\n",
      "    epoch          : 480\n",
      "    loss           : -410952.7633048448\n",
      "    val_loss       : -221012.3646972656\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch480.pth ...\n",
      "Train Epoch: 481 [128/17352 (1%)] Loss: -467862.625000\n",
      "Train Epoch: 481 [1536/17352 (9%)] Loss: -451990.625000\n",
      "Train Epoch: 481 [2944/17352 (17%)] Loss: -470936.031250\n",
      "Train Epoch: 481 [4352/17352 (25%)] Loss: -383426.750000\n",
      "Train Epoch: 481 [5760/17352 (33%)] Loss: -496390.593750\n",
      "Train Epoch: 481 [7168/17352 (41%)] Loss: -484358.375000\n",
      "Train Epoch: 481 [8576/17352 (49%)] Loss: -365467.000000\n",
      "Train Epoch: 481 [9984/17352 (58%)] Loss: -488350.000000\n",
      "Train Epoch: 481 [11392/17352 (66%)] Loss: -434670.750000\n",
      "Train Epoch: 481 [12800/17352 (74%)] Loss: -300689.125000\n",
      "Train Epoch: 481 [14208/17352 (82%)] Loss: -444354.031250\n",
      "Train Epoch: 481 [15514/17352 (89%)] Loss: -278928.593750\n",
      "Train Epoch: 481 [16301/17352 (94%)] Loss: -202198.609375\n",
      "Train Epoch: 481 [17085/17352 (98%)] Loss: -369637.000000\n",
      "    epoch          : 481\n",
      "    loss           : -406698.3248151741\n",
      "    val_loss       : -226960.64710286458\n",
      "Train Epoch: 482 [128/17352 (1%)] Loss: -377880.343750\n",
      "Train Epoch: 482 [1536/17352 (9%)] Loss: -392873.250000\n",
      "Train Epoch: 482 [2944/17352 (17%)] Loss: -475121.468750\n",
      "Train Epoch: 482 [4352/17352 (25%)] Loss: -449741.875000\n",
      "Train Epoch: 482 [5760/17352 (33%)] Loss: -439901.281250\n",
      "Train Epoch: 482 [7168/17352 (41%)] Loss: -431126.875000\n",
      "Train Epoch: 482 [8576/17352 (49%)] Loss: -481545.937500\n",
      "Train Epoch: 482 [9984/17352 (58%)] Loss: -489634.593750\n",
      "Train Epoch: 482 [11392/17352 (66%)] Loss: -263356.562500\n",
      "Train Epoch: 482 [12800/17352 (74%)] Loss: -484762.843750\n",
      "Train Epoch: 482 [14208/17352 (82%)] Loss: -410930.937500\n",
      "Train Epoch: 482 [15572/17352 (90%)] Loss: -414553.937500\n",
      "Train Epoch: 482 [16313/17352 (94%)] Loss: -274744.031250\n",
      "Train Epoch: 482 [16987/17352 (98%)] Loss: -176812.296875\n",
      "    epoch          : 482\n",
      "    loss           : -407189.0892932047\n",
      "    val_loss       : -225128.66686197917\n",
      "Train Epoch: 483 [128/17352 (1%)] Loss: -422479.437500\n",
      "Train Epoch: 483 [1536/17352 (9%)] Loss: -492202.718750\n",
      "Train Epoch: 483 [2944/17352 (17%)] Loss: -445427.562500\n",
      "Train Epoch: 483 [4352/17352 (25%)] Loss: -437671.437500\n",
      "Train Epoch: 483 [5760/17352 (33%)] Loss: -467059.250000\n",
      "Train Epoch: 483 [7168/17352 (41%)] Loss: -433548.375000\n",
      "Train Epoch: 483 [8576/17352 (49%)] Loss: -491701.781250\n",
      "Train Epoch: 483 [9984/17352 (58%)] Loss: -479280.937500\n",
      "Train Epoch: 483 [11392/17352 (66%)] Loss: -459731.312500\n",
      "Train Epoch: 483 [12800/17352 (74%)] Loss: -429129.093750\n",
      "Train Epoch: 483 [14208/17352 (82%)] Loss: -432877.875000\n",
      "Train Epoch: 483 [15533/17352 (90%)] Loss: -355436.187500\n",
      "Train Epoch: 483 [16325/17352 (94%)] Loss: -188669.656250\n",
      "Train Epoch: 483 [17011/17352 (98%)] Loss: -212549.515625\n",
      "    epoch          : 483\n",
      "    loss           : -409309.02528575924\n",
      "    val_loss       : -226724.39138997396\n",
      "Train Epoch: 484 [128/17352 (1%)] Loss: -377800.687500\n",
      "Train Epoch: 484 [1536/17352 (9%)] Loss: -449963.125000\n",
      "Train Epoch: 484 [2944/17352 (17%)] Loss: -444882.437500\n",
      "Train Epoch: 484 [4352/17352 (25%)] Loss: -489535.937500\n",
      "Train Epoch: 484 [5760/17352 (33%)] Loss: -468934.781250\n",
      "Train Epoch: 484 [7168/17352 (41%)] Loss: -461139.062500\n",
      "Train Epoch: 484 [8576/17352 (49%)] Loss: -449301.187500\n",
      "Train Epoch: 484 [9984/17352 (58%)] Loss: -442045.906250\n",
      "Train Epoch: 484 [11392/17352 (66%)] Loss: -437652.125000\n",
      "Train Epoch: 484 [12800/17352 (74%)] Loss: -483192.531250\n",
      "Train Epoch: 484 [14208/17352 (82%)] Loss: -465561.343750\n",
      "Train Epoch: 484 [15547/17352 (90%)] Loss: -302028.843750\n",
      "Train Epoch: 484 [16380/17352 (94%)] Loss: -407336.375000\n",
      "Train Epoch: 484 [17097/17352 (99%)] Loss: -284593.375000\n",
      "    epoch          : 484\n",
      "    loss           : -410021.7563443792\n",
      "    val_loss       : -223463.5899576823\n",
      "Train Epoch: 485 [128/17352 (1%)] Loss: -482666.187500\n",
      "Train Epoch: 485 [1536/17352 (9%)] Loss: -486122.687500\n",
      "Train Epoch: 485 [2944/17352 (17%)] Loss: -289713.312500\n",
      "Train Epoch: 485 [4352/17352 (25%)] Loss: -475102.531250\n",
      "Train Epoch: 485 [5760/17352 (33%)] Loss: -416969.187500\n",
      "Train Epoch: 485 [7168/17352 (41%)] Loss: -476793.687500\n",
      "Train Epoch: 485 [8576/17352 (49%)] Loss: -476799.875000\n",
      "Train Epoch: 485 [9984/17352 (58%)] Loss: -432125.562500\n",
      "Train Epoch: 485 [11392/17352 (66%)] Loss: -389390.437500\n",
      "Train Epoch: 485 [12800/17352 (74%)] Loss: -486815.093750\n",
      "Train Epoch: 485 [14208/17352 (82%)] Loss: -477361.125000\n",
      "Train Epoch: 485 [15519/17352 (89%)] Loss: -202486.781250\n",
      "Train Epoch: 485 [16277/17352 (94%)] Loss: -257395.375000\n",
      "Train Epoch: 485 [16999/17352 (98%)] Loss: -280760.750000\n",
      "    epoch          : 485\n",
      "    loss           : -414488.3264602559\n",
      "    val_loss       : -230475.51217447917\n",
      "Train Epoch: 486 [128/17352 (1%)] Loss: -343592.343750\n",
      "Train Epoch: 486 [1536/17352 (9%)] Loss: -470387.750000\n",
      "Train Epoch: 486 [2944/17352 (17%)] Loss: -330454.781250\n",
      "Train Epoch: 486 [4352/17352 (25%)] Loss: -438080.406250\n",
      "Train Epoch: 486 [5760/17352 (33%)] Loss: -504439.781250\n",
      "Train Epoch: 486 [7168/17352 (41%)] Loss: -483599.375000\n",
      "Train Epoch: 486 [8576/17352 (49%)] Loss: -487953.875000\n",
      "Train Epoch: 486 [9984/17352 (58%)] Loss: -452897.093750\n",
      "Train Epoch: 486 [11392/17352 (66%)] Loss: -465632.843750\n",
      "Train Epoch: 486 [12800/17352 (74%)] Loss: -485337.656250\n",
      "Train Epoch: 486 [14208/17352 (82%)] Loss: -453386.406250\n",
      "Train Epoch: 486 [15451/17352 (89%)] Loss: -49687.421875\n",
      "Train Epoch: 486 [16364/17352 (94%)] Loss: -350040.125000\n",
      "Train Epoch: 486 [17004/17352 (98%)] Loss: -135424.500000\n",
      "    epoch          : 486\n",
      "    loss           : -410881.51609689597\n",
      "    val_loss       : -220291.04007161458\n",
      "Train Epoch: 487 [128/17352 (1%)] Loss: -359978.000000\n",
      "Train Epoch: 487 [1536/17352 (9%)] Loss: -481506.281250\n",
      "Train Epoch: 487 [2944/17352 (17%)] Loss: -482857.812500\n",
      "Train Epoch: 487 [4352/17352 (25%)] Loss: -488495.218750\n",
      "Train Epoch: 487 [5760/17352 (33%)] Loss: -464558.687500\n",
      "Train Epoch: 487 [7168/17352 (41%)] Loss: -412408.125000\n",
      "Train Epoch: 487 [8576/17352 (49%)] Loss: -486198.468750\n",
      "Train Epoch: 487 [9984/17352 (58%)] Loss: -478318.218750\n",
      "Train Epoch: 487 [11392/17352 (66%)] Loss: -404612.343750\n",
      "Train Epoch: 487 [12800/17352 (74%)] Loss: -449696.218750\n",
      "Train Epoch: 487 [14208/17352 (82%)] Loss: -476104.687500\n",
      "Train Epoch: 487 [15448/17352 (89%)] Loss: -27824.328125\n",
      "Train Epoch: 487 [16313/17352 (94%)] Loss: -338641.312500\n",
      "Train Epoch: 487 [16998/17352 (98%)] Loss: -199870.031250\n",
      "    epoch          : 487\n",
      "    loss           : -408891.299359008\n",
      "    val_loss       : -226855.30662434894\n",
      "Train Epoch: 488 [128/17352 (1%)] Loss: -465659.562500\n",
      "Train Epoch: 488 [1536/17352 (9%)] Loss: -499533.187500\n",
      "Train Epoch: 488 [2944/17352 (17%)] Loss: -353114.562500\n",
      "Train Epoch: 488 [4352/17352 (25%)] Loss: -493308.312500\n",
      "Train Epoch: 488 [5760/17352 (33%)] Loss: -458248.250000\n",
      "Train Epoch: 488 [7168/17352 (41%)] Loss: -464939.656250\n",
      "Train Epoch: 488 [8576/17352 (49%)] Loss: -501251.031250\n",
      "Train Epoch: 488 [9984/17352 (58%)] Loss: -451641.875000\n",
      "Train Epoch: 488 [11392/17352 (66%)] Loss: -504432.375000\n",
      "Train Epoch: 488 [12800/17352 (74%)] Loss: -451670.250000\n",
      "Train Epoch: 488 [14208/17352 (82%)] Loss: -480439.781250\n",
      "Train Epoch: 488 [15485/17352 (89%)] Loss: -195023.593750\n",
      "Train Epoch: 488 [16153/17352 (93%)] Loss: -307379.968750\n",
      "Train Epoch: 488 [17111/17352 (99%)] Loss: -122161.375000\n",
      "    epoch          : 488\n",
      "    loss           : -410519.09410392196\n",
      "    val_loss       : -227694.1380859375\n",
      "Train Epoch: 489 [128/17352 (1%)] Loss: -454265.000000\n",
      "Train Epoch: 489 [1536/17352 (9%)] Loss: -479591.906250\n",
      "Train Epoch: 489 [2944/17352 (17%)] Loss: -465865.125000\n",
      "Train Epoch: 489 [4352/17352 (25%)] Loss: -460442.500000\n",
      "Train Epoch: 489 [5760/17352 (33%)] Loss: -490565.687500\n",
      "Train Epoch: 489 [7168/17352 (41%)] Loss: -400743.156250\n",
      "Train Epoch: 489 [8576/17352 (49%)] Loss: -451520.281250\n",
      "Train Epoch: 489 [9984/17352 (58%)] Loss: -472688.593750\n",
      "Train Epoch: 489 [11392/17352 (66%)] Loss: -461773.406250\n",
      "Train Epoch: 489 [12800/17352 (74%)] Loss: -412551.125000\n",
      "Train Epoch: 489 [14208/17352 (82%)] Loss: -438955.062500\n",
      "Train Epoch: 489 [15397/17352 (89%)] Loss: -12339.765625\n",
      "Train Epoch: 489 [16156/17352 (93%)] Loss: -331225.781250\n",
      "Train Epoch: 489 [16852/17352 (97%)] Loss: -15629.750977\n",
      "    epoch          : 489\n",
      "    loss           : -405605.75144845847\n",
      "    val_loss       : -231464.5437825521\n",
      "Train Epoch: 490 [128/17352 (1%)] Loss: -366197.156250\n",
      "Train Epoch: 490 [1536/17352 (9%)] Loss: -451356.562500\n",
      "Train Epoch: 490 [2944/17352 (17%)] Loss: -451447.250000\n",
      "Train Epoch: 490 [4352/17352 (25%)] Loss: -500645.437500\n",
      "Train Epoch: 490 [5760/17352 (33%)] Loss: -477065.500000\n",
      "Train Epoch: 490 [7168/17352 (41%)] Loss: -470318.281250\n",
      "Train Epoch: 490 [8576/17352 (49%)] Loss: -376026.343750\n",
      "Train Epoch: 490 [9984/17352 (58%)] Loss: -393216.250000\n",
      "Train Epoch: 490 [11392/17352 (66%)] Loss: -478087.312500\n",
      "Train Epoch: 490 [12800/17352 (74%)] Loss: -451404.000000\n",
      "Train Epoch: 490 [14208/17352 (82%)] Loss: -402045.687500\n",
      "Train Epoch: 490 [15551/17352 (90%)] Loss: -359514.218750\n",
      "Train Epoch: 490 [16280/17352 (94%)] Loss: -258877.093750\n",
      "Train Epoch: 490 [17013/17352 (98%)] Loss: -254020.625000\n",
      "    epoch          : 490\n",
      "    loss           : -412324.0214057257\n",
      "    val_loss       : -225148.52628580728\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch490.pth ...\n",
      "Train Epoch: 491 [128/17352 (1%)] Loss: -440558.593750\n",
      "Train Epoch: 491 [1536/17352 (9%)] Loss: -446158.156250\n",
      "Train Epoch: 491 [2944/17352 (17%)] Loss: -472471.968750\n",
      "Train Epoch: 491 [4352/17352 (25%)] Loss: -452940.593750\n",
      "Train Epoch: 491 [5760/17352 (33%)] Loss: -465041.812500\n",
      "Train Epoch: 491 [7168/17352 (41%)] Loss: -456908.187500\n",
      "Train Epoch: 491 [8576/17352 (49%)] Loss: -361929.156250\n",
      "Train Epoch: 491 [9984/17352 (58%)] Loss: -463762.875000\n",
      "Train Epoch: 491 [11392/17352 (66%)] Loss: -474861.875000\n",
      "Train Epoch: 491 [12800/17352 (74%)] Loss: -466349.312500\n",
      "Train Epoch: 491 [14208/17352 (82%)] Loss: -473827.875000\n",
      "Train Epoch: 491 [15417/17352 (89%)] Loss: -176217.171875\n",
      "Train Epoch: 491 [16114/17352 (93%)] Loss: -181813.156250\n",
      "Train Epoch: 491 [16895/17352 (97%)] Loss: -290502.687500\n",
      "    epoch          : 491\n",
      "    loss           : -404236.30429425335\n",
      "    val_loss       : -218550.41370442708\n",
      "Train Epoch: 492 [128/17352 (1%)] Loss: -447347.875000\n",
      "Train Epoch: 492 [1536/17352 (9%)] Loss: -464547.093750\n",
      "Train Epoch: 492 [2944/17352 (17%)] Loss: -471492.750000\n",
      "Train Epoch: 492 [4352/17352 (25%)] Loss: -485042.937500\n",
      "Train Epoch: 492 [5760/17352 (33%)] Loss: -386557.343750\n",
      "Train Epoch: 492 [7168/17352 (41%)] Loss: -295610.812500\n",
      "Train Epoch: 492 [8576/17352 (49%)] Loss: -466926.718750\n",
      "Train Epoch: 492 [9984/17352 (58%)] Loss: -479339.875000\n",
      "Train Epoch: 492 [11392/17352 (66%)] Loss: -436317.937500\n",
      "Train Epoch: 492 [12800/17352 (74%)] Loss: -471977.062500\n",
      "Train Epoch: 492 [14208/17352 (82%)] Loss: -397999.875000\n",
      "Train Epoch: 492 [15368/17352 (89%)] Loss: -19428.236328\n",
      "Train Epoch: 492 [16106/17352 (93%)] Loss: -316140.625000\n",
      "Train Epoch: 492 [16982/17352 (98%)] Loss: -128009.414062\n",
      "    epoch          : 492\n",
      "    loss           : -408827.9417274014\n",
      "    val_loss       : -228435.9528483073\n",
      "Train Epoch: 493 [128/17352 (1%)] Loss: -288196.843750\n",
      "Train Epoch: 493 [1536/17352 (9%)] Loss: -400236.625000\n",
      "Train Epoch: 493 [2944/17352 (17%)] Loss: -350772.093750\n",
      "Train Epoch: 493 [4352/17352 (25%)] Loss: -477818.437500\n",
      "Train Epoch: 493 [5760/17352 (33%)] Loss: -458893.031250\n",
      "Train Epoch: 493 [7168/17352 (41%)] Loss: -290576.156250\n",
      "Train Epoch: 493 [8576/17352 (49%)] Loss: -492424.062500\n",
      "Train Epoch: 493 [9984/17352 (58%)] Loss: -472989.312500\n",
      "Train Epoch: 493 [11392/17352 (66%)] Loss: -482820.093750\n",
      "Train Epoch: 493 [12800/17352 (74%)] Loss: -486256.000000\n",
      "Train Epoch: 493 [14208/17352 (82%)] Loss: -468476.531250\n",
      "Train Epoch: 493 [15440/17352 (89%)] Loss: -252617.875000\n",
      "Train Epoch: 493 [16187/17352 (93%)] Loss: -294886.031250\n",
      "Train Epoch: 493 [17008/17352 (98%)] Loss: -273598.250000\n",
      "    epoch          : 493\n",
      "    loss           : -406260.5206192324\n",
      "    val_loss       : -218917.11848958334\n",
      "Train Epoch: 494 [128/17352 (1%)] Loss: -497339.375000\n",
      "Train Epoch: 494 [1536/17352 (9%)] Loss: -461877.906250\n",
      "Train Epoch: 494 [2944/17352 (17%)] Loss: -424858.625000\n",
      "Train Epoch: 494 [4352/17352 (25%)] Loss: -453311.531250\n",
      "Train Epoch: 494 [5760/17352 (33%)] Loss: -435290.500000\n",
      "Train Epoch: 494 [7168/17352 (41%)] Loss: -350634.625000\n",
      "Train Epoch: 494 [8576/17352 (49%)] Loss: -498079.093750\n",
      "Train Epoch: 494 [9984/17352 (58%)] Loss: -388546.500000\n",
      "Train Epoch: 494 [11392/17352 (66%)] Loss: -404065.406250\n",
      "Train Epoch: 494 [12800/17352 (74%)] Loss: -438315.000000\n",
      "Train Epoch: 494 [14208/17352 (82%)] Loss: -494663.781250\n",
      "Train Epoch: 494 [15519/17352 (89%)] Loss: -338409.687500\n",
      "Train Epoch: 494 [16230/17352 (94%)] Loss: -276155.312500\n",
      "Train Epoch: 494 [16991/17352 (98%)] Loss: -113734.992188\n",
      "    epoch          : 494\n",
      "    loss           : -409077.5166081166\n",
      "    val_loss       : -225515.121484375\n",
      "Train Epoch: 495 [128/17352 (1%)] Loss: -486233.500000\n",
      "Train Epoch: 495 [1536/17352 (9%)] Loss: -458651.625000\n",
      "Train Epoch: 495 [2944/17352 (17%)] Loss: -473007.718750\n",
      "Train Epoch: 495 [4352/17352 (25%)] Loss: -461642.031250\n",
      "Train Epoch: 495 [5760/17352 (33%)] Loss: -467465.093750\n",
      "Train Epoch: 495 [7168/17352 (41%)] Loss: -490694.937500\n",
      "Train Epoch: 495 [8576/17352 (49%)] Loss: -501372.500000\n",
      "Train Epoch: 495 [9984/17352 (58%)] Loss: -485169.000000\n",
      "Train Epoch: 495 [11392/17352 (66%)] Loss: -462848.562500\n",
      "Train Epoch: 495 [12800/17352 (74%)] Loss: -453329.625000\n",
      "Train Epoch: 495 [14208/17352 (82%)] Loss: -464698.437500\n",
      "Train Epoch: 495 [15448/17352 (89%)] Loss: -193416.156250\n",
      "Train Epoch: 495 [16046/17352 (92%)] Loss: -99537.289062\n",
      "Train Epoch: 495 [16922/17352 (98%)] Loss: -330768.406250\n",
      "    epoch          : 495\n",
      "    loss           : -415848.47335753986\n",
      "    val_loss       : -218826.98413085938\n",
      "Train Epoch: 496 [128/17352 (1%)] Loss: -475047.531250\n",
      "Train Epoch: 496 [1536/17352 (9%)] Loss: -494538.937500\n",
      "Train Epoch: 496 [2944/17352 (17%)] Loss: -470709.562500\n",
      "Train Epoch: 496 [4352/17352 (25%)] Loss: -418315.812500\n",
      "Train Epoch: 496 [5760/17352 (33%)] Loss: -496843.500000\n",
      "Train Epoch: 496 [7168/17352 (41%)] Loss: -390174.343750\n",
      "Train Epoch: 496 [8576/17352 (49%)] Loss: -468446.125000\n",
      "Train Epoch: 496 [9984/17352 (58%)] Loss: -406675.781250\n",
      "Train Epoch: 496 [11392/17352 (66%)] Loss: -474272.375000\n",
      "Train Epoch: 496 [12800/17352 (74%)] Loss: -457254.312500\n",
      "Train Epoch: 496 [14208/17352 (82%)] Loss: -425353.187500\n",
      "Train Epoch: 496 [15500/17352 (89%)] Loss: -167079.875000\n",
      "Train Epoch: 496 [16401/17352 (95%)] Loss: -299878.750000\n",
      "Train Epoch: 496 [16890/17352 (97%)] Loss: -180936.312500\n",
      "    epoch          : 496\n",
      "    loss           : -405253.6988713821\n",
      "    val_loss       : -223443.69907226562\n",
      "Train Epoch: 497 [128/17352 (1%)] Loss: -471857.500000\n",
      "Train Epoch: 497 [1536/17352 (9%)] Loss: -433823.468750\n",
      "Train Epoch: 497 [2944/17352 (17%)] Loss: -491954.656250\n",
      "Train Epoch: 497 [4352/17352 (25%)] Loss: -463670.093750\n",
      "Train Epoch: 497 [5760/17352 (33%)] Loss: -481386.843750\n",
      "Train Epoch: 497 [7168/17352 (41%)] Loss: -412520.125000\n",
      "Train Epoch: 497 [8576/17352 (49%)] Loss: -372010.156250\n",
      "Train Epoch: 497 [9984/17352 (58%)] Loss: -477724.375000\n",
      "Train Epoch: 497 [11392/17352 (66%)] Loss: -475373.281250\n",
      "Train Epoch: 497 [12800/17352 (74%)] Loss: -473872.687500\n",
      "Train Epoch: 497 [14208/17352 (82%)] Loss: -403900.687500\n",
      "Train Epoch: 497 [15472/17352 (89%)] Loss: -113630.828125\n",
      "Train Epoch: 497 [16263/17352 (94%)] Loss: -271659.250000\n",
      "Train Epoch: 497 [17090/17352 (98%)] Loss: -188855.281250\n",
      "    epoch          : 497\n",
      "    loss           : -414876.85861472314\n",
      "    val_loss       : -232409.13121744792\n",
      "Train Epoch: 498 [128/17352 (1%)] Loss: -457527.562500\n",
      "Train Epoch: 498 [1536/17352 (9%)] Loss: -432230.375000\n",
      "Train Epoch: 498 [2944/17352 (17%)] Loss: -280468.437500\n",
      "Train Epoch: 498 [4352/17352 (25%)] Loss: -471272.437500\n",
      "Train Epoch: 498 [5760/17352 (33%)] Loss: -501143.687500\n",
      "Train Epoch: 498 [7168/17352 (41%)] Loss: -473542.812500\n",
      "Train Epoch: 498 [8576/17352 (49%)] Loss: -493851.375000\n",
      "Train Epoch: 498 [9984/17352 (58%)] Loss: -455512.031250\n",
      "Train Epoch: 498 [11392/17352 (66%)] Loss: -450754.218750\n",
      "Train Epoch: 498 [12800/17352 (74%)] Loss: -422284.937500\n",
      "Train Epoch: 498 [14208/17352 (82%)] Loss: -463233.281250\n",
      "Train Epoch: 498 [15528/17352 (89%)] Loss: -270079.781250\n",
      "Train Epoch: 498 [16182/17352 (93%)] Loss: -374244.781250\n",
      "Train Epoch: 498 [16944/17352 (98%)] Loss: -117759.906250\n",
      "    epoch          : 498\n",
      "    loss           : -418600.9008821833\n",
      "    val_loss       : -225063.73930664064\n",
      "Train Epoch: 499 [128/17352 (1%)] Loss: -411780.593750\n",
      "Train Epoch: 499 [1536/17352 (9%)] Loss: -487670.281250\n",
      "Train Epoch: 499 [2944/17352 (17%)] Loss: -494995.625000\n",
      "Train Epoch: 499 [4352/17352 (25%)] Loss: -472731.562500\n",
      "Train Epoch: 499 [5760/17352 (33%)] Loss: -505171.875000\n",
      "Train Epoch: 499 [7168/17352 (41%)] Loss: -412557.718750\n",
      "Train Epoch: 499 [8576/17352 (49%)] Loss: -491899.156250\n",
      "Train Epoch: 499 [9984/17352 (58%)] Loss: -462111.906250\n",
      "Train Epoch: 499 [11392/17352 (66%)] Loss: -478234.625000\n",
      "Train Epoch: 499 [12800/17352 (74%)] Loss: -506650.187500\n",
      "Train Epoch: 499 [14208/17352 (82%)] Loss: -447688.250000\n",
      "Train Epoch: 499 [15534/17352 (90%)] Loss: -309803.250000\n",
      "Train Epoch: 499 [16369/17352 (94%)] Loss: -112763.398438\n",
      "Train Epoch: 499 [16924/17352 (98%)] Loss: -150925.656250\n",
      "    epoch          : 499\n",
      "    loss           : -408923.48122247274\n",
      "    val_loss       : -231221.73696289063\n",
      "Train Epoch: 500 [128/17352 (1%)] Loss: -494505.125000\n",
      "Train Epoch: 500 [1536/17352 (9%)] Loss: -490178.750000\n",
      "Train Epoch: 500 [2944/17352 (17%)] Loss: -481394.656250\n",
      "Train Epoch: 500 [4352/17352 (25%)] Loss: -508432.562500\n",
      "Train Epoch: 500 [5760/17352 (33%)] Loss: -480849.375000\n",
      "Train Epoch: 500 [7168/17352 (41%)] Loss: -490030.343750\n",
      "Train Epoch: 500 [8576/17352 (49%)] Loss: -418395.718750\n",
      "Train Epoch: 500 [9984/17352 (58%)] Loss: -315655.500000\n",
      "Train Epoch: 500 [11392/17352 (66%)] Loss: -479681.031250\n",
      "Train Epoch: 500 [12800/17352 (74%)] Loss: -496141.937500\n",
      "Train Epoch: 500 [14208/17352 (82%)] Loss: -417842.000000\n",
      "Train Epoch: 500 [15506/17352 (89%)] Loss: -403590.406250\n",
      "Train Epoch: 500 [16235/17352 (94%)] Loss: -303438.937500\n",
      "Train Epoch: 500 [17015/17352 (98%)] Loss: -414880.375000\n",
      "    epoch          : 500\n",
      "    loss           : -416700.9048932991\n",
      "    val_loss       : -231603.29278971354\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/0921_213351/checkpoint-epoch500.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
