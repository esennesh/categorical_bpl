{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='omniglot_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.StepLR({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"step_size\": 500,\n",
    "    \"gamma\": 0.1,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [128/17352 (1%)] Loss: 156084.656250\n",
      "Train Epoch: 1 [1536/17352 (9%)] Loss: 144727.000000\n",
      "Train Epoch: 1 [2944/17352 (17%)] Loss: 121160.765625\n",
      "Train Epoch: 1 [4352/17352 (25%)] Loss: 131154.453125\n",
      "Train Epoch: 1 [5760/17352 (33%)] Loss: 112790.750000\n",
      "Train Epoch: 1 [7168/17352 (41%)] Loss: 214601.734375\n",
      "Train Epoch: 1 [8576/17352 (49%)] Loss: 180091.109375\n",
      "Train Epoch: 1 [9984/17352 (58%)] Loss: 130796.515625\n",
      "Train Epoch: 1 [11392/17352 (66%)] Loss: 220483.812500\n",
      "Train Epoch: 1 [12800/17352 (74%)] Loss: 84912.031250\n",
      "Train Epoch: 1 [14208/17352 (82%)] Loss: 97091.000000\n",
      "Train Epoch: 1 [15487/17352 (89%)] Loss: 41555.039062\n",
      "Train Epoch: 1 [16269/17352 (94%)] Loss: 50394.253906\n",
      "Train Epoch: 1 [17008/17352 (98%)] Loss: 130562.960938\n",
      "    epoch          : 1\n",
      "    loss           : 126307.14979747798\n",
      "    val_loss       : 56277.06922200521\n",
      "Train Epoch: 2 [128/17352 (1%)] Loss: 106973.382812\n",
      "Train Epoch: 2 [1536/17352 (9%)] Loss: 85905.453125\n",
      "Train Epoch: 2 [2944/17352 (17%)] Loss: 48964.335938\n",
      "Train Epoch: 2 [4352/17352 (25%)] Loss: 112224.867188\n",
      "Train Epoch: 2 [5760/17352 (33%)] Loss: 155547.781250\n",
      "Train Epoch: 2 [7168/17352 (41%)] Loss: 80671.062500\n",
      "Train Epoch: 2 [8576/17352 (49%)] Loss: 88858.500000\n",
      "Train Epoch: 2 [9984/17352 (58%)] Loss: 130449.000000\n",
      "Train Epoch: 2 [11392/17352 (66%)] Loss: 52064.058594\n",
      "Train Epoch: 2 [12800/17352 (74%)] Loss: 105576.117188\n",
      "Train Epoch: 2 [14208/17352 (82%)] Loss: 48401.671875\n",
      "Train Epoch: 2 [15459/17352 (89%)] Loss: 52689.898438\n",
      "Train Epoch: 2 [16220/17352 (93%)] Loss: 88163.937500\n",
      "Train Epoch: 2 [16997/17352 (98%)] Loss: 12902.015625\n",
      "    epoch          : 2\n",
      "    loss           : 92695.33510103161\n",
      "    val_loss       : 42059.679227701825\n",
      "Train Epoch: 3 [128/17352 (1%)] Loss: 107375.953125\n",
      "Train Epoch: 3 [1536/17352 (9%)] Loss: 159058.250000\n",
      "Train Epoch: 3 [2944/17352 (17%)] Loss: 106932.281250\n",
      "Train Epoch: 3 [4352/17352 (25%)] Loss: 51175.984375\n",
      "Train Epoch: 3 [5760/17352 (33%)] Loss: 60029.320312\n",
      "Train Epoch: 3 [7168/17352 (41%)] Loss: 69765.242188\n",
      "Train Epoch: 3 [8576/17352 (49%)] Loss: 127517.546875\n",
      "Train Epoch: 3 [9984/17352 (58%)] Loss: 11247.930664\n",
      "Train Epoch: 3 [11392/17352 (66%)] Loss: 150815.718750\n",
      "Train Epoch: 3 [12800/17352 (74%)] Loss: 67930.500000\n",
      "Train Epoch: 3 [14208/17352 (82%)] Loss: 78193.000000\n",
      "Train Epoch: 3 [15540/17352 (90%)] Loss: 60212.769531\n",
      "Train Epoch: 3 [16240/17352 (94%)] Loss: 33038.660156\n",
      "Train Epoch: 3 [17053/17352 (98%)] Loss: 34508.117188\n",
      "    epoch          : 3\n",
      "    loss           : 75136.43868137847\n",
      "    val_loss       : 36169.97866210937\n",
      "Train Epoch: 4 [128/17352 (1%)] Loss: 110197.187500\n",
      "Train Epoch: 4 [1536/17352 (9%)] Loss: 7454.653320\n",
      "Train Epoch: 4 [2944/17352 (17%)] Loss: 62734.960938\n",
      "Train Epoch: 4 [4352/17352 (25%)] Loss: 91673.640625\n",
      "Train Epoch: 4 [5760/17352 (33%)] Loss: 27207.449219\n",
      "Train Epoch: 4 [7168/17352 (41%)] Loss: 24093.150391\n",
      "Train Epoch: 4 [8576/17352 (49%)] Loss: 85135.265625\n",
      "Train Epoch: 4 [9984/17352 (58%)] Loss: 89912.320312\n",
      "Train Epoch: 4 [11392/17352 (66%)] Loss: 52908.398438\n",
      "Train Epoch: 4 [12800/17352 (74%)] Loss: 68038.117188\n",
      "Train Epoch: 4 [14208/17352 (82%)] Loss: 17588.283203\n",
      "Train Epoch: 4 [15529/17352 (89%)] Loss: 44864.453125\n",
      "Train Epoch: 4 [16407/17352 (95%)] Loss: 35625.820312\n",
      "Train Epoch: 4 [17022/17352 (98%)] Loss: 8458.036133\n",
      "    epoch          : 4\n",
      "    loss           : 56033.124547356725\n",
      "    val_loss       : 37116.48905436198\n",
      "Train Epoch: 5 [128/17352 (1%)] Loss: 53831.925781\n",
      "Train Epoch: 5 [1536/17352 (9%)] Loss: 16656.173828\n",
      "Train Epoch: 5 [2944/17352 (17%)] Loss: 85878.968750\n",
      "Train Epoch: 5 [4352/17352 (25%)] Loss: 61911.562500\n",
      "Train Epoch: 5 [5760/17352 (33%)] Loss: 128469.414062\n",
      "Train Epoch: 5 [7168/17352 (41%)] Loss: 51204.734375\n",
      "Train Epoch: 5 [8576/17352 (49%)] Loss: 16845.564453\n",
      "Train Epoch: 5 [9984/17352 (58%)] Loss: 32543.546875\n",
      "Train Epoch: 5 [11392/17352 (66%)] Loss: 186466.125000\n",
      "Train Epoch: 5 [12800/17352 (74%)] Loss: 22892.441406\n",
      "Train Epoch: 5 [14208/17352 (82%)] Loss: 47859.343750\n",
      "Train Epoch: 5 [15560/17352 (90%)] Loss: 138782.093750\n",
      "Train Epoch: 5 [16304/17352 (94%)] Loss: 6266.209961\n",
      "Train Epoch: 5 [17007/17352 (98%)] Loss: 50351.148438\n",
      "    epoch          : 5\n",
      "    loss           : 47069.28637756757\n",
      "    val_loss       : 32447.252062988282\n",
      "Train Epoch: 6 [128/17352 (1%)] Loss: 56324.945312\n",
      "Train Epoch: 6 [1536/17352 (9%)] Loss: 49895.664062\n",
      "Train Epoch: 6 [2944/17352 (17%)] Loss: 29448.097656\n",
      "Train Epoch: 6 [4352/17352 (25%)] Loss: 9000.395508\n",
      "Train Epoch: 6 [5760/17352 (33%)] Loss: 15546.585938\n",
      "Train Epoch: 6 [7168/17352 (41%)] Loss: 136282.375000\n",
      "Train Epoch: 6 [8576/17352 (49%)] Loss: 170623.109375\n",
      "Train Epoch: 6 [9984/17352 (58%)] Loss: 53051.648438\n",
      "Train Epoch: 6 [11392/17352 (66%)] Loss: 49849.265625\n",
      "Train Epoch: 6 [12800/17352 (74%)] Loss: 64232.847656\n",
      "Train Epoch: 6 [14208/17352 (82%)] Loss: 23453.134766\n",
      "Train Epoch: 6 [15467/17352 (89%)] Loss: 25128.679688\n",
      "Train Epoch: 6 [16242/17352 (94%)] Loss: 25594.853516\n",
      "Train Epoch: 6 [16925/17352 (98%)] Loss: -29.020508\n",
      "    epoch          : 6\n",
      "    loss           : 40921.68400215302\n",
      "    val_loss       : 19714.132963053384\n",
      "Train Epoch: 7 [128/17352 (1%)] Loss: 42851.062500\n",
      "Train Epoch: 7 [1536/17352 (9%)] Loss: 93785.156250\n",
      "Train Epoch: 7 [2944/17352 (17%)] Loss: 9038.314453\n",
      "Train Epoch: 7 [4352/17352 (25%)] Loss: 95396.789062\n",
      "Train Epoch: 7 [5760/17352 (33%)] Loss: 49601.515625\n",
      "Train Epoch: 7 [7168/17352 (41%)] Loss: 10430.515625\n",
      "Train Epoch: 7 [8576/17352 (49%)] Loss: 59421.945312\n",
      "Train Epoch: 7 [9984/17352 (58%)] Loss: 89832.757812\n",
      "Train Epoch: 7 [11392/17352 (66%)] Loss: 89990.812500\n",
      "Train Epoch: 7 [12800/17352 (74%)] Loss: -23635.916016\n",
      "Train Epoch: 7 [14208/17352 (82%)] Loss: 23501.859375\n",
      "Train Epoch: 7 [15490/17352 (89%)] Loss: 48069.054688\n",
      "Train Epoch: 7 [16331/17352 (94%)] Loss: 3516.014648\n",
      "Train Epoch: 7 [17104/17352 (99%)] Loss: 24003.097656\n",
      "    epoch          : 7\n",
      "    loss           : 32874.60399571361\n",
      "    val_loss       : 19966.674279785155\n",
      "Train Epoch: 8 [128/17352 (1%)] Loss: 79014.312500\n",
      "Train Epoch: 8 [1536/17352 (9%)] Loss: 23633.220703\n",
      "Train Epoch: 8 [2944/17352 (17%)] Loss: 6842.177246\n",
      "Train Epoch: 8 [4352/17352 (25%)] Loss: 2267.783203\n",
      "Train Epoch: 8 [5760/17352 (33%)] Loss: 6214.406738\n",
      "Train Epoch: 8 [7168/17352 (41%)] Loss: 39012.460938\n",
      "Train Epoch: 8 [8576/17352 (49%)] Loss: 8462.601562\n",
      "Train Epoch: 8 [9984/17352 (58%)] Loss: 30960.968750\n",
      "Train Epoch: 8 [11392/17352 (66%)] Loss: 10790.708984\n",
      "Train Epoch: 8 [12800/17352 (74%)] Loss: -6840.625000\n",
      "Train Epoch: 8 [14208/17352 (82%)] Loss: 56616.820312\n",
      "Train Epoch: 8 [15536/17352 (90%)] Loss: 46850.683594\n",
      "Train Epoch: 8 [16239/17352 (94%)] Loss: 19336.974609\n",
      "Train Epoch: 8 [16988/17352 (98%)] Loss: -3937.627441\n",
      "    epoch          : 8\n",
      "    loss           : 19860.15073041468\n",
      "    val_loss       : 11578.317110188802\n",
      "Train Epoch: 9 [128/17352 (1%)] Loss: 68019.203125\n",
      "Train Epoch: 9 [1536/17352 (9%)] Loss: 59301.460938\n",
      "Train Epoch: 9 [2944/17352 (17%)] Loss: 64566.449219\n",
      "Train Epoch: 9 [4352/17352 (25%)] Loss: 33469.253906\n",
      "Train Epoch: 9 [5760/17352 (33%)] Loss: 43645.707031\n",
      "Train Epoch: 9 [7168/17352 (41%)] Loss: 21684.212891\n",
      "Train Epoch: 9 [8576/17352 (49%)] Loss: 49714.789062\n",
      "Train Epoch: 9 [9984/17352 (58%)] Loss: -5120.990234\n",
      "Train Epoch: 9 [11392/17352 (66%)] Loss: 25026.798828\n",
      "Train Epoch: 9 [12800/17352 (74%)] Loss: 30930.515625\n",
      "Train Epoch: 9 [14208/17352 (82%)] Loss: 34386.195312\n",
      "Train Epoch: 9 [15546/17352 (90%)] Loss: 60416.921875\n",
      "Train Epoch: 9 [16404/17352 (95%)] Loss: -5781.592773\n",
      "Train Epoch: 9 [17091/17352 (98%)] Loss: -6982.530273\n",
      "    epoch          : 9\n",
      "    loss           : 21157.272944303168\n",
      "    val_loss       : 11740.712548828125\n",
      "Train Epoch: 10 [128/17352 (1%)] Loss: 6309.460938\n",
      "Train Epoch: 10 [1536/17352 (9%)] Loss: 54229.761719\n",
      "Train Epoch: 10 [2944/17352 (17%)] Loss: 38309.273438\n",
      "Train Epoch: 10 [4352/17352 (25%)] Loss: 56505.382812\n",
      "Train Epoch: 10 [5760/17352 (33%)] Loss: 8740.681641\n",
      "Train Epoch: 10 [7168/17352 (41%)] Loss: 7815.116211\n",
      "Train Epoch: 10 [8576/17352 (49%)] Loss: -17451.283203\n",
      "Train Epoch: 10 [9984/17352 (58%)] Loss: 64840.511719\n",
      "Train Epoch: 10 [11392/17352 (66%)] Loss: 42899.578125\n",
      "Train Epoch: 10 [12800/17352 (74%)] Loss: -21690.714844\n",
      "Train Epoch: 10 [14208/17352 (82%)] Loss: 62813.046875\n",
      "Train Epoch: 10 [15546/17352 (90%)] Loss: 60410.812500\n",
      "Train Epoch: 10 [16148/17352 (93%)] Loss: 344.087402\n",
      "Train Epoch: 10 [17002/17352 (98%)] Loss: -7458.493164\n",
      "    epoch          : 10\n",
      "    loss           : 16726.328328689473\n",
      "    val_loss       : 12199.926501464844\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch10.pth ...\n",
      "Train Epoch: 11 [128/17352 (1%)] Loss: 66770.000000\n",
      "Train Epoch: 11 [1536/17352 (9%)] Loss: 22016.324219\n",
      "Train Epoch: 11 [2944/17352 (17%)] Loss: -27536.181641\n",
      "Train Epoch: 11 [4352/17352 (25%)] Loss: 15219.217773\n",
      "Train Epoch: 11 [5760/17352 (33%)] Loss: -3924.951172\n",
      "Train Epoch: 11 [7168/17352 (41%)] Loss: -41463.730469\n",
      "Train Epoch: 11 [8576/17352 (49%)] Loss: 17231.017578\n",
      "Train Epoch: 11 [9984/17352 (58%)] Loss: 12561.454102\n",
      "Train Epoch: 11 [11392/17352 (66%)] Loss: 56329.578125\n",
      "Train Epoch: 11 [12800/17352 (74%)] Loss: 14543.395508\n",
      "Train Epoch: 11 [14208/17352 (82%)] Loss: -6449.043945\n",
      "Train Epoch: 11 [15409/17352 (89%)] Loss: 1389.506470\n",
      "Train Epoch: 11 [16196/17352 (93%)] Loss: -2814.980957\n",
      "Train Epoch: 11 [16933/17352 (98%)] Loss: -22206.525391\n",
      "    epoch          : 11\n",
      "    loss           : 14569.051710704829\n",
      "    val_loss       : 9743.870137532553\n",
      "Train Epoch: 12 [128/17352 (1%)] Loss: 57965.082031\n",
      "Train Epoch: 12 [1536/17352 (9%)] Loss: 17913.966797\n",
      "Train Epoch: 12 [2944/17352 (17%)] Loss: -23760.275391\n",
      "Train Epoch: 12 [4352/17352 (25%)] Loss: 25801.869141\n",
      "Train Epoch: 12 [5760/17352 (33%)] Loss: 47669.953125\n",
      "Train Epoch: 12 [7168/17352 (41%)] Loss: -9526.485352\n",
      "Train Epoch: 12 [8576/17352 (49%)] Loss: 17871.527344\n",
      "Train Epoch: 12 [9984/17352 (58%)] Loss: 74299.257812\n",
      "Train Epoch: 12 [11392/17352 (66%)] Loss: -9941.731445\n",
      "Train Epoch: 12 [12800/17352 (74%)] Loss: -25273.189453\n",
      "Train Epoch: 12 [14208/17352 (82%)] Loss: 20268.568359\n",
      "Train Epoch: 12 [15530/17352 (89%)] Loss: -1692.968750\n",
      "Train Epoch: 12 [16105/17352 (93%)] Loss: -1100.720215\n",
      "Train Epoch: 12 [16955/17352 (98%)] Loss: -9994.623047\n",
      "    epoch          : 12\n",
      "    loss           : 8531.219728201027\n",
      "    val_loss       : 1668.7586791992187\n",
      "Train Epoch: 13 [128/17352 (1%)] Loss: 2659.325195\n",
      "Train Epoch: 13 [1536/17352 (9%)] Loss: -29424.013672\n",
      "Train Epoch: 13 [2944/17352 (17%)] Loss: 27253.406250\n",
      "Train Epoch: 13 [4352/17352 (25%)] Loss: -5046.902344\n",
      "Train Epoch: 13 [5760/17352 (33%)] Loss: 600.547852\n",
      "Train Epoch: 13 [7168/17352 (41%)] Loss: -12921.669922\n",
      "Train Epoch: 13 [8576/17352 (49%)] Loss: -6858.952637\n",
      "Train Epoch: 13 [9984/17352 (58%)] Loss: 2076.512695\n",
      "Train Epoch: 13 [11392/17352 (66%)] Loss: -24543.859375\n",
      "Train Epoch: 13 [12800/17352 (74%)] Loss: -6071.594727\n",
      "Train Epoch: 13 [14208/17352 (82%)] Loss: -16335.146484\n",
      "Train Epoch: 13 [15423/17352 (89%)] Loss: 5081.322754\n",
      "Train Epoch: 13 [16080/17352 (93%)] Loss: -604.392090\n",
      "Train Epoch: 13 [16946/17352 (98%)] Loss: -5513.882812\n",
      "    epoch          : 13\n",
      "    loss           : 2637.066740304832\n",
      "    val_loss       : -725.3816162109375\n",
      "Train Epoch: 14 [128/17352 (1%)] Loss: 50772.476562\n",
      "Train Epoch: 14 [1536/17352 (9%)] Loss: 19183.042969\n",
      "Train Epoch: 14 [2944/17352 (17%)] Loss: 42978.710938\n",
      "Train Epoch: 14 [4352/17352 (25%)] Loss: -7125.275391\n",
      "Train Epoch: 14 [5760/17352 (33%)] Loss: 30943.771484\n",
      "Train Epoch: 14 [7168/17352 (41%)] Loss: -9987.435547\n",
      "Train Epoch: 14 [8576/17352 (49%)] Loss: -18148.039062\n",
      "Train Epoch: 14 [9984/17352 (58%)] Loss: 73527.390625\n",
      "Train Epoch: 14 [11392/17352 (66%)] Loss: -8123.619141\n",
      "Train Epoch: 14 [12800/17352 (74%)] Loss: -5743.826172\n",
      "Train Epoch: 14 [14208/17352 (82%)] Loss: -39940.578125\n",
      "Train Epoch: 14 [15519/17352 (89%)] Loss: -1336.649414\n",
      "Train Epoch: 14 [16351/17352 (94%)] Loss: -13528.064453\n",
      "Train Epoch: 14 [17070/17352 (98%)] Loss: -8531.332031\n",
      "    epoch          : 14\n",
      "    loss           : -292.85609159533607\n",
      "    val_loss       : 1010.0221110026042\n",
      "Train Epoch: 15 [128/17352 (1%)] Loss: -28002.402344\n",
      "Train Epoch: 15 [1536/17352 (9%)] Loss: 23211.394531\n",
      "Train Epoch: 15 [2944/17352 (17%)] Loss: -12820.498047\n",
      "Train Epoch: 15 [4352/17352 (25%)] Loss: 11689.415039\n",
      "Train Epoch: 15 [5760/17352 (33%)] Loss: -20980.281250\n",
      "Train Epoch: 15 [7168/17352 (41%)] Loss: 30298.458984\n",
      "Train Epoch: 15 [8576/17352 (49%)] Loss: -35614.582031\n",
      "Train Epoch: 15 [9984/17352 (58%)] Loss: 4066.613281\n",
      "Train Epoch: 15 [11392/17352 (66%)] Loss: 12926.722656\n",
      "Train Epoch: 15 [12800/17352 (74%)] Loss: -24106.750000\n",
      "Train Epoch: 15 [14208/17352 (82%)] Loss: -10824.283203\n",
      "Train Epoch: 15 [15453/17352 (89%)] Loss: 23231.605469\n",
      "Train Epoch: 15 [16265/17352 (94%)] Loss: -8479.886719\n",
      "Train Epoch: 15 [16930/17352 (98%)] Loss: -10227.640625\n",
      "    epoch          : 15\n",
      "    loss           : -8859.297817972683\n",
      "    val_loss       : -7331.075773111979\n",
      "Train Epoch: 16 [128/17352 (1%)] Loss: 33850.976562\n",
      "Train Epoch: 16 [1536/17352 (9%)] Loss: 3754.220703\n",
      "Train Epoch: 16 [2944/17352 (17%)] Loss: 52684.632812\n",
      "Train Epoch: 16 [4352/17352 (25%)] Loss: -2434.634766\n",
      "Train Epoch: 16 [5760/17352 (33%)] Loss: -8846.621094\n",
      "Train Epoch: 16 [7168/17352 (41%)] Loss: -65379.617188\n",
      "Train Epoch: 16 [8576/17352 (49%)] Loss: 4269.067383\n",
      "Train Epoch: 16 [9984/17352 (58%)] Loss: -12190.777344\n",
      "Train Epoch: 16 [11392/17352 (66%)] Loss: -26149.705078\n",
      "Train Epoch: 16 [12800/17352 (74%)] Loss: 18910.099609\n",
      "Train Epoch: 16 [14208/17352 (82%)] Loss: -23485.875000\n",
      "Train Epoch: 16 [15485/17352 (89%)] Loss: 4108.677734\n",
      "Train Epoch: 16 [16351/17352 (94%)] Loss: -44247.457031\n",
      "Train Epoch: 16 [16956/17352 (98%)] Loss: -11800.463867\n",
      "    epoch          : 16\n",
      "    loss           : -10891.930241936805\n",
      "    val_loss       : -4731.478653971354\n",
      "Train Epoch: 17 [128/17352 (1%)] Loss: -31240.167969\n",
      "Train Epoch: 17 [1536/17352 (9%)] Loss: -47518.062500\n",
      "Train Epoch: 17 [2944/17352 (17%)] Loss: 17882.697266\n",
      "Train Epoch: 17 [4352/17352 (25%)] Loss: -18235.750000\n",
      "Train Epoch: 17 [5760/17352 (33%)] Loss: -14929.115234\n",
      "Train Epoch: 17 [7168/17352 (41%)] Loss: -50811.230469\n",
      "Train Epoch: 17 [8576/17352 (49%)] Loss: -17358.148438\n",
      "Train Epoch: 17 [9984/17352 (58%)] Loss: -14093.331055\n",
      "Train Epoch: 17 [11392/17352 (66%)] Loss: 40085.671875\n",
      "Train Epoch: 17 [12800/17352 (74%)] Loss: -47988.367188\n",
      "Train Epoch: 17 [14208/17352 (82%)] Loss: -8828.146484\n",
      "Train Epoch: 17 [15495/17352 (89%)] Loss: -6047.826660\n",
      "Train Epoch: 17 [16305/17352 (94%)] Loss: 11496.256836\n",
      "Train Epoch: 17 [17086/17352 (98%)] Loss: -21979.812500\n",
      "    epoch          : 17\n",
      "    loss           : -10351.900897339687\n",
      "    val_loss       : -2471.2228434244794\n",
      "Train Epoch: 18 [128/17352 (1%)] Loss: 17581.613281\n",
      "Train Epoch: 18 [1536/17352 (9%)] Loss: -41020.406250\n",
      "Train Epoch: 18 [2944/17352 (17%)] Loss: -44549.753906\n",
      "Train Epoch: 18 [4352/17352 (25%)] Loss: -15956.027344\n",
      "Train Epoch: 18 [5760/17352 (33%)] Loss: -21746.082031\n",
      "Train Epoch: 18 [7168/17352 (41%)] Loss: -51466.160156\n",
      "Train Epoch: 18 [8576/17352 (49%)] Loss: 69448.984375\n",
      "Train Epoch: 18 [9984/17352 (58%)] Loss: -24737.384766\n",
      "Train Epoch: 18 [11392/17352 (66%)] Loss: 3715.276367\n",
      "Train Epoch: 18 [12800/17352 (74%)] Loss: -26682.529297\n",
      "Train Epoch: 18 [14208/17352 (82%)] Loss: 21930.058594\n",
      "Train Epoch: 18 [15490/17352 (89%)] Loss: -569.515198\n",
      "Train Epoch: 18 [16351/17352 (94%)] Loss: 13131.078125\n",
      "Train Epoch: 18 [17012/17352 (98%)] Loss: -40141.222656\n",
      "    epoch          : 18\n",
      "    loss           : -15982.62456292274\n",
      "    val_loss       : -5452.43994140625\n",
      "Train Epoch: 19 [128/17352 (1%)] Loss: -33648.648438\n",
      "Train Epoch: 19 [1536/17352 (9%)] Loss: 11862.038086\n",
      "Train Epoch: 19 [2944/17352 (17%)] Loss: -20524.486328\n",
      "Train Epoch: 19 [4352/17352 (25%)] Loss: 66113.812500\n",
      "Train Epoch: 19 [5760/17352 (33%)] Loss: 25590.113281\n",
      "Train Epoch: 19 [7168/17352 (41%)] Loss: -19025.166016\n",
      "Train Epoch: 19 [8576/17352 (49%)] Loss: -15032.333984\n",
      "Train Epoch: 19 [9984/17352 (58%)] Loss: 16258.832031\n",
      "Train Epoch: 19 [11392/17352 (66%)] Loss: -32321.802734\n",
      "Train Epoch: 19 [12800/17352 (74%)] Loss: 15142.277344\n",
      "Train Epoch: 19 [14208/17352 (82%)] Loss: -32617.242188\n",
      "Train Epoch: 19 [15471/17352 (89%)] Loss: -3370.807617\n",
      "Train Epoch: 19 [16199/17352 (93%)] Loss: 13909.412109\n",
      "Train Epoch: 19 [16965/17352 (98%)] Loss: -10698.363281\n",
      "    epoch          : 19\n",
      "    loss           : -15787.3860514596\n",
      "    val_loss       : -17089.981229654946\n",
      "Train Epoch: 20 [128/17352 (1%)] Loss: -34588.558594\n",
      "Train Epoch: 20 [1536/17352 (9%)] Loss: -55959.343750\n",
      "Train Epoch: 20 [2944/17352 (17%)] Loss: 30797.431641\n",
      "Train Epoch: 20 [4352/17352 (25%)] Loss: -48386.726562\n",
      "Train Epoch: 20 [5760/17352 (33%)] Loss: -36891.703125\n",
      "Train Epoch: 20 [7168/17352 (41%)] Loss: -40601.414062\n",
      "Train Epoch: 20 [8576/17352 (49%)] Loss: -19935.615234\n",
      "Train Epoch: 20 [9984/17352 (58%)] Loss: -49419.957031\n",
      "Train Epoch: 20 [11392/17352 (66%)] Loss: -17048.000000\n",
      "Train Epoch: 20 [12800/17352 (74%)] Loss: -17256.207031\n",
      "Train Epoch: 20 [14208/17352 (82%)] Loss: 2216.130859\n",
      "Train Epoch: 20 [15519/17352 (89%)] Loss: -21739.005859\n",
      "Train Epoch: 20 [16322/17352 (94%)] Loss: -8467.961914\n",
      "Train Epoch: 20 [17032/17352 (98%)] Loss: 607.774414\n",
      "    epoch          : 20\n",
      "    loss           : -17921.872024638542\n",
      "    val_loss       : -11538.686462402344\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch20.pth ...\n",
      "Train Epoch: 21 [128/17352 (1%)] Loss: -17129.998047\n",
      "Train Epoch: 21 [1536/17352 (9%)] Loss: -3084.496094\n",
      "Train Epoch: 21 [2944/17352 (17%)] Loss: -53712.296875\n",
      "Train Epoch: 21 [4352/17352 (25%)] Loss: 41703.328125\n",
      "Train Epoch: 21 [5760/17352 (33%)] Loss: -15657.783203\n",
      "Train Epoch: 21 [7168/17352 (41%)] Loss: -50854.484375\n",
      "Train Epoch: 21 [8576/17352 (49%)] Loss: 12261.720703\n",
      "Train Epoch: 21 [9984/17352 (58%)] Loss: -31536.240234\n",
      "Train Epoch: 21 [11392/17352 (66%)] Loss: -15925.928711\n",
      "Train Epoch: 21 [12800/17352 (74%)] Loss: -20772.708984\n",
      "Train Epoch: 21 [14208/17352 (82%)] Loss: -2614.246094\n",
      "Train Epoch: 21 [15542/17352 (90%)] Loss: -11698.976562\n",
      "Train Epoch: 21 [16313/17352 (94%)] Loss: -25756.654297\n",
      "Train Epoch: 21 [16892/17352 (97%)] Loss: -1079.307373\n",
      "    epoch          : 21\n",
      "    loss           : -18261.141617538146\n",
      "    val_loss       : -10382.306103515624\n",
      "Train Epoch: 22 [128/17352 (1%)] Loss: 6463.473633\n",
      "Train Epoch: 22 [1536/17352 (9%)] Loss: 15921.058594\n",
      "Train Epoch: 22 [2944/17352 (17%)] Loss: -35290.179688\n",
      "Train Epoch: 22 [4352/17352 (25%)] Loss: -56712.605469\n",
      "Train Epoch: 22 [5760/17352 (33%)] Loss: -46836.261719\n",
      "Train Epoch: 22 [7168/17352 (41%)] Loss: -6960.794922\n",
      "Train Epoch: 22 [8576/17352 (49%)] Loss: -75554.421875\n",
      "Train Epoch: 22 [9984/17352 (58%)] Loss: -63224.046875\n",
      "Train Epoch: 22 [11392/17352 (66%)] Loss: -67931.773438\n",
      "Train Epoch: 22 [12800/17352 (74%)] Loss: -70471.898438\n",
      "Train Epoch: 22 [14208/17352 (82%)] Loss: 12999.209961\n",
      "Train Epoch: 22 [15448/17352 (89%)] Loss: 56.016815\n",
      "Train Epoch: 22 [16178/17352 (93%)] Loss: -8348.998047\n",
      "Train Epoch: 22 [17036/17352 (98%)] Loss: -6982.281250\n",
      "    epoch          : 22\n",
      "    loss           : -22643.330116835215\n",
      "    val_loss       : -12739.673364257813\n",
      "Train Epoch: 23 [128/17352 (1%)] Loss: -16264.048828\n",
      "Train Epoch: 23 [1536/17352 (9%)] Loss: -19159.804688\n",
      "Train Epoch: 23 [2944/17352 (17%)] Loss: 12156.595703\n",
      "Train Epoch: 23 [4352/17352 (25%)] Loss: 1051.296875\n",
      "Train Epoch: 23 [5760/17352 (33%)] Loss: -32711.429688\n",
      "Train Epoch: 23 [7168/17352 (41%)] Loss: -4746.862305\n",
      "Train Epoch: 23 [8576/17352 (49%)] Loss: -8860.393555\n",
      "Train Epoch: 23 [9984/17352 (58%)] Loss: -45398.359375\n",
      "Train Epoch: 23 [11392/17352 (66%)] Loss: -58170.015625\n",
      "Train Epoch: 23 [12800/17352 (74%)] Loss: -33326.609375\n",
      "Train Epoch: 23 [14208/17352 (82%)] Loss: -21206.445312\n",
      "Train Epoch: 23 [15559/17352 (90%)] Loss: 20206.490234\n",
      "Train Epoch: 23 [16250/17352 (94%)] Loss: -38651.292969\n",
      "Train Epoch: 23 [16945/17352 (98%)] Loss: -37514.859375\n",
      "    epoch          : 23\n",
      "    loss           : -24418.3626139596\n",
      "    val_loss       : -17676.965657552082\n",
      "Train Epoch: 24 [128/17352 (1%)] Loss: -68882.375000\n",
      "Train Epoch: 24 [1536/17352 (9%)] Loss: -53514.031250\n",
      "Train Epoch: 24 [2944/17352 (17%)] Loss: -67985.062500\n",
      "Train Epoch: 24 [4352/17352 (25%)] Loss: -17090.082031\n",
      "Train Epoch: 24 [5760/17352 (33%)] Loss: -86244.812500\n",
      "Train Epoch: 24 [7168/17352 (41%)] Loss: -38458.644531\n",
      "Train Epoch: 24 [8576/17352 (49%)] Loss: 31074.640625\n",
      "Train Epoch: 24 [9984/17352 (58%)] Loss: -41404.585938\n",
      "Train Epoch: 24 [11392/17352 (66%)] Loss: -49364.476562\n",
      "Train Epoch: 24 [12800/17352 (74%)] Loss: -10381.363281\n",
      "Train Epoch: 24 [14208/17352 (82%)] Loss: -49684.101562\n",
      "Train Epoch: 24 [15448/17352 (89%)] Loss: -31776.068359\n",
      "Train Epoch: 24 [16168/17352 (93%)] Loss: -11056.849609\n",
      "Train Epoch: 24 [17052/17352 (98%)] Loss: -12778.784180\n",
      "    epoch          : 24\n",
      "    loss           : -31197.557475454854\n",
      "    val_loss       : -18374.799055989584\n",
      "Train Epoch: 25 [128/17352 (1%)] Loss: -7083.935547\n",
      "Train Epoch: 25 [1536/17352 (9%)] Loss: -23603.904297\n",
      "Train Epoch: 25 [2944/17352 (17%)] Loss: -18591.082031\n",
      "Train Epoch: 25 [4352/17352 (25%)] Loss: -66988.281250\n",
      "Train Epoch: 25 [5760/17352 (33%)] Loss: 15405.638672\n",
      "Train Epoch: 25 [7168/17352 (41%)] Loss: -34491.335938\n",
      "Train Epoch: 25 [8576/17352 (49%)] Loss: -4411.485352\n",
      "Train Epoch: 25 [9984/17352 (58%)] Loss: -50296.757812\n",
      "Train Epoch: 25 [11392/17352 (66%)] Loss: -23441.093750\n",
      "Train Epoch: 25 [12800/17352 (74%)] Loss: -30324.708984\n",
      "Train Epoch: 25 [14208/17352 (82%)] Loss: -20176.445312\n",
      "Train Epoch: 25 [15551/17352 (90%)] Loss: -35963.402344\n",
      "Train Epoch: 25 [16243/17352 (94%)] Loss: 143.534760\n",
      "Train Epoch: 25 [17004/17352 (98%)] Loss: -22773.468750\n",
      "    epoch          : 25\n",
      "    loss           : -31671.38350825982\n",
      "    val_loss       : -18277.793579101562\n",
      "Train Epoch: 26 [128/17352 (1%)] Loss: -39806.390625\n",
      "Train Epoch: 26 [1536/17352 (9%)] Loss: -39977.179688\n",
      "Train Epoch: 26 [2944/17352 (17%)] Loss: 20198.253906\n",
      "Train Epoch: 26 [4352/17352 (25%)] Loss: -71642.390625\n",
      "Train Epoch: 26 [5760/17352 (33%)] Loss: 6044.503906\n",
      "Train Epoch: 26 [7168/17352 (41%)] Loss: -37643.164062\n",
      "Train Epoch: 26 [8576/17352 (49%)] Loss: -17909.945312\n",
      "Train Epoch: 26 [9984/17352 (58%)] Loss: -41594.195312\n",
      "Train Epoch: 26 [11392/17352 (66%)] Loss: -48995.652344\n",
      "Train Epoch: 26 [12800/17352 (74%)] Loss: -43223.394531\n",
      "Train Epoch: 26 [14208/17352 (82%)] Loss: -50042.859375\n",
      "Train Epoch: 26 [15539/17352 (90%)] Loss: -36847.046875\n",
      "Train Epoch: 26 [16271/17352 (94%)] Loss: -6824.938965\n",
      "Train Epoch: 26 [16995/17352 (98%)] Loss: 8622.121094\n",
      "    epoch          : 26\n",
      "    loss           : -29106.712520317742\n",
      "    val_loss       : -15105.44687906901\n",
      "Train Epoch: 27 [128/17352 (1%)] Loss: -54227.820312\n",
      "Train Epoch: 27 [1536/17352 (9%)] Loss: 25395.582031\n",
      "Train Epoch: 27 [2944/17352 (17%)] Loss: -41113.507812\n",
      "Train Epoch: 27 [4352/17352 (25%)] Loss: -55946.609375\n",
      "Train Epoch: 27 [5760/17352 (33%)] Loss: -69582.312500\n",
      "Train Epoch: 27 [7168/17352 (41%)] Loss: -21886.392578\n",
      "Train Epoch: 27 [8576/17352 (49%)] Loss: -51075.382812\n",
      "Train Epoch: 27 [9984/17352 (58%)] Loss: -46479.671875\n",
      "Train Epoch: 27 [11392/17352 (66%)] Loss: -32904.812500\n",
      "Train Epoch: 27 [12800/17352 (74%)] Loss: -38117.875000\n",
      "Train Epoch: 27 [14208/17352 (82%)] Loss: -1931.260254\n",
      "Train Epoch: 27 [15560/17352 (90%)] Loss: -60951.984375\n",
      "Train Epoch: 27 [16498/17352 (95%)] Loss: -52844.570312\n",
      "Train Epoch: 27 [17024/17352 (98%)] Loss: -4944.657227\n",
      "    epoch          : 27\n",
      "    loss           : -33020.02939549388\n",
      "    val_loss       : -21595.87841796875\n",
      "Train Epoch: 28 [128/17352 (1%)] Loss: -70001.781250\n",
      "Train Epoch: 28 [1536/17352 (9%)] Loss: -90865.945312\n",
      "Train Epoch: 28 [2944/17352 (17%)] Loss: 18447.468750\n",
      "Train Epoch: 28 [4352/17352 (25%)] Loss: -67346.179688\n",
      "Train Epoch: 28 [5760/17352 (33%)] Loss: -74547.523438\n",
      "Train Epoch: 28 [7168/17352 (41%)] Loss: -59736.109375\n",
      "Train Epoch: 28 [8576/17352 (49%)] Loss: -39192.242188\n",
      "Train Epoch: 28 [9984/17352 (58%)] Loss: -53091.972656\n",
      "Train Epoch: 28 [11392/17352 (66%)] Loss: -16531.435547\n",
      "Train Epoch: 28 [12800/17352 (74%)] Loss: -55090.812500\n",
      "Train Epoch: 28 [14208/17352 (82%)] Loss: 2256.739258\n",
      "Train Epoch: 28 [15419/17352 (89%)] Loss: -5902.602539\n",
      "Train Epoch: 28 [16093/17352 (93%)] Loss: -76514.523438\n",
      "Train Epoch: 28 [16874/17352 (97%)] Loss: -45911.945312\n",
      "    epoch          : 28\n",
      "    loss           : -33908.135246532875\n",
      "    val_loss       : -14876.627282714844\n",
      "Train Epoch: 29 [128/17352 (1%)] Loss: -62794.738281\n",
      "Train Epoch: 29 [1536/17352 (9%)] Loss: -37819.750000\n",
      "Train Epoch: 29 [2944/17352 (17%)] Loss: -44056.621094\n",
      "Train Epoch: 29 [4352/17352 (25%)] Loss: -1383.767578\n",
      "Train Epoch: 29 [5760/17352 (33%)] Loss: -42596.492188\n",
      "Train Epoch: 29 [7168/17352 (41%)] Loss: -37751.578125\n",
      "Train Epoch: 29 [8576/17352 (49%)] Loss: -50582.023438\n",
      "Train Epoch: 29 [9984/17352 (58%)] Loss: -75166.570312\n",
      "Train Epoch: 29 [11392/17352 (66%)] Loss: -54299.835938\n",
      "Train Epoch: 29 [12800/17352 (74%)] Loss: 10516.966797\n",
      "Train Epoch: 29 [14208/17352 (82%)] Loss: -65067.996094\n",
      "Train Epoch: 29 [15553/17352 (90%)] Loss: -48087.250000\n",
      "Train Epoch: 29 [16331/17352 (94%)] Loss: -6501.275879\n",
      "Train Epoch: 29 [16986/17352 (98%)] Loss: -6657.443359\n",
      "    epoch          : 29\n",
      "    loss           : -36579.640169898936\n",
      "    val_loss       : -23224.93244018555\n",
      "Train Epoch: 30 [128/17352 (1%)] Loss: -74544.875000\n",
      "Train Epoch: 30 [1536/17352 (9%)] Loss: -50199.304688\n",
      "Train Epoch: 30 [2944/17352 (17%)] Loss: -52660.062500\n",
      "Train Epoch: 30 [4352/17352 (25%)] Loss: -47433.640625\n",
      "Train Epoch: 30 [5760/17352 (33%)] Loss: -50781.515625\n",
      "Train Epoch: 30 [7168/17352 (41%)] Loss: -70589.929688\n",
      "Train Epoch: 30 [8576/17352 (49%)] Loss: -67512.539062\n",
      "Train Epoch: 30 [9984/17352 (58%)] Loss: -56971.796875\n",
      "Train Epoch: 30 [11392/17352 (66%)] Loss: -62707.292969\n",
      "Train Epoch: 30 [12800/17352 (74%)] Loss: -62362.007812\n",
      "Train Epoch: 30 [14208/17352 (82%)] Loss: -31531.654297\n",
      "Train Epoch: 30 [15388/17352 (89%)] Loss: -1837.307739\n",
      "Train Epoch: 30 [16167/17352 (93%)] Loss: -20200.898438\n",
      "Train Epoch: 30 [17000/17352 (98%)] Loss: -49193.921875\n",
      "    epoch          : 30\n",
      "    loss           : -39030.970710908005\n",
      "    val_loss       : -19115.747308349608\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch30.pth ...\n",
      "Train Epoch: 31 [128/17352 (1%)] Loss: -47571.363281\n",
      "Train Epoch: 31 [1536/17352 (9%)] Loss: 13224.957031\n",
      "Train Epoch: 31 [2944/17352 (17%)] Loss: 37966.355469\n",
      "Train Epoch: 31 [4352/17352 (25%)] Loss: -69821.679688\n",
      "Train Epoch: 31 [5760/17352 (33%)] Loss: -50188.867188\n",
      "Train Epoch: 31 [7168/17352 (41%)] Loss: -77561.351562\n",
      "Train Epoch: 31 [8576/17352 (49%)] Loss: -55603.480469\n",
      "Train Epoch: 31 [9984/17352 (58%)] Loss: -96527.734375\n",
      "Train Epoch: 31 [11392/17352 (66%)] Loss: -67127.664062\n",
      "Train Epoch: 31 [12800/17352 (74%)] Loss: -71697.679688\n",
      "Train Epoch: 31 [14208/17352 (82%)] Loss: -47825.125000\n",
      "Train Epoch: 31 [15546/17352 (90%)] Loss: -10405.786133\n",
      "Train Epoch: 31 [16377/17352 (94%)] Loss: -14831.185547\n",
      "Train Epoch: 31 [17123/17352 (99%)] Loss: -35944.832031\n",
      "    epoch          : 31\n",
      "    loss           : -38549.082557217385\n",
      "    val_loss       : -22910.563006591798\n",
      "Train Epoch: 32 [128/17352 (1%)] Loss: -77643.289062\n",
      "Train Epoch: 32 [1536/17352 (9%)] Loss: -93225.632812\n",
      "Train Epoch: 32 [2944/17352 (17%)] Loss: 1786.534180\n",
      "Train Epoch: 32 [4352/17352 (25%)] Loss: -9319.619141\n",
      "Train Epoch: 32 [5760/17352 (33%)] Loss: -90747.429688\n",
      "Train Epoch: 32 [7168/17352 (41%)] Loss: -33576.023438\n",
      "Train Epoch: 32 [8576/17352 (49%)] Loss: -63916.828125\n",
      "Train Epoch: 32 [9984/17352 (58%)] Loss: -46928.968750\n",
      "Train Epoch: 32 [11392/17352 (66%)] Loss: -34432.371094\n",
      "Train Epoch: 32 [12800/17352 (74%)] Loss: -46935.867188\n",
      "Train Epoch: 32 [14208/17352 (82%)] Loss: 9659.059570\n",
      "Train Epoch: 32 [15543/17352 (90%)] Loss: -61616.281250\n",
      "Train Epoch: 32 [16137/17352 (93%)] Loss: -61862.953125\n",
      "Train Epoch: 32 [16979/17352 (98%)] Loss: -56286.277344\n",
      "    epoch          : 32\n",
      "    loss           : -40977.47793906807\n",
      "    val_loss       : -23259.797926839194\n",
      "Train Epoch: 33 [128/17352 (1%)] Loss: -95887.351562\n",
      "Train Epoch: 33 [1536/17352 (9%)] Loss: -49899.320312\n",
      "Train Epoch: 33 [2944/17352 (17%)] Loss: -62512.257812\n",
      "Train Epoch: 33 [4352/17352 (25%)] Loss: -63810.519531\n",
      "Train Epoch: 33 [5760/17352 (33%)] Loss: -46832.148438\n",
      "Train Epoch: 33 [7168/17352 (41%)] Loss: -54755.992188\n",
      "Train Epoch: 33 [8576/17352 (49%)] Loss: -26656.550781\n",
      "Train Epoch: 33 [9984/17352 (58%)] Loss: -56826.281250\n",
      "Train Epoch: 33 [11392/17352 (66%)] Loss: -38847.863281\n",
      "Train Epoch: 33 [12800/17352 (74%)] Loss: -57942.281250\n",
      "Train Epoch: 33 [14208/17352 (82%)] Loss: -43853.480469\n",
      "Train Epoch: 33 [15453/17352 (89%)] Loss: -7856.747070\n",
      "Train Epoch: 33 [16281/17352 (94%)] Loss: -36877.832031\n",
      "Train Epoch: 33 [16928/17352 (98%)] Loss: -22089.632812\n",
      "    epoch          : 33\n",
      "    loss           : -43028.92082060743\n",
      "    val_loss       : -20458.771675745647\n",
      "Train Epoch: 34 [128/17352 (1%)] Loss: -16747.070312\n",
      "Train Epoch: 34 [1536/17352 (9%)] Loss: -49917.390625\n",
      "Train Epoch: 34 [2944/17352 (17%)] Loss: -72141.492188\n",
      "Train Epoch: 34 [4352/17352 (25%)] Loss: -80571.656250\n",
      "Train Epoch: 34 [5760/17352 (33%)] Loss: -83453.796875\n",
      "Train Epoch: 34 [7168/17352 (41%)] Loss: -75705.531250\n",
      "Train Epoch: 34 [8576/17352 (49%)] Loss: -43403.164062\n",
      "Train Epoch: 34 [9984/17352 (58%)] Loss: -42485.492188\n",
      "Train Epoch: 34 [11392/17352 (66%)] Loss: -65453.281250\n",
      "Train Epoch: 34 [12800/17352 (74%)] Loss: -61117.218750\n",
      "Train Epoch: 34 [14208/17352 (82%)] Loss: -47584.316406\n",
      "Train Epoch: 34 [15549/17352 (90%)] Loss: -43599.742188\n",
      "Train Epoch: 34 [16092/17352 (93%)] Loss: -1031.729004\n",
      "Train Epoch: 34 [16959/17352 (98%)] Loss: -51380.425781\n",
      "    epoch          : 34\n",
      "    loss           : -44278.375549726035\n",
      "    val_loss       : -25021.244107055663\n",
      "Train Epoch: 35 [128/17352 (1%)] Loss: -99441.468750\n",
      "Train Epoch: 35 [1536/17352 (9%)] Loss: -58346.632812\n",
      "Train Epoch: 35 [2944/17352 (17%)] Loss: 1549.388672\n",
      "Train Epoch: 35 [4352/17352 (25%)] Loss: -47313.328125\n",
      "Train Epoch: 35 [5760/17352 (33%)] Loss: -44493.421875\n",
      "Train Epoch: 35 [7168/17352 (41%)] Loss: -107500.781250\n",
      "Train Epoch: 35 [8576/17352 (49%)] Loss: -31054.042969\n",
      "Train Epoch: 35 [9984/17352 (58%)] Loss: -71301.828125\n",
      "Train Epoch: 35 [11392/17352 (66%)] Loss: -40428.546875\n",
      "Train Epoch: 35 [12800/17352 (74%)] Loss: -48475.476562\n",
      "Train Epoch: 35 [14208/17352 (82%)] Loss: -24043.355469\n",
      "Train Epoch: 35 [15520/17352 (89%)] Loss: -32468.117188\n",
      "Train Epoch: 35 [16315/17352 (94%)] Loss: -66400.929688\n",
      "Train Epoch: 35 [16985/17352 (98%)] Loss: -13891.476562\n",
      "    epoch          : 35\n",
      "    loss           : -47330.94542719694\n",
      "    val_loss       : -22577.37105178833\n",
      "Train Epoch: 36 [128/17352 (1%)] Loss: -52575.320312\n",
      "Train Epoch: 36 [1536/17352 (9%)] Loss: -53459.382812\n",
      "Train Epoch: 36 [2944/17352 (17%)] Loss: -35352.105469\n",
      "Train Epoch: 36 [4352/17352 (25%)] Loss: -80241.992188\n",
      "Train Epoch: 36 [5760/17352 (33%)] Loss: -14892.296875\n",
      "Train Epoch: 36 [7168/17352 (41%)] Loss: -85378.984375\n",
      "Train Epoch: 36 [8576/17352 (49%)] Loss: -69257.414062\n",
      "Train Epoch: 36 [9984/17352 (58%)] Loss: -42718.148438\n",
      "Train Epoch: 36 [11392/17352 (66%)] Loss: -44315.609375\n",
      "Train Epoch: 36 [12800/17352 (74%)] Loss: -25545.242188\n",
      "Train Epoch: 36 [14208/17352 (82%)] Loss: -101969.218750\n",
      "Train Epoch: 36 [15502/17352 (89%)] Loss: -7716.552734\n",
      "Train Epoch: 36 [16244/17352 (94%)] Loss: -16064.050781\n",
      "Train Epoch: 36 [16972/17352 (98%)] Loss: -25882.806641\n",
      "    epoch          : 36\n",
      "    loss           : -43951.29189838179\n",
      "    val_loss       : -21279.867267290752\n",
      "Train Epoch: 37 [128/17352 (1%)] Loss: -11129.032227\n",
      "Train Epoch: 37 [1536/17352 (9%)] Loss: -58402.886719\n",
      "Train Epoch: 37 [2944/17352 (17%)] Loss: -28266.851562\n",
      "Train Epoch: 37 [4352/17352 (25%)] Loss: -67649.359375\n",
      "Train Epoch: 37 [5760/17352 (33%)] Loss: -36573.128906\n",
      "Train Epoch: 37 [7168/17352 (41%)] Loss: -27455.560547\n",
      "Train Epoch: 37 [8576/17352 (49%)] Loss: -44010.839844\n",
      "Train Epoch: 37 [9984/17352 (58%)] Loss: -43451.449219\n",
      "Train Epoch: 37 [11392/17352 (66%)] Loss: -23748.914062\n",
      "Train Epoch: 37 [12800/17352 (74%)] Loss: -49744.597656\n",
      "Train Epoch: 37 [14208/17352 (82%)] Loss: -22617.707031\n",
      "Train Epoch: 37 [15419/17352 (89%)] Loss: -17890.205078\n",
      "Train Epoch: 37 [16169/17352 (93%)] Loss: 3495.782715\n",
      "Train Epoch: 37 [17019/17352 (98%)] Loss: -44923.085938\n",
      "    epoch          : 37\n",
      "    loss           : -45510.158189197515\n",
      "    val_loss       : -28077.833548990886\n",
      "Train Epoch: 38 [128/17352 (1%)] Loss: -43173.628906\n",
      "Train Epoch: 38 [1536/17352 (9%)] Loss: -46938.257812\n",
      "Train Epoch: 38 [2944/17352 (17%)] Loss: -47784.292969\n",
      "Train Epoch: 38 [4352/17352 (25%)] Loss: -40796.679688\n",
      "Train Epoch: 38 [5760/17352 (33%)] Loss: -79140.203125\n",
      "Train Epoch: 38 [7168/17352 (41%)] Loss: -45091.093750\n",
      "Train Epoch: 38 [8576/17352 (49%)] Loss: -1084.598633\n",
      "Train Epoch: 38 [9984/17352 (58%)] Loss: -19814.738281\n",
      "Train Epoch: 38 [11392/17352 (66%)] Loss: -62976.351562\n",
      "Train Epoch: 38 [12800/17352 (74%)] Loss: -59979.343750\n",
      "Train Epoch: 38 [14208/17352 (82%)] Loss: -76931.703125\n",
      "Train Epoch: 38 [15522/17352 (89%)] Loss: -56120.042969\n",
      "Train Epoch: 38 [16075/17352 (93%)] Loss: -15839.958008\n",
      "Train Epoch: 38 [16884/17352 (97%)] Loss: -26513.703125\n",
      "    epoch          : 38\n",
      "    loss           : -46515.757959967494\n",
      "    val_loss       : -23181.73394012451\n",
      "Train Epoch: 39 [128/17352 (1%)] Loss: -30642.171875\n",
      "Train Epoch: 39 [1536/17352 (9%)] Loss: -72100.390625\n",
      "Train Epoch: 39 [2944/17352 (17%)] Loss: -79654.390625\n",
      "Train Epoch: 39 [4352/17352 (25%)] Loss: -39656.093750\n",
      "Train Epoch: 39 [5760/17352 (33%)] Loss: -56911.332031\n",
      "Train Epoch: 39 [7168/17352 (41%)] Loss: -12619.380859\n",
      "Train Epoch: 39 [8576/17352 (49%)] Loss: -21658.785156\n",
      "Train Epoch: 39 [9984/17352 (58%)] Loss: -45468.152344\n",
      "Train Epoch: 39 [11392/17352 (66%)] Loss: -61087.820312\n",
      "Train Epoch: 39 [12800/17352 (74%)] Loss: -38250.394531\n",
      "Train Epoch: 39 [14208/17352 (82%)] Loss: -48822.816406\n",
      "Train Epoch: 39 [15560/17352 (90%)] Loss: -56362.468750\n",
      "Train Epoch: 39 [16338/17352 (94%)] Loss: -39946.257812\n",
      "Train Epoch: 39 [17047/17352 (98%)] Loss: -18343.992188\n",
      "    epoch          : 39\n",
      "    loss           : -49714.73669228778\n",
      "    val_loss       : -27494.097675069173\n",
      "Train Epoch: 40 [128/17352 (1%)] Loss: -75256.578125\n",
      "Train Epoch: 40 [1536/17352 (9%)] Loss: -32644.826172\n",
      "Train Epoch: 40 [2944/17352 (17%)] Loss: -39164.179688\n",
      "Train Epoch: 40 [4352/17352 (25%)] Loss: -62612.679688\n",
      "Train Epoch: 40 [5760/17352 (33%)] Loss: -89864.890625\n",
      "Train Epoch: 40 [7168/17352 (41%)] Loss: -76506.132812\n",
      "Train Epoch: 40 [8576/17352 (49%)] Loss: -2912.767578\n",
      "Train Epoch: 40 [9984/17352 (58%)] Loss: -43703.468750\n",
      "Train Epoch: 40 [11392/17352 (66%)] Loss: -26736.796875\n",
      "Train Epoch: 40 [12800/17352 (74%)] Loss: -89987.335938\n",
      "Train Epoch: 40 [14208/17352 (82%)] Loss: -57833.937500\n",
      "Train Epoch: 40 [15437/17352 (89%)] Loss: -22970.710938\n",
      "Train Epoch: 40 [16206/17352 (93%)] Loss: -20094.296875\n",
      "Train Epoch: 40 [17096/17352 (99%)] Loss: -76284.062500\n",
      "    epoch          : 40\n",
      "    loss           : -49430.12060505912\n",
      "    val_loss       : -29581.15269165039\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch40.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 41 [128/17352 (1%)] Loss: -73415.492188\n",
      "Train Epoch: 41 [1536/17352 (9%)] Loss: -55622.632812\n",
      "Train Epoch: 41 [2944/17352 (17%)] Loss: -26850.984375\n",
      "Train Epoch: 41 [4352/17352 (25%)] Loss: -98344.875000\n",
      "Train Epoch: 41 [5760/17352 (33%)] Loss: -68738.109375\n",
      "Train Epoch: 41 [7168/17352 (41%)] Loss: -78121.187500\n",
      "Train Epoch: 41 [8576/17352 (49%)] Loss: -100820.070312\n",
      "Train Epoch: 41 [9984/17352 (58%)] Loss: -69700.375000\n",
      "Train Epoch: 41 [11392/17352 (66%)] Loss: -90947.546875\n",
      "Train Epoch: 41 [12800/17352 (74%)] Loss: -56534.574219\n",
      "Train Epoch: 41 [14208/17352 (82%)] Loss: -85179.968750\n",
      "Train Epoch: 41 [15468/17352 (89%)] Loss: -63604.734375\n",
      "Train Epoch: 41 [16182/17352 (93%)] Loss: -9748.705078\n",
      "Train Epoch: 41 [16993/17352 (98%)] Loss: -3891.620117\n",
      "    epoch          : 41\n",
      "    loss           : -54176.57772110293\n",
      "    val_loss       : -27816.08091875712\n",
      "Train Epoch: 42 [128/17352 (1%)] Loss: -93308.609375\n",
      "Train Epoch: 42 [1536/17352 (9%)] Loss: -54842.003906\n",
      "Train Epoch: 42 [2944/17352 (17%)] Loss: -50064.042969\n",
      "Train Epoch: 42 [4352/17352 (25%)] Loss: -67065.757812\n",
      "Train Epoch: 42 [5760/17352 (33%)] Loss: -49705.644531\n",
      "Train Epoch: 42 [7168/17352 (41%)] Loss: -53853.441406\n",
      "Train Epoch: 42 [8576/17352 (49%)] Loss: -67754.289062\n",
      "Train Epoch: 42 [9984/17352 (58%)] Loss: -90176.984375\n",
      "Train Epoch: 42 [11392/17352 (66%)] Loss: -80339.695312\n",
      "Train Epoch: 42 [12800/17352 (74%)] Loss: -75742.484375\n",
      "Train Epoch: 42 [14208/17352 (82%)] Loss: -75035.539062\n",
      "Train Epoch: 42 [15471/17352 (89%)] Loss: -33643.472656\n",
      "Train Epoch: 42 [16035/17352 (92%)] Loss: -6426.053711\n",
      "Train Epoch: 42 [16885/17352 (97%)] Loss: -15972.712891\n",
      "    epoch          : 42\n",
      "    loss           : -52890.69399860561\n",
      "    val_loss       : -22321.625131734214\n",
      "Train Epoch: 43 [128/17352 (1%)] Loss: -42686.750000\n",
      "Train Epoch: 43 [1536/17352 (9%)] Loss: -67616.531250\n",
      "Train Epoch: 43 [2944/17352 (17%)] Loss: -56150.363281\n",
      "Train Epoch: 43 [4352/17352 (25%)] Loss: -92193.914062\n",
      "Train Epoch: 43 [5760/17352 (33%)] Loss: -53125.156250\n",
      "Train Epoch: 43 [7168/17352 (41%)] Loss: -15122.799805\n",
      "Train Epoch: 43 [8576/17352 (49%)] Loss: -22312.613281\n",
      "Train Epoch: 43 [9984/17352 (58%)] Loss: -49657.964844\n",
      "Train Epoch: 43 [11392/17352 (66%)] Loss: -51091.613281\n",
      "Train Epoch: 43 [12800/17352 (74%)] Loss: -85649.406250\n",
      "Train Epoch: 43 [14208/17352 (82%)] Loss: -67563.281250\n",
      "Train Epoch: 43 [15500/17352 (89%)] Loss: -33982.429688\n",
      "Train Epoch: 43 [16143/17352 (93%)] Loss: -6889.513184\n",
      "Train Epoch: 43 [17019/17352 (98%)] Loss: -41479.011719\n",
      "    epoch          : 43\n",
      "    loss           : -54346.08061556208\n",
      "    val_loss       : -30715.941259765626\n",
      "Train Epoch: 44 [128/17352 (1%)] Loss: -56936.023438\n",
      "Train Epoch: 44 [1536/17352 (9%)] Loss: -109919.382812\n",
      "Train Epoch: 44 [2944/17352 (17%)] Loss: -38849.101562\n",
      "Train Epoch: 44 [4352/17352 (25%)] Loss: -27955.304688\n",
      "Train Epoch: 44 [5760/17352 (33%)] Loss: -72962.875000\n",
      "Train Epoch: 44 [7168/17352 (41%)] Loss: -104822.570312\n",
      "Train Epoch: 44 [8576/17352 (49%)] Loss: -7668.119141\n",
      "Train Epoch: 44 [9984/17352 (58%)] Loss: -68772.609375\n",
      "Train Epoch: 44 [11392/17352 (66%)] Loss: -74868.843750\n",
      "Train Epoch: 44 [12800/17352 (74%)] Loss: -79558.734375\n",
      "Train Epoch: 44 [14208/17352 (82%)] Loss: -74360.125000\n",
      "Train Epoch: 44 [15519/17352 (89%)] Loss: -38234.695312\n",
      "Train Epoch: 44 [16197/17352 (93%)] Loss: -33047.425781\n",
      "Train Epoch: 44 [17070/17352 (98%)] Loss: -22774.703125\n",
      "    epoch          : 44\n",
      "    loss           : -57676.19307099413\n",
      "    val_loss       : -35462.9675394694\n",
      "Train Epoch: 45 [128/17352 (1%)] Loss: -66728.921875\n",
      "Train Epoch: 45 [1536/17352 (9%)] Loss: -74940.492188\n",
      "Train Epoch: 45 [2944/17352 (17%)] Loss: -9253.808594\n",
      "Train Epoch: 45 [4352/17352 (25%)] Loss: -70639.531250\n",
      "Train Epoch: 45 [5760/17352 (33%)] Loss: -65021.171875\n",
      "Train Epoch: 45 [7168/17352 (41%)] Loss: -90349.851562\n",
      "Train Epoch: 45 [8576/17352 (49%)] Loss: -89794.609375\n",
      "Train Epoch: 45 [9984/17352 (58%)] Loss: -48015.175781\n",
      "Train Epoch: 45 [11392/17352 (66%)] Loss: -68047.500000\n",
      "Train Epoch: 45 [12800/17352 (74%)] Loss: -60342.824219\n",
      "Train Epoch: 45 [14208/17352 (82%)] Loss: -73143.421875\n",
      "Train Epoch: 45 [15500/17352 (89%)] Loss: -19329.816406\n",
      "Train Epoch: 45 [16282/17352 (94%)] Loss: -47790.078125\n",
      "Train Epoch: 45 [17027/17352 (98%)] Loss: -4433.431641\n",
      "    epoch          : 45\n",
      "    loss           : -59518.691654486945\n",
      "    val_loss       : -24642.84986292521\n",
      "Train Epoch: 46 [128/17352 (1%)] Loss: -49043.968750\n",
      "Train Epoch: 46 [1536/17352 (9%)] Loss: -78068.812500\n",
      "Train Epoch: 46 [2944/17352 (17%)] Loss: -96902.375000\n",
      "Train Epoch: 46 [4352/17352 (25%)] Loss: -94510.718750\n",
      "Train Epoch: 46 [5760/17352 (33%)] Loss: -54501.093750\n",
      "Train Epoch: 46 [7168/17352 (41%)] Loss: 789.710938\n",
      "Train Epoch: 46 [8576/17352 (49%)] Loss: -76267.500000\n",
      "Train Epoch: 46 [9984/17352 (58%)] Loss: -87093.703125\n",
      "Train Epoch: 46 [11392/17352 (66%)] Loss: -76908.890625\n",
      "Train Epoch: 46 [12800/17352 (74%)] Loss: -56206.609375\n",
      "Train Epoch: 46 [14208/17352 (82%)] Loss: -75321.906250\n",
      "Train Epoch: 46 [15452/17352 (89%)] Loss: -50586.109375\n",
      "Train Epoch: 46 [16192/17352 (93%)] Loss: -31874.000000\n",
      "Train Epoch: 46 [16996/17352 (98%)] Loss: -42567.781250\n",
      "    epoch          : 46\n",
      "    loss           : -57581.63195882708\n",
      "    val_loss       : -32058.31305287679\n",
      "Train Epoch: 47 [128/17352 (1%)] Loss: -26657.218750\n",
      "Train Epoch: 47 [1536/17352 (9%)] Loss: -96906.281250\n",
      "Train Epoch: 47 [2944/17352 (17%)] Loss: -51588.476562\n",
      "Train Epoch: 47 [4352/17352 (25%)] Loss: -68535.937500\n",
      "Train Epoch: 47 [5760/17352 (33%)] Loss: -61532.468750\n",
      "Train Epoch: 47 [7168/17352 (41%)] Loss: -98321.125000\n",
      "Train Epoch: 47 [8576/17352 (49%)] Loss: -91293.015625\n",
      "Train Epoch: 47 [9984/17352 (58%)] Loss: -11556.148438\n",
      "Train Epoch: 47 [11392/17352 (66%)] Loss: -9750.805664\n",
      "Train Epoch: 47 [12800/17352 (74%)] Loss: -66612.171875\n",
      "Train Epoch: 47 [14208/17352 (82%)] Loss: -87031.171875\n",
      "Train Epoch: 47 [15519/17352 (89%)] Loss: -11202.810547\n",
      "Train Epoch: 47 [16371/17352 (94%)] Loss: -4116.977539\n",
      "Train Epoch: 47 [17111/17352 (99%)] Loss: -7722.867676\n",
      "    epoch          : 47\n",
      "    loss           : -59008.88179988989\n",
      "    val_loss       : -32314.945186360677\n",
      "Train Epoch: 48 [128/17352 (1%)] Loss: -70413.210938\n",
      "Train Epoch: 48 [1536/17352 (9%)] Loss: -68879.742188\n",
      "Train Epoch: 48 [2944/17352 (17%)] Loss: -80066.750000\n",
      "Train Epoch: 48 [4352/17352 (25%)] Loss: -20078.355469\n",
      "Train Epoch: 48 [5760/17352 (33%)] Loss: -73597.070312\n",
      "Train Epoch: 48 [7168/17352 (41%)] Loss: -54042.875000\n",
      "Train Epoch: 48 [8576/17352 (49%)] Loss: -59141.695312\n",
      "Train Epoch: 48 [9984/17352 (58%)] Loss: -45807.289062\n",
      "Train Epoch: 48 [11392/17352 (66%)] Loss: -52845.117188\n",
      "Train Epoch: 48 [12800/17352 (74%)] Loss: -12151.293945\n",
      "Train Epoch: 48 [14208/17352 (82%)] Loss: -74374.171875\n",
      "Train Epoch: 48 [15525/17352 (89%)] Loss: -44620.269531\n",
      "Train Epoch: 48 [16316/17352 (94%)] Loss: -5078.632812\n",
      "Train Epoch: 48 [17035/17352 (98%)] Loss: -46235.976562\n",
      "    epoch          : 48\n",
      "    loss           : -57573.186061782326\n",
      "    val_loss       : -34568.1775197347\n",
      "Train Epoch: 49 [128/17352 (1%)] Loss: -38274.144531\n",
      "Train Epoch: 49 [1536/17352 (9%)] Loss: -67097.335938\n",
      "Train Epoch: 49 [2944/17352 (17%)] Loss: -58963.746094\n",
      "Train Epoch: 49 [4352/17352 (25%)] Loss: -74786.578125\n",
      "Train Epoch: 49 [5760/17352 (33%)] Loss: -85403.132812\n",
      "Train Epoch: 49 [7168/17352 (41%)] Loss: -81253.812500\n",
      "Train Epoch: 49 [8576/17352 (49%)] Loss: -120126.070312\n",
      "Train Epoch: 49 [9984/17352 (58%)] Loss: -78689.289062\n",
      "Train Epoch: 49 [11392/17352 (66%)] Loss: -32810.765625\n",
      "Train Epoch: 49 [12800/17352 (74%)] Loss: -51137.972656\n",
      "Train Epoch: 49 [14208/17352 (82%)] Loss: -94225.289062\n",
      "Train Epoch: 49 [15541/17352 (90%)] Loss: -79353.320312\n",
      "Train Epoch: 49 [16430/17352 (95%)] Loss: -92870.914062\n",
      "Train Epoch: 49 [16989/17352 (98%)] Loss: -35296.640625\n",
      "    epoch          : 49\n",
      "    loss           : -60508.986705600815\n",
      "    val_loss       : -29828.783290863037\n",
      "Train Epoch: 50 [128/17352 (1%)] Loss: -46066.304688\n",
      "Train Epoch: 50 [1536/17352 (9%)] Loss: -27222.494141\n",
      "Train Epoch: 50 [2944/17352 (17%)] Loss: -105450.312500\n",
      "Train Epoch: 50 [4352/17352 (25%)] Loss: -23456.351562\n",
      "Train Epoch: 50 [5760/17352 (33%)] Loss: -19532.632812\n",
      "Train Epoch: 50 [7168/17352 (41%)] Loss: -29749.367188\n",
      "Train Epoch: 50 [8576/17352 (49%)] Loss: -31096.496094\n",
      "Train Epoch: 50 [9984/17352 (58%)] Loss: -69925.632812\n",
      "Train Epoch: 50 [11392/17352 (66%)] Loss: -46978.820312\n",
      "Train Epoch: 50 [12800/17352 (74%)] Loss: -36876.992188\n",
      "Train Epoch: 50 [14208/17352 (82%)] Loss: -107922.617188\n",
      "Train Epoch: 50 [15464/17352 (89%)] Loss: -12404.390625\n",
      "Train Epoch: 50 [16258/17352 (94%)] Loss: -43064.398438\n",
      "Train Epoch: 50 [16993/17352 (98%)] Loss: -2094.202881\n",
      "    epoch          : 50\n",
      "    loss           : -57098.22753742397\n",
      "    val_loss       : -32287.154772440594\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [128/17352 (1%)] Loss: -81714.429688\n",
      "Train Epoch: 51 [1536/17352 (9%)] Loss: -48790.734375\n",
      "Train Epoch: 51 [2944/17352 (17%)] Loss: -87989.468750\n",
      "Train Epoch: 51 [4352/17352 (25%)] Loss: -68695.695312\n",
      "Train Epoch: 51 [5760/17352 (33%)] Loss: -77134.453125\n",
      "Train Epoch: 51 [7168/17352 (41%)] Loss: -70566.726562\n",
      "Train Epoch: 51 [8576/17352 (49%)] Loss: -106522.156250\n",
      "Train Epoch: 51 [9984/17352 (58%)] Loss: -78562.359375\n",
      "Train Epoch: 51 [11392/17352 (66%)] Loss: -86885.859375\n",
      "Train Epoch: 51 [12800/17352 (74%)] Loss: -91219.078125\n",
      "Train Epoch: 51 [14208/17352 (82%)] Loss: -2529.257324\n",
      "Train Epoch: 51 [15460/17352 (89%)] Loss: -3720.773193\n",
      "Train Epoch: 51 [16217/17352 (93%)] Loss: -47694.101562\n",
      "Train Epoch: 51 [16964/17352 (98%)] Loss: -6729.742188\n",
      "    epoch          : 51\n",
      "    loss           : -62645.407896720324\n",
      "    val_loss       : -29950.813056437175\n",
      "Train Epoch: 52 [128/17352 (1%)] Loss: -53724.242188\n",
      "Train Epoch: 52 [1536/17352 (9%)] Loss: -67236.960938\n",
      "Train Epoch: 52 [2944/17352 (17%)] Loss: -74395.257812\n",
      "Train Epoch: 52 [4352/17352 (25%)] Loss: -97968.570312\n",
      "Train Epoch: 52 [5760/17352 (33%)] Loss: -73067.804688\n",
      "Train Epoch: 52 [7168/17352 (41%)] Loss: -95584.484375\n",
      "Train Epoch: 52 [8576/17352 (49%)] Loss: -93779.078125\n",
      "Train Epoch: 52 [9984/17352 (58%)] Loss: -59673.898438\n",
      "Train Epoch: 52 [11392/17352 (66%)] Loss: -107258.742188\n",
      "Train Epoch: 52 [12800/17352 (74%)] Loss: -116764.898438\n",
      "Train Epoch: 52 [14208/17352 (82%)] Loss: -45756.039062\n",
      "Train Epoch: 52 [15478/17352 (89%)] Loss: -51004.289062\n",
      "Train Epoch: 52 [16154/17352 (93%)] Loss: -54951.273438\n",
      "Train Epoch: 52 [16939/17352 (98%)] Loss: -13789.380859\n",
      "    epoch          : 52\n",
      "    loss           : -62427.2580730259\n",
      "    val_loss       : -32904.092713419595\n",
      "Train Epoch: 53 [128/17352 (1%)] Loss: -79351.296875\n",
      "Train Epoch: 53 [1536/17352 (9%)] Loss: -64505.097656\n",
      "Train Epoch: 53 [2944/17352 (17%)] Loss: -55136.871094\n",
      "Train Epoch: 53 [4352/17352 (25%)] Loss: -102313.195312\n",
      "Train Epoch: 53 [5760/17352 (33%)] Loss: -82117.101562\n",
      "Train Epoch: 53 [7168/17352 (41%)] Loss: -59859.531250\n",
      "Train Epoch: 53 [8576/17352 (49%)] Loss: -94611.046875\n",
      "Train Epoch: 53 [9984/17352 (58%)] Loss: -93832.757812\n",
      "Train Epoch: 53 [11392/17352 (66%)] Loss: -85625.796875\n",
      "Train Epoch: 53 [12800/17352 (74%)] Loss: -47897.207031\n",
      "Train Epoch: 53 [14208/17352 (82%)] Loss: -67475.687500\n",
      "Train Epoch: 53 [15448/17352 (89%)] Loss: -20462.777344\n",
      "Train Epoch: 53 [16210/17352 (93%)] Loss: -2170.269531\n",
      "Train Epoch: 53 [16993/17352 (98%)] Loss: -20865.492188\n",
      "    epoch          : 53\n",
      "    loss           : -63884.00261181313\n",
      "    val_loss       : -36380.94264424642\n",
      "Train Epoch: 54 [128/17352 (1%)] Loss: -39191.851562\n",
      "Train Epoch: 54 [1536/17352 (9%)] Loss: -71479.796875\n",
      "Train Epoch: 54 [2944/17352 (17%)] Loss: -91438.101562\n",
      "Train Epoch: 54 [4352/17352 (25%)] Loss: -56647.753906\n",
      "Train Epoch: 54 [5760/17352 (33%)] Loss: -84567.757812\n",
      "Train Epoch: 54 [7168/17352 (41%)] Loss: -71432.789062\n",
      "Train Epoch: 54 [8576/17352 (49%)] Loss: -100360.179688\n",
      "Train Epoch: 54 [9984/17352 (58%)] Loss: -100629.031250\n",
      "Train Epoch: 54 [11392/17352 (66%)] Loss: -77151.429688\n",
      "Train Epoch: 54 [12800/17352 (74%)] Loss: -85863.195312\n",
      "Train Epoch: 54 [14208/17352 (82%)] Loss: -80028.234375\n",
      "Train Epoch: 54 [15518/17352 (89%)] Loss: -43502.847656\n",
      "Train Epoch: 54 [16213/17352 (93%)] Loss: -27410.076172\n",
      "Train Epoch: 54 [17011/17352 (98%)] Loss: -27193.220703\n",
      "    epoch          : 54\n",
      "    loss           : -64236.90437716286\n",
      "    val_loss       : -26198.729256184895\n",
      "Train Epoch: 55 [128/17352 (1%)] Loss: -57191.015625\n",
      "Train Epoch: 55 [1536/17352 (9%)] Loss: -38859.074219\n",
      "Train Epoch: 55 [2944/17352 (17%)] Loss: -71710.046875\n",
      "Train Epoch: 55 [4352/17352 (25%)] Loss: 2356.233398\n",
      "Train Epoch: 55 [5760/17352 (33%)] Loss: -51859.875000\n",
      "Train Epoch: 55 [7168/17352 (41%)] Loss: -85571.609375\n",
      "Train Epoch: 55 [8576/17352 (49%)] Loss: -73060.171875\n",
      "Train Epoch: 55 [9984/17352 (58%)] Loss: -74280.687500\n",
      "Train Epoch: 55 [11392/17352 (66%)] Loss: -77989.882812\n",
      "Train Epoch: 55 [12800/17352 (74%)] Loss: -62883.546875\n",
      "Train Epoch: 55 [14208/17352 (82%)] Loss: -34452.496094\n",
      "Train Epoch: 55 [15554/17352 (90%)] Loss: -63340.578125\n",
      "Train Epoch: 55 [16398/17352 (95%)] Loss: -14844.433594\n",
      "Train Epoch: 55 [17037/17352 (98%)] Loss: -32895.437500\n",
      "    epoch          : 55\n",
      "    loss           : -59098.26345722788\n",
      "    val_loss       : -31995.514049021403\n",
      "Train Epoch: 56 [128/17352 (1%)] Loss: -101018.625000\n",
      "Train Epoch: 56 [1536/17352 (9%)] Loss: -79632.726562\n",
      "Train Epoch: 56 [2944/17352 (17%)] Loss: -53558.574219\n",
      "Train Epoch: 56 [4352/17352 (25%)] Loss: -42590.351562\n",
      "Train Epoch: 56 [5760/17352 (33%)] Loss: -74111.265625\n",
      "Train Epoch: 56 [7168/17352 (41%)] Loss: -42133.839844\n",
      "Train Epoch: 56 [8576/17352 (49%)] Loss: -102111.859375\n",
      "Train Epoch: 56 [9984/17352 (58%)] Loss: -69081.664062\n",
      "Train Epoch: 56 [11392/17352 (66%)] Loss: -80649.054688\n",
      "Train Epoch: 56 [12800/17352 (74%)] Loss: -60481.394531\n",
      "Train Epoch: 56 [14208/17352 (82%)] Loss: -74052.484375\n",
      "Train Epoch: 56 [15541/17352 (90%)] Loss: -66359.929688\n",
      "Train Epoch: 56 [16443/17352 (95%)] Loss: -51533.785156\n",
      "Train Epoch: 56 [17081/17352 (98%)] Loss: -35312.632812\n",
      "    epoch          : 56\n",
      "    loss           : -63860.83783327653\n",
      "    val_loss       : -30866.423535919188\n",
      "Train Epoch: 57 [128/17352 (1%)] Loss: -71584.601562\n",
      "Train Epoch: 57 [1536/17352 (9%)] Loss: -100922.664062\n",
      "Train Epoch: 57 [2944/17352 (17%)] Loss: -25336.564453\n",
      "Train Epoch: 57 [4352/17352 (25%)] Loss: -47497.214844\n",
      "Train Epoch: 57 [5760/17352 (33%)] Loss: -53511.351562\n",
      "Train Epoch: 57 [7168/17352 (41%)] Loss: -16751.285156\n",
      "Train Epoch: 57 [8576/17352 (49%)] Loss: -27922.164062\n",
      "Train Epoch: 57 [9984/17352 (58%)] Loss: -82423.195312\n",
      "Train Epoch: 57 [11392/17352 (66%)] Loss: -80715.421875\n",
      "Train Epoch: 57 [12800/17352 (74%)] Loss: -80275.687500\n",
      "Train Epoch: 57 [14208/17352 (82%)] Loss: -72389.804688\n",
      "Train Epoch: 57 [15465/17352 (89%)] Loss: -6489.418945\n",
      "Train Epoch: 57 [16262/17352 (94%)] Loss: -60016.933594\n",
      "Train Epoch: 57 [16862/17352 (97%)] Loss: 10181.978516\n",
      "    epoch          : 57\n",
      "    loss           : -51630.2536932414\n",
      "    val_loss       : -33004.70328076681\n",
      "Train Epoch: 58 [128/17352 (1%)] Loss: -49117.695312\n",
      "Train Epoch: 58 [1536/17352 (9%)] Loss: -95527.625000\n",
      "Train Epoch: 58 [2944/17352 (17%)] Loss: -70047.273438\n",
      "Train Epoch: 58 [4352/17352 (25%)] Loss: -94337.648438\n",
      "Train Epoch: 58 [5760/17352 (33%)] Loss: -24098.292969\n",
      "Train Epoch: 58 [7168/17352 (41%)] Loss: -123661.843750\n",
      "Train Epoch: 58 [8576/17352 (49%)] Loss: -17330.511719\n",
      "Train Epoch: 58 [9984/17352 (58%)] Loss: -80921.125000\n",
      "Train Epoch: 58 [11392/17352 (66%)] Loss: -69373.031250\n",
      "Train Epoch: 58 [12800/17352 (74%)] Loss: -56201.246094\n",
      "Train Epoch: 58 [14208/17352 (82%)] Loss: -103594.273438\n",
      "Train Epoch: 58 [15479/17352 (89%)] Loss: -18318.048828\n",
      "Train Epoch: 58 [16157/17352 (93%)] Loss: -68583.125000\n",
      "Train Epoch: 58 [16991/17352 (98%)] Loss: -33807.289062\n",
      "    epoch          : 58\n",
      "    loss           : -61204.34524269872\n",
      "    val_loss       : -33558.0892141978\n",
      "Train Epoch: 59 [128/17352 (1%)] Loss: -57503.960938\n",
      "Train Epoch: 59 [1536/17352 (9%)] Loss: -77329.515625\n",
      "Train Epoch: 59 [2944/17352 (17%)] Loss: -45677.867188\n",
      "Train Epoch: 59 [4352/17352 (25%)] Loss: -76314.679688\n",
      "Train Epoch: 59 [5760/17352 (33%)] Loss: -36158.121094\n",
      "Train Epoch: 59 [7168/17352 (41%)] Loss: -68284.468750\n",
      "Train Epoch: 59 [8576/17352 (49%)] Loss: -62275.789062\n",
      "Train Epoch: 59 [9984/17352 (58%)] Loss: -47039.906250\n",
      "Train Epoch: 59 [11392/17352 (66%)] Loss: -58990.367188\n",
      "Train Epoch: 59 [12800/17352 (74%)] Loss: -64486.453125\n",
      "Train Epoch: 59 [14208/17352 (82%)] Loss: -97651.500000\n",
      "Train Epoch: 59 [15498/17352 (89%)] Loss: -13911.679688\n",
      "Train Epoch: 59 [16099/17352 (93%)] Loss: -83595.914062\n",
      "Train Epoch: 59 [16937/17352 (98%)] Loss: -35598.171875\n",
      "    epoch          : 59\n",
      "    loss           : -64122.02998300847\n",
      "    val_loss       : -38765.404548517865\n",
      "Train Epoch: 60 [128/17352 (1%)] Loss: -76563.804688\n",
      "Train Epoch: 60 [1536/17352 (9%)] Loss: -117908.226562\n",
      "Train Epoch: 60 [2944/17352 (17%)] Loss: -71219.875000\n",
      "Train Epoch: 60 [4352/17352 (25%)] Loss: -102867.273438\n",
      "Train Epoch: 60 [5760/17352 (33%)] Loss: -102029.328125\n",
      "Train Epoch: 60 [7168/17352 (41%)] Loss: -66079.031250\n",
      "Train Epoch: 60 [8576/17352 (49%)] Loss: -95708.718750\n",
      "Train Epoch: 60 [9984/17352 (58%)] Loss: -119089.953125\n",
      "Train Epoch: 60 [11392/17352 (66%)] Loss: -65725.437500\n",
      "Train Epoch: 60 [12800/17352 (74%)] Loss: -78655.515625\n",
      "Train Epoch: 60 [14208/17352 (82%)] Loss: -63008.691406\n",
      "Train Epoch: 60 [15492/17352 (89%)] Loss: -14503.587891\n",
      "Train Epoch: 60 [16162/17352 (93%)] Loss: -29213.480469\n",
      "Train Epoch: 60 [16979/17352 (98%)] Loss: -36217.953125\n",
      "    epoch          : 60\n",
      "    loss           : -69284.63059393351\n",
      "    val_loss       : -33314.60023523967\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch60.pth ...\n",
      "Train Epoch: 61 [128/17352 (1%)] Loss: -52680.566406\n",
      "Train Epoch: 61 [1536/17352 (9%)] Loss: -118938.617188\n",
      "Train Epoch: 61 [2944/17352 (17%)] Loss: -71339.265625\n",
      "Train Epoch: 61 [4352/17352 (25%)] Loss: -71323.203125\n",
      "Train Epoch: 61 [5760/17352 (33%)] Loss: -88800.257812\n",
      "Train Epoch: 61 [7168/17352 (41%)] Loss: -43787.343750\n",
      "Train Epoch: 61 [8576/17352 (49%)] Loss: -70379.781250\n",
      "Train Epoch: 61 [9984/17352 (58%)] Loss: -77683.781250\n",
      "Train Epoch: 61 [11392/17352 (66%)] Loss: -83664.421875\n",
      "Train Epoch: 61 [12800/17352 (74%)] Loss: -74144.484375\n",
      "Train Epoch: 61 [14208/17352 (82%)] Loss: -72982.351562\n",
      "Train Epoch: 61 [15483/17352 (89%)] Loss: -59317.441406\n",
      "Train Epoch: 61 [16310/17352 (94%)] Loss: -11707.668945\n",
      "Train Epoch: 61 [17050/17352 (98%)] Loss: -26808.205078\n",
      "    epoch          : 61\n",
      "    loss           : -70008.01901511378\n",
      "    val_loss       : -36246.46116561889\n",
      "Train Epoch: 62 [128/17352 (1%)] Loss: -101279.414062\n",
      "Train Epoch: 62 [1536/17352 (9%)] Loss: -95817.890625\n",
      "Train Epoch: 62 [2944/17352 (17%)] Loss: -56366.828125\n",
      "Train Epoch: 62 [4352/17352 (25%)] Loss: -65899.476562\n",
      "Train Epoch: 62 [5760/17352 (33%)] Loss: -90654.015625\n",
      "Train Epoch: 62 [7168/17352 (41%)] Loss: -80794.140625\n",
      "Train Epoch: 62 [8576/17352 (49%)] Loss: -40852.421875\n",
      "Train Epoch: 62 [9984/17352 (58%)] Loss: -83879.484375\n",
      "Train Epoch: 62 [11392/17352 (66%)] Loss: -72654.617188\n",
      "Train Epoch: 62 [12800/17352 (74%)] Loss: -72241.406250\n",
      "Train Epoch: 62 [14208/17352 (82%)] Loss: -68819.500000\n",
      "Train Epoch: 62 [15486/17352 (89%)] Loss: -49747.539062\n",
      "Train Epoch: 62 [16267/17352 (94%)] Loss: -51034.531250\n",
      "Train Epoch: 62 [17051/17352 (98%)] Loss: -71330.539062\n",
      "    epoch          : 62\n",
      "    loss           : -68057.11057440226\n",
      "    val_loss       : -34419.09117838542\n",
      "Train Epoch: 63 [128/17352 (1%)] Loss: -89570.265625\n",
      "Train Epoch: 63 [1536/17352 (9%)] Loss: -93577.468750\n",
      "Train Epoch: 63 [2944/17352 (17%)] Loss: -96706.617188\n",
      "Train Epoch: 63 [4352/17352 (25%)] Loss: -105904.640625\n",
      "Train Epoch: 63 [5760/17352 (33%)] Loss: -108426.671875\n",
      "Train Epoch: 63 [7168/17352 (41%)] Loss: -107466.781250\n",
      "Train Epoch: 63 [8576/17352 (49%)] Loss: -95361.390625\n",
      "Train Epoch: 63 [9984/17352 (58%)] Loss: -84994.890625\n",
      "Train Epoch: 63 [11392/17352 (66%)] Loss: -70614.320312\n",
      "Train Epoch: 63 [12800/17352 (74%)] Loss: -64446.835938\n",
      "Train Epoch: 63 [14208/17352 (82%)] Loss: -30435.322266\n",
      "Train Epoch: 63 [15557/17352 (90%)] Loss: -76078.687500\n",
      "Train Epoch: 63 [16138/17352 (93%)] Loss: -27629.136719\n",
      "Train Epoch: 63 [17017/17352 (98%)] Loss: -49742.117188\n",
      "    epoch          : 63\n",
      "    loss           : -69398.89417814728\n",
      "    val_loss       : -36621.56922162374\n",
      "Train Epoch: 64 [128/17352 (1%)] Loss: -82436.492188\n",
      "Train Epoch: 64 [1536/17352 (9%)] Loss: -87308.195312\n",
      "Train Epoch: 64 [2944/17352 (17%)] Loss: -79355.593750\n",
      "Train Epoch: 64 [4352/17352 (25%)] Loss: -75595.351562\n",
      "Train Epoch: 64 [5760/17352 (33%)] Loss: -64963.582031\n",
      "Train Epoch: 64 [7168/17352 (41%)] Loss: -53260.039062\n",
      "Train Epoch: 64 [8576/17352 (49%)] Loss: -103399.015625\n",
      "Train Epoch: 64 [9984/17352 (58%)] Loss: -61628.019531\n",
      "Train Epoch: 64 [11392/17352 (66%)] Loss: -113711.750000\n",
      "Train Epoch: 64 [12800/17352 (74%)] Loss: -87123.062500\n",
      "Train Epoch: 64 [14208/17352 (82%)] Loss: -60142.000000\n",
      "Train Epoch: 64 [15547/17352 (90%)] Loss: -41190.492188\n",
      "Train Epoch: 64 [16169/17352 (93%)] Loss: -8917.855469\n",
      "Train Epoch: 64 [17058/17352 (98%)] Loss: -41073.285156\n",
      "    epoch          : 64\n",
      "    loss           : -66300.5775154677\n",
      "    val_loss       : -35099.6631573995\n",
      "Train Epoch: 65 [128/17352 (1%)] Loss: -55919.816406\n",
      "Train Epoch: 65 [1536/17352 (9%)] Loss: -106272.171875\n",
      "Train Epoch: 65 [2944/17352 (17%)] Loss: -110727.648438\n",
      "Train Epoch: 65 [4352/17352 (25%)] Loss: -95634.281250\n",
      "Train Epoch: 65 [5760/17352 (33%)] Loss: -64522.601562\n",
      "Train Epoch: 65 [7168/17352 (41%)] Loss: -74187.156250\n",
      "Train Epoch: 65 [8576/17352 (49%)] Loss: -84714.632812\n",
      "Train Epoch: 65 [9984/17352 (58%)] Loss: -59356.433594\n",
      "Train Epoch: 65 [11392/17352 (66%)] Loss: -53037.156250\n",
      "Train Epoch: 65 [12800/17352 (74%)] Loss: -94696.468750\n",
      "Train Epoch: 65 [14208/17352 (82%)] Loss: -20766.035156\n",
      "Train Epoch: 65 [15437/17352 (89%)] Loss: -23626.550781\n",
      "Train Epoch: 65 [16217/17352 (93%)] Loss: -8205.699219\n",
      "Train Epoch: 65 [16967/17352 (98%)] Loss: -46207.617188\n",
      "    epoch          : 65\n",
      "    loss           : -75290.90739696939\n",
      "    val_loss       : -42207.54494069417\n",
      "Train Epoch: 66 [128/17352 (1%)] Loss: -64100.242188\n",
      "Train Epoch: 66 [1536/17352 (9%)] Loss: -78387.976562\n",
      "Train Epoch: 66 [2944/17352 (17%)] Loss: -95647.484375\n",
      "Train Epoch: 66 [4352/17352 (25%)] Loss: -110257.953125\n",
      "Train Epoch: 66 [5760/17352 (33%)] Loss: -75213.812500\n",
      "Train Epoch: 66 [7168/17352 (41%)] Loss: -66528.507812\n",
      "Train Epoch: 66 [8576/17352 (49%)] Loss: -23950.109375\n",
      "Train Epoch: 66 [9984/17352 (58%)] Loss: -65836.750000\n",
      "Train Epoch: 66 [11392/17352 (66%)] Loss: -42228.316406\n",
      "Train Epoch: 66 [12800/17352 (74%)] Loss: -52617.824219\n",
      "Train Epoch: 66 [14208/17352 (82%)] Loss: -107958.492188\n",
      "Train Epoch: 66 [15555/17352 (90%)] Loss: -57402.859375\n",
      "Train Epoch: 66 [16260/17352 (94%)] Loss: -72940.632812\n",
      "Train Epoch: 66 [17039/17352 (98%)] Loss: -41511.406250\n",
      "    epoch          : 66\n",
      "    loss           : -71607.7897605128\n",
      "    val_loss       : -36617.70117085775\n",
      "Train Epoch: 67 [128/17352 (1%)] Loss: -50291.210938\n",
      "Train Epoch: 67 [1536/17352 (9%)] Loss: -41129.527344\n",
      "Train Epoch: 67 [2944/17352 (17%)] Loss: -64765.121094\n",
      "Train Epoch: 67 [4352/17352 (25%)] Loss: -87130.296875\n",
      "Train Epoch: 67 [5760/17352 (33%)] Loss: -109115.445312\n",
      "Train Epoch: 67 [7168/17352 (41%)] Loss: -54964.570312\n",
      "Train Epoch: 67 [8576/17352 (49%)] Loss: -64557.195312\n",
      "Train Epoch: 67 [9984/17352 (58%)] Loss: -70839.203125\n",
      "Train Epoch: 67 [11392/17352 (66%)] Loss: -72509.031250\n",
      "Train Epoch: 67 [12800/17352 (74%)] Loss: -48357.222656\n",
      "Train Epoch: 67 [14208/17352 (82%)] Loss: -78533.382812\n",
      "Train Epoch: 67 [15512/17352 (89%)] Loss: -93438.515625\n",
      "Train Epoch: 67 [16177/17352 (93%)] Loss: -15943.234375\n",
      "Train Epoch: 67 [16960/17352 (98%)] Loss: -74113.078125\n",
      "    epoch          : 67\n",
      "    loss           : -70502.86835740876\n",
      "    val_loss       : -34317.70822410584\n",
      "Train Epoch: 68 [128/17352 (1%)] Loss: -75509.156250\n",
      "Train Epoch: 68 [1536/17352 (9%)] Loss: -105998.593750\n",
      "Train Epoch: 68 [2944/17352 (17%)] Loss: -76549.500000\n",
      "Train Epoch: 68 [4352/17352 (25%)] Loss: -7800.988770\n",
      "Train Epoch: 68 [5760/17352 (33%)] Loss: -64199.953125\n",
      "Train Epoch: 68 [7168/17352 (41%)] Loss: -52021.398438\n",
      "Train Epoch: 68 [8576/17352 (49%)] Loss: -87723.953125\n",
      "Train Epoch: 68 [9984/17352 (58%)] Loss: -100334.234375\n",
      "Train Epoch: 68 [11392/17352 (66%)] Loss: -34439.875000\n",
      "Train Epoch: 68 [12800/17352 (74%)] Loss: -82093.906250\n",
      "Train Epoch: 68 [14208/17352 (82%)] Loss: -91794.234375\n",
      "Train Epoch: 68 [15476/17352 (89%)] Loss: -28679.054688\n",
      "Train Epoch: 68 [16292/17352 (94%)] Loss: -19023.554688\n",
      "Train Epoch: 68 [17087/17352 (98%)] Loss: -77446.796875\n",
      "    epoch          : 68\n",
      "    loss           : -71082.7594024863\n",
      "    val_loss       : -36866.54285481771\n",
      "Train Epoch: 69 [128/17352 (1%)] Loss: -67796.742188\n",
      "Train Epoch: 69 [1536/17352 (9%)] Loss: -88303.281250\n",
      "Train Epoch: 69 [2944/17352 (17%)] Loss: -81200.859375\n",
      "Train Epoch: 69 [4352/17352 (25%)] Loss: -82206.531250\n",
      "Train Epoch: 69 [5760/17352 (33%)] Loss: -125954.304688\n",
      "Train Epoch: 69 [7168/17352 (41%)] Loss: -39522.382812\n",
      "Train Epoch: 69 [8576/17352 (49%)] Loss: -117608.179688\n",
      "Train Epoch: 69 [9984/17352 (58%)] Loss: -105117.953125\n",
      "Train Epoch: 69 [11392/17352 (66%)] Loss: -62786.820312\n",
      "Train Epoch: 69 [12800/17352 (74%)] Loss: -66272.484375\n",
      "Train Epoch: 69 [14208/17352 (82%)] Loss: -59981.742188\n",
      "Train Epoch: 69 [15451/17352 (89%)] Loss: -23347.724609\n",
      "Train Epoch: 69 [16155/17352 (93%)] Loss: -24123.558594\n",
      "Train Epoch: 69 [17047/17352 (98%)] Loss: -64595.199219\n",
      "    epoch          : 69\n",
      "    loss           : -74739.29043312841\n",
      "    val_loss       : -36508.54629313151\n",
      "Train Epoch: 70 [128/17352 (1%)] Loss: -20081.054688\n",
      "Train Epoch: 70 [1536/17352 (9%)] Loss: -81334.078125\n",
      "Train Epoch: 70 [2944/17352 (17%)] Loss: -78940.164062\n",
      "Train Epoch: 70 [4352/17352 (25%)] Loss: -62077.945312\n",
      "Train Epoch: 70 [5760/17352 (33%)] Loss: -67508.414062\n",
      "Train Epoch: 70 [7168/17352 (41%)] Loss: -74587.414062\n",
      "Train Epoch: 70 [8576/17352 (49%)] Loss: -90974.812500\n",
      "Train Epoch: 70 [9984/17352 (58%)] Loss: -97990.148438\n",
      "Train Epoch: 70 [11392/17352 (66%)] Loss: -86429.921875\n",
      "Train Epoch: 70 [12800/17352 (74%)] Loss: -99442.390625\n",
      "Train Epoch: 70 [14208/17352 (82%)] Loss: -108179.132812\n",
      "Train Epoch: 70 [15530/17352 (89%)] Loss: -111114.007812\n",
      "Train Epoch: 70 [16202/17352 (93%)] Loss: -57070.851562\n",
      "Train Epoch: 70 [16970/17352 (98%)] Loss: -67307.328125\n",
      "    epoch          : 70\n",
      "    loss           : -71695.12821479132\n",
      "    val_loss       : -41022.3665725708\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch70.pth ...\n",
      "Train Epoch: 71 [128/17352 (1%)] Loss: -87397.093750\n",
      "Train Epoch: 71 [1536/17352 (9%)] Loss: -74582.515625\n",
      "Train Epoch: 71 [2944/17352 (17%)] Loss: -117144.632812\n",
      "Train Epoch: 71 [4352/17352 (25%)] Loss: -89637.007812\n",
      "Train Epoch: 71 [5760/17352 (33%)] Loss: -74182.960938\n",
      "Train Epoch: 71 [7168/17352 (41%)] Loss: -95445.203125\n",
      "Train Epoch: 71 [8576/17352 (49%)] Loss: -90691.750000\n",
      "Train Epoch: 71 [9984/17352 (58%)] Loss: -52945.210938\n",
      "Train Epoch: 71 [11392/17352 (66%)] Loss: -76880.453125\n",
      "Train Epoch: 71 [12800/17352 (74%)] Loss: -110820.765625\n",
      "Train Epoch: 71 [14208/17352 (82%)] Loss: -111697.687500\n",
      "Train Epoch: 71 [15509/17352 (89%)] Loss: -66036.625000\n",
      "Train Epoch: 71 [16225/17352 (94%)] Loss: -51442.804688\n",
      "Train Epoch: 71 [17009/17352 (98%)] Loss: -64296.257812\n",
      "    epoch          : 71\n",
      "    loss           : -78218.2896900561\n",
      "    val_loss       : -43613.04540850322\n",
      "Train Epoch: 72 [128/17352 (1%)] Loss: -109838.531250\n",
      "Train Epoch: 72 [1536/17352 (9%)] Loss: -82076.265625\n",
      "Train Epoch: 72 [2944/17352 (17%)] Loss: -68900.820312\n",
      "Train Epoch: 72 [4352/17352 (25%)] Loss: -69541.882812\n",
      "Train Epoch: 72 [5760/17352 (33%)] Loss: -52490.781250\n",
      "Train Epoch: 72 [7168/17352 (41%)] Loss: -87228.140625\n",
      "Train Epoch: 72 [8576/17352 (49%)] Loss: -102013.265625\n",
      "Train Epoch: 72 [9984/17352 (58%)] Loss: -36291.839844\n",
      "Train Epoch: 72 [11392/17352 (66%)] Loss: -112635.312500\n",
      "Train Epoch: 72 [12800/17352 (74%)] Loss: -82160.906250\n",
      "Train Epoch: 72 [14208/17352 (82%)] Loss: -93723.250000\n",
      "Train Epoch: 72 [15378/17352 (89%)] Loss: -10102.063477\n",
      "Train Epoch: 72 [16147/17352 (93%)] Loss: -55638.601562\n",
      "Train Epoch: 72 [17066/17352 (98%)] Loss: -60915.023438\n",
      "    epoch          : 72\n",
      "    loss           : -72782.64732657823\n",
      "    val_loss       : -39151.75549608866\n",
      "Train Epoch: 73 [128/17352 (1%)] Loss: -72337.593750\n",
      "Train Epoch: 73 [1536/17352 (9%)] Loss: -59122.054688\n",
      "Train Epoch: 73 [2944/17352 (17%)] Loss: -96991.242188\n",
      "Train Epoch: 73 [4352/17352 (25%)] Loss: -81967.867188\n",
      "Train Epoch: 73 [5760/17352 (33%)] Loss: -105312.921875\n",
      "Train Epoch: 73 [7168/17352 (41%)] Loss: -96507.296875\n",
      "Train Epoch: 73 [8576/17352 (49%)] Loss: -94828.187500\n",
      "Train Epoch: 73 [9984/17352 (58%)] Loss: -80999.960938\n",
      "Train Epoch: 73 [11392/17352 (66%)] Loss: -68410.093750\n",
      "Train Epoch: 73 [12800/17352 (74%)] Loss: -106366.226562\n",
      "Train Epoch: 73 [14208/17352 (82%)] Loss: -72361.859375\n",
      "Train Epoch: 73 [15472/17352 (89%)] Loss: -43031.390625\n",
      "Train Epoch: 73 [16203/17352 (93%)] Loss: -2844.955078\n",
      "Train Epoch: 73 [17028/17352 (98%)] Loss: -76747.382812\n",
      "    epoch          : 73\n",
      "    loss           : -75113.72411617817\n",
      "    val_loss       : -39487.987796020505\n",
      "Train Epoch: 74 [128/17352 (1%)] Loss: -110329.515625\n",
      "Train Epoch: 74 [1536/17352 (9%)] Loss: -128630.968750\n",
      "Train Epoch: 74 [2944/17352 (17%)] Loss: -86766.250000\n",
      "Train Epoch: 74 [4352/17352 (25%)] Loss: -81904.468750\n",
      "Train Epoch: 74 [5760/17352 (33%)] Loss: -114719.195312\n",
      "Train Epoch: 74 [7168/17352 (41%)] Loss: -59001.753906\n",
      "Train Epoch: 74 [8576/17352 (49%)] Loss: -106597.359375\n",
      "Train Epoch: 74 [9984/17352 (58%)] Loss: -126415.710938\n",
      "Train Epoch: 74 [11392/17352 (66%)] Loss: -62971.078125\n",
      "Train Epoch: 74 [12800/17352 (74%)] Loss: -21540.130859\n",
      "Train Epoch: 74 [14208/17352 (82%)] Loss: -74704.453125\n",
      "Train Epoch: 74 [15531/17352 (90%)] Loss: -9207.062500\n",
      "Train Epoch: 74 [16231/17352 (94%)] Loss: -82840.554688\n",
      "Train Epoch: 74 [16997/17352 (98%)] Loss: -67782.898438\n",
      "    epoch          : 74\n",
      "    loss           : -74927.53543971528\n",
      "    val_loss       : -38924.81908416748\n",
      "Train Epoch: 75 [128/17352 (1%)] Loss: -99497.343750\n",
      "Train Epoch: 75 [1536/17352 (9%)] Loss: -69146.460938\n",
      "Train Epoch: 75 [2944/17352 (17%)] Loss: -75010.421875\n",
      "Train Epoch: 75 [4352/17352 (25%)] Loss: -62958.117188\n",
      "Train Epoch: 75 [5760/17352 (33%)] Loss: -86444.257812\n",
      "Train Epoch: 75 [7168/17352 (41%)] Loss: -82498.203125\n",
      "Train Epoch: 75 [8576/17352 (49%)] Loss: -100751.921875\n",
      "Train Epoch: 75 [9984/17352 (58%)] Loss: -113920.187500\n",
      "Train Epoch: 75 [11392/17352 (66%)] Loss: -85604.242188\n",
      "Train Epoch: 75 [12800/17352 (74%)] Loss: -53688.648438\n",
      "Train Epoch: 75 [14208/17352 (82%)] Loss: -67987.640625\n",
      "Train Epoch: 75 [15427/17352 (89%)] Loss: -7120.739258\n",
      "Train Epoch: 75 [16218/17352 (93%)] Loss: -69300.187500\n",
      "Train Epoch: 75 [16992/17352 (98%)] Loss: -75238.718750\n",
      "    epoch          : 75\n",
      "    loss           : -77058.36348442743\n",
      "    val_loss       : -45165.253742472334\n",
      "Train Epoch: 76 [128/17352 (1%)] Loss: -128879.757812\n",
      "Train Epoch: 76 [1536/17352 (9%)] Loss: -116595.312500\n",
      "Train Epoch: 76 [2944/17352 (17%)] Loss: -59843.835938\n",
      "Train Epoch: 76 [4352/17352 (25%)] Loss: -53895.468750\n",
      "Train Epoch: 76 [5760/17352 (33%)] Loss: -104823.070312\n",
      "Train Epoch: 76 [7168/17352 (41%)] Loss: -55245.511719\n",
      "Train Epoch: 76 [8576/17352 (49%)] Loss: -60313.265625\n",
      "Train Epoch: 76 [9984/17352 (58%)] Loss: -73858.187500\n",
      "Train Epoch: 76 [11392/17352 (66%)] Loss: -118670.500000\n",
      "Train Epoch: 76 [12800/17352 (74%)] Loss: -82647.453125\n",
      "Train Epoch: 76 [14208/17352 (82%)] Loss: -79154.468750\n",
      "Train Epoch: 76 [15511/17352 (89%)] Loss: -66425.382812\n",
      "Train Epoch: 76 [16258/17352 (94%)] Loss: -87895.679688\n",
      "Train Epoch: 76 [17056/17352 (98%)] Loss: -85854.765625\n",
      "    epoch          : 76\n",
      "    loss           : -78430.49151816143\n",
      "    val_loss       : -40176.79379475911\n",
      "Train Epoch: 77 [128/17352 (1%)] Loss: -87011.085938\n",
      "Train Epoch: 77 [1536/17352 (9%)] Loss: -90465.093750\n",
      "Train Epoch: 77 [2944/17352 (17%)] Loss: -52505.628906\n",
      "Train Epoch: 77 [4352/17352 (25%)] Loss: -80938.843750\n",
      "Train Epoch: 77 [5760/17352 (33%)] Loss: -63581.878906\n",
      "Train Epoch: 77 [7168/17352 (41%)] Loss: -117876.937500\n",
      "Train Epoch: 77 [8576/17352 (49%)] Loss: -74456.343750\n",
      "Train Epoch: 77 [9984/17352 (58%)] Loss: -86715.828125\n",
      "Train Epoch: 77 [11392/17352 (66%)] Loss: -129599.421875\n",
      "Train Epoch: 77 [12800/17352 (74%)] Loss: -93552.226562\n",
      "Train Epoch: 77 [14208/17352 (82%)] Loss: -94443.296875\n",
      "Train Epoch: 77 [15525/17352 (89%)] Loss: -21296.152344\n",
      "Train Epoch: 77 [16338/17352 (94%)] Loss: -106147.796875\n",
      "Train Epoch: 77 [17001/17352 (98%)] Loss: -34348.976562\n",
      "    epoch          : 77\n",
      "    loss           : -81370.99229973914\n",
      "    val_loss       : -42030.991584269206\n",
      "Train Epoch: 78 [128/17352 (1%)] Loss: -82856.359375\n",
      "Train Epoch: 78 [1536/17352 (9%)] Loss: -52967.105469\n",
      "Train Epoch: 78 [2944/17352 (17%)] Loss: -99867.898438\n",
      "Train Epoch: 78 [4352/17352 (25%)] Loss: -130222.578125\n",
      "Train Epoch: 78 [5760/17352 (33%)] Loss: -107096.421875\n",
      "Train Epoch: 78 [7168/17352 (41%)] Loss: -77880.484375\n",
      "Train Epoch: 78 [8576/17352 (49%)] Loss: -71241.421875\n",
      "Train Epoch: 78 [9984/17352 (58%)] Loss: -110761.421875\n",
      "Train Epoch: 78 [11392/17352 (66%)] Loss: -84696.257812\n",
      "Train Epoch: 78 [12800/17352 (74%)] Loss: -93099.203125\n",
      "Train Epoch: 78 [14208/17352 (82%)] Loss: -126799.609375\n",
      "Train Epoch: 78 [15448/17352 (89%)] Loss: -2628.555664\n",
      "Train Epoch: 78 [16217/17352 (93%)] Loss: -11892.180664\n",
      "Train Epoch: 78 [16954/17352 (98%)] Loss: -27213.091797\n",
      "    epoch          : 78\n",
      "    loss           : -78614.68099340657\n",
      "    val_loss       : -42256.95659688314\n",
      "Train Epoch: 79 [128/17352 (1%)] Loss: -46866.304688\n",
      "Train Epoch: 79 [1536/17352 (9%)] Loss: -74938.023438\n",
      "Train Epoch: 79 [2944/17352 (17%)] Loss: -108557.156250\n",
      "Train Epoch: 79 [4352/17352 (25%)] Loss: -115146.171875\n",
      "Train Epoch: 79 [5760/17352 (33%)] Loss: -88699.382812\n",
      "Train Epoch: 79 [7168/17352 (41%)] Loss: -57638.929688\n",
      "Train Epoch: 79 [8576/17352 (49%)] Loss: -86463.539062\n",
      "Train Epoch: 79 [9984/17352 (58%)] Loss: -82451.156250\n",
      "Train Epoch: 79 [11392/17352 (66%)] Loss: -91424.210938\n",
      "Train Epoch: 79 [12800/17352 (74%)] Loss: -95411.218750\n",
      "Train Epoch: 79 [14208/17352 (82%)] Loss: -100955.968750\n",
      "Train Epoch: 79 [15541/17352 (90%)] Loss: -29033.835938\n",
      "Train Epoch: 79 [16219/17352 (93%)] Loss: -24887.390625\n",
      "Train Epoch: 79 [16877/17352 (97%)] Loss: -74423.984375\n",
      "    epoch          : 79\n",
      "    loss           : -80370.19651517933\n",
      "    val_loss       : -47649.626170857744\n",
      "Train Epoch: 80 [128/17352 (1%)] Loss: -76674.476562\n",
      "Train Epoch: 80 [1536/17352 (9%)] Loss: -116165.125000\n",
      "Train Epoch: 80 [2944/17352 (17%)] Loss: -102384.992188\n",
      "Train Epoch: 80 [4352/17352 (25%)] Loss: -83913.546875\n",
      "Train Epoch: 80 [5760/17352 (33%)] Loss: -37322.187500\n",
      "Train Epoch: 80 [7168/17352 (41%)] Loss: -109620.929688\n",
      "Train Epoch: 80 [8576/17352 (49%)] Loss: -61606.367188\n",
      "Train Epoch: 80 [9984/17352 (58%)] Loss: -97677.078125\n",
      "Train Epoch: 80 [11392/17352 (66%)] Loss: -92677.312500\n",
      "Train Epoch: 80 [12800/17352 (74%)] Loss: -104436.750000\n",
      "Train Epoch: 80 [14208/17352 (82%)] Loss: -99040.828125\n",
      "Train Epoch: 80 [15437/17352 (89%)] Loss: -19614.492188\n",
      "Train Epoch: 80 [16199/17352 (93%)] Loss: -15958.996094\n",
      "Train Epoch: 80 [17015/17352 (98%)] Loss: -31242.443359\n",
      "    epoch          : 80\n",
      "    loss           : -82159.79801705379\n",
      "    val_loss       : -32204.291890589397\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch80.pth ...\n",
      "Train Epoch: 81 [128/17352 (1%)] Loss: -20517.085938\n",
      "Train Epoch: 81 [1536/17352 (9%)] Loss: -113410.406250\n",
      "Train Epoch: 81 [2944/17352 (17%)] Loss: -82781.015625\n",
      "Train Epoch: 81 [4352/17352 (25%)] Loss: -77758.718750\n",
      "Train Epoch: 81 [5760/17352 (33%)] Loss: -93027.093750\n",
      "Train Epoch: 81 [7168/17352 (41%)] Loss: -61186.117188\n",
      "Train Epoch: 81 [8576/17352 (49%)] Loss: -80877.921875\n",
      "Train Epoch: 81 [9984/17352 (58%)] Loss: -103621.609375\n",
      "Train Epoch: 81 [11392/17352 (66%)] Loss: -64062.617188\n",
      "Train Epoch: 81 [12800/17352 (74%)] Loss: -107538.265625\n",
      "Train Epoch: 81 [14208/17352 (82%)] Loss: -124763.789062\n",
      "Train Epoch: 81 [15444/17352 (89%)] Loss: -26752.791016\n",
      "Train Epoch: 81 [16193/17352 (93%)] Loss: -4515.431152\n",
      "Train Epoch: 81 [16973/17352 (98%)] Loss: -62567.898438\n",
      "    epoch          : 81\n",
      "    loss           : -75644.42693641201\n",
      "    val_loss       : -47337.10239817302\n",
      "Train Epoch: 82 [128/17352 (1%)] Loss: -105591.656250\n",
      "Train Epoch: 82 [1536/17352 (9%)] Loss: -67886.265625\n",
      "Train Epoch: 82 [2944/17352 (17%)] Loss: -101647.109375\n",
      "Train Epoch: 82 [4352/17352 (25%)] Loss: -104199.203125\n",
      "Train Epoch: 82 [5760/17352 (33%)] Loss: -21274.666016\n",
      "Train Epoch: 82 [7168/17352 (41%)] Loss: -108187.476562\n",
      "Train Epoch: 82 [8576/17352 (49%)] Loss: -115583.906250\n",
      "Train Epoch: 82 [9984/17352 (58%)] Loss: -105823.531250\n",
      "Train Epoch: 82 [11392/17352 (66%)] Loss: -105453.281250\n",
      "Train Epoch: 82 [12800/17352 (74%)] Loss: -111885.109375\n",
      "Train Epoch: 82 [14208/17352 (82%)] Loss: -90306.601562\n",
      "Train Epoch: 82 [15490/17352 (89%)] Loss: -3018.795410\n",
      "Train Epoch: 82 [16176/17352 (93%)] Loss: -56196.843750\n",
      "Train Epoch: 82 [17047/17352 (98%)] Loss: -32310.425781\n",
      "    epoch          : 82\n",
      "    loss           : -81914.4132628985\n",
      "    val_loss       : -44748.451709111534\n",
      "Train Epoch: 83 [128/17352 (1%)] Loss: -106624.289062\n",
      "Train Epoch: 83 [1536/17352 (9%)] Loss: -122131.359375\n",
      "Train Epoch: 83 [2944/17352 (17%)] Loss: -83051.226562\n",
      "Train Epoch: 83 [4352/17352 (25%)] Loss: -94589.125000\n",
      "Train Epoch: 83 [5760/17352 (33%)] Loss: -114254.085938\n",
      "Train Epoch: 83 [7168/17352 (41%)] Loss: -66030.484375\n",
      "Train Epoch: 83 [8576/17352 (49%)] Loss: -60125.199219\n",
      "Train Epoch: 83 [9984/17352 (58%)] Loss: -90925.132812\n",
      "Train Epoch: 83 [11392/17352 (66%)] Loss: -64815.804688\n",
      "Train Epoch: 83 [12800/17352 (74%)] Loss: -69673.781250\n",
      "Train Epoch: 83 [14208/17352 (82%)] Loss: -85299.414062\n",
      "Train Epoch: 83 [15438/17352 (89%)] Loss: -2848.761230\n",
      "Train Epoch: 83 [16193/17352 (93%)] Loss: -45133.011719\n",
      "Train Epoch: 83 [16910/17352 (97%)] Loss: -54105.210938\n",
      "    epoch          : 83\n",
      "    loss           : -82625.42691511115\n",
      "    val_loss       : -43705.200940450035\n",
      "Train Epoch: 84 [128/17352 (1%)] Loss: -60088.898438\n",
      "Train Epoch: 84 [1536/17352 (9%)] Loss: -66860.765625\n",
      "Train Epoch: 84 [2944/17352 (17%)] Loss: -61621.433594\n",
      "Train Epoch: 84 [4352/17352 (25%)] Loss: -58440.960938\n",
      "Train Epoch: 84 [5760/17352 (33%)] Loss: -111423.070312\n",
      "Train Epoch: 84 [7168/17352 (41%)] Loss: -78614.171875\n",
      "Train Epoch: 84 [8576/17352 (49%)] Loss: -92183.765625\n",
      "Train Epoch: 84 [9984/17352 (58%)] Loss: -81390.929688\n",
      "Train Epoch: 84 [11392/17352 (66%)] Loss: -82737.343750\n",
      "Train Epoch: 84 [12800/17352 (74%)] Loss: -70009.492188\n",
      "Train Epoch: 84 [14208/17352 (82%)] Loss: -90722.203125\n",
      "Train Epoch: 84 [15552/17352 (90%)] Loss: -74258.921875\n",
      "Train Epoch: 84 [16375/17352 (94%)] Loss: -56389.117188\n",
      "Train Epoch: 84 [17002/17352 (98%)] Loss: -46935.039062\n",
      "    epoch          : 84\n",
      "    loss           : -77844.81139563234\n",
      "    val_loss       : -42462.924143473305\n",
      "Train Epoch: 85 [128/17352 (1%)] Loss: -112948.492188\n",
      "Train Epoch: 85 [1536/17352 (9%)] Loss: -108726.828125\n",
      "Train Epoch: 85 [2944/17352 (17%)] Loss: -83908.187500\n",
      "Train Epoch: 85 [4352/17352 (25%)] Loss: -78803.929688\n",
      "Train Epoch: 85 [5760/17352 (33%)] Loss: -142768.328125\n",
      "Train Epoch: 85 [7168/17352 (41%)] Loss: -90686.726562\n",
      "Train Epoch: 85 [8576/17352 (49%)] Loss: -91471.695312\n",
      "Train Epoch: 85 [9984/17352 (58%)] Loss: -77274.890625\n",
      "Train Epoch: 85 [11392/17352 (66%)] Loss: -102293.000000\n",
      "Train Epoch: 85 [12800/17352 (74%)] Loss: -101572.132812\n",
      "Train Epoch: 85 [14208/17352 (82%)] Loss: -64093.484375\n",
      "Train Epoch: 85 [15551/17352 (90%)] Loss: -66746.953125\n",
      "Train Epoch: 85 [16342/17352 (94%)] Loss: -70428.500000\n",
      "Train Epoch: 85 [17046/17352 (98%)] Loss: -26355.451172\n",
      "    epoch          : 85\n",
      "    loss           : -83342.2308423343\n",
      "    val_loss       : -42818.99628855388\n",
      "Train Epoch: 86 [128/17352 (1%)] Loss: -95626.476562\n",
      "Train Epoch: 86 [1536/17352 (9%)] Loss: -94129.359375\n",
      "Train Epoch: 86 [2944/17352 (17%)] Loss: -85602.757812\n",
      "Train Epoch: 86 [4352/17352 (25%)] Loss: -85202.617188\n",
      "Train Epoch: 86 [5760/17352 (33%)] Loss: -81345.539062\n",
      "Train Epoch: 86 [7168/17352 (41%)] Loss: -53631.593750\n",
      "Train Epoch: 86 [8576/17352 (49%)] Loss: -66586.250000\n",
      "Train Epoch: 86 [9984/17352 (58%)] Loss: -81560.757812\n",
      "Train Epoch: 86 [11392/17352 (66%)] Loss: -78132.992188\n",
      "Train Epoch: 86 [12800/17352 (74%)] Loss: -100883.789062\n",
      "Train Epoch: 86 [14208/17352 (82%)] Loss: -89723.460938\n",
      "Train Epoch: 86 [15514/17352 (89%)] Loss: -49014.148438\n",
      "Train Epoch: 86 [16158/17352 (93%)] Loss: -2694.811035\n",
      "Train Epoch: 86 [16998/17352 (98%)] Loss: -38880.531250\n",
      "    epoch          : 86\n",
      "    loss           : -79644.26085033032\n",
      "    val_loss       : -47613.01592203776\n",
      "Train Epoch: 87 [128/17352 (1%)] Loss: -98782.023438\n",
      "Train Epoch: 87 [1536/17352 (9%)] Loss: -93520.367188\n",
      "Train Epoch: 87 [2944/17352 (17%)] Loss: -124675.500000\n",
      "Train Epoch: 87 [4352/17352 (25%)] Loss: -56109.953125\n",
      "Train Epoch: 87 [5760/17352 (33%)] Loss: -104545.234375\n",
      "Train Epoch: 87 [7168/17352 (41%)] Loss: -106231.273438\n",
      "Train Epoch: 87 [8576/17352 (49%)] Loss: -116431.148438\n",
      "Train Epoch: 87 [9984/17352 (58%)] Loss: -117320.890625\n",
      "Train Epoch: 87 [11392/17352 (66%)] Loss: -101870.609375\n",
      "Train Epoch: 87 [12800/17352 (74%)] Loss: -96156.546875\n",
      "Train Epoch: 87 [14208/17352 (82%)] Loss: -77218.789062\n",
      "Train Epoch: 87 [15510/17352 (89%)] Loss: -28441.703125\n",
      "Train Epoch: 87 [16289/17352 (94%)] Loss: -56578.570312\n",
      "Train Epoch: 87 [17081/17352 (98%)] Loss: -5207.580078\n",
      "    epoch          : 87\n",
      "    loss           : -88000.33121690174\n",
      "    val_loss       : -44843.28816731771\n",
      "Train Epoch: 88 [128/17352 (1%)] Loss: -39621.277344\n",
      "Train Epoch: 88 [1536/17352 (9%)] Loss: -133460.000000\n",
      "Train Epoch: 88 [2944/17352 (17%)] Loss: -93649.460938\n",
      "Train Epoch: 88 [4352/17352 (25%)] Loss: -104436.648438\n",
      "Train Epoch: 88 [5760/17352 (33%)] Loss: -54868.074219\n",
      "Train Epoch: 88 [7168/17352 (41%)] Loss: -116531.765625\n",
      "Train Epoch: 88 [8576/17352 (49%)] Loss: -54138.921875\n",
      "Train Epoch: 88 [9984/17352 (58%)] Loss: -68973.179688\n",
      "Train Epoch: 88 [11392/17352 (66%)] Loss: -110685.976562\n",
      "Train Epoch: 88 [12800/17352 (74%)] Loss: -88264.656250\n",
      "Train Epoch: 88 [14208/17352 (82%)] Loss: -137554.265625\n",
      "Train Epoch: 88 [15459/17352 (89%)] Loss: -12668.117188\n",
      "Train Epoch: 88 [16193/17352 (93%)] Loss: -2918.130615\n",
      "Train Epoch: 88 [16968/17352 (98%)] Loss: -1096.035645\n",
      "    epoch          : 88\n",
      "    loss           : -83866.03819899591\n",
      "    val_loss       : -18656.977172851562\n",
      "Train Epoch: 89 [128/17352 (1%)] Loss: -60164.703125\n",
      "Train Epoch: 89 [1536/17352 (9%)] Loss: -77155.328125\n",
      "Train Epoch: 89 [2944/17352 (17%)] Loss: -104331.054688\n",
      "Train Epoch: 89 [4352/17352 (25%)] Loss: -67801.406250\n",
      "Train Epoch: 89 [5760/17352 (33%)] Loss: -77593.789062\n",
      "Train Epoch: 89 [7168/17352 (41%)] Loss: -64622.718750\n",
      "Train Epoch: 89 [8576/17352 (49%)] Loss: -98823.265625\n",
      "Train Epoch: 89 [9984/17352 (58%)] Loss: -71384.093750\n",
      "Train Epoch: 89 [11392/17352 (66%)] Loss: -105326.375000\n",
      "Train Epoch: 89 [12800/17352 (74%)] Loss: -87504.492188\n",
      "Train Epoch: 89 [14208/17352 (82%)] Loss: -108581.296875\n",
      "Train Epoch: 89 [15564/17352 (90%)] Loss: -83505.796875\n",
      "Train Epoch: 89 [16266/17352 (94%)] Loss: -70375.835938\n",
      "Train Epoch: 89 [17074/17352 (98%)] Loss: -25517.826172\n",
      "    epoch          : 89\n",
      "    loss           : -79233.89272968881\n",
      "    val_loss       : -46361.62195129394\n",
      "Train Epoch: 90 [128/17352 (1%)] Loss: -88230.671875\n",
      "Train Epoch: 90 [1536/17352 (9%)] Loss: -114905.843750\n",
      "Train Epoch: 90 [2944/17352 (17%)] Loss: -96870.835938\n",
      "Train Epoch: 90 [4352/17352 (25%)] Loss: -103577.656250\n",
      "Train Epoch: 90 [5760/17352 (33%)] Loss: -89681.140625\n",
      "Train Epoch: 90 [7168/17352 (41%)] Loss: -90292.367188\n",
      "Train Epoch: 90 [8576/17352 (49%)] Loss: -102326.843750\n",
      "Train Epoch: 90 [9984/17352 (58%)] Loss: -60681.039062\n",
      "Train Epoch: 90 [11392/17352 (66%)] Loss: -58494.468750\n",
      "Train Epoch: 90 [12800/17352 (74%)] Loss: -58769.527344\n",
      "Train Epoch: 90 [14208/17352 (82%)] Loss: -92937.601562\n",
      "Train Epoch: 90 [15502/17352 (89%)] Loss: -28296.955078\n",
      "Train Epoch: 90 [16342/17352 (94%)] Loss: -24686.246094\n",
      "Train Epoch: 90 [17037/17352 (98%)] Loss: -10104.368164\n",
      "    epoch          : 90\n",
      "    loss           : -84981.99895789639\n",
      "    val_loss       : -43241.67850952149\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch90.pth ...\n",
      "Train Epoch: 91 [128/17352 (1%)] Loss: -97189.367188\n",
      "Train Epoch: 91 [1536/17352 (9%)] Loss: -103647.171875\n",
      "Train Epoch: 91 [2944/17352 (17%)] Loss: -28686.974609\n",
      "Train Epoch: 91 [4352/17352 (25%)] Loss: -111542.000000\n",
      "Train Epoch: 91 [5760/17352 (33%)] Loss: -111863.375000\n",
      "Train Epoch: 91 [7168/17352 (41%)] Loss: -103337.976562\n",
      "Train Epoch: 91 [8576/17352 (49%)] Loss: -76032.875000\n",
      "Train Epoch: 91 [9984/17352 (58%)] Loss: -99510.773438\n",
      "Train Epoch: 91 [11392/17352 (66%)] Loss: -56815.593750\n",
      "Train Epoch: 91 [12800/17352 (74%)] Loss: -124607.921875\n",
      "Train Epoch: 91 [14208/17352 (82%)] Loss: -139618.906250\n",
      "Train Epoch: 91 [15560/17352 (90%)] Loss: -76192.812500\n",
      "Train Epoch: 91 [16336/17352 (94%)] Loss: -10144.526367\n",
      "Train Epoch: 91 [16988/17352 (98%)] Loss: -35918.820312\n",
      "    epoch          : 91\n",
      "    loss           : -85774.55584593907\n",
      "    val_loss       : -51446.707331339516\n",
      "Train Epoch: 92 [128/17352 (1%)] Loss: -55678.164062\n",
      "Train Epoch: 92 [1536/17352 (9%)] Loss: -94664.820312\n",
      "Train Epoch: 92 [2944/17352 (17%)] Loss: -80328.054688\n",
      "Train Epoch: 92 [4352/17352 (25%)] Loss: -82105.789062\n",
      "Train Epoch: 92 [5760/17352 (33%)] Loss: -112131.265625\n",
      "Train Epoch: 92 [7168/17352 (41%)] Loss: -95075.796875\n",
      "Train Epoch: 92 [8576/17352 (49%)] Loss: -86793.171875\n",
      "Train Epoch: 92 [9984/17352 (58%)] Loss: -66958.242188\n",
      "Train Epoch: 92 [11392/17352 (66%)] Loss: -86498.078125\n",
      "Train Epoch: 92 [12800/17352 (74%)] Loss: -91183.515625\n",
      "Train Epoch: 92 [14208/17352 (82%)] Loss: -103744.375000\n",
      "Train Epoch: 92 [15574/17352 (90%)] Loss: -78174.093750\n",
      "Train Epoch: 92 [16327/17352 (94%)] Loss: -64552.699219\n",
      "Train Epoch: 92 [16996/17352 (98%)] Loss: -30795.605469\n",
      "    epoch          : 92\n",
      "    loss           : -87428.72173539744\n",
      "    val_loss       : -49671.04069925944\n",
      "Train Epoch: 93 [128/17352 (1%)] Loss: -84809.601562\n",
      "Train Epoch: 93 [1536/17352 (9%)] Loss: -104257.125000\n",
      "Train Epoch: 93 [2944/17352 (17%)] Loss: -125365.859375\n",
      "Train Epoch: 93 [4352/17352 (25%)] Loss: -71088.250000\n",
      "Train Epoch: 93 [5760/17352 (33%)] Loss: -74087.953125\n",
      "Train Epoch: 93 [7168/17352 (41%)] Loss: -110353.937500\n",
      "Train Epoch: 93 [8576/17352 (49%)] Loss: -72975.046875\n",
      "Train Epoch: 93 [9984/17352 (58%)] Loss: -82824.960938\n",
      "Train Epoch: 93 [11392/17352 (66%)] Loss: -104576.500000\n",
      "Train Epoch: 93 [12800/17352 (74%)] Loss: -138411.703125\n",
      "Train Epoch: 93 [14208/17352 (82%)] Loss: -51983.167969\n",
      "Train Epoch: 93 [15525/17352 (89%)] Loss: -76476.890625\n",
      "Train Epoch: 93 [16192/17352 (93%)] Loss: -8051.087891\n",
      "Train Epoch: 93 [16928/17352 (98%)] Loss: -10716.855469\n",
      "    epoch          : 93\n",
      "    loss           : -85956.19305870518\n",
      "    val_loss       : -23272.925026448567\n",
      "Train Epoch: 94 [128/17352 (1%)] Loss: -37850.500000\n",
      "Train Epoch: 94 [1536/17352 (9%)] Loss: -129377.328125\n",
      "Train Epoch: 94 [2944/17352 (17%)] Loss: -79175.085938\n",
      "Train Epoch: 94 [4352/17352 (25%)] Loss: -101542.109375\n",
      "Train Epoch: 94 [5760/17352 (33%)] Loss: -69114.554688\n",
      "Train Epoch: 94 [7168/17352 (41%)] Loss: -105222.984375\n",
      "Train Epoch: 94 [8576/17352 (49%)] Loss: -87313.718750\n",
      "Train Epoch: 94 [9984/17352 (58%)] Loss: -121380.101562\n",
      "Train Epoch: 94 [11392/17352 (66%)] Loss: -95853.914062\n",
      "Train Epoch: 94 [12800/17352 (74%)] Loss: -89771.640625\n",
      "Train Epoch: 94 [14208/17352 (82%)] Loss: -94746.125000\n",
      "Train Epoch: 94 [15490/17352 (89%)] Loss: -16685.025391\n",
      "Train Epoch: 94 [16379/17352 (94%)] Loss: -75753.046875\n",
      "Train Epoch: 94 [17012/17352 (98%)] Loss: -65852.445312\n",
      "    epoch          : 94\n",
      "    loss           : -76444.09078426489\n",
      "    val_loss       : -48849.09780273437\n",
      "Train Epoch: 95 [128/17352 (1%)] Loss: -81887.125000\n",
      "Train Epoch: 95 [1536/17352 (9%)] Loss: -104308.796875\n",
      "Train Epoch: 95 [2944/17352 (17%)] Loss: -61838.429688\n",
      "Train Epoch: 95 [4352/17352 (25%)] Loss: -106942.843750\n",
      "Train Epoch: 95 [5760/17352 (33%)] Loss: -43305.578125\n",
      "Train Epoch: 95 [7168/17352 (41%)] Loss: -45924.300781\n",
      "Train Epoch: 95 [8576/17352 (49%)] Loss: -72245.914062\n",
      "Train Epoch: 95 [9984/17352 (58%)] Loss: -115740.000000\n",
      "Train Epoch: 95 [11392/17352 (66%)] Loss: -31745.937500\n",
      "Train Epoch: 95 [12800/17352 (74%)] Loss: -110477.015625\n",
      "Train Epoch: 95 [14208/17352 (82%)] Loss: -121223.320312\n",
      "Train Epoch: 95 [15550/17352 (90%)] Loss: -84079.218750\n",
      "Train Epoch: 95 [16197/17352 (93%)] Loss: -65079.441406\n",
      "Train Epoch: 95 [17018/17352 (98%)] Loss: -51821.445312\n",
      "    epoch          : 95\n",
      "    loss           : -87347.32382550335\n",
      "    val_loss       : -46447.14909261068\n",
      "Train Epoch: 96 [128/17352 (1%)] Loss: -66204.156250\n",
      "Train Epoch: 96 [1536/17352 (9%)] Loss: -60885.000000\n",
      "Train Epoch: 96 [2944/17352 (17%)] Loss: -127152.125000\n",
      "Train Epoch: 96 [4352/17352 (25%)] Loss: -103948.476562\n",
      "Train Epoch: 96 [5760/17352 (33%)] Loss: -70973.664062\n",
      "Train Epoch: 96 [7168/17352 (41%)] Loss: -131663.156250\n",
      "Train Epoch: 96 [8576/17352 (49%)] Loss: -120168.359375\n",
      "Train Epoch: 96 [9984/17352 (58%)] Loss: -118697.367188\n",
      "Train Epoch: 96 [11392/17352 (66%)] Loss: -100557.312500\n",
      "Train Epoch: 96 [12800/17352 (74%)] Loss: -111637.546875\n",
      "Train Epoch: 96 [14208/17352 (82%)] Loss: -119568.476562\n",
      "Train Epoch: 96 [15457/17352 (89%)] Loss: -6614.816895\n",
      "Train Epoch: 96 [16187/17352 (93%)] Loss: -100236.625000\n",
      "Train Epoch: 96 [16910/17352 (97%)] Loss: -21471.060547\n",
      "    epoch          : 96\n",
      "    loss           : -85602.04808996827\n",
      "    val_loss       : -39366.684391276045\n",
      "Train Epoch: 97 [128/17352 (1%)] Loss: -56507.359375\n",
      "Train Epoch: 97 [1536/17352 (9%)] Loss: -60618.656250\n",
      "Train Epoch: 97 [2944/17352 (17%)] Loss: -75025.375000\n",
      "Train Epoch: 97 [4352/17352 (25%)] Loss: -104425.484375\n",
      "Train Epoch: 97 [5760/17352 (33%)] Loss: -70317.523438\n",
      "Train Epoch: 97 [7168/17352 (41%)] Loss: -81045.828125\n",
      "Train Epoch: 97 [8576/17352 (49%)] Loss: -119540.484375\n",
      "Train Epoch: 97 [9984/17352 (58%)] Loss: -61672.109375\n",
      "Train Epoch: 97 [11392/17352 (66%)] Loss: -109241.750000\n",
      "Train Epoch: 97 [12800/17352 (74%)] Loss: -73147.859375\n",
      "Train Epoch: 97 [14208/17352 (82%)] Loss: -78898.968750\n",
      "Train Epoch: 97 [15546/17352 (90%)] Loss: -41733.925781\n",
      "Train Epoch: 97 [16343/17352 (94%)] Loss: -62687.953125\n",
      "Train Epoch: 97 [17049/17352 (98%)] Loss: -29694.507812\n",
      "    epoch          : 97\n",
      "    loss           : -86721.18976444525\n",
      "    val_loss       : -54428.69417724609\n",
      "Train Epoch: 98 [128/17352 (1%)] Loss: -55324.664062\n",
      "Train Epoch: 98 [1536/17352 (9%)] Loss: -106898.960938\n",
      "Train Epoch: 98 [2944/17352 (17%)] Loss: -103081.835938\n",
      "Train Epoch: 98 [4352/17352 (25%)] Loss: -74958.734375\n",
      "Train Epoch: 98 [5760/17352 (33%)] Loss: -98945.078125\n",
      "Train Epoch: 98 [7168/17352 (41%)] Loss: -48430.828125\n",
      "Train Epoch: 98 [8576/17352 (49%)] Loss: -68955.398438\n",
      "Train Epoch: 98 [9984/17352 (58%)] Loss: -112272.210938\n",
      "Train Epoch: 98 [11392/17352 (66%)] Loss: -61909.625000\n",
      "Train Epoch: 98 [12800/17352 (74%)] Loss: -75257.039062\n",
      "Train Epoch: 98 [14208/17352 (82%)] Loss: -121536.859375\n",
      "Train Epoch: 98 [15471/17352 (89%)] Loss: -24970.078125\n",
      "Train Epoch: 98 [16217/17352 (93%)] Loss: -60464.472656\n",
      "Train Epoch: 98 [16926/17352 (98%)] Loss: -322.828552\n",
      "    epoch          : 98\n",
      "    loss           : -88497.81165083303\n",
      "    val_loss       : -28273.976627604166\n",
      "Train Epoch: 99 [128/17352 (1%)] Loss: -57853.664062\n",
      "Train Epoch: 99 [1536/17352 (9%)] Loss: -84005.281250\n",
      "Train Epoch: 99 [2944/17352 (17%)] Loss: -97859.273438\n",
      "Train Epoch: 99 [4352/17352 (25%)] Loss: -140704.156250\n",
      "Train Epoch: 99 [5760/17352 (33%)] Loss: -113829.046875\n",
      "Train Epoch: 99 [7168/17352 (41%)] Loss: -93508.781250\n",
      "Train Epoch: 99 [8576/17352 (49%)] Loss: -95211.890625\n",
      "Train Epoch: 99 [9984/17352 (58%)] Loss: -110230.609375\n",
      "Train Epoch: 99 [11392/17352 (66%)] Loss: -122410.234375\n",
      "Train Epoch: 99 [12800/17352 (74%)] Loss: -113548.859375\n",
      "Train Epoch: 99 [14208/17352 (82%)] Loss: -126299.468750\n",
      "Train Epoch: 99 [15577/17352 (90%)] Loss: -116255.289062\n",
      "Train Epoch: 99 [16204/17352 (93%)] Loss: -1808.657227\n",
      "Train Epoch: 99 [16975/17352 (98%)] Loss: -53243.871094\n",
      "    epoch          : 99\n",
      "    loss           : -85887.33310940121\n",
      "    val_loss       : -47379.793395996094\n",
      "Train Epoch: 100 [128/17352 (1%)] Loss: -106534.757812\n",
      "Train Epoch: 100 [1536/17352 (9%)] Loss: -117481.226562\n",
      "Train Epoch: 100 [2944/17352 (17%)] Loss: -97105.531250\n",
      "Train Epoch: 100 [4352/17352 (25%)] Loss: -83717.968750\n",
      "Train Epoch: 100 [5760/17352 (33%)] Loss: -72519.593750\n",
      "Train Epoch: 100 [7168/17352 (41%)] Loss: -103937.875000\n",
      "Train Epoch: 100 [8576/17352 (49%)] Loss: -109863.867188\n",
      "Train Epoch: 100 [9984/17352 (58%)] Loss: -94508.343750\n",
      "Train Epoch: 100 [11392/17352 (66%)] Loss: -111241.960938\n",
      "Train Epoch: 100 [12800/17352 (74%)] Loss: -133176.421875\n",
      "Train Epoch: 100 [14208/17352 (82%)] Loss: -100522.515625\n",
      "Train Epoch: 100 [15408/17352 (89%)] Loss: -22839.523438\n",
      "Train Epoch: 100 [16205/17352 (93%)] Loss: -74177.296875\n",
      "Train Epoch: 100 [16900/17352 (97%)] Loss: -36061.171875\n",
      "    epoch          : 100\n",
      "    loss           : -90313.2205556575\n",
      "    val_loss       : -50244.219978841145\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [128/17352 (1%)] Loss: -84736.687500\n",
      "Train Epoch: 101 [1536/17352 (9%)] Loss: -102702.960938\n",
      "Train Epoch: 101 [2944/17352 (17%)] Loss: -113951.148438\n",
      "Train Epoch: 101 [4352/17352 (25%)] Loss: -129586.609375\n",
      "Train Epoch: 101 [5760/17352 (33%)] Loss: -97679.820312\n",
      "Train Epoch: 101 [7168/17352 (41%)] Loss: -78766.921875\n",
      "Train Epoch: 101 [8576/17352 (49%)] Loss: -91937.468750\n",
      "Train Epoch: 101 [9984/17352 (58%)] Loss: -122556.625000\n",
      "Train Epoch: 101 [11392/17352 (66%)] Loss: -111809.398438\n",
      "Train Epoch: 101 [12800/17352 (74%)] Loss: -82784.914062\n",
      "Train Epoch: 101 [14208/17352 (82%)] Loss: -126859.609375\n",
      "Train Epoch: 101 [15414/17352 (89%)] Loss: -25623.685547\n",
      "Train Epoch: 101 [16243/17352 (94%)] Loss: -47650.617188\n",
      "Train Epoch: 101 [17004/17352 (98%)] Loss: -82984.757812\n",
      "    epoch          : 101\n",
      "    loss           : -90830.34329612783\n",
      "    val_loss       : -49726.203586832686\n",
      "Train Epoch: 102 [128/17352 (1%)] Loss: -88402.984375\n",
      "Train Epoch: 102 [1536/17352 (9%)] Loss: -112662.843750\n",
      "Train Epoch: 102 [2944/17352 (17%)] Loss: -146004.734375\n",
      "Train Epoch: 102 [4352/17352 (25%)] Loss: -102672.679688\n",
      "Train Epoch: 102 [5760/17352 (33%)] Loss: -109628.523438\n",
      "Train Epoch: 102 [7168/17352 (41%)] Loss: -137976.250000\n",
      "Train Epoch: 102 [8576/17352 (49%)] Loss: -116776.984375\n",
      "Train Epoch: 102 [9984/17352 (58%)] Loss: -74108.031250\n",
      "Train Epoch: 102 [11392/17352 (66%)] Loss: -114541.250000\n",
      "Train Epoch: 102 [12800/17352 (74%)] Loss: -104462.343750\n",
      "Train Epoch: 102 [14208/17352 (82%)] Loss: -81507.015625\n",
      "Train Epoch: 102 [15491/17352 (89%)] Loss: -83245.500000\n",
      "Train Epoch: 102 [16324/17352 (94%)] Loss: -54676.597656\n",
      "Train Epoch: 102 [16871/17352 (97%)] Loss: -32264.146484\n",
      "    epoch          : 102\n",
      "    loss           : -90154.54683895239\n",
      "    val_loss       : -48517.69365641276\n",
      "Train Epoch: 103 [128/17352 (1%)] Loss: -108439.031250\n",
      "Train Epoch: 103 [1536/17352 (9%)] Loss: -143308.375000\n",
      "Train Epoch: 103 [2944/17352 (17%)] Loss: -82028.390625\n",
      "Train Epoch: 103 [4352/17352 (25%)] Loss: -84696.593750\n",
      "Train Epoch: 103 [5760/17352 (33%)] Loss: -104785.937500\n",
      "Train Epoch: 103 [7168/17352 (41%)] Loss: -95462.945312\n",
      "Train Epoch: 103 [8576/17352 (49%)] Loss: -122799.882812\n",
      "Train Epoch: 103 [9984/17352 (58%)] Loss: -101690.625000\n",
      "Train Epoch: 103 [11392/17352 (66%)] Loss: -112506.546875\n",
      "Train Epoch: 103 [12800/17352 (74%)] Loss: -127819.015625\n",
      "Train Epoch: 103 [14208/17352 (82%)] Loss: -95132.851562\n",
      "Train Epoch: 103 [15550/17352 (90%)] Loss: -64090.257812\n",
      "Train Epoch: 103 [16326/17352 (94%)] Loss: -74121.234375\n",
      "Train Epoch: 103 [16916/17352 (97%)] Loss: -39598.664062\n",
      "    epoch          : 103\n",
      "    loss           : -95795.74937080537\n",
      "    val_loss       : -52605.78658955892\n",
      "Train Epoch: 104 [128/17352 (1%)] Loss: -114968.031250\n",
      "Train Epoch: 104 [1536/17352 (9%)] Loss: -125988.406250\n",
      "Train Epoch: 104 [2944/17352 (17%)] Loss: -74091.734375\n",
      "Train Epoch: 104 [4352/17352 (25%)] Loss: -110685.765625\n",
      "Train Epoch: 104 [5760/17352 (33%)] Loss: -91903.390625\n",
      "Train Epoch: 104 [7168/17352 (41%)] Loss: -67142.539062\n",
      "Train Epoch: 104 [8576/17352 (49%)] Loss: -107010.585938\n",
      "Train Epoch: 104 [9984/17352 (58%)] Loss: -133848.718750\n",
      "Train Epoch: 104 [11392/17352 (66%)] Loss: -103672.335938\n",
      "Train Epoch: 104 [12800/17352 (74%)] Loss: -57265.335938\n",
      "Train Epoch: 104 [14208/17352 (82%)] Loss: -116932.703125\n",
      "Train Epoch: 104 [15533/17352 (90%)] Loss: -52689.121094\n",
      "Train Epoch: 104 [16263/17352 (94%)] Loss: -27585.945312\n",
      "Train Epoch: 104 [17042/17352 (98%)] Loss: -28679.164062\n",
      "    epoch          : 104\n",
      "    loss           : -90499.54083047138\n",
      "    val_loss       : -14595.589381917318\n",
      "Train Epoch: 105 [128/17352 (1%)] Loss: -34455.976562\n",
      "Train Epoch: 105 [1536/17352 (9%)] Loss: -70971.937500\n",
      "Train Epoch: 105 [2944/17352 (17%)] Loss: -81433.718750\n",
      "Train Epoch: 105 [4352/17352 (25%)] Loss: -57569.210938\n",
      "Train Epoch: 105 [5760/17352 (33%)] Loss: -94316.390625\n",
      "Train Epoch: 105 [7168/17352 (41%)] Loss: -77435.539062\n",
      "Train Epoch: 105 [8576/17352 (49%)] Loss: -89976.187500\n",
      "Train Epoch: 105 [9984/17352 (58%)] Loss: -88301.906250\n",
      "Train Epoch: 105 [11392/17352 (66%)] Loss: -108185.031250\n",
      "Train Epoch: 105 [12800/17352 (74%)] Loss: -96511.000000\n",
      "Train Epoch: 105 [14208/17352 (82%)] Loss: -98512.750000\n",
      "Train Epoch: 105 [15504/17352 (89%)] Loss: -60509.960938\n",
      "Train Epoch: 105 [16208/17352 (93%)] Loss: -5520.734375\n",
      "Train Epoch: 105 [16952/17352 (98%)] Loss: -20008.402344\n",
      "    epoch          : 105\n",
      "    loss           : -77216.26997037542\n",
      "    val_loss       : -43237.85625\n",
      "Train Epoch: 106 [128/17352 (1%)] Loss: -96066.828125\n",
      "Train Epoch: 106 [1536/17352 (9%)] Loss: -90799.734375\n",
      "Train Epoch: 106 [2944/17352 (17%)] Loss: -44786.304688\n",
      "Train Epoch: 106 [4352/17352 (25%)] Loss: -97263.320312\n",
      "Train Epoch: 106 [5760/17352 (33%)] Loss: -92719.734375\n",
      "Train Epoch: 106 [7168/17352 (41%)] Loss: -110168.640625\n",
      "Train Epoch: 106 [8576/17352 (49%)] Loss: -105116.093750\n",
      "Train Epoch: 106 [9984/17352 (58%)] Loss: -94332.156250\n",
      "Train Epoch: 106 [11392/17352 (66%)] Loss: -114260.773438\n",
      "Train Epoch: 106 [12800/17352 (74%)] Loss: -100362.906250\n",
      "Train Epoch: 106 [14208/17352 (82%)] Loss: -107787.437500\n",
      "Train Epoch: 106 [15564/17352 (90%)] Loss: -49224.585938\n",
      "Train Epoch: 106 [16409/17352 (95%)] Loss: -43782.085938\n",
      "Train Epoch: 106 [17111/17352 (99%)] Loss: -63951.269531\n",
      "    epoch          : 106\n",
      "    loss           : -89622.20038374318\n",
      "    val_loss       : -49450.33049723307\n",
      "Train Epoch: 107 [128/17352 (1%)] Loss: -75449.265625\n",
      "Train Epoch: 107 [1536/17352 (9%)] Loss: -122876.625000\n",
      "Train Epoch: 107 [2944/17352 (17%)] Loss: -108111.109375\n",
      "Train Epoch: 107 [4352/17352 (25%)] Loss: -110624.570312\n",
      "Train Epoch: 107 [5760/17352 (33%)] Loss: -132902.421875\n",
      "Train Epoch: 107 [7168/17352 (41%)] Loss: -103783.523438\n",
      "Train Epoch: 107 [8576/17352 (49%)] Loss: -119069.898438\n",
      "Train Epoch: 107 [9984/17352 (58%)] Loss: -96880.710938\n",
      "Train Epoch: 107 [11392/17352 (66%)] Loss: -72546.335938\n",
      "Train Epoch: 107 [12800/17352 (74%)] Loss: -105832.117188\n",
      "Train Epoch: 107 [14208/17352 (82%)] Loss: -78642.312500\n",
      "Train Epoch: 107 [15491/17352 (89%)] Loss: -71275.132812\n",
      "Train Epoch: 107 [16330/17352 (94%)] Loss: -60141.671875\n",
      "Train Epoch: 107 [17144/17352 (99%)] Loss: -87801.398438\n",
      "    epoch          : 107\n",
      "    loss           : -93532.87475422084\n",
      "    val_loss       : -52213.76818033854\n",
      "Train Epoch: 108 [128/17352 (1%)] Loss: -130400.601562\n",
      "Train Epoch: 108 [1536/17352 (9%)] Loss: -120108.812500\n",
      "Train Epoch: 108 [2944/17352 (17%)] Loss: -85414.804688\n",
      "Train Epoch: 108 [4352/17352 (25%)] Loss: -113656.195312\n",
      "Train Epoch: 108 [5760/17352 (33%)] Loss: -68942.820312\n",
      "Train Epoch: 108 [7168/17352 (41%)] Loss: -132528.375000\n",
      "Train Epoch: 108 [8576/17352 (49%)] Loss: -69811.937500\n",
      "Train Epoch: 108 [9984/17352 (58%)] Loss: -92863.000000\n",
      "Train Epoch: 108 [11392/17352 (66%)] Loss: -88041.828125\n",
      "Train Epoch: 108 [12800/17352 (74%)] Loss: -57448.390625\n",
      "Train Epoch: 108 [14208/17352 (82%)] Loss: -145618.578125\n",
      "Train Epoch: 108 [15571/17352 (90%)] Loss: -84997.351562\n",
      "Train Epoch: 108 [16140/17352 (93%)] Loss: -34105.707031\n",
      "Train Epoch: 108 [17040/17352 (98%)] Loss: -60714.273438\n",
      "    epoch          : 108\n",
      "    loss           : -94738.81604331612\n",
      "    val_loss       : -49995.18232421875\n",
      "Train Epoch: 109 [128/17352 (1%)] Loss: -77183.804688\n",
      "Train Epoch: 109 [1536/17352 (9%)] Loss: -87721.679688\n",
      "Train Epoch: 109 [2944/17352 (17%)] Loss: -73721.171875\n",
      "Train Epoch: 109 [4352/17352 (25%)] Loss: -97106.437500\n",
      "Train Epoch: 109 [5760/17352 (33%)] Loss: -82603.296875\n",
      "Train Epoch: 109 [7168/17352 (41%)] Loss: -104361.554688\n",
      "Train Epoch: 109 [8576/17352 (49%)] Loss: -97025.109375\n",
      "Train Epoch: 109 [9984/17352 (58%)] Loss: -135940.218750\n",
      "Train Epoch: 109 [11392/17352 (66%)] Loss: -100067.796875\n",
      "Train Epoch: 109 [12800/17352 (74%)] Loss: -106252.281250\n",
      "Train Epoch: 109 [14208/17352 (82%)] Loss: -105977.031250\n",
      "Train Epoch: 109 [15399/17352 (89%)] Loss: -37674.949219\n",
      "Train Epoch: 109 [16303/17352 (94%)] Loss: -77904.773438\n",
      "Train Epoch: 109 [17016/17352 (98%)] Loss: -50194.171875\n",
      "    epoch          : 109\n",
      "    loss           : -95184.60622247274\n",
      "    val_loss       : -49870.43893229167\n",
      "Train Epoch: 110 [128/17352 (1%)] Loss: -114704.593750\n",
      "Train Epoch: 110 [1536/17352 (9%)] Loss: -131165.375000\n",
      "Train Epoch: 110 [2944/17352 (17%)] Loss: -92149.937500\n",
      "Train Epoch: 110 [4352/17352 (25%)] Loss: -63510.929688\n",
      "Train Epoch: 110 [5760/17352 (33%)] Loss: -66800.156250\n",
      "Train Epoch: 110 [7168/17352 (41%)] Loss: -112159.187500\n",
      "Train Epoch: 110 [8576/17352 (49%)] Loss: -66064.796875\n",
      "Train Epoch: 110 [9984/17352 (58%)] Loss: -73377.390625\n",
      "Train Epoch: 110 [11392/17352 (66%)] Loss: -104434.367188\n",
      "Train Epoch: 110 [12800/17352 (74%)] Loss: -92522.625000\n",
      "Train Epoch: 110 [14208/17352 (82%)] Loss: -111159.226562\n",
      "Train Epoch: 110 [15519/17352 (89%)] Loss: -48053.835938\n",
      "Train Epoch: 110 [16288/17352 (94%)] Loss: -51497.226562\n",
      "Train Epoch: 110 [17068/17352 (98%)] Loss: -4195.103516\n",
      "    epoch          : 110\n",
      "    loss           : -92623.19865345795\n",
      "    val_loss       : -52958.661641438805\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch110.pth ...\n",
      "Train Epoch: 111 [128/17352 (1%)] Loss: -78996.484375\n",
      "Train Epoch: 111 [1536/17352 (9%)] Loss: -132498.875000\n",
      "Train Epoch: 111 [2944/17352 (17%)] Loss: -106552.921875\n",
      "Train Epoch: 111 [4352/17352 (25%)] Loss: -57085.554688\n",
      "Train Epoch: 111 [5760/17352 (33%)] Loss: -60781.976562\n",
      "Train Epoch: 111 [7168/17352 (41%)] Loss: -91213.289062\n",
      "Train Epoch: 111 [8576/17352 (49%)] Loss: -72730.117188\n",
      "Train Epoch: 111 [9984/17352 (58%)] Loss: -91442.093750\n",
      "Train Epoch: 111 [11392/17352 (66%)] Loss: -119309.164062\n",
      "Train Epoch: 111 [12800/17352 (74%)] Loss: -134005.453125\n",
      "Train Epoch: 111 [14208/17352 (82%)] Loss: -52345.898438\n",
      "Train Epoch: 111 [15520/17352 (89%)] Loss: -41619.558594\n",
      "Train Epoch: 111 [16245/17352 (94%)] Loss: -42054.105469\n",
      "Train Epoch: 111 [17099/17352 (99%)] Loss: -21620.404297\n",
      "    epoch          : 111\n",
      "    loss           : -85893.75748807153\n",
      "    val_loss       : -22447.428788248697\n",
      "Train Epoch: 112 [128/17352 (1%)] Loss: -42735.808594\n",
      "Train Epoch: 112 [1536/17352 (9%)] Loss: -67391.156250\n",
      "Train Epoch: 112 [2944/17352 (17%)] Loss: -76510.046875\n",
      "Train Epoch: 112 [4352/17352 (25%)] Loss: -95160.898438\n",
      "Train Epoch: 112 [5760/17352 (33%)] Loss: -112346.296875\n",
      "Train Epoch: 112 [7168/17352 (41%)] Loss: -101771.031250\n",
      "Train Epoch: 112 [8576/17352 (49%)] Loss: -122932.093750\n",
      "Train Epoch: 112 [9984/17352 (58%)] Loss: -97895.515625\n",
      "Train Epoch: 112 [11392/17352 (66%)] Loss: -99345.265625\n",
      "Train Epoch: 112 [12800/17352 (74%)] Loss: -122893.406250\n",
      "Train Epoch: 112 [14208/17352 (82%)] Loss: -90183.125000\n",
      "Train Epoch: 112 [15535/17352 (90%)] Loss: -73087.375000\n",
      "Train Epoch: 112 [16253/17352 (94%)] Loss: -43159.222656\n",
      "Train Epoch: 112 [16880/17352 (97%)] Loss: -28087.695312\n",
      "    epoch          : 112\n",
      "    loss           : -87818.07816186687\n",
      "    val_loss       : -50058.315755208336\n",
      "Train Epoch: 113 [128/17352 (1%)] Loss: -102211.562500\n",
      "Train Epoch: 113 [1536/17352 (9%)] Loss: -107207.445312\n",
      "Train Epoch: 113 [2944/17352 (17%)] Loss: -122723.531250\n",
      "Train Epoch: 113 [4352/17352 (25%)] Loss: -70062.640625\n",
      "Train Epoch: 113 [5760/17352 (33%)] Loss: -88681.640625\n",
      "Train Epoch: 113 [7168/17352 (41%)] Loss: -81626.484375\n",
      "Train Epoch: 113 [8576/17352 (49%)] Loss: -117067.890625\n",
      "Train Epoch: 113 [9984/17352 (58%)] Loss: -121687.390625\n",
      "Train Epoch: 113 [11392/17352 (66%)] Loss: -130065.109375\n",
      "Train Epoch: 113 [12800/17352 (74%)] Loss: -56911.000000\n",
      "Train Epoch: 113 [14208/17352 (82%)] Loss: -65288.062500\n",
      "Train Epoch: 113 [15533/17352 (90%)] Loss: -84549.781250\n",
      "Train Epoch: 113 [16199/17352 (93%)] Loss: -65093.968750\n",
      "Train Epoch: 113 [16981/17352 (98%)] Loss: -94765.468750\n",
      "    epoch          : 113\n",
      "    loss           : -91726.99367856019\n",
      "    val_loss       : -45184.163338216145\n",
      "Train Epoch: 114 [128/17352 (1%)] Loss: -105043.484375\n",
      "Train Epoch: 114 [1536/17352 (9%)] Loss: -104548.710938\n",
      "Train Epoch: 114 [2944/17352 (17%)] Loss: -65889.078125\n",
      "Train Epoch: 114 [4352/17352 (25%)] Loss: -107178.617188\n",
      "Train Epoch: 114 [5760/17352 (33%)] Loss: -121423.242188\n",
      "Train Epoch: 114 [7168/17352 (41%)] Loss: -65244.800781\n",
      "Train Epoch: 114 [8576/17352 (49%)] Loss: -121851.757812\n",
      "Train Epoch: 114 [9984/17352 (58%)] Loss: -69647.914062\n",
      "Train Epoch: 114 [11392/17352 (66%)] Loss: -91200.757812\n",
      "Train Epoch: 114 [12800/17352 (74%)] Loss: -93929.781250\n",
      "Train Epoch: 114 [14208/17352 (82%)] Loss: -83453.726562\n",
      "Train Epoch: 114 [15520/17352 (89%)] Loss: -53764.566406\n",
      "Train Epoch: 114 [16322/17352 (94%)] Loss: -57355.039062\n",
      "Train Epoch: 114 [17008/17352 (98%)] Loss: -41847.152344\n",
      "    epoch          : 114\n",
      "    loss           : -90976.79775816642\n",
      "    val_loss       : -46714.493168131514\n",
      "Train Epoch: 115 [128/17352 (1%)] Loss: -101751.031250\n",
      "Train Epoch: 115 [1536/17352 (9%)] Loss: -94080.187500\n",
      "Train Epoch: 115 [2944/17352 (17%)] Loss: -109484.843750\n",
      "Train Epoch: 115 [4352/17352 (25%)] Loss: -94617.914062\n",
      "Train Epoch: 115 [5760/17352 (33%)] Loss: -112820.718750\n",
      "Train Epoch: 115 [7168/17352 (41%)] Loss: -110749.734375\n",
      "Train Epoch: 115 [8576/17352 (49%)] Loss: -130392.703125\n",
      "Train Epoch: 115 [9984/17352 (58%)] Loss: -96902.578125\n",
      "Train Epoch: 115 [11392/17352 (66%)] Loss: -103206.687500\n",
      "Train Epoch: 115 [12800/17352 (74%)] Loss: -99450.132812\n",
      "Train Epoch: 115 [14208/17352 (82%)] Loss: -101192.359375\n",
      "Train Epoch: 115 [15481/17352 (89%)] Loss: -41578.390625\n",
      "Train Epoch: 115 [16150/17352 (93%)] Loss: -107187.570312\n",
      "Train Epoch: 115 [17056/17352 (98%)] Loss: -65440.492188\n",
      "    epoch          : 115\n",
      "    loss           : -96860.93605481858\n",
      "    val_loss       : -53166.20079345703\n",
      "Train Epoch: 116 [128/17352 (1%)] Loss: -107214.687500\n",
      "Train Epoch: 116 [1536/17352 (9%)] Loss: -88954.304688\n",
      "Train Epoch: 116 [2944/17352 (17%)] Loss: -131895.015625\n",
      "Train Epoch: 116 [4352/17352 (25%)] Loss: -73953.132812\n",
      "Train Epoch: 116 [5760/17352 (33%)] Loss: -137694.375000\n",
      "Train Epoch: 116 [7168/17352 (41%)] Loss: -115071.906250\n",
      "Train Epoch: 116 [8576/17352 (49%)] Loss: -76916.476562\n",
      "Train Epoch: 116 [9984/17352 (58%)] Loss: -82040.023438\n",
      "Train Epoch: 116 [11392/17352 (66%)] Loss: -76066.281250\n",
      "Train Epoch: 116 [12800/17352 (74%)] Loss: -134588.578125\n",
      "Train Epoch: 116 [14208/17352 (82%)] Loss: -135586.843750\n",
      "Train Epoch: 116 [15417/17352 (89%)] Loss: -44453.171875\n",
      "Train Epoch: 116 [16181/17352 (93%)] Loss: -45249.464844\n",
      "Train Epoch: 116 [16960/17352 (98%)] Loss: -49379.757812\n",
      "    epoch          : 116\n",
      "    loss           : -93214.47315272389\n",
      "    val_loss       : -50782.65194905599\n",
      "Train Epoch: 117 [128/17352 (1%)] Loss: -106825.273438\n",
      "Train Epoch: 117 [1536/17352 (9%)] Loss: -94358.500000\n",
      "Train Epoch: 117 [2944/17352 (17%)] Loss: -108240.625000\n",
      "Train Epoch: 117 [4352/17352 (25%)] Loss: -149256.000000\n",
      "Train Epoch: 117 [5760/17352 (33%)] Loss: -86064.750000\n",
      "Train Epoch: 117 [7168/17352 (41%)] Loss: -117575.171875\n",
      "Train Epoch: 117 [8576/17352 (49%)] Loss: -101886.187500\n",
      "Train Epoch: 117 [9984/17352 (58%)] Loss: -51850.406250\n",
      "Train Epoch: 117 [11392/17352 (66%)] Loss: -96334.515625\n",
      "Train Epoch: 117 [12800/17352 (74%)] Loss: -112019.015625\n",
      "Train Epoch: 117 [14208/17352 (82%)] Loss: -125661.523438\n",
      "Train Epoch: 117 [15496/17352 (89%)] Loss: -65539.492188\n",
      "Train Epoch: 117 [16412/17352 (95%)] Loss: -30919.140625\n",
      "Train Epoch: 117 [17015/17352 (98%)] Loss: -68964.234375\n",
      "    epoch          : 117\n",
      "    loss           : -94477.85344025273\n",
      "    val_loss       : -53645.73261311849\n",
      "Train Epoch: 118 [128/17352 (1%)] Loss: -88220.671875\n",
      "Train Epoch: 118 [1536/17352 (9%)] Loss: -94172.453125\n",
      "Train Epoch: 118 [2944/17352 (17%)] Loss: -88347.265625\n",
      "Train Epoch: 118 [4352/17352 (25%)] Loss: -81829.390625\n",
      "Train Epoch: 118 [5760/17352 (33%)] Loss: -97306.609375\n",
      "Train Epoch: 118 [7168/17352 (41%)] Loss: -116191.406250\n",
      "Train Epoch: 118 [8576/17352 (49%)] Loss: -103768.304688\n",
      "Train Epoch: 118 [9984/17352 (58%)] Loss: -119682.546875\n",
      "Train Epoch: 118 [11392/17352 (66%)] Loss: -144236.796875\n",
      "Train Epoch: 118 [12800/17352 (74%)] Loss: -96804.187500\n",
      "Train Epoch: 118 [14208/17352 (82%)] Loss: -81906.312500\n",
      "Train Epoch: 118 [15561/17352 (90%)] Loss: -73785.828125\n",
      "Train Epoch: 118 [16341/17352 (94%)] Loss: -12619.734375\n",
      "Train Epoch: 118 [17130/17352 (99%)] Loss: -47160.484375\n",
      "    epoch          : 118\n",
      "    loss           : -90701.78573956585\n",
      "    val_loss       : -42871.747338867186\n",
      "Train Epoch: 119 [128/17352 (1%)] Loss: -84219.398438\n",
      "Train Epoch: 119 [1536/17352 (9%)] Loss: -77113.210938\n",
      "Train Epoch: 119 [2944/17352 (17%)] Loss: -87907.875000\n",
      "Train Epoch: 119 [4352/17352 (25%)] Loss: -89191.429688\n",
      "Train Epoch: 119 [5760/17352 (33%)] Loss: -104669.765625\n",
      "Train Epoch: 119 [7168/17352 (41%)] Loss: -120072.859375\n",
      "Train Epoch: 119 [8576/17352 (49%)] Loss: -108243.718750\n",
      "Train Epoch: 119 [9984/17352 (58%)] Loss: -101847.671875\n",
      "Train Epoch: 119 [11392/17352 (66%)] Loss: -127962.453125\n",
      "Train Epoch: 119 [12800/17352 (74%)] Loss: -113345.234375\n",
      "Train Epoch: 119 [14208/17352 (82%)] Loss: -109265.281250\n",
      "Train Epoch: 119 [15573/17352 (90%)] Loss: -58992.949219\n",
      "Train Epoch: 119 [16293/17352 (94%)] Loss: -77515.421875\n",
      "Train Epoch: 119 [17011/17352 (98%)] Loss: -4038.626709\n",
      "    epoch          : 119\n",
      "    loss           : -91919.31597204016\n",
      "    val_loss       : -35982.235677083336\n",
      "Train Epoch: 120 [128/17352 (1%)] Loss: -85578.718750\n",
      "Train Epoch: 120 [1536/17352 (9%)] Loss: -69270.617188\n",
      "Train Epoch: 120 [2944/17352 (17%)] Loss: -42842.132812\n",
      "Train Epoch: 120 [4352/17352 (25%)] Loss: -110369.812500\n",
      "Train Epoch: 120 [5760/17352 (33%)] Loss: -79444.281250\n",
      "Train Epoch: 120 [7168/17352 (41%)] Loss: -72031.609375\n",
      "Train Epoch: 120 [8576/17352 (49%)] Loss: -131246.781250\n",
      "Train Epoch: 120 [9984/17352 (58%)] Loss: -89928.546875\n",
      "Train Epoch: 120 [11392/17352 (66%)] Loss: -119412.890625\n",
      "Train Epoch: 120 [12800/17352 (74%)] Loss: -142564.843750\n",
      "Train Epoch: 120 [14208/17352 (82%)] Loss: -113122.242188\n",
      "Train Epoch: 120 [15470/17352 (89%)] Loss: -10761.049805\n",
      "Train Epoch: 120 [16163/17352 (93%)] Loss: -35505.171875\n",
      "Train Epoch: 120 [16892/17352 (97%)] Loss: -101148.328125\n",
      "    epoch          : 120\n",
      "    loss           : -91076.00150089136\n",
      "    val_loss       : -51444.45433349609\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch120.pth ...\n",
      "Train Epoch: 121 [128/17352 (1%)] Loss: -49318.214844\n",
      "Train Epoch: 121 [1536/17352 (9%)] Loss: -125347.250000\n",
      "Train Epoch: 121 [2944/17352 (17%)] Loss: -79141.734375\n",
      "Train Epoch: 121 [4352/17352 (25%)] Loss: -68856.664062\n",
      "Train Epoch: 121 [5760/17352 (33%)] Loss: -103565.117188\n",
      "Train Epoch: 121 [7168/17352 (41%)] Loss: -110822.757812\n",
      "Train Epoch: 121 [8576/17352 (49%)] Loss: -108153.000000\n",
      "Train Epoch: 121 [9984/17352 (58%)] Loss: -92362.976562\n",
      "Train Epoch: 121 [11392/17352 (66%)] Loss: -90199.617188\n",
      "Train Epoch: 121 [12800/17352 (74%)] Loss: -110855.859375\n",
      "Train Epoch: 121 [14208/17352 (82%)] Loss: -132827.750000\n",
      "Train Epoch: 121 [15576/17352 (90%)] Loss: -86643.375000\n",
      "Train Epoch: 121 [16407/17352 (95%)] Loss: -32772.175781\n",
      "Train Epoch: 121 [17162/17352 (99%)] Loss: -88532.375000\n",
      "    epoch          : 121\n",
      "    loss           : -96030.96718356754\n",
      "    val_loss       : -52645.65701497396\n",
      "Train Epoch: 122 [128/17352 (1%)] Loss: -136970.093750\n",
      "Train Epoch: 122 [1536/17352 (9%)] Loss: -103599.250000\n",
      "Train Epoch: 122 [2944/17352 (17%)] Loss: -103878.781250\n",
      "Train Epoch: 122 [4352/17352 (25%)] Loss: -134123.828125\n",
      "Train Epoch: 122 [5760/17352 (33%)] Loss: -84780.703125\n",
      "Train Epoch: 122 [7168/17352 (41%)] Loss: -89619.656250\n",
      "Train Epoch: 122 [8576/17352 (49%)] Loss: -97945.593750\n",
      "Train Epoch: 122 [9984/17352 (58%)] Loss: -80693.007812\n",
      "Train Epoch: 122 [11392/17352 (66%)] Loss: -87230.007812\n",
      "Train Epoch: 122 [12800/17352 (74%)] Loss: -136806.375000\n",
      "Train Epoch: 122 [14208/17352 (82%)] Loss: -91234.500000\n",
      "Train Epoch: 122 [15489/17352 (89%)] Loss: -31197.232422\n",
      "Train Epoch: 122 [16367/17352 (94%)] Loss: -55276.097656\n",
      "Train Epoch: 122 [16985/17352 (98%)] Loss: -8648.647461\n",
      "    epoch          : 122\n",
      "    loss           : -97465.11270776531\n",
      "    val_loss       : -52153.2857421875\n",
      "Train Epoch: 123 [128/17352 (1%)] Loss: -122141.140625\n",
      "Train Epoch: 123 [1536/17352 (9%)] Loss: -100157.187500\n",
      "Train Epoch: 123 [2944/17352 (17%)] Loss: -120380.984375\n",
      "Train Epoch: 123 [4352/17352 (25%)] Loss: -112964.312500\n",
      "Train Epoch: 123 [5760/17352 (33%)] Loss: -147178.515625\n",
      "Train Epoch: 123 [7168/17352 (41%)] Loss: -141519.953125\n",
      "Train Epoch: 123 [8576/17352 (49%)] Loss: -103208.078125\n",
      "Train Epoch: 123 [9984/17352 (58%)] Loss: -143987.937500\n",
      "Train Epoch: 123 [11392/17352 (66%)] Loss: -112618.351562\n",
      "Train Epoch: 123 [12800/17352 (74%)] Loss: -124802.359375\n",
      "Train Epoch: 123 [14208/17352 (82%)] Loss: -78262.000000\n",
      "Train Epoch: 123 [15489/17352 (89%)] Loss: -32766.121094\n",
      "Train Epoch: 123 [16131/17352 (93%)] Loss: -87692.304688\n",
      "Train Epoch: 123 [16969/17352 (98%)] Loss: -81502.671875\n",
      "    epoch          : 123\n",
      "    loss           : -102795.53787292891\n",
      "    val_loss       : -55806.68649902344\n",
      "Train Epoch: 124 [128/17352 (1%)] Loss: -151219.578125\n",
      "Train Epoch: 124 [1536/17352 (9%)] Loss: -118700.421875\n",
      "Train Epoch: 124 [2944/17352 (17%)] Loss: -103050.906250\n",
      "Train Epoch: 124 [4352/17352 (25%)] Loss: -105088.507812\n",
      "Train Epoch: 124 [5760/17352 (33%)] Loss: -86829.328125\n",
      "Train Epoch: 124 [7168/17352 (41%)] Loss: -110980.164062\n",
      "Train Epoch: 124 [8576/17352 (49%)] Loss: -91082.046875\n",
      "Train Epoch: 124 [9984/17352 (58%)] Loss: -140994.656250\n",
      "Train Epoch: 124 [11392/17352 (66%)] Loss: -93984.546875\n",
      "Train Epoch: 124 [12800/17352 (74%)] Loss: -125943.968750\n",
      "Train Epoch: 124 [14208/17352 (82%)] Loss: -144976.593750\n",
      "Train Epoch: 124 [15451/17352 (89%)] Loss: -28038.554688\n",
      "Train Epoch: 124 [16309/17352 (94%)] Loss: -125388.937500\n",
      "Train Epoch: 124 [16985/17352 (98%)] Loss: -31824.394531\n",
      "    epoch          : 124\n",
      "    loss           : -101348.03104026846\n",
      "    val_loss       : -52990.35664469401\n",
      "Train Epoch: 125 [128/17352 (1%)] Loss: -70549.312500\n",
      "Train Epoch: 125 [1536/17352 (9%)] Loss: -125024.156250\n",
      "Train Epoch: 125 [2944/17352 (17%)] Loss: -113157.562500\n",
      "Train Epoch: 125 [4352/17352 (25%)] Loss: -70978.960938\n",
      "Train Epoch: 125 [5760/17352 (33%)] Loss: -91716.187500\n",
      "Train Epoch: 125 [7168/17352 (41%)] Loss: -60205.847656\n",
      "Train Epoch: 125 [8576/17352 (49%)] Loss: -107969.031250\n",
      "Train Epoch: 125 [9984/17352 (58%)] Loss: -88052.359375\n",
      "Train Epoch: 125 [11392/17352 (66%)] Loss: -102838.390625\n",
      "Train Epoch: 125 [12800/17352 (74%)] Loss: -124774.882812\n",
      "Train Epoch: 125 [14208/17352 (82%)] Loss: -42087.101562\n",
      "Train Epoch: 125 [15458/17352 (89%)] Loss: -49559.308594\n",
      "Train Epoch: 125 [16222/17352 (93%)] Loss: -98055.304688\n",
      "Train Epoch: 125 [16951/17352 (98%)] Loss: -50839.890625\n",
      "    epoch          : 125\n",
      "    loss           : -97439.35676154835\n",
      "    val_loss       : -55546.54575602213\n",
      "Train Epoch: 126 [128/17352 (1%)] Loss: -118094.484375\n",
      "Train Epoch: 126 [1536/17352 (9%)] Loss: -139764.609375\n",
      "Train Epoch: 126 [2944/17352 (17%)] Loss: -116159.718750\n",
      "Train Epoch: 126 [4352/17352 (25%)] Loss: -134864.187500\n",
      "Train Epoch: 126 [5760/17352 (33%)] Loss: -114123.578125\n",
      "Train Epoch: 126 [7168/17352 (41%)] Loss: -123680.390625\n",
      "Train Epoch: 126 [8576/17352 (49%)] Loss: -125628.781250\n",
      "Train Epoch: 126 [9984/17352 (58%)] Loss: -118095.945312\n",
      "Train Epoch: 126 [11392/17352 (66%)] Loss: -100565.671875\n",
      "Train Epoch: 126 [12800/17352 (74%)] Loss: -115823.015625\n",
      "Train Epoch: 126 [14208/17352 (82%)] Loss: -73920.664062\n",
      "Train Epoch: 126 [15523/17352 (89%)] Loss: -61141.070312\n",
      "Train Epoch: 126 [16278/17352 (94%)] Loss: -76292.726562\n",
      "Train Epoch: 126 [16894/17352 (97%)] Loss: -87889.257812\n",
      "    epoch          : 126\n",
      "    loss           : -99180.39128368814\n",
      "    val_loss       : -52640.07629394531\n",
      "Train Epoch: 127 [128/17352 (1%)] Loss: -137049.125000\n",
      "Train Epoch: 127 [1536/17352 (9%)] Loss: -150286.328125\n",
      "Train Epoch: 127 [2944/17352 (17%)] Loss: -122914.578125\n",
      "Train Epoch: 127 [4352/17352 (25%)] Loss: -91951.750000\n",
      "Train Epoch: 127 [5760/17352 (33%)] Loss: -118364.109375\n",
      "Train Epoch: 127 [7168/17352 (41%)] Loss: -101186.000000\n",
      "Train Epoch: 127 [8576/17352 (49%)] Loss: -118898.078125\n",
      "Train Epoch: 127 [9984/17352 (58%)] Loss: -86023.500000\n",
      "Train Epoch: 127 [11392/17352 (66%)] Loss: -108958.976562\n",
      "Train Epoch: 127 [12800/17352 (74%)] Loss: -98545.453125\n",
      "Train Epoch: 127 [14208/17352 (82%)] Loss: -133745.671875\n",
      "Train Epoch: 127 [15544/17352 (90%)] Loss: -48765.097656\n",
      "Train Epoch: 127 [16126/17352 (93%)] Loss: -37426.457031\n",
      "Train Epoch: 127 [17052/17352 (98%)] Loss: -47007.871094\n",
      "    epoch          : 127\n",
      "    loss           : -97142.14554058305\n",
      "    val_loss       : -48565.795454915366\n",
      "Train Epoch: 128 [128/17352 (1%)] Loss: -91505.750000\n",
      "Train Epoch: 128 [1536/17352 (9%)] Loss: -107424.726562\n",
      "Train Epoch: 128 [2944/17352 (17%)] Loss: -97589.703125\n",
      "Train Epoch: 128 [4352/17352 (25%)] Loss: -137190.421875\n",
      "Train Epoch: 128 [5760/17352 (33%)] Loss: -125706.906250\n",
      "Train Epoch: 128 [7168/17352 (41%)] Loss: -86755.781250\n",
      "Train Epoch: 128 [8576/17352 (49%)] Loss: -126296.984375\n",
      "Train Epoch: 128 [9984/17352 (58%)] Loss: -87768.421875\n",
      "Train Epoch: 128 [11392/17352 (66%)] Loss: -157925.890625\n",
      "Train Epoch: 128 [12800/17352 (74%)] Loss: -96505.929688\n",
      "Train Epoch: 128 [14208/17352 (82%)] Loss: -66793.140625\n",
      "Train Epoch: 128 [15480/17352 (89%)] Loss: -83326.312500\n",
      "Train Epoch: 128 [16069/17352 (93%)] Loss: -65565.906250\n",
      "Train Epoch: 128 [16935/17352 (98%)] Loss: -84212.460938\n",
      "    epoch          : 128\n",
      "    loss           : -96149.70713283871\n",
      "    val_loss       : -53775.93201904297\n",
      "Train Epoch: 129 [128/17352 (1%)] Loss: -113769.382812\n",
      "Train Epoch: 129 [1536/17352 (9%)] Loss: -100774.609375\n",
      "Train Epoch: 129 [2944/17352 (17%)] Loss: -123026.390625\n",
      "Train Epoch: 129 [4352/17352 (25%)] Loss: -108028.031250\n",
      "Train Epoch: 129 [5760/17352 (33%)] Loss: -123731.812500\n",
      "Train Epoch: 129 [7168/17352 (41%)] Loss: -130878.070312\n",
      "Train Epoch: 129 [8576/17352 (49%)] Loss: -100762.625000\n",
      "Train Epoch: 129 [9984/17352 (58%)] Loss: -133728.562500\n",
      "Train Epoch: 129 [11392/17352 (66%)] Loss: -144734.703125\n",
      "Train Epoch: 129 [12800/17352 (74%)] Loss: -119069.093750\n",
      "Train Epoch: 129 [14208/17352 (82%)] Loss: -119912.351562\n",
      "Train Epoch: 129 [15464/17352 (89%)] Loss: -57585.148438\n",
      "Train Epoch: 129 [16359/17352 (94%)] Loss: -3520.982178\n",
      "Train Epoch: 129 [16988/17352 (98%)] Loss: -119667.796875\n",
      "    epoch          : 129\n",
      "    loss           : -100959.80966862416\n",
      "    val_loss       : -56443.26523844401\n",
      "Train Epoch: 130 [128/17352 (1%)] Loss: -117921.921875\n",
      "Train Epoch: 130 [1536/17352 (9%)] Loss: -115576.570312\n",
      "Train Epoch: 130 [2944/17352 (17%)] Loss: -106350.757812\n",
      "Train Epoch: 130 [4352/17352 (25%)] Loss: -97182.710938\n",
      "Train Epoch: 130 [5760/17352 (33%)] Loss: -104312.164062\n",
      "Train Epoch: 130 [7168/17352 (41%)] Loss: -77576.734375\n",
      "Train Epoch: 130 [8576/17352 (49%)] Loss: -84789.429688\n",
      "Train Epoch: 130 [9984/17352 (58%)] Loss: -107483.515625\n",
      "Train Epoch: 130 [11392/17352 (66%)] Loss: -81186.921875\n",
      "Train Epoch: 130 [12800/17352 (74%)] Loss: -98270.585938\n",
      "Train Epoch: 130 [14208/17352 (82%)] Loss: -87625.929688\n",
      "Train Epoch: 130 [15455/17352 (89%)] Loss: -33245.457031\n",
      "Train Epoch: 130 [16259/17352 (94%)] Loss: -71028.843750\n",
      "Train Epoch: 130 [17051/17352 (98%)] Loss: -34427.796875\n",
      "    epoch          : 130\n",
      "    loss           : -82649.4895363622\n",
      "    val_loss       : -53871.52506917318\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch130.pth ...\n",
      "Train Epoch: 131 [128/17352 (1%)] Loss: -57770.753906\n",
      "Train Epoch: 131 [1536/17352 (9%)] Loss: -125526.625000\n",
      "Train Epoch: 131 [2944/17352 (17%)] Loss: -118942.140625\n",
      "Train Epoch: 131 [4352/17352 (25%)] Loss: -115303.546875\n",
      "Train Epoch: 131 [5760/17352 (33%)] Loss: -94323.343750\n",
      "Train Epoch: 131 [7168/17352 (41%)] Loss: -127334.296875\n",
      "Train Epoch: 131 [8576/17352 (49%)] Loss: -112606.406250\n",
      "Train Epoch: 131 [9984/17352 (58%)] Loss: -121766.953125\n",
      "Train Epoch: 131 [11392/17352 (66%)] Loss: -117177.554688\n",
      "Train Epoch: 131 [12800/17352 (74%)] Loss: -79257.296875\n",
      "Train Epoch: 131 [14208/17352 (82%)] Loss: -62294.363281\n",
      "Train Epoch: 131 [15531/17352 (90%)] Loss: -74232.632812\n",
      "Train Epoch: 131 [16244/17352 (94%)] Loss: -11588.037109\n",
      "Train Epoch: 131 [17086/17352 (98%)] Loss: -53780.667969\n",
      "    epoch          : 131\n",
      "    loss           : -99814.08554425335\n",
      "    val_loss       : -52456.06688639323\n",
      "Train Epoch: 132 [128/17352 (1%)] Loss: -101630.140625\n",
      "Train Epoch: 132 [1536/17352 (9%)] Loss: -121025.539062\n",
      "Train Epoch: 132 [2944/17352 (17%)] Loss: -117621.984375\n",
      "Train Epoch: 132 [4352/17352 (25%)] Loss: -119260.562500\n",
      "Train Epoch: 132 [5760/17352 (33%)] Loss: -121765.484375\n",
      "Train Epoch: 132 [7168/17352 (41%)] Loss: -136037.203125\n",
      "Train Epoch: 132 [8576/17352 (49%)] Loss: -62740.296875\n",
      "Train Epoch: 132 [9984/17352 (58%)] Loss: -119794.968750\n",
      "Train Epoch: 132 [11392/17352 (66%)] Loss: -102422.234375\n",
      "Train Epoch: 132 [12800/17352 (74%)] Loss: -138017.796875\n",
      "Train Epoch: 132 [14208/17352 (82%)] Loss: -141221.031250\n",
      "Train Epoch: 132 [15539/17352 (90%)] Loss: -50271.500000\n",
      "Train Epoch: 132 [16236/17352 (94%)] Loss: -101285.156250\n",
      "Train Epoch: 132 [17043/17352 (98%)] Loss: -36195.054688\n",
      "    epoch          : 132\n",
      "    loss           : -96721.02457381896\n",
      "    val_loss       : -30179.104036458335\n",
      "Train Epoch: 133 [128/17352 (1%)] Loss: -52571.425781\n",
      "Train Epoch: 133 [1536/17352 (9%)] Loss: -60026.953125\n",
      "Train Epoch: 133 [2944/17352 (17%)] Loss: -97558.406250\n",
      "Train Epoch: 133 [4352/17352 (25%)] Loss: -97199.875000\n",
      "Train Epoch: 133 [5760/17352 (33%)] Loss: -107860.843750\n",
      "Train Epoch: 133 [7168/17352 (41%)] Loss: -119931.109375\n",
      "Train Epoch: 133 [8576/17352 (49%)] Loss: -83113.000000\n",
      "Train Epoch: 133 [9984/17352 (58%)] Loss: -126359.000000\n",
      "Train Epoch: 133 [11392/17352 (66%)] Loss: -102018.093750\n",
      "Train Epoch: 133 [12800/17352 (74%)] Loss: -103032.695312\n",
      "Train Epoch: 133 [14208/17352 (82%)] Loss: -132509.562500\n",
      "Train Epoch: 133 [15457/17352 (89%)] Loss: -29588.867188\n",
      "Train Epoch: 133 [16189/17352 (93%)] Loss: -50760.886719\n",
      "Train Epoch: 133 [16959/17352 (98%)] Loss: -2108.205566\n",
      "    epoch          : 133\n",
      "    loss           : -87795.89289927643\n",
      "    val_loss       : -50775.55605061849\n",
      "Train Epoch: 134 [128/17352 (1%)] Loss: -150624.500000\n",
      "Train Epoch: 134 [1536/17352 (9%)] Loss: -114239.421875\n",
      "Train Epoch: 134 [2944/17352 (17%)] Loss: -68545.171875\n",
      "Train Epoch: 134 [4352/17352 (25%)] Loss: -140131.437500\n",
      "Train Epoch: 134 [5760/17352 (33%)] Loss: -129054.500000\n",
      "Train Epoch: 134 [7168/17352 (41%)] Loss: -114090.531250\n",
      "Train Epoch: 134 [8576/17352 (49%)] Loss: -117658.046875\n",
      "Train Epoch: 134 [9984/17352 (58%)] Loss: -113370.187500\n",
      "Train Epoch: 134 [11392/17352 (66%)] Loss: -90167.414062\n",
      "Train Epoch: 134 [12800/17352 (74%)] Loss: -114422.328125\n",
      "Train Epoch: 134 [14208/17352 (82%)] Loss: -98791.984375\n",
      "Train Epoch: 134 [15543/17352 (90%)] Loss: -65792.367188\n",
      "Train Epoch: 134 [16359/17352 (94%)] Loss: -10467.062500\n",
      "Train Epoch: 134 [16966/17352 (98%)] Loss: -18161.535156\n",
      "    epoch          : 134\n",
      "    loss           : -99975.40389215866\n",
      "    val_loss       : -44310.68268636068\n",
      "Train Epoch: 135 [128/17352 (1%)] Loss: -107234.828125\n",
      "Train Epoch: 135 [1536/17352 (9%)] Loss: -61640.593750\n",
      "Train Epoch: 135 [2944/17352 (17%)] Loss: -113770.296875\n",
      "Train Epoch: 135 [4352/17352 (25%)] Loss: -135548.218750\n",
      "Train Epoch: 135 [5760/17352 (33%)] Loss: -98447.859375\n",
      "Train Epoch: 135 [7168/17352 (41%)] Loss: -121093.218750\n",
      "Train Epoch: 135 [8576/17352 (49%)] Loss: -77233.812500\n",
      "Train Epoch: 135 [9984/17352 (58%)] Loss: -129038.453125\n",
      "Train Epoch: 135 [11392/17352 (66%)] Loss: -80001.812500\n",
      "Train Epoch: 135 [12800/17352 (74%)] Loss: -108651.250000\n",
      "Train Epoch: 135 [14208/17352 (82%)] Loss: -121773.265625\n",
      "Train Epoch: 135 [15468/17352 (89%)] Loss: -3491.999756\n",
      "Train Epoch: 135 [16170/17352 (93%)] Loss: -27842.986328\n",
      "Train Epoch: 135 [16908/17352 (97%)] Loss: -47761.437500\n",
      "    epoch          : 135\n",
      "    loss           : -97549.31910162805\n",
      "    val_loss       : -58065.68799641927\n",
      "Train Epoch: 136 [128/17352 (1%)] Loss: -156322.531250\n",
      "Train Epoch: 136 [1536/17352 (9%)] Loss: -103740.062500\n",
      "Train Epoch: 136 [2944/17352 (17%)] Loss: -114974.914062\n",
      "Train Epoch: 136 [4352/17352 (25%)] Loss: -58129.492188\n",
      "Train Epoch: 136 [5760/17352 (33%)] Loss: -111375.640625\n",
      "Train Epoch: 136 [7168/17352 (41%)] Loss: -72779.625000\n",
      "Train Epoch: 136 [8576/17352 (49%)] Loss: -115225.062500\n",
      "Train Epoch: 136 [9984/17352 (58%)] Loss: -129989.718750\n",
      "Train Epoch: 136 [11392/17352 (66%)] Loss: -86515.171875\n",
      "Train Epoch: 136 [12800/17352 (74%)] Loss: -106286.210938\n",
      "Train Epoch: 136 [14208/17352 (82%)] Loss: -100512.531250\n",
      "Train Epoch: 136 [15533/17352 (90%)] Loss: -69739.382812\n",
      "Train Epoch: 136 [16254/17352 (94%)] Loss: -58244.859375\n",
      "Train Epoch: 136 [17023/17352 (98%)] Loss: -5315.277344\n",
      "    epoch          : 136\n",
      "    loss           : -101883.20754247064\n",
      "    val_loss       : -56660.328251139326\n",
      "Train Epoch: 137 [128/17352 (1%)] Loss: -114386.171875\n",
      "Train Epoch: 137 [1536/17352 (9%)] Loss: -110442.398438\n",
      "Train Epoch: 137 [2944/17352 (17%)] Loss: -118907.476562\n",
      "Train Epoch: 137 [4352/17352 (25%)] Loss: -101497.171875\n",
      "Train Epoch: 137 [5760/17352 (33%)] Loss: -102741.796875\n",
      "Train Epoch: 137 [7168/17352 (41%)] Loss: -79947.476562\n",
      "Train Epoch: 137 [8576/17352 (49%)] Loss: -155311.218750\n",
      "Train Epoch: 137 [9984/17352 (58%)] Loss: -119650.507812\n",
      "Train Epoch: 137 [11392/17352 (66%)] Loss: -133227.859375\n",
      "Train Epoch: 137 [12800/17352 (74%)] Loss: -106532.171875\n",
      "Train Epoch: 137 [14208/17352 (82%)] Loss: -96193.781250\n",
      "Train Epoch: 137 [15457/17352 (89%)] Loss: -27396.429688\n",
      "Train Epoch: 137 [16252/17352 (94%)] Loss: -56727.195312\n",
      "Train Epoch: 137 [17047/17352 (98%)] Loss: -26996.371094\n",
      "    epoch          : 137\n",
      "    loss           : -100875.39121323144\n",
      "    val_loss       : -57099.084956868486\n",
      "Train Epoch: 138 [128/17352 (1%)] Loss: -155802.890625\n",
      "Train Epoch: 138 [1536/17352 (9%)] Loss: -132279.765625\n",
      "Train Epoch: 138 [2944/17352 (17%)] Loss: -105828.929688\n",
      "Train Epoch: 138 [4352/17352 (25%)] Loss: -114988.875000\n",
      "Train Epoch: 138 [5760/17352 (33%)] Loss: -140546.578125\n",
      "Train Epoch: 138 [7168/17352 (41%)] Loss: -94301.960938\n",
      "Train Epoch: 138 [8576/17352 (49%)] Loss: -142522.812500\n",
      "Train Epoch: 138 [9984/17352 (58%)] Loss: -111789.656250\n",
      "Train Epoch: 138 [11392/17352 (66%)] Loss: -113127.781250\n",
      "Train Epoch: 138 [12800/17352 (74%)] Loss: -125964.109375\n",
      "Train Epoch: 138 [14208/17352 (82%)] Loss: -105776.062500\n",
      "Train Epoch: 138 [15504/17352 (89%)] Loss: -70081.796875\n",
      "Train Epoch: 138 [16288/17352 (94%)] Loss: -22969.529297\n",
      "Train Epoch: 138 [17037/17352 (98%)] Loss: -83454.742188\n",
      "    epoch          : 138\n",
      "    loss           : -98844.44741677918\n",
      "    val_loss       : 16062.727030436197\n",
      "Train Epoch: 139 [128/17352 (1%)] Loss: 32776.625000\n",
      "Train Epoch: 139 [1536/17352 (9%)] Loss: -84022.953125\n",
      "Train Epoch: 139 [2944/17352 (17%)] Loss: -63390.710938\n",
      "Train Epoch: 139 [4352/17352 (25%)] Loss: -74639.742188\n",
      "Train Epoch: 139 [5760/17352 (33%)] Loss: -131712.296875\n",
      "Train Epoch: 139 [7168/17352 (41%)] Loss: -93039.320312\n",
      "Train Epoch: 139 [8576/17352 (49%)] Loss: -115430.085938\n",
      "Train Epoch: 139 [9984/17352 (58%)] Loss: -121215.796875\n",
      "Train Epoch: 139 [11392/17352 (66%)] Loss: -117745.500000\n",
      "Train Epoch: 139 [12800/17352 (74%)] Loss: -99959.335938\n",
      "Train Epoch: 139 [14208/17352 (82%)] Loss: -120941.078125\n",
      "Train Epoch: 139 [15559/17352 (90%)] Loss: -45941.562500\n",
      "Train Epoch: 139 [16316/17352 (94%)] Loss: -104734.031250\n",
      "Train Epoch: 139 [17092/17352 (99%)] Loss: -51381.894531\n",
      "    epoch          : 139\n",
      "    loss           : -84364.96142905831\n",
      "    val_loss       : -48054.43057454427\n",
      "Train Epoch: 140 [128/17352 (1%)] Loss: -88551.976562\n",
      "Train Epoch: 140 [1536/17352 (9%)] Loss: -97456.812500\n",
      "Train Epoch: 140 [2944/17352 (17%)] Loss: -74967.179688\n",
      "Train Epoch: 140 [4352/17352 (25%)] Loss: -116270.523438\n",
      "Train Epoch: 140 [5760/17352 (33%)] Loss: -92031.554688\n",
      "Train Epoch: 140 [7168/17352 (41%)] Loss: -103601.421875\n",
      "Train Epoch: 140 [8576/17352 (49%)] Loss: -91519.281250\n",
      "Train Epoch: 140 [9984/17352 (58%)] Loss: -88006.765625\n",
      "Train Epoch: 140 [11392/17352 (66%)] Loss: -100350.882812\n",
      "Train Epoch: 140 [12800/17352 (74%)] Loss: -100108.554688\n",
      "Train Epoch: 140 [14208/17352 (82%)] Loss: -104786.335938\n",
      "Train Epoch: 140 [15547/17352 (90%)] Loss: -102685.398438\n",
      "Train Epoch: 140 [16094/17352 (93%)] Loss: -3568.385742\n",
      "Train Epoch: 140 [17056/17352 (98%)] Loss: -78972.359375\n",
      "    epoch          : 140\n",
      "    loss           : -99475.89277474832\n",
      "    val_loss       : -53479.095703125\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch140.pth ...\n",
      "Train Epoch: 141 [128/17352 (1%)] Loss: -109452.812500\n",
      "Train Epoch: 141 [1536/17352 (9%)] Loss: -118190.187500\n",
      "Train Epoch: 141 [2944/17352 (17%)] Loss: -111869.890625\n",
      "Train Epoch: 141 [4352/17352 (25%)] Loss: -122762.492188\n",
      "Train Epoch: 141 [5760/17352 (33%)] Loss: -119058.117188\n",
      "Train Epoch: 141 [7168/17352 (41%)] Loss: -132574.656250\n",
      "Train Epoch: 141 [8576/17352 (49%)] Loss: -124273.562500\n",
      "Train Epoch: 141 [9984/17352 (58%)] Loss: -136764.281250\n",
      "Train Epoch: 141 [11392/17352 (66%)] Loss: -117825.218750\n",
      "Train Epoch: 141 [12800/17352 (74%)] Loss: -98007.789062\n",
      "Train Epoch: 141 [14208/17352 (82%)] Loss: -117339.070312\n",
      "Train Epoch: 141 [15574/17352 (90%)] Loss: -86879.234375\n",
      "Train Epoch: 141 [16247/17352 (94%)] Loss: -2718.778809\n",
      "Train Epoch: 141 [16934/17352 (98%)] Loss: -4258.550781\n",
      "    epoch          : 141\n",
      "    loss           : -103924.70462916841\n",
      "    val_loss       : -55984.55612792969\n",
      "Train Epoch: 142 [128/17352 (1%)] Loss: -114383.859375\n",
      "Train Epoch: 142 [1536/17352 (9%)] Loss: -95039.062500\n",
      "Train Epoch: 142 [2944/17352 (17%)] Loss: -118038.671875\n",
      "Train Epoch: 142 [4352/17352 (25%)] Loss: -81895.320312\n",
      "Train Epoch: 142 [5760/17352 (33%)] Loss: -111958.679688\n",
      "Train Epoch: 142 [7168/17352 (41%)] Loss: -122666.828125\n",
      "Train Epoch: 142 [8576/17352 (49%)] Loss: -142388.093750\n",
      "Train Epoch: 142 [9984/17352 (58%)] Loss: -143584.468750\n",
      "Train Epoch: 142 [11392/17352 (66%)] Loss: -92602.406250\n",
      "Train Epoch: 142 [12800/17352 (74%)] Loss: -115604.367188\n",
      "Train Epoch: 142 [14208/17352 (82%)] Loss: -102437.421875\n",
      "Train Epoch: 142 [15492/17352 (89%)] Loss: -62984.140625\n",
      "Train Epoch: 142 [16376/17352 (94%)] Loss: -76064.531250\n",
      "Train Epoch: 142 [17094/17352 (99%)] Loss: -94643.937500\n",
      "    epoch          : 142\n",
      "    loss           : -105312.58885981413\n",
      "    val_loss       : -44716.77650146485\n",
      "Train Epoch: 143 [128/17352 (1%)] Loss: -32041.609375\n",
      "Train Epoch: 143 [1536/17352 (9%)] Loss: -142098.546875\n",
      "Train Epoch: 143 [2944/17352 (17%)] Loss: -136106.109375\n",
      "Train Epoch: 143 [4352/17352 (25%)] Loss: -111759.437500\n",
      "Train Epoch: 143 [5760/17352 (33%)] Loss: -92683.156250\n",
      "Train Epoch: 143 [7168/17352 (41%)] Loss: -101093.937500\n",
      "Train Epoch: 143 [8576/17352 (49%)] Loss: -135610.421875\n",
      "Train Epoch: 143 [9984/17352 (58%)] Loss: -116559.859375\n",
      "Train Epoch: 143 [11392/17352 (66%)] Loss: -84447.382812\n",
      "Train Epoch: 143 [12800/17352 (74%)] Loss: -122074.054688\n",
      "Train Epoch: 143 [14208/17352 (82%)] Loss: -128938.046875\n",
      "Train Epoch: 143 [15515/17352 (89%)] Loss: -54196.312500\n",
      "Train Epoch: 143 [16239/17352 (94%)] Loss: -49355.562500\n",
      "Train Epoch: 143 [17013/17352 (98%)] Loss: -50504.523438\n",
      "    epoch          : 143\n",
      "    loss           : -100956.51272316747\n",
      "    val_loss       : -44012.469771321616\n",
      "Train Epoch: 144 [128/17352 (1%)] Loss: -88283.273438\n",
      "Train Epoch: 144 [1536/17352 (9%)] Loss: -86893.687500\n",
      "Train Epoch: 144 [2944/17352 (17%)] Loss: -114101.210938\n",
      "Train Epoch: 144 [4352/17352 (25%)] Loss: -130788.812500\n",
      "Train Epoch: 144 [5760/17352 (33%)] Loss: -109199.156250\n",
      "Train Epoch: 144 [7168/17352 (41%)] Loss: -73899.867188\n",
      "Train Epoch: 144 [8576/17352 (49%)] Loss: -82577.289062\n",
      "Train Epoch: 144 [9984/17352 (58%)] Loss: -114607.226562\n",
      "Train Epoch: 144 [11392/17352 (66%)] Loss: -104130.164062\n",
      "Train Epoch: 144 [12800/17352 (74%)] Loss: -122890.750000\n",
      "Train Epoch: 144 [14208/17352 (82%)] Loss: -128689.890625\n",
      "Train Epoch: 144 [15482/17352 (89%)] Loss: -23942.154297\n",
      "Train Epoch: 144 [16206/17352 (93%)] Loss: -2306.435059\n",
      "Train Epoch: 144 [16944/17352 (98%)] Loss: -34119.375000\n",
      "    epoch          : 144\n",
      "    loss           : -100450.8193949245\n",
      "    val_loss       : -56802.980078125\n",
      "Train Epoch: 145 [128/17352 (1%)] Loss: -120688.195312\n",
      "Train Epoch: 145 [1536/17352 (9%)] Loss: -98571.031250\n",
      "Train Epoch: 145 [2944/17352 (17%)] Loss: -110812.929688\n",
      "Train Epoch: 145 [4352/17352 (25%)] Loss: -112151.242188\n",
      "Train Epoch: 145 [5760/17352 (33%)] Loss: -103216.609375\n",
      "Train Epoch: 145 [7168/17352 (41%)] Loss: -131833.015625\n",
      "Train Epoch: 145 [8576/17352 (49%)] Loss: -86460.054688\n",
      "Train Epoch: 145 [9984/17352 (58%)] Loss: -134955.781250\n",
      "Train Epoch: 145 [11392/17352 (66%)] Loss: -134741.921875\n",
      "Train Epoch: 145 [12800/17352 (74%)] Loss: -156354.265625\n",
      "Train Epoch: 145 [14208/17352 (82%)] Loss: -129691.937500\n",
      "Train Epoch: 145 [15549/17352 (90%)] Loss: -57460.234375\n",
      "Train Epoch: 145 [16189/17352 (93%)] Loss: -41354.570312\n",
      "Train Epoch: 145 [17007/17352 (98%)] Loss: -93872.343750\n",
      "    epoch          : 145\n",
      "    loss           : -105959.48864991873\n",
      "    val_loss       : -53566.75731608073\n",
      "Train Epoch: 146 [128/17352 (1%)] Loss: -93290.773438\n",
      "Train Epoch: 146 [1536/17352 (9%)] Loss: -93936.203125\n",
      "Train Epoch: 146 [2944/17352 (17%)] Loss: -62519.218750\n",
      "Train Epoch: 146 [4352/17352 (25%)] Loss: -115475.843750\n",
      "Train Epoch: 146 [5760/17352 (33%)] Loss: -70807.968750\n",
      "Train Epoch: 146 [7168/17352 (41%)] Loss: -132282.734375\n",
      "Train Epoch: 146 [8576/17352 (49%)] Loss: -57224.347656\n",
      "Train Epoch: 146 [9984/17352 (58%)] Loss: -124115.093750\n",
      "Train Epoch: 146 [11392/17352 (66%)] Loss: -105349.164062\n",
      "Train Epoch: 146 [12800/17352 (74%)] Loss: -109234.718750\n",
      "Train Epoch: 146 [14208/17352 (82%)] Loss: -113460.375000\n",
      "Train Epoch: 146 [15522/17352 (89%)] Loss: -82429.515625\n",
      "Train Epoch: 146 [16299/17352 (94%)] Loss: -74278.437500\n",
      "Train Epoch: 146 [17071/17352 (98%)] Loss: -91995.625000\n",
      "    epoch          : 146\n",
      "    loss           : -99676.51928547084\n",
      "    val_loss       : -55549.59569498698\n",
      "Train Epoch: 147 [128/17352 (1%)] Loss: -100818.445312\n",
      "Train Epoch: 147 [1536/17352 (9%)] Loss: -77952.093750\n",
      "Train Epoch: 147 [2944/17352 (17%)] Loss: -113458.085938\n",
      "Train Epoch: 147 [4352/17352 (25%)] Loss: -102141.140625\n",
      "Train Epoch: 147 [5760/17352 (33%)] Loss: -133098.468750\n",
      "Train Epoch: 147 [7168/17352 (41%)] Loss: -149206.531250\n",
      "Train Epoch: 147 [8576/17352 (49%)] Loss: -98988.023438\n",
      "Train Epoch: 147 [9984/17352 (58%)] Loss: -133803.500000\n",
      "Train Epoch: 147 [11392/17352 (66%)] Loss: -115636.023438\n",
      "Train Epoch: 147 [12800/17352 (74%)] Loss: -131501.109375\n",
      "Train Epoch: 147 [14208/17352 (82%)] Loss: -115940.906250\n",
      "Train Epoch: 147 [15457/17352 (89%)] Loss: -52586.937500\n",
      "Train Epoch: 147 [16221/17352 (93%)] Loss: -101944.304688\n",
      "Train Epoch: 147 [16960/17352 (98%)] Loss: -47346.859375\n",
      "    epoch          : 147\n",
      "    loss           : -105319.33911460519\n",
      "    val_loss       : -55664.32269694011\n",
      "Train Epoch: 148 [128/17352 (1%)] Loss: -81343.687500\n",
      "Train Epoch: 148 [1536/17352 (9%)] Loss: -98316.531250\n",
      "Train Epoch: 148 [2944/17352 (17%)] Loss: -125125.796875\n",
      "Train Epoch: 148 [4352/17352 (25%)] Loss: -112368.359375\n",
      "Train Epoch: 148 [5760/17352 (33%)] Loss: -92293.960938\n",
      "Train Epoch: 148 [7168/17352 (41%)] Loss: -115954.593750\n",
      "Train Epoch: 148 [8576/17352 (49%)] Loss: -108810.062500\n",
      "Train Epoch: 148 [9984/17352 (58%)] Loss: -95371.398438\n",
      "Train Epoch: 148 [11392/17352 (66%)] Loss: -119873.664062\n",
      "Train Epoch: 148 [12800/17352 (74%)] Loss: -134557.156250\n",
      "Train Epoch: 148 [14208/17352 (82%)] Loss: -138127.906250\n",
      "Train Epoch: 148 [15538/17352 (90%)] Loss: -68912.781250\n",
      "Train Epoch: 148 [16198/17352 (93%)] Loss: -46127.671875\n",
      "Train Epoch: 148 [16953/17352 (98%)] Loss: -82252.031250\n",
      "    epoch          : 148\n",
      "    loss           : -106982.98512872274\n",
      "    val_loss       : -58229.74936116536\n",
      "Train Epoch: 149 [128/17352 (1%)] Loss: -146294.390625\n",
      "Train Epoch: 149 [1536/17352 (9%)] Loss: -157839.328125\n",
      "Train Epoch: 149 [2944/17352 (17%)] Loss: -138167.296875\n",
      "Train Epoch: 149 [4352/17352 (25%)] Loss: -18854.757812\n",
      "Train Epoch: 149 [5760/17352 (33%)] Loss: -52946.695312\n",
      "Train Epoch: 149 [7168/17352 (41%)] Loss: -83739.953125\n",
      "Train Epoch: 149 [8576/17352 (49%)] Loss: -88305.328125\n",
      "Train Epoch: 149 [9984/17352 (58%)] Loss: -94379.062500\n",
      "Train Epoch: 149 [11392/17352 (66%)] Loss: -66029.921875\n",
      "Train Epoch: 149 [12800/17352 (74%)] Loss: -88821.507812\n",
      "Train Epoch: 149 [14208/17352 (82%)] Loss: -120712.398438\n",
      "Train Epoch: 149 [15488/17352 (89%)] Loss: -41212.953125\n",
      "Train Epoch: 149 [16193/17352 (93%)] Loss: -32265.035156\n",
      "Train Epoch: 149 [16939/17352 (98%)] Loss: -30637.679688\n",
      "    epoch          : 149\n",
      "    loss           : -79738.47523856963\n",
      "    val_loss       : -52892.39033203125\n",
      "Train Epoch: 150 [128/17352 (1%)] Loss: -100400.273438\n",
      "Train Epoch: 150 [1536/17352 (9%)] Loss: -145771.859375\n",
      "Train Epoch: 150 [2944/17352 (17%)] Loss: -106167.734375\n",
      "Train Epoch: 150 [4352/17352 (25%)] Loss: -101568.289062\n",
      "Train Epoch: 150 [5760/17352 (33%)] Loss: -134736.906250\n",
      "Train Epoch: 150 [7168/17352 (41%)] Loss: -98920.375000\n",
      "Train Epoch: 150 [8576/17352 (49%)] Loss: -67857.867188\n",
      "Train Epoch: 150 [9984/17352 (58%)] Loss: -63268.351562\n",
      "Train Epoch: 150 [11392/17352 (66%)] Loss: -131919.828125\n",
      "Train Epoch: 150 [12800/17352 (74%)] Loss: -118624.570312\n",
      "Train Epoch: 150 [14208/17352 (82%)] Loss: -88837.562500\n",
      "Train Epoch: 150 [15376/17352 (89%)] Loss: -3134.282471\n",
      "Train Epoch: 150 [16173/17352 (93%)] Loss: -82820.265625\n",
      "Train Epoch: 150 [17040/17352 (98%)] Loss: -73884.398438\n",
      "    epoch          : 150\n",
      "    loss           : -99992.91896989041\n",
      "    val_loss       : -53843.26102294922\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [128/17352 (1%)] Loss: -102538.421875\n",
      "Train Epoch: 151 [1536/17352 (9%)] Loss: -88730.609375\n",
      "Train Epoch: 151 [2944/17352 (17%)] Loss: -127645.148438\n",
      "Train Epoch: 151 [4352/17352 (25%)] Loss: -97709.671875\n",
      "Train Epoch: 151 [5760/17352 (33%)] Loss: -121467.671875\n",
      "Train Epoch: 151 [7168/17352 (41%)] Loss: -127882.734375\n",
      "Train Epoch: 151 [8576/17352 (49%)] Loss: -126902.195312\n",
      "Train Epoch: 151 [9984/17352 (58%)] Loss: -119454.898438\n",
      "Train Epoch: 151 [11392/17352 (66%)] Loss: -82463.921875\n",
      "Train Epoch: 151 [12800/17352 (74%)] Loss: -143342.359375\n",
      "Train Epoch: 151 [14208/17352 (82%)] Loss: -124677.265625\n",
      "Train Epoch: 151 [15518/17352 (89%)] Loss: -54097.523438\n",
      "Train Epoch: 151 [16289/17352 (94%)] Loss: -1180.319336\n",
      "Train Epoch: 151 [17059/17352 (98%)] Loss: -47456.664062\n",
      "    epoch          : 151\n",
      "    loss           : -101314.86676639838\n",
      "    val_loss       : -38441.95567220052\n",
      "Train Epoch: 152 [128/17352 (1%)] Loss: -82624.187500\n",
      "Train Epoch: 152 [1536/17352 (9%)] Loss: -90277.468750\n",
      "Train Epoch: 152 [2944/17352 (17%)] Loss: -91856.640625\n",
      "Train Epoch: 152 [4352/17352 (25%)] Loss: -124913.070312\n",
      "Train Epoch: 152 [5760/17352 (33%)] Loss: -79156.609375\n",
      "Train Epoch: 152 [7168/17352 (41%)] Loss: -137884.312500\n",
      "Train Epoch: 152 [8576/17352 (49%)] Loss: -117674.984375\n",
      "Train Epoch: 152 [9984/17352 (58%)] Loss: -122874.734375\n",
      "Train Epoch: 152 [11392/17352 (66%)] Loss: -94529.718750\n",
      "Train Epoch: 152 [12800/17352 (74%)] Loss: -135738.906250\n",
      "Train Epoch: 152 [14208/17352 (82%)] Loss: -117977.960938\n",
      "Train Epoch: 152 [15452/17352 (89%)] Loss: -59138.898438\n",
      "Train Epoch: 152 [16201/17352 (93%)] Loss: -3253.724121\n",
      "Train Epoch: 152 [16953/17352 (98%)] Loss: -29246.273438\n",
      "    epoch          : 152\n",
      "    loss           : -101858.68808986996\n",
      "    val_loss       : -54850.19298095703\n",
      "Train Epoch: 153 [128/17352 (1%)] Loss: -111246.109375\n",
      "Train Epoch: 153 [1536/17352 (9%)] Loss: -144972.671875\n",
      "Train Epoch: 153 [2944/17352 (17%)] Loss: -149118.734375\n",
      "Train Epoch: 153 [4352/17352 (25%)] Loss: -146169.812500\n",
      "Train Epoch: 153 [5760/17352 (33%)] Loss: -134197.093750\n",
      "Train Epoch: 153 [7168/17352 (41%)] Loss: -138952.578125\n",
      "Train Epoch: 153 [8576/17352 (49%)] Loss: -116794.156250\n",
      "Train Epoch: 153 [9984/17352 (58%)] Loss: -123335.406250\n",
      "Train Epoch: 153 [11392/17352 (66%)] Loss: -113602.351562\n",
      "Train Epoch: 153 [12800/17352 (74%)] Loss: -100347.117188\n",
      "Train Epoch: 153 [14208/17352 (82%)] Loss: -106926.718750\n",
      "Train Epoch: 153 [15592/17352 (90%)] Loss: -82963.484375\n",
      "Train Epoch: 153 [16279/17352 (94%)] Loss: -59191.164062\n",
      "Train Epoch: 153 [16926/17352 (98%)] Loss: -5854.287598\n",
      "    epoch          : 153\n",
      "    loss           : -107857.88510971582\n",
      "    val_loss       : -59270.952734375\n",
      "Train Epoch: 154 [128/17352 (1%)] Loss: -88077.375000\n",
      "Train Epoch: 154 [1536/17352 (9%)] Loss: -134113.812500\n",
      "Train Epoch: 154 [2944/17352 (17%)] Loss: -96836.718750\n",
      "Train Epoch: 154 [4352/17352 (25%)] Loss: -136271.515625\n",
      "Train Epoch: 154 [5760/17352 (33%)] Loss: -115750.476562\n",
      "Train Epoch: 154 [7168/17352 (41%)] Loss: -114362.617188\n",
      "Train Epoch: 154 [8576/17352 (49%)] Loss: -112222.039062\n",
      "Train Epoch: 154 [9984/17352 (58%)] Loss: -119964.796875\n",
      "Train Epoch: 154 [11392/17352 (66%)] Loss: -109546.203125\n",
      "Train Epoch: 154 [12800/17352 (74%)] Loss: -115690.023438\n",
      "Train Epoch: 154 [14208/17352 (82%)] Loss: -123143.312500\n",
      "Train Epoch: 154 [15480/17352 (89%)] Loss: -60777.300781\n",
      "Train Epoch: 154 [16126/17352 (93%)] Loss: -21893.085938\n",
      "Train Epoch: 154 [16909/17352 (97%)] Loss: -20242.742188\n",
      "    epoch          : 154\n",
      "    loss           : -101196.41013494914\n",
      "    val_loss       : -37838.51995035807\n",
      "Train Epoch: 155 [128/17352 (1%)] Loss: -61080.703125\n",
      "Train Epoch: 155 [1536/17352 (9%)] Loss: -59599.097656\n",
      "Train Epoch: 155 [2944/17352 (17%)] Loss: -111198.117188\n",
      "Train Epoch: 155 [4352/17352 (25%)] Loss: -134895.046875\n",
      "Train Epoch: 155 [5760/17352 (33%)] Loss: -104519.164062\n",
      "Train Epoch: 155 [7168/17352 (41%)] Loss: -83388.046875\n",
      "Train Epoch: 155 [8576/17352 (49%)] Loss: -100688.734375\n",
      "Train Epoch: 155 [9984/17352 (58%)] Loss: -105864.125000\n",
      "Train Epoch: 155 [11392/17352 (66%)] Loss: -125562.093750\n",
      "Train Epoch: 155 [12800/17352 (74%)] Loss: -134260.218750\n",
      "Train Epoch: 155 [14208/17352 (82%)] Loss: -115274.164062\n",
      "Train Epoch: 155 [15534/17352 (90%)] Loss: -70812.421875\n",
      "Train Epoch: 155 [16340/17352 (94%)] Loss: -29667.859375\n",
      "Train Epoch: 155 [17098/17352 (99%)] Loss: -74029.429688\n",
      "    epoch          : 155\n",
      "    loss           : -97467.27803520868\n",
      "    val_loss       : -54052.03092854818\n",
      "Train Epoch: 156 [128/17352 (1%)] Loss: -119123.390625\n",
      "Train Epoch: 156 [1536/17352 (9%)] Loss: -95601.609375\n",
      "Train Epoch: 156 [2944/17352 (17%)] Loss: -130619.375000\n",
      "Train Epoch: 156 [4352/17352 (25%)] Loss: -121441.171875\n",
      "Train Epoch: 156 [5760/17352 (33%)] Loss: -142289.531250\n",
      "Train Epoch: 156 [7168/17352 (41%)] Loss: -95530.476562\n",
      "Train Epoch: 156 [8576/17352 (49%)] Loss: -153919.640625\n",
      "Train Epoch: 156 [9984/17352 (58%)] Loss: -150381.406250\n",
      "Train Epoch: 156 [11392/17352 (66%)] Loss: -94624.164062\n",
      "Train Epoch: 156 [12800/17352 (74%)] Loss: -147600.625000\n",
      "Train Epoch: 156 [14208/17352 (82%)] Loss: -89949.593750\n",
      "Train Epoch: 156 [15550/17352 (90%)] Loss: -122716.546875\n",
      "Train Epoch: 156 [16140/17352 (93%)] Loss: -26554.759766\n",
      "Train Epoch: 156 [16990/17352 (98%)] Loss: -40563.953125\n",
      "    epoch          : 156\n",
      "    loss           : -102066.6467858641\n",
      "    val_loss       : -57769.66378173828\n",
      "Train Epoch: 157 [128/17352 (1%)] Loss: -113925.312500\n",
      "Train Epoch: 157 [1536/17352 (9%)] Loss: -107557.484375\n",
      "Train Epoch: 157 [2944/17352 (17%)] Loss: -150600.484375\n",
      "Train Epoch: 157 [4352/17352 (25%)] Loss: -136148.109375\n",
      "Train Epoch: 157 [5760/17352 (33%)] Loss: -98691.968750\n",
      "Train Epoch: 157 [7168/17352 (41%)] Loss: -116641.265625\n",
      "Train Epoch: 157 [8576/17352 (49%)] Loss: -82134.468750\n",
      "Train Epoch: 157 [9984/17352 (58%)] Loss: -92664.601562\n",
      "Train Epoch: 157 [11392/17352 (66%)] Loss: -130188.187500\n",
      "Train Epoch: 157 [12800/17352 (74%)] Loss: -117738.187500\n",
      "Train Epoch: 157 [14208/17352 (82%)] Loss: -148643.078125\n",
      "Train Epoch: 157 [15527/17352 (89%)] Loss: -68730.609375\n",
      "Train Epoch: 157 [16192/17352 (93%)] Loss: -2401.639648\n",
      "Train Epoch: 157 [17069/17352 (98%)] Loss: -63360.656250\n",
      "    epoch          : 157\n",
      "    loss           : -104391.49070954803\n",
      "    val_loss       : -54399.41236572266\n",
      "Train Epoch: 158 [128/17352 (1%)] Loss: -127249.648438\n",
      "Train Epoch: 158 [1536/17352 (9%)] Loss: -140154.093750\n",
      "Train Epoch: 158 [2944/17352 (17%)] Loss: -73553.812500\n",
      "Train Epoch: 158 [4352/17352 (25%)] Loss: -98173.804688\n",
      "Train Epoch: 158 [5760/17352 (33%)] Loss: -100791.609375\n",
      "Train Epoch: 158 [7168/17352 (41%)] Loss: -114208.734375\n",
      "Train Epoch: 158 [8576/17352 (49%)] Loss: -115377.031250\n",
      "Train Epoch: 158 [9984/17352 (58%)] Loss: -120706.664062\n",
      "Train Epoch: 158 [11392/17352 (66%)] Loss: -133020.218750\n",
      "Train Epoch: 158 [12800/17352 (74%)] Loss: -127737.921875\n",
      "Train Epoch: 158 [14208/17352 (82%)] Loss: -112755.953125\n",
      "Train Epoch: 158 [15480/17352 (89%)] Loss: -44116.964844\n",
      "Train Epoch: 158 [16346/17352 (94%)] Loss: -101553.500000\n",
      "Train Epoch: 158 [17027/17352 (98%)] Loss: -82869.609375\n",
      "    epoch          : 158\n",
      "    loss           : -103602.71871067534\n",
      "    val_loss       : -48825.66532796224\n",
      "Train Epoch: 159 [128/17352 (1%)] Loss: -88491.125000\n",
      "Train Epoch: 159 [1536/17352 (9%)] Loss: -118159.148438\n",
      "Train Epoch: 159 [2944/17352 (17%)] Loss: -159355.171875\n",
      "Train Epoch: 159 [4352/17352 (25%)] Loss: -112770.539062\n",
      "Train Epoch: 159 [5760/17352 (33%)] Loss: -111448.718750\n",
      "Train Epoch: 159 [7168/17352 (41%)] Loss: -132133.875000\n",
      "Train Epoch: 159 [8576/17352 (49%)] Loss: -104194.054688\n",
      "Train Epoch: 159 [9984/17352 (58%)] Loss: -73627.781250\n",
      "Train Epoch: 159 [11392/17352 (66%)] Loss: -127888.328125\n",
      "Train Epoch: 159 [12800/17352 (74%)] Loss: -74719.914062\n",
      "Train Epoch: 159 [14208/17352 (82%)] Loss: -137642.437500\n",
      "Train Epoch: 159 [15451/17352 (89%)] Loss: -2825.777832\n",
      "Train Epoch: 159 [16111/17352 (93%)] Loss: -102491.062500\n",
      "Train Epoch: 159 [16948/17352 (98%)] Loss: -77229.875000\n",
      "    epoch          : 159\n",
      "    loss           : -104013.91294830118\n",
      "    val_loss       : -55769.868880208334\n",
      "Train Epoch: 160 [128/17352 (1%)] Loss: -113120.101562\n",
      "Train Epoch: 160 [1536/17352 (9%)] Loss: -65406.781250\n",
      "Train Epoch: 160 [2944/17352 (17%)] Loss: -112469.843750\n",
      "Train Epoch: 160 [4352/17352 (25%)] Loss: -156424.656250\n",
      "Train Epoch: 160 [5760/17352 (33%)] Loss: -127674.226562\n",
      "Train Epoch: 160 [7168/17352 (41%)] Loss: -117357.609375\n",
      "Train Epoch: 160 [8576/17352 (49%)] Loss: -117252.796875\n",
      "Train Epoch: 160 [9984/17352 (58%)] Loss: -122356.679688\n",
      "Train Epoch: 160 [11392/17352 (66%)] Loss: -138841.218750\n",
      "Train Epoch: 160 [12800/17352 (74%)] Loss: -116324.031250\n",
      "Train Epoch: 160 [14208/17352 (82%)] Loss: -110771.875000\n",
      "Train Epoch: 160 [15546/17352 (90%)] Loss: -74169.554688\n",
      "Train Epoch: 160 [16215/17352 (93%)] Loss: -12441.208984\n",
      "Train Epoch: 160 [16953/17352 (98%)] Loss: -67155.140625\n",
      "    epoch          : 160\n",
      "    loss           : -102713.42613189493\n",
      "    val_loss       : -50923.01699625651\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch160.pth ...\n",
      "Train Epoch: 161 [128/17352 (1%)] Loss: -116412.593750\n",
      "Train Epoch: 161 [1536/17352 (9%)] Loss: -109818.132812\n",
      "Train Epoch: 161 [2944/17352 (17%)] Loss: -109293.898438\n",
      "Train Epoch: 161 [4352/17352 (25%)] Loss: -121957.335938\n",
      "Train Epoch: 161 [5760/17352 (33%)] Loss: -95250.328125\n",
      "Train Epoch: 161 [7168/17352 (41%)] Loss: -109793.734375\n",
      "Train Epoch: 161 [8576/17352 (49%)] Loss: -117469.132812\n",
      "Train Epoch: 161 [9984/17352 (58%)] Loss: -104263.968750\n",
      "Train Epoch: 161 [11392/17352 (66%)] Loss: -100915.671875\n",
      "Train Epoch: 161 [12800/17352 (74%)] Loss: -122083.992188\n",
      "Train Epoch: 161 [14208/17352 (82%)] Loss: -105064.218750\n",
      "Train Epoch: 161 [15499/17352 (89%)] Loss: -43826.921875\n",
      "Train Epoch: 161 [16362/17352 (94%)] Loss: -96166.601562\n",
      "Train Epoch: 161 [17090/17352 (98%)] Loss: -85981.460938\n",
      "    epoch          : 161\n",
      "    loss           : -97418.69820613989\n",
      "    val_loss       : -50384.40900472005\n",
      "Train Epoch: 162 [128/17352 (1%)] Loss: -91423.937500\n",
      "Train Epoch: 162 [1536/17352 (9%)] Loss: -121504.015625\n",
      "Train Epoch: 162 [2944/17352 (17%)] Loss: -94997.882812\n",
      "Train Epoch: 162 [4352/17352 (25%)] Loss: -84112.109375\n",
      "Train Epoch: 162 [5760/17352 (33%)] Loss: -134433.718750\n",
      "Train Epoch: 162 [7168/17352 (41%)] Loss: -115274.531250\n",
      "Train Epoch: 162 [8576/17352 (49%)] Loss: -132577.000000\n",
      "Train Epoch: 162 [9984/17352 (58%)] Loss: -147412.734375\n",
      "Train Epoch: 162 [11392/17352 (66%)] Loss: -126951.937500\n",
      "Train Epoch: 162 [12800/17352 (74%)] Loss: -112711.781250\n",
      "Train Epoch: 162 [14208/17352 (82%)] Loss: -119491.210938\n",
      "Train Epoch: 162 [15549/17352 (90%)] Loss: -95399.523438\n",
      "Train Epoch: 162 [16248/17352 (94%)] Loss: -41391.640625\n",
      "Train Epoch: 162 [17044/17352 (98%)] Loss: -73753.796875\n",
      "    epoch          : 162\n",
      "    loss           : -107966.1134057781\n",
      "    val_loss       : -60539.300618489586\n",
      "Train Epoch: 163 [128/17352 (1%)] Loss: -160485.468750\n",
      "Train Epoch: 163 [1536/17352 (9%)] Loss: -120769.953125\n",
      "Train Epoch: 163 [2944/17352 (17%)] Loss: -144407.171875\n",
      "Train Epoch: 163 [4352/17352 (25%)] Loss: -129528.289062\n",
      "Train Epoch: 163 [5760/17352 (33%)] Loss: -130539.976562\n",
      "Train Epoch: 163 [7168/17352 (41%)] Loss: -118119.195312\n",
      "Train Epoch: 163 [8576/17352 (49%)] Loss: -104020.796875\n",
      "Train Epoch: 163 [9984/17352 (58%)] Loss: -119118.062500\n",
      "Train Epoch: 163 [11392/17352 (66%)] Loss: -128102.171875\n",
      "Train Epoch: 163 [12800/17352 (74%)] Loss: -126661.156250\n",
      "Train Epoch: 163 [14208/17352 (82%)] Loss: -97573.171875\n",
      "Train Epoch: 163 [15552/17352 (90%)] Loss: -74361.109375\n",
      "Train Epoch: 163 [16300/17352 (94%)] Loss: -25841.388672\n",
      "Train Epoch: 163 [16954/17352 (98%)] Loss: -3931.328125\n",
      "    epoch          : 163\n",
      "    loss           : -107762.66479983745\n",
      "    val_loss       : -61179.103983561195\n",
      "Train Epoch: 164 [128/17352 (1%)] Loss: -104545.968750\n",
      "Train Epoch: 164 [1536/17352 (9%)] Loss: -108739.515625\n",
      "Train Epoch: 164 [2944/17352 (17%)] Loss: -134092.406250\n",
      "Train Epoch: 164 [4352/17352 (25%)] Loss: -133130.062500\n",
      "Train Epoch: 164 [5760/17352 (33%)] Loss: -123990.093750\n",
      "Train Epoch: 164 [7168/17352 (41%)] Loss: -103642.890625\n",
      "Train Epoch: 164 [8576/17352 (49%)] Loss: -112536.234375\n",
      "Train Epoch: 164 [9984/17352 (58%)] Loss: -108567.984375\n",
      "Train Epoch: 164 [11392/17352 (66%)] Loss: -76148.554688\n",
      "Train Epoch: 164 [12800/17352 (74%)] Loss: -111151.796875\n",
      "Train Epoch: 164 [14208/17352 (82%)] Loss: -130223.937500\n",
      "Train Epoch: 164 [15448/17352 (89%)] Loss: -2768.411377\n",
      "Train Epoch: 164 [16198/17352 (93%)] Loss: -121026.429688\n",
      "Train Epoch: 164 [17066/17352 (98%)] Loss: -37097.027344\n",
      "    epoch          : 164\n",
      "    loss           : -108020.428414364\n",
      "    val_loss       : -38213.0302734375\n",
      "Train Epoch: 165 [128/17352 (1%)] Loss: -85510.953125\n",
      "Train Epoch: 165 [1536/17352 (9%)] Loss: -113945.867188\n",
      "Train Epoch: 165 [2944/17352 (17%)] Loss: -151855.625000\n",
      "Train Epoch: 165 [4352/17352 (25%)] Loss: -101955.734375\n",
      "Train Epoch: 165 [5760/17352 (33%)] Loss: -112705.140625\n",
      "Train Epoch: 165 [7168/17352 (41%)] Loss: -109327.687500\n",
      "Train Epoch: 165 [8576/17352 (49%)] Loss: -129333.742188\n",
      "Train Epoch: 165 [9984/17352 (58%)] Loss: -102523.007812\n",
      "Train Epoch: 165 [11392/17352 (66%)] Loss: -134536.156250\n",
      "Train Epoch: 165 [12800/17352 (74%)] Loss: -114092.000000\n",
      "Train Epoch: 165 [14208/17352 (82%)] Loss: -130799.687500\n",
      "Train Epoch: 165 [15447/17352 (89%)] Loss: -88883.421875\n",
      "Train Epoch: 165 [16307/17352 (94%)] Loss: -82271.468750\n",
      "Train Epoch: 165 [17101/17352 (99%)] Loss: -13590.126953\n",
      "    epoch          : 165\n",
      "    loss           : -109442.1425011142\n",
      "    val_loss       : -61870.44755859375\n",
      "Train Epoch: 166 [128/17352 (1%)] Loss: -100103.109375\n",
      "Train Epoch: 166 [1536/17352 (9%)] Loss: -98288.570312\n",
      "Train Epoch: 166 [2944/17352 (17%)] Loss: -124816.406250\n",
      "Train Epoch: 166 [4352/17352 (25%)] Loss: -148038.984375\n",
      "Train Epoch: 166 [5760/17352 (33%)] Loss: -115048.539062\n",
      "Train Epoch: 166 [7168/17352 (41%)] Loss: -149431.906250\n",
      "Train Epoch: 166 [8576/17352 (49%)] Loss: -96115.859375\n",
      "Train Epoch: 166 [9984/17352 (58%)] Loss: -100941.859375\n",
      "Train Epoch: 166 [11392/17352 (66%)] Loss: -97434.851562\n",
      "Train Epoch: 166 [12800/17352 (74%)] Loss: -113904.484375\n",
      "Train Epoch: 166 [14208/17352 (82%)] Loss: -127616.796875\n",
      "Train Epoch: 166 [15521/17352 (89%)] Loss: -77919.296875\n",
      "Train Epoch: 166 [16266/17352 (94%)] Loss: -36328.539062\n",
      "Train Epoch: 166 [16910/17352 (97%)] Loss: -20570.273438\n",
      "    epoch          : 166\n",
      "    loss           : -107066.70341829646\n",
      "    val_loss       : -49442.98773600261\n",
      "Train Epoch: 167 [128/17352 (1%)] Loss: -83240.734375\n",
      "Train Epoch: 167 [1536/17352 (9%)] Loss: -123627.437500\n",
      "Train Epoch: 167 [2944/17352 (17%)] Loss: -105244.250000\n",
      "Train Epoch: 167 [4352/17352 (25%)] Loss: -146455.312500\n",
      "Train Epoch: 167 [5760/17352 (33%)] Loss: -122337.695312\n",
      "Train Epoch: 167 [7168/17352 (41%)] Loss: -125212.734375\n",
      "Train Epoch: 167 [8576/17352 (49%)] Loss: -101028.679688\n",
      "Train Epoch: 167 [9984/17352 (58%)] Loss: -139890.406250\n",
      "Train Epoch: 167 [11392/17352 (66%)] Loss: -158652.640625\n",
      "Train Epoch: 167 [12800/17352 (74%)] Loss: -138127.187500\n",
      "Train Epoch: 167 [14208/17352 (82%)] Loss: -88266.796875\n",
      "Train Epoch: 167 [15448/17352 (89%)] Loss: -41825.554688\n",
      "Train Epoch: 167 [16012/17352 (92%)] Loss: -17874.531250\n",
      "Train Epoch: 167 [16967/17352 (98%)] Loss: -93594.328125\n",
      "    epoch          : 167\n",
      "    loss           : -104029.85810432179\n",
      "    val_loss       : -50100.75135498047\n",
      "Train Epoch: 168 [128/17352 (1%)] Loss: -112748.539062\n",
      "Train Epoch: 168 [1536/17352 (9%)] Loss: -66232.914062\n",
      "Train Epoch: 168 [2944/17352 (17%)] Loss: -127008.984375\n",
      "Train Epoch: 168 [4352/17352 (25%)] Loss: -129413.460938\n",
      "Train Epoch: 168 [5760/17352 (33%)] Loss: -140924.593750\n",
      "Train Epoch: 168 [7168/17352 (41%)] Loss: -106225.937500\n",
      "Train Epoch: 168 [8576/17352 (49%)] Loss: -141693.046875\n",
      "Train Epoch: 168 [9984/17352 (58%)] Loss: -59626.136719\n",
      "Train Epoch: 168 [11392/17352 (66%)] Loss: -159535.515625\n",
      "Train Epoch: 168 [12800/17352 (74%)] Loss: -125477.039062\n",
      "Train Epoch: 168 [14208/17352 (82%)] Loss: -117220.921875\n",
      "Train Epoch: 168 [15458/17352 (89%)] Loss: -104606.343750\n",
      "Train Epoch: 168 [16232/17352 (94%)] Loss: -57990.425781\n",
      "Train Epoch: 168 [17096/17352 (99%)] Loss: -88004.203125\n",
      "    epoch          : 168\n",
      "    loss           : -103029.89122142407\n",
      "    val_loss       : -57879.932588704425\n",
      "Train Epoch: 169 [128/17352 (1%)] Loss: -96346.164062\n",
      "Train Epoch: 169 [1536/17352 (9%)] Loss: -129651.515625\n",
      "Train Epoch: 169 [2944/17352 (17%)] Loss: -105474.210938\n",
      "Train Epoch: 169 [4352/17352 (25%)] Loss: -85721.750000\n",
      "Train Epoch: 169 [5760/17352 (33%)] Loss: -145047.687500\n",
      "Train Epoch: 169 [7168/17352 (41%)] Loss: -143415.859375\n",
      "Train Epoch: 169 [8576/17352 (49%)] Loss: -151220.671875\n",
      "Train Epoch: 169 [9984/17352 (58%)] Loss: -124871.468750\n",
      "Train Epoch: 169 [11392/17352 (66%)] Loss: -98300.820312\n",
      "Train Epoch: 169 [12800/17352 (74%)] Loss: -164976.093750\n",
      "Train Epoch: 169 [14208/17352 (82%)] Loss: -151738.187500\n",
      "Train Epoch: 169 [15540/17352 (90%)] Loss: -96786.328125\n",
      "Train Epoch: 169 [16244/17352 (94%)] Loss: -99293.726562\n",
      "Train Epoch: 169 [17054/17352 (98%)] Loss: -38835.785156\n",
      "    epoch          : 169\n",
      "    loss           : -111339.17846024277\n",
      "    val_loss       : -57050.97242431641\n",
      "Train Epoch: 170 [128/17352 (1%)] Loss: -129709.757812\n",
      "Train Epoch: 170 [1536/17352 (9%)] Loss: -78727.734375\n",
      "Train Epoch: 170 [2944/17352 (17%)] Loss: -87673.632812\n",
      "Train Epoch: 170 [4352/17352 (25%)] Loss: -127889.390625\n",
      "Train Epoch: 170 [5760/17352 (33%)] Loss: -113255.000000\n",
      "Train Epoch: 170 [7168/17352 (41%)] Loss: -126995.359375\n",
      "Train Epoch: 170 [8576/17352 (49%)] Loss: -145740.000000\n",
      "Train Epoch: 170 [9984/17352 (58%)] Loss: -139229.437500\n",
      "Train Epoch: 170 [11392/17352 (66%)] Loss: -122324.445312\n",
      "Train Epoch: 170 [12800/17352 (74%)] Loss: -115924.312500\n",
      "Train Epoch: 170 [14208/17352 (82%)] Loss: -101183.671875\n",
      "Train Epoch: 170 [15488/17352 (89%)] Loss: -98897.835938\n",
      "Train Epoch: 170 [16314/17352 (94%)] Loss: -58302.562500\n",
      "Train Epoch: 170 [17054/17352 (98%)] Loss: -82669.320312\n",
      "    epoch          : 170\n",
      "    loss           : -108551.83231963088\n",
      "    val_loss       : -59069.66723225911\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch170.pth ...\n",
      "Train Epoch: 171 [128/17352 (1%)] Loss: -107279.109375\n",
      "Train Epoch: 171 [1536/17352 (9%)] Loss: -147777.718750\n",
      "Train Epoch: 171 [2944/17352 (17%)] Loss: -124523.031250\n",
      "Train Epoch: 171 [4352/17352 (25%)] Loss: -110342.265625\n",
      "Train Epoch: 171 [5760/17352 (33%)] Loss: -152834.078125\n",
      "Train Epoch: 171 [7168/17352 (41%)] Loss: -148266.875000\n",
      "Train Epoch: 171 [8576/17352 (49%)] Loss: -94344.726562\n",
      "Train Epoch: 171 [9984/17352 (58%)] Loss: -107984.125000\n",
      "Train Epoch: 171 [11392/17352 (66%)] Loss: -101346.437500\n",
      "Train Epoch: 171 [12800/17352 (74%)] Loss: -117800.382812\n",
      "Train Epoch: 171 [14208/17352 (82%)] Loss: -112429.812500\n",
      "Train Epoch: 171 [15451/17352 (89%)] Loss: -9783.583984\n",
      "Train Epoch: 171 [16052/17352 (93%)] Loss: -4324.810059\n",
      "Train Epoch: 171 [16946/17352 (98%)] Loss: -22450.066406\n",
      "    epoch          : 171\n",
      "    loss           : -102416.65145239094\n",
      "    val_loss       : -56385.218033854166\n",
      "Train Epoch: 172 [128/17352 (1%)] Loss: -101508.187500\n",
      "Train Epoch: 172 [1536/17352 (9%)] Loss: -145937.375000\n",
      "Train Epoch: 172 [2944/17352 (17%)] Loss: -133803.750000\n",
      "Train Epoch: 172 [4352/17352 (25%)] Loss: -160906.906250\n",
      "Train Epoch: 172 [5760/17352 (33%)] Loss: -102585.390625\n",
      "Train Epoch: 172 [7168/17352 (41%)] Loss: -152569.031250\n",
      "Train Epoch: 172 [8576/17352 (49%)] Loss: -124352.015625\n",
      "Train Epoch: 172 [9984/17352 (58%)] Loss: -150106.625000\n",
      "Train Epoch: 172 [11392/17352 (66%)] Loss: -82894.078125\n",
      "Train Epoch: 172 [12800/17352 (74%)] Loss: -97750.312500\n",
      "Train Epoch: 172 [14208/17352 (82%)] Loss: -128642.273438\n",
      "Train Epoch: 172 [15471/17352 (89%)] Loss: -57182.566406\n",
      "Train Epoch: 172 [16272/17352 (94%)] Loss: -79268.757812\n",
      "Train Epoch: 172 [17036/17352 (98%)] Loss: -60349.511719\n",
      "    epoch          : 172\n",
      "    loss           : -107637.03086986157\n",
      "    val_loss       : -58813.234220377606\n",
      "Train Epoch: 173 [128/17352 (1%)] Loss: -100709.835938\n",
      "Train Epoch: 173 [1536/17352 (9%)] Loss: -145186.781250\n",
      "Train Epoch: 173 [2944/17352 (17%)] Loss: -124156.421875\n",
      "Train Epoch: 173 [4352/17352 (25%)] Loss: -150320.218750\n",
      "Train Epoch: 173 [5760/17352 (33%)] Loss: -95793.718750\n",
      "Train Epoch: 173 [7168/17352 (41%)] Loss: -112530.375000\n",
      "Train Epoch: 173 [8576/17352 (49%)] Loss: -133427.328125\n",
      "Train Epoch: 173 [9984/17352 (58%)] Loss: -113349.265625\n",
      "Train Epoch: 173 [11392/17352 (66%)] Loss: -136093.562500\n",
      "Train Epoch: 173 [12800/17352 (74%)] Loss: -94739.875000\n",
      "Train Epoch: 173 [14208/17352 (82%)] Loss: -126134.039062\n",
      "Train Epoch: 173 [15552/17352 (90%)] Loss: -88394.773438\n",
      "Train Epoch: 173 [16095/17352 (93%)] Loss: -45991.691406\n",
      "Train Epoch: 173 [16982/17352 (98%)] Loss: -95867.257812\n",
      "    epoch          : 173\n",
      "    loss           : -112433.90645481597\n",
      "    val_loss       : -59067.32285563151\n",
      "Train Epoch: 174 [128/17352 (1%)] Loss: -115595.437500\n",
      "Train Epoch: 174 [1536/17352 (9%)] Loss: -122219.812500\n",
      "Train Epoch: 174 [2944/17352 (17%)] Loss: -119064.835938\n",
      "Train Epoch: 174 [4352/17352 (25%)] Loss: -116753.976562\n",
      "Train Epoch: 174 [5760/17352 (33%)] Loss: -160568.375000\n",
      "Train Epoch: 174 [7168/17352 (41%)] Loss: -139422.437500\n",
      "Train Epoch: 174 [8576/17352 (49%)] Loss: -119517.828125\n",
      "Train Epoch: 174 [9984/17352 (58%)] Loss: -134994.265625\n",
      "Train Epoch: 174 [11392/17352 (66%)] Loss: -117071.640625\n",
      "Train Epoch: 174 [12800/17352 (74%)] Loss: -132492.812500\n",
      "Train Epoch: 174 [14208/17352 (82%)] Loss: -101371.257812\n",
      "Train Epoch: 174 [15559/17352 (90%)] Loss: -138398.406250\n",
      "Train Epoch: 174 [16406/17352 (95%)] Loss: -26915.468750\n",
      "Train Epoch: 174 [16997/17352 (98%)] Loss: -15723.626953\n",
      "    epoch          : 174\n",
      "    loss           : -109881.21945128984\n",
      "    val_loss       : -63345.431107584634\n",
      "Train Epoch: 175 [128/17352 (1%)] Loss: -100249.117188\n",
      "Train Epoch: 175 [1536/17352 (9%)] Loss: -125178.140625\n",
      "Train Epoch: 175 [2944/17352 (17%)] Loss: -144817.468750\n",
      "Train Epoch: 175 [4352/17352 (25%)] Loss: -138065.843750\n",
      "Train Epoch: 175 [5760/17352 (33%)] Loss: -124840.867188\n",
      "Train Epoch: 175 [7168/17352 (41%)] Loss: -152418.609375\n",
      "Train Epoch: 175 [8576/17352 (49%)] Loss: -167279.218750\n",
      "Train Epoch: 175 [9984/17352 (58%)] Loss: -154470.640625\n",
      "Train Epoch: 175 [11392/17352 (66%)] Loss: -156560.281250\n",
      "Train Epoch: 175 [12800/17352 (74%)] Loss: -113550.140625\n",
      "Train Epoch: 175 [14208/17352 (82%)] Loss: -130327.742188\n",
      "Train Epoch: 175 [15492/17352 (89%)] Loss: -153951.593750\n",
      "Train Epoch: 175 [16234/17352 (94%)] Loss: -48423.914062\n",
      "Train Epoch: 175 [17015/17352 (98%)] Loss: -50390.378906\n",
      "    epoch          : 175\n",
      "    loss           : -110554.60887688758\n",
      "    val_loss       : -56889.88455810547\n",
      "Train Epoch: 176 [128/17352 (1%)] Loss: -85454.695312\n",
      "Train Epoch: 176 [1536/17352 (9%)] Loss: -107801.890625\n",
      "Train Epoch: 176 [2944/17352 (17%)] Loss: -131459.671875\n",
      "Train Epoch: 176 [4352/17352 (25%)] Loss: -86826.359375\n",
      "Train Epoch: 176 [5760/17352 (33%)] Loss: -152017.281250\n",
      "Train Epoch: 176 [7168/17352 (41%)] Loss: -122540.765625\n",
      "Train Epoch: 176 [8576/17352 (49%)] Loss: -152259.093750\n",
      "Train Epoch: 176 [9984/17352 (58%)] Loss: -122883.359375\n",
      "Train Epoch: 176 [11392/17352 (66%)] Loss: -125600.546875\n",
      "Train Epoch: 176 [12800/17352 (74%)] Loss: -120842.796875\n",
      "Train Epoch: 176 [14208/17352 (82%)] Loss: -80367.640625\n",
      "Train Epoch: 176 [15418/17352 (89%)] Loss: -16808.000000\n",
      "Train Epoch: 176 [16110/17352 (93%)] Loss: -34059.449219\n",
      "Train Epoch: 176 [17008/17352 (98%)] Loss: -72630.789062\n",
      "    epoch          : 176\n",
      "    loss           : -107265.01047510748\n",
      "    val_loss       : -62875.88283691406\n",
      "Train Epoch: 177 [128/17352 (1%)] Loss: -130727.914062\n",
      "Train Epoch: 177 [1536/17352 (9%)] Loss: -121102.375000\n",
      "Train Epoch: 177 [2944/17352 (17%)] Loss: -116110.320312\n",
      "Train Epoch: 177 [4352/17352 (25%)] Loss: -140188.171875\n",
      "Train Epoch: 177 [5760/17352 (33%)] Loss: -124880.062500\n",
      "Train Epoch: 177 [7168/17352 (41%)] Loss: -124683.960938\n",
      "Train Epoch: 177 [8576/17352 (49%)] Loss: -100239.265625\n",
      "Train Epoch: 177 [9984/17352 (58%)] Loss: -138562.218750\n",
      "Train Epoch: 177 [11392/17352 (66%)] Loss: -143962.718750\n",
      "Train Epoch: 177 [12800/17352 (74%)] Loss: -139854.359375\n",
      "Train Epoch: 177 [14208/17352 (82%)] Loss: -104770.046875\n",
      "Train Epoch: 177 [15407/17352 (89%)] Loss: -12400.603516\n",
      "Train Epoch: 177 [16085/17352 (93%)] Loss: -111144.390625\n",
      "Train Epoch: 177 [16901/17352 (97%)] Loss: -38326.019531\n",
      "    epoch          : 177\n",
      "    loss           : -111784.49722105704\n",
      "    val_loss       : -58133.164050292966\n",
      "Train Epoch: 178 [128/17352 (1%)] Loss: -121061.984375\n",
      "Train Epoch: 178 [1536/17352 (9%)] Loss: -89125.437500\n",
      "Train Epoch: 178 [2944/17352 (17%)] Loss: -125521.375000\n",
      "Train Epoch: 178 [4352/17352 (25%)] Loss: -103419.750000\n",
      "Train Epoch: 178 [5760/17352 (33%)] Loss: -136313.281250\n",
      "Train Epoch: 178 [7168/17352 (41%)] Loss: -115311.296875\n",
      "Train Epoch: 178 [8576/17352 (49%)] Loss: -160245.109375\n",
      "Train Epoch: 178 [9984/17352 (58%)] Loss: -139812.109375\n",
      "Train Epoch: 178 [11392/17352 (66%)] Loss: -130482.468750\n",
      "Train Epoch: 178 [12800/17352 (74%)] Loss: -109566.789062\n",
      "Train Epoch: 178 [14208/17352 (82%)] Loss: -141322.734375\n",
      "Train Epoch: 178 [15516/17352 (89%)] Loss: -94408.937500\n",
      "Train Epoch: 178 [16400/17352 (95%)] Loss: -55642.617188\n",
      "Train Epoch: 178 [16970/17352 (98%)] Loss: -79113.625000\n",
      "    epoch          : 178\n",
      "    loss           : -110864.76164993184\n",
      "    val_loss       : -63940.73558349609\n",
      "Train Epoch: 179 [128/17352 (1%)] Loss: -139662.750000\n",
      "Train Epoch: 179 [1536/17352 (9%)] Loss: -121279.281250\n",
      "Train Epoch: 179 [2944/17352 (17%)] Loss: -109695.039062\n",
      "Train Epoch: 179 [4352/17352 (25%)] Loss: -115477.437500\n",
      "Train Epoch: 179 [5760/17352 (33%)] Loss: -121284.171875\n",
      "Train Epoch: 179 [7168/17352 (41%)] Loss: -150289.828125\n",
      "Train Epoch: 179 [8576/17352 (49%)] Loss: -120734.304688\n",
      "Train Epoch: 179 [9984/17352 (58%)] Loss: -97441.250000\n",
      "Train Epoch: 179 [11392/17352 (66%)] Loss: -116404.992188\n",
      "Train Epoch: 179 [12800/17352 (74%)] Loss: -114842.242188\n",
      "Train Epoch: 179 [14208/17352 (82%)] Loss: -134874.718750\n",
      "Train Epoch: 179 [15515/17352 (89%)] Loss: -45206.398438\n",
      "Train Epoch: 179 [16358/17352 (94%)] Loss: -21583.617188\n",
      "Train Epoch: 179 [16954/17352 (98%)] Loss: -44313.667969\n",
      "    epoch          : 179\n",
      "    loss           : -114497.20687558987\n",
      "    val_loss       : -62562.61133626302\n",
      "Train Epoch: 180 [128/17352 (1%)] Loss: -84439.617188\n",
      "Train Epoch: 180 [1536/17352 (9%)] Loss: -111643.523438\n",
      "Train Epoch: 180 [2944/17352 (17%)] Loss: -99019.015625\n",
      "Train Epoch: 180 [4352/17352 (25%)] Loss: -155154.187500\n",
      "Train Epoch: 180 [5760/17352 (33%)] Loss: -130453.500000\n",
      "Train Epoch: 180 [7168/17352 (41%)] Loss: -70447.117188\n",
      "Train Epoch: 180 [8576/17352 (49%)] Loss: -108717.187500\n",
      "Train Epoch: 180 [9984/17352 (58%)] Loss: -143872.375000\n",
      "Train Epoch: 180 [11392/17352 (66%)] Loss: -133695.140625\n",
      "Train Epoch: 180 [12800/17352 (74%)] Loss: -94685.773438\n",
      "Train Epoch: 180 [14208/17352 (82%)] Loss: -108670.851562\n",
      "Train Epoch: 180 [15448/17352 (89%)] Loss: -2631.320801\n",
      "Train Epoch: 180 [16225/17352 (94%)] Loss: -74675.468750\n",
      "Train Epoch: 180 [16997/17352 (98%)] Loss: -74246.593750\n",
      "    epoch          : 180\n",
      "    loss           : -103173.15225526951\n",
      "    val_loss       : -59292.14064127604\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch180.pth ...\n",
      "Train Epoch: 181 [128/17352 (1%)] Loss: -94773.289062\n",
      "Train Epoch: 181 [1536/17352 (9%)] Loss: -129763.437500\n",
      "Train Epoch: 181 [2944/17352 (17%)] Loss: -110596.000000\n",
      "Train Epoch: 181 [4352/17352 (25%)] Loss: -114737.500000\n",
      "Train Epoch: 181 [5760/17352 (33%)] Loss: -121558.500000\n",
      "Train Epoch: 181 [7168/17352 (41%)] Loss: -119740.273438\n",
      "Train Epoch: 181 [8576/17352 (49%)] Loss: -113799.859375\n",
      "Train Epoch: 181 [9984/17352 (58%)] Loss: -124191.914062\n",
      "Train Epoch: 181 [11392/17352 (66%)] Loss: -117841.718750\n",
      "Train Epoch: 181 [12800/17352 (74%)] Loss: -123863.789062\n",
      "Train Epoch: 181 [14208/17352 (82%)] Loss: -100374.546875\n",
      "Train Epoch: 181 [15520/17352 (89%)] Loss: -87143.593750\n",
      "Train Epoch: 181 [16091/17352 (93%)] Loss: -60356.226562\n",
      "Train Epoch: 181 [17021/17352 (98%)] Loss: -114852.296875\n",
      "    epoch          : 181\n",
      "    loss           : -110285.26074874161\n",
      "    val_loss       : -48957.35259602864\n",
      "Train Epoch: 182 [128/17352 (1%)] Loss: -125323.007812\n",
      "Train Epoch: 182 [1536/17352 (9%)] Loss: -128264.562500\n",
      "Train Epoch: 182 [2944/17352 (17%)] Loss: -129873.812500\n",
      "Train Epoch: 182 [4352/17352 (25%)] Loss: -117112.500000\n",
      "Train Epoch: 182 [5760/17352 (33%)] Loss: -93327.875000\n",
      "Train Epoch: 182 [7168/17352 (41%)] Loss: -107169.031250\n",
      "Train Epoch: 182 [8576/17352 (49%)] Loss: -100642.625000\n",
      "Train Epoch: 182 [9984/17352 (58%)] Loss: -101829.671875\n",
      "Train Epoch: 182 [11392/17352 (66%)] Loss: -119603.585938\n",
      "Train Epoch: 182 [12800/17352 (74%)] Loss: -93466.515625\n",
      "Train Epoch: 182 [14208/17352 (82%)] Loss: -132772.859375\n",
      "Train Epoch: 182 [15518/17352 (89%)] Loss: -89163.937500\n",
      "Train Epoch: 182 [16198/17352 (93%)] Loss: -16051.978516\n",
      "Train Epoch: 182 [16900/17352 (97%)] Loss: -5855.762207\n",
      "    epoch          : 182\n",
      "    loss           : -104463.50948379823\n",
      "    val_loss       : -58321.957283528645\n",
      "Train Epoch: 183 [128/17352 (1%)] Loss: -116880.148438\n",
      "Train Epoch: 183 [1536/17352 (9%)] Loss: -115975.757812\n",
      "Train Epoch: 183 [2944/17352 (17%)] Loss: -116615.039062\n",
      "Train Epoch: 183 [4352/17352 (25%)] Loss: -131538.828125\n",
      "Train Epoch: 183 [5760/17352 (33%)] Loss: -146315.546875\n",
      "Train Epoch: 183 [7168/17352 (41%)] Loss: -132271.906250\n",
      "Train Epoch: 183 [8576/17352 (49%)] Loss: -64901.585938\n",
      "Train Epoch: 183 [9984/17352 (58%)] Loss: -86458.906250\n",
      "Train Epoch: 183 [11392/17352 (66%)] Loss: -126985.320312\n",
      "Train Epoch: 183 [12800/17352 (74%)] Loss: -123197.734375\n",
      "Train Epoch: 183 [14208/17352 (82%)] Loss: -117478.796875\n",
      "Train Epoch: 183 [15512/17352 (89%)] Loss: -60791.726562\n",
      "Train Epoch: 183 [16313/17352 (94%)] Loss: -86860.117188\n",
      "Train Epoch: 183 [16983/17352 (98%)] Loss: -93568.828125\n",
      "    epoch          : 183\n",
      "    loss           : -112911.34531970952\n",
      "    val_loss       : -62015.37036539714\n",
      "Train Epoch: 184 [128/17352 (1%)] Loss: -139782.156250\n",
      "Train Epoch: 184 [1536/17352 (9%)] Loss: -119094.429688\n",
      "Train Epoch: 184 [2944/17352 (17%)] Loss: -90861.906250\n",
      "Train Epoch: 184 [4352/17352 (25%)] Loss: -115187.531250\n",
      "Train Epoch: 184 [5760/17352 (33%)] Loss: -132976.484375\n",
      "Train Epoch: 184 [7168/17352 (41%)] Loss: -148931.421875\n",
      "Train Epoch: 184 [8576/17352 (49%)] Loss: -145191.000000\n",
      "Train Epoch: 184 [9984/17352 (58%)] Loss: -114612.765625\n",
      "Train Epoch: 184 [11392/17352 (66%)] Loss: -154448.156250\n",
      "Train Epoch: 184 [12800/17352 (74%)] Loss: -122504.250000\n",
      "Train Epoch: 184 [14208/17352 (82%)] Loss: -175530.500000\n",
      "Train Epoch: 184 [15552/17352 (90%)] Loss: -35185.472656\n",
      "Train Epoch: 184 [16383/17352 (94%)] Loss: -115062.781250\n",
      "Train Epoch: 184 [17160/17352 (99%)] Loss: -14082.126953\n",
      "    epoch          : 184\n",
      "    loss           : -113984.99841062815\n",
      "    val_loss       : -66303.03785807292\n",
      "Train Epoch: 185 [128/17352 (1%)] Loss: -151810.375000\n",
      "Train Epoch: 185 [1536/17352 (9%)] Loss: -81690.304688\n",
      "Train Epoch: 185 [2944/17352 (17%)] Loss: -103728.273438\n",
      "Train Epoch: 185 [4352/17352 (25%)] Loss: -114726.703125\n",
      "Train Epoch: 185 [5760/17352 (33%)] Loss: -90687.023438\n",
      "Train Epoch: 185 [7168/17352 (41%)] Loss: -92720.500000\n",
      "Train Epoch: 185 [8576/17352 (49%)] Loss: -141616.906250\n",
      "Train Epoch: 185 [9984/17352 (58%)] Loss: -151218.000000\n",
      "Train Epoch: 185 [11392/17352 (66%)] Loss: -145815.609375\n",
      "Train Epoch: 185 [12800/17352 (74%)] Loss: -135937.515625\n",
      "Train Epoch: 185 [14208/17352 (82%)] Loss: -100550.500000\n",
      "Train Epoch: 185 [15458/17352 (89%)] Loss: -91286.898438\n",
      "Train Epoch: 185 [16215/17352 (93%)] Loss: -45371.640625\n",
      "Train Epoch: 185 [16979/17352 (98%)] Loss: -104785.500000\n",
      "    epoch          : 185\n",
      "    loss           : -107475.36572593331\n",
      "    val_loss       : -55276.91215820312\n",
      "Train Epoch: 186 [128/17352 (1%)] Loss: -130599.812500\n",
      "Train Epoch: 186 [1536/17352 (9%)] Loss: -73448.015625\n",
      "Train Epoch: 186 [2944/17352 (17%)] Loss: -85001.546875\n",
      "Train Epoch: 186 [4352/17352 (25%)] Loss: -143430.437500\n",
      "Train Epoch: 186 [5760/17352 (33%)] Loss: -155011.875000\n",
      "Train Epoch: 186 [7168/17352 (41%)] Loss: -129436.281250\n",
      "Train Epoch: 186 [8576/17352 (49%)] Loss: -113394.093750\n",
      "Train Epoch: 186 [9984/17352 (58%)] Loss: -98973.875000\n",
      "Train Epoch: 186 [11392/17352 (66%)] Loss: -122731.437500\n",
      "Train Epoch: 186 [12800/17352 (74%)] Loss: -123386.312500\n",
      "Train Epoch: 186 [14208/17352 (82%)] Loss: -119975.742188\n",
      "Train Epoch: 186 [15460/17352 (89%)] Loss: -13858.251953\n",
      "Train Epoch: 186 [16220/17352 (93%)] Loss: -41688.718750\n",
      "Train Epoch: 186 [17026/17352 (98%)] Loss: -97843.718750\n",
      "    epoch          : 186\n",
      "    loss           : -113154.56239841129\n",
      "    val_loss       : -63774.44556070964\n",
      "Train Epoch: 187 [128/17352 (1%)] Loss: -109161.570312\n",
      "Train Epoch: 187 [1536/17352 (9%)] Loss: -132366.093750\n",
      "Train Epoch: 187 [2944/17352 (17%)] Loss: -109899.078125\n",
      "Train Epoch: 187 [4352/17352 (25%)] Loss: -128577.593750\n",
      "Train Epoch: 187 [5760/17352 (33%)] Loss: -120652.296875\n",
      "Train Epoch: 187 [7168/17352 (41%)] Loss: -118610.507812\n",
      "Train Epoch: 187 [8576/17352 (49%)] Loss: -111554.437500\n",
      "Train Epoch: 187 [9984/17352 (58%)] Loss: -100292.968750\n",
      "Train Epoch: 187 [11392/17352 (66%)] Loss: -113179.453125\n",
      "Train Epoch: 187 [12800/17352 (74%)] Loss: -82681.101562\n",
      "Train Epoch: 187 [14208/17352 (82%)] Loss: -162361.906250\n",
      "Train Epoch: 187 [15570/17352 (90%)] Loss: -119744.117188\n",
      "Train Epoch: 187 [16315/17352 (94%)] Loss: -94599.835938\n",
      "Train Epoch: 187 [17115/17352 (99%)] Loss: -66069.875000\n",
      "    epoch          : 187\n",
      "    loss           : -110411.01606084837\n",
      "    val_loss       : -63326.68135172526\n",
      "Train Epoch: 188 [128/17352 (1%)] Loss: -116108.531250\n",
      "Train Epoch: 188 [1536/17352 (9%)] Loss: -123871.242188\n",
      "Train Epoch: 188 [2944/17352 (17%)] Loss: -111672.890625\n",
      "Train Epoch: 188 [4352/17352 (25%)] Loss: -84416.437500\n",
      "Train Epoch: 188 [5760/17352 (33%)] Loss: -87952.976562\n",
      "Train Epoch: 188 [7168/17352 (41%)] Loss: -114072.960938\n",
      "Train Epoch: 188 [8576/17352 (49%)] Loss: -117928.179688\n",
      "Train Epoch: 188 [9984/17352 (58%)] Loss: -151456.437500\n",
      "Train Epoch: 188 [11392/17352 (66%)] Loss: -119270.062500\n",
      "Train Epoch: 188 [12800/17352 (74%)] Loss: -138216.062500\n",
      "Train Epoch: 188 [14208/17352 (82%)] Loss: -107907.265625\n",
      "Train Epoch: 188 [15533/17352 (90%)] Loss: -83851.656250\n",
      "Train Epoch: 188 [16361/17352 (94%)] Loss: -93697.085938\n",
      "Train Epoch: 188 [16936/17352 (98%)] Loss: -42760.984375\n",
      "    epoch          : 188\n",
      "    loss           : -108432.29485305684\n",
      "    val_loss       : -61504.507279459634\n",
      "Train Epoch: 189 [128/17352 (1%)] Loss: -130767.875000\n",
      "Train Epoch: 189 [1536/17352 (9%)] Loss: -120374.140625\n",
      "Train Epoch: 189 [2944/17352 (17%)] Loss: -76172.734375\n",
      "Train Epoch: 189 [4352/17352 (25%)] Loss: -92827.687500\n",
      "Train Epoch: 189 [5760/17352 (33%)] Loss: -72925.523438\n",
      "Train Epoch: 189 [7168/17352 (41%)] Loss: -104377.242188\n",
      "Train Epoch: 189 [8576/17352 (49%)] Loss: -141008.281250\n",
      "Train Epoch: 189 [9984/17352 (58%)] Loss: -92758.109375\n",
      "Train Epoch: 189 [11392/17352 (66%)] Loss: -105044.750000\n",
      "Train Epoch: 189 [12800/17352 (74%)] Loss: -108646.039062\n",
      "Train Epoch: 189 [14208/17352 (82%)] Loss: -130749.593750\n",
      "Train Epoch: 189 [15523/17352 (89%)] Loss: -86328.007812\n",
      "Train Epoch: 189 [16147/17352 (93%)] Loss: -3647.617676\n",
      "Train Epoch: 189 [16877/17352 (97%)] Loss: -52796.359375\n",
      "    epoch          : 189\n",
      "    loss           : -103476.79215276321\n",
      "    val_loss       : -61252.37503255208\n",
      "Train Epoch: 190 [128/17352 (1%)] Loss: -125024.000000\n",
      "Train Epoch: 190 [1536/17352 (9%)] Loss: -123230.593750\n",
      "Train Epoch: 190 [2944/17352 (17%)] Loss: -127930.296875\n",
      "Train Epoch: 190 [4352/17352 (25%)] Loss: -145538.921875\n",
      "Train Epoch: 190 [5760/17352 (33%)] Loss: -156211.984375\n",
      "Train Epoch: 190 [7168/17352 (41%)] Loss: -132872.375000\n",
      "Train Epoch: 190 [8576/17352 (49%)] Loss: -126005.234375\n",
      "Train Epoch: 190 [9984/17352 (58%)] Loss: -105367.156250\n",
      "Train Epoch: 190 [11392/17352 (66%)] Loss: -158736.781250\n",
      "Train Epoch: 190 [12800/17352 (74%)] Loss: -177609.984375\n",
      "Train Epoch: 190 [14208/17352 (82%)] Loss: -159408.593750\n",
      "Train Epoch: 190 [15465/17352 (89%)] Loss: -13028.669922\n",
      "Train Epoch: 190 [16214/17352 (93%)] Loss: -35143.078125\n",
      "Train Epoch: 190 [17047/17352 (98%)] Loss: -81788.375000\n",
      "    epoch          : 190\n",
      "    loss           : -112084.90750183516\n",
      "    val_loss       : -48568.22005615234\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch190.pth ...\n",
      "Train Epoch: 191 [128/17352 (1%)] Loss: -115439.382812\n",
      "Train Epoch: 191 [1536/17352 (9%)] Loss: -122343.179688\n",
      "Train Epoch: 191 [2944/17352 (17%)] Loss: -146918.875000\n",
      "Train Epoch: 191 [4352/17352 (25%)] Loss: -114470.187500\n",
      "Train Epoch: 191 [5760/17352 (33%)] Loss: -123392.195312\n",
      "Train Epoch: 191 [7168/17352 (41%)] Loss: -91435.195312\n",
      "Train Epoch: 191 [8576/17352 (49%)] Loss: -152788.687500\n",
      "Train Epoch: 191 [9984/17352 (58%)] Loss: -129110.015625\n",
      "Train Epoch: 191 [11392/17352 (66%)] Loss: -119731.468750\n",
      "Train Epoch: 191 [12800/17352 (74%)] Loss: -100142.796875\n",
      "Train Epoch: 191 [14208/17352 (82%)] Loss: -134211.578125\n",
      "Train Epoch: 191 [15423/17352 (89%)] Loss: -38948.179688\n",
      "Train Epoch: 191 [16219/17352 (93%)] Loss: -43809.949219\n",
      "Train Epoch: 191 [16972/17352 (98%)] Loss: -64535.707031\n",
      "    epoch          : 191\n",
      "    loss           : -105609.45891555684\n",
      "    val_loss       : -47328.00027669271\n",
      "Train Epoch: 192 [128/17352 (1%)] Loss: -108432.632812\n",
      "Train Epoch: 192 [1536/17352 (9%)] Loss: -90139.078125\n",
      "Train Epoch: 192 [2944/17352 (17%)] Loss: -108597.203125\n",
      "Train Epoch: 192 [4352/17352 (25%)] Loss: -102434.664062\n",
      "Train Epoch: 192 [5760/17352 (33%)] Loss: -134417.312500\n",
      "Train Epoch: 192 [7168/17352 (41%)] Loss: -137963.875000\n",
      "Train Epoch: 192 [8576/17352 (49%)] Loss: -131732.609375\n",
      "Train Epoch: 192 [9984/17352 (58%)] Loss: -128362.132812\n",
      "Train Epoch: 192 [11392/17352 (66%)] Loss: -106740.343750\n",
      "Train Epoch: 192 [12800/17352 (74%)] Loss: -137544.781250\n",
      "Train Epoch: 192 [14208/17352 (82%)] Loss: -120206.765625\n",
      "Train Epoch: 192 [15523/17352 (89%)] Loss: -48702.324219\n",
      "Train Epoch: 192 [16252/17352 (94%)] Loss: -120168.250000\n",
      "Train Epoch: 192 [16934/17352 (98%)] Loss: -120146.703125\n",
      "    epoch          : 192\n",
      "    loss           : -110474.68958748427\n",
      "    val_loss       : -64599.577640787764\n",
      "Train Epoch: 193 [128/17352 (1%)] Loss: -126276.882812\n",
      "Train Epoch: 193 [1536/17352 (9%)] Loss: -167845.203125\n",
      "Train Epoch: 193 [2944/17352 (17%)] Loss: -117237.984375\n",
      "Train Epoch: 193 [4352/17352 (25%)] Loss: -136151.406250\n",
      "Train Epoch: 193 [5760/17352 (33%)] Loss: -127664.000000\n",
      "Train Epoch: 193 [7168/17352 (41%)] Loss: -129674.406250\n",
      "Train Epoch: 193 [8576/17352 (49%)] Loss: -105877.000000\n",
      "Train Epoch: 193 [9984/17352 (58%)] Loss: -118258.664062\n",
      "Train Epoch: 193 [11392/17352 (66%)] Loss: -148922.171875\n",
      "Train Epoch: 193 [12800/17352 (74%)] Loss: -111015.140625\n",
      "Train Epoch: 193 [14208/17352 (82%)] Loss: -135498.734375\n",
      "Train Epoch: 193 [15560/17352 (90%)] Loss: -103612.414062\n",
      "Train Epoch: 193 [16233/17352 (94%)] Loss: -6052.792969\n",
      "Train Epoch: 193 [17040/17352 (98%)] Loss: -91336.335938\n",
      "    epoch          : 193\n",
      "    loss           : -116020.86737920775\n",
      "    val_loss       : -58676.073901367185\n",
      "Train Epoch: 194 [128/17352 (1%)] Loss: -105167.687500\n",
      "Train Epoch: 194 [1536/17352 (9%)] Loss: -152856.796875\n",
      "Train Epoch: 194 [2944/17352 (17%)] Loss: -151350.812500\n",
      "Train Epoch: 194 [4352/17352 (25%)] Loss: -93130.195312\n",
      "Train Epoch: 194 [5760/17352 (33%)] Loss: -131995.781250\n",
      "Train Epoch: 194 [7168/17352 (41%)] Loss: -147377.265625\n",
      "Train Epoch: 194 [8576/17352 (49%)] Loss: -125377.734375\n",
      "Train Epoch: 194 [9984/17352 (58%)] Loss: -117393.648438\n",
      "Train Epoch: 194 [11392/17352 (66%)] Loss: -138684.921875\n",
      "Train Epoch: 194 [12800/17352 (74%)] Loss: -147461.265625\n",
      "Train Epoch: 194 [14208/17352 (82%)] Loss: -135920.000000\n",
      "Train Epoch: 194 [15560/17352 (90%)] Loss: -80351.906250\n",
      "Train Epoch: 194 [16314/17352 (94%)] Loss: -96962.906250\n",
      "Train Epoch: 194 [17007/17352 (98%)] Loss: -72925.320312\n",
      "    epoch          : 194\n",
      "    loss           : -114367.30559852139\n",
      "    val_loss       : -61319.87110188802\n",
      "Train Epoch: 195 [128/17352 (1%)] Loss: -101754.984375\n",
      "Train Epoch: 195 [1536/17352 (9%)] Loss: -135761.203125\n",
      "Train Epoch: 195 [2944/17352 (17%)] Loss: -108953.937500\n",
      "Train Epoch: 195 [4352/17352 (25%)] Loss: -124410.445312\n",
      "Train Epoch: 195 [5760/17352 (33%)] Loss: -135478.500000\n",
      "Train Epoch: 195 [7168/17352 (41%)] Loss: -142031.531250\n",
      "Train Epoch: 195 [8576/17352 (49%)] Loss: -82038.484375\n",
      "Train Epoch: 195 [9984/17352 (58%)] Loss: -121273.500000\n",
      "Train Epoch: 195 [11392/17352 (66%)] Loss: -96505.656250\n",
      "Train Epoch: 195 [12800/17352 (74%)] Loss: -124501.492188\n",
      "Train Epoch: 195 [14208/17352 (82%)] Loss: -153466.968750\n",
      "Train Epoch: 195 [15531/17352 (90%)] Loss: -91311.531250\n",
      "Train Epoch: 195 [16228/17352 (94%)] Loss: -83461.390625\n",
      "Train Epoch: 195 [16965/17352 (98%)] Loss: -46890.437500\n",
      "    epoch          : 195\n",
      "    loss           : -106605.146307414\n",
      "    val_loss       : -61993.985668945315\n",
      "Train Epoch: 196 [128/17352 (1%)] Loss: -152081.812500\n",
      "Train Epoch: 196 [1536/17352 (9%)] Loss: -144804.140625\n",
      "Train Epoch: 196 [2944/17352 (17%)] Loss: -126997.640625\n",
      "Train Epoch: 196 [4352/17352 (25%)] Loss: -131004.890625\n",
      "Train Epoch: 196 [5760/17352 (33%)] Loss: -109059.320312\n",
      "Train Epoch: 196 [7168/17352 (41%)] Loss: -85823.765625\n",
      "Train Epoch: 196 [8576/17352 (49%)] Loss: -121893.023438\n",
      "Train Epoch: 196 [9984/17352 (58%)] Loss: -142405.625000\n",
      "Train Epoch: 196 [11392/17352 (66%)] Loss: -106793.132812\n",
      "Train Epoch: 196 [12800/17352 (74%)] Loss: -122475.867188\n",
      "Train Epoch: 196 [14208/17352 (82%)] Loss: -134469.187500\n",
      "Train Epoch: 196 [15582/17352 (90%)] Loss: -114317.828125\n",
      "Train Epoch: 196 [16245/17352 (94%)] Loss: -43391.492188\n",
      "Train Epoch: 196 [16978/17352 (98%)] Loss: -65556.218750\n",
      "    epoch          : 196\n",
      "    loss           : -112787.96245641516\n",
      "    val_loss       : -61351.23123779297\n",
      "Train Epoch: 197 [128/17352 (1%)] Loss: -132304.968750\n",
      "Train Epoch: 197 [1536/17352 (9%)] Loss: -137002.625000\n",
      "Train Epoch: 197 [2944/17352 (17%)] Loss: -114552.343750\n",
      "Train Epoch: 197 [4352/17352 (25%)] Loss: -130234.718750\n",
      "Train Epoch: 197 [5760/17352 (33%)] Loss: -97500.531250\n",
      "Train Epoch: 197 [7168/17352 (41%)] Loss: -128466.296875\n",
      "Train Epoch: 197 [8576/17352 (49%)] Loss: -77222.187500\n",
      "Train Epoch: 197 [9984/17352 (58%)] Loss: -116217.375000\n",
      "Train Epoch: 197 [11392/17352 (66%)] Loss: -83688.953125\n",
      "Train Epoch: 197 [12800/17352 (74%)] Loss: -129703.507812\n",
      "Train Epoch: 197 [14208/17352 (82%)] Loss: -95612.156250\n",
      "Train Epoch: 197 [15534/17352 (90%)] Loss: -79302.398438\n",
      "Train Epoch: 197 [16414/17352 (95%)] Loss: -91503.328125\n",
      "Train Epoch: 197 [17068/17352 (98%)] Loss: -2305.045410\n",
      "    epoch          : 197\n",
      "    loss           : -101982.57702718645\n",
      "    val_loss       : -47916.5836303711\n",
      "Train Epoch: 198 [128/17352 (1%)] Loss: -81256.117188\n",
      "Train Epoch: 198 [1536/17352 (9%)] Loss: -110703.421875\n",
      "Train Epoch: 198 [2944/17352 (17%)] Loss: -129339.570312\n",
      "Train Epoch: 198 [4352/17352 (25%)] Loss: -90340.437500\n",
      "Train Epoch: 198 [5760/17352 (33%)] Loss: -117101.812500\n",
      "Train Epoch: 198 [7168/17352 (41%)] Loss: -141313.750000\n",
      "Train Epoch: 198 [8576/17352 (49%)] Loss: -101606.250000\n",
      "Train Epoch: 198 [9984/17352 (58%)] Loss: -117198.382812\n",
      "Train Epoch: 198 [11392/17352 (66%)] Loss: -143082.046875\n",
      "Train Epoch: 198 [12800/17352 (74%)] Loss: -123609.195312\n",
      "Train Epoch: 198 [14208/17352 (82%)] Loss: -87372.664062\n",
      "Train Epoch: 198 [15368/17352 (89%)] Loss: -3845.971924\n",
      "Train Epoch: 198 [16080/17352 (93%)] Loss: -76367.656250\n",
      "Train Epoch: 198 [16918/17352 (97%)] Loss: -134536.421875\n",
      "    epoch          : 198\n",
      "    loss           : -112615.64383815278\n",
      "    val_loss       : -61273.36364746094\n",
      "Train Epoch: 199 [128/17352 (1%)] Loss: -113816.468750\n",
      "Train Epoch: 199 [1536/17352 (9%)] Loss: -157004.859375\n",
      "Train Epoch: 199 [2944/17352 (17%)] Loss: -148228.046875\n",
      "Train Epoch: 199 [4352/17352 (25%)] Loss: -110783.781250\n",
      "Train Epoch: 199 [5760/17352 (33%)] Loss: -157788.312500\n",
      "Train Epoch: 199 [7168/17352 (41%)] Loss: -143311.671875\n",
      "Train Epoch: 199 [8576/17352 (49%)] Loss: -134442.359375\n",
      "Train Epoch: 199 [9984/17352 (58%)] Loss: -144675.656250\n",
      "Train Epoch: 199 [11392/17352 (66%)] Loss: -156258.640625\n",
      "Train Epoch: 199 [12800/17352 (74%)] Loss: -121222.085938\n",
      "Train Epoch: 199 [14208/17352 (82%)] Loss: -170516.437500\n",
      "Train Epoch: 199 [15406/17352 (89%)] Loss: -3090.462402\n",
      "Train Epoch: 199 [16185/17352 (93%)] Loss: -96802.335938\n",
      "Train Epoch: 199 [17019/17352 (98%)] Loss: -105690.898438\n",
      "    epoch          : 199\n",
      "    loss           : -114848.97694263843\n",
      "    val_loss       : -64920.4002726237\n",
      "Train Epoch: 200 [128/17352 (1%)] Loss: -135882.015625\n",
      "Train Epoch: 200 [1536/17352 (9%)] Loss: -112244.937500\n",
      "Train Epoch: 200 [2944/17352 (17%)] Loss: -154650.671875\n",
      "Train Epoch: 200 [4352/17352 (25%)] Loss: -150136.531250\n",
      "Train Epoch: 200 [5760/17352 (33%)] Loss: -116033.734375\n",
      "Train Epoch: 200 [7168/17352 (41%)] Loss: -170626.968750\n",
      "Train Epoch: 200 [8576/17352 (49%)] Loss: -165082.421875\n",
      "Train Epoch: 200 [9984/17352 (58%)] Loss: -111704.765625\n",
      "Train Epoch: 200 [11392/17352 (66%)] Loss: -104164.890625\n",
      "Train Epoch: 200 [12800/17352 (74%)] Loss: -121146.679688\n",
      "Train Epoch: 200 [14208/17352 (82%)] Loss: -127982.859375\n",
      "Train Epoch: 200 [15564/17352 (90%)] Loss: -118386.812500\n",
      "Train Epoch: 200 [16278/17352 (94%)] Loss: -26464.132812\n",
      "Train Epoch: 200 [16979/17352 (98%)] Loss: -101040.718750\n",
      "    epoch          : 200\n",
      "    loss           : -110570.87180159397\n",
      "    val_loss       : -59476.20626627604\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [128/17352 (1%)] Loss: -137970.843750\n",
      "Train Epoch: 201 [1536/17352 (9%)] Loss: -151835.828125\n",
      "Train Epoch: 201 [2944/17352 (17%)] Loss: -106289.843750\n",
      "Train Epoch: 201 [4352/17352 (25%)] Loss: -107419.250000\n",
      "Train Epoch: 201 [5760/17352 (33%)] Loss: -119569.500000\n",
      "Train Epoch: 201 [7168/17352 (41%)] Loss: -115348.726562\n",
      "Train Epoch: 201 [8576/17352 (49%)] Loss: -110223.171875\n",
      "Train Epoch: 201 [9984/17352 (58%)] Loss: -102666.843750\n",
      "Train Epoch: 201 [11392/17352 (66%)] Loss: -109337.882812\n",
      "Train Epoch: 201 [12800/17352 (74%)] Loss: -105319.304688\n",
      "Train Epoch: 201 [14208/17352 (82%)] Loss: -118671.398438\n",
      "Train Epoch: 201 [15491/17352 (89%)] Loss: -50131.500000\n",
      "Train Epoch: 201 [16309/17352 (94%)] Loss: -91932.656250\n",
      "Train Epoch: 201 [17079/17352 (98%)] Loss: -95358.132812\n",
      "    epoch          : 201\n",
      "    loss           : -111050.08519524695\n",
      "    val_loss       : -64992.41632080078\n",
      "Train Epoch: 202 [128/17352 (1%)] Loss: -134772.609375\n",
      "Train Epoch: 202 [1536/17352 (9%)] Loss: -141241.281250\n",
      "Train Epoch: 202 [2944/17352 (17%)] Loss: -94667.070312\n",
      "Train Epoch: 202 [4352/17352 (25%)] Loss: -94354.953125\n",
      "Train Epoch: 202 [5760/17352 (33%)] Loss: -164798.484375\n",
      "Train Epoch: 202 [7168/17352 (41%)] Loss: -101990.085938\n",
      "Train Epoch: 202 [8576/17352 (49%)] Loss: -82316.515625\n",
      "Train Epoch: 202 [9984/17352 (58%)] Loss: -145901.125000\n",
      "Train Epoch: 202 [11392/17352 (66%)] Loss: -152583.343750\n",
      "Train Epoch: 202 [12800/17352 (74%)] Loss: -117253.562500\n",
      "Train Epoch: 202 [14208/17352 (82%)] Loss: -101156.312500\n",
      "Train Epoch: 202 [15580/17352 (90%)] Loss: -103072.562500\n",
      "Train Epoch: 202 [16332/17352 (94%)] Loss: -74388.656250\n",
      "Train Epoch: 202 [16940/17352 (98%)] Loss: -67825.703125\n",
      "    epoch          : 202\n",
      "    loss           : -108537.11169679373\n",
      "    val_loss       : -56847.60119222005\n",
      "Train Epoch: 203 [128/17352 (1%)] Loss: -103992.015625\n",
      "Train Epoch: 203 [1536/17352 (9%)] Loss: -75857.171875\n",
      "Train Epoch: 203 [2944/17352 (17%)] Loss: -128839.937500\n",
      "Train Epoch: 203 [4352/17352 (25%)] Loss: -140311.250000\n",
      "Train Epoch: 203 [5760/17352 (33%)] Loss: -127082.000000\n",
      "Train Epoch: 203 [7168/17352 (41%)] Loss: -114076.062500\n",
      "Train Epoch: 203 [8576/17352 (49%)] Loss: -148495.343750\n",
      "Train Epoch: 203 [9984/17352 (58%)] Loss: -132848.953125\n",
      "Train Epoch: 203 [11392/17352 (66%)] Loss: -102773.328125\n",
      "Train Epoch: 203 [12800/17352 (74%)] Loss: -150610.093750\n",
      "Train Epoch: 203 [14208/17352 (82%)] Loss: -160864.781250\n",
      "Train Epoch: 203 [15531/17352 (90%)] Loss: -80146.742188\n",
      "Train Epoch: 203 [16410/17352 (95%)] Loss: -72952.835938\n",
      "Train Epoch: 203 [17122/17352 (99%)] Loss: -88931.296875\n",
      "    epoch          : 203\n",
      "    loss           : -113743.67386417261\n",
      "    val_loss       : -62154.403515625\n",
      "Train Epoch: 204 [128/17352 (1%)] Loss: -163079.984375\n",
      "Train Epoch: 204 [1536/17352 (9%)] Loss: -119926.500000\n",
      "Train Epoch: 204 [2944/17352 (17%)] Loss: -116345.617188\n",
      "Train Epoch: 204 [4352/17352 (25%)] Loss: -120685.484375\n",
      "Train Epoch: 204 [5760/17352 (33%)] Loss: -100871.546875\n",
      "Train Epoch: 204 [7168/17352 (41%)] Loss: -170990.843750\n",
      "Train Epoch: 204 [8576/17352 (49%)] Loss: -151750.375000\n",
      "Train Epoch: 204 [9984/17352 (58%)] Loss: -154444.718750\n",
      "Train Epoch: 204 [11392/17352 (66%)] Loss: -159962.984375\n",
      "Train Epoch: 204 [12800/17352 (74%)] Loss: -105618.593750\n",
      "Train Epoch: 204 [14208/17352 (82%)] Loss: -139115.078125\n",
      "Train Epoch: 204 [15488/17352 (89%)] Loss: -41314.660156\n",
      "Train Epoch: 204 [16205/17352 (93%)] Loss: -48709.195312\n",
      "Train Epoch: 204 [17099/17352 (99%)] Loss: -68324.953125\n",
      "    epoch          : 204\n",
      "    loss           : -116806.2972420302\n",
      "    val_loss       : -62611.26284586589\n",
      "Train Epoch: 205 [128/17352 (1%)] Loss: -123989.101562\n",
      "Train Epoch: 205 [1536/17352 (9%)] Loss: -92196.593750\n",
      "Train Epoch: 205 [2944/17352 (17%)] Loss: -132733.984375\n",
      "Train Epoch: 205 [4352/17352 (25%)] Loss: -124013.656250\n",
      "Train Epoch: 205 [5760/17352 (33%)] Loss: -122485.125000\n",
      "Train Epoch: 205 [7168/17352 (41%)] Loss: -145938.375000\n",
      "Train Epoch: 205 [8576/17352 (49%)] Loss: -114688.859375\n",
      "Train Epoch: 205 [9984/17352 (58%)] Loss: -159666.703125\n",
      "Train Epoch: 205 [11392/17352 (66%)] Loss: -124911.890625\n",
      "Train Epoch: 205 [12800/17352 (74%)] Loss: -125853.718750\n",
      "Train Epoch: 205 [14208/17352 (82%)] Loss: -146006.609375\n",
      "Train Epoch: 205 [15510/17352 (89%)] Loss: -80086.578125\n",
      "Train Epoch: 205 [16084/17352 (93%)] Loss: -3316.918945\n",
      "Train Epoch: 205 [17013/17352 (98%)] Loss: -123069.109375\n",
      "    epoch          : 205\n",
      "    loss           : -121313.57988150168\n",
      "    val_loss       : -62062.75869954427\n",
      "Train Epoch: 206 [128/17352 (1%)] Loss: -155694.687500\n",
      "Train Epoch: 206 [1536/17352 (9%)] Loss: -151325.296875\n",
      "Train Epoch: 206 [2944/17352 (17%)] Loss: -101106.296875\n",
      "Train Epoch: 206 [4352/17352 (25%)] Loss: -138286.593750\n",
      "Train Epoch: 206 [5760/17352 (33%)] Loss: -125616.156250\n",
      "Train Epoch: 206 [7168/17352 (41%)] Loss: -83816.218750\n",
      "Train Epoch: 206 [8576/17352 (49%)] Loss: -156731.671875\n",
      "Train Epoch: 206 [9984/17352 (58%)] Loss: -140809.062500\n",
      "Train Epoch: 206 [11392/17352 (66%)] Loss: -148077.796875\n",
      "Train Epoch: 206 [12800/17352 (74%)] Loss: -158152.375000\n",
      "Train Epoch: 206 [14208/17352 (82%)] Loss: -108290.046875\n",
      "Train Epoch: 206 [15534/17352 (90%)] Loss: -99044.601562\n",
      "Train Epoch: 206 [16287/17352 (94%)] Loss: -5805.673340\n",
      "Train Epoch: 206 [17070/17352 (98%)] Loss: -107988.648438\n",
      "    epoch          : 206\n",
      "    loss           : -115917.80108110057\n",
      "    val_loss       : -65889.2254313151\n",
      "Train Epoch: 207 [128/17352 (1%)] Loss: -114576.531250\n",
      "Train Epoch: 207 [1536/17352 (9%)] Loss: -144000.703125\n",
      "Train Epoch: 207 [2944/17352 (17%)] Loss: -131850.843750\n",
      "Train Epoch: 207 [4352/17352 (25%)] Loss: -116321.500000\n",
      "Train Epoch: 207 [5760/17352 (33%)] Loss: -117654.695312\n",
      "Train Epoch: 207 [7168/17352 (41%)] Loss: -141119.640625\n",
      "Train Epoch: 207 [8576/17352 (49%)] Loss: -137175.343750\n",
      "Train Epoch: 207 [9984/17352 (58%)] Loss: -119030.437500\n",
      "Train Epoch: 207 [11392/17352 (66%)] Loss: -99010.601562\n",
      "Train Epoch: 207 [12800/17352 (74%)] Loss: -155709.203125\n",
      "Train Epoch: 207 [14208/17352 (82%)] Loss: -109782.500000\n",
      "Train Epoch: 207 [15486/17352 (89%)] Loss: -121943.703125\n",
      "Train Epoch: 207 [16336/17352 (94%)] Loss: -80659.398438\n",
      "Train Epoch: 207 [16998/17352 (98%)] Loss: -85380.796875\n",
      "    epoch          : 207\n",
      "    loss           : -117340.76939852926\n",
      "    val_loss       : -62132.16752115885\n",
      "Train Epoch: 208 [128/17352 (1%)] Loss: -109481.734375\n",
      "Train Epoch: 208 [1536/17352 (9%)] Loss: -133037.375000\n",
      "Train Epoch: 208 [2944/17352 (17%)] Loss: -119193.914062\n",
      "Train Epoch: 208 [4352/17352 (25%)] Loss: -145810.031250\n",
      "Train Epoch: 208 [5760/17352 (33%)] Loss: -140631.125000\n",
      "Train Epoch: 208 [7168/17352 (41%)] Loss: -140941.656250\n",
      "Train Epoch: 208 [8576/17352 (49%)] Loss: -126415.500000\n",
      "Train Epoch: 208 [9984/17352 (58%)] Loss: -156952.171875\n",
      "Train Epoch: 208 [11392/17352 (66%)] Loss: -103179.718750\n",
      "Train Epoch: 208 [12800/17352 (74%)] Loss: -131783.015625\n",
      "Train Epoch: 208 [14208/17352 (82%)] Loss: -113750.500000\n",
      "Train Epoch: 208 [15522/17352 (89%)] Loss: -97042.406250\n",
      "Train Epoch: 208 [16321/17352 (94%)] Loss: -66582.062500\n",
      "Train Epoch: 208 [16997/17352 (98%)] Loss: -5948.697754\n",
      "    epoch          : 208\n",
      "    loss           : -114414.4148850409\n",
      "    val_loss       : -59716.934533691405\n",
      "Train Epoch: 209 [128/17352 (1%)] Loss: -128690.804688\n",
      "Train Epoch: 209 [1536/17352 (9%)] Loss: -117092.718750\n",
      "Train Epoch: 209 [2944/17352 (17%)] Loss: -98618.617188\n",
      "Train Epoch: 209 [4352/17352 (25%)] Loss: -113750.171875\n",
      "Train Epoch: 209 [5760/17352 (33%)] Loss: -102754.109375\n",
      "Train Epoch: 209 [7168/17352 (41%)] Loss: -133686.468750\n",
      "Train Epoch: 209 [8576/17352 (49%)] Loss: -59282.832031\n",
      "Train Epoch: 209 [9984/17352 (58%)] Loss: -88599.812500\n",
      "Train Epoch: 209 [11392/17352 (66%)] Loss: -158855.984375\n",
      "Train Epoch: 209 [12800/17352 (74%)] Loss: -122717.828125\n",
      "Train Epoch: 209 [14208/17352 (82%)] Loss: -108434.562500\n",
      "Train Epoch: 209 [15457/17352 (89%)] Loss: -79958.078125\n",
      "Train Epoch: 209 [16240/17352 (94%)] Loss: -4222.057617\n",
      "Train Epoch: 209 [17001/17352 (98%)] Loss: -31201.078125\n",
      "    epoch          : 209\n",
      "    loss           : -103387.09823792733\n",
      "    val_loss       : -23478.455139160156\n",
      "Train Epoch: 210 [128/17352 (1%)] Loss: -55195.406250\n",
      "Train Epoch: 210 [1536/17352 (9%)] Loss: -82380.132812\n",
      "Train Epoch: 210 [2944/17352 (17%)] Loss: -87941.625000\n",
      "Train Epoch: 210 [4352/17352 (25%)] Loss: -95932.773438\n",
      "Train Epoch: 210 [5760/17352 (33%)] Loss: -102485.718750\n",
      "Train Epoch: 210 [7168/17352 (41%)] Loss: -132559.625000\n",
      "Train Epoch: 210 [8576/17352 (49%)] Loss: -63401.605469\n",
      "Train Epoch: 210 [9984/17352 (58%)] Loss: -83045.867188\n",
      "Train Epoch: 210 [11392/17352 (66%)] Loss: -123060.968750\n",
      "Train Epoch: 210 [12800/17352 (74%)] Loss: -118050.093750\n",
      "Train Epoch: 210 [14208/17352 (82%)] Loss: -127921.914062\n",
      "Train Epoch: 210 [15519/17352 (89%)] Loss: -97209.367188\n",
      "Train Epoch: 210 [16359/17352 (94%)] Loss: -61943.164062\n",
      "Train Epoch: 210 [17044/17352 (98%)] Loss: -44784.343750\n",
      "    epoch          : 210\n",
      "    loss           : -107789.95707057466\n",
      "    val_loss       : -62914.36992594401\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch210.pth ...\n",
      "Train Epoch: 211 [128/17352 (1%)] Loss: -124613.664062\n",
      "Train Epoch: 211 [1536/17352 (9%)] Loss: -124743.546875\n",
      "Train Epoch: 211 [2944/17352 (17%)] Loss: -153175.078125\n",
      "Train Epoch: 211 [4352/17352 (25%)] Loss: -69476.539062\n",
      "Train Epoch: 211 [5760/17352 (33%)] Loss: -97559.921875\n",
      "Train Epoch: 211 [7168/17352 (41%)] Loss: -86839.171875\n",
      "Train Epoch: 211 [8576/17352 (49%)] Loss: -132373.828125\n",
      "Train Epoch: 211 [9984/17352 (58%)] Loss: -138985.171875\n",
      "Train Epoch: 211 [11392/17352 (66%)] Loss: -154713.312500\n",
      "Train Epoch: 211 [12800/17352 (74%)] Loss: -89316.937500\n",
      "Train Epoch: 211 [14208/17352 (82%)] Loss: -138594.437500\n",
      "Train Epoch: 211 [15505/17352 (89%)] Loss: -56270.882812\n",
      "Train Epoch: 211 [16266/17352 (94%)] Loss: -92135.171875\n",
      "Train Epoch: 211 [17020/17352 (98%)] Loss: -20883.824219\n",
      "    epoch          : 211\n",
      "    loss           : -111647.50009339608\n",
      "    val_loss       : -40967.1624593099\n",
      "Train Epoch: 212 [128/17352 (1%)] Loss: -97748.515625\n",
      "Train Epoch: 212 [1536/17352 (9%)] Loss: -149154.250000\n",
      "Train Epoch: 212 [2944/17352 (17%)] Loss: -136238.515625\n",
      "Train Epoch: 212 [4352/17352 (25%)] Loss: -134697.343750\n",
      "Train Epoch: 212 [5760/17352 (33%)] Loss: -148022.500000\n",
      "Train Epoch: 212 [7168/17352 (41%)] Loss: -104064.781250\n",
      "Train Epoch: 212 [8576/17352 (49%)] Loss: -142095.453125\n",
      "Train Epoch: 212 [9984/17352 (58%)] Loss: -142175.187500\n",
      "Train Epoch: 212 [11392/17352 (66%)] Loss: -126911.796875\n",
      "Train Epoch: 212 [12800/17352 (74%)] Loss: -110998.382812\n",
      "Train Epoch: 212 [14208/17352 (82%)] Loss: -119107.304688\n",
      "Train Epoch: 212 [15448/17352 (89%)] Loss: -3382.149658\n",
      "Train Epoch: 212 [16113/17352 (93%)] Loss: -41004.742188\n",
      "Train Epoch: 212 [16938/17352 (98%)] Loss: -38846.609375\n",
      "    epoch          : 212\n",
      "    loss           : -113206.83875740615\n",
      "    val_loss       : -65009.69686279297\n",
      "Train Epoch: 213 [128/17352 (1%)] Loss: -99106.695312\n",
      "Train Epoch: 213 [1536/17352 (9%)] Loss: -147864.968750\n",
      "Train Epoch: 213 [2944/17352 (17%)] Loss: -162594.531250\n",
      "Train Epoch: 213 [4352/17352 (25%)] Loss: -72312.070312\n",
      "Train Epoch: 213 [5760/17352 (33%)] Loss: -105832.039062\n",
      "Train Epoch: 213 [7168/17352 (41%)] Loss: -120383.609375\n",
      "Train Epoch: 213 [8576/17352 (49%)] Loss: -107411.429688\n",
      "Train Epoch: 213 [9984/17352 (58%)] Loss: -112778.765625\n",
      "Train Epoch: 213 [11392/17352 (66%)] Loss: -110457.671875\n",
      "Train Epoch: 213 [12800/17352 (74%)] Loss: -127295.921875\n",
      "Train Epoch: 213 [14208/17352 (82%)] Loss: -154036.171875\n",
      "Train Epoch: 213 [15459/17352 (89%)] Loss: -65909.304688\n",
      "Train Epoch: 213 [16291/17352 (94%)] Loss: -34375.812500\n",
      "Train Epoch: 213 [17064/17352 (98%)] Loss: -94791.937500\n",
      "    epoch          : 213\n",
      "    loss           : -110381.2698097997\n",
      "    val_loss       : -61841.2957804362\n",
      "Train Epoch: 214 [128/17352 (1%)] Loss: -115797.562500\n",
      "Train Epoch: 214 [1536/17352 (9%)] Loss: -165651.828125\n",
      "Train Epoch: 214 [2944/17352 (17%)] Loss: -109648.828125\n",
      "Train Epoch: 214 [4352/17352 (25%)] Loss: -104470.492188\n",
      "Train Epoch: 214 [5760/17352 (33%)] Loss: -104149.273438\n",
      "Train Epoch: 214 [7168/17352 (41%)] Loss: -93351.812500\n",
      "Train Epoch: 214 [8576/17352 (49%)] Loss: -114734.195312\n",
      "Train Epoch: 214 [9984/17352 (58%)] Loss: -142484.484375\n",
      "Train Epoch: 214 [11392/17352 (66%)] Loss: -129319.015625\n",
      "Train Epoch: 214 [12800/17352 (74%)] Loss: -154671.109375\n",
      "Train Epoch: 214 [14208/17352 (82%)] Loss: -167207.515625\n",
      "Train Epoch: 214 [15487/17352 (89%)] Loss: -80303.804688\n",
      "Train Epoch: 214 [16236/17352 (94%)] Loss: -32274.990234\n",
      "Train Epoch: 214 [16890/17352 (97%)] Loss: -7922.788086\n",
      "    epoch          : 214\n",
      "    loss           : -107082.68619163564\n",
      "    val_loss       : -11168.776684570312\n",
      "Train Epoch: 215 [128/17352 (1%)] Loss: -33238.804688\n",
      "Train Epoch: 215 [1536/17352 (9%)] Loss: -72720.023438\n",
      "Train Epoch: 215 [2944/17352 (17%)] Loss: -99904.828125\n",
      "Train Epoch: 215 [4352/17352 (25%)] Loss: -117329.835938\n",
      "Train Epoch: 215 [5760/17352 (33%)] Loss: -119635.718750\n",
      "Train Epoch: 215 [7168/17352 (41%)] Loss: -79147.265625\n",
      "Train Epoch: 215 [8576/17352 (49%)] Loss: -121362.750000\n",
      "Train Epoch: 215 [9984/17352 (58%)] Loss: -94230.296875\n",
      "Train Epoch: 215 [11392/17352 (66%)] Loss: -132860.500000\n",
      "Train Epoch: 215 [12800/17352 (74%)] Loss: -77582.242188\n",
      "Train Epoch: 215 [14208/17352 (82%)] Loss: -138824.984375\n",
      "Train Epoch: 215 [15513/17352 (89%)] Loss: -77350.429688\n",
      "Train Epoch: 215 [16255/17352 (94%)] Loss: -41378.402344\n",
      "Train Epoch: 215 [17032/17352 (98%)] Loss: -79544.687500\n",
      "    epoch          : 215\n",
      "    loss           : -101583.36338939282\n",
      "    val_loss       : -62633.49794514974\n",
      "Train Epoch: 216 [128/17352 (1%)] Loss: -148155.859375\n",
      "Train Epoch: 216 [1536/17352 (9%)] Loss: -122240.343750\n",
      "Train Epoch: 216 [2944/17352 (17%)] Loss: -114027.109375\n",
      "Train Epoch: 216 [4352/17352 (25%)] Loss: -116652.421875\n",
      "Train Epoch: 216 [5760/17352 (33%)] Loss: -127191.000000\n",
      "Train Epoch: 216 [7168/17352 (41%)] Loss: -83334.453125\n",
      "Train Epoch: 216 [8576/17352 (49%)] Loss: -117681.601562\n",
      "Train Epoch: 216 [9984/17352 (58%)] Loss: -115540.210938\n",
      "Train Epoch: 216 [11392/17352 (66%)] Loss: -127174.406250\n",
      "Train Epoch: 216 [12800/17352 (74%)] Loss: -97499.210938\n",
      "Train Epoch: 216 [14208/17352 (82%)] Loss: -115628.390625\n",
      "Train Epoch: 216 [15533/17352 (90%)] Loss: -89302.179688\n",
      "Train Epoch: 216 [16214/17352 (93%)] Loss: -29719.482422\n",
      "Train Epoch: 216 [16950/17352 (98%)] Loss: -85902.984375\n",
      "    epoch          : 216\n",
      "    loss           : -115391.01098632812\n",
      "    val_loss       : -54461.76444498698\n",
      "Train Epoch: 217 [128/17352 (1%)] Loss: -92793.796875\n",
      "Train Epoch: 217 [1536/17352 (9%)] Loss: -101471.406250\n",
      "Train Epoch: 217 [2944/17352 (17%)] Loss: -98961.710938\n",
      "Train Epoch: 217 [4352/17352 (25%)] Loss: -89157.648438\n",
      "Train Epoch: 217 [5760/17352 (33%)] Loss: -142073.578125\n",
      "Train Epoch: 217 [7168/17352 (41%)] Loss: -97403.515625\n",
      "Train Epoch: 217 [8576/17352 (49%)] Loss: -106309.242188\n",
      "Train Epoch: 217 [9984/17352 (58%)] Loss: -127164.296875\n",
      "Train Epoch: 217 [11392/17352 (66%)] Loss: -152409.625000\n",
      "Train Epoch: 217 [12800/17352 (74%)] Loss: -127878.929688\n",
      "Train Epoch: 217 [14208/17352 (82%)] Loss: -147677.187500\n",
      "Train Epoch: 217 [15489/17352 (89%)] Loss: -66140.492188\n",
      "Train Epoch: 217 [16220/17352 (93%)] Loss: -98254.109375\n",
      "Train Epoch: 217 [17025/17352 (98%)] Loss: -44029.496094\n",
      "    epoch          : 217\n",
      "    loss           : -109401.98229406984\n",
      "    val_loss       : -41120.56342773438\n",
      "Train Epoch: 218 [128/17352 (1%)] Loss: -77581.625000\n",
      "Train Epoch: 218 [1536/17352 (9%)] Loss: -95843.953125\n",
      "Train Epoch: 218 [2944/17352 (17%)] Loss: -103510.507812\n",
      "Train Epoch: 218 [4352/17352 (25%)] Loss: -162345.687500\n",
      "Train Epoch: 218 [5760/17352 (33%)] Loss: -86016.718750\n",
      "Train Epoch: 218 [7168/17352 (41%)] Loss: -131369.625000\n",
      "Train Epoch: 218 [8576/17352 (49%)] Loss: -141802.593750\n",
      "Train Epoch: 218 [9984/17352 (58%)] Loss: -138574.265625\n",
      "Train Epoch: 218 [11392/17352 (66%)] Loss: -125155.703125\n",
      "Train Epoch: 218 [12800/17352 (74%)] Loss: -153683.140625\n",
      "Train Epoch: 218 [14208/17352 (82%)] Loss: -139745.718750\n",
      "Train Epoch: 218 [15519/17352 (89%)] Loss: -78931.523438\n",
      "Train Epoch: 218 [16149/17352 (93%)] Loss: -71493.304688\n",
      "Train Epoch: 218 [16921/17352 (98%)] Loss: -10465.917969\n",
      "    epoch          : 218\n",
      "    loss           : -104848.56703216757\n",
      "    val_loss       : -56180.20261230469\n",
      "Train Epoch: 219 [128/17352 (1%)] Loss: -56738.437500\n",
      "Train Epoch: 219 [1536/17352 (9%)] Loss: -98708.906250\n",
      "Train Epoch: 219 [2944/17352 (17%)] Loss: -145235.015625\n",
      "Train Epoch: 219 [4352/17352 (25%)] Loss: -117810.468750\n",
      "Train Epoch: 219 [5760/17352 (33%)] Loss: -143988.125000\n",
      "Train Epoch: 219 [7168/17352 (41%)] Loss: -112831.445312\n",
      "Train Epoch: 219 [8576/17352 (49%)] Loss: -143117.000000\n",
      "Train Epoch: 219 [9984/17352 (58%)] Loss: -168410.968750\n",
      "Train Epoch: 219 [11392/17352 (66%)] Loss: -104390.546875\n",
      "Train Epoch: 219 [12800/17352 (74%)] Loss: -105831.515625\n",
      "Train Epoch: 219 [14208/17352 (82%)] Loss: -129821.671875\n",
      "Train Epoch: 219 [15451/17352 (89%)] Loss: -5011.094238\n",
      "Train Epoch: 219 [16176/17352 (93%)] Loss: -11186.079102\n",
      "Train Epoch: 219 [16894/17352 (97%)] Loss: -39735.160156\n",
      "    epoch          : 219\n",
      "    loss           : -114401.45558934564\n",
      "    val_loss       : -62441.3866780599\n",
      "Train Epoch: 220 [128/17352 (1%)] Loss: -123739.218750\n",
      "Train Epoch: 220 [1536/17352 (9%)] Loss: -172461.375000\n",
      "Train Epoch: 220 [2944/17352 (17%)] Loss: -131318.625000\n",
      "Train Epoch: 220 [4352/17352 (25%)] Loss: -138173.421875\n",
      "Train Epoch: 220 [5760/17352 (33%)] Loss: -111448.625000\n",
      "Train Epoch: 220 [7168/17352 (41%)] Loss: -94596.789062\n",
      "Train Epoch: 220 [8576/17352 (49%)] Loss: -112087.078125\n",
      "Train Epoch: 220 [9984/17352 (58%)] Loss: -137360.125000\n",
      "Train Epoch: 220 [11392/17352 (66%)] Loss: -114840.351562\n",
      "Train Epoch: 220 [12800/17352 (74%)] Loss: -148907.234375\n",
      "Train Epoch: 220 [14208/17352 (82%)] Loss: -110738.843750\n",
      "Train Epoch: 220 [15448/17352 (89%)] Loss: -74021.859375\n",
      "Train Epoch: 220 [16235/17352 (94%)] Loss: -129480.328125\n",
      "Train Epoch: 220 [16909/17352 (97%)] Loss: -39187.304688\n",
      "    epoch          : 220\n",
      "    loss           : -117665.31031911966\n",
      "    val_loss       : -65019.00629069011\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch220.pth ...\n",
      "Train Epoch: 221 [128/17352 (1%)] Loss: -150638.140625\n",
      "Train Epoch: 221 [1536/17352 (9%)] Loss: -120232.742188\n",
      "Train Epoch: 221 [2944/17352 (17%)] Loss: -137756.250000\n",
      "Train Epoch: 221 [4352/17352 (25%)] Loss: -140805.312500\n",
      "Train Epoch: 221 [5760/17352 (33%)] Loss: -136866.546875\n",
      "Train Epoch: 221 [7168/17352 (41%)] Loss: -133409.703125\n",
      "Train Epoch: 221 [8576/17352 (49%)] Loss: -128693.234375\n",
      "Train Epoch: 221 [9984/17352 (58%)] Loss: -122973.421875\n",
      "Train Epoch: 221 [11392/17352 (66%)] Loss: -106060.867188\n",
      "Train Epoch: 221 [12800/17352 (74%)] Loss: -145475.437500\n",
      "Train Epoch: 221 [14208/17352 (82%)] Loss: -110181.539062\n",
      "Train Epoch: 221 [15406/17352 (89%)] Loss: -2822.299072\n",
      "Train Epoch: 221 [16149/17352 (93%)] Loss: -35035.140625\n",
      "Train Epoch: 221 [16890/17352 (97%)] Loss: -68554.609375\n",
      "    epoch          : 221\n",
      "    loss           : -120234.18810461671\n",
      "    val_loss       : -68537.56735432943\n",
      "Train Epoch: 222 [128/17352 (1%)] Loss: -99963.515625\n",
      "Train Epoch: 222 [1536/17352 (9%)] Loss: -160457.421875\n",
      "Train Epoch: 222 [2944/17352 (17%)] Loss: -101965.968750\n",
      "Train Epoch: 222 [4352/17352 (25%)] Loss: -157343.828125\n",
      "Train Epoch: 222 [5760/17352 (33%)] Loss: -130350.687500\n",
      "Train Epoch: 222 [7168/17352 (41%)] Loss: -141908.437500\n",
      "Train Epoch: 222 [8576/17352 (49%)] Loss: -118399.250000\n",
      "Train Epoch: 222 [9984/17352 (58%)] Loss: -92974.007812\n",
      "Train Epoch: 222 [11392/17352 (66%)] Loss: -144590.515625\n",
      "Train Epoch: 222 [12800/17352 (74%)] Loss: -125906.796875\n",
      "Train Epoch: 222 [14208/17352 (82%)] Loss: -117775.125000\n",
      "Train Epoch: 222 [15525/17352 (89%)] Loss: -67275.585938\n",
      "Train Epoch: 222 [16212/17352 (93%)] Loss: -71700.257812\n",
      "Train Epoch: 222 [16945/17352 (98%)] Loss: 28664.132812\n",
      "    epoch          : 222\n",
      "    loss           : -110484.0316620897\n",
      "    val_loss       : -22773.667346191407\n",
      "Train Epoch: 223 [128/17352 (1%)] Loss: -38972.527344\n",
      "Train Epoch: 223 [1536/17352 (9%)] Loss: -54919.046875\n",
      "Train Epoch: 223 [2944/17352 (17%)] Loss: -93250.476562\n",
      "Train Epoch: 223 [4352/17352 (25%)] Loss: -76154.000000\n",
      "Train Epoch: 223 [5760/17352 (33%)] Loss: -123902.695312\n",
      "Train Epoch: 223 [7168/17352 (41%)] Loss: -105953.117188\n",
      "Train Epoch: 223 [8576/17352 (49%)] Loss: -103394.593750\n",
      "Train Epoch: 223 [9984/17352 (58%)] Loss: -111327.609375\n",
      "Train Epoch: 223 [11392/17352 (66%)] Loss: -79580.609375\n",
      "Train Epoch: 223 [12800/17352 (74%)] Loss: -138016.203125\n",
      "Train Epoch: 223 [14208/17352 (82%)] Loss: -120589.070312\n",
      "Train Epoch: 223 [15482/17352 (89%)] Loss: -41971.953125\n",
      "Train Epoch: 223 [16445/17352 (95%)] Loss: -132906.687500\n",
      "Train Epoch: 223 [17016/17352 (98%)] Loss: -80313.781250\n",
      "    epoch          : 223\n",
      "    loss           : -95155.7014594366\n",
      "    val_loss       : -65203.44326171875\n",
      "Train Epoch: 224 [128/17352 (1%)] Loss: -140890.812500\n",
      "Train Epoch: 224 [1536/17352 (9%)] Loss: -126221.359375\n",
      "Train Epoch: 224 [2944/17352 (17%)] Loss: -152353.968750\n",
      "Train Epoch: 224 [4352/17352 (25%)] Loss: -110322.460938\n",
      "Train Epoch: 224 [5760/17352 (33%)] Loss: -91098.398438\n",
      "Train Epoch: 224 [7168/17352 (41%)] Loss: -139683.265625\n",
      "Train Epoch: 224 [8576/17352 (49%)] Loss: -65501.828125\n",
      "Train Epoch: 224 [9984/17352 (58%)] Loss: -167922.578125\n",
      "Train Epoch: 224 [11392/17352 (66%)] Loss: -108726.656250\n",
      "Train Epoch: 224 [12800/17352 (74%)] Loss: -106400.195312\n",
      "Train Epoch: 224 [14208/17352 (82%)] Loss: -134793.687500\n",
      "Train Epoch: 224 [15487/17352 (89%)] Loss: -60145.031250\n",
      "Train Epoch: 224 [16314/17352 (94%)] Loss: -75969.492188\n",
      "Train Epoch: 224 [17076/17352 (98%)] Loss: -14879.061523\n",
      "    epoch          : 224\n",
      "    loss           : -112197.58595388528\n",
      "    val_loss       : -63805.17469889323\n",
      "Train Epoch: 225 [128/17352 (1%)] Loss: -130848.250000\n",
      "Train Epoch: 225 [1536/17352 (9%)] Loss: -133674.828125\n",
      "Train Epoch: 225 [2944/17352 (17%)] Loss: -137749.609375\n",
      "Train Epoch: 225 [4352/17352 (25%)] Loss: -134480.656250\n",
      "Train Epoch: 225 [5760/17352 (33%)] Loss: -115839.890625\n",
      "Train Epoch: 225 [7168/17352 (41%)] Loss: -111588.460938\n",
      "Train Epoch: 225 [8576/17352 (49%)] Loss: -96325.968750\n",
      "Train Epoch: 225 [9984/17352 (58%)] Loss: -147875.968750\n",
      "Train Epoch: 225 [11392/17352 (66%)] Loss: -147622.203125\n",
      "Train Epoch: 225 [12800/17352 (74%)] Loss: -133959.656250\n",
      "Train Epoch: 225 [14208/17352 (82%)] Loss: -133839.671875\n",
      "Train Epoch: 225 [15416/17352 (89%)] Loss: -39048.304688\n",
      "Train Epoch: 225 [16158/17352 (93%)] Loss: -27177.109375\n",
      "Train Epoch: 225 [17030/17352 (98%)] Loss: -52360.132812\n",
      "    epoch          : 225\n",
      "    loss           : -117076.52981464975\n",
      "    val_loss       : -59946.2956258138\n",
      "Train Epoch: 226 [128/17352 (1%)] Loss: -111529.734375\n",
      "Train Epoch: 226 [1536/17352 (9%)] Loss: -136609.968750\n",
      "Train Epoch: 226 [2944/17352 (17%)] Loss: -136126.125000\n",
      "Train Epoch: 226 [4352/17352 (25%)] Loss: -109530.132812\n",
      "Train Epoch: 226 [5760/17352 (33%)] Loss: -141323.156250\n",
      "Train Epoch: 226 [7168/17352 (41%)] Loss: -141625.687500\n",
      "Train Epoch: 226 [8576/17352 (49%)] Loss: -132680.625000\n",
      "Train Epoch: 226 [9984/17352 (58%)] Loss: -118869.117188\n",
      "Train Epoch: 226 [11392/17352 (66%)] Loss: -158894.125000\n",
      "Train Epoch: 226 [12800/17352 (74%)] Loss: -145007.734375\n",
      "Train Epoch: 226 [14208/17352 (82%)] Loss: -181219.593750\n",
      "Train Epoch: 226 [15551/17352 (90%)] Loss: -58036.363281\n",
      "Train Epoch: 226 [16342/17352 (94%)] Loss: -91023.210938\n",
      "Train Epoch: 226 [17005/17352 (98%)] Loss: -134032.390625\n",
      "    epoch          : 226\n",
      "    loss           : -117236.89472131922\n",
      "    val_loss       : -59235.2693359375\n",
      "Train Epoch: 227 [128/17352 (1%)] Loss: -109303.609375\n",
      "Train Epoch: 227 [1536/17352 (9%)] Loss: -128098.101562\n",
      "Train Epoch: 227 [2944/17352 (17%)] Loss: -130013.406250\n",
      "Train Epoch: 227 [4352/17352 (25%)] Loss: -155371.687500\n",
      "Train Epoch: 227 [5760/17352 (33%)] Loss: -134647.484375\n",
      "Train Epoch: 227 [7168/17352 (41%)] Loss: -129555.000000\n",
      "Train Epoch: 227 [8576/17352 (49%)] Loss: -111974.078125\n",
      "Train Epoch: 227 [9984/17352 (58%)] Loss: -137951.093750\n",
      "Train Epoch: 227 [11392/17352 (66%)] Loss: -144798.609375\n",
      "Train Epoch: 227 [12800/17352 (74%)] Loss: -135486.234375\n",
      "Train Epoch: 227 [14208/17352 (82%)] Loss: -137374.109375\n",
      "Train Epoch: 227 [15533/17352 (90%)] Loss: -91390.296875\n",
      "Train Epoch: 227 [16340/17352 (94%)] Loss: -77203.085938\n",
      "Train Epoch: 227 [17110/17352 (99%)] Loss: -115042.929688\n",
      "    epoch          : 227\n",
      "    loss           : -119781.42612370229\n",
      "    val_loss       : -64276.58496907552\n",
      "Train Epoch: 228 [128/17352 (1%)] Loss: -104564.539062\n",
      "Train Epoch: 228 [1536/17352 (9%)] Loss: -104116.984375\n",
      "Train Epoch: 228 [2944/17352 (17%)] Loss: -110431.492188\n",
      "Train Epoch: 228 [4352/17352 (25%)] Loss: -142853.750000\n",
      "Train Epoch: 228 [5760/17352 (33%)] Loss: -116954.453125\n",
      "Train Epoch: 228 [7168/17352 (41%)] Loss: -121832.046875\n",
      "Train Epoch: 228 [8576/17352 (49%)] Loss: -147058.343750\n",
      "Train Epoch: 228 [9984/17352 (58%)] Loss: -139343.296875\n",
      "Train Epoch: 228 [11392/17352 (66%)] Loss: -113888.640625\n",
      "Train Epoch: 228 [12800/17352 (74%)] Loss: -105650.695312\n",
      "Train Epoch: 228 [14208/17352 (82%)] Loss: -115543.421875\n",
      "Train Epoch: 228 [15523/17352 (89%)] Loss: -105781.234375\n",
      "Train Epoch: 228 [16152/17352 (93%)] Loss: -4741.718750\n",
      "Train Epoch: 228 [16955/17352 (98%)] Loss: -71143.859375\n",
      "    epoch          : 228\n",
      "    loss           : -114622.49931509543\n",
      "    val_loss       : -57349.66094156901\n",
      "Train Epoch: 229 [128/17352 (1%)] Loss: -151695.203125\n",
      "Train Epoch: 229 [1536/17352 (9%)] Loss: -127008.359375\n",
      "Train Epoch: 229 [2944/17352 (17%)] Loss: -146076.859375\n",
      "Train Epoch: 229 [4352/17352 (25%)] Loss: -140834.625000\n",
      "Train Epoch: 229 [5760/17352 (33%)] Loss: -142974.937500\n",
      "Train Epoch: 229 [7168/17352 (41%)] Loss: -149241.796875\n",
      "Train Epoch: 229 [8576/17352 (49%)] Loss: -69132.328125\n",
      "Train Epoch: 229 [9984/17352 (58%)] Loss: -122161.351562\n",
      "Train Epoch: 229 [11392/17352 (66%)] Loss: -154044.906250\n",
      "Train Epoch: 229 [12800/17352 (74%)] Loss: -136426.078125\n",
      "Train Epoch: 229 [14208/17352 (82%)] Loss: -155592.500000\n",
      "Train Epoch: 229 [15562/17352 (90%)] Loss: -159526.062500\n",
      "Train Epoch: 229 [16273/17352 (94%)] Loss: -29247.648438\n",
      "Train Epoch: 229 [17133/17352 (99%)] Loss: -118388.882812\n",
      "    epoch          : 229\n",
      "    loss           : -118939.87725133703\n",
      "    val_loss       : -66102.620703125\n",
      "Train Epoch: 230 [128/17352 (1%)] Loss: -122128.898438\n",
      "Train Epoch: 230 [1536/17352 (9%)] Loss: -113988.765625\n",
      "Train Epoch: 230 [2944/17352 (17%)] Loss: -107917.226562\n",
      "Train Epoch: 230 [4352/17352 (25%)] Loss: -136962.359375\n",
      "Train Epoch: 230 [5760/17352 (33%)] Loss: -146140.531250\n",
      "Train Epoch: 230 [7168/17352 (41%)] Loss: -102599.585938\n",
      "Train Epoch: 230 [8576/17352 (49%)] Loss: -115958.484375\n",
      "Train Epoch: 230 [9984/17352 (58%)] Loss: -73803.953125\n",
      "Train Epoch: 230 [11392/17352 (66%)] Loss: -95909.710938\n",
      "Train Epoch: 230 [12800/17352 (74%)] Loss: -108721.562500\n",
      "Train Epoch: 230 [14208/17352 (82%)] Loss: -130682.140625\n",
      "Train Epoch: 230 [15478/17352 (89%)] Loss: -37280.863281\n",
      "Train Epoch: 230 [16151/17352 (93%)] Loss: -51059.703125\n",
      "Train Epoch: 230 [16925/17352 (98%)] Loss: -119466.445312\n",
      "    epoch          : 230\n",
      "    loss           : -107338.13962877517\n",
      "    val_loss       : -63760.7609375\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch230.pth ...\n",
      "Train Epoch: 231 [128/17352 (1%)] Loss: -123750.828125\n",
      "Train Epoch: 231 [1536/17352 (9%)] Loss: -142643.375000\n",
      "Train Epoch: 231 [2944/17352 (17%)] Loss: -107291.890625\n",
      "Train Epoch: 231 [4352/17352 (25%)] Loss: -143203.015625\n",
      "Train Epoch: 231 [5760/17352 (33%)] Loss: -113764.585938\n",
      "Train Epoch: 231 [7168/17352 (41%)] Loss: -108138.296875\n",
      "Train Epoch: 231 [8576/17352 (49%)] Loss: -91507.085938\n",
      "Train Epoch: 231 [9984/17352 (58%)] Loss: -85931.132812\n",
      "Train Epoch: 231 [11392/17352 (66%)] Loss: -162594.562500\n",
      "Train Epoch: 231 [12800/17352 (74%)] Loss: -127032.468750\n",
      "Train Epoch: 231 [14208/17352 (82%)] Loss: -159015.875000\n",
      "Train Epoch: 231 [15565/17352 (90%)] Loss: -87713.750000\n",
      "Train Epoch: 231 [16383/17352 (94%)] Loss: -100475.171875\n",
      "Train Epoch: 231 [17140/17352 (99%)] Loss: -88337.515625\n",
      "    epoch          : 231\n",
      "    loss           : -120289.3001061766\n",
      "    val_loss       : -53137.14913736979\n",
      "Train Epoch: 232 [128/17352 (1%)] Loss: -133236.656250\n",
      "Train Epoch: 232 [1536/17352 (9%)] Loss: -132692.765625\n",
      "Train Epoch: 232 [2944/17352 (17%)] Loss: -116174.375000\n",
      "Train Epoch: 232 [4352/17352 (25%)] Loss: -120622.984375\n",
      "Train Epoch: 232 [5760/17352 (33%)] Loss: -115887.765625\n",
      "Train Epoch: 232 [7168/17352 (41%)] Loss: -100647.617188\n",
      "Train Epoch: 232 [8576/17352 (49%)] Loss: -98909.250000\n",
      "Train Epoch: 232 [9984/17352 (58%)] Loss: -129384.312500\n",
      "Train Epoch: 232 [11392/17352 (66%)] Loss: -122199.429688\n",
      "Train Epoch: 232 [12800/17352 (74%)] Loss: -121151.671875\n",
      "Train Epoch: 232 [14208/17352 (82%)] Loss: -125842.234375\n",
      "Train Epoch: 232 [15450/17352 (89%)] Loss: -104760.000000\n",
      "Train Epoch: 232 [16212/17352 (93%)] Loss: -140819.812500\n",
      "Train Epoch: 232 [17072/17352 (98%)] Loss: -69939.109375\n",
      "    epoch          : 232\n",
      "    loss           : -116294.59627988674\n",
      "    val_loss       : -56638.60307617187\n",
      "Train Epoch: 233 [128/17352 (1%)] Loss: -116052.078125\n",
      "Train Epoch: 233 [1536/17352 (9%)] Loss: -120622.007812\n",
      "Train Epoch: 233 [2944/17352 (17%)] Loss: -131206.500000\n",
      "Train Epoch: 233 [4352/17352 (25%)] Loss: -129013.562500\n",
      "Train Epoch: 233 [5760/17352 (33%)] Loss: -139570.281250\n",
      "Train Epoch: 233 [7168/17352 (41%)] Loss: -89453.742188\n",
      "Train Epoch: 233 [8576/17352 (49%)] Loss: -124755.437500\n",
      "Train Epoch: 233 [9984/17352 (58%)] Loss: -45066.324219\n",
      "Train Epoch: 233 [11392/17352 (66%)] Loss: -125785.335938\n",
      "Train Epoch: 233 [12800/17352 (74%)] Loss: -108656.390625\n",
      "Train Epoch: 233 [14208/17352 (82%)] Loss: -115021.625000\n",
      "Train Epoch: 233 [15466/17352 (89%)] Loss: -4649.060059\n",
      "Train Epoch: 233 [16271/17352 (94%)] Loss: -16349.400391\n",
      "Train Epoch: 233 [16941/17352 (98%)] Loss: -35200.785156\n",
      "    epoch          : 233\n",
      "    loss           : -108628.74240378565\n",
      "    val_loss       : -53180.236836751305\n",
      "Train Epoch: 234 [128/17352 (1%)] Loss: -96633.382812\n",
      "Train Epoch: 234 [1536/17352 (9%)] Loss: -95084.632812\n",
      "Train Epoch: 234 [2944/17352 (17%)] Loss: -132092.296875\n",
      "Train Epoch: 234 [4352/17352 (25%)] Loss: -133328.796875\n",
      "Train Epoch: 234 [5760/17352 (33%)] Loss: -135186.968750\n",
      "Train Epoch: 234 [7168/17352 (41%)] Loss: -140335.593750\n",
      "Train Epoch: 234 [8576/17352 (49%)] Loss: -124074.648438\n",
      "Train Epoch: 234 [9984/17352 (58%)] Loss: -135579.812500\n",
      "Train Epoch: 234 [11392/17352 (66%)] Loss: -80049.609375\n",
      "Train Epoch: 234 [12800/17352 (74%)] Loss: -150233.468750\n",
      "Train Epoch: 234 [14208/17352 (82%)] Loss: -141943.328125\n",
      "Train Epoch: 234 [15526/17352 (89%)] Loss: -97862.406250\n",
      "Train Epoch: 234 [16122/17352 (93%)] Loss: -14450.814453\n",
      "Train Epoch: 234 [16967/17352 (98%)] Loss: -99387.179688\n",
      "    epoch          : 234\n",
      "    loss           : -116757.62821642985\n",
      "    val_loss       : -65762.72389322917\n",
      "Train Epoch: 235 [128/17352 (1%)] Loss: -154159.937500\n",
      "Train Epoch: 235 [1536/17352 (9%)] Loss: -147590.968750\n",
      "Train Epoch: 235 [2944/17352 (17%)] Loss: -148032.281250\n",
      "Train Epoch: 235 [4352/17352 (25%)] Loss: -139852.375000\n",
      "Train Epoch: 235 [5760/17352 (33%)] Loss: -112327.828125\n",
      "Train Epoch: 235 [7168/17352 (41%)] Loss: -125676.718750\n",
      "Train Epoch: 235 [8576/17352 (49%)] Loss: -130810.296875\n",
      "Train Epoch: 235 [9984/17352 (58%)] Loss: -161550.296875\n",
      "Train Epoch: 235 [11392/17352 (66%)] Loss: -113568.570312\n",
      "Train Epoch: 235 [12800/17352 (74%)] Loss: -138657.656250\n",
      "Train Epoch: 235 [14208/17352 (82%)] Loss: -124435.867188\n",
      "Train Epoch: 235 [15459/17352 (89%)] Loss: -94238.789062\n",
      "Train Epoch: 235 [16218/17352 (93%)] Loss: -53285.890625\n",
      "Train Epoch: 235 [17013/17352 (98%)] Loss: -41579.062500\n",
      "    epoch          : 235\n",
      "    loss           : -113617.1286277003\n",
      "    val_loss       : -66468.31224772135\n",
      "Train Epoch: 236 [128/17352 (1%)] Loss: -155406.718750\n",
      "Train Epoch: 236 [1536/17352 (9%)] Loss: -152014.875000\n",
      "Train Epoch: 236 [2944/17352 (17%)] Loss: -133127.875000\n",
      "Train Epoch: 236 [4352/17352 (25%)] Loss: -136917.437500\n",
      "Train Epoch: 236 [5760/17352 (33%)] Loss: -157138.250000\n",
      "Train Epoch: 236 [7168/17352 (41%)] Loss: -110900.562500\n",
      "Train Epoch: 236 [8576/17352 (49%)] Loss: -123942.906250\n",
      "Train Epoch: 236 [9984/17352 (58%)] Loss: -120499.421875\n",
      "Train Epoch: 236 [11392/17352 (66%)] Loss: -173348.796875\n",
      "Train Epoch: 236 [12800/17352 (74%)] Loss: -148540.984375\n",
      "Train Epoch: 236 [14208/17352 (82%)] Loss: -147490.718750\n",
      "Train Epoch: 236 [15577/17352 (90%)] Loss: -80214.140625\n",
      "Train Epoch: 236 [16227/17352 (94%)] Loss: -3594.357422\n",
      "Train Epoch: 236 [17082/17352 (98%)] Loss: -85351.414062\n",
      "    epoch          : 236\n",
      "    loss           : -122654.628847263\n",
      "    val_loss       : -57828.789786783855\n",
      "Train Epoch: 237 [128/17352 (1%)] Loss: -101548.531250\n",
      "Train Epoch: 237 [1536/17352 (9%)] Loss: -80643.234375\n",
      "Train Epoch: 237 [2944/17352 (17%)] Loss: -111738.632812\n",
      "Train Epoch: 237 [4352/17352 (25%)] Loss: -119824.539062\n",
      "Train Epoch: 237 [5760/17352 (33%)] Loss: -125646.390625\n",
      "Train Epoch: 237 [7168/17352 (41%)] Loss: -132134.343750\n",
      "Train Epoch: 237 [8576/17352 (49%)] Loss: -111357.390625\n",
      "Train Epoch: 237 [9984/17352 (58%)] Loss: -130081.500000\n",
      "Train Epoch: 237 [11392/17352 (66%)] Loss: -125102.171875\n",
      "Train Epoch: 237 [12800/17352 (74%)] Loss: -101198.445312\n",
      "Train Epoch: 237 [14208/17352 (82%)] Loss: -149432.078125\n",
      "Train Epoch: 237 [15478/17352 (89%)] Loss: -112229.187500\n",
      "Train Epoch: 237 [16330/17352 (94%)] Loss: -113852.632812\n",
      "Train Epoch: 237 [17035/17352 (98%)] Loss: -27993.273438\n",
      "    epoch          : 237\n",
      "    loss           : -109461.28319984794\n",
      "    val_loss       : -50927.08791503906\n",
      "Train Epoch: 238 [128/17352 (1%)] Loss: -102036.632812\n",
      "Train Epoch: 238 [1536/17352 (9%)] Loss: -131870.406250\n",
      "Train Epoch: 238 [2944/17352 (17%)] Loss: -155390.687500\n",
      "Train Epoch: 238 [4352/17352 (25%)] Loss: -173092.000000\n",
      "Train Epoch: 238 [5760/17352 (33%)] Loss: -117194.031250\n",
      "Train Epoch: 238 [7168/17352 (41%)] Loss: -161335.171875\n",
      "Train Epoch: 238 [8576/17352 (49%)] Loss: -121425.937500\n",
      "Train Epoch: 238 [9984/17352 (58%)] Loss: -153932.906250\n",
      "Train Epoch: 238 [11392/17352 (66%)] Loss: -111420.789062\n",
      "Train Epoch: 238 [12800/17352 (74%)] Loss: -134615.843750\n",
      "Train Epoch: 238 [14208/17352 (82%)] Loss: -153066.484375\n",
      "Train Epoch: 238 [15499/17352 (89%)] Loss: -94747.523438\n",
      "Train Epoch: 238 [16258/17352 (94%)] Loss: -5722.698730\n",
      "Train Epoch: 238 [17053/17352 (98%)] Loss: -173839.078125\n",
      "    epoch          : 238\n",
      "    loss           : -124533.12906354865\n",
      "    val_loss       : -67163.30434163411\n",
      "Train Epoch: 239 [128/17352 (1%)] Loss: -147054.656250\n",
      "Train Epoch: 239 [1536/17352 (9%)] Loss: -113253.062500\n",
      "Train Epoch: 239 [2944/17352 (17%)] Loss: -143827.109375\n",
      "Train Epoch: 239 [4352/17352 (25%)] Loss: -103867.515625\n",
      "Train Epoch: 239 [5760/17352 (33%)] Loss: -163211.593750\n",
      "Train Epoch: 239 [7168/17352 (41%)] Loss: -121180.906250\n",
      "Train Epoch: 239 [8576/17352 (49%)] Loss: -173185.140625\n",
      "Train Epoch: 239 [9984/17352 (58%)] Loss: -137372.218750\n",
      "Train Epoch: 239 [11392/17352 (66%)] Loss: -103456.390625\n",
      "Train Epoch: 239 [12800/17352 (74%)] Loss: -106951.968750\n",
      "Train Epoch: 239 [14208/17352 (82%)] Loss: -147863.062500\n",
      "Train Epoch: 239 [15498/17352 (89%)] Loss: -64268.843750\n",
      "Train Epoch: 239 [16485/17352 (95%)] Loss: -63063.648438\n",
      "Train Epoch: 239 [17121/17352 (99%)] Loss: -19776.167969\n",
      "    epoch          : 239\n",
      "    loss           : -118988.7372047373\n",
      "    val_loss       : -64400.84394124349\n",
      "Train Epoch: 240 [128/17352 (1%)] Loss: -137123.000000\n",
      "Train Epoch: 240 [1536/17352 (9%)] Loss: -144417.625000\n",
      "Train Epoch: 240 [2944/17352 (17%)] Loss: -93751.054688\n",
      "Train Epoch: 240 [4352/17352 (25%)] Loss: -147286.109375\n",
      "Train Epoch: 240 [5760/17352 (33%)] Loss: -99743.945312\n",
      "Train Epoch: 240 [7168/17352 (41%)] Loss: -134422.859375\n",
      "Train Epoch: 240 [8576/17352 (49%)] Loss: -106222.117188\n",
      "Train Epoch: 240 [9984/17352 (58%)] Loss: -121552.460938\n",
      "Train Epoch: 240 [11392/17352 (66%)] Loss: -140103.531250\n",
      "Train Epoch: 240 [12800/17352 (74%)] Loss: -112676.375000\n",
      "Train Epoch: 240 [14208/17352 (82%)] Loss: -117342.500000\n",
      "Train Epoch: 240 [15515/17352 (89%)] Loss: -87427.351562\n",
      "Train Epoch: 240 [16215/17352 (93%)] Loss: -87153.687500\n",
      "Train Epoch: 240 [17044/17352 (98%)] Loss: -112534.320312\n",
      "    epoch          : 240\n",
      "    loss           : -111846.1032731229\n",
      "    val_loss       : -57046.35463460287\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch240.pth ...\n",
      "Train Epoch: 241 [128/17352 (1%)] Loss: -114176.875000\n",
      "Train Epoch: 241 [1536/17352 (9%)] Loss: -101457.265625\n",
      "Train Epoch: 241 [2944/17352 (17%)] Loss: -89573.507812\n",
      "Train Epoch: 241 [4352/17352 (25%)] Loss: -113805.640625\n",
      "Train Epoch: 241 [5760/17352 (33%)] Loss: -124132.164062\n",
      "Train Epoch: 241 [7168/17352 (41%)] Loss: -104832.328125\n",
      "Train Epoch: 241 [8576/17352 (49%)] Loss: -146202.703125\n",
      "Train Epoch: 241 [9984/17352 (58%)] Loss: -138344.000000\n",
      "Train Epoch: 241 [11392/17352 (66%)] Loss: -159081.125000\n",
      "Train Epoch: 241 [12800/17352 (74%)] Loss: -158713.359375\n",
      "Train Epoch: 241 [14208/17352 (82%)] Loss: -141153.296875\n",
      "Train Epoch: 241 [15444/17352 (89%)] Loss: -58855.867188\n",
      "Train Epoch: 241 [16210/17352 (93%)] Loss: -14654.572266\n",
      "Train Epoch: 241 [17031/17352 (98%)] Loss: -4998.543457\n",
      "    epoch          : 241\n",
      "    loss           : -116533.21019852402\n",
      "    val_loss       : -60770.42659912109\n",
      "Train Epoch: 242 [128/17352 (1%)] Loss: -102859.867188\n",
      "Train Epoch: 242 [1536/17352 (9%)] Loss: -126018.843750\n",
      "Train Epoch: 242 [2944/17352 (17%)] Loss: -132315.453125\n",
      "Train Epoch: 242 [4352/17352 (25%)] Loss: -129425.789062\n",
      "Train Epoch: 242 [5760/17352 (33%)] Loss: -125431.507812\n",
      "Train Epoch: 242 [7168/17352 (41%)] Loss: -123021.601562\n",
      "Train Epoch: 242 [8576/17352 (49%)] Loss: -99014.375000\n",
      "Train Epoch: 242 [9984/17352 (58%)] Loss: -152879.625000\n",
      "Train Epoch: 242 [11392/17352 (66%)] Loss: -103964.101562\n",
      "Train Epoch: 242 [12800/17352 (74%)] Loss: -139453.750000\n",
      "Train Epoch: 242 [14208/17352 (82%)] Loss: -167984.375000\n",
      "Train Epoch: 242 [15537/17352 (90%)] Loss: -119589.632812\n",
      "Train Epoch: 242 [16162/17352 (93%)] Loss: -6492.145020\n",
      "Train Epoch: 242 [16941/17352 (98%)] Loss: -61870.234375\n",
      "    epoch          : 242\n",
      "    loss           : -120977.06386817062\n",
      "    val_loss       : -66335.63040364583\n",
      "Train Epoch: 243 [128/17352 (1%)] Loss: -133120.968750\n",
      "Train Epoch: 243 [1536/17352 (9%)] Loss: -118919.687500\n",
      "Train Epoch: 243 [2944/17352 (17%)] Loss: -119153.617188\n",
      "Train Epoch: 243 [4352/17352 (25%)] Loss: -110280.921875\n",
      "Train Epoch: 243 [5760/17352 (33%)] Loss: -130279.710938\n",
      "Train Epoch: 243 [7168/17352 (41%)] Loss: -156542.593750\n",
      "Train Epoch: 243 [8576/17352 (49%)] Loss: -122635.742188\n",
      "Train Epoch: 243 [9984/17352 (58%)] Loss: -163147.062500\n",
      "Train Epoch: 243 [11392/17352 (66%)] Loss: -147771.312500\n",
      "Train Epoch: 243 [12800/17352 (74%)] Loss: -138841.265625\n",
      "Train Epoch: 243 [14208/17352 (82%)] Loss: -114886.789062\n",
      "Train Epoch: 243 [15491/17352 (89%)] Loss: -47926.359375\n",
      "Train Epoch: 243 [16121/17352 (93%)] Loss: -5490.255859\n",
      "Train Epoch: 243 [17012/17352 (98%)] Loss: -38738.546875\n",
      "    epoch          : 243\n",
      "    loss           : -115904.44983483641\n",
      "    val_loss       : -49206.424841308595\n",
      "Train Epoch: 244 [128/17352 (1%)] Loss: -115789.031250\n",
      "Train Epoch: 244 [1536/17352 (9%)] Loss: -130402.281250\n",
      "Train Epoch: 244 [2944/17352 (17%)] Loss: -135874.625000\n",
      "Train Epoch: 244 [4352/17352 (25%)] Loss: -147719.000000\n",
      "Train Epoch: 244 [5760/17352 (33%)] Loss: -141710.531250\n",
      "Train Epoch: 244 [7168/17352 (41%)] Loss: -147824.625000\n",
      "Train Epoch: 244 [8576/17352 (49%)] Loss: -140321.359375\n",
      "Train Epoch: 244 [9984/17352 (58%)] Loss: -117654.656250\n",
      "Train Epoch: 244 [11392/17352 (66%)] Loss: -150768.531250\n",
      "Train Epoch: 244 [12800/17352 (74%)] Loss: -156091.312500\n",
      "Train Epoch: 244 [14208/17352 (82%)] Loss: -113618.851562\n",
      "Train Epoch: 244 [15479/17352 (89%)] Loss: -35123.796875\n",
      "Train Epoch: 244 [16340/17352 (94%)] Loss: -66683.195312\n",
      "Train Epoch: 244 [17189/17352 (99%)] Loss: -93349.437500\n",
      "    epoch          : 244\n",
      "    loss           : -113767.08241138843\n",
      "    val_loss       : -64584.506807454425\n",
      "Train Epoch: 245 [128/17352 (1%)] Loss: -126628.281250\n",
      "Train Epoch: 245 [1536/17352 (9%)] Loss: -141701.031250\n",
      "Train Epoch: 245 [2944/17352 (17%)] Loss: -141911.218750\n",
      "Train Epoch: 245 [4352/17352 (25%)] Loss: -138951.812500\n",
      "Train Epoch: 245 [5760/17352 (33%)] Loss: -105640.453125\n",
      "Train Epoch: 245 [7168/17352 (41%)] Loss: -143044.031250\n",
      "Train Epoch: 245 [8576/17352 (49%)] Loss: -126235.515625\n",
      "Train Epoch: 245 [9984/17352 (58%)] Loss: -128971.062500\n",
      "Train Epoch: 245 [11392/17352 (66%)] Loss: -145910.250000\n",
      "Train Epoch: 245 [12800/17352 (74%)] Loss: -122748.234375\n",
      "Train Epoch: 245 [14208/17352 (82%)] Loss: -125925.562500\n",
      "Train Epoch: 245 [15528/17352 (89%)] Loss: -162463.312500\n",
      "Train Epoch: 245 [16338/17352 (94%)] Loss: -78967.078125\n",
      "Train Epoch: 245 [17078/17352 (98%)] Loss: -98890.000000\n",
      "    epoch          : 245\n",
      "    loss           : -116709.30932289481\n",
      "    val_loss       : -51872.76278889974\n",
      "Train Epoch: 246 [128/17352 (1%)] Loss: -104249.609375\n",
      "Train Epoch: 246 [1536/17352 (9%)] Loss: -131413.375000\n",
      "Train Epoch: 246 [2944/17352 (17%)] Loss: -110781.250000\n",
      "Train Epoch: 246 [4352/17352 (25%)] Loss: -3043.608154\n",
      "Train Epoch: 246 [5760/17352 (33%)] Loss: -61278.742188\n",
      "Train Epoch: 246 [7168/17352 (41%)] Loss: -85098.921875\n",
      "Train Epoch: 246 [8576/17352 (49%)] Loss: -121352.929688\n",
      "Train Epoch: 246 [9984/17352 (58%)] Loss: -126557.546875\n",
      "Train Epoch: 246 [11392/17352 (66%)] Loss: -125654.281250\n",
      "Train Epoch: 246 [12800/17352 (74%)] Loss: -134586.812500\n",
      "Train Epoch: 246 [14208/17352 (82%)] Loss: -130740.062500\n",
      "Train Epoch: 246 [15513/17352 (89%)] Loss: -78025.000000\n",
      "Train Epoch: 246 [16308/17352 (94%)] Loss: -75563.265625\n",
      "Train Epoch: 246 [16966/17352 (98%)] Loss: -135172.125000\n",
      "    epoch          : 246\n",
      "    loss           : -104430.54664724466\n",
      "    val_loss       : -65248.79326578776\n",
      "Train Epoch: 247 [128/17352 (1%)] Loss: -125161.984375\n",
      "Train Epoch: 247 [1536/17352 (9%)] Loss: -151889.093750\n",
      "Train Epoch: 247 [2944/17352 (17%)] Loss: -118912.156250\n",
      "Train Epoch: 247 [4352/17352 (25%)] Loss: -149346.062500\n",
      "Train Epoch: 247 [5760/17352 (33%)] Loss: -115367.234375\n",
      "Train Epoch: 247 [7168/17352 (41%)] Loss: -136334.140625\n",
      "Train Epoch: 247 [8576/17352 (49%)] Loss: -120423.148438\n",
      "Train Epoch: 247 [9984/17352 (58%)] Loss: -107533.765625\n",
      "Train Epoch: 247 [11392/17352 (66%)] Loss: -133071.093750\n",
      "Train Epoch: 247 [12800/17352 (74%)] Loss: -159214.750000\n",
      "Train Epoch: 247 [14208/17352 (82%)] Loss: -168371.265625\n",
      "Train Epoch: 247 [15416/17352 (89%)] Loss: -10074.538086\n",
      "Train Epoch: 247 [16313/17352 (94%)] Loss: -100271.851562\n",
      "Train Epoch: 247 [17046/17352 (98%)] Loss: -35191.074219\n",
      "    epoch          : 247\n",
      "    loss           : -120345.24840898962\n",
      "    val_loss       : -68348.10772298177\n",
      "Train Epoch: 248 [128/17352 (1%)] Loss: -116725.679688\n",
      "Train Epoch: 248 [1536/17352 (9%)] Loss: -176866.343750\n",
      "Train Epoch: 248 [2944/17352 (17%)] Loss: -123364.445312\n",
      "Train Epoch: 248 [4352/17352 (25%)] Loss: -117874.414062\n",
      "Train Epoch: 248 [5760/17352 (33%)] Loss: -118139.390625\n",
      "Train Epoch: 248 [7168/17352 (41%)] Loss: -142347.843750\n",
      "Train Epoch: 248 [8576/17352 (49%)] Loss: -144130.234375\n",
      "Train Epoch: 248 [9984/17352 (58%)] Loss: -170980.390625\n",
      "Train Epoch: 248 [11392/17352 (66%)] Loss: -146879.906250\n",
      "Train Epoch: 248 [12800/17352 (74%)] Loss: -101539.468750\n",
      "Train Epoch: 248 [14208/17352 (82%)] Loss: -123090.265625\n",
      "Train Epoch: 248 [15529/17352 (89%)] Loss: -40458.585938\n",
      "Train Epoch: 248 [16267/17352 (94%)] Loss: -53577.726562\n",
      "Train Epoch: 248 [16961/17352 (98%)] Loss: -2663.385498\n",
      "    epoch          : 248\n",
      "    loss           : -115508.10442173081\n",
      "    val_loss       : -53752.55260416667\n",
      "Train Epoch: 249 [128/17352 (1%)] Loss: -135572.703125\n",
      "Train Epoch: 249 [1536/17352 (9%)] Loss: -139785.406250\n",
      "Train Epoch: 249 [2944/17352 (17%)] Loss: -147533.375000\n",
      "Train Epoch: 249 [4352/17352 (25%)] Loss: -126103.148438\n",
      "Train Epoch: 249 [5760/17352 (33%)] Loss: -125915.632812\n",
      "Train Epoch: 249 [7168/17352 (41%)] Loss: -119801.648438\n",
      "Train Epoch: 249 [8576/17352 (49%)] Loss: -149331.453125\n",
      "Train Epoch: 249 [9984/17352 (58%)] Loss: -119034.992188\n",
      "Train Epoch: 249 [11392/17352 (66%)] Loss: -135569.828125\n",
      "Train Epoch: 249 [12800/17352 (74%)] Loss: -136049.703125\n",
      "Train Epoch: 249 [14208/17352 (82%)] Loss: -119939.273438\n",
      "Train Epoch: 249 [15491/17352 (89%)] Loss: -80026.093750\n",
      "Train Epoch: 249 [16250/17352 (94%)] Loss: -69223.039062\n",
      "Train Epoch: 249 [16952/17352 (98%)] Loss: -100507.140625\n",
      "    epoch          : 249\n",
      "    loss           : -115119.74166808672\n",
      "    val_loss       : -55740.13878580729\n",
      "Train Epoch: 250 [128/17352 (1%)] Loss: -129422.554688\n",
      "Train Epoch: 250 [1536/17352 (9%)] Loss: -105394.578125\n",
      "Train Epoch: 250 [2944/17352 (17%)] Loss: -105434.625000\n",
      "Train Epoch: 250 [4352/17352 (25%)] Loss: -136325.984375\n",
      "Train Epoch: 250 [5760/17352 (33%)] Loss: -137112.406250\n",
      "Train Epoch: 250 [7168/17352 (41%)] Loss: -140915.781250\n",
      "Train Epoch: 250 [8576/17352 (49%)] Loss: -110492.250000\n",
      "Train Epoch: 250 [9984/17352 (58%)] Loss: -168144.609375\n",
      "Train Epoch: 250 [11392/17352 (66%)] Loss: -101549.625000\n",
      "Train Epoch: 250 [12800/17352 (74%)] Loss: -115147.046875\n",
      "Train Epoch: 250 [14208/17352 (82%)] Loss: -105806.007812\n",
      "Train Epoch: 250 [15560/17352 (90%)] Loss: -128298.734375\n",
      "Train Epoch: 250 [16231/17352 (94%)] Loss: -37723.027344\n",
      "Train Epoch: 250 [16931/17352 (98%)] Loss: -59425.457031\n",
      "    epoch          : 250\n",
      "    loss           : -121879.80457280306\n",
      "    val_loss       : -65750.6887451172\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [128/17352 (1%)] Loss: -138169.093750\n",
      "Train Epoch: 251 [1536/17352 (9%)] Loss: -120525.171875\n",
      "Train Epoch: 251 [2944/17352 (17%)] Loss: -127709.937500\n",
      "Train Epoch: 251 [4352/17352 (25%)] Loss: -139056.984375\n",
      "Train Epoch: 251 [5760/17352 (33%)] Loss: -110936.843750\n",
      "Train Epoch: 251 [7168/17352 (41%)] Loss: -107662.015625\n",
      "Train Epoch: 251 [8576/17352 (49%)] Loss: -147896.015625\n",
      "Train Epoch: 251 [9984/17352 (58%)] Loss: -122022.406250\n",
      "Train Epoch: 251 [11392/17352 (66%)] Loss: -154010.984375\n",
      "Train Epoch: 251 [12800/17352 (74%)] Loss: -172642.546875\n",
      "Train Epoch: 251 [14208/17352 (82%)] Loss: -102597.171875\n",
      "Train Epoch: 251 [15487/17352 (89%)] Loss: -101151.046875\n",
      "Train Epoch: 251 [16366/17352 (94%)] Loss: -116272.882812\n",
      "Train Epoch: 251 [17130/17352 (99%)] Loss: -14605.649414\n",
      "    epoch          : 251\n",
      "    loss           : -122905.29473672138\n",
      "    val_loss       : -62887.42069905599\n",
      "Train Epoch: 252 [128/17352 (1%)] Loss: -125335.554688\n",
      "Train Epoch: 252 [1536/17352 (9%)] Loss: -129090.632812\n",
      "Train Epoch: 252 [2944/17352 (17%)] Loss: -98377.101562\n",
      "Train Epoch: 252 [4352/17352 (25%)] Loss: -71608.226562\n",
      "Train Epoch: 252 [5760/17352 (33%)] Loss: -87919.109375\n",
      "Train Epoch: 252 [7168/17352 (41%)] Loss: -107838.523438\n",
      "Train Epoch: 252 [8576/17352 (49%)] Loss: -132305.828125\n",
      "Train Epoch: 252 [9984/17352 (58%)] Loss: -148457.468750\n",
      "Train Epoch: 252 [11392/17352 (66%)] Loss: -153036.140625\n",
      "Train Epoch: 252 [12800/17352 (74%)] Loss: -138521.562500\n",
      "Train Epoch: 252 [14208/17352 (82%)] Loss: -124278.265625\n",
      "Train Epoch: 252 [15542/17352 (90%)] Loss: -111189.296875\n",
      "Train Epoch: 252 [16372/17352 (94%)] Loss: -45691.593750\n",
      "Train Epoch: 252 [17068/17352 (98%)] Loss: -102458.734375\n",
      "    epoch          : 252\n",
      "    loss           : -113471.7493249266\n",
      "    val_loss       : -63787.7603881836\n",
      "Train Epoch: 253 [128/17352 (1%)] Loss: -137608.546875\n",
      "Train Epoch: 253 [1536/17352 (9%)] Loss: -104227.265625\n",
      "Train Epoch: 253 [2944/17352 (17%)] Loss: -144988.562500\n",
      "Train Epoch: 253 [4352/17352 (25%)] Loss: -112279.398438\n",
      "Train Epoch: 253 [5760/17352 (33%)] Loss: -137017.453125\n",
      "Train Epoch: 253 [7168/17352 (41%)] Loss: -112738.859375\n",
      "Train Epoch: 253 [8576/17352 (49%)] Loss: -127287.789062\n",
      "Train Epoch: 253 [9984/17352 (58%)] Loss: -114597.781250\n",
      "Train Epoch: 253 [11392/17352 (66%)] Loss: -129072.531250\n",
      "Train Epoch: 253 [12800/17352 (74%)] Loss: -135913.093750\n",
      "Train Epoch: 253 [14208/17352 (82%)] Loss: -137975.171875\n",
      "Train Epoch: 253 [15476/17352 (89%)] Loss: -54536.230469\n",
      "Train Epoch: 253 [16284/17352 (94%)] Loss: -86927.656250\n",
      "Train Epoch: 253 [16889/17352 (97%)] Loss: -39280.054688\n",
      "    epoch          : 253\n",
      "    loss           : -120844.4034972735\n",
      "    val_loss       : -59635.103959147134\n",
      "Train Epoch: 254 [128/17352 (1%)] Loss: -124135.554688\n",
      "Train Epoch: 254 [1536/17352 (9%)] Loss: -104821.101562\n",
      "Train Epoch: 254 [2944/17352 (17%)] Loss: -118388.640625\n",
      "Train Epoch: 254 [4352/17352 (25%)] Loss: -112959.289062\n",
      "Train Epoch: 254 [5760/17352 (33%)] Loss: -145453.671875\n",
      "Train Epoch: 254 [7168/17352 (41%)] Loss: -154856.312500\n",
      "Train Epoch: 254 [8576/17352 (49%)] Loss: -123054.843750\n",
      "Train Epoch: 254 [9984/17352 (58%)] Loss: -111338.328125\n",
      "Train Epoch: 254 [11392/17352 (66%)] Loss: -129181.242188\n",
      "Train Epoch: 254 [12800/17352 (74%)] Loss: -126002.421875\n",
      "Train Epoch: 254 [14208/17352 (82%)] Loss: -119774.773438\n",
      "Train Epoch: 254 [15493/17352 (89%)] Loss: -69472.453125\n",
      "Train Epoch: 254 [16262/17352 (94%)] Loss: -126278.726562\n",
      "Train Epoch: 254 [16992/17352 (98%)] Loss: -81639.375000\n",
      "    epoch          : 254\n",
      "    loss           : -118014.81178724045\n",
      "    val_loss       : -49085.87260742187\n",
      "Train Epoch: 255 [128/17352 (1%)] Loss: -83432.156250\n",
      "Train Epoch: 255 [1536/17352 (9%)] Loss: -108107.250000\n",
      "Train Epoch: 255 [2944/17352 (17%)] Loss: -90232.062500\n",
      "Train Epoch: 255 [4352/17352 (25%)] Loss: -104089.187500\n",
      "Train Epoch: 255 [5760/17352 (33%)] Loss: -121283.312500\n",
      "Train Epoch: 255 [7168/17352 (41%)] Loss: -120080.656250\n",
      "Train Epoch: 255 [8576/17352 (49%)] Loss: -124563.343750\n",
      "Train Epoch: 255 [9984/17352 (58%)] Loss: -124219.281250\n",
      "Train Epoch: 255 [11392/17352 (66%)] Loss: -129482.765625\n",
      "Train Epoch: 255 [12800/17352 (74%)] Loss: -105556.359375\n",
      "Train Epoch: 255 [14208/17352 (82%)] Loss: -130190.226562\n",
      "Train Epoch: 255 [15552/17352 (90%)] Loss: -60606.964844\n",
      "Train Epoch: 255 [16437/17352 (95%)] Loss: -32947.789062\n",
      "Train Epoch: 255 [17008/17352 (98%)] Loss: -6571.071777\n",
      "    epoch          : 255\n",
      "    loss           : -115302.7467622693\n",
      "    val_loss       : -35653.32276204427\n",
      "Train Epoch: 256 [128/17352 (1%)] Loss: -68726.046875\n",
      "Train Epoch: 256 [1536/17352 (9%)] Loss: -79478.914062\n",
      "Train Epoch: 256 [2944/17352 (17%)] Loss: -94854.757812\n",
      "Train Epoch: 256 [4352/17352 (25%)] Loss: -139448.531250\n",
      "Train Epoch: 256 [5760/17352 (33%)] Loss: -126311.484375\n",
      "Train Epoch: 256 [7168/17352 (41%)] Loss: -90817.953125\n",
      "Train Epoch: 256 [8576/17352 (49%)] Loss: -141138.468750\n",
      "Train Epoch: 256 [9984/17352 (58%)] Loss: -147189.218750\n",
      "Train Epoch: 256 [11392/17352 (66%)] Loss: -102539.703125\n",
      "Train Epoch: 256 [12800/17352 (74%)] Loss: -134693.062500\n",
      "Train Epoch: 256 [14208/17352 (82%)] Loss: -164194.437500\n",
      "Train Epoch: 256 [15551/17352 (90%)] Loss: -74878.265625\n",
      "Train Epoch: 256 [16446/17352 (95%)] Loss: -86601.546875\n",
      "Train Epoch: 256 [17048/17352 (98%)] Loss: -48702.000000\n",
      "    epoch          : 256\n",
      "    loss           : -112416.51200713088\n",
      "    val_loss       : -48183.486482747394\n",
      "Train Epoch: 257 [128/17352 (1%)] Loss: -102640.664062\n",
      "Train Epoch: 257 [1536/17352 (9%)] Loss: -124110.914062\n",
      "Train Epoch: 257 [2944/17352 (17%)] Loss: -107757.140625\n",
      "Train Epoch: 257 [4352/17352 (25%)] Loss: -125312.507812\n",
      "Train Epoch: 257 [5760/17352 (33%)] Loss: -157320.859375\n",
      "Train Epoch: 257 [7168/17352 (41%)] Loss: -130880.640625\n",
      "Train Epoch: 257 [8576/17352 (49%)] Loss: -82280.640625\n",
      "Train Epoch: 257 [9984/17352 (58%)] Loss: -121662.484375\n",
      "Train Epoch: 257 [11392/17352 (66%)] Loss: -152050.578125\n",
      "Train Epoch: 257 [12800/17352 (74%)] Loss: -132179.578125\n",
      "Train Epoch: 257 [14208/17352 (82%)] Loss: -117737.546875\n",
      "Train Epoch: 257 [15523/17352 (89%)] Loss: -89856.648438\n",
      "Train Epoch: 257 [16272/17352 (94%)] Loss: -117888.632812\n",
      "Train Epoch: 257 [17132/17352 (99%)] Loss: -69473.859375\n",
      "    epoch          : 257\n",
      "    loss           : -116815.41071826499\n",
      "    val_loss       : -64357.39390055338\n",
      "Train Epoch: 258 [128/17352 (1%)] Loss: -140397.843750\n",
      "Train Epoch: 258 [1536/17352 (9%)] Loss: -146580.468750\n",
      "Train Epoch: 258 [2944/17352 (17%)] Loss: -112978.921875\n",
      "Train Epoch: 258 [4352/17352 (25%)] Loss: -163827.515625\n",
      "Train Epoch: 258 [5760/17352 (33%)] Loss: -131679.921875\n",
      "Train Epoch: 258 [7168/17352 (41%)] Loss: -120602.148438\n",
      "Train Epoch: 258 [8576/17352 (49%)] Loss: -172120.484375\n",
      "Train Epoch: 258 [9984/17352 (58%)] Loss: -140741.859375\n",
      "Train Epoch: 258 [11392/17352 (66%)] Loss: -136066.312500\n",
      "Train Epoch: 258 [12800/17352 (74%)] Loss: -116843.164062\n",
      "Train Epoch: 258 [14208/17352 (82%)] Loss: -154908.000000\n",
      "Train Epoch: 258 [15460/17352 (89%)] Loss: -15967.857422\n",
      "Train Epoch: 258 [16227/17352 (94%)] Loss: -104439.250000\n",
      "Train Epoch: 258 [17022/17352 (98%)] Loss: -102728.578125\n",
      "    epoch          : 258\n",
      "    loss           : -123562.85481006186\n",
      "    val_loss       : -58384.17701416016\n",
      "Train Epoch: 259 [128/17352 (1%)] Loss: -129453.398438\n",
      "Train Epoch: 259 [1536/17352 (9%)] Loss: -120217.312500\n",
      "Train Epoch: 259 [2944/17352 (17%)] Loss: -124419.609375\n",
      "Train Epoch: 259 [4352/17352 (25%)] Loss: -101306.515625\n",
      "Train Epoch: 259 [5760/17352 (33%)] Loss: -138559.531250\n",
      "Train Epoch: 259 [7168/17352 (41%)] Loss: -129387.617188\n",
      "Train Epoch: 259 [8576/17352 (49%)] Loss: -154757.078125\n",
      "Train Epoch: 259 [9984/17352 (58%)] Loss: -120092.359375\n",
      "Train Epoch: 259 [11392/17352 (66%)] Loss: -88101.859375\n",
      "Train Epoch: 259 [12800/17352 (74%)] Loss: -136144.843750\n",
      "Train Epoch: 259 [14208/17352 (82%)] Loss: -144073.375000\n",
      "Train Epoch: 259 [15378/17352 (89%)] Loss: -5341.246094\n",
      "Train Epoch: 259 [16106/17352 (93%)] Loss: -2881.833008\n",
      "Train Epoch: 259 [16917/17352 (97%)] Loss: -109147.156250\n",
      "    epoch          : 259\n",
      "    loss           : -119191.93409841653\n",
      "    val_loss       : -59161.43227132162\n",
      "Train Epoch: 260 [128/17352 (1%)] Loss: -136559.984375\n",
      "Train Epoch: 260 [1536/17352 (9%)] Loss: -109820.765625\n",
      "Train Epoch: 260 [2944/17352 (17%)] Loss: -111708.453125\n",
      "Train Epoch: 260 [4352/17352 (25%)] Loss: -142580.125000\n",
      "Train Epoch: 260 [5760/17352 (33%)] Loss: -134649.593750\n",
      "Train Epoch: 260 [7168/17352 (41%)] Loss: -99095.062500\n",
      "Train Epoch: 260 [8576/17352 (49%)] Loss: -135897.687500\n",
      "Train Epoch: 260 [9984/17352 (58%)] Loss: -143516.531250\n",
      "Train Epoch: 260 [11392/17352 (66%)] Loss: -110657.875000\n",
      "Train Epoch: 260 [12800/17352 (74%)] Loss: -97557.164062\n",
      "Train Epoch: 260 [14208/17352 (82%)] Loss: -113812.460938\n",
      "Train Epoch: 260 [15376/17352 (89%)] Loss: -1570.806885\n",
      "Train Epoch: 260 [16210/17352 (93%)] Loss: -19256.626953\n",
      "Train Epoch: 260 [16963/17352 (98%)] Loss: -36894.855469\n",
      "    epoch          : 260\n",
      "    loss           : -110912.62902913957\n",
      "    val_loss       : -56932.20204671224\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch260.pth ...\n",
      "Train Epoch: 261 [128/17352 (1%)] Loss: -94168.171875\n",
      "Train Epoch: 261 [1536/17352 (9%)] Loss: -127419.367188\n",
      "Train Epoch: 261 [2944/17352 (17%)] Loss: -138579.875000\n",
      "Train Epoch: 261 [4352/17352 (25%)] Loss: -144856.125000\n",
      "Train Epoch: 261 [5760/17352 (33%)] Loss: -113267.687500\n",
      "Train Epoch: 261 [7168/17352 (41%)] Loss: -89286.703125\n",
      "Train Epoch: 261 [8576/17352 (49%)] Loss: -135853.093750\n",
      "Train Epoch: 261 [9984/17352 (58%)] Loss: -175030.171875\n",
      "Train Epoch: 261 [11392/17352 (66%)] Loss: -152978.062500\n",
      "Train Epoch: 261 [12800/17352 (74%)] Loss: -139698.484375\n",
      "Train Epoch: 261 [14208/17352 (82%)] Loss: -163718.828125\n",
      "Train Epoch: 261 [15522/17352 (89%)] Loss: -105388.187500\n",
      "Train Epoch: 261 [16285/17352 (94%)] Loss: -78970.265625\n",
      "Train Epoch: 261 [16962/17352 (98%)] Loss: -35433.878906\n",
      "    epoch          : 261\n",
      "    loss           : -119597.91572396707\n",
      "    val_loss       : -70030.87299397787\n",
      "Train Epoch: 262 [128/17352 (1%)] Loss: -165622.171875\n",
      "Train Epoch: 262 [1536/17352 (9%)] Loss: -124362.664062\n",
      "Train Epoch: 262 [2944/17352 (17%)] Loss: -166863.484375\n",
      "Train Epoch: 262 [4352/17352 (25%)] Loss: -142592.421875\n",
      "Train Epoch: 262 [5760/17352 (33%)] Loss: -102621.859375\n",
      "Train Epoch: 262 [7168/17352 (41%)] Loss: -125405.937500\n",
      "Train Epoch: 262 [8576/17352 (49%)] Loss: -105596.171875\n",
      "Train Epoch: 262 [9984/17352 (58%)] Loss: -115897.421875\n",
      "Train Epoch: 262 [11392/17352 (66%)] Loss: -127825.976562\n",
      "Train Epoch: 262 [12800/17352 (74%)] Loss: -121622.218750\n",
      "Train Epoch: 262 [14208/17352 (82%)] Loss: -99212.101562\n",
      "Train Epoch: 262 [15453/17352 (89%)] Loss: -82445.062500\n",
      "Train Epoch: 262 [16223/17352 (93%)] Loss: -79331.312500\n",
      "Train Epoch: 262 [17018/17352 (98%)] Loss: -88931.898438\n",
      "    epoch          : 262\n",
      "    loss           : -112045.34455779415\n",
      "    val_loss       : -69597.94950764974\n",
      "Train Epoch: 263 [128/17352 (1%)] Loss: -173882.968750\n",
      "Train Epoch: 263 [1536/17352 (9%)] Loss: -144078.468750\n",
      "Train Epoch: 263 [2944/17352 (17%)] Loss: -93870.523438\n",
      "Train Epoch: 263 [4352/17352 (25%)] Loss: -118936.515625\n",
      "Train Epoch: 263 [5760/17352 (33%)] Loss: -122740.562500\n",
      "Train Epoch: 263 [7168/17352 (41%)] Loss: -150295.312500\n",
      "Train Epoch: 263 [8576/17352 (49%)] Loss: -143483.265625\n",
      "Train Epoch: 263 [9984/17352 (58%)] Loss: -127424.390625\n",
      "Train Epoch: 263 [11392/17352 (66%)] Loss: -137251.890625\n",
      "Train Epoch: 263 [12800/17352 (74%)] Loss: -155028.140625\n",
      "Train Epoch: 263 [14208/17352 (82%)] Loss: -146168.093750\n",
      "Train Epoch: 263 [15564/17352 (90%)] Loss: -89617.640625\n",
      "Train Epoch: 263 [16252/17352 (94%)] Loss: -91737.296875\n",
      "Train Epoch: 263 [16975/17352 (98%)] Loss: -95927.226562\n",
      "    epoch          : 263\n",
      "    loss           : -115012.49265284186\n",
      "    val_loss       : -66020.2267985026\n",
      "Train Epoch: 264 [128/17352 (1%)] Loss: -146063.968750\n",
      "Train Epoch: 264 [1536/17352 (9%)] Loss: -117197.031250\n",
      "Train Epoch: 264 [2944/17352 (17%)] Loss: -84854.835938\n",
      "Train Epoch: 264 [4352/17352 (25%)] Loss: -176671.453125\n",
      "Train Epoch: 264 [5760/17352 (33%)] Loss: -153275.578125\n",
      "Train Epoch: 264 [7168/17352 (41%)] Loss: -149388.406250\n",
      "Train Epoch: 264 [8576/17352 (49%)] Loss: -112079.546875\n",
      "Train Epoch: 264 [9984/17352 (58%)] Loss: -120624.921875\n",
      "Train Epoch: 264 [11392/17352 (66%)] Loss: -110033.085938\n",
      "Train Epoch: 264 [12800/17352 (74%)] Loss: -143196.468750\n",
      "Train Epoch: 264 [14208/17352 (82%)] Loss: -158007.156250\n",
      "Train Epoch: 264 [15429/17352 (89%)] Loss: -30807.996094\n",
      "Train Epoch: 264 [16129/17352 (93%)] Loss: -99441.781250\n",
      "Train Epoch: 264 [16986/17352 (98%)] Loss: -2781.627441\n",
      "    epoch          : 264\n",
      "    loss           : -122777.78612298133\n",
      "    val_loss       : -39033.125309244795\n",
      "Train Epoch: 265 [128/17352 (1%)] Loss: -80056.640625\n",
      "Train Epoch: 265 [1536/17352 (9%)] Loss: -92869.179688\n",
      "Train Epoch: 265 [2944/17352 (17%)] Loss: -117435.828125\n",
      "Train Epoch: 265 [4352/17352 (25%)] Loss: -99656.101562\n",
      "Train Epoch: 265 [5760/17352 (33%)] Loss: -106141.328125\n",
      "Train Epoch: 265 [7168/17352 (41%)] Loss: -105773.375000\n",
      "Train Epoch: 265 [8576/17352 (49%)] Loss: -139907.421875\n",
      "Train Epoch: 265 [9984/17352 (58%)] Loss: -127780.257812\n",
      "Train Epoch: 265 [11392/17352 (66%)] Loss: -133029.984375\n",
      "Train Epoch: 265 [12800/17352 (74%)] Loss: -120572.859375\n",
      "Train Epoch: 265 [14208/17352 (82%)] Loss: -148044.203125\n",
      "Train Epoch: 265 [15602/17352 (90%)] Loss: -130181.796875\n",
      "Train Epoch: 265 [16348/17352 (94%)] Loss: -49203.828125\n",
      "Train Epoch: 265 [16960/17352 (98%)] Loss: -61919.617188\n",
      "    epoch          : 265\n",
      "    loss           : -118657.12771340185\n",
      "    val_loss       : -68221.55192871093\n",
      "Train Epoch: 266 [128/17352 (1%)] Loss: -156447.562500\n",
      "Train Epoch: 266 [1536/17352 (9%)] Loss: -107526.531250\n",
      "Train Epoch: 266 [2944/17352 (17%)] Loss: -136248.640625\n",
      "Train Epoch: 266 [4352/17352 (25%)] Loss: -167526.609375\n",
      "Train Epoch: 266 [5760/17352 (33%)] Loss: -134471.546875\n",
      "Train Epoch: 266 [7168/17352 (41%)] Loss: -137165.937500\n",
      "Train Epoch: 266 [8576/17352 (49%)] Loss: -89695.921875\n",
      "Train Epoch: 266 [9984/17352 (58%)] Loss: -123347.562500\n",
      "Train Epoch: 266 [11392/17352 (66%)] Loss: -127426.562500\n",
      "Train Epoch: 266 [12800/17352 (74%)] Loss: -128249.671875\n",
      "Train Epoch: 266 [14208/17352 (82%)] Loss: -134152.750000\n",
      "Train Epoch: 266 [15455/17352 (89%)] Loss: -5838.304688\n",
      "Train Epoch: 266 [16409/17352 (95%)] Loss: -118204.046875\n",
      "Train Epoch: 266 [17098/17352 (99%)] Loss: -51456.378906\n",
      "    epoch          : 266\n",
      "    loss           : -116435.77819742293\n",
      "    val_loss       : -66179.01567789713\n",
      "Train Epoch: 267 [128/17352 (1%)] Loss: -122384.062500\n",
      "Train Epoch: 267 [1536/17352 (9%)] Loss: -119274.414062\n",
      "Train Epoch: 267 [2944/17352 (17%)] Loss: -138079.593750\n",
      "Train Epoch: 267 [4352/17352 (25%)] Loss: -157583.687500\n",
      "Train Epoch: 267 [5760/17352 (33%)] Loss: -134420.640625\n",
      "Train Epoch: 267 [7168/17352 (41%)] Loss: -85244.671875\n",
      "Train Epoch: 267 [8576/17352 (49%)] Loss: -123631.609375\n",
      "Train Epoch: 267 [9984/17352 (58%)] Loss: -101027.953125\n",
      "Train Epoch: 267 [11392/17352 (66%)] Loss: -95588.070312\n",
      "Train Epoch: 267 [12800/17352 (74%)] Loss: -116920.578125\n",
      "Train Epoch: 267 [14208/17352 (82%)] Loss: -116888.781250\n",
      "Train Epoch: 267 [15378/17352 (89%)] Loss: -1761.679688\n",
      "Train Epoch: 267 [16177/17352 (93%)] Loss: -30587.429688\n",
      "Train Epoch: 267 [16847/17352 (97%)] Loss: -34886.070312\n",
      "    epoch          : 267\n",
      "    loss           : -110802.68983817901\n",
      "    val_loss       : -37864.80303548177\n",
      "Train Epoch: 268 [128/17352 (1%)] Loss: -58467.582031\n",
      "Train Epoch: 268 [1536/17352 (9%)] Loss: -66235.609375\n",
      "Train Epoch: 268 [2944/17352 (17%)] Loss: -117924.835938\n",
      "Train Epoch: 268 [4352/17352 (25%)] Loss: -126144.343750\n",
      "Train Epoch: 268 [5760/17352 (33%)] Loss: -151983.031250\n",
      "Train Epoch: 268 [7168/17352 (41%)] Loss: -127530.453125\n",
      "Train Epoch: 268 [8576/17352 (49%)] Loss: -95568.437500\n",
      "Train Epoch: 268 [9984/17352 (58%)] Loss: -154013.562500\n",
      "Train Epoch: 268 [11392/17352 (66%)] Loss: -123070.914062\n",
      "Train Epoch: 268 [12800/17352 (74%)] Loss: -125111.953125\n",
      "Train Epoch: 268 [14208/17352 (82%)] Loss: -133613.328125\n",
      "Train Epoch: 268 [15460/17352 (89%)] Loss: -18753.759766\n",
      "Train Epoch: 268 [16317/17352 (94%)] Loss: -102435.218750\n",
      "Train Epoch: 268 [16996/17352 (98%)] Loss: -111747.335938\n",
      "    epoch          : 268\n",
      "    loss           : -110323.84858693373\n",
      "    val_loss       : -60081.619783528644\n",
      "Train Epoch: 269 [128/17352 (1%)] Loss: -115245.257812\n",
      "Train Epoch: 269 [1536/17352 (9%)] Loss: -111022.835938\n",
      "Train Epoch: 269 [2944/17352 (17%)] Loss: -116198.367188\n",
      "Train Epoch: 269 [4352/17352 (25%)] Loss: -110402.093750\n",
      "Train Epoch: 269 [5760/17352 (33%)] Loss: -124387.148438\n",
      "Train Epoch: 269 [7168/17352 (41%)] Loss: -110221.937500\n",
      "Train Epoch: 269 [8576/17352 (49%)] Loss: -141310.968750\n",
      "Train Epoch: 269 [9984/17352 (58%)] Loss: -158384.890625\n",
      "Train Epoch: 269 [11392/17352 (66%)] Loss: -123947.781250\n",
      "Train Epoch: 269 [12800/17352 (74%)] Loss: -95611.281250\n",
      "Train Epoch: 269 [14208/17352 (82%)] Loss: -110111.773438\n",
      "Train Epoch: 269 [15470/17352 (89%)] Loss: -6975.917969\n",
      "Train Epoch: 269 [16314/17352 (94%)] Loss: -101221.734375\n",
      "Train Epoch: 269 [17059/17352 (98%)] Loss: -83285.242188\n",
      "    epoch          : 269\n",
      "    loss           : -116117.45148155674\n",
      "    val_loss       : -50014.96714274088\n",
      "Train Epoch: 270 [128/17352 (1%)] Loss: -102422.687500\n",
      "Train Epoch: 270 [1536/17352 (9%)] Loss: -88739.984375\n",
      "Train Epoch: 270 [2944/17352 (17%)] Loss: -123640.312500\n",
      "Train Epoch: 270 [4352/17352 (25%)] Loss: -146680.890625\n",
      "Train Epoch: 270 [5760/17352 (33%)] Loss: -138758.109375\n",
      "Train Epoch: 270 [7168/17352 (41%)] Loss: -124751.382812\n",
      "Train Epoch: 270 [8576/17352 (49%)] Loss: -145092.593750\n",
      "Train Epoch: 270 [9984/17352 (58%)] Loss: -127998.601562\n",
      "Train Epoch: 270 [11392/17352 (66%)] Loss: -138712.468750\n",
      "Train Epoch: 270 [12800/17352 (74%)] Loss: -120042.929688\n",
      "Train Epoch: 270 [14208/17352 (82%)] Loss: -156105.593750\n",
      "Train Epoch: 270 [15504/17352 (89%)] Loss: -100049.914062\n",
      "Train Epoch: 270 [16277/17352 (94%)] Loss: -33341.996094\n",
      "Train Epoch: 270 [17121/17352 (99%)] Loss: -53243.078125\n",
      "    epoch          : 270\n",
      "    loss           : -120730.79927872011\n",
      "    val_loss       : -62082.77008463542\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch270.pth ...\n",
      "Train Epoch: 271 [128/17352 (1%)] Loss: -113881.453125\n",
      "Train Epoch: 271 [1536/17352 (9%)] Loss: -114312.664062\n",
      "Train Epoch: 271 [2944/17352 (17%)] Loss: -112506.140625\n",
      "Train Epoch: 271 [4352/17352 (25%)] Loss: -102910.984375\n",
      "Train Epoch: 271 [5760/17352 (33%)] Loss: -139698.031250\n",
      "Train Epoch: 271 [7168/17352 (41%)] Loss: -116205.367188\n",
      "Train Epoch: 271 [8576/17352 (49%)] Loss: -142788.062500\n",
      "Train Epoch: 271 [9984/17352 (58%)] Loss: -108351.453125\n",
      "Train Epoch: 271 [11392/17352 (66%)] Loss: -133360.125000\n",
      "Train Epoch: 271 [12800/17352 (74%)] Loss: -102888.750000\n",
      "Train Epoch: 271 [14208/17352 (82%)] Loss: -157523.125000\n",
      "Train Epoch: 271 [15479/17352 (89%)] Loss: -39804.917969\n",
      "Train Epoch: 271 [16105/17352 (93%)] Loss: -66881.179688\n",
      "Train Epoch: 271 [16810/17352 (97%)] Loss: -49247.960938\n",
      "    epoch          : 271\n",
      "    loss           : -119313.25032115143\n",
      "    val_loss       : -65581.34822998047\n",
      "Train Epoch: 272 [128/17352 (1%)] Loss: -134248.828125\n",
      "Train Epoch: 272 [1536/17352 (9%)] Loss: -155589.421875\n",
      "Train Epoch: 272 [2944/17352 (17%)] Loss: -119002.234375\n",
      "Train Epoch: 272 [4352/17352 (25%)] Loss: -148836.375000\n",
      "Train Epoch: 272 [5760/17352 (33%)] Loss: -173488.218750\n",
      "Train Epoch: 272 [7168/17352 (41%)] Loss: -108402.960938\n",
      "Train Epoch: 272 [8576/17352 (49%)] Loss: -123069.937500\n",
      "Train Epoch: 272 [9984/17352 (58%)] Loss: -134169.140625\n",
      "Train Epoch: 272 [11392/17352 (66%)] Loss: -156096.781250\n",
      "Train Epoch: 272 [12800/17352 (74%)] Loss: -161237.546875\n",
      "Train Epoch: 272 [14208/17352 (82%)] Loss: -130211.750000\n",
      "Train Epoch: 272 [15542/17352 (90%)] Loss: -73656.375000\n",
      "Train Epoch: 272 [16395/17352 (94%)] Loss: -129450.757812\n",
      "Train Epoch: 272 [17080/17352 (98%)] Loss: -120311.875000\n",
      "    epoch          : 272\n",
      "    loss           : -126205.98876297714\n",
      "    val_loss       : -66330.60782877605\n",
      "Train Epoch: 273 [128/17352 (1%)] Loss: -156693.203125\n",
      "Train Epoch: 273 [1536/17352 (9%)] Loss: -130296.125000\n",
      "Train Epoch: 273 [2944/17352 (17%)] Loss: -117674.687500\n",
      "Train Epoch: 273 [4352/17352 (25%)] Loss: -141553.562500\n",
      "Train Epoch: 273 [5760/17352 (33%)] Loss: -133224.375000\n",
      "Train Epoch: 273 [7168/17352 (41%)] Loss: -89258.593750\n",
      "Train Epoch: 273 [8576/17352 (49%)] Loss: -168532.687500\n",
      "Train Epoch: 273 [9984/17352 (58%)] Loss: -117985.937500\n",
      "Train Epoch: 273 [11392/17352 (66%)] Loss: -128086.328125\n",
      "Train Epoch: 273 [12800/17352 (74%)] Loss: -166870.046875\n",
      "Train Epoch: 273 [14208/17352 (82%)] Loss: -142395.937500\n",
      "Train Epoch: 273 [15547/17352 (90%)] Loss: -131016.281250\n",
      "Train Epoch: 273 [16210/17352 (93%)] Loss: -59868.476562\n",
      "Train Epoch: 273 [17010/17352 (98%)] Loss: -85397.031250\n",
      "    epoch          : 273\n",
      "    loss           : -122574.93778182677\n",
      "    val_loss       : -67492.81650390624\n",
      "Train Epoch: 274 [128/17352 (1%)] Loss: -145015.609375\n",
      "Train Epoch: 274 [1536/17352 (9%)] Loss: -113416.203125\n",
      "Train Epoch: 274 [2944/17352 (17%)] Loss: -111498.468750\n",
      "Train Epoch: 274 [4352/17352 (25%)] Loss: -134603.062500\n",
      "Train Epoch: 274 [5760/17352 (33%)] Loss: -162773.531250\n",
      "Train Epoch: 274 [7168/17352 (41%)] Loss: -126822.367188\n",
      "Train Epoch: 274 [8576/17352 (49%)] Loss: -172288.765625\n",
      "Train Epoch: 274 [9984/17352 (58%)] Loss: -143615.171875\n",
      "Train Epoch: 274 [11392/17352 (66%)] Loss: -95616.328125\n",
      "Train Epoch: 274 [12800/17352 (74%)] Loss: -128785.398438\n",
      "Train Epoch: 274 [14208/17352 (82%)] Loss: -130088.484375\n",
      "Train Epoch: 274 [15510/17352 (89%)] Loss: -64607.125000\n",
      "Train Epoch: 274 [16265/17352 (94%)] Loss: -89064.406250\n",
      "Train Epoch: 274 [16922/17352 (98%)] Loss: -4928.071289\n",
      "    epoch          : 274\n",
      "    loss           : -117838.36737429215\n",
      "    val_loss       : -49172.26791992188\n",
      "Train Epoch: 275 [128/17352 (1%)] Loss: -86282.523438\n",
      "Train Epoch: 275 [1536/17352 (9%)] Loss: -142963.625000\n",
      "Train Epoch: 275 [2944/17352 (17%)] Loss: -109460.671875\n",
      "Train Epoch: 275 [4352/17352 (25%)] Loss: -158185.906250\n",
      "Train Epoch: 275 [5760/17352 (33%)] Loss: -128238.828125\n",
      "Train Epoch: 275 [7168/17352 (41%)] Loss: -103744.765625\n",
      "Train Epoch: 275 [8576/17352 (49%)] Loss: -148901.343750\n",
      "Train Epoch: 275 [9984/17352 (58%)] Loss: -154309.250000\n",
      "Train Epoch: 275 [11392/17352 (66%)] Loss: -103739.859375\n",
      "Train Epoch: 275 [12800/17352 (74%)] Loss: -146557.484375\n",
      "Train Epoch: 275 [14208/17352 (82%)] Loss: -147794.656250\n",
      "Train Epoch: 275 [15417/17352 (89%)] Loss: -3402.539062\n",
      "Train Epoch: 275 [16219/17352 (93%)] Loss: -88971.914062\n",
      "Train Epoch: 275 [16960/17352 (98%)] Loss: -79080.851562\n",
      "    epoch          : 275\n",
      "    loss           : -120416.03645396393\n",
      "    val_loss       : -68429.56934407553\n",
      "Train Epoch: 276 [128/17352 (1%)] Loss: -143730.031250\n",
      "Train Epoch: 276 [1536/17352 (9%)] Loss: -108781.218750\n",
      "Train Epoch: 276 [2944/17352 (17%)] Loss: -161503.125000\n",
      "Train Epoch: 276 [4352/17352 (25%)] Loss: -118479.953125\n",
      "Train Epoch: 276 [5760/17352 (33%)] Loss: -134370.046875\n",
      "Train Epoch: 276 [7168/17352 (41%)] Loss: -123441.062500\n",
      "Train Epoch: 276 [8576/17352 (49%)] Loss: -125257.421875\n",
      "Train Epoch: 276 [9984/17352 (58%)] Loss: -108421.890625\n",
      "Train Epoch: 276 [11392/17352 (66%)] Loss: -106115.046875\n",
      "Train Epoch: 276 [12800/17352 (74%)] Loss: -138862.656250\n",
      "Train Epoch: 276 [14208/17352 (82%)] Loss: -101100.218750\n",
      "Train Epoch: 276 [15529/17352 (89%)] Loss: -99076.984375\n",
      "Train Epoch: 276 [16184/17352 (93%)] Loss: -56910.769531\n",
      "Train Epoch: 276 [16869/17352 (97%)] Loss: -69936.726562\n",
      "    epoch          : 276\n",
      "    loss           : -116989.60392525692\n",
      "    val_loss       : -58354.54158528646\n",
      "Train Epoch: 277 [128/17352 (1%)] Loss: -135108.718750\n",
      "Train Epoch: 277 [1536/17352 (9%)] Loss: -123092.257812\n",
      "Train Epoch: 277 [2944/17352 (17%)] Loss: -107445.859375\n",
      "Train Epoch: 277 [4352/17352 (25%)] Loss: -138579.515625\n",
      "Train Epoch: 277 [5760/17352 (33%)] Loss: -175221.437500\n",
      "Train Epoch: 277 [7168/17352 (41%)] Loss: -138030.812500\n",
      "Train Epoch: 277 [8576/17352 (49%)] Loss: -71992.367188\n",
      "Train Epoch: 277 [9984/17352 (58%)] Loss: -100313.195312\n",
      "Train Epoch: 277 [11392/17352 (66%)] Loss: -105904.507812\n",
      "Train Epoch: 277 [12800/17352 (74%)] Loss: -120135.632812\n",
      "Train Epoch: 277 [14208/17352 (82%)] Loss: -132658.250000\n",
      "Train Epoch: 277 [15498/17352 (89%)] Loss: -51547.839844\n",
      "Train Epoch: 277 [16266/17352 (94%)] Loss: -131514.750000\n",
      "Train Epoch: 277 [16994/17352 (98%)] Loss: -53083.023438\n",
      "    epoch          : 277\n",
      "    loss           : -115953.99427170721\n",
      "    val_loss       : -59844.88883870443\n",
      "Train Epoch: 278 [128/17352 (1%)] Loss: -144347.953125\n",
      "Train Epoch: 278 [1536/17352 (9%)] Loss: -136308.906250\n",
      "Train Epoch: 278 [2944/17352 (17%)] Loss: -91232.132812\n",
      "Train Epoch: 278 [4352/17352 (25%)] Loss: -111151.265625\n",
      "Train Epoch: 278 [5760/17352 (33%)] Loss: -154998.640625\n",
      "Train Epoch: 278 [7168/17352 (41%)] Loss: -103719.140625\n",
      "Train Epoch: 278 [8576/17352 (49%)] Loss: -111010.554688\n",
      "Train Epoch: 278 [9984/17352 (58%)] Loss: -137864.328125\n",
      "Train Epoch: 278 [11392/17352 (66%)] Loss: -110021.625000\n",
      "Train Epoch: 278 [12800/17352 (74%)] Loss: -128755.992188\n",
      "Train Epoch: 278 [14208/17352 (82%)] Loss: -161737.750000\n",
      "Train Epoch: 278 [15480/17352 (89%)] Loss: -97144.968750\n",
      "Train Epoch: 278 [16378/17352 (94%)] Loss: -141868.437500\n",
      "Train Epoch: 278 [16967/17352 (98%)] Loss: -41834.210938\n",
      "    epoch          : 278\n",
      "    loss           : -119590.40120988885\n",
      "    val_loss       : -66474.72714436849\n",
      "Train Epoch: 279 [128/17352 (1%)] Loss: -123781.351562\n",
      "Train Epoch: 279 [1536/17352 (9%)] Loss: -103202.890625\n",
      "Train Epoch: 279 [2944/17352 (17%)] Loss: -100058.054688\n",
      "Train Epoch: 279 [4352/17352 (25%)] Loss: -59150.519531\n",
      "Train Epoch: 279 [5760/17352 (33%)] Loss: -79667.093750\n",
      "Train Epoch: 279 [7168/17352 (41%)] Loss: -115167.125000\n",
      "Train Epoch: 279 [8576/17352 (49%)] Loss: -90887.398438\n",
      "Train Epoch: 279 [9984/17352 (58%)] Loss: -161526.187500\n",
      "Train Epoch: 279 [11392/17352 (66%)] Loss: -104360.226562\n",
      "Train Epoch: 279 [12800/17352 (74%)] Loss: -103528.992188\n",
      "Train Epoch: 279 [14208/17352 (82%)] Loss: -163419.218750\n",
      "Train Epoch: 279 [15453/17352 (89%)] Loss: -101903.203125\n",
      "Train Epoch: 279 [16290/17352 (94%)] Loss: -18006.345703\n",
      "Train Epoch: 279 [16990/17352 (98%)] Loss: -126950.109375\n",
      "    epoch          : 279\n",
      "    loss           : -106909.59062041213\n",
      "    val_loss       : -64750.32244059245\n",
      "Train Epoch: 280 [128/17352 (1%)] Loss: -113278.554688\n",
      "Train Epoch: 280 [1536/17352 (9%)] Loss: -134206.250000\n",
      "Train Epoch: 280 [2944/17352 (17%)] Loss: -135580.750000\n",
      "Train Epoch: 280 [4352/17352 (25%)] Loss: -108641.578125\n",
      "Train Epoch: 280 [5760/17352 (33%)] Loss: -107277.125000\n",
      "Train Epoch: 280 [7168/17352 (41%)] Loss: -130825.601562\n",
      "Train Epoch: 280 [8576/17352 (49%)] Loss: -165502.609375\n",
      "Train Epoch: 280 [9984/17352 (58%)] Loss: -160916.687500\n",
      "Train Epoch: 280 [11392/17352 (66%)] Loss: -67731.234375\n",
      "Train Epoch: 280 [12800/17352 (74%)] Loss: -113611.023438\n",
      "Train Epoch: 280 [14208/17352 (82%)] Loss: -146280.968750\n",
      "Train Epoch: 280 [15449/17352 (89%)] Loss: -101300.929688\n",
      "Train Epoch: 280 [16278/17352 (94%)] Loss: -99487.375000\n",
      "Train Epoch: 280 [17058/17352 (98%)] Loss: -76593.187500\n",
      "    epoch          : 280\n",
      "    loss           : -122587.7424889891\n",
      "    val_loss       : -69051.48110351563\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch280.pth ...\n",
      "Train Epoch: 281 [128/17352 (1%)] Loss: -111679.671875\n",
      "Train Epoch: 281 [1536/17352 (9%)] Loss: -125610.570312\n",
      "Train Epoch: 281 [2944/17352 (17%)] Loss: -125273.796875\n",
      "Train Epoch: 281 [4352/17352 (25%)] Loss: -151322.578125\n",
      "Train Epoch: 281 [5760/17352 (33%)] Loss: -104380.921875\n",
      "Train Epoch: 281 [7168/17352 (41%)] Loss: -155285.671875\n",
      "Train Epoch: 281 [8576/17352 (49%)] Loss: -151567.687500\n",
      "Train Epoch: 281 [9984/17352 (58%)] Loss: -149671.890625\n",
      "Train Epoch: 281 [11392/17352 (66%)] Loss: -127611.500000\n",
      "Train Epoch: 281 [12800/17352 (74%)] Loss: -153494.296875\n",
      "Train Epoch: 281 [14208/17352 (82%)] Loss: -151392.734375\n",
      "Train Epoch: 281 [15592/17352 (90%)] Loss: -141395.312500\n",
      "Train Epoch: 281 [16192/17352 (93%)] Loss: -53153.906250\n",
      "Train Epoch: 281 [17006/17352 (98%)] Loss: -116479.289062\n",
      "    epoch          : 281\n",
      "    loss           : -121526.11466089031\n",
      "    val_loss       : -65209.389611816405\n",
      "Train Epoch: 282 [128/17352 (1%)] Loss: -150017.078125\n",
      "Train Epoch: 282 [1536/17352 (9%)] Loss: -139349.468750\n",
      "Train Epoch: 282 [2944/17352 (17%)] Loss: -150098.046875\n",
      "Train Epoch: 282 [4352/17352 (25%)] Loss: -151597.671875\n",
      "Train Epoch: 282 [5760/17352 (33%)] Loss: -177107.546875\n",
      "Train Epoch: 282 [7168/17352 (41%)] Loss: -158742.375000\n",
      "Train Epoch: 282 [8576/17352 (49%)] Loss: -93971.210938\n",
      "Train Epoch: 282 [9984/17352 (58%)] Loss: -130972.062500\n",
      "Train Epoch: 282 [11392/17352 (66%)] Loss: -112168.710938\n",
      "Train Epoch: 282 [12800/17352 (74%)] Loss: -74808.523438\n",
      "Train Epoch: 282 [14208/17352 (82%)] Loss: -152402.812500\n",
      "Train Epoch: 282 [15498/17352 (89%)] Loss: -57055.484375\n",
      "Train Epoch: 282 [16347/17352 (94%)] Loss: -73045.328125\n",
      "Train Epoch: 282 [16922/17352 (98%)] Loss: -15038.240234\n",
      "    epoch          : 282\n",
      "    loss           : -115995.89365955327\n",
      "    val_loss       : -55801.27882486979\n",
      "Train Epoch: 283 [128/17352 (1%)] Loss: -136234.250000\n",
      "Train Epoch: 283 [1536/17352 (9%)] Loss: -129476.421875\n",
      "Train Epoch: 283 [2944/17352 (17%)] Loss: -132871.109375\n",
      "Train Epoch: 283 [4352/17352 (25%)] Loss: -138667.234375\n",
      "Train Epoch: 283 [5760/17352 (33%)] Loss: -111445.125000\n",
      "Train Epoch: 283 [7168/17352 (41%)] Loss: -148905.687500\n",
      "Train Epoch: 283 [8576/17352 (49%)] Loss: -157806.218750\n",
      "Train Epoch: 283 [9984/17352 (58%)] Loss: -165167.171875\n",
      "Train Epoch: 283 [11392/17352 (66%)] Loss: -144557.859375\n",
      "Train Epoch: 283 [12800/17352 (74%)] Loss: -119887.085938\n",
      "Train Epoch: 283 [14208/17352 (82%)] Loss: -153294.640625\n",
      "Train Epoch: 283 [15478/17352 (89%)] Loss: -90510.820312\n",
      "Train Epoch: 283 [16311/17352 (94%)] Loss: -44099.484375\n",
      "Train Epoch: 283 [16981/17352 (98%)] Loss: -98101.578125\n",
      "    epoch          : 283\n",
      "    loss           : -118890.78509562448\n",
      "    val_loss       : -70599.85075683593\n",
      "Train Epoch: 284 [128/17352 (1%)] Loss: -138319.531250\n",
      "Train Epoch: 284 [1536/17352 (9%)] Loss: -128064.570312\n",
      "Train Epoch: 284 [2944/17352 (17%)] Loss: -115708.281250\n",
      "Train Epoch: 284 [4352/17352 (25%)] Loss: -123981.843750\n",
      "Train Epoch: 284 [5760/17352 (33%)] Loss: -162417.125000\n",
      "Train Epoch: 284 [7168/17352 (41%)] Loss: -144901.609375\n",
      "Train Epoch: 284 [8576/17352 (49%)] Loss: -140985.921875\n",
      "Train Epoch: 284 [9984/17352 (58%)] Loss: -117540.593750\n",
      "Train Epoch: 284 [11392/17352 (66%)] Loss: -121571.484375\n",
      "Train Epoch: 284 [12800/17352 (74%)] Loss: -147498.468750\n",
      "Train Epoch: 284 [14208/17352 (82%)] Loss: -132179.328125\n",
      "Train Epoch: 284 [15455/17352 (89%)] Loss: -24482.259766\n",
      "Train Epoch: 284 [16128/17352 (93%)] Loss: -39663.953125\n",
      "Train Epoch: 284 [16965/17352 (98%)] Loss: -105713.960938\n",
      "    epoch          : 284\n",
      "    loss           : -119799.01779768772\n",
      "    val_loss       : -55564.75157877604\n",
      "Train Epoch: 285 [128/17352 (1%)] Loss: -111109.570312\n",
      "Train Epoch: 285 [1536/17352 (9%)] Loss: -115850.031250\n",
      "Train Epoch: 285 [2944/17352 (17%)] Loss: -163663.218750\n",
      "Train Epoch: 285 [4352/17352 (25%)] Loss: -159432.578125\n",
      "Train Epoch: 285 [5760/17352 (33%)] Loss: -130400.007812\n",
      "Train Epoch: 285 [7168/17352 (41%)] Loss: -132512.968750\n",
      "Train Epoch: 285 [8576/17352 (49%)] Loss: -103852.328125\n",
      "Train Epoch: 285 [9984/17352 (58%)] Loss: -132990.875000\n",
      "Train Epoch: 285 [11392/17352 (66%)] Loss: -100157.531250\n",
      "Train Epoch: 285 [12800/17352 (74%)] Loss: -120275.476562\n",
      "Train Epoch: 285 [14208/17352 (82%)] Loss: -134507.093750\n",
      "Train Epoch: 285 [15530/17352 (89%)] Loss: -96207.117188\n",
      "Train Epoch: 285 [16295/17352 (94%)] Loss: -70904.390625\n",
      "Train Epoch: 285 [16939/17352 (98%)] Loss: -15589.332031\n",
      "    epoch          : 285\n",
      "    loss           : -120160.29210033032\n",
      "    val_loss       : -63765.49569905599\n",
      "Train Epoch: 286 [128/17352 (1%)] Loss: -137547.390625\n",
      "Train Epoch: 286 [1536/17352 (9%)] Loss: -144484.265625\n",
      "Train Epoch: 286 [2944/17352 (17%)] Loss: -139725.062500\n",
      "Train Epoch: 286 [4352/17352 (25%)] Loss: -102256.898438\n",
      "Train Epoch: 286 [5760/17352 (33%)] Loss: -169413.765625\n",
      "Train Epoch: 286 [7168/17352 (41%)] Loss: -127169.234375\n",
      "Train Epoch: 286 [8576/17352 (49%)] Loss: -165565.781250\n",
      "Train Epoch: 286 [9984/17352 (58%)] Loss: -137559.406250\n",
      "Train Epoch: 286 [11392/17352 (66%)] Loss: -165723.078125\n",
      "Train Epoch: 286 [12800/17352 (74%)] Loss: -127617.078125\n",
      "Train Epoch: 286 [14208/17352 (82%)] Loss: -130721.328125\n",
      "Train Epoch: 286 [15545/17352 (90%)] Loss: -104773.273438\n",
      "Train Epoch: 286 [16325/17352 (94%)] Loss: -106797.843750\n",
      "Train Epoch: 286 [16991/17352 (98%)] Loss: -2575.524902\n",
      "    epoch          : 286\n",
      "    loss           : -128641.11043348888\n",
      "    val_loss       : -18011.88380940755\n",
      "Train Epoch: 287 [128/17352 (1%)] Loss: -25573.878906\n",
      "Train Epoch: 287 [1536/17352 (9%)] Loss: -81277.257812\n",
      "Train Epoch: 287 [2944/17352 (17%)] Loss: -118838.921875\n",
      "Train Epoch: 287 [4352/17352 (25%)] Loss: -146109.343750\n",
      "Train Epoch: 287 [5760/17352 (33%)] Loss: -123273.890625\n",
      "Train Epoch: 287 [7168/17352 (41%)] Loss: -118179.296875\n",
      "Train Epoch: 287 [8576/17352 (49%)] Loss: -157086.031250\n",
      "Train Epoch: 287 [9984/17352 (58%)] Loss: -160656.421875\n",
      "Train Epoch: 287 [11392/17352 (66%)] Loss: -108508.093750\n",
      "Train Epoch: 287 [12800/17352 (74%)] Loss: -174089.781250\n",
      "Train Epoch: 287 [14208/17352 (82%)] Loss: -155189.093750\n",
      "Train Epoch: 287 [15596/17352 (90%)] Loss: -103553.406250\n",
      "Train Epoch: 287 [16251/17352 (94%)] Loss: -16092.888672\n",
      "Train Epoch: 287 [17042/17352 (98%)] Loss: -101446.296875\n",
      "    epoch          : 287\n",
      "    loss           : -111891.15857507079\n",
      "    val_loss       : -66767.80380452474\n",
      "Train Epoch: 288 [128/17352 (1%)] Loss: -112740.296875\n",
      "Train Epoch: 288 [1536/17352 (9%)] Loss: -124384.859375\n",
      "Train Epoch: 288 [2944/17352 (17%)] Loss: -150673.468750\n",
      "Train Epoch: 288 [4352/17352 (25%)] Loss: -131328.812500\n",
      "Train Epoch: 288 [5760/17352 (33%)] Loss: -143997.390625\n",
      "Train Epoch: 288 [7168/17352 (41%)] Loss: -109279.828125\n",
      "Train Epoch: 288 [8576/17352 (49%)] Loss: -111378.375000\n",
      "Train Epoch: 288 [9984/17352 (58%)] Loss: -110423.218750\n",
      "Train Epoch: 288 [11392/17352 (66%)] Loss: -135466.000000\n",
      "Train Epoch: 288 [12800/17352 (74%)] Loss: -138663.812500\n",
      "Train Epoch: 288 [14208/17352 (82%)] Loss: -95805.312500\n",
      "Train Epoch: 288 [15532/17352 (90%)] Loss: -102304.460938\n",
      "Train Epoch: 288 [16179/17352 (93%)] Loss: -43075.222656\n",
      "Train Epoch: 288 [17105/17352 (99%)] Loss: -44731.312500\n",
      "    epoch          : 288\n",
      "    loss           : -116439.45517971371\n",
      "    val_loss       : -64267.83604736328\n",
      "Train Epoch: 289 [128/17352 (1%)] Loss: -141388.296875\n",
      "Train Epoch: 289 [1536/17352 (9%)] Loss: -145282.703125\n",
      "Train Epoch: 289 [2944/17352 (17%)] Loss: -182753.296875\n",
      "Train Epoch: 289 [4352/17352 (25%)] Loss: -148774.640625\n",
      "Train Epoch: 289 [5760/17352 (33%)] Loss: -168398.046875\n",
      "Train Epoch: 289 [7168/17352 (41%)] Loss: -136097.250000\n",
      "Train Epoch: 289 [8576/17352 (49%)] Loss: -134944.625000\n",
      "Train Epoch: 289 [9984/17352 (58%)] Loss: -110433.148438\n",
      "Train Epoch: 289 [11392/17352 (66%)] Loss: -136158.625000\n",
      "Train Epoch: 289 [12800/17352 (74%)] Loss: -128775.984375\n",
      "Train Epoch: 289 [14208/17352 (82%)] Loss: -138070.000000\n",
      "Train Epoch: 289 [15521/17352 (89%)] Loss: -164761.750000\n",
      "Train Epoch: 289 [16267/17352 (94%)] Loss: -107223.875000\n",
      "Train Epoch: 289 [17038/17352 (98%)] Loss: -43191.343750\n",
      "    epoch          : 289\n",
      "    loss           : -117636.96846981176\n",
      "    val_loss       : -67183.40604248046\n",
      "Train Epoch: 290 [128/17352 (1%)] Loss: -135167.937500\n",
      "Train Epoch: 290 [1536/17352 (9%)] Loss: -114126.078125\n",
      "Train Epoch: 290 [2944/17352 (17%)] Loss: -130623.906250\n",
      "Train Epoch: 290 [4352/17352 (25%)] Loss: -121419.609375\n",
      "Train Epoch: 290 [5760/17352 (33%)] Loss: -139630.234375\n",
      "Train Epoch: 290 [7168/17352 (41%)] Loss: -141242.781250\n",
      "Train Epoch: 290 [8576/17352 (49%)] Loss: -127433.281250\n",
      "Train Epoch: 290 [9984/17352 (58%)] Loss: -127013.742188\n",
      "Train Epoch: 290 [11392/17352 (66%)] Loss: -177403.437500\n",
      "Train Epoch: 290 [12800/17352 (74%)] Loss: -122431.781250\n",
      "Train Epoch: 290 [14208/17352 (82%)] Loss: -120329.296875\n",
      "Train Epoch: 290 [15378/17352 (89%)] Loss: -10057.944336\n",
      "Train Epoch: 290 [16231/17352 (94%)] Loss: -71458.750000\n",
      "Train Epoch: 290 [17053/17352 (98%)] Loss: -30396.931641\n",
      "    epoch          : 290\n",
      "    loss           : -122396.91964004823\n",
      "    val_loss       : -61361.18309733073\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch290.pth ...\n",
      "Train Epoch: 291 [128/17352 (1%)] Loss: -108001.593750\n",
      "Train Epoch: 291 [1536/17352 (9%)] Loss: -152856.750000\n",
      "Train Epoch: 291 [2944/17352 (17%)] Loss: -141396.609375\n",
      "Train Epoch: 291 [4352/17352 (25%)] Loss: -176156.281250\n",
      "Train Epoch: 291 [5760/17352 (33%)] Loss: -142882.312500\n",
      "Train Epoch: 291 [7168/17352 (41%)] Loss: -103769.421875\n",
      "Train Epoch: 291 [8576/17352 (49%)] Loss: -162628.718750\n",
      "Train Epoch: 291 [9984/17352 (58%)] Loss: -136592.078125\n",
      "Train Epoch: 291 [11392/17352 (66%)] Loss: -140596.218750\n",
      "Train Epoch: 291 [12800/17352 (74%)] Loss: -106014.953125\n",
      "Train Epoch: 291 [14208/17352 (82%)] Loss: -124416.976562\n",
      "Train Epoch: 291 [15510/17352 (89%)] Loss: -52536.144531\n",
      "Train Epoch: 291 [16375/17352 (94%)] Loss: -29801.521484\n",
      "Train Epoch: 291 [17001/17352 (98%)] Loss: -113554.562500\n",
      "    epoch          : 291\n",
      "    loss           : -122716.87924050965\n",
      "    val_loss       : -66396.88605143229\n",
      "Train Epoch: 292 [128/17352 (1%)] Loss: -121402.125000\n",
      "Train Epoch: 292 [1536/17352 (9%)] Loss: -144425.843750\n",
      "Train Epoch: 292 [2944/17352 (17%)] Loss: -135971.875000\n",
      "Train Epoch: 292 [4352/17352 (25%)] Loss: -153072.156250\n",
      "Train Epoch: 292 [5760/17352 (33%)] Loss: -170564.109375\n",
      "Train Epoch: 292 [7168/17352 (41%)] Loss: -162162.406250\n",
      "Train Epoch: 292 [8576/17352 (49%)] Loss: -161659.140625\n",
      "Train Epoch: 292 [9984/17352 (58%)] Loss: -134588.000000\n",
      "Train Epoch: 292 [11392/17352 (66%)] Loss: -164652.765625\n",
      "Train Epoch: 292 [12800/17352 (74%)] Loss: -127593.210938\n",
      "Train Epoch: 292 [14208/17352 (82%)] Loss: -129577.945312\n",
      "Train Epoch: 292 [15596/17352 (90%)] Loss: -154297.843750\n",
      "Train Epoch: 292 [16122/17352 (93%)] Loss: -102328.414062\n",
      "Train Epoch: 292 [16956/17352 (98%)] Loss: -117106.078125\n",
      "    epoch          : 292\n",
      "    loss           : -118148.34981746801\n",
      "    val_loss       : -52056.08580322265\n",
      "Train Epoch: 293 [128/17352 (1%)] Loss: -94031.578125\n",
      "Train Epoch: 293 [1536/17352 (9%)] Loss: -130100.890625\n",
      "Train Epoch: 293 [2944/17352 (17%)] Loss: -123131.414062\n",
      "Train Epoch: 293 [4352/17352 (25%)] Loss: -120284.710938\n",
      "Train Epoch: 293 [5760/17352 (33%)] Loss: -118597.859375\n",
      "Train Epoch: 293 [7168/17352 (41%)] Loss: -155166.875000\n",
      "Train Epoch: 293 [8576/17352 (49%)] Loss: -123497.453125\n",
      "Train Epoch: 293 [9984/17352 (58%)] Loss: -144344.578125\n",
      "Train Epoch: 293 [11392/17352 (66%)] Loss: -145266.671875\n",
      "Train Epoch: 293 [12800/17352 (74%)] Loss: -157701.750000\n",
      "Train Epoch: 293 [14208/17352 (82%)] Loss: -142497.328125\n",
      "Train Epoch: 293 [15557/17352 (90%)] Loss: -81088.117188\n",
      "Train Epoch: 293 [16380/17352 (94%)] Loss: -72106.609375\n",
      "Train Epoch: 293 [17088/17352 (98%)] Loss: -99127.992188\n",
      "    epoch          : 293\n",
      "    loss           : -123378.58893272861\n",
      "    val_loss       : -70625.15201416015\n",
      "Train Epoch: 294 [128/17352 (1%)] Loss: -174077.593750\n",
      "Train Epoch: 294 [1536/17352 (9%)] Loss: -127064.039062\n",
      "Train Epoch: 294 [2944/17352 (17%)] Loss: -168339.843750\n",
      "Train Epoch: 294 [4352/17352 (25%)] Loss: -157403.156250\n",
      "Train Epoch: 294 [5760/17352 (33%)] Loss: -120444.093750\n",
      "Train Epoch: 294 [7168/17352 (41%)] Loss: -124827.804688\n",
      "Train Epoch: 294 [8576/17352 (49%)] Loss: -132854.578125\n",
      "Train Epoch: 294 [9984/17352 (58%)] Loss: -121908.859375\n",
      "Train Epoch: 294 [11392/17352 (66%)] Loss: -133211.046875\n",
      "Train Epoch: 294 [12800/17352 (74%)] Loss: -168702.406250\n",
      "Train Epoch: 294 [14208/17352 (82%)] Loss: -130753.187500\n",
      "Train Epoch: 294 [15460/17352 (89%)] Loss: -99863.312500\n",
      "Train Epoch: 294 [16385/17352 (94%)] Loss: -57879.355469\n",
      "Train Epoch: 294 [17040/17352 (98%)] Loss: -51344.230469\n",
      "    epoch          : 294\n",
      "    loss           : -122995.13027442062\n",
      "    val_loss       : -65915.28871663411\n",
      "Train Epoch: 295 [128/17352 (1%)] Loss: -109066.570312\n",
      "Train Epoch: 295 [1536/17352 (9%)] Loss: -150142.265625\n",
      "Train Epoch: 295 [2944/17352 (17%)] Loss: -166608.875000\n",
      "Train Epoch: 295 [4352/17352 (25%)] Loss: -136123.250000\n",
      "Train Epoch: 295 [5760/17352 (33%)] Loss: -142059.093750\n",
      "Train Epoch: 295 [7168/17352 (41%)] Loss: -137841.578125\n",
      "Train Epoch: 295 [8576/17352 (49%)] Loss: -151834.234375\n",
      "Train Epoch: 295 [9984/17352 (58%)] Loss: -124898.093750\n",
      "Train Epoch: 295 [11392/17352 (66%)] Loss: -122581.546875\n",
      "Train Epoch: 295 [12800/17352 (74%)] Loss: -137056.593750\n",
      "Train Epoch: 295 [14208/17352 (82%)] Loss: -121219.937500\n",
      "Train Epoch: 295 [15478/17352 (89%)] Loss: -55556.855469\n",
      "Train Epoch: 295 [16330/17352 (94%)] Loss: -31523.046875\n",
      "Train Epoch: 295 [17039/17352 (98%)] Loss: -39348.703125\n",
      "    epoch          : 295\n",
      "    loss           : -121987.57934734166\n",
      "    val_loss       : -68677.57069091797\n",
      "Train Epoch: 296 [128/17352 (1%)] Loss: -137114.750000\n",
      "Train Epoch: 296 [1536/17352 (9%)] Loss: -66613.148438\n",
      "Train Epoch: 296 [2944/17352 (17%)] Loss: -120717.750000\n",
      "Train Epoch: 296 [4352/17352 (25%)] Loss: -102392.281250\n",
      "Train Epoch: 296 [5760/17352 (33%)] Loss: -79996.539062\n",
      "Train Epoch: 296 [7168/17352 (41%)] Loss: -114375.671875\n",
      "Train Epoch: 296 [8576/17352 (49%)] Loss: -164712.484375\n",
      "Train Epoch: 296 [9984/17352 (58%)] Loss: -103347.265625\n",
      "Train Epoch: 296 [11392/17352 (66%)] Loss: -136172.265625\n",
      "Train Epoch: 296 [12800/17352 (74%)] Loss: -127598.234375\n",
      "Train Epoch: 296 [14208/17352 (82%)] Loss: -99347.250000\n",
      "Train Epoch: 296 [15534/17352 (90%)] Loss: -84953.085938\n",
      "Train Epoch: 296 [16269/17352 (94%)] Loss: -45464.300781\n",
      "Train Epoch: 296 [17064/17352 (98%)] Loss: -81069.539062\n",
      "    epoch          : 296\n",
      "    loss           : -114751.13269780306\n",
      "    val_loss       : -66702.95174560547\n",
      "Train Epoch: 297 [128/17352 (1%)] Loss: -105342.125000\n",
      "Train Epoch: 297 [1536/17352 (9%)] Loss: -146000.093750\n",
      "Train Epoch: 297 [2944/17352 (17%)] Loss: -107538.750000\n",
      "Train Epoch: 297 [4352/17352 (25%)] Loss: -125987.101562\n",
      "Train Epoch: 297 [5760/17352 (33%)] Loss: -153718.031250\n",
      "Train Epoch: 297 [7168/17352 (41%)] Loss: -141401.718750\n",
      "Train Epoch: 297 [8576/17352 (49%)] Loss: -128048.210938\n",
      "Train Epoch: 297 [9984/17352 (58%)] Loss: -119568.242188\n",
      "Train Epoch: 297 [11392/17352 (66%)] Loss: -149474.968750\n",
      "Train Epoch: 297 [12800/17352 (74%)] Loss: -183974.781250\n",
      "Train Epoch: 297 [14208/17352 (82%)] Loss: -146757.343750\n",
      "Train Epoch: 297 [15478/17352 (89%)] Loss: -42274.671875\n",
      "Train Epoch: 297 [16224/17352 (93%)] Loss: -43465.691406\n",
      "Train Epoch: 297 [16960/17352 (98%)] Loss: -104362.632812\n",
      "    epoch          : 297\n",
      "    loss           : -127122.31493485214\n",
      "    val_loss       : -73459.42446289063\n",
      "Train Epoch: 298 [128/17352 (1%)] Loss: -143660.890625\n",
      "Train Epoch: 298 [1536/17352 (9%)] Loss: -169077.828125\n",
      "Train Epoch: 298 [2944/17352 (17%)] Loss: -126651.671875\n",
      "Train Epoch: 298 [4352/17352 (25%)] Loss: -166317.687500\n",
      "Train Epoch: 298 [5760/17352 (33%)] Loss: -86871.601562\n",
      "Train Epoch: 298 [7168/17352 (41%)] Loss: -144561.750000\n",
      "Train Epoch: 298 [8576/17352 (49%)] Loss: -111724.703125\n",
      "Train Epoch: 298 [9984/17352 (58%)] Loss: -171103.437500\n",
      "Train Epoch: 298 [11392/17352 (66%)] Loss: -126306.617188\n",
      "Train Epoch: 298 [12800/17352 (74%)] Loss: -146098.375000\n",
      "Train Epoch: 298 [14208/17352 (82%)] Loss: -167864.515625\n",
      "Train Epoch: 298 [15518/17352 (89%)] Loss: -81222.976562\n",
      "Train Epoch: 298 [16276/17352 (94%)] Loss: -93140.234375\n",
      "Train Epoch: 298 [16941/17352 (98%)] Loss: -65489.070312\n",
      "    epoch          : 298\n",
      "    loss           : -118429.49249062763\n",
      "    val_loss       : -37848.53686116536\n",
      "Train Epoch: 299 [128/17352 (1%)] Loss: -71671.406250\n",
      "Train Epoch: 299 [1536/17352 (9%)] Loss: -103590.539062\n",
      "Train Epoch: 299 [2944/17352 (17%)] Loss: -139076.812500\n",
      "Train Epoch: 299 [4352/17352 (25%)] Loss: -114996.171875\n",
      "Train Epoch: 299 [5760/17352 (33%)] Loss: -147009.609375\n",
      "Train Epoch: 299 [7168/17352 (41%)] Loss: -172858.375000\n",
      "Train Epoch: 299 [8576/17352 (49%)] Loss: -137413.671875\n",
      "Train Epoch: 299 [9984/17352 (58%)] Loss: -148730.234375\n",
      "Train Epoch: 299 [11392/17352 (66%)] Loss: -77907.468750\n",
      "Train Epoch: 299 [12800/17352 (74%)] Loss: -129965.796875\n",
      "Train Epoch: 299 [14208/17352 (82%)] Loss: -47049.789062\n",
      "Train Epoch: 299 [15527/17352 (89%)] Loss: -59999.375000\n",
      "Train Epoch: 299 [16253/17352 (94%)] Loss: -14800.002930\n",
      "Train Epoch: 299 [16992/17352 (98%)] Loss: -89485.750000\n",
      "    epoch          : 299\n",
      "    loss           : -113221.36500498113\n",
      "    val_loss       : -64468.082946777344\n",
      "Train Epoch: 300 [128/17352 (1%)] Loss: -129872.546875\n",
      "Train Epoch: 300 [1536/17352 (9%)] Loss: -118991.843750\n",
      "Train Epoch: 300 [2944/17352 (17%)] Loss: -117581.500000\n",
      "Train Epoch: 300 [4352/17352 (25%)] Loss: -149890.812500\n",
      "Train Epoch: 300 [5760/17352 (33%)] Loss: -142831.250000\n",
      "Train Epoch: 300 [7168/17352 (41%)] Loss: -101560.296875\n",
      "Train Epoch: 300 [8576/17352 (49%)] Loss: -132022.812500\n",
      "Train Epoch: 300 [9984/17352 (58%)] Loss: -114113.375000\n",
      "Train Epoch: 300 [11392/17352 (66%)] Loss: -156586.718750\n",
      "Train Epoch: 300 [12800/17352 (74%)] Loss: -179819.484375\n",
      "Train Epoch: 300 [14208/17352 (82%)] Loss: -109355.437500\n",
      "Train Epoch: 300 [15495/17352 (89%)] Loss: -43732.863281\n",
      "Train Epoch: 300 [16147/17352 (93%)] Loss: -58019.542969\n",
      "Train Epoch: 300 [17046/17352 (98%)] Loss: -73730.507812\n",
      "    epoch          : 300\n",
      "    loss           : -122855.69833066799\n",
      "    val_loss       : -68879.750785319\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [128/17352 (1%)] Loss: -137640.484375\n",
      "Train Epoch: 301 [1536/17352 (9%)] Loss: -153406.140625\n",
      "Train Epoch: 301 [2944/17352 (17%)] Loss: -140615.437500\n",
      "Train Epoch: 301 [4352/17352 (25%)] Loss: -128158.437500\n",
      "Train Epoch: 301 [5760/17352 (33%)] Loss: -122858.242188\n",
      "Train Epoch: 301 [7168/17352 (41%)] Loss: -121184.164062\n",
      "Train Epoch: 301 [8576/17352 (49%)] Loss: -136817.000000\n",
      "Train Epoch: 301 [9984/17352 (58%)] Loss: -136182.531250\n",
      "Train Epoch: 301 [11392/17352 (66%)] Loss: -146689.093750\n",
      "Train Epoch: 301 [12800/17352 (74%)] Loss: -135853.187500\n",
      "Train Epoch: 301 [14208/17352 (82%)] Loss: -141907.937500\n",
      "Train Epoch: 301 [15435/17352 (89%)] Loss: -30399.093750\n",
      "Train Epoch: 301 [16033/17352 (92%)] Loss: -36920.382812\n",
      "Train Epoch: 301 [16976/17352 (98%)] Loss: -96542.257812\n",
      "    epoch          : 301\n",
      "    loss           : -121425.83133487574\n",
      "    val_loss       : -51958.060502115884\n",
      "Train Epoch: 302 [128/17352 (1%)] Loss: -108764.664062\n",
      "Train Epoch: 302 [1536/17352 (9%)] Loss: -111540.875000\n",
      "Train Epoch: 302 [2944/17352 (17%)] Loss: -110916.820312\n",
      "Train Epoch: 302 [4352/17352 (25%)] Loss: -132885.609375\n",
      "Train Epoch: 302 [5760/17352 (33%)] Loss: -159673.968750\n",
      "Train Epoch: 302 [7168/17352 (41%)] Loss: -110921.515625\n",
      "Train Epoch: 302 [8576/17352 (49%)] Loss: -143880.109375\n",
      "Train Epoch: 302 [9984/17352 (58%)] Loss: -154008.421875\n",
      "Train Epoch: 302 [11392/17352 (66%)] Loss: -132061.375000\n",
      "Train Epoch: 302 [12800/17352 (74%)] Loss: -183320.828125\n",
      "Train Epoch: 302 [14208/17352 (82%)] Loss: -159378.031250\n",
      "Train Epoch: 302 [15495/17352 (89%)] Loss: -106871.195312\n",
      "Train Epoch: 302 [16286/17352 (94%)] Loss: -98577.343750\n",
      "Train Epoch: 302 [16956/17352 (98%)] Loss: -49061.382812\n",
      "    epoch          : 302\n",
      "    loss           : -124925.9099137479\n",
      "    val_loss       : -45422.39008789063\n",
      "Train Epoch: 303 [128/17352 (1%)] Loss: -93645.328125\n",
      "Train Epoch: 303 [1536/17352 (9%)] Loss: -92944.359375\n",
      "Train Epoch: 303 [2944/17352 (17%)] Loss: -106770.296875\n",
      "Train Epoch: 303 [4352/17352 (25%)] Loss: -132527.500000\n",
      "Train Epoch: 303 [5760/17352 (33%)] Loss: -149180.500000\n",
      "Train Epoch: 303 [7168/17352 (41%)] Loss: -147565.125000\n",
      "Train Epoch: 303 [8576/17352 (49%)] Loss: -100502.062500\n",
      "Train Epoch: 303 [9984/17352 (58%)] Loss: -156614.812500\n",
      "Train Epoch: 303 [11392/17352 (66%)] Loss: -120736.109375\n",
      "Train Epoch: 303 [12800/17352 (74%)] Loss: -136593.015625\n",
      "Train Epoch: 303 [14208/17352 (82%)] Loss: -108580.421875\n",
      "Train Epoch: 303 [15548/17352 (90%)] Loss: -86511.578125\n",
      "Train Epoch: 303 [16439/17352 (95%)] Loss: -32775.203125\n",
      "Train Epoch: 303 [17106/17352 (99%)] Loss: -34125.734375\n",
      "    epoch          : 303\n",
      "    loss           : -118802.67582712877\n",
      "    val_loss       : -65556.60338541666\n",
      "Train Epoch: 304 [128/17352 (1%)] Loss: -119346.101562\n",
      "Train Epoch: 304 [1536/17352 (9%)] Loss: -151632.718750\n",
      "Train Epoch: 304 [2944/17352 (17%)] Loss: -136333.875000\n",
      "Train Epoch: 304 [4352/17352 (25%)] Loss: -137834.046875\n",
      "Train Epoch: 304 [5760/17352 (33%)] Loss: -100172.289062\n",
      "Train Epoch: 304 [7168/17352 (41%)] Loss: -139400.734375\n",
      "Train Epoch: 304 [8576/17352 (49%)] Loss: -150682.953125\n",
      "Train Epoch: 304 [9984/17352 (58%)] Loss: -144956.968750\n",
      "Train Epoch: 304 [11392/17352 (66%)] Loss: -177312.843750\n",
      "Train Epoch: 304 [12800/17352 (74%)] Loss: -157201.125000\n",
      "Train Epoch: 304 [14208/17352 (82%)] Loss: -135362.765625\n",
      "Train Epoch: 304 [15480/17352 (89%)] Loss: -48022.441406\n",
      "Train Epoch: 304 [16286/17352 (94%)] Loss: -16524.869141\n",
      "Train Epoch: 304 [17053/17352 (98%)] Loss: -49007.031250\n",
      "    epoch          : 304\n",
      "    loss           : -127982.61225225462\n",
      "    val_loss       : -59800.53035888672\n",
      "Train Epoch: 305 [128/17352 (1%)] Loss: -122716.812500\n",
      "Train Epoch: 305 [1536/17352 (9%)] Loss: -163527.843750\n",
      "Train Epoch: 305 [2944/17352 (17%)] Loss: -120044.734375\n",
      "Train Epoch: 305 [4352/17352 (25%)] Loss: -147607.734375\n",
      "Train Epoch: 305 [5760/17352 (33%)] Loss: -114466.062500\n",
      "Train Epoch: 305 [7168/17352 (41%)] Loss: -109560.914062\n",
      "Train Epoch: 305 [8576/17352 (49%)] Loss: -135453.453125\n",
      "Train Epoch: 305 [9984/17352 (58%)] Loss: -126394.929688\n",
      "Train Epoch: 305 [11392/17352 (66%)] Loss: -137314.406250\n",
      "Train Epoch: 305 [12800/17352 (74%)] Loss: -148214.921875\n",
      "Train Epoch: 305 [14208/17352 (82%)] Loss: -122334.265625\n",
      "Train Epoch: 305 [15409/17352 (89%)] Loss: -45484.785156\n",
      "Train Epoch: 305 [16228/17352 (94%)] Loss: -61065.679688\n",
      "Train Epoch: 305 [17043/17352 (98%)] Loss: -51022.082031\n",
      "    epoch          : 305\n",
      "    loss           : -122727.35436274382\n",
      "    val_loss       : -71083.9045654297\n",
      "Train Epoch: 306 [128/17352 (1%)] Loss: -168106.593750\n",
      "Train Epoch: 306 [1536/17352 (9%)] Loss: -146605.953125\n",
      "Train Epoch: 306 [2944/17352 (17%)] Loss: -172464.875000\n",
      "Train Epoch: 306 [4352/17352 (25%)] Loss: -140250.406250\n",
      "Train Epoch: 306 [5760/17352 (33%)] Loss: -144109.671875\n",
      "Train Epoch: 306 [7168/17352 (41%)] Loss: -139912.375000\n",
      "Train Epoch: 306 [8576/17352 (49%)] Loss: -129447.953125\n",
      "Train Epoch: 306 [9984/17352 (58%)] Loss: -136229.343750\n",
      "Train Epoch: 306 [11392/17352 (66%)] Loss: -164247.187500\n",
      "Train Epoch: 306 [12800/17352 (74%)] Loss: -159034.656250\n",
      "Train Epoch: 306 [14208/17352 (82%)] Loss: -130268.492188\n",
      "Train Epoch: 306 [15464/17352 (89%)] Loss: -60723.144531\n",
      "Train Epoch: 306 [16176/17352 (93%)] Loss: -43639.710938\n",
      "Train Epoch: 306 [16870/17352 (97%)] Loss: -74932.867188\n",
      "    epoch          : 306\n",
      "    loss           : -122438.71021654781\n",
      "    val_loss       : -68323.62165120443\n",
      "Train Epoch: 307 [128/17352 (1%)] Loss: -124783.812500\n",
      "Train Epoch: 307 [1536/17352 (9%)] Loss: -145047.484375\n",
      "Train Epoch: 307 [2944/17352 (17%)] Loss: -148440.500000\n",
      "Train Epoch: 307 [4352/17352 (25%)] Loss: -127307.625000\n",
      "Train Epoch: 307 [5760/17352 (33%)] Loss: -94375.937500\n",
      "Train Epoch: 307 [7168/17352 (41%)] Loss: -135101.343750\n",
      "Train Epoch: 307 [8576/17352 (49%)] Loss: -162426.937500\n",
      "Train Epoch: 307 [9984/17352 (58%)] Loss: -147224.796875\n",
      "Train Epoch: 307 [11392/17352 (66%)] Loss: -165840.531250\n",
      "Train Epoch: 307 [12800/17352 (74%)] Loss: -113260.296875\n",
      "Train Epoch: 307 [14208/17352 (82%)] Loss: -166641.140625\n",
      "Train Epoch: 307 [15465/17352 (89%)] Loss: -12500.831055\n",
      "Train Epoch: 307 [16243/17352 (94%)] Loss: -102275.453125\n",
      "Train Epoch: 307 [16921/17352 (98%)] Loss: -79206.656250\n",
      "    epoch          : 307\n",
      "    loss           : -123528.95002982121\n",
      "    val_loss       : -64943.51866455078\n",
      "Train Epoch: 308 [128/17352 (1%)] Loss: -152447.531250\n",
      "Train Epoch: 308 [1536/17352 (9%)] Loss: -109365.257812\n",
      "Train Epoch: 308 [2944/17352 (17%)] Loss: -134424.468750\n",
      "Train Epoch: 308 [4352/17352 (25%)] Loss: -136623.000000\n",
      "Train Epoch: 308 [5760/17352 (33%)] Loss: -147671.906250\n",
      "Train Epoch: 308 [7168/17352 (41%)] Loss: -102196.039062\n",
      "Train Epoch: 308 [8576/17352 (49%)] Loss: -153198.968750\n",
      "Train Epoch: 308 [9984/17352 (58%)] Loss: -134094.000000\n",
      "Train Epoch: 308 [11392/17352 (66%)] Loss: -146895.000000\n",
      "Train Epoch: 308 [12800/17352 (74%)] Loss: -148675.343750\n",
      "Train Epoch: 308 [14208/17352 (82%)] Loss: -149129.156250\n",
      "Train Epoch: 308 [15447/17352 (89%)] Loss: -3627.130371\n",
      "Train Epoch: 308 [16166/17352 (93%)] Loss: -80993.078125\n",
      "Train Epoch: 308 [16970/17352 (98%)] Loss: -28694.876953\n",
      "    epoch          : 308\n",
      "    loss           : -121917.87108391883\n",
      "    val_loss       : -34014.74013671875\n",
      "Train Epoch: 309 [128/17352 (1%)] Loss: -65916.375000\n",
      "Train Epoch: 309 [1536/17352 (9%)] Loss: -110986.320312\n",
      "Train Epoch: 309 [2944/17352 (17%)] Loss: -130671.820312\n",
      "Train Epoch: 309 [4352/17352 (25%)] Loss: -157765.000000\n",
      "Train Epoch: 309 [5760/17352 (33%)] Loss: -131997.281250\n",
      "Train Epoch: 309 [7168/17352 (41%)] Loss: -132984.796875\n",
      "Train Epoch: 309 [8576/17352 (49%)] Loss: -111204.203125\n",
      "Train Epoch: 309 [9984/17352 (58%)] Loss: -132561.375000\n",
      "Train Epoch: 309 [11392/17352 (66%)] Loss: -172705.375000\n",
      "Train Epoch: 309 [12800/17352 (74%)] Loss: -149743.203125\n",
      "Train Epoch: 309 [14208/17352 (82%)] Loss: -132744.171875\n",
      "Train Epoch: 309 [15388/17352 (89%)] Loss: -13947.811523\n",
      "Train Epoch: 309 [16332/17352 (94%)] Loss: -81018.539062\n",
      "Train Epoch: 309 [16876/17352 (97%)] Loss: -108696.718750\n",
      "    epoch          : 309\n",
      "    loss           : -119482.81535431523\n",
      "    val_loss       : -64172.3011311849\n",
      "Train Epoch: 310 [128/17352 (1%)] Loss: -119100.234375\n",
      "Train Epoch: 310 [1536/17352 (9%)] Loss: -133723.687500\n",
      "Train Epoch: 310 [2944/17352 (17%)] Loss: -117453.476562\n",
      "Train Epoch: 310 [4352/17352 (25%)] Loss: -152929.375000\n",
      "Train Epoch: 310 [5760/17352 (33%)] Loss: -154388.468750\n",
      "Train Epoch: 310 [7168/17352 (41%)] Loss: -143213.750000\n",
      "Train Epoch: 310 [8576/17352 (49%)] Loss: -159846.015625\n",
      "Train Epoch: 310 [9984/17352 (58%)] Loss: -107807.023438\n",
      "Train Epoch: 310 [11392/17352 (66%)] Loss: -150802.281250\n",
      "Train Epoch: 310 [12800/17352 (74%)] Loss: -143754.546875\n",
      "Train Epoch: 310 [14208/17352 (82%)] Loss: -185226.828125\n",
      "Train Epoch: 310 [15546/17352 (90%)] Loss: -102237.265625\n",
      "Train Epoch: 310 [16267/17352 (94%)] Loss: -90363.789062\n",
      "Train Epoch: 310 [16986/17352 (98%)] Loss: -35747.984375\n",
      "    epoch          : 310\n",
      "    loss           : -123423.26905443844\n",
      "    val_loss       : -34522.466145833336\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch310.pth ...\n",
      "Train Epoch: 311 [128/17352 (1%)] Loss: -81827.593750\n",
      "Train Epoch: 311 [1536/17352 (9%)] Loss: -76863.414062\n",
      "Train Epoch: 311 [2944/17352 (17%)] Loss: -107494.640625\n",
      "Train Epoch: 311 [4352/17352 (25%)] Loss: -120210.390625\n",
      "Train Epoch: 311 [5760/17352 (33%)] Loss: -87755.890625\n",
      "Train Epoch: 311 [7168/17352 (41%)] Loss: -139604.968750\n",
      "Train Epoch: 311 [8576/17352 (49%)] Loss: -138760.078125\n",
      "Train Epoch: 311 [9984/17352 (58%)] Loss: -131744.484375\n",
      "Train Epoch: 311 [11392/17352 (66%)] Loss: -149096.218750\n",
      "Train Epoch: 311 [12800/17352 (74%)] Loss: -109974.750000\n",
      "Train Epoch: 311 [14208/17352 (82%)] Loss: -167937.375000\n",
      "Train Epoch: 311 [15504/17352 (89%)] Loss: -57238.691406\n",
      "Train Epoch: 311 [16187/17352 (93%)] Loss: -60182.019531\n",
      "Train Epoch: 311 [16932/17352 (98%)] Loss: -99501.710938\n",
      "    epoch          : 311\n",
      "    loss           : -116221.80483824454\n",
      "    val_loss       : -20848.16328938802\n",
      "Train Epoch: 312 [128/17352 (1%)] Loss: -38954.093750\n",
      "Train Epoch: 312 [1536/17352 (9%)] Loss: -74797.703125\n",
      "Train Epoch: 312 [2944/17352 (17%)] Loss: -138119.515625\n",
      "Train Epoch: 312 [4352/17352 (25%)] Loss: -134149.281250\n",
      "Train Epoch: 312 [5760/17352 (33%)] Loss: -126922.953125\n",
      "Train Epoch: 312 [7168/17352 (41%)] Loss: -156763.156250\n",
      "Train Epoch: 312 [8576/17352 (49%)] Loss: -155898.203125\n",
      "Train Epoch: 312 [9984/17352 (58%)] Loss: -105642.390625\n",
      "Train Epoch: 312 [11392/17352 (66%)] Loss: -139621.796875\n",
      "Train Epoch: 312 [12800/17352 (74%)] Loss: -136615.234375\n",
      "Train Epoch: 312 [14208/17352 (82%)] Loss: -120605.796875\n",
      "Train Epoch: 312 [15474/17352 (89%)] Loss: -113866.320312\n",
      "Train Epoch: 312 [16182/17352 (93%)] Loss: -15187.827148\n",
      "Train Epoch: 312 [16969/17352 (98%)] Loss: -43423.500000\n",
      "    epoch          : 312\n",
      "    loss           : -116143.10860161493\n",
      "    val_loss       : -67813.14336751302\n",
      "Train Epoch: 313 [128/17352 (1%)] Loss: -101999.335938\n",
      "Train Epoch: 313 [1536/17352 (9%)] Loss: -148160.265625\n",
      "Train Epoch: 313 [2944/17352 (17%)] Loss: -111481.421875\n",
      "Train Epoch: 313 [4352/17352 (25%)] Loss: -134761.250000\n",
      "Train Epoch: 313 [5760/17352 (33%)] Loss: -110215.828125\n",
      "Train Epoch: 313 [7168/17352 (41%)] Loss: -120500.195312\n",
      "Train Epoch: 313 [8576/17352 (49%)] Loss: -150941.187500\n",
      "Train Epoch: 313 [9984/17352 (58%)] Loss: -164197.906250\n",
      "Train Epoch: 313 [11392/17352 (66%)] Loss: -121400.609375\n",
      "Train Epoch: 313 [12800/17352 (74%)] Loss: -148629.531250\n",
      "Train Epoch: 313 [14208/17352 (82%)] Loss: -158785.984375\n",
      "Train Epoch: 313 [15495/17352 (89%)] Loss: -116208.109375\n",
      "Train Epoch: 313 [16342/17352 (94%)] Loss: -65128.429688\n",
      "Train Epoch: 313 [17135/17352 (99%)] Loss: -67074.187500\n",
      "    epoch          : 313\n",
      "    loss           : -117359.83956028472\n",
      "    val_loss       : -68990.80865885416\n",
      "Train Epoch: 314 [128/17352 (1%)] Loss: -160543.453125\n",
      "Train Epoch: 314 [1536/17352 (9%)] Loss: -143407.328125\n",
      "Train Epoch: 314 [2944/17352 (17%)] Loss: -141446.515625\n",
      "Train Epoch: 314 [4352/17352 (25%)] Loss: -134530.250000\n",
      "Train Epoch: 314 [5760/17352 (33%)] Loss: -124340.992188\n",
      "Train Epoch: 314 [7168/17352 (41%)] Loss: -142778.515625\n",
      "Train Epoch: 314 [8576/17352 (49%)] Loss: -162746.296875\n",
      "Train Epoch: 314 [9984/17352 (58%)] Loss: -132196.937500\n",
      "Train Epoch: 314 [11392/17352 (66%)] Loss: -135177.843750\n",
      "Train Epoch: 314 [12800/17352 (74%)] Loss: -146713.343750\n",
      "Train Epoch: 314 [14208/17352 (82%)] Loss: -125192.398438\n",
      "Train Epoch: 314 [15448/17352 (89%)] Loss: -61574.027344\n",
      "Train Epoch: 314 [16175/17352 (93%)] Loss: -83307.687500\n",
      "Train Epoch: 314 [16968/17352 (98%)] Loss: -76020.781250\n",
      "    epoch          : 314\n",
      "    loss           : -127680.33766778524\n",
      "    val_loss       : -49713.58530680338\n",
      "Train Epoch: 315 [128/17352 (1%)] Loss: -93149.328125\n",
      "Train Epoch: 315 [1536/17352 (9%)] Loss: -164063.718750\n",
      "Train Epoch: 315 [2944/17352 (17%)] Loss: -90795.929688\n",
      "Train Epoch: 315 [4352/17352 (25%)] Loss: -127968.921875\n",
      "Train Epoch: 315 [5760/17352 (33%)] Loss: -153149.000000\n",
      "Train Epoch: 315 [7168/17352 (41%)] Loss: -155832.187500\n",
      "Train Epoch: 315 [8576/17352 (49%)] Loss: -131593.812500\n",
      "Train Epoch: 315 [9984/17352 (58%)] Loss: -158881.593750\n",
      "Train Epoch: 315 [11392/17352 (66%)] Loss: -143908.781250\n",
      "Train Epoch: 315 [12800/17352 (74%)] Loss: -146843.500000\n",
      "Train Epoch: 315 [14208/17352 (82%)] Loss: -110144.140625\n",
      "Train Epoch: 315 [15479/17352 (89%)] Loss: -89118.414062\n",
      "Train Epoch: 315 [16164/17352 (93%)] Loss: -43925.722656\n",
      "Train Epoch: 315 [17042/17352 (98%)] Loss: -36349.656250\n",
      "    epoch          : 315\n",
      "    loss           : -125454.04986039744\n",
      "    val_loss       : -54928.25576578776\n",
      "Train Epoch: 316 [128/17352 (1%)] Loss: -113199.937500\n",
      "Train Epoch: 316 [1536/17352 (9%)] Loss: -152412.390625\n",
      "Train Epoch: 316 [2944/17352 (17%)] Loss: -127368.062500\n",
      "Train Epoch: 316 [4352/17352 (25%)] Loss: -123819.929688\n",
      "Train Epoch: 316 [5760/17352 (33%)] Loss: -117503.343750\n",
      "Train Epoch: 316 [7168/17352 (41%)] Loss: -168293.843750\n",
      "Train Epoch: 316 [8576/17352 (49%)] Loss: -133443.921875\n",
      "Train Epoch: 316 [9984/17352 (58%)] Loss: -141410.625000\n",
      "Train Epoch: 316 [11392/17352 (66%)] Loss: -128069.093750\n",
      "Train Epoch: 316 [12800/17352 (74%)] Loss: -136545.328125\n",
      "Train Epoch: 316 [14208/17352 (82%)] Loss: -133945.218750\n",
      "Train Epoch: 316 [15512/17352 (89%)] Loss: -92463.054688\n",
      "Train Epoch: 316 [16131/17352 (93%)] Loss: -5532.763184\n",
      "Train Epoch: 316 [16994/17352 (98%)] Loss: -116325.914062\n",
      "    epoch          : 316\n",
      "    loss           : -125201.65759523123\n",
      "    val_loss       : -67110.70353597005\n",
      "Train Epoch: 317 [128/17352 (1%)] Loss: -120919.132812\n",
      "Train Epoch: 317 [1536/17352 (9%)] Loss: -145566.796875\n",
      "Train Epoch: 317 [2944/17352 (17%)] Loss: -138725.218750\n",
      "Train Epoch: 317 [4352/17352 (25%)] Loss: -105251.375000\n",
      "Train Epoch: 317 [5760/17352 (33%)] Loss: -141547.125000\n",
      "Train Epoch: 317 [7168/17352 (41%)] Loss: -133598.406250\n",
      "Train Epoch: 317 [8576/17352 (49%)] Loss: -152777.281250\n",
      "Train Epoch: 317 [9984/17352 (58%)] Loss: -156432.562500\n",
      "Train Epoch: 317 [11392/17352 (66%)] Loss: -163312.343750\n",
      "Train Epoch: 317 [12800/17352 (74%)] Loss: -117762.570312\n",
      "Train Epoch: 317 [14208/17352 (82%)] Loss: -125028.250000\n",
      "Train Epoch: 317 [15480/17352 (89%)] Loss: -104894.093750\n",
      "Train Epoch: 317 [16206/17352 (93%)] Loss: -62762.437500\n",
      "Train Epoch: 317 [16978/17352 (98%)] Loss: -127624.195312\n",
      "    epoch          : 317\n",
      "    loss           : -119799.66296796351\n",
      "    val_loss       : -68880.28965657552\n",
      "Train Epoch: 318 [128/17352 (1%)] Loss: -126958.289062\n",
      "Train Epoch: 318 [1536/17352 (9%)] Loss: -122439.679688\n",
      "Train Epoch: 318 [2944/17352 (17%)] Loss: -117203.289062\n",
      "Train Epoch: 318 [4352/17352 (25%)] Loss: -123453.390625\n",
      "Train Epoch: 318 [5760/17352 (33%)] Loss: -127975.992188\n",
      "Train Epoch: 318 [7168/17352 (41%)] Loss: -155690.546875\n",
      "Train Epoch: 318 [8576/17352 (49%)] Loss: -119622.960938\n",
      "Train Epoch: 318 [9984/17352 (58%)] Loss: -134079.671875\n",
      "Train Epoch: 318 [11392/17352 (66%)] Loss: -140526.531250\n",
      "Train Epoch: 318 [12800/17352 (74%)] Loss: -128429.015625\n",
      "Train Epoch: 318 [14208/17352 (82%)] Loss: -139968.125000\n",
      "Train Epoch: 318 [15577/17352 (90%)] Loss: -157808.187500\n",
      "Train Epoch: 318 [16358/17352 (94%)] Loss: -100912.109375\n",
      "Train Epoch: 318 [17030/17352 (98%)] Loss: -88849.781250\n",
      "    epoch          : 318\n",
      "    loss           : -127672.31818569107\n",
      "    val_loss       : -61120.776338704425\n",
      "Train Epoch: 319 [128/17352 (1%)] Loss: -138236.156250\n",
      "Train Epoch: 319 [1536/17352 (9%)] Loss: -150575.312500\n",
      "Train Epoch: 319 [2944/17352 (17%)] Loss: -167498.515625\n",
      "Train Epoch: 319 [4352/17352 (25%)] Loss: -123400.929688\n",
      "Train Epoch: 319 [5760/17352 (33%)] Loss: -121507.257812\n",
      "Train Epoch: 319 [7168/17352 (41%)] Loss: -133991.703125\n",
      "Train Epoch: 319 [8576/17352 (49%)] Loss: -172702.000000\n",
      "Train Epoch: 319 [9984/17352 (58%)] Loss: -172109.140625\n",
      "Train Epoch: 319 [11392/17352 (66%)] Loss: -148583.718750\n",
      "Train Epoch: 319 [12800/17352 (74%)] Loss: -138813.843750\n",
      "Train Epoch: 319 [14208/17352 (82%)] Loss: -172698.468750\n",
      "Train Epoch: 319 [15490/17352 (89%)] Loss: -47644.179688\n",
      "Train Epoch: 319 [16328/17352 (94%)] Loss: -11217.717773\n",
      "Train Epoch: 319 [16975/17352 (98%)] Loss: -84791.125000\n",
      "    epoch          : 319\n",
      "    loss           : -129900.47614959102\n",
      "    val_loss       : -60161.51005045573\n",
      "Train Epoch: 320 [128/17352 (1%)] Loss: -113455.289062\n",
      "Train Epoch: 320 [1536/17352 (9%)] Loss: -104746.109375\n",
      "Train Epoch: 320 [2944/17352 (17%)] Loss: -137455.031250\n",
      "Train Epoch: 320 [4352/17352 (25%)] Loss: -157879.906250\n",
      "Train Epoch: 320 [5760/17352 (33%)] Loss: -105553.101562\n",
      "Train Epoch: 320 [7168/17352 (41%)] Loss: -150708.187500\n",
      "Train Epoch: 320 [8576/17352 (49%)] Loss: -127255.914062\n",
      "Train Epoch: 320 [9984/17352 (58%)] Loss: -108922.296875\n",
      "Train Epoch: 320 [11392/17352 (66%)] Loss: -147991.078125\n",
      "Train Epoch: 320 [12800/17352 (74%)] Loss: -105233.171875\n",
      "Train Epoch: 320 [14208/17352 (82%)] Loss: -125854.062500\n",
      "Train Epoch: 320 [15453/17352 (89%)] Loss: -48584.488281\n",
      "Train Epoch: 320 [16118/17352 (93%)] Loss: -13459.580078\n",
      "Train Epoch: 320 [16913/17352 (97%)] Loss: -71177.195312\n",
      "    epoch          : 320\n",
      "    loss           : -122242.85835255873\n",
      "    val_loss       : -65973.6846883138\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch320.pth ...\n",
      "Train Epoch: 321 [128/17352 (1%)] Loss: -136529.609375\n",
      "Train Epoch: 321 [1536/17352 (9%)] Loss: -143284.890625\n",
      "Train Epoch: 321 [2944/17352 (17%)] Loss: -153965.031250\n",
      "Train Epoch: 321 [4352/17352 (25%)] Loss: -131162.750000\n",
      "Train Epoch: 321 [5760/17352 (33%)] Loss: -162692.734375\n",
      "Train Epoch: 321 [7168/17352 (41%)] Loss: -127683.375000\n",
      "Train Epoch: 321 [8576/17352 (49%)] Loss: -129023.703125\n",
      "Train Epoch: 321 [9984/17352 (58%)] Loss: -120777.382812\n",
      "Train Epoch: 321 [11392/17352 (66%)] Loss: -135570.140625\n",
      "Train Epoch: 321 [12800/17352 (74%)] Loss: -158676.390625\n",
      "Train Epoch: 321 [14208/17352 (82%)] Loss: -122180.507812\n",
      "Train Epoch: 321 [15368/17352 (89%)] Loss: -5219.552734\n",
      "Train Epoch: 321 [16289/17352 (94%)] Loss: -92771.515625\n",
      "Train Epoch: 321 [16926/17352 (98%)] Loss: -81376.250000\n",
      "    epoch          : 321\n",
      "    loss           : -126300.63266011693\n",
      "    val_loss       : -65202.01389567057\n",
      "Train Epoch: 322 [128/17352 (1%)] Loss: -143011.703125\n",
      "Train Epoch: 322 [1536/17352 (9%)] Loss: -141243.937500\n",
      "Train Epoch: 322 [2944/17352 (17%)] Loss: -128760.406250\n",
      "Train Epoch: 322 [4352/17352 (25%)] Loss: -135743.093750\n",
      "Train Epoch: 322 [5760/17352 (33%)] Loss: -143477.031250\n",
      "Train Epoch: 322 [7168/17352 (41%)] Loss: -159439.093750\n",
      "Train Epoch: 322 [8576/17352 (49%)] Loss: -154807.390625\n",
      "Train Epoch: 322 [9984/17352 (58%)] Loss: -170768.906250\n",
      "Train Epoch: 322 [11392/17352 (66%)] Loss: -111782.140625\n",
      "Train Epoch: 322 [12800/17352 (74%)] Loss: -151522.875000\n",
      "Train Epoch: 322 [14208/17352 (82%)] Loss: -169317.250000\n",
      "Train Epoch: 322 [15452/17352 (89%)] Loss: -8620.803711\n",
      "Train Epoch: 322 [16260/17352 (94%)] Loss: -23958.232422\n",
      "Train Epoch: 322 [17019/17352 (98%)] Loss: -2344.076172\n",
      "    epoch          : 322\n",
      "    loss           : -121345.74931017985\n",
      "    val_loss       : -53143.55248616536\n",
      "Train Epoch: 323 [128/17352 (1%)] Loss: -117708.000000\n",
      "Train Epoch: 323 [1536/17352 (9%)] Loss: -141649.921875\n",
      "Train Epoch: 323 [2944/17352 (17%)] Loss: -85849.671875\n",
      "Train Epoch: 323 [4352/17352 (25%)] Loss: -140618.468750\n",
      "Train Epoch: 323 [5760/17352 (33%)] Loss: -161063.593750\n",
      "Train Epoch: 323 [7168/17352 (41%)] Loss: -128405.835938\n",
      "Train Epoch: 323 [8576/17352 (49%)] Loss: -136741.703125\n",
      "Train Epoch: 323 [9984/17352 (58%)] Loss: -144993.906250\n",
      "Train Epoch: 323 [11392/17352 (66%)] Loss: -125948.976562\n",
      "Train Epoch: 323 [12800/17352 (74%)] Loss: -116244.859375\n",
      "Train Epoch: 323 [14208/17352 (82%)] Loss: -158627.546875\n",
      "Train Epoch: 323 [15457/17352 (89%)] Loss: -41580.812500\n",
      "Train Epoch: 323 [16272/17352 (94%)] Loss: -96890.320312\n",
      "Train Epoch: 323 [16964/17352 (98%)] Loss: -98819.476562\n",
      "    epoch          : 323\n",
      "    loss           : -127451.17853561504\n",
      "    val_loss       : -71052.33955891927\n",
      "Train Epoch: 324 [128/17352 (1%)] Loss: -163764.781250\n",
      "Train Epoch: 324 [1536/17352 (9%)] Loss: -144818.234375\n",
      "Train Epoch: 324 [2944/17352 (17%)] Loss: -154901.156250\n",
      "Train Epoch: 324 [4352/17352 (25%)] Loss: -149677.734375\n",
      "Train Epoch: 324 [5760/17352 (33%)] Loss: -146380.250000\n",
      "Train Epoch: 324 [7168/17352 (41%)] Loss: -142266.484375\n",
      "Train Epoch: 324 [8576/17352 (49%)] Loss: -146172.468750\n",
      "Train Epoch: 324 [9984/17352 (58%)] Loss: -157259.953125\n",
      "Train Epoch: 324 [11392/17352 (66%)] Loss: -157965.171875\n",
      "Train Epoch: 324 [12800/17352 (74%)] Loss: -135029.296875\n",
      "Train Epoch: 324 [14208/17352 (82%)] Loss: -127502.218750\n",
      "Train Epoch: 324 [15513/17352 (89%)] Loss: -68101.148438\n",
      "Train Epoch: 324 [16262/17352 (94%)] Loss: -90045.453125\n",
      "Train Epoch: 324 [16964/17352 (98%)] Loss: -121825.781250\n",
      "    epoch          : 324\n",
      "    loss           : -128293.65030378303\n",
      "    val_loss       : -71720.75489095053\n",
      "Train Epoch: 325 [128/17352 (1%)] Loss: -151836.843750\n",
      "Train Epoch: 325 [1536/17352 (9%)] Loss: -136322.593750\n",
      "Train Epoch: 325 [2944/17352 (17%)] Loss: -163278.578125\n",
      "Train Epoch: 325 [4352/17352 (25%)] Loss: -91202.250000\n",
      "Train Epoch: 325 [5760/17352 (33%)] Loss: -109758.328125\n",
      "Train Epoch: 325 [7168/17352 (41%)] Loss: -171234.625000\n",
      "Train Epoch: 325 [8576/17352 (49%)] Loss: -116718.515625\n",
      "Train Epoch: 325 [9984/17352 (58%)] Loss: -158787.609375\n",
      "Train Epoch: 325 [11392/17352 (66%)] Loss: -133612.437500\n",
      "Train Epoch: 325 [12800/17352 (74%)] Loss: -148643.750000\n",
      "Train Epoch: 325 [14208/17352 (82%)] Loss: -136061.640625\n",
      "Train Epoch: 325 [15521/17352 (89%)] Loss: -34492.183594\n",
      "Train Epoch: 325 [16194/17352 (93%)] Loss: -3629.160400\n",
      "Train Epoch: 325 [16911/17352 (97%)] Loss: -42290.785156\n",
      "    epoch          : 325\n",
      "    loss           : -130621.27667686924\n",
      "    val_loss       : -68125.1820678711\n",
      "Train Epoch: 326 [128/17352 (1%)] Loss: -113821.531250\n",
      "Train Epoch: 326 [1536/17352 (9%)] Loss: -147354.375000\n",
      "Train Epoch: 326 [2944/17352 (17%)] Loss: -170019.000000\n",
      "Train Epoch: 326 [4352/17352 (25%)] Loss: -124118.882812\n",
      "Train Epoch: 326 [5760/17352 (33%)] Loss: -79156.492188\n",
      "Train Epoch: 326 [7168/17352 (41%)] Loss: -137486.828125\n",
      "Train Epoch: 326 [8576/17352 (49%)] Loss: -134310.812500\n",
      "Train Epoch: 326 [9984/17352 (58%)] Loss: -138337.812500\n",
      "Train Epoch: 326 [11392/17352 (66%)] Loss: -144295.453125\n",
      "Train Epoch: 326 [12800/17352 (74%)] Loss: -138872.343750\n",
      "Train Epoch: 326 [14208/17352 (82%)] Loss: -156921.812500\n",
      "Train Epoch: 326 [15520/17352 (89%)] Loss: -83349.500000\n",
      "Train Epoch: 326 [16319/17352 (94%)] Loss: -47345.882812\n",
      "Train Epoch: 326 [16988/17352 (98%)] Loss: -4319.527832\n",
      "    epoch          : 326\n",
      "    loss           : -119326.52973272337\n",
      "    val_loss       : -64465.909151204425\n",
      "Train Epoch: 327 [128/17352 (1%)] Loss: -139362.031250\n",
      "Train Epoch: 327 [1536/17352 (9%)] Loss: -64311.988281\n",
      "Train Epoch: 327 [2944/17352 (17%)] Loss: -110915.593750\n",
      "Train Epoch: 327 [4352/17352 (25%)] Loss: -121112.406250\n",
      "Train Epoch: 327 [5760/17352 (33%)] Loss: -151961.484375\n",
      "Train Epoch: 327 [7168/17352 (41%)] Loss: -159314.312500\n",
      "Train Epoch: 327 [8576/17352 (49%)] Loss: -140837.687500\n",
      "Train Epoch: 327 [9984/17352 (58%)] Loss: -86706.671875\n",
      "Train Epoch: 327 [11392/17352 (66%)] Loss: -120258.031250\n",
      "Train Epoch: 327 [12800/17352 (74%)] Loss: -124395.359375\n",
      "Train Epoch: 327 [14208/17352 (82%)] Loss: -165246.781250\n",
      "Train Epoch: 327 [15427/17352 (89%)] Loss: -53345.855469\n",
      "Train Epoch: 327 [16021/17352 (92%)] Loss: -3540.824707\n",
      "Train Epoch: 327 [16955/17352 (98%)] Loss: -121361.765625\n",
      "    epoch          : 327\n",
      "    loss           : -123841.33601287227\n",
      "    val_loss       : -70860.30287272135\n",
      "Train Epoch: 328 [128/17352 (1%)] Loss: -171767.031250\n",
      "Train Epoch: 328 [1536/17352 (9%)] Loss: -138433.140625\n",
      "Train Epoch: 328 [2944/17352 (17%)] Loss: -140314.359375\n",
      "Train Epoch: 328 [4352/17352 (25%)] Loss: -164454.218750\n",
      "Train Epoch: 328 [5760/17352 (33%)] Loss: -140739.265625\n",
      "Train Epoch: 328 [7168/17352 (41%)] Loss: -120145.335938\n",
      "Train Epoch: 328 [8576/17352 (49%)] Loss: -152185.781250\n",
      "Train Epoch: 328 [9984/17352 (58%)] Loss: -132669.187500\n",
      "Train Epoch: 328 [11392/17352 (66%)] Loss: -153441.546875\n",
      "Train Epoch: 328 [12800/17352 (74%)] Loss: -131889.406250\n",
      "Train Epoch: 328 [14208/17352 (82%)] Loss: -109057.500000\n",
      "Train Epoch: 328 [15560/17352 (90%)] Loss: -100726.406250\n",
      "Train Epoch: 328 [16334/17352 (94%)] Loss: -47506.175781\n",
      "Train Epoch: 328 [16957/17352 (98%)] Loss: -3536.869629\n",
      "    epoch          : 328\n",
      "    loss           : -126465.31924745701\n",
      "    val_loss       : -66432.70908203124\n",
      "Train Epoch: 329 [128/17352 (1%)] Loss: -98193.734375\n",
      "Train Epoch: 329 [1536/17352 (9%)] Loss: -132402.406250\n",
      "Train Epoch: 329 [2944/17352 (17%)] Loss: -144374.687500\n",
      "Train Epoch: 329 [4352/17352 (25%)] Loss: -129286.632812\n",
      "Train Epoch: 329 [5760/17352 (33%)] Loss: -157676.171875\n",
      "Train Epoch: 329 [7168/17352 (41%)] Loss: -130240.859375\n",
      "Train Epoch: 329 [8576/17352 (49%)] Loss: -166276.781250\n",
      "Train Epoch: 329 [9984/17352 (58%)] Loss: -131471.687500\n",
      "Train Epoch: 329 [11392/17352 (66%)] Loss: -151037.406250\n",
      "Train Epoch: 329 [12800/17352 (74%)] Loss: -190326.187500\n",
      "Train Epoch: 329 [14208/17352 (82%)] Loss: -111128.328125\n",
      "Train Epoch: 329 [15500/17352 (89%)] Loss: -53049.566406\n",
      "Train Epoch: 329 [16260/17352 (94%)] Loss: -98706.562500\n",
      "Train Epoch: 329 [17084/17352 (98%)] Loss: -37795.609375\n",
      "    epoch          : 329\n",
      "    loss           : -128926.06419096058\n",
      "    val_loss       : -70407.7326944987\n",
      "Train Epoch: 330 [128/17352 (1%)] Loss: -127498.515625\n",
      "Train Epoch: 330 [1536/17352 (9%)] Loss: -135292.468750\n",
      "Train Epoch: 330 [2944/17352 (17%)] Loss: -138711.593750\n",
      "Train Epoch: 330 [4352/17352 (25%)] Loss: -140155.515625\n",
      "Train Epoch: 330 [5760/17352 (33%)] Loss: -149434.546875\n",
      "Train Epoch: 330 [7168/17352 (41%)] Loss: -151012.640625\n",
      "Train Epoch: 330 [8576/17352 (49%)] Loss: -148958.281250\n",
      "Train Epoch: 330 [9984/17352 (58%)] Loss: -147634.500000\n",
      "Train Epoch: 330 [11392/17352 (66%)] Loss: -132658.375000\n",
      "Train Epoch: 330 [12800/17352 (74%)] Loss: -185005.187500\n",
      "Train Epoch: 330 [14208/17352 (82%)] Loss: -133946.437500\n",
      "Train Epoch: 330 [15529/17352 (89%)] Loss: -62918.984375\n",
      "Train Epoch: 330 [16276/17352 (94%)] Loss: -18681.574219\n",
      "Train Epoch: 330 [16922/17352 (98%)] Loss: -39488.066406\n",
      "    epoch          : 330\n",
      "    loss           : -132847.08833302747\n",
      "    val_loss       : -71259.1966796875\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch330.pth ...\n",
      "Train Epoch: 331 [128/17352 (1%)] Loss: -143783.031250\n",
      "Train Epoch: 331 [1536/17352 (9%)] Loss: -164407.187500\n",
      "Train Epoch: 331 [2944/17352 (17%)] Loss: -141123.703125\n",
      "Train Epoch: 331 [4352/17352 (25%)] Loss: -149541.359375\n",
      "Train Epoch: 331 [5760/17352 (33%)] Loss: -106514.117188\n",
      "Train Epoch: 331 [7168/17352 (41%)] Loss: -134098.328125\n",
      "Train Epoch: 331 [8576/17352 (49%)] Loss: -131661.093750\n",
      "Train Epoch: 331 [9984/17352 (58%)] Loss: -114841.437500\n",
      "Train Epoch: 331 [11392/17352 (66%)] Loss: -135414.687500\n",
      "Train Epoch: 331 [12800/17352 (74%)] Loss: -135293.500000\n",
      "Train Epoch: 331 [14208/17352 (82%)] Loss: -130437.593750\n",
      "Train Epoch: 331 [15508/17352 (89%)] Loss: -69376.203125\n",
      "Train Epoch: 331 [16232/17352 (94%)] Loss: -103690.898438\n",
      "Train Epoch: 331 [16887/17352 (97%)] Loss: -39583.035156\n",
      "    epoch          : 331\n",
      "    loss           : -127443.04532331428\n",
      "    val_loss       : -64711.264774576826\n",
      "Train Epoch: 332 [128/17352 (1%)] Loss: -147503.406250\n",
      "Train Epoch: 332 [1536/17352 (9%)] Loss: -149704.875000\n",
      "Train Epoch: 332 [2944/17352 (17%)] Loss: -114080.015625\n",
      "Train Epoch: 332 [4352/17352 (25%)] Loss: -149214.687500\n",
      "Train Epoch: 332 [5760/17352 (33%)] Loss: -154188.890625\n",
      "Train Epoch: 332 [7168/17352 (41%)] Loss: -106690.554688\n",
      "Train Epoch: 332 [8576/17352 (49%)] Loss: -164782.953125\n",
      "Train Epoch: 332 [9984/17352 (58%)] Loss: -88841.773438\n",
      "Train Epoch: 332 [11392/17352 (66%)] Loss: -158614.703125\n",
      "Train Epoch: 332 [12800/17352 (74%)] Loss: -168253.125000\n",
      "Train Epoch: 332 [14208/17352 (82%)] Loss: -131872.640625\n",
      "Train Epoch: 332 [15474/17352 (89%)] Loss: -11668.591797\n",
      "Train Epoch: 332 [16151/17352 (93%)] Loss: -4228.910645\n",
      "Train Epoch: 332 [16974/17352 (98%)] Loss: -32019.994141\n",
      "    epoch          : 332\n",
      "    loss           : -120896.2762901767\n",
      "    val_loss       : -43263.19359944661\n",
      "Train Epoch: 333 [128/17352 (1%)] Loss: -88400.125000\n",
      "Train Epoch: 333 [1536/17352 (9%)] Loss: -122995.000000\n",
      "Train Epoch: 333 [2944/17352 (17%)] Loss: -117456.671875\n",
      "Train Epoch: 333 [4352/17352 (25%)] Loss: -131570.937500\n",
      "Train Epoch: 333 [5760/17352 (33%)] Loss: -133041.015625\n",
      "Train Epoch: 333 [7168/17352 (41%)] Loss: -126379.242188\n",
      "Train Epoch: 333 [8576/17352 (49%)] Loss: -107059.125000\n",
      "Train Epoch: 333 [9984/17352 (58%)] Loss: -163675.562500\n",
      "Train Epoch: 333 [11392/17352 (66%)] Loss: -126818.140625\n",
      "Train Epoch: 333 [12800/17352 (74%)] Loss: -157248.218750\n",
      "Train Epoch: 333 [14208/17352 (82%)] Loss: -146832.250000\n",
      "Train Epoch: 333 [15544/17352 (90%)] Loss: -85412.953125\n",
      "Train Epoch: 333 [16439/17352 (95%)] Loss: -64945.445312\n",
      "Train Epoch: 333 [17075/17352 (98%)] Loss: -63777.656250\n",
      "    epoch          : 333\n",
      "    loss           : -125808.14619599412\n",
      "    val_loss       : -54832.45322672526\n",
      "Train Epoch: 334 [128/17352 (1%)] Loss: -111104.906250\n",
      "Train Epoch: 334 [1536/17352 (9%)] Loss: -139632.031250\n",
      "Train Epoch: 334 [2944/17352 (17%)] Loss: -88556.992188\n",
      "Train Epoch: 334 [4352/17352 (25%)] Loss: -133021.781250\n",
      "Train Epoch: 334 [5760/17352 (33%)] Loss: -125642.398438\n",
      "Train Epoch: 334 [7168/17352 (41%)] Loss: -111901.484375\n",
      "Train Epoch: 334 [8576/17352 (49%)] Loss: -81032.320312\n",
      "Train Epoch: 334 [9984/17352 (58%)] Loss: -129567.718750\n",
      "Train Epoch: 334 [11392/17352 (66%)] Loss: -133422.578125\n",
      "Train Epoch: 334 [12800/17352 (74%)] Loss: -136176.921875\n",
      "Train Epoch: 334 [14208/17352 (82%)] Loss: -135502.875000\n",
      "Train Epoch: 334 [15480/17352 (89%)] Loss: -116793.507812\n",
      "Train Epoch: 334 [16234/17352 (94%)] Loss: -13987.906250\n",
      "Train Epoch: 334 [16936/17352 (98%)] Loss: -92742.171875\n",
      "    epoch          : 334\n",
      "    loss           : -118268.90161952077\n",
      "    val_loss       : -64741.55613606771\n",
      "Train Epoch: 335 [128/17352 (1%)] Loss: -135338.921875\n",
      "Train Epoch: 335 [1536/17352 (9%)] Loss: -150671.968750\n",
      "Train Epoch: 335 [2944/17352 (17%)] Loss: -144669.812500\n",
      "Train Epoch: 335 [4352/17352 (25%)] Loss: -156672.781250\n",
      "Train Epoch: 335 [5760/17352 (33%)] Loss: -110236.085938\n",
      "Train Epoch: 335 [7168/17352 (41%)] Loss: -24621.039062\n",
      "Train Epoch: 335 [8576/17352 (49%)] Loss: -88096.875000\n",
      "Train Epoch: 335 [9984/17352 (58%)] Loss: -129441.539062\n",
      "Train Epoch: 335 [11392/17352 (66%)] Loss: -136744.015625\n",
      "Train Epoch: 335 [12800/17352 (74%)] Loss: -149687.968750\n",
      "Train Epoch: 335 [14208/17352 (82%)] Loss: -161861.906250\n",
      "Train Epoch: 335 [15527/17352 (89%)] Loss: -90702.500000\n",
      "Train Epoch: 335 [16273/17352 (94%)] Loss: -69837.992188\n",
      "Train Epoch: 335 [17021/17352 (98%)] Loss: -77708.054688\n",
      "    epoch          : 335\n",
      "    loss           : -115917.75977217911\n",
      "    val_loss       : -66326.85329996745\n",
      "Train Epoch: 336 [128/17352 (1%)] Loss: -123215.000000\n",
      "Train Epoch: 336 [1536/17352 (9%)] Loss: -112849.656250\n",
      "Train Epoch: 336 [2944/17352 (17%)] Loss: -170616.968750\n",
      "Train Epoch: 336 [4352/17352 (25%)] Loss: -166759.484375\n",
      "Train Epoch: 336 [5760/17352 (33%)] Loss: -89980.375000\n",
      "Train Epoch: 336 [7168/17352 (41%)] Loss: -176900.593750\n",
      "Train Epoch: 336 [8576/17352 (49%)] Loss: -136026.062500\n",
      "Train Epoch: 336 [9984/17352 (58%)] Loss: -126581.500000\n",
      "Train Epoch: 336 [11392/17352 (66%)] Loss: -139254.375000\n",
      "Train Epoch: 336 [12800/17352 (74%)] Loss: -158950.875000\n",
      "Train Epoch: 336 [14208/17352 (82%)] Loss: -172755.015625\n",
      "Train Epoch: 336 [15536/17352 (90%)] Loss: -76060.531250\n",
      "Train Epoch: 336 [16250/17352 (94%)] Loss: -94236.140625\n",
      "Train Epoch: 336 [16938/17352 (98%)] Loss: -99765.109375\n",
      "    epoch          : 336\n",
      "    loss           : -129752.3377071099\n",
      "    val_loss       : -72261.90881754557\n",
      "Train Epoch: 337 [128/17352 (1%)] Loss: -126461.781250\n",
      "Train Epoch: 337 [1536/17352 (9%)] Loss: -125796.234375\n",
      "Train Epoch: 337 [2944/17352 (17%)] Loss: -103957.007812\n",
      "Train Epoch: 337 [4352/17352 (25%)] Loss: -149231.546875\n",
      "Train Epoch: 337 [5760/17352 (33%)] Loss: -144895.765625\n",
      "Train Epoch: 337 [7168/17352 (41%)] Loss: -128346.210938\n",
      "Train Epoch: 337 [8576/17352 (49%)] Loss: -79766.187500\n",
      "Train Epoch: 337 [9984/17352 (58%)] Loss: -135798.828125\n",
      "Train Epoch: 337 [11392/17352 (66%)] Loss: -92446.312500\n",
      "Train Epoch: 337 [12800/17352 (74%)] Loss: -185316.078125\n",
      "Train Epoch: 337 [14208/17352 (82%)] Loss: -175739.437500\n",
      "Train Epoch: 337 [15485/17352 (89%)] Loss: -21867.857422\n",
      "Train Epoch: 337 [16237/17352 (94%)] Loss: -103738.718750\n",
      "Train Epoch: 337 [17051/17352 (98%)] Loss: -70289.937500\n",
      "    epoch          : 337\n",
      "    loss           : -119372.23137813287\n",
      "    val_loss       : -52070.620145670575\n",
      "Train Epoch: 338 [128/17352 (1%)] Loss: -111989.875000\n",
      "Train Epoch: 338 [1536/17352 (9%)] Loss: -116004.398438\n",
      "Train Epoch: 338 [2944/17352 (17%)] Loss: -147862.218750\n",
      "Train Epoch: 338 [4352/17352 (25%)] Loss: -148690.125000\n",
      "Train Epoch: 338 [5760/17352 (33%)] Loss: -150577.328125\n",
      "Train Epoch: 338 [7168/17352 (41%)] Loss: -150189.484375\n",
      "Train Epoch: 338 [8576/17352 (49%)] Loss: -121128.609375\n",
      "Train Epoch: 338 [9984/17352 (58%)] Loss: -106761.843750\n",
      "Train Epoch: 338 [11392/17352 (66%)] Loss: -153493.937500\n",
      "Train Epoch: 338 [12800/17352 (74%)] Loss: -134048.187500\n",
      "Train Epoch: 338 [14208/17352 (82%)] Loss: -143634.750000\n",
      "Train Epoch: 338 [15438/17352 (89%)] Loss: -4802.974121\n",
      "Train Epoch: 338 [16271/17352 (94%)] Loss: -101428.250000\n",
      "Train Epoch: 338 [17075/17352 (98%)] Loss: -42355.808594\n",
      "    epoch          : 338\n",
      "    loss           : -123169.65335636011\n",
      "    val_loss       : -60010.79176839193\n",
      "Train Epoch: 339 [128/17352 (1%)] Loss: -124709.320312\n",
      "Train Epoch: 339 [1536/17352 (9%)] Loss: -139109.281250\n",
      "Train Epoch: 339 [2944/17352 (17%)] Loss: -128338.593750\n",
      "Train Epoch: 339 [4352/17352 (25%)] Loss: -114962.062500\n",
      "Train Epoch: 339 [5760/17352 (33%)] Loss: -160587.437500\n",
      "Train Epoch: 339 [7168/17352 (41%)] Loss: -138406.328125\n",
      "Train Epoch: 339 [8576/17352 (49%)] Loss: -153697.265625\n",
      "Train Epoch: 339 [9984/17352 (58%)] Loss: -139657.593750\n",
      "Train Epoch: 339 [11392/17352 (66%)] Loss: -132905.125000\n",
      "Train Epoch: 339 [12800/17352 (74%)] Loss: -157445.031250\n",
      "Train Epoch: 339 [14208/17352 (82%)] Loss: -154519.406250\n",
      "Train Epoch: 339 [15531/17352 (90%)] Loss: -95974.968750\n",
      "Train Epoch: 339 [16387/17352 (94%)] Loss: -43153.226562\n",
      "Train Epoch: 339 [17061/17352 (98%)] Loss: -114093.476562\n",
      "    epoch          : 339\n",
      "    loss           : -126510.67037902422\n",
      "    val_loss       : -66961.26927897135\n",
      "Train Epoch: 340 [128/17352 (1%)] Loss: -143404.609375\n",
      "Train Epoch: 340 [1536/17352 (9%)] Loss: -119741.484375\n",
      "Train Epoch: 340 [2944/17352 (17%)] Loss: -118202.679688\n",
      "Train Epoch: 340 [4352/17352 (25%)] Loss: -136444.640625\n",
      "Train Epoch: 340 [5760/17352 (33%)] Loss: -149455.625000\n",
      "Train Epoch: 340 [7168/17352 (41%)] Loss: -133144.093750\n",
      "Train Epoch: 340 [8576/17352 (49%)] Loss: -162825.062500\n",
      "Train Epoch: 340 [9984/17352 (58%)] Loss: -139034.500000\n",
      "Train Epoch: 340 [11392/17352 (66%)] Loss: -148568.375000\n",
      "Train Epoch: 340 [12800/17352 (74%)] Loss: -168611.437500\n",
      "Train Epoch: 340 [14208/17352 (82%)] Loss: -91878.406250\n",
      "Train Epoch: 340 [15457/17352 (89%)] Loss: -64004.898438\n",
      "Train Epoch: 340 [16289/17352 (94%)] Loss: -121873.375000\n",
      "Train Epoch: 340 [16968/17352 (98%)] Loss: -66199.812500\n",
      "    epoch          : 340\n",
      "    loss           : -129153.25004096319\n",
      "    val_loss       : -63420.093688964844\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch340.pth ...\n",
      "Train Epoch: 341 [128/17352 (1%)] Loss: -134708.593750\n",
      "Train Epoch: 341 [1536/17352 (9%)] Loss: -132957.718750\n",
      "Train Epoch: 341 [2944/17352 (17%)] Loss: -156965.062500\n",
      "Train Epoch: 341 [4352/17352 (25%)] Loss: -166123.562500\n",
      "Train Epoch: 341 [5760/17352 (33%)] Loss: -166419.890625\n",
      "Train Epoch: 341 [7168/17352 (41%)] Loss: -142745.906250\n",
      "Train Epoch: 341 [8576/17352 (49%)] Loss: -145056.531250\n",
      "Train Epoch: 341 [9984/17352 (58%)] Loss: -114597.882812\n",
      "Train Epoch: 341 [11392/17352 (66%)] Loss: -134018.328125\n",
      "Train Epoch: 341 [12800/17352 (74%)] Loss: -159797.453125\n",
      "Train Epoch: 341 [14208/17352 (82%)] Loss: -157125.968750\n",
      "Train Epoch: 341 [15482/17352 (89%)] Loss: -80025.429688\n",
      "Train Epoch: 341 [16164/17352 (93%)] Loss: -32871.273438\n",
      "Train Epoch: 341 [16981/17352 (98%)] Loss: -45669.574219\n",
      "    epoch          : 341\n",
      "    loss           : -127596.65158838873\n",
      "    val_loss       : -43900.136210123695\n",
      "Train Epoch: 342 [128/17352 (1%)] Loss: -84479.781250\n",
      "Train Epoch: 342 [1536/17352 (9%)] Loss: -124979.742188\n",
      "Train Epoch: 342 [2944/17352 (17%)] Loss: -148286.750000\n",
      "Train Epoch: 342 [4352/17352 (25%)] Loss: -110410.585938\n",
      "Train Epoch: 342 [5760/17352 (33%)] Loss: -159367.421875\n",
      "Train Epoch: 342 [7168/17352 (41%)] Loss: -162842.031250\n",
      "Train Epoch: 342 [8576/17352 (49%)] Loss: -136930.187500\n",
      "Train Epoch: 342 [9984/17352 (58%)] Loss: -151040.515625\n",
      "Train Epoch: 342 [11392/17352 (66%)] Loss: -116454.671875\n",
      "Train Epoch: 342 [12800/17352 (74%)] Loss: -150526.343750\n",
      "Train Epoch: 342 [14208/17352 (82%)] Loss: -154591.718750\n",
      "Train Epoch: 342 [15572/17352 (90%)] Loss: -129617.960938\n",
      "Train Epoch: 342 [16232/17352 (94%)] Loss: -5816.913086\n",
      "Train Epoch: 342 [16936/17352 (98%)] Loss: -43855.359375\n",
      "    epoch          : 342\n",
      "    loss           : -123737.45320856491\n",
      "    val_loss       : -58673.50546061198\n",
      "Train Epoch: 343 [128/17352 (1%)] Loss: -118466.859375\n",
      "Train Epoch: 343 [1536/17352 (9%)] Loss: -135259.734375\n",
      "Train Epoch: 343 [2944/17352 (17%)] Loss: -190818.734375\n",
      "Train Epoch: 343 [4352/17352 (25%)] Loss: -118580.257812\n",
      "Train Epoch: 343 [5760/17352 (33%)] Loss: -89052.718750\n",
      "Train Epoch: 343 [7168/17352 (41%)] Loss: -135722.718750\n",
      "Train Epoch: 343 [8576/17352 (49%)] Loss: -110857.843750\n",
      "Train Epoch: 343 [9984/17352 (58%)] Loss: -126068.937500\n",
      "Train Epoch: 343 [11392/17352 (66%)] Loss: -108250.984375\n",
      "Train Epoch: 343 [12800/17352 (74%)] Loss: -149867.437500\n",
      "Train Epoch: 343 [14208/17352 (82%)] Loss: -144858.187500\n",
      "Train Epoch: 343 [15490/17352 (89%)] Loss: -33173.804688\n",
      "Train Epoch: 343 [16162/17352 (93%)] Loss: -49204.773438\n",
      "Train Epoch: 343 [16981/17352 (98%)] Loss: -138593.765625\n",
      "    epoch          : 343\n",
      "    loss           : -121823.52682433672\n",
      "    val_loss       : -72572.05399169921\n",
      "Train Epoch: 344 [128/17352 (1%)] Loss: -163042.421875\n",
      "Train Epoch: 344 [1536/17352 (9%)] Loss: -167175.718750\n",
      "Train Epoch: 344 [2944/17352 (17%)] Loss: -179388.937500\n",
      "Train Epoch: 344 [4352/17352 (25%)] Loss: -163649.687500\n",
      "Train Epoch: 344 [5760/17352 (33%)] Loss: -151445.093750\n",
      "Train Epoch: 344 [7168/17352 (41%)] Loss: -120746.203125\n",
      "Train Epoch: 344 [8576/17352 (49%)] Loss: -148491.687500\n",
      "Train Epoch: 344 [9984/17352 (58%)] Loss: -142748.015625\n",
      "Train Epoch: 344 [11392/17352 (66%)] Loss: -166641.312500\n",
      "Train Epoch: 344 [12800/17352 (74%)] Loss: -87942.101562\n",
      "Train Epoch: 344 [14208/17352 (82%)] Loss: -163441.296875\n",
      "Train Epoch: 344 [15565/17352 (90%)] Loss: -133401.234375\n",
      "Train Epoch: 344 [16212/17352 (93%)] Loss: -93929.218750\n",
      "Train Epoch: 344 [16980/17352 (98%)] Loss: -42961.105469\n",
      "    epoch          : 344\n",
      "    loss           : -124428.42493249266\n",
      "    val_loss       : -71199.02521972657\n",
      "Train Epoch: 345 [128/17352 (1%)] Loss: -122422.734375\n",
      "Train Epoch: 345 [1536/17352 (9%)] Loss: -106846.179688\n",
      "Train Epoch: 345 [2944/17352 (17%)] Loss: -119519.468750\n",
      "Train Epoch: 345 [4352/17352 (25%)] Loss: -151594.406250\n",
      "Train Epoch: 345 [5760/17352 (33%)] Loss: -161643.546875\n",
      "Train Epoch: 345 [7168/17352 (41%)] Loss: -157058.156250\n",
      "Train Epoch: 345 [8576/17352 (49%)] Loss: -155014.406250\n",
      "Train Epoch: 345 [9984/17352 (58%)] Loss: -176260.781250\n",
      "Train Epoch: 345 [11392/17352 (66%)] Loss: -140198.140625\n",
      "Train Epoch: 345 [12800/17352 (74%)] Loss: -143494.437500\n",
      "Train Epoch: 345 [14208/17352 (82%)] Loss: -152670.156250\n",
      "Train Epoch: 345 [15526/17352 (89%)] Loss: -55542.574219\n",
      "Train Epoch: 345 [16302/17352 (94%)] Loss: -101700.476562\n",
      "Train Epoch: 345 [17008/17352 (98%)] Loss: -68925.156250\n",
      "    epoch          : 345\n",
      "    loss           : -126464.3028589031\n",
      "    val_loss       : -68242.643359375\n",
      "Train Epoch: 346 [128/17352 (1%)] Loss: -148569.218750\n",
      "Train Epoch: 346 [1536/17352 (9%)] Loss: -170506.968750\n",
      "Train Epoch: 346 [2944/17352 (17%)] Loss: -122457.257812\n",
      "Train Epoch: 346 [4352/17352 (25%)] Loss: -114840.765625\n",
      "Train Epoch: 346 [5760/17352 (33%)] Loss: -139205.921875\n",
      "Train Epoch: 346 [7168/17352 (41%)] Loss: -130984.250000\n",
      "Train Epoch: 346 [8576/17352 (49%)] Loss: -121290.687500\n",
      "Train Epoch: 346 [9984/17352 (58%)] Loss: -150380.718750\n",
      "Train Epoch: 346 [11392/17352 (66%)] Loss: -115133.765625\n",
      "Train Epoch: 346 [12800/17352 (74%)] Loss: -161289.265625\n",
      "Train Epoch: 346 [14208/17352 (82%)] Loss: -129438.945312\n",
      "Train Epoch: 346 [15533/17352 (90%)] Loss: -101462.671875\n",
      "Train Epoch: 346 [16293/17352 (94%)] Loss: -85623.539062\n",
      "Train Epoch: 346 [17143/17352 (99%)] Loss: -123867.773438\n",
      "    epoch          : 346\n",
      "    loss           : -124834.38867515206\n",
      "    val_loss       : -71798.41318766277\n",
      "Train Epoch: 347 [128/17352 (1%)] Loss: -150011.562500\n",
      "Train Epoch: 347 [1536/17352 (9%)] Loss: -136721.906250\n",
      "Train Epoch: 347 [2944/17352 (17%)] Loss: -112490.531250\n",
      "Train Epoch: 347 [4352/17352 (25%)] Loss: -168093.265625\n",
      "Train Epoch: 347 [5760/17352 (33%)] Loss: -115320.132812\n",
      "Train Epoch: 347 [7168/17352 (41%)] Loss: -114333.828125\n",
      "Train Epoch: 347 [8576/17352 (49%)] Loss: -125369.343750\n",
      "Train Epoch: 347 [9984/17352 (58%)] Loss: -101433.296875\n",
      "Train Epoch: 347 [11392/17352 (66%)] Loss: -128475.710938\n",
      "Train Epoch: 347 [12800/17352 (74%)] Loss: -143497.187500\n",
      "Train Epoch: 347 [14208/17352 (82%)] Loss: -128378.898438\n",
      "Train Epoch: 347 [15484/17352 (89%)] Loss: -40106.007812\n",
      "Train Epoch: 347 [16264/17352 (94%)] Loss: -3949.130859\n",
      "Train Epoch: 347 [16908/17352 (97%)] Loss: -4923.992188\n",
      "    epoch          : 347\n",
      "    loss           : -127671.38883572778\n",
      "    val_loss       : -53666.500390625\n",
      "Train Epoch: 348 [128/17352 (1%)] Loss: -92800.671875\n",
      "Train Epoch: 348 [1536/17352 (9%)] Loss: -152244.187500\n",
      "Train Epoch: 348 [2944/17352 (17%)] Loss: -145087.421875\n",
      "Train Epoch: 348 [4352/17352 (25%)] Loss: -159474.000000\n",
      "Train Epoch: 348 [5760/17352 (33%)] Loss: -163840.531250\n",
      "Train Epoch: 348 [7168/17352 (41%)] Loss: -171714.843750\n",
      "Train Epoch: 348 [8576/17352 (49%)] Loss: -131409.234375\n",
      "Train Epoch: 348 [9984/17352 (58%)] Loss: -133538.265625\n",
      "Train Epoch: 348 [11392/17352 (66%)] Loss: -160459.656250\n",
      "Train Epoch: 348 [12800/17352 (74%)] Loss: -145886.703125\n",
      "Train Epoch: 348 [14208/17352 (82%)] Loss: -185504.609375\n",
      "Train Epoch: 348 [15419/17352 (89%)] Loss: -6197.219727\n",
      "Train Epoch: 348 [16353/17352 (94%)] Loss: -114838.203125\n",
      "Train Epoch: 348 [17117/17352 (99%)] Loss: -17390.914062\n",
      "    epoch          : 348\n",
      "    loss           : -128049.76867266149\n",
      "    val_loss       : -70183.3607014974\n",
      "Train Epoch: 349 [128/17352 (1%)] Loss: -135775.953125\n",
      "Train Epoch: 349 [1536/17352 (9%)] Loss: -122235.328125\n",
      "Train Epoch: 349 [2944/17352 (17%)] Loss: -160394.078125\n",
      "Train Epoch: 349 [4352/17352 (25%)] Loss: -142707.328125\n",
      "Train Epoch: 349 [5760/17352 (33%)] Loss: -128314.140625\n",
      "Train Epoch: 349 [7168/17352 (41%)] Loss: -143762.781250\n",
      "Train Epoch: 349 [8576/17352 (49%)] Loss: -161294.031250\n",
      "Train Epoch: 349 [9984/17352 (58%)] Loss: -136287.281250\n",
      "Train Epoch: 349 [11392/17352 (66%)] Loss: -124822.742188\n",
      "Train Epoch: 349 [12800/17352 (74%)] Loss: -126130.656250\n",
      "Train Epoch: 349 [14208/17352 (82%)] Loss: -110639.734375\n",
      "Train Epoch: 349 [15474/17352 (89%)] Loss: -17683.613281\n",
      "Train Epoch: 349 [16240/17352 (94%)] Loss: -114330.648438\n",
      "Train Epoch: 349 [17003/17352 (98%)] Loss: -48843.921875\n",
      "    epoch          : 349\n",
      "    loss           : -127286.09779388632\n",
      "    val_loss       : -53698.736332194014\n",
      "Train Epoch: 350 [128/17352 (1%)] Loss: -73276.156250\n",
      "Train Epoch: 350 [1536/17352 (9%)] Loss: -133421.984375\n",
      "Train Epoch: 350 [2944/17352 (17%)] Loss: -140371.890625\n",
      "Train Epoch: 350 [4352/17352 (25%)] Loss: -163822.531250\n",
      "Train Epoch: 350 [5760/17352 (33%)] Loss: -146167.000000\n",
      "Train Epoch: 350 [7168/17352 (41%)] Loss: -140677.750000\n",
      "Train Epoch: 350 [8576/17352 (49%)] Loss: -107515.171875\n",
      "Train Epoch: 350 [9984/17352 (58%)] Loss: -110316.359375\n",
      "Train Epoch: 350 [11392/17352 (66%)] Loss: -131031.750000\n",
      "Train Epoch: 350 [12800/17352 (74%)] Loss: -142232.078125\n",
      "Train Epoch: 350 [14208/17352 (82%)] Loss: -128700.265625\n",
      "Train Epoch: 350 [15480/17352 (89%)] Loss: -51677.734375\n",
      "Train Epoch: 350 [16228/17352 (94%)] Loss: -81501.101562\n",
      "Train Epoch: 350 [16880/17352 (97%)] Loss: -83254.554688\n",
      "    epoch          : 350\n",
      "    loss           : -120152.5345286939\n",
      "    val_loss       : -68804.75114339193\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [128/17352 (1%)] Loss: -145971.984375\n",
      "Train Epoch: 351 [1536/17352 (9%)] Loss: -155257.250000\n",
      "Train Epoch: 351 [2944/17352 (17%)] Loss: -154429.718750\n",
      "Train Epoch: 351 [4352/17352 (25%)] Loss: -153051.171875\n",
      "Train Epoch: 351 [5760/17352 (33%)] Loss: -160886.703125\n",
      "Train Epoch: 351 [7168/17352 (41%)] Loss: -166128.031250\n",
      "Train Epoch: 351 [8576/17352 (49%)] Loss: -131573.187500\n",
      "Train Epoch: 351 [9984/17352 (58%)] Loss: -166393.484375\n",
      "Train Epoch: 351 [11392/17352 (66%)] Loss: -143276.734375\n",
      "Train Epoch: 351 [12800/17352 (74%)] Loss: -149510.859375\n",
      "Train Epoch: 351 [14208/17352 (82%)] Loss: -174481.593750\n",
      "Train Epoch: 351 [15565/17352 (90%)] Loss: -88551.125000\n",
      "Train Epoch: 351 [16486/17352 (95%)] Loss: -44195.953125\n",
      "Train Epoch: 351 [17075/17352 (98%)] Loss: -5217.398926\n",
      "    epoch          : 351\n",
      "    loss           : -133487.1667496854\n",
      "    val_loss       : -71579.78245035808\n",
      "Train Epoch: 352 [128/17352 (1%)] Loss: -140012.343750\n",
      "Train Epoch: 352 [1536/17352 (9%)] Loss: -152987.281250\n",
      "Train Epoch: 352 [2944/17352 (17%)] Loss: -140865.875000\n",
      "Train Epoch: 352 [4352/17352 (25%)] Loss: -141126.187500\n",
      "Train Epoch: 352 [5760/17352 (33%)] Loss: -155917.265625\n",
      "Train Epoch: 352 [7168/17352 (41%)] Loss: -125493.445312\n",
      "Train Epoch: 352 [8576/17352 (49%)] Loss: -165653.140625\n",
      "Train Epoch: 352 [9984/17352 (58%)] Loss: -172213.562500\n",
      "Train Epoch: 352 [11392/17352 (66%)] Loss: -156578.250000\n",
      "Train Epoch: 352 [12800/17352 (74%)] Loss: -149308.390625\n",
      "Train Epoch: 352 [14208/17352 (82%)] Loss: -158826.968750\n",
      "Train Epoch: 352 [15534/17352 (90%)] Loss: -86135.687500\n",
      "Train Epoch: 352 [16211/17352 (93%)] Loss: -141536.734375\n",
      "Train Epoch: 352 [16986/17352 (98%)] Loss: -3177.236084\n",
      "    epoch          : 352\n",
      "    loss           : -132593.77017519137\n",
      "    val_loss       : -65858.3259033203\n",
      "Train Epoch: 353 [128/17352 (1%)] Loss: -155061.015625\n",
      "Train Epoch: 353 [1536/17352 (9%)] Loss: -136911.906250\n",
      "Train Epoch: 353 [2944/17352 (17%)] Loss: -136122.640625\n",
      "Train Epoch: 353 [4352/17352 (25%)] Loss: -149724.687500\n",
      "Train Epoch: 353 [5760/17352 (33%)] Loss: -154902.078125\n",
      "Train Epoch: 353 [7168/17352 (41%)] Loss: -127402.968750\n",
      "Train Epoch: 353 [8576/17352 (49%)] Loss: -103350.031250\n",
      "Train Epoch: 353 [9984/17352 (58%)] Loss: -139153.500000\n",
      "Train Epoch: 353 [11392/17352 (66%)] Loss: -106165.757812\n",
      "Train Epoch: 353 [12800/17352 (74%)] Loss: -139499.546875\n",
      "Train Epoch: 353 [14208/17352 (82%)] Loss: -124165.171875\n",
      "Train Epoch: 353 [15488/17352 (89%)] Loss: -82093.960938\n",
      "Train Epoch: 353 [16330/17352 (94%)] Loss: -42608.714844\n",
      "Train Epoch: 353 [17052/17352 (98%)] Loss: -57384.964844\n",
      "    epoch          : 353\n",
      "    loss           : -123695.19704442377\n",
      "    val_loss       : -72391.41153971355\n",
      "Train Epoch: 354 [128/17352 (1%)] Loss: -135295.593750\n",
      "Train Epoch: 354 [1536/17352 (9%)] Loss: -103172.726562\n",
      "Train Epoch: 354 [2944/17352 (17%)] Loss: -193031.125000\n",
      "Train Epoch: 354 [4352/17352 (25%)] Loss: -166495.765625\n",
      "Train Epoch: 354 [5760/17352 (33%)] Loss: -138867.921875\n",
      "Train Epoch: 354 [7168/17352 (41%)] Loss: -144918.593750\n",
      "Train Epoch: 354 [8576/17352 (49%)] Loss: -151487.562500\n",
      "Train Epoch: 354 [9984/17352 (58%)] Loss: -129488.500000\n",
      "Train Epoch: 354 [11392/17352 (66%)] Loss: -144500.656250\n",
      "Train Epoch: 354 [12800/17352 (74%)] Loss: -113324.609375\n",
      "Train Epoch: 354 [14208/17352 (82%)] Loss: -142888.203125\n",
      "Train Epoch: 354 [15570/17352 (90%)] Loss: -72757.773438\n",
      "Train Epoch: 354 [16286/17352 (94%)] Loss: -122396.609375\n",
      "Train Epoch: 354 [17091/17352 (98%)] Loss: -48901.875000\n",
      "    epoch          : 354\n",
      "    loss           : -128906.437323039\n",
      "    val_loss       : -65185.05310465495\n",
      "Train Epoch: 355 [128/17352 (1%)] Loss: -135918.500000\n",
      "Train Epoch: 355 [1536/17352 (9%)] Loss: -103580.289062\n",
      "Train Epoch: 355 [2944/17352 (17%)] Loss: -168300.562500\n",
      "Train Epoch: 355 [4352/17352 (25%)] Loss: -158980.734375\n",
      "Train Epoch: 355 [5760/17352 (33%)] Loss: -165078.015625\n",
      "Train Epoch: 355 [7168/17352 (41%)] Loss: -127495.906250\n",
      "Train Epoch: 355 [8576/17352 (49%)] Loss: -125886.648438\n",
      "Train Epoch: 355 [9984/17352 (58%)] Loss: -109584.125000\n",
      "Train Epoch: 355 [11392/17352 (66%)] Loss: -165226.859375\n",
      "Train Epoch: 355 [12800/17352 (74%)] Loss: -120800.078125\n",
      "Train Epoch: 355 [14208/17352 (82%)] Loss: -109917.406250\n",
      "Train Epoch: 355 [15461/17352 (89%)] Loss: -4993.913574\n",
      "Train Epoch: 355 [16435/17352 (95%)] Loss: -100015.828125\n",
      "Train Epoch: 355 [17121/17352 (99%)] Loss: -93275.234375\n",
      "    epoch          : 355\n",
      "    loss           : -126547.27874141412\n",
      "    val_loss       : -65521.4160563151\n",
      "Train Epoch: 356 [128/17352 (1%)] Loss: -111883.382812\n",
      "Train Epoch: 356 [1536/17352 (9%)] Loss: -146446.687500\n",
      "Train Epoch: 356 [2944/17352 (17%)] Loss: -133539.593750\n",
      "Train Epoch: 356 [4352/17352 (25%)] Loss: -152851.421875\n",
      "Train Epoch: 356 [5760/17352 (33%)] Loss: -138506.781250\n",
      "Train Epoch: 356 [7168/17352 (41%)] Loss: -96204.765625\n",
      "Train Epoch: 356 [8576/17352 (49%)] Loss: -107426.601562\n",
      "Train Epoch: 356 [9984/17352 (58%)] Loss: -127127.359375\n",
      "Train Epoch: 356 [11392/17352 (66%)] Loss: -171384.093750\n",
      "Train Epoch: 356 [12800/17352 (74%)] Loss: -172795.312500\n",
      "Train Epoch: 356 [14208/17352 (82%)] Loss: -150272.375000\n",
      "Train Epoch: 356 [15542/17352 (90%)] Loss: -100592.273438\n",
      "Train Epoch: 356 [16081/17352 (93%)] Loss: -89517.375000\n",
      "Train Epoch: 356 [17006/17352 (98%)] Loss: -45855.585938\n",
      "    epoch          : 356\n",
      "    loss           : -125694.07945220743\n",
      "    val_loss       : -72383.59388427735\n",
      "Train Epoch: 357 [128/17352 (1%)] Loss: -137681.906250\n",
      "Train Epoch: 357 [1536/17352 (9%)] Loss: -173493.812500\n",
      "Train Epoch: 357 [2944/17352 (17%)] Loss: -134166.468750\n",
      "Train Epoch: 357 [4352/17352 (25%)] Loss: -137207.328125\n",
      "Train Epoch: 357 [5760/17352 (33%)] Loss: -141314.906250\n",
      "Train Epoch: 357 [7168/17352 (41%)] Loss: -139033.765625\n",
      "Train Epoch: 357 [8576/17352 (49%)] Loss: -140294.375000\n",
      "Train Epoch: 357 [9984/17352 (58%)] Loss: -142410.109375\n",
      "Train Epoch: 357 [11392/17352 (66%)] Loss: -122480.585938\n",
      "Train Epoch: 357 [12800/17352 (74%)] Loss: -123370.265625\n",
      "Train Epoch: 357 [14208/17352 (82%)] Loss: -165421.890625\n",
      "Train Epoch: 357 [15555/17352 (90%)] Loss: -125643.796875\n",
      "Train Epoch: 357 [16391/17352 (94%)] Loss: -69567.359375\n",
      "Train Epoch: 357 [17121/17352 (99%)] Loss: -127022.687500\n",
      "    epoch          : 357\n",
      "    loss           : -130678.7303327522\n",
      "    val_loss       : -48027.48768717448\n",
      "Train Epoch: 358 [128/17352 (1%)] Loss: -97397.484375\n",
      "Train Epoch: 358 [1536/17352 (9%)] Loss: -72023.492188\n",
      "Train Epoch: 358 [2944/17352 (17%)] Loss: -143336.250000\n",
      "Train Epoch: 358 [4352/17352 (25%)] Loss: -127347.054688\n",
      "Train Epoch: 358 [5760/17352 (33%)] Loss: -153908.812500\n",
      "Train Epoch: 358 [7168/17352 (41%)] Loss: -162146.843750\n",
      "Train Epoch: 358 [8576/17352 (49%)] Loss: -173504.281250\n",
      "Train Epoch: 358 [9984/17352 (58%)] Loss: -141817.796875\n",
      "Train Epoch: 358 [11392/17352 (66%)] Loss: -136084.687500\n",
      "Train Epoch: 358 [12800/17352 (74%)] Loss: -153437.421875\n",
      "Train Epoch: 358 [14208/17352 (82%)] Loss: -102354.500000\n",
      "Train Epoch: 358 [15571/17352 (90%)] Loss: -145712.140625\n",
      "Train Epoch: 358 [16191/17352 (93%)] Loss: -43468.019531\n",
      "Train Epoch: 358 [16921/17352 (98%)] Loss: -83720.390625\n",
      "    epoch          : 358\n",
      "    loss           : -127999.74477473521\n",
      "    val_loss       : -58582.53375651042\n",
      "Train Epoch: 359 [128/17352 (1%)] Loss: -121610.984375\n",
      "Train Epoch: 359 [1536/17352 (9%)] Loss: -77416.750000\n",
      "Train Epoch: 359 [2944/17352 (17%)] Loss: -123736.429688\n",
      "Train Epoch: 359 [4352/17352 (25%)] Loss: -137052.906250\n",
      "Train Epoch: 359 [5760/17352 (33%)] Loss: -127001.148438\n",
      "Train Epoch: 359 [7168/17352 (41%)] Loss: -104661.750000\n",
      "Train Epoch: 359 [8576/17352 (49%)] Loss: -140112.187500\n",
      "Train Epoch: 359 [9984/17352 (58%)] Loss: -132086.156250\n",
      "Train Epoch: 359 [11392/17352 (66%)] Loss: -171125.921875\n",
      "Train Epoch: 359 [12800/17352 (74%)] Loss: -105486.203125\n",
      "Train Epoch: 359 [14208/17352 (82%)] Loss: -185682.437500\n",
      "Train Epoch: 359 [15453/17352 (89%)] Loss: -54815.242188\n",
      "Train Epoch: 359 [16173/17352 (93%)] Loss: -95781.992188\n",
      "Train Epoch: 359 [16974/17352 (98%)] Loss: -22900.046875\n",
      "    epoch          : 359\n",
      "    loss           : -120373.38372679845\n",
      "    val_loss       : -44423.17708740234\n",
      "Train Epoch: 360 [128/17352 (1%)] Loss: -71863.984375\n",
      "Train Epoch: 360 [1536/17352 (9%)] Loss: -136998.234375\n",
      "Train Epoch: 360 [2944/17352 (17%)] Loss: -123087.046875\n",
      "Train Epoch: 360 [4352/17352 (25%)] Loss: -155917.593750\n",
      "Train Epoch: 360 [5760/17352 (33%)] Loss: -160569.187500\n",
      "Train Epoch: 360 [7168/17352 (41%)] Loss: -155475.734375\n",
      "Train Epoch: 360 [8576/17352 (49%)] Loss: -115439.890625\n",
      "Train Epoch: 360 [9984/17352 (58%)] Loss: -126127.015625\n",
      "Train Epoch: 360 [11392/17352 (66%)] Loss: -159243.640625\n",
      "Train Epoch: 360 [12800/17352 (74%)] Loss: -130448.476562\n",
      "Train Epoch: 360 [14208/17352 (82%)] Loss: -117109.109375\n",
      "Train Epoch: 360 [15542/17352 (90%)] Loss: -97002.828125\n",
      "Train Epoch: 360 [16131/17352 (93%)] Loss: -40899.945312\n",
      "Train Epoch: 360 [16922/17352 (98%)] Loss: -40686.609375\n",
      "    epoch          : 360\n",
      "    loss           : -120715.22956100566\n",
      "    val_loss       : -59742.259537760416\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch360.pth ...\n",
      "Train Epoch: 361 [128/17352 (1%)] Loss: -113602.234375\n",
      "Train Epoch: 361 [1536/17352 (9%)] Loss: -130494.710938\n",
      "Train Epoch: 361 [2944/17352 (17%)] Loss: -161311.859375\n",
      "Train Epoch: 361 [4352/17352 (25%)] Loss: -160832.750000\n",
      "Train Epoch: 361 [5760/17352 (33%)] Loss: -128867.054688\n",
      "Train Epoch: 361 [7168/17352 (41%)] Loss: -196109.453125\n",
      "Train Epoch: 361 [8576/17352 (49%)] Loss: -158320.718750\n",
      "Train Epoch: 361 [9984/17352 (58%)] Loss: -166581.375000\n",
      "Train Epoch: 361 [11392/17352 (66%)] Loss: -159944.000000\n",
      "Train Epoch: 361 [12800/17352 (74%)] Loss: -138953.687500\n",
      "Train Epoch: 361 [14208/17352 (82%)] Loss: -133957.875000\n",
      "Train Epoch: 361 [15471/17352 (89%)] Loss: -106419.101562\n",
      "Train Epoch: 361 [16101/17352 (93%)] Loss: -62940.421875\n",
      "Train Epoch: 361 [16908/17352 (97%)] Loss: -101583.093750\n",
      "    epoch          : 361\n",
      "    loss           : -131792.5961062028\n",
      "    val_loss       : -74139.38981526693\n",
      "Train Epoch: 362 [128/17352 (1%)] Loss: -128808.132812\n",
      "Train Epoch: 362 [1536/17352 (9%)] Loss: -134945.031250\n",
      "Train Epoch: 362 [2944/17352 (17%)] Loss: -137228.609375\n",
      "Train Epoch: 362 [4352/17352 (25%)] Loss: -166702.687500\n",
      "Train Epoch: 362 [5760/17352 (33%)] Loss: -141392.781250\n",
      "Train Epoch: 362 [7168/17352 (41%)] Loss: -130330.523438\n",
      "Train Epoch: 362 [8576/17352 (49%)] Loss: -142793.156250\n",
      "Train Epoch: 362 [9984/17352 (58%)] Loss: -149910.187500\n",
      "Train Epoch: 362 [11392/17352 (66%)] Loss: -172590.531250\n",
      "Train Epoch: 362 [12800/17352 (74%)] Loss: -102362.921875\n",
      "Train Epoch: 362 [14208/17352 (82%)] Loss: -99112.984375\n",
      "Train Epoch: 362 [15540/17352 (90%)] Loss: -95848.101562\n",
      "Train Epoch: 362 [16322/17352 (94%)] Loss: -69007.265625\n",
      "Train Epoch: 362 [17093/17352 (99%)] Loss: -109079.210938\n",
      "    epoch          : 362\n",
      "    loss           : -119407.6061257996\n",
      "    val_loss       : -63716.14383951823\n",
      "Train Epoch: 363 [128/17352 (1%)] Loss: -130815.406250\n",
      "Train Epoch: 363 [1536/17352 (9%)] Loss: -103379.156250\n",
      "Train Epoch: 363 [2944/17352 (17%)] Loss: -125715.234375\n",
      "Train Epoch: 363 [4352/17352 (25%)] Loss: -153455.375000\n",
      "Train Epoch: 363 [5760/17352 (33%)] Loss: -126495.132812\n",
      "Train Epoch: 363 [7168/17352 (41%)] Loss: -131441.468750\n",
      "Train Epoch: 363 [8576/17352 (49%)] Loss: -130801.101562\n",
      "Train Epoch: 363 [9984/17352 (58%)] Loss: -168553.343750\n",
      "Train Epoch: 363 [11392/17352 (66%)] Loss: -144883.968750\n",
      "Train Epoch: 363 [12800/17352 (74%)] Loss: -155140.593750\n",
      "Train Epoch: 363 [14208/17352 (82%)] Loss: -162516.828125\n",
      "Train Epoch: 363 [15470/17352 (89%)] Loss: -4907.824219\n",
      "Train Epoch: 363 [16328/17352 (94%)] Loss: -91133.890625\n",
      "Train Epoch: 363 [16965/17352 (98%)] Loss: -83843.085938\n",
      "    epoch          : 363\n",
      "    loss           : -127919.78318837825\n",
      "    val_loss       : -54507.36804199219\n",
      "Train Epoch: 364 [128/17352 (1%)] Loss: -130461.203125\n",
      "Train Epoch: 364 [1536/17352 (9%)] Loss: -151822.312500\n",
      "Train Epoch: 364 [2944/17352 (17%)] Loss: -106179.585938\n",
      "Train Epoch: 364 [4352/17352 (25%)] Loss: -136757.343750\n",
      "Train Epoch: 364 [5760/17352 (33%)] Loss: -150055.625000\n",
      "Train Epoch: 364 [7168/17352 (41%)] Loss: -155509.171875\n",
      "Train Epoch: 364 [8576/17352 (49%)] Loss: -139447.140625\n",
      "Train Epoch: 364 [9984/17352 (58%)] Loss: -138107.406250\n",
      "Train Epoch: 364 [11392/17352 (66%)] Loss: -138548.281250\n",
      "Train Epoch: 364 [12800/17352 (74%)] Loss: -141648.234375\n",
      "Train Epoch: 364 [14208/17352 (82%)] Loss: -155509.156250\n",
      "Train Epoch: 364 [15468/17352 (89%)] Loss: -64048.218750\n",
      "Train Epoch: 364 [16172/17352 (93%)] Loss: -128120.250000\n",
      "Train Epoch: 364 [17073/17352 (98%)] Loss: -111232.171875\n",
      "    epoch          : 364\n",
      "    loss           : -126634.74669345113\n",
      "    val_loss       : -74044.65616455078\n",
      "Train Epoch: 365 [128/17352 (1%)] Loss: -156466.281250\n",
      "Train Epoch: 365 [1536/17352 (9%)] Loss: -149753.656250\n",
      "Train Epoch: 365 [2944/17352 (17%)] Loss: -151629.921875\n",
      "Train Epoch: 365 [4352/17352 (25%)] Loss: -163128.296875\n",
      "Train Epoch: 365 [5760/17352 (33%)] Loss: -135541.359375\n",
      "Train Epoch: 365 [7168/17352 (41%)] Loss: -130931.796875\n",
      "Train Epoch: 365 [8576/17352 (49%)] Loss: -164231.187500\n",
      "Train Epoch: 365 [9984/17352 (58%)] Loss: -155994.843750\n",
      "Train Epoch: 365 [11392/17352 (66%)] Loss: -136695.484375\n",
      "Train Epoch: 365 [12800/17352 (74%)] Loss: -153801.078125\n",
      "Train Epoch: 365 [14208/17352 (82%)] Loss: -122369.687500\n",
      "Train Epoch: 365 [15522/17352 (89%)] Loss: -79467.132812\n",
      "Train Epoch: 365 [16327/17352 (94%)] Loss: -43807.359375\n",
      "Train Epoch: 365 [17141/17352 (99%)] Loss: -99409.226562\n",
      "    epoch          : 365\n",
      "    loss           : -134012.80963913066\n",
      "    val_loss       : -72052.39635009765\n",
      "Train Epoch: 366 [128/17352 (1%)] Loss: -123764.210938\n",
      "Train Epoch: 366 [1536/17352 (9%)] Loss: -106330.148438\n",
      "Train Epoch: 366 [2944/17352 (17%)] Loss: -133687.515625\n",
      "Train Epoch: 366 [4352/17352 (25%)] Loss: -141561.500000\n",
      "Train Epoch: 366 [5760/17352 (33%)] Loss: -135025.562500\n",
      "Train Epoch: 366 [7168/17352 (41%)] Loss: -182593.203125\n",
      "Train Epoch: 366 [8576/17352 (49%)] Loss: -123054.476562\n",
      "Train Epoch: 366 [9984/17352 (58%)] Loss: -177482.140625\n",
      "Train Epoch: 366 [11392/17352 (66%)] Loss: -130448.835938\n",
      "Train Epoch: 366 [12800/17352 (74%)] Loss: -123801.140625\n",
      "Train Epoch: 366 [14208/17352 (82%)] Loss: -166153.187500\n",
      "Train Epoch: 366 [15459/17352 (89%)] Loss: -112123.296875\n",
      "Train Epoch: 366 [16256/17352 (94%)] Loss: -77916.835938\n",
      "Train Epoch: 366 [16965/17352 (98%)] Loss: -108436.195312\n",
      "    epoch          : 366\n",
      "    loss           : -133190.97053599518\n",
      "    val_loss       : -70458.78157958985\n",
      "Train Epoch: 367 [128/17352 (1%)] Loss: -140868.312500\n",
      "Train Epoch: 367 [1536/17352 (9%)] Loss: -122879.812500\n",
      "Train Epoch: 367 [2944/17352 (17%)] Loss: -141406.156250\n",
      "Train Epoch: 367 [4352/17352 (25%)] Loss: -150648.625000\n",
      "Train Epoch: 367 [5760/17352 (33%)] Loss: -135180.750000\n",
      "Train Epoch: 367 [7168/17352 (41%)] Loss: -176557.671875\n",
      "Train Epoch: 367 [8576/17352 (49%)] Loss: -116257.789062\n",
      "Train Epoch: 367 [9984/17352 (58%)] Loss: -151954.125000\n",
      "Train Epoch: 367 [11392/17352 (66%)] Loss: -111747.085938\n",
      "Train Epoch: 367 [12800/17352 (74%)] Loss: -113268.335938\n",
      "Train Epoch: 367 [14208/17352 (82%)] Loss: -139097.593750\n",
      "Train Epoch: 367 [15483/17352 (89%)] Loss: -48654.281250\n",
      "Train Epoch: 367 [16460/17352 (95%)] Loss: -104519.039062\n",
      "Train Epoch: 367 [17064/17352 (98%)] Loss: -101946.656250\n",
      "    epoch          : 367\n",
      "    loss           : -125300.58323065226\n",
      "    val_loss       : -70290.862113444\n",
      "Train Epoch: 368 [128/17352 (1%)] Loss: -117927.070312\n",
      "Train Epoch: 368 [1536/17352 (9%)] Loss: -173056.265625\n",
      "Train Epoch: 368 [2944/17352 (17%)] Loss: -84796.953125\n",
      "Train Epoch: 368 [4352/17352 (25%)] Loss: -119102.992188\n",
      "Train Epoch: 368 [5760/17352 (33%)] Loss: -121283.390625\n",
      "Train Epoch: 368 [7168/17352 (41%)] Loss: -134688.875000\n",
      "Train Epoch: 368 [8576/17352 (49%)] Loss: -154690.312500\n",
      "Train Epoch: 368 [9984/17352 (58%)] Loss: -155559.281250\n",
      "Train Epoch: 368 [11392/17352 (66%)] Loss: -98206.984375\n",
      "Train Epoch: 368 [12800/17352 (74%)] Loss: -108812.601562\n",
      "Train Epoch: 368 [14208/17352 (82%)] Loss: -124458.265625\n",
      "Train Epoch: 368 [15490/17352 (89%)] Loss: -90957.203125\n",
      "Train Epoch: 368 [16284/17352 (94%)] Loss: -63628.964844\n",
      "Train Epoch: 368 [16935/17352 (98%)] Loss: -104794.000000\n",
      "    epoch          : 368\n",
      "    loss           : -124447.14540622379\n",
      "    val_loss       : -66249.20132242839\n",
      "Train Epoch: 369 [128/17352 (1%)] Loss: -132372.734375\n",
      "Train Epoch: 369 [1536/17352 (9%)] Loss: -148311.546875\n",
      "Train Epoch: 369 [2944/17352 (17%)] Loss: -92880.546875\n",
      "Train Epoch: 369 [4352/17352 (25%)] Loss: -132164.609375\n",
      "Train Epoch: 369 [5760/17352 (33%)] Loss: -131711.875000\n",
      "Train Epoch: 369 [7168/17352 (41%)] Loss: -161059.437500\n",
      "Train Epoch: 369 [8576/17352 (49%)] Loss: -163371.187500\n",
      "Train Epoch: 369 [9984/17352 (58%)] Loss: -115992.937500\n",
      "Train Epoch: 369 [11392/17352 (66%)] Loss: -109881.203125\n",
      "Train Epoch: 369 [12800/17352 (74%)] Loss: -173036.296875\n",
      "Train Epoch: 369 [14208/17352 (82%)] Loss: -154285.796875\n",
      "Train Epoch: 369 [15466/17352 (89%)] Loss: -6012.011719\n",
      "Train Epoch: 369 [16031/17352 (92%)] Loss: -10379.197266\n",
      "Train Epoch: 369 [17004/17352 (98%)] Loss: -90721.218750\n",
      "    epoch          : 369\n",
      "    loss           : -128729.34991004482\n",
      "    val_loss       : -57084.764587402344\n",
      "Train Epoch: 370 [128/17352 (1%)] Loss: -107555.203125\n",
      "Train Epoch: 370 [1536/17352 (9%)] Loss: -100671.015625\n",
      "Train Epoch: 370 [2944/17352 (17%)] Loss: -96544.578125\n",
      "Train Epoch: 370 [4352/17352 (25%)] Loss: -121177.406250\n",
      "Train Epoch: 370 [5760/17352 (33%)] Loss: -115227.359375\n",
      "Train Epoch: 370 [7168/17352 (41%)] Loss: -142093.187500\n",
      "Train Epoch: 370 [8576/17352 (49%)] Loss: -121757.609375\n",
      "Train Epoch: 370 [9984/17352 (58%)] Loss: -160474.312500\n",
      "Train Epoch: 370 [11392/17352 (66%)] Loss: -131566.406250\n",
      "Train Epoch: 370 [12800/17352 (74%)] Loss: -94689.000000\n",
      "Train Epoch: 370 [14208/17352 (82%)] Loss: -131284.421875\n",
      "Train Epoch: 370 [15409/17352 (89%)] Loss: -9484.375000\n",
      "Train Epoch: 370 [16138/17352 (93%)] Loss: -81638.937500\n",
      "Train Epoch: 370 [16957/17352 (98%)] Loss: -35624.144531\n",
      "    epoch          : 370\n",
      "    loss           : -122773.84343867975\n",
      "    val_loss       : -69417.98553873698\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch370.pth ...\n",
      "Train Epoch: 371 [128/17352 (1%)] Loss: -117631.078125\n",
      "Train Epoch: 371 [1536/17352 (9%)] Loss: -163788.656250\n",
      "Train Epoch: 371 [2944/17352 (17%)] Loss: -125581.390625\n",
      "Train Epoch: 371 [4352/17352 (25%)] Loss: -152916.078125\n",
      "Train Epoch: 371 [5760/17352 (33%)] Loss: -88976.062500\n",
      "Train Epoch: 371 [7168/17352 (41%)] Loss: -135613.781250\n",
      "Train Epoch: 371 [8576/17352 (49%)] Loss: -129003.359375\n",
      "Train Epoch: 371 [9984/17352 (58%)] Loss: -123506.101562\n",
      "Train Epoch: 371 [11392/17352 (66%)] Loss: -158166.531250\n",
      "Train Epoch: 371 [12800/17352 (74%)] Loss: -131211.562500\n",
      "Train Epoch: 371 [14208/17352 (82%)] Loss: -147806.937500\n",
      "Train Epoch: 371 [15492/17352 (89%)] Loss: -84524.109375\n",
      "Train Epoch: 371 [16313/17352 (94%)] Loss: -105174.460938\n",
      "Train Epoch: 371 [16943/17352 (98%)] Loss: -120477.164062\n",
      "    epoch          : 371\n",
      "    loss           : -126411.62460675335\n",
      "    val_loss       : -72604.86747639974\n",
      "Train Epoch: 372 [128/17352 (1%)] Loss: -169705.359375\n",
      "Train Epoch: 372 [1536/17352 (9%)] Loss: -180569.296875\n",
      "Train Epoch: 372 [2944/17352 (17%)] Loss: -129392.593750\n",
      "Train Epoch: 372 [4352/17352 (25%)] Loss: -115855.015625\n",
      "Train Epoch: 372 [5760/17352 (33%)] Loss: -139361.687500\n",
      "Train Epoch: 372 [7168/17352 (41%)] Loss: -146153.750000\n",
      "Train Epoch: 372 [8576/17352 (49%)] Loss: -139856.562500\n",
      "Train Epoch: 372 [9984/17352 (58%)] Loss: -58732.441406\n",
      "Train Epoch: 372 [11392/17352 (66%)] Loss: -62260.468750\n",
      "Train Epoch: 372 [12800/17352 (74%)] Loss: -129269.312500\n",
      "Train Epoch: 372 [14208/17352 (82%)] Loss: -147942.421875\n",
      "Train Epoch: 372 [15484/17352 (89%)] Loss: -76358.468750\n",
      "Train Epoch: 372 [16397/17352 (94%)] Loss: -65775.570312\n",
      "Train Epoch: 372 [17111/17352 (99%)] Loss: -43119.632812\n",
      "    epoch          : 372\n",
      "    loss           : -118713.85160018614\n",
      "    val_loss       : -70982.52427571615\n",
      "Train Epoch: 373 [128/17352 (1%)] Loss: -138041.562500\n",
      "Train Epoch: 373 [1536/17352 (9%)] Loss: -153257.156250\n",
      "Train Epoch: 373 [2944/17352 (17%)] Loss: -163600.531250\n",
      "Train Epoch: 373 [4352/17352 (25%)] Loss: -129465.382812\n",
      "Train Epoch: 373 [5760/17352 (33%)] Loss: -107834.718750\n",
      "Train Epoch: 373 [7168/17352 (41%)] Loss: -164040.562500\n",
      "Train Epoch: 373 [8576/17352 (49%)] Loss: -134913.078125\n",
      "Train Epoch: 373 [9984/17352 (58%)] Loss: -158406.968750\n",
      "Train Epoch: 373 [11392/17352 (66%)] Loss: -155396.359375\n",
      "Train Epoch: 373 [12800/17352 (74%)] Loss: -143697.500000\n",
      "Train Epoch: 373 [14208/17352 (82%)] Loss: -157306.281250\n",
      "Train Epoch: 373 [15494/17352 (89%)] Loss: -56327.648438\n",
      "Train Epoch: 373 [16110/17352 (93%)] Loss: -82462.015625\n",
      "Train Epoch: 373 [16921/17352 (98%)] Loss: -39939.230469\n",
      "    epoch          : 373\n",
      "    loss           : -131796.37088729552\n",
      "    val_loss       : -58553.20470377604\n",
      "Train Epoch: 374 [128/17352 (1%)] Loss: -87070.437500\n",
      "Train Epoch: 374 [1536/17352 (9%)] Loss: -125980.289062\n",
      "Train Epoch: 374 [2944/17352 (17%)] Loss: -112257.609375\n",
      "Train Epoch: 374 [4352/17352 (25%)] Loss: -162908.437500\n",
      "Train Epoch: 374 [5760/17352 (33%)] Loss: -127142.539062\n",
      "Train Epoch: 374 [7168/17352 (41%)] Loss: -135270.015625\n",
      "Train Epoch: 374 [8576/17352 (49%)] Loss: -91927.234375\n",
      "Train Epoch: 374 [9984/17352 (58%)] Loss: -173976.015625\n",
      "Train Epoch: 374 [11392/17352 (66%)] Loss: -111705.609375\n",
      "Train Epoch: 374 [12800/17352 (74%)] Loss: -160347.140625\n",
      "Train Epoch: 374 [14208/17352 (82%)] Loss: -148592.609375\n",
      "Train Epoch: 374 [15527/17352 (89%)] Loss: -89681.585938\n",
      "Train Epoch: 374 [16125/17352 (93%)] Loss: -62563.679688\n",
      "Train Epoch: 374 [16977/17352 (98%)] Loss: -92313.460938\n",
      "    epoch          : 374\n",
      "    loss           : -129040.20451611001\n",
      "    val_loss       : -69779.34752197265\n",
      "Train Epoch: 375 [128/17352 (1%)] Loss: -170581.156250\n",
      "Train Epoch: 375 [1536/17352 (9%)] Loss: -104601.578125\n",
      "Train Epoch: 375 [2944/17352 (17%)] Loss: -146559.031250\n",
      "Train Epoch: 375 [4352/17352 (25%)] Loss: -158469.828125\n",
      "Train Epoch: 375 [5760/17352 (33%)] Loss: -165448.671875\n",
      "Train Epoch: 375 [7168/17352 (41%)] Loss: -112763.812500\n",
      "Train Epoch: 375 [8576/17352 (49%)] Loss: -155490.468750\n",
      "Train Epoch: 375 [9984/17352 (58%)] Loss: -129741.046875\n",
      "Train Epoch: 375 [11392/17352 (66%)] Loss: -135861.296875\n",
      "Train Epoch: 375 [12800/17352 (74%)] Loss: -149575.500000\n",
      "Train Epoch: 375 [14208/17352 (82%)] Loss: -132119.812500\n",
      "Train Epoch: 375 [15534/17352 (90%)] Loss: -88921.015625\n",
      "Train Epoch: 375 [16225/17352 (94%)] Loss: -70421.382812\n",
      "Train Epoch: 375 [17076/17352 (98%)] Loss: -6051.397461\n",
      "    epoch          : 375\n",
      "    loss           : -125737.96546639052\n",
      "    val_loss       : -37448.806860351564\n",
      "Train Epoch: 376 [128/17352 (1%)] Loss: -74015.187500\n",
      "Train Epoch: 376 [1536/17352 (9%)] Loss: -141189.296875\n",
      "Train Epoch: 376 [2944/17352 (17%)] Loss: -141309.812500\n",
      "Train Epoch: 376 [4352/17352 (25%)] Loss: -161876.390625\n",
      "Train Epoch: 376 [5760/17352 (33%)] Loss: -132490.250000\n",
      "Train Epoch: 376 [7168/17352 (41%)] Loss: -144373.093750\n",
      "Train Epoch: 376 [8576/17352 (49%)] Loss: -172416.187500\n",
      "Train Epoch: 376 [9984/17352 (58%)] Loss: -145758.453125\n",
      "Train Epoch: 376 [11392/17352 (66%)] Loss: -156089.906250\n",
      "Train Epoch: 376 [12800/17352 (74%)] Loss: -146626.687500\n",
      "Train Epoch: 376 [14208/17352 (82%)] Loss: -190682.203125\n",
      "Train Epoch: 376 [15504/17352 (89%)] Loss: -100169.007812\n",
      "Train Epoch: 376 [16310/17352 (94%)] Loss: -86034.812500\n",
      "Train Epoch: 376 [16877/17352 (97%)] Loss: -30416.851562\n",
      "    epoch          : 376\n",
      "    loss           : -128666.43831434826\n",
      "    val_loss       : -65597.85126953125\n",
      "Train Epoch: 377 [128/17352 (1%)] Loss: -140382.500000\n",
      "Train Epoch: 377 [1536/17352 (9%)] Loss: -160235.187500\n",
      "Train Epoch: 377 [2944/17352 (17%)] Loss: -143539.062500\n",
      "Train Epoch: 377 [4352/17352 (25%)] Loss: -178874.125000\n",
      "Train Epoch: 377 [5760/17352 (33%)] Loss: -131839.828125\n",
      "Train Epoch: 377 [7168/17352 (41%)] Loss: -146979.343750\n",
      "Train Epoch: 377 [8576/17352 (49%)] Loss: -151823.812500\n",
      "Train Epoch: 377 [9984/17352 (58%)] Loss: -165875.281250\n",
      "Train Epoch: 377 [11392/17352 (66%)] Loss: -122925.343750\n",
      "Train Epoch: 377 [12800/17352 (74%)] Loss: -167753.531250\n",
      "Train Epoch: 377 [14208/17352 (82%)] Loss: -132164.984375\n",
      "Train Epoch: 377 [15557/17352 (90%)] Loss: -105570.328125\n",
      "Train Epoch: 377 [16356/17352 (94%)] Loss: -4474.500000\n",
      "Train Epoch: 377 [16906/17352 (97%)] Loss: -53629.757812\n",
      "    epoch          : 377\n",
      "    loss           : -128605.67853725357\n",
      "    val_loss       : -69412.89477539062\n",
      "Train Epoch: 378 [128/17352 (1%)] Loss: -140034.093750\n",
      "Train Epoch: 378 [1536/17352 (9%)] Loss: -132673.687500\n",
      "Train Epoch: 378 [2944/17352 (17%)] Loss: -146968.656250\n",
      "Train Epoch: 378 [4352/17352 (25%)] Loss: -154946.468750\n",
      "Train Epoch: 378 [5760/17352 (33%)] Loss: -145708.296875\n",
      "Train Epoch: 378 [7168/17352 (41%)] Loss: -149869.812500\n",
      "Train Epoch: 378 [8576/17352 (49%)] Loss: -168060.812500\n",
      "Train Epoch: 378 [9984/17352 (58%)] Loss: -127598.234375\n",
      "Train Epoch: 378 [11392/17352 (66%)] Loss: -142514.078125\n",
      "Train Epoch: 378 [12800/17352 (74%)] Loss: -159036.156250\n",
      "Train Epoch: 378 [14208/17352 (82%)] Loss: -162094.718750\n",
      "Train Epoch: 378 [15541/17352 (90%)] Loss: -141225.812500\n",
      "Train Epoch: 378 [16233/17352 (94%)] Loss: -50019.500000\n",
      "Train Epoch: 378 [17003/17352 (98%)] Loss: -127690.664062\n",
      "    epoch          : 378\n",
      "    loss           : -135401.07325857278\n",
      "    val_loss       : -71391.3076212565\n",
      "Train Epoch: 379 [128/17352 (1%)] Loss: -140685.609375\n",
      "Train Epoch: 379 [1536/17352 (9%)] Loss: -104604.757812\n",
      "Train Epoch: 379 [2944/17352 (17%)] Loss: -148391.531250\n",
      "Train Epoch: 379 [4352/17352 (25%)] Loss: -126253.242188\n",
      "Train Epoch: 379 [5760/17352 (33%)] Loss: -149371.625000\n",
      "Train Epoch: 379 [7168/17352 (41%)] Loss: -110625.187500\n",
      "Train Epoch: 379 [8576/17352 (49%)] Loss: -134904.812500\n",
      "Train Epoch: 379 [9984/17352 (58%)] Loss: -139106.093750\n",
      "Train Epoch: 379 [11392/17352 (66%)] Loss: -126299.218750\n",
      "Train Epoch: 379 [12800/17352 (74%)] Loss: -118124.328125\n",
      "Train Epoch: 379 [14208/17352 (82%)] Loss: -148172.218750\n",
      "Train Epoch: 379 [15425/17352 (89%)] Loss: -19728.671875\n",
      "Train Epoch: 379 [16274/17352 (94%)] Loss: -117491.539062\n",
      "Train Epoch: 379 [17041/17352 (98%)] Loss: -38701.195312\n",
      "    epoch          : 379\n",
      "    loss           : -130145.34532626363\n",
      "    val_loss       : -71098.04033203125\n",
      "Train Epoch: 380 [128/17352 (1%)] Loss: -156343.046875\n",
      "Train Epoch: 380 [1536/17352 (9%)] Loss: -149680.609375\n",
      "Train Epoch: 380 [2944/17352 (17%)] Loss: -163222.343750\n",
      "Train Epoch: 380 [4352/17352 (25%)] Loss: -153740.500000\n",
      "Train Epoch: 380 [5760/17352 (33%)] Loss: -174265.140625\n",
      "Train Epoch: 380 [7168/17352 (41%)] Loss: -145853.453125\n",
      "Train Epoch: 380 [8576/17352 (49%)] Loss: -113199.554688\n",
      "Train Epoch: 380 [9984/17352 (58%)] Loss: -159879.875000\n",
      "Train Epoch: 380 [11392/17352 (66%)] Loss: -151605.828125\n",
      "Train Epoch: 380 [12800/17352 (74%)] Loss: -147418.984375\n",
      "Train Epoch: 380 [14208/17352 (82%)] Loss: -152015.562500\n",
      "Train Epoch: 380 [15455/17352 (89%)] Loss: -39207.441406\n",
      "Train Epoch: 380 [16281/17352 (94%)] Loss: -84714.218750\n",
      "Train Epoch: 380 [16975/17352 (98%)] Loss: -53740.738281\n",
      "    epoch          : 380\n",
      "    loss           : -131651.67184222944\n",
      "    val_loss       : -68864.9087483724\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch380.pth ...\n",
      "Train Epoch: 381 [128/17352 (1%)] Loss: -145481.656250\n",
      "Train Epoch: 381 [1536/17352 (9%)] Loss: -141727.437500\n",
      "Train Epoch: 381 [2944/17352 (17%)] Loss: -1996.276123\n",
      "Train Epoch: 381 [4352/17352 (25%)] Loss: -35957.605469\n",
      "Train Epoch: 381 [5760/17352 (33%)] Loss: -75295.812500\n",
      "Train Epoch: 381 [7168/17352 (41%)] Loss: -101654.507812\n",
      "Train Epoch: 381 [8576/17352 (49%)] Loss: -116516.632812\n",
      "Train Epoch: 381 [9984/17352 (58%)] Loss: -105085.531250\n",
      "Train Epoch: 381 [11392/17352 (66%)] Loss: -118503.007812\n",
      "Train Epoch: 381 [12800/17352 (74%)] Loss: -126724.507812\n",
      "Train Epoch: 381 [14208/17352 (82%)] Loss: -116655.671875\n",
      "Train Epoch: 381 [15546/17352 (90%)] Loss: -78903.500000\n",
      "Train Epoch: 381 [16310/17352 (94%)] Loss: -17089.794922\n",
      "Train Epoch: 381 [17085/17352 (98%)] Loss: -29191.416016\n",
      "    epoch          : 381\n",
      "    loss           : -87282.06132640454\n",
      "    val_loss       : -63428.070971679685\n",
      "Train Epoch: 382 [128/17352 (1%)] Loss: -117245.828125\n",
      "Train Epoch: 382 [1536/17352 (9%)] Loss: -132381.218750\n",
      "Train Epoch: 382 [2944/17352 (17%)] Loss: -88857.234375\n",
      "Train Epoch: 382 [4352/17352 (25%)] Loss: -160978.500000\n",
      "Train Epoch: 382 [5760/17352 (33%)] Loss: -138918.937500\n",
      "Train Epoch: 382 [7168/17352 (41%)] Loss: -110080.843750\n",
      "Train Epoch: 382 [8576/17352 (49%)] Loss: -108288.195312\n",
      "Train Epoch: 382 [9984/17352 (58%)] Loss: -169249.468750\n",
      "Train Epoch: 382 [11392/17352 (66%)] Loss: -118087.101562\n",
      "Train Epoch: 382 [12800/17352 (74%)] Loss: -156005.937500\n",
      "Train Epoch: 382 [14208/17352 (82%)] Loss: -136168.281250\n",
      "Train Epoch: 382 [15570/17352 (90%)] Loss: -101895.585938\n",
      "Train Epoch: 382 [16309/17352 (94%)] Loss: -96266.257812\n",
      "Train Epoch: 382 [17158/17352 (99%)] Loss: -115735.625000\n",
      "    epoch          : 382\n",
      "    loss           : -121746.49423238255\n",
      "    val_loss       : -67855.82442626954\n",
      "Train Epoch: 383 [128/17352 (1%)] Loss: -157264.593750\n",
      "Train Epoch: 383 [1536/17352 (9%)] Loss: -162745.250000\n",
      "Train Epoch: 383 [2944/17352 (17%)] Loss: -137571.953125\n",
      "Train Epoch: 383 [4352/17352 (25%)] Loss: -141930.593750\n",
      "Train Epoch: 383 [5760/17352 (33%)] Loss: -137890.140625\n",
      "Train Epoch: 383 [7168/17352 (41%)] Loss: -131483.125000\n",
      "Train Epoch: 383 [8576/17352 (49%)] Loss: -145203.921875\n",
      "Train Epoch: 383 [9984/17352 (58%)] Loss: -169009.125000\n",
      "Train Epoch: 383 [11392/17352 (66%)] Loss: -164011.156250\n",
      "Train Epoch: 383 [12800/17352 (74%)] Loss: -140531.937500\n",
      "Train Epoch: 383 [14208/17352 (82%)] Loss: -125784.218750\n",
      "Train Epoch: 383 [15457/17352 (89%)] Loss: -48132.277344\n",
      "Train Epoch: 383 [16248/17352 (94%)] Loss: -120172.726562\n",
      "Train Epoch: 383 [17041/17352 (98%)] Loss: -85432.304688\n",
      "    epoch          : 383\n",
      "    loss           : -127888.22870241715\n",
      "    val_loss       : -73213.54348958333\n",
      "Train Epoch: 384 [128/17352 (1%)] Loss: -114198.093750\n",
      "Train Epoch: 384 [1536/17352 (9%)] Loss: -131681.359375\n",
      "Train Epoch: 384 [2944/17352 (17%)] Loss: -152282.859375\n",
      "Train Epoch: 384 [4352/17352 (25%)] Loss: -160594.500000\n",
      "Train Epoch: 384 [5760/17352 (33%)] Loss: -132073.515625\n",
      "Train Epoch: 384 [7168/17352 (41%)] Loss: -126009.953125\n",
      "Train Epoch: 384 [8576/17352 (49%)] Loss: -145230.734375\n",
      "Train Epoch: 384 [9984/17352 (58%)] Loss: -134150.078125\n",
      "Train Epoch: 384 [11392/17352 (66%)] Loss: -132623.421875\n",
      "Train Epoch: 384 [12800/17352 (74%)] Loss: -151265.218750\n",
      "Train Epoch: 384 [14208/17352 (82%)] Loss: -166670.796875\n",
      "Train Epoch: 384 [15531/17352 (90%)] Loss: -84743.734375\n",
      "Train Epoch: 384 [16208/17352 (93%)] Loss: -90824.492188\n",
      "Train Epoch: 384 [17022/17352 (98%)] Loss: -66558.523438\n",
      "    epoch          : 384\n",
      "    loss           : -131595.32768587457\n",
      "    val_loss       : -73588.9959391276\n",
      "Train Epoch: 385 [128/17352 (1%)] Loss: -126873.742188\n",
      "Train Epoch: 385 [1536/17352 (9%)] Loss: -141444.921875\n",
      "Train Epoch: 385 [2944/17352 (17%)] Loss: -142866.312500\n",
      "Train Epoch: 385 [4352/17352 (25%)] Loss: -159766.531250\n",
      "Train Epoch: 385 [5760/17352 (33%)] Loss: -160598.812500\n",
      "Train Epoch: 385 [7168/17352 (41%)] Loss: -143652.625000\n",
      "Train Epoch: 385 [8576/17352 (49%)] Loss: -123575.203125\n",
      "Train Epoch: 385 [9984/17352 (58%)] Loss: -137241.546875\n",
      "Train Epoch: 385 [11392/17352 (66%)] Loss: -160477.843750\n",
      "Train Epoch: 385 [12800/17352 (74%)] Loss: -157412.125000\n",
      "Train Epoch: 385 [14208/17352 (82%)] Loss: -166273.750000\n",
      "Train Epoch: 385 [15460/17352 (89%)] Loss: -82039.250000\n",
      "Train Epoch: 385 [16323/17352 (94%)] Loss: -87018.835938\n",
      "Train Epoch: 385 [17006/17352 (98%)] Loss: -83364.406250\n",
      "    epoch          : 385\n",
      "    loss           : -132028.1441347263\n",
      "    val_loss       : -73389.2947265625\n",
      "Train Epoch: 386 [128/17352 (1%)] Loss: -140368.031250\n",
      "Train Epoch: 386 [1536/17352 (9%)] Loss: -133267.687500\n",
      "Train Epoch: 386 [2944/17352 (17%)] Loss: -145436.187500\n",
      "Train Epoch: 386 [4352/17352 (25%)] Loss: -125021.343750\n",
      "Train Epoch: 386 [5760/17352 (33%)] Loss: -121653.515625\n",
      "Train Epoch: 386 [7168/17352 (41%)] Loss: -133770.890625\n",
      "Train Epoch: 386 [8576/17352 (49%)] Loss: -134817.312500\n",
      "Train Epoch: 386 [9984/17352 (58%)] Loss: -160655.781250\n",
      "Train Epoch: 386 [11392/17352 (66%)] Loss: -118312.031250\n",
      "Train Epoch: 386 [12800/17352 (74%)] Loss: -136330.437500\n",
      "Train Epoch: 386 [14208/17352 (82%)] Loss: -168805.968750\n",
      "Train Epoch: 386 [15542/17352 (90%)] Loss: -77546.164062\n",
      "Train Epoch: 386 [16336/17352 (94%)] Loss: -80790.875000\n",
      "Train Epoch: 386 [17036/17352 (98%)] Loss: -126946.171875\n",
      "    epoch          : 386\n",
      "    loss           : -125648.13405122692\n",
      "    val_loss       : -67273.86440429688\n",
      "Train Epoch: 387 [128/17352 (1%)] Loss: -134437.218750\n",
      "Train Epoch: 387 [1536/17352 (9%)] Loss: -145556.765625\n",
      "Train Epoch: 387 [2944/17352 (17%)] Loss: -94932.140625\n",
      "Train Epoch: 387 [4352/17352 (25%)] Loss: -115860.390625\n",
      "Train Epoch: 387 [5760/17352 (33%)] Loss: -124689.781250\n",
      "Train Epoch: 387 [7168/17352 (41%)] Loss: -152600.359375\n",
      "Train Epoch: 387 [8576/17352 (49%)] Loss: -160427.265625\n",
      "Train Epoch: 387 [9984/17352 (58%)] Loss: -178165.640625\n",
      "Train Epoch: 387 [11392/17352 (66%)] Loss: -138796.250000\n",
      "Train Epoch: 387 [12800/17352 (74%)] Loss: -137158.640625\n",
      "Train Epoch: 387 [14208/17352 (82%)] Loss: -143558.781250\n",
      "Train Epoch: 387 [15447/17352 (89%)] Loss: -109118.757812\n",
      "Train Epoch: 387 [16185/17352 (93%)] Loss: -76596.765625\n",
      "Train Epoch: 387 [16945/17352 (98%)] Loss: -28475.691406\n",
      "    epoch          : 387\n",
      "    loss           : -122435.6140808515\n",
      "    val_loss       : -55214.49818522135\n",
      "Train Epoch: 388 [128/17352 (1%)] Loss: -104865.523438\n",
      "Train Epoch: 388 [1536/17352 (9%)] Loss: -96772.617188\n",
      "Train Epoch: 388 [2944/17352 (17%)] Loss: -143552.343750\n",
      "Train Epoch: 388 [4352/17352 (25%)] Loss: -90080.703125\n",
      "Train Epoch: 388 [5760/17352 (33%)] Loss: -129468.125000\n",
      "Train Epoch: 388 [7168/17352 (41%)] Loss: -130756.937500\n",
      "Train Epoch: 388 [8576/17352 (49%)] Loss: -137364.250000\n",
      "Train Epoch: 388 [9984/17352 (58%)] Loss: -150037.375000\n",
      "Train Epoch: 388 [11392/17352 (66%)] Loss: -165035.218750\n",
      "Train Epoch: 388 [12800/17352 (74%)] Loss: -155649.656250\n",
      "Train Epoch: 388 [14208/17352 (82%)] Loss: -177397.906250\n",
      "Train Epoch: 388 [15429/17352 (89%)] Loss: -63109.164062\n",
      "Train Epoch: 388 [16243/17352 (94%)] Loss: -97146.390625\n",
      "Train Epoch: 388 [16966/17352 (98%)] Loss: -46152.359375\n",
      "    epoch          : 388\n",
      "    loss           : -132590.3381396812\n",
      "    val_loss       : -70312.3167602539\n",
      "Train Epoch: 389 [128/17352 (1%)] Loss: -172887.843750\n",
      "Train Epoch: 389 [1536/17352 (9%)] Loss: -128972.781250\n",
      "Train Epoch: 389 [2944/17352 (17%)] Loss: -165464.406250\n",
      "Train Epoch: 389 [4352/17352 (25%)] Loss: -137872.718750\n",
      "Train Epoch: 389 [5760/17352 (33%)] Loss: -142904.250000\n",
      "Train Epoch: 389 [7168/17352 (41%)] Loss: -135764.218750\n",
      "Train Epoch: 389 [8576/17352 (49%)] Loss: -152817.656250\n",
      "Train Epoch: 389 [9984/17352 (58%)] Loss: -139310.812500\n",
      "Train Epoch: 389 [11392/17352 (66%)] Loss: -152413.906250\n",
      "Train Epoch: 389 [12800/17352 (74%)] Loss: -176091.687500\n",
      "Train Epoch: 389 [14208/17352 (82%)] Loss: -149909.421875\n",
      "Train Epoch: 389 [15448/17352 (89%)] Loss: -4062.828857\n",
      "Train Epoch: 389 [16159/17352 (93%)] Loss: -106953.445312\n",
      "Train Epoch: 389 [16966/17352 (98%)] Loss: -105757.812500\n",
      "    epoch          : 389\n",
      "    loss           : -129370.73804366348\n",
      "    val_loss       : -62331.87284749349\n",
      "Train Epoch: 390 [128/17352 (1%)] Loss: -122855.531250\n",
      "Train Epoch: 390 [1536/17352 (9%)] Loss: -124982.625000\n",
      "Train Epoch: 390 [2944/17352 (17%)] Loss: -95254.500000\n",
      "Train Epoch: 390 [4352/17352 (25%)] Loss: -148348.375000\n",
      "Train Epoch: 390 [5760/17352 (33%)] Loss: -136382.312500\n",
      "Train Epoch: 390 [7168/17352 (41%)] Loss: -136320.500000\n",
      "Train Epoch: 390 [8576/17352 (49%)] Loss: -153086.906250\n",
      "Train Epoch: 390 [9984/17352 (58%)] Loss: -108082.445312\n",
      "Train Epoch: 390 [11392/17352 (66%)] Loss: -169599.656250\n",
      "Train Epoch: 390 [12800/17352 (74%)] Loss: -146694.453125\n",
      "Train Epoch: 390 [14208/17352 (82%)] Loss: -168662.703125\n",
      "Train Epoch: 390 [15523/17352 (89%)] Loss: -119409.859375\n",
      "Train Epoch: 390 [16141/17352 (93%)] Loss: -4076.792969\n",
      "Train Epoch: 390 [17056/17352 (98%)] Loss: -88988.359375\n",
      "    epoch          : 390\n",
      "    loss           : -122810.92298264471\n",
      "    val_loss       : -69533.91268717448\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch390.pth ...\n",
      "Train Epoch: 391 [128/17352 (1%)] Loss: -99062.296875\n",
      "Train Epoch: 391 [1536/17352 (9%)] Loss: -140154.031250\n",
      "Train Epoch: 391 [2944/17352 (17%)] Loss: -148914.968750\n",
      "Train Epoch: 391 [4352/17352 (25%)] Loss: -135680.156250\n",
      "Train Epoch: 391 [5760/17352 (33%)] Loss: -84707.890625\n",
      "Train Epoch: 391 [7168/17352 (41%)] Loss: -127608.960938\n",
      "Train Epoch: 391 [8576/17352 (49%)] Loss: -138251.281250\n",
      "Train Epoch: 391 [9984/17352 (58%)] Loss: -148507.453125\n",
      "Train Epoch: 391 [11392/17352 (66%)] Loss: -82855.296875\n",
      "Train Epoch: 391 [12800/17352 (74%)] Loss: -111279.812500\n",
      "Train Epoch: 391 [14208/17352 (82%)] Loss: -127249.296875\n",
      "Train Epoch: 391 [15368/17352 (89%)] Loss: -3583.876953\n",
      "Train Epoch: 391 [16231/17352 (94%)] Loss: -14519.173828\n",
      "Train Epoch: 391 [16998/17352 (98%)] Loss: -38748.484375\n",
      "    epoch          : 391\n",
      "    loss           : -125349.6424535969\n",
      "    val_loss       : -65002.62180582682\n",
      "Train Epoch: 392 [128/17352 (1%)] Loss: -134308.828125\n",
      "Train Epoch: 392 [1536/17352 (9%)] Loss: -127731.859375\n",
      "Train Epoch: 392 [2944/17352 (17%)] Loss: -139214.765625\n",
      "Train Epoch: 392 [4352/17352 (25%)] Loss: -170188.562500\n",
      "Train Epoch: 392 [5760/17352 (33%)] Loss: -189088.296875\n",
      "Train Epoch: 392 [7168/17352 (41%)] Loss: -182733.031250\n",
      "Train Epoch: 392 [8576/17352 (49%)] Loss: -131539.531250\n",
      "Train Epoch: 392 [9984/17352 (58%)] Loss: -137945.578125\n",
      "Train Epoch: 392 [11392/17352 (66%)] Loss: -167778.875000\n",
      "Train Epoch: 392 [12800/17352 (74%)] Loss: -154433.609375\n",
      "Train Epoch: 392 [14208/17352 (82%)] Loss: -135221.453125\n",
      "Train Epoch: 392 [15537/17352 (90%)] Loss: -143540.437500\n",
      "Train Epoch: 392 [16432/17352 (95%)] Loss: -141556.937500\n",
      "Train Epoch: 392 [17133/17352 (99%)] Loss: -92197.789062\n",
      "    epoch          : 392\n",
      "    loss           : -134917.0399325582\n",
      "    val_loss       : -52856.375244140625\n",
      "Train Epoch: 393 [128/17352 (1%)] Loss: -115883.812500\n",
      "Train Epoch: 393 [1536/17352 (9%)] Loss: -120651.390625\n",
      "Train Epoch: 393 [2944/17352 (17%)] Loss: -135678.406250\n",
      "Train Epoch: 393 [4352/17352 (25%)] Loss: -111215.906250\n",
      "Train Epoch: 393 [5760/17352 (33%)] Loss: -147754.468750\n",
      "Train Epoch: 393 [7168/17352 (41%)] Loss: -124807.039062\n",
      "Train Epoch: 393 [8576/17352 (49%)] Loss: -130471.437500\n",
      "Train Epoch: 393 [9984/17352 (58%)] Loss: -157058.218750\n",
      "Train Epoch: 393 [11392/17352 (66%)] Loss: -143963.328125\n",
      "Train Epoch: 393 [12800/17352 (74%)] Loss: -111068.226562\n",
      "Train Epoch: 393 [14208/17352 (82%)] Loss: -134442.250000\n",
      "Train Epoch: 393 [15522/17352 (89%)] Loss: -96306.578125\n",
      "Train Epoch: 393 [16235/17352 (94%)] Loss: -37138.441406\n",
      "Train Epoch: 393 [16979/17352 (98%)] Loss: -95940.875000\n",
      "    epoch          : 393\n",
      "    loss           : -130044.70948084889\n",
      "    val_loss       : -68389.37309163412\n",
      "Train Epoch: 394 [128/17352 (1%)] Loss: -145420.250000\n",
      "Train Epoch: 394 [1536/17352 (9%)] Loss: -134711.234375\n",
      "Train Epoch: 394 [2944/17352 (17%)] Loss: -141191.593750\n",
      "Train Epoch: 394 [4352/17352 (25%)] Loss: -167613.703125\n",
      "Train Epoch: 394 [5760/17352 (33%)] Loss: -126784.421875\n",
      "Train Epoch: 394 [7168/17352 (41%)] Loss: -142344.484375\n",
      "Train Epoch: 394 [8576/17352 (49%)] Loss: -137341.187500\n",
      "Train Epoch: 394 [9984/17352 (58%)] Loss: -129929.179688\n",
      "Train Epoch: 394 [11392/17352 (66%)] Loss: -142183.015625\n",
      "Train Epoch: 394 [12800/17352 (74%)] Loss: -137961.984375\n",
      "Train Epoch: 394 [14208/17352 (82%)] Loss: -138181.515625\n",
      "Train Epoch: 394 [15480/17352 (89%)] Loss: -46342.175781\n",
      "Train Epoch: 394 [16127/17352 (93%)] Loss: -88048.468750\n",
      "Train Epoch: 394 [16961/17352 (98%)] Loss: -1673.960693\n",
      "    epoch          : 394\n",
      "    loss           : -132013.77582483483\n",
      "    val_loss       : -47298.00357666016\n",
      "Train Epoch: 395 [128/17352 (1%)] Loss: -85775.601562\n",
      "Train Epoch: 395 [1536/17352 (9%)] Loss: -147449.843750\n",
      "Train Epoch: 395 [2944/17352 (17%)] Loss: -174341.218750\n",
      "Train Epoch: 395 [4352/17352 (25%)] Loss: -133841.390625\n",
      "Train Epoch: 395 [5760/17352 (33%)] Loss: -148247.578125\n",
      "Train Epoch: 395 [7168/17352 (41%)] Loss: -154060.000000\n",
      "Train Epoch: 395 [8576/17352 (49%)] Loss: -138367.156250\n",
      "Train Epoch: 395 [9984/17352 (58%)] Loss: -164540.218750\n",
      "Train Epoch: 395 [11392/17352 (66%)] Loss: -124782.171875\n",
      "Train Epoch: 395 [12800/17352 (74%)] Loss: -145810.265625\n",
      "Train Epoch: 395 [14208/17352 (82%)] Loss: -156185.828125\n",
      "Train Epoch: 395 [15547/17352 (90%)] Loss: -116183.562500\n",
      "Train Epoch: 395 [16227/17352 (94%)] Loss: -43182.906250\n",
      "Train Epoch: 395 [17073/17352 (98%)] Loss: -51974.453125\n",
      "    epoch          : 395\n",
      "    loss           : -124714.26673264471\n",
      "    val_loss       : -64075.19790039062\n",
      "Train Epoch: 396 [128/17352 (1%)] Loss: -125745.296875\n",
      "Train Epoch: 396 [1536/17352 (9%)] Loss: -158067.968750\n",
      "Train Epoch: 396 [2944/17352 (17%)] Loss: -147178.703125\n",
      "Train Epoch: 396 [4352/17352 (25%)] Loss: -137829.468750\n",
      "Train Epoch: 396 [5760/17352 (33%)] Loss: -123643.859375\n",
      "Train Epoch: 396 [7168/17352 (41%)] Loss: -145320.343750\n",
      "Train Epoch: 396 [8576/17352 (49%)] Loss: -100962.953125\n",
      "Train Epoch: 396 [9984/17352 (58%)] Loss: -117329.453125\n",
      "Train Epoch: 396 [11392/17352 (66%)] Loss: -145416.750000\n",
      "Train Epoch: 396 [12800/17352 (74%)] Loss: -132470.140625\n",
      "Train Epoch: 396 [14208/17352 (82%)] Loss: -122667.828125\n",
      "Train Epoch: 396 [15510/17352 (89%)] Loss: -96042.125000\n",
      "Train Epoch: 396 [16274/17352 (94%)] Loss: -46845.507812\n",
      "Train Epoch: 396 [16988/17352 (98%)] Loss: -102317.851562\n",
      "    epoch          : 396\n",
      "    loss           : -124867.55827259857\n",
      "    val_loss       : -71578.95176595052\n",
      "Train Epoch: 397 [128/17352 (1%)] Loss: -152728.718750\n",
      "Train Epoch: 397 [1536/17352 (9%)] Loss: -142470.484375\n",
      "Train Epoch: 397 [2944/17352 (17%)] Loss: -164586.187500\n",
      "Train Epoch: 397 [4352/17352 (25%)] Loss: -152922.093750\n",
      "Train Epoch: 397 [5760/17352 (33%)] Loss: -136878.343750\n",
      "Train Epoch: 397 [7168/17352 (41%)] Loss: -141360.312500\n",
      "Train Epoch: 397 [8576/17352 (49%)] Loss: -146361.125000\n",
      "Train Epoch: 397 [9984/17352 (58%)] Loss: -140579.640625\n",
      "Train Epoch: 397 [11392/17352 (66%)] Loss: -144488.468750\n",
      "Train Epoch: 397 [12800/17352 (74%)] Loss: -126401.148438\n",
      "Train Epoch: 397 [14208/17352 (82%)] Loss: -161394.796875\n",
      "Train Epoch: 397 [15525/17352 (89%)] Loss: -92793.828125\n",
      "Train Epoch: 397 [16319/17352 (94%)] Loss: -15896.449219\n",
      "Train Epoch: 397 [16985/17352 (98%)] Loss: -49061.757812\n",
      "    epoch          : 397\n",
      "    loss           : -129875.73953963927\n",
      "    val_loss       : -72914.2713256836\n",
      "Train Epoch: 398 [128/17352 (1%)] Loss: -167202.312500\n",
      "Train Epoch: 398 [1536/17352 (9%)] Loss: -168226.406250\n",
      "Train Epoch: 398 [2944/17352 (17%)] Loss: -181429.640625\n",
      "Train Epoch: 398 [4352/17352 (25%)] Loss: -133472.656250\n",
      "Train Epoch: 398 [5760/17352 (33%)] Loss: -174862.296875\n",
      "Train Epoch: 398 [7168/17352 (41%)] Loss: -109860.031250\n",
      "Train Epoch: 398 [8576/17352 (49%)] Loss: -165467.453125\n",
      "Train Epoch: 398 [9984/17352 (58%)] Loss: -126214.187500\n",
      "Train Epoch: 398 [11392/17352 (66%)] Loss: -147223.718750\n",
      "Train Epoch: 398 [12800/17352 (74%)] Loss: -159115.000000\n",
      "Train Epoch: 398 [14208/17352 (82%)] Loss: -188222.531250\n",
      "Train Epoch: 398 [15474/17352 (89%)] Loss: -105619.640625\n",
      "Train Epoch: 398 [16231/17352 (94%)] Loss: -3181.801270\n",
      "Train Epoch: 398 [16955/17352 (98%)] Loss: -59015.390625\n",
      "    epoch          : 398\n",
      "    loss           : -138370.63169502412\n",
      "    val_loss       : -66201.70215657553\n",
      "Train Epoch: 399 [128/17352 (1%)] Loss: -166595.796875\n",
      "Train Epoch: 399 [1536/17352 (9%)] Loss: -156075.093750\n",
      "Train Epoch: 399 [2944/17352 (17%)] Loss: -150677.640625\n",
      "Train Epoch: 399 [4352/17352 (25%)] Loss: -87579.593750\n",
      "Train Epoch: 399 [5760/17352 (33%)] Loss: -123553.640625\n",
      "Train Epoch: 399 [7168/17352 (41%)] Loss: -88387.914062\n",
      "Train Epoch: 399 [8576/17352 (49%)] Loss: -130532.281250\n",
      "Train Epoch: 399 [9984/17352 (58%)] Loss: -134551.750000\n",
      "Train Epoch: 399 [11392/17352 (66%)] Loss: -158207.937500\n",
      "Train Epoch: 399 [12800/17352 (74%)] Loss: -98702.078125\n",
      "Train Epoch: 399 [14208/17352 (82%)] Loss: -121194.117188\n",
      "Train Epoch: 399 [15532/17352 (90%)] Loss: -108864.992188\n",
      "Train Epoch: 399 [16364/17352 (94%)] Loss: -107489.734375\n",
      "Train Epoch: 399 [17044/17352 (98%)] Loss: -57739.476562\n",
      "    epoch          : 399\n",
      "    loss           : -127342.95109977978\n",
      "    val_loss       : -74065.66858723959\n",
      "Train Epoch: 400 [128/17352 (1%)] Loss: -154774.000000\n",
      "Train Epoch: 400 [1536/17352 (9%)] Loss: -76602.640625\n",
      "Train Epoch: 400 [2944/17352 (17%)] Loss: -166600.375000\n",
      "Train Epoch: 400 [4352/17352 (25%)] Loss: -93075.468750\n",
      "Train Epoch: 400 [5760/17352 (33%)] Loss: -111248.445312\n",
      "Train Epoch: 400 [7168/17352 (41%)] Loss: -66501.343750\n",
      "Train Epoch: 400 [8576/17352 (49%)] Loss: -136212.000000\n",
      "Train Epoch: 400 [9984/17352 (58%)] Loss: -120685.671875\n",
      "Train Epoch: 400 [11392/17352 (66%)] Loss: -173841.671875\n",
      "Train Epoch: 400 [12800/17352 (74%)] Loss: -147552.515625\n",
      "Train Epoch: 400 [14208/17352 (82%)] Loss: -138563.109375\n",
      "Train Epoch: 400 [15546/17352 (90%)] Loss: -117320.656250\n",
      "Train Epoch: 400 [16284/17352 (94%)] Loss: -145532.187500\n",
      "Train Epoch: 400 [17032/17352 (98%)] Loss: -14881.458008\n",
      "    epoch          : 400\n",
      "    loss           : -123546.41856189702\n",
      "    val_loss       : -59764.529260253905\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [128/17352 (1%)] Loss: -156605.828125\n",
      "Train Epoch: 401 [1536/17352 (9%)] Loss: -111770.906250\n",
      "Train Epoch: 401 [2944/17352 (17%)] Loss: -120291.125000\n",
      "Train Epoch: 401 [4352/17352 (25%)] Loss: -124432.265625\n",
      "Train Epoch: 401 [5760/17352 (33%)] Loss: -172974.765625\n",
      "Train Epoch: 401 [7168/17352 (41%)] Loss: -147852.078125\n",
      "Train Epoch: 401 [8576/17352 (49%)] Loss: -130574.445312\n",
      "Train Epoch: 401 [9984/17352 (58%)] Loss: -149799.484375\n",
      "Train Epoch: 401 [11392/17352 (66%)] Loss: -149388.015625\n",
      "Train Epoch: 401 [12800/17352 (74%)] Loss: -151513.828125\n",
      "Train Epoch: 401 [14208/17352 (82%)] Loss: -190800.625000\n",
      "Train Epoch: 401 [15399/17352 (89%)] Loss: -43595.937500\n",
      "Train Epoch: 401 [16310/17352 (94%)] Loss: -117286.171875\n",
      "Train Epoch: 401 [16947/17352 (98%)] Loss: -90256.750000\n",
      "    epoch          : 401\n",
      "    loss           : -133520.61857041737\n",
      "    val_loss       : -69763.66608072916\n",
      "Train Epoch: 402 [128/17352 (1%)] Loss: -137034.562500\n",
      "Train Epoch: 402 [1536/17352 (9%)] Loss: -126560.078125\n",
      "Train Epoch: 402 [2944/17352 (17%)] Loss: -167232.625000\n",
      "Train Epoch: 402 [4352/17352 (25%)] Loss: -119444.328125\n",
      "Train Epoch: 402 [5760/17352 (33%)] Loss: -131022.093750\n",
      "Train Epoch: 402 [7168/17352 (41%)] Loss: -157757.875000\n",
      "Train Epoch: 402 [8576/17352 (49%)] Loss: -167734.031250\n",
      "Train Epoch: 402 [9984/17352 (58%)] Loss: -126093.007812\n",
      "Train Epoch: 402 [11392/17352 (66%)] Loss: -141671.812500\n",
      "Train Epoch: 402 [12800/17352 (74%)] Loss: -146730.718750\n",
      "Train Epoch: 402 [14208/17352 (82%)] Loss: -154934.500000\n",
      "Train Epoch: 402 [15436/17352 (89%)] Loss: -81416.843750\n",
      "Train Epoch: 402 [16320/17352 (94%)] Loss: -19500.210938\n",
      "Train Epoch: 402 [17070/17352 (98%)] Loss: -85779.468750\n",
      "    epoch          : 402\n",
      "    loss           : -132666.53441563548\n",
      "    val_loss       : -66601.29700927735\n",
      "Train Epoch: 403 [128/17352 (1%)] Loss: -155646.031250\n",
      "Train Epoch: 403 [1536/17352 (9%)] Loss: -89367.937500\n",
      "Train Epoch: 403 [2944/17352 (17%)] Loss: -132446.468750\n",
      "Train Epoch: 403 [4352/17352 (25%)] Loss: -109268.304688\n",
      "Train Epoch: 403 [5760/17352 (33%)] Loss: -115190.148438\n",
      "Train Epoch: 403 [7168/17352 (41%)] Loss: -134786.671875\n",
      "Train Epoch: 403 [8576/17352 (49%)] Loss: -115374.476562\n",
      "Train Epoch: 403 [9984/17352 (58%)] Loss: -84403.796875\n",
      "Train Epoch: 403 [11392/17352 (66%)] Loss: -126093.031250\n",
      "Train Epoch: 403 [12800/17352 (74%)] Loss: -161422.578125\n",
      "Train Epoch: 403 [14208/17352 (82%)] Loss: -143718.484375\n",
      "Train Epoch: 403 [15565/17352 (90%)] Loss: -109035.945312\n",
      "Train Epoch: 403 [16368/17352 (94%)] Loss: -114421.570312\n",
      "Train Epoch: 403 [17019/17352 (98%)] Loss: -14456.982422\n",
      "    epoch          : 403\n",
      "    loss           : -120816.96894006922\n",
      "    val_loss       : -33253.99033610026\n",
      "Train Epoch: 404 [128/17352 (1%)] Loss: -69721.234375\n",
      "Train Epoch: 404 [1536/17352 (9%)] Loss: -122226.234375\n",
      "Train Epoch: 404 [2944/17352 (17%)] Loss: -117377.882812\n",
      "Train Epoch: 404 [4352/17352 (25%)] Loss: -128700.390625\n",
      "Train Epoch: 404 [5760/17352 (33%)] Loss: -156203.734375\n",
      "Train Epoch: 404 [7168/17352 (41%)] Loss: -109268.875000\n",
      "Train Epoch: 404 [8576/17352 (49%)] Loss: -161650.156250\n",
      "Train Epoch: 404 [9984/17352 (58%)] Loss: -146587.203125\n",
      "Train Epoch: 404 [11392/17352 (66%)] Loss: -139159.812500\n",
      "Train Epoch: 404 [12800/17352 (74%)] Loss: -143205.578125\n",
      "Train Epoch: 404 [14208/17352 (82%)] Loss: -124180.742188\n",
      "Train Epoch: 404 [15463/17352 (89%)] Loss: -16450.859375\n",
      "Train Epoch: 404 [16267/17352 (94%)] Loss: -112108.445312\n",
      "Train Epoch: 404 [17027/17352 (98%)] Loss: -47925.894531\n",
      "    epoch          : 404\n",
      "    loss           : -123683.91024309197\n",
      "    val_loss       : -42148.35677490234\n",
      "Train Epoch: 405 [128/17352 (1%)] Loss: -77219.281250\n",
      "Train Epoch: 405 [1536/17352 (9%)] Loss: -127733.656250\n",
      "Train Epoch: 405 [2944/17352 (17%)] Loss: -152998.031250\n",
      "Train Epoch: 405 [4352/17352 (25%)] Loss: -144537.406250\n",
      "Train Epoch: 405 [5760/17352 (33%)] Loss: -162272.765625\n",
      "Train Epoch: 405 [7168/17352 (41%)] Loss: -159878.312500\n",
      "Train Epoch: 405 [8576/17352 (49%)] Loss: -176544.468750\n",
      "Train Epoch: 405 [9984/17352 (58%)] Loss: -148867.625000\n",
      "Train Epoch: 405 [11392/17352 (66%)] Loss: -76711.156250\n",
      "Train Epoch: 405 [12800/17352 (74%)] Loss: -143115.125000\n",
      "Train Epoch: 405 [14208/17352 (82%)] Loss: -159708.328125\n",
      "Train Epoch: 405 [15584/17352 (90%)] Loss: -67522.976562\n",
      "Train Epoch: 405 [16440/17352 (95%)] Loss: -64228.121094\n",
      "Train Epoch: 405 [17107/17352 (99%)] Loss: -69749.242188\n",
      "    epoch          : 405\n",
      "    loss           : -125303.14581257864\n",
      "    val_loss       : -60541.44055175781\n",
      "Train Epoch: 406 [128/17352 (1%)] Loss: -101023.664062\n",
      "Train Epoch: 406 [1536/17352 (9%)] Loss: -155519.937500\n",
      "Train Epoch: 406 [2944/17352 (17%)] Loss: -159613.921875\n",
      "Train Epoch: 406 [4352/17352 (25%)] Loss: -81786.046875\n",
      "Train Epoch: 406 [5760/17352 (33%)] Loss: -145135.406250\n",
      "Train Epoch: 406 [7168/17352 (41%)] Loss: -189400.578125\n",
      "Train Epoch: 406 [8576/17352 (49%)] Loss: -165440.390625\n",
      "Train Epoch: 406 [9984/17352 (58%)] Loss: -159368.515625\n",
      "Train Epoch: 406 [11392/17352 (66%)] Loss: -161275.359375\n",
      "Train Epoch: 406 [12800/17352 (74%)] Loss: -158097.781250\n",
      "Train Epoch: 406 [14208/17352 (82%)] Loss: -170968.593750\n",
      "Train Epoch: 406 [15527/17352 (89%)] Loss: -91919.218750\n",
      "Train Epoch: 406 [16132/17352 (93%)] Loss: -97843.984375\n",
      "Train Epoch: 406 [17002/17352 (98%)] Loss: -94067.750000\n",
      "    epoch          : 406\n",
      "    loss           : -131248.22159776112\n",
      "    val_loss       : -75505.18009033203\n",
      "Train Epoch: 407 [128/17352 (1%)] Loss: -136028.515625\n",
      "Train Epoch: 407 [1536/17352 (9%)] Loss: -128203.718750\n",
      "Train Epoch: 407 [2944/17352 (17%)] Loss: -158648.718750\n",
      "Train Epoch: 407 [4352/17352 (25%)] Loss: -168314.421875\n",
      "Train Epoch: 407 [5760/17352 (33%)] Loss: -146658.781250\n",
      "Train Epoch: 407 [7168/17352 (41%)] Loss: -128020.109375\n",
      "Train Epoch: 407 [8576/17352 (49%)] Loss: -155534.453125\n",
      "Train Epoch: 407 [9984/17352 (58%)] Loss: -141447.296875\n",
      "Train Epoch: 407 [11392/17352 (66%)] Loss: -130661.156250\n",
      "Train Epoch: 407 [12800/17352 (74%)] Loss: -150538.796875\n",
      "Train Epoch: 407 [14208/17352 (82%)] Loss: -147403.843750\n",
      "Train Epoch: 407 [15491/17352 (89%)] Loss: -54257.101562\n",
      "Train Epoch: 407 [16249/17352 (94%)] Loss: -89373.429688\n",
      "Train Epoch: 407 [17014/17352 (98%)] Loss: -4372.646973\n",
      "    epoch          : 407\n",
      "    loss           : -137428.68871578755\n",
      "    val_loss       : -75562.42606201171\n",
      "Train Epoch: 408 [128/17352 (1%)] Loss: -150590.687500\n",
      "Train Epoch: 408 [1536/17352 (9%)] Loss: -180070.312500\n",
      "Train Epoch: 408 [2944/17352 (17%)] Loss: -130221.046875\n",
      "Train Epoch: 408 [4352/17352 (25%)] Loss: -134623.000000\n",
      "Train Epoch: 408 [5760/17352 (33%)] Loss: -135266.031250\n",
      "Train Epoch: 408 [7168/17352 (41%)] Loss: -129421.515625\n",
      "Train Epoch: 408 [8576/17352 (49%)] Loss: -153873.468750\n",
      "Train Epoch: 408 [9984/17352 (58%)] Loss: -125775.562500\n",
      "Train Epoch: 408 [11392/17352 (66%)] Loss: -154321.015625\n",
      "Train Epoch: 408 [12800/17352 (74%)] Loss: -177282.359375\n",
      "Train Epoch: 408 [14208/17352 (82%)] Loss: -168503.640625\n",
      "Train Epoch: 408 [15478/17352 (89%)] Loss: -14553.404297\n",
      "Train Epoch: 408 [16156/17352 (93%)] Loss: -134578.562500\n",
      "Train Epoch: 408 [17010/17352 (98%)] Loss: -96195.945312\n",
      "    epoch          : 408\n",
      "    loss           : -131284.2893279415\n",
      "    val_loss       : -67913.85440673828\n",
      "Train Epoch: 409 [128/17352 (1%)] Loss: -120229.843750\n",
      "Train Epoch: 409 [1536/17352 (9%)] Loss: -141483.671875\n",
      "Train Epoch: 409 [2944/17352 (17%)] Loss: -141001.046875\n",
      "Train Epoch: 409 [4352/17352 (25%)] Loss: -149217.843750\n",
      "Train Epoch: 409 [5760/17352 (33%)] Loss: -129957.414062\n",
      "Train Epoch: 409 [7168/17352 (41%)] Loss: -154478.078125\n",
      "Train Epoch: 409 [8576/17352 (49%)] Loss: -128587.937500\n",
      "Train Epoch: 409 [9984/17352 (58%)] Loss: -134330.281250\n",
      "Train Epoch: 409 [11392/17352 (66%)] Loss: -94093.773438\n",
      "Train Epoch: 409 [12800/17352 (74%)] Loss: -144579.906250\n",
      "Train Epoch: 409 [14208/17352 (82%)] Loss: -96917.007812\n",
      "Train Epoch: 409 [15488/17352 (89%)] Loss: -43822.093750\n",
      "Train Epoch: 409 [16256/17352 (94%)] Loss: -47928.937500\n",
      "Train Epoch: 409 [16982/17352 (98%)] Loss: -56206.289062\n",
      "    epoch          : 409\n",
      "    loss           : -119498.18517820627\n",
      "    val_loss       : -49839.41068522135\n",
      "Train Epoch: 410 [128/17352 (1%)] Loss: -99402.437500\n",
      "Train Epoch: 410 [1536/17352 (9%)] Loss: -139443.015625\n",
      "Train Epoch: 410 [2944/17352 (17%)] Loss: -125035.757812\n",
      "Train Epoch: 410 [4352/17352 (25%)] Loss: -169022.937500\n",
      "Train Epoch: 410 [5760/17352 (33%)] Loss: -160394.531250\n",
      "Train Epoch: 410 [7168/17352 (41%)] Loss: -117375.039062\n",
      "Train Epoch: 410 [8576/17352 (49%)] Loss: -117319.312500\n",
      "Train Epoch: 410 [9984/17352 (58%)] Loss: -120431.007812\n",
      "Train Epoch: 410 [11392/17352 (66%)] Loss: -115544.812500\n",
      "Train Epoch: 410 [12800/17352 (74%)] Loss: -147740.500000\n",
      "Train Epoch: 410 [14208/17352 (82%)] Loss: -169471.656250\n",
      "Train Epoch: 410 [15479/17352 (89%)] Loss: -41191.320312\n",
      "Train Epoch: 410 [16100/17352 (93%)] Loss: -69881.148438\n",
      "Train Epoch: 410 [17022/17352 (98%)] Loss: -150200.578125\n",
      "    epoch          : 410\n",
      "    loss           : -130716.03546757025\n",
      "    val_loss       : -75390.07559407552\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch410.pth ...\n",
      "Train Epoch: 411 [128/17352 (1%)] Loss: -155494.984375\n",
      "Train Epoch: 411 [1536/17352 (9%)] Loss: -160810.750000\n",
      "Train Epoch: 411 [2944/17352 (17%)] Loss: -86124.601562\n",
      "Train Epoch: 411 [4352/17352 (25%)] Loss: -54409.531250\n",
      "Train Epoch: 411 [5760/17352 (33%)] Loss: -113677.835938\n",
      "Train Epoch: 411 [7168/17352 (41%)] Loss: -128227.726562\n",
      "Train Epoch: 411 [8576/17352 (49%)] Loss: -142973.906250\n",
      "Train Epoch: 411 [9984/17352 (58%)] Loss: -174180.437500\n",
      "Train Epoch: 411 [11392/17352 (66%)] Loss: -179383.156250\n",
      "Train Epoch: 411 [12800/17352 (74%)] Loss: -154606.406250\n",
      "Train Epoch: 411 [14208/17352 (82%)] Loss: -157814.265625\n",
      "Train Epoch: 411 [15523/17352 (89%)] Loss: -85294.156250\n",
      "Train Epoch: 411 [16210/17352 (93%)] Loss: -116355.109375\n",
      "Train Epoch: 411 [17000/17352 (98%)] Loss: -52743.343750\n",
      "    epoch          : 411\n",
      "    loss           : -117142.1284802328\n",
      "    val_loss       : -75702.2185546875\n",
      "Train Epoch: 412 [128/17352 (1%)] Loss: -164366.281250\n",
      "Train Epoch: 412 [1536/17352 (9%)] Loss: -167022.750000\n",
      "Train Epoch: 412 [2944/17352 (17%)] Loss: -145437.890625\n",
      "Train Epoch: 412 [4352/17352 (25%)] Loss: -117600.937500\n",
      "Train Epoch: 412 [5760/17352 (33%)] Loss: -168419.500000\n",
      "Train Epoch: 412 [7168/17352 (41%)] Loss: -176757.750000\n",
      "Train Epoch: 412 [8576/17352 (49%)] Loss: -142051.250000\n",
      "Train Epoch: 412 [9984/17352 (58%)] Loss: -146965.625000\n",
      "Train Epoch: 412 [11392/17352 (66%)] Loss: -176989.906250\n",
      "Train Epoch: 412 [12800/17352 (74%)] Loss: -159678.609375\n",
      "Train Epoch: 412 [14208/17352 (82%)] Loss: -138609.265625\n",
      "Train Epoch: 412 [15538/17352 (90%)] Loss: -141345.500000\n",
      "Train Epoch: 412 [16404/17352 (95%)] Loss: -16484.646484\n",
      "Train Epoch: 412 [17092/17352 (99%)] Loss: -91476.148438\n",
      "    epoch          : 412\n",
      "    loss           : -136929.406603922\n",
      "    val_loss       : -74964.9723836263\n",
      "Train Epoch: 413 [128/17352 (1%)] Loss: -155829.109375\n",
      "Train Epoch: 413 [1536/17352 (9%)] Loss: -174948.531250\n",
      "Train Epoch: 413 [2944/17352 (17%)] Loss: -145569.796875\n",
      "Train Epoch: 413 [4352/17352 (25%)] Loss: -169708.062500\n",
      "Train Epoch: 413 [5760/17352 (33%)] Loss: -136075.578125\n",
      "Train Epoch: 413 [7168/17352 (41%)] Loss: -114753.570312\n",
      "Train Epoch: 413 [8576/17352 (49%)] Loss: -152495.921875\n",
      "Train Epoch: 413 [9984/17352 (58%)] Loss: -154803.187500\n",
      "Train Epoch: 413 [11392/17352 (66%)] Loss: -126476.437500\n",
      "Train Epoch: 413 [12800/17352 (74%)] Loss: -131315.000000\n",
      "Train Epoch: 413 [14208/17352 (82%)] Loss: -154632.218750\n",
      "Train Epoch: 413 [15525/17352 (89%)] Loss: -92053.414062\n",
      "Train Epoch: 413 [16194/17352 (93%)] Loss: -90312.554688\n",
      "Train Epoch: 413 [16960/17352 (98%)] Loss: -34726.425781\n",
      "    epoch          : 413\n",
      "    loss           : -123945.37178520868\n",
      "    val_loss       : -62884.3346069336\n",
      "Train Epoch: 414 [128/17352 (1%)] Loss: -123584.929688\n",
      "Train Epoch: 414 [1536/17352 (9%)] Loss: -163728.781250\n",
      "Train Epoch: 414 [2944/17352 (17%)] Loss: -129776.007812\n",
      "Train Epoch: 414 [4352/17352 (25%)] Loss: -132928.328125\n",
      "Train Epoch: 414 [5760/17352 (33%)] Loss: -152266.468750\n",
      "Train Epoch: 414 [7168/17352 (41%)] Loss: -153086.359375\n",
      "Train Epoch: 414 [8576/17352 (49%)] Loss: -131337.062500\n",
      "Train Epoch: 414 [9984/17352 (58%)] Loss: -162899.687500\n",
      "Train Epoch: 414 [11392/17352 (66%)] Loss: -155675.781250\n",
      "Train Epoch: 414 [12800/17352 (74%)] Loss: -132323.687500\n",
      "Train Epoch: 414 [14208/17352 (82%)] Loss: -156353.937500\n",
      "Train Epoch: 414 [15521/17352 (89%)] Loss: -115289.453125\n",
      "Train Epoch: 414 [16190/17352 (93%)] Loss: -58039.316406\n",
      "Train Epoch: 414 [16922/17352 (98%)] Loss: -34894.308594\n",
      "    epoch          : 414\n",
      "    loss           : -133829.39894872063\n",
      "    val_loss       : -71147.20725911458\n",
      "Train Epoch: 415 [128/17352 (1%)] Loss: -157431.187500\n",
      "Train Epoch: 415 [1536/17352 (9%)] Loss: -126336.039062\n",
      "Train Epoch: 415 [2944/17352 (17%)] Loss: -138275.484375\n",
      "Train Epoch: 415 [4352/17352 (25%)] Loss: -129504.046875\n",
      "Train Epoch: 415 [5760/17352 (33%)] Loss: -130116.578125\n",
      "Train Epoch: 415 [7168/17352 (41%)] Loss: -136014.515625\n",
      "Train Epoch: 415 [8576/17352 (49%)] Loss: -157222.781250\n",
      "Train Epoch: 415 [9984/17352 (58%)] Loss: -136225.781250\n",
      "Train Epoch: 415 [11392/17352 (66%)] Loss: -144365.734375\n",
      "Train Epoch: 415 [12800/17352 (74%)] Loss: -156913.687500\n",
      "Train Epoch: 415 [14208/17352 (82%)] Loss: -167867.453125\n",
      "Train Epoch: 415 [15538/17352 (90%)] Loss: -115251.640625\n",
      "Train Epoch: 415 [16191/17352 (93%)] Loss: -3720.735840\n",
      "Train Epoch: 415 [17005/17352 (98%)] Loss: -61071.953125\n",
      "    epoch          : 415\n",
      "    loss           : -132448.07290792785\n",
      "    val_loss       : -69199.45127766927\n",
      "Train Epoch: 416 [128/17352 (1%)] Loss: -136685.953125\n",
      "Train Epoch: 416 [1536/17352 (9%)] Loss: -146388.250000\n",
      "Train Epoch: 416 [2944/17352 (17%)] Loss: -124297.671875\n",
      "Train Epoch: 416 [4352/17352 (25%)] Loss: -131359.750000\n",
      "Train Epoch: 416 [5760/17352 (33%)] Loss: -133257.546875\n",
      "Train Epoch: 416 [7168/17352 (41%)] Loss: -151401.718750\n",
      "Train Epoch: 416 [8576/17352 (49%)] Loss: -125555.750000\n",
      "Train Epoch: 416 [9984/17352 (58%)] Loss: -163828.906250\n",
      "Train Epoch: 416 [11392/17352 (66%)] Loss: -130376.820312\n",
      "Train Epoch: 416 [12800/17352 (74%)] Loss: -156498.968750\n",
      "Train Epoch: 416 [14208/17352 (82%)] Loss: -133024.703125\n",
      "Train Epoch: 416 [15540/17352 (90%)] Loss: -68183.484375\n",
      "Train Epoch: 416 [16156/17352 (93%)] Loss: -30234.437500\n",
      "Train Epoch: 416 [16958/17352 (98%)] Loss: -36967.492188\n",
      "    epoch          : 416\n",
      "    loss           : -128127.08554589188\n",
      "    val_loss       : -69807.65434570312\n",
      "Train Epoch: 417 [128/17352 (1%)] Loss: -136377.593750\n",
      "Train Epoch: 417 [1536/17352 (9%)] Loss: -168403.718750\n",
      "Train Epoch: 417 [2944/17352 (17%)] Loss: -166111.406250\n",
      "Train Epoch: 417 [4352/17352 (25%)] Loss: -154532.234375\n",
      "Train Epoch: 417 [5760/17352 (33%)] Loss: -120838.054688\n",
      "Train Epoch: 417 [7168/17352 (41%)] Loss: -117433.046875\n",
      "Train Epoch: 417 [8576/17352 (49%)] Loss: -146964.562500\n",
      "Train Epoch: 417 [9984/17352 (58%)] Loss: -154352.421875\n",
      "Train Epoch: 417 [11392/17352 (66%)] Loss: -122056.601562\n",
      "Train Epoch: 417 [12800/17352 (74%)] Loss: -118735.664062\n",
      "Train Epoch: 417 [14208/17352 (82%)] Loss: -167782.875000\n",
      "Train Epoch: 417 [15573/17352 (90%)] Loss: -168772.796875\n",
      "Train Epoch: 417 [16365/17352 (94%)] Loss: -77089.101562\n",
      "Train Epoch: 417 [17024/17352 (98%)] Loss: -6787.528809\n",
      "    epoch          : 417\n",
      "    loss           : -133641.24821072776\n",
      "    val_loss       : -73716.57189127603\n",
      "Train Epoch: 418 [128/17352 (1%)] Loss: -140390.750000\n",
      "Train Epoch: 418 [1536/17352 (9%)] Loss: -150404.468750\n",
      "Train Epoch: 418 [2944/17352 (17%)] Loss: -127480.640625\n",
      "Train Epoch: 418 [4352/17352 (25%)] Loss: -115198.687500\n",
      "Train Epoch: 418 [5760/17352 (33%)] Loss: -120238.281250\n",
      "Train Epoch: 418 [7168/17352 (41%)] Loss: -119869.195312\n",
      "Train Epoch: 418 [8576/17352 (49%)] Loss: -119436.898438\n",
      "Train Epoch: 418 [9984/17352 (58%)] Loss: -136911.500000\n",
      "Train Epoch: 418 [11392/17352 (66%)] Loss: -142322.046875\n",
      "Train Epoch: 418 [12800/17352 (74%)] Loss: -115288.437500\n",
      "Train Epoch: 418 [14208/17352 (82%)] Loss: -129366.117188\n",
      "Train Epoch: 418 [15503/17352 (89%)] Loss: -40958.156250\n",
      "Train Epoch: 418 [16013/17352 (92%)] Loss: -13716.195312\n",
      "Train Epoch: 418 [16887/17352 (97%)] Loss: -86233.421875\n",
      "    epoch          : 418\n",
      "    loss           : -123048.06441707739\n",
      "    val_loss       : -67031.95598551432\n",
      "Train Epoch: 419 [128/17352 (1%)] Loss: -141885.250000\n",
      "Train Epoch: 419 [1536/17352 (9%)] Loss: -130246.140625\n",
      "Train Epoch: 419 [2944/17352 (17%)] Loss: -139468.546875\n",
      "Train Epoch: 419 [4352/17352 (25%)] Loss: -188651.687500\n",
      "Train Epoch: 419 [5760/17352 (33%)] Loss: -160154.796875\n",
      "Train Epoch: 419 [7168/17352 (41%)] Loss: -148494.437500\n",
      "Train Epoch: 419 [8576/17352 (49%)] Loss: -161737.468750\n",
      "Train Epoch: 419 [9984/17352 (58%)] Loss: -129691.890625\n",
      "Train Epoch: 419 [11392/17352 (66%)] Loss: -140402.171875\n",
      "Train Epoch: 419 [12800/17352 (74%)] Loss: -140449.625000\n",
      "Train Epoch: 419 [14208/17352 (82%)] Loss: -108552.609375\n",
      "Train Epoch: 419 [15468/17352 (89%)] Loss: -118489.125000\n",
      "Train Epoch: 419 [16137/17352 (93%)] Loss: -67412.875000\n",
      "Train Epoch: 419 [16951/17352 (98%)] Loss: -90856.515625\n",
      "    epoch          : 419\n",
      "    loss           : -135687.10237684826\n",
      "    val_loss       : -75197.73133951823\n",
      "Train Epoch: 420 [128/17352 (1%)] Loss: -157600.640625\n",
      "Train Epoch: 420 [1536/17352 (9%)] Loss: -152781.265625\n",
      "Train Epoch: 420 [2944/17352 (17%)] Loss: -147117.656250\n",
      "Train Epoch: 420 [4352/17352 (25%)] Loss: -162180.093750\n",
      "Train Epoch: 420 [5760/17352 (33%)] Loss: -146888.312500\n",
      "Train Epoch: 420 [7168/17352 (41%)] Loss: -160788.046875\n",
      "Train Epoch: 420 [8576/17352 (49%)] Loss: -120653.406250\n",
      "Train Epoch: 420 [9984/17352 (58%)] Loss: -176412.890625\n",
      "Train Epoch: 420 [11392/17352 (66%)] Loss: -99025.843750\n",
      "Train Epoch: 420 [12800/17352 (74%)] Loss: -117263.000000\n",
      "Train Epoch: 420 [14208/17352 (82%)] Loss: -125606.265625\n",
      "Train Epoch: 420 [15518/17352 (89%)] Loss: -109034.328125\n",
      "Train Epoch: 420 [16366/17352 (94%)] Loss: -100466.695312\n",
      "Train Epoch: 420 [17071/17352 (98%)] Loss: -44656.824219\n",
      "    epoch          : 420\n",
      "    loss           : -129686.37864736262\n",
      "    val_loss       : -73628.27837727865\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch420.pth ...\n",
      "Train Epoch: 421 [128/17352 (1%)] Loss: -175671.296875\n",
      "Train Epoch: 421 [1536/17352 (9%)] Loss: -141100.750000\n",
      "Train Epoch: 421 [2944/17352 (17%)] Loss: -133829.078125\n",
      "Train Epoch: 421 [4352/17352 (25%)] Loss: -132672.828125\n",
      "Train Epoch: 421 [5760/17352 (33%)] Loss: -152076.562500\n",
      "Train Epoch: 421 [7168/17352 (41%)] Loss: -181141.156250\n",
      "Train Epoch: 421 [8576/17352 (49%)] Loss: -166474.640625\n",
      "Train Epoch: 421 [9984/17352 (58%)] Loss: -134031.890625\n",
      "Train Epoch: 421 [11392/17352 (66%)] Loss: -140908.875000\n",
      "Train Epoch: 421 [12800/17352 (74%)] Loss: -154856.203125\n",
      "Train Epoch: 421 [14208/17352 (82%)] Loss: -153805.687500\n",
      "Train Epoch: 421 [15529/17352 (89%)] Loss: -118462.046875\n",
      "Train Epoch: 421 [16336/17352 (94%)] Loss: -32536.902344\n",
      "Train Epoch: 421 [16976/17352 (98%)] Loss: -7311.377441\n",
      "    epoch          : 421\n",
      "    loss           : -136511.20775220217\n",
      "    val_loss       : -75009.57930501302\n",
      "Train Epoch: 422 [128/17352 (1%)] Loss: -143023.468750\n",
      "Train Epoch: 422 [1536/17352 (9%)] Loss: -146252.953125\n",
      "Train Epoch: 422 [2944/17352 (17%)] Loss: -187643.343750\n",
      "Train Epoch: 422 [4352/17352 (25%)] Loss: -133751.734375\n",
      "Train Epoch: 422 [5760/17352 (33%)] Loss: -158951.609375\n",
      "Train Epoch: 422 [7168/17352 (41%)] Loss: -175545.609375\n",
      "Train Epoch: 422 [8576/17352 (49%)] Loss: -124544.476562\n",
      "Train Epoch: 422 [9984/17352 (58%)] Loss: -162498.125000\n",
      "Train Epoch: 422 [11392/17352 (66%)] Loss: -157638.968750\n",
      "Train Epoch: 422 [12800/17352 (74%)] Loss: -160667.734375\n",
      "Train Epoch: 422 [14208/17352 (82%)] Loss: -163721.406250\n",
      "Train Epoch: 422 [15378/17352 (89%)] Loss: -3169.349854\n",
      "Train Epoch: 422 [16155/17352 (93%)] Loss: -69430.632812\n",
      "Train Epoch: 422 [17074/17352 (98%)] Loss: -85971.171875\n",
      "    epoch          : 422\n",
      "    loss           : -137496.99121257602\n",
      "    val_loss       : -72813.12250569662\n",
      "Train Epoch: 423 [128/17352 (1%)] Loss: -109859.859375\n",
      "Train Epoch: 423 [1536/17352 (9%)] Loss: -165199.687500\n",
      "Train Epoch: 423 [2944/17352 (17%)] Loss: -131661.531250\n",
      "Train Epoch: 423 [4352/17352 (25%)] Loss: -137008.687500\n",
      "Train Epoch: 423 [5760/17352 (33%)] Loss: -109359.937500\n",
      "Train Epoch: 423 [7168/17352 (41%)] Loss: -162361.406250\n",
      "Train Epoch: 423 [8576/17352 (49%)] Loss: -154161.468750\n",
      "Train Epoch: 423 [9984/17352 (58%)] Loss: -159239.015625\n",
      "Train Epoch: 423 [11392/17352 (66%)] Loss: -161021.281250\n",
      "Train Epoch: 423 [12800/17352 (74%)] Loss: -124061.851562\n",
      "Train Epoch: 423 [14208/17352 (82%)] Loss: -111382.976562\n",
      "Train Epoch: 423 [15472/17352 (89%)] Loss: -115230.484375\n",
      "Train Epoch: 423 [16208/17352 (93%)] Loss: -113199.242188\n",
      "Train Epoch: 423 [17007/17352 (98%)] Loss: -51970.042969\n",
      "    epoch          : 423\n",
      "    loss           : -129687.48904644242\n",
      "    val_loss       : -70271.46586914062\n",
      "Train Epoch: 424 [128/17352 (1%)] Loss: -148811.609375\n",
      "Train Epoch: 424 [1536/17352 (9%)] Loss: -133749.078125\n",
      "Train Epoch: 424 [2944/17352 (17%)] Loss: -129275.125000\n",
      "Train Epoch: 424 [4352/17352 (25%)] Loss: -149880.765625\n",
      "Train Epoch: 424 [5760/17352 (33%)] Loss: -183202.656250\n",
      "Train Epoch: 424 [7168/17352 (41%)] Loss: -140752.281250\n",
      "Train Epoch: 424 [8576/17352 (49%)] Loss: -145740.921875\n",
      "Train Epoch: 424 [9984/17352 (58%)] Loss: -147946.937500\n",
      "Train Epoch: 424 [11392/17352 (66%)] Loss: -134549.250000\n",
      "Train Epoch: 424 [12800/17352 (74%)] Loss: -142903.906250\n",
      "Train Epoch: 424 [14208/17352 (82%)] Loss: -143517.296875\n",
      "Train Epoch: 424 [15512/17352 (89%)] Loss: -81169.367188\n",
      "Train Epoch: 424 [16360/17352 (94%)] Loss: -126070.328125\n",
      "Train Epoch: 424 [17071/17352 (98%)] Loss: -3554.072754\n",
      "    epoch          : 424\n",
      "    loss           : -129536.22100297557\n",
      "    val_loss       : -59261.32706705729\n",
      "Train Epoch: 425 [128/17352 (1%)] Loss: -79985.609375\n",
      "Train Epoch: 425 [1536/17352 (9%)] Loss: -134843.828125\n",
      "Train Epoch: 425 [2944/17352 (17%)] Loss: -153746.953125\n",
      "Train Epoch: 425 [4352/17352 (25%)] Loss: -169808.140625\n",
      "Train Epoch: 425 [5760/17352 (33%)] Loss: -156507.750000\n",
      "Train Epoch: 425 [7168/17352 (41%)] Loss: -143656.000000\n",
      "Train Epoch: 425 [8576/17352 (49%)] Loss: -161138.015625\n",
      "Train Epoch: 425 [9984/17352 (58%)] Loss: -186794.718750\n",
      "Train Epoch: 425 [11392/17352 (66%)] Loss: -168709.734375\n",
      "Train Epoch: 425 [12800/17352 (74%)] Loss: -162492.718750\n",
      "Train Epoch: 425 [14208/17352 (82%)] Loss: -163812.718750\n",
      "Train Epoch: 425 [15498/17352 (89%)] Loss: -90181.085938\n",
      "Train Epoch: 425 [16223/17352 (93%)] Loss: -5027.767578\n",
      "Train Epoch: 425 [17092/17352 (99%)] Loss: -57016.019531\n",
      "    epoch          : 425\n",
      "    loss           : -133125.2961180002\n",
      "    val_loss       : -49853.884602864586\n",
      "Train Epoch: 426 [128/17352 (1%)] Loss: -106005.328125\n",
      "Train Epoch: 426 [1536/17352 (9%)] Loss: -149380.859375\n",
      "Train Epoch: 426 [2944/17352 (17%)] Loss: -153761.125000\n",
      "Train Epoch: 426 [4352/17352 (25%)] Loss: -102110.093750\n",
      "Train Epoch: 426 [5760/17352 (33%)] Loss: -141046.421875\n",
      "Train Epoch: 426 [7168/17352 (41%)] Loss: -139700.734375\n",
      "Train Epoch: 426 [8576/17352 (49%)] Loss: -128562.421875\n",
      "Train Epoch: 426 [9984/17352 (58%)] Loss: -155695.656250\n",
      "Train Epoch: 426 [11392/17352 (66%)] Loss: -144691.468750\n",
      "Train Epoch: 426 [12800/17352 (74%)] Loss: -175636.750000\n",
      "Train Epoch: 426 [14208/17352 (82%)] Loss: -163146.171875\n",
      "Train Epoch: 426 [15407/17352 (89%)] Loss: -14041.418945\n",
      "Train Epoch: 426 [16188/17352 (93%)] Loss: -116477.289062\n",
      "Train Epoch: 426 [16873/17352 (97%)] Loss: -52222.097656\n",
      "    epoch          : 426\n",
      "    loss           : -127833.26283622588\n",
      "    val_loss       : -68203.99180501302\n",
      "Train Epoch: 427 [128/17352 (1%)] Loss: -156960.671875\n",
      "Train Epoch: 427 [1536/17352 (9%)] Loss: -151676.953125\n",
      "Train Epoch: 427 [2944/17352 (17%)] Loss: -159750.890625\n",
      "Train Epoch: 427 [4352/17352 (25%)] Loss: -147265.812500\n",
      "Train Epoch: 427 [5760/17352 (33%)] Loss: -156135.781250\n",
      "Train Epoch: 427 [7168/17352 (41%)] Loss: -134743.265625\n",
      "Train Epoch: 427 [8576/17352 (49%)] Loss: -146545.468750\n",
      "Train Epoch: 427 [9984/17352 (58%)] Loss: -95289.460938\n",
      "Train Epoch: 427 [11392/17352 (66%)] Loss: -134756.171875\n",
      "Train Epoch: 427 [12800/17352 (74%)] Loss: -150738.781250\n",
      "Train Epoch: 427 [14208/17352 (82%)] Loss: -154844.062500\n",
      "Train Epoch: 427 [15480/17352 (89%)] Loss: -39449.503906\n",
      "Train Epoch: 427 [16213/17352 (93%)] Loss: -14646.350586\n",
      "Train Epoch: 427 [17069/17352 (98%)] Loss: -150265.437500\n",
      "    epoch          : 427\n",
      "    loss           : -127227.39485895554\n",
      "    val_loss       : -72692.2302327474\n",
      "Train Epoch: 428 [128/17352 (1%)] Loss: -162541.593750\n",
      "Train Epoch: 428 [1536/17352 (9%)] Loss: -150572.093750\n",
      "Train Epoch: 428 [2944/17352 (17%)] Loss: -170283.328125\n",
      "Train Epoch: 428 [4352/17352 (25%)] Loss: -187538.531250\n",
      "Train Epoch: 428 [5760/17352 (33%)] Loss: -138115.531250\n",
      "Train Epoch: 428 [7168/17352 (41%)] Loss: -158803.312500\n",
      "Train Epoch: 428 [8576/17352 (49%)] Loss: -107751.453125\n",
      "Train Epoch: 428 [9984/17352 (58%)] Loss: -138988.109375\n",
      "Train Epoch: 428 [11392/17352 (66%)] Loss: -178056.437500\n",
      "Train Epoch: 428 [12800/17352 (74%)] Loss: -175441.375000\n",
      "Train Epoch: 428 [14208/17352 (82%)] Loss: -144643.906250\n",
      "Train Epoch: 428 [15523/17352 (89%)] Loss: -85362.921875\n",
      "Train Epoch: 428 [16336/17352 (94%)] Loss: -85751.921875\n",
      "Train Epoch: 428 [17052/17352 (98%)] Loss: -81919.828125\n",
      "    epoch          : 428\n",
      "    loss           : -133556.5870861079\n",
      "    val_loss       : -73117.52536214193\n",
      "Train Epoch: 429 [128/17352 (1%)] Loss: -129505.203125\n",
      "Train Epoch: 429 [1536/17352 (9%)] Loss: -108072.765625\n",
      "Train Epoch: 429 [2944/17352 (17%)] Loss: -139825.500000\n",
      "Train Epoch: 429 [4352/17352 (25%)] Loss: -89448.523438\n",
      "Train Epoch: 429 [5760/17352 (33%)] Loss: -145141.078125\n",
      "Train Epoch: 429 [7168/17352 (41%)] Loss: -148469.812500\n",
      "Train Epoch: 429 [8576/17352 (49%)] Loss: -149333.718750\n",
      "Train Epoch: 429 [9984/17352 (58%)] Loss: -169485.187500\n",
      "Train Epoch: 429 [11392/17352 (66%)] Loss: -170382.140625\n",
      "Train Epoch: 429 [12800/17352 (74%)] Loss: -143997.203125\n",
      "Train Epoch: 429 [14208/17352 (82%)] Loss: -189430.375000\n",
      "Train Epoch: 429 [15486/17352 (89%)] Loss: -95259.843750\n",
      "Train Epoch: 429 [16243/17352 (94%)] Loss: -65685.328125\n",
      "Train Epoch: 429 [17058/17352 (98%)] Loss: -160146.968750\n",
      "    epoch          : 429\n",
      "    loss           : -130182.68075254299\n",
      "    val_loss       : -74972.0735148112\n",
      "Train Epoch: 430 [128/17352 (1%)] Loss: -149902.843750\n",
      "Train Epoch: 430 [1536/17352 (9%)] Loss: -122929.242188\n",
      "Train Epoch: 430 [2944/17352 (17%)] Loss: -126272.078125\n",
      "Train Epoch: 430 [4352/17352 (25%)] Loss: -143980.531250\n",
      "Train Epoch: 430 [5760/17352 (33%)] Loss: -110834.859375\n",
      "Train Epoch: 430 [7168/17352 (41%)] Loss: -118023.046875\n",
      "Train Epoch: 430 [8576/17352 (49%)] Loss: -143163.015625\n",
      "Train Epoch: 430 [9984/17352 (58%)] Loss: -143758.203125\n",
      "Train Epoch: 430 [11392/17352 (66%)] Loss: -144777.937500\n",
      "Train Epoch: 430 [12800/17352 (74%)] Loss: -148976.687500\n",
      "Train Epoch: 430 [14208/17352 (82%)] Loss: -159760.343750\n",
      "Train Epoch: 430 [15602/17352 (90%)] Loss: -111955.695312\n",
      "Train Epoch: 430 [16172/17352 (93%)] Loss: -12518.039062\n",
      "Train Epoch: 430 [16883/17352 (97%)] Loss: -120442.695312\n",
      "    epoch          : 430\n",
      "    loss           : -129624.78689636641\n",
      "    val_loss       : -76547.94204915364\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch430.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 431 [128/17352 (1%)] Loss: -137329.578125\n",
      "Train Epoch: 431 [1536/17352 (9%)] Loss: -122850.109375\n",
      "Train Epoch: 431 [2944/17352 (17%)] Loss: -141184.687500\n",
      "Train Epoch: 431 [4352/17352 (25%)] Loss: -162046.718750\n",
      "Train Epoch: 431 [5760/17352 (33%)] Loss: -165860.625000\n",
      "Train Epoch: 431 [7168/17352 (41%)] Loss: -121158.398438\n",
      "Train Epoch: 431 [8576/17352 (49%)] Loss: -182610.187500\n",
      "Train Epoch: 431 [9984/17352 (58%)] Loss: -149145.796875\n",
      "Train Epoch: 431 [11392/17352 (66%)] Loss: -169219.062500\n",
      "Train Epoch: 431 [12800/17352 (74%)] Loss: -175297.593750\n",
      "Train Epoch: 431 [14208/17352 (82%)] Loss: -161704.500000\n",
      "Train Epoch: 431 [15435/17352 (89%)] Loss: -46811.531250\n",
      "Train Epoch: 431 [16126/17352 (93%)] Loss: -6460.899414\n",
      "Train Epoch: 431 [16975/17352 (98%)] Loss: -92727.429688\n",
      "    epoch          : 431\n",
      "    loss           : -137587.86931430895\n",
      "    val_loss       : -78178.25170898438\n",
      "Train Epoch: 432 [128/17352 (1%)] Loss: -158878.484375\n",
      "Train Epoch: 432 [1536/17352 (9%)] Loss: -157012.453125\n",
      "Train Epoch: 432 [2944/17352 (17%)] Loss: -160944.437500\n",
      "Train Epoch: 432 [4352/17352 (25%)] Loss: -109077.796875\n",
      "Train Epoch: 432 [5760/17352 (33%)] Loss: -148737.968750\n",
      "Train Epoch: 432 [7168/17352 (41%)] Loss: -144471.062500\n",
      "Train Epoch: 432 [8576/17352 (49%)] Loss: -170163.109375\n",
      "Train Epoch: 432 [9984/17352 (58%)] Loss: -161577.234375\n",
      "Train Epoch: 432 [11392/17352 (66%)] Loss: -161498.578125\n",
      "Train Epoch: 432 [12800/17352 (74%)] Loss: -166784.281250\n",
      "Train Epoch: 432 [14208/17352 (82%)] Loss: -182518.515625\n",
      "Train Epoch: 432 [15544/17352 (90%)] Loss: -95725.570312\n",
      "Train Epoch: 432 [16228/17352 (94%)] Loss: -7286.914062\n",
      "Train Epoch: 432 [16976/17352 (98%)] Loss: -99409.906250\n",
      "    epoch          : 432\n",
      "    loss           : -132371.92657102033\n",
      "    val_loss       : -77590.92598876954\n",
      "Train Epoch: 433 [128/17352 (1%)] Loss: -179002.531250\n",
      "Train Epoch: 433 [1536/17352 (9%)] Loss: -154075.937500\n",
      "Train Epoch: 433 [2944/17352 (17%)] Loss: -178301.000000\n",
      "Train Epoch: 433 [4352/17352 (25%)] Loss: -140519.578125\n",
      "Train Epoch: 433 [5760/17352 (33%)] Loss: -161657.265625\n",
      "Train Epoch: 433 [7168/17352 (41%)] Loss: -118209.906250\n",
      "Train Epoch: 433 [8576/17352 (49%)] Loss: -139835.437500\n",
      "Train Epoch: 433 [9984/17352 (58%)] Loss: -152086.078125\n",
      "Train Epoch: 433 [11392/17352 (66%)] Loss: -132414.375000\n",
      "Train Epoch: 433 [12800/17352 (74%)] Loss: -159235.953125\n",
      "Train Epoch: 433 [14208/17352 (82%)] Loss: -175729.593750\n",
      "Train Epoch: 433 [15541/17352 (90%)] Loss: -51337.796875\n",
      "Train Epoch: 433 [16351/17352 (94%)] Loss: -127099.421875\n",
      "Train Epoch: 433 [16995/17352 (98%)] Loss: -96494.812500\n",
      "    epoch          : 433\n",
      "    loss           : -129373.3355409763\n",
      "    val_loss       : -78492.4266398112\n",
      "Train Epoch: 434 [128/17352 (1%)] Loss: -165028.109375\n",
      "Train Epoch: 434 [1536/17352 (9%)] Loss: -154328.531250\n",
      "Train Epoch: 434 [2944/17352 (17%)] Loss: -145919.875000\n",
      "Train Epoch: 434 [4352/17352 (25%)] Loss: -190490.828125\n",
      "Train Epoch: 434 [5760/17352 (33%)] Loss: -164014.062500\n",
      "Train Epoch: 434 [7168/17352 (41%)] Loss: -161147.062500\n",
      "Train Epoch: 434 [8576/17352 (49%)] Loss: -136662.328125\n",
      "Train Epoch: 434 [9984/17352 (58%)] Loss: -157689.625000\n",
      "Train Epoch: 434 [11392/17352 (66%)] Loss: -112112.703125\n",
      "Train Epoch: 434 [12800/17352 (74%)] Loss: -178178.906250\n",
      "Train Epoch: 434 [14208/17352 (82%)] Loss: -170746.406250\n",
      "Train Epoch: 434 [15559/17352 (90%)] Loss: -101094.296875\n",
      "Train Epoch: 434 [16536/17352 (95%)] Loss: -116131.000000\n",
      "Train Epoch: 434 [17053/17352 (98%)] Loss: -58868.714844\n",
      "    epoch          : 434\n",
      "    loss           : -138799.83425309355\n",
      "    val_loss       : -75836.66955973307\n",
      "Train Epoch: 435 [128/17352 (1%)] Loss: -142537.609375\n",
      "Train Epoch: 435 [1536/17352 (9%)] Loss: -109587.929688\n",
      "Train Epoch: 435 [2944/17352 (17%)] Loss: -77132.734375\n",
      "Train Epoch: 435 [4352/17352 (25%)] Loss: -147009.671875\n",
      "Train Epoch: 435 [5760/17352 (33%)] Loss: -189087.953125\n",
      "Train Epoch: 435 [7168/17352 (41%)] Loss: -177286.796875\n",
      "Train Epoch: 435 [8576/17352 (49%)] Loss: -152557.609375\n",
      "Train Epoch: 435 [9984/17352 (58%)] Loss: -168415.281250\n",
      "Train Epoch: 435 [11392/17352 (66%)] Loss: -152204.765625\n",
      "Train Epoch: 435 [12800/17352 (74%)] Loss: -79890.289062\n",
      "Train Epoch: 435 [14208/17352 (82%)] Loss: -150106.734375\n",
      "Train Epoch: 435 [15502/17352 (89%)] Loss: -44412.660156\n",
      "Train Epoch: 435 [16361/17352 (94%)] Loss: -87875.234375\n",
      "Train Epoch: 435 [16966/17352 (98%)] Loss: -17845.601562\n",
      "    epoch          : 435\n",
      "    loss           : -124615.80116138843\n",
      "    val_loss       : -55603.332157389326\n",
      "Train Epoch: 436 [128/17352 (1%)] Loss: -86203.046875\n",
      "Train Epoch: 436 [1536/17352 (9%)] Loss: -120625.507812\n",
      "Train Epoch: 436 [2944/17352 (17%)] Loss: -159500.750000\n",
      "Train Epoch: 436 [4352/17352 (25%)] Loss: -140186.468750\n",
      "Train Epoch: 436 [5760/17352 (33%)] Loss: -159877.125000\n",
      "Train Epoch: 436 [7168/17352 (41%)] Loss: -166642.796875\n",
      "Train Epoch: 436 [8576/17352 (49%)] Loss: -149253.781250\n",
      "Train Epoch: 436 [9984/17352 (58%)] Loss: -118362.703125\n",
      "Train Epoch: 436 [11392/17352 (66%)] Loss: -141295.031250\n",
      "Train Epoch: 436 [12800/17352 (74%)] Loss: -151685.515625\n",
      "Train Epoch: 436 [14208/17352 (82%)] Loss: -138900.500000\n",
      "Train Epoch: 436 [15522/17352 (89%)] Loss: -71485.460938\n",
      "Train Epoch: 436 [16219/17352 (93%)] Loss: -35176.687500\n",
      "Train Epoch: 436 [16918/17352 (97%)] Loss: -59135.882812\n",
      "    epoch          : 436\n",
      "    loss           : -131016.40653182677\n",
      "    val_loss       : -78163.47850748697\n",
      "Train Epoch: 437 [128/17352 (1%)] Loss: -145358.781250\n",
      "Train Epoch: 437 [1536/17352 (9%)] Loss: -168137.781250\n",
      "Train Epoch: 437 [2944/17352 (17%)] Loss: -150591.640625\n",
      "Train Epoch: 437 [4352/17352 (25%)] Loss: -172440.187500\n",
      "Train Epoch: 437 [5760/17352 (33%)] Loss: -132004.968750\n",
      "Train Epoch: 437 [7168/17352 (41%)] Loss: -156129.015625\n",
      "Train Epoch: 437 [8576/17352 (49%)] Loss: -194336.750000\n",
      "Train Epoch: 437 [9984/17352 (58%)] Loss: -134429.187500\n",
      "Train Epoch: 437 [11392/17352 (66%)] Loss: -146887.281250\n",
      "Train Epoch: 437 [12800/17352 (74%)] Loss: -173854.468750\n",
      "Train Epoch: 437 [14208/17352 (82%)] Loss: -136987.875000\n",
      "Train Epoch: 437 [15457/17352 (89%)] Loss: -50880.031250\n",
      "Train Epoch: 437 [16092/17352 (93%)] Loss: -78384.703125\n",
      "Train Epoch: 437 [16986/17352 (98%)] Loss: -98898.640625\n",
      "    epoch          : 437\n",
      "    loss           : -133837.73427832685\n",
      "    val_loss       : -72593.8767618815\n",
      "Train Epoch: 438 [128/17352 (1%)] Loss: -116731.507812\n",
      "Train Epoch: 438 [1536/17352 (9%)] Loss: -107219.828125\n",
      "Train Epoch: 438 [2944/17352 (17%)] Loss: -110012.703125\n",
      "Train Epoch: 438 [4352/17352 (25%)] Loss: -123132.578125\n",
      "Train Epoch: 438 [5760/17352 (33%)] Loss: -171796.812500\n",
      "Train Epoch: 438 [7168/17352 (41%)] Loss: -114951.953125\n",
      "Train Epoch: 438 [8576/17352 (49%)] Loss: -92381.218750\n",
      "Train Epoch: 438 [9984/17352 (58%)] Loss: -141108.843750\n",
      "Train Epoch: 438 [11392/17352 (66%)] Loss: -141312.687500\n",
      "Train Epoch: 438 [12800/17352 (74%)] Loss: -179732.343750\n",
      "Train Epoch: 438 [14208/17352 (82%)] Loss: -136206.546875\n",
      "Train Epoch: 438 [15435/17352 (89%)] Loss: -44313.121094\n",
      "Train Epoch: 438 [16180/17352 (93%)] Loss: -58168.609375\n",
      "Train Epoch: 438 [16991/17352 (98%)] Loss: -60793.417969\n",
      "    epoch          : 438\n",
      "    loss           : -124028.04997837143\n",
      "    val_loss       : -74019.09456380208\n",
      "Train Epoch: 439 [128/17352 (1%)] Loss: -145657.468750\n",
      "Train Epoch: 439 [1536/17352 (9%)] Loss: -165650.562500\n",
      "Train Epoch: 439 [2944/17352 (17%)] Loss: -127807.468750\n",
      "Train Epoch: 439 [4352/17352 (25%)] Loss: -123665.671875\n",
      "Train Epoch: 439 [5760/17352 (33%)] Loss: -107724.562500\n",
      "Train Epoch: 439 [7168/17352 (41%)] Loss: -105556.265625\n",
      "Train Epoch: 439 [8576/17352 (49%)] Loss: -186251.546875\n",
      "Train Epoch: 439 [9984/17352 (58%)] Loss: -124783.328125\n",
      "Train Epoch: 439 [11392/17352 (66%)] Loss: -137545.968750\n",
      "Train Epoch: 439 [12800/17352 (74%)] Loss: -143533.656250\n",
      "Train Epoch: 439 [14208/17352 (82%)] Loss: -120090.875000\n",
      "Train Epoch: 439 [15500/17352 (89%)] Loss: -117371.625000\n",
      "Train Epoch: 439 [16256/17352 (94%)] Loss: -2133.209961\n",
      "Train Epoch: 439 [17012/17352 (98%)] Loss: -18451.984375\n",
      "    epoch          : 439\n",
      "    loss           : -123335.16338414953\n",
      "    val_loss       : -51643.32842203776\n",
      "Train Epoch: 440 [128/17352 (1%)] Loss: -102220.335938\n",
      "Train Epoch: 440 [1536/17352 (9%)] Loss: -113800.445312\n",
      "Train Epoch: 440 [2944/17352 (17%)] Loss: -135879.250000\n",
      "Train Epoch: 440 [4352/17352 (25%)] Loss: -100746.609375\n",
      "Train Epoch: 440 [5760/17352 (33%)] Loss: -157372.500000\n",
      "Train Epoch: 440 [7168/17352 (41%)] Loss: -133163.734375\n",
      "Train Epoch: 440 [8576/17352 (49%)] Loss: -139271.890625\n",
      "Train Epoch: 440 [9984/17352 (58%)] Loss: -137531.984375\n",
      "Train Epoch: 440 [11392/17352 (66%)] Loss: -164551.312500\n",
      "Train Epoch: 440 [12800/17352 (74%)] Loss: -138814.359375\n",
      "Train Epoch: 440 [14208/17352 (82%)] Loss: -138566.859375\n",
      "Train Epoch: 440 [15463/17352 (89%)] Loss: -115833.429688\n",
      "Train Epoch: 440 [16209/17352 (93%)] Loss: -84994.640625\n",
      "Train Epoch: 440 [17157/17352 (99%)] Loss: -143266.046875\n",
      "    epoch          : 440\n",
      "    loss           : -125322.33107434983\n",
      "    val_loss       : -75523.04652913411\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch440.pth ...\n",
      "Train Epoch: 441 [128/17352 (1%)] Loss: -138504.531250\n",
      "Train Epoch: 441 [1536/17352 (9%)] Loss: -136898.046875\n",
      "Train Epoch: 441 [2944/17352 (17%)] Loss: -141339.781250\n",
      "Train Epoch: 441 [4352/17352 (25%)] Loss: -143590.656250\n",
      "Train Epoch: 441 [5760/17352 (33%)] Loss: -179325.015625\n",
      "Train Epoch: 441 [7168/17352 (41%)] Loss: -163706.968750\n",
      "Train Epoch: 441 [8576/17352 (49%)] Loss: -146687.343750\n",
      "Train Epoch: 441 [9984/17352 (58%)] Loss: -137790.703125\n",
      "Train Epoch: 441 [11392/17352 (66%)] Loss: -151566.953125\n",
      "Train Epoch: 441 [12800/17352 (74%)] Loss: -142535.093750\n",
      "Train Epoch: 441 [14208/17352 (82%)] Loss: -176462.656250\n",
      "Train Epoch: 441 [15574/17352 (90%)] Loss: -121138.359375\n",
      "Train Epoch: 441 [16273/17352 (94%)] Loss: -48128.664062\n",
      "Train Epoch: 441 [16965/17352 (98%)] Loss: -21379.080078\n",
      "    epoch          : 441\n",
      "    loss           : -139387.09793807677\n",
      "    val_loss       : -64719.65512695313\n",
      "Train Epoch: 442 [128/17352 (1%)] Loss: -113408.492188\n",
      "Train Epoch: 442 [1536/17352 (9%)] Loss: -139076.140625\n",
      "Train Epoch: 442 [2944/17352 (17%)] Loss: -139437.000000\n",
      "Train Epoch: 442 [4352/17352 (25%)] Loss: -189486.109375\n",
      "Train Epoch: 442 [5760/17352 (33%)] Loss: -118610.218750\n",
      "Train Epoch: 442 [7168/17352 (41%)] Loss: -124408.390625\n",
      "Train Epoch: 442 [8576/17352 (49%)] Loss: -139841.671875\n",
      "Train Epoch: 442 [9984/17352 (58%)] Loss: -160579.656250\n",
      "Train Epoch: 442 [11392/17352 (66%)] Loss: -159708.750000\n",
      "Train Epoch: 442 [12800/17352 (74%)] Loss: -129843.078125\n",
      "Train Epoch: 442 [14208/17352 (82%)] Loss: -159070.031250\n",
      "Train Epoch: 442 [15544/17352 (90%)] Loss: -87675.781250\n",
      "Train Epoch: 442 [16266/17352 (94%)] Loss: -120123.039062\n",
      "Train Epoch: 442 [17016/17352 (98%)] Loss: -54734.761719\n",
      "    epoch          : 442\n",
      "    loss           : -134117.62789691694\n",
      "    val_loss       : -74752.79087320964\n",
      "Train Epoch: 443 [128/17352 (1%)] Loss: -118353.484375\n",
      "Train Epoch: 443 [1536/17352 (9%)] Loss: -116628.968750\n",
      "Train Epoch: 443 [2944/17352 (17%)] Loss: -161302.421875\n",
      "Train Epoch: 443 [4352/17352 (25%)] Loss: -171226.546875\n",
      "Train Epoch: 443 [5760/17352 (33%)] Loss: -133098.500000\n",
      "Train Epoch: 443 [7168/17352 (41%)] Loss: -125914.515625\n",
      "Train Epoch: 443 [8576/17352 (49%)] Loss: -160146.781250\n",
      "Train Epoch: 443 [9984/17352 (58%)] Loss: -126899.312500\n",
      "Train Epoch: 443 [11392/17352 (66%)] Loss: -147277.812500\n",
      "Train Epoch: 443 [12800/17352 (74%)] Loss: -136747.453125\n",
      "Train Epoch: 443 [14208/17352 (82%)] Loss: -139363.140625\n",
      "Train Epoch: 443 [15546/17352 (90%)] Loss: -106656.125000\n",
      "Train Epoch: 443 [16140/17352 (93%)] Loss: -103081.765625\n",
      "Train Epoch: 443 [16933/17352 (98%)] Loss: -91377.843750\n",
      "    epoch          : 443\n",
      "    loss           : -123458.68264012689\n",
      "    val_loss       : -70515.38892822266\n",
      "Train Epoch: 444 [128/17352 (1%)] Loss: -130446.218750\n",
      "Train Epoch: 444 [1536/17352 (9%)] Loss: -165634.218750\n",
      "Train Epoch: 444 [2944/17352 (17%)] Loss: -166905.484375\n",
      "Train Epoch: 444 [4352/17352 (25%)] Loss: -145951.937500\n",
      "Train Epoch: 444 [5760/17352 (33%)] Loss: -153622.140625\n",
      "Train Epoch: 444 [7168/17352 (41%)] Loss: -147277.968750\n",
      "Train Epoch: 444 [8576/17352 (49%)] Loss: -150924.812500\n",
      "Train Epoch: 444 [9984/17352 (58%)] Loss: -159521.515625\n",
      "Train Epoch: 444 [11392/17352 (66%)] Loss: -165025.078125\n",
      "Train Epoch: 444 [12800/17352 (74%)] Loss: -129050.289062\n",
      "Train Epoch: 444 [14208/17352 (82%)] Loss: -119113.351562\n",
      "Train Epoch: 444 [15576/17352 (90%)] Loss: -144818.765625\n",
      "Train Epoch: 444 [16211/17352 (93%)] Loss: -87045.531250\n",
      "Train Epoch: 444 [16967/17352 (98%)] Loss: -155265.375000\n",
      "    epoch          : 444\n",
      "    loss           : -135884.82843632027\n",
      "    val_loss       : -75831.33376871745\n",
      "Train Epoch: 445 [128/17352 (1%)] Loss: -128944.328125\n",
      "Train Epoch: 445 [1536/17352 (9%)] Loss: -151420.593750\n",
      "Train Epoch: 445 [2944/17352 (17%)] Loss: -143671.515625\n",
      "Train Epoch: 445 [4352/17352 (25%)] Loss: -149572.921875\n",
      "Train Epoch: 445 [5760/17352 (33%)] Loss: -151705.921875\n",
      "Train Epoch: 445 [7168/17352 (41%)] Loss: -165023.078125\n",
      "Train Epoch: 445 [8576/17352 (49%)] Loss: -177854.125000\n",
      "Train Epoch: 445 [9984/17352 (58%)] Loss: -144195.562500\n",
      "Train Epoch: 445 [11392/17352 (66%)] Loss: -117224.109375\n",
      "Train Epoch: 445 [12800/17352 (74%)] Loss: -157978.062500\n",
      "Train Epoch: 445 [14208/17352 (82%)] Loss: -166469.687500\n",
      "Train Epoch: 445 [15452/17352 (89%)] Loss: -18383.812500\n",
      "Train Epoch: 445 [16229/17352 (94%)] Loss: -99964.421875\n",
      "Train Epoch: 445 [16925/17352 (98%)] Loss: -87495.671875\n",
      "    epoch          : 445\n",
      "    loss           : -134533.3180513318\n",
      "    val_loss       : -70240.41174723307\n",
      "Train Epoch: 446 [128/17352 (1%)] Loss: -144724.765625\n",
      "Train Epoch: 446 [1536/17352 (9%)] Loss: -152495.921875\n",
      "Train Epoch: 446 [2944/17352 (17%)] Loss: -129177.328125\n",
      "Train Epoch: 446 [4352/17352 (25%)] Loss: -182358.250000\n",
      "Train Epoch: 446 [5760/17352 (33%)] Loss: -142859.375000\n",
      "Train Epoch: 446 [7168/17352 (41%)] Loss: -100228.429688\n",
      "Train Epoch: 446 [8576/17352 (49%)] Loss: -124729.593750\n",
      "Train Epoch: 446 [9984/17352 (58%)] Loss: -109060.023438\n",
      "Train Epoch: 446 [11392/17352 (66%)] Loss: -154177.171875\n",
      "Train Epoch: 446 [12800/17352 (74%)] Loss: -191682.734375\n",
      "Train Epoch: 446 [14208/17352 (82%)] Loss: -170360.328125\n",
      "Train Epoch: 446 [15490/17352 (89%)] Loss: -115578.328125\n",
      "Train Epoch: 446 [16235/17352 (94%)] Loss: -120675.929688\n",
      "Train Epoch: 446 [16966/17352 (98%)] Loss: -75854.656250\n",
      "    epoch          : 446\n",
      "    loss           : -132722.82357644715\n",
      "    val_loss       : -69026.56979166667\n",
      "Train Epoch: 447 [128/17352 (1%)] Loss: -129856.593750\n",
      "Train Epoch: 447 [1536/17352 (9%)] Loss: -130711.187500\n",
      "Train Epoch: 447 [2944/17352 (17%)] Loss: -114216.859375\n",
      "Train Epoch: 447 [4352/17352 (25%)] Loss: -139966.046875\n",
      "Train Epoch: 447 [5760/17352 (33%)] Loss: -172854.656250\n",
      "Train Epoch: 447 [7168/17352 (41%)] Loss: -149303.203125\n",
      "Train Epoch: 447 [8576/17352 (49%)] Loss: -152932.281250\n",
      "Train Epoch: 447 [9984/17352 (58%)] Loss: -125054.554688\n",
      "Train Epoch: 447 [11392/17352 (66%)] Loss: -113133.984375\n",
      "Train Epoch: 447 [12800/17352 (74%)] Loss: -119678.343750\n",
      "Train Epoch: 447 [14208/17352 (82%)] Loss: -122523.765625\n",
      "Train Epoch: 447 [15480/17352 (89%)] Loss: -77189.335938\n",
      "Train Epoch: 447 [16380/17352 (94%)] Loss: -91708.171875\n",
      "Train Epoch: 447 [17133/17352 (99%)] Loss: -60304.562500\n",
      "    epoch          : 447\n",
      "    loss           : -127822.57356989304\n",
      "    val_loss       : -64501.57517903646\n",
      "Train Epoch: 448 [128/17352 (1%)] Loss: -134069.171875\n",
      "Train Epoch: 448 [1536/17352 (9%)] Loss: -130275.250000\n",
      "Train Epoch: 448 [2944/17352 (17%)] Loss: -164557.671875\n",
      "Train Epoch: 448 [4352/17352 (25%)] Loss: -174598.937500\n",
      "Train Epoch: 448 [5760/17352 (33%)] Loss: -117878.281250\n",
      "Train Epoch: 448 [7168/17352 (41%)] Loss: -165771.968750\n",
      "Train Epoch: 448 [8576/17352 (49%)] Loss: -131257.031250\n",
      "Train Epoch: 448 [9984/17352 (58%)] Loss: -140987.562500\n",
      "Train Epoch: 448 [11392/17352 (66%)] Loss: -149216.687500\n",
      "Train Epoch: 448 [12800/17352 (74%)] Loss: -142511.046875\n",
      "Train Epoch: 448 [14208/17352 (82%)] Loss: -126429.781250\n",
      "Train Epoch: 448 [15467/17352 (89%)] Loss: -52609.468750\n",
      "Train Epoch: 448 [16084/17352 (93%)] Loss: -13218.957031\n",
      "Train Epoch: 448 [17033/17352 (98%)] Loss: -57178.625000\n",
      "    epoch          : 448\n",
      "    loss           : -134230.3196144872\n",
      "    val_loss       : -75548.83116455078\n",
      "Train Epoch: 449 [128/17352 (1%)] Loss: -128047.867188\n",
      "Train Epoch: 449 [1536/17352 (9%)] Loss: -130771.757812\n",
      "Train Epoch: 449 [2944/17352 (17%)] Loss: -171092.593750\n",
      "Train Epoch: 449 [4352/17352 (25%)] Loss: -184960.906250\n",
      "Train Epoch: 449 [5760/17352 (33%)] Loss: -154748.187500\n",
      "Train Epoch: 449 [7168/17352 (41%)] Loss: -54362.039062\n",
      "Train Epoch: 449 [8576/17352 (49%)] Loss: -114797.351562\n",
      "Train Epoch: 449 [9984/17352 (58%)] Loss: -144320.703125\n",
      "Train Epoch: 449 [11392/17352 (66%)] Loss: -147189.546875\n",
      "Train Epoch: 449 [12800/17352 (74%)] Loss: -173990.750000\n",
      "Train Epoch: 449 [14208/17352 (82%)] Loss: -127678.523438\n",
      "Train Epoch: 449 [15522/17352 (89%)] Loss: -80302.625000\n",
      "Train Epoch: 449 [16332/17352 (94%)] Loss: -114554.906250\n",
      "Train Epoch: 449 [17046/17352 (98%)] Loss: -45511.914062\n",
      "    epoch          : 449\n",
      "    loss           : -127455.99898083578\n",
      "    val_loss       : -63691.78462727865\n",
      "Train Epoch: 450 [128/17352 (1%)] Loss: -97100.476562\n",
      "Train Epoch: 450 [1536/17352 (9%)] Loss: -153894.093750\n",
      "Train Epoch: 450 [2944/17352 (17%)] Loss: -142613.781250\n",
      "Train Epoch: 450 [4352/17352 (25%)] Loss: -163240.218750\n",
      "Train Epoch: 450 [5760/17352 (33%)] Loss: -154355.421875\n",
      "Train Epoch: 450 [7168/17352 (41%)] Loss: -159747.000000\n",
      "Train Epoch: 450 [8576/17352 (49%)] Loss: -174145.015625\n",
      "Train Epoch: 450 [9984/17352 (58%)] Loss: -118517.921875\n",
      "Train Epoch: 450 [11392/17352 (66%)] Loss: -122552.203125\n",
      "Train Epoch: 450 [12800/17352 (74%)] Loss: -173863.828125\n",
      "Train Epoch: 450 [14208/17352 (82%)] Loss: -138229.515625\n",
      "Train Epoch: 450 [15451/17352 (89%)] Loss: -88088.765625\n",
      "Train Epoch: 450 [16382/17352 (94%)] Loss: -161286.687500\n",
      "Train Epoch: 450 [16943/17352 (98%)] Loss: -89924.500000\n",
      "    epoch          : 450\n",
      "    loss           : -135352.6934839031\n",
      "    val_loss       : -76547.26021728516\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [128/17352 (1%)] Loss: -143278.187500\n",
      "Train Epoch: 451 [1536/17352 (9%)] Loss: -147324.031250\n",
      "Train Epoch: 451 [2944/17352 (17%)] Loss: -151908.578125\n",
      "Train Epoch: 451 [4352/17352 (25%)] Loss: -125608.929688\n",
      "Train Epoch: 451 [5760/17352 (33%)] Loss: -147031.781250\n",
      "Train Epoch: 451 [7168/17352 (41%)] Loss: -157193.906250\n",
      "Train Epoch: 451 [8576/17352 (49%)] Loss: -171409.968750\n",
      "Train Epoch: 451 [9984/17352 (58%)] Loss: -180493.843750\n",
      "Train Epoch: 451 [11392/17352 (66%)] Loss: -150056.359375\n",
      "Train Epoch: 451 [12800/17352 (74%)] Loss: -161923.812500\n",
      "Train Epoch: 451 [14208/17352 (82%)] Loss: -158911.781250\n",
      "Train Epoch: 451 [15376/17352 (89%)] Loss: -15286.945312\n",
      "Train Epoch: 451 [16257/17352 (94%)] Loss: -33787.343750\n",
      "Train Epoch: 451 [17016/17352 (98%)] Loss: -84589.007812\n",
      "    epoch          : 451\n",
      "    loss           : -132559.82900325084\n",
      "    val_loss       : -66271.34329020182\n",
      "Train Epoch: 452 [128/17352 (1%)] Loss: -101032.578125\n",
      "Train Epoch: 452 [1536/17352 (9%)] Loss: -176790.625000\n",
      "Train Epoch: 452 [2944/17352 (17%)] Loss: -165339.843750\n",
      "Train Epoch: 452 [4352/17352 (25%)] Loss: -114114.968750\n",
      "Train Epoch: 452 [5760/17352 (33%)] Loss: -151045.093750\n",
      "Train Epoch: 452 [7168/17352 (41%)] Loss: -202235.531250\n",
      "Train Epoch: 452 [8576/17352 (49%)] Loss: -146283.859375\n",
      "Train Epoch: 452 [9984/17352 (58%)] Loss: -157620.656250\n",
      "Train Epoch: 452 [11392/17352 (66%)] Loss: -115015.421875\n",
      "Train Epoch: 452 [12800/17352 (74%)] Loss: -123800.351562\n",
      "Train Epoch: 452 [14208/17352 (82%)] Loss: -148126.281250\n",
      "Train Epoch: 452 [15515/17352 (89%)] Loss: -77856.960938\n",
      "Train Epoch: 452 [16129/17352 (93%)] Loss: -67108.750000\n",
      "Train Epoch: 452 [16982/17352 (98%)] Loss: -80608.867188\n",
      "    epoch          : 452\n",
      "    loss           : -131277.5246139629\n",
      "    val_loss       : -74917.04720865886\n",
      "Train Epoch: 453 [128/17352 (1%)] Loss: -182158.781250\n",
      "Train Epoch: 453 [1536/17352 (9%)] Loss: -151865.031250\n",
      "Train Epoch: 453 [2944/17352 (17%)] Loss: -125773.843750\n",
      "Train Epoch: 453 [4352/17352 (25%)] Loss: -138875.187500\n",
      "Train Epoch: 453 [5760/17352 (33%)] Loss: -156990.828125\n",
      "Train Epoch: 453 [7168/17352 (41%)] Loss: -157851.968750\n",
      "Train Epoch: 453 [8576/17352 (49%)] Loss: -168253.906250\n",
      "Train Epoch: 453 [9984/17352 (58%)] Loss: -146711.593750\n",
      "Train Epoch: 453 [11392/17352 (66%)] Loss: -172260.609375\n",
      "Train Epoch: 453 [12800/17352 (74%)] Loss: -169632.062500\n",
      "Train Epoch: 453 [14208/17352 (82%)] Loss: -134325.609375\n",
      "Train Epoch: 453 [15451/17352 (89%)] Loss: -101401.671875\n",
      "Train Epoch: 453 [16205/17352 (93%)] Loss: -115880.125000\n",
      "Train Epoch: 453 [16846/17352 (97%)] Loss: -96579.843750\n",
      "    epoch          : 453\n",
      "    loss           : -139557.42381501678\n",
      "    val_loss       : -74707.5628540039\n",
      "Train Epoch: 454 [128/17352 (1%)] Loss: -143472.046875\n",
      "Train Epoch: 454 [1536/17352 (9%)] Loss: -164993.625000\n",
      "Train Epoch: 454 [2944/17352 (17%)] Loss: -130097.328125\n",
      "Train Epoch: 454 [4352/17352 (25%)] Loss: -169870.890625\n",
      "Train Epoch: 454 [5760/17352 (33%)] Loss: -173486.687500\n",
      "Train Epoch: 454 [7168/17352 (41%)] Loss: -150446.593750\n",
      "Train Epoch: 454 [8576/17352 (49%)] Loss: -149224.687500\n",
      "Train Epoch: 454 [9984/17352 (58%)] Loss: -164536.125000\n",
      "Train Epoch: 454 [11392/17352 (66%)] Loss: -123732.406250\n",
      "Train Epoch: 454 [12800/17352 (74%)] Loss: -143860.359375\n",
      "Train Epoch: 454 [14208/17352 (82%)] Loss: -130325.578125\n",
      "Train Epoch: 454 [15479/17352 (89%)] Loss: -27322.859375\n",
      "Train Epoch: 454 [16232/17352 (94%)] Loss: -72551.578125\n",
      "Train Epoch: 454 [16933/17352 (98%)] Loss: -2922.249268\n",
      "    epoch          : 454\n",
      "    loss           : -135198.5625114697\n",
      "    val_loss       : -70273.51996663412\n",
      "Train Epoch: 455 [128/17352 (1%)] Loss: -128801.953125\n",
      "Train Epoch: 455 [1536/17352 (9%)] Loss: -158590.718750\n",
      "Train Epoch: 455 [2944/17352 (17%)] Loss: -139212.312500\n",
      "Train Epoch: 455 [4352/17352 (25%)] Loss: -168735.203125\n",
      "Train Epoch: 455 [5760/17352 (33%)] Loss: -165902.656250\n",
      "Train Epoch: 455 [7168/17352 (41%)] Loss: -86829.664062\n",
      "Train Epoch: 455 [8576/17352 (49%)] Loss: -115021.273438\n",
      "Train Epoch: 455 [9984/17352 (58%)] Loss: -145010.281250\n",
      "Train Epoch: 455 [11392/17352 (66%)] Loss: -133845.531250\n",
      "Train Epoch: 455 [12800/17352 (74%)] Loss: -166001.937500\n",
      "Train Epoch: 455 [14208/17352 (82%)] Loss: -164962.281250\n",
      "Train Epoch: 455 [15492/17352 (89%)] Loss: -57288.238281\n",
      "Train Epoch: 455 [16208/17352 (93%)] Loss: -79294.484375\n",
      "Train Epoch: 455 [16943/17352 (98%)] Loss: -54123.292969\n",
      "    epoch          : 455\n",
      "    loss           : -133855.33071551227\n",
      "    val_loss       : -62050.725638834636\n",
      "Train Epoch: 456 [128/17352 (1%)] Loss: -141333.765625\n",
      "Train Epoch: 456 [1536/17352 (9%)] Loss: -137227.281250\n",
      "Train Epoch: 456 [2944/17352 (17%)] Loss: -155896.500000\n",
      "Train Epoch: 456 [4352/17352 (25%)] Loss: -127863.093750\n",
      "Train Epoch: 456 [5760/17352 (33%)] Loss: -138255.203125\n",
      "Train Epoch: 456 [7168/17352 (41%)] Loss: -184242.140625\n",
      "Train Epoch: 456 [8576/17352 (49%)] Loss: -138131.296875\n",
      "Train Epoch: 456 [9984/17352 (58%)] Loss: -156003.171875\n",
      "Train Epoch: 456 [11392/17352 (66%)] Loss: -125879.453125\n",
      "Train Epoch: 456 [12800/17352 (74%)] Loss: -129830.875000\n",
      "Train Epoch: 456 [14208/17352 (82%)] Loss: -179315.125000\n",
      "Train Epoch: 456 [15539/17352 (90%)] Loss: -118449.617188\n",
      "Train Epoch: 456 [16443/17352 (95%)] Loss: -95363.609375\n",
      "Train Epoch: 456 [17175/17352 (99%)] Loss: -113533.757812\n",
      "    epoch          : 456\n",
      "    loss           : -129284.9645537306\n",
      "    val_loss       : -68608.97388102213\n",
      "Train Epoch: 457 [128/17352 (1%)] Loss: -131691.921875\n",
      "Train Epoch: 457 [1536/17352 (9%)] Loss: -158409.500000\n",
      "Train Epoch: 457 [2944/17352 (17%)] Loss: -129288.015625\n",
      "Train Epoch: 457 [4352/17352 (25%)] Loss: -152756.765625\n",
      "Train Epoch: 457 [5760/17352 (33%)] Loss: -144883.375000\n",
      "Train Epoch: 457 [7168/17352 (41%)] Loss: -165178.640625\n",
      "Train Epoch: 457 [8576/17352 (49%)] Loss: -136991.968750\n",
      "Train Epoch: 457 [9984/17352 (58%)] Loss: -155370.593750\n",
      "Train Epoch: 457 [11392/17352 (66%)] Loss: -126346.984375\n",
      "Train Epoch: 457 [12800/17352 (74%)] Loss: -88969.359375\n",
      "Train Epoch: 457 [14208/17352 (82%)] Loss: -120080.890625\n",
      "Train Epoch: 457 [15460/17352 (89%)] Loss: -16531.050781\n",
      "Train Epoch: 457 [16332/17352 (94%)] Loss: -39760.605469\n",
      "Train Epoch: 457 [17072/17352 (98%)] Loss: -5496.244141\n",
      "    epoch          : 457\n",
      "    loss           : -128507.80022415059\n",
      "    val_loss       : -67749.58094889323\n",
      "Train Epoch: 458 [128/17352 (1%)] Loss: -81759.875000\n",
      "Train Epoch: 458 [1536/17352 (9%)] Loss: -140948.500000\n",
      "Train Epoch: 458 [2944/17352 (17%)] Loss: -184221.031250\n",
      "Train Epoch: 458 [4352/17352 (25%)] Loss: -111106.234375\n",
      "Train Epoch: 458 [5760/17352 (33%)] Loss: -162246.437500\n",
      "Train Epoch: 458 [7168/17352 (41%)] Loss: -137846.140625\n",
      "Train Epoch: 458 [8576/17352 (49%)] Loss: -173151.578125\n",
      "Train Epoch: 458 [9984/17352 (58%)] Loss: -150263.078125\n",
      "Train Epoch: 458 [11392/17352 (66%)] Loss: -122721.351562\n",
      "Train Epoch: 458 [12800/17352 (74%)] Loss: -171291.078125\n",
      "Train Epoch: 458 [14208/17352 (82%)] Loss: -162240.468750\n",
      "Train Epoch: 458 [15527/17352 (89%)] Loss: -91578.500000\n",
      "Train Epoch: 458 [16235/17352 (94%)] Loss: -119547.523438\n",
      "Train Epoch: 458 [17029/17352 (98%)] Loss: -34278.503906\n",
      "    epoch          : 458\n",
      "    loss           : -135061.48157639473\n",
      "    val_loss       : -67432.90468343098\n",
      "Train Epoch: 459 [128/17352 (1%)] Loss: -128328.984375\n",
      "Train Epoch: 459 [1536/17352 (9%)] Loss: -146082.296875\n",
      "Train Epoch: 459 [2944/17352 (17%)] Loss: -121254.218750\n",
      "Train Epoch: 459 [4352/17352 (25%)] Loss: -118676.281250\n",
      "Train Epoch: 459 [5760/17352 (33%)] Loss: -125762.789062\n",
      "Train Epoch: 459 [7168/17352 (41%)] Loss: -158037.625000\n",
      "Train Epoch: 459 [8576/17352 (49%)] Loss: -153917.625000\n",
      "Train Epoch: 459 [9984/17352 (58%)] Loss: -143750.625000\n",
      "Train Epoch: 459 [11392/17352 (66%)] Loss: -194918.031250\n",
      "Train Epoch: 459 [12800/17352 (74%)] Loss: -186666.968750\n",
      "Train Epoch: 459 [14208/17352 (82%)] Loss: -124754.195312\n",
      "Train Epoch: 459 [15531/17352 (90%)] Loss: -104587.812500\n",
      "Train Epoch: 459 [16329/17352 (94%)] Loss: -51632.191406\n",
      "Train Epoch: 459 [17088/17352 (98%)] Loss: -120459.312500\n",
      "    epoch          : 459\n",
      "    loss           : -133299.7929327024\n",
      "    val_loss       : -56454.79065755208\n",
      "Train Epoch: 460 [128/17352 (1%)] Loss: -119194.609375\n",
      "Train Epoch: 460 [1536/17352 (9%)] Loss: -163855.437500\n",
      "Train Epoch: 460 [2944/17352 (17%)] Loss: -126426.429688\n",
      "Train Epoch: 460 [4352/17352 (25%)] Loss: -159438.796875\n",
      "Train Epoch: 460 [5760/17352 (33%)] Loss: -176781.703125\n",
      "Train Epoch: 460 [7168/17352 (41%)] Loss: -144423.406250\n",
      "Train Epoch: 460 [8576/17352 (49%)] Loss: -140483.796875\n",
      "Train Epoch: 460 [9984/17352 (58%)] Loss: -157402.031250\n",
      "Train Epoch: 460 [11392/17352 (66%)] Loss: -186200.875000\n",
      "Train Epoch: 460 [12800/17352 (74%)] Loss: -179766.453125\n",
      "Train Epoch: 460 [14208/17352 (82%)] Loss: -149269.921875\n",
      "Train Epoch: 460 [15476/17352 (89%)] Loss: -19051.259766\n",
      "Train Epoch: 460 [16179/17352 (93%)] Loss: -52870.445312\n",
      "Train Epoch: 460 [17026/17352 (98%)] Loss: -28861.738281\n",
      "    epoch          : 460\n",
      "    loss           : -129459.66119343802\n",
      "    val_loss       : -54667.78740641276\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch460.pth ...\n",
      "Train Epoch: 461 [128/17352 (1%)] Loss: -117700.546875\n",
      "Train Epoch: 461 [1536/17352 (9%)] Loss: -148598.375000\n",
      "Train Epoch: 461 [2944/17352 (17%)] Loss: -153712.671875\n",
      "Train Epoch: 461 [4352/17352 (25%)] Loss: -157480.156250\n",
      "Train Epoch: 461 [5760/17352 (33%)] Loss: -139521.703125\n",
      "Train Epoch: 461 [7168/17352 (41%)] Loss: -162406.937500\n",
      "Train Epoch: 461 [8576/17352 (49%)] Loss: -164074.062500\n",
      "Train Epoch: 461 [9984/17352 (58%)] Loss: -140380.484375\n",
      "Train Epoch: 461 [11392/17352 (66%)] Loss: -185517.250000\n",
      "Train Epoch: 461 [12800/17352 (74%)] Loss: -155351.250000\n",
      "Train Epoch: 461 [14208/17352 (82%)] Loss: -160634.828125\n",
      "Train Epoch: 461 [15526/17352 (89%)] Loss: -97834.093750\n",
      "Train Epoch: 461 [16403/17352 (95%)] Loss: -19933.957031\n",
      "Train Epoch: 461 [16959/17352 (98%)] Loss: -51435.562500\n",
      "    epoch          : 461\n",
      "    loss           : -133028.66601070942\n",
      "    val_loss       : -73872.11535644531\n",
      "Train Epoch: 462 [128/17352 (1%)] Loss: -126550.343750\n",
      "Train Epoch: 462 [1536/17352 (9%)] Loss: -135748.281250\n",
      "Train Epoch: 462 [2944/17352 (17%)] Loss: -163727.406250\n",
      "Train Epoch: 462 [4352/17352 (25%)] Loss: -163346.890625\n",
      "Train Epoch: 462 [5760/17352 (33%)] Loss: -135905.843750\n",
      "Train Epoch: 462 [7168/17352 (41%)] Loss: -174550.500000\n",
      "Train Epoch: 462 [8576/17352 (49%)] Loss: -195600.000000\n",
      "Train Epoch: 462 [9984/17352 (58%)] Loss: -132450.828125\n",
      "Train Epoch: 462 [11392/17352 (66%)] Loss: -153141.718750\n",
      "Train Epoch: 462 [12800/17352 (74%)] Loss: -150056.781250\n",
      "Train Epoch: 462 [14208/17352 (82%)] Loss: -120101.046875\n",
      "Train Epoch: 462 [15584/17352 (90%)] Loss: -108972.296875\n",
      "Train Epoch: 462 [16185/17352 (93%)] Loss: -44272.898438\n",
      "Train Epoch: 462 [16933/17352 (98%)] Loss: -85623.351562\n",
      "    epoch          : 462\n",
      "    loss           : -135326.69403117136\n",
      "    val_loss       : -76040.55074869792\n",
      "Train Epoch: 463 [128/17352 (1%)] Loss: -155883.078125\n",
      "Train Epoch: 463 [1536/17352 (9%)] Loss: -178341.390625\n",
      "Train Epoch: 463 [2944/17352 (17%)] Loss: -128371.851562\n",
      "Train Epoch: 463 [4352/17352 (25%)] Loss: -155020.843750\n",
      "Train Epoch: 463 [5760/17352 (33%)] Loss: -170047.671875\n",
      "Train Epoch: 463 [7168/17352 (41%)] Loss: -154777.593750\n",
      "Train Epoch: 463 [8576/17352 (49%)] Loss: -147293.531250\n",
      "Train Epoch: 463 [9984/17352 (58%)] Loss: -129848.843750\n",
      "Train Epoch: 463 [11392/17352 (66%)] Loss: -123718.312500\n",
      "Train Epoch: 463 [12800/17352 (74%)] Loss: -164607.046875\n",
      "Train Epoch: 463 [14208/17352 (82%)] Loss: -146143.890625\n",
      "Train Epoch: 463 [15409/17352 (89%)] Loss: -12012.027344\n",
      "Train Epoch: 463 [16298/17352 (94%)] Loss: -31387.886719\n",
      "Train Epoch: 463 [17033/17352 (98%)] Loss: -76450.335938\n",
      "    epoch          : 463\n",
      "    loss           : -136305.12816235842\n",
      "    val_loss       : -64189.35219319662\n",
      "Train Epoch: 464 [128/17352 (1%)] Loss: -133790.468750\n",
      "Train Epoch: 464 [1536/17352 (9%)] Loss: -166841.015625\n",
      "Train Epoch: 464 [2944/17352 (17%)] Loss: -144440.015625\n",
      "Train Epoch: 464 [4352/17352 (25%)] Loss: -159049.546875\n",
      "Train Epoch: 464 [5760/17352 (33%)] Loss: -158166.781250\n",
      "Train Epoch: 464 [7168/17352 (41%)] Loss: -155009.421875\n",
      "Train Epoch: 464 [8576/17352 (49%)] Loss: -69793.859375\n",
      "Train Epoch: 464 [9984/17352 (58%)] Loss: -94187.906250\n",
      "Train Epoch: 464 [11392/17352 (66%)] Loss: -119509.421875\n",
      "Train Epoch: 464 [12800/17352 (74%)] Loss: -143017.406250\n",
      "Train Epoch: 464 [14208/17352 (82%)] Loss: -177461.718750\n",
      "Train Epoch: 464 [15538/17352 (90%)] Loss: -131531.203125\n",
      "Train Epoch: 464 [16398/17352 (95%)] Loss: -121663.718750\n",
      "Train Epoch: 464 [17139/17352 (99%)] Loss: -52771.000000\n",
      "    epoch          : 464\n",
      "    loss           : -131445.46161585045\n",
      "    val_loss       : -74583.69968261718\n",
      "Train Epoch: 465 [128/17352 (1%)] Loss: -163299.843750\n",
      "Train Epoch: 465 [1536/17352 (9%)] Loss: -164395.328125\n",
      "Train Epoch: 465 [2944/17352 (17%)] Loss: -159537.609375\n",
      "Train Epoch: 465 [4352/17352 (25%)] Loss: -151672.140625\n",
      "Train Epoch: 465 [5760/17352 (33%)] Loss: -152282.218750\n",
      "Train Epoch: 465 [7168/17352 (41%)] Loss: -109480.234375\n",
      "Train Epoch: 465 [8576/17352 (49%)] Loss: -142869.171875\n",
      "Train Epoch: 465 [9984/17352 (58%)] Loss: -126578.640625\n",
      "Train Epoch: 465 [11392/17352 (66%)] Loss: -151092.500000\n",
      "Train Epoch: 465 [12800/17352 (74%)] Loss: -127880.937500\n",
      "Train Epoch: 465 [14208/17352 (82%)] Loss: -109106.859375\n",
      "Train Epoch: 465 [15515/17352 (89%)] Loss: -81071.968750\n",
      "Train Epoch: 465 [16407/17352 (95%)] Loss: -140125.656250\n",
      "Train Epoch: 465 [17018/17352 (98%)] Loss: -82324.546875\n",
      "    epoch          : 465\n",
      "    loss           : -129458.42493208303\n",
      "    val_loss       : -69440.63135172526\n",
      "Train Epoch: 466 [128/17352 (1%)] Loss: -158061.046875\n",
      "Train Epoch: 466 [1536/17352 (9%)] Loss: -131864.359375\n",
      "Train Epoch: 466 [2944/17352 (17%)] Loss: -182174.750000\n",
      "Train Epoch: 466 [4352/17352 (25%)] Loss: -156510.921875\n",
      "Train Epoch: 466 [5760/17352 (33%)] Loss: -158973.750000\n",
      "Train Epoch: 466 [7168/17352 (41%)] Loss: -160171.953125\n",
      "Train Epoch: 466 [8576/17352 (49%)] Loss: -136063.281250\n",
      "Train Epoch: 466 [9984/17352 (58%)] Loss: -127148.718750\n",
      "Train Epoch: 466 [11392/17352 (66%)] Loss: -71168.578125\n",
      "Train Epoch: 466 [12800/17352 (74%)] Loss: -141379.531250\n",
      "Train Epoch: 466 [14208/17352 (82%)] Loss: -180195.890625\n",
      "Train Epoch: 466 [15538/17352 (90%)] Loss: -97426.140625\n",
      "Train Epoch: 466 [16170/17352 (93%)] Loss: -20751.296875\n",
      "Train Epoch: 466 [16875/17352 (97%)] Loss: -87831.187500\n",
      "    epoch          : 466\n",
      "    loss           : -137393.27691773282\n",
      "    val_loss       : -76070.21420084636\n",
      "Train Epoch: 467 [128/17352 (1%)] Loss: -191241.687500\n",
      "Train Epoch: 467 [1536/17352 (9%)] Loss: -158634.250000\n",
      "Train Epoch: 467 [2944/17352 (17%)] Loss: -151195.468750\n",
      "Train Epoch: 467 [4352/17352 (25%)] Loss: -123447.460938\n",
      "Train Epoch: 467 [5760/17352 (33%)] Loss: -161276.500000\n",
      "Train Epoch: 467 [7168/17352 (41%)] Loss: -148586.937500\n",
      "Train Epoch: 467 [8576/17352 (49%)] Loss: -156422.156250\n",
      "Train Epoch: 467 [9984/17352 (58%)] Loss: -136019.531250\n",
      "Train Epoch: 467 [11392/17352 (66%)] Loss: -157845.906250\n",
      "Train Epoch: 467 [12800/17352 (74%)] Loss: -152415.515625\n",
      "Train Epoch: 467 [14208/17352 (82%)] Loss: -157725.968750\n",
      "Train Epoch: 467 [15429/17352 (89%)] Loss: -16533.250000\n",
      "Train Epoch: 467 [16286/17352 (94%)] Loss: -116584.625000\n",
      "Train Epoch: 467 [17074/17352 (98%)] Loss: -99054.835938\n",
      "    epoch          : 467\n",
      "    loss           : -132326.7795262689\n",
      "    val_loss       : -62472.29420166016\n",
      "Train Epoch: 468 [128/17352 (1%)] Loss: -110057.093750\n",
      "Train Epoch: 468 [1536/17352 (9%)] Loss: -71546.156250\n",
      "Train Epoch: 468 [2944/17352 (17%)] Loss: -138292.218750\n",
      "Train Epoch: 468 [4352/17352 (25%)] Loss: -133171.625000\n",
      "Train Epoch: 468 [5760/17352 (33%)] Loss: -173290.718750\n",
      "Train Epoch: 468 [7168/17352 (41%)] Loss: -142835.578125\n",
      "Train Epoch: 468 [8576/17352 (49%)] Loss: -165195.468750\n",
      "Train Epoch: 468 [9984/17352 (58%)] Loss: -146174.375000\n",
      "Train Epoch: 468 [11392/17352 (66%)] Loss: -152402.421875\n",
      "Train Epoch: 468 [12800/17352 (74%)] Loss: -124810.906250\n",
      "Train Epoch: 468 [14208/17352 (82%)] Loss: -169517.109375\n",
      "Train Epoch: 468 [15469/17352 (89%)] Loss: -98932.757812\n",
      "Train Epoch: 468 [16197/17352 (93%)] Loss: -97419.593750\n",
      "Train Epoch: 468 [16895/17352 (97%)] Loss: -61852.679688\n",
      "    epoch          : 468\n",
      "    loss           : -127714.27082404835\n",
      "    val_loss       : -59146.52646077474\n",
      "Train Epoch: 469 [128/17352 (1%)] Loss: -132050.171875\n",
      "Train Epoch: 469 [1536/17352 (9%)] Loss: -162355.875000\n",
      "Train Epoch: 469 [2944/17352 (17%)] Loss: -160270.828125\n",
      "Train Epoch: 469 [4352/17352 (25%)] Loss: -152557.593750\n",
      "Train Epoch: 469 [5760/17352 (33%)] Loss: -163586.781250\n",
      "Train Epoch: 469 [7168/17352 (41%)] Loss: -151247.796875\n",
      "Train Epoch: 469 [8576/17352 (49%)] Loss: -115193.765625\n",
      "Train Epoch: 469 [9984/17352 (58%)] Loss: -165893.468750\n",
      "Train Epoch: 469 [11392/17352 (66%)] Loss: -157852.375000\n",
      "Train Epoch: 469 [12800/17352 (74%)] Loss: -163022.140625\n",
      "Train Epoch: 469 [14208/17352 (82%)] Loss: -131908.843750\n",
      "Train Epoch: 469 [15455/17352 (89%)] Loss: -50478.019531\n",
      "Train Epoch: 469 [16124/17352 (93%)] Loss: -52983.632812\n",
      "Train Epoch: 469 [16967/17352 (98%)] Loss: -109290.273438\n",
      "    epoch          : 469\n",
      "    loss           : -132126.5188922242\n",
      "    val_loss       : -73350.5735148112\n",
      "Train Epoch: 470 [128/17352 (1%)] Loss: -156535.140625\n",
      "Train Epoch: 470 [1536/17352 (9%)] Loss: -154787.187500\n",
      "Train Epoch: 470 [2944/17352 (17%)] Loss: -152052.968750\n",
      "Train Epoch: 470 [4352/17352 (25%)] Loss: -171066.781250\n",
      "Train Epoch: 470 [5760/17352 (33%)] Loss: -160061.656250\n",
      "Train Epoch: 470 [7168/17352 (41%)] Loss: -141415.093750\n",
      "Train Epoch: 470 [8576/17352 (49%)] Loss: -139512.140625\n",
      "Train Epoch: 470 [9984/17352 (58%)] Loss: -153638.156250\n",
      "Train Epoch: 470 [11392/17352 (66%)] Loss: -169265.453125\n",
      "Train Epoch: 470 [12800/17352 (74%)] Loss: -166474.078125\n",
      "Train Epoch: 470 [14208/17352 (82%)] Loss: -149069.328125\n",
      "Train Epoch: 470 [15448/17352 (89%)] Loss: -4327.443848\n",
      "Train Epoch: 470 [16100/17352 (93%)] Loss: -107861.531250\n",
      "Train Epoch: 470 [16915/17352 (97%)] Loss: -99577.218750\n",
      "    epoch          : 470\n",
      "    loss           : -138772.0114467544\n",
      "    val_loss       : -75423.05603027344\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch470.pth ...\n",
      "Train Epoch: 471 [128/17352 (1%)] Loss: -149291.953125\n",
      "Train Epoch: 471 [1536/17352 (9%)] Loss: -167504.312500\n",
      "Train Epoch: 471 [2944/17352 (17%)] Loss: -104245.281250\n",
      "Train Epoch: 471 [4352/17352 (25%)] Loss: -112527.421875\n",
      "Train Epoch: 471 [5760/17352 (33%)] Loss: -165705.750000\n",
      "Train Epoch: 471 [7168/17352 (41%)] Loss: -152513.625000\n",
      "Train Epoch: 471 [8576/17352 (49%)] Loss: -153621.375000\n",
      "Train Epoch: 471 [9984/17352 (58%)] Loss: -176790.000000\n",
      "Train Epoch: 471 [11392/17352 (66%)] Loss: -128137.609375\n",
      "Train Epoch: 471 [12800/17352 (74%)] Loss: -141799.031250\n",
      "Train Epoch: 471 [14208/17352 (82%)] Loss: -131068.687500\n",
      "Train Epoch: 471 [15427/17352 (89%)] Loss: -33035.453125\n",
      "Train Epoch: 471 [16150/17352 (93%)] Loss: -75924.203125\n",
      "Train Epoch: 471 [16972/17352 (98%)] Loss: -41719.027344\n",
      "    epoch          : 471\n",
      "    loss           : -128085.56829547242\n",
      "    val_loss       : -74579.53289388021\n",
      "Train Epoch: 472 [128/17352 (1%)] Loss: -131655.578125\n",
      "Train Epoch: 472 [1536/17352 (9%)] Loss: -150915.921875\n",
      "Train Epoch: 472 [2944/17352 (17%)] Loss: -182793.203125\n",
      "Train Epoch: 472 [4352/17352 (25%)] Loss: -174480.031250\n",
      "Train Epoch: 472 [5760/17352 (33%)] Loss: -117295.281250\n",
      "Train Epoch: 472 [7168/17352 (41%)] Loss: -110312.765625\n",
      "Train Epoch: 472 [8576/17352 (49%)] Loss: -104306.500000\n",
      "Train Epoch: 472 [9984/17352 (58%)] Loss: -131713.078125\n",
      "Train Epoch: 472 [11392/17352 (66%)] Loss: -126118.203125\n",
      "Train Epoch: 472 [12800/17352 (74%)] Loss: -140266.015625\n",
      "Train Epoch: 472 [14208/17352 (82%)] Loss: -171110.718750\n",
      "Train Epoch: 472 [15561/17352 (90%)] Loss: -137426.250000\n",
      "Train Epoch: 472 [16392/17352 (94%)] Loss: -108243.500000\n",
      "Train Epoch: 472 [16970/17352 (98%)] Loss: -5589.276855\n",
      "    epoch          : 472\n",
      "    loss           : -132666.77143030358\n",
      "    val_loss       : -75322.77544352213\n",
      "Train Epoch: 473 [128/17352 (1%)] Loss: -165713.593750\n",
      "Train Epoch: 473 [1536/17352 (9%)] Loss: -160890.000000\n",
      "Train Epoch: 473 [2944/17352 (17%)] Loss: -177621.234375\n",
      "Train Epoch: 473 [4352/17352 (25%)] Loss: -168589.656250\n",
      "Train Epoch: 473 [5760/17352 (33%)] Loss: -179135.000000\n",
      "Train Epoch: 473 [7168/17352 (41%)] Loss: -25532.751953\n",
      "Train Epoch: 473 [8576/17352 (49%)] Loss: -95808.257812\n",
      "Train Epoch: 473 [9984/17352 (58%)] Loss: -127105.046875\n",
      "Train Epoch: 473 [11392/17352 (66%)] Loss: -171067.000000\n",
      "Train Epoch: 473 [12800/17352 (74%)] Loss: -184626.250000\n",
      "Train Epoch: 473 [14208/17352 (82%)] Loss: -146689.453125\n",
      "Train Epoch: 473 [15499/17352 (89%)] Loss: -126413.703125\n",
      "Train Epoch: 473 [16230/17352 (94%)] Loss: -62550.679688\n",
      "Train Epoch: 473 [16949/17352 (98%)] Loss: -75854.234375\n",
      "    epoch          : 473\n",
      "    loss           : -128893.04016850618\n",
      "    val_loss       : -74331.58833414713\n",
      "Train Epoch: 474 [128/17352 (1%)] Loss: -131242.578125\n",
      "Train Epoch: 474 [1536/17352 (9%)] Loss: -155013.515625\n",
      "Train Epoch: 474 [2944/17352 (17%)] Loss: -134327.484375\n",
      "Train Epoch: 474 [4352/17352 (25%)] Loss: -175165.093750\n",
      "Train Epoch: 474 [5760/17352 (33%)] Loss: -128550.703125\n",
      "Train Epoch: 474 [7168/17352 (41%)] Loss: -167979.906250\n",
      "Train Epoch: 474 [8576/17352 (49%)] Loss: -155402.093750\n",
      "Train Epoch: 474 [9984/17352 (58%)] Loss: -158455.453125\n",
      "Train Epoch: 474 [11392/17352 (66%)] Loss: -165340.468750\n",
      "Train Epoch: 474 [12800/17352 (74%)] Loss: -143855.609375\n",
      "Train Epoch: 474 [14208/17352 (82%)] Loss: -159006.234375\n",
      "Train Epoch: 474 [15542/17352 (90%)] Loss: -88945.093750\n",
      "Train Epoch: 474 [16310/17352 (94%)] Loss: -54247.867188\n",
      "Train Epoch: 474 [16997/17352 (98%)] Loss: -44846.234375\n",
      "    epoch          : 474\n",
      "    loss           : -137917.0073930369\n",
      "    val_loss       : -67231.96274007161\n",
      "Train Epoch: 475 [128/17352 (1%)] Loss: -140644.687500\n",
      "Train Epoch: 475 [1536/17352 (9%)] Loss: -157414.406250\n",
      "Train Epoch: 475 [2944/17352 (17%)] Loss: -140273.062500\n",
      "Train Epoch: 475 [4352/17352 (25%)] Loss: -152387.843750\n",
      "Train Epoch: 475 [5760/17352 (33%)] Loss: -175193.437500\n",
      "Train Epoch: 475 [7168/17352 (41%)] Loss: -142601.437500\n",
      "Train Epoch: 475 [8576/17352 (49%)] Loss: -96973.757812\n",
      "Train Epoch: 475 [9984/17352 (58%)] Loss: -142260.187500\n",
      "Train Epoch: 475 [11392/17352 (66%)] Loss: -133213.750000\n",
      "Train Epoch: 475 [12800/17352 (74%)] Loss: -120779.992188\n",
      "Train Epoch: 475 [14208/17352 (82%)] Loss: -182322.046875\n",
      "Train Epoch: 475 [15450/17352 (89%)] Loss: -13656.211914\n",
      "Train Epoch: 475 [16183/17352 (93%)] Loss: -100213.937500\n",
      "Train Epoch: 475 [17066/17352 (98%)] Loss: -133751.453125\n",
      "    epoch          : 475\n",
      "    loss           : -132334.58942756397\n",
      "    val_loss       : -57312.66791992188\n",
      "Train Epoch: 476 [128/17352 (1%)] Loss: -115196.609375\n",
      "Train Epoch: 476 [1536/17352 (9%)] Loss: -164536.062500\n",
      "Train Epoch: 476 [2944/17352 (17%)] Loss: -172936.312500\n",
      "Train Epoch: 476 [4352/17352 (25%)] Loss: -122632.625000\n",
      "Train Epoch: 476 [5760/17352 (33%)] Loss: -164008.718750\n",
      "Train Epoch: 476 [7168/17352 (41%)] Loss: -154560.062500\n",
      "Train Epoch: 476 [8576/17352 (49%)] Loss: -156483.937500\n",
      "Train Epoch: 476 [9984/17352 (58%)] Loss: -112232.484375\n",
      "Train Epoch: 476 [11392/17352 (66%)] Loss: -161377.437500\n",
      "Train Epoch: 476 [12800/17352 (74%)] Loss: -181247.109375\n",
      "Train Epoch: 476 [14208/17352 (82%)] Loss: -143889.656250\n",
      "Train Epoch: 476 [15487/17352 (89%)] Loss: -61212.644531\n",
      "Train Epoch: 476 [16345/17352 (94%)] Loss: -129008.296875\n",
      "Train Epoch: 476 [17068/17352 (98%)] Loss: -62090.386719\n",
      "    epoch          : 476\n",
      "    loss           : -131785.67810959785\n",
      "    val_loss       : -78887.04903157552\n",
      "Train Epoch: 477 [128/17352 (1%)] Loss: -179849.093750\n",
      "Train Epoch: 477 [1536/17352 (9%)] Loss: -189698.546875\n",
      "Train Epoch: 477 [2944/17352 (17%)] Loss: -127406.820312\n",
      "Train Epoch: 477 [4352/17352 (25%)] Loss: -125246.671875\n",
      "Train Epoch: 477 [5760/17352 (33%)] Loss: -133441.828125\n",
      "Train Epoch: 477 [7168/17352 (41%)] Loss: -161207.562500\n",
      "Train Epoch: 477 [8576/17352 (49%)] Loss: -153477.843750\n",
      "Train Epoch: 477 [9984/17352 (58%)] Loss: 13156.370117\n",
      "Train Epoch: 477 [11392/17352 (66%)] Loss: -41260.339844\n",
      "Train Epoch: 477 [12800/17352 (74%)] Loss: -96938.164062\n",
      "Train Epoch: 477 [14208/17352 (82%)] Loss: -124763.625000\n",
      "Train Epoch: 477 [15440/17352 (89%)] Loss: -84043.156250\n",
      "Train Epoch: 477 [16210/17352 (93%)] Loss: -45707.867188\n",
      "Train Epoch: 477 [17062/17352 (98%)] Loss: -49900.179688\n",
      "    epoch          : 477\n",
      "    loss           : -111585.00062919463\n",
      "    val_loss       : -65349.587654622395\n",
      "Train Epoch: 478 [128/17352 (1%)] Loss: -145884.109375\n",
      "Train Epoch: 478 [1536/17352 (9%)] Loss: -141484.375000\n",
      "Train Epoch: 478 [2944/17352 (17%)] Loss: -132337.125000\n",
      "Train Epoch: 478 [4352/17352 (25%)] Loss: -145651.937500\n",
      "Train Epoch: 478 [5760/17352 (33%)] Loss: -154445.609375\n",
      "Train Epoch: 478 [7168/17352 (41%)] Loss: -171582.218750\n",
      "Train Epoch: 478 [8576/17352 (49%)] Loss: -139550.578125\n",
      "Train Epoch: 478 [9984/17352 (58%)] Loss: -142252.250000\n",
      "Train Epoch: 478 [11392/17352 (66%)] Loss: -177463.187500\n",
      "Train Epoch: 478 [12800/17352 (74%)] Loss: -146228.500000\n",
      "Train Epoch: 478 [14208/17352 (82%)] Loss: -153999.734375\n",
      "Train Epoch: 478 [15368/17352 (89%)] Loss: -4492.349121\n",
      "Train Epoch: 478 [16282/17352 (94%)] Loss: -130107.140625\n",
      "Train Epoch: 478 [17127/17352 (99%)] Loss: -106950.242188\n",
      "    epoch          : 478\n",
      "    loss           : -133141.3217986446\n",
      "    val_loss       : -77239.10942382812\n",
      "Train Epoch: 479 [128/17352 (1%)] Loss: -168826.500000\n",
      "Train Epoch: 479 [1536/17352 (9%)] Loss: -180067.437500\n",
      "Train Epoch: 479 [2944/17352 (17%)] Loss: -149416.406250\n",
      "Train Epoch: 479 [4352/17352 (25%)] Loss: -176960.687500\n",
      "Train Epoch: 479 [5760/17352 (33%)] Loss: -168521.968750\n",
      "Train Epoch: 479 [7168/17352 (41%)] Loss: -179748.906250\n",
      "Train Epoch: 479 [8576/17352 (49%)] Loss: -155009.218750\n",
      "Train Epoch: 479 [9984/17352 (58%)] Loss: -141189.718750\n",
      "Train Epoch: 479 [11392/17352 (66%)] Loss: -143340.437500\n",
      "Train Epoch: 479 [12800/17352 (74%)] Loss: -147531.984375\n",
      "Train Epoch: 479 [14208/17352 (82%)] Loss: -169576.843750\n",
      "Train Epoch: 479 [15503/17352 (89%)] Loss: -35628.421875\n",
      "Train Epoch: 479 [16341/17352 (94%)] Loss: -110448.468750\n",
      "Train Epoch: 479 [17109/17352 (99%)] Loss: -101956.312500\n",
      "    epoch          : 479\n",
      "    loss           : -138912.1967576814\n",
      "    val_loss       : -72401.47915445964\n",
      "Train Epoch: 480 [128/17352 (1%)] Loss: -163480.031250\n",
      "Train Epoch: 480 [1536/17352 (9%)] Loss: -143312.000000\n",
      "Train Epoch: 480 [2944/17352 (17%)] Loss: -125182.531250\n",
      "Train Epoch: 480 [4352/17352 (25%)] Loss: -129584.335938\n",
      "Train Epoch: 480 [5760/17352 (33%)] Loss: -147120.078125\n",
      "Train Epoch: 480 [7168/17352 (41%)] Loss: -170958.515625\n",
      "Train Epoch: 480 [8576/17352 (49%)] Loss: -149372.593750\n",
      "Train Epoch: 480 [9984/17352 (58%)] Loss: -137920.625000\n",
      "Train Epoch: 480 [11392/17352 (66%)] Loss: -125748.546875\n",
      "Train Epoch: 480 [12800/17352 (74%)] Loss: -135768.671875\n",
      "Train Epoch: 480 [14208/17352 (82%)] Loss: -170471.343750\n",
      "Train Epoch: 480 [15458/17352 (89%)] Loss: -4058.662842\n",
      "Train Epoch: 480 [16164/17352 (93%)] Loss: -91967.281250\n",
      "Train Epoch: 480 [17007/17352 (98%)] Loss: -54196.929688\n",
      "    epoch          : 480\n",
      "    loss           : -133434.20660359427\n",
      "    val_loss       : -76335.29287109376\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch480.pth ...\n",
      "Train Epoch: 481 [128/17352 (1%)] Loss: -151967.437500\n",
      "Train Epoch: 481 [1536/17352 (9%)] Loss: -170185.234375\n",
      "Train Epoch: 481 [2944/17352 (17%)] Loss: -179984.406250\n",
      "Train Epoch: 481 [4352/17352 (25%)] Loss: -139806.906250\n",
      "Train Epoch: 481 [5760/17352 (33%)] Loss: -161908.515625\n",
      "Train Epoch: 481 [7168/17352 (41%)] Loss: -174097.718750\n",
      "Train Epoch: 481 [8576/17352 (49%)] Loss: -156221.500000\n",
      "Train Epoch: 481 [9984/17352 (58%)] Loss: -120617.867188\n",
      "Train Epoch: 481 [11392/17352 (66%)] Loss: -151826.562500\n",
      "Train Epoch: 481 [12800/17352 (74%)] Loss: -145615.171875\n",
      "Train Epoch: 481 [14208/17352 (82%)] Loss: -126947.031250\n",
      "Train Epoch: 481 [15538/17352 (90%)] Loss: -84295.093750\n",
      "Train Epoch: 481 [16279/17352 (94%)] Loss: -100078.210938\n",
      "Train Epoch: 481 [16994/17352 (98%)] Loss: -32677.542969\n",
      "    epoch          : 481\n",
      "    loss           : -132014.06046822568\n",
      "    val_loss       : -37647.910030110674\n",
      "Train Epoch: 482 [128/17352 (1%)] Loss: -92521.679688\n",
      "Train Epoch: 482 [1536/17352 (9%)] Loss: -84467.187500\n",
      "Train Epoch: 482 [2944/17352 (17%)] Loss: -118328.273438\n",
      "Train Epoch: 482 [4352/17352 (25%)] Loss: -100561.312500\n",
      "Train Epoch: 482 [5760/17352 (33%)] Loss: -133199.062500\n",
      "Train Epoch: 482 [7168/17352 (41%)] Loss: -147044.656250\n",
      "Train Epoch: 482 [8576/17352 (49%)] Loss: -98653.156250\n",
      "Train Epoch: 482 [9984/17352 (58%)] Loss: -147154.656250\n",
      "Train Epoch: 482 [11392/17352 (66%)] Loss: -139332.656250\n",
      "Train Epoch: 482 [12800/17352 (74%)] Loss: -128021.460938\n",
      "Train Epoch: 482 [14208/17352 (82%)] Loss: -181206.828125\n",
      "Train Epoch: 482 [15534/17352 (90%)] Loss: -115211.945312\n",
      "Train Epoch: 482 [16245/17352 (94%)] Loss: -99431.593750\n",
      "Train Epoch: 482 [17046/17352 (98%)] Loss: -65144.546875\n",
      "    epoch          : 482\n",
      "    loss           : -127514.55687657298\n",
      "    val_loss       : -59245.338928222656\n",
      "Train Epoch: 483 [128/17352 (1%)] Loss: -119774.429688\n",
      "Train Epoch: 483 [1536/17352 (9%)] Loss: -150207.750000\n",
      "Train Epoch: 483 [2944/17352 (17%)] Loss: -121046.648438\n",
      "Train Epoch: 483 [4352/17352 (25%)] Loss: -106007.882812\n",
      "Train Epoch: 483 [5760/17352 (33%)] Loss: -149439.468750\n",
      "Train Epoch: 483 [7168/17352 (41%)] Loss: -171308.734375\n",
      "Train Epoch: 483 [8576/17352 (49%)] Loss: -154774.250000\n",
      "Train Epoch: 483 [9984/17352 (58%)] Loss: -134320.593750\n",
      "Train Epoch: 483 [11392/17352 (66%)] Loss: -177494.687500\n",
      "Train Epoch: 483 [12800/17352 (74%)] Loss: -145669.828125\n",
      "Train Epoch: 483 [14208/17352 (82%)] Loss: -181753.281250\n",
      "Train Epoch: 483 [15534/17352 (90%)] Loss: -88666.125000\n",
      "Train Epoch: 483 [16354/17352 (94%)] Loss: -109120.757812\n",
      "Train Epoch: 483 [17119/17352 (99%)] Loss: -117645.593750\n",
      "    epoch          : 483\n",
      "    loss           : -132875.51206611787\n",
      "    val_loss       : -74306.69776611328\n",
      "Train Epoch: 484 [128/17352 (1%)] Loss: -144967.468750\n",
      "Train Epoch: 484 [1536/17352 (9%)] Loss: -168581.328125\n",
      "Train Epoch: 484 [2944/17352 (17%)] Loss: -179968.703125\n",
      "Train Epoch: 484 [4352/17352 (25%)] Loss: -138612.250000\n",
      "Train Epoch: 484 [5760/17352 (33%)] Loss: -165870.437500\n",
      "Train Epoch: 484 [7168/17352 (41%)] Loss: -171178.640625\n",
      "Train Epoch: 484 [8576/17352 (49%)] Loss: -123988.250000\n",
      "Train Epoch: 484 [9984/17352 (58%)] Loss: -182542.343750\n",
      "Train Epoch: 484 [11392/17352 (66%)] Loss: -174575.578125\n",
      "Train Epoch: 484 [12800/17352 (74%)] Loss: -149682.437500\n",
      "Train Epoch: 484 [14208/17352 (82%)] Loss: -151031.015625\n",
      "Train Epoch: 484 [15460/17352 (89%)] Loss: -90190.234375\n",
      "Train Epoch: 484 [16039/17352 (92%)] Loss: -26676.980469\n",
      "Train Epoch: 484 [16993/17352 (98%)] Loss: -96877.945312\n",
      "    epoch          : 484\n",
      "    loss           : -138770.99781420408\n",
      "    val_loss       : -70979.55697835286\n",
      "Train Epoch: 485 [128/17352 (1%)] Loss: -138458.765625\n",
      "Train Epoch: 485 [1536/17352 (9%)] Loss: -143259.843750\n",
      "Train Epoch: 485 [2944/17352 (17%)] Loss: -160944.609375\n",
      "Train Epoch: 485 [4352/17352 (25%)] Loss: -136769.828125\n",
      "Train Epoch: 485 [5760/17352 (33%)] Loss: -165163.484375\n",
      "Train Epoch: 485 [7168/17352 (41%)] Loss: -152511.265625\n",
      "Train Epoch: 485 [8576/17352 (49%)] Loss: -163250.812500\n",
      "Train Epoch: 485 [9984/17352 (58%)] Loss: -159886.218750\n",
      "Train Epoch: 485 [11392/17352 (66%)] Loss: -175553.921875\n",
      "Train Epoch: 485 [12800/17352 (74%)] Loss: -128110.179688\n",
      "Train Epoch: 485 [14208/17352 (82%)] Loss: -121354.453125\n",
      "Train Epoch: 485 [15435/17352 (89%)] Loss: -34521.718750\n",
      "Train Epoch: 485 [16228/17352 (94%)] Loss: -99388.468750\n",
      "Train Epoch: 485 [17000/17352 (98%)] Loss: -97659.968750\n",
      "    epoch          : 485\n",
      "    loss           : -131289.82734669934\n",
      "    val_loss       : -74067.62918701171\n",
      "Train Epoch: 486 [128/17352 (1%)] Loss: -126400.046875\n",
      "Train Epoch: 486 [1536/17352 (9%)] Loss: -139118.421875\n",
      "Train Epoch: 486 [2944/17352 (17%)] Loss: -154477.062500\n",
      "Train Epoch: 486 [4352/17352 (25%)] Loss: -164965.984375\n",
      "Train Epoch: 486 [5760/17352 (33%)] Loss: -154563.125000\n",
      "Train Epoch: 486 [7168/17352 (41%)] Loss: -113670.156250\n",
      "Train Epoch: 486 [8576/17352 (49%)] Loss: -158226.656250\n",
      "Train Epoch: 486 [9984/17352 (58%)] Loss: -161533.500000\n",
      "Train Epoch: 486 [11392/17352 (66%)] Loss: -147165.453125\n",
      "Train Epoch: 486 [12800/17352 (74%)] Loss: -165646.281250\n",
      "Train Epoch: 486 [14208/17352 (82%)] Loss: -138309.515625\n",
      "Train Epoch: 486 [15516/17352 (89%)] Loss: -42659.738281\n",
      "Train Epoch: 486 [16130/17352 (93%)] Loss: -118506.093750\n",
      "Train Epoch: 486 [16934/17352 (98%)] Loss: -56780.093750\n",
      "    epoch          : 486\n",
      "    loss           : -138268.49711946832\n",
      "    val_loss       : -73786.8073404948\n",
      "Train Epoch: 487 [128/17352 (1%)] Loss: -164400.187500\n",
      "Train Epoch: 487 [1536/17352 (9%)] Loss: -167847.515625\n",
      "Train Epoch: 487 [2944/17352 (17%)] Loss: -173756.671875\n",
      "Train Epoch: 487 [4352/17352 (25%)] Loss: -133881.828125\n",
      "Train Epoch: 487 [5760/17352 (33%)] Loss: -141613.968750\n",
      "Train Epoch: 487 [7168/17352 (41%)] Loss: -116371.460938\n",
      "Train Epoch: 487 [8576/17352 (49%)] Loss: -141446.343750\n",
      "Train Epoch: 487 [9984/17352 (58%)] Loss: -143387.671875\n",
      "Train Epoch: 487 [11392/17352 (66%)] Loss: -110508.226562\n",
      "Train Epoch: 487 [12800/17352 (74%)] Loss: -132856.828125\n",
      "Train Epoch: 487 [14208/17352 (82%)] Loss: -139978.234375\n",
      "Train Epoch: 487 [15509/17352 (89%)] Loss: -48653.046875\n",
      "Train Epoch: 487 [16279/17352 (94%)] Loss: -20081.550781\n",
      "Train Epoch: 487 [17003/17352 (98%)] Loss: -6421.651367\n",
      "    epoch          : 487\n",
      "    loss           : -134522.50619363465\n",
      "    val_loss       : -77563.59075113932\n",
      "Train Epoch: 488 [128/17352 (1%)] Loss: -160722.906250\n",
      "Train Epoch: 488 [1536/17352 (9%)] Loss: -164045.640625\n",
      "Train Epoch: 488 [2944/17352 (17%)] Loss: -144129.812500\n",
      "Train Epoch: 488 [4352/17352 (25%)] Loss: -129517.812500\n",
      "Train Epoch: 488 [5760/17352 (33%)] Loss: -90196.453125\n",
      "Train Epoch: 488 [7168/17352 (41%)] Loss: -112760.406250\n",
      "Train Epoch: 488 [8576/17352 (49%)] Loss: -152976.812500\n",
      "Train Epoch: 488 [9984/17352 (58%)] Loss: -165560.531250\n",
      "Train Epoch: 488 [11392/17352 (66%)] Loss: -152370.250000\n",
      "Train Epoch: 488 [12800/17352 (74%)] Loss: -151921.437500\n",
      "Train Epoch: 488 [14208/17352 (82%)] Loss: -150124.640625\n",
      "Train Epoch: 488 [15469/17352 (89%)] Loss: -35105.156250\n",
      "Train Epoch: 488 [16257/17352 (94%)] Loss: -52418.886719\n",
      "Train Epoch: 488 [17042/17352 (98%)] Loss: -155262.921875\n",
      "    epoch          : 488\n",
      "    loss           : -133142.13172451762\n",
      "    val_loss       : -63542.95743001302\n",
      "Train Epoch: 489 [128/17352 (1%)] Loss: -123490.640625\n",
      "Train Epoch: 489 [1536/17352 (9%)] Loss: -150600.640625\n",
      "Train Epoch: 489 [2944/17352 (17%)] Loss: -103444.007812\n",
      "Train Epoch: 489 [4352/17352 (25%)] Loss: -151833.218750\n",
      "Train Epoch: 489 [5760/17352 (33%)] Loss: -163660.562500\n",
      "Train Epoch: 489 [7168/17352 (41%)] Loss: -171602.718750\n",
      "Train Epoch: 489 [8576/17352 (49%)] Loss: -135072.328125\n",
      "Train Epoch: 489 [9984/17352 (58%)] Loss: -158867.359375\n",
      "Train Epoch: 489 [11392/17352 (66%)] Loss: -168795.140625\n",
      "Train Epoch: 489 [12800/17352 (74%)] Loss: -183277.093750\n",
      "Train Epoch: 489 [14208/17352 (82%)] Loss: -134382.468750\n",
      "Train Epoch: 489 [15461/17352 (89%)] Loss: -106879.125000\n",
      "Train Epoch: 489 [16148/17352 (93%)] Loss: -88331.609375\n",
      "Train Epoch: 489 [17002/17352 (98%)] Loss: -87946.937500\n",
      "    epoch          : 489\n",
      "    loss           : -139767.04268528472\n",
      "    val_loss       : -78577.35054524739\n",
      "Train Epoch: 490 [128/17352 (1%)] Loss: -173359.718750\n",
      "Train Epoch: 490 [1536/17352 (9%)] Loss: -162155.312500\n",
      "Train Epoch: 490 [2944/17352 (17%)] Loss: -118613.468750\n",
      "Train Epoch: 490 [4352/17352 (25%)] Loss: -92439.101562\n",
      "Train Epoch: 490 [5760/17352 (33%)] Loss: -117145.312500\n",
      "Train Epoch: 490 [7168/17352 (41%)] Loss: -164839.531250\n",
      "Train Epoch: 490 [8576/17352 (49%)] Loss: -179392.593750\n",
      "Train Epoch: 490 [9984/17352 (58%)] Loss: -139677.531250\n",
      "Train Epoch: 490 [11392/17352 (66%)] Loss: -167727.875000\n",
      "Train Epoch: 490 [12800/17352 (74%)] Loss: -171236.140625\n",
      "Train Epoch: 490 [14208/17352 (82%)] Loss: -168505.843750\n",
      "Train Epoch: 490 [15564/17352 (90%)] Loss: -120693.343750\n",
      "Train Epoch: 490 [16382/17352 (94%)] Loss: -95608.296875\n",
      "Train Epoch: 490 [17168/17352 (99%)] Loss: -100627.320312\n",
      "    epoch          : 490\n",
      "    loss           : -127964.07454809407\n",
      "    val_loss       : -76566.3044108073\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch490.pth ...\n",
      "Train Epoch: 491 [128/17352 (1%)] Loss: -148685.296875\n",
      "Train Epoch: 491 [1536/17352 (9%)] Loss: -116447.242188\n",
      "Train Epoch: 491 [2944/17352 (17%)] Loss: -115211.859375\n",
      "Train Epoch: 491 [4352/17352 (25%)] Loss: -138011.296875\n",
      "Train Epoch: 491 [5760/17352 (33%)] Loss: -166852.265625\n",
      "Train Epoch: 491 [7168/17352 (41%)] Loss: -176588.765625\n",
      "Train Epoch: 491 [8576/17352 (49%)] Loss: -149717.250000\n",
      "Train Epoch: 491 [9984/17352 (58%)] Loss: -170779.140625\n",
      "Train Epoch: 491 [11392/17352 (66%)] Loss: -139313.875000\n",
      "Train Epoch: 491 [12800/17352 (74%)] Loss: -181914.375000\n",
      "Train Epoch: 491 [14208/17352 (82%)] Loss: -164440.828125\n",
      "Train Epoch: 491 [15457/17352 (89%)] Loss: -13183.212891\n",
      "Train Epoch: 491 [16126/17352 (93%)] Loss: -81637.718750\n",
      "Train Epoch: 491 [17025/17352 (98%)] Loss: -59828.523438\n",
      "    epoch          : 491\n",
      "    loss           : -138640.99315423134\n",
      "    val_loss       : -79480.39594319662\n",
      "Train Epoch: 492 [128/17352 (1%)] Loss: -193875.625000\n",
      "Train Epoch: 492 [1536/17352 (9%)] Loss: -166377.500000\n",
      "Train Epoch: 492 [2944/17352 (17%)] Loss: -147023.796875\n",
      "Train Epoch: 492 [4352/17352 (25%)] Loss: -180015.843750\n",
      "Train Epoch: 492 [5760/17352 (33%)] Loss: -157864.687500\n",
      "Train Epoch: 492 [7168/17352 (41%)] Loss: -114419.460938\n",
      "Train Epoch: 492 [8576/17352 (49%)] Loss: -161160.296875\n",
      "Train Epoch: 492 [9984/17352 (58%)] Loss: -154715.718750\n",
      "Train Epoch: 492 [11392/17352 (66%)] Loss: -144905.843750\n",
      "Train Epoch: 492 [12800/17352 (74%)] Loss: -125856.687500\n",
      "Train Epoch: 492 [14208/17352 (82%)] Loss: -161357.046875\n",
      "Train Epoch: 492 [15498/17352 (89%)] Loss: -78985.007812\n",
      "Train Epoch: 492 [16331/17352 (94%)] Loss: -74987.195312\n",
      "Train Epoch: 492 [17058/17352 (98%)] Loss: -11646.997070\n",
      "    epoch          : 492\n",
      "    loss           : -132540.82306686504\n",
      "    val_loss       : -68913.00393473307\n",
      "Train Epoch: 493 [128/17352 (1%)] Loss: -109086.515625\n",
      "Train Epoch: 493 [1536/17352 (9%)] Loss: -158443.625000\n",
      "Train Epoch: 493 [2944/17352 (17%)] Loss: -132127.250000\n",
      "Train Epoch: 493 [4352/17352 (25%)] Loss: -151802.296875\n",
      "Train Epoch: 493 [5760/17352 (33%)] Loss: -124638.015625\n",
      "Train Epoch: 493 [7168/17352 (41%)] Loss: -84688.960938\n",
      "Train Epoch: 493 [8576/17352 (49%)] Loss: -124408.335938\n",
      "Train Epoch: 493 [9984/17352 (58%)] Loss: -166968.937500\n",
      "Train Epoch: 493 [11392/17352 (66%)] Loss: -175415.593750\n",
      "Train Epoch: 493 [12800/17352 (74%)] Loss: -134547.375000\n",
      "Train Epoch: 493 [14208/17352 (82%)] Loss: -147153.609375\n",
      "Train Epoch: 493 [15546/17352 (90%)] Loss: -127646.468750\n",
      "Train Epoch: 493 [16363/17352 (94%)] Loss: -39102.125000\n",
      "Train Epoch: 493 [17043/17352 (98%)] Loss: -42324.492188\n",
      "    epoch          : 493\n",
      "    loss           : -132562.77590839975\n",
      "    val_loss       : -73526.2880900065\n",
      "Train Epoch: 494 [128/17352 (1%)] Loss: -145502.578125\n",
      "Train Epoch: 494 [1536/17352 (9%)] Loss: -144925.640625\n",
      "Train Epoch: 494 [2944/17352 (17%)] Loss: -124665.320312\n",
      "Train Epoch: 494 [4352/17352 (25%)] Loss: -119832.625000\n",
      "Train Epoch: 494 [5760/17352 (33%)] Loss: -149438.718750\n",
      "Train Epoch: 494 [7168/17352 (41%)] Loss: -148654.687500\n",
      "Train Epoch: 494 [8576/17352 (49%)] Loss: -148718.984375\n",
      "Train Epoch: 494 [9984/17352 (58%)] Loss: -141958.187500\n",
      "Train Epoch: 494 [11392/17352 (66%)] Loss: -166001.875000\n",
      "Train Epoch: 494 [12800/17352 (74%)] Loss: -116811.929688\n",
      "Train Epoch: 494 [14208/17352 (82%)] Loss: -160933.203125\n",
      "Train Epoch: 494 [15546/17352 (90%)] Loss: -108416.804688\n",
      "Train Epoch: 494 [16351/17352 (94%)] Loss: -16844.419922\n",
      "Train Epoch: 494 [17055/17352 (98%)] Loss: -88223.085938\n",
      "    epoch          : 494\n",
      "    loss           : -133899.99539246014\n",
      "    val_loss       : -53405.11410725911\n",
      "Train Epoch: 495 [128/17352 (1%)] Loss: -108120.671875\n",
      "Train Epoch: 495 [1536/17352 (9%)] Loss: -144915.718750\n",
      "Train Epoch: 495 [2944/17352 (17%)] Loss: -124552.539062\n",
      "Train Epoch: 495 [4352/17352 (25%)] Loss: -152639.312500\n",
      "Train Epoch: 495 [5760/17352 (33%)] Loss: -125062.164062\n",
      "Train Epoch: 495 [7168/17352 (41%)] Loss: -144453.125000\n",
      "Train Epoch: 495 [8576/17352 (49%)] Loss: -140174.343750\n",
      "Train Epoch: 495 [9984/17352 (58%)] Loss: -176295.390625\n",
      "Train Epoch: 495 [11392/17352 (66%)] Loss: -163319.906250\n",
      "Train Epoch: 495 [12800/17352 (74%)] Loss: -153636.156250\n",
      "Train Epoch: 495 [14208/17352 (82%)] Loss: -157846.109375\n",
      "Train Epoch: 495 [15508/17352 (89%)] Loss: -122958.507812\n",
      "Train Epoch: 495 [16356/17352 (94%)] Loss: -115137.015625\n",
      "Train Epoch: 495 [17023/17352 (98%)] Loss: -63614.914062\n",
      "    epoch          : 495\n",
      "    loss           : -130498.707090237\n",
      "    val_loss       : -62273.1826944987\n",
      "Train Epoch: 496 [128/17352 (1%)] Loss: -151537.484375\n",
      "Train Epoch: 496 [1536/17352 (9%)] Loss: -147796.093750\n",
      "Train Epoch: 496 [2944/17352 (17%)] Loss: -128020.718750\n",
      "Train Epoch: 496 [4352/17352 (25%)] Loss: -161041.343750\n",
      "Train Epoch: 496 [5760/17352 (33%)] Loss: -132785.656250\n",
      "Train Epoch: 496 [7168/17352 (41%)] Loss: -128103.367188\n",
      "Train Epoch: 496 [8576/17352 (49%)] Loss: -167127.203125\n",
      "Train Epoch: 496 [9984/17352 (58%)] Loss: -142465.515625\n",
      "Train Epoch: 496 [11392/17352 (66%)] Loss: -153187.984375\n",
      "Train Epoch: 496 [12800/17352 (74%)] Loss: -194552.250000\n",
      "Train Epoch: 496 [14208/17352 (82%)] Loss: -168845.781250\n",
      "Train Epoch: 496 [15453/17352 (89%)] Loss: -20428.599609\n",
      "Train Epoch: 496 [16389/17352 (94%)] Loss: -106641.492188\n",
      "Train Epoch: 496 [16971/17352 (98%)] Loss: -3615.376709\n",
      "    epoch          : 496\n",
      "    loss           : -134953.02090597473\n",
      "    val_loss       : -55273.25158284505\n",
      "Train Epoch: 497 [128/17352 (1%)] Loss: -126977.484375\n",
      "Train Epoch: 497 [1536/17352 (9%)] Loss: -167096.562500\n",
      "Train Epoch: 497 [2944/17352 (17%)] Loss: -147374.781250\n",
      "Train Epoch: 497 [4352/17352 (25%)] Loss: -144855.343750\n",
      "Train Epoch: 497 [5760/17352 (33%)] Loss: -164273.828125\n",
      "Train Epoch: 497 [7168/17352 (41%)] Loss: -166313.125000\n",
      "Train Epoch: 497 [8576/17352 (49%)] Loss: -120666.492188\n",
      "Train Epoch: 497 [9984/17352 (58%)] Loss: -88196.078125\n",
      "Train Epoch: 497 [11392/17352 (66%)] Loss: -116819.562500\n",
      "Train Epoch: 497 [12800/17352 (74%)] Loss: -136014.687500\n",
      "Train Epoch: 497 [14208/17352 (82%)] Loss: -152770.468750\n",
      "Train Epoch: 497 [15512/17352 (89%)] Loss: -106009.984375\n",
      "Train Epoch: 497 [16290/17352 (94%)] Loss: -98735.328125\n",
      "Train Epoch: 497 [17135/17352 (99%)] Loss: -111897.437500\n",
      "    epoch          : 497\n",
      "    loss           : -128458.38154100251\n",
      "    val_loss       : -75255.88326009114\n",
      "Train Epoch: 498 [128/17352 (1%)] Loss: -163479.343750\n",
      "Train Epoch: 498 [1536/17352 (9%)] Loss: -147764.468750\n",
      "Train Epoch: 498 [2944/17352 (17%)] Loss: -135834.937500\n",
      "Train Epoch: 498 [4352/17352 (25%)] Loss: -170256.500000\n",
      "Train Epoch: 498 [5760/17352 (33%)] Loss: -161683.593750\n",
      "Train Epoch: 498 [7168/17352 (41%)] Loss: -151365.812500\n",
      "Train Epoch: 498 [8576/17352 (49%)] Loss: -191268.000000\n",
      "Train Epoch: 498 [9984/17352 (58%)] Loss: -154527.828125\n",
      "Train Epoch: 498 [11392/17352 (66%)] Loss: -115587.687500\n",
      "Train Epoch: 498 [12800/17352 (74%)] Loss: -149000.156250\n",
      "Train Epoch: 498 [14208/17352 (82%)] Loss: -130712.351562\n",
      "Train Epoch: 498 [15442/17352 (89%)] Loss: -6118.725098\n",
      "Train Epoch: 498 [16244/17352 (94%)] Loss: -48742.226562\n",
      "Train Epoch: 498 [16970/17352 (98%)] Loss: -41079.261719\n",
      "    epoch          : 498\n",
      "    loss           : -139564.56861170827\n",
      "    val_loss       : -74988.8719889323\n",
      "Train Epoch: 499 [128/17352 (1%)] Loss: -147924.781250\n",
      "Train Epoch: 499 [1536/17352 (9%)] Loss: -141984.156250\n",
      "Train Epoch: 499 [2944/17352 (17%)] Loss: -160590.031250\n",
      "Train Epoch: 499 [4352/17352 (25%)] Loss: -115636.937500\n",
      "Train Epoch: 499 [5760/17352 (33%)] Loss: -179183.515625\n",
      "Train Epoch: 499 [7168/17352 (41%)] Loss: -162939.468750\n",
      "Train Epoch: 499 [8576/17352 (49%)] Loss: -166784.031250\n",
      "Train Epoch: 499 [9984/17352 (58%)] Loss: -104457.382812\n",
      "Train Epoch: 499 [11392/17352 (66%)] Loss: -71578.945312\n",
      "Train Epoch: 499 [12800/17352 (74%)] Loss: -154594.562500\n",
      "Train Epoch: 499 [14208/17352 (82%)] Loss: -145644.453125\n",
      "Train Epoch: 499 [15451/17352 (89%)] Loss: -6607.770020\n",
      "Train Epoch: 499 [16075/17352 (93%)] Loss: -95916.945312\n",
      "Train Epoch: 499 [16955/17352 (98%)] Loss: -132162.859375\n",
      "    epoch          : 499\n",
      "    loss           : -129689.2914416422\n",
      "    val_loss       : -73360.44083658855\n",
      "Train Epoch: 500 [128/17352 (1%)] Loss: -159664.937500\n",
      "Train Epoch: 500 [1536/17352 (9%)] Loss: -135935.406250\n",
      "Train Epoch: 500 [2944/17352 (17%)] Loss: -125255.484375\n",
      "Train Epoch: 500 [4352/17352 (25%)] Loss: -163983.796875\n",
      "Train Epoch: 500 [5760/17352 (33%)] Loss: -157622.296875\n",
      "Train Epoch: 500 [7168/17352 (41%)] Loss: -145784.750000\n",
      "Train Epoch: 500 [8576/17352 (49%)] Loss: -162319.546875\n",
      "Train Epoch: 500 [9984/17352 (58%)] Loss: -133095.890625\n",
      "Train Epoch: 500 [11392/17352 (66%)] Loss: -165743.250000\n",
      "Train Epoch: 500 [12800/17352 (74%)] Loss: -171469.078125\n",
      "Train Epoch: 500 [14208/17352 (82%)] Loss: -122136.343750\n",
      "Train Epoch: 500 [15417/17352 (89%)] Loss: -4406.868652\n",
      "Train Epoch: 500 [16330/17352 (94%)] Loss: -106899.523438\n",
      "Train Epoch: 500 [17113/17352 (99%)] Loss: 41706.984375\n",
      "    epoch          : 500\n",
      "    loss           : -131998.9389652534\n",
      "    val_loss       : 17350.460127766928\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch500.pth ...\n",
      "Train Epoch: 501 [128/17352 (1%)] Loss: 38939.957031\n",
      "Train Epoch: 501 [1536/17352 (9%)] Loss: -75175.437500\n",
      "Train Epoch: 501 [2944/17352 (17%)] Loss: -100939.070312\n",
      "Train Epoch: 501 [4352/17352 (25%)] Loss: -126164.515625\n",
      "Train Epoch: 501 [5760/17352 (33%)] Loss: -121616.390625\n",
      "Train Epoch: 501 [7168/17352 (41%)] Loss: -154370.953125\n",
      "Train Epoch: 501 [8576/17352 (49%)] Loss: -156571.328125\n",
      "Train Epoch: 501 [9984/17352 (58%)] Loss: -176971.671875\n",
      "Train Epoch: 501 [11392/17352 (66%)] Loss: -155499.187500\n",
      "Train Epoch: 501 [12800/17352 (74%)] Loss: -163746.718750\n",
      "Train Epoch: 501 [14208/17352 (82%)] Loss: -140570.234375\n",
      "Train Epoch: 501 [15499/17352 (89%)] Loss: -49196.742188\n",
      "Train Epoch: 501 [16183/17352 (93%)] Loss: -6282.467773\n",
      "Train Epoch: 501 [17128/17352 (99%)] Loss: -41376.589844\n",
      "    epoch          : 501\n",
      "    loss           : -113059.56771707215\n",
      "    val_loss       : -75559.30275065104\n",
      "Train Epoch: 502 [128/17352 (1%)] Loss: -143520.281250\n",
      "Train Epoch: 502 [1536/17352 (9%)] Loss: -151612.218750\n",
      "Train Epoch: 502 [2944/17352 (17%)] Loss: -175995.062500\n",
      "Train Epoch: 502 [4352/17352 (25%)] Loss: -154698.265625\n",
      "Train Epoch: 502 [5760/17352 (33%)] Loss: -134682.500000\n",
      "Train Epoch: 502 [7168/17352 (41%)] Loss: -146204.625000\n",
      "Train Epoch: 502 [8576/17352 (49%)] Loss: -173902.703125\n",
      "Train Epoch: 502 [9984/17352 (58%)] Loss: -146207.375000\n",
      "Train Epoch: 502 [11392/17352 (66%)] Loss: -159553.765625\n",
      "Train Epoch: 502 [12800/17352 (74%)] Loss: -154829.421875\n",
      "Train Epoch: 502 [14208/17352 (82%)] Loss: -130115.953125\n",
      "Train Epoch: 502 [15459/17352 (89%)] Loss: -16643.882812\n",
      "Train Epoch: 502 [16297/17352 (94%)] Loss: -122973.312500\n",
      "Train Epoch: 502 [16922/17352 (98%)] Loss: -33610.878906\n",
      "    epoch          : 502\n",
      "    loss           : -137650.88385788066\n",
      "    val_loss       : -35238.15189615885\n",
      "Train Epoch: 503 [128/17352 (1%)] Loss: -59951.398438\n",
      "Train Epoch: 503 [1536/17352 (9%)] Loss: -120485.710938\n",
      "Train Epoch: 503 [2944/17352 (17%)] Loss: -165990.953125\n",
      "Train Epoch: 503 [4352/17352 (25%)] Loss: -170237.453125\n",
      "Train Epoch: 503 [5760/17352 (33%)] Loss: -172853.625000\n",
      "Train Epoch: 503 [7168/17352 (41%)] Loss: -159968.125000\n",
      "Train Epoch: 503 [8576/17352 (49%)] Loss: -172832.625000\n",
      "Train Epoch: 503 [9984/17352 (58%)] Loss: -175797.781250\n",
      "Train Epoch: 503 [11392/17352 (66%)] Loss: -169869.781250\n",
      "Train Epoch: 503 [12800/17352 (74%)] Loss: -119872.906250\n",
      "Train Epoch: 503 [14208/17352 (82%)] Loss: -168801.593750\n",
      "Train Epoch: 503 [15547/17352 (90%)] Loss: -120597.593750\n",
      "Train Epoch: 503 [16111/17352 (93%)] Loss: -3699.420166\n",
      "Train Epoch: 503 [16977/17352 (98%)] Loss: -111225.945312\n",
      "    epoch          : 503\n",
      "    loss           : -133253.56526747326\n",
      "    val_loss       : -76597.04650472006\n",
      "Train Epoch: 504 [128/17352 (1%)] Loss: -103976.437500\n",
      "Train Epoch: 504 [1536/17352 (9%)] Loss: -151390.437500\n",
      "Train Epoch: 504 [2944/17352 (17%)] Loss: -159722.921875\n",
      "Train Epoch: 504 [4352/17352 (25%)] Loss: -156349.046875\n",
      "Train Epoch: 504 [5760/17352 (33%)] Loss: -157253.375000\n",
      "Train Epoch: 504 [7168/17352 (41%)] Loss: -169073.890625\n",
      "Train Epoch: 504 [8576/17352 (49%)] Loss: -143237.250000\n",
      "Train Epoch: 504 [9984/17352 (58%)] Loss: -177577.828125\n",
      "Train Epoch: 504 [11392/17352 (66%)] Loss: -145105.281250\n",
      "Train Epoch: 504 [12800/17352 (74%)] Loss: -149574.234375\n",
      "Train Epoch: 504 [14208/17352 (82%)] Loss: -164661.328125\n",
      "Train Epoch: 504 [15436/17352 (89%)] Loss: -88463.843750\n",
      "Train Epoch: 504 [16171/17352 (93%)] Loss: -22143.882812\n",
      "Train Epoch: 504 [17072/17352 (98%)] Loss: -146419.296875\n",
      "    epoch          : 504\n",
      "    loss           : -141320.94796691486\n",
      "    val_loss       : -72431.47975260417\n",
      "Train Epoch: 505 [128/17352 (1%)] Loss: -154858.812500\n",
      "Train Epoch: 505 [1536/17352 (9%)] Loss: -157617.140625\n",
      "Train Epoch: 505 [2944/17352 (17%)] Loss: -175977.343750\n",
      "Train Epoch: 505 [4352/17352 (25%)] Loss: -176578.203125\n",
      "Train Epoch: 505 [5760/17352 (33%)] Loss: -148070.359375\n",
      "Train Epoch: 505 [7168/17352 (41%)] Loss: -170489.312500\n",
      "Train Epoch: 505 [8576/17352 (49%)] Loss: -194105.765625\n",
      "Train Epoch: 505 [9984/17352 (58%)] Loss: -155765.812500\n",
      "Train Epoch: 505 [11392/17352 (66%)] Loss: -135404.406250\n",
      "Train Epoch: 505 [12800/17352 (74%)] Loss: -163428.671875\n",
      "Train Epoch: 505 [14208/17352 (82%)] Loss: -150446.937500\n",
      "Train Epoch: 505 [15551/17352 (90%)] Loss: -129102.867188\n",
      "Train Epoch: 505 [16383/17352 (94%)] Loss: -133210.687500\n",
      "Train Epoch: 505 [17112/17352 (99%)] Loss: -106610.031250\n",
      "    epoch          : 505\n",
      "    loss           : -139497.20040012847\n",
      "    val_loss       : -70244.0985514323\n",
      "Train Epoch: 506 [128/17352 (1%)] Loss: -140160.906250\n",
      "Train Epoch: 506 [1536/17352 (9%)] Loss: -146507.375000\n",
      "Train Epoch: 506 [2944/17352 (17%)] Loss: -159262.343750\n",
      "Train Epoch: 506 [4352/17352 (25%)] Loss: -129805.890625\n",
      "Train Epoch: 506 [5760/17352 (33%)] Loss: -160466.343750\n",
      "Train Epoch: 506 [7168/17352 (41%)] Loss: -156991.875000\n",
      "Train Epoch: 506 [8576/17352 (49%)] Loss: -128393.921875\n",
      "Train Epoch: 506 [9984/17352 (58%)] Loss: -106799.187500\n",
      "Train Epoch: 506 [11392/17352 (66%)] Loss: -145833.656250\n",
      "Train Epoch: 506 [12800/17352 (74%)] Loss: -140938.656250\n",
      "Train Epoch: 506 [14208/17352 (82%)] Loss: -160601.015625\n",
      "Train Epoch: 506 [15522/17352 (89%)] Loss: -108599.507812\n",
      "Train Epoch: 506 [16148/17352 (93%)] Loss: -111788.226562\n",
      "Train Epoch: 506 [16930/17352 (98%)] Loss: -112910.492188\n",
      "    epoch          : 506\n",
      "    loss           : -132990.2507045669\n",
      "    val_loss       : -78847.6382446289\n",
      "Train Epoch: 507 [128/17352 (1%)] Loss: -133597.984375\n",
      "Train Epoch: 507 [1536/17352 (9%)] Loss: -138079.125000\n",
      "Train Epoch: 507 [2944/17352 (17%)] Loss: -148304.375000\n",
      "Train Epoch: 507 [4352/17352 (25%)] Loss: -143510.703125\n",
      "Train Epoch: 507 [5760/17352 (33%)] Loss: -166711.296875\n",
      "Train Epoch: 507 [7168/17352 (41%)] Loss: -176419.218750\n",
      "Train Epoch: 507 [8576/17352 (49%)] Loss: -163009.781250\n",
      "Train Epoch: 507 [9984/17352 (58%)] Loss: -142294.437500\n",
      "Train Epoch: 507 [11392/17352 (66%)] Loss: -128849.382812\n",
      "Train Epoch: 507 [12800/17352 (74%)] Loss: -118908.734375\n",
      "Train Epoch: 507 [14208/17352 (82%)] Loss: -120816.804688\n",
      "Train Epoch: 507 [15531/17352 (90%)] Loss: -93230.093750\n",
      "Train Epoch: 507 [16334/17352 (94%)] Loss: -5757.945801\n",
      "Train Epoch: 507 [17137/17352 (99%)] Loss: -113070.632812\n",
      "    epoch          : 507\n",
      "    loss           : -132766.8022428167\n",
      "    val_loss       : -64985.33825683594\n",
      "Train Epoch: 508 [128/17352 (1%)] Loss: -145702.468750\n",
      "Train Epoch: 508 [1536/17352 (9%)] Loss: -143601.906250\n",
      "Train Epoch: 508 [2944/17352 (17%)] Loss: -153013.906250\n",
      "Train Epoch: 508 [4352/17352 (25%)] Loss: -161792.390625\n",
      "Train Epoch: 508 [5760/17352 (33%)] Loss: -159454.000000\n",
      "Train Epoch: 508 [7168/17352 (41%)] Loss: -128175.578125\n",
      "Train Epoch: 508 [8576/17352 (49%)] Loss: -155574.781250\n",
      "Train Epoch: 508 [9984/17352 (58%)] Loss: -132817.250000\n",
      "Train Epoch: 508 [11392/17352 (66%)] Loss: -154366.296875\n",
      "Train Epoch: 508 [12800/17352 (74%)] Loss: -127071.734375\n",
      "Train Epoch: 508 [14208/17352 (82%)] Loss: -191185.671875\n",
      "Train Epoch: 508 [15509/17352 (89%)] Loss: -44825.031250\n",
      "Train Epoch: 508 [16270/17352 (94%)] Loss: -91331.671875\n",
      "Train Epoch: 508 [16996/17352 (98%)] Loss: -101289.500000\n",
      "    epoch          : 508\n",
      "    loss           : -130348.10715151794\n",
      "    val_loss       : -36464.40832519531\n",
      "Train Epoch: 509 [128/17352 (1%)] Loss: -72455.195312\n",
      "Train Epoch: 509 [1536/17352 (9%)] Loss: -147048.203125\n",
      "Train Epoch: 509 [2944/17352 (17%)] Loss: -149594.515625\n",
      "Train Epoch: 509 [4352/17352 (25%)] Loss: -142512.468750\n",
      "Train Epoch: 509 [5760/17352 (33%)] Loss: -152658.578125\n",
      "Train Epoch: 509 [7168/17352 (41%)] Loss: -144277.468750\n",
      "Train Epoch: 509 [8576/17352 (49%)] Loss: -140430.640625\n",
      "Train Epoch: 509 [9984/17352 (58%)] Loss: -114026.648438\n",
      "Train Epoch: 509 [11392/17352 (66%)] Loss: -163413.890625\n",
      "Train Epoch: 509 [12800/17352 (74%)] Loss: -155846.687500\n",
      "Train Epoch: 509 [14208/17352 (82%)] Loss: -189020.843750\n",
      "Train Epoch: 509 [15551/17352 (90%)] Loss: -96929.750000\n",
      "Train Epoch: 509 [16213/17352 (93%)] Loss: -119853.710938\n",
      "Train Epoch: 509 [16945/17352 (98%)] Loss: -94243.125000\n",
      "    epoch          : 509\n",
      "    loss           : -138442.22399001152\n",
      "    val_loss       : -75081.18686523437\n",
      "Train Epoch: 510 [128/17352 (1%)] Loss: -191915.718750\n",
      "Train Epoch: 510 [1536/17352 (9%)] Loss: -135303.312500\n",
      "Train Epoch: 510 [2944/17352 (17%)] Loss: -120036.742188\n",
      "Train Epoch: 510 [4352/17352 (25%)] Loss: -172864.281250\n",
      "Train Epoch: 510 [5760/17352 (33%)] Loss: -164226.062500\n",
      "Train Epoch: 510 [7168/17352 (41%)] Loss: -135335.781250\n",
      "Train Epoch: 510 [8576/17352 (49%)] Loss: -120365.656250\n",
      "Train Epoch: 510 [9984/17352 (58%)] Loss: -128032.640625\n",
      "Train Epoch: 510 [11392/17352 (66%)] Loss: -132625.593750\n",
      "Train Epoch: 510 [12800/17352 (74%)] Loss: -154323.468750\n",
      "Train Epoch: 510 [14208/17352 (82%)] Loss: -159860.625000\n",
      "Train Epoch: 510 [15457/17352 (89%)] Loss: -104914.609375\n",
      "Train Epoch: 510 [16335/17352 (94%)] Loss: -105490.148438\n",
      "Train Epoch: 510 [16950/17352 (98%)] Loss: -58138.078125\n",
      "    epoch          : 510\n",
      "    loss           : -132042.12120516988\n",
      "    val_loss       : -76993.14748128255\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch510.pth ...\n",
      "Train Epoch: 511 [128/17352 (1%)] Loss: -173928.843750\n",
      "Train Epoch: 511 [1536/17352 (9%)] Loss: -166698.578125\n",
      "Train Epoch: 511 [2944/17352 (17%)] Loss: -166372.781250\n",
      "Train Epoch: 511 [4352/17352 (25%)] Loss: -163616.593750\n",
      "Train Epoch: 511 [5760/17352 (33%)] Loss: -178321.015625\n",
      "Train Epoch: 511 [7168/17352 (41%)] Loss: -157672.359375\n",
      "Train Epoch: 511 [8576/17352 (49%)] Loss: -165498.093750\n",
      "Train Epoch: 511 [9984/17352 (58%)] Loss: -123912.046875\n",
      "Train Epoch: 511 [11392/17352 (66%)] Loss: -105844.593750\n",
      "Train Epoch: 511 [12800/17352 (74%)] Loss: -84513.484375\n",
      "Train Epoch: 511 [14208/17352 (82%)] Loss: -157423.937500\n",
      "Train Epoch: 511 [15571/17352 (90%)] Loss: -80605.304688\n",
      "Train Epoch: 511 [16168/17352 (93%)] Loss: -120546.109375\n",
      "Train Epoch: 511 [16943/17352 (98%)] Loss: -83427.609375\n",
      "    epoch          : 511\n",
      "    loss           : -137476.51944604656\n",
      "    val_loss       : -78283.21184895834\n",
      "Train Epoch: 512 [128/17352 (1%)] Loss: -159749.406250\n",
      "Train Epoch: 512 [1536/17352 (9%)] Loss: -164233.781250\n",
      "Train Epoch: 512 [2944/17352 (17%)] Loss: -141358.843750\n",
      "Train Epoch: 512 [4352/17352 (25%)] Loss: -156156.578125\n",
      "Train Epoch: 512 [5760/17352 (33%)] Loss: -157352.625000\n",
      "Train Epoch: 512 [7168/17352 (41%)] Loss: -153445.531250\n",
      "Train Epoch: 512 [8576/17352 (49%)] Loss: -134951.828125\n",
      "Train Epoch: 512 [9984/17352 (58%)] Loss: -139232.859375\n",
      "Train Epoch: 512 [11392/17352 (66%)] Loss: -107644.421875\n",
      "Train Epoch: 512 [12800/17352 (74%)] Loss: -109650.945312\n",
      "Train Epoch: 512 [14208/17352 (82%)] Loss: -119617.226562\n",
      "Train Epoch: 512 [15508/17352 (89%)] Loss: -44366.421875\n",
      "Train Epoch: 512 [16093/17352 (93%)] Loss: -16054.771484\n",
      "Train Epoch: 512 [17086/17352 (98%)] Loss: -88969.609375\n",
      "    epoch          : 512\n",
      "    loss           : -134840.49360646497\n",
      "    val_loss       : -66214.71201171874\n",
      "Train Epoch: 513 [128/17352 (1%)] Loss: -139315.265625\n",
      "Train Epoch: 513 [1536/17352 (9%)] Loss: -144133.125000\n",
      "Train Epoch: 513 [2944/17352 (17%)] Loss: -169298.765625\n",
      "Train Epoch: 513 [4352/17352 (25%)] Loss: -146859.312500\n",
      "Train Epoch: 513 [5760/17352 (33%)] Loss: -133676.750000\n",
      "Train Epoch: 513 [7168/17352 (41%)] Loss: -159501.937500\n",
      "Train Epoch: 513 [8576/17352 (49%)] Loss: -159242.234375\n",
      "Train Epoch: 513 [9984/17352 (58%)] Loss: -141333.828125\n",
      "Train Epoch: 513 [11392/17352 (66%)] Loss: -165278.203125\n",
      "Train Epoch: 513 [12800/17352 (74%)] Loss: -149838.531250\n",
      "Train Epoch: 513 [14208/17352 (82%)] Loss: -160506.203125\n",
      "Train Epoch: 513 [15482/17352 (89%)] Loss: -20424.857422\n",
      "Train Epoch: 513 [16351/17352 (94%)] Loss: -120676.867188\n",
      "Train Epoch: 513 [17060/17352 (98%)] Loss: -58542.460938\n",
      "    epoch          : 513\n",
      "    loss           : -138531.86177380453\n",
      "    val_loss       : -71012.44510498046\n",
      "Train Epoch: 514 [128/17352 (1%)] Loss: -133933.843750\n",
      "Train Epoch: 514 [1536/17352 (9%)] Loss: -151486.750000\n",
      "Train Epoch: 514 [2944/17352 (17%)] Loss: -178567.640625\n",
      "Train Epoch: 514 [4352/17352 (25%)] Loss: -137467.843750\n",
      "Train Epoch: 514 [5760/17352 (33%)] Loss: -174572.218750\n",
      "Train Epoch: 514 [7168/17352 (41%)] Loss: -160627.781250\n",
      "Train Epoch: 514 [8576/17352 (49%)] Loss: -133826.531250\n",
      "Train Epoch: 514 [9984/17352 (58%)] Loss: -169332.515625\n",
      "Train Epoch: 514 [11392/17352 (66%)] Loss: -123143.437500\n",
      "Train Epoch: 514 [12800/17352 (74%)] Loss: -171267.203125\n",
      "Train Epoch: 514 [14208/17352 (82%)] Loss: -157469.765625\n",
      "Train Epoch: 514 [15538/17352 (90%)] Loss: -83126.390625\n",
      "Train Epoch: 514 [16348/17352 (94%)] Loss: -2197.607422\n",
      "Train Epoch: 514 [17026/17352 (98%)] Loss: -28744.648438\n",
      "    epoch          : 514\n",
      "    loss           : -126354.59817074769\n",
      "    val_loss       : -50439.70865478516\n",
      "Train Epoch: 515 [128/17352 (1%)] Loss: -96997.203125\n",
      "Train Epoch: 515 [1536/17352 (9%)] Loss: -151602.125000\n",
      "Train Epoch: 515 [2944/17352 (17%)] Loss: -140109.062500\n",
      "Train Epoch: 515 [4352/17352 (25%)] Loss: -175527.859375\n",
      "Train Epoch: 515 [5760/17352 (33%)] Loss: -142275.546875\n",
      "Train Epoch: 515 [7168/17352 (41%)] Loss: -151621.437500\n",
      "Train Epoch: 515 [8576/17352 (49%)] Loss: -151522.515625\n",
      "Train Epoch: 515 [9984/17352 (58%)] Loss: -183390.390625\n",
      "Train Epoch: 515 [11392/17352 (66%)] Loss: -151211.781250\n",
      "Train Epoch: 515 [12800/17352 (74%)] Loss: -161718.375000\n",
      "Train Epoch: 515 [14208/17352 (82%)] Loss: -176016.640625\n",
      "Train Epoch: 515 [15517/17352 (89%)] Loss: -129261.804688\n",
      "Train Epoch: 515 [16105/17352 (93%)] Loss: -91311.468750\n",
      "Train Epoch: 515 [16964/17352 (98%)] Loss: -113069.898438\n",
      "    epoch          : 515\n",
      "    loss           : -135307.55974891203\n",
      "    val_loss       : -39269.08030192057\n",
      "Train Epoch: 516 [128/17352 (1%)] Loss: -74882.023438\n",
      "Train Epoch: 516 [1536/17352 (9%)] Loss: -130088.765625\n",
      "Train Epoch: 516 [2944/17352 (17%)] Loss: -117325.875000\n",
      "Train Epoch: 516 [4352/17352 (25%)] Loss: -156441.406250\n",
      "Train Epoch: 516 [5760/17352 (33%)] Loss: -196996.312500\n",
      "Train Epoch: 516 [7168/17352 (41%)] Loss: -171456.953125\n",
      "Train Epoch: 516 [8576/17352 (49%)] Loss: -138816.562500\n",
      "Train Epoch: 516 [9984/17352 (58%)] Loss: -142430.500000\n",
      "Train Epoch: 516 [11392/17352 (66%)] Loss: -166544.656250\n",
      "Train Epoch: 516 [12800/17352 (74%)] Loss: -121006.203125\n",
      "Train Epoch: 516 [14208/17352 (82%)] Loss: -143497.203125\n",
      "Train Epoch: 516 [15564/17352 (90%)] Loss: -138635.546875\n",
      "Train Epoch: 516 [16333/17352 (94%)] Loss: -42150.683594\n",
      "Train Epoch: 516 [16970/17352 (98%)] Loss: -51846.039062\n",
      "    epoch          : 516\n",
      "    loss           : -132598.6516342675\n",
      "    val_loss       : -67654.78317871093\n",
      "Train Epoch: 517 [128/17352 (1%)] Loss: -136179.937500\n",
      "Train Epoch: 517 [1536/17352 (9%)] Loss: -169107.468750\n",
      "Train Epoch: 517 [2944/17352 (17%)] Loss: -116744.687500\n",
      "Train Epoch: 517 [4352/17352 (25%)] Loss: -150109.875000\n",
      "Train Epoch: 517 [5760/17352 (33%)] Loss: -122452.312500\n",
      "Train Epoch: 517 [7168/17352 (41%)] Loss: -161920.906250\n",
      "Train Epoch: 517 [8576/17352 (49%)] Loss: -138258.031250\n",
      "Train Epoch: 517 [9984/17352 (58%)] Loss: -157739.781250\n",
      "Train Epoch: 517 [11392/17352 (66%)] Loss: -155312.718750\n",
      "Train Epoch: 517 [12800/17352 (74%)] Loss: -136012.890625\n",
      "Train Epoch: 517 [14208/17352 (82%)] Loss: -160094.531250\n",
      "Train Epoch: 517 [15448/17352 (89%)] Loss: -62101.808594\n",
      "Train Epoch: 517 [16321/17352 (94%)] Loss: -102557.898438\n",
      "Train Epoch: 517 [17060/17352 (98%)] Loss: -81329.515625\n",
      "    epoch          : 517\n",
      "    loss           : -139092.59060402686\n",
      "    val_loss       : -79526.69995524088\n",
      "Train Epoch: 518 [128/17352 (1%)] Loss: -164428.937500\n",
      "Train Epoch: 518 [1536/17352 (9%)] Loss: -145246.562500\n",
      "Train Epoch: 518 [2944/17352 (17%)] Loss: -180975.484375\n",
      "Train Epoch: 518 [4352/17352 (25%)] Loss: -172637.328125\n",
      "Train Epoch: 518 [5760/17352 (33%)] Loss: -132053.796875\n",
      "Train Epoch: 518 [7168/17352 (41%)] Loss: -134018.156250\n",
      "Train Epoch: 518 [8576/17352 (49%)] Loss: -149297.062500\n",
      "Train Epoch: 518 [9984/17352 (58%)] Loss: -141048.109375\n",
      "Train Epoch: 518 [11392/17352 (66%)] Loss: -146099.265625\n",
      "Train Epoch: 518 [12800/17352 (74%)] Loss: -146352.390625\n",
      "Train Epoch: 518 [14208/17352 (82%)] Loss: -156923.187500\n",
      "Train Epoch: 518 [15482/17352 (89%)] Loss: -44336.148438\n",
      "Train Epoch: 518 [16334/17352 (94%)] Loss: -4280.219238\n",
      "Train Epoch: 518 [17008/17352 (98%)] Loss: -16878.031250\n",
      "    epoch          : 518\n",
      "    loss           : -138724.889677931\n",
      "    val_loss       : -78549.47417399088\n",
      "Train Epoch: 519 [128/17352 (1%)] Loss: -164801.234375\n",
      "Train Epoch: 519 [1536/17352 (9%)] Loss: -148809.062500\n",
      "Train Epoch: 519 [2944/17352 (17%)] Loss: -156783.390625\n",
      "Train Epoch: 519 [4352/17352 (25%)] Loss: -125730.742188\n",
      "Train Epoch: 519 [5760/17352 (33%)] Loss: -123459.656250\n",
      "Train Epoch: 519 [7168/17352 (41%)] Loss: -135802.281250\n",
      "Train Epoch: 519 [8576/17352 (49%)] Loss: -127293.585938\n",
      "Train Epoch: 519 [9984/17352 (58%)] Loss: -147808.265625\n",
      "Train Epoch: 519 [11392/17352 (66%)] Loss: -130298.453125\n",
      "Train Epoch: 519 [12800/17352 (74%)] Loss: -178173.953125\n",
      "Train Epoch: 519 [14208/17352 (82%)] Loss: -156815.203125\n",
      "Train Epoch: 519 [15417/17352 (89%)] Loss: -24852.093750\n",
      "Train Epoch: 519 [16248/17352 (94%)] Loss: -98157.304688\n",
      "Train Epoch: 519 [16838/17352 (97%)] Loss: -85879.531250\n",
      "    epoch          : 519\n",
      "    loss           : -129247.52612140834\n",
      "    val_loss       : -69147.74569498698\n",
      "Train Epoch: 520 [128/17352 (1%)] Loss: -156651.156250\n",
      "Train Epoch: 520 [1536/17352 (9%)] Loss: -127000.171875\n",
      "Train Epoch: 520 [2944/17352 (17%)] Loss: -159828.718750\n",
      "Train Epoch: 520 [4352/17352 (25%)] Loss: -149084.093750\n",
      "Train Epoch: 520 [5760/17352 (33%)] Loss: -149731.265625\n",
      "Train Epoch: 520 [7168/17352 (41%)] Loss: -152251.031250\n",
      "Train Epoch: 520 [8576/17352 (49%)] Loss: -144617.406250\n",
      "Train Epoch: 520 [9984/17352 (58%)] Loss: -106672.062500\n",
      "Train Epoch: 520 [11392/17352 (66%)] Loss: -129789.828125\n",
      "Train Epoch: 520 [12800/17352 (74%)] Loss: -152717.765625\n",
      "Train Epoch: 520 [14208/17352 (82%)] Loss: -159659.250000\n",
      "Train Epoch: 520 [15497/17352 (89%)] Loss: -45825.144531\n",
      "Train Epoch: 520 [16272/17352 (94%)] Loss: -83367.500000\n",
      "Train Epoch: 520 [16902/17352 (97%)] Loss: -112354.695312\n",
      "    epoch          : 520\n",
      "    loss           : -136626.3588473941\n",
      "    val_loss       : -77394.26627604167\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch520.pth ...\n",
      "Train Epoch: 521 [128/17352 (1%)] Loss: -165967.750000\n",
      "Train Epoch: 521 [1536/17352 (9%)] Loss: -152147.125000\n",
      "Train Epoch: 521 [2944/17352 (17%)] Loss: -150698.453125\n",
      "Train Epoch: 521 [4352/17352 (25%)] Loss: -139106.093750\n",
      "Train Epoch: 521 [5760/17352 (33%)] Loss: -118891.742188\n",
      "Train Epoch: 521 [7168/17352 (41%)] Loss: -156396.796875\n",
      "Train Epoch: 521 [8576/17352 (49%)] Loss: -184088.906250\n",
      "Train Epoch: 521 [9984/17352 (58%)] Loss: -148797.156250\n",
      "Train Epoch: 521 [11392/17352 (66%)] Loss: -124199.031250\n",
      "Train Epoch: 521 [12800/17352 (74%)] Loss: -137342.031250\n",
      "Train Epoch: 521 [14208/17352 (82%)] Loss: -131575.578125\n",
      "Train Epoch: 521 [15498/17352 (89%)] Loss: -86868.648438\n",
      "Train Epoch: 521 [16368/17352 (94%)] Loss: -109194.382812\n",
      "Train Epoch: 521 [17129/17352 (99%)] Loss: -100410.992188\n",
      "    epoch          : 521\n",
      "    loss           : -131753.5938384805\n",
      "    val_loss       : -69245.09620361328\n",
      "Train Epoch: 522 [128/17352 (1%)] Loss: -116668.531250\n",
      "Train Epoch: 522 [1536/17352 (9%)] Loss: -146317.687500\n",
      "Train Epoch: 522 [2944/17352 (17%)] Loss: -143765.328125\n",
      "Train Epoch: 522 [4352/17352 (25%)] Loss: -129858.906250\n",
      "Train Epoch: 522 [5760/17352 (33%)] Loss: -149667.359375\n",
      "Train Epoch: 522 [7168/17352 (41%)] Loss: -149784.593750\n",
      "Train Epoch: 522 [8576/17352 (49%)] Loss: -166865.171875\n",
      "Train Epoch: 522 [9984/17352 (58%)] Loss: -170857.781250\n",
      "Train Epoch: 522 [11392/17352 (66%)] Loss: -151344.953125\n",
      "Train Epoch: 522 [12800/17352 (74%)] Loss: -174701.937500\n",
      "Train Epoch: 522 [14208/17352 (82%)] Loss: -171218.734375\n",
      "Train Epoch: 522 [15463/17352 (89%)] Loss: -107624.679688\n",
      "Train Epoch: 522 [16244/17352 (94%)] Loss: -93241.531250\n",
      "Train Epoch: 522 [17020/17352 (98%)] Loss: -78002.023438\n",
      "    epoch          : 522\n",
      "    loss           : -139583.43409841653\n",
      "    val_loss       : -58238.373547363284\n",
      "Train Epoch: 523 [128/17352 (1%)] Loss: -93060.757812\n",
      "Train Epoch: 523 [1536/17352 (9%)] Loss: -102079.140625\n",
      "Train Epoch: 523 [2944/17352 (17%)] Loss: -139720.125000\n",
      "Train Epoch: 523 [4352/17352 (25%)] Loss: -137112.656250\n",
      "Train Epoch: 523 [5760/17352 (33%)] Loss: -161172.640625\n",
      "Train Epoch: 523 [7168/17352 (41%)] Loss: -148780.250000\n",
      "Train Epoch: 523 [8576/17352 (49%)] Loss: -165767.671875\n",
      "Train Epoch: 523 [9984/17352 (58%)] Loss: -177347.515625\n",
      "Train Epoch: 523 [11392/17352 (66%)] Loss: -170962.343750\n",
      "Train Epoch: 523 [12800/17352 (74%)] Loss: -174679.546875\n",
      "Train Epoch: 523 [14208/17352 (82%)] Loss: -179749.875000\n",
      "Train Epoch: 523 [15534/17352 (90%)] Loss: -95960.328125\n",
      "Train Epoch: 523 [16129/17352 (93%)] Loss: -16622.546875\n",
      "Train Epoch: 523 [16898/17352 (97%)] Loss: -53482.046875\n",
      "    epoch          : 523\n",
      "    loss           : -135036.81764497692\n",
      "    val_loss       : -73224.60010579428\n",
      "Train Epoch: 524 [128/17352 (1%)] Loss: -140643.421875\n",
      "Train Epoch: 524 [1536/17352 (9%)] Loss: -172247.031250\n",
      "Train Epoch: 524 [2944/17352 (17%)] Loss: -142447.921875\n",
      "Train Epoch: 524 [4352/17352 (25%)] Loss: -129400.867188\n",
      "Train Epoch: 524 [5760/17352 (33%)] Loss: -166223.156250\n",
      "Train Epoch: 524 [7168/17352 (41%)] Loss: -141976.187500\n",
      "Train Epoch: 524 [8576/17352 (49%)] Loss: -136325.015625\n",
      "Train Epoch: 524 [9984/17352 (58%)] Loss: -103541.703125\n",
      "Train Epoch: 524 [11392/17352 (66%)] Loss: -121346.046875\n",
      "Train Epoch: 524 [12800/17352 (74%)] Loss: -135140.421875\n",
      "Train Epoch: 524 [14208/17352 (82%)] Loss: -144036.156250\n",
      "Train Epoch: 524 [15451/17352 (89%)] Loss: -46604.652344\n",
      "Train Epoch: 524 [16287/17352 (94%)] Loss: -67686.054688\n",
      "Train Epoch: 524 [16965/17352 (98%)] Loss: -51751.031250\n",
      "    epoch          : 524\n",
      "    loss           : -135177.4554205773\n",
      "    val_loss       : -80527.21088460287\n",
      "Train Epoch: 525 [128/17352 (1%)] Loss: -151870.156250\n",
      "Train Epoch: 525 [1536/17352 (9%)] Loss: -183596.093750\n",
      "Train Epoch: 525 [2944/17352 (17%)] Loss: -163342.343750\n",
      "Train Epoch: 525 [4352/17352 (25%)] Loss: -157942.968750\n",
      "Train Epoch: 525 [5760/17352 (33%)] Loss: -165225.015625\n",
      "Train Epoch: 525 [7168/17352 (41%)] Loss: -182053.562500\n",
      "Train Epoch: 525 [8576/17352 (49%)] Loss: -131026.250000\n",
      "Train Epoch: 525 [9984/17352 (58%)] Loss: -145564.625000\n",
      "Train Epoch: 525 [11392/17352 (66%)] Loss: -150711.765625\n",
      "Train Epoch: 525 [12800/17352 (74%)] Loss: -166747.656250\n",
      "Train Epoch: 525 [14208/17352 (82%)] Loss: -162447.343750\n",
      "Train Epoch: 525 [15577/17352 (90%)] Loss: -110869.023438\n",
      "Train Epoch: 525 [16506/17352 (95%)] Loss: -116258.835938\n",
      "Train Epoch: 525 [17097/17352 (99%)] Loss: -55651.281250\n",
      "    epoch          : 525\n",
      "    loss           : -144093.71973311662\n",
      "    val_loss       : -77709.14280598958\n",
      "Train Epoch: 526 [128/17352 (1%)] Loss: -148321.906250\n",
      "Train Epoch: 526 [1536/17352 (9%)] Loss: -102957.414062\n",
      "Train Epoch: 526 [2944/17352 (17%)] Loss: -128380.281250\n",
      "Train Epoch: 526 [4352/17352 (25%)] Loss: -158679.687500\n",
      "Train Epoch: 526 [5760/17352 (33%)] Loss: -141760.234375\n",
      "Train Epoch: 526 [7168/17352 (41%)] Loss: -172909.984375\n",
      "Train Epoch: 526 [8576/17352 (49%)] Loss: -167724.093750\n",
      "Train Epoch: 526 [9984/17352 (58%)] Loss: -107790.015625\n",
      "Train Epoch: 526 [11392/17352 (66%)] Loss: -152386.468750\n",
      "Train Epoch: 526 [12800/17352 (74%)] Loss: -48133.722656\n",
      "Train Epoch: 526 [14208/17352 (82%)] Loss: -104350.804688\n",
      "Train Epoch: 526 [15544/17352 (90%)] Loss: -49052.871094\n",
      "Train Epoch: 526 [16347/17352 (94%)] Loss: -43507.546875\n",
      "Train Epoch: 526 [17082/17352 (98%)] Loss: -3564.632324\n",
      "    epoch          : 526\n",
      "    loss           : -122691.09406787436\n",
      "    val_loss       : -56756.44126383463\n",
      "Train Epoch: 527 [128/17352 (1%)] Loss: -118399.742188\n",
      "Train Epoch: 527 [1536/17352 (9%)] Loss: -129194.882812\n",
      "Train Epoch: 527 [2944/17352 (17%)] Loss: -158167.734375\n",
      "Train Epoch: 527 [4352/17352 (25%)] Loss: -164905.171875\n",
      "Train Epoch: 527 [5760/17352 (33%)] Loss: -173213.062500\n",
      "Train Epoch: 527 [7168/17352 (41%)] Loss: -136786.843750\n",
      "Train Epoch: 527 [8576/17352 (49%)] Loss: -160490.093750\n",
      "Train Epoch: 527 [9984/17352 (58%)] Loss: -151420.781250\n",
      "Train Epoch: 527 [11392/17352 (66%)] Loss: -133103.234375\n",
      "Train Epoch: 527 [12800/17352 (74%)] Loss: -177261.890625\n",
      "Train Epoch: 527 [14208/17352 (82%)] Loss: -146800.750000\n",
      "Train Epoch: 527 [15427/17352 (89%)] Loss: -62511.710938\n",
      "Train Epoch: 527 [16024/17352 (92%)] Loss: -76948.500000\n",
      "Train Epoch: 527 [16976/17352 (98%)] Loss: -166252.000000\n",
      "    epoch          : 527\n",
      "    loss           : -139723.71457503145\n",
      "    val_loss       : -80654.16158447266\n",
      "Train Epoch: 528 [128/17352 (1%)] Loss: -140157.531250\n",
      "Train Epoch: 528 [1536/17352 (9%)] Loss: -174426.484375\n",
      "Train Epoch: 528 [2944/17352 (17%)] Loss: -166015.609375\n",
      "Train Epoch: 528 [4352/17352 (25%)] Loss: -174257.125000\n",
      "Train Epoch: 528 [5760/17352 (33%)] Loss: -168492.234375\n",
      "Train Epoch: 528 [7168/17352 (41%)] Loss: -143058.656250\n",
      "Train Epoch: 528 [8576/17352 (49%)] Loss: -139773.578125\n",
      "Train Epoch: 528 [9984/17352 (58%)] Loss: -179514.875000\n",
      "Train Epoch: 528 [11392/17352 (66%)] Loss: -115619.812500\n",
      "Train Epoch: 528 [12800/17352 (74%)] Loss: -160903.140625\n",
      "Train Epoch: 528 [14208/17352 (82%)] Loss: -163335.703125\n",
      "Train Epoch: 528 [15453/17352 (89%)] Loss: -17548.156250\n",
      "Train Epoch: 528 [16314/17352 (94%)] Loss: -80637.585938\n",
      "Train Epoch: 528 [16957/17352 (98%)] Loss: -6061.396484\n",
      "    epoch          : 528\n",
      "    loss           : -140079.4732788905\n",
      "    val_loss       : -77676.31263834635\n",
      "Train Epoch: 529 [128/17352 (1%)] Loss: -171193.218750\n",
      "Train Epoch: 529 [1536/17352 (9%)] Loss: -158409.687500\n",
      "Train Epoch: 529 [2944/17352 (17%)] Loss: -171648.734375\n",
      "Train Epoch: 529 [4352/17352 (25%)] Loss: -146038.437500\n",
      "Train Epoch: 529 [5760/17352 (33%)] Loss: -173766.250000\n",
      "Train Epoch: 529 [7168/17352 (41%)] Loss: -158016.015625\n",
      "Train Epoch: 529 [8576/17352 (49%)] Loss: -152472.265625\n",
      "Train Epoch: 529 [9984/17352 (58%)] Loss: -139526.093750\n",
      "Train Epoch: 529 [11392/17352 (66%)] Loss: -151324.187500\n",
      "Train Epoch: 529 [12800/17352 (74%)] Loss: -154005.078125\n",
      "Train Epoch: 529 [14208/17352 (82%)] Loss: -176288.734375\n",
      "Train Epoch: 529 [15455/17352 (89%)] Loss: -89230.171875\n",
      "Train Epoch: 529 [16170/17352 (93%)] Loss: -92166.500000\n",
      "Train Epoch: 529 [16894/17352 (97%)] Loss: -72164.218750\n",
      "    epoch          : 529\n",
      "    loss           : -132944.4773489933\n",
      "    val_loss       : -70350.20647379557\n",
      "Train Epoch: 530 [128/17352 (1%)] Loss: -141052.937500\n",
      "Train Epoch: 530 [1536/17352 (9%)] Loss: -132169.281250\n",
      "Train Epoch: 530 [2944/17352 (17%)] Loss: -129811.859375\n",
      "Train Epoch: 530 [4352/17352 (25%)] Loss: -162779.359375\n",
      "Train Epoch: 530 [5760/17352 (33%)] Loss: -122535.203125\n",
      "Train Epoch: 530 [7168/17352 (41%)] Loss: -149784.156250\n",
      "Train Epoch: 530 [8576/17352 (49%)] Loss: -141785.343750\n",
      "Train Epoch: 530 [9984/17352 (58%)] Loss: -158243.437500\n",
      "Train Epoch: 530 [11392/17352 (66%)] Loss: -165130.156250\n",
      "Train Epoch: 530 [12800/17352 (74%)] Loss: -155445.140625\n",
      "Train Epoch: 530 [14208/17352 (82%)] Loss: -146705.500000\n",
      "Train Epoch: 530 [15479/17352 (89%)] Loss: -42772.335938\n",
      "Train Epoch: 530 [16277/17352 (94%)] Loss: -114365.906250\n",
      "Train Epoch: 530 [16941/17352 (98%)] Loss: -17845.507812\n",
      "    epoch          : 530\n",
      "    loss           : -135106.7843795879\n",
      "    val_loss       : -79169.69403483073\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch530.pth ...\n",
      "Train Epoch: 531 [128/17352 (1%)] Loss: -172568.812500\n",
      "Train Epoch: 531 [1536/17352 (9%)] Loss: -146300.765625\n",
      "Train Epoch: 531 [2944/17352 (17%)] Loss: -160028.218750\n",
      "Train Epoch: 531 [4352/17352 (25%)] Loss: -153952.656250\n",
      "Train Epoch: 531 [5760/17352 (33%)] Loss: -170180.843750\n",
      "Train Epoch: 531 [7168/17352 (41%)] Loss: -145699.343750\n",
      "Train Epoch: 531 [8576/17352 (49%)] Loss: -144778.140625\n",
      "Train Epoch: 531 [9984/17352 (58%)] Loss: -158821.375000\n",
      "Train Epoch: 531 [11392/17352 (66%)] Loss: -176666.468750\n",
      "Train Epoch: 531 [12800/17352 (74%)] Loss: -191342.062500\n",
      "Train Epoch: 531 [14208/17352 (82%)] Loss: -161930.093750\n",
      "Train Epoch: 531 [15570/17352 (90%)] Loss: -117156.570312\n",
      "Train Epoch: 531 [16275/17352 (94%)] Loss: -109302.203125\n",
      "Train Epoch: 531 [17061/17352 (98%)] Loss: -50456.132812\n",
      "    epoch          : 531\n",
      "    loss           : -144680.28943280724\n",
      "    val_loss       : -61810.86333007812\n",
      "Train Epoch: 532 [128/17352 (1%)] Loss: -120242.062500\n",
      "Train Epoch: 532 [1536/17352 (9%)] Loss: -142869.140625\n",
      "Train Epoch: 532 [2944/17352 (17%)] Loss: -147070.953125\n",
      "Train Epoch: 532 [4352/17352 (25%)] Loss: -154994.562500\n",
      "Train Epoch: 532 [5760/17352 (33%)] Loss: -144240.312500\n",
      "Train Epoch: 532 [7168/17352 (41%)] Loss: -178088.093750\n",
      "Train Epoch: 532 [8576/17352 (49%)] Loss: -134035.109375\n",
      "Train Epoch: 532 [9984/17352 (58%)] Loss: -140240.343750\n",
      "Train Epoch: 532 [11392/17352 (66%)] Loss: -150427.578125\n",
      "Train Epoch: 532 [12800/17352 (74%)] Loss: -134171.968750\n",
      "Train Epoch: 532 [14208/17352 (82%)] Loss: -124100.406250\n",
      "Train Epoch: 532 [15533/17352 (90%)] Loss: -89288.617188\n",
      "Train Epoch: 532 [16378/17352 (94%)] Loss: -51834.628906\n",
      "Train Epoch: 532 [17109/17352 (99%)] Loss: -2472.938965\n",
      "    epoch          : 532\n",
      "    loss           : -132622.8021477821\n",
      "    val_loss       : -65852.92316487631\n",
      "Train Epoch: 533 [128/17352 (1%)] Loss: -124330.828125\n",
      "Train Epoch: 533 [1536/17352 (9%)] Loss: -128877.953125\n",
      "Train Epoch: 533 [2944/17352 (17%)] Loss: -141177.687500\n",
      "Train Epoch: 533 [4352/17352 (25%)] Loss: -135652.421875\n",
      "Train Epoch: 533 [5760/17352 (33%)] Loss: -158664.843750\n",
      "Train Epoch: 533 [7168/17352 (41%)] Loss: -101888.750000\n",
      "Train Epoch: 533 [8576/17352 (49%)] Loss: -103968.320312\n",
      "Train Epoch: 533 [9984/17352 (58%)] Loss: -122094.351562\n",
      "Train Epoch: 533 [11392/17352 (66%)] Loss: -138266.640625\n",
      "Train Epoch: 533 [12800/17352 (74%)] Loss: -156448.390625\n",
      "Train Epoch: 533 [14208/17352 (82%)] Loss: -140797.187500\n",
      "Train Epoch: 533 [15523/17352 (89%)] Loss: -100910.906250\n",
      "Train Epoch: 533 [16346/17352 (94%)] Loss: -115303.953125\n",
      "Train Epoch: 533 [16909/17352 (97%)] Loss: -38180.257812\n",
      "    epoch          : 533\n",
      "    loss           : -128735.19349045721\n",
      "    val_loss       : -29292.627376302084\n",
      "Train Epoch: 534 [128/17352 (1%)] Loss: -63864.203125\n",
      "Train Epoch: 534 [1536/17352 (9%)] Loss: -116740.375000\n",
      "Train Epoch: 534 [2944/17352 (17%)] Loss: -143996.468750\n",
      "Train Epoch: 534 [4352/17352 (25%)] Loss: -168294.937500\n",
      "Train Epoch: 534 [5760/17352 (33%)] Loss: -157949.750000\n",
      "Train Epoch: 534 [7168/17352 (41%)] Loss: -140047.812500\n",
      "Train Epoch: 534 [8576/17352 (49%)] Loss: -176214.234375\n",
      "Train Epoch: 534 [9984/17352 (58%)] Loss: -122115.109375\n",
      "Train Epoch: 534 [11392/17352 (66%)] Loss: -155702.093750\n",
      "Train Epoch: 534 [12800/17352 (74%)] Loss: -180856.968750\n",
      "Train Epoch: 534 [14208/17352 (82%)] Loss: -142014.234375\n",
      "Train Epoch: 534 [15491/17352 (89%)] Loss: -105091.656250\n",
      "Train Epoch: 534 [16358/17352 (94%)] Loss: -88709.906250\n",
      "Train Epoch: 534 [16927/17352 (98%)] Loss: -125453.781250\n",
      "    epoch          : 534\n",
      "    loss           : -135860.79841521604\n",
      "    val_loss       : -81982.51850585938\n",
      "Train Epoch: 535 [128/17352 (1%)] Loss: -184031.203125\n",
      "Train Epoch: 535 [1536/17352 (9%)] Loss: -169415.140625\n",
      "Train Epoch: 535 [2944/17352 (17%)] Loss: -171538.531250\n",
      "Train Epoch: 535 [4352/17352 (25%)] Loss: -168379.109375\n",
      "Train Epoch: 535 [5760/17352 (33%)] Loss: -198584.187500\n",
      "Train Epoch: 535 [7168/17352 (41%)] Loss: -154115.359375\n",
      "Train Epoch: 535 [8576/17352 (49%)] Loss: -149651.718750\n",
      "Train Epoch: 535 [9984/17352 (58%)] Loss: -183772.812500\n",
      "Train Epoch: 535 [11392/17352 (66%)] Loss: -150234.687500\n",
      "Train Epoch: 535 [12800/17352 (74%)] Loss: -154897.187500\n",
      "Train Epoch: 535 [14208/17352 (82%)] Loss: -155122.843750\n",
      "Train Epoch: 535 [15447/17352 (89%)] Loss: -4112.300293\n",
      "Train Epoch: 535 [16402/17352 (95%)] Loss: -128224.046875\n",
      "Train Epoch: 535 [17056/17352 (98%)] Loss: -126072.296875\n",
      "    epoch          : 535\n",
      "    loss           : -140884.36274708997\n",
      "    val_loss       : -79584.915230306\n",
      "Train Epoch: 536 [128/17352 (1%)] Loss: -127527.335938\n",
      "Train Epoch: 536 [1536/17352 (9%)] Loss: -168921.171875\n",
      "Train Epoch: 536 [2944/17352 (17%)] Loss: -132811.187500\n",
      "Train Epoch: 536 [4352/17352 (25%)] Loss: -134403.796875\n",
      "Train Epoch: 536 [5760/17352 (33%)] Loss: -151644.687500\n",
      "Train Epoch: 536 [7168/17352 (41%)] Loss: -127985.492188\n",
      "Train Epoch: 536 [8576/17352 (49%)] Loss: -148486.937500\n",
      "Train Epoch: 536 [9984/17352 (58%)] Loss: -152448.203125\n",
      "Train Epoch: 536 [11392/17352 (66%)] Loss: -122591.632812\n",
      "Train Epoch: 536 [12800/17352 (74%)] Loss: -166645.718750\n",
      "Train Epoch: 536 [14208/17352 (82%)] Loss: -176358.718750\n",
      "Train Epoch: 536 [15513/17352 (89%)] Loss: -96747.546875\n",
      "Train Epoch: 536 [16241/17352 (94%)] Loss: -114948.812500\n",
      "Train Epoch: 536 [17023/17352 (98%)] Loss: -57442.500000\n",
      "    epoch          : 536\n",
      "    loss           : -137053.31779572146\n",
      "    val_loss       : -75674.05123291016\n",
      "Train Epoch: 537 [128/17352 (1%)] Loss: -185482.234375\n",
      "Train Epoch: 537 [1536/17352 (9%)] Loss: -152269.843750\n",
      "Train Epoch: 537 [2944/17352 (17%)] Loss: -141513.140625\n",
      "Train Epoch: 537 [4352/17352 (25%)] Loss: -163990.468750\n",
      "Train Epoch: 537 [5760/17352 (33%)] Loss: -145050.984375\n",
      "Train Epoch: 537 [7168/17352 (41%)] Loss: -186596.343750\n",
      "Train Epoch: 537 [8576/17352 (49%)] Loss: -123359.242188\n",
      "Train Epoch: 537 [9984/17352 (58%)] Loss: -151007.281250\n",
      "Train Epoch: 537 [11392/17352 (66%)] Loss: -166878.093750\n",
      "Train Epoch: 537 [12800/17352 (74%)] Loss: -134985.359375\n",
      "Train Epoch: 537 [14208/17352 (82%)] Loss: -183373.750000\n",
      "Train Epoch: 537 [15440/17352 (89%)] Loss: -83048.757812\n",
      "Train Epoch: 537 [16065/17352 (93%)] Loss: -62410.070312\n",
      "Train Epoch: 537 [16912/17352 (97%)] Loss: -134991.968750\n",
      "    epoch          : 537\n",
      "    loss           : -142087.5914069054\n",
      "    val_loss       : -78802.99977213542\n",
      "Train Epoch: 538 [128/17352 (1%)] Loss: -173439.234375\n",
      "Train Epoch: 538 [1536/17352 (9%)] Loss: -153841.687500\n",
      "Train Epoch: 538 [2944/17352 (17%)] Loss: -171402.812500\n",
      "Train Epoch: 538 [4352/17352 (25%)] Loss: -172916.765625\n",
      "Train Epoch: 538 [5760/17352 (33%)] Loss: -130484.664062\n",
      "Train Epoch: 538 [7168/17352 (41%)] Loss: -110920.734375\n",
      "Train Epoch: 538 [8576/17352 (49%)] Loss: -128575.125000\n",
      "Train Epoch: 538 [9984/17352 (58%)] Loss: -142804.187500\n",
      "Train Epoch: 538 [11392/17352 (66%)] Loss: -166851.234375\n",
      "Train Epoch: 538 [12800/17352 (74%)] Loss: -151360.125000\n",
      "Train Epoch: 538 [14208/17352 (82%)] Loss: -155451.625000\n",
      "Train Epoch: 538 [15573/17352 (90%)] Loss: -114926.328125\n",
      "Train Epoch: 538 [16473/17352 (95%)] Loss: -138549.578125\n",
      "Train Epoch: 538 [17056/17352 (98%)] Loss: -17143.044922\n",
      "    epoch          : 538\n",
      "    loss           : -139687.5754148752\n",
      "    val_loss       : -78333.40742594401\n",
      "Train Epoch: 539 [128/17352 (1%)] Loss: -151544.250000\n",
      "Train Epoch: 539 [1536/17352 (9%)] Loss: -177216.656250\n",
      "Train Epoch: 539 [2944/17352 (17%)] Loss: -212109.000000\n",
      "Train Epoch: 539 [4352/17352 (25%)] Loss: -193993.453125\n",
      "Train Epoch: 539 [5760/17352 (33%)] Loss: -127449.929688\n",
      "Train Epoch: 539 [7168/17352 (41%)] Loss: -139521.671875\n",
      "Train Epoch: 539 [8576/17352 (49%)] Loss: -138792.937500\n",
      "Train Epoch: 539 [9984/17352 (58%)] Loss: -113901.906250\n",
      "Train Epoch: 539 [11392/17352 (66%)] Loss: -148454.656250\n",
      "Train Epoch: 539 [12800/17352 (74%)] Loss: -134346.375000\n",
      "Train Epoch: 539 [14208/17352 (82%)] Loss: -168859.859375\n",
      "Train Epoch: 539 [15526/17352 (89%)] Loss: -99345.062500\n",
      "Train Epoch: 539 [16368/17352 (94%)] Loss: -109120.906250\n",
      "Train Epoch: 539 [16950/17352 (98%)] Loss: -3846.520752\n",
      "    epoch          : 539\n",
      "    loss           : -143508.1154522992\n",
      "    val_loss       : -77190.73409423829\n",
      "Train Epoch: 540 [128/17352 (1%)] Loss: -145463.531250\n",
      "Train Epoch: 540 [1536/17352 (9%)] Loss: -141634.859375\n",
      "Train Epoch: 540 [2944/17352 (17%)] Loss: -190653.468750\n",
      "Train Epoch: 540 [4352/17352 (25%)] Loss: -138937.875000\n",
      "Train Epoch: 540 [5760/17352 (33%)] Loss: -186636.312500\n",
      "Train Epoch: 540 [7168/17352 (41%)] Loss: -141498.187500\n",
      "Train Epoch: 540 [8576/17352 (49%)] Loss: -168983.125000\n",
      "Train Epoch: 540 [9984/17352 (58%)] Loss: -120911.085938\n",
      "Train Epoch: 540 [11392/17352 (66%)] Loss: -100071.851562\n",
      "Train Epoch: 540 [12800/17352 (74%)] Loss: -108950.851562\n",
      "Train Epoch: 540 [14208/17352 (82%)] Loss: -165810.562500\n",
      "Train Epoch: 540 [15471/17352 (89%)] Loss: -97062.937500\n",
      "Train Epoch: 540 [16162/17352 (93%)] Loss: -12376.577148\n",
      "Train Epoch: 540 [17004/17352 (98%)] Loss: -41366.566406\n",
      "    epoch          : 540\n",
      "    loss           : -132646.09925053743\n",
      "    val_loss       : -72406.84869791666\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch540.pth ...\n",
      "Train Epoch: 541 [128/17352 (1%)] Loss: -158983.093750\n",
      "Train Epoch: 541 [1536/17352 (9%)] Loss: -160120.703125\n",
      "Train Epoch: 541 [2944/17352 (17%)] Loss: -108802.375000\n",
      "Train Epoch: 541 [4352/17352 (25%)] Loss: -141603.515625\n",
      "Train Epoch: 541 [5760/17352 (33%)] Loss: -163692.375000\n",
      "Train Epoch: 541 [7168/17352 (41%)] Loss: -179869.328125\n",
      "Train Epoch: 541 [8576/17352 (49%)] Loss: -197538.156250\n",
      "Train Epoch: 541 [9984/17352 (58%)] Loss: -167425.328125\n",
      "Train Epoch: 541 [11392/17352 (66%)] Loss: -152306.656250\n",
      "Train Epoch: 541 [12800/17352 (74%)] Loss: -166760.218750\n",
      "Train Epoch: 541 [14208/17352 (82%)] Loss: -161782.625000\n",
      "Train Epoch: 541 [15506/17352 (89%)] Loss: -49044.800781\n",
      "Train Epoch: 541 [16287/17352 (94%)] Loss: -98369.203125\n",
      "Train Epoch: 541 [16956/17352 (98%)] Loss: -6681.585449\n",
      "    epoch          : 541\n",
      "    loss           : -139742.50533996173\n",
      "    val_loss       : -67856.84672037761\n",
      "Train Epoch: 542 [128/17352 (1%)] Loss: -165530.218750\n",
      "Train Epoch: 542 [1536/17352 (9%)] Loss: -179880.093750\n",
      "Train Epoch: 542 [2944/17352 (17%)] Loss: -184769.500000\n",
      "Train Epoch: 542 [4352/17352 (25%)] Loss: -130694.015625\n",
      "Train Epoch: 542 [5760/17352 (33%)] Loss: -155945.343750\n",
      "Train Epoch: 542 [7168/17352 (41%)] Loss: -141203.250000\n",
      "Train Epoch: 542 [8576/17352 (49%)] Loss: -119269.210938\n",
      "Train Epoch: 542 [9984/17352 (58%)] Loss: -160499.296875\n",
      "Train Epoch: 542 [11392/17352 (66%)] Loss: -126093.828125\n",
      "Train Epoch: 542 [12800/17352 (74%)] Loss: -87275.171875\n",
      "Train Epoch: 542 [14208/17352 (82%)] Loss: -132400.562500\n",
      "Train Epoch: 542 [15533/17352 (90%)] Loss: -71253.812500\n",
      "Train Epoch: 542 [16156/17352 (93%)] Loss: -5263.430664\n",
      "Train Epoch: 542 [16870/17352 (97%)] Loss: -3846.492676\n",
      "    epoch          : 542\n",
      "    loss           : -132512.86681063863\n",
      "    val_loss       : -71247.52338867188\n",
      "Train Epoch: 543 [128/17352 (1%)] Loss: -159910.781250\n",
      "Train Epoch: 543 [1536/17352 (9%)] Loss: -145585.906250\n",
      "Train Epoch: 543 [2944/17352 (17%)] Loss: -149921.812500\n",
      "Train Epoch: 543 [4352/17352 (25%)] Loss: -166771.937500\n",
      "Train Epoch: 543 [5760/17352 (33%)] Loss: -156432.906250\n",
      "Train Epoch: 543 [7168/17352 (41%)] Loss: -160663.250000\n",
      "Train Epoch: 543 [8576/17352 (49%)] Loss: -137966.406250\n",
      "Train Epoch: 543 [9984/17352 (58%)] Loss: -199328.875000\n",
      "Train Epoch: 543 [11392/17352 (66%)] Loss: -142503.406250\n",
      "Train Epoch: 543 [12800/17352 (74%)] Loss: -31265.525391\n",
      "Train Epoch: 543 [14208/17352 (82%)] Loss: -110974.234375\n",
      "Train Epoch: 543 [15469/17352 (89%)] Loss: -72301.992188\n",
      "Train Epoch: 543 [16358/17352 (94%)] Loss: -101924.757812\n",
      "Train Epoch: 543 [17058/17352 (98%)] Loss: -56610.605469\n",
      "    epoch          : 543\n",
      "    loss           : -132815.07069919253\n",
      "    val_loss       : -76847.53912760416\n",
      "Train Epoch: 544 [128/17352 (1%)] Loss: -150347.671875\n",
      "Train Epoch: 544 [1536/17352 (9%)] Loss: -140824.281250\n",
      "Train Epoch: 544 [2944/17352 (17%)] Loss: -126608.539062\n",
      "Train Epoch: 544 [4352/17352 (25%)] Loss: -158149.093750\n",
      "Train Epoch: 544 [5760/17352 (33%)] Loss: -169833.468750\n",
      "Train Epoch: 544 [7168/17352 (41%)] Loss: -159396.531250\n",
      "Train Epoch: 544 [8576/17352 (49%)] Loss: -171839.484375\n",
      "Train Epoch: 544 [9984/17352 (58%)] Loss: -117290.234375\n",
      "Train Epoch: 544 [11392/17352 (66%)] Loss: -131250.781250\n",
      "Train Epoch: 544 [12800/17352 (74%)] Loss: -122382.937500\n",
      "Train Epoch: 544 [14208/17352 (82%)] Loss: -151646.687500\n",
      "Train Epoch: 544 [15455/17352 (89%)] Loss: -50004.687500\n",
      "Train Epoch: 544 [16408/17352 (95%)] Loss: -97876.640625\n",
      "Train Epoch: 544 [16956/17352 (98%)] Loss: -18361.707031\n",
      "    epoch          : 544\n",
      "    loss           : -141603.00200555788\n",
      "    val_loss       : -76449.8448038737\n",
      "Train Epoch: 545 [128/17352 (1%)] Loss: -143201.843750\n",
      "Train Epoch: 545 [1536/17352 (9%)] Loss: -93728.914062\n",
      "Train Epoch: 545 [2944/17352 (17%)] Loss: -133001.718750\n",
      "Train Epoch: 545 [4352/17352 (25%)] Loss: -164620.484375\n",
      "Train Epoch: 545 [5760/17352 (33%)] Loss: -144218.812500\n",
      "Train Epoch: 545 [7168/17352 (41%)] Loss: -169925.375000\n",
      "Train Epoch: 545 [8576/17352 (49%)] Loss: -185230.000000\n",
      "Train Epoch: 545 [9984/17352 (58%)] Loss: -144725.968750\n",
      "Train Epoch: 545 [11392/17352 (66%)] Loss: -121639.429688\n",
      "Train Epoch: 545 [12800/17352 (74%)] Loss: -172560.953125\n",
      "Train Epoch: 545 [14208/17352 (82%)] Loss: -116560.601562\n",
      "Train Epoch: 545 [15536/17352 (90%)] Loss: -91968.601562\n",
      "Train Epoch: 545 [16260/17352 (94%)] Loss: -53359.093750\n",
      "Train Epoch: 545 [17082/17352 (98%)] Loss: -5957.536621\n",
      "    epoch          : 545\n",
      "    loss           : -133605.71793237468\n",
      "    val_loss       : -68940.77502034506\n",
      "Train Epoch: 546 [128/17352 (1%)] Loss: -162253.687500\n",
      "Train Epoch: 546 [1536/17352 (9%)] Loss: -115154.703125\n",
      "Train Epoch: 546 [2944/17352 (17%)] Loss: -105419.585938\n",
      "Train Epoch: 546 [4352/17352 (25%)] Loss: -151581.625000\n",
      "Train Epoch: 546 [5760/17352 (33%)] Loss: -135146.468750\n",
      "Train Epoch: 546 [7168/17352 (41%)] Loss: -160862.203125\n",
      "Train Epoch: 546 [8576/17352 (49%)] Loss: -151028.515625\n",
      "Train Epoch: 546 [9984/17352 (58%)] Loss: -140092.781250\n",
      "Train Epoch: 546 [11392/17352 (66%)] Loss: -149225.562500\n",
      "Train Epoch: 546 [12800/17352 (74%)] Loss: -141813.546875\n",
      "Train Epoch: 546 [14208/17352 (82%)] Loss: -153272.781250\n",
      "Train Epoch: 546 [15436/17352 (89%)] Loss: -87875.984375\n",
      "Train Epoch: 546 [16284/17352 (94%)] Loss: -146774.562500\n",
      "Train Epoch: 546 [17109/17352 (99%)] Loss: -61044.867188\n",
      "    epoch          : 546\n",
      "    loss           : -139974.54073379823\n",
      "    val_loss       : -80486.79548746745\n",
      "Train Epoch: 547 [128/17352 (1%)] Loss: -166555.796875\n",
      "Train Epoch: 547 [1536/17352 (9%)] Loss: -162748.906250\n",
      "Train Epoch: 547 [2944/17352 (17%)] Loss: -129565.851562\n",
      "Train Epoch: 547 [4352/17352 (25%)] Loss: -146136.468750\n",
      "Train Epoch: 547 [5760/17352 (33%)] Loss: -170033.703125\n",
      "Train Epoch: 547 [7168/17352 (41%)] Loss: -162198.468750\n",
      "Train Epoch: 547 [8576/17352 (49%)] Loss: -142134.312500\n",
      "Train Epoch: 547 [9984/17352 (58%)] Loss: -162642.781250\n",
      "Train Epoch: 547 [11392/17352 (66%)] Loss: -176093.437500\n",
      "Train Epoch: 547 [12800/17352 (74%)] Loss: -176894.281250\n",
      "Train Epoch: 547 [14208/17352 (82%)] Loss: -162856.375000\n",
      "Train Epoch: 547 [15580/17352 (90%)] Loss: -154324.546875\n",
      "Train Epoch: 547 [16303/17352 (94%)] Loss: -40021.843750\n",
      "Train Epoch: 547 [16981/17352 (98%)] Loss: -81542.976562\n",
      "    epoch          : 547\n",
      "    loss           : -138949.63903071257\n",
      "    val_loss       : -48425.65609130859\n",
      "Train Epoch: 548 [128/17352 (1%)] Loss: -98730.437500\n",
      "Train Epoch: 548 [1536/17352 (9%)] Loss: -165138.640625\n",
      "Train Epoch: 548 [2944/17352 (17%)] Loss: -148057.156250\n",
      "Train Epoch: 548 [4352/17352 (25%)] Loss: -159191.500000\n",
      "Train Epoch: 548 [5760/17352 (33%)] Loss: -158815.453125\n",
      "Train Epoch: 548 [7168/17352 (41%)] Loss: -112371.960938\n",
      "Train Epoch: 548 [8576/17352 (49%)] Loss: -162783.718750\n",
      "Train Epoch: 548 [9984/17352 (58%)] Loss: -145178.796875\n",
      "Train Epoch: 548 [11392/17352 (66%)] Loss: -137943.156250\n",
      "Train Epoch: 548 [12800/17352 (74%)] Loss: -165955.593750\n",
      "Train Epoch: 548 [14208/17352 (82%)] Loss: -157745.937500\n",
      "Train Epoch: 548 [15495/17352 (89%)] Loss: -133118.421875\n",
      "Train Epoch: 548 [16173/17352 (93%)] Loss: -71629.101562\n",
      "Train Epoch: 548 [16856/17352 (97%)] Loss: -10708.227539\n",
      "    epoch          : 548\n",
      "    loss           : -134128.78089280098\n",
      "    val_loss       : -51928.01069335938\n",
      "Train Epoch: 549 [128/17352 (1%)] Loss: -120545.960938\n",
      "Train Epoch: 549 [1536/17352 (9%)] Loss: -133320.406250\n",
      "Train Epoch: 549 [2944/17352 (17%)] Loss: -168814.921875\n",
      "Train Epoch: 549 [4352/17352 (25%)] Loss: -141079.281250\n",
      "Train Epoch: 549 [5760/17352 (33%)] Loss: -162453.765625\n",
      "Train Epoch: 549 [7168/17352 (41%)] Loss: -164719.734375\n",
      "Train Epoch: 549 [8576/17352 (49%)] Loss: -169359.000000\n",
      "Train Epoch: 549 [9984/17352 (58%)] Loss: -164576.468750\n",
      "Train Epoch: 549 [11392/17352 (66%)] Loss: -159475.515625\n",
      "Train Epoch: 549 [12800/17352 (74%)] Loss: -162896.515625\n",
      "Train Epoch: 549 [14208/17352 (82%)] Loss: -168593.796875\n",
      "Train Epoch: 549 [15440/17352 (89%)] Loss: -92542.109375\n",
      "Train Epoch: 549 [16235/17352 (94%)] Loss: -115397.773438\n",
      "Train Epoch: 549 [16996/17352 (98%)] Loss: -29465.742188\n",
      "    epoch          : 549\n",
      "    loss           : -133505.7580599177\n",
      "    val_loss       : -47506.925048828125\n",
      "Train Epoch: 550 [128/17352 (1%)] Loss: -89705.125000\n",
      "Train Epoch: 550 [1536/17352 (9%)] Loss: -155519.906250\n",
      "Train Epoch: 550 [2944/17352 (17%)] Loss: -164040.406250\n",
      "Train Epoch: 550 [4352/17352 (25%)] Loss: -143554.125000\n",
      "Train Epoch: 550 [5760/17352 (33%)] Loss: -124637.132812\n",
      "Train Epoch: 550 [7168/17352 (41%)] Loss: -168093.906250\n",
      "Train Epoch: 550 [8576/17352 (49%)] Loss: -153000.546875\n",
      "Train Epoch: 550 [9984/17352 (58%)] Loss: -169754.609375\n",
      "Train Epoch: 550 [11392/17352 (66%)] Loss: -164977.312500\n",
      "Train Epoch: 550 [12800/17352 (74%)] Loss: -154889.015625\n",
      "Train Epoch: 550 [14208/17352 (82%)] Loss: -162272.390625\n",
      "Train Epoch: 550 [15533/17352 (90%)] Loss: -101343.023438\n",
      "Train Epoch: 550 [16214/17352 (93%)] Loss: -94788.476562\n",
      "Train Epoch: 550 [17024/17352 (98%)] Loss: -18398.796875\n",
      "    epoch          : 550\n",
      "    loss           : -137980.92807027316\n",
      "    val_loss       : -80957.94337158203\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch550.pth ...\n",
      "Train Epoch: 551 [128/17352 (1%)] Loss: -173239.000000\n",
      "Train Epoch: 551 [1536/17352 (9%)] Loss: -168929.296875\n",
      "Train Epoch: 551 [2944/17352 (17%)] Loss: -163385.468750\n",
      "Train Epoch: 551 [4352/17352 (25%)] Loss: -157762.812500\n",
      "Train Epoch: 551 [5760/17352 (33%)] Loss: -162191.109375\n",
      "Train Epoch: 551 [7168/17352 (41%)] Loss: -142689.937500\n",
      "Train Epoch: 551 [8576/17352 (49%)] Loss: -141188.375000\n",
      "Train Epoch: 551 [9984/17352 (58%)] Loss: -156065.203125\n",
      "Train Epoch: 551 [11392/17352 (66%)] Loss: -151783.218750\n",
      "Train Epoch: 551 [12800/17352 (74%)] Loss: -173700.046875\n",
      "Train Epoch: 551 [14208/17352 (82%)] Loss: -144887.500000\n",
      "Train Epoch: 551 [15499/17352 (89%)] Loss: -133263.375000\n",
      "Train Epoch: 551 [16196/17352 (93%)] Loss: -64970.113281\n",
      "Train Epoch: 551 [16945/17352 (98%)] Loss: -6494.613281\n",
      "    epoch          : 551\n",
      "    loss           : -146366.134647651\n",
      "    val_loss       : -78619.9395345052\n",
      "Train Epoch: 552 [128/17352 (1%)] Loss: -149942.515625\n",
      "Train Epoch: 552 [1536/17352 (9%)] Loss: -156528.765625\n",
      "Train Epoch: 552 [2944/17352 (17%)] Loss: -122432.242188\n",
      "Train Epoch: 552 [4352/17352 (25%)] Loss: -126898.179688\n",
      "Train Epoch: 552 [5760/17352 (33%)] Loss: -112836.687500\n",
      "Train Epoch: 552 [7168/17352 (41%)] Loss: -102351.640625\n",
      "Train Epoch: 552 [8576/17352 (49%)] Loss: -163049.468750\n",
      "Train Epoch: 552 [9984/17352 (58%)] Loss: -115631.687500\n",
      "Train Epoch: 552 [11392/17352 (66%)] Loss: -138415.296875\n",
      "Train Epoch: 552 [12800/17352 (74%)] Loss: -148534.734375\n",
      "Train Epoch: 552 [14208/17352 (82%)] Loss: -150740.062500\n",
      "Train Epoch: 552 [15471/17352 (89%)] Loss: -50085.054688\n",
      "Train Epoch: 552 [16307/17352 (94%)] Loss: -62819.671875\n",
      "Train Epoch: 552 [16957/17352 (98%)] Loss: -48607.566406\n",
      "    epoch          : 552\n",
      "    loss           : -133025.37390874056\n",
      "    val_loss       : -77554.87902832031\n",
      "Train Epoch: 553 [128/17352 (1%)] Loss: -180825.171875\n",
      "Train Epoch: 553 [1536/17352 (9%)] Loss: -158968.859375\n",
      "Train Epoch: 553 [2944/17352 (17%)] Loss: -154837.421875\n",
      "Train Epoch: 553 [4352/17352 (25%)] Loss: -154045.890625\n",
      "Train Epoch: 553 [5760/17352 (33%)] Loss: -173110.687500\n",
      "Train Epoch: 553 [7168/17352 (41%)] Loss: -184780.906250\n",
      "Train Epoch: 553 [8576/17352 (49%)] Loss: -160578.640625\n",
      "Train Epoch: 553 [9984/17352 (58%)] Loss: -176023.250000\n",
      "Train Epoch: 553 [11392/17352 (66%)] Loss: -159018.187500\n",
      "Train Epoch: 553 [12800/17352 (74%)] Loss: -149494.578125\n",
      "Train Epoch: 553 [14208/17352 (82%)] Loss: -141361.546875\n",
      "Train Epoch: 553 [15500/17352 (89%)] Loss: -130114.343750\n",
      "Train Epoch: 553 [16070/17352 (93%)] Loss: -10566.432617\n",
      "Train Epoch: 553 [16968/17352 (98%)] Loss: -107671.328125\n",
      "    epoch          : 553\n",
      "    loss           : -138641.1067402475\n",
      "    val_loss       : -68964.55677083334\n",
      "Train Epoch: 554 [128/17352 (1%)] Loss: -171020.890625\n",
      "Train Epoch: 554 [1536/17352 (9%)] Loss: -156698.062500\n",
      "Train Epoch: 554 [2944/17352 (17%)] Loss: -145502.046875\n",
      "Train Epoch: 554 [4352/17352 (25%)] Loss: -143419.671875\n",
      "Train Epoch: 554 [5760/17352 (33%)] Loss: -139564.000000\n",
      "Train Epoch: 554 [7168/17352 (41%)] Loss: -154205.390625\n",
      "Train Epoch: 554 [8576/17352 (49%)] Loss: -131932.562500\n",
      "Train Epoch: 554 [9984/17352 (58%)] Loss: -183012.000000\n",
      "Train Epoch: 554 [11392/17352 (66%)] Loss: -173341.984375\n",
      "Train Epoch: 554 [12800/17352 (74%)] Loss: -163239.125000\n",
      "Train Epoch: 554 [14208/17352 (82%)] Loss: -160100.359375\n",
      "Train Epoch: 554 [15544/17352 (90%)] Loss: -148405.531250\n",
      "Train Epoch: 554 [16368/17352 (94%)] Loss: -19922.695312\n",
      "Train Epoch: 554 [17038/17352 (98%)] Loss: -131166.625000\n",
      "    epoch          : 554\n",
      "    loss           : -140939.38422491087\n",
      "    val_loss       : -79375.07537027994\n",
      "Train Epoch: 555 [128/17352 (1%)] Loss: -140825.000000\n",
      "Train Epoch: 555 [1536/17352 (9%)] Loss: -96280.843750\n",
      "Train Epoch: 555 [2944/17352 (17%)] Loss: -172406.375000\n",
      "Train Epoch: 555 [4352/17352 (25%)] Loss: -153015.390625\n",
      "Train Epoch: 555 [5760/17352 (33%)] Loss: -162602.812500\n",
      "Train Epoch: 555 [7168/17352 (41%)] Loss: -98466.179688\n",
      "Train Epoch: 555 [8576/17352 (49%)] Loss: -158243.000000\n",
      "Train Epoch: 555 [9984/17352 (58%)] Loss: -66242.414062\n",
      "Train Epoch: 555 [11392/17352 (66%)] Loss: -110481.125000\n",
      "Train Epoch: 555 [12800/17352 (74%)] Loss: -162412.906250\n",
      "Train Epoch: 555 [14208/17352 (82%)] Loss: -143575.656250\n",
      "Train Epoch: 555 [15504/17352 (89%)] Loss: -88589.304688\n",
      "Train Epoch: 555 [16269/17352 (94%)] Loss: -110117.335938\n",
      "Train Epoch: 555 [17006/17352 (98%)] Loss: -105485.210938\n",
      "    epoch          : 555\n",
      "    loss           : -133495.64450503356\n",
      "    val_loss       : -77081.1490641276\n",
      "Train Epoch: 556 [128/17352 (1%)] Loss: -180458.250000\n",
      "Train Epoch: 556 [1536/17352 (9%)] Loss: -166824.640625\n",
      "Train Epoch: 556 [2944/17352 (17%)] Loss: -150070.750000\n",
      "Train Epoch: 556 [4352/17352 (25%)] Loss: -173308.515625\n",
      "Train Epoch: 556 [5760/17352 (33%)] Loss: -149998.750000\n",
      "Train Epoch: 556 [7168/17352 (41%)] Loss: -183127.859375\n",
      "Train Epoch: 556 [8576/17352 (49%)] Loss: -179847.250000\n",
      "Train Epoch: 556 [9984/17352 (58%)] Loss: -152880.593750\n",
      "Train Epoch: 556 [11392/17352 (66%)] Loss: -156136.687500\n",
      "Train Epoch: 556 [12800/17352 (74%)] Loss: -189702.578125\n",
      "Train Epoch: 556 [14208/17352 (82%)] Loss: -188235.843750\n",
      "Train Epoch: 556 [15512/17352 (89%)] Loss: -41359.609375\n",
      "Train Epoch: 556 [16091/17352 (93%)] Loss: -86757.781250\n",
      "Train Epoch: 556 [16994/17352 (98%)] Loss: -92987.984375\n",
      "    epoch          : 556\n",
      "    loss           : -146724.93507170197\n",
      "    val_loss       : -79435.7103149414\n",
      "Train Epoch: 557 [128/17352 (1%)] Loss: -144740.562500\n",
      "Train Epoch: 557 [1536/17352 (9%)] Loss: -115734.367188\n",
      "Train Epoch: 557 [2944/17352 (17%)] Loss: -134498.296875\n",
      "Train Epoch: 557 [4352/17352 (25%)] Loss: -134818.765625\n",
      "Train Epoch: 557 [5760/17352 (33%)] Loss: -162042.562500\n",
      "Train Epoch: 557 [7168/17352 (41%)] Loss: -125522.960938\n",
      "Train Epoch: 557 [8576/17352 (49%)] Loss: -170011.156250\n",
      "Train Epoch: 557 [9984/17352 (58%)] Loss: -159517.875000\n",
      "Train Epoch: 557 [11392/17352 (66%)] Loss: -107025.703125\n",
      "Train Epoch: 557 [12800/17352 (74%)] Loss: -127755.265625\n",
      "Train Epoch: 557 [14208/17352 (82%)] Loss: -162674.359375\n",
      "Train Epoch: 557 [15461/17352 (89%)] Loss: -18820.958984\n",
      "Train Epoch: 557 [16111/17352 (93%)] Loss: -110627.093750\n",
      "Train Epoch: 557 [16909/17352 (97%)] Loss: -2352.070312\n",
      "    epoch          : 557\n",
      "    loss           : -133630.7587890625\n",
      "    val_loss       : -68002.09916178386\n",
      "Train Epoch: 558 [128/17352 (1%)] Loss: -165158.000000\n",
      "Train Epoch: 558 [1536/17352 (9%)] Loss: -162086.203125\n",
      "Train Epoch: 558 [2944/17352 (17%)] Loss: -180797.906250\n",
      "Train Epoch: 558 [4352/17352 (25%)] Loss: -139522.187500\n",
      "Train Epoch: 558 [5760/17352 (33%)] Loss: -171190.390625\n",
      "Train Epoch: 558 [7168/17352 (41%)] Loss: -170454.796875\n",
      "Train Epoch: 558 [8576/17352 (49%)] Loss: -164574.000000\n",
      "Train Epoch: 558 [9984/17352 (58%)] Loss: -146654.718750\n",
      "Train Epoch: 558 [11392/17352 (66%)] Loss: -161410.031250\n",
      "Train Epoch: 558 [12800/17352 (74%)] Loss: -141948.484375\n",
      "Train Epoch: 558 [14208/17352 (82%)] Loss: -156238.187500\n",
      "Train Epoch: 558 [15483/17352 (89%)] Loss: -91390.406250\n",
      "Train Epoch: 558 [16185/17352 (93%)] Loss: -89915.656250\n",
      "Train Epoch: 558 [16997/17352 (98%)] Loss: -5110.788086\n",
      "    epoch          : 558\n",
      "    loss           : -142221.90123610527\n",
      "    val_loss       : -67174.5208984375\n",
      "Train Epoch: 559 [128/17352 (1%)] Loss: -116800.679688\n",
      "Train Epoch: 559 [1536/17352 (9%)] Loss: -101887.609375\n",
      "Train Epoch: 559 [2944/17352 (17%)] Loss: -164788.093750\n",
      "Train Epoch: 559 [4352/17352 (25%)] Loss: -170837.968750\n",
      "Train Epoch: 559 [5760/17352 (33%)] Loss: -151443.093750\n",
      "Train Epoch: 559 [7168/17352 (41%)] Loss: -160660.718750\n",
      "Train Epoch: 559 [8576/17352 (49%)] Loss: -160749.703125\n",
      "Train Epoch: 559 [9984/17352 (58%)] Loss: -129853.601562\n",
      "Train Epoch: 559 [11392/17352 (66%)] Loss: -173422.750000\n",
      "Train Epoch: 559 [12800/17352 (74%)] Loss: -178228.406250\n",
      "Train Epoch: 559 [14208/17352 (82%)] Loss: -181239.515625\n",
      "Train Epoch: 559 [15529/17352 (89%)] Loss: -121708.476562\n",
      "Train Epoch: 559 [16079/17352 (93%)] Loss: -15206.863281\n",
      "Train Epoch: 559 [17034/17352 (98%)] Loss: -45879.960938\n",
      "    epoch          : 559\n",
      "    loss           : -140247.7014209312\n",
      "    val_loss       : -78695.03354085286\n",
      "Train Epoch: 560 [128/17352 (1%)] Loss: -126430.664062\n",
      "Train Epoch: 560 [1536/17352 (9%)] Loss: -149658.515625\n",
      "Train Epoch: 560 [2944/17352 (17%)] Loss: -161232.921875\n",
      "Train Epoch: 560 [4352/17352 (25%)] Loss: -158400.531250\n",
      "Train Epoch: 560 [5760/17352 (33%)] Loss: -176724.468750\n",
      "Train Epoch: 560 [7168/17352 (41%)] Loss: -146433.578125\n",
      "Train Epoch: 560 [8576/17352 (49%)] Loss: -120175.226562\n",
      "Train Epoch: 560 [9984/17352 (58%)] Loss: -160511.796875\n",
      "Train Epoch: 560 [11392/17352 (66%)] Loss: -155443.218750\n",
      "Train Epoch: 560 [12800/17352 (74%)] Loss: -154198.234375\n",
      "Train Epoch: 560 [14208/17352 (82%)] Loss: -128791.500000\n",
      "Train Epoch: 560 [15532/17352 (90%)] Loss: -115960.148438\n",
      "Train Epoch: 560 [16214/17352 (93%)] Loss: -46750.539062\n",
      "Train Epoch: 560 [16949/17352 (98%)] Loss: -14135.822266\n",
      "    epoch          : 560\n",
      "    loss           : -137367.03736662384\n",
      "    val_loss       : -65920.2626139323\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch560.pth ...\n",
      "Train Epoch: 561 [128/17352 (1%)] Loss: -125041.664062\n",
      "Train Epoch: 561 [1536/17352 (9%)] Loss: -132182.937500\n",
      "Train Epoch: 561 [2944/17352 (17%)] Loss: -150347.968750\n",
      "Train Epoch: 561 [4352/17352 (25%)] Loss: -166720.218750\n",
      "Train Epoch: 561 [5760/17352 (33%)] Loss: -165071.359375\n",
      "Train Epoch: 561 [7168/17352 (41%)] Loss: -157355.140625\n",
      "Train Epoch: 561 [8576/17352 (49%)] Loss: -151579.156250\n",
      "Train Epoch: 561 [9984/17352 (58%)] Loss: -181421.843750\n",
      "Train Epoch: 561 [11392/17352 (66%)] Loss: -131039.781250\n",
      "Train Epoch: 561 [12800/17352 (74%)] Loss: -145819.500000\n",
      "Train Epoch: 561 [14208/17352 (82%)] Loss: -165442.750000\n",
      "Train Epoch: 561 [15419/17352 (89%)] Loss: -6872.906250\n",
      "Train Epoch: 561 [16062/17352 (93%)] Loss: -131730.453125\n",
      "Train Epoch: 561 [17023/17352 (98%)] Loss: -49223.140625\n",
      "    epoch          : 561\n",
      "    loss           : -138725.76420275797\n",
      "    val_loss       : -48278.565161132814\n",
      "Train Epoch: 562 [128/17352 (1%)] Loss: -87972.195312\n",
      "Train Epoch: 562 [1536/17352 (9%)] Loss: -113421.578125\n",
      "Train Epoch: 562 [2944/17352 (17%)] Loss: -121979.875000\n",
      "Train Epoch: 562 [4352/17352 (25%)] Loss: -128776.484375\n",
      "Train Epoch: 562 [5760/17352 (33%)] Loss: -166995.062500\n",
      "Train Epoch: 562 [7168/17352 (41%)] Loss: -185232.203125\n",
      "Train Epoch: 562 [8576/17352 (49%)] Loss: -173408.703125\n",
      "Train Epoch: 562 [9984/17352 (58%)] Loss: -157048.875000\n",
      "Train Epoch: 562 [11392/17352 (66%)] Loss: -151595.843750\n",
      "Train Epoch: 562 [12800/17352 (74%)] Loss: -108146.242188\n",
      "Train Epoch: 562 [14208/17352 (82%)] Loss: -158437.500000\n",
      "Train Epoch: 562 [15467/17352 (89%)] Loss: -49092.000000\n",
      "Train Epoch: 562 [16313/17352 (94%)] Loss: -51561.156250\n",
      "Train Epoch: 562 [17121/17352 (99%)] Loss: -108683.585938\n",
      "    epoch          : 562\n",
      "    loss           : -135496.20918099832\n",
      "    val_loss       : -81225.10842285157\n",
      "Train Epoch: 563 [128/17352 (1%)] Loss: -156735.703125\n",
      "Train Epoch: 563 [1536/17352 (9%)] Loss: -137518.906250\n",
      "Train Epoch: 563 [2944/17352 (17%)] Loss: -171163.046875\n",
      "Train Epoch: 563 [4352/17352 (25%)] Loss: -163505.203125\n",
      "Train Epoch: 563 [5760/17352 (33%)] Loss: -179238.328125\n",
      "Train Epoch: 563 [7168/17352 (41%)] Loss: -162291.656250\n",
      "Train Epoch: 563 [8576/17352 (49%)] Loss: -147103.609375\n",
      "Train Epoch: 563 [9984/17352 (58%)] Loss: -179556.421875\n",
      "Train Epoch: 563 [11392/17352 (66%)] Loss: -127413.703125\n",
      "Train Epoch: 563 [12800/17352 (74%)] Loss: -159580.093750\n",
      "Train Epoch: 563 [14208/17352 (82%)] Loss: -162549.890625\n",
      "Train Epoch: 563 [15522/17352 (89%)] Loss: -93569.679688\n",
      "Train Epoch: 563 [16405/17352 (95%)] Loss: -19300.261719\n",
      "Train Epoch: 563 [17061/17352 (98%)] Loss: -35288.203125\n",
      "    epoch          : 563\n",
      "    loss           : -141011.80126461567\n",
      "    val_loss       : -72202.63631591797\n",
      "Train Epoch: 564 [128/17352 (1%)] Loss: -172356.578125\n",
      "Train Epoch: 564 [1536/17352 (9%)] Loss: -147193.906250\n",
      "Train Epoch: 564 [2944/17352 (17%)] Loss: -143686.875000\n",
      "Train Epoch: 564 [4352/17352 (25%)] Loss: -143554.468750\n",
      "Train Epoch: 564 [5760/17352 (33%)] Loss: -185763.484375\n",
      "Train Epoch: 564 [7168/17352 (41%)] Loss: -163455.796875\n",
      "Train Epoch: 564 [8576/17352 (49%)] Loss: -139804.625000\n",
      "Train Epoch: 564 [9984/17352 (58%)] Loss: -157258.156250\n",
      "Train Epoch: 564 [11392/17352 (66%)] Loss: -169401.781250\n",
      "Train Epoch: 564 [12800/17352 (74%)] Loss: -143223.531250\n",
      "Train Epoch: 564 [14208/17352 (82%)] Loss: -156888.031250\n",
      "Train Epoch: 564 [15536/17352 (90%)] Loss: -97055.148438\n",
      "Train Epoch: 564 [16349/17352 (94%)] Loss: -44309.265625\n",
      "Train Epoch: 564 [16989/17352 (98%)] Loss: -36093.828125\n",
      "    epoch          : 564\n",
      "    loss           : -136676.74118308251\n",
      "    val_loss       : -74110.61323649088\n",
      "Train Epoch: 565 [128/17352 (1%)] Loss: -131235.093750\n",
      "Train Epoch: 565 [1536/17352 (9%)] Loss: -136440.359375\n",
      "Train Epoch: 565 [2944/17352 (17%)] Loss: -126766.296875\n",
      "Train Epoch: 565 [4352/17352 (25%)] Loss: -117817.187500\n",
      "Train Epoch: 565 [5760/17352 (33%)] Loss: -107495.343750\n",
      "Train Epoch: 565 [7168/17352 (41%)] Loss: -149261.953125\n",
      "Train Epoch: 565 [8576/17352 (49%)] Loss: -125278.898438\n",
      "Train Epoch: 565 [9984/17352 (58%)] Loss: -162275.968750\n",
      "Train Epoch: 565 [11392/17352 (66%)] Loss: -143816.156250\n",
      "Train Epoch: 565 [12800/17352 (74%)] Loss: -169967.437500\n",
      "Train Epoch: 565 [14208/17352 (82%)] Loss: -136592.750000\n",
      "Train Epoch: 565 [15474/17352 (89%)] Loss: -50691.597656\n",
      "Train Epoch: 565 [16116/17352 (93%)] Loss: -46327.984375\n",
      "Train Epoch: 565 [17015/17352 (98%)] Loss: -107512.593750\n",
      "    epoch          : 565\n",
      "    loss           : -132440.41320555002\n",
      "    val_loss       : -77011.17624104818\n",
      "Train Epoch: 566 [128/17352 (1%)] Loss: -172506.265625\n",
      "Train Epoch: 566 [1536/17352 (9%)] Loss: -151009.328125\n",
      "Train Epoch: 566 [2944/17352 (17%)] Loss: -176969.406250\n",
      "Train Epoch: 566 [4352/17352 (25%)] Loss: -142075.000000\n",
      "Train Epoch: 566 [5760/17352 (33%)] Loss: -187810.500000\n",
      "Train Epoch: 566 [7168/17352 (41%)] Loss: -138848.265625\n",
      "Train Epoch: 566 [8576/17352 (49%)] Loss: -136147.843750\n",
      "Train Epoch: 566 [9984/17352 (58%)] Loss: -112196.937500\n",
      "Train Epoch: 566 [11392/17352 (66%)] Loss: -149098.296875\n",
      "Train Epoch: 566 [12800/17352 (74%)] Loss: -179259.687500\n",
      "Train Epoch: 566 [14208/17352 (82%)] Loss: -141657.468750\n",
      "Train Epoch: 566 [15546/17352 (90%)] Loss: -136277.343750\n",
      "Train Epoch: 566 [16218/17352 (93%)] Loss: -18250.408203\n",
      "Train Epoch: 566 [16905/17352 (97%)] Loss: -47637.273438\n",
      "    epoch          : 566\n",
      "    loss           : -143335.60498046875\n",
      "    val_loss       : -75896.68726806641\n",
      "Train Epoch: 567 [128/17352 (1%)] Loss: -122234.585938\n",
      "Train Epoch: 567 [1536/17352 (9%)] Loss: -167327.093750\n",
      "Train Epoch: 567 [2944/17352 (17%)] Loss: -131533.687500\n",
      "Train Epoch: 567 [4352/17352 (25%)] Loss: -69933.289062\n",
      "Train Epoch: 567 [5760/17352 (33%)] Loss: -123355.468750\n",
      "Train Epoch: 567 [7168/17352 (41%)] Loss: -132663.671875\n",
      "Train Epoch: 567 [8576/17352 (49%)] Loss: -120726.804688\n",
      "Train Epoch: 567 [9984/17352 (58%)] Loss: -166321.156250\n",
      "Train Epoch: 567 [11392/17352 (66%)] Loss: -149771.937500\n",
      "Train Epoch: 567 [12800/17352 (74%)] Loss: -164813.750000\n",
      "Train Epoch: 567 [14208/17352 (82%)] Loss: -118308.554688\n",
      "Train Epoch: 567 [15444/17352 (89%)] Loss: -29456.550781\n",
      "Train Epoch: 567 [16263/17352 (94%)] Loss: -80585.617188\n",
      "Train Epoch: 567 [17040/17352 (98%)] Loss: -102136.359375\n",
      "    epoch          : 567\n",
      "    loss           : -130627.08861649277\n",
      "    val_loss       : -76142.53423665365\n",
      "Train Epoch: 568 [128/17352 (1%)] Loss: -141553.937500\n",
      "Train Epoch: 568 [1536/17352 (9%)] Loss: -144844.656250\n",
      "Train Epoch: 568 [2944/17352 (17%)] Loss: -125472.562500\n",
      "Train Epoch: 568 [4352/17352 (25%)] Loss: -153412.218750\n",
      "Train Epoch: 568 [5760/17352 (33%)] Loss: -145090.421875\n",
      "Train Epoch: 568 [7168/17352 (41%)] Loss: -151829.875000\n",
      "Train Epoch: 568 [8576/17352 (49%)] Loss: -155334.031250\n",
      "Train Epoch: 568 [9984/17352 (58%)] Loss: -148603.500000\n",
      "Train Epoch: 568 [11392/17352 (66%)] Loss: -155088.390625\n",
      "Train Epoch: 568 [12800/17352 (74%)] Loss: -145180.562500\n",
      "Train Epoch: 568 [14208/17352 (82%)] Loss: -162603.187500\n",
      "Train Epoch: 568 [15459/17352 (89%)] Loss: -117904.460938\n",
      "Train Epoch: 568 [16204/17352 (93%)] Loss: -103804.718750\n",
      "Train Epoch: 568 [16873/17352 (97%)] Loss: -45452.351562\n",
      "    epoch          : 568\n",
      "    loss           : -139125.05744350355\n",
      "    val_loss       : -73401.16130777994\n",
      "Train Epoch: 569 [128/17352 (1%)] Loss: -146942.062500\n",
      "Train Epoch: 569 [1536/17352 (9%)] Loss: -142869.390625\n",
      "Train Epoch: 569 [2944/17352 (17%)] Loss: -140785.500000\n",
      "Train Epoch: 569 [4352/17352 (25%)] Loss: -149328.562500\n",
      "Train Epoch: 569 [5760/17352 (33%)] Loss: -163791.000000\n",
      "Train Epoch: 569 [7168/17352 (41%)] Loss: -159973.687500\n",
      "Train Epoch: 569 [8576/17352 (49%)] Loss: -173955.968750\n",
      "Train Epoch: 569 [9984/17352 (58%)] Loss: -176231.531250\n",
      "Train Epoch: 569 [11392/17352 (66%)] Loss: -145494.187500\n",
      "Train Epoch: 569 [12800/17352 (74%)] Loss: -170924.468750\n",
      "Train Epoch: 569 [14208/17352 (82%)] Loss: -174556.843750\n",
      "Train Epoch: 569 [15446/17352 (89%)] Loss: -90431.976562\n",
      "Train Epoch: 569 [16131/17352 (93%)] Loss: -17139.638672\n",
      "Train Epoch: 569 [16946/17352 (98%)] Loss: -82589.804688\n",
      "    epoch          : 569\n",
      "    loss           : -138210.2077177931\n",
      "    val_loss       : -64716.77177734375\n",
      "Train Epoch: 570 [128/17352 (1%)] Loss: -128916.500000\n",
      "Train Epoch: 570 [1536/17352 (9%)] Loss: -112571.140625\n",
      "Train Epoch: 570 [2944/17352 (17%)] Loss: -158189.656250\n",
      "Train Epoch: 570 [4352/17352 (25%)] Loss: -175649.328125\n",
      "Train Epoch: 570 [5760/17352 (33%)] Loss: -178165.328125\n",
      "Train Epoch: 570 [7168/17352 (41%)] Loss: -149755.828125\n",
      "Train Epoch: 570 [8576/17352 (49%)] Loss: -142084.593750\n",
      "Train Epoch: 570 [9984/17352 (58%)] Loss: -116855.906250\n",
      "Train Epoch: 570 [11392/17352 (66%)] Loss: -186640.343750\n",
      "Train Epoch: 570 [12800/17352 (74%)] Loss: -171641.843750\n",
      "Train Epoch: 570 [14208/17352 (82%)] Loss: -162408.515625\n",
      "Train Epoch: 570 [15467/17352 (89%)] Loss: -46989.000000\n",
      "Train Epoch: 570 [16118/17352 (93%)] Loss: -121192.093750\n",
      "Train Epoch: 570 [16968/17352 (98%)] Loss: -53640.746094\n",
      "    epoch          : 570\n",
      "    loss           : -141484.34610292575\n",
      "    val_loss       : -78695.06578776041\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch570.pth ...\n",
      "Train Epoch: 571 [128/17352 (1%)] Loss: -162977.625000\n",
      "Train Epoch: 571 [1536/17352 (9%)] Loss: -163544.718750\n",
      "Train Epoch: 571 [2944/17352 (17%)] Loss: -152904.609375\n",
      "Train Epoch: 571 [4352/17352 (25%)] Loss: -197103.031250\n",
      "Train Epoch: 571 [5760/17352 (33%)] Loss: -177121.156250\n",
      "Train Epoch: 571 [7168/17352 (41%)] Loss: -156727.156250\n",
      "Train Epoch: 571 [8576/17352 (49%)] Loss: -149657.375000\n",
      "Train Epoch: 571 [9984/17352 (58%)] Loss: -173276.625000\n",
      "Train Epoch: 571 [11392/17352 (66%)] Loss: -128108.320312\n",
      "Train Epoch: 571 [12800/17352 (74%)] Loss: -132097.421875\n",
      "Train Epoch: 571 [14208/17352 (82%)] Loss: -163542.687500\n",
      "Train Epoch: 571 [15521/17352 (89%)] Loss: -82567.390625\n",
      "Train Epoch: 571 [16350/17352 (94%)] Loss: -49029.703125\n",
      "Train Epoch: 571 [17003/17352 (98%)] Loss: 2472.660645\n",
      "    epoch          : 571\n",
      "    loss           : -132411.0343951539\n",
      "    val_loss       : -5230.088212076823\n",
      "Train Epoch: 572 [128/17352 (1%)] Loss: 23031.029297\n",
      "Train Epoch: 572 [1536/17352 (9%)] Loss: -111386.054688\n",
      "Train Epoch: 572 [2944/17352 (17%)] Loss: -120661.296875\n",
      "Train Epoch: 572 [4352/17352 (25%)] Loss: -119767.750000\n",
      "Train Epoch: 572 [5760/17352 (33%)] Loss: -146921.796875\n",
      "Train Epoch: 572 [7168/17352 (41%)] Loss: -135185.000000\n",
      "Train Epoch: 572 [8576/17352 (49%)] Loss: -158802.328125\n",
      "Train Epoch: 572 [9984/17352 (58%)] Loss: -173124.218750\n",
      "Train Epoch: 572 [11392/17352 (66%)] Loss: -160668.656250\n",
      "Train Epoch: 572 [12800/17352 (74%)] Loss: -169707.843750\n",
      "Train Epoch: 572 [14208/17352 (82%)] Loss: -157565.906250\n",
      "Train Epoch: 572 [15459/17352 (89%)] Loss: -18147.386719\n",
      "Train Epoch: 572 [16288/17352 (94%)] Loss: -48273.480469\n",
      "Train Epoch: 572 [17104/17352 (99%)] Loss: -85960.859375\n",
      "    epoch          : 572\n",
      "    loss           : -127639.47350173029\n",
      "    val_loss       : -79395.03409016927\n",
      "Train Epoch: 573 [128/17352 (1%)] Loss: -153762.765625\n",
      "Train Epoch: 573 [1536/17352 (9%)] Loss: -150297.218750\n",
      "Train Epoch: 573 [2944/17352 (17%)] Loss: -139720.156250\n",
      "Train Epoch: 573 [4352/17352 (25%)] Loss: -198890.843750\n",
      "Train Epoch: 573 [5760/17352 (33%)] Loss: -158701.203125\n",
      "Train Epoch: 573 [7168/17352 (41%)] Loss: -150212.468750\n",
      "Train Epoch: 573 [8576/17352 (49%)] Loss: -151569.234375\n",
      "Train Epoch: 573 [9984/17352 (58%)] Loss: -165845.687500\n",
      "Train Epoch: 573 [11392/17352 (66%)] Loss: -149920.015625\n",
      "Train Epoch: 573 [12800/17352 (74%)] Loss: -179191.718750\n",
      "Train Epoch: 573 [14208/17352 (82%)] Loss: -166082.843750\n",
      "Train Epoch: 573 [15527/17352 (89%)] Loss: -88752.171875\n",
      "Train Epoch: 573 [16213/17352 (93%)] Loss: -84070.945312\n",
      "Train Epoch: 573 [16901/17352 (97%)] Loss: -55768.605469\n",
      "    epoch          : 573\n",
      "    loss           : -142967.23814361368\n",
      "    val_loss       : -64181.088423665366\n",
      "Train Epoch: 574 [128/17352 (1%)] Loss: -136841.718750\n",
      "Train Epoch: 574 [1536/17352 (9%)] Loss: -151916.359375\n",
      "Train Epoch: 574 [2944/17352 (17%)] Loss: -161467.828125\n",
      "Train Epoch: 574 [4352/17352 (25%)] Loss: -170575.578125\n",
      "Train Epoch: 574 [5760/17352 (33%)] Loss: -145749.343750\n",
      "Train Epoch: 574 [7168/17352 (41%)] Loss: -150918.984375\n",
      "Train Epoch: 574 [8576/17352 (49%)] Loss: -161076.828125\n",
      "Train Epoch: 574 [9984/17352 (58%)] Loss: -169793.656250\n",
      "Train Epoch: 574 [11392/17352 (66%)] Loss: -164513.625000\n",
      "Train Epoch: 574 [12800/17352 (74%)] Loss: -137036.843750\n",
      "Train Epoch: 574 [14208/17352 (82%)] Loss: -180216.031250\n",
      "Train Epoch: 574 [15499/17352 (89%)] Loss: -99643.757812\n",
      "Train Epoch: 574 [16359/17352 (94%)] Loss: -120531.421875\n",
      "Train Epoch: 574 [17015/17352 (98%)] Loss: -43434.140625\n",
      "    epoch          : 574\n",
      "    loss           : -138553.08709020424\n",
      "    val_loss       : -41554.92267659505\n",
      "Train Epoch: 575 [128/17352 (1%)] Loss: -95601.218750\n",
      "Train Epoch: 575 [1536/17352 (9%)] Loss: -97722.015625\n",
      "Train Epoch: 575 [2944/17352 (17%)] Loss: -154351.078125\n",
      "Train Epoch: 575 [4352/17352 (25%)] Loss: -157399.015625\n",
      "Train Epoch: 575 [5760/17352 (33%)] Loss: -195214.703125\n",
      "Train Epoch: 575 [7168/17352 (41%)] Loss: -163984.062500\n",
      "Train Epoch: 575 [8576/17352 (49%)] Loss: -163602.062500\n",
      "Train Epoch: 575 [9984/17352 (58%)] Loss: -168792.906250\n",
      "Train Epoch: 575 [11392/17352 (66%)] Loss: -148757.437500\n",
      "Train Epoch: 575 [12800/17352 (74%)] Loss: -150916.656250\n",
      "Train Epoch: 575 [14208/17352 (82%)] Loss: -180843.921875\n",
      "Train Epoch: 575 [15509/17352 (89%)] Loss: -144192.765625\n",
      "Train Epoch: 575 [16178/17352 (93%)] Loss: -84703.015625\n",
      "Train Epoch: 575 [16989/17352 (98%)] Loss: -58123.519531\n",
      "    epoch          : 575\n",
      "    loss           : -137885.25205143666\n",
      "    val_loss       : -60895.78260091146\n",
      "Train Epoch: 576 [128/17352 (1%)] Loss: -115229.710938\n",
      "Train Epoch: 576 [1536/17352 (9%)] Loss: -147786.625000\n",
      "Train Epoch: 576 [2944/17352 (17%)] Loss: -168274.140625\n",
      "Train Epoch: 576 [4352/17352 (25%)] Loss: -130409.382812\n",
      "Train Epoch: 576 [5760/17352 (33%)] Loss: -164130.343750\n",
      "Train Epoch: 576 [7168/17352 (41%)] Loss: -140514.187500\n",
      "Train Epoch: 576 [8576/17352 (49%)] Loss: -126356.687500\n",
      "Train Epoch: 576 [9984/17352 (58%)] Loss: -143510.109375\n",
      "Train Epoch: 576 [11392/17352 (66%)] Loss: -163565.781250\n",
      "Train Epoch: 576 [12800/17352 (74%)] Loss: -134592.859375\n",
      "Train Epoch: 576 [14208/17352 (82%)] Loss: -142778.609375\n",
      "Train Epoch: 576 [15442/17352 (89%)] Loss: -78731.070312\n",
      "Train Epoch: 576 [16157/17352 (93%)] Loss: -12847.523438\n",
      "Train Epoch: 576 [16953/17352 (98%)] Loss: -112681.789062\n",
      "    epoch          : 576\n",
      "    loss           : -137661.3756226405\n",
      "    val_loss       : -76394.18279215494\n",
      "Train Epoch: 577 [128/17352 (1%)] Loss: -167743.000000\n",
      "Train Epoch: 577 [1536/17352 (9%)] Loss: -168355.562500\n",
      "Train Epoch: 577 [2944/17352 (17%)] Loss: -93505.601562\n",
      "Train Epoch: 577 [4352/17352 (25%)] Loss: -151413.125000\n",
      "Train Epoch: 577 [5760/17352 (33%)] Loss: -130829.164062\n",
      "Train Epoch: 577 [7168/17352 (41%)] Loss: -177258.859375\n",
      "Train Epoch: 577 [8576/17352 (49%)] Loss: -152298.843750\n",
      "Train Epoch: 577 [9984/17352 (58%)] Loss: -143737.578125\n",
      "Train Epoch: 577 [11392/17352 (66%)] Loss: -160244.250000\n",
      "Train Epoch: 577 [12800/17352 (74%)] Loss: -150767.593750\n",
      "Train Epoch: 577 [14208/17352 (82%)] Loss: -141655.718750\n",
      "Train Epoch: 577 [15547/17352 (90%)] Loss: -159101.125000\n",
      "Train Epoch: 577 [16259/17352 (94%)] Loss: -3759.774658\n",
      "Train Epoch: 577 [17034/17352 (98%)] Loss: -126854.593750\n",
      "    epoch          : 577\n",
      "    loss           : -136121.71396058358\n",
      "    val_loss       : -79877.61456705729\n",
      "Train Epoch: 578 [128/17352 (1%)] Loss: -165052.640625\n",
      "Train Epoch: 578 [1536/17352 (9%)] Loss: -147379.750000\n",
      "Train Epoch: 578 [2944/17352 (17%)] Loss: -170696.578125\n",
      "Train Epoch: 578 [4352/17352 (25%)] Loss: -134620.078125\n",
      "Train Epoch: 578 [5760/17352 (33%)] Loss: -121631.312500\n",
      "Train Epoch: 578 [7168/17352 (41%)] Loss: -153673.796875\n",
      "Train Epoch: 578 [8576/17352 (49%)] Loss: -165548.343750\n",
      "Train Epoch: 578 [9984/17352 (58%)] Loss: -143833.500000\n",
      "Train Epoch: 578 [11392/17352 (66%)] Loss: -138994.671875\n",
      "Train Epoch: 578 [12800/17352 (74%)] Loss: -162068.531250\n",
      "Train Epoch: 578 [14208/17352 (82%)] Loss: -149018.218750\n",
      "Train Epoch: 578 [15540/17352 (90%)] Loss: -116832.421875\n",
      "Train Epoch: 578 [16189/17352 (93%)] Loss: -42725.613281\n",
      "Train Epoch: 578 [16877/17352 (97%)] Loss: -63196.187500\n",
      "    epoch          : 578\n",
      "    loss           : -134868.8131324717\n",
      "    val_loss       : -80371.22451171876\n",
      "Train Epoch: 579 [128/17352 (1%)] Loss: -183319.296875\n",
      "Train Epoch: 579 [1536/17352 (9%)] Loss: -121485.703125\n",
      "Train Epoch: 579 [2944/17352 (17%)] Loss: -115856.843750\n",
      "Train Epoch: 579 [4352/17352 (25%)] Loss: -138006.218750\n",
      "Train Epoch: 579 [5760/17352 (33%)] Loss: -167978.046875\n",
      "Train Epoch: 579 [7168/17352 (41%)] Loss: -188327.359375\n",
      "Train Epoch: 579 [8576/17352 (49%)] Loss: -153212.031250\n",
      "Train Epoch: 579 [9984/17352 (58%)] Loss: -144387.156250\n",
      "Train Epoch: 579 [11392/17352 (66%)] Loss: -150287.328125\n",
      "Train Epoch: 579 [12800/17352 (74%)] Loss: -149374.750000\n",
      "Train Epoch: 579 [14208/17352 (82%)] Loss: -159034.656250\n",
      "Train Epoch: 579 [15535/17352 (90%)] Loss: -99643.875000\n",
      "Train Epoch: 579 [16373/17352 (94%)] Loss: -112157.859375\n",
      "Train Epoch: 579 [17047/17352 (98%)] Loss: -50164.472656\n",
      "    epoch          : 579\n",
      "    loss           : -137579.41973999844\n",
      "    val_loss       : -79942.25736083984\n",
      "Train Epoch: 580 [128/17352 (1%)] Loss: -139233.062500\n",
      "Train Epoch: 580 [1536/17352 (9%)] Loss: -155987.453125\n",
      "Train Epoch: 580 [2944/17352 (17%)] Loss: -162421.875000\n",
      "Train Epoch: 580 [4352/17352 (25%)] Loss: -171500.875000\n",
      "Train Epoch: 580 [5760/17352 (33%)] Loss: -195744.968750\n",
      "Train Epoch: 580 [7168/17352 (41%)] Loss: -158718.375000\n",
      "Train Epoch: 580 [8576/17352 (49%)] Loss: -152833.187500\n",
      "Train Epoch: 580 [9984/17352 (58%)] Loss: -187570.359375\n",
      "Train Epoch: 580 [11392/17352 (66%)] Loss: -145494.625000\n",
      "Train Epoch: 580 [12800/17352 (74%)] Loss: -194769.687500\n",
      "Train Epoch: 580 [14208/17352 (82%)] Loss: -164556.812500\n",
      "Train Epoch: 580 [15515/17352 (89%)] Loss: -93698.429688\n",
      "Train Epoch: 580 [16095/17352 (93%)] Loss: -117998.703125\n",
      "Train Epoch: 580 [16917/17352 (97%)] Loss: -118652.234375\n",
      "    epoch          : 580\n",
      "    loss           : -142616.03026032928\n",
      "    val_loss       : -76124.41892089843\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch580.pth ...\n",
      "Train Epoch: 581 [128/17352 (1%)] Loss: -138435.859375\n",
      "Train Epoch: 581 [1536/17352 (9%)] Loss: -168680.312500\n",
      "Train Epoch: 581 [2944/17352 (17%)] Loss: -166199.046875\n",
      "Train Epoch: 581 [4352/17352 (25%)] Loss: -181533.015625\n",
      "Train Epoch: 581 [5760/17352 (33%)] Loss: -127408.125000\n",
      "Train Epoch: 581 [7168/17352 (41%)] Loss: -157690.453125\n",
      "Train Epoch: 581 [8576/17352 (49%)] Loss: -167725.531250\n",
      "Train Epoch: 581 [9984/17352 (58%)] Loss: -142476.203125\n",
      "Train Epoch: 581 [11392/17352 (66%)] Loss: -162423.437500\n",
      "Train Epoch: 581 [12800/17352 (74%)] Loss: -149661.578125\n",
      "Train Epoch: 581 [14208/17352 (82%)] Loss: -187504.625000\n",
      "Train Epoch: 581 [15540/17352 (90%)] Loss: -106240.304688\n",
      "Train Epoch: 581 [16367/17352 (94%)] Loss: -76971.085938\n",
      "Train Epoch: 581 [16961/17352 (98%)] Loss: -4646.483398\n",
      "    epoch          : 581\n",
      "    loss           : -147407.90332686662\n",
      "    val_loss       : -64631.15420328776\n",
      "Train Epoch: 582 [128/17352 (1%)] Loss: -138546.656250\n",
      "Train Epoch: 582 [1536/17352 (9%)] Loss: -156634.031250\n",
      "Train Epoch: 582 [2944/17352 (17%)] Loss: -170404.984375\n",
      "Train Epoch: 582 [4352/17352 (25%)] Loss: -178072.140625\n",
      "Train Epoch: 582 [5760/17352 (33%)] Loss: -170381.015625\n",
      "Train Epoch: 582 [7168/17352 (41%)] Loss: -120020.734375\n",
      "Train Epoch: 582 [8576/17352 (49%)] Loss: -156241.421875\n",
      "Train Epoch: 582 [9984/17352 (58%)] Loss: -121913.195312\n",
      "Train Epoch: 582 [11392/17352 (66%)] Loss: -137947.625000\n",
      "Train Epoch: 582 [12800/17352 (74%)] Loss: -169545.187500\n",
      "Train Epoch: 582 [14208/17352 (82%)] Loss: -150182.625000\n",
      "Train Epoch: 582 [15546/17352 (90%)] Loss: -94183.304688\n",
      "Train Epoch: 582 [16270/17352 (94%)] Loss: -104652.906250\n",
      "Train Epoch: 582 [17026/17352 (98%)] Loss: -111298.343750\n",
      "    epoch          : 582\n",
      "    loss           : -138636.12898817638\n",
      "    val_loss       : -50912.614013671875\n",
      "Train Epoch: 583 [128/17352 (1%)] Loss: -117889.515625\n",
      "Train Epoch: 583 [1536/17352 (9%)] Loss: -145932.812500\n",
      "Train Epoch: 583 [2944/17352 (17%)] Loss: -123224.664062\n",
      "Train Epoch: 583 [4352/17352 (25%)] Loss: -142930.484375\n",
      "Train Epoch: 583 [5760/17352 (33%)] Loss: -177310.515625\n",
      "Train Epoch: 583 [7168/17352 (41%)] Loss: -141162.031250\n",
      "Train Epoch: 583 [8576/17352 (49%)] Loss: -161052.234375\n",
      "Train Epoch: 583 [9984/17352 (58%)] Loss: -134046.484375\n",
      "Train Epoch: 583 [11392/17352 (66%)] Loss: -178732.953125\n",
      "Train Epoch: 583 [12800/17352 (74%)] Loss: -160731.718750\n",
      "Train Epoch: 583 [14208/17352 (82%)] Loss: -137501.281250\n",
      "Train Epoch: 583 [15546/17352 (90%)] Loss: -86632.500000\n",
      "Train Epoch: 583 [16264/17352 (94%)] Loss: -74274.703125\n",
      "Train Epoch: 583 [17042/17352 (98%)] Loss: -4510.971191\n",
      "    epoch          : 583\n",
      "    loss           : -139043.9256567219\n",
      "    val_loss       : -66774.04247639974\n",
      "Train Epoch: 584 [128/17352 (1%)] Loss: -138505.593750\n",
      "Train Epoch: 584 [1536/17352 (9%)] Loss: -89941.992188\n",
      "Train Epoch: 584 [2944/17352 (17%)] Loss: -137404.828125\n",
      "Train Epoch: 584 [4352/17352 (25%)] Loss: -149552.718750\n",
      "Train Epoch: 584 [5760/17352 (33%)] Loss: -134527.687500\n",
      "Train Epoch: 584 [7168/17352 (41%)] Loss: -151235.328125\n",
      "Train Epoch: 584 [8576/17352 (49%)] Loss: -127516.539062\n",
      "Train Epoch: 584 [9984/17352 (58%)] Loss: -174648.125000\n",
      "Train Epoch: 584 [11392/17352 (66%)] Loss: -170369.359375\n",
      "Train Epoch: 584 [12800/17352 (74%)] Loss: -148354.937500\n",
      "Train Epoch: 584 [14208/17352 (82%)] Loss: -155515.453125\n",
      "Train Epoch: 584 [15517/17352 (89%)] Loss: -90215.320312\n",
      "Train Epoch: 584 [16349/17352 (94%)] Loss: -109507.046875\n",
      "Train Epoch: 584 [17050/17352 (98%)] Loss: -21453.261719\n",
      "    epoch          : 584\n",
      "    loss           : -134753.63327292626\n",
      "    val_loss       : -77866.37709147135\n",
      "Train Epoch: 585 [128/17352 (1%)] Loss: -168809.796875\n",
      "Train Epoch: 585 [1536/17352 (9%)] Loss: -129061.812500\n",
      "Train Epoch: 585 [2944/17352 (17%)] Loss: -173362.875000\n",
      "Train Epoch: 585 [4352/17352 (25%)] Loss: -168965.281250\n",
      "Train Epoch: 585 [5760/17352 (33%)] Loss: -154766.968750\n",
      "Train Epoch: 585 [7168/17352 (41%)] Loss: -146129.843750\n",
      "Train Epoch: 585 [8576/17352 (49%)] Loss: -145865.718750\n",
      "Train Epoch: 585 [9984/17352 (58%)] Loss: -157811.750000\n",
      "Train Epoch: 585 [11392/17352 (66%)] Loss: -167369.078125\n",
      "Train Epoch: 585 [12800/17352 (74%)] Loss: -141558.953125\n",
      "Train Epoch: 585 [14208/17352 (82%)] Loss: -155239.562500\n",
      "Train Epoch: 585 [15520/17352 (89%)] Loss: -75976.382812\n",
      "Train Epoch: 585 [16159/17352 (93%)] Loss: -101987.671875\n",
      "Train Epoch: 585 [17053/17352 (98%)] Loss: -146320.781250\n",
      "    epoch          : 585\n",
      "    loss           : -145028.0255905254\n",
      "    val_loss       : -71937.59608154297\n",
      "Train Epoch: 586 [128/17352 (1%)] Loss: -157854.015625\n",
      "Train Epoch: 586 [1536/17352 (9%)] Loss: -128648.867188\n",
      "Train Epoch: 586 [2944/17352 (17%)] Loss: -122806.867188\n",
      "Train Epoch: 586 [4352/17352 (25%)] Loss: -141887.890625\n",
      "Train Epoch: 586 [5760/17352 (33%)] Loss: -160759.265625\n",
      "Train Epoch: 586 [7168/17352 (41%)] Loss: -164087.718750\n",
      "Train Epoch: 586 [8576/17352 (49%)] Loss: -188518.531250\n",
      "Train Epoch: 586 [9984/17352 (58%)] Loss: -164463.015625\n",
      "Train Epoch: 586 [11392/17352 (66%)] Loss: -134754.109375\n",
      "Train Epoch: 586 [12800/17352 (74%)] Loss: -123425.492188\n",
      "Train Epoch: 586 [14208/17352 (82%)] Loss: -131935.531250\n",
      "Train Epoch: 586 [15543/17352 (90%)] Loss: -79608.468750\n",
      "Train Epoch: 586 [16456/17352 (95%)] Loss: -114960.335938\n",
      "Train Epoch: 586 [17055/17352 (98%)] Loss: -133574.968750\n",
      "    epoch          : 586\n",
      "    loss           : -141497.36299614617\n",
      "    val_loss       : -81987.74022623697\n",
      "Train Epoch: 587 [128/17352 (1%)] Loss: -178149.593750\n",
      "Train Epoch: 587 [1536/17352 (9%)] Loss: -158567.843750\n",
      "Train Epoch: 587 [2944/17352 (17%)] Loss: -123457.078125\n",
      "Train Epoch: 587 [4352/17352 (25%)] Loss: -160976.578125\n",
      "Train Epoch: 587 [5760/17352 (33%)] Loss: -118019.828125\n",
      "Train Epoch: 587 [7168/17352 (41%)] Loss: -111993.078125\n",
      "Train Epoch: 587 [8576/17352 (49%)] Loss: -124566.031250\n",
      "Train Epoch: 587 [9984/17352 (58%)] Loss: -191872.921875\n",
      "Train Epoch: 587 [11392/17352 (66%)] Loss: -141065.656250\n",
      "Train Epoch: 587 [12800/17352 (74%)] Loss: -171436.015625\n",
      "Train Epoch: 587 [14208/17352 (82%)] Loss: -134518.671875\n",
      "Train Epoch: 587 [15440/17352 (89%)] Loss: -99603.656250\n",
      "Train Epoch: 587 [16314/17352 (94%)] Loss: -92122.375000\n",
      "Train Epoch: 587 [17003/17352 (98%)] Loss: -54802.265625\n",
      "    epoch          : 587\n",
      "    loss           : -137753.4376605757\n",
      "    val_loss       : -77152.62104492188\n",
      "Train Epoch: 588 [128/17352 (1%)] Loss: -159365.562500\n",
      "Train Epoch: 588 [1536/17352 (9%)] Loss: -185446.781250\n",
      "Train Epoch: 588 [2944/17352 (17%)] Loss: -151129.250000\n",
      "Train Epoch: 588 [4352/17352 (25%)] Loss: -111658.531250\n",
      "Train Epoch: 588 [5760/17352 (33%)] Loss: -117109.421875\n",
      "Train Epoch: 588 [7168/17352 (41%)] Loss: -126444.484375\n",
      "Train Epoch: 588 [8576/17352 (49%)] Loss: -156629.468750\n",
      "Train Epoch: 588 [9984/17352 (58%)] Loss: -141635.093750\n",
      "Train Epoch: 588 [11392/17352 (66%)] Loss: -159406.593750\n",
      "Train Epoch: 588 [12800/17352 (74%)] Loss: -145558.921875\n",
      "Train Epoch: 588 [14208/17352 (82%)] Loss: -199048.265625\n",
      "Train Epoch: 588 [15574/17352 (90%)] Loss: -129290.187500\n",
      "Train Epoch: 588 [16255/17352 (94%)] Loss: -131419.250000\n",
      "Train Epoch: 588 [16991/17352 (98%)] Loss: -17780.220703\n",
      "    epoch          : 588\n",
      "    loss           : -137555.4155666684\n",
      "    val_loss       : -76354.73736979166\n",
      "Train Epoch: 589 [128/17352 (1%)] Loss: -157417.000000\n",
      "Train Epoch: 589 [1536/17352 (9%)] Loss: -177527.468750\n",
      "Train Epoch: 589 [2944/17352 (17%)] Loss: -138960.343750\n",
      "Train Epoch: 589 [4352/17352 (25%)] Loss: -156355.437500\n",
      "Train Epoch: 589 [5760/17352 (33%)] Loss: -166696.750000\n",
      "Train Epoch: 589 [7168/17352 (41%)] Loss: -176779.281250\n",
      "Train Epoch: 589 [8576/17352 (49%)] Loss: -146160.046875\n",
      "Train Epoch: 589 [9984/17352 (58%)] Loss: -169090.781250\n",
      "Train Epoch: 589 [11392/17352 (66%)] Loss: -160834.000000\n",
      "Train Epoch: 589 [12800/17352 (74%)] Loss: -155452.531250\n",
      "Train Epoch: 589 [14208/17352 (82%)] Loss: -185783.031250\n",
      "Train Epoch: 589 [15453/17352 (89%)] Loss: -20054.066406\n",
      "Train Epoch: 589 [16099/17352 (93%)] Loss: -63472.242188\n",
      "Train Epoch: 589 [16956/17352 (98%)] Loss: -66415.523438\n",
      "    epoch          : 589\n",
      "    loss           : -145794.72686398908\n",
      "    val_loss       : -78714.8067586263\n",
      "Train Epoch: 590 [128/17352 (1%)] Loss: -148699.484375\n",
      "Train Epoch: 590 [1536/17352 (9%)] Loss: -145989.562500\n",
      "Train Epoch: 590 [2944/17352 (17%)] Loss: -156788.875000\n",
      "Train Epoch: 590 [4352/17352 (25%)] Loss: -165214.953125\n",
      "Train Epoch: 590 [5760/17352 (33%)] Loss: -158338.078125\n",
      "Train Epoch: 590 [7168/17352 (41%)] Loss: -147242.562500\n",
      "Train Epoch: 590 [8576/17352 (49%)] Loss: -136760.375000\n",
      "Train Epoch: 590 [9984/17352 (58%)] Loss: -142284.968750\n",
      "Train Epoch: 590 [11392/17352 (66%)] Loss: -168338.093750\n",
      "Train Epoch: 590 [12800/17352 (74%)] Loss: -128585.320312\n",
      "Train Epoch: 590 [14208/17352 (82%)] Loss: -155648.906250\n",
      "Train Epoch: 590 [15368/17352 (89%)] Loss: -3764.752197\n",
      "Train Epoch: 590 [16139/17352 (93%)] Loss: -96719.117188\n",
      "Train Epoch: 590 [17014/17352 (98%)] Loss: -83223.000000\n",
      "    epoch          : 590\n",
      "    loss           : -140241.6795809957\n",
      "    val_loss       : -74749.17815755209\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch590.pth ...\n",
      "Train Epoch: 591 [128/17352 (1%)] Loss: -173968.843750\n",
      "Train Epoch: 591 [1536/17352 (9%)] Loss: -138491.265625\n",
      "Train Epoch: 591 [2944/17352 (17%)] Loss: -138174.156250\n",
      "Train Epoch: 591 [4352/17352 (25%)] Loss: -154401.656250\n",
      "Train Epoch: 591 [5760/17352 (33%)] Loss: -133932.156250\n",
      "Train Epoch: 591 [7168/17352 (41%)] Loss: -159924.187500\n",
      "Train Epoch: 591 [8576/17352 (49%)] Loss: -120045.429688\n",
      "Train Epoch: 591 [9984/17352 (58%)] Loss: -131192.437500\n",
      "Train Epoch: 591 [11392/17352 (66%)] Loss: -167947.281250\n",
      "Train Epoch: 591 [12800/17352 (74%)] Loss: -188580.656250\n",
      "Train Epoch: 591 [14208/17352 (82%)] Loss: -169495.765625\n",
      "Train Epoch: 591 [15557/17352 (90%)] Loss: -149551.406250\n",
      "Train Epoch: 591 [16339/17352 (94%)] Loss: -18884.570312\n",
      "Train Epoch: 591 [16967/17352 (98%)] Loss: -84598.429688\n",
      "    epoch          : 591\n",
      "    loss           : -137559.89221437185\n",
      "    val_loss       : -81431.09918619791\n",
      "Train Epoch: 592 [128/17352 (1%)] Loss: -150816.750000\n",
      "Train Epoch: 592 [1536/17352 (9%)] Loss: -129902.492188\n",
      "Train Epoch: 592 [2944/17352 (17%)] Loss: -148073.843750\n",
      "Train Epoch: 592 [4352/17352 (25%)] Loss: -172921.781250\n",
      "Train Epoch: 592 [5760/17352 (33%)] Loss: -135627.968750\n",
      "Train Epoch: 592 [7168/17352 (41%)] Loss: -171704.765625\n",
      "Train Epoch: 592 [8576/17352 (49%)] Loss: -157637.250000\n",
      "Train Epoch: 592 [9984/17352 (58%)] Loss: -196712.406250\n",
      "Train Epoch: 592 [11392/17352 (66%)] Loss: -158475.406250\n",
      "Train Epoch: 592 [12800/17352 (74%)] Loss: -158477.187500\n",
      "Train Epoch: 592 [14208/17352 (82%)] Loss: -125050.101562\n",
      "Train Epoch: 592 [15453/17352 (89%)] Loss: -101279.843750\n",
      "Train Epoch: 592 [16138/17352 (93%)] Loss: -3003.144287\n",
      "Train Epoch: 592 [16970/17352 (98%)] Loss: -118370.875000\n",
      "    epoch          : 592\n",
      "    loss           : -145860.62759378934\n",
      "    val_loss       : -77116.2233601888\n",
      "Train Epoch: 593 [128/17352 (1%)] Loss: -125034.140625\n",
      "Train Epoch: 593 [1536/17352 (9%)] Loss: -139972.984375\n",
      "Train Epoch: 593 [2944/17352 (17%)] Loss: -155702.328125\n",
      "Train Epoch: 593 [4352/17352 (25%)] Loss: -178257.968750\n",
      "Train Epoch: 593 [5760/17352 (33%)] Loss: -187370.968750\n",
      "Train Epoch: 593 [7168/17352 (41%)] Loss: -133830.484375\n",
      "Train Epoch: 593 [8576/17352 (49%)] Loss: -154850.921875\n",
      "Train Epoch: 593 [9984/17352 (58%)] Loss: -167057.937500\n",
      "Train Epoch: 593 [11392/17352 (66%)] Loss: -196796.468750\n",
      "Train Epoch: 593 [12800/17352 (74%)] Loss: -143381.078125\n",
      "Train Epoch: 593 [14208/17352 (82%)] Loss: -194353.953125\n",
      "Train Epoch: 593 [15546/17352 (90%)] Loss: -108505.414062\n",
      "Train Epoch: 593 [16306/17352 (94%)] Loss: -4016.119385\n",
      "Train Epoch: 593 [17048/17352 (98%)] Loss: -43090.003906\n",
      "    epoch          : 593\n",
      "    loss           : -146704.16878965238\n",
      "    val_loss       : -79424.478519694\n",
      "Train Epoch: 594 [128/17352 (1%)] Loss: -179506.234375\n",
      "Train Epoch: 594 [1536/17352 (9%)] Loss: -173701.609375\n",
      "Train Epoch: 594 [2944/17352 (17%)] Loss: -127822.781250\n",
      "Train Epoch: 594 [4352/17352 (25%)] Loss: -143430.000000\n",
      "Train Epoch: 594 [5760/17352 (33%)] Loss: -187957.734375\n",
      "Train Epoch: 594 [7168/17352 (41%)] Loss: -132236.500000\n",
      "Train Epoch: 594 [8576/17352 (49%)] Loss: -112331.921875\n",
      "Train Epoch: 594 [9984/17352 (58%)] Loss: -141561.906250\n",
      "Train Epoch: 594 [11392/17352 (66%)] Loss: -151514.437500\n",
      "Train Epoch: 594 [12800/17352 (74%)] Loss: -146557.875000\n",
      "Train Epoch: 594 [14208/17352 (82%)] Loss: -135867.234375\n",
      "Train Epoch: 594 [15495/17352 (89%)] Loss: -109913.453125\n",
      "Train Epoch: 594 [16476/17352 (95%)] Loss: -89715.500000\n",
      "Train Epoch: 594 [17071/17352 (98%)] Loss: -80914.937500\n",
      "    epoch          : 594\n",
      "    loss           : -136274.0699880715\n",
      "    val_loss       : -82084.6087524414\n",
      "Train Epoch: 595 [128/17352 (1%)] Loss: -135575.890625\n",
      "Train Epoch: 595 [1536/17352 (9%)] Loss: -159379.546875\n",
      "Train Epoch: 595 [2944/17352 (17%)] Loss: -156617.031250\n",
      "Train Epoch: 595 [4352/17352 (25%)] Loss: -123655.812500\n",
      "Train Epoch: 595 [5760/17352 (33%)] Loss: -168009.281250\n",
      "Train Epoch: 595 [7168/17352 (41%)] Loss: -178594.531250\n",
      "Train Epoch: 595 [8576/17352 (49%)] Loss: -167064.937500\n",
      "Train Epoch: 595 [9984/17352 (58%)] Loss: -182112.390625\n",
      "Train Epoch: 595 [11392/17352 (66%)] Loss: -153890.343750\n",
      "Train Epoch: 595 [12800/17352 (74%)] Loss: -168372.906250\n",
      "Train Epoch: 595 [14208/17352 (82%)] Loss: -153212.843750\n",
      "Train Epoch: 595 [15474/17352 (89%)] Loss: -96927.968750\n",
      "Train Epoch: 595 [16224/17352 (93%)] Loss: -123866.117188\n",
      "Train Epoch: 595 [16983/17352 (98%)] Loss: -189716.609375\n",
      "    epoch          : 595\n",
      "    loss           : -143443.4488517198\n",
      "    val_loss       : -74907.80455729166\n",
      "Train Epoch: 596 [128/17352 (1%)] Loss: -184483.296875\n",
      "Train Epoch: 596 [1536/17352 (9%)] Loss: -198525.000000\n",
      "Train Epoch: 596 [2944/17352 (17%)] Loss: -190058.890625\n",
      "Train Epoch: 596 [4352/17352 (25%)] Loss: -171240.656250\n",
      "Train Epoch: 596 [5760/17352 (33%)] Loss: -170581.656250\n",
      "Train Epoch: 596 [7168/17352 (41%)] Loss: -152794.203125\n",
      "Train Epoch: 596 [8576/17352 (49%)] Loss: -161247.296875\n",
      "Train Epoch: 596 [9984/17352 (58%)] Loss: -110489.210938\n",
      "Train Epoch: 596 [11392/17352 (66%)] Loss: -185868.406250\n",
      "Train Epoch: 596 [12800/17352 (74%)] Loss: -168020.718750\n",
      "Train Epoch: 596 [14208/17352 (82%)] Loss: -171902.234375\n",
      "Train Epoch: 596 [15467/17352 (89%)] Loss: -44126.546875\n",
      "Train Epoch: 596 [16274/17352 (94%)] Loss: -138638.687500\n",
      "Train Epoch: 596 [17094/17352 (99%)] Loss: -71730.929688\n",
      "    epoch          : 596\n",
      "    loss           : -141240.65497194842\n",
      "    val_loss       : -80656.9901570638\n",
      "Train Epoch: 597 [128/17352 (1%)] Loss: -180420.203125\n",
      "Train Epoch: 597 [1536/17352 (9%)] Loss: -156972.468750\n",
      "Train Epoch: 597 [2944/17352 (17%)] Loss: -180127.187500\n",
      "Train Epoch: 597 [4352/17352 (25%)] Loss: -144726.218750\n",
      "Train Epoch: 597 [5760/17352 (33%)] Loss: -147600.171875\n",
      "Train Epoch: 597 [7168/17352 (41%)] Loss: -96071.609375\n",
      "Train Epoch: 597 [8576/17352 (49%)] Loss: -115047.507812\n",
      "Train Epoch: 597 [9984/17352 (58%)] Loss: -126653.046875\n",
      "Train Epoch: 597 [11392/17352 (66%)] Loss: -147687.015625\n",
      "Train Epoch: 597 [12800/17352 (74%)] Loss: -182618.562500\n",
      "Train Epoch: 597 [14208/17352 (82%)] Loss: -137350.328125\n",
      "Train Epoch: 597 [15530/17352 (89%)] Loss: -62773.562500\n",
      "Train Epoch: 597 [16292/17352 (94%)] Loss: -145582.250000\n",
      "Train Epoch: 597 [17128/17352 (99%)] Loss: -87400.570312\n",
      "    epoch          : 597\n",
      "    loss           : -127985.72208931942\n",
      "    val_loss       : -72397.13870849609\n",
      "Train Epoch: 598 [128/17352 (1%)] Loss: -119359.679688\n",
      "Train Epoch: 598 [1536/17352 (9%)] Loss: -138455.703125\n",
      "Train Epoch: 598 [2944/17352 (17%)] Loss: -146425.125000\n",
      "Train Epoch: 598 [4352/17352 (25%)] Loss: -169724.734375\n",
      "Train Epoch: 598 [5760/17352 (33%)] Loss: -173128.984375\n",
      "Train Epoch: 598 [7168/17352 (41%)] Loss: -177876.593750\n",
      "Train Epoch: 598 [8576/17352 (49%)] Loss: -141227.015625\n",
      "Train Epoch: 598 [9984/17352 (58%)] Loss: -158359.828125\n",
      "Train Epoch: 598 [11392/17352 (66%)] Loss: -181984.312500\n",
      "Train Epoch: 598 [12800/17352 (74%)] Loss: -176632.093750\n",
      "Train Epoch: 598 [14208/17352 (82%)] Loss: -141468.593750\n",
      "Train Epoch: 598 [15481/17352 (89%)] Loss: -46175.734375\n",
      "Train Epoch: 598 [16405/17352 (95%)] Loss: -42055.289062\n",
      "Train Epoch: 598 [17091/17352 (98%)] Loss: -48970.765625\n",
      "    epoch          : 598\n",
      "    loss           : -137379.1100730128\n",
      "    val_loss       : -69102.26899820963\n",
      "Train Epoch: 599 [128/17352 (1%)] Loss: -146595.656250\n",
      "Train Epoch: 599 [1536/17352 (9%)] Loss: -141107.562500\n",
      "Train Epoch: 599 [2944/17352 (17%)] Loss: -128762.960938\n",
      "Train Epoch: 599 [4352/17352 (25%)] Loss: -157247.906250\n",
      "Train Epoch: 599 [5760/17352 (33%)] Loss: -143140.531250\n",
      "Train Epoch: 599 [7168/17352 (41%)] Loss: -180767.812500\n",
      "Train Epoch: 599 [8576/17352 (49%)] Loss: -177025.906250\n",
      "Train Epoch: 599 [9984/17352 (58%)] Loss: -174593.281250\n",
      "Train Epoch: 599 [11392/17352 (66%)] Loss: -126542.156250\n",
      "Train Epoch: 599 [12800/17352 (74%)] Loss: -184484.015625\n",
      "Train Epoch: 599 [14208/17352 (82%)] Loss: -149131.062500\n",
      "Train Epoch: 599 [15495/17352 (89%)] Loss: -120282.218750\n",
      "Train Epoch: 599 [16095/17352 (93%)] Loss: -92451.609375\n",
      "Train Epoch: 599 [16976/17352 (98%)] Loss: -46726.605469\n",
      "    epoch          : 599\n",
      "    loss           : -143759.19196007235\n",
      "    val_loss       : -75969.46082356772\n",
      "Train Epoch: 600 [128/17352 (1%)] Loss: -179599.015625\n",
      "Train Epoch: 600 [1536/17352 (9%)] Loss: -166397.093750\n",
      "Train Epoch: 600 [2944/17352 (17%)] Loss: -165420.125000\n",
      "Train Epoch: 600 [4352/17352 (25%)] Loss: -189370.468750\n",
      "Train Epoch: 600 [5760/17352 (33%)] Loss: -152473.921875\n",
      "Train Epoch: 600 [7168/17352 (41%)] Loss: -124165.421875\n",
      "Train Epoch: 600 [8576/17352 (49%)] Loss: -189235.843750\n",
      "Train Epoch: 600 [9984/17352 (58%)] Loss: -173960.671875\n",
      "Train Epoch: 600 [11392/17352 (66%)] Loss: -136786.765625\n",
      "Train Epoch: 600 [12800/17352 (74%)] Loss: -170045.968750\n",
      "Train Epoch: 600 [14208/17352 (82%)] Loss: -159568.718750\n",
      "Train Epoch: 600 [15509/17352 (89%)] Loss: -123277.343750\n",
      "Train Epoch: 600 [16352/17352 (94%)] Loss: -69316.859375\n",
      "Train Epoch: 600 [17087/17352 (98%)] Loss: -25748.816406\n",
      "    epoch          : 600\n",
      "    loss           : -142016.96146182885\n",
      "    val_loss       : -59514.797823079425\n",
      "Saving checkpoint: saved/models/Omniglot_GlimpseCategory/1026_115726/checkpoint-epoch600.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
