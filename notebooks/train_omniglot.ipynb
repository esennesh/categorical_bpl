{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='omniglot_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [128/17352 (1%)] Loss: 9604.445312\n",
      "Train Epoch: 1 [1536/17352 (9%)] Loss: -93475.468750\n",
      "Train Epoch: 1 [2944/17352 (17%)] Loss: -148512.828125\n",
      "Train Epoch: 1 [4352/17352 (25%)] Loss: -165840.515625\n",
      "Train Epoch: 1 [5760/17352 (33%)] Loss: -177662.875000\n",
      "Train Epoch: 1 [7168/17352 (41%)] Loss: -158383.812500\n",
      "Train Epoch: 1 [8576/17352 (49%)] Loss: -182215.718750\n",
      "Train Epoch: 1 [9984/17352 (58%)] Loss: -191267.703125\n",
      "Train Epoch: 1 [11392/17352 (66%)] Loss: -192849.562500\n",
      "Train Epoch: 1 [12800/17352 (74%)] Loss: -172790.906250\n",
      "Train Epoch: 1 [14208/17352 (82%)] Loss: -196945.968750\n",
      "Train Epoch: 1 [15459/17352 (89%)] Loss: -21194.355469\n",
      "Train Epoch: 1 [16394/17352 (94%)] Loss: -152299.546875\n",
      "Train Epoch: 1 [17066/17352 (98%)] Loss: -7689.462891\n",
      "    epoch          : 1\n",
      "    loss           : -150732.6939623532\n",
      "    val_loss       : -93237.39029876392\n",
      "Train Epoch: 2 [128/17352 (1%)] Loss: -157931.921875\n",
      "Train Epoch: 2 [1536/17352 (9%)] Loss: -174163.750000\n",
      "Train Epoch: 2 [2944/17352 (17%)] Loss: -157916.843750\n",
      "Train Epoch: 2 [4352/17352 (25%)] Loss: -179451.062500\n",
      "Train Epoch: 2 [5760/17352 (33%)] Loss: -196938.156250\n",
      "Train Epoch: 2 [7168/17352 (41%)] Loss: -183681.562500\n",
      "Train Epoch: 2 [8576/17352 (49%)] Loss: -180103.734375\n",
      "Train Epoch: 2 [9984/17352 (58%)] Loss: -163159.828125\n",
      "Train Epoch: 2 [11392/17352 (66%)] Loss: -167367.828125\n",
      "Train Epoch: 2 [12800/17352 (74%)] Loss: -193459.906250\n",
      "Train Epoch: 2 [14208/17352 (82%)] Loss: -190272.265625\n",
      "Train Epoch: 2 [15437/17352 (89%)] Loss: -57213.953125\n",
      "Train Epoch: 2 [16193/17352 (93%)] Loss: -71310.343750\n",
      "Train Epoch: 2 [16845/17352 (97%)] Loss: -55177.585938\n",
      "    epoch          : 2\n",
      "    loss           : -170606.1501251835\n",
      "    val_loss       : -94523.70429793994\n",
      "Train Epoch: 3 [128/17352 (1%)] Loss: -196864.234375\n",
      "Train Epoch: 3 [1536/17352 (9%)] Loss: -196453.656250\n",
      "Train Epoch: 3 [2944/17352 (17%)] Loss: -180427.953125\n",
      "Train Epoch: 3 [4352/17352 (25%)] Loss: -186096.156250\n",
      "Train Epoch: 3 [5760/17352 (33%)] Loss: -194575.906250\n",
      "Train Epoch: 3 [7168/17352 (41%)] Loss: -194311.531250\n",
      "Train Epoch: 3 [8576/17352 (49%)] Loss: -164285.687500\n",
      "Train Epoch: 3 [9984/17352 (58%)] Loss: -180085.843750\n",
      "Train Epoch: 3 [11392/17352 (66%)] Loss: -185707.125000\n",
      "Train Epoch: 3 [12800/17352 (74%)] Loss: -179729.328125\n",
      "Train Epoch: 3 [14208/17352 (82%)] Loss: -181537.375000\n",
      "Train Epoch: 3 [15531/17352 (90%)] Loss: -150340.656250\n",
      "Train Epoch: 3 [16192/17352 (93%)] Loss: -155753.156250\n",
      "Train Epoch: 3 [17061/17352 (98%)] Loss: -55426.488281\n",
      "    epoch          : 3\n",
      "    loss           : -171982.35009765625\n",
      "    val_loss       : -94975.80241355897\n",
      "Train Epoch: 4 [128/17352 (1%)] Loss: -165041.312500\n",
      "Train Epoch: 4 [1536/17352 (9%)] Loss: -177593.437500\n",
      "Train Epoch: 4 [2944/17352 (17%)] Loss: -206632.265625\n",
      "Train Epoch: 4 [4352/17352 (25%)] Loss: -202668.984375\n",
      "Train Epoch: 4 [5760/17352 (33%)] Loss: -191511.500000\n",
      "Train Epoch: 4 [7168/17352 (41%)] Loss: -200644.437500\n",
      "Train Epoch: 4 [8576/17352 (49%)] Loss: -215455.625000\n",
      "Train Epoch: 4 [9984/17352 (58%)] Loss: -195866.453125\n",
      "Train Epoch: 4 [11392/17352 (66%)] Loss: -196253.625000\n",
      "Train Epoch: 4 [12800/17352 (74%)] Loss: -189717.812500\n",
      "Train Epoch: 4 [14208/17352 (82%)] Loss: -202318.078125\n",
      "Train Epoch: 4 [15500/17352 (89%)] Loss: -123853.359375\n",
      "Train Epoch: 4 [16424/17352 (95%)] Loss: -185896.734375\n",
      "Train Epoch: 4 [17153/17352 (99%)] Loss: -19818.525391\n",
      "    epoch          : 4\n",
      "    loss           : -172568.5087120517\n",
      "    val_loss       : -94679.39608255228\n",
      "Train Epoch: 5 [128/17352 (1%)] Loss: -195460.375000\n",
      "Train Epoch: 5 [1536/17352 (9%)] Loss: -183303.937500\n",
      "Train Epoch: 5 [2944/17352 (17%)] Loss: -185508.140625\n",
      "Train Epoch: 5 [4352/17352 (25%)] Loss: -181210.625000\n",
      "Train Epoch: 5 [5760/17352 (33%)] Loss: -186309.593750\n",
      "Train Epoch: 5 [7168/17352 (41%)] Loss: -176456.046875\n",
      "Train Epoch: 5 [8576/17352 (49%)] Loss: -184835.875000\n",
      "Train Epoch: 5 [9984/17352 (58%)] Loss: -196943.250000\n",
      "Train Epoch: 5 [11392/17352 (66%)] Loss: -199982.781250\n",
      "Train Epoch: 5 [12800/17352 (74%)] Loss: -196730.421875\n",
      "Train Epoch: 5 [14208/17352 (82%)] Loss: -203100.000000\n",
      "Train Epoch: 5 [15471/17352 (89%)] Loss: -121013.453125\n",
      "Train Epoch: 5 [16355/17352 (94%)] Loss: -159063.171875\n",
      "Train Epoch: 5 [17021/17352 (98%)] Loss: -124078.000000\n",
      "    epoch          : 5\n",
      "    loss           : -172905.28080759753\n",
      "    val_loss       : -95454.5827712357\n",
      "Train Epoch: 6 [128/17352 (1%)] Loss: -166520.468750\n",
      "Train Epoch: 6 [1536/17352 (9%)] Loss: -178929.734375\n",
      "Train Epoch: 6 [2944/17352 (17%)] Loss: -201173.156250\n",
      "Train Epoch: 6 [4352/17352 (25%)] Loss: -181544.546875\n",
      "Train Epoch: 6 [5760/17352 (33%)] Loss: -181818.500000\n",
      "Train Epoch: 6 [7168/17352 (41%)] Loss: -200823.125000\n",
      "Train Epoch: 6 [8576/17352 (49%)] Loss: -186316.328125\n",
      "Train Epoch: 6 [9984/17352 (58%)] Loss: -167833.078125\n",
      "Train Epoch: 6 [11392/17352 (66%)] Loss: -196533.531250\n",
      "Train Epoch: 6 [12800/17352 (74%)] Loss: -198912.390625\n",
      "Train Epoch: 6 [14208/17352 (82%)] Loss: -200317.781250\n",
      "Train Epoch: 6 [15564/17352 (90%)] Loss: -166684.156250\n",
      "Train Epoch: 6 [16188/17352 (93%)] Loss: -127966.375000\n",
      "Train Epoch: 6 [17112/17352 (99%)] Loss: -111448.523438\n",
      "    epoch          : 6\n",
      "    loss           : -173423.6286932414\n",
      "    val_loss       : -96007.95889132618\n",
      "Train Epoch: 7 [128/17352 (1%)] Loss: -196314.515625\n",
      "Train Epoch: 7 [1536/17352 (9%)] Loss: -183723.890625\n",
      "Train Epoch: 7 [2944/17352 (17%)] Loss: -220296.218750\n",
      "Train Epoch: 7 [4352/17352 (25%)] Loss: -184980.078125\n",
      "Train Epoch: 7 [5760/17352 (33%)] Loss: -183913.531250\n",
      "Train Epoch: 7 [7168/17352 (41%)] Loss: -203958.890625\n",
      "Train Epoch: 7 [8576/17352 (49%)] Loss: -189554.437500\n",
      "Train Epoch: 7 [9984/17352 (58%)] Loss: -197718.093750\n",
      "Train Epoch: 7 [11392/17352 (66%)] Loss: -199908.906250\n",
      "Train Epoch: 7 [12800/17352 (74%)] Loss: -196953.218750\n",
      "Train Epoch: 7 [14208/17352 (82%)] Loss: -203563.250000\n",
      "Train Epoch: 7 [15468/17352 (89%)] Loss: -71583.453125\n",
      "Train Epoch: 7 [16072/17352 (93%)] Loss: -168287.000000\n",
      "Train Epoch: 7 [16941/17352 (98%)] Loss: -118912.539062\n",
      "    epoch          : 7\n",
      "    loss           : -175634.3332568687\n",
      "    val_loss       : -97457.6198457559\n",
      "Train Epoch: 8 [128/17352 (1%)] Loss: -169459.000000\n",
      "Train Epoch: 8 [1536/17352 (9%)] Loss: -184899.140625\n",
      "Train Epoch: 8 [2944/17352 (17%)] Loss: -189137.984375\n",
      "Train Epoch: 8 [4352/17352 (25%)] Loss: -183205.062500\n",
      "Train Epoch: 8 [5760/17352 (33%)] Loss: -190305.531250\n",
      "Train Epoch: 8 [7168/17352 (41%)] Loss: -184735.921875\n",
      "Train Epoch: 8 [8576/17352 (49%)] Loss: -185671.500000\n",
      "Train Epoch: 8 [9984/17352 (58%)] Loss: -188972.156250\n",
      "Train Epoch: 8 [11392/17352 (66%)] Loss: -170067.781250\n",
      "Train Epoch: 8 [12800/17352 (74%)] Loss: -185862.343750\n",
      "Train Epoch: 8 [14208/17352 (82%)] Loss: -187869.765625\n",
      "Train Epoch: 8 [15487/17352 (89%)] Loss: -75199.015625\n",
      "Train Epoch: 8 [16323/17352 (94%)] Loss: -146644.156250\n",
      "Train Epoch: 8 [17066/17352 (98%)] Loss: -167202.609375\n",
      "    epoch          : 8\n",
      "    loss           : -176792.0610548186\n",
      "    val_loss       : -97713.1844961524\n",
      "Train Epoch: 9 [128/17352 (1%)] Loss: -202990.718750\n",
      "Train Epoch: 9 [1536/17352 (9%)] Loss: -197391.500000\n",
      "Train Epoch: 9 [2944/17352 (17%)] Loss: -167914.437500\n",
      "Train Epoch: 9 [4352/17352 (25%)] Loss: -188695.031250\n",
      "Train Epoch: 9 [5760/17352 (33%)] Loss: -189336.640625\n",
      "Train Epoch: 9 [7168/17352 (41%)] Loss: -169012.921875\n",
      "Train Epoch: 9 [8576/17352 (49%)] Loss: -183752.968750\n",
      "Train Epoch: 9 [9984/17352 (58%)] Loss: -221435.593750\n",
      "Train Epoch: 9 [11392/17352 (66%)] Loss: -189768.281250\n",
      "Train Epoch: 9 [12800/17352 (74%)] Loss: -186727.078125\n",
      "Train Epoch: 9 [14208/17352 (82%)] Loss: -221454.546875\n",
      "Train Epoch: 9 [15560/17352 (90%)] Loss: -114864.335938\n",
      "Train Epoch: 9 [16171/17352 (93%)] Loss: -52873.863281\n",
      "Train Epoch: 9 [16894/17352 (97%)] Loss: -22005.392578\n",
      "    epoch          : 9\n",
      "    loss           : -177302.5152579698\n",
      "    val_loss       : -98004.58728618622\n",
      "Train Epoch: 10 [128/17352 (1%)] Loss: -173069.375000\n",
      "Train Epoch: 10 [1536/17352 (9%)] Loss: -186179.390625\n",
      "Train Epoch: 10 [2944/17352 (17%)] Loss: -219168.359375\n",
      "Train Epoch: 10 [4352/17352 (25%)] Loss: -198747.343750\n",
      "Train Epoch: 10 [5760/17352 (33%)] Loss: -189168.171875\n",
      "Train Epoch: 10 [7168/17352 (41%)] Loss: -206408.968750\n",
      "Train Epoch: 10 [8576/17352 (49%)] Loss: -191295.093750\n",
      "Train Epoch: 10 [9984/17352 (58%)] Loss: -169955.140625\n",
      "Train Epoch: 10 [11392/17352 (66%)] Loss: -200644.687500\n",
      "Train Epoch: 10 [12800/17352 (74%)] Loss: -200899.281250\n",
      "Train Epoch: 10 [14208/17352 (82%)] Loss: -203517.453125\n",
      "Train Epoch: 10 [15406/17352 (89%)] Loss: -4863.450195\n",
      "Train Epoch: 10 [16290/17352 (94%)] Loss: -122389.726562\n",
      "Train Epoch: 10 [17101/17352 (99%)] Loss: -124387.429688\n",
      "    epoch          : 10\n",
      "    loss           : -177936.93040189808\n",
      "    val_loss       : -98324.62128626506\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0806_145705/checkpoint-epoch10.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 11 [128/17352 (1%)] Loss: -166835.187500\n",
      "Train Epoch: 11 [1536/17352 (9%)] Loss: -197412.937500\n",
      "Train Epoch: 11 [2944/17352 (17%)] Loss: -232310.062500\n",
      "Train Epoch: 11 [4352/17352 (25%)] Loss: -187623.812500\n",
      "Train Epoch: 11 [5760/17352 (33%)] Loss: -184533.671875\n",
      "Train Epoch: 11 [7168/17352 (41%)] Loss: -206174.187500\n",
      "Train Epoch: 11 [8576/17352 (49%)] Loss: -221812.500000\n",
      "Train Epoch: 11 [9984/17352 (58%)] Loss: -224561.765625\n",
      "Train Epoch: 11 [11392/17352 (66%)] Loss: -200915.328125\n",
      "Train Epoch: 11 [12800/17352 (74%)] Loss: -199477.859375\n",
      "Train Epoch: 11 [14208/17352 (82%)] Loss: -187748.187500\n",
      "Train Epoch: 11 [15505/17352 (89%)] Loss: -148420.578125\n",
      "Train Epoch: 11 [16260/17352 (94%)] Loss: -25135.687500\n",
      "Train Epoch: 11 [17026/17352 (98%)] Loss: -72887.828125\n",
      "    epoch          : 11\n",
      "    loss           : -178233.18029375526\n",
      "    val_loss       : -98470.34016151428\n",
      "Train Epoch: 12 [128/17352 (1%)] Loss: -201478.546875\n",
      "Train Epoch: 12 [1536/17352 (9%)] Loss: -188599.203125\n",
      "Train Epoch: 12 [2944/17352 (17%)] Loss: -205442.203125\n",
      "Train Epoch: 12 [4352/17352 (25%)] Loss: -188595.015625\n",
      "Train Epoch: 12 [5760/17352 (33%)] Loss: -189720.531250\n",
      "Train Epoch: 12 [7168/17352 (41%)] Loss: -184121.781250\n",
      "Train Epoch: 12 [8576/17352 (49%)] Loss: -190411.187500\n",
      "Train Epoch: 12 [9984/17352 (58%)] Loss: -200877.593750\n",
      "Train Epoch: 12 [11392/17352 (66%)] Loss: -184769.500000\n",
      "Train Epoch: 12 [12800/17352 (74%)] Loss: -202216.343750\n",
      "Train Epoch: 12 [14208/17352 (82%)] Loss: -207819.125000\n",
      "Train Epoch: 12 [15450/17352 (89%)] Loss: -123867.093750\n",
      "Train Epoch: 12 [16321/17352 (94%)] Loss: -120253.625000\n",
      "Train Epoch: 12 [17041/17352 (98%)] Loss: -4521.733398\n",
      "    epoch          : 12\n",
      "    loss           : -178594.05398621014\n",
      "    val_loss       : -98553.92087418238\n",
      "Train Epoch: 13 [128/17352 (1%)] Loss: -202308.156250\n",
      "Train Epoch: 13 [1536/17352 (9%)] Loss: -199911.468750\n",
      "Train Epoch: 13 [2944/17352 (17%)] Loss: -195129.062500\n",
      "Train Epoch: 13 [4352/17352 (25%)] Loss: -206978.312500\n",
      "Train Epoch: 13 [5760/17352 (33%)] Loss: -222212.062500\n",
      "Train Epoch: 13 [7168/17352 (41%)] Loss: -226163.250000\n",
      "Train Epoch: 13 [8576/17352 (49%)] Loss: -185042.500000\n",
      "Train Epoch: 13 [9984/17352 (58%)] Loss: -226346.500000\n",
      "Train Epoch: 13 [11392/17352 (66%)] Loss: -206865.218750\n",
      "Train Epoch: 13 [12800/17352 (74%)] Loss: -182646.343750\n",
      "Train Epoch: 13 [14208/17352 (82%)] Loss: -222390.156250\n",
      "Train Epoch: 13 [15557/17352 (90%)] Loss: -139203.406250\n",
      "Train Epoch: 13 [16341/17352 (94%)] Loss: -115818.421875\n",
      "Train Epoch: 13 [17114/17352 (99%)] Loss: -126239.617188\n",
      "    epoch          : 13\n",
      "    loss           : -178903.8916015625\n",
      "    val_loss       : -98666.79269692102\n",
      "Train Epoch: 14 [128/17352 (1%)] Loss: -186862.796875\n",
      "Train Epoch: 14 [1536/17352 (9%)] Loss: -201919.218750\n",
      "Train Epoch: 14 [2944/17352 (17%)] Loss: -237557.312500\n",
      "Train Epoch: 14 [4352/17352 (25%)] Loss: -188762.515625\n",
      "Train Epoch: 14 [5760/17352 (33%)] Loss: -167857.671875\n",
      "Train Epoch: 14 [7168/17352 (41%)] Loss: -223959.656250\n",
      "Train Epoch: 14 [8576/17352 (49%)] Loss: -222467.890625\n",
      "Train Epoch: 14 [9984/17352 (58%)] Loss: -201228.859375\n",
      "Train Epoch: 14 [11392/17352 (66%)] Loss: -204432.734375\n",
      "Train Epoch: 14 [12800/17352 (74%)] Loss: -201905.250000\n",
      "Train Epoch: 14 [14208/17352 (82%)] Loss: -187038.437500\n",
      "Train Epoch: 14 [15602/17352 (90%)] Loss: -190051.875000\n",
      "Train Epoch: 14 [16440/17352 (95%)] Loss: -162329.343750\n",
      "Train Epoch: 14 [17078/17352 (98%)] Loss: -25252.800781\n",
      "    epoch          : 14\n",
      "    loss           : -178960.34306509543\n",
      "    val_loss       : -98621.46132698058\n",
      "Train Epoch: 15 [128/17352 (1%)] Loss: -199914.718750\n",
      "Train Epoch: 15 [1536/17352 (9%)] Loss: -187448.859375\n",
      "Train Epoch: 15 [2944/17352 (17%)] Loss: -170260.156250\n",
      "Train Epoch: 15 [4352/17352 (25%)] Loss: -186515.562500\n",
      "Train Epoch: 15 [5760/17352 (33%)] Loss: -200610.718750\n",
      "Train Epoch: 15 [7168/17352 (41%)] Loss: -188086.281250\n",
      "Train Epoch: 15 [8576/17352 (49%)] Loss: -189111.562500\n",
      "Train Epoch: 15 [9984/17352 (58%)] Loss: -184279.640625\n",
      "Train Epoch: 15 [11392/17352 (66%)] Loss: -195474.734375\n",
      "Train Epoch: 15 [12800/17352 (74%)] Loss: -188324.781250\n",
      "Train Epoch: 15 [14208/17352 (82%)] Loss: -186502.906250\n",
      "Train Epoch: 15 [15565/17352 (90%)] Loss: -194728.656250\n",
      "Train Epoch: 15 [16360/17352 (94%)] Loss: -139356.640625\n",
      "Train Epoch: 15 [17085/17352 (98%)] Loss: -156168.921875\n",
      "    epoch          : 15\n",
      "    loss           : -179306.65217989724\n",
      "    val_loss       : -98855.99381917318\n",
      "Train Epoch: 16 [128/17352 (1%)] Loss: -206639.593750\n",
      "Train Epoch: 16 [1536/17352 (9%)] Loss: -228805.125000\n",
      "Train Epoch: 16 [2944/17352 (17%)] Loss: -170286.078125\n",
      "Train Epoch: 16 [4352/17352 (25%)] Loss: -197683.718750\n",
      "Train Epoch: 16 [5760/17352 (33%)] Loss: -202526.078125\n",
      "Train Epoch: 16 [7168/17352 (41%)] Loss: -170251.250000\n",
      "Train Epoch: 16 [8576/17352 (49%)] Loss: -189479.546875\n",
      "Train Epoch: 16 [9984/17352 (58%)] Loss: -228885.781250\n",
      "Train Epoch: 16 [11392/17352 (66%)] Loss: -206920.687500\n",
      "Train Epoch: 16 [12800/17352 (74%)] Loss: -201591.156250\n",
      "Train Epoch: 16 [14208/17352 (82%)] Loss: -187058.734375\n",
      "Train Epoch: 16 [15503/17352 (89%)] Loss: -58980.378906\n",
      "Train Epoch: 16 [16380/17352 (94%)] Loss: -122832.320312\n",
      "Train Epoch: 16 [17193/17352 (99%)] Loss: -107934.960938\n",
      "    epoch          : 16\n",
      "    loss           : -179620.65792785236\n",
      "    val_loss       : -98984.05837841034\n",
      "Train Epoch: 17 [128/17352 (1%)] Loss: -203717.921875\n",
      "Train Epoch: 17 [1536/17352 (9%)] Loss: -187946.296875\n",
      "Train Epoch: 17 [2944/17352 (17%)] Loss: -189683.515625\n",
      "Train Epoch: 17 [4352/17352 (25%)] Loss: -200551.375000\n",
      "Train Epoch: 17 [5760/17352 (33%)] Loss: -200518.406250\n",
      "Train Epoch: 17 [7168/17352 (41%)] Loss: -168391.500000\n",
      "Train Epoch: 17 [8576/17352 (49%)] Loss: -186485.953125\n",
      "Train Epoch: 17 [9984/17352 (58%)] Loss: -203584.156250\n",
      "Train Epoch: 17 [11392/17352 (66%)] Loss: -204346.500000\n",
      "Train Epoch: 17 [12800/17352 (74%)] Loss: -185881.468750\n",
      "Train Epoch: 17 [14208/17352 (82%)] Loss: -200250.437500\n",
      "Train Epoch: 17 [15573/17352 (90%)] Loss: -195350.093750\n",
      "Train Epoch: 17 [16494/17352 (95%)] Loss: -139942.296875\n",
      "Train Epoch: 17 [17040/17352 (98%)] Loss: -74429.859375\n",
      "    epoch          : 17\n",
      "    loss           : -179611.83210989932\n",
      "    val_loss       : -98964.41043170293\n",
      "Train Epoch: 18 [128/17352 (1%)] Loss: -171256.140625\n",
      "Train Epoch: 18 [1536/17352 (9%)] Loss: -188364.593750\n",
      "Train Epoch: 18 [2944/17352 (17%)] Loss: -224381.171875\n",
      "Train Epoch: 18 [4352/17352 (25%)] Loss: -209579.687500\n",
      "Train Epoch: 18 [5760/17352 (33%)] Loss: -179327.062500\n",
      "Train Epoch: 18 [7168/17352 (41%)] Loss: -201135.468750\n",
      "Train Epoch: 18 [8576/17352 (49%)] Loss: -210247.031250\n",
      "Train Epoch: 18 [9984/17352 (58%)] Loss: -211949.250000\n",
      "Train Epoch: 18 [11392/17352 (66%)] Loss: -176667.562500\n",
      "Train Epoch: 18 [12800/17352 (74%)] Loss: -188894.437500\n",
      "Train Epoch: 18 [14208/17352 (82%)] Loss: -203452.375000\n",
      "Train Epoch: 18 [15488/17352 (89%)] Loss: -23111.283203\n",
      "Train Epoch: 18 [16170/17352 (93%)] Loss: -196154.218750\n",
      "Train Epoch: 18 [16974/17352 (98%)] Loss: -73098.890625\n",
      "    epoch          : 18\n",
      "    loss           : -179884.76202023908\n",
      "    val_loss       : -99166.36519683202\n",
      "Train Epoch: 19 [128/17352 (1%)] Loss: -203030.656250\n",
      "Train Epoch: 19 [1536/17352 (9%)] Loss: -183989.828125\n",
      "Train Epoch: 19 [2944/17352 (17%)] Loss: -187822.968750\n",
      "Train Epoch: 19 [4352/17352 (25%)] Loss: -227252.281250\n",
      "Train Epoch: 19 [5760/17352 (33%)] Loss: -203691.625000\n",
      "Train Epoch: 19 [7168/17352 (41%)] Loss: -204253.875000\n",
      "Train Epoch: 19 [8576/17352 (49%)] Loss: -193277.546875\n",
      "Train Epoch: 19 [9984/17352 (58%)] Loss: -211137.437500\n",
      "Train Epoch: 19 [11392/17352 (66%)] Loss: -171748.796875\n",
      "Train Epoch: 19 [12800/17352 (74%)] Loss: -199730.421875\n",
      "Train Epoch: 19 [14208/17352 (82%)] Loss: -189358.375000\n",
      "Train Epoch: 19 [15521/17352 (89%)] Loss: -128939.601562\n",
      "Train Epoch: 19 [16264/17352 (94%)] Loss: -58287.285156\n",
      "Train Epoch: 19 [16952/17352 (98%)] Loss: -8284.393555\n",
      "    epoch          : 19\n",
      "    loss           : -180086.99115850462\n",
      "    val_loss       : -99182.43625933329\n",
      "Train Epoch: 20 [128/17352 (1%)] Loss: -204610.125000\n",
      "Train Epoch: 20 [1536/17352 (9%)] Loss: -192593.906250\n",
      "Train Epoch: 20 [2944/17352 (17%)] Loss: -242855.906250\n",
      "Train Epoch: 20 [4352/17352 (25%)] Loss: -200739.937500\n",
      "Train Epoch: 20 [5760/17352 (33%)] Loss: -197137.296875\n",
      "Train Epoch: 20 [7168/17352 (41%)] Loss: -201533.406250\n",
      "Train Epoch: 20 [8576/17352 (49%)] Loss: -203934.015625\n",
      "Train Epoch: 20 [9984/17352 (58%)] Loss: -183663.890625\n",
      "Train Epoch: 20 [11392/17352 (66%)] Loss: -203150.093750\n",
      "Train Epoch: 20 [12800/17352 (74%)] Loss: -201588.625000\n",
      "Train Epoch: 20 [14208/17352 (82%)] Loss: -187772.812500\n",
      "Train Epoch: 20 [15499/17352 (89%)] Loss: -128839.843750\n",
      "Train Epoch: 20 [16120/17352 (93%)] Loss: -5085.012695\n",
      "Train Epoch: 20 [16900/17352 (97%)] Loss: -23307.375000\n",
      "    epoch          : 20\n",
      "    loss           : -180153.13890782298\n",
      "    val_loss       : -99285.41441733042\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0806_145705/checkpoint-epoch20.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 21 [128/17352 (1%)] Loss: -207014.500000\n",
      "Train Epoch: 21 [1536/17352 (9%)] Loss: -186945.765625\n",
      "Train Epoch: 21 [2944/17352 (17%)] Loss: -193378.593750\n",
      "Train Epoch: 21 [4352/17352 (25%)] Loss: -209786.500000\n",
      "Train Epoch: 21 [5760/17352 (33%)] Loss: -202039.250000\n",
      "Train Epoch: 21 [7168/17352 (41%)] Loss: -190329.218750\n",
      "Train Epoch: 21 [8576/17352 (49%)] Loss: -167991.156250\n",
      "Train Epoch: 21 [9984/17352 (58%)] Loss: -174554.343750\n",
      "Train Epoch: 21 [11392/17352 (66%)] Loss: -179157.953125\n",
      "Train Epoch: 21 [12800/17352 (74%)] Loss: -204279.562500\n",
      "Train Epoch: 21 [14208/17352 (82%)] Loss: -188047.468750\n",
      "Train Epoch: 21 [15582/17352 (90%)] Loss: -150317.343750\n",
      "Train Epoch: 21 [16338/17352 (94%)] Loss: -124421.828125\n",
      "Train Epoch: 21 [16980/17352 (98%)] Loss: -8212.206055\n",
      "    epoch          : 21\n",
      "    loss           : -180026.59200005242\n",
      "    val_loss       : -99379.23267498016\n",
      "Train Epoch: 22 [128/17352 (1%)] Loss: -203664.687500\n",
      "Train Epoch: 22 [1536/17352 (9%)] Loss: -211614.484375\n",
      "Train Epoch: 22 [2944/17352 (17%)] Loss: -190398.656250\n",
      "Train Epoch: 22 [4352/17352 (25%)] Loss: -187117.453125\n",
      "Train Epoch: 22 [5760/17352 (33%)] Loss: -201862.234375\n",
      "Train Epoch: 22 [7168/17352 (41%)] Loss: -188385.390625\n",
      "Train Epoch: 22 [8576/17352 (49%)] Loss: -188737.625000\n",
      "Train Epoch: 22 [9984/17352 (58%)] Loss: -203915.734375\n",
      "Train Epoch: 22 [11392/17352 (66%)] Loss: -189887.015625\n",
      "Train Epoch: 22 [12800/17352 (74%)] Loss: -192883.609375\n",
      "Train Epoch: 22 [14208/17352 (82%)] Loss: -205147.656250\n",
      "Train Epoch: 22 [15490/17352 (89%)] Loss: -167610.484375\n",
      "Train Epoch: 22 [16355/17352 (94%)] Loss: -131195.343750\n",
      "Train Epoch: 22 [17073/17352 (98%)] Loss: -124245.007812\n",
      "    epoch          : 22\n",
      "    loss           : -180522.23747181732\n",
      "    val_loss       : -99323.40220864613\n",
      "Train Epoch: 23 [128/17352 (1%)] Loss: -201808.312500\n",
      "Train Epoch: 23 [1536/17352 (9%)] Loss: -182935.484375\n",
      "Train Epoch: 23 [2944/17352 (17%)] Loss: -173017.343750\n",
      "Train Epoch: 23 [4352/17352 (25%)] Loss: -194818.078125\n",
      "Train Epoch: 23 [5760/17352 (33%)] Loss: -208455.125000\n",
      "Train Epoch: 23 [7168/17352 (41%)] Loss: -186199.734375\n",
      "Train Epoch: 23 [8576/17352 (49%)] Loss: -192792.062500\n",
      "Train Epoch: 23 [9984/17352 (58%)] Loss: -199868.578125\n",
      "Train Epoch: 23 [11392/17352 (66%)] Loss: -187466.468750\n",
      "Train Epoch: 23 [12800/17352 (74%)] Loss: -206705.015625\n",
      "Train Epoch: 23 [14208/17352 (82%)] Loss: -187325.281250\n",
      "Train Epoch: 23 [15494/17352 (89%)] Loss: -140011.953125\n",
      "Train Epoch: 23 [16255/17352 (94%)] Loss: -8572.574219\n",
      "Train Epoch: 23 [16926/17352 (98%)] Loss: -171161.218750\n",
      "    epoch          : 23\n",
      "    loss           : -180697.92742960886\n",
      "    val_loss       : -99450.60714906057\n",
      "Train Epoch: 24 [128/17352 (1%)] Loss: -189668.515625\n",
      "Train Epoch: 24 [1536/17352 (9%)] Loss: -189180.687500\n",
      "Train Epoch: 24 [2944/17352 (17%)] Loss: -191843.750000\n",
      "Train Epoch: 24 [4352/17352 (25%)] Loss: -205226.875000\n",
      "Train Epoch: 24 [5760/17352 (33%)] Loss: -187066.437500\n",
      "Train Epoch: 24 [7168/17352 (41%)] Loss: -203839.078125\n",
      "Train Epoch: 24 [8576/17352 (49%)] Loss: -200704.312500\n",
      "Train Epoch: 24 [9984/17352 (58%)] Loss: -190699.796875\n",
      "Train Epoch: 24 [11392/17352 (66%)] Loss: -204638.750000\n",
      "Train Epoch: 24 [12800/17352 (74%)] Loss: -203386.031250\n",
      "Train Epoch: 24 [14208/17352 (82%)] Loss: -188022.031250\n",
      "Train Epoch: 24 [15451/17352 (89%)] Loss: -54571.460938\n",
      "Train Epoch: 24 [16058/17352 (93%)] Loss: -81945.828125\n",
      "Train Epoch: 24 [16942/17352 (98%)] Loss: -108080.335938\n",
      "    epoch          : 24\n",
      "    loss           : -180821.8280889524\n",
      "    val_loss       : -99318.85076694489\n",
      "Train Epoch: 25 [128/17352 (1%)] Loss: -208373.140625\n",
      "Train Epoch: 25 [1536/17352 (9%)] Loss: -211734.875000\n",
      "Train Epoch: 25 [2944/17352 (17%)] Loss: -207675.187500\n",
      "Train Epoch: 25 [4352/17352 (25%)] Loss: -206482.296875\n",
      "Train Epoch: 25 [5760/17352 (33%)] Loss: -202882.421875\n",
      "Train Epoch: 25 [7168/17352 (41%)] Loss: -187961.890625\n",
      "Train Epoch: 25 [8576/17352 (49%)] Loss: -213708.093750\n",
      "Train Epoch: 25 [9984/17352 (58%)] Loss: -191101.750000\n",
      "Train Epoch: 25 [11392/17352 (66%)] Loss: -204913.609375\n",
      "Train Epoch: 25 [12800/17352 (74%)] Loss: -196203.140625\n",
      "Train Epoch: 25 [14208/17352 (82%)] Loss: -206855.406250\n",
      "Train Epoch: 25 [15451/17352 (89%)] Loss: -55061.906250\n",
      "Train Epoch: 25 [16258/17352 (94%)] Loss: -23582.707031\n",
      "Train Epoch: 25 [17110/17352 (99%)] Loss: -161236.531250\n",
      "    epoch          : 25\n",
      "    loss           : -180780.39354485634\n",
      "    val_loss       : -99568.29857416153\n",
      "Train Epoch: 26 [128/17352 (1%)] Loss: -173789.234375\n",
      "Train Epoch: 26 [1536/17352 (9%)] Loss: -191826.562500\n",
      "Train Epoch: 26 [2944/17352 (17%)] Loss: -223013.562500\n",
      "Train Epoch: 26 [4352/17352 (25%)] Loss: -193855.875000\n",
      "Train Epoch: 26 [5760/17352 (33%)] Loss: -188804.531250\n",
      "Train Epoch: 26 [7168/17352 (41%)] Loss: -195421.593750\n",
      "Train Epoch: 26 [8576/17352 (49%)] Loss: -192277.796875\n",
      "Train Epoch: 26 [9984/17352 (58%)] Loss: -188981.406250\n",
      "Train Epoch: 26 [11392/17352 (66%)] Loss: -179912.781250\n",
      "Train Epoch: 26 [12800/17352 (74%)] Loss: -189459.812500\n",
      "Train Epoch: 26 [14208/17352 (82%)] Loss: -225592.687500\n",
      "Train Epoch: 26 [15448/17352 (89%)] Loss: -59236.148438\n",
      "Train Epoch: 26 [16283/17352 (94%)] Loss: -172635.593750\n",
      "Train Epoch: 26 [16993/17352 (98%)] Loss: -58456.718750\n",
      "    epoch          : 26\n",
      "    loss           : -181081.37300427328\n",
      "    val_loss       : -99660.61341908773\n",
      "Train Epoch: 27 [128/17352 (1%)] Loss: -179272.781250\n",
      "Train Epoch: 27 [1536/17352 (9%)] Loss: -190645.546875\n",
      "Train Epoch: 27 [2944/17352 (17%)] Loss: -189048.343750\n",
      "Train Epoch: 27 [4352/17352 (25%)] Loss: -209672.593750\n",
      "Train Epoch: 27 [5760/17352 (33%)] Loss: -189431.921875\n",
      "Train Epoch: 27 [7168/17352 (41%)] Loss: -193124.046875\n",
      "Train Epoch: 27 [8576/17352 (49%)] Loss: -224184.093750\n",
      "Train Epoch: 27 [9984/17352 (58%)] Loss: -203205.843750\n",
      "Train Epoch: 27 [11392/17352 (66%)] Loss: -208592.375000\n",
      "Train Epoch: 27 [12800/17352 (74%)] Loss: -191319.125000\n",
      "Train Epoch: 27 [14208/17352 (82%)] Loss: -191729.062500\n",
      "Train Epoch: 27 [15446/17352 (89%)] Loss: -23084.359375\n",
      "Train Epoch: 27 [16269/17352 (94%)] Loss: -59070.871094\n",
      "Train Epoch: 27 [17023/17352 (98%)] Loss: -169735.734375\n",
      "    epoch          : 27\n",
      "    loss           : -181299.15498177957\n",
      "    val_loss       : -99692.93957599004\n",
      "Train Epoch: 28 [128/17352 (1%)] Loss: -174165.890625\n",
      "Train Epoch: 28 [1536/17352 (9%)] Loss: -188053.000000\n",
      "Train Epoch: 28 [2944/17352 (17%)] Loss: -193222.484375\n",
      "Train Epoch: 28 [4352/17352 (25%)] Loss: -194732.609375\n",
      "Train Epoch: 28 [5760/17352 (33%)] Loss: -206172.328125\n",
      "Train Epoch: 28 [7168/17352 (41%)] Loss: -187357.312500\n",
      "Train Epoch: 28 [8576/17352 (49%)] Loss: -193338.000000\n",
      "Train Epoch: 28 [9984/17352 (58%)] Loss: -210306.156250\n",
      "Train Epoch: 28 [11392/17352 (66%)] Loss: -179770.609375\n",
      "Train Epoch: 28 [12800/17352 (74%)] Loss: -189279.828125\n",
      "Train Epoch: 28 [14208/17352 (82%)] Loss: -204686.734375\n",
      "Train Epoch: 28 [15588/17352 (90%)] Loss: -160791.031250\n",
      "Train Epoch: 28 [16219/17352 (93%)] Loss: -118119.281250\n",
      "Train Epoch: 28 [16972/17352 (98%)] Loss: -159299.609375\n",
      "    epoch          : 28\n",
      "    loss           : -181373.0121742607\n",
      "    val_loss       : -99868.78175970713\n",
      "Train Epoch: 29 [128/17352 (1%)] Loss: -170772.843750\n",
      "Train Epoch: 29 [1536/17352 (9%)] Loss: -200000.531250\n",
      "Train Epoch: 29 [2944/17352 (17%)] Loss: -193986.671875\n",
      "Train Epoch: 29 [4352/17352 (25%)] Loss: -195253.406250\n",
      "Train Epoch: 29 [5760/17352 (33%)] Loss: -210328.093750\n",
      "Train Epoch: 29 [7168/17352 (41%)] Loss: -228933.359375\n",
      "Train Epoch: 29 [8576/17352 (49%)] Loss: -207445.406250\n",
      "Train Epoch: 29 [9984/17352 (58%)] Loss: -176356.140625\n",
      "Train Epoch: 29 [11392/17352 (66%)] Loss: -192449.640625\n",
      "Train Epoch: 29 [12800/17352 (74%)] Loss: -205908.265625\n",
      "Train Epoch: 29 [14208/17352 (82%)] Loss: -220300.687500\n",
      "Train Epoch: 29 [15399/17352 (89%)] Loss: -59791.742188\n",
      "Train Epoch: 29 [16305/17352 (94%)] Loss: -125774.929688\n",
      "Train Epoch: 29 [16975/17352 (98%)] Loss: -23300.453125\n",
      "    epoch          : 29\n",
      "    loss           : -181553.4637426594\n",
      "    val_loss       : -99763.05589195887\n",
      "Train Epoch: 30 [128/17352 (1%)] Loss: -207424.140625\n",
      "Train Epoch: 30 [1536/17352 (9%)] Loss: -195038.406250\n",
      "Train Epoch: 30 [2944/17352 (17%)] Loss: -206089.125000\n",
      "Train Epoch: 30 [4352/17352 (25%)] Loss: -193894.109375\n",
      "Train Epoch: 30 [5760/17352 (33%)] Loss: -191577.375000\n",
      "Train Epoch: 30 [7168/17352 (41%)] Loss: -169867.375000\n",
      "Train Epoch: 30 [8576/17352 (49%)] Loss: -190754.593750\n",
      "Train Epoch: 30 [9984/17352 (58%)] Loss: -230007.562500\n",
      "Train Epoch: 30 [11392/17352 (66%)] Loss: -204383.125000\n",
      "Train Epoch: 30 [12800/17352 (74%)] Loss: -202420.093750\n",
      "Train Epoch: 30 [14208/17352 (82%)] Loss: -191054.843750\n",
      "Train Epoch: 30 [15470/17352 (89%)] Loss: -164392.781250\n",
      "Train Epoch: 30 [16294/17352 (94%)] Loss: -128114.390625\n",
      "Train Epoch: 30 [17115/17352 (99%)] Loss: -124523.429688\n",
      "    epoch          : 30\n",
      "    loss           : -181655.458630453\n",
      "    val_loss       : -99917.47974274954\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0806_145705/checkpoint-epoch30.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 31 [128/17352 (1%)] Loss: -208712.062500\n",
      "Train Epoch: 31 [1536/17352 (9%)] Loss: -196180.562500\n",
      "Train Epoch: 31 [2944/17352 (17%)] Loss: -190730.062500\n",
      "Train Epoch: 31 [4352/17352 (25%)] Loss: -198703.296875\n",
      "Train Epoch: 31 [5760/17352 (33%)] Loss: -192765.125000\n",
      "Train Epoch: 31 [7168/17352 (41%)] Loss: -188693.500000\n",
      "Train Epoch: 31 [8576/17352 (49%)] Loss: -190392.500000\n",
      "Train Epoch: 31 [9984/17352 (58%)] Loss: -206645.343750\n",
      "Train Epoch: 31 [11392/17352 (66%)] Loss: -209344.984375\n",
      "Train Epoch: 31 [12800/17352 (74%)] Loss: -221926.718750\n",
      "Train Epoch: 31 [14208/17352 (82%)] Loss: -212760.468750\n",
      "Train Epoch: 31 [15478/17352 (89%)] Loss: -23581.886719\n",
      "Train Epoch: 31 [16124/17352 (93%)] Loss: -76510.242188\n",
      "Train Epoch: 31 [17028/17352 (98%)] Loss: -125046.718750\n",
      "    epoch          : 31\n",
      "    loss           : -181714.68244678061\n",
      "    val_loss       : -99885.3454914093\n",
      "Train Epoch: 32 [128/17352 (1%)] Loss: -205606.187500\n",
      "Train Epoch: 32 [1536/17352 (9%)] Loss: -189964.093750\n",
      "Train Epoch: 32 [2944/17352 (17%)] Loss: -187474.906250\n",
      "Train Epoch: 32 [4352/17352 (25%)] Loss: -202955.562500\n",
      "Train Epoch: 32 [5760/17352 (33%)] Loss: -193234.437500\n",
      "Train Epoch: 32 [7168/17352 (41%)] Loss: -204373.250000\n",
      "Train Epoch: 32 [8576/17352 (49%)] Loss: -190415.968750\n",
      "Train Epoch: 32 [9984/17352 (58%)] Loss: -204938.437500\n",
      "Train Epoch: 32 [11392/17352 (66%)] Loss: -210376.046875\n",
      "Train Epoch: 32 [12800/17352 (74%)] Loss: -190874.937500\n",
      "Train Epoch: 32 [14208/17352 (82%)] Loss: -209578.218750\n",
      "Train Epoch: 32 [15444/17352 (89%)] Loss: -55911.828125\n",
      "Train Epoch: 32 [16157/17352 (93%)] Loss: -4933.004883\n",
      "Train Epoch: 32 [16960/17352 (98%)] Loss: -132984.312500\n",
      "    epoch          : 32\n",
      "    loss           : -181815.09220322987\n",
      "    val_loss       : -99948.93650598526\n",
      "Train Epoch: 33 [128/17352 (1%)] Loss: -171757.390625\n",
      "Train Epoch: 33 [1536/17352 (9%)] Loss: -192129.187500\n",
      "Train Epoch: 33 [2944/17352 (17%)] Loss: -189397.906250\n",
      "Train Epoch: 33 [4352/17352 (25%)] Loss: -210672.718750\n",
      "Train Epoch: 33 [5760/17352 (33%)] Loss: -202634.218750\n",
      "Train Epoch: 33 [7168/17352 (41%)] Loss: -191823.359375\n",
      "Train Epoch: 33 [8576/17352 (49%)] Loss: -198896.437500\n",
      "Train Epoch: 33 [9984/17352 (58%)] Loss: -207066.937500\n",
      "Train Epoch: 33 [11392/17352 (66%)] Loss: -208491.093750\n",
      "Train Epoch: 33 [12800/17352 (74%)] Loss: -205110.765625\n",
      "Train Epoch: 33 [14208/17352 (82%)] Loss: -206690.500000\n",
      "Train Epoch: 33 [15464/17352 (89%)] Loss: -75113.726562\n",
      "Train Epoch: 33 [16404/17352 (95%)] Loss: -82126.453125\n",
      "Train Epoch: 33 [17057/17352 (98%)] Loss: -172375.203125\n",
      "    epoch          : 33\n",
      "    loss           : -181952.52556103189\n",
      "    val_loss       : -100074.32658341726\n",
      "Train Epoch: 34 [128/17352 (1%)] Loss: -209231.843750\n",
      "Train Epoch: 34 [1536/17352 (9%)] Loss: -231807.218750\n",
      "Train Epoch: 34 [2944/17352 (17%)] Loss: -192797.968750\n",
      "Train Epoch: 34 [4352/17352 (25%)] Loss: -213338.328125\n",
      "Train Epoch: 34 [5760/17352 (33%)] Loss: -187861.656250\n",
      "Train Epoch: 34 [7168/17352 (41%)] Loss: -189373.500000\n",
      "Train Epoch: 34 [8576/17352 (49%)] Loss: -187646.281250\n",
      "Train Epoch: 34 [9984/17352 (58%)] Loss: -171515.968750\n",
      "Train Epoch: 34 [11392/17352 (66%)] Loss: -197980.125000\n",
      "Train Epoch: 34 [12800/17352 (74%)] Loss: -192857.718750\n",
      "Train Epoch: 34 [14208/17352 (82%)] Loss: -191803.015625\n",
      "Train Epoch: 34 [15480/17352 (89%)] Loss: -8336.416016\n",
      "Train Epoch: 34 [16327/17352 (94%)] Loss: -165298.093750\n",
      "Train Epoch: 34 [16988/17352 (98%)] Loss: -129360.406250\n",
      "    epoch          : 34\n",
      "    loss           : -182147.46169777686\n",
      "    val_loss       : -100107.30209611257\n",
      "Train Epoch: 35 [128/17352 (1%)] Loss: -208664.687500\n",
      "Train Epoch: 35 [1536/17352 (9%)] Loss: -191071.625000\n",
      "Train Epoch: 35 [2944/17352 (17%)] Loss: -192018.281250\n",
      "Train Epoch: 35 [4352/17352 (25%)] Loss: -187742.328125\n",
      "Train Epoch: 35 [5760/17352 (33%)] Loss: -205315.046875\n",
      "Train Epoch: 35 [7168/17352 (41%)] Loss: -174238.687500\n",
      "Train Epoch: 35 [8576/17352 (49%)] Loss: -191770.625000\n",
      "Train Epoch: 35 [9984/17352 (58%)] Loss: -212039.734375\n",
      "Train Epoch: 35 [11392/17352 (66%)] Loss: -190369.046875\n",
      "Train Epoch: 35 [12800/17352 (74%)] Loss: -204739.468750\n",
      "Train Epoch: 35 [14208/17352 (82%)] Loss: -208139.187500\n",
      "Train Epoch: 35 [15488/17352 (89%)] Loss: -160031.500000\n",
      "Train Epoch: 35 [16176/17352 (93%)] Loss: -55351.824219\n",
      "Train Epoch: 35 [16977/17352 (98%)] Loss: -117918.093750\n",
      "    epoch          : 35\n",
      "    loss           : -182195.77072409814\n",
      "    val_loss       : -99924.39528052013\n",
      "Train Epoch: 36 [128/17352 (1%)] Loss: -206441.593750\n",
      "Train Epoch: 36 [1536/17352 (9%)] Loss: -198999.437500\n",
      "Train Epoch: 36 [2944/17352 (17%)] Loss: -207669.109375\n",
      "Train Epoch: 36 [4352/17352 (25%)] Loss: -213444.187500\n",
      "Train Epoch: 36 [5760/17352 (33%)] Loss: -170548.781250\n",
      "Train Epoch: 36 [7168/17352 (41%)] Loss: -199113.859375\n",
      "Train Epoch: 36 [8576/17352 (49%)] Loss: -241698.031250\n",
      "Train Epoch: 36 [9984/17352 (58%)] Loss: -191578.500000\n",
      "Train Epoch: 36 [11392/17352 (66%)] Loss: -206185.625000\n",
      "Train Epoch: 36 [12800/17352 (74%)] Loss: -202289.656250\n",
      "Train Epoch: 36 [14208/17352 (82%)] Loss: -191684.187500\n",
      "Train Epoch: 36 [15509/17352 (89%)] Loss: -76182.664062\n",
      "Train Epoch: 36 [16174/17352 (93%)] Loss: -59348.457031\n",
      "Train Epoch: 36 [16952/17352 (98%)] Loss: -199270.453125\n",
      "    epoch          : 36\n",
      "    loss           : -181931.54337510487\n",
      "    val_loss       : -99955.45936991373\n",
      "Train Epoch: 37 [128/17352 (1%)] Loss: -212294.078125\n",
      "Train Epoch: 37 [1536/17352 (9%)] Loss: -201139.421875\n",
      "Train Epoch: 37 [2944/17352 (17%)] Loss: -192716.921875\n",
      "Train Epoch: 37 [4352/17352 (25%)] Loss: -199573.281250\n",
      "Train Epoch: 37 [5760/17352 (33%)] Loss: -192086.750000\n",
      "Train Epoch: 37 [7168/17352 (41%)] Loss: -192227.593750\n",
      "Train Epoch: 37 [8576/17352 (49%)] Loss: -188219.531250\n",
      "Train Epoch: 37 [9984/17352 (58%)] Loss: -188898.062500\n",
      "Train Epoch: 37 [11392/17352 (66%)] Loss: -191434.187500\n",
      "Train Epoch: 37 [12800/17352 (74%)] Loss: -200205.656250\n",
      "Train Epoch: 37 [14208/17352 (82%)] Loss: -208357.375000\n",
      "Train Epoch: 37 [15486/17352 (89%)] Loss: -130331.984375\n",
      "Train Epoch: 37 [16157/17352 (93%)] Loss: -25631.621094\n",
      "Train Epoch: 37 [16995/17352 (98%)] Loss: -4834.326172\n",
      "    epoch          : 37\n",
      "    loss           : -182243.0530948511\n",
      "    val_loss       : -100222.86104507446\n",
      "Train Epoch: 38 [128/17352 (1%)] Loss: -175660.046875\n",
      "Train Epoch: 38 [1536/17352 (9%)] Loss: -204636.046875\n",
      "Train Epoch: 38 [2944/17352 (17%)] Loss: -193646.078125\n",
      "Train Epoch: 38 [4352/17352 (25%)] Loss: -194418.359375\n",
      "Train Epoch: 38 [5760/17352 (33%)] Loss: -207011.953125\n",
      "Train Epoch: 38 [7168/17352 (41%)] Loss: -198366.734375\n",
      "Train Epoch: 38 [8576/17352 (49%)] Loss: -187056.656250\n",
      "Train Epoch: 38 [9984/17352 (58%)] Loss: -205458.343750\n",
      "Train Epoch: 38 [11392/17352 (66%)] Loss: -189162.171875\n",
      "Train Epoch: 38 [12800/17352 (74%)] Loss: -192246.750000\n",
      "Train Epoch: 38 [14208/17352 (82%)] Loss: -191500.187500\n",
      "Train Epoch: 38 [15560/17352 (90%)] Loss: -147419.218750\n",
      "Train Epoch: 38 [16238/17352 (94%)] Loss: -75171.398438\n",
      "Train Epoch: 38 [16970/17352 (98%)] Loss: -24723.507812\n",
      "    epoch          : 38\n",
      "    loss           : -182419.65544712145\n",
      "    val_loss       : -100161.87385622661\n",
      "Train Epoch: 39 [128/17352 (1%)] Loss: -177738.171875\n",
      "Train Epoch: 39 [1536/17352 (9%)] Loss: -195751.031250\n",
      "Train Epoch: 39 [2944/17352 (17%)] Loss: -190445.437500\n",
      "Train Epoch: 39 [4352/17352 (25%)] Loss: -192238.625000\n",
      "Train Epoch: 39 [5760/17352 (33%)] Loss: -191969.109375\n",
      "Train Epoch: 39 [7168/17352 (41%)] Loss: -188653.609375\n",
      "Train Epoch: 39 [8576/17352 (49%)] Loss: -206649.781250\n",
      "Train Epoch: 39 [9984/17352 (58%)] Loss: -171943.500000\n",
      "Train Epoch: 39 [11392/17352 (66%)] Loss: -190249.437500\n",
      "Train Epoch: 39 [12800/17352 (74%)] Loss: -215639.406250\n",
      "Train Epoch: 39 [14208/17352 (82%)] Loss: -225514.078125\n",
      "Train Epoch: 39 [15409/17352 (89%)] Loss: -60454.707031\n",
      "Train Epoch: 39 [16199/17352 (93%)] Loss: -115375.718750\n",
      "Train Epoch: 39 [16992/17352 (98%)] Loss: -140844.390625\n",
      "    epoch          : 39\n",
      "    loss           : -182312.6488340237\n",
      "    val_loss       : -100190.36782821019\n",
      "Train Epoch: 40 [128/17352 (1%)] Loss: -208086.031250\n",
      "Train Epoch: 40 [1536/17352 (9%)] Loss: -206237.015625\n",
      "Train Epoch: 40 [2944/17352 (17%)] Loss: -209013.578125\n",
      "Train Epoch: 40 [4352/17352 (25%)] Loss: -213693.375000\n",
      "Train Epoch: 40 [5760/17352 (33%)] Loss: -199287.625000\n",
      "Train Epoch: 40 [7168/17352 (41%)] Loss: -244290.406250\n",
      "Train Epoch: 40 [8576/17352 (49%)] Loss: -193598.687500\n",
      "Train Epoch: 40 [9984/17352 (58%)] Loss: -204481.843750\n",
      "Train Epoch: 40 [11392/17352 (66%)] Loss: -208749.937500\n",
      "Train Epoch: 40 [12800/17352 (74%)] Loss: -192711.390625\n",
      "Train Epoch: 40 [14208/17352 (82%)] Loss: -208004.531250\n",
      "Train Epoch: 40 [15523/17352 (89%)] Loss: -127052.765625\n",
      "Train Epoch: 40 [16181/17352 (93%)] Loss: -58761.000000\n",
      "Train Epoch: 40 [16924/17352 (98%)] Loss: -130461.843750\n",
      "    epoch          : 40\n",
      "    loss           : -182370.32107933096\n",
      "    val_loss       : -100268.32804047267\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0806_145705/checkpoint-epoch40.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 41 [128/17352 (1%)] Loss: -210321.468750\n",
      "Train Epoch: 41 [1536/17352 (9%)] Loss: -203522.781250\n",
      "Train Epoch: 41 [2944/17352 (17%)] Loss: -198299.328125\n",
      "Train Epoch: 41 [4352/17352 (25%)] Loss: -179997.609375\n",
      "Train Epoch: 41 [5760/17352 (33%)] Loss: -190049.000000\n",
      "Train Epoch: 41 [7168/17352 (41%)] Loss: -201796.796875\n",
      "Train Epoch: 41 [8576/17352 (49%)] Loss: -178249.562500\n",
      "Train Epoch: 41 [9984/17352 (58%)] Loss: -190901.531250\n",
      "Train Epoch: 41 [11392/17352 (66%)] Loss: -179773.593750\n",
      "Train Epoch: 41 [12800/17352 (74%)] Loss: -208505.578125\n",
      "Train Epoch: 41 [14208/17352 (82%)] Loss: -216139.156250\n",
      "Train Epoch: 41 [15572/17352 (90%)] Loss: -158727.515625\n",
      "Train Epoch: 41 [16329/17352 (94%)] Loss: -70086.789062\n",
      "Train Epoch: 41 [16984/17352 (98%)] Loss: -141281.890625\n",
      "    epoch          : 41\n",
      "    loss           : -182541.56141201762\n",
      "    val_loss       : -100263.60254968007\n",
      "Train Epoch: 42 [128/17352 (1%)] Loss: -191925.843750\n",
      "Train Epoch: 42 [1536/17352 (9%)] Loss: -193353.062500\n",
      "Train Epoch: 42 [2944/17352 (17%)] Loss: -192238.187500\n",
      "Train Epoch: 42 [4352/17352 (25%)] Loss: -195549.281250\n",
      "Train Epoch: 42 [5760/17352 (33%)] Loss: -206870.875000\n",
      "Train Epoch: 42 [7168/17352 (41%)] Loss: -214824.796875\n",
      "Train Epoch: 42 [8576/17352 (49%)] Loss: -225518.078125\n",
      "Train Epoch: 42 [9984/17352 (58%)] Loss: -193584.046875\n",
      "Train Epoch: 42 [11392/17352 (66%)] Loss: -179683.812500\n",
      "Train Epoch: 42 [12800/17352 (74%)] Loss: -205082.437500\n",
      "Train Epoch: 42 [14208/17352 (82%)] Loss: -208295.984375\n",
      "Train Epoch: 42 [15514/17352 (89%)] Loss: -127773.046875\n",
      "Train Epoch: 42 [16240/17352 (94%)] Loss: -58934.082031\n",
      "Train Epoch: 42 [17077/17352 (98%)] Loss: -75718.429688\n",
      "    epoch          : 42\n",
      "    loss           : -182542.42585334522\n",
      "    val_loss       : -100289.46176141103\n",
      "Train Epoch: 43 [128/17352 (1%)] Loss: -176731.359375\n",
      "Train Epoch: 43 [1536/17352 (9%)] Loss: -190498.031250\n",
      "Train Epoch: 43 [2944/17352 (17%)] Loss: -197765.843750\n",
      "Train Epoch: 43 [4352/17352 (25%)] Loss: -211873.625000\n",
      "Train Epoch: 43 [5760/17352 (33%)] Loss: -205285.468750\n",
      "Train Epoch: 43 [7168/17352 (41%)] Loss: -196306.140625\n",
      "Train Epoch: 43 [8576/17352 (49%)] Loss: -192930.687500\n",
      "Train Epoch: 43 [9984/17352 (58%)] Loss: -205044.750000\n",
      "Train Epoch: 43 [11392/17352 (66%)] Loss: -180345.078125\n",
      "Train Epoch: 43 [12800/17352 (74%)] Loss: -224858.562500\n",
      "Train Epoch: 43 [14208/17352 (82%)] Loss: -190860.234375\n",
      "Train Epoch: 43 [15563/17352 (90%)] Loss: -126330.101562\n",
      "Train Epoch: 43 [16296/17352 (94%)] Loss: -200430.906250\n",
      "Train Epoch: 43 [17140/17352 (99%)] Loss: -165504.171875\n",
      "    epoch          : 43\n",
      "    loss           : -182713.37292562396\n",
      "    val_loss       : -100281.93935648601\n",
      "Train Epoch: 44 [128/17352 (1%)] Loss: -176303.953125\n",
      "Train Epoch: 44 [1536/17352 (9%)] Loss: -189952.171875\n",
      "Train Epoch: 44 [2944/17352 (17%)] Loss: -190383.750000\n",
      "Train Epoch: 44 [4352/17352 (25%)] Loss: -210504.359375\n",
      "Train Epoch: 44 [5760/17352 (33%)] Loss: -192245.015625\n",
      "Train Epoch: 44 [7168/17352 (41%)] Loss: -177167.359375\n",
      "Train Epoch: 44 [8576/17352 (49%)] Loss: -191914.156250\n",
      "Train Epoch: 44 [9984/17352 (58%)] Loss: -199153.125000\n",
      "Train Epoch: 44 [11392/17352 (66%)] Loss: -208499.625000\n",
      "Train Epoch: 44 [12800/17352 (74%)] Loss: -192207.312500\n",
      "Train Epoch: 44 [14208/17352 (82%)] Loss: -204395.125000\n",
      "Train Epoch: 44 [15531/17352 (90%)] Loss: -131324.375000\n",
      "Train Epoch: 44 [16118/17352 (93%)] Loss: -53259.937500\n",
      "Train Epoch: 44 [16931/17352 (98%)] Loss: -128724.351562\n",
      "    epoch          : 44\n",
      "    loss           : -182871.33453819735\n",
      "    val_loss       : -100371.20455106099\n",
      "Train Epoch: 45 [128/17352 (1%)] Loss: -205570.406250\n",
      "Train Epoch: 45 [1536/17352 (9%)] Loss: -200672.750000\n",
      "Train Epoch: 45 [2944/17352 (17%)] Loss: -189636.656250\n",
      "Train Epoch: 45 [4352/17352 (25%)] Loss: -192255.203125\n",
      "Train Epoch: 45 [5760/17352 (33%)] Loss: -205203.015625\n",
      "Train Epoch: 45 [7168/17352 (41%)] Loss: -229493.875000\n",
      "Train Epoch: 45 [8576/17352 (49%)] Loss: -201191.359375\n",
      "Train Epoch: 45 [9984/17352 (58%)] Loss: -230757.671875\n",
      "Train Epoch: 45 [11392/17352 (66%)] Loss: -199251.156250\n",
      "Train Epoch: 45 [12800/17352 (74%)] Loss: -206523.687500\n",
      "Train Epoch: 45 [14208/17352 (82%)] Loss: -190732.921875\n",
      "Train Epoch: 45 [15378/17352 (89%)] Loss: -7861.416016\n",
      "Train Epoch: 45 [16221/17352 (93%)] Loss: -139746.500000\n",
      "Train Epoch: 45 [17066/17352 (98%)] Loss: -127447.679688\n",
      "    epoch          : 45\n",
      "    loss           : -182929.18320050335\n",
      "    val_loss       : -100433.67346858978\n",
      "Train Epoch: 46 [128/17352 (1%)] Loss: -208235.062500\n",
      "Train Epoch: 46 [1536/17352 (9%)] Loss: -192815.312500\n",
      "Train Epoch: 46 [2944/17352 (17%)] Loss: -189594.406250\n",
      "Train Epoch: 46 [4352/17352 (25%)] Loss: -192779.906250\n",
      "Train Epoch: 46 [5760/17352 (33%)] Loss: -224720.875000\n",
      "Train Epoch: 46 [7168/17352 (41%)] Loss: -212202.218750\n",
      "Train Epoch: 46 [8576/17352 (49%)] Loss: -188368.781250\n",
      "Train Epoch: 46 [9984/17352 (58%)] Loss: -208861.531250\n",
      "Train Epoch: 46 [11392/17352 (66%)] Loss: -189557.406250\n",
      "Train Epoch: 46 [12800/17352 (74%)] Loss: -201664.562500\n",
      "Train Epoch: 46 [14208/17352 (82%)] Loss: -189339.703125\n",
      "Train Epoch: 46 [15442/17352 (89%)] Loss: -8356.550781\n",
      "Train Epoch: 46 [16273/17352 (94%)] Loss: -59450.718750\n",
      "Train Epoch: 46 [16871/17352 (97%)] Loss: -77038.968750\n",
      "    epoch          : 46\n",
      "    loss           : -182952.50889065122\n",
      "    val_loss       : -100422.17746483485\n",
      "Train Epoch: 47 [128/17352 (1%)] Loss: -207676.531250\n",
      "Train Epoch: 47 [1536/17352 (9%)] Loss: -231924.718750\n",
      "Train Epoch: 47 [2944/17352 (17%)] Loss: -194002.375000\n",
      "Train Epoch: 47 [4352/17352 (25%)] Loss: -192617.843750\n",
      "Train Epoch: 47 [5760/17352 (33%)] Loss: -193342.625000\n",
      "Train Epoch: 47 [7168/17352 (41%)] Loss: -193341.093750\n",
      "Train Epoch: 47 [8576/17352 (49%)] Loss: -224109.906250\n",
      "Train Epoch: 47 [9984/17352 (58%)] Loss: -196811.500000\n",
      "Train Epoch: 47 [11392/17352 (66%)] Loss: -197641.187500\n",
      "Train Epoch: 47 [12800/17352 (74%)] Loss: -193466.140625\n",
      "Train Epoch: 47 [14208/17352 (82%)] Loss: -213075.953125\n",
      "Train Epoch: 47 [15448/17352 (89%)] Loss: -25302.882812\n",
      "Train Epoch: 47 [16409/17352 (95%)] Loss: -144617.468750\n",
      "Train Epoch: 47 [17048/17352 (98%)] Loss: -59677.503906\n",
      "    epoch          : 47\n",
      "    loss           : -182974.89282390414\n",
      "    val_loss       : -100435.26804533004\n",
      "Train Epoch: 48 [128/17352 (1%)] Loss: -173552.937500\n",
      "Train Epoch: 48 [1536/17352 (9%)] Loss: -206369.765625\n",
      "Train Epoch: 48 [2944/17352 (17%)] Loss: -194435.078125\n",
      "Train Epoch: 48 [4352/17352 (25%)] Loss: -205208.234375\n",
      "Train Epoch: 48 [5760/17352 (33%)] Loss: -189697.984375\n",
      "Train Epoch: 48 [7168/17352 (41%)] Loss: -203959.390625\n",
      "Train Epoch: 48 [8576/17352 (49%)] Loss: -196264.656250\n",
      "Train Epoch: 48 [9984/17352 (58%)] Loss: -213017.281250\n",
      "Train Epoch: 48 [11392/17352 (66%)] Loss: -191639.781250\n",
      "Train Epoch: 48 [12800/17352 (74%)] Loss: -201998.687500\n",
      "Train Epoch: 48 [14208/17352 (82%)] Loss: -191342.296875\n",
      "Train Epoch: 48 [15468/17352 (89%)] Loss: -75619.562500\n",
      "Train Epoch: 48 [16270/17352 (94%)] Loss: -177656.093750\n",
      "Train Epoch: 48 [16979/17352 (98%)] Loss: -76280.390625\n",
      "    epoch          : 48\n",
      "    loss           : -183200.07300951658\n",
      "    val_loss       : -100458.91344439189\n",
      "Train Epoch: 49 [128/17352 (1%)] Loss: -206445.031250\n",
      "Train Epoch: 49 [1536/17352 (9%)] Loss: -194129.218750\n",
      "Train Epoch: 49 [2944/17352 (17%)] Loss: -180019.156250\n",
      "Train Epoch: 49 [4352/17352 (25%)] Loss: -229213.296875\n",
      "Train Epoch: 49 [5760/17352 (33%)] Loss: -199898.015625\n",
      "Train Epoch: 49 [7168/17352 (41%)] Loss: -198265.343750\n",
      "Train Epoch: 49 [8576/17352 (49%)] Loss: -178743.140625\n",
      "Train Epoch: 49 [9984/17352 (58%)] Loss: -177383.281250\n",
      "Train Epoch: 49 [11392/17352 (66%)] Loss: -179445.562500\n",
      "Train Epoch: 49 [12800/17352 (74%)] Loss: -206214.625000\n",
      "Train Epoch: 49 [14208/17352 (82%)] Loss: -208720.234375\n",
      "Train Epoch: 49 [15455/17352 (89%)] Loss: -81265.023438\n",
      "Train Epoch: 49 [16311/17352 (94%)] Loss: -172515.171875\n",
      "Train Epoch: 49 [17090/17352 (98%)] Loss: -116905.578125\n",
      "    epoch          : 49\n",
      "    loss           : -183179.9945142093\n",
      "    val_loss       : -100370.61879876455\n",
      "Train Epoch: 50 [128/17352 (1%)] Loss: -172399.468750\n",
      "Train Epoch: 50 [1536/17352 (9%)] Loss: -210193.968750\n",
      "Train Epoch: 50 [2944/17352 (17%)] Loss: -190805.125000\n",
      "Train Epoch: 50 [4352/17352 (25%)] Loss: -191167.843750\n",
      "Train Epoch: 50 [5760/17352 (33%)] Loss: -194689.078125\n",
      "Train Epoch: 50 [7168/17352 (41%)] Loss: -190622.625000\n",
      "Train Epoch: 50 [8576/17352 (49%)] Loss: -196575.218750\n",
      "Train Epoch: 50 [9984/17352 (58%)] Loss: -202499.859375\n",
      "Train Epoch: 50 [11392/17352 (66%)] Loss: -194077.640625\n",
      "Train Epoch: 50 [12800/17352 (74%)] Loss: -195333.031250\n",
      "Train Epoch: 50 [14208/17352 (82%)] Loss: -215082.500000\n",
      "Train Epoch: 50 [15561/17352 (90%)] Loss: -162949.656250\n",
      "Train Epoch: 50 [16344/17352 (94%)] Loss: -129983.062500\n",
      "Train Epoch: 50 [16992/17352 (98%)] Loss: -169760.687500\n",
      "    epoch          : 50\n",
      "    loss           : -183126.69318569105\n",
      "    val_loss       : -100225.61951999665\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0806_145705/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [128/17352 (1%)] Loss: -209418.312500\n",
      "Train Epoch: 51 [1536/17352 (9%)] Loss: -207038.781250\n",
      "Train Epoch: 51 [2944/17352 (17%)] Loss: -194137.390625\n",
      "Train Epoch: 51 [4352/17352 (25%)] Loss: -198534.593750\n",
      "Train Epoch: 51 [5760/17352 (33%)] Loss: -188790.093750\n",
      "Train Epoch: 51 [7168/17352 (41%)] Loss: -208625.343750\n",
      "Train Epoch: 51 [8576/17352 (49%)] Loss: -194347.109375\n",
      "Train Epoch: 51 [9984/17352 (58%)] Loss: -232249.843750\n",
      "Train Epoch: 51 [11392/17352 (66%)] Loss: -211220.437500\n",
      "Train Epoch: 51 [12800/17352 (74%)] Loss: -197659.875000\n",
      "Train Epoch: 51 [14208/17352 (82%)] Loss: -225811.875000\n",
      "Train Epoch: 51 [15492/17352 (89%)] Loss: -115295.695312\n",
      "Train Epoch: 51 [16320/17352 (94%)] Loss: -59219.902344\n",
      "Train Epoch: 51 [17021/17352 (98%)] Loss: -150424.937500\n",
      "    epoch          : 51\n",
      "    loss           : -183121.8226981963\n",
      "    val_loss       : -100358.77731323242\n",
      "Train Epoch: 52 [128/17352 (1%)] Loss: -205406.406250\n",
      "Train Epoch: 52 [1536/17352 (9%)] Loss: -194358.921875\n",
      "Train Epoch: 52 [2944/17352 (17%)] Loss: -184046.671875\n",
      "Train Epoch: 52 [4352/17352 (25%)] Loss: -213042.156250\n",
      "Train Epoch: 52 [5760/17352 (33%)] Loss: -210315.187500\n",
      "Train Epoch: 52 [7168/17352 (41%)] Loss: -171075.718750\n",
      "Train Epoch: 52 [8576/17352 (49%)] Loss: -180566.171875\n",
      "Train Epoch: 52 [9984/17352 (58%)] Loss: -207082.296875\n",
      "Train Epoch: 52 [11392/17352 (66%)] Loss: -206032.781250\n",
      "Train Epoch: 52 [12800/17352 (74%)] Loss: -203201.968750\n",
      "Train Epoch: 52 [14208/17352 (82%)] Loss: -189648.890625\n",
      "Train Epoch: 52 [15546/17352 (90%)] Loss: -175641.187500\n",
      "Train Epoch: 52 [16327/17352 (94%)] Loss: -69465.984375\n",
      "Train Epoch: 52 [16982/17352 (98%)] Loss: -115868.671875\n",
      "    epoch          : 52\n",
      "    loss           : -183207.0451512689\n",
      "    val_loss       : -100573.71462809245\n",
      "Train Epoch: 53 [128/17352 (1%)] Loss: -208759.406250\n",
      "Train Epoch: 53 [1536/17352 (9%)] Loss: -207210.437500\n",
      "Train Epoch: 53 [2944/17352 (17%)] Loss: -176191.984375\n",
      "Train Epoch: 53 [4352/17352 (25%)] Loss: -191908.234375\n",
      "Train Epoch: 53 [5760/17352 (33%)] Loss: -195248.875000\n",
      "Train Epoch: 53 [7168/17352 (41%)] Loss: -194259.593750\n",
      "Train Epoch: 53 [8576/17352 (49%)] Loss: -192604.234375\n",
      "Train Epoch: 53 [9984/17352 (58%)] Loss: -213238.703125\n",
      "Train Epoch: 53 [11392/17352 (66%)] Loss: -207418.875000\n",
      "Train Epoch: 53 [12800/17352 (74%)] Loss: -231308.640625\n",
      "Train Epoch: 53 [14208/17352 (82%)] Loss: -223327.625000\n",
      "Train Epoch: 53 [15566/17352 (90%)] Loss: -162545.015625\n",
      "Train Epoch: 53 [16458/17352 (95%)] Loss: -128817.164062\n",
      "Train Epoch: 53 [17186/17352 (99%)] Loss: -117060.976562\n",
      "    epoch          : 53\n",
      "    loss           : -183352.8811084312\n",
      "    val_loss       : -100516.07657648722\n",
      "Train Epoch: 54 [128/17352 (1%)] Loss: -208077.953125\n",
      "Train Epoch: 54 [1536/17352 (9%)] Loss: -206604.312500\n",
      "Train Epoch: 54 [2944/17352 (17%)] Loss: -192979.000000\n",
      "Train Epoch: 54 [4352/17352 (25%)] Loss: -208062.312500\n",
      "Train Epoch: 54 [5760/17352 (33%)] Loss: -199473.593750\n",
      "Train Epoch: 54 [7168/17352 (41%)] Loss: -204718.406250\n",
      "Train Epoch: 54 [8576/17352 (49%)] Loss: -209807.468750\n",
      "Train Epoch: 54 [9984/17352 (58%)] Loss: -192520.140625\n",
      "Train Epoch: 54 [11392/17352 (66%)] Loss: -208536.500000\n",
      "Train Epoch: 54 [12800/17352 (74%)] Loss: -194102.281250\n",
      "Train Epoch: 54 [14208/17352 (82%)] Loss: -195156.250000\n",
      "Train Epoch: 54 [15504/17352 (89%)] Loss: -78836.609375\n",
      "Train Epoch: 54 [16176/17352 (93%)] Loss: -166112.140625\n",
      "Train Epoch: 54 [16973/17352 (98%)] Loss: -130491.539062\n",
      "    epoch          : 54\n",
      "    loss           : -183383.384824612\n",
      "    val_loss       : -100595.85010318756\n",
      "Train Epoch: 55 [128/17352 (1%)] Loss: -206870.812500\n",
      "Train Epoch: 55 [1536/17352 (9%)] Loss: -189906.328125\n",
      "Train Epoch: 55 [2944/17352 (17%)] Loss: -207379.281250\n",
      "Train Epoch: 55 [4352/17352 (25%)] Loss: -195611.562500\n",
      "Train Epoch: 55 [5760/17352 (33%)] Loss: -205759.093750\n",
      "Train Epoch: 55 [7168/17352 (41%)] Loss: -195297.218750\n",
      "Train Epoch: 55 [8576/17352 (49%)] Loss: -195825.343750\n",
      "Train Epoch: 55 [9984/17352 (58%)] Loss: -212562.281250\n",
      "Train Epoch: 55 [11392/17352 (66%)] Loss: -207298.046875\n",
      "Train Epoch: 55 [12800/17352 (74%)] Loss: -204725.843750\n",
      "Train Epoch: 55 [14208/17352 (82%)] Loss: -187535.078125\n",
      "Train Epoch: 55 [15532/17352 (90%)] Loss: -153541.468750\n",
      "Train Epoch: 55 [16221/17352 (93%)] Loss: -174417.796875\n",
      "Train Epoch: 55 [17024/17352 (98%)] Loss: -114558.117188\n",
      "    epoch          : 55\n",
      "    loss           : -183508.432587694\n",
      "    val_loss       : -100619.5789281845\n",
      "Train Epoch: 56 [128/17352 (1%)] Loss: -175107.625000\n",
      "Train Epoch: 56 [1536/17352 (9%)] Loss: -206297.062500\n",
      "Train Epoch: 56 [2944/17352 (17%)] Loss: -184498.593750\n",
      "Train Epoch: 56 [4352/17352 (25%)] Loss: -212901.031250\n",
      "Train Epoch: 56 [5760/17352 (33%)] Loss: -181524.937500\n",
      "Train Epoch: 56 [7168/17352 (41%)] Loss: -215738.656250\n",
      "Train Epoch: 56 [8576/17352 (49%)] Loss: -195722.546875\n",
      "Train Epoch: 56 [9984/17352 (58%)] Loss: -190201.593750\n",
      "Train Epoch: 56 [11392/17352 (66%)] Loss: -191694.015625\n",
      "Train Epoch: 56 [12800/17352 (74%)] Loss: -205821.406250\n",
      "Train Epoch: 56 [14208/17352 (82%)] Loss: -189438.328125\n",
      "Train Epoch: 56 [15517/17352 (89%)] Loss: -119415.257812\n",
      "Train Epoch: 56 [16120/17352 (93%)] Loss: -153048.609375\n",
      "Train Epoch: 56 [17083/17352 (98%)] Loss: -171822.859375\n",
      "    epoch          : 56\n",
      "    loss           : -183429.85044830118\n",
      "    val_loss       : -100577.95611419677\n",
      "Train Epoch: 57 [128/17352 (1%)] Loss: -172806.750000\n",
      "Train Epoch: 57 [1536/17352 (9%)] Loss: -211736.796875\n",
      "Train Epoch: 57 [2944/17352 (17%)] Loss: -200028.453125\n",
      "Train Epoch: 57 [4352/17352 (25%)] Loss: -199486.703125\n",
      "Train Epoch: 57 [5760/17352 (33%)] Loss: -193517.828125\n",
      "Train Epoch: 57 [7168/17352 (41%)] Loss: -243705.703125\n",
      "Train Epoch: 57 [8576/17352 (49%)] Loss: -208792.468750\n",
      "Train Epoch: 57 [9984/17352 (58%)] Loss: -204530.109375\n",
      "Train Epoch: 57 [11392/17352 (66%)] Loss: -195009.109375\n",
      "Train Epoch: 57 [12800/17352 (74%)] Loss: -195143.109375\n",
      "Train Epoch: 57 [14208/17352 (82%)] Loss: -211913.859375\n",
      "Train Epoch: 57 [15474/17352 (89%)] Loss: -164705.890625\n",
      "Train Epoch: 57 [16111/17352 (93%)] Loss: -59316.859375\n",
      "Train Epoch: 57 [17012/17352 (98%)] Loss: -129613.570312\n",
      "    epoch          : 57\n",
      "    loss           : -183524.26447475355\n",
      "    val_loss       : -100541.57824910482\n",
      "Train Epoch: 58 [128/17352 (1%)] Loss: -191523.437500\n",
      "Train Epoch: 58 [1536/17352 (9%)] Loss: -208502.156250\n",
      "Train Epoch: 58 [2944/17352 (17%)] Loss: -193996.468750\n",
      "Train Epoch: 58 [4352/17352 (25%)] Loss: -211884.171875\n",
      "Train Epoch: 58 [5760/17352 (33%)] Loss: -205479.828125\n",
      "Train Epoch: 58 [7168/17352 (41%)] Loss: -192437.609375\n",
      "Train Epoch: 58 [8576/17352 (49%)] Loss: -193718.812500\n",
      "Train Epoch: 58 [9984/17352 (58%)] Loss: -209598.078125\n",
      "Train Epoch: 58 [11392/17352 (66%)] Loss: -195310.843750\n",
      "Train Epoch: 58 [12800/17352 (74%)] Loss: -194200.078125\n",
      "Train Epoch: 58 [14208/17352 (82%)] Loss: -193187.109375\n",
      "Train Epoch: 58 [15518/17352 (89%)] Loss: -162956.218750\n",
      "Train Epoch: 58 [16129/17352 (93%)] Loss: -116869.281250\n",
      "Train Epoch: 58 [16955/17352 (98%)] Loss: -58396.566406\n",
      "    epoch          : 58\n",
      "    loss           : -183392.36188850147\n",
      "    val_loss       : -100594.11774787903\n",
      "Train Epoch: 59 [128/17352 (1%)] Loss: -180079.906250\n",
      "Train Epoch: 59 [1536/17352 (9%)] Loss: -230196.937500\n",
      "Train Epoch: 59 [2944/17352 (17%)] Loss: -169299.531250\n",
      "Train Epoch: 59 [4352/17352 (25%)] Loss: -197566.062500\n",
      "Train Epoch: 59 [5760/17352 (33%)] Loss: -208866.906250\n",
      "Train Epoch: 59 [7168/17352 (41%)] Loss: -210650.562500\n",
      "Train Epoch: 59 [8576/17352 (49%)] Loss: -188670.171875\n",
      "Train Epoch: 59 [9984/17352 (58%)] Loss: -206676.812500\n",
      "Train Epoch: 59 [11392/17352 (66%)] Loss: -208901.187500\n",
      "Train Epoch: 59 [12800/17352 (74%)] Loss: -193126.953125\n",
      "Train Epoch: 59 [14208/17352 (82%)] Loss: -211007.984375\n",
      "Train Epoch: 59 [15499/17352 (89%)] Loss: -69787.054688\n",
      "Train Epoch: 59 [16308/17352 (94%)] Loss: -165072.656250\n",
      "Train Epoch: 59 [17132/17352 (99%)] Loss: -118547.031250\n",
      "    epoch          : 59\n",
      "    loss           : -183604.94285470847\n",
      "    val_loss       : -100645.78023541768\n",
      "Train Epoch: 60 [128/17352 (1%)] Loss: -176810.937500\n",
      "Train Epoch: 60 [1536/17352 (9%)] Loss: -192057.015625\n",
      "Train Epoch: 60 [2944/17352 (17%)] Loss: -191983.765625\n",
      "Train Epoch: 60 [4352/17352 (25%)] Loss: -192340.546875\n",
      "Train Epoch: 60 [5760/17352 (33%)] Loss: -182659.437500\n",
      "Train Epoch: 60 [7168/17352 (41%)] Loss: -212553.437500\n",
      "Train Epoch: 60 [8576/17352 (49%)] Loss: -194638.828125\n",
      "Train Epoch: 60 [9984/17352 (58%)] Loss: -231900.218750\n",
      "Train Epoch: 60 [11392/17352 (66%)] Loss: -211995.968750\n",
      "Train Epoch: 60 [12800/17352 (74%)] Loss: -191873.640625\n",
      "Train Epoch: 60 [14208/17352 (82%)] Loss: -193757.890625\n",
      "Train Epoch: 60 [15470/17352 (89%)] Loss: -8314.334961\n",
      "Train Epoch: 60 [16236/17352 (94%)] Loss: -120620.976562\n",
      "Train Epoch: 60 [16965/17352 (98%)] Loss: -127493.015625\n",
      "    epoch          : 60\n",
      "    loss           : -183715.25606910654\n",
      "    val_loss       : -100678.21350498199\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0806_145705/checkpoint-epoch60.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 61 [128/17352 (1%)] Loss: -173839.968750\n",
      "Train Epoch: 61 [1536/17352 (9%)] Loss: -199869.109375\n",
      "Train Epoch: 61 [2944/17352 (17%)] Loss: -193609.625000\n",
      "Train Epoch: 61 [4352/17352 (25%)] Loss: -213961.968750\n",
      "Train Epoch: 61 [5760/17352 (33%)] Loss: -225613.687500\n",
      "Train Epoch: 61 [7168/17352 (41%)] Loss: -207980.328125\n",
      "Train Epoch: 61 [8576/17352 (49%)] Loss: -182123.343750\n",
      "Train Epoch: 61 [9984/17352 (58%)] Loss: -215087.937500\n",
      "Train Epoch: 61 [11392/17352 (66%)] Loss: -191853.734375\n",
      "Train Epoch: 61 [12800/17352 (74%)] Loss: -203447.093750\n",
      "Train Epoch: 61 [14208/17352 (82%)] Loss: -210152.781250\n",
      "Train Epoch: 61 [15518/17352 (89%)] Loss: -117891.765625\n",
      "Train Epoch: 61 [16297/17352 (94%)] Loss: -165727.359375\n",
      "Train Epoch: 61 [16920/17352 (98%)] Loss: -60260.339844\n",
      "    epoch          : 61\n",
      "    loss           : -183829.206438103\n",
      "    val_loss       : -100756.36321767171\n",
      "Train Epoch: 62 [128/17352 (1%)] Loss: -211742.062500\n",
      "Train Epoch: 62 [1536/17352 (9%)] Loss: -232237.968750\n",
      "Train Epoch: 62 [2944/17352 (17%)] Loss: -210247.937500\n",
      "Train Epoch: 62 [4352/17352 (25%)] Loss: -205881.890625\n",
      "Train Epoch: 62 [5760/17352 (33%)] Loss: -193050.156250\n",
      "Train Epoch: 62 [7168/17352 (41%)] Loss: -190692.312500\n",
      "Train Epoch: 62 [8576/17352 (49%)] Loss: -224501.328125\n",
      "Train Epoch: 62 [9984/17352 (58%)] Loss: -191082.828125\n",
      "Train Epoch: 62 [11392/17352 (66%)] Loss: -178354.031250\n",
      "Train Epoch: 62 [12800/17352 (74%)] Loss: -189191.687500\n",
      "Train Epoch: 62 [14208/17352 (82%)] Loss: -208760.578125\n",
      "Train Epoch: 62 [15418/17352 (89%)] Loss: -71693.023438\n",
      "Train Epoch: 62 [16236/17352 (94%)] Loss: -82287.328125\n",
      "Train Epoch: 62 [17057/17352 (98%)] Loss: -75908.406250\n",
      "    epoch          : 62\n",
      "    loss           : -183788.20039848992\n",
      "    val_loss       : -100702.5421754837\n",
      "Train Epoch: 63 [128/17352 (1%)] Loss: -207956.250000\n",
      "Train Epoch: 63 [1536/17352 (9%)] Loss: -215023.390625\n",
      "Train Epoch: 63 [2944/17352 (17%)] Loss: -192697.562500\n",
      "Train Epoch: 63 [4352/17352 (25%)] Loss: -194610.281250\n",
      "Train Epoch: 63 [5760/17352 (33%)] Loss: -194748.187500\n",
      "Train Epoch: 63 [7168/17352 (41%)] Loss: -206544.171875\n",
      "Train Epoch: 63 [8576/17352 (49%)] Loss: -196414.750000\n",
      "Train Epoch: 63 [9984/17352 (58%)] Loss: -173660.812500\n",
      "Train Epoch: 63 [11392/17352 (66%)] Loss: -193681.765625\n",
      "Train Epoch: 63 [12800/17352 (74%)] Loss: -203854.656250\n",
      "Train Epoch: 63 [14208/17352 (82%)] Loss: -209103.390625\n",
      "Train Epoch: 63 [15414/17352 (89%)] Loss: -23374.054688\n",
      "Train Epoch: 63 [16207/17352 (93%)] Loss: -59438.546875\n",
      "Train Epoch: 63 [17092/17352 (99%)] Loss: -113208.812500\n",
      "    epoch          : 63\n",
      "    loss           : -183799.0737108064\n",
      "    val_loss       : -100761.64313309987\n",
      "Train Epoch: 64 [128/17352 (1%)] Loss: -177093.000000\n",
      "Train Epoch: 64 [1536/17352 (9%)] Loss: -232502.890625\n",
      "Train Epoch: 64 [2944/17352 (17%)] Loss: -195511.312500\n",
      "Train Epoch: 64 [4352/17352 (25%)] Loss: -192365.843750\n",
      "Train Epoch: 64 [5760/17352 (33%)] Loss: -189627.968750\n",
      "Train Epoch: 64 [7168/17352 (41%)] Loss: -211064.453125\n",
      "Train Epoch: 64 [8576/17352 (49%)] Loss: -191887.468750\n",
      "Train Epoch: 64 [9984/17352 (58%)] Loss: -215734.015625\n",
      "Train Epoch: 64 [11392/17352 (66%)] Loss: -182907.218750\n",
      "Train Epoch: 64 [12800/17352 (74%)] Loss: -195853.187500\n",
      "Train Epoch: 64 [14208/17352 (82%)] Loss: -208396.703125\n",
      "Train Epoch: 64 [15533/17352 (90%)] Loss: -134435.250000\n",
      "Train Epoch: 64 [16259/17352 (94%)] Loss: -124673.476562\n",
      "Train Epoch: 64 [17043/17352 (98%)] Loss: -58394.613281\n",
      "    epoch          : 64\n",
      "    loss           : -183848.0436307152\n",
      "    val_loss       : -100724.00648008983\n",
      "Train Epoch: 65 [128/17352 (1%)] Loss: -206556.968750\n",
      "Train Epoch: 65 [1536/17352 (9%)] Loss: -192151.968750\n",
      "Train Epoch: 65 [2944/17352 (17%)] Loss: -200942.328125\n",
      "Train Epoch: 65 [4352/17352 (25%)] Loss: -200877.000000\n",
      "Train Epoch: 65 [5760/17352 (33%)] Loss: -210202.500000\n",
      "Train Epoch: 65 [7168/17352 (41%)] Loss: -198608.250000\n",
      "Train Epoch: 65 [8576/17352 (49%)] Loss: -225019.500000\n",
      "Train Epoch: 65 [9984/17352 (58%)] Loss: -211730.453125\n",
      "Train Epoch: 65 [11392/17352 (66%)] Loss: -194083.031250\n",
      "Train Epoch: 65 [12800/17352 (74%)] Loss: -202396.218750\n",
      "Train Epoch: 65 [14208/17352 (82%)] Loss: -194162.156250\n",
      "Train Epoch: 65 [15499/17352 (89%)] Loss: -76449.460938\n",
      "Train Epoch: 65 [16486/17352 (95%)] Loss: -162849.171875\n",
      "Train Epoch: 65 [17030/17352 (98%)] Loss: -24000.166016\n",
      "    epoch          : 65\n",
      "    loss           : -183887.13412987627\n",
      "    val_loss       : -100663.43132747014\n",
      "Train Epoch: 66 [128/17352 (1%)] Loss: -209521.984375\n",
      "Train Epoch: 66 [1536/17352 (9%)] Loss: -192074.578125\n",
      "Train Epoch: 66 [2944/17352 (17%)] Loss: -173022.843750\n",
      "Train Epoch: 66 [4352/17352 (25%)] Loss: -197177.250000\n",
      "Train Epoch: 66 [5760/17352 (33%)] Loss: -182562.156250\n",
      "Train Epoch: 66 [7168/17352 (41%)] Loss: -173395.656250\n",
      "Train Epoch: 66 [8576/17352 (49%)] Loss: -200785.281250\n",
      "Train Epoch: 66 [9984/17352 (58%)] Loss: -190166.109375\n",
      "Train Epoch: 66 [11392/17352 (66%)] Loss: -212273.062500\n",
      "Train Epoch: 66 [12800/17352 (74%)] Loss: -207694.031250\n",
      "Train Epoch: 66 [14208/17352 (82%)] Loss: -188996.656250\n",
      "Train Epoch: 66 [15444/17352 (89%)] Loss: -75624.687500\n",
      "Train Epoch: 66 [16259/17352 (94%)] Loss: -4926.392090\n",
      "Train Epoch: 66 [16917/17352 (97%)] Loss: -117526.609375\n",
      "    epoch          : 66\n",
      "    loss           : -183765.97485515414\n",
      "    val_loss       : -100538.91932767232\n",
      "Train Epoch: 67 [128/17352 (1%)] Loss: -207404.531250\n",
      "Train Epoch: 67 [1536/17352 (9%)] Loss: -202459.406250\n",
      "Train Epoch: 67 [2944/17352 (17%)] Loss: -195088.937500\n",
      "Train Epoch: 67 [4352/17352 (25%)] Loss: -206228.281250\n",
      "Train Epoch: 67 [5760/17352 (33%)] Loss: -208995.343750\n",
      "Train Epoch: 67 [7168/17352 (41%)] Loss: -193985.718750\n",
      "Train Epoch: 67 [8576/17352 (49%)] Loss: -193623.343750\n",
      "Train Epoch: 67 [9984/17352 (58%)] Loss: -173258.812500\n",
      "Train Epoch: 67 [11392/17352 (66%)] Loss: -192490.671875\n",
      "Train Epoch: 67 [12800/17352 (74%)] Loss: -207552.656250\n",
      "Train Epoch: 67 [14208/17352 (82%)] Loss: -199040.500000\n",
      "Train Epoch: 67 [15460/17352 (89%)] Loss: -152046.281250\n",
      "Train Epoch: 67 [16136/17352 (93%)] Loss: -59337.703125\n",
      "Train Epoch: 67 [16935/17352 (98%)] Loss: -166737.250000\n",
      "    epoch          : 67\n",
      "    loss           : -183902.12479682258\n",
      "    val_loss       : -100577.70402666727\n",
      "Train Epoch: 68 [128/17352 (1%)] Loss: -208585.093750\n",
      "Train Epoch: 68 [1536/17352 (9%)] Loss: -195576.593750\n",
      "Train Epoch: 68 [2944/17352 (17%)] Loss: -217906.515625\n",
      "Train Epoch: 68 [4352/17352 (25%)] Loss: -190741.718750\n",
      "Train Epoch: 68 [5760/17352 (33%)] Loss: -212977.718750\n",
      "Train Epoch: 68 [7168/17352 (41%)] Loss: -205324.687500\n",
      "Train Epoch: 68 [8576/17352 (49%)] Loss: -190560.640625\n",
      "Train Epoch: 68 [9984/17352 (58%)] Loss: -214199.421875\n",
      "Train Epoch: 68 [11392/17352 (66%)] Loss: -184338.328125\n",
      "Train Epoch: 68 [12800/17352 (74%)] Loss: -203437.937500\n",
      "Train Epoch: 68 [14208/17352 (82%)] Loss: -190669.500000\n",
      "Train Epoch: 68 [15522/17352 (89%)] Loss: -125933.796875\n",
      "Train Epoch: 68 [16337/17352 (94%)] Loss: -83678.281250\n",
      "Train Epoch: 68 [17020/17352 (98%)] Loss: -123808.937500\n",
      "    epoch          : 68\n",
      "    loss           : -183806.6669561399\n",
      "    val_loss       : -100605.83082326253\n",
      "Train Epoch: 69 [128/17352 (1%)] Loss: -176961.296875\n",
      "Train Epoch: 69 [1536/17352 (9%)] Loss: -195011.500000\n",
      "Train Epoch: 69 [2944/17352 (17%)] Loss: -193895.921875\n",
      "Train Epoch: 69 [4352/17352 (25%)] Loss: -198566.687500\n",
      "Train Epoch: 69 [5760/17352 (33%)] Loss: -195217.406250\n",
      "Train Epoch: 69 [7168/17352 (41%)] Loss: -191029.109375\n",
      "Train Epoch: 69 [8576/17352 (49%)] Loss: -225493.187500\n",
      "Train Epoch: 69 [9984/17352 (58%)] Loss: -203402.015625\n",
      "Train Epoch: 69 [11392/17352 (66%)] Loss: -206963.500000\n",
      "Train Epoch: 69 [12800/17352 (74%)] Loss: -194443.140625\n",
      "Train Epoch: 69 [14208/17352 (82%)] Loss: -212082.656250\n",
      "Train Epoch: 69 [15499/17352 (89%)] Loss: -60768.531250\n",
      "Train Epoch: 69 [16372/17352 (94%)] Loss: -23330.417969\n",
      "Train Epoch: 69 [16974/17352 (98%)] Loss: -165680.953125\n",
      "    epoch          : 69\n",
      "    loss           : -183965.81582293415\n",
      "    val_loss       : -100676.38628463745\n",
      "Train Epoch: 70 [128/17352 (1%)] Loss: -213015.421875\n",
      "Train Epoch: 70 [1536/17352 (9%)] Loss: -201958.281250\n",
      "Train Epoch: 70 [2944/17352 (17%)] Loss: -192684.468750\n",
      "Train Epoch: 70 [4352/17352 (25%)] Loss: -198979.109375\n",
      "Train Epoch: 70 [5760/17352 (33%)] Loss: -194684.656250\n",
      "Train Epoch: 70 [7168/17352 (41%)] Loss: -205011.734375\n",
      "Train Epoch: 70 [8576/17352 (49%)] Loss: -209702.187500\n",
      "Train Epoch: 70 [9984/17352 (58%)] Loss: -192843.453125\n",
      "Train Epoch: 70 [11392/17352 (66%)] Loss: -182209.187500\n",
      "Train Epoch: 70 [12800/17352 (74%)] Loss: -194516.375000\n",
      "Train Epoch: 70 [14208/17352 (82%)] Loss: -192042.984375\n",
      "Train Epoch: 70 [15510/17352 (89%)] Loss: -81740.351562\n",
      "Train Epoch: 70 [16259/17352 (94%)] Loss: -160765.687500\n",
      "Train Epoch: 70 [16900/17352 (97%)] Loss: -23467.279297\n",
      "    epoch          : 70\n",
      "    loss           : -184090.97248584311\n",
      "    val_loss       : -100754.9162571907\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0806_145705/checkpoint-epoch70.pth ...\n",
      "Train Epoch: 71 [128/17352 (1%)] Loss: -211630.031250\n",
      "Train Epoch: 71 [1536/17352 (9%)] Loss: -189166.000000\n",
      "Train Epoch: 71 [2944/17352 (17%)] Loss: -209854.359375\n",
      "Train Epoch: 71 [4352/17352 (25%)] Loss: -201460.593750\n",
      "Train Epoch: 71 [5760/17352 (33%)] Loss: -208586.156250\n",
      "Train Epoch: 71 [7168/17352 (41%)] Loss: -173408.000000\n",
      "Train Epoch: 71 [8576/17352 (49%)] Loss: -194407.062500\n",
      "Train Epoch: 71 [9984/17352 (58%)] Loss: -213825.484375\n",
      "Train Epoch: 71 [11392/17352 (66%)] Loss: -209938.140625\n",
      "Train Epoch: 71 [12800/17352 (74%)] Loss: -222873.656250\n",
      "Train Epoch: 71 [14208/17352 (82%)] Loss: -213075.093750\n",
      "Train Epoch: 71 [15464/17352 (89%)] Loss: -5010.725586\n",
      "Train Epoch: 71 [16318/17352 (94%)] Loss: -173014.109375\n",
      "Train Epoch: 71 [17006/17352 (98%)] Loss: -118416.296875\n",
      "    epoch          : 71\n",
      "    loss           : -184180.97573012795\n",
      "    val_loss       : -100768.60406239827\n",
      "Train Epoch: 72 [128/17352 (1%)] Loss: -211429.062500\n",
      "Train Epoch: 72 [1536/17352 (9%)] Loss: -192204.531250\n",
      "Train Epoch: 72 [2944/17352 (17%)] Loss: -218454.218750\n",
      "Train Epoch: 72 [4352/17352 (25%)] Loss: -201562.296875\n",
      "Train Epoch: 72 [5760/17352 (33%)] Loss: -190864.734375\n",
      "Train Epoch: 72 [7168/17352 (41%)] Loss: -207111.437500\n",
      "Train Epoch: 72 [8576/17352 (49%)] Loss: -192546.765625\n",
      "Train Epoch: 72 [9984/17352 (58%)] Loss: -175029.062500\n",
      "Train Epoch: 72 [11392/17352 (66%)] Loss: -194638.703125\n",
      "Train Epoch: 72 [12800/17352 (74%)] Loss: -206369.625000\n",
      "Train Epoch: 72 [14208/17352 (82%)] Loss: -193519.875000\n",
      "Train Epoch: 72 [15519/17352 (89%)] Loss: -108964.515625\n",
      "Train Epoch: 72 [16412/17352 (95%)] Loss: -143966.671875\n",
      "Train Epoch: 72 [17108/17352 (99%)] Loss: -72995.375000\n",
      "    epoch          : 72\n",
      "    loss           : -184153.14327941486\n",
      "    val_loss       : -100646.29329039255\n",
      "Train Epoch: 73 [128/17352 (1%)] Loss: -173352.593750\n",
      "Train Epoch: 73 [1536/17352 (9%)] Loss: -192872.843750\n",
      "Train Epoch: 73 [2944/17352 (17%)] Loss: -214627.890625\n",
      "Train Epoch: 73 [4352/17352 (25%)] Loss: -214163.406250\n",
      "Train Epoch: 73 [5760/17352 (33%)] Loss: -192529.296875\n",
      "Train Epoch: 73 [7168/17352 (41%)] Loss: -209716.390625\n",
      "Train Epoch: 73 [8576/17352 (49%)] Loss: -210178.875000\n",
      "Train Epoch: 73 [9984/17352 (58%)] Loss: -213348.625000\n",
      "Train Epoch: 73 [11392/17352 (66%)] Loss: -209403.828125\n",
      "Train Epoch: 73 [12800/17352 (74%)] Loss: -205358.546875\n",
      "Train Epoch: 73 [14208/17352 (82%)] Loss: -196937.875000\n",
      "Train Epoch: 73 [15450/17352 (89%)] Loss: -128018.734375\n",
      "Train Epoch: 73 [16211/17352 (93%)] Loss: -161259.812500\n",
      "Train Epoch: 73 [17006/17352 (98%)] Loss: -56315.132812\n",
      "    epoch          : 73\n",
      "    loss           : -184091.24214162122\n",
      "    val_loss       : -100687.4847963969\n",
      "Train Epoch: 74 [128/17352 (1%)] Loss: -209994.859375\n",
      "Train Epoch: 74 [1536/17352 (9%)] Loss: -190693.031250\n",
      "Train Epoch: 74 [2944/17352 (17%)] Loss: -177229.453125\n",
      "Train Epoch: 74 [4352/17352 (25%)] Loss: -214163.125000\n",
      "Train Epoch: 74 [5760/17352 (33%)] Loss: -173343.343750\n",
      "Train Epoch: 74 [7168/17352 (41%)] Loss: -198510.406250\n",
      "Train Epoch: 74 [8576/17352 (49%)] Loss: -226178.015625\n",
      "Train Epoch: 74 [9984/17352 (58%)] Loss: -200752.437500\n",
      "Train Epoch: 74 [11392/17352 (66%)] Loss: -209855.843750\n",
      "Train Epoch: 74 [12800/17352 (74%)] Loss: -212666.812500\n",
      "Train Epoch: 74 [14208/17352 (82%)] Loss: -216137.593750\n",
      "Train Epoch: 74 [15531/17352 (90%)] Loss: -128735.195312\n",
      "Train Epoch: 74 [16213/17352 (93%)] Loss: -164656.578125\n",
      "Train Epoch: 74 [17033/17352 (98%)] Loss: -133141.171875\n",
      "    epoch          : 74\n",
      "    loss           : -184242.82988150168\n",
      "    val_loss       : -100713.42890834808\n",
      "Train Epoch: 75 [128/17352 (1%)] Loss: -212683.093750\n",
      "Train Epoch: 75 [1536/17352 (9%)] Loss: -191285.625000\n",
      "Train Epoch: 75 [2944/17352 (17%)] Loss: -179159.234375\n",
      "Train Epoch: 75 [4352/17352 (25%)] Loss: -216255.406250\n",
      "Train Epoch: 75 [5760/17352 (33%)] Loss: -207193.640625\n",
      "Train Epoch: 75 [7168/17352 (41%)] Loss: -205114.656250\n",
      "Train Epoch: 75 [8576/17352 (49%)] Loss: -195665.937500\n",
      "Train Epoch: 75 [9984/17352 (58%)] Loss: -177495.781250\n",
      "Train Epoch: 75 [11392/17352 (66%)] Loss: -213169.812500\n",
      "Train Epoch: 75 [12800/17352 (74%)] Loss: -205154.046875\n",
      "Train Epoch: 75 [14208/17352 (82%)] Loss: -210962.656250\n",
      "Train Epoch: 75 [15388/17352 (89%)] Loss: -23522.142578\n",
      "Train Epoch: 75 [16154/17352 (93%)] Loss: -133987.984375\n",
      "Train Epoch: 75 [16850/17352 (97%)] Loss: -130038.945312\n",
      "    epoch          : 75\n",
      "    loss           : -184298.65389052013\n",
      "    val_loss       : -100681.3368923823\n",
      "Train Epoch: 76 [128/17352 (1%)] Loss: -212566.218750\n",
      "Train Epoch: 76 [1536/17352 (9%)] Loss: -206429.531250\n",
      "Train Epoch: 76 [2944/17352 (17%)] Loss: -211461.671875\n",
      "Train Epoch: 76 [4352/17352 (25%)] Loss: -215526.875000\n",
      "Train Epoch: 76 [5760/17352 (33%)] Loss: -209891.000000\n",
      "Train Epoch: 76 [7168/17352 (41%)] Loss: -205101.828125\n",
      "Train Epoch: 76 [8576/17352 (49%)] Loss: -224269.203125\n",
      "Train Epoch: 76 [9984/17352 (58%)] Loss: -190389.281250\n",
      "Train Epoch: 76 [11392/17352 (66%)] Loss: -211275.515625\n",
      "Train Epoch: 76 [12800/17352 (74%)] Loss: -193674.906250\n",
      "Train Epoch: 76 [14208/17352 (82%)] Loss: -228875.125000\n",
      "Train Epoch: 76 [15449/17352 (89%)] Loss: -141888.343750\n",
      "Train Epoch: 76 [16178/17352 (93%)] Loss: -145379.468750\n",
      "Train Epoch: 76 [17057/17352 (98%)] Loss: -172184.687500\n",
      "    epoch          : 76\n",
      "    loss           : -184501.700817953\n",
      "    val_loss       : -100742.61979516347\n",
      "Train Epoch: 77 [128/17352 (1%)] Loss: -192468.812500\n",
      "Train Epoch: 77 [1536/17352 (9%)] Loss: -212366.500000\n",
      "Train Epoch: 77 [2944/17352 (17%)] Loss: -195107.937500\n",
      "Train Epoch: 77 [4352/17352 (25%)] Loss: -231407.328125\n",
      "Train Epoch: 77 [5760/17352 (33%)] Loss: -208107.906250\n",
      "Train Epoch: 77 [7168/17352 (41%)] Loss: -174486.265625\n",
      "Train Epoch: 77 [8576/17352 (49%)] Loss: -193999.515625\n",
      "Train Epoch: 77 [9984/17352 (58%)] Loss: -172826.296875\n",
      "Train Epoch: 77 [11392/17352 (66%)] Loss: -209792.781250\n",
      "Train Epoch: 77 [12800/17352 (74%)] Loss: -192971.046875\n",
      "Train Epoch: 77 [14208/17352 (82%)] Loss: -210958.687500\n",
      "Train Epoch: 77 [15478/17352 (89%)] Loss: -142309.937500\n",
      "Train Epoch: 77 [16305/17352 (94%)] Loss: -126673.781250\n",
      "Train Epoch: 77 [16985/17352 (98%)] Loss: -152332.046875\n",
      "    epoch          : 77\n",
      "    loss           : -184494.71309380242\n",
      "    val_loss       : -100728.05355593363\n",
      "Train Epoch: 78 [128/17352 (1%)] Loss: -172491.859375\n",
      "Train Epoch: 78 [1536/17352 (9%)] Loss: -209225.656250\n",
      "Train Epoch: 78 [2944/17352 (17%)] Loss: -189610.875000\n",
      "Train Epoch: 78 [4352/17352 (25%)] Loss: -202828.625000\n",
      "Train Epoch: 78 [5760/17352 (33%)] Loss: -210584.000000\n",
      "Train Epoch: 78 [7168/17352 (41%)] Loss: -206750.218750\n",
      "Train Epoch: 78 [8576/17352 (49%)] Loss: -172904.593750\n",
      "Train Epoch: 78 [9984/17352 (58%)] Loss: -199258.656250\n",
      "Train Epoch: 78 [11392/17352 (66%)] Loss: -209471.109375\n",
      "Train Epoch: 78 [12800/17352 (74%)] Loss: -196558.656250\n",
      "Train Epoch: 78 [14208/17352 (82%)] Loss: -216158.171875\n",
      "Train Epoch: 78 [15453/17352 (89%)] Loss: -4788.243164\n",
      "Train Epoch: 78 [16168/17352 (93%)] Loss: -70766.367188\n",
      "Train Epoch: 78 [16933/17352 (98%)] Loss: -175460.875000\n",
      "    epoch          : 78\n",
      "    loss           : -184354.3112088402\n",
      "    val_loss       : -100722.0424088796\n",
      "Train Epoch: 79 [128/17352 (1%)] Loss: -210592.703125\n",
      "Train Epoch: 79 [1536/17352 (9%)] Loss: -200725.140625\n",
      "Train Epoch: 79 [2944/17352 (17%)] Loss: -176468.187500\n",
      "Train Epoch: 79 [4352/17352 (25%)] Loss: -193944.906250\n",
      "Train Epoch: 79 [5760/17352 (33%)] Loss: -192499.390625\n",
      "Train Epoch: 79 [7168/17352 (41%)] Loss: -188934.031250\n",
      "Train Epoch: 79 [8576/17352 (49%)] Loss: -215317.500000\n",
      "Train Epoch: 79 [9984/17352 (58%)] Loss: -209618.406250\n",
      "Train Epoch: 79 [11392/17352 (66%)] Loss: -182290.406250\n",
      "Train Epoch: 79 [12800/17352 (74%)] Loss: -208670.734375\n",
      "Train Epoch: 79 [14208/17352 (82%)] Loss: -225830.593750\n",
      "Train Epoch: 79 [15488/17352 (89%)] Loss: -128393.578125\n",
      "Train Epoch: 79 [16372/17352 (94%)] Loss: -145873.546875\n",
      "Train Epoch: 79 [17100/17352 (99%)] Loss: -8449.676758\n",
      "    epoch          : 79\n",
      "    loss           : -184434.62420039848\n",
      "    val_loss       : -100758.38565508525\n",
      "Train Epoch: 80 [128/17352 (1%)] Loss: -208294.843750\n",
      "Train Epoch: 80 [1536/17352 (9%)] Loss: -199704.281250\n",
      "Train Epoch: 80 [2944/17352 (17%)] Loss: -191830.921875\n",
      "Train Epoch: 80 [4352/17352 (25%)] Loss: -204771.015625\n",
      "Train Epoch: 80 [5760/17352 (33%)] Loss: -225469.000000\n",
      "Train Epoch: 80 [7168/17352 (41%)] Loss: -212553.625000\n",
      "Train Epoch: 80 [8576/17352 (49%)] Loss: -213134.546875\n",
      "Train Epoch: 80 [9984/17352 (58%)] Loss: -231327.031250\n",
      "Train Epoch: 80 [11392/17352 (66%)] Loss: -182625.593750\n",
      "Train Epoch: 80 [12800/17352 (74%)] Loss: -196714.406250\n",
      "Train Epoch: 80 [14208/17352 (82%)] Loss: -208121.875000\n",
      "Train Epoch: 80 [15469/17352 (89%)] Loss: -110414.773438\n",
      "Train Epoch: 80 [16327/17352 (94%)] Loss: -121257.437500\n",
      "Train Epoch: 80 [17020/17352 (98%)] Loss: -172674.953125\n",
      "    epoch          : 80\n",
      "    loss           : -184595.99402592806\n",
      "    val_loss       : -100791.48612219492\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0806_145705/checkpoint-epoch80.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 81 [128/17352 (1%)] Loss: -176044.000000\n",
      "Train Epoch: 81 [1536/17352 (9%)] Loss: -215724.593750\n",
      "Train Epoch: 81 [2944/17352 (17%)] Loss: -174245.515625\n",
      "Train Epoch: 81 [4352/17352 (25%)] Loss: -211832.859375\n",
      "Train Epoch: 81 [5760/17352 (33%)] Loss: -195450.953125\n",
      "Train Epoch: 81 [7168/17352 (41%)] Loss: -190518.625000\n",
      "Train Epoch: 81 [8576/17352 (49%)] Loss: -195645.484375\n",
      "Train Epoch: 81 [9984/17352 (58%)] Loss: -207306.031250\n",
      "Train Epoch: 81 [11392/17352 (66%)] Loss: -183220.671875\n",
      "Train Epoch: 81 [12800/17352 (74%)] Loss: -195371.281250\n",
      "Train Epoch: 81 [14208/17352 (82%)] Loss: -194009.468750\n",
      "Train Epoch: 81 [15546/17352 (90%)] Loss: -176715.125000\n",
      "Train Epoch: 81 [16128/17352 (93%)] Loss: -23710.355469\n",
      "Train Epoch: 81 [17010/17352 (98%)] Loss: -205819.515625\n",
      "    epoch          : 81\n",
      "    loss           : -184542.2727034396\n",
      "    val_loss       : -100578.82925065358\n",
      "Train Epoch: 82 [128/17352 (1%)] Loss: -195167.453125\n",
      "Train Epoch: 82 [1536/17352 (9%)] Loss: -196934.593750\n",
      "Train Epoch: 82 [2944/17352 (17%)] Loss: -194636.218750\n",
      "Train Epoch: 82 [4352/17352 (25%)] Loss: -207909.187500\n",
      "Train Epoch: 82 [5760/17352 (33%)] Loss: -207979.843750\n",
      "Train Epoch: 82 [7168/17352 (41%)] Loss: -195256.625000\n",
      "Train Epoch: 82 [8576/17352 (49%)] Loss: -204089.875000\n",
      "Train Epoch: 82 [9984/17352 (58%)] Loss: -179151.000000\n",
      "Train Epoch: 82 [11392/17352 (66%)] Loss: -192517.312500\n",
      "Train Epoch: 82 [12800/17352 (74%)] Loss: -192711.843750\n",
      "Train Epoch: 82 [14208/17352 (82%)] Loss: -210502.968750\n",
      "Train Epoch: 82 [15552/17352 (90%)] Loss: -123376.929688\n",
      "Train Epoch: 82 [16360/17352 (94%)] Loss: -5275.572754\n",
      "Train Epoch: 82 [16974/17352 (98%)] Loss: -71067.109375\n",
      "    epoch          : 82\n",
      "    loss           : -184522.70643154887\n",
      "    val_loss       : -100785.31793937684\n",
      "Train Epoch: 83 [128/17352 (1%)] Loss: -210762.765625\n",
      "Train Epoch: 83 [1536/17352 (9%)] Loss: -202103.312500\n",
      "Train Epoch: 83 [2944/17352 (17%)] Loss: -193885.437500\n",
      "Train Epoch: 83 [4352/17352 (25%)] Loss: -207846.609375\n",
      "Train Epoch: 83 [5760/17352 (33%)] Loss: -193422.531250\n",
      "Train Epoch: 83 [7168/17352 (41%)] Loss: -213943.625000\n",
      "Train Epoch: 83 [8576/17352 (49%)] Loss: -195406.046875\n",
      "Train Epoch: 83 [9984/17352 (58%)] Loss: -202980.187500\n",
      "Train Epoch: 83 [11392/17352 (66%)] Loss: -191123.562500\n",
      "Train Epoch: 83 [12800/17352 (74%)] Loss: -192117.312500\n",
      "Train Epoch: 83 [14208/17352 (82%)] Loss: -196144.562500\n",
      "Train Epoch: 83 [15425/17352 (89%)] Loss: -25285.523438\n",
      "Train Epoch: 83 [16379/17352 (94%)] Loss: -121956.328125\n",
      "Train Epoch: 83 [17131/17352 (99%)] Loss: -83178.039062\n",
      "    epoch          : 83\n",
      "    loss           : -184638.6476903314\n",
      "    val_loss       : -100730.73436075846\n",
      "Train Epoch: 84 [128/17352 (1%)] Loss: -207770.281250\n",
      "Train Epoch: 84 [1536/17352 (9%)] Loss: -232122.093750\n",
      "Train Epoch: 84 [2944/17352 (17%)] Loss: -175993.062500\n",
      "Train Epoch: 84 [4352/17352 (25%)] Loss: -200901.265625\n",
      "Train Epoch: 84 [5760/17352 (33%)] Loss: -192668.640625\n",
      "Train Epoch: 84 [7168/17352 (41%)] Loss: -198199.250000\n",
      "Train Epoch: 84 [8576/17352 (49%)] Loss: -192924.484375\n",
      "Train Epoch: 84 [9984/17352 (58%)] Loss: -205722.812500\n",
      "Train Epoch: 84 [11392/17352 (66%)] Loss: -202756.640625\n",
      "Train Epoch: 84 [12800/17352 (74%)] Loss: -198160.000000\n",
      "Train Epoch: 84 [14208/17352 (82%)] Loss: -211730.578125\n",
      "Train Epoch: 84 [15502/17352 (89%)] Loss: -82861.265625\n",
      "Train Epoch: 84 [16288/17352 (94%)] Loss: -129433.656250\n",
      "Train Epoch: 84 [17028/17352 (98%)] Loss: -61429.171875\n",
      "    epoch          : 84\n",
      "    loss           : -184674.97948563338\n",
      "    val_loss       : -100789.16412614187\n",
      "Train Epoch: 85 [128/17352 (1%)] Loss: -207870.843750\n",
      "Train Epoch: 85 [1536/17352 (9%)] Loss: -198517.734375\n",
      "Train Epoch: 85 [2944/17352 (17%)] Loss: -173836.703125\n",
      "Train Epoch: 85 [4352/17352 (25%)] Loss: -192208.312500\n",
      "Train Epoch: 85 [5760/17352 (33%)] Loss: -201783.234375\n",
      "Train Epoch: 85 [7168/17352 (41%)] Loss: -214903.921875\n",
      "Train Epoch: 85 [8576/17352 (49%)] Loss: -196291.453125\n",
      "Train Epoch: 85 [9984/17352 (58%)] Loss: -209394.218750\n",
      "Train Epoch: 85 [11392/17352 (66%)] Loss: -191532.500000\n",
      "Train Epoch: 85 [12800/17352 (74%)] Loss: -204132.812500\n",
      "Train Epoch: 85 [14208/17352 (82%)] Loss: -194933.546875\n",
      "Train Epoch: 85 [15503/17352 (89%)] Loss: -174707.218750\n",
      "Train Epoch: 85 [16249/17352 (94%)] Loss: -176271.437500\n",
      "Train Epoch: 85 [17056/17352 (98%)] Loss: -116954.093750\n",
      "    epoch          : 85\n",
      "    loss           : -184746.36872771604\n",
      "    val_loss       : -100685.57116832733\n",
      "Train Epoch: 86 [128/17352 (1%)] Loss: -213436.859375\n",
      "Train Epoch: 86 [1536/17352 (9%)] Loss: -205822.156250\n",
      "Train Epoch: 86 [2944/17352 (17%)] Loss: -182566.375000\n",
      "Train Epoch: 86 [4352/17352 (25%)] Loss: -207639.812500\n",
      "Train Epoch: 86 [5760/17352 (33%)] Loss: -207533.515625\n",
      "Train Epoch: 86 [7168/17352 (41%)] Loss: -205471.656250\n",
      "Train Epoch: 86 [8576/17352 (49%)] Loss: -216731.812500\n",
      "Train Epoch: 86 [9984/17352 (58%)] Loss: -206687.703125\n",
      "Train Epoch: 86 [11392/17352 (66%)] Loss: -192808.937500\n",
      "Train Epoch: 86 [12800/17352 (74%)] Loss: -203217.390625\n",
      "Train Epoch: 86 [14208/17352 (82%)] Loss: -210415.140625\n",
      "Train Epoch: 86 [15478/17352 (89%)] Loss: -23983.767578\n",
      "Train Epoch: 86 [16357/17352 (94%)] Loss: -141358.765625\n",
      "Train Epoch: 86 [17014/17352 (98%)] Loss: -60751.171875\n",
      "    epoch          : 86\n",
      "    loss           : -184724.843691013\n",
      "    val_loss       : -100640.17243588765\n",
      "Train Epoch: 87 [128/17352 (1%)] Loss: -175707.125000\n",
      "Train Epoch: 87 [1536/17352 (9%)] Loss: -200022.218750\n",
      "Train Epoch: 87 [2944/17352 (17%)] Loss: -215678.859375\n",
      "Train Epoch: 87 [4352/17352 (25%)] Loss: -215454.734375\n",
      "Train Epoch: 87 [5760/17352 (33%)] Loss: -194301.281250\n",
      "Train Epoch: 87 [7168/17352 (41%)] Loss: -199932.015625\n",
      "Train Epoch: 87 [8576/17352 (49%)] Loss: -190360.609375\n",
      "Train Epoch: 87 [9984/17352 (58%)] Loss: -215890.421875\n",
      "Train Epoch: 87 [11392/17352 (66%)] Loss: -177718.843750\n",
      "Train Epoch: 87 [12800/17352 (74%)] Loss: -208751.171875\n",
      "Train Epoch: 87 [14208/17352 (82%)] Loss: -209870.281250\n",
      "Train Epoch: 87 [15499/17352 (89%)] Loss: -59561.632812\n",
      "Train Epoch: 87 [16181/17352 (93%)] Loss: -125118.562500\n",
      "Train Epoch: 87 [17037/17352 (98%)] Loss: -151762.296875\n",
      "    epoch          : 87\n",
      "    loss           : -184506.15718396078\n",
      "    val_loss       : -100841.4580634435\n",
      "Train Epoch: 88 [128/17352 (1%)] Loss: -212324.093750\n",
      "Train Epoch: 88 [1536/17352 (9%)] Loss: -194044.218750\n",
      "Train Epoch: 88 [2944/17352 (17%)] Loss: -194132.906250\n",
      "Train Epoch: 88 [4352/17352 (25%)] Loss: -214906.234375\n",
      "Train Epoch: 88 [5760/17352 (33%)] Loss: -190622.781250\n",
      "Train Epoch: 88 [7168/17352 (41%)] Loss: -195759.031250\n",
      "Train Epoch: 88 [8576/17352 (49%)] Loss: -225019.296875\n",
      "Train Epoch: 88 [9984/17352 (58%)] Loss: -207231.406250\n",
      "Train Epoch: 88 [11392/17352 (66%)] Loss: -190414.328125\n",
      "Train Epoch: 88 [12800/17352 (74%)] Loss: -225766.531250\n",
      "Train Epoch: 88 [14208/17352 (82%)] Loss: -212089.968750\n",
      "Train Epoch: 88 [15457/17352 (89%)] Loss: -142720.875000\n",
      "Train Epoch: 88 [16270/17352 (94%)] Loss: -203841.750000\n",
      "Train Epoch: 88 [16997/17352 (98%)] Loss: -140904.593750\n",
      "    epoch          : 88\n",
      "    loss           : -184648.35740221266\n",
      "    val_loss       : -100796.54718335469\n",
      "Train Epoch: 89 [128/17352 (1%)] Loss: -192075.375000\n",
      "Train Epoch: 89 [1536/17352 (9%)] Loss: -210679.734375\n",
      "Train Epoch: 89 [2944/17352 (17%)] Loss: -231404.218750\n",
      "Train Epoch: 89 [4352/17352 (25%)] Loss: -195518.750000\n",
      "Train Epoch: 89 [5760/17352 (33%)] Loss: -182276.187500\n",
      "Train Epoch: 89 [7168/17352 (41%)] Loss: -194314.718750\n",
      "Train Epoch: 89 [8576/17352 (49%)] Loss: -191197.921875\n",
      "Train Epoch: 89 [9984/17352 (58%)] Loss: -210696.734375\n",
      "Train Epoch: 89 [11392/17352 (66%)] Loss: -181502.468750\n",
      "Train Epoch: 89 [12800/17352 (74%)] Loss: -207556.234375\n",
      "Train Epoch: 89 [14208/17352 (82%)] Loss: -192601.687500\n",
      "Train Epoch: 89 [15542/17352 (90%)] Loss: -136701.343750\n",
      "Train Epoch: 89 [16261/17352 (94%)] Loss: -84519.125000\n",
      "Train Epoch: 89 [16935/17352 (98%)] Loss: -8359.168945\n",
      "    epoch          : 89\n",
      "    loss           : -184710.35654362416\n",
      "    val_loss       : -100734.31364669799\n",
      "Train Epoch: 90 [128/17352 (1%)] Loss: -209529.625000\n",
      "Train Epoch: 90 [1536/17352 (9%)] Loss: -208290.718750\n",
      "Train Epoch: 90 [2944/17352 (17%)] Loss: -242063.937500\n",
      "Train Epoch: 90 [4352/17352 (25%)] Loss: -191845.500000\n",
      "Train Epoch: 90 [5760/17352 (33%)] Loss: -213133.468750\n",
      "Train Epoch: 90 [7168/17352 (41%)] Loss: -206566.750000\n",
      "Train Epoch: 90 [8576/17352 (49%)] Loss: -227317.375000\n",
      "Train Epoch: 90 [9984/17352 (58%)] Loss: -171817.640625\n",
      "Train Epoch: 90 [11392/17352 (66%)] Loss: -192519.921875\n",
      "Train Epoch: 90 [12800/17352 (74%)] Loss: -194577.953125\n",
      "Train Epoch: 90 [14208/17352 (82%)] Loss: -210754.593750\n",
      "Train Epoch: 90 [15542/17352 (90%)] Loss: -176157.718750\n",
      "Train Epoch: 90 [16382/17352 (94%)] Loss: -144428.468750\n",
      "Train Epoch: 90 [17066/17352 (98%)] Loss: -80837.242188\n",
      "    epoch          : 90\n",
      "    loss           : -184847.16108037962\n",
      "    val_loss       : -100797.14063154857\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0806_145705/checkpoint-epoch90.pth ...\n",
      "Train Epoch: 91 [128/17352 (1%)] Loss: -208100.390625\n",
      "Train Epoch: 91 [1536/17352 (9%)] Loss: -194349.531250\n",
      "Train Epoch: 91 [2944/17352 (17%)] Loss: -225227.421875\n",
      "Train Epoch: 91 [4352/17352 (25%)] Loss: -231491.750000\n",
      "Train Epoch: 91 [5760/17352 (33%)] Loss: -207456.828125\n",
      "Train Epoch: 91 [7168/17352 (41%)] Loss: -193719.125000\n",
      "Train Epoch: 91 [8576/17352 (49%)] Loss: -194495.562500\n",
      "Train Epoch: 91 [9984/17352 (58%)] Loss: -203435.187500\n",
      "Train Epoch: 91 [11392/17352 (66%)] Loss: -193510.625000\n",
      "Train Epoch: 91 [12800/17352 (74%)] Loss: -196178.234375\n",
      "Train Epoch: 91 [14208/17352 (82%)] Loss: -212522.875000\n",
      "Train Epoch: 91 [15489/17352 (89%)] Loss: -55801.656250\n",
      "Train Epoch: 91 [16359/17352 (94%)] Loss: -70224.859375\n",
      "Train Epoch: 91 [17108/17352 (99%)] Loss: -147522.890625\n",
      "    epoch          : 91\n",
      "    loss           : -184700.62888331062\n",
      "    val_loss       : -100723.4765701294\n",
      "Train Epoch: 92 [128/17352 (1%)] Loss: -192678.343750\n",
      "Train Epoch: 92 [1536/17352 (9%)] Loss: -209984.437500\n",
      "Train Epoch: 92 [2944/17352 (17%)] Loss: -193034.218750\n",
      "Train Epoch: 92 [4352/17352 (25%)] Loss: -194365.671875\n",
      "Train Epoch: 92 [5760/17352 (33%)] Loss: -204832.750000\n",
      "Train Epoch: 92 [7168/17352 (41%)] Loss: -193962.593750\n",
      "Train Epoch: 92 [8576/17352 (49%)] Loss: -212780.140625\n",
      "Train Epoch: 92 [9984/17352 (58%)] Loss: -195435.640625\n",
      "Train Epoch: 92 [11392/17352 (66%)] Loss: -182465.890625\n",
      "Train Epoch: 92 [12800/17352 (74%)] Loss: -210069.453125\n",
      "Train Epoch: 92 [14208/17352 (82%)] Loss: -207396.125000\n",
      "Train Epoch: 92 [15469/17352 (89%)] Loss: -59018.421875\n",
      "Train Epoch: 92 [16169/17352 (93%)] Loss: -76216.078125\n",
      "Train Epoch: 92 [17003/17352 (98%)] Loss: -134361.375000\n",
      "    epoch          : 92\n",
      "    loss           : -184888.2866243708\n",
      "    val_loss       : -100893.5604350408\n",
      "Train Epoch: 93 [128/17352 (1%)] Loss: -176884.703125\n",
      "Train Epoch: 93 [1536/17352 (9%)] Loss: -216437.921875\n",
      "Train Epoch: 93 [2944/17352 (17%)] Loss: -248290.875000\n",
      "Train Epoch: 93 [4352/17352 (25%)] Loss: -192968.015625\n",
      "Train Epoch: 93 [5760/17352 (33%)] Loss: -207759.281250\n",
      "Train Epoch: 93 [7168/17352 (41%)] Loss: -192719.281250\n",
      "Train Epoch: 93 [8576/17352 (49%)] Loss: -194851.656250\n",
      "Train Epoch: 93 [9984/17352 (58%)] Loss: -207250.562500\n",
      "Train Epoch: 93 [11392/17352 (66%)] Loss: -192632.562500\n",
      "Train Epoch: 93 [12800/17352 (74%)] Loss: -208631.062500\n",
      "Train Epoch: 93 [14208/17352 (82%)] Loss: -213216.437500\n",
      "Train Epoch: 93 [15406/17352 (89%)] Loss: -56694.156250\n",
      "Train Epoch: 93 [16148/17352 (93%)] Loss: -128114.085938\n",
      "Train Epoch: 93 [17023/17352 (98%)] Loss: -82187.328125\n",
      "    epoch          : 93\n",
      "    loss           : -184949.63366781146\n",
      "    val_loss       : -100826.29952691396\n",
      "Train Epoch: 94 [128/17352 (1%)] Loss: -204957.625000\n",
      "Train Epoch: 94 [1536/17352 (9%)] Loss: -209352.859375\n",
      "Train Epoch: 94 [2944/17352 (17%)] Loss: -225591.328125\n",
      "Train Epoch: 94 [4352/17352 (25%)] Loss: -182745.671875\n",
      "Train Epoch: 94 [5760/17352 (33%)] Loss: -209053.359375\n",
      "Train Epoch: 94 [7168/17352 (41%)] Loss: -194975.187500\n",
      "Train Epoch: 94 [8576/17352 (49%)] Loss: -193764.968750\n",
      "Train Epoch: 94 [9984/17352 (58%)] Loss: -173434.937500\n",
      "Train Epoch: 94 [11392/17352 (66%)] Loss: -181870.046875\n",
      "Train Epoch: 94 [12800/17352 (74%)] Loss: -191948.500000\n",
      "Train Epoch: 94 [14208/17352 (82%)] Loss: -193334.640625\n",
      "Train Epoch: 94 [15419/17352 (89%)] Loss: -8395.388672\n",
      "Train Epoch: 94 [16156/17352 (93%)] Loss: -144575.906250\n",
      "Train Epoch: 94 [16862/17352 (97%)] Loss: -119277.984375\n",
      "    epoch          : 94\n",
      "    loss           : -184935.3912050388\n",
      "    val_loss       : -100388.77105464935\n",
      "Train Epoch: 95 [128/17352 (1%)] Loss: -206055.968750\n",
      "Train Epoch: 95 [1536/17352 (9%)] Loss: -199820.937500\n",
      "Train Epoch: 95 [2944/17352 (17%)] Loss: -226798.421875\n",
      "Train Epoch: 95 [4352/17352 (25%)] Loss: -198013.484375\n",
      "Train Epoch: 95 [5760/17352 (33%)] Loss: -208585.734375\n",
      "Train Epoch: 95 [7168/17352 (41%)] Loss: -206828.718750\n",
      "Train Epoch: 95 [8576/17352 (49%)] Loss: -193843.781250\n",
      "Train Epoch: 95 [9984/17352 (58%)] Loss: -194280.156250\n",
      "Train Epoch: 95 [11392/17352 (66%)] Loss: -211696.015625\n",
      "Train Epoch: 95 [12800/17352 (74%)] Loss: -198223.312500\n",
      "Train Epoch: 95 [14208/17352 (82%)] Loss: -210219.984375\n",
      "Train Epoch: 95 [15528/17352 (89%)] Loss: -122778.570312\n",
      "Train Epoch: 95 [16343/17352 (94%)] Loss: -8975.001953\n",
      "Train Epoch: 95 [17158/17352 (99%)] Loss: -204593.468750\n",
      "    epoch          : 95\n",
      "    loss           : -184887.68662174916\n",
      "    val_loss       : -100564.11765518188\n",
      "Train Epoch: 96 [128/17352 (1%)] Loss: -209076.953125\n",
      "Train Epoch: 96 [1536/17352 (9%)] Loss: -214521.750000\n",
      "Train Epoch: 96 [2944/17352 (17%)] Loss: -200328.171875\n",
      "Train Epoch: 96 [4352/17352 (25%)] Loss: -198031.937500\n",
      "Train Epoch: 96 [5760/17352 (33%)] Loss: -194001.093750\n",
      "Train Epoch: 96 [7168/17352 (41%)] Loss: -200644.375000\n",
      "Train Epoch: 96 [8576/17352 (49%)] Loss: -192203.687500\n",
      "Train Epoch: 96 [9984/17352 (58%)] Loss: -207302.093750\n",
      "Train Epoch: 96 [11392/17352 (66%)] Loss: -212047.312500\n",
      "Train Epoch: 96 [12800/17352 (74%)] Loss: -205739.578125\n",
      "Train Epoch: 96 [14208/17352 (82%)] Loss: -213524.812500\n",
      "Train Epoch: 96 [15547/17352 (90%)] Loss: -177085.765625\n",
      "Train Epoch: 96 [16384/17352 (94%)] Loss: -131528.406250\n",
      "Train Epoch: 96 [17080/17352 (98%)] Loss: -81661.781250\n",
      "    epoch          : 96\n",
      "    loss           : -184960.02751087982\n",
      "    val_loss       : -100813.57669887543\n",
      "Train Epoch: 97 [128/17352 (1%)] Loss: -212557.812500\n",
      "Train Epoch: 97 [1536/17352 (9%)] Loss: -210303.671875\n",
      "Train Epoch: 97 [2944/17352 (17%)] Loss: -195482.343750\n",
      "Train Epoch: 97 [4352/17352 (25%)] Loss: -195948.656250\n",
      "Train Epoch: 97 [5760/17352 (33%)] Loss: -205488.281250\n",
      "Train Epoch: 97 [7168/17352 (41%)] Loss: -215081.593750\n",
      "Train Epoch: 97 [8576/17352 (49%)] Loss: -196984.562500\n",
      "Train Epoch: 97 [9984/17352 (58%)] Loss: -193615.343750\n",
      "Train Epoch: 97 [11392/17352 (66%)] Loss: -184655.312500\n",
      "Train Epoch: 97 [12800/17352 (74%)] Loss: -194804.328125\n",
      "Train Epoch: 97 [14208/17352 (82%)] Loss: -197330.343750\n",
      "Train Epoch: 97 [15508/17352 (89%)] Loss: -112841.875000\n",
      "Train Epoch: 97 [16197/17352 (93%)] Loss: -205018.250000\n",
      "Train Epoch: 97 [17066/17352 (98%)] Loss: -135118.234375\n",
      "    epoch          : 97\n",
      "    loss           : -185095.4335642565\n",
      "    val_loss       : -100772.9590285619\n",
      "Train Epoch: 98 [128/17352 (1%)] Loss: -209896.531250\n",
      "Train Epoch: 98 [1536/17352 (9%)] Loss: -208352.578125\n",
      "Train Epoch: 98 [2944/17352 (17%)] Loss: -214310.218750\n",
      "Train Epoch: 98 [4352/17352 (25%)] Loss: -179619.000000\n",
      "Train Epoch: 98 [5760/17352 (33%)] Loss: -193973.015625\n",
      "Train Epoch: 98 [7168/17352 (41%)] Loss: -174102.546875\n",
      "Train Epoch: 98 [8576/17352 (49%)] Loss: -229154.781250\n",
      "Train Epoch: 98 [9984/17352 (58%)] Loss: -196368.218750\n",
      "Train Epoch: 98 [11392/17352 (66%)] Loss: -190827.187500\n",
      "Train Epoch: 98 [12800/17352 (74%)] Loss: -204706.125000\n",
      "Train Epoch: 98 [14208/17352 (82%)] Loss: -211004.281250\n",
      "Train Epoch: 98 [15523/17352 (89%)] Loss: -132967.265625\n",
      "Train Epoch: 98 [16225/17352 (94%)] Loss: -134129.734375\n",
      "Train Epoch: 98 [17023/17352 (98%)] Loss: -166997.140625\n",
      "    epoch          : 98\n",
      "    loss           : -185148.32754168415\n",
      "    val_loss       : -100833.31626440684\n",
      "Train Epoch: 99 [128/17352 (1%)] Loss: -212487.859375\n",
      "Train Epoch: 99 [1536/17352 (9%)] Loss: -193881.718750\n",
      "Train Epoch: 99 [2944/17352 (17%)] Loss: -196670.250000\n",
      "Train Epoch: 99 [4352/17352 (25%)] Loss: -215457.156250\n",
      "Train Epoch: 99 [5760/17352 (33%)] Loss: -203864.031250\n",
      "Train Epoch: 99 [7168/17352 (41%)] Loss: -215661.984375\n",
      "Train Epoch: 99 [8576/17352 (49%)] Loss: -190690.109375\n",
      "Train Epoch: 99 [9984/17352 (58%)] Loss: -209325.890625\n",
      "Train Epoch: 99 [11392/17352 (66%)] Loss: -202068.484375\n",
      "Train Epoch: 99 [12800/17352 (74%)] Loss: -208143.781250\n",
      "Train Epoch: 99 [14208/17352 (82%)] Loss: -194250.562500\n",
      "Train Epoch: 99 [15538/17352 (90%)] Loss: -164571.812500\n",
      "Train Epoch: 99 [16172/17352 (93%)] Loss: -75571.109375\n",
      "Train Epoch: 99 [17039/17352 (98%)] Loss: -145709.484375\n",
      "    epoch          : 99\n",
      "    loss           : -185192.64152619021\n",
      "    val_loss       : -100848.84447682698\n",
      "Train Epoch: 100 [128/17352 (1%)] Loss: -192995.640625\n",
      "Train Epoch: 100 [1536/17352 (9%)] Loss: -214710.359375\n",
      "Train Epoch: 100 [2944/17352 (17%)] Loss: -193740.859375\n",
      "Train Epoch: 100 [4352/17352 (25%)] Loss: -193076.062500\n",
      "Train Epoch: 100 [5760/17352 (33%)] Loss: -195932.890625\n",
      "Train Epoch: 100 [7168/17352 (41%)] Loss: -195497.734375\n",
      "Train Epoch: 100 [8576/17352 (49%)] Loss: -228494.593750\n",
      "Train Epoch: 100 [9984/17352 (58%)] Loss: -193655.812500\n",
      "Train Epoch: 100 [11392/17352 (66%)] Loss: -214386.437500\n",
      "Train Epoch: 100 [12800/17352 (74%)] Loss: -193615.718750\n",
      "Train Epoch: 100 [14208/17352 (82%)] Loss: -181974.718750\n",
      "Train Epoch: 100 [15530/17352 (89%)] Loss: -135818.640625\n",
      "Train Epoch: 100 [16278/17352 (94%)] Loss: -206339.203125\n",
      "Train Epoch: 100 [17113/17352 (99%)] Loss: -152740.562500\n",
      "    epoch          : 100\n",
      "    loss           : -185245.8740234375\n",
      "    val_loss       : -100881.88905207317\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0806_145705/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [128/17352 (1%)] Loss: -209421.343750\n",
      "Train Epoch: 101 [1536/17352 (9%)] Loss: -213777.875000\n",
      "Train Epoch: 101 [2944/17352 (17%)] Loss: -190810.343750\n",
      "Train Epoch: 101 [4352/17352 (25%)] Loss: -193742.859375\n",
      "Train Epoch: 101 [5760/17352 (33%)] Loss: -224037.734375\n",
      "Train Epoch: 101 [7168/17352 (41%)] Loss: -244427.796875\n",
      "Train Epoch: 101 [8576/17352 (49%)] Loss: -194979.187500\n",
      "Train Epoch: 101 [9984/17352 (58%)] Loss: -205276.125000\n",
      "Train Epoch: 101 [11392/17352 (66%)] Loss: -185152.468750\n",
      "Train Epoch: 101 [12800/17352 (74%)] Loss: -194273.187500\n",
      "Train Epoch: 101 [14208/17352 (82%)] Loss: -226841.968750\n",
      "Train Epoch: 101 [15551/17352 (90%)] Loss: -118504.609375\n",
      "Train Epoch: 101 [16530/17352 (95%)] Loss: -171467.109375\n",
      "Train Epoch: 101 [17013/17352 (98%)] Loss: -126056.625000\n",
      "    epoch          : 101\n",
      "    loss           : -185169.64438050546\n",
      "    val_loss       : -100846.70280183156\n",
      "Train Epoch: 102 [128/17352 (1%)] Loss: -211668.500000\n",
      "Train Epoch: 102 [1536/17352 (9%)] Loss: -197752.562500\n",
      "Train Epoch: 102 [2944/17352 (17%)] Loss: -174176.734375\n",
      "Train Epoch: 102 [4352/17352 (25%)] Loss: -192522.687500\n",
      "Train Epoch: 102 [5760/17352 (33%)] Loss: -194640.250000\n",
      "Train Epoch: 102 [7168/17352 (41%)] Loss: -175260.531250\n",
      "Train Epoch: 102 [8576/17352 (49%)] Loss: -229175.984375\n",
      "Train Epoch: 102 [9984/17352 (58%)] Loss: -210532.250000\n",
      "Train Epoch: 102 [11392/17352 (66%)] Loss: -194122.828125\n",
      "Train Epoch: 102 [12800/17352 (74%)] Loss: -195324.453125\n",
      "Train Epoch: 102 [14208/17352 (82%)] Loss: -227041.656250\n",
      "Train Epoch: 102 [15479/17352 (89%)] Loss: -59529.632812\n",
      "Train Epoch: 102 [16255/17352 (94%)] Loss: -59444.515625\n",
      "Train Epoch: 102 [16967/17352 (98%)] Loss: -174366.140625\n",
      "    epoch          : 102\n",
      "    loss           : -185321.53460898175\n",
      "    val_loss       : -100952.61992473602\n",
      "Train Epoch: 103 [128/17352 (1%)] Loss: -178949.750000\n",
      "Train Epoch: 103 [1536/17352 (9%)] Loss: -209271.687500\n",
      "Train Epoch: 103 [2944/17352 (17%)] Loss: -212147.906250\n",
      "Train Epoch: 103 [4352/17352 (25%)] Loss: -200249.468750\n",
      "Train Epoch: 103 [5760/17352 (33%)] Loss: -210382.468750\n",
      "Train Epoch: 103 [7168/17352 (41%)] Loss: -213832.906250\n",
      "Train Epoch: 103 [8576/17352 (49%)] Loss: -228251.359375\n",
      "Train Epoch: 103 [9984/17352 (58%)] Loss: -214530.406250\n",
      "Train Epoch: 103 [11392/17352 (66%)] Loss: -211198.234375\n",
      "Train Epoch: 103 [12800/17352 (74%)] Loss: -210764.781250\n",
      "Train Epoch: 103 [14208/17352 (82%)] Loss: -216060.234375\n",
      "Train Epoch: 103 [15522/17352 (89%)] Loss: -142082.312500\n",
      "Train Epoch: 103 [16266/17352 (94%)] Loss: -206956.968750\n",
      "Train Epoch: 103 [17044/17352 (98%)] Loss: -71570.687500\n",
      "    epoch          : 103\n",
      "    loss           : -185361.00057676175\n",
      "    val_loss       : -100883.47893288931\n",
      "Train Epoch: 104 [128/17352 (1%)] Loss: -210372.703125\n",
      "Train Epoch: 104 [1536/17352 (9%)] Loss: -201758.812500\n",
      "Train Epoch: 104 [2944/17352 (17%)] Loss: -194168.390625\n",
      "Train Epoch: 104 [4352/17352 (25%)] Loss: -195103.843750\n",
      "Train Epoch: 104 [5760/17352 (33%)] Loss: -226467.500000\n",
      "Train Epoch: 104 [7168/17352 (41%)] Loss: -191016.156250\n",
      "Train Epoch: 104 [8576/17352 (49%)] Loss: -195000.953125\n",
      "Train Epoch: 104 [9984/17352 (58%)] Loss: -232608.578125\n",
      "Train Epoch: 104 [11392/17352 (66%)] Loss: -190183.593750\n",
      "Train Epoch: 104 [12800/17352 (74%)] Loss: -196608.359375\n",
      "Train Epoch: 104 [14208/17352 (82%)] Loss: -210479.656250\n",
      "Train Epoch: 104 [15492/17352 (89%)] Loss: -82586.601562\n",
      "Train Epoch: 104 [16109/17352 (93%)] Loss: -127850.468750\n",
      "Train Epoch: 104 [17015/17352 (98%)] Loss: -136412.968750\n",
      "    epoch          : 104\n",
      "    loss           : -185157.41509149538\n",
      "    val_loss       : -100795.20399112701\n",
      "Train Epoch: 105 [128/17352 (1%)] Loss: -207867.562500\n",
      "Train Epoch: 105 [1536/17352 (9%)] Loss: -214162.750000\n",
      "Train Epoch: 105 [2944/17352 (17%)] Loss: -210988.312500\n",
      "Train Epoch: 105 [4352/17352 (25%)] Loss: -195227.015625\n",
      "Train Epoch: 105 [5760/17352 (33%)] Loss: -189990.156250\n",
      "Train Epoch: 105 [7168/17352 (41%)] Loss: -205745.765625\n",
      "Train Epoch: 105 [8576/17352 (49%)] Loss: -198286.796875\n",
      "Train Epoch: 105 [9984/17352 (58%)] Loss: -193924.656250\n",
      "Train Epoch: 105 [11392/17352 (66%)] Loss: -191546.656250\n",
      "Train Epoch: 105 [12800/17352 (74%)] Loss: -194986.500000\n",
      "Train Epoch: 105 [14208/17352 (82%)] Loss: -190504.093750\n",
      "Train Epoch: 105 [15460/17352 (89%)] Loss: -24968.355469\n",
      "Train Epoch: 105 [16340/17352 (94%)] Loss: -71058.898438\n",
      "Train Epoch: 105 [16879/17352 (97%)] Loss: -166611.281250\n",
      "    epoch          : 105\n",
      "    loss           : -185129.53643757864\n",
      "    val_loss       : -100900.05183957418\n",
      "Train Epoch: 106 [128/17352 (1%)] Loss: -213412.906250\n",
      "Train Epoch: 106 [1536/17352 (9%)] Loss: -195208.796875\n",
      "Train Epoch: 106 [2944/17352 (17%)] Loss: -185268.218750\n",
      "Train Epoch: 106 [4352/17352 (25%)] Loss: -201997.234375\n",
      "Train Epoch: 106 [5760/17352 (33%)] Loss: -185971.781250\n",
      "Train Epoch: 106 [7168/17352 (41%)] Loss: -194437.875000\n",
      "Train Epoch: 106 [8576/17352 (49%)] Loss: -216801.984375\n",
      "Train Epoch: 106 [9984/17352 (58%)] Loss: -193720.203125\n",
      "Train Epoch: 106 [11392/17352 (66%)] Loss: -213078.875000\n",
      "Train Epoch: 106 [12800/17352 (74%)] Loss: -209947.484375\n",
      "Train Epoch: 106 [14208/17352 (82%)] Loss: -195249.531250\n",
      "Train Epoch: 106 [15522/17352 (89%)] Loss: -140247.312500\n",
      "Train Epoch: 106 [16272/17352 (94%)] Loss: -150285.625000\n",
      "Train Epoch: 106 [16975/17352 (98%)] Loss: -81593.468750\n",
      "    epoch          : 106\n",
      "    loss           : -185314.57207227874\n",
      "    val_loss       : -100868.91289634704\n",
      "Train Epoch: 107 [128/17352 (1%)] Loss: -178410.984375\n",
      "Train Epoch: 107 [1536/17352 (9%)] Loss: -213219.750000\n",
      "Train Epoch: 107 [2944/17352 (17%)] Loss: -182284.656250\n",
      "Train Epoch: 107 [4352/17352 (25%)] Loss: -226470.968750\n",
      "Train Epoch: 107 [5760/17352 (33%)] Loss: -232770.406250\n",
      "Train Epoch: 107 [7168/17352 (41%)] Loss: -214262.078125\n",
      "Train Epoch: 107 [8576/17352 (49%)] Loss: -194830.937500\n",
      "Train Epoch: 107 [9984/17352 (58%)] Loss: -214142.968750\n",
      "Train Epoch: 107 [11392/17352 (66%)] Loss: -193310.625000\n",
      "Train Epoch: 107 [12800/17352 (74%)] Loss: -194108.468750\n",
      "Train Epoch: 107 [14208/17352 (82%)] Loss: -193234.265625\n",
      "Train Epoch: 107 [15448/17352 (89%)] Loss: -76595.562500\n",
      "Train Epoch: 107 [16272/17352 (94%)] Loss: -163996.468750\n",
      "Train Epoch: 107 [17092/17352 (99%)] Loss: -173959.546875\n",
      "    epoch          : 107\n",
      "    loss           : -185295.59714502937\n",
      "    val_loss       : -100886.39561901093\n",
      "Train Epoch: 108 [128/17352 (1%)] Loss: -178239.421875\n",
      "Train Epoch: 108 [1536/17352 (9%)] Loss: -196603.359375\n",
      "Train Epoch: 108 [2944/17352 (17%)] Loss: -173103.453125\n",
      "Train Epoch: 108 [4352/17352 (25%)] Loss: -217995.656250\n",
      "Train Epoch: 108 [5760/17352 (33%)] Loss: -195168.859375\n",
      "Train Epoch: 108 [7168/17352 (41%)] Loss: -172717.812500\n",
      "Train Epoch: 108 [8576/17352 (49%)] Loss: -226291.171875\n",
      "Train Epoch: 108 [9984/17352 (58%)] Loss: -201840.531250\n",
      "Train Epoch: 108 [11392/17352 (66%)] Loss: -199264.218750\n",
      "Train Epoch: 108 [12800/17352 (74%)] Loss: -209328.281250\n",
      "Train Epoch: 108 [14208/17352 (82%)] Loss: -210508.093750\n",
      "Train Epoch: 108 [15502/17352 (89%)] Loss: -131767.968750\n",
      "Train Epoch: 108 [16265/17352 (94%)] Loss: -112583.265625\n",
      "Train Epoch: 108 [16999/17352 (98%)] Loss: -145066.500000\n",
      "    epoch          : 108\n",
      "    loss           : -185377.10549496644\n",
      "    val_loss       : -100723.96944821675\n",
      "Train Epoch: 109 [128/17352 (1%)] Loss: -175493.000000\n",
      "Train Epoch: 109 [1536/17352 (9%)] Loss: -210534.625000\n",
      "Train Epoch: 109 [2944/17352 (17%)] Loss: -199751.296875\n",
      "Train Epoch: 109 [4352/17352 (25%)] Loss: -194332.281250\n",
      "Train Epoch: 109 [5760/17352 (33%)] Loss: -194490.375000\n",
      "Train Epoch: 109 [7168/17352 (41%)] Loss: -192845.015625\n",
      "Train Epoch: 109 [8576/17352 (49%)] Loss: -192799.343750\n",
      "Train Epoch: 109 [9984/17352 (58%)] Loss: -174975.750000\n",
      "Train Epoch: 109 [11392/17352 (66%)] Loss: -213032.781250\n",
      "Train Epoch: 109 [12800/17352 (74%)] Loss: -205652.062500\n",
      "Train Epoch: 109 [14208/17352 (82%)] Loss: -194307.265625\n",
      "Train Epoch: 109 [15486/17352 (89%)] Loss: -162157.890625\n",
      "Train Epoch: 109 [16219/17352 (93%)] Loss: -23061.429688\n",
      "Train Epoch: 109 [17011/17352 (98%)] Loss: -177610.515625\n",
      "    epoch          : 109\n",
      "    loss           : -184968.74777815645\n",
      "    val_loss       : -100737.89118137359\n",
      "Train Epoch: 110 [128/17352 (1%)] Loss: -212002.343750\n",
      "Train Epoch: 110 [1536/17352 (9%)] Loss: -213375.406250\n",
      "Train Epoch: 110 [2944/17352 (17%)] Loss: -245132.250000\n",
      "Train Epoch: 110 [4352/17352 (25%)] Loss: -196191.500000\n",
      "Train Epoch: 110 [5760/17352 (33%)] Loss: -184790.343750\n",
      "Train Epoch: 110 [7168/17352 (41%)] Loss: -193895.750000\n",
      "Train Epoch: 110 [8576/17352 (49%)] Loss: -206038.171875\n",
      "Train Epoch: 110 [9984/17352 (58%)] Loss: -207597.890625\n",
      "Train Epoch: 110 [11392/17352 (66%)] Loss: -182276.000000\n",
      "Train Epoch: 110 [12800/17352 (74%)] Loss: -207292.187500\n",
      "Train Epoch: 110 [14208/17352 (82%)] Loss: -225757.125000\n",
      "Train Epoch: 110 [15553/17352 (90%)] Loss: -175446.343750\n",
      "Train Epoch: 110 [16220/17352 (93%)] Loss: -131579.093750\n",
      "Train Epoch: 110 [16869/17352 (97%)] Loss: -122542.203125\n",
      "    epoch          : 110\n",
      "    loss           : -185237.57391726092\n",
      "    val_loss       : -100859.06996040345\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0806_145705/checkpoint-epoch110.pth ...\n",
      "Train Epoch: 111 [128/17352 (1%)] Loss: -213711.687500\n",
      "Train Epoch: 111 [1536/17352 (9%)] Loss: -194703.578125\n",
      "Train Epoch: 111 [2944/17352 (17%)] Loss: -227903.562500\n",
      "Train Epoch: 111 [4352/17352 (25%)] Loss: -195660.968750\n",
      "Train Epoch: 111 [5760/17352 (33%)] Loss: -195095.953125\n",
      "Train Epoch: 111 [7168/17352 (41%)] Loss: -196228.093750\n",
      "Train Epoch: 111 [8576/17352 (49%)] Loss: -211756.437500\n",
      "Train Epoch: 111 [9984/17352 (58%)] Loss: -190257.312500\n",
      "Train Epoch: 111 [11392/17352 (66%)] Loss: -211346.093750\n",
      "Train Epoch: 111 [12800/17352 (74%)] Loss: -196708.937500\n",
      "Train Epoch: 111 [14208/17352 (82%)] Loss: -211407.687500\n",
      "Train Epoch: 111 [15448/17352 (89%)] Loss: -82717.226562\n",
      "Train Epoch: 111 [16328/17352 (94%)] Loss: -161976.718750\n",
      "Train Epoch: 111 [17039/17352 (98%)] Loss: -59130.429688\n",
      "    epoch          : 111\n",
      "    loss           : -185439.75091429843\n",
      "    val_loss       : -100890.17730369567\n",
      "Train Epoch: 112 [128/17352 (1%)] Loss: -213073.718750\n",
      "Train Epoch: 112 [1536/17352 (9%)] Loss: -190154.500000\n",
      "Train Epoch: 112 [2944/17352 (17%)] Loss: -214838.656250\n",
      "Train Epoch: 112 [4352/17352 (25%)] Loss: -217671.750000\n",
      "Train Epoch: 112 [5760/17352 (33%)] Loss: -196838.000000\n",
      "Train Epoch: 112 [7168/17352 (41%)] Loss: -211197.500000\n",
      "Train Epoch: 112 [8576/17352 (49%)] Loss: -210403.359375\n",
      "Train Epoch: 112 [9984/17352 (58%)] Loss: -216812.796875\n",
      "Train Epoch: 112 [11392/17352 (66%)] Loss: -212699.093750\n",
      "Train Epoch: 112 [12800/17352 (74%)] Loss: -206923.203125\n",
      "Train Epoch: 112 [14208/17352 (82%)] Loss: -215342.750000\n",
      "Train Epoch: 112 [15416/17352 (89%)] Loss: -24570.748047\n",
      "Train Epoch: 112 [16248/17352 (94%)] Loss: -127207.343750\n",
      "Train Epoch: 112 [17058/17352 (98%)] Loss: -71133.507812\n",
      "    epoch          : 112\n",
      "    loss           : -185467.3754194631\n",
      "    val_loss       : -100722.06905110677\n",
      "Train Epoch: 113 [128/17352 (1%)] Loss: -209511.296875\n",
      "Train Epoch: 113 [1536/17352 (9%)] Loss: -201189.000000\n",
      "Train Epoch: 113 [2944/17352 (17%)] Loss: -198728.843750\n",
      "Train Epoch: 113 [4352/17352 (25%)] Loss: -197180.687500\n",
      "Train Epoch: 113 [5760/17352 (33%)] Loss: -197441.203125\n",
      "Train Epoch: 113 [7168/17352 (41%)] Loss: -204885.500000\n",
      "Train Epoch: 113 [8576/17352 (49%)] Loss: -192906.828125\n",
      "Train Epoch: 113 [9984/17352 (58%)] Loss: -233247.046875\n",
      "Train Epoch: 113 [11392/17352 (66%)] Loss: -184538.625000\n",
      "Train Epoch: 113 [12800/17352 (74%)] Loss: -196123.718750\n",
      "Train Epoch: 113 [14208/17352 (82%)] Loss: -191571.500000\n",
      "Train Epoch: 113 [15499/17352 (89%)] Loss: -70900.078125\n",
      "Train Epoch: 113 [16274/17352 (94%)] Loss: -127264.140625\n",
      "Train Epoch: 113 [17047/17352 (98%)] Loss: -178998.656250\n",
      "    epoch          : 113\n",
      "    loss           : -185491.75179582633\n",
      "    val_loss       : -100823.87064806621\n",
      "Train Epoch: 114 [128/17352 (1%)] Loss: -195856.875000\n",
      "Train Epoch: 114 [1536/17352 (9%)] Loss: -202349.093750\n",
      "Train Epoch: 114 [2944/17352 (17%)] Loss: -192205.500000\n",
      "Train Epoch: 114 [4352/17352 (25%)] Loss: -197646.093750\n",
      "Train Epoch: 114 [5760/17352 (33%)] Loss: -195735.640625\n",
      "Train Epoch: 114 [7168/17352 (41%)] Loss: -193571.437500\n",
      "Train Epoch: 114 [8576/17352 (49%)] Loss: -194894.265625\n",
      "Train Epoch: 114 [9984/17352 (58%)] Loss: -173918.343750\n",
      "Train Epoch: 114 [11392/17352 (66%)] Loss: -204194.109375\n",
      "Train Epoch: 114 [12800/17352 (74%)] Loss: -207301.109375\n",
      "Train Epoch: 114 [14208/17352 (82%)] Loss: -191774.281250\n",
      "Train Epoch: 114 [15442/17352 (89%)] Loss: -8548.130859\n",
      "Train Epoch: 114 [16218/17352 (93%)] Loss: -179211.937500\n",
      "Train Epoch: 114 [16973/17352 (98%)] Loss: -174506.906250\n",
      "    epoch          : 114\n",
      "    loss           : -185453.58307990772\n",
      "    val_loss       : -100868.31087652843\n",
      "Train Epoch: 115 [128/17352 (1%)] Loss: -209351.671875\n",
      "Train Epoch: 115 [1536/17352 (9%)] Loss: -191308.281250\n",
      "Train Epoch: 115 [2944/17352 (17%)] Loss: -177651.937500\n",
      "Train Epoch: 115 [4352/17352 (25%)] Loss: -218256.578125\n",
      "Train Epoch: 115 [5760/17352 (33%)] Loss: -226402.781250\n",
      "Train Epoch: 115 [7168/17352 (41%)] Loss: -245789.203125\n",
      "Train Epoch: 115 [8576/17352 (49%)] Loss: -192631.593750\n",
      "Train Epoch: 115 [9984/17352 (58%)] Loss: -175745.968750\n",
      "Train Epoch: 115 [11392/17352 (66%)] Loss: -212025.078125\n",
      "Train Epoch: 115 [12800/17352 (74%)] Loss: -208651.531250\n",
      "Train Epoch: 115 [14208/17352 (82%)] Loss: -193984.406250\n",
      "Train Epoch: 115 [15499/17352 (89%)] Loss: -162845.890625\n",
      "Train Epoch: 115 [16399/17352 (95%)] Loss: -136898.687500\n",
      "Train Epoch: 115 [17066/17352 (98%)] Loss: -82493.570312\n",
      "    epoch          : 115\n",
      "    loss           : -185458.25711121014\n",
      "    val_loss       : -100970.51165911356\n",
      "Train Epoch: 116 [128/17352 (1%)] Loss: -174079.468750\n",
      "Train Epoch: 116 [1536/17352 (9%)] Loss: -233314.906250\n",
      "Train Epoch: 116 [2944/17352 (17%)] Loss: -197273.062500\n",
      "Train Epoch: 116 [4352/17352 (25%)] Loss: -233042.718750\n",
      "Train Epoch: 116 [5760/17352 (33%)] Loss: -206664.140625\n",
      "Train Epoch: 116 [7168/17352 (41%)] Loss: -208973.406250\n",
      "Train Epoch: 116 [8576/17352 (49%)] Loss: -210265.625000\n",
      "Train Epoch: 116 [9984/17352 (58%)] Loss: -208900.468750\n",
      "Train Epoch: 116 [11392/17352 (66%)] Loss: -214767.343750\n",
      "Train Epoch: 116 [12800/17352 (74%)] Loss: -206839.781250\n",
      "Train Epoch: 116 [14208/17352 (82%)] Loss: -210604.250000\n",
      "Train Epoch: 116 [15436/17352 (89%)] Loss: -5290.496094\n",
      "Train Epoch: 116 [16208/17352 (93%)] Loss: -113573.484375\n",
      "Train Epoch: 116 [16943/17352 (98%)] Loss: -163508.421875\n",
      "    epoch          : 116\n",
      "    loss           : -185751.7260774958\n",
      "    val_loss       : -100821.97261943817\n",
      "Train Epoch: 117 [128/17352 (1%)] Loss: -196941.578125\n",
      "Train Epoch: 117 [1536/17352 (9%)] Loss: -194807.125000\n",
      "Train Epoch: 117 [2944/17352 (17%)] Loss: -215860.875000\n",
      "Train Epoch: 117 [4352/17352 (25%)] Loss: -215830.671875\n",
      "Train Epoch: 117 [5760/17352 (33%)] Loss: -227683.156250\n",
      "Train Epoch: 117 [7168/17352 (41%)] Loss: -195885.796875\n",
      "Train Epoch: 117 [8576/17352 (49%)] Loss: -191760.468750\n",
      "Train Epoch: 117 [9984/17352 (58%)] Loss: -232988.140625\n",
      "Train Epoch: 117 [11392/17352 (66%)] Loss: -196684.796875\n",
      "Train Epoch: 117 [12800/17352 (74%)] Loss: -193684.609375\n",
      "Train Epoch: 117 [14208/17352 (82%)] Loss: -210654.328125\n",
      "Train Epoch: 117 [15514/17352 (89%)] Loss: -126783.296875\n",
      "Train Epoch: 117 [16514/17352 (95%)] Loss: -136832.328125\n",
      "Train Epoch: 117 [17057/17352 (98%)] Loss: -4857.827148\n",
      "    epoch          : 117\n",
      "    loss           : -185572.93246644296\n",
      "    val_loss       : -100817.17625045776\n",
      "Train Epoch: 118 [128/17352 (1%)] Loss: -213668.546875\n",
      "Train Epoch: 118 [1536/17352 (9%)] Loss: -201018.765625\n",
      "Train Epoch: 118 [2944/17352 (17%)] Loss: -177644.562500\n",
      "Train Epoch: 118 [4352/17352 (25%)] Loss: -192191.937500\n",
      "Train Epoch: 118 [5760/17352 (33%)] Loss: -196262.046875\n",
      "Train Epoch: 118 [7168/17352 (41%)] Loss: -198380.359375\n",
      "Train Epoch: 118 [8576/17352 (49%)] Loss: -193451.625000\n",
      "Train Epoch: 118 [9984/17352 (58%)] Loss: -210261.000000\n",
      "Train Epoch: 118 [11392/17352 (66%)] Loss: -201325.312500\n",
      "Train Epoch: 118 [12800/17352 (74%)] Loss: -194598.546875\n",
      "Train Epoch: 118 [14208/17352 (82%)] Loss: -192112.609375\n",
      "Train Epoch: 118 [15476/17352 (89%)] Loss: -25535.146484\n",
      "Train Epoch: 118 [16199/17352 (93%)] Loss: -8444.374023\n",
      "Train Epoch: 118 [17035/17352 (98%)] Loss: -133175.203125\n",
      "    epoch          : 118\n",
      "    loss           : -185664.41339070364\n",
      "    val_loss       : -100957.91676578522\n",
      "Train Epoch: 119 [128/17352 (1%)] Loss: -174978.671875\n",
      "Train Epoch: 119 [1536/17352 (9%)] Loss: -215157.390625\n",
      "Train Epoch: 119 [2944/17352 (17%)] Loss: -198707.937500\n",
      "Train Epoch: 119 [4352/17352 (25%)] Loss: -200330.453125\n",
      "Train Epoch: 119 [5760/17352 (33%)] Loss: -196748.484375\n",
      "Train Epoch: 119 [7168/17352 (41%)] Loss: -216731.250000\n",
      "Train Epoch: 119 [8576/17352 (49%)] Loss: -209510.218750\n",
      "Train Epoch: 119 [9984/17352 (58%)] Loss: -208965.171875\n",
      "Train Epoch: 119 [11392/17352 (66%)] Loss: -195935.250000\n",
      "Train Epoch: 119 [12800/17352 (74%)] Loss: -208578.796875\n",
      "Train Epoch: 119 [14208/17352 (82%)] Loss: -213074.640625\n",
      "Train Epoch: 119 [15440/17352 (89%)] Loss: -5026.993164\n",
      "Train Epoch: 119 [16247/17352 (94%)] Loss: -206047.500000\n",
      "Train Epoch: 119 [17005/17352 (98%)] Loss: -8680.282227\n",
      "    epoch          : 119\n",
      "    loss           : -185821.98585622903\n",
      "    val_loss       : -100893.29644819895\n",
      "Train Epoch: 120 [128/17352 (1%)] Loss: -210564.656250\n",
      "Train Epoch: 120 [1536/17352 (9%)] Loss: -208803.921875\n",
      "Train Epoch: 120 [2944/17352 (17%)] Loss: -202990.781250\n",
      "Train Epoch: 120 [4352/17352 (25%)] Loss: -195607.968750\n",
      "Train Epoch: 120 [5760/17352 (33%)] Loss: -206150.125000\n",
      "Train Epoch: 120 [7168/17352 (41%)] Loss: -209673.343750\n",
      "Train Epoch: 120 [8576/17352 (49%)] Loss: -191616.171875\n",
      "Train Epoch: 120 [9984/17352 (58%)] Loss: -177749.062500\n",
      "Train Epoch: 120 [11392/17352 (66%)] Loss: -215857.750000\n",
      "Train Epoch: 120 [12800/17352 (74%)] Loss: -206078.859375\n",
      "Train Epoch: 120 [14208/17352 (82%)] Loss: -209554.000000\n",
      "Train Epoch: 120 [15499/17352 (89%)] Loss: -130649.875000\n",
      "Train Epoch: 120 [16272/17352 (94%)] Loss: -165715.187500\n",
      "Train Epoch: 120 [16998/17352 (98%)] Loss: -59070.058594\n",
      "    epoch          : 120\n",
      "    loss           : -185728.81833315856\n",
      "    val_loss       : -100815.31382528941\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0806_145705/checkpoint-epoch120.pth ...\n",
      "Train Epoch: 121 [128/17352 (1%)] Loss: -210739.890625\n",
      "Train Epoch: 121 [1536/17352 (9%)] Loss: -210454.656250\n",
      "Train Epoch: 121 [2944/17352 (17%)] Loss: -194216.390625\n",
      "Train Epoch: 121 [4352/17352 (25%)] Loss: -216531.328125\n",
      "Train Epoch: 121 [5760/17352 (33%)] Loss: -194649.984375\n",
      "Train Epoch: 121 [7168/17352 (41%)] Loss: -217692.328125\n",
      "Train Epoch: 121 [8576/17352 (49%)] Loss: -194782.093750\n",
      "Train Epoch: 121 [9984/17352 (58%)] Loss: -196188.937500\n",
      "Train Epoch: 121 [11392/17352 (66%)] Loss: -184901.406250\n",
      "Train Epoch: 121 [12800/17352 (74%)] Loss: -205766.203125\n",
      "Train Epoch: 121 [14208/17352 (82%)] Loss: -228466.484375\n",
      "Train Epoch: 121 [15570/17352 (90%)] Loss: -167944.187500\n",
      "Train Epoch: 121 [16314/17352 (94%)] Loss: -72130.859375\n",
      "Train Epoch: 121 [16995/17352 (98%)] Loss: -146779.000000\n",
      "    epoch          : 121\n",
      "    loss           : -185680.34177721266\n",
      "    val_loss       : -100868.90643113454\n",
      "Train Epoch: 122 [128/17352 (1%)] Loss: -212359.187500\n",
      "Train Epoch: 122 [1536/17352 (9%)] Loss: -200754.171875\n",
      "Train Epoch: 122 [2944/17352 (17%)] Loss: -196179.968750\n",
      "Train Epoch: 122 [4352/17352 (25%)] Loss: -195819.000000\n",
      "Train Epoch: 122 [5760/17352 (33%)] Loss: -213412.562500\n",
      "Train Epoch: 122 [7168/17352 (41%)] Loss: -217780.328125\n",
      "Train Epoch: 122 [8576/17352 (49%)] Loss: -193669.625000\n",
      "Train Epoch: 122 [9984/17352 (58%)] Loss: -215995.531250\n",
      "Train Epoch: 122 [11392/17352 (66%)] Loss: -205924.687500\n",
      "Train Epoch: 122 [12800/17352 (74%)] Loss: -208043.812500\n",
      "Train Epoch: 122 [14208/17352 (82%)] Loss: -194563.468750\n",
      "Train Epoch: 122 [15435/17352 (89%)] Loss: -60579.507812\n",
      "Train Epoch: 122 [16237/17352 (94%)] Loss: -132492.812500\n",
      "Train Epoch: 122 [16946/17352 (98%)] Loss: -148039.421875\n",
      "    epoch          : 122\n",
      "    loss           : -185808.222951185\n",
      "    val_loss       : -100965.79561386109\n",
      "Train Epoch: 123 [128/17352 (1%)] Loss: -211745.593750\n",
      "Train Epoch: 123 [1536/17352 (9%)] Loss: -196548.937500\n",
      "Train Epoch: 123 [2944/17352 (17%)] Loss: -220243.203125\n",
      "Train Epoch: 123 [4352/17352 (25%)] Loss: -206178.359375\n",
      "Train Epoch: 123 [5760/17352 (33%)] Loss: -207923.968750\n",
      "Train Epoch: 123 [7168/17352 (41%)] Loss: -210392.218750\n",
      "Train Epoch: 123 [8576/17352 (49%)] Loss: -193382.390625\n",
      "Train Epoch: 123 [9984/17352 (58%)] Loss: -207560.625000\n",
      "Train Epoch: 123 [11392/17352 (66%)] Loss: -180995.750000\n",
      "Train Epoch: 123 [12800/17352 (74%)] Loss: -203489.375000\n",
      "Train Epoch: 123 [14208/17352 (82%)] Loss: -227704.921875\n",
      "Train Epoch: 123 [15397/17352 (89%)] Loss: -5147.275391\n",
      "Train Epoch: 123 [16412/17352 (95%)] Loss: -114164.468750\n",
      "Train Epoch: 123 [17023/17352 (98%)] Loss: -57341.656250\n",
      "    epoch          : 123\n",
      "    loss           : -185615.4106478083\n",
      "    val_loss       : -100915.67874711355\n",
      "Train Epoch: 124 [128/17352 (1%)] Loss: -210795.500000\n",
      "Train Epoch: 124 [1536/17352 (9%)] Loss: -196434.343750\n",
      "Train Epoch: 124 [2944/17352 (17%)] Loss: -226663.062500\n",
      "Train Epoch: 124 [4352/17352 (25%)] Loss: -195523.531250\n",
      "Train Epoch: 124 [5760/17352 (33%)] Loss: -194754.343750\n",
      "Train Epoch: 124 [7168/17352 (41%)] Loss: -195023.875000\n",
      "Train Epoch: 124 [8576/17352 (49%)] Loss: -196379.984375\n",
      "Train Epoch: 124 [9984/17352 (58%)] Loss: -193991.578125\n",
      "Train Epoch: 124 [11392/17352 (66%)] Loss: -175673.859375\n",
      "Train Epoch: 124 [12800/17352 (74%)] Loss: -209391.796875\n",
      "Train Epoch: 124 [14208/17352 (82%)] Loss: -227802.000000\n",
      "Train Epoch: 124 [15556/17352 (90%)] Loss: -167733.781250\n",
      "Train Epoch: 124 [16316/17352 (94%)] Loss: -126541.625000\n",
      "Train Epoch: 124 [17093/17352 (99%)] Loss: -117574.250000\n",
      "    epoch          : 124\n",
      "    loss           : -185857.02838913066\n",
      "    val_loss       : -100887.55881551107\n",
      "Train Epoch: 125 [128/17352 (1%)] Loss: -212284.562500\n",
      "Train Epoch: 125 [1536/17352 (9%)] Loss: -205738.984375\n",
      "Train Epoch: 125 [2944/17352 (17%)] Loss: -217895.906250\n",
      "Train Epoch: 125 [4352/17352 (25%)] Loss: -208139.203125\n",
      "Train Epoch: 125 [5760/17352 (33%)] Loss: -196947.109375\n",
      "Train Epoch: 125 [7168/17352 (41%)] Loss: -245349.046875\n",
      "Train Epoch: 125 [8576/17352 (49%)] Loss: -193721.062500\n",
      "Train Epoch: 125 [9984/17352 (58%)] Loss: -195884.203125\n",
      "Train Epoch: 125 [11392/17352 (66%)] Loss: -212223.734375\n",
      "Train Epoch: 125 [12800/17352 (74%)] Loss: -197338.968750\n",
      "Train Epoch: 125 [14208/17352 (82%)] Loss: -207713.359375\n",
      "Train Epoch: 125 [15485/17352 (89%)] Loss: -144222.187500\n",
      "Train Epoch: 125 [16262/17352 (94%)] Loss: -161808.062500\n",
      "Train Epoch: 125 [16942/17352 (98%)] Loss: -132377.562500\n",
      "    epoch          : 125\n",
      "    loss           : -185934.09472983956\n",
      "    val_loss       : -100973.97652581533\n",
      "Train Epoch: 126 [128/17352 (1%)] Loss: -211319.000000\n",
      "Train Epoch: 126 [1536/17352 (9%)] Loss: -214154.390625\n",
      "Train Epoch: 126 [2944/17352 (17%)] Loss: -194430.625000\n",
      "Train Epoch: 126 [4352/17352 (25%)] Loss: -193900.796875\n",
      "Train Epoch: 126 [5760/17352 (33%)] Loss: -209526.984375\n",
      "Train Epoch: 126 [7168/17352 (41%)] Loss: -246345.484375\n",
      "Train Epoch: 126 [8576/17352 (49%)] Loss: -213019.703125\n",
      "Train Epoch: 126 [9984/17352 (58%)] Loss: -217146.312500\n",
      "Train Epoch: 126 [11392/17352 (66%)] Loss: -212456.500000\n",
      "Train Epoch: 126 [12800/17352 (74%)] Loss: -211562.703125\n",
      "Train Epoch: 126 [14208/17352 (82%)] Loss: -193089.640625\n",
      "Train Epoch: 126 [15453/17352 (89%)] Loss: -79419.937500\n",
      "Train Epoch: 126 [16172/17352 (93%)] Loss: -115767.453125\n",
      "Train Epoch: 126 [17018/17352 (98%)] Loss: -5174.466309\n",
      "    epoch          : 126\n",
      "    loss           : -185980.326824009\n",
      "    val_loss       : -100894.2658212026\n",
      "Train Epoch: 127 [128/17352 (1%)] Loss: -173671.687500\n",
      "Train Epoch: 127 [1536/17352 (9%)] Loss: -192351.125000\n",
      "Train Epoch: 127 [2944/17352 (17%)] Loss: -184799.312500\n",
      "Train Epoch: 127 [4352/17352 (25%)] Loss: -193504.781250\n",
      "Train Epoch: 127 [5760/17352 (33%)] Loss: -194522.796875\n",
      "Train Epoch: 127 [7168/17352 (41%)] Loss: -210140.343750\n",
      "Train Epoch: 127 [8576/17352 (49%)] Loss: -194583.062500\n",
      "Train Epoch: 127 [9984/17352 (58%)] Loss: -177844.031250\n",
      "Train Epoch: 127 [11392/17352 (66%)] Loss: -210222.078125\n",
      "Train Epoch: 127 [12800/17352 (74%)] Loss: -227305.078125\n",
      "Train Epoch: 127 [14208/17352 (82%)] Loss: -196974.250000\n",
      "Train Epoch: 127 [15521/17352 (89%)] Loss: -131396.593750\n",
      "Train Epoch: 127 [16149/17352 (93%)] Loss: -8356.464844\n",
      "Train Epoch: 127 [16959/17352 (98%)] Loss: -71092.992188\n",
      "    epoch          : 127\n",
      "    loss           : -186094.8422163381\n",
      "    val_loss       : -100993.27234465281\n",
      "Train Epoch: 128 [128/17352 (1%)] Loss: -209773.296875\n",
      "Train Epoch: 128 [1536/17352 (9%)] Loss: -193523.968750\n",
      "Train Epoch: 128 [2944/17352 (17%)] Loss: -197102.437500\n",
      "Train Epoch: 128 [4352/17352 (25%)] Loss: -194687.968750\n",
      "Train Epoch: 128 [5760/17352 (33%)] Loss: -208324.671875\n",
      "Train Epoch: 128 [7168/17352 (41%)] Loss: -200627.437500\n",
      "Train Epoch: 128 [8576/17352 (49%)] Loss: -225139.156250\n",
      "Train Epoch: 128 [9984/17352 (58%)] Loss: -192620.968750\n",
      "Train Epoch: 128 [11392/17352 (66%)] Loss: -199468.031250\n",
      "Train Epoch: 128 [12800/17352 (74%)] Loss: -193674.968750\n",
      "Train Epoch: 128 [14208/17352 (82%)] Loss: -193056.968750\n",
      "Train Epoch: 128 [15562/17352 (90%)] Loss: -207193.234375\n",
      "Train Epoch: 128 [16214/17352 (93%)] Loss: -122367.859375\n",
      "Train Epoch: 128 [16994/17352 (98%)] Loss: -133379.640625\n",
      "    epoch          : 128\n",
      "    loss           : -185887.17362494758\n",
      "    val_loss       : -100885.6369895935\n",
      "Train Epoch: 129 [128/17352 (1%)] Loss: -213743.984375\n",
      "Train Epoch: 129 [1536/17352 (9%)] Loss: -217586.578125\n",
      "Train Epoch: 129 [2944/17352 (17%)] Loss: -215495.390625\n",
      "Train Epoch: 129 [4352/17352 (25%)] Loss: -210474.031250\n",
      "Train Epoch: 129 [5760/17352 (33%)] Loss: -232694.859375\n",
      "Train Epoch: 129 [7168/17352 (41%)] Loss: -205516.687500\n",
      "Train Epoch: 129 [8576/17352 (49%)] Loss: -191334.984375\n",
      "Train Epoch: 129 [9984/17352 (58%)] Loss: -215954.296875\n",
      "Train Epoch: 129 [11392/17352 (66%)] Loss: -194924.171875\n",
      "Train Epoch: 129 [12800/17352 (74%)] Loss: -195242.437500\n",
      "Train Epoch: 129 [14208/17352 (82%)] Loss: -194548.125000\n",
      "Train Epoch: 129 [15584/17352 (90%)] Loss: -178972.656250\n",
      "Train Epoch: 129 [16408/17352 (95%)] Loss: -125267.007812\n",
      "Train Epoch: 129 [17104/17352 (99%)] Loss: -150000.921875\n",
      "    epoch          : 129\n",
      "    loss           : -185933.93012662543\n",
      "    val_loss       : -100888.97366371154\n",
      "Train Epoch: 130 [128/17352 (1%)] Loss: -176456.687500\n",
      "Train Epoch: 130 [1536/17352 (9%)] Loss: -213021.937500\n",
      "Train Epoch: 130 [2944/17352 (17%)] Loss: -213742.046875\n",
      "Train Epoch: 130 [4352/17352 (25%)] Loss: -217896.406250\n",
      "Train Epoch: 130 [5760/17352 (33%)] Loss: -209879.687500\n",
      "Train Epoch: 130 [7168/17352 (41%)] Loss: -173921.843750\n",
      "Train Epoch: 130 [8576/17352 (49%)] Loss: -197354.125000\n",
      "Train Epoch: 130 [9984/17352 (58%)] Loss: -209543.562500\n",
      "Train Epoch: 130 [11392/17352 (66%)] Loss: -210392.796875\n",
      "Train Epoch: 130 [12800/17352 (74%)] Loss: -207911.453125\n",
      "Train Epoch: 130 [14208/17352 (82%)] Loss: -196804.656250\n",
      "Train Epoch: 130 [15486/17352 (89%)] Loss: -57620.488281\n",
      "Train Epoch: 130 [16333/17352 (94%)] Loss: -137547.406250\n",
      "Train Epoch: 130 [16940/17352 (98%)] Loss: -132771.968750\n",
      "    epoch          : 130\n",
      "    loss           : -185919.69001677854\n",
      "    val_loss       : -100881.05615666708\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0806_145705/checkpoint-epoch130.pth ...\n",
      "Train Epoch: 131 [128/17352 (1%)] Loss: -215411.171875\n",
      "Train Epoch: 131 [1536/17352 (9%)] Loss: -195386.828125\n",
      "Train Epoch: 131 [2944/17352 (17%)] Loss: -178567.218750\n",
      "Train Epoch: 131 [4352/17352 (25%)] Loss: -215053.593750\n",
      "Train Epoch: 131 [5760/17352 (33%)] Loss: -199593.250000\n",
      "Train Epoch: 131 [7168/17352 (41%)] Loss: -193617.156250\n",
      "Train Epoch: 131 [8576/17352 (49%)] Loss: -195627.437500\n",
      "Train Epoch: 131 [9984/17352 (58%)] Loss: -204008.046875\n",
      "Train Epoch: 131 [11392/17352 (66%)] Loss: -195773.656250\n",
      "Train Epoch: 131 [12800/17352 (74%)] Loss: -195432.046875\n",
      "Train Epoch: 131 [14208/17352 (82%)] Loss: -192816.718750\n",
      "Train Epoch: 131 [15535/17352 (90%)] Loss: -130623.898438\n",
      "Train Epoch: 131 [16308/17352 (94%)] Loss: -134582.437500\n",
      "Train Epoch: 131 [17019/17352 (98%)] Loss: -70353.710938\n",
      "    epoch          : 131\n",
      "    loss           : -186036.0228804006\n",
      "    val_loss       : -100771.8547609965\n",
      "Train Epoch: 132 [128/17352 (1%)] Loss: -211901.906250\n",
      "Train Epoch: 132 [1536/17352 (9%)] Loss: -196498.406250\n",
      "Train Epoch: 132 [2944/17352 (17%)] Loss: -175037.843750\n",
      "Train Epoch: 132 [4352/17352 (25%)] Loss: -216958.343750\n",
      "Train Epoch: 132 [5760/17352 (33%)] Loss: -193602.515625\n",
      "Train Epoch: 132 [7168/17352 (41%)] Loss: -246340.781250\n",
      "Train Epoch: 132 [8576/17352 (49%)] Loss: -183587.187500\n",
      "Train Epoch: 132 [9984/17352 (58%)] Loss: -194476.187500\n",
      "Train Epoch: 132 [11392/17352 (66%)] Loss: -212801.265625\n",
      "Train Epoch: 132 [12800/17352 (74%)] Loss: -205659.343750\n",
      "Train Epoch: 132 [14208/17352 (82%)] Loss: -191351.296875\n",
      "Train Epoch: 132 [15499/17352 (89%)] Loss: -133807.640625\n",
      "Train Epoch: 132 [16308/17352 (94%)] Loss: -77029.148438\n",
      "Train Epoch: 132 [17025/17352 (98%)] Loss: -153290.890625\n",
      "    epoch          : 132\n",
      "    loss           : -186026.9986695155\n",
      "    val_loss       : -100885.53027610779\n",
      "Train Epoch: 133 [128/17352 (1%)] Loss: -211267.656250\n",
      "Train Epoch: 133 [1536/17352 (9%)] Loss: -218038.062500\n",
      "Train Epoch: 133 [2944/17352 (17%)] Loss: -196289.468750\n",
      "Train Epoch: 133 [4352/17352 (25%)] Loss: -231942.781250\n",
      "Train Epoch: 133 [5760/17352 (33%)] Loss: -198044.625000\n",
      "Train Epoch: 133 [7168/17352 (41%)] Loss: -207747.484375\n",
      "Train Epoch: 133 [8576/17352 (49%)] Loss: -194842.656250\n",
      "Train Epoch: 133 [9984/17352 (58%)] Loss: -206762.546875\n",
      "Train Epoch: 133 [11392/17352 (66%)] Loss: -183566.187500\n",
      "Train Epoch: 133 [12800/17352 (74%)] Loss: -194064.062500\n",
      "Train Epoch: 133 [14208/17352 (82%)] Loss: -215508.828125\n",
      "Train Epoch: 133 [15406/17352 (89%)] Loss: -70642.726562\n",
      "Train Epoch: 133 [16288/17352 (94%)] Loss: -145395.296875\n",
      "Train Epoch: 133 [17051/17352 (98%)] Loss: -205456.000000\n",
      "    epoch          : 133\n",
      "    loss           : -186036.09526727663\n",
      "    val_loss       : -100843.26345945994\n",
      "Train Epoch: 134 [128/17352 (1%)] Loss: -213735.953125\n",
      "Train Epoch: 134 [1536/17352 (9%)] Loss: -195892.531250\n",
      "Train Epoch: 134 [2944/17352 (17%)] Loss: -193215.796875\n",
      "Train Epoch: 134 [4352/17352 (25%)] Loss: -206726.546875\n",
      "Train Epoch: 134 [5760/17352 (33%)] Loss: -203741.390625\n",
      "Train Epoch: 134 [7168/17352 (41%)] Loss: -232908.859375\n",
      "Train Epoch: 134 [8576/17352 (49%)] Loss: -195809.546875\n",
      "Train Epoch: 134 [9984/17352 (58%)] Loss: -195145.671875\n",
      "Train Epoch: 134 [11392/17352 (66%)] Loss: -200158.640625\n",
      "Train Epoch: 134 [12800/17352 (74%)] Loss: -210115.968750\n",
      "Train Epoch: 134 [14208/17352 (82%)] Loss: -195576.312500\n",
      "Train Epoch: 134 [15502/17352 (89%)] Loss: -25717.167969\n",
      "Train Epoch: 134 [16152/17352 (93%)] Loss: -127376.742188\n",
      "Train Epoch: 134 [16972/17352 (98%)] Loss: -59546.699219\n",
      "    epoch          : 134\n",
      "    loss           : -186079.24580536914\n",
      "    val_loss       : -100928.87919038137\n",
      "Train Epoch: 135 [128/17352 (1%)] Loss: -195434.593750\n",
      "Train Epoch: 135 [1536/17352 (9%)] Loss: -208415.765625\n",
      "Train Epoch: 135 [2944/17352 (17%)] Loss: -196796.093750\n",
      "Train Epoch: 135 [4352/17352 (25%)] Loss: -208969.734375\n",
      "Train Epoch: 135 [5760/17352 (33%)] Loss: -195368.359375\n",
      "Train Epoch: 135 [7168/17352 (41%)] Loss: -215958.843750\n",
      "Train Epoch: 135 [8576/17352 (49%)] Loss: -212508.281250\n",
      "Train Epoch: 135 [9984/17352 (58%)] Loss: -193490.468750\n",
      "Train Epoch: 135 [11392/17352 (66%)] Loss: -181121.109375\n",
      "Train Epoch: 135 [12800/17352 (74%)] Loss: -219836.343750\n",
      "Train Epoch: 135 [14208/17352 (82%)] Loss: -215516.968750\n",
      "Train Epoch: 135 [15499/17352 (89%)] Loss: -148024.531250\n",
      "Train Epoch: 135 [16210/17352 (93%)] Loss: -78174.632812\n",
      "Train Epoch: 135 [17063/17352 (98%)] Loss: -137106.406250\n",
      "    epoch          : 135\n",
      "    loss           : -186150.80718133913\n",
      "    val_loss       : -100864.16890436808\n",
      "Train Epoch: 136 [128/17352 (1%)] Loss: -177795.203125\n",
      "Train Epoch: 136 [1536/17352 (9%)] Loss: -214622.937500\n",
      "Train Epoch: 136 [2944/17352 (17%)] Loss: -196925.437500\n",
      "Train Epoch: 136 [4352/17352 (25%)] Loss: -204831.125000\n",
      "Train Epoch: 136 [5760/17352 (33%)] Loss: -194299.500000\n",
      "Train Epoch: 136 [7168/17352 (41%)] Loss: -195351.906250\n",
      "Train Epoch: 136 [8576/17352 (49%)] Loss: -195884.453125\n",
      "Train Epoch: 136 [9984/17352 (58%)] Loss: -211240.046875\n",
      "Train Epoch: 136 [11392/17352 (66%)] Loss: -197534.687500\n",
      "Train Epoch: 136 [12800/17352 (74%)] Loss: -211429.906250\n",
      "Train Epoch: 136 [14208/17352 (82%)] Loss: -196929.265625\n",
      "Train Epoch: 136 [15500/17352 (89%)] Loss: -125619.921875\n",
      "Train Epoch: 136 [16174/17352 (93%)] Loss: -5385.048828\n",
      "Train Epoch: 136 [16916/17352 (97%)] Loss: -129552.335938\n",
      "    epoch          : 136\n",
      "    loss           : -186005.23458473154\n",
      "    val_loss       : -100761.9783431371\n",
      "Train Epoch: 137 [128/17352 (1%)] Loss: -177503.531250\n",
      "Train Epoch: 137 [1536/17352 (9%)] Loss: -202511.218750\n",
      "Train Epoch: 137 [2944/17352 (17%)] Loss: -194940.031250\n",
      "Train Epoch: 137 [4352/17352 (25%)] Loss: -217159.515625\n",
      "Train Epoch: 137 [5760/17352 (33%)] Loss: -209487.750000\n",
      "Train Epoch: 137 [7168/17352 (41%)] Loss: -209534.468750\n",
      "Train Epoch: 137 [8576/17352 (49%)] Loss: -197245.281250\n",
      "Train Epoch: 137 [9984/17352 (58%)] Loss: -209422.265625\n",
      "Train Epoch: 137 [11392/17352 (66%)] Loss: -209358.906250\n",
      "Train Epoch: 137 [12800/17352 (74%)] Loss: -207711.812500\n",
      "Train Epoch: 137 [14208/17352 (82%)] Loss: -213282.609375\n",
      "Train Epoch: 137 [15490/17352 (89%)] Loss: -143481.000000\n",
      "Train Epoch: 137 [16170/17352 (93%)] Loss: -82833.734375\n",
      "Train Epoch: 137 [16895/17352 (97%)] Loss: -8110.713867\n",
      "    epoch          : 137\n",
      "    loss           : -186048.01655568372\n",
      "    val_loss       : -100805.03010209401\n",
      "Train Epoch: 138 [128/17352 (1%)] Loss: -195932.093750\n",
      "Train Epoch: 138 [1536/17352 (9%)] Loss: -208973.078125\n",
      "Train Epoch: 138 [2944/17352 (17%)] Loss: -243071.937500\n",
      "Train Epoch: 138 [4352/17352 (25%)] Loss: -205889.734375\n",
      "Train Epoch: 138 [5760/17352 (33%)] Loss: -194982.812500\n",
      "Train Epoch: 138 [7168/17352 (41%)] Loss: -179334.781250\n",
      "Train Epoch: 138 [8576/17352 (49%)] Loss: -195293.453125\n",
      "Train Epoch: 138 [9984/17352 (58%)] Loss: -204641.187500\n",
      "Train Epoch: 138 [11392/17352 (66%)] Loss: -204175.750000\n",
      "Train Epoch: 138 [12800/17352 (74%)] Loss: -196629.968750\n",
      "Train Epoch: 138 [14208/17352 (82%)] Loss: -225938.375000\n",
      "Train Epoch: 138 [15479/17352 (89%)] Loss: -162654.500000\n",
      "Train Epoch: 138 [16099/17352 (93%)] Loss: -142231.875000\n",
      "Train Epoch: 138 [17058/17352 (98%)] Loss: -207744.437500\n",
      "    epoch          : 138\n",
      "    loss           : -186166.34135447253\n",
      "    val_loss       : -100578.001009051\n",
      "Train Epoch: 139 [128/17352 (1%)] Loss: -211854.640625\n",
      "Train Epoch: 139 [1536/17352 (9%)] Loss: -201493.531250\n",
      "Train Epoch: 139 [2944/17352 (17%)] Loss: -227000.953125\n",
      "Train Epoch: 139 [4352/17352 (25%)] Loss: -194329.968750\n",
      "Train Epoch: 139 [5760/17352 (33%)] Loss: -200116.640625\n",
      "Train Epoch: 139 [7168/17352 (41%)] Loss: -195947.500000\n",
      "Train Epoch: 139 [8576/17352 (49%)] Loss: -212850.750000\n",
      "Train Epoch: 139 [9984/17352 (58%)] Loss: -194316.562500\n",
      "Train Epoch: 139 [11392/17352 (66%)] Loss: -193603.921875\n",
      "Train Epoch: 139 [12800/17352 (74%)] Loss: -206618.000000\n",
      "Train Epoch: 139 [14208/17352 (82%)] Loss: -215683.531250\n",
      "Train Epoch: 139 [15525/17352 (89%)] Loss: -172553.328125\n",
      "Train Epoch: 139 [16304/17352 (94%)] Loss: -23631.033203\n",
      "Train Epoch: 139 [17047/17352 (98%)] Loss: -61176.476562\n",
      "    epoch          : 139\n",
      "    loss           : -186080.22261364828\n",
      "    val_loss       : -100863.05121625264\n",
      "Train Epoch: 140 [128/17352 (1%)] Loss: -215389.968750\n",
      "Train Epoch: 140 [1536/17352 (9%)] Loss: -204897.625000\n",
      "Train Epoch: 140 [2944/17352 (17%)] Loss: -199137.718750\n",
      "Train Epoch: 140 [4352/17352 (25%)] Loss: -209479.109375\n",
      "Train Epoch: 140 [5760/17352 (33%)] Loss: -208576.625000\n",
      "Train Epoch: 140 [7168/17352 (41%)] Loss: -198120.156250\n",
      "Train Epoch: 140 [8576/17352 (49%)] Loss: -206717.265625\n",
      "Train Epoch: 140 [9984/17352 (58%)] Loss: -175941.187500\n",
      "Train Epoch: 140 [11392/17352 (66%)] Loss: -175106.078125\n",
      "Train Epoch: 140 [12800/17352 (74%)] Loss: -226471.562500\n",
      "Train Epoch: 140 [14208/17352 (82%)] Loss: -211621.328125\n",
      "Train Epoch: 140 [15554/17352 (90%)] Loss: -176011.875000\n",
      "Train Epoch: 140 [16232/17352 (94%)] Loss: -208875.875000\n",
      "Train Epoch: 140 [16948/17352 (98%)] Loss: -4989.824219\n",
      "    epoch          : 140\n",
      "    loss           : -185907.29056994547\n",
      "    val_loss       : -100875.60127970377\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0806_145705/checkpoint-epoch140.pth ...\n",
      "Train Epoch: 141 [128/17352 (1%)] Loss: -210741.984375\n",
      "Train Epoch: 141 [1536/17352 (9%)] Loss: -210529.828125\n",
      "Train Epoch: 141 [2944/17352 (17%)] Loss: -215492.156250\n",
      "Train Epoch: 141 [4352/17352 (25%)] Loss: -197003.421875\n",
      "Train Epoch: 141 [5760/17352 (33%)] Loss: -198141.875000\n",
      "Train Epoch: 141 [7168/17352 (41%)] Loss: -210198.359375\n",
      "Train Epoch: 141 [8576/17352 (49%)] Loss: -210768.875000\n",
      "Train Epoch: 141 [9984/17352 (58%)] Loss: -197983.078125\n",
      "Train Epoch: 141 [11392/17352 (66%)] Loss: -212080.875000\n",
      "Train Epoch: 141 [12800/17352 (74%)] Loss: -196686.000000\n",
      "Train Epoch: 141 [14208/17352 (82%)] Loss: -194332.484375\n",
      "Train Epoch: 141 [15487/17352 (89%)] Loss: -58230.406250\n",
      "Train Epoch: 141 [16182/17352 (93%)] Loss: -83795.914062\n",
      "Train Epoch: 141 [16942/17352 (98%)] Loss: -114417.937500\n",
      "    epoch          : 141\n",
      "    loss           : -186149.8833695994\n",
      "    val_loss       : -100838.00611877441\n",
      "Train Epoch: 142 [128/17352 (1%)] Loss: -233384.625000\n",
      "Train Epoch: 142 [1536/17352 (9%)] Loss: -195795.046875\n",
      "Train Epoch: 142 [2944/17352 (17%)] Loss: -197152.984375\n",
      "Train Epoch: 142 [4352/17352 (25%)] Loss: -202024.843750\n",
      "Train Epoch: 142 [5760/17352 (33%)] Loss: -226858.515625\n",
      "Train Epoch: 142 [7168/17352 (41%)] Loss: -174404.718750\n",
      "Train Epoch: 142 [8576/17352 (49%)] Loss: -194818.312500\n",
      "Train Epoch: 142 [9984/17352 (58%)] Loss: -196096.671875\n",
      "Train Epoch: 142 [11392/17352 (66%)] Loss: -194566.625000\n",
      "Train Epoch: 142 [12800/17352 (74%)] Loss: -195012.343750\n",
      "Train Epoch: 142 [14208/17352 (82%)] Loss: -196314.000000\n",
      "Train Epoch: 142 [15522/17352 (89%)] Loss: -162025.078125\n",
      "Train Epoch: 142 [16207/17352 (93%)] Loss: -123266.546875\n",
      "Train Epoch: 142 [16953/17352 (98%)] Loss: -144523.281250\n",
      "    epoch          : 142\n",
      "    loss           : -186150.065341207\n",
      "    val_loss       : -100860.38502960205\n",
      "Train Epoch: 143 [128/17352 (1%)] Loss: -177554.062500\n",
      "Train Epoch: 143 [1536/17352 (9%)] Loss: -195079.984375\n",
      "Train Epoch: 143 [2944/17352 (17%)] Loss: -182315.187500\n",
      "Train Epoch: 143 [4352/17352 (25%)] Loss: -193318.281250\n",
      "Train Epoch: 143 [5760/17352 (33%)] Loss: -227253.718750\n",
      "Train Epoch: 143 [7168/17352 (41%)] Loss: -214168.984375\n",
      "Train Epoch: 143 [8576/17352 (49%)] Loss: -219133.437500\n",
      "Train Epoch: 143 [9984/17352 (58%)] Loss: -208820.859375\n",
      "Train Epoch: 143 [11392/17352 (66%)] Loss: -183950.406250\n",
      "Train Epoch: 143 [12800/17352 (74%)] Loss: -211268.750000\n",
      "Train Epoch: 143 [14208/17352 (82%)] Loss: -210513.515625\n",
      "Train Epoch: 143 [15455/17352 (89%)] Loss: -54887.449219\n",
      "Train Epoch: 143 [16188/17352 (93%)] Loss: -61017.289062\n",
      "Train Epoch: 143 [16992/17352 (98%)] Loss: -119684.828125\n",
      "    epoch          : 143\n",
      "    loss           : -186220.09811503775\n",
      "    val_loss       : -100846.48919436136\n",
      "Train Epoch: 144 [128/17352 (1%)] Loss: -185176.062500\n",
      "Train Epoch: 144 [1536/17352 (9%)] Loss: -193441.609375\n",
      "Train Epoch: 144 [2944/17352 (17%)] Loss: -184731.515625\n",
      "Train Epoch: 144 [4352/17352 (25%)] Loss: -232016.359375\n",
      "Train Epoch: 144 [5760/17352 (33%)] Loss: -198689.187500\n",
      "Train Epoch: 144 [7168/17352 (41%)] Loss: -210704.546875\n",
      "Train Epoch: 144 [8576/17352 (49%)] Loss: -194275.515625\n",
      "Train Epoch: 144 [9984/17352 (58%)] Loss: -205212.218750\n",
      "Train Epoch: 144 [11392/17352 (66%)] Loss: -209471.156250\n",
      "Train Epoch: 144 [12800/17352 (74%)] Loss: -207805.281250\n",
      "Train Epoch: 144 [14208/17352 (82%)] Loss: -214198.281250\n",
      "Train Epoch: 144 [15440/17352 (89%)] Loss: -8266.578125\n",
      "Train Epoch: 144 [16163/17352 (93%)] Loss: -144717.875000\n",
      "Train Epoch: 144 [17033/17352 (98%)] Loss: -132343.031250\n",
      "    epoch          : 144\n",
      "    loss           : -186286.87397100462\n",
      "    val_loss       : -100884.72397829691\n",
      "Train Epoch: 145 [128/17352 (1%)] Loss: -215276.062500\n",
      "Train Epoch: 145 [1536/17352 (9%)] Loss: -196471.156250\n",
      "Train Epoch: 145 [2944/17352 (17%)] Loss: -194578.437500\n",
      "Train Epoch: 145 [4352/17352 (25%)] Loss: -214155.390625\n",
      "Train Epoch: 145 [5760/17352 (33%)] Loss: -208619.312500\n",
      "Train Epoch: 145 [7168/17352 (41%)] Loss: -213180.765625\n",
      "Train Epoch: 145 [8576/17352 (49%)] Loss: -196067.421875\n",
      "Train Epoch: 145 [9984/17352 (58%)] Loss: -202273.562500\n",
      "Train Epoch: 145 [11392/17352 (66%)] Loss: -196754.859375\n",
      "Train Epoch: 145 [12800/17352 (74%)] Loss: -207839.046875\n",
      "Train Epoch: 145 [14208/17352 (82%)] Loss: -212832.437500\n",
      "Train Epoch: 145 [15467/17352 (89%)] Loss: -60229.242188\n",
      "Train Epoch: 145 [16327/17352 (94%)] Loss: -153693.265625\n",
      "Train Epoch: 145 [17044/17352 (98%)] Loss: -24871.050781\n",
      "    epoch          : 145\n",
      "    loss           : -186429.90195705747\n",
      "    val_loss       : -100863.94387906393\n",
      "Train Epoch: 146 [128/17352 (1%)] Loss: -214426.250000\n",
      "Train Epoch: 146 [1536/17352 (9%)] Loss: -195986.687500\n",
      "Train Epoch: 146 [2944/17352 (17%)] Loss: -195059.265625\n",
      "Train Epoch: 146 [4352/17352 (25%)] Loss: -218826.031250\n",
      "Train Epoch: 146 [5760/17352 (33%)] Loss: -207372.500000\n",
      "Train Epoch: 146 [7168/17352 (41%)] Loss: -202659.843750\n",
      "Train Epoch: 146 [8576/17352 (49%)] Loss: -198950.156250\n",
      "Train Epoch: 146 [9984/17352 (58%)] Loss: -202539.687500\n",
      "Train Epoch: 146 [11392/17352 (66%)] Loss: -203846.687500\n",
      "Train Epoch: 146 [12800/17352 (74%)] Loss: -207949.937500\n",
      "Train Epoch: 146 [14208/17352 (82%)] Loss: -199468.500000\n",
      "Train Epoch: 146 [15575/17352 (90%)] Loss: -135966.187500\n",
      "Train Epoch: 146 [16394/17352 (94%)] Loss: -160837.437500\n",
      "Train Epoch: 146 [17042/17352 (98%)] Loss: -77558.390625\n",
      "    epoch          : 146\n",
      "    loss           : -186478.80298015414\n",
      "    val_loss       : -100878.82833309173\n",
      "Train Epoch: 147 [128/17352 (1%)] Loss: -181830.796875\n",
      "Train Epoch: 147 [1536/17352 (9%)] Loss: -208132.312500\n",
      "Train Epoch: 147 [2944/17352 (17%)] Loss: -194873.656250\n",
      "Train Epoch: 147 [4352/17352 (25%)] Loss: -215357.109375\n",
      "Train Epoch: 147 [5760/17352 (33%)] Loss: -193291.843750\n",
      "Train Epoch: 147 [7168/17352 (41%)] Loss: -193123.484375\n",
      "Train Epoch: 147 [8576/17352 (49%)] Loss: -195654.828125\n",
      "Train Epoch: 147 [9984/17352 (58%)] Loss: -194868.906250\n",
      "Train Epoch: 147 [11392/17352 (66%)] Loss: -208993.453125\n",
      "Train Epoch: 147 [12800/17352 (74%)] Loss: -208244.484375\n",
      "Train Epoch: 147 [14208/17352 (82%)] Loss: -209060.343750\n",
      "Train Epoch: 147 [15538/17352 (90%)] Loss: -129451.656250\n",
      "Train Epoch: 147 [16228/17352 (94%)] Loss: -130709.585938\n",
      "Train Epoch: 147 [17028/17352 (98%)] Loss: -60935.074219\n",
      "    epoch          : 147\n",
      "    loss           : -186508.46382786284\n",
      "    val_loss       : -100953.2575630188\n",
      "Train Epoch: 148 [128/17352 (1%)] Loss: -205607.140625\n",
      "Train Epoch: 148 [1536/17352 (9%)] Loss: -212266.468750\n",
      "Train Epoch: 148 [2944/17352 (17%)] Loss: -198344.500000\n",
      "Train Epoch: 148 [4352/17352 (25%)] Loss: -207588.062500\n",
      "Train Epoch: 148 [5760/17352 (33%)] Loss: -227168.421875\n",
      "Train Epoch: 148 [7168/17352 (41%)] Loss: -196523.296875\n",
      "Train Epoch: 148 [8576/17352 (49%)] Loss: -211674.750000\n",
      "Train Epoch: 148 [9984/17352 (58%)] Loss: -195339.437500\n",
      "Train Epoch: 148 [11392/17352 (66%)] Loss: -193333.437500\n",
      "Train Epoch: 148 [12800/17352 (74%)] Loss: -195173.968750\n",
      "Train Epoch: 148 [14208/17352 (82%)] Loss: -185014.953125\n",
      "Train Epoch: 148 [15550/17352 (90%)] Loss: -129837.289062\n",
      "Train Epoch: 148 [16297/17352 (94%)] Loss: -23503.724609\n",
      "Train Epoch: 148 [17034/17352 (98%)] Loss: -125576.000000\n",
      "    epoch          : 148\n",
      "    loss           : -186424.67370687396\n",
      "    val_loss       : -100774.4609992981\n",
      "Validation performance didn't improve for 20 epochs. Training stops.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
