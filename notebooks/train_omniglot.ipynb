{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='omniglot_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 25,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [128/17352 (1%)] Loss: 8152.815918\n",
      "Train Epoch: 1 [1536/17352 (9%)] Loss: -32550.820312\n",
      "Train Epoch: 1 [2944/17352 (17%)] Loss: -65452.242188\n",
      "Train Epoch: 1 [4352/17352 (25%)] Loss: -121182.585938\n",
      "Train Epoch: 1 [5760/17352 (33%)] Loss: -135717.250000\n",
      "Train Epoch: 1 [7168/17352 (41%)] Loss: -147573.890625\n",
      "Train Epoch: 1 [8576/17352 (49%)] Loss: -172549.515625\n",
      "Train Epoch: 1 [9984/17352 (58%)] Loss: -172883.906250\n",
      "Train Epoch: 1 [11392/17352 (66%)] Loss: -181811.437500\n",
      "Train Epoch: 1 [12800/17352 (74%)] Loss: -173528.093750\n",
      "Train Epoch: 1 [14208/17352 (82%)] Loss: -143310.968750\n",
      "Train Epoch: 1 [15496/17352 (89%)] Loss: -68286.578125\n",
      "Train Epoch: 1 [16220/17352 (93%)] Loss: -5103.033691\n",
      "Train Epoch: 1 [16902/17352 (97%)] Loss: -55904.179688\n",
      "    epoch          : 1\n",
      "    loss           : -131332.93696583997\n",
      "    val_loss       : -92339.5450416565\n",
      "Train Epoch: 2 [128/17352 (1%)] Loss: -173818.625000\n",
      "Train Epoch: 2 [1536/17352 (9%)] Loss: -188852.828125\n",
      "Train Epoch: 2 [2944/17352 (17%)] Loss: -215442.750000\n",
      "Train Epoch: 2 [4352/17352 (25%)] Loss: -176085.203125\n",
      "Train Epoch: 2 [5760/17352 (33%)] Loss: -185747.937500\n",
      "Train Epoch: 2 [7168/17352 (41%)] Loss: -177017.593750\n",
      "Train Epoch: 2 [8576/17352 (49%)] Loss: -174935.859375\n",
      "Train Epoch: 2 [9984/17352 (58%)] Loss: -207335.312500\n",
      "Train Epoch: 2 [11392/17352 (66%)] Loss: -175050.968750\n",
      "Train Epoch: 2 [12800/17352 (74%)] Loss: -176681.250000\n",
      "Train Epoch: 2 [14208/17352 (82%)] Loss: -195818.640625\n",
      "Train Epoch: 2 [15560/17352 (90%)] Loss: -111925.914062\n",
      "Train Epoch: 2 [16155/17352 (93%)] Loss: -23892.355469\n",
      "Train Epoch: 2 [17033/17352 (98%)] Loss: -8239.724609\n",
      "    epoch          : 2\n",
      "    loss           : -170008.2287941747\n",
      "    val_loss       : -94261.99761047363\n",
      "Train Epoch: 3 [128/17352 (1%)] Loss: -198398.625000\n",
      "Train Epoch: 3 [1536/17352 (9%)] Loss: -176606.625000\n",
      "Train Epoch: 3 [2944/17352 (17%)] Loss: -183025.031250\n",
      "Train Epoch: 3 [4352/17352 (25%)] Loss: -200639.421875\n",
      "Train Epoch: 3 [5760/17352 (33%)] Loss: -200757.781250\n",
      "Train Epoch: 3 [7168/17352 (41%)] Loss: -162650.437500\n",
      "Train Epoch: 3 [8576/17352 (49%)] Loss: -184940.125000\n",
      "Train Epoch: 3 [9984/17352 (58%)] Loss: -178270.062500\n",
      "Train Epoch: 3 [11392/17352 (66%)] Loss: -186120.046875\n",
      "Train Epoch: 3 [12800/17352 (74%)] Loss: -179768.562500\n",
      "Train Epoch: 3 [14208/17352 (82%)] Loss: -139295.406250\n",
      "Train Epoch: 3 [15489/17352 (89%)] Loss: -142306.468750\n",
      "Train Epoch: 3 [16216/17352 (93%)] Loss: -131816.968750\n",
      "Train Epoch: 3 [17163/17352 (99%)] Loss: -156141.000000\n",
      "    epoch          : 3\n",
      "    loss           : -171390.642460151\n",
      "    val_loss       : -94526.69425767263\n",
      "Train Epoch: 4 [128/17352 (1%)] Loss: -167004.187500\n",
      "Train Epoch: 4 [1536/17352 (9%)] Loss: -191136.265625\n",
      "Train Epoch: 4 [2944/17352 (17%)] Loss: -200174.906250\n",
      "Train Epoch: 4 [4352/17352 (25%)] Loss: -181569.531250\n",
      "Train Epoch: 4 [5760/17352 (33%)] Loss: -193304.625000\n",
      "Train Epoch: 4 [7168/17352 (41%)] Loss: -187631.703125\n",
      "Train Epoch: 4 [8576/17352 (49%)] Loss: -181162.437500\n",
      "Train Epoch: 4 [9984/17352 (58%)] Loss: -201385.468750\n",
      "Train Epoch: 4 [11392/17352 (66%)] Loss: -188712.406250\n",
      "Train Epoch: 4 [12800/17352 (74%)] Loss: -179639.234375\n",
      "Train Epoch: 4 [14208/17352 (82%)] Loss: -203397.765625\n",
      "Train Epoch: 4 [15442/17352 (89%)] Loss: -121978.390625\n",
      "Train Epoch: 4 [16265/17352 (94%)] Loss: -80625.726562\n",
      "Train Epoch: 4 [17015/17352 (98%)] Loss: -134753.125000\n",
      "    epoch          : 4\n",
      "    loss           : -173296.4568018561\n",
      "    val_loss       : -95919.6942232132\n",
      "Train Epoch: 5 [128/17352 (1%)] Loss: -196513.968750\n",
      "Train Epoch: 5 [1536/17352 (9%)] Loss: -197878.671875\n",
      "Train Epoch: 5 [2944/17352 (17%)] Loss: -202533.781250\n",
      "Train Epoch: 5 [4352/17352 (25%)] Loss: -181016.984375\n",
      "Train Epoch: 5 [5760/17352 (33%)] Loss: -181304.250000\n",
      "Train Epoch: 5 [7168/17352 (41%)] Loss: -206391.500000\n",
      "Train Epoch: 5 [8576/17352 (49%)] Loss: -188827.656250\n",
      "Train Epoch: 5 [9984/17352 (58%)] Loss: -194248.843750\n",
      "Train Epoch: 5 [11392/17352 (66%)] Loss: -190258.109375\n",
      "Train Epoch: 5 [12800/17352 (74%)] Loss: -197028.437500\n",
      "Train Epoch: 5 [14208/17352 (82%)] Loss: -186162.296875\n",
      "Train Epoch: 5 [15521/17352 (89%)] Loss: -128651.195312\n",
      "Train Epoch: 5 [16320/17352 (94%)] Loss: -127324.640625\n",
      "Train Epoch: 5 [16984/17352 (98%)] Loss: -23712.867188\n",
      "    epoch          : 5\n",
      "    loss           : -176066.616833578\n",
      "    val_loss       : -97767.92315826417\n",
      "Train Epoch: 6 [128/17352 (1%)] Loss: -202048.000000\n",
      "Train Epoch: 6 [1536/17352 (9%)] Loss: -191741.078125\n",
      "Train Epoch: 6 [2944/17352 (17%)] Loss: -189087.312500\n",
      "Train Epoch: 6 [4352/17352 (25%)] Loss: -184829.703125\n",
      "Train Epoch: 6 [5760/17352 (33%)] Loss: -189257.828125\n",
      "Train Epoch: 6 [7168/17352 (41%)] Loss: -191547.312500\n",
      "Train Epoch: 6 [8576/17352 (49%)] Loss: -188913.312500\n",
      "Train Epoch: 6 [9984/17352 (58%)] Loss: -186420.906250\n",
      "Train Epoch: 6 [11392/17352 (66%)] Loss: -171715.343750\n",
      "Train Epoch: 6 [12800/17352 (74%)] Loss: -189685.531250\n",
      "Train Epoch: 6 [14208/17352 (82%)] Loss: -184832.687500\n",
      "Train Epoch: 6 [15519/17352 (89%)] Loss: -75388.570312\n",
      "Train Epoch: 6 [16249/17352 (94%)] Loss: -54060.250000\n",
      "Train Epoch: 6 [17009/17352 (98%)] Loss: -25445.076172\n",
      "    epoch          : 6\n",
      "    loss           : -178735.71174037855\n",
      "    val_loss       : -99435.60556844076\n",
      "Train Epoch: 7 [128/17352 (1%)] Loss: -203297.953125\n",
      "Train Epoch: 7 [1536/17352 (9%)] Loss: -209486.937500\n",
      "Train Epoch: 7 [2944/17352 (17%)] Loss: -188525.812500\n",
      "Train Epoch: 7 [4352/17352 (25%)] Loss: -184231.875000\n",
      "Train Epoch: 7 [5760/17352 (33%)] Loss: -201154.703125\n",
      "Train Epoch: 7 [7168/17352 (41%)] Loss: -211475.187500\n",
      "Train Epoch: 7 [8576/17352 (49%)] Loss: -179281.203125\n",
      "Train Epoch: 7 [9984/17352 (58%)] Loss: -206747.875000\n",
      "Train Epoch: 7 [11392/17352 (66%)] Loss: -176200.343750\n",
      "Train Epoch: 7 [12800/17352 (74%)] Loss: -196907.656250\n",
      "Train Epoch: 7 [14208/17352 (82%)] Loss: -210859.171875\n",
      "Train Epoch: 7 [15443/17352 (89%)] Loss: -117284.585938\n",
      "Train Epoch: 7 [16205/17352 (93%)] Loss: -81829.859375\n",
      "Train Epoch: 7 [17013/17352 (98%)] Loss: -22611.636719\n",
      "    epoch          : 7\n",
      "    loss           : -181402.7886299287\n",
      "    val_loss       : -100587.5297972997\n",
      "Train Epoch: 8 [128/17352 (1%)] Loss: -202793.609375\n",
      "Train Epoch: 8 [1536/17352 (9%)] Loss: -193650.968750\n",
      "Train Epoch: 8 [2944/17352 (17%)] Loss: -190931.468750\n",
      "Train Epoch: 8 [4352/17352 (25%)] Loss: -214836.375000\n",
      "Train Epoch: 8 [5760/17352 (33%)] Loss: -208771.000000\n",
      "Train Epoch: 8 [7168/17352 (41%)] Loss: -243175.234375\n",
      "Train Epoch: 8 [8576/17352 (49%)] Loss: -223308.640625\n",
      "Train Epoch: 8 [9984/17352 (58%)] Loss: -208621.265625\n",
      "Train Epoch: 8 [11392/17352 (66%)] Loss: -197895.656250\n",
      "Train Epoch: 8 [12800/17352 (74%)] Loss: -203005.312500\n",
      "Train Epoch: 8 [14208/17352 (82%)] Loss: -194013.515625\n",
      "Train Epoch: 8 [15504/17352 (89%)] Loss: -71251.218750\n",
      "Train Epoch: 8 [16488/17352 (95%)] Loss: -140781.421875\n",
      "Train Epoch: 8 [17009/17352 (98%)] Loss: -169361.687500\n",
      "    epoch          : 8\n",
      "    loss           : -183608.714548815\n",
      "    val_loss       : -101384.60588963826\n",
      "Train Epoch: 9 [128/17352 (1%)] Loss: -176733.062500\n",
      "Train Epoch: 9 [1536/17352 (9%)] Loss: -193152.421875\n",
      "Train Epoch: 9 [2944/17352 (17%)] Loss: -239727.703125\n",
      "Train Epoch: 9 [4352/17352 (25%)] Loss: -230321.718750\n",
      "Train Epoch: 9 [5760/17352 (33%)] Loss: -191008.281250\n",
      "Train Epoch: 9 [7168/17352 (41%)] Loss: -207688.343750\n",
      "Train Epoch: 9 [8576/17352 (49%)] Loss: -210732.125000\n",
      "Train Epoch: 9 [9984/17352 (58%)] Loss: -210505.000000\n",
      "Train Epoch: 9 [11392/17352 (66%)] Loss: -210062.890625\n",
      "Train Epoch: 9 [12800/17352 (74%)] Loss: -210010.718750\n",
      "Train Epoch: 9 [14208/17352 (82%)] Loss: -208748.109375\n",
      "Train Epoch: 9 [15480/17352 (89%)] Loss: -129641.789062\n",
      "Train Epoch: 9 [16183/17352 (93%)] Loss: -23503.328125\n",
      "Train Epoch: 9 [17010/17352 (98%)] Loss: -207385.656250\n",
      "    epoch          : 9\n",
      "    loss           : -185159.88243236157\n",
      "    val_loss       : -102382.22885437011\n",
      "Train Epoch: 10 [128/17352 (1%)] Loss: -208132.828125\n",
      "Train Epoch: 10 [1536/17352 (9%)] Loss: -216735.859375\n",
      "Train Epoch: 10 [2944/17352 (17%)] Loss: -210143.718750\n",
      "Train Epoch: 10 [4352/17352 (25%)] Loss: -193346.875000\n",
      "Train Epoch: 10 [5760/17352 (33%)] Loss: -209263.156250\n",
      "Train Epoch: 10 [7168/17352 (41%)] Loss: -216922.281250\n",
      "Train Epoch: 10 [8576/17352 (49%)] Loss: -197554.484375\n",
      "Train Epoch: 10 [9984/17352 (58%)] Loss: -202870.515625\n",
      "Train Epoch: 10 [11392/17352 (66%)] Loss: -210934.765625\n",
      "Train Epoch: 10 [12800/17352 (74%)] Loss: -197239.187500\n",
      "Train Epoch: 10 [14208/17352 (82%)] Loss: -193876.750000\n",
      "Train Epoch: 10 [15555/17352 (90%)] Loss: -121334.406250\n",
      "Train Epoch: 10 [16394/17352 (94%)] Loss: -86115.468750\n",
      "Train Epoch: 10 [17070/17352 (98%)] Loss: -126479.953125\n",
      "    epoch          : 10\n",
      "    loss           : -186621.99774866295\n",
      "    val_loss       : -102976.1096110026\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0813_152913/checkpoint-epoch10.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 11 [128/17352 (1%)] Loss: -184209.312500\n",
      "Train Epoch: 11 [1536/17352 (9%)] Loss: -206736.593750\n",
      "Train Epoch: 11 [2944/17352 (17%)] Loss: -211133.703125\n",
      "Train Epoch: 11 [4352/17352 (25%)] Loss: -194417.328125\n",
      "Train Epoch: 11 [5760/17352 (33%)] Loss: -195770.343750\n",
      "Train Epoch: 11 [7168/17352 (41%)] Loss: -212500.625000\n",
      "Train Epoch: 11 [8576/17352 (49%)] Loss: -200443.937500\n",
      "Train Epoch: 11 [9984/17352 (58%)] Loss: -207774.671875\n",
      "Train Epoch: 11 [11392/17352 (66%)] Loss: -179027.734375\n",
      "Train Epoch: 11 [12800/17352 (74%)] Loss: -198532.906250\n",
      "Train Epoch: 11 [14208/17352 (82%)] Loss: -219618.031250\n",
      "Train Epoch: 11 [15533/17352 (90%)] Loss: -161460.843750\n",
      "Train Epoch: 11 [16239/17352 (94%)] Loss: -119257.046875\n",
      "Train Epoch: 11 [17070/17352 (98%)] Loss: -211598.250000\n",
      "    epoch          : 11\n",
      "    loss           : -187788.4591449507\n",
      "    val_loss       : -103807.6205078125\n",
      "Train Epoch: 12 [128/17352 (1%)] Loss: -178787.187500\n",
      "Train Epoch: 12 [1536/17352 (9%)] Loss: -200511.812500\n",
      "Train Epoch: 12 [2944/17352 (17%)] Loss: -229924.031250\n",
      "Train Epoch: 12 [4352/17352 (25%)] Loss: -184509.875000\n",
      "Train Epoch: 12 [5760/17352 (33%)] Loss: -199607.703125\n",
      "Train Epoch: 12 [7168/17352 (41%)] Loss: -213570.062500\n",
      "Train Epoch: 12 [8576/17352 (49%)] Loss: -200116.734375\n",
      "Train Epoch: 12 [9984/17352 (58%)] Loss: -183321.625000\n",
      "Train Epoch: 12 [11392/17352 (66%)] Loss: -200557.906250\n",
      "Train Epoch: 12 [12800/17352 (74%)] Loss: -214733.031250\n",
      "Train Epoch: 12 [14208/17352 (82%)] Loss: -201183.187500\n",
      "Train Epoch: 12 [15570/17352 (90%)] Loss: -153963.843750\n",
      "Train Epoch: 12 [16365/17352 (94%)] Loss: -174220.578125\n",
      "Train Epoch: 12 [17049/17352 (98%)] Loss: -23789.132812\n",
      "    epoch          : 12\n",
      "    loss           : -189279.12263724307\n",
      "    val_loss       : -104484.24627990722\n",
      "Train Epoch: 13 [128/17352 (1%)] Loss: -198457.625000\n",
      "Train Epoch: 13 [1536/17352 (9%)] Loss: -202800.156250\n",
      "Train Epoch: 13 [2944/17352 (17%)] Loss: -247428.156250\n",
      "Train Epoch: 13 [4352/17352 (25%)] Loss: -233049.156250\n",
      "Train Epoch: 13 [5760/17352 (33%)] Loss: -214192.781250\n",
      "Train Epoch: 13 [7168/17352 (41%)] Loss: -199618.437500\n",
      "Train Epoch: 13 [8576/17352 (49%)] Loss: -203000.843750\n",
      "Train Epoch: 13 [9984/17352 (58%)] Loss: -201335.500000\n",
      "Train Epoch: 13 [11392/17352 (66%)] Loss: -214462.062500\n",
      "Train Epoch: 13 [12800/17352 (74%)] Loss: -204047.578125\n",
      "Train Epoch: 13 [14208/17352 (82%)] Loss: -199211.046875\n",
      "Train Epoch: 13 [15575/17352 (90%)] Loss: -135893.000000\n",
      "Train Epoch: 13 [16221/17352 (93%)] Loss: -131309.953125\n",
      "Train Epoch: 13 [16986/17352 (98%)] Loss: -62019.835938\n",
      "    epoch          : 13\n",
      "    loss           : -190562.38394636117\n",
      "    val_loss       : -105267.64001770019\n",
      "Train Epoch: 14 [128/17352 (1%)] Loss: -215395.250000\n",
      "Train Epoch: 14 [1536/17352 (9%)] Loss: -200793.312500\n",
      "Train Epoch: 14 [2944/17352 (17%)] Loss: -251496.718750\n",
      "Train Epoch: 14 [4352/17352 (25%)] Loss: -221818.718750\n",
      "Train Epoch: 14 [5760/17352 (33%)] Loss: -231491.515625\n",
      "Train Epoch: 14 [7168/17352 (41%)] Loss: -205675.875000\n",
      "Train Epoch: 14 [8576/17352 (49%)] Loss: -231162.187500\n",
      "Train Epoch: 14 [9984/17352 (58%)] Loss: -202638.156250\n",
      "Train Epoch: 14 [11392/17352 (66%)] Loss: -189947.281250\n",
      "Train Epoch: 14 [12800/17352 (74%)] Loss: -212007.656250\n",
      "Train Epoch: 14 [14208/17352 (82%)] Loss: -204197.218750\n",
      "Train Epoch: 14 [15409/17352 (89%)] Loss: -25847.736328\n",
      "Train Epoch: 14 [16180/17352 (93%)] Loss: -152693.750000\n",
      "Train Epoch: 14 [16921/17352 (98%)] Loss: -134833.531250\n",
      "    epoch          : 14\n",
      "    loss           : -191562.2763082005\n",
      "    val_loss       : -105704.52562154134\n",
      "Train Epoch: 15 [128/17352 (1%)] Loss: -220774.218750\n",
      "Train Epoch: 15 [1536/17352 (9%)] Loss: -200756.031250\n",
      "Train Epoch: 15 [2944/17352 (17%)] Loss: -231762.343750\n",
      "Train Epoch: 15 [4352/17352 (25%)] Loss: -203486.265625\n",
      "Train Epoch: 15 [5760/17352 (33%)] Loss: -203281.328125\n",
      "Train Epoch: 15 [7168/17352 (41%)] Loss: -182835.250000\n",
      "Train Epoch: 15 [8576/17352 (49%)] Loss: -203052.156250\n",
      "Train Epoch: 15 [9984/17352 (58%)] Loss: -220332.718750\n",
      "Train Epoch: 15 [11392/17352 (66%)] Loss: -191012.812500\n",
      "Train Epoch: 15 [12800/17352 (74%)] Loss: -219959.968750\n",
      "Train Epoch: 15 [14208/17352 (82%)] Loss: -198210.890625\n",
      "Train Epoch: 15 [15570/17352 (90%)] Loss: -185523.281250\n",
      "Train Epoch: 15 [16366/17352 (94%)] Loss: -133250.750000\n",
      "Train Epoch: 15 [17083/17352 (98%)] Loss: -79204.851562\n",
      "    epoch          : 15\n",
      "    loss           : -192639.77338179006\n",
      "    val_loss       : -106051.99858907064\n",
      "Train Epoch: 16 [128/17352 (1%)] Loss: -215498.906250\n",
      "Train Epoch: 16 [1536/17352 (9%)] Loss: -214717.875000\n",
      "Train Epoch: 16 [2944/17352 (17%)] Loss: -220138.062500\n",
      "Train Epoch: 16 [4352/17352 (25%)] Loss: -214539.953125\n",
      "Train Epoch: 16 [5760/17352 (33%)] Loss: -219875.687500\n",
      "Train Epoch: 16 [7168/17352 (41%)] Loss: -225898.156250\n",
      "Train Epoch: 16 [8576/17352 (49%)] Loss: -204040.625000\n",
      "Train Epoch: 16 [9984/17352 (58%)] Loss: -234958.890625\n",
      "Train Epoch: 16 [11392/17352 (66%)] Loss: -210756.062500\n",
      "Train Epoch: 16 [12800/17352 (74%)] Loss: -219453.171875\n",
      "Train Epoch: 16 [14208/17352 (82%)] Loss: -219793.828125\n",
      "Train Epoch: 16 [15397/17352 (89%)] Loss: -5442.535156\n",
      "Train Epoch: 16 [16123/17352 (93%)] Loss: -63458.343750\n",
      "Train Epoch: 16 [16855/17352 (97%)] Loss: -127213.414062\n",
      "    epoch          : 16\n",
      "    loss           : -193493.61115771811\n",
      "    val_loss       : -106634.39377746583\n",
      "Train Epoch: 17 [128/17352 (1%)] Loss: -217811.968750\n",
      "Train Epoch: 17 [1536/17352 (9%)] Loss: -203357.515625\n",
      "Train Epoch: 17 [2944/17352 (17%)] Loss: -234400.281250\n",
      "Train Epoch: 17 [4352/17352 (25%)] Loss: -228000.796875\n",
      "Train Epoch: 17 [5760/17352 (33%)] Loss: -190763.218750\n",
      "Train Epoch: 17 [7168/17352 (41%)] Loss: -250560.921875\n",
      "Train Epoch: 17 [8576/17352 (49%)] Loss: -202096.656250\n",
      "Train Epoch: 17 [9984/17352 (58%)] Loss: -214520.062500\n",
      "Train Epoch: 17 [11392/17352 (66%)] Loss: -194027.718750\n",
      "Train Epoch: 17 [12800/17352 (74%)] Loss: -213956.500000\n",
      "Train Epoch: 17 [14208/17352 (82%)] Loss: -201971.859375\n",
      "Train Epoch: 17 [15491/17352 (89%)] Loss: -80100.234375\n",
      "Train Epoch: 17 [16133/17352 (93%)] Loss: -79225.960938\n",
      "Train Epoch: 17 [16984/17352 (98%)] Loss: -176953.156250\n",
      "    epoch          : 17\n",
      "    loss           : -194165.9823268404\n",
      "    val_loss       : -107011.7694498698\n",
      "Train Epoch: 18 [128/17352 (1%)] Loss: -222222.031250\n",
      "Train Epoch: 18 [1536/17352 (9%)] Loss: -216777.078125\n",
      "Train Epoch: 18 [2944/17352 (17%)] Loss: -202381.515625\n",
      "Train Epoch: 18 [4352/17352 (25%)] Loss: -207347.343750\n",
      "Train Epoch: 18 [5760/17352 (33%)] Loss: -215668.765625\n",
      "Train Epoch: 18 [7168/17352 (41%)] Loss: -226077.000000\n",
      "Train Epoch: 18 [8576/17352 (49%)] Loss: -235126.046875\n",
      "Train Epoch: 18 [9984/17352 (58%)] Loss: -225481.312500\n",
      "Train Epoch: 18 [11392/17352 (66%)] Loss: -215501.078125\n",
      "Train Epoch: 18 [12800/17352 (74%)] Loss: -212087.421875\n",
      "Train Epoch: 18 [14208/17352 (82%)] Loss: -207678.921875\n",
      "Train Epoch: 18 [15543/17352 (90%)] Loss: -126534.125000\n",
      "Train Epoch: 18 [16326/17352 (94%)] Loss: -186023.515625\n",
      "Train Epoch: 18 [17058/17352 (98%)] Loss: -120837.882812\n",
      "    epoch          : 18\n",
      "    loss           : -194978.1912817219\n",
      "    val_loss       : -107514.86833801269\n",
      "Train Epoch: 19 [128/17352 (1%)] Loss: -223143.703125\n",
      "Train Epoch: 19 [1536/17352 (9%)] Loss: -225572.546875\n",
      "Train Epoch: 19 [2944/17352 (17%)] Loss: -233902.250000\n",
      "Train Epoch: 19 [4352/17352 (25%)] Loss: -208321.703125\n",
      "Train Epoch: 19 [5760/17352 (33%)] Loss: -191249.015625\n",
      "Train Epoch: 19 [7168/17352 (41%)] Loss: -210296.593750\n",
      "Train Epoch: 19 [8576/17352 (49%)] Loss: -203447.796875\n",
      "Train Epoch: 19 [9984/17352 (58%)] Loss: -227544.093750\n",
      "Train Epoch: 19 [11392/17352 (66%)] Loss: -195527.562500\n",
      "Train Epoch: 19 [12800/17352 (74%)] Loss: -214360.093750\n",
      "Train Epoch: 19 [14208/17352 (82%)] Loss: -208077.687500\n",
      "Train Epoch: 19 [15478/17352 (89%)] Loss: -61249.593750\n",
      "Train Epoch: 19 [16148/17352 (93%)] Loss: -26825.185547\n",
      "Train Epoch: 19 [16977/17352 (98%)] Loss: -9080.357422\n",
      "    epoch          : 19\n",
      "    loss           : -195611.69721974622\n",
      "    val_loss       : -107829.53988749186\n",
      "Train Epoch: 20 [128/17352 (1%)] Loss: -224047.562500\n",
      "Train Epoch: 20 [1536/17352 (9%)] Loss: -214117.656250\n",
      "Train Epoch: 20 [2944/17352 (17%)] Loss: -235076.218750\n",
      "Train Epoch: 20 [4352/17352 (25%)] Loss: -210704.093750\n",
      "Train Epoch: 20 [5760/17352 (33%)] Loss: -217109.343750\n",
      "Train Epoch: 20 [7168/17352 (41%)] Loss: -203783.593750\n",
      "Train Epoch: 20 [8576/17352 (49%)] Loss: -206385.578125\n",
      "Train Epoch: 20 [9984/17352 (58%)] Loss: -218551.375000\n",
      "Train Epoch: 20 [11392/17352 (66%)] Loss: -208235.671875\n",
      "Train Epoch: 20 [12800/17352 (74%)] Loss: -206357.046875\n",
      "Train Epoch: 20 [14208/17352 (82%)] Loss: -223788.406250\n",
      "Train Epoch: 20 [15491/17352 (89%)] Loss: -81197.859375\n",
      "Train Epoch: 20 [16267/17352 (94%)] Loss: -187526.250000\n",
      "Train Epoch: 20 [17103/17352 (99%)] Loss: -179228.218750\n",
      "    epoch          : 20\n",
      "    loss           : -196013.001953125\n",
      "    val_loss       : -108072.13426717122\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0813_152913/checkpoint-epoch20.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 21 [128/17352 (1%)] Loss: -225408.562500\n",
      "Train Epoch: 21 [1536/17352 (9%)] Loss: -207501.718750\n",
      "Train Epoch: 21 [2944/17352 (17%)] Loss: -208971.500000\n",
      "Train Epoch: 21 [4352/17352 (25%)] Loss: -224281.765625\n",
      "Train Epoch: 21 [5760/17352 (33%)] Loss: -221825.375000\n",
      "Train Epoch: 21 [7168/17352 (41%)] Loss: -230725.046875\n",
      "Train Epoch: 21 [8576/17352 (49%)] Loss: -211903.703125\n",
      "Train Epoch: 21 [9984/17352 (58%)] Loss: -225894.343750\n",
      "Train Epoch: 21 [11392/17352 (66%)] Loss: -204869.906250\n",
      "Train Epoch: 21 [12800/17352 (74%)] Loss: -220019.609375\n",
      "Train Epoch: 21 [14208/17352 (82%)] Loss: -224727.015625\n",
      "Train Epoch: 21 [15471/17352 (89%)] Loss: -135541.218750\n",
      "Train Epoch: 21 [16186/17352 (93%)] Loss: -135431.453125\n",
      "Train Epoch: 21 [17085/17352 (98%)] Loss: -119144.984375\n",
      "    epoch          : 21\n",
      "    loss           : -196817.67666605496\n",
      "    val_loss       : -108436.61108093262\n",
      "Train Epoch: 22 [128/17352 (1%)] Loss: -194306.484375\n",
      "Train Epoch: 22 [1536/17352 (9%)] Loss: -221612.515625\n",
      "Train Epoch: 22 [2944/17352 (17%)] Loss: -238539.156250\n",
      "Train Epoch: 22 [4352/17352 (25%)] Loss: -216344.812500\n",
      "Train Epoch: 22 [5760/17352 (33%)] Loss: -215157.859375\n",
      "Train Epoch: 22 [7168/17352 (41%)] Loss: -209599.000000\n",
      "Train Epoch: 22 [8576/17352 (49%)] Loss: -238350.468750\n",
      "Train Epoch: 22 [9984/17352 (58%)] Loss: -191088.687500\n",
      "Train Epoch: 22 [11392/17352 (66%)] Loss: -211810.937500\n",
      "Train Epoch: 22 [12800/17352 (74%)] Loss: -222595.218750\n",
      "Train Epoch: 22 [14208/17352 (82%)] Loss: -211261.062500\n",
      "Train Epoch: 22 [15523/17352 (89%)] Loss: -140330.296875\n",
      "Train Epoch: 22 [16340/17352 (94%)] Loss: -62162.933594\n",
      "Train Epoch: 22 [17059/17352 (98%)] Loss: -147411.828125\n",
      "    epoch          : 22\n",
      "    loss           : -197297.70846660025\n",
      "    val_loss       : -108550.27157185873\n",
      "Train Epoch: 23 [128/17352 (1%)] Loss: -192208.703125\n",
      "Train Epoch: 23 [1536/17352 (9%)] Loss: -215518.656250\n",
      "Train Epoch: 23 [2944/17352 (17%)] Loss: -237925.437500\n",
      "Train Epoch: 23 [4352/17352 (25%)] Loss: -237135.156250\n",
      "Train Epoch: 23 [5760/17352 (33%)] Loss: -195808.546875\n",
      "Train Epoch: 23 [7168/17352 (41%)] Loss: -221895.718750\n",
      "Train Epoch: 23 [8576/17352 (49%)] Loss: -222676.468750\n",
      "Train Epoch: 23 [9984/17352 (58%)] Loss: -218090.750000\n",
      "Train Epoch: 23 [11392/17352 (66%)] Loss: -208262.968750\n",
      "Train Epoch: 23 [12800/17352 (74%)] Loss: -207108.703125\n",
      "Train Epoch: 23 [14208/17352 (82%)] Loss: -227822.718750\n",
      "Train Epoch: 23 [15543/17352 (90%)] Loss: -138572.250000\n",
      "Train Epoch: 23 [16221/17352 (93%)] Loss: -150158.609375\n",
      "Train Epoch: 23 [17074/17352 (98%)] Loss: -185186.218750\n",
      "    epoch          : 23\n",
      "    loss           : -197185.55759424812\n",
      "    val_loss       : -108242.6315633138\n",
      "Train Epoch: 24 [128/17352 (1%)] Loss: -223529.187500\n",
      "Train Epoch: 24 [1536/17352 (9%)] Loss: -229309.093750\n",
      "Train Epoch: 24 [2944/17352 (17%)] Loss: -253579.968750\n",
      "Train Epoch: 24 [4352/17352 (25%)] Loss: -213321.625000\n",
      "Train Epoch: 24 [5760/17352 (33%)] Loss: -212259.031250\n",
      "Train Epoch: 24 [7168/17352 (41%)] Loss: -221405.234375\n",
      "Train Epoch: 24 [8576/17352 (49%)] Loss: -214482.796875\n",
      "Train Epoch: 24 [9984/17352 (58%)] Loss: -221300.281250\n",
      "Train Epoch: 24 [11392/17352 (66%)] Loss: -196546.406250\n",
      "Train Epoch: 24 [12800/17352 (74%)] Loss: -206852.781250\n",
      "Train Epoch: 24 [14208/17352 (82%)] Loss: -229073.531250\n",
      "Train Epoch: 24 [15546/17352 (90%)] Loss: -135328.453125\n",
      "Train Epoch: 24 [16086/17352 (93%)] Loss: -128514.625000\n",
      "Train Epoch: 24 [16917/17352 (97%)] Loss: -160650.687500\n",
      "    epoch          : 24\n",
      "    loss           : -198228.94099661807\n",
      "    val_loss       : -109266.97458292643\n",
      "Train Epoch: 25 [128/17352 (1%)] Loss: -225831.156250\n",
      "Train Epoch: 25 [1536/17352 (9%)] Loss: -219388.171875\n",
      "Train Epoch: 25 [2944/17352 (17%)] Loss: -186685.437500\n",
      "Train Epoch: 25 [4352/17352 (25%)] Loss: -230057.687500\n",
      "Train Epoch: 25 [5760/17352 (33%)] Loss: -205345.625000\n",
      "Train Epoch: 25 [7168/17352 (41%)] Loss: -224996.843750\n",
      "Train Epoch: 25 [8576/17352 (49%)] Loss: -208861.578125\n",
      "Train Epoch: 25 [9984/17352 (58%)] Loss: -206700.718750\n",
      "Train Epoch: 25 [11392/17352 (66%)] Loss: -193138.062500\n",
      "Train Epoch: 25 [12800/17352 (74%)] Loss: -224393.765625\n",
      "Train Epoch: 25 [14208/17352 (82%)] Loss: -208053.625000\n",
      "Train Epoch: 25 [15469/17352 (89%)] Loss: -24104.890625\n",
      "Train Epoch: 25 [16205/17352 (93%)] Loss: -85607.460938\n",
      "Train Epoch: 25 [16973/17352 (98%)] Loss: -5173.028320\n",
      "    epoch          : 25\n",
      "    loss           : -199071.681581638\n",
      "    val_loss       : -109520.18619181315\n",
      "Train Epoch: 26 [128/17352 (1%)] Loss: -192294.281250\n",
      "Train Epoch: 26 [1536/17352 (9%)] Loss: -228453.515625\n",
      "Train Epoch: 26 [2944/17352 (17%)] Loss: -191768.218750\n",
      "Train Epoch: 26 [4352/17352 (25%)] Loss: -223136.812500\n",
      "Train Epoch: 26 [5760/17352 (33%)] Loss: -212962.171875\n",
      "Train Epoch: 26 [7168/17352 (41%)] Loss: -224692.953125\n",
      "Train Epoch: 26 [8576/17352 (49%)] Loss: -215730.937500\n",
      "Train Epoch: 26 [9984/17352 (58%)] Loss: -227070.296875\n",
      "Train Epoch: 26 [11392/17352 (66%)] Loss: -230079.093750\n",
      "Train Epoch: 26 [12800/17352 (74%)] Loss: -207886.828125\n",
      "Train Epoch: 26 [14208/17352 (82%)] Loss: -228141.781250\n",
      "Train Epoch: 26 [15444/17352 (89%)] Loss: -82149.914062\n",
      "Train Epoch: 26 [16423/17352 (95%)] Loss: -226980.687500\n",
      "Train Epoch: 26 [17040/17352 (98%)] Loss: -158301.218750\n",
      "    epoch          : 26\n",
      "    loss           : -199570.04982762688\n",
      "    val_loss       : -109833.38567810059\n",
      "Train Epoch: 27 [128/17352 (1%)] Loss: -192888.046875\n",
      "Train Epoch: 27 [1536/17352 (9%)] Loss: -237280.515625\n",
      "Train Epoch: 27 [2944/17352 (17%)] Loss: -255251.812500\n",
      "Train Epoch: 27 [4352/17352 (25%)] Loss: -222440.203125\n",
      "Train Epoch: 27 [5760/17352 (33%)] Loss: -211223.828125\n",
      "Train Epoch: 27 [7168/17352 (41%)] Loss: -215958.093750\n",
      "Train Epoch: 27 [8576/17352 (49%)] Loss: -209962.500000\n",
      "Train Epoch: 27 [9984/17352 (58%)] Loss: -209323.218750\n",
      "Train Epoch: 27 [11392/17352 (66%)] Loss: -224089.046875\n",
      "Train Epoch: 27 [12800/17352 (74%)] Loss: -224312.000000\n",
      "Train Epoch: 27 [14208/17352 (82%)] Loss: -239150.218750\n",
      "Train Epoch: 27 [15466/17352 (89%)] Loss: -178511.937500\n",
      "Train Epoch: 27 [16293/17352 (94%)] Loss: -135582.312500\n",
      "Train Epoch: 27 [17009/17352 (98%)] Loss: -143129.390625\n",
      "    epoch          : 27\n",
      "    loss           : -199600.73532534606\n",
      "    val_loss       : -108145.92456258138\n",
      "Train Epoch: 28 [128/17352 (1%)] Loss: -190144.859375\n",
      "Train Epoch: 28 [1536/17352 (9%)] Loss: -228659.359375\n",
      "Train Epoch: 28 [2944/17352 (17%)] Loss: -233428.156250\n",
      "Train Epoch: 28 [4352/17352 (25%)] Loss: -211881.062500\n",
      "Train Epoch: 28 [5760/17352 (33%)] Loss: -228733.812500\n",
      "Train Epoch: 28 [7168/17352 (41%)] Loss: -225909.093750\n",
      "Train Epoch: 28 [8576/17352 (49%)] Loss: -216697.453125\n",
      "Train Epoch: 28 [9984/17352 (58%)] Loss: -239290.218750\n",
      "Train Epoch: 28 [11392/17352 (66%)] Loss: -226363.218750\n",
      "Train Epoch: 28 [12800/17352 (74%)] Loss: -209938.656250\n",
      "Train Epoch: 28 [14208/17352 (82%)] Loss: -226206.296875\n",
      "Train Epoch: 28 [15482/17352 (89%)] Loss: -194246.921875\n",
      "Train Epoch: 28 [16321/17352 (94%)] Loss: -63994.839844\n",
      "Train Epoch: 28 [16994/17352 (98%)] Loss: -138508.187500\n",
      "    epoch          : 28\n",
      "    loss           : -200030.95936123637\n",
      "    val_loss       : -110360.88056030273\n",
      "Train Epoch: 29 [128/17352 (1%)] Loss: -226171.609375\n",
      "Train Epoch: 29 [1536/17352 (9%)] Loss: -226225.343750\n",
      "Train Epoch: 29 [2944/17352 (17%)] Loss: -207856.781250\n",
      "Train Epoch: 29 [4352/17352 (25%)] Loss: -213250.781250\n",
      "Train Epoch: 29 [5760/17352 (33%)] Loss: -225263.390625\n",
      "Train Epoch: 29 [7168/17352 (41%)] Loss: -214052.343750\n",
      "Train Epoch: 29 [8576/17352 (49%)] Loss: -222198.812500\n",
      "Train Epoch: 29 [9984/17352 (58%)] Loss: -227913.796875\n",
      "Train Epoch: 29 [11392/17352 (66%)] Loss: -195332.281250\n",
      "Train Epoch: 29 [12800/17352 (74%)] Loss: -226527.843750\n",
      "Train Epoch: 29 [14208/17352 (82%)] Loss: -230858.828125\n",
      "Train Epoch: 29 [15523/17352 (89%)] Loss: -144590.406250\n",
      "Train Epoch: 29 [16391/17352 (94%)] Loss: -130720.296875\n",
      "Train Epoch: 29 [17013/17352 (98%)] Loss: -139563.203125\n",
      "    epoch          : 29\n",
      "    loss           : -201083.6156079593\n",
      "    val_loss       : -110630.17313639323\n",
      "Train Epoch: 30 [128/17352 (1%)] Loss: -223460.468750\n",
      "Train Epoch: 30 [1536/17352 (9%)] Loss: -216069.468750\n",
      "Train Epoch: 30 [2944/17352 (17%)] Loss: -233934.906250\n",
      "Train Epoch: 30 [4352/17352 (25%)] Loss: -212806.796875\n",
      "Train Epoch: 30 [5760/17352 (33%)] Loss: -221877.500000\n",
      "Train Epoch: 30 [7168/17352 (41%)] Loss: -228775.046875\n",
      "Train Epoch: 30 [8576/17352 (49%)] Loss: -228684.812500\n",
      "Train Epoch: 30 [9984/17352 (58%)] Loss: -229908.328125\n",
      "Train Epoch: 30 [11392/17352 (66%)] Loss: -213467.109375\n",
      "Train Epoch: 30 [12800/17352 (74%)] Loss: -213293.062500\n",
      "Train Epoch: 30 [14208/17352 (82%)] Loss: -215942.078125\n",
      "Train Epoch: 30 [15517/17352 (89%)] Loss: -153536.468750\n",
      "Train Epoch: 30 [16367/17352 (94%)] Loss: -135671.625000\n",
      "Train Epoch: 30 [17017/17352 (98%)] Loss: -61785.125000\n",
      "    epoch          : 30\n",
      "    loss           : -201249.9184472001\n",
      "    val_loss       : -110565.5281850179\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0813_152913/checkpoint-epoch30.pth ...\n",
      "Train Epoch: 31 [128/17352 (1%)] Loss: -199993.328125\n",
      "Train Epoch: 31 [1536/17352 (9%)] Loss: -230475.015625\n",
      "Train Epoch: 31 [2944/17352 (17%)] Loss: -196946.531250\n",
      "Train Epoch: 31 [4352/17352 (25%)] Loss: -234390.343750\n",
      "Train Epoch: 31 [5760/17352 (33%)] Loss: -199509.093750\n",
      "Train Epoch: 31 [7168/17352 (41%)] Loss: -234501.171875\n",
      "Train Epoch: 31 [8576/17352 (49%)] Loss: -210631.406250\n",
      "Train Epoch: 31 [9984/17352 (58%)] Loss: -228251.546875\n",
      "Train Epoch: 31 [11392/17352 (66%)] Loss: -216029.687500\n",
      "Train Epoch: 31 [12800/17352 (74%)] Loss: -213577.921875\n",
      "Train Epoch: 31 [14208/17352 (82%)] Loss: -216732.046875\n",
      "Train Epoch: 31 [15479/17352 (89%)] Loss: -170916.125000\n",
      "Train Epoch: 31 [16150/17352 (93%)] Loss: -26928.835938\n",
      "Train Epoch: 31 [16963/17352 (98%)] Loss: -65402.027344\n",
      "    epoch          : 31\n",
      "    loss           : -201679.67423447987\n",
      "    val_loss       : -110887.56215515136\n",
      "Train Epoch: 32 [128/17352 (1%)] Loss: -200635.000000\n",
      "Train Epoch: 32 [1536/17352 (9%)] Loss: -214360.343750\n",
      "Train Epoch: 32 [2944/17352 (17%)] Loss: -225757.875000\n",
      "Train Epoch: 32 [4352/17352 (25%)] Loss: -226825.218750\n",
      "Train Epoch: 32 [5760/17352 (33%)] Loss: -209752.140625\n",
      "Train Epoch: 32 [7168/17352 (41%)] Loss: -224949.812500\n",
      "Train Epoch: 32 [8576/17352 (49%)] Loss: -202575.453125\n",
      "Train Epoch: 32 [9984/17352 (58%)] Loss: -199465.125000\n",
      "Train Epoch: 32 [11392/17352 (66%)] Loss: -216571.765625\n",
      "Train Epoch: 32 [12800/17352 (74%)] Loss: -227610.468750\n",
      "Train Epoch: 32 [14208/17352 (82%)] Loss: -216691.265625\n",
      "Train Epoch: 32 [15474/17352 (89%)] Loss: -191372.328125\n",
      "Train Epoch: 32 [16102/17352 (93%)] Loss: -63788.656250\n",
      "Train Epoch: 32 [16874/17352 (97%)] Loss: -137548.125000\n",
      "    epoch          : 32\n",
      "    loss           : -202046.71594156354\n",
      "    val_loss       : -111063.93724772135\n",
      "Train Epoch: 33 [128/17352 (1%)] Loss: -227423.046875\n",
      "Train Epoch: 33 [1536/17352 (9%)] Loss: -212895.515625\n",
      "Train Epoch: 33 [2944/17352 (17%)] Loss: -229255.125000\n",
      "Train Epoch: 33 [4352/17352 (25%)] Loss: -231818.359375\n",
      "Train Epoch: 33 [5760/17352 (33%)] Loss: -227533.734375\n",
      "Train Epoch: 33 [7168/17352 (41%)] Loss: -236147.656250\n",
      "Train Epoch: 33 [8576/17352 (49%)] Loss: -235581.640625\n",
      "Train Epoch: 33 [9984/17352 (58%)] Loss: -200181.250000\n",
      "Train Epoch: 33 [11392/17352 (66%)] Loss: -216485.593750\n",
      "Train Epoch: 33 [12800/17352 (74%)] Loss: -226859.687500\n",
      "Train Epoch: 33 [14208/17352 (82%)] Loss: -233830.375000\n",
      "Train Epoch: 33 [15510/17352 (89%)] Loss: -192997.875000\n",
      "Train Epoch: 33 [16286/17352 (94%)] Loss: -159771.125000\n",
      "Train Epoch: 33 [17010/17352 (98%)] Loss: -189601.406250\n",
      "    epoch          : 33\n",
      "    loss           : -202292.97740142618\n",
      "    val_loss       : -110981.78724060059\n",
      "Train Epoch: 34 [128/17352 (1%)] Loss: -195078.562500\n",
      "Train Epoch: 34 [1536/17352 (9%)] Loss: -229431.265625\n",
      "Train Epoch: 34 [2944/17352 (17%)] Loss: -209410.765625\n",
      "Train Epoch: 34 [4352/17352 (25%)] Loss: -211726.281250\n",
      "Train Epoch: 34 [5760/17352 (33%)] Loss: -230051.359375\n",
      "Train Epoch: 34 [7168/17352 (41%)] Loss: -216274.109375\n",
      "Train Epoch: 34 [8576/17352 (49%)] Loss: -214772.500000\n",
      "Train Epoch: 34 [9984/17352 (58%)] Loss: -197833.312500\n",
      "Train Epoch: 34 [11392/17352 (66%)] Loss: -227143.218750\n",
      "Train Epoch: 34 [12800/17352 (74%)] Loss: -229767.703125\n",
      "Train Epoch: 34 [14208/17352 (82%)] Loss: -219409.968750\n",
      "Train Epoch: 34 [15471/17352 (89%)] Loss: -143265.000000\n",
      "Train Epoch: 34 [16446/17352 (95%)] Loss: -152087.703125\n",
      "Train Epoch: 34 [17047/17352 (98%)] Loss: -27160.150391\n",
      "    epoch          : 34\n",
      "    loss           : -202671.48770448825\n",
      "    val_loss       : -111330.0365661621\n",
      "Train Epoch: 35 [128/17352 (1%)] Loss: -230200.187500\n",
      "Train Epoch: 35 [1536/17352 (9%)] Loss: -218551.406250\n",
      "Train Epoch: 35 [2944/17352 (17%)] Loss: -201918.781250\n",
      "Train Epoch: 35 [4352/17352 (25%)] Loss: -225466.796875\n",
      "Train Epoch: 35 [5760/17352 (33%)] Loss: -228694.453125\n",
      "Train Epoch: 35 [7168/17352 (41%)] Loss: -217970.781250\n",
      "Train Epoch: 35 [8576/17352 (49%)] Loss: -231279.156250\n",
      "Train Epoch: 35 [9984/17352 (58%)] Loss: -213444.093750\n",
      "Train Epoch: 35 [11392/17352 (66%)] Loss: -226661.531250\n",
      "Train Epoch: 35 [12800/17352 (74%)] Loss: -226756.812500\n",
      "Train Epoch: 35 [14208/17352 (82%)] Loss: -231703.515625\n",
      "Train Epoch: 35 [15493/17352 (89%)] Loss: -77699.968750\n",
      "Train Epoch: 35 [16199/17352 (93%)] Loss: -130769.507812\n",
      "Train Epoch: 35 [16952/17352 (98%)] Loss: -63987.015625\n",
      "    epoch          : 35\n",
      "    loss           : -202881.35611105285\n",
      "    val_loss       : -110916.8602183024\n",
      "Train Epoch: 36 [128/17352 (1%)] Loss: -211698.031250\n",
      "Train Epoch: 36 [1536/17352 (9%)] Loss: -231611.500000\n",
      "Train Epoch: 36 [2944/17352 (17%)] Loss: -210735.062500\n",
      "Train Epoch: 36 [4352/17352 (25%)] Loss: -235928.765625\n",
      "Train Epoch: 36 [5760/17352 (33%)] Loss: -213187.125000\n",
      "Train Epoch: 36 [7168/17352 (41%)] Loss: -226278.062500\n",
      "Train Epoch: 36 [8576/17352 (49%)] Loss: -202146.937500\n",
      "Train Epoch: 36 [9984/17352 (58%)] Loss: -229422.593750\n",
      "Train Epoch: 36 [11392/17352 (66%)] Loss: -205019.859375\n",
      "Train Epoch: 36 [12800/17352 (74%)] Loss: -226113.578125\n",
      "Train Epoch: 36 [14208/17352 (82%)] Loss: -235354.484375\n",
      "Train Epoch: 36 [15470/17352 (89%)] Loss: -182191.640625\n",
      "Train Epoch: 36 [16253/17352 (94%)] Loss: -64810.933594\n",
      "Train Epoch: 36 [16976/17352 (98%)] Loss: -154689.000000\n",
      "    epoch          : 36\n",
      "    loss           : -203169.97788970743\n",
      "    val_loss       : -111599.26368611654\n",
      "Train Epoch: 37 [128/17352 (1%)] Loss: -198448.000000\n",
      "Train Epoch: 37 [1536/17352 (9%)] Loss: -211970.718750\n",
      "Train Epoch: 37 [2944/17352 (17%)] Loss: -243892.796875\n",
      "Train Epoch: 37 [4352/17352 (25%)] Loss: -233200.812500\n",
      "Train Epoch: 37 [5760/17352 (33%)] Loss: -224430.312500\n",
      "Train Epoch: 37 [7168/17352 (41%)] Loss: -200228.859375\n",
      "Train Epoch: 37 [8576/17352 (49%)] Loss: -240830.609375\n",
      "Train Epoch: 37 [9984/17352 (58%)] Loss: -202880.187500\n",
      "Train Epoch: 37 [11392/17352 (66%)] Loss: -220183.156250\n",
      "Train Epoch: 37 [12800/17352 (74%)] Loss: -216542.031250\n",
      "Train Epoch: 37 [14208/17352 (82%)] Loss: -217266.906250\n",
      "Train Epoch: 37 [15470/17352 (89%)] Loss: -182657.187500\n",
      "Train Epoch: 37 [16188/17352 (93%)] Loss: -193655.750000\n",
      "Train Epoch: 37 [16992/17352 (98%)] Loss: -83663.437500\n",
      "    epoch          : 37\n",
      "    loss           : -203480.68200437815\n",
      "    val_loss       : -111643.27434590658\n",
      "Train Epoch: 38 [128/17352 (1%)] Loss: -205161.812500\n",
      "Train Epoch: 38 [1536/17352 (9%)] Loss: -235360.468750\n",
      "Train Epoch: 38 [2944/17352 (17%)] Loss: -213038.437500\n",
      "Train Epoch: 38 [4352/17352 (25%)] Loss: -224100.171875\n",
      "Train Epoch: 38 [5760/17352 (33%)] Loss: -219416.531250\n",
      "Train Epoch: 38 [7168/17352 (41%)] Loss: -229284.343750\n",
      "Train Epoch: 38 [8576/17352 (49%)] Loss: -217773.312500\n",
      "Train Epoch: 38 [9984/17352 (58%)] Loss: -216781.546875\n",
      "Train Epoch: 38 [11392/17352 (66%)] Loss: -201988.437500\n",
      "Train Epoch: 38 [12800/17352 (74%)] Loss: -257817.218750\n",
      "Train Epoch: 38 [14208/17352 (82%)] Loss: -243084.343750\n",
      "Train Epoch: 38 [15560/17352 (90%)] Loss: -165757.000000\n",
      "Train Epoch: 38 [16264/17352 (94%)] Loss: -27975.123047\n",
      "Train Epoch: 38 [16925/17352 (98%)] Loss: -9410.166016\n",
      "    epoch          : 38\n",
      "    loss           : -203531.1385112993\n",
      "    val_loss       : -111612.07914326986\n",
      "Train Epoch: 39 [128/17352 (1%)] Loss: -229770.843750\n",
      "Train Epoch: 39 [1536/17352 (9%)] Loss: -217219.796875\n",
      "Train Epoch: 39 [2944/17352 (17%)] Loss: -239162.218750\n",
      "Train Epoch: 39 [4352/17352 (25%)] Loss: -213013.406250\n",
      "Train Epoch: 39 [5760/17352 (33%)] Loss: -213006.375000\n",
      "Train Epoch: 39 [7168/17352 (41%)] Loss: -233843.546875\n",
      "Train Epoch: 39 [8576/17352 (49%)] Loss: -232164.625000\n",
      "Train Epoch: 39 [9984/17352 (58%)] Loss: -226252.109375\n",
      "Train Epoch: 39 [11392/17352 (66%)] Loss: -233425.156250\n",
      "Train Epoch: 39 [12800/17352 (74%)] Loss: -231289.234375\n",
      "Train Epoch: 39 [14208/17352 (82%)] Loss: -232193.781250\n",
      "Train Epoch: 39 [15564/17352 (90%)] Loss: -196426.031250\n",
      "Train Epoch: 39 [16203/17352 (93%)] Loss: -181444.093750\n",
      "Train Epoch: 39 [17093/17352 (99%)] Loss: -64832.242188\n",
      "    epoch          : 39\n",
      "    loss           : -203838.2505800388\n",
      "    val_loss       : -111912.68831278483\n",
      "Train Epoch: 40 [128/17352 (1%)] Loss: -229516.484375\n",
      "Train Epoch: 40 [1536/17352 (9%)] Loss: -217507.656250\n",
      "Train Epoch: 40 [2944/17352 (17%)] Loss: -207064.843750\n",
      "Train Epoch: 40 [4352/17352 (25%)] Loss: -233888.375000\n",
      "Train Epoch: 40 [5760/17352 (33%)] Loss: -205603.843750\n",
      "Train Epoch: 40 [7168/17352 (41%)] Loss: -233543.828125\n",
      "Train Epoch: 40 [8576/17352 (49%)] Loss: -218790.109375\n",
      "Train Epoch: 40 [9984/17352 (58%)] Loss: -229075.468750\n",
      "Train Epoch: 40 [11392/17352 (66%)] Loss: -229903.421875\n",
      "Train Epoch: 40 [12800/17352 (74%)] Loss: -197892.828125\n",
      "Train Epoch: 40 [14208/17352 (82%)] Loss: -211252.562500\n",
      "Train Epoch: 40 [15503/17352 (89%)] Loss: -195998.828125\n",
      "Train Epoch: 40 [16289/17352 (94%)] Loss: -168866.078125\n",
      "Train Epoch: 40 [17110/17352 (99%)] Loss: -62670.179688\n",
      "    epoch          : 40\n",
      "    loss           : -204233.4950516464\n",
      "    val_loss       : -112062.53089497885\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0813_152913/checkpoint-epoch40.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 41 [128/17352 (1%)] Loss: -205641.687500\n",
      "Train Epoch: 41 [1536/17352 (9%)] Loss: -228465.375000\n",
      "Train Epoch: 41 [2944/17352 (17%)] Loss: -218994.875000\n",
      "Train Epoch: 41 [4352/17352 (25%)] Loss: -233909.625000\n",
      "Train Epoch: 41 [5760/17352 (33%)] Loss: -233744.562500\n",
      "Train Epoch: 41 [7168/17352 (41%)] Loss: -256648.031250\n",
      "Train Epoch: 41 [8576/17352 (49%)] Loss: -214761.937500\n",
      "Train Epoch: 41 [9984/17352 (58%)] Loss: -232427.859375\n",
      "Train Epoch: 41 [11392/17352 (66%)] Loss: -238150.187500\n",
      "Train Epoch: 41 [12800/17352 (74%)] Loss: -229023.406250\n",
      "Train Epoch: 41 [14208/17352 (82%)] Loss: -245215.593750\n",
      "Train Epoch: 41 [15468/17352 (89%)] Loss: -85602.640625\n",
      "Train Epoch: 41 [16127/17352 (93%)] Loss: -65277.265625\n",
      "Train Epoch: 41 [17032/17352 (98%)] Loss: -141743.203125\n",
      "    epoch          : 41\n",
      "    loss           : -204371.53703727978\n",
      "    val_loss       : -112044.34778849284\n",
      "Train Epoch: 42 [128/17352 (1%)] Loss: -229103.546875\n",
      "Train Epoch: 42 [1536/17352 (9%)] Loss: -229064.125000\n",
      "Train Epoch: 42 [2944/17352 (17%)] Loss: -239948.218750\n",
      "Train Epoch: 42 [4352/17352 (25%)] Loss: -238032.937500\n",
      "Train Epoch: 42 [5760/17352 (33%)] Loss: -233180.421875\n",
      "Train Epoch: 42 [7168/17352 (41%)] Loss: -211520.656250\n",
      "Train Epoch: 42 [8576/17352 (49%)] Loss: -218592.062500\n",
      "Train Epoch: 42 [9984/17352 (58%)] Loss: -215638.296875\n",
      "Train Epoch: 42 [11392/17352 (66%)] Loss: -229323.312500\n",
      "Train Epoch: 42 [12800/17352 (74%)] Loss: -232116.187500\n",
      "Train Epoch: 42 [14208/17352 (82%)] Loss: -230837.000000\n",
      "Train Epoch: 42 [15570/17352 (90%)] Loss: -192468.593750\n",
      "Train Epoch: 42 [16398/17352 (95%)] Loss: -180791.312500\n",
      "Train Epoch: 42 [17131/17352 (99%)] Loss: -231162.875000\n",
      "    epoch          : 42\n",
      "    loss           : -204126.24247260383\n",
      "    val_loss       : -111887.2221903483\n",
      "Train Epoch: 43 [128/17352 (1%)] Loss: -231062.656250\n",
      "Train Epoch: 43 [1536/17352 (9%)] Loss: -228370.125000\n",
      "Train Epoch: 43 [2944/17352 (17%)] Loss: -205734.390625\n",
      "Train Epoch: 43 [4352/17352 (25%)] Loss: -235645.312500\n",
      "Train Epoch: 43 [5760/17352 (33%)] Loss: -217136.796875\n",
      "Train Epoch: 43 [7168/17352 (41%)] Loss: -204564.218750\n",
      "Train Epoch: 43 [8576/17352 (49%)] Loss: -217621.484375\n",
      "Train Epoch: 43 [9984/17352 (58%)] Loss: -235346.312500\n",
      "Train Epoch: 43 [11392/17352 (66%)] Loss: -218041.250000\n",
      "Train Epoch: 43 [12800/17352 (74%)] Loss: -230431.812500\n",
      "Train Epoch: 43 [14208/17352 (82%)] Loss: -210693.500000\n",
      "Train Epoch: 43 [15532/17352 (90%)] Loss: -166617.046875\n",
      "Train Epoch: 43 [16362/17352 (94%)] Loss: -156876.531250\n",
      "Train Epoch: 43 [17126/17352 (99%)] Loss: -129283.546875\n",
      "    epoch          : 43\n",
      "    loss           : -204341.14607802013\n",
      "    val_loss       : -112110.05300394694\n",
      "Train Epoch: 44 [128/17352 (1%)] Loss: -232956.343750\n",
      "Train Epoch: 44 [1536/17352 (9%)] Loss: -217999.531250\n",
      "Train Epoch: 44 [2944/17352 (17%)] Loss: -219028.703125\n",
      "Train Epoch: 44 [4352/17352 (25%)] Loss: -220820.750000\n",
      "Train Epoch: 44 [5760/17352 (33%)] Loss: -215607.078125\n",
      "Train Epoch: 44 [7168/17352 (41%)] Loss: -230474.937500\n",
      "Train Epoch: 44 [8576/17352 (49%)] Loss: -212640.500000\n",
      "Train Epoch: 44 [9984/17352 (58%)] Loss: -232919.187500\n",
      "Train Epoch: 44 [11392/17352 (66%)] Loss: -229928.125000\n",
      "Train Epoch: 44 [12800/17352 (74%)] Loss: -215428.421875\n",
      "Train Epoch: 44 [14208/17352 (82%)] Loss: -214073.656250\n",
      "Train Epoch: 44 [15531/17352 (90%)] Loss: -168999.875000\n",
      "Train Epoch: 44 [16257/17352 (94%)] Loss: -65181.937500\n",
      "Train Epoch: 44 [17117/17352 (99%)] Loss: -27018.748047\n",
      "    epoch          : 44\n",
      "    loss           : -204796.62596345428\n",
      "    val_loss       : -112238.55699971518\n",
      "Train Epoch: 45 [128/17352 (1%)] Loss: -229773.468750\n",
      "Train Epoch: 45 [1536/17352 (9%)] Loss: -231620.281250\n",
      "Train Epoch: 45 [2944/17352 (17%)] Loss: -214864.468750\n",
      "Train Epoch: 45 [4352/17352 (25%)] Loss: -235346.531250\n",
      "Train Epoch: 45 [5760/17352 (33%)] Loss: -231066.343750\n",
      "Train Epoch: 45 [7168/17352 (41%)] Loss: -217188.593750\n",
      "Train Epoch: 45 [8576/17352 (49%)] Loss: -223247.390625\n",
      "Train Epoch: 45 [9984/17352 (58%)] Loss: -203850.593750\n",
      "Train Epoch: 45 [11392/17352 (66%)] Loss: -205726.671875\n",
      "Train Epoch: 45 [12800/17352 (74%)] Loss: -230008.171875\n",
      "Train Epoch: 45 [14208/17352 (82%)] Loss: -231544.875000\n",
      "Train Epoch: 45 [15440/17352 (89%)] Loss: -131644.531250\n",
      "Train Epoch: 45 [16365/17352 (94%)] Loss: -183403.062500\n",
      "Train Epoch: 45 [17037/17352 (98%)] Loss: -5675.504883\n",
      "    epoch          : 45\n",
      "    loss           : -205016.10433488883\n",
      "    val_loss       : -112448.60585988362\n",
      "Train Epoch: 46 [128/17352 (1%)] Loss: -232259.000000\n",
      "Train Epoch: 46 [1536/17352 (9%)] Loss: -217956.406250\n",
      "Train Epoch: 46 [2944/17352 (17%)] Loss: -257068.671875\n",
      "Train Epoch: 46 [4352/17352 (25%)] Loss: -216637.140625\n",
      "Train Epoch: 46 [5760/17352 (33%)] Loss: -225415.546875\n",
      "Train Epoch: 46 [7168/17352 (41%)] Loss: -205295.296875\n",
      "Train Epoch: 46 [8576/17352 (49%)] Loss: -233786.265625\n",
      "Train Epoch: 46 [9984/17352 (58%)] Loss: -215124.812500\n",
      "Train Epoch: 46 [11392/17352 (66%)] Loss: -228069.546875\n",
      "Train Epoch: 46 [12800/17352 (74%)] Loss: -212084.281250\n",
      "Train Epoch: 46 [14208/17352 (82%)] Loss: -243229.078125\n",
      "Train Epoch: 46 [15440/17352 (89%)] Loss: -5661.579590\n",
      "Train Epoch: 46 [16199/17352 (93%)] Loss: -153954.906250\n",
      "Train Epoch: 46 [16999/17352 (98%)] Loss: -27324.003906\n",
      "    epoch          : 46\n",
      "    loss           : -205259.19606294567\n",
      "    val_loss       : -112541.03577168782\n",
      "Train Epoch: 47 [128/17352 (1%)] Loss: -201936.937500\n",
      "Train Epoch: 47 [1536/17352 (9%)] Loss: -230448.406250\n",
      "Train Epoch: 47 [2944/17352 (17%)] Loss: -232521.734375\n",
      "Train Epoch: 47 [4352/17352 (25%)] Loss: -221508.031250\n",
      "Train Epoch: 47 [5760/17352 (33%)] Loss: -245536.187500\n",
      "Train Epoch: 47 [7168/17352 (41%)] Loss: -239544.062500\n",
      "Train Epoch: 47 [8576/17352 (49%)] Loss: -235645.593750\n",
      "Train Epoch: 47 [9984/17352 (58%)] Loss: -234994.546875\n",
      "Train Epoch: 47 [11392/17352 (66%)] Loss: -211614.078125\n",
      "Train Epoch: 47 [12800/17352 (74%)] Loss: -231555.671875\n",
      "Train Epoch: 47 [14208/17352 (82%)] Loss: -234015.875000\n",
      "Train Epoch: 47 [15565/17352 (90%)] Loss: -166955.390625\n",
      "Train Epoch: 47 [16145/17352 (93%)] Loss: -90778.921875\n",
      "Train Epoch: 47 [16981/17352 (98%)] Loss: -88624.843750\n",
      "    epoch          : 47\n",
      "    loss           : -205250.28130243288\n",
      "    val_loss       : -112411.62477213542\n",
      "Train Epoch: 48 [128/17352 (1%)] Loss: -202453.312500\n",
      "Train Epoch: 48 [1536/17352 (9%)] Loss: -226808.031250\n",
      "Train Epoch: 48 [2944/17352 (17%)] Loss: -221916.937500\n",
      "Train Epoch: 48 [4352/17352 (25%)] Loss: -223273.937500\n",
      "Train Epoch: 48 [5760/17352 (33%)] Loss: -245234.593750\n",
      "Train Epoch: 48 [7168/17352 (41%)] Loss: -204425.250000\n",
      "Train Epoch: 48 [8576/17352 (49%)] Loss: -216967.703125\n",
      "Train Epoch: 48 [9984/17352 (58%)] Loss: -230509.718750\n",
      "Train Epoch: 48 [11392/17352 (66%)] Loss: -206724.875000\n",
      "Train Epoch: 48 [12800/17352 (74%)] Loss: -215869.015625\n",
      "Train Epoch: 48 [14208/17352 (82%)] Loss: -234803.062500\n",
      "Train Epoch: 48 [15553/17352 (90%)] Loss: -133795.953125\n",
      "Train Epoch: 48 [16171/17352 (93%)] Loss: -9228.898438\n",
      "Train Epoch: 48 [16888/17352 (97%)] Loss: -65899.109375\n",
      "    epoch          : 48\n",
      "    loss           : -205434.42418532402\n",
      "    val_loss       : -112504.59470367432\n",
      "Train Epoch: 49 [128/17352 (1%)] Loss: -229682.218750\n",
      "Train Epoch: 49 [1536/17352 (9%)] Loss: -236676.656250\n",
      "Train Epoch: 49 [2944/17352 (17%)] Loss: -217043.875000\n",
      "Train Epoch: 49 [4352/17352 (25%)] Loss: -225774.406250\n",
      "Train Epoch: 49 [5760/17352 (33%)] Loss: -246349.578125\n",
      "Train Epoch: 49 [7168/17352 (41%)] Loss: -221001.593750\n",
      "Train Epoch: 49 [8576/17352 (49%)] Loss: -238063.156250\n",
      "Train Epoch: 49 [9984/17352 (58%)] Loss: -233856.000000\n",
      "Train Epoch: 49 [11392/17352 (66%)] Loss: -224869.468750\n",
      "Train Epoch: 49 [12800/17352 (74%)] Loss: -230874.515625\n",
      "Train Epoch: 49 [14208/17352 (82%)] Loss: -221427.609375\n",
      "Train Epoch: 49 [15573/17352 (90%)] Loss: -233790.171875\n",
      "Train Epoch: 49 [16296/17352 (94%)] Loss: -142167.343750\n",
      "Train Epoch: 49 [17042/17352 (98%)] Loss: -170388.718750\n",
      "    epoch          : 49\n",
      "    loss           : -205594.11381868707\n",
      "    val_loss       : -112669.75623575847\n",
      "Train Epoch: 50 [128/17352 (1%)] Loss: -213521.000000\n",
      "Train Epoch: 50 [1536/17352 (9%)] Loss: -220679.640625\n",
      "Train Epoch: 50 [2944/17352 (17%)] Loss: -220991.968750\n",
      "Train Epoch: 50 [4352/17352 (25%)] Loss: -235556.265625\n",
      "Train Epoch: 50 [5760/17352 (33%)] Loss: -231828.234375\n",
      "Train Epoch: 50 [7168/17352 (41%)] Loss: -238494.953125\n",
      "Train Epoch: 50 [8576/17352 (49%)] Loss: -233699.968750\n",
      "Train Epoch: 50 [9984/17352 (58%)] Loss: -237540.406250\n",
      "Train Epoch: 50 [11392/17352 (66%)] Loss: -232622.593750\n",
      "Train Epoch: 50 [12800/17352 (74%)] Loss: -229492.328125\n",
      "Train Epoch: 50 [14208/17352 (82%)] Loss: -233692.031250\n",
      "Train Epoch: 50 [15538/17352 (90%)] Loss: -146602.656250\n",
      "Train Epoch: 50 [16339/17352 (94%)] Loss: -27118.171875\n",
      "Train Epoch: 50 [16982/17352 (98%)] Loss: -85759.789062\n",
      "    epoch          : 50\n",
      "    loss           : -205725.03008664533\n",
      "    val_loss       : -112610.63759155273\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0813_152913/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [128/17352 (1%)] Loss: -232551.203125\n",
      "Train Epoch: 51 [1536/17352 (9%)] Loss: -230175.156250\n",
      "Train Epoch: 51 [2944/17352 (17%)] Loss: -257873.703125\n",
      "Train Epoch: 51 [4352/17352 (25%)] Loss: -237017.062500\n",
      "Train Epoch: 51 [5760/17352 (33%)] Loss: -230794.593750\n",
      "Train Epoch: 51 [7168/17352 (41%)] Loss: -220048.281250\n",
      "Train Epoch: 51 [8576/17352 (49%)] Loss: -215227.656250\n",
      "Train Epoch: 51 [9984/17352 (58%)] Loss: -233404.843750\n",
      "Train Epoch: 51 [11392/17352 (66%)] Loss: -215127.468750\n",
      "Train Epoch: 51 [12800/17352 (74%)] Loss: -234131.609375\n",
      "Train Epoch: 51 [14208/17352 (82%)] Loss: -219240.812500\n",
      "Train Epoch: 51 [15490/17352 (89%)] Loss: -64606.328125\n",
      "Train Epoch: 51 [16169/17352 (93%)] Loss: -90578.671875\n",
      "Train Epoch: 51 [16885/17352 (97%)] Loss: -168949.078125\n",
      "    epoch          : 51\n",
      "    loss           : -205646.59190174076\n",
      "    val_loss       : -112607.3581741333\n",
      "Train Epoch: 52 [128/17352 (1%)] Loss: -231386.625000\n",
      "Train Epoch: 52 [1536/17352 (9%)] Loss: -231692.468750\n",
      "Train Epoch: 52 [2944/17352 (17%)] Loss: -208530.421875\n",
      "Train Epoch: 52 [4352/17352 (25%)] Loss: -234334.953125\n",
      "Train Epoch: 52 [5760/17352 (33%)] Loss: -206434.078125\n",
      "Train Epoch: 52 [7168/17352 (41%)] Loss: -232484.875000\n",
      "Train Epoch: 52 [8576/17352 (49%)] Loss: -213178.250000\n",
      "Train Epoch: 52 [9984/17352 (58%)] Loss: -215585.109375\n",
      "Train Epoch: 52 [11392/17352 (66%)] Loss: -228739.734375\n",
      "Train Epoch: 52 [12800/17352 (74%)] Loss: -228740.265625\n",
      "Train Epoch: 52 [14208/17352 (82%)] Loss: -221722.687500\n",
      "Train Epoch: 52 [15521/17352 (89%)] Loss: -140281.812500\n",
      "Train Epoch: 52 [16259/17352 (94%)] Loss: -185039.296875\n",
      "Train Epoch: 52 [16994/17352 (98%)] Loss: -24585.126953\n",
      "    epoch          : 52\n",
      "    loss           : -205772.89895527475\n",
      "    val_loss       : -112768.52882029215\n",
      "Train Epoch: 53 [128/17352 (1%)] Loss: -206019.093750\n",
      "Train Epoch: 53 [1536/17352 (9%)] Loss: -229924.906250\n",
      "Train Epoch: 53 [2944/17352 (17%)] Loss: -226329.234375\n",
      "Train Epoch: 53 [4352/17352 (25%)] Loss: -220229.031250\n",
      "Train Epoch: 53 [5760/17352 (33%)] Loss: -216285.281250\n",
      "Train Epoch: 53 [7168/17352 (41%)] Loss: -231817.906250\n",
      "Train Epoch: 53 [8576/17352 (49%)] Loss: -204824.750000\n",
      "Train Epoch: 53 [9984/17352 (58%)] Loss: -234249.343750\n",
      "Train Epoch: 53 [11392/17352 (66%)] Loss: -232430.703125\n",
      "Train Epoch: 53 [12800/17352 (74%)] Loss: -219975.937500\n",
      "Train Epoch: 53 [14208/17352 (82%)] Loss: -237184.359375\n",
      "Train Epoch: 53 [15499/17352 (89%)] Loss: -187264.859375\n",
      "Train Epoch: 53 [16241/17352 (94%)] Loss: -64557.164062\n",
      "Train Epoch: 53 [17002/17352 (98%)] Loss: -142172.312500\n",
      "    epoch          : 53\n",
      "    loss           : -206182.48184511325\n",
      "    val_loss       : -112824.866549174\n",
      "Train Epoch: 54 [128/17352 (1%)] Loss: -232096.906250\n",
      "Train Epoch: 54 [1536/17352 (9%)] Loss: -236052.875000\n",
      "Train Epoch: 54 [2944/17352 (17%)] Loss: -233791.562500\n",
      "Train Epoch: 54 [4352/17352 (25%)] Loss: -229989.437500\n",
      "Train Epoch: 54 [5760/17352 (33%)] Loss: -215476.453125\n",
      "Train Epoch: 54 [7168/17352 (41%)] Loss: -232341.703125\n",
      "Train Epoch: 54 [8576/17352 (49%)] Loss: -234069.562500\n",
      "Train Epoch: 54 [9984/17352 (58%)] Loss: -238699.781250\n",
      "Train Epoch: 54 [11392/17352 (66%)] Loss: -231354.500000\n",
      "Train Epoch: 54 [12800/17352 (74%)] Loss: -223906.984375\n",
      "Train Epoch: 54 [14208/17352 (82%)] Loss: -231764.281250\n",
      "Train Epoch: 54 [15571/17352 (90%)] Loss: -193772.000000\n",
      "Train Epoch: 54 [16268/17352 (94%)] Loss: -196707.562500\n",
      "Train Epoch: 54 [17033/17352 (98%)] Loss: -143616.203125\n",
      "    epoch          : 54\n",
      "    loss           : -205986.470703125\n",
      "    val_loss       : -112658.69677734375\n",
      "Train Epoch: 55 [128/17352 (1%)] Loss: -231598.375000\n",
      "Train Epoch: 55 [1536/17352 (9%)] Loss: -232128.937500\n",
      "Train Epoch: 55 [2944/17352 (17%)] Loss: -222697.343750\n",
      "Train Epoch: 55 [4352/17352 (25%)] Loss: -233737.781250\n",
      "Train Epoch: 55 [5760/17352 (33%)] Loss: -217820.765625\n",
      "Train Epoch: 55 [7168/17352 (41%)] Loss: -259766.531250\n",
      "Train Epoch: 55 [8576/17352 (49%)] Loss: -221246.859375\n",
      "Train Epoch: 55 [9984/17352 (58%)] Loss: -216678.250000\n",
      "Train Epoch: 55 [11392/17352 (66%)] Loss: -226425.218750\n",
      "Train Epoch: 55 [12800/17352 (74%)] Loss: -228594.781250\n",
      "Train Epoch: 55 [14208/17352 (82%)] Loss: -223478.187500\n",
      "Train Epoch: 55 [15528/17352 (89%)] Loss: -134994.640625\n",
      "Train Epoch: 55 [16206/17352 (93%)] Loss: -170134.546875\n",
      "Train Epoch: 55 [16965/17352 (98%)] Loss: -156697.968750\n",
      "    epoch          : 55\n",
      "    loss           : -206208.45362966653\n",
      "    val_loss       : -112939.64040934245\n",
      "Train Epoch: 56 [128/17352 (1%)] Loss: -230174.000000\n",
      "Train Epoch: 56 [1536/17352 (9%)] Loss: -221615.468750\n",
      "Train Epoch: 56 [2944/17352 (17%)] Loss: -239027.875000\n",
      "Train Epoch: 56 [4352/17352 (25%)] Loss: -229764.484375\n",
      "Train Epoch: 56 [5760/17352 (33%)] Loss: -229659.265625\n",
      "Train Epoch: 56 [7168/17352 (41%)] Loss: -236762.984375\n",
      "Train Epoch: 56 [8576/17352 (49%)] Loss: -234045.453125\n",
      "Train Epoch: 56 [9984/17352 (58%)] Loss: -205207.125000\n",
      "Train Epoch: 56 [11392/17352 (66%)] Loss: -216709.781250\n",
      "Train Epoch: 56 [12800/17352 (74%)] Loss: -230179.546875\n",
      "Train Epoch: 56 [14208/17352 (82%)] Loss: -218985.828125\n",
      "Train Epoch: 56 [15545/17352 (90%)] Loss: -167362.468750\n",
      "Train Epoch: 56 [16293/17352 (94%)] Loss: -90807.882812\n",
      "Train Epoch: 56 [17042/17352 (98%)] Loss: -87459.968750\n",
      "    epoch          : 56\n",
      "    loss           : -206317.53646051805\n",
      "    val_loss       : -112792.99059193929\n",
      "Train Epoch: 57 [128/17352 (1%)] Loss: -233302.437500\n",
      "Train Epoch: 57 [1536/17352 (9%)] Loss: -236247.765625\n",
      "Train Epoch: 57 [2944/17352 (17%)] Loss: -231651.625000\n",
      "Train Epoch: 57 [4352/17352 (25%)] Loss: -225978.812500\n",
      "Train Epoch: 57 [5760/17352 (33%)] Loss: -224562.859375\n",
      "Train Epoch: 57 [7168/17352 (41%)] Loss: -231074.312500\n",
      "Train Epoch: 57 [8576/17352 (49%)] Loss: -233319.593750\n",
      "Train Epoch: 57 [9984/17352 (58%)] Loss: -235754.218750\n",
      "Train Epoch: 57 [11392/17352 (66%)] Loss: -232865.968750\n",
      "Train Epoch: 57 [12800/17352 (74%)] Loss: -239808.203125\n",
      "Train Epoch: 57 [14208/17352 (82%)] Loss: -244135.125000\n",
      "Train Epoch: 57 [15541/17352 (90%)] Loss: -167066.296875\n",
      "Train Epoch: 57 [16324/17352 (94%)] Loss: -142425.750000\n",
      "Train Epoch: 57 [17041/17352 (98%)] Loss: -90749.367188\n",
      "    epoch          : 57\n",
      "    loss           : -205740.65588624685\n",
      "    val_loss       : -112997.02323913574\n",
      "Train Epoch: 58 [128/17352 (1%)] Loss: -233896.765625\n",
      "Train Epoch: 58 [1536/17352 (9%)] Loss: -219472.234375\n",
      "Train Epoch: 58 [2944/17352 (17%)] Loss: -217336.000000\n",
      "Train Epoch: 58 [4352/17352 (25%)] Loss: -228408.375000\n",
      "Train Epoch: 58 [5760/17352 (33%)] Loss: -207060.375000\n",
      "Train Epoch: 58 [7168/17352 (41%)] Loss: -220366.515625\n",
      "Train Epoch: 58 [8576/17352 (49%)] Loss: -234096.296875\n",
      "Train Epoch: 58 [9984/17352 (58%)] Loss: -233403.125000\n",
      "Train Epoch: 58 [11392/17352 (66%)] Loss: -220293.656250\n",
      "Train Epoch: 58 [12800/17352 (74%)] Loss: -229797.656250\n",
      "Train Epoch: 58 [14208/17352 (82%)] Loss: -233961.187500\n",
      "Train Epoch: 58 [15419/17352 (89%)] Loss: -9071.642578\n",
      "Train Epoch: 58 [16123/17352 (93%)] Loss: -130048.562500\n",
      "Train Epoch: 58 [16843/17352 (97%)] Loss: -165542.562500\n",
      "    epoch          : 58\n",
      "    loss           : -206402.38235371225\n",
      "    val_loss       : -112908.1202682495\n",
      "Train Epoch: 59 [128/17352 (1%)] Loss: -207344.687500\n",
      "Train Epoch: 59 [1536/17352 (9%)] Loss: -235804.156250\n",
      "Train Epoch: 59 [2944/17352 (17%)] Loss: -210240.031250\n",
      "Train Epoch: 59 [4352/17352 (25%)] Loss: -219698.093750\n",
      "Train Epoch: 59 [5760/17352 (33%)] Loss: -233596.218750\n",
      "Train Epoch: 59 [7168/17352 (41%)] Loss: -208882.328125\n",
      "Train Epoch: 59 [8576/17352 (49%)] Loss: -206775.906250\n",
      "Train Epoch: 59 [9984/17352 (58%)] Loss: -206733.218750\n",
      "Train Epoch: 59 [11392/17352 (66%)] Loss: -222318.687500\n",
      "Train Epoch: 59 [12800/17352 (74%)] Loss: -221081.156250\n",
      "Train Epoch: 59 [14208/17352 (82%)] Loss: -235109.484375\n",
      "Train Epoch: 59 [15525/17352 (89%)] Loss: -129950.757812\n",
      "Train Epoch: 59 [16249/17352 (94%)] Loss: -145991.812500\n",
      "Train Epoch: 59 [16980/17352 (98%)] Loss: -164991.953125\n",
      "    epoch          : 59\n",
      "    loss           : -206394.9773391621\n",
      "    val_loss       : -113000.73838755289\n",
      "Train Epoch: 60 [128/17352 (1%)] Loss: -207926.656250\n",
      "Train Epoch: 60 [1536/17352 (9%)] Loss: -227718.890625\n",
      "Train Epoch: 60 [2944/17352 (17%)] Loss: -211572.828125\n",
      "Train Epoch: 60 [4352/17352 (25%)] Loss: -231473.203125\n",
      "Train Epoch: 60 [5760/17352 (33%)] Loss: -239631.906250\n",
      "Train Epoch: 60 [7168/17352 (41%)] Loss: -219953.734375\n",
      "Train Epoch: 60 [8576/17352 (49%)] Loss: -218686.859375\n",
      "Train Epoch: 60 [9984/17352 (58%)] Loss: -215651.312500\n",
      "Train Epoch: 60 [11392/17352 (66%)] Loss: -221738.437500\n",
      "Train Epoch: 60 [12800/17352 (74%)] Loss: -217802.968750\n",
      "Train Epoch: 60 [14208/17352 (82%)] Loss: -234111.906250\n",
      "Train Epoch: 60 [15528/17352 (89%)] Loss: -133066.250000\n",
      "Train Epoch: 60 [16335/17352 (94%)] Loss: -183548.359375\n",
      "Train Epoch: 60 [16927/17352 (98%)] Loss: -65366.757812\n",
      "    epoch          : 60\n",
      "    loss           : -206756.7972944631\n",
      "    val_loss       : -113077.15777435302\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0813_152913/checkpoint-epoch60.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 61 [128/17352 (1%)] Loss: -233810.343750\n",
      "Train Epoch: 61 [1536/17352 (9%)] Loss: -222060.562500\n",
      "Train Epoch: 61 [2944/17352 (17%)] Loss: -217666.531250\n",
      "Train Epoch: 61 [4352/17352 (25%)] Loss: -217955.265625\n",
      "Train Epoch: 61 [5760/17352 (33%)] Loss: -217652.406250\n",
      "Train Epoch: 61 [7168/17352 (41%)] Loss: -237114.796875\n",
      "Train Epoch: 61 [8576/17352 (49%)] Loss: -245470.828125\n",
      "Train Epoch: 61 [9984/17352 (58%)] Loss: -233289.093750\n",
      "Train Epoch: 61 [11392/17352 (66%)] Loss: -207979.625000\n",
      "Train Epoch: 61 [12800/17352 (74%)] Loss: -231128.421875\n",
      "Train Epoch: 61 [14208/17352 (82%)] Loss: -235020.000000\n",
      "Train Epoch: 61 [15525/17352 (89%)] Loss: -142643.203125\n",
      "Train Epoch: 61 [16103/17352 (93%)] Loss: -5817.633789\n",
      "Train Epoch: 61 [16904/17352 (97%)] Loss: -87569.539062\n",
      "    epoch          : 61\n",
      "    loss           : -206859.85506894925\n",
      "    val_loss       : -113123.630909729\n",
      "Train Epoch: 62 [128/17352 (1%)] Loss: -234794.093750\n",
      "Train Epoch: 62 [1536/17352 (9%)] Loss: -230139.468750\n",
      "Train Epoch: 62 [2944/17352 (17%)] Loss: -219390.312500\n",
      "Train Epoch: 62 [4352/17352 (25%)] Loss: -225791.625000\n",
      "Train Epoch: 62 [5760/17352 (33%)] Loss: -213577.468750\n",
      "Train Epoch: 62 [7168/17352 (41%)] Loss: -216040.046875\n",
      "Train Epoch: 62 [8576/17352 (49%)] Loss: -223553.296875\n",
      "Train Epoch: 62 [9984/17352 (58%)] Loss: -236037.250000\n",
      "Train Epoch: 62 [11392/17352 (66%)] Loss: -233197.843750\n",
      "Train Epoch: 62 [12800/17352 (74%)] Loss: -206324.781250\n",
      "Train Epoch: 62 [14208/17352 (82%)] Loss: -231141.031250\n",
      "Train Epoch: 62 [15415/17352 (89%)] Loss: -85843.070312\n",
      "Train Epoch: 62 [16261/17352 (94%)] Loss: -142995.765625\n",
      "Train Epoch: 62 [17035/17352 (98%)] Loss: -133081.531250\n",
      "    epoch          : 62\n",
      "    loss           : -206966.40355626048\n",
      "    val_loss       : -112982.26081644694\n",
      "Train Epoch: 63 [128/17352 (1%)] Loss: -235847.984375\n",
      "Train Epoch: 63 [1536/17352 (9%)] Loss: -237119.078125\n",
      "Train Epoch: 63 [2944/17352 (17%)] Loss: -217911.546875\n",
      "Train Epoch: 63 [4352/17352 (25%)] Loss: -239490.796875\n",
      "Train Epoch: 63 [5760/17352 (33%)] Loss: -220705.875000\n",
      "Train Epoch: 63 [7168/17352 (41%)] Loss: -233428.781250\n",
      "Train Epoch: 63 [8576/17352 (49%)] Loss: -243357.843750\n",
      "Train Epoch: 63 [9984/17352 (58%)] Loss: -206802.296875\n",
      "Train Epoch: 63 [11392/17352 (66%)] Loss: -207255.968750\n",
      "Train Epoch: 63 [12800/17352 (74%)] Loss: -221282.140625\n",
      "Train Epoch: 63 [14208/17352 (82%)] Loss: -231394.140625\n",
      "Train Epoch: 63 [15478/17352 (89%)] Loss: -141451.468750\n",
      "Train Epoch: 63 [16163/17352 (93%)] Loss: -9306.413086\n",
      "Train Epoch: 63 [16882/17352 (97%)] Loss: -134671.375000\n",
      "    epoch          : 63\n",
      "    loss           : -206907.27550532194\n",
      "    val_loss       : -112650.17039031982\n",
      "Train Epoch: 64 [128/17352 (1%)] Loss: -232707.687500\n",
      "Train Epoch: 64 [1536/17352 (9%)] Loss: -224138.765625\n",
      "Train Epoch: 64 [2944/17352 (17%)] Loss: -216888.875000\n",
      "Train Epoch: 64 [4352/17352 (25%)] Loss: -221359.687500\n",
      "Train Epoch: 64 [5760/17352 (33%)] Loss: -234822.343750\n",
      "Train Epoch: 64 [7168/17352 (41%)] Loss: -204896.843750\n",
      "Train Epoch: 64 [8576/17352 (49%)] Loss: -207749.390625\n",
      "Train Epoch: 64 [9984/17352 (58%)] Loss: -205376.921875\n",
      "Train Epoch: 64 [11392/17352 (66%)] Loss: -211091.421875\n",
      "Train Epoch: 64 [12800/17352 (74%)] Loss: -227886.718750\n",
      "Train Epoch: 64 [14208/17352 (82%)] Loss: -242960.062500\n",
      "Train Epoch: 64 [15489/17352 (89%)] Loss: -129337.812500\n",
      "Train Epoch: 64 [16128/17352 (93%)] Loss: -64618.664062\n",
      "Train Epoch: 64 [17024/17352 (98%)] Loss: -85036.148438\n",
      "    epoch          : 64\n",
      "    loss           : -206777.97070967912\n",
      "    val_loss       : -113036.66234690348\n",
      "Train Epoch: 65 [128/17352 (1%)] Loss: -230579.031250\n",
      "Train Epoch: 65 [1536/17352 (9%)] Loss: -216729.656250\n",
      "Train Epoch: 65 [2944/17352 (17%)] Loss: -235835.812500\n",
      "Train Epoch: 65 [4352/17352 (25%)] Loss: -235192.765625\n",
      "Train Epoch: 65 [5760/17352 (33%)] Loss: -220128.187500\n",
      "Train Epoch: 65 [7168/17352 (41%)] Loss: -224815.593750\n",
      "Train Epoch: 65 [8576/17352 (49%)] Loss: -217664.593750\n",
      "Train Epoch: 65 [9984/17352 (58%)] Loss: -205826.328125\n",
      "Train Epoch: 65 [11392/17352 (66%)] Loss: -222736.312500\n",
      "Train Epoch: 65 [12800/17352 (74%)] Loss: -230833.953125\n",
      "Train Epoch: 65 [14208/17352 (82%)] Loss: -231997.625000\n",
      "Train Epoch: 65 [15541/17352 (90%)] Loss: -170174.296875\n",
      "Train Epoch: 65 [16220/17352 (93%)] Loss: -9647.503906\n",
      "Train Epoch: 65 [17111/17352 (99%)] Loss: -184421.890625\n",
      "    epoch          : 65\n",
      "    loss           : -207041.9688450346\n",
      "    val_loss       : -112826.96288045247\n",
      "Train Epoch: 66 [128/17352 (1%)] Loss: -232237.718750\n",
      "Train Epoch: 66 [1536/17352 (9%)] Loss: -208769.750000\n",
      "Train Epoch: 66 [2944/17352 (17%)] Loss: -243925.421875\n",
      "Train Epoch: 66 [4352/17352 (25%)] Loss: -226997.031250\n",
      "Train Epoch: 66 [5760/17352 (33%)] Loss: -245157.250000\n",
      "Train Epoch: 66 [7168/17352 (41%)] Loss: -258666.687500\n",
      "Train Epoch: 66 [8576/17352 (49%)] Loss: -227198.156250\n",
      "Train Epoch: 66 [9984/17352 (58%)] Loss: -218039.125000\n",
      "Train Epoch: 66 [11392/17352 (66%)] Loss: -211302.875000\n",
      "Train Epoch: 66 [12800/17352 (74%)] Loss: -236926.359375\n",
      "Train Epoch: 66 [14208/17352 (82%)] Loss: -227417.234375\n",
      "Train Epoch: 66 [15399/17352 (89%)] Loss: -9662.589844\n",
      "Train Epoch: 66 [16195/17352 (93%)] Loss: -131259.125000\n",
      "Train Epoch: 66 [16984/17352 (98%)] Loss: -143453.890625\n",
      "    epoch          : 66\n",
      "    loss           : -206847.06846096372\n",
      "    val_loss       : -113256.89327545166\n",
      "Train Epoch: 67 [128/17352 (1%)] Loss: -208203.796875\n",
      "Train Epoch: 67 [1536/17352 (9%)] Loss: -235967.750000\n",
      "Train Epoch: 67 [2944/17352 (17%)] Loss: -239643.718750\n",
      "Train Epoch: 67 [4352/17352 (25%)] Loss: -232664.953125\n",
      "Train Epoch: 67 [5760/17352 (33%)] Loss: -234089.515625\n",
      "Train Epoch: 67 [7168/17352 (41%)] Loss: -240003.578125\n",
      "Train Epoch: 67 [8576/17352 (49%)] Loss: -223629.562500\n",
      "Train Epoch: 67 [9984/17352 (58%)] Loss: -210899.468750\n",
      "Train Epoch: 67 [11392/17352 (66%)] Loss: -208454.421875\n",
      "Train Epoch: 67 [12800/17352 (74%)] Loss: -219868.406250\n",
      "Train Epoch: 67 [14208/17352 (82%)] Loss: -232292.718750\n",
      "Train Epoch: 67 [15478/17352 (89%)] Loss: -67985.601562\n",
      "Train Epoch: 67 [16303/17352 (94%)] Loss: -167225.734375\n",
      "Train Epoch: 67 [16979/17352 (98%)] Loss: -23852.566406\n",
      "    epoch          : 67\n",
      "    loss           : -207286.22173539744\n",
      "    val_loss       : -113165.15331573486\n",
      "Train Epoch: 68 [128/17352 (1%)] Loss: -235441.328125\n",
      "Train Epoch: 68 [1536/17352 (9%)] Loss: -230175.859375\n",
      "Train Epoch: 68 [2944/17352 (17%)] Loss: -225265.250000\n",
      "Train Epoch: 68 [4352/17352 (25%)] Loss: -238634.906250\n",
      "Train Epoch: 68 [5760/17352 (33%)] Loss: -237325.375000\n",
      "Train Epoch: 68 [7168/17352 (41%)] Loss: -235454.281250\n",
      "Train Epoch: 68 [8576/17352 (49%)] Loss: -223021.375000\n",
      "Train Epoch: 68 [9984/17352 (58%)] Loss: -235817.250000\n",
      "Train Epoch: 68 [11392/17352 (66%)] Loss: -223663.843750\n",
      "Train Epoch: 68 [12800/17352 (74%)] Loss: -218013.640625\n",
      "Train Epoch: 68 [14208/17352 (82%)] Loss: -237569.593750\n",
      "Train Epoch: 68 [15487/17352 (89%)] Loss: -65861.406250\n",
      "Train Epoch: 68 [16187/17352 (93%)] Loss: -170170.906250\n",
      "Train Epoch: 68 [17036/17352 (98%)] Loss: -132084.828125\n",
      "    epoch          : 68\n",
      "    loss           : -207333.44004299497\n",
      "    val_loss       : -113168.81079203288\n",
      "Train Epoch: 69 [128/17352 (1%)] Loss: -234241.703125\n",
      "Train Epoch: 69 [1536/17352 (9%)] Loss: -222031.859375\n",
      "Train Epoch: 69 [2944/17352 (17%)] Loss: -215952.468750\n",
      "Train Epoch: 69 [4352/17352 (25%)] Loss: -236452.953125\n",
      "Train Epoch: 69 [5760/17352 (33%)] Loss: -225602.765625\n",
      "Train Epoch: 69 [7168/17352 (41%)] Loss: -237649.812500\n",
      "Train Epoch: 69 [8576/17352 (49%)] Loss: -216630.890625\n",
      "Train Epoch: 69 [9984/17352 (58%)] Loss: -235852.765625\n",
      "Train Epoch: 69 [11392/17352 (66%)] Loss: -224860.734375\n",
      "Train Epoch: 69 [12800/17352 (74%)] Loss: -217779.093750\n",
      "Train Epoch: 69 [14208/17352 (82%)] Loss: -218530.671875\n",
      "Train Epoch: 69 [15427/17352 (89%)] Loss: -91354.406250\n",
      "Train Epoch: 69 [16158/17352 (93%)] Loss: -164425.906250\n",
      "Train Epoch: 69 [16849/17352 (97%)] Loss: -5862.703125\n",
      "    epoch          : 69\n",
      "    loss           : -207427.90053481545\n",
      "    val_loss       : -113334.59890899659\n",
      "Train Epoch: 70 [128/17352 (1%)] Loss: -236175.156250\n",
      "Train Epoch: 70 [1536/17352 (9%)] Loss: -220551.796875\n",
      "Train Epoch: 70 [2944/17352 (17%)] Loss: -223676.140625\n",
      "Train Epoch: 70 [4352/17352 (25%)] Loss: -226540.875000\n",
      "Train Epoch: 70 [5760/17352 (33%)] Loss: -209890.609375\n",
      "Train Epoch: 70 [7168/17352 (41%)] Loss: -204807.609375\n",
      "Train Epoch: 70 [8576/17352 (49%)] Loss: -216292.515625\n",
      "Train Epoch: 70 [9984/17352 (58%)] Loss: -208566.500000\n",
      "Train Epoch: 70 [11392/17352 (66%)] Loss: -220392.406250\n",
      "Train Epoch: 70 [12800/17352 (74%)] Loss: -232040.609375\n",
      "Train Epoch: 70 [14208/17352 (82%)] Loss: -218644.062500\n",
      "Train Epoch: 70 [15546/17352 (90%)] Loss: -171752.203125\n",
      "Train Epoch: 70 [16420/17352 (95%)] Loss: -195942.531250\n",
      "Train Epoch: 70 [17060/17352 (98%)] Loss: -143220.203125\n",
      "    epoch          : 70\n",
      "    loss           : -207543.2513206533\n",
      "    val_loss       : -113350.95887908936\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0813_152913/checkpoint-epoch70.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 71 [128/17352 (1%)] Loss: -207577.671875\n",
      "Train Epoch: 71 [1536/17352 (9%)] Loss: -238441.546875\n",
      "Train Epoch: 71 [2944/17352 (17%)] Loss: -221107.437500\n",
      "Train Epoch: 71 [4352/17352 (25%)] Loss: -218008.625000\n",
      "Train Epoch: 71 [5760/17352 (33%)] Loss: -218414.265625\n",
      "Train Epoch: 71 [7168/17352 (41%)] Loss: -236225.515625\n",
      "Train Epoch: 71 [8576/17352 (49%)] Loss: -222713.593750\n",
      "Train Epoch: 71 [9984/17352 (58%)] Loss: -239492.187500\n",
      "Train Epoch: 71 [11392/17352 (66%)] Loss: -209295.875000\n",
      "Train Epoch: 71 [12800/17352 (74%)] Loss: -226562.875000\n",
      "Train Epoch: 71 [14208/17352 (82%)] Loss: -220916.781250\n",
      "Train Epoch: 71 [15564/17352 (90%)] Loss: -142495.765625\n",
      "Train Epoch: 71 [16258/17352 (94%)] Loss: -127071.812500\n",
      "Train Epoch: 71 [17008/17352 (98%)] Loss: -135266.140625\n",
      "    epoch          : 71\n",
      "    loss           : -207180.59457581796\n",
      "    val_loss       : -113048.77444763183\n",
      "Train Epoch: 72 [128/17352 (1%)] Loss: -206740.343750\n",
      "Train Epoch: 72 [1536/17352 (9%)] Loss: -216605.515625\n",
      "Train Epoch: 72 [2944/17352 (17%)] Loss: -234405.656250\n",
      "Train Epoch: 72 [4352/17352 (25%)] Loss: -237329.906250\n",
      "Train Epoch: 72 [5760/17352 (33%)] Loss: -209840.625000\n",
      "Train Epoch: 72 [7168/17352 (41%)] Loss: -239532.468750\n",
      "Train Epoch: 72 [8576/17352 (49%)] Loss: -210026.343750\n",
      "Train Epoch: 72 [9984/17352 (58%)] Loss: -217836.953125\n",
      "Train Epoch: 72 [11392/17352 (66%)] Loss: -225906.781250\n",
      "Train Epoch: 72 [12800/17352 (74%)] Loss: -230144.812500\n",
      "Train Epoch: 72 [14208/17352 (82%)] Loss: -244957.187500\n",
      "Train Epoch: 72 [15480/17352 (89%)] Loss: -65507.710938\n",
      "Train Epoch: 72 [16209/17352 (93%)] Loss: -85863.468750\n",
      "Train Epoch: 72 [16912/17352 (97%)] Loss: -66116.554688\n",
      "    epoch          : 72\n",
      "    loss           : -207537.4354223469\n",
      "    val_loss       : -113260.71577097574\n",
      "Train Epoch: 73 [128/17352 (1%)] Loss: -235084.468750\n",
      "Train Epoch: 73 [1536/17352 (9%)] Loss: -231615.375000\n",
      "Train Epoch: 73 [2944/17352 (17%)] Loss: -225726.656250\n",
      "Train Epoch: 73 [4352/17352 (25%)] Loss: -231876.765625\n",
      "Train Epoch: 73 [5760/17352 (33%)] Loss: -221438.656250\n",
      "Train Epoch: 73 [7168/17352 (41%)] Loss: -232333.187500\n",
      "Train Epoch: 73 [8576/17352 (49%)] Loss: -258728.593750\n",
      "Train Epoch: 73 [9984/17352 (58%)] Loss: -237209.984375\n",
      "Train Epoch: 73 [11392/17352 (66%)] Loss: -232415.468750\n",
      "Train Epoch: 73 [12800/17352 (74%)] Loss: -215092.375000\n",
      "Train Epoch: 73 [14208/17352 (82%)] Loss: -224415.187500\n",
      "Train Epoch: 73 [15533/17352 (90%)] Loss: -142360.359375\n",
      "Train Epoch: 73 [16278/17352 (94%)] Loss: -234284.218750\n",
      "Train Epoch: 73 [16965/17352 (98%)] Loss: -194558.578125\n",
      "    epoch          : 73\n",
      "    loss           : -207742.45676908558\n",
      "    val_loss       : -113394.13329671224\n",
      "Train Epoch: 74 [128/17352 (1%)] Loss: -233521.234375\n",
      "Train Epoch: 74 [1536/17352 (9%)] Loss: -226845.125000\n",
      "Train Epoch: 74 [2944/17352 (17%)] Loss: -234368.562500\n",
      "Train Epoch: 74 [4352/17352 (25%)] Loss: -240441.906250\n",
      "Train Epoch: 74 [5760/17352 (33%)] Loss: -218350.468750\n",
      "Train Epoch: 74 [7168/17352 (41%)] Loss: -223444.796875\n",
      "Train Epoch: 74 [8576/17352 (49%)] Loss: -215626.468750\n",
      "Train Epoch: 74 [9984/17352 (58%)] Loss: -232696.781250\n",
      "Train Epoch: 74 [11392/17352 (66%)] Loss: -235227.734375\n",
      "Train Epoch: 74 [12800/17352 (74%)] Loss: -214725.562500\n",
      "Train Epoch: 74 [14208/17352 (82%)] Loss: -244332.531250\n",
      "Train Epoch: 74 [15546/17352 (90%)] Loss: -197893.890625\n",
      "Train Epoch: 74 [16221/17352 (93%)] Loss: -167148.218750\n",
      "Train Epoch: 74 [16992/17352 (98%)] Loss: -235097.093750\n",
      "    epoch          : 74\n",
      "    loss           : -207710.8357244914\n",
      "    val_loss       : -113454.11145935059\n",
      "Train Epoch: 75 [128/17352 (1%)] Loss: -233236.203125\n",
      "Train Epoch: 75 [1536/17352 (9%)] Loss: -232998.343750\n",
      "Train Epoch: 75 [2944/17352 (17%)] Loss: -230251.265625\n",
      "Train Epoch: 75 [4352/17352 (25%)] Loss: -221780.375000\n",
      "Train Epoch: 75 [5760/17352 (33%)] Loss: -233938.015625\n",
      "Train Epoch: 75 [7168/17352 (41%)] Loss: -234005.546875\n",
      "Train Epoch: 75 [8576/17352 (49%)] Loss: -246554.281250\n",
      "Train Epoch: 75 [9984/17352 (58%)] Loss: -229712.531250\n",
      "Train Epoch: 75 [11392/17352 (66%)] Loss: -206424.250000\n",
      "Train Epoch: 75 [12800/17352 (74%)] Loss: -224421.781250\n",
      "Train Epoch: 75 [14208/17352 (82%)] Loss: -224512.812500\n",
      "Train Epoch: 75 [15570/17352 (90%)] Loss: -196247.531250\n",
      "Train Epoch: 75 [16241/17352 (94%)] Loss: -137044.953125\n",
      "Train Epoch: 75 [16962/17352 (98%)] Loss: -66046.929688\n",
      "    epoch          : 75\n",
      "    loss           : -207664.28846607593\n",
      "    val_loss       : -113416.22704315186\n",
      "Train Epoch: 76 [128/17352 (1%)] Loss: -233819.500000\n",
      "Train Epoch: 76 [1536/17352 (9%)] Loss: -217805.468750\n",
      "Train Epoch: 76 [2944/17352 (17%)] Loss: -208744.765625\n",
      "Train Epoch: 76 [4352/17352 (25%)] Loss: -224465.640625\n",
      "Train Epoch: 76 [5760/17352 (33%)] Loss: -233474.562500\n",
      "Train Epoch: 76 [7168/17352 (41%)] Loss: -237322.359375\n",
      "Train Epoch: 76 [8576/17352 (49%)] Loss: -226681.468750\n",
      "Train Epoch: 76 [9984/17352 (58%)] Loss: -231980.515625\n",
      "Train Epoch: 76 [11392/17352 (66%)] Loss: -225337.890625\n",
      "Train Epoch: 76 [12800/17352 (74%)] Loss: -226449.171875\n",
      "Train Epoch: 76 [14208/17352 (82%)] Loss: -215728.312500\n",
      "Train Epoch: 76 [15561/17352 (90%)] Loss: -186558.531250\n",
      "Train Epoch: 76 [16245/17352 (94%)] Loss: -24988.339844\n",
      "Train Epoch: 76 [17095/17352 (99%)] Loss: -197446.984375\n",
      "    epoch          : 76\n",
      "    loss           : -207917.63730206585\n",
      "    val_loss       : -113327.54971872966\n",
      "Train Epoch: 77 [128/17352 (1%)] Loss: -235729.375000\n",
      "Train Epoch: 77 [1536/17352 (9%)] Loss: -240486.437500\n",
      "Train Epoch: 77 [2944/17352 (17%)] Loss: -218845.390625\n",
      "Train Epoch: 77 [4352/17352 (25%)] Loss: -240380.062500\n",
      "Train Epoch: 77 [5760/17352 (33%)] Loss: -235808.718750\n",
      "Train Epoch: 77 [7168/17352 (41%)] Loss: -222697.093750\n",
      "Train Epoch: 77 [8576/17352 (49%)] Loss: -234868.078125\n",
      "Train Epoch: 77 [9984/17352 (58%)] Loss: -205797.359375\n",
      "Train Epoch: 77 [11392/17352 (66%)] Loss: -235771.984375\n",
      "Train Epoch: 77 [12800/17352 (74%)] Loss: -221359.000000\n",
      "Train Epoch: 77 [14208/17352 (82%)] Loss: -220764.609375\n",
      "Train Epoch: 77 [15538/17352 (90%)] Loss: -148210.250000\n",
      "Train Epoch: 77 [16247/17352 (94%)] Loss: -91673.468750\n",
      "Train Epoch: 77 [17076/17352 (98%)] Loss: -182128.312500\n",
      "    epoch          : 77\n",
      "    loss           : -207979.37633376152\n",
      "    val_loss       : -113478.2359471639\n",
      "Train Epoch: 78 [128/17352 (1%)] Loss: -233627.593750\n",
      "Train Epoch: 78 [1536/17352 (9%)] Loss: -222870.687500\n",
      "Train Epoch: 78 [2944/17352 (17%)] Loss: -206260.250000\n",
      "Train Epoch: 78 [4352/17352 (25%)] Loss: -237679.593750\n",
      "Train Epoch: 78 [5760/17352 (33%)] Loss: -237146.187500\n",
      "Train Epoch: 78 [7168/17352 (41%)] Loss: -232962.406250\n",
      "Train Epoch: 78 [8576/17352 (49%)] Loss: -224714.484375\n",
      "Train Epoch: 78 [9984/17352 (58%)] Loss: -208270.437500\n",
      "Train Epoch: 78 [11392/17352 (66%)] Loss: -210155.328125\n",
      "Train Epoch: 78 [12800/17352 (74%)] Loss: -230727.968750\n",
      "Train Epoch: 78 [14208/17352 (82%)] Loss: -235375.093750\n",
      "Train Epoch: 78 [15535/17352 (90%)] Loss: -146349.218750\n",
      "Train Epoch: 78 [16145/17352 (93%)] Loss: -182199.500000\n",
      "Train Epoch: 78 [16918/17352 (97%)] Loss: -5642.283203\n",
      "    epoch          : 78\n",
      "    loss           : -208103.42574847944\n",
      "    val_loss       : -113491.26120402018\n",
      "Train Epoch: 79 [128/17352 (1%)] Loss: -234486.750000\n",
      "Train Epoch: 79 [1536/17352 (9%)] Loss: -238384.421875\n",
      "Train Epoch: 79 [2944/17352 (17%)] Loss: -214546.890625\n",
      "Train Epoch: 79 [4352/17352 (25%)] Loss: -236096.265625\n",
      "Train Epoch: 79 [5760/17352 (33%)] Loss: -221869.250000\n",
      "Train Epoch: 79 [7168/17352 (41%)] Loss: -240084.875000\n",
      "Train Epoch: 79 [8576/17352 (49%)] Loss: -216446.250000\n",
      "Train Epoch: 79 [9984/17352 (58%)] Loss: -222589.078125\n",
      "Train Epoch: 79 [11392/17352 (66%)] Loss: -230799.078125\n",
      "Train Epoch: 79 [12800/17352 (74%)] Loss: -230572.859375\n",
      "Train Epoch: 79 [14208/17352 (82%)] Loss: -247407.953125\n",
      "Train Epoch: 79 [15466/17352 (89%)] Loss: -183037.968750\n",
      "Train Epoch: 79 [16209/17352 (93%)] Loss: -157794.203125\n",
      "Train Epoch: 79 [17024/17352 (98%)] Loss: -65480.382812\n",
      "    epoch          : 79\n",
      "    loss           : -208105.99851877097\n",
      "    val_loss       : -113466.60936991374\n",
      "Train Epoch: 80 [128/17352 (1%)] Loss: -232668.109375\n",
      "Train Epoch: 80 [1536/17352 (9%)] Loss: -240660.875000\n",
      "Train Epoch: 80 [2944/17352 (17%)] Loss: -258963.359375\n",
      "Train Epoch: 80 [4352/17352 (25%)] Loss: -231906.656250\n",
      "Train Epoch: 80 [5760/17352 (33%)] Loss: -246025.812500\n",
      "Train Epoch: 80 [7168/17352 (41%)] Loss: -233337.093750\n",
      "Train Epoch: 80 [8576/17352 (49%)] Loss: -218010.265625\n",
      "Train Epoch: 80 [9984/17352 (58%)] Loss: -204915.937500\n",
      "Train Epoch: 80 [11392/17352 (66%)] Loss: -235850.390625\n",
      "Train Epoch: 80 [12800/17352 (74%)] Loss: -227386.625000\n",
      "Train Epoch: 80 [14208/17352 (82%)] Loss: -236766.140625\n",
      "Train Epoch: 80 [15528/17352 (89%)] Loss: -148640.296875\n",
      "Train Epoch: 80 [16302/17352 (94%)] Loss: -169683.781250\n",
      "Train Epoch: 80 [17009/17352 (98%)] Loss: -170781.687500\n",
      "    epoch          : 80\n",
      "    loss           : -208163.44927118288\n",
      "    val_loss       : -113507.9298924764\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0813_152913/checkpoint-epoch80.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 81 [128/17352 (1%)] Loss: -208708.421875\n",
      "Train Epoch: 81 [1536/17352 (9%)] Loss: -239591.000000\n",
      "Train Epoch: 81 [2944/17352 (17%)] Loss: -217395.156250\n",
      "Train Epoch: 81 [4352/17352 (25%)] Loss: -238491.140625\n",
      "Train Epoch: 81 [5760/17352 (33%)] Loss: -234547.125000\n",
      "Train Epoch: 81 [7168/17352 (41%)] Loss: -241742.750000\n",
      "Train Epoch: 81 [8576/17352 (49%)] Loss: -244999.031250\n",
      "Train Epoch: 81 [9984/17352 (58%)] Loss: -230467.906250\n",
      "Train Epoch: 81 [11392/17352 (66%)] Loss: -233772.265625\n",
      "Train Epoch: 81 [12800/17352 (74%)] Loss: -216366.593750\n",
      "Train Epoch: 81 [14208/17352 (82%)] Loss: -221962.625000\n",
      "Train Epoch: 81 [15453/17352 (89%)] Loss: -9216.921875\n",
      "Train Epoch: 81 [16207/17352 (93%)] Loss: -89684.218750\n",
      "Train Epoch: 81 [16941/17352 (98%)] Loss: -27629.621094\n",
      "    epoch          : 81\n",
      "    loss           : -208034.6772362626\n",
      "    val_loss       : -113128.57234649658\n",
      "Train Epoch: 82 [128/17352 (1%)] Loss: -232135.421875\n",
      "Train Epoch: 82 [1536/17352 (9%)] Loss: -223236.093750\n",
      "Train Epoch: 82 [2944/17352 (17%)] Loss: -217492.281250\n",
      "Train Epoch: 82 [4352/17352 (25%)] Loss: -223381.375000\n",
      "Train Epoch: 82 [5760/17352 (33%)] Loss: -226477.812500\n",
      "Train Epoch: 82 [7168/17352 (41%)] Loss: -224734.406250\n",
      "Train Epoch: 82 [8576/17352 (49%)] Loss: -244927.843750\n",
      "Train Epoch: 82 [9984/17352 (58%)] Loss: -236236.046875\n",
      "Train Epoch: 82 [11392/17352 (66%)] Loss: -227945.828125\n",
      "Train Epoch: 82 [12800/17352 (74%)] Loss: -231980.218750\n",
      "Train Epoch: 82 [14208/17352 (82%)] Loss: -236329.781250\n",
      "Train Epoch: 82 [15435/17352 (89%)] Loss: -65303.003906\n",
      "Train Epoch: 82 [16070/17352 (93%)] Loss: -165885.890625\n",
      "Train Epoch: 82 [17009/17352 (98%)] Loss: -148974.375000\n",
      "    epoch          : 82\n",
      "    loss           : -207626.23318215186\n",
      "    val_loss       : -113573.04419606527\n",
      "Train Epoch: 83 [128/17352 (1%)] Loss: -234716.921875\n",
      "Train Epoch: 83 [1536/17352 (9%)] Loss: -224045.546875\n",
      "Train Epoch: 83 [2944/17352 (17%)] Loss: -209233.609375\n",
      "Train Epoch: 83 [4352/17352 (25%)] Loss: -218529.250000\n",
      "Train Epoch: 83 [5760/17352 (33%)] Loss: -234765.093750\n",
      "Train Epoch: 83 [7168/17352 (41%)] Loss: -234637.234375\n",
      "Train Epoch: 83 [8576/17352 (49%)] Loss: -225638.750000\n",
      "Train Epoch: 83 [9984/17352 (58%)] Loss: -221976.203125\n",
      "Train Epoch: 83 [11392/17352 (66%)] Loss: -227781.843750\n",
      "Train Epoch: 83 [12800/17352 (74%)] Loss: -215616.281250\n",
      "Train Epoch: 83 [14208/17352 (82%)] Loss: -226194.390625\n",
      "Train Epoch: 83 [15602/17352 (90%)] Loss: -236233.656250\n",
      "Train Epoch: 83 [16417/17352 (95%)] Loss: -66266.710938\n",
      "Train Epoch: 83 [17016/17352 (98%)] Loss: -129079.859375\n",
      "    epoch          : 83\n",
      "    loss           : -208363.80043388213\n",
      "    val_loss       : -113586.79130859375\n",
      "Train Epoch: 84 [128/17352 (1%)] Loss: -236573.640625\n",
      "Train Epoch: 84 [1536/17352 (9%)] Loss: -234315.484375\n",
      "Train Epoch: 84 [2944/17352 (17%)] Loss: -209781.437500\n",
      "Train Epoch: 84 [4352/17352 (25%)] Loss: -237872.593750\n",
      "Train Epoch: 84 [5760/17352 (33%)] Loss: -237415.781250\n",
      "Train Epoch: 84 [7168/17352 (41%)] Loss: -232188.906250\n",
      "Train Epoch: 84 [8576/17352 (49%)] Loss: -244868.515625\n",
      "Train Epoch: 84 [9984/17352 (58%)] Loss: -220815.921875\n",
      "Train Epoch: 84 [11392/17352 (66%)] Loss: -211940.375000\n",
      "Train Epoch: 84 [12800/17352 (74%)] Loss: -232590.328125\n",
      "Train Epoch: 84 [14208/17352 (82%)] Loss: -225882.265625\n",
      "Train Epoch: 84 [15575/17352 (90%)] Loss: -235452.437500\n",
      "Train Epoch: 84 [16206/17352 (93%)] Loss: -92218.742188\n",
      "Train Epoch: 84 [17016/17352 (98%)] Loss: -134343.156250\n",
      "    epoch          : 84\n",
      "    loss           : -208375.0052793362\n",
      "    val_loss       : -113584.612256368\n",
      "Train Epoch: 85 [128/17352 (1%)] Loss: -236027.421875\n",
      "Train Epoch: 85 [1536/17352 (9%)] Loss: -237318.375000\n",
      "Train Epoch: 85 [2944/17352 (17%)] Loss: -224635.406250\n",
      "Train Epoch: 85 [4352/17352 (25%)] Loss: -228763.953125\n",
      "Train Epoch: 85 [5760/17352 (33%)] Loss: -233465.468750\n",
      "Train Epoch: 85 [7168/17352 (41%)] Loss: -207097.296875\n",
      "Train Epoch: 85 [8576/17352 (49%)] Loss: -237468.593750\n",
      "Train Epoch: 85 [9984/17352 (58%)] Loss: -209846.531250\n",
      "Train Epoch: 85 [11392/17352 (66%)] Loss: -211557.359375\n",
      "Train Epoch: 85 [12800/17352 (74%)] Loss: -222586.953125\n",
      "Train Epoch: 85 [14208/17352 (82%)] Loss: -224691.562500\n",
      "Train Epoch: 85 [15536/17352 (90%)] Loss: -146785.828125\n",
      "Train Epoch: 85 [16237/17352 (94%)] Loss: -89952.328125\n",
      "Train Epoch: 85 [17024/17352 (98%)] Loss: -86764.718750\n",
      "    epoch          : 85\n",
      "    loss           : -208468.68546822568\n",
      "    val_loss       : -113620.98767089844\n",
      "Train Epoch: 86 [128/17352 (1%)] Loss: -226809.234375\n",
      "Train Epoch: 86 [1536/17352 (9%)] Loss: -237709.968750\n",
      "Train Epoch: 86 [2944/17352 (17%)] Loss: -235726.062500\n",
      "Train Epoch: 86 [4352/17352 (25%)] Loss: -240400.406250\n",
      "Train Epoch: 86 [5760/17352 (33%)] Loss: -234198.281250\n",
      "Train Epoch: 86 [7168/17352 (41%)] Loss: -223099.140625\n",
      "Train Epoch: 86 [8576/17352 (49%)] Loss: -235685.843750\n",
      "Train Epoch: 86 [9984/17352 (58%)] Loss: -236613.437500\n",
      "Train Epoch: 86 [11392/17352 (66%)] Loss: -229124.515625\n",
      "Train Epoch: 86 [12800/17352 (74%)] Loss: -217062.234375\n",
      "Train Epoch: 86 [14208/17352 (82%)] Loss: -245157.062500\n",
      "Train Epoch: 86 [15506/17352 (89%)] Loss: -87531.437500\n",
      "Train Epoch: 86 [16402/17352 (95%)] Loss: -149128.468750\n",
      "Train Epoch: 86 [17122/17352 (99%)] Loss: -91778.265625\n",
      "    epoch          : 86\n",
      "    loss           : -208540.76343264995\n",
      "    val_loss       : -113624.70977783203\n",
      "Train Epoch: 87 [128/17352 (1%)] Loss: -231360.484375\n",
      "Train Epoch: 87 [1536/17352 (9%)] Loss: -237723.562500\n",
      "Train Epoch: 87 [2944/17352 (17%)] Loss: -209755.953125\n",
      "Train Epoch: 87 [4352/17352 (25%)] Loss: -224715.640625\n",
      "Train Epoch: 87 [5760/17352 (33%)] Loss: -223363.312500\n",
      "Train Epoch: 87 [7168/17352 (41%)] Loss: -224841.500000\n",
      "Train Epoch: 87 [8576/17352 (49%)] Loss: -238040.734375\n",
      "Train Epoch: 87 [9984/17352 (58%)] Loss: -225159.437500\n",
      "Train Epoch: 87 [11392/17352 (66%)] Loss: -210856.718750\n",
      "Train Epoch: 87 [12800/17352 (74%)] Loss: -217869.625000\n",
      "Train Epoch: 87 [14208/17352 (82%)] Loss: -217610.734375\n",
      "Train Epoch: 87 [15541/17352 (90%)] Loss: -235771.031250\n",
      "Train Epoch: 87 [16271/17352 (94%)] Loss: -148585.437500\n",
      "Train Epoch: 87 [16992/17352 (98%)] Loss: -171284.578125\n",
      "    epoch          : 87\n",
      "    loss           : -208624.39663511954\n",
      "    val_loss       : -113649.36788889566\n",
      "Train Epoch: 88 [128/17352 (1%)] Loss: -232497.015625\n",
      "Train Epoch: 88 [1536/17352 (9%)] Loss: -233680.093750\n",
      "Train Epoch: 88 [2944/17352 (17%)] Loss: -208320.062500\n",
      "Train Epoch: 88 [4352/17352 (25%)] Loss: -238606.500000\n",
      "Train Epoch: 88 [5760/17352 (33%)] Loss: -237814.140625\n",
      "Train Epoch: 88 [7168/17352 (41%)] Loss: -221313.343750\n",
      "Train Epoch: 88 [8576/17352 (49%)] Loss: -209232.781250\n",
      "Train Epoch: 88 [9984/17352 (58%)] Loss: -237798.187500\n",
      "Train Epoch: 88 [11392/17352 (66%)] Loss: -233748.734375\n",
      "Train Epoch: 88 [12800/17352 (74%)] Loss: -245831.718750\n",
      "Train Epoch: 88 [14208/17352 (82%)] Loss: -226972.218750\n",
      "Train Epoch: 88 [15488/17352 (89%)] Loss: -197736.484375\n",
      "Train Epoch: 88 [16439/17352 (95%)] Loss: -187423.968750\n",
      "Train Epoch: 88 [17125/17352 (99%)] Loss: -132680.546875\n",
      "    epoch          : 88\n",
      "    loss           : -208555.8159146917\n",
      "    val_loss       : -113609.8461924235\n",
      "Train Epoch: 89 [128/17352 (1%)] Loss: -233571.578125\n",
      "Train Epoch: 89 [1536/17352 (9%)] Loss: -229163.875000\n",
      "Train Epoch: 89 [2944/17352 (17%)] Loss: -220636.843750\n",
      "Train Epoch: 89 [4352/17352 (25%)] Loss: -218646.031250\n",
      "Train Epoch: 89 [5760/17352 (33%)] Loss: -227624.812500\n",
      "Train Epoch: 89 [7168/17352 (41%)] Loss: -238585.781250\n",
      "Train Epoch: 89 [8576/17352 (49%)] Loss: -220017.218750\n",
      "Train Epoch: 89 [9984/17352 (58%)] Loss: -238071.750000\n",
      "Train Epoch: 89 [11392/17352 (66%)] Loss: -235270.984375\n",
      "Train Epoch: 89 [12800/17352 (74%)] Loss: -231489.421875\n",
      "Train Epoch: 89 [14208/17352 (82%)] Loss: -223120.125000\n",
      "Train Epoch: 89 [15463/17352 (89%)] Loss: -145799.515625\n",
      "Train Epoch: 89 [16183/17352 (93%)] Loss: -168024.671875\n",
      "Train Epoch: 89 [16925/17352 (98%)] Loss: -141031.234375\n",
      "    epoch          : 89\n",
      "    loss           : -208706.7133821833\n",
      "    val_loss       : -113711.2617940267\n",
      "Train Epoch: 90 [128/17352 (1%)] Loss: -210675.968750\n",
      "Train Epoch: 90 [1536/17352 (9%)] Loss: -224822.812500\n",
      "Train Epoch: 90 [2944/17352 (17%)] Loss: -236097.250000\n",
      "Train Epoch: 90 [4352/17352 (25%)] Loss: -223884.125000\n",
      "Train Epoch: 90 [5760/17352 (33%)] Loss: -221039.250000\n",
      "Train Epoch: 90 [7168/17352 (41%)] Loss: -224696.968750\n",
      "Train Epoch: 90 [8576/17352 (49%)] Loss: -220805.437500\n",
      "Train Epoch: 90 [9984/17352 (58%)] Loss: -231465.187500\n",
      "Train Epoch: 90 [11392/17352 (66%)] Loss: -235598.593750\n",
      "Train Epoch: 90 [12800/17352 (74%)] Loss: -234765.750000\n",
      "Train Epoch: 90 [14208/17352 (82%)] Loss: -226318.281250\n",
      "Train Epoch: 90 [15471/17352 (89%)] Loss: -143114.765625\n",
      "Train Epoch: 90 [16293/17352 (94%)] Loss: -79256.578125\n",
      "Train Epoch: 90 [16991/17352 (98%)] Loss: -5533.831055\n",
      "    epoch          : 90\n",
      "    loss           : -208784.49641490143\n",
      "    val_loss       : -113723.36601511638\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0813_152913/checkpoint-epoch90.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 91 [128/17352 (1%)] Loss: -233119.687500\n",
      "Train Epoch: 91 [1536/17352 (9%)] Loss: -226696.593750\n",
      "Train Epoch: 91 [2944/17352 (17%)] Loss: -220034.484375\n",
      "Train Epoch: 91 [4352/17352 (25%)] Loss: -239050.359375\n",
      "Train Epoch: 91 [5760/17352 (33%)] Loss: -222794.109375\n",
      "Train Epoch: 91 [7168/17352 (41%)] Loss: -238729.234375\n",
      "Train Epoch: 91 [8576/17352 (49%)] Loss: -216251.937500\n",
      "Train Epoch: 91 [9984/17352 (58%)] Loss: -217851.187500\n",
      "Train Epoch: 91 [11392/17352 (66%)] Loss: -212420.593750\n",
      "Train Epoch: 91 [12800/17352 (74%)] Loss: -246841.687500\n",
      "Train Epoch: 91 [14208/17352 (82%)] Loss: -220016.375000\n",
      "Train Epoch: 91 [15471/17352 (89%)] Loss: -65082.828125\n",
      "Train Epoch: 91 [16221/17352 (93%)] Loss: -143223.968750\n",
      "Train Epoch: 91 [17030/17352 (98%)] Loss: -170478.281250\n",
      "    epoch          : 91\n",
      "    loss           : -208824.9019242869\n",
      "    val_loss       : -113609.63527018229\n",
      "Train Epoch: 92 [128/17352 (1%)] Loss: -235105.500000\n",
      "Train Epoch: 92 [1536/17352 (9%)] Loss: -236454.328125\n",
      "Train Epoch: 92 [2944/17352 (17%)] Loss: -258656.937500\n",
      "Train Epoch: 92 [4352/17352 (25%)] Loss: -233088.562500\n",
      "Train Epoch: 92 [5760/17352 (33%)] Loss: -247986.546875\n",
      "Train Epoch: 92 [7168/17352 (41%)] Loss: -233484.687500\n",
      "Train Epoch: 92 [8576/17352 (49%)] Loss: -235187.031250\n",
      "Train Epoch: 92 [9984/17352 (58%)] Loss: -209566.421875\n",
      "Train Epoch: 92 [11392/17352 (66%)] Loss: -211785.140625\n",
      "Train Epoch: 92 [12800/17352 (74%)] Loss: -216037.640625\n",
      "Train Epoch: 92 [14208/17352 (82%)] Loss: -246037.937500\n",
      "Train Epoch: 92 [15544/17352 (90%)] Loss: -167247.609375\n",
      "Train Epoch: 92 [16303/17352 (94%)] Loss: -27874.742188\n",
      "Train Epoch: 92 [17050/17352 (98%)] Loss: -64830.164062\n",
      "    epoch          : 92\n",
      "    loss           : -208742.94826512688\n",
      "    val_loss       : -113744.46287841797\n",
      "Train Epoch: 93 [128/17352 (1%)] Loss: -221170.437500\n",
      "Train Epoch: 93 [1536/17352 (9%)] Loss: -223012.906250\n",
      "Train Epoch: 93 [2944/17352 (17%)] Loss: -227343.906250\n",
      "Train Epoch: 93 [4352/17352 (25%)] Loss: -219427.468750\n",
      "Train Epoch: 93 [5760/17352 (33%)] Loss: -224481.015625\n",
      "Train Epoch: 93 [7168/17352 (41%)] Loss: -218380.218750\n",
      "Train Epoch: 93 [8576/17352 (49%)] Loss: -219460.953125\n",
      "Train Epoch: 93 [9984/17352 (58%)] Loss: -220526.062500\n",
      "Train Epoch: 93 [11392/17352 (66%)] Loss: -234193.562500\n",
      "Train Epoch: 93 [12800/17352 (74%)] Loss: -223870.843750\n",
      "Train Epoch: 93 [14208/17352 (82%)] Loss: -234957.218750\n",
      "Train Epoch: 93 [15562/17352 (90%)] Loss: -130796.304688\n",
      "Train Epoch: 93 [16267/17352 (94%)] Loss: -149682.421875\n",
      "Train Epoch: 93 [17019/17352 (98%)] Loss: -186808.093750\n",
      "    epoch          : 93\n",
      "    loss           : -208840.63980573617\n",
      "    val_loss       : -113632.31017506917\n",
      "Train Epoch: 94 [128/17352 (1%)] Loss: -235579.656250\n",
      "Train Epoch: 94 [1536/17352 (9%)] Loss: -222470.765625\n",
      "Train Epoch: 94 [2944/17352 (17%)] Loss: -246819.187500\n",
      "Train Epoch: 94 [4352/17352 (25%)] Loss: -232196.281250\n",
      "Train Epoch: 94 [5760/17352 (33%)] Loss: -237621.187500\n",
      "Train Epoch: 94 [7168/17352 (41%)] Loss: -224961.187500\n",
      "Train Epoch: 94 [8576/17352 (49%)] Loss: -240471.140625\n",
      "Train Epoch: 94 [9984/17352 (58%)] Loss: -236641.000000\n",
      "Train Epoch: 94 [11392/17352 (66%)] Loss: -220062.562500\n",
      "Train Epoch: 94 [12800/17352 (74%)] Loss: -233392.765625\n",
      "Train Epoch: 94 [14208/17352 (82%)] Loss: -220651.062500\n",
      "Train Epoch: 94 [15515/17352 (89%)] Loss: -89570.140625\n",
      "Train Epoch: 94 [16317/17352 (94%)] Loss: -64074.371094\n",
      "Train Epoch: 94 [16955/17352 (98%)] Loss: -152760.218750\n",
      "    epoch          : 94\n",
      "    loss           : -208851.76029650797\n",
      "    val_loss       : -113707.96607462566\n",
      "Train Epoch: 95 [128/17352 (1%)] Loss: -234683.468750\n",
      "Train Epoch: 95 [1536/17352 (9%)] Loss: -235734.671875\n",
      "Train Epoch: 95 [2944/17352 (17%)] Loss: -236769.531250\n",
      "Train Epoch: 95 [4352/17352 (25%)] Loss: -239316.078125\n",
      "Train Epoch: 95 [5760/17352 (33%)] Loss: -235175.718750\n",
      "Train Epoch: 95 [7168/17352 (41%)] Loss: -226061.093750\n",
      "Train Epoch: 95 [8576/17352 (49%)] Loss: -220660.468750\n",
      "Train Epoch: 95 [9984/17352 (58%)] Loss: -219702.546875\n",
      "Train Epoch: 95 [11392/17352 (66%)] Loss: -234630.625000\n",
      "Train Epoch: 95 [12800/17352 (74%)] Loss: -237233.937500\n",
      "Train Epoch: 95 [14208/17352 (82%)] Loss: -225195.437500\n",
      "Train Epoch: 95 [15523/17352 (89%)] Loss: -151388.156250\n",
      "Train Epoch: 95 [16252/17352 (94%)] Loss: -158333.937500\n",
      "Train Epoch: 95 [17006/17352 (98%)] Loss: -169606.359375\n",
      "    epoch          : 95\n",
      "    loss           : -208944.55827259857\n",
      "    val_loss       : -113210.10816040038\n",
      "Train Epoch: 96 [128/17352 (1%)] Loss: -231788.250000\n",
      "Train Epoch: 96 [1536/17352 (9%)] Loss: -221240.125000\n",
      "Train Epoch: 96 [2944/17352 (17%)] Loss: -245927.968750\n",
      "Train Epoch: 96 [4352/17352 (25%)] Loss: -219086.687500\n",
      "Train Epoch: 96 [5760/17352 (33%)] Loss: -219009.562500\n",
      "Train Epoch: 96 [7168/17352 (41%)] Loss: -241316.109375\n",
      "Train Epoch: 96 [8576/17352 (49%)] Loss: -221426.343750\n",
      "Train Epoch: 96 [9984/17352 (58%)] Loss: -222720.843750\n",
      "Train Epoch: 96 [11392/17352 (66%)] Loss: -229123.203125\n",
      "Train Epoch: 96 [12800/17352 (74%)] Loss: -237233.312500\n",
      "Train Epoch: 96 [14208/17352 (82%)] Loss: -224430.937500\n",
      "Train Epoch: 96 [15489/17352 (89%)] Loss: -153680.562500\n",
      "Train Epoch: 96 [16476/17352 (95%)] Loss: -168453.125000\n",
      "Train Epoch: 96 [17099/17352 (99%)] Loss: -5726.999512\n",
      "    epoch          : 96\n",
      "    loss           : -208582.49237429217\n",
      "    val_loss       : -113799.65566151937\n",
      "Train Epoch: 97 [128/17352 (1%)] Loss: -234166.484375\n",
      "Train Epoch: 97 [1536/17352 (9%)] Loss: -238622.625000\n",
      "Train Epoch: 97 [2944/17352 (17%)] Loss: -209819.468750\n",
      "Train Epoch: 97 [4352/17352 (25%)] Loss: -238351.031250\n",
      "Train Epoch: 97 [5760/17352 (33%)] Loss: -237902.609375\n",
      "Train Epoch: 97 [7168/17352 (41%)] Loss: -224720.250000\n",
      "Train Epoch: 97 [8576/17352 (49%)] Loss: -245775.359375\n",
      "Train Epoch: 97 [9984/17352 (58%)] Loss: -241385.281250\n",
      "Train Epoch: 97 [11392/17352 (66%)] Loss: -227640.937500\n",
      "Train Epoch: 97 [12800/17352 (74%)] Loss: -225090.828125\n",
      "Train Epoch: 97 [14208/17352 (82%)] Loss: -220270.218750\n",
      "Train Epoch: 97 [15368/17352 (89%)] Loss: -5641.150879\n",
      "Train Epoch: 97 [16286/17352 (94%)] Loss: -156900.453125\n",
      "Train Epoch: 97 [17074/17352 (98%)] Loss: -65118.453125\n",
      "    epoch          : 97\n",
      "    loss           : -209118.95575647545\n",
      "    val_loss       : -113771.09908498128\n",
      "Train Epoch: 98 [128/17352 (1%)] Loss: -228957.312500\n",
      "Train Epoch: 98 [1536/17352 (9%)] Loss: -225003.296875\n",
      "Train Epoch: 98 [2944/17352 (17%)] Loss: -217409.062500\n",
      "Train Epoch: 98 [4352/17352 (25%)] Loss: -217158.859375\n",
      "Train Epoch: 98 [5760/17352 (33%)] Loss: -227889.546875\n",
      "Train Epoch: 98 [7168/17352 (41%)] Loss: -241141.375000\n",
      "Train Epoch: 98 [8576/17352 (49%)] Loss: -222359.859375\n",
      "Train Epoch: 98 [9984/17352 (58%)] Loss: -208550.718750\n",
      "Train Epoch: 98 [11392/17352 (66%)] Loss: -215409.593750\n",
      "Train Epoch: 98 [12800/17352 (74%)] Loss: -218574.625000\n",
      "Train Epoch: 98 [14208/17352 (82%)] Loss: -232925.750000\n",
      "Train Epoch: 98 [15437/17352 (89%)] Loss: -65827.046875\n",
      "Train Epoch: 98 [16189/17352 (93%)] Loss: -156013.703125\n",
      "Train Epoch: 98 [17048/17352 (98%)] Loss: -65501.390625\n",
      "    epoch          : 98\n",
      "    loss           : -209060.93853227244\n",
      "    val_loss       : -113760.89365488688\n",
      "Train Epoch: 99 [128/17352 (1%)] Loss: -235158.312500\n",
      "Train Epoch: 99 [1536/17352 (9%)] Loss: -224963.187500\n",
      "Train Epoch: 99 [2944/17352 (17%)] Loss: -257966.828125\n",
      "Train Epoch: 99 [4352/17352 (25%)] Loss: -209857.765625\n",
      "Train Epoch: 99 [5760/17352 (33%)] Loss: -229045.906250\n",
      "Train Epoch: 99 [7168/17352 (41%)] Loss: -238031.468750\n",
      "Train Epoch: 99 [8576/17352 (49%)] Loss: -227321.781250\n",
      "Train Epoch: 99 [9984/17352 (58%)] Loss: -233578.906250\n",
      "Train Epoch: 99 [11392/17352 (66%)] Loss: -237855.187500\n",
      "Train Epoch: 99 [12800/17352 (74%)] Loss: -228792.562500\n",
      "Train Epoch: 99 [14208/17352 (82%)] Loss: -246197.843750\n",
      "Train Epoch: 99 [15510/17352 (89%)] Loss: -166850.906250\n",
      "Train Epoch: 99 [16165/17352 (93%)] Loss: -143045.843750\n",
      "Train Epoch: 99 [16980/17352 (98%)] Loss: -134079.187500\n",
      "    epoch          : 99\n",
      "    loss           : -209101.9832378618\n",
      "    val_loss       : -113790.5673365275\n",
      "Train Epoch: 100 [128/17352 (1%)] Loss: -234881.703125\n",
      "Train Epoch: 100 [1536/17352 (9%)] Loss: -225480.859375\n",
      "Train Epoch: 100 [2944/17352 (17%)] Loss: -236347.187500\n",
      "Train Epoch: 100 [4352/17352 (25%)] Loss: -233218.796875\n",
      "Train Epoch: 100 [5760/17352 (33%)] Loss: -246119.718750\n",
      "Train Epoch: 100 [7168/17352 (41%)] Loss: -236760.484375\n",
      "Train Epoch: 100 [8576/17352 (49%)] Loss: -212230.593750\n",
      "Train Epoch: 100 [9984/17352 (58%)] Loss: -223485.515625\n",
      "Train Epoch: 100 [11392/17352 (66%)] Loss: -221631.406250\n",
      "Train Epoch: 100 [12800/17352 (74%)] Loss: -224015.750000\n",
      "Train Epoch: 100 [14208/17352 (82%)] Loss: -226493.000000\n",
      "Train Epoch: 100 [15451/17352 (89%)] Loss: -5608.836914\n",
      "Train Epoch: 100 [16241/17352 (94%)] Loss: -199004.281250\n",
      "Train Epoch: 100 [16971/17352 (98%)] Loss: -79279.617188\n",
      "    epoch          : 100\n",
      "    loss           : -209164.70749003775\n",
      "    val_loss       : -113299.35249735514\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0813_152913/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [128/17352 (1%)] Loss: -233610.406250\n",
      "Train Epoch: 101 [1536/17352 (9%)] Loss: -231829.359375\n",
      "Train Epoch: 101 [2944/17352 (17%)] Loss: -220133.218750\n",
      "Train Epoch: 101 [4352/17352 (25%)] Loss: -238478.156250\n",
      "Train Epoch: 101 [5760/17352 (33%)] Loss: -224460.078125\n",
      "Train Epoch: 101 [7168/17352 (41%)] Loss: -233509.750000\n",
      "Train Epoch: 101 [8576/17352 (49%)] Loss: -220204.687500\n",
      "Train Epoch: 101 [9984/17352 (58%)] Loss: -234829.375000\n",
      "Train Epoch: 101 [11392/17352 (66%)] Loss: -228873.390625\n",
      "Train Epoch: 101 [12800/17352 (74%)] Loss: -221175.765625\n",
      "Train Epoch: 101 [14208/17352 (82%)] Loss: -226609.906250\n",
      "Train Epoch: 101 [15471/17352 (89%)] Loss: -66156.414062\n",
      "Train Epoch: 101 [16234/17352 (94%)] Loss: -88600.750000\n",
      "Train Epoch: 101 [17007/17352 (98%)] Loss: -149570.812500\n",
      "    epoch          : 101\n",
      "    loss           : -208849.0923408662\n",
      "    val_loss       : -113737.3752263387\n",
      "Train Epoch: 102 [128/17352 (1%)] Loss: -230640.437500\n",
      "Train Epoch: 102 [1536/17352 (9%)] Loss: -225978.968750\n",
      "Train Epoch: 102 [2944/17352 (17%)] Loss: -245922.625000\n",
      "Train Epoch: 102 [4352/17352 (25%)] Loss: -234537.312500\n",
      "Train Epoch: 102 [5760/17352 (33%)] Loss: -237442.078125\n",
      "Train Epoch: 102 [7168/17352 (41%)] Loss: -224668.734375\n",
      "Train Epoch: 102 [8576/17352 (49%)] Loss: -221549.687500\n",
      "Train Epoch: 102 [9984/17352 (58%)] Loss: -235472.734375\n",
      "Train Epoch: 102 [11392/17352 (66%)] Loss: -214049.093750\n",
      "Train Epoch: 102 [12800/17352 (74%)] Loss: -232048.890625\n",
      "Train Epoch: 102 [14208/17352 (82%)] Loss: -219191.687500\n",
      "Train Epoch: 102 [15482/17352 (89%)] Loss: -65791.132812\n",
      "Train Epoch: 102 [16242/17352 (94%)] Loss: -79400.140625\n",
      "Train Epoch: 102 [17113/17352 (99%)] Loss: -187887.921875\n",
      "    epoch          : 102\n",
      "    loss           : -209231.86826237416\n",
      "    val_loss       : -113860.35679982504\n",
      "Train Epoch: 103 [128/17352 (1%)] Loss: -234637.890625\n",
      "Train Epoch: 103 [1536/17352 (9%)] Loss: -236851.062500\n",
      "Train Epoch: 103 [2944/17352 (17%)] Loss: -227060.593750\n",
      "Train Epoch: 103 [4352/17352 (25%)] Loss: -223838.796875\n",
      "Train Epoch: 103 [5760/17352 (33%)] Loss: -236699.281250\n",
      "Train Epoch: 103 [7168/17352 (41%)] Loss: -233176.265625\n",
      "Train Epoch: 103 [8576/17352 (49%)] Loss: -217468.812500\n",
      "Train Epoch: 103 [9984/17352 (58%)] Loss: -241130.906250\n",
      "Train Epoch: 103 [11392/17352 (66%)] Loss: -233984.484375\n",
      "Train Epoch: 103 [12800/17352 (74%)] Loss: -220432.296875\n",
      "Train Epoch: 103 [14208/17352 (82%)] Loss: -239711.687500\n",
      "Train Epoch: 103 [15562/17352 (90%)] Loss: -235885.812500\n",
      "Train Epoch: 103 [16345/17352 (94%)] Loss: -171650.171875\n",
      "Train Epoch: 103 [17018/17352 (98%)] Loss: -92733.656250\n",
      "    epoch          : 103\n",
      "    loss           : -209419.2038525063\n",
      "    val_loss       : -113853.08433380126\n",
      "Train Epoch: 104 [128/17352 (1%)] Loss: -208403.781250\n",
      "Train Epoch: 104 [1536/17352 (9%)] Loss: -218792.515625\n",
      "Train Epoch: 104 [2944/17352 (17%)] Loss: -220575.500000\n",
      "Train Epoch: 104 [4352/17352 (25%)] Loss: -230104.687500\n",
      "Train Epoch: 104 [5760/17352 (33%)] Loss: -230530.750000\n",
      "Train Epoch: 104 [7168/17352 (41%)] Loss: -241765.343750\n",
      "Train Epoch: 104 [8576/17352 (49%)] Loss: -246321.093750\n",
      "Train Epoch: 104 [9984/17352 (58%)] Loss: -236133.468750\n",
      "Train Epoch: 104 [11392/17352 (66%)] Loss: -214563.265625\n",
      "Train Epoch: 104 [12800/17352 (74%)] Loss: -233887.562500\n",
      "Train Epoch: 104 [14208/17352 (82%)] Loss: -235875.953125\n",
      "Train Epoch: 104 [15530/17352 (89%)] Loss: -131929.531250\n",
      "Train Epoch: 104 [16254/17352 (94%)] Loss: -171089.484375\n",
      "Train Epoch: 104 [17021/17352 (98%)] Loss: -5364.739258\n",
      "    epoch          : 104\n",
      "    loss           : -209445.97245307258\n",
      "    val_loss       : -113818.39867808024\n",
      "Train Epoch: 105 [128/17352 (1%)] Loss: -234054.796875\n",
      "Train Epoch: 105 [1536/17352 (9%)] Loss: -221789.281250\n",
      "Train Epoch: 105 [2944/17352 (17%)] Loss: -213274.250000\n",
      "Train Epoch: 105 [4352/17352 (25%)] Loss: -234100.500000\n",
      "Train Epoch: 105 [5760/17352 (33%)] Loss: -227949.671875\n",
      "Train Epoch: 105 [7168/17352 (41%)] Loss: -233010.953125\n",
      "Train Epoch: 105 [8576/17352 (49%)] Loss: -257810.578125\n",
      "Train Epoch: 105 [9984/17352 (58%)] Loss: -228644.968750\n",
      "Train Epoch: 105 [11392/17352 (66%)] Loss: -213459.359375\n",
      "Train Epoch: 105 [12800/17352 (74%)] Loss: -247607.625000\n",
      "Train Epoch: 105 [14208/17352 (82%)] Loss: -236475.968750\n",
      "Train Epoch: 105 [15588/17352 (90%)] Loss: -184902.062500\n",
      "Train Epoch: 105 [16228/17352 (94%)] Loss: -139368.984375\n",
      "Train Epoch: 105 [17100/17352 (99%)] Loss: -131758.187500\n",
      "    epoch          : 105\n",
      "    loss           : -209325.99938063652\n",
      "    val_loss       : -113771.51054026285\n",
      "Train Epoch: 106 [128/17352 (1%)] Loss: -236311.031250\n",
      "Train Epoch: 106 [1536/17352 (9%)] Loss: -227708.375000\n",
      "Train Epoch: 106 [2944/17352 (17%)] Loss: -221356.171875\n",
      "Train Epoch: 106 [4352/17352 (25%)] Loss: -234781.250000\n",
      "Train Epoch: 106 [5760/17352 (33%)] Loss: -230310.578125\n",
      "Train Epoch: 106 [7168/17352 (41%)] Loss: -240488.250000\n",
      "Train Epoch: 106 [8576/17352 (49%)] Loss: -222115.000000\n",
      "Train Epoch: 106 [9984/17352 (58%)] Loss: -232977.906250\n",
      "Train Epoch: 106 [11392/17352 (66%)] Loss: -225221.187500\n",
      "Train Epoch: 106 [12800/17352 (74%)] Loss: -231923.156250\n",
      "Train Epoch: 106 [14208/17352 (82%)] Loss: -233220.218750\n",
      "Train Epoch: 106 [15480/17352 (89%)] Loss: -79588.906250\n",
      "Train Epoch: 106 [16470/17352 (95%)] Loss: -149767.687500\n",
      "Train Epoch: 106 [17179/17352 (99%)] Loss: -27982.531250\n",
      "    epoch          : 106\n",
      "    loss           : -209211.92440488675\n",
      "    val_loss       : -113738.2460917155\n",
      "Train Epoch: 107 [128/17352 (1%)] Loss: -234716.250000\n",
      "Train Epoch: 107 [1536/17352 (9%)] Loss: -239613.953125\n",
      "Train Epoch: 107 [2944/17352 (17%)] Loss: -238642.375000\n",
      "Train Epoch: 107 [4352/17352 (25%)] Loss: -232458.406250\n",
      "Train Epoch: 107 [5760/17352 (33%)] Loss: -237907.484375\n",
      "Train Epoch: 107 [7168/17352 (41%)] Loss: -241783.546875\n",
      "Train Epoch: 107 [8576/17352 (49%)] Loss: -245581.031250\n",
      "Train Epoch: 107 [9984/17352 (58%)] Loss: -238203.453125\n",
      "Train Epoch: 107 [11392/17352 (66%)] Loss: -226091.375000\n",
      "Train Epoch: 107 [12800/17352 (74%)] Loss: -225146.375000\n",
      "Train Epoch: 107 [14208/17352 (82%)] Loss: -222846.156250\n",
      "Train Epoch: 107 [15489/17352 (89%)] Loss: -80247.632812\n",
      "Train Epoch: 107 [16196/17352 (93%)] Loss: -168000.968750\n",
      "Train Epoch: 107 [17017/17352 (98%)] Loss: -168904.125000\n",
      "    epoch          : 107\n",
      "    loss           : -209292.33991912229\n",
      "    val_loss       : -113833.20323333741\n",
      "Train Epoch: 108 [128/17352 (1%)] Loss: -236747.093750\n",
      "Train Epoch: 108 [1536/17352 (9%)] Loss: -238309.078125\n",
      "Train Epoch: 108 [2944/17352 (17%)] Loss: -228619.500000\n",
      "Train Epoch: 108 [4352/17352 (25%)] Loss: -234473.937500\n",
      "Train Epoch: 108 [5760/17352 (33%)] Loss: -231279.031250\n",
      "Train Epoch: 108 [7168/17352 (41%)] Loss: -213274.328125\n",
      "Train Epoch: 108 [8576/17352 (49%)] Loss: -223056.453125\n",
      "Train Epoch: 108 [9984/17352 (58%)] Loss: -219796.281250\n",
      "Train Epoch: 108 [11392/17352 (66%)] Loss: -235352.375000\n",
      "Train Epoch: 108 [12800/17352 (74%)] Loss: -233623.000000\n",
      "Train Epoch: 108 [14208/17352 (82%)] Loss: -235692.984375\n",
      "Train Epoch: 108 [15550/17352 (90%)] Loss: -150122.312500\n",
      "Train Epoch: 108 [16319/17352 (94%)] Loss: -86648.304688\n",
      "Train Epoch: 108 [17048/17352 (98%)] Loss: -141815.625000\n",
      "    epoch          : 108\n",
      "    loss           : -209468.9651878408\n",
      "    val_loss       : -113821.22406056723\n",
      "Train Epoch: 109 [128/17352 (1%)] Loss: -214342.937500\n",
      "Train Epoch: 109 [1536/17352 (9%)] Loss: -237957.531250\n",
      "Train Epoch: 109 [2944/17352 (17%)] Loss: -246803.968750\n",
      "Train Epoch: 109 [4352/17352 (25%)] Loss: -226117.031250\n",
      "Train Epoch: 109 [5760/17352 (33%)] Loss: -230586.515625\n",
      "Train Epoch: 109 [7168/17352 (41%)] Loss: -233126.796875\n",
      "Train Epoch: 109 [8576/17352 (49%)] Loss: -225586.015625\n",
      "Train Epoch: 109 [9984/17352 (58%)] Loss: -221497.375000\n",
      "Train Epoch: 109 [11392/17352 (66%)] Loss: -235477.343750\n",
      "Train Epoch: 109 [12800/17352 (74%)] Loss: -218631.234375\n",
      "Train Epoch: 109 [14208/17352 (82%)] Loss: -236964.437500\n",
      "Train Epoch: 109 [15494/17352 (89%)] Loss: -159283.593750\n",
      "Train Epoch: 109 [16277/17352 (94%)] Loss: -24298.199219\n",
      "Train Epoch: 109 [17097/17352 (99%)] Loss: -139818.218750\n",
      "    epoch          : 109\n",
      "    loss           : -209373.25917575503\n",
      "    val_loss       : -113785.8689549764\n",
      "Train Epoch: 110 [128/17352 (1%)] Loss: -211083.734375\n",
      "Train Epoch: 110 [1536/17352 (9%)] Loss: -233240.750000\n",
      "Train Epoch: 110 [2944/17352 (17%)] Loss: -227948.406250\n",
      "Train Epoch: 110 [4352/17352 (25%)] Loss: -221213.093750\n",
      "Train Epoch: 110 [5760/17352 (33%)] Loss: -247975.343750\n",
      "Train Epoch: 110 [7168/17352 (41%)] Loss: -238851.125000\n",
      "Train Epoch: 110 [8576/17352 (49%)] Loss: -228025.984375\n",
      "Train Epoch: 110 [9984/17352 (58%)] Loss: -234307.843750\n",
      "Train Epoch: 110 [11392/17352 (66%)] Loss: -212193.875000\n",
      "Train Epoch: 110 [12800/17352 (74%)] Loss: -218636.250000\n",
      "Train Epoch: 110 [14208/17352 (82%)] Loss: -227578.000000\n",
      "Train Epoch: 110 [15429/17352 (89%)] Loss: -28020.812500\n",
      "Train Epoch: 110 [16292/17352 (94%)] Loss: -171626.250000\n",
      "Train Epoch: 110 [16902/17352 (97%)] Loss: -87915.984375\n",
      "    epoch          : 110\n",
      "    loss           : -209579.63110515414\n",
      "    val_loss       : -113881.35724639893\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0813_152913/checkpoint-epoch110.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 111 [128/17352 (1%)] Loss: -222632.625000\n",
      "Train Epoch: 111 [1536/17352 (9%)] Loss: -237356.890625\n",
      "Train Epoch: 111 [2944/17352 (17%)] Loss: -213925.625000\n",
      "Train Epoch: 111 [4352/17352 (25%)] Loss: -239398.500000\n",
      "Train Epoch: 111 [5760/17352 (33%)] Loss: -221316.984375\n",
      "Train Epoch: 111 [7168/17352 (41%)] Loss: -239983.437500\n",
      "Train Epoch: 111 [8576/17352 (49%)] Loss: -228500.312500\n",
      "Train Epoch: 111 [9984/17352 (58%)] Loss: -229386.359375\n",
      "Train Epoch: 111 [11392/17352 (66%)] Loss: -225908.359375\n",
      "Train Epoch: 111 [12800/17352 (74%)] Loss: -229105.828125\n",
      "Train Epoch: 111 [14208/17352 (82%)] Loss: -235535.000000\n",
      "Train Epoch: 111 [15436/17352 (89%)] Loss: -5367.530762\n",
      "Train Epoch: 111 [16369/17352 (94%)] Loss: -149693.484375\n",
      "Train Epoch: 111 [16986/17352 (98%)] Loss: -138133.984375\n",
      "    epoch          : 111\n",
      "    loss           : -209651.64159828544\n",
      "    val_loss       : -113786.72435658773\n",
      "Train Epoch: 112 [128/17352 (1%)] Loss: -234245.968750\n",
      "Train Epoch: 112 [1536/17352 (9%)] Loss: -233695.843750\n",
      "Train Epoch: 112 [2944/17352 (17%)] Loss: -236665.375000\n",
      "Train Epoch: 112 [4352/17352 (25%)] Loss: -240284.843750\n",
      "Train Epoch: 112 [5760/17352 (33%)] Loss: -228239.625000\n",
      "Train Epoch: 112 [7168/17352 (41%)] Loss: -239860.375000\n",
      "Train Epoch: 112 [8576/17352 (49%)] Loss: -221953.343750\n",
      "Train Epoch: 112 [9984/17352 (58%)] Loss: -210805.359375\n",
      "Train Epoch: 112 [11392/17352 (66%)] Loss: -227276.234375\n",
      "Train Epoch: 112 [12800/17352 (74%)] Loss: -232937.093750\n",
      "Train Epoch: 112 [14208/17352 (82%)] Loss: -245727.703125\n",
      "Train Epoch: 112 [15596/17352 (90%)] Loss: -197143.968750\n",
      "Train Epoch: 112 [16246/17352 (94%)] Loss: -188661.281250\n",
      "Train Epoch: 112 [17043/17352 (98%)] Loss: -87346.679688\n",
      "    epoch          : 112\n",
      "    loss           : -209608.70901059144\n",
      "    val_loss       : -113868.50392710368\n",
      "Train Epoch: 113 [128/17352 (1%)] Loss: -233454.984375\n",
      "Train Epoch: 113 [1536/17352 (9%)] Loss: -219740.453125\n",
      "Train Epoch: 113 [2944/17352 (17%)] Loss: -241802.921875\n",
      "Train Epoch: 113 [4352/17352 (25%)] Loss: -225453.937500\n",
      "Train Epoch: 113 [5760/17352 (33%)] Loss: -229985.546875\n",
      "Train Epoch: 113 [7168/17352 (41%)] Loss: -236095.437500\n",
      "Train Epoch: 113 [8576/17352 (49%)] Loss: -217851.921875\n",
      "Train Epoch: 113 [9984/17352 (58%)] Loss: -234102.156250\n",
      "Train Epoch: 113 [11392/17352 (66%)] Loss: -235635.250000\n",
      "Train Epoch: 113 [12800/17352 (74%)] Loss: -235702.890625\n",
      "Train Epoch: 113 [14208/17352 (82%)] Loss: -245507.968750\n",
      "Train Epoch: 113 [15547/17352 (90%)] Loss: -138801.750000\n",
      "Train Epoch: 113 [16401/17352 (95%)] Loss: -28124.632812\n",
      "Train Epoch: 113 [17026/17352 (98%)] Loss: -9449.575195\n",
      "    epoch          : 113\n",
      "    loss           : -209703.28057164955\n",
      "    val_loss       : -113874.81311340332\n",
      "Train Epoch: 114 [128/17352 (1%)] Loss: -212467.546875\n",
      "Train Epoch: 114 [1536/17352 (9%)] Loss: -229375.625000\n",
      "Train Epoch: 114 [2944/17352 (17%)] Loss: -210610.359375\n",
      "Train Epoch: 114 [4352/17352 (25%)] Loss: -232554.203125\n",
      "Train Epoch: 114 [5760/17352 (33%)] Loss: -230572.843750\n",
      "Train Epoch: 114 [7168/17352 (41%)] Loss: -224174.781250\n",
      "Train Epoch: 114 [8576/17352 (49%)] Loss: -236535.734375\n",
      "Train Epoch: 114 [9984/17352 (58%)] Loss: -210679.109375\n",
      "Train Epoch: 114 [11392/17352 (66%)] Loss: -236074.171875\n",
      "Train Epoch: 114 [12800/17352 (74%)] Loss: -230095.500000\n",
      "Train Epoch: 114 [14208/17352 (82%)] Loss: -233571.546875\n",
      "Train Epoch: 114 [15519/17352 (89%)] Loss: -190866.921875\n",
      "Train Epoch: 114 [16174/17352 (93%)] Loss: -65645.796875\n",
      "Train Epoch: 114 [16961/17352 (98%)] Loss: -88215.390625\n",
      "    epoch          : 114\n",
      "    loss           : -209734.6898496487\n",
      "    val_loss       : -113873.80206553142\n",
      "Train Epoch: 115 [128/17352 (1%)] Loss: -237945.921875\n",
      "Train Epoch: 115 [1536/17352 (9%)] Loss: -226019.406250\n",
      "Train Epoch: 115 [2944/17352 (17%)] Loss: -215000.734375\n",
      "Train Epoch: 115 [4352/17352 (25%)] Loss: -222891.484375\n",
      "Train Epoch: 115 [5760/17352 (33%)] Loss: -213876.796875\n",
      "Train Epoch: 115 [7168/17352 (41%)] Loss: -212066.562500\n",
      "Train Epoch: 115 [8576/17352 (49%)] Loss: -227633.781250\n",
      "Train Epoch: 115 [9984/17352 (58%)] Loss: -208886.906250\n",
      "Train Epoch: 115 [11392/17352 (66%)] Loss: -237720.562500\n",
      "Train Epoch: 115 [12800/17352 (74%)] Loss: -227945.140625\n",
      "Train Epoch: 115 [14208/17352 (82%)] Loss: -238946.578125\n",
      "Train Epoch: 115 [15546/17352 (90%)] Loss: -184804.656250\n",
      "Train Epoch: 115 [16475/17352 (95%)] Loss: -187990.562500\n",
      "Train Epoch: 115 [17007/17352 (98%)] Loss: -9042.580078\n",
      "    epoch          : 115\n",
      "    loss           : -209748.8626651636\n",
      "    val_loss       : -113732.22666829427\n",
      "Train Epoch: 116 [128/17352 (1%)] Loss: -212099.203125\n",
      "Train Epoch: 116 [1536/17352 (9%)] Loss: -223562.968750\n",
      "Train Epoch: 116 [2944/17352 (17%)] Loss: -258546.531250\n",
      "Train Epoch: 116 [4352/17352 (25%)] Loss: -221361.437500\n",
      "Train Epoch: 116 [5760/17352 (33%)] Loss: -229784.468750\n",
      "Train Epoch: 116 [7168/17352 (41%)] Loss: -220945.250000\n",
      "Train Epoch: 116 [8576/17352 (49%)] Loss: -236606.953125\n",
      "Train Epoch: 116 [9984/17352 (58%)] Loss: -239203.781250\n",
      "Train Epoch: 116 [11392/17352 (66%)] Loss: -235449.828125\n",
      "Train Epoch: 116 [12800/17352 (74%)] Loss: -224236.796875\n",
      "Train Epoch: 116 [14208/17352 (82%)] Loss: -232650.718750\n",
      "Train Epoch: 116 [15510/17352 (89%)] Loss: -142387.812500\n",
      "Train Epoch: 116 [16280/17352 (94%)] Loss: -67080.734375\n",
      "Train Epoch: 116 [16980/17352 (98%)] Loss: -85755.203125\n",
      "    epoch          : 116\n",
      "    loss           : -209704.50295590394\n",
      "    val_loss       : -113056.22131551107\n",
      "Train Epoch: 117 [128/17352 (1%)] Loss: -231037.343750\n",
      "Train Epoch: 117 [1536/17352 (9%)] Loss: -224257.765625\n",
      "Train Epoch: 117 [2944/17352 (17%)] Loss: -244892.843750\n",
      "Train Epoch: 117 [4352/17352 (25%)] Loss: -228403.187500\n",
      "Train Epoch: 117 [5760/17352 (33%)] Loss: -234878.812500\n",
      "Train Epoch: 117 [7168/17352 (41%)] Loss: -238342.281250\n",
      "Train Epoch: 117 [8576/17352 (49%)] Loss: -225411.390625\n",
      "Train Epoch: 117 [9984/17352 (58%)] Loss: -241986.156250\n",
      "Train Epoch: 117 [11392/17352 (66%)] Loss: -231025.093750\n",
      "Train Epoch: 117 [12800/17352 (74%)] Loss: -228735.109375\n",
      "Train Epoch: 117 [14208/17352 (82%)] Loss: -237901.968750\n",
      "Train Epoch: 117 [15452/17352 (89%)] Loss: -143638.359375\n",
      "Train Epoch: 117 [16303/17352 (94%)] Loss: -185961.062500\n",
      "Train Epoch: 117 [16995/17352 (98%)] Loss: -156878.718750\n",
      "    epoch          : 117\n",
      "    loss           : -208848.3923356229\n",
      "    val_loss       : -113850.31566416423\n",
      "Train Epoch: 118 [128/17352 (1%)] Loss: -235340.968750\n",
      "Train Epoch: 118 [1536/17352 (9%)] Loss: -237432.578125\n",
      "Train Epoch: 118 [2944/17352 (17%)] Loss: -256388.875000\n",
      "Train Epoch: 118 [4352/17352 (25%)] Loss: -224845.312500\n",
      "Train Epoch: 118 [5760/17352 (33%)] Loss: -246823.218750\n",
      "Train Epoch: 118 [7168/17352 (41%)] Loss: -221540.406250\n",
      "Train Epoch: 118 [8576/17352 (49%)] Loss: -237035.500000\n",
      "Train Epoch: 118 [9984/17352 (58%)] Loss: -235214.515625\n",
      "Train Epoch: 118 [11392/17352 (66%)] Loss: -238475.078125\n",
      "Train Epoch: 118 [12800/17352 (74%)] Loss: -228449.062500\n",
      "Train Epoch: 118 [14208/17352 (82%)] Loss: -228181.765625\n",
      "Train Epoch: 118 [15539/17352 (90%)] Loss: -159004.406250\n",
      "Train Epoch: 118 [16213/17352 (93%)] Loss: -27728.576172\n",
      "Train Epoch: 118 [17032/17352 (98%)] Loss: -144300.125000\n",
      "    epoch          : 118\n",
      "    loss           : -209826.69371001993\n",
      "    val_loss       : -113723.20271046956\n",
      "Train Epoch: 119 [128/17352 (1%)] Loss: -235065.671875\n",
      "Train Epoch: 119 [1536/17352 (9%)] Loss: -233155.359375\n",
      "Train Epoch: 119 [2944/17352 (17%)] Loss: -217518.312500\n",
      "Train Epoch: 119 [4352/17352 (25%)] Loss: -225301.625000\n",
      "Train Epoch: 119 [5760/17352 (33%)] Loss: -221531.015625\n",
      "Train Epoch: 119 [7168/17352 (41%)] Loss: -225640.703125\n",
      "Train Epoch: 119 [8576/17352 (49%)] Loss: -222293.828125\n",
      "Train Epoch: 119 [9984/17352 (58%)] Loss: -234949.218750\n",
      "Train Epoch: 119 [11392/17352 (66%)] Loss: -228261.171875\n",
      "Train Epoch: 119 [12800/17352 (74%)] Loss: -219959.609375\n",
      "Train Epoch: 119 [14208/17352 (82%)] Loss: -219543.578125\n",
      "Train Epoch: 119 [15437/17352 (89%)] Loss: -80623.953125\n",
      "Train Epoch: 119 [16077/17352 (93%)] Loss: -170499.921875\n",
      "Train Epoch: 119 [16951/17352 (98%)] Loss: -235760.734375\n",
      "    epoch          : 119\n",
      "    loss           : -209794.98202862835\n",
      "    val_loss       : -113903.31317036947\n",
      "Train Epoch: 120 [128/17352 (1%)] Loss: -214192.484375\n",
      "Train Epoch: 120 [1536/17352 (9%)] Loss: -236884.125000\n",
      "Train Epoch: 120 [2944/17352 (17%)] Loss: -218717.156250\n",
      "Train Epoch: 120 [4352/17352 (25%)] Loss: -224786.031250\n",
      "Train Epoch: 120 [5760/17352 (33%)] Loss: -235618.921875\n",
      "Train Epoch: 120 [7168/17352 (41%)] Loss: -227758.593750\n",
      "Train Epoch: 120 [8576/17352 (49%)] Loss: -237506.984375\n",
      "Train Epoch: 120 [9984/17352 (58%)] Loss: -216338.640625\n",
      "Train Epoch: 120 [11392/17352 (66%)] Loss: -229362.359375\n",
      "Train Epoch: 120 [12800/17352 (74%)] Loss: -224514.875000\n",
      "Train Epoch: 120 [14208/17352 (82%)] Loss: -222900.703125\n",
      "Train Epoch: 120 [15557/17352 (90%)] Loss: -168724.609375\n",
      "Train Epoch: 120 [16180/17352 (93%)] Loss: -65792.859375\n",
      "Train Epoch: 120 [16984/17352 (98%)] Loss: -197435.156250\n",
      "    epoch          : 120\n",
      "    loss           : -209957.480527737\n",
      "    val_loss       : -113843.0592086792\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0813_152913/checkpoint-epoch120.pth ...\n",
      "Train Epoch: 121 [128/17352 (1%)] Loss: -232039.578125\n",
      "Train Epoch: 121 [1536/17352 (9%)] Loss: -226166.062500\n",
      "Train Epoch: 121 [2944/17352 (17%)] Loss: -230645.937500\n",
      "Train Epoch: 121 [4352/17352 (25%)] Loss: -240465.687500\n",
      "Train Epoch: 121 [5760/17352 (33%)] Loss: -231306.671875\n",
      "Train Epoch: 121 [7168/17352 (41%)] Loss: -235620.281250\n",
      "Train Epoch: 121 [8576/17352 (49%)] Loss: -247571.453125\n",
      "Train Epoch: 121 [9984/17352 (58%)] Loss: -237536.875000\n",
      "Train Epoch: 121 [11392/17352 (66%)] Loss: -212730.453125\n",
      "Train Epoch: 121 [12800/17352 (74%)] Loss: -226032.109375\n",
      "Train Epoch: 121 [14208/17352 (82%)] Loss: -225083.187500\n",
      "Train Epoch: 121 [15414/17352 (89%)] Loss: -24110.343750\n",
      "Train Epoch: 121 [16274/17352 (94%)] Loss: -169332.203125\n",
      "Train Epoch: 121 [17011/17352 (98%)] Loss: -134908.125000\n",
      "    epoch          : 121\n",
      "    loss           : -209836.2027874633\n",
      "    val_loss       : -113676.38243713378\n",
      "Train Epoch: 122 [128/17352 (1%)] Loss: -235643.687500\n",
      "Train Epoch: 122 [1536/17352 (9%)] Loss: -224266.031250\n",
      "Train Epoch: 122 [2944/17352 (17%)] Loss: -220065.843750\n",
      "Train Epoch: 122 [4352/17352 (25%)] Loss: -234417.375000\n",
      "Train Epoch: 122 [5760/17352 (33%)] Loss: -237651.375000\n",
      "Train Epoch: 122 [7168/17352 (41%)] Loss: -233475.125000\n",
      "Train Epoch: 122 [8576/17352 (49%)] Loss: -245231.906250\n",
      "Train Epoch: 122 [9984/17352 (58%)] Loss: -220818.625000\n",
      "Train Epoch: 122 [11392/17352 (66%)] Loss: -227224.984375\n",
      "Train Epoch: 122 [12800/17352 (74%)] Loss: -236259.875000\n",
      "Train Epoch: 122 [14208/17352 (82%)] Loss: -235635.546875\n",
      "Train Epoch: 122 [15458/17352 (89%)] Loss: -5677.017578\n",
      "Train Epoch: 122 [16334/17352 (94%)] Loss: -198970.140625\n",
      "Train Epoch: 122 [17005/17352 (98%)] Loss: -144780.750000\n",
      "    epoch          : 122\n",
      "    loss           : -209919.0200883494\n",
      "    val_loss       : -113789.11548817952\n",
      "Train Epoch: 123 [128/17352 (1%)] Loss: -235689.562500\n",
      "Train Epoch: 123 [1536/17352 (9%)] Loss: -225014.546875\n",
      "Train Epoch: 123 [2944/17352 (17%)] Loss: -224458.703125\n",
      "Train Epoch: 123 [4352/17352 (25%)] Loss: -233564.937500\n",
      "Train Epoch: 123 [5760/17352 (33%)] Loss: -246525.750000\n",
      "Train Epoch: 123 [7168/17352 (41%)] Loss: -233907.406250\n",
      "Train Epoch: 123 [8576/17352 (49%)] Loss: -219987.812500\n",
      "Train Epoch: 123 [9984/17352 (58%)] Loss: -228866.984375\n",
      "Train Epoch: 123 [11392/17352 (66%)] Loss: -225188.062500\n",
      "Train Epoch: 123 [12800/17352 (74%)] Loss: -233091.921875\n",
      "Train Epoch: 123 [14208/17352 (82%)] Loss: -236830.500000\n",
      "Train Epoch: 123 [15416/17352 (89%)] Loss: -68280.156250\n",
      "Train Epoch: 123 [16195/17352 (93%)] Loss: -24232.679688\n",
      "Train Epoch: 123 [16931/17352 (98%)] Loss: -133987.078125\n",
      "    epoch          : 123\n",
      "    loss           : -209742.4269478817\n",
      "    val_loss       : -113845.77505086263\n",
      "Train Epoch: 124 [128/17352 (1%)] Loss: -236151.734375\n",
      "Train Epoch: 124 [1536/17352 (9%)] Loss: -226040.984375\n",
      "Train Epoch: 124 [2944/17352 (17%)] Loss: -220713.468750\n",
      "Train Epoch: 124 [4352/17352 (25%)] Loss: -233747.609375\n",
      "Train Epoch: 124 [5760/17352 (33%)] Loss: -224156.640625\n",
      "Train Epoch: 124 [7168/17352 (41%)] Loss: -241647.531250\n",
      "Train Epoch: 124 [8576/17352 (49%)] Loss: -222711.671875\n",
      "Train Epoch: 124 [9984/17352 (58%)] Loss: -220524.468750\n",
      "Train Epoch: 124 [11392/17352 (66%)] Loss: -236312.453125\n",
      "Train Epoch: 124 [12800/17352 (74%)] Loss: -236660.000000\n",
      "Train Epoch: 124 [14208/17352 (82%)] Loss: -226632.328125\n",
      "Train Epoch: 124 [15467/17352 (89%)] Loss: -134662.796875\n",
      "Train Epoch: 124 [16233/17352 (94%)] Loss: -198685.843750\n",
      "Train Epoch: 124 [16948/17352 (98%)] Loss: -167627.109375\n",
      "    epoch          : 124\n",
      "    loss           : -209993.72450778628\n",
      "    val_loss       : -113805.20825907389\n",
      "Train Epoch: 125 [128/17352 (1%)] Loss: -234284.468750\n",
      "Train Epoch: 125 [1536/17352 (9%)] Loss: -240877.078125\n",
      "Train Epoch: 125 [2944/17352 (17%)] Loss: -213138.437500\n",
      "Train Epoch: 125 [4352/17352 (25%)] Loss: -227164.218750\n",
      "Train Epoch: 125 [5760/17352 (33%)] Loss: -246418.015625\n",
      "Train Epoch: 125 [7168/17352 (41%)] Loss: -225908.687500\n",
      "Train Epoch: 125 [8576/17352 (49%)] Loss: -236932.562500\n",
      "Train Epoch: 125 [9984/17352 (58%)] Loss: -241691.968750\n",
      "Train Epoch: 125 [11392/17352 (66%)] Loss: -215879.015625\n",
      "Train Epoch: 125 [12800/17352 (74%)] Loss: -233368.031250\n",
      "Train Epoch: 125 [14208/17352 (82%)] Loss: -237536.750000\n",
      "Train Epoch: 125 [15493/17352 (89%)] Loss: -80358.578125\n",
      "Train Epoch: 125 [16275/17352 (94%)] Loss: -9243.268555\n",
      "Train Epoch: 125 [17013/17352 (98%)] Loss: -133061.687500\n",
      "    epoch          : 125\n",
      "    loss           : -210034.89661545723\n",
      "    val_loss       : -113891.6901687622\n",
      "Train Epoch: 126 [128/17352 (1%)] Loss: -212915.953125\n",
      "Train Epoch: 126 [1536/17352 (9%)] Loss: -239230.312500\n",
      "Train Epoch: 126 [2944/17352 (17%)] Loss: -213587.218750\n",
      "Train Epoch: 126 [4352/17352 (25%)] Loss: -212757.812500\n",
      "Train Epoch: 126 [5760/17352 (33%)] Loss: -247174.546875\n",
      "Train Epoch: 126 [7168/17352 (41%)] Loss: -228459.656250\n",
      "Train Epoch: 126 [8576/17352 (49%)] Loss: -225096.312500\n",
      "Train Epoch: 126 [9984/17352 (58%)] Loss: -220797.921875\n",
      "Train Epoch: 126 [11392/17352 (66%)] Loss: -237167.500000\n",
      "Train Epoch: 126 [12800/17352 (74%)] Loss: -237100.312500\n",
      "Train Epoch: 126 [14208/17352 (82%)] Loss: -234135.812500\n",
      "Train Epoch: 126 [15510/17352 (89%)] Loss: -89647.734375\n",
      "Train Epoch: 126 [16226/17352 (94%)] Loss: -79646.984375\n",
      "Train Epoch: 126 [17081/17352 (98%)] Loss: -171231.937500\n",
      "    epoch          : 126\n",
      "    loss           : -210108.36043676594\n",
      "    val_loss       : -113807.27000071207\n",
      "Train Epoch: 127 [128/17352 (1%)] Loss: -238644.578125\n",
      "Train Epoch: 127 [1536/17352 (9%)] Loss: -223804.625000\n",
      "Train Epoch: 127 [2944/17352 (17%)] Loss: -219390.875000\n",
      "Train Epoch: 127 [4352/17352 (25%)] Loss: -220449.531250\n",
      "Train Epoch: 127 [5760/17352 (33%)] Loss: -247670.437500\n",
      "Train Epoch: 127 [7168/17352 (41%)] Loss: -234377.875000\n",
      "Train Epoch: 127 [8576/17352 (49%)] Loss: -257296.062500\n",
      "Train Epoch: 127 [9984/17352 (58%)] Loss: -230962.484375\n",
      "Train Epoch: 127 [11392/17352 (66%)] Loss: -236912.281250\n",
      "Train Epoch: 127 [12800/17352 (74%)] Loss: -228915.359375\n",
      "Train Epoch: 127 [14208/17352 (82%)] Loss: -218909.125000\n",
      "Train Epoch: 127 [15527/17352 (89%)] Loss: -142385.828125\n",
      "Train Epoch: 127 [16353/17352 (94%)] Loss: -172504.500000\n",
      "Train Epoch: 127 [17013/17352 (98%)] Loss: -5701.322266\n",
      "    epoch          : 127\n",
      "    loss           : -210027.9679635067\n",
      "    val_loss       : -113832.95343017578\n",
      "Train Epoch: 128 [128/17352 (1%)] Loss: -213668.718750\n",
      "Train Epoch: 128 [1536/17352 (9%)] Loss: -235262.187500\n",
      "Train Epoch: 128 [2944/17352 (17%)] Loss: -241457.734375\n",
      "Train Epoch: 128 [4352/17352 (25%)] Loss: -238019.171875\n",
      "Train Epoch: 128 [5760/17352 (33%)] Loss: -228860.625000\n",
      "Train Epoch: 128 [7168/17352 (41%)] Loss: -214177.468750\n",
      "Train Epoch: 128 [8576/17352 (49%)] Loss: -237614.093750\n",
      "Train Epoch: 128 [9984/17352 (58%)] Loss: -222724.656250\n",
      "Train Epoch: 128 [11392/17352 (66%)] Loss: -212825.796875\n",
      "Train Epoch: 128 [12800/17352 (74%)] Loss: -220899.578125\n",
      "Train Epoch: 128 [14208/17352 (82%)] Loss: -247708.843750\n",
      "Train Epoch: 128 [15488/17352 (89%)] Loss: -150516.312500\n",
      "Train Epoch: 128 [16354/17352 (94%)] Loss: -87444.945312\n",
      "Train Epoch: 128 [16957/17352 (98%)] Loss: -69538.484375\n",
      "    epoch          : 128\n",
      "    loss           : -210153.51187604867\n",
      "    val_loss       : -113720.27342173258\n",
      "Train Epoch: 129 [128/17352 (1%)] Loss: -237550.031250\n",
      "Train Epoch: 129 [1536/17352 (9%)] Loss: -239653.125000\n",
      "Train Epoch: 129 [2944/17352 (17%)] Loss: -212168.343750\n",
      "Train Epoch: 129 [4352/17352 (25%)] Loss: -227650.531250\n",
      "Train Epoch: 129 [5760/17352 (33%)] Loss: -235634.921875\n",
      "Train Epoch: 129 [7168/17352 (41%)] Loss: -226137.375000\n",
      "Train Epoch: 129 [8576/17352 (49%)] Loss: -223811.093750\n",
      "Train Epoch: 129 [9984/17352 (58%)] Loss: -211373.250000\n",
      "Train Epoch: 129 [11392/17352 (66%)] Loss: -228882.328125\n",
      "Train Epoch: 129 [12800/17352 (74%)] Loss: -222523.203125\n",
      "Train Epoch: 129 [14208/17352 (82%)] Loss: -240039.843750\n",
      "Train Epoch: 129 [15519/17352 (89%)] Loss: -133187.843750\n",
      "Train Epoch: 129 [16193/17352 (93%)] Loss: -168741.500000\n",
      "Train Epoch: 129 [17027/17352 (98%)] Loss: -5681.392578\n",
      "    epoch          : 129\n",
      "    loss           : -209937.14847682466\n",
      "    val_loss       : -113759.71629740397\n",
      "Train Epoch: 130 [128/17352 (1%)] Loss: -232320.187500\n",
      "Train Epoch: 130 [1536/17352 (9%)] Loss: -233830.156250\n",
      "Train Epoch: 130 [2944/17352 (17%)] Loss: -228817.781250\n",
      "Train Epoch: 130 [4352/17352 (25%)] Loss: -227684.312500\n",
      "Train Epoch: 130 [5760/17352 (33%)] Loss: -246711.250000\n",
      "Train Epoch: 130 [7168/17352 (41%)] Loss: -226042.062500\n",
      "Train Epoch: 130 [8576/17352 (49%)] Loss: -246388.296875\n",
      "Train Epoch: 130 [9984/17352 (58%)] Loss: -209920.968750\n",
      "Train Epoch: 130 [11392/17352 (66%)] Loss: -233523.890625\n",
      "Train Epoch: 130 [12800/17352 (74%)] Loss: -238018.000000\n",
      "Train Epoch: 130 [14208/17352 (82%)] Loss: -238876.406250\n",
      "Train Epoch: 130 [15564/17352 (90%)] Loss: -169967.484375\n",
      "Train Epoch: 130 [16248/17352 (94%)] Loss: -197267.593750\n",
      "Train Epoch: 130 [17072/17352 (98%)] Loss: -171191.437500\n",
      "    epoch          : 130\n",
      "    loss           : -209703.58806758598\n",
      "    val_loss       : -113771.91231282552\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0813_152913/checkpoint-epoch130.pth ...\n",
      "Train Epoch: 131 [128/17352 (1%)] Loss: -237862.781250\n",
      "Train Epoch: 131 [1536/17352 (9%)] Loss: -234208.453125\n",
      "Train Epoch: 131 [2944/17352 (17%)] Loss: -241064.140625\n",
      "Train Epoch: 131 [4352/17352 (25%)] Loss: -228660.843750\n",
      "Train Epoch: 131 [5760/17352 (33%)] Loss: -236199.375000\n",
      "Train Epoch: 131 [7168/17352 (41%)] Loss: -241300.343750\n",
      "Train Epoch: 131 [8576/17352 (49%)] Loss: -212638.828125\n",
      "Train Epoch: 131 [9984/17352 (58%)] Loss: -236224.203125\n",
      "Train Epoch: 131 [11392/17352 (66%)] Loss: -233787.000000\n",
      "Train Epoch: 131 [12800/17352 (74%)] Loss: -238744.640625\n",
      "Train Epoch: 131 [14208/17352 (82%)] Loss: -238580.078125\n",
      "Train Epoch: 131 [15448/17352 (89%)] Loss: -89551.164062\n",
      "Train Epoch: 131 [16066/17352 (93%)] Loss: -68875.625000\n",
      "Train Epoch: 131 [16868/17352 (97%)] Loss: -148181.296875\n",
      "    epoch          : 131\n",
      "    loss           : -210130.27927065856\n",
      "    val_loss       : -113822.94283650717\n",
      "Train Epoch: 132 [128/17352 (1%)] Loss: -230310.328125\n",
      "Train Epoch: 132 [1536/17352 (9%)] Loss: -231726.656250\n",
      "Train Epoch: 132 [2944/17352 (17%)] Loss: -215280.281250\n",
      "Train Epoch: 132 [4352/17352 (25%)] Loss: -234593.843750\n",
      "Train Epoch: 132 [5760/17352 (33%)] Loss: -228349.234375\n",
      "Train Epoch: 132 [7168/17352 (41%)] Loss: -240446.843750\n",
      "Train Epoch: 132 [8576/17352 (49%)] Loss: -229141.218750\n",
      "Train Epoch: 132 [9984/17352 (58%)] Loss: -236768.531250\n",
      "Train Epoch: 132 [11392/17352 (66%)] Loss: -228901.625000\n",
      "Train Epoch: 132 [12800/17352 (74%)] Loss: -220155.390625\n",
      "Train Epoch: 132 [14208/17352 (82%)] Loss: -239075.875000\n",
      "Train Epoch: 132 [15407/17352 (89%)] Loss: -66178.492188\n",
      "Train Epoch: 132 [16278/17352 (94%)] Loss: -152929.812500\n",
      "Train Epoch: 132 [16999/17352 (98%)] Loss: -68573.289062\n",
      "    epoch          : 132\n",
      "    loss           : -210101.42609912436\n",
      "    val_loss       : -113899.2684173584\n",
      "Train Epoch: 133 [128/17352 (1%)] Loss: -238763.156250\n",
      "Train Epoch: 133 [1536/17352 (9%)] Loss: -240209.281250\n",
      "Train Epoch: 133 [2944/17352 (17%)] Loss: -215370.984375\n",
      "Train Epoch: 133 [4352/17352 (25%)] Loss: -215213.312500\n",
      "Train Epoch: 133 [5760/17352 (33%)] Loss: -229278.281250\n",
      "Train Epoch: 133 [7168/17352 (41%)] Loss: -241478.937500\n",
      "Train Epoch: 133 [8576/17352 (49%)] Loss: -226053.562500\n",
      "Train Epoch: 133 [9984/17352 (58%)] Loss: -221186.500000\n",
      "Train Epoch: 133 [11392/17352 (66%)] Loss: -236369.453125\n",
      "Train Epoch: 133 [12800/17352 (74%)] Loss: -223710.593750\n",
      "Train Epoch: 133 [14208/17352 (82%)] Loss: -237030.781250\n",
      "Train Epoch: 133 [15491/17352 (89%)] Loss: -79408.078125\n",
      "Train Epoch: 133 [16335/17352 (94%)] Loss: -67581.882812\n",
      "Train Epoch: 133 [16954/17352 (98%)] Loss: -28272.595703\n",
      "    epoch          : 133\n",
      "    loss           : -210163.90644662332\n",
      "    val_loss       : -113872.81619669596\n",
      "Train Epoch: 134 [128/17352 (1%)] Loss: -214369.937500\n",
      "Train Epoch: 134 [1536/17352 (9%)] Loss: -238357.546875\n",
      "Train Epoch: 134 [2944/17352 (17%)] Loss: -214038.593750\n",
      "Train Epoch: 134 [4352/17352 (25%)] Loss: -229245.312500\n",
      "Train Epoch: 134 [5760/17352 (33%)] Loss: -246562.390625\n",
      "Train Epoch: 134 [7168/17352 (41%)] Loss: -236018.656250\n",
      "Train Epoch: 134 [8576/17352 (49%)] Loss: -219655.281250\n",
      "Train Epoch: 134 [9984/17352 (58%)] Loss: -222637.312500\n",
      "Train Epoch: 134 [11392/17352 (66%)] Loss: -213150.562500\n",
      "Train Epoch: 134 [12800/17352 (74%)] Loss: -233081.781250\n",
      "Train Epoch: 134 [14208/17352 (82%)] Loss: -237408.312500\n",
      "Train Epoch: 134 [15441/17352 (89%)] Loss: -5742.282227\n",
      "Train Epoch: 134 [16223/17352 (93%)] Loss: -79740.601562\n",
      "Train Epoch: 134 [16967/17352 (98%)] Loss: -89075.578125\n",
      "    epoch          : 134\n",
      "    loss           : -210207.81215263213\n",
      "    val_loss       : -113784.52039642334\n",
      "Train Epoch: 135 [128/17352 (1%)] Loss: -237505.000000\n",
      "Train Epoch: 135 [1536/17352 (9%)] Loss: -239128.562500\n",
      "Train Epoch: 135 [2944/17352 (17%)] Loss: -240311.359375\n",
      "Train Epoch: 135 [4352/17352 (25%)] Loss: -226572.968750\n",
      "Train Epoch: 135 [5760/17352 (33%)] Loss: -237003.312500\n",
      "Train Epoch: 135 [7168/17352 (41%)] Loss: -236461.734375\n",
      "Train Epoch: 135 [8576/17352 (49%)] Loss: -238938.546875\n",
      "Train Epoch: 135 [9984/17352 (58%)] Loss: -224932.109375\n",
      "Train Epoch: 135 [11392/17352 (66%)] Loss: -230526.937500\n",
      "Train Epoch: 135 [12800/17352 (74%)] Loss: -221851.125000\n",
      "Train Epoch: 135 [14208/17352 (82%)] Loss: -236173.125000\n",
      "Train Epoch: 135 [15451/17352 (89%)] Loss: -157952.500000\n",
      "Train Epoch: 135 [16106/17352 (93%)] Loss: -172670.921875\n",
      "Train Epoch: 135 [16986/17352 (98%)] Loss: -66569.078125\n",
      "    epoch          : 135\n",
      "    loss           : -210192.15722983956\n",
      "    val_loss       : -113654.28061014811\n",
      "Train Epoch: 136 [128/17352 (1%)] Loss: -214142.390625\n",
      "Train Epoch: 136 [1536/17352 (9%)] Loss: -240102.765625\n",
      "Train Epoch: 136 [2944/17352 (17%)] Loss: -228150.062500\n",
      "Train Epoch: 136 [4352/17352 (25%)] Loss: -219695.250000\n",
      "Train Epoch: 136 [5760/17352 (33%)] Loss: -238146.984375\n",
      "Train Epoch: 136 [7168/17352 (41%)] Loss: -257413.937500\n",
      "Train Epoch: 136 [8576/17352 (49%)] Loss: -225910.750000\n",
      "Train Epoch: 136 [9984/17352 (58%)] Loss: -233654.312500\n",
      "Train Epoch: 136 [11392/17352 (66%)] Loss: -230000.968750\n",
      "Train Epoch: 136 [12800/17352 (74%)] Loss: -233651.359375\n",
      "Train Epoch: 136 [14208/17352 (82%)] Loss: -222635.812500\n",
      "Train Epoch: 136 [15551/17352 (90%)] Loss: -171119.765625\n",
      "Train Epoch: 136 [16369/17352 (94%)] Loss: -157437.312500\n",
      "Train Epoch: 136 [17063/17352 (98%)] Loss: -28171.878906\n",
      "    epoch          : 136\n",
      "    loss           : -210339.76796481753\n",
      "    val_loss       : -113813.17855072021\n",
      "Train Epoch: 137 [128/17352 (1%)] Loss: -235242.453125\n",
      "Train Epoch: 137 [1536/17352 (9%)] Loss: -227671.593750\n",
      "Train Epoch: 137 [2944/17352 (17%)] Loss: -237885.343750\n",
      "Train Epoch: 137 [4352/17352 (25%)] Loss: -221204.406250\n",
      "Train Epoch: 137 [5760/17352 (33%)] Loss: -235954.062500\n",
      "Train Epoch: 137 [7168/17352 (41%)] Loss: -225599.609375\n",
      "Train Epoch: 137 [8576/17352 (49%)] Loss: -240484.625000\n",
      "Train Epoch: 137 [9984/17352 (58%)] Loss: -235222.671875\n",
      "Train Epoch: 137 [11392/17352 (66%)] Loss: -214442.781250\n",
      "Train Epoch: 137 [12800/17352 (74%)] Loss: -229869.718750\n",
      "Train Epoch: 137 [14208/17352 (82%)] Loss: -225746.562500\n",
      "Train Epoch: 137 [15459/17352 (89%)] Loss: -158116.062500\n",
      "Train Epoch: 137 [16249/17352 (94%)] Loss: -87486.960938\n",
      "Train Epoch: 137 [17013/17352 (98%)] Loss: -5900.966309\n",
      "    epoch          : 137\n",
      "    loss           : -210265.4133644872\n",
      "    val_loss       : -113754.64290161133\n",
      "Train Epoch: 138 [128/17352 (1%)] Loss: -235384.468750\n",
      "Train Epoch: 138 [1536/17352 (9%)] Loss: -235851.250000\n",
      "Train Epoch: 138 [2944/17352 (17%)] Loss: -224156.390625\n",
      "Train Epoch: 138 [4352/17352 (25%)] Loss: -240806.843750\n",
      "Train Epoch: 138 [5760/17352 (33%)] Loss: -226858.031250\n",
      "Train Epoch: 138 [7168/17352 (41%)] Loss: -221829.078125\n",
      "Train Epoch: 138 [8576/17352 (49%)] Loss: -245932.312500\n",
      "Train Epoch: 138 [9984/17352 (58%)] Loss: -212377.000000\n",
      "Train Epoch: 138 [11392/17352 (66%)] Loss: -240005.625000\n",
      "Train Epoch: 138 [12800/17352 (74%)] Loss: -235214.843750\n",
      "Train Epoch: 138 [14208/17352 (82%)] Loss: -228208.312500\n",
      "Train Epoch: 138 [15515/17352 (89%)] Loss: -187535.687500\n",
      "Train Epoch: 138 [16337/17352 (94%)] Loss: -158153.406250\n",
      "Train Epoch: 138 [17022/17352 (98%)] Loss: -89633.171875\n",
      "    epoch          : 138\n",
      "    loss           : -210166.4412522284\n",
      "    val_loss       : -113530.80463002523\n",
      "Train Epoch: 139 [128/17352 (1%)] Loss: -212825.484375\n",
      "Train Epoch: 139 [1536/17352 (9%)] Loss: -239548.593750\n",
      "Train Epoch: 139 [2944/17352 (17%)] Loss: -238887.281250\n",
      "Train Epoch: 139 [4352/17352 (25%)] Loss: -242045.000000\n",
      "Train Epoch: 139 [5760/17352 (33%)] Loss: -235514.906250\n",
      "Train Epoch: 139 [7168/17352 (41%)] Loss: -226146.015625\n",
      "Train Epoch: 139 [8576/17352 (49%)] Loss: -229078.453125\n",
      "Train Epoch: 139 [9984/17352 (58%)] Loss: -240161.875000\n",
      "Train Epoch: 139 [11392/17352 (66%)] Loss: -214380.062500\n",
      "Train Epoch: 139 [12800/17352 (74%)] Loss: -238182.406250\n",
      "Train Epoch: 139 [14208/17352 (82%)] Loss: -239305.531250\n",
      "Train Epoch: 139 [15499/17352 (89%)] Loss: -150113.187500\n",
      "Train Epoch: 139 [16192/17352 (93%)] Loss: -138989.687500\n",
      "Train Epoch: 139 [16970/17352 (98%)] Loss: -65882.343750\n",
      "    epoch          : 139\n",
      "    loss           : -210200.95767355285\n",
      "    val_loss       : -113863.76882375081\n",
      "Train Epoch: 140 [128/17352 (1%)] Loss: -211433.765625\n",
      "Train Epoch: 140 [1536/17352 (9%)] Loss: -237975.281250\n",
      "Train Epoch: 140 [2944/17352 (17%)] Loss: -241763.796875\n",
      "Train Epoch: 140 [4352/17352 (25%)] Loss: -234584.671875\n",
      "Train Epoch: 140 [5760/17352 (33%)] Loss: -237074.765625\n",
      "Train Epoch: 140 [7168/17352 (41%)] Loss: -240545.375000\n",
      "Train Epoch: 140 [8576/17352 (49%)] Loss: -220168.765625\n",
      "Train Epoch: 140 [9984/17352 (58%)] Loss: -234579.078125\n",
      "Train Epoch: 140 [11392/17352 (66%)] Loss: -233629.859375\n",
      "Train Epoch: 140 [12800/17352 (74%)] Loss: -237164.078125\n",
      "Train Epoch: 140 [14208/17352 (82%)] Loss: -237259.250000\n",
      "Train Epoch: 140 [15464/17352 (89%)] Loss: -86639.039062\n",
      "Train Epoch: 140 [16240/17352 (94%)] Loss: -188447.750000\n",
      "Train Epoch: 140 [17076/17352 (98%)] Loss: -171022.843750\n",
      "    epoch          : 140\n",
      "    loss           : -210409.35755623426\n",
      "    val_loss       : -113835.4722829183\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0813_152913/checkpoint-epoch140.pth ...\n",
      "Train Epoch: 141 [128/17352 (1%)] Loss: -236446.921875\n",
      "Train Epoch: 141 [1536/17352 (9%)] Loss: -222843.015625\n",
      "Train Epoch: 141 [2944/17352 (17%)] Loss: -242707.859375\n",
      "Train Epoch: 141 [4352/17352 (25%)] Loss: -226363.140625\n",
      "Train Epoch: 141 [5760/17352 (33%)] Loss: -238642.781250\n",
      "Train Epoch: 141 [7168/17352 (41%)] Loss: -234522.031250\n",
      "Train Epoch: 141 [8576/17352 (49%)] Loss: -225509.687500\n",
      "Train Epoch: 141 [9984/17352 (58%)] Loss: -220757.125000\n",
      "Train Epoch: 141 [11392/17352 (66%)] Loss: -215864.015625\n",
      "Train Epoch: 141 [12800/17352 (74%)] Loss: -220025.328125\n",
      "Train Epoch: 141 [14208/17352 (82%)] Loss: -235500.687500\n",
      "Train Epoch: 141 [15541/17352 (90%)] Loss: -170639.125000\n",
      "Train Epoch: 141 [16196/17352 (93%)] Loss: -91661.242188\n",
      "Train Epoch: 141 [16943/17352 (98%)] Loss: -65604.460938\n",
      "    epoch          : 141\n",
      "    loss           : -210090.08016005138\n",
      "    val_loss       : -113611.26244405111\n",
      "Train Epoch: 142 [128/17352 (1%)] Loss: -235647.437500\n",
      "Train Epoch: 142 [1536/17352 (9%)] Loss: -241062.671875\n",
      "Train Epoch: 142 [2944/17352 (17%)] Loss: -224518.140625\n",
      "Train Epoch: 142 [4352/17352 (25%)] Loss: -226704.375000\n",
      "Train Epoch: 142 [5760/17352 (33%)] Loss: -228315.812500\n",
      "Train Epoch: 142 [7168/17352 (41%)] Loss: -241516.515625\n",
      "Train Epoch: 142 [8576/17352 (49%)] Loss: -229446.687500\n",
      "Train Epoch: 142 [9984/17352 (58%)] Loss: -234442.234375\n",
      "Train Epoch: 142 [11392/17352 (66%)] Loss: -216036.703125\n",
      "Train Epoch: 142 [12800/17352 (74%)] Loss: -233371.984375\n",
      "Train Epoch: 142 [14208/17352 (82%)] Loss: -237550.750000\n",
      "Train Epoch: 142 [15541/17352 (90%)] Loss: -167763.656250\n",
      "Train Epoch: 142 [16216/17352 (93%)] Loss: -140668.421875\n",
      "Train Epoch: 142 [17085/17352 (98%)] Loss: -150728.671875\n",
      "    epoch          : 142\n",
      "    loss           : -210283.70779152686\n",
      "    val_loss       : -113772.27888081869\n",
      "Train Epoch: 143 [128/17352 (1%)] Loss: -235039.937500\n",
      "Train Epoch: 143 [1536/17352 (9%)] Loss: -238904.875000\n",
      "Train Epoch: 143 [2944/17352 (17%)] Loss: -223013.437500\n",
      "Train Epoch: 143 [4352/17352 (25%)] Loss: -233276.843750\n",
      "Train Epoch: 143 [5760/17352 (33%)] Loss: -224312.984375\n",
      "Train Epoch: 143 [7168/17352 (41%)] Loss: -228688.750000\n",
      "Train Epoch: 143 [8576/17352 (49%)] Loss: -238480.296875\n",
      "Train Epoch: 143 [9984/17352 (58%)] Loss: -220813.968750\n",
      "Train Epoch: 143 [11392/17352 (66%)] Loss: -234001.390625\n",
      "Train Epoch: 143 [12800/17352 (74%)] Loss: -231231.343750\n",
      "Train Epoch: 143 [14208/17352 (82%)] Loss: -237083.390625\n",
      "Train Epoch: 143 [15492/17352 (89%)] Loss: -9359.257812\n",
      "Train Epoch: 143 [16238/17352 (94%)] Loss: -69740.289062\n",
      "Train Epoch: 143 [16961/17352 (98%)] Loss: -92377.304688\n",
      "    epoch          : 143\n",
      "    loss           : -210267.36513606334\n",
      "    val_loss       : -113789.26912180583\n",
      "Train Epoch: 144 [128/17352 (1%)] Loss: -235346.093750\n",
      "Train Epoch: 144 [1536/17352 (9%)] Loss: -238369.828125\n",
      "Train Epoch: 144 [2944/17352 (17%)] Loss: -220774.343750\n",
      "Train Epoch: 144 [4352/17352 (25%)] Loss: -225199.484375\n",
      "Train Epoch: 144 [5760/17352 (33%)] Loss: -236426.156250\n",
      "Train Epoch: 144 [7168/17352 (41%)] Loss: -235305.312500\n",
      "Train Epoch: 144 [8576/17352 (49%)] Loss: -222129.734375\n",
      "Train Epoch: 144 [9984/17352 (58%)] Loss: -240530.171875\n",
      "Train Epoch: 144 [11392/17352 (66%)] Loss: -233994.093750\n",
      "Train Epoch: 144 [12800/17352 (74%)] Loss: -230172.218750\n",
      "Train Epoch: 144 [14208/17352 (82%)] Loss: -230421.843750\n",
      "Train Epoch: 144 [15553/17352 (90%)] Loss: -199340.515625\n",
      "Train Epoch: 144 [16296/17352 (94%)] Loss: -89769.984375\n",
      "Train Epoch: 144 [17062/17352 (98%)] Loss: -132469.906250\n",
      "    epoch          : 144\n",
      "    loss           : -210463.94709849518\n",
      "    val_loss       : -113813.21662139893\n",
      "Train Epoch: 145 [128/17352 (1%)] Loss: -226715.359375\n",
      "Train Epoch: 145 [1536/17352 (9%)] Loss: -230755.000000\n",
      "Train Epoch: 145 [2944/17352 (17%)] Loss: -229743.609375\n",
      "Train Epoch: 145 [4352/17352 (25%)] Loss: -234321.625000\n",
      "Train Epoch: 145 [5760/17352 (33%)] Loss: -246658.187500\n",
      "Train Epoch: 145 [7168/17352 (41%)] Loss: -225026.078125\n",
      "Train Epoch: 145 [8576/17352 (49%)] Loss: -219907.656250\n",
      "Train Epoch: 145 [9984/17352 (58%)] Loss: -235544.078125\n",
      "Train Epoch: 145 [11392/17352 (66%)] Loss: -232670.968750\n",
      "Train Epoch: 145 [12800/17352 (74%)] Loss: -226036.312500\n",
      "Train Epoch: 145 [14208/17352 (82%)] Loss: -237363.906250\n",
      "Train Epoch: 145 [15519/17352 (89%)] Loss: -134899.828125\n",
      "Train Epoch: 145 [16246/17352 (94%)] Loss: -88057.882812\n",
      "Train Epoch: 145 [17087/17352 (98%)] Loss: -171641.406250\n",
      "    epoch          : 145\n",
      "    loss           : -210425.76741099518\n",
      "    val_loss       : -113731.61270090738\n",
      "Train Epoch: 146 [128/17352 (1%)] Loss: -235568.187500\n",
      "Train Epoch: 146 [1536/17352 (9%)] Loss: -229010.562500\n",
      "Train Epoch: 146 [2944/17352 (17%)] Loss: -240523.031250\n",
      "Train Epoch: 146 [4352/17352 (25%)] Loss: -240529.812500\n",
      "Train Epoch: 146 [5760/17352 (33%)] Loss: -236013.296875\n",
      "Train Epoch: 146 [7168/17352 (41%)] Loss: -241067.125000\n",
      "Train Epoch: 146 [8576/17352 (49%)] Loss: -215382.828125\n",
      "Train Epoch: 146 [9984/17352 (58%)] Loss: -212719.093750\n",
      "Train Epoch: 146 [11392/17352 (66%)] Loss: -229430.234375\n",
      "Train Epoch: 146 [12800/17352 (74%)] Loss: -226381.921875\n",
      "Train Epoch: 146 [14208/17352 (82%)] Loss: -227980.281250\n",
      "Train Epoch: 146 [15468/17352 (89%)] Loss: -24641.826172\n",
      "Train Epoch: 146 [16195/17352 (93%)] Loss: -65759.531250\n",
      "Train Epoch: 146 [17120/17352 (99%)] Loss: -79686.453125\n",
      "    epoch          : 146\n",
      "    loss           : -210546.18295144715\n",
      "    val_loss       : -113722.9071492513\n",
      "Train Epoch: 147 [128/17352 (1%)] Loss: -236460.125000\n",
      "Train Epoch: 147 [1536/17352 (9%)] Loss: -238772.734375\n",
      "Train Epoch: 147 [2944/17352 (17%)] Loss: -259495.328125\n",
      "Train Epoch: 147 [4352/17352 (25%)] Loss: -232578.187500\n",
      "Train Epoch: 147 [5760/17352 (33%)] Loss: -238554.437500\n",
      "Train Epoch: 147 [7168/17352 (41%)] Loss: -224053.093750\n",
      "Train Epoch: 147 [8576/17352 (49%)] Loss: -243031.421875\n",
      "Train Epoch: 147 [9984/17352 (58%)] Loss: -239263.343750\n",
      "Train Epoch: 147 [11392/17352 (66%)] Loss: -229481.625000\n",
      "Train Epoch: 147 [12800/17352 (74%)] Loss: -219227.812500\n",
      "Train Epoch: 147 [14208/17352 (82%)] Loss: -237533.687500\n",
      "Train Epoch: 147 [15541/17352 (90%)] Loss: -237834.390625\n",
      "Train Epoch: 147 [16339/17352 (94%)] Loss: -170676.984375\n",
      "Train Epoch: 147 [16939/17352 (98%)] Loss: -24066.855469\n",
      "    epoch          : 147\n",
      "    loss           : -210404.09634542785\n",
      "    val_loss       : -113788.64580790202\n",
      "Train Epoch: 148 [128/17352 (1%)] Loss: -237090.875000\n",
      "Train Epoch: 148 [1536/17352 (9%)] Loss: -227148.625000\n",
      "Train Epoch: 148 [2944/17352 (17%)] Loss: -241259.609375\n",
      "Train Epoch: 148 [4352/17352 (25%)] Loss: -225791.125000\n",
      "Train Epoch: 148 [5760/17352 (33%)] Loss: -238481.875000\n",
      "Train Epoch: 148 [7168/17352 (41%)] Loss: -230145.218750\n",
      "Train Epoch: 148 [8576/17352 (49%)] Loss: -237322.500000\n",
      "Train Epoch: 148 [9984/17352 (58%)] Loss: -235770.812500\n",
      "Train Epoch: 148 [11392/17352 (66%)] Loss: -216855.890625\n",
      "Train Epoch: 148 [12800/17352 (74%)] Loss: -234622.062500\n",
      "Train Epoch: 148 [14208/17352 (82%)] Loss: -239495.203125\n",
      "Train Epoch: 148 [15485/17352 (89%)] Loss: -68172.523438\n",
      "Train Epoch: 148 [16022/17352 (92%)] Loss: -237007.359375\n",
      "Train Epoch: 148 [16927/17352 (98%)] Loss: -185194.484375\n",
      "    epoch          : 148\n",
      "    loss           : -210521.74998033766\n",
      "    val_loss       : -113795.331880188\n",
      "Train Epoch: 149 [128/17352 (1%)] Loss: -236983.234375\n",
      "Train Epoch: 149 [1536/17352 (9%)] Loss: -227270.140625\n",
      "Train Epoch: 149 [2944/17352 (17%)] Loss: -227802.343750\n",
      "Train Epoch: 149 [4352/17352 (25%)] Loss: -226617.656250\n",
      "Train Epoch: 149 [5760/17352 (33%)] Loss: -228524.250000\n",
      "Train Epoch: 149 [7168/17352 (41%)] Loss: -215613.515625\n",
      "Train Epoch: 149 [8576/17352 (49%)] Loss: -243478.000000\n",
      "Train Epoch: 149 [9984/17352 (58%)] Loss: -210173.046875\n",
      "Train Epoch: 149 [11392/17352 (66%)] Loss: -214241.703125\n",
      "Train Epoch: 149 [12800/17352 (74%)] Loss: -228478.703125\n",
      "Train Epoch: 149 [14208/17352 (82%)] Loss: -229158.906250\n",
      "Train Epoch: 149 [15506/17352 (89%)] Loss: -66520.578125\n",
      "Train Epoch: 149 [16194/17352 (93%)] Loss: -159850.765625\n",
      "Train Epoch: 149 [17114/17352 (99%)] Loss: -198734.890625\n",
      "    epoch          : 149\n",
      "    loss           : -209373.51642132446\n",
      "    val_loss       : -113460.42518107097\n",
      "Train Epoch: 150 [128/17352 (1%)] Loss: -235591.109375\n",
      "Train Epoch: 150 [1536/17352 (9%)] Loss: -227327.703125\n",
      "Train Epoch: 150 [2944/17352 (17%)] Loss: -246682.984375\n",
      "Train Epoch: 150 [4352/17352 (25%)] Loss: -234576.406250\n",
      "Train Epoch: 150 [5760/17352 (33%)] Loss: -222228.375000\n",
      "Train Epoch: 150 [7168/17352 (41%)] Loss: -235603.312500\n",
      "Train Epoch: 150 [8576/17352 (49%)] Loss: -236932.078125\n",
      "Train Epoch: 150 [9984/17352 (58%)] Loss: -240165.265625\n",
      "Train Epoch: 150 [11392/17352 (66%)] Loss: -237039.484375\n",
      "Train Epoch: 150 [12800/17352 (74%)] Loss: -241585.656250\n",
      "Train Epoch: 150 [14208/17352 (82%)] Loss: -229039.062500\n",
      "Train Epoch: 150 [15497/17352 (89%)] Loss: -68652.468750\n",
      "Train Epoch: 150 [16351/17352 (94%)] Loss: -143654.703125\n",
      "Train Epoch: 150 [17168/17352 (99%)] Loss: -24091.433594\n",
      "    epoch          : 150\n",
      "    loss           : -210343.57783661914\n",
      "    val_loss       : -113816.11036885579\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0813_152913/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [128/17352 (1%)] Loss: -238291.687500\n",
      "Train Epoch: 151 [1536/17352 (9%)] Loss: -237737.437500\n",
      "Train Epoch: 151 [2944/17352 (17%)] Loss: -225933.500000\n",
      "Train Epoch: 151 [4352/17352 (25%)] Loss: -231816.546875\n",
      "Train Epoch: 151 [5760/17352 (33%)] Loss: -246531.796875\n",
      "Train Epoch: 151 [7168/17352 (41%)] Loss: -234540.953125\n",
      "Train Epoch: 151 [8576/17352 (49%)] Loss: -239914.281250\n",
      "Train Epoch: 151 [9984/17352 (58%)] Loss: -216927.796875\n",
      "Train Epoch: 151 [11392/17352 (66%)] Loss: -259129.890625\n",
      "Train Epoch: 151 [12800/17352 (74%)] Loss: -230724.031250\n",
      "Train Epoch: 151 [14208/17352 (82%)] Loss: -246116.312500\n",
      "Train Epoch: 151 [15556/17352 (90%)] Loss: -171818.187500\n",
      "Train Epoch: 151 [16311/17352 (94%)] Loss: -237295.218750\n",
      "Train Epoch: 151 [17029/17352 (98%)] Loss: -170884.109375\n",
      "    epoch          : 151\n",
      "    loss           : -210599.56486275693\n",
      "    val_loss       : -113850.06810048422\n",
      "Train Epoch: 152 [128/17352 (1%)] Loss: -236912.000000\n",
      "Train Epoch: 152 [1536/17352 (9%)] Loss: -234094.625000\n",
      "Train Epoch: 152 [2944/17352 (17%)] Loss: -237779.453125\n",
      "Train Epoch: 152 [4352/17352 (25%)] Loss: -232279.046875\n",
      "Train Epoch: 152 [5760/17352 (33%)] Loss: -228738.156250\n",
      "Train Epoch: 152 [7168/17352 (41%)] Loss: -225637.906250\n",
      "Train Epoch: 152 [8576/17352 (49%)] Loss: -229483.515625\n",
      "Train Epoch: 152 [9984/17352 (58%)] Loss: -212014.687500\n",
      "Train Epoch: 152 [11392/17352 (66%)] Loss: -227732.750000\n",
      "Train Epoch: 152 [12800/17352 (74%)] Loss: -219423.906250\n",
      "Train Epoch: 152 [14208/17352 (82%)] Loss: -234606.359375\n",
      "Train Epoch: 152 [15542/17352 (90%)] Loss: -142611.718750\n",
      "Train Epoch: 152 [16371/17352 (94%)] Loss: -65016.250000\n",
      "Train Epoch: 152 [16930/17352 (98%)] Loss: -9180.347656\n",
      "    epoch          : 152\n",
      "    loss           : -210509.3956257865\n",
      "    val_loss       : -113791.99265594482\n",
      "Train Epoch: 153 [128/17352 (1%)] Loss: -236801.640625\n",
      "Train Epoch: 153 [1536/17352 (9%)] Loss: -233674.781250\n",
      "Train Epoch: 153 [2944/17352 (17%)] Loss: -238360.500000\n",
      "Train Epoch: 153 [4352/17352 (25%)] Loss: -229528.171875\n",
      "Train Epoch: 153 [5760/17352 (33%)] Loss: -246897.609375\n",
      "Train Epoch: 153 [7168/17352 (41%)] Loss: -232655.625000\n",
      "Train Epoch: 153 [8576/17352 (49%)] Loss: -230711.640625\n",
      "Train Epoch: 153 [9984/17352 (58%)] Loss: -222016.765625\n",
      "Train Epoch: 153 [11392/17352 (66%)] Loss: -230026.812500\n",
      "Train Epoch: 153 [12800/17352 (74%)] Loss: -237957.562500\n",
      "Train Epoch: 153 [14208/17352 (82%)] Loss: -235301.281250\n",
      "Train Epoch: 153 [15484/17352 (89%)] Loss: -150875.156250\n",
      "Train Epoch: 153 [16215/17352 (93%)] Loss: -90748.968750\n",
      "Train Epoch: 153 [16973/17352 (98%)] Loss: -190155.125000\n",
      "    epoch          : 153\n",
      "    loss           : -210635.79739277475\n",
      "    val_loss       : -113827.49957224527\n",
      "Train Epoch: 154 [128/17352 (1%)] Loss: -236883.953125\n",
      "Train Epoch: 154 [1536/17352 (9%)] Loss: -225293.093750\n",
      "Train Epoch: 154 [2944/17352 (17%)] Loss: -222809.937500\n",
      "Train Epoch: 154 [4352/17352 (25%)] Loss: -225506.937500\n",
      "Train Epoch: 154 [5760/17352 (33%)] Loss: -223608.906250\n",
      "Train Epoch: 154 [7168/17352 (41%)] Loss: -225718.250000\n",
      "Train Epoch: 154 [8576/17352 (49%)] Loss: -215490.484375\n",
      "Train Epoch: 154 [9984/17352 (58%)] Loss: -233471.937500\n",
      "Train Epoch: 154 [11392/17352 (66%)] Loss: -214940.281250\n",
      "Train Epoch: 154 [12800/17352 (74%)] Loss: -236199.156250\n",
      "Train Epoch: 154 [14208/17352 (82%)] Loss: -230157.515625\n",
      "Train Epoch: 154 [15492/17352 (89%)] Loss: -90208.218750\n",
      "Train Epoch: 154 [16256/17352 (94%)] Loss: -186864.453125\n",
      "Train Epoch: 154 [16985/17352 (98%)] Loss: -9508.780273\n",
      "    epoch          : 154\n",
      "    loss           : -210688.1703118446\n",
      "    val_loss       : -113764.50248311361\n",
      "Train Epoch: 155 [128/17352 (1%)] Loss: -235464.218750\n",
      "Train Epoch: 155 [1536/17352 (9%)] Loss: -235402.906250\n",
      "Train Epoch: 155 [2944/17352 (17%)] Loss: -242033.937500\n",
      "Train Epoch: 155 [4352/17352 (25%)] Loss: -234204.906250\n",
      "Train Epoch: 155 [5760/17352 (33%)] Loss: -226692.250000\n",
      "Train Epoch: 155 [7168/17352 (41%)] Loss: -215666.390625\n",
      "Train Epoch: 155 [8576/17352 (49%)] Loss: -237918.890625\n",
      "Train Epoch: 155 [9984/17352 (58%)] Loss: -215243.640625\n",
      "Train Epoch: 155 [11392/17352 (66%)] Loss: -215141.843750\n",
      "Train Epoch: 155 [12800/17352 (74%)] Loss: -234553.812500\n",
      "Train Epoch: 155 [14208/17352 (82%)] Loss: -247752.984375\n",
      "Train Epoch: 155 [15417/17352 (89%)] Loss: -90248.609375\n",
      "Train Epoch: 155 [16256/17352 (94%)] Loss: -154318.531250\n",
      "Train Epoch: 155 [17075/17352 (98%)] Loss: -237246.609375\n",
      "    epoch          : 155\n",
      "    loss           : -210657.03228882657\n",
      "    val_loss       : -113728.61155497233\n",
      "Train Epoch: 156 [128/17352 (1%)] Loss: -214090.250000\n",
      "Train Epoch: 156 [1536/17352 (9%)] Loss: -233018.250000\n",
      "Train Epoch: 156 [2944/17352 (17%)] Loss: -238723.734375\n",
      "Train Epoch: 156 [4352/17352 (25%)] Loss: -224040.296875\n",
      "Train Epoch: 156 [5760/17352 (33%)] Loss: -223841.890625\n",
      "Train Epoch: 156 [7168/17352 (41%)] Loss: -212151.765625\n",
      "Train Epoch: 156 [8576/17352 (49%)] Loss: -246697.375000\n",
      "Train Epoch: 156 [9984/17352 (58%)] Loss: -237750.687500\n",
      "Train Epoch: 156 [11392/17352 (66%)] Loss: -222287.484375\n",
      "Train Epoch: 156 [12800/17352 (74%)] Loss: -220659.765625\n",
      "Train Epoch: 156 [14208/17352 (82%)] Loss: -225077.078125\n",
      "Train Epoch: 156 [15583/17352 (90%)] Loss: -168548.140625\n",
      "Train Epoch: 156 [16189/17352 (93%)] Loss: -93887.476562\n",
      "Train Epoch: 156 [16956/17352 (98%)] Loss: -200228.171875\n",
      "    epoch          : 156\n",
      "    loss           : -210734.90351693582\n",
      "    val_loss       : -113814.73118082683\n",
      "Train Epoch: 157 [128/17352 (1%)] Loss: -214683.906250\n",
      "Train Epoch: 157 [1536/17352 (9%)] Loss: -225801.328125\n",
      "Train Epoch: 157 [2944/17352 (17%)] Loss: -220252.640625\n",
      "Train Epoch: 157 [4352/17352 (25%)] Loss: -236060.437500\n",
      "Train Epoch: 157 [5760/17352 (33%)] Loss: -238444.875000\n",
      "Train Epoch: 157 [7168/17352 (41%)] Loss: -241159.718750\n",
      "Train Epoch: 157 [8576/17352 (49%)] Loss: -221936.062500\n",
      "Train Epoch: 157 [9984/17352 (58%)] Loss: -240532.359375\n",
      "Train Epoch: 157 [11392/17352 (66%)] Loss: -214094.640625\n",
      "Train Epoch: 157 [12800/17352 (74%)] Loss: -223228.781250\n",
      "Train Epoch: 157 [14208/17352 (82%)] Loss: -236519.531250\n",
      "Train Epoch: 157 [15499/17352 (89%)] Loss: -91079.812500\n",
      "Train Epoch: 157 [16215/17352 (93%)] Loss: -154096.187500\n",
      "Train Epoch: 157 [16971/17352 (98%)] Loss: -191921.187500\n",
      "    epoch          : 157\n",
      "    loss           : -210738.70632340605\n",
      "    val_loss       : -113789.88997039796\n",
      "Train Epoch: 158 [128/17352 (1%)] Loss: -237853.484375\n",
      "Train Epoch: 158 [1536/17352 (9%)] Loss: -237853.296875\n",
      "Train Epoch: 158 [2944/17352 (17%)] Loss: -218173.390625\n",
      "Train Epoch: 158 [4352/17352 (25%)] Loss: -225859.578125\n",
      "Train Epoch: 158 [5760/17352 (33%)] Loss: -231302.859375\n",
      "Train Epoch: 158 [7168/17352 (41%)] Loss: -220936.687500\n",
      "Train Epoch: 158 [8576/17352 (49%)] Loss: -224831.046875\n",
      "Train Epoch: 158 [9984/17352 (58%)] Loss: -234041.218750\n",
      "Train Epoch: 158 [11392/17352 (66%)] Loss: -235751.156250\n",
      "Train Epoch: 158 [12800/17352 (74%)] Loss: -221395.531250\n",
      "Train Epoch: 158 [14208/17352 (82%)] Loss: -225377.656250\n",
      "Train Epoch: 158 [15560/17352 (90%)] Loss: -170787.187500\n",
      "Train Epoch: 158 [16206/17352 (93%)] Loss: -150697.078125\n",
      "Train Epoch: 158 [17097/17352 (99%)] Loss: -185889.859375\n",
      "    epoch          : 158\n",
      "    loss           : -210741.32910811662\n",
      "    val_loss       : -113695.25662231445\n",
      "Train Epoch: 159 [128/17352 (1%)] Loss: -222769.718750\n",
      "Train Epoch: 159 [1536/17352 (9%)] Loss: -235194.062500\n",
      "Train Epoch: 159 [2944/17352 (17%)] Loss: -247890.437500\n",
      "Train Epoch: 159 [4352/17352 (25%)] Loss: -224960.890625\n",
      "Train Epoch: 159 [5760/17352 (33%)] Loss: -223503.843750\n",
      "Train Epoch: 159 [7168/17352 (41%)] Loss: -216513.375000\n",
      "Train Epoch: 159 [8576/17352 (49%)] Loss: -237616.312500\n",
      "Train Epoch: 159 [9984/17352 (58%)] Loss: -240206.187500\n",
      "Train Epoch: 159 [11392/17352 (66%)] Loss: -231371.375000\n",
      "Train Epoch: 159 [12800/17352 (74%)] Loss: -237008.265625\n",
      "Train Epoch: 159 [14208/17352 (82%)] Loss: -245748.156250\n",
      "Train Epoch: 159 [15523/17352 (89%)] Loss: -156832.312500\n",
      "Train Epoch: 159 [16190/17352 (93%)] Loss: -79377.875000\n",
      "Train Epoch: 159 [17012/17352 (98%)] Loss: -5635.097656\n",
      "    epoch          : 159\n",
      "    loss           : -210709.10879168415\n",
      "    val_loss       : -113715.0236328125\n",
      "Train Epoch: 160 [128/17352 (1%)] Loss: -214749.875000\n",
      "Train Epoch: 160 [1536/17352 (9%)] Loss: -232013.843750\n",
      "Train Epoch: 160 [2944/17352 (17%)] Loss: -258139.921875\n",
      "Train Epoch: 160 [4352/17352 (25%)] Loss: -239707.921875\n",
      "Train Epoch: 160 [5760/17352 (33%)] Loss: -236423.640625\n",
      "Train Epoch: 160 [7168/17352 (41%)] Loss: -238558.937500\n",
      "Train Epoch: 160 [8576/17352 (49%)] Loss: -221893.000000\n",
      "Train Epoch: 160 [9984/17352 (58%)] Loss: -237193.343750\n",
      "Train Epoch: 160 [11392/17352 (66%)] Loss: -224152.187500\n",
      "Train Epoch: 160 [12800/17352 (74%)] Loss: -226160.890625\n",
      "Train Epoch: 160 [14208/17352 (82%)] Loss: -245967.312500\n",
      "Train Epoch: 160 [15521/17352 (89%)] Loss: -144648.281250\n",
      "Train Epoch: 160 [16177/17352 (93%)] Loss: -24359.994141\n",
      "Train Epoch: 160 [16896/17352 (97%)] Loss: -170179.500000\n",
      "    epoch          : 160\n",
      "    loss           : -210773.16340708893\n",
      "    val_loss       : -113774.46422220865\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0813_152913/checkpoint-epoch160.pth ...\n",
      "Train Epoch: 161 [128/17352 (1%)] Loss: -237313.593750\n",
      "Train Epoch: 161 [1536/17352 (9%)] Loss: -231636.718750\n",
      "Train Epoch: 161 [2944/17352 (17%)] Loss: -226476.687500\n",
      "Train Epoch: 161 [4352/17352 (25%)] Loss: -236232.687500\n",
      "Train Epoch: 161 [5760/17352 (33%)] Loss: -226106.328125\n",
      "Train Epoch: 161 [7168/17352 (41%)] Loss: -222402.390625\n",
      "Train Epoch: 161 [8576/17352 (49%)] Loss: -231786.484375\n",
      "Train Epoch: 161 [9984/17352 (58%)] Loss: -234353.468750\n",
      "Train Epoch: 161 [11392/17352 (66%)] Loss: -215509.328125\n",
      "Train Epoch: 161 [12800/17352 (74%)] Loss: -222537.234375\n",
      "Train Epoch: 161 [14208/17352 (82%)] Loss: -235805.437500\n",
      "Train Epoch: 161 [15498/17352 (89%)] Loss: -79818.789062\n",
      "Train Epoch: 161 [16448/17352 (95%)] Loss: -151060.031250\n",
      "Train Epoch: 161 [17039/17352 (98%)] Loss: -198670.328125\n",
      "    epoch          : 161\n",
      "    loss           : -210823.33351247903\n",
      "    val_loss       : -113713.7460454305\n",
      "Train Epoch: 162 [128/17352 (1%)] Loss: -214957.921875\n",
      "Train Epoch: 162 [1536/17352 (9%)] Loss: -235629.093750\n",
      "Train Epoch: 162 [2944/17352 (17%)] Loss: -213791.406250\n",
      "Train Epoch: 162 [4352/17352 (25%)] Loss: -234392.718750\n",
      "Train Epoch: 162 [5760/17352 (33%)] Loss: -225130.937500\n",
      "Train Epoch: 162 [7168/17352 (41%)] Loss: -223718.437500\n",
      "Train Epoch: 162 [8576/17352 (49%)] Loss: -222870.578125\n",
      "Train Epoch: 162 [9984/17352 (58%)] Loss: -233437.687500\n",
      "Train Epoch: 162 [11392/17352 (66%)] Loss: -213137.218750\n",
      "Train Epoch: 162 [12800/17352 (74%)] Loss: -223209.015625\n",
      "Train Epoch: 162 [14208/17352 (82%)] Loss: -236883.734375\n",
      "Train Epoch: 162 [15570/17352 (90%)] Loss: -183795.031250\n",
      "Train Epoch: 162 [16197/17352 (93%)] Loss: -9682.567383\n",
      "Train Epoch: 162 [17051/17352 (98%)] Loss: -152581.390625\n",
      "    epoch          : 162\n",
      "    loss           : -210822.97610371225\n",
      "    val_loss       : -113676.27470347086\n",
      "Train Epoch: 163 [128/17352 (1%)] Loss: -233429.968750\n",
      "Train Epoch: 163 [1536/17352 (9%)] Loss: -234686.343750\n",
      "Train Epoch: 163 [2944/17352 (17%)] Loss: -243070.625000\n",
      "Train Epoch: 163 [4352/17352 (25%)] Loss: -238889.578125\n",
      "Train Epoch: 163 [5760/17352 (33%)] Loss: -236767.890625\n",
      "Train Epoch: 163 [7168/17352 (41%)] Loss: -228161.031250\n",
      "Train Epoch: 163 [8576/17352 (49%)] Loss: -229532.484375\n",
      "Train Epoch: 163 [9984/17352 (58%)] Loss: -238354.656250\n",
      "Train Epoch: 163 [11392/17352 (66%)] Loss: -236755.828125\n",
      "Train Epoch: 163 [12800/17352 (74%)] Loss: -234669.343750\n",
      "Train Epoch: 163 [14208/17352 (82%)] Loss: -228382.140625\n",
      "Train Epoch: 163 [15481/17352 (89%)] Loss: -139490.500000\n",
      "Train Epoch: 163 [16100/17352 (93%)] Loss: -88050.296875\n",
      "Train Epoch: 163 [17003/17352 (98%)] Loss: -153771.921875\n",
      "    epoch          : 163\n",
      "    loss           : -210830.44459810192\n",
      "    val_loss       : -113716.32156829834\n",
      "Train Epoch: 164 [128/17352 (1%)] Loss: -235713.296875\n",
      "Train Epoch: 164 [1536/17352 (9%)] Loss: -241611.140625\n",
      "Train Epoch: 164 [2944/17352 (17%)] Loss: -246335.937500\n",
      "Train Epoch: 164 [4352/17352 (25%)] Loss: -225399.234375\n",
      "Train Epoch: 164 [5760/17352 (33%)] Loss: -215222.078125\n",
      "Train Epoch: 164 [7168/17352 (41%)] Loss: -240360.718750\n",
      "Train Epoch: 164 [8576/17352 (49%)] Loss: -247546.515625\n",
      "Train Epoch: 164 [9984/17352 (58%)] Loss: -214084.062500\n",
      "Train Epoch: 164 [11392/17352 (66%)] Loss: -229698.875000\n",
      "Train Epoch: 164 [12800/17352 (74%)] Loss: -239650.046875\n",
      "Train Epoch: 164 [14208/17352 (82%)] Loss: -239407.046875\n",
      "Train Epoch: 164 [15520/17352 (89%)] Loss: -173376.687500\n",
      "Train Epoch: 164 [16324/17352 (94%)] Loss: -88563.968750\n",
      "Train Epoch: 164 [16951/17352 (98%)] Loss: -143377.375000\n",
      "    epoch          : 164\n",
      "    loss           : -210886.17353974414\n",
      "    val_loss       : -113699.2716913859\n",
      "Train Epoch: 165 [128/17352 (1%)] Loss: -236714.984375\n",
      "Train Epoch: 165 [1536/17352 (9%)] Loss: -235799.781250\n",
      "Train Epoch: 165 [2944/17352 (17%)] Loss: -237163.187500\n",
      "Train Epoch: 165 [4352/17352 (25%)] Loss: -224867.515625\n",
      "Train Epoch: 165 [5760/17352 (33%)] Loss: -237228.031250\n",
      "Train Epoch: 165 [7168/17352 (41%)] Loss: -242700.562500\n",
      "Train Epoch: 165 [8576/17352 (49%)] Loss: -237659.875000\n",
      "Train Epoch: 165 [9984/17352 (58%)] Loss: -231445.031250\n",
      "Train Epoch: 165 [11392/17352 (66%)] Loss: -216275.000000\n",
      "Train Epoch: 165 [12800/17352 (74%)] Loss: -233178.328125\n",
      "Train Epoch: 165 [14208/17352 (82%)] Loss: -229492.125000\n",
      "Train Epoch: 165 [15487/17352 (89%)] Loss: -134952.218750\n",
      "Train Epoch: 165 [16173/17352 (93%)] Loss: -5539.893066\n",
      "Train Epoch: 165 [16944/17352 (98%)] Loss: -132298.531250\n",
      "    epoch          : 165\n",
      "    loss           : -210842.86509346162\n",
      "    val_loss       : -113696.4774734497\n",
      "Train Epoch: 166 [128/17352 (1%)] Loss: -236849.109375\n",
      "Train Epoch: 166 [1536/17352 (9%)] Loss: -231097.296875\n",
      "Train Epoch: 166 [2944/17352 (17%)] Loss: -221798.281250\n",
      "Train Epoch: 166 [4352/17352 (25%)] Loss: -233646.062500\n",
      "Train Epoch: 166 [5760/17352 (33%)] Loss: -238500.343750\n",
      "Train Epoch: 166 [7168/17352 (41%)] Loss: -234206.359375\n",
      "Train Epoch: 166 [8576/17352 (49%)] Loss: -222019.125000\n",
      "Train Epoch: 166 [9984/17352 (58%)] Loss: -233766.734375\n",
      "Train Epoch: 166 [11392/17352 (66%)] Loss: -214024.171875\n",
      "Train Epoch: 166 [12800/17352 (74%)] Loss: -234711.171875\n",
      "Train Epoch: 166 [14208/17352 (82%)] Loss: -246914.468750\n",
      "Train Epoch: 166 [15509/17352 (89%)] Loss: -93666.531250\n",
      "Train Epoch: 166 [16263/17352 (94%)] Loss: -236755.250000\n",
      "Train Epoch: 166 [17080/17352 (98%)] Loss: -159046.203125\n",
      "    epoch          : 166\n",
      "    loss           : -210846.23252018666\n",
      "    val_loss       : -113673.83457438152\n",
      "Train Epoch: 167 [128/17352 (1%)] Loss: -236973.125000\n",
      "Train Epoch: 167 [1536/17352 (9%)] Loss: -228570.765625\n",
      "Train Epoch: 167 [2944/17352 (17%)] Loss: -224700.203125\n",
      "Train Epoch: 167 [4352/17352 (25%)] Loss: -224269.375000\n",
      "Train Epoch: 167 [5760/17352 (33%)] Loss: -240664.156250\n",
      "Train Epoch: 167 [7168/17352 (41%)] Loss: -233975.937500\n",
      "Train Epoch: 167 [8576/17352 (49%)] Loss: -223712.437500\n",
      "Train Epoch: 167 [9984/17352 (58%)] Loss: -235270.890625\n",
      "Train Epoch: 167 [11392/17352 (66%)] Loss: -236676.968750\n",
      "Train Epoch: 167 [12800/17352 (74%)] Loss: -238353.453125\n",
      "Train Epoch: 167 [14208/17352 (82%)] Loss: -236759.734375\n",
      "Train Epoch: 167 [15438/17352 (89%)] Loss: -5638.797852\n",
      "Train Epoch: 167 [16273/17352 (94%)] Loss: -88473.875000\n",
      "Train Epoch: 167 [17009/17352 (98%)] Loss: -170259.781250\n",
      "    epoch          : 167\n",
      "    loss           : -210857.65885198195\n",
      "    val_loss       : -113391.28163655598\n",
      "Train Epoch: 168 [128/17352 (1%)] Loss: -212995.421875\n",
      "Train Epoch: 168 [1536/17352 (9%)] Loss: -240381.718750\n",
      "Train Epoch: 168 [2944/17352 (17%)] Loss: -230460.250000\n",
      "Train Epoch: 168 [4352/17352 (25%)] Loss: -226053.515625\n",
      "Train Epoch: 168 [5760/17352 (33%)] Loss: -248734.718750\n",
      "Train Epoch: 168 [7168/17352 (41%)] Loss: -225740.812500\n",
      "Train Epoch: 168 [8576/17352 (49%)] Loss: -218926.718750\n",
      "Train Epoch: 168 [9984/17352 (58%)] Loss: -217783.718750\n",
      "Train Epoch: 168 [11392/17352 (66%)] Loss: -236137.812500\n",
      "Train Epoch: 168 [12800/17352 (74%)] Loss: -246793.453125\n",
      "Train Epoch: 168 [14208/17352 (82%)] Loss: -239484.812500\n",
      "Train Epoch: 168 [15499/17352 (89%)] Loss: -150298.296875\n",
      "Train Epoch: 168 [16309/17352 (94%)] Loss: -132373.843750\n",
      "Train Epoch: 168 [17063/17352 (98%)] Loss: -68622.039062\n",
      "    epoch          : 168\n",
      "    loss           : -210810.82137426594\n",
      "    val_loss       : -113642.90365397136\n",
      "Train Epoch: 169 [128/17352 (1%)] Loss: -236578.968750\n",
      "Train Epoch: 169 [1536/17352 (9%)] Loss: -228127.656250\n",
      "Train Epoch: 169 [2944/17352 (17%)] Loss: -215957.218750\n",
      "Train Epoch: 169 [4352/17352 (25%)] Loss: -233114.171875\n",
      "Train Epoch: 169 [5760/17352 (33%)] Loss: -239293.656250\n",
      "Train Epoch: 169 [7168/17352 (41%)] Loss: -234055.796875\n",
      "Train Epoch: 169 [8576/17352 (49%)] Loss: -215592.234375\n",
      "Train Epoch: 169 [9984/17352 (58%)] Loss: -220647.421875\n",
      "Train Epoch: 169 [11392/17352 (66%)] Loss: -236897.953125\n",
      "Train Epoch: 169 [12800/17352 (74%)] Loss: -226242.875000\n",
      "Train Epoch: 169 [14208/17352 (82%)] Loss: -246904.375000\n",
      "Train Epoch: 169 [15559/17352 (90%)] Loss: -151139.937500\n",
      "Train Epoch: 169 [16356/17352 (94%)] Loss: -156913.218750\n",
      "Train Epoch: 169 [17081/17352 (98%)] Loss: -188072.218750\n",
      "    epoch          : 169\n",
      "    loss           : -210915.733015022\n",
      "    val_loss       : -113702.83524983724\n",
      "Train Epoch: 170 [128/17352 (1%)] Loss: -238887.093750\n",
      "Train Epoch: 170 [1536/17352 (9%)] Loss: -222353.625000\n",
      "Train Epoch: 170 [2944/17352 (17%)] Loss: -223989.625000\n",
      "Train Epoch: 170 [4352/17352 (25%)] Loss: -240627.078125\n",
      "Train Epoch: 170 [5760/17352 (33%)] Loss: -213674.812500\n",
      "Train Epoch: 170 [7168/17352 (41%)] Loss: -229474.234375\n",
      "Train Epoch: 170 [8576/17352 (49%)] Loss: -245440.500000\n",
      "Train Epoch: 170 [9984/17352 (58%)] Loss: -235977.390625\n",
      "Train Epoch: 170 [11392/17352 (66%)] Loss: -216858.312500\n",
      "Train Epoch: 170 [12800/17352 (74%)] Loss: -230870.234375\n",
      "Train Epoch: 170 [14208/17352 (82%)] Loss: -246529.468750\n",
      "Train Epoch: 170 [15549/17352 (90%)] Loss: -154226.921875\n",
      "Train Epoch: 170 [16248/17352 (94%)] Loss: -9460.355469\n",
      "Train Epoch: 170 [17088/17352 (98%)] Loss: -236401.296875\n",
      "    epoch          : 170\n",
      "    loss           : -210882.29422713927\n",
      "    val_loss       : -113454.39745737711\n",
      "Validation performance didn't improve for 50 epochs. Training stops.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
