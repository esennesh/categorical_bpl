{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='omniglot_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 50,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [128/17352 (1%)] Loss: 17992.117188\n",
      "Train Epoch: 1 [1536/17352 (9%)] Loss: -7096.289062\n",
      "Train Epoch: 1 [2944/17352 (17%)] Loss: -55400.480469\n",
      "Train Epoch: 1 [4352/17352 (25%)] Loss: -106588.304688\n",
      "Train Epoch: 1 [5760/17352 (33%)] Loss: -99689.367188\n",
      "Train Epoch: 1 [7168/17352 (41%)] Loss: -168373.328125\n",
      "Train Epoch: 1 [8576/17352 (49%)] Loss: -142678.343750\n",
      "Train Epoch: 1 [9984/17352 (58%)] Loss: -137653.046875\n",
      "Train Epoch: 1 [11392/17352 (66%)] Loss: -150130.015625\n",
      "Train Epoch: 1 [12800/17352 (74%)] Loss: -191826.734375\n",
      "Train Epoch: 1 [14208/17352 (82%)] Loss: -164560.218750\n",
      "Train Epoch: 1 [15515/17352 (89%)] Loss: -75496.343750\n",
      "Train Epoch: 1 [16366/17352 (94%)] Loss: -88766.609375\n",
      "Train Epoch: 1 [16907/17352 (97%)] Loss: -42005.359375\n",
      "    epoch          : 1\n",
      "    loss           : -101175.68137026794\n",
      "    val_loss       : -72244.02672487895\n",
      "Train Epoch: 2 [128/17352 (1%)] Loss: -90737.546875\n",
      "Train Epoch: 2 [1536/17352 (9%)] Loss: -144297.765625\n",
      "Train Epoch: 2 [2944/17352 (17%)] Loss: -122757.500000\n",
      "Train Epoch: 2 [4352/17352 (25%)] Loss: -143545.000000\n",
      "Train Epoch: 2 [5760/17352 (33%)] Loss: -106442.742188\n",
      "Train Epoch: 2 [7168/17352 (41%)] Loss: -146041.421875\n",
      "Train Epoch: 2 [8576/17352 (49%)] Loss: -166611.515625\n",
      "Train Epoch: 2 [9984/17352 (58%)] Loss: -147687.859375\n",
      "Train Epoch: 2 [11392/17352 (66%)] Loss: -130487.734375\n",
      "Train Epoch: 2 [12800/17352 (74%)] Loss: -159944.843750\n",
      "Train Epoch: 2 [14208/17352 (82%)] Loss: -119232.750000\n",
      "Train Epoch: 2 [15448/17352 (89%)] Loss: -90161.546875\n",
      "Train Epoch: 2 [16255/17352 (94%)] Loss: -56621.625000\n",
      "Train Epoch: 2 [16955/17352 (98%)] Loss: -68915.898438\n",
      "    epoch          : 2\n",
      "    loss           : -135798.6248459784\n",
      "    val_loss       : -76815.84053649902\n",
      "Train Epoch: 3 [128/17352 (1%)] Loss: -183520.031250\n",
      "Train Epoch: 3 [1536/17352 (9%)] Loss: -166571.843750\n",
      "Train Epoch: 3 [2944/17352 (17%)] Loss: -118177.570312\n",
      "Train Epoch: 3 [4352/17352 (25%)] Loss: -192602.343750\n",
      "Train Epoch: 3 [5760/17352 (33%)] Loss: -174926.687500\n",
      "Train Epoch: 3 [7168/17352 (41%)] Loss: -151393.390625\n",
      "Train Epoch: 3 [8576/17352 (49%)] Loss: -156032.718750\n",
      "Train Epoch: 3 [9984/17352 (58%)] Loss: -136201.500000\n",
      "Train Epoch: 3 [11392/17352 (66%)] Loss: -157702.515625\n",
      "Train Epoch: 3 [12800/17352 (74%)] Loss: -186429.843750\n",
      "Train Epoch: 3 [14208/17352 (82%)] Loss: -136067.093750\n",
      "Train Epoch: 3 [15565/17352 (90%)] Loss: -103090.578125\n",
      "Train Epoch: 3 [16286/17352 (94%)] Loss: -114534.109375\n",
      "Train Epoch: 3 [17059/17352 (98%)] Loss: -120519.968750\n",
      "    epoch          : 3\n",
      "    loss           : -148830.20361491977\n",
      "    val_loss       : -80127.88512592316\n",
      "Train Epoch: 4 [128/17352 (1%)] Loss: -164174.078125\n",
      "Train Epoch: 4 [1536/17352 (9%)] Loss: -174035.812500\n",
      "Train Epoch: 4 [2944/17352 (17%)] Loss: -153764.718750\n",
      "Train Epoch: 4 [4352/17352 (25%)] Loss: -186274.359375\n",
      "Train Epoch: 4 [5760/17352 (33%)] Loss: -176255.765625\n",
      "Train Epoch: 4 [7168/17352 (41%)] Loss: -175226.390625\n",
      "Train Epoch: 4 [8576/17352 (49%)] Loss: -144281.500000\n",
      "Train Epoch: 4 [9984/17352 (58%)] Loss: -168768.125000\n",
      "Train Epoch: 4 [11392/17352 (66%)] Loss: -179019.921875\n",
      "Train Epoch: 4 [12800/17352 (74%)] Loss: -190853.734375\n",
      "Train Epoch: 4 [14208/17352 (82%)] Loss: -142577.375000\n",
      "Train Epoch: 4 [15440/17352 (89%)] Loss: -114097.796875\n",
      "Train Epoch: 4 [16085/17352 (93%)] Loss: -51051.535156\n",
      "Train Epoch: 4 [16836/17352 (97%)] Loss: -80891.414062\n",
      "    epoch          : 4\n",
      "    loss           : -153755.28016201762\n",
      "    val_loss       : -86741.7961071968\n",
      "Train Epoch: 5 [128/17352 (1%)] Loss: -160433.703125\n",
      "Train Epoch: 5 [1536/17352 (9%)] Loss: -189412.468750\n",
      "Train Epoch: 5 [2944/17352 (17%)] Loss: -169319.437500\n",
      "Train Epoch: 5 [4352/17352 (25%)] Loss: -150609.531250\n",
      "Train Epoch: 5 [5760/17352 (33%)] Loss: -178025.062500\n",
      "Train Epoch: 5 [7168/17352 (41%)] Loss: -142673.140625\n",
      "Train Epoch: 5 [8576/17352 (49%)] Loss: -183553.437500\n",
      "Train Epoch: 5 [9984/17352 (58%)] Loss: -150414.718750\n",
      "Train Epoch: 5 [11392/17352 (66%)] Loss: -164573.828125\n",
      "Train Epoch: 5 [12800/17352 (74%)] Loss: -165497.937500\n",
      "Train Epoch: 5 [14208/17352 (82%)] Loss: -181263.343750\n",
      "Train Epoch: 5 [15524/17352 (89%)] Loss: -122127.406250\n",
      "Train Epoch: 5 [16335/17352 (94%)] Loss: -124181.210938\n",
      "Train Epoch: 5 [17044/17352 (98%)] Loss: -109719.109375\n",
      "    epoch          : 5\n",
      "    loss           : -158506.5826178429\n",
      "    val_loss       : -89155.91472180684\n",
      "Train Epoch: 6 [128/17352 (1%)] Loss: -170475.953125\n",
      "Train Epoch: 6 [1536/17352 (9%)] Loss: -166785.609375\n",
      "Train Epoch: 6 [2944/17352 (17%)] Loss: -165605.250000\n",
      "Train Epoch: 6 [4352/17352 (25%)] Loss: -168915.921875\n",
      "Train Epoch: 6 [5760/17352 (33%)] Loss: -179825.000000\n",
      "Train Epoch: 6 [7168/17352 (41%)] Loss: -156289.859375\n",
      "Train Epoch: 6 [8576/17352 (49%)] Loss: -154532.031250\n",
      "Train Epoch: 6 [9984/17352 (58%)] Loss: -189738.906250\n",
      "Train Epoch: 6 [11392/17352 (66%)] Loss: -162868.875000\n",
      "Train Epoch: 6 [12800/17352 (74%)] Loss: -180776.515625\n",
      "Train Epoch: 6 [14208/17352 (82%)] Loss: -195710.187500\n",
      "Train Epoch: 6 [15459/17352 (89%)] Loss: -4334.544434\n",
      "Train Epoch: 6 [16186/17352 (93%)] Loss: -55204.109375\n",
      "Train Epoch: 6 [17055/17352 (98%)] Loss: -20656.193359\n",
      "    epoch          : 6\n",
      "    loss           : -162493.53708643562\n",
      "    val_loss       : -90142.30305646261\n",
      "Train Epoch: 7 [128/17352 (1%)] Loss: -188897.125000\n",
      "Train Epoch: 7 [1536/17352 (9%)] Loss: -184142.156250\n",
      "Train Epoch: 7 [2944/17352 (17%)] Loss: -178410.031250\n",
      "Train Epoch: 7 [4352/17352 (25%)] Loss: -208788.203125\n",
      "Train Epoch: 7 [5760/17352 (33%)] Loss: -179930.140625\n",
      "Train Epoch: 7 [7168/17352 (41%)] Loss: -184527.343750\n",
      "Train Epoch: 7 [8576/17352 (49%)] Loss: -157230.656250\n",
      "Train Epoch: 7 [9984/17352 (58%)] Loss: -169199.000000\n",
      "Train Epoch: 7 [11392/17352 (66%)] Loss: -162401.765625\n",
      "Train Epoch: 7 [12800/17352 (74%)] Loss: -189629.953125\n",
      "Train Epoch: 7 [14208/17352 (82%)] Loss: -180608.125000\n",
      "Train Epoch: 7 [15518/17352 (89%)] Loss: -118895.937500\n",
      "Train Epoch: 7 [16253/17352 (94%)] Loss: -109452.187500\n",
      "Train Epoch: 7 [16962/17352 (98%)] Loss: -93244.281250\n",
      "    epoch          : 7\n",
      "    loss           : -162679.61967150797\n",
      "    val_loss       : -90438.039635849\n",
      "Train Epoch: 8 [128/17352 (1%)] Loss: -165812.281250\n",
      "Train Epoch: 8 [1536/17352 (9%)] Loss: -174153.031250\n",
      "Train Epoch: 8 [2944/17352 (17%)] Loss: -222400.484375\n",
      "Train Epoch: 8 [4352/17352 (25%)] Loss: -179625.281250\n",
      "Train Epoch: 8 [5760/17352 (33%)] Loss: -176586.296875\n",
      "Train Epoch: 8 [7168/17352 (41%)] Loss: -174304.546875\n",
      "Train Epoch: 8 [8576/17352 (49%)] Loss: -173723.109375\n",
      "Train Epoch: 8 [9984/17352 (58%)] Loss: -177865.500000\n",
      "Train Epoch: 8 [11392/17352 (66%)] Loss: -169524.500000\n",
      "Train Epoch: 8 [12800/17352 (74%)] Loss: -177206.687500\n",
      "Train Epoch: 8 [14208/17352 (82%)] Loss: -186258.296875\n",
      "Train Epoch: 8 [15500/17352 (89%)] Loss: -68815.218750\n",
      "Train Epoch: 8 [16258/17352 (94%)] Loss: -49855.660156\n",
      "Train Epoch: 8 [16954/17352 (98%)] Loss: -8037.670898\n",
      "    epoch          : 8\n",
      "    loss           : -166089.50056693057\n",
      "    val_loss       : -92478.59728953043\n",
      "Train Epoch: 9 [128/17352 (1%)] Loss: -192016.781250\n",
      "Train Epoch: 9 [1536/17352 (9%)] Loss: -184263.796875\n",
      "Train Epoch: 9 [2944/17352 (17%)] Loss: -177870.765625\n",
      "Train Epoch: 9 [4352/17352 (25%)] Loss: -177369.937500\n",
      "Train Epoch: 9 [5760/17352 (33%)] Loss: -180615.093750\n",
      "Train Epoch: 9 [7168/17352 (41%)] Loss: -175588.156250\n",
      "Train Epoch: 9 [8576/17352 (49%)] Loss: -180278.718750\n",
      "Train Epoch: 9 [9984/17352 (58%)] Loss: -143246.312500\n",
      "Train Epoch: 9 [11392/17352 (66%)] Loss: -197445.453125\n",
      "Train Epoch: 9 [12800/17352 (74%)] Loss: -179948.484375\n",
      "Train Epoch: 9 [14208/17352 (82%)] Loss: -198693.921875\n",
      "Train Epoch: 9 [15455/17352 (89%)] Loss: -7967.342773\n",
      "Train Epoch: 9 [16090/17352 (93%)] Loss: -21631.037109\n",
      "Train Epoch: 9 [17018/17352 (98%)] Loss: -161927.359375\n",
      "    epoch          : 9\n",
      "    loss           : -167790.17841927957\n",
      "    val_loss       : -94157.8668621699\n",
      "Train Epoch: 10 [128/17352 (1%)] Loss: -191349.000000\n",
      "Train Epoch: 10 [1536/17352 (9%)] Loss: -186056.656250\n",
      "Train Epoch: 10 [2944/17352 (17%)] Loss: -180733.984375\n",
      "Train Epoch: 10 [4352/17352 (25%)] Loss: -180099.875000\n",
      "Train Epoch: 10 [5760/17352 (33%)] Loss: -182785.406250\n",
      "Train Epoch: 10 [7168/17352 (41%)] Loss: -188044.187500\n",
      "Train Epoch: 10 [8576/17352 (49%)] Loss: -190053.921875\n",
      "Train Epoch: 10 [9984/17352 (58%)] Loss: -203727.500000\n",
      "Train Epoch: 10 [11392/17352 (66%)] Loss: -199249.765625\n",
      "Train Epoch: 10 [12800/17352 (74%)] Loss: -185296.625000\n",
      "Train Epoch: 10 [14208/17352 (82%)] Loss: -178423.343750\n",
      "Train Epoch: 10 [15489/17352 (89%)] Loss: -74259.757812\n",
      "Train Epoch: 10 [16166/17352 (93%)] Loss: -51064.613281\n",
      "Train Epoch: 10 [17118/17352 (99%)] Loss: -133899.671875\n",
      "    epoch          : 10\n",
      "    loss           : -170413.904650797\n",
      "    val_loss       : -94046.01748771667\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch10.pth ...\n",
      "Train Epoch: 11 [128/17352 (1%)] Loss: -189326.718750\n",
      "Train Epoch: 11 [1536/17352 (9%)] Loss: -175302.781250\n",
      "Train Epoch: 11 [2944/17352 (17%)] Loss: -152644.718750\n",
      "Train Epoch: 11 [4352/17352 (25%)] Loss: -213103.718750\n",
      "Train Epoch: 11 [5760/17352 (33%)] Loss: -192698.078125\n",
      "Train Epoch: 11 [7168/17352 (41%)] Loss: -201660.906250\n",
      "Train Epoch: 11 [8576/17352 (49%)] Loss: -183176.937500\n",
      "Train Epoch: 11 [9984/17352 (58%)] Loss: -181122.968750\n",
      "Train Epoch: 11 [11392/17352 (66%)] Loss: -168780.890625\n",
      "Train Epoch: 11 [12800/17352 (74%)] Loss: -184778.000000\n",
      "Train Epoch: 11 [14208/17352 (82%)] Loss: -180685.093750\n",
      "Train Epoch: 11 [15521/17352 (89%)] Loss: -135132.187500\n",
      "Train Epoch: 11 [16180/17352 (93%)] Loss: -51085.214844\n",
      "Train Epoch: 11 [16913/17352 (97%)] Loss: -8187.600098\n",
      "    epoch          : 11\n",
      "    loss           : -172083.71865824246\n",
      "    val_loss       : -94981.62274284363\n",
      "Train Epoch: 12 [128/17352 (1%)] Loss: -166657.437500\n",
      "Train Epoch: 12 [1536/17352 (9%)] Loss: -193654.296875\n",
      "Train Epoch: 12 [2944/17352 (17%)] Loss: -201123.437500\n",
      "Train Epoch: 12 [4352/17352 (25%)] Loss: -184488.468750\n",
      "Train Epoch: 12 [5760/17352 (33%)] Loss: -179927.625000\n",
      "Train Epoch: 12 [7168/17352 (41%)] Loss: -182662.515625\n",
      "Train Epoch: 12 [8576/17352 (49%)] Loss: -203511.671875\n",
      "Train Epoch: 12 [9984/17352 (58%)] Loss: -185033.468750\n",
      "Train Epoch: 12 [11392/17352 (66%)] Loss: -170749.906250\n",
      "Train Epoch: 12 [12800/17352 (74%)] Loss: -180574.250000\n",
      "Train Epoch: 12 [14208/17352 (82%)] Loss: -195054.312500\n",
      "Train Epoch: 12 [15543/17352 (90%)] Loss: -124425.062500\n",
      "Train Epoch: 12 [16375/17352 (94%)] Loss: -114090.109375\n",
      "Train Epoch: 12 [16953/17352 (98%)] Loss: -20116.248047\n",
      "    epoch          : 12\n",
      "    loss           : -173656.27603292785\n",
      "    val_loss       : -95801.18572095236\n",
      "Train Epoch: 13 [128/17352 (1%)] Loss: -182111.468750\n",
      "Train Epoch: 13 [1536/17352 (9%)] Loss: -187958.250000\n",
      "Train Epoch: 13 [2944/17352 (17%)] Loss: -217812.140625\n",
      "Train Epoch: 13 [4352/17352 (25%)] Loss: -183882.187500\n",
      "Train Epoch: 13 [5760/17352 (33%)] Loss: -194163.109375\n",
      "Train Epoch: 13 [7168/17352 (41%)] Loss: -174978.750000\n",
      "Train Epoch: 13 [8576/17352 (49%)] Loss: -197065.906250\n",
      "Train Epoch: 13 [9984/17352 (58%)] Loss: -196708.718750\n",
      "Train Epoch: 13 [11392/17352 (66%)] Loss: -181497.546875\n",
      "Train Epoch: 13 [12800/17352 (74%)] Loss: -198830.296875\n",
      "Train Epoch: 13 [14208/17352 (82%)] Loss: -206237.921875\n",
      "Train Epoch: 13 [15482/17352 (89%)] Loss: -162247.625000\n",
      "Train Epoch: 13 [16142/17352 (93%)] Loss: -120376.093750\n",
      "Train Epoch: 13 [17071/17352 (98%)] Loss: -161312.265625\n",
      "    epoch          : 13\n",
      "    loss           : -174992.08582608012\n",
      "    val_loss       : -96014.1195552826\n",
      "Train Epoch: 14 [128/17352 (1%)] Loss: -196073.000000\n",
      "Train Epoch: 14 [1536/17352 (9%)] Loss: -195188.093750\n",
      "Train Epoch: 14 [2944/17352 (17%)] Loss: -172667.812500\n",
      "Train Epoch: 14 [4352/17352 (25%)] Loss: -181092.531250\n",
      "Train Epoch: 14 [5760/17352 (33%)] Loss: -188422.375000\n",
      "Train Epoch: 14 [7168/17352 (41%)] Loss: -207295.281250\n",
      "Train Epoch: 14 [8576/17352 (49%)] Loss: -200843.265625\n",
      "Train Epoch: 14 [9984/17352 (58%)] Loss: -221731.125000\n",
      "Train Epoch: 14 [11392/17352 (66%)] Loss: -183577.343750\n",
      "Train Epoch: 14 [12800/17352 (74%)] Loss: -178761.312500\n",
      "Train Epoch: 14 [14208/17352 (82%)] Loss: -201895.500000\n",
      "Train Epoch: 14 [15452/17352 (89%)] Loss: -111253.953125\n",
      "Train Epoch: 14 [16212/17352 (93%)] Loss: -152433.125000\n",
      "Train Epoch: 14 [17036/17352 (98%)] Loss: -123999.679688\n",
      "    epoch          : 14\n",
      "    loss           : -175261.096650194\n",
      "    val_loss       : -97183.6837542216\n",
      "Train Epoch: 15 [128/17352 (1%)] Loss: -179496.890625\n",
      "Train Epoch: 15 [1536/17352 (9%)] Loss: -193689.281250\n",
      "Train Epoch: 15 [2944/17352 (17%)] Loss: -203947.656250\n",
      "Train Epoch: 15 [4352/17352 (25%)] Loss: -181794.406250\n",
      "Train Epoch: 15 [5760/17352 (33%)] Loss: -203732.843750\n",
      "Train Epoch: 15 [7168/17352 (41%)] Loss: -180626.656250\n",
      "Train Epoch: 15 [8576/17352 (49%)] Loss: -222311.906250\n",
      "Train Epoch: 15 [9984/17352 (58%)] Loss: -200933.000000\n",
      "Train Epoch: 15 [11392/17352 (66%)] Loss: -175115.859375\n",
      "Train Epoch: 15 [12800/17352 (74%)] Loss: -187873.062500\n",
      "Train Epoch: 15 [14208/17352 (82%)] Loss: -188886.625000\n",
      "Train Epoch: 15 [15450/17352 (89%)] Loss: -7769.341797\n",
      "Train Epoch: 15 [16265/17352 (94%)] Loss: -74066.265625\n",
      "Train Epoch: 15 [17066/17352 (98%)] Loss: -199120.406250\n",
      "    epoch          : 15\n",
      "    loss           : -177312.43230586723\n",
      "    val_loss       : -97703.64877837499\n",
      "Train Epoch: 16 [128/17352 (1%)] Loss: -169749.375000\n",
      "Train Epoch: 16 [1536/17352 (9%)] Loss: -204224.593750\n",
      "Train Epoch: 16 [2944/17352 (17%)] Loss: -215791.187500\n",
      "Train Epoch: 16 [4352/17352 (25%)] Loss: -196266.609375\n",
      "Train Epoch: 16 [5760/17352 (33%)] Loss: -203519.812500\n",
      "Train Epoch: 16 [7168/17352 (41%)] Loss: -194737.640625\n",
      "Train Epoch: 16 [8576/17352 (49%)] Loss: -186438.468750\n",
      "Train Epoch: 16 [9984/17352 (58%)] Loss: -165964.171875\n",
      "Train Epoch: 16 [11392/17352 (66%)] Loss: -196214.031250\n",
      "Train Epoch: 16 [12800/17352 (74%)] Loss: -182605.812500\n",
      "Train Epoch: 16 [14208/17352 (82%)] Loss: -187309.687500\n",
      "Train Epoch: 16 [15440/17352 (89%)] Loss: -112431.617188\n",
      "Train Epoch: 16 [16137/17352 (93%)] Loss: -129486.414062\n",
      "Train Epoch: 16 [16966/17352 (98%)] Loss: -129923.953125\n",
      "    epoch          : 16\n",
      "    loss           : -177449.71682964556\n",
      "    val_loss       : -98260.4550388972\n",
      "Train Epoch: 17 [128/17352 (1%)] Loss: -195558.171875\n",
      "Train Epoch: 17 [1536/17352 (9%)] Loss: -177014.812500\n",
      "Train Epoch: 17 [2944/17352 (17%)] Loss: -190245.609375\n",
      "Train Epoch: 17 [4352/17352 (25%)] Loss: -186204.687500\n",
      "Train Epoch: 17 [5760/17352 (33%)] Loss: -189047.625000\n",
      "Train Epoch: 17 [7168/17352 (41%)] Loss: -198684.562500\n",
      "Train Epoch: 17 [8576/17352 (49%)] Loss: -213329.593750\n",
      "Train Epoch: 17 [9984/17352 (58%)] Loss: -203142.312500\n",
      "Train Epoch: 17 [11392/17352 (66%)] Loss: -190824.796875\n",
      "Train Epoch: 17 [12800/17352 (74%)] Loss: -221108.156250\n",
      "Train Epoch: 17 [14208/17352 (82%)] Loss: -184422.625000\n",
      "Train Epoch: 17 [15416/17352 (89%)] Loss: -24956.435547\n",
      "Train Epoch: 17 [16301/17352 (94%)] Loss: -144855.828125\n",
      "Train Epoch: 17 [17140/17352 (99%)] Loss: -161606.062500\n",
      "    epoch          : 17\n",
      "    loss           : -179673.5961717439\n",
      "    val_loss       : -98204.94614461264\n",
      "Train Epoch: 18 [128/17352 (1%)] Loss: -170551.687500\n",
      "Train Epoch: 18 [1536/17352 (9%)] Loss: -187465.125000\n",
      "Train Epoch: 18 [2944/17352 (17%)] Loss: -229091.468750\n",
      "Train Epoch: 18 [4352/17352 (25%)] Loss: -221392.531250\n",
      "Train Epoch: 18 [5760/17352 (33%)] Loss: -184701.078125\n",
      "Train Epoch: 18 [7168/17352 (41%)] Loss: -191681.125000\n",
      "Train Epoch: 18 [8576/17352 (49%)] Loss: -193373.562500\n",
      "Train Epoch: 18 [9984/17352 (58%)] Loss: -198556.281250\n",
      "Train Epoch: 18 [11392/17352 (66%)] Loss: -198671.781250\n",
      "Train Epoch: 18 [12800/17352 (74%)] Loss: -190227.328125\n",
      "Train Epoch: 18 [14208/17352 (82%)] Loss: -194628.234375\n",
      "Train Epoch: 18 [15526/17352 (89%)] Loss: -119628.398438\n",
      "Train Epoch: 18 [16314/17352 (94%)] Loss: -22186.070312\n",
      "Train Epoch: 18 [16914/17352 (97%)] Loss: -56587.539062\n",
      "    epoch          : 18\n",
      "    loss           : -179949.94961855075\n",
      "    val_loss       : -98415.71624158224\n",
      "Train Epoch: 19 [128/17352 (1%)] Loss: -187155.265625\n",
      "Train Epoch: 19 [1536/17352 (9%)] Loss: -204749.375000\n",
      "Train Epoch: 19 [2944/17352 (17%)] Loss: -189603.218750\n",
      "Train Epoch: 19 [4352/17352 (25%)] Loss: -209926.656250\n",
      "Train Epoch: 19 [5760/17352 (33%)] Loss: -203903.281250\n",
      "Train Epoch: 19 [7168/17352 (41%)] Loss: -207392.312500\n",
      "Train Epoch: 19 [8576/17352 (49%)] Loss: -200525.781250\n",
      "Train Epoch: 19 [9984/17352 (58%)] Loss: -198218.703125\n",
      "Train Epoch: 19 [11392/17352 (66%)] Loss: -189054.531250\n",
      "Train Epoch: 19 [12800/17352 (74%)] Loss: -202938.562500\n",
      "Train Epoch: 19 [14208/17352 (82%)] Loss: -195324.765625\n",
      "Train Epoch: 19 [15564/17352 (90%)] Loss: -124240.601562\n",
      "Train Epoch: 19 [16100/17352 (93%)] Loss: -8531.023438\n",
      "Train Epoch: 19 [16899/17352 (97%)] Loss: -23847.962891\n",
      "    epoch          : 19\n",
      "    loss           : -180847.29184472002\n",
      "    val_loss       : -100248.91674509048\n",
      "Train Epoch: 20 [128/17352 (1%)] Loss: -201960.531250\n",
      "Train Epoch: 20 [1536/17352 (9%)] Loss: -193790.593750\n",
      "Train Epoch: 20 [2944/17352 (17%)] Loss: -193178.500000\n",
      "Train Epoch: 20 [4352/17352 (25%)] Loss: -190761.031250\n",
      "Train Epoch: 20 [5760/17352 (33%)] Loss: -189419.734375\n",
      "Train Epoch: 20 [7168/17352 (41%)] Loss: -195561.578125\n",
      "Train Epoch: 20 [8576/17352 (49%)] Loss: -197167.093750\n",
      "Train Epoch: 20 [9984/17352 (58%)] Loss: -205673.484375\n",
      "Train Epoch: 20 [11392/17352 (66%)] Loss: -202335.968750\n",
      "Train Epoch: 20 [12800/17352 (74%)] Loss: -222813.812500\n",
      "Train Epoch: 20 [14208/17352 (82%)] Loss: -198761.640625\n",
      "Train Epoch: 20 [15518/17352 (89%)] Loss: -114092.062500\n",
      "Train Epoch: 20 [16212/17352 (93%)] Loss: -8104.518066\n",
      "Train Epoch: 20 [16983/17352 (98%)] Loss: -129554.492188\n",
      "    epoch          : 20\n",
      "    loss           : -180936.5444598102\n",
      "    val_loss       : -98476.31226228079\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch20.pth ...\n",
      "Train Epoch: 21 [128/17352 (1%)] Loss: -176138.687500\n",
      "Train Epoch: 21 [1536/17352 (9%)] Loss: -207789.390625\n",
      "Train Epoch: 21 [2944/17352 (17%)] Loss: -217038.593750\n",
      "Train Epoch: 21 [4352/17352 (25%)] Loss: -204942.906250\n",
      "Train Epoch: 21 [5760/17352 (33%)] Loss: -192373.875000\n",
      "Train Epoch: 21 [7168/17352 (41%)] Loss: -205642.296875\n",
      "Train Epoch: 21 [8576/17352 (49%)] Loss: -185584.156250\n",
      "Train Epoch: 21 [9984/17352 (58%)] Loss: -195679.015625\n",
      "Train Epoch: 21 [11392/17352 (66%)] Loss: -190121.000000\n",
      "Train Epoch: 21 [12800/17352 (74%)] Loss: -196137.171875\n",
      "Train Epoch: 21 [14208/17352 (82%)] Loss: -219383.281250\n",
      "Train Epoch: 21 [15470/17352 (89%)] Loss: -24957.384766\n",
      "Train Epoch: 21 [16292/17352 (94%)] Loss: -79071.585938\n",
      "Train Epoch: 21 [17028/17352 (98%)] Loss: -126156.609375\n",
      "    epoch          : 21\n",
      "    loss           : -182004.6978980967\n",
      "    val_loss       : -100735.656208992\n",
      "Train Epoch: 22 [128/17352 (1%)] Loss: -211481.718750\n",
      "Train Epoch: 22 [1536/17352 (9%)] Loss: -205117.890625\n",
      "Train Epoch: 22 [2944/17352 (17%)] Loss: -210827.718750\n",
      "Train Epoch: 22 [4352/17352 (25%)] Loss: -212248.968750\n",
      "Train Epoch: 22 [5760/17352 (33%)] Loss: -179424.656250\n",
      "Train Epoch: 22 [7168/17352 (41%)] Loss: -196022.062500\n",
      "Train Epoch: 22 [8576/17352 (49%)] Loss: -190512.781250\n",
      "Train Epoch: 22 [9984/17352 (58%)] Loss: -201726.500000\n",
      "Train Epoch: 22 [11392/17352 (66%)] Loss: -197270.718750\n",
      "Train Epoch: 22 [12800/17352 (74%)] Loss: -206873.750000\n",
      "Train Epoch: 22 [14208/17352 (82%)] Loss: -222518.953125\n",
      "Train Epoch: 22 [15535/17352 (90%)] Loss: -132754.906250\n",
      "Train Epoch: 22 [16228/17352 (94%)] Loss: -129867.984375\n",
      "Train Epoch: 22 [16952/17352 (98%)] Loss: -56571.269531\n",
      "    epoch          : 22\n",
      "    loss           : -182515.23874003775\n",
      "    val_loss       : -100446.36476033529\n",
      "Train Epoch: 23 [128/17352 (1%)] Loss: -167988.406250\n",
      "Train Epoch: 23 [1536/17352 (9%)] Loss: -207728.140625\n",
      "Train Epoch: 23 [2944/17352 (17%)] Loss: -197085.281250\n",
      "Train Epoch: 23 [4352/17352 (25%)] Loss: -208737.390625\n",
      "Train Epoch: 23 [5760/17352 (33%)] Loss: -191878.187500\n",
      "Train Epoch: 23 [7168/17352 (41%)] Loss: -208885.765625\n",
      "Train Epoch: 23 [8576/17352 (49%)] Loss: -198263.203125\n",
      "Train Epoch: 23 [9984/17352 (58%)] Loss: -213054.468750\n",
      "Train Epoch: 23 [11392/17352 (66%)] Loss: -189298.453125\n",
      "Train Epoch: 23 [12800/17352 (74%)] Loss: -190417.046875\n",
      "Train Epoch: 23 [14208/17352 (82%)] Loss: -193634.640625\n",
      "Train Epoch: 23 [15557/17352 (90%)] Loss: -141683.437500\n",
      "Train Epoch: 23 [16199/17352 (93%)] Loss: -143898.656250\n",
      "Train Epoch: 23 [17002/17352 (98%)] Loss: -71670.367188\n",
      "    epoch          : 23\n",
      "    loss           : -183009.88401190226\n",
      "    val_loss       : -101129.6640867869\n",
      "Train Epoch: 24 [128/17352 (1%)] Loss: -204707.890625\n",
      "Train Epoch: 24 [1536/17352 (9%)] Loss: -214393.046875\n",
      "Train Epoch: 24 [2944/17352 (17%)] Loss: -237425.312500\n",
      "Train Epoch: 24 [4352/17352 (25%)] Loss: -185731.234375\n",
      "Train Epoch: 24 [5760/17352 (33%)] Loss: -202203.234375\n",
      "Train Epoch: 24 [7168/17352 (41%)] Loss: -196891.687500\n",
      "Train Epoch: 24 [8576/17352 (49%)] Loss: -196190.890625\n",
      "Train Epoch: 24 [9984/17352 (58%)] Loss: -188822.296875\n",
      "Train Epoch: 24 [11392/17352 (66%)] Loss: -203843.578125\n",
      "Train Epoch: 24 [12800/17352 (74%)] Loss: -199899.437500\n",
      "Train Epoch: 24 [14208/17352 (82%)] Loss: -213410.734375\n",
      "Train Epoch: 24 [15482/17352 (89%)] Loss: -137314.062500\n",
      "Train Epoch: 24 [16472/17352 (95%)] Loss: -166967.734375\n",
      "Train Epoch: 24 [17042/17352 (98%)] Loss: -121372.875000\n",
      "    epoch          : 24\n",
      "    loss           : -183170.39274197776\n",
      "    val_loss       : -100608.25349232355\n",
      "Train Epoch: 25 [128/17352 (1%)] Loss: -210389.171875\n",
      "Train Epoch: 25 [1536/17352 (9%)] Loss: -217813.093750\n",
      "Train Epoch: 25 [2944/17352 (17%)] Loss: -200493.000000\n",
      "Train Epoch: 25 [4352/17352 (25%)] Loss: -188343.937500\n",
      "Train Epoch: 25 [5760/17352 (33%)] Loss: -214523.531250\n",
      "Train Epoch: 25 [7168/17352 (41%)] Loss: -197626.062500\n",
      "Train Epoch: 25 [8576/17352 (49%)] Loss: -209017.312500\n",
      "Train Epoch: 25 [9984/17352 (58%)] Loss: -206714.703125\n",
      "Train Epoch: 25 [11392/17352 (66%)] Loss: -193982.828125\n",
      "Train Epoch: 25 [12800/17352 (74%)] Loss: -188041.500000\n",
      "Train Epoch: 25 [14208/17352 (82%)] Loss: -212060.296875\n",
      "Train Epoch: 25 [15441/17352 (89%)] Loss: -5386.890625\n",
      "Train Epoch: 25 [16208/17352 (93%)] Loss: -76781.343750\n",
      "Train Epoch: 25 [17104/17352 (99%)] Loss: -194984.625000\n",
      "    epoch          : 25\n",
      "    loss           : -182856.93959731545\n",
      "    val_loss       : -101406.89352327982\n",
      "Train Epoch: 26 [128/17352 (1%)] Loss: -212998.781250\n",
      "Train Epoch: 26 [1536/17352 (9%)] Loss: -208960.968750\n",
      "Train Epoch: 26 [2944/17352 (17%)] Loss: -198460.265625\n",
      "Train Epoch: 26 [4352/17352 (25%)] Loss: -201091.000000\n",
      "Train Epoch: 26 [5760/17352 (33%)] Loss: -207802.062500\n",
      "Train Epoch: 26 [7168/17352 (41%)] Loss: -197930.671875\n",
      "Train Epoch: 26 [8576/17352 (49%)] Loss: -179912.125000\n",
      "Train Epoch: 26 [9984/17352 (58%)] Loss: -193697.562500\n",
      "Train Epoch: 26 [11392/17352 (66%)] Loss: -194506.000000\n",
      "Train Epoch: 26 [12800/17352 (74%)] Loss: -192710.093750\n",
      "Train Epoch: 26 [14208/17352 (82%)] Loss: -214638.750000\n",
      "Train Epoch: 26 [15487/17352 (89%)] Loss: -136562.968750\n",
      "Train Epoch: 26 [16170/17352 (93%)] Loss: -138882.156250\n",
      "Train Epoch: 26 [17097/17352 (99%)] Loss: -172281.625000\n",
      "    epoch          : 26\n",
      "    loss           : -184283.71570889262\n",
      "    val_loss       : -100500.27394993305\n",
      "Train Epoch: 27 [128/17352 (1%)] Loss: -212351.609375\n",
      "Train Epoch: 27 [1536/17352 (9%)] Loss: -203411.187500\n",
      "Train Epoch: 27 [2944/17352 (17%)] Loss: -195325.578125\n",
      "Train Epoch: 27 [4352/17352 (25%)] Loss: -198858.093750\n",
      "Train Epoch: 27 [5760/17352 (33%)] Loss: -176907.937500\n",
      "Train Epoch: 27 [7168/17352 (41%)] Loss: -163424.296875\n",
      "Train Epoch: 27 [8576/17352 (49%)] Loss: -211909.843750\n",
      "Train Epoch: 27 [9984/17352 (58%)] Loss: -192034.953125\n",
      "Train Epoch: 27 [11392/17352 (66%)] Loss: -174345.968750\n",
      "Train Epoch: 27 [12800/17352 (74%)] Loss: -227378.781250\n",
      "Train Epoch: 27 [14208/17352 (82%)] Loss: -198186.843750\n",
      "Train Epoch: 27 [15483/17352 (89%)] Loss: -116993.054688\n",
      "Train Epoch: 27 [16284/17352 (94%)] Loss: -123246.687500\n",
      "Train Epoch: 27 [16897/17352 (97%)] Loss: -68596.890625\n",
      "    epoch          : 27\n",
      "    loss           : -184907.72454711093\n",
      "    val_loss       : -102215.20881627401\n",
      "Train Epoch: 28 [128/17352 (1%)] Loss: -200132.812500\n",
      "Train Epoch: 28 [1536/17352 (9%)] Loss: -213015.531250\n",
      "Train Epoch: 28 [2944/17352 (17%)] Loss: -196718.843750\n",
      "Train Epoch: 28 [4352/17352 (25%)] Loss: -214909.640625\n",
      "Train Epoch: 28 [5760/17352 (33%)] Loss: -214781.312500\n",
      "Train Epoch: 28 [7168/17352 (41%)] Loss: -198492.171875\n",
      "Train Epoch: 28 [8576/17352 (49%)] Loss: -201317.671875\n",
      "Train Epoch: 28 [9984/17352 (58%)] Loss: -189122.187500\n",
      "Train Epoch: 28 [11392/17352 (66%)] Loss: -205017.937500\n",
      "Train Epoch: 28 [12800/17352 (74%)] Loss: -203368.343750\n",
      "Train Epoch: 28 [14208/17352 (82%)] Loss: -222896.843750\n",
      "Train Epoch: 28 [15559/17352 (90%)] Loss: -123553.203125\n",
      "Train Epoch: 28 [16231/17352 (94%)] Loss: -8626.584961\n",
      "Train Epoch: 28 [17091/17352 (98%)] Loss: -153639.703125\n",
      "    epoch          : 28\n",
      "    loss           : -185719.30846594484\n",
      "    val_loss       : -102321.149138546\n",
      "Train Epoch: 29 [128/17352 (1%)] Loss: -180234.281250\n",
      "Train Epoch: 29 [1536/17352 (9%)] Loss: -201663.796875\n",
      "Train Epoch: 29 [2944/17352 (17%)] Loss: -215745.968750\n",
      "Train Epoch: 29 [4352/17352 (25%)] Loss: -177398.906250\n",
      "Train Epoch: 29 [5760/17352 (33%)] Loss: -224324.593750\n",
      "Train Epoch: 29 [7168/17352 (41%)] Loss: -242040.875000\n",
      "Train Epoch: 29 [8576/17352 (49%)] Loss: -186276.500000\n",
      "Train Epoch: 29 [9984/17352 (58%)] Loss: -186503.859375\n",
      "Train Epoch: 29 [11392/17352 (66%)] Loss: -214876.234375\n",
      "Train Epoch: 29 [12800/17352 (74%)] Loss: -192148.406250\n",
      "Train Epoch: 29 [14208/17352 (82%)] Loss: -203217.562500\n",
      "Train Epoch: 29 [15506/17352 (89%)] Loss: -170000.250000\n",
      "Train Epoch: 29 [16298/17352 (94%)] Loss: -121884.070312\n",
      "Train Epoch: 29 [16978/17352 (98%)] Loss: -25640.722656\n",
      "    epoch          : 29\n",
      "    loss           : -186322.78762714975\n",
      "    val_loss       : -102813.12895037333\n",
      "Train Epoch: 30 [128/17352 (1%)] Loss: -212547.234375\n",
      "Train Epoch: 30 [1536/17352 (9%)] Loss: -199648.937500\n",
      "Train Epoch: 30 [2944/17352 (17%)] Loss: -193651.875000\n",
      "Train Epoch: 30 [4352/17352 (25%)] Loss: -225830.453125\n",
      "Train Epoch: 30 [5760/17352 (33%)] Loss: -208779.796875\n",
      "Train Epoch: 30 [7168/17352 (41%)] Loss: -213015.843750\n",
      "Train Epoch: 30 [8576/17352 (49%)] Loss: -227649.406250\n",
      "Train Epoch: 30 [9984/17352 (58%)] Loss: -181932.312500\n",
      "Train Epoch: 30 [11392/17352 (66%)] Loss: -203654.093750\n",
      "Train Epoch: 30 [12800/17352 (74%)] Loss: -206644.203125\n",
      "Train Epoch: 30 [14208/17352 (82%)] Loss: -230320.812500\n",
      "Train Epoch: 30 [15465/17352 (89%)] Loss: -135342.437500\n",
      "Train Epoch: 30 [16356/17352 (94%)] Loss: -107990.718750\n",
      "Train Epoch: 30 [16998/17352 (98%)] Loss: -137299.093750\n",
      "    epoch          : 30\n",
      "    loss           : -187445.93465223888\n",
      "    val_loss       : -102548.18406244913\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch30.pth ...\n",
      "Train Epoch: 31 [128/17352 (1%)] Loss: -205756.000000\n",
      "Train Epoch: 31 [1536/17352 (9%)] Loss: -184231.453125\n",
      "Train Epoch: 31 [2944/17352 (17%)] Loss: -222322.015625\n",
      "Train Epoch: 31 [4352/17352 (25%)] Loss: -222382.625000\n",
      "Train Epoch: 31 [5760/17352 (33%)] Loss: -193823.437500\n",
      "Train Epoch: 31 [7168/17352 (41%)] Loss: -206330.000000\n",
      "Train Epoch: 31 [8576/17352 (49%)] Loss: -229236.375000\n",
      "Train Epoch: 31 [9984/17352 (58%)] Loss: -214450.687500\n",
      "Train Epoch: 31 [11392/17352 (66%)] Loss: -190749.031250\n",
      "Train Epoch: 31 [12800/17352 (74%)] Loss: -191863.375000\n",
      "Train Epoch: 31 [14208/17352 (82%)] Loss: -206426.593750\n",
      "Train Epoch: 31 [15541/17352 (90%)] Loss: -130481.046875\n",
      "Train Epoch: 31 [16243/17352 (94%)] Loss: -76614.796875\n",
      "Train Epoch: 31 [17010/17352 (98%)] Loss: -26597.269531\n",
      "    epoch          : 31\n",
      "    loss           : -186546.62709076132\n",
      "    val_loss       : -102887.0972627004\n",
      "Train Epoch: 32 [128/17352 (1%)] Loss: -212472.593750\n",
      "Train Epoch: 32 [1536/17352 (9%)] Loss: -190660.406250\n",
      "Train Epoch: 32 [2944/17352 (17%)] Loss: -230658.203125\n",
      "Train Epoch: 32 [4352/17352 (25%)] Loss: -203288.343750\n",
      "Train Epoch: 32 [5760/17352 (33%)] Loss: -202509.765625\n",
      "Train Epoch: 32 [7168/17352 (41%)] Loss: -207214.390625\n",
      "Train Epoch: 32 [8576/17352 (49%)] Loss: -202274.671875\n",
      "Train Epoch: 32 [9984/17352 (58%)] Loss: -199140.437500\n",
      "Train Epoch: 32 [11392/17352 (66%)] Loss: -203423.750000\n",
      "Train Epoch: 32 [12800/17352 (74%)] Loss: -217237.500000\n",
      "Train Epoch: 32 [14208/17352 (82%)] Loss: -231797.078125\n",
      "Train Epoch: 32 [15496/17352 (89%)] Loss: -77858.054688\n",
      "Train Epoch: 32 [16150/17352 (93%)] Loss: -121012.187500\n",
      "Train Epoch: 32 [16959/17352 (98%)] Loss: -204278.421875\n",
      "    epoch          : 32\n",
      "    loss           : -186533.9546586619\n",
      "    val_loss       : -102226.9923113505\n",
      "Train Epoch: 33 [128/17352 (1%)] Loss: -210117.406250\n",
      "Train Epoch: 33 [1536/17352 (9%)] Loss: -210903.328125\n",
      "Train Epoch: 33 [2944/17352 (17%)] Loss: -242890.656250\n",
      "Train Epoch: 33 [4352/17352 (25%)] Loss: -210498.296875\n",
      "Train Epoch: 33 [5760/17352 (33%)] Loss: -199388.234375\n",
      "Train Epoch: 33 [7168/17352 (41%)] Loss: -199275.031250\n",
      "Train Epoch: 33 [8576/17352 (49%)] Loss: -209211.828125\n",
      "Train Epoch: 33 [9984/17352 (58%)] Loss: -211625.875000\n",
      "Train Epoch: 33 [11392/17352 (66%)] Loss: -195718.890625\n",
      "Train Epoch: 33 [12800/17352 (74%)] Loss: -201414.312500\n",
      "Train Epoch: 33 [14208/17352 (82%)] Loss: -205167.750000\n",
      "Train Epoch: 33 [15502/17352 (89%)] Loss: -26515.224609\n",
      "Train Epoch: 33 [16333/17352 (94%)] Loss: -184740.109375\n",
      "Train Epoch: 33 [16953/17352 (98%)] Loss: -8442.498047\n",
      "    epoch          : 33\n",
      "    loss           : -187870.75329999474\n",
      "    val_loss       : -103075.48602759044\n",
      "Train Epoch: 34 [128/17352 (1%)] Loss: -209195.937500\n",
      "Train Epoch: 34 [1536/17352 (9%)] Loss: -216848.031250\n",
      "Train Epoch: 34 [2944/17352 (17%)] Loss: -211225.859375\n",
      "Train Epoch: 34 [4352/17352 (25%)] Loss: -202919.890625\n",
      "Train Epoch: 34 [5760/17352 (33%)] Loss: -202535.765625\n",
      "Train Epoch: 34 [7168/17352 (41%)] Loss: -184720.796875\n",
      "Train Epoch: 34 [8576/17352 (49%)] Loss: -234239.593750\n",
      "Train Epoch: 34 [9984/17352 (58%)] Loss: -179908.171875\n",
      "Train Epoch: 34 [11392/17352 (66%)] Loss: -171927.156250\n",
      "Train Epoch: 34 [12800/17352 (74%)] Loss: -212565.843750\n",
      "Train Epoch: 34 [14208/17352 (82%)] Loss: -214782.796875\n",
      "Train Epoch: 34 [15429/17352 (89%)] Loss: -87054.750000\n",
      "Train Epoch: 34 [16268/17352 (94%)] Loss: -172960.859375\n",
      "Train Epoch: 34 [17025/17352 (98%)] Loss: -118826.046875\n",
      "    epoch          : 34\n",
      "    loss           : -187860.7509405149\n",
      "    val_loss       : -104186.28999315898\n",
      "Train Epoch: 35 [128/17352 (1%)] Loss: -179660.718750\n",
      "Train Epoch: 35 [1536/17352 (9%)] Loss: -197509.343750\n",
      "Train Epoch: 35 [2944/17352 (17%)] Loss: -250955.609375\n",
      "Train Epoch: 35 [4352/17352 (25%)] Loss: -210062.296875\n",
      "Train Epoch: 35 [5760/17352 (33%)] Loss: -196671.062500\n",
      "Train Epoch: 35 [7168/17352 (41%)] Loss: -188501.125000\n",
      "Train Epoch: 35 [8576/17352 (49%)] Loss: -237946.968750\n",
      "Train Epoch: 35 [9984/17352 (58%)] Loss: -212544.078125\n",
      "Train Epoch: 35 [11392/17352 (66%)] Loss: -188124.562500\n",
      "Train Epoch: 35 [12800/17352 (74%)] Loss: -200119.406250\n",
      "Train Epoch: 35 [14208/17352 (82%)] Loss: -220536.609375\n",
      "Train Epoch: 35 [15546/17352 (90%)] Loss: -141607.078125\n",
      "Train Epoch: 35 [16448/17352 (95%)] Loss: -131834.750000\n",
      "Train Epoch: 35 [17093/17352 (99%)] Loss: -127860.750000\n",
      "    epoch          : 35\n",
      "    loss           : -189148.7319893037\n",
      "    val_loss       : -103944.26175676982\n",
      "Train Epoch: 36 [128/17352 (1%)] Loss: -195353.093750\n",
      "Train Epoch: 36 [1536/17352 (9%)] Loss: -194652.000000\n",
      "Train Epoch: 36 [2944/17352 (17%)] Loss: -199478.906250\n",
      "Train Epoch: 36 [4352/17352 (25%)] Loss: -213352.078125\n",
      "Train Epoch: 36 [5760/17352 (33%)] Loss: -185407.328125\n",
      "Train Epoch: 36 [7168/17352 (41%)] Loss: -179117.500000\n",
      "Train Epoch: 36 [8576/17352 (49%)] Loss: -212169.015625\n",
      "Train Epoch: 36 [9984/17352 (58%)] Loss: -199785.968750\n",
      "Train Epoch: 36 [11392/17352 (66%)] Loss: -206158.578125\n",
      "Train Epoch: 36 [12800/17352 (74%)] Loss: -188575.781250\n",
      "Train Epoch: 36 [14208/17352 (82%)] Loss: -200057.734375\n",
      "Train Epoch: 36 [15533/17352 (90%)] Loss: -133394.109375\n",
      "Train Epoch: 36 [16193/17352 (93%)] Loss: -70941.609375\n",
      "Train Epoch: 36 [16912/17352 (97%)] Loss: -81051.984375\n",
      "    epoch          : 36\n",
      "    loss           : -188338.0768403943\n",
      "    val_loss       : -104558.64116468429\n",
      "Train Epoch: 37 [128/17352 (1%)] Loss: -217130.265625\n",
      "Train Epoch: 37 [1536/17352 (9%)] Loss: -208691.593750\n",
      "Train Epoch: 37 [2944/17352 (17%)] Loss: -245370.046875\n",
      "Train Epoch: 37 [4352/17352 (25%)] Loss: -218211.875000\n",
      "Train Epoch: 37 [5760/17352 (33%)] Loss: -224286.218750\n",
      "Train Epoch: 37 [7168/17352 (41%)] Loss: -197344.343750\n",
      "Train Epoch: 37 [8576/17352 (49%)] Loss: -186341.703125\n",
      "Train Epoch: 37 [9984/17352 (58%)] Loss: -218047.250000\n",
      "Train Epoch: 37 [11392/17352 (66%)] Loss: -202767.453125\n",
      "Train Epoch: 37 [12800/17352 (74%)] Loss: -212161.046875\n",
      "Train Epoch: 37 [14208/17352 (82%)] Loss: -203500.796875\n",
      "Train Epoch: 37 [15480/17352 (89%)] Loss: -131607.500000\n",
      "Train Epoch: 37 [16223/17352 (93%)] Loss: -161221.734375\n",
      "Train Epoch: 37 [16956/17352 (98%)] Loss: -139525.015625\n",
      "    epoch          : 37\n",
      "    loss           : -188673.2796049182\n",
      "    val_loss       : -104176.73684870402\n",
      "Train Epoch: 38 [128/17352 (1%)] Loss: -194176.296875\n",
      "Train Epoch: 38 [1536/17352 (9%)] Loss: -197576.906250\n",
      "Train Epoch: 38 [2944/17352 (17%)] Loss: -192324.265625\n",
      "Train Epoch: 38 [4352/17352 (25%)] Loss: -217136.593750\n",
      "Train Epoch: 38 [5760/17352 (33%)] Loss: -226062.921875\n",
      "Train Epoch: 38 [7168/17352 (41%)] Loss: -191456.859375\n",
      "Train Epoch: 38 [8576/17352 (49%)] Loss: -198736.234375\n",
      "Train Epoch: 38 [9984/17352 (58%)] Loss: -213215.406250\n",
      "Train Epoch: 38 [11392/17352 (66%)] Loss: -189661.656250\n",
      "Train Epoch: 38 [12800/17352 (74%)] Loss: -192885.625000\n",
      "Train Epoch: 38 [14208/17352 (82%)] Loss: -209603.031250\n",
      "Train Epoch: 38 [15511/17352 (89%)] Loss: -117494.695312\n",
      "Train Epoch: 38 [16175/17352 (93%)] Loss: -134652.515625\n",
      "Train Epoch: 38 [17092/17352 (99%)] Loss: -161654.750000\n",
      "    epoch          : 38\n",
      "    loss           : -189133.01276740772\n",
      "    val_loss       : -102467.45508208274\n",
      "Train Epoch: 39 [128/17352 (1%)] Loss: -219992.828125\n",
      "Train Epoch: 39 [1536/17352 (9%)] Loss: -204544.687500\n",
      "Train Epoch: 39 [2944/17352 (17%)] Loss: -190997.375000\n",
      "Train Epoch: 39 [4352/17352 (25%)] Loss: -219201.203125\n",
      "Train Epoch: 39 [5760/17352 (33%)] Loss: -212856.750000\n",
      "Train Epoch: 39 [7168/17352 (41%)] Loss: -200781.531250\n",
      "Train Epoch: 39 [8576/17352 (49%)] Loss: -200072.937500\n",
      "Train Epoch: 39 [9984/17352 (58%)] Loss: -215133.281250\n",
      "Train Epoch: 39 [11392/17352 (66%)] Loss: -216781.187500\n",
      "Train Epoch: 39 [12800/17352 (74%)] Loss: -208759.687500\n",
      "Train Epoch: 39 [14208/17352 (82%)] Loss: -220960.156250\n",
      "Train Epoch: 39 [15505/17352 (89%)] Loss: -76721.929688\n",
      "Train Epoch: 39 [16207/17352 (93%)] Loss: -82034.734375\n",
      "Train Epoch: 39 [17172/17352 (99%)] Loss: -145553.781250\n",
      "    epoch          : 39\n",
      "    loss           : -190463.54009477244\n",
      "    val_loss       : -104455.55277338823\n",
      "Train Epoch: 40 [128/17352 (1%)] Loss: -209380.453125\n",
      "Train Epoch: 40 [1536/17352 (9%)] Loss: -205678.968750\n",
      "Train Epoch: 40 [2944/17352 (17%)] Loss: -252663.765625\n",
      "Train Epoch: 40 [4352/17352 (25%)] Loss: -226959.843750\n",
      "Train Epoch: 40 [5760/17352 (33%)] Loss: -204997.593750\n",
      "Train Epoch: 40 [7168/17352 (41%)] Loss: -204510.671875\n",
      "Train Epoch: 40 [8576/17352 (49%)] Loss: -216023.156250\n",
      "Train Epoch: 40 [9984/17352 (58%)] Loss: -207060.562500\n",
      "Train Epoch: 40 [11392/17352 (66%)] Loss: -214448.265625\n",
      "Train Epoch: 40 [12800/17352 (74%)] Loss: -206125.875000\n",
      "Train Epoch: 40 [14208/17352 (82%)] Loss: -211156.406250\n",
      "Train Epoch: 40 [15440/17352 (89%)] Loss: -8522.687500\n",
      "Train Epoch: 40 [16172/17352 (93%)] Loss: -132808.281250\n",
      "Train Epoch: 40 [17080/17352 (98%)] Loss: -143578.593750\n",
      "    epoch          : 40\n",
      "    loss           : -190620.23564977455\n",
      "    val_loss       : -104720.87905101776\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch40.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 41 [128/17352 (1%)] Loss: -220113.421875\n",
      "Train Epoch: 41 [1536/17352 (9%)] Loss: -223780.375000\n",
      "Train Epoch: 41 [2944/17352 (17%)] Loss: -202463.906250\n",
      "Train Epoch: 41 [4352/17352 (25%)] Loss: -206695.265625\n",
      "Train Epoch: 41 [5760/17352 (33%)] Loss: -212879.921875\n",
      "Train Epoch: 41 [7168/17352 (41%)] Loss: -199620.875000\n",
      "Train Epoch: 41 [8576/17352 (49%)] Loss: -249841.312500\n",
      "Train Epoch: 41 [9984/17352 (58%)] Loss: -208582.281250\n",
      "Train Epoch: 41 [11392/17352 (66%)] Loss: -207213.531250\n",
      "Train Epoch: 41 [12800/17352 (74%)] Loss: -218048.718750\n",
      "Train Epoch: 41 [14208/17352 (82%)] Loss: -219282.250000\n",
      "Train Epoch: 41 [15499/17352 (89%)] Loss: -175861.000000\n",
      "Train Epoch: 41 [16193/17352 (93%)] Loss: -137639.375000\n",
      "Train Epoch: 41 [17008/17352 (98%)] Loss: -62520.945312\n",
      "    epoch          : 41\n",
      "    loss           : -189598.50840892407\n",
      "    val_loss       : -105080.33192672729\n",
      "Train Epoch: 42 [128/17352 (1%)] Loss: -221526.312500\n",
      "Train Epoch: 42 [1536/17352 (9%)] Loss: -201211.062500\n",
      "Train Epoch: 42 [2944/17352 (17%)] Loss: -229139.187500\n",
      "Train Epoch: 42 [4352/17352 (25%)] Loss: -232858.906250\n",
      "Train Epoch: 42 [5760/17352 (33%)] Loss: -205152.687500\n",
      "Train Epoch: 42 [7168/17352 (41%)] Loss: -203736.500000\n",
      "Train Epoch: 42 [8576/17352 (49%)] Loss: -215760.750000\n",
      "Train Epoch: 42 [9984/17352 (58%)] Loss: -231018.890625\n",
      "Train Epoch: 42 [11392/17352 (66%)] Loss: -202670.453125\n",
      "Train Epoch: 42 [12800/17352 (74%)] Loss: -204152.781250\n",
      "Train Epoch: 42 [14208/17352 (82%)] Loss: -193483.031250\n",
      "Train Epoch: 42 [15459/17352 (89%)] Loss: -24693.666016\n",
      "Train Epoch: 42 [16194/17352 (93%)] Loss: -142580.750000\n",
      "Train Epoch: 42 [17020/17352 (98%)] Loss: -175639.250000\n",
      "    epoch          : 42\n",
      "    loss           : -189118.74765362835\n",
      "    val_loss       : -105586.19282592137\n",
      "Train Epoch: 43 [128/17352 (1%)] Loss: -217914.968750\n",
      "Train Epoch: 43 [1536/17352 (9%)] Loss: -199205.640625\n",
      "Train Epoch: 43 [2944/17352 (17%)] Loss: -202232.375000\n",
      "Train Epoch: 43 [4352/17352 (25%)] Loss: -215870.046875\n",
      "Train Epoch: 43 [5760/17352 (33%)] Loss: -199141.296875\n",
      "Train Epoch: 43 [7168/17352 (41%)] Loss: -204744.937500\n",
      "Train Epoch: 43 [8576/17352 (49%)] Loss: -201526.250000\n",
      "Train Epoch: 43 [9984/17352 (58%)] Loss: -183641.625000\n",
      "Train Epoch: 43 [11392/17352 (66%)] Loss: -205148.312500\n",
      "Train Epoch: 43 [12800/17352 (74%)] Loss: -218214.250000\n",
      "Train Epoch: 43 [14208/17352 (82%)] Loss: -216960.125000\n",
      "Train Epoch: 43 [15521/17352 (89%)] Loss: -210277.375000\n",
      "Train Epoch: 43 [16217/17352 (93%)] Loss: -23042.937500\n",
      "Train Epoch: 43 [17008/17352 (98%)] Loss: -140283.671875\n",
      "    epoch          : 43\n",
      "    loss           : -190559.30546088508\n",
      "    val_loss       : -105800.19333918889\n",
      "Train Epoch: 44 [128/17352 (1%)] Loss: -194567.156250\n",
      "Train Epoch: 44 [1536/17352 (9%)] Loss: -216262.453125\n",
      "Train Epoch: 44 [2944/17352 (17%)] Loss: -217289.500000\n",
      "Train Epoch: 44 [4352/17352 (25%)] Loss: -218948.250000\n",
      "Train Epoch: 44 [5760/17352 (33%)] Loss: -210179.921875\n",
      "Train Epoch: 44 [7168/17352 (41%)] Loss: -205397.500000\n",
      "Train Epoch: 44 [8576/17352 (49%)] Loss: -218104.781250\n",
      "Train Epoch: 44 [9984/17352 (58%)] Loss: -208036.203125\n",
      "Train Epoch: 44 [11392/17352 (66%)] Loss: -198933.281250\n",
      "Train Epoch: 44 [12800/17352 (74%)] Loss: -209528.093750\n",
      "Train Epoch: 44 [14208/17352 (82%)] Loss: -209562.843750\n",
      "Train Epoch: 44 [15492/17352 (89%)] Loss: -9138.533203\n",
      "Train Epoch: 44 [16265/17352 (94%)] Loss: -77067.953125\n",
      "Train Epoch: 44 [17124/17352 (99%)] Loss: -120673.500000\n",
      "    epoch          : 44\n",
      "    loss           : -191017.84968474729\n",
      "    val_loss       : -105247.37223815918\n",
      "Train Epoch: 45 [128/17352 (1%)] Loss: -218850.593750\n",
      "Train Epoch: 45 [1536/17352 (9%)] Loss: -216461.890625\n",
      "Train Epoch: 45 [2944/17352 (17%)] Loss: -203927.062500\n",
      "Train Epoch: 45 [4352/17352 (25%)] Loss: -227926.109375\n",
      "Train Epoch: 45 [5760/17352 (33%)] Loss: -214774.953125\n",
      "Train Epoch: 45 [7168/17352 (41%)] Loss: -211244.921875\n",
      "Train Epoch: 45 [8576/17352 (49%)] Loss: -217266.515625\n",
      "Train Epoch: 45 [9984/17352 (58%)] Loss: -214408.546875\n",
      "Train Epoch: 45 [11392/17352 (66%)] Loss: -218077.281250\n",
      "Train Epoch: 45 [12800/17352 (74%)] Loss: -203214.359375\n",
      "Train Epoch: 45 [14208/17352 (82%)] Loss: -200862.718750\n",
      "Train Epoch: 45 [15532/17352 (90%)] Loss: -132954.609375\n",
      "Train Epoch: 45 [16287/17352 (94%)] Loss: -117175.968750\n",
      "Train Epoch: 45 [17138/17352 (99%)] Loss: -123725.648438\n",
      "    epoch          : 45\n",
      "    loss           : -191399.78068634646\n",
      "    val_loss       : -106080.72756147385\n",
      "Train Epoch: 46 [128/17352 (1%)] Loss: -213721.000000\n",
      "Train Epoch: 46 [1536/17352 (9%)] Loss: -197206.968750\n",
      "Train Epoch: 46 [2944/17352 (17%)] Loss: -199733.359375\n",
      "Train Epoch: 46 [4352/17352 (25%)] Loss: -199836.421875\n",
      "Train Epoch: 46 [5760/17352 (33%)] Loss: -201231.781250\n",
      "Train Epoch: 46 [7168/17352 (41%)] Loss: -209899.546875\n",
      "Train Epoch: 46 [8576/17352 (49%)] Loss: -195737.437500\n",
      "Train Epoch: 46 [9984/17352 (58%)] Loss: -217537.765625\n",
      "Train Epoch: 46 [11392/17352 (66%)] Loss: -199122.531250\n",
      "Train Epoch: 46 [12800/17352 (74%)] Loss: -230697.218750\n",
      "Train Epoch: 46 [14208/17352 (82%)] Loss: -196032.406250\n",
      "Train Epoch: 46 [15529/17352 (89%)] Loss: -87557.804688\n",
      "Train Epoch: 46 [16305/17352 (94%)] Loss: -61740.054688\n",
      "Train Epoch: 46 [17088/17352 (98%)] Loss: -9072.906250\n",
      "    epoch          : 46\n",
      "    loss           : -191467.233309957\n",
      "    val_loss       : -104185.39337315559\n",
      "Train Epoch: 47 [128/17352 (1%)] Loss: -198163.062500\n",
      "Train Epoch: 47 [1536/17352 (9%)] Loss: -202569.718750\n",
      "Train Epoch: 47 [2944/17352 (17%)] Loss: -212573.687500\n",
      "Train Epoch: 47 [4352/17352 (25%)] Loss: -206726.468750\n",
      "Train Epoch: 47 [5760/17352 (33%)] Loss: -200773.390625\n",
      "Train Epoch: 47 [7168/17352 (41%)] Loss: -222026.968750\n",
      "Train Epoch: 47 [8576/17352 (49%)] Loss: -238359.343750\n",
      "Train Epoch: 47 [9984/17352 (58%)] Loss: -212602.718750\n",
      "Train Epoch: 47 [11392/17352 (66%)] Loss: -210993.078125\n",
      "Train Epoch: 47 [12800/17352 (74%)] Loss: -201865.875000\n",
      "Train Epoch: 47 [14208/17352 (82%)] Loss: -208805.906250\n",
      "Train Epoch: 47 [15478/17352 (89%)] Loss: -4958.265625\n",
      "Train Epoch: 47 [16148/17352 (93%)] Loss: -136911.812500\n",
      "Train Epoch: 47 [16970/17352 (98%)] Loss: -153890.093750\n",
      "    epoch          : 47\n",
      "    loss           : -191904.54464660236\n",
      "    val_loss       : -105650.8692931811\n",
      "Train Epoch: 48 [128/17352 (1%)] Loss: -193071.656250\n",
      "Train Epoch: 48 [1536/17352 (9%)] Loss: -207503.468750\n",
      "Train Epoch: 48 [2944/17352 (17%)] Loss: -220895.421875\n",
      "Train Epoch: 48 [4352/17352 (25%)] Loss: -209322.781250\n",
      "Train Epoch: 48 [5760/17352 (33%)] Loss: -221446.875000\n",
      "Train Epoch: 48 [7168/17352 (41%)] Loss: -213089.171875\n",
      "Train Epoch: 48 [8576/17352 (49%)] Loss: -209377.140625\n",
      "Train Epoch: 48 [9984/17352 (58%)] Loss: -227814.781250\n",
      "Train Epoch: 48 [11392/17352 (66%)] Loss: -204465.718750\n",
      "Train Epoch: 48 [12800/17352 (74%)] Loss: -207015.218750\n",
      "Train Epoch: 48 [14208/17352 (82%)] Loss: -211448.437500\n",
      "Train Epoch: 48 [15515/17352 (89%)] Loss: -77593.859375\n",
      "Train Epoch: 48 [16234/17352 (94%)] Loss: -75020.359375\n",
      "Train Epoch: 48 [16971/17352 (98%)] Loss: -125509.359375\n",
      "    epoch          : 48\n",
      "    loss           : -191426.2386712196\n",
      "    val_loss       : -104351.44771620432\n",
      "Train Epoch: 49 [128/17352 (1%)] Loss: -190165.187500\n",
      "Train Epoch: 49 [1536/17352 (9%)] Loss: -224099.593750\n",
      "Train Epoch: 49 [2944/17352 (17%)] Loss: -211825.234375\n",
      "Train Epoch: 49 [4352/17352 (25%)] Loss: -231700.218750\n",
      "Train Epoch: 49 [5760/17352 (33%)] Loss: -221744.812500\n",
      "Train Epoch: 49 [7168/17352 (41%)] Loss: -218780.281250\n",
      "Train Epoch: 49 [8576/17352 (49%)] Loss: -203268.125000\n",
      "Train Epoch: 49 [9984/17352 (58%)] Loss: -204126.750000\n",
      "Train Epoch: 49 [11392/17352 (66%)] Loss: -218322.968750\n",
      "Train Epoch: 49 [12800/17352 (74%)] Loss: -220098.359375\n",
      "Train Epoch: 49 [14208/17352 (82%)] Loss: -224233.265625\n",
      "Train Epoch: 49 [15425/17352 (89%)] Loss: -80386.671875\n",
      "Train Epoch: 49 [16093/17352 (93%)] Loss: -8101.113281\n",
      "Train Epoch: 49 [16944/17352 (98%)] Loss: -120375.687500\n",
      "    epoch          : 49\n",
      "    loss           : -191684.4073510906\n",
      "    val_loss       : -107024.6586687088\n",
      "Train Epoch: 50 [128/17352 (1%)] Loss: -219442.281250\n",
      "Train Epoch: 50 [1536/17352 (9%)] Loss: -194101.453125\n",
      "Train Epoch: 50 [2944/17352 (17%)] Loss: -187313.015625\n",
      "Train Epoch: 50 [4352/17352 (25%)] Loss: -226086.421875\n",
      "Train Epoch: 50 [5760/17352 (33%)] Loss: -230774.875000\n",
      "Train Epoch: 50 [7168/17352 (41%)] Loss: -189298.296875\n",
      "Train Epoch: 50 [8576/17352 (49%)] Loss: -205575.593750\n",
      "Train Epoch: 50 [9984/17352 (58%)] Loss: -209454.562500\n",
      "Train Epoch: 50 [11392/17352 (66%)] Loss: -213013.093750\n",
      "Train Epoch: 50 [12800/17352 (74%)] Loss: -208302.609375\n",
      "Train Epoch: 50 [14208/17352 (82%)] Loss: -210733.203125\n",
      "Train Epoch: 50 [15546/17352 (90%)] Loss: -136942.812500\n",
      "Train Epoch: 50 [16250/17352 (94%)] Loss: -62448.574219\n",
      "Train Epoch: 50 [17055/17352 (98%)] Loss: -182515.656250\n",
      "    epoch          : 50\n",
      "    loss           : -192540.40888802957\n",
      "    val_loss       : -105193.02708965937\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [128/17352 (1%)] Loss: -217849.906250\n",
      "Train Epoch: 51 [1536/17352 (9%)] Loss: -221027.609375\n",
      "Train Epoch: 51 [2944/17352 (17%)] Loss: -236854.671875\n",
      "Train Epoch: 51 [4352/17352 (25%)] Loss: -194237.000000\n",
      "Train Epoch: 51 [5760/17352 (33%)] Loss: -222727.484375\n",
      "Train Epoch: 51 [7168/17352 (41%)] Loss: -188340.343750\n",
      "Train Epoch: 51 [8576/17352 (49%)] Loss: -218087.812500\n",
      "Train Epoch: 51 [9984/17352 (58%)] Loss: -220520.468750\n",
      "Train Epoch: 51 [11392/17352 (66%)] Loss: -222321.015625\n",
      "Train Epoch: 51 [12800/17352 (74%)] Loss: -214330.859375\n",
      "Train Epoch: 51 [14208/17352 (82%)] Loss: -217914.296875\n",
      "Train Epoch: 51 [15448/17352 (89%)] Loss: -25709.968750\n",
      "Train Epoch: 51 [16223/17352 (93%)] Loss: -8364.554688\n",
      "Train Epoch: 51 [16953/17352 (98%)] Loss: -80556.929688\n",
      "    epoch          : 51\n",
      "    loss           : -192339.66984977978\n",
      "    val_loss       : -105984.0562737465\n",
      "Train Epoch: 52 [128/17352 (1%)] Loss: -194615.156250\n",
      "Train Epoch: 52 [1536/17352 (9%)] Loss: -219703.875000\n",
      "Train Epoch: 52 [2944/17352 (17%)] Loss: -236778.000000\n",
      "Train Epoch: 52 [4352/17352 (25%)] Loss: -217918.531250\n",
      "Train Epoch: 52 [5760/17352 (33%)] Loss: -212116.859375\n",
      "Train Epoch: 52 [7168/17352 (41%)] Loss: -212281.593750\n",
      "Train Epoch: 52 [8576/17352 (49%)] Loss: -198967.125000\n",
      "Train Epoch: 52 [9984/17352 (58%)] Loss: -188371.296875\n",
      "Train Epoch: 52 [11392/17352 (66%)] Loss: -181975.562500\n",
      "Train Epoch: 52 [12800/17352 (74%)] Loss: -218519.687500\n",
      "Train Epoch: 52 [14208/17352 (82%)] Loss: -215955.812500\n",
      "Train Epoch: 52 [15554/17352 (90%)] Loss: -156700.328125\n",
      "Train Epoch: 52 [16152/17352 (93%)] Loss: -135406.031250\n",
      "Train Epoch: 52 [17020/17352 (98%)] Loss: -142910.406250\n",
      "    epoch          : 52\n",
      "    loss           : -193493.5679857907\n",
      "    val_loss       : -106296.01958929698\n",
      "Train Epoch: 53 [128/17352 (1%)] Loss: -194579.171875\n",
      "Train Epoch: 53 [1536/17352 (9%)] Loss: -233200.390625\n",
      "Train Epoch: 53 [2944/17352 (17%)] Loss: -190649.406250\n",
      "Train Epoch: 53 [4352/17352 (25%)] Loss: -205909.656250\n",
      "Train Epoch: 53 [5760/17352 (33%)] Loss: -202584.593750\n",
      "Train Epoch: 53 [7168/17352 (41%)] Loss: -206847.546875\n",
      "Train Epoch: 53 [8576/17352 (49%)] Loss: -190333.234375\n",
      "Train Epoch: 53 [9984/17352 (58%)] Loss: -236085.390625\n",
      "Train Epoch: 53 [11392/17352 (66%)] Loss: -210066.062500\n",
      "Train Epoch: 53 [12800/17352 (74%)] Loss: -203518.812500\n",
      "Train Epoch: 53 [14208/17352 (82%)] Loss: -215687.156250\n",
      "Train Epoch: 53 [15488/17352 (89%)] Loss: -72331.945312\n",
      "Train Epoch: 53 [16279/17352 (94%)] Loss: -78789.375000\n",
      "Train Epoch: 53 [17004/17352 (98%)] Loss: -170890.468750\n",
      "    epoch          : 53\n",
      "    loss           : -192410.86152147126\n",
      "    val_loss       : -106102.72865327199\n",
      "Train Epoch: 54 [128/17352 (1%)] Loss: -223734.937500\n",
      "Train Epoch: 54 [1536/17352 (9%)] Loss: -214629.875000\n",
      "Train Epoch: 54 [2944/17352 (17%)] Loss: -218770.750000\n",
      "Train Epoch: 54 [4352/17352 (25%)] Loss: -221510.781250\n",
      "Train Epoch: 54 [5760/17352 (33%)] Loss: -204768.593750\n",
      "Train Epoch: 54 [7168/17352 (41%)] Loss: -229646.640625\n",
      "Train Epoch: 54 [8576/17352 (49%)] Loss: -210097.187500\n",
      "Train Epoch: 54 [9984/17352 (58%)] Loss: -224618.000000\n",
      "Train Epoch: 54 [11392/17352 (66%)] Loss: -200410.812500\n",
      "Train Epoch: 54 [12800/17352 (74%)] Loss: -204502.906250\n",
      "Train Epoch: 54 [14208/17352 (82%)] Loss: -201409.000000\n",
      "Train Epoch: 54 [15464/17352 (89%)] Loss: -79650.851562\n",
      "Train Epoch: 54 [16350/17352 (94%)] Loss: -60568.632812\n",
      "Train Epoch: 54 [17080/17352 (98%)] Loss: -24260.832031\n",
      "    epoch          : 54\n",
      "    loss           : -193361.9184177066\n",
      "    val_loss       : -106231.54842076103\n",
      "Train Epoch: 55 [128/17352 (1%)] Loss: -224831.078125\n",
      "Train Epoch: 55 [1536/17352 (9%)] Loss: -223301.843750\n",
      "Train Epoch: 55 [2944/17352 (17%)] Loss: -204860.015625\n",
      "Train Epoch: 55 [4352/17352 (25%)] Loss: -214763.828125\n",
      "Train Epoch: 55 [5760/17352 (33%)] Loss: -232305.062500\n",
      "Train Epoch: 55 [7168/17352 (41%)] Loss: -224337.703125\n",
      "Train Epoch: 55 [8576/17352 (49%)] Loss: -236665.453125\n",
      "Train Epoch: 55 [9984/17352 (58%)] Loss: -228793.937500\n",
      "Train Epoch: 55 [11392/17352 (66%)] Loss: -212877.828125\n",
      "Train Epoch: 55 [12800/17352 (74%)] Loss: -225471.906250\n",
      "Train Epoch: 55 [14208/17352 (82%)] Loss: -222905.875000\n",
      "Train Epoch: 55 [15572/17352 (90%)] Loss: -133265.078125\n",
      "Train Epoch: 55 [16281/17352 (94%)] Loss: -86944.125000\n",
      "Train Epoch: 55 [17018/17352 (98%)] Loss: -182352.656250\n",
      "    epoch          : 55\n",
      "    loss           : -193724.33850343435\n",
      "    val_loss       : -105729.58347981771\n",
      "Train Epoch: 56 [128/17352 (1%)] Loss: -201414.093750\n",
      "Train Epoch: 56 [1536/17352 (9%)] Loss: -227502.093750\n",
      "Train Epoch: 56 [2944/17352 (17%)] Loss: -228462.625000\n",
      "Train Epoch: 56 [4352/17352 (25%)] Loss: -195440.640625\n",
      "Train Epoch: 56 [5760/17352 (33%)] Loss: -225073.437500\n",
      "Train Epoch: 56 [7168/17352 (41%)] Loss: -218659.250000\n",
      "Train Epoch: 56 [8576/17352 (49%)] Loss: -208849.031250\n",
      "Train Epoch: 56 [9984/17352 (58%)] Loss: -200387.218750\n",
      "Train Epoch: 56 [11392/17352 (66%)] Loss: -181387.812500\n",
      "Train Epoch: 56 [12800/17352 (74%)] Loss: -212771.171875\n",
      "Train Epoch: 56 [14208/17352 (82%)] Loss: -241240.531250\n",
      "Train Epoch: 56 [15518/17352 (89%)] Loss: -122331.656250\n",
      "Train Epoch: 56 [16388/17352 (94%)] Loss: -161139.187500\n",
      "Train Epoch: 56 [17056/17352 (98%)] Loss: -88613.250000\n",
      "    epoch          : 56\n",
      "    loss           : -192976.22839437396\n",
      "    val_loss       : -106612.30693364143\n",
      "Train Epoch: 57 [128/17352 (1%)] Loss: -219897.953125\n",
      "Train Epoch: 57 [1536/17352 (9%)] Loss: -197544.062500\n",
      "Train Epoch: 57 [2944/17352 (17%)] Loss: -230617.468750\n",
      "Train Epoch: 57 [4352/17352 (25%)] Loss: -206299.640625\n",
      "Train Epoch: 57 [5760/17352 (33%)] Loss: -210344.843750\n",
      "Train Epoch: 57 [7168/17352 (41%)] Loss: -228998.437500\n",
      "Train Epoch: 57 [8576/17352 (49%)] Loss: -215673.593750\n",
      "Train Epoch: 57 [9984/17352 (58%)] Loss: -224847.156250\n",
      "Train Epoch: 57 [11392/17352 (66%)] Loss: -213958.437500\n",
      "Train Epoch: 57 [12800/17352 (74%)] Loss: -199870.781250\n",
      "Train Epoch: 57 [14208/17352 (82%)] Loss: -235275.703125\n",
      "Train Epoch: 57 [15478/17352 (89%)] Loss: -61278.679688\n",
      "Train Epoch: 57 [16035/17352 (92%)] Loss: -126371.156250\n",
      "Train Epoch: 57 [17012/17352 (98%)] Loss: -135289.515625\n",
      "    epoch          : 57\n",
      "    loss           : -193238.27705536914\n",
      "    val_loss       : -106248.81287984848\n",
      "Train Epoch: 58 [128/17352 (1%)] Loss: -215536.921875\n",
      "Train Epoch: 58 [1536/17352 (9%)] Loss: -215335.437500\n",
      "Train Epoch: 58 [2944/17352 (17%)] Loss: -221196.046875\n",
      "Train Epoch: 58 [4352/17352 (25%)] Loss: -197210.265625\n",
      "Train Epoch: 58 [5760/17352 (33%)] Loss: -220208.796875\n",
      "Train Epoch: 58 [7168/17352 (41%)] Loss: -218997.250000\n",
      "Train Epoch: 58 [8576/17352 (49%)] Loss: -206040.687500\n",
      "Train Epoch: 58 [9984/17352 (58%)] Loss: -219269.296875\n",
      "Train Epoch: 58 [11392/17352 (66%)] Loss: -191383.671875\n",
      "Train Epoch: 58 [12800/17352 (74%)] Loss: -200753.187500\n",
      "Train Epoch: 58 [14208/17352 (82%)] Loss: -227167.250000\n",
      "Train Epoch: 58 [15552/17352 (90%)] Loss: -134941.546875\n",
      "Train Epoch: 58 [16268/17352 (94%)] Loss: -191687.062500\n",
      "Train Epoch: 58 [16919/17352 (98%)] Loss: -62646.773438\n",
      "    epoch          : 58\n",
      "    loss           : -193512.92007917367\n",
      "    val_loss       : -106603.7095530192\n",
      "Train Epoch: 59 [128/17352 (1%)] Loss: -222393.765625\n",
      "Train Epoch: 59 [1536/17352 (9%)] Loss: -207388.500000\n",
      "Train Epoch: 59 [2944/17352 (17%)] Loss: -187057.531250\n",
      "Train Epoch: 59 [4352/17352 (25%)] Loss: -210690.187500\n",
      "Train Epoch: 59 [5760/17352 (33%)] Loss: -217577.562500\n",
      "Train Epoch: 59 [7168/17352 (41%)] Loss: -218813.859375\n",
      "Train Epoch: 59 [8576/17352 (49%)] Loss: -211404.250000\n",
      "Train Epoch: 59 [9984/17352 (58%)] Loss: -200669.937500\n",
      "Train Epoch: 59 [11392/17352 (66%)] Loss: -202751.281250\n",
      "Train Epoch: 59 [12800/17352 (74%)] Loss: -208075.953125\n",
      "Train Epoch: 59 [14208/17352 (82%)] Loss: -207889.031250\n",
      "Train Epoch: 59 [15500/17352 (89%)] Loss: -89379.703125\n",
      "Train Epoch: 59 [16134/17352 (93%)] Loss: -24070.076172\n",
      "Train Epoch: 59 [17026/17352 (98%)] Loss: -26248.625000\n",
      "    epoch          : 59\n",
      "    loss           : -193541.02465984164\n",
      "    val_loss       : -106537.03174422582\n",
      "Train Epoch: 60 [128/17352 (1%)] Loss: -185637.796875\n",
      "Train Epoch: 60 [1536/17352 (9%)] Loss: -184797.703125\n",
      "Train Epoch: 60 [2944/17352 (17%)] Loss: -226597.546875\n",
      "Train Epoch: 60 [4352/17352 (25%)] Loss: -220994.359375\n",
      "Train Epoch: 60 [5760/17352 (33%)] Loss: -237062.468750\n",
      "Train Epoch: 60 [7168/17352 (41%)] Loss: -226145.500000\n",
      "Train Epoch: 60 [8576/17352 (49%)] Loss: -211899.265625\n",
      "Train Epoch: 60 [9984/17352 (58%)] Loss: -211675.859375\n",
      "Train Epoch: 60 [11392/17352 (66%)] Loss: -228351.921875\n",
      "Train Epoch: 60 [12800/17352 (74%)] Loss: -223833.515625\n",
      "Train Epoch: 60 [14208/17352 (82%)] Loss: -223958.437500\n",
      "Train Epoch: 60 [15491/17352 (89%)] Loss: -73236.593750\n",
      "Train Epoch: 60 [16334/17352 (94%)] Loss: -151060.875000\n",
      "Train Epoch: 60 [17001/17352 (98%)] Loss: -162587.250000\n",
      "    epoch          : 60\n",
      "    loss           : -194039.38887832948\n",
      "    val_loss       : -106052.98050812086\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch60.pth ...\n",
      "Train Epoch: 61 [128/17352 (1%)] Loss: -229106.656250\n",
      "Train Epoch: 61 [1536/17352 (9%)] Loss: -208118.468750\n",
      "Train Epoch: 61 [2944/17352 (17%)] Loss: -198606.812500\n",
      "Train Epoch: 61 [4352/17352 (25%)] Loss: -220275.390625\n",
      "Train Epoch: 61 [5760/17352 (33%)] Loss: -208858.125000\n",
      "Train Epoch: 61 [7168/17352 (41%)] Loss: -199498.312500\n",
      "Train Epoch: 61 [8576/17352 (49%)] Loss: -203638.281250\n",
      "Train Epoch: 61 [9984/17352 (58%)] Loss: -213917.187500\n",
      "Train Epoch: 61 [11392/17352 (66%)] Loss: -215697.656250\n",
      "Train Epoch: 61 [12800/17352 (74%)] Loss: -226472.406250\n",
      "Train Epoch: 61 [14208/17352 (82%)] Loss: -215940.968750\n",
      "Train Epoch: 61 [15511/17352 (89%)] Loss: -168907.000000\n",
      "Train Epoch: 61 [16356/17352 (94%)] Loss: -168408.281250\n",
      "Train Epoch: 61 [17156/17352 (99%)] Loss: -126063.460938\n",
      "    epoch          : 61\n",
      "    loss           : -194264.1919928429\n",
      "    val_loss       : -105988.97092909813\n",
      "Train Epoch: 62 [128/17352 (1%)] Loss: -210848.937500\n",
      "Train Epoch: 62 [1536/17352 (9%)] Loss: -217576.937500\n",
      "Train Epoch: 62 [2944/17352 (17%)] Loss: -249961.015625\n",
      "Train Epoch: 62 [4352/17352 (25%)] Loss: -213917.578125\n",
      "Train Epoch: 62 [5760/17352 (33%)] Loss: -224720.937500\n",
      "Train Epoch: 62 [7168/17352 (41%)] Loss: -216646.875000\n",
      "Train Epoch: 62 [8576/17352 (49%)] Loss: -192202.593750\n",
      "Train Epoch: 62 [9984/17352 (58%)] Loss: -223034.968750\n",
      "Train Epoch: 62 [11392/17352 (66%)] Loss: -215420.093750\n",
      "Train Epoch: 62 [12800/17352 (74%)] Loss: -209016.875000\n",
      "Train Epoch: 62 [14208/17352 (82%)] Loss: -239197.562500\n",
      "Train Epoch: 62 [15487/17352 (89%)] Loss: -140964.218750\n",
      "Train Epoch: 62 [16308/17352 (94%)] Loss: -189472.296875\n",
      "Train Epoch: 62 [17130/17352 (99%)] Loss: -162307.781250\n",
      "    epoch          : 62\n",
      "    loss           : -194173.54237888003\n",
      "    val_loss       : -107767.53569997151\n",
      "Train Epoch: 63 [128/17352 (1%)] Loss: -189850.187500\n",
      "Train Epoch: 63 [1536/17352 (9%)] Loss: -236797.312500\n",
      "Train Epoch: 63 [2944/17352 (17%)] Loss: -208483.343750\n",
      "Train Epoch: 63 [4352/17352 (25%)] Loss: -227383.390625\n",
      "Train Epoch: 63 [5760/17352 (33%)] Loss: -225622.453125\n",
      "Train Epoch: 63 [7168/17352 (41%)] Loss: -212994.796875\n",
      "Train Epoch: 63 [8576/17352 (49%)] Loss: -205006.062500\n",
      "Train Epoch: 63 [9984/17352 (58%)] Loss: -210562.906250\n",
      "Train Epoch: 63 [11392/17352 (66%)] Loss: -221436.000000\n",
      "Train Epoch: 63 [12800/17352 (74%)] Loss: -220706.671875\n",
      "Train Epoch: 63 [14208/17352 (82%)] Loss: -237977.031250\n",
      "Train Epoch: 63 [15444/17352 (89%)] Loss: -62763.417969\n",
      "Train Epoch: 63 [16379/17352 (94%)] Loss: -80461.648438\n",
      "Train Epoch: 63 [16975/17352 (98%)] Loss: -182289.515625\n",
      "    epoch          : 63\n",
      "    loss           : -195521.5907842649\n",
      "    val_loss       : -106691.76437725226\n",
      "Train Epoch: 64 [128/17352 (1%)] Loss: -196596.312500\n",
      "Train Epoch: 64 [1536/17352 (9%)] Loss: -209657.625000\n",
      "Train Epoch: 64 [2944/17352 (17%)] Loss: -227668.921875\n",
      "Train Epoch: 64 [4352/17352 (25%)] Loss: -212940.453125\n",
      "Train Epoch: 64 [5760/17352 (33%)] Loss: -197960.859375\n",
      "Train Epoch: 64 [7168/17352 (41%)] Loss: -193772.859375\n",
      "Train Epoch: 64 [8576/17352 (49%)] Loss: -218513.781250\n",
      "Train Epoch: 64 [9984/17352 (58%)] Loss: -217839.578125\n",
      "Train Epoch: 64 [11392/17352 (66%)] Loss: -194221.718750\n",
      "Train Epoch: 64 [12800/17352 (74%)] Loss: -205488.718750\n",
      "Train Epoch: 64 [14208/17352 (82%)] Loss: -219730.578125\n",
      "Train Epoch: 64 [15525/17352 (89%)] Loss: -187558.750000\n",
      "Train Epoch: 64 [16210/17352 (93%)] Loss: -223065.937500\n",
      "Train Epoch: 64 [17064/17352 (98%)] Loss: -178566.640625\n",
      "    epoch          : 64\n",
      "    loss           : -194908.80424837457\n",
      "    val_loss       : -106981.74288590749\n",
      "Train Epoch: 65 [128/17352 (1%)] Loss: -227222.156250\n",
      "Train Epoch: 65 [1536/17352 (9%)] Loss: -222739.281250\n",
      "Train Epoch: 65 [2944/17352 (17%)] Loss: -213165.171875\n",
      "Train Epoch: 65 [4352/17352 (25%)] Loss: -226591.937500\n",
      "Train Epoch: 65 [5760/17352 (33%)] Loss: -237438.500000\n",
      "Train Epoch: 65 [7168/17352 (41%)] Loss: -206292.968750\n",
      "Train Epoch: 65 [8576/17352 (49%)] Loss: -220530.781250\n",
      "Train Epoch: 65 [9984/17352 (58%)] Loss: -196094.140625\n",
      "Train Epoch: 65 [11392/17352 (66%)] Loss: -219852.031250\n",
      "Train Epoch: 65 [12800/17352 (74%)] Loss: -221704.843750\n",
      "Train Epoch: 65 [14208/17352 (82%)] Loss: -228138.203125\n",
      "Train Epoch: 65 [15553/17352 (90%)] Loss: -145717.328125\n",
      "Train Epoch: 65 [16224/17352 (93%)] Loss: -167754.296875\n",
      "Train Epoch: 65 [17020/17352 (98%)] Loss: -178591.281250\n",
      "    epoch          : 65\n",
      "    loss           : -194641.68209941275\n",
      "    val_loss       : -106157.0856359323\n",
      "Train Epoch: 66 [128/17352 (1%)] Loss: -187639.546875\n",
      "Train Epoch: 66 [1536/17352 (9%)] Loss: -206974.046875\n",
      "Train Epoch: 66 [2944/17352 (17%)] Loss: -192793.125000\n",
      "Train Epoch: 66 [4352/17352 (25%)] Loss: -218240.843750\n",
      "Train Epoch: 66 [5760/17352 (33%)] Loss: -215228.953125\n",
      "Train Epoch: 66 [7168/17352 (41%)] Loss: -223306.718750\n",
      "Train Epoch: 66 [8576/17352 (49%)] Loss: -216795.671875\n",
      "Train Epoch: 66 [9984/17352 (58%)] Loss: -228621.234375\n",
      "Train Epoch: 66 [11392/17352 (66%)] Loss: -196451.781250\n",
      "Train Epoch: 66 [12800/17352 (74%)] Loss: -242758.656250\n",
      "Train Epoch: 66 [14208/17352 (82%)] Loss: -225918.187500\n",
      "Train Epoch: 66 [15546/17352 (90%)] Loss: -124930.875000\n",
      "Train Epoch: 66 [16291/17352 (94%)] Loss: -140023.500000\n",
      "Train Epoch: 66 [17009/17352 (98%)] Loss: -155037.281250\n",
      "    epoch          : 66\n",
      "    loss           : -195142.64848337878\n",
      "    val_loss       : -106246.70295583407\n",
      "Train Epoch: 67 [128/17352 (1%)] Loss: -206661.937500\n",
      "Train Epoch: 67 [1536/17352 (9%)] Loss: -215038.328125\n",
      "Train Epoch: 67 [2944/17352 (17%)] Loss: -227782.843750\n",
      "Train Epoch: 67 [4352/17352 (25%)] Loss: -226520.437500\n",
      "Train Epoch: 67 [5760/17352 (33%)] Loss: -220029.078125\n",
      "Train Epoch: 67 [7168/17352 (41%)] Loss: -190167.359375\n",
      "Train Epoch: 67 [8576/17352 (49%)] Loss: -203336.875000\n",
      "Train Epoch: 67 [9984/17352 (58%)] Loss: -224327.406250\n",
      "Train Epoch: 67 [11392/17352 (66%)] Loss: -223525.453125\n",
      "Train Epoch: 67 [12800/17352 (74%)] Loss: -211260.437500\n",
      "Train Epoch: 67 [14208/17352 (82%)] Loss: -224313.312500\n",
      "Train Epoch: 67 [15488/17352 (89%)] Loss: -23396.328125\n",
      "Train Epoch: 67 [16195/17352 (93%)] Loss: -141180.578125\n",
      "Train Epoch: 67 [17039/17352 (98%)] Loss: -167927.875000\n",
      "    epoch          : 67\n",
      "    loss           : -194872.08320115876\n",
      "    val_loss       : -107723.68951809804\n",
      "Train Epoch: 68 [128/17352 (1%)] Loss: -186217.875000\n",
      "Train Epoch: 68 [1536/17352 (9%)] Loss: -222133.765625\n",
      "Train Epoch: 68 [2944/17352 (17%)] Loss: -208013.906250\n",
      "Train Epoch: 68 [4352/17352 (25%)] Loss: -205616.937500\n",
      "Train Epoch: 68 [5760/17352 (33%)] Loss: -213938.156250\n",
      "Train Epoch: 68 [7168/17352 (41%)] Loss: -216122.406250\n",
      "Train Epoch: 68 [8576/17352 (49%)] Loss: -211245.750000\n",
      "Train Epoch: 68 [9984/17352 (58%)] Loss: -224072.187500\n",
      "Train Epoch: 68 [11392/17352 (66%)] Loss: -208618.031250\n",
      "Train Epoch: 68 [12800/17352 (74%)] Loss: -204111.875000\n",
      "Train Epoch: 68 [14208/17352 (82%)] Loss: -211618.093750\n",
      "Train Epoch: 68 [15560/17352 (90%)] Loss: -176200.796875\n",
      "Train Epoch: 68 [16170/17352 (93%)] Loss: -5322.042969\n",
      "Train Epoch: 68 [16999/17352 (98%)] Loss: -184944.343750\n",
      "    epoch          : 68\n",
      "    loss           : -195027.38704645555\n",
      "    val_loss       : -107623.40926411947\n",
      "Train Epoch: 69 [128/17352 (1%)] Loss: -204663.562500\n",
      "Train Epoch: 69 [1536/17352 (9%)] Loss: -207489.250000\n",
      "Train Epoch: 69 [2944/17352 (17%)] Loss: -206804.765625\n",
      "Train Epoch: 69 [4352/17352 (25%)] Loss: -207984.437500\n",
      "Train Epoch: 69 [5760/17352 (33%)] Loss: -219248.343750\n",
      "Train Epoch: 69 [7168/17352 (41%)] Loss: -208023.593750\n",
      "Train Epoch: 69 [8576/17352 (49%)] Loss: -207458.796875\n",
      "Train Epoch: 69 [9984/17352 (58%)] Loss: -223794.468750\n",
      "Train Epoch: 69 [11392/17352 (66%)] Loss: -191769.765625\n",
      "Train Epoch: 69 [12800/17352 (74%)] Loss: -214315.187500\n",
      "Train Epoch: 69 [14208/17352 (82%)] Loss: -208787.421875\n",
      "Train Epoch: 69 [15523/17352 (89%)] Loss: -180371.546875\n",
      "Train Epoch: 69 [16260/17352 (94%)] Loss: -132630.328125\n",
      "Train Epoch: 69 [17047/17352 (98%)] Loss: -57480.113281\n",
      "    epoch          : 69\n",
      "    loss           : -196091.38409055158\n",
      "    val_loss       : -107915.40812358857\n",
      "Train Epoch: 70 [128/17352 (1%)] Loss: -206694.890625\n",
      "Train Epoch: 70 [1536/17352 (9%)] Loss: -220355.984375\n",
      "Train Epoch: 70 [2944/17352 (17%)] Loss: -239666.468750\n",
      "Train Epoch: 70 [4352/17352 (25%)] Loss: -177099.968750\n",
      "Train Epoch: 70 [5760/17352 (33%)] Loss: -202860.750000\n",
      "Train Epoch: 70 [7168/17352 (41%)] Loss: -222044.312500\n",
      "Train Epoch: 70 [8576/17352 (49%)] Loss: -205304.078125\n",
      "Train Epoch: 70 [9984/17352 (58%)] Loss: -216085.156250\n",
      "Train Epoch: 70 [11392/17352 (66%)] Loss: -225518.453125\n",
      "Train Epoch: 70 [12800/17352 (74%)] Loss: -232101.062500\n",
      "Train Epoch: 70 [14208/17352 (82%)] Loss: -215485.781250\n",
      "Train Epoch: 70 [15550/17352 (90%)] Loss: -167617.484375\n",
      "Train Epoch: 70 [16183/17352 (93%)] Loss: -23863.015625\n",
      "Train Epoch: 70 [16998/17352 (98%)] Loss: -126792.429688\n",
      "    epoch          : 70\n",
      "    loss           : -195653.79966377412\n",
      "    val_loss       : -107682.72957526843\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch70.pth ...\n",
      "Train Epoch: 71 [128/17352 (1%)] Loss: -225314.359375\n",
      "Train Epoch: 71 [1536/17352 (9%)] Loss: -226062.250000\n",
      "Train Epoch: 71 [2944/17352 (17%)] Loss: -254841.625000\n",
      "Train Epoch: 71 [4352/17352 (25%)] Loss: -225504.296875\n",
      "Train Epoch: 71 [5760/17352 (33%)] Loss: -235247.437500\n",
      "Train Epoch: 71 [7168/17352 (41%)] Loss: -223558.281250\n",
      "Train Epoch: 71 [8576/17352 (49%)] Loss: -205193.093750\n",
      "Train Epoch: 71 [9984/17352 (58%)] Loss: -234998.875000\n",
      "Train Epoch: 71 [11392/17352 (66%)] Loss: -209420.093750\n",
      "Train Epoch: 71 [12800/17352 (74%)] Loss: -219778.875000\n",
      "Train Epoch: 71 [14208/17352 (82%)] Loss: -223137.578125\n",
      "Train Epoch: 71 [15548/17352 (90%)] Loss: -130187.031250\n",
      "Train Epoch: 71 [16279/17352 (94%)] Loss: -142188.765625\n",
      "Train Epoch: 71 [17091/17352 (98%)] Loss: -149498.375000\n",
      "    epoch          : 71\n",
      "    loss           : -196550.5699094222\n",
      "    val_loss       : -108288.96611336073\n",
      "Train Epoch: 72 [128/17352 (1%)] Loss: -216299.109375\n",
      "Train Epoch: 72 [1536/17352 (9%)] Loss: -186018.656250\n",
      "Train Epoch: 72 [2944/17352 (17%)] Loss: -253771.078125\n",
      "Train Epoch: 72 [4352/17352 (25%)] Loss: -221319.390625\n",
      "Train Epoch: 72 [5760/17352 (33%)] Loss: -204772.312500\n",
      "Train Epoch: 72 [7168/17352 (41%)] Loss: -218571.468750\n",
      "Train Epoch: 72 [8576/17352 (49%)] Loss: -238552.203125\n",
      "Train Epoch: 72 [9984/17352 (58%)] Loss: -210566.468750\n",
      "Train Epoch: 72 [11392/17352 (66%)] Loss: -226672.937500\n",
      "Train Epoch: 72 [12800/17352 (74%)] Loss: -208338.765625\n",
      "Train Epoch: 72 [14208/17352 (82%)] Loss: -242208.953125\n",
      "Train Epoch: 72 [15527/17352 (89%)] Loss: -134136.921875\n",
      "Train Epoch: 72 [16287/17352 (94%)] Loss: -229766.968750\n",
      "Train Epoch: 72 [17091/17352 (98%)] Loss: -170656.031250\n",
      "    epoch          : 72\n",
      "    loss           : -196021.96024604133\n",
      "    val_loss       : -109217.03809298674\n",
      "Train Epoch: 73 [128/17352 (1%)] Loss: -192180.781250\n",
      "Train Epoch: 73 [1536/17352 (9%)] Loss: -207088.312500\n",
      "Train Epoch: 73 [2944/17352 (17%)] Loss: -215513.062500\n",
      "Train Epoch: 73 [4352/17352 (25%)] Loss: -216574.265625\n",
      "Train Epoch: 73 [5760/17352 (33%)] Loss: -218899.625000\n",
      "Train Epoch: 73 [7168/17352 (41%)] Loss: -208229.375000\n",
      "Train Epoch: 73 [8576/17352 (49%)] Loss: -208036.812500\n",
      "Train Epoch: 73 [9984/17352 (58%)] Loss: -227699.500000\n",
      "Train Epoch: 73 [11392/17352 (66%)] Loss: -214513.218750\n",
      "Train Epoch: 73 [12800/17352 (74%)] Loss: -225870.937500\n",
      "Train Epoch: 73 [14208/17352 (82%)] Loss: -212029.546875\n",
      "Train Epoch: 73 [15546/17352 (90%)] Loss: -175616.703125\n",
      "Train Epoch: 73 [16174/17352 (93%)] Loss: -23523.476562\n",
      "Train Epoch: 73 [17056/17352 (98%)] Loss: -5653.347656\n",
      "    epoch          : 73\n",
      "    loss           : -196381.26443870596\n",
      "    val_loss       : -107644.13596979778\n",
      "Train Epoch: 74 [128/17352 (1%)] Loss: -218996.984375\n",
      "Train Epoch: 74 [1536/17352 (9%)] Loss: -226791.953125\n",
      "Train Epoch: 74 [2944/17352 (17%)] Loss: -202752.578125\n",
      "Train Epoch: 74 [4352/17352 (25%)] Loss: -226626.234375\n",
      "Train Epoch: 74 [5760/17352 (33%)] Loss: -200648.812500\n",
      "Train Epoch: 74 [7168/17352 (41%)] Loss: -220774.625000\n",
      "Train Epoch: 74 [8576/17352 (49%)] Loss: -228694.359375\n",
      "Train Epoch: 74 [9984/17352 (58%)] Loss: -210649.406250\n",
      "Train Epoch: 74 [11392/17352 (66%)] Loss: -231296.031250\n",
      "Train Epoch: 74 [12800/17352 (74%)] Loss: -208388.296875\n",
      "Train Epoch: 74 [14208/17352 (82%)] Loss: -223692.515625\n",
      "Train Epoch: 74 [15488/17352 (89%)] Loss: -24012.746094\n",
      "Train Epoch: 74 [16135/17352 (93%)] Loss: -165638.375000\n",
      "Train Epoch: 74 [17037/17352 (98%)] Loss: -137285.906250\n",
      "    epoch          : 74\n",
      "    loss           : -197027.46294305788\n",
      "    val_loss       : -107545.02616270383\n",
      "Train Epoch: 75 [128/17352 (1%)] Loss: -210904.593750\n",
      "Train Epoch: 75 [1536/17352 (9%)] Loss: -218462.937500\n",
      "Train Epoch: 75 [2944/17352 (17%)] Loss: -205439.312500\n",
      "Train Epoch: 75 [4352/17352 (25%)] Loss: -227037.406250\n",
      "Train Epoch: 75 [5760/17352 (33%)] Loss: -238903.687500\n",
      "Train Epoch: 75 [7168/17352 (41%)] Loss: -229800.343750\n",
      "Train Epoch: 75 [8576/17352 (49%)] Loss: -225422.750000\n",
      "Train Epoch: 75 [9984/17352 (58%)] Loss: -227908.796875\n",
      "Train Epoch: 75 [11392/17352 (66%)] Loss: -194889.031250\n",
      "Train Epoch: 75 [12800/17352 (74%)] Loss: -196831.671875\n",
      "Train Epoch: 75 [14208/17352 (82%)] Loss: -202362.562500\n",
      "Train Epoch: 75 [15559/17352 (90%)] Loss: -153915.640625\n",
      "Train Epoch: 75 [16347/17352 (94%)] Loss: -163889.156250\n",
      "Train Epoch: 75 [17051/17352 (98%)] Loss: -131128.500000\n",
      "    epoch          : 75\n",
      "    loss           : -196154.557292759\n",
      "    val_loss       : -106611.65196568171\n",
      "Train Epoch: 76 [128/17352 (1%)] Loss: -184336.468750\n",
      "Train Epoch: 76 [1536/17352 (9%)] Loss: -211005.750000\n",
      "Train Epoch: 76 [2944/17352 (17%)] Loss: -209068.531250\n",
      "Train Epoch: 76 [4352/17352 (25%)] Loss: -227717.734375\n",
      "Train Epoch: 76 [5760/17352 (33%)] Loss: -219268.406250\n",
      "Train Epoch: 76 [7168/17352 (41%)] Loss: -237339.406250\n",
      "Train Epoch: 76 [8576/17352 (49%)] Loss: -182730.390625\n",
      "Train Epoch: 76 [9984/17352 (58%)] Loss: -223600.125000\n",
      "Train Epoch: 76 [11392/17352 (66%)] Loss: -213808.593750\n",
      "Train Epoch: 76 [12800/17352 (74%)] Loss: -208470.218750\n",
      "Train Epoch: 76 [14208/17352 (82%)] Loss: -227394.531250\n",
      "Train Epoch: 76 [15492/17352 (89%)] Loss: -128391.921875\n",
      "Train Epoch: 76 [16323/17352 (94%)] Loss: -23518.671875\n",
      "Train Epoch: 76 [17005/17352 (98%)] Loss: -164789.078125\n",
      "    epoch          : 76\n",
      "    loss           : -194875.13206205433\n",
      "    val_loss       : -108235.96253492037\n",
      "Train Epoch: 77 [128/17352 (1%)] Loss: -230937.000000\n",
      "Train Epoch: 77 [1536/17352 (9%)] Loss: -205246.609375\n",
      "Train Epoch: 77 [2944/17352 (17%)] Loss: -227616.468750\n",
      "Train Epoch: 77 [4352/17352 (25%)] Loss: -211212.140625\n",
      "Train Epoch: 77 [5760/17352 (33%)] Loss: -233443.906250\n",
      "Train Epoch: 77 [7168/17352 (41%)] Loss: -241296.109375\n",
      "Train Epoch: 77 [8576/17352 (49%)] Loss: -218672.390625\n",
      "Train Epoch: 77 [9984/17352 (58%)] Loss: -225868.140625\n",
      "Train Epoch: 77 [11392/17352 (66%)] Loss: -215774.328125\n",
      "Train Epoch: 77 [12800/17352 (74%)] Loss: -218612.890625\n",
      "Train Epoch: 77 [14208/17352 (82%)] Loss: -212066.281250\n",
      "Train Epoch: 77 [15487/17352 (89%)] Loss: -118111.296875\n",
      "Train Epoch: 77 [16286/17352 (94%)] Loss: -186677.468750\n",
      "Train Epoch: 77 [17080/17352 (98%)] Loss: -119084.296875\n",
      "    epoch          : 77\n",
      "    loss           : -196276.29223141255\n",
      "    val_loss       : -107181.26383884747\n",
      "Train Epoch: 78 [128/17352 (1%)] Loss: -208005.812500\n",
      "Train Epoch: 78 [1536/17352 (9%)] Loss: -213485.046875\n",
      "Train Epoch: 78 [2944/17352 (17%)] Loss: -183742.796875\n",
      "Train Epoch: 78 [4352/17352 (25%)] Loss: -230170.718750\n",
      "Train Epoch: 78 [5760/17352 (33%)] Loss: -224635.218750\n",
      "Train Epoch: 78 [7168/17352 (41%)] Loss: -199830.953125\n",
      "Train Epoch: 78 [8576/17352 (49%)] Loss: -212798.234375\n",
      "Train Epoch: 78 [9984/17352 (58%)] Loss: -219588.171875\n",
      "Train Epoch: 78 [11392/17352 (66%)] Loss: -222615.765625\n",
      "Train Epoch: 78 [12800/17352 (74%)] Loss: -208753.031250\n",
      "Train Epoch: 78 [14208/17352 (82%)] Loss: -224955.125000\n",
      "Train Epoch: 78 [15533/17352 (90%)] Loss: -142082.968750\n",
      "Train Epoch: 78 [16419/17352 (95%)] Loss: -27478.062500\n",
      "Train Epoch: 78 [17081/17352 (98%)] Loss: -59745.023438\n",
      "    epoch          : 78\n",
      "    loss           : -196705.85233260802\n",
      "    val_loss       : -108213.75866597494\n",
      "Train Epoch: 79 [128/17352 (1%)] Loss: -193465.671875\n",
      "Train Epoch: 79 [1536/17352 (9%)] Loss: -214110.921875\n",
      "Train Epoch: 79 [2944/17352 (17%)] Loss: -230442.171875\n",
      "Train Epoch: 79 [4352/17352 (25%)] Loss: -200690.000000\n",
      "Train Epoch: 79 [5760/17352 (33%)] Loss: -224527.156250\n",
      "Train Epoch: 79 [7168/17352 (41%)] Loss: -186485.078125\n",
      "Train Epoch: 79 [8576/17352 (49%)] Loss: -218933.984375\n",
      "Train Epoch: 79 [9984/17352 (58%)] Loss: -211956.734375\n",
      "Train Epoch: 79 [11392/17352 (66%)] Loss: -204139.921875\n",
      "Train Epoch: 79 [12800/17352 (74%)] Loss: -228912.406250\n",
      "Train Epoch: 79 [14208/17352 (82%)] Loss: -227169.562500\n",
      "Train Epoch: 79 [15550/17352 (90%)] Loss: -165801.437500\n",
      "Train Epoch: 79 [16372/17352 (94%)] Loss: -150924.546875\n",
      "Train Epoch: 79 [17149/17352 (99%)] Loss: -158260.640625\n",
      "    epoch          : 79\n",
      "    loss           : -197129.41032993395\n",
      "    val_loss       : -108005.5167899847\n",
      "Train Epoch: 80 [128/17352 (1%)] Loss: -201252.265625\n",
      "Train Epoch: 80 [1536/17352 (9%)] Loss: -220307.671875\n",
      "Train Epoch: 80 [2944/17352 (17%)] Loss: -220773.718750\n",
      "Train Epoch: 80 [4352/17352 (25%)] Loss: -210062.375000\n",
      "Train Epoch: 80 [5760/17352 (33%)] Loss: -236024.828125\n",
      "Train Epoch: 80 [7168/17352 (41%)] Loss: -202320.718750\n",
      "Train Epoch: 80 [8576/17352 (49%)] Loss: -240211.156250\n",
      "Train Epoch: 80 [9984/17352 (58%)] Loss: -230299.312500\n",
      "Train Epoch: 80 [11392/17352 (66%)] Loss: -197230.812500\n",
      "Train Epoch: 80 [12800/17352 (74%)] Loss: -224781.125000\n",
      "Train Epoch: 80 [14208/17352 (82%)] Loss: -233264.906250\n",
      "Train Epoch: 80 [15512/17352 (89%)] Loss: -125914.179688\n",
      "Train Epoch: 80 [16248/17352 (94%)] Loss: -148140.843750\n",
      "Train Epoch: 80 [17153/17352 (99%)] Loss: -176299.390625\n",
      "    epoch          : 80\n",
      "    loss           : -196553.4018816852\n",
      "    val_loss       : -108283.65752425193\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch80.pth ...\n",
      "Train Epoch: 81 [128/17352 (1%)] Loss: -223518.000000\n",
      "Train Epoch: 81 [1536/17352 (9%)] Loss: -221632.062500\n",
      "Train Epoch: 81 [2944/17352 (17%)] Loss: -204962.187500\n",
      "Train Epoch: 81 [4352/17352 (25%)] Loss: -235860.015625\n",
      "Train Epoch: 81 [5760/17352 (33%)] Loss: -214764.656250\n",
      "Train Epoch: 81 [7168/17352 (41%)] Loss: -223111.906250\n",
      "Train Epoch: 81 [8576/17352 (49%)] Loss: -217463.578125\n",
      "Train Epoch: 81 [9984/17352 (58%)] Loss: -227793.046875\n",
      "Train Epoch: 81 [11392/17352 (66%)] Loss: -217599.765625\n",
      "Train Epoch: 81 [12800/17352 (74%)] Loss: -214814.468750\n",
      "Train Epoch: 81 [14208/17352 (82%)] Loss: -213963.843750\n",
      "Train Epoch: 81 [15478/17352 (89%)] Loss: -135073.906250\n",
      "Train Epoch: 81 [16188/17352 (93%)] Loss: -175861.843750\n",
      "Train Epoch: 81 [17022/17352 (98%)] Loss: -77857.406250\n",
      "    epoch          : 81\n",
      "    loss           : -196998.20561228503\n",
      "    val_loss       : -108376.13245549201\n",
      "Train Epoch: 82 [128/17352 (1%)] Loss: -204354.828125\n",
      "Train Epoch: 82 [1536/17352 (9%)] Loss: -208990.375000\n",
      "Train Epoch: 82 [2944/17352 (17%)] Loss: -208590.046875\n",
      "Train Epoch: 82 [4352/17352 (25%)] Loss: -215808.234375\n",
      "Train Epoch: 82 [5760/17352 (33%)] Loss: -226711.656250\n",
      "Train Epoch: 82 [7168/17352 (41%)] Loss: -213220.703125\n",
      "Train Epoch: 82 [8576/17352 (49%)] Loss: -198801.781250\n",
      "Train Epoch: 82 [9984/17352 (58%)] Loss: -225574.656250\n",
      "Train Epoch: 82 [11392/17352 (66%)] Loss: -228873.750000\n",
      "Train Epoch: 82 [12800/17352 (74%)] Loss: -224895.281250\n",
      "Train Epoch: 82 [14208/17352 (82%)] Loss: -214658.703125\n",
      "Train Epoch: 82 [15448/17352 (89%)] Loss: -28458.699219\n",
      "Train Epoch: 82 [16096/17352 (93%)] Loss: -58364.429688\n",
      "Train Epoch: 82 [16996/17352 (98%)] Loss: -73554.593750\n",
      "    epoch          : 82\n",
      "    loss           : -198041.43244022652\n",
      "    val_loss       : -108281.48892521858\n",
      "Train Epoch: 83 [128/17352 (1%)] Loss: -194727.984375\n",
      "Train Epoch: 83 [1536/17352 (9%)] Loss: -220630.750000\n",
      "Train Epoch: 83 [2944/17352 (17%)] Loss: -244533.640625\n",
      "Train Epoch: 83 [4352/17352 (25%)] Loss: -205013.000000\n",
      "Train Epoch: 83 [5760/17352 (33%)] Loss: -226110.187500\n",
      "Train Epoch: 83 [7168/17352 (41%)] Loss: -223127.656250\n",
      "Train Epoch: 83 [8576/17352 (49%)] Loss: -223161.875000\n",
      "Train Epoch: 83 [9984/17352 (58%)] Loss: -219382.640625\n",
      "Train Epoch: 83 [11392/17352 (66%)] Loss: -226957.734375\n",
      "Train Epoch: 83 [12800/17352 (74%)] Loss: -212938.093750\n",
      "Train Epoch: 83 [14208/17352 (82%)] Loss: -211795.062500\n",
      "Train Epoch: 83 [15457/17352 (89%)] Loss: -151484.125000\n",
      "Train Epoch: 83 [16217/17352 (93%)] Loss: -159282.703125\n",
      "Train Epoch: 83 [16939/17352 (98%)] Loss: -227652.312500\n",
      "    epoch          : 83\n",
      "    loss           : -197201.24266922713\n",
      "    val_loss       : -109572.22375825247\n",
      "Train Epoch: 84 [128/17352 (1%)] Loss: -209955.937500\n",
      "Train Epoch: 84 [1536/17352 (9%)] Loss: -223863.734375\n",
      "Train Epoch: 84 [2944/17352 (17%)] Loss: -201062.421875\n",
      "Train Epoch: 84 [4352/17352 (25%)] Loss: -207243.796875\n",
      "Train Epoch: 84 [5760/17352 (33%)] Loss: -217849.812500\n",
      "Train Epoch: 84 [7168/17352 (41%)] Loss: -208376.437500\n",
      "Train Epoch: 84 [8576/17352 (49%)] Loss: -209844.500000\n",
      "Train Epoch: 84 [9984/17352 (58%)] Loss: -195569.781250\n",
      "Train Epoch: 84 [11392/17352 (66%)] Loss: -207661.593750\n",
      "Train Epoch: 84 [12800/17352 (74%)] Loss: -220772.031250\n",
      "Train Epoch: 84 [14208/17352 (82%)] Loss: -215272.765625\n",
      "Train Epoch: 84 [15551/17352 (90%)] Loss: -152707.093750\n",
      "Train Epoch: 84 [16166/17352 (93%)] Loss: -81356.093750\n",
      "Train Epoch: 84 [16998/17352 (98%)] Loss: -136412.406250\n",
      "    epoch          : 84\n",
      "    loss           : -197244.31760565226\n",
      "    val_loss       : -108476.92571872075\n",
      "Train Epoch: 85 [128/17352 (1%)] Loss: -209503.984375\n",
      "Train Epoch: 85 [1536/17352 (9%)] Loss: -216084.781250\n",
      "Train Epoch: 85 [2944/17352 (17%)] Loss: -206719.687500\n",
      "Train Epoch: 85 [4352/17352 (25%)] Loss: -232154.093750\n",
      "Train Epoch: 85 [5760/17352 (33%)] Loss: -212688.156250\n",
      "Train Epoch: 85 [7168/17352 (41%)] Loss: -214406.906250\n",
      "Train Epoch: 85 [8576/17352 (49%)] Loss: -211935.468750\n",
      "Train Epoch: 85 [9984/17352 (58%)] Loss: -216774.234375\n",
      "Train Epoch: 85 [11392/17352 (66%)] Loss: -225542.375000\n",
      "Train Epoch: 85 [12800/17352 (74%)] Loss: -205796.437500\n",
      "Train Epoch: 85 [14208/17352 (82%)] Loss: -214271.187500\n",
      "Train Epoch: 85 [15500/17352 (89%)] Loss: -84623.000000\n",
      "Train Epoch: 85 [16118/17352 (93%)] Loss: -178012.125000\n",
      "Train Epoch: 85 [16964/17352 (98%)] Loss: -78433.179688\n",
      "    epoch          : 85\n",
      "    loss           : -197728.93538957633\n",
      "    val_loss       : -109646.48544726372\n",
      "Train Epoch: 86 [128/17352 (1%)] Loss: -210453.625000\n",
      "Train Epoch: 86 [1536/17352 (9%)] Loss: -237563.890625\n",
      "Train Epoch: 86 [2944/17352 (17%)] Loss: -184944.265625\n",
      "Train Epoch: 86 [4352/17352 (25%)] Loss: -229539.593750\n",
      "Train Epoch: 86 [5760/17352 (33%)] Loss: -226981.546875\n",
      "Train Epoch: 86 [7168/17352 (41%)] Loss: -200892.656250\n",
      "Train Epoch: 86 [8576/17352 (49%)] Loss: -224246.453125\n",
      "Train Epoch: 86 [9984/17352 (58%)] Loss: -241881.609375\n",
      "Train Epoch: 86 [11392/17352 (66%)] Loss: -221603.656250\n",
      "Train Epoch: 86 [12800/17352 (74%)] Loss: -212333.000000\n",
      "Train Epoch: 86 [14208/17352 (82%)] Loss: -208210.937500\n",
      "Train Epoch: 86 [15455/17352 (89%)] Loss: -8796.566406\n",
      "Train Epoch: 86 [16263/17352 (94%)] Loss: -138214.156250\n",
      "Train Epoch: 86 [16878/17352 (97%)] Loss: -76941.515625\n",
      "    epoch          : 86\n",
      "    loss           : -197587.53000471895\n",
      "    val_loss       : -108271.78187783559\n",
      "Train Epoch: 87 [128/17352 (1%)] Loss: -191326.421875\n",
      "Train Epoch: 87 [1536/17352 (9%)] Loss: -212804.140625\n",
      "Train Epoch: 87 [2944/17352 (17%)] Loss: -205702.953125\n",
      "Train Epoch: 87 [4352/17352 (25%)] Loss: -223380.125000\n",
      "Train Epoch: 87 [5760/17352 (33%)] Loss: -211140.328125\n",
      "Train Epoch: 87 [7168/17352 (41%)] Loss: -232573.437500\n",
      "Train Epoch: 87 [8576/17352 (49%)] Loss: -210749.953125\n",
      "Train Epoch: 87 [9984/17352 (58%)] Loss: -200856.593750\n",
      "Train Epoch: 87 [11392/17352 (66%)] Loss: -203789.968750\n",
      "Train Epoch: 87 [12800/17352 (74%)] Loss: -207023.609375\n",
      "Train Epoch: 87 [14208/17352 (82%)] Loss: -229995.187500\n",
      "Train Epoch: 87 [15491/17352 (89%)] Loss: -63312.968750\n",
      "Train Epoch: 87 [16413/17352 (95%)] Loss: -118690.109375\n",
      "Train Epoch: 87 [17061/17352 (98%)] Loss: -176290.125000\n",
      "    epoch          : 87\n",
      "    loss           : -198570.62183436452\n",
      "    val_loss       : -107541.89504445394\n",
      "Train Epoch: 88 [128/17352 (1%)] Loss: -221819.218750\n",
      "Train Epoch: 88 [1536/17352 (9%)] Loss: -207924.953125\n",
      "Train Epoch: 88 [2944/17352 (17%)] Loss: -204602.625000\n",
      "Train Epoch: 88 [4352/17352 (25%)] Loss: -204080.375000\n",
      "Train Epoch: 88 [5760/17352 (33%)] Loss: -210557.906250\n",
      "Train Epoch: 88 [7168/17352 (41%)] Loss: -234701.968750\n",
      "Train Epoch: 88 [8576/17352 (49%)] Loss: -215640.484375\n",
      "Train Epoch: 88 [9984/17352 (58%)] Loss: -194329.359375\n",
      "Train Epoch: 88 [11392/17352 (66%)] Loss: -207176.453125\n",
      "Train Epoch: 88 [12800/17352 (74%)] Loss: -204374.578125\n",
      "Train Epoch: 88 [14208/17352 (82%)] Loss: -210830.406250\n",
      "Train Epoch: 88 [15491/17352 (89%)] Loss: -86510.750000\n",
      "Train Epoch: 88 [16212/17352 (93%)] Loss: -182128.984375\n",
      "Train Epoch: 88 [16976/17352 (98%)] Loss: -187883.062500\n",
      "    epoch          : 88\n",
      "    loss           : -198927.39362678272\n",
      "    val_loss       : -109463.22580159505\n",
      "Train Epoch: 89 [128/17352 (1%)] Loss: -188311.906250\n",
      "Train Epoch: 89 [1536/17352 (9%)] Loss: -207646.500000\n",
      "Train Epoch: 89 [2944/17352 (17%)] Loss: -204040.625000\n",
      "Train Epoch: 89 [4352/17352 (25%)] Loss: -217144.625000\n",
      "Train Epoch: 89 [5760/17352 (33%)] Loss: -216397.078125\n",
      "Train Epoch: 89 [7168/17352 (41%)] Loss: -208331.187500\n",
      "Train Epoch: 89 [8576/17352 (49%)] Loss: -196706.968750\n",
      "Train Epoch: 89 [9984/17352 (58%)] Loss: -231571.578125\n",
      "Train Epoch: 89 [11392/17352 (66%)] Loss: -230593.859375\n",
      "Train Epoch: 89 [12800/17352 (74%)] Loss: -226112.406250\n",
      "Train Epoch: 89 [14208/17352 (82%)] Loss: -211415.015625\n",
      "Train Epoch: 89 [15535/17352 (90%)] Loss: -129835.312500\n",
      "Train Epoch: 89 [16311/17352 (94%)] Loss: -189225.843750\n",
      "Train Epoch: 89 [17111/17352 (99%)] Loss: -128857.429688\n",
      "    epoch          : 89\n",
      "    loss           : -198902.31581638003\n",
      "    val_loss       : -106930.62687180837\n",
      "Train Epoch: 90 [128/17352 (1%)] Loss: -224822.828125\n",
      "Train Epoch: 90 [1536/17352 (9%)] Loss: -206444.359375\n",
      "Train Epoch: 90 [2944/17352 (17%)] Loss: -216661.500000\n",
      "Train Epoch: 90 [4352/17352 (25%)] Loss: -215228.203125\n",
      "Train Epoch: 90 [5760/17352 (33%)] Loss: -234348.906250\n",
      "Train Epoch: 90 [7168/17352 (41%)] Loss: -221561.781250\n",
      "Train Epoch: 90 [8576/17352 (49%)] Loss: -211763.718750\n",
      "Train Epoch: 90 [9984/17352 (58%)] Loss: -200691.281250\n",
      "Train Epoch: 90 [11392/17352 (66%)] Loss: -211517.359375\n",
      "Train Epoch: 90 [12800/17352 (74%)] Loss: -225803.703125\n",
      "Train Epoch: 90 [14208/17352 (82%)] Loss: -222209.937500\n",
      "Train Epoch: 90 [15515/17352 (89%)] Loss: -166684.468750\n",
      "Train Epoch: 90 [16299/17352 (94%)] Loss: -5053.782227\n",
      "Train Epoch: 90 [17135/17352 (99%)] Loss: -144877.000000\n",
      "    epoch          : 90\n",
      "    loss           : -198342.28505138424\n",
      "    val_loss       : -107893.83356529872\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch90.pth ...\n",
      "Train Epoch: 91 [128/17352 (1%)] Loss: -210823.734375\n",
      "Train Epoch: 91 [1536/17352 (9%)] Loss: -217351.156250\n",
      "Train Epoch: 91 [2944/17352 (17%)] Loss: -210525.625000\n",
      "Train Epoch: 91 [4352/17352 (25%)] Loss: -218763.656250\n",
      "Train Epoch: 91 [5760/17352 (33%)] Loss: -210073.687500\n",
      "Train Epoch: 91 [7168/17352 (41%)] Loss: -254511.312500\n",
      "Train Epoch: 91 [8576/17352 (49%)] Loss: -242596.234375\n",
      "Train Epoch: 91 [9984/17352 (58%)] Loss: -235949.734375\n",
      "Train Epoch: 91 [11392/17352 (66%)] Loss: -227485.140625\n",
      "Train Epoch: 91 [12800/17352 (74%)] Loss: -201429.500000\n",
      "Train Epoch: 91 [14208/17352 (82%)] Loss: -229318.343750\n",
      "Train Epoch: 91 [15499/17352 (89%)] Loss: -88596.015625\n",
      "Train Epoch: 91 [16320/17352 (94%)] Loss: -146317.062500\n",
      "Train Epoch: 91 [16943/17352 (98%)] Loss: -5487.891602\n",
      "    epoch          : 91\n",
      "    loss           : -198170.81857566064\n",
      "    val_loss       : -109446.99857417743\n",
      "Train Epoch: 92 [128/17352 (1%)] Loss: -216871.125000\n",
      "Train Epoch: 92 [1536/17352 (9%)] Loss: -205383.531250\n",
      "Train Epoch: 92 [2944/17352 (17%)] Loss: -230971.343750\n",
      "Train Epoch: 92 [4352/17352 (25%)] Loss: -234203.468750\n",
      "Train Epoch: 92 [5760/17352 (33%)] Loss: -220676.843750\n",
      "Train Epoch: 92 [7168/17352 (41%)] Loss: -218042.046875\n",
      "Train Epoch: 92 [8576/17352 (49%)] Loss: -225850.968750\n",
      "Train Epoch: 92 [9984/17352 (58%)] Loss: -232689.437500\n",
      "Train Epoch: 92 [11392/17352 (66%)] Loss: -232238.843750\n",
      "Train Epoch: 92 [12800/17352 (74%)] Loss: -229852.625000\n",
      "Train Epoch: 92 [14208/17352 (82%)] Loss: -229645.250000\n",
      "Train Epoch: 92 [15523/17352 (89%)] Loss: -113970.312500\n",
      "Train Epoch: 92 [16308/17352 (94%)] Loss: -26773.789062\n",
      "Train Epoch: 92 [17053/17352 (98%)] Loss: -82908.039062\n",
      "    epoch          : 92\n",
      "    loss           : -198332.59053520867\n",
      "    val_loss       : -110609.55342311859\n",
      "Train Epoch: 93 [128/17352 (1%)] Loss: -230103.062500\n",
      "Train Epoch: 93 [1536/17352 (9%)] Loss: -226588.500000\n",
      "Train Epoch: 93 [2944/17352 (17%)] Loss: -232064.531250\n",
      "Train Epoch: 93 [4352/17352 (25%)] Loss: -217348.562500\n",
      "Train Epoch: 93 [5760/17352 (33%)] Loss: -231150.156250\n",
      "Train Epoch: 93 [7168/17352 (41%)] Loss: -202821.500000\n",
      "Train Epoch: 93 [8576/17352 (49%)] Loss: -230550.453125\n",
      "Train Epoch: 93 [9984/17352 (58%)] Loss: -229931.000000\n",
      "Train Epoch: 93 [11392/17352 (66%)] Loss: -214867.093750\n",
      "Train Epoch: 93 [12800/17352 (74%)] Loss: -231835.187500\n",
      "Train Epoch: 93 [14208/17352 (82%)] Loss: -211621.437500\n",
      "Train Epoch: 93 [15515/17352 (89%)] Loss: -167280.921875\n",
      "Train Epoch: 93 [16223/17352 (93%)] Loss: -136997.656250\n",
      "Train Epoch: 93 [16992/17352 (98%)] Loss: -133851.093750\n",
      "    epoch          : 93\n",
      "    loss           : -198078.0096050493\n",
      "    val_loss       : -109244.09901428223\n",
      "Train Epoch: 94 [128/17352 (1%)] Loss: -225579.250000\n",
      "Train Epoch: 94 [1536/17352 (9%)] Loss: -216673.546875\n",
      "Train Epoch: 94 [2944/17352 (17%)] Loss: -231554.843750\n",
      "Train Epoch: 94 [4352/17352 (25%)] Loss: -218999.468750\n",
      "Train Epoch: 94 [5760/17352 (33%)] Loss: -226430.078125\n",
      "Train Epoch: 94 [7168/17352 (41%)] Loss: -214418.937500\n",
      "Train Epoch: 94 [8576/17352 (49%)] Loss: -199065.953125\n",
      "Train Epoch: 94 [9984/17352 (58%)] Loss: -237881.953125\n",
      "Train Epoch: 94 [11392/17352 (66%)] Loss: -225991.875000\n",
      "Train Epoch: 94 [12800/17352 (74%)] Loss: -225384.187500\n",
      "Train Epoch: 94 [14208/17352 (82%)] Loss: -212267.078125\n",
      "Train Epoch: 94 [15549/17352 (90%)] Loss: -172810.343750\n",
      "Train Epoch: 94 [16197/17352 (93%)] Loss: -143766.656250\n",
      "Train Epoch: 94 [17081/17352 (98%)] Loss: -160247.203125\n",
      "    epoch          : 94\n",
      "    loss           : -199182.3784572934\n",
      "    val_loss       : -109855.49216251374\n",
      "Train Epoch: 95 [128/17352 (1%)] Loss: -217882.937500\n",
      "Train Epoch: 95 [1536/17352 (9%)] Loss: -217946.140625\n",
      "Train Epoch: 95 [2944/17352 (17%)] Loss: -217498.750000\n",
      "Train Epoch: 95 [4352/17352 (25%)] Loss: -189516.093750\n",
      "Train Epoch: 95 [5760/17352 (33%)] Loss: -230971.406250\n",
      "Train Epoch: 95 [7168/17352 (41%)] Loss: -209044.406250\n",
      "Train Epoch: 95 [8576/17352 (49%)] Loss: -214181.109375\n",
      "Train Epoch: 95 [9984/17352 (58%)] Loss: -210403.296875\n",
      "Train Epoch: 95 [11392/17352 (66%)] Loss: -202103.453125\n",
      "Train Epoch: 95 [12800/17352 (74%)] Loss: -214381.875000\n",
      "Train Epoch: 95 [14208/17352 (82%)] Loss: -244205.406250\n",
      "Train Epoch: 95 [15592/17352 (90%)] Loss: -217245.906250\n",
      "Train Epoch: 95 [16293/17352 (94%)] Loss: -63661.750000\n",
      "Train Epoch: 95 [16949/17352 (98%)] Loss: -128459.304688\n",
      "    epoch          : 95\n",
      "    loss           : -198908.12432164955\n",
      "    val_loss       : -108524.19575821559\n",
      "Train Epoch: 96 [128/17352 (1%)] Loss: -198647.187500\n",
      "Train Epoch: 96 [1536/17352 (9%)] Loss: -214276.406250\n",
      "Train Epoch: 96 [2944/17352 (17%)] Loss: -193763.468750\n",
      "Train Epoch: 96 [4352/17352 (25%)] Loss: -208463.937500\n",
      "Train Epoch: 96 [5760/17352 (33%)] Loss: -229299.515625\n",
      "Train Epoch: 96 [7168/17352 (41%)] Loss: -227319.390625\n",
      "Train Epoch: 96 [8576/17352 (49%)] Loss: -216662.875000\n",
      "Train Epoch: 96 [9984/17352 (58%)] Loss: -191193.531250\n",
      "Train Epoch: 96 [11392/17352 (66%)] Loss: -200996.000000\n",
      "Train Epoch: 96 [12800/17352 (74%)] Loss: -225546.500000\n",
      "Train Epoch: 96 [14208/17352 (82%)] Loss: -226654.796875\n",
      "Train Epoch: 96 [15588/17352 (90%)] Loss: -180142.343750\n",
      "Train Epoch: 96 [16362/17352 (94%)] Loss: -155998.484375\n",
      "Train Epoch: 96 [17028/17352 (98%)] Loss: -26137.250000\n",
      "    epoch          : 96\n",
      "    loss           : -197875.79192009228\n",
      "    val_loss       : -109020.79155178864\n",
      "Train Epoch: 97 [128/17352 (1%)] Loss: -228439.828125\n",
      "Train Epoch: 97 [1536/17352 (9%)] Loss: -219161.031250\n",
      "Train Epoch: 97 [2944/17352 (17%)] Loss: -203064.968750\n",
      "Train Epoch: 97 [4352/17352 (25%)] Loss: -204937.281250\n",
      "Train Epoch: 97 [5760/17352 (33%)] Loss: -205378.375000\n",
      "Train Epoch: 97 [7168/17352 (41%)] Loss: -236422.218750\n",
      "Train Epoch: 97 [8576/17352 (49%)] Loss: -228003.031250\n",
      "Train Epoch: 97 [9984/17352 (58%)] Loss: -208406.625000\n",
      "Train Epoch: 97 [11392/17352 (66%)] Loss: -207807.875000\n",
      "Train Epoch: 97 [12800/17352 (74%)] Loss: -231327.265625\n",
      "Train Epoch: 97 [14208/17352 (82%)] Loss: -222251.250000\n",
      "Train Epoch: 97 [15448/17352 (89%)] Loss: -62564.085938\n",
      "Train Epoch: 97 [16262/17352 (94%)] Loss: -77480.875000\n",
      "Train Epoch: 97 [16922/17352 (98%)] Loss: -120323.843750\n",
      "    epoch          : 97\n",
      "    loss           : -198865.04496119966\n",
      "    val_loss       : -108498.9208111445\n",
      "Train Epoch: 98 [128/17352 (1%)] Loss: -226627.187500\n",
      "Train Epoch: 98 [1536/17352 (9%)] Loss: -218932.406250\n",
      "Train Epoch: 98 [2944/17352 (17%)] Loss: -236853.968750\n",
      "Train Epoch: 98 [4352/17352 (25%)] Loss: -208199.906250\n",
      "Train Epoch: 98 [5760/17352 (33%)] Loss: -212689.265625\n",
      "Train Epoch: 98 [7168/17352 (41%)] Loss: -217599.921875\n",
      "Train Epoch: 98 [8576/17352 (49%)] Loss: -232433.437500\n",
      "Train Epoch: 98 [9984/17352 (58%)] Loss: -206566.531250\n",
      "Train Epoch: 98 [11392/17352 (66%)] Loss: -231341.265625\n",
      "Train Epoch: 98 [12800/17352 (74%)] Loss: -220057.578125\n",
      "Train Epoch: 98 [14208/17352 (82%)] Loss: -194095.109375\n",
      "Train Epoch: 98 [15583/17352 (90%)] Loss: -229458.953125\n",
      "Train Epoch: 98 [16212/17352 (93%)] Loss: -145967.234375\n",
      "Train Epoch: 98 [17030/17352 (98%)] Loss: -5297.543945\n",
      "    epoch          : 98\n",
      "    loss           : -198512.34947829277\n",
      "    val_loss       : -109769.12098449071\n",
      "Train Epoch: 99 [128/17352 (1%)] Loss: -228435.781250\n",
      "Train Epoch: 99 [1536/17352 (9%)] Loss: -225464.390625\n",
      "Train Epoch: 99 [2944/17352 (17%)] Loss: -209220.406250\n",
      "Train Epoch: 99 [4352/17352 (25%)] Loss: -212856.609375\n",
      "Train Epoch: 99 [5760/17352 (33%)] Loss: -189793.765625\n",
      "Train Epoch: 99 [7168/17352 (41%)] Loss: -226507.812500\n",
      "Train Epoch: 99 [8576/17352 (49%)] Loss: -209428.734375\n",
      "Train Epoch: 99 [9984/17352 (58%)] Loss: -205632.812500\n",
      "Train Epoch: 99 [11392/17352 (66%)] Loss: -194420.359375\n",
      "Train Epoch: 99 [12800/17352 (74%)] Loss: -211250.218750\n",
      "Train Epoch: 99 [14208/17352 (82%)] Loss: -219042.031250\n",
      "Train Epoch: 99 [15574/17352 (90%)] Loss: -181216.203125\n",
      "Train Epoch: 99 [16234/17352 (94%)] Loss: -119927.882812\n",
      "Train Epoch: 99 [16940/17352 (98%)] Loss: -64229.460938\n",
      "    epoch          : 99\n",
      "    loss           : -198536.40206847736\n",
      "    val_loss       : -110093.14486796061\n",
      "Train Epoch: 100 [128/17352 (1%)] Loss: -216926.109375\n",
      "Train Epoch: 100 [1536/17352 (9%)] Loss: -219118.250000\n",
      "Train Epoch: 100 [2944/17352 (17%)] Loss: -191178.046875\n",
      "Train Epoch: 100 [4352/17352 (25%)] Loss: -235511.796875\n",
      "Train Epoch: 100 [5760/17352 (33%)] Loss: -204710.843750\n",
      "Train Epoch: 100 [7168/17352 (41%)] Loss: -253981.375000\n",
      "Train Epoch: 100 [8576/17352 (49%)] Loss: -233007.875000\n",
      "Train Epoch: 100 [9984/17352 (58%)] Loss: -208652.140625\n",
      "Train Epoch: 100 [11392/17352 (66%)] Loss: -231664.625000\n",
      "Train Epoch: 100 [12800/17352 (74%)] Loss: -231620.187500\n",
      "Train Epoch: 100 [14208/17352 (82%)] Loss: -210746.468750\n",
      "Train Epoch: 100 [15406/17352 (89%)] Loss: -62067.195312\n",
      "Train Epoch: 100 [16266/17352 (94%)] Loss: -140728.500000\n",
      "Train Epoch: 100 [17028/17352 (98%)] Loss: -148434.218750\n",
      "    epoch          : 100\n",
      "    loss           : -199173.22929556418\n",
      "    val_loss       : -109426.99556474686\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [128/17352 (1%)] Loss: -228324.781250\n",
      "Train Epoch: 101 [1536/17352 (9%)] Loss: -227557.250000\n",
      "Train Epoch: 101 [2944/17352 (17%)] Loss: -250888.812500\n",
      "Train Epoch: 101 [4352/17352 (25%)] Loss: -228984.812500\n",
      "Train Epoch: 101 [5760/17352 (33%)] Loss: -241830.593750\n",
      "Train Epoch: 101 [7168/17352 (41%)] Loss: -233991.765625\n",
      "Train Epoch: 101 [8576/17352 (49%)] Loss: -217626.250000\n",
      "Train Epoch: 101 [9984/17352 (58%)] Loss: -200647.718750\n",
      "Train Epoch: 101 [11392/17352 (66%)] Loss: -209847.718750\n",
      "Train Epoch: 101 [12800/17352 (74%)] Loss: -226101.546875\n",
      "Train Epoch: 101 [14208/17352 (82%)] Loss: -204682.843750\n",
      "Train Epoch: 101 [15559/17352 (90%)] Loss: -151703.437500\n",
      "Train Epoch: 101 [16190/17352 (93%)] Loss: -26885.277344\n",
      "Train Epoch: 101 [16918/17352 (97%)] Loss: -148559.093750\n",
      "    epoch          : 101\n",
      "    loss           : -198785.45085072357\n",
      "    val_loss       : -109035.46464525859\n",
      "Train Epoch: 102 [128/17352 (1%)] Loss: -228221.968750\n",
      "Train Epoch: 102 [1536/17352 (9%)] Loss: -229687.718750\n",
      "Train Epoch: 102 [2944/17352 (17%)] Loss: -207225.000000\n",
      "Train Epoch: 102 [4352/17352 (25%)] Loss: -221230.531250\n",
      "Train Epoch: 102 [5760/17352 (33%)] Loss: -206198.968750\n",
      "Train Epoch: 102 [7168/17352 (41%)] Loss: -218373.781250\n",
      "Train Epoch: 102 [8576/17352 (49%)] Loss: -194974.437500\n",
      "Train Epoch: 102 [9984/17352 (58%)] Loss: -190866.562500\n",
      "Train Epoch: 102 [11392/17352 (66%)] Loss: -210303.078125\n",
      "Train Epoch: 102 [12800/17352 (74%)] Loss: -196885.234375\n",
      "Train Epoch: 102 [14208/17352 (82%)] Loss: -230350.281250\n",
      "Train Epoch: 102 [15520/17352 (89%)] Loss: -127913.562500\n",
      "Train Epoch: 102 [16200/17352 (93%)] Loss: -162212.250000\n",
      "Train Epoch: 102 [17030/17352 (98%)] Loss: -143763.218750\n",
      "    epoch          : 102\n",
      "    loss           : -198632.76326552013\n",
      "    val_loss       : -107858.27356554667\n",
      "Train Epoch: 103 [128/17352 (1%)] Loss: -221918.640625\n",
      "Train Epoch: 103 [1536/17352 (9%)] Loss: -226787.406250\n",
      "Train Epoch: 103 [2944/17352 (17%)] Loss: -219127.640625\n",
      "Train Epoch: 103 [4352/17352 (25%)] Loss: -203364.468750\n",
      "Train Epoch: 103 [5760/17352 (33%)] Loss: -232678.000000\n",
      "Train Epoch: 103 [7168/17352 (41%)] Loss: -228833.250000\n",
      "Train Epoch: 103 [8576/17352 (49%)] Loss: -212618.593750\n",
      "Train Epoch: 103 [9984/17352 (58%)] Loss: -214628.187500\n",
      "Train Epoch: 103 [11392/17352 (66%)] Loss: -212843.859375\n",
      "Train Epoch: 103 [12800/17352 (74%)] Loss: -211042.093750\n",
      "Train Epoch: 103 [14208/17352 (82%)] Loss: -229093.968750\n",
      "Train Epoch: 103 [15514/17352 (89%)] Loss: -141394.312500\n",
      "Train Epoch: 103 [16228/17352 (94%)] Loss: -142008.000000\n",
      "Train Epoch: 103 [16930/17352 (98%)] Loss: -72891.859375\n",
      "    epoch          : 103\n",
      "    loss           : -198814.86534907194\n",
      "    val_loss       : -109456.71497939428\n",
      "Train Epoch: 104 [128/17352 (1%)] Loss: -227770.046875\n",
      "Train Epoch: 104 [1536/17352 (9%)] Loss: -238074.640625\n",
      "Train Epoch: 104 [2944/17352 (17%)] Loss: -217968.062500\n",
      "Train Epoch: 104 [4352/17352 (25%)] Loss: -234062.781250\n",
      "Train Epoch: 104 [5760/17352 (33%)] Loss: -221089.546875\n",
      "Train Epoch: 104 [7168/17352 (41%)] Loss: -232495.156250\n",
      "Train Epoch: 104 [8576/17352 (49%)] Loss: -208197.171875\n",
      "Train Epoch: 104 [9984/17352 (58%)] Loss: -233500.875000\n",
      "Train Epoch: 104 [11392/17352 (66%)] Loss: -225294.328125\n",
      "Train Epoch: 104 [12800/17352 (74%)] Loss: -225397.000000\n",
      "Train Epoch: 104 [14208/17352 (82%)] Loss: -233577.375000\n",
      "Train Epoch: 104 [15577/17352 (90%)] Loss: -139163.687500\n",
      "Train Epoch: 104 [16386/17352 (94%)] Loss: -62261.078125\n",
      "Train Epoch: 104 [17053/17352 (98%)] Loss: -27085.443359\n",
      "    epoch          : 104\n",
      "    loss           : -200399.04653418623\n",
      "    val_loss       : -109635.80084374746\n",
      "Train Epoch: 105 [128/17352 (1%)] Loss: -186828.406250\n",
      "Train Epoch: 105 [1536/17352 (9%)] Loss: -228486.140625\n",
      "Train Epoch: 105 [2944/17352 (17%)] Loss: -217106.890625\n",
      "Train Epoch: 105 [4352/17352 (25%)] Loss: -206681.812500\n",
      "Train Epoch: 105 [5760/17352 (33%)] Loss: -214264.953125\n",
      "Train Epoch: 105 [7168/17352 (41%)] Loss: -217137.656250\n",
      "Train Epoch: 105 [8576/17352 (49%)] Loss: -210746.578125\n",
      "Train Epoch: 105 [9984/17352 (58%)] Loss: -217225.875000\n",
      "Train Epoch: 105 [11392/17352 (66%)] Loss: -235376.562500\n",
      "Train Epoch: 105 [12800/17352 (74%)] Loss: -226518.859375\n",
      "Train Epoch: 105 [14208/17352 (82%)] Loss: -217687.625000\n",
      "Train Epoch: 105 [15491/17352 (89%)] Loss: -85979.148438\n",
      "Train Epoch: 105 [16326/17352 (94%)] Loss: -124861.546875\n",
      "Train Epoch: 105 [17063/17352 (98%)] Loss: -166022.812500\n",
      "    epoch          : 105\n",
      "    loss           : -198605.42675125838\n",
      "    val_loss       : -108819.2860771497\n",
      "Train Epoch: 106 [128/17352 (1%)] Loss: -216757.531250\n",
      "Train Epoch: 106 [1536/17352 (9%)] Loss: -216520.453125\n",
      "Train Epoch: 106 [2944/17352 (17%)] Loss: -204225.171875\n",
      "Train Epoch: 106 [4352/17352 (25%)] Loss: -228255.156250\n",
      "Train Epoch: 106 [5760/17352 (33%)] Loss: -219011.171875\n",
      "Train Epoch: 106 [7168/17352 (41%)] Loss: -226448.484375\n",
      "Train Epoch: 106 [8576/17352 (49%)] Loss: -216971.953125\n",
      "Train Epoch: 106 [9984/17352 (58%)] Loss: -217983.765625\n",
      "Train Epoch: 106 [11392/17352 (66%)] Loss: -188124.953125\n",
      "Train Epoch: 106 [12800/17352 (74%)] Loss: -212273.843750\n",
      "Train Epoch: 106 [14208/17352 (82%)] Loss: -221170.250000\n",
      "Train Epoch: 106 [15471/17352 (89%)] Loss: -135302.593750\n",
      "Train Epoch: 106 [16277/17352 (94%)] Loss: -164052.593750\n",
      "Train Epoch: 106 [17056/17352 (98%)] Loss: -177103.531250\n",
      "    epoch          : 106\n",
      "    loss           : -198978.4257517565\n",
      "    val_loss       : -110573.70406926473\n",
      "Train Epoch: 107 [128/17352 (1%)] Loss: -211054.218750\n",
      "Train Epoch: 107 [1536/17352 (9%)] Loss: -208644.843750\n",
      "Train Epoch: 107 [2944/17352 (17%)] Loss: -199864.640625\n",
      "Train Epoch: 107 [4352/17352 (25%)] Loss: -224778.093750\n",
      "Train Epoch: 107 [5760/17352 (33%)] Loss: -236036.062500\n",
      "Train Epoch: 107 [7168/17352 (41%)] Loss: -232316.828125\n",
      "Train Epoch: 107 [8576/17352 (49%)] Loss: -216519.875000\n",
      "Train Epoch: 107 [9984/17352 (58%)] Loss: -211230.000000\n",
      "Train Epoch: 107 [11392/17352 (66%)] Loss: -215864.140625\n",
      "Train Epoch: 107 [12800/17352 (74%)] Loss: -236822.234375\n",
      "Train Epoch: 107 [14208/17352 (82%)] Loss: -225667.562500\n",
      "Train Epoch: 107 [15500/17352 (89%)] Loss: -230122.968750\n",
      "Train Epoch: 107 [16239/17352 (94%)] Loss: -85563.796875\n",
      "Train Epoch: 107 [16975/17352 (98%)] Loss: -77073.359375\n",
      "    epoch          : 107\n",
      "    loss           : -199434.56645540582\n",
      "    val_loss       : -108286.62210585276\n",
      "Train Epoch: 108 [128/17352 (1%)] Loss: -222668.953125\n",
      "Train Epoch: 108 [1536/17352 (9%)] Loss: -215410.078125\n",
      "Train Epoch: 108 [2944/17352 (17%)] Loss: -237943.062500\n",
      "Train Epoch: 108 [4352/17352 (25%)] Loss: -213783.687500\n",
      "Train Epoch: 108 [5760/17352 (33%)] Loss: -219359.656250\n",
      "Train Epoch: 108 [7168/17352 (41%)] Loss: -252412.890625\n",
      "Train Epoch: 108 [8576/17352 (49%)] Loss: -216057.156250\n",
      "Train Epoch: 108 [9984/17352 (58%)] Loss: -198366.203125\n",
      "Train Epoch: 108 [11392/17352 (66%)] Loss: -221666.171875\n",
      "Train Epoch: 108 [12800/17352 (74%)] Loss: -212394.859375\n",
      "Train Epoch: 108 [14208/17352 (82%)] Loss: -216909.140625\n",
      "Train Epoch: 108 [15539/17352 (90%)] Loss: -155342.375000\n",
      "Train Epoch: 108 [16208/17352 (93%)] Loss: -79962.046875\n",
      "Train Epoch: 108 [16959/17352 (98%)] Loss: -227593.046875\n",
      "    epoch          : 108\n",
      "    loss           : -200115.96292339556\n",
      "    val_loss       : -110210.7804283142\n",
      "Train Epoch: 109 [128/17352 (1%)] Loss: -228683.500000\n",
      "Train Epoch: 109 [1536/17352 (9%)] Loss: -205517.281250\n",
      "Train Epoch: 109 [2944/17352 (17%)] Loss: -209902.234375\n",
      "Train Epoch: 109 [4352/17352 (25%)] Loss: -220464.703125\n",
      "Train Epoch: 109 [5760/17352 (33%)] Loss: -216414.125000\n",
      "Train Epoch: 109 [7168/17352 (41%)] Loss: -230471.781250\n",
      "Train Epoch: 109 [8576/17352 (49%)] Loss: -184620.984375\n",
      "Train Epoch: 109 [9984/17352 (58%)] Loss: -236345.359375\n",
      "Train Epoch: 109 [11392/17352 (66%)] Loss: -221401.687500\n",
      "Train Epoch: 109 [12800/17352 (74%)] Loss: -218771.750000\n",
      "Train Epoch: 109 [14208/17352 (82%)] Loss: -213818.921875\n",
      "Train Epoch: 109 [15570/17352 (90%)] Loss: -190681.734375\n",
      "Train Epoch: 109 [16366/17352 (94%)] Loss: -195646.906250\n",
      "Train Epoch: 109 [17120/17352 (99%)] Loss: -148060.171875\n",
      "    epoch          : 109\n",
      "    loss           : -200056.51491060192\n",
      "    val_loss       : -109781.20446689923\n",
      "Train Epoch: 110 [128/17352 (1%)] Loss: -208458.843750\n",
      "Train Epoch: 110 [1536/17352 (9%)] Loss: -218952.203125\n",
      "Train Epoch: 110 [2944/17352 (17%)] Loss: -187580.765625\n",
      "Train Epoch: 110 [4352/17352 (25%)] Loss: -226607.765625\n",
      "Train Epoch: 110 [5760/17352 (33%)] Loss: -215993.437500\n",
      "Train Epoch: 110 [7168/17352 (41%)] Loss: -224367.687500\n",
      "Train Epoch: 110 [8576/17352 (49%)] Loss: -197765.359375\n",
      "Train Epoch: 110 [9984/17352 (58%)] Loss: -244128.937500\n",
      "Train Epoch: 110 [11392/17352 (66%)] Loss: -192565.671875\n",
      "Train Epoch: 110 [12800/17352 (74%)] Loss: -190309.484375\n",
      "Train Epoch: 110 [14208/17352 (82%)] Loss: -225632.640625\n",
      "Train Epoch: 110 [15480/17352 (89%)] Loss: -64534.367188\n",
      "Train Epoch: 110 [16331/17352 (94%)] Loss: -132669.703125\n",
      "Train Epoch: 110 [16959/17352 (98%)] Loss: -4836.046875\n",
      "    epoch          : 110\n",
      "    loss           : -198751.78862337457\n",
      "    val_loss       : -109184.40244051615\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch110.pth ...\n",
      "Train Epoch: 111 [128/17352 (1%)] Loss: -196729.562500\n",
      "Train Epoch: 111 [1536/17352 (9%)] Loss: -216151.265625\n",
      "Train Epoch: 111 [2944/17352 (17%)] Loss: -198882.687500\n",
      "Train Epoch: 111 [4352/17352 (25%)] Loss: -218005.421875\n",
      "Train Epoch: 111 [5760/17352 (33%)] Loss: -204306.468750\n",
      "Train Epoch: 111 [7168/17352 (41%)] Loss: -222758.078125\n",
      "Train Epoch: 111 [8576/17352 (49%)] Loss: -192315.078125\n",
      "Train Epoch: 111 [9984/17352 (58%)] Loss: -221883.640625\n",
      "Train Epoch: 111 [11392/17352 (66%)] Loss: -232309.906250\n",
      "Train Epoch: 111 [12800/17352 (74%)] Loss: -217825.343750\n",
      "Train Epoch: 111 [14208/17352 (82%)] Loss: -217538.734375\n",
      "Train Epoch: 111 [15493/17352 (89%)] Loss: -73582.648438\n",
      "Train Epoch: 111 [16206/17352 (93%)] Loss: -161695.031250\n",
      "Train Epoch: 111 [17024/17352 (98%)] Loss: -63088.273438\n",
      "    epoch          : 111\n",
      "    loss           : -200268.16794253356\n",
      "    val_loss       : -109050.23415331841\n",
      "Train Epoch: 112 [128/17352 (1%)] Loss: -214178.281250\n",
      "Train Epoch: 112 [1536/17352 (9%)] Loss: -221618.531250\n",
      "Train Epoch: 112 [2944/17352 (17%)] Loss: -218294.718750\n",
      "Train Epoch: 112 [4352/17352 (25%)] Loss: -212058.125000\n",
      "Train Epoch: 112 [5760/17352 (33%)] Loss: -230469.859375\n",
      "Train Epoch: 112 [7168/17352 (41%)] Loss: -202151.031250\n",
      "Train Epoch: 112 [8576/17352 (49%)] Loss: -192840.203125\n",
      "Train Epoch: 112 [9984/17352 (58%)] Loss: -233078.656250\n",
      "Train Epoch: 112 [11392/17352 (66%)] Loss: -231304.625000\n",
      "Train Epoch: 112 [12800/17352 (74%)] Loss: -210864.593750\n",
      "Train Epoch: 112 [14208/17352 (82%)] Loss: -212373.468750\n",
      "Train Epoch: 112 [15550/17352 (90%)] Loss: -167029.500000\n",
      "Train Epoch: 112 [16290/17352 (94%)] Loss: -138750.000000\n",
      "Train Epoch: 112 [17037/17352 (98%)] Loss: -146079.031250\n",
      "    epoch          : 112\n",
      "    loss           : -200334.3468795879\n",
      "    val_loss       : -110108.51032975515\n",
      "Train Epoch: 113 [128/17352 (1%)] Loss: -189483.687500\n",
      "Train Epoch: 113 [1536/17352 (9%)] Loss: -188818.000000\n",
      "Train Epoch: 113 [2944/17352 (17%)] Loss: -190870.468750\n",
      "Train Epoch: 113 [4352/17352 (25%)] Loss: -228029.312500\n",
      "Train Epoch: 113 [5760/17352 (33%)] Loss: -233553.500000\n",
      "Train Epoch: 113 [7168/17352 (41%)] Loss: -217627.578125\n",
      "Train Epoch: 113 [8576/17352 (49%)] Loss: -215526.343750\n",
      "Train Epoch: 113 [9984/17352 (58%)] Loss: -226508.640625\n",
      "Train Epoch: 113 [11392/17352 (66%)] Loss: -226189.828125\n",
      "Train Epoch: 113 [12800/17352 (74%)] Loss: -199864.875000\n",
      "Train Epoch: 113 [14208/17352 (82%)] Loss: -230631.781250\n",
      "Train Epoch: 113 [15511/17352 (89%)] Loss: -134684.187500\n",
      "Train Epoch: 113 [16142/17352 (93%)] Loss: -73629.367188\n",
      "Train Epoch: 113 [17010/17352 (98%)] Loss: -193881.843750\n",
      "    epoch          : 113\n",
      "    loss           : -199373.6459403838\n",
      "    val_loss       : -109640.03906243642\n",
      "Train Epoch: 114 [128/17352 (1%)] Loss: -216646.437500\n",
      "Train Epoch: 114 [1536/17352 (9%)] Loss: -229841.406250\n",
      "Train Epoch: 114 [2944/17352 (17%)] Loss: -231466.390625\n",
      "Train Epoch: 114 [4352/17352 (25%)] Loss: -237793.250000\n",
      "Train Epoch: 114 [5760/17352 (33%)] Loss: -225757.031250\n",
      "Train Epoch: 114 [7168/17352 (41%)] Loss: -254430.718750\n",
      "Train Epoch: 114 [8576/17352 (49%)] Loss: -217183.843750\n",
      "Train Epoch: 114 [9984/17352 (58%)] Loss: -233909.906250\n",
      "Train Epoch: 114 [11392/17352 (66%)] Loss: -229710.046875\n",
      "Train Epoch: 114 [12800/17352 (74%)] Loss: -218501.796875\n",
      "Train Epoch: 114 [14208/17352 (82%)] Loss: -236298.750000\n",
      "Train Epoch: 114 [15499/17352 (89%)] Loss: -148231.515625\n",
      "Train Epoch: 114 [16246/17352 (94%)] Loss: -136334.640625\n",
      "Train Epoch: 114 [17002/17352 (98%)] Loss: -150743.750000\n",
      "    epoch          : 114\n",
      "    loss           : -200245.75080943268\n",
      "    val_loss       : -108717.19879948298\n",
      "Train Epoch: 115 [128/17352 (1%)] Loss: -218246.078125\n",
      "Train Epoch: 115 [1536/17352 (9%)] Loss: -192179.187500\n",
      "Train Epoch: 115 [2944/17352 (17%)] Loss: -215852.578125\n",
      "Train Epoch: 115 [4352/17352 (25%)] Loss: -215018.562500\n",
      "Train Epoch: 115 [5760/17352 (33%)] Loss: -227896.015625\n",
      "Train Epoch: 115 [7168/17352 (41%)] Loss: -221538.109375\n",
      "Train Epoch: 115 [8576/17352 (49%)] Loss: -220299.265625\n",
      "Train Epoch: 115 [9984/17352 (58%)] Loss: -233006.531250\n",
      "Train Epoch: 115 [11392/17352 (66%)] Loss: -206020.031250\n",
      "Train Epoch: 115 [12800/17352 (74%)] Loss: -219239.515625\n",
      "Train Epoch: 115 [14208/17352 (82%)] Loss: -219329.671875\n",
      "Train Epoch: 115 [15476/17352 (89%)] Loss: -131615.343750\n",
      "Train Epoch: 115 [16275/17352 (94%)] Loss: -152212.703125\n",
      "Train Epoch: 115 [16983/17352 (98%)] Loss: -126970.750000\n",
      "    epoch          : 115\n",
      "    loss           : -199347.43840119021\n",
      "    val_loss       : -110242.27936147054\n",
      "Train Epoch: 116 [128/17352 (1%)] Loss: -226278.062500\n",
      "Train Epoch: 116 [1536/17352 (9%)] Loss: -213742.671875\n",
      "Train Epoch: 116 [2944/17352 (17%)] Loss: -203138.343750\n",
      "Train Epoch: 116 [4352/17352 (25%)] Loss: -223700.250000\n",
      "Train Epoch: 116 [5760/17352 (33%)] Loss: -225474.875000\n",
      "Train Epoch: 116 [7168/17352 (41%)] Loss: -205394.250000\n",
      "Train Epoch: 116 [8576/17352 (49%)] Loss: -232683.765625\n",
      "Train Epoch: 116 [9984/17352 (58%)] Loss: -226648.515625\n",
      "Train Epoch: 116 [11392/17352 (66%)] Loss: -213475.671875\n",
      "Train Epoch: 116 [12800/17352 (74%)] Loss: -216241.593750\n",
      "Train Epoch: 116 [14208/17352 (82%)] Loss: -214525.281250\n",
      "Train Epoch: 116 [15550/17352 (90%)] Loss: -177573.984375\n",
      "Train Epoch: 116 [16262/17352 (94%)] Loss: -74810.859375\n",
      "Train Epoch: 116 [16946/17352 (98%)] Loss: -63872.855469\n",
      "    epoch          : 116\n",
      "    loss           : -199603.60579645555\n",
      "    val_loss       : -109770.89529164632\n",
      "Train Epoch: 117 [128/17352 (1%)] Loss: -199432.281250\n",
      "Train Epoch: 117 [1536/17352 (9%)] Loss: -232278.265625\n",
      "Train Epoch: 117 [2944/17352 (17%)] Loss: -210000.421875\n",
      "Train Epoch: 117 [4352/17352 (25%)] Loss: -218524.578125\n",
      "Train Epoch: 117 [5760/17352 (33%)] Loss: -236782.843750\n",
      "Train Epoch: 117 [7168/17352 (41%)] Loss: -211775.187500\n",
      "Train Epoch: 117 [8576/17352 (49%)] Loss: -212879.953125\n",
      "Train Epoch: 117 [9984/17352 (58%)] Loss: -202367.015625\n",
      "Train Epoch: 117 [11392/17352 (66%)] Loss: -218261.656250\n",
      "Train Epoch: 117 [12800/17352 (74%)] Loss: -215762.875000\n",
      "Train Epoch: 117 [14208/17352 (82%)] Loss: -242366.921875\n",
      "Train Epoch: 117 [15415/17352 (89%)] Loss: -9068.016602\n",
      "Train Epoch: 117 [16290/17352 (94%)] Loss: -148955.843750\n",
      "Train Epoch: 117 [16975/17352 (98%)] Loss: -5331.339844\n",
      "    epoch          : 117\n",
      "    loss           : -200860.90041028734\n",
      "    val_loss       : -110997.67642313639\n",
      "Train Epoch: 118 [128/17352 (1%)] Loss: -224266.953125\n",
      "Train Epoch: 118 [1536/17352 (9%)] Loss: -214993.656250\n",
      "Train Epoch: 118 [2944/17352 (17%)] Loss: -209389.421875\n",
      "Train Epoch: 118 [4352/17352 (25%)] Loss: -237628.015625\n",
      "Train Epoch: 118 [5760/17352 (33%)] Loss: -215032.515625\n",
      "Train Epoch: 118 [7168/17352 (41%)] Loss: -213608.875000\n",
      "Train Epoch: 118 [8576/17352 (49%)] Loss: -220549.921875\n",
      "Train Epoch: 118 [9984/17352 (58%)] Loss: -215325.562500\n",
      "Train Epoch: 118 [11392/17352 (66%)] Loss: -231819.437500\n",
      "Train Epoch: 118 [12800/17352 (74%)] Loss: -221828.328125\n",
      "Train Epoch: 118 [14208/17352 (82%)] Loss: -233943.187500\n",
      "Train Epoch: 118 [15505/17352 (89%)] Loss: -84910.140625\n",
      "Train Epoch: 118 [16199/17352 (93%)] Loss: -74496.367188\n",
      "Train Epoch: 118 [16996/17352 (98%)] Loss: -5748.661621\n",
      "    epoch          : 118\n",
      "    loss           : -199913.120382629\n",
      "    val_loss       : -111162.70967922211\n",
      "Train Epoch: 119 [128/17352 (1%)] Loss: -203161.687500\n",
      "Train Epoch: 119 [1536/17352 (9%)] Loss: -210514.515625\n",
      "Train Epoch: 119 [2944/17352 (17%)] Loss: -191727.078125\n",
      "Train Epoch: 119 [4352/17352 (25%)] Loss: -211360.343750\n",
      "Train Epoch: 119 [5760/17352 (33%)] Loss: -234926.593750\n",
      "Train Epoch: 119 [7168/17352 (41%)] Loss: -209079.890625\n",
      "Train Epoch: 119 [8576/17352 (49%)] Loss: -240188.421875\n",
      "Train Epoch: 119 [9984/17352 (58%)] Loss: -233134.750000\n",
      "Train Epoch: 119 [11392/17352 (66%)] Loss: -211498.015625\n",
      "Train Epoch: 119 [12800/17352 (74%)] Loss: -215804.281250\n",
      "Train Epoch: 119 [14208/17352 (82%)] Loss: -237855.375000\n",
      "Train Epoch: 119 [15580/17352 (90%)] Loss: -191130.156250\n",
      "Train Epoch: 119 [16173/17352 (93%)] Loss: -127982.976562\n",
      "Train Epoch: 119 [17080/17352 (98%)] Loss: -154371.046875\n",
      "    epoch          : 119\n",
      "    loss           : -199953.9766870281\n",
      "    val_loss       : -108683.91158868471\n",
      "Train Epoch: 120 [128/17352 (1%)] Loss: -212229.828125\n",
      "Train Epoch: 120 [1536/17352 (9%)] Loss: -228619.515625\n",
      "Train Epoch: 120 [2944/17352 (17%)] Loss: -197489.734375\n",
      "Train Epoch: 120 [4352/17352 (25%)] Loss: -208166.781250\n",
      "Train Epoch: 120 [5760/17352 (33%)] Loss: -220483.468750\n",
      "Train Epoch: 120 [7168/17352 (41%)] Loss: -220210.312500\n",
      "Train Epoch: 120 [8576/17352 (49%)] Loss: -208999.093750\n",
      "Train Epoch: 120 [9984/17352 (58%)] Loss: -224794.843750\n",
      "Train Epoch: 120 [11392/17352 (66%)] Loss: -219936.906250\n",
      "Train Epoch: 120 [12800/17352 (74%)] Loss: -219880.781250\n",
      "Train Epoch: 120 [14208/17352 (82%)] Loss: -217060.656250\n",
      "Train Epoch: 120 [15416/17352 (89%)] Loss: -67173.984375\n",
      "Train Epoch: 120 [16183/17352 (93%)] Loss: -145454.750000\n",
      "Train Epoch: 120 [16973/17352 (98%)] Loss: -5237.174805\n",
      "    epoch          : 120\n",
      "    loss           : -199643.98649197776\n",
      "    val_loss       : -110399.86877237956\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch120.pth ...\n",
      "Train Epoch: 121 [128/17352 (1%)] Loss: -212694.171875\n",
      "Train Epoch: 121 [1536/17352 (9%)] Loss: -234585.312500\n",
      "Train Epoch: 121 [2944/17352 (17%)] Loss: -203042.718750\n",
      "Train Epoch: 121 [4352/17352 (25%)] Loss: -212464.953125\n",
      "Train Epoch: 121 [5760/17352 (33%)] Loss: -231277.312500\n",
      "Train Epoch: 121 [7168/17352 (41%)] Loss: -225705.750000\n",
      "Train Epoch: 121 [8576/17352 (49%)] Loss: -215142.562500\n",
      "Train Epoch: 121 [9984/17352 (58%)] Loss: -226282.843750\n",
      "Train Epoch: 121 [11392/17352 (66%)] Loss: -228795.062500\n",
      "Train Epoch: 121 [12800/17352 (74%)] Loss: -211881.703125\n",
      "Train Epoch: 121 [14208/17352 (82%)] Loss: -219707.093750\n",
      "Train Epoch: 121 [15554/17352 (90%)] Loss: -169474.406250\n",
      "Train Epoch: 121 [16428/17352 (95%)] Loss: -183467.062500\n",
      "Train Epoch: 121 [17047/17352 (98%)] Loss: -84215.078125\n",
      "    epoch          : 121\n",
      "    loss           : -200102.4146851405\n",
      "    val_loss       : -110331.3015557607\n",
      "Train Epoch: 122 [128/17352 (1%)] Loss: -230342.671875\n",
      "Train Epoch: 122 [1536/17352 (9%)] Loss: -210667.750000\n",
      "Train Epoch: 122 [2944/17352 (17%)] Loss: -210315.343750\n",
      "Train Epoch: 122 [4352/17352 (25%)] Loss: -236926.875000\n",
      "Train Epoch: 122 [5760/17352 (33%)] Loss: -229136.390625\n",
      "Train Epoch: 122 [7168/17352 (41%)] Loss: -227600.250000\n",
      "Train Epoch: 122 [8576/17352 (49%)] Loss: -210388.468750\n",
      "Train Epoch: 122 [9984/17352 (58%)] Loss: -214689.125000\n",
      "Train Epoch: 122 [11392/17352 (66%)] Loss: -223411.218750\n",
      "Train Epoch: 122 [12800/17352 (74%)] Loss: -236119.750000\n",
      "Train Epoch: 122 [14208/17352 (82%)] Loss: -218413.734375\n",
      "Train Epoch: 122 [15460/17352 (89%)] Loss: -167590.875000\n",
      "Train Epoch: 122 [16108/17352 (93%)] Loss: -189147.015625\n",
      "Train Epoch: 122 [16863/17352 (97%)] Loss: -163303.953125\n",
      "    epoch          : 122\n",
      "    loss           : -199575.3355540845\n",
      "    val_loss       : -109510.38442794482\n",
      "Train Epoch: 123 [128/17352 (1%)] Loss: -226177.390625\n",
      "Train Epoch: 123 [1536/17352 (9%)] Loss: -219629.890625\n",
      "Train Epoch: 123 [2944/17352 (17%)] Loss: -216785.968750\n",
      "Train Epoch: 123 [4352/17352 (25%)] Loss: -223102.843750\n",
      "Train Epoch: 123 [5760/17352 (33%)] Loss: -224723.687500\n",
      "Train Epoch: 123 [7168/17352 (41%)] Loss: -255005.218750\n",
      "Train Epoch: 123 [8576/17352 (49%)] Loss: -209981.781250\n",
      "Train Epoch: 123 [9984/17352 (58%)] Loss: -198840.859375\n",
      "Train Epoch: 123 [11392/17352 (66%)] Loss: -224327.218750\n",
      "Train Epoch: 123 [12800/17352 (74%)] Loss: -211736.437500\n",
      "Train Epoch: 123 [14208/17352 (82%)] Loss: -199934.796875\n",
      "Train Epoch: 123 [15457/17352 (89%)] Loss: -77224.000000\n",
      "Train Epoch: 123 [16223/17352 (93%)] Loss: -129814.101562\n",
      "Train Epoch: 123 [17079/17352 (98%)] Loss: -144647.843750\n",
      "    epoch          : 123\n",
      "    loss           : -200092.1884110214\n",
      "    val_loss       : -110478.49025147756\n",
      "Train Epoch: 124 [128/17352 (1%)] Loss: -206161.234375\n",
      "Train Epoch: 124 [1536/17352 (9%)] Loss: -222287.187500\n",
      "Train Epoch: 124 [2944/17352 (17%)] Loss: -213464.812500\n",
      "Train Epoch: 124 [4352/17352 (25%)] Loss: -224899.781250\n",
      "Train Epoch: 124 [5760/17352 (33%)] Loss: -227041.093750\n",
      "Train Epoch: 124 [7168/17352 (41%)] Loss: -244600.921875\n",
      "Train Epoch: 124 [8576/17352 (49%)] Loss: -214805.984375\n",
      "Train Epoch: 124 [9984/17352 (58%)] Loss: -217012.421875\n",
      "Train Epoch: 124 [11392/17352 (66%)] Loss: -202850.078125\n",
      "Train Epoch: 124 [12800/17352 (74%)] Loss: -232416.562500\n",
      "Train Epoch: 124 [14208/17352 (82%)] Loss: -237102.937500\n",
      "Train Epoch: 124 [15538/17352 (90%)] Loss: -147269.062500\n",
      "Train Epoch: 124 [16346/17352 (94%)] Loss: -217075.156250\n",
      "Train Epoch: 124 [17189/17352 (99%)] Loss: -130585.101562\n",
      "    epoch          : 124\n",
      "    loss           : -200465.97267263528\n",
      "    val_loss       : -110026.37524763744\n",
      "Train Epoch: 125 [128/17352 (1%)] Loss: -190922.312500\n",
      "Train Epoch: 125 [1536/17352 (9%)] Loss: -227791.578125\n",
      "Train Epoch: 125 [2944/17352 (17%)] Loss: -217981.750000\n",
      "Train Epoch: 125 [4352/17352 (25%)] Loss: -214688.468750\n",
      "Train Epoch: 125 [5760/17352 (33%)] Loss: -228626.000000\n",
      "Train Epoch: 125 [7168/17352 (41%)] Loss: -220970.281250\n",
      "Train Epoch: 125 [8576/17352 (49%)] Loss: -217499.078125\n",
      "Train Epoch: 125 [9984/17352 (58%)] Loss: -218316.437500\n",
      "Train Epoch: 125 [11392/17352 (66%)] Loss: -207470.218750\n",
      "Train Epoch: 125 [12800/17352 (74%)] Loss: -226985.546875\n",
      "Train Epoch: 125 [14208/17352 (82%)] Loss: -239978.343750\n",
      "Train Epoch: 125 [15553/17352 (90%)] Loss: -188135.593750\n",
      "Train Epoch: 125 [16223/17352 (93%)] Loss: -27696.101562\n",
      "Train Epoch: 125 [16959/17352 (98%)] Loss: -5701.117188\n",
      "    epoch          : 125\n",
      "    loss           : -200813.1671822567\n",
      "    val_loss       : -110188.54674038888\n",
      "Train Epoch: 126 [128/17352 (1%)] Loss: -228966.687500\n",
      "Train Epoch: 126 [1536/17352 (9%)] Loss: -227457.265625\n",
      "Train Epoch: 126 [2944/17352 (17%)] Loss: -229235.078125\n",
      "Train Epoch: 126 [4352/17352 (25%)] Loss: -218945.093750\n",
      "Train Epoch: 126 [5760/17352 (33%)] Loss: -203329.156250\n",
      "Train Epoch: 126 [7168/17352 (41%)] Loss: -228890.343750\n",
      "Train Epoch: 126 [8576/17352 (49%)] Loss: -214076.953125\n",
      "Train Epoch: 126 [9984/17352 (58%)] Loss: -223733.390625\n",
      "Train Epoch: 126 [11392/17352 (66%)] Loss: -233313.437500\n",
      "Train Epoch: 126 [12800/17352 (74%)] Loss: -212572.750000\n",
      "Train Epoch: 126 [14208/17352 (82%)] Loss: -212453.937500\n",
      "Train Epoch: 126 [15523/17352 (89%)] Loss: -198546.703125\n",
      "Train Epoch: 126 [16275/17352 (94%)] Loss: -24812.589844\n",
      "Train Epoch: 126 [17094/17352 (99%)] Loss: -155950.015625\n",
      "    epoch          : 126\n",
      "    loss           : -200768.3527291317\n",
      "    val_loss       : -110283.15701073011\n",
      "Train Epoch: 127 [128/17352 (1%)] Loss: -204224.984375\n",
      "Train Epoch: 127 [1536/17352 (9%)] Loss: -231592.890625\n",
      "Train Epoch: 127 [2944/17352 (17%)] Loss: -231002.500000\n",
      "Train Epoch: 127 [4352/17352 (25%)] Loss: -240522.468750\n",
      "Train Epoch: 127 [5760/17352 (33%)] Loss: -222575.281250\n",
      "Train Epoch: 127 [7168/17352 (41%)] Loss: -201921.562500\n",
      "Train Epoch: 127 [8576/17352 (49%)] Loss: -222082.765625\n",
      "Train Epoch: 127 [9984/17352 (58%)] Loss: -225980.062500\n",
      "Train Epoch: 127 [11392/17352 (66%)] Loss: -210742.234375\n",
      "Train Epoch: 127 [12800/17352 (74%)] Loss: -214822.359375\n",
      "Train Epoch: 127 [14208/17352 (82%)] Loss: -226643.687500\n",
      "Train Epoch: 127 [15547/17352 (90%)] Loss: -173717.656250\n",
      "Train Epoch: 127 [16270/17352 (94%)] Loss: -63103.441406\n",
      "Train Epoch: 127 [16946/17352 (98%)] Loss: -5321.013672\n",
      "    epoch          : 127\n",
      "    loss           : -200952.7650089136\n",
      "    val_loss       : -110098.95170462926\n",
      "Train Epoch: 128 [128/17352 (1%)] Loss: -227797.765625\n",
      "Train Epoch: 128 [1536/17352 (9%)] Loss: -233066.500000\n",
      "Train Epoch: 128 [2944/17352 (17%)] Loss: -226465.718750\n",
      "Train Epoch: 128 [4352/17352 (25%)] Loss: -230868.359375\n",
      "Train Epoch: 128 [5760/17352 (33%)] Loss: -217086.125000\n",
      "Train Epoch: 128 [7168/17352 (41%)] Loss: -226735.156250\n",
      "Train Epoch: 128 [8576/17352 (49%)] Loss: -234633.375000\n",
      "Train Epoch: 128 [9984/17352 (58%)] Loss: -210613.343750\n",
      "Train Epoch: 128 [11392/17352 (66%)] Loss: -211788.609375\n",
      "Train Epoch: 128 [12800/17352 (74%)] Loss: -216701.750000\n",
      "Train Epoch: 128 [14208/17352 (82%)] Loss: -220251.250000\n",
      "Train Epoch: 128 [15489/17352 (89%)] Loss: -63623.218750\n",
      "Train Epoch: 128 [16270/17352 (94%)] Loss: -183502.375000\n",
      "Train Epoch: 128 [16943/17352 (98%)] Loss: -8962.371094\n",
      "    epoch          : 128\n",
      "    loss           : -201536.74360974203\n",
      "    val_loss       : -110144.8967274348\n",
      "Train Epoch: 129 [128/17352 (1%)] Loss: -191088.343750\n",
      "Train Epoch: 129 [1536/17352 (9%)] Loss: -234967.218750\n",
      "Train Epoch: 129 [2944/17352 (17%)] Loss: -238064.875000\n",
      "Train Epoch: 129 [4352/17352 (25%)] Loss: -229400.406250\n",
      "Train Epoch: 129 [5760/17352 (33%)] Loss: -229269.687500\n",
      "Train Epoch: 129 [7168/17352 (41%)] Loss: -202835.656250\n",
      "Train Epoch: 129 [8576/17352 (49%)] Loss: -242191.000000\n",
      "Train Epoch: 129 [9984/17352 (58%)] Loss: -211679.781250\n",
      "Train Epoch: 129 [11392/17352 (66%)] Loss: -213956.406250\n",
      "Train Epoch: 129 [12800/17352 (74%)] Loss: -225233.187500\n",
      "Train Epoch: 129 [14208/17352 (82%)] Loss: -229940.375000\n",
      "Train Epoch: 129 [15533/17352 (90%)] Loss: -157404.890625\n",
      "Train Epoch: 129 [16403/17352 (95%)] Loss: -151643.375000\n",
      "Train Epoch: 129 [16914/17352 (97%)] Loss: -24595.878906\n",
      "    epoch          : 129\n",
      "    loss           : -200278.62501638528\n",
      "    val_loss       : -110965.87533404032\n",
      "Train Epoch: 130 [128/17352 (1%)] Loss: -205026.093750\n",
      "Train Epoch: 130 [1536/17352 (9%)] Loss: -219405.562500\n",
      "Train Epoch: 130 [2944/17352 (17%)] Loss: -197351.500000\n",
      "Train Epoch: 130 [4352/17352 (25%)] Loss: -228202.234375\n",
      "Train Epoch: 130 [5760/17352 (33%)] Loss: -231002.203125\n",
      "Train Epoch: 130 [7168/17352 (41%)] Loss: -218720.218750\n",
      "Train Epoch: 130 [8576/17352 (49%)] Loss: -216742.687500\n",
      "Train Epoch: 130 [9984/17352 (58%)] Loss: -205932.343750\n",
      "Train Epoch: 130 [11392/17352 (66%)] Loss: -220479.234375\n",
      "Train Epoch: 130 [12800/17352 (74%)] Loss: -228456.093750\n",
      "Train Epoch: 130 [14208/17352 (82%)] Loss: -216245.734375\n",
      "Train Epoch: 130 [15388/17352 (89%)] Loss: -23909.730469\n",
      "Train Epoch: 130 [16217/17352 (93%)] Loss: -165776.390625\n",
      "Train Epoch: 130 [17017/17352 (98%)] Loss: -183860.000000\n",
      "    epoch          : 130\n",
      "    loss           : -201467.25492541422\n",
      "    val_loss       : -111129.56533870698\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch130.pth ...\n",
      "Train Epoch: 131 [128/17352 (1%)] Loss: -234152.500000\n",
      "Train Epoch: 131 [1536/17352 (9%)] Loss: -217861.250000\n",
      "Train Epoch: 131 [2944/17352 (17%)] Loss: -207544.109375\n",
      "Train Epoch: 131 [4352/17352 (25%)] Loss: -218968.171875\n",
      "Train Epoch: 131 [5760/17352 (33%)] Loss: -244272.984375\n",
      "Train Epoch: 131 [7168/17352 (41%)] Loss: -219112.984375\n",
      "Train Epoch: 131 [8576/17352 (49%)] Loss: -220655.046875\n",
      "Train Epoch: 131 [9984/17352 (58%)] Loss: -234133.406250\n",
      "Train Epoch: 131 [11392/17352 (66%)] Loss: -209990.812500\n",
      "Train Epoch: 131 [12800/17352 (74%)] Loss: -213088.781250\n",
      "Train Epoch: 131 [14208/17352 (82%)] Loss: -210401.062500\n",
      "Train Epoch: 131 [15529/17352 (89%)] Loss: -189474.187500\n",
      "Train Epoch: 131 [16224/17352 (93%)] Loss: -163280.953125\n",
      "Train Epoch: 131 [16987/17352 (98%)] Loss: -164428.312500\n",
      "    epoch          : 131\n",
      "    loss           : -200746.62088401846\n",
      "    val_loss       : -109786.88648866018\n",
      "Train Epoch: 132 [128/17352 (1%)] Loss: -225773.312500\n",
      "Train Epoch: 132 [1536/17352 (9%)] Loss: -204898.250000\n",
      "Train Epoch: 132 [2944/17352 (17%)] Loss: -193047.687500\n",
      "Train Epoch: 132 [4352/17352 (25%)] Loss: -227545.671875\n",
      "Train Epoch: 132 [5760/17352 (33%)] Loss: -206360.625000\n",
      "Train Epoch: 132 [7168/17352 (41%)] Loss: -221693.140625\n",
      "Train Epoch: 132 [8576/17352 (49%)] Loss: -223388.218750\n",
      "Train Epoch: 132 [9984/17352 (58%)] Loss: -210808.156250\n",
      "Train Epoch: 132 [11392/17352 (66%)] Loss: -228889.937500\n",
      "Train Epoch: 132 [12800/17352 (74%)] Loss: -226542.250000\n",
      "Train Epoch: 132 [14208/17352 (82%)] Loss: -219217.625000\n",
      "Train Epoch: 132 [15448/17352 (89%)] Loss: -126391.039062\n",
      "Train Epoch: 132 [16337/17352 (94%)] Loss: -144564.031250\n",
      "Train Epoch: 132 [16977/17352 (98%)] Loss: -213044.625000\n",
      "    epoch          : 132\n",
      "    loss           : -200175.3679150063\n",
      "    val_loss       : -110713.6330739975\n",
      "Train Epoch: 133 [128/17352 (1%)] Loss: -230490.375000\n",
      "Train Epoch: 133 [1536/17352 (9%)] Loss: -221658.828125\n",
      "Train Epoch: 133 [2944/17352 (17%)] Loss: -204645.171875\n",
      "Train Epoch: 133 [4352/17352 (25%)] Loss: -228020.937500\n",
      "Train Epoch: 133 [5760/17352 (33%)] Loss: -220435.265625\n",
      "Train Epoch: 133 [7168/17352 (41%)] Loss: -203356.468750\n",
      "Train Epoch: 133 [8576/17352 (49%)] Loss: -201296.562500\n",
      "Train Epoch: 133 [9984/17352 (58%)] Loss: -238534.156250\n",
      "Train Epoch: 133 [11392/17352 (66%)] Loss: -211970.375000\n",
      "Train Epoch: 133 [12800/17352 (74%)] Loss: -199343.671875\n",
      "Train Epoch: 133 [14208/17352 (82%)] Loss: -222213.687500\n",
      "Train Epoch: 133 [15584/17352 (90%)] Loss: -185575.375000\n",
      "Train Epoch: 133 [16305/17352 (94%)] Loss: -126312.601562\n",
      "Train Epoch: 133 [16922/17352 (98%)] Loss: -180154.171875\n",
      "    epoch          : 133\n",
      "    loss           : -200662.97567114094\n",
      "    val_loss       : -110549.9327809016\n",
      "Train Epoch: 134 [128/17352 (1%)] Loss: -209341.312500\n",
      "Train Epoch: 134 [1536/17352 (9%)] Loss: -204351.375000\n",
      "Train Epoch: 134 [2944/17352 (17%)] Loss: -215410.390625\n",
      "Train Epoch: 134 [4352/17352 (25%)] Loss: -227579.796875\n",
      "Train Epoch: 134 [5760/17352 (33%)] Loss: -240648.828125\n",
      "Train Epoch: 134 [7168/17352 (41%)] Loss: -230824.953125\n",
      "Train Epoch: 134 [8576/17352 (49%)] Loss: -212341.359375\n",
      "Train Epoch: 134 [9984/17352 (58%)] Loss: -224474.765625\n",
      "Train Epoch: 134 [11392/17352 (66%)] Loss: -225369.906250\n",
      "Train Epoch: 134 [12800/17352 (74%)] Loss: -213558.468750\n",
      "Train Epoch: 134 [14208/17352 (82%)] Loss: -220514.640625\n",
      "Train Epoch: 134 [15523/17352 (89%)] Loss: -138439.921875\n",
      "Train Epoch: 134 [16360/17352 (94%)] Loss: -182479.781250\n",
      "Train Epoch: 134 [16956/17352 (98%)] Loss: -5524.193848\n",
      "    epoch          : 134\n",
      "    loss           : -200806.2038295669\n",
      "    val_loss       : -110194.17981646856\n",
      "Train Epoch: 135 [128/17352 (1%)] Loss: -217391.312500\n",
      "Train Epoch: 135 [1536/17352 (9%)] Loss: -226837.796875\n",
      "Train Epoch: 135 [2944/17352 (17%)] Loss: -206182.437500\n",
      "Train Epoch: 135 [4352/17352 (25%)] Loss: -225353.390625\n",
      "Train Epoch: 135 [5760/17352 (33%)] Loss: -197732.531250\n",
      "Train Epoch: 135 [7168/17352 (41%)] Loss: -214909.718750\n",
      "Train Epoch: 135 [8576/17352 (49%)] Loss: -219360.281250\n",
      "Train Epoch: 135 [9984/17352 (58%)] Loss: -214469.875000\n",
      "Train Epoch: 135 [11392/17352 (66%)] Loss: -224620.687500\n",
      "Train Epoch: 135 [12800/17352 (74%)] Loss: -209584.359375\n",
      "Train Epoch: 135 [14208/17352 (82%)] Loss: -213234.968750\n",
      "Train Epoch: 135 [15478/17352 (89%)] Loss: -139707.062500\n",
      "Train Epoch: 135 [16398/17352 (95%)] Loss: -8997.591797\n",
      "Train Epoch: 135 [17070/17352 (98%)] Loss: -141856.750000\n",
      "    epoch          : 135\n",
      "    loss           : -201179.0099950189\n",
      "    val_loss       : -111334.18671798706\n",
      "Train Epoch: 136 [128/17352 (1%)] Loss: -218516.750000\n",
      "Train Epoch: 136 [1536/17352 (9%)] Loss: -222082.109375\n",
      "Train Epoch: 136 [2944/17352 (17%)] Loss: -239684.859375\n",
      "Train Epoch: 136 [4352/17352 (25%)] Loss: -208357.125000\n",
      "Train Epoch: 136 [5760/17352 (33%)] Loss: -225087.562500\n",
      "Train Epoch: 136 [7168/17352 (41%)] Loss: -233307.296875\n",
      "Train Epoch: 136 [8576/17352 (49%)] Loss: -232559.078125\n",
      "Train Epoch: 136 [9984/17352 (58%)] Loss: -207984.921875\n",
      "Train Epoch: 136 [11392/17352 (66%)] Loss: -207695.046875\n",
      "Train Epoch: 136 [12800/17352 (74%)] Loss: -215927.250000\n",
      "Train Epoch: 136 [14208/17352 (82%)] Loss: -220636.281250\n",
      "Train Epoch: 136 [15504/17352 (89%)] Loss: -148021.343750\n",
      "Train Epoch: 136 [16378/17352 (94%)] Loss: -152285.593750\n",
      "Train Epoch: 136 [16848/17352 (97%)] Loss: -23898.734375\n",
      "    epoch          : 136\n",
      "    loss           : -202063.11109545408\n",
      "    val_loss       : -110536.79271200499\n",
      "Train Epoch: 137 [128/17352 (1%)] Loss: -231446.343750\n",
      "Train Epoch: 137 [1536/17352 (9%)] Loss: -216795.250000\n",
      "Train Epoch: 137 [2944/17352 (17%)] Loss: -222474.859375\n",
      "Train Epoch: 137 [4352/17352 (25%)] Loss: -223141.734375\n",
      "Train Epoch: 137 [5760/17352 (33%)] Loss: -218018.781250\n",
      "Train Epoch: 137 [7168/17352 (41%)] Loss: -217293.562500\n",
      "Train Epoch: 137 [8576/17352 (49%)] Loss: -219714.218750\n",
      "Train Epoch: 137 [9984/17352 (58%)] Loss: -229135.718750\n",
      "Train Epoch: 137 [11392/17352 (66%)] Loss: -229646.515625\n",
      "Train Epoch: 137 [12800/17352 (74%)] Loss: -234567.250000\n",
      "Train Epoch: 137 [14208/17352 (82%)] Loss: -217856.281250\n",
      "Train Epoch: 137 [15546/17352 (90%)] Loss: -143324.375000\n",
      "Train Epoch: 137 [16175/17352 (93%)] Loss: -170484.187500\n",
      "Train Epoch: 137 [17033/17352 (98%)] Loss: -150278.437500\n",
      "    epoch          : 137\n",
      "    loss           : -201687.74966901742\n",
      "    val_loss       : -110357.98525155385\n",
      "Train Epoch: 138 [128/17352 (1%)] Loss: -200022.812500\n",
      "Train Epoch: 138 [1536/17352 (9%)] Loss: -209755.140625\n",
      "Train Epoch: 138 [2944/17352 (17%)] Loss: -238440.281250\n",
      "Train Epoch: 138 [4352/17352 (25%)] Loss: -189903.140625\n",
      "Train Epoch: 138 [5760/17352 (33%)] Loss: -219431.484375\n",
      "Train Epoch: 138 [7168/17352 (41%)] Loss: -222344.500000\n",
      "Train Epoch: 138 [8576/17352 (49%)] Loss: -224049.843750\n",
      "Train Epoch: 138 [9984/17352 (58%)] Loss: -206613.828125\n",
      "Train Epoch: 138 [11392/17352 (66%)] Loss: -229280.812500\n",
      "Train Epoch: 138 [12800/17352 (74%)] Loss: -217147.484375\n",
      "Train Epoch: 138 [14208/17352 (82%)] Loss: -234785.609375\n",
      "Train Epoch: 138 [15453/17352 (89%)] Loss: -5262.552734\n",
      "Train Epoch: 138 [16297/17352 (94%)] Loss: -211562.750000\n",
      "Train Epoch: 138 [16952/17352 (98%)] Loss: -125902.609375\n",
      "    epoch          : 138\n",
      "    loss           : -200364.89603869547\n",
      "    val_loss       : -109855.44166081747\n",
      "Train Epoch: 139 [128/17352 (1%)] Loss: -229189.546875\n",
      "Train Epoch: 139 [1536/17352 (9%)] Loss: -204159.437500\n",
      "Train Epoch: 139 [2944/17352 (17%)] Loss: -205351.406250\n",
      "Train Epoch: 139 [4352/17352 (25%)] Loss: -224049.656250\n",
      "Train Epoch: 139 [5760/17352 (33%)] Loss: -230818.968750\n",
      "Train Epoch: 139 [7168/17352 (41%)] Loss: -222857.531250\n",
      "Train Epoch: 139 [8576/17352 (49%)] Loss: -231942.734375\n",
      "Train Epoch: 139 [9984/17352 (58%)] Loss: -218416.250000\n",
      "Train Epoch: 139 [11392/17352 (66%)] Loss: -226375.343750\n",
      "Train Epoch: 139 [12800/17352 (74%)] Loss: -208821.531250\n",
      "Train Epoch: 139 [14208/17352 (82%)] Loss: -215183.093750\n",
      "Train Epoch: 139 [15487/17352 (89%)] Loss: -150364.468750\n",
      "Train Epoch: 139 [16372/17352 (94%)] Loss: -132425.265625\n",
      "Train Epoch: 139 [17188/17352 (99%)] Loss: -141331.343750\n",
      "    epoch          : 139\n",
      "    loss           : -201284.8392571571\n",
      "    val_loss       : -110073.17187258402\n",
      "Train Epoch: 140 [128/17352 (1%)] Loss: -231794.343750\n",
      "Train Epoch: 140 [1536/17352 (9%)] Loss: -220312.218750\n",
      "Train Epoch: 140 [2944/17352 (17%)] Loss: -224543.093750\n",
      "Train Epoch: 140 [4352/17352 (25%)] Loss: -213558.703125\n",
      "Train Epoch: 140 [5760/17352 (33%)] Loss: -207216.406250\n",
      "Train Epoch: 140 [7168/17352 (41%)] Loss: -225647.546875\n",
      "Train Epoch: 140 [8576/17352 (49%)] Loss: -197162.578125\n",
      "Train Epoch: 140 [9984/17352 (58%)] Loss: -193342.562500\n",
      "Train Epoch: 140 [11392/17352 (66%)] Loss: -206716.281250\n",
      "Train Epoch: 140 [12800/17352 (74%)] Loss: -241258.046875\n",
      "Train Epoch: 140 [14208/17352 (82%)] Loss: -205949.625000\n",
      "Train Epoch: 140 [15534/17352 (90%)] Loss: -126246.281250\n",
      "Train Epoch: 140 [16374/17352 (94%)] Loss: -137274.359375\n",
      "Train Epoch: 140 [17082/17352 (98%)] Loss: -170564.125000\n",
      "    epoch          : 140\n",
      "    loss           : -199845.8287476405\n",
      "    val_loss       : -109884.08697357177\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch140.pth ...\n",
      "Train Epoch: 141 [128/17352 (1%)] Loss: -206297.578125\n",
      "Train Epoch: 141 [1536/17352 (9%)] Loss: -209858.718750\n",
      "Train Epoch: 141 [2944/17352 (17%)] Loss: -196548.531250\n",
      "Train Epoch: 141 [4352/17352 (25%)] Loss: -237019.000000\n",
      "Train Epoch: 141 [5760/17352 (33%)] Loss: -218467.468750\n",
      "Train Epoch: 141 [7168/17352 (41%)] Loss: -223232.343750\n",
      "Train Epoch: 141 [8576/17352 (49%)] Loss: -215278.218750\n",
      "Train Epoch: 141 [9984/17352 (58%)] Loss: -225197.953125\n",
      "Train Epoch: 141 [11392/17352 (66%)] Loss: -202139.421875\n",
      "Train Epoch: 141 [12800/17352 (74%)] Loss: -223174.953125\n",
      "Train Epoch: 141 [14208/17352 (82%)] Loss: -221273.546875\n",
      "Train Epoch: 141 [15582/17352 (90%)] Loss: -219163.453125\n",
      "Train Epoch: 141 [16281/17352 (94%)] Loss: -145443.890625\n",
      "Train Epoch: 141 [17068/17352 (98%)] Loss: -8985.998047\n",
      "    epoch          : 141\n",
      "    loss           : -200672.445194526\n",
      "    val_loss       : -110115.80044085185\n",
      "Train Epoch: 142 [128/17352 (1%)] Loss: -230552.640625\n",
      "Train Epoch: 142 [1536/17352 (9%)] Loss: -218571.296875\n",
      "Train Epoch: 142 [2944/17352 (17%)] Loss: -201267.234375\n",
      "Train Epoch: 142 [4352/17352 (25%)] Loss: -230363.031250\n",
      "Train Epoch: 142 [5760/17352 (33%)] Loss: -220421.000000\n",
      "Train Epoch: 142 [7168/17352 (41%)] Loss: -218880.250000\n",
      "Train Epoch: 142 [8576/17352 (49%)] Loss: -241505.937500\n",
      "Train Epoch: 142 [9984/17352 (58%)] Loss: -235784.656250\n",
      "Train Epoch: 142 [11392/17352 (66%)] Loss: -223412.734375\n",
      "Train Epoch: 142 [12800/17352 (74%)] Loss: -232914.046875\n",
      "Train Epoch: 142 [14208/17352 (82%)] Loss: -219493.656250\n",
      "Train Epoch: 142 [15448/17352 (89%)] Loss: -61825.453125\n",
      "Train Epoch: 142 [16116/17352 (93%)] Loss: -162818.281250\n",
      "Train Epoch: 142 [16987/17352 (98%)] Loss: -81518.351562\n",
      "    epoch          : 142\n",
      "    loss           : -201943.85749397022\n",
      "    val_loss       : -109636.84804865519\n",
      "Train Epoch: 143 [128/17352 (1%)] Loss: -233483.890625\n",
      "Train Epoch: 143 [1536/17352 (9%)] Loss: -207157.468750\n",
      "Train Epoch: 143 [2944/17352 (17%)] Loss: -237654.250000\n",
      "Train Epoch: 143 [4352/17352 (25%)] Loss: -211796.281250\n",
      "Train Epoch: 143 [5760/17352 (33%)] Loss: -218591.703125\n",
      "Train Epoch: 143 [7168/17352 (41%)] Loss: -227525.906250\n",
      "Train Epoch: 143 [8576/17352 (49%)] Loss: -215122.453125\n",
      "Train Epoch: 143 [9984/17352 (58%)] Loss: -236881.218750\n",
      "Train Epoch: 143 [11392/17352 (66%)] Loss: -226849.531250\n",
      "Train Epoch: 143 [12800/17352 (74%)] Loss: -217137.187500\n",
      "Train Epoch: 143 [14208/17352 (82%)] Loss: -225161.718750\n",
      "Train Epoch: 143 [15557/17352 (90%)] Loss: -182700.375000\n",
      "Train Epoch: 143 [16256/17352 (94%)] Loss: -26547.519531\n",
      "Train Epoch: 143 [16959/17352 (98%)] Loss: -144359.968750\n",
      "    epoch          : 143\n",
      "    loss           : -201951.75388986472\n",
      "    val_loss       : -111102.83254909515\n",
      "Train Epoch: 144 [128/17352 (1%)] Loss: -228024.437500\n",
      "Train Epoch: 144 [1536/17352 (9%)] Loss: -200015.750000\n",
      "Train Epoch: 144 [2944/17352 (17%)] Loss: -207236.593750\n",
      "Train Epoch: 144 [4352/17352 (25%)] Loss: -234701.296875\n",
      "Train Epoch: 144 [5760/17352 (33%)] Loss: -227886.562500\n",
      "Train Epoch: 144 [7168/17352 (41%)] Loss: -256920.500000\n",
      "Train Epoch: 144 [8576/17352 (49%)] Loss: -213567.500000\n",
      "Train Epoch: 144 [9984/17352 (58%)] Loss: -217156.296875\n",
      "Train Epoch: 144 [11392/17352 (66%)] Loss: -214876.546875\n",
      "Train Epoch: 144 [12800/17352 (74%)] Loss: -228876.468750\n",
      "Train Epoch: 144 [14208/17352 (82%)] Loss: -206168.765625\n",
      "Train Epoch: 144 [15541/17352 (90%)] Loss: -169395.796875\n",
      "Train Epoch: 144 [16183/17352 (93%)] Loss: -24225.310547\n",
      "Train Epoch: 144 [16994/17352 (98%)] Loss: -137297.421875\n",
      "    epoch          : 144\n",
      "    loss           : -201381.44642997588\n",
      "    val_loss       : -110084.37921969096\n",
      "Train Epoch: 145 [128/17352 (1%)] Loss: -229668.515625\n",
      "Train Epoch: 145 [1536/17352 (9%)] Loss: -235298.281250\n",
      "Train Epoch: 145 [2944/17352 (17%)] Loss: -219969.250000\n",
      "Train Epoch: 145 [4352/17352 (25%)] Loss: -238025.468750\n",
      "Train Epoch: 145 [5760/17352 (33%)] Loss: -217680.234375\n",
      "Train Epoch: 145 [7168/17352 (41%)] Loss: -219799.781250\n",
      "Train Epoch: 145 [8576/17352 (49%)] Loss: -215923.093750\n",
      "Train Epoch: 145 [9984/17352 (58%)] Loss: -238348.937500\n",
      "Train Epoch: 145 [11392/17352 (66%)] Loss: -224102.343750\n",
      "Train Epoch: 145 [12800/17352 (74%)] Loss: -223060.109375\n",
      "Train Epoch: 145 [14208/17352 (82%)] Loss: -235203.953125\n",
      "Train Epoch: 145 [15492/17352 (89%)] Loss: -124696.125000\n",
      "Train Epoch: 145 [16366/17352 (94%)] Loss: -141825.671875\n",
      "Train Epoch: 145 [16945/17352 (98%)] Loss: -88653.132812\n",
      "    epoch          : 145\n",
      "    loss           : -200693.21279231334\n",
      "    val_loss       : -109038.31976782481\n",
      "Train Epoch: 146 [128/17352 (1%)] Loss: -231464.890625\n",
      "Train Epoch: 146 [1536/17352 (9%)] Loss: -226555.687500\n",
      "Train Epoch: 146 [2944/17352 (17%)] Loss: -203776.875000\n",
      "Train Epoch: 146 [4352/17352 (25%)] Loss: -232789.640625\n",
      "Train Epoch: 146 [5760/17352 (33%)] Loss: -241217.328125\n",
      "Train Epoch: 146 [7168/17352 (41%)] Loss: -208654.906250\n",
      "Train Epoch: 146 [8576/17352 (49%)] Loss: -208035.500000\n",
      "Train Epoch: 146 [9984/17352 (58%)] Loss: -208418.312500\n",
      "Train Epoch: 146 [11392/17352 (66%)] Loss: -213286.750000\n",
      "Train Epoch: 146 [12800/17352 (74%)] Loss: -217360.484375\n",
      "Train Epoch: 146 [14208/17352 (82%)] Loss: -244200.500000\n",
      "Train Epoch: 146 [15527/17352 (89%)] Loss: -139784.390625\n",
      "Train Epoch: 146 [16321/17352 (94%)] Loss: -183183.078125\n",
      "Train Epoch: 146 [17147/17352 (99%)] Loss: -149855.109375\n",
      "    epoch          : 146\n",
      "    loss           : -201240.84150849414\n",
      "    val_loss       : -110394.4140900294\n",
      "Train Epoch: 147 [128/17352 (1%)] Loss: -226386.781250\n",
      "Train Epoch: 147 [1536/17352 (9%)] Loss: -215057.531250\n",
      "Train Epoch: 147 [2944/17352 (17%)] Loss: -228140.031250\n",
      "Train Epoch: 147 [4352/17352 (25%)] Loss: -229386.000000\n",
      "Train Epoch: 147 [5760/17352 (33%)] Loss: -225111.828125\n",
      "Train Epoch: 147 [7168/17352 (41%)] Loss: -215081.750000\n",
      "Train Epoch: 147 [8576/17352 (49%)] Loss: -219758.906250\n",
      "Train Epoch: 147 [9984/17352 (58%)] Loss: -215159.984375\n",
      "Train Epoch: 147 [11392/17352 (66%)] Loss: -206301.203125\n",
      "Train Epoch: 147 [12800/17352 (74%)] Loss: -235691.312500\n",
      "Train Epoch: 147 [14208/17352 (82%)] Loss: -200181.718750\n",
      "Train Epoch: 147 [15525/17352 (89%)] Loss: -152197.156250\n",
      "Train Epoch: 147 [16352/17352 (94%)] Loss: -63956.992188\n",
      "Train Epoch: 147 [17003/17352 (98%)] Loss: -130572.046875\n",
      "    epoch          : 147\n",
      "    loss           : -202015.34982566064\n",
      "    val_loss       : -110647.31954698563\n",
      "Train Epoch: 148 [128/17352 (1%)] Loss: -226196.437500\n",
      "Train Epoch: 148 [1536/17352 (9%)] Loss: -212663.875000\n",
      "Train Epoch: 148 [2944/17352 (17%)] Loss: -234237.015625\n",
      "Train Epoch: 148 [4352/17352 (25%)] Loss: -227882.921875\n",
      "Train Epoch: 148 [5760/17352 (33%)] Loss: -212877.781250\n",
      "Train Epoch: 148 [7168/17352 (41%)] Loss: -223993.828125\n",
      "Train Epoch: 148 [8576/17352 (49%)] Loss: -209872.062500\n",
      "Train Epoch: 148 [9984/17352 (58%)] Loss: -225744.250000\n",
      "Train Epoch: 148 [11392/17352 (66%)] Loss: -218638.812500\n",
      "Train Epoch: 148 [12800/17352 (74%)] Loss: -211768.218750\n",
      "Train Epoch: 148 [14208/17352 (82%)] Loss: -218463.125000\n",
      "Train Epoch: 148 [15455/17352 (89%)] Loss: -92153.593750\n",
      "Train Epoch: 148 [16223/17352 (93%)] Loss: -62776.203125\n",
      "Train Epoch: 148 [16988/17352 (98%)] Loss: -86085.242188\n",
      "    epoch          : 148\n",
      "    loss           : -201609.9672163381\n",
      "    val_loss       : -110543.17276407877\n",
      "Train Epoch: 149 [128/17352 (1%)] Loss: -229745.906250\n",
      "Train Epoch: 149 [1536/17352 (9%)] Loss: -222915.484375\n",
      "Train Epoch: 149 [2944/17352 (17%)] Loss: -204124.437500\n",
      "Train Epoch: 149 [4352/17352 (25%)] Loss: -230546.781250\n",
      "Train Epoch: 149 [5760/17352 (33%)] Loss: -199009.328125\n",
      "Train Epoch: 149 [7168/17352 (41%)] Loss: -198736.062500\n",
      "Train Epoch: 149 [8576/17352 (49%)] Loss: -206401.781250\n",
      "Train Epoch: 149 [9984/17352 (58%)] Loss: -219120.875000\n",
      "Train Epoch: 149 [11392/17352 (66%)] Loss: -210922.656250\n",
      "Train Epoch: 149 [12800/17352 (74%)] Loss: -211515.281250\n",
      "Train Epoch: 149 [14208/17352 (82%)] Loss: -232932.593750\n",
      "Train Epoch: 149 [15561/17352 (90%)] Loss: -174635.468750\n",
      "Train Epoch: 149 [16334/17352 (94%)] Loss: -87307.945312\n",
      "Train Epoch: 149 [16998/17352 (98%)] Loss: -127365.312500\n",
      "    epoch          : 149\n",
      "    loss           : -201860.43532403524\n",
      "    val_loss       : -109887.79745210012\n",
      "Train Epoch: 150 [128/17352 (1%)] Loss: -207804.656250\n",
      "Train Epoch: 150 [1536/17352 (9%)] Loss: -208382.375000\n",
      "Train Epoch: 150 [2944/17352 (17%)] Loss: -202933.062500\n",
      "Train Epoch: 150 [4352/17352 (25%)] Loss: -200824.593750\n",
      "Train Epoch: 150 [5760/17352 (33%)] Loss: -209078.593750\n",
      "Train Epoch: 150 [7168/17352 (41%)] Loss: -244718.593750\n",
      "Train Epoch: 150 [8576/17352 (49%)] Loss: -246493.562500\n",
      "Train Epoch: 150 [9984/17352 (58%)] Loss: -227660.000000\n",
      "Train Epoch: 150 [11392/17352 (66%)] Loss: -208891.109375\n",
      "Train Epoch: 150 [12800/17352 (74%)] Loss: -219905.765625\n",
      "Train Epoch: 150 [14208/17352 (82%)] Loss: -238182.656250\n",
      "Train Epoch: 150 [15529/17352 (89%)] Loss: -156128.531250\n",
      "Train Epoch: 150 [16265/17352 (94%)] Loss: -8724.201172\n",
      "Train Epoch: 150 [16889/17352 (97%)] Loss: -148802.625000\n",
      "    epoch          : 150\n",
      "    loss           : -201691.44294318897\n",
      "    val_loss       : -110447.85274893443\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [128/17352 (1%)] Loss: -218708.562500\n",
      "Train Epoch: 151 [1536/17352 (9%)] Loss: -230520.765625\n",
      "Train Epoch: 151 [2944/17352 (17%)] Loss: -254899.890625\n",
      "Train Epoch: 151 [4352/17352 (25%)] Loss: -213742.843750\n",
      "Train Epoch: 151 [5760/17352 (33%)] Loss: -199765.640625\n",
      "Train Epoch: 151 [7168/17352 (41%)] Loss: -229931.156250\n",
      "Train Epoch: 151 [8576/17352 (49%)] Loss: -209345.671875\n",
      "Train Epoch: 151 [9984/17352 (58%)] Loss: -199258.906250\n",
      "Train Epoch: 151 [11392/17352 (66%)] Loss: -221615.171875\n",
      "Train Epoch: 151 [12800/17352 (74%)] Loss: -219168.125000\n",
      "Train Epoch: 151 [14208/17352 (82%)] Loss: -245800.500000\n",
      "Train Epoch: 151 [15561/17352 (90%)] Loss: -155999.703125\n",
      "Train Epoch: 151 [16278/17352 (94%)] Loss: -23598.570312\n",
      "Train Epoch: 151 [16884/17352 (97%)] Loss: -26557.656250\n",
      "    epoch          : 151\n",
      "    loss           : -201849.78657849203\n",
      "    val_loss       : -110112.80099913279\n",
      "Train Epoch: 152 [128/17352 (1%)] Loss: -223938.500000\n",
      "Train Epoch: 152 [1536/17352 (9%)] Loss: -228174.406250\n",
      "Train Epoch: 152 [2944/17352 (17%)] Loss: -257506.312500\n",
      "Train Epoch: 152 [4352/17352 (25%)] Loss: -224571.000000\n",
      "Train Epoch: 152 [5760/17352 (33%)] Loss: -197913.890625\n",
      "Train Epoch: 152 [7168/17352 (41%)] Loss: -216193.203125\n",
      "Train Epoch: 152 [8576/17352 (49%)] Loss: -222179.406250\n",
      "Train Epoch: 152 [9984/17352 (58%)] Loss: -234008.046875\n",
      "Train Epoch: 152 [11392/17352 (66%)] Loss: -196712.093750\n",
      "Train Epoch: 152 [12800/17352 (74%)] Loss: -232523.109375\n",
      "Train Epoch: 152 [14208/17352 (82%)] Loss: -240615.968750\n",
      "Train Epoch: 152 [15487/17352 (89%)] Loss: -79336.257812\n",
      "Train Epoch: 152 [16265/17352 (94%)] Loss: -121869.343750\n",
      "Train Epoch: 152 [16999/17352 (98%)] Loss: -189359.984375\n",
      "    epoch          : 152\n",
      "    loss           : -201986.36412017618\n",
      "    val_loss       : -110333.31282927195\n",
      "Train Epoch: 153 [128/17352 (1%)] Loss: -229999.968750\n",
      "Train Epoch: 153 [1536/17352 (9%)] Loss: -220124.109375\n",
      "Train Epoch: 153 [2944/17352 (17%)] Loss: -239208.890625\n",
      "Train Epoch: 153 [4352/17352 (25%)] Loss: -225802.046875\n",
      "Train Epoch: 153 [5760/17352 (33%)] Loss: -221157.687500\n",
      "Train Epoch: 153 [7168/17352 (41%)] Loss: -254958.968750\n",
      "Train Epoch: 153 [8576/17352 (49%)] Loss: -215917.296875\n",
      "Train Epoch: 153 [9984/17352 (58%)] Loss: -201370.968750\n",
      "Train Epoch: 153 [11392/17352 (66%)] Loss: -212072.046875\n",
      "Train Epoch: 153 [12800/17352 (74%)] Loss: -217583.468750\n",
      "Train Epoch: 153 [14208/17352 (82%)] Loss: -244969.421875\n",
      "Train Epoch: 153 [15380/17352 (89%)] Loss: -28307.226562\n",
      "Train Epoch: 153 [16142/17352 (93%)] Loss: -137166.906250\n",
      "Train Epoch: 153 [17023/17352 (98%)] Loss: -169603.421875\n",
      "    epoch          : 153\n",
      "    loss           : -201028.89711356963\n",
      "    val_loss       : -110413.96895809173\n",
      "Train Epoch: 154 [128/17352 (1%)] Loss: -228739.781250\n",
      "Train Epoch: 154 [1536/17352 (9%)] Loss: -221747.781250\n",
      "Train Epoch: 154 [2944/17352 (17%)] Loss: -256194.843750\n",
      "Train Epoch: 154 [4352/17352 (25%)] Loss: -222443.296875\n",
      "Train Epoch: 154 [5760/17352 (33%)] Loss: -216580.687500\n",
      "Train Epoch: 154 [7168/17352 (41%)] Loss: -222222.156250\n",
      "Train Epoch: 154 [8576/17352 (49%)] Loss: -217123.156250\n",
      "Train Epoch: 154 [9984/17352 (58%)] Loss: -236749.515625\n",
      "Train Epoch: 154 [11392/17352 (66%)] Loss: -207530.140625\n",
      "Train Epoch: 154 [12800/17352 (74%)] Loss: -236623.625000\n",
      "Train Epoch: 154 [14208/17352 (82%)] Loss: -227979.812500\n",
      "Train Epoch: 154 [15471/17352 (89%)] Loss: -63073.445312\n",
      "Train Epoch: 154 [16086/17352 (93%)] Loss: -77357.007812\n",
      "Train Epoch: 154 [16927/17352 (98%)] Loss: -162360.687500\n",
      "    epoch          : 154\n",
      "    loss           : -201571.26943949246\n",
      "    val_loss       : -110974.77683223088\n",
      "Train Epoch: 155 [128/17352 (1%)] Loss: -194521.109375\n",
      "Train Epoch: 155 [1536/17352 (9%)] Loss: -234458.687500\n",
      "Train Epoch: 155 [2944/17352 (17%)] Loss: -208392.890625\n",
      "Train Epoch: 155 [4352/17352 (25%)] Loss: -213527.828125\n",
      "Train Epoch: 155 [5760/17352 (33%)] Loss: -202398.593750\n",
      "Train Epoch: 155 [7168/17352 (41%)] Loss: -226045.312500\n",
      "Train Epoch: 155 [8576/17352 (49%)] Loss: -219444.703125\n",
      "Train Epoch: 155 [9984/17352 (58%)] Loss: -207424.609375\n",
      "Train Epoch: 155 [11392/17352 (66%)] Loss: -206270.765625\n",
      "Train Epoch: 155 [12800/17352 (74%)] Loss: -243805.546875\n",
      "Train Epoch: 155 [14208/17352 (82%)] Loss: -206957.843750\n",
      "Train Epoch: 155 [15506/17352 (89%)] Loss: -185549.515625\n",
      "Train Epoch: 155 [16238/17352 (94%)] Loss: -9098.458984\n",
      "Train Epoch: 155 [16865/17352 (97%)] Loss: -88208.132812\n",
      "    epoch          : 155\n",
      "    loss           : -202255.5141044463\n",
      "    val_loss       : -110189.75513830184\n",
      "Train Epoch: 156 [128/17352 (1%)] Loss: -215381.609375\n",
      "Train Epoch: 156 [1536/17352 (9%)] Loss: -227729.593750\n",
      "Train Epoch: 156 [2944/17352 (17%)] Loss: -215941.859375\n",
      "Train Epoch: 156 [4352/17352 (25%)] Loss: -237003.312500\n",
      "Train Epoch: 156 [5760/17352 (33%)] Loss: -231600.671875\n",
      "Train Epoch: 156 [7168/17352 (41%)] Loss: -234137.750000\n",
      "Train Epoch: 156 [8576/17352 (49%)] Loss: -204474.765625\n",
      "Train Epoch: 156 [9984/17352 (58%)] Loss: -214571.828125\n",
      "Train Epoch: 156 [11392/17352 (66%)] Loss: -202939.250000\n",
      "Train Epoch: 156 [12800/17352 (74%)] Loss: -214113.031250\n",
      "Train Epoch: 156 [14208/17352 (82%)] Loss: -233342.531250\n",
      "Train Epoch: 156 [15457/17352 (89%)] Loss: -154595.500000\n",
      "Train Epoch: 156 [16242/17352 (94%)] Loss: -64778.921875\n",
      "Train Epoch: 156 [16979/17352 (98%)] Loss: -8817.948242\n",
      "    epoch          : 156\n",
      "    loss           : -202301.02174326236\n",
      "    val_loss       : -110849.1884581248\n",
      "Train Epoch: 157 [128/17352 (1%)] Loss: -204111.953125\n",
      "Train Epoch: 157 [1536/17352 (9%)] Loss: -221156.718750\n",
      "Train Epoch: 157 [2944/17352 (17%)] Loss: -224324.718750\n",
      "Train Epoch: 157 [4352/17352 (25%)] Loss: -201752.781250\n",
      "Train Epoch: 157 [5760/17352 (33%)] Loss: -197782.468750\n",
      "Train Epoch: 157 [7168/17352 (41%)] Loss: -234278.281250\n",
      "Train Epoch: 157 [8576/17352 (49%)] Loss: -222222.000000\n",
      "Train Epoch: 157 [9984/17352 (58%)] Loss: -218353.062500\n",
      "Train Epoch: 157 [11392/17352 (66%)] Loss: -230687.406250\n",
      "Train Epoch: 157 [12800/17352 (74%)] Loss: -210738.906250\n",
      "Train Epoch: 157 [14208/17352 (82%)] Loss: -221775.781250\n",
      "Train Epoch: 157 [15450/17352 (89%)] Loss: -9137.753906\n",
      "Train Epoch: 157 [16065/17352 (93%)] Loss: -64506.644531\n",
      "Train Epoch: 157 [16997/17352 (98%)] Loss: -187920.921875\n",
      "    epoch          : 157\n",
      "    loss           : -201002.14711356963\n",
      "    val_loss       : -111253.00971132914\n",
      "Train Epoch: 158 [128/17352 (1%)] Loss: -192318.156250\n",
      "Train Epoch: 158 [1536/17352 (9%)] Loss: -220453.187500\n",
      "Train Epoch: 158 [2944/17352 (17%)] Loss: -218833.437500\n",
      "Train Epoch: 158 [4352/17352 (25%)] Loss: -209619.812500\n",
      "Train Epoch: 158 [5760/17352 (33%)] Loss: -217257.500000\n",
      "Train Epoch: 158 [7168/17352 (41%)] Loss: -228785.515625\n",
      "Train Epoch: 158 [8576/17352 (49%)] Loss: -221129.546875\n",
      "Train Epoch: 158 [9984/17352 (58%)] Loss: -216625.906250\n",
      "Train Epoch: 158 [11392/17352 (66%)] Loss: -207565.906250\n",
      "Train Epoch: 158 [12800/17352 (74%)] Loss: -227841.000000\n",
      "Train Epoch: 158 [14208/17352 (82%)] Loss: -228403.750000\n",
      "Train Epoch: 158 [15506/17352 (89%)] Loss: -157743.000000\n",
      "Train Epoch: 158 [16110/17352 (93%)] Loss: -196960.171875\n",
      "Train Epoch: 158 [16874/17352 (97%)] Loss: -85264.351562\n",
      "    epoch          : 158\n",
      "    loss           : -201909.001599203\n",
      "    val_loss       : -110642.3695230484\n",
      "Train Epoch: 159 [128/17352 (1%)] Loss: -219093.453125\n",
      "Train Epoch: 159 [1536/17352 (9%)] Loss: -203184.609375\n",
      "Train Epoch: 159 [2944/17352 (17%)] Loss: -233812.312500\n",
      "Train Epoch: 159 [4352/17352 (25%)] Loss: -236027.312500\n",
      "Train Epoch: 159 [5760/17352 (33%)] Loss: -218679.140625\n",
      "Train Epoch: 159 [7168/17352 (41%)] Loss: -229086.156250\n",
      "Train Epoch: 159 [8576/17352 (49%)] Loss: -214662.609375\n",
      "Train Epoch: 159 [9984/17352 (58%)] Loss: -212648.343750\n",
      "Train Epoch: 159 [11392/17352 (66%)] Loss: -227593.062500\n",
      "Train Epoch: 159 [12800/17352 (74%)] Loss: -206457.312500\n",
      "Train Epoch: 159 [14208/17352 (82%)] Loss: -219075.812500\n",
      "Train Epoch: 159 [15528/17352 (89%)] Loss: -133134.203125\n",
      "Train Epoch: 159 [16239/17352 (94%)] Loss: -9530.095703\n",
      "Train Epoch: 159 [16893/17352 (97%)] Loss: -27460.125000\n",
      "    epoch          : 159\n",
      "    loss           : -201841.4961887846\n",
      "    val_loss       : -110819.80941904386\n",
      "Train Epoch: 160 [128/17352 (1%)] Loss: -181482.265625\n",
      "Train Epoch: 160 [1536/17352 (9%)] Loss: -231171.281250\n",
      "Train Epoch: 160 [2944/17352 (17%)] Loss: -239352.578125\n",
      "Train Epoch: 160 [4352/17352 (25%)] Loss: -226901.312500\n",
      "Train Epoch: 160 [5760/17352 (33%)] Loss: -208246.000000\n",
      "Train Epoch: 160 [7168/17352 (41%)] Loss: -232569.593750\n",
      "Train Epoch: 160 [8576/17352 (49%)] Loss: -220560.218750\n",
      "Train Epoch: 160 [9984/17352 (58%)] Loss: -218491.843750\n",
      "Train Epoch: 160 [11392/17352 (66%)] Loss: -231740.656250\n",
      "Train Epoch: 160 [12800/17352 (74%)] Loss: -205989.468750\n",
      "Train Epoch: 160 [14208/17352 (82%)] Loss: -236547.859375\n",
      "Train Epoch: 160 [15479/17352 (89%)] Loss: -144407.093750\n",
      "Train Epoch: 160 [16217/17352 (93%)] Loss: -161003.578125\n",
      "Train Epoch: 160 [16891/17352 (97%)] Loss: -82519.937500\n",
      "    epoch          : 160\n",
      "    loss           : -201437.04410916526\n",
      "    val_loss       : -110998.05719653766\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch160.pth ...\n",
      "Train Epoch: 161 [128/17352 (1%)] Loss: -231981.328125\n",
      "Train Epoch: 161 [1536/17352 (9%)] Loss: -227057.578125\n",
      "Train Epoch: 161 [2944/17352 (17%)] Loss: -216789.531250\n",
      "Train Epoch: 161 [4352/17352 (25%)] Loss: -232263.234375\n",
      "Train Epoch: 161 [5760/17352 (33%)] Loss: -220291.000000\n",
      "Train Epoch: 161 [7168/17352 (41%)] Loss: -226292.687500\n",
      "Train Epoch: 161 [8576/17352 (49%)] Loss: -232768.671875\n",
      "Train Epoch: 161 [9984/17352 (58%)] Loss: -206037.062500\n",
      "Train Epoch: 161 [11392/17352 (66%)] Loss: -190923.625000\n",
      "Train Epoch: 161 [12800/17352 (74%)] Loss: -230039.796875\n",
      "Train Epoch: 161 [14208/17352 (82%)] Loss: -216880.796875\n",
      "Train Epoch: 161 [15464/17352 (89%)] Loss: -81536.671875\n",
      "Train Epoch: 161 [16234/17352 (94%)] Loss: -162875.218750\n",
      "Train Epoch: 161 [17041/17352 (98%)] Loss: -149897.421875\n",
      "    epoch          : 161\n",
      "    loss           : -201713.8862206376\n",
      "    val_loss       : -110870.39916617076\n",
      "Train Epoch: 162 [128/17352 (1%)] Loss: -216910.359375\n",
      "Train Epoch: 162 [1536/17352 (9%)] Loss: -225592.171875\n",
      "Train Epoch: 162 [2944/17352 (17%)] Loss: -242362.328125\n",
      "Train Epoch: 162 [4352/17352 (25%)] Loss: -236035.312500\n",
      "Train Epoch: 162 [5760/17352 (33%)] Loss: -239853.125000\n",
      "Train Epoch: 162 [7168/17352 (41%)] Loss: -251354.218750\n",
      "Train Epoch: 162 [8576/17352 (49%)] Loss: -225256.093750\n",
      "Train Epoch: 162 [9984/17352 (58%)] Loss: -229064.968750\n",
      "Train Epoch: 162 [11392/17352 (66%)] Loss: -220677.593750\n",
      "Train Epoch: 162 [12800/17352 (74%)] Loss: -222173.703125\n",
      "Train Epoch: 162 [14208/17352 (82%)] Loss: -225865.468750\n",
      "Train Epoch: 162 [15481/17352 (89%)] Loss: -131600.843750\n",
      "Train Epoch: 162 [16069/17352 (93%)] Loss: -65781.585938\n",
      "Train Epoch: 162 [16971/17352 (98%)] Loss: -216403.328125\n",
      "    epoch          : 162\n",
      "    loss           : -201702.15396589242\n",
      "    val_loss       : -110764.20086161296\n",
      "Train Epoch: 163 [128/17352 (1%)] Loss: -192327.093750\n",
      "Train Epoch: 163 [1536/17352 (9%)] Loss: -228700.796875\n",
      "Train Epoch: 163 [2944/17352 (17%)] Loss: -232421.468750\n",
      "Train Epoch: 163 [4352/17352 (25%)] Loss: -234443.718750\n",
      "Train Epoch: 163 [5760/17352 (33%)] Loss: -208041.968750\n",
      "Train Epoch: 163 [7168/17352 (41%)] Loss: -204410.187500\n",
      "Train Epoch: 163 [8576/17352 (49%)] Loss: -239737.250000\n",
      "Train Epoch: 163 [9984/17352 (58%)] Loss: -230038.593750\n",
      "Train Epoch: 163 [11392/17352 (66%)] Loss: -223839.843750\n",
      "Train Epoch: 163 [12800/17352 (74%)] Loss: -215602.078125\n",
      "Train Epoch: 163 [14208/17352 (82%)] Loss: -240129.250000\n",
      "Train Epoch: 163 [15560/17352 (90%)] Loss: -168985.687500\n",
      "Train Epoch: 163 [16164/17352 (93%)] Loss: -89177.546875\n",
      "Train Epoch: 163 [16972/17352 (98%)] Loss: -155690.625000\n",
      "    epoch          : 163\n",
      "    loss           : -202870.61011561452\n",
      "    val_loss       : -111072.73951301575\n",
      "Train Epoch: 164 [128/17352 (1%)] Loss: -223722.125000\n",
      "Train Epoch: 164 [1536/17352 (9%)] Loss: -228437.390625\n",
      "Train Epoch: 164 [2944/17352 (17%)] Loss: -203927.093750\n",
      "Train Epoch: 164 [4352/17352 (25%)] Loss: -222316.171875\n",
      "Train Epoch: 164 [5760/17352 (33%)] Loss: -224423.562500\n",
      "Train Epoch: 164 [7168/17352 (41%)] Loss: -239278.109375\n",
      "Train Epoch: 164 [8576/17352 (49%)] Loss: -210306.078125\n",
      "Train Epoch: 164 [9984/17352 (58%)] Loss: -227365.656250\n",
      "Train Epoch: 164 [11392/17352 (66%)] Loss: -194459.453125\n",
      "Train Epoch: 164 [12800/17352 (74%)] Loss: -232815.718750\n",
      "Train Epoch: 164 [14208/17352 (82%)] Loss: -207561.046875\n",
      "Train Epoch: 164 [15533/17352 (90%)] Loss: -137982.218750\n",
      "Train Epoch: 164 [16103/17352 (93%)] Loss: -78517.671875\n",
      "Train Epoch: 164 [16973/17352 (98%)] Loss: -22269.744141\n",
      "    epoch          : 164\n",
      "    loss           : -201145.73568254508\n",
      "    val_loss       : -110339.78634635608\n",
      "Train Epoch: 165 [128/17352 (1%)] Loss: -230208.937500\n",
      "Train Epoch: 165 [1536/17352 (9%)] Loss: -231005.968750\n",
      "Train Epoch: 165 [2944/17352 (17%)] Loss: -207907.984375\n",
      "Train Epoch: 165 [4352/17352 (25%)] Loss: -238283.156250\n",
      "Train Epoch: 165 [5760/17352 (33%)] Loss: -203636.218750\n",
      "Train Epoch: 165 [7168/17352 (41%)] Loss: -207324.515625\n",
      "Train Epoch: 165 [8576/17352 (49%)] Loss: -205797.546875\n",
      "Train Epoch: 165 [9984/17352 (58%)] Loss: -219562.562500\n",
      "Train Epoch: 165 [11392/17352 (66%)] Loss: -229076.203125\n",
      "Train Epoch: 165 [12800/17352 (74%)] Loss: -218220.218750\n",
      "Train Epoch: 165 [14208/17352 (82%)] Loss: -225894.000000\n",
      "Train Epoch: 165 [15486/17352 (89%)] Loss: -169502.843750\n",
      "Train Epoch: 165 [16449/17352 (95%)] Loss: -181918.437500\n",
      "Train Epoch: 165 [16995/17352 (98%)] Loss: -154706.984375\n",
      "    epoch          : 165\n",
      "    loss           : -202316.06928022756\n",
      "    val_loss       : -110810.50344931285\n",
      "Train Epoch: 166 [128/17352 (1%)] Loss: -220364.578125\n",
      "Train Epoch: 166 [1536/17352 (9%)] Loss: -222075.656250\n",
      "Train Epoch: 166 [2944/17352 (17%)] Loss: -218655.406250\n",
      "Train Epoch: 166 [4352/17352 (25%)] Loss: -200687.187500\n",
      "Train Epoch: 166 [5760/17352 (33%)] Loss: -209895.687500\n",
      "Train Epoch: 166 [7168/17352 (41%)] Loss: -246133.203125\n",
      "Train Epoch: 166 [8576/17352 (49%)] Loss: -218691.703125\n",
      "Train Epoch: 166 [9984/17352 (58%)] Loss: -194512.953125\n",
      "Train Epoch: 166 [11392/17352 (66%)] Loss: -214447.062500\n",
      "Train Epoch: 166 [12800/17352 (74%)] Loss: -227523.640625\n",
      "Train Epoch: 166 [14208/17352 (82%)] Loss: -232993.031250\n",
      "Train Epoch: 166 [15530/17352 (89%)] Loss: -128974.421875\n",
      "Train Epoch: 166 [16119/17352 (93%)] Loss: -5776.476562\n",
      "Train Epoch: 166 [17011/17352 (98%)] Loss: -192274.421875\n",
      "    epoch          : 166\n",
      "    loss           : -201951.9425663276\n",
      "    val_loss       : -110333.9054333369\n",
      "Train Epoch: 167 [128/17352 (1%)] Loss: -231837.500000\n",
      "Train Epoch: 167 [1536/17352 (9%)] Loss: -218112.296875\n",
      "Train Epoch: 167 [2944/17352 (17%)] Loss: -217143.250000\n",
      "Train Epoch: 167 [4352/17352 (25%)] Loss: -225094.578125\n",
      "Train Epoch: 167 [5760/17352 (33%)] Loss: -212931.093750\n",
      "Train Epoch: 167 [7168/17352 (41%)] Loss: -217784.250000\n",
      "Train Epoch: 167 [8576/17352 (49%)] Loss: -227073.343750\n",
      "Train Epoch: 167 [9984/17352 (58%)] Loss: -232986.531250\n",
      "Train Epoch: 167 [11392/17352 (66%)] Loss: -220748.828125\n",
      "Train Epoch: 167 [12800/17352 (74%)] Loss: -225507.125000\n",
      "Train Epoch: 167 [14208/17352 (82%)] Loss: -231197.781250\n",
      "Train Epoch: 167 [15571/17352 (90%)] Loss: -222502.218750\n",
      "Train Epoch: 167 [16167/17352 (93%)] Loss: -5635.099609\n",
      "Train Epoch: 167 [17086/17352 (98%)] Loss: -169139.187500\n",
      "    epoch          : 167\n",
      "    loss           : -202661.2837012374\n",
      "    val_loss       : -110530.93501313527\n",
      "Train Epoch: 168 [128/17352 (1%)] Loss: -192847.406250\n",
      "Train Epoch: 168 [1536/17352 (9%)] Loss: -216324.859375\n",
      "Train Epoch: 168 [2944/17352 (17%)] Loss: -204446.468750\n",
      "Train Epoch: 168 [4352/17352 (25%)] Loss: -229661.093750\n",
      "Train Epoch: 168 [5760/17352 (33%)] Loss: -224509.593750\n",
      "Train Epoch: 168 [7168/17352 (41%)] Loss: -228722.937500\n",
      "Train Epoch: 168 [8576/17352 (49%)] Loss: -207751.734375\n",
      "Train Epoch: 168 [9984/17352 (58%)] Loss: -233192.921875\n",
      "Train Epoch: 168 [11392/17352 (66%)] Loss: -226555.531250\n",
      "Train Epoch: 168 [12800/17352 (74%)] Loss: -230429.687500\n",
      "Train Epoch: 168 [14208/17352 (82%)] Loss: -225117.515625\n",
      "Train Epoch: 168 [15521/17352 (89%)] Loss: -66100.429688\n",
      "Train Epoch: 168 [16323/17352 (94%)] Loss: -136566.500000\n",
      "Train Epoch: 168 [17064/17352 (98%)] Loss: -161781.812500\n",
      "    epoch          : 168\n",
      "    loss           : -202836.18356097944\n",
      "    val_loss       : -111794.21659638087\n",
      "Train Epoch: 169 [128/17352 (1%)] Loss: -230904.250000\n",
      "Train Epoch: 169 [1536/17352 (9%)] Loss: -234457.468750\n",
      "Train Epoch: 169 [2944/17352 (17%)] Loss: -222900.312500\n",
      "Train Epoch: 169 [4352/17352 (25%)] Loss: -224484.390625\n",
      "Train Epoch: 169 [5760/17352 (33%)] Loss: -219155.718750\n",
      "Train Epoch: 169 [7168/17352 (41%)] Loss: -224994.234375\n",
      "Train Epoch: 169 [8576/17352 (49%)] Loss: -215401.906250\n",
      "Train Epoch: 169 [9984/17352 (58%)] Loss: -197509.453125\n",
      "Train Epoch: 169 [11392/17352 (66%)] Loss: -214947.218750\n",
      "Train Epoch: 169 [12800/17352 (74%)] Loss: -225801.468750\n",
      "Train Epoch: 169 [14208/17352 (82%)] Loss: -221043.812500\n",
      "Train Epoch: 169 [15464/17352 (89%)] Loss: -177499.078125\n",
      "Train Epoch: 169 [16219/17352 (93%)] Loss: -91543.593750\n",
      "Train Epoch: 169 [16969/17352 (98%)] Loss: -77761.406250\n",
      "    epoch          : 169\n",
      "    loss           : -201962.9356419096\n",
      "    val_loss       : -111888.43567841848\n",
      "Train Epoch: 170 [128/17352 (1%)] Loss: -202793.484375\n",
      "Train Epoch: 170 [1536/17352 (9%)] Loss: -216176.109375\n",
      "Train Epoch: 170 [2944/17352 (17%)] Loss: -225060.578125\n",
      "Train Epoch: 170 [4352/17352 (25%)] Loss: -216303.640625\n",
      "Train Epoch: 170 [5760/17352 (33%)] Loss: -207293.078125\n",
      "Train Epoch: 170 [7168/17352 (41%)] Loss: -213280.031250\n",
      "Train Epoch: 170 [8576/17352 (49%)] Loss: -232607.796875\n",
      "Train Epoch: 170 [9984/17352 (58%)] Loss: -222661.390625\n",
      "Train Epoch: 170 [11392/17352 (66%)] Loss: -198148.937500\n",
      "Train Epoch: 170 [12800/17352 (74%)] Loss: -215295.187500\n",
      "Train Epoch: 170 [14208/17352 (82%)] Loss: -234682.406250\n",
      "Train Epoch: 170 [15397/17352 (89%)] Loss: -64065.515625\n",
      "Train Epoch: 170 [16269/17352 (94%)] Loss: -182600.953125\n",
      "Train Epoch: 170 [17073/17352 (98%)] Loss: -90093.726562\n",
      "    epoch          : 170\n",
      "    loss           : -202817.2945974465\n",
      "    val_loss       : -111063.95103683471\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch170.pth ...\n",
      "Train Epoch: 171 [128/17352 (1%)] Loss: -230422.281250\n",
      "Train Epoch: 171 [1536/17352 (9%)] Loss: -227404.531250\n",
      "Train Epoch: 171 [2944/17352 (17%)] Loss: -222647.328125\n",
      "Train Epoch: 171 [4352/17352 (25%)] Loss: -214028.828125\n",
      "Train Epoch: 171 [5760/17352 (33%)] Loss: -203057.531250\n",
      "Train Epoch: 171 [7168/17352 (41%)] Loss: -218869.812500\n",
      "Train Epoch: 171 [8576/17352 (49%)] Loss: -233700.671875\n",
      "Train Epoch: 171 [9984/17352 (58%)] Loss: -217938.515625\n",
      "Train Epoch: 171 [11392/17352 (66%)] Loss: -203526.640625\n",
      "Train Epoch: 171 [12800/17352 (74%)] Loss: -214946.578125\n",
      "Train Epoch: 171 [14208/17352 (82%)] Loss: -232409.125000\n",
      "Train Epoch: 171 [15522/17352 (89%)] Loss: -146591.625000\n",
      "Train Epoch: 171 [16189/17352 (93%)] Loss: -192153.000000\n",
      "Train Epoch: 171 [17025/17352 (98%)] Loss: -182594.703125\n",
      "    epoch          : 171\n",
      "    loss           : -201833.239169332\n",
      "    val_loss       : -110298.77960710526\n",
      "Train Epoch: 172 [128/17352 (1%)] Loss: -194197.687500\n",
      "Train Epoch: 172 [1536/17352 (9%)] Loss: -229829.468750\n",
      "Train Epoch: 172 [2944/17352 (17%)] Loss: -223817.437500\n",
      "Train Epoch: 172 [4352/17352 (25%)] Loss: -215547.625000\n",
      "Train Epoch: 172 [5760/17352 (33%)] Loss: -218295.156250\n",
      "Train Epoch: 172 [7168/17352 (41%)] Loss: -231887.593750\n",
      "Train Epoch: 172 [8576/17352 (49%)] Loss: -222484.812500\n",
      "Train Epoch: 172 [9984/17352 (58%)] Loss: -201613.250000\n",
      "Train Epoch: 172 [11392/17352 (66%)] Loss: -215440.328125\n",
      "Train Epoch: 172 [12800/17352 (74%)] Loss: -224741.375000\n",
      "Train Epoch: 172 [14208/17352 (82%)] Loss: -221985.593750\n",
      "Train Epoch: 172 [15515/17352 (89%)] Loss: -87500.531250\n",
      "Train Epoch: 172 [16188/17352 (93%)] Loss: -195311.265625\n",
      "Train Epoch: 172 [16987/17352 (98%)] Loss: -144076.937500\n",
      "    epoch          : 172\n",
      "    loss           : -202618.42051174497\n",
      "    val_loss       : -111375.35457127889\n",
      "Train Epoch: 173 [128/17352 (1%)] Loss: -225824.359375\n",
      "Train Epoch: 173 [1536/17352 (9%)] Loss: -236598.343750\n",
      "Train Epoch: 173 [2944/17352 (17%)] Loss: -218384.328125\n",
      "Train Epoch: 173 [4352/17352 (25%)] Loss: -215785.031250\n",
      "Train Epoch: 173 [5760/17352 (33%)] Loss: -240744.500000\n",
      "Train Epoch: 173 [7168/17352 (41%)] Loss: -234105.281250\n",
      "Train Epoch: 173 [8576/17352 (49%)] Loss: -228010.125000\n",
      "Train Epoch: 173 [9984/17352 (58%)] Loss: -236561.843750\n",
      "Train Epoch: 173 [11392/17352 (66%)] Loss: -202201.343750\n",
      "Train Epoch: 173 [12800/17352 (74%)] Loss: -212164.218750\n",
      "Train Epoch: 173 [14208/17352 (82%)] Loss: -212401.609375\n",
      "Train Epoch: 173 [15523/17352 (89%)] Loss: -194311.250000\n",
      "Train Epoch: 173 [16187/17352 (93%)] Loss: -157632.703125\n",
      "Train Epoch: 173 [16992/17352 (98%)] Loss: -63647.128906\n",
      "    epoch          : 173\n",
      "    loss           : -202797.90406092702\n",
      "    val_loss       : -111082.13480072022\n",
      "Train Epoch: 174 [128/17352 (1%)] Loss: -203308.187500\n",
      "Train Epoch: 174 [1536/17352 (9%)] Loss: -223236.890625\n",
      "Train Epoch: 174 [2944/17352 (17%)] Loss: -199313.906250\n",
      "Train Epoch: 174 [4352/17352 (25%)] Loss: -235295.343750\n",
      "Train Epoch: 174 [5760/17352 (33%)] Loss: -233720.265625\n",
      "Train Epoch: 174 [7168/17352 (41%)] Loss: -203855.921875\n",
      "Train Epoch: 174 [8576/17352 (49%)] Loss: -243489.062500\n",
      "Train Epoch: 174 [9984/17352 (58%)] Loss: -227202.750000\n",
      "Train Epoch: 174 [11392/17352 (66%)] Loss: -221499.703125\n",
      "Train Epoch: 174 [12800/17352 (74%)] Loss: -212719.031250\n",
      "Train Epoch: 174 [14208/17352 (82%)] Loss: -241635.546875\n",
      "Train Epoch: 174 [15458/17352 (89%)] Loss: -142434.296875\n",
      "Train Epoch: 174 [16238/17352 (94%)] Loss: -63872.054688\n",
      "Train Epoch: 174 [16923/17352 (98%)] Loss: -169814.906250\n",
      "    epoch          : 174\n",
      "    loss           : -202537.16904362416\n",
      "    val_loss       : -110690.28632208506\n",
      "Train Epoch: 175 [128/17352 (1%)] Loss: -208719.625000\n",
      "Train Epoch: 175 [1536/17352 (9%)] Loss: -234948.531250\n",
      "Train Epoch: 175 [2944/17352 (17%)] Loss: -257867.390625\n",
      "Train Epoch: 175 [4352/17352 (25%)] Loss: -210354.046875\n",
      "Train Epoch: 175 [5760/17352 (33%)] Loss: -220114.765625\n",
      "Train Epoch: 175 [7168/17352 (41%)] Loss: -228067.218750\n",
      "Train Epoch: 175 [8576/17352 (49%)] Loss: -217150.718750\n",
      "Train Epoch: 175 [9984/17352 (58%)] Loss: -238056.234375\n",
      "Train Epoch: 175 [11392/17352 (66%)] Loss: -231496.093750\n",
      "Train Epoch: 175 [12800/17352 (74%)] Loss: -207508.671875\n",
      "Train Epoch: 175 [14208/17352 (82%)] Loss: -212161.843750\n",
      "Train Epoch: 175 [15527/17352 (89%)] Loss: -145029.875000\n",
      "Train Epoch: 175 [16311/17352 (94%)] Loss: -65218.496094\n",
      "Train Epoch: 175 [17151/17352 (99%)] Loss: -27918.599609\n",
      "    epoch          : 175\n",
      "    loss           : -202831.36471004615\n",
      "    val_loss       : -110128.12646261851\n",
      "Train Epoch: 176 [128/17352 (1%)] Loss: -222384.656250\n",
      "Train Epoch: 176 [1536/17352 (9%)] Loss: -199636.281250\n",
      "Train Epoch: 176 [2944/17352 (17%)] Loss: -243943.953125\n",
      "Train Epoch: 176 [4352/17352 (25%)] Loss: -214309.140625\n",
      "Train Epoch: 176 [5760/17352 (33%)] Loss: -233990.640625\n",
      "Train Epoch: 176 [7168/17352 (41%)] Loss: -227598.250000\n",
      "Train Epoch: 176 [8576/17352 (49%)] Loss: -224688.000000\n",
      "Train Epoch: 176 [9984/17352 (58%)] Loss: -230687.187500\n",
      "Train Epoch: 176 [11392/17352 (66%)] Loss: -237166.500000\n",
      "Train Epoch: 176 [12800/17352 (74%)] Loss: -219939.812500\n",
      "Train Epoch: 176 [14208/17352 (82%)] Loss: -226563.656250\n",
      "Train Epoch: 176 [15517/17352 (89%)] Loss: -153896.687500\n",
      "Train Epoch: 176 [16218/17352 (93%)] Loss: -187442.718750\n",
      "Train Epoch: 176 [16960/17352 (98%)] Loss: -175666.781250\n",
      "    epoch          : 176\n",
      "    loss           : -202397.5226608379\n",
      "    val_loss       : -111472.07790527344\n",
      "Train Epoch: 177 [128/17352 (1%)] Loss: -234009.062500\n",
      "Train Epoch: 177 [1536/17352 (9%)] Loss: -218672.515625\n",
      "Train Epoch: 177 [2944/17352 (17%)] Loss: -190102.171875\n",
      "Train Epoch: 177 [4352/17352 (25%)] Loss: -210853.468750\n",
      "Train Epoch: 177 [5760/17352 (33%)] Loss: -218723.312500\n",
      "Train Epoch: 177 [7168/17352 (41%)] Loss: -220112.171875\n",
      "Train Epoch: 177 [8576/17352 (49%)] Loss: -211087.984375\n",
      "Train Epoch: 177 [9984/17352 (58%)] Loss: -235302.687500\n",
      "Train Epoch: 177 [11392/17352 (66%)] Loss: -204976.750000\n",
      "Train Epoch: 177 [12800/17352 (74%)] Loss: -214606.781250\n",
      "Train Epoch: 177 [14208/17352 (82%)] Loss: -232790.703125\n",
      "Train Epoch: 177 [15525/17352 (89%)] Loss: -151183.765625\n",
      "Train Epoch: 177 [16199/17352 (93%)] Loss: -77154.640625\n",
      "Train Epoch: 177 [17074/17352 (98%)] Loss: -63753.882812\n",
      "    epoch          : 177\n",
      "    loss           : -202846.16712982382\n",
      "    val_loss       : -110473.97613245646\n",
      "Train Epoch: 178 [128/17352 (1%)] Loss: -209337.078125\n",
      "Train Epoch: 178 [1536/17352 (9%)] Loss: -220482.296875\n",
      "Train Epoch: 178 [2944/17352 (17%)] Loss: -235783.781250\n",
      "Train Epoch: 178 [4352/17352 (25%)] Loss: -224356.703125\n",
      "Train Epoch: 178 [5760/17352 (33%)] Loss: -218998.640625\n",
      "Train Epoch: 178 [7168/17352 (41%)] Loss: -229415.906250\n",
      "Train Epoch: 178 [8576/17352 (49%)] Loss: -227217.468750\n",
      "Train Epoch: 178 [9984/17352 (58%)] Loss: -217847.468750\n",
      "Train Epoch: 178 [11392/17352 (66%)] Loss: -218057.671875\n",
      "Train Epoch: 178 [12800/17352 (74%)] Loss: -225687.937500\n",
      "Train Epoch: 178 [14208/17352 (82%)] Loss: -231312.500000\n",
      "Train Epoch: 178 [15511/17352 (89%)] Loss: -126734.640625\n",
      "Train Epoch: 178 [16308/17352 (94%)] Loss: -182431.578125\n",
      "Train Epoch: 178 [16975/17352 (98%)] Loss: -89950.171875\n",
      "    epoch          : 178\n",
      "    loss           : -202687.7437375472\n",
      "    val_loss       : -110747.03087151845\n",
      "Train Epoch: 179 [128/17352 (1%)] Loss: -221033.562500\n",
      "Train Epoch: 179 [1536/17352 (9%)] Loss: -229697.343750\n",
      "Train Epoch: 179 [2944/17352 (17%)] Loss: -233610.125000\n",
      "Train Epoch: 179 [4352/17352 (25%)] Loss: -214985.500000\n",
      "Train Epoch: 179 [5760/17352 (33%)] Loss: -210747.359375\n",
      "Train Epoch: 179 [7168/17352 (41%)] Loss: -219460.000000\n",
      "Train Epoch: 179 [8576/17352 (49%)] Loss: -216643.718750\n",
      "Train Epoch: 179 [9984/17352 (58%)] Loss: -233452.390625\n",
      "Train Epoch: 179 [11392/17352 (66%)] Loss: -231749.218750\n",
      "Train Epoch: 179 [12800/17352 (74%)] Loss: -217100.703125\n",
      "Train Epoch: 179 [14208/17352 (82%)] Loss: -229649.015625\n",
      "Train Epoch: 179 [15551/17352 (90%)] Loss: -168397.656250\n",
      "Train Epoch: 179 [16165/17352 (93%)] Loss: -170227.984375\n",
      "Train Epoch: 179 [16892/17352 (97%)] Loss: -153859.562500\n",
      "    epoch          : 179\n",
      "    loss           : -202504.05788590605\n",
      "    val_loss       : -111316.24822133382\n",
      "Train Epoch: 180 [128/17352 (1%)] Loss: -225695.125000\n",
      "Train Epoch: 180 [1536/17352 (9%)] Loss: -215414.281250\n",
      "Train Epoch: 180 [2944/17352 (17%)] Loss: -202658.765625\n",
      "Train Epoch: 180 [4352/17352 (25%)] Loss: -217734.921875\n",
      "Train Epoch: 180 [5760/17352 (33%)] Loss: -189279.875000\n",
      "Train Epoch: 180 [7168/17352 (41%)] Loss: -210225.234375\n",
      "Train Epoch: 180 [8576/17352 (49%)] Loss: -202455.593750\n",
      "Train Epoch: 180 [9984/17352 (58%)] Loss: -226199.562500\n",
      "Train Epoch: 180 [11392/17352 (66%)] Loss: -223661.328125\n",
      "Train Epoch: 180 [12800/17352 (74%)] Loss: -214014.593750\n",
      "Train Epoch: 180 [14208/17352 (82%)] Loss: -225014.312500\n",
      "Train Epoch: 180 [15518/17352 (89%)] Loss: -130610.945312\n",
      "Train Epoch: 180 [16161/17352 (93%)] Loss: -5652.640625\n",
      "Train Epoch: 180 [16992/17352 (98%)] Loss: -90921.664062\n",
      "    epoch          : 180\n",
      "    loss           : -202185.8874003775\n",
      "    val_loss       : -111132.56903330486\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch180.pth ...\n",
      "Train Epoch: 181 [128/17352 (1%)] Loss: -231000.406250\n",
      "Train Epoch: 181 [1536/17352 (9%)] Loss: -230374.437500\n",
      "Train Epoch: 181 [2944/17352 (17%)] Loss: -205826.593750\n",
      "Train Epoch: 181 [4352/17352 (25%)] Loss: -216033.343750\n",
      "Train Epoch: 181 [5760/17352 (33%)] Loss: -229374.984375\n",
      "Train Epoch: 181 [7168/17352 (41%)] Loss: -229836.312500\n",
      "Train Epoch: 181 [8576/17352 (49%)] Loss: -232418.984375\n",
      "Train Epoch: 181 [9984/17352 (58%)] Loss: -212236.109375\n",
      "Train Epoch: 181 [11392/17352 (66%)] Loss: -222596.734375\n",
      "Train Epoch: 181 [12800/17352 (74%)] Loss: -217552.859375\n",
      "Train Epoch: 181 [14208/17352 (82%)] Loss: -232117.890625\n",
      "Train Epoch: 181 [15448/17352 (89%)] Loss: -88961.976562\n",
      "Train Epoch: 181 [16145/17352 (93%)] Loss: -138194.750000\n",
      "Train Epoch: 181 [17049/17352 (98%)] Loss: -5413.952637\n",
      "    epoch          : 181\n",
      "    loss           : -202497.31138907824\n",
      "    val_loss       : -111765.72011464437\n",
      "Train Epoch: 182 [128/17352 (1%)] Loss: -177196.140625\n",
      "Train Epoch: 182 [1536/17352 (9%)] Loss: -197456.953125\n",
      "Train Epoch: 182 [2944/17352 (17%)] Loss: -244964.578125\n",
      "Train Epoch: 182 [4352/17352 (25%)] Loss: -235952.640625\n",
      "Train Epoch: 182 [5760/17352 (33%)] Loss: -235343.718750\n",
      "Train Epoch: 182 [7168/17352 (41%)] Loss: -235971.687500\n",
      "Train Epoch: 182 [8576/17352 (49%)] Loss: -226639.859375\n",
      "Train Epoch: 182 [9984/17352 (58%)] Loss: -233699.375000\n",
      "Train Epoch: 182 [11392/17352 (66%)] Loss: -233946.015625\n",
      "Train Epoch: 182 [12800/17352 (74%)] Loss: -210348.125000\n",
      "Train Epoch: 182 [14208/17352 (82%)] Loss: -233522.484375\n",
      "Train Epoch: 182 [15519/17352 (89%)] Loss: -148684.062500\n",
      "Train Epoch: 182 [16210/17352 (93%)] Loss: -184196.421875\n",
      "Train Epoch: 182 [16944/17352 (98%)] Loss: -194892.281250\n",
      "    epoch          : 182\n",
      "    loss           : -202041.4281014052\n",
      "    val_loss       : -111054.53815256755\n",
      "Train Epoch: 183 [128/17352 (1%)] Loss: -225274.078125\n",
      "Train Epoch: 183 [1536/17352 (9%)] Loss: -209588.343750\n",
      "Train Epoch: 183 [2944/17352 (17%)] Loss: -243864.921875\n",
      "Train Epoch: 183 [4352/17352 (25%)] Loss: -230230.093750\n",
      "Train Epoch: 183 [5760/17352 (33%)] Loss: -203261.937500\n",
      "Train Epoch: 183 [7168/17352 (41%)] Loss: -238152.875000\n",
      "Train Epoch: 183 [8576/17352 (49%)] Loss: -222664.062500\n",
      "Train Epoch: 183 [9984/17352 (58%)] Loss: -235627.359375\n",
      "Train Epoch: 183 [11392/17352 (66%)] Loss: -222141.062500\n",
      "Train Epoch: 183 [12800/17352 (74%)] Loss: -211745.703125\n",
      "Train Epoch: 183 [14208/17352 (82%)] Loss: -233453.781250\n",
      "Train Epoch: 183 [15523/17352 (89%)] Loss: -171515.000000\n",
      "Train Epoch: 183 [16430/17352 (95%)] Loss: -147440.156250\n",
      "Train Epoch: 183 [17127/17352 (99%)] Loss: -65539.125000\n",
      "    epoch          : 183\n",
      "    loss           : -200246.31767119336\n",
      "    val_loss       : -112247.80772711436\n",
      "Train Epoch: 184 [128/17352 (1%)] Loss: -231899.203125\n",
      "Train Epoch: 184 [1536/17352 (9%)] Loss: -218201.640625\n",
      "Train Epoch: 184 [2944/17352 (17%)] Loss: -231773.484375\n",
      "Train Epoch: 184 [4352/17352 (25%)] Loss: -230307.828125\n",
      "Train Epoch: 184 [5760/17352 (33%)] Loss: -212990.937500\n",
      "Train Epoch: 184 [7168/17352 (41%)] Loss: -247675.937500\n",
      "Train Epoch: 184 [8576/17352 (49%)] Loss: -222348.843750\n",
      "Train Epoch: 184 [9984/17352 (58%)] Loss: -217441.031250\n",
      "Train Epoch: 184 [11392/17352 (66%)] Loss: -198124.093750\n",
      "Train Epoch: 184 [12800/17352 (74%)] Loss: -233817.890625\n",
      "Train Epoch: 184 [14208/17352 (82%)] Loss: -211230.906250\n",
      "Train Epoch: 184 [15485/17352 (89%)] Loss: -87501.125000\n",
      "Train Epoch: 184 [16439/17352 (95%)] Loss: -175705.906250\n",
      "Train Epoch: 184 [17092/17352 (99%)] Loss: -135958.406250\n",
      "    epoch          : 184\n",
      "    loss           : -202709.34549339346\n",
      "    val_loss       : -111067.16882890066\n",
      "Train Epoch: 185 [128/17352 (1%)] Loss: -190362.343750\n",
      "Train Epoch: 185 [1536/17352 (9%)] Loss: -222787.750000\n",
      "Train Epoch: 185 [2944/17352 (17%)] Loss: -186248.484375\n",
      "Train Epoch: 185 [4352/17352 (25%)] Loss: -238327.312500\n",
      "Train Epoch: 185 [5760/17352 (33%)] Loss: -225219.234375\n",
      "Train Epoch: 185 [7168/17352 (41%)] Loss: -235151.500000\n",
      "Train Epoch: 185 [8576/17352 (49%)] Loss: -230797.843750\n",
      "Train Epoch: 185 [9984/17352 (58%)] Loss: -231564.015625\n",
      "Train Epoch: 185 [11392/17352 (66%)] Loss: -232719.187500\n",
      "Train Epoch: 185 [12800/17352 (74%)] Loss: -242789.500000\n",
      "Train Epoch: 185 [14208/17352 (82%)] Loss: -215287.187500\n",
      "Train Epoch: 185 [15470/17352 (89%)] Loss: -8527.624023\n",
      "Train Epoch: 185 [16200/17352 (93%)] Loss: -146861.843750\n",
      "Train Epoch: 185 [16947/17352 (98%)] Loss: -76195.625000\n",
      "    epoch          : 185\n",
      "    loss           : -202083.01592976614\n",
      "    val_loss       : -111347.29920132954\n",
      "Train Epoch: 186 [128/17352 (1%)] Loss: -233267.250000\n",
      "Train Epoch: 186 [1536/17352 (9%)] Loss: -227467.093750\n",
      "Train Epoch: 186 [2944/17352 (17%)] Loss: -199748.812500\n",
      "Train Epoch: 186 [4352/17352 (25%)] Loss: -221512.828125\n",
      "Train Epoch: 186 [5760/17352 (33%)] Loss: -240792.750000\n",
      "Train Epoch: 186 [7168/17352 (41%)] Loss: -237685.015625\n",
      "Train Epoch: 186 [8576/17352 (49%)] Loss: -245278.265625\n",
      "Train Epoch: 186 [9984/17352 (58%)] Loss: -215979.687500\n",
      "Train Epoch: 186 [11392/17352 (66%)] Loss: -219692.953125\n",
      "Train Epoch: 186 [12800/17352 (74%)] Loss: -228297.125000\n",
      "Train Epoch: 186 [14208/17352 (82%)] Loss: -232689.328125\n",
      "Train Epoch: 186 [15521/17352 (89%)] Loss: -155982.281250\n",
      "Train Epoch: 186 [16396/17352 (94%)] Loss: -5331.041992\n",
      "Train Epoch: 186 [16994/17352 (98%)] Loss: -195319.250000\n",
      "    epoch          : 186\n",
      "    loss           : -202944.41269269085\n",
      "    val_loss       : -111679.97745418549\n",
      "Train Epoch: 187 [128/17352 (1%)] Loss: -214744.015625\n",
      "Train Epoch: 187 [1536/17352 (9%)] Loss: -218419.437500\n",
      "Train Epoch: 187 [2944/17352 (17%)] Loss: -214697.046875\n",
      "Train Epoch: 187 [4352/17352 (25%)] Loss: -232246.218750\n",
      "Train Epoch: 187 [5760/17352 (33%)] Loss: -212781.828125\n",
      "Train Epoch: 187 [7168/17352 (41%)] Loss: -222631.656250\n",
      "Train Epoch: 187 [8576/17352 (49%)] Loss: -208242.578125\n",
      "Train Epoch: 187 [9984/17352 (58%)] Loss: -229984.281250\n",
      "Train Epoch: 187 [11392/17352 (66%)] Loss: -216211.218750\n",
      "Train Epoch: 187 [12800/17352 (74%)] Loss: -231386.250000\n",
      "Train Epoch: 187 [14208/17352 (82%)] Loss: -231904.406250\n",
      "Train Epoch: 187 [15455/17352 (89%)] Loss: -85838.476562\n",
      "Train Epoch: 187 [16073/17352 (93%)] Loss: -23690.156250\n",
      "Train Epoch: 187 [16995/17352 (98%)] Loss: -184535.265625\n",
      "    epoch          : 187\n",
      "    loss           : -202717.36153457948\n",
      "    val_loss       : -111315.67822227479\n",
      "Train Epoch: 188 [128/17352 (1%)] Loss: -217926.062500\n",
      "Train Epoch: 188 [1536/17352 (9%)] Loss: -227752.328125\n",
      "Train Epoch: 188 [2944/17352 (17%)] Loss: -256218.093750\n",
      "Train Epoch: 188 [4352/17352 (25%)] Loss: -233777.578125\n",
      "Train Epoch: 188 [5760/17352 (33%)] Loss: -234693.125000\n",
      "Train Epoch: 188 [7168/17352 (41%)] Loss: -229201.640625\n",
      "Train Epoch: 188 [8576/17352 (49%)] Loss: -218687.375000\n",
      "Train Epoch: 188 [9984/17352 (58%)] Loss: -225103.500000\n",
      "Train Epoch: 188 [11392/17352 (66%)] Loss: -221813.062500\n",
      "Train Epoch: 188 [12800/17352 (74%)] Loss: -232634.421875\n",
      "Train Epoch: 188 [14208/17352 (82%)] Loss: -225611.046875\n",
      "Train Epoch: 188 [15479/17352 (89%)] Loss: -63127.937500\n",
      "Train Epoch: 188 [16140/17352 (93%)] Loss: -83841.781250\n",
      "Train Epoch: 188 [17003/17352 (98%)] Loss: -174293.218750\n",
      "    epoch          : 188\n",
      "    loss           : -202874.36510656984\n",
      "    val_loss       : -111169.89841156006\n",
      "Train Epoch: 189 [128/17352 (1%)] Loss: -202772.875000\n",
      "Train Epoch: 189 [1536/17352 (9%)] Loss: -236805.218750\n",
      "Train Epoch: 189 [2944/17352 (17%)] Loss: -215470.359375\n",
      "Train Epoch: 189 [4352/17352 (25%)] Loss: -239280.953125\n",
      "Train Epoch: 189 [5760/17352 (33%)] Loss: -229282.281250\n",
      "Train Epoch: 189 [7168/17352 (41%)] Loss: -238203.781250\n",
      "Train Epoch: 189 [8576/17352 (49%)] Loss: -208188.937500\n",
      "Train Epoch: 189 [9984/17352 (58%)] Loss: -211282.453125\n",
      "Train Epoch: 189 [11392/17352 (66%)] Loss: -220048.312500\n",
      "Train Epoch: 189 [12800/17352 (74%)] Loss: -224433.843750\n",
      "Train Epoch: 189 [14208/17352 (82%)] Loss: -245976.593750\n",
      "Train Epoch: 189 [15480/17352 (89%)] Loss: -64654.125000\n",
      "Train Epoch: 189 [16369/17352 (94%)] Loss: -169179.406250\n",
      "Train Epoch: 189 [16983/17352 (98%)] Loss: -144445.968750\n",
      "    epoch          : 189\n",
      "    loss           : -203078.16188653524\n",
      "    val_loss       : -111406.73016090393\n",
      "Train Epoch: 190 [128/17352 (1%)] Loss: -232720.500000\n",
      "Train Epoch: 190 [1536/17352 (9%)] Loss: -231691.500000\n",
      "Train Epoch: 190 [2944/17352 (17%)] Loss: -203176.671875\n",
      "Train Epoch: 190 [4352/17352 (25%)] Loss: -209621.234375\n",
      "Train Epoch: 190 [5760/17352 (33%)] Loss: -211843.031250\n",
      "Train Epoch: 190 [7168/17352 (41%)] Loss: -223879.265625\n",
      "Train Epoch: 190 [8576/17352 (49%)] Loss: -214227.281250\n",
      "Train Epoch: 190 [9984/17352 (58%)] Loss: -178742.968750\n",
      "Train Epoch: 190 [11392/17352 (66%)] Loss: -209257.781250\n",
      "Train Epoch: 190 [12800/17352 (74%)] Loss: -214729.296875\n",
      "Train Epoch: 190 [14208/17352 (82%)] Loss: -214034.609375\n",
      "Train Epoch: 190 [15490/17352 (89%)] Loss: -63940.214844\n",
      "Train Epoch: 190 [16068/17352 (93%)] Loss: -183429.562500\n",
      "Train Epoch: 190 [16963/17352 (98%)] Loss: -132839.484375\n",
      "    epoch          : 190\n",
      "    loss           : -201733.7690167523\n",
      "    val_loss       : -110534.40257930756\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch190.pth ...\n",
      "Train Epoch: 191 [128/17352 (1%)] Loss: -193698.078125\n",
      "Train Epoch: 191 [1536/17352 (9%)] Loss: -210563.390625\n",
      "Train Epoch: 191 [2944/17352 (17%)] Loss: -239546.265625\n",
      "Train Epoch: 191 [4352/17352 (25%)] Loss: -212279.265625\n",
      "Train Epoch: 191 [5760/17352 (33%)] Loss: -218995.781250\n",
      "Train Epoch: 191 [7168/17352 (41%)] Loss: -230426.703125\n",
      "Train Epoch: 191 [8576/17352 (49%)] Loss: -223807.281250\n",
      "Train Epoch: 191 [9984/17352 (58%)] Loss: -236531.250000\n",
      "Train Epoch: 191 [11392/17352 (66%)] Loss: -193118.937500\n",
      "Train Epoch: 191 [12800/17352 (74%)] Loss: -229719.015625\n",
      "Train Epoch: 191 [14208/17352 (82%)] Loss: -227700.484375\n",
      "Train Epoch: 191 [15459/17352 (89%)] Loss: -163295.953125\n",
      "Train Epoch: 191 [16267/17352 (94%)] Loss: -145994.765625\n",
      "Train Epoch: 191 [16938/17352 (98%)] Loss: -169685.765625\n",
      "    epoch          : 191\n",
      "    loss           : -202478.69270068686\n",
      "    val_loss       : -111720.80401846567\n",
      "Train Epoch: 192 [128/17352 (1%)] Loss: -208709.218750\n",
      "Train Epoch: 192 [1536/17352 (9%)] Loss: -214639.375000\n",
      "Train Epoch: 192 [2944/17352 (17%)] Loss: -210793.343750\n",
      "Train Epoch: 192 [4352/17352 (25%)] Loss: -229429.750000\n",
      "Train Epoch: 192 [5760/17352 (33%)] Loss: -245753.437500\n",
      "Train Epoch: 192 [7168/17352 (41%)] Loss: -206414.093750\n",
      "Train Epoch: 192 [8576/17352 (49%)] Loss: -214187.109375\n",
      "Train Epoch: 192 [9984/17352 (58%)] Loss: -227865.687500\n",
      "Train Epoch: 192 [11392/17352 (66%)] Loss: -195389.250000\n",
      "Train Epoch: 192 [12800/17352 (74%)] Loss: -230880.015625\n",
      "Train Epoch: 192 [14208/17352 (82%)] Loss: -223144.953125\n",
      "Train Epoch: 192 [15484/17352 (89%)] Loss: -26768.300781\n",
      "Train Epoch: 192 [16122/17352 (93%)] Loss: -89685.062500\n",
      "Train Epoch: 192 [16875/17352 (97%)] Loss: -135534.578125\n",
      "    epoch          : 192\n",
      "    loss           : -203269.17595165689\n",
      "    val_loss       : -111492.11800181071\n",
      "Train Epoch: 193 [128/17352 (1%)] Loss: -200374.015625\n",
      "Train Epoch: 193 [1536/17352 (9%)] Loss: -221822.718750\n",
      "Train Epoch: 193 [2944/17352 (17%)] Loss: -197888.593750\n",
      "Train Epoch: 193 [4352/17352 (25%)] Loss: -238368.265625\n",
      "Train Epoch: 193 [5760/17352 (33%)] Loss: -223703.109375\n",
      "Train Epoch: 193 [7168/17352 (41%)] Loss: -222847.812500\n",
      "Train Epoch: 193 [8576/17352 (49%)] Loss: -215065.203125\n",
      "Train Epoch: 193 [9984/17352 (58%)] Loss: -240383.187500\n",
      "Train Epoch: 193 [11392/17352 (66%)] Loss: -235118.093750\n",
      "Train Epoch: 193 [12800/17352 (74%)] Loss: -237822.031250\n",
      "Train Epoch: 193 [14208/17352 (82%)] Loss: -210027.312500\n",
      "Train Epoch: 193 [15487/17352 (89%)] Loss: -76574.093750\n",
      "Train Epoch: 193 [16076/17352 (93%)] Loss: -68355.726562\n",
      "Train Epoch: 193 [16974/17352 (98%)] Loss: -83348.921875\n",
      "    epoch          : 193\n",
      "    loss           : -203413.52495477663\n",
      "    val_loss       : -110567.1963087082\n",
      "Train Epoch: 194 [128/17352 (1%)] Loss: -234888.640625\n",
      "Train Epoch: 194 [1536/17352 (9%)] Loss: -224030.828125\n",
      "Train Epoch: 194 [2944/17352 (17%)] Loss: -237689.609375\n",
      "Train Epoch: 194 [4352/17352 (25%)] Loss: -224633.437500\n",
      "Train Epoch: 194 [5760/17352 (33%)] Loss: -209702.625000\n",
      "Train Epoch: 194 [7168/17352 (41%)] Loss: -197763.171875\n",
      "Train Epoch: 194 [8576/17352 (49%)] Loss: -217989.812500\n",
      "Train Epoch: 194 [9984/17352 (58%)] Loss: -233338.890625\n",
      "Train Epoch: 194 [11392/17352 (66%)] Loss: -212813.031250\n",
      "Train Epoch: 194 [12800/17352 (74%)] Loss: -217522.218750\n",
      "Train Epoch: 194 [14208/17352 (82%)] Loss: -222627.156250\n",
      "Train Epoch: 194 [15543/17352 (90%)] Loss: -134260.062500\n",
      "Train Epoch: 194 [16244/17352 (94%)] Loss: -169278.296875\n",
      "Train Epoch: 194 [16927/17352 (98%)] Loss: -169785.468750\n",
      "    epoch          : 194\n",
      "    loss           : -203544.83084823302\n",
      "    val_loss       : -111879.40375591913\n",
      "Train Epoch: 195 [128/17352 (1%)] Loss: -230963.937500\n",
      "Train Epoch: 195 [1536/17352 (9%)] Loss: -214273.593750\n",
      "Train Epoch: 195 [2944/17352 (17%)] Loss: -235932.781250\n",
      "Train Epoch: 195 [4352/17352 (25%)] Loss: -214099.203125\n",
      "Train Epoch: 195 [5760/17352 (33%)] Loss: -219430.546875\n",
      "Train Epoch: 195 [7168/17352 (41%)] Loss: -211544.406250\n",
      "Train Epoch: 195 [8576/17352 (49%)] Loss: -234445.437500\n",
      "Train Epoch: 195 [9984/17352 (58%)] Loss: -226209.687500\n",
      "Train Epoch: 195 [11392/17352 (66%)] Loss: -227750.375000\n",
      "Train Epoch: 195 [12800/17352 (74%)] Loss: -227945.906250\n",
      "Train Epoch: 195 [14208/17352 (82%)] Loss: -223462.500000\n",
      "Train Epoch: 195 [15552/17352 (90%)] Loss: -143961.625000\n",
      "Train Epoch: 195 [16204/17352 (93%)] Loss: -168967.031250\n",
      "Train Epoch: 195 [16977/17352 (98%)] Loss: -135144.906250\n",
      "    epoch          : 195\n",
      "    loss           : -203059.60261443479\n",
      "    val_loss       : -111817.89018389383\n",
      "Train Epoch: 196 [128/17352 (1%)] Loss: -211994.468750\n",
      "Train Epoch: 196 [1536/17352 (9%)] Loss: -224595.734375\n",
      "Train Epoch: 196 [2944/17352 (17%)] Loss: -239539.968750\n",
      "Train Epoch: 196 [4352/17352 (25%)] Loss: -216909.812500\n",
      "Train Epoch: 196 [5760/17352 (33%)] Loss: -220021.250000\n",
      "Train Epoch: 196 [7168/17352 (41%)] Loss: -217266.937500\n",
      "Train Epoch: 196 [8576/17352 (49%)] Loss: -236529.187500\n",
      "Train Epoch: 196 [9984/17352 (58%)] Loss: -217226.765625\n",
      "Train Epoch: 196 [11392/17352 (66%)] Loss: -208065.703125\n",
      "Train Epoch: 196 [12800/17352 (74%)] Loss: -207489.343750\n",
      "Train Epoch: 196 [14208/17352 (82%)] Loss: -232729.796875\n",
      "Train Epoch: 196 [15492/17352 (89%)] Loss: -9054.701172\n",
      "Train Epoch: 196 [16305/17352 (94%)] Loss: -147717.859375\n",
      "Train Epoch: 196 [17009/17352 (98%)] Loss: -24854.398438\n",
      "    epoch          : 196\n",
      "    loss           : -202603.83387623218\n",
      "    val_loss       : -111093.59400749207\n",
      "Train Epoch: 197 [128/17352 (1%)] Loss: -203507.625000\n",
      "Train Epoch: 197 [1536/17352 (9%)] Loss: -217776.125000\n",
      "Train Epoch: 197 [2944/17352 (17%)] Loss: -220833.546875\n",
      "Train Epoch: 197 [4352/17352 (25%)] Loss: -219831.390625\n",
      "Train Epoch: 197 [5760/17352 (33%)] Loss: -232323.343750\n",
      "Train Epoch: 197 [7168/17352 (41%)] Loss: -194875.140625\n",
      "Train Epoch: 197 [8576/17352 (49%)] Loss: -214079.593750\n",
      "Train Epoch: 197 [9984/17352 (58%)] Loss: -229686.656250\n",
      "Train Epoch: 197 [11392/17352 (66%)] Loss: -220112.750000\n",
      "Train Epoch: 197 [12800/17352 (74%)] Loss: -223293.656250\n",
      "Train Epoch: 197 [14208/17352 (82%)] Loss: -220351.531250\n",
      "Train Epoch: 197 [15451/17352 (89%)] Loss: -24457.144531\n",
      "Train Epoch: 197 [16151/17352 (93%)] Loss: -155803.609375\n",
      "Train Epoch: 197 [16903/17352 (97%)] Loss: -153865.640625\n",
      "    epoch          : 197\n",
      "    loss           : -203939.12463624685\n",
      "    val_loss       : -111508.60242303212\n",
      "Train Epoch: 198 [128/17352 (1%)] Loss: -191260.437500\n",
      "Train Epoch: 198 [1536/17352 (9%)] Loss: -219299.625000\n",
      "Train Epoch: 198 [2944/17352 (17%)] Loss: -213676.812500\n",
      "Train Epoch: 198 [4352/17352 (25%)] Loss: -217150.484375\n",
      "Train Epoch: 198 [5760/17352 (33%)] Loss: -225535.828125\n",
      "Train Epoch: 198 [7168/17352 (41%)] Loss: -219919.375000\n",
      "Train Epoch: 198 [8576/17352 (49%)] Loss: -218706.125000\n",
      "Train Epoch: 198 [9984/17352 (58%)] Loss: -231721.421875\n",
      "Train Epoch: 198 [11392/17352 (66%)] Loss: -227045.625000\n",
      "Train Epoch: 198 [12800/17352 (74%)] Loss: -220382.015625\n",
      "Train Epoch: 198 [14208/17352 (82%)] Loss: -221717.250000\n",
      "Train Epoch: 198 [15527/17352 (89%)] Loss: -138357.781250\n",
      "Train Epoch: 198 [16395/17352 (94%)] Loss: -124329.945312\n",
      "Train Epoch: 198 [17199/17352 (99%)] Loss: -197931.062500\n",
      "    epoch          : 198\n",
      "    loss           : -203520.88433633075\n",
      "    val_loss       : -111698.22218170165\n",
      "Train Epoch: 199 [128/17352 (1%)] Loss: -219799.343750\n",
      "Train Epoch: 199 [1536/17352 (9%)] Loss: -219760.875000\n",
      "Train Epoch: 199 [2944/17352 (17%)] Loss: -236305.625000\n",
      "Train Epoch: 199 [4352/17352 (25%)] Loss: -233718.843750\n",
      "Train Epoch: 199 [5760/17352 (33%)] Loss: -243905.890625\n",
      "Train Epoch: 199 [7168/17352 (41%)] Loss: -245214.921875\n",
      "Train Epoch: 199 [8576/17352 (49%)] Loss: -245563.234375\n",
      "Train Epoch: 199 [9984/17352 (58%)] Loss: -208528.828125\n",
      "Train Epoch: 199 [11392/17352 (66%)] Loss: -255458.015625\n",
      "Train Epoch: 199 [12800/17352 (74%)] Loss: -216774.750000\n",
      "Train Epoch: 199 [14208/17352 (82%)] Loss: -228186.671875\n",
      "Train Epoch: 199 [15531/17352 (90%)] Loss: -154961.390625\n",
      "Train Epoch: 199 [16281/17352 (94%)] Loss: -62418.218750\n",
      "Train Epoch: 199 [16935/17352 (98%)] Loss: -79559.453125\n",
      "    epoch          : 199\n",
      "    loss           : -202470.69478161703\n",
      "    val_loss       : -110915.38052482605\n",
      "Train Epoch: 200 [128/17352 (1%)] Loss: -224585.062500\n",
      "Train Epoch: 200 [1536/17352 (9%)] Loss: -234379.593750\n",
      "Train Epoch: 200 [2944/17352 (17%)] Loss: -223606.281250\n",
      "Train Epoch: 200 [4352/17352 (25%)] Loss: -216803.453125\n",
      "Train Epoch: 200 [5760/17352 (33%)] Loss: -217224.312500\n",
      "Train Epoch: 200 [7168/17352 (41%)] Loss: -199003.687500\n",
      "Train Epoch: 200 [8576/17352 (49%)] Loss: -214770.796875\n",
      "Train Epoch: 200 [9984/17352 (58%)] Loss: -226727.843750\n",
      "Train Epoch: 200 [11392/17352 (66%)] Loss: -222547.062500\n",
      "Train Epoch: 200 [12800/17352 (74%)] Loss: -213722.640625\n",
      "Train Epoch: 200 [14208/17352 (82%)] Loss: -222751.375000\n",
      "Train Epoch: 200 [15535/17352 (90%)] Loss: -141249.453125\n",
      "Train Epoch: 200 [16358/17352 (94%)] Loss: -150358.312500\n",
      "Train Epoch: 200 [17065/17352 (98%)] Loss: -124360.304688\n",
      "    epoch          : 200\n",
      "    loss           : -202990.6955124004\n",
      "    val_loss       : -110087.57885974248\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [128/17352 (1%)] Loss: -234989.343750\n",
      "Train Epoch: 201 [1536/17352 (9%)] Loss: -203635.015625\n",
      "Train Epoch: 201 [2944/17352 (17%)] Loss: -206746.625000\n",
      "Train Epoch: 201 [4352/17352 (25%)] Loss: -228459.984375\n",
      "Train Epoch: 201 [5760/17352 (33%)] Loss: -222974.656250\n",
      "Train Epoch: 201 [7168/17352 (41%)] Loss: -218055.890625\n",
      "Train Epoch: 201 [8576/17352 (49%)] Loss: -238084.312500\n",
      "Train Epoch: 201 [9984/17352 (58%)] Loss: -233634.093750\n",
      "Train Epoch: 201 [11392/17352 (66%)] Loss: -215704.375000\n",
      "Train Epoch: 201 [12800/17352 (74%)] Loss: -236252.312500\n",
      "Train Epoch: 201 [14208/17352 (82%)] Loss: -215821.750000\n",
      "Train Epoch: 201 [15523/17352 (89%)] Loss: -191198.625000\n",
      "Train Epoch: 201 [16364/17352 (94%)] Loss: -149550.703125\n",
      "Train Epoch: 201 [17035/17352 (98%)] Loss: -5639.505859\n",
      "    epoch          : 201\n",
      "    loss           : -203076.67923526635\n",
      "    val_loss       : -112986.81243502299\n",
      "Train Epoch: 202 [128/17352 (1%)] Loss: -232236.406250\n",
      "Train Epoch: 202 [1536/17352 (9%)] Loss: -215176.046875\n",
      "Train Epoch: 202 [2944/17352 (17%)] Loss: -209191.843750\n",
      "Train Epoch: 202 [4352/17352 (25%)] Loss: -218812.984375\n",
      "Train Epoch: 202 [5760/17352 (33%)] Loss: -222641.734375\n",
      "Train Epoch: 202 [7168/17352 (41%)] Loss: -236927.187500\n",
      "Train Epoch: 202 [8576/17352 (49%)] Loss: -236016.796875\n",
      "Train Epoch: 202 [9984/17352 (58%)] Loss: -215978.468750\n",
      "Train Epoch: 202 [11392/17352 (66%)] Loss: -215268.953125\n",
      "Train Epoch: 202 [12800/17352 (74%)] Loss: -216108.093750\n",
      "Train Epoch: 202 [14208/17352 (82%)] Loss: -235275.000000\n",
      "Train Epoch: 202 [15417/17352 (89%)] Loss: -5714.031738\n",
      "Train Epoch: 202 [16128/17352 (93%)] Loss: -154093.484375\n",
      "Train Epoch: 202 [16863/17352 (97%)] Loss: -156752.062500\n",
      "    epoch          : 202\n",
      "    loss           : -203741.3390212091\n",
      "    val_loss       : -111663.40502929688\n",
      "Train Epoch: 203 [128/17352 (1%)] Loss: -231602.843750\n",
      "Train Epoch: 203 [1536/17352 (9%)] Loss: -237843.625000\n",
      "Train Epoch: 203 [2944/17352 (17%)] Loss: -226111.312500\n",
      "Train Epoch: 203 [4352/17352 (25%)] Loss: -226581.796875\n",
      "Train Epoch: 203 [5760/17352 (33%)] Loss: -210943.031250\n",
      "Train Epoch: 203 [7168/17352 (41%)] Loss: -207854.515625\n",
      "Train Epoch: 203 [8576/17352 (49%)] Loss: -232341.515625\n",
      "Train Epoch: 203 [9984/17352 (58%)] Loss: -207340.000000\n",
      "Train Epoch: 203 [11392/17352 (66%)] Loss: -216028.000000\n",
      "Train Epoch: 203 [12800/17352 (74%)] Loss: -222640.265625\n",
      "Train Epoch: 203 [14208/17352 (82%)] Loss: -227765.375000\n",
      "Train Epoch: 203 [15471/17352 (89%)] Loss: -140556.671875\n",
      "Train Epoch: 203 [16218/17352 (93%)] Loss: -23762.654297\n",
      "Train Epoch: 203 [17078/17352 (98%)] Loss: -191467.281250\n",
      "    epoch          : 203\n",
      "    loss           : -204145.6910392198\n",
      "    val_loss       : -111236.35911191304\n",
      "Train Epoch: 204 [128/17352 (1%)] Loss: -202436.437500\n",
      "Train Epoch: 204 [1536/17352 (9%)] Loss: -204402.265625\n",
      "Train Epoch: 204 [2944/17352 (17%)] Loss: -220062.484375\n",
      "Train Epoch: 204 [4352/17352 (25%)] Loss: -203058.921875\n",
      "Train Epoch: 204 [5760/17352 (33%)] Loss: -223778.078125\n",
      "Train Epoch: 204 [7168/17352 (41%)] Loss: -202640.593750\n",
      "Train Epoch: 204 [8576/17352 (49%)] Loss: -218175.578125\n",
      "Train Epoch: 204 [9984/17352 (58%)] Loss: -229644.453125\n",
      "Train Epoch: 204 [11392/17352 (66%)] Loss: -232499.843750\n",
      "Train Epoch: 204 [12800/17352 (74%)] Loss: -215848.953125\n",
      "Train Epoch: 204 [14208/17352 (82%)] Loss: -234398.859375\n",
      "Train Epoch: 204 [15458/17352 (89%)] Loss: -4987.337891\n",
      "Train Epoch: 204 [16187/17352 (93%)] Loss: -169046.171875\n",
      "Train Epoch: 204 [16910/17352 (97%)] Loss: -57471.730469\n",
      "    epoch          : 204\n",
      "    loss           : -203258.5376074874\n",
      "    val_loss       : -110910.41645723979\n",
      "Train Epoch: 205 [128/17352 (1%)] Loss: -230745.265625\n",
      "Train Epoch: 205 [1536/17352 (9%)] Loss: -227946.656250\n",
      "Train Epoch: 205 [2944/17352 (17%)] Loss: -204859.781250\n",
      "Train Epoch: 205 [4352/17352 (25%)] Loss: -236730.437500\n",
      "Train Epoch: 205 [5760/17352 (33%)] Loss: -223153.203125\n",
      "Train Epoch: 205 [7168/17352 (41%)] Loss: -204767.437500\n",
      "Train Epoch: 205 [8576/17352 (49%)] Loss: -206233.125000\n",
      "Train Epoch: 205 [9984/17352 (58%)] Loss: -216532.468750\n",
      "Train Epoch: 205 [11392/17352 (66%)] Loss: -210506.875000\n",
      "Train Epoch: 205 [12800/17352 (74%)] Loss: -204555.656250\n",
      "Train Epoch: 205 [14208/17352 (82%)] Loss: -214019.750000\n",
      "Train Epoch: 205 [15470/17352 (89%)] Loss: -9237.027344\n",
      "Train Epoch: 205 [16269/17352 (94%)] Loss: -126063.960938\n",
      "Train Epoch: 205 [16987/17352 (98%)] Loss: -141003.671875\n",
      "    epoch          : 205\n",
      "    loss           : -203335.9382045669\n",
      "    val_loss       : -110521.0374909083\n",
      "Train Epoch: 206 [128/17352 (1%)] Loss: -233823.375000\n",
      "Train Epoch: 206 [1536/17352 (9%)] Loss: -233290.453125\n",
      "Train Epoch: 206 [2944/17352 (17%)] Loss: -214521.968750\n",
      "Train Epoch: 206 [4352/17352 (25%)] Loss: -216882.296875\n",
      "Train Epoch: 206 [5760/17352 (33%)] Loss: -224049.093750\n",
      "Train Epoch: 206 [7168/17352 (41%)] Loss: -231752.187500\n",
      "Train Epoch: 206 [8576/17352 (49%)] Loss: -219562.406250\n",
      "Train Epoch: 206 [9984/17352 (58%)] Loss: -223906.796875\n",
      "Train Epoch: 206 [11392/17352 (66%)] Loss: -220155.687500\n",
      "Train Epoch: 206 [12800/17352 (74%)] Loss: -230607.546875\n",
      "Train Epoch: 206 [14208/17352 (82%)] Loss: -205602.218750\n",
      "Train Epoch: 206 [15549/17352 (90%)] Loss: -139228.093750\n",
      "Train Epoch: 206 [16281/17352 (94%)] Loss: -168015.062500\n",
      "Train Epoch: 206 [17139/17352 (99%)] Loss: -235631.859375\n",
      "    epoch          : 206\n",
      "    loss           : -203832.35780201343\n",
      "    val_loss       : -112640.73902918497\n",
      "Train Epoch: 207 [128/17352 (1%)] Loss: -203117.250000\n",
      "Train Epoch: 207 [1536/17352 (9%)] Loss: -234091.015625\n",
      "Train Epoch: 207 [2944/17352 (17%)] Loss: -238523.781250\n",
      "Train Epoch: 207 [4352/17352 (25%)] Loss: -221173.421875\n",
      "Train Epoch: 207 [5760/17352 (33%)] Loss: -207165.140625\n",
      "Train Epoch: 207 [7168/17352 (41%)] Loss: -212790.656250\n",
      "Train Epoch: 207 [8576/17352 (49%)] Loss: -238577.093750\n",
      "Train Epoch: 207 [9984/17352 (58%)] Loss: -239106.250000\n",
      "Train Epoch: 207 [11392/17352 (66%)] Loss: -210303.000000\n",
      "Train Epoch: 207 [12800/17352 (74%)] Loss: -216734.031250\n",
      "Train Epoch: 207 [14208/17352 (82%)] Loss: -243070.781250\n",
      "Train Epoch: 207 [15532/17352 (90%)] Loss: -157809.156250\n",
      "Train Epoch: 207 [16249/17352 (94%)] Loss: -197712.812500\n",
      "Train Epoch: 207 [16965/17352 (98%)] Loss: -87308.226562\n",
      "    epoch          : 207\n",
      "    loss           : -202698.53156132027\n",
      "    val_loss       : -111102.9035950025\n",
      "Train Epoch: 208 [128/17352 (1%)] Loss: -229318.796875\n",
      "Train Epoch: 208 [1536/17352 (9%)] Loss: -237126.984375\n",
      "Train Epoch: 208 [2944/17352 (17%)] Loss: -202927.875000\n",
      "Train Epoch: 208 [4352/17352 (25%)] Loss: -216119.296875\n",
      "Train Epoch: 208 [5760/17352 (33%)] Loss: -226618.718750\n",
      "Train Epoch: 208 [7168/17352 (41%)] Loss: -239488.312500\n",
      "Train Epoch: 208 [8576/17352 (49%)] Loss: -238765.781250\n",
      "Train Epoch: 208 [9984/17352 (58%)] Loss: -213978.984375\n",
      "Train Epoch: 208 [11392/17352 (66%)] Loss: -239727.250000\n",
      "Train Epoch: 208 [12800/17352 (74%)] Loss: -213461.093750\n",
      "Train Epoch: 208 [14208/17352 (82%)] Loss: -216120.937500\n",
      "Train Epoch: 208 [15553/17352 (90%)] Loss: -193603.265625\n",
      "Train Epoch: 208 [16258/17352 (94%)] Loss: -84313.023438\n",
      "Train Epoch: 208 [17027/17352 (98%)] Loss: -161004.203125\n",
      "    epoch          : 208\n",
      "    loss           : -203105.31487914218\n",
      "    val_loss       : -111912.44208189646\n",
      "Train Epoch: 209 [128/17352 (1%)] Loss: -220326.968750\n",
      "Train Epoch: 209 [1536/17352 (9%)] Loss: -229595.843750\n",
      "Train Epoch: 209 [2944/17352 (17%)] Loss: -204405.812500\n",
      "Train Epoch: 209 [4352/17352 (25%)] Loss: -211525.171875\n",
      "Train Epoch: 209 [5760/17352 (33%)] Loss: -225311.125000\n",
      "Train Epoch: 209 [7168/17352 (41%)] Loss: -203876.468750\n",
      "Train Epoch: 209 [8576/17352 (49%)] Loss: -211169.156250\n",
      "Train Epoch: 209 [9984/17352 (58%)] Loss: -232685.531250\n",
      "Train Epoch: 209 [11392/17352 (66%)] Loss: -222974.906250\n",
      "Train Epoch: 209 [12800/17352 (74%)] Loss: -216752.937500\n",
      "Train Epoch: 209 [14208/17352 (82%)] Loss: -224709.531250\n",
      "Train Epoch: 209 [15511/17352 (89%)] Loss: -89138.070312\n",
      "Train Epoch: 209 [16236/17352 (94%)] Loss: -65136.593750\n",
      "Train Epoch: 209 [16910/17352 (97%)] Loss: -130090.531250\n",
      "    epoch          : 209\n",
      "    loss           : -203289.0961520816\n",
      "    val_loss       : -111232.77381439209\n",
      "Train Epoch: 210 [128/17352 (1%)] Loss: -202971.234375\n",
      "Train Epoch: 210 [1536/17352 (9%)] Loss: -239076.718750\n",
      "Train Epoch: 210 [2944/17352 (17%)] Loss: -202795.125000\n",
      "Train Epoch: 210 [4352/17352 (25%)] Loss: -227463.796875\n",
      "Train Epoch: 210 [5760/17352 (33%)] Loss: -234659.718750\n",
      "Train Epoch: 210 [7168/17352 (41%)] Loss: -236221.000000\n",
      "Train Epoch: 210 [8576/17352 (49%)] Loss: -230950.437500\n",
      "Train Epoch: 210 [9984/17352 (58%)] Loss: -234911.500000\n",
      "Train Epoch: 210 [11392/17352 (66%)] Loss: -204862.062500\n",
      "Train Epoch: 210 [12800/17352 (74%)] Loss: -190202.468750\n",
      "Train Epoch: 210 [14208/17352 (82%)] Loss: -222447.812500\n",
      "Train Epoch: 210 [15474/17352 (89%)] Loss: -8637.897461\n",
      "Train Epoch: 210 [16330/17352 (94%)] Loss: -27328.740234\n",
      "Train Epoch: 210 [17076/17352 (98%)] Loss: -78804.710938\n",
      "    epoch          : 210\n",
      "    loss           : -203531.4933180841\n",
      "    val_loss       : -111116.84322319031\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch210.pth ...\n",
      "Train Epoch: 211 [128/17352 (1%)] Loss: -215081.031250\n",
      "Train Epoch: 211 [1536/17352 (9%)] Loss: -213869.562500\n",
      "Train Epoch: 211 [2944/17352 (17%)] Loss: -225793.078125\n",
      "Train Epoch: 211 [4352/17352 (25%)] Loss: -222013.312500\n",
      "Train Epoch: 211 [5760/17352 (33%)] Loss: -233982.171875\n",
      "Train Epoch: 211 [7168/17352 (41%)] Loss: -218067.406250\n",
      "Train Epoch: 211 [8576/17352 (49%)] Loss: -221336.218750\n",
      "Train Epoch: 211 [9984/17352 (58%)] Loss: -234432.265625\n",
      "Train Epoch: 211 [11392/17352 (66%)] Loss: -186884.031250\n",
      "Train Epoch: 211 [12800/17352 (74%)] Loss: -218942.812500\n",
      "Train Epoch: 211 [14208/17352 (82%)] Loss: -232133.203125\n",
      "Train Epoch: 211 [15438/17352 (89%)] Loss: -9290.719727\n",
      "Train Epoch: 211 [16032/17352 (92%)] Loss: -135663.750000\n",
      "Train Epoch: 211 [16941/17352 (98%)] Loss: -152252.046875\n",
      "    epoch          : 211\n",
      "    loss           : -204220.13066602874\n",
      "    val_loss       : -112014.45913483301\n",
      "Train Epoch: 212 [128/17352 (1%)] Loss: -226549.671875\n",
      "Train Epoch: 212 [1536/17352 (9%)] Loss: -217878.593750\n",
      "Train Epoch: 212 [2944/17352 (17%)] Loss: -216804.500000\n",
      "Train Epoch: 212 [4352/17352 (25%)] Loss: -244849.156250\n",
      "Train Epoch: 212 [5760/17352 (33%)] Loss: -221389.640625\n",
      "Train Epoch: 212 [7168/17352 (41%)] Loss: -237221.156250\n",
      "Train Epoch: 212 [8576/17352 (49%)] Loss: -216102.218750\n",
      "Train Epoch: 212 [9984/17352 (58%)] Loss: -241165.000000\n",
      "Train Epoch: 212 [11392/17352 (66%)] Loss: -221348.500000\n",
      "Train Epoch: 212 [12800/17352 (74%)] Loss: -241109.734375\n",
      "Train Epoch: 212 [14208/17352 (82%)] Loss: -209051.500000\n",
      "Train Epoch: 212 [15397/17352 (89%)] Loss: -5604.263184\n",
      "Train Epoch: 212 [16156/17352 (93%)] Loss: -184769.437500\n",
      "Train Epoch: 212 [16988/17352 (98%)] Loss: -168729.375000\n",
      "    epoch          : 212\n",
      "    loss           : -204334.18555015206\n",
      "    val_loss       : -112055.51753877004\n",
      "Train Epoch: 213 [128/17352 (1%)] Loss: -231803.828125\n",
      "Train Epoch: 213 [1536/17352 (9%)] Loss: -208386.296875\n",
      "Train Epoch: 213 [2944/17352 (17%)] Loss: -212810.968750\n",
      "Train Epoch: 213 [4352/17352 (25%)] Loss: -228889.468750\n",
      "Train Epoch: 213 [5760/17352 (33%)] Loss: -236018.750000\n",
      "Train Epoch: 213 [7168/17352 (41%)] Loss: -206158.078125\n",
      "Train Epoch: 213 [8576/17352 (49%)] Loss: -213815.890625\n",
      "Train Epoch: 213 [9984/17352 (58%)] Loss: -219959.031250\n",
      "Train Epoch: 213 [11392/17352 (66%)] Loss: -221408.234375\n",
      "Train Epoch: 213 [12800/17352 (74%)] Loss: -225871.296875\n",
      "Train Epoch: 213 [14208/17352 (82%)] Loss: -235285.000000\n",
      "Train Epoch: 213 [15516/17352 (89%)] Loss: -194939.937500\n",
      "Train Epoch: 213 [16262/17352 (94%)] Loss: -78504.187500\n",
      "Train Epoch: 213 [16945/17352 (98%)] Loss: -171181.421875\n",
      "    epoch          : 213\n",
      "    loss           : -203813.90901583474\n",
      "    val_loss       : -111311.80752544403\n",
      "Train Epoch: 214 [128/17352 (1%)] Loss: -235097.031250\n",
      "Train Epoch: 214 [1536/17352 (9%)] Loss: -233564.875000\n",
      "Train Epoch: 214 [2944/17352 (17%)] Loss: -240104.375000\n",
      "Train Epoch: 214 [4352/17352 (25%)] Loss: -229984.390625\n",
      "Train Epoch: 214 [5760/17352 (33%)] Loss: -244595.203125\n",
      "Train Epoch: 214 [7168/17352 (41%)] Loss: -210896.265625\n",
      "Train Epoch: 214 [8576/17352 (49%)] Loss: -213504.546875\n",
      "Train Epoch: 214 [9984/17352 (58%)] Loss: -236654.187500\n",
      "Train Epoch: 214 [11392/17352 (66%)] Loss: -239052.500000\n",
      "Train Epoch: 214 [12800/17352 (74%)] Loss: -245817.656250\n",
      "Train Epoch: 214 [14208/17352 (82%)] Loss: -232636.093750\n",
      "Train Epoch: 214 [15499/17352 (89%)] Loss: -90711.726562\n",
      "Train Epoch: 214 [16269/17352 (94%)] Loss: -139127.187500\n",
      "Train Epoch: 214 [17051/17352 (98%)] Loss: -144835.234375\n",
      "    epoch          : 214\n",
      "    loss           : -203490.67080667996\n",
      "    val_loss       : -112407.3677992185\n",
      "Train Epoch: 215 [128/17352 (1%)] Loss: -235687.453125\n",
      "Train Epoch: 215 [1536/17352 (9%)] Loss: -219173.937500\n",
      "Train Epoch: 215 [2944/17352 (17%)] Loss: -212677.203125\n",
      "Train Epoch: 215 [4352/17352 (25%)] Loss: -230257.484375\n",
      "Train Epoch: 215 [5760/17352 (33%)] Loss: -213235.453125\n",
      "Train Epoch: 215 [7168/17352 (41%)] Loss: -228535.484375\n",
      "Train Epoch: 215 [8576/17352 (49%)] Loss: -203897.984375\n",
      "Train Epoch: 215 [9984/17352 (58%)] Loss: -215589.140625\n",
      "Train Epoch: 215 [11392/17352 (66%)] Loss: -216205.359375\n",
      "Train Epoch: 215 [12800/17352 (74%)] Loss: -230828.062500\n",
      "Train Epoch: 215 [14208/17352 (82%)] Loss: -235050.484375\n",
      "Train Epoch: 215 [15478/17352 (89%)] Loss: -196063.656250\n",
      "Train Epoch: 215 [16253/17352 (94%)] Loss: -152092.234375\n",
      "Train Epoch: 215 [16973/17352 (98%)] Loss: -91831.343750\n",
      "    epoch          : 215\n",
      "    loss           : -203300.29088454277\n",
      "    val_loss       : -111236.65653152466\n",
      "Train Epoch: 216 [128/17352 (1%)] Loss: -227690.906250\n",
      "Train Epoch: 216 [1536/17352 (9%)] Loss: -223649.546875\n",
      "Train Epoch: 216 [2944/17352 (17%)] Loss: -220664.406250\n",
      "Train Epoch: 216 [4352/17352 (25%)] Loss: -224231.765625\n",
      "Train Epoch: 216 [5760/17352 (33%)] Loss: -228935.625000\n",
      "Train Epoch: 216 [7168/17352 (41%)] Loss: -226725.859375\n",
      "Train Epoch: 216 [8576/17352 (49%)] Loss: -224897.812500\n",
      "Train Epoch: 216 [9984/17352 (58%)] Loss: -207039.531250\n",
      "Train Epoch: 216 [11392/17352 (66%)] Loss: -227020.093750\n",
      "Train Epoch: 216 [12800/17352 (74%)] Loss: -230218.828125\n",
      "Train Epoch: 216 [14208/17352 (82%)] Loss: -226311.218750\n",
      "Train Epoch: 216 [15543/17352 (90%)] Loss: -138010.890625\n",
      "Train Epoch: 216 [16314/17352 (94%)] Loss: -160697.406250\n",
      "Train Epoch: 216 [16973/17352 (98%)] Loss: -62822.679688\n",
      "    epoch          : 216\n",
      "    loss           : -203843.68201093224\n",
      "    val_loss       : -111233.85645656586\n",
      "Train Epoch: 217 [128/17352 (1%)] Loss: -191744.562500\n",
      "Train Epoch: 217 [1536/17352 (9%)] Loss: -228171.750000\n",
      "Train Epoch: 217 [2944/17352 (17%)] Loss: -201727.109375\n",
      "Train Epoch: 217 [4352/17352 (25%)] Loss: -241338.875000\n",
      "Train Epoch: 217 [5760/17352 (33%)] Loss: -210881.687500\n",
      "Train Epoch: 217 [7168/17352 (41%)] Loss: -207083.000000\n",
      "Train Epoch: 217 [8576/17352 (49%)] Loss: -219808.562500\n",
      "Train Epoch: 217 [9984/17352 (58%)] Loss: -232446.875000\n",
      "Train Epoch: 217 [11392/17352 (66%)] Loss: -225546.625000\n",
      "Train Epoch: 217 [12800/17352 (74%)] Loss: -200578.937500\n",
      "Train Epoch: 217 [14208/17352 (82%)] Loss: -215513.437500\n",
      "Train Epoch: 217 [15499/17352 (89%)] Loss: -65071.132812\n",
      "Train Epoch: 217 [16127/17352 (93%)] Loss: -162793.828125\n",
      "Train Epoch: 217 [17027/17352 (98%)] Loss: -229819.015625\n",
      "    epoch          : 217\n",
      "    loss           : -203844.7866243708\n",
      "    val_loss       : -111229.38712552389\n",
      "Train Epoch: 218 [128/17352 (1%)] Loss: -187531.953125\n",
      "Train Epoch: 218 [1536/17352 (9%)] Loss: -221662.593750\n",
      "Train Epoch: 218 [2944/17352 (17%)] Loss: -239430.953125\n",
      "Train Epoch: 218 [4352/17352 (25%)] Loss: -215697.953125\n",
      "Train Epoch: 218 [5760/17352 (33%)] Loss: -229478.078125\n",
      "Train Epoch: 218 [7168/17352 (41%)] Loss: -234830.640625\n",
      "Train Epoch: 218 [8576/17352 (49%)] Loss: -202439.875000\n",
      "Train Epoch: 218 [9984/17352 (58%)] Loss: -236502.093750\n",
      "Train Epoch: 218 [11392/17352 (66%)] Loss: -207979.531250\n",
      "Train Epoch: 218 [12800/17352 (74%)] Loss: -218189.265625\n",
      "Train Epoch: 218 [14208/17352 (82%)] Loss: -225343.234375\n",
      "Train Epoch: 218 [15529/17352 (89%)] Loss: -157635.968750\n",
      "Train Epoch: 218 [16380/17352 (94%)] Loss: -83993.140625\n",
      "Train Epoch: 218 [17019/17352 (98%)] Loss: -5578.950195\n",
      "    epoch          : 218\n",
      "    loss           : -203529.50005243288\n",
      "    val_loss       : -110897.76603711446\n",
      "Train Epoch: 219 [128/17352 (1%)] Loss: -221753.625000\n",
      "Train Epoch: 219 [1536/17352 (9%)] Loss: -231525.062500\n",
      "Train Epoch: 219 [2944/17352 (17%)] Loss: -239376.234375\n",
      "Train Epoch: 219 [4352/17352 (25%)] Loss: -217589.562500\n",
      "Train Epoch: 219 [5760/17352 (33%)] Loss: -233047.125000\n",
      "Train Epoch: 219 [7168/17352 (41%)] Loss: -216847.531250\n",
      "Train Epoch: 219 [8576/17352 (49%)] Loss: -234460.000000\n",
      "Train Epoch: 219 [9984/17352 (58%)] Loss: -236111.265625\n",
      "Train Epoch: 219 [11392/17352 (66%)] Loss: -204924.406250\n",
      "Train Epoch: 219 [12800/17352 (74%)] Loss: -227559.515625\n",
      "Train Epoch: 219 [14208/17352 (82%)] Loss: -241627.250000\n",
      "Train Epoch: 219 [15514/17352 (89%)] Loss: -133612.656250\n",
      "Train Epoch: 219 [16184/17352 (93%)] Loss: -78582.062500\n",
      "Train Epoch: 219 [17080/17352 (98%)] Loss: -188973.671875\n",
      "    epoch          : 219\n",
      "    loss           : -204012.40236668938\n",
      "    val_loss       : -110687.34515660604\n",
      "Train Epoch: 220 [128/17352 (1%)] Loss: -215600.359375\n",
      "Train Epoch: 220 [1536/17352 (9%)] Loss: -211189.093750\n",
      "Train Epoch: 220 [2944/17352 (17%)] Loss: -216464.750000\n",
      "Train Epoch: 220 [4352/17352 (25%)] Loss: -212748.546875\n",
      "Train Epoch: 220 [5760/17352 (33%)] Loss: -225930.000000\n",
      "Train Epoch: 220 [7168/17352 (41%)] Loss: -232759.500000\n",
      "Train Epoch: 220 [8576/17352 (49%)] Loss: -200963.031250\n",
      "Train Epoch: 220 [9984/17352 (58%)] Loss: -214224.031250\n",
      "Train Epoch: 220 [11392/17352 (66%)] Loss: -206106.812500\n",
      "Train Epoch: 220 [12800/17352 (74%)] Loss: -229928.000000\n",
      "Train Epoch: 220 [14208/17352 (82%)] Loss: -228768.250000\n",
      "Train Epoch: 220 [15544/17352 (90%)] Loss: -129106.546875\n",
      "Train Epoch: 220 [16302/17352 (94%)] Loss: -87442.250000\n",
      "Train Epoch: 220 [17109/17352 (99%)] Loss: -129416.890625\n",
      "    epoch          : 220\n",
      "    loss           : -203462.10676646393\n",
      "    val_loss       : -112077.57034695943\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch220.pth ...\n",
      "Train Epoch: 221 [128/17352 (1%)] Loss: -220605.656250\n",
      "Train Epoch: 221 [1536/17352 (9%)] Loss: -233787.718750\n",
      "Train Epoch: 221 [2944/17352 (17%)] Loss: -243981.796875\n",
      "Train Epoch: 221 [4352/17352 (25%)] Loss: -218093.937500\n",
      "Train Epoch: 221 [5760/17352 (33%)] Loss: -228104.109375\n",
      "Train Epoch: 221 [7168/17352 (41%)] Loss: -207790.828125\n",
      "Train Epoch: 221 [8576/17352 (49%)] Loss: -222904.671875\n",
      "Train Epoch: 221 [9984/17352 (58%)] Loss: -212765.671875\n",
      "Train Epoch: 221 [11392/17352 (66%)] Loss: -211199.250000\n",
      "Train Epoch: 221 [12800/17352 (74%)] Loss: -222239.218750\n",
      "Train Epoch: 221 [14208/17352 (82%)] Loss: -201988.500000\n",
      "Train Epoch: 221 [15479/17352 (89%)] Loss: -64723.781250\n",
      "Train Epoch: 221 [16118/17352 (93%)] Loss: -79471.828125\n",
      "Train Epoch: 221 [16966/17352 (98%)] Loss: -160595.015625\n",
      "    epoch          : 221\n",
      "    loss           : -203136.05586068582\n",
      "    val_loss       : -111982.38602574666\n",
      "Train Epoch: 222 [128/17352 (1%)] Loss: -210431.187500\n",
      "Train Epoch: 222 [1536/17352 (9%)] Loss: -224432.921875\n",
      "Train Epoch: 222 [2944/17352 (17%)] Loss: -233054.484375\n",
      "Train Epoch: 222 [4352/17352 (25%)] Loss: -226889.015625\n",
      "Train Epoch: 222 [5760/17352 (33%)] Loss: -222950.343750\n",
      "Train Epoch: 222 [7168/17352 (41%)] Loss: -203754.437500\n",
      "Train Epoch: 222 [8576/17352 (49%)] Loss: -226097.687500\n",
      "Train Epoch: 222 [9984/17352 (58%)] Loss: -208835.593750\n",
      "Train Epoch: 222 [11392/17352 (66%)] Loss: -227393.421875\n",
      "Train Epoch: 222 [12800/17352 (74%)] Loss: -230180.593750\n",
      "Train Epoch: 222 [14208/17352 (82%)] Loss: -224545.062500\n",
      "Train Epoch: 222 [15448/17352 (89%)] Loss: -5468.597656\n",
      "Train Epoch: 222 [16212/17352 (93%)] Loss: -65902.851562\n",
      "Train Epoch: 222 [16928/17352 (98%)] Loss: -122814.093750\n",
      "    epoch          : 222\n",
      "    loss           : -204130.84157403524\n",
      "    val_loss       : -112056.66778081258\n",
      "Train Epoch: 223 [128/17352 (1%)] Loss: -207399.375000\n",
      "Train Epoch: 223 [1536/17352 (9%)] Loss: -229008.859375\n",
      "Train Epoch: 223 [2944/17352 (17%)] Loss: -218685.781250\n",
      "Train Epoch: 223 [4352/17352 (25%)] Loss: -223239.765625\n",
      "Train Epoch: 223 [5760/17352 (33%)] Loss: -234791.515625\n",
      "Train Epoch: 223 [7168/17352 (41%)] Loss: -218673.531250\n",
      "Train Epoch: 223 [8576/17352 (49%)] Loss: -225290.593750\n",
      "Train Epoch: 223 [9984/17352 (58%)] Loss: -207538.984375\n",
      "Train Epoch: 223 [11392/17352 (66%)] Loss: -224098.187500\n",
      "Train Epoch: 223 [12800/17352 (74%)] Loss: -256508.437500\n",
      "Train Epoch: 223 [14208/17352 (82%)] Loss: -222125.281250\n",
      "Train Epoch: 223 [15498/17352 (89%)] Loss: -90253.687500\n",
      "Train Epoch: 223 [16202/17352 (93%)] Loss: -141702.500000\n",
      "Train Epoch: 223 [16872/17352 (97%)] Loss: -27866.716797\n",
      "    epoch          : 223\n",
      "    loss           : -204583.3756685193\n",
      "    val_loss       : -112053.20664691925\n",
      "Train Epoch: 224 [128/17352 (1%)] Loss: -220911.859375\n",
      "Train Epoch: 224 [1536/17352 (9%)] Loss: -218384.000000\n",
      "Train Epoch: 224 [2944/17352 (17%)] Loss: -216112.015625\n",
      "Train Epoch: 224 [4352/17352 (25%)] Loss: -216741.000000\n",
      "Train Epoch: 224 [5760/17352 (33%)] Loss: -225110.500000\n",
      "Train Epoch: 224 [7168/17352 (41%)] Loss: -233254.812500\n",
      "Train Epoch: 224 [8576/17352 (49%)] Loss: -223469.531250\n",
      "Train Epoch: 224 [9984/17352 (58%)] Loss: -233252.625000\n",
      "Train Epoch: 224 [11392/17352 (66%)] Loss: -226189.593750\n",
      "Train Epoch: 224 [12800/17352 (74%)] Loss: -245947.312500\n",
      "Train Epoch: 224 [14208/17352 (82%)] Loss: -234672.750000\n",
      "Train Epoch: 224 [15368/17352 (89%)] Loss: -9299.875000\n",
      "Train Epoch: 224 [16272/17352 (94%)] Loss: -225841.109375\n",
      "Train Epoch: 224 [16963/17352 (98%)] Loss: -87676.234375\n",
      "    epoch          : 224\n",
      "    loss           : -203241.2662115929\n",
      "    val_loss       : -111324.06470286052\n",
      "Train Epoch: 225 [128/17352 (1%)] Loss: -227399.328125\n",
      "Train Epoch: 225 [1536/17352 (9%)] Loss: -232048.000000\n",
      "Train Epoch: 225 [2944/17352 (17%)] Loss: -219218.328125\n",
      "Train Epoch: 225 [4352/17352 (25%)] Loss: -240130.187500\n",
      "Train Epoch: 225 [5760/17352 (33%)] Loss: -211282.062500\n",
      "Train Epoch: 225 [7168/17352 (41%)] Loss: -241048.093750\n",
      "Train Epoch: 225 [8576/17352 (49%)] Loss: -237934.171875\n",
      "Train Epoch: 225 [9984/17352 (58%)] Loss: -238683.843750\n",
      "Train Epoch: 225 [11392/17352 (66%)] Loss: -216288.281250\n",
      "Train Epoch: 225 [12800/17352 (74%)] Loss: -214314.125000\n",
      "Train Epoch: 225 [14208/17352 (82%)] Loss: -227007.781250\n",
      "Train Epoch: 225 [15478/17352 (89%)] Loss: -136649.531250\n",
      "Train Epoch: 225 [16144/17352 (93%)] Loss: -86898.164062\n",
      "Train Epoch: 225 [17023/17352 (98%)] Loss: -140354.031250\n",
      "    epoch          : 225\n",
      "    loss           : -204034.15926161388\n",
      "    val_loss       : -110243.04001604716\n",
      "Train Epoch: 226 [128/17352 (1%)] Loss: -228281.296875\n",
      "Train Epoch: 226 [1536/17352 (9%)] Loss: -221662.718750\n",
      "Train Epoch: 226 [2944/17352 (17%)] Loss: -235093.640625\n",
      "Train Epoch: 226 [4352/17352 (25%)] Loss: -226503.796875\n",
      "Train Epoch: 226 [5760/17352 (33%)] Loss: -220913.062500\n",
      "Train Epoch: 226 [7168/17352 (41%)] Loss: -235262.265625\n",
      "Train Epoch: 226 [8576/17352 (49%)] Loss: -211376.062500\n",
      "Train Epoch: 226 [9984/17352 (58%)] Loss: -232035.562500\n",
      "Train Epoch: 226 [11392/17352 (66%)] Loss: -235251.656250\n",
      "Train Epoch: 226 [12800/17352 (74%)] Loss: -197310.031250\n",
      "Train Epoch: 226 [14208/17352 (82%)] Loss: -236197.296875\n",
      "Train Epoch: 226 [15533/17352 (90%)] Loss: -164052.093750\n",
      "Train Epoch: 226 [16366/17352 (94%)] Loss: -24441.841797\n",
      "Train Epoch: 226 [17115/17352 (99%)] Loss: -193097.718750\n",
      "    epoch          : 226\n",
      "    loss           : -203322.71076053902\n",
      "    val_loss       : -112123.73762404124\n",
      "Train Epoch: 227 [128/17352 (1%)] Loss: -222929.234375\n",
      "Train Epoch: 227 [1536/17352 (9%)] Loss: -226915.718750\n",
      "Train Epoch: 227 [2944/17352 (17%)] Loss: -223086.187500\n",
      "Train Epoch: 227 [4352/17352 (25%)] Loss: -227131.265625\n",
      "Train Epoch: 227 [5760/17352 (33%)] Loss: -220420.156250\n",
      "Train Epoch: 227 [7168/17352 (41%)] Loss: -231040.984375\n",
      "Train Epoch: 227 [8576/17352 (49%)] Loss: -233095.265625\n",
      "Train Epoch: 227 [9984/17352 (58%)] Loss: -206632.781250\n",
      "Train Epoch: 227 [11392/17352 (66%)] Loss: -204334.281250\n",
      "Train Epoch: 227 [12800/17352 (74%)] Loss: -244632.593750\n",
      "Train Epoch: 227 [14208/17352 (82%)] Loss: -233457.921875\n",
      "Train Epoch: 227 [15555/17352 (90%)] Loss: -173467.234375\n",
      "Train Epoch: 227 [16418/17352 (95%)] Loss: -183239.437500\n",
      "Train Epoch: 227 [17105/17352 (99%)] Loss: -23798.638672\n",
      "    epoch          : 227\n",
      "    loss           : -204783.46655764995\n",
      "    val_loss       : -111265.39243780772\n",
      "Train Epoch: 228 [128/17352 (1%)] Loss: -228913.968750\n",
      "Train Epoch: 228 [1536/17352 (9%)] Loss: -231320.593750\n",
      "Train Epoch: 228 [2944/17352 (17%)] Loss: -239730.593750\n",
      "Train Epoch: 228 [4352/17352 (25%)] Loss: -211001.406250\n",
      "Train Epoch: 228 [5760/17352 (33%)] Loss: -220094.140625\n",
      "Train Epoch: 228 [7168/17352 (41%)] Loss: -202536.468750\n",
      "Train Epoch: 228 [8576/17352 (49%)] Loss: -212038.359375\n",
      "Train Epoch: 228 [9984/17352 (58%)] Loss: -209935.562500\n",
      "Train Epoch: 228 [11392/17352 (66%)] Loss: -227409.875000\n",
      "Train Epoch: 228 [12800/17352 (74%)] Loss: -215400.250000\n",
      "Train Epoch: 228 [14208/17352 (82%)] Loss: -226776.546875\n",
      "Train Epoch: 228 [15519/17352 (89%)] Loss: -190278.703125\n",
      "Train Epoch: 228 [16453/17352 (95%)] Loss: -172942.187500\n",
      "Train Epoch: 228 [17046/17352 (98%)] Loss: -135667.250000\n",
      "    epoch          : 228\n",
      "    loss           : -203092.81587536703\n",
      "    val_loss       : -110217.34824186961\n",
      "Train Epoch: 229 [128/17352 (1%)] Loss: -236058.312500\n",
      "Train Epoch: 229 [1536/17352 (9%)] Loss: -230536.187500\n",
      "Train Epoch: 229 [2944/17352 (17%)] Loss: -237579.125000\n",
      "Train Epoch: 229 [4352/17352 (25%)] Loss: -237548.937500\n",
      "Train Epoch: 229 [5760/17352 (33%)] Loss: -211983.718750\n",
      "Train Epoch: 229 [7168/17352 (41%)] Loss: -236484.968750\n",
      "Train Epoch: 229 [8576/17352 (49%)] Loss: -245202.203125\n",
      "Train Epoch: 229 [9984/17352 (58%)] Loss: -228857.796875\n",
      "Train Epoch: 229 [11392/17352 (66%)] Loss: -211746.578125\n",
      "Train Epoch: 229 [12800/17352 (74%)] Loss: -220234.593750\n",
      "Train Epoch: 229 [14208/17352 (82%)] Loss: -215745.265625\n",
      "Train Epoch: 229 [15406/17352 (89%)] Loss: -66826.625000\n",
      "Train Epoch: 229 [16119/17352 (93%)] Loss: -85108.937500\n",
      "Train Epoch: 229 [16903/17352 (97%)] Loss: -88222.429688\n",
      "    epoch          : 229\n",
      "    loss           : -203894.57423185822\n",
      "    val_loss       : -110999.94132645924\n",
      "Train Epoch: 230 [128/17352 (1%)] Loss: -232852.046875\n",
      "Train Epoch: 230 [1536/17352 (9%)] Loss: -219978.718750\n",
      "Train Epoch: 230 [2944/17352 (17%)] Loss: -192196.890625\n",
      "Train Epoch: 230 [4352/17352 (25%)] Loss: -228057.187500\n",
      "Train Epoch: 230 [5760/17352 (33%)] Loss: -217181.609375\n",
      "Train Epoch: 230 [7168/17352 (41%)] Loss: -232154.312500\n",
      "Train Epoch: 230 [8576/17352 (49%)] Loss: -208413.843750\n",
      "Train Epoch: 230 [9984/17352 (58%)] Loss: -232287.406250\n",
      "Train Epoch: 230 [11392/17352 (66%)] Loss: -216696.687500\n",
      "Train Epoch: 230 [12800/17352 (74%)] Loss: -229061.843750\n",
      "Train Epoch: 230 [14208/17352 (82%)] Loss: -217339.796875\n",
      "Train Epoch: 230 [15455/17352 (89%)] Loss: -88814.585938\n",
      "Train Epoch: 230 [16215/17352 (93%)] Loss: -156505.484375\n",
      "Train Epoch: 230 [16952/17352 (98%)] Loss: -8964.591797\n",
      "    epoch          : 230\n",
      "    loss           : -203936.53145317742\n",
      "    val_loss       : -111342.32837905883\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch230.pth ...\n",
      "Train Epoch: 231 [128/17352 (1%)] Loss: -232912.765625\n",
      "Train Epoch: 231 [1536/17352 (9%)] Loss: -203814.500000\n",
      "Train Epoch: 231 [2944/17352 (17%)] Loss: -235967.890625\n",
      "Train Epoch: 231 [4352/17352 (25%)] Loss: -230415.437500\n",
      "Train Epoch: 231 [5760/17352 (33%)] Loss: -231071.375000\n",
      "Train Epoch: 231 [7168/17352 (41%)] Loss: -230245.656250\n",
      "Train Epoch: 231 [8576/17352 (49%)] Loss: -213170.515625\n",
      "Train Epoch: 231 [9984/17352 (58%)] Loss: -212130.296875\n",
      "Train Epoch: 231 [11392/17352 (66%)] Loss: -229507.968750\n",
      "Train Epoch: 231 [12800/17352 (74%)] Loss: -234162.468750\n",
      "Train Epoch: 231 [14208/17352 (82%)] Loss: -240476.343750\n",
      "Train Epoch: 231 [15482/17352 (89%)] Loss: -147719.062500\n",
      "Train Epoch: 231 [16218/17352 (93%)] Loss: -142453.203125\n",
      "Train Epoch: 231 [16997/17352 (98%)] Loss: -133517.406250\n",
      "    epoch          : 231\n",
      "    loss           : -204184.3503958683\n",
      "    val_loss       : -111007.36468289694\n",
      "Train Epoch: 232 [128/17352 (1%)] Loss: -232472.640625\n",
      "Train Epoch: 232 [1536/17352 (9%)] Loss: -210980.750000\n",
      "Train Epoch: 232 [2944/17352 (17%)] Loss: -216664.718750\n",
      "Train Epoch: 232 [4352/17352 (25%)] Loss: -233049.390625\n",
      "Train Epoch: 232 [5760/17352 (33%)] Loss: -212084.031250\n",
      "Train Epoch: 232 [7168/17352 (41%)] Loss: -219755.140625\n",
      "Train Epoch: 232 [8576/17352 (49%)] Loss: -211725.187500\n",
      "Train Epoch: 232 [9984/17352 (58%)] Loss: -216760.187500\n",
      "Train Epoch: 232 [11392/17352 (66%)] Loss: -227626.562500\n",
      "Train Epoch: 232 [12800/17352 (74%)] Loss: -236452.296875\n",
      "Train Epoch: 232 [14208/17352 (82%)] Loss: -226165.031250\n",
      "Train Epoch: 232 [15526/17352 (89%)] Loss: -150223.437500\n",
      "Train Epoch: 232 [16134/17352 (93%)] Loss: -65802.335938\n",
      "Train Epoch: 232 [16864/17352 (97%)] Loss: -171927.750000\n",
      "    epoch          : 232\n",
      "    loss           : -203832.61420537962\n",
      "    val_loss       : -110859.12257919312\n",
      "Train Epoch: 233 [128/17352 (1%)] Loss: -231208.625000\n",
      "Train Epoch: 233 [1536/17352 (9%)] Loss: -220861.578125\n",
      "Train Epoch: 233 [2944/17352 (17%)] Loss: -246039.140625\n",
      "Train Epoch: 233 [4352/17352 (25%)] Loss: -218668.765625\n",
      "Train Epoch: 233 [5760/17352 (33%)] Loss: -219873.234375\n",
      "Train Epoch: 233 [7168/17352 (41%)] Loss: -198184.875000\n",
      "Train Epoch: 233 [8576/17352 (49%)] Loss: -197256.875000\n",
      "Train Epoch: 233 [9984/17352 (58%)] Loss: -237823.640625\n",
      "Train Epoch: 233 [11392/17352 (66%)] Loss: -211828.796875\n",
      "Train Epoch: 233 [12800/17352 (74%)] Loss: -218395.812500\n",
      "Train Epoch: 233 [14208/17352 (82%)] Loss: -236774.406250\n",
      "Train Epoch: 233 [15455/17352 (89%)] Loss: -65767.546875\n",
      "Train Epoch: 233 [16141/17352 (93%)] Loss: -156797.406250\n",
      "Train Epoch: 233 [16962/17352 (98%)] Loss: -125617.812500\n",
      "    epoch          : 233\n",
      "    loss           : -203581.1695024119\n",
      "    val_loss       : -112024.33643083573\n",
      "Train Epoch: 234 [128/17352 (1%)] Loss: -215430.875000\n",
      "Train Epoch: 234 [1536/17352 (9%)] Loss: -227329.390625\n",
      "Train Epoch: 234 [2944/17352 (17%)] Loss: -214098.171875\n",
      "Train Epoch: 234 [4352/17352 (25%)] Loss: -222875.828125\n",
      "Train Epoch: 234 [5760/17352 (33%)] Loss: -241708.937500\n",
      "Train Epoch: 234 [7168/17352 (41%)] Loss: -215208.953125\n",
      "Train Epoch: 234 [8576/17352 (49%)] Loss: -244639.843750\n",
      "Train Epoch: 234 [9984/17352 (58%)] Loss: -231505.218750\n",
      "Train Epoch: 234 [11392/17352 (66%)] Loss: -236135.953125\n",
      "Train Epoch: 234 [12800/17352 (74%)] Loss: -232188.343750\n",
      "Train Epoch: 234 [14208/17352 (82%)] Loss: -224074.000000\n",
      "Train Epoch: 234 [15459/17352 (89%)] Loss: -156772.593750\n",
      "Train Epoch: 234 [16115/17352 (93%)] Loss: -174480.687500\n",
      "Train Epoch: 234 [16938/17352 (98%)] Loss: -141700.390625\n",
      "    epoch          : 234\n",
      "    loss           : -204198.38907822987\n",
      "    val_loss       : -110757.14163227081\n",
      "Train Epoch: 235 [128/17352 (1%)] Loss: -226639.125000\n",
      "Train Epoch: 235 [1536/17352 (9%)] Loss: -215678.250000\n",
      "Train Epoch: 235 [2944/17352 (17%)] Loss: -219462.609375\n",
      "Train Epoch: 235 [4352/17352 (25%)] Loss: -228046.375000\n",
      "Train Epoch: 235 [5760/17352 (33%)] Loss: -226287.078125\n",
      "Train Epoch: 235 [7168/17352 (41%)] Loss: -252479.156250\n",
      "Train Epoch: 235 [8576/17352 (49%)] Loss: -218011.531250\n",
      "Train Epoch: 235 [9984/17352 (58%)] Loss: -221547.750000\n",
      "Train Epoch: 235 [11392/17352 (66%)] Loss: -231970.875000\n",
      "Train Epoch: 235 [12800/17352 (74%)] Loss: -231519.296875\n",
      "Train Epoch: 235 [14208/17352 (82%)] Loss: -237338.578125\n",
      "Train Epoch: 235 [15519/17352 (89%)] Loss: -89740.015625\n",
      "Train Epoch: 235 [16308/17352 (94%)] Loss: -162019.406250\n",
      "Train Epoch: 235 [16880/17352 (97%)] Loss: -136754.671875\n",
      "    epoch          : 235\n",
      "    loss           : -204067.36410706796\n",
      "    val_loss       : -112117.76363906861\n",
      "Train Epoch: 236 [128/17352 (1%)] Loss: -232072.218750\n",
      "Train Epoch: 236 [1536/17352 (9%)] Loss: -236138.406250\n",
      "Train Epoch: 236 [2944/17352 (17%)] Loss: -216885.796875\n",
      "Train Epoch: 236 [4352/17352 (25%)] Loss: -220265.390625\n",
      "Train Epoch: 236 [5760/17352 (33%)] Loss: -230569.984375\n",
      "Train Epoch: 236 [7168/17352 (41%)] Loss: -223533.000000\n",
      "Train Epoch: 236 [8576/17352 (49%)] Loss: -235873.718750\n",
      "Train Epoch: 236 [9984/17352 (58%)] Loss: -213066.625000\n",
      "Train Epoch: 236 [11392/17352 (66%)] Loss: -218389.578125\n",
      "Train Epoch: 236 [12800/17352 (74%)] Loss: -230460.953125\n",
      "Train Epoch: 236 [14208/17352 (82%)] Loss: -233722.000000\n",
      "Train Epoch: 236 [15523/17352 (89%)] Loss: -138351.453125\n",
      "Train Epoch: 236 [16344/17352 (94%)] Loss: -158527.062500\n",
      "Train Epoch: 236 [17025/17352 (98%)] Loss: -25315.751953\n",
      "    epoch          : 236\n",
      "    loss           : -203823.11737429217\n",
      "    val_loss       : -111195.49372552236\n",
      "Train Epoch: 237 [128/17352 (1%)] Loss: -223184.328125\n",
      "Train Epoch: 237 [1536/17352 (9%)] Loss: -217739.968750\n",
      "Train Epoch: 237 [2944/17352 (17%)] Loss: -209447.078125\n",
      "Train Epoch: 237 [4352/17352 (25%)] Loss: -223465.750000\n",
      "Train Epoch: 237 [5760/17352 (33%)] Loss: -220375.640625\n",
      "Train Epoch: 237 [7168/17352 (41%)] Loss: -234844.031250\n",
      "Train Epoch: 237 [8576/17352 (49%)] Loss: -204602.718750\n",
      "Train Epoch: 237 [9984/17352 (58%)] Loss: -225157.062500\n",
      "Train Epoch: 237 [11392/17352 (66%)] Loss: -230540.734375\n",
      "Train Epoch: 237 [12800/17352 (74%)] Loss: -232366.375000\n",
      "Train Epoch: 237 [14208/17352 (82%)] Loss: -234609.140625\n",
      "Train Epoch: 237 [15551/17352 (90%)] Loss: -187431.234375\n",
      "Train Epoch: 237 [16354/17352 (94%)] Loss: -141050.796875\n",
      "Train Epoch: 237 [17110/17352 (99%)] Loss: -133597.234375\n",
      "    epoch          : 237\n",
      "    loss           : -204510.88761666318\n",
      "    val_loss       : -110136.69696617126\n",
      "Train Epoch: 238 [128/17352 (1%)] Loss: -224971.859375\n",
      "Train Epoch: 238 [1536/17352 (9%)] Loss: -216838.453125\n",
      "Train Epoch: 238 [2944/17352 (17%)] Loss: -198812.750000\n",
      "Train Epoch: 238 [4352/17352 (25%)] Loss: -234028.968750\n",
      "Train Epoch: 238 [5760/17352 (33%)] Loss: -232487.828125\n",
      "Train Epoch: 238 [7168/17352 (41%)] Loss: -226177.078125\n",
      "Train Epoch: 238 [8576/17352 (49%)] Loss: -221211.312500\n",
      "Train Epoch: 238 [9984/17352 (58%)] Loss: -215277.281250\n",
      "Train Epoch: 238 [11392/17352 (66%)] Loss: -233193.984375\n",
      "Train Epoch: 238 [12800/17352 (74%)] Loss: -240802.281250\n",
      "Train Epoch: 238 [14208/17352 (82%)] Loss: -233102.781250\n",
      "Train Epoch: 238 [15504/17352 (89%)] Loss: -138315.484375\n",
      "Train Epoch: 238 [16511/17352 (95%)] Loss: -223670.421875\n",
      "Train Epoch: 238 [17164/17352 (99%)] Loss: -27932.988281\n",
      "    epoch          : 238\n",
      "    loss           : -204239.17423447987\n",
      "    val_loss       : -111406.09138183594\n",
      "Train Epoch: 239 [128/17352 (1%)] Loss: -211756.531250\n",
      "Train Epoch: 239 [1536/17352 (9%)] Loss: -231706.109375\n",
      "Train Epoch: 239 [2944/17352 (17%)] Loss: -193543.203125\n",
      "Train Epoch: 239 [4352/17352 (25%)] Loss: -236563.015625\n",
      "Train Epoch: 239 [5760/17352 (33%)] Loss: -231617.296875\n",
      "Train Epoch: 239 [7168/17352 (41%)] Loss: -209281.484375\n",
      "Train Epoch: 239 [8576/17352 (49%)] Loss: -236506.937500\n",
      "Train Epoch: 239 [9984/17352 (58%)] Loss: -202353.375000\n",
      "Train Epoch: 239 [11392/17352 (66%)] Loss: -219348.546875\n",
      "Train Epoch: 239 [12800/17352 (74%)] Loss: -210937.125000\n",
      "Train Epoch: 239 [14208/17352 (82%)] Loss: -222751.562500\n",
      "Train Epoch: 239 [15515/17352 (89%)] Loss: -139240.078125\n",
      "Train Epoch: 239 [16252/17352 (94%)] Loss: -64342.410156\n",
      "Train Epoch: 239 [16991/17352 (98%)] Loss: -194167.781250\n",
      "    epoch          : 239\n",
      "    loss           : -204151.848898254\n",
      "    val_loss       : -111717.79727776845\n",
      "Train Epoch: 240 [128/17352 (1%)] Loss: -199605.281250\n",
      "Train Epoch: 240 [1536/17352 (9%)] Loss: -199656.125000\n",
      "Train Epoch: 240 [2944/17352 (17%)] Loss: -222255.000000\n",
      "Train Epoch: 240 [4352/17352 (25%)] Loss: -242321.171875\n",
      "Train Epoch: 240 [5760/17352 (33%)] Loss: -217763.281250\n",
      "Train Epoch: 240 [7168/17352 (41%)] Loss: -233522.593750\n",
      "Train Epoch: 240 [8576/17352 (49%)] Loss: -215995.125000\n",
      "Train Epoch: 240 [9984/17352 (58%)] Loss: -233424.406250\n",
      "Train Epoch: 240 [11392/17352 (66%)] Loss: -257023.531250\n",
      "Train Epoch: 240 [12800/17352 (74%)] Loss: -221789.078125\n",
      "Train Epoch: 240 [14208/17352 (82%)] Loss: -237945.843750\n",
      "Train Epoch: 240 [15499/17352 (89%)] Loss: -186686.562500\n",
      "Train Epoch: 240 [16189/17352 (93%)] Loss: -77558.148438\n",
      "Train Epoch: 240 [17011/17352 (98%)] Loss: -5412.272461\n",
      "    epoch          : 240\n",
      "    loss           : -203835.5454789744\n",
      "    val_loss       : -111909.82003954252\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch240.pth ...\n",
      "Train Epoch: 241 [128/17352 (1%)] Loss: -214669.359375\n",
      "Train Epoch: 241 [1536/17352 (9%)] Loss: -236959.125000\n",
      "Train Epoch: 241 [2944/17352 (17%)] Loss: -229557.437500\n",
      "Train Epoch: 241 [4352/17352 (25%)] Loss: -218645.031250\n",
      "Train Epoch: 241 [5760/17352 (33%)] Loss: -233686.406250\n",
      "Train Epoch: 241 [7168/17352 (41%)] Loss: -207591.468750\n",
      "Train Epoch: 241 [8576/17352 (49%)] Loss: -234580.578125\n",
      "Train Epoch: 241 [9984/17352 (58%)] Loss: -191687.531250\n",
      "Train Epoch: 241 [11392/17352 (66%)] Loss: -202593.953125\n",
      "Train Epoch: 241 [12800/17352 (74%)] Loss: -219505.687500\n",
      "Train Epoch: 241 [14208/17352 (82%)] Loss: -223502.687500\n",
      "Train Epoch: 241 [15498/17352 (89%)] Loss: -87594.132812\n",
      "Train Epoch: 241 [16370/17352 (94%)] Loss: -233769.781250\n",
      "Train Epoch: 241 [17018/17352 (98%)] Loss: -27891.828125\n",
      "    epoch          : 241\n",
      "    loss           : -204571.20679530202\n",
      "    val_loss       : -112045.24992637635\n",
      "Train Epoch: 242 [128/17352 (1%)] Loss: -210115.187500\n",
      "Train Epoch: 242 [1536/17352 (9%)] Loss: -209479.968750\n",
      "Train Epoch: 242 [2944/17352 (17%)] Loss: -208220.640625\n",
      "Train Epoch: 242 [4352/17352 (25%)] Loss: -212990.390625\n",
      "Train Epoch: 242 [5760/17352 (33%)] Loss: -222497.218750\n",
      "Train Epoch: 242 [7168/17352 (41%)] Loss: -239575.593750\n",
      "Train Epoch: 242 [8576/17352 (49%)] Loss: -212766.000000\n",
      "Train Epoch: 242 [9984/17352 (58%)] Loss: -211591.562500\n",
      "Train Epoch: 242 [11392/17352 (66%)] Loss: -223300.031250\n",
      "Train Epoch: 242 [12800/17352 (74%)] Loss: -225584.968750\n",
      "Train Epoch: 242 [14208/17352 (82%)] Loss: -234695.656250\n",
      "Train Epoch: 242 [15542/17352 (90%)] Loss: -155719.906250\n",
      "Train Epoch: 242 [16263/17352 (94%)] Loss: -78633.007812\n",
      "Train Epoch: 242 [16927/17352 (98%)] Loss: -92015.343750\n",
      "    epoch          : 242\n",
      "    loss           : -204747.6302498427\n",
      "    val_loss       : -112366.19759755135\n",
      "Train Epoch: 243 [128/17352 (1%)] Loss: -206716.296875\n",
      "Train Epoch: 243 [1536/17352 (9%)] Loss: -234181.406250\n",
      "Train Epoch: 243 [2944/17352 (17%)] Loss: -230885.515625\n",
      "Train Epoch: 243 [4352/17352 (25%)] Loss: -234801.687500\n",
      "Train Epoch: 243 [5760/17352 (33%)] Loss: -224893.906250\n",
      "Train Epoch: 243 [7168/17352 (41%)] Loss: -218548.046875\n",
      "Train Epoch: 243 [8576/17352 (49%)] Loss: -237231.312500\n",
      "Train Epoch: 243 [9984/17352 (58%)] Loss: -237702.109375\n",
      "Train Epoch: 243 [11392/17352 (66%)] Loss: -231908.656250\n",
      "Train Epoch: 243 [12800/17352 (74%)] Loss: -224584.906250\n",
      "Train Epoch: 243 [14208/17352 (82%)] Loss: -237103.328125\n",
      "Train Epoch: 243 [15443/17352 (89%)] Loss: -136794.531250\n",
      "Train Epoch: 243 [16248/17352 (94%)] Loss: -141995.843750\n",
      "Train Epoch: 243 [17069/17352 (98%)] Loss: -139264.109375\n",
      "    epoch          : 243\n",
      "    loss           : -204615.3618262374\n",
      "    val_loss       : -111072.12366364797\n",
      "Train Epoch: 244 [128/17352 (1%)] Loss: -219163.937500\n",
      "Train Epoch: 244 [1536/17352 (9%)] Loss: -227245.515625\n",
      "Train Epoch: 244 [2944/17352 (17%)] Loss: -195974.984375\n",
      "Train Epoch: 244 [4352/17352 (25%)] Loss: -220740.062500\n",
      "Train Epoch: 244 [5760/17352 (33%)] Loss: -224100.359375\n",
      "Train Epoch: 244 [7168/17352 (41%)] Loss: -228608.375000\n",
      "Train Epoch: 244 [8576/17352 (49%)] Loss: -214998.968750\n",
      "Train Epoch: 244 [9984/17352 (58%)] Loss: -216040.593750\n",
      "Train Epoch: 244 [11392/17352 (66%)] Loss: -222900.281250\n",
      "Train Epoch: 244 [12800/17352 (74%)] Loss: -232289.703125\n",
      "Train Epoch: 244 [14208/17352 (82%)] Loss: -225940.734375\n",
      "Train Epoch: 244 [15455/17352 (89%)] Loss: -92603.695312\n",
      "Train Epoch: 244 [16186/17352 (93%)] Loss: -140717.531250\n",
      "Train Epoch: 244 [16969/17352 (98%)] Loss: -9369.314453\n",
      "    epoch          : 244\n",
      "    loss           : -203167.54336199665\n",
      "    val_loss       : -112158.65656534831\n",
      "Train Epoch: 245 [128/17352 (1%)] Loss: -234009.500000\n",
      "Train Epoch: 245 [1536/17352 (9%)] Loss: -230790.218750\n",
      "Train Epoch: 245 [2944/17352 (17%)] Loss: -210430.203125\n",
      "Train Epoch: 245 [4352/17352 (25%)] Loss: -207593.687500\n",
      "Train Epoch: 245 [5760/17352 (33%)] Loss: -210522.843750\n",
      "Train Epoch: 245 [7168/17352 (41%)] Loss: -234712.734375\n",
      "Train Epoch: 245 [8576/17352 (49%)] Loss: -220106.453125\n",
      "Train Epoch: 245 [9984/17352 (58%)] Loss: -227014.125000\n",
      "Train Epoch: 245 [11392/17352 (66%)] Loss: -231560.578125\n",
      "Train Epoch: 245 [12800/17352 (74%)] Loss: -221178.953125\n",
      "Train Epoch: 245 [14208/17352 (82%)] Loss: -218317.218750\n",
      "Train Epoch: 245 [15498/17352 (89%)] Loss: -83369.148438\n",
      "Train Epoch: 245 [16311/17352 (94%)] Loss: -127813.992188\n",
      "Train Epoch: 245 [16930/17352 (98%)] Loss: -141305.031250\n",
      "    epoch          : 245\n",
      "    loss           : -203815.9050440436\n",
      "    val_loss       : -110782.34739405314\n",
      "Train Epoch: 246 [128/17352 (1%)] Loss: -214009.968750\n",
      "Train Epoch: 246 [1536/17352 (9%)] Loss: -229384.859375\n",
      "Train Epoch: 246 [2944/17352 (17%)] Loss: -226258.328125\n",
      "Train Epoch: 246 [4352/17352 (25%)] Loss: -236123.343750\n",
      "Train Epoch: 246 [5760/17352 (33%)] Loss: -225350.156250\n",
      "Train Epoch: 246 [7168/17352 (41%)] Loss: -237033.671875\n",
      "Train Epoch: 246 [8576/17352 (49%)] Loss: -222468.250000\n",
      "Train Epoch: 246 [9984/17352 (58%)] Loss: -234320.125000\n",
      "Train Epoch: 246 [11392/17352 (66%)] Loss: -214696.843750\n",
      "Train Epoch: 246 [12800/17352 (74%)] Loss: -219427.906250\n",
      "Train Epoch: 246 [14208/17352 (82%)] Loss: -208876.156250\n",
      "Train Epoch: 246 [15406/17352 (89%)] Loss: -5257.065430\n",
      "Train Epoch: 246 [16225/17352 (94%)] Loss: -65316.242188\n",
      "Train Epoch: 246 [17033/17352 (98%)] Loss: -157467.968750\n",
      "    epoch          : 246\n",
      "    loss           : -205968.90687919463\n",
      "    val_loss       : -112569.58594201406\n",
      "Train Epoch: 247 [128/17352 (1%)] Loss: -197805.531250\n",
      "Train Epoch: 247 [1536/17352 (9%)] Loss: -222621.078125\n",
      "Train Epoch: 247 [2944/17352 (17%)] Loss: -218622.062500\n",
      "Train Epoch: 247 [4352/17352 (25%)] Loss: -233702.218750\n",
      "Train Epoch: 247 [5760/17352 (33%)] Loss: -234056.781250\n",
      "Train Epoch: 247 [7168/17352 (41%)] Loss: -204427.437500\n",
      "Train Epoch: 247 [8576/17352 (49%)] Loss: -225189.312500\n",
      "Train Epoch: 247 [9984/17352 (58%)] Loss: -223752.609375\n",
      "Train Epoch: 247 [11392/17352 (66%)] Loss: -235456.765625\n",
      "Train Epoch: 247 [12800/17352 (74%)] Loss: -222511.906250\n",
      "Train Epoch: 247 [14208/17352 (82%)] Loss: -223825.562500\n",
      "Train Epoch: 247 [15448/17352 (89%)] Loss: -166307.984375\n",
      "Train Epoch: 247 [16174/17352 (93%)] Loss: -63939.234375\n",
      "Train Epoch: 247 [16965/17352 (98%)] Loss: -89840.039062\n",
      "    epoch          : 247\n",
      "    loss           : -204620.29197580222\n",
      "    val_loss       : -112127.30790405274\n",
      "Train Epoch: 248 [128/17352 (1%)] Loss: -210276.031250\n",
      "Train Epoch: 248 [1536/17352 (9%)] Loss: -226831.078125\n",
      "Train Epoch: 248 [2944/17352 (17%)] Loss: -233910.312500\n",
      "Train Epoch: 248 [4352/17352 (25%)] Loss: -227029.156250\n",
      "Train Epoch: 248 [5760/17352 (33%)] Loss: -233213.468750\n",
      "Train Epoch: 248 [7168/17352 (41%)] Loss: -224544.343750\n",
      "Train Epoch: 248 [8576/17352 (49%)] Loss: -244829.703125\n",
      "Train Epoch: 248 [9984/17352 (58%)] Loss: -219205.828125\n",
      "Train Epoch: 248 [11392/17352 (66%)] Loss: -234034.484375\n",
      "Train Epoch: 248 [12800/17352 (74%)] Loss: -231645.875000\n",
      "Train Epoch: 248 [14208/17352 (82%)] Loss: -234126.187500\n",
      "Train Epoch: 248 [15525/17352 (89%)] Loss: -149852.671875\n",
      "Train Epoch: 248 [16379/17352 (94%)] Loss: -141454.062500\n",
      "Train Epoch: 248 [16943/17352 (98%)] Loss: -195902.890625\n",
      "    epoch          : 248\n",
      "    loss           : -204671.69928101407\n",
      "    val_loss       : -112847.09117202759\n",
      "Train Epoch: 249 [128/17352 (1%)] Loss: -217551.437500\n",
      "Train Epoch: 249 [1536/17352 (9%)] Loss: -230689.859375\n",
      "Train Epoch: 249 [2944/17352 (17%)] Loss: -217605.421875\n",
      "Train Epoch: 249 [4352/17352 (25%)] Loss: -225490.109375\n",
      "Train Epoch: 249 [5760/17352 (33%)] Loss: -214525.437500\n",
      "Train Epoch: 249 [7168/17352 (41%)] Loss: -231377.250000\n",
      "Train Epoch: 249 [8576/17352 (49%)] Loss: -222582.875000\n",
      "Train Epoch: 249 [9984/17352 (58%)] Loss: -212178.125000\n",
      "Train Epoch: 249 [11392/17352 (66%)] Loss: -234428.093750\n",
      "Train Epoch: 249 [12800/17352 (74%)] Loss: -231281.031250\n",
      "Train Epoch: 249 [14208/17352 (82%)] Loss: -245153.250000\n",
      "Train Epoch: 249 [15417/17352 (89%)] Loss: -5784.114746\n",
      "Train Epoch: 249 [16279/17352 (94%)] Loss: -65088.992188\n",
      "Train Epoch: 249 [16983/17352 (98%)] Loss: -8510.323242\n",
      "    epoch          : 249\n",
      "    loss           : -203637.53601156146\n",
      "    val_loss       : -112162.37730992635\n",
      "Train Epoch: 250 [128/17352 (1%)] Loss: -221597.437500\n",
      "Train Epoch: 250 [1536/17352 (9%)] Loss: -232224.500000\n",
      "Train Epoch: 250 [2944/17352 (17%)] Loss: -215931.218750\n",
      "Train Epoch: 250 [4352/17352 (25%)] Loss: -219086.187500\n",
      "Train Epoch: 250 [5760/17352 (33%)] Loss: -225865.031250\n",
      "Train Epoch: 250 [7168/17352 (41%)] Loss: -240493.015625\n",
      "Train Epoch: 250 [8576/17352 (49%)] Loss: -215524.281250\n",
      "Train Epoch: 250 [9984/17352 (58%)] Loss: -231539.468750\n",
      "Train Epoch: 250 [11392/17352 (66%)] Loss: -212237.062500\n",
      "Train Epoch: 250 [12800/17352 (74%)] Loss: -231338.750000\n",
      "Train Epoch: 250 [14208/17352 (82%)] Loss: -233543.031250\n",
      "Train Epoch: 250 [15520/17352 (89%)] Loss: -169195.562500\n",
      "Train Epoch: 250 [16226/17352 (94%)] Loss: -8967.602539\n",
      "Train Epoch: 250 [16990/17352 (98%)] Loss: -133700.250000\n",
      "    epoch          : 250\n",
      "    loss           : -205128.77530869862\n",
      "    val_loss       : -111152.36522789001\n",
      "Saving checkpoint: saved/models/Omniglot_VaeCategory/0906_120549/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [128/17352 (1%)] Loss: -235694.609375\n",
      "Train Epoch: 251 [1536/17352 (9%)] Loss: -235580.500000\n",
      "Train Epoch: 251 [2944/17352 (17%)] Loss: -227651.781250\n",
      "Train Epoch: 251 [4352/17352 (25%)] Loss: -219801.296875\n",
      "Train Epoch: 251 [5760/17352 (33%)] Loss: -200286.593750\n",
      "Train Epoch: 251 [7168/17352 (41%)] Loss: -215239.421875\n",
      "Train Epoch: 251 [8576/17352 (49%)] Loss: -218101.390625\n",
      "Train Epoch: 251 [9984/17352 (58%)] Loss: -224181.781250\n",
      "Train Epoch: 251 [11392/17352 (66%)] Loss: -214118.843750\n",
      "Train Epoch: 251 [12800/17352 (74%)] Loss: -222238.750000\n",
      "Train Epoch: 251 [14208/17352 (82%)] Loss: -244898.687500\n",
      "Train Epoch: 251 [15602/17352 (90%)] Loss: -188350.031250\n",
      "Train Epoch: 251 [16359/17352 (94%)] Loss: -147297.218750\n",
      "Train Epoch: 251 [16930/17352 (98%)] Loss: -5071.608398\n",
      "    epoch          : 251\n",
      "    loss           : -204028.62337458055\n",
      "    val_loss       : -111871.48916231791\n",
      "Train Epoch: 252 [128/17352 (1%)] Loss: -232814.281250\n",
      "Train Epoch: 252 [1536/17352 (9%)] Loss: -231340.250000\n",
      "Train Epoch: 252 [2944/17352 (17%)] Loss: -228685.890625\n",
      "Train Epoch: 252 [4352/17352 (25%)] Loss: -217390.718750\n",
      "Train Epoch: 252 [5760/17352 (33%)] Loss: -234086.750000\n",
      "Train Epoch: 252 [7168/17352 (41%)] Loss: -228918.328125\n",
      "Train Epoch: 252 [8576/17352 (49%)] Loss: -224066.093750\n",
      "Train Epoch: 252 [9984/17352 (58%)] Loss: -217511.656250\n",
      "Train Epoch: 252 [11392/17352 (66%)] Loss: -222474.406250\n",
      "Train Epoch: 252 [12800/17352 (74%)] Loss: -215326.203125\n",
      "Train Epoch: 252 [14208/17352 (82%)] Loss: -226258.656250\n",
      "Train Epoch: 252 [15416/17352 (89%)] Loss: -79027.046875\n",
      "Train Epoch: 252 [16181/17352 (93%)] Loss: -179298.000000\n",
      "Train Epoch: 252 [16944/17352 (98%)] Loss: -170293.359375\n",
      "    epoch          : 252\n",
      "    loss           : -205233.85493131293\n",
      "    val_loss       : -110813.82514190674\n",
      "Validation performance didn't improve for 50 epochs. Training stops.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
