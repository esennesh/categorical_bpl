{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eli/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='deepgen_mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch,\n",
    "                        dataset_length=len(data_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 100,\n",
    "    \"cooldown\": 100,\n",
    "    \"factor\": 0.5,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 863.543030\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: -87.482849\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: 31.678562\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: -298.260376\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: -286.645813\n",
      "    epoch          : 1\n",
      "    loss           : -101.67118227363812\n",
      "    val_loss       : -397.88366718671404\n",
      "    val_log_likelihood: 495.1394700720759\n",
      "    val_log_marginal: 479.88702309598204\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: -421.185669\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: -388.619141\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: -49.080067\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: -448.061340\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: -495.716949\n",
      "    epoch          : 2\n",
      "    loss           : -442.9320887197362\n",
      "    val_loss       : -474.81303116515056\n",
      "    val_log_likelihood: 561.3179489362358\n",
      "    val_log_marginal: 544.5813177807976\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: -868.107239\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -454.474731\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -450.819885\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -888.158691\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -504.513550\n",
      "    epoch          : 3\n",
      "    loss           : -501.36920620191216\n",
      "    val_loss       : -523.7290719464459\n",
      "    val_log_likelihood: 573.3827896118164\n",
      "    val_log_marginal: 552.816499823864\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -616.817261\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -451.429077\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -646.213989\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -974.641968\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -476.928284\n",
      "    epoch          : 4\n",
      "    loss           : -531.2848453144036\n",
      "    val_loss       : -526.3976734816143\n",
      "    val_log_likelihood: 570.857123875382\n",
      "    val_log_marginal: 559.5254751355894\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -890.548218\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -409.648499\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -734.469788\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -708.314636\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -579.973755\n",
      "    epoch          : 5\n",
      "    loss           : -562.4352527278485\n",
      "    val_loss       : -571.1508881877278\n",
      "    val_log_likelihood: 620.8749848167495\n",
      "    val_log_marginal: 609.2298035679736\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -964.688904\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -498.517059\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -530.612793\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -531.968384\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -576.763123\n",
      "    epoch          : 6\n",
      "    loss           : -566.8086995795221\n",
      "    val_loss       : -563.935363981172\n",
      "    val_log_likelihood: 610.931605272954\n",
      "    val_log_marginal: 601.0291683115646\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -942.763550\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -513.980103\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -403.876282\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -485.009216\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -667.319092\n",
      "    epoch          : 7\n",
      "    loss           : -579.0006895159731\n",
      "    val_loss       : -587.6412877886196\n",
      "    val_log_likelihood: 641.0265857960918\n",
      "    val_log_marginal: 631.3947891153507\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -1050.736816\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -521.861877\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -421.949341\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -149.281113\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -506.574829\n",
      "    epoch          : 8\n",
      "    loss           : -584.5861071596051\n",
      "    val_loss       : -586.2354511410773\n",
      "    val_log_likelihood: 646.3493419684986\n",
      "    val_log_marginal: 634.0861742723881\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -537.935242\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -635.502808\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -545.711365\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -534.920654\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -698.216797\n",
      "    epoch          : 9\n",
      "    loss           : -594.4381120134108\n",
      "    val_loss       : -610.9619482790978\n",
      "    val_log_likelihood: 683.7141959313119\n",
      "    val_log_marginal: 670.5677782915865\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -1033.902832\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -254.152252\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -479.485413\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -1097.230713\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -756.344849\n",
      "    epoch          : 10\n",
      "    loss           : -617.4872169116936\n",
      "    val_loss       : -620.2233563991504\n",
      "    val_log_likelihood: 693.6021515496886\n",
      "    val_log_marginal: 679.8163968089525\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch10.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -952.129150\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -713.388184\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -744.424561\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -597.491760\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -549.542603\n",
      "    epoch          : 11\n",
      "    loss           : -633.6839989388343\n",
      "    val_loss       : -630.3597527738698\n",
      "    val_log_likelihood: 715.129760137879\n",
      "    val_log_marginal: 702.2573821726924\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -992.807617\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -559.921509\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -597.389099\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -217.016815\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -710.410095\n",
      "    epoch          : 12\n",
      "    loss           : -630.5946165783571\n",
      "    val_loss       : -654.2550327308144\n",
      "    val_log_likelihood: 730.7266552613513\n",
      "    val_log_marginal: 717.3621873864397\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -1027.367798\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -643.977478\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -418.137939\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -501.873840\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -458.117126\n",
      "    epoch          : 13\n",
      "    loss           : -656.7205812057646\n",
      "    val_loss       : -662.363562410061\n",
      "    val_log_likelihood: 730.575248491646\n",
      "    val_log_marginal: 712.09592983951\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -1015.796936\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -537.026367\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -746.385620\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -762.847778\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -721.907959\n",
      "    epoch          : 14\n",
      "    loss           : -655.750857664807\n",
      "    val_loss       : -673.3039200715629\n",
      "    val_log_likelihood: 757.0936236995282\n",
      "    val_log_marginal: 740.0540350035828\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -992.510254\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -502.836731\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -618.455505\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -651.658386\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -592.741211\n",
      "    epoch          : 15\n",
      "    loss           : -663.1276959711962\n",
      "    val_loss       : -641.1037871202933\n",
      "    val_log_likelihood: 718.2392272949219\n",
      "    val_log_marginal: 702.6576613573048\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -1041.356812\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -590.137939\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -677.398804\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -783.166687\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -866.381653\n",
      "    epoch          : 16\n",
      "    loss           : -679.957047717406\n",
      "    val_loss       : -711.5020313411767\n",
      "    val_log_likelihood: 778.6652472467706\n",
      "    val_log_marginal: 762.9420830685793\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -1101.989380\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -886.063538\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -780.846558\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -289.284302\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -699.783569\n",
      "    epoch          : 17\n",
      "    loss           : -713.3851052463644\n",
      "    val_loss       : -700.4568809461365\n",
      "    val_log_likelihood: 818.2439120453183\n",
      "    val_log_marginal: 798.4339921634926\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -1067.659180\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -632.129639\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -874.297241\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -1040.036377\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -662.855042\n",
      "    epoch          : 18\n",
      "    loss           : -729.340850679001\n",
      "    val_loss       : -725.6033602967342\n",
      "    val_log_likelihood: 836.9777378799891\n",
      "    val_log_marginal: 812.7879688793241\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -1035.683960\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -561.968201\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -903.647095\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -537.062988\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -586.537842\n",
      "    epoch          : 19\n",
      "    loss           : -741.2416510251489\n",
      "    val_loss       : -762.1676192955474\n",
      "    val_log_likelihood: 934.4022111042891\n",
      "    val_log_marginal: 915.1316930780367\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -1147.148682\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -944.279663\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -753.692871\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -262.662537\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -731.853638\n",
      "    epoch          : 20\n",
      "    loss           : -776.5750815514291\n",
      "    val_loss       : -787.9608848945394\n",
      "    val_log_likelihood: 963.6907602442374\n",
      "    val_log_marginal: 943.4511860114624\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch20.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -990.137390\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -803.478394\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -733.817871\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -807.892944\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -882.441772\n",
      "    epoch          : 21\n",
      "    loss           : -770.0238872565845\n",
      "    val_loss       : -797.2522921934724\n",
      "    val_log_likelihood: 1014.8625367419554\n",
      "    val_log_marginal: 993.1697077102948\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -1122.452515\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -909.746216\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -904.885376\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -1026.870483\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -994.183105\n",
      "    epoch          : 22\n",
      "    loss           : -790.0465010841294\n",
      "    val_loss       : -794.5207533231281\n",
      "    val_log_likelihood: 991.4957894806814\n",
      "    val_log_marginal: 970.3287308509467\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -1178.605469\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -945.336243\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -509.498993\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -756.785400\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -816.258667\n",
      "    epoch          : 23\n",
      "    loss           : -808.678101190246\n",
      "    val_loss       : -797.4781125937081\n",
      "    val_log_likelihood: 1038.766738136216\n",
      "    val_log_marginal: 1010.4365913456509\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -1063.313477\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -645.053040\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -394.701294\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -1043.089844\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -724.416016\n",
      "    epoch          : 24\n",
      "    loss           : -833.2985898763826\n",
      "    val_loss       : -850.3446229991623\n",
      "    val_log_likelihood: 1115.4615829014542\n",
      "    val_log_marginal: 1082.8846542190317\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -969.220093\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -944.861206\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -396.993988\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -834.774414\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -894.044434\n",
      "    epoch          : 25\n",
      "    loss           : -853.957180514194\n",
      "    val_loss       : -882.263943180534\n",
      "    val_log_likelihood: 1153.483901826462\n",
      "    val_log_marginal: 1112.8170485015737\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -1293.926025\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -593.368958\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -1174.208252\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -490.731964\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -769.033020\n",
      "    epoch          : 26\n",
      "    loss           : -885.4456759915494\n",
      "    val_loss       : -891.7392331549233\n",
      "    val_log_likelihood: 1176.588586184058\n",
      "    val_log_marginal: 1148.5993548673532\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -669.198303\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -800.448059\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -777.425110\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -952.026733\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -920.219849\n",
      "    epoch          : 27\n",
      "    loss           : -895.156987558497\n",
      "    val_loss       : -918.1817524334286\n",
      "    val_log_likelihood: 1269.257978080523\n",
      "    val_log_marginal: 1240.4769414003354\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -1458.781250\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -649.845703\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -765.753845\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -1539.841309\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -1375.219727\n",
      "    epoch          : 28\n",
      "    loss           : -935.2730193185334\n",
      "    val_loss       : -939.5427890883593\n",
      "    val_log_likelihood: 1279.2485127968364\n",
      "    val_log_marginal: 1254.5567192607987\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -1659.228027\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -1094.372559\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -905.764771\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -1420.247314\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -889.296509\n",
      "    epoch          : 29\n",
      "    loss           : -940.3717584893255\n",
      "    val_loss       : -959.319295985744\n",
      "    val_log_likelihood: 1296.4173623264426\n",
      "    val_log_marginal: 1270.2853267323962\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -1074.184570\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -1142.199829\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -925.076965\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -821.640686\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -715.834106\n",
      "    epoch          : 30\n",
      "    loss           : -973.2135153288889\n",
      "    val_loss       : -972.490569386539\n",
      "    val_log_likelihood: 1329.2061051472579\n",
      "    val_log_marginal: 1305.9558908618656\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch30.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -991.071533\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -792.407959\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -1053.403931\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -1039.858765\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -969.664429\n",
      "    epoch          : 31\n",
      "    loss           : -986.8661949233253\n",
      "    val_loss       : -967.4122631206629\n",
      "    val_log_likelihood: 1320.9471205909654\n",
      "    val_log_marginal: 1297.0538829427655\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -1211.786621\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -1174.614014\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -798.884644\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -1345.410889\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -620.466187\n",
      "    epoch          : 32\n",
      "    loss           : -961.2659492114983\n",
      "    val_loss       : -973.6618976483103\n",
      "    val_log_likelihood: 1332.256203226524\n",
      "    val_log_marginal: 1308.1880026983592\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -765.707031\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -767.936707\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -1089.004517\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -1166.595337\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -826.293579\n",
      "    epoch          : 33\n",
      "    loss           : -968.5175453412651\n",
      "    val_loss       : -967.3941046589118\n",
      "    val_log_likelihood: 1345.8278902261563\n",
      "    val_log_marginal: 1321.7325016297953\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -1225.897827\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -968.116333\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -827.535156\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -979.355225\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -972.313904\n",
      "    epoch          : 34\n",
      "    loss           : -969.5113727833965\n",
      "    val_loss       : -953.7132713668381\n",
      "    val_log_likelihood: 1372.2660558719447\n",
      "    val_log_marginal: 1348.9517910919262\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -1754.047974\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -660.847595\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -951.290649\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -276.359375\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -1183.465820\n",
      "    epoch          : 35\n",
      "    loss           : -1015.1560702182279\n",
      "    val_loss       : -987.8980517689101\n",
      "    val_log_likelihood: 1344.9915538825612\n",
      "    val_log_marginal: 1321.2255549048543\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -1453.199829\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -978.505066\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -698.524963\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -1571.069824\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -1312.854370\n",
      "    epoch          : 36\n",
      "    loss           : -957.085497261274\n",
      "    val_loss       : -976.4550164682643\n",
      "    val_log_likelihood: 1295.6070224270961\n",
      "    val_log_marginal: 1274.0894224255226\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -1522.656006\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -803.390259\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -1088.240967\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -960.500122\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -1156.872070\n",
      "    epoch          : 37\n",
      "    loss           : -997.127161913579\n",
      "    val_loss       : -980.8698827569272\n",
      "    val_log_likelihood: 1350.022479671063\n",
      "    val_log_marginal: 1328.6292797635576\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -1554.344482\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -1122.981812\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -943.737976\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -1149.397461\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -953.281677\n",
      "    epoch          : 38\n",
      "    loss           : -982.3634238668008\n",
      "    val_loss       : -975.8761447587668\n",
      "    val_log_likelihood: 1351.5180833268873\n",
      "    val_log_marginal: 1323.9185342889944\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -966.309387\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -974.640015\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -792.175171\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -1210.088379\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -957.952515\n",
      "    epoch          : 39\n",
      "    loss           : -996.5040600465076\n",
      "    val_loss       : -959.6996051326837\n",
      "    val_log_likelihood: 1324.2746787496133\n",
      "    val_log_marginal: 1303.8085876898563\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -1414.887085\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -814.021606\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -1096.149902\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -1107.993896\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -1283.656372\n",
      "    epoch          : 40\n",
      "    loss           : -1006.9057967686417\n",
      "    val_loss       : -1024.5671705122538\n",
      "    val_log_likelihood: 1386.770372447401\n",
      "    val_log_marginal: 1363.0486319454037\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch40.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -1263.123047\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -1308.002319\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -986.751343\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -725.866211\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -1011.312744\n",
      "    epoch          : 41\n",
      "    loss           : -959.735444021697\n",
      "    val_loss       : -1006.4026408118868\n",
      "    val_log_likelihood: 1412.564507512763\n",
      "    val_log_marginal: 1387.7941435401576\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -1254.285889\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -1156.317139\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -1156.088745\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -1284.646729\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -1206.582275\n",
      "    epoch          : 42\n",
      "    loss           : -980.3054141809445\n",
      "    val_loss       : -974.9182473243039\n",
      "    val_log_likelihood: 1278.0183691647974\n",
      "    val_log_marginal: 1247.384255819578\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -1293.854126\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -812.767029\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -763.884949\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -909.866821\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -1000.213562\n",
      "    epoch          : 43\n",
      "    loss           : -956.8943421014465\n",
      "    val_loss       : -985.3641265998087\n",
      "    val_log_likelihood: 1364.9437612401377\n",
      "    val_log_marginal: 1335.5489747054178\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -1552.611816\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -1298.018311\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -1094.024292\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -1351.322998\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -809.114624\n",
      "    epoch          : 44\n",
      "    loss           : -990.875345362295\n",
      "    val_loss       : -987.5136969336078\n",
      "    val_log_likelihood: 1383.0142937084236\n",
      "    val_log_marginal: 1355.8370370440148\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -1420.017212\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -1132.287109\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -1158.653564\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -1105.544922\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -1112.664307\n",
      "    epoch          : 45\n",
      "    loss           : -1008.5105892974551\n",
      "    val_loss       : -1015.619005342761\n",
      "    val_log_likelihood: 1368.0921794022663\n",
      "    val_log_marginal: 1341.3411465337924\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -1291.761475\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -728.148987\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -786.862427\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -1404.287354\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -1046.319092\n",
      "    epoch          : 46\n",
      "    loss           : -998.215483712678\n",
      "    val_loss       : -983.5425376588349\n",
      "    val_log_likelihood: 1340.8486856894917\n",
      "    val_log_marginal: 1309.142289766009\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -1579.693848\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -815.254761\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -823.123291\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -1146.895508\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -649.871460\n",
      "    epoch          : 47\n",
      "    loss           : -962.8862990577622\n",
      "    val_loss       : -991.9881239194644\n",
      "    val_log_likelihood: 1342.9645851059715\n",
      "    val_log_marginal: 1305.5009156910905\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -550.871033\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -1158.062988\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -1155.132812\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -1209.344360\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -1010.456604\n",
      "    epoch          : 48\n",
      "    loss           : -972.8120652000503\n",
      "    val_loss       : -968.4069880166155\n",
      "    val_log_likelihood: 1333.4976936566948\n",
      "    val_log_marginal: 1304.1965608033397\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -1075.084106\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -1308.343262\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -549.479431\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -1414.173340\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -1246.773071\n",
      "    epoch          : 49\n",
      "    loss           : -992.857726748627\n",
      "    val_loss       : -985.438405136338\n",
      "    val_log_likelihood: 1381.6472597027769\n",
      "    val_log_marginal: 1350.2920340305322\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -1402.540283\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -1190.387207\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -586.619019\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -1108.995239\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -829.726501\n",
      "    epoch          : 50\n",
      "    loss           : -968.5391452902614\n",
      "    val_loss       : -985.1014411614212\n",
      "    val_log_likelihood: 1351.2406830740447\n",
      "    val_log_marginal: 1315.8574953205084\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -1612.222412\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -997.973999\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -1040.188232\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -1229.495239\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -1080.870850\n",
      "    epoch          : 51\n",
      "    loss           : -987.2268178203318\n",
      "    val_loss       : -970.7559318188854\n",
      "    val_log_likelihood: 1360.5459715588258\n",
      "    val_log_marginal: 1331.3860829471198\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -1269.732666\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -742.549622\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -708.719116\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -922.536011\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -1422.239502\n",
      "    epoch          : 52\n",
      "    loss           : -1012.5549183458385\n",
      "    val_loss       : -1000.7179796762285\n",
      "    val_log_likelihood: 1359.358938387125\n",
      "    val_log_marginal: 1332.6046946480408\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -1055.936035\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -994.624817\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -1231.140625\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -789.381226\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -1242.481201\n",
      "    epoch          : 53\n",
      "    loss           : -1000.5049100063815\n",
      "    val_loss       : -1028.9389037399342\n",
      "    val_log_likelihood: 1347.7192292166228\n",
      "    val_log_marginal: 1320.3278656864618\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -1419.227417\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -620.391113\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -1036.683472\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -994.992920\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -749.571045\n",
      "    epoch          : 54\n",
      "    loss           : -1015.1140360312887\n",
      "    val_loss       : -1037.5605682121443\n",
      "    val_log_likelihood: 1390.7715327196781\n",
      "    val_log_marginal: 1364.142088427355\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -1391.936768\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -721.003296\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -1104.036011\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -1573.577637\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -1411.921143\n",
      "    epoch          : 55\n",
      "    loss           : -1045.8547088320893\n",
      "    val_loss       : -996.5023438363327\n",
      "    val_log_likelihood: 1403.0968192827584\n",
      "    val_log_marginal: 1373.6754579246597\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -1403.558350\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -1193.704590\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -1043.822510\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -1139.959473\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -966.514038\n",
      "    epoch          : 56\n",
      "    loss           : -1025.5658360849513\n",
      "    val_loss       : -1018.5444878673476\n",
      "    val_log_likelihood: 1388.1067507904354\n",
      "    val_log_marginal: 1361.61330980234\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -1307.775024\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -1045.411011\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -1109.302612\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -871.426514\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -974.273682\n",
      "    epoch          : 57\n",
      "    loss           : -949.5563970886835\n",
      "    val_loss       : -980.4828503944883\n",
      "    val_log_likelihood: 1362.6065368652344\n",
      "    val_log_marginal: 1336.0911301615997\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -1293.867676\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -1195.363525\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -747.666199\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -735.517273\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -793.378906\n",
      "    epoch          : 58\n",
      "    loss           : -1041.0382599783416\n",
      "    val_loss       : -979.3025200499665\n",
      "    val_log_likelihood: 1376.7137919510944\n",
      "    val_log_marginal: 1350.6578231924352\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -1429.122437\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -1022.321045\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -778.468872\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -716.736328\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -921.350464\n",
      "    epoch          : 59\n",
      "    loss           : -1019.8015236429649\n",
      "    val_loss       : -1024.2212973155247\n",
      "    val_log_likelihood: 1433.4220297029703\n",
      "    val_log_marginal: 1406.6676330854286\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -1430.600220\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -1084.019775\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -1298.339233\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -1192.733398\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -1040.459106\n",
      "    epoch          : 60\n",
      "    loss           : -1071.7191488435953\n",
      "    val_loss       : -1042.3174703108184\n",
      "    val_log_likelihood: 1433.3570420671217\n",
      "    val_log_marginal: 1406.3079358009381\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch60.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -1440.226685\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -1223.851685\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -1123.538452\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -779.490234\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -642.287964\n",
      "    epoch          : 61\n",
      "    loss           : -1035.1140221321937\n",
      "    val_loss       : -1052.5793270519612\n",
      "    val_log_likelihood: 1379.288132771407\n",
      "    val_log_marginal: 1355.0261076670313\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -1605.750488\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -903.450500\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -1143.409668\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -1447.236694\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -997.335999\n",
      "    epoch          : 62\n",
      "    loss           : -1044.965528129351\n",
      "    val_loss       : -1040.812942443517\n",
      "    val_log_likelihood: 1411.1641996780245\n",
      "    val_log_marginal: 1384.9189858712075\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -1275.164429\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -839.265564\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -937.853638\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -1121.185181\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -958.199280\n",
      "    epoch          : 63\n",
      "    loss           : -1017.4711654209855\n",
      "    val_loss       : -1012.5732634300828\n",
      "    val_log_likelihood: 1411.2282587938969\n",
      "    val_log_marginal: 1384.8931343285274\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -989.193481\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -1228.459229\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -1099.234619\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -949.881104\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -1007.740540\n",
      "    epoch          : 64\n",
      "    loss           : -1047.558882307298\n",
      "    val_loss       : -1017.1716506119061\n",
      "    val_log_likelihood: 1382.463407913057\n",
      "    val_log_marginal: 1352.672417193522\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -1359.858398\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -604.551331\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -1355.766846\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -687.859375\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -691.994995\n",
      "    epoch          : 65\n",
      "    loss           : -1038.0822962392674\n",
      "    val_loss       : -1027.9534899758378\n",
      "    val_log_likelihood: 1419.3896164091507\n",
      "    val_log_marginal: 1391.7465694827204\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -1075.988525\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -1165.571167\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -1128.541016\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -767.979553\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -885.438354\n",
      "    epoch          : 66\n",
      "    loss           : -1007.8194646552057\n",
      "    val_loss       : -1014.1535725441973\n",
      "    val_log_likelihood: 1388.248622478825\n",
      "    val_log_marginal: 1362.4005132023415\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -1177.766479\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -578.499329\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -1174.089844\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -656.625793\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -771.391235\n",
      "    epoch          : 67\n",
      "    loss           : -1047.9486815197633\n",
      "    val_loss       : -1042.6381930799528\n",
      "    val_log_likelihood: 1432.8237195911975\n",
      "    val_log_marginal: 1404.961270470117\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -1383.357788\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -1109.717285\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -1154.152954\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -1256.147217\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -781.988281\n",
      "    epoch          : 68\n",
      "    loss           : -1062.7513376368154\n",
      "    val_loss       : -1034.5687373975882\n",
      "    val_log_likelihood: 1382.936042710106\n",
      "    val_log_marginal: 1357.711842611635\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -1446.476196\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -1386.849487\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -456.996399\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -1166.584351\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -1150.724609\n",
      "    epoch          : 69\n",
      "    loss           : -1038.415945827371\n",
      "    val_loss       : -1039.226246336982\n",
      "    val_log_likelihood: 1413.1674593179532\n",
      "    val_log_marginal: 1383.1695225901146\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -1410.127441\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -1097.364990\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -883.110413\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -1389.123291\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -1039.147949\n",
      "    epoch          : 70\n",
      "    loss           : -1058.73368427541\n",
      "    val_loss       : -1044.9176515184324\n",
      "    val_log_likelihood: 1434.4646715220838\n",
      "    val_log_marginal: 1402.2206368673003\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch70.pth ...\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -1315.828613\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -948.792725\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -581.625427\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -1120.715332\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -682.521240\n",
      "    epoch          : 71\n",
      "    loss           : -1066.8940629109297\n",
      "    val_loss       : -1039.6891643091537\n",
      "    val_log_likelihood: 1439.4497191174196\n",
      "    val_log_marginal: 1410.0374476043053\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -1276.043457\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -1271.868164\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -848.215271\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -996.649719\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -1329.979736\n",
      "    epoch          : 72\n",
      "    loss           : -1061.488018375812\n",
      "    val_loss       : -1069.6813950452326\n",
      "    val_log_likelihood: 1451.7668753142405\n",
      "    val_log_marginal: 1423.4374774544879\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -1368.074951\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -910.484009\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -1202.402588\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -841.506470\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -1254.493774\n",
      "    epoch          : 73\n",
      "    loss           : -1049.8120050713567\n",
      "    val_loss       : -1094.799818601533\n",
      "    val_log_likelihood: 1433.6930517819849\n",
      "    val_log_marginal: 1401.8541908739958\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -1273.415405\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -860.382324\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -1109.386353\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -1110.246338\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -693.086670\n",
      "    epoch          : 74\n",
      "    loss           : -1083.1656294718828\n",
      "    val_loss       : -1048.6688238768425\n",
      "    val_log_likelihood: 1439.9388853771852\n",
      "    val_log_marginal: 1410.0341899412417\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -1093.007935\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -943.498657\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -916.721191\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -475.546265\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -884.270325\n",
      "    epoch          : 75\n",
      "    loss           : -1051.8810660484994\n",
      "    val_loss       : -1088.0466803089246\n",
      "    val_log_likelihood: 1472.2535539381574\n",
      "    val_log_marginal: 1441.3601574846314\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -1237.307617\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -1162.349854\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -1112.961548\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -443.295135\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -1240.269287\n",
      "    epoch          : 76\n",
      "    loss           : -1097.9103952540029\n",
      "    val_loss       : -1075.3577600845624\n",
      "    val_log_likelihood: 1436.7865736555345\n",
      "    val_log_marginal: 1405.500584428171\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -1150.542603\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -878.104248\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -1109.273560\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -1082.488770\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -830.500977\n",
      "    epoch          : 77\n",
      "    loss           : -1070.0548748356282\n",
      "    val_loss       : -1066.2543388238214\n",
      "    val_log_likelihood: 1460.1667559028851\n",
      "    val_log_marginal: 1429.7363818024874\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -1259.364258\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -1024.643677\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -1107.883667\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -1488.306641\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -930.289001\n",
      "    epoch          : 78\n",
      "    loss           : -1083.582856131072\n",
      "    val_loss       : -1076.2787676502646\n",
      "    val_log_likelihood: 1437.6174969059407\n",
      "    val_log_marginal: 1408.0919964324019\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -1607.084595\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -1234.132568\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -994.910156\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -1039.980103\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -1309.656250\n",
      "    epoch          : 79\n",
      "    loss           : -1107.9762603457611\n",
      "    val_loss       : -1097.9593082713793\n",
      "    val_log_likelihood: 1441.370077907449\n",
      "    val_log_marginal: 1409.0794772611803\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -748.427002\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -1232.005615\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -1158.479370\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -1157.386353\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -1330.335815\n",
      "    epoch          : 80\n",
      "    loss           : -1089.4397444016863\n",
      "    val_loss       : -1081.4472987123277\n",
      "    val_log_likelihood: 1425.717798818456\n",
      "    val_log_marginal: 1397.7449260958756\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch80.pth ...\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -1074.643921\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -1042.613525\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -813.599731\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -389.403351\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -916.308350\n",
      "    epoch          : 81\n",
      "    loss           : -1092.9784198421062\n",
      "    val_loss       : -1117.7722074833725\n",
      "    val_log_likelihood: 1469.2706996804416\n",
      "    val_log_marginal: 1440.465960451743\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -1706.331665\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -981.917603\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -1026.334473\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -1239.745361\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -987.175659\n",
      "    epoch          : 82\n",
      "    loss           : -1118.031848265393\n",
      "    val_loss       : -1128.4689224855606\n",
      "    val_log_likelihood: 1485.4856561339727\n",
      "    val_log_marginal: 1453.1393479254687\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -1432.801758\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -1267.936768\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -994.151917\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -1083.148804\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -878.256348\n",
      "    epoch          : 83\n",
      "    loss           : -1110.6153383160581\n",
      "    val_loss       : -1086.500224230304\n",
      "    val_log_likelihood: 1474.4220351417466\n",
      "    val_log_marginal: 1442.2629085423075\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -1183.011719\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -1353.745605\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -969.620667\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -969.291870\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -1014.636963\n",
      "    epoch          : 84\n",
      "    loss           : -1100.783676298538\n",
      "    val_loss       : -1126.7730274354708\n",
      "    val_log_likelihood: 1472.9209105236696\n",
      "    val_log_marginal: 1442.0703777823705\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -1020.663147\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -920.018066\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -712.517822\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -865.877686\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -1247.606445\n",
      "    epoch          : 85\n",
      "    loss           : -1111.0545460918163\n",
      "    val_loss       : -1141.11445499301\n",
      "    val_log_likelihood: 1503.2945949441134\n",
      "    val_log_marginal: 1472.9760352984108\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -1576.322510\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -976.673950\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -725.763428\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -926.980835\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -1168.728638\n",
      "    epoch          : 86\n",
      "    loss           : -1132.2475869962484\n",
      "    val_loss       : -1156.258152755254\n",
      "    val_log_likelihood: 1472.069576452274\n",
      "    val_log_marginal: 1442.460795748734\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -1709.275024\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -909.158508\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -1128.226074\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -1402.107178\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -910.490845\n",
      "    epoch          : 87\n",
      "    loss           : -1131.9789664391244\n",
      "    val_loss       : -1134.2404460481341\n",
      "    val_log_likelihood: 1509.9391521793782\n",
      "    val_log_marginal: 1478.643993589931\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -1653.170532\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -1406.480469\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -894.232056\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -658.755066\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -1256.880615\n",
      "    epoch          : 88\n",
      "    loss           : -1135.5487924707998\n",
      "    val_loss       : -1121.6416262580856\n",
      "    val_log_likelihood: 1473.1495041044632\n",
      "    val_log_marginal: 1442.2976765868873\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -1138.169922\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -1297.494995\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -704.332153\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -1223.879028\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -1203.919189\n",
      "    epoch          : 89\n",
      "    loss           : -1127.746449083385\n",
      "    val_loss       : -1124.4764062066258\n",
      "    val_log_likelihood: 1483.624570940981\n",
      "    val_log_marginal: 1449.8613681489012\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -1146.025391\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -1160.915039\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -1270.033447\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -1295.197510\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -1211.906250\n",
      "    epoch          : 90\n",
      "    loss           : -1169.685468314898\n",
      "    val_loss       : -1170.7889165592537\n",
      "    val_log_likelihood: 1464.1859620349242\n",
      "    val_log_marginal: 1431.3371415034046\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch90.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -1372.052246\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -1151.480957\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -976.368103\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -1156.595215\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -1052.261963\n",
      "    epoch          : 91\n",
      "    loss           : -1118.3557491491338\n",
      "    val_loss       : -1168.1461560409423\n",
      "    val_log_likelihood: 1490.5945893844755\n",
      "    val_log_marginal: 1457.3014571036265\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -1656.529785\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -949.561279\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -1214.350830\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -1186.287476\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -987.895569\n",
      "    epoch          : 92\n",
      "    loss           : -1151.814789724822\n",
      "    val_loss       : -1161.211940354843\n",
      "    val_log_likelihood: 1485.6533523408493\n",
      "    val_log_marginal: 1453.8789720531406\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -1559.156250\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -1182.085327\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -1266.128906\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -617.503174\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -1181.234497\n",
      "    epoch          : 93\n",
      "    loss           : -1161.697659029819\n",
      "    val_loss       : -1158.0834533601428\n",
      "    val_log_likelihood: 1477.727956639658\n",
      "    val_log_marginal: 1444.9566498975464\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -1719.316528\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -983.289490\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -1052.324829\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -1171.353516\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -1068.525391\n",
      "    epoch          : 94\n",
      "    loss           : -1198.5451835405709\n",
      "    val_loss       : -1178.3491027356392\n",
      "    val_log_likelihood: 1514.3163440062267\n",
      "    val_log_marginal: 1480.0297844836875\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -1683.503784\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -1205.939941\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -1075.914429\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -1302.889404\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -811.931641\n",
      "    epoch          : 95\n",
      "    loss           : -1196.6049738213567\n",
      "    val_loss       : -1201.7881543008739\n",
      "    val_log_likelihood: 1470.7861539632968\n",
      "    val_log_marginal: 1437.958777018832\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -1529.154541\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -1478.811523\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -1196.463501\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -971.482910\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -1144.050293\n",
      "    epoch          : 96\n",
      "    loss           : -1218.4228140953744\n",
      "    val_loss       : -1180.5500362249147\n",
      "    val_log_likelihood: 1415.501230976369\n",
      "    val_log_marginal: 1378.2790675478443\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -1422.560791\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -1315.580566\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -1057.975586\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -1265.530029\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -1088.544678\n",
      "    epoch          : 97\n",
      "    loss           : -1190.296649592938\n",
      "    val_loss       : -1199.1870005595138\n",
      "    val_log_likelihood: 1490.9809074779548\n",
      "    val_log_marginal: 1454.4121637722324\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -1062.401001\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -1164.267578\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -1206.364624\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -819.988892\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -1163.188232\n",
      "    epoch          : 98\n",
      "    loss           : -1166.7541008373298\n",
      "    val_loss       : -1194.9542538474434\n",
      "    val_log_likelihood: 1511.4999516553219\n",
      "    val_log_marginal: 1473.1958101023183\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -1473.516479\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -1163.821533\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -1260.376953\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -1158.916260\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -1241.853638\n",
      "    epoch          : 99\n",
      "    loss           : -1216.2123123017868\n",
      "    val_loss       : -1240.9004743879957\n",
      "    val_log_likelihood: 1496.7897804184715\n",
      "    val_log_marginal: 1463.091587759162\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -1455.662231\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -1065.442749\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -1192.333130\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -1279.922119\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -1300.301025\n",
      "    epoch          : 100\n",
      "    loss           : -1228.726239799273\n",
      "    val_loss       : -1240.023702004574\n",
      "    val_log_likelihood: 1495.7925300220452\n",
      "    val_log_marginal: 1462.9214644088124\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -820.009766\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -1351.542847\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -1301.069092\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -1506.280029\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -1380.236328\n",
      "    epoch          : 101\n",
      "    loss           : -1261.4832195621907\n",
      "    val_loss       : -1265.1526913773453\n",
      "    val_log_likelihood: 1524.530272228883\n",
      "    val_log_marginal: 1490.4138497430463\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -1682.707153\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -1170.740479\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -1343.913574\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -1304.752319\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -1221.332764\n",
      "    epoch          : 102\n",
      "    loss           : -1279.7594012269878\n",
      "    val_loss       : -1270.2368519484148\n",
      "    val_log_likelihood: 1518.2087650110225\n",
      "    val_log_marginal: 1483.7502617666948\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -1482.618408\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -1336.240234\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -1196.047363\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -1100.918335\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -1326.371094\n",
      "    epoch          : 103\n",
      "    loss           : -1284.3623089176594\n",
      "    val_loss       : -1267.3270395872007\n",
      "    val_log_likelihood: 1502.2183602210318\n",
      "    val_log_marginal: 1469.6302670187474\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -1480.829346\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -1242.999878\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -1258.418335\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -969.533447\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -1055.161621\n",
      "    epoch          : 104\n",
      "    loss           : -1264.657374618077\n",
      "    val_loss       : -1277.253526787292\n",
      "    val_log_likelihood: 1510.9560099686726\n",
      "    val_log_marginal: 1476.2607705214493\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -1619.403687\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -1255.632446\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -1336.938110\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -1107.640625\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -1246.612793\n",
      "    epoch          : 105\n",
      "    loss           : -1294.8056042359608\n",
      "    val_loss       : -1311.7728192605687\n",
      "    val_log_likelihood: 1530.011796101485\n",
      "    val_log_marginal: 1495.771222451628\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -1444.167480\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -1075.675537\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -1310.794434\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -1261.802246\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -1376.468018\n",
      "    epoch          : 106\n",
      "    loss           : -1299.925784875851\n",
      "    val_loss       : -1301.791442342084\n",
      "    val_log_likelihood: 1524.6941383286278\n",
      "    val_log_marginal: 1490.9933915810607\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -1668.052002\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -1329.001465\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -1260.155762\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -1355.346680\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -1268.093994\n",
      "    epoch          : 107\n",
      "    loss           : -1298.0792846679688\n",
      "    val_loss       : -1303.3053336068895\n",
      "    val_log_likelihood: 1511.2191887279548\n",
      "    val_log_marginal: 1474.3254607027525\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -1557.682007\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -1300.292847\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -1165.440796\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -1234.583008\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -1419.626709\n",
      "    epoch          : 108\n",
      "    loss           : -1311.1053478883045\n",
      "    val_loss       : -1306.844347667945\n",
      "    val_log_likelihood: 1521.441813553914\n",
      "    val_log_marginal: 1487.377454917862\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -1471.245117\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -1047.881104\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -1206.155640\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -1356.322754\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -1306.250854\n",
      "    epoch          : 109\n",
      "    loss           : -1296.349476427135\n",
      "    val_loss       : -1312.100772568117\n",
      "    val_log_likelihood: 1532.089759146813\n",
      "    val_log_marginal: 1497.336860885508\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -1605.000000\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -1154.605713\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -1176.022827\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -1317.974243\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -1386.688477\n",
      "    epoch          : 110\n",
      "    loss           : -1308.8181236946937\n",
      "    val_loss       : -1323.9870708888157\n",
      "    val_log_likelihood: 1498.6669825185643\n",
      "    val_log_marginal: 1467.8004399710105\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch110.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -1754.701904\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -1286.405640\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -1417.812500\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -1575.611816\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -1259.181763\n",
      "    epoch          : 111\n",
      "    loss           : -1317.9517417378945\n",
      "    val_loss       : -1320.8593487544354\n",
      "    val_log_likelihood: 1534.396021474706\n",
      "    val_log_marginal: 1501.843416148686\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -1635.505249\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -1235.967529\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -1479.513062\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -1263.355713\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -1297.368896\n",
      "    epoch          : 112\n",
      "    loss           : -1331.172577206451\n",
      "    val_loss       : -1330.3644260803312\n",
      "    val_log_likelihood: 1530.1035724299968\n",
      "    val_log_marginal: 1497.0847223175556\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -1627.572266\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -1182.020752\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -1376.539673\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -1287.694824\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -1366.131958\n",
      "    epoch          : 113\n",
      "    loss           : -1304.1671638111077\n",
      "    val_loss       : -1323.7090330795745\n",
      "    val_log_likelihood: 1525.4854567121752\n",
      "    val_log_marginal: 1486.2315303676457\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -1602.822998\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -1200.165283\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -1354.468262\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -1188.273926\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -1291.485718\n",
      "    epoch          : 114\n",
      "    loss           : -1332.0838496142094\n",
      "    val_loss       : -1321.8588155703712\n",
      "    val_log_likelihood: 1535.889810392172\n",
      "    val_log_marginal: 1498.134446001787\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -1682.273438\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -1191.675903\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -937.879761\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -1658.886353\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -1329.937256\n",
      "    epoch          : 115\n",
      "    loss           : -1321.0957460309019\n",
      "    val_loss       : -1318.3256704884645\n",
      "    val_log_likelihood: 1519.9740292388615\n",
      "    val_log_marginal: 1484.6739564576546\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -1660.181885\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -1455.972656\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -1206.896973\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -1585.249756\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -1310.978149\n",
      "    epoch          : 116\n",
      "    loss           : -1345.0175442837253\n",
      "    val_loss       : -1336.454649413908\n",
      "    val_log_likelihood: 1523.745874990331\n",
      "    val_log_marginal: 1490.707026193988\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -1588.383057\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -1508.965088\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -1221.122681\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -1358.614014\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -1425.870239\n",
      "    epoch          : 117\n",
      "    loss           : -1333.9358937480663\n",
      "    val_loss       : -1337.0420468724656\n",
      "    val_log_likelihood: 1523.3024273862934\n",
      "    val_log_marginal: 1491.2603915429092\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -1686.138672\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -1236.422852\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -1223.216064\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -1362.677490\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -1435.821289\n",
      "    epoch          : 118\n",
      "    loss           : -1344.5938799263226\n",
      "    val_loss       : -1348.875259713113\n",
      "    val_log_likelihood: 1548.9777771600402\n",
      "    val_log_marginal: 1515.0523843839949\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -1763.596436\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -1245.635376\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -1404.183105\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -808.915344\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -1436.944702\n",
      "    epoch          : 119\n",
      "    loss           : -1363.48454964515\n",
      "    val_loss       : -1350.295284540457\n",
      "    val_log_likelihood: 1532.2539388826578\n",
      "    val_log_marginal: 1497.808927005537\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -1836.562622\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -1452.345215\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -1231.038818\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -1200.361572\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -1412.458130\n",
      "    epoch          : 120\n",
      "    loss           : -1351.2083208442914\n",
      "    val_loss       : -1348.7011942662907\n",
      "    val_log_likelihood: 1535.2741372892172\n",
      "    val_log_marginal: 1502.216688769307\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch120.pth ...\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -1567.396973\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -1119.886841\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -1219.878906\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -1236.058838\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -1303.092285\n",
      "    epoch          : 121\n",
      "    loss           : -1354.1844675800587\n",
      "    val_loss       : -1367.3649668697046\n",
      "    val_log_likelihood: 1515.2791325030942\n",
      "    val_log_marginal: 1483.7601116268831\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -1605.034668\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -1368.462524\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -1357.885254\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -1490.797852\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -1351.421875\n",
      "    epoch          : 122\n",
      "    loss           : -1365.1625505201887\n",
      "    val_loss       : -1350.1291315103097\n",
      "    val_log_likelihood: 1510.3706211807705\n",
      "    val_log_marginal: 1479.8347034539456\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -1237.872437\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -1303.274780\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -1145.485229\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -1649.235596\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -1307.332520\n",
      "    epoch          : 123\n",
      "    loss           : -1360.3093310063427\n",
      "    val_loss       : -1349.478598633486\n",
      "    val_log_likelihood: 1527.8747764058633\n",
      "    val_log_marginal: 1492.800516133422\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -1679.289551\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -1226.779785\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -1142.101685\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -1096.044678\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -1391.316406\n",
      "    epoch          : 124\n",
      "    loss           : -1367.0101632599783\n",
      "    val_loss       : -1372.3567074389189\n",
      "    val_log_likelihood: 1529.9047114306156\n",
      "    val_log_marginal: 1495.5881533099541\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -1340.588623\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -1421.500244\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -1332.693359\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -1286.146118\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -1287.374634\n",
      "    epoch          : 125\n",
      "    loss           : -1369.3492751924118\n",
      "    val_loss       : -1355.73306868691\n",
      "    val_log_likelihood: 1539.4057242516244\n",
      "    val_log_marginal: 1503.7014059693838\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -1661.128418\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -1234.622314\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -1325.075562\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -1464.575684\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -1315.809082\n",
      "    epoch          : 126\n",
      "    loss           : -1352.4931114876624\n",
      "    val_loss       : -1369.2450630533517\n",
      "    val_log_likelihood: 1532.3379486386139\n",
      "    val_log_marginal: 1496.4642924905602\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -1691.415283\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -1156.604004\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -1219.539307\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -1418.366455\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -1368.569824\n",
      "    epoch          : 127\n",
      "    loss           : -1366.5602833965038\n",
      "    val_loss       : -1363.806886703984\n",
      "    val_log_likelihood: 1534.6984802850402\n",
      "    val_log_marginal: 1499.2024055411946\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -1773.245972\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -1215.179810\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -1392.841431\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -1603.058838\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -1022.145264\n",
      "    epoch          : 128\n",
      "    loss           : -1369.2761073348545\n",
      "    val_loss       : -1366.7740532727912\n",
      "    val_log_likelihood: 1535.210804552135\n",
      "    val_log_marginal: 1496.0923252167863\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -1364.619263\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -1276.986450\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -1231.007324\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -1239.280518\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -1484.505859\n",
      "    epoch          : 129\n",
      "    loss           : -1356.4306133005878\n",
      "    val_loss       : -1365.8055084847504\n",
      "    val_log_likelihood: 1548.04454478651\n",
      "    val_log_marginal: 1511.8983592537063\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -1337.960205\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -1292.183594\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -1258.581787\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -1240.611328\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -1321.462280\n",
      "    epoch          : 130\n",
      "    loss           : -1356.0390038820776\n",
      "    val_loss       : -1383.2854561250529\n",
      "    val_log_likelihood: 1536.4463519105816\n",
      "    val_log_marginal: 1500.0864476985532\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch130.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -1590.560059\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -1500.091309\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -1508.929077\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -1726.936523\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -1162.546509\n",
      "    epoch          : 131\n",
      "    loss           : -1372.5385899307705\n",
      "    val_loss       : -1370.0255109660309\n",
      "    val_log_likelihood: 1560.4578917852723\n",
      "    val_log_marginal: 1523.7388487025394\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -1669.533936\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -1270.534302\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -1026.434448\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -1593.524414\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -1317.426147\n",
      "    epoch          : 132\n",
      "    loss           : -1370.0478237643101\n",
      "    val_loss       : -1344.2555006168745\n",
      "    val_log_likelihood: 1476.4168169380414\n",
      "    val_log_marginal: 1440.3737335140347\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -1265.258911\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -1269.590454\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -1423.115479\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -1479.900757\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -1294.240967\n",
      "    epoch          : 133\n",
      "    loss           : -1340.7705223159035\n",
      "    val_loss       : -1360.2866542205713\n",
      "    val_log_likelihood: 1479.5988406946162\n",
      "    val_log_marginal: 1445.0960671969754\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -1760.138672\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -1435.112793\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -1291.145264\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -1399.322388\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -1382.066650\n",
      "    epoch          : 134\n",
      "    loss           : -1363.9588647219214\n",
      "    val_loss       : -1357.245889703688\n",
      "    val_log_likelihood: 1528.674989605894\n",
      "    val_log_marginal: 1493.108488923197\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -1625.791748\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -1205.057129\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -1449.837646\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -1274.985229\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -1358.179688\n",
      "    epoch          : 135\n",
      "    loss           : -1363.9784576113861\n",
      "    val_loss       : -1365.005629554769\n",
      "    val_log_likelihood: 1526.8146042021194\n",
      "    val_log_marginal: 1493.622310090312\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -1340.414307\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -1480.978271\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -1043.113159\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -1344.391357\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -1352.364624\n",
      "    epoch          : 136\n",
      "    loss           : -1360.7159864973314\n",
      "    val_loss       : -1364.4091827767247\n",
      "    val_log_likelihood: 1523.146243860226\n",
      "    val_log_marginal: 1486.3715060419854\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -1672.007812\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -1532.167358\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -1508.735107\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -1267.417480\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -1408.853516\n",
      "    epoch          : 137\n",
      "    loss           : -1383.9835434715346\n",
      "    val_loss       : -1361.9186770166316\n",
      "    val_log_likelihood: 1513.9825270246752\n",
      "    val_log_marginal: 1479.456999066461\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -1690.175293\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -1338.990723\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -1184.417236\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -1353.104980\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -1419.132690\n",
      "    epoch          : 138\n",
      "    loss           : -1370.7438227587406\n",
      "    val_loss       : -1383.1774332359396\n",
      "    val_log_likelihood: 1535.7066952544865\n",
      "    val_log_marginal: 1501.1058842085192\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -1382.588379\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -1319.483398\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -1461.483643\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -1387.864258\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -1322.906860\n",
      "    epoch          : 139\n",
      "    loss           : -1381.7860125551128\n",
      "    val_loss       : -1380.7421834742797\n",
      "    val_log_likelihood: 1532.223306485922\n",
      "    val_log_marginal: 1497.319395077055\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -1727.751343\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -1373.064575\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -1409.169922\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -1461.189819\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -1253.461670\n",
      "    epoch          : 140\n",
      "    loss           : -1371.486735428914\n",
      "    val_loss       : -1377.2800220164129\n",
      "    val_log_likelihood: 1538.3958123839727\n",
      "    val_log_marginal: 1500.286095154619\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch140.pth ...\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -1708.233765\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -1392.500366\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -1137.647583\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -1529.133545\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -1351.495972\n",
      "    epoch          : 141\n",
      "    loss           : -1384.8488358601485\n",
      "    val_loss       : -1381.5031451907396\n",
      "    val_log_likelihood: 1528.001357276841\n",
      "    val_log_marginal: 1493.4305287935392\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -1267.362305\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -1301.455566\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -1290.456055\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -1703.158691\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -1371.985352\n",
      "    epoch          : 142\n",
      "    loss           : -1396.3156955832303\n",
      "    val_loss       : -1389.191558040411\n",
      "    val_log_likelihood: 1551.9057206257735\n",
      "    val_log_marginal: 1517.215815906459\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -1752.379761\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -1273.773560\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -1286.729980\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -1326.482422\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -1508.935059\n",
      "    epoch          : 143\n",
      "    loss           : -1392.6144554213722\n",
      "    val_loss       : -1397.208799917712\n",
      "    val_log_likelihood: 1539.6418553720607\n",
      "    val_log_marginal: 1507.5477291469483\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -1602.851074\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -1248.729492\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -1368.003418\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -1458.219360\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -1136.186523\n",
      "    epoch          : 144\n",
      "    loss           : -1398.0798218982054\n",
      "    val_loss       : -1395.267150313999\n",
      "    val_log_likelihood: 1567.8924318823483\n",
      "    val_log_marginal: 1533.7415931493153\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -1639.275391\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -1433.059814\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -1257.114746\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -1804.030884\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -1345.386475\n",
      "    epoch          : 145\n",
      "    loss           : -1414.4796964437655\n",
      "    val_loss       : -1398.526508076596\n",
      "    val_log_likelihood: 1570.0185196376083\n",
      "    val_log_marginal: 1534.9649240866413\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -1848.075928\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -1229.602905\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -1203.174805\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -1654.461182\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -1317.309814\n",
      "    epoch          : 146\n",
      "    loss           : -1393.1572072246288\n",
      "    val_loss       : -1394.5647917964984\n",
      "    val_log_likelihood: 1549.4417905901919\n",
      "    val_log_marginal: 1514.6252213588375\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -1415.235352\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -1381.371460\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -1242.572998\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -1292.617432\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -1384.735107\n",
      "    epoch          : 147\n",
      "    loss           : -1398.5952063834313\n",
      "    val_loss       : -1383.838006795799\n",
      "    val_log_likelihood: 1547.4447613706684\n",
      "    val_log_marginal: 1513.0044956797974\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -1296.544922\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -1423.200439\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -1200.001099\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -1330.479736\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -1304.458252\n",
      "    epoch          : 148\n",
      "    loss           : -1395.6199709448483\n",
      "    val_loss       : -1399.7671384082296\n",
      "    val_log_likelihood: 1571.613089079904\n",
      "    val_log_marginal: 1535.5260702241956\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -1806.742432\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -1324.241211\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -1239.301514\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -1745.828247\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -1254.506592\n",
      "    epoch          : 149\n",
      "    loss           : -1398.0955556737315\n",
      "    val_loss       : -1413.3436829308325\n",
      "    val_log_likelihood: 1561.9336433032952\n",
      "    val_log_marginal: 1525.4254361266953\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -1493.611572\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -1435.268311\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -1270.501343\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -1355.408936\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -1359.501831\n",
      "    epoch          : 150\n",
      "    loss           : -1405.060355913521\n",
      "    val_loss       : -1398.3695676037948\n",
      "    val_log_likelihood: 1555.1730811997215\n",
      "    val_log_marginal: 1518.2098744904604\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -1801.631714\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -1231.894043\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -1311.826172\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -1321.855713\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -1332.027954\n",
      "    epoch          : 151\n",
      "    loss           : -1402.8943076558633\n",
      "    val_loss       : -1401.3984838334918\n",
      "    val_log_likelihood: 1555.0433663849783\n",
      "    val_log_marginal: 1518.9189818784557\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -1844.954834\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -1358.989990\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -1279.464722\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -1394.678223\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -1299.567017\n",
      "    epoch          : 152\n",
      "    loss           : -1388.0025562248607\n",
      "    val_loss       : -1386.2884847247826\n",
      "    val_log_likelihood: 1532.081869295328\n",
      "    val_log_marginal: 1487.041547420868\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -1717.886475\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -1350.525391\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -1307.501709\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -1746.762207\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -1513.888794\n",
      "    epoch          : 153\n",
      "    loss           : -1397.0258329788057\n",
      "    val_loss       : -1401.9942461645212\n",
      "    val_log_likelihood: 1567.9457149694463\n",
      "    val_log_marginal: 1526.6684017174036\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -1690.737305\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -1452.392578\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -1401.456177\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -1496.713745\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -1444.549316\n",
      "    epoch          : 154\n",
      "    loss           : -1400.358694548654\n",
      "    val_loss       : -1398.0808962233052\n",
      "    val_log_likelihood: 1560.5323184173885\n",
      "    val_log_marginal: 1522.0622912592794\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -1764.922974\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -1382.059326\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -1311.887329\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -1247.436157\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -1189.942627\n",
      "    epoch          : 155\n",
      "    loss           : -1391.569989799273\n",
      "    val_loss       : -1406.5045718155654\n",
      "    val_log_likelihood: 1563.7704413385675\n",
      "    val_log_marginal: 1524.901707790958\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -1733.836182\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -1379.787354\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -1571.224976\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -1104.763184\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -1484.645020\n",
      "    epoch          : 156\n",
      "    loss           : -1416.9064868889232\n",
      "    val_loss       : -1410.5037721834597\n",
      "    val_log_likelihood: 1564.9363228070854\n",
      "    val_log_marginal: 1526.162411680704\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -1724.654907\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -1440.110840\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -1510.922607\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -1221.195190\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -1429.216431\n",
      "    epoch          : 157\n",
      "    loss           : -1402.4294578627785\n",
      "    val_loss       : -1402.746383563099\n",
      "    val_log_likelihood: 1549.4179905051053\n",
      "    val_log_marginal: 1511.8639195484136\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -1501.959106\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -1329.667480\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -1494.614014\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -1295.745239\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -1418.102051\n",
      "    epoch          : 158\n",
      "    loss           : -1406.798610573948\n",
      "    val_loss       : -1385.1300652101174\n",
      "    val_log_likelihood: 1534.0067078241027\n",
      "    val_log_marginal: 1488.943416565021\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -1670.562988\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -1287.968872\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -1298.735352\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -1324.055542\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -1597.357422\n",
      "    epoch          : 159\n",
      "    loss           : -1395.9444580078125\n",
      "    val_loss       : -1410.5164015453106\n",
      "    val_log_likelihood: 1552.3546650197247\n",
      "    val_log_marginal: 1509.609015674155\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -1710.248779\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -1360.847778\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -1347.191406\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -1362.647461\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -1418.190674\n",
      "    epoch          : 160\n",
      "    loss           : -1398.7954161993348\n",
      "    val_loss       : -1408.7942837955209\n",
      "    val_log_likelihood: 1557.8735726233756\n",
      "    val_log_marginal: 1518.5833293905584\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch160.pth ...\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -1371.029053\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -1484.935547\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -1316.071045\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -1376.529053\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -1290.193848\n",
      "    epoch          : 161\n",
      "    loss           : -1407.0773502765317\n",
      "    val_loss       : -1409.2415956054529\n",
      "    val_log_likelihood: 1570.8164376740408\n",
      "    val_log_marginal: 1533.030583200091\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -1635.557861\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -1431.039673\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -1357.046509\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -1458.828491\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -1474.647827\n",
      "    epoch          : 162\n",
      "    loss           : -1426.7310210879486\n",
      "    val_loss       : -1414.778767517989\n",
      "    val_log_likelihood: 1567.469186310721\n",
      "    val_log_marginal: 1532.3507501058612\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -1675.707275\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -1296.256592\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -1310.468018\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -1321.178711\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -1282.505371\n",
      "    epoch          : 163\n",
      "    loss           : -1418.2602297339108\n",
      "    val_loss       : -1424.9800135700566\n",
      "    val_log_likelihood: 1560.897272393255\n",
      "    val_log_marginal: 1525.5683974005883\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -1808.917603\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -1517.114502\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -1357.795776\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -1314.457153\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -1342.993408\n",
      "    epoch          : 164\n",
      "    loss           : -1414.8533464186262\n",
      "    val_loss       : -1412.933564354343\n",
      "    val_log_likelihood: 1563.0099674640317\n",
      "    val_log_marginal: 1525.9448582097925\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -1738.272583\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -1456.451660\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -1434.339844\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -1227.290039\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -1492.780762\n",
      "    epoch          : 165\n",
      "    loss           : -1427.6964884842976\n",
      "    val_loss       : -1435.6861156048253\n",
      "    val_log_likelihood: 1577.4195725846998\n",
      "    val_log_marginal: 1540.087708841622\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -1731.656494\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -1540.455688\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -1511.779053\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -1252.421143\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -1521.023804\n",
      "    epoch          : 166\n",
      "    loss           : -1430.1529879428372\n",
      "    val_loss       : -1412.1957257724375\n",
      "    val_log_likelihood: 1582.5117658860613\n",
      "    val_log_marginal: 1543.895601121635\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -1703.632446\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -1377.910522\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -1243.132324\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -1811.277100\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -1391.611816\n",
      "    epoch          : 167\n",
      "    loss           : -1420.9941841352104\n",
      "    val_loss       : -1402.526665179105\n",
      "    val_log_likelihood: 1586.153998346612\n",
      "    val_log_marginal: 1546.6015189452153\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -1644.220459\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -1340.401245\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -1234.027832\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -1487.684570\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -1345.730103\n",
      "    epoch          : 168\n",
      "    loss           : -1408.5200509552908\n",
      "    val_loss       : -1414.1265628647318\n",
      "    val_log_likelihood: 1549.1721336440285\n",
      "    val_log_marginal: 1509.4365172655412\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -1754.785767\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -1163.016846\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -1291.412842\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -1305.535156\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -1316.591309\n",
      "    epoch          : 169\n",
      "    loss           : -1404.762152643487\n",
      "    val_loss       : -1370.3927308550603\n",
      "    val_log_likelihood: 1482.679473574799\n",
      "    val_log_marginal: 1435.325550636535\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -1717.004639\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -1363.368042\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -1333.836670\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -1491.729492\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -1437.399780\n",
      "    epoch          : 170\n",
      "    loss           : -1394.0955991839419\n",
      "    val_loss       : -1405.824251274984\n",
      "    val_log_likelihood: 1530.3825284750155\n",
      "    val_log_marginal: 1487.259069417797\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch170.pth ...\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -1704.654053\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -1207.575439\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -1518.536377\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -1680.927490\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -1416.592773\n",
      "    epoch          : 171\n",
      "    loss           : -1410.9969470335705\n",
      "    val_loss       : -1410.7475371596468\n",
      "    val_log_likelihood: 1542.726045211943\n",
      "    val_log_marginal: 1503.0672763791665\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -1733.802002\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -1335.995728\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -1228.755005\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -1509.624390\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -1452.806152\n",
      "    epoch          : 172\n",
      "    loss           : -1419.806071366414\n",
      "    val_loss       : -1420.1211928862635\n",
      "    val_log_likelihood: 1555.5790411316523\n",
      "    val_log_marginal: 1515.854274489963\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -1699.463135\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -1481.778687\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -1381.427124\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -1317.296021\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -1429.424072\n",
      "    epoch          : 173\n",
      "    loss           : -1408.4387303720607\n",
      "    val_loss       : -1426.612476782615\n",
      "    val_log_likelihood: 1559.5715271600402\n",
      "    val_log_marginal: 1519.954858565076\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -1731.254517\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -1480.875488\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -1204.297607\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -1353.443237\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -1425.788696\n",
      "    epoch          : 174\n",
      "    loss           : -1412.2297218247215\n",
      "    val_loss       : -1410.8771046917739\n",
      "    val_log_likelihood: 1556.1621843092512\n",
      "    val_log_marginal: 1516.0934526860383\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -1431.342407\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -1466.858887\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -1277.280273\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -1310.755371\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -1486.752197\n",
      "    epoch          : 175\n",
      "    loss           : -1416.6327436088336\n",
      "    val_loss       : -1428.0636421684544\n",
      "    val_log_likelihood: 1571.5701396677753\n",
      "    val_log_marginal: 1530.436121097541\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -1676.336060\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -1476.244263\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -1566.238281\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -1371.907349\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -1349.439453\n",
      "    epoch          : 176\n",
      "    loss           : -1431.267743705523\n",
      "    val_loss       : -1422.4076180388174\n",
      "    val_log_likelihood: 1575.1158930712406\n",
      "    val_log_marginal: 1536.7438783289883\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -1757.681152\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -1343.395874\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -1262.366455\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -1228.718018\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -1445.550659\n",
      "    epoch          : 177\n",
      "    loss           : -1428.4321977974164\n",
      "    val_loss       : -1428.328345704787\n",
      "    val_log_likelihood: 1582.1709056892018\n",
      "    val_log_marginal: 1542.4883588580717\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -1691.476196\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -1380.679932\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -1297.402588\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -1528.982422\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -1398.898315\n",
      "    epoch          : 178\n",
      "    loss           : -1432.3764575920482\n",
      "    val_loss       : -1428.1148870519114\n",
      "    val_log_likelihood: 1592.321836565981\n",
      "    val_log_marginal: 1553.3541612130348\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -1794.667236\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -1402.598389\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -1521.015137\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -1759.779663\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -1369.551025\n",
      "    epoch          : 179\n",
      "    loss           : -1438.2012081335088\n",
      "    val_loss       : -1437.900728743348\n",
      "    val_log_likelihood: 1590.1798627494586\n",
      "    val_log_marginal: 1550.6642570417227\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -1809.192505\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -1410.369263\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -1388.999512\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -1200.697876\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -1133.141602\n",
      "    epoch          : 180\n",
      "    loss           : -1432.775771339341\n",
      "    val_loss       : -1410.605392125638\n",
      "    val_log_likelihood: 1531.7238261912128\n",
      "    val_log_marginal: 1490.8335583591293\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch180.pth ...\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -1359.856201\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -1323.243774\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -1282.468506\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -1344.039307\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -1411.435303\n",
      "    epoch          : 181\n",
      "    loss           : -1429.8356329285273\n",
      "    val_loss       : -1428.3983959478999\n",
      "    val_log_likelihood: 1555.7220628190748\n",
      "    val_log_marginal: 1517.4640760722912\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -1703.285156\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -1294.671143\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -1492.700073\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -1413.164062\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -1278.387939\n",
      "    epoch          : 182\n",
      "    loss           : -1427.9585166402383\n",
      "    val_loss       : -1422.8759809521\n",
      "    val_log_likelihood: 1577.9688769047802\n",
      "    val_log_marginal: 1540.5864687873584\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -1776.449463\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -1272.565063\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -1331.106567\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -1400.607910\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -1455.379150\n",
      "    epoch          : 183\n",
      "    loss           : -1432.8067203937192\n",
      "    val_loss       : -1442.0191991583786\n",
      "    val_log_likelihood: 1569.5728010423113\n",
      "    val_log_marginal: 1532.9089725408173\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -1686.132080\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -1388.315186\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -1483.193970\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -1754.311401\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -1457.872803\n",
      "    epoch          : 184\n",
      "    loss           : -1428.4231645942914\n",
      "    val_loss       : -1433.685719904453\n",
      "    val_log_likelihood: 1585.5332418007426\n",
      "    val_log_marginal: 1547.6382130220604\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -1697.348755\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -1207.352295\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -1257.697754\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -1428.134399\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -1398.830811\n",
      "    epoch          : 185\n",
      "    loss           : -1440.6318758218595\n",
      "    val_loss       : -1434.8879644190893\n",
      "    val_log_likelihood: 1573.400338654471\n",
      "    val_log_marginal: 1535.5302418378726\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -1424.728760\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -1367.322754\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -1471.028076\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -1424.264893\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -1404.909058\n",
      "    epoch          : 186\n",
      "    loss           : -1439.6988017771503\n",
      "    val_loss       : -1428.6482704294901\n",
      "    val_log_likelihood: 1572.5667229076423\n",
      "    val_log_marginal: 1531.331297114608\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -1709.366577\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -1336.937988\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -1430.306396\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -1721.889893\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -1400.010376\n",
      "    epoch          : 187\n",
      "    loss           : -1413.360836217899\n",
      "    val_loss       : -1432.0352000479154\n",
      "    val_log_likelihood: 1550.5041600595607\n",
      "    val_log_marginal: 1507.5470381138753\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -1508.287354\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -1377.060303\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -1253.071045\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -1598.558105\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -1446.543335\n",
      "    epoch          : 188\n",
      "    loss           : -1427.3809065110613\n",
      "    val_loss       : -1423.0031627145697\n",
      "    val_log_likelihood: 1550.509467096612\n",
      "    val_log_marginal: 1509.0451079424618\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -1362.455811\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -1304.415527\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -1256.289673\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -1671.129517\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -1398.275391\n",
      "    epoch          : 189\n",
      "    loss           : -1421.7161333442914\n",
      "    val_loss       : -1439.8285617119918\n",
      "    val_log_likelihood: 1573.5982859394337\n",
      "    val_log_marginal: 1533.020285809265\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -1671.694214\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -1295.214600\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -1458.407837\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -1480.654663\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -1502.698486\n",
      "    epoch          : 190\n",
      "    loss           : -1431.9697749071781\n",
      "    val_loss       : -1425.6693377806391\n",
      "    val_log_likelihood: 1568.0807622021969\n",
      "    val_log_marginal: 1527.2761536203877\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch190.pth ...\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -1705.931763\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -1521.709839\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -1341.160522\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -1711.810547\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -1395.545654\n",
      "    epoch          : 191\n",
      "    loss           : -1423.8776021523051\n",
      "    val_loss       : -1422.8348283843773\n",
      "    val_log_likelihood: 1560.8878983601485\n",
      "    val_log_marginal: 1516.6683043713062\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -1718.463867\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -1543.567627\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -1321.109131\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -1365.381836\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -1461.861450\n",
      "    epoch          : 192\n",
      "    loss           : -1428.1810665319463\n",
      "    val_loss       : -1429.7335003323872\n",
      "    val_log_likelihood: 1563.6381437093905\n",
      "    val_log_marginal: 1522.615025423412\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -1725.951416\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -1366.306030\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -1340.302734\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -1333.006592\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -1354.937866\n",
      "    epoch          : 193\n",
      "    loss           : -1442.690864789604\n",
      "    val_loss       : -1438.6387768070408\n",
      "    val_log_likelihood: 1579.8458759572247\n",
      "    val_log_marginal: 1539.5571329892234\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -1374.934937\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -1388.396729\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -1522.658203\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -1377.793213\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -1443.823975\n",
      "    epoch          : 194\n",
      "    loss           : -1433.0036173905476\n",
      "    val_loss       : -1439.5164862165948\n",
      "    val_log_likelihood: 1576.422932539836\n",
      "    val_log_marginal: 1535.0853079584326\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -1366.614014\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -1349.548706\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -1305.578857\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -1388.649292\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -1340.200928\n",
      "    epoch          : 195\n",
      "    loss           : -1441.168453405399\n",
      "    val_loss       : -1447.5607992271698\n",
      "    val_log_likelihood: 1586.217500290068\n",
      "    val_log_marginal: 1544.983581092633\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -1755.598633\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -1485.967041\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -1240.940918\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -1391.156738\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -1332.811035\n",
      "    epoch          : 196\n",
      "    loss           : -1432.150280640857\n",
      "    val_loss       : -1430.6314383135407\n",
      "    val_log_likelihood: 1563.9460944751702\n",
      "    val_log_marginal: 1519.9394280666652\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -1690.405762\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -1440.263550\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -1262.132446\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -1204.682129\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -1448.295410\n",
      "    epoch          : 197\n",
      "    loss           : -1428.3149027305074\n",
      "    val_loss       : -1436.5675478807032\n",
      "    val_log_likelihood: 1569.613171265857\n",
      "    val_log_marginal: 1531.9799681954123\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -1753.371460\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -1268.980469\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -1333.185669\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -1350.273804\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -1460.619385\n",
      "    epoch          : 198\n",
      "    loss           : -1455.5575035775062\n",
      "    val_loss       : -1447.2586979445641\n",
      "    val_log_likelihood: 1572.5754769202506\n",
      "    val_log_marginal: 1535.009923605027\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -1758.605103\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -1244.533813\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -1351.227539\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -1677.530762\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -1414.528076\n",
      "    epoch          : 199\n",
      "    loss           : -1443.366051400062\n",
      "    val_loss       : -1446.4847348575747\n",
      "    val_log_likelihood: 1588.3834542756033\n",
      "    val_log_marginal: 1549.3548149771361\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -1625.094116\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -1281.413574\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -1384.864746\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -1471.884888\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -1483.884888\n",
      "    epoch          : 200\n",
      "    loss           : -1447.1546582514698\n",
      "    val_loss       : -1428.610015175674\n",
      "    val_log_likelihood: 1588.1136269144492\n",
      "    val_log_marginal: 1549.0041336066984\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -1640.775146\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -1464.991455\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -1387.976929\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -1228.623901\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -1406.352539\n",
      "    epoch          : 201\n",
      "    loss           : -1435.2544416673113\n",
      "    val_loss       : -1437.4397308686748\n",
      "    val_log_likelihood: 1580.4122254022277\n",
      "    val_log_marginal: 1539.1602007120957\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -1452.839111\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -1384.872559\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -1177.135376\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -1813.746338\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -1514.876953\n",
      "    epoch          : 202\n",
      "    loss           : -1443.2885355430074\n",
      "    val_loss       : -1437.7544465599176\n",
      "    val_log_likelihood: 1596.2321982808633\n",
      "    val_log_marginal: 1556.1380689515372\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -1717.333740\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -1369.880981\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -1270.774658\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -1422.245728\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -1365.320435\n",
      "    epoch          : 203\n",
      "    loss           : -1431.1287853883045\n",
      "    val_loss       : -1441.5722252892165\n",
      "    val_log_likelihood: 1585.0507413656405\n",
      "    val_log_marginal: 1544.5172521072338\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -1716.105957\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -1343.234741\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -1361.534546\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -1443.880127\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -1419.112305\n",
      "    epoch          : 204\n",
      "    loss           : -1442.0388582437345\n",
      "    val_loss       : -1449.9000644714913\n",
      "    val_log_likelihood: 1592.3363979830601\n",
      "    val_log_marginal: 1554.5072473387106\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -1775.685669\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -1379.214111\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -1353.929321\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -1518.129883\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -1378.578003\n",
      "    epoch          : 205\n",
      "    loss           : -1451.4226992767635\n",
      "    val_loss       : -1457.401795404917\n",
      "    val_log_likelihood: 1597.1308206992574\n",
      "    val_log_marginal: 1556.8823136986755\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -1748.486084\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -1359.525513\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -1450.136841\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -1578.882812\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -1517.120850\n",
      "    epoch          : 206\n",
      "    loss           : -1451.4032538385675\n",
      "    val_loss       : -1453.5727515880774\n",
      "    val_log_likelihood: 1600.0410796816986\n",
      "    val_log_marginal: 1559.9300861223462\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -1792.029541\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -1484.869629\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -1256.963501\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -1353.754150\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -1326.817383\n",
      "    epoch          : 207\n",
      "    loss           : -1452.3799504950496\n",
      "    val_loss       : -1433.0068648850804\n",
      "    val_log_likelihood: 1594.1847818204672\n",
      "    val_log_marginal: 1555.1881529321654\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -1773.322021\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -1400.562500\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -1379.242554\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -1770.744385\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -1416.404297\n",
      "    epoch          : 208\n",
      "    loss           : -1457.3666641688583\n",
      "    val_loss       : -1452.2643908709472\n",
      "    val_log_likelihood: 1584.9159588200032\n",
      "    val_log_marginal: 1541.9823070354944\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -1739.928955\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -1395.513428\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -1427.802490\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -1382.420288\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -1457.337646\n",
      "    epoch          : 209\n",
      "    loss           : -1443.3430067005725\n",
      "    val_loss       : -1459.0327944957403\n",
      "    val_log_likelihood: 1597.8237111308788\n",
      "    val_log_marginal: 1555.2149180461754\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -1752.722900\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -1310.413818\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -1245.232422\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -1495.134399\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -1367.062378\n",
      "    epoch          : 210\n",
      "    loss           : -1453.971734075263\n",
      "    val_loss       : -1442.6855985218806\n",
      "    val_log_likelihood: 1585.4290215520575\n",
      "    val_log_marginal: 1543.4610536810528\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch210.pth ...\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -1763.651245\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -1520.367432\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -1369.588379\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -1519.264648\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -1387.035645\n",
      "    epoch          : 211\n",
      "    loss           : -1446.187879505724\n",
      "    val_loss       : -1440.5607358637533\n",
      "    val_log_likelihood: 1586.204180122602\n",
      "    val_log_marginal: 1544.6874860370403\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -1800.758545\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -1313.746094\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -1331.509033\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -1540.247437\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -1490.440918\n",
      "    epoch          : 212\n",
      "    loss           : -1455.3713596457303\n",
      "    val_loss       : -1450.3504774609378\n",
      "    val_log_likelihood: 1605.3498728534962\n",
      "    val_log_marginal: 1561.8969162893436\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -1677.996582\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -1355.166260\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -1404.348389\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -1389.972290\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -1503.955322\n",
      "    epoch          : 213\n",
      "    loss           : -1452.637614335164\n",
      "    val_loss       : -1464.1940993661392\n",
      "    val_log_likelihood: 1605.2206825785117\n",
      "    val_log_marginal: 1563.580785121273\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -1816.338867\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -1587.489624\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -1411.891968\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -1328.809326\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -1425.880249\n",
      "    epoch          : 214\n",
      "    loss           : -1462.5272494778774\n",
      "    val_loss       : -1461.165776735393\n",
      "    val_log_likelihood: 1594.3154937441986\n",
      "    val_log_marginal: 1551.9540867292608\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -1705.624878\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -1513.428467\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -1519.003052\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -1737.954712\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -1447.497803\n",
      "    epoch          : 215\n",
      "    loss           : -1453.5704829149906\n",
      "    val_loss       : -1447.069415236731\n",
      "    val_log_likelihood: 1579.8541984935798\n",
      "    val_log_marginal: 1541.0399309002269\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -1708.687744\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -1313.830688\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -1573.066650\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -1362.922852\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -1504.479980\n",
      "    epoch          : 216\n",
      "    loss           : -1453.5203277285736\n",
      "    val_loss       : -1457.4747073230528\n",
      "    val_log_likelihood: 1610.2070820119122\n",
      "    val_log_marginal: 1566.898929510358\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -1787.619507\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -1326.914307\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -1350.711182\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -1214.492554\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -1500.744385\n",
      "    epoch          : 217\n",
      "    loss           : -1465.8156097714264\n",
      "    val_loss       : -1471.715899413055\n",
      "    val_log_likelihood: 1611.6953983118037\n",
      "    val_log_marginal: 1569.7223185594978\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -1764.329834\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -1456.955322\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -1268.392578\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -1746.862671\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -1462.609741\n",
      "    epoch          : 218\n",
      "    loss           : -1452.808594958617\n",
      "    val_loss       : -1468.70989019598\n",
      "    val_log_likelihood: 1599.454320322169\n",
      "    val_log_marginal: 1559.4953050097984\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -1780.543945\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -1310.520630\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -1444.870850\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -1390.220947\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -1381.635864\n",
      "    epoch          : 219\n",
      "    loss           : -1460.361572265625\n",
      "    val_loss       : -1467.9824125067985\n",
      "    val_log_likelihood: 1612.735866433323\n",
      "    val_log_marginal: 1567.8829631060546\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -1775.486816\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -1588.439941\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -1516.877930\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -1373.328247\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -1336.252808\n",
      "    epoch          : 220\n",
      "    loss           : -1467.8961000348081\n",
      "    val_loss       : -1456.3419713986264\n",
      "    val_log_likelihood: 1598.3845396136294\n",
      "    val_log_marginal: 1558.971589113393\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch220.pth ...\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -1759.352051\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -1537.413818\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -1485.678345\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -1797.078369\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -1469.408813\n",
      "    epoch          : 221\n",
      "    loss           : -1467.8887190110613\n",
      "    val_loss       : -1462.4323770162455\n",
      "    val_log_likelihood: 1597.9998416711787\n",
      "    val_log_marginal: 1555.8904811115308\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -1547.085693\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -1446.239868\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -1205.200317\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -1405.883667\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -1437.555054\n",
      "    epoch          : 222\n",
      "    loss           : -1449.2080416537747\n",
      "    val_loss       : -1459.0401023838226\n",
      "    val_log_likelihood: 1587.9272787264078\n",
      "    val_log_marginal: 1545.0947271938999\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -1808.579590\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -1555.737671\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -1374.834717\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -1442.603516\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -1536.660645\n",
      "    epoch          : 223\n",
      "    loss           : -1472.4633631942295\n",
      "    val_loss       : -1467.1741415972886\n",
      "    val_log_likelihood: 1589.6343438176825\n",
      "    val_log_marginal: 1550.0629325544223\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -1725.126709\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -1318.438477\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -1488.003540\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -1420.701782\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -1422.824707\n",
      "    epoch          : 224\n",
      "    loss           : -1452.3591888729889\n",
      "    val_loss       : -1455.4136295337782\n",
      "    val_log_likelihood: 1584.5363370687655\n",
      "    val_log_marginal: 1538.940852771135\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -1781.577637\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -1432.693359\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -1407.337036\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -1365.079712\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -1530.311768\n",
      "    epoch          : 225\n",
      "    loss           : -1462.3907216893565\n",
      "    val_loss       : -1472.2011782136829\n",
      "    val_log_likelihood: 1593.7263787902227\n",
      "    val_log_marginal: 1552.0049204982558\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -1842.791626\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -1509.096069\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -1356.622803\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -1571.059448\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -1422.636230\n",
      "    epoch          : 226\n",
      "    loss           : -1472.1746511931467\n",
      "    val_loss       : -1480.0709934103866\n",
      "    val_log_likelihood: 1601.455079333617\n",
      "    val_log_marginal: 1558.1390525038253\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -1407.863281\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -1386.035034\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -1395.762817\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -1451.383301\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -1476.454346\n",
      "    epoch          : 227\n",
      "    loss           : -1478.6551864170792\n",
      "    val_loss       : -1472.5273600286334\n",
      "    val_log_likelihood: 1592.8185974725402\n",
      "    val_log_marginal: 1551.666319307401\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -1544.885864\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -1317.543091\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -1324.968262\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -1436.022339\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -1453.554199\n",
      "    epoch          : 228\n",
      "    loss           : -1470.3786161819307\n",
      "    val_loss       : -1470.0505389183722\n",
      "    val_log_likelihood: 1602.610596911742\n",
      "    val_log_marginal: 1559.5315373852886\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -1769.899902\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -1566.361328\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -1497.891846\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -1767.391113\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -1551.182251\n",
      "    epoch          : 229\n",
      "    loss           : -1483.538496867265\n",
      "    val_loss       : -1480.9804700818374\n",
      "    val_log_likelihood: 1605.7402162457456\n",
      "    val_log_marginal: 1563.0977767528689\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -1860.438232\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -1528.086914\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -1300.388550\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -1542.724121\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -1450.306641\n",
      "    epoch          : 230\n",
      "    loss           : -1477.8681072575032\n",
      "    val_loss       : -1481.8732846424832\n",
      "    val_log_likelihood: 1617.1598105855508\n",
      "    val_log_marginal: 1573.75963754253\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch230.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -1817.719849\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -1345.444580\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -1506.863037\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -1818.113037\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -1297.720947\n",
      "    epoch          : 231\n",
      "    loss           : -1480.453313544245\n",
      "    val_loss       : -1478.5416888150266\n",
      "    val_log_likelihood: 1599.7963927618348\n",
      "    val_log_marginal: 1557.3357898558038\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -1806.131714\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -1362.340820\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -1588.326782\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -1314.194580\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -1454.142090\n",
      "    epoch          : 232\n",
      "    loss           : -1480.5867726543163\n",
      "    val_loss       : -1481.1150357925821\n",
      "    val_log_likelihood: 1613.5676511254642\n",
      "    val_log_marginal: 1569.7274682841423\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -1424.386108\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -1501.131226\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -1477.582031\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -1316.964233\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -1432.668457\n",
      "    epoch          : 233\n",
      "    loss           : -1477.3925587871288\n",
      "    val_loss       : -1472.981369383495\n",
      "    val_log_likelihood: 1603.184408357828\n",
      "    val_log_marginal: 1560.0970339649496\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -1754.317993\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -1334.908813\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -1300.537231\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -1472.449585\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -1247.867798\n",
      "    epoch          : 234\n",
      "    loss           : -1468.6223229134437\n",
      "    val_loss       : -1479.3021022463863\n",
      "    val_log_likelihood: 1601.2887603457611\n",
      "    val_log_marginal: 1555.297074757294\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -1781.628784\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -1442.495605\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -1580.621582\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -1419.821777\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -1439.950928\n",
      "    epoch          : 235\n",
      "    loss           : -1480.4317445660581\n",
      "    val_loss       : -1469.9244287239462\n",
      "    val_log_likelihood: 1599.3051334796567\n",
      "    val_log_marginal: 1551.5098397402983\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -1830.512329\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -1562.179199\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -1400.171387\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -1796.206421\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -1468.343018\n",
      "    epoch          : 236\n",
      "    loss           : -1478.3359229965965\n",
      "    val_loss       : -1466.1381964875884\n",
      "    val_log_likelihood: 1582.1740142520111\n",
      "    val_log_marginal: 1534.1153281042075\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -1774.442993\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -1365.445435\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -1402.913818\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -1307.278809\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -1414.135498\n",
      "    epoch          : 237\n",
      "    loss           : -1475.86566705987\n",
      "    val_loss       : -1480.6940085593942\n",
      "    val_log_likelihood: 1599.3076026840965\n",
      "    val_log_marginal: 1553.3074158830566\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -1765.054443\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -1440.920654\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -1292.846802\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -1415.546509\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -1369.130981\n",
      "    epoch          : 238\n",
      "    loss           : -1466.0808782294246\n",
      "    val_loss       : -1481.8862344554377\n",
      "    val_log_likelihood: 1602.9856911838644\n",
      "    val_log_marginal: 1560.44159560836\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -1801.770142\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -1543.444458\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -1494.928711\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -1485.045654\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -1313.328613\n",
      "    epoch          : 239\n",
      "    loss           : -1485.5511474609375\n",
      "    val_loss       : -1497.2139228819085\n",
      "    val_log_likelihood: 1610.4526766031095\n",
      "    val_log_marginal: 1566.7112793443882\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -1819.431885\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -1561.199219\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -1581.921387\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -1295.105591\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -1423.362061\n",
      "    epoch          : 240\n",
      "    loss           : -1487.9740074837562\n",
      "    val_loss       : -1491.3915577312785\n",
      "    val_log_likelihood: 1619.991781404703\n",
      "    val_log_marginal: 1574.8244157516717\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch240.pth ...\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -1522.798828\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -1499.062012\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -1538.859619\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -1556.146240\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -1394.334839\n",
      "    epoch          : 241\n",
      "    loss           : -1487.175621712562\n",
      "    val_loss       : -1492.4111250153937\n",
      "    val_log_likelihood: 1624.3279231609683\n",
      "    val_log_marginal: 1579.8679213723224\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -1783.034912\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -1396.692505\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -1329.756104\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -1447.292725\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -1563.651123\n",
      "    epoch          : 242\n",
      "    loss           : -1493.8250720335705\n",
      "    val_loss       : -1481.324320690983\n",
      "    val_log_likelihood: 1613.8727555983137\n",
      "    val_log_marginal: 1570.0655519686372\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -1827.333252\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -1344.272949\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -1404.425537\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -1463.795654\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -1510.805664\n",
      "    epoch          : 243\n",
      "    loss           : -1490.225989615563\n",
      "    val_loss       : -1483.2935087852052\n",
      "    val_log_likelihood: 1606.60557994276\n",
      "    val_log_marginal: 1561.5538212604415\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -1775.620605\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -1394.010742\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -1357.750244\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -1421.954102\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -1471.993164\n",
      "    epoch          : 244\n",
      "    loss           : -1475.179905051052\n",
      "    val_loss       : -1471.344024094172\n",
      "    val_log_likelihood: 1609.7194981338955\n",
      "    val_log_marginal: 1563.3149683050406\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -1445.787354\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -1619.272827\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -1329.892944\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -1761.432495\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -1455.291382\n",
      "    epoch          : 245\n",
      "    loss           : -1483.3010761525372\n",
      "    val_loss       : -1482.3973159442407\n",
      "    val_log_likelihood: 1596.4511392423422\n",
      "    val_log_marginal: 1551.0377806524127\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -1781.981445\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -1448.613403\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -1419.030884\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -1424.549194\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -1426.936279\n",
      "    epoch          : 246\n",
      "    loss           : -1475.1175875522122\n",
      "    val_loss       : -1482.327906082209\n",
      "    val_log_likelihood: 1582.4971911741955\n",
      "    val_log_marginal: 1537.8828974439518\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -1398.540771\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -1566.318604\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -1282.289307\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -1389.098877\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -1465.269287\n",
      "    epoch          : 247\n",
      "    loss           : -1481.310980768487\n",
      "    val_loss       : -1486.6743695942205\n",
      "    val_log_likelihood: 1603.9236299118193\n",
      "    val_log_marginal: 1559.9722235780637\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -1784.520020\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -1295.940186\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -1598.664551\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -1517.862549\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -1490.985352\n",
      "    epoch          : 248\n",
      "    loss           : -1489.68950872138\n",
      "    val_loss       : -1487.6963239007048\n",
      "    val_log_likelihood: 1612.6715160407643\n",
      "    val_log_marginal: 1569.5414008075684\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -1787.617065\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -1410.853760\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -1436.290283\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -1343.643066\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -1507.779785\n",
      "    epoch          : 249\n",
      "    loss           : -1486.1533082263304\n",
      "    val_loss       : -1478.8194413748893\n",
      "    val_log_likelihood: 1589.8953688215502\n",
      "    val_log_marginal: 1546.9738643048238\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -1779.288940\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -1357.196533\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -1512.604248\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -1593.459961\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -1493.450928\n",
      "    epoch          : 250\n",
      "    loss           : -1483.6435196376083\n",
      "    val_loss       : -1483.2603674585848\n",
      "    val_log_likelihood: 1594.4823783647896\n",
      "    val_log_marginal: 1550.3416494115115\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -1821.026611\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -1470.959106\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -1365.070312\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -1409.442993\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -1352.079346\n",
      "    epoch          : 251\n",
      "    loss           : -1491.611192759901\n",
      "    val_loss       : -1491.3157974666372\n",
      "    val_log_likelihood: 1612.6533444848392\n",
      "    val_log_marginal: 1570.0769531411147\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -1793.799194\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -1363.852661\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -1330.939575\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -1397.227661\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -1442.543701\n",
      "    epoch          : 252\n",
      "    loss           : -1487.9748680190285\n",
      "    val_loss       : -1495.9908019356374\n",
      "    val_log_likelihood: 1620.2948251856435\n",
      "    val_log_marginal: 1579.0725744234048\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -1808.894043\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -1348.853638\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -1372.582886\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -1390.134644\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -1486.030396\n",
      "    epoch          : 253\n",
      "    loss           : -1500.1038395343442\n",
      "    val_loss       : -1501.7580177376028\n",
      "    val_log_likelihood: 1623.955728360922\n",
      "    val_log_marginal: 1582.780002316902\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -1816.791504\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -1373.652954\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -1557.475342\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -1519.492432\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -1517.542969\n",
      "    epoch          : 254\n",
      "    loss           : -1506.5844605700804\n",
      "    val_loss       : -1497.1639291439667\n",
      "    val_log_likelihood: 1623.3934277827198\n",
      "    val_log_marginal: 1581.6018689163582\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -1560.474854\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -1320.498169\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -1427.843628\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -1352.867188\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -1483.508057\n",
      "    epoch          : 255\n",
      "    loss           : -1496.556019395885\n",
      "    val_loss       : -1504.5497268823501\n",
      "    val_log_likelihood: 1614.3550952873607\n",
      "    val_log_marginal: 1571.9551656441117\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -1598.580078\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -1438.678101\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -1601.047241\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -1422.212158\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -1511.647583\n",
      "    epoch          : 256\n",
      "    loss           : -1492.8597400023205\n",
      "    val_loss       : -1475.155087315874\n",
      "    val_log_likelihood: 1598.8166987353031\n",
      "    val_log_marginal: 1547.0862089625195\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -1753.460693\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -1370.430786\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -1518.961182\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -1466.740356\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -1489.284058\n",
      "    epoch          : 257\n",
      "    loss           : -1473.0226579420637\n",
      "    val_loss       : -1468.8062921217136\n",
      "    val_log_likelihood: 1593.905355623453\n",
      "    val_log_marginal: 1543.3485665375754\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -1815.592773\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -1556.867798\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -1499.851685\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -1468.409912\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -1497.752686\n",
      "    epoch          : 258\n",
      "    loss           : -1467.1576495784343\n",
      "    val_loss       : -1491.4255500494935\n",
      "    val_log_likelihood: 1606.7323276028774\n",
      "    val_log_marginal: 1559.824588947635\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -1626.801636\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -1394.835693\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -1594.368530\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -1450.029663\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -1418.491699\n",
      "    epoch          : 259\n",
      "    loss           : -1483.873483185721\n",
      "    val_loss       : -1493.0379861429112\n",
      "    val_log_likelihood: 1610.9575618328433\n",
      "    val_log_marginal: 1565.1558023954715\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -1813.838379\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -1464.536255\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -1547.535278\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -1794.275146\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -1433.834473\n",
      "    epoch          : 260\n",
      "    loss           : -1492.3086251740408\n",
      "    val_loss       : -1484.3519090386108\n",
      "    val_log_likelihood: 1605.8762074083386\n",
      "    val_log_marginal: 1558.8992626344102\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch260.pth ...\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -1777.436279\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -1362.695068\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -1456.262939\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -1793.911377\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -1454.078247\n",
      "    epoch          : 261\n",
      "    loss           : -1478.8703504505725\n",
      "    val_loss       : -1491.8376736351852\n",
      "    val_log_likelihood: 1613.2485327390161\n",
      "    val_log_marginal: 1566.5129177552533\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -1815.556396\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -1525.014038\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -1294.651245\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -1388.957275\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -1437.715210\n",
      "    epoch          : 262\n",
      "    loss           : -1493.1925266379178\n",
      "    val_loss       : -1499.120785418197\n",
      "    val_log_likelihood: 1617.6607158396503\n",
      "    val_log_marginal: 1569.9721966912361\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -1820.371704\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -1439.311401\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -1352.933594\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -1332.132568\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -1494.369019\n",
      "    epoch          : 263\n",
      "    loss           : -1500.2439798789449\n",
      "    val_loss       : -1495.5687946655355\n",
      "    val_log_likelihood: 1613.1299565381344\n",
      "    val_log_marginal: 1568.792234349129\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -1823.332642\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -1415.211426\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -1315.161865\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -1492.415649\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -1359.869629\n",
      "    epoch          : 264\n",
      "    loss           : -1496.8772830774287\n",
      "    val_loss       : -1509.4294841056412\n",
      "    val_log_likelihood: 1626.3414089089572\n",
      "    val_log_marginal: 1582.620813717765\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -1483.523926\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -1382.146240\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -1349.657227\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -1541.274780\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -1415.950562\n",
      "    epoch          : 265\n",
      "    loss           : -1500.7162735438583\n",
      "    val_loss       : -1496.36762195051\n",
      "    val_log_likelihood: 1617.0587895459469\n",
      "    val_log_marginal: 1571.5947367750146\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -1552.864990\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -1341.273193\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -1591.139160\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -1517.197266\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -1456.913330\n",
      "    epoch          : 266\n",
      "    loss           : -1499.1392532197556\n",
      "    val_loss       : -1505.4905696735523\n",
      "    val_log_likelihood: 1620.1116858756188\n",
      "    val_log_marginal: 1575.2247565034409\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -1800.961182\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -1427.384521\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -1358.082764\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -1468.535278\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -1450.513428\n",
      "    epoch          : 267\n",
      "    loss           : -1488.7098557878248\n",
      "    val_loss       : -1479.7390388118583\n",
      "    val_log_likelihood: 1587.9130110032488\n",
      "    val_log_marginal: 1542.054264141431\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -1778.425659\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -1537.744629\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -1554.375977\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -1408.703735\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -1442.497681\n",
      "    epoch          : 268\n",
      "    loss           : -1471.6622701210551\n",
      "    val_loss       : -1475.1159262525857\n",
      "    val_log_likelihood: 1573.8289238958075\n",
      "    val_log_marginal: 1529.9135249951062\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -1777.977905\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -1358.614868\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -1297.315063\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -1390.115723\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -1423.418701\n",
      "    epoch          : 269\n",
      "    loss           : -1486.218561455755\n",
      "    val_loss       : -1493.6068655623787\n",
      "    val_log_likelihood: 1611.5703572188274\n",
      "    val_log_marginal: 1566.464764792074\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -1749.910156\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -1504.091675\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -1522.092896\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -1404.289185\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -1469.613892\n",
      "    epoch          : 270\n",
      "    loss           : -1492.2698587851949\n",
      "    val_loss       : -1497.6915346484097\n",
      "    val_log_likelihood: 1612.7117122234683\n",
      "    val_log_marginal: 1568.7828793486235\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch270.pth ...\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -1828.167358\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -1410.174805\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -1494.324707\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -1833.484253\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -1606.248169\n",
      "    epoch          : 271\n",
      "    loss           : -1499.8511938718286\n",
      "    val_loss       : -1492.6571150659354\n",
      "    val_log_likelihood: 1622.1503966680848\n",
      "    val_log_marginal: 1578.8044867231372\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -1452.542725\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -1444.347534\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -1417.316162\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -1290.043579\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -1459.767822\n",
      "    epoch          : 272\n",
      "    loss           : -1480.7507831837872\n",
      "    val_loss       : -1481.5324048666043\n",
      "    val_log_likelihood: 1600.5404052734375\n",
      "    val_log_marginal: 1554.7558756472708\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -1609.714722\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -1391.087158\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -1338.577148\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -1514.065063\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -1589.828857\n",
      "    epoch          : 273\n",
      "    loss           : -1489.8394872079982\n",
      "    val_loss       : -1486.2584379712894\n",
      "    val_log_likelihood: 1585.5327897780012\n",
      "    val_log_marginal: 1538.777592561492\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -1780.693115\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -1516.270508\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -1369.784912\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -1799.551270\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -1499.738525\n",
      "    epoch          : 274\n",
      "    loss           : -1491.901830087794\n",
      "    val_loss       : -1483.1763494450433\n",
      "    val_log_likelihood: 1613.8171362546411\n",
      "    val_log_marginal: 1567.1303541212812\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -1771.363525\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -1538.424683\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -1463.490601\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -1334.346436\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -1444.970093\n",
      "    epoch          : 275\n",
      "    loss           : -1501.7510792949413\n",
      "    val_loss       : -1497.3256257550127\n",
      "    val_log_likelihood: 1603.8734614306156\n",
      "    val_log_marginal: 1559.4283806202325\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -1847.244263\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -1433.146973\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -1438.743286\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -1457.808838\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -1530.252563\n",
      "    epoch          : 276\n",
      "    loss           : -1490.4251479347154\n",
      "    val_loss       : -1498.419067054738\n",
      "    val_log_likelihood: 1611.9390832882116\n",
      "    val_log_marginal: 1567.4620763562305\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -1834.327881\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -1314.217773\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -1485.943726\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -1844.238037\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -1455.695801\n",
      "    epoch          : 277\n",
      "    loss           : -1504.7200734355663\n",
      "    val_loss       : -1497.2612245997211\n",
      "    val_log_likelihood: 1602.2836515218905\n",
      "    val_log_marginal: 1561.364167876966\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -1763.121094\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -1382.118408\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -1560.248291\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -1504.026978\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -1441.919678\n",
      "    epoch          : 278\n",
      "    loss           : -1486.6573873085551\n",
      "    val_loss       : -1496.555077758009\n",
      "    val_log_likelihood: 1609.529027353419\n",
      "    val_log_marginal: 1567.7964042050992\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -1433.041748\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -1355.807251\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -1437.986450\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -1353.413818\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -1504.428223\n",
      "    epoch          : 279\n",
      "    loss           : -1495.0740676728806\n",
      "    val_loss       : -1497.6037791079316\n",
      "    val_log_likelihood: 1612.0724698812655\n",
      "    val_log_marginal: 1568.566578831616\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -1769.446289\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -1272.697388\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -1530.871094\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -1272.440063\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -1406.709839\n",
      "    epoch          : 280\n",
      "    loss           : -1478.3898877436573\n",
      "    val_loss       : -1491.5377683209365\n",
      "    val_log_likelihood: 1606.7580336769029\n",
      "    val_log_marginal: 1564.2314902951118\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch280.pth ...\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -1796.372192\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -1509.862549\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -1402.310303\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -1460.192261\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -1520.359863\n",
      "    epoch          : 281\n",
      "    loss           : -1493.793699963258\n",
      "    val_loss       : -1504.9622704966714\n",
      "    val_log_likelihood: 1614.403455677599\n",
      "    val_log_marginal: 1571.7957837168717\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -1794.907593\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -1585.437622\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -1407.083374\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -1431.571289\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -1468.479248\n",
      "    epoch          : 282\n",
      "    loss           : -1497.5835009282177\n",
      "    val_loss       : -1502.0758948584298\n",
      "    val_log_likelihood: 1617.0259067044399\n",
      "    val_log_marginal: 1568.6475546288277\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -1628.252197\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -1385.885254\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -1308.594238\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -1806.817871\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -1442.244507\n",
      "    epoch          : 283\n",
      "    loss           : -1494.3466313428219\n",
      "    val_loss       : -1505.2399342249212\n",
      "    val_log_likelihood: 1611.7798044941212\n",
      "    val_log_marginal: 1563.6624273702048\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -1813.965576\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -1577.423828\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -1442.975098\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -1793.376099\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -1527.616699\n",
      "    epoch          : 284\n",
      "    loss           : -1499.6480471167233\n",
      "    val_loss       : -1491.6917967646668\n",
      "    val_log_likelihood: 1615.9493238996752\n",
      "    val_log_marginal: 1568.6627449937127\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -1814.509277\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -1597.187500\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -1344.083130\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -1422.598267\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -1459.476929\n",
      "    epoch          : 285\n",
      "    loss           : -1495.1452854269803\n",
      "    val_loss       : -1500.9256120376142\n",
      "    val_log_likelihood: 1624.2443618019029\n",
      "    val_log_marginal: 1576.3019545119773\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -1825.603882\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -1510.165771\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -1354.307007\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -1559.873291\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -1338.477417\n",
      "    epoch          : 286\n",
      "    loss           : -1480.3416977684096\n",
      "    val_loss       : -1485.262340651364\n",
      "    val_log_likelihood: 1581.6405222675587\n",
      "    val_log_marginal: 1535.1908330954427\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -1798.713501\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -1412.262939\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -1441.024658\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -1819.420166\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -1476.886353\n",
      "    epoch          : 287\n",
      "    loss           : -1485.0677296855663\n",
      "    val_loss       : -1487.443937311991\n",
      "    val_log_likelihood: 1596.9606981938427\n",
      "    val_log_marginal: 1552.5468331655986\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -1831.575684\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -1402.844238\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -1366.786987\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -1827.886719\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -1483.754639\n",
      "    epoch          : 288\n",
      "    loss           : -1495.645997302367\n",
      "    val_loss       : -1491.5218506773638\n",
      "    val_log_likelihood: 1597.9301624864636\n",
      "    val_log_marginal: 1555.1869160205974\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -1788.703735\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -1460.470947\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -1371.190674\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -1444.219849\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -1409.473022\n",
      "    epoch          : 289\n",
      "    loss           : -1492.8648862933169\n",
      "    val_loss       : -1496.7756749784555\n",
      "    val_log_likelihood: 1601.116345093982\n",
      "    val_log_marginal: 1558.4047339602523\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -1799.179199\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -1370.883789\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -1598.207520\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -1458.585205\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -1370.424927\n",
      "    epoch          : 290\n",
      "    loss           : -1494.1109921294865\n",
      "    val_loss       : -1501.354754076625\n",
      "    val_log_likelihood: 1611.2357528233292\n",
      "    val_log_marginal: 1567.7602044329005\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch290.pth ...\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -1499.157959\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -1600.478271\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -1382.719727\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -1508.290039\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -1396.669800\n",
      "    epoch          : 291\n",
      "    loss           : -1507.9655193668782\n",
      "    val_loss       : -1505.441238988698\n",
      "    val_log_likelihood: 1616.3794295811417\n",
      "    val_log_marginal: 1573.0169394943346\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -1848.240967\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -1543.125488\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -1361.506348\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -1451.510376\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -1560.809204\n",
      "    epoch          : 292\n",
      "    loss           : -1512.155709748221\n",
      "    val_loss       : -1512.0313351335612\n",
      "    val_log_likelihood: 1621.7985525603342\n",
      "    val_log_marginal: 1578.6232531016306\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -1843.702393\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -1569.708740\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -1407.822998\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -1842.102051\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -1553.228271\n",
      "    epoch          : 293\n",
      "    loss           : -1512.9844970703125\n",
      "    val_loss       : -1525.9064142402574\n",
      "    val_log_likelihood: 1634.3032685836943\n",
      "    val_log_marginal: 1590.5032196311047\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -1795.483887\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -1618.851318\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -1323.482056\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -1477.083374\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -1523.378052\n",
      "    epoch          : 294\n",
      "    loss           : -1517.4949975344214\n",
      "    val_loss       : -1513.7589777784497\n",
      "    val_log_likelihood: 1628.8026086788366\n",
      "    val_log_marginal: 1585.229952276878\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -1804.465088\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -1355.781250\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -1473.191284\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -1388.983398\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -1457.278809\n",
      "    epoch          : 295\n",
      "    loss           : -1508.278618840888\n",
      "    val_loss       : -1504.0843442900862\n",
      "    val_log_likelihood: 1638.343232711943\n",
      "    val_log_marginal: 1591.2988630220257\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -1869.793091\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -1586.199585\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -1453.507202\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -1398.618896\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -1491.816040\n",
      "    epoch          : 296\n",
      "    loss           : -1509.590547165068\n",
      "    val_loss       : -1514.2420663947498\n",
      "    val_log_likelihood: 1638.6253202834932\n",
      "    val_log_marginal: 1593.071131141328\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -1639.399902\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -1431.430176\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -1405.918701\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -1823.280029\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -1366.421387\n",
      "    epoch          : 297\n",
      "    loss           : -1511.051923393023\n",
      "    val_loss       : -1513.9048431429017\n",
      "    val_log_likelihood: 1632.8249100788985\n",
      "    val_log_marginal: 1589.086996952625\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -1378.296143\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -1578.934448\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -1593.489868\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -1487.753662\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -1396.395508\n",
      "    epoch          : 298\n",
      "    loss           : -1511.3097250154703\n",
      "    val_loss       : -1507.844387441643\n",
      "    val_log_likelihood: 1619.4264641185798\n",
      "    val_log_marginal: 1574.701362586867\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -1839.176880\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -1328.772705\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -1417.092529\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -1394.977905\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -1438.944336\n",
      "    epoch          : 299\n",
      "    loss           : -1488.0935215713955\n",
      "    val_loss       : -1490.087673788632\n",
      "    val_log_likelihood: 1599.5640289004486\n",
      "    val_log_marginal: 1555.2754333230787\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -1734.313843\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -1618.396240\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -1609.459106\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -1785.704102\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -1487.743652\n",
      "    epoch          : 300\n",
      "    loss           : -1500.2087716584158\n",
      "    val_loss       : -1500.6023183258822\n",
      "    val_log_likelihood: 1598.4147187790068\n",
      "    val_log_marginal: 1554.5772696414958\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -1791.156128\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -1347.337402\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -1492.981689\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -1555.379517\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -1437.066162\n",
      "    epoch          : 301\n",
      "    loss           : -1493.6563116394648\n",
      "    val_loss       : -1492.5831695736044\n",
      "    val_log_likelihood: 1587.4358707843442\n",
      "    val_log_marginal: 1544.5421853398952\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -1516.888550\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -1388.864746\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -1547.332275\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -1453.566528\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -1418.505249\n",
      "    epoch          : 302\n",
      "    loss           : -1494.4799623394956\n",
      "    val_loss       : -1505.3186038515098\n",
      "    val_log_likelihood: 1605.9284438331529\n",
      "    val_log_marginal: 1562.8057548645886\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -1791.231812\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -1445.681274\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -1576.624268\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -1509.050171\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -1473.967773\n",
      "    epoch          : 303\n",
      "    loss           : -1498.230850672958\n",
      "    val_loss       : -1481.348101200453\n",
      "    val_log_likelihood: 1569.1966456045018\n",
      "    val_log_marginal: 1524.2243460324398\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -1749.275757\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -1398.236694\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -1566.810181\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -1556.833496\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -1332.979858\n",
      "    epoch          : 304\n",
      "    loss           : -1483.6708803082456\n",
      "    val_loss       : -1503.7736357699546\n",
      "    val_log_likelihood: 1603.4179494121288\n",
      "    val_log_marginal: 1555.9534461558567\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -1800.515015\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -1435.244873\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -1560.842896\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -1469.226929\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -1468.490479\n",
      "    epoch          : 305\n",
      "    loss           : -1506.1862164487934\n",
      "    val_loss       : -1508.198652906869\n",
      "    val_log_likelihood: 1610.8500045927444\n",
      "    val_log_marginal: 1567.0594743183174\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -1813.993774\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -1373.446167\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -1572.858276\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -1444.511963\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -1508.824585\n",
      "    epoch          : 306\n",
      "    loss           : -1502.406142433091\n",
      "    val_loss       : -1504.1190866685592\n",
      "    val_log_likelihood: 1613.6459211594988\n",
      "    val_log_marginal: 1569.278837776291\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -1787.251343\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -1587.447266\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -1348.078857\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -1502.532959\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -1522.752686\n",
      "    epoch          : 307\n",
      "    loss           : -1516.1380832785428\n",
      "    val_loss       : -1502.5994571321598\n",
      "    val_log_likelihood: 1620.837050636216\n",
      "    val_log_marginal: 1576.2254350775354\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -1791.064209\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -1517.756836\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -1503.046631\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -1478.603760\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -1525.914307\n",
      "    epoch          : 308\n",
      "    loss           : -1497.604277053682\n",
      "    val_loss       : -1506.754142046316\n",
      "    val_log_likelihood: 1607.9243647509281\n",
      "    val_log_marginal: 1563.5753162895135\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -1781.391724\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -1470.337891\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -1431.441895\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -1537.251221\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -1400.329712\n",
      "    epoch          : 309\n",
      "    loss           : -1495.7463004234994\n",
      "    val_loss       : -1482.38947432088\n",
      "    val_log_likelihood: 1588.7916175162438\n",
      "    val_log_marginal: 1538.9575709777487\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -1555.584473\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -1437.912354\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -1428.799683\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -1795.628296\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -1476.397949\n",
      "    epoch          : 310\n",
      "    loss           : -1489.3848961556312\n",
      "    val_loss       : -1500.6598328626023\n",
      "    val_log_likelihood: 1609.1115396329672\n",
      "    val_log_marginal: 1559.5502603202506\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch310.pth ...\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -1810.666138\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -1324.449707\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -1375.228271\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -1457.739502\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -1446.145508\n",
      "    epoch          : 311\n",
      "    loss           : -1488.8220239016089\n",
      "    val_loss       : -1495.6296932382859\n",
      "    val_log_likelihood: 1591.7037401860302\n",
      "    val_log_marginal: 1544.7033206956405\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -1477.191650\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -1384.184570\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -1488.917480\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -1611.101807\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -1493.340576\n",
      "    epoch          : 312\n",
      "    loss           : -1504.1019238764698\n",
      "    val_loss       : -1501.9980459467297\n",
      "    val_log_likelihood: 1605.412623037206\n",
      "    val_log_marginal: 1559.7447010637768\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -1832.969727\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -1465.256592\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -1592.315430\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -1492.581909\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -1454.807739\n",
      "    epoch          : 313\n",
      "    loss           : -1506.0347707011913\n",
      "    val_loss       : -1513.3027117200343\n",
      "    val_log_likelihood: 1608.836639706451\n",
      "    val_log_marginal: 1562.9499799655105\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -1789.024536\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -1592.216064\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -1355.980713\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -1521.442871\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -1450.342773\n",
      "    epoch          : 314\n",
      "    loss           : -1509.2225087987315\n",
      "    val_loss       : -1510.5229139267328\n",
      "    val_log_likelihood: 1616.6755600730971\n",
      "    val_log_marginal: 1568.6393432780521\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -1812.331421\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -1372.920654\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -1483.614136\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -1413.856201\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -1494.970825\n",
      "    epoch          : 315\n",
      "    loss           : -1514.0985397489944\n",
      "    val_loss       : -1509.6333490491263\n",
      "    val_log_likelihood: 1622.7255786857982\n",
      "    val_log_marginal: 1577.2422722259923\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -1799.961304\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -1524.769531\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -1492.737793\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -1482.730957\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -1504.488647\n",
      "    epoch          : 316\n",
      "    loss           : -1517.1166315362004\n",
      "    val_loss       : -1516.182062353819\n",
      "    val_log_likelihood: 1625.0209284112004\n",
      "    val_log_marginal: 1577.8269259578337\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -1842.116333\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -1489.022095\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -1469.669556\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -1473.339600\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -1448.128906\n",
      "    epoch          : 317\n",
      "    loss           : -1519.2989248143565\n",
      "    val_loss       : -1512.1999211878065\n",
      "    val_log_likelihood: 1624.4194831470452\n",
      "    val_log_marginal: 1581.576598600037\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -1833.195312\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -1518.245361\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -1394.908936\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -1409.936401\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -1485.028564\n",
      "    epoch          : 318\n",
      "    loss           : -1518.0077266881963\n",
      "    val_loss       : -1512.844990378089\n",
      "    val_log_likelihood: 1638.4316502939357\n",
      "    val_log_marginal: 1593.809237129593\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -1812.739136\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -1400.867310\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -1394.951172\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -1469.699219\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -1431.840820\n",
      "    epoch          : 319\n",
      "    loss           : -1520.3866824914912\n",
      "    val_loss       : -1515.416039074384\n",
      "    val_log_likelihood: 1623.4242668529548\n",
      "    val_log_marginal: 1579.7942706760905\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -1814.757568\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -1644.627441\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -1498.776367\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -1849.833862\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -1475.584717\n",
      "    epoch          : 320\n",
      "    loss           : -1525.9856307530167\n",
      "    val_loss       : -1528.233745466807\n",
      "    val_log_likelihood: 1629.4823940768101\n",
      "    val_log_marginal: 1585.0396908840523\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch320.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -1358.620605\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -1397.430420\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -1601.125610\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -1515.207520\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -1465.246460\n",
      "    epoch          : 321\n",
      "    loss           : -1518.585368241414\n",
      "    val_loss       : -1519.0026322800186\n",
      "    val_log_likelihood: 1624.1965259514232\n",
      "    val_log_marginal: 1576.2923366609161\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -1801.018799\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -1557.682983\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -1622.757324\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -1565.222412\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -1548.180176\n",
      "    epoch          : 322\n",
      "    loss           : -1529.5839082321318\n",
      "    val_loss       : -1521.8083841265134\n",
      "    val_log_likelihood: 1615.7347073696628\n",
      "    val_log_marginal: 1570.703544875275\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -1829.197754\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -1571.845947\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -1621.542480\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -1415.136353\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -1499.016479\n",
      "    epoch          : 323\n",
      "    loss           : -1515.8257851175742\n",
      "    val_loss       : -1525.7631437212578\n",
      "    val_log_likelihood: 1614.8829877494586\n",
      "    val_log_marginal: 1569.3244817517882\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -1795.272705\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -1466.718872\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -1402.913818\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -1315.606689\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -1429.550171\n",
      "    epoch          : 324\n",
      "    loss           : -1520.3211488629331\n",
      "    val_loss       : -1509.5323003693713\n",
      "    val_log_likelihood: 1598.607174108524\n",
      "    val_log_marginal: 1550.7420348419375\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -1559.719116\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -1441.776001\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -1516.483643\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -1596.755615\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -1486.748779\n",
      "    epoch          : 325\n",
      "    loss           : -1517.8433934579982\n",
      "    val_loss       : -1520.6078029725388\n",
      "    val_log_likelihood: 1628.111763227104\n",
      "    val_log_marginal: 1579.847515877281\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -1816.292114\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -1453.808716\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -1573.605835\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -1394.141357\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -1519.711914\n",
      "    epoch          : 326\n",
      "    loss           : -1521.621936156018\n",
      "    val_loss       : -1530.5119372380823\n",
      "    val_log_likelihood: 1630.157148002398\n",
      "    val_log_marginal: 1583.6808242637221\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -1609.979614\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -1558.517090\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -1507.412109\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -1411.707275\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -1493.256592\n",
      "    epoch          : 327\n",
      "    loss           : -1529.1235810836943\n",
      "    val_loss       : -1525.5172065820932\n",
      "    val_log_likelihood: 1633.9190927637685\n",
      "    val_log_marginal: 1589.0672864715689\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -1596.288818\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -1602.705933\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -1576.229492\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -1559.935303\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -1452.493408\n",
      "    epoch          : 328\n",
      "    loss           : -1523.7905128403465\n",
      "    val_loss       : -1518.0281249010418\n",
      "    val_log_likelihood: 1610.1120508779393\n",
      "    val_log_marginal: 1563.5763884456182\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -1484.605957\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -1568.083130\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -1440.711060\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -1646.943848\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -1464.106689\n",
      "    epoch          : 329\n",
      "    loss           : -1523.0516623317606\n",
      "    val_loss       : -1526.7520432604736\n",
      "    val_log_likelihood: 1628.4596563660273\n",
      "    val_log_marginal: 1580.97546076758\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -1813.711426\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -1609.903931\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -1430.787720\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -1434.644531\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -1534.944336\n",
      "    epoch          : 330\n",
      "    loss           : -1532.4617581509128\n",
      "    val_loss       : -1534.126970395916\n",
      "    val_log_likelihood: 1630.9841115215038\n",
      "    val_log_marginal: 1585.1220652886473\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch330.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -1832.709717\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -1467.999634\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -1598.788574\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -1833.221558\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -1464.086182\n",
      "    epoch          : 331\n",
      "    loss           : -1534.122696376083\n",
      "    val_loss       : -1529.272788070926\n",
      "    val_log_likelihood: 1622.812422648515\n",
      "    val_log_marginal: 1577.205828286689\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -1814.559326\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -1389.191895\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -1528.353271\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -1502.398071\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -1458.645264\n",
      "    epoch          : 332\n",
      "    loss           : -1530.17253007039\n",
      "    val_loss       : -1535.8149725800693\n",
      "    val_log_likelihood: 1641.2381241297958\n",
      "    val_log_marginal: 1594.5551052961202\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -1804.300049\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -1621.169434\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -1568.256592\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -1813.883545\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -1507.770386\n",
      "    epoch          : 333\n",
      "    loss           : -1538.3648089418316\n",
      "    val_loss       : -1545.0223878496097\n",
      "    val_log_likelihood: 1639.6457797513149\n",
      "    val_log_marginal: 1594.2549096365635\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -1860.662476\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -1464.441528\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -1572.625854\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -1444.124023\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -1451.754028\n",
      "    epoch          : 334\n",
      "    loss           : -1542.164143477336\n",
      "    val_loss       : -1533.6322573862767\n",
      "    val_log_likelihood: 1641.7920115253712\n",
      "    val_log_marginal: 1590.7795772563666\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -1820.048096\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -1440.204102\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -1424.279541\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -1538.909424\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -1567.654297\n",
      "    epoch          : 335\n",
      "    loss           : -1537.2581617903002\n",
      "    val_loss       : -1546.1988746568763\n",
      "    val_log_likelihood: 1640.7083474338644\n",
      "    val_log_marginal: 1594.8160632719091\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -1792.750000\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -1682.960449\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -1515.572754\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -1387.389771\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -1478.525391\n",
      "    epoch          : 336\n",
      "    loss           : -1535.571481232596\n",
      "    val_loss       : -1530.603755895491\n",
      "    val_log_likelihood: 1626.4141591893565\n",
      "    val_log_marginal: 1571.2977222613686\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -1808.597778\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -1403.769043\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -1630.775513\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -1427.285156\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -1475.360962\n",
      "    epoch          : 337\n",
      "    loss           : -1531.737495648979\n",
      "    val_loss       : -1527.3351753071172\n",
      "    val_log_likelihood: 1627.0978206219058\n",
      "    val_log_marginal: 1576.1794234167144\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -1832.124023\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -1640.683594\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -1446.165771\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -1634.615112\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -1510.922607\n",
      "    epoch          : 338\n",
      "    loss           : -1536.67057694539\n",
      "    val_loss       : -1532.1454691859515\n",
      "    val_log_likelihood: 1645.869544303063\n",
      "    val_log_marginal: 1597.5327714000562\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -1834.519165\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -1637.265869\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -1509.961060\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -1823.591431\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -1546.714355\n",
      "    epoch          : 339\n",
      "    loss           : -1542.4263166673113\n",
      "    val_loss       : -1542.463430087036\n",
      "    val_log_likelihood: 1647.8660139329363\n",
      "    val_log_marginal: 1600.337601928188\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -1580.743652\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -1622.979126\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -1295.020020\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -1447.948120\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -1619.779907\n",
      "    epoch          : 340\n",
      "    loss           : -1541.3691768835088\n",
      "    val_loss       : -1547.1056944721536\n",
      "    val_log_likelihood: 1646.849419622138\n",
      "    val_log_marginal: 1602.0684508060476\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch340.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -1811.840332\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -1478.183960\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -1377.035156\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -1843.650269\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -1465.744019\n",
      "    epoch          : 341\n",
      "    loss           : -1541.2744285659035\n",
      "    val_loss       : -1546.2486054338608\n",
      "    val_log_likelihood: 1647.4408841274753\n",
      "    val_log_marginal: 1599.1117352605568\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -1814.286621\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -1516.852173\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -1375.080200\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -1495.578857\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -1525.217285\n",
      "    epoch          : 342\n",
      "    loss           : -1540.7206656578744\n",
      "    val_loss       : -1547.1575022031009\n",
      "    val_log_likelihood: 1650.362519821318\n",
      "    val_log_marginal: 1600.75721422297\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -1856.577393\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -1616.913330\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -1407.259155\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -1836.928833\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -1489.241333\n",
      "    epoch          : 343\n",
      "    loss           : -1538.9003023959622\n",
      "    val_loss       : -1538.5820809933957\n",
      "    val_log_likelihood: 1659.8187968943378\n",
      "    val_log_marginal: 1609.823498131486\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -1869.333252\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -1599.356812\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -1437.415039\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -1829.824463\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -1430.062622\n",
      "    epoch          : 344\n",
      "    loss           : -1529.8909549524287\n",
      "    val_loss       : -1524.4746901605758\n",
      "    val_log_likelihood: 1639.0304196801517\n",
      "    val_log_marginal: 1587.5635850322387\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -1846.803955\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -1297.481689\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -1533.852539\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -1596.285278\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -1509.259521\n",
      "    epoch          : 345\n",
      "    loss           : -1522.3679356338955\n",
      "    val_loss       : -1532.0112891431788\n",
      "    val_log_likelihood: 1649.5334545173268\n",
      "    val_log_marginal: 1599.281991773368\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -1839.540771\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -1329.603760\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -1580.308472\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -1471.044922\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -1500.063965\n",
      "    epoch          : 346\n",
      "    loss           : -1514.1908937964108\n",
      "    val_loss       : -1494.3304238484545\n",
      "    val_log_likelihood: 1626.979495813351\n",
      "    val_log_marginal: 1575.4671092519573\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -1799.106567\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -1489.618164\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -1473.546997\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -1805.726196\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -1483.041016\n",
      "    epoch          : 347\n",
      "    loss           : -1509.7990323812655\n",
      "    val_loss       : -1523.9026610576502\n",
      "    val_log_likelihood: 1629.5800829594677\n",
      "    val_log_marginal: 1576.2533522685528\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -1796.065308\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -1455.743042\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -1461.739136\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -1472.699219\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -1527.734009\n",
      "    epoch          : 348\n",
      "    loss           : -1531.0495254969833\n",
      "    val_loss       : -1527.5852451488504\n",
      "    val_log_likelihood: 1638.0565173460705\n",
      "    val_log_marginal: 1589.5234629142071\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -1818.215332\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -1595.483765\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -1457.948608\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -1547.119263\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -1473.223022\n",
      "    epoch          : 349\n",
      "    loss           : -1529.6668423189976\n",
      "    val_loss       : -1532.9814738799255\n",
      "    val_log_likelihood: 1638.8757964785736\n",
      "    val_log_marginal: 1588.952473069288\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -1858.467529\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -1493.762695\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -1393.647095\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -1420.622803\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -1601.677002\n",
      "    epoch          : 350\n",
      "    loss           : -1534.0797058709777\n",
      "    val_loss       : -1530.828420877641\n",
      "    val_log_likelihood: 1637.2514986850247\n",
      "    val_log_marginal: 1586.8874312075463\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -1817.943481\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -1496.390381\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -1398.786133\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -1509.331421\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -1499.105713\n",
      "    epoch          : 351\n",
      "    loss           : -1537.5784295714727\n",
      "    val_loss       : -1534.4058687136242\n",
      "    val_log_likelihood: 1636.0321431679301\n",
      "    val_log_marginal: 1590.6148092124\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -1864.045532\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -1452.358643\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -1453.531494\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -1422.741455\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -1507.216431\n",
      "    epoch          : 352\n",
      "    loss           : -1535.219400235922\n",
      "    val_loss       : -1541.187536146916\n",
      "    val_log_likelihood: 1639.8618067373143\n",
      "    val_log_marginal: 1594.3680518698684\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -1840.467896\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -1474.439453\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -1473.264282\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -1876.233398\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -1441.286377\n",
      "    epoch          : 353\n",
      "    loss           : -1525.0265605662128\n",
      "    val_loss       : -1529.9534776686683\n",
      "    val_log_likelihood: 1646.7377095741801\n",
      "    val_log_marginal: 1599.008657519641\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -1815.680420\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -1447.558594\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -1631.648438\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -1545.314453\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -1553.708496\n",
      "    epoch          : 354\n",
      "    loss           : -1534.585479434174\n",
      "    val_loss       : -1535.4371911543744\n",
      "    val_log_likelihood: 1644.2747476407797\n",
      "    val_log_marginal: 1595.7078928233188\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -1599.612305\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -1645.680298\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -1415.260986\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -1823.563843\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -1480.542358\n",
      "    epoch          : 355\n",
      "    loss           : -1539.899955522896\n",
      "    val_loss       : -1537.118072533224\n",
      "    val_log_likelihood: 1646.7802649771813\n",
      "    val_log_marginal: 1596.2095800954849\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -1849.622925\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -1618.622803\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -1593.911621\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -1518.372437\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -1512.502319\n",
      "    epoch          : 356\n",
      "    loss           : -1530.7471416209003\n",
      "    val_loss       : -1520.148150239684\n",
      "    val_log_likelihood: 1623.312149501083\n",
      "    val_log_marginal: 1573.543523998636\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -1616.307495\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -1431.108276\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -1586.932495\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -1369.381348\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -1530.182739\n",
      "    epoch          : 357\n",
      "    loss           : -1528.7874755859375\n",
      "    val_loss       : -1531.7505761501334\n",
      "    val_log_likelihood: 1648.1497561010983\n",
      "    val_log_marginal: 1598.3279397232957\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -1839.468994\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -1615.406494\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -1426.446045\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -1505.529053\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -1475.812134\n",
      "    epoch          : 358\n",
      "    loss           : -1532.6307989441523\n",
      "    val_loss       : -1527.0642048207544\n",
      "    val_log_likelihood: 1647.1389546913676\n",
      "    val_log_marginal: 1594.5201826636855\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -1836.564453\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -1618.425781\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -1376.612549\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -1457.995850\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -1506.792358\n",
      "    epoch          : 359\n",
      "    loss           : -1542.5020522315904\n",
      "    val_loss       : -1539.0970714605405\n",
      "    val_log_likelihood: 1657.8220698290531\n",
      "    val_log_marginal: 1605.6744401734322\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -1460.080078\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -1586.509766\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -1426.942627\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -1788.930054\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -1524.231812\n",
      "    epoch          : 360\n",
      "    loss           : -1545.4508697207611\n",
      "    val_loss       : -1539.2767927913585\n",
      "    val_log_likelihood: 1644.9276944906405\n",
      "    val_log_marginal: 1596.803452125852\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch360.pth ...\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -1851.833618\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -1479.895020\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -1539.783447\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -1554.677612\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -1482.115356\n",
      "    epoch          : 361\n",
      "    loss           : -1544.4282564975247\n",
      "    val_loss       : -1537.279053782915\n",
      "    val_log_likelihood: 1649.2488518138923\n",
      "    val_log_marginal: 1599.8438732574018\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -1464.516357\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -1663.107666\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -1438.813477\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -1575.785889\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -1516.121338\n",
      "    epoch          : 362\n",
      "    loss           : -1549.1267029412902\n",
      "    val_loss       : -1550.1163961471216\n",
      "    val_log_likelihood: 1648.4134352278002\n",
      "    val_log_marginal: 1602.3666357478255\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -1823.752441\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -1399.154175\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -1652.042969\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -1474.140869\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -1547.023682\n",
      "    epoch          : 363\n",
      "    loss           : -1541.9790800491182\n",
      "    val_loss       : -1547.1990539397455\n",
      "    val_log_likelihood: 1652.8614272315904\n",
      "    val_log_marginal: 1607.4496217701212\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -1820.191406\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -1372.673218\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -1540.564819\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -1627.466187\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -1492.409424\n",
      "    epoch          : 364\n",
      "    loss           : -1546.2093336653002\n",
      "    val_loss       : -1546.4173243826274\n",
      "    val_log_likelihood: 1663.4993691019492\n",
      "    val_log_marginal: 1614.5181893878016\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -1630.402832\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -1348.559204\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -1379.494385\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -1514.687988\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -1519.733643\n",
      "    epoch          : 365\n",
      "    loss           : -1550.2055422339108\n",
      "    val_loss       : -1549.431859673591\n",
      "    val_log_likelihood: 1659.7021689839883\n",
      "    val_log_marginal: 1606.053928792706\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -1868.581177\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -1504.264038\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -1535.152710\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -1482.937012\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -1561.541260\n",
      "    epoch          : 366\n",
      "    loss           : -1543.9697023901608\n",
      "    val_loss       : -1549.5964853278556\n",
      "    val_log_likelihood: 1658.890573029471\n",
      "    val_log_marginal: 1608.0372493743478\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -1841.744873\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -1649.197754\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -1460.908936\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -1571.845215\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -1566.273804\n",
      "    epoch          : 367\n",
      "    loss           : -1551.6428839050898\n",
      "    val_loss       : -1559.5610560022553\n",
      "    val_log_likelihood: 1662.5254220490408\n",
      "    val_log_marginal: 1613.651456610235\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -1565.584351\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -1487.727417\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -1434.351685\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -1510.755371\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -1481.961670\n",
      "    epoch          : 368\n",
      "    loss           : -1557.5967002339883\n",
      "    val_loss       : -1550.2273130682067\n",
      "    val_log_likelihood: 1673.3893595780476\n",
      "    val_log_marginal: 1621.8120769062432\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -1899.918701\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -1486.995728\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -1413.996704\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -1502.861328\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -1534.487305\n",
      "    epoch          : 369\n",
      "    loss           : -1554.0646174969058\n",
      "    val_loss       : -1546.705104361731\n",
      "    val_log_likelihood: 1647.9486253190748\n",
      "    val_log_marginal: 1598.8996741178264\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -1633.209473\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -1654.247925\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -1448.871338\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -1815.596436\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -1504.003174\n",
      "    epoch          : 370\n",
      "    loss           : -1553.9478119198639\n",
      "    val_loss       : -1557.5521554855704\n",
      "    val_log_likelihood: 1669.68663463026\n",
      "    val_log_marginal: 1619.0063945146665\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch370.pth ...\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -1838.538452\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -1465.481934\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -1474.983887\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -1426.874268\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -1577.459229\n",
      "    epoch          : 371\n",
      "    loss           : -1554.8430888865253\n",
      "    val_loss       : -1568.0669555108495\n",
      "    val_log_likelihood: 1661.7555233794865\n",
      "    val_log_marginal: 1614.1435741846649\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -1861.661255\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -1461.649170\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -1652.398804\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -1469.231689\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -1486.333740\n",
      "    epoch          : 372\n",
      "    loss           : -1557.230546101485\n",
      "    val_loss       : -1559.212848644805\n",
      "    val_log_likelihood: 1672.016653533029\n",
      "    val_log_marginal: 1623.0015300953262\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -1898.164795\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -1533.576172\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -1609.936035\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -1504.166504\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -1535.412598\n",
      "    epoch          : 373\n",
      "    loss           : -1552.1530979269803\n",
      "    val_loss       : -1552.168148245727\n",
      "    val_log_likelihood: 1655.8229074006033\n",
      "    val_log_marginal: 1607.0426867187723\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -1886.018433\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -1492.432739\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -1395.607788\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -1650.764771\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -1606.123291\n",
      "    epoch          : 374\n",
      "    loss           : -1546.5640518641708\n",
      "    val_loss       : -1550.875525147359\n",
      "    val_log_likelihood: 1659.7946160949102\n",
      "    val_log_marginal: 1610.030750336273\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -1843.165405\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -1458.206543\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -1465.769775\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -1478.882446\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -1476.195557\n",
      "    epoch          : 375\n",
      "    loss           : -1555.8278446008662\n",
      "    val_loss       : -1557.172905987479\n",
      "    val_log_likelihood: 1659.6055545618037\n",
      "    val_log_marginal: 1611.934087055726\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -1818.710449\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -1671.508911\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -1492.359131\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -1839.168457\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -1575.416626\n",
      "    epoch          : 376\n",
      "    loss           : -1552.1772134610922\n",
      "    val_loss       : -1558.389897440763\n",
      "    val_log_likelihood: 1660.5864016089108\n",
      "    val_log_marginal: 1611.5565591582308\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -1838.202393\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -1672.247314\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -1468.309814\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -1523.882080\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -1527.071289\n",
      "    epoch          : 377\n",
      "    loss           : -1553.3215489151455\n",
      "    val_loss       : -1552.9142766143518\n",
      "    val_log_likelihood: 1662.8398594620205\n",
      "    val_log_marginal: 1614.6617233522495\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -1865.694702\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -1557.042236\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -1458.727295\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -1541.202271\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -1473.233643\n",
      "    epoch          : 378\n",
      "    loss           : -1554.1129234993812\n",
      "    val_loss       : -1559.4363934088608\n",
      "    val_log_likelihood: 1669.8327467512377\n",
      "    val_log_marginal: 1621.9967102942132\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -1833.854858\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -1509.202393\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -1427.060181\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -1554.219727\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -1570.751221\n",
      "    epoch          : 379\n",
      "    loss           : -1559.6341069287594\n",
      "    val_loss       : -1551.2668145805837\n",
      "    val_log_likelihood: 1662.676054397432\n",
      "    val_log_marginal: 1613.8172065808005\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -1834.570679\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -1538.371460\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -1603.334961\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -1481.996338\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -1600.580200\n",
      "    epoch          : 380\n",
      "    loss           : -1563.3919629389698\n",
      "    val_loss       : -1554.7278276733352\n",
      "    val_log_likelihood: 1665.6248392539449\n",
      "    val_log_marginal: 1615.8511880150636\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch380.pth ...\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -1866.251465\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -1491.891846\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -1626.824097\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -1410.254272\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -1417.122437\n",
      "    epoch          : 381\n",
      "    loss           : -1540.0181908937964\n",
      "    val_loss       : -1554.3719647670496\n",
      "    val_log_likelihood: 1648.1030986521503\n",
      "    val_log_marginal: 1594.81329122206\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -1828.792114\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -1640.118652\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -1420.576172\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -1620.294922\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -1444.527466\n",
      "    epoch          : 382\n",
      "    loss           : -1549.2429888130414\n",
      "    val_loss       : -1548.7949291798425\n",
      "    val_log_likelihood: 1651.4180823599938\n",
      "    val_log_marginal: 1594.2641135824679\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -1607.901611\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -1508.668945\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -1544.436890\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -1514.657959\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -1547.993286\n",
      "    epoch          : 383\n",
      "    loss           : -1555.129693059638\n",
      "    val_loss       : -1555.2992861435591\n",
      "    val_log_likelihood: 1653.54627552599\n",
      "    val_log_marginal: 1601.1768577597334\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -1870.089600\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -1660.067749\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -1525.868896\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -1525.418335\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -1516.979736\n",
      "    epoch          : 384\n",
      "    loss           : -1548.2454592260983\n",
      "    val_loss       : -1545.6214720224868\n",
      "    val_log_likelihood: 1641.2075279915687\n",
      "    val_log_marginal: 1590.2668443366142\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -1840.991943\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -1474.481079\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -1432.981201\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -1502.160645\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -1530.322021\n",
      "    epoch          : 385\n",
      "    loss           : -1540.453040396813\n",
      "    val_loss       : -1541.5931217472005\n",
      "    val_log_likelihood: 1634.035371383818\n",
      "    val_log_marginal: 1584.3107241696982\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -1456.083984\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -1627.182617\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -1381.418213\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -1363.870117\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -1476.225098\n",
      "    epoch          : 386\n",
      "    loss           : -1543.8768564356435\n",
      "    val_loss       : -1541.0207992134117\n",
      "    val_log_likelihood: 1641.610947410659\n",
      "    val_log_marginal: 1590.4150081423877\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -1850.233398\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -1439.951660\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -1397.406738\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -1413.502319\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -1492.088745\n",
      "    epoch          : 387\n",
      "    loss           : -1545.8435397006497\n",
      "    val_loss       : -1548.0017560953827\n",
      "    val_log_likelihood: 1649.8069802463644\n",
      "    val_log_marginal: 1600.0573056261098\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -1821.573730\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -1400.566040\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -1589.487793\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -1561.652344\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -1562.243408\n",
      "    epoch          : 388\n",
      "    loss           : -1543.0998595587098\n",
      "    val_loss       : -1548.9373995171086\n",
      "    val_log_likelihood: 1637.465547165068\n",
      "    val_log_marginal: 1586.1637781160782\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -1885.143311\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -1482.890137\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -1404.788452\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -1838.145508\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -1425.570312\n",
      "    epoch          : 389\n",
      "    loss           : -1553.269967560721\n",
      "    val_loss       : -1529.9172658263105\n",
      "    val_log_likelihood: 1602.0679883295948\n",
      "    val_log_marginal: 1555.4729301702084\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -1595.196045\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -1447.840576\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -1545.530151\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -1547.737061\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -1432.285156\n",
      "    epoch          : 390\n",
      "    loss           : -1547.8991747563427\n",
      "    val_loss       : -1556.1661062343508\n",
      "    val_log_likelihood: 1644.104217831451\n",
      "    val_log_marginal: 1595.3197656878924\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch390.pth ...\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -1814.920166\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -1406.768921\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -1414.000488\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -1551.568970\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -1473.334717\n",
      "    epoch          : 391\n",
      "    loss           : -1548.5394782642327\n",
      "    val_loss       : -1543.1093268116733\n",
      "    val_log_likelihood: 1627.1585717531714\n",
      "    val_log_marginal: 1575.4739860746788\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -1824.076782\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -1369.241821\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -1408.906860\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -1490.642090\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -1559.012451\n",
      "    epoch          : 392\n",
      "    loss           : -1546.2700654586943\n",
      "    val_loss       : -1554.1021232431901\n",
      "    val_log_likelihood: 1651.9210676438738\n",
      "    val_log_marginal: 1600.931516947483\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -1828.630371\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -1488.536621\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -1498.059570\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -1840.857910\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -1578.051758\n",
      "    epoch          : 393\n",
      "    loss           : -1559.2644284692142\n",
      "    val_loss       : -1554.964859419198\n",
      "    val_log_likelihood: 1658.1590092725094\n",
      "    val_log_marginal: 1609.2429700584994\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -1847.952026\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -1413.817261\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -1566.243164\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -1591.726685\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -1536.793945\n",
      "    epoch          : 394\n",
      "    loss           : -1547.9444290010056\n",
      "    val_loss       : -1553.6817019471143\n",
      "    val_log_likelihood: 1642.1295915358137\n",
      "    val_log_marginal: 1590.9809719637442\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -1854.294434\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -1502.952026\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -1411.192261\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -1469.766968\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -1584.859375\n",
      "    epoch          : 395\n",
      "    loss           : -1552.8923412360768\n",
      "    val_loss       : -1557.3973709538966\n",
      "    val_log_likelihood: 1650.8673011099938\n",
      "    val_log_marginal: 1602.2092128811164\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -1848.330200\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -1642.843018\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -1596.014648\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -1610.316650\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -1522.960938\n",
      "    epoch          : 396\n",
      "    loss           : -1549.068522538289\n",
      "    val_loss       : -1553.349867310375\n",
      "    val_log_likelihood: 1638.8830082959469\n",
      "    val_log_marginal: 1590.348204800269\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -1856.749878\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -1481.814209\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -1595.786499\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -1538.187744\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -1532.222900\n",
      "    epoch          : 397\n",
      "    loss           : -1543.730955822633\n",
      "    val_loss       : -1543.960028559237\n",
      "    val_log_likelihood: 1630.0405164661975\n",
      "    val_log_marginal: 1575.6096822463549\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -1826.022461\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -1461.951904\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -1479.591064\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -1568.392944\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -1575.902100\n",
      "    epoch          : 398\n",
      "    loss           : -1550.2037667756033\n",
      "    val_loss       : -1549.496487929366\n",
      "    val_log_likelihood: 1635.062635365099\n",
      "    val_log_marginal: 1583.7904510580852\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -1629.318848\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -1455.976807\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -1560.418701\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -1622.723145\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -1547.708740\n",
      "    epoch          : 399\n",
      "    loss           : -1552.2429936475094\n",
      "    val_loss       : -1548.939361246939\n",
      "    val_log_likelihood: 1643.5161495397588\n",
      "    val_log_marginal: 1594.3694632555553\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -1600.239258\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -1496.117310\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -1444.618408\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -1865.008789\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -1566.572510\n",
      "    epoch          : 400\n",
      "    loss           : -1557.9729064337098\n",
      "    val_loss       : -1561.020999142107\n",
      "    val_log_likelihood: 1649.2405969600866\n",
      "    val_log_marginal: 1601.684621151629\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -1862.134521\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -1445.724487\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -1465.798096\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -1446.834473\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -1466.537354\n",
      "    epoch          : 401\n",
      "    loss           : -1550.2486294283726\n",
      "    val_loss       : -1557.3123386165882\n",
      "    val_log_likelihood: 1648.0246944616338\n",
      "    val_log_marginal: 1600.0142847720977\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -1856.975464\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -1496.322998\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -1406.729980\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -1446.213745\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -1554.872314\n",
      "    epoch          : 402\n",
      "    loss           : -1557.2327639135983\n",
      "    val_loss       : -1557.414906171879\n",
      "    val_log_likelihood: 1652.2379960163985\n",
      "    val_log_marginal: 1604.3531222605138\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -1857.714111\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -1528.101318\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -1476.169434\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -1421.614258\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -1534.043701\n",
      "    epoch          : 403\n",
      "    loss           : -1555.646181012144\n",
      "    val_loss       : -1549.1140529074714\n",
      "    val_log_likelihood: 1653.8910455987004\n",
      "    val_log_marginal: 1605.379395917178\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -1854.298462\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -1452.558350\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -1658.398071\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -1849.989380\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -1473.360229\n",
      "    epoch          : 404\n",
      "    loss           : -1559.8356353457611\n",
      "    val_loss       : -1562.3352975559853\n",
      "    val_log_likelihood: 1664.5509746287128\n",
      "    val_log_marginal: 1616.5323112868816\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -1839.877441\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -1459.050171\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -1513.771729\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -1550.319092\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -1542.416138\n",
      "    epoch          : 405\n",
      "    loss           : -1558.037572275294\n",
      "    val_loss       : -1559.9409336075832\n",
      "    val_log_likelihood: 1663.2711169554455\n",
      "    val_log_marginal: 1612.2066741560466\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -1855.508301\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -1518.613403\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -1464.348755\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -1449.760742\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -1492.013062\n",
      "    epoch          : 406\n",
      "    loss           : -1556.7848734336324\n",
      "    val_loss       : -1563.3011459539284\n",
      "    val_log_likelihood: 1671.4031511061262\n",
      "    val_log_marginal: 1620.3315093638657\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -1473.657715\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -1502.522705\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -1395.209229\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -1455.103149\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -1397.805664\n",
      "    epoch          : 407\n",
      "    loss           : -1552.005477452042\n",
      "    val_loss       : -1531.7654805342197\n",
      "    val_log_likelihood: 1608.2686127011139\n",
      "    val_log_marginal: 1557.1325911319823\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -1823.335449\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -1477.467407\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -1602.693848\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -1843.931519\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -1532.879517\n",
      "    epoch          : 408\n",
      "    loss           : -1541.4848765760364\n",
      "    val_loss       : -1546.737907806227\n",
      "    val_log_likelihood: 1623.3015245494275\n",
      "    val_log_marginal: 1570.9400012862106\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -1841.093750\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -1495.295776\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -1431.954468\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -1427.663086\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -1573.250366\n",
      "    epoch          : 409\n",
      "    loss           : -1547.9880419438427\n",
      "    val_loss       : -1547.90620843097\n",
      "    val_log_likelihood: 1627.129798209313\n",
      "    val_log_marginal: 1576.8945395969556\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -1858.136963\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -1481.676025\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -1635.252197\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -1464.852539\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -1454.989380\n",
      "    epoch          : 410\n",
      "    loss           : -1550.5325178391863\n",
      "    val_loss       : -1542.1485333773908\n",
      "    val_log_likelihood: 1630.930909411742\n",
      "    val_log_marginal: 1582.8412334819573\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch410.pth ...\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -1745.733276\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -1481.813965\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -1435.639771\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -1535.337280\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -1443.890259\n",
      "    epoch          : 411\n",
      "    loss           : -1546.318795685721\n",
      "    val_loss       : -1552.2111112483694\n",
      "    val_log_likelihood: 1649.6408570544554\n",
      "    val_log_marginal: 1602.362561627577\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -1857.801514\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -1486.820068\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -1401.208496\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -1581.744629\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -1528.173462\n",
      "    epoch          : 412\n",
      "    loss           : -1550.7105785407643\n",
      "    val_loss       : -1534.1898675686266\n",
      "    val_log_likelihood: 1635.7849266127785\n",
      "    val_log_marginal: 1588.703330379957\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -1820.227295\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -1483.009033\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -1433.092041\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -1583.141357\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -1516.619629\n",
      "    epoch          : 413\n",
      "    loss           : -1540.3352389193997\n",
      "    val_loss       : -1549.8878834659768\n",
      "    val_log_likelihood: 1642.1202863938738\n",
      "    val_log_marginal: 1595.0715727361453\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -1841.656372\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -1554.283447\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -1432.056152\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -1846.669678\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -1577.210815\n",
      "    epoch          : 414\n",
      "    loss           : -1554.4129070621907\n",
      "    val_loss       : -1552.2419895111204\n",
      "    val_log_likelihood: 1644.0538305905786\n",
      "    val_log_marginal: 1594.7119909016103\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -1847.996826\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -1666.617798\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -1467.228516\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -1452.955688\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -1494.404175\n",
      "    epoch          : 415\n",
      "    loss           : -1541.313661480894\n",
      "    val_loss       : -1535.8037145596993\n",
      "    val_log_likelihood: 1610.666829024211\n",
      "    val_log_marginal: 1558.5670045933844\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -1843.549927\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -1502.687134\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -1448.568237\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -1430.568237\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -1525.632935\n",
      "    epoch          : 416\n",
      "    loss           : -1543.4446284228031\n",
      "    val_loss       : -1550.0875619305298\n",
      "    val_log_likelihood: 1635.0596996345143\n",
      "    val_log_marginal: 1582.9696426679884\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -1863.269165\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -1647.695068\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -1491.646240\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -1515.253052\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -1526.435303\n",
      "    epoch          : 417\n",
      "    loss           : -1551.7911667021194\n",
      "    val_loss       : -1553.423078929489\n",
      "    val_log_likelihood: 1639.1213838180693\n",
      "    val_log_marginal: 1591.7163277580164\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -1833.696167\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -1407.128906\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -1645.473389\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -1626.474976\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -1544.439697\n",
      "    epoch          : 418\n",
      "    loss           : -1558.0396788946473\n",
      "    val_loss       : -1561.4313937198408\n",
      "    val_log_likelihood: 1650.1298417195235\n",
      "    val_log_marginal: 1600.673270963741\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -1860.954590\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -1518.730713\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -1521.234375\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -1843.766846\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -1523.812134\n",
      "    epoch          : 419\n",
      "    loss           : -1561.6860073580601\n",
      "    val_loss       : -1557.2643534415356\n",
      "    val_log_likelihood: 1646.1020024365719\n",
      "    val_log_marginal: 1596.3211384240242\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -1865.999268\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -1435.511963\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -1539.156494\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -1451.913574\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -1555.212769\n",
      "    epoch          : 420\n",
      "    loss           : -1564.1431570525217\n",
      "    val_loss       : -1567.9880133208515\n",
      "    val_log_likelihood: 1656.379942034731\n",
      "    val_log_marginal: 1607.5249302604668\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch420.pth ...\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -1856.802124\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -1635.765625\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -1392.877930\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -1508.392578\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -1557.134521\n",
      "    epoch          : 421\n",
      "    loss           : -1566.5701638401144\n",
      "    val_loss       : -1563.3910948945081\n",
      "    val_log_likelihood: 1661.6118079459313\n",
      "    val_log_marginal: 1610.6296492181602\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -1635.440063\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -1439.884766\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -1542.904785\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -1534.114746\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -1526.154297\n",
      "    epoch          : 422\n",
      "    loss           : -1556.669729704904\n",
      "    val_loss       : -1546.787386908574\n",
      "    val_log_likelihood: 1640.3042125135364\n",
      "    val_log_marginal: 1580.7411932688724\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -1637.137817\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -1523.840210\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -1484.192139\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -1354.429077\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -1555.985596\n",
      "    epoch          : 423\n",
      "    loss           : -1550.2733178469214\n",
      "    val_loss       : -1558.935625309687\n",
      "    val_log_likelihood: 1654.621904731977\n",
      "    val_log_marginal: 1600.2620790394071\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -1480.073242\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -1611.431519\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -1438.170654\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -1588.101318\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -1607.025269\n",
      "    epoch          : 424\n",
      "    loss           : -1558.331840288521\n",
      "    val_loss       : -1558.8517352824865\n",
      "    val_log_likelihood: 1659.8125700997834\n",
      "    val_log_marginal: 1606.3565709368318\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -1833.081055\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -1655.466064\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -1467.623047\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -1468.617065\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -1518.179565\n",
      "    epoch          : 425\n",
      "    loss           : -1555.637016069771\n",
      "    val_loss       : -1549.0624903559242\n",
      "    val_log_likelihood: 1643.8558869314666\n",
      "    val_log_marginal: 1590.0465404553365\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -1855.147583\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -1530.901367\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -1486.705078\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -1604.564941\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -1473.757568\n",
      "    epoch          : 426\n",
      "    loss           : -1553.4353825030942\n",
      "    val_loss       : -1559.3670836587594\n",
      "    val_log_likelihood: 1661.615749245823\n",
      "    val_log_marginal: 1609.3853591398918\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -1855.538452\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -1395.797729\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -1646.816162\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -1566.193726\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -1544.368286\n",
      "    epoch          : 427\n",
      "    loss           : -1559.7394584429146\n",
      "    val_loss       : -1564.1013210885744\n",
      "    val_log_likelihood: 1659.978109529703\n",
      "    val_log_marginal: 1610.6978414069565\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -1680.684937\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -1626.704102\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -1546.243286\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -1488.006348\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -1494.373779\n",
      "    epoch          : 428\n",
      "    loss           : -1560.2227771116955\n",
      "    val_loss       : -1556.5487604450416\n",
      "    val_log_likelihood: 1649.114476572169\n",
      "    val_log_marginal: 1599.308937542122\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -1839.614136\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -1483.396973\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -1502.979370\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -1564.343506\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -1561.057739\n",
      "    epoch          : 429\n",
      "    loss           : -1558.9085596670018\n",
      "    val_loss       : -1556.047004469864\n",
      "    val_log_likelihood: 1656.6311216448794\n",
      "    val_log_marginal: 1602.357509604019\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -1858.978760\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -1602.167969\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -1519.399536\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -1559.911133\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -1406.118896\n",
      "    epoch          : 430\n",
      "    loss           : -1556.8848901125464\n",
      "    val_loss       : -1548.1305780002367\n",
      "    val_log_likelihood: 1638.8707831354425\n",
      "    val_log_marginal: 1590.1348268657296\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch430.pth ...\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -1822.373413\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -1514.315063\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -1603.647461\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -1407.301636\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -1547.175781\n",
      "    epoch          : 431\n",
      "    loss           : -1559.4671655031714\n",
      "    val_loss       : -1559.2795521098346\n",
      "    val_log_likelihood: 1655.2379790957611\n",
      "    val_log_marginal: 1604.1583739284329\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -1858.087280\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -1521.821899\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -1577.046875\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -1556.573364\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -1543.291870\n",
      "    epoch          : 432\n",
      "    loss           : -1563.8763258528002\n",
      "    val_loss       : -1564.740818680135\n",
      "    val_log_likelihood: 1670.5548893390317\n",
      "    val_log_marginal: 1617.5597215721402\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -1862.307861\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -1571.890869\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -1389.367432\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -1425.511108\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -1538.754639\n",
      "    epoch          : 433\n",
      "    loss           : -1543.3201348333075\n",
      "    val_loss       : -1547.5730710638743\n",
      "    val_log_likelihood: 1627.8469625038676\n",
      "    val_log_marginal: 1576.3841253229875\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -1834.060791\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -1529.220337\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -1543.167847\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -1588.794922\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -1477.123535\n",
      "    epoch          : 434\n",
      "    loss           : -1550.042890189898\n",
      "    val_loss       : -1548.798465871787\n",
      "    val_log_likelihood: 1633.4893992206837\n",
      "    val_log_marginal: 1581.6461389972\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -1814.021973\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -1425.298340\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -1505.330322\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -1532.699829\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -1543.055786\n",
      "    epoch          : 435\n",
      "    loss           : -1554.756673982828\n",
      "    val_loss       : -1559.9272738632442\n",
      "    val_log_likelihood: 1646.7046587349164\n",
      "    val_log_marginal: 1596.708071185194\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -1827.257446\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -1477.956787\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -1485.606689\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -1560.809692\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -1546.251099\n",
      "    epoch          : 436\n",
      "    loss           : -1561.7155930925123\n",
      "    val_loss       : -1564.8376992449423\n",
      "    val_log_likelihood: 1656.8742313196162\n",
      "    val_log_marginal: 1606.340721772541\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -1841.091309\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -1414.226074\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -1460.975342\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -1858.729248\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -1582.338135\n",
      "    epoch          : 437\n",
      "    loss           : -1566.6214938022122\n",
      "    val_loss       : -1565.2738593307113\n",
      "    val_log_likelihood: 1659.9826539294554\n",
      "    val_log_marginal: 1610.3784849093113\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -1874.938354\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -1543.022461\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -1483.250488\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -1481.516724\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -1470.897949\n",
      "    epoch          : 438\n",
      "    loss           : -1565.900796720297\n",
      "    val_loss       : -1563.2641809721672\n",
      "    val_log_likelihood: 1650.9485262124845\n",
      "    val_log_marginal: 1599.4607332970124\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -1844.270996\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -1517.327637\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -1419.752686\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -1482.594604\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -1595.456665\n",
      "    epoch          : 439\n",
      "    loss           : -1563.7945653329982\n",
      "    val_loss       : -1558.3040927109944\n",
      "    val_log_likelihood: 1655.7299369585396\n",
      "    val_log_marginal: 1604.591411713566\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -1862.796753\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -1501.163452\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -1588.651245\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -1567.142822\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -1601.804932\n",
      "    epoch          : 440\n",
      "    loss           : -1568.5859713412747\n",
      "    val_loss       : -1571.692681128823\n",
      "    val_log_likelihood: 1668.2012661471226\n",
      "    val_log_marginal: 1617.517214818\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch440.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -1632.756470\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -1593.161987\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -1435.509155\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -1861.411133\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -1556.371094\n",
      "    epoch          : 441\n",
      "    loss           : -1571.1769681118503\n",
      "    val_loss       : -1571.6365407260394\n",
      "    val_log_likelihood: 1672.158637018487\n",
      "    val_log_marginal: 1621.2186944639438\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -1486.182861\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -1501.648682\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -1487.574951\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -1622.106323\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -1469.838257\n",
      "    epoch          : 442\n",
      "    loss           : -1565.4399293200804\n",
      "    val_loss       : -1563.2883367052043\n",
      "    val_log_likelihood: 1667.6206743599164\n",
      "    val_log_marginal: 1617.5582492866342\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -1864.235107\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -1621.232910\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -1447.707275\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -1499.888794\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -1487.930054\n",
      "    epoch          : 443\n",
      "    loss           : -1561.614446356745\n",
      "    val_loss       : -1565.5873201852203\n",
      "    val_log_likelihood: 1668.8482895652846\n",
      "    val_log_marginal: 1617.1504880008242\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -1885.174927\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -1445.725342\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -1438.188477\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -1396.603760\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -1529.718750\n",
      "    epoch          : 444\n",
      "    loss           : -1575.2068004041614\n",
      "    val_loss       : -1572.3794116319646\n",
      "    val_log_likelihood: 1668.8516132619122\n",
      "    val_log_marginal: 1618.3052871263594\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -1860.468018\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -1569.838257\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -1401.586304\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -1429.642578\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -1544.704712\n",
      "    epoch          : 445\n",
      "    loss           : -1570.2554049350247\n",
      "    val_loss       : -1576.668206651649\n",
      "    val_log_likelihood: 1672.7730519511913\n",
      "    val_log_marginal: 1623.4334570475032\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -1859.873047\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -1527.971924\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -1433.850464\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -1535.577393\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -1448.110596\n",
      "    epoch          : 446\n",
      "    loss           : -1559.2726144318533\n",
      "    val_loss       : -1533.816203329002\n",
      "    val_log_likelihood: 1609.4071987643101\n",
      "    val_log_marginal: 1557.9565070149222\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -1832.341919\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -1618.254639\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -1534.555542\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -1559.803467\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -1528.443237\n",
      "    epoch          : 447\n",
      "    loss           : -1541.824141398515\n",
      "    val_loss       : -1550.1845200348755\n",
      "    val_log_likelihood: 1624.707737082302\n",
      "    val_log_marginal: 1572.6386464799161\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -1846.900879\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -1472.325195\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -1418.545166\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -1529.750000\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -1549.454834\n",
      "    epoch          : 448\n",
      "    loss           : -1551.0768027730508\n",
      "    val_loss       : -1557.6323199615085\n",
      "    val_log_likelihood: 1648.1465871074413\n",
      "    val_log_marginal: 1595.6637246059715\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -1501.190918\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -1436.840698\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -1518.464111\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -1477.122681\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -1531.941406\n",
      "    epoch          : 449\n",
      "    loss           : -1560.0922972424196\n",
      "    val_loss       : -1561.3277645161431\n",
      "    val_log_likelihood: 1647.1183888652538\n",
      "    val_log_marginal: 1598.7405859826704\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -1492.180176\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -1627.914917\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -1440.737793\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -1529.539307\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -1538.798584\n",
      "    epoch          : 450\n",
      "    loss           : -1559.9958423576732\n",
      "    val_loss       : -1559.1156340794416\n",
      "    val_log_likelihood: 1644.0570938563583\n",
      "    val_log_marginal: 1591.620613760822\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -1852.827148\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -1563.333984\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -1541.008301\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -1543.735229\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -1627.924438\n",
      "    epoch          : 451\n",
      "    loss           : -1564.747505414604\n",
      "    val_loss       : -1563.9897955767533\n",
      "    val_log_likelihood: 1661.032899762144\n",
      "    val_log_marginal: 1608.908896967787\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -1855.127075\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -1481.815430\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -1513.149048\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -1590.833374\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -1500.511597\n",
      "    epoch          : 452\n",
      "    loss           : -1567.4643470084313\n",
      "    val_loss       : -1573.5002488033883\n",
      "    val_log_likelihood: 1664.3604397915378\n",
      "    val_log_marginal: 1613.071739486225\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -1874.222534\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -1508.252686\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -1472.808838\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -1624.241821\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -1516.143677\n",
      "    epoch          : 453\n",
      "    loss           : -1566.8616375309407\n",
      "    val_loss       : -1563.2813032940767\n",
      "    val_log_likelihood: 1646.530188834313\n",
      "    val_log_marginal: 1597.252231387901\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -1883.865234\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -1631.634033\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -1539.215820\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -1550.363892\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -1600.595093\n",
      "    epoch          : 454\n",
      "    loss           : -1566.5850080735613\n",
      "    val_loss       : -1567.463127808378\n",
      "    val_log_likelihood: 1661.9435527537128\n",
      "    val_log_marginal: 1611.7013719499184\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -1858.423828\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -1490.793213\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -1480.119141\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -1573.300903\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -1455.371460\n",
      "    epoch          : 455\n",
      "    loss           : -1568.0227969330135\n",
      "    val_loss       : -1564.1539224198568\n",
      "    val_log_likelihood: 1665.144231512995\n",
      "    val_log_marginal: 1614.6523110518176\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -1890.697876\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -1631.335205\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -1521.497559\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -1485.362549\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -1635.209961\n",
      "    epoch          : 456\n",
      "    loss           : -1570.0340842067606\n",
      "    val_loss       : -1569.718509629756\n",
      "    val_log_likelihood: 1653.0540844001393\n",
      "    val_log_marginal: 1604.055792144823\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -1866.932251\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -1526.195923\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -1424.063477\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -1601.354492\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -1582.161865\n",
      "    epoch          : 457\n",
      "    loss           : -1567.0546850827661\n",
      "    val_loss       : -1572.6717109990648\n",
      "    val_log_likelihood: 1665.81575117961\n",
      "    val_log_marginal: 1614.994564766138\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -1622.409668\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -1496.404419\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -1481.115479\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -1855.479492\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -1444.062866\n",
      "    epoch          : 458\n",
      "    loss           : -1566.2086689259747\n",
      "    val_loss       : -1571.5179459200342\n",
      "    val_log_likelihood: 1664.4848415261447\n",
      "    val_log_marginal: 1613.2197426258595\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -1869.517090\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -1544.145630\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -1435.320801\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -1541.256226\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -1565.520996\n",
      "    epoch          : 459\n",
      "    loss           : -1567.9782690671411\n",
      "    val_loss       : -1572.2158611461907\n",
      "    val_log_likelihood: 1661.7652491201268\n",
      "    val_log_marginal: 1612.7480188084755\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -1862.172607\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -1574.293701\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -1532.239624\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -1461.636719\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -1446.061890\n",
      "    epoch          : 460\n",
      "    loss           : -1569.6444176400062\n",
      "    val_loss       : -1567.5365788313181\n",
      "    val_log_likelihood: 1667.7452005820699\n",
      "    val_log_marginal: 1614.777929108387\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch460.pth ...\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -1886.282593\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -1513.139526\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -1456.719727\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -1618.220947\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -1548.459961\n",
      "    epoch          : 461\n",
      "    loss           : -1577.7847126875774\n",
      "    val_loss       : -1580.636474351915\n",
      "    val_log_likelihood: 1673.4850167756033\n",
      "    val_log_marginal: 1623.02729462322\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -1899.567505\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -1461.930786\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -1554.384399\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -1852.646240\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -1408.377563\n",
      "    epoch          : 462\n",
      "    loss           : -1578.1699931834003\n",
      "    val_loss       : -1577.0305782268654\n",
      "    val_log_likelihood: 1674.1802229173113\n",
      "    val_log_marginal: 1626.8806618521407\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -1841.787231\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -1500.773560\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -1559.613892\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -1550.479858\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -1622.184204\n",
      "    epoch          : 463\n",
      "    loss           : -1575.0287650835396\n",
      "    val_loss       : -1575.6513300929312\n",
      "    val_log_likelihood: 1670.934676670792\n",
      "    val_log_marginal: 1618.8911337569025\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -1860.505005\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -1536.763184\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -1543.029419\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -1837.201904\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -1488.216431\n",
      "    epoch          : 464\n",
      "    loss           : -1571.8619735264542\n",
      "    val_loss       : -1571.7240274688506\n",
      "    val_log_likelihood: 1669.9772224048577\n",
      "    val_log_marginal: 1615.4433747951016\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -1827.069580\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -1641.881348\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -1528.095947\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -1393.582031\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -1538.069092\n",
      "    epoch          : 465\n",
      "    loss           : -1570.8943016127785\n",
      "    val_loss       : -1550.5730450969227\n",
      "    val_log_likelihood: 1636.5060757174351\n",
      "    val_log_marginal: 1577.6719244286191\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -1843.453369\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -1597.833618\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -1401.789551\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -1421.057739\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -1552.262207\n",
      "    epoch          : 466\n",
      "    loss           : -1558.3321593633973\n",
      "    val_loss       : -1565.5673066205668\n",
      "    val_log_likelihood: 1666.139785011216\n",
      "    val_log_marginal: 1606.2156624690915\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -1874.831909\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -1681.915405\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -1414.126465\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -1563.174805\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -1504.666260\n",
      "    epoch          : 467\n",
      "    loss           : -1569.590328405399\n",
      "    val_loss       : -1572.1903963801037\n",
      "    val_log_likelihood: 1672.1785030553838\n",
      "    val_log_marginal: 1617.6473816776868\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -1848.898193\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -1550.543213\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -1647.156250\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -1401.130737\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -1520.453979\n",
      "    epoch          : 468\n",
      "    loss           : -1578.9306278039912\n",
      "    val_loss       : -1579.5044964960416\n",
      "    val_log_likelihood: 1685.1619292910736\n",
      "    val_log_marginal: 1630.4921924190246\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -1861.329102\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -1509.925049\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -1636.928711\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -1530.656616\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -1472.215454\n",
      "    epoch          : 469\n",
      "    loss           : -1580.8769676284035\n",
      "    val_loss       : -1579.3165037191316\n",
      "    val_log_likelihood: 1681.8380912554146\n",
      "    val_log_marginal: 1629.8839888597959\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -1864.034302\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -1680.747559\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -1546.722412\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -1640.372070\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -1521.801270\n",
      "    epoch          : 470\n",
      "    loss           : -1581.5252105410736\n",
      "    val_loss       : -1579.7971304982514\n",
      "    val_log_likelihood: 1681.917533647896\n",
      "    val_log_marginal: 1628.819199853844\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch470.pth ...\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -1860.537476\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -1641.216675\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -1589.972046\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -1582.231201\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -1536.648804\n",
      "    epoch          : 471\n",
      "    loss           : -1573.812692170096\n",
      "    val_loss       : -1579.1103235120784\n",
      "    val_log_likelihood: 1677.6726448890006\n",
      "    val_log_marginal: 1625.8292955777144\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -1878.602295\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -1493.803345\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -1496.546631\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -1599.229004\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -1478.176392\n",
      "    epoch          : 472\n",
      "    loss           : -1578.7721696608137\n",
      "    val_loss       : -1575.851385653231\n",
      "    val_log_likelihood: 1665.4186878770886\n",
      "    val_log_marginal: 1615.227816670635\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -1870.515869\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -1516.884888\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -1422.085571\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -1443.745117\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -1572.555786\n",
      "    epoch          : 473\n",
      "    loss           : -1571.5882145343442\n",
      "    val_loss       : -1571.9523443565565\n",
      "    val_log_likelihood: 1671.2740961962406\n",
      "    val_log_marginal: 1614.7041357428402\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -1827.726807\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -1496.469238\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -1430.843628\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -1593.662598\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -1476.606201\n",
      "    epoch          : 474\n",
      "    loss           : -1574.3890598410428\n",
      "    val_loss       : -1580.1597186405913\n",
      "    val_log_likelihood: 1675.503177453976\n",
      "    val_log_marginal: 1623.2027678008558\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -1871.674683\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -1612.120605\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -1461.434814\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -1839.257690\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -1563.902222\n",
      "    epoch          : 475\n",
      "    loss           : -1578.5298020768873\n",
      "    val_loss       : -1571.9732588445306\n",
      "    val_log_likelihood: 1679.6177410465657\n",
      "    val_log_marginal: 1628.1689575998905\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -1466.084229\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -1642.057129\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -1578.968140\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -1424.474731\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -1545.452393\n",
      "    epoch          : 476\n",
      "    loss           : -1576.6707703241027\n",
      "    val_loss       : -1575.3071412231848\n",
      "    val_log_likelihood: 1676.6048306002476\n",
      "    val_log_marginal: 1626.4300006197416\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -1838.623901\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -1493.345337\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -1592.586060\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -1878.837891\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -1541.062134\n",
      "    epoch          : 477\n",
      "    loss           : -1576.7901538811107\n",
      "    val_loss       : -1583.2320090092921\n",
      "    val_log_likelihood: 1684.768258576346\n",
      "    val_log_marginal: 1630.7840986347637\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -1876.074341\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -1463.737305\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -1417.272949\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -1496.199951\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -1575.672241\n",
      "    epoch          : 478\n",
      "    loss           : -1571.2235820505878\n",
      "    val_loss       : -1571.2281705671737\n",
      "    val_log_likelihood: 1678.830431041151\n",
      "    val_log_marginal: 1626.5771760469358\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -1825.563599\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -1511.796021\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -1545.433350\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -1450.708618\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -1471.195801\n",
      "    epoch          : 479\n",
      "    loss           : -1574.6621782661664\n",
      "    val_loss       : -1575.606352651123\n",
      "    val_log_likelihood: 1679.5503920753404\n",
      "    val_log_marginal: 1627.7807336523206\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -1821.725098\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -1474.405396\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -1494.623657\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -1601.447876\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -1528.459351\n",
      "    epoch          : 480\n",
      "    loss           : -1578.656359984143\n",
      "    val_loss       : -1581.922132894535\n",
      "    val_log_likelihood: 1689.558863271581\n",
      "    val_log_marginal: 1637.0504192357128\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch480.pth ...\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -1858.579346\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -1556.412354\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -1553.366699\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -1845.037354\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -1537.942139\n",
      "    epoch          : 481\n",
      "    loss           : -1574.063829478651\n",
      "    val_loss       : -1573.8809973664563\n",
      "    val_log_likelihood: 1668.3604833017482\n",
      "    val_log_marginal: 1615.319628475861\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -1881.780029\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -1477.209106\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -1585.312378\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -1549.447510\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -1579.647217\n",
      "    epoch          : 482\n",
      "    loss           : -1575.211152633818\n",
      "    val_loss       : -1582.6949267502375\n",
      "    val_log_likelihood: 1687.368085502398\n",
      "    val_log_marginal: 1635.3665489608072\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -1848.429443\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -1495.306763\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -1453.593262\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -1474.634277\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -1482.923706\n",
      "    epoch          : 483\n",
      "    loss           : -1578.8475825243656\n",
      "    val_loss       : -1567.1835590859894\n",
      "    val_log_likelihood: 1679.3075809289912\n",
      "    val_log_marginal: 1626.2579354410161\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -1658.329590\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -1481.891968\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -1433.536743\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -1614.059814\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -1600.477783\n",
      "    epoch          : 484\n",
      "    loss           : -1575.8721355778157\n",
      "    val_loss       : -1579.2560352913686\n",
      "    val_log_likelihood: 1678.8218196453433\n",
      "    val_log_marginal: 1627.3506120832656\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -1843.253418\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -1539.580078\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -1570.444458\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -1862.303955\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -1577.459961\n",
      "    epoch          : 485\n",
      "    loss           : -1572.1347607905323\n",
      "    val_loss       : -1568.5521678371208\n",
      "    val_log_likelihood: 1660.2905841487468\n",
      "    val_log_marginal: 1605.2246515885342\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -1666.214722\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -1519.032227\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -1579.583008\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -1474.866455\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -1559.845215\n",
      "    epoch          : 486\n",
      "    loss           : -1572.1507894685953\n",
      "    val_loss       : -1572.7660152012445\n",
      "    val_log_likelihood: 1670.9262719484839\n",
      "    val_log_marginal: 1616.168720532264\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -1873.408936\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -1683.595215\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -1456.044312\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -1476.012085\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -1609.143799\n",
      "    epoch          : 487\n",
      "    loss           : -1577.1347039855352\n",
      "    val_loss       : -1581.868429331653\n",
      "    val_log_likelihood: 1675.9950603825032\n",
      "    val_log_marginal: 1624.4109887666816\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -1872.001709\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -1641.331543\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -1637.064331\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -1465.701294\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -1576.187134\n",
      "    epoch          : 488\n",
      "    loss           : -1581.0006586962406\n",
      "    val_loss       : -1579.50626471339\n",
      "    val_log_likelihood: 1687.7697802250927\n",
      "    val_log_marginal: 1632.620664501157\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -1516.733032\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -1675.111938\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -1472.699219\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -1599.803711\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -1548.001465\n",
      "    epoch          : 489\n",
      "    loss           : -1577.3732462967976\n",
      "    val_loss       : -1582.5184468195432\n",
      "    val_log_likelihood: 1685.4717497872834\n",
      "    val_log_marginal: 1631.0068302214192\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -1872.368164\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -1491.238037\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -1528.700684\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -1880.485718\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -1518.202637\n",
      "    epoch          : 490\n",
      "    loss           : -1582.8838760829208\n",
      "    val_loss       : -1582.2066139364247\n",
      "    val_log_likelihood: 1685.163465443224\n",
      "    val_log_marginal: 1631.7189528307033\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch490.pth ...\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -1826.145996\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -1473.190674\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -1539.037842\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -1883.152100\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -1581.373169\n",
      "    epoch          : 491\n",
      "    loss           : -1580.6342277904548\n",
      "    val_loss       : -1587.2204468819764\n",
      "    val_log_likelihood: 1689.1751902363087\n",
      "    val_log_marginal: 1635.5338897323882\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -1855.480713\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -1567.748291\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -1527.356445\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -1646.938721\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -1595.661499\n",
      "    epoch          : 492\n",
      "    loss           : -1574.7707942547183\n",
      "    val_loss       : -1564.5939447378075\n",
      "    val_log_likelihood: 1661.1431135423113\n",
      "    val_log_marginal: 1606.3950163920658\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -1839.187012\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -1571.198853\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -1493.453979\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -1803.834839\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -1478.057129\n",
      "    epoch          : 493\n",
      "    loss           : -1566.489667533648\n",
      "    val_loss       : -1575.4222423239787\n",
      "    val_log_likelihood: 1674.491887762995\n",
      "    val_log_marginal: 1621.2077122817388\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -1660.289795\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -1482.642578\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -1470.879517\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -1627.964111\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -1528.681274\n",
      "    epoch          : 494\n",
      "    loss           : -1576.409153097927\n",
      "    val_loss       : -1584.1914033088372\n",
      "    val_log_likelihood: 1677.8116201268565\n",
      "    val_log_marginal: 1625.5563375285394\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -1881.406860\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -1529.185059\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -1511.738525\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -1880.141724\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -1575.905029\n",
      "    epoch          : 495\n",
      "    loss           : -1583.9308356861077\n",
      "    val_loss       : -1566.2891927654532\n",
      "    val_log_likelihood: 1650.4085995513615\n",
      "    val_log_marginal: 1595.3888601854924\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -1619.595337\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -1656.753906\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -1463.597778\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -1607.609985\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -1508.935669\n",
      "    epoch          : 496\n",
      "    loss           : -1569.7793198387221\n",
      "    val_loss       : -1566.1920080254463\n",
      "    val_log_likelihood: 1641.5904964031558\n",
      "    val_log_marginal: 1589.9734284495717\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -1866.732544\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -1507.045044\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -1495.902100\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -1481.951904\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -1630.463989\n",
      "    epoch          : 497\n",
      "    loss           : -1561.9111678623917\n",
      "    val_loss       : -1554.9914434496498\n",
      "    val_log_likelihood: 1648.497041305693\n",
      "    val_log_marginal: 1592.2839168483704\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -1822.911621\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -1442.751221\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -1562.394043\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -1558.728027\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -1560.102783\n",
      "    epoch          : 498\n",
      "    loss           : -1558.775553788289\n",
      "    val_loss       : -1568.9029055812055\n",
      "    val_log_likelihood: 1650.1005665996288\n",
      "    val_log_marginal: 1596.035596281456\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -1838.227173\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -1457.679688\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -1617.321533\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -1506.275879\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -1563.958252\n",
      "    epoch          : 499\n",
      "    loss           : -1573.6491807994275\n",
      "    val_loss       : -1581.631329740416\n",
      "    val_log_likelihood: 1672.9585939917233\n",
      "    val_log_marginal: 1618.8013250520487\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -1856.404175\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -1487.291504\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -1553.558716\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -1481.120361\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -1552.603760\n",
      "    epoch          : 500\n",
      "    loss           : -1578.6916310527538\n",
      "    val_loss       : -1585.9536151055818\n",
      "    val_log_likelihood: 1676.0560907042852\n",
      "    val_log_marginal: 1624.191322447769\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch500.pth ...\n",
      "Train Epoch: 501 [512/54000 (1%)] Loss: -1874.317505\n",
      "Train Epoch: 501 [11776/54000 (22%)] Loss: -1652.523560\n",
      "Train Epoch: 501 [23040/54000 (43%)] Loss: -1619.678467\n",
      "Train Epoch: 501 [34304/54000 (64%)] Loss: -1538.713013\n",
      "Train Epoch: 501 [45568/54000 (84%)] Loss: -1576.319580\n",
      "    epoch          : 501\n",
      "    loss           : -1580.8893462832611\n",
      "    val_loss       : -1570.4850559007505\n",
      "    val_log_likelihood: 1671.2398077332148\n",
      "    val_log_marginal: 1620.0049445983022\n",
      "Train Epoch: 502 [512/54000 (1%)] Loss: -1843.949951\n",
      "Train Epoch: 502 [11776/54000 (22%)] Loss: -1658.908447\n",
      "Train Epoch: 502 [23040/54000 (43%)] Loss: -1624.597656\n",
      "Train Epoch: 502 [34304/54000 (64%)] Loss: -1553.509766\n",
      "Train Epoch: 502 [45568/54000 (84%)] Loss: -1545.035156\n",
      "    epoch          : 502\n",
      "    loss           : -1575.069093005492\n",
      "    val_loss       : -1565.8409624514954\n",
      "    val_log_likelihood: 1657.9723915628867\n",
      "    val_log_marginal: 1600.403423915645\n",
      "Train Epoch: 503 [512/54000 (1%)] Loss: -1842.592651\n",
      "Train Epoch: 503 [11776/54000 (22%)] Loss: -1529.095703\n",
      "Train Epoch: 503 [23040/54000 (43%)] Loss: -1432.598267\n",
      "Train Epoch: 503 [34304/54000 (64%)] Loss: -1477.856689\n",
      "Train Epoch: 503 [45568/54000 (84%)] Loss: -1566.913452\n",
      "    epoch          : 503\n",
      "    loss           : -1571.3252412399443\n",
      "    val_loss       : -1578.3996659529553\n",
      "    val_log_likelihood: 1673.785914052831\n",
      "    val_log_marginal: 1618.551568924261\n",
      "Train Epoch: 504 [512/54000 (1%)] Loss: -1839.714478\n",
      "Train Epoch: 504 [11776/54000 (22%)] Loss: -1543.458252\n",
      "Train Epoch: 504 [23040/54000 (43%)] Loss: -1471.278442\n",
      "Train Epoch: 504 [34304/54000 (64%)] Loss: -1474.322754\n",
      "Train Epoch: 504 [45568/54000 (84%)] Loss: -1497.896729\n",
      "    epoch          : 504\n",
      "    loss           : -1582.985298383354\n",
      "    val_loss       : -1583.8184271235905\n",
      "    val_log_likelihood: 1671.9622005047183\n",
      "    val_log_marginal: 1621.3409589030339\n",
      "Train Epoch: 505 [512/54000 (1%)] Loss: -1872.839478\n",
      "Train Epoch: 505 [11776/54000 (22%)] Loss: -1640.849731\n",
      "Train Epoch: 505 [23040/54000 (43%)] Loss: -1610.420776\n",
      "Train Epoch: 505 [34304/54000 (64%)] Loss: -1541.016479\n",
      "Train Epoch: 505 [45568/54000 (84%)] Loss: -1563.099731\n",
      "    epoch          : 505\n",
      "    loss           : -1577.3617789391244\n",
      "    val_loss       : -1578.8229746992452\n",
      "    val_log_likelihood: 1668.723846737701\n",
      "    val_log_marginal: 1613.6362474711777\n",
      "Train Epoch: 506 [512/54000 (1%)] Loss: -1832.500000\n",
      "Train Epoch: 506 [11776/54000 (22%)] Loss: -1543.677490\n",
      "Train Epoch: 506 [23040/54000 (43%)] Loss: -1445.797485\n",
      "Train Epoch: 506 [34304/54000 (64%)] Loss: -1423.365112\n",
      "Train Epoch: 506 [45568/54000 (84%)] Loss: -1557.227295\n",
      "    epoch          : 506\n",
      "    loss           : -1570.1791339534343\n",
      "    val_loss       : -1572.3973948021262\n",
      "    val_log_likelihood: 1662.2419228128867\n",
      "    val_log_marginal: 1602.1565652029901\n",
      "Train Epoch: 507 [512/54000 (1%)] Loss: -1668.826904\n",
      "Train Epoch: 507 [11776/54000 (22%)] Loss: -1504.297119\n",
      "Train Epoch: 507 [23040/54000 (43%)] Loss: -1422.607910\n",
      "Train Epoch: 507 [34304/54000 (64%)] Loss: -1590.009277\n",
      "Train Epoch: 507 [45568/54000 (84%)] Loss: -1563.833862\n",
      "    epoch          : 507\n",
      "    loss           : -1575.6855674214883\n",
      "    val_loss       : -1583.6793221519374\n",
      "    val_log_likelihood: 1676.1547658183788\n",
      "    val_log_marginal: 1618.741412730076\n",
      "Train Epoch: 508 [512/54000 (1%)] Loss: -1875.392822\n",
      "Train Epoch: 508 [11776/54000 (22%)] Loss: -1537.275146\n",
      "Train Epoch: 508 [23040/54000 (43%)] Loss: -1456.323975\n",
      "Train Epoch: 508 [34304/54000 (64%)] Loss: -1505.958740\n",
      "Train Epoch: 508 [45568/54000 (84%)] Loss: -1548.589966\n",
      "    epoch          : 508\n",
      "    loss           : -1581.1537372853497\n",
      "    val_loss       : -1576.8097480168879\n",
      "    val_log_likelihood: 1663.0929269318533\n",
      "    val_log_marginal: 1609.8269295116868\n",
      "Train Epoch: 509 [512/54000 (1%)] Loss: -1875.965210\n",
      "Train Epoch: 509 [11776/54000 (22%)] Loss: -1506.064087\n",
      "Train Epoch: 509 [23040/54000 (43%)] Loss: -1583.642212\n",
      "Train Epoch: 509 [34304/54000 (64%)] Loss: -1475.479004\n",
      "Train Epoch: 509 [45568/54000 (84%)] Loss: -1528.403687\n",
      "    epoch          : 509\n",
      "    loss           : -1575.6190088857518\n",
      "    val_loss       : -1578.2296481715368\n",
      "    val_log_likelihood: 1674.9432276357518\n",
      "    val_log_marginal: 1620.593156847568\n",
      "Train Epoch: 510 [512/54000 (1%)] Loss: -1862.437378\n",
      "Train Epoch: 510 [11776/54000 (22%)] Loss: -1677.569092\n",
      "Train Epoch: 510 [23040/54000 (43%)] Loss: -1637.617798\n",
      "Train Epoch: 510 [34304/54000 (64%)] Loss: -1504.830322\n",
      "Train Epoch: 510 [45568/54000 (84%)] Loss: -1495.139404\n",
      "    epoch          : 510\n",
      "    loss           : -1578.2243023862934\n",
      "    val_loss       : -1582.7635927789447\n",
      "    val_log_likelihood: 1678.11219349474\n",
      "    val_log_marginal: 1625.5985209773858\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch510.pth ...\n",
      "Train Epoch: 511 [512/54000 (1%)] Loss: -1858.891357\n",
      "Train Epoch: 511 [11776/54000 (22%)] Loss: -1502.767822\n",
      "Train Epoch: 511 [23040/54000 (43%)] Loss: -1463.101562\n",
      "Train Epoch: 511 [34304/54000 (64%)] Loss: -1584.792725\n",
      "Train Epoch: 511 [45568/54000 (84%)] Loss: -1554.472656\n",
      "    epoch          : 511\n",
      "    loss           : -1586.869115244044\n",
      "    val_loss       : -1587.1223270177177\n",
      "    val_log_likelihood: 1681.6909324721535\n",
      "    val_log_marginal: 1628.5535223989905\n",
      "Train Epoch: 512 [512/54000 (1%)] Loss: -1865.911377\n",
      "Train Epoch: 512 [11776/54000 (22%)] Loss: -1529.958862\n",
      "Train Epoch: 512 [23040/54000 (43%)] Loss: -1562.513550\n",
      "Train Epoch: 512 [34304/54000 (64%)] Loss: -1636.143066\n",
      "Train Epoch: 512 [45568/54000 (84%)] Loss: -1637.838257\n",
      "    epoch          : 512\n",
      "    loss           : -1588.8172305267635\n",
      "    val_loss       : -1582.7673448313876\n",
      "    val_log_likelihood: 1692.0806800162438\n",
      "    val_log_marginal: 1638.684365058055\n",
      "Train Epoch: 513 [512/54000 (1%)] Loss: -1839.964844\n",
      "Train Epoch: 513 [11776/54000 (22%)] Loss: -1457.752686\n",
      "Train Epoch: 513 [23040/54000 (43%)] Loss: -1668.515625\n",
      "Train Epoch: 513 [34304/54000 (64%)] Loss: -1500.879761\n",
      "Train Epoch: 513 [45568/54000 (84%)] Loss: -1492.565430\n",
      "    epoch          : 513\n",
      "    loss           : -1592.8616955445545\n",
      "    val_loss       : -1585.9544092634933\n",
      "    val_log_likelihood: 1695.5368495223545\n",
      "    val_log_marginal: 1641.1120723904764\n",
      "Train Epoch: 514 [512/54000 (1%)] Loss: -1861.049561\n",
      "Train Epoch: 514 [11776/54000 (22%)] Loss: -1543.186646\n",
      "Train Epoch: 514 [23040/54000 (43%)] Loss: -1498.391846\n",
      "Train Epoch: 514 [34304/54000 (64%)] Loss: -1435.518188\n",
      "Train Epoch: 514 [45568/54000 (84%)] Loss: -1550.947266\n",
      "    epoch          : 514\n",
      "    loss           : -1587.0472387937036\n",
      "    val_loss       : -1591.2988280706697\n",
      "    val_log_likelihood: 1687.8253512240872\n",
      "    val_log_marginal: 1634.7920454027492\n",
      "Train Epoch: 515 [512/54000 (1%)] Loss: -1870.900879\n",
      "Train Epoch: 515 [11776/54000 (22%)] Loss: -1538.852905\n",
      "Train Epoch: 515 [23040/54000 (43%)] Loss: -1445.142334\n",
      "Train Epoch: 515 [34304/54000 (64%)] Loss: -1589.595337\n",
      "Train Epoch: 515 [45568/54000 (84%)] Loss: -1500.976196\n",
      "    epoch          : 515\n",
      "    loss           : -1586.3130837619894\n",
      "    val_loss       : -1589.6569910409612\n",
      "    val_log_likelihood: 1688.8419830020111\n",
      "    val_log_marginal: 1635.2647197297267\n",
      "Train Epoch: 516 [512/54000 (1%)] Loss: -1847.510986\n",
      "Train Epoch: 516 [11776/54000 (22%)] Loss: -1690.317871\n",
      "Train Epoch: 516 [23040/54000 (43%)] Loss: -1570.358643\n",
      "Train Epoch: 516 [34304/54000 (64%)] Loss: -1835.423706\n",
      "Train Epoch: 516 [45568/54000 (84%)] Loss: -1576.879272\n",
      "    epoch          : 516\n",
      "    loss           : -1578.1617673364017\n",
      "    val_loss       : -1581.6522380907716\n",
      "    val_log_likelihood: 1682.1358388768565\n",
      "    val_log_marginal: 1627.3978969301052\n",
      "Train Epoch: 517 [512/54000 (1%)] Loss: -1870.591797\n",
      "Train Epoch: 517 [11776/54000 (22%)] Loss: -1510.752686\n",
      "Train Epoch: 517 [23040/54000 (43%)] Loss: -1474.515747\n",
      "Train Epoch: 517 [34304/54000 (64%)] Loss: -1848.256226\n",
      "Train Epoch: 517 [45568/54000 (84%)] Loss: -1459.233887\n",
      "    epoch          : 517\n",
      "    loss           : -1587.1946550123762\n",
      "    val_loss       : -1584.4769355492888\n",
      "    val_log_likelihood: 1684.4811504099628\n",
      "    val_log_marginal: 1630.0714805575992\n",
      "Train Epoch: 518 [512/54000 (1%)] Loss: -1693.816162\n",
      "Train Epoch: 518 [11776/54000 (22%)] Loss: -1676.379639\n",
      "Train Epoch: 518 [23040/54000 (43%)] Loss: -1669.494019\n",
      "Train Epoch: 518 [34304/54000 (64%)] Loss: -1583.785767\n",
      "Train Epoch: 518 [45568/54000 (84%)] Loss: -1587.383545\n",
      "    epoch          : 518\n",
      "    loss           : -1585.9474420830754\n",
      "    val_loss       : -1587.1276981752403\n",
      "    val_log_likelihood: 1681.7949279180848\n",
      "    val_log_marginal: 1627.905379126623\n",
      "Train Epoch: 519 [512/54000 (1%)] Loss: -1863.771851\n",
      "Train Epoch: 519 [11776/54000 (22%)] Loss: -1491.145752\n",
      "Train Epoch: 519 [23040/54000 (43%)] Loss: -1455.185303\n",
      "Train Epoch: 519 [34304/54000 (64%)] Loss: -1443.205078\n",
      "Train Epoch: 519 [45568/54000 (84%)] Loss: -1502.595215\n",
      "    epoch          : 519\n",
      "    loss           : -1580.699896784112\n",
      "    val_loss       : -1584.538515131856\n",
      "    val_log_likelihood: 1680.578309918394\n",
      "    val_log_marginal: 1628.0969143908396\n",
      "Train Epoch: 520 [512/54000 (1%)] Loss: -1852.156982\n",
      "Train Epoch: 520 [11776/54000 (22%)] Loss: -1526.589111\n",
      "Train Epoch: 520 [23040/54000 (43%)] Loss: -1468.505981\n",
      "Train Epoch: 520 [34304/54000 (64%)] Loss: -1539.261475\n",
      "Train Epoch: 520 [45568/54000 (84%)] Loss: -1586.830811\n",
      "    epoch          : 520\n",
      "    loss           : -1584.5384231416306\n",
      "    val_loss       : -1588.8102660745308\n",
      "    val_log_likelihood: 1688.0267285639698\n",
      "    val_log_marginal: 1635.4915834181195\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch520.pth ...\n",
      "Train Epoch: 521 [512/54000 (1%)] Loss: -1856.194336\n",
      "Train Epoch: 521 [11776/54000 (22%)] Loss: -1562.094849\n",
      "Train Epoch: 521 [23040/54000 (43%)] Loss: -1675.091919\n",
      "Train Epoch: 521 [34304/54000 (64%)] Loss: -1570.331177\n",
      "Train Epoch: 521 [45568/54000 (84%)] Loss: -1519.929443\n",
      "    epoch          : 521\n",
      "    loss           : -1592.7343858775525\n",
      "    val_loss       : -1589.3205209908417\n",
      "    val_log_likelihood: 1687.7150455890317\n",
      "    val_log_marginal: 1633.2398522879173\n",
      "Train Epoch: 522 [512/54000 (1%)] Loss: -1859.624512\n",
      "Train Epoch: 522 [11776/54000 (22%)] Loss: -1501.699341\n",
      "Train Epoch: 522 [23040/54000 (43%)] Loss: -1451.409912\n",
      "Train Epoch: 522 [34304/54000 (64%)] Loss: -1521.843872\n",
      "Train Epoch: 522 [45568/54000 (84%)] Loss: -1572.542358\n",
      "    epoch          : 522\n",
      "    loss           : -1585.8402522625308\n",
      "    val_loss       : -1590.7010840742507\n",
      "    val_log_likelihood: 1688.9245992226176\n",
      "    val_log_marginal: 1636.8861823293132\n",
      "Train Epoch: 523 [512/54000 (1%)] Loss: -1887.267334\n",
      "Train Epoch: 523 [11776/54000 (22%)] Loss: -1662.396118\n",
      "Train Epoch: 523 [23040/54000 (43%)] Loss: -1554.316650\n",
      "Train Epoch: 523 [34304/54000 (64%)] Loss: -1858.252930\n",
      "Train Epoch: 523 [45568/54000 (84%)] Loss: -1608.636230\n",
      "    epoch          : 523\n",
      "    loss           : -1590.6718363242574\n",
      "    val_loss       : -1594.729543814159\n",
      "    val_log_likelihood: 1694.7026753944926\n",
      "    val_log_marginal: 1640.6874841506788\n",
      "Train Epoch: 524 [512/54000 (1%)] Loss: -1882.284180\n",
      "Train Epoch: 524 [11776/54000 (22%)] Loss: -1703.996216\n",
      "Train Epoch: 524 [23040/54000 (43%)] Loss: -1674.305542\n",
      "Train Epoch: 524 [34304/54000 (64%)] Loss: -1512.882324\n",
      "Train Epoch: 524 [45568/54000 (84%)] Loss: -1639.215210\n",
      "    epoch          : 524\n",
      "    loss           : -1593.565293113784\n",
      "    val_loss       : -1594.2053735155594\n",
      "    val_log_likelihood: 1694.5420659131344\n",
      "    val_log_marginal: 1642.3289324659486\n",
      "Train Epoch: 525 [512/54000 (1%)] Loss: -1888.406738\n",
      "Train Epoch: 525 [11776/54000 (22%)] Loss: -1609.398682\n",
      "Train Epoch: 525 [23040/54000 (43%)] Loss: -1586.171021\n",
      "Train Epoch: 525 [34304/54000 (64%)] Loss: -1564.571289\n",
      "Train Epoch: 525 [45568/54000 (84%)] Loss: -1495.406006\n",
      "    epoch          : 525\n",
      "    loss           : -1596.2162384939666\n",
      "    val_loss       : -1592.7076150609346\n",
      "    val_log_likelihood: 1690.481932385133\n",
      "    val_log_marginal: 1634.55194629015\n",
      "Train Epoch: 526 [512/54000 (1%)] Loss: -1883.512207\n",
      "Train Epoch: 526 [11776/54000 (22%)] Loss: -1619.931152\n",
      "Train Epoch: 526 [23040/54000 (43%)] Loss: -1504.202393\n",
      "Train Epoch: 526 [34304/54000 (64%)] Loss: -1621.259766\n",
      "Train Epoch: 526 [45568/54000 (84%)] Loss: -1563.717529\n",
      "    epoch          : 526\n",
      "    loss           : -1593.4223584467823\n",
      "    val_loss       : -1593.121624092636\n",
      "    val_log_likelihood: 1691.8291885829208\n",
      "    val_log_marginal: 1636.9307224449728\n",
      "Train Epoch: 527 [512/54000 (1%)] Loss: -1861.730225\n",
      "Train Epoch: 527 [11776/54000 (22%)] Loss: -1530.780273\n",
      "Train Epoch: 527 [23040/54000 (43%)] Loss: -1629.571167\n",
      "Train Epoch: 527 [34304/54000 (64%)] Loss: -1589.717041\n",
      "Train Epoch: 527 [45568/54000 (84%)] Loss: -1583.638672\n",
      "    epoch          : 527\n",
      "    loss           : -1599.7084852161975\n",
      "    val_loss       : -1599.239354697886\n",
      "    val_log_likelihood: 1697.9484258972773\n",
      "    val_log_marginal: 1645.0331001211475\n",
      "Train Epoch: 528 [512/54000 (1%)] Loss: -1862.986816\n",
      "Train Epoch: 528 [11776/54000 (22%)] Loss: -1665.592285\n",
      "Train Epoch: 528 [23040/54000 (43%)] Loss: -1499.200928\n",
      "Train Epoch: 528 [34304/54000 (64%)] Loss: -1500.166626\n",
      "Train Epoch: 528 [45568/54000 (84%)] Loss: -1673.408691\n",
      "    epoch          : 528\n",
      "    loss           : -1587.6028992303527\n",
      "    val_loss       : -1593.2136388593492\n",
      "    val_log_likelihood: 1695.3958450166306\n",
      "    val_log_marginal: 1639.0543848991083\n",
      "Train Epoch: 529 [512/54000 (1%)] Loss: -1893.031860\n",
      "Train Epoch: 529 [11776/54000 (22%)] Loss: -1597.411255\n",
      "Train Epoch: 529 [23040/54000 (43%)] Loss: -1420.931763\n",
      "Train Epoch: 529 [34304/54000 (64%)] Loss: -1858.404175\n",
      "Train Epoch: 529 [45568/54000 (84%)] Loss: -1559.516602\n",
      "    epoch          : 529\n",
      "    loss           : -1590.9075891475866\n",
      "    val_loss       : -1590.2359120892454\n",
      "    val_log_likelihood: 1698.5426303372524\n",
      "    val_log_marginal: 1642.6720380267996\n",
      "Train Epoch: 530 [512/54000 (1%)] Loss: -1500.905029\n",
      "Train Epoch: 530 [11776/54000 (22%)] Loss: -1508.098877\n",
      "Train Epoch: 530 [23040/54000 (43%)] Loss: -1437.799805\n",
      "Train Epoch: 530 [34304/54000 (64%)] Loss: -1567.938843\n",
      "Train Epoch: 530 [45568/54000 (84%)] Loss: -1498.054077\n",
      "    epoch          : 530\n",
      "    loss           : -1588.0326822110922\n",
      "    val_loss       : -1594.9906500954512\n",
      "    val_log_likelihood: 1700.4524203763149\n",
      "    val_log_marginal: 1644.658736452894\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch530.pth ...\n",
      "Train Epoch: 531 [512/54000 (1%)] Loss: -1862.950195\n",
      "Train Epoch: 531 [11776/54000 (22%)] Loss: -1504.388550\n",
      "Train Epoch: 531 [23040/54000 (43%)] Loss: -1514.325684\n",
      "Train Epoch: 531 [34304/54000 (64%)] Loss: -1430.250488\n",
      "Train Epoch: 531 [45568/54000 (84%)] Loss: -1611.192383\n",
      "    epoch          : 531\n",
      "    loss           : -1590.0582698406558\n",
      "    val_loss       : -1591.3751991143413\n",
      "    val_log_likelihood: 1694.43060846612\n",
      "    val_log_marginal: 1639.5676933277473\n",
      "Train Epoch: 532 [512/54000 (1%)] Loss: -1871.521484\n",
      "Train Epoch: 532 [11776/54000 (22%)] Loss: -1516.759521\n",
      "Train Epoch: 532 [23040/54000 (43%)] Loss: -1550.985840\n",
      "Train Epoch: 532 [34304/54000 (64%)] Loss: -1477.897949\n",
      "Train Epoch: 532 [45568/54000 (84%)] Loss: -1571.346191\n",
      "    epoch          : 532\n",
      "    loss           : -1590.4978909634128\n",
      "    val_loss       : -1583.7532757756826\n",
      "    val_log_likelihood: 1687.7944976504486\n",
      "    val_log_marginal: 1634.6652122713294\n",
      "Train Epoch: 533 [512/54000 (1%)] Loss: -1894.598633\n",
      "Train Epoch: 533 [11776/54000 (22%)] Loss: -1632.896973\n",
      "Train Epoch: 533 [23040/54000 (43%)] Loss: -1640.527832\n",
      "Train Epoch: 533 [34304/54000 (64%)] Loss: -1450.174805\n",
      "Train Epoch: 533 [45568/54000 (84%)] Loss: -1478.351929\n",
      "    epoch          : 533\n",
      "    loss           : -1590.139407922726\n",
      "    val_loss       : -1580.4241146262236\n",
      "    val_log_likelihood: 1673.1919634224164\n",
      "    val_log_marginal: 1616.6402683608271\n",
      "Train Epoch: 534 [512/54000 (1%)] Loss: -1862.163330\n",
      "Train Epoch: 534 [11776/54000 (22%)] Loss: -1618.882812\n",
      "Train Epoch: 534 [23040/54000 (43%)] Loss: -1559.669434\n",
      "Train Epoch: 534 [34304/54000 (64%)] Loss: -1484.132812\n",
      "Train Epoch: 534 [45568/54000 (84%)] Loss: -1522.742310\n",
      "    epoch          : 534\n",
      "    loss           : -1583.4252192431156\n",
      "    val_loss       : -1588.107776463456\n",
      "    val_log_likelihood: 1684.8991590443225\n",
      "    val_log_marginal: 1629.7153227236522\n",
      "Train Epoch: 535 [512/54000 (1%)] Loss: -1526.802246\n",
      "Train Epoch: 535 [11776/54000 (22%)] Loss: -1576.207397\n",
      "Train Epoch: 535 [23040/54000 (43%)] Loss: -1424.227539\n",
      "Train Epoch: 535 [34304/54000 (64%)] Loss: -1576.471924\n",
      "Train Epoch: 535 [45568/54000 (84%)] Loss: -1572.421143\n",
      "    epoch          : 535\n",
      "    loss           : -1586.2040508005878\n",
      "    val_loss       : -1582.6304000194673\n",
      "    val_log_likelihood: 1695.7164802173577\n",
      "    val_log_marginal: 1638.4563773540908\n",
      "Train Epoch: 536 [512/54000 (1%)] Loss: -1857.632568\n",
      "Train Epoch: 536 [11776/54000 (22%)] Loss: -1621.559937\n",
      "Train Epoch: 536 [23040/54000 (43%)] Loss: -1585.500000\n",
      "Train Epoch: 536 [34304/54000 (64%)] Loss: -1575.352661\n",
      "Train Epoch: 536 [45568/54000 (84%)] Loss: -1574.617676\n",
      "    epoch          : 536\n",
      "    loss           : -1583.989500744508\n",
      "    val_loss       : -1584.803600685837\n",
      "    val_log_likelihood: 1677.6336657835705\n",
      "    val_log_marginal: 1625.612116779007\n",
      "Train Epoch: 537 [512/54000 (1%)] Loss: -1488.252319\n",
      "Train Epoch: 537 [11776/54000 (22%)] Loss: -1687.068726\n",
      "Train Epoch: 537 [23040/54000 (43%)] Loss: -1487.247559\n",
      "Train Epoch: 537 [34304/54000 (64%)] Loss: -1507.150146\n",
      "Train Epoch: 537 [45568/54000 (84%)] Loss: -1578.259766\n",
      "    epoch          : 537\n",
      "    loss           : -1586.549996857596\n",
      "    val_loss       : -1592.6276298601106\n",
      "    val_log_likelihood: 1696.2134671352878\n",
      "    val_log_marginal: 1638.2983452830981\n",
      "Train Epoch: 538 [512/54000 (1%)] Loss: -1892.659912\n",
      "Train Epoch: 538 [11776/54000 (22%)] Loss: -1536.279663\n",
      "Train Epoch: 538 [23040/54000 (43%)] Loss: -1608.313721\n",
      "Train Epoch: 538 [34304/54000 (64%)] Loss: -1882.294556\n",
      "Train Epoch: 538 [45568/54000 (84%)] Loss: -1491.619995\n",
      "    epoch          : 538\n",
      "    loss           : -1592.8510464205601\n",
      "    val_loss       : -1563.3787997890781\n",
      "    val_log_likelihood: 1652.0736059812036\n",
      "    val_log_marginal: 1596.103916610287\n",
      "Train Epoch: 539 [512/54000 (1%)] Loss: -1821.404297\n",
      "Train Epoch: 539 [11776/54000 (22%)] Loss: -1443.125732\n",
      "Train Epoch: 539 [23040/54000 (43%)] Loss: -1509.737549\n",
      "Train Epoch: 539 [34304/54000 (64%)] Loss: -1578.581299\n",
      "Train Epoch: 539 [45568/54000 (84%)] Loss: -1478.165283\n",
      "    epoch          : 539\n",
      "    loss           : -1565.7759937248607\n",
      "    val_loss       : -1576.2857569074258\n",
      "    val_log_likelihood: 1674.1787048944152\n",
      "    val_log_marginal: 1613.1064636126935\n",
      "Train Epoch: 540 [512/54000 (1%)] Loss: -1868.616455\n",
      "Train Epoch: 540 [11776/54000 (22%)] Loss: -1622.239014\n",
      "Train Epoch: 540 [23040/54000 (43%)] Loss: -1532.443237\n",
      "Train Epoch: 540 [34304/54000 (64%)] Loss: -1607.909668\n",
      "Train Epoch: 540 [45568/54000 (84%)] Loss: -1573.048950\n",
      "    epoch          : 540\n",
      "    loss           : -1574.235870059174\n",
      "    val_loss       : -1578.466691477261\n",
      "    val_log_likelihood: 1676.9614306157177\n",
      "    val_log_marginal: 1618.435694719508\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch540.pth ...\n",
      "Train Epoch: 541 [512/54000 (1%)] Loss: -1867.097900\n",
      "Train Epoch: 541 [11776/54000 (22%)] Loss: -1657.051758\n",
      "Train Epoch: 541 [23040/54000 (43%)] Loss: -1654.376343\n",
      "Train Epoch: 541 [34304/54000 (64%)] Loss: -1445.507324\n",
      "Train Epoch: 541 [45568/54000 (84%)] Loss: -1564.345703\n",
      "    epoch          : 541\n",
      "    loss           : -1579.9933332688738\n",
      "    val_loss       : -1579.6759845582806\n",
      "    val_log_likelihood: 1677.2909539855352\n",
      "    val_log_marginal: 1619.9185824575143\n",
      "Train Epoch: 542 [512/54000 (1%)] Loss: -1848.165527\n",
      "Train Epoch: 542 [11776/54000 (22%)] Loss: -1665.194702\n",
      "Train Epoch: 542 [23040/54000 (43%)] Loss: -1456.526978\n",
      "Train Epoch: 542 [34304/54000 (64%)] Loss: -1550.005127\n",
      "Train Epoch: 542 [45568/54000 (84%)] Loss: -1577.764282\n",
      "    epoch          : 542\n",
      "    loss           : -1581.6515496886602\n",
      "    val_loss       : -1576.3710606076322\n",
      "    val_log_likelihood: 1685.5727986250774\n",
      "    val_log_marginal: 1630.52099646917\n",
      "Train Epoch: 543 [512/54000 (1%)] Loss: -1842.478882\n",
      "Train Epoch: 543 [11776/54000 (22%)] Loss: -1503.260742\n",
      "Train Epoch: 543 [23040/54000 (43%)] Loss: -1552.800537\n",
      "Train Epoch: 543 [34304/54000 (64%)] Loss: -1464.034058\n",
      "Train Epoch: 543 [45568/54000 (84%)] Loss: -1491.013672\n",
      "    epoch          : 543\n",
      "    loss           : -1574.2595057723545\n",
      "    val_loss       : -1579.507389520968\n",
      "    val_log_likelihood: 1683.0424297068378\n",
      "    val_log_marginal: 1629.5571338271925\n",
      "Train Epoch: 544 [512/54000 (1%)] Loss: -1868.610107\n",
      "Train Epoch: 544 [11776/54000 (22%)] Loss: -1474.190918\n",
      "Train Epoch: 544 [23040/54000 (43%)] Loss: -1580.467529\n",
      "Train Epoch: 544 [34304/54000 (64%)] Loss: -1632.925049\n",
      "Train Epoch: 544 [45568/54000 (84%)] Loss: -1545.127319\n",
      "    epoch          : 544\n",
      "    loss           : -1584.820015180229\n",
      "    val_loss       : -1588.7373689019887\n",
      "    val_log_likelihood: 1688.516678913985\n",
      "    val_log_marginal: 1632.8342792241278\n",
      "Train Epoch: 545 [512/54000 (1%)] Loss: -1856.142822\n",
      "Train Epoch: 545 [11776/54000 (22%)] Loss: -1506.147095\n",
      "Train Epoch: 545 [23040/54000 (43%)] Loss: -1426.199829\n",
      "Train Epoch: 545 [34304/54000 (64%)] Loss: -1557.318115\n",
      "Train Epoch: 545 [45568/54000 (84%)] Loss: -1554.835693\n",
      "    epoch          : 545\n",
      "    loss           : -1568.848392297726\n",
      "    val_loss       : -1570.6134228164533\n",
      "    val_log_likelihood: 1664.6778987469058\n",
      "    val_log_marginal: 1604.5234668776868\n",
      "Train Epoch: 546 [512/54000 (1%)] Loss: -1855.887085\n",
      "Train Epoch: 546 [11776/54000 (22%)] Loss: -1638.424194\n",
      "Train Epoch: 546 [23040/54000 (43%)] Loss: -1560.808594\n",
      "Train Epoch: 546 [34304/54000 (64%)] Loss: -1872.673462\n",
      "Train Epoch: 546 [45568/54000 (84%)] Loss: -1488.237183\n",
      "    epoch          : 546\n",
      "    loss           : -1571.2946088432086\n",
      "    val_loss       : -1572.9933393787924\n",
      "    val_log_likelihood: 1677.205700562732\n",
      "    val_log_marginal: 1615.0631330645283\n",
      "Train Epoch: 547 [512/54000 (1%)] Loss: -1841.898315\n",
      "Train Epoch: 547 [11776/54000 (22%)] Loss: -1492.904297\n",
      "Train Epoch: 547 [23040/54000 (43%)] Loss: -1639.997559\n",
      "Train Epoch: 547 [34304/54000 (64%)] Loss: -1511.669922\n",
      "Train Epoch: 547 [45568/54000 (84%)] Loss: -1547.106201\n",
      "    epoch          : 547\n",
      "    loss           : -1572.947350228187\n",
      "    val_loss       : -1574.405517155904\n",
      "    val_log_likelihood: 1664.5528878693533\n",
      "    val_log_marginal: 1609.01402336698\n",
      "Train Epoch: 548 [512/54000 (1%)] Loss: -1848.941162\n",
      "Train Epoch: 548 [11776/54000 (22%)] Loss: -1527.712402\n",
      "Train Epoch: 548 [23040/54000 (43%)] Loss: -1439.912964\n",
      "Train Epoch: 548 [34304/54000 (64%)] Loss: -1516.761353\n",
      "Train Epoch: 548 [45568/54000 (84%)] Loss: -1543.555298\n",
      "    epoch          : 548\n",
      "    loss           : -1570.1780063138149\n",
      "    val_loss       : -1564.5379445165727\n",
      "    val_log_likelihood: 1654.2476552831065\n",
      "    val_log_marginal: 1598.421954086211\n",
      "Train Epoch: 549 [512/54000 (1%)] Loss: -1858.863037\n",
      "Train Epoch: 549 [11776/54000 (22%)] Loss: -1635.596558\n",
      "Train Epoch: 549 [23040/54000 (43%)] Loss: -1590.646484\n",
      "Train Epoch: 549 [34304/54000 (64%)] Loss: -1546.423706\n",
      "Train Epoch: 549 [45568/54000 (84%)] Loss: -1586.663818\n",
      "    epoch          : 549\n",
      "    loss           : -1572.0315026009437\n",
      "    val_loss       : -1577.5588798791925\n",
      "    val_log_likelihood: 1681.4031776956992\n",
      "    val_log_marginal: 1623.3383321025326\n",
      "Train Epoch: 550 [512/54000 (1%)] Loss: -1854.434570\n",
      "Train Epoch: 550 [11776/54000 (22%)] Loss: -1469.242188\n",
      "Train Epoch: 550 [23040/54000 (43%)] Loss: -1529.952881\n",
      "Train Epoch: 550 [34304/54000 (64%)] Loss: -1590.116943\n",
      "Train Epoch: 550 [45568/54000 (84%)] Loss: -1514.877686\n",
      "    epoch          : 550\n",
      "    loss           : -1584.3693654277538\n",
      "    val_loss       : -1584.7716939682603\n",
      "    val_log_likelihood: 1688.6858091448794\n",
      "    val_log_marginal: 1633.412777741375\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch550.pth ...\n",
      "Train Epoch: 551 [512/54000 (1%)] Loss: -1842.210205\n",
      "Train Epoch: 551 [11776/54000 (22%)] Loss: -1510.161255\n",
      "Train Epoch: 551 [23040/54000 (43%)] Loss: -1470.745728\n",
      "Train Epoch: 551 [34304/54000 (64%)] Loss: -1661.902832\n",
      "Train Epoch: 551 [45568/54000 (84%)] Loss: -1559.651489\n",
      "    epoch          : 551\n",
      "    loss           : -1582.6103491452661\n",
      "    val_loss       : -1586.148777587699\n",
      "    val_log_likelihood: 1697.0387216700185\n",
      "    val_log_marginal: 1639.7207917537332\n",
      "Train Epoch: 552 [512/54000 (1%)] Loss: -1537.056885\n",
      "Train Epoch: 552 [11776/54000 (22%)] Loss: -1664.404907\n",
      "Train Epoch: 552 [23040/54000 (43%)] Loss: -1624.095459\n",
      "Train Epoch: 552 [34304/54000 (64%)] Loss: -1677.470703\n",
      "Train Epoch: 552 [45568/54000 (84%)] Loss: -1498.305298\n",
      "    epoch          : 552\n",
      "    loss           : -1593.808646929146\n",
      "    val_loss       : -1591.8949993240951\n",
      "    val_log_likelihood: 1691.0740737159654\n",
      "    val_log_marginal: 1639.2643510836297\n",
      "Train Epoch: 553 [512/54000 (1%)] Loss: -1851.369263\n",
      "Train Epoch: 553 [11776/54000 (22%)] Loss: -1526.734985\n",
      "Train Epoch: 553 [23040/54000 (43%)] Loss: -1682.091064\n",
      "Train Epoch: 553 [34304/54000 (64%)] Loss: -1589.018433\n",
      "Train Epoch: 553 [45568/54000 (84%)] Loss: -1621.859497\n",
      "    epoch          : 553\n",
      "    loss           : -1591.3442660794399\n",
      "    val_loss       : -1596.2434045922676\n",
      "    val_log_likelihood: 1701.1141671662283\n",
      "    val_log_marginal: 1645.8277776497462\n",
      "Train Epoch: 554 [512/54000 (1%)] Loss: -1887.278564\n",
      "Train Epoch: 554 [11776/54000 (22%)] Loss: -1555.909180\n",
      "Train Epoch: 554 [23040/54000 (43%)] Loss: -1638.857666\n",
      "Train Epoch: 554 [34304/54000 (64%)] Loss: -1509.437744\n",
      "Train Epoch: 554 [45568/54000 (84%)] Loss: -1596.630981\n",
      "    epoch          : 554\n",
      "    loss           : -1595.189102626083\n",
      "    val_loss       : -1590.4125250187049\n",
      "    val_log_likelihood: 1697.4677976098392\n",
      "    val_log_marginal: 1641.6484895376766\n",
      "Train Epoch: 555 [512/54000 (1%)] Loss: -1883.946289\n",
      "Train Epoch: 555 [11776/54000 (22%)] Loss: -1419.525513\n",
      "Train Epoch: 555 [23040/54000 (43%)] Loss: -1576.376953\n",
      "Train Epoch: 555 [34304/54000 (64%)] Loss: -1485.526245\n",
      "Train Epoch: 555 [45568/54000 (84%)] Loss: -1546.152344\n",
      "    epoch          : 555\n",
      "    loss           : -1577.8142367825649\n",
      "    val_loss       : -1582.828258206367\n",
      "    val_log_likelihood: 1684.4234377417233\n",
      "    val_log_marginal: 1622.495073282244\n",
      "Train Epoch: 556 [512/54000 (1%)] Loss: -1893.744385\n",
      "Train Epoch: 556 [11776/54000 (22%)] Loss: -1504.535278\n",
      "Train Epoch: 556 [23040/54000 (43%)] Loss: -1527.598511\n",
      "Train Epoch: 556 [34304/54000 (64%)] Loss: -1862.765747\n",
      "Train Epoch: 556 [45568/54000 (84%)] Loss: -1572.476440\n",
      "    epoch          : 556\n",
      "    loss           : -1582.62879505724\n",
      "    val_loss       : -1589.3831644300187\n",
      "    val_log_likelihood: 1691.2933579246596\n",
      "    val_log_marginal: 1633.5400491168557\n",
      "Train Epoch: 557 [512/54000 (1%)] Loss: -1632.020264\n",
      "Train Epoch: 557 [11776/54000 (22%)] Loss: -1487.826904\n",
      "Train Epoch: 557 [23040/54000 (43%)] Loss: -1584.607910\n",
      "Train Epoch: 557 [34304/54000 (64%)] Loss: -1474.373657\n",
      "Train Epoch: 557 [45568/54000 (84%)] Loss: -1577.489380\n",
      "    epoch          : 557\n",
      "    loss           : -1594.1126539778002\n",
      "    val_loss       : -1593.9278274290725\n",
      "    val_log_likelihood: 1691.1275175491182\n",
      "    val_log_marginal: 1633.9874756016243\n",
      "Train Epoch: 558 [512/54000 (1%)] Loss: -1897.279785\n",
      "Train Epoch: 558 [11776/54000 (22%)] Loss: -1554.862183\n",
      "Train Epoch: 558 [23040/54000 (43%)] Loss: -1484.625244\n",
      "Train Epoch: 558 [34304/54000 (64%)] Loss: -1542.863281\n",
      "Train Epoch: 558 [45568/54000 (84%)] Loss: -1517.799683\n",
      "    epoch          : 558\n",
      "    loss           : -1591.2307624439202\n",
      "    val_loss       : -1598.6937230387057\n",
      "    val_log_likelihood: 1690.8247130743348\n",
      "    val_log_marginal: 1637.9508376614476\n",
      "Train Epoch: 559 [512/54000 (1%)] Loss: -1872.301270\n",
      "Train Epoch: 559 [11776/54000 (22%)] Loss: -1542.616699\n",
      "Train Epoch: 559 [23040/54000 (43%)] Loss: -1483.226074\n",
      "Train Epoch: 559 [34304/54000 (64%)] Loss: -1574.255005\n",
      "Train Epoch: 559 [45568/54000 (84%)] Loss: -1447.105713\n",
      "    epoch          : 559\n",
      "    loss           : -1591.139781385365\n",
      "    val_loss       : -1588.0893502001936\n",
      "    val_log_likelihood: 1682.6235158183788\n",
      "    val_log_marginal: 1626.9352863888155\n",
      "Train Epoch: 560 [512/54000 (1%)] Loss: -1873.854370\n",
      "Train Epoch: 560 [11776/54000 (22%)] Loss: -1545.609497\n",
      "Train Epoch: 560 [23040/54000 (43%)] Loss: -1461.247314\n",
      "Train Epoch: 560 [34304/54000 (64%)] Loss: -1600.010864\n",
      "Train Epoch: 560 [45568/54000 (84%)] Loss: -1567.754150\n",
      "    epoch          : 560\n",
      "    loss           : -1589.7840890412283\n",
      "    val_loss       : -1590.6496294770334\n",
      "    val_log_likelihood: 1694.9427127649287\n",
      "    val_log_marginal: 1639.1679783163058\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch560.pth ...\n",
      "Train Epoch: 561 [512/54000 (1%)] Loss: -1868.233765\n",
      "Train Epoch: 561 [11776/54000 (22%)] Loss: -1677.859985\n",
      "Train Epoch: 561 [23040/54000 (43%)] Loss: -1501.385864\n",
      "Train Epoch: 561 [34304/54000 (64%)] Loss: -1486.438721\n",
      "Train Epoch: 561 [45568/54000 (84%)] Loss: -1552.541748\n",
      "    epoch          : 561\n",
      "    loss           : -1579.9757007561107\n",
      "    val_loss       : -1581.0320121998632\n",
      "    val_log_likelihood: 1680.2936443668782\n",
      "    val_log_marginal: 1623.029473593975\n",
      "Train Epoch: 562 [512/54000 (1%)] Loss: -1575.022583\n",
      "Train Epoch: 562 [11776/54000 (22%)] Loss: -1516.508789\n",
      "Train Epoch: 562 [23040/54000 (43%)] Loss: -1481.960693\n",
      "Train Epoch: 562 [34304/54000 (64%)] Loss: -1873.191406\n",
      "Train Epoch: 562 [45568/54000 (84%)] Loss: -1622.152588\n",
      "    epoch          : 562\n",
      "    loss           : -1584.6907668916306\n",
      "    val_loss       : -1582.9515494903328\n",
      "    val_log_likelihood: 1680.2635304668163\n",
      "    val_log_marginal: 1614.0045855257586\n",
      "Train Epoch: 563 [512/54000 (1%)] Loss: -1870.330322\n",
      "Train Epoch: 563 [11776/54000 (22%)] Loss: -1467.565308\n",
      "Train Epoch: 563 [23040/54000 (43%)] Loss: -1470.694092\n",
      "Train Epoch: 563 [34304/54000 (64%)] Loss: -1585.110352\n",
      "Train Epoch: 563 [45568/54000 (84%)] Loss: -1511.811401\n",
      "    epoch          : 563\n",
      "    loss           : -1588.9253824064047\n",
      "    val_loss       : -1594.2488885624057\n",
      "    val_log_likelihood: 1691.7427748878404\n",
      "    val_log_marginal: 1630.9539339181126\n",
      "Train Epoch: 564 [512/54000 (1%)] Loss: -1849.874268\n",
      "Train Epoch: 564 [11776/54000 (22%)] Loss: -1499.234741\n",
      "Train Epoch: 564 [23040/54000 (43%)] Loss: -1452.482300\n",
      "Train Epoch: 564 [34304/54000 (64%)] Loss: -1450.579712\n",
      "Train Epoch: 564 [45568/54000 (84%)] Loss: -1495.942261\n",
      "    epoch          : 564\n",
      "    loss           : -1593.1629723275062\n",
      "    val_loss       : -1594.1114725407379\n",
      "    val_log_likelihood: 1696.214925935953\n",
      "    val_log_marginal: 1637.0221012297575\n",
      "Train Epoch: 565 [512/54000 (1%)] Loss: -1857.635498\n",
      "Train Epoch: 565 [11776/54000 (22%)] Loss: -1524.067261\n",
      "Train Epoch: 565 [23040/54000 (43%)] Loss: -1470.541016\n",
      "Train Epoch: 565 [34304/54000 (64%)] Loss: -1564.546997\n",
      "Train Epoch: 565 [45568/54000 (84%)] Loss: -1547.955688\n",
      "    epoch          : 565\n",
      "    loss           : -1590.9248433632426\n",
      "    val_loss       : -1592.6965623893914\n",
      "    val_log_likelihood: 1693.837020420792\n",
      "    val_log_marginal: 1638.532555645258\n",
      "Train Epoch: 566 [512/54000 (1%)] Loss: -1683.926758\n",
      "Train Epoch: 566 [11776/54000 (22%)] Loss: -1510.412231\n",
      "Train Epoch: 566 [23040/54000 (43%)] Loss: -1638.422363\n",
      "Train Epoch: 566 [34304/54000 (64%)] Loss: -1670.689209\n",
      "Train Epoch: 566 [45568/54000 (84%)] Loss: -1465.230225\n",
      "    epoch          : 566\n",
      "    loss           : -1588.0687316290223\n",
      "    val_loss       : -1590.25083360478\n",
      "    val_log_likelihood: 1689.2288238223236\n",
      "    val_log_marginal: 1630.8933625403597\n",
      "Train Epoch: 567 [512/54000 (1%)] Loss: -1847.872803\n",
      "Train Epoch: 567 [11776/54000 (22%)] Loss: -1466.508789\n",
      "Train Epoch: 567 [23040/54000 (43%)] Loss: -1498.272461\n",
      "Train Epoch: 567 [34304/54000 (64%)] Loss: -1459.927979\n",
      "Train Epoch: 567 [45568/54000 (84%)] Loss: -1610.172607\n",
      "    epoch          : 567\n",
      "    loss           : -1594.4185851446473\n",
      "    val_loss       : -1589.1294812879037\n",
      "    val_log_likelihood: 1692.6094366394648\n",
      "    val_log_marginal: 1637.1339221174023\n",
      "Train Epoch: 568 [512/54000 (1%)] Loss: -1855.845337\n",
      "Train Epoch: 568 [11776/54000 (22%)] Loss: -1510.272583\n",
      "Train Epoch: 568 [23040/54000 (43%)] Loss: -1448.367432\n",
      "Train Epoch: 568 [34304/54000 (64%)] Loss: -1877.641846\n",
      "Train Epoch: 568 [45568/54000 (84%)] Loss: -1572.715088\n",
      "    epoch          : 568\n",
      "    loss           : -1580.8940187964108\n",
      "    val_loss       : -1580.0934985616605\n",
      "    val_log_likelihood: 1687.675699064047\n",
      "    val_log_marginal: 1631.811867132285\n",
      "Train Epoch: 569 [512/54000 (1%)] Loss: -1874.000244\n",
      "Train Epoch: 569 [11776/54000 (22%)] Loss: -1703.148682\n",
      "Train Epoch: 569 [23040/54000 (43%)] Loss: -1563.102905\n",
      "Train Epoch: 569 [34304/54000 (64%)] Loss: -1639.106567\n",
      "Train Epoch: 569 [45568/54000 (84%)] Loss: -1578.886475\n",
      "    epoch          : 569\n",
      "    loss           : -1589.3978247312036\n",
      "    val_loss       : -1586.7806638146724\n",
      "    val_log_likelihood: 1693.5458923944152\n",
      "    val_log_marginal: 1633.9221528615121\n",
      "Train Epoch: 570 [512/54000 (1%)] Loss: -1869.280884\n",
      "Train Epoch: 570 [11776/54000 (22%)] Loss: -1513.888184\n",
      "Train Epoch: 570 [23040/54000 (43%)] Loss: -1525.033447\n",
      "Train Epoch: 570 [34304/54000 (64%)] Loss: -1660.905029\n",
      "Train Epoch: 570 [45568/54000 (84%)] Loss: -1582.432007\n",
      "    epoch          : 570\n",
      "    loss           : -1591.3517208288213\n",
      "    val_loss       : -1592.7151039782266\n",
      "    val_log_likelihood: 1695.6277024675123\n",
      "    val_log_marginal: 1639.3225141698217\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch570.pth ...\n",
      "Train Epoch: 571 [512/54000 (1%)] Loss: -1882.453857\n",
      "Train Epoch: 571 [11776/54000 (22%)] Loss: -1511.945312\n",
      "Train Epoch: 571 [23040/54000 (43%)] Loss: -1564.540771\n",
      "Train Epoch: 571 [34304/54000 (64%)] Loss: -1543.179199\n",
      "Train Epoch: 571 [45568/54000 (84%)] Loss: -1473.353394\n",
      "    epoch          : 571\n",
      "    loss           : -1588.364095857828\n",
      "    val_loss       : -1573.0413404892909\n",
      "    val_log_likelihood: 1687.3793764019956\n",
      "    val_log_marginal: 1628.9079269927074\n",
      "Train Epoch: 572 [512/54000 (1%)] Loss: -1842.780762\n",
      "Train Epoch: 572 [11776/54000 (22%)] Loss: -1659.425049\n",
      "Train Epoch: 572 [23040/54000 (43%)] Loss: -1569.711914\n",
      "Train Epoch: 572 [34304/54000 (64%)] Loss: -1628.539062\n",
      "Train Epoch: 572 [45568/54000 (84%)] Loss: -1452.580322\n",
      "    epoch          : 572\n",
      "    loss           : -1581.1720695873298\n",
      "    val_loss       : -1579.2019827552324\n",
      "    val_log_likelihood: 1691.41362981513\n",
      "    val_log_marginal: 1632.0090498096033\n",
      "Train Epoch: 573 [512/54000 (1%)] Loss: -1854.849854\n",
      "Train Epoch: 573 [11776/54000 (22%)] Loss: -1615.692139\n",
      "Train Epoch: 573 [23040/54000 (43%)] Loss: -1548.349365\n",
      "Train Epoch: 573 [34304/54000 (64%)] Loss: -1547.617676\n",
      "Train Epoch: 573 [45568/54000 (84%)] Loss: -1565.466064\n",
      "    epoch          : 573\n",
      "    loss           : -1584.9498472308169\n",
      "    val_loss       : -1592.2057373942255\n",
      "    val_log_likelihood: 1699.660940642404\n",
      "    val_log_marginal: 1639.8586535347754\n",
      "Train Epoch: 574 [512/54000 (1%)] Loss: -1833.183228\n",
      "Train Epoch: 574 [11776/54000 (22%)] Loss: -1518.897217\n",
      "Train Epoch: 574 [23040/54000 (43%)] Loss: -1556.280884\n",
      "Train Epoch: 574 [34304/54000 (64%)] Loss: -1686.182251\n",
      "Train Epoch: 574 [45568/54000 (84%)] Loss: -1504.864014\n",
      "    epoch          : 574\n",
      "    loss           : -1592.3011390006188\n",
      "    val_loss       : -1593.8715554001271\n",
      "    val_log_likelihood: 1704.4506134939666\n",
      "    val_log_marginal: 1646.2783771085042\n",
      "Train Epoch: 575 [512/54000 (1%)] Loss: -1876.287476\n",
      "Train Epoch: 575 [11776/54000 (22%)] Loss: -1605.299927\n",
      "Train Epoch: 575 [23040/54000 (43%)] Loss: -1467.024414\n",
      "Train Epoch: 575 [34304/54000 (64%)] Loss: -1549.197021\n",
      "Train Epoch: 575 [45568/54000 (84%)] Loss: -1506.959717\n",
      "    epoch          : 575\n",
      "    loss           : -1591.9996567527846\n",
      "    val_loss       : -1582.3478675987478\n",
      "    val_log_likelihood: 1679.1845630607982\n",
      "    val_log_marginal: 1620.757989312476\n",
      "Train Epoch: 576 [512/54000 (1%)] Loss: -1848.224976\n",
      "Train Epoch: 576 [11776/54000 (22%)] Loss: -1573.226318\n",
      "Train Epoch: 576 [23040/54000 (43%)] Loss: -1646.234863\n",
      "Train Epoch: 576 [34304/54000 (64%)] Loss: -1628.306885\n",
      "Train Epoch: 576 [45568/54000 (84%)] Loss: -1473.076660\n",
      "    epoch          : 576\n",
      "    loss           : -1574.303681930693\n",
      "    val_loss       : -1572.7975027674513\n",
      "    val_log_likelihood: 1660.9756487855816\n",
      "    val_log_marginal: 1602.7468072686002\n",
      "Train Epoch: 577 [512/54000 (1%)] Loss: -1852.548706\n",
      "Train Epoch: 577 [11776/54000 (22%)] Loss: -1506.265015\n",
      "Train Epoch: 577 [23040/54000 (43%)] Loss: -1466.450073\n",
      "Train Epoch: 577 [34304/54000 (64%)] Loss: -1624.113892\n",
      "Train Epoch: 577 [45568/54000 (84%)] Loss: -1582.348633\n",
      "    epoch          : 577\n",
      "    loss           : -1578.1462813273515\n",
      "    val_loss       : -1587.461967657514\n",
      "    val_log_likelihood: 1678.9752173093286\n",
      "    val_log_marginal: 1624.6683943487594\n",
      "Train Epoch: 578 [512/54000 (1%)] Loss: -1860.794189\n",
      "Train Epoch: 578 [11776/54000 (22%)] Loss: -1623.773071\n",
      "Train Epoch: 578 [23040/54000 (43%)] Loss: -1449.400146\n",
      "Train Epoch: 578 [34304/54000 (64%)] Loss: -1585.935059\n",
      "Train Epoch: 578 [45568/54000 (84%)] Loss: -1601.451050\n",
      "    epoch          : 578\n",
      "    loss           : -1586.2159701810024\n",
      "    val_loss       : -1587.8610892973581\n",
      "    val_log_likelihood: 1678.4285127243193\n",
      "    val_log_marginal: 1625.9265724394782\n",
      "Train Epoch: 579 [512/54000 (1%)] Loss: -1849.263794\n",
      "Train Epoch: 579 [11776/54000 (22%)] Loss: -1519.593994\n",
      "Train Epoch: 579 [23040/54000 (43%)] Loss: -1591.906494\n",
      "Train Epoch: 579 [34304/54000 (64%)] Loss: -1650.807739\n",
      "Train Epoch: 579 [45568/54000 (84%)] Loss: -1475.382202\n",
      "    epoch          : 579\n",
      "    loss           : -1584.347276744276\n",
      "    val_loss       : -1586.9943894410671\n",
      "    val_log_likelihood: 1700.515354269802\n",
      "    val_log_marginal: 1645.6066346447049\n",
      "Train Epoch: 580 [512/54000 (1%)] Loss: -1875.211914\n",
      "Train Epoch: 580 [11776/54000 (22%)] Loss: -1645.786865\n",
      "Train Epoch: 580 [23040/54000 (43%)] Loss: -1492.018311\n",
      "Train Epoch: 580 [34304/54000 (64%)] Loss: -1538.103638\n",
      "Train Epoch: 580 [45568/54000 (84%)] Loss: -1481.551270\n",
      "    epoch          : 580\n",
      "    loss           : -1597.8035646948483\n",
      "    val_loss       : -1596.8337399399204\n",
      "    val_log_likelihood: 1698.2530771387685\n",
      "    val_log_marginal: 1645.9585393791335\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch580.pth ...\n",
      "Train Epoch: 581 [512/54000 (1%)] Loss: -1885.264893\n",
      "Train Epoch: 581 [11776/54000 (22%)] Loss: -1568.054565\n",
      "Train Epoch: 581 [23040/54000 (43%)] Loss: -1668.195068\n",
      "Train Epoch: 581 [34304/54000 (64%)] Loss: -1514.584717\n",
      "Train Epoch: 581 [45568/54000 (84%)] Loss: -1589.517822\n",
      "    epoch          : 581\n",
      "    loss           : -1598.1785332708075\n",
      "    val_loss       : -1599.1798880647334\n",
      "    val_log_likelihood: 1697.1670804165378\n",
      "    val_log_marginal: 1645.0940208183695\n",
      "Train Epoch: 582 [512/54000 (1%)] Loss: -1876.003174\n",
      "Train Epoch: 582 [11776/54000 (22%)] Loss: -1563.641113\n",
      "Train Epoch: 582 [23040/54000 (43%)] Loss: -1479.416382\n",
      "Train Epoch: 582 [34304/54000 (64%)] Loss: -1680.096680\n",
      "Train Epoch: 582 [45568/54000 (84%)] Loss: -1654.434326\n",
      "    epoch          : 582\n",
      "    loss           : -1598.37336836711\n",
      "    val_loss       : -1595.6727774766468\n",
      "    val_log_likelihood: 1698.8086300085088\n",
      "    val_log_marginal: 1645.341019115254\n",
      "Train Epoch: 583 [512/54000 (1%)] Loss: -1879.770264\n",
      "Train Epoch: 583 [11776/54000 (22%)] Loss: -1479.169922\n",
      "Train Epoch: 583 [23040/54000 (43%)] Loss: -1481.079956\n",
      "Train Epoch: 583 [34304/54000 (64%)] Loss: -1484.587891\n",
      "Train Epoch: 583 [45568/54000 (84%)] Loss: -1530.522217\n",
      "    epoch          : 583\n",
      "    loss           : -1592.0936835260675\n",
      "    val_loss       : -1597.216269153115\n",
      "    val_log_likelihood: 1699.4119570892635\n",
      "    val_log_marginal: 1645.3058259084773\n",
      "Train Epoch: 584 [512/54000 (1%)] Loss: -1879.805176\n",
      "Train Epoch: 584 [11776/54000 (22%)] Loss: -1574.322388\n",
      "Train Epoch: 584 [23040/54000 (43%)] Loss: -1466.139648\n",
      "Train Epoch: 584 [34304/54000 (64%)] Loss: -1510.999512\n",
      "Train Epoch: 584 [45568/54000 (84%)] Loss: -1539.517456\n",
      "    epoch          : 584\n",
      "    loss           : -1598.008598101021\n",
      "    val_loss       : -1601.1108818538764\n",
      "    val_log_likelihood: 1704.4020101717203\n",
      "    val_log_marginal: 1649.1284227853475\n",
      "Train Epoch: 585 [512/54000 (1%)] Loss: -1883.316284\n",
      "Train Epoch: 585 [11776/54000 (22%)] Loss: -1552.964844\n",
      "Train Epoch: 585 [23040/54000 (43%)] Loss: -1461.364990\n",
      "Train Epoch: 585 [34304/54000 (64%)] Loss: -1530.573242\n",
      "Train Epoch: 585 [45568/54000 (84%)] Loss: -1590.482178\n",
      "    epoch          : 585\n",
      "    loss           : -1599.4433255337253\n",
      "    val_loss       : -1594.0764928730482\n",
      "    val_log_likelihood: 1706.3350056563274\n",
      "    val_log_marginal: 1652.56819305213\n",
      "Train Epoch: 586 [512/54000 (1%)] Loss: -1524.236084\n",
      "Train Epoch: 586 [11776/54000 (22%)] Loss: -1475.095947\n",
      "Train Epoch: 586 [23040/54000 (43%)] Loss: -1484.617188\n",
      "Train Epoch: 586 [34304/54000 (64%)] Loss: -1584.404541\n",
      "Train Epoch: 586 [45568/54000 (84%)] Loss: -1541.019409\n",
      "    epoch          : 586\n",
      "    loss           : -1598.8894357209158\n",
      "    val_loss       : -1601.4977147444013\n",
      "    val_log_likelihood: 1707.0202576287902\n",
      "    val_log_marginal: 1653.0023994282958\n",
      "Train Epoch: 587 [512/54000 (1%)] Loss: -1683.721924\n",
      "Train Epoch: 587 [11776/54000 (22%)] Loss: -1528.627686\n",
      "Train Epoch: 587 [23040/54000 (43%)] Loss: -1469.126953\n",
      "Train Epoch: 587 [34304/54000 (64%)] Loss: -1469.179077\n",
      "Train Epoch: 587 [45568/54000 (84%)] Loss: -1581.945557\n",
      "    epoch          : 587\n",
      "    loss           : -1594.3381444345607\n",
      "    val_loss       : -1589.8700365067027\n",
      "    val_log_likelihood: 1690.5270283009747\n",
      "    val_log_marginal: 1635.2697754259416\n",
      "Train Epoch: 588 [512/54000 (1%)] Loss: -1845.193237\n",
      "Train Epoch: 588 [11776/54000 (22%)] Loss: -1710.184082\n",
      "Train Epoch: 588 [23040/54000 (43%)] Loss: -1518.092041\n",
      "Train Epoch: 588 [34304/54000 (64%)] Loss: -1518.032715\n",
      "Train Epoch: 588 [45568/54000 (84%)] Loss: -1544.398560\n",
      "    epoch          : 588\n",
      "    loss           : -1592.6304714089572\n",
      "    val_loss       : -1590.4199635234504\n",
      "    val_log_likelihood: 1708.3762690478031\n",
      "    val_log_marginal: 1649.4480738935827\n",
      "Train Epoch: 589 [512/54000 (1%)] Loss: -1855.909058\n",
      "Train Epoch: 589 [11776/54000 (22%)] Loss: -1537.394287\n",
      "Train Epoch: 589 [23040/54000 (43%)] Loss: -1515.447876\n",
      "Train Epoch: 589 [34304/54000 (64%)] Loss: -1439.582886\n",
      "Train Epoch: 589 [45568/54000 (84%)] Loss: -1523.631226\n",
      "    epoch          : 589\n",
      "    loss           : -1592.69395884901\n",
      "    val_loss       : -1598.040560963241\n",
      "    val_log_likelihood: 1691.5215085473392\n",
      "    val_log_marginal: 1637.5773900505155\n",
      "Train Epoch: 590 [512/54000 (1%)] Loss: -1884.762939\n",
      "Train Epoch: 590 [11776/54000 (22%)] Loss: -1549.552979\n",
      "Train Epoch: 590 [23040/54000 (43%)] Loss: -1541.940063\n",
      "Train Epoch: 590 [34304/54000 (64%)] Loss: -1580.335571\n",
      "Train Epoch: 590 [45568/54000 (84%)] Loss: -1482.105591\n",
      "    epoch          : 590\n",
      "    loss           : -1593.0473959139078\n",
      "    val_loss       : -1588.573741415202\n",
      "    val_log_likelihood: 1704.18669022664\n",
      "    val_log_marginal: 1646.9644769060767\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch590.pth ...\n",
      "Train Epoch: 591 [512/54000 (1%)] Loss: -1889.991211\n",
      "Train Epoch: 591 [11776/54000 (22%)] Loss: -1685.907593\n",
      "Train Epoch: 591 [23040/54000 (43%)] Loss: -1615.752686\n",
      "Train Epoch: 591 [34304/54000 (64%)] Loss: -1515.419434\n",
      "Train Epoch: 591 [45568/54000 (84%)] Loss: -1503.572144\n",
      "    epoch          : 591\n",
      "    loss           : -1585.3683211827042\n",
      "    val_loss       : -1585.8952556682455\n",
      "    val_log_likelihood: 1688.7166566754331\n",
      "    val_log_marginal: 1632.1930719407294\n",
      "Train Epoch: 592 [512/54000 (1%)] Loss: -1678.736450\n",
      "Train Epoch: 592 [11776/54000 (22%)] Loss: -1512.040527\n",
      "Train Epoch: 592 [23040/54000 (43%)] Loss: -1582.368774\n",
      "Train Epoch: 592 [34304/54000 (64%)] Loss: -1560.641724\n",
      "Train Epoch: 592 [45568/54000 (84%)] Loss: -1499.640869\n",
      "    epoch          : 592\n",
      "    loss           : -1589.1514965095143\n",
      "    val_loss       : -1591.2136953717263\n",
      "    val_log_likelihood: 1701.1068320699258\n",
      "    val_log_marginal: 1640.9971203328764\n",
      "Train Epoch: 593 [512/54000 (1%)] Loss: -1620.523926\n",
      "Train Epoch: 593 [11776/54000 (22%)] Loss: -1618.298218\n",
      "Train Epoch: 593 [23040/54000 (43%)] Loss: -1632.014404\n",
      "Train Epoch: 593 [34304/54000 (64%)] Loss: -1574.967041\n",
      "Train Epoch: 593 [45568/54000 (84%)] Loss: -1525.051270\n",
      "    epoch          : 593\n",
      "    loss           : -1590.9872853496288\n",
      "    val_loss       : -1593.7582153578815\n",
      "    val_log_likelihood: 1698.607284092667\n",
      "    val_log_marginal: 1642.428452017634\n",
      "Train Epoch: 594 [512/54000 (1%)] Loss: -1874.388550\n",
      "Train Epoch: 594 [11776/54000 (22%)] Loss: -1599.026611\n",
      "Train Epoch: 594 [23040/54000 (43%)] Loss: -1542.424072\n",
      "Train Epoch: 594 [34304/54000 (64%)] Loss: -1526.525879\n",
      "Train Epoch: 594 [45568/54000 (84%)] Loss: -1459.889404\n",
      "    epoch          : 594\n",
      "    loss           : -1593.0819587329827\n",
      "    val_loss       : -1589.3037107559198\n",
      "    val_log_likelihood: 1686.4924546043471\n",
      "    val_log_marginal: 1629.149918395528\n",
      "Train Epoch: 595 [512/54000 (1%)] Loss: -1862.344238\n",
      "Train Epoch: 595 [11776/54000 (22%)] Loss: -1505.447021\n",
      "Train Epoch: 595 [23040/54000 (43%)] Loss: -1625.552612\n",
      "Train Epoch: 595 [34304/54000 (64%)] Loss: -1482.732910\n",
      "Train Epoch: 595 [45568/54000 (84%)] Loss: -1571.321533\n",
      "    epoch          : 595\n",
      "    loss           : -1580.9863631748917\n",
      "    val_loss       : -1584.1496837128802\n",
      "    val_log_likelihood: 1673.003877243193\n",
      "    val_log_marginal: 1617.143046326548\n",
      "Train Epoch: 596 [512/54000 (1%)] Loss: -1872.178345\n",
      "Train Epoch: 596 [11776/54000 (22%)] Loss: -1441.436768\n",
      "Train Epoch: 596 [23040/54000 (43%)] Loss: -1510.553955\n",
      "Train Epoch: 596 [34304/54000 (64%)] Loss: -1878.664062\n",
      "Train Epoch: 596 [45568/54000 (84%)] Loss: -1508.224487\n",
      "    epoch          : 596\n",
      "    loss           : -1580.4045579362623\n",
      "    val_loss       : -1563.4804361505471\n",
      "    val_log_likelihood: 1665.7219395401455\n",
      "    val_log_marginal: 1612.7488749662211\n",
      "Train Epoch: 597 [512/54000 (1%)] Loss: -1851.316895\n",
      "Train Epoch: 597 [11776/54000 (22%)] Loss: -1554.328003\n",
      "Train Epoch: 597 [23040/54000 (43%)] Loss: -1558.726074\n",
      "Train Epoch: 597 [34304/54000 (64%)] Loss: -1535.278320\n",
      "Train Epoch: 597 [45568/54000 (84%)] Loss: -1553.698608\n",
      "    epoch          : 597\n",
      "    loss           : -1569.422173528388\n",
      "    val_loss       : -1581.9247943069602\n",
      "    val_log_likelihood: 1687.2041728709003\n",
      "    val_log_marginal: 1631.4466505343603\n",
      "Train Epoch: 598 [512/54000 (1%)] Loss: -1898.409912\n",
      "Train Epoch: 598 [11776/54000 (22%)] Loss: -1499.492920\n",
      "Train Epoch: 598 [23040/54000 (43%)] Loss: -1462.153076\n",
      "Train Epoch: 598 [34304/54000 (64%)] Loss: -1688.965088\n",
      "Train Epoch: 598 [45568/54000 (84%)] Loss: -1575.599243\n",
      "    epoch          : 598\n",
      "    loss           : -1586.7857085879486\n",
      "    val_loss       : -1590.7312103581735\n",
      "    val_log_likelihood: 1688.9578615698483\n",
      "    val_log_marginal: 1636.5978913962435\n",
      "Train Epoch: 599 [512/54000 (1%)] Loss: -1867.073242\n",
      "Train Epoch: 599 [11776/54000 (22%)] Loss: -1642.023315\n",
      "Train Epoch: 599 [23040/54000 (43%)] Loss: -1664.445312\n",
      "Train Epoch: 599 [34304/54000 (64%)] Loss: -1880.606323\n",
      "Train Epoch: 599 [45568/54000 (84%)] Loss: -1618.437622\n",
      "    epoch          : 599\n",
      "    loss           : -1589.756591796875\n",
      "    val_loss       : -1597.0587545720362\n",
      "    val_log_likelihood: 1699.8254793374845\n",
      "    val_log_marginal: 1645.5163013537294\n",
      "Train Epoch: 600 [512/54000 (1%)] Loss: -1851.326050\n",
      "Train Epoch: 600 [11776/54000 (22%)] Loss: -1570.941040\n",
      "Train Epoch: 600 [23040/54000 (43%)] Loss: -1486.142822\n",
      "Train Epoch: 600 [34304/54000 (64%)] Loss: -1634.750000\n",
      "Train Epoch: 600 [45568/54000 (84%)] Loss: -1507.423828\n",
      "    epoch          : 600\n",
      "    loss           : -1595.22151289836\n",
      "    val_loss       : -1595.7299494255647\n",
      "    val_log_likelihood: 1688.4575666673113\n",
      "    val_log_marginal: 1638.0402464012218\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch600.pth ...\n",
      "Train Epoch: 601 [512/54000 (1%)] Loss: -1862.923218\n",
      "Train Epoch: 601 [11776/54000 (22%)] Loss: -1689.524658\n",
      "Train Epoch: 601 [23040/54000 (43%)] Loss: -1602.138184\n",
      "Train Epoch: 601 [34304/54000 (64%)] Loss: -1622.196899\n",
      "Train Epoch: 601 [45568/54000 (84%)] Loss: -1545.906372\n",
      "    epoch          : 601\n",
      "    loss           : -1592.4652039178527\n",
      "    val_loss       : -1585.3190657050525\n",
      "    val_log_likelihood: 1690.1745750502785\n",
      "    val_log_marginal: 1633.8785167289436\n",
      "Train Epoch: 602 [512/54000 (1%)] Loss: -1856.857910\n",
      "Train Epoch: 602 [11776/54000 (22%)] Loss: -1465.493164\n",
      "Train Epoch: 602 [23040/54000 (43%)] Loss: -1433.167969\n",
      "Train Epoch: 602 [34304/54000 (64%)] Loss: -1890.707275\n",
      "Train Epoch: 602 [45568/54000 (84%)] Loss: -1438.500977\n",
      "    epoch          : 602\n",
      "    loss           : -1589.886340452893\n",
      "    val_loss       : -1585.9526634480399\n",
      "    val_log_likelihood: 1684.1487021871133\n",
      "    val_log_marginal: 1626.765038044865\n",
      "Train Epoch: 603 [512/54000 (1%)] Loss: -1518.958252\n",
      "Train Epoch: 603 [11776/54000 (22%)] Loss: -1663.693359\n",
      "Train Epoch: 603 [23040/54000 (43%)] Loss: -1574.884521\n",
      "Train Epoch: 603 [34304/54000 (64%)] Loss: -1510.165771\n",
      "Train Epoch: 603 [45568/54000 (84%)] Loss: -1481.493530\n",
      "    epoch          : 603\n",
      "    loss           : -1591.3893837503867\n",
      "    val_loss       : -1596.8848067452338\n",
      "    val_log_likelihood: 1700.4963354733911\n",
      "    val_log_marginal: 1645.2785709926509\n",
      "Train Epoch: 604 [512/54000 (1%)] Loss: -1874.201904\n",
      "Train Epoch: 604 [11776/54000 (22%)] Loss: -1539.336670\n",
      "Train Epoch: 604 [23040/54000 (43%)] Loss: -1437.273682\n",
      "Train Epoch: 604 [34304/54000 (64%)] Loss: -1489.054199\n",
      "Train Epoch: 604 [45568/54000 (84%)] Loss: -1571.239380\n",
      "    epoch          : 604\n",
      "    loss           : -1594.892713490099\n",
      "    val_loss       : -1596.9960887247678\n",
      "    val_log_likelihood: 1702.9014384959003\n",
      "    val_log_marginal: 1647.82214767709\n",
      "Train Epoch: 605 [512/54000 (1%)] Loss: -1887.230835\n",
      "Train Epoch: 605 [11776/54000 (22%)] Loss: -1559.812744\n",
      "Train Epoch: 605 [23040/54000 (43%)] Loss: -1638.350342\n",
      "Train Epoch: 605 [34304/54000 (64%)] Loss: -1461.200439\n",
      "Train Epoch: 605 [45568/54000 (84%)] Loss: -1483.768311\n",
      "    epoch          : 605\n",
      "    loss           : -1592.0617832901455\n",
      "    val_loss       : -1594.3584706162076\n",
      "    val_log_likelihood: 1688.5182791228342\n",
      "    val_log_marginal: 1636.3815856615684\n",
      "Train Epoch: 606 [512/54000 (1%)] Loss: -1902.647705\n",
      "Train Epoch: 606 [11776/54000 (22%)] Loss: -1649.680786\n",
      "Train Epoch: 606 [23040/54000 (43%)] Loss: -1486.984375\n",
      "Train Epoch: 606 [34304/54000 (64%)] Loss: -1458.743896\n",
      "Train Epoch: 606 [45568/54000 (84%)] Loss: -1528.949463\n",
      "    epoch          : 606\n",
      "    loss           : -1594.7892752165842\n",
      "    val_loss       : -1591.3864156669467\n",
      "    val_log_likelihood: 1695.0117562171256\n",
      "    val_log_marginal: 1641.4280505752615\n",
      "Train Epoch: 607 [512/54000 (1%)] Loss: -1686.849487\n",
      "Train Epoch: 607 [11776/54000 (22%)] Loss: -1548.205811\n",
      "Train Epoch: 607 [23040/54000 (43%)] Loss: -1664.553223\n",
      "Train Epoch: 607 [34304/54000 (64%)] Loss: -1564.837646\n",
      "Train Epoch: 607 [45568/54000 (84%)] Loss: -1529.961182\n",
      "    epoch          : 607\n",
      "    loss           : -1597.6977744527383\n",
      "    val_loss       : -1603.4275282365593\n",
      "    val_log_likelihood: 1704.9698425897277\n",
      "    val_log_marginal: 1652.3305198886792\n",
      "Train Epoch: 608 [512/54000 (1%)] Loss: -1862.163818\n",
      "Train Epoch: 608 [11776/54000 (22%)] Loss: -1499.526245\n",
      "Train Epoch: 608 [23040/54000 (43%)] Loss: -1533.528198\n",
      "Train Epoch: 608 [34304/54000 (64%)] Loss: -1594.158325\n",
      "Train Epoch: 608 [45568/54000 (84%)] Loss: -1621.401123\n",
      "    epoch          : 608\n",
      "    loss           : -1594.6184239151455\n",
      "    val_loss       : -1597.855638071987\n",
      "    val_log_likelihood: 1704.3241982035117\n",
      "    val_log_marginal: 1651.445951431455\n",
      "Train Epoch: 609 [512/54000 (1%)] Loss: -1870.525391\n",
      "Train Epoch: 609 [11776/54000 (22%)] Loss: -1497.756714\n",
      "Train Epoch: 609 [23040/54000 (43%)] Loss: -1626.449341\n",
      "Train Epoch: 609 [34304/54000 (64%)] Loss: -1865.615723\n",
      "Train Epoch: 609 [45568/54000 (84%)] Loss: -1575.690308\n",
      "    epoch          : 609\n",
      "    loss           : -1591.1862382038985\n",
      "    val_loss       : -1590.5619746072143\n",
      "    val_log_likelihood: 1690.1336029354889\n",
      "    val_log_marginal: 1634.595830098995\n",
      "Train Epoch: 610 [512/54000 (1%)] Loss: -1499.515869\n",
      "Train Epoch: 610 [11776/54000 (22%)] Loss: -1660.349609\n",
      "Train Epoch: 610 [23040/54000 (43%)] Loss: -1616.937012\n",
      "Train Epoch: 610 [34304/54000 (64%)] Loss: -1443.572632\n",
      "Train Epoch: 610 [45568/54000 (84%)] Loss: -1495.089478\n",
      "    epoch          : 610\n",
      "    loss           : -1595.3759258005878\n",
      "    val_loss       : -1603.0394772430882\n",
      "    val_log_likelihood: 1700.8205844388149\n",
      "    val_log_marginal: 1646.0935731923928\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch610.pth ...\n",
      "Train Epoch: 611 [512/54000 (1%)] Loss: -1663.884033\n",
      "Train Epoch: 611 [11776/54000 (22%)] Loss: -1530.254395\n",
      "Train Epoch: 611 [23040/54000 (43%)] Loss: -1474.689941\n",
      "Train Epoch: 611 [34304/54000 (64%)] Loss: -1597.124878\n",
      "Train Epoch: 611 [45568/54000 (84%)] Loss: -1565.879639\n",
      "    epoch          : 611\n",
      "    loss           : -1602.174479569539\n",
      "    val_loss       : -1603.0267908644153\n",
      "    val_log_likelihood: 1705.038821985226\n",
      "    val_log_marginal: 1650.161400659637\n",
      "Train Epoch: 612 [512/54000 (1%)] Loss: -1875.105713\n",
      "Train Epoch: 612 [11776/54000 (22%)] Loss: -1675.396973\n",
      "Train Epoch: 612 [23040/54000 (43%)] Loss: -1450.793335\n",
      "Train Epoch: 612 [34304/54000 (64%)] Loss: -1509.506714\n",
      "Train Epoch: 612 [45568/54000 (84%)] Loss: -1587.956543\n",
      "    epoch          : 612\n",
      "    loss           : -1587.0759108137377\n",
      "    val_loss       : -1579.9246501471146\n",
      "    val_log_likelihood: 1706.937929059019\n",
      "    val_log_marginal: 1651.7605146575202\n",
      "Train Epoch: 613 [512/54000 (1%)] Loss: -1844.249634\n",
      "Train Epoch: 613 [11776/54000 (22%)] Loss: -1638.014771\n",
      "Train Epoch: 613 [23040/54000 (43%)] Loss: -1387.881104\n",
      "Train Epoch: 613 [34304/54000 (64%)] Loss: -1887.094727\n",
      "Train Epoch: 613 [45568/54000 (84%)] Loss: -1515.289917\n",
      "    epoch          : 613\n",
      "    loss           : -1575.0327063834313\n",
      "    val_loss       : -1579.1320349801756\n",
      "    val_log_likelihood: 1693.7928527227723\n",
      "    val_log_marginal: 1640.7446167909236\n",
      "Train Epoch: 614 [512/54000 (1%)] Loss: -1856.371094\n",
      "Train Epoch: 614 [11776/54000 (22%)] Loss: -1654.154785\n",
      "Train Epoch: 614 [23040/54000 (43%)] Loss: -1393.243652\n",
      "Train Epoch: 614 [34304/54000 (64%)] Loss: -1674.135742\n",
      "Train Epoch: 614 [45568/54000 (84%)] Loss: -1614.305786\n",
      "    epoch          : 614\n",
      "    loss           : -1581.440969939279\n",
      "    val_loss       : -1581.5441122614309\n",
      "    val_log_likelihood: 1701.2455873394956\n",
      "    val_log_marginal: 1647.6473589662905\n",
      "Train Epoch: 615 [512/54000 (1%)] Loss: -1888.646973\n",
      "Train Epoch: 615 [11776/54000 (22%)] Loss: -1586.433594\n",
      "Train Epoch: 615 [23040/54000 (43%)] Loss: -1551.318726\n",
      "Train Epoch: 615 [34304/54000 (64%)] Loss: -1623.413696\n",
      "Train Epoch: 615 [45568/54000 (84%)] Loss: -1537.715698\n",
      "    epoch          : 615\n",
      "    loss           : -1591.3285746055074\n",
      "    val_loss       : -1587.4363043879223\n",
      "    val_log_likelihood: 1703.4735639213336\n",
      "    val_log_marginal: 1649.658911192052\n",
      "Train Epoch: 616 [512/54000 (1%)] Loss: -1845.530762\n",
      "Train Epoch: 616 [11776/54000 (22%)] Loss: -1656.385742\n",
      "Train Epoch: 616 [23040/54000 (43%)] Loss: -1460.820190\n",
      "Train Epoch: 616 [34304/54000 (64%)] Loss: -1582.553711\n",
      "Train Epoch: 616 [45568/54000 (84%)] Loss: -1528.132568\n",
      "    epoch          : 616\n",
      "    loss           : -1585.1715172493812\n",
      "    val_loss       : -1556.3282137799197\n",
      "    val_log_likelihood: 1630.278345693456\n",
      "    val_log_marginal: 1584.1615193884377\n",
      "Train Epoch: 617 [512/54000 (1%)] Loss: -1844.302246\n",
      "Train Epoch: 617 [11776/54000 (22%)] Loss: -1573.270874\n",
      "Train Epoch: 617 [23040/54000 (43%)] Loss: -1434.289917\n",
      "Train Epoch: 617 [34304/54000 (64%)] Loss: -1535.376953\n",
      "Train Epoch: 617 [45568/54000 (84%)] Loss: -1595.212524\n",
      "    epoch          : 617\n",
      "    loss           : -1573.7856626605044\n",
      "    val_loss       : -1591.0255591512039\n",
      "    val_log_likelihood: 1682.5476932336787\n",
      "    val_log_marginal: 1631.7280626196489\n",
      "Train Epoch: 618 [512/54000 (1%)] Loss: -1881.791626\n",
      "Train Epoch: 618 [11776/54000 (22%)] Loss: -1478.187256\n",
      "Train Epoch: 618 [23040/54000 (43%)] Loss: -1638.382080\n",
      "Train Epoch: 618 [34304/54000 (64%)] Loss: -1555.990356\n",
      "Train Epoch: 618 [45568/54000 (84%)] Loss: -1499.207886\n",
      "    epoch          : 618\n",
      "    loss           : -1588.1831489789604\n",
      "    val_loss       : -1567.6457650076488\n",
      "    val_log_likelihood: 1665.3927026125464\n",
      "    val_log_marginal: 1614.0392149619706\n",
      "Train Epoch: 619 [512/54000 (1%)] Loss: -1872.873535\n",
      "Train Epoch: 619 [11776/54000 (22%)] Loss: -1503.233643\n",
      "Train Epoch: 619 [23040/54000 (43%)] Loss: -1457.684570\n",
      "Train Epoch: 619 [34304/54000 (64%)] Loss: -1456.217529\n",
      "Train Epoch: 619 [45568/54000 (84%)] Loss: -1467.723022\n",
      "    epoch          : 619\n",
      "    loss           : -1573.480443369044\n",
      "    val_loss       : -1577.7896220566483\n",
      "    val_log_likelihood: 1676.5061953705135\n",
      "    val_log_marginal: 1622.0149950469374\n",
      "Train Epoch: 620 [512/54000 (1%)] Loss: -1835.497437\n",
      "Train Epoch: 620 [11776/54000 (22%)] Loss: -1626.641479\n",
      "Train Epoch: 620 [23040/54000 (43%)] Loss: -1592.228271\n",
      "Train Epoch: 620 [34304/54000 (64%)] Loss: -1593.970459\n",
      "Train Epoch: 620 [45568/54000 (84%)] Loss: -1547.266357\n",
      "    epoch          : 620\n",
      "    loss           : -1581.8650772547958\n",
      "    val_loss       : -1584.95622659766\n",
      "    val_log_likelihood: 1684.6959953685798\n",
      "    val_log_marginal: 1632.565155408963\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch620.pth ...\n",
      "Train Epoch: 621 [512/54000 (1%)] Loss: -1841.645020\n",
      "Train Epoch: 621 [11776/54000 (22%)] Loss: -1536.274414\n",
      "Train Epoch: 621 [23040/54000 (43%)] Loss: -1665.585327\n",
      "Train Epoch: 621 [34304/54000 (64%)] Loss: -1866.157715\n",
      "Train Epoch: 621 [45568/54000 (84%)] Loss: -1484.335571\n",
      "    epoch          : 621\n",
      "    loss           : -1587.447999255492\n",
      "    val_loss       : -1590.5023609206614\n",
      "    val_log_likelihood: 1698.6459284112004\n",
      "    val_log_marginal: 1641.124255339864\n",
      "Train Epoch: 622 [512/54000 (1%)] Loss: -1861.846313\n",
      "Train Epoch: 622 [11776/54000 (22%)] Loss: -1570.250732\n",
      "Train Epoch: 622 [23040/54000 (43%)] Loss: -1637.129272\n",
      "Train Epoch: 622 [34304/54000 (64%)] Loss: -1604.458374\n",
      "Train Epoch: 622 [45568/54000 (84%)] Loss: -1586.104980\n",
      "    epoch          : 622\n",
      "    loss           : -1589.4710874651919\n",
      "    val_loss       : -1592.1508453237816\n",
      "    val_log_likelihood: 1688.1060718498607\n",
      "    val_log_marginal: 1634.4525573647754\n",
      "Train Epoch: 623 [512/54000 (1%)] Loss: -1882.024292\n",
      "Train Epoch: 623 [11776/54000 (22%)] Loss: -1529.590332\n",
      "Train Epoch: 623 [23040/54000 (43%)] Loss: -1628.986084\n",
      "Train Epoch: 623 [34304/54000 (64%)] Loss: -1893.407471\n",
      "Train Epoch: 623 [45568/54000 (84%)] Loss: -1619.070557\n",
      "    epoch          : 623\n",
      "    loss           : -1595.1396665667544\n",
      "    val_loss       : -1577.6136031365206\n",
      "    val_log_likelihood: 1680.8421074895575\n",
      "    val_log_marginal: 1624.759369632125\n",
      "Train Epoch: 624 [512/54000 (1%)] Loss: -1860.813232\n",
      "Train Epoch: 624 [11776/54000 (22%)] Loss: -1542.734985\n",
      "Train Epoch: 624 [23040/54000 (43%)] Loss: -1439.000732\n",
      "Train Epoch: 624 [34304/54000 (64%)] Loss: -1481.178833\n",
      "Train Epoch: 624 [45568/54000 (84%)] Loss: -1562.164185\n",
      "    epoch          : 624\n",
      "    loss           : -1584.3598693243348\n",
      "    val_loss       : -1597.7702581133453\n",
      "    val_log_likelihood: 1694.1825446704827\n",
      "    val_log_marginal: 1640.4504098106397\n",
      "Train Epoch: 625 [512/54000 (1%)] Loss: -1548.327759\n",
      "Train Epoch: 625 [11776/54000 (22%)] Loss: -1645.506348\n",
      "Train Epoch: 625 [23040/54000 (43%)] Loss: -1420.228516\n",
      "Train Epoch: 625 [34304/54000 (64%)] Loss: -1540.788574\n",
      "Train Epoch: 625 [45568/54000 (84%)] Loss: -1504.002930\n",
      "    epoch          : 625\n",
      "    loss           : -1592.3020696356746\n",
      "    val_loss       : -1585.1383315152902\n",
      "    val_log_likelihood: 1671.7346795714727\n",
      "    val_log_marginal: 1617.542226056928\n",
      "Train Epoch: 626 [512/54000 (1%)] Loss: -1882.049438\n",
      "Train Epoch: 626 [11776/54000 (22%)] Loss: -1555.357422\n",
      "Train Epoch: 626 [23040/54000 (43%)] Loss: -1597.671631\n",
      "Train Epoch: 626 [34304/54000 (64%)] Loss: -1621.894043\n",
      "Train Epoch: 626 [45568/54000 (84%)] Loss: -1592.084717\n",
      "    epoch          : 626\n",
      "    loss           : -1592.2759357112468\n",
      "    val_loss       : -1600.0914754119083\n",
      "    val_log_likelihood: 1704.0158872698794\n",
      "    val_log_marginal: 1650.8307488840442\n",
      "Train Epoch: 627 [512/54000 (1%)] Loss: -1880.169800\n",
      "Train Epoch: 627 [11776/54000 (22%)] Loss: -1650.139648\n",
      "Train Epoch: 627 [23040/54000 (43%)] Loss: -1541.424438\n",
      "Train Epoch: 627 [34304/54000 (64%)] Loss: -1597.612305\n",
      "Train Epoch: 627 [45568/54000 (84%)] Loss: -1542.202393\n",
      "    epoch          : 627\n",
      "    loss           : -1597.4452363571318\n",
      "    val_loss       : -1592.2727205836777\n",
      "    val_log_likelihood: 1702.9941805093595\n",
      "    val_log_marginal: 1647.4549232634467\n",
      "Train Epoch: 628 [512/54000 (1%)] Loss: -1858.171021\n",
      "Train Epoch: 628 [11776/54000 (22%)] Loss: -1554.068115\n",
      "Train Epoch: 628 [23040/54000 (43%)] Loss: -1662.890137\n",
      "Train Epoch: 628 [34304/54000 (64%)] Loss: -1553.056641\n",
      "Train Epoch: 628 [45568/54000 (84%)] Loss: -1521.489990\n",
      "    epoch          : 628\n",
      "    loss           : -1594.0444517230044\n",
      "    val_loss       : -1601.8610407577937\n",
      "    val_log_likelihood: 1699.0644918007426\n",
      "    val_log_marginal: 1645.096703957047\n",
      "Train Epoch: 629 [512/54000 (1%)] Loss: -1516.196655\n",
      "Train Epoch: 629 [11776/54000 (22%)] Loss: -1568.821167\n",
      "Train Epoch: 629 [23040/54000 (43%)] Loss: -1552.981934\n",
      "Train Epoch: 629 [34304/54000 (64%)] Loss: -1536.883057\n",
      "Train Epoch: 629 [45568/54000 (84%)] Loss: -1523.436279\n",
      "    epoch          : 629\n",
      "    loss           : -1597.8121978457611\n",
      "    val_loss       : -1595.3245110799844\n",
      "    val_log_likelihood: 1704.5234858446781\n",
      "    val_log_marginal: 1648.4522689139599\n",
      "Train Epoch: 630 [512/54000 (1%)] Loss: -1877.022583\n",
      "Train Epoch: 630 [11776/54000 (22%)] Loss: -1555.676025\n",
      "Train Epoch: 630 [23040/54000 (43%)] Loss: -1552.344238\n",
      "Train Epoch: 630 [34304/54000 (64%)] Loss: -1530.194824\n",
      "Train Epoch: 630 [45568/54000 (84%)] Loss: -1636.092896\n",
      "    epoch          : 630\n",
      "    loss           : -1597.2126343982054\n",
      "    val_loss       : -1599.725055678093\n",
      "    val_log_likelihood: 1705.4440265315593\n",
      "    val_log_marginal: 1649.954279190409\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch630.pth ...\n",
      "Train Epoch: 631 [512/54000 (1%)] Loss: -1518.280518\n",
      "Train Epoch: 631 [11776/54000 (22%)] Loss: -1480.743042\n",
      "Train Epoch: 631 [23040/54000 (43%)] Loss: -1519.518799\n",
      "Train Epoch: 631 [34304/54000 (64%)] Loss: -1504.400757\n",
      "Train Epoch: 631 [45568/54000 (84%)] Loss: -1598.021240\n",
      "    epoch          : 631\n",
      "    loss           : -1597.7034573696628\n",
      "    val_loss       : -1593.1003895459955\n",
      "    val_log_likelihood: 1706.2682102316678\n",
      "    val_log_marginal: 1648.510038284657\n",
      "Train Epoch: 632 [512/54000 (1%)] Loss: -1895.312988\n",
      "Train Epoch: 632 [11776/54000 (22%)] Loss: -1636.438477\n",
      "Train Epoch: 632 [23040/54000 (43%)] Loss: -1555.752319\n",
      "Train Epoch: 632 [34304/54000 (64%)] Loss: -1529.696777\n",
      "Train Epoch: 632 [45568/54000 (84%)] Loss: -1634.440186\n",
      "    epoch          : 632\n",
      "    loss           : -1588.954993521813\n",
      "    val_loss       : -1597.6694714499934\n",
      "    val_log_likelihood: 1690.6860835009281\n",
      "    val_log_marginal: 1634.2751371413653\n",
      "Train Epoch: 633 [512/54000 (1%)] Loss: -1854.897583\n",
      "Train Epoch: 633 [11776/54000 (22%)] Loss: -1554.743408\n",
      "Train Epoch: 633 [23040/54000 (43%)] Loss: -1591.455566\n",
      "Train Epoch: 633 [34304/54000 (64%)] Loss: -1514.492188\n",
      "Train Epoch: 633 [45568/54000 (84%)] Loss: -1630.012695\n",
      "    epoch          : 633\n",
      "    loss           : -1595.2195875715502\n",
      "    val_loss       : -1596.6118225580321\n",
      "    val_log_likelihood: 1698.2054540048732\n",
      "    val_log_marginal: 1642.8551434450997\n",
      "Train Epoch: 634 [512/54000 (1%)] Loss: -1572.808105\n",
      "Train Epoch: 634 [11776/54000 (22%)] Loss: -1550.573975\n",
      "Train Epoch: 634 [23040/54000 (43%)] Loss: -1444.202637\n",
      "Train Epoch: 634 [34304/54000 (64%)] Loss: -1644.018188\n",
      "Train Epoch: 634 [45568/54000 (84%)] Loss: -1574.365234\n",
      "    epoch          : 634\n",
      "    loss           : -1596.051322710396\n",
      "    val_loss       : -1592.8798448267198\n",
      "    val_log_likelihood: 1684.644339079904\n",
      "    val_log_marginal: 1627.4245746856243\n",
      "Train Epoch: 635 [512/54000 (1%)] Loss: -1875.522217\n",
      "Train Epoch: 635 [11776/54000 (22%)] Loss: -1651.044922\n",
      "Train Epoch: 635 [23040/54000 (43%)] Loss: -1565.323730\n",
      "Train Epoch: 635 [34304/54000 (64%)] Loss: -1538.813477\n",
      "Train Epoch: 635 [45568/54000 (84%)] Loss: -1571.673950\n",
      "    epoch          : 635\n",
      "    loss           : -1589.157799446937\n",
      "    val_loss       : -1578.6630162243537\n",
      "    val_log_likelihood: 1662.383353960396\n",
      "    val_log_marginal: 1610.2900803964585\n",
      "Train Epoch: 636 [512/54000 (1%)] Loss: -1872.561035\n",
      "Train Epoch: 636 [11776/54000 (22%)] Loss: -1485.836670\n",
      "Train Epoch: 636 [23040/54000 (43%)] Loss: -1581.799438\n",
      "Train Epoch: 636 [34304/54000 (64%)] Loss: -1587.246460\n",
      "Train Epoch: 636 [45568/54000 (84%)] Loss: -1568.839355\n",
      "    epoch          : 636\n",
      "    loss           : -1571.1850162921567\n",
      "    val_loss       : -1580.5589519375474\n",
      "    val_log_likelihood: 1668.6962503867574\n",
      "    val_log_marginal: 1615.1802857980917\n",
      "Train Epoch: 637 [512/54000 (1%)] Loss: -1877.265015\n",
      "Train Epoch: 637 [11776/54000 (22%)] Loss: -1494.932129\n",
      "Train Epoch: 637 [23040/54000 (43%)] Loss: -1559.430664\n",
      "Train Epoch: 637 [34304/54000 (64%)] Loss: -1876.914795\n",
      "Train Epoch: 637 [45568/54000 (84%)] Loss: -1538.239258\n",
      "    epoch          : 637\n",
      "    loss           : -1584.9641560469524\n",
      "    val_loss       : -1586.309661049857\n",
      "    val_log_likelihood: 1683.9419259552908\n",
      "    val_log_marginal: 1628.4212449850145\n",
      "Train Epoch: 638 [512/54000 (1%)] Loss: -1661.562378\n",
      "Train Epoch: 638 [11776/54000 (22%)] Loss: -1523.512695\n",
      "Train Epoch: 638 [23040/54000 (43%)] Loss: -1445.471802\n",
      "Train Epoch: 638 [34304/54000 (64%)] Loss: -1636.195068\n",
      "Train Epoch: 638 [45568/54000 (84%)] Loss: -1576.041748\n",
      "    epoch          : 638\n",
      "    loss           : -1590.6130491955446\n",
      "    val_loss       : -1599.109097740447\n",
      "    val_log_likelihood: 1683.975068649443\n",
      "    val_log_marginal: 1631.1165694650263\n",
      "Train Epoch: 639 [512/54000 (1%)] Loss: -1896.479980\n",
      "Train Epoch: 639 [11776/54000 (22%)] Loss: -1492.991699\n",
      "Train Epoch: 639 [23040/54000 (43%)] Loss: -1495.947266\n",
      "Train Epoch: 639 [34304/54000 (64%)] Loss: -1813.937988\n",
      "Train Epoch: 639 [45568/54000 (84%)] Loss: -1569.536377\n",
      "    epoch          : 639\n",
      "    loss           : -1586.1894434560643\n",
      "    val_loss       : -1589.607031985859\n",
      "    val_log_likelihood: 1686.216446376083\n",
      "    val_log_marginal: 1634.240410259178\n",
      "Train Epoch: 640 [512/54000 (1%)] Loss: -1873.557617\n",
      "Train Epoch: 640 [11776/54000 (22%)] Loss: -1531.666992\n",
      "Train Epoch: 640 [23040/54000 (43%)] Loss: -1422.618774\n",
      "Train Epoch: 640 [34304/54000 (64%)] Loss: -1615.892212\n",
      "Train Epoch: 640 [45568/54000 (84%)] Loss: -1537.663696\n",
      "    epoch          : 640\n",
      "    loss           : -1592.792236328125\n",
      "    val_loss       : -1591.316943202223\n",
      "    val_log_likelihood: 1694.8660465655942\n",
      "    val_log_marginal: 1640.4262568210574\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch640.pth ...\n",
      "Train Epoch: 641 [512/54000 (1%)] Loss: -1869.088745\n",
      "Train Epoch: 641 [11776/54000 (22%)] Loss: -1534.475586\n",
      "Train Epoch: 641 [23040/54000 (43%)] Loss: -1485.003418\n",
      "Train Epoch: 641 [34304/54000 (64%)] Loss: -1859.701172\n",
      "Train Epoch: 641 [45568/54000 (84%)] Loss: -1589.809448\n",
      "    epoch          : 641\n",
      "    loss           : -1594.429014300356\n",
      "    val_loss       : -1596.6663224813205\n",
      "    val_log_likelihood: 1687.2491442991955\n",
      "    val_log_marginal: 1633.316633779316\n",
      "Train Epoch: 642 [512/54000 (1%)] Loss: -1848.392334\n",
      "Train Epoch: 642 [11776/54000 (22%)] Loss: -1578.489502\n",
      "Train Epoch: 642 [23040/54000 (43%)] Loss: -1615.158325\n",
      "Train Epoch: 642 [34304/54000 (64%)] Loss: -1554.948486\n",
      "Train Epoch: 642 [45568/54000 (84%)] Loss: -1558.915039\n",
      "    epoch          : 642\n",
      "    loss           : -1583.6829350537594\n",
      "    val_loss       : -1585.9857810048184\n",
      "    val_log_likelihood: 1689.1059352761447\n",
      "    val_log_marginal: 1634.1006402873277\n",
      "Train Epoch: 643 [512/54000 (1%)] Loss: -1846.604004\n",
      "Train Epoch: 643 [11776/54000 (22%)] Loss: -1521.306519\n",
      "Train Epoch: 643 [23040/54000 (43%)] Loss: -1548.861572\n",
      "Train Epoch: 643 [34304/54000 (64%)] Loss: -1634.956543\n",
      "Train Epoch: 643 [45568/54000 (84%)] Loss: -1531.480225\n",
      "    epoch          : 643\n",
      "    loss           : -1575.8407731764387\n",
      "    val_loss       : -1582.3614241882403\n",
      "    val_log_likelihood: 1688.9871016398515\n",
      "    val_log_marginal: 1628.3366639757105\n",
      "Train Epoch: 644 [512/54000 (1%)] Loss: -1513.611084\n",
      "Train Epoch: 644 [11776/54000 (22%)] Loss: -1647.002197\n",
      "Train Epoch: 644 [23040/54000 (43%)] Loss: -1564.680542\n",
      "Train Epoch: 644 [34304/54000 (64%)] Loss: -1631.885010\n",
      "Train Epoch: 644 [45568/54000 (84%)] Loss: -1628.083984\n",
      "    epoch          : 644\n",
      "    loss           : -1582.5178645672183\n",
      "    val_loss       : -1582.5139625340719\n",
      "    val_log_likelihood: 1685.4131572459003\n",
      "    val_log_marginal: 1626.6534982953574\n",
      "Train Epoch: 645 [512/54000 (1%)] Loss: -1856.926025\n",
      "Train Epoch: 645 [11776/54000 (22%)] Loss: -1632.877686\n",
      "Train Epoch: 645 [23040/54000 (43%)] Loss: -1644.291382\n",
      "Train Epoch: 645 [34304/54000 (64%)] Loss: -1513.400391\n",
      "Train Epoch: 645 [45568/54000 (84%)] Loss: -1549.452393\n",
      "    epoch          : 645\n",
      "    loss           : -1589.1202465095143\n",
      "    val_loss       : -1592.230691320724\n",
      "    val_log_likelihood: 1689.263242815981\n",
      "    val_log_marginal: 1632.6169020397647\n",
      "Train Epoch: 646 [512/54000 (1%)] Loss: -1870.124023\n",
      "Train Epoch: 646 [11776/54000 (22%)] Loss: -1671.236328\n",
      "Train Epoch: 646 [23040/54000 (43%)] Loss: -1515.687744\n",
      "Train Epoch: 646 [34304/54000 (64%)] Loss: -1657.769897\n",
      "Train Epoch: 646 [45568/54000 (84%)] Loss: -1451.891968\n",
      "    epoch          : 646\n",
      "    loss           : -1589.3098011583386\n",
      "    val_loss       : -1593.2112529434098\n",
      "    val_log_likelihood: 1700.9738660755725\n",
      "    val_log_marginal: 1644.009580367881\n",
      "Train Epoch: 647 [512/54000 (1%)] Loss: -1860.322510\n",
      "Train Epoch: 647 [11776/54000 (22%)] Loss: -1549.342896\n",
      "Train Epoch: 647 [23040/54000 (43%)] Loss: -1484.379883\n",
      "Train Epoch: 647 [34304/54000 (64%)] Loss: -1526.257812\n",
      "Train Epoch: 647 [45568/54000 (84%)] Loss: -1554.130371\n",
      "    epoch          : 647\n",
      "    loss           : -1597.1758235515933\n",
      "    val_loss       : -1600.5669864279282\n",
      "    val_log_likelihood: 1693.2487382038985\n",
      "    val_log_marginal: 1636.640528377511\n",
      "Train Epoch: 648 [512/54000 (1%)] Loss: -1880.806885\n",
      "Train Epoch: 648 [11776/54000 (22%)] Loss: -1543.087891\n",
      "Train Epoch: 648 [23040/54000 (43%)] Loss: -1559.703613\n",
      "Train Epoch: 648 [34304/54000 (64%)] Loss: -1567.566406\n",
      "Train Epoch: 648 [45568/54000 (84%)] Loss: -1469.318359\n",
      "    epoch          : 648\n",
      "    loss           : -1592.4430245881033\n",
      "    val_loss       : -1578.6836714898466\n",
      "    val_log_likelihood: 1704.0809084448483\n",
      "    val_log_marginal: 1649.067304120814\n",
      "Train Epoch: 649 [512/54000 (1%)] Loss: -1874.632690\n",
      "Train Epoch: 649 [11776/54000 (22%)] Loss: -1510.022461\n",
      "Train Epoch: 649 [23040/54000 (43%)] Loss: -1569.402100\n",
      "Train Epoch: 649 [34304/54000 (64%)] Loss: -1460.647705\n",
      "Train Epoch: 649 [45568/54000 (84%)] Loss: -1529.242188\n",
      "    epoch          : 649\n",
      "    loss           : -1585.7046889503404\n",
      "    val_loss       : -1594.7237425657302\n",
      "    val_log_likelihood: 1686.7800087503867\n",
      "    val_log_marginal: 1636.6532192332043\n",
      "Train Epoch: 650 [512/54000 (1%)] Loss: -1422.304077\n",
      "Train Epoch: 650 [11776/54000 (22%)] Loss: -1458.615967\n",
      "Train Epoch: 650 [23040/54000 (43%)] Loss: -1631.815186\n",
      "Train Epoch: 650 [34304/54000 (64%)] Loss: -1379.039062\n",
      "Train Epoch: 650 [45568/54000 (84%)] Loss: -1529.067017\n",
      "    epoch          : 650\n",
      "    loss           : -1594.718613426284\n",
      "    val_loss       : -1596.4604830300748\n",
      "    val_log_likelihood: 1705.3716255414604\n",
      "    val_log_marginal: 1651.3276513190376\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch650.pth ...\n",
      "Train Epoch: 651 [512/54000 (1%)] Loss: -1901.685547\n",
      "Train Epoch: 651 [11776/54000 (22%)] Loss: -1501.892700\n",
      "Train Epoch: 651 [23040/54000 (43%)] Loss: -1466.685181\n",
      "Train Epoch: 651 [34304/54000 (64%)] Loss: -1413.036499\n",
      "Train Epoch: 651 [45568/54000 (84%)] Loss: -1428.846924\n",
      "    epoch          : 651\n",
      "    loss           : -1598.324920956451\n",
      "    val_loss       : -1599.773512982081\n",
      "    val_log_likelihood: 1705.6148693726795\n",
      "    val_log_marginal: 1651.7275245123828\n",
      "Train Epoch: 652 [512/54000 (1%)] Loss: -1892.419189\n",
      "Train Epoch: 652 [11776/54000 (22%)] Loss: -1739.225830\n",
      "Train Epoch: 652 [23040/54000 (43%)] Loss: -1436.156738\n",
      "Train Epoch: 652 [34304/54000 (64%)] Loss: -1607.082520\n",
      "Train Epoch: 652 [45568/54000 (84%)] Loss: -1505.648804\n",
      "    epoch          : 652\n",
      "    loss           : -1600.079724000232\n",
      "    val_loss       : -1598.5083675211074\n",
      "    val_log_likelihood: 1705.0577743077042\n",
      "    val_log_marginal: 1652.077517433041\n",
      "Train Epoch: 653 [512/54000 (1%)] Loss: -1882.111450\n",
      "Train Epoch: 653 [11776/54000 (22%)] Loss: -1533.699829\n",
      "Train Epoch: 653 [23040/54000 (43%)] Loss: -1615.377930\n",
      "Train Epoch: 653 [34304/54000 (64%)] Loss: -1600.005127\n",
      "Train Epoch: 653 [45568/54000 (84%)] Loss: -1581.368164\n",
      "    epoch          : 653\n",
      "    loss           : -1600.4274721051206\n",
      "    val_loss       : -1598.4674031977718\n",
      "    val_log_likelihood: 1700.7879251914449\n",
      "    val_log_marginal: 1643.336165596187\n",
      "Train Epoch: 654 [512/54000 (1%)] Loss: -1844.781738\n",
      "Train Epoch: 654 [11776/54000 (22%)] Loss: -1693.076416\n",
      "Train Epoch: 654 [23040/54000 (43%)] Loss: -1570.929077\n",
      "Train Epoch: 654 [34304/54000 (64%)] Loss: -1446.914307\n",
      "Train Epoch: 654 [45568/54000 (84%)] Loss: -1578.866211\n",
      "    epoch          : 654\n",
      "    loss           : -1596.5392014909498\n",
      "    val_loss       : -1605.644714773983\n",
      "    val_log_likelihood: 1705.4323488745358\n",
      "    val_log_marginal: 1649.384391332488\n",
      "Train Epoch: 655 [512/54000 (1%)] Loss: -1875.495117\n",
      "Train Epoch: 655 [11776/54000 (22%)] Loss: -1690.266235\n",
      "Train Epoch: 655 [23040/54000 (43%)] Loss: -1494.249268\n",
      "Train Epoch: 655 [34304/54000 (64%)] Loss: -1605.405762\n",
      "Train Epoch: 655 [45568/54000 (84%)] Loss: -1517.489014\n",
      "    epoch          : 655\n",
      "    loss           : -1605.3859162283416\n",
      "    val_loss       : -1602.5614344356304\n",
      "    val_log_likelihood: 1706.6476422300434\n",
      "    val_log_marginal: 1648.7998607582583\n",
      "Train Epoch: 656 [512/54000 (1%)] Loss: -1876.085571\n",
      "Train Epoch: 656 [11776/54000 (22%)] Loss: -1496.876221\n",
      "Train Epoch: 656 [23040/54000 (43%)] Loss: -1505.655273\n",
      "Train Epoch: 656 [34304/54000 (64%)] Loss: -1534.327759\n",
      "Train Epoch: 656 [45568/54000 (84%)] Loss: -1542.862915\n",
      "    epoch          : 656\n",
      "    loss           : -1606.1535777479114\n",
      "    val_loss       : -1607.6588837445465\n",
      "    val_log_likelihood: 1710.1071825688427\n",
      "    val_log_marginal: 1654.9315827064188\n",
      "Train Epoch: 657 [512/54000 (1%)] Loss: -1881.171387\n",
      "Train Epoch: 657 [11776/54000 (22%)] Loss: -1528.464844\n",
      "Train Epoch: 657 [23040/54000 (43%)] Loss: -1524.029175\n",
      "Train Epoch: 657 [34304/54000 (64%)] Loss: -1581.801514\n",
      "Train Epoch: 657 [45568/54000 (84%)] Loss: -1520.713867\n",
      "    epoch          : 657\n",
      "    loss           : -1603.263101407797\n",
      "    val_loss       : -1608.90156369317\n",
      "    val_log_likelihood: 1704.897734084932\n",
      "    val_log_marginal: 1646.685042170439\n",
      "Train Epoch: 658 [512/54000 (1%)] Loss: -1865.274048\n",
      "Train Epoch: 658 [11776/54000 (22%)] Loss: -1510.214478\n",
      "Train Epoch: 658 [23040/54000 (43%)] Loss: -1591.612549\n",
      "Train Epoch: 658 [34304/54000 (64%)] Loss: -1649.460205\n",
      "Train Epoch: 658 [45568/54000 (84%)] Loss: -1567.206055\n",
      "    epoch          : 658\n",
      "    loss           : -1600.2039408164449\n",
      "    val_loss       : -1611.5153182778267\n",
      "    val_log_likelihood: 1701.7260609239636\n",
      "    val_log_marginal: 1647.2388467080618\n",
      "Train Epoch: 659 [512/54000 (1%)] Loss: -1521.394409\n",
      "Train Epoch: 659 [11776/54000 (22%)] Loss: -1662.941650\n",
      "Train Epoch: 659 [23040/54000 (43%)] Loss: -1577.943359\n",
      "Train Epoch: 659 [34304/54000 (64%)] Loss: -1527.332397\n",
      "Train Epoch: 659 [45568/54000 (84%)] Loss: -1641.648682\n",
      "    epoch          : 659\n",
      "    loss           : -1602.6847806118503\n",
      "    val_loss       : -1607.1583233626818\n",
      "    val_log_likelihood: 1705.9349957456684\n",
      "    val_log_marginal: 1651.5791027860043\n",
      "Train Epoch: 660 [512/54000 (1%)] Loss: -1883.284790\n",
      "Train Epoch: 660 [11776/54000 (22%)] Loss: -1577.071167\n",
      "Train Epoch: 660 [23040/54000 (43%)] Loss: -1488.659790\n",
      "Train Epoch: 660 [34304/54000 (64%)] Loss: -1645.867676\n",
      "Train Epoch: 660 [45568/54000 (84%)] Loss: -1608.823975\n",
      "    epoch          : 660\n",
      "    loss           : -1610.592149791151\n",
      "    val_loss       : -1597.301825722864\n",
      "    val_log_likelihood: 1691.9838480430074\n",
      "    val_log_marginal: 1630.2449185569947\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch660.pth ...\n",
      "Train Epoch: 661 [512/54000 (1%)] Loss: -1879.953369\n",
      "Train Epoch: 661 [11776/54000 (22%)] Loss: -1553.622192\n",
      "Train Epoch: 661 [23040/54000 (43%)] Loss: -1619.161865\n",
      "Train Epoch: 661 [34304/54000 (64%)] Loss: -1636.864746\n",
      "Train Epoch: 661 [45568/54000 (84%)] Loss: -1596.938965\n",
      "    epoch          : 661\n",
      "    loss           : -1597.7317692218442\n",
      "    val_loss       : -1598.018654033634\n",
      "    val_log_likelihood: 1699.0899863668008\n",
      "    val_log_marginal: 1641.3317611311418\n",
      "Train Epoch: 662 [512/54000 (1%)] Loss: -1795.525024\n",
      "Train Epoch: 662 [11776/54000 (22%)] Loss: -1611.751221\n",
      "Train Epoch: 662 [23040/54000 (43%)] Loss: -1586.196533\n",
      "Train Epoch: 662 [34304/54000 (64%)] Loss: -1684.598633\n",
      "Train Epoch: 662 [45568/54000 (84%)] Loss: -1519.035156\n",
      "    epoch          : 662\n",
      "    loss           : -1600.6953028310643\n",
      "    val_loss       : -1609.3636475195\n",
      "    val_log_likelihood: 1705.7734254138304\n",
      "    val_log_marginal: 1648.9976488693922\n",
      "Train Epoch: 663 [512/54000 (1%)] Loss: -1842.873169\n",
      "Train Epoch: 663 [11776/54000 (22%)] Loss: -1742.414307\n",
      "Train Epoch: 663 [23040/54000 (43%)] Loss: -1528.174194\n",
      "Train Epoch: 663 [34304/54000 (64%)] Loss: -1588.891357\n",
      "Train Epoch: 663 [45568/54000 (84%)] Loss: -1535.833984\n",
      "    epoch          : 663\n",
      "    loss           : -1592.8198955271503\n",
      "    val_loss       : -1592.1294524989617\n",
      "    val_log_likelihood: 1688.979951461943\n",
      "    val_log_marginal: 1629.411406455709\n",
      "Train Epoch: 664 [512/54000 (1%)] Loss: -1861.935547\n",
      "Train Epoch: 664 [11776/54000 (22%)] Loss: -1539.143066\n",
      "Train Epoch: 664 [23040/54000 (43%)] Loss: -1640.396973\n",
      "Train Epoch: 664 [34304/54000 (64%)] Loss: -1634.920654\n",
      "Train Epoch: 664 [45568/54000 (84%)] Loss: -1587.300781\n",
      "    epoch          : 664\n",
      "    loss           : -1597.4830962832611\n",
      "    val_loss       : -1597.276992457255\n",
      "    val_log_likelihood: 1698.076389426052\n",
      "    val_log_marginal: 1640.3496603797553\n",
      "Train Epoch: 665 [512/54000 (1%)] Loss: -1854.777222\n",
      "Train Epoch: 665 [11776/54000 (22%)] Loss: -1477.829834\n",
      "Train Epoch: 665 [23040/54000 (43%)] Loss: -1465.901489\n",
      "Train Epoch: 665 [34304/54000 (64%)] Loss: -1877.747803\n",
      "Train Epoch: 665 [45568/54000 (84%)] Loss: -1610.043091\n",
      "    epoch          : 665\n",
      "    loss           : -1601.613902479115\n",
      "    val_loss       : -1605.6170365046526\n",
      "    val_log_likelihood: 1696.193685701578\n",
      "    val_log_marginal: 1640.7763645184402\n",
      "Train Epoch: 666 [512/54000 (1%)] Loss: -1889.175781\n",
      "Train Epoch: 666 [11776/54000 (22%)] Loss: -1663.056885\n",
      "Train Epoch: 666 [23040/54000 (43%)] Loss: -1456.602539\n",
      "Train Epoch: 666 [34304/54000 (64%)] Loss: -1569.390869\n",
      "Train Epoch: 666 [45568/54000 (84%)] Loss: -1521.901123\n",
      "    epoch          : 666\n",
      "    loss           : -1593.4135403774753\n",
      "    val_loss       : -1596.6396174133654\n",
      "    val_log_likelihood: 1692.6016301825496\n",
      "    val_log_marginal: 1634.1844648402553\n",
      "Train Epoch: 667 [512/54000 (1%)] Loss: -1885.466064\n",
      "Train Epoch: 667 [11776/54000 (22%)] Loss: -1658.017822\n",
      "Train Epoch: 667 [23040/54000 (43%)] Loss: -1522.859375\n",
      "Train Epoch: 667 [34304/54000 (64%)] Loss: -1495.722656\n",
      "Train Epoch: 667 [45568/54000 (84%)] Loss: -1562.121338\n",
      "    epoch          : 667\n",
      "    loss           : -1605.3278313060798\n",
      "    val_loss       : -1604.7696135130775\n",
      "    val_log_likelihood: 1694.6484012414912\n",
      "    val_log_marginal: 1641.2074215544974\n",
      "Train Epoch: 668 [512/54000 (1%)] Loss: -1544.238525\n",
      "Train Epoch: 668 [11776/54000 (22%)] Loss: -1498.922974\n",
      "Train Epoch: 668 [23040/54000 (43%)] Loss: -1552.708496\n",
      "Train Epoch: 668 [34304/54000 (64%)] Loss: -1903.217529\n",
      "Train Epoch: 668 [45568/54000 (84%)] Loss: -1507.127563\n",
      "    epoch          : 668\n",
      "    loss           : -1603.4673721766708\n",
      "    val_loss       : -1609.2228364147486\n",
      "    val_log_likelihood: 1704.3652259146813\n",
      "    val_log_marginal: 1652.1714322924208\n",
      "Train Epoch: 669 [512/54000 (1%)] Loss: -1891.740112\n",
      "Train Epoch: 669 [11776/54000 (22%)] Loss: -1688.179321\n",
      "Train Epoch: 669 [23040/54000 (43%)] Loss: -1658.562256\n",
      "Train Epoch: 669 [34304/54000 (64%)] Loss: -1599.399536\n",
      "Train Epoch: 669 [45568/54000 (84%)] Loss: -1581.477051\n",
      "    epoch          : 669\n",
      "    loss           : -1606.1547754873143\n",
      "    val_loss       : -1611.4585364467953\n",
      "    val_log_likelihood: 1712.0767483852878\n",
      "    val_log_marginal: 1655.9650832230022\n",
      "Train Epoch: 670 [512/54000 (1%)] Loss: -1889.862793\n",
      "Train Epoch: 670 [11776/54000 (22%)] Loss: -1653.047607\n",
      "Train Epoch: 670 [23040/54000 (43%)] Loss: -1520.849243\n",
      "Train Epoch: 670 [34304/54000 (64%)] Loss: -1652.177368\n",
      "Train Epoch: 670 [45568/54000 (84%)] Loss: -1527.679810\n",
      "    epoch          : 670\n",
      "    loss           : -1610.4591572072247\n",
      "    val_loss       : -1605.7162158167966\n",
      "    val_log_likelihood: 1708.4582434928063\n",
      "    val_log_marginal: 1654.4266130245742\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch670.pth ...\n",
      "Train Epoch: 671 [512/54000 (1%)] Loss: -1865.113281\n",
      "Train Epoch: 671 [11776/54000 (22%)] Loss: -1573.421387\n",
      "Train Epoch: 671 [23040/54000 (43%)] Loss: -1668.781250\n",
      "Train Epoch: 671 [34304/54000 (64%)] Loss: -1492.766113\n",
      "Train Epoch: 671 [45568/54000 (84%)] Loss: -1514.565186\n",
      "    epoch          : 671\n",
      "    loss           : -1606.366592860458\n",
      "    val_loss       : -1606.4825845730334\n",
      "    val_log_likelihood: 1709.3296309367265\n",
      "    val_log_marginal: 1654.6678361100983\n",
      "Train Epoch: 672 [512/54000 (1%)] Loss: -1863.226562\n",
      "Train Epoch: 672 [11776/54000 (22%)] Loss: -1720.916016\n",
      "Train Epoch: 672 [23040/54000 (43%)] Loss: -1500.465698\n",
      "Train Epoch: 672 [34304/54000 (64%)] Loss: -1654.602051\n",
      "Train Epoch: 672 [45568/54000 (84%)] Loss: -1532.200562\n",
      "    epoch          : 672\n",
      "    loss           : -1610.2780665029393\n",
      "    val_loss       : -1614.5617859180334\n",
      "    val_log_likelihood: 1711.9534561610458\n",
      "    val_log_marginal: 1657.195376337257\n",
      "Train Epoch: 673 [512/54000 (1%)] Loss: -1890.058838\n",
      "Train Epoch: 673 [11776/54000 (22%)] Loss: -1487.816650\n",
      "Train Epoch: 673 [23040/54000 (43%)] Loss: -1598.617920\n",
      "Train Epoch: 673 [34304/54000 (64%)] Loss: -1546.307007\n",
      "Train Epoch: 673 [45568/54000 (84%)] Loss: -1587.972412\n",
      "    epoch          : 673\n",
      "    loss           : -1612.698812654703\n",
      "    val_loss       : -1613.3226951930192\n",
      "    val_log_likelihood: 1712.4553065536045\n",
      "    val_log_marginal: 1657.0115486567888\n",
      "Train Epoch: 674 [512/54000 (1%)] Loss: -1871.886719\n",
      "Train Epoch: 674 [11776/54000 (22%)] Loss: -1552.976562\n",
      "Train Epoch: 674 [23040/54000 (43%)] Loss: -1676.857666\n",
      "Train Epoch: 674 [34304/54000 (64%)] Loss: -1604.292114\n",
      "Train Epoch: 674 [45568/54000 (84%)] Loss: -1675.236816\n",
      "    epoch          : 674\n",
      "    loss           : -1609.5050520188738\n",
      "    val_loss       : -1606.1394350255498\n",
      "    val_log_likelihood: 1710.1112846147896\n",
      "    val_log_marginal: 1654.6263999144298\n",
      "Train Epoch: 675 [512/54000 (1%)] Loss: -1890.666504\n",
      "Train Epoch: 675 [11776/54000 (22%)] Loss: -1511.299316\n",
      "Train Epoch: 675 [23040/54000 (43%)] Loss: -1618.444458\n",
      "Train Epoch: 675 [34304/54000 (64%)] Loss: -1475.052979\n",
      "Train Epoch: 675 [45568/54000 (84%)] Loss: -1574.278198\n",
      "    epoch          : 675\n",
      "    loss           : -1609.3221689356435\n",
      "    val_loss       : -1605.2285802804283\n",
      "    val_log_likelihood: 1715.0149397141863\n",
      "    val_log_marginal: 1657.3992338190685\n",
      "Train Epoch: 676 [512/54000 (1%)] Loss: -1885.227051\n",
      "Train Epoch: 676 [11776/54000 (22%)] Loss: -1514.673706\n",
      "Train Epoch: 676 [23040/54000 (43%)] Loss: -1577.701904\n",
      "Train Epoch: 676 [34304/54000 (64%)] Loss: -1547.891113\n",
      "Train Epoch: 676 [45568/54000 (84%)] Loss: -1530.586792\n",
      "    epoch          : 676\n",
      "    loss           : -1601.4657526299504\n",
      "    val_loss       : -1591.14680932891\n",
      "    val_log_likelihood: 1704.6738075785117\n",
      "    val_log_marginal: 1650.5798514356081\n",
      "Train Epoch: 677 [512/54000 (1%)] Loss: -1824.180420\n",
      "Train Epoch: 677 [11776/54000 (22%)] Loss: -1500.734985\n",
      "Train Epoch: 677 [23040/54000 (43%)] Loss: -1605.142090\n",
      "Train Epoch: 677 [34304/54000 (64%)] Loss: -1567.631348\n",
      "Train Epoch: 677 [45568/54000 (84%)] Loss: -1533.240723\n",
      "    epoch          : 677\n",
      "    loss           : -1601.7545637376238\n",
      "    val_loss       : -1604.426944431655\n",
      "    val_log_likelihood: 1710.0960089050898\n",
      "    val_log_marginal: 1654.126651985792\n",
      "Train Epoch: 678 [512/54000 (1%)] Loss: -1901.581909\n",
      "Train Epoch: 678 [11776/54000 (22%)] Loss: -1709.650757\n",
      "Train Epoch: 678 [23040/54000 (43%)] Loss: -1494.500366\n",
      "Train Epoch: 678 [34304/54000 (64%)] Loss: -1655.930664\n",
      "Train Epoch: 678 [45568/54000 (84%)] Loss: -1600.447144\n",
      "    epoch          : 678\n",
      "    loss           : -1599.069199363784\n",
      "    val_loss       : -1597.1126370615889\n",
      "    val_log_likelihood: 1700.6919610051825\n",
      "    val_log_marginal: 1640.6702769560056\n",
      "Train Epoch: 679 [512/54000 (1%)] Loss: -1887.410156\n",
      "Train Epoch: 679 [11776/54000 (22%)] Loss: -1692.433105\n",
      "Train Epoch: 679 [23040/54000 (43%)] Loss: -1654.378418\n",
      "Train Epoch: 679 [34304/54000 (64%)] Loss: -1469.726562\n",
      "Train Epoch: 679 [45568/54000 (84%)] Loss: -1501.395020\n",
      "    epoch          : 679\n",
      "    loss           : -1594.3711590153157\n",
      "    val_loss       : -1602.9298132511667\n",
      "    val_log_likelihood: 1705.7533563292852\n",
      "    val_log_marginal: 1645.7896350872834\n",
      "Train Epoch: 680 [512/54000 (1%)] Loss: -1877.903564\n",
      "Train Epoch: 680 [11776/54000 (22%)] Loss: -1549.650635\n",
      "Train Epoch: 680 [23040/54000 (43%)] Loss: -1484.755615\n",
      "Train Epoch: 680 [34304/54000 (64%)] Loss: -1624.612061\n",
      "Train Epoch: 680 [45568/54000 (84%)] Loss: -1571.733765\n",
      "    epoch          : 680\n",
      "    loss           : -1610.659642587794\n",
      "    val_loss       : -1611.2950007296674\n",
      "    val_log_likelihood: 1711.3826022006497\n",
      "    val_log_marginal: 1654.5426012429418\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch680.pth ...\n",
      "Train Epoch: 681 [512/54000 (1%)] Loss: -1872.799683\n",
      "Train Epoch: 681 [11776/54000 (22%)] Loss: -1500.458130\n",
      "Train Epoch: 681 [23040/54000 (43%)] Loss: -1547.750854\n",
      "Train Epoch: 681 [34304/54000 (64%)] Loss: -1676.576416\n",
      "Train Epoch: 681 [45568/54000 (84%)] Loss: -1586.204346\n",
      "    epoch          : 681\n",
      "    loss           : -1607.0779715056467\n",
      "    val_loss       : -1615.9647232665136\n",
      "    val_log_likelihood: 1714.2300505685334\n",
      "    val_log_marginal: 1655.7424213518002\n",
      "Train Epoch: 682 [512/54000 (1%)] Loss: -1533.333008\n",
      "Train Epoch: 682 [11776/54000 (22%)] Loss: -1531.226074\n",
      "Train Epoch: 682 [23040/54000 (43%)] Loss: -1570.837402\n",
      "Train Epoch: 682 [34304/54000 (64%)] Loss: -1522.669556\n",
      "Train Epoch: 682 [45568/54000 (84%)] Loss: -1470.634155\n",
      "    epoch          : 682\n",
      "    loss           : -1612.6986724551361\n",
      "    val_loss       : -1604.5240775315312\n",
      "    val_log_likelihood: 1712.9301274365719\n",
      "    val_log_marginal: 1656.6443521711324\n",
      "Train Epoch: 683 [512/54000 (1%)] Loss: -1851.880249\n",
      "Train Epoch: 683 [11776/54000 (22%)] Loss: -1682.899170\n",
      "Train Epoch: 683 [23040/54000 (43%)] Loss: -1506.666260\n",
      "Train Epoch: 683 [34304/54000 (64%)] Loss: -1606.263428\n",
      "Train Epoch: 683 [45568/54000 (84%)] Loss: -1563.528442\n",
      "    epoch          : 683\n",
      "    loss           : -1607.7168331335088\n",
      "    val_loss       : -1604.6353078214636\n",
      "    val_log_likelihood: 1710.6143617535581\n",
      "    val_log_marginal: 1653.10829362955\n",
      "Train Epoch: 684 [512/54000 (1%)] Loss: -1876.518921\n",
      "Train Epoch: 684 [11776/54000 (22%)] Loss: -1451.340942\n",
      "Train Epoch: 684 [23040/54000 (43%)] Loss: -1535.282837\n",
      "Train Epoch: 684 [34304/54000 (64%)] Loss: -1554.128906\n",
      "Train Epoch: 684 [45568/54000 (84%)] Loss: -1597.612305\n",
      "    epoch          : 684\n",
      "    loss           : -1606.8371751237623\n",
      "    val_loss       : -1611.8164595417622\n",
      "    val_log_likelihood: 1718.3533464186262\n",
      "    val_log_marginal: 1661.2472987902527\n",
      "Train Epoch: 685 [512/54000 (1%)] Loss: -1921.646240\n",
      "Train Epoch: 685 [11776/54000 (22%)] Loss: -1522.854980\n",
      "Train Epoch: 685 [23040/54000 (43%)] Loss: -1559.573853\n",
      "Train Epoch: 685 [34304/54000 (64%)] Loss: -1527.880615\n",
      "Train Epoch: 685 [45568/54000 (84%)] Loss: -1602.217163\n",
      "    epoch          : 685\n",
      "    loss           : -1601.6373254757116\n",
      "    val_loss       : -1603.5442998527853\n",
      "    val_log_likelihood: 1698.4674797435798\n",
      "    val_log_marginal: 1636.8521004487575\n",
      "Train Epoch: 686 [512/54000 (1%)] Loss: -1548.871094\n",
      "Train Epoch: 686 [11776/54000 (22%)] Loss: -1494.945312\n",
      "Train Epoch: 686 [23040/54000 (43%)] Loss: -1578.702026\n",
      "Train Epoch: 686 [34304/54000 (64%)] Loss: -1633.603271\n",
      "Train Epoch: 686 [45568/54000 (84%)] Loss: -1536.804932\n",
      "    epoch          : 686\n",
      "    loss           : -1598.4372969523515\n",
      "    val_loss       : -1602.4676777545528\n",
      "    val_log_likelihood: 1703.9993582243967\n",
      "    val_log_marginal: 1644.5775398815806\n",
      "Train Epoch: 687 [512/54000 (1%)] Loss: -1875.896362\n",
      "Train Epoch: 687 [11776/54000 (22%)] Loss: -1652.053711\n",
      "Train Epoch: 687 [23040/54000 (43%)] Loss: -1536.039307\n",
      "Train Epoch: 687 [34304/54000 (64%)] Loss: -1585.031494\n",
      "Train Epoch: 687 [45568/54000 (84%)] Loss: -1502.026123\n",
      "    epoch          : 687\n",
      "    loss           : -1601.8476465810643\n",
      "    val_loss       : -1604.6661956726416\n",
      "    val_log_likelihood: 1704.2961836711015\n",
      "    val_log_marginal: 1645.5832536940711\n",
      "Train Epoch: 688 [512/54000 (1%)] Loss: -1897.845215\n",
      "Train Epoch: 688 [11776/54000 (22%)] Loss: -1641.549927\n",
      "Train Epoch: 688 [23040/54000 (43%)] Loss: -1640.080200\n",
      "Train Epoch: 688 [34304/54000 (64%)] Loss: -1852.245605\n",
      "Train Epoch: 688 [45568/54000 (84%)] Loss: -1607.271118\n",
      "    epoch          : 688\n",
      "    loss           : -1608.1971097134128\n",
      "    val_loss       : -1612.1039869756817\n",
      "    val_log_likelihood: 1707.8698343711324\n",
      "    val_log_marginal: 1650.9263425510155\n",
      "Train Epoch: 689 [512/54000 (1%)] Loss: -1880.714233\n",
      "Train Epoch: 689 [11776/54000 (22%)] Loss: -1530.701904\n",
      "Train Epoch: 689 [23040/54000 (43%)] Loss: -1556.215820\n",
      "Train Epoch: 689 [34304/54000 (64%)] Loss: -1625.365234\n",
      "Train Epoch: 689 [45568/54000 (84%)] Loss: -1521.379395\n",
      "    epoch          : 689\n",
      "    loss           : -1609.4241641205135\n",
      "    val_loss       : -1607.728679145336\n",
      "    val_log_likelihood: 1697.6498902575804\n",
      "    val_log_marginal: 1641.9417172347814\n",
      "Train Epoch: 690 [512/54000 (1%)] Loss: -1682.948120\n",
      "Train Epoch: 690 [11776/54000 (22%)] Loss: -1522.711670\n",
      "Train Epoch: 690 [23040/54000 (43%)] Loss: -1492.141113\n",
      "Train Epoch: 690 [34304/54000 (64%)] Loss: -1492.374268\n",
      "Train Epoch: 690 [45568/54000 (84%)] Loss: -1605.893066\n",
      "    epoch          : 690\n",
      "    loss           : -1603.5154461246907\n",
      "    val_loss       : -1607.593669789327\n",
      "    val_log_likelihood: 1702.6735948619275\n",
      "    val_log_marginal: 1644.355684945581\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch690.pth ...\n",
      "Train Epoch: 691 [512/54000 (1%)] Loss: -1879.013672\n",
      "Train Epoch: 691 [11776/54000 (22%)] Loss: -1549.971191\n",
      "Train Epoch: 691 [23040/54000 (43%)] Loss: -1607.597778\n",
      "Train Epoch: 691 [34304/54000 (64%)] Loss: -1495.537109\n",
      "Train Epoch: 691 [45568/54000 (84%)] Loss: -1502.086670\n",
      "    epoch          : 691\n",
      "    loss           : -1604.985355188351\n",
      "    val_loss       : -1612.2182126540488\n",
      "    val_log_likelihood: 1710.7423905476485\n",
      "    val_log_marginal: 1653.3095311255258\n",
      "Train Epoch: 692 [512/54000 (1%)] Loss: -1884.627808\n",
      "Train Epoch: 692 [11776/54000 (22%)] Loss: -1709.292725\n",
      "Train Epoch: 692 [23040/54000 (43%)] Loss: -1667.853149\n",
      "Train Epoch: 692 [34304/54000 (64%)] Loss: -1514.501953\n",
      "Train Epoch: 692 [45568/54000 (84%)] Loss: -1508.279053\n",
      "    epoch          : 692\n",
      "    loss           : -1610.2826701249226\n",
      "    val_loss       : -1603.2935745998905\n",
      "    val_log_likelihood: 1695.3699238087872\n",
      "    val_log_marginal: 1642.275128654991\n",
      "Train Epoch: 693 [512/54000 (1%)] Loss: -1887.996216\n",
      "Train Epoch: 693 [11776/54000 (22%)] Loss: -1549.562744\n",
      "Train Epoch: 693 [23040/54000 (43%)] Loss: -1499.113770\n",
      "Train Epoch: 693 [34304/54000 (64%)] Loss: -1512.122803\n",
      "Train Epoch: 693 [45568/54000 (84%)] Loss: -1623.041382\n",
      "    epoch          : 693\n",
      "    loss           : -1611.4428094542852\n",
      "    val_loss       : -1609.9952115612657\n",
      "    val_log_likelihood: 1712.9690050181775\n",
      "    val_log_marginal: 1655.7323691897868\n",
      "Train Epoch: 694 [512/54000 (1%)] Loss: -1860.196289\n",
      "Train Epoch: 694 [11776/54000 (22%)] Loss: -1548.400879\n",
      "Train Epoch: 694 [23040/54000 (43%)] Loss: -1607.197510\n",
      "Train Epoch: 694 [34304/54000 (64%)] Loss: -1540.985840\n",
      "Train Epoch: 694 [45568/54000 (84%)] Loss: -1520.861206\n",
      "    epoch          : 694\n",
      "    loss           : -1610.8606102065285\n",
      "    val_loss       : -1605.1215141069254\n",
      "    val_log_likelihood: 1701.3292478051517\n",
      "    val_log_marginal: 1644.8614301481875\n",
      "Train Epoch: 695 [512/54000 (1%)] Loss: -1865.109863\n",
      "Train Epoch: 695 [11776/54000 (22%)] Loss: -1537.859741\n",
      "Train Epoch: 695 [23040/54000 (43%)] Loss: -1591.554688\n",
      "Train Epoch: 695 [34304/54000 (64%)] Loss: -1529.220703\n",
      "Train Epoch: 695 [45568/54000 (84%)] Loss: -1528.388916\n",
      "    epoch          : 695\n",
      "    loss           : -1611.4271240234375\n",
      "    val_loss       : -1609.5642124969918\n",
      "    val_log_likelihood: 1704.2082592048268\n",
      "    val_log_marginal: 1648.686468412362\n",
      "Train Epoch: 696 [512/54000 (1%)] Loss: -1888.649414\n",
      "Train Epoch: 696 [11776/54000 (22%)] Loss: -1581.822388\n",
      "Train Epoch: 696 [23040/54000 (43%)] Loss: -1498.399780\n",
      "Train Epoch: 696 [34304/54000 (64%)] Loss: -1601.798706\n",
      "Train Epoch: 696 [45568/54000 (84%)] Loss: -1598.666138\n",
      "    epoch          : 696\n",
      "    loss           : -1611.7448295366646\n",
      "    val_loss       : -1609.2055469581876\n",
      "    val_log_likelihood: 1709.7909443165997\n",
      "    val_log_marginal: 1656.8721409121915\n",
      "Train Epoch: 697 [512/54000 (1%)] Loss: -1876.398438\n",
      "Train Epoch: 697 [11776/54000 (22%)] Loss: -1738.186035\n",
      "Train Epoch: 697 [23040/54000 (43%)] Loss: -1570.374268\n",
      "Train Epoch: 697 [34304/54000 (64%)] Loss: -1666.483521\n",
      "Train Epoch: 697 [45568/54000 (84%)] Loss: -1651.150391\n",
      "    epoch          : 697\n",
      "    loss           : -1613.9653417001857\n",
      "    val_loss       : -1617.1416090355247\n",
      "    val_log_likelihood: 1713.3888809657333\n",
      "    val_log_marginal: 1658.4434136796817\n",
      "Train Epoch: 698 [512/54000 (1%)] Loss: -1709.387695\n",
      "Train Epoch: 698 [11776/54000 (22%)] Loss: -1576.794189\n",
      "Train Epoch: 698 [23040/54000 (43%)] Loss: -1669.947754\n",
      "Train Epoch: 698 [34304/54000 (64%)] Loss: -1623.926636\n",
      "Train Epoch: 698 [45568/54000 (84%)] Loss: -1608.566284\n",
      "    epoch          : 698\n",
      "    loss           : -1612.476070592899\n",
      "    val_loss       : -1618.3738971861117\n",
      "    val_log_likelihood: 1718.7702213702817\n",
      "    val_log_marginal: 1663.7618394930291\n",
      "Train Epoch: 699 [512/54000 (1%)] Loss: -1882.055786\n",
      "Train Epoch: 699 [11776/54000 (22%)] Loss: -1530.086426\n",
      "Train Epoch: 699 [23040/54000 (43%)] Loss: -1538.948242\n",
      "Train Epoch: 699 [34304/54000 (64%)] Loss: -1669.177002\n",
      "Train Epoch: 699 [45568/54000 (84%)] Loss: -1583.620361\n",
      "    epoch          : 699\n",
      "    loss           : -1615.3138935353497\n",
      "    val_loss       : -1617.6572080110707\n",
      "    val_log_likelihood: 1718.82562799737\n",
      "    val_log_marginal: 1663.9334800106722\n",
      "Train Epoch: 700 [512/54000 (1%)] Loss: -1892.940552\n",
      "Train Epoch: 700 [11776/54000 (22%)] Loss: -1540.438477\n",
      "Train Epoch: 700 [23040/54000 (43%)] Loss: -1674.471680\n",
      "Train Epoch: 700 [34304/54000 (64%)] Loss: -1619.347412\n",
      "Train Epoch: 700 [45568/54000 (84%)] Loss: -1519.823853\n",
      "    epoch          : 700\n",
      "    loss           : -1607.8326319326268\n",
      "    val_loss       : -1603.6854276317772\n",
      "    val_log_likelihood: 1696.4253050549196\n",
      "    val_log_marginal: 1640.4120646870263\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch700.pth ...\n",
      "Train Epoch: 701 [512/54000 (1%)] Loss: -1888.110840\n",
      "Train Epoch: 701 [11776/54000 (22%)] Loss: -1483.701660\n",
      "Train Epoch: 701 [23040/54000 (43%)] Loss: -1603.573853\n",
      "Train Epoch: 701 [34304/54000 (64%)] Loss: -1505.711426\n",
      "Train Epoch: 701 [45568/54000 (84%)] Loss: -1528.443604\n",
      "    epoch          : 701\n",
      "    loss           : -1605.1121101001702\n",
      "    val_loss       : -1608.7782931242434\n",
      "    val_log_likelihood: 1704.5839565768101\n",
      "    val_log_marginal: 1649.1016814679388\n",
      "Train Epoch: 702 [512/54000 (1%)] Loss: -1897.917114\n",
      "Train Epoch: 702 [11776/54000 (22%)] Loss: -1540.559814\n",
      "Train Epoch: 702 [23040/54000 (43%)] Loss: -1570.192383\n",
      "Train Epoch: 702 [34304/54000 (64%)] Loss: -1584.922485\n",
      "Train Epoch: 702 [45568/54000 (84%)] Loss: -1530.287354\n",
      "    epoch          : 702\n",
      "    loss           : -1611.4269838238706\n",
      "    val_loss       : -1613.7130505839953\n",
      "    val_log_likelihood: 1716.3486050143101\n",
      "    val_log_marginal: 1660.520374060507\n",
      "Train Epoch: 703 [512/54000 (1%)] Loss: -1877.630005\n",
      "Train Epoch: 703 [11776/54000 (22%)] Loss: -1682.017944\n",
      "Train Epoch: 703 [23040/54000 (43%)] Loss: -1652.003418\n",
      "Train Epoch: 703 [34304/54000 (64%)] Loss: -1666.486206\n",
      "Train Epoch: 703 [45568/54000 (84%)] Loss: -1568.824463\n",
      "    epoch          : 703\n",
      "    loss           : -1610.5415800491182\n",
      "    val_loss       : -1614.856411548691\n",
      "    val_log_likelihood: 1709.8755861792233\n",
      "    val_log_marginal: 1653.1490952327613\n",
      "Train Epoch: 704 [512/54000 (1%)] Loss: -1883.931030\n",
      "Train Epoch: 704 [11776/54000 (22%)] Loss: -1604.006226\n",
      "Train Epoch: 704 [23040/54000 (43%)] Loss: -1580.186523\n",
      "Train Epoch: 704 [34304/54000 (64%)] Loss: -1646.096191\n",
      "Train Epoch: 704 [45568/54000 (84%)] Loss: -1619.842163\n",
      "    epoch          : 704\n",
      "    loss           : -1614.1212146116955\n",
      "    val_loss       : -1610.186326019051\n",
      "    val_log_likelihood: 1719.6805746248453\n",
      "    val_log_marginal: 1663.6208948529886\n",
      "Train Epoch: 705 [512/54000 (1%)] Loss: -1884.660034\n",
      "Train Epoch: 705 [11776/54000 (22%)] Loss: -1679.707275\n",
      "Train Epoch: 705 [23040/54000 (43%)] Loss: -1586.843262\n",
      "Train Epoch: 705 [34304/54000 (64%)] Loss: -1478.145264\n",
      "Train Epoch: 705 [45568/54000 (84%)] Loss: -1552.435791\n",
      "    epoch          : 705\n",
      "    loss           : -1606.3346829556003\n",
      "    val_loss       : -1604.4065999085508\n",
      "    val_log_likelihood: 1700.658745794013\n",
      "    val_log_marginal: 1645.5845764963012\n",
      "Train Epoch: 706 [512/54000 (1%)] Loss: -1853.104126\n",
      "Train Epoch: 706 [11776/54000 (22%)] Loss: -1564.230225\n",
      "Train Epoch: 706 [23040/54000 (43%)] Loss: -1600.405029\n",
      "Train Epoch: 706 [34304/54000 (64%)] Loss: -1856.370972\n",
      "Train Epoch: 706 [45568/54000 (84%)] Loss: -1594.854126\n",
      "    epoch          : 706\n",
      "    loss           : -1605.4577576287902\n",
      "    val_loss       : -1602.5486931284188\n",
      "    val_log_likelihood: 1711.7368599164604\n",
      "    val_log_marginal: 1657.496424622594\n",
      "Train Epoch: 707 [512/54000 (1%)] Loss: -1861.070923\n",
      "Train Epoch: 707 [11776/54000 (22%)] Loss: -1582.788452\n",
      "Train Epoch: 707 [23040/54000 (43%)] Loss: -1477.848999\n",
      "Train Epoch: 707 [34304/54000 (64%)] Loss: -1625.891113\n",
      "Train Epoch: 707 [45568/54000 (84%)] Loss: -1583.472168\n",
      "    epoch          : 707\n",
      "    loss           : -1610.096300181776\n",
      "    val_loss       : -1610.9192774492492\n",
      "    val_log_likelihood: 1708.0244817450496\n",
      "    val_log_marginal: 1652.4540186908907\n",
      "Train Epoch: 708 [512/54000 (1%)] Loss: -1854.823730\n",
      "Train Epoch: 708 [11776/54000 (22%)] Loss: -1662.467041\n",
      "Train Epoch: 708 [23040/54000 (43%)] Loss: -1647.807373\n",
      "Train Epoch: 708 [34304/54000 (64%)] Loss: -1529.632935\n",
      "Train Epoch: 708 [45568/54000 (84%)] Loss: -1550.968506\n",
      "    epoch          : 708\n",
      "    loss           : -1606.9802644937345\n",
      "    val_loss       : -1594.8857095567325\n",
      "    val_log_likelihood: 1671.4078647122524\n",
      "    val_log_marginal: 1617.2849732632792\n",
      "Train Epoch: 709 [512/54000 (1%)] Loss: -1862.865234\n",
      "Train Epoch: 709 [11776/54000 (22%)] Loss: -1532.700928\n",
      "Train Epoch: 709 [23040/54000 (43%)] Loss: -1495.569824\n",
      "Train Epoch: 709 [34304/54000 (64%)] Loss: -1534.134277\n",
      "Train Epoch: 709 [45568/54000 (84%)] Loss: -1615.861328\n",
      "    epoch          : 709\n",
      "    loss           : -1603.1343655727878\n",
      "    val_loss       : -1599.547100276133\n",
      "    val_log_likelihood: 1702.5189148553527\n",
      "    val_log_marginal: 1646.2695616824362\n",
      "Train Epoch: 710 [512/54000 (1%)] Loss: -1881.539185\n",
      "Train Epoch: 710 [11776/54000 (22%)] Loss: -1560.631226\n",
      "Train Epoch: 710 [23040/54000 (43%)] Loss: -1499.157471\n",
      "Train Epoch: 710 [34304/54000 (64%)] Loss: -1512.436035\n",
      "Train Epoch: 710 [45568/54000 (84%)] Loss: -1534.336182\n",
      "    epoch          : 710\n",
      "    loss           : -1605.141172503481\n",
      "    val_loss       : -1612.5031078048512\n",
      "    val_log_likelihood: 1714.3399041808477\n",
      "    val_log_marginal: 1657.340882112687\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch710.pth ...\n",
      "Train Epoch: 711 [512/54000 (1%)] Loss: -1657.519043\n",
      "Train Epoch: 711 [11776/54000 (22%)] Loss: -1493.923096\n",
      "Train Epoch: 711 [23040/54000 (43%)] Loss: -1461.597168\n",
      "Train Epoch: 711 [34304/54000 (64%)] Loss: -1614.321045\n",
      "Train Epoch: 711 [45568/54000 (84%)] Loss: -1502.578613\n",
      "    epoch          : 711\n",
      "    loss           : -1611.7819389116646\n",
      "    val_loss       : -1607.78562369418\n",
      "    val_log_likelihood: 1710.3929963064666\n",
      "    val_log_marginal: 1649.8902706710326\n",
      "Train Epoch: 712 [512/54000 (1%)] Loss: -1695.499756\n",
      "Train Epoch: 712 [11776/54000 (22%)] Loss: -1566.934570\n",
      "Train Epoch: 712 [23040/54000 (43%)] Loss: -1551.912720\n",
      "Train Epoch: 712 [34304/54000 (64%)] Loss: -1507.316895\n",
      "Train Epoch: 712 [45568/54000 (84%)] Loss: -1514.988281\n",
      "    epoch          : 712\n",
      "    loss           : -1610.4614584139078\n",
      "    val_loss       : -1614.1203784595916\n",
      "    val_log_likelihood: 1716.9602957243967\n",
      "    val_log_marginal: 1659.7622350701638\n",
      "Train Epoch: 713 [512/54000 (1%)] Loss: -1569.583740\n",
      "Train Epoch: 713 [11776/54000 (22%)] Loss: -1711.667236\n",
      "Train Epoch: 713 [23040/54000 (43%)] Loss: -1644.152100\n",
      "Train Epoch: 713 [34304/54000 (64%)] Loss: -1591.769409\n",
      "Train Epoch: 713 [45568/54000 (84%)] Loss: -1595.760742\n",
      "    epoch          : 713\n",
      "    loss           : -1617.6573389638768\n",
      "    val_loss       : -1615.065355458269\n",
      "    val_log_likelihood: 1713.5665621615872\n",
      "    val_log_marginal: 1658.9602671762245\n",
      "Train Epoch: 714 [512/54000 (1%)] Loss: -1903.132568\n",
      "Train Epoch: 714 [11776/54000 (22%)] Loss: -1508.519775\n",
      "Train Epoch: 714 [23040/54000 (43%)] Loss: -1493.944214\n",
      "Train Epoch: 714 [34304/54000 (64%)] Loss: -1595.067017\n",
      "Train Epoch: 714 [45568/54000 (84%)] Loss: -1585.530762\n",
      "    epoch          : 714\n",
      "    loss           : -1612.5414712735922\n",
      "    val_loss       : -1609.7099572025184\n",
      "    val_log_likelihood: 1715.0955194152227\n",
      "    val_log_marginal: 1658.5723203987732\n",
      "Train Epoch: 715 [512/54000 (1%)] Loss: -1904.717285\n",
      "Train Epoch: 715 [11776/54000 (22%)] Loss: -1660.435791\n",
      "Train Epoch: 715 [23040/54000 (43%)] Loss: -1642.336060\n",
      "Train Epoch: 715 [34304/54000 (64%)] Loss: -1535.273315\n",
      "Train Epoch: 715 [45568/54000 (84%)] Loss: -1614.009888\n",
      "    epoch          : 715\n",
      "    loss           : -1616.7325910813738\n",
      "    val_loss       : -1618.1624934496542\n",
      "    val_log_likelihood: 1723.4875899211015\n",
      "    val_log_marginal: 1666.2027665021653\n",
      "Train Epoch: 716 [512/54000 (1%)] Loss: -1895.368896\n",
      "Train Epoch: 716 [11776/54000 (22%)] Loss: -1716.836914\n",
      "Train Epoch: 716 [23040/54000 (43%)] Loss: -1667.142334\n",
      "Train Epoch: 716 [34304/54000 (64%)] Loss: -1498.432861\n",
      "Train Epoch: 716 [45568/54000 (84%)] Loss: -1588.874023\n",
      "    epoch          : 716\n",
      "    loss           : -1617.057479405167\n",
      "    val_loss       : -1617.7113964047726\n",
      "    val_log_likelihood: 1720.1010935566212\n",
      "    val_log_marginal: 1662.4349460326916\n",
      "Train Epoch: 717 [512/54000 (1%)] Loss: -1881.876465\n",
      "Train Epoch: 717 [11776/54000 (22%)] Loss: -1681.588257\n",
      "Train Epoch: 717 [23040/54000 (43%)] Loss: -1542.642944\n",
      "Train Epoch: 717 [34304/54000 (64%)] Loss: -1555.147217\n",
      "Train Epoch: 717 [45568/54000 (84%)] Loss: -1574.568481\n",
      "    epoch          : 717\n",
      "    loss           : -1621.7155846321937\n",
      "    val_loss       : -1617.0493243574579\n",
      "    val_log_likelihood: 1721.9333991626702\n",
      "    val_log_marginal: 1662.3588570173113\n",
      "Train Epoch: 718 [512/54000 (1%)] Loss: -1689.926514\n",
      "Train Epoch: 718 [11776/54000 (22%)] Loss: -1550.928955\n",
      "Train Epoch: 718 [23040/54000 (43%)] Loss: -1473.947754\n",
      "Train Epoch: 718 [34304/54000 (64%)] Loss: -1654.998657\n",
      "Train Epoch: 718 [45568/54000 (84%)] Loss: -1557.405762\n",
      "    epoch          : 718\n",
      "    loss           : -1612.2162711266244\n",
      "    val_loss       : -1614.6523649862756\n",
      "    val_log_likelihood: 1713.942000889542\n",
      "    val_log_marginal: 1660.6257981247647\n",
      "Train Epoch: 719 [512/54000 (1%)] Loss: -1556.312378\n",
      "Train Epoch: 719 [11776/54000 (22%)] Loss: -1549.191040\n",
      "Train Epoch: 719 [23040/54000 (43%)] Loss: -1522.223877\n",
      "Train Epoch: 719 [34304/54000 (64%)] Loss: -1559.031006\n",
      "Train Epoch: 719 [45568/54000 (84%)] Loss: -1508.614746\n",
      "    epoch          : 719\n",
      "    loss           : -1618.4995141359839\n",
      "    val_loss       : -1617.5271879962766\n",
      "    val_log_likelihood: 1713.6320667833386\n",
      "    val_log_marginal: 1659.7347863075472\n",
      "Train Epoch: 720 [512/54000 (1%)] Loss: -1871.160767\n",
      "Train Epoch: 720 [11776/54000 (22%)] Loss: -1575.990112\n",
      "Train Epoch: 720 [23040/54000 (43%)] Loss: -1551.675049\n",
      "Train Epoch: 720 [34304/54000 (64%)] Loss: -1553.725342\n",
      "Train Epoch: 720 [45568/54000 (84%)] Loss: -1531.826660\n",
      "    epoch          : 720\n",
      "    loss           : -1615.099686726485\n",
      "    val_loss       : -1619.1305116271728\n",
      "    val_log_likelihood: 1715.661543742265\n",
      "    val_log_marginal: 1661.392790386527\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch720.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 721 [512/54000 (1%)] Loss: -1883.533081\n",
      "Train Epoch: 721 [11776/54000 (22%)] Loss: -1718.498901\n",
      "Train Epoch: 721 [23040/54000 (43%)] Loss: -1607.956787\n",
      "Train Epoch: 721 [34304/54000 (64%)] Loss: -1887.590454\n",
      "Train Epoch: 721 [45568/54000 (84%)] Loss: -1575.230713\n",
      "    epoch          : 721\n",
      "    loss           : -1619.394557839573\n",
      "    val_loss       : -1622.0613322449608\n",
      "    val_log_likelihood: 1724.1661195660581\n",
      "    val_log_marginal: 1666.572080350066\n",
      "Train Epoch: 722 [512/54000 (1%)] Loss: -1679.181519\n",
      "Train Epoch: 722 [11776/54000 (22%)] Loss: -1554.774170\n",
      "Train Epoch: 722 [23040/54000 (43%)] Loss: -1515.270874\n",
      "Train Epoch: 722 [34304/54000 (64%)] Loss: -1570.722656\n",
      "Train Epoch: 722 [45568/54000 (84%)] Loss: -1510.093750\n",
      "    epoch          : 722\n",
      "    loss           : -1620.3854823348545\n",
      "    val_loss       : -1624.0185285066761\n",
      "    val_log_likelihood: 1718.3904968866027\n",
      "    val_log_marginal: 1661.3429208167893\n",
      "Train Epoch: 723 [512/54000 (1%)] Loss: -1894.501831\n",
      "Train Epoch: 723 [11776/54000 (22%)] Loss: -1524.137695\n",
      "Train Epoch: 723 [23040/54000 (43%)] Loss: -1652.776123\n",
      "Train Epoch: 723 [34304/54000 (64%)] Loss: -1497.930664\n",
      "Train Epoch: 723 [45568/54000 (84%)] Loss: -1507.961304\n",
      "    epoch          : 723\n",
      "    loss           : -1615.4386844446162\n",
      "    val_loss       : -1620.4705370012393\n",
      "    val_log_likelihood: 1726.4000510036356\n",
      "    val_log_marginal: 1668.9926364067533\n",
      "Train Epoch: 724 [512/54000 (1%)] Loss: -1559.448975\n",
      "Train Epoch: 724 [11776/54000 (22%)] Loss: -1687.147095\n",
      "Train Epoch: 724 [23040/54000 (43%)] Loss: -1502.956177\n",
      "Train Epoch: 724 [34304/54000 (64%)] Loss: -1532.124023\n",
      "Train Epoch: 724 [45568/54000 (84%)] Loss: -1476.062500\n",
      "    epoch          : 724\n",
      "    loss           : -1601.741621867265\n",
      "    val_loss       : -1597.6831801717567\n",
      "    val_log_likelihood: 1705.1821119856127\n",
      "    val_log_marginal: 1647.634042616325\n",
      "Train Epoch: 725 [512/54000 (1%)] Loss: -1722.641968\n",
      "Train Epoch: 725 [11776/54000 (22%)] Loss: -1533.432983\n",
      "Train Epoch: 725 [23040/54000 (43%)] Loss: -1702.534058\n",
      "Train Epoch: 725 [34304/54000 (64%)] Loss: -1578.950928\n",
      "Train Epoch: 725 [45568/54000 (84%)] Loss: -1527.739746\n",
      "    epoch          : 725\n",
      "    loss           : -1603.948352171643\n",
      "    val_loss       : -1609.3667517642464\n",
      "    val_log_likelihood: 1712.607039952042\n",
      "    val_log_marginal: 1651.7404903379343\n",
      "Train Epoch: 726 [512/54000 (1%)] Loss: -1894.736328\n",
      "Train Epoch: 726 [11776/54000 (22%)] Loss: -1746.164307\n",
      "Train Epoch: 726 [23040/54000 (43%)] Loss: -1478.620117\n",
      "Train Epoch: 726 [34304/54000 (64%)] Loss: -1615.852173\n",
      "Train Epoch: 726 [45568/54000 (84%)] Loss: -1655.464111\n",
      "    epoch          : 726\n",
      "    loss           : -1613.3427903581373\n",
      "    val_loss       : -1612.3575341735534\n",
      "    val_log_likelihood: 1712.983183303682\n",
      "    val_log_marginal: 1654.9332808799636\n",
      "Train Epoch: 727 [512/54000 (1%)] Loss: -1871.657104\n",
      "Train Epoch: 727 [11776/54000 (22%)] Loss: -1649.479614\n",
      "Train Epoch: 727 [23040/54000 (43%)] Loss: -1654.345215\n",
      "Train Epoch: 727 [34304/54000 (64%)] Loss: -1507.889160\n",
      "Train Epoch: 727 [45568/54000 (84%)] Loss: -1581.543457\n",
      "    epoch          : 727\n",
      "    loss           : -1612.526693514078\n",
      "    val_loss       : -1614.0435760117662\n",
      "    val_log_likelihood: 1720.12109375\n",
      "    val_log_marginal: 1660.2994117576045\n",
      "Train Epoch: 728 [512/54000 (1%)] Loss: -1906.537964\n",
      "Train Epoch: 728 [11776/54000 (22%)] Loss: -1552.492432\n",
      "Train Epoch: 728 [23040/54000 (43%)] Loss: -1682.356201\n",
      "Train Epoch: 728 [34304/54000 (64%)] Loss: -1651.635620\n",
      "Train Epoch: 728 [45568/54000 (84%)] Loss: -1624.714355\n",
      "    epoch          : 728\n",
      "    loss           : -1617.7461686842512\n",
      "    val_loss       : -1610.8909811613398\n",
      "    val_log_likelihood: 1717.8825055112934\n",
      "    val_log_marginal: 1660.6258605487903\n",
      "Train Epoch: 729 [512/54000 (1%)] Loss: -1628.268066\n",
      "Train Epoch: 729 [11776/54000 (22%)] Loss: -1549.832642\n",
      "Train Epoch: 729 [23040/54000 (43%)] Loss: -1676.274170\n",
      "Train Epoch: 729 [34304/54000 (64%)] Loss: -1598.859375\n",
      "Train Epoch: 729 [45568/54000 (84%)] Loss: -1497.703613\n",
      "    epoch          : 729\n",
      "    loss           : -1608.8328337716584\n",
      "    val_loss       : -1613.8504998576043\n",
      "    val_log_likelihood: 1713.6651538811107\n",
      "    val_log_marginal: 1658.5029255067468\n",
      "Train Epoch: 730 [512/54000 (1%)] Loss: -1896.283569\n",
      "Train Epoch: 730 [11776/54000 (22%)] Loss: -1516.224365\n",
      "Train Epoch: 730 [23040/54000 (43%)] Loss: -1560.444458\n",
      "Train Epoch: 730 [34304/54000 (64%)] Loss: -1490.502563\n",
      "Train Epoch: 730 [45568/54000 (84%)] Loss: -1586.076660\n",
      "    epoch          : 730\n",
      "    loss           : -1613.248938834313\n",
      "    val_loss       : -1615.045856195751\n",
      "    val_log_likelihood: 1711.215901289836\n",
      "    val_log_marginal: 1651.3935464308393\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch730.pth ...\n",
      "Train Epoch: 731 [512/54000 (1%)] Loss: -1912.678467\n",
      "Train Epoch: 731 [11776/54000 (22%)] Loss: -1530.418457\n",
      "Train Epoch: 731 [23040/54000 (43%)] Loss: -1509.526855\n",
      "Train Epoch: 731 [34304/54000 (64%)] Loss: -1888.624756\n",
      "Train Epoch: 731 [45568/54000 (84%)] Loss: -1628.202148\n",
      "    epoch          : 731\n",
      "    loss           : -1611.894449064047\n",
      "    val_loss       : -1607.4079710871424\n",
      "    val_log_likelihood: 1716.7575321008662\n",
      "    val_log_marginal: 1657.087990655052\n",
      "Train Epoch: 732 [512/54000 (1%)] Loss: -1899.390259\n",
      "Train Epoch: 732 [11776/54000 (22%)] Loss: -1545.677002\n",
      "Train Epoch: 732 [23040/54000 (43%)] Loss: -1583.946045\n",
      "Train Epoch: 732 [34304/54000 (64%)] Loss: -1550.068726\n",
      "Train Epoch: 732 [45568/54000 (84%)] Loss: -1484.262939\n",
      "    epoch          : 732\n",
      "    loss           : -1609.332216168394\n",
      "    val_loss       : -1614.777596812557\n",
      "    val_log_likelihood: 1721.9545958868348\n",
      "    val_log_marginal: 1665.1841491915027\n",
      "Train Epoch: 733 [512/54000 (1%)] Loss: -1520.144653\n",
      "Train Epoch: 733 [11776/54000 (22%)] Loss: -1541.896606\n",
      "Train Epoch: 733 [23040/54000 (43%)] Loss: -1544.379272\n",
      "Train Epoch: 733 [34304/54000 (64%)] Loss: -1530.308716\n",
      "Train Epoch: 733 [45568/54000 (84%)] Loss: -1519.761353\n",
      "    epoch          : 733\n",
      "    loss           : -1613.614364170792\n",
      "    val_loss       : -1614.008981655481\n",
      "    val_log_likelihood: 1715.8986768061573\n",
      "    val_log_marginal: 1658.5806721289784\n",
      "Train Epoch: 734 [512/54000 (1%)] Loss: -1886.295288\n",
      "Train Epoch: 734 [11776/54000 (22%)] Loss: -1534.676514\n",
      "Train Epoch: 734 [23040/54000 (43%)] Loss: -1590.330078\n",
      "Train Epoch: 734 [34304/54000 (64%)] Loss: -1870.279907\n",
      "Train Epoch: 734 [45568/54000 (84%)] Loss: -1484.708374\n",
      "    epoch          : 734\n",
      "    loss           : -1611.2899242438893\n",
      "    val_loss       : -1608.4197030341493\n",
      "    val_log_likelihood: 1719.43956431776\n",
      "    val_log_marginal: 1661.5437245008416\n",
      "Train Epoch: 735 [512/54000 (1%)] Loss: -1881.393921\n",
      "Train Epoch: 735 [11776/54000 (22%)] Loss: -1688.092529\n",
      "Train Epoch: 735 [23040/54000 (43%)] Loss: -1692.976562\n",
      "Train Epoch: 735 [34304/54000 (64%)] Loss: -1572.414307\n",
      "Train Epoch: 735 [45568/54000 (84%)] Loss: -1587.615234\n",
      "    epoch          : 735\n",
      "    loss           : -1603.9643119585396\n",
      "    val_loss       : -1602.8715442810392\n",
      "    val_log_likelihood: 1713.2119708674968\n",
      "    val_log_marginal: 1654.7580923575465\n",
      "Train Epoch: 736 [512/54000 (1%)] Loss: -1736.344238\n",
      "Train Epoch: 736 [11776/54000 (22%)] Loss: -1579.995239\n",
      "Train Epoch: 736 [23040/54000 (43%)] Loss: -1690.592407\n",
      "Train Epoch: 736 [34304/54000 (64%)] Loss: -1654.773071\n",
      "Train Epoch: 736 [45568/54000 (84%)] Loss: -1620.630371\n",
      "    epoch          : 736\n",
      "    loss           : -1610.1160090984683\n",
      "    val_loss       : -1614.269441386694\n",
      "    val_log_likelihood: 1732.3551774733137\n",
      "    val_log_marginal: 1671.0725895632859\n",
      "Train Epoch: 737 [512/54000 (1%)] Loss: -1723.066406\n",
      "Train Epoch: 737 [11776/54000 (22%)] Loss: -1702.672852\n",
      "Train Epoch: 737 [23040/54000 (43%)] Loss: -1453.846436\n",
      "Train Epoch: 737 [34304/54000 (64%)] Loss: -1625.714233\n",
      "Train Epoch: 737 [45568/54000 (84%)] Loss: -1639.006836\n",
      "    epoch          : 737\n",
      "    loss           : -1611.7406718943378\n",
      "    val_loss       : -1612.2926007172757\n",
      "    val_log_likelihood: 1726.6295214360302\n",
      "    val_log_marginal: 1668.0401653825388\n",
      "Train Epoch: 738 [512/54000 (1%)] Loss: -1720.170288\n",
      "Train Epoch: 738 [11776/54000 (22%)] Loss: -1659.126221\n",
      "Train Epoch: 738 [23040/54000 (43%)] Loss: -1562.814453\n",
      "Train Epoch: 738 [34304/54000 (64%)] Loss: -1545.466919\n",
      "Train Epoch: 738 [45568/54000 (84%)] Loss: -1519.900757\n",
      "    epoch          : 738\n",
      "    loss           : -1606.1882481338955\n",
      "    val_loss       : -1615.1769269067092\n",
      "    val_log_likelihood: 1722.7887192527846\n",
      "    val_log_marginal: 1665.2075487080376\n",
      "Train Epoch: 739 [512/54000 (1%)] Loss: -1891.490967\n",
      "Train Epoch: 739 [11776/54000 (22%)] Loss: -1537.789429\n",
      "Train Epoch: 739 [23040/54000 (43%)] Loss: -1628.579956\n",
      "Train Epoch: 739 [34304/54000 (64%)] Loss: -1886.620117\n",
      "Train Epoch: 739 [45568/54000 (84%)] Loss: -1543.869629\n",
      "    epoch          : 739\n",
      "    loss           : -1611.377903097927\n",
      "    val_loss       : -1609.3285228201732\n",
      "    val_log_likelihood: 1724.2039009320854\n",
      "    val_log_marginal: 1666.3978203077252\n",
      "Train Epoch: 740 [512/54000 (1%)] Loss: -1877.150391\n",
      "Train Epoch: 740 [11776/54000 (22%)] Loss: -1560.253906\n",
      "Train Epoch: 740 [23040/54000 (43%)] Loss: -1598.327026\n",
      "Train Epoch: 740 [34304/54000 (64%)] Loss: -1551.584595\n",
      "Train Epoch: 740 [45568/54000 (84%)] Loss: -1535.283813\n",
      "    epoch          : 740\n",
      "    loss           : -1616.2049282564976\n",
      "    val_loss       : -1617.244776864951\n",
      "    val_log_likelihood: 1725.4802693282024\n",
      "    val_log_marginal: 1669.2507287287458\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch740.pth ...\n",
      "Train Epoch: 741 [512/54000 (1%)] Loss: -1910.634644\n",
      "Train Epoch: 741 [11776/54000 (22%)] Loss: -1520.439575\n",
      "Train Epoch: 741 [23040/54000 (43%)] Loss: -1628.763184\n",
      "Train Epoch: 741 [34304/54000 (64%)] Loss: -1893.575806\n",
      "Train Epoch: 741 [45568/54000 (84%)] Loss: -1616.185547\n",
      "    epoch          : 741\n",
      "    loss           : -1618.9764670192606\n",
      "    val_loss       : -1614.9261071234112\n",
      "    val_log_likelihood: 1718.1476289352568\n",
      "    val_log_marginal: 1661.1706143592364\n",
      "Train Epoch: 742 [512/54000 (1%)] Loss: -1895.402588\n",
      "Train Epoch: 742 [11776/54000 (22%)] Loss: -1648.440674\n",
      "Train Epoch: 742 [23040/54000 (43%)] Loss: -1522.966064\n",
      "Train Epoch: 742 [34304/54000 (64%)] Loss: -1568.870239\n",
      "Train Epoch: 742 [45568/54000 (84%)] Loss: -1595.447510\n",
      "    epoch          : 742\n",
      "    loss           : -1613.6647344910273\n",
      "    val_loss       : -1615.8677913393863\n",
      "    val_log_likelihood: 1720.9584030302444\n",
      "    val_log_marginal: 1660.7816075597623\n",
      "Train Epoch: 743 [512/54000 (1%)] Loss: -1876.082764\n",
      "Train Epoch: 743 [11776/54000 (22%)] Loss: -1664.534180\n",
      "Train Epoch: 743 [23040/54000 (43%)] Loss: -1507.797363\n",
      "Train Epoch: 743 [34304/54000 (64%)] Loss: -1694.486572\n",
      "Train Epoch: 743 [45568/54000 (84%)] Loss: -1562.141724\n",
      "    epoch          : 743\n",
      "    loss           : -1615.5947084332456\n",
      "    val_loss       : -1608.9723017373408\n",
      "    val_log_likelihood: 1724.8540014890161\n",
      "    val_log_marginal: 1665.2756059880342\n",
      "Train Epoch: 744 [512/54000 (1%)] Loss: -1874.242676\n",
      "Train Epoch: 744 [11776/54000 (22%)] Loss: -1551.743652\n",
      "Train Epoch: 744 [23040/54000 (43%)] Loss: -1575.103027\n",
      "Train Epoch: 744 [34304/54000 (64%)] Loss: -1566.814331\n",
      "Train Epoch: 744 [45568/54000 (84%)] Loss: -1576.696533\n",
      "    epoch          : 744\n",
      "    loss           : -1615.5900250425434\n",
      "    val_loss       : -1619.9738560425449\n",
      "    val_log_likelihood: 1729.4159576113861\n",
      "    val_log_marginal: 1670.2294683794833\n",
      "Train Epoch: 745 [512/54000 (1%)] Loss: -1882.251709\n",
      "Train Epoch: 745 [11776/54000 (22%)] Loss: -1450.532715\n",
      "Train Epoch: 745 [23040/54000 (43%)] Loss: -1567.265625\n",
      "Train Epoch: 745 [34304/54000 (64%)] Loss: -1507.589478\n",
      "Train Epoch: 745 [45568/54000 (84%)] Loss: -1551.713379\n",
      "    epoch          : 745\n",
      "    loss           : -1617.8617825649753\n",
      "    val_loss       : -1622.2497704112718\n",
      "    val_log_likelihood: 1733.2078277285736\n",
      "    val_log_marginal: 1673.8085107223712\n",
      "Train Epoch: 746 [512/54000 (1%)] Loss: -1882.482788\n",
      "Train Epoch: 746 [11776/54000 (22%)] Loss: -1696.031006\n",
      "Train Epoch: 746 [23040/54000 (43%)] Loss: -1598.591797\n",
      "Train Epoch: 746 [34304/54000 (64%)] Loss: -1607.047485\n",
      "Train Epoch: 746 [45568/54000 (84%)] Loss: -1623.523438\n",
      "    epoch          : 746\n",
      "    loss           : -1619.4539831180384\n",
      "    val_loss       : -1617.868773497724\n",
      "    val_log_likelihood: 1714.0117139155323\n",
      "    val_log_marginal: 1654.2158101467176\n",
      "Train Epoch: 747 [512/54000 (1%)] Loss: -1511.565796\n",
      "Train Epoch: 747 [11776/54000 (22%)] Loss: -1568.638550\n",
      "Train Epoch: 747 [23040/54000 (43%)] Loss: -1615.761475\n",
      "Train Epoch: 747 [34304/54000 (64%)] Loss: -1553.700439\n",
      "Train Epoch: 747 [45568/54000 (84%)] Loss: -1573.139771\n",
      "    epoch          : 747\n",
      "    loss           : -1618.0477778368656\n",
      "    val_loss       : -1620.6126412185056\n",
      "    val_log_likelihood: 1724.570120329904\n",
      "    val_log_marginal: 1667.1641030870194\n",
      "Train Epoch: 748 [512/54000 (1%)] Loss: -1889.646118\n",
      "Train Epoch: 748 [11776/54000 (22%)] Loss: -1493.204102\n",
      "Train Epoch: 748 [23040/54000 (43%)] Loss: -1619.049072\n",
      "Train Epoch: 748 [34304/54000 (64%)] Loss: -1875.628662\n",
      "Train Epoch: 748 [45568/54000 (84%)] Loss: -1616.954346\n",
      "    epoch          : 748\n",
      "    loss           : -1623.2156378113398\n",
      "    val_loss       : -1617.5592549340063\n",
      "    val_log_likelihood: 1727.4321712078433\n",
      "    val_log_marginal: 1670.5387919957723\n",
      "Train Epoch: 749 [512/54000 (1%)] Loss: -1881.051636\n",
      "Train Epoch: 749 [11776/54000 (22%)] Loss: -1470.743896\n",
      "Train Epoch: 749 [23040/54000 (43%)] Loss: -1503.710083\n",
      "Train Epoch: 749 [34304/54000 (64%)] Loss: -1515.411377\n",
      "Train Epoch: 749 [45568/54000 (84%)] Loss: -1606.741089\n",
      "    epoch          : 749\n",
      "    loss           : -1619.1458522683322\n",
      "    val_loss       : -1625.227231823701\n",
      "    val_log_likelihood: 1725.9518692469833\n",
      "    val_log_marginal: 1669.5153018649446\n",
      "Train Epoch: 750 [512/54000 (1%)] Loss: -1897.564697\n",
      "Train Epoch: 750 [11776/54000 (22%)] Loss: -1573.258057\n",
      "Train Epoch: 750 [23040/54000 (43%)] Loss: -1523.094482\n",
      "Train Epoch: 750 [34304/54000 (64%)] Loss: -1517.760864\n",
      "Train Epoch: 750 [45568/54000 (84%)] Loss: -1590.783936\n",
      "    epoch          : 750\n",
      "    loss           : -1620.2671079730044\n",
      "    val_loss       : -1602.4165474083181\n",
      "    val_log_likelihood: 1684.4344808748453\n",
      "    val_log_marginal: 1625.3110388982673\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch750.pth ...\n",
      "Train Epoch: 751 [512/54000 (1%)] Loss: -1880.777832\n",
      "Train Epoch: 751 [11776/54000 (22%)] Loss: -1614.404785\n",
      "Train Epoch: 751 [23040/54000 (43%)] Loss: -1506.712280\n",
      "Train Epoch: 751 [34304/54000 (64%)] Loss: -1636.187744\n",
      "Train Epoch: 751 [45568/54000 (84%)] Loss: -1562.536987\n",
      "    epoch          : 751\n",
      "    loss           : -1617.8198085067295\n",
      "    val_loss       : -1618.9659350393893\n",
      "    val_log_likelihood: 1711.1656482054455\n",
      "    val_log_marginal: 1655.0333944364165\n",
      "Train Epoch: 752 [512/54000 (1%)] Loss: -1896.659912\n",
      "Train Epoch: 752 [11776/54000 (22%)] Loss: -1562.318359\n",
      "Train Epoch: 752 [23040/54000 (43%)] Loss: -1629.920410\n",
      "Train Epoch: 752 [34304/54000 (64%)] Loss: -1902.640381\n",
      "Train Epoch: 752 [45568/54000 (84%)] Loss: -1622.080444\n",
      "    epoch          : 752\n",
      "    loss           : -1623.1493657178219\n",
      "    val_loss       : -1624.0527567628144\n",
      "    val_log_likelihood: 1719.5189704517327\n",
      "    val_log_marginal: 1663.582346205372\n",
      "Train Epoch: 753 [512/54000 (1%)] Loss: -1882.778076\n",
      "Train Epoch: 753 [11776/54000 (22%)] Loss: -1530.125732\n",
      "Train Epoch: 753 [23040/54000 (43%)] Loss: -1595.861816\n",
      "Train Epoch: 753 [34304/54000 (64%)] Loss: -1646.626221\n",
      "Train Epoch: 753 [45568/54000 (84%)] Loss: -1686.787964\n",
      "    epoch          : 753\n",
      "    loss           : -1624.4246463586787\n",
      "    val_loss       : -1626.3854695119073\n",
      "    val_log_likelihood: 1726.1740529277538\n",
      "    val_log_marginal: 1669.399910934509\n",
      "Train Epoch: 754 [512/54000 (1%)] Loss: -1922.197388\n",
      "Train Epoch: 754 [11776/54000 (22%)] Loss: -1692.784180\n",
      "Train Epoch: 754 [23040/54000 (43%)] Loss: -1548.762695\n",
      "Train Epoch: 754 [34304/54000 (64%)] Loss: -1682.111572\n",
      "Train Epoch: 754 [45568/54000 (84%)] Loss: -1551.053833\n",
      "    epoch          : 754\n",
      "    loss           : -1614.3981474319307\n",
      "    val_loss       : -1596.7395412979813\n",
      "    val_log_likelihood: 1685.7064257329052\n",
      "    val_log_marginal: 1627.7143715429534\n",
      "Train Epoch: 755 [512/54000 (1%)] Loss: -1879.956055\n",
      "Train Epoch: 755 [11776/54000 (22%)] Loss: -1546.843140\n",
      "Train Epoch: 755 [23040/54000 (43%)] Loss: -1515.743652\n",
      "Train Epoch: 755 [34304/54000 (64%)] Loss: -1525.443604\n",
      "Train Epoch: 755 [45568/54000 (84%)] Loss: -1656.101440\n",
      "    epoch          : 755\n",
      "    loss           : -1610.9996603786356\n",
      "    val_loss       : -1610.6594103259395\n",
      "    val_log_likelihood: 1700.9414920618037\n",
      "    val_log_marginal: 1649.2273557334215\n",
      "Train Epoch: 756 [512/54000 (1%)] Loss: -1884.904053\n",
      "Train Epoch: 756 [11776/54000 (22%)] Loss: -1583.984131\n",
      "Train Epoch: 756 [23040/54000 (43%)] Loss: -1689.140869\n",
      "Train Epoch: 756 [34304/54000 (64%)] Loss: -1642.842529\n",
      "Train Epoch: 756 [45568/54000 (84%)] Loss: -1626.382080\n",
      "    epoch          : 756\n",
      "    loss           : -1612.421715462562\n",
      "    val_loss       : -1613.8045084760663\n",
      "    val_log_likelihood: 1716.239991442992\n",
      "    val_log_marginal: 1659.038323365928\n",
      "Train Epoch: 757 [512/54000 (1%)] Loss: -1890.812134\n",
      "Train Epoch: 757 [11776/54000 (22%)] Loss: -1538.640381\n",
      "Train Epoch: 757 [23040/54000 (43%)] Loss: -1612.461304\n",
      "Train Epoch: 757 [34304/54000 (64%)] Loss: -1529.722168\n",
      "Train Epoch: 757 [45568/54000 (84%)] Loss: -1544.681763\n",
      "    epoch          : 757\n",
      "    loss           : -1614.2451800355816\n",
      "    val_loss       : -1614.0227433199543\n",
      "    val_log_likelihood: 1719.3501097424196\n",
      "    val_log_marginal: 1663.9512752177802\n",
      "Train Epoch: 758 [512/54000 (1%)] Loss: -1898.257812\n",
      "Train Epoch: 758 [11776/54000 (22%)] Loss: -1672.401489\n",
      "Train Epoch: 758 [23040/54000 (43%)] Loss: -1600.716309\n",
      "Train Epoch: 758 [34304/54000 (64%)] Loss: -1586.907959\n",
      "Train Epoch: 758 [45568/54000 (84%)] Loss: -1544.032227\n",
      "    epoch          : 758\n",
      "    loss           : -1624.3230379312345\n",
      "    val_loss       : -1623.2272527534976\n",
      "    val_log_likelihood: 1725.0643358891552\n",
      "    val_log_marginal: 1670.842289912173\n",
      "Train Epoch: 759 [512/54000 (1%)] Loss: -1852.225098\n",
      "Train Epoch: 759 [11776/54000 (22%)] Loss: -1711.639038\n",
      "Train Epoch: 759 [23040/54000 (43%)] Loss: -1602.939697\n",
      "Train Epoch: 759 [34304/54000 (64%)] Loss: -1547.324707\n",
      "Train Epoch: 759 [45568/54000 (84%)] Loss: -1536.822021\n",
      "    epoch          : 759\n",
      "    loss           : -1612.5778240543782\n",
      "    val_loss       : -1612.0377331166574\n",
      "    val_log_likelihood: 1712.8770461885056\n",
      "    val_log_marginal: 1652.2284098237913\n",
      "Train Epoch: 760 [512/54000 (1%)] Loss: -1886.854248\n",
      "Train Epoch: 760 [11776/54000 (22%)] Loss: -1692.682495\n",
      "Train Epoch: 760 [23040/54000 (43%)] Loss: -1587.742920\n",
      "Train Epoch: 760 [34304/54000 (64%)] Loss: -1471.817261\n",
      "Train Epoch: 760 [45568/54000 (84%)] Loss: -1595.749756\n",
      "    epoch          : 760\n",
      "    loss           : -1614.5059198058477\n",
      "    val_loss       : -1608.7319997077034\n",
      "    val_log_likelihood: 1714.167672638846\n",
      "    val_log_marginal: 1652.6931969058505\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch760.pth ...\n",
      "Train Epoch: 761 [512/54000 (1%)] Loss: -1693.808594\n",
      "Train Epoch: 761 [11776/54000 (22%)] Loss: -1614.222412\n",
      "Train Epoch: 761 [23040/54000 (43%)] Loss: -1603.402710\n",
      "Train Epoch: 761 [34304/54000 (64%)] Loss: -1905.526855\n",
      "Train Epoch: 761 [45568/54000 (84%)] Loss: -1591.072876\n",
      "    epoch          : 761\n",
      "    loss           : -1610.7392070505878\n",
      "    val_loss       : -1610.2089338362089\n",
      "    val_log_likelihood: 1718.6432972520886\n",
      "    val_log_marginal: 1660.9664646278534\n",
      "Train Epoch: 762 [512/54000 (1%)] Loss: -1909.905396\n",
      "Train Epoch: 762 [11776/54000 (22%)] Loss: -1701.520020\n",
      "Train Epoch: 762 [23040/54000 (43%)] Loss: -1620.288086\n",
      "Train Epoch: 762 [34304/54000 (64%)] Loss: -1511.068359\n",
      "Train Epoch: 762 [45568/54000 (84%)] Loss: -1523.760132\n",
      "    epoch          : 762\n",
      "    loss           : -1616.2559258489325\n",
      "    val_loss       : -1622.1484461741356\n",
      "    val_log_likelihood: 1721.4029347636913\n",
      "    val_log_marginal: 1664.5660166484258\n",
      "Train Epoch: 763 [512/54000 (1%)] Loss: -1511.895020\n",
      "Train Epoch: 763 [11776/54000 (22%)] Loss: -1621.454956\n",
      "Train Epoch: 763 [23040/54000 (43%)] Loss: -1528.465088\n",
      "Train Epoch: 763 [34304/54000 (64%)] Loss: -1592.553955\n",
      "Train Epoch: 763 [45568/54000 (84%)] Loss: -1635.663452\n",
      "    epoch          : 763\n",
      "    loss           : -1620.4303051032643\n",
      "    val_loss       : -1617.1796369795493\n",
      "    val_log_likelihood: 1723.2947973874536\n",
      "    val_log_marginal: 1664.9366307870696\n",
      "Train Epoch: 764 [512/54000 (1%)] Loss: -1885.132080\n",
      "Train Epoch: 764 [11776/54000 (22%)] Loss: -1524.022217\n",
      "Train Epoch: 764 [23040/54000 (43%)] Loss: -1519.601196\n",
      "Train Epoch: 764 [34304/54000 (64%)] Loss: -1887.005859\n",
      "Train Epoch: 764 [45568/54000 (84%)] Loss: -1674.259033\n",
      "    epoch          : 764\n",
      "    loss           : -1620.283012163521\n",
      "    val_loss       : -1618.9970181146175\n",
      "    val_log_likelihood: 1728.527347375851\n",
      "    val_log_marginal: 1672.5879883901014\n",
      "Train Epoch: 765 [512/54000 (1%)] Loss: -1565.380127\n",
      "Train Epoch: 765 [11776/54000 (22%)] Loss: -1543.682617\n",
      "Train Epoch: 765 [23040/54000 (43%)] Loss: -1621.086548\n",
      "Train Epoch: 765 [34304/54000 (64%)] Loss: -1666.019043\n",
      "Train Epoch: 765 [45568/54000 (84%)] Loss: -1605.361694\n",
      "    epoch          : 765\n",
      "    loss           : -1620.4136999149134\n",
      "    val_loss       : -1621.5079479351696\n",
      "    val_log_likelihood: 1721.069876189279\n",
      "    val_log_marginal: 1664.630715763344\n",
      "Train Epoch: 766 [512/54000 (1%)] Loss: -1859.794678\n",
      "Train Epoch: 766 [11776/54000 (22%)] Loss: -1701.167358\n",
      "Train Epoch: 766 [23040/54000 (43%)] Loss: -1521.638672\n",
      "Train Epoch: 766 [34304/54000 (64%)] Loss: -1661.890381\n",
      "Train Epoch: 766 [45568/54000 (84%)] Loss: -1602.510254\n",
      "    epoch          : 766\n",
      "    loss           : -1616.9860610206529\n",
      "    val_loss       : -1621.6936584012121\n",
      "    val_log_likelihood: 1721.4681952448175\n",
      "    val_log_marginal: 1666.5265747795786\n",
      "Train Epoch: 767 [512/54000 (1%)] Loss: -1900.694580\n",
      "Train Epoch: 767 [11776/54000 (22%)] Loss: -1571.574951\n",
      "Train Epoch: 767 [23040/54000 (43%)] Loss: -1539.934692\n",
      "Train Epoch: 767 [34304/54000 (64%)] Loss: -1553.617920\n",
      "Train Epoch: 767 [45568/54000 (84%)] Loss: -1551.196411\n",
      "    epoch          : 767\n",
      "    loss           : -1614.6090571337406\n",
      "    val_loss       : -1595.8990252781382\n",
      "    val_log_likelihood: 1690.4820351175742\n",
      "    val_log_marginal: 1629.6545746451548\n",
      "Train Epoch: 768 [512/54000 (1%)] Loss: -1875.161255\n",
      "Train Epoch: 768 [11776/54000 (22%)] Loss: -1553.704224\n",
      "Train Epoch: 768 [23040/54000 (43%)] Loss: -1479.660889\n",
      "Train Epoch: 768 [34304/54000 (64%)] Loss: -1868.012695\n",
      "Train Epoch: 768 [45568/54000 (84%)] Loss: -1537.212891\n",
      "    epoch          : 768\n",
      "    loss           : -1603.7599904277538\n",
      "    val_loss       : -1607.2042444177284\n",
      "    val_log_likelihood: 1706.7844310798268\n",
      "    val_log_marginal: 1646.1661149477493\n",
      "Train Epoch: 769 [512/54000 (1%)] Loss: -1898.110229\n",
      "Train Epoch: 769 [11776/54000 (22%)] Loss: -1725.493164\n",
      "Train Epoch: 769 [23040/54000 (43%)] Loss: -1594.542236\n",
      "Train Epoch: 769 [34304/54000 (64%)] Loss: -1646.561890\n",
      "Train Epoch: 769 [45568/54000 (84%)] Loss: -1535.387329\n",
      "    epoch          : 769\n",
      "    loss           : -1615.3416796391552\n",
      "    val_loss       : -1617.6812125701863\n",
      "    val_log_likelihood: 1714.3141654741646\n",
      "    val_log_marginal: 1656.8544408507667\n",
      "Train Epoch: 770 [512/54000 (1%)] Loss: -1883.562744\n",
      "Train Epoch: 770 [11776/54000 (22%)] Loss: -1499.107422\n",
      "Train Epoch: 770 [23040/54000 (43%)] Loss: -1557.066650\n",
      "Train Epoch: 770 [34304/54000 (64%)] Loss: -1892.508789\n",
      "Train Epoch: 770 [45568/54000 (84%)] Loss: -1525.841797\n",
      "    epoch          : 770\n",
      "    loss           : -1615.125213925201\n",
      "    val_loss       : -1614.3591128299197\n",
      "    val_log_likelihood: 1710.0824627262532\n",
      "    val_log_marginal: 1649.1212267470132\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch770.pth ...\n",
      "Train Epoch: 771 [512/54000 (1%)] Loss: -1713.110840\n",
      "Train Epoch: 771 [11776/54000 (22%)] Loss: -1726.180420\n",
      "Train Epoch: 771 [23040/54000 (43%)] Loss: -1510.389404\n",
      "Train Epoch: 771 [34304/54000 (64%)] Loss: -1563.690796\n",
      "Train Epoch: 771 [45568/54000 (84%)] Loss: -1623.602661\n",
      "    epoch          : 771\n",
      "    loss           : -1619.6233043104114\n",
      "    val_loss       : -1620.0169238262156\n",
      "    val_log_likelihood: 1715.952746702893\n",
      "    val_log_marginal: 1657.63670068437\n",
      "Train Epoch: 772 [512/54000 (1%)] Loss: -1896.855957\n",
      "Train Epoch: 772 [11776/54000 (22%)] Loss: -1720.459961\n",
      "Train Epoch: 772 [23040/54000 (43%)] Loss: -1580.803955\n",
      "Train Epoch: 772 [34304/54000 (64%)] Loss: -1893.032227\n",
      "Train Epoch: 772 [45568/54000 (84%)] Loss: -1600.993164\n",
      "    epoch          : 772\n",
      "    loss           : -1623.0203555267635\n",
      "    val_loss       : -1623.5763263669842\n",
      "    val_log_likelihood: 1724.1764133566678\n",
      "    val_log_marginal: 1665.2736171064068\n",
      "Train Epoch: 773 [512/54000 (1%)] Loss: -1906.406128\n",
      "Train Epoch: 773 [11776/54000 (22%)] Loss: -1571.569702\n",
      "Train Epoch: 773 [23040/54000 (43%)] Loss: -1607.596680\n",
      "Train Epoch: 773 [34304/54000 (64%)] Loss: -1544.533325\n",
      "Train Epoch: 773 [45568/54000 (84%)] Loss: -1620.806641\n",
      "    epoch          : 773\n",
      "    loss           : -1623.70903997138\n",
      "    val_loss       : -1624.6675817321452\n",
      "    val_log_likelihood: 1725.0882749651919\n",
      "    val_log_marginal: 1668.0639291631962\n",
      "Train Epoch: 774 [512/54000 (1%)] Loss: -1911.906250\n",
      "Train Epoch: 774 [11776/54000 (22%)] Loss: -1667.891602\n",
      "Train Epoch: 774 [23040/54000 (43%)] Loss: -1623.291260\n",
      "Train Epoch: 774 [34304/54000 (64%)] Loss: -1536.886353\n",
      "Train Epoch: 774 [45568/54000 (84%)] Loss: -1624.053955\n",
      "    epoch          : 774\n",
      "    loss           : -1621.8511141031095\n",
      "    val_loss       : -1627.221555626075\n",
      "    val_log_likelihood: 1726.0388074818225\n",
      "    val_log_marginal: 1668.7357367478035\n",
      "Train Epoch: 775 [512/54000 (1%)] Loss: -1900.053589\n",
      "Train Epoch: 775 [11776/54000 (22%)] Loss: -1677.899658\n",
      "Train Epoch: 775 [23040/54000 (43%)] Loss: -1670.175659\n",
      "Train Epoch: 775 [34304/54000 (64%)] Loss: -1516.099854\n",
      "Train Epoch: 775 [45568/54000 (84%)] Loss: -1615.044312\n",
      "    epoch          : 775\n",
      "    loss           : -1620.7423615408416\n",
      "    val_loss       : -1620.6119019845407\n",
      "    val_log_likelihood: 1711.0767616800742\n",
      "    val_log_marginal: 1653.2009677426097\n",
      "Train Epoch: 776 [512/54000 (1%)] Loss: -1900.607056\n",
      "Train Epoch: 776 [11776/54000 (22%)] Loss: -1624.125000\n",
      "Train Epoch: 776 [23040/54000 (43%)] Loss: -1596.244141\n",
      "Train Epoch: 776 [34304/54000 (64%)] Loss: -1525.946045\n",
      "Train Epoch: 776 [45568/54000 (84%)] Loss: -1606.802734\n",
      "    epoch          : 776\n",
      "    loss           : -1617.9628954594677\n",
      "    val_loss       : -1614.9980697166543\n",
      "    val_log_likelihood: 1717.486138372138\n",
      "    val_log_marginal: 1656.2910822621675\n",
      "Train Epoch: 777 [512/54000 (1%)] Loss: -1874.855225\n",
      "Train Epoch: 777 [11776/54000 (22%)] Loss: -1512.241211\n",
      "Train Epoch: 777 [23040/54000 (43%)] Loss: -1596.453125\n",
      "Train Epoch: 777 [34304/54000 (64%)] Loss: -1633.988281\n",
      "Train Epoch: 777 [45568/54000 (84%)] Loss: -1622.693970\n",
      "    epoch          : 777\n",
      "    loss           : -1615.749590278852\n",
      "    val_loss       : -1607.591249004154\n",
      "    val_log_likelihood: 1707.8115621132426\n",
      "    val_log_marginal: 1649.9428586444321\n",
      "Train Epoch: 778 [512/54000 (1%)] Loss: -1682.517578\n",
      "Train Epoch: 778 [11776/54000 (22%)] Loss: -1669.366455\n",
      "Train Epoch: 778 [23040/54000 (43%)] Loss: -1593.696899\n",
      "Train Epoch: 778 [34304/54000 (64%)] Loss: -1471.233887\n",
      "Train Epoch: 778 [45568/54000 (84%)] Loss: -1559.870361\n",
      "    epoch          : 778\n",
      "    loss           : -1613.9337097772277\n",
      "    val_loss       : -1616.5173327500868\n",
      "    val_log_likelihood: 1722.311197110922\n",
      "    val_log_marginal: 1663.4207015504674\n",
      "Train Epoch: 779 [512/54000 (1%)] Loss: -1881.270996\n",
      "Train Epoch: 779 [11776/54000 (22%)] Loss: -1485.713257\n",
      "Train Epoch: 779 [23040/54000 (43%)] Loss: -1594.931885\n",
      "Train Epoch: 779 [34304/54000 (64%)] Loss: -1876.956055\n",
      "Train Epoch: 779 [45568/54000 (84%)] Loss: -1558.259766\n",
      "    epoch          : 779\n",
      "    loss           : -1619.9035741220607\n",
      "    val_loss       : -1619.1516385884347\n",
      "    val_log_likelihood: 1716.113066116182\n",
      "    val_log_marginal: 1658.152993505341\n",
      "Train Epoch: 780 [512/54000 (1%)] Loss: -1866.337158\n",
      "Train Epoch: 780 [11776/54000 (22%)] Loss: -1569.840210\n",
      "Train Epoch: 780 [23040/54000 (43%)] Loss: -1487.770020\n",
      "Train Epoch: 780 [34304/54000 (64%)] Loss: -1528.876709\n",
      "Train Epoch: 780 [45568/54000 (84%)] Loss: -1602.875977\n",
      "    epoch          : 780\n",
      "    loss           : -1620.46061600789\n",
      "    val_loss       : -1620.112514125008\n",
      "    val_log_likelihood: 1723.5028342067606\n",
      "    val_log_marginal: 1664.5112883004333\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch780.pth ...\n",
      "Train Epoch: 781 [512/54000 (1%)] Loss: -1895.393799\n",
      "Train Epoch: 781 [11776/54000 (22%)] Loss: -1619.002197\n",
      "Train Epoch: 781 [23040/54000 (43%)] Loss: -1525.704346\n",
      "Train Epoch: 781 [34304/54000 (64%)] Loss: -1585.395264\n",
      "Train Epoch: 781 [45568/54000 (84%)] Loss: -1665.693359\n",
      "    epoch          : 781\n",
      "    loss           : -1620.0502458326887\n",
      "    val_loss       : -1627.2663302798526\n",
      "    val_log_likelihood: 1724.8656066290223\n",
      "    val_log_marginal: 1669.308312793087\n",
      "Train Epoch: 782 [512/54000 (1%)] Loss: -1883.441284\n",
      "Train Epoch: 782 [11776/54000 (22%)] Loss: -1618.722046\n",
      "Train Epoch: 782 [23040/54000 (43%)] Loss: -1681.048828\n",
      "Train Epoch: 782 [34304/54000 (64%)] Loss: -1495.096436\n",
      "Train Epoch: 782 [45568/54000 (84%)] Loss: -1641.873413\n",
      "    epoch          : 782\n",
      "    loss           : -1622.3854182781558\n",
      "    val_loss       : -1629.3576391086092\n",
      "    val_log_likelihood: 1724.0756775506652\n",
      "    val_log_marginal: 1666.5286406529606\n",
      "Train Epoch: 783 [512/54000 (1%)] Loss: -1901.291504\n",
      "Train Epoch: 783 [11776/54000 (22%)] Loss: -1690.830688\n",
      "Train Epoch: 783 [23040/54000 (43%)] Loss: -1610.605957\n",
      "Train Epoch: 783 [34304/54000 (64%)] Loss: -1595.649658\n",
      "Train Epoch: 783 [45568/54000 (84%)] Loss: -1579.423584\n",
      "    epoch          : 783\n",
      "    loss           : -1627.4195604985302\n",
      "    val_loss       : -1622.614541613543\n",
      "    val_log_likelihood: 1733.570206141708\n",
      "    val_log_marginal: 1674.4647143833008\n",
      "Train Epoch: 784 [512/54000 (1%)] Loss: -1885.021362\n",
      "Train Epoch: 784 [11776/54000 (22%)] Loss: -1690.783936\n",
      "Train Epoch: 784 [23040/54000 (43%)] Loss: -1709.785400\n",
      "Train Epoch: 784 [34304/54000 (64%)] Loss: -1575.303589\n",
      "Train Epoch: 784 [45568/54000 (84%)] Loss: -1555.908569\n",
      "    epoch          : 784\n",
      "    loss           : -1623.245064008354\n",
      "    val_loss       : -1627.7613910694818\n",
      "    val_log_likelihood: 1731.5830936243037\n",
      "    val_log_marginal: 1674.763408625314\n",
      "Train Epoch: 785 [512/54000 (1%)] Loss: -1460.604980\n",
      "Train Epoch: 785 [11776/54000 (22%)] Loss: -1701.056641\n",
      "Train Epoch: 785 [23040/54000 (43%)] Loss: -1613.359619\n",
      "Train Epoch: 785 [34304/54000 (64%)] Loss: -1583.655029\n",
      "Train Epoch: 785 [45568/54000 (84%)] Loss: -1637.972412\n",
      "    epoch          : 785\n",
      "    loss           : -1625.9113286084469\n",
      "    val_loss       : -1627.257494588779\n",
      "    val_log_likelihood: 1726.2727558400372\n",
      "    val_log_marginal: 1670.4708728361506\n",
      "Train Epoch: 786 [512/54000 (1%)] Loss: -1885.497559\n",
      "Train Epoch: 786 [11776/54000 (22%)] Loss: -1562.957764\n",
      "Train Epoch: 786 [23040/54000 (43%)] Loss: -1583.957275\n",
      "Train Epoch: 786 [34304/54000 (64%)] Loss: -1861.370361\n",
      "Train Epoch: 786 [45568/54000 (84%)] Loss: -1532.735107\n",
      "    epoch          : 786\n",
      "    loss           : -1617.1629819964419\n",
      "    val_loss       : -1616.7096201173758\n",
      "    val_log_likelihood: 1731.6178401531558\n",
      "    val_log_marginal: 1671.1051031887569\n",
      "Train Epoch: 787 [512/54000 (1%)] Loss: -1900.645508\n",
      "Train Epoch: 787 [11776/54000 (22%)] Loss: -1539.196777\n",
      "Train Epoch: 787 [23040/54000 (43%)] Loss: -1537.986206\n",
      "Train Epoch: 787 [34304/54000 (64%)] Loss: -1577.349487\n",
      "Train Epoch: 787 [45568/54000 (84%)] Loss: -1538.079956\n",
      "    epoch          : 787\n",
      "    loss           : -1615.2658763923268\n",
      "    val_loss       : -1610.9520878607111\n",
      "    val_log_likelihood: 1731.567735728651\n",
      "    val_log_marginal: 1671.6720577218364\n",
      "Train Epoch: 788 [512/54000 (1%)] Loss: -1884.845459\n",
      "Train Epoch: 788 [11776/54000 (22%)] Loss: -1639.710205\n",
      "Train Epoch: 788 [23040/54000 (43%)] Loss: -1499.428955\n",
      "Train Epoch: 788 [34304/54000 (64%)] Loss: -1546.621338\n",
      "Train Epoch: 788 [45568/54000 (84%)] Loss: -1572.034058\n",
      "    epoch          : 788\n",
      "    loss           : -1606.029456412438\n",
      "    val_loss       : -1611.0578712902798\n",
      "    val_log_likelihood: 1724.6599290300123\n",
      "    val_log_marginal: 1666.2044364934843\n",
      "Train Epoch: 789 [512/54000 (1%)] Loss: -1867.645752\n",
      "Train Epoch: 789 [11776/54000 (22%)] Loss: -1510.943970\n",
      "Train Epoch: 789 [23040/54000 (43%)] Loss: -1474.325439\n",
      "Train Epoch: 789 [34304/54000 (64%)] Loss: -1548.021484\n",
      "Train Epoch: 789 [45568/54000 (84%)] Loss: -1525.321289\n",
      "    epoch          : 789\n",
      "    loss           : -1612.3391209970607\n",
      "    val_loss       : -1617.998068736949\n",
      "    val_log_likelihood: 1725.6652554049351\n",
      "    val_log_marginal: 1668.5469846673936\n",
      "Train Epoch: 790 [512/54000 (1%)] Loss: -1881.769165\n",
      "Train Epoch: 790 [11776/54000 (22%)] Loss: -1562.942383\n",
      "Train Epoch: 790 [23040/54000 (43%)] Loss: -1640.838379\n",
      "Train Epoch: 790 [34304/54000 (64%)] Loss: -1586.745605\n",
      "Train Epoch: 790 [45568/54000 (84%)] Loss: -1606.758301\n",
      "    epoch          : 790\n",
      "    loss           : -1617.7742605681467\n",
      "    val_loss       : -1616.0005518277314\n",
      "    val_log_likelihood: 1719.9770266089108\n",
      "    val_log_marginal: 1663.4720380037174\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch790.pth ...\n",
      "Train Epoch: 791 [512/54000 (1%)] Loss: -1873.340576\n",
      "Train Epoch: 791 [11776/54000 (22%)] Loss: -1674.581299\n",
      "Train Epoch: 791 [23040/54000 (43%)] Loss: -1562.763428\n",
      "Train Epoch: 791 [34304/54000 (64%)] Loss: -1591.082153\n",
      "Train Epoch: 791 [45568/54000 (84%)] Loss: -1480.198364\n",
      "    epoch          : 791\n",
      "    loss           : -1616.9098782681003\n",
      "    val_loss       : -1614.785112552059\n",
      "    val_log_likelihood: 1722.2982564491801\n",
      "    val_log_marginal: 1666.432031700805\n",
      "Train Epoch: 792 [512/54000 (1%)] Loss: -1882.100586\n",
      "Train Epoch: 792 [11776/54000 (22%)] Loss: -1655.012451\n",
      "Train Epoch: 792 [23040/54000 (43%)] Loss: -1543.000610\n",
      "Train Epoch: 792 [34304/54000 (64%)] Loss: -1572.135132\n",
      "Train Epoch: 792 [45568/54000 (84%)] Loss: -1568.075317\n",
      "    epoch          : 792\n",
      "    loss           : -1622.4722114789604\n",
      "    val_loss       : -1620.0397888061286\n",
      "    val_log_likelihood: 1726.5484425761913\n",
      "    val_log_marginal: 1668.8291561351812\n",
      "Train Epoch: 793 [512/54000 (1%)] Loss: -1882.077759\n",
      "Train Epoch: 793 [11776/54000 (22%)] Loss: -1701.591675\n",
      "Train Epoch: 793 [23040/54000 (43%)] Loss: -1623.411255\n",
      "Train Epoch: 793 [34304/54000 (64%)] Loss: -1654.840942\n",
      "Train Epoch: 793 [45568/54000 (84%)] Loss: -1590.738647\n",
      "    epoch          : 793\n",
      "    loss           : -1603.013752852336\n",
      "    val_loss       : -1601.6585870360643\n",
      "    val_log_likelihood: 1698.964707176284\n",
      "    val_log_marginal: 1642.6285232784835\n",
      "Train Epoch: 794 [512/54000 (1%)] Loss: -1867.775635\n",
      "Train Epoch: 794 [11776/54000 (22%)] Loss: -1648.151123\n",
      "Train Epoch: 794 [23040/54000 (43%)] Loss: -1507.161621\n",
      "Train Epoch: 794 [34304/54000 (64%)] Loss: -1890.995850\n",
      "Train Epoch: 794 [45568/54000 (84%)] Loss: -1515.673950\n",
      "    epoch          : 794\n",
      "    loss           : -1605.4570469620205\n",
      "    val_loss       : -1610.7262208542998\n",
      "    val_log_likelihood: 1714.9086732769956\n",
      "    val_log_marginal: 1656.7699762790685\n",
      "Train Epoch: 795 [512/54000 (1%)] Loss: -1906.008423\n",
      "Train Epoch: 795 [11776/54000 (22%)] Loss: -1585.667236\n",
      "Train Epoch: 795 [23040/54000 (43%)] Loss: -1447.971558\n",
      "Train Epoch: 795 [34304/54000 (64%)] Loss: -1631.781006\n",
      "Train Epoch: 795 [45568/54000 (84%)] Loss: -1612.875244\n",
      "    epoch          : 795\n",
      "    loss           : -1612.575708974706\n",
      "    val_loss       : -1621.062272734184\n",
      "    val_log_likelihood: 1724.4244372679455\n",
      "    val_log_marginal: 1664.8009366777974\n",
      "Train Epoch: 796 [512/54000 (1%)] Loss: -1903.779541\n",
      "Train Epoch: 796 [11776/54000 (22%)] Loss: -1709.673950\n",
      "Train Epoch: 796 [23040/54000 (43%)] Loss: -1631.135986\n",
      "Train Epoch: 796 [34304/54000 (64%)] Loss: -1562.817017\n",
      "Train Epoch: 796 [45568/54000 (84%)] Loss: -1524.228638\n",
      "    epoch          : 796\n",
      "    loss           : -1619.7291296024134\n",
      "    val_loss       : -1621.7808665612454\n",
      "    val_log_likelihood: 1724.706483746519\n",
      "    val_log_marginal: 1668.0300559261907\n",
      "Train Epoch: 797 [512/54000 (1%)] Loss: -1887.726807\n",
      "Train Epoch: 797 [11776/54000 (22%)] Loss: -1487.791504\n",
      "Train Epoch: 797 [23040/54000 (43%)] Loss: -1513.909912\n",
      "Train Epoch: 797 [34304/54000 (64%)] Loss: -1585.220947\n",
      "Train Epoch: 797 [45568/54000 (84%)] Loss: -1639.447510\n",
      "    epoch          : 797\n",
      "    loss           : -1618.7718288308322\n",
      "    val_loss       : -1616.3073218922123\n",
      "    val_log_likelihood: 1728.1148101504486\n",
      "    val_log_marginal: 1670.8563817510533\n",
      "Train Epoch: 798 [512/54000 (1%)] Loss: -1900.920654\n",
      "Train Epoch: 798 [11776/54000 (22%)] Loss: -1527.762573\n",
      "Train Epoch: 798 [23040/54000 (43%)] Loss: -1479.003662\n",
      "Train Epoch: 798 [34304/54000 (64%)] Loss: -1860.019653\n",
      "Train Epoch: 798 [45568/54000 (84%)] Loss: -1654.492920\n",
      "    epoch          : 798\n",
      "    loss           : -1623.7827474764078\n",
      "    val_loss       : -1623.4662449912048\n",
      "    val_log_likelihood: 1725.4341968498607\n",
      "    val_log_marginal: 1669.9404277569051\n",
      "Train Epoch: 799 [512/54000 (1%)] Loss: -1872.407471\n",
      "Train Epoch: 799 [11776/54000 (22%)] Loss: -1696.618286\n",
      "Train Epoch: 799 [23040/54000 (43%)] Loss: -1519.238281\n",
      "Train Epoch: 799 [34304/54000 (64%)] Loss: -1500.642334\n",
      "Train Epoch: 799 [45568/54000 (84%)] Loss: -1534.153320\n",
      "    epoch          : 799\n",
      "    loss           : -1613.4583184270575\n",
      "    val_loss       : -1611.210743486992\n",
      "    val_log_likelihood: 1707.574432675201\n",
      "    val_log_marginal: 1641.2519471884373\n",
      "Train Epoch: 800 [512/54000 (1%)] Loss: -1880.274536\n",
      "Train Epoch: 800 [11776/54000 (22%)] Loss: -1561.784424\n",
      "Train Epoch: 800 [23040/54000 (43%)] Loss: -1490.806396\n",
      "Train Epoch: 800 [34304/54000 (64%)] Loss: -1548.828125\n",
      "Train Epoch: 800 [45568/54000 (84%)] Loss: -1617.626221\n",
      "    epoch          : 800\n",
      "    loss           : -1614.126247292698\n",
      "    val_loss       : -1623.8425784490469\n",
      "    val_log_likelihood: 1723.8146501295637\n",
      "    val_log_marginal: 1659.6820403707989\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch800.pth ...\n",
      "Train Epoch: 801 [512/54000 (1%)] Loss: -1901.954102\n",
      "Train Epoch: 801 [11776/54000 (22%)] Loss: -1721.869019\n",
      "Train Epoch: 801 [23040/54000 (43%)] Loss: -1671.856812\n",
      "Train Epoch: 801 [34304/54000 (64%)] Loss: -1570.898682\n",
      "Train Epoch: 801 [45568/54000 (84%)] Loss: -1581.276367\n",
      "    epoch          : 801\n",
      "    loss           : -1621.952961836711\n",
      "    val_loss       : -1621.757981253253\n",
      "    val_log_likelihood: 1711.309218604966\n",
      "    val_log_marginal: 1654.058951513805\n",
      "Train Epoch: 802 [512/54000 (1%)] Loss: -1891.342285\n",
      "Train Epoch: 802 [11776/54000 (22%)] Loss: -1719.837769\n",
      "Train Epoch: 802 [23040/54000 (43%)] Loss: -1613.184326\n",
      "Train Epoch: 802 [34304/54000 (64%)] Loss: -1581.919800\n",
      "Train Epoch: 802 [45568/54000 (84%)] Loss: -1555.519775\n",
      "    epoch          : 802\n",
      "    loss           : -1621.6374717183633\n",
      "    val_loss       : -1624.5961000914456\n",
      "    val_log_likelihood: 1722.7335990679146\n",
      "    val_log_marginal: 1664.9994754644645\n",
      "Train Epoch: 803 [512/54000 (1%)] Loss: -1895.976440\n",
      "Train Epoch: 803 [11776/54000 (22%)] Loss: -1492.404419\n",
      "Train Epoch: 803 [23040/54000 (43%)] Loss: -1557.493652\n",
      "Train Epoch: 803 [34304/54000 (64%)] Loss: -1624.061035\n",
      "Train Epoch: 803 [45568/54000 (84%)] Loss: -1606.467163\n",
      "    epoch          : 803\n",
      "    loss           : -1623.508434937732\n",
      "    val_loss       : -1625.7209010622141\n",
      "    val_log_likelihood: 1725.9937248607673\n",
      "    val_log_marginal: 1666.102956654025\n",
      "Train Epoch: 804 [512/54000 (1%)] Loss: -1707.883057\n",
      "Train Epoch: 804 [11776/54000 (22%)] Loss: -1565.473022\n",
      "Train Epoch: 804 [23040/54000 (43%)] Loss: -1555.983765\n",
      "Train Epoch: 804 [34304/54000 (64%)] Loss: -1615.525635\n",
      "Train Epoch: 804 [45568/54000 (84%)] Loss: -1634.474731\n",
      "    epoch          : 804\n",
      "    loss           : -1625.6930028329982\n",
      "    val_loss       : -1624.0132701777218\n",
      "    val_log_likelihood: 1730.4955837136447\n",
      "    val_log_marginal: 1672.2180437186348\n",
      "Train Epoch: 805 [512/54000 (1%)] Loss: -1879.404419\n",
      "Train Epoch: 805 [11776/54000 (22%)] Loss: -1702.933105\n",
      "Train Epoch: 805 [23040/54000 (43%)] Loss: -1543.205566\n",
      "Train Epoch: 805 [34304/54000 (64%)] Loss: -1559.878662\n",
      "Train Epoch: 805 [45568/54000 (84%)] Loss: -1634.740234\n",
      "    epoch          : 805\n",
      "    loss           : -1618.2381096263923\n",
      "    val_loss       : -1621.7159516446218\n",
      "    val_log_likelihood: 1728.6227676844833\n",
      "    val_log_marginal: 1670.577910846468\n",
      "Train Epoch: 806 [512/54000 (1%)] Loss: -1710.944336\n",
      "Train Epoch: 806 [11776/54000 (22%)] Loss: -1581.801025\n",
      "Train Epoch: 806 [23040/54000 (43%)] Loss: -1678.842285\n",
      "Train Epoch: 806 [34304/54000 (64%)] Loss: -1589.660645\n",
      "Train Epoch: 806 [45568/54000 (84%)] Loss: -1630.182495\n",
      "    epoch          : 806\n",
      "    loss           : -1622.22504205987\n",
      "    val_loss       : -1629.3903635531701\n",
      "    val_log_likelihood: 1730.5676704633354\n",
      "    val_log_marginal: 1673.7248031135723\n",
      "Train Epoch: 807 [512/54000 (1%)] Loss: -1703.538818\n",
      "Train Epoch: 807 [11776/54000 (22%)] Loss: -1548.728027\n",
      "Train Epoch: 807 [23040/54000 (43%)] Loss: -1477.412842\n",
      "Train Epoch: 807 [34304/54000 (64%)] Loss: -1898.773804\n",
      "Train Epoch: 807 [45568/54000 (84%)] Loss: -1602.100098\n",
      "    epoch          : 807\n",
      "    loss           : -1623.0099493347773\n",
      "    val_loss       : -1621.8782814745562\n",
      "    val_log_likelihood: 1721.4599984046256\n",
      "    val_log_marginal: 1664.3978399265704\n",
      "Train Epoch: 808 [512/54000 (1%)] Loss: -1888.099609\n",
      "Train Epoch: 808 [11776/54000 (22%)] Loss: -1548.425781\n",
      "Train Epoch: 808 [23040/54000 (43%)] Loss: -1579.243652\n",
      "Train Epoch: 808 [34304/54000 (64%)] Loss: -1515.751831\n",
      "Train Epoch: 808 [45568/54000 (84%)] Loss: -1618.185669\n",
      "    epoch          : 808\n",
      "    loss           : -1625.3759548073947\n",
      "    val_loss       : -1624.5174941820746\n",
      "    val_log_likelihood: 1729.580725943688\n",
      "    val_log_marginal: 1669.7775968406363\n",
      "Train Epoch: 809 [512/54000 (1%)] Loss: -1872.123779\n",
      "Train Epoch: 809 [11776/54000 (22%)] Loss: -1682.857544\n",
      "Train Epoch: 809 [23040/54000 (43%)] Loss: -1514.635620\n",
      "Train Epoch: 809 [34304/54000 (64%)] Loss: -1601.254761\n",
      "Train Epoch: 809 [45568/54000 (84%)] Loss: -1537.838257\n",
      "    epoch          : 809\n",
      "    loss           : -1620.400337445854\n",
      "    val_loss       : -1624.830763971055\n",
      "    val_log_likelihood: 1729.6050928701268\n",
      "    val_log_marginal: 1672.3792720983856\n",
      "Train Epoch: 810 [512/54000 (1%)] Loss: -1898.210449\n",
      "Train Epoch: 810 [11776/54000 (22%)] Loss: -1545.682739\n",
      "Train Epoch: 810 [23040/54000 (43%)] Loss: -1646.844971\n",
      "Train Epoch: 810 [34304/54000 (64%)] Loss: -1654.266846\n",
      "Train Epoch: 810 [45568/54000 (84%)] Loss: -1550.553955\n",
      "    epoch          : 810\n",
      "    loss           : -1623.5378889329363\n",
      "    val_loss       : -1614.7801322559412\n",
      "    val_log_likelihood: 1723.4405904335551\n",
      "    val_log_marginal: 1666.6389463631688\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch810.pth ...\n",
      "Train Epoch: 811 [512/54000 (1%)] Loss: -1532.956543\n",
      "Train Epoch: 811 [11776/54000 (22%)] Loss: -1510.667603\n",
      "Train Epoch: 811 [23040/54000 (43%)] Loss: -1526.828613\n",
      "Train Epoch: 811 [34304/54000 (64%)] Loss: -1449.109375\n",
      "Train Epoch: 811 [45568/54000 (84%)] Loss: -1574.221313\n",
      "    epoch          : 811\n",
      "    loss           : -1613.364228805693\n",
      "    val_loss       : -1615.626227965016\n",
      "    val_log_likelihood: 1732.619002842667\n",
      "    val_log_marginal: 1675.7742685539704\n",
      "Train Epoch: 812 [512/54000 (1%)] Loss: -1892.917603\n",
      "Train Epoch: 812 [11776/54000 (22%)] Loss: -1709.956787\n",
      "Train Epoch: 812 [23040/54000 (43%)] Loss: -1622.483032\n",
      "Train Epoch: 812 [34304/54000 (64%)] Loss: -1659.828613\n",
      "Train Epoch: 812 [45568/54000 (84%)] Loss: -1510.669434\n",
      "    epoch          : 812\n",
      "    loss           : -1621.8553974415997\n",
      "    val_loss       : -1622.8876000180803\n",
      "    val_log_likelihood: 1724.4093005491955\n",
      "    val_log_marginal: 1668.340793400837\n",
      "Train Epoch: 813 [512/54000 (1%)] Loss: -1740.086426\n",
      "Train Epoch: 813 [11776/54000 (22%)] Loss: -1560.881470\n",
      "Train Epoch: 813 [23040/54000 (43%)] Loss: -1524.242920\n",
      "Train Epoch: 813 [34304/54000 (64%)] Loss: -1881.754272\n",
      "Train Epoch: 813 [45568/54000 (84%)] Loss: -1546.208008\n",
      "    epoch          : 813\n",
      "    loss           : -1618.5667917988087\n",
      "    val_loss       : -1620.1150023058647\n",
      "    val_log_likelihood: 1732.2476347366182\n",
      "    val_log_marginal: 1673.6648259385868\n",
      "Train Epoch: 814 [512/54000 (1%)] Loss: -1879.994507\n",
      "Train Epoch: 814 [11776/54000 (22%)] Loss: -1544.258545\n",
      "Train Epoch: 814 [23040/54000 (43%)] Loss: -1597.784180\n",
      "Train Epoch: 814 [34304/54000 (64%)] Loss: -1508.449951\n",
      "Train Epoch: 814 [45568/54000 (84%)] Loss: -1579.402222\n",
      "    epoch          : 814\n",
      "    loss           : -1627.0464205600247\n",
      "    val_loss       : -1622.0507493508462\n",
      "    val_log_likelihood: 1718.6026562983448\n",
      "    val_log_marginal: 1662.4377264848329\n",
      "Train Epoch: 815 [512/54000 (1%)] Loss: -1886.444580\n",
      "Train Epoch: 815 [11776/54000 (22%)] Loss: -1741.247192\n",
      "Train Epoch: 815 [23040/54000 (43%)] Loss: -1498.337158\n",
      "Train Epoch: 815 [34304/54000 (64%)] Loss: -1878.070801\n",
      "Train Epoch: 815 [45568/54000 (84%)] Loss: -1598.715820\n",
      "    epoch          : 815\n",
      "    loss           : -1615.5981215675279\n",
      "    val_loss       : -1592.9449949268676\n",
      "    val_log_likelihood: 1707.8263326210551\n",
      "    val_log_marginal: 1649.1902612200063\n",
      "Train Epoch: 816 [512/54000 (1%)] Loss: -1896.719727\n",
      "Train Epoch: 816 [11776/54000 (22%)] Loss: -1553.890869\n",
      "Train Epoch: 816 [23040/54000 (43%)] Loss: -1537.059204\n",
      "Train Epoch: 816 [34304/54000 (64%)] Loss: -1793.546021\n",
      "Train Epoch: 816 [45568/54000 (84%)] Loss: -1583.176270\n",
      "    epoch          : 816\n",
      "    loss           : -1599.174153242961\n",
      "    val_loss       : -1608.4538576990678\n",
      "    val_log_likelihood: 1720.8282301496752\n",
      "    val_log_marginal: 1663.6266240459672\n",
      "Train Epoch: 817 [512/54000 (1%)] Loss: -1838.507568\n",
      "Train Epoch: 817 [11776/54000 (22%)] Loss: -1528.705811\n",
      "Train Epoch: 817 [23040/54000 (43%)] Loss: -1662.395142\n",
      "Train Epoch: 817 [34304/54000 (64%)] Loss: -1571.698242\n",
      "Train Epoch: 817 [45568/54000 (84%)] Loss: -1587.476074\n",
      "    epoch          : 817\n",
      "    loss           : -1616.7416013207767\n",
      "    val_loss       : -1616.3837268967459\n",
      "    val_log_likelihood: 1731.2308301264698\n",
      "    val_log_marginal: 1672.5734262349922\n",
      "Train Epoch: 818 [512/54000 (1%)] Loss: -1904.952881\n",
      "Train Epoch: 818 [11776/54000 (22%)] Loss: -1512.647217\n",
      "Train Epoch: 818 [23040/54000 (43%)] Loss: -1615.265381\n",
      "Train Epoch: 818 [34304/54000 (64%)] Loss: -1600.531738\n",
      "Train Epoch: 818 [45568/54000 (84%)] Loss: -1621.814209\n",
      "    epoch          : 818\n",
      "    loss           : -1618.5991646039604\n",
      "    val_loss       : -1622.1972779548626\n",
      "    val_log_likelihood: 1730.4347383102568\n",
      "    val_log_marginal: 1675.4156244408782\n",
      "Train Epoch: 819 [512/54000 (1%)] Loss: -1876.430420\n",
      "Train Epoch: 819 [11776/54000 (22%)] Loss: -1715.539062\n",
      "Train Epoch: 819 [23040/54000 (43%)] Loss: -1690.357910\n",
      "Train Epoch: 819 [34304/54000 (64%)] Loss: -1695.287842\n",
      "Train Epoch: 819 [45568/54000 (84%)] Loss: -1613.648071\n",
      "    epoch          : 819\n",
      "    loss           : -1619.2441043664912\n",
      "    val_loss       : -1620.2112079977824\n",
      "    val_log_likelihood: 1727.7541842318997\n",
      "    val_log_marginal: 1670.1398945993901\n",
      "Train Epoch: 820 [512/54000 (1%)] Loss: -1883.458252\n",
      "Train Epoch: 820 [11776/54000 (22%)] Loss: -1542.918091\n",
      "Train Epoch: 820 [23040/54000 (43%)] Loss: -1495.592041\n",
      "Train Epoch: 820 [34304/54000 (64%)] Loss: -1888.741577\n",
      "Train Epoch: 820 [45568/54000 (84%)] Loss: -1557.764404\n",
      "    epoch          : 820\n",
      "    loss           : -1618.5251404412902\n",
      "    val_loss       : -1622.5729085418332\n",
      "    val_log_likelihood: 1722.1071596051206\n",
      "    val_log_marginal: 1662.7587682461865\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch820.pth ...\n",
      "Train Epoch: 821 [512/54000 (1%)] Loss: -1881.171387\n",
      "Train Epoch: 821 [11776/54000 (22%)] Loss: -1469.458496\n",
      "Train Epoch: 821 [23040/54000 (43%)] Loss: -1474.334839\n",
      "Train Epoch: 821 [34304/54000 (64%)] Loss: -1546.278198\n",
      "Train Epoch: 821 [45568/54000 (84%)] Loss: -1650.619873\n",
      "    epoch          : 821\n",
      "    loss           : -1620.9275409962872\n",
      "    val_loss       : -1627.0482191802878\n",
      "    val_log_likelihood: 1729.0637605874845\n",
      "    val_log_marginal: 1671.387711386556\n",
      "Train Epoch: 822 [512/54000 (1%)] Loss: -1912.610718\n",
      "Train Epoch: 822 [11776/54000 (22%)] Loss: -1607.069580\n",
      "Train Epoch: 822 [23040/54000 (43%)] Loss: -1508.587524\n",
      "Train Epoch: 822 [34304/54000 (64%)] Loss: -1635.467651\n",
      "Train Epoch: 822 [45568/54000 (84%)] Loss: -1582.594482\n",
      "    epoch          : 822\n",
      "    loss           : -1623.8401459042389\n",
      "    val_loss       : -1627.6873746943652\n",
      "    val_log_likelihood: 1722.3164715153157\n",
      "    val_log_marginal: 1667.4513070447063\n",
      "Train Epoch: 823 [512/54000 (1%)] Loss: -1689.898193\n",
      "Train Epoch: 823 [11776/54000 (22%)] Loss: -1687.701660\n",
      "Train Epoch: 823 [23040/54000 (43%)] Loss: -1476.637329\n",
      "Train Epoch: 823 [34304/54000 (64%)] Loss: -1919.532593\n",
      "Train Epoch: 823 [45568/54000 (84%)] Loss: -1574.047363\n",
      "    epoch          : 823\n",
      "    loss           : -1627.7352899230352\n",
      "    val_loss       : -1630.5046789575886\n",
      "    val_log_likelihood: 1732.1582732247834\n",
      "    val_log_marginal: 1674.90400348966\n",
      "Train Epoch: 824 [512/54000 (1%)] Loss: -1883.447021\n",
      "Train Epoch: 824 [11776/54000 (22%)] Loss: -1560.089600\n",
      "Train Epoch: 824 [23040/54000 (43%)] Loss: -1520.533691\n",
      "Train Epoch: 824 [34304/54000 (64%)] Loss: -1691.223022\n",
      "Train Epoch: 824 [45568/54000 (84%)] Loss: -1510.173462\n",
      "    epoch          : 824\n",
      "    loss           : -1625.1888065149287\n",
      "    val_loss       : -1631.4807792783595\n",
      "    val_log_likelihood: 1734.3401265663676\n",
      "    val_log_marginal: 1675.916374262257\n",
      "Train Epoch: 825 [512/54000 (1%)] Loss: -1904.697998\n",
      "Train Epoch: 825 [11776/54000 (22%)] Loss: -1562.327148\n",
      "Train Epoch: 825 [23040/54000 (43%)] Loss: -1531.703369\n",
      "Train Epoch: 825 [34304/54000 (64%)] Loss: -1592.246216\n",
      "Train Epoch: 825 [45568/54000 (84%)] Loss: -1623.382568\n",
      "    epoch          : 825\n",
      "    loss           : -1633.8637356899753\n",
      "    val_loss       : -1631.7900495714148\n",
      "    val_log_likelihood: 1735.0298794283726\n",
      "    val_log_marginal: 1676.5077340645844\n",
      "Train Epoch: 826 [512/54000 (1%)] Loss: -1891.132568\n",
      "Train Epoch: 826 [11776/54000 (22%)] Loss: -1637.492188\n",
      "Train Epoch: 826 [23040/54000 (43%)] Loss: -1534.163330\n",
      "Train Epoch: 826 [34304/54000 (64%)] Loss: -1913.544312\n",
      "Train Epoch: 826 [45568/54000 (84%)] Loss: -1565.983032\n",
      "    epoch          : 826\n",
      "    loss           : -1625.55604115099\n",
      "    val_loss       : -1629.396090349164\n",
      "    val_log_likelihood: 1723.9413192295792\n",
      "    val_log_marginal: 1665.1657994146283\n",
      "Train Epoch: 827 [512/54000 (1%)] Loss: -1722.056885\n",
      "Train Epoch: 827 [11776/54000 (22%)] Loss: -1557.650146\n",
      "Train Epoch: 827 [23040/54000 (43%)] Loss: -1659.928955\n",
      "Train Epoch: 827 [34304/54000 (64%)] Loss: -1542.532837\n",
      "Train Epoch: 827 [45568/54000 (84%)] Loss: -1673.885864\n",
      "    epoch          : 827\n",
      "    loss           : -1627.7329500406095\n",
      "    val_loss       : -1629.3554528925272\n",
      "    val_log_likelihood: 1728.282823619276\n",
      "    val_log_marginal: 1670.4893609570652\n",
      "Train Epoch: 828 [512/54000 (1%)] Loss: -1895.781250\n",
      "Train Epoch: 828 [11776/54000 (22%)] Loss: -1642.238525\n",
      "Train Epoch: 828 [23040/54000 (43%)] Loss: -1669.127686\n",
      "Train Epoch: 828 [34304/54000 (64%)] Loss: -1629.860962\n",
      "Train Epoch: 828 [45568/54000 (84%)] Loss: -1598.228638\n",
      "    epoch          : 828\n",
      "    loss           : -1630.7294063756963\n",
      "    val_loss       : -1634.119721681396\n",
      "    val_log_likelihood: 1732.329292523979\n",
      "    val_log_marginal: 1673.8995903571245\n",
      "Train Epoch: 829 [512/54000 (1%)] Loss: -1546.262207\n",
      "Train Epoch: 829 [11776/54000 (22%)] Loss: -1554.541016\n",
      "Train Epoch: 829 [23040/54000 (43%)] Loss: -1485.978271\n",
      "Train Epoch: 829 [34304/54000 (64%)] Loss: -1579.227783\n",
      "Train Epoch: 829 [45568/54000 (84%)] Loss: -1606.821045\n",
      "    epoch          : 829\n",
      "    loss           : -1627.6429781772122\n",
      "    val_loss       : -1628.100872192827\n",
      "    val_log_likelihood: 1728.9398809754023\n",
      "    val_log_marginal: 1670.3227690079314\n",
      "Train Epoch: 830 [512/54000 (1%)] Loss: -1886.916748\n",
      "Train Epoch: 830 [11776/54000 (22%)] Loss: -1713.592529\n",
      "Train Epoch: 830 [23040/54000 (43%)] Loss: -1533.172119\n",
      "Train Epoch: 830 [34304/54000 (64%)] Loss: -1583.334961\n",
      "Train Epoch: 830 [45568/54000 (84%)] Loss: -1642.979736\n",
      "    epoch          : 830\n",
      "    loss           : -1626.9137349648051\n",
      "    val_loss       : -1630.8763324561537\n",
      "    val_log_likelihood: 1727.5781926825496\n",
      "    val_log_marginal: 1669.035593819676\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch830.pth ...\n",
      "Train Epoch: 831 [512/54000 (1%)] Loss: -1905.386230\n",
      "Train Epoch: 831 [11776/54000 (22%)] Loss: -1718.937378\n",
      "Train Epoch: 831 [23040/54000 (43%)] Loss: -1640.119629\n",
      "Train Epoch: 831 [34304/54000 (64%)] Loss: -1643.970215\n",
      "Train Epoch: 831 [45568/54000 (84%)] Loss: -1546.998047\n",
      "    epoch          : 831\n",
      "    loss           : -1628.2292250831529\n",
      "    val_loss       : -1624.430088016352\n",
      "    val_log_likelihood: 1717.691596002862\n",
      "    val_log_marginal: 1659.633270573519\n",
      "Train Epoch: 832 [512/54000 (1%)] Loss: -1898.190186\n",
      "Train Epoch: 832 [11776/54000 (22%)] Loss: -1609.890625\n",
      "Train Epoch: 832 [23040/54000 (43%)] Loss: -1630.083984\n",
      "Train Epoch: 832 [34304/54000 (64%)] Loss: -1621.448730\n",
      "Train Epoch: 832 [45568/54000 (84%)] Loss: -1538.718994\n",
      "    epoch          : 832\n",
      "    loss           : -1620.3612447304301\n",
      "    val_loss       : -1618.6020130446402\n",
      "    val_log_likelihood: 1727.75396184638\n",
      "    val_log_marginal: 1667.9337097339073\n",
      "Train Epoch: 833 [512/54000 (1%)] Loss: -1895.000977\n",
      "Train Epoch: 833 [11776/54000 (22%)] Loss: -1669.952515\n",
      "Train Epoch: 833 [23040/54000 (43%)] Loss: -1568.948120\n",
      "Train Epoch: 833 [34304/54000 (64%)] Loss: -1704.613770\n",
      "Train Epoch: 833 [45568/54000 (84%)] Loss: -1551.418945\n",
      "    epoch          : 833\n",
      "    loss           : -1620.9359396755106\n",
      "    val_loss       : -1623.8511716553037\n",
      "    val_log_likelihood: 1732.6443330368193\n",
      "    val_log_marginal: 1672.8475239974548\n",
      "Train Epoch: 834 [512/54000 (1%)] Loss: -1876.819214\n",
      "Train Epoch: 834 [11776/54000 (22%)] Loss: -1567.661499\n",
      "Train Epoch: 834 [23040/54000 (43%)] Loss: -1592.230713\n",
      "Train Epoch: 834 [34304/54000 (64%)] Loss: -1701.583740\n",
      "Train Epoch: 834 [45568/54000 (84%)] Loss: -1606.506226\n",
      "    epoch          : 834\n",
      "    loss           : -1619.5007251701732\n",
      "    val_loss       : -1616.3815939461738\n",
      "    val_log_likelihood: 1708.7987543993656\n",
      "    val_log_marginal: 1652.2074181700775\n",
      "Train Epoch: 835 [512/54000 (1%)] Loss: -1899.113159\n",
      "Train Epoch: 835 [11776/54000 (22%)] Loss: -1554.697998\n",
      "Train Epoch: 835 [23040/54000 (43%)] Loss: -1632.736816\n",
      "Train Epoch: 835 [34304/54000 (64%)] Loss: -1621.819336\n",
      "Train Epoch: 835 [45568/54000 (84%)] Loss: -1654.012329\n",
      "    epoch          : 835\n",
      "    loss           : -1621.308243251083\n",
      "    val_loss       : -1628.700897020426\n",
      "    val_log_likelihood: 1724.1091743695854\n",
      "    val_log_marginal: 1667.7666988648512\n",
      "Train Epoch: 836 [512/54000 (1%)] Loss: -1886.587036\n",
      "Train Epoch: 836 [11776/54000 (22%)] Loss: -1546.007935\n",
      "Train Epoch: 836 [23040/54000 (43%)] Loss: -1577.056641\n",
      "Train Epoch: 836 [34304/54000 (64%)] Loss: -1537.208496\n",
      "Train Epoch: 836 [45568/54000 (84%)] Loss: -1544.541626\n",
      "    epoch          : 836\n",
      "    loss           : -1630.869086237237\n",
      "    val_loss       : -1629.216024041471\n",
      "    val_log_likelihood: 1726.5414458926361\n",
      "    val_log_marginal: 1669.5566918593418\n",
      "Train Epoch: 837 [512/54000 (1%)] Loss: -1896.052979\n",
      "Train Epoch: 837 [11776/54000 (22%)] Loss: -1594.430298\n",
      "Train Epoch: 837 [23040/54000 (43%)] Loss: -1478.231323\n",
      "Train Epoch: 837 [34304/54000 (64%)] Loss: -1688.325684\n",
      "Train Epoch: 837 [45568/54000 (84%)] Loss: -1590.993774\n",
      "    epoch          : 837\n",
      "    loss           : -1628.3571342241646\n",
      "    val_loss       : -1629.6346607354774\n",
      "    val_log_likelihood: 1728.0867895749536\n",
      "    val_log_marginal: 1671.3518142875948\n",
      "Train Epoch: 838 [512/54000 (1%)] Loss: -1895.352539\n",
      "Train Epoch: 838 [11776/54000 (22%)] Loss: -1535.047852\n",
      "Train Epoch: 838 [23040/54000 (43%)] Loss: -1630.254761\n",
      "Train Epoch: 838 [34304/54000 (64%)] Loss: -1610.728027\n",
      "Train Epoch: 838 [45568/54000 (84%)] Loss: -1625.176758\n",
      "    epoch          : 838\n",
      "    loss           : -1627.822344185102\n",
      "    val_loss       : -1634.3573269984215\n",
      "    val_log_likelihood: 1732.4335466139387\n",
      "    val_log_marginal: 1673.1357756336965\n",
      "Train Epoch: 839 [512/54000 (1%)] Loss: -1887.747681\n",
      "Train Epoch: 839 [11776/54000 (22%)] Loss: -1591.217041\n",
      "Train Epoch: 839 [23040/54000 (43%)] Loss: -1588.858887\n",
      "Train Epoch: 839 [34304/54000 (64%)] Loss: -1563.152832\n",
      "Train Epoch: 839 [45568/54000 (84%)] Loss: -1589.162476\n",
      "    epoch          : 839\n",
      "    loss           : -1629.954074972927\n",
      "    val_loss       : -1618.0861613740867\n",
      "    val_log_likelihood: 1706.772952844601\n",
      "    val_log_marginal: 1643.2291339262397\n",
      "Train Epoch: 840 [512/54000 (1%)] Loss: -1895.187744\n",
      "Train Epoch: 840 [11776/54000 (22%)] Loss: -1572.253906\n",
      "Train Epoch: 840 [23040/54000 (43%)] Loss: -1610.727051\n",
      "Train Epoch: 840 [34304/54000 (64%)] Loss: -1522.766602\n",
      "Train Epoch: 840 [45568/54000 (84%)] Loss: -1605.636108\n",
      "    epoch          : 840\n",
      "    loss           : -1609.0154279954363\n",
      "    val_loss       : -1618.0613549865764\n",
      "    val_log_likelihood: 1700.4193417388615\n",
      "    val_log_marginal: 1642.4909785689215\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch840.pth ...\n",
      "Train Epoch: 841 [512/54000 (1%)] Loss: -1896.588745\n",
      "Train Epoch: 841 [11776/54000 (22%)] Loss: -1571.982910\n",
      "Train Epoch: 841 [23040/54000 (43%)] Loss: -1599.333130\n",
      "Train Epoch: 841 [34304/54000 (64%)] Loss: -1541.398804\n",
      "Train Epoch: 841 [45568/54000 (84%)] Loss: -1653.297607\n",
      "    epoch          : 841\n",
      "    loss           : -1616.975612527073\n",
      "    val_loss       : -1619.3437326846847\n",
      "    val_log_likelihood: 1704.6345529084158\n",
      "    val_log_marginal: 1648.0650504403518\n",
      "Train Epoch: 842 [512/54000 (1%)] Loss: -1899.239258\n",
      "Train Epoch: 842 [11776/54000 (22%)] Loss: -1585.456543\n",
      "Train Epoch: 842 [23040/54000 (43%)] Loss: -1669.441162\n",
      "Train Epoch: 842 [34304/54000 (64%)] Loss: -1517.358032\n",
      "Train Epoch: 842 [45568/54000 (84%)] Loss: -1603.826538\n",
      "    epoch          : 842\n",
      "    loss           : -1626.0306420656714\n",
      "    val_loss       : -1624.8584771453966\n",
      "    val_log_likelihood: 1716.9880431524598\n",
      "    val_log_marginal: 1660.349716632102\n",
      "Train Epoch: 843 [512/54000 (1%)] Loss: -1891.979004\n",
      "Train Epoch: 843 [11776/54000 (22%)] Loss: -1658.731689\n",
      "Train Epoch: 843 [23040/54000 (43%)] Loss: -1506.323730\n",
      "Train Epoch: 843 [34304/54000 (64%)] Loss: -1604.514893\n",
      "Train Epoch: 843 [45568/54000 (84%)] Loss: -1616.130859\n",
      "    epoch          : 843\n",
      "    loss           : -1625.980057820235\n",
      "    val_loss       : -1630.9994811369475\n",
      "    val_log_likelihood: 1725.6746886602723\n",
      "    val_log_marginal: 1668.3288642728317\n",
      "Train Epoch: 844 [512/54000 (1%)] Loss: -1726.231567\n",
      "Train Epoch: 844 [11776/54000 (22%)] Loss: -1579.265137\n",
      "Train Epoch: 844 [23040/54000 (43%)] Loss: -1494.543335\n",
      "Train Epoch: 844 [34304/54000 (64%)] Loss: -1581.657227\n",
      "Train Epoch: 844 [45568/54000 (84%)] Loss: -1577.658447\n",
      "    epoch          : 844\n",
      "    loss           : -1631.8926542678682\n",
      "    val_loss       : -1629.12854344395\n",
      "    val_log_likelihood: 1725.9279543432858\n",
      "    val_log_marginal: 1668.114353423643\n",
      "Train Epoch: 845 [512/54000 (1%)] Loss: -1913.411621\n",
      "Train Epoch: 845 [11776/54000 (22%)] Loss: -1552.906616\n",
      "Train Epoch: 845 [23040/54000 (43%)] Loss: -1654.010620\n",
      "Train Epoch: 845 [34304/54000 (64%)] Loss: -1896.936890\n",
      "Train Epoch: 845 [45568/54000 (84%)] Loss: -1614.856689\n",
      "    epoch          : 845\n",
      "    loss           : -1632.842448319539\n",
      "    val_loss       : -1634.8933806889368\n",
      "    val_log_likelihood: 1725.9154922938583\n",
      "    val_log_marginal: 1667.886368295473\n",
      "Train Epoch: 846 [512/54000 (1%)] Loss: -1903.219971\n",
      "Train Epoch: 846 [11776/54000 (22%)] Loss: -1541.915283\n",
      "Train Epoch: 846 [23040/54000 (43%)] Loss: -1684.010986\n",
      "Train Epoch: 846 [34304/54000 (64%)] Loss: -1569.492920\n",
      "Train Epoch: 846 [45568/54000 (84%)] Loss: -1551.869141\n",
      "    epoch          : 846\n",
      "    loss           : -1634.4551119662747\n",
      "    val_loss       : -1631.21599489231\n",
      "    val_log_likelihood: 1735.9329797725866\n",
      "    val_log_marginal: 1676.8288838289927\n",
      "Train Epoch: 847 [512/54000 (1%)] Loss: -1902.136230\n",
      "Train Epoch: 847 [11776/54000 (22%)] Loss: -1539.234497\n",
      "Train Epoch: 847 [23040/54000 (43%)] Loss: -1551.074219\n",
      "Train Epoch: 847 [34304/54000 (64%)] Loss: -1688.805908\n",
      "Train Epoch: 847 [45568/54000 (84%)] Loss: -1514.237793\n",
      "    epoch          : 847\n",
      "    loss           : -1624.0530365292389\n",
      "    val_loss       : -1623.262173953503\n",
      "    val_log_likelihood: 1709.0251851601176\n",
      "    val_log_marginal: 1652.7839155105219\n",
      "Train Epoch: 848 [512/54000 (1%)] Loss: -1886.607910\n",
      "Train Epoch: 848 [11776/54000 (22%)] Loss: -1502.869019\n",
      "Train Epoch: 848 [23040/54000 (43%)] Loss: -1624.530273\n",
      "Train Epoch: 848 [34304/54000 (64%)] Loss: -1531.095947\n",
      "Train Epoch: 848 [45568/54000 (84%)] Loss: -1538.301025\n",
      "    epoch          : 848\n",
      "    loss           : -1627.7539968962717\n",
      "    val_loss       : -1613.6616954377364\n",
      "    val_log_likelihood: 1698.222222356513\n",
      "    val_log_marginal: 1637.88615218054\n",
      "Train Epoch: 849 [512/54000 (1%)] Loss: -1882.519043\n",
      "Train Epoch: 849 [11776/54000 (22%)] Loss: -1559.420288\n",
      "Train Epoch: 849 [23040/54000 (43%)] Loss: -1599.030151\n",
      "Train Epoch: 849 [34304/54000 (64%)] Loss: -1610.981567\n",
      "Train Epoch: 849 [45568/54000 (84%)] Loss: -1613.218262\n",
      "    epoch          : 849\n",
      "    loss           : -1621.263536509901\n",
      "    val_loss       : -1624.7063532264644\n",
      "    val_log_likelihood: 1717.5403581373762\n",
      "    val_log_marginal: 1659.0217771160758\n",
      "Train Epoch: 850 [512/54000 (1%)] Loss: -1886.748291\n",
      "Train Epoch: 850 [11776/54000 (22%)] Loss: -1684.265137\n",
      "Train Epoch: 850 [23040/54000 (43%)] Loss: -1604.219238\n",
      "Train Epoch: 850 [34304/54000 (64%)] Loss: -1449.235718\n",
      "Train Epoch: 850 [45568/54000 (84%)] Loss: -1659.690796\n",
      "    epoch          : 850\n",
      "    loss           : -1624.4009586749692\n",
      "    val_loss       : -1622.758253314108\n",
      "    val_log_likelihood: 1718.7213183110302\n",
      "    val_log_marginal: 1660.0216209499886\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch850.pth ...\n",
      "Train Epoch: 851 [512/54000 (1%)] Loss: -1900.627197\n",
      "Train Epoch: 851 [11776/54000 (22%)] Loss: -1523.669922\n",
      "Train Epoch: 851 [23040/54000 (43%)] Loss: -1511.395874\n",
      "Train Epoch: 851 [34304/54000 (64%)] Loss: -1897.690430\n",
      "Train Epoch: 851 [45568/54000 (84%)] Loss: -1622.860840\n",
      "    epoch          : 851\n",
      "    loss           : -1626.0784452834932\n",
      "    val_loss       : -1629.569628398801\n",
      "    val_log_likelihood: 1723.689507512763\n",
      "    val_log_marginal: 1667.9829381745456\n",
      "Train Epoch: 852 [512/54000 (1%)] Loss: -1909.140869\n",
      "Train Epoch: 852 [11776/54000 (22%)] Loss: -1713.142456\n",
      "Train Epoch: 852 [23040/54000 (43%)] Loss: -1668.438843\n",
      "Train Epoch: 852 [34304/54000 (64%)] Loss: -1520.071777\n",
      "Train Epoch: 852 [45568/54000 (84%)] Loss: -1581.889160\n",
      "    epoch          : 852\n",
      "    loss           : -1618.887506768255\n",
      "    val_loss       : -1622.0846266954732\n",
      "    val_log_likelihood: 1732.7275547745205\n",
      "    val_log_marginal: 1675.475175417056\n",
      "Train Epoch: 853 [512/54000 (1%)] Loss: -1875.586914\n",
      "Train Epoch: 853 [11776/54000 (22%)] Loss: -1713.331421\n",
      "Train Epoch: 853 [23040/54000 (43%)] Loss: -1569.115723\n",
      "Train Epoch: 853 [34304/54000 (64%)] Loss: -1900.977783\n",
      "Train Epoch: 853 [45568/54000 (84%)] Loss: -1530.569336\n",
      "    epoch          : 853\n",
      "    loss           : -1618.8164968962717\n",
      "    val_loss       : -1629.0246300191739\n",
      "    val_log_likelihood: 1730.6754319596998\n",
      "    val_log_marginal: 1673.9795181850284\n",
      "Train Epoch: 854 [512/54000 (1%)] Loss: -1898.191650\n",
      "Train Epoch: 854 [11776/54000 (22%)] Loss: -1616.471436\n",
      "Train Epoch: 854 [23040/54000 (43%)] Loss: -1511.866821\n",
      "Train Epoch: 854 [34304/54000 (64%)] Loss: -1550.058838\n",
      "Train Epoch: 854 [45568/54000 (84%)] Loss: -1644.814697\n",
      "    epoch          : 854\n",
      "    loss           : -1625.3135224899443\n",
      "    val_loss       : -1630.6874020986702\n",
      "    val_log_likelihood: 1728.973336701346\n",
      "    val_log_marginal: 1671.4721042204205\n",
      "Train Epoch: 855 [512/54000 (1%)] Loss: -1894.558594\n",
      "Train Epoch: 855 [11776/54000 (22%)] Loss: -1541.331421\n",
      "Train Epoch: 855 [23040/54000 (43%)] Loss: -1698.486084\n",
      "Train Epoch: 855 [34304/54000 (64%)] Loss: -1872.756104\n",
      "Train Epoch: 855 [45568/54000 (84%)] Loss: -1605.683350\n",
      "    epoch          : 855\n",
      "    loss           : -1627.52213944539\n",
      "    val_loss       : -1622.203237437348\n",
      "    val_log_likelihood: 1729.842958355894\n",
      "    val_log_marginal: 1671.8224248701872\n",
      "Train Epoch: 856 [512/54000 (1%)] Loss: -1897.468872\n",
      "Train Epoch: 856 [11776/54000 (22%)] Loss: -1682.328369\n",
      "Train Epoch: 856 [23040/54000 (43%)] Loss: -1508.122559\n",
      "Train Epoch: 856 [34304/54000 (64%)] Loss: -1884.598389\n",
      "Train Epoch: 856 [45568/54000 (84%)] Loss: -1621.666504\n",
      "    epoch          : 856\n",
      "    loss           : -1623.9623781714108\n",
      "    val_loss       : -1625.527581466242\n",
      "    val_log_likelihood: 1731.969588780167\n",
      "    val_log_marginal: 1671.5981081667308\n",
      "Train Epoch: 857 [512/54000 (1%)] Loss: -1880.366943\n",
      "Train Epoch: 857 [11776/54000 (22%)] Loss: -1541.118652\n",
      "Train Epoch: 857 [23040/54000 (43%)] Loss: -1647.517822\n",
      "Train Epoch: 857 [34304/54000 (64%)] Loss: -1646.173218\n",
      "Train Epoch: 857 [45568/54000 (84%)] Loss: -1531.822266\n",
      "    epoch          : 857\n",
      "    loss           : -1626.5024172339108\n",
      "    val_loss       : -1622.8894792108676\n",
      "    val_log_likelihood: 1730.6910690458694\n",
      "    val_log_marginal: 1671.2754636685559\n",
      "Train Epoch: 858 [512/54000 (1%)] Loss: -1896.802490\n",
      "Train Epoch: 858 [11776/54000 (22%)] Loss: -1703.676270\n",
      "Train Epoch: 858 [23040/54000 (43%)] Loss: -1512.507812\n",
      "Train Epoch: 858 [34304/54000 (64%)] Loss: -1581.108643\n",
      "Train Epoch: 858 [45568/54000 (84%)] Loss: -1675.310913\n",
      "    epoch          : 858\n",
      "    loss           : -1624.2645070293163\n",
      "    val_loss       : -1625.4188563298378\n",
      "    val_log_likelihood: 1735.225533966971\n",
      "    val_log_marginal: 1675.3960811778497\n",
      "Train Epoch: 859 [512/54000 (1%)] Loss: -1889.131592\n",
      "Train Epoch: 859 [11776/54000 (22%)] Loss: -1574.782715\n",
      "Train Epoch: 859 [23040/54000 (43%)] Loss: -1664.497681\n",
      "Train Epoch: 859 [34304/54000 (64%)] Loss: -1544.055298\n",
      "Train Epoch: 859 [45568/54000 (84%)] Loss: -1524.601440\n",
      "    epoch          : 859\n",
      "    loss           : -1626.055853815362\n",
      "    val_loss       : -1629.2427591091428\n",
      "    val_log_likelihood: 1733.6451488532643\n",
      "    val_log_marginal: 1675.5928700742406\n",
      "Train Epoch: 860 [512/54000 (1%)] Loss: -1892.103027\n",
      "Train Epoch: 860 [11776/54000 (22%)] Loss: -1570.121582\n",
      "Train Epoch: 860 [23040/54000 (43%)] Loss: -1578.804565\n",
      "Train Epoch: 860 [34304/54000 (64%)] Loss: -1896.020264\n",
      "Train Epoch: 860 [45568/54000 (84%)] Loss: -1614.403809\n",
      "    epoch          : 860\n",
      "    loss           : -1627.021318794477\n",
      "    val_loss       : -1623.74004280794\n",
      "    val_log_likelihood: 1708.4791320196473\n",
      "    val_log_marginal: 1647.4732283047397\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch860.pth ...\n",
      "Train Epoch: 861 [512/54000 (1%)] Loss: -1913.904785\n",
      "Train Epoch: 861 [11776/54000 (22%)] Loss: -1689.184570\n",
      "Train Epoch: 861 [23040/54000 (43%)] Loss: -1581.410034\n",
      "Train Epoch: 861 [34304/54000 (64%)] Loss: -1498.156738\n",
      "Train Epoch: 861 [45568/54000 (84%)] Loss: -1574.891602\n",
      "    epoch          : 861\n",
      "    loss           : -1625.2682078144337\n",
      "    val_loss       : -1627.8210856156093\n",
      "    val_log_likelihood: 1727.4303075204982\n",
      "    val_log_marginal: 1668.5030009909394\n",
      "Train Epoch: 862 [512/54000 (1%)] Loss: -1902.759644\n",
      "Train Epoch: 862 [11776/54000 (22%)] Loss: -1673.721680\n",
      "Train Epoch: 862 [23040/54000 (43%)] Loss: -1489.827393\n",
      "Train Epoch: 862 [34304/54000 (64%)] Loss: -1499.535645\n",
      "Train Epoch: 862 [45568/54000 (84%)] Loss: -1613.845093\n",
      "    epoch          : 862\n",
      "    loss           : -1626.3116455078125\n",
      "    val_loss       : -1628.111018286096\n",
      "    val_log_likelihood: 1733.90099009901\n",
      "    val_log_marginal: 1672.7092416336777\n",
      "Train Epoch: 863 [512/54000 (1%)] Loss: -1888.082764\n",
      "Train Epoch: 863 [11776/54000 (22%)] Loss: -1515.316162\n",
      "Train Epoch: 863 [23040/54000 (43%)] Loss: -1682.033447\n",
      "Train Epoch: 863 [34304/54000 (64%)] Loss: -1486.652832\n",
      "Train Epoch: 863 [45568/54000 (84%)] Loss: -1598.754517\n",
      "    epoch          : 863\n",
      "    loss           : -1611.713080377862\n",
      "    val_loss       : -1600.6701541850896\n",
      "    val_log_likelihood: 1688.7849701229889\n",
      "    val_log_marginal: 1628.8322500678496\n",
      "Train Epoch: 864 [512/54000 (1%)] Loss: -1882.748291\n",
      "Train Epoch: 864 [11776/54000 (22%)] Loss: -1567.877197\n",
      "Train Epoch: 864 [23040/54000 (43%)] Loss: -1484.041504\n",
      "Train Epoch: 864 [34304/54000 (64%)] Loss: -1881.860352\n",
      "Train Epoch: 864 [45568/54000 (84%)] Loss: -1586.174805\n",
      "    epoch          : 864\n",
      "    loss           : -1602.2374908145111\n",
      "    val_loss       : -1614.5337907302257\n",
      "    val_log_likelihood: 1703.7032410272277\n",
      "    val_log_marginal: 1642.093008551433\n",
      "Train Epoch: 865 [512/54000 (1%)] Loss: -1887.743164\n",
      "Train Epoch: 865 [11776/54000 (22%)] Loss: -1643.778564\n",
      "Train Epoch: 865 [23040/54000 (43%)] Loss: -1595.033203\n",
      "Train Epoch: 865 [34304/54000 (64%)] Loss: -1605.384766\n",
      "Train Epoch: 865 [45568/54000 (84%)] Loss: -1539.081909\n",
      "    epoch          : 865\n",
      "    loss           : -1619.837891833617\n",
      "    val_loss       : -1619.9545409532338\n",
      "    val_log_likelihood: 1709.06803788289\n",
      "    val_log_marginal: 1649.4465092780013\n",
      "Train Epoch: 866 [512/54000 (1%)] Loss: -1893.895508\n",
      "Train Epoch: 866 [11776/54000 (22%)] Loss: -1690.514526\n",
      "Train Epoch: 866 [23040/54000 (43%)] Loss: -1597.449463\n",
      "Train Epoch: 866 [34304/54000 (64%)] Loss: -1596.706177\n",
      "Train Epoch: 866 [45568/54000 (84%)] Loss: -1516.312012\n",
      "    epoch          : 866\n",
      "    loss           : -1622.5348685991646\n",
      "    val_loss       : -1627.7383364942662\n",
      "    val_log_likelihood: 1720.2058975672958\n",
      "    val_log_marginal: 1660.378609316276\n",
      "Train Epoch: 867 [512/54000 (1%)] Loss: -1666.348022\n",
      "Train Epoch: 867 [11776/54000 (22%)] Loss: -1671.084717\n",
      "Train Epoch: 867 [23040/54000 (43%)] Loss: -1540.848267\n",
      "Train Epoch: 867 [34304/54000 (64%)] Loss: -1581.055176\n",
      "Train Epoch: 867 [45568/54000 (84%)] Loss: -1540.398438\n",
      "    epoch          : 867\n",
      "    loss           : -1626.992272103187\n",
      "    val_loss       : -1625.647775495812\n",
      "    val_log_likelihood: 1717.1889624265161\n",
      "    val_log_marginal: 1656.457743016007\n",
      "Train Epoch: 868 [512/54000 (1%)] Loss: -1896.616699\n",
      "Train Epoch: 868 [11776/54000 (22%)] Loss: -1703.778687\n",
      "Train Epoch: 868 [23040/54000 (43%)] Loss: -1593.723022\n",
      "Train Epoch: 868 [34304/54000 (64%)] Loss: -1565.939209\n",
      "Train Epoch: 868 [45568/54000 (84%)] Loss: -1515.498657\n",
      "    epoch          : 868\n",
      "    loss           : -1630.246397112856\n",
      "    val_loss       : -1631.124664236948\n",
      "    val_log_likelihood: 1727.712806021813\n",
      "    val_log_marginal: 1666.5410649761889\n",
      "Train Epoch: 869 [512/54000 (1%)] Loss: -1898.292114\n",
      "Train Epoch: 869 [11776/54000 (22%)] Loss: -1561.458862\n",
      "Train Epoch: 869 [23040/54000 (43%)] Loss: -1578.508301\n",
      "Train Epoch: 869 [34304/54000 (64%)] Loss: -1568.767944\n",
      "Train Epoch: 869 [45568/54000 (84%)] Loss: -1539.975952\n",
      "    epoch          : 869\n",
      "    loss           : -1628.1360479675898\n",
      "    val_loss       : -1615.9812915960217\n",
      "    val_log_likelihood: 1712.6983896387685\n",
      "    val_log_marginal: 1654.353535248151\n",
      "Train Epoch: 870 [512/54000 (1%)] Loss: -1875.306152\n",
      "Train Epoch: 870 [11776/54000 (22%)] Loss: -1555.984131\n",
      "Train Epoch: 870 [23040/54000 (43%)] Loss: -1618.329224\n",
      "Train Epoch: 870 [34304/54000 (64%)] Loss: -1678.963867\n",
      "Train Epoch: 870 [45568/54000 (84%)] Loss: -1556.566650\n",
      "    epoch          : 870\n",
      "    loss           : -1622.0157410272277\n",
      "    val_loss       : -1633.7336697054216\n",
      "    val_log_likelihood: 1721.6244984239636\n",
      "    val_log_marginal: 1663.8824694235411\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch870.pth ...\n",
      "Train Epoch: 871 [512/54000 (1%)] Loss: -1904.173218\n",
      "Train Epoch: 871 [11776/54000 (22%)] Loss: -1561.559692\n",
      "Train Epoch: 871 [23040/54000 (43%)] Loss: -1615.877319\n",
      "Train Epoch: 871 [34304/54000 (64%)] Loss: -1509.648682\n",
      "Train Epoch: 871 [45568/54000 (84%)] Loss: -1620.781006\n",
      "    epoch          : 871\n",
      "    loss           : -1630.0085944751702\n",
      "    val_loss       : -1634.6633523344847\n",
      "    val_log_likelihood: 1726.2361250773515\n",
      "    val_log_marginal: 1665.6672696674998\n",
      "Train Epoch: 872 [512/54000 (1%)] Loss: -1880.359741\n",
      "Train Epoch: 872 [11776/54000 (22%)] Loss: -1594.792969\n",
      "Train Epoch: 872 [23040/54000 (43%)] Loss: -1540.946533\n",
      "Train Epoch: 872 [34304/54000 (64%)] Loss: -1601.384033\n",
      "Train Epoch: 872 [45568/54000 (84%)] Loss: -1634.400146\n",
      "    epoch          : 872\n",
      "    loss           : -1632.2744382348392\n",
      "    val_loss       : -1636.8659496209355\n",
      "    val_log_likelihood: 1726.2735027653157\n",
      "    val_log_marginal: 1668.7491456024766\n",
      "Train Epoch: 873 [512/54000 (1%)] Loss: -1881.446533\n",
      "Train Epoch: 873 [11776/54000 (22%)] Loss: -1546.419556\n",
      "Train Epoch: 873 [23040/54000 (43%)] Loss: -1683.073364\n",
      "Train Epoch: 873 [34304/54000 (64%)] Loss: -1646.192749\n",
      "Train Epoch: 873 [45568/54000 (84%)] Loss: -1547.089600\n",
      "    epoch          : 873\n",
      "    loss           : -1629.030003915919\n",
      "    val_loss       : -1629.55994005765\n",
      "    val_log_likelihood: 1723.0569246499845\n",
      "    val_log_marginal: 1660.600688225765\n",
      "Train Epoch: 874 [512/54000 (1%)] Loss: -1680.455078\n",
      "Train Epoch: 874 [11776/54000 (22%)] Loss: -1553.304932\n",
      "Train Epoch: 874 [23040/54000 (43%)] Loss: -1562.257080\n",
      "Train Epoch: 874 [34304/54000 (64%)] Loss: -1587.558105\n",
      "Train Epoch: 874 [45568/54000 (84%)] Loss: -1459.210693\n",
      "    epoch          : 874\n",
      "    loss           : -1626.33918867961\n",
      "    val_loss       : -1608.664315868252\n",
      "    val_log_likelihood: 1714.1825942237779\n",
      "    val_log_marginal: 1652.6759110191413\n",
      "Train Epoch: 875 [512/54000 (1%)] Loss: -1598.739014\n",
      "Train Epoch: 875 [11776/54000 (22%)] Loss: -1715.901611\n",
      "Train Epoch: 875 [23040/54000 (43%)] Loss: -1483.630737\n",
      "Train Epoch: 875 [34304/54000 (64%)] Loss: -1639.395752\n",
      "Train Epoch: 875 [45568/54000 (84%)] Loss: -1550.376831\n",
      "    epoch          : 875\n",
      "    loss           : -1615.098817730894\n",
      "    val_loss       : -1619.284569312302\n",
      "    val_log_likelihood: 1721.7607337271813\n",
      "    val_log_marginal: 1661.2486598752255\n",
      "Train Epoch: 876 [512/54000 (1%)] Loss: -1549.435791\n",
      "Train Epoch: 876 [11776/54000 (22%)] Loss: -1594.276123\n",
      "Train Epoch: 876 [23040/54000 (43%)] Loss: -1641.306274\n",
      "Train Epoch: 876 [34304/54000 (64%)] Loss: -1651.682617\n",
      "Train Epoch: 876 [45568/54000 (84%)] Loss: -1606.949951\n",
      "    epoch          : 876\n",
      "    loss           : -1622.2585086633662\n",
      "    val_loss       : -1618.4798927743927\n",
      "    val_log_likelihood: 1721.3758327370822\n",
      "    val_log_marginal: 1661.7117278024664\n",
      "Train Epoch: 877 [512/54000 (1%)] Loss: -1858.596191\n",
      "Train Epoch: 877 [11776/54000 (22%)] Loss: -1703.944092\n",
      "Train Epoch: 877 [23040/54000 (43%)] Loss: -1551.168335\n",
      "Train Epoch: 877 [34304/54000 (64%)] Loss: -1591.470947\n",
      "Train Epoch: 877 [45568/54000 (84%)] Loss: -1683.889160\n",
      "    epoch          : 877\n",
      "    loss           : -1627.7046079730044\n",
      "    val_loss       : -1630.7338977831664\n",
      "    val_log_likelihood: 1729.9314943823483\n",
      "    val_log_marginal: 1671.0122742172002\n",
      "Train Epoch: 878 [512/54000 (1%)] Loss: -1886.031128\n",
      "Train Epoch: 878 [11776/54000 (22%)] Loss: -1712.454834\n",
      "Train Epoch: 878 [23040/54000 (43%)] Loss: -1561.846191\n",
      "Train Epoch: 878 [34304/54000 (64%)] Loss: -1653.134644\n",
      "Train Epoch: 878 [45568/54000 (84%)] Loss: -1547.287109\n",
      "    epoch          : 878\n",
      "    loss           : -1622.9975307955601\n",
      "    val_loss       : -1616.21492132669\n",
      "    val_log_likelihood: 1713.8266251063583\n",
      "    val_log_marginal: 1650.2662838715692\n",
      "Train Epoch: 879 [512/54000 (1%)] Loss: -1886.286133\n",
      "Train Epoch: 879 [11776/54000 (22%)] Loss: -1535.727295\n",
      "Train Epoch: 879 [23040/54000 (43%)] Loss: -1598.837646\n",
      "Train Epoch: 879 [34304/54000 (64%)] Loss: -1481.987061\n",
      "Train Epoch: 879 [45568/54000 (84%)] Loss: -1578.175049\n",
      "    epoch          : 879\n",
      "    loss           : -1623.5162293084777\n",
      "    val_loss       : -1624.783335023661\n",
      "    val_log_likelihood: 1721.2916465230508\n",
      "    val_log_marginal: 1661.9936390148273\n",
      "Train Epoch: 880 [512/54000 (1%)] Loss: -1887.328613\n",
      "Train Epoch: 880 [11776/54000 (22%)] Loss: -1536.171753\n",
      "Train Epoch: 880 [23040/54000 (43%)] Loss: -1560.704102\n",
      "Train Epoch: 880 [34304/54000 (64%)] Loss: -1611.999878\n",
      "Train Epoch: 880 [45568/54000 (84%)] Loss: -1539.653442\n",
      "    epoch          : 880\n",
      "    loss           : -1629.1656204072556\n",
      "    val_loss       : -1630.0782317139558\n",
      "    val_log_likelihood: 1722.3617728960396\n",
      "    val_log_marginal: 1663.345204196778\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch880.pth ...\n",
      "Train Epoch: 881 [512/54000 (1%)] Loss: -1894.864380\n",
      "Train Epoch: 881 [11776/54000 (22%)] Loss: -1726.410767\n",
      "Train Epoch: 881 [23040/54000 (43%)] Loss: -1684.311523\n",
      "Train Epoch: 881 [34304/54000 (64%)] Loss: -1570.901611\n",
      "Train Epoch: 881 [45568/54000 (84%)] Loss: -1633.122925\n",
      "    epoch          : 881\n",
      "    loss           : -1631.250078560102\n",
      "    val_loss       : -1632.9739750216227\n",
      "    val_log_likelihood: 1725.2344982789295\n",
      "    val_log_marginal: 1667.2181985813365\n",
      "Train Epoch: 882 [512/54000 (1%)] Loss: -1913.292725\n",
      "Train Epoch: 882 [11776/54000 (22%)] Loss: -1544.285889\n",
      "Train Epoch: 882 [23040/54000 (43%)] Loss: -1518.941650\n",
      "Train Epoch: 882 [34304/54000 (64%)] Loss: -1900.622314\n",
      "Train Epoch: 882 [45568/54000 (84%)] Loss: -1538.642700\n",
      "    epoch          : 882\n",
      "    loss           : -1631.4643663463025\n",
      "    val_loss       : -1628.6812776889524\n",
      "    val_log_likelihood: 1724.0391374342512\n",
      "    val_log_marginal: 1667.0231820539857\n",
      "Train Epoch: 883 [512/54000 (1%)] Loss: -1896.736450\n",
      "Train Epoch: 883 [11776/54000 (22%)] Loss: -1701.094971\n",
      "Train Epoch: 883 [23040/54000 (43%)] Loss: -1512.742920\n",
      "Train Epoch: 883 [34304/54000 (64%)] Loss: -1908.116211\n",
      "Train Epoch: 883 [45568/54000 (84%)] Loss: -1536.211426\n",
      "    epoch          : 883\n",
      "    loss           : -1629.3400008702042\n",
      "    val_loss       : -1630.6074845489059\n",
      "    val_log_likelihood: 1732.0131944713025\n",
      "    val_log_marginal: 1675.426158722693\n",
      "Train Epoch: 884 [512/54000 (1%)] Loss: -1891.163208\n",
      "Train Epoch: 884 [11776/54000 (22%)] Loss: -1583.223022\n",
      "Train Epoch: 884 [23040/54000 (43%)] Loss: -1586.299561\n",
      "Train Epoch: 884 [34304/54000 (64%)] Loss: -1674.709351\n",
      "Train Epoch: 884 [45568/54000 (84%)] Loss: -1556.127319\n",
      "    epoch          : 884\n",
      "    loss           : -1626.3324192160428\n",
      "    val_loss       : -1628.8814402490873\n",
      "    val_log_likelihood: 1719.348555461015\n",
      "    val_log_marginal: 1659.9586621437147\n",
      "Train Epoch: 885 [512/54000 (1%)] Loss: -1914.179077\n",
      "Train Epoch: 885 [11776/54000 (22%)] Loss: -1592.331299\n",
      "Train Epoch: 885 [23040/54000 (43%)] Loss: -1561.412598\n",
      "Train Epoch: 885 [34304/54000 (64%)] Loss: -1528.029907\n",
      "Train Epoch: 885 [45568/54000 (84%)] Loss: -1608.689941\n",
      "    epoch          : 885\n",
      "    loss           : -1629.9101369121288\n",
      "    val_loss       : -1635.5987038095805\n",
      "    val_log_likelihood: 1728.69200572401\n",
      "    val_log_marginal: 1669.8796325522005\n",
      "Train Epoch: 886 [512/54000 (1%)] Loss: -1892.869385\n",
      "Train Epoch: 886 [11776/54000 (22%)] Loss: -1540.786621\n",
      "Train Epoch: 886 [23040/54000 (43%)] Loss: -1657.326782\n",
      "Train Epoch: 886 [34304/54000 (64%)] Loss: -1676.236084\n",
      "Train Epoch: 886 [45568/54000 (84%)] Loss: -1604.328125\n",
      "    epoch          : 886\n",
      "    loss           : -1628.9024585686107\n",
      "    val_loss       : -1626.586655028441\n",
      "    val_log_likelihood: 1726.8081393100247\n",
      "    val_log_marginal: 1666.0950930369297\n",
      "Train Epoch: 887 [512/54000 (1%)] Loss: -1901.657715\n",
      "Train Epoch: 887 [11776/54000 (22%)] Loss: -1556.531982\n",
      "Train Epoch: 887 [23040/54000 (43%)] Loss: -1630.537354\n",
      "Train Epoch: 887 [34304/54000 (64%)] Loss: -1673.977783\n",
      "Train Epoch: 887 [45568/54000 (84%)] Loss: -1529.121460\n",
      "    epoch          : 887\n",
      "    loss           : -1631.503636728419\n",
      "    val_loss       : -1636.0063802606637\n",
      "    val_log_likelihood: 1723.502466787206\n",
      "    val_log_marginal: 1666.3476682061369\n",
      "Train Epoch: 888 [512/54000 (1%)] Loss: -1901.390137\n",
      "Train Epoch: 888 [11776/54000 (22%)] Loss: -1566.938965\n",
      "Train Epoch: 888 [23040/54000 (43%)] Loss: -1499.850586\n",
      "Train Epoch: 888 [34304/54000 (64%)] Loss: -1547.517822\n",
      "Train Epoch: 888 [45568/54000 (84%)] Loss: -1541.192139\n",
      "    epoch          : 888\n",
      "    loss           : -1634.3934096534654\n",
      "    val_loss       : -1638.2886140019366\n",
      "    val_log_likelihood: 1726.325087745591\n",
      "    val_log_marginal: 1668.847116729429\n",
      "Train Epoch: 889 [512/54000 (1%)] Loss: -1906.907104\n",
      "Train Epoch: 889 [11776/54000 (22%)] Loss: -1718.335449\n",
      "Train Epoch: 889 [23040/54000 (43%)] Loss: -1662.901367\n",
      "Train Epoch: 889 [34304/54000 (64%)] Loss: -1528.176514\n",
      "Train Epoch: 889 [45568/54000 (84%)] Loss: -1626.511353\n",
      "    epoch          : 889\n",
      "    loss           : -1636.7365831431775\n",
      "    val_loss       : -1632.4467159696753\n",
      "    val_log_likelihood: 1727.2089239441523\n",
      "    val_log_marginal: 1668.077055516865\n",
      "Train Epoch: 890 [512/54000 (1%)] Loss: -1910.657837\n",
      "Train Epoch: 890 [11776/54000 (22%)] Loss: -1733.788696\n",
      "Train Epoch: 890 [23040/54000 (43%)] Loss: -1581.549561\n",
      "Train Epoch: 890 [34304/54000 (64%)] Loss: -1650.524536\n",
      "Train Epoch: 890 [45568/54000 (84%)] Loss: -1662.414062\n",
      "    epoch          : 890\n",
      "    loss           : -1637.262266253481\n",
      "    val_loss       : -1627.0902359882498\n",
      "    val_log_likelihood: 1725.3076256478187\n",
      "    val_log_marginal: 1667.8825570372403\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch890.pth ...\n",
      "Train Epoch: 891 [512/54000 (1%)] Loss: -1631.022217\n",
      "Train Epoch: 891 [11776/54000 (22%)] Loss: -1558.980957\n",
      "Train Epoch: 891 [23040/54000 (43%)] Loss: -1672.442627\n",
      "Train Epoch: 891 [34304/54000 (64%)] Loss: -1578.648926\n",
      "Train Epoch: 891 [45568/54000 (84%)] Loss: -1641.884033\n",
      "    epoch          : 891\n",
      "    loss           : -1625.0882918858292\n",
      "    val_loss       : -1626.4183805514458\n",
      "    val_log_likelihood: 1719.5931928275836\n",
      "    val_log_marginal: 1660.7985631798274\n",
      "Train Epoch: 892 [512/54000 (1%)] Loss: -1548.984497\n",
      "Train Epoch: 892 [11776/54000 (22%)] Loss: -1553.215820\n",
      "Train Epoch: 892 [23040/54000 (43%)] Loss: -1562.951660\n",
      "Train Epoch: 892 [34304/54000 (64%)] Loss: -1661.201904\n",
      "Train Epoch: 892 [45568/54000 (84%)] Loss: -1622.308105\n",
      "    epoch          : 892\n",
      "    loss           : -1624.694983756188\n",
      "    val_loss       : -1627.2422957499557\n",
      "    val_log_likelihood: 1723.5408452100094\n",
      "    val_log_marginal: 1663.2084582019597\n",
      "Train Epoch: 893 [512/54000 (1%)] Loss: -1911.624268\n",
      "Train Epoch: 893 [11776/54000 (22%)] Loss: -1552.492065\n",
      "Train Epoch: 893 [23040/54000 (43%)] Loss: -1590.972656\n",
      "Train Epoch: 893 [34304/54000 (64%)] Loss: -1635.674072\n",
      "Train Epoch: 893 [45568/54000 (84%)] Loss: -1665.812012\n",
      "    epoch          : 893\n",
      "    loss           : -1623.791044631807\n",
      "    val_loss       : -1623.9504996082092\n",
      "    val_log_likelihood: 1722.7565579556003\n",
      "    val_log_marginal: 1664.3673248712976\n",
      "Train Epoch: 894 [512/54000 (1%)] Loss: -1901.651489\n",
      "Train Epoch: 894 [11776/54000 (22%)] Loss: -1478.442505\n",
      "Train Epoch: 894 [23040/54000 (43%)] Loss: -1614.374023\n",
      "Train Epoch: 894 [34304/54000 (64%)] Loss: -1900.422852\n",
      "Train Epoch: 894 [45568/54000 (84%)] Loss: -1635.939697\n",
      "    epoch          : 894\n",
      "    loss           : -1620.9302688447556\n",
      "    val_loss       : -1626.7183160512607\n",
      "    val_log_likelihood: 1730.863742941677\n",
      "    val_log_marginal: 1671.7462231458376\n",
      "Train Epoch: 895 [512/54000 (1%)] Loss: -1880.125977\n",
      "Train Epoch: 895 [11776/54000 (22%)] Loss: -1566.476318\n",
      "Train Epoch: 895 [23040/54000 (43%)] Loss: -1589.603394\n",
      "Train Epoch: 895 [34304/54000 (64%)] Loss: -1551.238037\n",
      "Train Epoch: 895 [45568/54000 (84%)] Loss: -1602.672607\n",
      "    epoch          : 895\n",
      "    loss           : -1627.9199375870205\n",
      "    val_loss       : -1622.1553358713736\n",
      "    val_log_likelihood: 1733.8349440168627\n",
      "    val_log_marginal: 1674.076878161105\n",
      "Train Epoch: 896 [512/54000 (1%)] Loss: -1566.035278\n",
      "Train Epoch: 896 [11776/54000 (22%)] Loss: -1635.569336\n",
      "Train Epoch: 896 [23040/54000 (43%)] Loss: -1577.450439\n",
      "Train Epoch: 896 [34304/54000 (64%)] Loss: -1872.870361\n",
      "Train Epoch: 896 [45568/54000 (84%)] Loss: -1571.585449\n",
      "    epoch          : 896\n",
      "    loss           : -1625.807207466352\n",
      "    val_loss       : -1628.928593581616\n",
      "    val_log_likelihood: 1729.95079478651\n",
      "    val_log_marginal: 1670.6799172925587\n",
      "Train Epoch: 897 [512/54000 (1%)] Loss: -1897.181396\n",
      "Train Epoch: 897 [11776/54000 (22%)] Loss: -1693.954590\n",
      "Train Epoch: 897 [23040/54000 (43%)] Loss: -1679.317871\n",
      "Train Epoch: 897 [34304/54000 (64%)] Loss: -1883.853760\n",
      "Train Epoch: 897 [45568/54000 (84%)] Loss: -1543.654175\n",
      "    epoch          : 897\n",
      "    loss           : -1628.7058588915531\n",
      "    val_loss       : -1630.6277153947676\n",
      "    val_log_likelihood: 1723.7822023901608\n",
      "    val_log_marginal: 1667.1720836218815\n",
      "Train Epoch: 898 [512/54000 (1%)] Loss: -1885.339600\n",
      "Train Epoch: 898 [11776/54000 (22%)] Loss: -1608.038818\n",
      "Train Epoch: 898 [23040/54000 (43%)] Loss: -1587.038940\n",
      "Train Epoch: 898 [34304/54000 (64%)] Loss: -1586.067139\n",
      "Train Epoch: 898 [45568/54000 (84%)] Loss: -1575.615356\n",
      "    epoch          : 898\n",
      "    loss           : -1631.6386912128712\n",
      "    val_loss       : -1633.6008420553944\n",
      "    val_log_likelihood: 1732.0585296933014\n",
      "    val_log_marginal: 1673.4308362730717\n",
      "Train Epoch: 899 [512/54000 (1%)] Loss: -1885.309326\n",
      "Train Epoch: 899 [11776/54000 (22%)] Loss: -1556.617676\n",
      "Train Epoch: 899 [23040/54000 (43%)] Loss: -1706.101196\n",
      "Train Epoch: 899 [34304/54000 (64%)] Loss: -1877.394775\n",
      "Train Epoch: 899 [45568/54000 (84%)] Loss: -1586.967041\n",
      "    epoch          : 899\n",
      "    loss           : -1634.4685397006497\n",
      "    val_loss       : -1637.6916349832486\n",
      "    val_log_likelihood: 1738.7075618328433\n",
      "    val_log_marginal: 1681.06053798832\n",
      "Train Epoch: 900 [512/54000 (1%)] Loss: -1897.947144\n",
      "Train Epoch: 900 [11776/54000 (22%)] Loss: -1688.612427\n",
      "Train Epoch: 900 [23040/54000 (43%)] Loss: -1467.343506\n",
      "Train Epoch: 900 [34304/54000 (64%)] Loss: -1605.541260\n",
      "Train Epoch: 900 [45568/54000 (84%)] Loss: -1641.460449\n",
      "    epoch          : 900\n",
      "    loss           : -1630.2969330136139\n",
      "    val_loss       : -1631.1013108035734\n",
      "    val_log_likelihood: 1724.8248895324102\n",
      "    val_log_marginal: 1667.7676113009784\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch900.pth ...\n",
      "Train Epoch: 901 [512/54000 (1%)] Loss: -1888.334961\n",
      "Train Epoch: 901 [11776/54000 (22%)] Loss: -1560.484375\n",
      "Train Epoch: 901 [23040/54000 (43%)] Loss: -1548.356567\n",
      "Train Epoch: 901 [34304/54000 (64%)] Loss: -1579.314575\n",
      "Train Epoch: 901 [45568/54000 (84%)] Loss: -1650.153564\n",
      "    epoch          : 901\n",
      "    loss           : -1630.8831049853031\n",
      "    val_loss       : -1625.7866986261238\n",
      "    val_log_likelihood: 1720.5590119314666\n",
      "    val_log_marginal: 1658.1654019067894\n",
      "Train Epoch: 902 [512/54000 (1%)] Loss: -1906.093750\n",
      "Train Epoch: 902 [11776/54000 (22%)] Loss: -1663.217529\n",
      "Train Epoch: 902 [23040/54000 (43%)] Loss: -1499.311279\n",
      "Train Epoch: 902 [34304/54000 (64%)] Loss: -1599.701660\n",
      "Train Epoch: 902 [45568/54000 (84%)] Loss: -1660.833496\n",
      "    epoch          : 902\n",
      "    loss           : -1629.1377569519648\n",
      "    val_loss       : -1633.4046370661956\n",
      "    val_log_likelihood: 1730.676190971148\n",
      "    val_log_marginal: 1669.1971285736909\n",
      "Train Epoch: 903 [512/54000 (1%)] Loss: -1904.590576\n",
      "Train Epoch: 903 [11776/54000 (22%)] Loss: -1539.298462\n",
      "Train Epoch: 903 [23040/54000 (43%)] Loss: -1674.121582\n",
      "Train Epoch: 903 [34304/54000 (64%)] Loss: -1654.812866\n",
      "Train Epoch: 903 [45568/54000 (84%)] Loss: -1645.487793\n",
      "    epoch          : 903\n",
      "    loss           : -1637.1244984239636\n",
      "    val_loss       : -1637.1887531682278\n",
      "    val_log_likelihood: 1729.1379442875927\n",
      "    val_log_marginal: 1671.4042763609882\n",
      "Train Epoch: 904 [512/54000 (1%)] Loss: -1880.448975\n",
      "Train Epoch: 904 [11776/54000 (22%)] Loss: -1525.729248\n",
      "Train Epoch: 904 [23040/54000 (43%)] Loss: -1688.921631\n",
      "Train Epoch: 904 [34304/54000 (64%)] Loss: -1555.033569\n",
      "Train Epoch: 904 [45568/54000 (84%)] Loss: -1611.454834\n",
      "    epoch          : 904\n",
      "    loss           : -1631.383135200727\n",
      "    val_loss       : -1631.619511136564\n",
      "    val_log_likelihood: 1731.876029741646\n",
      "    val_log_marginal: 1671.2883306886745\n",
      "Train Epoch: 905 [512/54000 (1%)] Loss: -1890.838257\n",
      "Train Epoch: 905 [11776/54000 (22%)] Loss: -1536.617432\n",
      "Train Epoch: 905 [23040/54000 (43%)] Loss: -1677.191650\n",
      "Train Epoch: 905 [34304/54000 (64%)] Loss: -1906.546387\n",
      "Train Epoch: 905 [45568/54000 (84%)] Loss: -1654.044922\n",
      "    epoch          : 905\n",
      "    loss           : -1626.1443439143718\n",
      "    val_loss       : -1631.84619047904\n",
      "    val_log_likelihood: 1726.693822275294\n",
      "    val_log_marginal: 1668.85207862431\n",
      "Train Epoch: 906 [512/54000 (1%)] Loss: -1733.556396\n",
      "Train Epoch: 906 [11776/54000 (22%)] Loss: -1586.023560\n",
      "Train Epoch: 906 [23040/54000 (43%)] Loss: -1691.388428\n",
      "Train Epoch: 906 [34304/54000 (64%)] Loss: -1608.183838\n",
      "Train Epoch: 906 [45568/54000 (84%)] Loss: -1538.289307\n",
      "    epoch          : 906\n",
      "    loss           : -1633.8704290106746\n",
      "    val_loss       : -1634.4345283249836\n",
      "    val_log_likelihood: 1731.6955941077506\n",
      "    val_log_marginal: 1675.42650747633\n",
      "Train Epoch: 907 [512/54000 (1%)] Loss: -1909.684570\n",
      "Train Epoch: 907 [11776/54000 (22%)] Loss: -1466.957520\n",
      "Train Epoch: 907 [23040/54000 (43%)] Loss: -1539.492188\n",
      "Train Epoch: 907 [34304/54000 (64%)] Loss: -1911.870361\n",
      "Train Epoch: 907 [45568/54000 (84%)] Loss: -1624.281128\n",
      "    epoch          : 907\n",
      "    loss           : -1634.3148495513615\n",
      "    val_loss       : -1639.5107302917904\n",
      "    val_log_likelihood: 1731.2115273050742\n",
      "    val_log_marginal: 1674.9937813528713\n",
      "Train Epoch: 908 [512/54000 (1%)] Loss: -1893.871582\n",
      "Train Epoch: 908 [11776/54000 (22%)] Loss: -1724.076294\n",
      "Train Epoch: 908 [23040/54000 (43%)] Loss: -1550.354980\n",
      "Train Epoch: 908 [34304/54000 (64%)] Loss: -1601.730225\n",
      "Train Epoch: 908 [45568/54000 (84%)] Loss: -1532.368774\n",
      "    epoch          : 908\n",
      "    loss           : -1630.4111436900525\n",
      "    val_loss       : -1633.5386367388126\n",
      "    val_log_likelihood: 1724.3962088103342\n",
      "    val_log_marginal: 1665.5266683961972\n",
      "Train Epoch: 909 [512/54000 (1%)] Loss: -1892.927856\n",
      "Train Epoch: 909 [11776/54000 (22%)] Loss: -1579.771240\n",
      "Train Epoch: 909 [23040/54000 (43%)] Loss: -1564.671143\n",
      "Train Epoch: 909 [34304/54000 (64%)] Loss: -1617.793213\n",
      "Train Epoch: 909 [45568/54000 (84%)] Loss: -1620.984863\n",
      "    epoch          : 909\n",
      "    loss           : -1630.202121847927\n",
      "    val_loss       : -1627.7240153087562\n",
      "    val_log_likelihood: 1721.2590706702506\n",
      "    val_log_marginal: 1661.5709878115704\n",
      "Train Epoch: 910 [512/54000 (1%)] Loss: -1892.218872\n",
      "Train Epoch: 910 [11776/54000 (22%)] Loss: -1516.101929\n",
      "Train Epoch: 910 [23040/54000 (43%)] Loss: -1595.223022\n",
      "Train Epoch: 910 [34304/54000 (64%)] Loss: -1624.673828\n",
      "Train Epoch: 910 [45568/54000 (84%)] Loss: -1622.022705\n",
      "    epoch          : 910\n",
      "    loss           : -1615.2192552018873\n",
      "    val_loss       : -1615.5281226269274\n",
      "    val_log_likelihood: 1724.8552354869275\n",
      "    val_log_marginal: 1666.7198251862226\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch910.pth ...\n",
      "Train Epoch: 911 [512/54000 (1%)] Loss: -1573.516235\n",
      "Train Epoch: 911 [11776/54000 (22%)] Loss: -1560.428101\n",
      "Train Epoch: 911 [23040/54000 (43%)] Loss: -1640.652222\n",
      "Train Epoch: 911 [34304/54000 (64%)] Loss: -1673.964111\n",
      "Train Epoch: 911 [45568/54000 (84%)] Loss: -1600.377808\n",
      "    epoch          : 911\n",
      "    loss           : -1613.8425715984683\n",
      "    val_loss       : -1624.939727192182\n",
      "    val_log_likelihood: 1731.62017278388\n",
      "    val_log_marginal: 1673.8123091851537\n",
      "Train Epoch: 912 [512/54000 (1%)] Loss: -1884.900391\n",
      "Train Epoch: 912 [11776/54000 (22%)] Loss: -1555.490356\n",
      "Train Epoch: 912 [23040/54000 (43%)] Loss: -1619.856201\n",
      "Train Epoch: 912 [34304/54000 (64%)] Loss: -1562.336914\n",
      "Train Epoch: 912 [45568/54000 (84%)] Loss: -1606.043335\n",
      "    epoch          : 912\n",
      "    loss           : -1622.7362374787283\n",
      "    val_loss       : -1621.559744612655\n",
      "    val_log_likelihood: 1733.948812654703\n",
      "    val_log_marginal: 1674.3257048044306\n",
      "Train Epoch: 913 [512/54000 (1%)] Loss: -1571.599731\n",
      "Train Epoch: 913 [11776/54000 (22%)] Loss: -1569.379150\n",
      "Train Epoch: 913 [23040/54000 (43%)] Loss: -1592.031982\n",
      "Train Epoch: 913 [34304/54000 (64%)] Loss: -1506.607666\n",
      "Train Epoch: 913 [45568/54000 (84%)] Loss: -1625.775024\n",
      "    epoch          : 913\n",
      "    loss           : -1626.9954737295018\n",
      "    val_loss       : -1626.8120272082765\n",
      "    val_log_likelihood: 1736.3775356783726\n",
      "    val_log_marginal: 1675.902635089543\n",
      "Train Epoch: 914 [512/54000 (1%)] Loss: -1903.370605\n",
      "Train Epoch: 914 [11776/54000 (22%)] Loss: -1670.563599\n",
      "Train Epoch: 914 [23040/54000 (43%)] Loss: -1486.859985\n",
      "Train Epoch: 914 [34304/54000 (64%)] Loss: -1910.499268\n",
      "Train Epoch: 914 [45568/54000 (84%)] Loss: -1601.145874\n",
      "    epoch          : 914\n",
      "    loss           : -1624.7951200881807\n",
      "    val_loss       : -1631.108869492685\n",
      "    val_log_likelihood: 1741.7567440826115\n",
      "    val_log_marginal: 1682.5507293532944\n",
      "Train Epoch: 915 [512/54000 (1%)] Loss: -1811.111328\n",
      "Train Epoch: 915 [11776/54000 (22%)] Loss: -1585.936279\n",
      "Train Epoch: 915 [23040/54000 (43%)] Loss: -1668.224121\n",
      "Train Epoch: 915 [34304/54000 (64%)] Loss: -1528.056152\n",
      "Train Epoch: 915 [45568/54000 (84%)] Loss: -1648.284424\n",
      "    epoch          : 915\n",
      "    loss           : -1630.8287256826268\n",
      "    val_loss       : -1627.143714392447\n",
      "    val_log_likelihood: 1738.2687770730197\n",
      "    val_log_marginal: 1676.6601241684086\n",
      "Train Epoch: 916 [512/54000 (1%)] Loss: -1863.440430\n",
      "Train Epoch: 916 [11776/54000 (22%)] Loss: -1547.877686\n",
      "Train Epoch: 916 [23040/54000 (43%)] Loss: -1628.591797\n",
      "Train Epoch: 916 [34304/54000 (64%)] Loss: -1502.398926\n",
      "Train Epoch: 916 [45568/54000 (84%)] Loss: -1547.764648\n",
      "    epoch          : 916\n",
      "    loss           : -1622.9399800819926\n",
      "    val_loss       : -1615.609605753361\n",
      "    val_log_likelihood: 1705.50678034112\n",
      "    val_log_marginal: 1646.6200098340205\n",
      "Train Epoch: 917 [512/54000 (1%)] Loss: -1867.973389\n",
      "Train Epoch: 917 [11776/54000 (22%)] Loss: -1537.892334\n",
      "Train Epoch: 917 [23040/54000 (43%)] Loss: -1540.235718\n",
      "Train Epoch: 917 [34304/54000 (64%)] Loss: -1551.354248\n",
      "Train Epoch: 917 [45568/54000 (84%)] Loss: -1552.775269\n",
      "    epoch          : 917\n",
      "    loss           : -1621.848524036974\n",
      "    val_loss       : -1626.6407554285454\n",
      "    val_log_likelihood: 1721.2427096225247\n",
      "    val_log_marginal: 1664.520239487567\n",
      "Train Epoch: 918 [512/54000 (1%)] Loss: -1912.750366\n",
      "Train Epoch: 918 [11776/54000 (22%)] Loss: -1587.551758\n",
      "Train Epoch: 918 [23040/54000 (43%)] Loss: -1534.770386\n",
      "Train Epoch: 918 [34304/54000 (64%)] Loss: -1623.508423\n",
      "Train Epoch: 918 [45568/54000 (84%)] Loss: -1531.324463\n",
      "    epoch          : 918\n",
      "    loss           : -1630.0328925104425\n",
      "    val_loss       : -1632.2979618032361\n",
      "    val_log_likelihood: 1726.4657296662283\n",
      "    val_log_marginal: 1668.071069083249\n",
      "Train Epoch: 919 [512/54000 (1%)] Loss: -1577.328125\n",
      "Train Epoch: 919 [11776/54000 (22%)] Loss: -1728.359863\n",
      "Train Epoch: 919 [23040/54000 (43%)] Loss: -1507.272217\n",
      "Train Epoch: 919 [34304/54000 (64%)] Loss: -1889.083496\n",
      "Train Epoch: 919 [45568/54000 (84%)] Loss: -1540.484131\n",
      "    epoch          : 919\n",
      "    loss           : -1630.044541160659\n",
      "    val_loss       : -1625.0333454545662\n",
      "    val_log_likelihood: 1723.3801898012066\n",
      "    val_log_marginal: 1666.4821816272126\n",
      "Train Epoch: 920 [512/54000 (1%)] Loss: -1857.561401\n",
      "Train Epoch: 920 [11776/54000 (22%)] Loss: -1735.270386\n",
      "Train Epoch: 920 [23040/54000 (43%)] Loss: -1690.666504\n",
      "Train Epoch: 920 [34304/54000 (64%)] Loss: -1559.240112\n",
      "Train Epoch: 920 [45568/54000 (84%)] Loss: -1603.312256\n",
      "    epoch          : 920\n",
      "    loss           : -1622.2962984897122\n",
      "    val_loss       : -1626.785418602223\n",
      "    val_log_likelihood: 1733.6092722675587\n",
      "    val_log_marginal: 1671.399067591599\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch920.pth ...\n",
      "Train Epoch: 921 [512/54000 (1%)] Loss: -1894.029053\n",
      "Train Epoch: 921 [11776/54000 (22%)] Loss: -1622.848999\n",
      "Train Epoch: 921 [23040/54000 (43%)] Loss: -1649.787109\n",
      "Train Epoch: 921 [34304/54000 (64%)] Loss: -1613.690430\n",
      "Train Epoch: 921 [45568/54000 (84%)] Loss: -1554.508789\n",
      "    epoch          : 921\n",
      "    loss           : -1632.287003016708\n",
      "    val_loss       : -1637.116878135757\n",
      "    val_log_likelihood: 1734.2513705716274\n",
      "    val_log_marginal: 1674.520090873713\n",
      "Train Epoch: 922 [512/54000 (1%)] Loss: -1908.094238\n",
      "Train Epoch: 922 [11776/54000 (22%)] Loss: -1732.930176\n",
      "Train Epoch: 922 [23040/54000 (43%)] Loss: -1662.929199\n",
      "Train Epoch: 922 [34304/54000 (64%)] Loss: -1535.380127\n",
      "Train Epoch: 922 [45568/54000 (84%)] Loss: -1603.592896\n",
      "    epoch          : 922\n",
      "    loss           : -1624.063856068224\n",
      "    val_loss       : -1626.7802104059385\n",
      "    val_log_likelihood: 1726.47770585164\n",
      "    val_log_marginal: 1666.2652065674083\n",
      "Train Epoch: 923 [512/54000 (1%)] Loss: -1869.016968\n",
      "Train Epoch: 923 [11776/54000 (22%)] Loss: -1571.908447\n",
      "Train Epoch: 923 [23040/54000 (43%)] Loss: -1672.722046\n",
      "Train Epoch: 923 [34304/54000 (64%)] Loss: -1665.107666\n",
      "Train Epoch: 923 [45568/54000 (84%)] Loss: -1518.713501\n",
      "    epoch          : 923\n",
      "    loss           : -1636.283120939047\n",
      "    val_loss       : -1636.4979862974458\n",
      "    val_log_likelihood: 1731.5988974996133\n",
      "    val_log_marginal: 1674.6384347641656\n",
      "Train Epoch: 924 [512/54000 (1%)] Loss: -1581.138428\n",
      "Train Epoch: 924 [11776/54000 (22%)] Loss: -1718.931030\n",
      "Train Epoch: 924 [23040/54000 (43%)] Loss: -1591.166992\n",
      "Train Epoch: 924 [34304/54000 (64%)] Loss: -1889.549072\n",
      "Train Epoch: 924 [45568/54000 (84%)] Loss: -1641.952393\n",
      "    epoch          : 924\n",
      "    loss           : -1635.714488416615\n",
      "    val_loss       : -1637.7803578254131\n",
      "    val_log_likelihood: 1735.4124828376393\n",
      "    val_log_marginal: 1679.5460095980416\n",
      "Train Epoch: 925 [512/54000 (1%)] Loss: -1890.955444\n",
      "Train Epoch: 925 [11776/54000 (22%)] Loss: -1568.860718\n",
      "Train Epoch: 925 [23040/54000 (43%)] Loss: -1617.060913\n",
      "Train Epoch: 925 [34304/54000 (64%)] Loss: -1613.246338\n",
      "Train Epoch: 925 [45568/54000 (84%)] Loss: -1630.201660\n",
      "    epoch          : 925\n",
      "    loss           : -1638.4222363764698\n",
      "    val_loss       : -1639.0135328104388\n",
      "    val_log_likelihood: 1740.371881768255\n",
      "    val_log_marginal: 1682.2297328576851\n",
      "Train Epoch: 926 [512/54000 (1%)] Loss: -1888.907959\n",
      "Train Epoch: 926 [11776/54000 (22%)] Loss: -1589.669189\n",
      "Train Epoch: 926 [23040/54000 (43%)] Loss: -1620.190430\n",
      "Train Epoch: 926 [34304/54000 (64%)] Loss: -1903.724854\n",
      "Train Epoch: 926 [45568/54000 (84%)] Loss: -1655.478271\n",
      "    epoch          : 926\n",
      "    loss           : -1636.5269026048113\n",
      "    val_loss       : -1635.6405172501\n",
      "    val_log_likelihood: 1733.5781129138304\n",
      "    val_log_marginal: 1674.2877454157401\n",
      "Train Epoch: 927 [512/54000 (1%)] Loss: -1886.896118\n",
      "Train Epoch: 927 [11776/54000 (22%)] Loss: -1530.568604\n",
      "Train Epoch: 927 [23040/54000 (43%)] Loss: -1589.330078\n",
      "Train Epoch: 927 [34304/54000 (64%)] Loss: -1644.911621\n",
      "Train Epoch: 927 [45568/54000 (84%)] Loss: -1545.136719\n",
      "    epoch          : 927\n",
      "    loss           : -1639.1929967899134\n",
      "    val_loss       : -1640.3610328440877\n",
      "    val_log_likelihood: 1741.2498682607518\n",
      "    val_log_marginal: 1682.4841903705035\n",
      "Train Epoch: 928 [512/54000 (1%)] Loss: -1904.611572\n",
      "Train Epoch: 928 [11776/54000 (22%)] Loss: -1583.736694\n",
      "Train Epoch: 928 [23040/54000 (43%)] Loss: -1539.215332\n",
      "Train Epoch: 928 [34304/54000 (64%)] Loss: -1541.484863\n",
      "Train Epoch: 928 [45568/54000 (84%)] Loss: -1554.349609\n",
      "    epoch          : 928\n",
      "    loss           : -1638.4555470683788\n",
      "    val_loss       : -1638.0955436867707\n",
      "    val_log_likelihood: 1739.3855077158107\n",
      "    val_log_marginal: 1678.1986434420069\n",
      "Train Epoch: 929 [512/54000 (1%)] Loss: -1882.077759\n",
      "Train Epoch: 929 [11776/54000 (22%)] Loss: -1691.840088\n",
      "Train Epoch: 929 [23040/54000 (43%)] Loss: -1681.237793\n",
      "Train Epoch: 929 [34304/54000 (64%)] Loss: -1679.587646\n",
      "Train Epoch: 929 [45568/54000 (84%)] Loss: -1636.717041\n",
      "    epoch          : 929\n",
      "    loss           : -1635.8230137588955\n",
      "    val_loss       : -1628.0390075355178\n",
      "    val_log_likelihood: 1729.052108311417\n",
      "    val_log_marginal: 1669.802302110885\n",
      "Train Epoch: 930 [512/54000 (1%)] Loss: -1851.678223\n",
      "Train Epoch: 930 [11776/54000 (22%)] Loss: -1547.166138\n",
      "Train Epoch: 930 [23040/54000 (43%)] Loss: -1513.939941\n",
      "Train Epoch: 930 [34304/54000 (64%)] Loss: -1871.008789\n",
      "Train Epoch: 930 [45568/54000 (84%)] Loss: -1595.668579\n",
      "    epoch          : 930\n",
      "    loss           : -1624.2841446376083\n",
      "    val_loss       : -1624.0181450714533\n",
      "    val_log_likelihood: 1726.4139984433014\n",
      "    val_log_marginal: 1667.5132319657814\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch930.pth ...\n",
      "Train Epoch: 931 [512/54000 (1%)] Loss: -1877.917603\n",
      "Train Epoch: 931 [11776/54000 (22%)] Loss: -1700.361694\n",
      "Train Epoch: 931 [23040/54000 (43%)] Loss: -1535.339111\n",
      "Train Epoch: 931 [34304/54000 (64%)] Loss: -1577.820435\n",
      "Train Epoch: 931 [45568/54000 (84%)] Loss: -1601.933838\n",
      "    epoch          : 931\n",
      "    loss           : -1626.2698031888149\n",
      "    val_loss       : -1628.644606489633\n",
      "    val_log_likelihood: 1736.987056921024\n",
      "    val_log_marginal: 1677.9220862300792\n",
      "Train Epoch: 932 [512/54000 (1%)] Loss: -1900.278809\n",
      "Train Epoch: 932 [11776/54000 (22%)] Loss: -1528.750244\n",
      "Train Epoch: 932 [23040/54000 (43%)] Loss: -1524.085449\n",
      "Train Epoch: 932 [34304/54000 (64%)] Loss: -1624.405762\n",
      "Train Epoch: 932 [45568/54000 (84%)] Loss: -1548.558594\n",
      "    epoch          : 932\n",
      "    loss           : -1625.9393105081992\n",
      "    val_loss       : -1623.5045990031213\n",
      "    val_log_likelihood: 1720.7233499961324\n",
      "    val_log_marginal: 1659.693795116923\n",
      "Train Epoch: 933 [512/54000 (1%)] Loss: -1850.741943\n",
      "Train Epoch: 933 [11776/54000 (22%)] Loss: -1592.965576\n",
      "Train Epoch: 933 [23040/54000 (43%)] Loss: -1532.046753\n",
      "Train Epoch: 933 [34304/54000 (64%)] Loss: -1597.292358\n",
      "Train Epoch: 933 [45568/54000 (84%)] Loss: -1645.838989\n",
      "    epoch          : 933\n",
      "    loss           : -1631.3972675587872\n",
      "    val_loss       : -1637.1837144344122\n",
      "    val_log_likelihood: 1729.5569766205135\n",
      "    val_log_marginal: 1669.8259083759776\n",
      "Train Epoch: 934 [512/54000 (1%)] Loss: -1526.984863\n",
      "Train Epoch: 934 [11776/54000 (22%)] Loss: -1564.095337\n",
      "Train Epoch: 934 [23040/54000 (43%)] Loss: -1559.588745\n",
      "Train Epoch: 934 [34304/54000 (64%)] Loss: -1483.674072\n",
      "Train Epoch: 934 [45568/54000 (84%)] Loss: -1613.616211\n",
      "    epoch          : 934\n",
      "    loss           : -1636.0782676168008\n",
      "    val_loss       : -1637.748584723607\n",
      "    val_log_likelihood: 1736.1699412128712\n",
      "    val_log_marginal: 1678.9364836561024\n",
      "Train Epoch: 935 [512/54000 (1%)] Loss: -1909.624756\n",
      "Train Epoch: 935 [11776/54000 (22%)] Loss: -1726.410400\n",
      "Train Epoch: 935 [23040/54000 (43%)] Loss: -1622.215942\n",
      "Train Epoch: 935 [34304/54000 (64%)] Loss: -1695.058716\n",
      "Train Epoch: 935 [45568/54000 (84%)] Loss: -1651.369263\n",
      "    epoch          : 935\n",
      "    loss           : -1641.041937799737\n",
      "    val_loss       : -1637.7846222188748\n",
      "    val_log_likelihood: 1737.5149094987623\n",
      "    val_log_marginal: 1679.8989277849605\n",
      "Train Epoch: 936 [512/54000 (1%)] Loss: -1905.305664\n",
      "Train Epoch: 936 [11776/54000 (22%)] Loss: -1749.280640\n",
      "Train Epoch: 936 [23040/54000 (43%)] Loss: -1575.757568\n",
      "Train Epoch: 936 [34304/54000 (64%)] Loss: -1619.583984\n",
      "Train Epoch: 936 [45568/54000 (84%)] Loss: -1688.575073\n",
      "    epoch          : 936\n",
      "    loss           : -1641.2840092725094\n",
      "    val_loss       : -1635.697156965419\n",
      "    val_log_likelihood: 1733.8278059251238\n",
      "    val_log_marginal: 1674.7388334347668\n",
      "Train Epoch: 937 [512/54000 (1%)] Loss: -1553.501465\n",
      "Train Epoch: 937 [11776/54000 (22%)] Loss: -1572.656738\n",
      "Train Epoch: 937 [23040/54000 (43%)] Loss: -1554.532715\n",
      "Train Epoch: 937 [34304/54000 (64%)] Loss: -1545.245361\n",
      "Train Epoch: 937 [45568/54000 (84%)] Loss: -1644.176025\n",
      "    epoch          : 937\n",
      "    loss           : -1636.8429317663213\n",
      "    val_loss       : -1639.317428375751\n",
      "    val_log_likelihood: 1734.5841342435024\n",
      "    val_log_marginal: 1677.609614015271\n",
      "Train Epoch: 938 [512/54000 (1%)] Loss: -1874.335571\n",
      "Train Epoch: 938 [11776/54000 (22%)] Loss: -1504.378662\n",
      "Train Epoch: 938 [23040/54000 (43%)] Loss: -1684.064697\n",
      "Train Epoch: 938 [34304/54000 (64%)] Loss: -1597.840576\n",
      "Train Epoch: 938 [45568/54000 (84%)] Loss: -1585.842896\n",
      "    epoch          : 938\n",
      "    loss           : -1635.084068978187\n",
      "    val_loss       : -1630.918157694806\n",
      "    val_log_likelihood: 1738.4632955116801\n",
      "    val_log_marginal: 1680.3679713890658\n",
      "Train Epoch: 939 [512/54000 (1%)] Loss: -1914.427002\n",
      "Train Epoch: 939 [11776/54000 (22%)] Loss: -1690.442871\n",
      "Train Epoch: 939 [23040/54000 (43%)] Loss: -1673.539795\n",
      "Train Epoch: 939 [34304/54000 (64%)] Loss: -1703.298706\n",
      "Train Epoch: 939 [45568/54000 (84%)] Loss: -1639.316650\n",
      "    epoch          : 939\n",
      "    loss           : -1630.478574847231\n",
      "    val_loss       : -1640.3522545290502\n",
      "    val_log_likelihood: 1735.441077506188\n",
      "    val_log_marginal: 1677.2729627547167\n",
      "Train Epoch: 940 [512/54000 (1%)] Loss: -1924.121338\n",
      "Train Epoch: 940 [11776/54000 (22%)] Loss: -1571.095215\n",
      "Train Epoch: 940 [23040/54000 (43%)] Loss: -1516.469238\n",
      "Train Epoch: 940 [34304/54000 (64%)] Loss: -1893.493652\n",
      "Train Epoch: 940 [45568/54000 (84%)] Loss: -1594.313721\n",
      "    epoch          : 940\n",
      "    loss           : -1632.9483098700496\n",
      "    val_loss       : -1637.5320978743107\n",
      "    val_log_likelihood: 1738.7460454053219\n",
      "    val_log_marginal: 1679.924933625422\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch940.pth ...\n",
      "Train Epoch: 941 [512/54000 (1%)] Loss: -1714.708618\n",
      "Train Epoch: 941 [11776/54000 (22%)] Loss: -1565.228027\n",
      "Train Epoch: 941 [23040/54000 (43%)] Loss: -1652.471558\n",
      "Train Epoch: 941 [34304/54000 (64%)] Loss: -1888.584717\n",
      "Train Epoch: 941 [45568/54000 (84%)] Loss: -1615.950684\n",
      "    epoch          : 941\n",
      "    loss           : -1642.8059190806775\n",
      "    val_loss       : -1639.4792110889985\n",
      "    val_log_likelihood: 1742.3315079188583\n",
      "    val_log_marginal: 1683.745728902279\n",
      "Train Epoch: 942 [512/54000 (1%)] Loss: -1903.032959\n",
      "Train Epoch: 942 [11776/54000 (22%)] Loss: -1617.894043\n",
      "Train Epoch: 942 [23040/54000 (43%)] Loss: -1687.133301\n",
      "Train Epoch: 942 [34304/54000 (64%)] Loss: -1531.572998\n",
      "Train Epoch: 942 [45568/54000 (84%)] Loss: -1618.481934\n",
      "    epoch          : 942\n",
      "    loss           : -1640.1714362720452\n",
      "    val_loss       : -1629.37661051977\n",
      "    val_log_likelihood: 1736.6704730043316\n",
      "    val_log_marginal: 1680.7548888800943\n",
      "Train Epoch: 943 [512/54000 (1%)] Loss: -1889.969238\n",
      "Train Epoch: 943 [11776/54000 (22%)] Loss: -1674.153320\n",
      "Train Epoch: 943 [23040/54000 (43%)] Loss: -1463.751221\n",
      "Train Epoch: 943 [34304/54000 (64%)] Loss: -1674.286621\n",
      "Train Epoch: 943 [45568/54000 (84%)] Loss: -1624.979248\n",
      "    epoch          : 943\n",
      "    loss           : -1628.328991578357\n",
      "    val_loss       : -1627.5634165993513\n",
      "    val_log_likelihood: 1730.27989634901\n",
      "    val_log_marginal: 1671.9408129756748\n",
      "Train Epoch: 944 [512/54000 (1%)] Loss: -1881.415527\n",
      "Train Epoch: 944 [11776/54000 (22%)] Loss: -1719.801392\n",
      "Train Epoch: 944 [23040/54000 (43%)] Loss: -1524.488770\n",
      "Train Epoch: 944 [34304/54000 (64%)] Loss: -1647.358887\n",
      "Train Epoch: 944 [45568/54000 (84%)] Loss: -1595.169800\n",
      "    epoch          : 944\n",
      "    loss           : -1627.9168217725094\n",
      "    val_loss       : -1634.06020474551\n",
      "    val_log_likelihood: 1737.3068642191367\n",
      "    val_log_marginal: 1679.6085944160438\n",
      "Train Epoch: 945 [512/54000 (1%)] Loss: -1888.985107\n",
      "Train Epoch: 945 [11776/54000 (22%)] Loss: -1587.365601\n",
      "Train Epoch: 945 [23040/54000 (43%)] Loss: -1545.815918\n",
      "Train Epoch: 945 [34304/54000 (64%)] Loss: -1611.137207\n",
      "Train Epoch: 945 [45568/54000 (84%)] Loss: -1622.621948\n",
      "    epoch          : 945\n",
      "    loss           : -1638.0645292678682\n",
      "    val_loss       : -1640.6227425027564\n",
      "    val_log_likelihood: 1738.6848023669554\n",
      "    val_log_marginal: 1680.2777222969412\n",
      "Train Epoch: 946 [512/54000 (1%)] Loss: -1912.160156\n",
      "Train Epoch: 946 [11776/54000 (22%)] Loss: -1698.431396\n",
      "Train Epoch: 946 [23040/54000 (43%)] Loss: -1693.841797\n",
      "Train Epoch: 946 [34304/54000 (64%)] Loss: -1517.167725\n",
      "Train Epoch: 946 [45568/54000 (84%)] Loss: -1646.926514\n",
      "    epoch          : 946\n",
      "    loss           : -1641.762261419013\n",
      "    val_loss       : -1630.4825976233062\n",
      "    val_log_likelihood: 1731.0047812886758\n",
      "    val_log_marginal: 1673.4980433760857\n",
      "Train Epoch: 947 [512/54000 (1%)] Loss: -1903.466797\n",
      "Train Epoch: 947 [11776/54000 (22%)] Loss: -1522.419556\n",
      "Train Epoch: 947 [23040/54000 (43%)] Loss: -1556.528320\n",
      "Train Epoch: 947 [34304/54000 (64%)] Loss: -1879.974365\n",
      "Train Epoch: 947 [45568/54000 (84%)] Loss: -1563.930786\n",
      "    epoch          : 947\n",
      "    loss           : -1637.514973555461\n",
      "    val_loss       : -1637.267999849227\n",
      "    val_log_likelihood: 1745.4490580039449\n",
      "    val_log_marginal: 1687.6434433190097\n",
      "Train Epoch: 948 [512/54000 (1%)] Loss: -1894.613647\n",
      "Train Epoch: 948 [11776/54000 (22%)] Loss: -1725.016846\n",
      "Train Epoch: 948 [23040/54000 (43%)] Loss: -1624.816162\n",
      "Train Epoch: 948 [34304/54000 (64%)] Loss: -1902.102051\n",
      "Train Epoch: 948 [45568/54000 (84%)] Loss: -1605.025635\n",
      "    epoch          : 948\n",
      "    loss           : -1639.9637910446318\n",
      "    val_loss       : -1636.6903549884491\n",
      "    val_log_likelihood: 1746.6674236637532\n",
      "    val_log_marginal: 1688.112211905475\n",
      "Train Epoch: 949 [512/54000 (1%)] Loss: -1910.772705\n",
      "Train Epoch: 949 [11776/54000 (22%)] Loss: -1583.234131\n",
      "Train Epoch: 949 [23040/54000 (43%)] Loss: -1714.494873\n",
      "Train Epoch: 949 [34304/54000 (64%)] Loss: -1656.245239\n",
      "Train Epoch: 949 [45568/54000 (84%)] Loss: -1595.903320\n",
      "    epoch          : 949\n",
      "    loss           : -1643.1648009649598\n",
      "    val_loss       : -1641.7997570284872\n",
      "    val_log_likelihood: 1744.87109375\n",
      "    val_log_marginal: 1687.1753052686574\n",
      "Train Epoch: 950 [512/54000 (1%)] Loss: -1916.582642\n",
      "Train Epoch: 950 [11776/54000 (22%)] Loss: -1559.639160\n",
      "Train Epoch: 950 [23040/54000 (43%)] Loss: -1697.886230\n",
      "Train Epoch: 950 [34304/54000 (64%)] Loss: -1581.919678\n",
      "Train Epoch: 950 [45568/54000 (84%)] Loss: -1615.424194\n",
      "    epoch          : 950\n",
      "    loss           : -1641.6012796836324\n",
      "    val_loss       : -1641.3551406391832\n",
      "    val_log_likelihood: 1741.5072529103497\n",
      "    val_log_marginal: 1684.9104987099538\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch950.pth ...\n",
      "Train Epoch: 951 [512/54000 (1%)] Loss: -1906.807251\n",
      "Train Epoch: 951 [11776/54000 (22%)] Loss: -1540.772827\n",
      "Train Epoch: 951 [23040/54000 (43%)] Loss: -1620.677612\n",
      "Train Epoch: 951 [34304/54000 (64%)] Loss: -1607.453125\n",
      "Train Epoch: 951 [45568/54000 (84%)] Loss: -1592.376709\n",
      "    epoch          : 951\n",
      "    loss           : -1643.283850943688\n",
      "    val_loss       : -1640.3636932657514\n",
      "    val_log_likelihood: 1746.690484075263\n",
      "    val_log_marginal: 1688.750769601102\n",
      "Train Epoch: 952 [512/54000 (1%)] Loss: -1898.161865\n",
      "Train Epoch: 952 [11776/54000 (22%)] Loss: -1481.281738\n",
      "Train Epoch: 952 [23040/54000 (43%)] Loss: -1612.025391\n",
      "Train Epoch: 952 [34304/54000 (64%)] Loss: -1912.213867\n",
      "Train Epoch: 952 [45568/54000 (84%)] Loss: -1604.450928\n",
      "    epoch          : 952\n",
      "    loss           : -1640.9052021290997\n",
      "    val_loss       : -1643.620594425588\n",
      "    val_log_likelihood: 1744.5416042214572\n",
      "    val_log_marginal: 1686.534925941175\n",
      "Train Epoch: 953 [512/54000 (1%)] Loss: -1597.930420\n",
      "Train Epoch: 953 [11776/54000 (22%)] Loss: -1560.600952\n",
      "Train Epoch: 953 [23040/54000 (43%)] Loss: -1615.915405\n",
      "Train Epoch: 953 [34304/54000 (64%)] Loss: -1628.264038\n",
      "Train Epoch: 953 [45568/54000 (84%)] Loss: -1612.158936\n",
      "    epoch          : 953\n",
      "    loss           : -1640.6641229308477\n",
      "    val_loss       : -1645.3344836486872\n",
      "    val_log_likelihood: 1745.9276098874536\n",
      "    val_log_marginal: 1686.8444134875358\n",
      "Train Epoch: 954 [512/54000 (1%)] Loss: -1907.920410\n",
      "Train Epoch: 954 [11776/54000 (22%)] Loss: -1575.540894\n",
      "Train Epoch: 954 [23040/54000 (43%)] Loss: -1578.154297\n",
      "Train Epoch: 954 [34304/54000 (64%)] Loss: -1566.422119\n",
      "Train Epoch: 954 [45568/54000 (84%)] Loss: -1543.593750\n",
      "    epoch          : 954\n",
      "    loss           : -1630.8170202274134\n",
      "    val_loss       : -1625.366239040251\n",
      "    val_log_likelihood: 1703.5629520227412\n",
      "    val_log_marginal: 1646.2701020824104\n",
      "Train Epoch: 955 [512/54000 (1%)] Loss: -1884.755615\n",
      "Train Epoch: 955 [11776/54000 (22%)] Loss: -1636.282715\n",
      "Train Epoch: 955 [23040/54000 (43%)] Loss: -1626.657104\n",
      "Train Epoch: 955 [34304/54000 (64%)] Loss: -1668.075562\n",
      "Train Epoch: 955 [45568/54000 (84%)] Loss: -1562.421875\n",
      "    epoch          : 955\n",
      "    loss           : -1629.3185177038213\n",
      "    val_loss       : -1635.235832582901\n",
      "    val_log_likelihood: 1728.9203267616801\n",
      "    val_log_marginal: 1668.3652628129576\n",
      "Train Epoch: 956 [512/54000 (1%)] Loss: -1897.941772\n",
      "Train Epoch: 956 [11776/54000 (22%)] Loss: -1499.991211\n",
      "Train Epoch: 956 [23040/54000 (43%)] Loss: -1532.990479\n",
      "Train Epoch: 956 [34304/54000 (64%)] Loss: -1610.561035\n",
      "Train Epoch: 956 [45568/54000 (84%)] Loss: -1627.470093\n",
      "    epoch          : 956\n",
      "    loss           : -1635.7962658570545\n",
      "    val_loss       : -1640.1799622466604\n",
      "    val_log_likelihood: 1735.7436716816212\n",
      "    val_log_marginal: 1677.5247826157015\n",
      "Train Epoch: 957 [512/54000 (1%)] Loss: -1904.936768\n",
      "Train Epoch: 957 [11776/54000 (22%)] Loss: -1588.206787\n",
      "Train Epoch: 957 [23040/54000 (43%)] Loss: -1673.103760\n",
      "Train Epoch: 957 [34304/54000 (64%)] Loss: -1556.224731\n",
      "Train Epoch: 957 [45568/54000 (84%)] Loss: -1543.037476\n",
      "    epoch          : 957\n",
      "    loss           : -1640.0726052463644\n",
      "    val_loss       : -1642.5495364884432\n",
      "    val_log_likelihood: 1736.3122413559715\n",
      "    val_log_marginal: 1675.7992860438615\n",
      "Train Epoch: 958 [512/54000 (1%)] Loss: -1903.927002\n",
      "Train Epoch: 958 [11776/54000 (22%)] Loss: -1555.738037\n",
      "Train Epoch: 958 [23040/54000 (43%)] Loss: -1522.116455\n",
      "Train Epoch: 958 [34304/54000 (64%)] Loss: -1548.069458\n",
      "Train Epoch: 958 [45568/54000 (84%)] Loss: -1571.742676\n",
      "    epoch          : 958\n",
      "    loss           : -1646.6019601349783\n",
      "    val_loss       : -1641.921926292222\n",
      "    val_log_likelihood: 1738.4486374052444\n",
      "    val_log_marginal: 1679.076372961984\n",
      "Train Epoch: 959 [512/54000 (1%)] Loss: -1916.264404\n",
      "Train Epoch: 959 [11776/54000 (22%)] Loss: -1620.308960\n",
      "Train Epoch: 959 [23040/54000 (43%)] Loss: -1481.916138\n",
      "Train Epoch: 959 [34304/54000 (64%)] Loss: -1642.927490\n",
      "Train Epoch: 959 [45568/54000 (84%)] Loss: -1553.863525\n",
      "    epoch          : 959\n",
      "    loss           : -1634.4834552424968\n",
      "    val_loss       : -1636.0276836284372\n",
      "    val_log_likelihood: 1748.1502673460705\n",
      "    val_log_marginal: 1685.0409567073762\n",
      "Train Epoch: 960 [512/54000 (1%)] Loss: -1902.295654\n",
      "Train Epoch: 960 [11776/54000 (22%)] Loss: -1755.540771\n",
      "Train Epoch: 960 [23040/54000 (43%)] Loss: -1549.996826\n",
      "Train Epoch: 960 [34304/54000 (64%)] Loss: -1641.532227\n",
      "Train Epoch: 960 [45568/54000 (84%)] Loss: -1538.188232\n",
      "    epoch          : 960\n",
      "    loss           : -1639.843128770885\n",
      "    val_loss       : -1640.8639769708036\n",
      "    val_log_likelihood: 1743.6945305248298\n",
      "    val_log_marginal: 1685.7139937238499\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch960.pth ...\n",
      "Train Epoch: 961 [512/54000 (1%)] Loss: -1909.108276\n",
      "Train Epoch: 961 [11776/54000 (22%)] Loss: -1604.300415\n",
      "Train Epoch: 961 [23040/54000 (43%)] Loss: -1546.796753\n",
      "Train Epoch: 961 [34304/54000 (64%)] Loss: -1538.507080\n",
      "Train Epoch: 961 [45568/54000 (84%)] Loss: -1566.298462\n",
      "    epoch          : 961\n",
      "    loss           : -1643.517144231513\n",
      "    val_loss       : -1640.4961401203943\n",
      "    val_log_likelihood: 1734.7603119198639\n",
      "    val_log_marginal: 1678.0127518189388\n",
      "Train Epoch: 962 [512/54000 (1%)] Loss: -1882.500244\n",
      "Train Epoch: 962 [11776/54000 (22%)] Loss: -1564.625854\n",
      "Train Epoch: 962 [23040/54000 (43%)] Loss: -1631.039673\n",
      "Train Epoch: 962 [34304/54000 (64%)] Loss: -1680.324097\n",
      "Train Epoch: 962 [45568/54000 (84%)] Loss: -1645.091309\n",
      "    epoch          : 962\n",
      "    loss           : -1636.5925643467667\n",
      "    val_loss       : -1635.4791552422125\n",
      "    val_log_likelihood: 1735.1891739344833\n",
      "    val_log_marginal: 1671.88296371324\n",
      "Train Epoch: 963 [512/54000 (1%)] Loss: -1884.122070\n",
      "Train Epoch: 963 [11776/54000 (22%)] Loss: -1719.890869\n",
      "Train Epoch: 963 [23040/54000 (43%)] Loss: -1548.042725\n",
      "Train Epoch: 963 [34304/54000 (64%)] Loss: -1674.112427\n",
      "Train Epoch: 963 [45568/54000 (84%)] Loss: -1610.274902\n",
      "    epoch          : 963\n",
      "    loss           : -1631.8019306447247\n",
      "    val_loss       : -1636.8727015974462\n",
      "    val_log_likelihood: 1730.442819123221\n",
      "    val_log_marginal: 1670.557738261502\n",
      "Train Epoch: 964 [512/54000 (1%)] Loss: -1905.687988\n",
      "Train Epoch: 964 [11776/54000 (22%)] Loss: -1486.543701\n",
      "Train Epoch: 964 [23040/54000 (43%)] Loss: -1557.130371\n",
      "Train Epoch: 964 [34304/54000 (64%)] Loss: -1535.284790\n",
      "Train Epoch: 964 [45568/54000 (84%)] Loss: -1631.423340\n",
      "    epoch          : 964\n",
      "    loss           : -1636.9882244450032\n",
      "    val_loss       : -1640.4413300932727\n",
      "    val_log_likelihood: 1738.293320457534\n",
      "    val_log_marginal: 1678.1420133150173\n",
      "Train Epoch: 965 [512/54000 (1%)] Loss: -1630.354736\n",
      "Train Epoch: 965 [11776/54000 (22%)] Loss: -1620.869019\n",
      "Train Epoch: 965 [23040/54000 (43%)] Loss: -1585.965454\n",
      "Train Epoch: 965 [34304/54000 (64%)] Loss: -1680.539307\n",
      "Train Epoch: 965 [45568/54000 (84%)] Loss: -1614.462646\n",
      "    epoch          : 965\n",
      "    loss           : -1639.0778470181003\n",
      "    val_loss       : -1640.0772672630385\n",
      "    val_log_likelihood: 1732.3044228128867\n",
      "    val_log_marginal: 1673.4899146288892\n",
      "Train Epoch: 966 [512/54000 (1%)] Loss: -1717.027466\n",
      "Train Epoch: 966 [11776/54000 (22%)] Loss: -1571.181152\n",
      "Train Epoch: 966 [23040/54000 (43%)] Loss: -1711.765747\n",
      "Train Epoch: 966 [34304/54000 (64%)] Loss: -1574.403442\n",
      "Train Epoch: 966 [45568/54000 (84%)] Loss: -1629.771729\n",
      "    epoch          : 966\n",
      "    loss           : -1643.332411964341\n",
      "    val_loss       : -1644.9378087491368\n",
      "    val_log_likelihood: 1742.6178365273051\n",
      "    val_log_marginal: 1680.9909462736214\n",
      "Train Epoch: 967 [512/54000 (1%)] Loss: -1920.497314\n",
      "Train Epoch: 967 [11776/54000 (22%)] Loss: -1616.310303\n",
      "Train Epoch: 967 [23040/54000 (43%)] Loss: -1625.982300\n",
      "Train Epoch: 967 [34304/54000 (64%)] Loss: -1688.569580\n",
      "Train Epoch: 967 [45568/54000 (84%)] Loss: -1548.840088\n",
      "    epoch          : 967\n",
      "    loss           : -1648.5605976369122\n",
      "    val_loss       : -1649.8228431814027\n",
      "    val_log_likelihood: 1745.8518271871133\n",
      "    val_log_marginal: 1687.3585148490527\n",
      "Train Epoch: 968 [512/54000 (1%)] Loss: -1913.052979\n",
      "Train Epoch: 968 [11776/54000 (22%)] Loss: -1568.755249\n",
      "Train Epoch: 968 [23040/54000 (43%)] Loss: -1633.488770\n",
      "Train Epoch: 968 [34304/54000 (64%)] Loss: -1586.007080\n",
      "Train Epoch: 968 [45568/54000 (84%)] Loss: -1631.366211\n",
      "    epoch          : 968\n",
      "    loss           : -1650.2336292833386\n",
      "    val_loss       : -1648.4151077498082\n",
      "    val_log_likelihood: 1742.0608792446628\n",
      "    val_log_marginal: 1681.5565563298135\n",
      "Train Epoch: 969 [512/54000 (1%)] Loss: -1922.137451\n",
      "Train Epoch: 969 [11776/54000 (22%)] Loss: -1700.027100\n",
      "Train Epoch: 969 [23040/54000 (43%)] Loss: -1631.164062\n",
      "Train Epoch: 969 [34304/54000 (64%)] Loss: -1631.558838\n",
      "Train Epoch: 969 [45568/54000 (84%)] Loss: -1665.578003\n",
      "    epoch          : 969\n",
      "    loss           : -1648.2470727297339\n",
      "    val_loss       : -1655.9267364272498\n",
      "    val_log_likelihood: 1740.6203830832303\n",
      "    val_log_marginal: 1682.062820077887\n",
      "Train Epoch: 970 [512/54000 (1%)] Loss: -1910.305786\n",
      "Train Epoch: 970 [11776/54000 (22%)] Loss: -1578.628052\n",
      "Train Epoch: 970 [23040/54000 (43%)] Loss: -1540.890747\n",
      "Train Epoch: 970 [34304/54000 (64%)] Loss: -1612.077881\n",
      "Train Epoch: 970 [45568/54000 (84%)] Loss: -1618.059937\n",
      "    epoch          : 970\n",
      "    loss           : -1648.4230872428063\n",
      "    val_loss       : -1643.50471557864\n",
      "    val_log_likelihood: 1739.1221887569616\n",
      "    val_log_marginal: 1678.0112417996647\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch970.pth ...\n",
      "Train Epoch: 971 [512/54000 (1%)] Loss: -1574.516113\n",
      "Train Epoch: 971 [11776/54000 (22%)] Loss: -1760.060547\n",
      "Train Epoch: 971 [23040/54000 (43%)] Loss: -1702.397095\n",
      "Train Epoch: 971 [34304/54000 (64%)] Loss: -1559.011719\n",
      "Train Epoch: 971 [45568/54000 (84%)] Loss: -1636.085449\n",
      "    epoch          : 971\n",
      "    loss           : -1649.6735175104425\n",
      "    val_loss       : -1645.8234712072178\n",
      "    val_log_likelihood: 1742.4549512202198\n",
      "    val_log_marginal: 1683.1004344240973\n",
      "Train Epoch: 972 [512/54000 (1%)] Loss: -1727.091797\n",
      "Train Epoch: 972 [11776/54000 (22%)] Loss: -1607.603027\n",
      "Train Epoch: 972 [23040/54000 (43%)] Loss: -1576.839600\n",
      "Train Epoch: 972 [34304/54000 (64%)] Loss: -1579.157227\n",
      "Train Epoch: 972 [45568/54000 (84%)] Loss: -1551.319702\n",
      "    epoch          : 972\n",
      "    loss           : -1648.5394540918935\n",
      "    val_loss       : -1645.4642817072274\n",
      "    val_log_likelihood: 1744.9317409402072\n",
      "    val_log_marginal: 1684.4634241707217\n",
      "Train Epoch: 973 [512/54000 (1%)] Loss: -1885.194702\n",
      "Train Epoch: 973 [11776/54000 (22%)] Loss: -1582.542847\n",
      "Train Epoch: 973 [23040/54000 (43%)] Loss: -1536.173706\n",
      "Train Epoch: 973 [34304/54000 (64%)] Loss: -1533.078491\n",
      "Train Epoch: 973 [45568/54000 (84%)] Loss: -1603.573730\n",
      "    epoch          : 973\n",
      "    loss           : -1643.3085780379795\n",
      "    val_loss       : -1647.3666893485288\n",
      "    val_log_likelihood: 1747.6350593189202\n",
      "    val_log_marginal: 1686.4673429859415\n",
      "Train Epoch: 974 [512/54000 (1%)] Loss: -1892.999512\n",
      "Train Epoch: 974 [11776/54000 (22%)] Loss: -1580.949341\n",
      "Train Epoch: 974 [23040/54000 (43%)] Loss: -1696.547363\n",
      "Train Epoch: 974 [34304/54000 (64%)] Loss: -1654.110840\n",
      "Train Epoch: 974 [45568/54000 (84%)] Loss: -1652.166748\n",
      "    epoch          : 974\n",
      "    loss           : -1646.8587114692914\n",
      "    val_loss       : -1646.6398365383986\n",
      "    val_log_likelihood: 1746.2876423750774\n",
      "    val_log_marginal: 1686.4419930671147\n",
      "Train Epoch: 975 [512/54000 (1%)] Loss: -1913.646606\n",
      "Train Epoch: 975 [11776/54000 (22%)] Loss: -1707.458984\n",
      "Train Epoch: 975 [23040/54000 (43%)] Loss: -1663.864990\n",
      "Train Epoch: 975 [34304/54000 (64%)] Loss: -1589.781372\n",
      "Train Epoch: 975 [45568/54000 (84%)] Loss: -1561.034912\n",
      "    epoch          : 975\n",
      "    loss           : -1651.0004496055074\n",
      "    val_loss       : -1651.4771480537065\n",
      "    val_log_likelihood: 1751.6453277285736\n",
      "    val_log_marginal: 1689.9265497350273\n",
      "Train Epoch: 976 [512/54000 (1%)] Loss: -1911.008911\n",
      "Train Epoch: 976 [11776/54000 (22%)] Loss: -1565.872803\n",
      "Train Epoch: 976 [23040/54000 (43%)] Loss: -1544.642212\n",
      "Train Epoch: 976 [34304/54000 (64%)] Loss: -1577.382812\n",
      "Train Epoch: 976 [45568/54000 (84%)] Loss: -1632.145508\n",
      "    epoch          : 976\n",
      "    loss           : -1646.5520502978031\n",
      "    val_loss       : -1641.9517938846002\n",
      "    val_log_likelihood: 1743.8878463896194\n",
      "    val_log_marginal: 1681.4322063989457\n",
      "Train Epoch: 977 [512/54000 (1%)] Loss: -1908.290527\n",
      "Train Epoch: 977 [11776/54000 (22%)] Loss: -1517.565186\n",
      "Train Epoch: 977 [23040/54000 (43%)] Loss: -1571.529785\n",
      "Train Epoch: 977 [34304/54000 (64%)] Loss: -1641.454468\n",
      "Train Epoch: 977 [45568/54000 (84%)] Loss: -1682.518799\n",
      "    epoch          : 977\n",
      "    loss           : -1640.425619295328\n",
      "    val_loss       : -1647.161644806191\n",
      "    val_log_likelihood: 1746.3099135597154\n",
      "    val_log_marginal: 1685.4884025034894\n",
      "Train Epoch: 978 [512/54000 (1%)] Loss: -1898.741943\n",
      "Train Epoch: 978 [11776/54000 (22%)] Loss: -1601.545166\n",
      "Train Epoch: 978 [23040/54000 (43%)] Loss: -1696.010986\n",
      "Train Epoch: 978 [34304/54000 (64%)] Loss: -1563.747681\n",
      "Train Epoch: 978 [45568/54000 (84%)] Loss: -1631.512695\n",
      "    epoch          : 978\n",
      "    loss           : -1647.5928217821781\n",
      "    val_loss       : -1642.6919467720286\n",
      "    val_log_likelihood: 1737.8076220219677\n",
      "    val_log_marginal: 1674.6069629982223\n",
      "Train Epoch: 979 [512/54000 (1%)] Loss: -1906.314453\n",
      "Train Epoch: 979 [11776/54000 (22%)] Loss: -1556.646851\n",
      "Train Epoch: 979 [23040/54000 (43%)] Loss: -1636.691406\n",
      "Train Epoch: 979 [34304/54000 (64%)] Loss: -1561.319458\n",
      "Train Epoch: 979 [45568/54000 (84%)] Loss: -1502.441895\n",
      "    epoch          : 979\n",
      "    loss           : -1642.0259792214572\n",
      "    val_loss       : -1640.5948302675001\n",
      "    val_log_likelihood: 1728.4129602413366\n",
      "    val_log_marginal: 1668.5585704793104\n",
      "Train Epoch: 980 [512/54000 (1%)] Loss: -1910.779053\n",
      "Train Epoch: 980 [11776/54000 (22%)] Loss: -1557.907471\n",
      "Train Epoch: 980 [23040/54000 (43%)] Loss: -1644.278809\n",
      "Train Epoch: 980 [34304/54000 (64%)] Loss: -1551.861816\n",
      "Train Epoch: 980 [45568/54000 (84%)] Loss: -1644.716797\n",
      "    epoch          : 980\n",
      "    loss           : -1640.3485409576115\n",
      "    val_loss       : -1642.9073471829092\n",
      "    val_log_likelihood: 1728.3470108485458\n",
      "    val_log_marginal: 1671.290594081459\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch980.pth ...\n",
      "Train Epoch: 981 [512/54000 (1%)] Loss: -1912.883545\n",
      "Train Epoch: 981 [11776/54000 (22%)] Loss: -1721.902588\n",
      "Train Epoch: 981 [23040/54000 (43%)] Loss: -1670.065552\n",
      "Train Epoch: 981 [34304/54000 (64%)] Loss: -1568.825195\n",
      "Train Epoch: 981 [45568/54000 (84%)] Loss: -1591.353516\n",
      "    epoch          : 981\n",
      "    loss           : -1640.9876370571628\n",
      "    val_loss       : -1642.1665783619399\n",
      "    val_log_likelihood: 1732.0789263130414\n",
      "    val_log_marginal: 1671.21432422723\n",
      "Train Epoch: 982 [512/54000 (1%)] Loss: -1906.413330\n",
      "Train Epoch: 982 [11776/54000 (22%)] Loss: -1570.593140\n",
      "Train Epoch: 982 [23040/54000 (43%)] Loss: -1538.529541\n",
      "Train Epoch: 982 [34304/54000 (64%)] Loss: -1914.393799\n",
      "Train Epoch: 982 [45568/54000 (84%)] Loss: -1544.025757\n",
      "    epoch          : 982\n",
      "    loss           : -1642.0527694248917\n",
      "    val_loss       : -1643.4227367900774\n",
      "    val_log_likelihood: 1736.6773959622524\n",
      "    val_log_marginal: 1676.3749399547198\n",
      "Train Epoch: 983 [512/54000 (1%)] Loss: -1911.429688\n",
      "Train Epoch: 983 [11776/54000 (22%)] Loss: -1586.166504\n",
      "Train Epoch: 983 [23040/54000 (43%)] Loss: -1546.209961\n",
      "Train Epoch: 983 [34304/54000 (64%)] Loss: -1912.505859\n",
      "Train Epoch: 983 [45568/54000 (84%)] Loss: -1599.340210\n",
      "    epoch          : 983\n",
      "    loss           : -1646.5656363609994\n",
      "    val_loss       : -1645.2147539195669\n",
      "    val_log_likelihood: 1732.647569713026\n",
      "    val_log_marginal: 1671.546950486166\n",
      "Train Epoch: 984 [512/54000 (1%)] Loss: -1900.757935\n",
      "Train Epoch: 984 [11776/54000 (22%)] Loss: -1671.082520\n",
      "Train Epoch: 984 [23040/54000 (43%)] Loss: -1689.988525\n",
      "Train Epoch: 984 [34304/54000 (64%)] Loss: -1550.244873\n",
      "Train Epoch: 984 [45568/54000 (84%)] Loss: -1610.482910\n",
      "    epoch          : 984\n",
      "    loss           : -1647.6997517500774\n",
      "    val_loss       : -1643.4197402087289\n",
      "    val_log_likelihood: 1746.4184594484839\n",
      "    val_log_marginal: 1686.5221238361469\n",
      "Train Epoch: 985 [512/54000 (1%)] Loss: -1911.327393\n",
      "Train Epoch: 985 [11776/54000 (22%)] Loss: -1675.061768\n",
      "Train Epoch: 985 [23040/54000 (43%)] Loss: -1572.760742\n",
      "Train Epoch: 985 [34304/54000 (64%)] Loss: -1657.582764\n",
      "Train Epoch: 985 [45568/54000 (84%)] Loss: -1641.728027\n",
      "    epoch          : 985\n",
      "    loss           : -1641.6552359703744\n",
      "    val_loss       : -1642.1819921597507\n",
      "    val_log_likelihood: 1740.264571086015\n",
      "    val_log_marginal: 1680.9319336235633\n",
      "Train Epoch: 986 [512/54000 (1%)] Loss: -1895.226440\n",
      "Train Epoch: 986 [11776/54000 (22%)] Loss: -1566.082397\n",
      "Train Epoch: 986 [23040/54000 (43%)] Loss: -1511.779907\n",
      "Train Epoch: 986 [34304/54000 (64%)] Loss: -1644.989258\n",
      "Train Epoch: 986 [45568/54000 (84%)] Loss: -1531.939453\n",
      "    epoch          : 986\n",
      "    loss           : -1640.7422878152072\n",
      "    val_loss       : -1644.7069896901357\n",
      "    val_log_likelihood: 1738.0224222617574\n",
      "    val_log_marginal: 1681.3242657454039\n",
      "Train Epoch: 987 [512/54000 (1%)] Loss: -1914.356201\n",
      "Train Epoch: 987 [11776/54000 (22%)] Loss: -1507.774658\n",
      "Train Epoch: 987 [23040/54000 (43%)] Loss: -1512.240234\n",
      "Train Epoch: 987 [34304/54000 (64%)] Loss: -1913.609863\n",
      "Train Epoch: 987 [45568/54000 (84%)] Loss: -1578.085693\n",
      "    epoch          : 987\n",
      "    loss           : -1639.291200543394\n",
      "    val_loss       : -1632.4252307771862\n",
      "    val_log_likelihood: 1733.0206468034498\n",
      "    val_log_marginal: 1675.294866060875\n",
      "Train Epoch: 988 [512/54000 (1%)] Loss: -1913.698242\n",
      "Train Epoch: 988 [11776/54000 (22%)] Loss: -1667.588135\n",
      "Train Epoch: 988 [23040/54000 (43%)] Loss: -1560.436401\n",
      "Train Epoch: 988 [34304/54000 (64%)] Loss: -1616.485352\n",
      "Train Epoch: 988 [45568/54000 (84%)] Loss: -1637.972534\n",
      "    epoch          : 988\n",
      "    loss           : -1635.148815797107\n",
      "    val_loss       : -1640.5690224965076\n",
      "    val_log_likelihood: 1740.2137028155942\n",
      "    val_log_marginal: 1682.5515373537482\n",
      "Train Epoch: 989 [512/54000 (1%)] Loss: -1906.757568\n",
      "Train Epoch: 989 [11776/54000 (22%)] Loss: -1580.365967\n",
      "Train Epoch: 989 [23040/54000 (43%)] Loss: -1518.669800\n",
      "Train Epoch: 989 [34304/54000 (64%)] Loss: -1606.470215\n",
      "Train Epoch: 989 [45568/54000 (84%)] Loss: -1561.337646\n",
      "    epoch          : 989\n",
      "    loss           : -1631.3941686649134\n",
      "    val_loss       : -1633.9683913759297\n",
      "    val_log_likelihood: 1721.3800689395111\n",
      "    val_log_marginal: 1662.0485797834094\n",
      "Train Epoch: 990 [512/54000 (1%)] Loss: -1898.650391\n",
      "Train Epoch: 990 [11776/54000 (22%)] Loss: -1531.616211\n",
      "Train Epoch: 990 [23040/54000 (43%)] Loss: -1624.348999\n",
      "Train Epoch: 990 [34304/54000 (64%)] Loss: -1520.911133\n",
      "Train Epoch: 990 [45568/54000 (84%)] Loss: -1519.723022\n",
      "    epoch          : 990\n",
      "    loss           : -1624.9832812016552\n",
      "    val_loss       : -1629.98001847146\n",
      "    val_log_likelihood: 1727.8226910581684\n",
      "    val_log_marginal: 1670.1264056594916\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch990.pth ...\n",
      "Train Epoch: 991 [512/54000 (1%)] Loss: -1898.473267\n",
      "Train Epoch: 991 [11776/54000 (22%)] Loss: -1538.172729\n",
      "Train Epoch: 991 [23040/54000 (43%)] Loss: -1668.512207\n",
      "Train Epoch: 991 [34304/54000 (64%)] Loss: -1896.731079\n",
      "Train Epoch: 991 [45568/54000 (84%)] Loss: -1612.547241\n",
      "    epoch          : 991\n",
      "    loss           : -1631.6120097849628\n",
      "    val_loss       : -1638.9771073718766\n",
      "    val_log_likelihood: 1733.3592106280942\n",
      "    val_log_marginal: 1672.7439144346126\n",
      "Train Epoch: 992 [512/54000 (1%)] Loss: -1901.735107\n",
      "Train Epoch: 992 [11776/54000 (22%)] Loss: -1647.146973\n",
      "Train Epoch: 992 [23040/54000 (43%)] Loss: -1658.927490\n",
      "Train Epoch: 992 [34304/54000 (64%)] Loss: -1682.884155\n",
      "Train Epoch: 992 [45568/54000 (84%)] Loss: -1677.504028\n",
      "    epoch          : 992\n",
      "    loss           : -1639.6612573000464\n",
      "    val_loss       : -1640.9118071382238\n",
      "    val_log_likelihood: 1741.1538291402383\n",
      "    val_log_marginal: 1682.0642278530995\n",
      "Train Epoch: 993 [512/54000 (1%)] Loss: -1694.532959\n",
      "Train Epoch: 993 [11776/54000 (22%)] Loss: -1631.550293\n",
      "Train Epoch: 993 [23040/54000 (43%)] Loss: -1649.580444\n",
      "Train Epoch: 993 [34304/54000 (64%)] Loss: -1592.268799\n",
      "Train Epoch: 993 [45568/54000 (84%)] Loss: -1605.855225\n",
      "    epoch          : 993\n",
      "    loss           : -1641.1372130743348\n",
      "    val_loss       : -1645.2048650366153\n",
      "    val_log_likelihood: 1743.9141277653157\n",
      "    val_log_marginal: 1684.3782687956025\n",
      "Train Epoch: 994 [512/54000 (1%)] Loss: -1907.859375\n",
      "Train Epoch: 994 [11776/54000 (22%)] Loss: -1501.680786\n",
      "Train Epoch: 994 [23040/54000 (43%)] Loss: -1561.300781\n",
      "Train Epoch: 994 [34304/54000 (64%)] Loss: -1627.764404\n",
      "Train Epoch: 994 [45568/54000 (84%)] Loss: -1666.202637\n",
      "    epoch          : 994\n",
      "    loss           : -1644.5056575359683\n",
      "    val_loss       : -1648.9453563906695\n",
      "    val_log_likelihood: 1746.0757827003404\n",
      "    val_log_marginal: 1685.0220207599461\n",
      "Train Epoch: 995 [512/54000 (1%)] Loss: -1898.542480\n",
      "Train Epoch: 995 [11776/54000 (22%)] Loss: -1523.663208\n",
      "Train Epoch: 995 [23040/54000 (43%)] Loss: -1547.792725\n",
      "Train Epoch: 995 [34304/54000 (64%)] Loss: -1675.416992\n",
      "Train Epoch: 995 [45568/54000 (84%)] Loss: -1635.998535\n",
      "    epoch          : 995\n",
      "    loss           : -1641.9838419999226\n",
      "    val_loss       : -1636.279124849605\n",
      "    val_log_likelihood: 1732.8893656211324\n",
      "    val_log_marginal: 1673.0968895651179\n",
      "Train Epoch: 996 [512/54000 (1%)] Loss: -1568.077881\n",
      "Train Epoch: 996 [11776/54000 (22%)] Loss: -1671.355835\n",
      "Train Epoch: 996 [23040/54000 (43%)] Loss: -1603.628052\n",
      "Train Epoch: 996 [34304/54000 (64%)] Loss: -1635.483398\n",
      "Train Epoch: 996 [45568/54000 (84%)] Loss: -1584.221191\n",
      "    epoch          : 996\n",
      "    loss           : -1632.562854124768\n",
      "    val_loss       : -1639.4564602787948\n",
      "    val_log_likelihood: 1740.0985167852723\n",
      "    val_log_marginal: 1680.4714779619342\n",
      "Train Epoch: 997 [512/54000 (1%)] Loss: -1908.746704\n",
      "Train Epoch: 997 [11776/54000 (22%)] Loss: -1543.521973\n",
      "Train Epoch: 997 [23040/54000 (43%)] Loss: -1571.387939\n",
      "Train Epoch: 997 [34304/54000 (64%)] Loss: -1903.422607\n",
      "Train Epoch: 997 [45568/54000 (84%)] Loss: -1569.364258\n",
      "    epoch          : 997\n",
      "    loss           : -1641.1202755163213\n",
      "    val_loss       : -1645.3697898429589\n",
      "    val_log_likelihood: 1742.1207807182086\n",
      "    val_log_marginal: 1679.8891562599376\n",
      "Train Epoch: 998 [512/54000 (1%)] Loss: -1904.171143\n",
      "Train Epoch: 998 [11776/54000 (22%)] Loss: -1691.561279\n",
      "Train Epoch: 998 [23040/54000 (43%)] Loss: -1525.013550\n",
      "Train Epoch: 998 [34304/54000 (64%)] Loss: -1554.544434\n",
      "Train Epoch: 998 [45568/54000 (84%)] Loss: -1593.173340\n",
      "    epoch          : 998\n",
      "    loss           : -1639.425294177367\n",
      "    val_loss       : -1639.4438151956747\n",
      "    val_log_likelihood: 1742.9838238706684\n",
      "    val_log_marginal: 1681.0535003273094\n",
      "Train Epoch: 999 [512/54000 (1%)] Loss: -1914.140381\n",
      "Train Epoch: 999 [11776/54000 (22%)] Loss: -1621.995117\n",
      "Train Epoch: 999 [23040/54000 (43%)] Loss: -1512.666748\n",
      "Train Epoch: 999 [34304/54000 (64%)] Loss: -1561.843384\n",
      "Train Epoch: 999 [45568/54000 (84%)] Loss: -1615.915039\n",
      "    epoch          : 999\n",
      "    loss           : -1644.3847499129795\n",
      "    val_loss       : -1645.8704612474742\n",
      "    val_log_likelihood: 1748.3840537496133\n",
      "    val_log_marginal: 1687.4728135419916\n",
      "Train Epoch: 1000 [512/54000 (1%)] Loss: -1908.506348\n",
      "Train Epoch: 1000 [11776/54000 (22%)] Loss: -1615.525391\n",
      "Train Epoch: 1000 [23040/54000 (43%)] Loss: -1661.962891\n",
      "Train Epoch: 1000 [34304/54000 (64%)] Loss: -1547.291748\n",
      "Train Epoch: 1000 [45568/54000 (84%)] Loss: -1640.770142\n",
      "    epoch          : 1000\n",
      "    loss           : -1643.809980033648\n",
      "    val_loss       : -1646.805821139937\n",
      "    val_log_likelihood: 1745.946319277924\n",
      "    val_log_marginal: 1687.4227361384558\n",
      "Saving checkpoint: saved/models/Mnist_DeepGenerativeOperad/0201_155959/checkpoint-epoch1000.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepGenerativeOperadicModel(\n",
       "  (_operad): FreeOperad(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=72, bias=True)\n",
       "        (1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=72, out_features=72, bias=True)\n",
       "        (4): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=72, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=106, bias=True)\n",
       "        (1): LayerNorm((106,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=106, out_features=106, bias=True)\n",
       "        (4): LayerNorm((106,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=106, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_5): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=48, bias=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (4): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=48, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=80, bias=True)\n",
       "        (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=80, out_features=80, bias=True)\n",
       "        (4): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=80, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=114, bias=True)\n",
       "        (1): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=114, out_features=114, bias=True)\n",
       "        (4): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=114, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_9): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_10): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=130, bias=True)\n",
       "        (1): LayerNorm((130,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=130, out_features=130, bias=True)\n",
       "        (4): LayerNorm((130,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=130, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_11): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_12): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=162, bias=True)\n",
       "        (1): LayerNorm((162,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=162, out_features=162, bias=True)\n",
       "        (4): LayerNorm((162,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=162, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_13): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_14): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_weights): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=40, bias=True)\n",
       "  )\n",
       "  (asvi_params): PyroModuleDict(\n",
       "    (mean_fields): PyroModuleDict(\n",
       "      ($Z^{32}$): PyroParameterDict(\n",
       "          (loc): Object of type: ConvIncoder\n",
       "          (scale): Object of type: ConvIncoder\n",
       "        (loc): ConvIncoder(\n",
       "          (conv_layers): Sequential(\n",
       "            (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (5): PReLU(num_parameters=1)\n",
       "          )\n",
       "          (dense_layers): Sequential(\n",
       "            (0): Linear(in_features=2744, out_features=32, bias=True)\n",
       "            (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (scale): ConvIncoder(\n",
       "          (conv_layers): Sequential(\n",
       "            (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (5): PReLU(num_parameters=1)\n",
       "          )\n",
       "          (dense_layers): Sequential(\n",
       "            (0): Linear(in_features=2744, out_features=32, bias=True)\n",
       "            (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      ($Z^{128}$): PyroParameterDict(\n",
       "          (loc): Object of type: ConvIncoder\n",
       "          (scale): Object of type: ConvIncoder\n",
       "        (loc): ConvIncoder(\n",
       "          (conv_layers): Sequential(\n",
       "            (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (5): PReLU(num_parameters=1)\n",
       "          )\n",
       "          (dense_layers): Sequential(\n",
       "            (0): Linear(in_features=2744, out_features=128, bias=True)\n",
       "            (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (scale): ConvIncoder(\n",
       "          (conv_layers): Sequential(\n",
       "            (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (5): PReLU(num_parameters=1)\n",
       "          )\n",
       "          (dense_layers): Sequential(\n",
       "            (0): Linear(in_features=2744, out_features=128, bias=True)\n",
       "            (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      ($Z^{64}$): PyroParameterDict(\n",
       "          (loc): Object of type: ConvIncoder\n",
       "          (scale): Object of type: ConvIncoder\n",
       "        (loc): ConvIncoder(\n",
       "          (conv_layers): Sequential(\n",
       "            (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (5): PReLU(num_parameters=1)\n",
       "          )\n",
       "          (dense_layers): Sequential(\n",
       "            (0): Linear(in_features=2744, out_features=64, bias=True)\n",
       "            (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (scale): ConvIncoder(\n",
       "          (conv_layers): Sequential(\n",
       "            (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (5): PReLU(num_parameters=1)\n",
       "          )\n",
       "          (dense_layers): Sequential(\n",
       "            (0): Linear(in_features=2744, out_features=64, bias=True)\n",
       "            (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      ($Z^{16}$): PyroParameterDict(\n",
       "          (loc): Object of type: ConvIncoder\n",
       "          (scale): Object of type: ConvIncoder\n",
       "        (loc): ConvIncoder(\n",
       "          (conv_layers): Sequential(\n",
       "            (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (5): PReLU(num_parameters=1)\n",
       "          )\n",
       "          (dense_layers): Sequential(\n",
       "            (0): Linear(in_features=2744, out_features=16, bias=True)\n",
       "            (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (scale): ConvIncoder(\n",
       "          (conv_layers): Sequential(\n",
       "            (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (5): PReLU(num_parameters=1)\n",
       "          )\n",
       "          (dense_layers): Sequential(\n",
       "            (0): Linear(in_features=2744, out_features=16, bias=True)\n",
       "            (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      ($Z^{196}$): PyroParameterDict(\n",
       "          (loc): Object of type: ConvIncoder\n",
       "          (scale): Object of type: ConvIncoder\n",
       "        (loc): ConvIncoder(\n",
       "          (conv_layers): Sequential(\n",
       "            (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (5): PReLU(num_parameters=1)\n",
       "          )\n",
       "          (dense_layers): Sequential(\n",
       "            (0): Linear(in_features=2744, out_features=196, bias=True)\n",
       "            (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (scale): ConvIncoder(\n",
       "          (conv_layers): Sequential(\n",
       "            (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (5): PReLU(num_parameters=1)\n",
       "          )\n",
       "          (dense_layers): Sequential(\n",
       "            (0): Linear(in_features=2744, out_features=196, bias=True)\n",
       "            (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (prior_logits): PyroModuleDict(\n",
       "      ($Z^{32}$): ConvIncoder(\n",
       "        (conv_layers): Sequential(\n",
       "          (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (5): PReLU(num_parameters=1)\n",
       "        )\n",
       "        (dense_layers): Sequential(\n",
       "          (0): Linear(in_features=2744, out_features=1, bias=True)\n",
       "          (1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Linear(in_features=1, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "      ($Z^{128}$): ConvIncoder(\n",
       "        (conv_layers): Sequential(\n",
       "          (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (5): PReLU(num_parameters=1)\n",
       "        )\n",
       "        (dense_layers): Sequential(\n",
       "          (0): Linear(in_features=2744, out_features=1, bias=True)\n",
       "          (1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Linear(in_features=1, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "      ($Z^{64}$): ConvIncoder(\n",
       "        (conv_layers): Sequential(\n",
       "          (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (5): PReLU(num_parameters=1)\n",
       "        )\n",
       "        (dense_layers): Sequential(\n",
       "          (0): Linear(in_features=2744, out_features=1, bias=True)\n",
       "          (1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Linear(in_features=1, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "      ($Z^{16}$): ConvIncoder(\n",
       "        (conv_layers): Sequential(\n",
       "          (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (5): PReLU(num_parameters=1)\n",
       "        )\n",
       "        (dense_layers): Sequential(\n",
       "          (0): Linear(in_features=2744, out_features=1, bias=True)\n",
       "          (1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Linear(in_features=1, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "      ($Z^{196}$): ConvIncoder(\n",
       "        (conv_layers): Sequential(\n",
       "          (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (5): PReLU(num_parameters=1)\n",
       "        )\n",
       "        (dense_layers): Sequential(\n",
       "          (0): Linear(in_features=2744, out_features=1, bias=True)\n",
       "          (1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Linear(in_features=1, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEuCAYAAADx63eqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAASmklEQVR4nO3dT1LbSqOG8bdvnYEzU+kOcsZiBwpZwZWHmcnxCj5rB7hYAWXvAO0gRjtQ7yBEO6DHJwNUOqNk1neArM+GkIQANqafXxV18B+pO2fyVEttbLz3AgAgFP+z7wkAALBLhA8AEBTCBwAICuEDAASF8AEAgkL4AABBIXwAgKAQPgBAUAgfACAohA8AEBTCBwAICuEDAASF8AEAgkL4AABBIXwAgKAQPgBAUAgfACAohA8AEBTCBwAICuEDAASF8AEAgkL4AABBIXwAgKAQPgBAUAgfACAohA8AEBTCBwAICuEDAASF8AEAgkL4AABB+WuXg7158+af79+/v93lmACAwzAajb5++/bt7+cex3jvn3uM/w5mjN/leACAw2GMkffePPc4XOoEAASF8AEAgkL4AABBIXwAgKAQPgBAUAgfACAohA8AEBTCBwAICuEDXgDnnJqm0XK5VNd1dx4DeDqED3gB4jhWkiSSpLZt1TSN0jRVlmVarVZ7nh3wuhA+4JlVVbX1uCgKzedzVVWlsix1dHSkKIq23pPnuSTJWqssyyRJXdfdOReAhyN8wDOy1ipN063n3r17p8VioTzPdXFxobquVVWVoihSlmVD3Ky1yvNccRxL0hBH59xO/w3Aa8MfqQaeQNM0stYqSRIlSSLnnPI813w+12Kx+OExk8lEp6enStNUzjl1XSdrrWazmS4vL7VYLJQkicbj8bAClPTTcwKHbFd/pHqnX0sEvGbX19fKskxpmurs7Ex5nt+7MaUoCk2n02E1uL6/t36cZdlwifM2VnzA43CpE3gC61XbOlw/24m5XC51dHQ0hPGhuzbXlz4B/BnCBzyxqqpUFMW9r11dXenk5ESStFqt7mxsAfC8CB/wBDbv0bVtO9yT24xa0zSaz+caj8dDHK+urh48FqEEHofNLcATKMtSSZLcuS+3ueHlKTz1+YCXhG9gBw5E13W6uLj44WtZlqlpmicbRxLRAx6JFR8A4EVgxQcAwDMgfACAoBA+AEBQCB8AICiEDwAQFMIHAAgK4QMABIXwAQCCQvgAAEEhfACAoOz0i2hHo9FXY8zbXY4JADgMo9Ho6y7G2enf6gRwwxjzQdLMe/9h33MBQsOlTgBAUAgfACAohA8AEJSdbm4BgOdmjEkkRZIySaWkePOx977b19zwMrDiA/DatJJc/3ssKfXeN5KspI97mxVeDFZ8AA6OMSaTNOkfXkl6L+nce2+9950xJlq/13tf9b9mkioheKz4ABwc772VVEuqvfdLSZ8ktcaYyBiT95czraRcGkJZ6WY1iMARPgCvSSypMcak6u/p9dGb9z/ZPieHl4FLnQAOWWKMmUkaS/q0cVlTkpr+v7b/ASSx4gNw2NabWOpb0QPuRfgAHLqVbu7vzfY9ERwGwgfg4PT37cb9TyypkxQZYxb7nBcOA/f4ABycflfn5n07J+7j4Tex4gMABIXwAQCCQvgAAEEhfACAoBA+AEBQCB8AICiEDwAQFMIHAAgK4QMABIXwAQCCQvgAAEEhfACAoBA+AEBQCB8AICiEDwAQFMIHAAgK4QP2419JV/ueBBAi473f2WBv3rz55/v37293NiAA4GCMRqOv3759+/u5x9lp+IwxfpfjAQAOhzFG3nvz3ONwqRMAEBTCBwAICuEDAASF8AEAgkL4AABBIXwAgKAQPgBAUAgfACAohA/YgbIsZa1V0zTDc/P5/MHncc6paRotl0t1XXfnMYBfI3zAMyvLUkmSKMsypWkqSWqa5o9CFcexkiSRJLVtq6ZplKapsizTarV6ymkDrxbhAx6hqqqtx0VRaD6fq6oqlWWpo6MjffnyRV3XqaqqrRVfFEVbx5ZlKUmy1qooChVFoeVyqclkImvtD4/J83w4JsuyYRwA9yN8wB+y1g4ruLV3795psVgoz3NdXFyormtFUTSs9qy1Q5icc3LO3TlvlmUaj8caj8c6OTnRdDpVHMdD1NbnW5/HWqs8zxXH8RDGH50XwA3CB/zC+h7aesW2Dk5d18Nlx7XZbCZJmkwmWiwWSpJEp6enWq1WappGs9lMeZ4rSZIHX+ps21ZpmqppGllrNZvNZK3VYrHQYrEYVoV5nuv8/Pzx/3Dglfpr3xMADsH19fWwajs7O1Oe5/eGqygKTafTYTUYRdEQxLUoilTX9U/HdM6pLEvVda3pdDpc1pQ0nDvLMmVZ9sNjAfwYKz7gF9I0lXNuiM3PVmrL5VJHR0dDGB+z03K9mhyPx1vR+x1xHP/xuMBrR/iAB6iqSkVR3Pva1dWVTk5OJEmr1erOZpSH+vjxo+I4Hja+AHg8wgf8gnNOXdfJWqu2bYfV12bUmqbRfD7XeDwe4nh1dfVH41lrVde16rpW27aKokhd1z3oc3+PDS7wmvEN7MAvbH4Ob5O1VkmS3Nng8phxbt8L/BNPPS9gV/gGduAF6LpOFxcXP3wty7Ktz+W9BOt7ikQPuB8rPgDAi8CKDwCAZ0D4AABBIXwAgKAQPgBAUAgfACAohA8AEBTCBwAICuEDAASF8AEAgkL4AABB2ekX0Y5Go6/GmLe7HBMAcBhGo9HXXYyz07/VCeCGMeaDpJn3/sO+5wKEhkudAICgED4AQFAIHwAgKIQPwKtjjJkZYzJjTLrx3GKfc8LLsdNdnQDw3IwxM0nOe283nkslRXubFF4Uwgfg4BhjMkmT/uGVpPeSzvvYvZPUGmNy3QSw6d/X7XyieJEIH4CD4723xpio/73qI9f2z3WSrKRYUm6MSSQ5SYkxJvHeu/3MGi8F4QPwmsSSziR9lNRKKr33XR/EaI/zwgtC+AAcsqS/pzeW9Ml7X/XPl5tv8t53/XsAdnUCOGjry5b1RvSAnyJ8AA7dSjf392b7nggOA+EDcHD6XZ3j/ifWzYaWiM/q4Xdwjw/Awek/tmA3nnK3HgP3YsUHAAgK4QMABIXwAQCCQvgAAEEhfACAoBA+AEBQCB8AICiEDwAQFMIHAAgK4QMABIXwAQCCQvgAAEEhfACAoBA+AEBQCB8AICiEDwAQFMIH7Me/kq72PQkgRMZ7v7PB3rx588/379/f7mxAAMDBGI1GX799+/b3c4+z0/AZY/wuxwMAHA5jjLz35rnH4VInACAohA8AEBTCBwAICuEDAASF8AEAgkL4AABBIXwAgKAQPgBAUAgf8IzKspS1VpLUNI3KslRZluq6buv1pmn++Nybx87n8yeZN/Ca/bXvCQCv2fHxsZxzkqRPnz7p9PRUbduqLEtFUaQkSZRl2YPPW5blnWObphmCCuB+rPiAP1BV1dbjoig0n89VVZXKstTR0dGdY6bTqZxzappG19fX+vLli7quU1VVW6u2siwlSdZaFUWhoii0XC41mUyG1eN9x0ZRJEnDawDuYsUHPJC1Vmmabj337t07zWYzSdJ4PFZd13eOS9NUXdepbVu9f/9enz9/VpZlattWVVXdOWeWZcMKLs9zVVWlOI7VdZ2iKNo61jmnJEnknBt+l7T1O4AbhA+4R9M0stYqSZIhKnmeq65rLRaLrfeuozeZTLRYLIbYWGt1fX2tLMvknFPbtnLOaTabKcsyrVYrxXE8HP872rbV6enp1rFRFKnruq1LnXmeaz6f35krEDrCB/zEOlppmurs7Ex5nt97H60oCk2n062V28nJyfD77RVdFEW/FTznnMqyVF3Xmk6nyvNcku4cG0XRnZXm+v4igP/iHh9wjzRN5ZwbgvWzjSPL5VJHR0dDGJ9yk8l69Tgej4fo/a44jp9sHsBrQfiA31BVlYqiuPe1q6urYXW3Wq2GTSZP5ePHj4rjeNj4AuDPET7gHs45dV0na63ath1WW5tRa5pG8/lc4/F4iOPV1dWTjG+tVV3XqutabdsO9/Ee8lm9pw4w8BrwDezAPX70WTlJWxtenmvch2x2uc9zzxN4anwDO7BHXdfp4uLih69lWfZHf2lll9b3GIkecBcrPgDAi8CKDwCAZ0D4AABBIXwAgKAQPgBAUAgfACAohA8AEBTCBwAICuEDAASF8AEAgkL4AABB2ekX0Y5Go6/GmLe7HBMAcBhGo9HXXYyz07/VCeCGMeaDpJn3/sO+5wKEhkudAICgED4AQFAIHwAgKDvd3AIAu2CMmUly3ntrjEklHfcvrbz33fp1Sa33/mV/qzCeHOED8BpdSlp//fxU0pmkWNLMGNOpj+Ke5oY9I3wADo4xJpM06R9eSXov6fyemH3STQQTSf8r6UhSa4zJdRNAVnyBIXwADk5/CTPqf6/6iLXGmMh73916b9O/N5b0WTeRtP3jXBLhCwzhA/CaxJI6SZmk/zXGWN2s9GJJife+7J/7KKmVVO5rotgfPsAO7AEfYH+8fpWX6CZ0Y0mfvPfVXieFg8DHGQAcMtf/tyZ6+F2ED8ChW+nm/t5s3xPBYSB8AA5Ov6tz3P+s7+tFxpjFPueFw8DmFgAHp//YwuZHF9ytx8C9WPEBAIJC+AAAQSF8AICgED4AQFAIHwAgKIQPABAUwgcACArhAwAEhfABAIJC+AAAQSF8AICgED4AQFAIHwAgKIQPABAUwgcACArhAwAEhfAB+/GvpKt9TwIIkfHe72ywN2/e/PP9+/e3OxsQAHAwRqPR12/fvv393OPsNHzGGL/L8QAAh8MYI++9ee5xuNQJAAgK4QMABIXwAQCCQvgAAEEhfACAoBA+AEBQCB8AICiEDwAQFMIHPIOqqjSZTFQUhcqyVNd1appGVVWp67rhffP5/MHnLstS1lpJUtM0KstyGGPz9aZpnuKfArw6f+17AsBrlCSJLi4u5JxTHMe6vLyUc07Hx8dyzilNUzVNsxXB37U+hyR9+vRJp6enattWZVkqiiIlSaIsy574XwS8Hqz4gN9UVdWdx8aYYWVVFIXm87m6rlOappIk55yiKNLx8bG+fPmi//znP0qSZDhHFEXD72VZSpKstSqKQkVRaLlcajKZDCu826bTqZxzappG19fX+vLli7quU1VVw7zWjwHcYMUH/AZr7RCztTzPlee52rZV13WaTCZbK62qqoZjVquVzs/P1XWdyrJUkiRKkkTOOTnntmKYZdmwEszzXFVVKY5jdV23FUpJStNUXdepbVu9f/9enz9/VpZlatt2GH99zO1xgFCx4gM2NE2j5XI5rJjWK6W6rn8YjdPTUy0WC11eXt65vLi+zCndXJ601so5NwQzSZIHXeps21bSTYQ/f/483DdcX0bN81ynp6darVZqmkaz2Ww4Ns9znZ+fP/R/B/AqseIDbrm+vlaWZUrTVGdnZ8rz/N5ApWmqy8tLHR8f33nt5ORk6323RVGkuq7vnYdzTmVZqq5rTadT5Xn+y/NGUbQVvNvnA8CKD9iSpumw+UTSL1dkVVXp9PRUZ2dnTz6X9QpzPB4P0XuM9eoTCB3hA+5RVZWKovjp63meazabPdvmkY8fPyqO42HjC4DHI3zABuecuq6TtVZt2w4rrc1NJU3TaDwe39lo8iefyfsRa63qulZd12rbVlEUqeu6R5//9nyBUPEN7MCG9Y7L2xtVrLXDTsznHPu++3OPtYv5A4/FN7ADO9Z1nS4uLn74WpZlB/uXUNb3KYkecIMVHwDgRWDFBwDAMyB8AICgED4AQFAIHwAgKIQPABAUwgcACArhAwAEhfABAIJC+AAAQSF8AICg7PSLaEej0VdjzNtdjgkAOAyj0ejrLsbZ6d/qBHDDGPNB0sx7/2HfcwFCw6VOAEBQCB8AICiEDwAQFMIH4FUxxuTGmAtjzLkxZmaMiYwxaf98tPG+xR6niT3a6a5OANgB572fGGMSSa2kY0mJpMv+v40xJpUU7W+K2CfCB+DgGGMySZP+4ZWk95LOvffWe9/0zyfee2eMuezfW0j6v43TdLuaL14Wwgfg4Hjv7fqypfe+MsbkklpjTOS97/rH6wB+9N4X/ftnxhgnyUlKjDGJ997t49+A/SF8AF6TWDcruUSS7Z+77FeIraSqXwVG4lJnsAgfgEOWGGNmksaSPnnvK0ny3i/Xb9i49KmN57r+GASIXZ0ADtn6MmW9jh7wK4QPwKFb6eb+3mzfE8FhIHwADk5/z27c/6zv60V8Ng+/g3t8AA6O997qv5tXpJtLnvaetwNbWPEB+/Gvbj5/BmDH+FoiAEBQWPEBAIJC+AAAQSF8AICgED4AQFAIHwAgKIQPABAUwgcACArhAwAEhfABAIJC+AAAQSF8AICgED4AQFAIHwAgKIQPABAUwgcACArhAwAEhfABAIJC+AAAQSF8AICgED4AQFAIHwAgKIQPABAUwgcACArhAwAEhfABAIJC+AAAQSF8AICgED4AQFAIHwAgKIQPABCU/wc41K1ikojUhgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEuCAYAAADx63eqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAMR0lEQVR4nO3dQXITWbaA4XM7GJiZQj2oHosdqMwKnjysmYxXUNYOcHgFBOwA7QCTO7B2gMkdkONmYEW+yYPBi7g9sKy2DVRR5bJU4nxfBIFlpXSvmfxxMlOm1FoDALL4x7Y3AACbJHwApCJ8AKQifACkInwApCJ8AKQifACkInwApCJ8AKQifACkInwApCJ8AKQifACkInwApCJ8AKQifACkInwApCJ8AKQifACkInwApCJ8AKQifACkInwApCJ8AKQifACkInwApCJ8AKQifACkInwApCJ8AKTyaJOLPX78+N+fP3/+aZNrArAb9vb2Pn769OlfD71OqbU+9Br/XayUusn1ANgdpZSotZaHXsepTgBSET4AUhE+AFIRPgBSET4AUhE+AFIRPgBSET4AUhE+AFIRPgBSET4AUhE+AFIRPgBSET4AUhE+AFIRPgBSET4AUhE+AFIRPgBSET4AUhE+AFIRPgBSET4AUhE+AFIRPgBSET4AUnm07Q1AdvP5PEajUUwmk2jbNi4uLiIi4tmzZzEYDNbPD4fDGI/HW94t7D7hgy3b39+PrusiIuLNmzdxenoay+Uy5vN5DAaDdRSBv4ZTnfBAmqa59Xg2m8XJyUk0TRPz+TyePHnyxWuOjo6i67po2zYuLy/j/fv30fd9NE0TbdtGRKwfA3+OiQ8ewGKx+OK05M8//xzHx8cREXFwcBDn5+dfvG48Hkff97FcLuPp06fx7t27mEwmsVwuo2maGI/HMRgMIiKi67oYjUYP/rPAj6bUWje3WCl1k+vBQ2vbNhaLRYxGoxiNRtF1XUyn0zg5OYmXL19+9TWHh4dxenq6DuOrV6/i8vIyTk9Po+u6WC6X0XVdHB8fR9/3cXZ2FsPhMCaTyTp6EfGba8AuKqVErbU89DomPriny8vLmEwmMR6P48WLFzGdTqPv+68eO5vN4ujo6NY0+Pz58/XXd6fEwWCwnhLvur4uCPwxrvHBPYzH4+i6bh2sbwUv4mqye/LkyTqMv3Xs9xgOh/d6PWQlfPAXaZomZrPZN5/78OHDero7Ozu7ddoS2Bzhg3voui76vo/FYhHL5TKm02lExK2otW0bJycncXBwsI7jhw8f7r22cMKf4+YWuIebHz6/6eYNLw/hod8ftmFTN7eY+OBP6vs+3r59+9Xnrn8Ly0OtGxGiB3+SiQ+AvwUTHwA8AOEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMglUebXGxvb+9jKeWnTa4JwG7Y29v7uIl1Sq11E+sAN5RSfomI41rrL9veC2TjVCcAqQgfAKkIHwCpCB8AqQgfAKkIHwCpCB8AqQgfAKkIHwCpCB8AqWz0d3UCbEIp5TgiulrropQyjoj91VNntdb++vmIWNZa261tlK0QPuBHdBERo9XXRxHxIiKGEXFcSuljFcUt7Y0tEz5g55RSJhFxuHr4ISKeRsTrb8TsTVxFcBQR/4yIJxGxLKVM4yqAJr5khA/YOatTmIPV180qYstSyqDW2t85tl0dO4yId3EVycXq8TQihC8Z4QN+JMOI6CNiEhH/LKUs4mrSG0bEqNY6X33vWUQsI2K+rY2yPf4/PtgC/x/f/a2mvFFche4gIt7UWputboqd4OMMwC7rVn+fix7fS/iAXXcWV9f3jre9EXaD8AE7Z3VX58Hqz/V1vUEp5eU298VucHMLsHNWH1u4+dGF7s5j+CYTHwCpCB8AqQgfAKkIHwCpCB8AqQgfAKkIHwCpCB8AqQgfAKkIHwCpCB8AqQgfAKkIHwCpCB8AqQgfAKkIHwCpCB8AqQgfAKkIHwCpCB8AqQgfAKkIHwCpCB8AqQgfAKkIHwCpCB8AqQgfAKkIHwCpCB8AqQgfAKkIHwCpCB8AqQgfAKkIHwCpCB8AqQgfAKkIHwCpCB8AqQgfAKkIHwCpCB8AqQgfAKkIHwCpCB8AqQgfAKkIHwCpCB9sx/9HxP9texOQUam1bmyxx48f//vz588/bWxBAHbG3t7ex0+fPv3rodfZaPhKKXWT6wGwO0opUWstD72OU50ApCJ8AKQifACkInwApCJ8AKQifACkInwApCJ8AKQifACkInwApCJ8AKQifACkInwApCJ8AKQifACkInwApCJ8AKQifACkInwApCJ8AKQifACkInwApCJ8AKQifACkInwApCJ8AKQifPAAmqaJw8PDmM1mMZ/Po+/7aNs2mqaJvu/Xx52cnPzh957P57FYLCIiom3bmM/n6zVuPt+27V/xo8AP59G2NwA/otFoFG/fvo2u62I4HMbFxUV0XRf7+/vRdV2Mx+No2/ZWBL/X9XtERLx58yZOT09juVzGfD6PwWAQo9EoJpPJX/wTwY/DxAffqWmaLx6XUtaT1Ww2i5OTk+j7PsbjcUREdF0Xg8Eg9vf34/379/Hrr7/GaDRav8dgMFh/PZ/PIyJisVjEbDaL2WwWr169isPDw/WEd9fR0VF0XRdt28bl5WW8f/8++r6PpmnW+7p+DFwx8cF3WCwW65hdm06nMZ1OY7lcRt/3cXh4eGvSappm/Zqzs7N4/fp19H0f8/k8RqNRjEaj6Louuq67FcPJZLKeBKfTaTRNE8PhMPq+vxXKiIjxeBx938dyuYynT5/Gu3fvYjKZxHK5XK9//Zq760BWJj64oW3bePXq1Xpiup6Uzs/PvxqN09PTePnyZVxcXHxxevH6NGfE1enJxWIRXdetgzkajf7Qqc7lchkRVxF+9+7d+rrh9WnU6XQap6encXZ2Fm3bxvHx8fq10+k0Xr9+/Uf/OeCHZOKDOy4vL2MymcR4PI4XL17EdDr9ZqDG43FcXFzE/v7+F889f/781nF3DQaDOD8//+Y+uq6L+Xwe5+fncXR0FNPp9HffdzAY3Are3fcDTHxwy3g8Xt98EhG/O5E1TROnp6fx4sWLv3wv1xPmwcHBOnr3cT19QnbCB9/QNE3MZrPffH46ncbx8fGD3Tzy7NmzGA6H6xtfgPsTPrih67ro+z4Wi0Usl8v1pHXzppK2bePg4OCLG03+zGfyvmaxWMT5+Xmcn5/HcrmMwWAQfd/f+/3v7heyKrXWzS1WSt3kevBHXd9xefdGlcVisb4T8yHX/tb1ufvaxP7hvkopUWstD72OiQ9W+r6Pt2/ffvW5yWSys78J5fo6pejBFRMfAH8LJj4AeADCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0Aqjza52N7e3sdSyk+bXBOA3bC3t/dxE+uUWusm1gFuKKX8EhHHtdZftr0XyMapTgBSET4AUhE+AFIRPgBSET4AUhE+AFIRPgBSET4AUhE+AFIRPgBSET7gh1JKmZZS3pZSXpdSjkspg1LKePX9wY3jXm5xm2zRRn9JNcAGdLXWw1LKKCKWEbEfEaOIuFj93ZZSxhEx2N4W2SbhA3ZOKWUSEYerhx8i4mlEvK61Lmqt7er7o1prV0q5WB07i4j/ufE2/ab2y9+L8AE7p9a6uD5tWWttSinTiFiWUga11n71+DqAz2qts9Xxx6WULiK6iBiVUka11m4bPwPbI3zAj2QYV5PcKCIWq+9drCbEZUQ0qylwEE51piV8wC4blVKOI+IgIt7UWpuIiFrrq+sDbpz6jBvf61evISF3dQK77Po05fl19OD3CB+w687i6vre8bY3wm4QPmDnrK7ZHaz+XF/XG/hsHt/DNT5g59RaF/Hfm1cirk55Lr5xONxi4gMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhg+3434j4sO1NQEal1rrtPQDAxpj4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASOU/cOtlOs+04fsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAEFCAYAAADOqip4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAPzklEQVR4nO3db4xU53XH8XOAZTFgMizBMXGQy5BusOzY8TIYu1ETR15SqUKuZA/xqzT9oyyVqlaqKkGpLEV90VYgVa3UKhLbpopaNa3wvogUtU2y20huYtyKZdOkcuSYeBG1m6TYwDjG2LDA6Yt9xoyHuWd2Z/bO3D18P9KKmXvmzj1c8ePevc/cedTMBEBMK/rdAID8EHAgMAIOBEbAgcAIOBDYqn43EJmqVkVkSETOi0hNRMpmNp7zNkdF5KiZbV/g60dEpCIiO81sf569ofc4gudEVcsissvMxs1sQuZDXsp7u2Y2JSKzi1jlkIgcK2q4VfXlfvewnBHw/JRF5Fz9iZnNyOKC1yslM6v1uwnHzn43sJwR8PxMi8ghVT2QjuaSjuQiMn8qnX4Oq2qpYdkFVR1Jj4+qajk9P1p/n4bX3fQezVR1LL3mQPNr0un5UHpNWVWrqvpyev0zDX1V07Jq+hVgwb02bS+z71bbTv2dbFi/VR8te0ZiZvzk9CMiIyIyKSIm8/9QSw21o+nPURE53LB8UkRG0uPDInIg43Xvvl/azjON79Gw/HB6XKpvs6nHyebnab1yw3scaOy7YbsL6rXp/d2+G7fd4u/i9tG4Hj/zPxzBc2RmM2a2x8xURKZkPgT1WuPvvKWmVeun8ucaHp9v8f61+nZkPlTNnhKRc+lIWE4/7Qylvuvb3S8iMw31l5u2taBeF9h387YbeX14693SCHhO6qeQdWZ2UBoClk5PR8UJblJrri9CSURm0j/+GTPbs4B13HAmQ/UHS9jrQrfdqo/FrnfLIOD5KaVhMhERSb8bzqbHYyJyzuaveNfrI4vdQMPvr2WZP0No9oyI7Gl4/aK3kd6jcb1dGdtasAX03ZM+bgUEPGfpIlBVRMZE5GBaPCUi25uO8kP1U+mGC3N7RGRfCsR+ERltung1mt5jv4h8Pm2v/h5j6T+Q+gWom07hm7ZXSq+ppP+AROTdYbda/eKWzP8eP9tBr41a9X3Ttlv8XVr1cdN6uEHTRQosM6p60syW3RDScu17ueIIDgRGwJehdFpaXm6npcu17+WMU3QgMI7gQGAEHAgs99tFV+ugrZF1eW8GuKW9KRdeN7PNzcs7Cngah6zJAu5vXiPrZLc+1slmACzQlE2cabV80afo9U9n1T+F1eoDFACKoZPfwXfJjZsKZuW9Hx8UkXdvUZxW1ek5udxNfwC60EnAS03PNzW/wOa/xaRiZpUBGeyoMQDd6yTgNWm4mwhAcXUS8BNy4yhelvmb7QEU0KIDbvNfO1ROF9dKjbc8AiiWjobJzOxIeki4gQLjk2xAYAQcCIyAA4ERcCAwAg4ERsCBwAg4EBgBBwIj4EBgBBwIjIADgRFwIDACDgRGwIHACDgQGAEHAiPgQGAEHAiMgAOBEXAgMAIOBJb77KIonlVbP+TWf/CHH3Tr9+x41a3/2baJ7HVXr3XXfeHK2279j17d69Znnh/OrH3kiz921716uuX8fcsaR3AgMAIOBEbAgcAIOBAYAQcCI+BAYAQcCIxx8IAuPbHbrV8fe82tn75/vMsO/LFuz72rb3Prx8r/5tb/ZEP2GP3frH3UXXfHn/vHu2s/Ou3Wi4gjOBBYRwFX1QuqOqmqB5a6IQBLp9NT9H1mNrWknQBYcp2eopdUtZxVVNUxVZ1W1ek5udzhJgB0q9OAD4nIeVU92qpoZuNmVjGzyoAMdt4dgK50FPAU4JqI1FS1urQtAVgqiw54Ov0eyaMZAEurk4tsx0SkXD9ym1n2zb/oi9c/utKtP3nnKbd+za53tf2/f/POzNovr/PHkq+YufUPrVrv1vdu+F5m7ezDt7vrnhipuPX1y3AcfNEBT6fmM+mHcAMFxgddgMAIOBAYAQcCI+BAYAQcCIzbRZepVXdlf7XxO1uuuusOrXrLrX/t0ga3/nvfecqtb352dWbtK6f8r0U+d59/u+jmp/7Hrf/B3f+aWXti47S77j9Xdrn19cfcciFxBAcCI+BAYAQcCIyAA4ERcCAwAg4ERsCBwBgHX6Zsw7rM2qo3/NtFv/TDR9z63Ev+OPjwoefdejfu+L5/S+epT25362u3ZX9F2H0D/q2oes0tL0scwYHACDgQGAEHAiPgQGAEHAiMgAOBEXAgMMbBl6kLD27KrF27058u6u1X/bHmrc/1b0B4bufPu/Vfu++4W39ocKDzbX9gruN1i4ojOBAYAQcCI+BAYAQcCIyAA4ERcCAwAg4Exjh4QV3/xQfd+llnptvhrf/nrvvDuS1uXa+rW+/GpSd2u/Xbf+cVt/70+1/seNtz5o/vb3qu8zH0ouIIDgTWNuCqWlXVyRbLRlV1LL/WAHSrbcDNbKLxuapW0/Kp9Hw0n9YAdKuTU/RdIjKbHs+KyEjzC1R1TFWnVXV6TvzPRQPITycBLzU9v+muBzMbN7OKmVUGZLCjxgB0r5OA10RkaIn7AJCDTgJ+Qm4cxcsiMpn9UgD91HYcPF1Eq6hq1cwmzGxCVQ+k5aX6xTYsrTfKa/wXaPZ3fK9acd1dddeO0279xK+U3fqqT/rfq77i57LnH//sPc+663Yzzt3OA89/zq1v/VJ+3/feL20DngK8sWnZkfSQcAMFxgddgMAIOBAYAQcCI+BAYAQcCIzbRQvq8pB/y+YK5xt+733fT9x1H3/fjFt/Z4t/22R51RtufdvAereep8+/8vHM2m3f9L8uOiKO4EBgBBwIjIADgRFwIDACDgRGwIHACDgQGOPgffLO3ofc+qUPZN8OKiKiWy9l1n5z6Dl33eGBdW5dpN30wf0b53767Efd+re//kBm7e5xf+rhiDiCA4ERcCAwAg4ERsCBwAg4EBgBBwIj4EBgjIPn5OpjO936hWF/1w8O+/dc79v+3cxa+3Hu4nrhyttu/aWLd7j1LcevLGU7yx5HcCAwAg4ERsCBwAg4EBgBBwIj4EBgBBwIjHHwDq0c3u7WT+9Z7dbv+NhP3fqhD/+LW79rpTdOPuiu+5OrF916O69d9//ZvHK1lFlbo84XuovI82896Nb/6zvDbn3bN+JNAdyNtkdwVa2q6mTTsguqOqmqB/JrDUC3FjI/+ISq7m9avC/NGw6gwDr9HbykquUl7QTAkus04EMicl5Vj7YqquqYqk6r6vScXO68OwBd6SjgZjZuZjURqalqNaNeMbPKQJsLPgDys+iAp6PzSB7NAFhaC7mKPioilYYj9bG0vCoyfxEuv/YAdGMhV9GnRGRjw/OaiMykn9Dh1p33ZtZO/b4/h/bjH/lPt/7ZIX+8dmilP1784pWNmbWnz+5w1525sNWtX5rzx/BLg/4929cle27zhzeedtc9fs6/drvmrD9vOt6LT7IBgRFwIDACDgRGwIHACDgQGAEHAuN2Ucfskxsya3+9u+WndN/16G3X27y7/wm/f3jzg279C9OPZ9Y2fHuNu+6Wr51x64N33e7Wz1T83moj2V9d/OjDL7nrthui2/rVH7v1q2711sMRHAiMgAOBEXAgMAIOBEbAgcAIOBAYAQcCYxzc8bFPZI/Zth/n7s5fvvwpt77171Zm1la3+ergtmPF/+uPNctDv+CWP33/C5m1vbf/t7vu3158xK3ftv6aW8d7cQQHAiPgQGAEHAiMgAOBEXAgMAIOBEbAgcAYB3e8+Pod2cUuZ2a7bP7XIr99xf9a5o0/fSuz1u0IvT3ygFt/5HMzbv2Ld/2HU13nrrt2MPtechGR698/5dbxXhzBgcAIOBAYAQcCI+BAYAQcCIyAA4ERcCAwxsEdF2trc3vvQfXHuZ/Y9j23/uXf/nhmbcVbD7vrlj583q3/6T1fceufXuuP4Xfj4vHNbn2jMA6+GG7AVbUk8x/pKIvILjM7mJZXRaQmImUzG8+5RwAdaneK/hkRqZjZhIiIqo6lcIuZTaVlo/m2CKBTbsDNbLzhCF0WkVkR2ZX+lPTnSH7tAejGgi6yqWpZRM6no3apqbypxevHVHVaVafn5HL3XQLoyEKvolfNbH96XBORIe/F6chfMbPKQJtJ9gDkp23AVbVqZkfS4xEROSE3juJlEZnMrTsAXWl3FX1URA6r6qG06KCZTajqgVQr1S+2RXT3P2X///fSp7Jv1xQRGR7wb4ts5wubf+DX9/r1ovryz5xbcEVk6x8f71EntwY34Cm821ssP5Iehg03EAGfZAMCI+BAYAQcCIyAA4ERcCAwAg4Exu2ijtVfP5FZ+6Vv/a677m9V/t2tH9wU97bHXz3ziczauer6Nmu3mboYi8IRHAiMgAOBEXAgMAIOBEbAgcAIOBAYAQcCYxy8Q8O/ftKtf6vNNLnPDj/p1n/0G/5902t21DJrf3X/P7rr/sWre9z6+Xf83tcc9Ov23Rec6s/cdbG0OIIDgRFwIDACDgRGwIHACDgQGAEHAiPgQGBqZrluYIMO2W59LNdtALe6KZs4aWaV5uUcwYHACDgQGAEHAiPgQGAEHAiMgAOBEXAgMAIOBOYGXFVLqjqiqlVVPdyw/IKqTqrqgfxbBNCpdkfwz4hIxcwmRERUdSwt32dme8zsSK7dAeiK+5VNZjbe8LQsIpPpcUlVy2Y2m1tnALq2oN/BVbUsIufNbCotGhKR86p6NOP1Y6o6rarTc3J5iVoFsFgLvchWNbP99SdmNm5mNRGpqWq1+cWpXjGzyoAMLlGrABar7beqqmq1/ru2qo6ISEVEps1sJu/mAHSn3VX0URE5rKonVfWkzJ+aH0u1qohI/QIcgOJpd5FtSkS2tyjNpB/CDRQYH3QBAiPgQGAEHAiMgAOBEXAgMAIOBEbAgcAIOBAYAQcCI+BAYAQcCIyAA4ERcCAwAg4Elvv0war6moicaVj0fhF5PdeNdo7eOlPU3oral8jS93a3mW1uXph7wG/aoOp0q3mMi4DeOlPU3oral0jveuMUHQiMgAOB9SPg4+1f0jf01pmi9lbUvkR61FvPfwcH0DucogOBEXAgsJ4GPM1SOtowiWEhFHG21LSvJlss6/v+y+itr/vQmQm37/usn7P09izgDRMlTKXno73a9gIUbrbU5gklirT/Mia76Pc+vGkm3ALts77N0tvLI/guEanPRjorIiM93HY7pTTBYpEVef+J9Hkfpvnw6lemyzK/jwqxzzJ6E+nBPutlwEtNzzf1cNvtuLOlFkSp6XmR9p9IQfZh00y4paZyX/fZYmfpXQq9DHhN5v9ChdNuttSCqElB959IofZh40y4NSnWPlvULL1LoZcBPyE3/kcti8hk9kt7J/2uVrTT3VYKuf9EirMPW8yEW5h91txbr/ZZzwKeLjCU04WOUsNpSr8VcrbUtJ8qTX0VYv819yYF2IetZsItyj7r5yy9fJINCIwPugCBEXAgMAIOBEbAgcAIOBAYAQcCI+BAYP8Pr9/QoU3vFigAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(observations=None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    path.draw()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:categorical_bpl] *",
   "language": "python",
   "name": "conda-env-categorical_bpl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
