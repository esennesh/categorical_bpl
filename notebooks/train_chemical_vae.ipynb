{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eli/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [16:06:24] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='chemical_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-4,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 5,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer, log_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [256/225000 (0%)] Loss: 108654.578125\n",
      "Train Epoch: 1 [4352/225000 (2%)] Loss: 105890.843750\n",
      "Train Epoch: 1 [8448/225000 (4%)] Loss: 100840.898438\n",
      "Train Epoch: 1 [12544/225000 (6%)] Loss: 64374.320312\n",
      "Train Epoch: 1 [16640/225000 (7%)] Loss: 42894.789062\n",
      "Train Epoch: 1 [20736/225000 (9%)] Loss: 40233.292969\n",
      "Train Epoch: 1 [24832/225000 (11%)] Loss: 38380.078125\n",
      "Train Epoch: 1 [28928/225000 (13%)] Loss: 37625.941406\n",
      "Train Epoch: 1 [33024/225000 (15%)] Loss: 53789.648438\n",
      "Train Epoch: 1 [37120/225000 (16%)] Loss: 36654.996094\n",
      "Train Epoch: 1 [41216/225000 (18%)] Loss: 36137.359375\n",
      "Train Epoch: 1 [45312/225000 (20%)] Loss: 34716.187500\n",
      "Train Epoch: 1 [49408/225000 (22%)] Loss: 34661.859375\n",
      "Train Epoch: 1 [53504/225000 (24%)] Loss: 32724.808594\n",
      "Train Epoch: 1 [57600/225000 (26%)] Loss: 31868.669922\n",
      "Train Epoch: 1 [61696/225000 (27%)] Loss: 31352.904297\n",
      "Train Epoch: 1 [65792/225000 (29%)] Loss: 30721.220703\n",
      "Train Epoch: 1 [69888/225000 (31%)] Loss: 30691.523438\n",
      "Train Epoch: 1 [73984/225000 (33%)] Loss: 30135.056641\n",
      "Train Epoch: 1 [78080/225000 (35%)] Loss: 30464.908203\n",
      "Train Epoch: 1 [82176/225000 (37%)] Loss: 29784.488281\n",
      "Train Epoch: 1 [86272/225000 (38%)] Loss: 29731.208984\n",
      "Train Epoch: 1 [90368/225000 (40%)] Loss: 29672.671875\n",
      "Train Epoch: 1 [94464/225000 (42%)] Loss: 29284.251953\n",
      "Train Epoch: 1 [98560/225000 (44%)] Loss: 29231.066406\n",
      "Train Epoch: 1 [102656/225000 (46%)] Loss: 29067.832031\n",
      "Train Epoch: 1 [106752/225000 (47%)] Loss: 46662.953125\n",
      "Train Epoch: 1 [110848/225000 (49%)] Loss: 28782.306641\n",
      "Train Epoch: 1 [114944/225000 (51%)] Loss: 28562.328125\n",
      "Train Epoch: 1 [119040/225000 (53%)] Loss: 29167.720703\n",
      "Train Epoch: 1 [123136/225000 (55%)] Loss: 28860.597656\n",
      "Train Epoch: 1 [127232/225000 (57%)] Loss: 28494.753906\n",
      "Train Epoch: 1 [131328/225000 (58%)] Loss: 28194.322266\n",
      "Train Epoch: 1 [135424/225000 (60%)] Loss: 28228.578125\n",
      "Train Epoch: 1 [139520/225000 (62%)] Loss: 28332.714844\n",
      "Train Epoch: 1 [143616/225000 (64%)] Loss: 27937.394531\n",
      "Train Epoch: 1 [147712/225000 (66%)] Loss: 27275.000000\n",
      "Train Epoch: 1 [151808/225000 (67%)] Loss: 27414.832031\n",
      "Train Epoch: 1 [155904/225000 (69%)] Loss: 44701.429688\n",
      "Train Epoch: 1 [160000/225000 (71%)] Loss: 26987.816406\n",
      "Train Epoch: 1 [164096/225000 (73%)] Loss: 26940.464844\n",
      "Train Epoch: 1 [168192/225000 (75%)] Loss: 26918.800781\n",
      "Train Epoch: 1 [172288/225000 (77%)] Loss: 26653.416016\n",
      "Train Epoch: 1 [176384/225000 (78%)] Loss: 26354.509766\n",
      "Train Epoch: 1 [180480/225000 (80%)] Loss: 26671.679688\n",
      "Train Epoch: 1 [184576/225000 (82%)] Loss: 25845.705078\n",
      "Train Epoch: 1 [188672/225000 (84%)] Loss: 26443.150391\n",
      "Train Epoch: 1 [192768/225000 (86%)] Loss: 25971.349609\n",
      "Train Epoch: 1 [196864/225000 (87%)] Loss: 25682.757812\n",
      "Train Epoch: 1 [200960/225000 (89%)] Loss: 41928.644531\n",
      "Train Epoch: 1 [205056/225000 (91%)] Loss: 25347.841797\n",
      "Train Epoch: 1 [209152/225000 (93%)] Loss: 41149.664062\n",
      "Train Epoch: 1 [213248/225000 (95%)] Loss: 24667.867188\n",
      "Train Epoch: 1 [217344/225000 (97%)] Loss: 24321.771484\n",
      "Train Epoch: 1 [221440/225000 (98%)] Loss: 24529.265625\n",
      "    epoch          : 1\n",
      "    loss           : 34474.36414115828\n",
      "    val_loss       : 24704.45708253858\n",
      "Train Epoch: 2 [256/225000 (0%)] Loss: 24465.781250\n",
      "Train Epoch: 2 [4352/225000 (2%)] Loss: 23870.410156\n",
      "Train Epoch: 2 [8448/225000 (4%)] Loss: 23433.988281\n",
      "Train Epoch: 2 [12544/225000 (6%)] Loss: 23139.400391\n",
      "Train Epoch: 2 [16640/225000 (7%)] Loss: 23012.335938\n",
      "Train Epoch: 2 [20736/225000 (9%)] Loss: 22506.867188\n",
      "Train Epoch: 2 [24832/225000 (11%)] Loss: 22208.072266\n",
      "Train Epoch: 2 [28928/225000 (13%)] Loss: 22221.761719\n",
      "Train Epoch: 2 [33024/225000 (15%)] Loss: 21851.566406\n",
      "Train Epoch: 2 [37120/225000 (16%)] Loss: 21707.056641\n",
      "Train Epoch: 2 [41216/225000 (18%)] Loss: 21391.500000\n",
      "Train Epoch: 2 [45312/225000 (20%)] Loss: 21163.037109\n",
      "Train Epoch: 2 [49408/225000 (22%)] Loss: 20460.109375\n",
      "Train Epoch: 2 [53504/225000 (24%)] Loss: 20421.263672\n",
      "Train Epoch: 2 [57600/225000 (26%)] Loss: 20289.740234\n",
      "Train Epoch: 2 [61696/225000 (27%)] Loss: 20421.302734\n",
      "Train Epoch: 2 [65792/225000 (29%)] Loss: 20130.556641\n",
      "Train Epoch: 2 [69888/225000 (31%)] Loss: 19854.345703\n",
      "Train Epoch: 2 [73984/225000 (33%)] Loss: 19799.443359\n",
      "Train Epoch: 2 [78080/225000 (35%)] Loss: 19309.433594\n",
      "Train Epoch: 2 [82176/225000 (37%)] Loss: 19517.898438\n",
      "Train Epoch: 2 [86272/225000 (38%)] Loss: 19139.339844\n",
      "Train Epoch: 2 [90368/225000 (40%)] Loss: 18580.812500\n",
      "Train Epoch: 2 [94464/225000 (42%)] Loss: 19035.636719\n",
      "Train Epoch: 2 [98560/225000 (44%)] Loss: 18916.414062\n",
      "Train Epoch: 2 [102656/225000 (46%)] Loss: 18256.128906\n",
      "Train Epoch: 2 [106752/225000 (47%)] Loss: 18216.216797\n",
      "Train Epoch: 2 [110848/225000 (49%)] Loss: 18240.736328\n",
      "Train Epoch: 2 [114944/225000 (51%)] Loss: 18273.363281\n",
      "Train Epoch: 2 [119040/225000 (53%)] Loss: 17992.628906\n",
      "Train Epoch: 2 [123136/225000 (55%)] Loss: 17742.949219\n",
      "Train Epoch: 2 [127232/225000 (57%)] Loss: 18001.185547\n",
      "Train Epoch: 2 [131328/225000 (58%)] Loss: 17719.943359\n",
      "Train Epoch: 2 [135424/225000 (60%)] Loss: 17553.900391\n",
      "Train Epoch: 2 [139520/225000 (62%)] Loss: 17227.236328\n",
      "Train Epoch: 2 [143616/225000 (64%)] Loss: 17496.732422\n",
      "Train Epoch: 2 [147712/225000 (66%)] Loss: 17467.687500\n",
      "Train Epoch: 2 [151808/225000 (67%)] Loss: 17527.179688\n",
      "Train Epoch: 2 [155904/225000 (69%)] Loss: 17396.337891\n",
      "Train Epoch: 2 [160000/225000 (71%)] Loss: 16878.427734\n",
      "Train Epoch: 2 [164096/225000 (73%)] Loss: 16899.158203\n",
      "Train Epoch: 2 [168192/225000 (75%)] Loss: 17157.205078\n",
      "Train Epoch: 2 [172288/225000 (77%)] Loss: 16805.550781\n",
      "Train Epoch: 2 [176384/225000 (78%)] Loss: 16624.365234\n",
      "Train Epoch: 2 [180480/225000 (80%)] Loss: 16907.417969\n",
      "Train Epoch: 2 [184576/225000 (82%)] Loss: 16717.330078\n",
      "Train Epoch: 2 [188672/225000 (84%)] Loss: 16372.568359\n",
      "Train Epoch: 2 [192768/225000 (86%)] Loss: 16413.865234\n",
      "Train Epoch: 2 [196864/225000 (87%)] Loss: 16232.990234\n",
      "Train Epoch: 2 [200960/225000 (89%)] Loss: 22840.882812\n",
      "Train Epoch: 2 [205056/225000 (91%)] Loss: 16381.929688\n",
      "Train Epoch: 2 [209152/225000 (93%)] Loss: 16409.964844\n",
      "Train Epoch: 2 [213248/225000 (95%)] Loss: 16473.466797\n",
      "Train Epoch: 2 [217344/225000 (97%)] Loss: 15906.431641\n",
      "Train Epoch: 2 [221440/225000 (98%)] Loss: 16005.673828\n",
      "    epoch          : 2\n",
      "    loss           : 19550.220259838952\n",
      "    val_loss       : 17010.929408383614\n",
      "Train Epoch: 3 [256/225000 (0%)] Loss: 16092.669922\n",
      "Train Epoch: 3 [4352/225000 (2%)] Loss: 16131.433594\n",
      "Train Epoch: 3 [8448/225000 (4%)] Loss: 15766.226562\n",
      "Train Epoch: 3 [12544/225000 (6%)] Loss: 15845.310547\n",
      "Train Epoch: 3 [16640/225000 (7%)] Loss: 15943.208984\n",
      "Train Epoch: 3 [20736/225000 (9%)] Loss: 15773.611328\n",
      "Train Epoch: 3 [24832/225000 (11%)] Loss: 15796.355469\n",
      "Train Epoch: 3 [28928/225000 (13%)] Loss: 15995.511719\n",
      "Train Epoch: 3 [33024/225000 (15%)] Loss: 16007.023438\n",
      "Train Epoch: 3 [37120/225000 (16%)] Loss: 15697.066406\n",
      "Train Epoch: 3 [41216/225000 (18%)] Loss: 37305.511719\n",
      "Train Epoch: 3 [45312/225000 (20%)] Loss: 15795.972656\n",
      "Train Epoch: 3 [49408/225000 (22%)] Loss: 15892.847656\n",
      "Train Epoch: 3 [53504/225000 (24%)] Loss: 15473.777344\n",
      "Train Epoch: 3 [57600/225000 (26%)] Loss: 28732.333984\n",
      "Train Epoch: 3 [61696/225000 (27%)] Loss: 38647.691406\n",
      "Train Epoch: 3 [65792/225000 (29%)] Loss: 36848.835938\n",
      "Train Epoch: 3 [69888/225000 (31%)] Loss: 36543.390625\n",
      "Train Epoch: 3 [73984/225000 (33%)] Loss: 34461.648438\n",
      "Train Epoch: 3 [78080/225000 (35%)] Loss: 29222.082031\n",
      "Train Epoch: 3 [82176/225000 (37%)] Loss: 32910.640625\n",
      "Train Epoch: 3 [86272/225000 (38%)] Loss: 32675.126953\n",
      "Train Epoch: 3 [90368/225000 (40%)] Loss: 27551.195312\n",
      "Train Epoch: 3 [94464/225000 (42%)] Loss: 26900.652344\n",
      "Train Epoch: 3 [98560/225000 (44%)] Loss: 26944.312500\n",
      "Train Epoch: 3 [102656/225000 (46%)] Loss: 18961.515625\n",
      "Train Epoch: 3 [106752/225000 (47%)] Loss: 29726.480469\n",
      "Train Epoch: 3 [110848/225000 (49%)] Loss: 26135.449219\n",
      "Train Epoch: 3 [114944/225000 (51%)] Loss: 29943.789062\n",
      "Train Epoch: 3 [119040/225000 (53%)] Loss: 29794.083984\n",
      "Train Epoch: 3 [123136/225000 (55%)] Loss: 29095.621094\n",
      "Train Epoch: 3 [127232/225000 (57%)] Loss: 29266.966797\n",
      "Train Epoch: 3 [131328/225000 (58%)] Loss: 25550.236328\n",
      "Train Epoch: 3 [135424/225000 (60%)] Loss: 25117.009766\n",
      "Train Epoch: 3 [139520/225000 (62%)] Loss: 28721.425781\n",
      "Train Epoch: 3 [143616/225000 (64%)] Loss: 28262.478516\n",
      "Train Epoch: 3 [147712/225000 (66%)] Loss: 28369.412109\n",
      "Train Epoch: 3 [151808/225000 (67%)] Loss: 27530.835938\n",
      "Train Epoch: 3 [155904/225000 (69%)] Loss: 28880.486328\n",
      "Train Epoch: 3 [160000/225000 (71%)] Loss: 24406.519531\n",
      "Train Epoch: 3 [164096/225000 (73%)] Loss: 27390.953125\n",
      "Train Epoch: 3 [168192/225000 (75%)] Loss: 24530.224609\n",
      "Train Epoch: 3 [172288/225000 (77%)] Loss: 27383.222656\n",
      "Train Epoch: 3 [176384/225000 (78%)] Loss: 26592.123047\n",
      "Train Epoch: 3 [180480/225000 (80%)] Loss: 24169.029297\n",
      "Train Epoch: 3 [184576/225000 (82%)] Loss: 23660.830078\n",
      "Train Epoch: 3 [188672/225000 (84%)] Loss: 26320.878906\n",
      "Train Epoch: 3 [192768/225000 (86%)] Loss: 23268.693359\n",
      "Train Epoch: 3 [196864/225000 (87%)] Loss: 23365.662109\n",
      "Train Epoch: 3 [200960/225000 (89%)] Loss: 26065.064453\n",
      "Train Epoch: 3 [205056/225000 (91%)] Loss: 23236.294922\n",
      "Train Epoch: 3 [209152/225000 (93%)] Loss: 22606.656250\n",
      "Train Epoch: 3 [213248/225000 (95%)] Loss: 22934.558594\n",
      "Train Epoch: 3 [217344/225000 (97%)] Loss: 22302.183594\n",
      "Train Epoch: 3 [221440/225000 (98%)] Loss: 24790.525391\n",
      "    epoch          : 3\n",
      "    loss           : 25466.486308127132\n",
      "    val_loss       : 23565.813752572147\n",
      "Train Epoch: 4 [256/225000 (0%)] Loss: 24559.013672\n",
      "Train Epoch: 4 [4352/225000 (2%)] Loss: 19987.869141\n",
      "Train Epoch: 4 [8448/225000 (4%)] Loss: 24211.656250\n",
      "Train Epoch: 4 [12544/225000 (6%)] Loss: 21342.882812\n",
      "Train Epoch: 4 [16640/225000 (7%)] Loss: 19056.505859\n",
      "Train Epoch: 4 [20736/225000 (9%)] Loss: 22500.531250\n",
      "Train Epoch: 4 [24832/225000 (11%)] Loss: 22416.798828\n",
      "Train Epoch: 4 [28928/225000 (13%)] Loss: 18847.427734\n",
      "Train Epoch: 4 [33024/225000 (15%)] Loss: 22230.884766\n",
      "Train Epoch: 4 [37120/225000 (16%)] Loss: 21752.232422\n",
      "Train Epoch: 4 [41216/225000 (18%)] Loss: 21365.720703\n",
      "Train Epoch: 4 [45312/225000 (20%)] Loss: 21199.236328\n",
      "Train Epoch: 4 [49408/225000 (22%)] Loss: 19334.572266\n",
      "Train Epoch: 4 [53504/225000 (24%)] Loss: 18029.119141\n",
      "Train Epoch: 4 [57600/225000 (26%)] Loss: 20297.939453\n",
      "Train Epoch: 4 [61696/225000 (27%)] Loss: 20007.408203\n",
      "Train Epoch: 4 [65792/225000 (29%)] Loss: 18877.388672\n",
      "Train Epoch: 4 [69888/225000 (31%)] Loss: 19279.892578\n",
      "Train Epoch: 4 [73984/225000 (33%)] Loss: 19288.414062\n",
      "Train Epoch: 4 [78080/225000 (35%)] Loss: 19169.519531\n",
      "Train Epoch: 4 [82176/225000 (37%)] Loss: 19407.337891\n",
      "Train Epoch: 4 [86272/225000 (38%)] Loss: 18139.281250\n",
      "Train Epoch: 4 [90368/225000 (40%)] Loss: 18963.070312\n",
      "Train Epoch: 4 [94464/225000 (42%)] Loss: 16669.666016\n",
      "Train Epoch: 4 [98560/225000 (44%)] Loss: 17892.263672\n",
      "Train Epoch: 4 [102656/225000 (46%)] Loss: 18181.414062\n",
      "Train Epoch: 4 [106752/225000 (47%)] Loss: 18037.724609\n",
      "Train Epoch: 4 [110848/225000 (49%)] Loss: 17388.341797\n",
      "Train Epoch: 4 [114944/225000 (51%)] Loss: 17721.466797\n",
      "Train Epoch: 4 [119040/225000 (53%)] Loss: 17611.169922\n",
      "Train Epoch: 4 [123136/225000 (55%)] Loss: 17090.693359\n",
      "Train Epoch: 4 [127232/225000 (57%)] Loss: 17852.005859\n",
      "Train Epoch: 4 [131328/225000 (58%)] Loss: 16943.607422\n",
      "Train Epoch: 4 [135424/225000 (60%)] Loss: 16885.453125\n",
      "Train Epoch: 4 [139520/225000 (62%)] Loss: 17073.138672\n",
      "Train Epoch: 4 [143616/225000 (64%)] Loss: 16252.599609\n",
      "Train Epoch: 4 [147712/225000 (66%)] Loss: 15877.634766\n",
      "Train Epoch: 4 [151808/225000 (67%)] Loss: 17389.125000\n",
      "Train Epoch: 4 [155904/225000 (69%)] Loss: 16675.050781\n",
      "Train Epoch: 4 [160000/225000 (71%)] Loss: 16540.369141\n",
      "Train Epoch: 4 [164096/225000 (73%)] Loss: 16814.439453\n",
      "Train Epoch: 4 [168192/225000 (75%)] Loss: 15812.871094\n",
      "Train Epoch: 4 [172288/225000 (77%)] Loss: 16451.845703\n",
      "Train Epoch: 4 [176384/225000 (78%)] Loss: 16189.343750\n",
      "Train Epoch: 4 [180480/225000 (80%)] Loss: 16451.509766\n",
      "Train Epoch: 4 [184576/225000 (82%)] Loss: 16134.943359\n",
      "Train Epoch: 4 [188672/225000 (84%)] Loss: 16083.513672\n",
      "Train Epoch: 4 [192768/225000 (86%)] Loss: 16046.863281\n",
      "Train Epoch: 4 [196864/225000 (87%)] Loss: 16116.861328\n",
      "Train Epoch: 4 [200960/225000 (89%)] Loss: 16247.126953\n",
      "Train Epoch: 4 [205056/225000 (91%)] Loss: 15946.199219\n",
      "Train Epoch: 4 [209152/225000 (93%)] Loss: 16016.238281\n",
      "Train Epoch: 4 [213248/225000 (95%)] Loss: 15585.429688\n",
      "Train Epoch: 4 [217344/225000 (97%)] Loss: 15768.517578\n",
      "Train Epoch: 4 [221440/225000 (98%)] Loss: 15958.445312\n",
      "    epoch          : 4\n",
      "    loss           : 18847.257210342006\n",
      "    val_loss       : 17019.69024393753\n",
      "Train Epoch: 5 [256/225000 (0%)] Loss: 16053.406250\n",
      "Train Epoch: 5 [4352/225000 (2%)] Loss: 15603.289062\n",
      "Train Epoch: 5 [8448/225000 (4%)] Loss: 15795.681641\n",
      "Train Epoch: 5 [12544/225000 (6%)] Loss: 22407.009766\n",
      "Train Epoch: 5 [16640/225000 (7%)] Loss: 15408.208984\n",
      "Train Epoch: 5 [20736/225000 (9%)] Loss: 15385.560547\n",
      "Train Epoch: 5 [24832/225000 (11%)] Loss: 15688.398438\n",
      "Train Epoch: 5 [28928/225000 (13%)] Loss: 15763.199219\n",
      "Train Epoch: 5 [33024/225000 (15%)] Loss: 15265.365234\n",
      "Train Epoch: 5 [37120/225000 (16%)] Loss: 15312.982422\n",
      "Train Epoch: 5 [41216/225000 (18%)] Loss: 15704.263672\n",
      "Train Epoch: 5 [45312/225000 (20%)] Loss: 15228.414062\n",
      "Train Epoch: 5 [49408/225000 (22%)] Loss: 15372.462891\n",
      "Train Epoch: 5 [53504/225000 (24%)] Loss: 15171.890625\n",
      "Train Epoch: 5 [57600/225000 (26%)] Loss: 15198.275391\n",
      "Train Epoch: 5 [61696/225000 (27%)] Loss: 37727.183594\n",
      "Train Epoch: 5 [65792/225000 (29%)] Loss: 36800.351562\n",
      "Train Epoch: 5 [69888/225000 (31%)] Loss: 15207.853516\n",
      "Train Epoch: 5 [73984/225000 (33%)] Loss: 15096.533203\n",
      "Train Epoch: 5 [78080/225000 (35%)] Loss: 15118.316406\n",
      "Train Epoch: 5 [82176/225000 (37%)] Loss: 15140.539062\n",
      "Train Epoch: 5 [86272/225000 (38%)] Loss: 14890.742188\n",
      "Train Epoch: 5 [90368/225000 (40%)] Loss: 14793.271484\n",
      "Train Epoch: 5 [94464/225000 (42%)] Loss: 14722.277344\n",
      "Train Epoch: 5 [98560/225000 (44%)] Loss: 14970.910156\n",
      "Train Epoch: 5 [102656/225000 (46%)] Loss: 14547.064453\n",
      "Train Epoch: 5 [106752/225000 (47%)] Loss: 14305.443359\n",
      "Train Epoch: 5 [110848/225000 (49%)] Loss: 14902.234375\n",
      "Train Epoch: 5 [114944/225000 (51%)] Loss: 14660.025391\n",
      "Train Epoch: 5 [119040/225000 (53%)] Loss: 14503.158203\n",
      "Train Epoch: 5 [123136/225000 (55%)] Loss: 14530.486328\n",
      "Train Epoch: 5 [127232/225000 (57%)] Loss: 14179.199219\n",
      "Train Epoch: 5 [131328/225000 (58%)] Loss: 14474.076172\n",
      "Train Epoch: 5 [135424/225000 (60%)] Loss: 14125.468750\n",
      "Train Epoch: 5 [139520/225000 (62%)] Loss: 14627.882812\n",
      "Train Epoch: 5 [143616/225000 (64%)] Loss: 14595.208984\n",
      "Train Epoch: 5 [147712/225000 (66%)] Loss: 14399.619141\n",
      "Train Epoch: 5 [151808/225000 (67%)] Loss: 14364.244141\n",
      "Train Epoch: 5 [155904/225000 (69%)] Loss: 14113.451172\n",
      "Train Epoch: 5 [160000/225000 (71%)] Loss: 14159.453125\n",
      "Train Epoch: 5 [164096/225000 (73%)] Loss: 14205.064453\n",
      "Train Epoch: 5 [168192/225000 (75%)] Loss: 13863.496094\n",
      "Train Epoch: 5 [172288/225000 (77%)] Loss: 14193.646484\n",
      "Train Epoch: 5 [176384/225000 (78%)] Loss: 14463.064453\n",
      "Train Epoch: 5 [180480/225000 (80%)] Loss: 14441.304688\n",
      "Train Epoch: 5 [184576/225000 (82%)] Loss: 14512.558594\n",
      "Train Epoch: 5 [188672/225000 (84%)] Loss: 14052.482422\n",
      "Train Epoch: 5 [192768/225000 (86%)] Loss: 13753.763672\n",
      "Train Epoch: 5 [196864/225000 (87%)] Loss: 14359.390625\n",
      "Train Epoch: 5 [200960/225000 (89%)] Loss: 14146.673828\n",
      "Train Epoch: 5 [205056/225000 (91%)] Loss: 13992.662109\n",
      "Train Epoch: 5 [209152/225000 (93%)] Loss: 13796.226562\n",
      "Train Epoch: 5 [213248/225000 (95%)] Loss: 13963.171875\n",
      "Train Epoch: 5 [217344/225000 (97%)] Loss: 13992.693359\n",
      "Train Epoch: 5 [221440/225000 (98%)] Loss: 13865.630859\n",
      "    epoch          : 5\n",
      "    loss           : 15176.622099198308\n",
      "    val_loss       : 14046.272450419105\n",
      "Train Epoch: 6 [256/225000 (0%)] Loss: 13999.378906\n",
      "Train Epoch: 6 [4352/225000 (2%)] Loss: 13645.714844\n",
      "Train Epoch: 6 [8448/225000 (4%)] Loss: 14010.136719\n",
      "Train Epoch: 6 [12544/225000 (6%)] Loss: 13925.439453\n",
      "Train Epoch: 6 [16640/225000 (7%)] Loss: 13839.964844\n",
      "Train Epoch: 6 [20736/225000 (9%)] Loss: 14068.037109\n",
      "Train Epoch: 6 [24832/225000 (11%)] Loss: 13409.277344\n",
      "Train Epoch: 6 [28928/225000 (13%)] Loss: 13967.449219\n",
      "Train Epoch: 6 [33024/225000 (15%)] Loss: 13718.416016\n",
      "Train Epoch: 6 [37120/225000 (16%)] Loss: 13670.027344\n",
      "Train Epoch: 6 [41216/225000 (18%)] Loss: 13976.376953\n",
      "Train Epoch: 6 [45312/225000 (20%)] Loss: 13962.171875\n",
      "Train Epoch: 6 [49408/225000 (22%)] Loss: 14263.187500\n",
      "Train Epoch: 6 [53504/225000 (24%)] Loss: 13449.521484\n",
      "Train Epoch: 6 [57600/225000 (26%)] Loss: 14042.472656\n",
      "Train Epoch: 6 [61696/225000 (27%)] Loss: 13162.169922\n",
      "Train Epoch: 6 [65792/225000 (29%)] Loss: 13419.310547\n",
      "Train Epoch: 6 [69888/225000 (31%)] Loss: 13842.349609\n",
      "Train Epoch: 6 [73984/225000 (33%)] Loss: 13612.265625\n",
      "Train Epoch: 6 [78080/225000 (35%)] Loss: 13454.126953\n",
      "Train Epoch: 6 [82176/225000 (37%)] Loss: 13231.146484\n",
      "Train Epoch: 6 [86272/225000 (38%)] Loss: 13527.910156\n",
      "Train Epoch: 6 [90368/225000 (40%)] Loss: 12869.123047\n",
      "Train Epoch: 6 [94464/225000 (42%)] Loss: 13174.912109\n",
      "Train Epoch: 6 [98560/225000 (44%)] Loss: 13603.515625\n",
      "Train Epoch: 6 [102656/225000 (46%)] Loss: 13142.220703\n",
      "Train Epoch: 6 [106752/225000 (47%)] Loss: 12992.859375\n",
      "Train Epoch: 6 [110848/225000 (49%)] Loss: 13589.583984\n",
      "Train Epoch: 6 [114944/225000 (51%)] Loss: 13301.224609\n",
      "Train Epoch: 6 [119040/225000 (53%)] Loss: 13250.269531\n",
      "Train Epoch: 6 [123136/225000 (55%)] Loss: 13432.195312\n",
      "Train Epoch: 6 [127232/225000 (57%)] Loss: 12796.085938\n",
      "Train Epoch: 6 [131328/225000 (58%)] Loss: 13755.322266\n",
      "Train Epoch: 6 [135424/225000 (60%)] Loss: 12972.099609\n",
      "Train Epoch: 6 [139520/225000 (62%)] Loss: 13634.240234\n",
      "Train Epoch: 6 [143616/225000 (64%)] Loss: 13536.224609\n",
      "Train Epoch: 6 [147712/225000 (66%)] Loss: 13266.876953\n",
      "Train Epoch: 6 [151808/225000 (67%)] Loss: 13016.011719\n",
      "Train Epoch: 6 [155904/225000 (69%)] Loss: 12835.578125\n",
      "Train Epoch: 6 [160000/225000 (71%)] Loss: 13310.390625\n",
      "Train Epoch: 6 [164096/225000 (73%)] Loss: 12803.236328\n",
      "Train Epoch: 6 [168192/225000 (75%)] Loss: 13406.291016\n",
      "Train Epoch: 6 [172288/225000 (77%)] Loss: 13716.941406\n",
      "Train Epoch: 6 [176384/225000 (78%)] Loss: 13630.384766\n",
      "Train Epoch: 6 [180480/225000 (80%)] Loss: 13129.775391\n",
      "Train Epoch: 6 [184576/225000 (82%)] Loss: 13035.791016\n",
      "Train Epoch: 6 [188672/225000 (84%)] Loss: 13250.951172\n",
      "Train Epoch: 6 [192768/225000 (86%)] Loss: 13505.367188\n",
      "Train Epoch: 6 [196864/225000 (87%)] Loss: 13054.421875\n",
      "Train Epoch: 6 [200960/225000 (89%)] Loss: 12863.222656\n",
      "Train Epoch: 6 [205056/225000 (91%)] Loss: 12779.763672\n",
      "Train Epoch: 6 [209152/225000 (93%)] Loss: 12752.232422\n",
      "Train Epoch: 6 [213248/225000 (95%)] Loss: 12672.128906\n",
      "Train Epoch: 6 [217344/225000 (97%)] Loss: 12597.744141\n",
      "Train Epoch: 6 [221440/225000 (98%)] Loss: 12575.333984\n",
      "    epoch          : 6\n",
      "    loss           : 13573.517992525241\n",
      "    val_loss       : 12869.604916626093\n",
      "Train Epoch: 7 [256/225000 (0%)] Loss: 12658.699219\n",
      "Train Epoch: 7 [4352/225000 (2%)] Loss: 12761.527344\n",
      "Train Epoch: 7 [8448/225000 (4%)] Loss: 12601.550781\n",
      "Train Epoch: 7 [12544/225000 (6%)] Loss: 13130.181641\n",
      "Train Epoch: 7 [16640/225000 (7%)] Loss: 12643.726562\n",
      "Train Epoch: 7 [20736/225000 (9%)] Loss: 12534.568359\n",
      "Train Epoch: 7 [24832/225000 (11%)] Loss: 12338.566406\n",
      "Train Epoch: 7 [28928/225000 (13%)] Loss: 12826.789062\n",
      "Train Epoch: 7 [33024/225000 (15%)] Loss: 12572.583984\n",
      "Train Epoch: 7 [37120/225000 (16%)] Loss: 12456.470703\n",
      "Train Epoch: 7 [41216/225000 (18%)] Loss: 12510.468750\n",
      "Train Epoch: 7 [45312/225000 (20%)] Loss: 12405.677734\n",
      "Train Epoch: 7 [49408/225000 (22%)] Loss: 12410.007812\n",
      "Train Epoch: 7 [53504/225000 (24%)] Loss: 12305.248047\n",
      "Train Epoch: 7 [57600/225000 (26%)] Loss: 12441.128906\n",
      "Train Epoch: 7 [61696/225000 (27%)] Loss: 12190.880859\n",
      "Train Epoch: 7 [65792/225000 (29%)] Loss: 12483.392578\n",
      "Train Epoch: 7 [69888/225000 (31%)] Loss: 12227.960938\n",
      "Train Epoch: 7 [73984/225000 (33%)] Loss: 12332.585938\n",
      "Train Epoch: 7 [78080/225000 (35%)] Loss: 12347.003906\n",
      "Train Epoch: 7 [82176/225000 (37%)] Loss: 12923.447266\n",
      "Train Epoch: 7 [86272/225000 (38%)] Loss: 12496.556641\n",
      "Train Epoch: 7 [90368/225000 (40%)] Loss: 12461.083984\n",
      "Train Epoch: 7 [94464/225000 (42%)] Loss: 11999.421875\n",
      "Train Epoch: 7 [98560/225000 (44%)] Loss: 12503.291016\n",
      "Train Epoch: 7 [102656/225000 (46%)] Loss: 12115.974609\n",
      "Train Epoch: 7 [106752/225000 (47%)] Loss: 12448.675781\n",
      "Train Epoch: 7 [110848/225000 (49%)] Loss: 12105.972656\n",
      "Train Epoch: 7 [114944/225000 (51%)] Loss: 12249.349609\n",
      "Train Epoch: 7 [119040/225000 (53%)] Loss: 12104.482422\n",
      "Train Epoch: 7 [123136/225000 (55%)] Loss: 12146.681641\n",
      "Train Epoch: 7 [127232/225000 (57%)] Loss: 12277.542969\n",
      "Train Epoch: 7 [131328/225000 (58%)] Loss: 12185.281250\n",
      "Train Epoch: 7 [135424/225000 (60%)] Loss: 12391.675781\n",
      "Train Epoch: 7 [139520/225000 (62%)] Loss: 12264.804688\n",
      "Train Epoch: 7 [143616/225000 (64%)] Loss: 12334.927734\n",
      "Train Epoch: 7 [147712/225000 (66%)] Loss: 12328.882812\n",
      "Train Epoch: 7 [151808/225000 (67%)] Loss: 12292.972656\n",
      "Train Epoch: 7 [155904/225000 (69%)] Loss: 12053.537109\n",
      "Train Epoch: 7 [160000/225000 (71%)] Loss: 12299.865234\n",
      "Train Epoch: 7 [164096/225000 (73%)] Loss: 12260.529297\n",
      "Train Epoch: 7 [168192/225000 (75%)] Loss: 12077.054688\n",
      "Train Epoch: 7 [172288/225000 (77%)] Loss: 11875.089844\n",
      "Train Epoch: 7 [176384/225000 (78%)] Loss: 11990.554688\n",
      "Train Epoch: 7 [180480/225000 (80%)] Loss: 12185.962891\n",
      "Train Epoch: 7 [184576/225000 (82%)] Loss: 12169.341797\n",
      "Train Epoch: 7 [188672/225000 (84%)] Loss: 12101.185547\n",
      "Train Epoch: 7 [192768/225000 (86%)] Loss: 12072.601562\n",
      "Train Epoch: 7 [196864/225000 (87%)] Loss: 11862.867188\n",
      "Train Epoch: 7 [200960/225000 (89%)] Loss: 12017.451172\n",
      "Train Epoch: 7 [205056/225000 (91%)] Loss: 12072.445312\n",
      "Train Epoch: 7 [209152/225000 (93%)] Loss: 12248.154297\n",
      "Train Epoch: 7 [213248/225000 (95%)] Loss: 12098.103516\n",
      "Train Epoch: 7 [217344/225000 (97%)] Loss: 12052.359375\n",
      "Train Epoch: 7 [221440/225000 (98%)] Loss: 12014.091797\n",
      "    epoch          : 7\n",
      "    loss           : 12306.702548394838\n",
      "    val_loss       : 12182.89296761581\n",
      "Train Epoch: 8 [256/225000 (0%)] Loss: 11938.718750\n",
      "Train Epoch: 8 [4352/225000 (2%)] Loss: 11925.179688\n",
      "Train Epoch: 8 [8448/225000 (4%)] Loss: 11987.929688\n",
      "Train Epoch: 8 [12544/225000 (6%)] Loss: 12065.591797\n",
      "Train Epoch: 8 [16640/225000 (7%)] Loss: 11866.507812\n",
      "Train Epoch: 8 [20736/225000 (9%)] Loss: 11608.308594\n",
      "Train Epoch: 8 [24832/225000 (11%)] Loss: 11893.759766\n",
      "Train Epoch: 8 [28928/225000 (13%)] Loss: 11706.818359\n",
      "Train Epoch: 8 [33024/225000 (15%)] Loss: 11906.447266\n",
      "Train Epoch: 8 [37120/225000 (16%)] Loss: 11710.164062\n",
      "Train Epoch: 8 [41216/225000 (18%)] Loss: 11846.238281\n",
      "Train Epoch: 8 [45312/225000 (20%)] Loss: 11898.279297\n",
      "Train Epoch: 8 [49408/225000 (22%)] Loss: 11903.382812\n",
      "Train Epoch: 8 [53504/225000 (24%)] Loss: 11924.650391\n",
      "Train Epoch: 8 [57600/225000 (26%)] Loss: 11849.046875\n",
      "Train Epoch: 8 [61696/225000 (27%)] Loss: 11987.496094\n",
      "Train Epoch: 8 [65792/225000 (29%)] Loss: 11642.503906\n",
      "Train Epoch: 8 [69888/225000 (31%)] Loss: 11966.203125\n",
      "Train Epoch: 8 [73984/225000 (33%)] Loss: 11760.505859\n",
      "Train Epoch: 8 [78080/225000 (35%)] Loss: 11828.177734\n",
      "Train Epoch: 8 [82176/225000 (37%)] Loss: 11986.951172\n",
      "Train Epoch: 8 [86272/225000 (38%)] Loss: 11753.113281\n",
      "Train Epoch: 8 [90368/225000 (40%)] Loss: 11785.537109\n",
      "Train Epoch: 8 [94464/225000 (42%)] Loss: 11831.662109\n",
      "Train Epoch: 8 [98560/225000 (44%)] Loss: 11918.187500\n",
      "Train Epoch: 8 [102656/225000 (46%)] Loss: 11731.714844\n",
      "Train Epoch: 8 [106752/225000 (47%)] Loss: 11702.447266\n",
      "Train Epoch: 8 [110848/225000 (49%)] Loss: 11637.857422\n",
      "Train Epoch: 8 [114944/225000 (51%)] Loss: 12147.501953\n",
      "Train Epoch: 8 [119040/225000 (53%)] Loss: 11637.039062\n",
      "Train Epoch: 8 [123136/225000 (55%)] Loss: 11923.416016\n",
      "Train Epoch: 8 [127232/225000 (57%)] Loss: 11785.779297\n",
      "Train Epoch: 8 [131328/225000 (58%)] Loss: 11654.238281\n",
      "Train Epoch: 8 [135424/225000 (60%)] Loss: 12024.228516\n",
      "Train Epoch: 8 [139520/225000 (62%)] Loss: 11677.330078\n",
      "Train Epoch: 8 [143616/225000 (64%)] Loss: 11543.683594\n",
      "Train Epoch: 8 [147712/225000 (66%)] Loss: 11536.224609\n",
      "Train Epoch: 8 [151808/225000 (67%)] Loss: 11530.419922\n",
      "Train Epoch: 8 [155904/225000 (69%)] Loss: 11642.152344\n",
      "Train Epoch: 8 [160000/225000 (71%)] Loss: 11543.865234\n",
      "Train Epoch: 8 [164096/225000 (73%)] Loss: 11426.722656\n",
      "Train Epoch: 8 [168192/225000 (75%)] Loss: 11408.167969\n",
      "Train Epoch: 8 [172288/225000 (77%)] Loss: 11923.033203\n",
      "Train Epoch: 8 [176384/225000 (78%)] Loss: 11450.738281\n",
      "Train Epoch: 8 [180480/225000 (80%)] Loss: 11552.640625\n",
      "Train Epoch: 8 [184576/225000 (82%)] Loss: 11577.558594\n",
      "Train Epoch: 8 [188672/225000 (84%)] Loss: 11703.546875\n",
      "Train Epoch: 8 [192768/225000 (86%)] Loss: 11526.339844\n",
      "Train Epoch: 8 [196864/225000 (87%)] Loss: 11388.498047\n",
      "Train Epoch: 8 [200960/225000 (89%)] Loss: 11568.880859\n",
      "Train Epoch: 8 [205056/225000 (91%)] Loss: 11460.876953\n",
      "Train Epoch: 8 [209152/225000 (93%)] Loss: 11580.923828\n",
      "Train Epoch: 8 [213248/225000 (95%)] Loss: 11812.623047\n",
      "Train Epoch: 8 [217344/225000 (97%)] Loss: 11613.125000\n",
      "Train Epoch: 8 [221440/225000 (98%)] Loss: 11554.033203\n",
      "    epoch          : 8\n",
      "    loss           : 11791.60820623578\n",
      "    val_loss       : 11427.136046623089\n",
      "Train Epoch: 9 [256/225000 (0%)] Loss: 11273.093750\n",
      "Train Epoch: 9 [4352/225000 (2%)] Loss: 11525.712891\n",
      "Train Epoch: 9 [8448/225000 (4%)] Loss: 11383.384766\n",
      "Train Epoch: 9 [12544/225000 (6%)] Loss: 11542.763672\n",
      "Train Epoch: 9 [16640/225000 (7%)] Loss: 11695.070312\n",
      "Train Epoch: 9 [20736/225000 (9%)] Loss: 11220.718750\n",
      "Train Epoch: 9 [24832/225000 (11%)] Loss: 11590.095703\n",
      "Train Epoch: 9 [28928/225000 (13%)] Loss: 11360.019531\n",
      "Train Epoch: 9 [33024/225000 (15%)] Loss: 11506.160156\n",
      "Train Epoch: 9 [37120/225000 (16%)] Loss: 11441.714844\n",
      "Train Epoch: 9 [41216/225000 (18%)] Loss: 11545.205078\n",
      "Train Epoch: 9 [45312/225000 (20%)] Loss: 11441.919922\n",
      "Train Epoch: 9 [49408/225000 (22%)] Loss: 11319.259766\n",
      "Train Epoch: 9 [53504/225000 (24%)] Loss: 11258.490234\n",
      "Train Epoch: 9 [57600/225000 (26%)] Loss: 11270.470703\n",
      "Train Epoch: 9 [61696/225000 (27%)] Loss: 11366.353516\n",
      "Train Epoch: 9 [65792/225000 (29%)] Loss: 11269.343750\n",
      "Train Epoch: 9 [69888/225000 (31%)] Loss: 11249.550781\n",
      "Train Epoch: 9 [73984/225000 (33%)] Loss: 11455.419922\n",
      "Train Epoch: 9 [78080/225000 (35%)] Loss: 11465.925781\n",
      "Train Epoch: 9 [82176/225000 (37%)] Loss: 11322.652344\n",
      "Train Epoch: 9 [86272/225000 (38%)] Loss: 11559.642578\n",
      "Train Epoch: 9 [90368/225000 (40%)] Loss: 11147.667969\n",
      "Train Epoch: 9 [94464/225000 (42%)] Loss: 10995.878906\n",
      "Train Epoch: 9 [98560/225000 (44%)] Loss: 11500.414062\n",
      "Train Epoch: 9 [102656/225000 (46%)] Loss: 11432.525391\n",
      "Train Epoch: 9 [106752/225000 (47%)] Loss: 11359.328125\n",
      "Train Epoch: 9 [110848/225000 (49%)] Loss: 11400.404297\n",
      "Train Epoch: 9 [114944/225000 (51%)] Loss: 11388.310547\n",
      "Train Epoch: 9 [119040/225000 (53%)] Loss: 11182.705078\n",
      "Train Epoch: 9 [123136/225000 (55%)] Loss: 11268.691406\n",
      "Train Epoch: 9 [127232/225000 (57%)] Loss: 11395.601562\n",
      "Train Epoch: 9 [131328/225000 (58%)] Loss: 11093.599609\n",
      "Train Epoch: 9 [135424/225000 (60%)] Loss: 10950.896484\n",
      "Train Epoch: 9 [139520/225000 (62%)] Loss: 11407.613281\n",
      "Train Epoch: 9 [143616/225000 (64%)] Loss: 11178.316406\n",
      "Train Epoch: 9 [147712/225000 (66%)] Loss: 11005.400391\n",
      "Train Epoch: 9 [151808/225000 (67%)] Loss: 11155.652344\n",
      "Train Epoch: 9 [155904/225000 (69%)] Loss: 11229.068359\n",
      "Train Epoch: 9 [160000/225000 (71%)] Loss: 11190.160156\n",
      "Train Epoch: 9 [164096/225000 (73%)] Loss: 11335.021484\n",
      "Train Epoch: 9 [168192/225000 (75%)] Loss: 11359.777344\n",
      "Train Epoch: 9 [172288/225000 (77%)] Loss: 10697.953125\n",
      "Train Epoch: 9 [176384/225000 (78%)] Loss: 11067.896484\n",
      "Train Epoch: 9 [180480/225000 (80%)] Loss: 11221.833984\n",
      "Train Epoch: 9 [184576/225000 (82%)] Loss: 11036.845703\n",
      "Train Epoch: 9 [188672/225000 (84%)] Loss: 11318.294922\n",
      "Train Epoch: 9 [192768/225000 (86%)] Loss: 10944.052734\n",
      "Train Epoch: 9 [196864/225000 (87%)] Loss: 10933.035156\n",
      "Train Epoch: 9 [200960/225000 (89%)] Loss: 11156.802734\n",
      "Train Epoch: 9 [205056/225000 (91%)] Loss: 11274.074219\n",
      "Train Epoch: 9 [209152/225000 (93%)] Loss: 11128.169922\n",
      "Train Epoch: 9 [213248/225000 (95%)] Loss: 11011.695312\n",
      "Train Epoch: 9 [217344/225000 (97%)] Loss: 11028.886719\n",
      "Train Epoch: 9 [221440/225000 (98%)] Loss: 11179.044922\n",
      "    epoch          : 9\n",
      "    loss           : 11273.359401663822\n",
      "    val_loss       : 11087.873749561455\n",
      "Train Epoch: 10 [256/225000 (0%)] Loss: 11218.205078\n",
      "Train Epoch: 10 [4352/225000 (2%)] Loss: 10946.957031\n",
      "Train Epoch: 10 [8448/225000 (4%)] Loss: 10716.837891\n",
      "Train Epoch: 10 [12544/225000 (6%)] Loss: 11357.554688\n",
      "Train Epoch: 10 [16640/225000 (7%)] Loss: 11156.439453\n",
      "Train Epoch: 10 [20736/225000 (9%)] Loss: 10912.175781\n",
      "Train Epoch: 10 [24832/225000 (11%)] Loss: 10934.134766\n",
      "Train Epoch: 10 [28928/225000 (13%)] Loss: 10960.923828\n",
      "Train Epoch: 10 [33024/225000 (15%)] Loss: 11080.876953\n",
      "Train Epoch: 10 [37120/225000 (16%)] Loss: 11230.664062\n",
      "Train Epoch: 10 [41216/225000 (18%)] Loss: 11113.404297\n",
      "Train Epoch: 10 [45312/225000 (20%)] Loss: 11171.822266\n",
      "Train Epoch: 10 [49408/225000 (22%)] Loss: 10974.849609\n",
      "Train Epoch: 10 [53504/225000 (24%)] Loss: 10969.968750\n",
      "Train Epoch: 10 [57600/225000 (26%)] Loss: 10970.375000\n",
      "Train Epoch: 10 [61696/225000 (27%)] Loss: 11106.972656\n",
      "Train Epoch: 10 [65792/225000 (29%)] Loss: 10911.976562\n",
      "Train Epoch: 10 [69888/225000 (31%)] Loss: 11025.521484\n",
      "Train Epoch: 10 [73984/225000 (33%)] Loss: 10938.878906\n",
      "Train Epoch: 10 [78080/225000 (35%)] Loss: 11210.988281\n",
      "Train Epoch: 10 [82176/225000 (37%)] Loss: 10979.263672\n",
      "Train Epoch: 10 [86272/225000 (38%)] Loss: 10836.357422\n",
      "Train Epoch: 10 [90368/225000 (40%)] Loss: 10920.890625\n",
      "Train Epoch: 10 [94464/225000 (42%)] Loss: 10877.876953\n",
      "Train Epoch: 10 [98560/225000 (44%)] Loss: 11218.132812\n",
      "Train Epoch: 10 [102656/225000 (46%)] Loss: 10879.382812\n",
      "Train Epoch: 10 [106752/225000 (47%)] Loss: 11031.500000\n",
      "Train Epoch: 10 [110848/225000 (49%)] Loss: 10777.251953\n",
      "Train Epoch: 10 [114944/225000 (51%)] Loss: 10819.767578\n",
      "Train Epoch: 10 [119040/225000 (53%)] Loss: 10783.611328\n",
      "Train Epoch: 10 [123136/225000 (55%)] Loss: 11006.673828\n",
      "Train Epoch: 10 [127232/225000 (57%)] Loss: 11039.351562\n",
      "Train Epoch: 10 [131328/225000 (58%)] Loss: 10779.123047\n",
      "Train Epoch: 10 [135424/225000 (60%)] Loss: 10856.759766\n",
      "Train Epoch: 10 [139520/225000 (62%)] Loss: 10810.212891\n",
      "Train Epoch: 10 [143616/225000 (64%)] Loss: 10597.001953\n",
      "Train Epoch: 10 [147712/225000 (66%)] Loss: 10701.251953\n",
      "Train Epoch: 10 [151808/225000 (67%)] Loss: 11000.878906\n",
      "Train Epoch: 10 [155904/225000 (69%)] Loss: 10864.398438\n",
      "Train Epoch: 10 [160000/225000 (71%)] Loss: 10778.513672\n",
      "Train Epoch: 10 [164096/225000 (73%)] Loss: 10793.798828\n",
      "Train Epoch: 10 [168192/225000 (75%)] Loss: 10823.458984\n",
      "Train Epoch: 10 [172288/225000 (77%)] Loss: 10599.144531\n",
      "Train Epoch: 10 [176384/225000 (78%)] Loss: 10625.240234\n",
      "Train Epoch: 10 [180480/225000 (80%)] Loss: 10591.443359\n",
      "Train Epoch: 10 [184576/225000 (82%)] Loss: 10692.826172\n",
      "Train Epoch: 10 [188672/225000 (84%)] Loss: 10570.404297\n",
      "Train Epoch: 10 [192768/225000 (86%)] Loss: 10885.664062\n",
      "Train Epoch: 10 [196864/225000 (87%)] Loss: 10637.417969\n",
      "Train Epoch: 10 [200960/225000 (89%)] Loss: 10970.929688\n",
      "Train Epoch: 10 [205056/225000 (91%)] Loss: 11022.369141\n",
      "Train Epoch: 10 [209152/225000 (93%)] Loss: 10930.173828\n",
      "Train Epoch: 10 [213248/225000 (95%)] Loss: 10913.607422\n",
      "Train Epoch: 10 [217344/225000 (97%)] Loss: 10596.035156\n",
      "Train Epoch: 10 [221440/225000 (98%)] Loss: 10642.464844\n",
      "    epoch          : 10\n",
      "    loss           : 10928.089468234499\n",
      "    val_loss       : 10703.001592928658\n",
      "Train Epoch: 11 [256/225000 (0%)] Loss: 10738.468750\n",
      "Train Epoch: 11 [4352/225000 (2%)] Loss: 10555.451172\n",
      "Train Epoch: 11 [8448/225000 (4%)] Loss: 10672.269531\n",
      "Train Epoch: 11 [12544/225000 (6%)] Loss: 10778.613281\n",
      "Train Epoch: 11 [16640/225000 (7%)] Loss: 10710.753906\n",
      "Train Epoch: 11 [20736/225000 (9%)] Loss: 10620.859375\n",
      "Train Epoch: 11 [24832/225000 (11%)] Loss: 10558.962891\n",
      "Train Epoch: 11 [28928/225000 (13%)] Loss: 10598.226562\n",
      "Train Epoch: 11 [33024/225000 (15%)] Loss: 10644.720703\n",
      "Train Epoch: 11 [37120/225000 (16%)] Loss: 10698.818359\n",
      "Train Epoch: 11 [41216/225000 (18%)] Loss: 10490.148438\n",
      "Train Epoch: 11 [45312/225000 (20%)] Loss: 10561.177734\n",
      "Train Epoch: 11 [49408/225000 (22%)] Loss: 10633.208984\n",
      "Train Epoch: 11 [53504/225000 (24%)] Loss: 10831.294922\n",
      "Train Epoch: 11 [57600/225000 (26%)] Loss: 10661.990234\n",
      "Train Epoch: 11 [61696/225000 (27%)] Loss: 10729.826172\n",
      "Train Epoch: 11 [65792/225000 (29%)] Loss: 11087.275391\n",
      "Train Epoch: 11 [69888/225000 (31%)] Loss: 10632.480469\n",
      "Train Epoch: 11 [73984/225000 (33%)] Loss: 10786.839844\n",
      "Train Epoch: 11 [78080/225000 (35%)] Loss: 10555.855469\n",
      "Train Epoch: 11 [82176/225000 (37%)] Loss: 10593.783203\n",
      "Train Epoch: 11 [86272/225000 (38%)] Loss: 10583.126953\n",
      "Train Epoch: 11 [90368/225000 (40%)] Loss: 10568.156250\n",
      "Train Epoch: 11 [94464/225000 (42%)] Loss: 10669.070312\n",
      "Train Epoch: 11 [98560/225000 (44%)] Loss: 10608.833984\n",
      "Train Epoch: 11 [102656/225000 (46%)] Loss: 10651.521484\n",
      "Train Epoch: 11 [106752/225000 (47%)] Loss: 10295.722656\n",
      "Train Epoch: 11 [110848/225000 (49%)] Loss: 10447.419922\n",
      "Train Epoch: 11 [114944/225000 (51%)] Loss: 10634.937500\n",
      "Train Epoch: 11 [119040/225000 (53%)] Loss: 10570.126953\n",
      "Train Epoch: 11 [123136/225000 (55%)] Loss: 10352.667969\n",
      "Train Epoch: 11 [127232/225000 (57%)] Loss: 10889.257812\n",
      "Train Epoch: 11 [131328/225000 (58%)] Loss: 10450.591797\n",
      "Train Epoch: 11 [135424/225000 (60%)] Loss: 10734.533203\n",
      "Train Epoch: 11 [139520/225000 (62%)] Loss: 10342.957031\n",
      "Train Epoch: 11 [143616/225000 (64%)] Loss: 10781.650391\n",
      "Train Epoch: 11 [147712/225000 (66%)] Loss: 10424.976562\n",
      "Train Epoch: 11 [151808/225000 (67%)] Loss: 10584.148438\n",
      "Train Epoch: 11 [155904/225000 (69%)] Loss: 10624.406250\n",
      "Train Epoch: 11 [160000/225000 (71%)] Loss: 10491.654297\n",
      "Train Epoch: 11 [164096/225000 (73%)] Loss: 10405.984375\n",
      "Train Epoch: 11 [168192/225000 (75%)] Loss: 10658.468750\n",
      "Train Epoch: 11 [172288/225000 (77%)] Loss: 10387.609375\n",
      "Train Epoch: 11 [176384/225000 (78%)] Loss: 11609.224609\n",
      "Train Epoch: 11 [180480/225000 (80%)] Loss: 10510.164062\n",
      "Train Epoch: 11 [184576/225000 (82%)] Loss: 10544.814453\n",
      "Train Epoch: 11 [188672/225000 (84%)] Loss: 10722.582031\n",
      "Train Epoch: 11 [192768/225000 (86%)] Loss: 10384.830078\n",
      "Train Epoch: 11 [196864/225000 (87%)] Loss: 10436.597656\n",
      "Train Epoch: 11 [200960/225000 (89%)] Loss: 10615.488281\n",
      "Train Epoch: 11 [205056/225000 (91%)] Loss: 10535.939453\n",
      "Train Epoch: 11 [209152/225000 (93%)] Loss: 10393.667969\n",
      "Train Epoch: 11 [213248/225000 (95%)] Loss: 10556.976562\n",
      "Train Epoch: 11 [217344/225000 (97%)] Loss: 10244.667969\n",
      "Train Epoch: 11 [221440/225000 (98%)] Loss: 10440.025391\n",
      "    epoch          : 11\n",
      "    loss           : 10632.524837350682\n",
      "    val_loss       : 10409.356092254726\n",
      "Train Epoch: 12 [256/225000 (0%)] Loss: 10325.019531\n",
      "Train Epoch: 12 [4352/225000 (2%)] Loss: 10452.833984\n",
      "Train Epoch: 12 [8448/225000 (4%)] Loss: 10389.794922\n",
      "Train Epoch: 12 [12544/225000 (6%)] Loss: 10677.718750\n",
      "Train Epoch: 12 [16640/225000 (7%)] Loss: 10443.525391\n",
      "Train Epoch: 12 [20736/225000 (9%)] Loss: 10416.423828\n",
      "Train Epoch: 12 [24832/225000 (11%)] Loss: 10652.421875\n",
      "Train Epoch: 12 [28928/225000 (13%)] Loss: 10307.814453\n",
      "Train Epoch: 12 [33024/225000 (15%)] Loss: 10781.095703\n",
      "Train Epoch: 12 [37120/225000 (16%)] Loss: 10445.435547\n",
      "Train Epoch: 12 [41216/225000 (18%)] Loss: 10457.187500\n",
      "Train Epoch: 12 [45312/225000 (20%)] Loss: 10519.244141\n",
      "Train Epoch: 12 [49408/225000 (22%)] Loss: 10292.156250\n",
      "Train Epoch: 12 [53504/225000 (24%)] Loss: 10442.449219\n",
      "Train Epoch: 12 [57600/225000 (26%)] Loss: 10434.117188\n",
      "Train Epoch: 12 [61696/225000 (27%)] Loss: 10547.271484\n",
      "Train Epoch: 12 [65792/225000 (29%)] Loss: 10281.781250\n",
      "Train Epoch: 12 [69888/225000 (31%)] Loss: 10268.208984\n",
      "Train Epoch: 12 [73984/225000 (33%)] Loss: 10342.267578\n",
      "Train Epoch: 12 [78080/225000 (35%)] Loss: 10376.632812\n",
      "Train Epoch: 12 [82176/225000 (37%)] Loss: 10185.669922\n",
      "Train Epoch: 12 [86272/225000 (38%)] Loss: 10344.605469\n",
      "Train Epoch: 12 [90368/225000 (40%)] Loss: 10382.757812\n",
      "Train Epoch: 12 [94464/225000 (42%)] Loss: 10505.804688\n",
      "Train Epoch: 12 [98560/225000 (44%)] Loss: 10453.640625\n",
      "Train Epoch: 12 [102656/225000 (46%)] Loss: 10184.828125\n",
      "Train Epoch: 12 [106752/225000 (47%)] Loss: 10344.070312\n",
      "Train Epoch: 12 [110848/225000 (49%)] Loss: 10208.806641\n",
      "Train Epoch: 12 [114944/225000 (51%)] Loss: 10475.867188\n",
      "Train Epoch: 12 [119040/225000 (53%)] Loss: 10549.591797\n",
      "Train Epoch: 12 [123136/225000 (55%)] Loss: 10285.812500\n",
      "Train Epoch: 12 [127232/225000 (57%)] Loss: 10296.390625\n",
      "Train Epoch: 12 [131328/225000 (58%)] Loss: 10219.980469\n",
      "Train Epoch: 12 [135424/225000 (60%)] Loss: 10381.083984\n",
      "Train Epoch: 12 [139520/225000 (62%)] Loss: 10505.734375\n",
      "Train Epoch: 12 [143616/225000 (64%)] Loss: 10333.888672\n",
      "Train Epoch: 12 [147712/225000 (66%)] Loss: 10170.556641\n",
      "Train Epoch: 12 [151808/225000 (67%)] Loss: 10373.667969\n",
      "Train Epoch: 12 [155904/225000 (69%)] Loss: 10299.578125\n",
      "Train Epoch: 12 [160000/225000 (71%)] Loss: 10248.667969\n",
      "Train Epoch: 12 [164096/225000 (73%)] Loss: 10375.091797\n",
      "Train Epoch: 12 [168192/225000 (75%)] Loss: 10531.582031\n",
      "Train Epoch: 12 [172288/225000 (77%)] Loss: 10446.119141\n",
      "Train Epoch: 12 [176384/225000 (78%)] Loss: 10261.154297\n",
      "Train Epoch: 12 [180480/225000 (80%)] Loss: 10343.378906\n",
      "Train Epoch: 12 [184576/225000 (82%)] Loss: 10314.119141\n",
      "Train Epoch: 12 [188672/225000 (84%)] Loss: 10199.376953\n",
      "Train Epoch: 12 [192768/225000 (86%)] Loss: 10325.144531\n",
      "Train Epoch: 12 [196864/225000 (87%)] Loss: 10195.093750\n",
      "Train Epoch: 12 [200960/225000 (89%)] Loss: 10326.843750\n",
      "Train Epoch: 12 [205056/225000 (91%)] Loss: 10032.689453\n",
      "Train Epoch: 12 [209152/225000 (93%)] Loss: 10288.550781\n",
      "Train Epoch: 12 [213248/225000 (95%)] Loss: 10164.714844\n",
      "Train Epoch: 12 [217344/225000 (97%)] Loss: 10144.945312\n",
      "Train Epoch: 12 [221440/225000 (98%)] Loss: 10246.988281\n",
      "    epoch          : 12\n",
      "    loss           : 10386.413235921502\n",
      "    val_loss       : 10222.943097385825\n",
      "Train Epoch: 13 [256/225000 (0%)] Loss: 10269.416016\n",
      "Train Epoch: 13 [4352/225000 (2%)] Loss: 10218.896484\n",
      "Train Epoch: 13 [8448/225000 (4%)] Loss: 10213.248047\n",
      "Train Epoch: 13 [12544/225000 (6%)] Loss: 10299.007812\n",
      "Train Epoch: 13 [16640/225000 (7%)] Loss: 10269.593750\n",
      "Train Epoch: 13 [20736/225000 (9%)] Loss: 10237.859375\n",
      "Train Epoch: 13 [24832/225000 (11%)] Loss: 10271.873047\n",
      "Train Epoch: 13 [28928/225000 (13%)] Loss: 10359.458984\n",
      "Train Epoch: 13 [33024/225000 (15%)] Loss: 10190.570312\n",
      "Train Epoch: 13 [37120/225000 (16%)] Loss: 10141.570312\n",
      "Train Epoch: 13 [41216/225000 (18%)] Loss: 10399.433594\n",
      "Train Epoch: 13 [45312/225000 (20%)] Loss: 10429.056641\n",
      "Train Epoch: 13 [49408/225000 (22%)] Loss: 10072.335938\n",
      "Train Epoch: 13 [53504/225000 (24%)] Loss: 9975.355469\n",
      "Train Epoch: 13 [57600/225000 (26%)] Loss: 10098.191406\n",
      "Train Epoch: 13 [61696/225000 (27%)] Loss: 10147.212891\n",
      "Train Epoch: 13 [65792/225000 (29%)] Loss: 10470.242188\n",
      "Train Epoch: 13 [69888/225000 (31%)] Loss: 10328.916016\n",
      "Train Epoch: 13 [73984/225000 (33%)] Loss: 10197.787109\n",
      "Train Epoch: 13 [78080/225000 (35%)] Loss: 10238.013672\n",
      "Train Epoch: 13 [82176/225000 (37%)] Loss: 10084.355469\n",
      "Train Epoch: 13 [86272/225000 (38%)] Loss: 10248.470703\n",
      "Train Epoch: 13 [90368/225000 (40%)] Loss: 10090.669922\n",
      "Train Epoch: 13 [94464/225000 (42%)] Loss: 10253.000000\n",
      "Train Epoch: 13 [98560/225000 (44%)] Loss: 10225.300781\n",
      "Train Epoch: 13 [102656/225000 (46%)] Loss: 10372.832031\n",
      "Train Epoch: 13 [106752/225000 (47%)] Loss: 10127.253906\n",
      "Train Epoch: 13 [110848/225000 (49%)] Loss: 10049.910156\n",
      "Train Epoch: 13 [114944/225000 (51%)] Loss: 10251.505859\n",
      "Train Epoch: 13 [119040/225000 (53%)] Loss: 10044.982422\n",
      "Train Epoch: 13 [123136/225000 (55%)] Loss: 10154.148438\n",
      "Train Epoch: 13 [127232/225000 (57%)] Loss: 10165.421875\n",
      "Train Epoch: 13 [131328/225000 (58%)] Loss: 10096.328125\n",
      "Train Epoch: 13 [135424/225000 (60%)] Loss: 10136.617188\n",
      "Train Epoch: 13 [139520/225000 (62%)] Loss: 9929.119141\n",
      "Train Epoch: 13 [143616/225000 (64%)] Loss: 10220.679688\n",
      "Train Epoch: 13 [147712/225000 (66%)] Loss: 10387.591797\n",
      "Train Epoch: 13 [151808/225000 (67%)] Loss: 9914.781250\n",
      "Train Epoch: 13 [155904/225000 (69%)] Loss: 10222.478516\n",
      "Train Epoch: 13 [160000/225000 (71%)] Loss: 9979.595703\n",
      "Train Epoch: 13 [164096/225000 (73%)] Loss: 9936.158203\n",
      "Train Epoch: 13 [168192/225000 (75%)] Loss: 10045.644531\n",
      "Train Epoch: 13 [172288/225000 (77%)] Loss: 9928.537109\n",
      "Train Epoch: 13 [176384/225000 (78%)] Loss: 10010.023438\n",
      "Train Epoch: 13 [180480/225000 (80%)] Loss: 9953.103516\n",
      "Train Epoch: 13 [184576/225000 (82%)] Loss: 10219.857422\n",
      "Train Epoch: 13 [188672/225000 (84%)] Loss: 10019.755859\n",
      "Train Epoch: 13 [192768/225000 (86%)] Loss: 10118.953125\n",
      "Train Epoch: 13 [196864/225000 (87%)] Loss: 10034.195312\n",
      "Train Epoch: 13 [200960/225000 (89%)] Loss: 9908.605469\n",
      "Train Epoch: 13 [205056/225000 (91%)] Loss: 10106.017578\n",
      "Train Epoch: 13 [209152/225000 (93%)] Loss: 9971.019531\n",
      "Train Epoch: 13 [213248/225000 (95%)] Loss: 10057.498047\n",
      "Train Epoch: 13 [217344/225000 (97%)] Loss: 10005.468750\n",
      "Train Epoch: 13 [221440/225000 (98%)] Loss: 10188.343750\n",
      "    epoch          : 13\n",
      "    loss           : 10221.880533854166\n",
      "    val_loss       : 10044.13862327532\n",
      "Train Epoch: 14 [256/225000 (0%)] Loss: 9986.683594\n",
      "Train Epoch: 14 [4352/225000 (2%)] Loss: 10151.115234\n",
      "Train Epoch: 14 [8448/225000 (4%)] Loss: 10009.326172\n",
      "Train Epoch: 14 [12544/225000 (6%)] Loss: 9857.390625\n",
      "Train Epoch: 14 [16640/225000 (7%)] Loss: 9932.880859\n",
      "Train Epoch: 14 [20736/225000 (9%)] Loss: 10154.992188\n",
      "Train Epoch: 14 [24832/225000 (11%)] Loss: 9752.488281\n",
      "Train Epoch: 14 [28928/225000 (13%)] Loss: 9866.349609\n",
      "Train Epoch: 14 [33024/225000 (15%)] Loss: 10022.718750\n",
      "Train Epoch: 14 [37120/225000 (16%)] Loss: 10128.451172\n",
      "Train Epoch: 14 [41216/225000 (18%)] Loss: 9778.794922\n",
      "Train Epoch: 14 [45312/225000 (20%)] Loss: 9898.808594\n",
      "Train Epoch: 14 [49408/225000 (22%)] Loss: 9972.140625\n",
      "Train Epoch: 14 [53504/225000 (24%)] Loss: 10037.996094\n",
      "Train Epoch: 14 [57600/225000 (26%)] Loss: 10052.292969\n",
      "Train Epoch: 14 [61696/225000 (27%)] Loss: 10000.087891\n",
      "Train Epoch: 14 [65792/225000 (29%)] Loss: 10081.193359\n",
      "Train Epoch: 14 [69888/225000 (31%)] Loss: 10148.638672\n",
      "Train Epoch: 14 [73984/225000 (33%)] Loss: 10270.072266\n",
      "Train Epoch: 14 [78080/225000 (35%)] Loss: 9932.416016\n",
      "Train Epoch: 14 [82176/225000 (37%)] Loss: 10124.599609\n",
      "Train Epoch: 14 [86272/225000 (38%)] Loss: 10092.636719\n",
      "Train Epoch: 14 [90368/225000 (40%)] Loss: 9953.687500\n",
      "Train Epoch: 14 [94464/225000 (42%)] Loss: 9890.000000\n",
      "Train Epoch: 14 [98560/225000 (44%)] Loss: 10026.984375\n",
      "Train Epoch: 14 [102656/225000 (46%)] Loss: 10099.292969\n",
      "Train Epoch: 14 [106752/225000 (47%)] Loss: 10026.859375\n",
      "Train Epoch: 14 [110848/225000 (49%)] Loss: 10047.833984\n",
      "Train Epoch: 14 [114944/225000 (51%)] Loss: 10122.683594\n",
      "Train Epoch: 14 [119040/225000 (53%)] Loss: 9721.857422\n",
      "Train Epoch: 14 [123136/225000 (55%)] Loss: 9968.589844\n",
      "Train Epoch: 14 [127232/225000 (57%)] Loss: 9949.160156\n",
      "Train Epoch: 14 [131328/225000 (58%)] Loss: 9912.687500\n",
      "Train Epoch: 14 [135424/225000 (60%)] Loss: 9926.646484\n",
      "Train Epoch: 14 [139520/225000 (62%)] Loss: 10020.302734\n",
      "Train Epoch: 14 [143616/225000 (64%)] Loss: 9961.197266\n",
      "Train Epoch: 14 [147712/225000 (66%)] Loss: 9833.265625\n",
      "Train Epoch: 14 [151808/225000 (67%)] Loss: 9824.289062\n",
      "Train Epoch: 14 [155904/225000 (69%)] Loss: 9786.566406\n",
      "Train Epoch: 14 [160000/225000 (71%)] Loss: 9658.166016\n",
      "Train Epoch: 14 [164096/225000 (73%)] Loss: 10108.410156\n",
      "Train Epoch: 14 [168192/225000 (75%)] Loss: 9859.804688\n",
      "Train Epoch: 14 [172288/225000 (77%)] Loss: 9932.808594\n",
      "Train Epoch: 14 [176384/225000 (78%)] Loss: 10030.199219\n",
      "Train Epoch: 14 [180480/225000 (80%)] Loss: 9878.435547\n",
      "Train Epoch: 14 [184576/225000 (82%)] Loss: 10149.990234\n",
      "Train Epoch: 14 [188672/225000 (84%)] Loss: 9844.072266\n",
      "Train Epoch: 14 [192768/225000 (86%)] Loss: 9939.541016\n",
      "Train Epoch: 14 [196864/225000 (87%)] Loss: 9886.500000\n",
      "Train Epoch: 14 [200960/225000 (89%)] Loss: 9934.730469\n",
      "Train Epoch: 14 [205056/225000 (91%)] Loss: 9855.101562\n",
      "Train Epoch: 14 [209152/225000 (93%)] Loss: 9669.660156\n",
      "Train Epoch: 14 [213248/225000 (95%)] Loss: 9901.742188\n",
      "Train Epoch: 14 [217344/225000 (97%)] Loss: 9885.777344\n",
      "Train Epoch: 14 [221440/225000 (98%)] Loss: 9637.902344\n",
      "    epoch          : 14\n",
      "    loss           : 10005.340933633746\n",
      "    val_loss       : 10052.45327455596\n",
      "Train Epoch: 15 [256/225000 (0%)] Loss: 9862.808594\n",
      "Train Epoch: 15 [4352/225000 (2%)] Loss: 9741.599609\n",
      "Train Epoch: 15 [8448/225000 (4%)] Loss: 9590.263672\n",
      "Train Epoch: 15 [12544/225000 (6%)] Loss: 9744.294922\n",
      "Train Epoch: 15 [16640/225000 (7%)] Loss: 10117.468750\n",
      "Train Epoch: 15 [20736/225000 (9%)] Loss: 9601.972656\n",
      "Train Epoch: 15 [24832/225000 (11%)] Loss: 9971.261719\n",
      "Train Epoch: 15 [28928/225000 (13%)] Loss: 9843.943359\n",
      "Train Epoch: 15 [33024/225000 (15%)] Loss: 9651.011719\n",
      "Train Epoch: 15 [37120/225000 (16%)] Loss: 9709.675781\n",
      "Train Epoch: 15 [41216/225000 (18%)] Loss: 9554.798828\n",
      "Train Epoch: 15 [45312/225000 (20%)] Loss: 9886.476562\n",
      "Train Epoch: 15 [49408/225000 (22%)] Loss: 9875.943359\n",
      "Train Epoch: 15 [53504/225000 (24%)] Loss: 9718.980469\n",
      "Train Epoch: 15 [57600/225000 (26%)] Loss: 9586.289062\n",
      "Train Epoch: 15 [61696/225000 (27%)] Loss: 9703.763672\n",
      "Train Epoch: 15 [65792/225000 (29%)] Loss: 9938.212891\n",
      "Train Epoch: 15 [69888/225000 (31%)] Loss: 9717.591797\n",
      "Train Epoch: 15 [73984/225000 (33%)] Loss: 9688.343750\n",
      "Train Epoch: 15 [78080/225000 (35%)] Loss: 10203.718750\n",
      "Train Epoch: 15 [82176/225000 (37%)] Loss: 9935.611328\n",
      "Train Epoch: 15 [86272/225000 (38%)] Loss: 9664.681641\n",
      "Train Epoch: 15 [90368/225000 (40%)] Loss: 9828.339844\n",
      "Train Epoch: 15 [94464/225000 (42%)] Loss: 9689.089844\n",
      "Train Epoch: 15 [98560/225000 (44%)] Loss: 9766.398438\n",
      "Train Epoch: 15 [102656/225000 (46%)] Loss: 9761.681641\n",
      "Train Epoch: 15 [106752/225000 (47%)] Loss: 9788.666016\n",
      "Train Epoch: 15 [110848/225000 (49%)] Loss: 9700.210938\n",
      "Train Epoch: 15 [114944/225000 (51%)] Loss: 9940.632812\n",
      "Train Epoch: 15 [119040/225000 (53%)] Loss: 9980.607422\n",
      "Train Epoch: 15 [123136/225000 (55%)] Loss: 9782.208984\n",
      "Train Epoch: 15 [127232/225000 (57%)] Loss: 9907.341797\n",
      "Train Epoch: 15 [131328/225000 (58%)] Loss: 9690.410156\n",
      "Train Epoch: 15 [135424/225000 (60%)] Loss: 9866.177734\n",
      "Train Epoch: 15 [139520/225000 (62%)] Loss: 9783.175781\n",
      "Train Epoch: 15 [143616/225000 (64%)] Loss: 9887.566406\n",
      "Train Epoch: 15 [147712/225000 (66%)] Loss: 9754.138672\n",
      "Train Epoch: 15 [151808/225000 (67%)] Loss: 9755.371094\n",
      "Train Epoch: 15 [155904/225000 (69%)] Loss: 9700.720703\n",
      "Train Epoch: 15 [160000/225000 (71%)] Loss: 9860.720703\n",
      "Train Epoch: 15 [164096/225000 (73%)] Loss: 9652.857422\n",
      "Train Epoch: 15 [168192/225000 (75%)] Loss: 9808.113281\n",
      "Train Epoch: 15 [172288/225000 (77%)] Loss: 10007.207031\n",
      "Train Epoch: 15 [176384/225000 (78%)] Loss: 9807.720703\n",
      "Train Epoch: 15 [180480/225000 (80%)] Loss: 9575.292969\n",
      "Train Epoch: 15 [184576/225000 (82%)] Loss: 9647.494141\n",
      "Train Epoch: 15 [188672/225000 (84%)] Loss: 9929.732422\n",
      "Train Epoch: 15 [192768/225000 (86%)] Loss: 9993.845703\n",
      "Train Epoch: 15 [196864/225000 (87%)] Loss: 9728.832031\n",
      "Train Epoch: 15 [200960/225000 (89%)] Loss: 9841.798828\n",
      "Train Epoch: 15 [205056/225000 (91%)] Loss: 9658.144531\n",
      "Train Epoch: 15 [209152/225000 (93%)] Loss: 9806.679688\n",
      "Train Epoch: 15 [213248/225000 (95%)] Loss: 10878.056641\n",
      "Train Epoch: 15 [217344/225000 (97%)] Loss: 10025.632812\n",
      "Train Epoch: 15 [221440/225000 (98%)] Loss: 9647.060547\n",
      "    epoch          : 15\n",
      "    loss           : 9834.313930958475\n",
      "    val_loss       : 9628.85290663218\n",
      "Train Epoch: 16 [256/225000 (0%)] Loss: 9790.814453\n",
      "Train Epoch: 16 [4352/225000 (2%)] Loss: 9917.455078\n",
      "Train Epoch: 16 [8448/225000 (4%)] Loss: 9681.884766\n",
      "Train Epoch: 16 [12544/225000 (6%)] Loss: 9544.767578\n",
      "Train Epoch: 16 [16640/225000 (7%)] Loss: 9641.925781\n",
      "Train Epoch: 16 [20736/225000 (9%)] Loss: 9534.712891\n",
      "Train Epoch: 16 [24832/225000 (11%)] Loss: 9530.320312\n",
      "Train Epoch: 16 [28928/225000 (13%)] Loss: 9928.009766\n",
      "Train Epoch: 16 [33024/225000 (15%)] Loss: 9522.199219\n",
      "Train Epoch: 16 [37120/225000 (16%)] Loss: 9635.154297\n",
      "Train Epoch: 16 [41216/225000 (18%)] Loss: 9935.826172\n",
      "Train Epoch: 16 [45312/225000 (20%)] Loss: 9690.427734\n",
      "Train Epoch: 16 [49408/225000 (22%)] Loss: 9591.687500\n",
      "Train Epoch: 16 [53504/225000 (24%)] Loss: 9695.324219\n",
      "Train Epoch: 16 [57600/225000 (26%)] Loss: 9399.408203\n",
      "Train Epoch: 16 [61696/225000 (27%)] Loss: 9849.123047\n",
      "Train Epoch: 16 [65792/225000 (29%)] Loss: 10057.703125\n",
      "Train Epoch: 16 [69888/225000 (31%)] Loss: 9620.687500\n",
      "Train Epoch: 16 [73984/225000 (33%)] Loss: 9744.935547\n",
      "Train Epoch: 16 [78080/225000 (35%)] Loss: 9932.535156\n",
      "Train Epoch: 16 [82176/225000 (37%)] Loss: 9707.214844\n",
      "Train Epoch: 16 [86272/225000 (38%)] Loss: 9728.353516\n",
      "Train Epoch: 16 [90368/225000 (40%)] Loss: 9339.582031\n",
      "Train Epoch: 16 [94464/225000 (42%)] Loss: 9523.058594\n",
      "Train Epoch: 16 [98560/225000 (44%)] Loss: 9519.160156\n",
      "Train Epoch: 16 [102656/225000 (46%)] Loss: 9707.794922\n",
      "Train Epoch: 16 [106752/225000 (47%)] Loss: 9504.779297\n",
      "Train Epoch: 16 [110848/225000 (49%)] Loss: 9687.013672\n",
      "Train Epoch: 16 [114944/225000 (51%)] Loss: 9802.759766\n",
      "Train Epoch: 16 [119040/225000 (53%)] Loss: 9405.964844\n",
      "Train Epoch: 16 [123136/225000 (55%)] Loss: 9581.476562\n",
      "Train Epoch: 16 [127232/225000 (57%)] Loss: 9665.931641\n",
      "Train Epoch: 16 [131328/225000 (58%)] Loss: 9572.001953\n",
      "Train Epoch: 16 [135424/225000 (60%)] Loss: 9676.847656\n",
      "Train Epoch: 16 [139520/225000 (62%)] Loss: 9801.273438\n",
      "Train Epoch: 16 [143616/225000 (64%)] Loss: 9623.056641\n",
      "Train Epoch: 16 [147712/225000 (66%)] Loss: 9522.513672\n",
      "Train Epoch: 16 [151808/225000 (67%)] Loss: 9667.673828\n",
      "Train Epoch: 16 [155904/225000 (69%)] Loss: 9555.003906\n",
      "Train Epoch: 16 [160000/225000 (71%)] Loss: 9922.796875\n",
      "Train Epoch: 16 [164096/225000 (73%)] Loss: 9829.388672\n",
      "Train Epoch: 16 [168192/225000 (75%)] Loss: 9309.652344\n",
      "Train Epoch: 16 [172288/225000 (77%)] Loss: 9608.509766\n",
      "Train Epoch: 16 [176384/225000 (78%)] Loss: 9547.921875\n",
      "Train Epoch: 16 [180480/225000 (80%)] Loss: 9401.337891\n",
      "Train Epoch: 16 [184576/225000 (82%)] Loss: 9777.750000\n",
      "Train Epoch: 16 [188672/225000 (84%)] Loss: 9652.707031\n",
      "Train Epoch: 16 [192768/225000 (86%)] Loss: 9831.457031\n",
      "Train Epoch: 16 [196864/225000 (87%)] Loss: 9585.074219\n",
      "Train Epoch: 16 [200960/225000 (89%)] Loss: 9594.921875\n",
      "Train Epoch: 16 [205056/225000 (91%)] Loss: 9525.093750\n",
      "Train Epoch: 16 [209152/225000 (93%)] Loss: 9563.347656\n",
      "Train Epoch: 16 [213248/225000 (95%)] Loss: 9444.216797\n",
      "Train Epoch: 16 [217344/225000 (97%)] Loss: 9396.810547\n",
      "Train Epoch: 16 [221440/225000 (98%)] Loss: 9623.857422\n",
      "    epoch          : 16\n",
      "    loss           : 9691.275928345422\n",
      "    val_loss       : 9503.935907979401\n",
      "Train Epoch: 17 [256/225000 (0%)] Loss: 9512.560547\n",
      "Train Epoch: 17 [4352/225000 (2%)] Loss: 9426.376953\n",
      "Train Epoch: 17 [8448/225000 (4%)] Loss: 9522.203125\n",
      "Train Epoch: 17 [12544/225000 (6%)] Loss: 9716.642578\n",
      "Train Epoch: 17 [16640/225000 (7%)] Loss: 9565.117188\n",
      "Train Epoch: 17 [20736/225000 (9%)] Loss: 9458.349609\n",
      "Train Epoch: 17 [24832/225000 (11%)] Loss: 9544.019531\n",
      "Train Epoch: 17 [28928/225000 (13%)] Loss: 9680.117188\n",
      "Train Epoch: 17 [33024/225000 (15%)] Loss: 9408.285156\n",
      "Train Epoch: 17 [37120/225000 (16%)] Loss: 9649.470703\n",
      "Train Epoch: 17 [41216/225000 (18%)] Loss: 9831.531250\n",
      "Train Epoch: 17 [45312/225000 (20%)] Loss: 9834.898438\n",
      "Train Epoch: 17 [49408/225000 (22%)] Loss: 9511.035156\n",
      "Train Epoch: 17 [53504/225000 (24%)] Loss: 9379.685547\n",
      "Train Epoch: 17 [57600/225000 (26%)] Loss: 9697.613281\n",
      "Train Epoch: 17 [61696/225000 (27%)] Loss: 9373.218750\n",
      "Train Epoch: 17 [65792/225000 (29%)] Loss: 9503.345703\n",
      "Train Epoch: 17 [69888/225000 (31%)] Loss: 9490.265625\n",
      "Train Epoch: 17 [73984/225000 (33%)] Loss: 9568.988281\n",
      "Train Epoch: 17 [78080/225000 (35%)] Loss: 9574.667969\n",
      "Train Epoch: 17 [82176/225000 (37%)] Loss: 9294.839844\n",
      "Train Epoch: 17 [86272/225000 (38%)] Loss: 9424.416016\n",
      "Train Epoch: 17 [90368/225000 (40%)] Loss: 9520.587891\n",
      "Train Epoch: 17 [94464/225000 (42%)] Loss: 9444.330078\n",
      "Train Epoch: 17 [98560/225000 (44%)] Loss: 9433.392578\n",
      "Train Epoch: 17 [102656/225000 (46%)] Loss: 9489.087891\n",
      "Train Epoch: 17 [106752/225000 (47%)] Loss: 9401.750000\n",
      "Train Epoch: 17 [110848/225000 (49%)] Loss: 9232.363281\n",
      "Train Epoch: 17 [114944/225000 (51%)] Loss: 9531.398438\n",
      "Train Epoch: 17 [119040/225000 (53%)] Loss: 9544.541016\n",
      "Train Epoch: 17 [123136/225000 (55%)] Loss: 9157.455078\n",
      "Train Epoch: 17 [127232/225000 (57%)] Loss: 9424.302734\n",
      "Train Epoch: 17 [131328/225000 (58%)] Loss: 9521.896484\n",
      "Train Epoch: 17 [135424/225000 (60%)] Loss: 9215.441406\n",
      "Train Epoch: 17 [139520/225000 (62%)] Loss: 9450.369141\n",
      "Train Epoch: 17 [143616/225000 (64%)] Loss: 9639.505859\n",
      "Train Epoch: 17 [147712/225000 (66%)] Loss: 9470.529297\n",
      "Train Epoch: 17 [151808/225000 (67%)] Loss: 9394.431641\n",
      "Train Epoch: 17 [155904/225000 (69%)] Loss: 9251.128906\n",
      "Train Epoch: 17 [160000/225000 (71%)] Loss: 9524.095703\n",
      "Train Epoch: 17 [164096/225000 (73%)] Loss: 9461.906250\n",
      "Train Epoch: 17 [168192/225000 (75%)] Loss: 9444.304688\n",
      "Train Epoch: 17 [172288/225000 (77%)] Loss: 9445.427734\n",
      "Train Epoch: 17 [176384/225000 (78%)] Loss: 9479.347656\n",
      "Train Epoch: 17 [180480/225000 (80%)] Loss: 9383.402344\n",
      "Train Epoch: 17 [184576/225000 (82%)] Loss: 9567.052734\n",
      "Train Epoch: 17 [188672/225000 (84%)] Loss: 9348.562500\n",
      "Train Epoch: 17 [192768/225000 (86%)] Loss: 9247.511719\n",
      "Train Epoch: 17 [196864/225000 (87%)] Loss: 9513.884766\n",
      "Train Epoch: 17 [200960/225000 (89%)] Loss: 9563.404297\n",
      "Train Epoch: 17 [205056/225000 (91%)] Loss: 9621.089844\n",
      "Train Epoch: 17 [209152/225000 (93%)] Loss: 9480.054688\n",
      "Train Epoch: 17 [213248/225000 (95%)] Loss: 9478.195312\n",
      "Train Epoch: 17 [217344/225000 (97%)] Loss: 10697.482422\n",
      "Train Epoch: 17 [221440/225000 (98%)] Loss: 9244.636719\n",
      "    epoch          : 17\n",
      "    loss           : 9580.475382625853\n",
      "    val_loss       : 9357.650938585097\n",
      "Train Epoch: 18 [256/225000 (0%)] Loss: 9294.552734\n",
      "Train Epoch: 18 [4352/225000 (2%)] Loss: 9462.046875\n",
      "Train Epoch: 18 [8448/225000 (4%)] Loss: 9109.958984\n",
      "Train Epoch: 18 [12544/225000 (6%)] Loss: 9244.517578\n",
      "Train Epoch: 18 [16640/225000 (7%)] Loss: 9198.880859\n",
      "Train Epoch: 18 [20736/225000 (9%)] Loss: 9451.500000\n",
      "Train Epoch: 18 [24832/225000 (11%)] Loss: 9072.972656\n",
      "Train Epoch: 18 [28928/225000 (13%)] Loss: 9379.898438\n",
      "Train Epoch: 18 [33024/225000 (15%)] Loss: 9329.322266\n",
      "Train Epoch: 18 [37120/225000 (16%)] Loss: 9363.906250\n",
      "Train Epoch: 18 [41216/225000 (18%)] Loss: 9253.531250\n",
      "Train Epoch: 18 [45312/225000 (20%)] Loss: 9292.740234\n",
      "Train Epoch: 18 [49408/225000 (22%)] Loss: 9351.060547\n",
      "Train Epoch: 18 [53504/225000 (24%)] Loss: 9501.542969\n",
      "Train Epoch: 18 [57600/225000 (26%)] Loss: 9120.951172\n",
      "Train Epoch: 18 [61696/225000 (27%)] Loss: 9381.443359\n",
      "Train Epoch: 18 [65792/225000 (29%)] Loss: 9187.138672\n",
      "Train Epoch: 18 [69888/225000 (31%)] Loss: 9195.003906\n",
      "Train Epoch: 18 [73984/225000 (33%)] Loss: 9428.333984\n",
      "Train Epoch: 18 [78080/225000 (35%)] Loss: 9347.115234\n",
      "Train Epoch: 18 [82176/225000 (37%)] Loss: 9105.527344\n",
      "Train Epoch: 18 [86272/225000 (38%)] Loss: 9297.265625\n",
      "Train Epoch: 18 [90368/225000 (40%)] Loss: 9286.882812\n",
      "Train Epoch: 18 [94464/225000 (42%)] Loss: 9099.052734\n",
      "Train Epoch: 18 [98560/225000 (44%)] Loss: 9330.593750\n",
      "Train Epoch: 18 [102656/225000 (46%)] Loss: 9034.298828\n",
      "Train Epoch: 18 [106752/225000 (47%)] Loss: 9278.630859\n",
      "Train Epoch: 18 [110848/225000 (49%)] Loss: 9337.753906\n",
      "Train Epoch: 18 [114944/225000 (51%)] Loss: 9181.380859\n",
      "Train Epoch: 18 [119040/225000 (53%)] Loss: 9380.498047\n",
      "Train Epoch: 18 [123136/225000 (55%)] Loss: 9273.123047\n",
      "Train Epoch: 18 [127232/225000 (57%)] Loss: 9539.107422\n",
      "Train Epoch: 18 [131328/225000 (58%)] Loss: 9467.291016\n",
      "Train Epoch: 18 [135424/225000 (60%)] Loss: 9453.853516\n",
      "Train Epoch: 18 [139520/225000 (62%)] Loss: 9224.341797\n",
      "Train Epoch: 18 [143616/225000 (64%)] Loss: 9170.054688\n",
      "Train Epoch: 18 [147712/225000 (66%)] Loss: 9336.921875\n",
      "Train Epoch: 18 [151808/225000 (67%)] Loss: 9407.718750\n",
      "Train Epoch: 18 [155904/225000 (69%)] Loss: 9309.013672\n",
      "Train Epoch: 18 [160000/225000 (71%)] Loss: 9389.369141\n",
      "Train Epoch: 18 [164096/225000 (73%)] Loss: 9252.060547\n",
      "Train Epoch: 18 [168192/225000 (75%)] Loss: 9325.943359\n",
      "Train Epoch: 18 [172288/225000 (77%)] Loss: 9376.138672\n",
      "Train Epoch: 18 [176384/225000 (78%)] Loss: 8999.312500\n",
      "Train Epoch: 18 [180480/225000 (80%)] Loss: 9267.042969\n",
      "Train Epoch: 18 [184576/225000 (82%)] Loss: 9204.343750\n",
      "Train Epoch: 18 [188672/225000 (84%)] Loss: 9262.121094\n",
      "Train Epoch: 18 [192768/225000 (86%)] Loss: 9287.130859\n",
      "Train Epoch: 18 [196864/225000 (87%)] Loss: 9299.992188\n",
      "Train Epoch: 18 [200960/225000 (89%)] Loss: 9348.986328\n",
      "Train Epoch: 18 [205056/225000 (91%)] Loss: 9333.792969\n",
      "Train Epoch: 18 [209152/225000 (93%)] Loss: 9240.025391\n",
      "Train Epoch: 18 [213248/225000 (95%)] Loss: 9377.597656\n",
      "Train Epoch: 18 [217344/225000 (97%)] Loss: 9340.673828\n",
      "Train Epoch: 18 [221440/225000 (98%)] Loss: 9082.044922\n",
      "    epoch          : 18\n",
      "    loss           : 9449.459021037756\n",
      "    val_loss       : 9383.79040258028\n",
      "Train Epoch: 19 [256/225000 (0%)] Loss: 9404.767578\n",
      "Train Epoch: 19 [4352/225000 (2%)] Loss: 9238.826172\n",
      "Train Epoch: 19 [8448/225000 (4%)] Loss: 9202.753906\n",
      "Train Epoch: 19 [12544/225000 (6%)] Loss: 9111.031250\n",
      "Train Epoch: 19 [16640/225000 (7%)] Loss: 9031.599609\n",
      "Train Epoch: 19 [20736/225000 (9%)] Loss: 9256.796875\n",
      "Train Epoch: 19 [24832/225000 (11%)] Loss: 9121.863281\n",
      "Train Epoch: 19 [28928/225000 (13%)] Loss: 9301.291016\n",
      "Train Epoch: 19 [33024/225000 (15%)] Loss: 9393.275391\n",
      "Train Epoch: 19 [37120/225000 (16%)] Loss: 9227.138672\n",
      "Train Epoch: 19 [41216/225000 (18%)] Loss: 9355.671875\n",
      "Train Epoch: 19 [45312/225000 (20%)] Loss: 9158.652344\n",
      "Train Epoch: 19 [49408/225000 (22%)] Loss: 8814.837891\n",
      "Train Epoch: 19 [53504/225000 (24%)] Loss: 9262.738281\n",
      "Train Epoch: 19 [57600/225000 (26%)] Loss: 9250.767578\n",
      "Train Epoch: 19 [61696/225000 (27%)] Loss: 9318.724609\n",
      "Train Epoch: 19 [65792/225000 (29%)] Loss: 9115.396484\n",
      "Train Epoch: 19 [69888/225000 (31%)] Loss: 9095.240234\n",
      "Train Epoch: 19 [73984/225000 (33%)] Loss: 9340.257812\n",
      "Train Epoch: 19 [78080/225000 (35%)] Loss: 9159.679688\n",
      "Train Epoch: 19 [82176/225000 (37%)] Loss: 9292.642578\n",
      "Train Epoch: 19 [86272/225000 (38%)] Loss: 9304.625000\n",
      "Train Epoch: 19 [90368/225000 (40%)] Loss: 9290.419922\n",
      "Train Epoch: 19 [94464/225000 (42%)] Loss: 9141.441406\n",
      "Train Epoch: 19 [98560/225000 (44%)] Loss: 9086.488281\n",
      "Train Epoch: 19 [102656/225000 (46%)] Loss: 9111.794922\n",
      "Train Epoch: 19 [106752/225000 (47%)] Loss: 9289.718750\n",
      "Train Epoch: 19 [110848/225000 (49%)] Loss: 9206.787109\n",
      "Train Epoch: 19 [114944/225000 (51%)] Loss: 9364.576172\n",
      "Train Epoch: 19 [119040/225000 (53%)] Loss: 8973.578125\n",
      "Train Epoch: 19 [123136/225000 (55%)] Loss: 9032.419922\n",
      "Train Epoch: 19 [127232/225000 (57%)] Loss: 9184.500000\n",
      "Train Epoch: 19 [131328/225000 (58%)] Loss: 9267.548828\n",
      "Train Epoch: 19 [135424/225000 (60%)] Loss: 9058.789062\n",
      "Train Epoch: 19 [139520/225000 (62%)] Loss: 9101.791016\n",
      "Train Epoch: 19 [143616/225000 (64%)] Loss: 9047.000000\n",
      "Train Epoch: 19 [147712/225000 (66%)] Loss: 9153.605469\n",
      "Train Epoch: 19 [151808/225000 (67%)] Loss: 9048.832031\n",
      "Train Epoch: 19 [155904/225000 (69%)] Loss: 9055.707031\n",
      "Train Epoch: 19 [160000/225000 (71%)] Loss: 9076.343750\n",
      "Train Epoch: 19 [164096/225000 (73%)] Loss: 9076.302734\n",
      "Train Epoch: 19 [168192/225000 (75%)] Loss: 9182.533203\n",
      "Train Epoch: 19 [172288/225000 (77%)] Loss: 9189.046875\n",
      "Train Epoch: 19 [176384/225000 (78%)] Loss: 9190.777344\n",
      "Train Epoch: 19 [180480/225000 (80%)] Loss: 9205.593750\n",
      "Train Epoch: 19 [184576/225000 (82%)] Loss: 9406.726562\n",
      "Train Epoch: 19 [188672/225000 (84%)] Loss: 9230.480469\n",
      "Train Epoch: 19 [192768/225000 (86%)] Loss: 8963.994141\n",
      "Train Epoch: 19 [196864/225000 (87%)] Loss: 9175.722656\n",
      "Train Epoch: 19 [200960/225000 (89%)] Loss: 9347.921875\n",
      "Train Epoch: 19 [205056/225000 (91%)] Loss: 9183.628906\n",
      "Train Epoch: 19 [209152/225000 (93%)] Loss: 9056.882812\n",
      "Train Epoch: 19 [213248/225000 (95%)] Loss: 9022.126953\n",
      "Train Epoch: 19 [217344/225000 (97%)] Loss: 9208.638672\n",
      "Train Epoch: 19 [221440/225000 (98%)] Loss: 9296.759766\n",
      "    epoch          : 19\n",
      "    loss           : 9236.022814233149\n",
      "    val_loss       : 9647.591659544682\n",
      "Train Epoch: 20 [256/225000 (0%)] Loss: 9120.365234\n",
      "Train Epoch: 20 [4352/225000 (2%)] Loss: 9003.113281\n",
      "Train Epoch: 20 [8448/225000 (4%)] Loss: 9243.521484\n",
      "Train Epoch: 20 [12544/225000 (6%)] Loss: 9214.447266\n",
      "Train Epoch: 20 [16640/225000 (7%)] Loss: 9455.566406\n",
      "Train Epoch: 20 [20736/225000 (9%)] Loss: 9380.185547\n",
      "Train Epoch: 20 [24832/225000 (11%)] Loss: 9122.388672\n",
      "Train Epoch: 20 [28928/225000 (13%)] Loss: 9083.337891\n",
      "Train Epoch: 20 [33024/225000 (15%)] Loss: 10378.755859\n",
      "Train Epoch: 20 [37120/225000 (16%)] Loss: 9078.703125\n",
      "Train Epoch: 20 [41216/225000 (18%)] Loss: 9287.279297\n",
      "Train Epoch: 20 [45312/225000 (20%)] Loss: 9175.142578\n",
      "Train Epoch: 20 [49408/225000 (22%)] Loss: 9346.052734\n",
      "Train Epoch: 20 [53504/225000 (24%)] Loss: 8996.736328\n",
      "Train Epoch: 20 [57600/225000 (26%)] Loss: 9166.419922\n",
      "Train Epoch: 20 [61696/225000 (27%)] Loss: 9255.656250\n",
      "Train Epoch: 20 [65792/225000 (29%)] Loss: 9006.433594\n",
      "Train Epoch: 20 [69888/225000 (31%)] Loss: 9152.968750\n",
      "Train Epoch: 20 [73984/225000 (33%)] Loss: 9061.843750\n",
      "Train Epoch: 20 [78080/225000 (35%)] Loss: 8921.439453\n",
      "Train Epoch: 20 [82176/225000 (37%)] Loss: 9027.375000\n",
      "Train Epoch: 20 [86272/225000 (38%)] Loss: 9280.839844\n",
      "Train Epoch: 20 [90368/225000 (40%)] Loss: 8948.302734\n",
      "Train Epoch: 20 [94464/225000 (42%)] Loss: 9269.333984\n",
      "Train Epoch: 20 [98560/225000 (44%)] Loss: 9100.845703\n",
      "Train Epoch: 20 [102656/225000 (46%)] Loss: 9150.033203\n",
      "Train Epoch: 20 [106752/225000 (47%)] Loss: 9191.703125\n",
      "Train Epoch: 20 [110848/225000 (49%)] Loss: 8911.349609\n",
      "Train Epoch: 20 [114944/225000 (51%)] Loss: 8897.095703\n",
      "Train Epoch: 20 [119040/225000 (53%)] Loss: 9057.296875\n",
      "Train Epoch: 20 [123136/225000 (55%)] Loss: 9075.960938\n",
      "Train Epoch: 20 [127232/225000 (57%)] Loss: 8996.455078\n",
      "Train Epoch: 20 [131328/225000 (58%)] Loss: 9050.695312\n",
      "Train Epoch: 20 [135424/225000 (60%)] Loss: 9128.974609\n",
      "Train Epoch: 20 [139520/225000 (62%)] Loss: 8822.189453\n",
      "Train Epoch: 20 [143616/225000 (64%)] Loss: 9130.443359\n",
      "Train Epoch: 20 [147712/225000 (66%)] Loss: 9170.904297\n",
      "Train Epoch: 20 [151808/225000 (67%)] Loss: 9103.332031\n",
      "Train Epoch: 20 [155904/225000 (69%)] Loss: 9243.236328\n",
      "Train Epoch: 20 [160000/225000 (71%)] Loss: 8979.785156\n",
      "Train Epoch: 20 [164096/225000 (73%)] Loss: 8892.345703\n",
      "Train Epoch: 20 [168192/225000 (75%)] Loss: 9098.916016\n",
      "Train Epoch: 20 [172288/225000 (77%)] Loss: 8783.136719\n",
      "Train Epoch: 20 [176384/225000 (78%)] Loss: 10578.156250\n",
      "Train Epoch: 20 [180480/225000 (80%)] Loss: 9013.626953\n",
      "Train Epoch: 20 [184576/225000 (82%)] Loss: 8967.386719\n",
      "Train Epoch: 20 [188672/225000 (84%)] Loss: 9022.167969\n",
      "Train Epoch: 20 [192768/225000 (86%)] Loss: 9095.980469\n",
      "Train Epoch: 20 [196864/225000 (87%)] Loss: 9117.328125\n",
      "Train Epoch: 20 [200960/225000 (89%)] Loss: 9063.136719\n",
      "Train Epoch: 20 [205056/225000 (91%)] Loss: 8941.105469\n",
      "Train Epoch: 20 [209152/225000 (93%)] Loss: 8933.951172\n",
      "Train Epoch: 20 [213248/225000 (95%)] Loss: 9014.423828\n",
      "Train Epoch: 20 [217344/225000 (97%)] Loss: 9056.433594\n",
      "Train Epoch: 20 [221440/225000 (98%)] Loss: 8874.140625\n",
      "    epoch          : 20\n",
      "    loss           : 9145.822704467078\n",
      "    val_loss       : 8991.408293001201\n",
      "Train Epoch: 21 [256/225000 (0%)] Loss: 9136.716797\n",
      "Train Epoch: 21 [4352/225000 (2%)] Loss: 9068.654297\n",
      "Train Epoch: 21 [8448/225000 (4%)] Loss: 8998.453125\n",
      "Train Epoch: 21 [12544/225000 (6%)] Loss: 9049.728516\n",
      "Train Epoch: 21 [16640/225000 (7%)] Loss: 8936.005859\n",
      "Train Epoch: 21 [20736/225000 (9%)] Loss: 8904.335938\n",
      "Train Epoch: 21 [24832/225000 (11%)] Loss: 8933.072266\n",
      "Train Epoch: 21 [28928/225000 (13%)] Loss: 8943.468750\n",
      "Train Epoch: 21 [33024/225000 (15%)] Loss: 8952.339844\n",
      "Train Epoch: 21 [37120/225000 (16%)] Loss: 9012.697266\n",
      "Train Epoch: 21 [41216/225000 (18%)] Loss: 34876.781250\n",
      "Train Epoch: 21 [45312/225000 (20%)] Loss: 8939.322266\n",
      "Train Epoch: 21 [49408/225000 (22%)] Loss: 9155.470703\n",
      "Train Epoch: 21 [53504/225000 (24%)] Loss: 9386.453125\n",
      "Train Epoch: 21 [57600/225000 (26%)] Loss: 9040.101562\n",
      "Train Epoch: 21 [61696/225000 (27%)] Loss: 8981.105469\n",
      "Train Epoch: 21 [65792/225000 (29%)] Loss: 8756.310547\n",
      "Train Epoch: 21 [69888/225000 (31%)] Loss: 8868.111328\n",
      "Train Epoch: 21 [73984/225000 (33%)] Loss: 8862.503906\n",
      "Train Epoch: 21 [78080/225000 (35%)] Loss: 8936.099609\n",
      "Train Epoch: 21 [82176/225000 (37%)] Loss: 8850.750000\n",
      "Train Epoch: 21 [86272/225000 (38%)] Loss: 9058.095703\n",
      "Train Epoch: 21 [90368/225000 (40%)] Loss: 9026.177734\n",
      "Train Epoch: 21 [94464/225000 (42%)] Loss: 9207.074219\n",
      "Train Epoch: 21 [98560/225000 (44%)] Loss: 9029.958984\n",
      "Train Epoch: 21 [102656/225000 (46%)] Loss: 9033.302734\n",
      "Train Epoch: 21 [106752/225000 (47%)] Loss: 8889.742188\n",
      "Train Epoch: 21 [110848/225000 (49%)] Loss: 8939.832031\n",
      "Train Epoch: 21 [114944/225000 (51%)] Loss: 8994.072266\n",
      "Train Epoch: 21 [119040/225000 (53%)] Loss: 8917.980469\n",
      "Train Epoch: 21 [123136/225000 (55%)] Loss: 9007.980469\n",
      "Train Epoch: 21 [127232/225000 (57%)] Loss: 9217.914062\n",
      "Train Epoch: 21 [131328/225000 (58%)] Loss: 9011.359375\n",
      "Train Epoch: 21 [135424/225000 (60%)] Loss: 8955.460938\n",
      "Train Epoch: 21 [139520/225000 (62%)] Loss: 10251.519531\n",
      "Train Epoch: 21 [143616/225000 (64%)] Loss: 8903.498047\n",
      "Train Epoch: 21 [147712/225000 (66%)] Loss: 8834.914062\n",
      "Train Epoch: 21 [151808/225000 (67%)] Loss: 8964.564453\n",
      "Train Epoch: 21 [155904/225000 (69%)] Loss: 8834.337891\n",
      "Train Epoch: 21 [160000/225000 (71%)] Loss: 8985.958984\n",
      "Train Epoch: 21 [164096/225000 (73%)] Loss: 8993.050781\n",
      "Train Epoch: 21 [168192/225000 (75%)] Loss: 9080.048828\n",
      "Train Epoch: 21 [172288/225000 (77%)] Loss: 8916.533203\n",
      "Train Epoch: 21 [176384/225000 (78%)] Loss: 8778.457031\n",
      "Train Epoch: 21 [180480/225000 (80%)] Loss: 8949.859375\n",
      "Train Epoch: 21 [184576/225000 (82%)] Loss: 8777.392578\n",
      "Train Epoch: 21 [188672/225000 (84%)] Loss: 8760.376953\n",
      "Train Epoch: 21 [192768/225000 (86%)] Loss: 8695.636719\n",
      "Train Epoch: 21 [196864/225000 (87%)] Loss: 9126.273438\n",
      "Train Epoch: 21 [200960/225000 (89%)] Loss: 9072.560547\n",
      "Train Epoch: 21 [205056/225000 (91%)] Loss: 8962.548828\n",
      "Train Epoch: 21 [209152/225000 (93%)] Loss: 8733.126953\n",
      "Train Epoch: 21 [213248/225000 (95%)] Loss: 9226.921875\n",
      "Train Epoch: 21 [217344/225000 (97%)] Loss: 8966.238281\n",
      "Train Epoch: 21 [221440/225000 (98%)] Loss: 8910.093750\n",
      "    epoch          : 21\n",
      "    loss           : 9040.462565104166\n",
      "    val_loss       : 8886.966933639682\n",
      "Train Epoch: 22 [256/225000 (0%)] Loss: 8752.548828\n",
      "Train Epoch: 22 [4352/225000 (2%)] Loss: 9218.109375\n",
      "Train Epoch: 22 [8448/225000 (4%)] Loss: 8780.449219\n",
      "Train Epoch: 22 [12544/225000 (6%)] Loss: 8944.263672\n",
      "Train Epoch: 22 [16640/225000 (7%)] Loss: 8951.792969\n",
      "Train Epoch: 22 [20736/225000 (9%)] Loss: 9184.785156\n",
      "Train Epoch: 22 [24832/225000 (11%)] Loss: 9021.542969\n",
      "Train Epoch: 22 [28928/225000 (13%)] Loss: 8913.068359\n",
      "Train Epoch: 22 [33024/225000 (15%)] Loss: 8857.392578\n",
      "Train Epoch: 22 [37120/225000 (16%)] Loss: 8870.779297\n",
      "Train Epoch: 22 [41216/225000 (18%)] Loss: 8807.341797\n",
      "Train Epoch: 22 [45312/225000 (20%)] Loss: 9094.271484\n",
      "Train Epoch: 22 [49408/225000 (22%)] Loss: 9112.183594\n",
      "Train Epoch: 22 [53504/225000 (24%)] Loss: 8707.808594\n",
      "Train Epoch: 22 [57600/225000 (26%)] Loss: 8780.613281\n",
      "Train Epoch: 22 [61696/225000 (27%)] Loss: 8779.986328\n",
      "Train Epoch: 22 [65792/225000 (29%)] Loss: 8905.949219\n",
      "Train Epoch: 22 [69888/225000 (31%)] Loss: 8880.013672\n",
      "Train Epoch: 22 [73984/225000 (33%)] Loss: 8958.816406\n",
      "Train Epoch: 22 [78080/225000 (35%)] Loss: 8685.031250\n",
      "Train Epoch: 22 [82176/225000 (37%)] Loss: 8762.443359\n",
      "Train Epoch: 22 [86272/225000 (38%)] Loss: 8834.160156\n",
      "Train Epoch: 22 [90368/225000 (40%)] Loss: 9114.794922\n",
      "Train Epoch: 22 [94464/225000 (42%)] Loss: 8731.662109\n",
      "Train Epoch: 22 [98560/225000 (44%)] Loss: 8792.537109\n",
      "Train Epoch: 22 [102656/225000 (46%)] Loss: 8776.351562\n",
      "Train Epoch: 22 [106752/225000 (47%)] Loss: 8801.111328\n",
      "Train Epoch: 22 [110848/225000 (49%)] Loss: 8883.203125\n",
      "Train Epoch: 22 [114944/225000 (51%)] Loss: 8718.806641\n",
      "Train Epoch: 22 [119040/225000 (53%)] Loss: 8773.679688\n",
      "Train Epoch: 22 [123136/225000 (55%)] Loss: 8886.384766\n",
      "Train Epoch: 22 [127232/225000 (57%)] Loss: 8796.634766\n",
      "Train Epoch: 22 [131328/225000 (58%)] Loss: 8870.898438\n",
      "Train Epoch: 22 [135424/225000 (60%)] Loss: 8773.402344\n",
      "Train Epoch: 22 [139520/225000 (62%)] Loss: 8826.757812\n",
      "Train Epoch: 22 [143616/225000 (64%)] Loss: 8617.962891\n",
      "Train Epoch: 22 [147712/225000 (66%)] Loss: 8824.822266\n",
      "Train Epoch: 22 [151808/225000 (67%)] Loss: 9060.505859\n",
      "Train Epoch: 22 [155904/225000 (69%)] Loss: 8784.744141\n",
      "Train Epoch: 22 [160000/225000 (71%)] Loss: 8982.173828\n",
      "Train Epoch: 22 [164096/225000 (73%)] Loss: 8747.916016\n",
      "Train Epoch: 22 [168192/225000 (75%)] Loss: 8888.919922\n",
      "Train Epoch: 22 [172288/225000 (77%)] Loss: 8766.970703\n",
      "Train Epoch: 22 [176384/225000 (78%)] Loss: 8773.089844\n",
      "Train Epoch: 22 [180480/225000 (80%)] Loss: 8904.728516\n",
      "Train Epoch: 22 [184576/225000 (82%)] Loss: 8946.730469\n",
      "Train Epoch: 22 [188672/225000 (84%)] Loss: 8788.554688\n",
      "Train Epoch: 22 [192768/225000 (86%)] Loss: 8850.816406\n",
      "Train Epoch: 22 [196864/225000 (87%)] Loss: 8814.748047\n",
      "Train Epoch: 22 [200960/225000 (89%)] Loss: 8902.843750\n",
      "Train Epoch: 22 [205056/225000 (91%)] Loss: 8715.535156\n",
      "Train Epoch: 22 [209152/225000 (93%)] Loss: 8687.330078\n",
      "Train Epoch: 22 [213248/225000 (95%)] Loss: 8861.605469\n",
      "Train Epoch: 22 [217344/225000 (97%)] Loss: 8880.386719\n",
      "Train Epoch: 22 [221440/225000 (98%)] Loss: 8828.376953\n",
      "    epoch          : 22\n",
      "    loss           : 8942.211464110494\n",
      "    val_loss       : 8801.021800679211\n",
      "Train Epoch: 23 [256/225000 (0%)] Loss: 8727.935547\n",
      "Train Epoch: 23 [4352/225000 (2%)] Loss: 8693.980469\n",
      "Train Epoch: 23 [8448/225000 (4%)] Loss: 8849.007812\n",
      "Train Epoch: 23 [12544/225000 (6%)] Loss: 9006.587891\n",
      "Train Epoch: 23 [16640/225000 (7%)] Loss: 8602.548828\n",
      "Train Epoch: 23 [20736/225000 (9%)] Loss: 8815.480469\n",
      "Train Epoch: 23 [24832/225000 (11%)] Loss: 8631.496094\n",
      "Train Epoch: 23 [28928/225000 (13%)] Loss: 8765.107422\n",
      "Train Epoch: 23 [33024/225000 (15%)] Loss: 8627.664062\n",
      "Train Epoch: 23 [37120/225000 (16%)] Loss: 8875.205078\n",
      "Train Epoch: 23 [41216/225000 (18%)] Loss: 8721.220703\n",
      "Train Epoch: 23 [45312/225000 (20%)] Loss: 8820.812500\n",
      "Train Epoch: 23 [49408/225000 (22%)] Loss: 8785.519531\n",
      "Train Epoch: 23 [53504/225000 (24%)] Loss: 8669.550781\n",
      "Train Epoch: 23 [57600/225000 (26%)] Loss: 8651.978516\n",
      "Train Epoch: 23 [61696/225000 (27%)] Loss: 8782.953125\n",
      "Train Epoch: 23 [65792/225000 (29%)] Loss: 8920.333984\n",
      "Train Epoch: 23 [69888/225000 (31%)] Loss: 8462.494141\n",
      "Train Epoch: 23 [73984/225000 (33%)] Loss: 8885.435547\n",
      "Train Epoch: 23 [78080/225000 (35%)] Loss: 10074.464844\n",
      "Train Epoch: 23 [82176/225000 (37%)] Loss: 8718.757812\n",
      "Train Epoch: 23 [86272/225000 (38%)] Loss: 8581.742188\n",
      "Train Epoch: 23 [90368/225000 (40%)] Loss: 8613.507812\n",
      "Train Epoch: 23 [94464/225000 (42%)] Loss: 8795.490234\n",
      "Train Epoch: 23 [98560/225000 (44%)] Loss: 8945.837891\n",
      "Train Epoch: 23 [102656/225000 (46%)] Loss: 8694.710938\n",
      "Train Epoch: 23 [106752/225000 (47%)] Loss: 8813.925781\n",
      "Train Epoch: 23 [110848/225000 (49%)] Loss: 8893.728516\n",
      "Train Epoch: 23 [114944/225000 (51%)] Loss: 8759.441406\n",
      "Train Epoch: 23 [119040/225000 (53%)] Loss: 8748.904297\n",
      "Train Epoch: 23 [123136/225000 (55%)] Loss: 8743.232422\n",
      "Train Epoch: 23 [127232/225000 (57%)] Loss: 8880.386719\n",
      "Train Epoch: 23 [131328/225000 (58%)] Loss: 8673.400391\n",
      "Train Epoch: 23 [135424/225000 (60%)] Loss: 8577.289062\n",
      "Train Epoch: 23 [139520/225000 (62%)] Loss: 8670.119141\n",
      "Train Epoch: 23 [143616/225000 (64%)] Loss: 8794.156250\n",
      "Train Epoch: 23 [147712/225000 (66%)] Loss: 8830.757812\n",
      "Train Epoch: 23 [151808/225000 (67%)] Loss: 8806.755859\n",
      "Train Epoch: 23 [155904/225000 (69%)] Loss: 8676.083984\n",
      "Train Epoch: 23 [160000/225000 (71%)] Loss: 8737.427734\n",
      "Train Epoch: 23 [164096/225000 (73%)] Loss: 8699.820312\n",
      "Train Epoch: 23 [168192/225000 (75%)] Loss: 8613.041016\n",
      "Train Epoch: 23 [172288/225000 (77%)] Loss: 8740.285156\n",
      "Train Epoch: 23 [176384/225000 (78%)] Loss: 8592.595703\n",
      "Train Epoch: 23 [180480/225000 (80%)] Loss: 8512.490234\n",
      "Train Epoch: 23 [184576/225000 (82%)] Loss: 8787.970703\n",
      "Train Epoch: 23 [188672/225000 (84%)] Loss: 8957.923828\n",
      "Train Epoch: 23 [192768/225000 (86%)] Loss: 8791.503906\n",
      "Train Epoch: 23 [196864/225000 (87%)] Loss: 8784.476562\n",
      "Train Epoch: 23 [200960/225000 (89%)] Loss: 8623.820312\n",
      "Train Epoch: 23 [205056/225000 (91%)] Loss: 8884.380859\n",
      "Train Epoch: 23 [209152/225000 (93%)] Loss: 8599.478516\n",
      "Train Epoch: 23 [213248/225000 (95%)] Loss: 8698.642578\n",
      "Train Epoch: 23 [217344/225000 (97%)] Loss: 8744.779297\n",
      "Train Epoch: 23 [221440/225000 (98%)] Loss: 8842.074219\n",
      "    epoch          : 23\n",
      "    loss           : 8906.188252141994\n",
      "    val_loss       : 8698.00068890349\n",
      "Train Epoch: 24 [256/225000 (0%)] Loss: 8703.271484\n",
      "Train Epoch: 24 [4352/225000 (2%)] Loss: 8504.296875\n",
      "Train Epoch: 24 [8448/225000 (4%)] Loss: 8821.103516\n",
      "Train Epoch: 24 [12544/225000 (6%)] Loss: 8480.351562\n",
      "Train Epoch: 24 [16640/225000 (7%)] Loss: 8670.619141\n",
      "Train Epoch: 24 [20736/225000 (9%)] Loss: 8552.728516\n",
      "Train Epoch: 24 [24832/225000 (11%)] Loss: 8643.417969\n",
      "Train Epoch: 24 [28928/225000 (13%)] Loss: 8748.871094\n",
      "Train Epoch: 24 [33024/225000 (15%)] Loss: 8510.201172\n",
      "Train Epoch: 24 [37120/225000 (16%)] Loss: 8605.650391\n",
      "Train Epoch: 24 [41216/225000 (18%)] Loss: 8649.265625\n",
      "Train Epoch: 24 [45312/225000 (20%)] Loss: 8607.826172\n",
      "Train Epoch: 24 [49408/225000 (22%)] Loss: 8777.105469\n",
      "Train Epoch: 24 [53504/225000 (24%)] Loss: 8669.857422\n",
      "Train Epoch: 24 [57600/225000 (26%)] Loss: 8619.726562\n",
      "Train Epoch: 24 [61696/225000 (27%)] Loss: 8877.898438\n",
      "Train Epoch: 24 [65792/225000 (29%)] Loss: 8576.763672\n",
      "Train Epoch: 24 [69888/225000 (31%)] Loss: 8588.761719\n",
      "Train Epoch: 24 [73984/225000 (33%)] Loss: 8699.587891\n",
      "Train Epoch: 24 [78080/225000 (35%)] Loss: 8699.361328\n",
      "Train Epoch: 24 [82176/225000 (37%)] Loss: 8750.927734\n",
      "Train Epoch: 24 [86272/225000 (38%)] Loss: 8538.806641\n",
      "Train Epoch: 24 [90368/225000 (40%)] Loss: 8728.851562\n",
      "Train Epoch: 24 [94464/225000 (42%)] Loss: 8776.652344\n",
      "Train Epoch: 24 [98560/225000 (44%)] Loss: 8920.914062\n",
      "Train Epoch: 24 [102656/225000 (46%)] Loss: 8843.388672\n",
      "Train Epoch: 24 [106752/225000 (47%)] Loss: 8789.248047\n",
      "Train Epoch: 24 [110848/225000 (49%)] Loss: 16464.546875\n",
      "Train Epoch: 24 [114944/225000 (51%)] Loss: 8732.357422\n",
      "Train Epoch: 24 [119040/225000 (53%)] Loss: 8626.435547\n",
      "Train Epoch: 24 [123136/225000 (55%)] Loss: 8504.480469\n",
      "Train Epoch: 24 [127232/225000 (57%)] Loss: 8572.386719\n",
      "Train Epoch: 24 [131328/225000 (58%)] Loss: 8468.660156\n",
      "Train Epoch: 24 [135424/225000 (60%)] Loss: 8368.847656\n",
      "Train Epoch: 24 [139520/225000 (62%)] Loss: 9905.726562\n",
      "Train Epoch: 24 [143616/225000 (64%)] Loss: 8652.767578\n",
      "Train Epoch: 24 [147712/225000 (66%)] Loss: 8626.296875\n",
      "Train Epoch: 24 [151808/225000 (67%)] Loss: 8807.246094\n",
      "Train Epoch: 24 [155904/225000 (69%)] Loss: 8579.298828\n",
      "Train Epoch: 24 [160000/225000 (71%)] Loss: 8646.013672\n",
      "Train Epoch: 24 [164096/225000 (73%)] Loss: 8704.613281\n",
      "Train Epoch: 24 [168192/225000 (75%)] Loss: 8468.281250\n",
      "Train Epoch: 24 [172288/225000 (77%)] Loss: 8648.697266\n",
      "Train Epoch: 24 [176384/225000 (78%)] Loss: 8556.195312\n",
      "Train Epoch: 24 [180480/225000 (80%)] Loss: 8516.814453\n",
      "Train Epoch: 24 [184576/225000 (82%)] Loss: 8822.330078\n",
      "Train Epoch: 24 [188672/225000 (84%)] Loss: 8731.617188\n",
      "Train Epoch: 24 [192768/225000 (86%)] Loss: 8790.820312\n",
      "Train Epoch: 24 [196864/225000 (87%)] Loss: 8807.705078\n",
      "Train Epoch: 24 [200960/225000 (89%)] Loss: 8658.238281\n",
      "Train Epoch: 24 [205056/225000 (91%)] Loss: 8497.716797\n",
      "Train Epoch: 24 [209152/225000 (93%)] Loss: 8700.230469\n",
      "Train Epoch: 24 [213248/225000 (95%)] Loss: 8588.074219\n",
      "Train Epoch: 24 [217344/225000 (97%)] Loss: 8704.408203\n",
      "Train Epoch: 24 [221440/225000 (98%)] Loss: 8695.986328\n",
      "    epoch          : 24\n",
      "    loss           : 8794.735424888011\n",
      "    val_loss       : 8683.500038032022\n",
      "Train Epoch: 25 [256/225000 (0%)] Loss: 8762.222656\n",
      "Train Epoch: 25 [4352/225000 (2%)] Loss: 8807.714844\n",
      "Train Epoch: 25 [8448/225000 (4%)] Loss: 8730.814453\n",
      "Train Epoch: 25 [12544/225000 (6%)] Loss: 8599.480469\n",
      "Train Epoch: 25 [16640/225000 (7%)] Loss: 8676.029297\n",
      "Train Epoch: 25 [20736/225000 (9%)] Loss: 16781.820312\n",
      "Train Epoch: 25 [24832/225000 (11%)] Loss: 8555.777344\n",
      "Train Epoch: 25 [28928/225000 (13%)] Loss: 8516.910156\n",
      "Train Epoch: 25 [33024/225000 (15%)] Loss: 8782.677734\n",
      "Train Epoch: 25 [37120/225000 (16%)] Loss: 8579.279297\n",
      "Train Epoch: 25 [41216/225000 (18%)] Loss: 8557.750000\n",
      "Train Epoch: 25 [45312/225000 (20%)] Loss: 8548.031250\n",
      "Train Epoch: 25 [49408/225000 (22%)] Loss: 8637.638672\n",
      "Train Epoch: 25 [53504/225000 (24%)] Loss: 8421.912109\n",
      "Train Epoch: 25 [57600/225000 (26%)] Loss: 8795.773438\n",
      "Train Epoch: 25 [61696/225000 (27%)] Loss: 8608.898438\n",
      "Train Epoch: 25 [65792/225000 (29%)] Loss: 8837.781250\n",
      "Train Epoch: 25 [69888/225000 (31%)] Loss: 8699.451172\n",
      "Train Epoch: 25 [73984/225000 (33%)] Loss: 8437.111328\n",
      "Train Epoch: 25 [78080/225000 (35%)] Loss: 8544.480469\n",
      "Train Epoch: 25 [82176/225000 (37%)] Loss: 8756.070312\n",
      "Train Epoch: 25 [86272/225000 (38%)] Loss: 8778.638672\n",
      "Train Epoch: 25 [90368/225000 (40%)] Loss: 8586.419922\n",
      "Train Epoch: 25 [94464/225000 (42%)] Loss: 8701.974609\n",
      "Train Epoch: 25 [98560/225000 (44%)] Loss: 8653.208984\n",
      "Train Epoch: 25 [102656/225000 (46%)] Loss: 8372.640625\n",
      "Train Epoch: 25 [106752/225000 (47%)] Loss: 8545.796875\n",
      "Train Epoch: 25 [110848/225000 (49%)] Loss: 8573.191406\n",
      "Train Epoch: 25 [114944/225000 (51%)] Loss: 8747.312500\n",
      "Train Epoch: 25 [119040/225000 (53%)] Loss: 8737.267578\n",
      "Train Epoch: 25 [123136/225000 (55%)] Loss: 8383.029297\n",
      "Train Epoch: 25 [127232/225000 (57%)] Loss: 8788.628906\n",
      "Train Epoch: 25 [131328/225000 (58%)] Loss: 8687.396484\n",
      "Train Epoch: 25 [135424/225000 (60%)] Loss: 8494.724609\n",
      "Train Epoch: 25 [139520/225000 (62%)] Loss: 8696.150391\n",
      "Train Epoch: 25 [143616/225000 (64%)] Loss: 8712.675781\n",
      "Train Epoch: 25 [147712/225000 (66%)] Loss: 8588.556641\n",
      "Train Epoch: 25 [151808/225000 (67%)] Loss: 8935.689453\n",
      "Train Epoch: 25 [155904/225000 (69%)] Loss: 8840.541016\n",
      "Train Epoch: 25 [160000/225000 (71%)] Loss: 8596.349609\n",
      "Train Epoch: 25 [164096/225000 (73%)] Loss: 8491.673828\n",
      "Train Epoch: 25 [168192/225000 (75%)] Loss: 8497.751953\n",
      "Train Epoch: 25 [172288/225000 (77%)] Loss: 8703.105469\n",
      "Train Epoch: 25 [176384/225000 (78%)] Loss: 8832.482422\n",
      "Train Epoch: 25 [180480/225000 (80%)] Loss: 8606.972656\n",
      "Train Epoch: 25 [184576/225000 (82%)] Loss: 8604.656250\n",
      "Train Epoch: 25 [188672/225000 (84%)] Loss: 8389.328125\n",
      "Train Epoch: 25 [192768/225000 (86%)] Loss: 8545.246094\n",
      "Train Epoch: 25 [196864/225000 (87%)] Loss: 8446.792969\n",
      "Train Epoch: 25 [200960/225000 (89%)] Loss: 8587.732422\n",
      "Train Epoch: 25 [205056/225000 (91%)] Loss: 8518.654297\n",
      "Train Epoch: 25 [209152/225000 (93%)] Loss: 8796.703125\n",
      "Train Epoch: 25 [213248/225000 (95%)] Loss: 8548.796875\n",
      "Train Epoch: 25 [217344/225000 (97%)] Loss: 8521.644531\n",
      "Train Epoch: 25 [221440/225000 (98%)] Loss: 8541.341797\n",
      "    epoch          : 25\n",
      "    loss           : 8677.499545604025\n",
      "    val_loss       : 8618.150300410329\n",
      "Train Epoch: 26 [256/225000 (0%)] Loss: 8641.000000\n",
      "Train Epoch: 26 [4352/225000 (2%)] Loss: 8690.550781\n",
      "Train Epoch: 26 [8448/225000 (4%)] Loss: 8546.589844\n",
      "Train Epoch: 26 [12544/225000 (6%)] Loss: 8622.619141\n",
      "Train Epoch: 26 [16640/225000 (7%)] Loss: 8736.197266\n",
      "Train Epoch: 26 [20736/225000 (9%)] Loss: 8666.017578\n",
      "Train Epoch: 26 [24832/225000 (11%)] Loss: 8568.976562\n",
      "Train Epoch: 26 [28928/225000 (13%)] Loss: 8534.884766\n",
      "Train Epoch: 26 [33024/225000 (15%)] Loss: 8518.365234\n",
      "Train Epoch: 26 [37120/225000 (16%)] Loss: 8545.869141\n",
      "Train Epoch: 26 [41216/225000 (18%)] Loss: 8414.742188\n",
      "Train Epoch: 26 [45312/225000 (20%)] Loss: 8638.062500\n",
      "Train Epoch: 26 [49408/225000 (22%)] Loss: 8497.203125\n",
      "Train Epoch: 26 [53504/225000 (24%)] Loss: 8294.126953\n",
      "Train Epoch: 26 [57600/225000 (26%)] Loss: 8465.685547\n",
      "Train Epoch: 26 [61696/225000 (27%)] Loss: 8644.359375\n",
      "Train Epoch: 26 [65792/225000 (29%)] Loss: 8415.828125\n",
      "Train Epoch: 26 [69888/225000 (31%)] Loss: 8550.845703\n",
      "Train Epoch: 26 [73984/225000 (33%)] Loss: 8398.521484\n",
      "Train Epoch: 26 [78080/225000 (35%)] Loss: 8545.705078\n",
      "Train Epoch: 26 [82176/225000 (37%)] Loss: 8368.707031\n",
      "Train Epoch: 26 [86272/225000 (38%)] Loss: 8572.984375\n",
      "Train Epoch: 26 [90368/225000 (40%)] Loss: 8510.238281\n",
      "Train Epoch: 26 [94464/225000 (42%)] Loss: 8696.789062\n",
      "Train Epoch: 26 [98560/225000 (44%)] Loss: 8520.568359\n",
      "Train Epoch: 26 [102656/225000 (46%)] Loss: 8421.628906\n",
      "Train Epoch: 26 [106752/225000 (47%)] Loss: 8455.937500\n",
      "Train Epoch: 26 [110848/225000 (49%)] Loss: 8636.039062\n",
      "Train Epoch: 26 [114944/225000 (51%)] Loss: 8561.417969\n",
      "Train Epoch: 26 [119040/225000 (53%)] Loss: 8718.263672\n",
      "Train Epoch: 26 [123136/225000 (55%)] Loss: 8383.119141\n",
      "Train Epoch: 26 [127232/225000 (57%)] Loss: 8420.191406\n",
      "Train Epoch: 26 [131328/225000 (58%)] Loss: 8560.476562\n",
      "Train Epoch: 26 [135424/225000 (60%)] Loss: 8582.781250\n",
      "Train Epoch: 26 [139520/225000 (62%)] Loss: 8377.939453\n",
      "Train Epoch: 26 [143616/225000 (64%)] Loss: 8571.125000\n",
      "Train Epoch: 26 [147712/225000 (66%)] Loss: 8306.398438\n",
      "Train Epoch: 26 [151808/225000 (67%)] Loss: 8534.251953\n",
      "Train Epoch: 26 [155904/225000 (69%)] Loss: 8281.636719\n",
      "Train Epoch: 26 [160000/225000 (71%)] Loss: 8528.447266\n",
      "Train Epoch: 26 [164096/225000 (73%)] Loss: 8566.892578\n",
      "Train Epoch: 26 [168192/225000 (75%)] Loss: 8746.654297\n",
      "Train Epoch: 26 [172288/225000 (77%)] Loss: 8717.529297\n",
      "Train Epoch: 26 [176384/225000 (78%)] Loss: 9780.306641\n",
      "Train Epoch: 26 [180480/225000 (80%)] Loss: 8482.101562\n",
      "Train Epoch: 26 [184576/225000 (82%)] Loss: 8214.673828\n",
      "Train Epoch: 26 [188672/225000 (84%)] Loss: 8650.064453\n",
      "Train Epoch: 26 [192768/225000 (86%)] Loss: 8384.863281\n",
      "Train Epoch: 26 [196864/225000 (87%)] Loss: 8704.181641\n",
      "Train Epoch: 26 [200960/225000 (89%)] Loss: 8645.996094\n",
      "Train Epoch: 26 [205056/225000 (91%)] Loss: 8437.490234\n",
      "Train Epoch: 26 [209152/225000 (93%)] Loss: 8517.394531\n",
      "Train Epoch: 26 [213248/225000 (95%)] Loss: 8739.078125\n",
      "Train Epoch: 26 [217344/225000 (97%)] Loss: 8475.402344\n",
      "Train Epoch: 26 [221440/225000 (98%)] Loss: 8389.667969\n",
      "    epoch          : 26\n",
      "    loss           : 8579.000702147327\n",
      "    val_loss       : 8482.067006501617\n",
      "Train Epoch: 27 [256/225000 (0%)] Loss: 8520.285156\n",
      "Train Epoch: 27 [4352/225000 (2%)] Loss: 8356.417969\n",
      "Train Epoch: 27 [8448/225000 (4%)] Loss: 8557.023438\n",
      "Train Epoch: 27 [12544/225000 (6%)] Loss: 8523.832031\n",
      "Train Epoch: 27 [16640/225000 (7%)] Loss: 8630.625000\n",
      "Train Epoch: 27 [20736/225000 (9%)] Loss: 8339.472656\n",
      "Train Epoch: 27 [24832/225000 (11%)] Loss: 8446.339844\n",
      "Train Epoch: 27 [28928/225000 (13%)] Loss: 8534.455078\n",
      "Train Epoch: 27 [33024/225000 (15%)] Loss: 8288.771484\n",
      "Train Epoch: 27 [37120/225000 (16%)] Loss: 8455.824219\n",
      "Train Epoch: 27 [41216/225000 (18%)] Loss: 8596.349609\n",
      "Train Epoch: 27 [45312/225000 (20%)] Loss: 8313.500000\n",
      "Train Epoch: 27 [49408/225000 (22%)] Loss: 8428.867188\n",
      "Train Epoch: 27 [53504/225000 (24%)] Loss: 8273.007812\n",
      "Train Epoch: 27 [57600/225000 (26%)] Loss: 8449.041016\n",
      "Train Epoch: 27 [61696/225000 (27%)] Loss: 8425.146484\n",
      "Train Epoch: 27 [65792/225000 (29%)] Loss: 8580.441406\n",
      "Train Epoch: 27 [69888/225000 (31%)] Loss: 8330.410156\n",
      "Train Epoch: 27 [73984/225000 (33%)] Loss: 8443.705078\n",
      "Train Epoch: 27 [78080/225000 (35%)] Loss: 8468.853516\n",
      "Train Epoch: 27 [82176/225000 (37%)] Loss: 8295.755859\n",
      "Train Epoch: 27 [86272/225000 (38%)] Loss: 8638.792969\n",
      "Train Epoch: 27 [90368/225000 (40%)] Loss: 8563.490234\n",
      "Train Epoch: 27 [94464/225000 (42%)] Loss: 8428.058594\n",
      "Train Epoch: 27 [98560/225000 (44%)] Loss: 8450.279297\n",
      "Train Epoch: 27 [102656/225000 (46%)] Loss: 8382.400391\n",
      "Train Epoch: 27 [106752/225000 (47%)] Loss: 8439.197266\n",
      "Train Epoch: 27 [110848/225000 (49%)] Loss: 8410.714844\n",
      "Train Epoch: 27 [114944/225000 (51%)] Loss: 8192.996094\n",
      "Train Epoch: 27 [119040/225000 (53%)] Loss: 8477.802734\n",
      "Train Epoch: 27 [123136/225000 (55%)] Loss: 8618.695312\n",
      "Train Epoch: 27 [127232/225000 (57%)] Loss: 8477.890625\n",
      "Train Epoch: 27 [131328/225000 (58%)] Loss: 8349.900391\n",
      "Train Epoch: 27 [135424/225000 (60%)] Loss: 8503.173828\n",
      "Train Epoch: 27 [139520/225000 (62%)] Loss: 8571.830078\n",
      "Train Epoch: 27 [143616/225000 (64%)] Loss: 8288.082031\n",
      "Train Epoch: 27 [147712/225000 (66%)] Loss: 8506.285156\n",
      "Train Epoch: 27 [151808/225000 (67%)] Loss: 8479.208984\n",
      "Train Epoch: 27 [155904/225000 (69%)] Loss: 8517.923828\n",
      "Train Epoch: 27 [160000/225000 (71%)] Loss: 8549.210938\n",
      "Train Epoch: 27 [164096/225000 (73%)] Loss: 8259.117188\n",
      "Train Epoch: 27 [168192/225000 (75%)] Loss: 8417.417969\n",
      "Train Epoch: 27 [172288/225000 (77%)] Loss: 8498.449219\n",
      "Train Epoch: 27 [176384/225000 (78%)] Loss: 8363.488281\n",
      "Train Epoch: 27 [180480/225000 (80%)] Loss: 8359.652344\n",
      "Train Epoch: 27 [184576/225000 (82%)] Loss: 8493.810547\n",
      "Train Epoch: 27 [188672/225000 (84%)] Loss: 8598.962891\n",
      "Train Epoch: 27 [192768/225000 (86%)] Loss: 8454.072266\n",
      "Train Epoch: 27 [196864/225000 (87%)] Loss: 8331.150391\n",
      "Train Epoch: 27 [200960/225000 (89%)] Loss: 8359.810547\n",
      "Train Epoch: 27 [205056/225000 (91%)] Loss: 8307.947266\n",
      "Train Epoch: 27 [209152/225000 (93%)] Loss: 8383.837891\n",
      "Train Epoch: 27 [213248/225000 (95%)] Loss: 8586.025391\n",
      "Train Epoch: 27 [217344/225000 (97%)] Loss: 8370.957031\n",
      "Train Epoch: 27 [221440/225000 (98%)] Loss: 8344.972656\n",
      "    epoch          : 27\n",
      "    loss           : 8575.08120022753\n",
      "    val_loss       : 8622.995035717682\n",
      "Train Epoch: 28 [256/225000 (0%)] Loss: 8459.951172\n",
      "Train Epoch: 28 [4352/225000 (2%)] Loss: 8568.785156\n",
      "Train Epoch: 28 [8448/225000 (4%)] Loss: 8362.101562\n",
      "Train Epoch: 28 [12544/225000 (6%)] Loss: 8338.539062\n",
      "Train Epoch: 28 [16640/225000 (7%)] Loss: 8377.318359\n",
      "Train Epoch: 28 [20736/225000 (9%)] Loss: 8273.322266\n",
      "Train Epoch: 28 [24832/225000 (11%)] Loss: 8264.822266\n",
      "Train Epoch: 28 [28928/225000 (13%)] Loss: 9935.542969\n",
      "Train Epoch: 28 [33024/225000 (15%)] Loss: 8421.443359\n",
      "Train Epoch: 28 [37120/225000 (16%)] Loss: 8406.777344\n",
      "Train Epoch: 28 [41216/225000 (18%)] Loss: 8392.341797\n",
      "Train Epoch: 28 [45312/225000 (20%)] Loss: 8345.705078\n",
      "Train Epoch: 28 [49408/225000 (22%)] Loss: 8296.904297\n",
      "Train Epoch: 28 [53504/225000 (24%)] Loss: 8399.136719\n",
      "Train Epoch: 28 [57600/225000 (26%)] Loss: 8347.958984\n",
      "Train Epoch: 28 [61696/225000 (27%)] Loss: 8498.656250\n",
      "Train Epoch: 28 [65792/225000 (29%)] Loss: 8381.154297\n",
      "Train Epoch: 28 [69888/225000 (31%)] Loss: 8350.810547\n",
      "Train Epoch: 28 [73984/225000 (33%)] Loss: 8285.408203\n",
      "Train Epoch: 28 [78080/225000 (35%)] Loss: 8354.013672\n",
      "Train Epoch: 28 [82176/225000 (37%)] Loss: 8308.089844\n",
      "Train Epoch: 28 [86272/225000 (38%)] Loss: 8372.806641\n",
      "Train Epoch: 28 [90368/225000 (40%)] Loss: 8537.128906\n",
      "Train Epoch: 28 [94464/225000 (42%)] Loss: 8350.746094\n",
      "Train Epoch: 28 [98560/225000 (44%)] Loss: 8742.550781\n",
      "Train Epoch: 28 [102656/225000 (46%)] Loss: 7973.542969\n",
      "Train Epoch: 28 [106752/225000 (47%)] Loss: 8401.896484\n",
      "Train Epoch: 28 [110848/225000 (49%)] Loss: 8563.546875\n",
      "Train Epoch: 28 [114944/225000 (51%)] Loss: 8461.642578\n",
      "Train Epoch: 28 [119040/225000 (53%)] Loss: 8387.224609\n",
      "Train Epoch: 28 [123136/225000 (55%)] Loss: 8298.984375\n",
      "Train Epoch: 28 [127232/225000 (57%)] Loss: 8432.556641\n",
      "Train Epoch: 28 [131328/225000 (58%)] Loss: 8292.072266\n",
      "Train Epoch: 28 [135424/225000 (60%)] Loss: 8349.894531\n",
      "Train Epoch: 28 [139520/225000 (62%)] Loss: 8219.302734\n",
      "Train Epoch: 28 [143616/225000 (64%)] Loss: 8220.777344\n",
      "Train Epoch: 28 [147712/225000 (66%)] Loss: 8248.648438\n",
      "Train Epoch: 28 [151808/225000 (67%)] Loss: 8381.232422\n",
      "Train Epoch: 28 [155904/225000 (69%)] Loss: 8438.650391\n",
      "Train Epoch: 28 [160000/225000 (71%)] Loss: 8368.095703\n",
      "Train Epoch: 28 [164096/225000 (73%)] Loss: 8245.802734\n",
      "Train Epoch: 28 [168192/225000 (75%)] Loss: 8508.716797\n",
      "Train Epoch: 28 [172288/225000 (77%)] Loss: 8267.578125\n",
      "Train Epoch: 28 [176384/225000 (78%)] Loss: 8433.261719\n",
      "Train Epoch: 28 [180480/225000 (80%)] Loss: 8245.929688\n",
      "Train Epoch: 28 [184576/225000 (82%)] Loss: 8388.226562\n",
      "Train Epoch: 28 [188672/225000 (84%)] Loss: 8441.310547\n",
      "Train Epoch: 28 [192768/225000 (86%)] Loss: 8480.501953\n",
      "Train Epoch: 28 [196864/225000 (87%)] Loss: 8438.736328\n",
      "Train Epoch: 28 [200960/225000 (89%)] Loss: 8265.755859\n",
      "Train Epoch: 28 [205056/225000 (91%)] Loss: 8300.851562\n",
      "Train Epoch: 28 [209152/225000 (93%)] Loss: 8265.500000\n",
      "Train Epoch: 28 [213248/225000 (95%)] Loss: 8315.730469\n",
      "Train Epoch: 28 [217344/225000 (97%)] Loss: 8384.621094\n",
      "Train Epoch: 28 [221440/225000 (98%)] Loss: 8209.681641\n",
      "    epoch          : 28\n",
      "    loss           : 8553.222479602176\n",
      "    val_loss       : 8308.755939472694\n",
      "Train Epoch: 29 [256/225000 (0%)] Loss: 8425.578125\n",
      "Train Epoch: 29 [4352/225000 (2%)] Loss: 8328.410156\n",
      "Train Epoch: 29 [8448/225000 (4%)] Loss: 8312.160156\n",
      "Train Epoch: 29 [12544/225000 (6%)] Loss: 8332.556641\n",
      "Train Epoch: 29 [16640/225000 (7%)] Loss: 8272.888672\n",
      "Train Epoch: 29 [20736/225000 (9%)] Loss: 8386.597656\n",
      "Train Epoch: 29 [24832/225000 (11%)] Loss: 8470.421875\n",
      "Train Epoch: 29 [28928/225000 (13%)] Loss: 8155.328125\n",
      "Train Epoch: 29 [33024/225000 (15%)] Loss: 8559.710938\n",
      "Train Epoch: 29 [37120/225000 (16%)] Loss: 8109.560547\n",
      "Train Epoch: 29 [41216/225000 (18%)] Loss: 8433.117188\n",
      "Train Epoch: 29 [45312/225000 (20%)] Loss: 8220.580078\n",
      "Train Epoch: 29 [49408/225000 (22%)] Loss: 8359.457031\n",
      "Train Epoch: 29 [53504/225000 (24%)] Loss: 8238.351562\n",
      "Train Epoch: 29 [57600/225000 (26%)] Loss: 8217.470703\n",
      "Train Epoch: 29 [61696/225000 (27%)] Loss: 8422.029297\n",
      "Train Epoch: 29 [65792/225000 (29%)] Loss: 8376.035156\n",
      "Train Epoch: 29 [69888/225000 (31%)] Loss: 8417.222656\n",
      "Train Epoch: 29 [73984/225000 (33%)] Loss: 8253.070312\n",
      "Train Epoch: 29 [78080/225000 (35%)] Loss: 8412.779297\n",
      "Train Epoch: 29 [82176/225000 (37%)] Loss: 8331.117188\n",
      "Train Epoch: 29 [86272/225000 (38%)] Loss: 8369.753906\n",
      "Train Epoch: 29 [90368/225000 (40%)] Loss: 8319.761719\n",
      "Train Epoch: 29 [94464/225000 (42%)] Loss: 8169.488281\n",
      "Train Epoch: 29 [98560/225000 (44%)] Loss: 8505.382812\n",
      "Train Epoch: 29 [102656/225000 (46%)] Loss: 8627.546875\n",
      "Train Epoch: 29 [106752/225000 (47%)] Loss: 8386.984375\n",
      "Train Epoch: 29 [110848/225000 (49%)] Loss: 8293.197266\n",
      "Train Epoch: 29 [114944/225000 (51%)] Loss: 8202.349609\n",
      "Train Epoch: 29 [119040/225000 (53%)] Loss: 8242.791016\n",
      "Train Epoch: 29 [123136/225000 (55%)] Loss: 8237.343750\n",
      "Train Epoch: 29 [127232/225000 (57%)] Loss: 8379.482422\n",
      "Train Epoch: 29 [131328/225000 (58%)] Loss: 8259.894531\n",
      "Train Epoch: 29 [135424/225000 (60%)] Loss: 8357.794922\n",
      "Train Epoch: 29 [139520/225000 (62%)] Loss: 8426.285156\n",
      "Train Epoch: 29 [143616/225000 (64%)] Loss: 8535.707031\n",
      "Train Epoch: 29 [147712/225000 (66%)] Loss: 8410.671875\n",
      "Train Epoch: 29 [151808/225000 (67%)] Loss: 8234.341797\n",
      "Train Epoch: 29 [155904/225000 (69%)] Loss: 8278.066406\n",
      "Train Epoch: 29 [160000/225000 (71%)] Loss: 8393.343750\n",
      "Train Epoch: 29 [164096/225000 (73%)] Loss: 8302.248047\n",
      "Train Epoch: 29 [168192/225000 (75%)] Loss: 8286.437500\n",
      "Train Epoch: 29 [172288/225000 (77%)] Loss: 8382.634766\n",
      "Train Epoch: 29 [176384/225000 (78%)] Loss: 8293.890625\n",
      "Train Epoch: 29 [180480/225000 (80%)] Loss: 8427.361328\n",
      "Train Epoch: 29 [184576/225000 (82%)] Loss: 8306.554688\n",
      "Train Epoch: 29 [188672/225000 (84%)] Loss: 8439.484375\n",
      "Train Epoch: 29 [192768/225000 (86%)] Loss: 8261.904297\n",
      "Train Epoch: 29 [196864/225000 (87%)] Loss: 8228.972656\n",
      "Train Epoch: 29 [200960/225000 (89%)] Loss: 8102.105469\n",
      "Train Epoch: 29 [205056/225000 (91%)] Loss: 8172.187500\n",
      "Train Epoch: 29 [209152/225000 (93%)] Loss: 8339.425781\n",
      "Train Epoch: 29 [213248/225000 (95%)] Loss: 8300.451172\n",
      "Train Epoch: 29 [217344/225000 (97%)] Loss: 8291.207031\n",
      "Train Epoch: 29 [221440/225000 (98%)] Loss: 8345.949219\n",
      "    epoch          : 29\n",
      "    loss           : 8351.052489956626\n",
      "    val_loss       : 8278.03274978667\n",
      "Train Epoch: 30 [256/225000 (0%)] Loss: 8246.701172\n",
      "Train Epoch: 30 [4352/225000 (2%)] Loss: 8155.365234\n",
      "Train Epoch: 30 [8448/225000 (4%)] Loss: 8209.878906\n",
      "Train Epoch: 30 [12544/225000 (6%)] Loss: 8163.568359\n",
      "Train Epoch: 30 [16640/225000 (7%)] Loss: 8174.794922\n",
      "Train Epoch: 30 [20736/225000 (9%)] Loss: 8459.144531\n",
      "Train Epoch: 30 [24832/225000 (11%)] Loss: 8125.535156\n",
      "Train Epoch: 30 [28928/225000 (13%)] Loss: 8277.759766\n",
      "Train Epoch: 30 [33024/225000 (15%)] Loss: 8445.060547\n",
      "Train Epoch: 30 [37120/225000 (16%)] Loss: 8384.574219\n",
      "Train Epoch: 30 [41216/225000 (18%)] Loss: 8183.181641\n",
      "Train Epoch: 30 [45312/225000 (20%)] Loss: 8271.355469\n",
      "Train Epoch: 30 [49408/225000 (22%)] Loss: 8105.328125\n",
      "Train Epoch: 30 [53504/225000 (24%)] Loss: 8208.072266\n",
      "Train Epoch: 30 [57600/225000 (26%)] Loss: 8474.890625\n",
      "Train Epoch: 30 [61696/225000 (27%)] Loss: 8245.224609\n",
      "Train Epoch: 30 [65792/225000 (29%)] Loss: 8285.052734\n",
      "Train Epoch: 30 [69888/225000 (31%)] Loss: 8123.695312\n",
      "Train Epoch: 30 [73984/225000 (33%)] Loss: 8007.853516\n",
      "Train Epoch: 30 [78080/225000 (35%)] Loss: 8254.947266\n",
      "Train Epoch: 30 [82176/225000 (37%)] Loss: 8362.703125\n",
      "Train Epoch: 30 [86272/225000 (38%)] Loss: 8204.818359\n",
      "Train Epoch: 30 [90368/225000 (40%)] Loss: 8320.556641\n",
      "Train Epoch: 30 [94464/225000 (42%)] Loss: 8344.906250\n",
      "Train Epoch: 30 [98560/225000 (44%)] Loss: 7999.869141\n",
      "Train Epoch: 30 [102656/225000 (46%)] Loss: 8117.796875\n",
      "Train Epoch: 30 [106752/225000 (47%)] Loss: 8438.138672\n",
      "Train Epoch: 30 [110848/225000 (49%)] Loss: 8359.542969\n",
      "Train Epoch: 30 [114944/225000 (51%)] Loss: 8352.621094\n",
      "Train Epoch: 30 [119040/225000 (53%)] Loss: 8378.496094\n",
      "Train Epoch: 30 [123136/225000 (55%)] Loss: 8179.851562\n",
      "Train Epoch: 30 [127232/225000 (57%)] Loss: 8335.580078\n",
      "Train Epoch: 30 [131328/225000 (58%)] Loss: 8224.054688\n",
      "Train Epoch: 30 [135424/225000 (60%)] Loss: 8285.734375\n",
      "Train Epoch: 30 [139520/225000 (62%)] Loss: 8266.457031\n",
      "Train Epoch: 30 [143616/225000 (64%)] Loss: 8255.890625\n",
      "Train Epoch: 30 [147712/225000 (66%)] Loss: 8230.314453\n",
      "Train Epoch: 30 [151808/225000 (67%)] Loss: 8395.292969\n",
      "Train Epoch: 30 [155904/225000 (69%)] Loss: 8137.533203\n",
      "Train Epoch: 30 [160000/225000 (71%)] Loss: 8222.251953\n",
      "Train Epoch: 30 [164096/225000 (73%)] Loss: 8228.769531\n",
      "Train Epoch: 30 [168192/225000 (75%)] Loss: 8278.849609\n",
      "Train Epoch: 30 [172288/225000 (77%)] Loss: 8314.781250\n",
      "Train Epoch: 30 [176384/225000 (78%)] Loss: 8144.457031\n",
      "Train Epoch: 30 [180480/225000 (80%)] Loss: 8334.908203\n",
      "Train Epoch: 30 [184576/225000 (82%)] Loss: 8127.494141\n",
      "Train Epoch: 30 [188672/225000 (84%)] Loss: 8222.201172\n",
      "Train Epoch: 30 [192768/225000 (86%)] Loss: 8196.251953\n",
      "Train Epoch: 30 [196864/225000 (87%)] Loss: 8036.960938\n",
      "Train Epoch: 30 [200960/225000 (89%)] Loss: 8128.521484\n",
      "Train Epoch: 30 [205056/225000 (91%)] Loss: 8182.580078\n",
      "Train Epoch: 30 [209152/225000 (93%)] Loss: 8123.074219\n",
      "Train Epoch: 30 [213248/225000 (95%)] Loss: 8221.214844\n",
      "Train Epoch: 30 [217344/225000 (97%)] Loss: 8161.054688\n",
      "Train Epoch: 30 [221440/225000 (98%)] Loss: 8168.093750\n",
      "    epoch          : 30\n",
      "    loss           : 8347.620548252631\n",
      "    val_loss       : 8234.042263878851\n",
      "Train Epoch: 31 [256/225000 (0%)] Loss: 8142.542969\n",
      "Train Epoch: 31 [4352/225000 (2%)] Loss: 8156.464844\n",
      "Train Epoch: 31 [8448/225000 (4%)] Loss: 8138.269531\n",
      "Train Epoch: 31 [12544/225000 (6%)] Loss: 8431.195312\n",
      "Train Epoch: 31 [16640/225000 (7%)] Loss: 8150.617188\n",
      "Train Epoch: 31 [20736/225000 (9%)] Loss: 8258.433594\n",
      "Train Epoch: 31 [24832/225000 (11%)] Loss: 8177.937500\n",
      "Train Epoch: 31 [28928/225000 (13%)] Loss: 8018.925781\n",
      "Train Epoch: 31 [33024/225000 (15%)] Loss: 8576.035156\n",
      "Train Epoch: 31 [37120/225000 (16%)] Loss: 8396.242188\n",
      "Train Epoch: 31 [41216/225000 (18%)] Loss: 8222.169922\n",
      "Train Epoch: 31 [45312/225000 (20%)] Loss: 8226.058594\n",
      "Train Epoch: 31 [49408/225000 (22%)] Loss: 10000.136719\n",
      "Train Epoch: 31 [53504/225000 (24%)] Loss: 8306.650391\n",
      "Train Epoch: 31 [57600/225000 (26%)] Loss: 8050.966797\n",
      "Train Epoch: 31 [61696/225000 (27%)] Loss: 8125.132812\n",
      "Train Epoch: 31 [65792/225000 (29%)] Loss: 8265.421875\n",
      "Train Epoch: 31 [69888/225000 (31%)] Loss: 8165.695312\n",
      "Train Epoch: 31 [73984/225000 (33%)] Loss: 7993.580078\n",
      "Train Epoch: 31 [78080/225000 (35%)] Loss: 8094.548828\n",
      "Train Epoch: 31 [82176/225000 (37%)] Loss: 8203.652344\n",
      "Train Epoch: 31 [86272/225000 (38%)] Loss: 8187.505859\n",
      "Train Epoch: 31 [90368/225000 (40%)] Loss: 8045.617188\n",
      "Train Epoch: 31 [94464/225000 (42%)] Loss: 8202.255859\n",
      "Train Epoch: 31 [98560/225000 (44%)] Loss: 8076.769531\n",
      "Train Epoch: 31 [102656/225000 (46%)] Loss: 8294.570312\n",
      "Train Epoch: 31 [106752/225000 (47%)] Loss: 8129.976562\n",
      "Train Epoch: 31 [110848/225000 (49%)] Loss: 8061.013672\n",
      "Train Epoch: 31 [114944/225000 (51%)] Loss: 8108.546875\n",
      "Train Epoch: 31 [119040/225000 (53%)] Loss: 8047.876953\n",
      "Train Epoch: 31 [123136/225000 (55%)] Loss: 8142.066406\n",
      "Train Epoch: 31 [127232/225000 (57%)] Loss: 8295.000000\n",
      "Train Epoch: 31 [131328/225000 (58%)] Loss: 8099.214844\n",
      "Train Epoch: 31 [135424/225000 (60%)] Loss: 8322.484375\n",
      "Train Epoch: 31 [139520/225000 (62%)] Loss: 8206.564453\n",
      "Train Epoch: 31 [143616/225000 (64%)] Loss: 8090.337891\n",
      "Train Epoch: 31 [147712/225000 (66%)] Loss: 8071.757812\n",
      "Train Epoch: 31 [151808/225000 (67%)] Loss: 8146.425781\n",
      "Train Epoch: 31 [155904/225000 (69%)] Loss: 8253.902344\n",
      "Train Epoch: 31 [160000/225000 (71%)] Loss: 8229.919922\n",
      "Train Epoch: 31 [164096/225000 (73%)] Loss: 8200.880859\n",
      "Train Epoch: 31 [168192/225000 (75%)] Loss: 8162.427734\n",
      "Train Epoch: 31 [172288/225000 (77%)] Loss: 8276.859375\n",
      "Train Epoch: 31 [176384/225000 (78%)] Loss: 8023.802734\n",
      "Train Epoch: 31 [180480/225000 (80%)] Loss: 8186.869141\n",
      "Train Epoch: 31 [184576/225000 (82%)] Loss: 9676.751953\n",
      "Train Epoch: 31 [188672/225000 (84%)] Loss: 8268.064453\n",
      "Train Epoch: 31 [192768/225000 (86%)] Loss: 8243.710938\n",
      "Train Epoch: 31 [196864/225000 (87%)] Loss: 8143.609375\n",
      "Train Epoch: 31 [200960/225000 (89%)] Loss: 8219.398438\n",
      "Train Epoch: 31 [205056/225000 (91%)] Loss: 8474.115234\n",
      "Train Epoch: 31 [209152/225000 (93%)] Loss: 8128.767578\n",
      "Train Epoch: 31 [213248/225000 (95%)] Loss: 8157.082031\n",
      "Train Epoch: 31 [217344/225000 (97%)] Loss: 8214.652344\n",
      "Train Epoch: 31 [221440/225000 (98%)] Loss: 8106.916016\n",
      "    epoch          : 31\n",
      "    loss           : 8244.332580080347\n",
      "    val_loss       : 8155.591935346929\n",
      "Train Epoch: 32 [256/225000 (0%)] Loss: 8175.716797\n",
      "Train Epoch: 32 [4352/225000 (2%)] Loss: 8143.308594\n",
      "Train Epoch: 32 [8448/225000 (4%)] Loss: 8103.005859\n",
      "Train Epoch: 32 [12544/225000 (6%)] Loss: 8133.628906\n",
      "Train Epoch: 32 [16640/225000 (7%)] Loss: 8095.296875\n",
      "Train Epoch: 32 [20736/225000 (9%)] Loss: 8239.357422\n",
      "Train Epoch: 32 [24832/225000 (11%)] Loss: 7950.576172\n",
      "Train Epoch: 32 [28928/225000 (13%)] Loss: 8245.150391\n",
      "Train Epoch: 32 [33024/225000 (15%)] Loss: 8103.660156\n",
      "Train Epoch: 32 [37120/225000 (16%)] Loss: 8090.031250\n",
      "Train Epoch: 32 [41216/225000 (18%)] Loss: 8206.900391\n",
      "Train Epoch: 32 [45312/225000 (20%)] Loss: 7953.812500\n",
      "Train Epoch: 32 [49408/225000 (22%)] Loss: 8010.138672\n",
      "Train Epoch: 32 [53504/225000 (24%)] Loss: 8077.345703\n",
      "Train Epoch: 32 [57600/225000 (26%)] Loss: 8200.001953\n",
      "Train Epoch: 32 [61696/225000 (27%)] Loss: 8396.714844\n",
      "Train Epoch: 32 [65792/225000 (29%)] Loss: 8002.718750\n",
      "Train Epoch: 32 [69888/225000 (31%)] Loss: 8267.314453\n",
      "Train Epoch: 32 [73984/225000 (33%)] Loss: 8197.072266\n",
      "Train Epoch: 32 [78080/225000 (35%)] Loss: 8333.267578\n",
      "Train Epoch: 32 [82176/225000 (37%)] Loss: 7950.886719\n",
      "Train Epoch: 32 [86272/225000 (38%)] Loss: 8140.158203\n",
      "Train Epoch: 32 [90368/225000 (40%)] Loss: 8221.914062\n",
      "Train Epoch: 32 [94464/225000 (42%)] Loss: 8149.160156\n",
      "Train Epoch: 32 [98560/225000 (44%)] Loss: 8319.876953\n",
      "Train Epoch: 32 [102656/225000 (46%)] Loss: 8220.373047\n",
      "Train Epoch: 32 [106752/225000 (47%)] Loss: 8277.974609\n",
      "Train Epoch: 32 [110848/225000 (49%)] Loss: 8255.064453\n",
      "Train Epoch: 32 [114944/225000 (51%)] Loss: 7963.076172\n",
      "Train Epoch: 32 [119040/225000 (53%)] Loss: 8239.744141\n",
      "Train Epoch: 32 [123136/225000 (55%)] Loss: 8138.744141\n",
      "Train Epoch: 32 [127232/225000 (57%)] Loss: 8143.892578\n",
      "Train Epoch: 32 [131328/225000 (58%)] Loss: 8257.341797\n",
      "Train Epoch: 32 [135424/225000 (60%)] Loss: 7967.605469\n",
      "Train Epoch: 32 [139520/225000 (62%)] Loss: 9599.986328\n",
      "Train Epoch: 32 [143616/225000 (64%)] Loss: 7933.593750\n",
      "Train Epoch: 32 [147712/225000 (66%)] Loss: 7992.185547\n",
      "Train Epoch: 32 [151808/225000 (67%)] Loss: 8377.050781\n",
      "Train Epoch: 32 [155904/225000 (69%)] Loss: 8182.509766\n",
      "Train Epoch: 32 [160000/225000 (71%)] Loss: 8037.181641\n",
      "Train Epoch: 32 [164096/225000 (73%)] Loss: 8209.595703\n",
      "Train Epoch: 32 [168192/225000 (75%)] Loss: 8053.160156\n",
      "Train Epoch: 32 [172288/225000 (77%)] Loss: 8177.072266\n",
      "Train Epoch: 32 [176384/225000 (78%)] Loss: 8113.207031\n",
      "Train Epoch: 32 [180480/225000 (80%)] Loss: 8140.001953\n",
      "Train Epoch: 32 [184576/225000 (82%)] Loss: 8101.763672\n",
      "Train Epoch: 32 [188672/225000 (84%)] Loss: 8285.535156\n",
      "Train Epoch: 32 [192768/225000 (86%)] Loss: 8130.308594\n",
      "Train Epoch: 32 [196864/225000 (87%)] Loss: 8278.828125\n",
      "Train Epoch: 32 [200960/225000 (89%)] Loss: 8153.599609\n",
      "Train Epoch: 32 [205056/225000 (91%)] Loss: 8277.501953\n",
      "Train Epoch: 32 [209152/225000 (93%)] Loss: 8081.330078\n",
      "Train Epoch: 32 [213248/225000 (95%)] Loss: 8055.490234\n",
      "Train Epoch: 32 [217344/225000 (97%)] Loss: 8064.908203\n",
      "Train Epoch: 32 [221440/225000 (98%)] Loss: 8322.845703\n",
      "    epoch          : 32\n",
      "    loss           : 8156.423943668231\n",
      "    val_loss       : 8328.720755369688\n",
      "Train Epoch: 33 [256/225000 (0%)] Loss: 8102.960938\n",
      "Train Epoch: 33 [4352/225000 (2%)] Loss: 8151.056641\n",
      "Train Epoch: 33 [8448/225000 (4%)] Loss: 8103.832031\n",
      "Train Epoch: 33 [12544/225000 (6%)] Loss: 7829.429688\n",
      "Train Epoch: 33 [16640/225000 (7%)] Loss: 8247.791016\n",
      "Train Epoch: 33 [20736/225000 (9%)] Loss: 8076.437500\n",
      "Train Epoch: 33 [24832/225000 (11%)] Loss: 8069.498047\n",
      "Train Epoch: 33 [28928/225000 (13%)] Loss: 7958.244141\n",
      "Train Epoch: 33 [33024/225000 (15%)] Loss: 7995.310547\n",
      "Train Epoch: 33 [37120/225000 (16%)] Loss: 8245.044922\n",
      "Train Epoch: 33 [41216/225000 (18%)] Loss: 8115.101562\n",
      "Train Epoch: 33 [45312/225000 (20%)] Loss: 8125.935547\n",
      "Train Epoch: 33 [49408/225000 (22%)] Loss: 8192.916016\n",
      "Train Epoch: 33 [53504/225000 (24%)] Loss: 8063.306641\n",
      "Train Epoch: 33 [57600/225000 (26%)] Loss: 8144.849609\n",
      "Train Epoch: 33 [61696/225000 (27%)] Loss: 8135.494141\n",
      "Train Epoch: 33 [65792/225000 (29%)] Loss: 8135.390625\n",
      "Train Epoch: 33 [69888/225000 (31%)] Loss: 8186.910156\n",
      "Train Epoch: 33 [73984/225000 (33%)] Loss: 7951.580078\n",
      "Train Epoch: 33 [78080/225000 (35%)] Loss: 8027.306641\n",
      "Train Epoch: 33 [82176/225000 (37%)] Loss: 8049.683594\n",
      "Train Epoch: 33 [86272/225000 (38%)] Loss: 8018.994141\n",
      "Train Epoch: 33 [90368/225000 (40%)] Loss: 8182.103516\n",
      "Train Epoch: 33 [94464/225000 (42%)] Loss: 8172.630859\n",
      "Train Epoch: 33 [98560/225000 (44%)] Loss: 8028.029297\n",
      "Train Epoch: 33 [102656/225000 (46%)] Loss: 7968.044922\n",
      "Train Epoch: 33 [106752/225000 (47%)] Loss: 8216.769531\n",
      "Train Epoch: 33 [110848/225000 (49%)] Loss: 8274.146484\n",
      "Train Epoch: 33 [114944/225000 (51%)] Loss: 8141.058594\n",
      "Train Epoch: 33 [119040/225000 (53%)] Loss: 8114.583984\n",
      "Train Epoch: 33 [123136/225000 (55%)] Loss: 8065.289062\n",
      "Train Epoch: 33 [127232/225000 (57%)] Loss: 7884.091797\n",
      "Train Epoch: 33 [131328/225000 (58%)] Loss: 8115.583984\n",
      "Train Epoch: 33 [135424/225000 (60%)] Loss: 8086.810547\n",
      "Train Epoch: 33 [139520/225000 (62%)] Loss: 7837.285156\n",
      "Train Epoch: 33 [143616/225000 (64%)] Loss: 8117.361328\n",
      "Train Epoch: 33 [147712/225000 (66%)] Loss: 8137.398438\n",
      "Train Epoch: 33 [151808/225000 (67%)] Loss: 8149.792969\n",
      "Train Epoch: 33 [155904/225000 (69%)] Loss: 7992.105469\n",
      "Train Epoch: 33 [160000/225000 (71%)] Loss: 8064.791016\n",
      "Train Epoch: 33 [164096/225000 (73%)] Loss: 8039.316406\n",
      "Train Epoch: 33 [168192/225000 (75%)] Loss: 8268.812500\n",
      "Train Epoch: 33 [172288/225000 (77%)] Loss: 7918.203125\n",
      "Train Epoch: 33 [176384/225000 (78%)] Loss: 8240.601562\n",
      "Train Epoch: 33 [180480/225000 (80%)] Loss: 7986.369141\n",
      "Train Epoch: 33 [184576/225000 (82%)] Loss: 7899.406250\n",
      "Train Epoch: 33 [188672/225000 (84%)] Loss: 7823.917969\n",
      "Train Epoch: 33 [192768/225000 (86%)] Loss: 7733.398438\n",
      "Train Epoch: 33 [196864/225000 (87%)] Loss: 8049.509766\n",
      "Train Epoch: 33 [200960/225000 (89%)] Loss: 8209.128906\n",
      "Train Epoch: 33 [205056/225000 (91%)] Loss: 8178.572266\n",
      "Train Epoch: 33 [209152/225000 (93%)] Loss: 8138.875000\n",
      "Train Epoch: 33 [213248/225000 (95%)] Loss: 8009.308594\n",
      "Train Epoch: 33 [217344/225000 (97%)] Loss: 8159.349609\n",
      "Train Epoch: 33 [221440/225000 (98%)] Loss: 8142.830078\n",
      "    epoch          : 33\n",
      "    loss           : 8266.68932536085\n",
      "    val_loss       : 8274.086373077364\n",
      "Train Epoch: 34 [256/225000 (0%)] Loss: 8070.984375\n",
      "Train Epoch: 34 [4352/225000 (2%)] Loss: 8026.115234\n",
      "Train Epoch: 34 [8448/225000 (4%)] Loss: 8088.802734\n",
      "Train Epoch: 34 [12544/225000 (6%)] Loss: 8179.013672\n",
      "Train Epoch: 34 [16640/225000 (7%)] Loss: 8073.404297\n",
      "Train Epoch: 34 [20736/225000 (9%)] Loss: 7960.837891\n",
      "Train Epoch: 34 [24832/225000 (11%)] Loss: 8024.769531\n",
      "Train Epoch: 34 [28928/225000 (13%)] Loss: 7915.046875\n",
      "Train Epoch: 34 [33024/225000 (15%)] Loss: 8146.605469\n",
      "Train Epoch: 34 [37120/225000 (16%)] Loss: 8052.085938\n",
      "Train Epoch: 34 [41216/225000 (18%)] Loss: 8156.843750\n",
      "Train Epoch: 34 [45312/225000 (20%)] Loss: 8108.931641\n",
      "Train Epoch: 34 [49408/225000 (22%)] Loss: 8123.992188\n",
      "Train Epoch: 34 [53504/225000 (24%)] Loss: 8010.646484\n",
      "Train Epoch: 34 [57600/225000 (26%)] Loss: 8094.130859\n",
      "Train Epoch: 34 [61696/225000 (27%)] Loss: 7944.029297\n",
      "Train Epoch: 34 [65792/225000 (29%)] Loss: 8228.636719\n",
      "Train Epoch: 34 [69888/225000 (31%)] Loss: 8014.007812\n",
      "Train Epoch: 34 [73984/225000 (33%)] Loss: 7947.925781\n",
      "Train Epoch: 34 [78080/225000 (35%)] Loss: 8124.205078\n",
      "Train Epoch: 34 [82176/225000 (37%)] Loss: 8044.689453\n",
      "Train Epoch: 34 [86272/225000 (38%)] Loss: 7964.726562\n",
      "Train Epoch: 34 [90368/225000 (40%)] Loss: 8022.640625\n",
      "Train Epoch: 34 [94464/225000 (42%)] Loss: 8014.169922\n",
      "Train Epoch: 34 [98560/225000 (44%)] Loss: 7943.923828\n",
      "Train Epoch: 34 [102656/225000 (46%)] Loss: 8212.023438\n",
      "Train Epoch: 34 [106752/225000 (47%)] Loss: 8241.253906\n",
      "Train Epoch: 34 [110848/225000 (49%)] Loss: 7840.892578\n",
      "Train Epoch: 34 [114944/225000 (51%)] Loss: 8044.228516\n",
      "Train Epoch: 34 [119040/225000 (53%)] Loss: 8071.947266\n",
      "Train Epoch: 34 [123136/225000 (55%)] Loss: 8073.265625\n",
      "Train Epoch: 34 [127232/225000 (57%)] Loss: 7994.615234\n",
      "Train Epoch: 34 [131328/225000 (58%)] Loss: 8216.873047\n",
      "Train Epoch: 34 [135424/225000 (60%)] Loss: 8302.875000\n",
      "Train Epoch: 34 [139520/225000 (62%)] Loss: 7893.847656\n",
      "Train Epoch: 34 [143616/225000 (64%)] Loss: 8064.181641\n",
      "Train Epoch: 34 [147712/225000 (66%)] Loss: 7957.277344\n",
      "Train Epoch: 34 [151808/225000 (67%)] Loss: 8036.494141\n",
      "Train Epoch: 34 [155904/225000 (69%)] Loss: 7976.988281\n",
      "Train Epoch: 34 [160000/225000 (71%)] Loss: 8059.498047\n",
      "Train Epoch: 34 [164096/225000 (73%)] Loss: 7894.607422\n",
      "Train Epoch: 34 [168192/225000 (75%)] Loss: 8070.412109\n",
      "Train Epoch: 34 [172288/225000 (77%)] Loss: 8168.808594\n",
      "Train Epoch: 34 [176384/225000 (78%)] Loss: 7991.316406\n",
      "Train Epoch: 34 [180480/225000 (80%)] Loss: 7906.005859\n",
      "Train Epoch: 34 [184576/225000 (82%)] Loss: 8220.080078\n",
      "Train Epoch: 34 [188672/225000 (84%)] Loss: 8039.310547\n",
      "Train Epoch: 34 [192768/225000 (86%)] Loss: 8133.701172\n",
      "Train Epoch: 34 [196864/225000 (87%)] Loss: 7996.802734\n",
      "Train Epoch: 34 [200960/225000 (89%)] Loss: 7987.699219\n",
      "Train Epoch: 34 [205056/225000 (91%)] Loss: 7888.261719\n",
      "Train Epoch: 34 [209152/225000 (93%)] Loss: 8043.878906\n",
      "Train Epoch: 34 [213248/225000 (95%)] Loss: 8204.916016\n",
      "Train Epoch: 34 [217344/225000 (97%)] Loss: 8021.541016\n",
      "Train Epoch: 34 [221440/225000 (98%)] Loss: 7971.630859\n",
      "    epoch          : 34\n",
      "    loss           : 8116.316231824161\n",
      "    val_loss       : 8003.69006603348\n",
      "Train Epoch: 35 [256/225000 (0%)] Loss: 7973.699219\n",
      "Train Epoch: 35 [4352/225000 (2%)] Loss: 9645.917969\n",
      "Train Epoch: 35 [8448/225000 (4%)] Loss: 7918.542969\n",
      "Train Epoch: 35 [12544/225000 (6%)] Loss: 8144.339844\n",
      "Train Epoch: 35 [16640/225000 (7%)] Loss: 8024.259766\n",
      "Train Epoch: 35 [20736/225000 (9%)] Loss: 8231.062500\n",
      "Train Epoch: 35 [24832/225000 (11%)] Loss: 8165.648438\n",
      "Train Epoch: 35 [28928/225000 (13%)] Loss: 7872.460938\n",
      "Train Epoch: 35 [33024/225000 (15%)] Loss: 8068.544922\n",
      "Train Epoch: 35 [37120/225000 (16%)] Loss: 8180.134766\n",
      "Train Epoch: 35 [41216/225000 (18%)] Loss: 7875.501953\n",
      "Train Epoch: 35 [45312/225000 (20%)] Loss: 8085.171875\n",
      "Train Epoch: 35 [49408/225000 (22%)] Loss: 8051.376953\n",
      "Train Epoch: 35 [53504/225000 (24%)] Loss: 8285.541016\n",
      "Train Epoch: 35 [57600/225000 (26%)] Loss: 8017.185547\n",
      "Train Epoch: 35 [61696/225000 (27%)] Loss: 8202.443359\n",
      "Train Epoch: 35 [65792/225000 (29%)] Loss: 8031.437500\n",
      "Train Epoch: 35 [69888/225000 (31%)] Loss: 7868.568359\n",
      "Train Epoch: 35 [73984/225000 (33%)] Loss: 7854.509766\n",
      "Train Epoch: 35 [78080/225000 (35%)] Loss: 7918.017578\n",
      "Train Epoch: 35 [82176/225000 (37%)] Loss: 8027.937500\n",
      "Train Epoch: 35 [86272/225000 (38%)] Loss: 8187.486328\n",
      "Train Epoch: 35 [90368/225000 (40%)] Loss: 8098.839844\n",
      "Train Epoch: 35 [94464/225000 (42%)] Loss: 8106.949219\n",
      "Train Epoch: 35 [98560/225000 (44%)] Loss: 7950.994141\n",
      "Train Epoch: 35 [102656/225000 (46%)] Loss: 7830.937500\n",
      "Train Epoch: 35 [106752/225000 (47%)] Loss: 8151.271484\n",
      "Train Epoch: 35 [110848/225000 (49%)] Loss: 7859.507812\n",
      "Train Epoch: 35 [114944/225000 (51%)] Loss: 7991.792969\n",
      "Train Epoch: 35 [119040/225000 (53%)] Loss: 7904.314453\n",
      "Train Epoch: 35 [123136/225000 (55%)] Loss: 7950.896484\n",
      "Train Epoch: 35 [127232/225000 (57%)] Loss: 8151.771484\n",
      "Train Epoch: 35 [131328/225000 (58%)] Loss: 7926.761719\n",
      "Train Epoch: 35 [135424/225000 (60%)] Loss: 7954.009766\n",
      "Train Epoch: 35 [139520/225000 (62%)] Loss: 8104.445312\n",
      "Train Epoch: 35 [143616/225000 (64%)] Loss: 7979.464844\n",
      "Train Epoch: 35 [147712/225000 (66%)] Loss: 7779.119141\n",
      "Train Epoch: 35 [151808/225000 (67%)] Loss: 7988.976562\n",
      "Train Epoch: 35 [155904/225000 (69%)] Loss: 8112.728516\n",
      "Train Epoch: 35 [160000/225000 (71%)] Loss: 7892.000000\n",
      "Train Epoch: 35 [164096/225000 (73%)] Loss: 7913.289062\n",
      "Train Epoch: 35 [168192/225000 (75%)] Loss: 8065.951172\n",
      "Train Epoch: 35 [172288/225000 (77%)] Loss: 8086.847656\n",
      "Train Epoch: 35 [176384/225000 (78%)] Loss: 7940.929688\n",
      "Train Epoch: 35 [180480/225000 (80%)] Loss: 8002.300781\n",
      "Train Epoch: 35 [184576/225000 (82%)] Loss: 7968.011719\n",
      "Train Epoch: 35 [188672/225000 (84%)] Loss: 7923.835938\n",
      "Train Epoch: 35 [192768/225000 (86%)] Loss: 8080.656250\n",
      "Train Epoch: 35 [196864/225000 (87%)] Loss: 7917.765625\n",
      "Train Epoch: 35 [200960/225000 (89%)] Loss: 7856.076172\n",
      "Train Epoch: 35 [205056/225000 (91%)] Loss: 7896.974609\n",
      "Train Epoch: 35 [209152/225000 (93%)] Loss: 7964.089844\n",
      "Train Epoch: 35 [213248/225000 (95%)] Loss: 7940.681641\n",
      "Train Epoch: 35 [217344/225000 (97%)] Loss: 8002.417969\n",
      "Train Epoch: 35 [221440/225000 (98%)] Loss: 7856.269531\n",
      "    epoch          : 35\n",
      "    loss           : 8079.361770300057\n",
      "    val_loss       : 7935.896282429598\n",
      "Train Epoch: 36 [256/225000 (0%)] Loss: 7990.542969\n",
      "Train Epoch: 36 [4352/225000 (2%)] Loss: 7917.671875\n",
      "Train Epoch: 36 [8448/225000 (4%)] Loss: 8167.675781\n",
      "Train Epoch: 36 [12544/225000 (6%)] Loss: 7853.519531\n",
      "Train Epoch: 36 [16640/225000 (7%)] Loss: 7975.496094\n",
      "Train Epoch: 36 [20736/225000 (9%)] Loss: 7756.228516\n",
      "Train Epoch: 36 [24832/225000 (11%)] Loss: 7880.214844\n",
      "Train Epoch: 36 [28928/225000 (13%)] Loss: 8184.505859\n",
      "Train Epoch: 36 [33024/225000 (15%)] Loss: 7947.363281\n",
      "Train Epoch: 36 [37120/225000 (16%)] Loss: 7993.125000\n",
      "Train Epoch: 36 [41216/225000 (18%)] Loss: 7852.845703\n",
      "Train Epoch: 36 [45312/225000 (20%)] Loss: 7817.236328\n",
      "Train Epoch: 36 [49408/225000 (22%)] Loss: 7891.851562\n",
      "Train Epoch: 36 [53504/225000 (24%)] Loss: 8058.123047\n",
      "Train Epoch: 36 [57600/225000 (26%)] Loss: 8047.623047\n",
      "Train Epoch: 36 [61696/225000 (27%)] Loss: 8315.042969\n",
      "Train Epoch: 36 [65792/225000 (29%)] Loss: 7953.437500\n",
      "Train Epoch: 36 [69888/225000 (31%)] Loss: 7821.275391\n",
      "Train Epoch: 36 [73984/225000 (33%)] Loss: 8072.927734\n",
      "Train Epoch: 36 [78080/225000 (35%)] Loss: 8136.267578\n",
      "Train Epoch: 36 [82176/225000 (37%)] Loss: 7945.281250\n",
      "Train Epoch: 36 [86272/225000 (38%)] Loss: 7861.349609\n",
      "Train Epoch: 36 [90368/225000 (40%)] Loss: 7837.357422\n",
      "Train Epoch: 36 [94464/225000 (42%)] Loss: 8057.609375\n",
      "Train Epoch: 36 [98560/225000 (44%)] Loss: 7881.429688\n",
      "Train Epoch: 36 [102656/225000 (46%)] Loss: 7935.175781\n",
      "Train Epoch: 36 [106752/225000 (47%)] Loss: 7914.294922\n",
      "Train Epoch: 36 [110848/225000 (49%)] Loss: 7963.583984\n",
      "Train Epoch: 36 [114944/225000 (51%)] Loss: 8037.193359\n",
      "Train Epoch: 36 [119040/225000 (53%)] Loss: 8060.943359\n",
      "Train Epoch: 36 [123136/225000 (55%)] Loss: 7850.548828\n",
      "Train Epoch: 36 [127232/225000 (57%)] Loss: 7947.574219\n",
      "Train Epoch: 36 [131328/225000 (58%)] Loss: 7738.583984\n",
      "Train Epoch: 36 [135424/225000 (60%)] Loss: 7854.652344\n",
      "Train Epoch: 36 [139520/225000 (62%)] Loss: 8115.257812\n",
      "Train Epoch: 36 [143616/225000 (64%)] Loss: 7868.273438\n",
      "Train Epoch: 36 [147712/225000 (66%)] Loss: 8173.593750\n",
      "Train Epoch: 36 [151808/225000 (67%)] Loss: 7892.980469\n",
      "Train Epoch: 36 [155904/225000 (69%)] Loss: 7921.369141\n",
      "Train Epoch: 36 [160000/225000 (71%)] Loss: 7920.535156\n",
      "Train Epoch: 36 [164096/225000 (73%)] Loss: 8152.671875\n",
      "Train Epoch: 36 [168192/225000 (75%)] Loss: 7884.031250\n",
      "Train Epoch: 36 [172288/225000 (77%)] Loss: 7965.429688\n",
      "Train Epoch: 36 [176384/225000 (78%)] Loss: 7768.382812\n",
      "Train Epoch: 36 [180480/225000 (80%)] Loss: 7877.714844\n",
      "Train Epoch: 36 [184576/225000 (82%)] Loss: 9445.955078\n",
      "Train Epoch: 36 [188672/225000 (84%)] Loss: 7841.693359\n",
      "Train Epoch: 36 [192768/225000 (86%)] Loss: 7915.070312\n",
      "Train Epoch: 36 [196864/225000 (87%)] Loss: 8044.802734\n",
      "Train Epoch: 36 [200960/225000 (89%)] Loss: 7990.537109\n",
      "Train Epoch: 36 [205056/225000 (91%)] Loss: 7957.781250\n",
      "Train Epoch: 36 [209152/225000 (93%)] Loss: 7900.634766\n",
      "Train Epoch: 36 [213248/225000 (95%)] Loss: 8002.558594\n",
      "Train Epoch: 36 [217344/225000 (97%)] Loss: 8035.941406\n",
      "Train Epoch: 36 [221440/225000 (98%)] Loss: 7901.037109\n",
      "    epoch          : 36\n",
      "    loss           : 8058.09448769909\n",
      "    val_loss       : 7898.369305889217\n",
      "Train Epoch: 37 [256/225000 (0%)] Loss: 7706.552734\n",
      "Train Epoch: 37 [4352/225000 (2%)] Loss: 7795.701172\n",
      "Train Epoch: 37 [8448/225000 (4%)] Loss: 7783.443359\n",
      "Train Epoch: 37 [12544/225000 (6%)] Loss: 7909.175781\n",
      "Train Epoch: 37 [16640/225000 (7%)] Loss: 8024.042969\n",
      "Train Epoch: 37 [20736/225000 (9%)] Loss: 7966.984375\n",
      "Train Epoch: 37 [24832/225000 (11%)] Loss: 7848.000000\n",
      "Train Epoch: 37 [28928/225000 (13%)] Loss: 7998.894531\n",
      "Train Epoch: 37 [33024/225000 (15%)] Loss: 7968.664062\n",
      "Train Epoch: 37 [37120/225000 (16%)] Loss: 7969.595703\n",
      "Train Epoch: 37 [41216/225000 (18%)] Loss: 7935.515625\n",
      "Train Epoch: 37 [45312/225000 (20%)] Loss: 7924.365234\n",
      "Train Epoch: 37 [49408/225000 (22%)] Loss: 7935.992188\n",
      "Train Epoch: 37 [53504/225000 (24%)] Loss: 7983.802734\n",
      "Train Epoch: 37 [57600/225000 (26%)] Loss: 7714.597656\n",
      "Train Epoch: 37 [61696/225000 (27%)] Loss: 7977.916016\n",
      "Train Epoch: 37 [65792/225000 (29%)] Loss: 7930.460938\n",
      "Train Epoch: 37 [69888/225000 (31%)] Loss: 8055.562500\n",
      "Train Epoch: 37 [73984/225000 (33%)] Loss: 7966.566406\n",
      "Train Epoch: 37 [78080/225000 (35%)] Loss: 7815.212891\n",
      "Train Epoch: 37 [82176/225000 (37%)] Loss: 7788.871094\n",
      "Train Epoch: 37 [86272/225000 (38%)] Loss: 7733.529297\n",
      "Train Epoch: 37 [90368/225000 (40%)] Loss: 8052.697266\n",
      "Train Epoch: 37 [94464/225000 (42%)] Loss: 8005.099609\n",
      "Train Epoch: 37 [98560/225000 (44%)] Loss: 7983.357422\n",
      "Train Epoch: 37 [102656/225000 (46%)] Loss: 7818.470703\n",
      "Train Epoch: 37 [106752/225000 (47%)] Loss: 7834.810547\n",
      "Train Epoch: 37 [110848/225000 (49%)] Loss: 7976.849609\n",
      "Train Epoch: 37 [114944/225000 (51%)] Loss: 7923.279297\n",
      "Train Epoch: 37 [119040/225000 (53%)] Loss: 8038.091797\n",
      "Train Epoch: 37 [123136/225000 (55%)] Loss: 7898.994141\n",
      "Train Epoch: 37 [127232/225000 (57%)] Loss: 7916.988281\n",
      "Train Epoch: 37 [131328/225000 (58%)] Loss: 7861.810547\n",
      "Train Epoch: 37 [135424/225000 (60%)] Loss: 7813.615234\n",
      "Train Epoch: 37 [139520/225000 (62%)] Loss: 8026.197266\n",
      "Train Epoch: 37 [143616/225000 (64%)] Loss: 7925.820312\n",
      "Train Epoch: 37 [147712/225000 (66%)] Loss: 7917.937500\n",
      "Train Epoch: 37 [151808/225000 (67%)] Loss: 7807.085938\n",
      "Train Epoch: 37 [155904/225000 (69%)] Loss: 7927.212891\n",
      "Train Epoch: 37 [160000/225000 (71%)] Loss: 7934.472656\n",
      "Train Epoch: 37 [164096/225000 (73%)] Loss: 7858.587891\n",
      "Train Epoch: 37 [168192/225000 (75%)] Loss: 7909.679688\n",
      "Train Epoch: 37 [172288/225000 (77%)] Loss: 8014.863281\n",
      "Train Epoch: 37 [176384/225000 (78%)] Loss: 7836.708984\n",
      "Train Epoch: 37 [180480/225000 (80%)] Loss: 8021.128906\n",
      "Train Epoch: 37 [184576/225000 (82%)] Loss: 7621.033203\n",
      "Train Epoch: 37 [188672/225000 (84%)] Loss: 7879.337891\n",
      "Train Epoch: 37 [192768/225000 (86%)] Loss: 7785.082031\n",
      "Train Epoch: 37 [196864/225000 (87%)] Loss: 8031.835938\n",
      "Train Epoch: 37 [200960/225000 (89%)] Loss: 7792.378906\n",
      "Train Epoch: 37 [205056/225000 (91%)] Loss: 7809.484375\n",
      "Train Epoch: 37 [209152/225000 (93%)] Loss: 7951.779297\n",
      "Train Epoch: 37 [213248/225000 (95%)] Loss: 7858.736328\n",
      "Train Epoch: 37 [217344/225000 (97%)] Loss: 7826.222656\n",
      "Train Epoch: 37 [221440/225000 (98%)] Loss: 7957.296875\n",
      "    epoch          : 37\n",
      "    loss           : 7941.407793168729\n",
      "    val_loss       : 7925.496292934125\n",
      "Train Epoch: 38 [256/225000 (0%)] Loss: 7765.732422\n",
      "Train Epoch: 38 [4352/225000 (2%)] Loss: 7898.208984\n",
      "Train Epoch: 38 [8448/225000 (4%)] Loss: 8033.894531\n",
      "Train Epoch: 38 [12544/225000 (6%)] Loss: 8048.076172\n",
      "Train Epoch: 38 [16640/225000 (7%)] Loss: 7756.332031\n",
      "Train Epoch: 38 [20736/225000 (9%)] Loss: 7932.033203\n",
      "Train Epoch: 38 [24832/225000 (11%)] Loss: 7749.664062\n",
      "Train Epoch: 38 [28928/225000 (13%)] Loss: 7843.048828\n",
      "Train Epoch: 38 [33024/225000 (15%)] Loss: 8092.636719\n",
      "Train Epoch: 38 [37120/225000 (16%)] Loss: 8021.533203\n",
      "Train Epoch: 38 [41216/225000 (18%)] Loss: 8048.802734\n",
      "Train Epoch: 38 [45312/225000 (20%)] Loss: 7866.765625\n",
      "Train Epoch: 38 [49408/225000 (22%)] Loss: 7954.060547\n",
      "Train Epoch: 38 [53504/225000 (24%)] Loss: 8039.025391\n",
      "Train Epoch: 38 [57600/225000 (26%)] Loss: 7683.013672\n",
      "Train Epoch: 38 [61696/225000 (27%)] Loss: 7963.277344\n",
      "Train Epoch: 38 [65792/225000 (29%)] Loss: 7770.097656\n",
      "Train Epoch: 38 [69888/225000 (31%)] Loss: 7753.824219\n",
      "Train Epoch: 38 [73984/225000 (33%)] Loss: 7871.687500\n",
      "Train Epoch: 38 [78080/225000 (35%)] Loss: 7690.576172\n",
      "Train Epoch: 38 [82176/225000 (37%)] Loss: 7823.597656\n",
      "Train Epoch: 38 [86272/225000 (38%)] Loss: 7822.505859\n",
      "Train Epoch: 38 [90368/225000 (40%)] Loss: 7938.195312\n",
      "Train Epoch: 38 [94464/225000 (42%)] Loss: 7984.919922\n",
      "Train Epoch: 38 [98560/225000 (44%)] Loss: 8003.798828\n",
      "Train Epoch: 38 [102656/225000 (46%)] Loss: 9096.943359\n",
      "Train Epoch: 38 [106752/225000 (47%)] Loss: 7859.750000\n",
      "Train Epoch: 38 [110848/225000 (49%)] Loss: 7677.044922\n",
      "Train Epoch: 38 [114944/225000 (51%)] Loss: 7833.166016\n",
      "Train Epoch: 38 [119040/225000 (53%)] Loss: 7883.304688\n",
      "Train Epoch: 38 [123136/225000 (55%)] Loss: 7943.787109\n",
      "Train Epoch: 38 [127232/225000 (57%)] Loss: 7899.458984\n",
      "Train Epoch: 38 [131328/225000 (58%)] Loss: 7996.341797\n",
      "Train Epoch: 38 [135424/225000 (60%)] Loss: 7955.269531\n",
      "Train Epoch: 38 [139520/225000 (62%)] Loss: 7833.628906\n",
      "Train Epoch: 38 [143616/225000 (64%)] Loss: 7729.582031\n",
      "Train Epoch: 38 [147712/225000 (66%)] Loss: 7793.941406\n",
      "Train Epoch: 38 [151808/225000 (67%)] Loss: 7789.535156\n",
      "Train Epoch: 38 [155904/225000 (69%)] Loss: 7646.806641\n",
      "Train Epoch: 38 [160000/225000 (71%)] Loss: 7941.488281\n",
      "Train Epoch: 38 [164096/225000 (73%)] Loss: 7752.318359\n",
      "Train Epoch: 38 [168192/225000 (75%)] Loss: 7894.564453\n",
      "Train Epoch: 38 [172288/225000 (77%)] Loss: 7732.062500\n",
      "Train Epoch: 38 [176384/225000 (78%)] Loss: 7931.687500\n",
      "Train Epoch: 38 [180480/225000 (80%)] Loss: 8002.857422\n",
      "Train Epoch: 38 [184576/225000 (82%)] Loss: 8017.384766\n",
      "Train Epoch: 38 [188672/225000 (84%)] Loss: 7702.048828\n",
      "Train Epoch: 38 [192768/225000 (86%)] Loss: 7946.285156\n",
      "Train Epoch: 38 [196864/225000 (87%)] Loss: 7857.787109\n",
      "Train Epoch: 38 [200960/225000 (89%)] Loss: 7867.576172\n",
      "Train Epoch: 38 [205056/225000 (91%)] Loss: 8000.537109\n",
      "Train Epoch: 38 [209152/225000 (93%)] Loss: 7811.582031\n",
      "Train Epoch: 38 [213248/225000 (95%)] Loss: 7824.962891\n",
      "Train Epoch: 38 [217344/225000 (97%)] Loss: 7859.175781\n",
      "Train Epoch: 38 [221440/225000 (98%)] Loss: 7836.615234\n",
      "    epoch          : 38\n",
      "    loss           : 7970.804756381542\n",
      "    val_loss       : 8054.503333967559\n",
      "Train Epoch: 39 [256/225000 (0%)] Loss: 7858.492188\n",
      "Train Epoch: 39 [4352/225000 (2%)] Loss: 8011.435547\n",
      "Train Epoch: 39 [8448/225000 (4%)] Loss: 7741.539062\n",
      "Train Epoch: 39 [12544/225000 (6%)] Loss: 7633.044922\n",
      "Train Epoch: 39 [16640/225000 (7%)] Loss: 7888.814453\n",
      "Train Epoch: 39 [20736/225000 (9%)] Loss: 7922.724609\n",
      "Train Epoch: 39 [24832/225000 (11%)] Loss: 7835.480469\n",
      "Train Epoch: 39 [28928/225000 (13%)] Loss: 7763.011719\n",
      "Train Epoch: 39 [33024/225000 (15%)] Loss: 7904.847656\n",
      "Train Epoch: 39 [37120/225000 (16%)] Loss: 7759.371094\n",
      "Train Epoch: 39 [41216/225000 (18%)] Loss: 7670.269531\n",
      "Train Epoch: 39 [45312/225000 (20%)] Loss: 7929.447266\n",
      "Train Epoch: 39 [49408/225000 (22%)] Loss: 7611.394531\n",
      "Train Epoch: 39 [53504/225000 (24%)] Loss: 7792.484375\n",
      "Train Epoch: 39 [57600/225000 (26%)] Loss: 7761.708984\n",
      "Train Epoch: 39 [61696/225000 (27%)] Loss: 7720.525391\n",
      "Train Epoch: 39 [65792/225000 (29%)] Loss: 7692.642578\n",
      "Train Epoch: 39 [69888/225000 (31%)] Loss: 7882.097656\n",
      "Train Epoch: 39 [73984/225000 (33%)] Loss: 7830.115234\n",
      "Train Epoch: 39 [78080/225000 (35%)] Loss: 7420.312500\n",
      "Train Epoch: 39 [82176/225000 (37%)] Loss: 7789.591797\n",
      "Train Epoch: 39 [86272/225000 (38%)] Loss: 7822.888672\n",
      "Train Epoch: 39 [90368/225000 (40%)] Loss: 7728.748047\n",
      "Train Epoch: 39 [94464/225000 (42%)] Loss: 7822.121094\n",
      "Train Epoch: 39 [98560/225000 (44%)] Loss: 7837.443359\n",
      "Train Epoch: 39 [102656/225000 (46%)] Loss: 9494.119141\n",
      "Train Epoch: 39 [106752/225000 (47%)] Loss: 7808.753906\n",
      "Train Epoch: 39 [110848/225000 (49%)] Loss: 7638.134766\n",
      "Train Epoch: 39 [114944/225000 (51%)] Loss: 7734.242188\n",
      "Train Epoch: 39 [119040/225000 (53%)] Loss: 7847.109375\n",
      "Train Epoch: 39 [123136/225000 (55%)] Loss: 7998.111328\n",
      "Train Epoch: 39 [127232/225000 (57%)] Loss: 7697.666016\n",
      "Train Epoch: 39 [131328/225000 (58%)] Loss: 7805.914062\n",
      "Train Epoch: 39 [135424/225000 (60%)] Loss: 8001.408203\n",
      "Train Epoch: 39 [139520/225000 (62%)] Loss: 7740.462891\n",
      "Train Epoch: 39 [143616/225000 (64%)] Loss: 7991.863281\n",
      "Train Epoch: 39 [147712/225000 (66%)] Loss: 7714.619141\n",
      "Train Epoch: 39 [151808/225000 (67%)] Loss: 7675.726562\n",
      "Train Epoch: 39 [155904/225000 (69%)] Loss: 7709.531250\n",
      "Train Epoch: 39 [160000/225000 (71%)] Loss: 7664.443359\n",
      "Train Epoch: 39 [164096/225000 (73%)] Loss: 7768.417969\n",
      "Train Epoch: 39 [168192/225000 (75%)] Loss: 7638.234375\n",
      "Train Epoch: 39 [172288/225000 (77%)] Loss: 7711.072266\n",
      "Train Epoch: 39 [176384/225000 (78%)] Loss: 7832.939453\n",
      "Train Epoch: 39 [180480/225000 (80%)] Loss: 7776.033203\n",
      "Train Epoch: 39 [184576/225000 (82%)] Loss: 7765.226562\n",
      "Train Epoch: 39 [188672/225000 (84%)] Loss: 7718.285156\n",
      "Train Epoch: 39 [192768/225000 (86%)] Loss: 7808.449219\n",
      "Train Epoch: 39 [196864/225000 (87%)] Loss: 7771.613281\n",
      "Train Epoch: 39 [200960/225000 (89%)] Loss: 7880.509766\n",
      "Train Epoch: 39 [205056/225000 (91%)] Loss: 7726.490234\n",
      "Train Epoch: 39 [209152/225000 (93%)] Loss: 7798.611328\n",
      "Train Epoch: 39 [213248/225000 (95%)] Loss: 7755.916016\n",
      "Train Epoch: 39 [217344/225000 (97%)] Loss: 7645.767578\n",
      "Train Epoch: 39 [221440/225000 (98%)] Loss: 7811.689453\n",
      "    epoch          : 39\n",
      "    loss           : 7950.59862836853\n",
      "    val_loss       : 7787.00561977041\n",
      "Train Epoch: 40 [256/225000 (0%)] Loss: 7952.445312\n",
      "Train Epoch: 40 [4352/225000 (2%)] Loss: 7740.205078\n",
      "Train Epoch: 40 [8448/225000 (4%)] Loss: 7780.460938\n",
      "Train Epoch: 40 [12544/225000 (6%)] Loss: 7738.914062\n",
      "Train Epoch: 40 [16640/225000 (7%)] Loss: 9338.353516\n",
      "Train Epoch: 40 [20736/225000 (9%)] Loss: 7913.187500\n",
      "Train Epoch: 40 [24832/225000 (11%)] Loss: 7618.078125\n",
      "Train Epoch: 40 [28928/225000 (13%)] Loss: 7766.253906\n",
      "Train Epoch: 40 [33024/225000 (15%)] Loss: 7748.970703\n",
      "Train Epoch: 40 [37120/225000 (16%)] Loss: 7671.314453\n",
      "Train Epoch: 40 [41216/225000 (18%)] Loss: 7624.400391\n",
      "Train Epoch: 40 [45312/225000 (20%)] Loss: 7462.625000\n",
      "Train Epoch: 40 [49408/225000 (22%)] Loss: 7929.240234\n",
      "Train Epoch: 40 [53504/225000 (24%)] Loss: 7689.087891\n",
      "Train Epoch: 40 [57600/225000 (26%)] Loss: 7897.728516\n",
      "Train Epoch: 40 [61696/225000 (27%)] Loss: 7816.341797\n",
      "Train Epoch: 40 [65792/225000 (29%)] Loss: 7826.718750\n",
      "Train Epoch: 40 [69888/225000 (31%)] Loss: 9307.683594\n",
      "Train Epoch: 40 [73984/225000 (33%)] Loss: 8018.357422\n",
      "Train Epoch: 40 [78080/225000 (35%)] Loss: 7785.890625\n",
      "Train Epoch: 40 [82176/225000 (37%)] Loss: 7664.619141\n",
      "Train Epoch: 40 [86272/225000 (38%)] Loss: 7693.507812\n",
      "Train Epoch: 40 [90368/225000 (40%)] Loss: 7835.607422\n",
      "Train Epoch: 40 [94464/225000 (42%)] Loss: 7765.039062\n",
      "Train Epoch: 40 [98560/225000 (44%)] Loss: 9467.384766\n",
      "Train Epoch: 40 [102656/225000 (46%)] Loss: 7792.011719\n",
      "Train Epoch: 40 [106752/225000 (47%)] Loss: 7738.111328\n",
      "Train Epoch: 40 [110848/225000 (49%)] Loss: 7685.861328\n",
      "Train Epoch: 40 [114944/225000 (51%)] Loss: 7751.724609\n",
      "Train Epoch: 40 [119040/225000 (53%)] Loss: 7837.867188\n",
      "Train Epoch: 40 [123136/225000 (55%)] Loss: 7841.908203\n",
      "Train Epoch: 40 [127232/225000 (57%)] Loss: 7714.404297\n",
      "Train Epoch: 40 [131328/225000 (58%)] Loss: 7816.605469\n",
      "Train Epoch: 40 [135424/225000 (60%)] Loss: 7887.128906\n",
      "Train Epoch: 40 [139520/225000 (62%)] Loss: 7778.775391\n",
      "Train Epoch: 40 [143616/225000 (64%)] Loss: 7709.427734\n",
      "Train Epoch: 40 [147712/225000 (66%)] Loss: 7696.216797\n",
      "Train Epoch: 40 [151808/225000 (67%)] Loss: 7523.011719\n",
      "Train Epoch: 40 [155904/225000 (69%)] Loss: 7896.888672\n",
      "Train Epoch: 40 [160000/225000 (71%)] Loss: 7896.767578\n",
      "Train Epoch: 40 [164096/225000 (73%)] Loss: 7811.521484\n",
      "Train Epoch: 40 [168192/225000 (75%)] Loss: 7864.281250\n",
      "Train Epoch: 40 [172288/225000 (77%)] Loss: 7735.830078\n",
      "Train Epoch: 40 [176384/225000 (78%)] Loss: 7645.910156\n",
      "Train Epoch: 40 [180480/225000 (80%)] Loss: 7839.714844\n",
      "Train Epoch: 40 [184576/225000 (82%)] Loss: 7624.480469\n",
      "Train Epoch: 40 [188672/225000 (84%)] Loss: 7814.380859\n",
      "Train Epoch: 40 [192768/225000 (86%)] Loss: 7646.642578\n",
      "Train Epoch: 40 [196864/225000 (87%)] Loss: 7715.990234\n",
      "Train Epoch: 40 [200960/225000 (89%)] Loss: 7846.314453\n",
      "Train Epoch: 40 [205056/225000 (91%)] Loss: 7761.703125\n",
      "Train Epoch: 40 [209152/225000 (93%)] Loss: 7655.322266\n",
      "Train Epoch: 40 [213248/225000 (95%)] Loss: 8034.476562\n",
      "Train Epoch: 40 [217344/225000 (97%)] Loss: 8045.843750\n",
      "Train Epoch: 40 [221440/225000 (98%)] Loss: 7685.964844\n",
      "    epoch          : 40\n",
      "    loss           : 7878.840535898393\n",
      "    val_loss       : 7738.641557703213\n",
      "Train Epoch: 41 [256/225000 (0%)] Loss: 7853.662109\n",
      "Train Epoch: 41 [4352/225000 (2%)] Loss: 7758.101562\n",
      "Train Epoch: 41 [8448/225000 (4%)] Loss: 7878.638672\n",
      "Train Epoch: 41 [12544/225000 (6%)] Loss: 7834.623047\n",
      "Train Epoch: 41 [16640/225000 (7%)] Loss: 7738.638672\n",
      "Train Epoch: 41 [20736/225000 (9%)] Loss: 9270.376953\n",
      "Train Epoch: 41 [24832/225000 (11%)] Loss: 7819.597656\n",
      "Train Epoch: 41 [28928/225000 (13%)] Loss: 7647.121094\n",
      "Train Epoch: 41 [33024/225000 (15%)] Loss: 7846.445312\n",
      "Train Epoch: 41 [37120/225000 (16%)] Loss: 7939.695312\n",
      "Train Epoch: 41 [41216/225000 (18%)] Loss: 7688.123047\n",
      "Train Epoch: 41 [45312/225000 (20%)] Loss: 7568.095703\n",
      "Train Epoch: 41 [49408/225000 (22%)] Loss: 7843.382812\n",
      "Train Epoch: 41 [53504/225000 (24%)] Loss: 7820.939453\n",
      "Train Epoch: 41 [57600/225000 (26%)] Loss: 7695.107422\n",
      "Train Epoch: 41 [61696/225000 (27%)] Loss: 7771.019531\n",
      "Train Epoch: 41 [65792/225000 (29%)] Loss: 7794.898438\n",
      "Train Epoch: 41 [69888/225000 (31%)] Loss: 7842.736328\n",
      "Train Epoch: 41 [73984/225000 (33%)] Loss: 7742.138672\n",
      "Train Epoch: 41 [78080/225000 (35%)] Loss: 7695.619141\n",
      "Train Epoch: 41 [82176/225000 (37%)] Loss: 7648.849609\n",
      "Train Epoch: 41 [86272/225000 (38%)] Loss: 7828.628906\n",
      "Train Epoch: 41 [90368/225000 (40%)] Loss: 7743.695312\n",
      "Train Epoch: 41 [94464/225000 (42%)] Loss: 7708.687500\n",
      "Train Epoch: 41 [98560/225000 (44%)] Loss: 7859.765625\n",
      "Train Epoch: 41 [102656/225000 (46%)] Loss: 7593.388672\n",
      "Train Epoch: 41 [106752/225000 (47%)] Loss: 7779.271484\n",
      "Train Epoch: 41 [110848/225000 (49%)] Loss: 7637.052734\n",
      "Train Epoch: 41 [114944/225000 (51%)] Loss: 7897.861328\n",
      "Train Epoch: 41 [119040/225000 (53%)] Loss: 7685.402344\n",
      "Train Epoch: 41 [123136/225000 (55%)] Loss: 7943.099609\n",
      "Train Epoch: 41 [127232/225000 (57%)] Loss: 7804.378906\n",
      "Train Epoch: 41 [131328/225000 (58%)] Loss: 7631.699219\n",
      "Train Epoch: 41 [135424/225000 (60%)] Loss: 7823.775391\n",
      "Train Epoch: 41 [139520/225000 (62%)] Loss: 7688.013672\n",
      "Train Epoch: 41 [143616/225000 (64%)] Loss: 7840.996094\n",
      "Train Epoch: 41 [147712/225000 (66%)] Loss: 7715.937500\n",
      "Train Epoch: 41 [151808/225000 (67%)] Loss: 7649.714844\n",
      "Train Epoch: 41 [155904/225000 (69%)] Loss: 7769.347656\n",
      "Train Epoch: 41 [160000/225000 (71%)] Loss: 7717.054688\n",
      "Train Epoch: 41 [164096/225000 (73%)] Loss: 7649.759766\n",
      "Train Epoch: 41 [168192/225000 (75%)] Loss: 7802.449219\n",
      "Train Epoch: 41 [172288/225000 (77%)] Loss: 7742.445312\n",
      "Train Epoch: 41 [176384/225000 (78%)] Loss: 7618.707031\n",
      "Train Epoch: 41 [180480/225000 (80%)] Loss: 7655.603516\n",
      "Train Epoch: 41 [184576/225000 (82%)] Loss: 7622.328125\n",
      "Train Epoch: 41 [188672/225000 (84%)] Loss: 7854.126953\n",
      "Train Epoch: 41 [192768/225000 (86%)] Loss: 7729.519531\n",
      "Train Epoch: 41 [196864/225000 (87%)] Loss: 7838.181641\n",
      "Train Epoch: 41 [200960/225000 (89%)] Loss: 7710.804688\n",
      "Train Epoch: 41 [205056/225000 (91%)] Loss: 7843.152344\n",
      "Train Epoch: 41 [209152/225000 (93%)] Loss: 7694.652344\n",
      "Train Epoch: 41 [213248/225000 (95%)] Loss: 7857.070312\n",
      "Train Epoch: 41 [217344/225000 (97%)] Loss: 7845.658203\n",
      "Train Epoch: 41 [221440/225000 (98%)] Loss: 7676.183594\n",
      "    epoch          : 41\n",
      "    loss           : 7867.994006194895\n",
      "    val_loss       : 7706.699121087181\n",
      "Train Epoch: 42 [256/225000 (0%)] Loss: 7849.082031\n",
      "Train Epoch: 42 [4352/225000 (2%)] Loss: 7771.042969\n",
      "Train Epoch: 42 [8448/225000 (4%)] Loss: 7612.185547\n",
      "Train Epoch: 42 [12544/225000 (6%)] Loss: 7688.628906\n",
      "Train Epoch: 42 [16640/225000 (7%)] Loss: 7763.736328\n",
      "Train Epoch: 42 [20736/225000 (9%)] Loss: 7854.369141\n",
      "Train Epoch: 42 [24832/225000 (11%)] Loss: 7989.511719\n",
      "Train Epoch: 42 [28928/225000 (13%)] Loss: 7722.644531\n",
      "Train Epoch: 42 [33024/225000 (15%)] Loss: 7680.839844\n",
      "Train Epoch: 42 [37120/225000 (16%)] Loss: 7712.761719\n",
      "Train Epoch: 42 [41216/225000 (18%)] Loss: 7617.986328\n",
      "Train Epoch: 42 [45312/225000 (20%)] Loss: 7773.109375\n",
      "Train Epoch: 42 [49408/225000 (22%)] Loss: 7790.199219\n",
      "Train Epoch: 42 [53504/225000 (24%)] Loss: 7926.912109\n",
      "Train Epoch: 42 [57600/225000 (26%)] Loss: 7795.812500\n",
      "Train Epoch: 42 [61696/225000 (27%)] Loss: 7761.492188\n",
      "Train Epoch: 42 [65792/225000 (29%)] Loss: 7783.699219\n",
      "Train Epoch: 42 [69888/225000 (31%)] Loss: 7798.216797\n",
      "Train Epoch: 42 [73984/225000 (33%)] Loss: 7653.062500\n",
      "Train Epoch: 42 [78080/225000 (35%)] Loss: 31228.863281\n",
      "Train Epoch: 42 [82176/225000 (37%)] Loss: 7788.626953\n",
      "Train Epoch: 42 [86272/225000 (38%)] Loss: 7507.412109\n",
      "Train Epoch: 42 [90368/225000 (40%)] Loss: 7714.076172\n",
      "Train Epoch: 42 [94464/225000 (42%)] Loss: 7742.474609\n",
      "Train Epoch: 42 [98560/225000 (44%)] Loss: 7699.638672\n",
      "Train Epoch: 42 [102656/225000 (46%)] Loss: 7637.015625\n",
      "Train Epoch: 42 [106752/225000 (47%)] Loss: 7644.244141\n",
      "Train Epoch: 42 [110848/225000 (49%)] Loss: 7636.085938\n",
      "Train Epoch: 42 [114944/225000 (51%)] Loss: 7881.193359\n",
      "Train Epoch: 42 [119040/225000 (53%)] Loss: 7779.080078\n",
      "Train Epoch: 42 [123136/225000 (55%)] Loss: 7731.089844\n",
      "Train Epoch: 42 [127232/225000 (57%)] Loss: 7746.259766\n",
      "Train Epoch: 42 [131328/225000 (58%)] Loss: 7696.947266\n",
      "Train Epoch: 42 [135424/225000 (60%)] Loss: 7570.433594\n",
      "Train Epoch: 42 [139520/225000 (62%)] Loss: 7544.166016\n",
      "Train Epoch: 42 [143616/225000 (64%)] Loss: 7738.744141\n",
      "Train Epoch: 42 [147712/225000 (66%)] Loss: 7684.322266\n",
      "Train Epoch: 42 [151808/225000 (67%)] Loss: 7717.585938\n",
      "Train Epoch: 42 [155904/225000 (69%)] Loss: 7735.263672\n",
      "Train Epoch: 42 [160000/225000 (71%)] Loss: 7879.304688\n",
      "Train Epoch: 42 [164096/225000 (73%)] Loss: 7422.726562\n",
      "Train Epoch: 42 [168192/225000 (75%)] Loss: 7656.865234\n",
      "Train Epoch: 42 [172288/225000 (77%)] Loss: 7796.355469\n",
      "Train Epoch: 42 [176384/225000 (78%)] Loss: 7682.388672\n",
      "Train Epoch: 42 [180480/225000 (80%)] Loss: 7848.417969\n",
      "Train Epoch: 42 [184576/225000 (82%)] Loss: 7583.562500\n",
      "Train Epoch: 42 [188672/225000 (84%)] Loss: 7682.906250\n",
      "Train Epoch: 42 [192768/225000 (86%)] Loss: 7758.919922\n",
      "Train Epoch: 42 [196864/225000 (87%)] Loss: 7768.673828\n",
      "Train Epoch: 42 [200960/225000 (89%)] Loss: 7702.214844\n",
      "Train Epoch: 42 [205056/225000 (91%)] Loss: 7664.175781\n",
      "Train Epoch: 42 [209152/225000 (93%)] Loss: 7726.710938\n",
      "Train Epoch: 42 [213248/225000 (95%)] Loss: 7860.359375\n",
      "Train Epoch: 42 [217344/225000 (97%)] Loss: 7751.726562\n",
      "Train Epoch: 42 [221440/225000 (98%)] Loss: 9300.789062\n",
      "    epoch          : 42\n",
      "    loss           : 7782.903368085182\n",
      "    val_loss       : 7907.318007764768\n",
      "Train Epoch: 43 [256/225000 (0%)] Loss: 7744.042969\n",
      "Train Epoch: 43 [4352/225000 (2%)] Loss: 7762.333984\n",
      "Train Epoch: 43 [8448/225000 (4%)] Loss: 7622.974609\n",
      "Train Epoch: 43 [12544/225000 (6%)] Loss: 7654.107422\n",
      "Train Epoch: 43 [16640/225000 (7%)] Loss: 7753.921875\n",
      "Train Epoch: 43 [20736/225000 (9%)] Loss: 7723.242188\n",
      "Train Epoch: 43 [24832/225000 (11%)] Loss: 7705.318359\n",
      "Train Epoch: 43 [28928/225000 (13%)] Loss: 7725.134766\n",
      "Train Epoch: 43 [33024/225000 (15%)] Loss: 7627.599609\n",
      "Train Epoch: 43 [37120/225000 (16%)] Loss: 7516.851562\n",
      "Train Epoch: 43 [41216/225000 (18%)] Loss: 7435.548828\n",
      "Train Epoch: 43 [45312/225000 (20%)] Loss: 7681.441406\n",
      "Train Epoch: 43 [49408/225000 (22%)] Loss: 7714.916016\n",
      "Train Epoch: 43 [53504/225000 (24%)] Loss: 7481.166016\n",
      "Train Epoch: 43 [57600/225000 (26%)] Loss: 7508.337891\n",
      "Train Epoch: 43 [61696/225000 (27%)] Loss: 7646.406250\n",
      "Train Epoch: 43 [65792/225000 (29%)] Loss: 7622.906250\n",
      "Train Epoch: 43 [69888/225000 (31%)] Loss: 7615.582031\n",
      "Train Epoch: 43 [73984/225000 (33%)] Loss: 7502.458984\n",
      "Train Epoch: 43 [78080/225000 (35%)] Loss: 7688.689453\n",
      "Train Epoch: 43 [82176/225000 (37%)] Loss: 7575.105469\n",
      "Train Epoch: 43 [86272/225000 (38%)] Loss: 7473.583984\n",
      "Train Epoch: 43 [90368/225000 (40%)] Loss: 7617.912109\n",
      "Train Epoch: 43 [94464/225000 (42%)] Loss: 7726.789062\n",
      "Train Epoch: 43 [98560/225000 (44%)] Loss: 7779.464844\n",
      "Train Epoch: 43 [102656/225000 (46%)] Loss: 7767.916016\n",
      "Train Epoch: 43 [106752/225000 (47%)] Loss: 7692.908203\n",
      "Train Epoch: 43 [110848/225000 (49%)] Loss: 7755.574219\n",
      "Train Epoch: 43 [114944/225000 (51%)] Loss: 7738.357422\n",
      "Train Epoch: 43 [119040/225000 (53%)] Loss: 7535.697266\n",
      "Train Epoch: 43 [123136/225000 (55%)] Loss: 7728.382812\n",
      "Train Epoch: 43 [127232/225000 (57%)] Loss: 7585.072266\n",
      "Train Epoch: 43 [131328/225000 (58%)] Loss: 7744.994141\n",
      "Train Epoch: 43 [135424/225000 (60%)] Loss: 7600.183594\n",
      "Train Epoch: 43 [139520/225000 (62%)] Loss: 7735.121094\n",
      "Train Epoch: 43 [143616/225000 (64%)] Loss: 7466.535156\n",
      "Train Epoch: 43 [147712/225000 (66%)] Loss: 7590.724609\n",
      "Train Epoch: 43 [151808/225000 (67%)] Loss: 7749.080078\n",
      "Train Epoch: 43 [155904/225000 (69%)] Loss: 7632.029297\n",
      "Train Epoch: 43 [160000/225000 (71%)] Loss: 7614.769531\n",
      "Train Epoch: 43 [164096/225000 (73%)] Loss: 7697.505859\n",
      "Train Epoch: 43 [168192/225000 (75%)] Loss: 7693.416016\n",
      "Train Epoch: 43 [172288/225000 (77%)] Loss: 7766.275391\n",
      "Train Epoch: 43 [176384/225000 (78%)] Loss: 15259.556641\n",
      "Train Epoch: 43 [180480/225000 (80%)] Loss: 7781.708984\n",
      "Train Epoch: 43 [184576/225000 (82%)] Loss: 7701.808594\n",
      "Train Epoch: 43 [188672/225000 (84%)] Loss: 7697.867188\n",
      "Train Epoch: 43 [192768/225000 (86%)] Loss: 7634.794922\n",
      "Train Epoch: 43 [196864/225000 (87%)] Loss: 7654.048828\n",
      "Train Epoch: 43 [200960/225000 (89%)] Loss: 7759.900391\n",
      "Train Epoch: 43 [205056/225000 (91%)] Loss: 7553.800781\n",
      "Train Epoch: 43 [209152/225000 (93%)] Loss: 7603.517578\n",
      "Train Epoch: 43 [213248/225000 (95%)] Loss: 7521.757812\n",
      "Train Epoch: 43 [217344/225000 (97%)] Loss: 7759.242188\n",
      "Train Epoch: 43 [221440/225000 (98%)] Loss: 7472.228516\n",
      "    epoch          : 43\n",
      "    loss           : 7776.314305362984\n",
      "    val_loss       : 7709.109445636978\n",
      "Train Epoch: 44 [256/225000 (0%)] Loss: 7553.923828\n",
      "Train Epoch: 44 [4352/225000 (2%)] Loss: 7815.603516\n",
      "Train Epoch: 44 [8448/225000 (4%)] Loss: 7909.421875\n",
      "Train Epoch: 44 [12544/225000 (6%)] Loss: 7689.513672\n",
      "Train Epoch: 44 [16640/225000 (7%)] Loss: 7582.470703\n",
      "Train Epoch: 44 [20736/225000 (9%)] Loss: 7615.714844\n",
      "Train Epoch: 44 [24832/225000 (11%)] Loss: 7551.597656\n",
      "Train Epoch: 44 [28928/225000 (13%)] Loss: 7673.205078\n",
      "Train Epoch: 44 [33024/225000 (15%)] Loss: 7660.240234\n",
      "Train Epoch: 44 [37120/225000 (16%)] Loss: 7684.917969\n",
      "Train Epoch: 44 [41216/225000 (18%)] Loss: 7833.810547\n",
      "Train Epoch: 44 [45312/225000 (20%)] Loss: 7814.437500\n",
      "Train Epoch: 44 [49408/225000 (22%)] Loss: 7738.906250\n",
      "Train Epoch: 44 [53504/225000 (24%)] Loss: 7579.396484\n",
      "Train Epoch: 44 [57600/225000 (26%)] Loss: 7698.867188\n",
      "Train Epoch: 44 [61696/225000 (27%)] Loss: 7619.400391\n",
      "Train Epoch: 44 [65792/225000 (29%)] Loss: 7632.585938\n",
      "Train Epoch: 44 [69888/225000 (31%)] Loss: 7675.794922\n",
      "Train Epoch: 44 [73984/225000 (33%)] Loss: 7687.037109\n",
      "Train Epoch: 44 [78080/225000 (35%)] Loss: 7615.501953\n",
      "Train Epoch: 44 [82176/225000 (37%)] Loss: 7619.953125\n",
      "Train Epoch: 44 [86272/225000 (38%)] Loss: 7670.144531\n",
      "Train Epoch: 44 [90368/225000 (40%)] Loss: 7728.962891\n",
      "Train Epoch: 44 [94464/225000 (42%)] Loss: 7427.478516\n",
      "Train Epoch: 44 [98560/225000 (44%)] Loss: 7670.429688\n",
      "Train Epoch: 44 [102656/225000 (46%)] Loss: 7602.287109\n",
      "Train Epoch: 44 [106752/225000 (47%)] Loss: 7892.912109\n",
      "Train Epoch: 44 [110848/225000 (49%)] Loss: 7419.863281\n",
      "Train Epoch: 44 [114944/225000 (51%)] Loss: 7587.535156\n",
      "Train Epoch: 44 [119040/225000 (53%)] Loss: 7623.039062\n",
      "Train Epoch: 44 [123136/225000 (55%)] Loss: 7466.205078\n",
      "Train Epoch: 44 [127232/225000 (57%)] Loss: 7777.703125\n",
      "Train Epoch: 44 [131328/225000 (58%)] Loss: 7928.792969\n",
      "Train Epoch: 44 [135424/225000 (60%)] Loss: 7528.613281\n",
      "Train Epoch: 44 [139520/225000 (62%)] Loss: 7733.712891\n",
      "Train Epoch: 44 [143616/225000 (64%)] Loss: 7743.603516\n",
      "Train Epoch: 44 [147712/225000 (66%)] Loss: 7598.066406\n",
      "Train Epoch: 44 [151808/225000 (67%)] Loss: 7600.074219\n",
      "Train Epoch: 44 [155904/225000 (69%)] Loss: 9385.738281\n",
      "Train Epoch: 44 [160000/225000 (71%)] Loss: 7457.484375\n",
      "Train Epoch: 44 [164096/225000 (73%)] Loss: 7547.023438\n",
      "Train Epoch: 44 [168192/225000 (75%)] Loss: 7845.089844\n",
      "Train Epoch: 44 [172288/225000 (77%)] Loss: 7620.107422\n",
      "Train Epoch: 44 [176384/225000 (78%)] Loss: 7400.708984\n",
      "Train Epoch: 44 [180480/225000 (80%)] Loss: 7631.960938\n",
      "Train Epoch: 44 [184576/225000 (82%)] Loss: 7686.753906\n",
      "Train Epoch: 44 [188672/225000 (84%)] Loss: 7494.294922\n",
      "Train Epoch: 44 [192768/225000 (86%)] Loss: 7462.345703\n",
      "Train Epoch: 44 [196864/225000 (87%)] Loss: 7634.062500\n",
      "Train Epoch: 44 [200960/225000 (89%)] Loss: 7832.681641\n",
      "Train Epoch: 44 [205056/225000 (91%)] Loss: 7585.408203\n",
      "Train Epoch: 44 [209152/225000 (93%)] Loss: 7594.251953\n",
      "Train Epoch: 44 [213248/225000 (95%)] Loss: 7656.929688\n",
      "Train Epoch: 44 [217344/225000 (97%)] Loss: 9243.373047\n",
      "Train Epoch: 44 [221440/225000 (98%)] Loss: 7615.738281\n",
      "    epoch          : 44\n",
      "    loss           : 7753.132833608859\n",
      "    val_loss       : 7629.191297385158\n",
      "Train Epoch: 45 [256/225000 (0%)] Loss: 7799.916016\n",
      "Train Epoch: 45 [4352/225000 (2%)] Loss: 7655.203125\n",
      "Train Epoch: 45 [8448/225000 (4%)] Loss: 7638.677734\n",
      "Train Epoch: 45 [12544/225000 (6%)] Loss: 7656.324219\n",
      "Train Epoch: 45 [16640/225000 (7%)] Loss: 7733.132812\n",
      "Train Epoch: 45 [20736/225000 (9%)] Loss: 7550.341797\n",
      "Train Epoch: 45 [24832/225000 (11%)] Loss: 7600.855469\n",
      "Train Epoch: 45 [28928/225000 (13%)] Loss: 31188.771484\n",
      "Train Epoch: 45 [33024/225000 (15%)] Loss: 7806.373047\n",
      "Train Epoch: 45 [37120/225000 (16%)] Loss: 7780.250000\n",
      "Train Epoch: 45 [41216/225000 (18%)] Loss: 7434.226562\n",
      "Train Epoch: 45 [45312/225000 (20%)] Loss: 7675.662109\n",
      "Train Epoch: 45 [49408/225000 (22%)] Loss: 7418.453125\n",
      "Train Epoch: 45 [53504/225000 (24%)] Loss: 7506.806641\n",
      "Train Epoch: 45 [57600/225000 (26%)] Loss: 7699.259766\n",
      "Train Epoch: 45 [61696/225000 (27%)] Loss: 7512.568359\n",
      "Train Epoch: 45 [65792/225000 (29%)] Loss: 7570.806641\n",
      "Train Epoch: 45 [69888/225000 (31%)] Loss: 7676.046875\n",
      "Train Epoch: 45 [73984/225000 (33%)] Loss: 7654.343750\n",
      "Train Epoch: 45 [78080/225000 (35%)] Loss: 7654.523438\n",
      "Train Epoch: 45 [82176/225000 (37%)] Loss: 7546.523438\n",
      "Train Epoch: 45 [86272/225000 (38%)] Loss: 7788.980469\n",
      "Train Epoch: 45 [90368/225000 (40%)] Loss: 7602.232422\n",
      "Train Epoch: 45 [94464/225000 (42%)] Loss: 7523.224609\n",
      "Train Epoch: 45 [98560/225000 (44%)] Loss: 7832.806641\n",
      "Train Epoch: 45 [102656/225000 (46%)] Loss: 7530.191406\n",
      "Train Epoch: 45 [106752/225000 (47%)] Loss: 7641.783203\n",
      "Train Epoch: 45 [110848/225000 (49%)] Loss: 7485.181641\n",
      "Train Epoch: 45 [114944/225000 (51%)] Loss: 7665.271484\n",
      "Train Epoch: 45 [119040/225000 (53%)] Loss: 7589.375000\n",
      "Train Epoch: 45 [123136/225000 (55%)] Loss: 7511.609375\n",
      "Train Epoch: 45 [127232/225000 (57%)] Loss: 7632.210938\n",
      "Train Epoch: 45 [131328/225000 (58%)] Loss: 7666.083984\n",
      "Train Epoch: 45 [135424/225000 (60%)] Loss: 7722.023438\n",
      "Train Epoch: 45 [139520/225000 (62%)] Loss: 7606.736328\n",
      "Train Epoch: 45 [143616/225000 (64%)] Loss: 7440.564453\n",
      "Train Epoch: 45 [147712/225000 (66%)] Loss: 7655.279297\n",
      "Train Epoch: 45 [151808/225000 (67%)] Loss: 7804.792969\n",
      "Train Epoch: 45 [155904/225000 (69%)] Loss: 7574.109375\n",
      "Train Epoch: 45 [160000/225000 (71%)] Loss: 7612.732422\n",
      "Train Epoch: 45 [164096/225000 (73%)] Loss: 7504.339844\n",
      "Train Epoch: 45 [168192/225000 (75%)] Loss: 7597.113281\n",
      "Train Epoch: 45 [172288/225000 (77%)] Loss: 7602.585938\n",
      "Train Epoch: 45 [176384/225000 (78%)] Loss: 7716.048828\n",
      "Train Epoch: 45 [180480/225000 (80%)] Loss: 7673.251953\n",
      "Train Epoch: 45 [184576/225000 (82%)] Loss: 7563.248047\n",
      "Train Epoch: 45 [188672/225000 (84%)] Loss: 7766.218750\n",
      "Train Epoch: 45 [192768/225000 (86%)] Loss: 7457.046875\n",
      "Train Epoch: 45 [196864/225000 (87%)] Loss: 7617.677734\n",
      "Train Epoch: 45 [200960/225000 (89%)] Loss: 7685.763672\n",
      "Train Epoch: 45 [205056/225000 (91%)] Loss: 7575.281250\n",
      "Train Epoch: 45 [209152/225000 (93%)] Loss: 7766.488281\n",
      "Train Epoch: 45 [213248/225000 (95%)] Loss: 7559.453125\n",
      "Train Epoch: 45 [217344/225000 (97%)] Loss: 7498.250000\n",
      "Train Epoch: 45 [221440/225000 (98%)] Loss: 7597.179688\n",
      "    epoch          : 45\n",
      "    loss           : 7704.909025259528\n",
      "    val_loss       : 7609.726421215097\n",
      "Train Epoch: 46 [256/225000 (0%)] Loss: 7468.433594\n",
      "Train Epoch: 46 [4352/225000 (2%)] Loss: 7706.460938\n",
      "Train Epoch: 46 [8448/225000 (4%)] Loss: 7532.511719\n",
      "Train Epoch: 46 [12544/225000 (6%)] Loss: 7503.306641\n",
      "Train Epoch: 46 [16640/225000 (7%)] Loss: 7471.210938\n",
      "Train Epoch: 46 [20736/225000 (9%)] Loss: 7545.771484\n",
      "Train Epoch: 46 [24832/225000 (11%)] Loss: 7574.992188\n",
      "Train Epoch: 46 [28928/225000 (13%)] Loss: 7678.599609\n",
      "Train Epoch: 46 [33024/225000 (15%)] Loss: 7564.828125\n",
      "Train Epoch: 46 [37120/225000 (16%)] Loss: 7598.558594\n",
      "Train Epoch: 46 [41216/225000 (18%)] Loss: 7638.068359\n",
      "Train Epoch: 46 [45312/225000 (20%)] Loss: 7642.421875\n",
      "Train Epoch: 46 [49408/225000 (22%)] Loss: 7648.753906\n",
      "Train Epoch: 46 [53504/225000 (24%)] Loss: 7594.033203\n",
      "Train Epoch: 46 [57600/225000 (26%)] Loss: 7363.400391\n",
      "Train Epoch: 46 [61696/225000 (27%)] Loss: 7347.542969\n",
      "Train Epoch: 46 [65792/225000 (29%)] Loss: 7759.527344\n",
      "Train Epoch: 46 [69888/225000 (31%)] Loss: 7718.392578\n",
      "Train Epoch: 46 [73984/225000 (33%)] Loss: 7487.445312\n",
      "Train Epoch: 46 [78080/225000 (35%)] Loss: 7703.054688\n",
      "Train Epoch: 46 [82176/225000 (37%)] Loss: 7636.435547\n",
      "Train Epoch: 46 [86272/225000 (38%)] Loss: 7570.171875\n",
      "Train Epoch: 46 [90368/225000 (40%)] Loss: 7734.980469\n",
      "Train Epoch: 46 [94464/225000 (42%)] Loss: 7471.554688\n",
      "Train Epoch: 46 [98560/225000 (44%)] Loss: 7620.347656\n",
      "Train Epoch: 46 [102656/225000 (46%)] Loss: 7626.039062\n",
      "Train Epoch: 46 [106752/225000 (47%)] Loss: 7503.945312\n",
      "Train Epoch: 46 [110848/225000 (49%)] Loss: 7803.312500\n",
      "Train Epoch: 46 [114944/225000 (51%)] Loss: 7519.037109\n",
      "Train Epoch: 46 [119040/225000 (53%)] Loss: 7595.027344\n",
      "Train Epoch: 46 [123136/225000 (55%)] Loss: 7665.513672\n",
      "Train Epoch: 46 [127232/225000 (57%)] Loss: 7724.310547\n",
      "Train Epoch: 46 [131328/225000 (58%)] Loss: 7579.039062\n",
      "Train Epoch: 46 [135424/225000 (60%)] Loss: 7727.939453\n",
      "Train Epoch: 46 [139520/225000 (62%)] Loss: 7670.326172\n",
      "Train Epoch: 46 [143616/225000 (64%)] Loss: 7678.353516\n",
      "Train Epoch: 46 [147712/225000 (66%)] Loss: 7587.292969\n",
      "Train Epoch: 46 [151808/225000 (67%)] Loss: 7678.228516\n",
      "Train Epoch: 46 [155904/225000 (69%)] Loss: 7616.056641\n",
      "Train Epoch: 46 [160000/225000 (71%)] Loss: 7559.208984\n",
      "Train Epoch: 46 [164096/225000 (73%)] Loss: 7537.703125\n",
      "Train Epoch: 46 [168192/225000 (75%)] Loss: 7341.798828\n",
      "Train Epoch: 46 [172288/225000 (77%)] Loss: 7648.048828\n",
      "Train Epoch: 46 [176384/225000 (78%)] Loss: 7505.107422\n",
      "Train Epoch: 46 [180480/225000 (80%)] Loss: 7555.929688\n",
      "Train Epoch: 46 [184576/225000 (82%)] Loss: 7450.835938\n",
      "Train Epoch: 46 [188672/225000 (84%)] Loss: 7590.900391\n",
      "Train Epoch: 46 [192768/225000 (86%)] Loss: 7567.632812\n",
      "Train Epoch: 46 [196864/225000 (87%)] Loss: 7595.623047\n",
      "Train Epoch: 46 [200960/225000 (89%)] Loss: 7709.730469\n",
      "Train Epoch: 46 [205056/225000 (91%)] Loss: 7483.787109\n",
      "Train Epoch: 46 [209152/225000 (93%)] Loss: 7444.109375\n",
      "Train Epoch: 46 [213248/225000 (95%)] Loss: 7429.285156\n",
      "Train Epoch: 46 [217344/225000 (97%)] Loss: 7565.830078\n",
      "Train Epoch: 46 [221440/225000 (98%)] Loss: 7411.365234\n",
      "    epoch          : 46\n",
      "    loss           : 7671.689688655432\n",
      "    val_loss       : 7585.56836729147\n",
      "Train Epoch: 47 [256/225000 (0%)] Loss: 7421.488281\n",
      "Train Epoch: 47 [4352/225000 (2%)] Loss: 7417.931641\n",
      "Train Epoch: 47 [8448/225000 (4%)] Loss: 7469.337891\n",
      "Train Epoch: 47 [12544/225000 (6%)] Loss: 7497.230469\n",
      "Train Epoch: 47 [16640/225000 (7%)] Loss: 7490.886719\n",
      "Train Epoch: 47 [20736/225000 (9%)] Loss: 7650.804688\n",
      "Train Epoch: 47 [24832/225000 (11%)] Loss: 7694.941406\n",
      "Train Epoch: 47 [28928/225000 (13%)] Loss: 7609.208984\n",
      "Train Epoch: 47 [33024/225000 (15%)] Loss: 7530.558594\n",
      "Train Epoch: 47 [37120/225000 (16%)] Loss: 7606.679688\n",
      "Train Epoch: 47 [41216/225000 (18%)] Loss: 7640.265625\n",
      "Train Epoch: 47 [45312/225000 (20%)] Loss: 7594.757812\n",
      "Train Epoch: 47 [49408/225000 (22%)] Loss: 7453.857422\n",
      "Train Epoch: 47 [53504/225000 (24%)] Loss: 7569.558594\n",
      "Train Epoch: 47 [57600/225000 (26%)] Loss: 7708.916016\n",
      "Train Epoch: 47 [61696/225000 (27%)] Loss: 7530.603516\n",
      "Train Epoch: 47 [65792/225000 (29%)] Loss: 7464.486328\n",
      "Train Epoch: 47 [69888/225000 (31%)] Loss: 7373.839844\n",
      "Train Epoch: 47 [73984/225000 (33%)] Loss: 7519.326172\n",
      "Train Epoch: 47 [78080/225000 (35%)] Loss: 7651.785156\n",
      "Train Epoch: 47 [82176/225000 (37%)] Loss: 7608.142578\n",
      "Train Epoch: 47 [86272/225000 (38%)] Loss: 7744.064453\n",
      "Train Epoch: 47 [90368/225000 (40%)] Loss: 7579.746094\n",
      "Train Epoch: 47 [94464/225000 (42%)] Loss: 15267.728516\n",
      "Train Epoch: 47 [98560/225000 (44%)] Loss: 7580.046875\n",
      "Train Epoch: 47 [102656/225000 (46%)] Loss: 7444.638672\n",
      "Train Epoch: 47 [106752/225000 (47%)] Loss: 7551.355469\n",
      "Train Epoch: 47 [110848/225000 (49%)] Loss: 7513.367188\n",
      "Train Epoch: 47 [114944/225000 (51%)] Loss: 7599.828125\n",
      "Train Epoch: 47 [119040/225000 (53%)] Loss: 7482.443359\n",
      "Train Epoch: 47 [123136/225000 (55%)] Loss: 7414.949219\n",
      "Train Epoch: 47 [127232/225000 (57%)] Loss: 7543.992188\n",
      "Train Epoch: 47 [131328/225000 (58%)] Loss: 7667.994141\n",
      "Train Epoch: 47 [135424/225000 (60%)] Loss: 7858.025391\n",
      "Train Epoch: 47 [139520/225000 (62%)] Loss: 7427.509766\n",
      "Train Epoch: 47 [143616/225000 (64%)] Loss: 7497.033203\n",
      "Train Epoch: 47 [147712/225000 (66%)] Loss: 7561.666016\n",
      "Train Epoch: 47 [151808/225000 (67%)] Loss: 7432.324219\n",
      "Train Epoch: 47 [155904/225000 (69%)] Loss: 7415.732422\n",
      "Train Epoch: 47 [160000/225000 (71%)] Loss: 7520.654297\n",
      "Train Epoch: 47 [164096/225000 (73%)] Loss: 7503.505859\n",
      "Train Epoch: 47 [168192/225000 (75%)] Loss: 7416.091797\n",
      "Train Epoch: 47 [172288/225000 (77%)] Loss: 7432.859375\n",
      "Train Epoch: 47 [176384/225000 (78%)] Loss: 7506.578125\n",
      "Train Epoch: 47 [180480/225000 (80%)] Loss: 7566.515625\n",
      "Train Epoch: 47 [184576/225000 (82%)] Loss: 7449.568359\n",
      "Train Epoch: 47 [188672/225000 (84%)] Loss: 7518.378906\n",
      "Train Epoch: 47 [192768/225000 (86%)] Loss: 7439.242188\n",
      "Train Epoch: 47 [196864/225000 (87%)] Loss: 7399.121094\n",
      "Train Epoch: 47 [200960/225000 (89%)] Loss: 7515.697266\n",
      "Train Epoch: 47 [205056/225000 (91%)] Loss: 7604.824219\n",
      "Train Epoch: 47 [209152/225000 (93%)] Loss: 7503.263672\n",
      "Train Epoch: 47 [213248/225000 (95%)] Loss: 7627.841797\n",
      "Train Epoch: 47 [217344/225000 (97%)] Loss: 7783.388672\n",
      "Train Epoch: 47 [221440/225000 (98%)] Loss: 7572.248047\n",
      "    epoch          : 47\n",
      "    loss           : 7642.1607917377705\n",
      "    val_loss       : 7536.739169316633\n",
      "Train Epoch: 48 [256/225000 (0%)] Loss: 7603.775391\n",
      "Train Epoch: 48 [4352/225000 (2%)] Loss: 7500.589844\n",
      "Train Epoch: 48 [8448/225000 (4%)] Loss: 7635.511719\n",
      "Train Epoch: 48 [12544/225000 (6%)] Loss: 7649.714844\n",
      "Train Epoch: 48 [16640/225000 (7%)] Loss: 7493.130859\n",
      "Train Epoch: 48 [20736/225000 (9%)] Loss: 7314.058594\n",
      "Train Epoch: 48 [24832/225000 (11%)] Loss: 7500.951172\n",
      "Train Epoch: 48 [28928/225000 (13%)] Loss: 7578.576172\n",
      "Train Epoch: 48 [33024/225000 (15%)] Loss: 7618.347656\n",
      "Train Epoch: 48 [37120/225000 (16%)] Loss: 7524.917969\n",
      "Train Epoch: 48 [41216/225000 (18%)] Loss: 7407.417969\n",
      "Train Epoch: 48 [45312/225000 (20%)] Loss: 7452.216797\n",
      "Train Epoch: 48 [49408/225000 (22%)] Loss: 7659.609375\n",
      "Train Epoch: 48 [53504/225000 (24%)] Loss: 7492.316406\n",
      "Train Epoch: 48 [57600/225000 (26%)] Loss: 7763.816406\n",
      "Train Epoch: 48 [61696/225000 (27%)] Loss: 7449.080078\n",
      "Train Epoch: 48 [65792/225000 (29%)] Loss: 7510.523438\n",
      "Train Epoch: 48 [69888/225000 (31%)] Loss: 7654.289062\n",
      "Train Epoch: 48 [73984/225000 (33%)] Loss: 7519.361328\n",
      "Train Epoch: 48 [78080/225000 (35%)] Loss: 7551.824219\n",
      "Train Epoch: 48 [82176/225000 (37%)] Loss: 7590.033203\n",
      "Train Epoch: 48 [86272/225000 (38%)] Loss: 7488.882812\n",
      "Train Epoch: 48 [90368/225000 (40%)] Loss: 7570.050781\n",
      "Train Epoch: 48 [94464/225000 (42%)] Loss: 7540.052734\n",
      "Train Epoch: 48 [98560/225000 (44%)] Loss: 7505.396484\n",
      "Train Epoch: 48 [102656/225000 (46%)] Loss: 7491.132812\n",
      "Train Epoch: 48 [106752/225000 (47%)] Loss: 7642.716797\n",
      "Train Epoch: 48 [110848/225000 (49%)] Loss: 7572.080078\n",
      "Train Epoch: 48 [114944/225000 (51%)] Loss: 7610.587891\n",
      "Train Epoch: 48 [119040/225000 (53%)] Loss: 7654.824219\n",
      "Train Epoch: 48 [123136/225000 (55%)] Loss: 7485.738281\n",
      "Train Epoch: 48 [127232/225000 (57%)] Loss: 7645.960938\n",
      "Train Epoch: 48 [131328/225000 (58%)] Loss: 7520.208984\n",
      "Train Epoch: 48 [135424/225000 (60%)] Loss: 7713.369141\n",
      "Train Epoch: 48 [139520/225000 (62%)] Loss: 7448.833984\n",
      "Train Epoch: 48 [143616/225000 (64%)] Loss: 7602.615234\n",
      "Train Epoch: 48 [147712/225000 (66%)] Loss: 7327.333984\n",
      "Train Epoch: 48 [151808/225000 (67%)] Loss: 7523.826172\n",
      "Train Epoch: 48 [155904/225000 (69%)] Loss: 7359.017578\n",
      "Train Epoch: 48 [160000/225000 (71%)] Loss: 7447.423828\n",
      "Train Epoch: 48 [164096/225000 (73%)] Loss: 7614.863281\n",
      "Train Epoch: 48 [168192/225000 (75%)] Loss: 7490.535156\n",
      "Train Epoch: 48 [172288/225000 (77%)] Loss: 7384.677734\n",
      "Train Epoch: 48 [176384/225000 (78%)] Loss: 7708.783203\n",
      "Train Epoch: 48 [180480/225000 (80%)] Loss: 7387.169922\n",
      "Train Epoch: 48 [184576/225000 (82%)] Loss: 7458.187500\n",
      "Train Epoch: 48 [188672/225000 (84%)] Loss: 7567.132812\n",
      "Train Epoch: 48 [192768/225000 (86%)] Loss: 7624.119141\n",
      "Train Epoch: 48 [196864/225000 (87%)] Loss: 7695.033203\n",
      "Train Epoch: 48 [200960/225000 (89%)] Loss: 7573.222656\n",
      "Train Epoch: 48 [205056/225000 (91%)] Loss: 7356.433594\n",
      "Train Epoch: 48 [209152/225000 (93%)] Loss: 7644.990234\n",
      "Train Epoch: 48 [213248/225000 (95%)] Loss: 7533.873047\n",
      "Train Epoch: 48 [217344/225000 (97%)] Loss: 7493.144531\n",
      "Train Epoch: 48 [221440/225000 (98%)] Loss: 7796.632812\n",
      "    epoch          : 48\n",
      "    loss           : 7625.554203107225\n",
      "    val_loss       : 7549.0555220757215\n",
      "Train Epoch: 49 [256/225000 (0%)] Loss: 7448.046875\n",
      "Train Epoch: 49 [4352/225000 (2%)] Loss: 7563.533203\n",
      "Train Epoch: 49 [8448/225000 (4%)] Loss: 7525.755859\n",
      "Train Epoch: 49 [12544/225000 (6%)] Loss: 7832.726562\n",
      "Train Epoch: 49 [16640/225000 (7%)] Loss: 7398.437500\n",
      "Train Epoch: 49 [20736/225000 (9%)] Loss: 7348.207031\n",
      "Train Epoch: 49 [24832/225000 (11%)] Loss: 7331.054688\n",
      "Train Epoch: 49 [28928/225000 (13%)] Loss: 7537.093750\n",
      "Train Epoch: 49 [33024/225000 (15%)] Loss: 7451.697266\n",
      "Train Epoch: 49 [37120/225000 (16%)] Loss: 7675.738281\n",
      "Train Epoch: 49 [41216/225000 (18%)] Loss: 7577.527344\n",
      "Train Epoch: 49 [45312/225000 (20%)] Loss: 9245.675781\n",
      "Train Epoch: 49 [49408/225000 (22%)] Loss: 7361.197266\n",
      "Train Epoch: 49 [53504/225000 (24%)] Loss: 7691.560547\n",
      "Train Epoch: 49 [57600/225000 (26%)] Loss: 7438.628906\n",
      "Train Epoch: 49 [61696/225000 (27%)] Loss: 7414.880859\n",
      "Train Epoch: 49 [65792/225000 (29%)] Loss: 7587.105469\n",
      "Train Epoch: 49 [69888/225000 (31%)] Loss: 7622.859375\n",
      "Train Epoch: 49 [73984/225000 (33%)] Loss: 7590.064453\n",
      "Train Epoch: 49 [78080/225000 (35%)] Loss: 7533.921875\n",
      "Train Epoch: 49 [82176/225000 (37%)] Loss: 7523.958984\n",
      "Train Epoch: 49 [86272/225000 (38%)] Loss: 7441.949219\n",
      "Train Epoch: 49 [90368/225000 (40%)] Loss: 7491.634766\n",
      "Train Epoch: 49 [94464/225000 (42%)] Loss: 7474.607422\n",
      "Train Epoch: 49 [98560/225000 (44%)] Loss: 7469.863281\n",
      "Train Epoch: 49 [102656/225000 (46%)] Loss: 7758.669922\n",
      "Train Epoch: 49 [106752/225000 (47%)] Loss: 7420.855469\n",
      "Train Epoch: 49 [110848/225000 (49%)] Loss: 7500.949219\n",
      "Train Epoch: 49 [114944/225000 (51%)] Loss: 7779.855469\n",
      "Train Epoch: 49 [119040/225000 (53%)] Loss: 7537.865234\n",
      "Train Epoch: 49 [123136/225000 (55%)] Loss: 7502.408203\n",
      "Train Epoch: 49 [127232/225000 (57%)] Loss: 7531.125000\n",
      "Train Epoch: 49 [131328/225000 (58%)] Loss: 7449.382812\n",
      "Train Epoch: 49 [135424/225000 (60%)] Loss: 7579.742188\n",
      "Train Epoch: 49 [139520/225000 (62%)] Loss: 7524.732422\n",
      "Train Epoch: 49 [143616/225000 (64%)] Loss: 7532.386719\n",
      "Train Epoch: 49 [147712/225000 (66%)] Loss: 7550.900391\n",
      "Train Epoch: 49 [151808/225000 (67%)] Loss: 7745.796875\n",
      "Train Epoch: 49 [155904/225000 (69%)] Loss: 7448.560547\n",
      "Train Epoch: 49 [160000/225000 (71%)] Loss: 7317.150391\n",
      "Train Epoch: 49 [164096/225000 (73%)] Loss: 7575.533203\n",
      "Train Epoch: 49 [168192/225000 (75%)] Loss: 7477.208984\n",
      "Train Epoch: 49 [172288/225000 (77%)] Loss: 7572.134766\n",
      "Train Epoch: 49 [176384/225000 (78%)] Loss: 7266.042969\n",
      "Train Epoch: 49 [180480/225000 (80%)] Loss: 7630.626953\n",
      "Train Epoch: 49 [184576/225000 (82%)] Loss: 7653.988281\n",
      "Train Epoch: 49 [188672/225000 (84%)] Loss: 7369.677734\n",
      "Train Epoch: 49 [192768/225000 (86%)] Loss: 7451.679688\n",
      "Train Epoch: 49 [196864/225000 (87%)] Loss: 31027.933594\n",
      "Train Epoch: 49 [200960/225000 (89%)] Loss: 7451.363281\n",
      "Train Epoch: 49 [205056/225000 (91%)] Loss: 7587.605469\n",
      "Train Epoch: 49 [209152/225000 (93%)] Loss: 7587.048828\n",
      "Train Epoch: 49 [213248/225000 (95%)] Loss: 7291.414062\n",
      "Train Epoch: 49 [217344/225000 (97%)] Loss: 7568.322266\n",
      "Train Epoch: 49 [221440/225000 (98%)] Loss: 7615.820312\n",
      "    epoch          : 49\n",
      "    loss           : 7635.614445570251\n",
      "    val_loss       : 8047.944923742693\n",
      "Train Epoch: 50 [256/225000 (0%)] Loss: 7384.068359\n",
      "Train Epoch: 50 [4352/225000 (2%)] Loss: 7637.896484\n",
      "Train Epoch: 50 [8448/225000 (4%)] Loss: 7374.025391\n",
      "Train Epoch: 50 [12544/225000 (6%)] Loss: 7591.097656\n",
      "Train Epoch: 50 [16640/225000 (7%)] Loss: 7523.693359\n",
      "Train Epoch: 50 [20736/225000 (9%)] Loss: 7482.044922\n",
      "Train Epoch: 50 [24832/225000 (11%)] Loss: 7419.078125\n",
      "Train Epoch: 50 [28928/225000 (13%)] Loss: 7355.216797\n",
      "Train Epoch: 50 [33024/225000 (15%)] Loss: 7528.716797\n",
      "Train Epoch: 50 [37120/225000 (16%)] Loss: 7624.722656\n",
      "Train Epoch: 50 [41216/225000 (18%)] Loss: 7633.699219\n",
      "Train Epoch: 50 [45312/225000 (20%)] Loss: 7288.445312\n",
      "Train Epoch: 50 [49408/225000 (22%)] Loss: 7401.435547\n",
      "Train Epoch: 50 [53504/225000 (24%)] Loss: 7412.664062\n",
      "Train Epoch: 50 [57600/225000 (26%)] Loss: 7397.087891\n",
      "Train Epoch: 50 [61696/225000 (27%)] Loss: 7375.169922\n",
      "Train Epoch: 50 [65792/225000 (29%)] Loss: 7576.179688\n",
      "Train Epoch: 50 [69888/225000 (31%)] Loss: 7585.664062\n",
      "Train Epoch: 50 [73984/225000 (33%)] Loss: 7487.617188\n",
      "Train Epoch: 50 [78080/225000 (35%)] Loss: 7469.748047\n",
      "Train Epoch: 50 [82176/225000 (37%)] Loss: 7346.083984\n",
      "Train Epoch: 50 [86272/225000 (38%)] Loss: 7562.486328\n",
      "Train Epoch: 50 [90368/225000 (40%)] Loss: 7352.794922\n",
      "Train Epoch: 50 [94464/225000 (42%)] Loss: 7475.906250\n",
      "Train Epoch: 50 [98560/225000 (44%)] Loss: 7505.642578\n",
      "Train Epoch: 50 [102656/225000 (46%)] Loss: 7609.660156\n",
      "Train Epoch: 50 [106752/225000 (47%)] Loss: 7480.281250\n",
      "Train Epoch: 50 [110848/225000 (49%)] Loss: 7467.935547\n",
      "Train Epoch: 50 [114944/225000 (51%)] Loss: 7454.332031\n",
      "Train Epoch: 50 [119040/225000 (53%)] Loss: 7537.591797\n",
      "Train Epoch: 50 [123136/225000 (55%)] Loss: 7624.621094\n",
      "Train Epoch: 50 [127232/225000 (57%)] Loss: 7487.283203\n",
      "Train Epoch: 50 [131328/225000 (58%)] Loss: 7363.173828\n",
      "Train Epoch: 50 [135424/225000 (60%)] Loss: 7455.373047\n",
      "Train Epoch: 50 [139520/225000 (62%)] Loss: 7439.798828\n",
      "Train Epoch: 50 [143616/225000 (64%)] Loss: 7401.912109\n",
      "Train Epoch: 50 [147712/225000 (66%)] Loss: 7385.083984\n",
      "Train Epoch: 50 [151808/225000 (67%)] Loss: 7439.074219\n",
      "Train Epoch: 50 [155904/225000 (69%)] Loss: 7496.769531\n",
      "Train Epoch: 50 [160000/225000 (71%)] Loss: 7397.466797\n",
      "Train Epoch: 50 [164096/225000 (73%)] Loss: 7277.593750\n",
      "Train Epoch: 50 [168192/225000 (75%)] Loss: 7451.294922\n",
      "Train Epoch: 50 [172288/225000 (77%)] Loss: 7440.634766\n",
      "Train Epoch: 50 [176384/225000 (78%)] Loss: 7495.699219\n",
      "Train Epoch: 50 [180480/225000 (80%)] Loss: 7647.232422\n",
      "Train Epoch: 50 [184576/225000 (82%)] Loss: 7427.177734\n",
      "Train Epoch: 50 [188672/225000 (84%)] Loss: 7656.093750\n",
      "Train Epoch: 50 [192768/225000 (86%)] Loss: 7466.921875\n",
      "Train Epoch: 50 [196864/225000 (87%)] Loss: 7523.041016\n",
      "Train Epoch: 50 [200960/225000 (89%)] Loss: 7465.021484\n",
      "Train Epoch: 50 [205056/225000 (91%)] Loss: 7446.250000\n",
      "Train Epoch: 50 [209152/225000 (93%)] Loss: 7449.404297\n",
      "Train Epoch: 50 [213248/225000 (95%)] Loss: 7419.539062\n",
      "Train Epoch: 50 [217344/225000 (97%)] Loss: 9095.355469\n",
      "Train Epoch: 50 [221440/225000 (98%)] Loss: 7556.712891\n",
      "    epoch          : 50\n",
      "    loss           : 7582.4706209115475\n",
      "    val_loss       : 7446.985438192377\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0815_160624/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [256/225000 (0%)] Loss: 7344.884766\n",
      "Train Epoch: 51 [4352/225000 (2%)] Loss: 7315.398438\n",
      "Train Epoch: 51 [8448/225000 (4%)] Loss: 7622.677734\n",
      "Train Epoch: 51 [12544/225000 (6%)] Loss: 7209.648438\n",
      "Train Epoch: 51 [16640/225000 (7%)] Loss: 7540.292969\n",
      "Train Epoch: 51 [20736/225000 (9%)] Loss: 7510.550781\n",
      "Train Epoch: 51 [24832/225000 (11%)] Loss: 7319.287109\n",
      "Train Epoch: 51 [28928/225000 (13%)] Loss: 7383.527344\n",
      "Train Epoch: 51 [33024/225000 (15%)] Loss: 7478.119141\n",
      "Train Epoch: 51 [37120/225000 (16%)] Loss: 7434.029297\n",
      "Train Epoch: 51 [41216/225000 (18%)] Loss: 7493.337891\n",
      "Train Epoch: 51 [45312/225000 (20%)] Loss: 7459.841797\n",
      "Train Epoch: 51 [49408/225000 (22%)] Loss: 7446.914062\n",
      "Train Epoch: 51 [53504/225000 (24%)] Loss: 7299.626953\n",
      "Train Epoch: 51 [57600/225000 (26%)] Loss: 7471.166016\n",
      "Train Epoch: 51 [61696/225000 (27%)] Loss: 7557.748047\n",
      "Train Epoch: 51 [65792/225000 (29%)] Loss: 7438.703125\n",
      "Train Epoch: 51 [69888/225000 (31%)] Loss: 7457.925781\n",
      "Train Epoch: 51 [73984/225000 (33%)] Loss: 7464.757812\n",
      "Train Epoch: 51 [78080/225000 (35%)] Loss: 7494.908203\n",
      "Train Epoch: 51 [82176/225000 (37%)] Loss: 7364.953125\n",
      "Train Epoch: 51 [86272/225000 (38%)] Loss: 7629.089844\n",
      "Train Epoch: 51 [90368/225000 (40%)] Loss: 7445.656250\n",
      "Train Epoch: 51 [94464/225000 (42%)] Loss: 7333.414062\n",
      "Train Epoch: 51 [98560/225000 (44%)] Loss: 7371.472656\n",
      "Train Epoch: 51 [102656/225000 (46%)] Loss: 7408.894531\n",
      "Train Epoch: 51 [106752/225000 (47%)] Loss: 7449.191406\n",
      "Train Epoch: 51 [110848/225000 (49%)] Loss: 7582.410156\n",
      "Train Epoch: 51 [114944/225000 (51%)] Loss: 7374.558594\n",
      "Train Epoch: 51 [119040/225000 (53%)] Loss: 7472.259766\n",
      "Train Epoch: 51 [123136/225000 (55%)] Loss: 7573.613281\n",
      "Train Epoch: 51 [127232/225000 (57%)] Loss: 7659.126953\n",
      "Train Epoch: 51 [131328/225000 (58%)] Loss: 7513.509766\n",
      "Train Epoch: 51 [135424/225000 (60%)] Loss: 7487.687500\n",
      "Train Epoch: 51 [139520/225000 (62%)] Loss: 7400.441406\n",
      "Train Epoch: 51 [143616/225000 (64%)] Loss: 7507.054688\n",
      "Train Epoch: 51 [147712/225000 (66%)] Loss: 7720.271484\n",
      "Train Epoch: 51 [151808/225000 (67%)] Loss: 7484.322266\n",
      "Train Epoch: 51 [155904/225000 (69%)] Loss: 7269.875000\n",
      "Train Epoch: 51 [160000/225000 (71%)] Loss: 7463.662109\n",
      "Train Epoch: 51 [164096/225000 (73%)] Loss: 7448.142578\n",
      "Train Epoch: 51 [168192/225000 (75%)] Loss: 7445.265625\n",
      "Train Epoch: 51 [172288/225000 (77%)] Loss: 7589.091797\n",
      "Train Epoch: 51 [176384/225000 (78%)] Loss: 7387.064453\n",
      "Train Epoch: 51 [180480/225000 (80%)] Loss: 7440.291016\n",
      "Train Epoch: 51 [184576/225000 (82%)] Loss: 7544.511719\n",
      "Train Epoch: 51 [188672/225000 (84%)] Loss: 7502.892578\n",
      "Train Epoch: 51 [192768/225000 (86%)] Loss: 7415.843750\n",
      "Train Epoch: 51 [196864/225000 (87%)] Loss: 7642.394531\n",
      "Train Epoch: 51 [200960/225000 (89%)] Loss: 7352.011719\n",
      "Train Epoch: 51 [205056/225000 (91%)] Loss: 7380.707031\n",
      "Train Epoch: 51 [209152/225000 (93%)] Loss: 7431.531250\n",
      "Train Epoch: 51 [213248/225000 (95%)] Loss: 7437.580078\n",
      "Train Epoch: 51 [217344/225000 (97%)] Loss: 7462.953125\n",
      "Train Epoch: 51 [221440/225000 (98%)] Loss: 7387.267578\n",
      "    epoch          : 51\n",
      "    loss           : 7539.042461026379\n",
      "    val_loss       : 7457.764700733886\n",
      "Train Epoch: 52 [256/225000 (0%)] Loss: 7484.400391\n",
      "Train Epoch: 52 [4352/225000 (2%)] Loss: 7266.777344\n",
      "Train Epoch: 52 [8448/225000 (4%)] Loss: 7424.462891\n",
      "Train Epoch: 52 [12544/225000 (6%)] Loss: 7432.841797\n",
      "Train Epoch: 52 [16640/225000 (7%)] Loss: 7359.892578\n",
      "Train Epoch: 52 [20736/225000 (9%)] Loss: 7438.826172\n",
      "Train Epoch: 52 [24832/225000 (11%)] Loss: 7413.978516\n",
      "Train Epoch: 52 [28928/225000 (13%)] Loss: 7639.162109\n",
      "Train Epoch: 52 [33024/225000 (15%)] Loss: 7451.857422\n",
      "Train Epoch: 52 [37120/225000 (16%)] Loss: 7538.578125\n",
      "Train Epoch: 52 [41216/225000 (18%)] Loss: 7520.654297\n",
      "Train Epoch: 52 [45312/225000 (20%)] Loss: 7410.738281\n",
      "Train Epoch: 52 [49408/225000 (22%)] Loss: 7296.107422\n",
      "Train Epoch: 52 [53504/225000 (24%)] Loss: 7588.878906\n",
      "Train Epoch: 52 [57600/225000 (26%)] Loss: 7492.642578\n",
      "Train Epoch: 52 [61696/225000 (27%)] Loss: 7457.634766\n",
      "Train Epoch: 52 [65792/225000 (29%)] Loss: 7474.792969\n",
      "Train Epoch: 52 [69888/225000 (31%)] Loss: 7416.521484\n",
      "Train Epoch: 52 [73984/225000 (33%)] Loss: 7417.119141\n",
      "Train Epoch: 52 [78080/225000 (35%)] Loss: 7345.515625\n",
      "Train Epoch: 52 [82176/225000 (37%)] Loss: 7414.261719\n",
      "Train Epoch: 52 [86272/225000 (38%)] Loss: 7102.144531\n",
      "Train Epoch: 52 [90368/225000 (40%)] Loss: 7507.648438\n",
      "Train Epoch: 52 [94464/225000 (42%)] Loss: 7358.988281\n",
      "Train Epoch: 52 [98560/225000 (44%)] Loss: 7399.617188\n",
      "Train Epoch: 52 [102656/225000 (46%)] Loss: 30839.550781\n",
      "Train Epoch: 52 [106752/225000 (47%)] Loss: 7453.048828\n",
      "Train Epoch: 52 [110848/225000 (49%)] Loss: 7352.464844\n",
      "Train Epoch: 52 [114944/225000 (51%)] Loss: 7539.550781\n",
      "Train Epoch: 52 [119040/225000 (53%)] Loss: 7372.773438\n",
      "Train Epoch: 52 [123136/225000 (55%)] Loss: 7631.830078\n",
      "Train Epoch: 52 [127232/225000 (57%)] Loss: 7450.484375\n",
      "Train Epoch: 52 [131328/225000 (58%)] Loss: 7335.486328\n",
      "Train Epoch: 52 [135424/225000 (60%)] Loss: 7675.718750\n",
      "Train Epoch: 52 [139520/225000 (62%)] Loss: 7496.294922\n",
      "Train Epoch: 52 [143616/225000 (64%)] Loss: 7573.781250\n",
      "Train Epoch: 52 [147712/225000 (66%)] Loss: 7511.091797\n",
      "Train Epoch: 52 [151808/225000 (67%)] Loss: 7535.351562\n",
      "Train Epoch: 52 [155904/225000 (69%)] Loss: 7455.214844\n",
      "Train Epoch: 52 [160000/225000 (71%)] Loss: 7368.857422\n",
      "Train Epoch: 52 [164096/225000 (73%)] Loss: 7439.787109\n",
      "Train Epoch: 52 [168192/225000 (75%)] Loss: 9009.966797\n",
      "Train Epoch: 52 [172288/225000 (77%)] Loss: 7391.726562\n",
      "Train Epoch: 52 [176384/225000 (78%)] Loss: 7329.765625\n",
      "Train Epoch: 52 [180480/225000 (80%)] Loss: 7454.398438\n",
      "Train Epoch: 52 [184576/225000 (82%)] Loss: 7452.392578\n",
      "Train Epoch: 52 [188672/225000 (84%)] Loss: 7383.238281\n",
      "Train Epoch: 52 [192768/225000 (86%)] Loss: 7521.478516\n",
      "Train Epoch: 52 [196864/225000 (87%)] Loss: 7408.900391\n",
      "Train Epoch: 52 [200960/225000 (89%)] Loss: 7383.611328\n",
      "Train Epoch: 52 [205056/225000 (91%)] Loss: 7456.406250\n",
      "Train Epoch: 52 [209152/225000 (93%)] Loss: 7480.119141\n",
      "Train Epoch: 52 [213248/225000 (95%)] Loss: 7463.560547\n",
      "Train Epoch: 52 [217344/225000 (97%)] Loss: 7441.685547\n",
      "Train Epoch: 52 [221440/225000 (98%)] Loss: 7444.701172\n",
      "    epoch          : 52\n",
      "    loss           : 7558.243346265287\n",
      "    val_loss       : 7494.545705942475\n",
      "Train Epoch: 53 [256/225000 (0%)] Loss: 7335.792969\n",
      "Train Epoch: 53 [4352/225000 (2%)] Loss: 7304.343750\n",
      "Train Epoch: 53 [8448/225000 (4%)] Loss: 7320.441406\n",
      "Train Epoch: 53 [12544/225000 (6%)] Loss: 7463.664062\n",
      "Train Epoch: 53 [16640/225000 (7%)] Loss: 7360.427734\n",
      "Train Epoch: 53 [20736/225000 (9%)] Loss: 7367.158203\n",
      "Train Epoch: 53 [24832/225000 (11%)] Loss: 7491.070312\n",
      "Train Epoch: 53 [28928/225000 (13%)] Loss: 7343.906250\n",
      "Train Epoch: 53 [33024/225000 (15%)] Loss: 7333.976562\n",
      "Train Epoch: 53 [37120/225000 (16%)] Loss: 7317.025391\n",
      "Train Epoch: 53 [41216/225000 (18%)] Loss: 7401.359375\n",
      "Train Epoch: 53 [45312/225000 (20%)] Loss: 7361.935547\n",
      "Train Epoch: 53 [49408/225000 (22%)] Loss: 7267.123047\n",
      "Train Epoch: 53 [53504/225000 (24%)] Loss: 9209.482422\n",
      "Train Epoch: 53 [57600/225000 (26%)] Loss: 7379.841797\n",
      "Train Epoch: 53 [61696/225000 (27%)] Loss: 7353.613281\n",
      "Train Epoch: 53 [65792/225000 (29%)] Loss: 7454.785156\n",
      "Train Epoch: 53 [69888/225000 (31%)] Loss: 7344.267578\n",
      "Train Epoch: 53 [73984/225000 (33%)] Loss: 7407.115234\n",
      "Train Epoch: 53 [78080/225000 (35%)] Loss: 7366.464844\n",
      "Train Epoch: 53 [82176/225000 (37%)] Loss: 7232.472656\n",
      "Train Epoch: 53 [86272/225000 (38%)] Loss: 7411.076172\n",
      "Train Epoch: 53 [90368/225000 (40%)] Loss: 7412.636719\n",
      "Train Epoch: 53 [94464/225000 (42%)] Loss: 7499.412109\n",
      "Train Epoch: 53 [98560/225000 (44%)] Loss: 7200.898438\n",
      "Train Epoch: 53 [102656/225000 (46%)] Loss: 7371.560547\n",
      "Train Epoch: 53 [106752/225000 (47%)] Loss: 7331.554688\n",
      "Train Epoch: 53 [110848/225000 (49%)] Loss: 7447.511719\n",
      "Train Epoch: 53 [114944/225000 (51%)] Loss: 7434.232422\n",
      "Train Epoch: 53 [119040/225000 (53%)] Loss: 7429.078125\n",
      "Train Epoch: 53 [123136/225000 (55%)] Loss: 7418.943359\n",
      "Train Epoch: 53 [127232/225000 (57%)] Loss: 7373.275391\n",
      "Train Epoch: 53 [131328/225000 (58%)] Loss: 7244.144531\n",
      "Train Epoch: 53 [135424/225000 (60%)] Loss: 7476.746094\n",
      "Train Epoch: 53 [139520/225000 (62%)] Loss: 9069.892578\n",
      "Train Epoch: 53 [143616/225000 (64%)] Loss: 7359.679688\n",
      "Train Epoch: 53 [147712/225000 (66%)] Loss: 7256.832031\n",
      "Train Epoch: 53 [151808/225000 (67%)] Loss: 7196.154297\n",
      "Train Epoch: 53 [155904/225000 (69%)] Loss: 7483.046875\n",
      "Train Epoch: 53 [160000/225000 (71%)] Loss: 7505.656250\n",
      "Train Epoch: 53 [164096/225000 (73%)] Loss: 7563.847656\n",
      "Train Epoch: 53 [168192/225000 (75%)] Loss: 7302.322266\n",
      "Train Epoch: 53 [172288/225000 (77%)] Loss: 7416.800781\n",
      "Train Epoch: 53 [176384/225000 (78%)] Loss: 7327.869141\n",
      "Train Epoch: 53 [180480/225000 (80%)] Loss: 7227.939453\n",
      "Train Epoch: 53 [184576/225000 (82%)] Loss: 7445.363281\n",
      "Train Epoch: 53 [188672/225000 (84%)] Loss: 7281.121094\n",
      "Train Epoch: 53 [192768/225000 (86%)] Loss: 7528.248047\n",
      "Train Epoch: 53 [196864/225000 (87%)] Loss: 7437.388672\n",
      "Train Epoch: 53 [200960/225000 (89%)] Loss: 7333.806641\n",
      "Train Epoch: 53 [205056/225000 (91%)] Loss: 7497.250000\n",
      "Train Epoch: 53 [209152/225000 (93%)] Loss: 7316.044922\n",
      "Train Epoch: 53 [213248/225000 (95%)] Loss: 7481.740234\n",
      "Train Epoch: 53 [217344/225000 (97%)] Loss: 7348.583984\n",
      "Train Epoch: 53 [221440/225000 (98%)] Loss: 7514.982422\n",
      "    epoch          : 53\n",
      "    loss           : 7594.694734783845\n",
      "    val_loss       : 7413.790755482352\n",
      "Train Epoch: 54 [256/225000 (0%)] Loss: 7400.812500\n",
      "Train Epoch: 54 [4352/225000 (2%)] Loss: 7395.173828\n",
      "Train Epoch: 54 [8448/225000 (4%)] Loss: 7736.019531\n",
      "Train Epoch: 54 [12544/225000 (6%)] Loss: 7253.423828\n",
      "Train Epoch: 54 [16640/225000 (7%)] Loss: 7210.269531\n",
      "Train Epoch: 54 [20736/225000 (9%)] Loss: 7375.923828\n",
      "Train Epoch: 54 [24832/225000 (11%)] Loss: 7507.082031\n",
      "Train Epoch: 54 [28928/225000 (13%)] Loss: 7565.132812\n",
      "Train Epoch: 54 [33024/225000 (15%)] Loss: 7478.630859\n",
      "Train Epoch: 54 [37120/225000 (16%)] Loss: 7477.757812\n",
      "Train Epoch: 54 [41216/225000 (18%)] Loss: 7392.607422\n",
      "Train Epoch: 54 [45312/225000 (20%)] Loss: 7662.326172\n",
      "Train Epoch: 54 [49408/225000 (22%)] Loss: 7492.958984\n",
      "Train Epoch: 54 [53504/225000 (24%)] Loss: 7413.767578\n",
      "Train Epoch: 54 [57600/225000 (26%)] Loss: 7421.683594\n",
      "Train Epoch: 54 [61696/225000 (27%)] Loss: 7616.439453\n",
      "Train Epoch: 54 [65792/225000 (29%)] Loss: 7454.574219\n",
      "Train Epoch: 54 [69888/225000 (31%)] Loss: 7340.361328\n",
      "Train Epoch: 54 [73984/225000 (33%)] Loss: 7343.199219\n",
      "Train Epoch: 54 [78080/225000 (35%)] Loss: 7334.964844\n",
      "Train Epoch: 54 [82176/225000 (37%)] Loss: 7367.527344\n",
      "Train Epoch: 54 [86272/225000 (38%)] Loss: 7466.310547\n",
      "Train Epoch: 54 [90368/225000 (40%)] Loss: 7299.810547\n",
      "Train Epoch: 54 [94464/225000 (42%)] Loss: 30637.230469\n",
      "Train Epoch: 54 [98560/225000 (44%)] Loss: 7305.664062\n",
      "Train Epoch: 54 [102656/225000 (46%)] Loss: 7241.970703\n",
      "Train Epoch: 54 [106752/225000 (47%)] Loss: 7402.121094\n",
      "Train Epoch: 54 [110848/225000 (49%)] Loss: 7670.164062\n",
      "Train Epoch: 54 [114944/225000 (51%)] Loss: 7353.089844\n",
      "Train Epoch: 54 [119040/225000 (53%)] Loss: 7439.267578\n",
      "Train Epoch: 54 [123136/225000 (55%)] Loss: 7484.890625\n",
      "Train Epoch: 54 [127232/225000 (57%)] Loss: 7361.574219\n",
      "Train Epoch: 54 [131328/225000 (58%)] Loss: 7383.544922\n",
      "Train Epoch: 54 [135424/225000 (60%)] Loss: 7244.765625\n",
      "Train Epoch: 54 [139520/225000 (62%)] Loss: 7268.466797\n",
      "Train Epoch: 54 [143616/225000 (64%)] Loss: 7335.714844\n",
      "Train Epoch: 54 [147712/225000 (66%)] Loss: 7143.960938\n",
      "Train Epoch: 54 [151808/225000 (67%)] Loss: 7514.056641\n",
      "Train Epoch: 54 [155904/225000 (69%)] Loss: 7372.636719\n",
      "Train Epoch: 54 [160000/225000 (71%)] Loss: 7510.052734\n",
      "Train Epoch: 54 [164096/225000 (73%)] Loss: 7318.642578\n",
      "Train Epoch: 54 [168192/225000 (75%)] Loss: 7464.910156\n",
      "Train Epoch: 54 [172288/225000 (77%)] Loss: 7456.740234\n",
      "Train Epoch: 54 [176384/225000 (78%)] Loss: 7377.464844\n",
      "Train Epoch: 54 [180480/225000 (80%)] Loss: 7345.574219\n",
      "Train Epoch: 54 [184576/225000 (82%)] Loss: 7319.531250\n",
      "Train Epoch: 54 [188672/225000 (84%)] Loss: 7511.642578\n",
      "Train Epoch: 54 [192768/225000 (86%)] Loss: 7378.208984\n",
      "Train Epoch: 54 [196864/225000 (87%)] Loss: 7407.187500\n",
      "Train Epoch: 54 [200960/225000 (89%)] Loss: 7274.783203\n",
      "Train Epoch: 54 [205056/225000 (91%)] Loss: 7458.804688\n",
      "Train Epoch: 54 [209152/225000 (93%)] Loss: 7412.837891\n",
      "Train Epoch: 54 [213248/225000 (95%)] Loss: 7428.783203\n",
      "Train Epoch: 54 [217344/225000 (97%)] Loss: 7271.177734\n",
      "Train Epoch: 54 [221440/225000 (98%)] Loss: 7252.851562\n",
      "    epoch          : 54\n",
      "    loss           : 7545.173475940344\n",
      "    val_loss       : 7365.267454084085\n",
      "Train Epoch: 55 [256/225000 (0%)] Loss: 7348.482422\n",
      "Train Epoch: 55 [4352/225000 (2%)] Loss: 7354.617188\n",
      "Train Epoch: 55 [8448/225000 (4%)] Loss: 7362.775391\n",
      "Train Epoch: 55 [12544/225000 (6%)] Loss: 7440.976562\n",
      "Train Epoch: 55 [16640/225000 (7%)] Loss: 7357.345703\n",
      "Train Epoch: 55 [20736/225000 (9%)] Loss: 7430.496094\n",
      "Train Epoch: 55 [24832/225000 (11%)] Loss: 7416.410156\n",
      "Train Epoch: 55 [28928/225000 (13%)] Loss: 7444.605469\n",
      "Train Epoch: 55 [33024/225000 (15%)] Loss: 7258.970703\n",
      "Train Epoch: 55 [37120/225000 (16%)] Loss: 7478.714844\n",
      "Train Epoch: 55 [41216/225000 (18%)] Loss: 7328.869141\n",
      "Train Epoch: 55 [45312/225000 (20%)] Loss: 7467.031250\n",
      "Train Epoch: 55 [49408/225000 (22%)] Loss: 7473.492188\n",
      "Train Epoch: 55 [53504/225000 (24%)] Loss: 7397.585938\n",
      "Train Epoch: 55 [57600/225000 (26%)] Loss: 7375.355469\n",
      "Train Epoch: 55 [61696/225000 (27%)] Loss: 7236.017578\n",
      "Train Epoch: 55 [65792/225000 (29%)] Loss: 7460.167969\n",
      "Train Epoch: 55 [69888/225000 (31%)] Loss: 7435.525391\n",
      "Train Epoch: 55 [73984/225000 (33%)] Loss: 7496.560547\n",
      "Train Epoch: 55 [78080/225000 (35%)] Loss: 7347.521484\n",
      "Train Epoch: 55 [82176/225000 (37%)] Loss: 7258.595703\n",
      "Train Epoch: 55 [86272/225000 (38%)] Loss: 7215.150391\n",
      "Train Epoch: 55 [90368/225000 (40%)] Loss: 7254.917969\n",
      "Train Epoch: 55 [94464/225000 (42%)] Loss: 7279.818359\n",
      "Train Epoch: 55 [98560/225000 (44%)] Loss: 7393.880859\n",
      "Train Epoch: 55 [102656/225000 (46%)] Loss: 7350.730469\n",
      "Train Epoch: 55 [106752/225000 (47%)] Loss: 7342.021484\n",
      "Train Epoch: 55 [110848/225000 (49%)] Loss: 7484.267578\n",
      "Train Epoch: 55 [114944/225000 (51%)] Loss: 7388.777344\n",
      "Train Epoch: 55 [119040/225000 (53%)] Loss: 7434.302734\n",
      "Train Epoch: 55 [123136/225000 (55%)] Loss: 7408.818359\n",
      "Train Epoch: 55 [127232/225000 (57%)] Loss: 7348.878906\n",
      "Train Epoch: 55 [131328/225000 (58%)] Loss: 7334.396484\n",
      "Train Epoch: 55 [135424/225000 (60%)] Loss: 7270.380859\n",
      "Train Epoch: 55 [139520/225000 (62%)] Loss: 7381.531250\n",
      "Train Epoch: 55 [143616/225000 (64%)] Loss: 7284.791016\n",
      "Train Epoch: 55 [147712/225000 (66%)] Loss: 7538.230469\n",
      "Train Epoch: 55 [151808/225000 (67%)] Loss: 7286.537109\n",
      "Train Epoch: 55 [155904/225000 (69%)] Loss: 7343.857422\n",
      "Train Epoch: 55 [160000/225000 (71%)] Loss: 7227.267578\n",
      "Train Epoch: 55 [164096/225000 (73%)] Loss: 7320.712891\n",
      "Train Epoch: 55 [168192/225000 (75%)] Loss: 7385.132812\n",
      "Train Epoch: 55 [172288/225000 (77%)] Loss: 7472.394531\n",
      "Train Epoch: 55 [176384/225000 (78%)] Loss: 7375.689453\n",
      "Train Epoch: 55 [180480/225000 (80%)] Loss: 7277.300781\n",
      "Train Epoch: 55 [184576/225000 (82%)] Loss: 7324.386719\n",
      "Train Epoch: 55 [188672/225000 (84%)] Loss: 7329.619141\n",
      "Train Epoch: 55 [192768/225000 (86%)] Loss: 7454.042969\n",
      "Train Epoch: 55 [196864/225000 (87%)] Loss: 7458.677734\n",
      "Train Epoch: 55 [200960/225000 (89%)] Loss: 7423.257812\n",
      "Train Epoch: 55 [205056/225000 (91%)] Loss: 7485.785156\n",
      "Train Epoch: 55 [209152/225000 (93%)] Loss: 7462.203125\n",
      "Train Epoch: 55 [213248/225000 (95%)] Loss: 7180.761719\n",
      "Train Epoch: 55 [217344/225000 (97%)] Loss: 7245.919922\n",
      "Train Epoch: 55 [221440/225000 (98%)] Loss: 7351.697266\n",
      "    epoch          : 55\n",
      "    loss           : 7447.369048412614\n",
      "    val_loss       : 7455.831728418263\n",
      "Train Epoch: 56 [256/225000 (0%)] Loss: 7513.373047\n",
      "Train Epoch: 56 [4352/225000 (2%)] Loss: 7402.406250\n",
      "Train Epoch: 56 [8448/225000 (4%)] Loss: 7297.357422\n",
      "Train Epoch: 56 [12544/225000 (6%)] Loss: 7435.691406\n",
      "Train Epoch: 56 [16640/225000 (7%)] Loss: 7380.269531\n",
      "Train Epoch: 56 [20736/225000 (9%)] Loss: 7333.033203\n",
      "Train Epoch: 56 [24832/225000 (11%)] Loss: 7240.726562\n",
      "Train Epoch: 56 [28928/225000 (13%)] Loss: 7497.078125\n",
      "Train Epoch: 56 [33024/225000 (15%)] Loss: 7288.507812\n",
      "Train Epoch: 56 [37120/225000 (16%)] Loss: 7524.681641\n",
      "Train Epoch: 56 [41216/225000 (18%)] Loss: 7328.080078\n",
      "Train Epoch: 56 [45312/225000 (20%)] Loss: 7317.009766\n",
      "Train Epoch: 56 [49408/225000 (22%)] Loss: 7279.431641\n",
      "Train Epoch: 56 [53504/225000 (24%)] Loss: 7302.644531\n",
      "Train Epoch: 56 [57600/225000 (26%)] Loss: 7355.904297\n",
      "Train Epoch: 56 [61696/225000 (27%)] Loss: 7304.267578\n",
      "Train Epoch: 56 [65792/225000 (29%)] Loss: 7540.132812\n",
      "Train Epoch: 56 [69888/225000 (31%)] Loss: 7283.486328\n",
      "Train Epoch: 56 [73984/225000 (33%)] Loss: 7355.359375\n",
      "Train Epoch: 56 [78080/225000 (35%)] Loss: 7374.876953\n",
      "Train Epoch: 56 [82176/225000 (37%)] Loss: 7458.972656\n",
      "Train Epoch: 56 [86272/225000 (38%)] Loss: 7311.992188\n",
      "Train Epoch: 56 [90368/225000 (40%)] Loss: 7275.371094\n",
      "Train Epoch: 56 [94464/225000 (42%)] Loss: 7426.304688\n",
      "Train Epoch: 56 [98560/225000 (44%)] Loss: 7678.457031\n",
      "Train Epoch: 56 [102656/225000 (46%)] Loss: 7479.349609\n",
      "Train Epoch: 56 [106752/225000 (47%)] Loss: 7305.966797\n",
      "Train Epoch: 56 [110848/225000 (49%)] Loss: 7412.820312\n",
      "Train Epoch: 56 [114944/225000 (51%)] Loss: 7340.800781\n",
      "Train Epoch: 56 [119040/225000 (53%)] Loss: 7266.851562\n",
      "Train Epoch: 56 [123136/225000 (55%)] Loss: 7269.755859\n",
      "Train Epoch: 56 [127232/225000 (57%)] Loss: 7213.685547\n",
      "Train Epoch: 56 [131328/225000 (58%)] Loss: 7387.218750\n",
      "Train Epoch: 56 [135424/225000 (60%)] Loss: 7550.261719\n",
      "Train Epoch: 56 [139520/225000 (62%)] Loss: 7346.492188\n",
      "Train Epoch: 56 [143616/225000 (64%)] Loss: 7265.498047\n",
      "Train Epoch: 56 [147712/225000 (66%)] Loss: 7458.503906\n",
      "Train Epoch: 56 [151808/225000 (67%)] Loss: 7414.171875\n",
      "Train Epoch: 56 [155904/225000 (69%)] Loss: 7349.462891\n",
      "Train Epoch: 56 [160000/225000 (71%)] Loss: 7180.472656\n",
      "Train Epoch: 56 [164096/225000 (73%)] Loss: 7447.513672\n",
      "Train Epoch: 56 [168192/225000 (75%)] Loss: 7337.539062\n",
      "Train Epoch: 56 [172288/225000 (77%)] Loss: 7401.287109\n",
      "Train Epoch: 56 [176384/225000 (78%)] Loss: 7362.765625\n",
      "Train Epoch: 56 [180480/225000 (80%)] Loss: 7442.869141\n",
      "Train Epoch: 56 [184576/225000 (82%)] Loss: 7259.541016\n",
      "Train Epoch: 56 [188672/225000 (84%)] Loss: 7286.488281\n",
      "Train Epoch: 56 [192768/225000 (86%)] Loss: 7377.000000\n",
      "Train Epoch: 56 [196864/225000 (87%)] Loss: 7368.593750\n",
      "Train Epoch: 56 [200960/225000 (89%)] Loss: 7309.910156\n",
      "Train Epoch: 56 [205056/225000 (91%)] Loss: 7347.400391\n",
      "Train Epoch: 56 [209152/225000 (93%)] Loss: 7487.576172\n",
      "Train Epoch: 56 [213248/225000 (95%)] Loss: 7383.244141\n",
      "Train Epoch: 56 [217344/225000 (97%)] Loss: 7473.982422\n",
      "Train Epoch: 56 [221440/225000 (98%)] Loss: 7356.970703\n",
      "    epoch          : 56\n",
      "    loss           : 7476.5346118636235\n",
      "    val_loss       : 7720.370328750538\n",
      "Train Epoch: 57 [256/225000 (0%)] Loss: 7345.537109\n",
      "Train Epoch: 57 [4352/225000 (2%)] Loss: 7396.925781\n",
      "Train Epoch: 57 [8448/225000 (4%)] Loss: 7221.035156\n",
      "Train Epoch: 57 [12544/225000 (6%)] Loss: 7409.328125\n",
      "Train Epoch: 57 [16640/225000 (7%)] Loss: 7381.705078\n",
      "Train Epoch: 57 [20736/225000 (9%)] Loss: 7297.001953\n",
      "Train Epoch: 57 [24832/225000 (11%)] Loss: 7319.750000\n",
      "Train Epoch: 57 [28928/225000 (13%)] Loss: 7224.835938\n",
      "Train Epoch: 57 [33024/225000 (15%)] Loss: 7228.978516\n",
      "Train Epoch: 57 [37120/225000 (16%)] Loss: 7281.900391\n",
      "Train Epoch: 57 [41216/225000 (18%)] Loss: 7261.601562\n",
      "Train Epoch: 57 [45312/225000 (20%)] Loss: 7191.230469\n",
      "Train Epoch: 57 [49408/225000 (22%)] Loss: 7456.976562\n",
      "Train Epoch: 57 [53504/225000 (24%)] Loss: 7223.367188\n",
      "Train Epoch: 57 [57600/225000 (26%)] Loss: 7354.515625\n",
      "Train Epoch: 57 [61696/225000 (27%)] Loss: 7272.277344\n",
      "Train Epoch: 57 [65792/225000 (29%)] Loss: 7215.615234\n",
      "Train Epoch: 57 [69888/225000 (31%)] Loss: 7276.648438\n",
      "Train Epoch: 57 [73984/225000 (33%)] Loss: 7550.734375\n",
      "Train Epoch: 57 [78080/225000 (35%)] Loss: 7388.609375\n",
      "Train Epoch: 57 [82176/225000 (37%)] Loss: 7319.759766\n",
      "Train Epoch: 57 [86272/225000 (38%)] Loss: 7366.490234\n",
      "Train Epoch: 57 [90368/225000 (40%)] Loss: 7318.505859\n",
      "Train Epoch: 57 [94464/225000 (42%)] Loss: 7258.783203\n",
      "Train Epoch: 57 [98560/225000 (44%)] Loss: 8938.488281\n",
      "Train Epoch: 57 [102656/225000 (46%)] Loss: 7342.931641\n",
      "Train Epoch: 57 [106752/225000 (47%)] Loss: 7429.996094\n",
      "Train Epoch: 57 [110848/225000 (49%)] Loss: 7290.464844\n",
      "Train Epoch: 57 [114944/225000 (51%)] Loss: 7350.875000\n",
      "Train Epoch: 57 [119040/225000 (53%)] Loss: 7178.777344\n",
      "Train Epoch: 57 [123136/225000 (55%)] Loss: 7291.511719\n",
      "Train Epoch: 57 [127232/225000 (57%)] Loss: 7315.039062\n",
      "Train Epoch: 57 [131328/225000 (58%)] Loss: 7446.169922\n",
      "Train Epoch: 57 [135424/225000 (60%)] Loss: 7172.158203\n",
      "Train Epoch: 57 [139520/225000 (62%)] Loss: 7332.333984\n",
      "Train Epoch: 57 [143616/225000 (64%)] Loss: 7330.273438\n",
      "Train Epoch: 57 [147712/225000 (66%)] Loss: 7192.609375\n",
      "Train Epoch: 57 [151808/225000 (67%)] Loss: 7417.837891\n",
      "Train Epoch: 57 [155904/225000 (69%)] Loss: 7396.330078\n",
      "Train Epoch: 57 [160000/225000 (71%)] Loss: 7284.312500\n",
      "Train Epoch: 57 [164096/225000 (73%)] Loss: 7416.228516\n",
      "Train Epoch: 57 [168192/225000 (75%)] Loss: 7227.533203\n",
      "Train Epoch: 57 [172288/225000 (77%)] Loss: 7318.738281\n",
      "Train Epoch: 57 [176384/225000 (78%)] Loss: 7412.482422\n",
      "Train Epoch: 57 [180480/225000 (80%)] Loss: 7206.882812\n",
      "Train Epoch: 57 [184576/225000 (82%)] Loss: 30930.949219\n",
      "Train Epoch: 57 [188672/225000 (84%)] Loss: 9057.023438\n",
      "Train Epoch: 57 [192768/225000 (86%)] Loss: 7303.958984\n",
      "Train Epoch: 57 [196864/225000 (87%)] Loss: 7424.707031\n",
      "Train Epoch: 57 [200960/225000 (89%)] Loss: 7305.990234\n",
      "Train Epoch: 57 [205056/225000 (91%)] Loss: 7187.021484\n",
      "Train Epoch: 57 [209152/225000 (93%)] Loss: 7398.421875\n",
      "Train Epoch: 57 [213248/225000 (95%)] Loss: 7202.343750\n",
      "Train Epoch: 57 [217344/225000 (97%)] Loss: 7301.826172\n",
      "Train Epoch: 57 [221440/225000 (98%)] Loss: 14909.697266\n",
      "    epoch          : 57\n",
      "    loss           : 7622.29658058696\n",
      "    val_loss       : 7836.513255105335\n",
      "Train Epoch: 58 [256/225000 (0%)] Loss: 7151.496094\n",
      "Train Epoch: 58 [4352/225000 (2%)] Loss: 7291.617188\n",
      "Train Epoch: 58 [8448/225000 (4%)] Loss: 7221.333984\n",
      "Train Epoch: 58 [12544/225000 (6%)] Loss: 7311.337891\n",
      "Train Epoch: 58 [16640/225000 (7%)] Loss: 7250.925781\n",
      "Train Epoch: 58 [20736/225000 (9%)] Loss: 7494.650391\n",
      "Train Epoch: 58 [24832/225000 (11%)] Loss: 7242.646484\n",
      "Train Epoch: 58 [28928/225000 (13%)] Loss: 7336.308594\n",
      "Train Epoch: 58 [33024/225000 (15%)] Loss: 7130.982422\n",
      "Train Epoch: 58 [37120/225000 (16%)] Loss: 7285.634766\n",
      "Train Epoch: 58 [41216/225000 (18%)] Loss: 7186.349609\n",
      "Train Epoch: 58 [45312/225000 (20%)] Loss: 9004.720703\n",
      "Train Epoch: 58 [49408/225000 (22%)] Loss: 7317.339844\n",
      "Train Epoch: 58 [53504/225000 (24%)] Loss: 8865.646484\n",
      "Train Epoch: 58 [57600/225000 (26%)] Loss: 8797.703125\n",
      "Train Epoch: 58 [61696/225000 (27%)] Loss: 7333.744141\n",
      "Train Epoch: 58 [65792/225000 (29%)] Loss: 7305.449219\n",
      "Train Epoch: 58 [69888/225000 (31%)] Loss: 7230.929688\n",
      "Train Epoch: 58 [73984/225000 (33%)] Loss: 7365.992188\n",
      "Train Epoch: 58 [78080/225000 (35%)] Loss: 7481.986328\n",
      "Train Epoch: 58 [82176/225000 (37%)] Loss: 7177.457031\n",
      "Train Epoch: 58 [86272/225000 (38%)] Loss: 7413.796875\n",
      "Train Epoch: 58 [90368/225000 (40%)] Loss: 7224.035156\n",
      "Train Epoch: 58 [94464/225000 (42%)] Loss: 7372.105469\n",
      "Train Epoch: 58 [98560/225000 (44%)] Loss: 7259.580078\n",
      "Train Epoch: 58 [102656/225000 (46%)] Loss: 7317.675781\n",
      "Train Epoch: 58 [106752/225000 (47%)] Loss: 7153.988281\n",
      "Train Epoch: 58 [110848/225000 (49%)] Loss: 7284.023438\n",
      "Train Epoch: 58 [114944/225000 (51%)] Loss: 7255.095703\n",
      "Train Epoch: 58 [119040/225000 (53%)] Loss: 7330.666016\n",
      "Train Epoch: 58 [123136/225000 (55%)] Loss: 8844.634766\n",
      "Train Epoch: 58 [127232/225000 (57%)] Loss: 7150.769531\n",
      "Train Epoch: 58 [131328/225000 (58%)] Loss: 7309.876953\n",
      "Train Epoch: 58 [135424/225000 (60%)] Loss: 7321.335938\n",
      "Train Epoch: 58 [139520/225000 (62%)] Loss: 7349.000000\n",
      "Train Epoch: 58 [143616/225000 (64%)] Loss: 7069.996094\n",
      "Train Epoch: 58 [147712/225000 (66%)] Loss: 7410.828125\n",
      "Train Epoch: 58 [151808/225000 (67%)] Loss: 7362.867188\n",
      "Train Epoch: 58 [155904/225000 (69%)] Loss: 7245.000000\n",
      "Train Epoch: 58 [160000/225000 (71%)] Loss: 7178.607422\n",
      "Train Epoch: 58 [164096/225000 (73%)] Loss: 7455.853516\n",
      "Train Epoch: 58 [168192/225000 (75%)] Loss: 7094.412109\n",
      "Train Epoch: 58 [172288/225000 (77%)] Loss: 7294.863281\n",
      "Train Epoch: 58 [176384/225000 (78%)] Loss: 7151.660156\n",
      "Train Epoch: 58 [180480/225000 (80%)] Loss: 9001.445312\n",
      "Train Epoch: 58 [184576/225000 (82%)] Loss: 7364.722656\n",
      "Train Epoch: 58 [188672/225000 (84%)] Loss: 7328.148438\n",
      "Train Epoch: 58 [192768/225000 (86%)] Loss: 7307.230469\n",
      "Train Epoch: 58 [196864/225000 (87%)] Loss: 7236.419922\n",
      "Train Epoch: 58 [200960/225000 (89%)] Loss: 7233.714844\n",
      "Train Epoch: 58 [205056/225000 (91%)] Loss: 7465.345703\n",
      "Train Epoch: 58 [209152/225000 (93%)] Loss: 7251.417969\n",
      "Train Epoch: 58 [213248/225000 (95%)] Loss: 7137.357422\n",
      "Train Epoch: 58 [217344/225000 (97%)] Loss: 7479.044922\n",
      "Train Epoch: 58 [221440/225000 (98%)] Loss: 7411.232422\n",
      "    epoch          : 58\n",
      "    loss           : 7647.529416862201\n",
      "    val_loss       : 7317.340559791545\n",
      "Train Epoch: 59 [256/225000 (0%)] Loss: 9034.248047\n",
      "Train Epoch: 59 [4352/225000 (2%)] Loss: 7227.070312\n",
      "Train Epoch: 59 [8448/225000 (4%)] Loss: 7302.966797\n",
      "Train Epoch: 59 [12544/225000 (6%)] Loss: 7354.576172\n",
      "Train Epoch: 59 [16640/225000 (7%)] Loss: 7304.591797\n",
      "Train Epoch: 59 [20736/225000 (9%)] Loss: 7445.009766\n",
      "Train Epoch: 59 [24832/225000 (11%)] Loss: 7199.060547\n",
      "Train Epoch: 59 [28928/225000 (13%)] Loss: 7351.218750\n",
      "Train Epoch: 59 [33024/225000 (15%)] Loss: 7309.093750\n",
      "Train Epoch: 59 [37120/225000 (16%)] Loss: 7339.716797\n",
      "Train Epoch: 59 [41216/225000 (18%)] Loss: 7304.472656\n",
      "Train Epoch: 59 [45312/225000 (20%)] Loss: 7304.650391\n",
      "Train Epoch: 59 [49408/225000 (22%)] Loss: 7179.615234\n",
      "Train Epoch: 59 [53504/225000 (24%)] Loss: 7401.472656\n",
      "Train Epoch: 59 [57600/225000 (26%)] Loss: 7204.976562\n",
      "Train Epoch: 59 [61696/225000 (27%)] Loss: 7283.843750\n",
      "Train Epoch: 59 [65792/225000 (29%)] Loss: 7288.232422\n",
      "Train Epoch: 59 [69888/225000 (31%)] Loss: 7249.058594\n",
      "Train Epoch: 59 [73984/225000 (33%)] Loss: 7200.035156\n",
      "Train Epoch: 59 [78080/225000 (35%)] Loss: 7448.216797\n",
      "Train Epoch: 59 [82176/225000 (37%)] Loss: 7311.916016\n",
      "Train Epoch: 59 [86272/225000 (38%)] Loss: 7235.406250\n",
      "Train Epoch: 59 [90368/225000 (40%)] Loss: 7221.091797\n",
      "Train Epoch: 59 [94464/225000 (42%)] Loss: 7212.324219\n",
      "Train Epoch: 59 [98560/225000 (44%)] Loss: 7404.226562\n",
      "Train Epoch: 59 [102656/225000 (46%)] Loss: 7339.578125\n",
      "Train Epoch: 59 [106752/225000 (47%)] Loss: 7272.546875\n",
      "Train Epoch: 59 [110848/225000 (49%)] Loss: 7195.218750\n",
      "Train Epoch: 59 [114944/225000 (51%)] Loss: 7468.376953\n",
      "Train Epoch: 59 [119040/225000 (53%)] Loss: 7304.607422\n",
      "Train Epoch: 59 [123136/225000 (55%)] Loss: 7241.949219\n",
      "Train Epoch: 59 [127232/225000 (57%)] Loss: 7204.714844\n",
      "Train Epoch: 59 [131328/225000 (58%)] Loss: 7272.357422\n",
      "Train Epoch: 59 [135424/225000 (60%)] Loss: 7121.134766\n",
      "Train Epoch: 59 [139520/225000 (62%)] Loss: 7170.605469\n",
      "Train Epoch: 59 [143616/225000 (64%)] Loss: 7128.626953\n",
      "Train Epoch: 59 [147712/225000 (66%)] Loss: 7174.062500\n",
      "Train Epoch: 59 [151808/225000 (67%)] Loss: 7259.005859\n",
      "Train Epoch: 59 [155904/225000 (69%)] Loss: 7415.947266\n",
      "Train Epoch: 59 [160000/225000 (71%)] Loss: 7362.271484\n",
      "Train Epoch: 59 [164096/225000 (73%)] Loss: 7250.447266\n",
      "Train Epoch: 59 [168192/225000 (75%)] Loss: 7166.531250\n",
      "Train Epoch: 59 [172288/225000 (77%)] Loss: 7337.636719\n",
      "Train Epoch: 59 [176384/225000 (78%)] Loss: 7330.265625\n",
      "Train Epoch: 59 [180480/225000 (80%)] Loss: 7161.830078\n",
      "Train Epoch: 59 [184576/225000 (82%)] Loss: 7313.177734\n",
      "Train Epoch: 59 [188672/225000 (84%)] Loss: 7468.517578\n",
      "Train Epoch: 59 [192768/225000 (86%)] Loss: 7330.054688\n",
      "Train Epoch: 59 [196864/225000 (87%)] Loss: 7099.167969\n",
      "Train Epoch: 59 [200960/225000 (89%)] Loss: 7139.666016\n",
      "Train Epoch: 59 [205056/225000 (91%)] Loss: 7245.818359\n",
      "Train Epoch: 59 [209152/225000 (93%)] Loss: 7327.828125\n",
      "Train Epoch: 59 [213248/225000 (95%)] Loss: 30105.582031\n",
      "Train Epoch: 59 [217344/225000 (97%)] Loss: 7375.826172\n",
      "Train Epoch: 59 [221440/225000 (98%)] Loss: 8817.923828\n",
      "    epoch          : 59\n",
      "    loss           : 7592.721596363054\n",
      "    val_loss       : 7370.757132480339\n",
      "Train Epoch: 60 [256/225000 (0%)] Loss: 7215.328125\n",
      "Train Epoch: 60 [4352/225000 (2%)] Loss: 7410.162109\n",
      "Train Epoch: 60 [8448/225000 (4%)] Loss: 7199.003906\n",
      "Train Epoch: 60 [12544/225000 (6%)] Loss: 7301.158203\n",
      "Train Epoch: 60 [16640/225000 (7%)] Loss: 7241.080078\n",
      "Train Epoch: 60 [20736/225000 (9%)] Loss: 7445.572266\n",
      "Train Epoch: 60 [24832/225000 (11%)] Loss: 7385.019531\n",
      "Train Epoch: 60 [28928/225000 (13%)] Loss: 7427.187500\n",
      "Train Epoch: 60 [33024/225000 (15%)] Loss: 7283.908203\n",
      "Train Epoch: 60 [37120/225000 (16%)] Loss: 7124.041016\n",
      "Train Epoch: 60 [41216/225000 (18%)] Loss: 7052.519531\n",
      "Train Epoch: 60 [45312/225000 (20%)] Loss: 7256.791016\n",
      "Train Epoch: 60 [49408/225000 (22%)] Loss: 7204.011719\n",
      "Train Epoch: 60 [53504/225000 (24%)] Loss: 7299.498047\n",
      "Train Epoch: 60 [57600/225000 (26%)] Loss: 7286.937500\n",
      "Train Epoch: 60 [61696/225000 (27%)] Loss: 7168.613281\n",
      "Train Epoch: 60 [65792/225000 (29%)] Loss: 7131.300781\n",
      "Train Epoch: 60 [69888/225000 (31%)] Loss: 7265.574219\n",
      "Train Epoch: 60 [73984/225000 (33%)] Loss: 7289.515625\n",
      "Train Epoch: 60 [78080/225000 (35%)] Loss: 7251.335938\n",
      "Train Epoch: 60 [82176/225000 (37%)] Loss: 7337.027344\n",
      "Train Epoch: 60 [86272/225000 (38%)] Loss: 7205.890625\n",
      "Train Epoch: 60 [90368/225000 (40%)] Loss: 7083.134766\n",
      "Train Epoch: 60 [94464/225000 (42%)] Loss: 7243.580078\n",
      "Train Epoch: 60 [98560/225000 (44%)] Loss: 7160.445312\n",
      "Train Epoch: 60 [102656/225000 (46%)] Loss: 7345.402344\n",
      "Train Epoch: 60 [106752/225000 (47%)] Loss: 7411.378906\n",
      "Train Epoch: 60 [110848/225000 (49%)] Loss: 9015.589844\n",
      "Train Epoch: 60 [114944/225000 (51%)] Loss: 7140.423828\n",
      "Train Epoch: 60 [119040/225000 (53%)] Loss: 7270.765625\n",
      "Train Epoch: 60 [123136/225000 (55%)] Loss: 7343.285156\n",
      "Train Epoch: 60 [127232/225000 (57%)] Loss: 7243.343750\n",
      "Train Epoch: 60 [131328/225000 (58%)] Loss: 7321.201172\n",
      "Train Epoch: 60 [135424/225000 (60%)] Loss: 7308.916016\n",
      "Train Epoch: 60 [139520/225000 (62%)] Loss: 7371.183594\n",
      "Train Epoch: 60 [143616/225000 (64%)] Loss: 32069.246094\n",
      "Train Epoch: 60 [147712/225000 (66%)] Loss: 7287.724609\n",
      "Train Epoch: 60 [151808/225000 (67%)] Loss: 7242.710938\n",
      "Train Epoch: 60 [155904/225000 (69%)] Loss: 8901.437500\n",
      "Train Epoch: 60 [160000/225000 (71%)] Loss: 7192.011719\n",
      "Train Epoch: 60 [164096/225000 (73%)] Loss: 7341.937500\n",
      "Train Epoch: 60 [168192/225000 (75%)] Loss: 7256.527344\n",
      "Train Epoch: 60 [172288/225000 (77%)] Loss: 7134.373047\n",
      "Train Epoch: 60 [176384/225000 (78%)] Loss: 7245.527344\n",
      "Train Epoch: 60 [180480/225000 (80%)] Loss: 7422.410156\n",
      "Train Epoch: 60 [184576/225000 (82%)] Loss: 7081.175781\n",
      "Train Epoch: 60 [188672/225000 (84%)] Loss: 7170.708984\n",
      "Train Epoch: 60 [192768/225000 (86%)] Loss: 7208.226562\n",
      "Train Epoch: 60 [196864/225000 (87%)] Loss: 7236.027344\n",
      "Train Epoch: 60 [200960/225000 (89%)] Loss: 7451.177734\n",
      "Train Epoch: 60 [205056/225000 (91%)] Loss: 7215.914062\n",
      "Train Epoch: 60 [209152/225000 (93%)] Loss: 7175.796875\n",
      "Train Epoch: 60 [213248/225000 (95%)] Loss: 7179.980469\n",
      "Train Epoch: 60 [217344/225000 (97%)] Loss: 7199.179688\n",
      "Train Epoch: 60 [221440/225000 (98%)] Loss: 7233.306641\n",
      "    epoch          : 60\n",
      "    loss           : 7634.564847527375\n",
      "    val_loss       : 7306.232990002146\n",
      "Train Epoch: 61 [256/225000 (0%)] Loss: 7401.599609\n",
      "Train Epoch: 61 [4352/225000 (2%)] Loss: 7120.324219\n",
      "Train Epoch: 61 [8448/225000 (4%)] Loss: 7177.027344\n",
      "Train Epoch: 61 [12544/225000 (6%)] Loss: 7114.791016\n",
      "Train Epoch: 61 [16640/225000 (7%)] Loss: 30507.978516\n",
      "Train Epoch: 61 [20736/225000 (9%)] Loss: 7223.910156\n",
      "Train Epoch: 61 [24832/225000 (11%)] Loss: 7243.619141\n",
      "Train Epoch: 61 [28928/225000 (13%)] Loss: 7227.324219\n",
      "Train Epoch: 61 [33024/225000 (15%)] Loss: 7238.371094\n",
      "Train Epoch: 61 [37120/225000 (16%)] Loss: 7174.328125\n",
      "Train Epoch: 61 [41216/225000 (18%)] Loss: 7265.957031\n",
      "Train Epoch: 61 [45312/225000 (20%)] Loss: 7295.947266\n",
      "Train Epoch: 61 [49408/225000 (22%)] Loss: 7194.859375\n",
      "Train Epoch: 61 [53504/225000 (24%)] Loss: 7310.070312\n",
      "Train Epoch: 61 [57600/225000 (26%)] Loss: 7413.675781\n",
      "Train Epoch: 61 [61696/225000 (27%)] Loss: 7218.212891\n",
      "Train Epoch: 61 [65792/225000 (29%)] Loss: 7153.316406\n",
      "Train Epoch: 61 [69888/225000 (31%)] Loss: 7082.119141\n",
      "Train Epoch: 61 [73984/225000 (33%)] Loss: 7318.447266\n",
      "Train Epoch: 61 [78080/225000 (35%)] Loss: 7350.117188\n",
      "Train Epoch: 61 [82176/225000 (37%)] Loss: 7207.091797\n",
      "Train Epoch: 61 [86272/225000 (38%)] Loss: 7150.839844\n",
      "Train Epoch: 61 [90368/225000 (40%)] Loss: 7228.607422\n",
      "Train Epoch: 61 [94464/225000 (42%)] Loss: 7463.074219\n",
      "Train Epoch: 61 [98560/225000 (44%)] Loss: 7227.486328\n",
      "Train Epoch: 61 [102656/225000 (46%)] Loss: 7370.353516\n",
      "Train Epoch: 61 [106752/225000 (47%)] Loss: 7422.902344\n",
      "Train Epoch: 61 [110848/225000 (49%)] Loss: 7137.039062\n",
      "Train Epoch: 61 [114944/225000 (51%)] Loss: 8943.361328\n",
      "Train Epoch: 61 [119040/225000 (53%)] Loss: 7199.291016\n",
      "Train Epoch: 61 [123136/225000 (55%)] Loss: 7267.556641\n",
      "Train Epoch: 61 [127232/225000 (57%)] Loss: 7057.203125\n",
      "Train Epoch: 61 [131328/225000 (58%)] Loss: 7338.068359\n",
      "Train Epoch: 61 [135424/225000 (60%)] Loss: 8955.939453\n",
      "Train Epoch: 61 [139520/225000 (62%)] Loss: 7296.093750\n",
      "Train Epoch: 61 [143616/225000 (64%)] Loss: 7435.654297\n",
      "Train Epoch: 61 [147712/225000 (66%)] Loss: 7247.093750\n",
      "Train Epoch: 61 [151808/225000 (67%)] Loss: 7263.171875\n",
      "Train Epoch: 61 [155904/225000 (69%)] Loss: 7167.734375\n",
      "Train Epoch: 61 [160000/225000 (71%)] Loss: 7229.457031\n",
      "Train Epoch: 61 [164096/225000 (73%)] Loss: 7153.406250\n",
      "Train Epoch: 61 [168192/225000 (75%)] Loss: 7334.107422\n",
      "Train Epoch: 61 [172288/225000 (77%)] Loss: 7246.650391\n",
      "Train Epoch: 61 [176384/225000 (78%)] Loss: 7233.441406\n",
      "Train Epoch: 61 [180480/225000 (80%)] Loss: 7215.404297\n",
      "Train Epoch: 61 [184576/225000 (82%)] Loss: 7320.623047\n",
      "Train Epoch: 61 [188672/225000 (84%)] Loss: 8831.625000\n",
      "Train Epoch: 61 [192768/225000 (86%)] Loss: 7282.625000\n",
      "Train Epoch: 61 [196864/225000 (87%)] Loss: 8858.597656\n",
      "Train Epoch: 61 [200960/225000 (89%)] Loss: 7220.402344\n",
      "Train Epoch: 61 [205056/225000 (91%)] Loss: 7388.802734\n",
      "Train Epoch: 61 [209152/225000 (93%)] Loss: 7178.095703\n",
      "Train Epoch: 61 [213248/225000 (95%)] Loss: 7061.351562\n",
      "Train Epoch: 61 [217344/225000 (97%)] Loss: 7031.498047\n",
      "Train Epoch: 61 [221440/225000 (98%)] Loss: 7138.601562\n",
      "    epoch          : 61\n",
      "    loss           : 7642.108552865472\n",
      "    val_loss       : 7383.057610581116\n",
      "Train Epoch: 62 [256/225000 (0%)] Loss: 8972.148438\n",
      "Train Epoch: 62 [4352/225000 (2%)] Loss: 7241.707031\n",
      "Train Epoch: 62 [8448/225000 (4%)] Loss: 7434.238281\n",
      "Train Epoch: 62 [12544/225000 (6%)] Loss: 7195.916016\n",
      "Train Epoch: 62 [16640/225000 (7%)] Loss: 7183.390625\n",
      "Train Epoch: 62 [20736/225000 (9%)] Loss: 7053.984375\n",
      "Train Epoch: 62 [24832/225000 (11%)] Loss: 7408.205078\n",
      "Train Epoch: 62 [28928/225000 (13%)] Loss: 7157.652344\n",
      "Train Epoch: 62 [33024/225000 (15%)] Loss: 7151.158203\n",
      "Train Epoch: 62 [37120/225000 (16%)] Loss: 7470.138672\n",
      "Train Epoch: 62 [41216/225000 (18%)] Loss: 7269.541016\n",
      "Train Epoch: 62 [45312/225000 (20%)] Loss: 7370.849609\n",
      "Train Epoch: 62 [49408/225000 (22%)] Loss: 7336.384766\n",
      "Train Epoch: 62 [53504/225000 (24%)] Loss: 7178.599609\n",
      "Train Epoch: 62 [57600/225000 (26%)] Loss: 7149.615234\n",
      "Train Epoch: 62 [61696/225000 (27%)] Loss: 7196.232422\n",
      "Train Epoch: 62 [65792/225000 (29%)] Loss: 7031.742188\n",
      "Train Epoch: 62 [69888/225000 (31%)] Loss: 7229.972656\n",
      "Train Epoch: 62 [73984/225000 (33%)] Loss: 7262.312500\n",
      "Train Epoch: 62 [78080/225000 (35%)] Loss: 7183.027344\n",
      "Train Epoch: 62 [82176/225000 (37%)] Loss: 7374.253906\n",
      "Train Epoch: 62 [86272/225000 (38%)] Loss: 7282.437500\n",
      "Train Epoch: 62 [90368/225000 (40%)] Loss: 7371.802734\n",
      "Train Epoch: 62 [94464/225000 (42%)] Loss: 7146.072266\n",
      "Train Epoch: 62 [98560/225000 (44%)] Loss: 7100.683594\n",
      "Train Epoch: 62 [102656/225000 (46%)] Loss: 7285.656250\n",
      "Train Epoch: 62 [106752/225000 (47%)] Loss: 7190.373047\n",
      "Train Epoch: 62 [110848/225000 (49%)] Loss: 7261.701172\n",
      "Train Epoch: 62 [114944/225000 (51%)] Loss: 29631.289062\n",
      "Train Epoch: 62 [119040/225000 (53%)] Loss: 7365.607422\n",
      "Train Epoch: 62 [123136/225000 (55%)] Loss: 7153.400391\n",
      "Train Epoch: 62 [127232/225000 (57%)] Loss: 7162.906250\n",
      "Train Epoch: 62 [131328/225000 (58%)] Loss: 8953.923828\n",
      "Train Epoch: 62 [135424/225000 (60%)] Loss: 7134.707031\n",
      "Train Epoch: 62 [139520/225000 (62%)] Loss: 7274.705078\n",
      "Train Epoch: 62 [143616/225000 (64%)] Loss: 7379.146484\n",
      "Train Epoch: 62 [147712/225000 (66%)] Loss: 7130.488281\n",
      "Train Epoch: 62 [151808/225000 (67%)] Loss: 7282.740234\n",
      "Train Epoch: 62 [155904/225000 (69%)] Loss: 7278.132812\n",
      "Train Epoch: 62 [160000/225000 (71%)] Loss: 7148.375000\n",
      "Train Epoch: 62 [164096/225000 (73%)] Loss: 7176.873047\n",
      "Train Epoch: 62 [168192/225000 (75%)] Loss: 7399.628906\n",
      "Train Epoch: 62 [172288/225000 (77%)] Loss: 7419.263672\n",
      "Train Epoch: 62 [176384/225000 (78%)] Loss: 7296.750000\n",
      "Train Epoch: 62 [180480/225000 (80%)] Loss: 7105.943359\n",
      "Train Epoch: 62 [184576/225000 (82%)] Loss: 7195.445312\n",
      "Train Epoch: 62 [188672/225000 (84%)] Loss: 7172.429688\n",
      "Train Epoch: 62 [192768/225000 (86%)] Loss: 7463.828125\n",
      "Train Epoch: 62 [196864/225000 (87%)] Loss: 7289.847656\n",
      "Train Epoch: 62 [200960/225000 (89%)] Loss: 7304.785156\n",
      "Train Epoch: 62 [205056/225000 (91%)] Loss: 7114.691406\n",
      "Train Epoch: 62 [209152/225000 (93%)] Loss: 7230.521484\n",
      "Train Epoch: 62 [213248/225000 (95%)] Loss: 7233.988281\n",
      "Train Epoch: 62 [217344/225000 (97%)] Loss: 7263.484375\n",
      "Train Epoch: 62 [221440/225000 (98%)] Loss: 7258.355469\n",
      "    epoch          : 62\n",
      "    loss           : 7452.305788493672\n",
      "    val_loss       : 7317.757307288598\n",
      "Train Epoch: 63 [256/225000 (0%)] Loss: 7233.541016\n",
      "Train Epoch: 63 [4352/225000 (2%)] Loss: 7167.169922\n",
      "Train Epoch: 63 [8448/225000 (4%)] Loss: 7116.787109\n",
      "Train Epoch: 63 [12544/225000 (6%)] Loss: 7252.556641\n",
      "Train Epoch: 63 [16640/225000 (7%)] Loss: 7039.296875\n",
      "Train Epoch: 63 [20736/225000 (9%)] Loss: 7241.533203\n",
      "Train Epoch: 63 [24832/225000 (11%)] Loss: 7253.433594\n",
      "Train Epoch: 63 [28928/225000 (13%)] Loss: 7115.644531\n",
      "Train Epoch: 63 [33024/225000 (15%)] Loss: 7231.318359\n",
      "Train Epoch: 63 [37120/225000 (16%)] Loss: 7148.859375\n",
      "Train Epoch: 63 [41216/225000 (18%)] Loss: 7084.763672\n",
      "Train Epoch: 63 [45312/225000 (20%)] Loss: 7089.236328\n",
      "Train Epoch: 63 [49408/225000 (22%)] Loss: 7182.128906\n",
      "Train Epoch: 63 [53504/225000 (24%)] Loss: 7282.476562\n",
      "Train Epoch: 63 [57600/225000 (26%)] Loss: 7181.230469\n",
      "Train Epoch: 63 [61696/225000 (27%)] Loss: 7025.824219\n",
      "Train Epoch: 63 [65792/225000 (29%)] Loss: 7286.046875\n",
      "Train Epoch: 63 [69888/225000 (31%)] Loss: 7152.566406\n",
      "Train Epoch: 63 [73984/225000 (33%)] Loss: 7091.013672\n",
      "Train Epoch: 63 [78080/225000 (35%)] Loss: 7268.298828\n",
      "Train Epoch: 63 [82176/225000 (37%)] Loss: 7260.794922\n",
      "Train Epoch: 63 [86272/225000 (38%)] Loss: 7311.371094\n",
      "Train Epoch: 63 [90368/225000 (40%)] Loss: 7169.755859\n",
      "Train Epoch: 63 [94464/225000 (42%)] Loss: 7263.591797\n",
      "Train Epoch: 63 [98560/225000 (44%)] Loss: 7360.734375\n",
      "Train Epoch: 63 [102656/225000 (46%)] Loss: 7326.476562\n",
      "Train Epoch: 63 [106752/225000 (47%)] Loss: 7021.427734\n",
      "Train Epoch: 63 [110848/225000 (49%)] Loss: 7188.521484\n",
      "Train Epoch: 63 [114944/225000 (51%)] Loss: 7295.001953\n",
      "Train Epoch: 63 [119040/225000 (53%)] Loss: 7061.042969\n",
      "Train Epoch: 63 [123136/225000 (55%)] Loss: 7055.072266\n",
      "Train Epoch: 63 [127232/225000 (57%)] Loss: 7031.294922\n",
      "Train Epoch: 63 [131328/225000 (58%)] Loss: 7173.759766\n",
      "Train Epoch: 63 [135424/225000 (60%)] Loss: 7053.228516\n",
      "Train Epoch: 63 [139520/225000 (62%)] Loss: 7263.941406\n",
      "Train Epoch: 63 [143616/225000 (64%)] Loss: 7421.892578\n",
      "Train Epoch: 63 [147712/225000 (66%)] Loss: 7417.287109\n",
      "Train Epoch: 63 [151808/225000 (67%)] Loss: 7239.373047\n",
      "Train Epoch: 63 [155904/225000 (69%)] Loss: 7263.199219\n",
      "Train Epoch: 63 [160000/225000 (71%)] Loss: 7123.162109\n",
      "Train Epoch: 63 [164096/225000 (73%)] Loss: 7298.675781\n",
      "Train Epoch: 63 [168192/225000 (75%)] Loss: 7221.054688\n",
      "Train Epoch: 63 [172288/225000 (77%)] Loss: 7268.623047\n",
      "Train Epoch: 63 [176384/225000 (78%)] Loss: 7094.671875\n",
      "Train Epoch: 63 [180480/225000 (80%)] Loss: 7243.646484\n",
      "Train Epoch: 63 [184576/225000 (82%)] Loss: 7077.136719\n",
      "Train Epoch: 63 [188672/225000 (84%)] Loss: 7217.841797\n",
      "Train Epoch: 63 [192768/225000 (86%)] Loss: 7173.349609\n",
      "Train Epoch: 63 [196864/225000 (87%)] Loss: 7245.794922\n",
      "Train Epoch: 63 [200960/225000 (89%)] Loss: 7332.611328\n",
      "Train Epoch: 63 [205056/225000 (91%)] Loss: 7376.722656\n",
      "Train Epoch: 63 [209152/225000 (93%)] Loss: 7359.501953\n",
      "Train Epoch: 63 [213248/225000 (95%)] Loss: 7132.802734\n",
      "Train Epoch: 63 [217344/225000 (97%)] Loss: 7131.642578\n",
      "Train Epoch: 63 [221440/225000 (98%)] Loss: 7039.351562\n",
      "    epoch          : 63\n",
      "    loss           : 7291.617093065629\n",
      "    val_loss       : 7228.425960733574\n",
      "Train Epoch: 64 [256/225000 (0%)] Loss: 7187.832031\n",
      "Train Epoch: 64 [4352/225000 (2%)] Loss: 7128.753906\n",
      "Train Epoch: 64 [8448/225000 (4%)] Loss: 7229.847656\n",
      "Train Epoch: 64 [12544/225000 (6%)] Loss: 7161.042969\n",
      "Train Epoch: 64 [16640/225000 (7%)] Loss: 7248.099609\n",
      "Train Epoch: 64 [20736/225000 (9%)] Loss: 7176.185547\n",
      "Train Epoch: 64 [24832/225000 (11%)] Loss: 7273.923828\n",
      "Train Epoch: 64 [28928/225000 (13%)] Loss: 7146.119141\n",
      "Train Epoch: 64 [33024/225000 (15%)] Loss: 7170.523438\n",
      "Train Epoch: 64 [37120/225000 (16%)] Loss: 7222.101562\n",
      "Train Epoch: 64 [41216/225000 (18%)] Loss: 7245.105469\n",
      "Train Epoch: 64 [45312/225000 (20%)] Loss: 7160.070312\n",
      "Train Epoch: 64 [49408/225000 (22%)] Loss: 7191.222656\n",
      "Train Epoch: 64 [53504/225000 (24%)] Loss: 7180.853516\n",
      "Train Epoch: 64 [57600/225000 (26%)] Loss: 6988.623047\n",
      "Train Epoch: 64 [61696/225000 (27%)] Loss: 7199.091797\n",
      "Train Epoch: 64 [65792/225000 (29%)] Loss: 7180.373047\n",
      "Train Epoch: 64 [69888/225000 (31%)] Loss: 7286.609375\n",
      "Train Epoch: 64 [73984/225000 (33%)] Loss: 7153.322266\n",
      "Train Epoch: 64 [78080/225000 (35%)] Loss: 7218.658203\n",
      "Train Epoch: 64 [82176/225000 (37%)] Loss: 7187.875000\n",
      "Train Epoch: 64 [86272/225000 (38%)] Loss: 7211.861328\n",
      "Train Epoch: 64 [90368/225000 (40%)] Loss: 7333.585938\n",
      "Train Epoch: 64 [94464/225000 (42%)] Loss: 7237.076172\n",
      "Train Epoch: 64 [98560/225000 (44%)] Loss: 7283.494141\n",
      "Train Epoch: 64 [102656/225000 (46%)] Loss: 7063.177734\n",
      "Train Epoch: 64 [106752/225000 (47%)] Loss: 7001.476562\n",
      "Train Epoch: 64 [110848/225000 (49%)] Loss: 7243.480469\n",
      "Train Epoch: 64 [114944/225000 (51%)] Loss: 7176.816406\n",
      "Train Epoch: 64 [119040/225000 (53%)] Loss: 7274.716797\n",
      "Train Epoch: 64 [123136/225000 (55%)] Loss: 7243.181641\n",
      "Train Epoch: 64 [127232/225000 (57%)] Loss: 7082.544922\n",
      "Train Epoch: 64 [131328/225000 (58%)] Loss: 7189.394531\n",
      "Train Epoch: 64 [135424/225000 (60%)] Loss: 7281.736328\n",
      "Train Epoch: 64 [139520/225000 (62%)] Loss: 7128.589844\n",
      "Train Epoch: 64 [143616/225000 (64%)] Loss: 7234.642578\n",
      "Train Epoch: 64 [147712/225000 (66%)] Loss: 7260.867188\n",
      "Train Epoch: 64 [151808/225000 (67%)] Loss: 7200.189453\n",
      "Train Epoch: 64 [155904/225000 (69%)] Loss: 7191.824219\n",
      "Train Epoch: 64 [160000/225000 (71%)] Loss: 7032.494141\n",
      "Train Epoch: 64 [164096/225000 (73%)] Loss: 7221.705078\n",
      "Train Epoch: 64 [168192/225000 (75%)] Loss: 7404.814453\n",
      "Train Epoch: 64 [172288/225000 (77%)] Loss: 7195.353516\n",
      "Train Epoch: 64 [176384/225000 (78%)] Loss: 7175.074219\n",
      "Train Epoch: 64 [180480/225000 (80%)] Loss: 7240.164062\n",
      "Train Epoch: 64 [184576/225000 (82%)] Loss: 7153.041016\n",
      "Train Epoch: 64 [188672/225000 (84%)] Loss: 7006.806641\n",
      "Train Epoch: 64 [192768/225000 (86%)] Loss: 7214.224609\n",
      "Train Epoch: 64 [196864/225000 (87%)] Loss: 7132.187500\n",
      "Train Epoch: 64 [200960/225000 (89%)] Loss: 7287.068359\n",
      "Train Epoch: 64 [205056/225000 (91%)] Loss: 7196.025391\n",
      "Train Epoch: 64 [209152/225000 (93%)] Loss: 7193.361328\n",
      "Train Epoch: 64 [213248/225000 (95%)] Loss: 7058.289062\n",
      "Train Epoch: 64 [217344/225000 (97%)] Loss: 7245.687500\n",
      "Train Epoch: 64 [221440/225000 (98%)] Loss: 7198.431641\n",
      "    epoch          : 64\n",
      "    loss           : 7249.044036413894\n",
      "    val_loss       : 7183.480528645369\n",
      "Train Epoch: 65 [256/225000 (0%)] Loss: 7112.105469\n",
      "Train Epoch: 65 [4352/225000 (2%)] Loss: 7326.921875\n",
      "Train Epoch: 65 [8448/225000 (4%)] Loss: 7145.019531\n",
      "Train Epoch: 65 [12544/225000 (6%)] Loss: 7167.228516\n",
      "Train Epoch: 65 [16640/225000 (7%)] Loss: 7270.357422\n",
      "Train Epoch: 65 [20736/225000 (9%)] Loss: 7267.230469\n",
      "Train Epoch: 65 [24832/225000 (11%)] Loss: 7198.683594\n",
      "Train Epoch: 65 [28928/225000 (13%)] Loss: 7172.507812\n",
      "Train Epoch: 65 [33024/225000 (15%)] Loss: 7322.203125\n",
      "Train Epoch: 65 [37120/225000 (16%)] Loss: 7198.130859\n",
      "Train Epoch: 65 [41216/225000 (18%)] Loss: 7114.220703\n",
      "Train Epoch: 65 [45312/225000 (20%)] Loss: 7142.888672\n",
      "Train Epoch: 65 [49408/225000 (22%)] Loss: 6980.015625\n",
      "Train Epoch: 65 [53504/225000 (24%)] Loss: 7222.671875\n",
      "Train Epoch: 65 [57600/225000 (26%)] Loss: 6943.425781\n",
      "Train Epoch: 65 [61696/225000 (27%)] Loss: 7243.818359\n",
      "Train Epoch: 65 [65792/225000 (29%)] Loss: 7112.410156\n",
      "Train Epoch: 65 [69888/225000 (31%)] Loss: 7159.654297\n",
      "Train Epoch: 65 [73984/225000 (33%)] Loss: 7120.955078\n",
      "Train Epoch: 65 [78080/225000 (35%)] Loss: 7128.123047\n",
      "Train Epoch: 65 [82176/225000 (37%)] Loss: 7123.998047\n",
      "Train Epoch: 65 [86272/225000 (38%)] Loss: 7094.861328\n",
      "Train Epoch: 65 [90368/225000 (40%)] Loss: 7147.322266\n",
      "Train Epoch: 65 [94464/225000 (42%)] Loss: 7388.193359\n",
      "Train Epoch: 65 [98560/225000 (44%)] Loss: 7235.525391\n",
      "Train Epoch: 65 [102656/225000 (46%)] Loss: 7186.794922\n",
      "Train Epoch: 65 [106752/225000 (47%)] Loss: 7162.609375\n",
      "Train Epoch: 65 [110848/225000 (49%)] Loss: 7399.324219\n",
      "Train Epoch: 65 [114944/225000 (51%)] Loss: 7048.095703\n",
      "Train Epoch: 65 [119040/225000 (53%)] Loss: 7220.765625\n",
      "Train Epoch: 65 [123136/225000 (55%)] Loss: 7193.695312\n",
      "Train Epoch: 65 [127232/225000 (57%)] Loss: 7197.212891\n",
      "Train Epoch: 65 [131328/225000 (58%)] Loss: 7149.445312\n",
      "Train Epoch: 65 [135424/225000 (60%)] Loss: 7422.357422\n",
      "Train Epoch: 65 [139520/225000 (62%)] Loss: 7307.228516\n",
      "Train Epoch: 65 [143616/225000 (64%)] Loss: 7301.732422\n",
      "Train Epoch: 65 [147712/225000 (66%)] Loss: 6996.423828\n",
      "Train Epoch: 65 [151808/225000 (67%)] Loss: 7155.916016\n",
      "Train Epoch: 65 [155904/225000 (69%)] Loss: 7128.337891\n",
      "Train Epoch: 65 [160000/225000 (71%)] Loss: 7206.384766\n",
      "Train Epoch: 65 [164096/225000 (73%)] Loss: 7181.316406\n",
      "Train Epoch: 65 [168192/225000 (75%)] Loss: 7166.531250\n",
      "Train Epoch: 65 [172288/225000 (77%)] Loss: 7264.902344\n",
      "Train Epoch: 65 [176384/225000 (78%)] Loss: 7190.535156\n",
      "Train Epoch: 65 [180480/225000 (80%)] Loss: 7023.216797\n",
      "Train Epoch: 65 [184576/225000 (82%)] Loss: 7099.873047\n",
      "Train Epoch: 65 [188672/225000 (84%)] Loss: 7146.955078\n",
      "Train Epoch: 65 [192768/225000 (86%)] Loss: 7310.427734\n",
      "Train Epoch: 65 [196864/225000 (87%)] Loss: 7104.632812\n",
      "Train Epoch: 65 [200960/225000 (89%)] Loss: 7244.093750\n",
      "Train Epoch: 65 [205056/225000 (91%)] Loss: 7349.910156\n",
      "Train Epoch: 65 [209152/225000 (93%)] Loss: 6977.117188\n",
      "Train Epoch: 65 [213248/225000 (95%)] Loss: 7159.148438\n",
      "Train Epoch: 65 [217344/225000 (97%)] Loss: 7200.392578\n",
      "Train Epoch: 65 [221440/225000 (98%)] Loss: 7148.314453\n",
      "    epoch          : 65\n",
      "    loss           : 7212.1372420275175\n",
      "    val_loss       : 7515.116377663856\n",
      "Train Epoch: 66 [256/225000 (0%)] Loss: 7240.673828\n",
      "Train Epoch: 66 [4352/225000 (2%)] Loss: 7152.183594\n",
      "Train Epoch: 66 [8448/225000 (4%)] Loss: 7240.548828\n",
      "Train Epoch: 66 [12544/225000 (6%)] Loss: 7127.554688\n",
      "Train Epoch: 66 [16640/225000 (7%)] Loss: 7159.480469\n",
      "Train Epoch: 66 [20736/225000 (9%)] Loss: 7233.707031\n",
      "Train Epoch: 66 [24832/225000 (11%)] Loss: 7181.945312\n",
      "Train Epoch: 66 [28928/225000 (13%)] Loss: 7074.886719\n",
      "Train Epoch: 66 [33024/225000 (15%)] Loss: 6943.818359\n",
      "Train Epoch: 66 [37120/225000 (16%)] Loss: 7096.982422\n",
      "Train Epoch: 66 [41216/225000 (18%)] Loss: 7252.255859\n",
      "Train Epoch: 66 [45312/225000 (20%)] Loss: 7108.542969\n",
      "Train Epoch: 66 [49408/225000 (22%)] Loss: 7149.828125\n",
      "Train Epoch: 66 [53504/225000 (24%)] Loss: 7244.177734\n",
      "Train Epoch: 66 [57600/225000 (26%)] Loss: 7208.222656\n",
      "Train Epoch: 66 [61696/225000 (27%)] Loss: 7196.994141\n",
      "Train Epoch: 66 [65792/225000 (29%)] Loss: 7046.312500\n",
      "Train Epoch: 66 [69888/225000 (31%)] Loss: 7230.035156\n",
      "Train Epoch: 66 [73984/225000 (33%)] Loss: 7126.664062\n",
      "Train Epoch: 66 [78080/225000 (35%)] Loss: 7385.031250\n",
      "Train Epoch: 66 [82176/225000 (37%)] Loss: 7120.542969\n",
      "Train Epoch: 66 [86272/225000 (38%)] Loss: 7089.367188\n",
      "Train Epoch: 66 [90368/225000 (40%)] Loss: 7096.462891\n",
      "Train Epoch: 66 [94464/225000 (42%)] Loss: 7006.363281\n",
      "Train Epoch: 66 [98560/225000 (44%)] Loss: 7447.800781\n",
      "Train Epoch: 66 [102656/225000 (46%)] Loss: 7153.287109\n",
      "Train Epoch: 66 [106752/225000 (47%)] Loss: 7067.343750\n",
      "Train Epoch: 66 [110848/225000 (49%)] Loss: 7234.548828\n",
      "Train Epoch: 66 [114944/225000 (51%)] Loss: 7116.464844\n",
      "Train Epoch: 66 [119040/225000 (53%)] Loss: 7153.751953\n",
      "Train Epoch: 66 [123136/225000 (55%)] Loss: 7109.072266\n",
      "Train Epoch: 66 [127232/225000 (57%)] Loss: 7136.570312\n",
      "Train Epoch: 66 [131328/225000 (58%)] Loss: 7169.257812\n",
      "Train Epoch: 66 [135424/225000 (60%)] Loss: 7025.558594\n",
      "Train Epoch: 66 [139520/225000 (62%)] Loss: 7242.130859\n",
      "Train Epoch: 66 [143616/225000 (64%)] Loss: 7164.796875\n",
      "Train Epoch: 66 [147712/225000 (66%)] Loss: 7191.335938\n",
      "Train Epoch: 66 [151808/225000 (67%)] Loss: 7067.724609\n",
      "Train Epoch: 66 [155904/225000 (69%)] Loss: 6962.562500\n",
      "Train Epoch: 66 [160000/225000 (71%)] Loss: 7116.556641\n",
      "Train Epoch: 66 [164096/225000 (73%)] Loss: 7172.267578\n",
      "Train Epoch: 66 [168192/225000 (75%)] Loss: 7266.750000\n",
      "Train Epoch: 66 [172288/225000 (77%)] Loss: 7118.359375\n",
      "Train Epoch: 66 [176384/225000 (78%)] Loss: 7217.511719\n",
      "Train Epoch: 66 [180480/225000 (80%)] Loss: 7052.322266\n",
      "Train Epoch: 66 [184576/225000 (82%)] Loss: 7066.906250\n",
      "Train Epoch: 66 [188672/225000 (84%)] Loss: 6999.162109\n",
      "Train Epoch: 66 [192768/225000 (86%)] Loss: 7351.076172\n",
      "Train Epoch: 66 [196864/225000 (87%)] Loss: 7215.041016\n",
      "Train Epoch: 66 [200960/225000 (89%)] Loss: 7211.023438\n",
      "Train Epoch: 66 [205056/225000 (91%)] Loss: 7112.138672\n",
      "Train Epoch: 66 [209152/225000 (93%)] Loss: 7175.007812\n",
      "Train Epoch: 66 [213248/225000 (95%)] Loss: 7161.972656\n",
      "Train Epoch: 66 [217344/225000 (97%)] Loss: 7135.718750\n",
      "Train Epoch: 66 [221440/225000 (98%)] Loss: 7160.164062\n",
      "    epoch          : 66\n",
      "    loss           : 7234.659348558376\n",
      "    val_loss       : 7159.024191884362\n",
      "Train Epoch: 67 [256/225000 (0%)] Loss: 6979.349609\n",
      "Train Epoch: 67 [4352/225000 (2%)] Loss: 7147.667969\n",
      "Train Epoch: 67 [8448/225000 (4%)] Loss: 6996.562500\n",
      "Train Epoch: 67 [12544/225000 (6%)] Loss: 7175.972656\n",
      "Train Epoch: 67 [16640/225000 (7%)] Loss: 7362.865234\n",
      "Train Epoch: 67 [20736/225000 (9%)] Loss: 7079.652344\n",
      "Train Epoch: 67 [24832/225000 (11%)] Loss: 7157.074219\n",
      "Train Epoch: 67 [28928/225000 (13%)] Loss: 7107.667969\n",
      "Train Epoch: 67 [33024/225000 (15%)] Loss: 7202.716797\n",
      "Train Epoch: 67 [37120/225000 (16%)] Loss: 7143.255859\n",
      "Train Epoch: 67 [41216/225000 (18%)] Loss: 7147.794922\n",
      "Train Epoch: 67 [45312/225000 (20%)] Loss: 7127.152344\n",
      "Train Epoch: 67 [49408/225000 (22%)] Loss: 7131.250000\n",
      "Train Epoch: 67 [53504/225000 (24%)] Loss: 7084.294922\n",
      "Train Epoch: 67 [57600/225000 (26%)] Loss: 7105.398438\n",
      "Train Epoch: 67 [61696/225000 (27%)] Loss: 7219.222656\n",
      "Train Epoch: 67 [65792/225000 (29%)] Loss: 7132.589844\n",
      "Train Epoch: 67 [69888/225000 (31%)] Loss: 7216.351562\n",
      "Train Epoch: 67 [73984/225000 (33%)] Loss: 7208.396484\n",
      "Train Epoch: 67 [78080/225000 (35%)] Loss: 7107.335938\n",
      "Train Epoch: 67 [82176/225000 (37%)] Loss: 7342.914062\n",
      "Train Epoch: 67 [86272/225000 (38%)] Loss: 7056.117188\n",
      "Train Epoch: 67 [90368/225000 (40%)] Loss: 7004.978516\n",
      "Train Epoch: 67 [94464/225000 (42%)] Loss: 7239.337891\n",
      "Train Epoch: 67 [98560/225000 (44%)] Loss: 7249.320312\n",
      "Train Epoch: 67 [102656/225000 (46%)] Loss: 7086.242188\n",
      "Train Epoch: 67 [106752/225000 (47%)] Loss: 7513.572266\n",
      "Train Epoch: 67 [110848/225000 (49%)] Loss: 7107.095703\n",
      "Train Epoch: 67 [114944/225000 (51%)] Loss: 7135.935547\n",
      "Train Epoch: 67 [119040/225000 (53%)] Loss: 7046.769531\n",
      "Train Epoch: 67 [123136/225000 (55%)] Loss: 7229.876953\n",
      "Train Epoch: 67 [127232/225000 (57%)] Loss: 7159.226562\n",
      "Train Epoch: 67 [131328/225000 (58%)] Loss: 7036.189453\n",
      "Train Epoch: 67 [135424/225000 (60%)] Loss: 7110.685547\n",
      "Train Epoch: 67 [139520/225000 (62%)] Loss: 7037.957031\n",
      "Train Epoch: 67 [143616/225000 (64%)] Loss: 7088.849609\n",
      "Train Epoch: 67 [147712/225000 (66%)] Loss: 7236.490234\n",
      "Train Epoch: 67 [151808/225000 (67%)] Loss: 7151.619141\n",
      "Train Epoch: 67 [155904/225000 (69%)] Loss: 7183.035156\n",
      "Train Epoch: 67 [160000/225000 (71%)] Loss: 7028.857422\n",
      "Train Epoch: 67 [164096/225000 (73%)] Loss: 7206.750000\n",
      "Train Epoch: 67 [168192/225000 (75%)] Loss: 7197.943359\n",
      "Train Epoch: 67 [172288/225000 (77%)] Loss: 7059.632812\n",
      "Train Epoch: 67 [176384/225000 (78%)] Loss: 7111.365234\n",
      "Train Epoch: 67 [180480/225000 (80%)] Loss: 8828.107422\n",
      "Train Epoch: 67 [184576/225000 (82%)] Loss: 7160.289062\n",
      "Train Epoch: 67 [188672/225000 (84%)] Loss: 7110.419922\n",
      "Train Epoch: 67 [192768/225000 (86%)] Loss: 7061.861328\n",
      "Train Epoch: 67 [196864/225000 (87%)] Loss: 7247.675781\n",
      "Train Epoch: 67 [200960/225000 (89%)] Loss: 7192.968750\n",
      "Train Epoch: 67 [205056/225000 (91%)] Loss: 7256.603516\n",
      "Train Epoch: 67 [209152/225000 (93%)] Loss: 7282.560547\n",
      "Train Epoch: 67 [213248/225000 (95%)] Loss: 7067.654297\n",
      "Train Epoch: 67 [217344/225000 (97%)] Loss: 7283.648438\n",
      "Train Epoch: 67 [221440/225000 (98%)] Loss: 7083.539062\n",
      "    epoch          : 67\n",
      "    loss           : 7355.212078489406\n",
      "    val_loss       : 7170.868015443184\n",
      "Train Epoch: 68 [256/225000 (0%)] Loss: 7156.136719\n",
      "Train Epoch: 68 [4352/225000 (2%)] Loss: 7107.951172\n",
      "Train Epoch: 68 [8448/225000 (4%)] Loss: 7148.414062\n",
      "Train Epoch: 68 [12544/225000 (6%)] Loss: 6968.042969\n",
      "Train Epoch: 68 [16640/225000 (7%)] Loss: 7140.400391\n",
      "Train Epoch: 68 [20736/225000 (9%)] Loss: 7255.353516\n",
      "Train Epoch: 68 [24832/225000 (11%)] Loss: 7140.619141\n",
      "Train Epoch: 68 [28928/225000 (13%)] Loss: 7215.199219\n",
      "Train Epoch: 68 [33024/225000 (15%)] Loss: 7188.626953\n",
      "Train Epoch: 68 [37120/225000 (16%)] Loss: 7166.894531\n",
      "Train Epoch: 68 [41216/225000 (18%)] Loss: 7109.457031\n",
      "Train Epoch: 68 [45312/225000 (20%)] Loss: 7116.941406\n",
      "Train Epoch: 68 [49408/225000 (22%)] Loss: 7167.042969\n",
      "Train Epoch: 68 [53504/225000 (24%)] Loss: 7220.640625\n",
      "Train Epoch: 68 [57600/225000 (26%)] Loss: 6991.136719\n",
      "Train Epoch: 68 [61696/225000 (27%)] Loss: 7098.681641\n",
      "Train Epoch: 68 [65792/225000 (29%)] Loss: 7084.642578\n",
      "Train Epoch: 68 [69888/225000 (31%)] Loss: 7045.605469\n",
      "Train Epoch: 68 [73984/225000 (33%)] Loss: 7022.193359\n",
      "Train Epoch: 68 [78080/225000 (35%)] Loss: 6885.431641\n",
      "Train Epoch: 68 [82176/225000 (37%)] Loss: 7149.281250\n",
      "Train Epoch: 68 [86272/225000 (38%)] Loss: 6992.375000\n",
      "Train Epoch: 68 [90368/225000 (40%)] Loss: 7113.044922\n",
      "Train Epoch: 68 [94464/225000 (42%)] Loss: 7199.804688\n",
      "Train Epoch: 68 [98560/225000 (44%)] Loss: 7014.125000\n",
      "Train Epoch: 68 [102656/225000 (46%)] Loss: 7216.099609\n",
      "Train Epoch: 68 [106752/225000 (47%)] Loss: 7076.107422\n",
      "Train Epoch: 68 [110848/225000 (49%)] Loss: 7050.369141\n",
      "Train Epoch: 68 [114944/225000 (51%)] Loss: 7170.650391\n",
      "Train Epoch: 68 [119040/225000 (53%)] Loss: 7122.919922\n",
      "Train Epoch: 68 [123136/225000 (55%)] Loss: 7104.783203\n",
      "Train Epoch: 68 [127232/225000 (57%)] Loss: 7251.894531\n",
      "Train Epoch: 68 [131328/225000 (58%)] Loss: 7093.951172\n",
      "Train Epoch: 68 [135424/225000 (60%)] Loss: 7181.910156\n",
      "Train Epoch: 68 [139520/225000 (62%)] Loss: 7007.789062\n",
      "Train Epoch: 68 [143616/225000 (64%)] Loss: 7211.894531\n",
      "Train Epoch: 68 [147712/225000 (66%)] Loss: 7175.904297\n",
      "Train Epoch: 68 [151808/225000 (67%)] Loss: 7045.941406\n",
      "Train Epoch: 68 [155904/225000 (69%)] Loss: 7308.818359\n",
      "Train Epoch: 68 [160000/225000 (71%)] Loss: 7054.943359\n",
      "Train Epoch: 68 [164096/225000 (73%)] Loss: 7112.812500\n",
      "Train Epoch: 68 [168192/225000 (75%)] Loss: 7097.867188\n",
      "Train Epoch: 68 [172288/225000 (77%)] Loss: 7243.896484\n",
      "Train Epoch: 68 [176384/225000 (78%)] Loss: 7273.189453\n",
      "Train Epoch: 68 [180480/225000 (80%)] Loss: 7197.228516\n",
      "Train Epoch: 68 [184576/225000 (82%)] Loss: 7154.296875\n",
      "Train Epoch: 68 [188672/225000 (84%)] Loss: 7302.394531\n",
      "Train Epoch: 68 [192768/225000 (86%)] Loss: 7327.457031\n",
      "Train Epoch: 68 [196864/225000 (87%)] Loss: 7126.097656\n",
      "Train Epoch: 68 [200960/225000 (89%)] Loss: 6983.435547\n",
      "Train Epoch: 68 [205056/225000 (91%)] Loss: 7249.179688\n",
      "Train Epoch: 68 [209152/225000 (93%)] Loss: 7264.412109\n",
      "Train Epoch: 68 [213248/225000 (95%)] Loss: 7210.585938\n",
      "Train Epoch: 68 [217344/225000 (97%)] Loss: 7227.488281\n",
      "Train Epoch: 68 [221440/225000 (98%)] Loss: 7097.384766\n",
      "    epoch          : 68\n",
      "    loss           : 7247.833077805034\n",
      "    val_loss       : 7113.185249404031\n",
      "Train Epoch: 69 [256/225000 (0%)] Loss: 7311.472656\n",
      "Train Epoch: 69 [4352/225000 (2%)] Loss: 7004.439453\n",
      "Train Epoch: 69 [8448/225000 (4%)] Loss: 7144.078125\n",
      "Train Epoch: 69 [12544/225000 (6%)] Loss: 7265.240234\n",
      "Train Epoch: 69 [16640/225000 (7%)] Loss: 6938.435547\n",
      "Train Epoch: 69 [20736/225000 (9%)] Loss: 7207.457031\n",
      "Train Epoch: 69 [24832/225000 (11%)] Loss: 7130.128906\n",
      "Train Epoch: 69 [28928/225000 (13%)] Loss: 7240.496094\n",
      "Train Epoch: 69 [33024/225000 (15%)] Loss: 7179.511719\n",
      "Train Epoch: 69 [37120/225000 (16%)] Loss: 7189.125000\n",
      "Train Epoch: 69 [41216/225000 (18%)] Loss: 7194.925781\n",
      "Train Epoch: 69 [45312/225000 (20%)] Loss: 7115.982422\n",
      "Train Epoch: 69 [49408/225000 (22%)] Loss: 7171.900391\n",
      "Train Epoch: 69 [53504/225000 (24%)] Loss: 7263.763672\n",
      "Train Epoch: 69 [57600/225000 (26%)] Loss: 7124.078125\n",
      "Train Epoch: 69 [61696/225000 (27%)] Loss: 7057.195312\n",
      "Train Epoch: 69 [65792/225000 (29%)] Loss: 7160.773438\n",
      "Train Epoch: 69 [69888/225000 (31%)] Loss: 7019.158203\n",
      "Train Epoch: 69 [73984/225000 (33%)] Loss: 7015.906250\n",
      "Train Epoch: 69 [78080/225000 (35%)] Loss: 7056.582031\n",
      "Train Epoch: 69 [82176/225000 (37%)] Loss: 7059.863281\n",
      "Train Epoch: 69 [86272/225000 (38%)] Loss: 7357.599609\n",
      "Train Epoch: 69 [90368/225000 (40%)] Loss: 7154.103516\n",
      "Train Epoch: 69 [94464/225000 (42%)] Loss: 7085.314453\n",
      "Train Epoch: 69 [98560/225000 (44%)] Loss: 7240.835938\n",
      "Train Epoch: 69 [102656/225000 (46%)] Loss: 7208.011719\n",
      "Train Epoch: 69 [106752/225000 (47%)] Loss: 7208.154297\n",
      "Train Epoch: 69 [110848/225000 (49%)] Loss: 7000.035156\n",
      "Train Epoch: 69 [114944/225000 (51%)] Loss: 7000.769531\n",
      "Train Epoch: 69 [119040/225000 (53%)] Loss: 7104.400391\n",
      "Train Epoch: 69 [123136/225000 (55%)] Loss: 6981.068359\n",
      "Train Epoch: 69 [127232/225000 (57%)] Loss: 7134.937500\n",
      "Train Epoch: 69 [131328/225000 (58%)] Loss: 7247.470703\n",
      "Train Epoch: 69 [135424/225000 (60%)] Loss: 7099.708984\n",
      "Train Epoch: 69 [139520/225000 (62%)] Loss: 7021.078125\n",
      "Train Epoch: 69 [143616/225000 (64%)] Loss: 7037.871094\n",
      "Train Epoch: 69 [147712/225000 (66%)] Loss: 7114.671875\n",
      "Train Epoch: 69 [151808/225000 (67%)] Loss: 7121.248047\n",
      "Train Epoch: 69 [155904/225000 (69%)] Loss: 7186.236328\n",
      "Train Epoch: 69 [160000/225000 (71%)] Loss: 7104.125000\n",
      "Train Epoch: 69 [164096/225000 (73%)] Loss: 7034.367188\n",
      "Train Epoch: 69 [168192/225000 (75%)] Loss: 7066.074219\n",
      "Train Epoch: 69 [172288/225000 (77%)] Loss: 7045.773438\n",
      "Train Epoch: 69 [176384/225000 (78%)] Loss: 7059.294922\n",
      "Train Epoch: 69 [180480/225000 (80%)] Loss: 7031.431641\n",
      "Train Epoch: 69 [184576/225000 (82%)] Loss: 7115.199219\n",
      "Train Epoch: 69 [188672/225000 (84%)] Loss: 7099.958984\n",
      "Train Epoch: 69 [192768/225000 (86%)] Loss: 7055.013672\n",
      "Train Epoch: 69 [196864/225000 (87%)] Loss: 7169.970703\n",
      "Train Epoch: 69 [200960/225000 (89%)] Loss: 7035.626953\n",
      "Train Epoch: 69 [205056/225000 (91%)] Loss: 6983.978516\n",
      "Train Epoch: 69 [209152/225000 (93%)] Loss: 7184.281250\n",
      "Train Epoch: 69 [213248/225000 (95%)] Loss: 7161.910156\n",
      "Train Epoch: 69 [217344/225000 (97%)] Loss: 7172.574219\n",
      "Train Epoch: 69 [221440/225000 (98%)] Loss: 6973.529297\n",
      "    epoch          : 69\n",
      "    loss           : 7268.2953262763085\n",
      "    val_loss       : 7104.1442569044175\n",
      "Train Epoch: 70 [256/225000 (0%)] Loss: 7136.599609\n",
      "Train Epoch: 70 [4352/225000 (2%)] Loss: 30314.628906\n",
      "Train Epoch: 70 [8448/225000 (4%)] Loss: 7032.625000\n",
      "Train Epoch: 70 [12544/225000 (6%)] Loss: 7191.685547\n",
      "Train Epoch: 70 [16640/225000 (7%)] Loss: 7283.333984\n",
      "Train Epoch: 70 [20736/225000 (9%)] Loss: 7015.621094\n",
      "Train Epoch: 70 [24832/225000 (11%)] Loss: 7164.439453\n",
      "Train Epoch: 70 [28928/225000 (13%)] Loss: 7195.337891\n",
      "Train Epoch: 70 [33024/225000 (15%)] Loss: 7092.263672\n",
      "Train Epoch: 70 [37120/225000 (16%)] Loss: 7167.787109\n",
      "Train Epoch: 70 [41216/225000 (18%)] Loss: 7066.332031\n",
      "Train Epoch: 70 [45312/225000 (20%)] Loss: 7109.322266\n",
      "Train Epoch: 70 [49408/225000 (22%)] Loss: 7093.798828\n",
      "Train Epoch: 70 [53504/225000 (24%)] Loss: 7037.654297\n",
      "Train Epoch: 70 [57600/225000 (26%)] Loss: 7091.419922\n",
      "Train Epoch: 70 [61696/225000 (27%)] Loss: 7116.601562\n",
      "Train Epoch: 70 [65792/225000 (29%)] Loss: 7049.107422\n",
      "Train Epoch: 70 [69888/225000 (31%)] Loss: 7083.396484\n",
      "Train Epoch: 70 [73984/225000 (33%)] Loss: 7021.679688\n",
      "Train Epoch: 70 [78080/225000 (35%)] Loss: 7071.996094\n",
      "Train Epoch: 70 [82176/225000 (37%)] Loss: 7084.962891\n",
      "Train Epoch: 70 [86272/225000 (38%)] Loss: 7170.171875\n",
      "Train Epoch: 70 [90368/225000 (40%)] Loss: 7247.277344\n",
      "Train Epoch: 70 [94464/225000 (42%)] Loss: 6928.013672\n",
      "Train Epoch: 70 [98560/225000 (44%)] Loss: 7124.308594\n",
      "Train Epoch: 70 [102656/225000 (46%)] Loss: 7146.220703\n",
      "Train Epoch: 70 [106752/225000 (47%)] Loss: 7049.029297\n",
      "Train Epoch: 70 [110848/225000 (49%)] Loss: 7130.796875\n",
      "Train Epoch: 70 [114944/225000 (51%)] Loss: 7110.271484\n",
      "Train Epoch: 70 [119040/225000 (53%)] Loss: 7006.503906\n",
      "Train Epoch: 70 [123136/225000 (55%)] Loss: 7233.042969\n",
      "Train Epoch: 70 [127232/225000 (57%)] Loss: 7144.662109\n",
      "Train Epoch: 70 [131328/225000 (58%)] Loss: 7111.759766\n",
      "Train Epoch: 70 [135424/225000 (60%)] Loss: 7184.806641\n",
      "Train Epoch: 70 [139520/225000 (62%)] Loss: 6893.125000\n",
      "Train Epoch: 70 [143616/225000 (64%)] Loss: 7086.076172\n",
      "Train Epoch: 70 [147712/225000 (66%)] Loss: 7233.347656\n",
      "Train Epoch: 70 [151808/225000 (67%)] Loss: 6981.480469\n",
      "Train Epoch: 70 [155904/225000 (69%)] Loss: 6985.982422\n",
      "Train Epoch: 70 [160000/225000 (71%)] Loss: 6994.812500\n",
      "Train Epoch: 70 [164096/225000 (73%)] Loss: 7120.658203\n",
      "Train Epoch: 70 [168192/225000 (75%)] Loss: 7075.791016\n",
      "Train Epoch: 70 [172288/225000 (77%)] Loss: 7276.658203\n",
      "Train Epoch: 70 [176384/225000 (78%)] Loss: 7017.951172\n",
      "Train Epoch: 70 [180480/225000 (80%)] Loss: 7167.251953\n",
      "Train Epoch: 70 [184576/225000 (82%)] Loss: 7041.941406\n",
      "Train Epoch: 70 [188672/225000 (84%)] Loss: 7003.222656\n",
      "Train Epoch: 70 [192768/225000 (86%)] Loss: 7052.974609\n",
      "Train Epoch: 70 [196864/225000 (87%)] Loss: 7012.283203\n",
      "Train Epoch: 70 [200960/225000 (89%)] Loss: 7048.933594\n",
      "Train Epoch: 70 [205056/225000 (91%)] Loss: 6944.068359\n",
      "Train Epoch: 70 [209152/225000 (93%)] Loss: 7133.123047\n",
      "Train Epoch: 70 [213248/225000 (95%)] Loss: 6957.373047\n",
      "Train Epoch: 70 [217344/225000 (97%)] Loss: 6896.070312\n",
      "Train Epoch: 70 [221440/225000 (98%)] Loss: 7002.105469\n",
      "    epoch          : 70\n",
      "    loss           : 7261.14545670684\n",
      "    val_loss       : 7090.264039782845\n",
      "Train Epoch: 71 [256/225000 (0%)] Loss: 7029.466797\n",
      "Train Epoch: 71 [4352/225000 (2%)] Loss: 7011.515625\n",
      "Train Epoch: 71 [8448/225000 (4%)] Loss: 7078.593750\n",
      "Train Epoch: 71 [12544/225000 (6%)] Loss: 6942.164062\n",
      "Train Epoch: 71 [16640/225000 (7%)] Loss: 7088.146484\n",
      "Train Epoch: 71 [20736/225000 (9%)] Loss: 7024.812500\n",
      "Train Epoch: 71 [24832/225000 (11%)] Loss: 7166.910156\n",
      "Train Epoch: 71 [28928/225000 (13%)] Loss: 7238.138672\n",
      "Train Epoch: 71 [33024/225000 (15%)] Loss: 7039.605469\n",
      "Train Epoch: 71 [37120/225000 (16%)] Loss: 7070.066406\n",
      "Train Epoch: 71 [41216/225000 (18%)] Loss: 7150.015625\n",
      "Train Epoch: 71 [45312/225000 (20%)] Loss: 7116.689453\n",
      "Train Epoch: 71 [49408/225000 (22%)] Loss: 7129.414062\n",
      "Train Epoch: 71 [53504/225000 (24%)] Loss: 6976.531250\n",
      "Train Epoch: 71 [57600/225000 (26%)] Loss: 7206.660156\n",
      "Train Epoch: 71 [61696/225000 (27%)] Loss: 6982.908203\n",
      "Train Epoch: 71 [65792/225000 (29%)] Loss: 7042.798828\n",
      "Train Epoch: 71 [69888/225000 (31%)] Loss: 7051.832031\n",
      "Train Epoch: 71 [73984/225000 (33%)] Loss: 6965.292969\n",
      "Train Epoch: 71 [78080/225000 (35%)] Loss: 7045.353516\n",
      "Train Epoch: 71 [82176/225000 (37%)] Loss: 7122.439453\n",
      "Train Epoch: 71 [86272/225000 (38%)] Loss: 7035.710938\n",
      "Train Epoch: 71 [90368/225000 (40%)] Loss: 7084.527344\n",
      "Train Epoch: 71 [94464/225000 (42%)] Loss: 7146.816406\n",
      "Train Epoch: 71 [98560/225000 (44%)] Loss: 6993.744141\n",
      "Train Epoch: 71 [102656/225000 (46%)] Loss: 7082.458984\n",
      "Train Epoch: 71 [106752/225000 (47%)] Loss: 7108.785156\n",
      "Train Epoch: 71 [110848/225000 (49%)] Loss: 7063.199219\n",
      "Train Epoch: 71 [114944/225000 (51%)] Loss: 7051.841797\n",
      "Train Epoch: 71 [119040/225000 (53%)] Loss: 6910.191406\n",
      "Train Epoch: 71 [123136/225000 (55%)] Loss: 6981.789062\n",
      "Train Epoch: 71 [127232/225000 (57%)] Loss: 6914.205078\n",
      "Train Epoch: 71 [131328/225000 (58%)] Loss: 7125.712891\n",
      "Train Epoch: 71 [135424/225000 (60%)] Loss: 7041.605469\n",
      "Train Epoch: 71 [139520/225000 (62%)] Loss: 7076.966797\n",
      "Train Epoch: 71 [143616/225000 (64%)] Loss: 6999.388672\n",
      "Train Epoch: 71 [147712/225000 (66%)] Loss: 7142.824219\n",
      "Train Epoch: 71 [151808/225000 (67%)] Loss: 7045.466797\n",
      "Train Epoch: 71 [155904/225000 (69%)] Loss: 7055.591797\n",
      "Train Epoch: 71 [160000/225000 (71%)] Loss: 6953.628906\n",
      "Train Epoch: 71 [164096/225000 (73%)] Loss: 7096.861328\n",
      "Train Epoch: 71 [168192/225000 (75%)] Loss: 7059.822266\n",
      "Train Epoch: 71 [172288/225000 (77%)] Loss: 7164.390625\n",
      "Train Epoch: 71 [176384/225000 (78%)] Loss: 7185.007812\n",
      "Train Epoch: 71 [180480/225000 (80%)] Loss: 7021.621094\n",
      "Train Epoch: 71 [184576/225000 (82%)] Loss: 7259.265625\n",
      "Train Epoch: 71 [188672/225000 (84%)] Loss: 7073.619141\n",
      "Train Epoch: 71 [192768/225000 (86%)] Loss: 7077.599609\n",
      "Train Epoch: 71 [196864/225000 (87%)] Loss: 7170.511719\n",
      "Train Epoch: 71 [200960/225000 (89%)] Loss: 7174.263672\n",
      "Train Epoch: 71 [205056/225000 (91%)] Loss: 7077.140625\n",
      "Train Epoch: 71 [209152/225000 (93%)] Loss: 6951.095703\n",
      "Train Epoch: 71 [213248/225000 (95%)] Loss: 7098.228516\n",
      "Train Epoch: 71 [217344/225000 (97%)] Loss: 7182.990234\n",
      "Train Epoch: 71 [221440/225000 (98%)] Loss: 7079.171875\n",
      "    epoch          : 71\n",
      "    loss           : 7186.089674879124\n",
      "    val_loss       : 7090.945526256854\n",
      "Train Epoch: 72 [256/225000 (0%)] Loss: 7058.085938\n",
      "Train Epoch: 72 [4352/225000 (2%)] Loss: 7034.158203\n",
      "Train Epoch: 72 [8448/225000 (4%)] Loss: 6983.214844\n",
      "Train Epoch: 72 [12544/225000 (6%)] Loss: 7056.648438\n",
      "Train Epoch: 72 [16640/225000 (7%)] Loss: 7111.488281\n",
      "Train Epoch: 72 [20736/225000 (9%)] Loss: 7037.062500\n",
      "Train Epoch: 72 [24832/225000 (11%)] Loss: 6885.677734\n",
      "Train Epoch: 72 [28928/225000 (13%)] Loss: 7073.380859\n",
      "Train Epoch: 72 [33024/225000 (15%)] Loss: 7063.654297\n",
      "Train Epoch: 72 [37120/225000 (16%)] Loss: 6954.673828\n",
      "Train Epoch: 72 [41216/225000 (18%)] Loss: 7014.064453\n",
      "Train Epoch: 72 [45312/225000 (20%)] Loss: 7334.871094\n",
      "Train Epoch: 72 [49408/225000 (22%)] Loss: 7078.867188\n",
      "Train Epoch: 72 [53504/225000 (24%)] Loss: 6970.669922\n",
      "Train Epoch: 72 [57600/225000 (26%)] Loss: 7024.853516\n",
      "Train Epoch: 72 [61696/225000 (27%)] Loss: 7106.027344\n",
      "Train Epoch: 72 [65792/225000 (29%)] Loss: 7132.021484\n",
      "Train Epoch: 72 [69888/225000 (31%)] Loss: 7028.371094\n",
      "Train Epoch: 72 [73984/225000 (33%)] Loss: 7090.964844\n",
      "Train Epoch: 72 [78080/225000 (35%)] Loss: 7126.042969\n",
      "Train Epoch: 72 [82176/225000 (37%)] Loss: 7231.119141\n",
      "Train Epoch: 72 [86272/225000 (38%)] Loss: 7132.121094\n",
      "Train Epoch: 72 [90368/225000 (40%)] Loss: 7097.011719\n",
      "Train Epoch: 72 [94464/225000 (42%)] Loss: 6989.869141\n",
      "Train Epoch: 72 [98560/225000 (44%)] Loss: 7023.945312\n",
      "Train Epoch: 72 [102656/225000 (46%)] Loss: 7020.541016\n",
      "Train Epoch: 72 [106752/225000 (47%)] Loss: 7071.615234\n",
      "Train Epoch: 72 [110848/225000 (49%)] Loss: 7030.357422\n",
      "Train Epoch: 72 [114944/225000 (51%)] Loss: 7061.474609\n",
      "Train Epoch: 72 [119040/225000 (53%)] Loss: 6955.222656\n",
      "Train Epoch: 72 [123136/225000 (55%)] Loss: 6938.585938\n",
      "Train Epoch: 72 [127232/225000 (57%)] Loss: 7092.662109\n",
      "Train Epoch: 72 [131328/225000 (58%)] Loss: 7092.183594\n",
      "Train Epoch: 72 [135424/225000 (60%)] Loss: 7102.574219\n",
      "Train Epoch: 72 [139520/225000 (62%)] Loss: 7095.208984\n",
      "Train Epoch: 72 [143616/225000 (64%)] Loss: 7149.998047\n",
      "Train Epoch: 72 [147712/225000 (66%)] Loss: 7253.878906\n",
      "Train Epoch: 72 [151808/225000 (67%)] Loss: 7027.578125\n",
      "Train Epoch: 72 [155904/225000 (69%)] Loss: 7125.015625\n",
      "Train Epoch: 72 [160000/225000 (71%)] Loss: 7119.892578\n",
      "Train Epoch: 72 [164096/225000 (73%)] Loss: 7061.960938\n",
      "Train Epoch: 72 [168192/225000 (75%)] Loss: 7079.462891\n",
      "Train Epoch: 72 [172288/225000 (77%)] Loss: 6844.884766\n",
      "Train Epoch: 72 [176384/225000 (78%)] Loss: 7222.013672\n",
      "Train Epoch: 72 [180480/225000 (80%)] Loss: 7036.642578\n",
      "Train Epoch: 72 [184576/225000 (82%)] Loss: 7010.531250\n",
      "Train Epoch: 72 [188672/225000 (84%)] Loss: 7266.386719\n",
      "Train Epoch: 72 [192768/225000 (86%)] Loss: 7173.169922\n",
      "Train Epoch: 72 [196864/225000 (87%)] Loss: 6960.474609\n",
      "Train Epoch: 72 [200960/225000 (89%)] Loss: 6985.945312\n",
      "Train Epoch: 72 [205056/225000 (91%)] Loss: 7120.050781\n",
      "Train Epoch: 72 [209152/225000 (93%)] Loss: 7000.400391\n",
      "Train Epoch: 72 [213248/225000 (95%)] Loss: 7006.042969\n",
      "Train Epoch: 72 [217344/225000 (97%)] Loss: 7053.267578\n",
      "Train Epoch: 72 [221440/225000 (98%)] Loss: 7073.662109\n",
      "    epoch          : 72\n",
      "    loss           : 7076.164135825512\n",
      "    val_loss       : 7338.09375824612\n",
      "Train Epoch: 73 [256/225000 (0%)] Loss: 7055.683594\n",
      "Train Epoch: 73 [4352/225000 (2%)] Loss: 7058.253906\n",
      "Train Epoch: 73 [8448/225000 (4%)] Loss: 7176.984375\n",
      "Train Epoch: 73 [12544/225000 (6%)] Loss: 6989.464844\n",
      "Train Epoch: 73 [16640/225000 (7%)] Loss: 7009.726562\n",
      "Train Epoch: 73 [20736/225000 (9%)] Loss: 7084.822266\n",
      "Train Epoch: 73 [24832/225000 (11%)] Loss: 6928.113281\n",
      "Train Epoch: 73 [28928/225000 (13%)] Loss: 7048.740234\n",
      "Train Epoch: 73 [33024/225000 (15%)] Loss: 7219.787109\n",
      "Train Epoch: 73 [37120/225000 (16%)] Loss: 7125.164062\n",
      "Train Epoch: 73 [41216/225000 (18%)] Loss: 6996.685547\n",
      "Train Epoch: 73 [45312/225000 (20%)] Loss: 7160.197266\n",
      "Train Epoch: 73 [49408/225000 (22%)] Loss: 6972.156250\n",
      "Train Epoch: 73 [53504/225000 (24%)] Loss: 6977.923828\n",
      "Train Epoch: 73 [57600/225000 (26%)] Loss: 7152.212891\n",
      "Train Epoch: 73 [61696/225000 (27%)] Loss: 7007.871094\n",
      "Train Epoch: 73 [65792/225000 (29%)] Loss: 6972.847656\n",
      "Train Epoch: 73 [69888/225000 (31%)] Loss: 7055.539062\n",
      "Train Epoch: 73 [73984/225000 (33%)] Loss: 7015.914062\n",
      "Train Epoch: 73 [78080/225000 (35%)] Loss: 6962.431641\n",
      "Train Epoch: 73 [82176/225000 (37%)] Loss: 7043.615234\n",
      "Train Epoch: 73 [86272/225000 (38%)] Loss: 6960.130859\n",
      "Train Epoch: 73 [90368/225000 (40%)] Loss: 6966.208984\n",
      "Train Epoch: 73 [94464/225000 (42%)] Loss: 7001.531250\n",
      "Train Epoch: 73 [98560/225000 (44%)] Loss: 7131.134766\n",
      "Train Epoch: 73 [102656/225000 (46%)] Loss: 6929.076172\n",
      "Train Epoch: 73 [106752/225000 (47%)] Loss: 7063.708984\n",
      "Train Epoch: 73 [110848/225000 (49%)] Loss: 7073.773438\n",
      "Train Epoch: 73 [114944/225000 (51%)] Loss: 7087.992188\n",
      "Train Epoch: 73 [119040/225000 (53%)] Loss: 8728.783203\n",
      "Train Epoch: 73 [123136/225000 (55%)] Loss: 6835.851562\n",
      "Train Epoch: 73 [127232/225000 (57%)] Loss: 6886.234375\n",
      "Train Epoch: 73 [131328/225000 (58%)] Loss: 6998.292969\n",
      "Train Epoch: 73 [135424/225000 (60%)] Loss: 6974.808594\n",
      "Train Epoch: 73 [139520/225000 (62%)] Loss: 7052.705078\n",
      "Train Epoch: 73 [143616/225000 (64%)] Loss: 7057.308594\n",
      "Train Epoch: 73 [147712/225000 (66%)] Loss: 7131.134766\n",
      "Train Epoch: 73 [151808/225000 (67%)] Loss: 7053.474609\n",
      "Train Epoch: 73 [155904/225000 (69%)] Loss: 7087.775391\n",
      "Train Epoch: 73 [160000/225000 (71%)] Loss: 6953.417969\n",
      "Train Epoch: 73 [164096/225000 (73%)] Loss: 7004.328125\n",
      "Train Epoch: 73 [168192/225000 (75%)] Loss: 6975.896484\n",
      "Train Epoch: 73 [172288/225000 (77%)] Loss: 7150.107422\n",
      "Train Epoch: 73 [176384/225000 (78%)] Loss: 7174.746094\n",
      "Train Epoch: 73 [180480/225000 (80%)] Loss: 6908.810547\n",
      "Train Epoch: 73 [184576/225000 (82%)] Loss: 7028.480469\n",
      "Train Epoch: 73 [188672/225000 (84%)] Loss: 7254.792969\n",
      "Train Epoch: 73 [192768/225000 (86%)] Loss: 6983.253906\n",
      "Train Epoch: 73 [196864/225000 (87%)] Loss: 7019.716797\n",
      "Train Epoch: 73 [200960/225000 (89%)] Loss: 7014.919922\n",
      "Train Epoch: 73 [205056/225000 (91%)] Loss: 7221.429688\n",
      "Train Epoch: 73 [209152/225000 (93%)] Loss: 7246.316406\n",
      "Train Epoch: 73 [213248/225000 (95%)] Loss: 7035.345703\n",
      "Train Epoch: 73 [217344/225000 (97%)] Loss: 8850.710938\n",
      "Train Epoch: 73 [221440/225000 (98%)] Loss: 7008.615234\n",
      "    epoch          : 73\n",
      "    loss           : 7185.576316304038\n",
      "    val_loss       : 7083.476734392497\n",
      "Train Epoch: 74 [256/225000 (0%)] Loss: 7092.195312\n",
      "Train Epoch: 74 [4352/225000 (2%)] Loss: 7003.798828\n",
      "Train Epoch: 74 [8448/225000 (4%)] Loss: 6990.441406\n",
      "Train Epoch: 74 [12544/225000 (6%)] Loss: 7038.216797\n",
      "Train Epoch: 74 [16640/225000 (7%)] Loss: 6974.179688\n",
      "Train Epoch: 74 [20736/225000 (9%)] Loss: 6969.244141\n",
      "Train Epoch: 74 [24832/225000 (11%)] Loss: 6983.433594\n",
      "Train Epoch: 74 [28928/225000 (13%)] Loss: 7035.261719\n",
      "Train Epoch: 74 [33024/225000 (15%)] Loss: 7028.710938\n",
      "Train Epoch: 74 [37120/225000 (16%)] Loss: 6880.091797\n",
      "Train Epoch: 74 [41216/225000 (18%)] Loss: 7167.673828\n",
      "Train Epoch: 74 [45312/225000 (20%)] Loss: 7049.408203\n",
      "Train Epoch: 74 [49408/225000 (22%)] Loss: 7043.800781\n",
      "Train Epoch: 74 [53504/225000 (24%)] Loss: 6901.722656\n",
      "Train Epoch: 74 [57600/225000 (26%)] Loss: 7096.660156\n",
      "Train Epoch: 74 [61696/225000 (27%)] Loss: 7146.849609\n",
      "Train Epoch: 74 [65792/225000 (29%)] Loss: 7147.289062\n",
      "Train Epoch: 74 [69888/225000 (31%)] Loss: 7130.685547\n",
      "Train Epoch: 74 [73984/225000 (33%)] Loss: 6945.531250\n",
      "Train Epoch: 74 [78080/225000 (35%)] Loss: 7000.365234\n",
      "Train Epoch: 74 [82176/225000 (37%)] Loss: 6942.058594\n",
      "Train Epoch: 74 [86272/225000 (38%)] Loss: 6951.787109\n",
      "Train Epoch: 74 [90368/225000 (40%)] Loss: 7323.345703\n",
      "Train Epoch: 74 [94464/225000 (42%)] Loss: 7066.453125\n",
      "Train Epoch: 74 [98560/225000 (44%)] Loss: 7052.164062\n",
      "Train Epoch: 74 [102656/225000 (46%)] Loss: 6991.080078\n",
      "Train Epoch: 74 [106752/225000 (47%)] Loss: 7193.728516\n",
      "Train Epoch: 74 [110848/225000 (49%)] Loss: 7095.425781\n",
      "Train Epoch: 74 [114944/225000 (51%)] Loss: 7117.218750\n",
      "Train Epoch: 74 [119040/225000 (53%)] Loss: 7037.496094\n",
      "Train Epoch: 74 [123136/225000 (55%)] Loss: 7056.314453\n",
      "Train Epoch: 74 [127232/225000 (57%)] Loss: 7108.886719\n",
      "Train Epoch: 74 [131328/225000 (58%)] Loss: 7138.027344\n",
      "Train Epoch: 74 [135424/225000 (60%)] Loss: 7095.005859\n",
      "Train Epoch: 74 [139520/225000 (62%)] Loss: 7041.882812\n",
      "Train Epoch: 74 [143616/225000 (64%)] Loss: 6944.742188\n",
      "Train Epoch: 74 [147712/225000 (66%)] Loss: 7005.250000\n",
      "Train Epoch: 74 [151808/225000 (67%)] Loss: 7118.324219\n",
      "Train Epoch: 74 [155904/225000 (69%)] Loss: 6980.892578\n",
      "Train Epoch: 74 [160000/225000 (71%)] Loss: 7089.548828\n",
      "Train Epoch: 74 [164096/225000 (73%)] Loss: 7001.972656\n",
      "Train Epoch: 74 [168192/225000 (75%)] Loss: 6953.455078\n",
      "Train Epoch: 74 [172288/225000 (77%)] Loss: 7018.591797\n",
      "Train Epoch: 74 [176384/225000 (78%)] Loss: 7083.017578\n",
      "Train Epoch: 74 [180480/225000 (80%)] Loss: 7040.605469\n",
      "Train Epoch: 74 [184576/225000 (82%)] Loss: 7005.205078\n",
      "Train Epoch: 74 [188672/225000 (84%)] Loss: 7008.583984\n",
      "Train Epoch: 74 [192768/225000 (86%)] Loss: 7127.884766\n",
      "Train Epoch: 74 [196864/225000 (87%)] Loss: 6982.316406\n",
      "Train Epoch: 74 [200960/225000 (89%)] Loss: 7180.152344\n",
      "Train Epoch: 74 [205056/225000 (91%)] Loss: 7006.943359\n",
      "Train Epoch: 74 [209152/225000 (93%)] Loss: 7069.728516\n",
      "Train Epoch: 74 [213248/225000 (95%)] Loss: 7039.130859\n",
      "Train Epoch: 74 [217344/225000 (97%)] Loss: 7144.607422\n",
      "Train Epoch: 74 [221440/225000 (98%)] Loss: 6907.541016\n",
      "    epoch          : 74\n",
      "    loss           : 7165.320504701721\n",
      "    val_loss       : 7032.571557209199\n",
      "Train Epoch: 75 [256/225000 (0%)] Loss: 6893.091797\n",
      "Train Epoch: 75 [4352/225000 (2%)] Loss: 7111.101562\n",
      "Train Epoch: 75 [8448/225000 (4%)] Loss: 7009.855469\n",
      "Train Epoch: 75 [12544/225000 (6%)] Loss: 7127.792969\n",
      "Train Epoch: 75 [16640/225000 (7%)] Loss: 6986.656250\n",
      "Train Epoch: 75 [20736/225000 (9%)] Loss: 6882.121094\n",
      "Train Epoch: 75 [24832/225000 (11%)] Loss: 6939.335938\n",
      "Train Epoch: 75 [28928/225000 (13%)] Loss: 7055.644531\n",
      "Train Epoch: 75 [33024/225000 (15%)] Loss: 6862.541016\n",
      "Train Epoch: 75 [37120/225000 (16%)] Loss: 6990.445312\n",
      "Train Epoch: 75 [41216/225000 (18%)] Loss: 6898.425781\n",
      "Train Epoch: 75 [45312/225000 (20%)] Loss: 6961.972656\n",
      "Train Epoch: 75 [49408/225000 (22%)] Loss: 6965.318359\n",
      "Train Epoch: 75 [53504/225000 (24%)] Loss: 6877.333984\n",
      "Train Epoch: 75 [57600/225000 (26%)] Loss: 7170.029297\n",
      "Train Epoch: 75 [61696/225000 (27%)] Loss: 6978.201172\n",
      "Train Epoch: 75 [65792/225000 (29%)] Loss: 7038.619141\n",
      "Train Epoch: 75 [69888/225000 (31%)] Loss: 7098.718750\n",
      "Train Epoch: 75 [73984/225000 (33%)] Loss: 7016.048828\n",
      "Train Epoch: 75 [78080/225000 (35%)] Loss: 6983.787109\n",
      "Train Epoch: 75 [82176/225000 (37%)] Loss: 6982.388672\n",
      "Train Epoch: 75 [86272/225000 (38%)] Loss: 7208.626953\n",
      "Train Epoch: 75 [90368/225000 (40%)] Loss: 7007.080078\n",
      "Train Epoch: 75 [94464/225000 (42%)] Loss: 6994.972656\n",
      "Train Epoch: 75 [98560/225000 (44%)] Loss: 7047.814453\n",
      "Train Epoch: 75 [102656/225000 (46%)] Loss: 7030.390625\n",
      "Train Epoch: 75 [106752/225000 (47%)] Loss: 7034.380859\n",
      "Train Epoch: 75 [110848/225000 (49%)] Loss: 6969.767578\n",
      "Train Epoch: 75 [114944/225000 (51%)] Loss: 7036.865234\n",
      "Train Epoch: 75 [119040/225000 (53%)] Loss: 6774.621094\n",
      "Train Epoch: 75 [123136/225000 (55%)] Loss: 6947.585938\n",
      "Train Epoch: 75 [127232/225000 (57%)] Loss: 7059.277344\n",
      "Train Epoch: 75 [131328/225000 (58%)] Loss: 7212.986328\n",
      "Train Epoch: 75 [135424/225000 (60%)] Loss: 6933.521484\n",
      "Train Epoch: 75 [139520/225000 (62%)] Loss: 6962.310547\n",
      "Train Epoch: 75 [143616/225000 (64%)] Loss: 7132.203125\n",
      "Train Epoch: 75 [147712/225000 (66%)] Loss: 6946.617188\n",
      "Train Epoch: 75 [151808/225000 (67%)] Loss: 6925.955078\n",
      "Train Epoch: 75 [155904/225000 (69%)] Loss: 7061.226562\n",
      "Train Epoch: 75 [160000/225000 (71%)] Loss: 7229.830078\n",
      "Train Epoch: 75 [164096/225000 (73%)] Loss: 7119.318359\n",
      "Train Epoch: 75 [168192/225000 (75%)] Loss: 7081.925781\n",
      "Train Epoch: 75 [172288/225000 (77%)] Loss: 7061.203125\n",
      "Train Epoch: 75 [176384/225000 (78%)] Loss: 8896.679688\n",
      "Train Epoch: 75 [180480/225000 (80%)] Loss: 7014.679688\n",
      "Train Epoch: 75 [184576/225000 (82%)] Loss: 7059.382812\n",
      "Train Epoch: 75 [188672/225000 (84%)] Loss: 7048.271484\n",
      "Train Epoch: 75 [192768/225000 (86%)] Loss: 6976.212891\n",
      "Train Epoch: 75 [196864/225000 (87%)] Loss: 7124.019531\n",
      "Train Epoch: 75 [200960/225000 (89%)] Loss: 7010.318359\n",
      "Train Epoch: 75 [205056/225000 (91%)] Loss: 6995.371094\n",
      "Train Epoch: 75 [209152/225000 (93%)] Loss: 7107.775391\n",
      "Train Epoch: 75 [213248/225000 (95%)] Loss: 7187.027344\n",
      "Train Epoch: 75 [217344/225000 (97%)] Loss: 7043.300781\n",
      "Train Epoch: 75 [221440/225000 (98%)] Loss: 7099.816406\n",
      "    epoch          : 75\n",
      "    loss           : 7138.406924372512\n",
      "    val_loss       : 7043.664355041421\n",
      "Train Epoch: 76 [256/225000 (0%)] Loss: 6872.693359\n",
      "Train Epoch: 76 [4352/225000 (2%)] Loss: 6962.875000\n",
      "Train Epoch: 76 [8448/225000 (4%)] Loss: 7034.707031\n",
      "Train Epoch: 76 [12544/225000 (6%)] Loss: 7087.878906\n",
      "Train Epoch: 76 [16640/225000 (7%)] Loss: 6967.179688\n",
      "Train Epoch: 76 [20736/225000 (9%)] Loss: 7048.433594\n",
      "Train Epoch: 76 [24832/225000 (11%)] Loss: 7046.501953\n",
      "Train Epoch: 76 [28928/225000 (13%)] Loss: 7066.716797\n",
      "Train Epoch: 76 [33024/225000 (15%)] Loss: 6971.552734\n",
      "Train Epoch: 76 [37120/225000 (16%)] Loss: 7021.744141\n",
      "Train Epoch: 76 [41216/225000 (18%)] Loss: 7124.412109\n",
      "Train Epoch: 76 [45312/225000 (20%)] Loss: 6872.181641\n",
      "Train Epoch: 76 [49408/225000 (22%)] Loss: 7053.281250\n",
      "Train Epoch: 76 [53504/225000 (24%)] Loss: 6958.386719\n",
      "Train Epoch: 76 [57600/225000 (26%)] Loss: 6947.953125\n",
      "Train Epoch: 76 [61696/225000 (27%)] Loss: 7106.982422\n",
      "Train Epoch: 76 [65792/225000 (29%)] Loss: 7104.886719\n",
      "Train Epoch: 76 [69888/225000 (31%)] Loss: 6965.269531\n",
      "Train Epoch: 76 [73984/225000 (33%)] Loss: 7023.421875\n",
      "Train Epoch: 76 [78080/225000 (35%)] Loss: 7037.214844\n",
      "Train Epoch: 76 [82176/225000 (37%)] Loss: 7063.839844\n",
      "Train Epoch: 76 [86272/225000 (38%)] Loss: 7053.949219\n",
      "Train Epoch: 76 [90368/225000 (40%)] Loss: 7030.966797\n",
      "Train Epoch: 76 [94464/225000 (42%)] Loss: 7019.703125\n",
      "Train Epoch: 76 [98560/225000 (44%)] Loss: 7056.902344\n",
      "Train Epoch: 76 [102656/225000 (46%)] Loss: 6952.019531\n",
      "Train Epoch: 76 [106752/225000 (47%)] Loss: 7021.193359\n",
      "Train Epoch: 76 [110848/225000 (49%)] Loss: 7066.113281\n",
      "Train Epoch: 76 [114944/225000 (51%)] Loss: 6995.664062\n",
      "Train Epoch: 76 [119040/225000 (53%)] Loss: 6956.324219\n",
      "Train Epoch: 76 [123136/225000 (55%)] Loss: 7066.666016\n",
      "Train Epoch: 76 [127232/225000 (57%)] Loss: 7062.097656\n",
      "Train Epoch: 76 [131328/225000 (58%)] Loss: 6982.875000\n",
      "Train Epoch: 76 [135424/225000 (60%)] Loss: 7031.693359\n",
      "Train Epoch: 76 [139520/225000 (62%)] Loss: 7001.841797\n",
      "Train Epoch: 76 [143616/225000 (64%)] Loss: 6975.304688\n",
      "Train Epoch: 76 [147712/225000 (66%)] Loss: 7000.152344\n",
      "Train Epoch: 76 [151808/225000 (67%)] Loss: 6903.984375\n",
      "Train Epoch: 76 [155904/225000 (69%)] Loss: 7177.695312\n",
      "Train Epoch: 76 [160000/225000 (71%)] Loss: 7014.501953\n",
      "Train Epoch: 76 [164096/225000 (73%)] Loss: 6979.927734\n",
      "Train Epoch: 76 [168192/225000 (75%)] Loss: 7087.751953\n",
      "Train Epoch: 76 [172288/225000 (77%)] Loss: 6936.783203\n",
      "Train Epoch: 76 [176384/225000 (78%)] Loss: 6944.445312\n",
      "Train Epoch: 76 [180480/225000 (80%)] Loss: 7184.832031\n",
      "Train Epoch: 76 [184576/225000 (82%)] Loss: 6896.509766\n",
      "Train Epoch: 76 [188672/225000 (84%)] Loss: 6996.085938\n",
      "Train Epoch: 76 [192768/225000 (86%)] Loss: 6883.044922\n",
      "Train Epoch: 76 [196864/225000 (87%)] Loss: 6958.910156\n",
      "Train Epoch: 76 [200960/225000 (89%)] Loss: 7039.371094\n",
      "Train Epoch: 76 [205056/225000 (91%)] Loss: 7161.326172\n",
      "Train Epoch: 76 [209152/225000 (93%)] Loss: 7081.222656\n",
      "Train Epoch: 76 [213248/225000 (95%)] Loss: 6946.470703\n",
      "Train Epoch: 76 [217344/225000 (97%)] Loss: 7156.740234\n",
      "Train Epoch: 76 [221440/225000 (98%)] Loss: 7020.375000\n",
      "    epoch          : 76\n",
      "    loss           : 7093.231194228171\n",
      "    val_loss       : 7010.1603140283605\n",
      "Train Epoch: 77 [256/225000 (0%)] Loss: 7043.261719\n",
      "Train Epoch: 77 [4352/225000 (2%)] Loss: 6977.146484\n",
      "Train Epoch: 77 [8448/225000 (4%)] Loss: 7001.746094\n",
      "Train Epoch: 77 [12544/225000 (6%)] Loss: 6983.658203\n",
      "Train Epoch: 77 [16640/225000 (7%)] Loss: 7032.763672\n",
      "Train Epoch: 77 [20736/225000 (9%)] Loss: 7149.472656\n",
      "Train Epoch: 77 [24832/225000 (11%)] Loss: 7068.626953\n",
      "Train Epoch: 77 [28928/225000 (13%)] Loss: 6973.937500\n",
      "Train Epoch: 77 [33024/225000 (15%)] Loss: 7091.027344\n",
      "Train Epoch: 77 [37120/225000 (16%)] Loss: 6963.697266\n",
      "Train Epoch: 77 [41216/225000 (18%)] Loss: 6978.990234\n",
      "Train Epoch: 77 [45312/225000 (20%)] Loss: 7038.101562\n",
      "Train Epoch: 77 [49408/225000 (22%)] Loss: 6856.259766\n",
      "Train Epoch: 77 [53504/225000 (24%)] Loss: 6957.939453\n",
      "Train Epoch: 77 [57600/225000 (26%)] Loss: 6998.117188\n",
      "Train Epoch: 77 [61696/225000 (27%)] Loss: 6947.572266\n",
      "Train Epoch: 77 [65792/225000 (29%)] Loss: 7044.908203\n",
      "Train Epoch: 77 [69888/225000 (31%)] Loss: 7115.824219\n",
      "Train Epoch: 77 [73984/225000 (33%)] Loss: 6832.242188\n",
      "Train Epoch: 77 [78080/225000 (35%)] Loss: 7019.798828\n",
      "Train Epoch: 77 [82176/225000 (37%)] Loss: 7010.855469\n",
      "Train Epoch: 77 [86272/225000 (38%)] Loss: 7158.123047\n",
      "Train Epoch: 77 [90368/225000 (40%)] Loss: 6992.048828\n",
      "Train Epoch: 77 [94464/225000 (42%)] Loss: 7010.371094\n",
      "Train Epoch: 77 [98560/225000 (44%)] Loss: 7127.193359\n",
      "Train Epoch: 77 [102656/225000 (46%)] Loss: 6952.474609\n",
      "Train Epoch: 77 [106752/225000 (47%)] Loss: 6964.496094\n",
      "Train Epoch: 77 [110848/225000 (49%)] Loss: 6936.220703\n",
      "Train Epoch: 77 [114944/225000 (51%)] Loss: 6829.691406\n",
      "Train Epoch: 77 [119040/225000 (53%)] Loss: 7004.601562\n",
      "Train Epoch: 77 [123136/225000 (55%)] Loss: 6862.378906\n",
      "Train Epoch: 77 [127232/225000 (57%)] Loss: 6996.898438\n",
      "Train Epoch: 77 [131328/225000 (58%)] Loss: 7032.115234\n",
      "Train Epoch: 77 [135424/225000 (60%)] Loss: 6912.308594\n",
      "Train Epoch: 77 [139520/225000 (62%)] Loss: 7117.945312\n",
      "Train Epoch: 77 [143616/225000 (64%)] Loss: 7166.205078\n",
      "Train Epoch: 77 [147712/225000 (66%)] Loss: 6982.808594\n",
      "Train Epoch: 77 [151808/225000 (67%)] Loss: 7105.771484\n",
      "Train Epoch: 77 [155904/225000 (69%)] Loss: 8957.470703\n",
      "Train Epoch: 77 [160000/225000 (71%)] Loss: 7055.685547\n",
      "Train Epoch: 77 [164096/225000 (73%)] Loss: 6967.806641\n",
      "Train Epoch: 77 [168192/225000 (75%)] Loss: 6927.701172\n",
      "Train Epoch: 77 [172288/225000 (77%)] Loss: 6898.847656\n",
      "Train Epoch: 77 [176384/225000 (78%)] Loss: 7319.023438\n",
      "Train Epoch: 77 [180480/225000 (80%)] Loss: 6854.027344\n",
      "Train Epoch: 77 [184576/225000 (82%)] Loss: 6879.121094\n",
      "Train Epoch: 77 [188672/225000 (84%)] Loss: 6899.964844\n",
      "Train Epoch: 77 [192768/225000 (86%)] Loss: 7062.701172\n",
      "Train Epoch: 77 [196864/225000 (87%)] Loss: 6916.451172\n",
      "Train Epoch: 77 [200960/225000 (89%)] Loss: 6985.544922\n",
      "Train Epoch: 77 [205056/225000 (91%)] Loss: 7036.808594\n",
      "Train Epoch: 77 [209152/225000 (93%)] Loss: 6920.962891\n",
      "Train Epoch: 77 [213248/225000 (95%)] Loss: 6955.855469\n",
      "Train Epoch: 77 [217344/225000 (97%)] Loss: 6945.552734\n",
      "Train Epoch: 77 [221440/225000 (98%)] Loss: 7243.267578\n",
      "    epoch          : 77\n",
      "    loss           : 7081.354459968715\n",
      "    val_loss       : 7264.029079211609\n",
      "Train Epoch: 78 [256/225000 (0%)] Loss: 7180.787109\n",
      "Train Epoch: 78 [4352/225000 (2%)] Loss: 6912.802734\n",
      "Train Epoch: 78 [8448/225000 (4%)] Loss: 6975.425781\n",
      "Train Epoch: 78 [12544/225000 (6%)] Loss: 7057.037109\n",
      "Train Epoch: 78 [16640/225000 (7%)] Loss: 7127.296875\n",
      "Train Epoch: 78 [20736/225000 (9%)] Loss: 6979.753906\n",
      "Train Epoch: 78 [24832/225000 (11%)] Loss: 6980.728516\n",
      "Train Epoch: 78 [28928/225000 (13%)] Loss: 6985.015625\n",
      "Train Epoch: 78 [33024/225000 (15%)] Loss: 7030.107422\n",
      "Train Epoch: 78 [37120/225000 (16%)] Loss: 7253.621094\n",
      "Train Epoch: 78 [41216/225000 (18%)] Loss: 6933.667969\n",
      "Train Epoch: 78 [45312/225000 (20%)] Loss: 6984.859375\n",
      "Train Epoch: 78 [49408/225000 (22%)] Loss: 7038.806641\n",
      "Train Epoch: 78 [53504/225000 (24%)] Loss: 7081.113281\n",
      "Train Epoch: 78 [57600/225000 (26%)] Loss: 7021.556641\n",
      "Train Epoch: 78 [61696/225000 (27%)] Loss: 6986.964844\n",
      "Train Epoch: 78 [65792/225000 (29%)] Loss: 6888.548828\n",
      "Train Epoch: 78 [69888/225000 (31%)] Loss: 7073.234375\n",
      "Train Epoch: 78 [73984/225000 (33%)] Loss: 6959.957031\n",
      "Train Epoch: 78 [78080/225000 (35%)] Loss: 6779.919922\n",
      "Train Epoch: 78 [82176/225000 (37%)] Loss: 6903.113281\n",
      "Train Epoch: 78 [86272/225000 (38%)] Loss: 6991.160156\n",
      "Train Epoch: 78 [90368/225000 (40%)] Loss: 7088.998047\n",
      "Train Epoch: 78 [94464/225000 (42%)] Loss: 7005.544922\n",
      "Train Epoch: 78 [98560/225000 (44%)] Loss: 7071.806641\n",
      "Train Epoch: 78 [102656/225000 (46%)] Loss: 6877.177734\n",
      "Train Epoch: 78 [106752/225000 (47%)] Loss: 7083.382812\n",
      "Train Epoch: 78 [110848/225000 (49%)] Loss: 6979.441406\n",
      "Train Epoch: 78 [114944/225000 (51%)] Loss: 7121.382812\n",
      "Train Epoch: 78 [119040/225000 (53%)] Loss: 6964.576172\n",
      "Train Epoch: 78 [123136/225000 (55%)] Loss: 6956.285156\n",
      "Train Epoch: 78 [127232/225000 (57%)] Loss: 6854.683594\n",
      "Train Epoch: 78 [131328/225000 (58%)] Loss: 7174.498047\n",
      "Train Epoch: 78 [135424/225000 (60%)] Loss: 6915.777344\n",
      "Train Epoch: 78 [139520/225000 (62%)] Loss: 6960.664062\n",
      "Train Epoch: 78 [143616/225000 (64%)] Loss: 6985.892578\n",
      "Train Epoch: 78 [147712/225000 (66%)] Loss: 6879.724609\n",
      "Train Epoch: 78 [151808/225000 (67%)] Loss: 6982.878906\n",
      "Train Epoch: 78 [155904/225000 (69%)] Loss: 7020.810547\n",
      "Train Epoch: 78 [160000/225000 (71%)] Loss: 7111.669922\n",
      "Train Epoch: 78 [164096/225000 (73%)] Loss: 7046.767578\n",
      "Train Epoch: 78 [168192/225000 (75%)] Loss: 6960.470703\n",
      "Train Epoch: 78 [172288/225000 (77%)] Loss: 6892.273438\n",
      "Train Epoch: 78 [176384/225000 (78%)] Loss: 6974.947266\n",
      "Train Epoch: 78 [180480/225000 (80%)] Loss: 6958.792969\n",
      "Train Epoch: 78 [184576/225000 (82%)] Loss: 7004.966797\n",
      "Train Epoch: 78 [188672/225000 (84%)] Loss: 6905.869141\n",
      "Train Epoch: 78 [192768/225000 (86%)] Loss: 6990.837891\n",
      "Train Epoch: 78 [196864/225000 (87%)] Loss: 6913.453125\n",
      "Train Epoch: 78 [200960/225000 (89%)] Loss: 6988.314453\n",
      "Train Epoch: 78 [205056/225000 (91%)] Loss: 6864.955078\n",
      "Train Epoch: 78 [209152/225000 (93%)] Loss: 7030.458984\n",
      "Train Epoch: 78 [213248/225000 (95%)] Loss: 7060.789062\n",
      "Train Epoch: 78 [217344/225000 (97%)] Loss: 6985.908203\n",
      "Train Epoch: 78 [221440/225000 (98%)] Loss: 7214.080078\n",
      "    epoch          : 78\n",
      "    loss           : 7048.572145637799\n",
      "    val_loss       : 7004.49042129395\n",
      "Train Epoch: 79 [256/225000 (0%)] Loss: 6966.666016\n",
      "Train Epoch: 79 [4352/225000 (2%)] Loss: 6984.775391\n",
      "Train Epoch: 79 [8448/225000 (4%)] Loss: 6964.861328\n",
      "Train Epoch: 79 [12544/225000 (6%)] Loss: 6833.152344\n",
      "Train Epoch: 79 [16640/225000 (7%)] Loss: 6960.113281\n",
      "Train Epoch: 79 [20736/225000 (9%)] Loss: 6934.150391\n",
      "Train Epoch: 79 [24832/225000 (11%)] Loss: 6968.708984\n",
      "Train Epoch: 79 [28928/225000 (13%)] Loss: 6936.037109\n",
      "Train Epoch: 79 [33024/225000 (15%)] Loss: 7097.921875\n",
      "Train Epoch: 79 [37120/225000 (16%)] Loss: 7003.429688\n",
      "Train Epoch: 79 [41216/225000 (18%)] Loss: 6932.525391\n",
      "Train Epoch: 79 [45312/225000 (20%)] Loss: 6910.386719\n",
      "Train Epoch: 79 [49408/225000 (22%)] Loss: 7041.845703\n",
      "Train Epoch: 79 [53504/225000 (24%)] Loss: 7134.679688\n",
      "Train Epoch: 79 [57600/225000 (26%)] Loss: 28857.025391\n",
      "Train Epoch: 79 [61696/225000 (27%)] Loss: 7018.025391\n",
      "Train Epoch: 79 [65792/225000 (29%)] Loss: 6848.091797\n",
      "Train Epoch: 79 [69888/225000 (31%)] Loss: 6949.968750\n",
      "Train Epoch: 79 [73984/225000 (33%)] Loss: 6962.548828\n",
      "Train Epoch: 79 [78080/225000 (35%)] Loss: 6983.763672\n",
      "Train Epoch: 79 [82176/225000 (37%)] Loss: 8675.492188\n",
      "Train Epoch: 79 [86272/225000 (38%)] Loss: 6952.244141\n",
      "Train Epoch: 79 [90368/225000 (40%)] Loss: 6931.669922\n",
      "Train Epoch: 79 [94464/225000 (42%)] Loss: 7063.990234\n",
      "Train Epoch: 79 [98560/225000 (44%)] Loss: 6963.308594\n",
      "Train Epoch: 79 [102656/225000 (46%)] Loss: 6945.363281\n",
      "Train Epoch: 79 [106752/225000 (47%)] Loss: 7022.882812\n",
      "Train Epoch: 79 [110848/225000 (49%)] Loss: 6918.734375\n",
      "Train Epoch: 79 [114944/225000 (51%)] Loss: 6996.455078\n",
      "Train Epoch: 79 [119040/225000 (53%)] Loss: 7008.605469\n",
      "Train Epoch: 79 [123136/225000 (55%)] Loss: 6803.955078\n",
      "Train Epoch: 79 [127232/225000 (57%)] Loss: 7074.121094\n",
      "Train Epoch: 79 [131328/225000 (58%)] Loss: 7112.933594\n",
      "Train Epoch: 79 [135424/225000 (60%)] Loss: 6993.507812\n",
      "Train Epoch: 79 [139520/225000 (62%)] Loss: 6947.636719\n",
      "Train Epoch: 79 [143616/225000 (64%)] Loss: 7032.503906\n",
      "Train Epoch: 79 [147712/225000 (66%)] Loss: 6910.060547\n",
      "Train Epoch: 79 [151808/225000 (67%)] Loss: 6882.892578\n",
      "Train Epoch: 79 [155904/225000 (69%)] Loss: 7015.654297\n",
      "Train Epoch: 79 [160000/225000 (71%)] Loss: 6917.300781\n",
      "Train Epoch: 79 [164096/225000 (73%)] Loss: 7088.248047\n",
      "Train Epoch: 79 [168192/225000 (75%)] Loss: 6968.154297\n",
      "Train Epoch: 79 [172288/225000 (77%)] Loss: 6950.439453\n",
      "Train Epoch: 79 [176384/225000 (78%)] Loss: 6997.058594\n",
      "Train Epoch: 79 [180480/225000 (80%)] Loss: 6998.076172\n",
      "Train Epoch: 79 [184576/225000 (82%)] Loss: 7005.406250\n",
      "Train Epoch: 79 [188672/225000 (84%)] Loss: 6886.007812\n",
      "Train Epoch: 79 [192768/225000 (86%)] Loss: 7089.558594\n",
      "Train Epoch: 79 [196864/225000 (87%)] Loss: 6932.720703\n",
      "Train Epoch: 79 [200960/225000 (89%)] Loss: 6946.878906\n",
      "Train Epoch: 79 [205056/225000 (91%)] Loss: 7087.062500\n",
      "Train Epoch: 79 [209152/225000 (93%)] Loss: 6999.376953\n",
      "Train Epoch: 79 [213248/225000 (95%)] Loss: 7007.011719\n",
      "Train Epoch: 79 [217344/225000 (97%)] Loss: 6874.302734\n",
      "Train Epoch: 79 [221440/225000 (98%)] Loss: 7058.250000\n",
      "    epoch          : 79\n",
      "    loss           : 7097.785078480518\n",
      "    val_loss       : 7032.031065247496\n",
      "Train Epoch: 80 [256/225000 (0%)] Loss: 6919.074219\n",
      "Train Epoch: 80 [4352/225000 (2%)] Loss: 6852.742188\n",
      "Train Epoch: 80 [8448/225000 (4%)] Loss: 6878.148438\n",
      "Train Epoch: 80 [12544/225000 (6%)] Loss: 6923.041016\n",
      "Train Epoch: 80 [16640/225000 (7%)] Loss: 7160.041016\n",
      "Train Epoch: 80 [20736/225000 (9%)] Loss: 7025.884766\n",
      "Train Epoch: 80 [24832/225000 (11%)] Loss: 6962.898438\n",
      "Train Epoch: 80 [28928/225000 (13%)] Loss: 7008.966797\n",
      "Train Epoch: 80 [33024/225000 (15%)] Loss: 7078.242188\n",
      "Train Epoch: 80 [37120/225000 (16%)] Loss: 6978.167969\n",
      "Train Epoch: 80 [41216/225000 (18%)] Loss: 6817.138672\n",
      "Train Epoch: 80 [45312/225000 (20%)] Loss: 6963.326172\n",
      "Train Epoch: 80 [49408/225000 (22%)] Loss: 7133.771484\n",
      "Train Epoch: 80 [53504/225000 (24%)] Loss: 7072.214844\n",
      "Train Epoch: 80 [57600/225000 (26%)] Loss: 6972.052734\n",
      "Train Epoch: 80 [61696/225000 (27%)] Loss: 6993.875000\n",
      "Train Epoch: 80 [65792/225000 (29%)] Loss: 7169.609375\n",
      "Train Epoch: 80 [69888/225000 (31%)] Loss: 6902.064453\n",
      "Train Epoch: 80 [73984/225000 (33%)] Loss: 6891.947266\n",
      "Train Epoch: 80 [78080/225000 (35%)] Loss: 6906.982422\n",
      "Train Epoch: 80 [82176/225000 (37%)] Loss: 6938.371094\n",
      "Train Epoch: 80 [86272/225000 (38%)] Loss: 6910.792969\n",
      "Train Epoch: 80 [90368/225000 (40%)] Loss: 6979.031250\n",
      "Train Epoch: 80 [94464/225000 (42%)] Loss: 7110.347656\n",
      "Train Epoch: 80 [98560/225000 (44%)] Loss: 7062.986328\n",
      "Train Epoch: 80 [102656/225000 (46%)] Loss: 8774.455078\n",
      "Train Epoch: 80 [106752/225000 (47%)] Loss: 6961.519531\n",
      "Train Epoch: 80 [110848/225000 (49%)] Loss: 6989.091797\n",
      "Train Epoch: 80 [114944/225000 (51%)] Loss: 6969.871094\n",
      "Train Epoch: 80 [119040/225000 (53%)] Loss: 7034.882812\n",
      "Train Epoch: 80 [123136/225000 (55%)] Loss: 6971.425781\n",
      "Train Epoch: 80 [127232/225000 (57%)] Loss: 6969.828125\n",
      "Train Epoch: 80 [131328/225000 (58%)] Loss: 7002.230469\n",
      "Train Epoch: 80 [135424/225000 (60%)] Loss: 7008.507812\n",
      "Train Epoch: 80 [139520/225000 (62%)] Loss: 7135.886719\n",
      "Train Epoch: 80 [143616/225000 (64%)] Loss: 6982.861328\n",
      "Train Epoch: 80 [147712/225000 (66%)] Loss: 6952.447266\n",
      "Train Epoch: 80 [151808/225000 (67%)] Loss: 6969.187500\n",
      "Train Epoch: 80 [155904/225000 (69%)] Loss: 7065.687500\n",
      "Train Epoch: 80 [160000/225000 (71%)] Loss: 7072.398438\n",
      "Train Epoch: 80 [164096/225000 (73%)] Loss: 6933.855469\n",
      "Train Epoch: 80 [168192/225000 (75%)] Loss: 6924.634766\n",
      "Train Epoch: 80 [172288/225000 (77%)] Loss: 7013.300781\n",
      "Train Epoch: 80 [176384/225000 (78%)] Loss: 6923.980469\n",
      "Train Epoch: 80 [180480/225000 (80%)] Loss: 6909.179688\n",
      "Train Epoch: 80 [184576/225000 (82%)] Loss: 6847.611328\n",
      "Train Epoch: 80 [188672/225000 (84%)] Loss: 6908.949219\n",
      "Train Epoch: 80 [192768/225000 (86%)] Loss: 6999.513672\n",
      "Train Epoch: 80 [196864/225000 (87%)] Loss: 6845.437500\n",
      "Train Epoch: 80 [200960/225000 (89%)] Loss: 6923.763672\n",
      "Train Epoch: 80 [205056/225000 (91%)] Loss: 6961.587891\n",
      "Train Epoch: 80 [209152/225000 (93%)] Loss: 6987.707031\n",
      "Train Epoch: 80 [213248/225000 (95%)] Loss: 6805.154297\n",
      "Train Epoch: 80 [217344/225000 (97%)] Loss: 7020.048828\n",
      "Train Epoch: 80 [221440/225000 (98%)] Loss: 6855.113281\n",
      "    epoch          : 80\n",
      "    loss           : 7010.472748462386\n",
      "    val_loss       : 6959.462091080996\n",
      "Train Epoch: 81 [256/225000 (0%)] Loss: 6955.533203\n",
      "Train Epoch: 81 [4352/225000 (2%)] Loss: 6998.835938\n",
      "Train Epoch: 81 [8448/225000 (4%)] Loss: 7009.031250\n",
      "Train Epoch: 81 [12544/225000 (6%)] Loss: 6975.988281\n",
      "Train Epoch: 81 [16640/225000 (7%)] Loss: 7101.849609\n",
      "Train Epoch: 81 [20736/225000 (9%)] Loss: 6796.039062\n",
      "Train Epoch: 81 [24832/225000 (11%)] Loss: 7018.761719\n",
      "Train Epoch: 81 [28928/225000 (13%)] Loss: 6888.093750\n",
      "Train Epoch: 81 [33024/225000 (15%)] Loss: 6926.515625\n",
      "Train Epoch: 81 [37120/225000 (16%)] Loss: 6939.361328\n",
      "Train Epoch: 81 [41216/225000 (18%)] Loss: 6636.361328\n",
      "Train Epoch: 81 [45312/225000 (20%)] Loss: 7142.119141\n",
      "Train Epoch: 81 [49408/225000 (22%)] Loss: 7095.644531\n",
      "Train Epoch: 81 [53504/225000 (24%)] Loss: 7040.076172\n",
      "Train Epoch: 81 [57600/225000 (26%)] Loss: 6954.498047\n",
      "Train Epoch: 81 [61696/225000 (27%)] Loss: 6985.250000\n",
      "Train Epoch: 81 [65792/225000 (29%)] Loss: 7027.167969\n",
      "Train Epoch: 81 [69888/225000 (31%)] Loss: 8850.380859\n",
      "Train Epoch: 81 [73984/225000 (33%)] Loss: 6821.474609\n",
      "Train Epoch: 81 [78080/225000 (35%)] Loss: 6965.437500\n",
      "Train Epoch: 81 [82176/225000 (37%)] Loss: 6997.572266\n",
      "Train Epoch: 81 [86272/225000 (38%)] Loss: 6994.371094\n",
      "Train Epoch: 81 [90368/225000 (40%)] Loss: 7126.535156\n",
      "Train Epoch: 81 [94464/225000 (42%)] Loss: 6954.382812\n",
      "Train Epoch: 81 [98560/225000 (44%)] Loss: 7156.216797\n",
      "Train Epoch: 81 [102656/225000 (46%)] Loss: 7052.992188\n",
      "Train Epoch: 81 [106752/225000 (47%)] Loss: 6964.851562\n",
      "Train Epoch: 81 [110848/225000 (49%)] Loss: 7031.722656\n",
      "Train Epoch: 81 [114944/225000 (51%)] Loss: 7099.937500\n",
      "Train Epoch: 81 [119040/225000 (53%)] Loss: 6885.865234\n",
      "Train Epoch: 81 [123136/225000 (55%)] Loss: 6888.697266\n",
      "Train Epoch: 81 [127232/225000 (57%)] Loss: 6784.412109\n",
      "Train Epoch: 81 [131328/225000 (58%)] Loss: 6923.369141\n",
      "Train Epoch: 81 [135424/225000 (60%)] Loss: 6968.183594\n",
      "Train Epoch: 81 [139520/225000 (62%)] Loss: 6916.917969\n",
      "Train Epoch: 81 [143616/225000 (64%)] Loss: 7136.230469\n",
      "Train Epoch: 81 [147712/225000 (66%)] Loss: 6931.779297\n",
      "Train Epoch: 81 [151808/225000 (67%)] Loss: 6755.253906\n",
      "Train Epoch: 81 [155904/225000 (69%)] Loss: 6925.736328\n",
      "Train Epoch: 81 [160000/225000 (71%)] Loss: 6870.234375\n",
      "Train Epoch: 81 [164096/225000 (73%)] Loss: 6951.226562\n",
      "Train Epoch: 81 [168192/225000 (75%)] Loss: 6974.541016\n",
      "Train Epoch: 81 [172288/225000 (77%)] Loss: 6941.482422\n",
      "Train Epoch: 81 [176384/225000 (78%)] Loss: 7047.011719\n",
      "Train Epoch: 81 [180480/225000 (80%)] Loss: 6824.623047\n",
      "Train Epoch: 81 [184576/225000 (82%)] Loss: 6914.882812\n",
      "Train Epoch: 81 [188672/225000 (84%)] Loss: 7050.087891\n",
      "Train Epoch: 81 [192768/225000 (86%)] Loss: 7051.507812\n",
      "Train Epoch: 81 [196864/225000 (87%)] Loss: 6848.951172\n",
      "Train Epoch: 81 [200960/225000 (89%)] Loss: 6913.964844\n",
      "Train Epoch: 81 [205056/225000 (91%)] Loss: 6959.769531\n",
      "Train Epoch: 81 [209152/225000 (93%)] Loss: 6952.146484\n",
      "Train Epoch: 81 [213248/225000 (95%)] Loss: 33177.101562\n",
      "Train Epoch: 81 [217344/225000 (97%)] Loss: 6874.830078\n",
      "Train Epoch: 81 [221440/225000 (98%)] Loss: 6953.904297\n",
      "    epoch          : 81\n",
      "    loss           : 6991.594627684158\n",
      "    val_loss       : 7004.502799129\n",
      "Train Epoch: 82 [256/225000 (0%)] Loss: 6847.449219\n",
      "Train Epoch: 82 [4352/225000 (2%)] Loss: 6996.669922\n",
      "Train Epoch: 82 [8448/225000 (4%)] Loss: 6982.988281\n",
      "Train Epoch: 82 [12544/225000 (6%)] Loss: 6846.240234\n",
      "Train Epoch: 82 [16640/225000 (7%)] Loss: 6963.058594\n",
      "Train Epoch: 82 [20736/225000 (9%)] Loss: 6865.994141\n",
      "Train Epoch: 82 [24832/225000 (11%)] Loss: 7040.523438\n",
      "Train Epoch: 82 [28928/225000 (13%)] Loss: 6886.660156\n",
      "Train Epoch: 82 [33024/225000 (15%)] Loss: 6951.324219\n",
      "Train Epoch: 82 [37120/225000 (16%)] Loss: 6786.824219\n",
      "Train Epoch: 82 [41216/225000 (18%)] Loss: 6956.912109\n",
      "Train Epoch: 82 [45312/225000 (20%)] Loss: 6981.197266\n",
      "Train Epoch: 82 [49408/225000 (22%)] Loss: 6954.132812\n",
      "Train Epoch: 82 [53504/225000 (24%)] Loss: 6954.021484\n",
      "Train Epoch: 82 [57600/225000 (26%)] Loss: 7037.556641\n",
      "Train Epoch: 82 [61696/225000 (27%)] Loss: 6800.210938\n",
      "Train Epoch: 82 [65792/225000 (29%)] Loss: 6894.666016\n",
      "Train Epoch: 82 [69888/225000 (31%)] Loss: 6947.207031\n",
      "Train Epoch: 82 [73984/225000 (33%)] Loss: 6846.417969\n",
      "Train Epoch: 82 [78080/225000 (35%)] Loss: 6937.044922\n",
      "Train Epoch: 82 [82176/225000 (37%)] Loss: 6919.150391\n",
      "Train Epoch: 82 [86272/225000 (38%)] Loss: 6885.615234\n",
      "Train Epoch: 82 [90368/225000 (40%)] Loss: 6767.976562\n",
      "Train Epoch: 82 [94464/225000 (42%)] Loss: 7013.234375\n",
      "Train Epoch: 82 [98560/225000 (44%)] Loss: 6805.738281\n",
      "Train Epoch: 82 [102656/225000 (46%)] Loss: 6893.576172\n",
      "Train Epoch: 82 [106752/225000 (47%)] Loss: 6925.660156\n",
      "Train Epoch: 82 [110848/225000 (49%)] Loss: 7016.394531\n",
      "Train Epoch: 82 [114944/225000 (51%)] Loss: 6873.578125\n",
      "Train Epoch: 82 [119040/225000 (53%)] Loss: 7016.623047\n",
      "Train Epoch: 82 [123136/225000 (55%)] Loss: 6906.039062\n",
      "Train Epoch: 82 [127232/225000 (57%)] Loss: 6930.982422\n",
      "Train Epoch: 82 [131328/225000 (58%)] Loss: 6857.572266\n",
      "Train Epoch: 82 [135424/225000 (60%)] Loss: 6989.824219\n",
      "Train Epoch: 82 [139520/225000 (62%)] Loss: 7022.037109\n",
      "Train Epoch: 82 [143616/225000 (64%)] Loss: 6999.937500\n",
      "Train Epoch: 82 [147712/225000 (66%)] Loss: 7042.265625\n",
      "Train Epoch: 82 [151808/225000 (67%)] Loss: 6998.726562\n",
      "Train Epoch: 82 [155904/225000 (69%)] Loss: 7111.232422\n",
      "Train Epoch: 82 [160000/225000 (71%)] Loss: 6998.445312\n",
      "Train Epoch: 82 [164096/225000 (73%)] Loss: 7012.728516\n",
      "Train Epoch: 82 [168192/225000 (75%)] Loss: 6899.037109\n",
      "Train Epoch: 82 [172288/225000 (77%)] Loss: 6966.185547\n",
      "Train Epoch: 82 [176384/225000 (78%)] Loss: 6796.630859\n",
      "Train Epoch: 82 [180480/225000 (80%)] Loss: 6950.833984\n",
      "Train Epoch: 82 [184576/225000 (82%)] Loss: 6795.046875\n",
      "Train Epoch: 82 [188672/225000 (84%)] Loss: 6772.824219\n",
      "Train Epoch: 82 [192768/225000 (86%)] Loss: 6921.740234\n",
      "Train Epoch: 82 [196864/225000 (87%)] Loss: 7079.943359\n",
      "Train Epoch: 82 [200960/225000 (89%)] Loss: 6907.412109\n",
      "Train Epoch: 82 [205056/225000 (91%)] Loss: 6848.929688\n",
      "Train Epoch: 82 [209152/225000 (93%)] Loss: 6924.974609\n",
      "Train Epoch: 82 [213248/225000 (95%)] Loss: 6916.953125\n",
      "Train Epoch: 82 [217344/225000 (97%)] Loss: 7016.462891\n",
      "Train Epoch: 82 [221440/225000 (98%)] Loss: 6974.861328\n",
      "    epoch          : 82\n",
      "    loss           : 6999.1142578125\n",
      "    val_loss       : 6936.910244331068\n",
      "Train Epoch: 83 [256/225000 (0%)] Loss: 6973.630859\n",
      "Train Epoch: 83 [4352/225000 (2%)] Loss: 6973.529297\n",
      "Train Epoch: 83 [8448/225000 (4%)] Loss: 6832.703125\n",
      "Train Epoch: 83 [12544/225000 (6%)] Loss: 6882.494141\n",
      "Train Epoch: 83 [16640/225000 (7%)] Loss: 6974.341797\n",
      "Train Epoch: 83 [20736/225000 (9%)] Loss: 6983.123047\n",
      "Train Epoch: 83 [24832/225000 (11%)] Loss: 6900.650391\n",
      "Train Epoch: 83 [28928/225000 (13%)] Loss: 7065.427734\n",
      "Train Epoch: 83 [33024/225000 (15%)] Loss: 6928.904297\n",
      "Train Epoch: 83 [37120/225000 (16%)] Loss: 6995.333984\n",
      "Train Epoch: 83 [41216/225000 (18%)] Loss: 8706.982422\n",
      "Train Epoch: 83 [45312/225000 (20%)] Loss: 6730.554688\n",
      "Train Epoch: 83 [49408/225000 (22%)] Loss: 6854.880859\n",
      "Train Epoch: 83 [53504/225000 (24%)] Loss: 6930.019531\n",
      "Train Epoch: 83 [57600/225000 (26%)] Loss: 7092.419922\n",
      "Train Epoch: 83 [61696/225000 (27%)] Loss: 6899.208984\n",
      "Train Epoch: 83 [65792/225000 (29%)] Loss: 6951.382812\n",
      "Train Epoch: 83 [69888/225000 (31%)] Loss: 6981.716797\n",
      "Train Epoch: 83 [73984/225000 (33%)] Loss: 7011.654297\n",
      "Train Epoch: 83 [78080/225000 (35%)] Loss: 6990.300781\n",
      "Train Epoch: 83 [82176/225000 (37%)] Loss: 6720.503906\n",
      "Train Epoch: 83 [86272/225000 (38%)] Loss: 7169.437500\n",
      "Train Epoch: 83 [90368/225000 (40%)] Loss: 6878.718750\n",
      "Train Epoch: 83 [94464/225000 (42%)] Loss: 6812.187500\n",
      "Train Epoch: 83 [98560/225000 (44%)] Loss: 7093.166016\n",
      "Train Epoch: 83 [102656/225000 (46%)] Loss: 6910.921875\n",
      "Train Epoch: 83 [106752/225000 (47%)] Loss: 6829.812500\n",
      "Train Epoch: 83 [110848/225000 (49%)] Loss: 6897.150391\n",
      "Train Epoch: 83 [114944/225000 (51%)] Loss: 6716.146484\n",
      "Train Epoch: 83 [119040/225000 (53%)] Loss: 7017.738281\n",
      "Train Epoch: 83 [123136/225000 (55%)] Loss: 6927.349609\n",
      "Train Epoch: 83 [127232/225000 (57%)] Loss: 6919.050781\n",
      "Train Epoch: 83 [131328/225000 (58%)] Loss: 6991.236328\n",
      "Train Epoch: 83 [135424/225000 (60%)] Loss: 6899.748047\n",
      "Train Epoch: 83 [139520/225000 (62%)] Loss: 6894.201172\n",
      "Train Epoch: 83 [143616/225000 (64%)] Loss: 6966.580078\n",
      "Train Epoch: 83 [147712/225000 (66%)] Loss: 6924.537109\n",
      "Train Epoch: 83 [151808/225000 (67%)] Loss: 6946.093750\n",
      "Train Epoch: 83 [155904/225000 (69%)] Loss: 7087.673828\n",
      "Train Epoch: 83 [160000/225000 (71%)] Loss: 6896.267578\n",
      "Train Epoch: 83 [164096/225000 (73%)] Loss: 7000.521484\n",
      "Train Epoch: 83 [168192/225000 (75%)] Loss: 6927.687500\n",
      "Train Epoch: 83 [172288/225000 (77%)] Loss: 6851.628906\n",
      "Train Epoch: 83 [176384/225000 (78%)] Loss: 6998.785156\n",
      "Train Epoch: 83 [180480/225000 (80%)] Loss: 7015.373047\n",
      "Train Epoch: 83 [184576/225000 (82%)] Loss: 6886.726562\n",
      "Train Epoch: 83 [188672/225000 (84%)] Loss: 6990.419922\n",
      "Train Epoch: 83 [192768/225000 (86%)] Loss: 6937.193359\n",
      "Train Epoch: 83 [196864/225000 (87%)] Loss: 6916.693359\n",
      "Train Epoch: 83 [200960/225000 (89%)] Loss: 7078.958984\n",
      "Train Epoch: 83 [205056/225000 (91%)] Loss: 6897.998047\n",
      "Train Epoch: 83 [209152/225000 (93%)] Loss: 7010.097656\n",
      "Train Epoch: 83 [213248/225000 (95%)] Loss: 6926.765625\n",
      "Train Epoch: 83 [217344/225000 (97%)] Loss: 6929.763672\n",
      "Train Epoch: 83 [221440/225000 (98%)] Loss: 7007.578125\n",
      "    epoch          : 83\n",
      "    loss           : 7010.078533845279\n",
      "    val_loss       : 7147.136987771307\n",
      "Train Epoch: 84 [256/225000 (0%)] Loss: 7114.833984\n",
      "Train Epoch: 84 [4352/225000 (2%)] Loss: 6826.853516\n",
      "Train Epoch: 84 [8448/225000 (4%)] Loss: 6793.880859\n",
      "Train Epoch: 84 [12544/225000 (6%)] Loss: 6967.996094\n",
      "Train Epoch: 84 [16640/225000 (7%)] Loss: 6941.476562\n",
      "Train Epoch: 84 [20736/225000 (9%)] Loss: 7058.853516\n",
      "Train Epoch: 84 [24832/225000 (11%)] Loss: 6961.667969\n",
      "Train Epoch: 84 [28928/225000 (13%)] Loss: 6865.427734\n",
      "Train Epoch: 84 [33024/225000 (15%)] Loss: 7047.306641\n",
      "Train Epoch: 84 [37120/225000 (16%)] Loss: 6961.751953\n",
      "Train Epoch: 84 [41216/225000 (18%)] Loss: 6955.056641\n",
      "Train Epoch: 84 [45312/225000 (20%)] Loss: 6872.427734\n",
      "Train Epoch: 84 [49408/225000 (22%)] Loss: 6778.988281\n",
      "Train Epoch: 84 [53504/225000 (24%)] Loss: 6909.419922\n",
      "Train Epoch: 84 [57600/225000 (26%)] Loss: 6917.121094\n",
      "Train Epoch: 84 [61696/225000 (27%)] Loss: 7039.193359\n",
      "Train Epoch: 84 [65792/225000 (29%)] Loss: 6987.589844\n",
      "Train Epoch: 84 [69888/225000 (31%)] Loss: 6835.552734\n",
      "Train Epoch: 84 [73984/225000 (33%)] Loss: 6975.939453\n",
      "Train Epoch: 84 [78080/225000 (35%)] Loss: 6967.693359\n",
      "Train Epoch: 84 [82176/225000 (37%)] Loss: 6993.041016\n",
      "Train Epoch: 84 [86272/225000 (38%)] Loss: 6834.371094\n",
      "Train Epoch: 84 [90368/225000 (40%)] Loss: 6895.220703\n",
      "Train Epoch: 84 [94464/225000 (42%)] Loss: 6889.294922\n",
      "Train Epoch: 84 [98560/225000 (44%)] Loss: 6787.701172\n",
      "Train Epoch: 84 [102656/225000 (46%)] Loss: 6886.619141\n",
      "Train Epoch: 84 [106752/225000 (47%)] Loss: 6970.003906\n",
      "Train Epoch: 84 [110848/225000 (49%)] Loss: 6787.171875\n",
      "Train Epoch: 84 [114944/225000 (51%)] Loss: 6763.419922\n",
      "Train Epoch: 84 [119040/225000 (53%)] Loss: 6858.236328\n",
      "Train Epoch: 84 [123136/225000 (55%)] Loss: 6716.638672\n",
      "Train Epoch: 84 [127232/225000 (57%)] Loss: 6950.470703\n",
      "Train Epoch: 84 [131328/225000 (58%)] Loss: 6903.193359\n",
      "Train Epoch: 84 [135424/225000 (60%)] Loss: 6894.593750\n",
      "Train Epoch: 84 [139520/225000 (62%)] Loss: 6753.402344\n",
      "Train Epoch: 84 [143616/225000 (64%)] Loss: 6833.818359\n",
      "Train Epoch: 84 [147712/225000 (66%)] Loss: 6848.974609\n",
      "Train Epoch: 84 [151808/225000 (67%)] Loss: 6894.527344\n",
      "Train Epoch: 84 [155904/225000 (69%)] Loss: 6971.984375\n",
      "Train Epoch: 84 [160000/225000 (71%)] Loss: 7088.175781\n",
      "Train Epoch: 84 [164096/225000 (73%)] Loss: 6759.306641\n",
      "Train Epoch: 84 [168192/225000 (75%)] Loss: 6878.322266\n",
      "Train Epoch: 84 [172288/225000 (77%)] Loss: 6866.951172\n",
      "Train Epoch: 84 [176384/225000 (78%)] Loss: 7078.498047\n",
      "Train Epoch: 84 [180480/225000 (80%)] Loss: 6874.001953\n",
      "Train Epoch: 84 [184576/225000 (82%)] Loss: 6852.830078\n",
      "Train Epoch: 84 [188672/225000 (84%)] Loss: 7114.031250\n",
      "Train Epoch: 84 [192768/225000 (86%)] Loss: 7089.611328\n",
      "Train Epoch: 84 [196864/225000 (87%)] Loss: 7008.968750\n",
      "Train Epoch: 84 [200960/225000 (89%)] Loss: 7068.978516\n",
      "Train Epoch: 84 [205056/225000 (91%)] Loss: 6938.724609\n",
      "Train Epoch: 84 [209152/225000 (93%)] Loss: 6853.119141\n",
      "Train Epoch: 84 [213248/225000 (95%)] Loss: 6841.224609\n",
      "Train Epoch: 84 [217344/225000 (97%)] Loss: 6788.384766\n",
      "Train Epoch: 84 [221440/225000 (98%)] Loss: 6996.880859\n",
      "    epoch          : 84\n",
      "    loss           : 6980.560706857935\n",
      "    val_loss       : 6952.597344891149\n",
      "Train Epoch: 85 [256/225000 (0%)] Loss: 6837.417969\n",
      "Train Epoch: 85 [4352/225000 (2%)] Loss: 6840.802734\n",
      "Train Epoch: 85 [8448/225000 (4%)] Loss: 6837.113281\n",
      "Train Epoch: 85 [12544/225000 (6%)] Loss: 6978.757812\n",
      "Train Epoch: 85 [16640/225000 (7%)] Loss: 6838.474609\n",
      "Train Epoch: 85 [20736/225000 (9%)] Loss: 7006.150391\n",
      "Train Epoch: 85 [24832/225000 (11%)] Loss: 6923.310547\n",
      "Train Epoch: 85 [28928/225000 (13%)] Loss: 6840.548828\n",
      "Train Epoch: 85 [33024/225000 (15%)] Loss: 6931.574219\n",
      "Train Epoch: 85 [37120/225000 (16%)] Loss: 7006.072266\n",
      "Train Epoch: 85 [41216/225000 (18%)] Loss: 6930.228516\n",
      "Train Epoch: 85 [45312/225000 (20%)] Loss: 6903.636719\n",
      "Train Epoch: 85 [49408/225000 (22%)] Loss: 6941.613281\n",
      "Train Epoch: 85 [53504/225000 (24%)] Loss: 6953.130859\n",
      "Train Epoch: 85 [57600/225000 (26%)] Loss: 6884.189453\n",
      "Train Epoch: 85 [61696/225000 (27%)] Loss: 6856.148438\n",
      "Train Epoch: 85 [65792/225000 (29%)] Loss: 6937.556641\n",
      "Train Epoch: 85 [69888/225000 (31%)] Loss: 6928.093750\n",
      "Train Epoch: 85 [73984/225000 (33%)] Loss: 6990.328125\n",
      "Train Epoch: 85 [78080/225000 (35%)] Loss: 6902.892578\n",
      "Train Epoch: 85 [82176/225000 (37%)] Loss: 6873.007812\n",
      "Train Epoch: 85 [86272/225000 (38%)] Loss: 6851.759766\n",
      "Train Epoch: 85 [90368/225000 (40%)] Loss: 6815.148438\n",
      "Train Epoch: 85 [94464/225000 (42%)] Loss: 6750.728516\n",
      "Train Epoch: 85 [98560/225000 (44%)] Loss: 6831.271484\n",
      "Train Epoch: 85 [102656/225000 (46%)] Loss: 6883.570312\n",
      "Train Epoch: 85 [106752/225000 (47%)] Loss: 6901.056641\n",
      "Train Epoch: 85 [110848/225000 (49%)] Loss: 6883.548828\n",
      "Train Epoch: 85 [114944/225000 (51%)] Loss: 6857.369141\n",
      "Train Epoch: 85 [119040/225000 (53%)] Loss: 6869.009766\n",
      "Train Epoch: 85 [123136/225000 (55%)] Loss: 7004.416016\n",
      "Train Epoch: 85 [127232/225000 (57%)] Loss: 6802.583984\n",
      "Train Epoch: 85 [131328/225000 (58%)] Loss: 6944.884766\n",
      "Train Epoch: 85 [135424/225000 (60%)] Loss: 7028.509766\n",
      "Train Epoch: 85 [139520/225000 (62%)] Loss: 6933.154297\n",
      "Train Epoch: 85 [143616/225000 (64%)] Loss: 6877.966797\n",
      "Train Epoch: 85 [147712/225000 (66%)] Loss: 6935.554688\n",
      "Train Epoch: 85 [151808/225000 (67%)] Loss: 6811.599609\n",
      "Train Epoch: 85 [155904/225000 (69%)] Loss: 6698.542969\n",
      "Train Epoch: 85 [160000/225000 (71%)] Loss: 6808.546875\n",
      "Train Epoch: 85 [164096/225000 (73%)] Loss: 7031.494141\n",
      "Train Epoch: 85 [168192/225000 (75%)] Loss: 6910.652344\n",
      "Train Epoch: 85 [172288/225000 (77%)] Loss: 6731.302734\n",
      "Train Epoch: 85 [176384/225000 (78%)] Loss: 7024.597656\n",
      "Train Epoch: 85 [180480/225000 (80%)] Loss: 6937.878906\n",
      "Train Epoch: 85 [184576/225000 (82%)] Loss: 6930.994141\n",
      "Train Epoch: 85 [188672/225000 (84%)] Loss: 6985.105469\n",
      "Train Epoch: 85 [192768/225000 (86%)] Loss: 6846.666016\n",
      "Train Epoch: 85 [196864/225000 (87%)] Loss: 6949.265625\n",
      "Train Epoch: 85 [200960/225000 (89%)] Loss: 6826.996094\n",
      "Train Epoch: 85 [205056/225000 (91%)] Loss: 6988.628906\n",
      "Train Epoch: 85 [209152/225000 (93%)] Loss: 6865.566406\n",
      "Train Epoch: 85 [213248/225000 (95%)] Loss: 6961.492188\n",
      "Train Epoch: 85 [217344/225000 (97%)] Loss: 6813.505859\n",
      "Train Epoch: 85 [221440/225000 (98%)] Loss: 6774.769531\n",
      "    epoch          : 85\n",
      "    loss           : 7030.002416408916\n",
      "    val_loss       : 6907.487476264944\n",
      "Train Epoch: 86 [256/225000 (0%)] Loss: 7055.984375\n",
      "Train Epoch: 86 [4352/225000 (2%)] Loss: 28405.291016\n",
      "Train Epoch: 86 [8448/225000 (4%)] Loss: 6933.978516\n",
      "Train Epoch: 86 [12544/225000 (6%)] Loss: 6845.330078\n",
      "Train Epoch: 86 [16640/225000 (7%)] Loss: 6874.732422\n",
      "Train Epoch: 86 [20736/225000 (9%)] Loss: 6743.582031\n",
      "Train Epoch: 86 [24832/225000 (11%)] Loss: 6951.683594\n",
      "Train Epoch: 86 [28928/225000 (13%)] Loss: 6966.082031\n",
      "Train Epoch: 86 [33024/225000 (15%)] Loss: 6892.208984\n",
      "Train Epoch: 86 [37120/225000 (16%)] Loss: 6944.535156\n",
      "Train Epoch: 86 [41216/225000 (18%)] Loss: 6866.082031\n",
      "Train Epoch: 86 [45312/225000 (20%)] Loss: 6947.982422\n",
      "Train Epoch: 86 [49408/225000 (22%)] Loss: 8715.927734\n",
      "Train Epoch: 86 [53504/225000 (24%)] Loss: 6850.896484\n",
      "Train Epoch: 86 [57600/225000 (26%)] Loss: 6910.013672\n",
      "Train Epoch: 86 [61696/225000 (27%)] Loss: 6870.960938\n",
      "Train Epoch: 86 [65792/225000 (29%)] Loss: 6859.181641\n",
      "Train Epoch: 86 [69888/225000 (31%)] Loss: 7009.779297\n",
      "Train Epoch: 86 [73984/225000 (33%)] Loss: 6890.996094\n",
      "Train Epoch: 86 [78080/225000 (35%)] Loss: 6887.917969\n",
      "Train Epoch: 86 [82176/225000 (37%)] Loss: 6896.398438\n",
      "Train Epoch: 86 [86272/225000 (38%)] Loss: 6896.515625\n",
      "Train Epoch: 86 [90368/225000 (40%)] Loss: 6868.523438\n",
      "Train Epoch: 86 [94464/225000 (42%)] Loss: 6923.507812\n",
      "Train Epoch: 86 [98560/225000 (44%)] Loss: 6864.460938\n",
      "Train Epoch: 86 [102656/225000 (46%)] Loss: 6957.593750\n",
      "Train Epoch: 86 [106752/225000 (47%)] Loss: 6947.015625\n",
      "Train Epoch: 86 [110848/225000 (49%)] Loss: 6787.210938\n",
      "Train Epoch: 86 [114944/225000 (51%)] Loss: 6990.798828\n",
      "Train Epoch: 86 [119040/225000 (53%)] Loss: 7009.187500\n",
      "Train Epoch: 86 [123136/225000 (55%)] Loss: 6772.982422\n",
      "Train Epoch: 86 [127232/225000 (57%)] Loss: 6879.814453\n",
      "Train Epoch: 86 [131328/225000 (58%)] Loss: 6939.363281\n",
      "Train Epoch: 86 [135424/225000 (60%)] Loss: 6922.296875\n",
      "Train Epoch: 86 [139520/225000 (62%)] Loss: 6916.470703\n",
      "Train Epoch: 86 [143616/225000 (64%)] Loss: 6995.279297\n",
      "Train Epoch: 86 [147712/225000 (66%)] Loss: 6784.029297\n",
      "Train Epoch: 86 [151808/225000 (67%)] Loss: 6986.207031\n",
      "Train Epoch: 86 [155904/225000 (69%)] Loss: 7007.349609\n",
      "Train Epoch: 86 [160000/225000 (71%)] Loss: 8874.376953\n",
      "Train Epoch: 86 [164096/225000 (73%)] Loss: 6824.292969\n",
      "Train Epoch: 86 [168192/225000 (75%)] Loss: 6820.041016\n",
      "Train Epoch: 86 [172288/225000 (77%)] Loss: 6826.751953\n",
      "Train Epoch: 86 [176384/225000 (78%)] Loss: 6872.367188\n",
      "Train Epoch: 86 [180480/225000 (80%)] Loss: 6945.595703\n",
      "Train Epoch: 86 [184576/225000 (82%)] Loss: 6931.664062\n",
      "Train Epoch: 86 [188672/225000 (84%)] Loss: 6903.486328\n",
      "Train Epoch: 86 [192768/225000 (86%)] Loss: 6945.812500\n",
      "Train Epoch: 86 [196864/225000 (87%)] Loss: 6864.445312\n",
      "Train Epoch: 86 [200960/225000 (89%)] Loss: 6941.843750\n",
      "Train Epoch: 86 [205056/225000 (91%)] Loss: 6863.179688\n",
      "Train Epoch: 86 [209152/225000 (93%)] Loss: 6981.878906\n",
      "Train Epoch: 86 [213248/225000 (95%)] Loss: 6814.308594\n",
      "Train Epoch: 86 [217344/225000 (97%)] Loss: 6827.675781\n",
      "Train Epoch: 86 [221440/225000 (98%)] Loss: 6812.376953\n",
      "    epoch          : 86\n",
      "    loss           : 7054.208198903229\n",
      "    val_loss       : 6892.649656172918\n",
      "Train Epoch: 87 [256/225000 (0%)] Loss: 6935.511719\n",
      "Train Epoch: 87 [4352/225000 (2%)] Loss: 7027.707031\n",
      "Train Epoch: 87 [8448/225000 (4%)] Loss: 8706.250000\n",
      "Train Epoch: 87 [12544/225000 (6%)] Loss: 6889.023438\n",
      "Train Epoch: 87 [16640/225000 (7%)] Loss: 6792.769531\n",
      "Train Epoch: 87 [20736/225000 (9%)] Loss: 6864.808594\n",
      "Train Epoch: 87 [24832/225000 (11%)] Loss: 6927.169922\n",
      "Train Epoch: 87 [28928/225000 (13%)] Loss: 6853.949219\n",
      "Train Epoch: 87 [33024/225000 (15%)] Loss: 6651.771484\n",
      "Train Epoch: 87 [37120/225000 (16%)] Loss: 6826.425781\n",
      "Train Epoch: 87 [41216/225000 (18%)] Loss: 6858.843750\n",
      "Train Epoch: 87 [45312/225000 (20%)] Loss: 6891.031250\n",
      "Train Epoch: 87 [49408/225000 (22%)] Loss: 6739.558594\n",
      "Train Epoch: 87 [53504/225000 (24%)] Loss: 6835.273438\n",
      "Train Epoch: 87 [57600/225000 (26%)] Loss: 6855.625000\n",
      "Train Epoch: 87 [61696/225000 (27%)] Loss: 6857.080078\n",
      "Train Epoch: 87 [65792/225000 (29%)] Loss: 6723.599609\n",
      "Train Epoch: 87 [69888/225000 (31%)] Loss: 6919.599609\n",
      "Train Epoch: 87 [73984/225000 (33%)] Loss: 6883.294922\n",
      "Train Epoch: 87 [78080/225000 (35%)] Loss: 6750.951172\n",
      "Train Epoch: 87 [82176/225000 (37%)] Loss: 6973.740234\n",
      "Train Epoch: 87 [86272/225000 (38%)] Loss: 6781.652344\n",
      "Train Epoch: 87 [90368/225000 (40%)] Loss: 6866.759766\n",
      "Train Epoch: 87 [94464/225000 (42%)] Loss: 6811.019531\n",
      "Train Epoch: 87 [98560/225000 (44%)] Loss: 6753.912109\n",
      "Train Epoch: 87 [102656/225000 (46%)] Loss: 6829.916016\n",
      "Train Epoch: 87 [106752/225000 (47%)] Loss: 6786.226562\n",
      "Train Epoch: 87 [110848/225000 (49%)] Loss: 6772.128906\n",
      "Train Epoch: 87 [114944/225000 (51%)] Loss: 6987.507812\n",
      "Train Epoch: 87 [119040/225000 (53%)] Loss: 6825.417969\n",
      "Train Epoch: 87 [123136/225000 (55%)] Loss: 6988.480469\n",
      "Train Epoch: 87 [127232/225000 (57%)] Loss: 6853.207031\n",
      "Train Epoch: 87 [131328/225000 (58%)] Loss: 6795.400391\n",
      "Train Epoch: 87 [135424/225000 (60%)] Loss: 6702.359375\n",
      "Train Epoch: 87 [139520/225000 (62%)] Loss: 6880.636719\n",
      "Train Epoch: 87 [143616/225000 (64%)] Loss: 6985.197266\n",
      "Train Epoch: 87 [147712/225000 (66%)] Loss: 6917.603516\n",
      "Train Epoch: 87 [151808/225000 (67%)] Loss: 6857.240234\n",
      "Train Epoch: 87 [155904/225000 (69%)] Loss: 6782.679688\n",
      "Train Epoch: 87 [160000/225000 (71%)] Loss: 6854.939453\n",
      "Train Epoch: 87 [164096/225000 (73%)] Loss: 6747.398438\n",
      "Train Epoch: 87 [168192/225000 (75%)] Loss: 6923.181641\n",
      "Train Epoch: 87 [172288/225000 (77%)] Loss: 6910.529297\n",
      "Train Epoch: 87 [176384/225000 (78%)] Loss: 6910.214844\n",
      "Train Epoch: 87 [180480/225000 (80%)] Loss: 7086.890625\n",
      "Train Epoch: 87 [184576/225000 (82%)] Loss: 6751.310547\n",
      "Train Epoch: 87 [188672/225000 (84%)] Loss: 6736.896484\n",
      "Train Epoch: 87 [192768/225000 (86%)] Loss: 6746.560547\n",
      "Train Epoch: 87 [196864/225000 (87%)] Loss: 6899.912109\n",
      "Train Epoch: 87 [200960/225000 (89%)] Loss: 6991.160156\n",
      "Train Epoch: 87 [205056/225000 (91%)] Loss: 6781.269531\n",
      "Train Epoch: 87 [209152/225000 (93%)] Loss: 6747.281250\n",
      "Train Epoch: 87 [213248/225000 (95%)] Loss: 7026.748047\n",
      "Train Epoch: 87 [217344/225000 (97%)] Loss: 6773.087891\n",
      "Train Epoch: 87 [221440/225000 (98%)] Loss: 6792.398438\n",
      "    epoch          : 87\n",
      "    loss           : 6965.437916622227\n",
      "    val_loss       : 6903.067000318547\n",
      "Train Epoch: 88 [256/225000 (0%)] Loss: 6917.855469\n",
      "Train Epoch: 88 [4352/225000 (2%)] Loss: 6758.480469\n",
      "Train Epoch: 88 [8448/225000 (4%)] Loss: 14103.763672\n",
      "Train Epoch: 88 [12544/225000 (6%)] Loss: 6777.570312\n",
      "Train Epoch: 88 [16640/225000 (7%)] Loss: 6858.763672\n",
      "Train Epoch: 88 [20736/225000 (9%)] Loss: 6838.857422\n",
      "Train Epoch: 88 [24832/225000 (11%)] Loss: 7009.816406\n",
      "Train Epoch: 88 [28928/225000 (13%)] Loss: 6760.560547\n",
      "Train Epoch: 88 [33024/225000 (15%)] Loss: 6848.873047\n",
      "Train Epoch: 88 [37120/225000 (16%)] Loss: 6915.646484\n",
      "Train Epoch: 88 [41216/225000 (18%)] Loss: 6885.712891\n",
      "Train Epoch: 88 [45312/225000 (20%)] Loss: 6763.820312\n",
      "Train Epoch: 88 [49408/225000 (22%)] Loss: 6840.691406\n",
      "Train Epoch: 88 [53504/225000 (24%)] Loss: 6930.738281\n",
      "Train Epoch: 88 [57600/225000 (26%)] Loss: 7026.244141\n",
      "Train Epoch: 88 [61696/225000 (27%)] Loss: 6882.308594\n",
      "Train Epoch: 88 [65792/225000 (29%)] Loss: 6994.638672\n",
      "Train Epoch: 88 [69888/225000 (31%)] Loss: 6948.576172\n",
      "Train Epoch: 88 [73984/225000 (33%)] Loss: 6891.330078\n",
      "Train Epoch: 88 [78080/225000 (35%)] Loss: 6873.833984\n",
      "Train Epoch: 88 [82176/225000 (37%)] Loss: 6862.123047\n",
      "Train Epoch: 88 [86272/225000 (38%)] Loss: 6987.849609\n",
      "Train Epoch: 88 [90368/225000 (40%)] Loss: 6876.539062\n",
      "Train Epoch: 88 [94464/225000 (42%)] Loss: 6871.445312\n",
      "Train Epoch: 88 [98560/225000 (44%)] Loss: 6824.697266\n",
      "Train Epoch: 88 [102656/225000 (46%)] Loss: 6811.765625\n",
      "Train Epoch: 88 [106752/225000 (47%)] Loss: 6922.289062\n",
      "Train Epoch: 88 [110848/225000 (49%)] Loss: 6829.435547\n",
      "Train Epoch: 88 [114944/225000 (51%)] Loss: 6780.523438\n",
      "Train Epoch: 88 [119040/225000 (53%)] Loss: 6802.394531\n",
      "Train Epoch: 88 [123136/225000 (55%)] Loss: 6965.593750\n",
      "Train Epoch: 88 [127232/225000 (57%)] Loss: 6931.785156\n",
      "Train Epoch: 88 [131328/225000 (58%)] Loss: 7030.785156\n",
      "Train Epoch: 88 [135424/225000 (60%)] Loss: 6782.986328\n",
      "Train Epoch: 88 [139520/225000 (62%)] Loss: 6767.537109\n",
      "Train Epoch: 88 [143616/225000 (64%)] Loss: 6848.029297\n",
      "Train Epoch: 88 [147712/225000 (66%)] Loss: 6864.765625\n",
      "Train Epoch: 88 [151808/225000 (67%)] Loss: 6994.171875\n",
      "Train Epoch: 88 [155904/225000 (69%)] Loss: 6815.216797\n",
      "Train Epoch: 88 [160000/225000 (71%)] Loss: 6980.859375\n",
      "Train Epoch: 88 [164096/225000 (73%)] Loss: 6924.537109\n",
      "Train Epoch: 88 [168192/225000 (75%)] Loss: 6878.025391\n",
      "Train Epoch: 88 [172288/225000 (77%)] Loss: 6893.056641\n",
      "Train Epoch: 88 [176384/225000 (78%)] Loss: 6879.437500\n",
      "Train Epoch: 88 [180480/225000 (80%)] Loss: 6819.728516\n",
      "Train Epoch: 88 [184576/225000 (82%)] Loss: 6897.779297\n",
      "Train Epoch: 88 [188672/225000 (84%)] Loss: 6922.439453\n",
      "Train Epoch: 88 [192768/225000 (86%)] Loss: 6975.851562\n",
      "Train Epoch: 88 [196864/225000 (87%)] Loss: 6996.289062\n",
      "Train Epoch: 88 [200960/225000 (89%)] Loss: 6912.515625\n",
      "Train Epoch: 88 [205056/225000 (91%)] Loss: 6749.533203\n",
      "Train Epoch: 88 [209152/225000 (93%)] Loss: 6880.484375\n",
      "Train Epoch: 88 [213248/225000 (95%)] Loss: 6799.634766\n",
      "Train Epoch: 88 [217344/225000 (97%)] Loss: 6775.990234\n",
      "Train Epoch: 88 [221440/225000 (98%)] Loss: 6620.548828\n",
      "    epoch          : 88\n",
      "    loss           : 6944.841393584685\n",
      "    val_loss       : 7155.525806024974\n",
      "Train Epoch: 89 [256/225000 (0%)] Loss: 6860.562500\n",
      "Train Epoch: 89 [4352/225000 (2%)] Loss: 6779.693359\n",
      "Train Epoch: 89 [8448/225000 (4%)] Loss: 6927.042969\n",
      "Train Epoch: 89 [12544/225000 (6%)] Loss: 6777.873047\n",
      "Train Epoch: 89 [16640/225000 (7%)] Loss: 6909.312500\n",
      "Train Epoch: 89 [20736/225000 (9%)] Loss: 6928.218750\n",
      "Train Epoch: 89 [24832/225000 (11%)] Loss: 6949.292969\n",
      "Train Epoch: 89 [28928/225000 (13%)] Loss: 6918.001953\n",
      "Train Epoch: 89 [33024/225000 (15%)] Loss: 6981.576172\n",
      "Train Epoch: 89 [37120/225000 (16%)] Loss: 7026.632812\n",
      "Train Epoch: 89 [41216/225000 (18%)] Loss: 6907.947266\n",
      "Train Epoch: 89 [45312/225000 (20%)] Loss: 6798.929688\n",
      "Train Epoch: 89 [49408/225000 (22%)] Loss: 6940.609375\n",
      "Train Epoch: 89 [53504/225000 (24%)] Loss: 6817.441406\n",
      "Train Epoch: 89 [57600/225000 (26%)] Loss: 7032.808594\n",
      "Train Epoch: 89 [61696/225000 (27%)] Loss: 6856.185547\n",
      "Train Epoch: 89 [65792/225000 (29%)] Loss: 6931.566406\n",
      "Train Epoch: 89 [69888/225000 (31%)] Loss: 6845.212891\n",
      "Train Epoch: 89 [73984/225000 (33%)] Loss: 6741.531250\n",
      "Train Epoch: 89 [78080/225000 (35%)] Loss: 6949.361328\n",
      "Train Epoch: 89 [82176/225000 (37%)] Loss: 6783.322266\n",
      "Train Epoch: 89 [86272/225000 (38%)] Loss: 6799.546875\n",
      "Train Epoch: 89 [90368/225000 (40%)] Loss: 6709.318359\n",
      "Train Epoch: 89 [94464/225000 (42%)] Loss: 6957.355469\n",
      "Train Epoch: 89 [98560/225000 (44%)] Loss: 6837.634766\n",
      "Train Epoch: 89 [102656/225000 (46%)] Loss: 6850.488281\n",
      "Train Epoch: 89 [106752/225000 (47%)] Loss: 6907.041016\n",
      "Train Epoch: 89 [110848/225000 (49%)] Loss: 6782.347656\n",
      "Train Epoch: 89 [114944/225000 (51%)] Loss: 6747.878906\n",
      "Train Epoch: 89 [119040/225000 (53%)] Loss: 6856.205078\n",
      "Train Epoch: 89 [123136/225000 (55%)] Loss: 6820.048828\n",
      "Train Epoch: 89 [127232/225000 (57%)] Loss: 6923.107422\n",
      "Train Epoch: 89 [131328/225000 (58%)] Loss: 6841.035156\n",
      "Train Epoch: 89 [135424/225000 (60%)] Loss: 6866.699219\n",
      "Train Epoch: 89 [139520/225000 (62%)] Loss: 6957.177734\n",
      "Train Epoch: 89 [143616/225000 (64%)] Loss: 6987.394531\n",
      "Train Epoch: 89 [147712/225000 (66%)] Loss: 6945.988281\n",
      "Train Epoch: 89 [151808/225000 (67%)] Loss: 6977.013672\n",
      "Train Epoch: 89 [155904/225000 (69%)] Loss: 6888.513672\n",
      "Train Epoch: 89 [160000/225000 (71%)] Loss: 6961.761719\n",
      "Train Epoch: 89 [164096/225000 (73%)] Loss: 6831.173828\n",
      "Train Epoch: 89 [168192/225000 (75%)] Loss: 6867.634766\n",
      "Train Epoch: 89 [172288/225000 (77%)] Loss: 6962.849609\n",
      "Train Epoch: 89 [176384/225000 (78%)] Loss: 6905.078125\n",
      "Train Epoch: 89 [180480/225000 (80%)] Loss: 6655.875000\n",
      "Train Epoch: 89 [184576/225000 (82%)] Loss: 14120.259766\n",
      "Train Epoch: 89 [188672/225000 (84%)] Loss: 6955.998047\n",
      "Train Epoch: 89 [192768/225000 (86%)] Loss: 6845.533203\n",
      "Train Epoch: 89 [196864/225000 (87%)] Loss: 6849.617188\n",
      "Train Epoch: 89 [200960/225000 (89%)] Loss: 7099.140625\n",
      "Train Epoch: 89 [205056/225000 (91%)] Loss: 6919.449219\n",
      "Train Epoch: 89 [209152/225000 (93%)] Loss: 6834.498047\n",
      "Train Epoch: 89 [213248/225000 (95%)] Loss: 6876.875000\n",
      "Train Epoch: 89 [217344/225000 (97%)] Loss: 6976.080078\n",
      "Train Epoch: 89 [221440/225000 (98%)] Loss: 6862.996094\n",
      "    epoch          : 89\n",
      "    loss           : 6891.23558598194\n",
      "    val_loss       : 6866.232330597177\n",
      "Train Epoch: 90 [256/225000 (0%)] Loss: 6833.525391\n",
      "Train Epoch: 90 [4352/225000 (2%)] Loss: 6856.492188\n",
      "Train Epoch: 90 [8448/225000 (4%)] Loss: 6669.800781\n",
      "Train Epoch: 90 [12544/225000 (6%)] Loss: 6779.773438\n",
      "Train Epoch: 90 [16640/225000 (7%)] Loss: 6866.890625\n",
      "Train Epoch: 90 [20736/225000 (9%)] Loss: 6957.070312\n",
      "Train Epoch: 90 [24832/225000 (11%)] Loss: 6764.826172\n",
      "Train Epoch: 90 [28928/225000 (13%)] Loss: 6848.921875\n",
      "Train Epoch: 90 [33024/225000 (15%)] Loss: 28015.201172\n",
      "Train Epoch: 90 [37120/225000 (16%)] Loss: 6805.503906\n",
      "Train Epoch: 90 [41216/225000 (18%)] Loss: 6890.517578\n",
      "Train Epoch: 90 [45312/225000 (20%)] Loss: 6764.677734\n",
      "Train Epoch: 90 [49408/225000 (22%)] Loss: 7080.386719\n",
      "Train Epoch: 90 [53504/225000 (24%)] Loss: 6672.605469\n",
      "Train Epoch: 90 [57600/225000 (26%)] Loss: 6964.708984\n",
      "Train Epoch: 90 [61696/225000 (27%)] Loss: 7122.435547\n",
      "Train Epoch: 90 [65792/225000 (29%)] Loss: 6815.136719\n",
      "Train Epoch: 90 [69888/225000 (31%)] Loss: 6870.875000\n",
      "Train Epoch: 90 [73984/225000 (33%)] Loss: 6911.775391\n",
      "Train Epoch: 90 [78080/225000 (35%)] Loss: 6817.529297\n",
      "Train Epoch: 90 [82176/225000 (37%)] Loss: 7138.490234\n",
      "Train Epoch: 90 [86272/225000 (38%)] Loss: 6826.769531\n",
      "Train Epoch: 90 [90368/225000 (40%)] Loss: 6858.146484\n",
      "Train Epoch: 90 [94464/225000 (42%)] Loss: 6739.917969\n",
      "Train Epoch: 90 [98560/225000 (44%)] Loss: 6679.281250\n",
      "Train Epoch: 90 [102656/225000 (46%)] Loss: 7014.835938\n",
      "Train Epoch: 90 [106752/225000 (47%)] Loss: 6899.332031\n",
      "Train Epoch: 90 [110848/225000 (49%)] Loss: 6963.939453\n",
      "Train Epoch: 90 [114944/225000 (51%)] Loss: 6936.726562\n",
      "Train Epoch: 90 [119040/225000 (53%)] Loss: 6794.070312\n",
      "Train Epoch: 90 [123136/225000 (55%)] Loss: 6845.685547\n",
      "Train Epoch: 90 [127232/225000 (57%)] Loss: 6846.582031\n",
      "Train Epoch: 90 [131328/225000 (58%)] Loss: 6961.093750\n",
      "Train Epoch: 90 [135424/225000 (60%)] Loss: 6776.919922\n",
      "Train Epoch: 90 [139520/225000 (62%)] Loss: 6955.218750\n",
      "Train Epoch: 90 [143616/225000 (64%)] Loss: 6876.255859\n",
      "Train Epoch: 90 [147712/225000 (66%)] Loss: 6786.044922\n",
      "Train Epoch: 90 [151808/225000 (67%)] Loss: 6881.660156\n",
      "Train Epoch: 90 [155904/225000 (69%)] Loss: 6864.082031\n",
      "Train Epoch: 90 [160000/225000 (71%)] Loss: 6925.367188\n",
      "Train Epoch: 90 [164096/225000 (73%)] Loss: 6876.068359\n",
      "Train Epoch: 90 [168192/225000 (75%)] Loss: 6860.775391\n",
      "Train Epoch: 90 [172288/225000 (77%)] Loss: 6967.486328\n",
      "Train Epoch: 90 [176384/225000 (78%)] Loss: 6767.777344\n",
      "Train Epoch: 90 [180480/225000 (80%)] Loss: 6918.162109\n",
      "Train Epoch: 90 [184576/225000 (82%)] Loss: 6758.039062\n",
      "Train Epoch: 90 [188672/225000 (84%)] Loss: 6812.677734\n",
      "Train Epoch: 90 [192768/225000 (86%)] Loss: 6923.587891\n",
      "Train Epoch: 90 [196864/225000 (87%)] Loss: 6992.066406\n",
      "Train Epoch: 90 [200960/225000 (89%)] Loss: 6706.873047\n",
      "Train Epoch: 90 [205056/225000 (91%)] Loss: 6997.132812\n",
      "Train Epoch: 90 [209152/225000 (93%)] Loss: 6818.703125\n",
      "Train Epoch: 90 [213248/225000 (95%)] Loss: 6783.083984\n",
      "Train Epoch: 90 [217344/225000 (97%)] Loss: 6834.025391\n",
      "Train Epoch: 90 [221440/225000 (98%)] Loss: 6977.847656\n",
      "    epoch          : 90\n",
      "    loss           : 6944.248633479096\n",
      "    val_loss       : 6891.8234069760965\n",
      "Train Epoch: 91 [256/225000 (0%)] Loss: 6732.855469\n",
      "Train Epoch: 91 [4352/225000 (2%)] Loss: 6716.835938\n",
      "Train Epoch: 91 [8448/225000 (4%)] Loss: 6863.642578\n",
      "Train Epoch: 91 [12544/225000 (6%)] Loss: 6837.226562\n",
      "Train Epoch: 91 [16640/225000 (7%)] Loss: 6868.095703\n",
      "Train Epoch: 91 [20736/225000 (9%)] Loss: 6700.974609\n",
      "Train Epoch: 91 [24832/225000 (11%)] Loss: 6743.154297\n",
      "Train Epoch: 91 [28928/225000 (13%)] Loss: 6881.207031\n",
      "Train Epoch: 91 [33024/225000 (15%)] Loss: 6912.347656\n",
      "Train Epoch: 91 [37120/225000 (16%)] Loss: 6974.033203\n",
      "Train Epoch: 91 [41216/225000 (18%)] Loss: 6781.169922\n",
      "Train Epoch: 91 [45312/225000 (20%)] Loss: 6876.865234\n",
      "Train Epoch: 91 [49408/225000 (22%)] Loss: 6908.572266\n",
      "Train Epoch: 91 [53504/225000 (24%)] Loss: 6823.068359\n",
      "Train Epoch: 91 [57600/225000 (26%)] Loss: 6746.824219\n",
      "Train Epoch: 91 [61696/225000 (27%)] Loss: 6831.925781\n",
      "Train Epoch: 91 [65792/225000 (29%)] Loss: 6922.599609\n",
      "Train Epoch: 91 [69888/225000 (31%)] Loss: 6876.736328\n",
      "Train Epoch: 91 [73984/225000 (33%)] Loss: 6807.103516\n",
      "Train Epoch: 91 [78080/225000 (35%)] Loss: 6964.455078\n",
      "Train Epoch: 91 [82176/225000 (37%)] Loss: 6962.337891\n",
      "Train Epoch: 91 [86272/225000 (38%)] Loss: 6908.062500\n",
      "Train Epoch: 91 [90368/225000 (40%)] Loss: 6869.126953\n",
      "Train Epoch: 91 [94464/225000 (42%)] Loss: 6904.246094\n",
      "Train Epoch: 91 [98560/225000 (44%)] Loss: 6741.041016\n",
      "Train Epoch: 91 [102656/225000 (46%)] Loss: 6732.535156\n",
      "Train Epoch: 91 [106752/225000 (47%)] Loss: 6981.263672\n",
      "Train Epoch: 91 [110848/225000 (49%)] Loss: 6744.693359\n",
      "Train Epoch: 91 [114944/225000 (51%)] Loss: 6878.798828\n",
      "Train Epoch: 91 [119040/225000 (53%)] Loss: 6673.935547\n",
      "Train Epoch: 91 [123136/225000 (55%)] Loss: 6647.224609\n",
      "Train Epoch: 91 [127232/225000 (57%)] Loss: 6686.943359\n",
      "Train Epoch: 91 [131328/225000 (58%)] Loss: 6928.623047\n",
      "Train Epoch: 91 [135424/225000 (60%)] Loss: 6778.046875\n",
      "Train Epoch: 91 [139520/225000 (62%)] Loss: 6880.087891\n",
      "Train Epoch: 91 [143616/225000 (64%)] Loss: 6946.779297\n",
      "Train Epoch: 91 [147712/225000 (66%)] Loss: 6945.759766\n",
      "Train Epoch: 91 [151808/225000 (67%)] Loss: 6788.476562\n",
      "Train Epoch: 91 [155904/225000 (69%)] Loss: 6824.703125\n",
      "Train Epoch: 91 [160000/225000 (71%)] Loss: 6747.535156\n",
      "Train Epoch: 91 [164096/225000 (73%)] Loss: 6865.820312\n",
      "Train Epoch: 91 [168192/225000 (75%)] Loss: 6776.070312\n",
      "Train Epoch: 91 [172288/225000 (77%)] Loss: 6706.167969\n",
      "Train Epoch: 91 [176384/225000 (78%)] Loss: 8708.185547\n",
      "Train Epoch: 91 [180480/225000 (80%)] Loss: 6833.908203\n",
      "Train Epoch: 91 [184576/225000 (82%)] Loss: 6902.806641\n",
      "Train Epoch: 91 [188672/225000 (84%)] Loss: 6773.669922\n",
      "Train Epoch: 91 [192768/225000 (86%)] Loss: 6806.527344\n",
      "Train Epoch: 91 [196864/225000 (87%)] Loss: 6933.236328\n",
      "Train Epoch: 91 [200960/225000 (89%)] Loss: 6685.632812\n",
      "Train Epoch: 91 [205056/225000 (91%)] Loss: 6829.046875\n",
      "Train Epoch: 91 [209152/225000 (93%)] Loss: 6676.628906\n",
      "Train Epoch: 91 [213248/225000 (95%)] Loss: 6862.560547\n",
      "Train Epoch: 91 [217344/225000 (97%)] Loss: 6864.773438\n",
      "Train Epoch: 91 [221440/225000 (98%)] Loss: 6695.000000\n",
      "    epoch          : 91\n",
      "    loss           : 6979.889164044725\n",
      "    val_loss       : 6860.8446667230855\n",
      "Train Epoch: 92 [256/225000 (0%)] Loss: 6832.527344\n",
      "Train Epoch: 92 [4352/225000 (2%)] Loss: 7041.210938\n",
      "Train Epoch: 92 [8448/225000 (4%)] Loss: 6728.705078\n",
      "Train Epoch: 92 [12544/225000 (6%)] Loss: 6755.500000\n",
      "Train Epoch: 92 [16640/225000 (7%)] Loss: 6772.599609\n",
      "Train Epoch: 92 [20736/225000 (9%)] Loss: 6985.009766\n",
      "Train Epoch: 92 [24832/225000 (11%)] Loss: 6808.009766\n",
      "Train Epoch: 92 [28928/225000 (13%)] Loss: 6791.853516\n",
      "Train Epoch: 92 [33024/225000 (15%)] Loss: 6828.365234\n",
      "Train Epoch: 92 [37120/225000 (16%)] Loss: 6694.931641\n",
      "Train Epoch: 92 [41216/225000 (18%)] Loss: 7003.082031\n",
      "Train Epoch: 92 [45312/225000 (20%)] Loss: 6876.843750\n",
      "Train Epoch: 92 [49408/225000 (22%)] Loss: 6726.375000\n",
      "Train Epoch: 92 [53504/225000 (24%)] Loss: 6749.699219\n",
      "Train Epoch: 92 [57600/225000 (26%)] Loss: 6800.339844\n",
      "Train Epoch: 92 [61696/225000 (27%)] Loss: 6931.445312\n",
      "Train Epoch: 92 [65792/225000 (29%)] Loss: 6707.353516\n",
      "Train Epoch: 92 [69888/225000 (31%)] Loss: 6660.830078\n",
      "Train Epoch: 92 [73984/225000 (33%)] Loss: 6888.720703\n",
      "Train Epoch: 92 [78080/225000 (35%)] Loss: 6891.548828\n",
      "Train Epoch: 92 [82176/225000 (37%)] Loss: 6723.996094\n",
      "Train Epoch: 92 [86272/225000 (38%)] Loss: 6781.931641\n",
      "Train Epoch: 92 [90368/225000 (40%)] Loss: 6733.152344\n",
      "Train Epoch: 92 [94464/225000 (42%)] Loss: 6964.005859\n",
      "Train Epoch: 92 [98560/225000 (44%)] Loss: 6807.955078\n",
      "Train Epoch: 92 [102656/225000 (46%)] Loss: 6741.181641\n",
      "Train Epoch: 92 [106752/225000 (47%)] Loss: 6812.763672\n",
      "Train Epoch: 92 [110848/225000 (49%)] Loss: 6789.556641\n",
      "Train Epoch: 92 [114944/225000 (51%)] Loss: 6741.564453\n",
      "Train Epoch: 92 [119040/225000 (53%)] Loss: 6891.056641\n",
      "Train Epoch: 92 [123136/225000 (55%)] Loss: 6744.750000\n",
      "Train Epoch: 92 [127232/225000 (57%)] Loss: 6873.095703\n",
      "Train Epoch: 92 [131328/225000 (58%)] Loss: 6951.550781\n",
      "Train Epoch: 92 [135424/225000 (60%)] Loss: 6876.634766\n",
      "Train Epoch: 92 [139520/225000 (62%)] Loss: 6838.662109\n",
      "Train Epoch: 92 [143616/225000 (64%)] Loss: 6889.894531\n",
      "Train Epoch: 92 [147712/225000 (66%)] Loss: 6820.863281\n",
      "Train Epoch: 92 [151808/225000 (67%)] Loss: 6705.628906\n",
      "Train Epoch: 92 [155904/225000 (69%)] Loss: 6759.054688\n",
      "Train Epoch: 92 [160000/225000 (71%)] Loss: 7117.912109\n",
      "Train Epoch: 92 [164096/225000 (73%)] Loss: 6742.113281\n",
      "Train Epoch: 92 [168192/225000 (75%)] Loss: 6901.427734\n",
      "Train Epoch: 92 [172288/225000 (77%)] Loss: 6691.041016\n",
      "Train Epoch: 92 [176384/225000 (78%)] Loss: 6836.013672\n",
      "Train Epoch: 92 [180480/225000 (80%)] Loss: 6906.513672\n",
      "Train Epoch: 92 [184576/225000 (82%)] Loss: 6960.998047\n",
      "Train Epoch: 92 [188672/225000 (84%)] Loss: 6902.238281\n",
      "Train Epoch: 92 [192768/225000 (86%)] Loss: 6969.015625\n",
      "Train Epoch: 92 [196864/225000 (87%)] Loss: 6918.896484\n",
      "Train Epoch: 92 [200960/225000 (89%)] Loss: 6889.808594\n",
      "Train Epoch: 92 [205056/225000 (91%)] Loss: 6818.697266\n",
      "Train Epoch: 92 [209152/225000 (93%)] Loss: 6894.218750\n",
      "Train Epoch: 92 [213248/225000 (95%)] Loss: 6736.025391\n",
      "Train Epoch: 92 [217344/225000 (97%)] Loss: 6731.841797\n",
      "Train Epoch: 92 [221440/225000 (98%)] Loss: 7077.029297\n",
      "    epoch          : 92\n",
      "    loss           : 6863.939047612699\n",
      "    val_loss       : 6858.087638289345\n",
      "Train Epoch: 93 [256/225000 (0%)] Loss: 6891.171875\n",
      "Train Epoch: 93 [4352/225000 (2%)] Loss: 6836.589844\n",
      "Train Epoch: 93 [8448/225000 (4%)] Loss: 7046.341797\n",
      "Train Epoch: 93 [12544/225000 (6%)] Loss: 6810.060547\n",
      "Train Epoch: 93 [16640/225000 (7%)] Loss: 6768.255859\n",
      "Train Epoch: 93 [20736/225000 (9%)] Loss: 6764.505859\n",
      "Train Epoch: 93 [24832/225000 (11%)] Loss: 6912.402344\n",
      "Train Epoch: 93 [28928/225000 (13%)] Loss: 6896.517578\n",
      "Train Epoch: 93 [33024/225000 (15%)] Loss: 6768.496094\n",
      "Train Epoch: 93 [37120/225000 (16%)] Loss: 6767.654297\n",
      "Train Epoch: 93 [41216/225000 (18%)] Loss: 6745.970703\n",
      "Train Epoch: 93 [45312/225000 (20%)] Loss: 6917.437500\n",
      "Train Epoch: 93 [49408/225000 (22%)] Loss: 6889.921875\n",
      "Train Epoch: 93 [53504/225000 (24%)] Loss: 6864.353516\n",
      "Train Epoch: 93 [57600/225000 (26%)] Loss: 6834.113281\n",
      "Train Epoch: 93 [61696/225000 (27%)] Loss: 6974.759766\n",
      "Train Epoch: 93 [65792/225000 (29%)] Loss: 6778.984375\n",
      "Train Epoch: 93 [69888/225000 (31%)] Loss: 6716.287109\n",
      "Train Epoch: 93 [73984/225000 (33%)] Loss: 6991.941406\n",
      "Train Epoch: 93 [78080/225000 (35%)] Loss: 6797.583984\n",
      "Train Epoch: 93 [82176/225000 (37%)] Loss: 6736.167969\n",
      "Train Epoch: 93 [86272/225000 (38%)] Loss: 6809.423828\n",
      "Train Epoch: 93 [90368/225000 (40%)] Loss: 6861.679688\n",
      "Train Epoch: 93 [94464/225000 (42%)] Loss: 6817.845703\n",
      "Train Epoch: 93 [98560/225000 (44%)] Loss: 6954.363281\n",
      "Train Epoch: 93 [102656/225000 (46%)] Loss: 6668.531250\n",
      "Train Epoch: 93 [106752/225000 (47%)] Loss: 7049.214844\n",
      "Train Epoch: 93 [110848/225000 (49%)] Loss: 6861.755859\n",
      "Train Epoch: 93 [114944/225000 (51%)] Loss: 6943.701172\n",
      "Train Epoch: 93 [119040/225000 (53%)] Loss: 6851.757812\n",
      "Train Epoch: 93 [123136/225000 (55%)] Loss: 6733.591797\n",
      "Train Epoch: 93 [127232/225000 (57%)] Loss: 6882.515625\n",
      "Train Epoch: 93 [131328/225000 (58%)] Loss: 6758.640625\n",
      "Train Epoch: 93 [135424/225000 (60%)] Loss: 13759.279297\n",
      "Train Epoch: 93 [139520/225000 (62%)] Loss: 6858.861328\n",
      "Train Epoch: 93 [143616/225000 (64%)] Loss: 6813.867188\n",
      "Train Epoch: 93 [147712/225000 (66%)] Loss: 6773.722656\n",
      "Train Epoch: 93 [151808/225000 (67%)] Loss: 6714.896484\n",
      "Train Epoch: 93 [155904/225000 (69%)] Loss: 6900.259766\n",
      "Train Epoch: 93 [160000/225000 (71%)] Loss: 6865.349609\n",
      "Train Epoch: 93 [164096/225000 (73%)] Loss: 6762.945312\n",
      "Train Epoch: 93 [168192/225000 (75%)] Loss: 6943.400391\n",
      "Train Epoch: 93 [172288/225000 (77%)] Loss: 6669.476562\n",
      "Train Epoch: 93 [176384/225000 (78%)] Loss: 6905.650391\n",
      "Train Epoch: 93 [180480/225000 (80%)] Loss: 6956.230469\n",
      "Train Epoch: 93 [184576/225000 (82%)] Loss: 6834.320312\n",
      "Train Epoch: 93 [188672/225000 (84%)] Loss: 6659.363281\n",
      "Train Epoch: 93 [192768/225000 (86%)] Loss: 6911.960938\n",
      "Train Epoch: 93 [196864/225000 (87%)] Loss: 6858.054688\n",
      "Train Epoch: 93 [200960/225000 (89%)] Loss: 6755.917969\n",
      "Train Epoch: 93 [205056/225000 (91%)] Loss: 6738.890625\n",
      "Train Epoch: 93 [209152/225000 (93%)] Loss: 6799.119141\n",
      "Train Epoch: 93 [213248/225000 (95%)] Loss: 6784.976562\n",
      "Train Epoch: 93 [217344/225000 (97%)] Loss: 6880.871094\n",
      "Train Epoch: 93 [221440/225000 (98%)] Loss: 6845.906250\n",
      "    epoch          : 93\n",
      "    loss           : 6928.888416346701\n",
      "    val_loss       : 6881.5676781760185\n",
      "Train Epoch: 94 [256/225000 (0%)] Loss: 6830.033203\n",
      "Train Epoch: 94 [4352/225000 (2%)] Loss: 6864.691406\n",
      "Train Epoch: 94 [8448/225000 (4%)] Loss: 6752.808594\n",
      "Train Epoch: 94 [12544/225000 (6%)] Loss: 6816.724609\n",
      "Train Epoch: 94 [16640/225000 (7%)] Loss: 6696.132812\n",
      "Train Epoch: 94 [20736/225000 (9%)] Loss: 6704.111328\n",
      "Train Epoch: 94 [24832/225000 (11%)] Loss: 6756.201172\n",
      "Train Epoch: 94 [28928/225000 (13%)] Loss: 6854.794922\n",
      "Train Epoch: 94 [33024/225000 (15%)] Loss: 6925.183594\n",
      "Train Epoch: 94 [37120/225000 (16%)] Loss: 6852.339844\n",
      "Train Epoch: 94 [41216/225000 (18%)] Loss: 6830.945312\n",
      "Train Epoch: 94 [45312/225000 (20%)] Loss: 6773.011719\n",
      "Train Epoch: 94 [49408/225000 (22%)] Loss: 6806.537109\n",
      "Train Epoch: 94 [53504/225000 (24%)] Loss: 8692.734375\n",
      "Train Epoch: 94 [57600/225000 (26%)] Loss: 6759.865234\n",
      "Train Epoch: 94 [61696/225000 (27%)] Loss: 6699.212891\n",
      "Train Epoch: 94 [65792/225000 (29%)] Loss: 6887.800781\n",
      "Train Epoch: 94 [69888/225000 (31%)] Loss: 6815.058594\n",
      "Train Epoch: 94 [73984/225000 (33%)] Loss: 6768.888672\n",
      "Train Epoch: 94 [78080/225000 (35%)] Loss: 6899.089844\n",
      "Train Epoch: 94 [82176/225000 (37%)] Loss: 6805.318359\n",
      "Train Epoch: 94 [86272/225000 (38%)] Loss: 6755.482422\n",
      "Train Epoch: 94 [90368/225000 (40%)] Loss: 6952.923828\n",
      "Train Epoch: 94 [94464/225000 (42%)] Loss: 6835.277344\n",
      "Train Epoch: 94 [98560/225000 (44%)] Loss: 8489.250000\n",
      "Train Epoch: 94 [102656/225000 (46%)] Loss: 6638.453125\n",
      "Train Epoch: 94 [106752/225000 (47%)] Loss: 6773.783203\n",
      "Train Epoch: 94 [110848/225000 (49%)] Loss: 8528.128906\n",
      "Train Epoch: 94 [114944/225000 (51%)] Loss: 6696.720703\n",
      "Train Epoch: 94 [119040/225000 (53%)] Loss: 6757.082031\n",
      "Train Epoch: 94 [123136/225000 (55%)] Loss: 6883.843750\n",
      "Train Epoch: 94 [127232/225000 (57%)] Loss: 6890.400391\n",
      "Train Epoch: 94 [131328/225000 (58%)] Loss: 6945.902344\n",
      "Train Epoch: 94 [135424/225000 (60%)] Loss: 6785.689453\n",
      "Train Epoch: 94 [139520/225000 (62%)] Loss: 6860.132812\n",
      "Train Epoch: 94 [143616/225000 (64%)] Loss: 6755.310547\n",
      "Train Epoch: 94 [147712/225000 (66%)] Loss: 6712.957031\n",
      "Train Epoch: 94 [151808/225000 (67%)] Loss: 6955.960938\n",
      "Train Epoch: 94 [155904/225000 (69%)] Loss: 6805.162109\n",
      "Train Epoch: 94 [160000/225000 (71%)] Loss: 6892.605469\n",
      "Train Epoch: 94 [164096/225000 (73%)] Loss: 6858.703125\n",
      "Train Epoch: 94 [168192/225000 (75%)] Loss: 6889.718750\n",
      "Train Epoch: 94 [172288/225000 (77%)] Loss: 6854.230469\n",
      "Train Epoch: 94 [176384/225000 (78%)] Loss: 6626.031250\n",
      "Train Epoch: 94 [180480/225000 (80%)] Loss: 6888.585938\n",
      "Train Epoch: 94 [184576/225000 (82%)] Loss: 6869.992188\n",
      "Train Epoch: 94 [188672/225000 (84%)] Loss: 6791.810547\n",
      "Train Epoch: 94 [192768/225000 (86%)] Loss: 6882.992188\n",
      "Train Epoch: 94 [196864/225000 (87%)] Loss: 6834.228516\n",
      "Train Epoch: 94 [200960/225000 (89%)] Loss: 6733.847656\n",
      "Train Epoch: 94 [205056/225000 (91%)] Loss: 6784.894531\n",
      "Train Epoch: 94 [209152/225000 (93%)] Loss: 6678.621094\n",
      "Train Epoch: 94 [213248/225000 (95%)] Loss: 6844.173828\n",
      "Train Epoch: 94 [217344/225000 (97%)] Loss: 6732.755859\n",
      "Train Epoch: 94 [221440/225000 (98%)] Loss: 6866.197266\n",
      "    epoch          : 94\n",
      "    loss           : 6850.033403103669\n",
      "    val_loss       : 7073.550028658643\n",
      "Train Epoch: 95 [256/225000 (0%)] Loss: 6694.101562\n",
      "Train Epoch: 95 [4352/225000 (2%)] Loss: 6783.117188\n",
      "Train Epoch: 95 [8448/225000 (4%)] Loss: 6786.966797\n",
      "Train Epoch: 95 [12544/225000 (6%)] Loss: 6878.558594\n",
      "Train Epoch: 95 [16640/225000 (7%)] Loss: 6825.330078\n",
      "Train Epoch: 95 [20736/225000 (9%)] Loss: 6719.326172\n",
      "Train Epoch: 95 [24832/225000 (11%)] Loss: 6772.207031\n",
      "Train Epoch: 95 [28928/225000 (13%)] Loss: 6671.716797\n",
      "Train Epoch: 95 [33024/225000 (15%)] Loss: 6747.992188\n",
      "Train Epoch: 95 [37120/225000 (16%)] Loss: 6724.693359\n",
      "Train Epoch: 95 [41216/225000 (18%)] Loss: 6984.923828\n",
      "Train Epoch: 95 [45312/225000 (20%)] Loss: 6841.179688\n",
      "Train Epoch: 95 [49408/225000 (22%)] Loss: 6850.763672\n",
      "Train Epoch: 95 [53504/225000 (24%)] Loss: 6694.199219\n",
      "Train Epoch: 95 [57600/225000 (26%)] Loss: 6675.201172\n",
      "Train Epoch: 95 [61696/225000 (27%)] Loss: 6737.433594\n",
      "Train Epoch: 95 [65792/225000 (29%)] Loss: 6767.455078\n",
      "Train Epoch: 95 [69888/225000 (31%)] Loss: 6882.978516\n",
      "Train Epoch: 95 [73984/225000 (33%)] Loss: 6731.554688\n",
      "Train Epoch: 95 [78080/225000 (35%)] Loss: 6815.146484\n",
      "Train Epoch: 95 [82176/225000 (37%)] Loss: 6907.353516\n",
      "Train Epoch: 95 [86272/225000 (38%)] Loss: 6813.986328\n",
      "Train Epoch: 95 [90368/225000 (40%)] Loss: 6784.113281\n",
      "Train Epoch: 95 [94464/225000 (42%)] Loss: 6944.750000\n",
      "Train Epoch: 95 [98560/225000 (44%)] Loss: 6961.234375\n",
      "Train Epoch: 95 [102656/225000 (46%)] Loss: 6638.945312\n",
      "Train Epoch: 95 [106752/225000 (47%)] Loss: 6897.095703\n",
      "Train Epoch: 95 [110848/225000 (49%)] Loss: 6939.496094\n",
      "Train Epoch: 95 [114944/225000 (51%)] Loss: 6774.550781\n",
      "Train Epoch: 95 [119040/225000 (53%)] Loss: 6995.503906\n",
      "Train Epoch: 95 [123136/225000 (55%)] Loss: 6912.611328\n",
      "Train Epoch: 95 [127232/225000 (57%)] Loss: 6747.408203\n",
      "Train Epoch: 95 [131328/225000 (58%)] Loss: 6685.058594\n",
      "Train Epoch: 95 [135424/225000 (60%)] Loss: 6945.734375\n",
      "Train Epoch: 95 [139520/225000 (62%)] Loss: 6859.273438\n",
      "Train Epoch: 95 [143616/225000 (64%)] Loss: 6733.378906\n",
      "Train Epoch: 95 [147712/225000 (66%)] Loss: 6699.347656\n",
      "Train Epoch: 95 [151808/225000 (67%)] Loss: 6835.400391\n",
      "Train Epoch: 95 [155904/225000 (69%)] Loss: 6659.857422\n",
      "Train Epoch: 95 [160000/225000 (71%)] Loss: 6673.029297\n",
      "Train Epoch: 95 [164096/225000 (73%)] Loss: 6593.318359\n",
      "Train Epoch: 95 [168192/225000 (75%)] Loss: 6840.925781\n",
      "Train Epoch: 95 [172288/225000 (77%)] Loss: 6899.679688\n",
      "Train Epoch: 95 [176384/225000 (78%)] Loss: 6733.841797\n",
      "Train Epoch: 95 [180480/225000 (80%)] Loss: 6710.298828\n",
      "Train Epoch: 95 [184576/225000 (82%)] Loss: 6721.185547\n",
      "Train Epoch: 95 [188672/225000 (84%)] Loss: 6886.003906\n",
      "Train Epoch: 95 [192768/225000 (86%)] Loss: 6689.648438\n",
      "Train Epoch: 95 [196864/225000 (87%)] Loss: 6669.591797\n",
      "Train Epoch: 95 [200960/225000 (89%)] Loss: 6692.626953\n",
      "Train Epoch: 95 [205056/225000 (91%)] Loss: 6861.587891\n",
      "Train Epoch: 95 [209152/225000 (93%)] Loss: 6585.507812\n",
      "Train Epoch: 95 [213248/225000 (95%)] Loss: 6793.439453\n",
      "Train Epoch: 95 [217344/225000 (97%)] Loss: 6724.601562\n",
      "Train Epoch: 95 [221440/225000 (98%)] Loss: 6776.314453\n",
      "    epoch          : 95\n",
      "    loss           : 6867.801922239406\n",
      "    val_loss       : 6814.937132179129\n",
      "Train Epoch: 96 [256/225000 (0%)] Loss: 6674.328125\n",
      "Train Epoch: 96 [4352/225000 (2%)] Loss: 6770.875000\n",
      "Train Epoch: 96 [8448/225000 (4%)] Loss: 6870.134766\n",
      "Train Epoch: 96 [12544/225000 (6%)] Loss: 6903.976562\n",
      "Train Epoch: 96 [16640/225000 (7%)] Loss: 6932.236328\n",
      "Train Epoch: 96 [20736/225000 (9%)] Loss: 6803.677734\n",
      "Train Epoch: 96 [24832/225000 (11%)] Loss: 6868.240234\n",
      "Train Epoch: 96 [28928/225000 (13%)] Loss: 6822.539062\n",
      "Train Epoch: 96 [33024/225000 (15%)] Loss: 6923.367188\n",
      "Train Epoch: 96 [37120/225000 (16%)] Loss: 6818.529297\n",
      "Train Epoch: 96 [41216/225000 (18%)] Loss: 6561.783203\n",
      "Train Epoch: 96 [45312/225000 (20%)] Loss: 6701.958984\n",
      "Train Epoch: 96 [49408/225000 (22%)] Loss: 6674.595703\n",
      "Train Epoch: 96 [53504/225000 (24%)] Loss: 6929.835938\n",
      "Train Epoch: 96 [57600/225000 (26%)] Loss: 6744.298828\n",
      "Train Epoch: 96 [61696/225000 (27%)] Loss: 6817.863281\n",
      "Train Epoch: 96 [65792/225000 (29%)] Loss: 6767.970703\n",
      "Train Epoch: 96 [69888/225000 (31%)] Loss: 6727.205078\n",
      "Train Epoch: 96 [73984/225000 (33%)] Loss: 6664.359375\n",
      "Train Epoch: 96 [78080/225000 (35%)] Loss: 6824.873047\n",
      "Train Epoch: 96 [82176/225000 (37%)] Loss: 6930.310547\n",
      "Train Epoch: 96 [86272/225000 (38%)] Loss: 6822.457031\n",
      "Train Epoch: 96 [90368/225000 (40%)] Loss: 6787.296875\n",
      "Train Epoch: 96 [94464/225000 (42%)] Loss: 6748.902344\n",
      "Train Epoch: 96 [98560/225000 (44%)] Loss: 6761.214844\n",
      "Train Epoch: 96 [102656/225000 (46%)] Loss: 6821.035156\n",
      "Train Epoch: 96 [106752/225000 (47%)] Loss: 6920.474609\n",
      "Train Epoch: 96 [110848/225000 (49%)] Loss: 6700.681641\n",
      "Train Epoch: 96 [114944/225000 (51%)] Loss: 6669.490234\n",
      "Train Epoch: 96 [119040/225000 (53%)] Loss: 6843.806641\n",
      "Train Epoch: 96 [123136/225000 (55%)] Loss: 6656.240234\n",
      "Train Epoch: 96 [127232/225000 (57%)] Loss: 8561.707031\n",
      "Train Epoch: 96 [131328/225000 (58%)] Loss: 6721.658203\n",
      "Train Epoch: 96 [135424/225000 (60%)] Loss: 6779.099609\n",
      "Train Epoch: 96 [139520/225000 (62%)] Loss: 6878.539062\n",
      "Train Epoch: 96 [143616/225000 (64%)] Loss: 6740.464844\n",
      "Train Epoch: 96 [147712/225000 (66%)] Loss: 6859.259766\n",
      "Train Epoch: 96 [151808/225000 (67%)] Loss: 6731.900391\n",
      "Train Epoch: 96 [155904/225000 (69%)] Loss: 6720.671875\n",
      "Train Epoch: 96 [160000/225000 (71%)] Loss: 6793.884766\n",
      "Train Epoch: 96 [164096/225000 (73%)] Loss: 6827.603516\n",
      "Train Epoch: 96 [168192/225000 (75%)] Loss: 6911.437500\n",
      "Train Epoch: 96 [172288/225000 (77%)] Loss: 6749.712891\n",
      "Train Epoch: 96 [176384/225000 (78%)] Loss: 6828.794922\n",
      "Train Epoch: 96 [180480/225000 (80%)] Loss: 6818.591797\n",
      "Train Epoch: 96 [184576/225000 (82%)] Loss: 6843.208984\n",
      "Train Epoch: 96 [188672/225000 (84%)] Loss: 6876.820312\n",
      "Train Epoch: 96 [192768/225000 (86%)] Loss: 6817.804688\n",
      "Train Epoch: 96 [196864/225000 (87%)] Loss: 6887.617188\n",
      "Train Epoch: 96 [200960/225000 (89%)] Loss: 6830.828125\n",
      "Train Epoch: 96 [205056/225000 (91%)] Loss: 6788.093750\n",
      "Train Epoch: 96 [209152/225000 (93%)] Loss: 6796.677734\n",
      "Train Epoch: 96 [213248/225000 (95%)] Loss: 6707.587891\n",
      "Train Epoch: 96 [217344/225000 (97%)] Loss: 6686.189453\n",
      "Train Epoch: 96 [221440/225000 (98%)] Loss: 6789.789062\n",
      "    epoch          : 96\n",
      "    loss           : 6923.382154792378\n",
      "    val_loss       : 6836.510267682222\n",
      "Train Epoch: 97 [256/225000 (0%)] Loss: 6861.429688\n",
      "Train Epoch: 97 [4352/225000 (2%)] Loss: 6895.871094\n",
      "Train Epoch: 97 [8448/225000 (4%)] Loss: 6707.037109\n",
      "Train Epoch: 97 [12544/225000 (6%)] Loss: 6935.564453\n",
      "Train Epoch: 97 [16640/225000 (7%)] Loss: 6797.982422\n",
      "Train Epoch: 97 [20736/225000 (9%)] Loss: 6797.439453\n",
      "Train Epoch: 97 [24832/225000 (11%)] Loss: 6841.978516\n",
      "Train Epoch: 97 [28928/225000 (13%)] Loss: 6645.609375\n",
      "Train Epoch: 97 [33024/225000 (15%)] Loss: 6674.646484\n",
      "Train Epoch: 97 [37120/225000 (16%)] Loss: 6735.222656\n",
      "Train Epoch: 97 [41216/225000 (18%)] Loss: 6846.714844\n",
      "Train Epoch: 97 [45312/225000 (20%)] Loss: 6718.677734\n",
      "Train Epoch: 97 [49408/225000 (22%)] Loss: 6791.607422\n",
      "Train Epoch: 97 [53504/225000 (24%)] Loss: 6821.283203\n",
      "Train Epoch: 97 [57600/225000 (26%)] Loss: 6870.601562\n",
      "Train Epoch: 97 [61696/225000 (27%)] Loss: 6759.146484\n",
      "Train Epoch: 97 [65792/225000 (29%)] Loss: 6879.099609\n",
      "Train Epoch: 97 [69888/225000 (31%)] Loss: 6858.853516\n",
      "Train Epoch: 97 [73984/225000 (33%)] Loss: 6705.548828\n",
      "Train Epoch: 97 [78080/225000 (35%)] Loss: 6761.052734\n",
      "Train Epoch: 97 [82176/225000 (37%)] Loss: 6779.576172\n",
      "Train Epoch: 97 [86272/225000 (38%)] Loss: 6623.289062\n",
      "Train Epoch: 97 [90368/225000 (40%)] Loss: 6743.921875\n",
      "Train Epoch: 97 [94464/225000 (42%)] Loss: 6676.085938\n",
      "Train Epoch: 97 [98560/225000 (44%)] Loss: 6712.238281\n",
      "Train Epoch: 97 [102656/225000 (46%)] Loss: 6646.716797\n",
      "Train Epoch: 97 [106752/225000 (47%)] Loss: 6743.416016\n",
      "Train Epoch: 97 [110848/225000 (49%)] Loss: 6830.410156\n",
      "Train Epoch: 97 [114944/225000 (51%)] Loss: 6766.822266\n",
      "Train Epoch: 97 [119040/225000 (53%)] Loss: 6671.332031\n",
      "Train Epoch: 97 [123136/225000 (55%)] Loss: 6803.439453\n",
      "Train Epoch: 97 [127232/225000 (57%)] Loss: 6789.660156\n",
      "Train Epoch: 97 [131328/225000 (58%)] Loss: 6643.148438\n",
      "Train Epoch: 97 [135424/225000 (60%)] Loss: 6831.089844\n",
      "Train Epoch: 97 [139520/225000 (62%)] Loss: 6728.445312\n",
      "Train Epoch: 97 [143616/225000 (64%)] Loss: 6885.396484\n",
      "Train Epoch: 97 [147712/225000 (66%)] Loss: 6904.113281\n",
      "Train Epoch: 97 [151808/225000 (67%)] Loss: 6893.976562\n",
      "Train Epoch: 97 [155904/225000 (69%)] Loss: 6900.601562\n",
      "Train Epoch: 97 [160000/225000 (71%)] Loss: 6708.990234\n",
      "Train Epoch: 97 [164096/225000 (73%)] Loss: 6805.896484\n",
      "Train Epoch: 97 [168192/225000 (75%)] Loss: 6674.898438\n",
      "Train Epoch: 97 [172288/225000 (77%)] Loss: 6889.531250\n",
      "Train Epoch: 97 [176384/225000 (78%)] Loss: 6565.078125\n",
      "Train Epoch: 97 [180480/225000 (80%)] Loss: 6756.035156\n",
      "Train Epoch: 97 [184576/225000 (82%)] Loss: 6793.546875\n",
      "Train Epoch: 97 [188672/225000 (84%)] Loss: 6975.443359\n",
      "Train Epoch: 97 [192768/225000 (86%)] Loss: 6651.224609\n",
      "Train Epoch: 97 [196864/225000 (87%)] Loss: 6680.769531\n",
      "Train Epoch: 97 [200960/225000 (89%)] Loss: 6811.757812\n",
      "Train Epoch: 97 [205056/225000 (91%)] Loss: 6781.285156\n",
      "Train Epoch: 97 [209152/225000 (93%)] Loss: 6791.001953\n",
      "Train Epoch: 97 [213248/225000 (95%)] Loss: 6663.767578\n",
      "Train Epoch: 97 [217344/225000 (97%)] Loss: 6737.775391\n",
      "Train Epoch: 97 [221440/225000 (98%)] Loss: 6763.132812\n",
      "    epoch          : 97\n",
      "    loss           : 6837.417744329494\n",
      "    val_loss       : 6790.483677830015\n",
      "Train Epoch: 98 [256/225000 (0%)] Loss: 6913.982422\n",
      "Train Epoch: 98 [4352/225000 (2%)] Loss: 6541.054688\n",
      "Train Epoch: 98 [8448/225000 (4%)] Loss: 6646.234375\n",
      "Train Epoch: 98 [12544/225000 (6%)] Loss: 6655.691406\n",
      "Train Epoch: 98 [16640/225000 (7%)] Loss: 6980.476562\n",
      "Train Epoch: 98 [20736/225000 (9%)] Loss: 6877.853516\n",
      "Train Epoch: 98 [24832/225000 (11%)] Loss: 6761.318359\n",
      "Train Epoch: 98 [28928/225000 (13%)] Loss: 6735.744141\n",
      "Train Epoch: 98 [33024/225000 (15%)] Loss: 6904.132812\n",
      "Train Epoch: 98 [37120/225000 (16%)] Loss: 8517.160156\n",
      "Train Epoch: 98 [41216/225000 (18%)] Loss: 6874.523438\n",
      "Train Epoch: 98 [45312/225000 (20%)] Loss: 6768.855469\n",
      "Train Epoch: 98 [49408/225000 (22%)] Loss: 6712.427734\n",
      "Train Epoch: 98 [53504/225000 (24%)] Loss: 6903.519531\n",
      "Train Epoch: 98 [57600/225000 (26%)] Loss: 6725.966797\n",
      "Train Epoch: 98 [61696/225000 (27%)] Loss: 6810.855469\n",
      "Train Epoch: 98 [65792/225000 (29%)] Loss: 6740.580078\n",
      "Train Epoch: 98 [69888/225000 (31%)] Loss: 6821.953125\n",
      "Train Epoch: 98 [73984/225000 (33%)] Loss: 6818.349609\n",
      "Train Epoch: 98 [78080/225000 (35%)] Loss: 6809.089844\n",
      "Train Epoch: 98 [82176/225000 (37%)] Loss: 6823.312500\n",
      "Train Epoch: 98 [86272/225000 (38%)] Loss: 6917.003906\n",
      "Train Epoch: 98 [90368/225000 (40%)] Loss: 6746.695312\n",
      "Train Epoch: 98 [94464/225000 (42%)] Loss: 6721.111328\n",
      "Train Epoch: 98 [98560/225000 (44%)] Loss: 6853.363281\n",
      "Train Epoch: 98 [102656/225000 (46%)] Loss: 6930.777344\n",
      "Train Epoch: 98 [106752/225000 (47%)] Loss: 6787.833984\n",
      "Train Epoch: 98 [110848/225000 (49%)] Loss: 6650.802734\n",
      "Train Epoch: 98 [114944/225000 (51%)] Loss: 6635.550781\n",
      "Train Epoch: 98 [119040/225000 (53%)] Loss: 6700.890625\n",
      "Train Epoch: 98 [123136/225000 (55%)] Loss: 6846.349609\n",
      "Train Epoch: 98 [127232/225000 (57%)] Loss: 6705.701172\n",
      "Train Epoch: 98 [131328/225000 (58%)] Loss: 6753.716797\n",
      "Train Epoch: 98 [135424/225000 (60%)] Loss: 6754.708984\n",
      "Train Epoch: 98 [139520/225000 (62%)] Loss: 6876.943359\n",
      "Train Epoch: 98 [143616/225000 (64%)] Loss: 6750.542969\n",
      "Train Epoch: 98 [147712/225000 (66%)] Loss: 6778.205078\n",
      "Train Epoch: 98 [151808/225000 (67%)] Loss: 6853.521484\n",
      "Train Epoch: 98 [155904/225000 (69%)] Loss: 6804.632812\n",
      "Train Epoch: 98 [160000/225000 (71%)] Loss: 6794.050781\n",
      "Train Epoch: 98 [164096/225000 (73%)] Loss: 6859.617188\n",
      "Train Epoch: 98 [168192/225000 (75%)] Loss: 27380.660156\n",
      "Train Epoch: 98 [172288/225000 (77%)] Loss: 6835.318359\n",
      "Train Epoch: 98 [176384/225000 (78%)] Loss: 6720.607422\n",
      "Train Epoch: 98 [180480/225000 (80%)] Loss: 6957.173828\n",
      "Train Epoch: 98 [184576/225000 (82%)] Loss: 6701.412109\n",
      "Train Epoch: 98 [188672/225000 (84%)] Loss: 6603.400391\n",
      "Train Epoch: 98 [192768/225000 (86%)] Loss: 6700.753906\n",
      "Train Epoch: 98 [196864/225000 (87%)] Loss: 6732.312500\n",
      "Train Epoch: 98 [200960/225000 (89%)] Loss: 6824.523438\n",
      "Train Epoch: 98 [205056/225000 (91%)] Loss: 6811.835938\n",
      "Train Epoch: 98 [209152/225000 (93%)] Loss: 6952.343750\n",
      "Train Epoch: 98 [213248/225000 (95%)] Loss: 6729.453125\n",
      "Train Epoch: 98 [217344/225000 (97%)] Loss: 6721.253906\n",
      "Train Epoch: 98 [221440/225000 (98%)] Loss: 6857.015625\n",
      "    epoch          : 98\n",
      "    loss           : 6846.255933811504\n",
      "    val_loss       : 7005.0259947521345\n",
      "Train Epoch: 99 [256/225000 (0%)] Loss: 6808.089844\n",
      "Train Epoch: 99 [4352/225000 (2%)] Loss: 6627.175781\n",
      "Train Epoch: 99 [8448/225000 (4%)] Loss: 6634.525391\n",
      "Train Epoch: 99 [12544/225000 (6%)] Loss: 6641.455078\n",
      "Train Epoch: 99 [16640/225000 (7%)] Loss: 6734.931641\n",
      "Train Epoch: 99 [20736/225000 (9%)] Loss: 6695.001953\n",
      "Train Epoch: 99 [24832/225000 (11%)] Loss: 6712.958984\n",
      "Train Epoch: 99 [28928/225000 (13%)] Loss: 6826.916016\n",
      "Train Epoch: 99 [33024/225000 (15%)] Loss: 6826.431641\n",
      "Train Epoch: 99 [37120/225000 (16%)] Loss: 6700.103516\n",
      "Train Epoch: 99 [41216/225000 (18%)] Loss: 6745.451172\n",
      "Train Epoch: 99 [45312/225000 (20%)] Loss: 6653.189453\n",
      "Train Epoch: 99 [49408/225000 (22%)] Loss: 6759.568359\n",
      "Train Epoch: 99 [53504/225000 (24%)] Loss: 6846.539062\n",
      "Train Epoch: 99 [57600/225000 (26%)] Loss: 6917.792969\n",
      "Train Epoch: 99 [61696/225000 (27%)] Loss: 6783.339844\n",
      "Train Epoch: 99 [65792/225000 (29%)] Loss: 6732.990234\n",
      "Train Epoch: 99 [69888/225000 (31%)] Loss: 6765.517578\n",
      "Train Epoch: 99 [73984/225000 (33%)] Loss: 6787.896484\n",
      "Train Epoch: 99 [78080/225000 (35%)] Loss: 6786.494141\n",
      "Train Epoch: 99 [82176/225000 (37%)] Loss: 6711.703125\n",
      "Train Epoch: 99 [86272/225000 (38%)] Loss: 6730.970703\n",
      "Train Epoch: 99 [90368/225000 (40%)] Loss: 6601.193359\n",
      "Train Epoch: 99 [94464/225000 (42%)] Loss: 6964.074219\n",
      "Train Epoch: 99 [98560/225000 (44%)] Loss: 6856.173828\n",
      "Train Epoch: 99 [102656/225000 (46%)] Loss: 6979.613281\n",
      "Train Epoch: 99 [106752/225000 (47%)] Loss: 6938.621094\n",
      "Train Epoch: 99 [110848/225000 (49%)] Loss: 6615.519531\n",
      "Train Epoch: 99 [114944/225000 (51%)] Loss: 6730.865234\n",
      "Train Epoch: 99 [119040/225000 (53%)] Loss: 6731.255859\n",
      "Train Epoch: 99 [123136/225000 (55%)] Loss: 6771.324219\n",
      "Train Epoch: 99 [127232/225000 (57%)] Loss: 6838.761719\n",
      "Train Epoch: 99 [131328/225000 (58%)] Loss: 6762.679688\n",
      "Train Epoch: 99 [135424/225000 (60%)] Loss: 6748.609375\n",
      "Train Epoch: 99 [139520/225000 (62%)] Loss: 6768.113281\n",
      "Train Epoch: 99 [143616/225000 (64%)] Loss: 6775.304688\n",
      "Train Epoch: 99 [147712/225000 (66%)] Loss: 6702.160156\n",
      "Train Epoch: 99 [151808/225000 (67%)] Loss: 6681.949219\n",
      "Train Epoch: 99 [155904/225000 (69%)] Loss: 6817.996094\n",
      "Train Epoch: 99 [160000/225000 (71%)] Loss: 6814.701172\n",
      "Train Epoch: 99 [164096/225000 (73%)] Loss: 6754.025391\n",
      "Train Epoch: 99 [168192/225000 (75%)] Loss: 6659.138672\n",
      "Train Epoch: 99 [172288/225000 (77%)] Loss: 6800.621094\n",
      "Train Epoch: 99 [176384/225000 (78%)] Loss: 6705.296875\n",
      "Train Epoch: 99 [180480/225000 (80%)] Loss: 6727.117188\n",
      "Train Epoch: 99 [184576/225000 (82%)] Loss: 6753.035156\n",
      "Train Epoch: 99 [188672/225000 (84%)] Loss: 6761.080078\n",
      "Train Epoch: 99 [192768/225000 (86%)] Loss: 6637.902344\n",
      "Train Epoch: 99 [196864/225000 (87%)] Loss: 6963.494141\n",
      "Train Epoch: 99 [200960/225000 (89%)] Loss: 6770.162109\n",
      "Train Epoch: 99 [205056/225000 (91%)] Loss: 6630.078125\n",
      "Train Epoch: 99 [209152/225000 (93%)] Loss: 6720.613281\n",
      "Train Epoch: 99 [213248/225000 (95%)] Loss: 6679.333984\n",
      "Train Epoch: 99 [217344/225000 (97%)] Loss: 6902.822266\n",
      "Train Epoch: 99 [221440/225000 (98%)] Loss: 6781.931641\n",
      "    epoch          : 99\n",
      "    loss           : 6850.907353215657\n",
      "    val_loss       : 7051.517807321889\n",
      "Train Epoch: 100 [256/225000 (0%)] Loss: 6768.158203\n",
      "Train Epoch: 100 [4352/225000 (2%)] Loss: 6744.781250\n",
      "Train Epoch: 100 [8448/225000 (4%)] Loss: 6769.197266\n",
      "Train Epoch: 100 [12544/225000 (6%)] Loss: 6756.453125\n",
      "Train Epoch: 100 [16640/225000 (7%)] Loss: 6835.003906\n",
      "Train Epoch: 100 [20736/225000 (9%)] Loss: 6668.449219\n",
      "Train Epoch: 100 [24832/225000 (11%)] Loss: 6696.693359\n",
      "Train Epoch: 100 [28928/225000 (13%)] Loss: 6705.246094\n",
      "Train Epoch: 100 [33024/225000 (15%)] Loss: 6610.453125\n",
      "Train Epoch: 100 [37120/225000 (16%)] Loss: 6905.275391\n",
      "Train Epoch: 100 [41216/225000 (18%)] Loss: 6696.169922\n",
      "Train Epoch: 100 [45312/225000 (20%)] Loss: 6669.041016\n",
      "Train Epoch: 100 [49408/225000 (22%)] Loss: 6759.914062\n",
      "Train Epoch: 100 [53504/225000 (24%)] Loss: 6633.138672\n",
      "Train Epoch: 100 [57600/225000 (26%)] Loss: 6733.240234\n",
      "Train Epoch: 100 [61696/225000 (27%)] Loss: 6792.832031\n",
      "Train Epoch: 100 [65792/225000 (29%)] Loss: 6792.207031\n",
      "Train Epoch: 100 [69888/225000 (31%)] Loss: 6734.152344\n",
      "Train Epoch: 100 [73984/225000 (33%)] Loss: 6834.787109\n",
      "Train Epoch: 100 [78080/225000 (35%)] Loss: 6892.863281\n",
      "Train Epoch: 100 [82176/225000 (37%)] Loss: 6819.144531\n",
      "Train Epoch: 100 [86272/225000 (38%)] Loss: 6633.679688\n",
      "Train Epoch: 100 [90368/225000 (40%)] Loss: 6769.585938\n",
      "Train Epoch: 100 [94464/225000 (42%)] Loss: 6783.521484\n",
      "Train Epoch: 100 [98560/225000 (44%)] Loss: 6678.220703\n",
      "Train Epoch: 100 [102656/225000 (46%)] Loss: 6786.707031\n",
      "Train Epoch: 100 [106752/225000 (47%)] Loss: 6681.111328\n",
      "Train Epoch: 100 [110848/225000 (49%)] Loss: 6802.341797\n",
      "Train Epoch: 100 [114944/225000 (51%)] Loss: 6791.861328\n",
      "Train Epoch: 100 [119040/225000 (53%)] Loss: 6561.642578\n",
      "Train Epoch: 100 [123136/225000 (55%)] Loss: 6759.000000\n",
      "Train Epoch: 100 [127232/225000 (57%)] Loss: 6779.017578\n",
      "Train Epoch: 100 [131328/225000 (58%)] Loss: 6711.806641\n",
      "Train Epoch: 100 [135424/225000 (60%)] Loss: 6777.185547\n",
      "Train Epoch: 100 [139520/225000 (62%)] Loss: 6752.931641\n",
      "Train Epoch: 100 [143616/225000 (64%)] Loss: 6775.693359\n",
      "Train Epoch: 100 [147712/225000 (66%)] Loss: 6726.259766\n",
      "Train Epoch: 100 [151808/225000 (67%)] Loss: 6713.439453\n",
      "Train Epoch: 100 [155904/225000 (69%)] Loss: 6808.894531\n",
      "Train Epoch: 100 [160000/225000 (71%)] Loss: 6766.732422\n",
      "Train Epoch: 100 [164096/225000 (73%)] Loss: 6884.750000\n",
      "Train Epoch: 100 [168192/225000 (75%)] Loss: 6486.734375\n",
      "Train Epoch: 100 [172288/225000 (77%)] Loss: 6737.136719\n",
      "Train Epoch: 100 [176384/225000 (78%)] Loss: 6734.923828\n",
      "Train Epoch: 100 [180480/225000 (80%)] Loss: 6835.558594\n",
      "Train Epoch: 100 [184576/225000 (82%)] Loss: 6593.263672\n",
      "Train Epoch: 100 [188672/225000 (84%)] Loss: 6671.144531\n",
      "Train Epoch: 100 [192768/225000 (86%)] Loss: 6767.542969\n",
      "Train Epoch: 100 [196864/225000 (87%)] Loss: 6623.449219\n",
      "Train Epoch: 100 [200960/225000 (89%)] Loss: 6817.050781\n",
      "Train Epoch: 100 [205056/225000 (91%)] Loss: 6687.021484\n",
      "Train Epoch: 100 [209152/225000 (93%)] Loss: 6722.277344\n",
      "Train Epoch: 100 [213248/225000 (95%)] Loss: 6781.464844\n",
      "Train Epoch: 100 [217344/225000 (97%)] Loss: 6500.806641\n",
      "Train Epoch: 100 [221440/225000 (98%)] Loss: 6653.531250\n",
      "    epoch          : 100\n",
      "    loss           : 6831.018850211533\n",
      "    val_loss       : 7023.001256200732\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0815_160624/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [256/225000 (0%)] Loss: 6674.751953\n",
      "Train Epoch: 101 [4352/225000 (2%)] Loss: 6763.222656\n",
      "Train Epoch: 101 [8448/225000 (4%)] Loss: 6711.195312\n",
      "Train Epoch: 101 [12544/225000 (6%)] Loss: 6849.080078\n",
      "Train Epoch: 101 [16640/225000 (7%)] Loss: 6891.955078\n",
      "Train Epoch: 101 [20736/225000 (9%)] Loss: 6831.501953\n",
      "Train Epoch: 101 [24832/225000 (11%)] Loss: 6711.556641\n",
      "Train Epoch: 101 [28928/225000 (13%)] Loss: 6653.353516\n",
      "Train Epoch: 101 [33024/225000 (15%)] Loss: 6841.929688\n",
      "Train Epoch: 101 [37120/225000 (16%)] Loss: 6773.066406\n",
      "Train Epoch: 101 [41216/225000 (18%)] Loss: 6815.863281\n",
      "Train Epoch: 101 [45312/225000 (20%)] Loss: 6586.449219\n",
      "Train Epoch: 101 [49408/225000 (22%)] Loss: 6755.941406\n",
      "Train Epoch: 101 [53504/225000 (24%)] Loss: 6785.912109\n",
      "Train Epoch: 101 [57600/225000 (26%)] Loss: 6765.876953\n",
      "Train Epoch: 101 [61696/225000 (27%)] Loss: 6778.078125\n",
      "Train Epoch: 101 [65792/225000 (29%)] Loss: 6623.871094\n",
      "Train Epoch: 101 [69888/225000 (31%)] Loss: 6775.208984\n",
      "Train Epoch: 101 [73984/225000 (33%)] Loss: 6737.630859\n",
      "Train Epoch: 101 [78080/225000 (35%)] Loss: 6748.673828\n",
      "Train Epoch: 101 [82176/225000 (37%)] Loss: 6882.660156\n",
      "Train Epoch: 101 [86272/225000 (38%)] Loss: 6930.982422\n",
      "Train Epoch: 101 [90368/225000 (40%)] Loss: 6717.910156\n",
      "Train Epoch: 101 [94464/225000 (42%)] Loss: 6672.746094\n",
      "Train Epoch: 101 [98560/225000 (44%)] Loss: 6670.591797\n",
      "Train Epoch: 101 [102656/225000 (46%)] Loss: 6685.675781\n",
      "Train Epoch: 101 [106752/225000 (47%)] Loss: 6912.703125\n",
      "Train Epoch: 101 [110848/225000 (49%)] Loss: 6684.966797\n",
      "Train Epoch: 101 [114944/225000 (51%)] Loss: 6791.244141\n",
      "Train Epoch: 101 [119040/225000 (53%)] Loss: 6723.697266\n",
      "Train Epoch: 101 [123136/225000 (55%)] Loss: 6699.796875\n",
      "Train Epoch: 101 [127232/225000 (57%)] Loss: 6630.816406\n",
      "Train Epoch: 101 [131328/225000 (58%)] Loss: 6863.964844\n",
      "Train Epoch: 101 [135424/225000 (60%)] Loss: 6773.593750\n",
      "Train Epoch: 101 [139520/225000 (62%)] Loss: 6622.316406\n",
      "Train Epoch: 101 [143616/225000 (64%)] Loss: 6815.728516\n",
      "Train Epoch: 101 [147712/225000 (66%)] Loss: 6863.902344\n",
      "Train Epoch: 101 [151808/225000 (67%)] Loss: 6652.826172\n",
      "Train Epoch: 101 [155904/225000 (69%)] Loss: 6708.476562\n",
      "Train Epoch: 101 [160000/225000 (71%)] Loss: 6638.343750\n",
      "Train Epoch: 101 [164096/225000 (73%)] Loss: 6703.810547\n",
      "Train Epoch: 101 [168192/225000 (75%)] Loss: 6699.142578\n",
      "Train Epoch: 101 [172288/225000 (77%)] Loss: 6744.212891\n",
      "Train Epoch: 101 [176384/225000 (78%)] Loss: 6675.253906\n",
      "Train Epoch: 101 [180480/225000 (80%)] Loss: 6559.955078\n",
      "Train Epoch: 101 [184576/225000 (82%)] Loss: 6823.365234\n",
      "Train Epoch: 101 [188672/225000 (84%)] Loss: 6864.898438\n",
      "Train Epoch: 101 [192768/225000 (86%)] Loss: 6628.914062\n",
      "Train Epoch: 101 [196864/225000 (87%)] Loss: 6729.900391\n",
      "Train Epoch: 101 [200960/225000 (89%)] Loss: 6774.265625\n",
      "Train Epoch: 101 [205056/225000 (91%)] Loss: 6658.873047\n",
      "Train Epoch: 101 [209152/225000 (93%)] Loss: 7003.847656\n",
      "Train Epoch: 101 [213248/225000 (95%)] Loss: 6726.517578\n",
      "Train Epoch: 101 [217344/225000 (97%)] Loss: 6818.578125\n",
      "Train Epoch: 101 [221440/225000 (98%)] Loss: 6806.595703\n",
      "    epoch          : 101\n",
      "    loss           : 6893.39075720812\n",
      "    val_loss       : 6777.448109350642\n",
      "Train Epoch: 102 [256/225000 (0%)] Loss: 6709.074219\n",
      "Train Epoch: 102 [4352/225000 (2%)] Loss: 6748.658203\n",
      "Train Epoch: 102 [8448/225000 (4%)] Loss: 6653.921875\n",
      "Train Epoch: 102 [12544/225000 (6%)] Loss: 6885.958984\n",
      "Train Epoch: 102 [16640/225000 (7%)] Loss: 6763.792969\n",
      "Train Epoch: 102 [20736/225000 (9%)] Loss: 6983.425781\n",
      "Train Epoch: 102 [24832/225000 (11%)] Loss: 6670.583984\n",
      "Train Epoch: 102 [28928/225000 (13%)] Loss: 6696.505859\n",
      "Train Epoch: 102 [33024/225000 (15%)] Loss: 6785.851562\n",
      "Train Epoch: 102 [37120/225000 (16%)] Loss: 6795.429688\n",
      "Train Epoch: 102 [41216/225000 (18%)] Loss: 6896.527344\n",
      "Train Epoch: 102 [45312/225000 (20%)] Loss: 6966.464844\n",
      "Train Epoch: 102 [49408/225000 (22%)] Loss: 6727.197266\n",
      "Train Epoch: 102 [53504/225000 (24%)] Loss: 6616.837891\n",
      "Train Epoch: 102 [57600/225000 (26%)] Loss: 6747.814453\n",
      "Train Epoch: 102 [61696/225000 (27%)] Loss: 6612.787109\n",
      "Train Epoch: 102 [65792/225000 (29%)] Loss: 6847.234375\n",
      "Train Epoch: 102 [69888/225000 (31%)] Loss: 6816.560547\n",
      "Train Epoch: 102 [73984/225000 (33%)] Loss: 6674.003906\n",
      "Train Epoch: 102 [78080/225000 (35%)] Loss: 6873.109375\n",
      "Train Epoch: 102 [82176/225000 (37%)] Loss: 8322.736328\n",
      "Train Epoch: 102 [86272/225000 (38%)] Loss: 6561.462891\n",
      "Train Epoch: 102 [90368/225000 (40%)] Loss: 6842.007812\n",
      "Train Epoch: 102 [94464/225000 (42%)] Loss: 6668.683594\n",
      "Train Epoch: 102 [98560/225000 (44%)] Loss: 6686.804688\n",
      "Train Epoch: 102 [102656/225000 (46%)] Loss: 6856.322266\n",
      "Train Epoch: 102 [106752/225000 (47%)] Loss: 6654.294922\n",
      "Train Epoch: 102 [110848/225000 (49%)] Loss: 6686.109375\n",
      "Train Epoch: 102 [114944/225000 (51%)] Loss: 6763.242188\n",
      "Train Epoch: 102 [119040/225000 (53%)] Loss: 8593.259766\n",
      "Train Epoch: 102 [123136/225000 (55%)] Loss: 6628.232422\n",
      "Train Epoch: 102 [127232/225000 (57%)] Loss: 6716.134766\n",
      "Train Epoch: 102 [131328/225000 (58%)] Loss: 6840.134766\n",
      "Train Epoch: 102 [135424/225000 (60%)] Loss: 6594.148438\n",
      "Train Epoch: 102 [139520/225000 (62%)] Loss: 6705.275391\n",
      "Train Epoch: 102 [143616/225000 (64%)] Loss: 6768.976562\n",
      "Train Epoch: 102 [147712/225000 (66%)] Loss: 6811.244141\n",
      "Train Epoch: 102 [151808/225000 (67%)] Loss: 6697.898438\n",
      "Train Epoch: 102 [155904/225000 (69%)] Loss: 6797.302734\n",
      "Train Epoch: 102 [160000/225000 (71%)] Loss: 6954.853516\n",
      "Train Epoch: 102 [164096/225000 (73%)] Loss: 6727.291016\n",
      "Train Epoch: 102 [168192/225000 (75%)] Loss: 8600.251953\n",
      "Train Epoch: 102 [172288/225000 (77%)] Loss: 6881.548828\n",
      "Train Epoch: 102 [176384/225000 (78%)] Loss: 6708.931641\n",
      "Train Epoch: 102 [180480/225000 (80%)] Loss: 6577.207031\n",
      "Train Epoch: 102 [184576/225000 (82%)] Loss: 6805.298828\n",
      "Train Epoch: 102 [188672/225000 (84%)] Loss: 6649.294922\n",
      "Train Epoch: 102 [192768/225000 (86%)] Loss: 6829.945312\n",
      "Train Epoch: 102 [196864/225000 (87%)] Loss: 6764.470703\n",
      "Train Epoch: 102 [200960/225000 (89%)] Loss: 6806.591797\n",
      "Train Epoch: 102 [205056/225000 (91%)] Loss: 6924.679688\n",
      "Train Epoch: 102 [209152/225000 (93%)] Loss: 6645.214844\n",
      "Train Epoch: 102 [213248/225000 (95%)] Loss: 6572.972656\n",
      "Train Epoch: 102 [217344/225000 (97%)] Loss: 6658.523438\n",
      "Train Epoch: 102 [221440/225000 (98%)] Loss: 6824.103516\n",
      "    epoch          : 102\n",
      "    loss           : 6946.29734828285\n",
      "    val_loss       : 6856.204167645805\n",
      "Train Epoch: 103 [256/225000 (0%)] Loss: 6893.398438\n",
      "Train Epoch: 103 [4352/225000 (2%)] Loss: 6617.388672\n",
      "Train Epoch: 103 [8448/225000 (4%)] Loss: 6790.207031\n",
      "Train Epoch: 103 [12544/225000 (6%)] Loss: 6688.195312\n",
      "Train Epoch: 103 [16640/225000 (7%)] Loss: 6701.679688\n",
      "Train Epoch: 103 [20736/225000 (9%)] Loss: 6594.673828\n",
      "Train Epoch: 103 [24832/225000 (11%)] Loss: 6987.742188\n",
      "Train Epoch: 103 [28928/225000 (13%)] Loss: 6815.451172\n",
      "Train Epoch: 103 [33024/225000 (15%)] Loss: 6708.103516\n",
      "Train Epoch: 103 [37120/225000 (16%)] Loss: 6690.246094\n",
      "Train Epoch: 103 [41216/225000 (18%)] Loss: 6587.337891\n",
      "Train Epoch: 103 [45312/225000 (20%)] Loss: 6557.490234\n",
      "Train Epoch: 103 [49408/225000 (22%)] Loss: 6648.390625\n",
      "Train Epoch: 103 [53504/225000 (24%)] Loss: 8475.976562\n",
      "Train Epoch: 103 [57600/225000 (26%)] Loss: 6655.320312\n",
      "Train Epoch: 103 [61696/225000 (27%)] Loss: 6837.869141\n",
      "Train Epoch: 103 [65792/225000 (29%)] Loss: 6855.425781\n",
      "Train Epoch: 103 [69888/225000 (31%)] Loss: 6697.240234\n",
      "Train Epoch: 103 [73984/225000 (33%)] Loss: 6727.830078\n",
      "Train Epoch: 103 [78080/225000 (35%)] Loss: 6567.091797\n",
      "Train Epoch: 103 [82176/225000 (37%)] Loss: 6687.671875\n",
      "Train Epoch: 103 [86272/225000 (38%)] Loss: 6640.720703\n",
      "Train Epoch: 103 [90368/225000 (40%)] Loss: 6873.433594\n",
      "Train Epoch: 103 [94464/225000 (42%)] Loss: 6733.650391\n",
      "Train Epoch: 103 [98560/225000 (44%)] Loss: 6791.242188\n",
      "Train Epoch: 103 [102656/225000 (46%)] Loss: 6692.972656\n",
      "Train Epoch: 103 [106752/225000 (47%)] Loss: 6755.160156\n",
      "Train Epoch: 103 [110848/225000 (49%)] Loss: 6808.048828\n",
      "Train Epoch: 103 [114944/225000 (51%)] Loss: 6619.623047\n",
      "Train Epoch: 103 [119040/225000 (53%)] Loss: 6702.591797\n",
      "Train Epoch: 103 [123136/225000 (55%)] Loss: 6761.781250\n",
      "Train Epoch: 103 [127232/225000 (57%)] Loss: 6904.031250\n",
      "Train Epoch: 103 [131328/225000 (58%)] Loss: 6903.935547\n",
      "Train Epoch: 103 [135424/225000 (60%)] Loss: 6685.048828\n",
      "Train Epoch: 103 [139520/225000 (62%)] Loss: 6774.169922\n",
      "Train Epoch: 103 [143616/225000 (64%)] Loss: 6963.710938\n",
      "Train Epoch: 103 [147712/225000 (66%)] Loss: 6709.203125\n",
      "Train Epoch: 103 [151808/225000 (67%)] Loss: 6804.326172\n",
      "Train Epoch: 103 [155904/225000 (69%)] Loss: 6672.150391\n",
      "Train Epoch: 103 [160000/225000 (71%)] Loss: 6805.962891\n",
      "Train Epoch: 103 [164096/225000 (73%)] Loss: 6726.154297\n",
      "Train Epoch: 103 [168192/225000 (75%)] Loss: 6796.332031\n",
      "Train Epoch: 103 [172288/225000 (77%)] Loss: 6598.955078\n",
      "Train Epoch: 103 [176384/225000 (78%)] Loss: 6633.181641\n",
      "Train Epoch: 103 [180480/225000 (80%)] Loss: 6765.197266\n",
      "Train Epoch: 103 [184576/225000 (82%)] Loss: 6693.230469\n",
      "Train Epoch: 103 [188672/225000 (84%)] Loss: 6775.144531\n",
      "Train Epoch: 103 [192768/225000 (86%)] Loss: 6771.888672\n",
      "Train Epoch: 103 [196864/225000 (87%)] Loss: 6687.187500\n",
      "Train Epoch: 103 [200960/225000 (89%)] Loss: 6680.773438\n",
      "Train Epoch: 103 [205056/225000 (91%)] Loss: 6509.937500\n",
      "Train Epoch: 103 [209152/225000 (93%)] Loss: 6788.708984\n",
      "Train Epoch: 103 [213248/225000 (95%)] Loss: 6708.000000\n",
      "Train Epoch: 103 [217344/225000 (97%)] Loss: 6764.789062\n",
      "Train Epoch: 103 [221440/225000 (98%)] Loss: 6828.425781\n",
      "    epoch          : 103\n",
      "    loss           : 6965.0537687091155\n",
      "    val_loss       : 6791.616976263572\n",
      "Train Epoch: 104 [256/225000 (0%)] Loss: 6723.046875\n",
      "Train Epoch: 104 [4352/225000 (2%)] Loss: 6852.871094\n",
      "Train Epoch: 104 [8448/225000 (4%)] Loss: 6770.638672\n",
      "Train Epoch: 104 [12544/225000 (6%)] Loss: 6589.267578\n",
      "Train Epoch: 104 [16640/225000 (7%)] Loss: 6702.810547\n",
      "Train Epoch: 104 [20736/225000 (9%)] Loss: 6704.175781\n",
      "Train Epoch: 104 [24832/225000 (11%)] Loss: 6800.144531\n",
      "Train Epoch: 104 [28928/225000 (13%)] Loss: 6768.818359\n",
      "Train Epoch: 104 [33024/225000 (15%)] Loss: 6646.089844\n",
      "Train Epoch: 104 [37120/225000 (16%)] Loss: 6740.925781\n",
      "Train Epoch: 104 [41216/225000 (18%)] Loss: 6874.902344\n",
      "Train Epoch: 104 [45312/225000 (20%)] Loss: 6676.349609\n",
      "Train Epoch: 104 [49408/225000 (22%)] Loss: 6744.917969\n",
      "Train Epoch: 104 [53504/225000 (24%)] Loss: 6807.992188\n",
      "Train Epoch: 104 [57600/225000 (26%)] Loss: 6830.703125\n",
      "Train Epoch: 104 [61696/225000 (27%)] Loss: 6661.591797\n",
      "Train Epoch: 104 [65792/225000 (29%)] Loss: 6881.195312\n",
      "Train Epoch: 104 [69888/225000 (31%)] Loss: 6705.718750\n",
      "Train Epoch: 104 [73984/225000 (33%)] Loss: 6865.667969\n",
      "Train Epoch: 104 [78080/225000 (35%)] Loss: 6797.701172\n",
      "Train Epoch: 104 [82176/225000 (37%)] Loss: 6671.306641\n",
      "Train Epoch: 104 [86272/225000 (38%)] Loss: 6780.777344\n",
      "Train Epoch: 104 [90368/225000 (40%)] Loss: 6772.314453\n",
      "Train Epoch: 104 [94464/225000 (42%)] Loss: 6657.091797\n",
      "Train Epoch: 104 [98560/225000 (44%)] Loss: 6876.691406\n",
      "Train Epoch: 104 [102656/225000 (46%)] Loss: 6586.791016\n",
      "Train Epoch: 104 [106752/225000 (47%)] Loss: 6759.326172\n",
      "Train Epoch: 104 [110848/225000 (49%)] Loss: 6731.398438\n",
      "Train Epoch: 104 [114944/225000 (51%)] Loss: 6751.443359\n",
      "Train Epoch: 104 [119040/225000 (53%)] Loss: 6682.492188\n",
      "Train Epoch: 104 [123136/225000 (55%)] Loss: 6675.525391\n",
      "Train Epoch: 104 [127232/225000 (57%)] Loss: 6936.201172\n",
      "Train Epoch: 104 [131328/225000 (58%)] Loss: 6665.285156\n",
      "Train Epoch: 104 [135424/225000 (60%)] Loss: 6791.033203\n",
      "Train Epoch: 104 [139520/225000 (62%)] Loss: 6753.115234\n",
      "Train Epoch: 104 [143616/225000 (64%)] Loss: 6834.537109\n",
      "Train Epoch: 104 [147712/225000 (66%)] Loss: 6621.583984\n",
      "Train Epoch: 104 [151808/225000 (67%)] Loss: 27727.396484\n",
      "Train Epoch: 104 [155904/225000 (69%)] Loss: 6818.955078\n",
      "Train Epoch: 104 [160000/225000 (71%)] Loss: 6792.869141\n",
      "Train Epoch: 104 [164096/225000 (73%)] Loss: 6804.890625\n",
      "Train Epoch: 104 [168192/225000 (75%)] Loss: 6697.453125\n",
      "Train Epoch: 104 [172288/225000 (77%)] Loss: 6735.925781\n",
      "Train Epoch: 104 [176384/225000 (78%)] Loss: 6750.544922\n",
      "Train Epoch: 104 [180480/225000 (80%)] Loss: 6706.707031\n",
      "Train Epoch: 104 [184576/225000 (82%)] Loss: 6698.734375\n",
      "Train Epoch: 104 [188672/225000 (84%)] Loss: 6778.603516\n",
      "Train Epoch: 104 [192768/225000 (86%)] Loss: 6720.882812\n",
      "Train Epoch: 104 [196864/225000 (87%)] Loss: 6728.566406\n",
      "Train Epoch: 104 [200960/225000 (89%)] Loss: 6825.398438\n",
      "Train Epoch: 104 [205056/225000 (91%)] Loss: 6723.144531\n",
      "Train Epoch: 104 [209152/225000 (93%)] Loss: 6718.564453\n",
      "Train Epoch: 104 [213248/225000 (95%)] Loss: 6737.480469\n",
      "Train Epoch: 104 [217344/225000 (97%)] Loss: 6751.558594\n",
      "Train Epoch: 104 [221440/225000 (98%)] Loss: 6626.447266\n",
      "    epoch          : 104\n",
      "    loss           : 6886.513569663681\n",
      "    val_loss       : 6825.4213427986415\n",
      "Train Epoch: 105 [256/225000 (0%)] Loss: 6742.919922\n",
      "Train Epoch: 105 [4352/225000 (2%)] Loss: 6705.330078\n",
      "Train Epoch: 105 [8448/225000 (4%)] Loss: 6796.941406\n",
      "Train Epoch: 105 [12544/225000 (6%)] Loss: 6767.890625\n",
      "Train Epoch: 105 [16640/225000 (7%)] Loss: 6721.587891\n",
      "Train Epoch: 105 [20736/225000 (9%)] Loss: 6770.205078\n",
      "Train Epoch: 105 [24832/225000 (11%)] Loss: 6588.201172\n",
      "Train Epoch: 105 [28928/225000 (13%)] Loss: 6665.419922\n",
      "Train Epoch: 105 [33024/225000 (15%)] Loss: 6667.890625\n",
      "Train Epoch: 105 [37120/225000 (16%)] Loss: 6757.960938\n",
      "Train Epoch: 105 [41216/225000 (18%)] Loss: 6664.408203\n",
      "Train Epoch: 105 [45312/225000 (20%)] Loss: 6703.353516\n",
      "Train Epoch: 105 [49408/225000 (22%)] Loss: 6704.046875\n",
      "Train Epoch: 105 [53504/225000 (24%)] Loss: 6543.996094\n",
      "Train Epoch: 105 [57600/225000 (26%)] Loss: 6722.371094\n",
      "Train Epoch: 105 [61696/225000 (27%)] Loss: 6595.285156\n",
      "Train Epoch: 105 [65792/225000 (29%)] Loss: 6625.078125\n",
      "Train Epoch: 105 [69888/225000 (31%)] Loss: 6698.605469\n",
      "Train Epoch: 105 [73984/225000 (33%)] Loss: 6687.539062\n",
      "Train Epoch: 105 [78080/225000 (35%)] Loss: 6709.460938\n",
      "Train Epoch: 105 [82176/225000 (37%)] Loss: 6707.207031\n",
      "Train Epoch: 105 [86272/225000 (38%)] Loss: 6731.947266\n",
      "Train Epoch: 105 [90368/225000 (40%)] Loss: 8491.183594\n",
      "Train Epoch: 105 [94464/225000 (42%)] Loss: 6641.951172\n",
      "Train Epoch: 105 [98560/225000 (44%)] Loss: 6599.642578\n",
      "Train Epoch: 105 [102656/225000 (46%)] Loss: 6763.968750\n",
      "Train Epoch: 105 [106752/225000 (47%)] Loss: 6818.234375\n",
      "Train Epoch: 105 [110848/225000 (49%)] Loss: 6704.544922\n",
      "Train Epoch: 105 [114944/225000 (51%)] Loss: 6662.175781\n",
      "Train Epoch: 105 [119040/225000 (53%)] Loss: 6723.044922\n",
      "Train Epoch: 105 [123136/225000 (55%)] Loss: 6742.037109\n",
      "Train Epoch: 105 [127232/225000 (57%)] Loss: 6696.384766\n",
      "Train Epoch: 105 [131328/225000 (58%)] Loss: 6688.742188\n",
      "Train Epoch: 105 [135424/225000 (60%)] Loss: 6721.447266\n",
      "Train Epoch: 105 [139520/225000 (62%)] Loss: 6700.986328\n",
      "Train Epoch: 105 [143616/225000 (64%)] Loss: 6602.527344\n",
      "Train Epoch: 105 [147712/225000 (66%)] Loss: 6719.144531\n",
      "Train Epoch: 105 [151808/225000 (67%)] Loss: 6618.250000\n",
      "Train Epoch: 105 [155904/225000 (69%)] Loss: 6608.583984\n",
      "Train Epoch: 105 [160000/225000 (71%)] Loss: 6815.236328\n",
      "Train Epoch: 105 [164096/225000 (73%)] Loss: 6593.320312\n",
      "Train Epoch: 105 [168192/225000 (75%)] Loss: 6751.615234\n",
      "Train Epoch: 105 [172288/225000 (77%)] Loss: 6747.619141\n",
      "Train Epoch: 105 [176384/225000 (78%)] Loss: 6863.626953\n",
      "Train Epoch: 105 [180480/225000 (80%)] Loss: 6630.007812\n",
      "Train Epoch: 105 [184576/225000 (82%)] Loss: 6735.085938\n",
      "Train Epoch: 105 [188672/225000 (84%)] Loss: 6794.466797\n",
      "Train Epoch: 105 [192768/225000 (86%)] Loss: 6650.589844\n",
      "Train Epoch: 105 [196864/225000 (87%)] Loss: 6653.126953\n",
      "Train Epoch: 105 [200960/225000 (89%)] Loss: 6621.375000\n",
      "Train Epoch: 105 [205056/225000 (91%)] Loss: 6808.130859\n",
      "Train Epoch: 105 [209152/225000 (93%)] Loss: 6959.705078\n",
      "Train Epoch: 105 [213248/225000 (95%)] Loss: 6648.439453\n",
      "Train Epoch: 105 [217344/225000 (97%)] Loss: 6558.111328\n",
      "Train Epoch: 105 [221440/225000 (98%)] Loss: 6662.054688\n",
      "    epoch          : 105\n",
      "    loss           : 6855.186801185651\n",
      "    val_loss       : 6748.174172237212\n",
      "Train Epoch: 106 [256/225000 (0%)] Loss: 6685.324219\n",
      "Train Epoch: 106 [4352/225000 (2%)] Loss: 6590.673828\n",
      "Train Epoch: 106 [8448/225000 (4%)] Loss: 6651.529297\n",
      "Train Epoch: 106 [12544/225000 (6%)] Loss: 6871.640625\n",
      "Train Epoch: 106 [16640/225000 (7%)] Loss: 6681.052734\n",
      "Train Epoch: 106 [20736/225000 (9%)] Loss: 6755.589844\n",
      "Train Epoch: 106 [24832/225000 (11%)] Loss: 6827.998047\n",
      "Train Epoch: 106 [28928/225000 (13%)] Loss: 6695.765625\n",
      "Train Epoch: 106 [33024/225000 (15%)] Loss: 6693.435547\n",
      "Train Epoch: 106 [37120/225000 (16%)] Loss: 6731.785156\n",
      "Train Epoch: 106 [41216/225000 (18%)] Loss: 6773.130859\n",
      "Train Epoch: 106 [45312/225000 (20%)] Loss: 26033.470703\n",
      "Train Epoch: 106 [49408/225000 (22%)] Loss: 6659.412109\n",
      "Train Epoch: 106 [53504/225000 (24%)] Loss: 6668.271484\n",
      "Train Epoch: 106 [57600/225000 (26%)] Loss: 6656.611328\n",
      "Train Epoch: 106 [61696/225000 (27%)] Loss: 6621.904297\n",
      "Train Epoch: 106 [65792/225000 (29%)] Loss: 6604.578125\n",
      "Train Epoch: 106 [69888/225000 (31%)] Loss: 6554.099609\n",
      "Train Epoch: 106 [73984/225000 (33%)] Loss: 6633.917969\n",
      "Train Epoch: 106 [78080/225000 (35%)] Loss: 6680.878906\n",
      "Train Epoch: 106 [82176/225000 (37%)] Loss: 6731.023438\n",
      "Train Epoch: 106 [86272/225000 (38%)] Loss: 6703.529297\n",
      "Train Epoch: 106 [90368/225000 (40%)] Loss: 6634.222656\n",
      "Train Epoch: 106 [94464/225000 (42%)] Loss: 6637.761719\n",
      "Train Epoch: 106 [98560/225000 (44%)] Loss: 6616.544922\n",
      "Train Epoch: 106 [102656/225000 (46%)] Loss: 6764.082031\n",
      "Train Epoch: 106 [106752/225000 (47%)] Loss: 6719.324219\n",
      "Train Epoch: 106 [110848/225000 (49%)] Loss: 6727.875000\n",
      "Train Epoch: 106 [114944/225000 (51%)] Loss: 6712.417969\n",
      "Train Epoch: 106 [119040/225000 (53%)] Loss: 6653.681641\n",
      "Train Epoch: 106 [123136/225000 (55%)] Loss: 6756.039062\n",
      "Train Epoch: 106 [127232/225000 (57%)] Loss: 6788.269531\n",
      "Train Epoch: 106 [131328/225000 (58%)] Loss: 6735.671875\n",
      "Train Epoch: 106 [135424/225000 (60%)] Loss: 6827.335938\n",
      "Train Epoch: 106 [139520/225000 (62%)] Loss: 6558.691406\n",
      "Train Epoch: 106 [143616/225000 (64%)] Loss: 6739.476562\n",
      "Train Epoch: 106 [147712/225000 (66%)] Loss: 6633.523438\n",
      "Train Epoch: 106 [151808/225000 (67%)] Loss: 6799.732422\n",
      "Train Epoch: 106 [155904/225000 (69%)] Loss: 6658.250000\n",
      "Train Epoch: 106 [160000/225000 (71%)] Loss: 6724.789062\n",
      "Train Epoch: 106 [164096/225000 (73%)] Loss: 6787.341797\n",
      "Train Epoch: 106 [168192/225000 (75%)] Loss: 6742.183594\n",
      "Train Epoch: 106 [172288/225000 (77%)] Loss: 6663.460938\n",
      "Train Epoch: 106 [176384/225000 (78%)] Loss: 6705.074219\n",
      "Train Epoch: 106 [180480/225000 (80%)] Loss: 6772.677734\n",
      "Train Epoch: 106 [184576/225000 (82%)] Loss: 6652.970703\n",
      "Train Epoch: 106 [188672/225000 (84%)] Loss: 6677.718750\n",
      "Train Epoch: 106 [192768/225000 (86%)] Loss: 6838.785156\n",
      "Train Epoch: 106 [196864/225000 (87%)] Loss: 6720.816406\n",
      "Train Epoch: 106 [200960/225000 (89%)] Loss: 6872.441406\n",
      "Train Epoch: 106 [205056/225000 (91%)] Loss: 6733.644531\n",
      "Train Epoch: 106 [209152/225000 (93%)] Loss: 6603.253906\n",
      "Train Epoch: 106 [213248/225000 (95%)] Loss: 6697.994141\n",
      "Train Epoch: 106 [217344/225000 (97%)] Loss: 6658.640625\n",
      "Train Epoch: 106 [221440/225000 (98%)] Loss: 6546.476562\n",
      "    epoch          : 106\n",
      "    loss           : 6860.15255706058\n",
      "    val_loss       : 6792.700156429592\n",
      "Train Epoch: 107 [256/225000 (0%)] Loss: 6676.556641\n",
      "Train Epoch: 107 [4352/225000 (2%)] Loss: 6675.333984\n",
      "Train Epoch: 107 [8448/225000 (4%)] Loss: 6597.800781\n",
      "Train Epoch: 107 [12544/225000 (6%)] Loss: 6798.648438\n",
      "Train Epoch: 107 [16640/225000 (7%)] Loss: 6771.669922\n",
      "Train Epoch: 107 [20736/225000 (9%)] Loss: 6714.189453\n",
      "Train Epoch: 107 [24832/225000 (11%)] Loss: 6698.177734\n",
      "Train Epoch: 107 [28928/225000 (13%)] Loss: 6625.167969\n",
      "Train Epoch: 107 [33024/225000 (15%)] Loss: 6743.361328\n",
      "Train Epoch: 107 [37120/225000 (16%)] Loss: 6650.400391\n",
      "Train Epoch: 107 [41216/225000 (18%)] Loss: 6761.667969\n",
      "Train Epoch: 107 [45312/225000 (20%)] Loss: 6761.281250\n",
      "Train Epoch: 107 [49408/225000 (22%)] Loss: 6571.003906\n",
      "Train Epoch: 107 [53504/225000 (24%)] Loss: 6760.148438\n",
      "Train Epoch: 107 [57600/225000 (26%)] Loss: 6670.984375\n",
      "Train Epoch: 107 [61696/225000 (27%)] Loss: 6725.144531\n",
      "Train Epoch: 107 [65792/225000 (29%)] Loss: 6595.017578\n",
      "Train Epoch: 107 [69888/225000 (31%)] Loss: 6537.957031\n",
      "Train Epoch: 107 [73984/225000 (33%)] Loss: 6724.945312\n",
      "Train Epoch: 107 [78080/225000 (35%)] Loss: 6734.345703\n",
      "Train Epoch: 107 [82176/225000 (37%)] Loss: 6566.855469\n",
      "Train Epoch: 107 [86272/225000 (38%)] Loss: 6726.294922\n",
      "Train Epoch: 107 [90368/225000 (40%)] Loss: 6728.031250\n",
      "Train Epoch: 107 [94464/225000 (42%)] Loss: 6765.173828\n",
      "Train Epoch: 107 [98560/225000 (44%)] Loss: 6494.884766\n",
      "Train Epoch: 107 [102656/225000 (46%)] Loss: 6783.576172\n",
      "Train Epoch: 107 [106752/225000 (47%)] Loss: 6710.628906\n",
      "Train Epoch: 107 [110848/225000 (49%)] Loss: 6625.671875\n",
      "Train Epoch: 107 [114944/225000 (51%)] Loss: 6624.525391\n",
      "Train Epoch: 107 [119040/225000 (53%)] Loss: 6644.728516\n",
      "Train Epoch: 107 [123136/225000 (55%)] Loss: 6707.130859\n",
      "Train Epoch: 107 [127232/225000 (57%)] Loss: 6707.949219\n",
      "Train Epoch: 107 [131328/225000 (58%)] Loss: 6624.134766\n",
      "Train Epoch: 107 [135424/225000 (60%)] Loss: 6734.746094\n",
      "Train Epoch: 107 [139520/225000 (62%)] Loss: 6696.882812\n",
      "Train Epoch: 107 [143616/225000 (64%)] Loss: 6815.226562\n",
      "Train Epoch: 107 [147712/225000 (66%)] Loss: 8433.517578\n",
      "Train Epoch: 107 [151808/225000 (67%)] Loss: 6748.464844\n",
      "Train Epoch: 107 [155904/225000 (69%)] Loss: 6675.814453\n",
      "Train Epoch: 107 [160000/225000 (71%)] Loss: 6623.732422\n",
      "Train Epoch: 107 [164096/225000 (73%)] Loss: 6477.998047\n",
      "Train Epoch: 107 [168192/225000 (75%)] Loss: 6972.613281\n",
      "Train Epoch: 107 [172288/225000 (77%)] Loss: 6811.076172\n",
      "Train Epoch: 107 [176384/225000 (78%)] Loss: 6577.109375\n",
      "Train Epoch: 107 [180480/225000 (80%)] Loss: 6824.117188\n",
      "Train Epoch: 107 [184576/225000 (82%)] Loss: 6628.050781\n",
      "Train Epoch: 107 [188672/225000 (84%)] Loss: 6729.703125\n",
      "Train Epoch: 107 [192768/225000 (86%)] Loss: 6698.384766\n",
      "Train Epoch: 107 [196864/225000 (87%)] Loss: 6857.021484\n",
      "Train Epoch: 107 [200960/225000 (89%)] Loss: 6755.521484\n",
      "Train Epoch: 107 [205056/225000 (91%)] Loss: 6732.208984\n",
      "Train Epoch: 107 [209152/225000 (93%)] Loss: 6709.457031\n",
      "Train Epoch: 107 [213248/225000 (95%)] Loss: 6796.052734\n",
      "Train Epoch: 107 [217344/225000 (97%)] Loss: 6746.566406\n",
      "Train Epoch: 107 [221440/225000 (98%)] Loss: 6646.449219\n",
      "    epoch          : 107\n",
      "    loss           : 6903.219123293516\n",
      "    val_loss       : 6911.248906092985\n",
      "Train Epoch: 108 [256/225000 (0%)] Loss: 6573.867188\n",
      "Train Epoch: 108 [4352/225000 (2%)] Loss: 6711.394531\n",
      "Train Epoch: 108 [8448/225000 (4%)] Loss: 6691.646484\n",
      "Train Epoch: 108 [12544/225000 (6%)] Loss: 6719.248047\n",
      "Train Epoch: 108 [16640/225000 (7%)] Loss: 6682.193359\n",
      "Train Epoch: 108 [20736/225000 (9%)] Loss: 6637.701172\n",
      "Train Epoch: 108 [24832/225000 (11%)] Loss: 6833.457031\n",
      "Train Epoch: 108 [28928/225000 (13%)] Loss: 6717.666016\n",
      "Train Epoch: 108 [33024/225000 (15%)] Loss: 6758.244141\n",
      "Train Epoch: 108 [37120/225000 (16%)] Loss: 6630.230469\n",
      "Train Epoch: 108 [41216/225000 (18%)] Loss: 6782.349609\n",
      "Train Epoch: 108 [45312/225000 (20%)] Loss: 6659.429688\n",
      "Train Epoch: 108 [49408/225000 (22%)] Loss: 6762.095703\n",
      "Train Epoch: 108 [53504/225000 (24%)] Loss: 6687.605469\n",
      "Train Epoch: 108 [57600/225000 (26%)] Loss: 6859.316406\n",
      "Train Epoch: 108 [61696/225000 (27%)] Loss: 6763.521484\n",
      "Train Epoch: 108 [65792/225000 (29%)] Loss: 6705.292969\n",
      "Train Epoch: 108 [69888/225000 (31%)] Loss: 6712.902344\n",
      "Train Epoch: 108 [73984/225000 (33%)] Loss: 6653.529297\n",
      "Train Epoch: 108 [78080/225000 (35%)] Loss: 6579.480469\n",
      "Train Epoch: 108 [82176/225000 (37%)] Loss: 8412.648438\n",
      "Train Epoch: 108 [86272/225000 (38%)] Loss: 6561.501953\n",
      "Train Epoch: 108 [90368/225000 (40%)] Loss: 6636.242188\n",
      "Train Epoch: 108 [94464/225000 (42%)] Loss: 6756.353516\n",
      "Train Epoch: 108 [98560/225000 (44%)] Loss: 6643.300781\n",
      "Train Epoch: 108 [102656/225000 (46%)] Loss: 6567.757812\n",
      "Train Epoch: 108 [106752/225000 (47%)] Loss: 6670.050781\n",
      "Train Epoch: 108 [110848/225000 (49%)] Loss: 6752.125000\n",
      "Train Epoch: 108 [114944/225000 (51%)] Loss: 6648.753906\n",
      "Train Epoch: 108 [119040/225000 (53%)] Loss: 6642.388672\n",
      "Train Epoch: 108 [123136/225000 (55%)] Loss: 6837.753906\n",
      "Train Epoch: 108 [127232/225000 (57%)] Loss: 6747.185547\n",
      "Train Epoch: 108 [131328/225000 (58%)] Loss: 6810.109375\n",
      "Train Epoch: 108 [135424/225000 (60%)] Loss: 6859.435547\n",
      "Train Epoch: 108 [139520/225000 (62%)] Loss: 6728.712891\n",
      "Train Epoch: 108 [143616/225000 (64%)] Loss: 6602.501953\n",
      "Train Epoch: 108 [147712/225000 (66%)] Loss: 6785.103516\n",
      "Train Epoch: 108 [151808/225000 (67%)] Loss: 6644.701172\n",
      "Train Epoch: 108 [155904/225000 (69%)] Loss: 6644.728516\n",
      "Train Epoch: 108 [160000/225000 (71%)] Loss: 6849.013672\n",
      "Train Epoch: 108 [164096/225000 (73%)] Loss: 6604.828125\n",
      "Train Epoch: 108 [168192/225000 (75%)] Loss: 6670.400391\n",
      "Train Epoch: 108 [172288/225000 (77%)] Loss: 6655.757812\n",
      "Train Epoch: 108 [176384/225000 (78%)] Loss: 6601.574219\n",
      "Train Epoch: 108 [180480/225000 (80%)] Loss: 6639.935547\n",
      "Train Epoch: 108 [184576/225000 (82%)] Loss: 29251.591797\n",
      "Train Epoch: 108 [188672/225000 (84%)] Loss: 6564.863281\n",
      "Train Epoch: 108 [192768/225000 (86%)] Loss: 6816.958984\n",
      "Train Epoch: 108 [196864/225000 (87%)] Loss: 6602.988281\n",
      "Train Epoch: 108 [200960/225000 (89%)] Loss: 6562.322266\n",
      "Train Epoch: 108 [205056/225000 (91%)] Loss: 6598.017578\n",
      "Train Epoch: 108 [209152/225000 (93%)] Loss: 6528.701172\n",
      "Train Epoch: 108 [213248/225000 (95%)] Loss: 7016.232422\n",
      "Train Epoch: 108 [217344/225000 (97%)] Loss: 6672.728516\n",
      "Train Epoch: 108 [221440/225000 (98%)] Loss: 6731.699219\n",
      "    epoch          : 108\n",
      "    loss           : 6800.805320765785\n",
      "    val_loss       : 6731.491824879939\n",
      "Train Epoch: 109 [256/225000 (0%)] Loss: 6584.785156\n",
      "Train Epoch: 109 [4352/225000 (2%)] Loss: 6615.642578\n",
      "Train Epoch: 109 [8448/225000 (4%)] Loss: 6667.207031\n",
      "Train Epoch: 109 [12544/225000 (6%)] Loss: 6760.835938\n",
      "Train Epoch: 109 [16640/225000 (7%)] Loss: 6632.433594\n",
      "Train Epoch: 109 [20736/225000 (9%)] Loss: 6646.160156\n",
      "Train Epoch: 109 [24832/225000 (11%)] Loss: 6636.734375\n",
      "Train Epoch: 109 [28928/225000 (13%)] Loss: 6548.015625\n",
      "Train Epoch: 109 [33024/225000 (15%)] Loss: 6821.699219\n",
      "Train Epoch: 109 [37120/225000 (16%)] Loss: 6594.326172\n",
      "Train Epoch: 109 [41216/225000 (18%)] Loss: 6475.220703\n",
      "Train Epoch: 109 [45312/225000 (20%)] Loss: 6660.617188\n",
      "Train Epoch: 109 [49408/225000 (22%)] Loss: 6678.562500\n",
      "Train Epoch: 109 [53504/225000 (24%)] Loss: 6697.583984\n",
      "Train Epoch: 109 [57600/225000 (26%)] Loss: 6749.250000\n",
      "Train Epoch: 109 [61696/225000 (27%)] Loss: 6659.271484\n",
      "Train Epoch: 109 [65792/225000 (29%)] Loss: 6683.748047\n",
      "Train Epoch: 109 [69888/225000 (31%)] Loss: 6833.597656\n",
      "Train Epoch: 109 [73984/225000 (33%)] Loss: 6640.931641\n",
      "Train Epoch: 109 [78080/225000 (35%)] Loss: 6810.509766\n",
      "Train Epoch: 109 [82176/225000 (37%)] Loss: 6705.666016\n",
      "Train Epoch: 109 [86272/225000 (38%)] Loss: 6578.628906\n",
      "Train Epoch: 109 [90368/225000 (40%)] Loss: 6558.109375\n",
      "Train Epoch: 109 [94464/225000 (42%)] Loss: 6545.718750\n",
      "Train Epoch: 109 [98560/225000 (44%)] Loss: 6809.242188\n",
      "Train Epoch: 109 [102656/225000 (46%)] Loss: 6692.023438\n",
      "Train Epoch: 109 [106752/225000 (47%)] Loss: 6732.335938\n",
      "Train Epoch: 109 [110848/225000 (49%)] Loss: 6648.535156\n",
      "Train Epoch: 109 [114944/225000 (51%)] Loss: 6772.261719\n",
      "Train Epoch: 109 [119040/225000 (53%)] Loss: 6815.466797\n",
      "Train Epoch: 109 [123136/225000 (55%)] Loss: 6762.681641\n",
      "Train Epoch: 109 [127232/225000 (57%)] Loss: 6558.861328\n",
      "Train Epoch: 109 [131328/225000 (58%)] Loss: 6827.705078\n",
      "Train Epoch: 109 [135424/225000 (60%)] Loss: 6633.699219\n",
      "Train Epoch: 109 [139520/225000 (62%)] Loss: 6590.744141\n",
      "Train Epoch: 109 [143616/225000 (64%)] Loss: 6664.529297\n",
      "Train Epoch: 109 [147712/225000 (66%)] Loss: 6692.414062\n",
      "Train Epoch: 109 [151808/225000 (67%)] Loss: 6549.490234\n",
      "Train Epoch: 109 [155904/225000 (69%)] Loss: 6675.298828\n",
      "Train Epoch: 109 [160000/225000 (71%)] Loss: 6594.269531\n",
      "Train Epoch: 109 [164096/225000 (73%)] Loss: 6554.046875\n",
      "Train Epoch: 109 [168192/225000 (75%)] Loss: 6794.117188\n",
      "Train Epoch: 109 [172288/225000 (77%)] Loss: 6772.955078\n",
      "Train Epoch: 109 [176384/225000 (78%)] Loss: 6650.917969\n",
      "Train Epoch: 109 [180480/225000 (80%)] Loss: 6682.445312\n",
      "Train Epoch: 109 [184576/225000 (82%)] Loss: 6664.513672\n",
      "Train Epoch: 109 [188672/225000 (84%)] Loss: 6723.037109\n",
      "Train Epoch: 109 [192768/225000 (86%)] Loss: 6780.582031\n",
      "Train Epoch: 109 [196864/225000 (87%)] Loss: 6568.275391\n",
      "Train Epoch: 109 [200960/225000 (89%)] Loss: 6743.937500\n",
      "Train Epoch: 109 [205056/225000 (91%)] Loss: 6776.107422\n",
      "Train Epoch: 109 [209152/225000 (93%)] Loss: 6715.951172\n",
      "Train Epoch: 109 [213248/225000 (95%)] Loss: 6512.738281\n",
      "Train Epoch: 109 [217344/225000 (97%)] Loss: 6769.443359\n",
      "Train Epoch: 109 [221440/225000 (98%)] Loss: 6853.886719\n",
      "    epoch          : 109\n",
      "    loss           : 6735.5614134492325\n",
      "    val_loss       : 6743.879605141221\n",
      "Train Epoch: 110 [256/225000 (0%)] Loss: 6617.242188\n",
      "Train Epoch: 110 [4352/225000 (2%)] Loss: 6632.724609\n",
      "Train Epoch: 110 [8448/225000 (4%)] Loss: 6848.042969\n",
      "Train Epoch: 110 [12544/225000 (6%)] Loss: 6681.886719\n",
      "Train Epoch: 110 [16640/225000 (7%)] Loss: 6454.121094\n",
      "Train Epoch: 110 [20736/225000 (9%)] Loss: 6612.462891\n",
      "Train Epoch: 110 [24832/225000 (11%)] Loss: 6652.992188\n",
      "Train Epoch: 110 [28928/225000 (13%)] Loss: 6762.183594\n",
      "Train Epoch: 110 [33024/225000 (15%)] Loss: 6884.658203\n",
      "Train Epoch: 110 [37120/225000 (16%)] Loss: 6579.912109\n",
      "Train Epoch: 110 [41216/225000 (18%)] Loss: 6595.722656\n",
      "Train Epoch: 110 [45312/225000 (20%)] Loss: 6710.644531\n",
      "Train Epoch: 110 [49408/225000 (22%)] Loss: 6640.875000\n",
      "Train Epoch: 110 [53504/225000 (24%)] Loss: 6714.611328\n",
      "Train Epoch: 110 [57600/225000 (26%)] Loss: 6624.945312\n",
      "Train Epoch: 110 [61696/225000 (27%)] Loss: 6879.464844\n",
      "Train Epoch: 110 [65792/225000 (29%)] Loss: 6643.761719\n",
      "Train Epoch: 110 [69888/225000 (31%)] Loss: 6675.994141\n",
      "Train Epoch: 110 [73984/225000 (33%)] Loss: 6551.347656\n",
      "Train Epoch: 110 [78080/225000 (35%)] Loss: 6761.109375\n",
      "Train Epoch: 110 [82176/225000 (37%)] Loss: 6844.853516\n",
      "Train Epoch: 110 [86272/225000 (38%)] Loss: 6667.578125\n",
      "Train Epoch: 110 [90368/225000 (40%)] Loss: 6822.169922\n",
      "Train Epoch: 110 [94464/225000 (42%)] Loss: 6565.812500\n",
      "Train Epoch: 110 [98560/225000 (44%)] Loss: 6517.228516\n",
      "Train Epoch: 110 [102656/225000 (46%)] Loss: 6695.589844\n",
      "Train Epoch: 110 [106752/225000 (47%)] Loss: 6600.212891\n",
      "Train Epoch: 110 [110848/225000 (49%)] Loss: 6791.568359\n",
      "Train Epoch: 110 [114944/225000 (51%)] Loss: 6684.562500\n",
      "Train Epoch: 110 [119040/225000 (53%)] Loss: 6652.447266\n",
      "Train Epoch: 110 [123136/225000 (55%)] Loss: 6582.767578\n",
      "Train Epoch: 110 [127232/225000 (57%)] Loss: 6679.529297\n",
      "Train Epoch: 110 [131328/225000 (58%)] Loss: 6773.083984\n",
      "Train Epoch: 110 [135424/225000 (60%)] Loss: 6676.818359\n",
      "Train Epoch: 110 [139520/225000 (62%)] Loss: 6746.322266\n",
      "Train Epoch: 110 [143616/225000 (64%)] Loss: 6761.630859\n",
      "Train Epoch: 110 [147712/225000 (66%)] Loss: 6584.324219\n",
      "Train Epoch: 110 [151808/225000 (67%)] Loss: 6781.933594\n",
      "Train Epoch: 110 [155904/225000 (69%)] Loss: 6610.816406\n",
      "Train Epoch: 110 [160000/225000 (71%)] Loss: 6645.595703\n",
      "Train Epoch: 110 [164096/225000 (73%)] Loss: 6665.800781\n",
      "Train Epoch: 110 [168192/225000 (75%)] Loss: 6774.632812\n",
      "Train Epoch: 110 [172288/225000 (77%)] Loss: 6673.947266\n",
      "Train Epoch: 110 [176384/225000 (78%)] Loss: 6655.474609\n",
      "Train Epoch: 110 [180480/225000 (80%)] Loss: 6758.671875\n",
      "Train Epoch: 110 [184576/225000 (82%)] Loss: 6742.134766\n",
      "Train Epoch: 110 [188672/225000 (84%)] Loss: 6745.861328\n",
      "Train Epoch: 110 [192768/225000 (86%)] Loss: 6768.478516\n",
      "Train Epoch: 110 [196864/225000 (87%)] Loss: 6771.009766\n",
      "Train Epoch: 110 [200960/225000 (89%)] Loss: 6690.111328\n",
      "Train Epoch: 110 [205056/225000 (91%)] Loss: 6621.142578\n",
      "Train Epoch: 110 [209152/225000 (93%)] Loss: 6618.962891\n",
      "Train Epoch: 110 [213248/225000 (95%)] Loss: 6916.914062\n",
      "Train Epoch: 110 [217344/225000 (97%)] Loss: 6571.707031\n",
      "Train Epoch: 110 [221440/225000 (98%)] Loss: 6764.931641\n",
      "    epoch          : 110\n",
      "    loss           : 6737.302671048422\n",
      "    val_loss       : 6916.322816003342\n",
      "Train Epoch: 111 [256/225000 (0%)] Loss: 6747.125000\n",
      "Train Epoch: 111 [4352/225000 (2%)] Loss: 6798.373047\n",
      "Train Epoch: 111 [8448/225000 (4%)] Loss: 6597.642578\n",
      "Train Epoch: 111 [12544/225000 (6%)] Loss: 6677.458984\n",
      "Train Epoch: 111 [16640/225000 (7%)] Loss: 6695.910156\n",
      "Train Epoch: 111 [20736/225000 (9%)] Loss: 6631.351562\n",
      "Train Epoch: 111 [24832/225000 (11%)] Loss: 6652.167969\n",
      "Train Epoch: 111 [28928/225000 (13%)] Loss: 6670.205078\n",
      "Train Epoch: 111 [33024/225000 (15%)] Loss: 6701.427734\n",
      "Train Epoch: 111 [37120/225000 (16%)] Loss: 6703.125000\n",
      "Train Epoch: 111 [41216/225000 (18%)] Loss: 6607.847656\n",
      "Train Epoch: 111 [45312/225000 (20%)] Loss: 6697.515625\n",
      "Train Epoch: 111 [49408/225000 (22%)] Loss: 6735.683594\n",
      "Train Epoch: 111 [53504/225000 (24%)] Loss: 6719.070312\n",
      "Train Epoch: 111 [57600/225000 (26%)] Loss: 6685.501953\n",
      "Train Epoch: 111 [61696/225000 (27%)] Loss: 6613.906250\n",
      "Train Epoch: 111 [65792/225000 (29%)] Loss: 6624.117188\n",
      "Train Epoch: 111 [69888/225000 (31%)] Loss: 6586.533203\n",
      "Train Epoch: 111 [73984/225000 (33%)] Loss: 6697.851562\n",
      "Train Epoch: 111 [78080/225000 (35%)] Loss: 6446.687500\n",
      "Train Epoch: 111 [82176/225000 (37%)] Loss: 6606.537109\n",
      "Train Epoch: 111 [86272/225000 (38%)] Loss: 6761.972656\n",
      "Train Epoch: 111 [90368/225000 (40%)] Loss: 6598.582031\n",
      "Train Epoch: 111 [94464/225000 (42%)] Loss: 6455.568359\n",
      "Train Epoch: 111 [98560/225000 (44%)] Loss: 6683.531250\n",
      "Train Epoch: 111 [102656/225000 (46%)] Loss: 6728.427734\n",
      "Train Epoch: 111 [106752/225000 (47%)] Loss: 6621.835938\n",
      "Train Epoch: 111 [110848/225000 (49%)] Loss: 6764.154297\n",
      "Train Epoch: 111 [114944/225000 (51%)] Loss: 6721.226562\n",
      "Train Epoch: 111 [119040/225000 (53%)] Loss: 6640.365234\n",
      "Train Epoch: 111 [123136/225000 (55%)] Loss: 6571.968750\n",
      "Train Epoch: 111 [127232/225000 (57%)] Loss: 6604.820312\n",
      "Train Epoch: 111 [131328/225000 (58%)] Loss: 6787.138672\n",
      "Train Epoch: 111 [135424/225000 (60%)] Loss: 6674.121094\n",
      "Train Epoch: 111 [139520/225000 (62%)] Loss: 6660.669922\n",
      "Train Epoch: 111 [143616/225000 (64%)] Loss: 6691.789062\n",
      "Train Epoch: 111 [147712/225000 (66%)] Loss: 6732.685547\n",
      "Train Epoch: 111 [151808/225000 (67%)] Loss: 6538.271484\n",
      "Train Epoch: 111 [155904/225000 (69%)] Loss: 6767.990234\n",
      "Train Epoch: 111 [160000/225000 (71%)] Loss: 6473.685547\n",
      "Train Epoch: 111 [164096/225000 (73%)] Loss: 6740.308594\n",
      "Train Epoch: 111 [168192/225000 (75%)] Loss: 6831.310547\n",
      "Train Epoch: 111 [172288/225000 (77%)] Loss: 6680.140625\n",
      "Train Epoch: 111 [176384/225000 (78%)] Loss: 6679.552734\n",
      "Train Epoch: 111 [180480/225000 (80%)] Loss: 6737.130859\n",
      "Train Epoch: 111 [184576/225000 (82%)] Loss: 6723.625000\n",
      "Train Epoch: 111 [188672/225000 (84%)] Loss: 6737.746094\n",
      "Train Epoch: 111 [192768/225000 (86%)] Loss: 6747.281250\n",
      "Train Epoch: 111 [196864/225000 (87%)] Loss: 6722.941406\n",
      "Train Epoch: 111 [200960/225000 (89%)] Loss: 6594.210938\n",
      "Train Epoch: 111 [205056/225000 (91%)] Loss: 6710.804688\n",
      "Train Epoch: 111 [209152/225000 (93%)] Loss: 6660.949219\n",
      "Train Epoch: 111 [213248/225000 (95%)] Loss: 6638.611328\n",
      "Train Epoch: 111 [217344/225000 (97%)] Loss: 6619.689453\n",
      "Train Epoch: 111 [221440/225000 (98%)] Loss: 6711.496094\n",
      "    epoch          : 111\n",
      "    loss           : 6823.599454947028\n",
      "    val_loss       : 6706.688919999162\n",
      "Train Epoch: 112 [256/225000 (0%)] Loss: 6703.140625\n",
      "Train Epoch: 112 [4352/225000 (2%)] Loss: 6568.529297\n",
      "Train Epoch: 112 [8448/225000 (4%)] Loss: 6753.589844\n",
      "Train Epoch: 112 [12544/225000 (6%)] Loss: 6766.587891\n",
      "Train Epoch: 112 [16640/225000 (7%)] Loss: 6823.599609\n",
      "Train Epoch: 112 [20736/225000 (9%)] Loss: 6686.287109\n",
      "Train Epoch: 112 [24832/225000 (11%)] Loss: 6564.289062\n",
      "Train Epoch: 112 [28928/225000 (13%)] Loss: 6707.867188\n",
      "Train Epoch: 112 [33024/225000 (15%)] Loss: 6746.779297\n",
      "Train Epoch: 112 [37120/225000 (16%)] Loss: 6575.998047\n",
      "Train Epoch: 112 [41216/225000 (18%)] Loss: 6713.541016\n",
      "Train Epoch: 112 [45312/225000 (20%)] Loss: 6648.560547\n",
      "Train Epoch: 112 [49408/225000 (22%)] Loss: 6765.496094\n",
      "Train Epoch: 112 [53504/225000 (24%)] Loss: 6800.736328\n",
      "Train Epoch: 112 [57600/225000 (26%)] Loss: 6729.511719\n",
      "Train Epoch: 112 [61696/225000 (27%)] Loss: 6601.099609\n",
      "Train Epoch: 112 [65792/225000 (29%)] Loss: 6721.105469\n",
      "Train Epoch: 112 [69888/225000 (31%)] Loss: 6491.615234\n",
      "Train Epoch: 112 [73984/225000 (33%)] Loss: 6673.072266\n",
      "Train Epoch: 112 [78080/225000 (35%)] Loss: 6625.347656\n",
      "Train Epoch: 112 [82176/225000 (37%)] Loss: 6648.234375\n",
      "Train Epoch: 112 [86272/225000 (38%)] Loss: 6684.597656\n",
      "Train Epoch: 112 [90368/225000 (40%)] Loss: 6602.650391\n",
      "Train Epoch: 112 [94464/225000 (42%)] Loss: 6633.066406\n",
      "Train Epoch: 112 [98560/225000 (44%)] Loss: 6625.554688\n",
      "Train Epoch: 112 [102656/225000 (46%)] Loss: 6680.947266\n",
      "Train Epoch: 112 [106752/225000 (47%)] Loss: 6670.154297\n",
      "Train Epoch: 112 [110848/225000 (49%)] Loss: 6702.031250\n",
      "Train Epoch: 112 [114944/225000 (51%)] Loss: 8414.253906\n",
      "Train Epoch: 112 [119040/225000 (53%)] Loss: 6567.289062\n",
      "Train Epoch: 112 [123136/225000 (55%)] Loss: 6667.097656\n",
      "Train Epoch: 112 [127232/225000 (57%)] Loss: 6613.962891\n",
      "Train Epoch: 112 [131328/225000 (58%)] Loss: 6675.335938\n",
      "Train Epoch: 112 [135424/225000 (60%)] Loss: 6607.248047\n",
      "Train Epoch: 112 [139520/225000 (62%)] Loss: 6736.833984\n",
      "Train Epoch: 112 [143616/225000 (64%)] Loss: 6612.000000\n",
      "Train Epoch: 112 [147712/225000 (66%)] Loss: 6669.757812\n",
      "Train Epoch: 112 [151808/225000 (67%)] Loss: 6442.480469\n",
      "Train Epoch: 112 [155904/225000 (69%)] Loss: 6695.767578\n",
      "Train Epoch: 112 [160000/225000 (71%)] Loss: 6667.949219\n",
      "Train Epoch: 112 [164096/225000 (73%)] Loss: 6619.933594\n",
      "Train Epoch: 112 [168192/225000 (75%)] Loss: 6689.412109\n",
      "Train Epoch: 112 [172288/225000 (77%)] Loss: 6588.191406\n",
      "Train Epoch: 112 [176384/225000 (78%)] Loss: 6638.539062\n",
      "Train Epoch: 112 [180480/225000 (80%)] Loss: 6744.126953\n",
      "Train Epoch: 112 [184576/225000 (82%)] Loss: 6725.101562\n",
      "Train Epoch: 112 [188672/225000 (84%)] Loss: 6613.261719\n",
      "Train Epoch: 112 [192768/225000 (86%)] Loss: 6764.900391\n",
      "Train Epoch: 112 [196864/225000 (87%)] Loss: 6703.517578\n",
      "Train Epoch: 112 [200960/225000 (89%)] Loss: 6893.812500\n",
      "Train Epoch: 112 [205056/225000 (91%)] Loss: 6726.998047\n",
      "Train Epoch: 112 [209152/225000 (93%)] Loss: 8359.296875\n",
      "Train Epoch: 112 [213248/225000 (95%)] Loss: 6723.380859\n",
      "Train Epoch: 112 [217344/225000 (97%)] Loss: 6631.652344\n",
      "Train Epoch: 112 [221440/225000 (98%)] Loss: 6558.267578\n",
      "    epoch          : 112\n",
      "    loss           : 6792.377215319255\n",
      "    val_loss       : 6946.246622168288\n",
      "Train Epoch: 113 [256/225000 (0%)] Loss: 6558.556641\n",
      "Train Epoch: 113 [4352/225000 (2%)] Loss: 6693.283203\n",
      "Train Epoch: 113 [8448/225000 (4%)] Loss: 6734.013672\n",
      "Train Epoch: 113 [12544/225000 (6%)] Loss: 6742.853516\n",
      "Train Epoch: 113 [16640/225000 (7%)] Loss: 6596.687500\n",
      "Train Epoch: 113 [20736/225000 (9%)] Loss: 6685.267578\n",
      "Train Epoch: 113 [24832/225000 (11%)] Loss: 6747.937500\n",
      "Train Epoch: 113 [28928/225000 (13%)] Loss: 6670.876953\n",
      "Train Epoch: 113 [33024/225000 (15%)] Loss: 6575.994141\n",
      "Train Epoch: 113 [37120/225000 (16%)] Loss: 6716.265625\n",
      "Train Epoch: 113 [41216/225000 (18%)] Loss: 6621.966797\n",
      "Train Epoch: 113 [45312/225000 (20%)] Loss: 6641.759766\n",
      "Train Epoch: 113 [49408/225000 (22%)] Loss: 6618.109375\n",
      "Train Epoch: 113 [53504/225000 (24%)] Loss: 6689.320312\n",
      "Train Epoch: 113 [57600/225000 (26%)] Loss: 6575.810547\n",
      "Train Epoch: 113 [61696/225000 (27%)] Loss: 6737.900391\n",
      "Train Epoch: 113 [65792/225000 (29%)] Loss: 8426.201172\n",
      "Train Epoch: 113 [69888/225000 (31%)] Loss: 6648.042969\n",
      "Train Epoch: 113 [73984/225000 (33%)] Loss: 6658.695312\n",
      "Train Epoch: 113 [78080/225000 (35%)] Loss: 6618.027344\n",
      "Train Epoch: 113 [82176/225000 (37%)] Loss: 6624.988281\n",
      "Train Epoch: 113 [86272/225000 (38%)] Loss: 6583.863281\n",
      "Train Epoch: 113 [90368/225000 (40%)] Loss: 6493.642578\n",
      "Train Epoch: 113 [94464/225000 (42%)] Loss: 6643.894531\n",
      "Train Epoch: 113 [98560/225000 (44%)] Loss: 6717.941406\n",
      "Train Epoch: 113 [102656/225000 (46%)] Loss: 6703.109375\n",
      "Train Epoch: 113 [106752/225000 (47%)] Loss: 6577.218750\n",
      "Train Epoch: 113 [110848/225000 (49%)] Loss: 6683.550781\n",
      "Train Epoch: 113 [114944/225000 (51%)] Loss: 6507.068359\n",
      "Train Epoch: 113 [119040/225000 (53%)] Loss: 6774.916016\n",
      "Train Epoch: 113 [123136/225000 (55%)] Loss: 8465.248047\n",
      "Train Epoch: 113 [127232/225000 (57%)] Loss: 6669.710938\n",
      "Train Epoch: 113 [131328/225000 (58%)] Loss: 6617.023438\n",
      "Train Epoch: 113 [135424/225000 (60%)] Loss: 6608.888672\n",
      "Train Epoch: 113 [139520/225000 (62%)] Loss: 6759.390625\n",
      "Train Epoch: 113 [143616/225000 (64%)] Loss: 6628.716797\n",
      "Train Epoch: 113 [147712/225000 (66%)] Loss: 6715.642578\n",
      "Train Epoch: 113 [151808/225000 (67%)] Loss: 6807.150391\n",
      "Train Epoch: 113 [155904/225000 (69%)] Loss: 6738.695312\n",
      "Train Epoch: 113 [160000/225000 (71%)] Loss: 6667.675781\n",
      "Train Epoch: 113 [164096/225000 (73%)] Loss: 6704.791016\n",
      "Train Epoch: 113 [168192/225000 (75%)] Loss: 6715.519531\n",
      "Train Epoch: 113 [172288/225000 (77%)] Loss: 6586.171875\n",
      "Train Epoch: 113 [176384/225000 (78%)] Loss: 6712.847656\n",
      "Train Epoch: 113 [180480/225000 (80%)] Loss: 6647.628906\n",
      "Train Epoch: 113 [184576/225000 (82%)] Loss: 6714.605469\n",
      "Train Epoch: 113 [188672/225000 (84%)] Loss: 6747.570312\n",
      "Train Epoch: 113 [192768/225000 (86%)] Loss: 6543.718750\n",
      "Train Epoch: 113 [196864/225000 (87%)] Loss: 6649.837891\n",
      "Train Epoch: 113 [200960/225000 (89%)] Loss: 6606.896484\n",
      "Train Epoch: 113 [205056/225000 (91%)] Loss: 6688.822266\n",
      "Train Epoch: 113 [209152/225000 (93%)] Loss: 6586.132812\n",
      "Train Epoch: 113 [213248/225000 (95%)] Loss: 6656.933594\n",
      "Train Epoch: 113 [217344/225000 (97%)] Loss: 6547.703125\n",
      "Train Epoch: 113 [221440/225000 (98%)] Loss: 6542.826172\n",
      "    epoch          : 113\n",
      "    loss           : 6761.043926425626\n",
      "    val_loss       : 6934.320773768181\n",
      "Train Epoch: 114 [256/225000 (0%)] Loss: 6727.355469\n",
      "Train Epoch: 114 [4352/225000 (2%)] Loss: 6841.779297\n",
      "Train Epoch: 114 [8448/225000 (4%)] Loss: 6656.556641\n",
      "Train Epoch: 114 [12544/225000 (6%)] Loss: 6724.937500\n",
      "Train Epoch: 114 [16640/225000 (7%)] Loss: 6692.650391\n",
      "Train Epoch: 114 [20736/225000 (9%)] Loss: 6594.171875\n",
      "Train Epoch: 114 [24832/225000 (11%)] Loss: 6611.703125\n",
      "Train Epoch: 114 [28928/225000 (13%)] Loss: 6817.255859\n",
      "Train Epoch: 114 [33024/225000 (15%)] Loss: 6642.000000\n",
      "Train Epoch: 114 [37120/225000 (16%)] Loss: 6539.044922\n",
      "Train Epoch: 114 [41216/225000 (18%)] Loss: 6674.035156\n",
      "Train Epoch: 114 [45312/225000 (20%)] Loss: 6713.460938\n",
      "Train Epoch: 114 [49408/225000 (22%)] Loss: 6706.943359\n",
      "Train Epoch: 114 [53504/225000 (24%)] Loss: 6647.414062\n",
      "Train Epoch: 114 [57600/225000 (26%)] Loss: 6582.767578\n",
      "Train Epoch: 114 [61696/225000 (27%)] Loss: 6675.847656\n",
      "Train Epoch: 114 [65792/225000 (29%)] Loss: 6677.701172\n",
      "Train Epoch: 114 [69888/225000 (31%)] Loss: 6846.878906\n",
      "Train Epoch: 114 [73984/225000 (33%)] Loss: 6733.175781\n",
      "Train Epoch: 114 [78080/225000 (35%)] Loss: 6546.373047\n",
      "Train Epoch: 114 [82176/225000 (37%)] Loss: 6632.271484\n",
      "Train Epoch: 114 [86272/225000 (38%)] Loss: 6591.705078\n",
      "Train Epoch: 114 [90368/225000 (40%)] Loss: 6680.593750\n",
      "Train Epoch: 114 [94464/225000 (42%)] Loss: 6717.972656\n",
      "Train Epoch: 114 [98560/225000 (44%)] Loss: 6698.947266\n",
      "Train Epoch: 114 [102656/225000 (46%)] Loss: 6716.484375\n",
      "Train Epoch: 114 [106752/225000 (47%)] Loss: 6661.048828\n",
      "Train Epoch: 114 [110848/225000 (49%)] Loss: 6668.953125\n",
      "Train Epoch: 114 [114944/225000 (51%)] Loss: 6650.894531\n",
      "Train Epoch: 114 [119040/225000 (53%)] Loss: 6614.548828\n",
      "Train Epoch: 114 [123136/225000 (55%)] Loss: 6521.025391\n",
      "Train Epoch: 114 [127232/225000 (57%)] Loss: 6661.685547\n",
      "Train Epoch: 114 [131328/225000 (58%)] Loss: 6732.507812\n",
      "Train Epoch: 114 [135424/225000 (60%)] Loss: 6646.535156\n",
      "Train Epoch: 114 [139520/225000 (62%)] Loss: 6571.494141\n",
      "Train Epoch: 114 [143616/225000 (64%)] Loss: 6757.527344\n",
      "Train Epoch: 114 [147712/225000 (66%)] Loss: 6754.771484\n",
      "Train Epoch: 114 [151808/225000 (67%)] Loss: 6612.769531\n",
      "Train Epoch: 114 [155904/225000 (69%)] Loss: 6607.177734\n",
      "Train Epoch: 114 [160000/225000 (71%)] Loss: 6728.841797\n",
      "Train Epoch: 114 [164096/225000 (73%)] Loss: 6767.253906\n",
      "Train Epoch: 114 [168192/225000 (75%)] Loss: 6709.582031\n",
      "Train Epoch: 114 [172288/225000 (77%)] Loss: 6659.750000\n",
      "Train Epoch: 114 [176384/225000 (78%)] Loss: 6636.013672\n",
      "Train Epoch: 114 [180480/225000 (80%)] Loss: 6565.884766\n",
      "Train Epoch: 114 [184576/225000 (82%)] Loss: 6855.910156\n",
      "Train Epoch: 114 [188672/225000 (84%)] Loss: 6669.875000\n",
      "Train Epoch: 114 [192768/225000 (86%)] Loss: 6589.208984\n",
      "Train Epoch: 114 [196864/225000 (87%)] Loss: 6577.753906\n",
      "Train Epoch: 114 [200960/225000 (89%)] Loss: 6861.744141\n",
      "Train Epoch: 114 [205056/225000 (91%)] Loss: 6747.718750\n",
      "Train Epoch: 114 [209152/225000 (93%)] Loss: 6534.693359\n",
      "Train Epoch: 114 [213248/225000 (95%)] Loss: 6646.175781\n",
      "Train Epoch: 114 [217344/225000 (97%)] Loss: 6504.644531\n",
      "Train Epoch: 114 [221440/225000 (98%)] Loss: 6461.423828\n",
      "    epoch          : 114\n",
      "    loss           : 6779.5862996835895\n",
      "    val_loss       : 6772.736891105467\n",
      "Train Epoch: 115 [256/225000 (0%)] Loss: 6596.673828\n",
      "Train Epoch: 115 [4352/225000 (2%)] Loss: 6811.089844\n",
      "Train Epoch: 115 [8448/225000 (4%)] Loss: 6477.712891\n",
      "Train Epoch: 115 [12544/225000 (6%)] Loss: 6762.841797\n",
      "Train Epoch: 115 [16640/225000 (7%)] Loss: 6693.220703\n",
      "Train Epoch: 115 [20736/225000 (9%)] Loss: 6601.726562\n",
      "Train Epoch: 115 [24832/225000 (11%)] Loss: 6527.333984\n",
      "Train Epoch: 115 [28928/225000 (13%)] Loss: 6612.351562\n",
      "Train Epoch: 115 [33024/225000 (15%)] Loss: 6742.603516\n",
      "Train Epoch: 115 [37120/225000 (16%)] Loss: 6633.646484\n",
      "Train Epoch: 115 [41216/225000 (18%)] Loss: 6522.515625\n",
      "Train Epoch: 115 [45312/225000 (20%)] Loss: 6619.533203\n",
      "Train Epoch: 115 [49408/225000 (22%)] Loss: 6779.035156\n",
      "Train Epoch: 115 [53504/225000 (24%)] Loss: 6733.730469\n",
      "Train Epoch: 115 [57600/225000 (26%)] Loss: 6671.880859\n",
      "Train Epoch: 115 [61696/225000 (27%)] Loss: 6658.259766\n",
      "Train Epoch: 115 [65792/225000 (29%)] Loss: 6465.820312\n",
      "Train Epoch: 115 [69888/225000 (31%)] Loss: 6590.222656\n",
      "Train Epoch: 115 [73984/225000 (33%)] Loss: 6538.705078\n",
      "Train Epoch: 115 [78080/225000 (35%)] Loss: 6716.417969\n",
      "Train Epoch: 115 [82176/225000 (37%)] Loss: 6538.287109\n",
      "Train Epoch: 115 [86272/225000 (38%)] Loss: 6749.365234\n",
      "Train Epoch: 115 [90368/225000 (40%)] Loss: 6650.935547\n",
      "Train Epoch: 115 [94464/225000 (42%)] Loss: 6606.396484\n",
      "Train Epoch: 115 [98560/225000 (44%)] Loss: 6599.052734\n",
      "Train Epoch: 115 [102656/225000 (46%)] Loss: 6465.355469\n",
      "Train Epoch: 115 [106752/225000 (47%)] Loss: 6664.320312\n",
      "Train Epoch: 115 [110848/225000 (49%)] Loss: 6554.601562\n",
      "Train Epoch: 115 [114944/225000 (51%)] Loss: 6637.986328\n",
      "Train Epoch: 115 [119040/225000 (53%)] Loss: 6673.667969\n",
      "Train Epoch: 115 [123136/225000 (55%)] Loss: 6632.833984\n",
      "Train Epoch: 115 [127232/225000 (57%)] Loss: 6641.603516\n",
      "Train Epoch: 115 [131328/225000 (58%)] Loss: 6679.398438\n",
      "Train Epoch: 115 [135424/225000 (60%)] Loss: 6682.550781\n",
      "Train Epoch: 115 [139520/225000 (62%)] Loss: 6737.542969\n",
      "Train Epoch: 115 [143616/225000 (64%)] Loss: 6641.750000\n",
      "Train Epoch: 115 [147712/225000 (66%)] Loss: 6602.783203\n",
      "Train Epoch: 115 [151808/225000 (67%)] Loss: 6571.736328\n",
      "Train Epoch: 115 [155904/225000 (69%)] Loss: 6547.435547\n",
      "Train Epoch: 115 [160000/225000 (71%)] Loss: 6738.822266\n",
      "Train Epoch: 115 [164096/225000 (73%)] Loss: 6612.419922\n",
      "Train Epoch: 115 [168192/225000 (75%)] Loss: 6644.937500\n",
      "Train Epoch: 115 [172288/225000 (77%)] Loss: 6667.710938\n",
      "Train Epoch: 115 [176384/225000 (78%)] Loss: 6771.896484\n",
      "Train Epoch: 115 [180480/225000 (80%)] Loss: 6700.773438\n",
      "Train Epoch: 115 [184576/225000 (82%)] Loss: 6701.062500\n",
      "Train Epoch: 115 [188672/225000 (84%)] Loss: 6604.900391\n",
      "Train Epoch: 115 [192768/225000 (86%)] Loss: 6555.777344\n",
      "Train Epoch: 115 [196864/225000 (87%)] Loss: 6752.769531\n",
      "Train Epoch: 115 [200960/225000 (89%)] Loss: 6650.001953\n",
      "Train Epoch: 115 [205056/225000 (91%)] Loss: 6701.519531\n",
      "Train Epoch: 115 [209152/225000 (93%)] Loss: 6580.548828\n",
      "Train Epoch: 115 [213248/225000 (95%)] Loss: 6814.447266\n",
      "Train Epoch: 115 [217344/225000 (97%)] Loss: 6851.697266\n",
      "Train Epoch: 115 [221440/225000 (98%)] Loss: 6585.339844\n",
      "    epoch          : 115\n",
      "    loss           : 6842.803167662116\n",
      "    val_loss       : 6727.830184820963\n",
      "Train Epoch: 116 [256/225000 (0%)] Loss: 6543.783203\n",
      "Train Epoch: 116 [4352/225000 (2%)] Loss: 6510.595703\n",
      "Train Epoch: 116 [8448/225000 (4%)] Loss: 6668.332031\n",
      "Train Epoch: 116 [12544/225000 (6%)] Loss: 6832.488281\n",
      "Train Epoch: 116 [16640/225000 (7%)] Loss: 6546.548828\n",
      "Train Epoch: 116 [20736/225000 (9%)] Loss: 6557.341797\n",
      "Train Epoch: 116 [24832/225000 (11%)] Loss: 6574.673828\n",
      "Train Epoch: 116 [28928/225000 (13%)] Loss: 6576.062500\n",
      "Train Epoch: 116 [33024/225000 (15%)] Loss: 6627.873047\n",
      "Train Epoch: 116 [37120/225000 (16%)] Loss: 6601.824219\n",
      "Train Epoch: 116 [41216/225000 (18%)] Loss: 6588.845703\n",
      "Train Epoch: 116 [45312/225000 (20%)] Loss: 8497.095703\n",
      "Train Epoch: 116 [49408/225000 (22%)] Loss: 6658.871094\n",
      "Train Epoch: 116 [53504/225000 (24%)] Loss: 6694.427734\n",
      "Train Epoch: 116 [57600/225000 (26%)] Loss: 6650.001953\n",
      "Train Epoch: 116 [61696/225000 (27%)] Loss: 6629.613281\n",
      "Train Epoch: 116 [65792/225000 (29%)] Loss: 6655.552734\n",
      "Train Epoch: 116 [69888/225000 (31%)] Loss: 6723.181641\n",
      "Train Epoch: 116 [73984/225000 (33%)] Loss: 6658.814453\n",
      "Train Epoch: 116 [78080/225000 (35%)] Loss: 6547.291016\n",
      "Train Epoch: 116 [82176/225000 (37%)] Loss: 6636.013672\n",
      "Train Epoch: 116 [86272/225000 (38%)] Loss: 6722.732422\n",
      "Train Epoch: 116 [90368/225000 (40%)] Loss: 6496.091797\n",
      "Train Epoch: 116 [94464/225000 (42%)] Loss: 6678.312500\n",
      "Train Epoch: 116 [98560/225000 (44%)] Loss: 23900.917969\n",
      "Train Epoch: 116 [102656/225000 (46%)] Loss: 6767.126953\n",
      "Train Epoch: 116 [106752/225000 (47%)] Loss: 6553.675781\n",
      "Train Epoch: 116 [110848/225000 (49%)] Loss: 6712.708984\n",
      "Train Epoch: 116 [114944/225000 (51%)] Loss: 6526.021484\n",
      "Train Epoch: 116 [119040/225000 (53%)] Loss: 6454.332031\n",
      "Train Epoch: 116 [123136/225000 (55%)] Loss: 6612.378906\n",
      "Train Epoch: 116 [127232/225000 (57%)] Loss: 6672.683594\n",
      "Train Epoch: 116 [131328/225000 (58%)] Loss: 6667.722656\n",
      "Train Epoch: 116 [135424/225000 (60%)] Loss: 6685.363281\n",
      "Train Epoch: 116 [139520/225000 (62%)] Loss: 6663.361328\n",
      "Train Epoch: 116 [143616/225000 (64%)] Loss: 6540.521484\n",
      "Train Epoch: 116 [147712/225000 (66%)] Loss: 6862.607422\n",
      "Train Epoch: 116 [151808/225000 (67%)] Loss: 6746.070312\n",
      "Train Epoch: 116 [155904/225000 (69%)] Loss: 6567.744141\n",
      "Train Epoch: 116 [160000/225000 (71%)] Loss: 6596.292969\n",
      "Train Epoch: 116 [164096/225000 (73%)] Loss: 6691.574219\n",
      "Train Epoch: 116 [168192/225000 (75%)] Loss: 6563.583984\n",
      "Train Epoch: 116 [172288/225000 (77%)] Loss: 6656.488281\n",
      "Train Epoch: 116 [176384/225000 (78%)] Loss: 6559.302734\n",
      "Train Epoch: 116 [180480/225000 (80%)] Loss: 6572.380859\n",
      "Train Epoch: 116 [184576/225000 (82%)] Loss: 6596.126953\n",
      "Train Epoch: 116 [188672/225000 (84%)] Loss: 6667.689453\n",
      "Train Epoch: 116 [192768/225000 (86%)] Loss: 6763.130859\n",
      "Train Epoch: 116 [196864/225000 (87%)] Loss: 6667.181641\n",
      "Train Epoch: 116 [200960/225000 (89%)] Loss: 6561.263672\n",
      "Train Epoch: 116 [205056/225000 (91%)] Loss: 6744.548828\n",
      "Train Epoch: 116 [209152/225000 (93%)] Loss: 6828.833984\n",
      "Train Epoch: 116 [213248/225000 (95%)] Loss: 6527.953125\n",
      "Train Epoch: 116 [217344/225000 (97%)] Loss: 6595.501953\n",
      "Train Epoch: 116 [221440/225000 (98%)] Loss: 6502.599609\n",
      "    epoch          : 116\n",
      "    loss           : 6735.5163582551195\n",
      "    val_loss       : 6673.787258537448\n",
      "Train Epoch: 117 [256/225000 (0%)] Loss: 6543.236328\n",
      "Train Epoch: 117 [4352/225000 (2%)] Loss: 6653.792969\n",
      "Train Epoch: 117 [8448/225000 (4%)] Loss: 6576.869141\n",
      "Train Epoch: 117 [12544/225000 (6%)] Loss: 6537.783203\n",
      "Train Epoch: 117 [16640/225000 (7%)] Loss: 8608.421875\n",
      "Train Epoch: 117 [20736/225000 (9%)] Loss: 6543.443359\n",
      "Train Epoch: 117 [24832/225000 (11%)] Loss: 6669.318359\n",
      "Train Epoch: 117 [28928/225000 (13%)] Loss: 6511.292969\n",
      "Train Epoch: 117 [33024/225000 (15%)] Loss: 6513.386719\n",
      "Train Epoch: 117 [37120/225000 (16%)] Loss: 6647.812500\n",
      "Train Epoch: 117 [41216/225000 (18%)] Loss: 6642.025391\n",
      "Train Epoch: 117 [45312/225000 (20%)] Loss: 6628.017578\n",
      "Train Epoch: 117 [49408/225000 (22%)] Loss: 6614.605469\n",
      "Train Epoch: 117 [53504/225000 (24%)] Loss: 6564.933594\n",
      "Train Epoch: 117 [57600/225000 (26%)] Loss: 6555.732422\n",
      "Train Epoch: 117 [61696/225000 (27%)] Loss: 6850.265625\n",
      "Train Epoch: 117 [65792/225000 (29%)] Loss: 6490.658203\n",
      "Train Epoch: 117 [69888/225000 (31%)] Loss: 6655.042969\n",
      "Train Epoch: 117 [73984/225000 (33%)] Loss: 6674.291016\n",
      "Train Epoch: 117 [78080/225000 (35%)] Loss: 6464.964844\n",
      "Train Epoch: 117 [82176/225000 (37%)] Loss: 6711.406250\n",
      "Train Epoch: 117 [86272/225000 (38%)] Loss: 6690.935547\n",
      "Train Epoch: 117 [90368/225000 (40%)] Loss: 6655.562500\n",
      "Train Epoch: 117 [94464/225000 (42%)] Loss: 6605.210938\n",
      "Train Epoch: 117 [98560/225000 (44%)] Loss: 6754.173828\n",
      "Train Epoch: 117 [102656/225000 (46%)] Loss: 6669.521484\n",
      "Train Epoch: 117 [106752/225000 (47%)] Loss: 6604.107422\n",
      "Train Epoch: 117 [110848/225000 (49%)] Loss: 6652.837891\n",
      "Train Epoch: 117 [114944/225000 (51%)] Loss: 6784.464844\n",
      "Train Epoch: 117 [119040/225000 (53%)] Loss: 6659.759766\n",
      "Train Epoch: 117 [123136/225000 (55%)] Loss: 6551.136719\n",
      "Train Epoch: 117 [127232/225000 (57%)] Loss: 6555.787109\n",
      "Train Epoch: 117 [131328/225000 (58%)] Loss: 6646.201172\n",
      "Train Epoch: 117 [135424/225000 (60%)] Loss: 6705.273438\n",
      "Train Epoch: 117 [139520/225000 (62%)] Loss: 6615.593750\n",
      "Train Epoch: 117 [143616/225000 (64%)] Loss: 6642.160156\n",
      "Train Epoch: 117 [147712/225000 (66%)] Loss: 6624.021484\n",
      "Train Epoch: 117 [151808/225000 (67%)] Loss: 6602.339844\n",
      "Train Epoch: 117 [155904/225000 (69%)] Loss: 6628.269531\n",
      "Train Epoch: 117 [160000/225000 (71%)] Loss: 6772.017578\n",
      "Train Epoch: 117 [164096/225000 (73%)] Loss: 6624.355469\n",
      "Train Epoch: 117 [168192/225000 (75%)] Loss: 6687.021484\n",
      "Train Epoch: 117 [172288/225000 (77%)] Loss: 6656.712891\n",
      "Train Epoch: 117 [176384/225000 (78%)] Loss: 6751.806641\n",
      "Train Epoch: 117 [180480/225000 (80%)] Loss: 6560.968750\n",
      "Train Epoch: 117 [184576/225000 (82%)] Loss: 6533.087891\n",
      "Train Epoch: 117 [188672/225000 (84%)] Loss: 6662.998047\n",
      "Train Epoch: 117 [192768/225000 (86%)] Loss: 6631.425781\n",
      "Train Epoch: 117 [196864/225000 (87%)] Loss: 6537.669922\n",
      "Train Epoch: 117 [200960/225000 (89%)] Loss: 6662.351562\n",
      "Train Epoch: 117 [205056/225000 (91%)] Loss: 6618.251953\n",
      "Train Epoch: 117 [209152/225000 (93%)] Loss: 6670.234375\n",
      "Train Epoch: 117 [213248/225000 (95%)] Loss: 6680.107422\n",
      "Train Epoch: 117 [217344/225000 (97%)] Loss: 6619.902344\n",
      "Train Epoch: 117 [221440/225000 (98%)] Loss: 6695.275391\n",
      "    epoch          : 117\n",
      "    loss           : 6826.566008514647\n",
      "    val_loss       : 6923.177690485302\n",
      "Train Epoch: 118 [256/225000 (0%)] Loss: 6606.283203\n",
      "Train Epoch: 118 [4352/225000 (2%)] Loss: 6629.330078\n",
      "Train Epoch: 118 [8448/225000 (4%)] Loss: 28925.152344\n",
      "Train Epoch: 118 [12544/225000 (6%)] Loss: 6590.437500\n",
      "Train Epoch: 118 [16640/225000 (7%)] Loss: 6845.400391\n",
      "Train Epoch: 118 [20736/225000 (9%)] Loss: 6501.871094\n",
      "Train Epoch: 118 [24832/225000 (11%)] Loss: 6510.921875\n",
      "Train Epoch: 118 [28928/225000 (13%)] Loss: 6645.146484\n",
      "Train Epoch: 118 [33024/225000 (15%)] Loss: 6554.761719\n",
      "Train Epoch: 118 [37120/225000 (16%)] Loss: 6516.156250\n",
      "Train Epoch: 118 [41216/225000 (18%)] Loss: 6731.722656\n",
      "Train Epoch: 118 [45312/225000 (20%)] Loss: 6657.925781\n",
      "Train Epoch: 118 [49408/225000 (22%)] Loss: 6718.027344\n",
      "Train Epoch: 118 [53504/225000 (24%)] Loss: 6564.523438\n",
      "Train Epoch: 118 [57600/225000 (26%)] Loss: 6644.769531\n",
      "Train Epoch: 118 [61696/225000 (27%)] Loss: 6546.867188\n",
      "Train Epoch: 118 [65792/225000 (29%)] Loss: 6575.035156\n",
      "Train Epoch: 118 [69888/225000 (31%)] Loss: 6628.906250\n",
      "Train Epoch: 118 [73984/225000 (33%)] Loss: 8293.402344\n",
      "Train Epoch: 118 [78080/225000 (35%)] Loss: 6626.197266\n",
      "Train Epoch: 118 [82176/225000 (37%)] Loss: 6901.984375\n",
      "Train Epoch: 118 [86272/225000 (38%)] Loss: 6696.390625\n",
      "Train Epoch: 118 [90368/225000 (40%)] Loss: 6642.195312\n",
      "Train Epoch: 118 [94464/225000 (42%)] Loss: 6743.654297\n",
      "Train Epoch: 118 [98560/225000 (44%)] Loss: 6610.736328\n",
      "Train Epoch: 118 [102656/225000 (46%)] Loss: 8450.835938\n",
      "Train Epoch: 118 [106752/225000 (47%)] Loss: 6498.355469\n",
      "Train Epoch: 118 [110848/225000 (49%)] Loss: 6500.279297\n",
      "Train Epoch: 118 [114944/225000 (51%)] Loss: 6717.865234\n",
      "Train Epoch: 118 [119040/225000 (53%)] Loss: 6644.333984\n",
      "Train Epoch: 118 [123136/225000 (55%)] Loss: 6716.523438\n",
      "Train Epoch: 118 [127232/225000 (57%)] Loss: 6547.791016\n",
      "Train Epoch: 118 [131328/225000 (58%)] Loss: 6541.562500\n",
      "Train Epoch: 118 [135424/225000 (60%)] Loss: 6682.501953\n",
      "Train Epoch: 118 [139520/225000 (62%)] Loss: 6662.380859\n",
      "Train Epoch: 118 [143616/225000 (64%)] Loss: 6579.271484\n",
      "Train Epoch: 118 [147712/225000 (66%)] Loss: 6709.755859\n",
      "Train Epoch: 118 [151808/225000 (67%)] Loss: 6594.765625\n",
      "Train Epoch: 118 [155904/225000 (69%)] Loss: 6660.421875\n",
      "Train Epoch: 118 [160000/225000 (71%)] Loss: 6657.078125\n",
      "Train Epoch: 118 [164096/225000 (73%)] Loss: 8383.082031\n",
      "Train Epoch: 118 [168192/225000 (75%)] Loss: 6676.490234\n",
      "Train Epoch: 118 [172288/225000 (77%)] Loss: 6535.371094\n",
      "Train Epoch: 118 [176384/225000 (78%)] Loss: 6689.470703\n",
      "Train Epoch: 118 [180480/225000 (80%)] Loss: 6519.597656\n",
      "Train Epoch: 118 [184576/225000 (82%)] Loss: 6637.427734\n",
      "Train Epoch: 118 [188672/225000 (84%)] Loss: 6682.578125\n",
      "Train Epoch: 118 [192768/225000 (86%)] Loss: 6492.970703\n",
      "Train Epoch: 118 [196864/225000 (87%)] Loss: 6692.373047\n",
      "Train Epoch: 118 [200960/225000 (89%)] Loss: 6823.158203\n",
      "Train Epoch: 118 [205056/225000 (91%)] Loss: 8404.544922\n",
      "Train Epoch: 118 [209152/225000 (93%)] Loss: 6578.376953\n",
      "Train Epoch: 118 [213248/225000 (95%)] Loss: 6523.998047\n",
      "Train Epoch: 118 [217344/225000 (97%)] Loss: 6633.656250\n",
      "Train Epoch: 118 [221440/225000 (98%)] Loss: 6505.755859\n",
      "    epoch          : 118\n",
      "    loss           : 6764.018484694966\n",
      "    val_loss       : 6662.611493122821\n",
      "Train Epoch: 119 [256/225000 (0%)] Loss: 6770.687500\n",
      "Train Epoch: 119 [4352/225000 (2%)] Loss: 6531.751953\n",
      "Train Epoch: 119 [8448/225000 (4%)] Loss: 6596.607422\n",
      "Train Epoch: 119 [12544/225000 (6%)] Loss: 6565.734375\n",
      "Train Epoch: 119 [16640/225000 (7%)] Loss: 6679.621094\n",
      "Train Epoch: 119 [20736/225000 (9%)] Loss: 6653.728516\n",
      "Train Epoch: 119 [24832/225000 (11%)] Loss: 6705.968750\n",
      "Train Epoch: 119 [28928/225000 (13%)] Loss: 6612.142578\n",
      "Train Epoch: 119 [33024/225000 (15%)] Loss: 6516.244141\n",
      "Train Epoch: 119 [37120/225000 (16%)] Loss: 6538.843750\n",
      "Train Epoch: 119 [41216/225000 (18%)] Loss: 6594.099609\n",
      "Train Epoch: 119 [45312/225000 (20%)] Loss: 6538.351562\n",
      "Train Epoch: 119 [49408/225000 (22%)] Loss: 6637.328125\n",
      "Train Epoch: 119 [53504/225000 (24%)] Loss: 6627.294922\n",
      "Train Epoch: 119 [57600/225000 (26%)] Loss: 6588.324219\n",
      "Train Epoch: 119 [61696/225000 (27%)] Loss: 6506.814453\n",
      "Train Epoch: 119 [65792/225000 (29%)] Loss: 6607.117188\n",
      "Train Epoch: 119 [69888/225000 (31%)] Loss: 6684.685547\n",
      "Train Epoch: 119 [73984/225000 (33%)] Loss: 22998.656250\n",
      "Train Epoch: 119 [78080/225000 (35%)] Loss: 6538.900391\n",
      "Train Epoch: 119 [82176/225000 (37%)] Loss: 6575.597656\n",
      "Train Epoch: 119 [86272/225000 (38%)] Loss: 6738.658203\n",
      "Train Epoch: 119 [90368/225000 (40%)] Loss: 6597.613281\n",
      "Train Epoch: 119 [94464/225000 (42%)] Loss: 6568.541016\n",
      "Train Epoch: 119 [98560/225000 (44%)] Loss: 6705.455078\n",
      "Train Epoch: 119 [102656/225000 (46%)] Loss: 6679.927734\n",
      "Train Epoch: 119 [106752/225000 (47%)] Loss: 6716.312500\n",
      "Train Epoch: 119 [110848/225000 (49%)] Loss: 6708.869141\n",
      "Train Epoch: 119 [114944/225000 (51%)] Loss: 6571.041016\n",
      "Train Epoch: 119 [119040/225000 (53%)] Loss: 6757.720703\n",
      "Train Epoch: 119 [123136/225000 (55%)] Loss: 6500.626953\n",
      "Train Epoch: 119 [127232/225000 (57%)] Loss: 6634.068359\n",
      "Train Epoch: 119 [131328/225000 (58%)] Loss: 6667.498047\n",
      "Train Epoch: 119 [135424/225000 (60%)] Loss: 6560.382812\n",
      "Train Epoch: 119 [139520/225000 (62%)] Loss: 6592.304688\n",
      "Train Epoch: 119 [143616/225000 (64%)] Loss: 6528.609375\n",
      "Train Epoch: 119 [147712/225000 (66%)] Loss: 6638.746094\n",
      "Train Epoch: 119 [151808/225000 (67%)] Loss: 6764.429688\n",
      "Train Epoch: 119 [155904/225000 (69%)] Loss: 6662.367188\n",
      "Train Epoch: 119 [160000/225000 (71%)] Loss: 6495.380859\n",
      "Train Epoch: 119 [164096/225000 (73%)] Loss: 6712.748047\n",
      "Train Epoch: 119 [168192/225000 (75%)] Loss: 6558.542969\n",
      "Train Epoch: 119 [172288/225000 (77%)] Loss: 6598.330078\n",
      "Train Epoch: 119 [176384/225000 (78%)] Loss: 6651.136719\n",
      "Train Epoch: 119 [180480/225000 (80%)] Loss: 6702.902344\n",
      "Train Epoch: 119 [184576/225000 (82%)] Loss: 6562.423828\n",
      "Train Epoch: 119 [188672/225000 (84%)] Loss: 6642.789062\n",
      "Train Epoch: 119 [192768/225000 (86%)] Loss: 6743.513672\n",
      "Train Epoch: 119 [196864/225000 (87%)] Loss: 6780.158203\n",
      "Train Epoch: 119 [200960/225000 (89%)] Loss: 6483.070312\n",
      "Train Epoch: 119 [205056/225000 (91%)] Loss: 6574.820312\n",
      "Train Epoch: 119 [209152/225000 (93%)] Loss: 6760.578125\n",
      "Train Epoch: 119 [213248/225000 (95%)] Loss: 6545.500000\n",
      "Train Epoch: 119 [217344/225000 (97%)] Loss: 6583.603516\n",
      "Train Epoch: 119 [221440/225000 (98%)] Loss: 6780.558594\n",
      "    epoch          : 119\n",
      "    loss           : 6775.467314597554\n",
      "    val_loss       : 6674.570489053824\n",
      "Train Epoch: 120 [256/225000 (0%)] Loss: 6681.865234\n",
      "Train Epoch: 120 [4352/225000 (2%)] Loss: 6719.021484\n",
      "Train Epoch: 120 [8448/225000 (4%)] Loss: 6585.572266\n",
      "Train Epoch: 120 [12544/225000 (6%)] Loss: 6664.478516\n",
      "Train Epoch: 120 [16640/225000 (7%)] Loss: 6625.320312\n",
      "Train Epoch: 120 [20736/225000 (9%)] Loss: 6716.822266\n",
      "Train Epoch: 120 [24832/225000 (11%)] Loss: 6479.230469\n",
      "Train Epoch: 120 [28928/225000 (13%)] Loss: 6626.333984\n",
      "Train Epoch: 120 [33024/225000 (15%)] Loss: 6596.841797\n",
      "Train Epoch: 120 [37120/225000 (16%)] Loss: 6670.361328\n",
      "Train Epoch: 120 [41216/225000 (18%)] Loss: 6498.150391\n",
      "Train Epoch: 120 [45312/225000 (20%)] Loss: 6589.861328\n",
      "Train Epoch: 120 [49408/225000 (22%)] Loss: 6580.847656\n",
      "Train Epoch: 120 [53504/225000 (24%)] Loss: 6743.513672\n",
      "Train Epoch: 120 [57600/225000 (26%)] Loss: 6708.019531\n",
      "Train Epoch: 120 [61696/225000 (27%)] Loss: 6541.349609\n",
      "Train Epoch: 120 [65792/225000 (29%)] Loss: 6631.441406\n",
      "Train Epoch: 120 [69888/225000 (31%)] Loss: 6754.687500\n",
      "Train Epoch: 120 [73984/225000 (33%)] Loss: 6441.453125\n",
      "Train Epoch: 120 [78080/225000 (35%)] Loss: 6754.812500\n",
      "Train Epoch: 120 [82176/225000 (37%)] Loss: 6666.136719\n",
      "Train Epoch: 120 [86272/225000 (38%)] Loss: 6437.169922\n",
      "Train Epoch: 120 [90368/225000 (40%)] Loss: 6611.134766\n",
      "Train Epoch: 120 [94464/225000 (42%)] Loss: 6548.892578\n",
      "Train Epoch: 120 [98560/225000 (44%)] Loss: 6490.255859\n",
      "Train Epoch: 120 [102656/225000 (46%)] Loss: 6700.798828\n",
      "Train Epoch: 120 [106752/225000 (47%)] Loss: 6565.695312\n",
      "Train Epoch: 120 [110848/225000 (49%)] Loss: 6488.839844\n",
      "Train Epoch: 120 [114944/225000 (51%)] Loss: 6612.962891\n",
      "Train Epoch: 120 [119040/225000 (53%)] Loss: 6594.843750\n",
      "Train Epoch: 120 [123136/225000 (55%)] Loss: 13184.320312\n",
      "Train Epoch: 120 [127232/225000 (57%)] Loss: 6633.275391\n",
      "Train Epoch: 120 [131328/225000 (58%)] Loss: 6504.013672\n",
      "Train Epoch: 120 [135424/225000 (60%)] Loss: 6635.880859\n",
      "Train Epoch: 120 [139520/225000 (62%)] Loss: 6670.144531\n",
      "Train Epoch: 120 [143616/225000 (64%)] Loss: 6578.695312\n",
      "Train Epoch: 120 [147712/225000 (66%)] Loss: 6663.898438\n",
      "Train Epoch: 120 [151808/225000 (67%)] Loss: 6617.595703\n",
      "Train Epoch: 120 [155904/225000 (69%)] Loss: 6488.597656\n",
      "Train Epoch: 120 [160000/225000 (71%)] Loss: 6676.519531\n",
      "Train Epoch: 120 [164096/225000 (73%)] Loss: 6621.455078\n",
      "Train Epoch: 120 [168192/225000 (75%)] Loss: 6620.273438\n",
      "Train Epoch: 120 [172288/225000 (77%)] Loss: 22477.902344\n",
      "Train Epoch: 120 [176384/225000 (78%)] Loss: 6597.115234\n",
      "Train Epoch: 120 [180480/225000 (80%)] Loss: 6555.992188\n",
      "Train Epoch: 120 [184576/225000 (82%)] Loss: 6589.242188\n",
      "Train Epoch: 120 [188672/225000 (84%)] Loss: 6445.673828\n",
      "Train Epoch: 120 [192768/225000 (86%)] Loss: 6645.369141\n",
      "Train Epoch: 120 [196864/225000 (87%)] Loss: 6627.078125\n",
      "Train Epoch: 120 [200960/225000 (89%)] Loss: 6596.208984\n",
      "Train Epoch: 120 [205056/225000 (91%)] Loss: 6694.691406\n",
      "Train Epoch: 120 [209152/225000 (93%)] Loss: 6695.289062\n",
      "Train Epoch: 120 [213248/225000 (95%)] Loss: 6667.765625\n",
      "Train Epoch: 120 [217344/225000 (97%)] Loss: 6563.755859\n",
      "Train Epoch: 120 [221440/225000 (98%)] Loss: 6553.199219\n",
      "    epoch          : 120\n",
      "    loss           : 6744.895651130546\n",
      "    val_loss       : 6907.575435608017\n",
      "Train Epoch: 121 [256/225000 (0%)] Loss: 6716.984375\n",
      "Train Epoch: 121 [4352/225000 (2%)] Loss: 6739.427734\n",
      "Train Epoch: 121 [8448/225000 (4%)] Loss: 6637.798828\n",
      "Train Epoch: 121 [12544/225000 (6%)] Loss: 6489.123047\n",
      "Train Epoch: 121 [16640/225000 (7%)] Loss: 6599.029297\n",
      "Train Epoch: 121 [20736/225000 (9%)] Loss: 6414.515625\n",
      "Train Epoch: 121 [24832/225000 (11%)] Loss: 6627.068359\n",
      "Train Epoch: 121 [28928/225000 (13%)] Loss: 6525.341797\n",
      "Train Epoch: 121 [33024/225000 (15%)] Loss: 6540.580078\n",
      "Train Epoch: 121 [37120/225000 (16%)] Loss: 6627.261719\n",
      "Train Epoch: 121 [41216/225000 (18%)] Loss: 6723.865234\n",
      "Train Epoch: 121 [45312/225000 (20%)] Loss: 6657.802734\n",
      "Train Epoch: 121 [49408/225000 (22%)] Loss: 6624.832031\n",
      "Train Epoch: 121 [53504/225000 (24%)] Loss: 6491.240234\n",
      "Train Epoch: 121 [57600/225000 (26%)] Loss: 6537.890625\n",
      "Train Epoch: 121 [61696/225000 (27%)] Loss: 6476.087891\n",
      "Train Epoch: 121 [65792/225000 (29%)] Loss: 6738.962891\n",
      "Train Epoch: 121 [69888/225000 (31%)] Loss: 6536.046875\n",
      "Train Epoch: 121 [73984/225000 (33%)] Loss: 6747.755859\n",
      "Train Epoch: 121 [78080/225000 (35%)] Loss: 6564.910156\n",
      "Train Epoch: 121 [82176/225000 (37%)] Loss: 6446.166016\n",
      "Train Epoch: 121 [86272/225000 (38%)] Loss: 6562.228516\n",
      "Train Epoch: 121 [90368/225000 (40%)] Loss: 6745.128906\n",
      "Train Epoch: 121 [94464/225000 (42%)] Loss: 6654.222656\n",
      "Train Epoch: 121 [98560/225000 (44%)] Loss: 6586.359375\n",
      "Train Epoch: 121 [102656/225000 (46%)] Loss: 6518.304688\n",
      "Train Epoch: 121 [106752/225000 (47%)] Loss: 6707.720703\n",
      "Train Epoch: 121 [110848/225000 (49%)] Loss: 6560.902344\n",
      "Train Epoch: 121 [114944/225000 (51%)] Loss: 6587.416016\n",
      "Train Epoch: 121 [119040/225000 (53%)] Loss: 6564.355469\n",
      "Train Epoch: 121 [123136/225000 (55%)] Loss: 6721.150391\n",
      "Train Epoch: 121 [127232/225000 (57%)] Loss: 6548.232422\n",
      "Train Epoch: 121 [131328/225000 (58%)] Loss: 6523.140625\n",
      "Train Epoch: 121 [135424/225000 (60%)] Loss: 6445.880859\n",
      "Train Epoch: 121 [139520/225000 (62%)] Loss: 6703.216797\n",
      "Train Epoch: 121 [143616/225000 (64%)] Loss: 6528.462891\n",
      "Train Epoch: 121 [147712/225000 (66%)] Loss: 6541.205078\n",
      "Train Epoch: 121 [151808/225000 (67%)] Loss: 6591.167969\n",
      "Train Epoch: 121 [155904/225000 (69%)] Loss: 6570.558594\n",
      "Train Epoch: 121 [160000/225000 (71%)] Loss: 6658.890625\n",
      "Train Epoch: 121 [164096/225000 (73%)] Loss: 6593.320312\n",
      "Train Epoch: 121 [168192/225000 (75%)] Loss: 6633.355469\n",
      "Train Epoch: 121 [172288/225000 (77%)] Loss: 6592.167969\n",
      "Train Epoch: 121 [176384/225000 (78%)] Loss: 6640.449219\n",
      "Train Epoch: 121 [180480/225000 (80%)] Loss: 6583.175781\n",
      "Train Epoch: 121 [184576/225000 (82%)] Loss: 6583.498047\n",
      "Train Epoch: 121 [188672/225000 (84%)] Loss: 6683.568359\n",
      "Train Epoch: 121 [192768/225000 (86%)] Loss: 6558.173828\n",
      "Train Epoch: 121 [196864/225000 (87%)] Loss: 6571.853516\n",
      "Train Epoch: 121 [200960/225000 (89%)] Loss: 6632.488281\n",
      "Train Epoch: 121 [205056/225000 (91%)] Loss: 6565.580078\n",
      "Train Epoch: 121 [209152/225000 (93%)] Loss: 6525.298828\n",
      "Train Epoch: 121 [213248/225000 (95%)] Loss: 6554.052734\n",
      "Train Epoch: 121 [217344/225000 (97%)] Loss: 6666.646484\n",
      "Train Epoch: 121 [221440/225000 (98%)] Loss: 6691.705078\n",
      "    epoch          : 121\n",
      "    loss           : 6677.228467852318\n",
      "    val_loss       : 6649.022100024077\n",
      "Train Epoch: 122 [256/225000 (0%)] Loss: 6587.845703\n",
      "Train Epoch: 122 [4352/225000 (2%)] Loss: 6440.056641\n",
      "Train Epoch: 122 [8448/225000 (4%)] Loss: 6685.957031\n",
      "Train Epoch: 122 [12544/225000 (6%)] Loss: 6675.646484\n",
      "Train Epoch: 122 [16640/225000 (7%)] Loss: 6480.523438\n",
      "Train Epoch: 122 [20736/225000 (9%)] Loss: 6448.023438\n",
      "Train Epoch: 122 [24832/225000 (11%)] Loss: 6707.068359\n",
      "Train Epoch: 122 [28928/225000 (13%)] Loss: 6576.455078\n",
      "Train Epoch: 122 [33024/225000 (15%)] Loss: 6728.937500\n",
      "Train Epoch: 122 [37120/225000 (16%)] Loss: 6546.310547\n",
      "Train Epoch: 122 [41216/225000 (18%)] Loss: 6551.212891\n",
      "Train Epoch: 122 [45312/225000 (20%)] Loss: 6810.429688\n",
      "Train Epoch: 122 [49408/225000 (22%)] Loss: 6558.775391\n",
      "Train Epoch: 122 [53504/225000 (24%)] Loss: 6526.416016\n",
      "Train Epoch: 122 [57600/225000 (26%)] Loss: 6698.058594\n",
      "Train Epoch: 122 [61696/225000 (27%)] Loss: 6578.619141\n",
      "Train Epoch: 122 [65792/225000 (29%)] Loss: 6646.925781\n",
      "Train Epoch: 122 [69888/225000 (31%)] Loss: 6588.726562\n",
      "Train Epoch: 122 [73984/225000 (33%)] Loss: 6718.863281\n",
      "Train Epoch: 122 [78080/225000 (35%)] Loss: 6573.371094\n",
      "Train Epoch: 122 [82176/225000 (37%)] Loss: 6603.074219\n",
      "Train Epoch: 122 [86272/225000 (38%)] Loss: 6493.664062\n",
      "Train Epoch: 122 [90368/225000 (40%)] Loss: 6696.369141\n",
      "Train Epoch: 122 [94464/225000 (42%)] Loss: 6513.328125\n",
      "Train Epoch: 122 [98560/225000 (44%)] Loss: 6655.910156\n",
      "Train Epoch: 122 [102656/225000 (46%)] Loss: 6591.445312\n",
      "Train Epoch: 122 [106752/225000 (47%)] Loss: 6474.824219\n",
      "Train Epoch: 122 [110848/225000 (49%)] Loss: 6574.414062\n",
      "Train Epoch: 122 [114944/225000 (51%)] Loss: 6663.945312\n",
      "Train Epoch: 122 [119040/225000 (53%)] Loss: 6395.511719\n",
      "Train Epoch: 122 [123136/225000 (55%)] Loss: 6684.130859\n",
      "Train Epoch: 122 [127232/225000 (57%)] Loss: 6580.777344\n",
      "Train Epoch: 122 [131328/225000 (58%)] Loss: 6628.062500\n",
      "Train Epoch: 122 [135424/225000 (60%)] Loss: 6640.445312\n",
      "Train Epoch: 122 [139520/225000 (62%)] Loss: 6596.589844\n",
      "Train Epoch: 122 [143616/225000 (64%)] Loss: 6628.589844\n",
      "Train Epoch: 122 [147712/225000 (66%)] Loss: 6623.128906\n",
      "Train Epoch: 122 [151808/225000 (67%)] Loss: 6591.681641\n",
      "Train Epoch: 122 [155904/225000 (69%)] Loss: 6552.472656\n",
      "Train Epoch: 122 [160000/225000 (71%)] Loss: 6663.666016\n",
      "Train Epoch: 122 [164096/225000 (73%)] Loss: 6549.470703\n",
      "Train Epoch: 122 [168192/225000 (75%)] Loss: 6511.011719\n",
      "Train Epoch: 122 [172288/225000 (77%)] Loss: 6613.830078\n",
      "Train Epoch: 122 [176384/225000 (78%)] Loss: 6561.990234\n",
      "Train Epoch: 122 [180480/225000 (80%)] Loss: 6436.023438\n",
      "Train Epoch: 122 [184576/225000 (82%)] Loss: 6560.734375\n",
      "Train Epoch: 122 [188672/225000 (84%)] Loss: 6594.474609\n",
      "Train Epoch: 122 [192768/225000 (86%)] Loss: 6552.933594\n",
      "Train Epoch: 122 [196864/225000 (87%)] Loss: 6529.552734\n",
      "Train Epoch: 122 [200960/225000 (89%)] Loss: 6514.753906\n",
      "Train Epoch: 122 [205056/225000 (91%)] Loss: 6612.675781\n",
      "Train Epoch: 122 [209152/225000 (93%)] Loss: 6576.509766\n",
      "Train Epoch: 122 [213248/225000 (95%)] Loss: 6589.425781\n",
      "Train Epoch: 122 [217344/225000 (97%)] Loss: 6598.589844\n",
      "Train Epoch: 122 [221440/225000 (98%)] Loss: 6510.070312\n",
      "    epoch          : 122\n",
      "    loss           : 6724.481094238837\n",
      "    val_loss       : 6627.793177049987\n",
      "Train Epoch: 123 [256/225000 (0%)] Loss: 6596.679688\n",
      "Train Epoch: 123 [4352/225000 (2%)] Loss: 6595.830078\n",
      "Train Epoch: 123 [8448/225000 (4%)] Loss: 6502.349609\n",
      "Train Epoch: 123 [12544/225000 (6%)] Loss: 6737.375000\n",
      "Train Epoch: 123 [16640/225000 (7%)] Loss: 6676.013672\n",
      "Train Epoch: 123 [20736/225000 (9%)] Loss: 6549.480469\n",
      "Train Epoch: 123 [24832/225000 (11%)] Loss: 6730.457031\n",
      "Train Epoch: 123 [28928/225000 (13%)] Loss: 6603.212891\n",
      "Train Epoch: 123 [33024/225000 (15%)] Loss: 6454.261719\n",
      "Train Epoch: 123 [37120/225000 (16%)] Loss: 6634.496094\n",
      "Train Epoch: 123 [41216/225000 (18%)] Loss: 6800.808594\n",
      "Train Epoch: 123 [45312/225000 (20%)] Loss: 6600.884766\n",
      "Train Epoch: 123 [49408/225000 (22%)] Loss: 6566.957031\n",
      "Train Epoch: 123 [53504/225000 (24%)] Loss: 6593.208984\n",
      "Train Epoch: 123 [57600/225000 (26%)] Loss: 6636.341797\n",
      "Train Epoch: 123 [61696/225000 (27%)] Loss: 6669.509766\n",
      "Train Epoch: 123 [65792/225000 (29%)] Loss: 6405.767578\n",
      "Train Epoch: 123 [69888/225000 (31%)] Loss: 6640.833984\n",
      "Train Epoch: 123 [73984/225000 (33%)] Loss: 6547.187500\n",
      "Train Epoch: 123 [78080/225000 (35%)] Loss: 6549.578125\n",
      "Train Epoch: 123 [82176/225000 (37%)] Loss: 6602.958984\n",
      "Train Epoch: 123 [86272/225000 (38%)] Loss: 6403.998047\n",
      "Train Epoch: 123 [90368/225000 (40%)] Loss: 6610.722656\n",
      "Train Epoch: 123 [94464/225000 (42%)] Loss: 6618.722656\n",
      "Train Epoch: 123 [98560/225000 (44%)] Loss: 6661.751953\n",
      "Train Epoch: 123 [102656/225000 (46%)] Loss: 6591.183594\n",
      "Train Epoch: 123 [106752/225000 (47%)] Loss: 6445.046875\n",
      "Train Epoch: 123 [110848/225000 (49%)] Loss: 6573.466797\n",
      "Train Epoch: 123 [114944/225000 (51%)] Loss: 6632.955078\n",
      "Train Epoch: 123 [119040/225000 (53%)] Loss: 6590.339844\n",
      "Train Epoch: 123 [123136/225000 (55%)] Loss: 6610.609375\n",
      "Train Epoch: 123 [127232/225000 (57%)] Loss: 6464.750000\n",
      "Train Epoch: 123 [131328/225000 (58%)] Loss: 6635.886719\n",
      "Train Epoch: 123 [135424/225000 (60%)] Loss: 6689.853516\n",
      "Train Epoch: 123 [139520/225000 (62%)] Loss: 6532.384766\n",
      "Train Epoch: 123 [143616/225000 (64%)] Loss: 6571.152344\n",
      "Train Epoch: 123 [147712/225000 (66%)] Loss: 6542.822266\n",
      "Train Epoch: 123 [151808/225000 (67%)] Loss: 6646.812500\n",
      "Train Epoch: 123 [155904/225000 (69%)] Loss: 6596.365234\n",
      "Train Epoch: 123 [160000/225000 (71%)] Loss: 6638.960938\n",
      "Train Epoch: 123 [164096/225000 (73%)] Loss: 6560.126953\n",
      "Train Epoch: 123 [168192/225000 (75%)] Loss: 6650.777344\n",
      "Train Epoch: 123 [172288/225000 (77%)] Loss: 6645.427734\n",
      "Train Epoch: 123 [176384/225000 (78%)] Loss: 6849.132812\n",
      "Train Epoch: 123 [180480/225000 (80%)] Loss: 6542.119141\n",
      "Train Epoch: 123 [184576/225000 (82%)] Loss: 6563.521484\n",
      "Train Epoch: 123 [188672/225000 (84%)] Loss: 6618.796875\n",
      "Train Epoch: 123 [192768/225000 (86%)] Loss: 6521.134766\n",
      "Train Epoch: 123 [196864/225000 (87%)] Loss: 6641.425781\n",
      "Train Epoch: 123 [200960/225000 (89%)] Loss: 6545.193359\n",
      "Train Epoch: 123 [205056/225000 (91%)] Loss: 6533.927734\n",
      "Train Epoch: 123 [209152/225000 (93%)] Loss: 6595.630859\n",
      "Train Epoch: 123 [213248/225000 (95%)] Loss: 6580.699219\n",
      "Train Epoch: 123 [217344/225000 (97%)] Loss: 6631.900391\n",
      "Train Epoch: 123 [221440/225000 (98%)] Loss: 6526.392578\n",
      "    epoch          : 123\n",
      "    loss           : 6731.55228664498\n",
      "    val_loss       : 6642.908434173282\n",
      "Train Epoch: 124 [256/225000 (0%)] Loss: 6536.435547\n",
      "Train Epoch: 124 [4352/225000 (2%)] Loss: 6603.273438\n",
      "Train Epoch: 124 [8448/225000 (4%)] Loss: 6707.177734\n",
      "Train Epoch: 124 [12544/225000 (6%)] Loss: 6493.812500\n",
      "Train Epoch: 124 [16640/225000 (7%)] Loss: 6620.552734\n",
      "Train Epoch: 124 [20736/225000 (9%)] Loss: 6392.552734\n",
      "Train Epoch: 124 [24832/225000 (11%)] Loss: 6680.308594\n",
      "Train Epoch: 124 [28928/225000 (13%)] Loss: 6783.726562\n",
      "Train Epoch: 124 [33024/225000 (15%)] Loss: 6656.814453\n",
      "Train Epoch: 124 [37120/225000 (16%)] Loss: 6859.154297\n",
      "Train Epoch: 124 [41216/225000 (18%)] Loss: 6564.193359\n",
      "Train Epoch: 124 [45312/225000 (20%)] Loss: 6574.619141\n",
      "Train Epoch: 124 [49408/225000 (22%)] Loss: 6613.966797\n",
      "Train Epoch: 124 [53504/225000 (24%)] Loss: 6512.636719\n",
      "Train Epoch: 124 [57600/225000 (26%)] Loss: 6641.568359\n",
      "Train Epoch: 124 [61696/225000 (27%)] Loss: 6450.330078\n",
      "Train Epoch: 124 [65792/225000 (29%)] Loss: 6565.828125\n",
      "Train Epoch: 124 [69888/225000 (31%)] Loss: 6473.171875\n",
      "Train Epoch: 124 [73984/225000 (33%)] Loss: 6629.515625\n",
      "Train Epoch: 124 [78080/225000 (35%)] Loss: 6687.326172\n",
      "Train Epoch: 124 [82176/225000 (37%)] Loss: 6497.523438\n",
      "Train Epoch: 124 [86272/225000 (38%)] Loss: 6658.591797\n",
      "Train Epoch: 124 [90368/225000 (40%)] Loss: 6613.267578\n",
      "Train Epoch: 124 [94464/225000 (42%)] Loss: 6485.890625\n",
      "Train Epoch: 124 [98560/225000 (44%)] Loss: 6610.410156\n",
      "Train Epoch: 124 [102656/225000 (46%)] Loss: 6469.386719\n",
      "Train Epoch: 124 [106752/225000 (47%)] Loss: 6474.810547\n",
      "Train Epoch: 124 [110848/225000 (49%)] Loss: 6541.857422\n",
      "Train Epoch: 124 [114944/225000 (51%)] Loss: 6580.699219\n",
      "Train Epoch: 124 [119040/225000 (53%)] Loss: 6718.572266\n",
      "Train Epoch: 124 [123136/225000 (55%)] Loss: 6645.269531\n",
      "Train Epoch: 124 [127232/225000 (57%)] Loss: 6515.154297\n",
      "Train Epoch: 124 [131328/225000 (58%)] Loss: 6748.578125\n",
      "Train Epoch: 124 [135424/225000 (60%)] Loss: 6650.207031\n",
      "Train Epoch: 124 [139520/225000 (62%)] Loss: 6582.035156\n",
      "Train Epoch: 124 [143616/225000 (64%)] Loss: 6604.123047\n",
      "Train Epoch: 124 [147712/225000 (66%)] Loss: 6497.318359\n",
      "Train Epoch: 124 [151808/225000 (67%)] Loss: 20328.583984\n",
      "Train Epoch: 124 [155904/225000 (69%)] Loss: 6509.513672\n",
      "Train Epoch: 124 [160000/225000 (71%)] Loss: 6539.978516\n",
      "Train Epoch: 124 [164096/225000 (73%)] Loss: 6573.222656\n",
      "Train Epoch: 124 [168192/225000 (75%)] Loss: 6530.218750\n",
      "Train Epoch: 124 [172288/225000 (77%)] Loss: 6615.914062\n",
      "Train Epoch: 124 [176384/225000 (78%)] Loss: 6504.964844\n",
      "Train Epoch: 124 [180480/225000 (80%)] Loss: 6601.619141\n",
      "Train Epoch: 124 [184576/225000 (82%)] Loss: 6717.582031\n",
      "Train Epoch: 124 [188672/225000 (84%)] Loss: 6575.423828\n",
      "Train Epoch: 124 [192768/225000 (86%)] Loss: 6547.625000\n",
      "Train Epoch: 124 [196864/225000 (87%)] Loss: 6552.505859\n",
      "Train Epoch: 124 [200960/225000 (89%)] Loss: 6595.126953\n",
      "Train Epoch: 124 [205056/225000 (91%)] Loss: 6487.845703\n",
      "Train Epoch: 124 [209152/225000 (93%)] Loss: 6605.042969\n",
      "Train Epoch: 124 [213248/225000 (95%)] Loss: 6686.224609\n",
      "Train Epoch: 124 [217344/225000 (97%)] Loss: 6523.072266\n",
      "Train Epoch: 124 [221440/225000 (98%)] Loss: 6708.216797\n",
      "    epoch          : 124\n",
      "    loss           : 6688.042123284627\n",
      "    val_loss       : 6647.942744845031\n",
      "Train Epoch: 125 [256/225000 (0%)] Loss: 6486.361328\n",
      "Train Epoch: 125 [4352/225000 (2%)] Loss: 6506.369141\n",
      "Train Epoch: 125 [8448/225000 (4%)] Loss: 6637.546875\n",
      "Train Epoch: 125 [12544/225000 (6%)] Loss: 6656.753906\n",
      "Train Epoch: 125 [16640/225000 (7%)] Loss: 6526.958984\n",
      "Train Epoch: 125 [20736/225000 (9%)] Loss: 6508.792969\n",
      "Train Epoch: 125 [24832/225000 (11%)] Loss: 6568.441406\n",
      "Train Epoch: 125 [28928/225000 (13%)] Loss: 6463.902344\n",
      "Train Epoch: 125 [33024/225000 (15%)] Loss: 6779.976562\n",
      "Train Epoch: 125 [37120/225000 (16%)] Loss: 6581.351562\n",
      "Train Epoch: 125 [41216/225000 (18%)] Loss: 6567.732422\n",
      "Train Epoch: 125 [45312/225000 (20%)] Loss: 6609.271484\n",
      "Train Epoch: 125 [49408/225000 (22%)] Loss: 6567.671875\n",
      "Train Epoch: 125 [53504/225000 (24%)] Loss: 6463.962891\n",
      "Train Epoch: 125 [57600/225000 (26%)] Loss: 6614.712891\n",
      "Train Epoch: 125 [61696/225000 (27%)] Loss: 6463.847656\n",
      "Train Epoch: 125 [65792/225000 (29%)] Loss: 6563.021484\n",
      "Train Epoch: 125 [69888/225000 (31%)] Loss: 6626.158203\n",
      "Train Epoch: 125 [73984/225000 (33%)] Loss: 6726.058594\n",
      "Train Epoch: 125 [78080/225000 (35%)] Loss: 6633.669922\n",
      "Train Epoch: 125 [82176/225000 (37%)] Loss: 6565.134766\n",
      "Train Epoch: 125 [86272/225000 (38%)] Loss: 6622.224609\n",
      "Train Epoch: 125 [90368/225000 (40%)] Loss: 6423.376953\n",
      "Train Epoch: 125 [94464/225000 (42%)] Loss: 6516.957031\n",
      "Train Epoch: 125 [98560/225000 (44%)] Loss: 6692.498047\n",
      "Train Epoch: 125 [102656/225000 (46%)] Loss: 6461.558594\n",
      "Train Epoch: 125 [106752/225000 (47%)] Loss: 6610.091797\n",
      "Train Epoch: 125 [110848/225000 (49%)] Loss: 6509.996094\n",
      "Train Epoch: 125 [114944/225000 (51%)] Loss: 6716.300781\n",
      "Train Epoch: 125 [119040/225000 (53%)] Loss: 6497.865234\n",
      "Train Epoch: 125 [123136/225000 (55%)] Loss: 6520.376953\n",
      "Train Epoch: 125 [127232/225000 (57%)] Loss: 6485.509766\n",
      "Train Epoch: 125 [131328/225000 (58%)] Loss: 6584.296875\n",
      "Train Epoch: 125 [135424/225000 (60%)] Loss: 6488.107422\n",
      "Train Epoch: 125 [139520/225000 (62%)] Loss: 6787.658203\n",
      "Train Epoch: 125 [143616/225000 (64%)] Loss: 6511.476562\n",
      "Train Epoch: 125 [147712/225000 (66%)] Loss: 6508.046875\n",
      "Train Epoch: 125 [151808/225000 (67%)] Loss: 6561.947266\n",
      "Train Epoch: 125 [155904/225000 (69%)] Loss: 6692.759766\n",
      "Train Epoch: 125 [160000/225000 (71%)] Loss: 6569.449219\n",
      "Train Epoch: 125 [164096/225000 (73%)] Loss: 6543.363281\n",
      "Train Epoch: 125 [168192/225000 (75%)] Loss: 6531.591797\n",
      "Train Epoch: 125 [172288/225000 (77%)] Loss: 6448.080078\n",
      "Train Epoch: 125 [176384/225000 (78%)] Loss: 6627.925781\n",
      "Train Epoch: 125 [180480/225000 (80%)] Loss: 6594.710938\n",
      "Train Epoch: 125 [184576/225000 (82%)] Loss: 6593.533203\n",
      "Train Epoch: 125 [188672/225000 (84%)] Loss: 6548.492188\n",
      "Train Epoch: 125 [192768/225000 (86%)] Loss: 6672.753906\n",
      "Train Epoch: 125 [196864/225000 (87%)] Loss: 6502.980469\n",
      "Train Epoch: 125 [200960/225000 (89%)] Loss: 6621.121094\n",
      "Train Epoch: 125 [205056/225000 (91%)] Loss: 6668.056641\n",
      "Train Epoch: 125 [209152/225000 (93%)] Loss: 6527.578125\n",
      "Train Epoch: 125 [213248/225000 (95%)] Loss: 6586.421875\n",
      "Train Epoch: 125 [217344/225000 (97%)] Loss: 6438.189453\n",
      "Train Epoch: 125 [221440/225000 (98%)] Loss: 6520.296875\n",
      "    epoch          : 125\n",
      "    loss           : 6689.536496107082\n",
      "    val_loss       : 6644.772089787284\n",
      "Train Epoch: 126 [256/225000 (0%)] Loss: 6506.847656\n",
      "Train Epoch: 126 [4352/225000 (2%)] Loss: 6601.199219\n",
      "Train Epoch: 126 [8448/225000 (4%)] Loss: 6502.863281\n",
      "Train Epoch: 126 [12544/225000 (6%)] Loss: 6432.326172\n",
      "Train Epoch: 126 [16640/225000 (7%)] Loss: 6511.759766\n",
      "Train Epoch: 126 [20736/225000 (9%)] Loss: 6628.037109\n",
      "Train Epoch: 126 [24832/225000 (11%)] Loss: 6439.269531\n",
      "Train Epoch: 126 [28928/225000 (13%)] Loss: 6495.255859\n",
      "Train Epoch: 126 [33024/225000 (15%)] Loss: 6560.154297\n",
      "Train Epoch: 126 [37120/225000 (16%)] Loss: 6593.703125\n",
      "Train Epoch: 126 [41216/225000 (18%)] Loss: 6504.291016\n",
      "Train Epoch: 126 [45312/225000 (20%)] Loss: 6515.587891\n",
      "Train Epoch: 126 [49408/225000 (22%)] Loss: 6624.902344\n",
      "Train Epoch: 126 [53504/225000 (24%)] Loss: 6535.750000\n",
      "Train Epoch: 126 [57600/225000 (26%)] Loss: 6489.972656\n",
      "Train Epoch: 126 [61696/225000 (27%)] Loss: 6650.423828\n",
      "Train Epoch: 126 [65792/225000 (29%)] Loss: 6558.341797\n",
      "Train Epoch: 126 [69888/225000 (31%)] Loss: 6373.017578\n",
      "Train Epoch: 126 [73984/225000 (33%)] Loss: 6523.480469\n",
      "Train Epoch: 126 [78080/225000 (35%)] Loss: 6537.783203\n",
      "Train Epoch: 126 [82176/225000 (37%)] Loss: 6567.681641\n",
      "Train Epoch: 126 [86272/225000 (38%)] Loss: 6598.253906\n",
      "Train Epoch: 126 [90368/225000 (40%)] Loss: 6503.064453\n",
      "Train Epoch: 126 [94464/225000 (42%)] Loss: 6579.222656\n",
      "Train Epoch: 126 [98560/225000 (44%)] Loss: 6639.162109\n",
      "Train Epoch: 126 [102656/225000 (46%)] Loss: 6632.027344\n",
      "Train Epoch: 126 [106752/225000 (47%)] Loss: 6526.349609\n",
      "Train Epoch: 126 [110848/225000 (49%)] Loss: 6594.121094\n",
      "Train Epoch: 126 [114944/225000 (51%)] Loss: 6445.435547\n",
      "Train Epoch: 126 [119040/225000 (53%)] Loss: 6495.371094\n",
      "Train Epoch: 126 [123136/225000 (55%)] Loss: 6647.392578\n",
      "Train Epoch: 126 [127232/225000 (57%)] Loss: 6710.916016\n",
      "Train Epoch: 126 [131328/225000 (58%)] Loss: 6440.312500\n",
      "Train Epoch: 126 [135424/225000 (60%)] Loss: 6737.244141\n",
      "Train Epoch: 126 [139520/225000 (62%)] Loss: 6648.509766\n",
      "Train Epoch: 126 [143616/225000 (64%)] Loss: 6641.273438\n",
      "Train Epoch: 126 [147712/225000 (66%)] Loss: 6491.437500\n",
      "Train Epoch: 126 [151808/225000 (67%)] Loss: 6534.136719\n",
      "Train Epoch: 126 [155904/225000 (69%)] Loss: 6525.740234\n",
      "Train Epoch: 126 [160000/225000 (71%)] Loss: 6605.966797\n",
      "Train Epoch: 126 [164096/225000 (73%)] Loss: 6550.320312\n",
      "Train Epoch: 126 [168192/225000 (75%)] Loss: 6500.406250\n",
      "Train Epoch: 126 [172288/225000 (77%)] Loss: 6560.798828\n",
      "Train Epoch: 126 [176384/225000 (78%)] Loss: 6677.304688\n",
      "Train Epoch: 126 [180480/225000 (80%)] Loss: 6597.857422\n",
      "Train Epoch: 126 [184576/225000 (82%)] Loss: 8404.253906\n",
      "Train Epoch: 126 [188672/225000 (84%)] Loss: 6522.800781\n",
      "Train Epoch: 126 [192768/225000 (86%)] Loss: 6622.437500\n",
      "Train Epoch: 126 [196864/225000 (87%)] Loss: 6696.390625\n",
      "Train Epoch: 126 [200960/225000 (89%)] Loss: 6484.769531\n",
      "Train Epoch: 126 [205056/225000 (91%)] Loss: 6504.294922\n",
      "Train Epoch: 126 [209152/225000 (93%)] Loss: 6743.041016\n",
      "Train Epoch: 126 [213248/225000 (95%)] Loss: 6590.326172\n",
      "Train Epoch: 126 [217344/225000 (97%)] Loss: 6508.550781\n",
      "Train Epoch: 126 [221440/225000 (98%)] Loss: 6669.226562\n",
      "    epoch          : 126\n",
      "    loss           : 6627.73010212244\n",
      "    val_loss       : 6715.94701346086\n",
      "Train Epoch: 127 [256/225000 (0%)] Loss: 6409.154297\n",
      "Train Epoch: 127 [4352/225000 (2%)] Loss: 6552.919922\n",
      "Train Epoch: 127 [8448/225000 (4%)] Loss: 6671.912109\n",
      "Train Epoch: 127 [12544/225000 (6%)] Loss: 6463.025391\n",
      "Train Epoch: 127 [16640/225000 (7%)] Loss: 6591.138672\n",
      "Train Epoch: 127 [20736/225000 (9%)] Loss: 6648.130859\n",
      "Train Epoch: 127 [24832/225000 (11%)] Loss: 6458.916016\n",
      "Train Epoch: 127 [28928/225000 (13%)] Loss: 6457.851562\n",
      "Train Epoch: 127 [33024/225000 (15%)] Loss: 6698.914062\n",
      "Train Epoch: 127 [37120/225000 (16%)] Loss: 6589.285156\n",
      "Train Epoch: 127 [41216/225000 (18%)] Loss: 6764.248047\n",
      "Train Epoch: 127 [45312/225000 (20%)] Loss: 6546.224609\n",
      "Train Epoch: 127 [49408/225000 (22%)] Loss: 6611.632812\n",
      "Train Epoch: 127 [53504/225000 (24%)] Loss: 6564.583984\n",
      "Train Epoch: 127 [57600/225000 (26%)] Loss: 6567.916016\n",
      "Train Epoch: 127 [61696/225000 (27%)] Loss: 6522.322266\n",
      "Train Epoch: 127 [65792/225000 (29%)] Loss: 6487.511719\n",
      "Train Epoch: 127 [69888/225000 (31%)] Loss: 6563.496094\n",
      "Train Epoch: 127 [73984/225000 (33%)] Loss: 6647.689453\n",
      "Train Epoch: 127 [78080/225000 (35%)] Loss: 6619.107422\n",
      "Train Epoch: 127 [82176/225000 (37%)] Loss: 6562.205078\n",
      "Train Epoch: 127 [86272/225000 (38%)] Loss: 6638.894531\n",
      "Train Epoch: 127 [90368/225000 (40%)] Loss: 6531.041016\n",
      "Train Epoch: 127 [94464/225000 (42%)] Loss: 6564.222656\n",
      "Train Epoch: 127 [98560/225000 (44%)] Loss: 6534.650391\n",
      "Train Epoch: 127 [102656/225000 (46%)] Loss: 6652.179688\n",
      "Train Epoch: 127 [106752/225000 (47%)] Loss: 6524.960938\n",
      "Train Epoch: 127 [110848/225000 (49%)] Loss: 6556.107422\n",
      "Train Epoch: 127 [114944/225000 (51%)] Loss: 6569.367188\n",
      "Train Epoch: 127 [119040/225000 (53%)] Loss: 6555.626953\n",
      "Train Epoch: 127 [123136/225000 (55%)] Loss: 6537.841797\n",
      "Train Epoch: 127 [127232/225000 (57%)] Loss: 6482.158203\n",
      "Train Epoch: 127 [131328/225000 (58%)] Loss: 6451.119141\n",
      "Train Epoch: 127 [135424/225000 (60%)] Loss: 6620.443359\n",
      "Train Epoch: 127 [139520/225000 (62%)] Loss: 6630.248047\n",
      "Train Epoch: 127 [143616/225000 (64%)] Loss: 6693.164062\n",
      "Train Epoch: 127 [147712/225000 (66%)] Loss: 6568.998047\n",
      "Train Epoch: 127 [151808/225000 (67%)] Loss: 6664.947266\n",
      "Train Epoch: 127 [155904/225000 (69%)] Loss: 6351.892578\n",
      "Train Epoch: 127 [160000/225000 (71%)] Loss: 6492.226562\n",
      "Train Epoch: 127 [164096/225000 (73%)] Loss: 6646.099609\n",
      "Train Epoch: 127 [168192/225000 (75%)] Loss: 6583.619141\n",
      "Train Epoch: 127 [172288/225000 (77%)] Loss: 6616.019531\n",
      "Train Epoch: 127 [176384/225000 (78%)] Loss: 6535.408203\n",
      "Train Epoch: 127 [180480/225000 (80%)] Loss: 6631.441406\n",
      "Train Epoch: 127 [184576/225000 (82%)] Loss: 6532.896484\n",
      "Train Epoch: 127 [188672/225000 (84%)] Loss: 6512.728516\n",
      "Train Epoch: 127 [192768/225000 (86%)] Loss: 6435.349609\n",
      "Train Epoch: 127 [196864/225000 (87%)] Loss: 6660.958984\n",
      "Train Epoch: 127 [200960/225000 (89%)] Loss: 6545.732422\n",
      "Train Epoch: 127 [205056/225000 (91%)] Loss: 6458.267578\n",
      "Train Epoch: 127 [209152/225000 (93%)] Loss: 6429.300781\n",
      "Train Epoch: 127 [213248/225000 (95%)] Loss: 8321.849609\n",
      "Train Epoch: 127 [217344/225000 (97%)] Loss: 6610.904297\n",
      "Train Epoch: 127 [221440/225000 (98%)] Loss: 6470.105469\n",
      "    epoch          : 127\n",
      "    loss           : 6633.32461093039\n",
      "    val_loss       : 6635.172122292981\n",
      "Train Epoch: 128 [256/225000 (0%)] Loss: 6589.324219\n",
      "Train Epoch: 128 [4352/225000 (2%)] Loss: 6468.203125\n",
      "Train Epoch: 128 [8448/225000 (4%)] Loss: 6567.125000\n",
      "Train Epoch: 128 [12544/225000 (6%)] Loss: 6578.882812\n",
      "Train Epoch: 128 [16640/225000 (7%)] Loss: 6611.191406\n",
      "Train Epoch: 128 [20736/225000 (9%)] Loss: 6552.716797\n",
      "Train Epoch: 128 [24832/225000 (11%)] Loss: 6356.796875\n",
      "Train Epoch: 128 [28928/225000 (13%)] Loss: 6438.119141\n",
      "Train Epoch: 128 [33024/225000 (15%)] Loss: 6514.529297\n",
      "Train Epoch: 128 [37120/225000 (16%)] Loss: 6607.343750\n",
      "Train Epoch: 128 [41216/225000 (18%)] Loss: 6687.349609\n",
      "Train Epoch: 128 [45312/225000 (20%)] Loss: 6517.113281\n",
      "Train Epoch: 128 [49408/225000 (22%)] Loss: 6482.642578\n",
      "Train Epoch: 128 [53504/225000 (24%)] Loss: 6553.546875\n",
      "Train Epoch: 128 [57600/225000 (26%)] Loss: 6525.349609\n",
      "Train Epoch: 128 [61696/225000 (27%)] Loss: 6613.326172\n",
      "Train Epoch: 128 [65792/225000 (29%)] Loss: 6630.167969\n",
      "Train Epoch: 128 [69888/225000 (31%)] Loss: 6626.480469\n",
      "Train Epoch: 128 [73984/225000 (33%)] Loss: 6535.888672\n",
      "Train Epoch: 128 [78080/225000 (35%)] Loss: 6530.683594\n",
      "Train Epoch: 128 [82176/225000 (37%)] Loss: 6552.980469\n",
      "Train Epoch: 128 [86272/225000 (38%)] Loss: 6608.857422\n",
      "Train Epoch: 128 [90368/225000 (40%)] Loss: 6510.916016\n",
      "Train Epoch: 128 [94464/225000 (42%)] Loss: 6591.347656\n",
      "Train Epoch: 128 [98560/225000 (44%)] Loss: 6616.789062\n",
      "Train Epoch: 128 [102656/225000 (46%)] Loss: 6509.937500\n",
      "Train Epoch: 128 [106752/225000 (47%)] Loss: 6633.152344\n",
      "Train Epoch: 128 [110848/225000 (49%)] Loss: 8250.191406\n",
      "Train Epoch: 128 [114944/225000 (51%)] Loss: 6488.896484\n",
      "Train Epoch: 128 [119040/225000 (53%)] Loss: 6573.847656\n",
      "Train Epoch: 128 [123136/225000 (55%)] Loss: 6530.640625\n",
      "Train Epoch: 128 [127232/225000 (57%)] Loss: 6517.912109\n",
      "Train Epoch: 128 [131328/225000 (58%)] Loss: 6678.361328\n",
      "Train Epoch: 128 [135424/225000 (60%)] Loss: 6634.099609\n",
      "Train Epoch: 128 [139520/225000 (62%)] Loss: 6412.808594\n",
      "Train Epoch: 128 [143616/225000 (64%)] Loss: 6471.683594\n",
      "Train Epoch: 128 [147712/225000 (66%)] Loss: 6605.095703\n",
      "Train Epoch: 128 [151808/225000 (67%)] Loss: 6683.828125\n",
      "Train Epoch: 128 [155904/225000 (69%)] Loss: 6610.542969\n",
      "Train Epoch: 128 [160000/225000 (71%)] Loss: 6522.126953\n",
      "Train Epoch: 128 [164096/225000 (73%)] Loss: 6575.687500\n",
      "Train Epoch: 128 [168192/225000 (75%)] Loss: 6484.369141\n",
      "Train Epoch: 128 [172288/225000 (77%)] Loss: 6446.777344\n",
      "Train Epoch: 128 [176384/225000 (78%)] Loss: 6463.925781\n",
      "Train Epoch: 128 [180480/225000 (80%)] Loss: 6546.072266\n",
      "Train Epoch: 128 [184576/225000 (82%)] Loss: 6514.058594\n",
      "Train Epoch: 128 [188672/225000 (84%)] Loss: 6545.285156\n",
      "Train Epoch: 128 [192768/225000 (86%)] Loss: 6453.347656\n",
      "Train Epoch: 128 [196864/225000 (87%)] Loss: 6595.033203\n",
      "Train Epoch: 128 [200960/225000 (89%)] Loss: 6623.841797\n",
      "Train Epoch: 128 [205056/225000 (91%)] Loss: 6566.923828\n",
      "Train Epoch: 128 [209152/225000 (93%)] Loss: 6507.755859\n",
      "Train Epoch: 128 [213248/225000 (95%)] Loss: 6571.003906\n",
      "Train Epoch: 128 [217344/225000 (97%)] Loss: 6605.740234\n",
      "Train Epoch: 128 [221440/225000 (98%)] Loss: 6550.082031\n",
      "    epoch          : 128\n",
      "    loss           : 6642.098802794369\n",
      "    val_loss       : 6617.485605671698\n",
      "Train Epoch: 129 [256/225000 (0%)] Loss: 6511.451172\n",
      "Train Epoch: 129 [4352/225000 (2%)] Loss: 6622.666016\n",
      "Train Epoch: 129 [8448/225000 (4%)] Loss: 6550.164062\n",
      "Train Epoch: 129 [12544/225000 (6%)] Loss: 6570.148438\n",
      "Train Epoch: 129 [16640/225000 (7%)] Loss: 6498.525391\n",
      "Train Epoch: 129 [20736/225000 (9%)] Loss: 6648.214844\n",
      "Train Epoch: 129 [24832/225000 (11%)] Loss: 6509.238281\n",
      "Train Epoch: 129 [28928/225000 (13%)] Loss: 6479.589844\n",
      "Train Epoch: 129 [33024/225000 (15%)] Loss: 6566.970703\n",
      "Train Epoch: 129 [37120/225000 (16%)] Loss: 6512.734375\n",
      "Train Epoch: 129 [41216/225000 (18%)] Loss: 6453.464844\n",
      "Train Epoch: 129 [45312/225000 (20%)] Loss: 6502.095703\n",
      "Train Epoch: 129 [49408/225000 (22%)] Loss: 6455.548828\n",
      "Train Epoch: 129 [53504/225000 (24%)] Loss: 6489.541016\n",
      "Train Epoch: 129 [57600/225000 (26%)] Loss: 6549.892578\n",
      "Train Epoch: 129 [61696/225000 (27%)] Loss: 6507.691406\n",
      "Train Epoch: 129 [65792/225000 (29%)] Loss: 6747.683594\n",
      "Train Epoch: 129 [69888/225000 (31%)] Loss: 6378.917969\n",
      "Train Epoch: 129 [73984/225000 (33%)] Loss: 6778.496094\n",
      "Train Epoch: 129 [78080/225000 (35%)] Loss: 6554.742188\n",
      "Train Epoch: 129 [82176/225000 (37%)] Loss: 6537.298828\n",
      "Train Epoch: 129 [86272/225000 (38%)] Loss: 6494.974609\n",
      "Train Epoch: 129 [90368/225000 (40%)] Loss: 6478.556641\n",
      "Train Epoch: 129 [94464/225000 (42%)] Loss: 6483.619141\n",
      "Train Epoch: 129 [98560/225000 (44%)] Loss: 6399.974609\n",
      "Train Epoch: 129 [102656/225000 (46%)] Loss: 6652.900391\n",
      "Train Epoch: 129 [106752/225000 (47%)] Loss: 6565.201172\n",
      "Train Epoch: 129 [110848/225000 (49%)] Loss: 6552.443359\n",
      "Train Epoch: 129 [114944/225000 (51%)] Loss: 6635.681641\n",
      "Train Epoch: 129 [119040/225000 (53%)] Loss: 6546.570312\n",
      "Train Epoch: 129 [123136/225000 (55%)] Loss: 6613.412109\n",
      "Train Epoch: 129 [127232/225000 (57%)] Loss: 6644.384766\n",
      "Train Epoch: 129 [131328/225000 (58%)] Loss: 6593.984375\n",
      "Train Epoch: 129 [135424/225000 (60%)] Loss: 6515.410156\n",
      "Train Epoch: 129 [139520/225000 (62%)] Loss: 6597.949219\n",
      "Train Epoch: 129 [143616/225000 (64%)] Loss: 6532.250000\n",
      "Train Epoch: 129 [147712/225000 (66%)] Loss: 6486.802734\n",
      "Train Epoch: 129 [151808/225000 (67%)] Loss: 6474.685547\n",
      "Train Epoch: 129 [155904/225000 (69%)] Loss: 6608.892578\n",
      "Train Epoch: 129 [160000/225000 (71%)] Loss: 6615.197266\n",
      "Train Epoch: 129 [164096/225000 (73%)] Loss: 6557.804688\n",
      "Train Epoch: 129 [168192/225000 (75%)] Loss: 6561.777344\n",
      "Train Epoch: 129 [172288/225000 (77%)] Loss: 6602.958984\n",
      "Train Epoch: 129 [176384/225000 (78%)] Loss: 6697.968750\n",
      "Train Epoch: 129 [180480/225000 (80%)] Loss: 6614.123047\n",
      "Train Epoch: 129 [184576/225000 (82%)] Loss: 6449.156250\n",
      "Train Epoch: 129 [188672/225000 (84%)] Loss: 6508.035156\n",
      "Train Epoch: 129 [192768/225000 (86%)] Loss: 6478.939453\n",
      "Train Epoch: 129 [196864/225000 (87%)] Loss: 6651.433594\n",
      "Train Epoch: 129 [200960/225000 (89%)] Loss: 6607.013672\n",
      "Train Epoch: 129 [205056/225000 (91%)] Loss: 6391.876953\n",
      "Train Epoch: 129 [209152/225000 (93%)] Loss: 6607.126953\n",
      "Train Epoch: 129 [213248/225000 (95%)] Loss: 6672.009766\n",
      "Train Epoch: 129 [217344/225000 (97%)] Loss: 6630.587891\n",
      "Train Epoch: 129 [221440/225000 (98%)] Loss: 6587.033203\n",
      "    epoch          : 129\n",
      "    loss           : 6663.936550101323\n",
      "    val_loss       : 6660.287838523485\n",
      "Train Epoch: 130 [256/225000 (0%)] Loss: 6489.017578\n",
      "Train Epoch: 130 [4352/225000 (2%)] Loss: 6522.636719\n",
      "Train Epoch: 130 [8448/225000 (4%)] Loss: 6514.546875\n",
      "Train Epoch: 130 [12544/225000 (6%)] Loss: 8394.619141\n",
      "Train Epoch: 130 [16640/225000 (7%)] Loss: 6418.125000\n",
      "Train Epoch: 130 [20736/225000 (9%)] Loss: 6536.816406\n",
      "Train Epoch: 130 [24832/225000 (11%)] Loss: 6452.410156\n",
      "Train Epoch: 130 [28928/225000 (13%)] Loss: 6459.007812\n",
      "Train Epoch: 130 [33024/225000 (15%)] Loss: 6550.876953\n",
      "Train Epoch: 130 [37120/225000 (16%)] Loss: 6495.953125\n",
      "Train Epoch: 130 [41216/225000 (18%)] Loss: 6476.542969\n",
      "Train Epoch: 130 [45312/225000 (20%)] Loss: 6617.205078\n",
      "Train Epoch: 130 [49408/225000 (22%)] Loss: 6487.970703\n",
      "Train Epoch: 130 [53504/225000 (24%)] Loss: 6486.177734\n",
      "Train Epoch: 130 [57600/225000 (26%)] Loss: 6393.470703\n",
      "Train Epoch: 130 [61696/225000 (27%)] Loss: 6632.861328\n",
      "Train Epoch: 130 [65792/225000 (29%)] Loss: 6634.375000\n",
      "Train Epoch: 130 [69888/225000 (31%)] Loss: 6443.347656\n",
      "Train Epoch: 130 [73984/225000 (33%)] Loss: 6561.726562\n",
      "Train Epoch: 130 [78080/225000 (35%)] Loss: 6516.214844\n",
      "Train Epoch: 130 [82176/225000 (37%)] Loss: 6535.433594\n",
      "Train Epoch: 130 [86272/225000 (38%)] Loss: 6546.804688\n",
      "Train Epoch: 130 [90368/225000 (40%)] Loss: 6593.822266\n",
      "Train Epoch: 130 [94464/225000 (42%)] Loss: 6583.388672\n",
      "Train Epoch: 130 [98560/225000 (44%)] Loss: 6555.212891\n",
      "Train Epoch: 130 [102656/225000 (46%)] Loss: 6486.892578\n",
      "Train Epoch: 130 [106752/225000 (47%)] Loss: 6547.050781\n",
      "Train Epoch: 130 [110848/225000 (49%)] Loss: 6561.005859\n",
      "Train Epoch: 130 [114944/225000 (51%)] Loss: 6707.228516\n",
      "Train Epoch: 130 [119040/225000 (53%)] Loss: 6601.535156\n",
      "Train Epoch: 130 [123136/225000 (55%)] Loss: 6434.623047\n",
      "Train Epoch: 130 [127232/225000 (57%)] Loss: 6661.974609\n",
      "Train Epoch: 130 [131328/225000 (58%)] Loss: 6611.488281\n",
      "Train Epoch: 130 [135424/225000 (60%)] Loss: 6609.080078\n",
      "Train Epoch: 130 [139520/225000 (62%)] Loss: 6558.281250\n",
      "Train Epoch: 130 [143616/225000 (64%)] Loss: 6575.818359\n",
      "Train Epoch: 130 [147712/225000 (66%)] Loss: 6508.839844\n",
      "Train Epoch: 130 [151808/225000 (67%)] Loss: 6558.253906\n",
      "Train Epoch: 130 [155904/225000 (69%)] Loss: 6651.728516\n",
      "Train Epoch: 130 [160000/225000 (71%)] Loss: 6500.763672\n",
      "Train Epoch: 130 [164096/225000 (73%)] Loss: 6488.197266\n",
      "Train Epoch: 130 [168192/225000 (75%)] Loss: 6557.154297\n",
      "Train Epoch: 130 [172288/225000 (77%)] Loss: 6558.355469\n",
      "Train Epoch: 130 [176384/225000 (78%)] Loss: 6518.687500\n",
      "Train Epoch: 130 [180480/225000 (80%)] Loss: 6599.091797\n",
      "Train Epoch: 130 [184576/225000 (82%)] Loss: 6471.736328\n",
      "Train Epoch: 130 [188672/225000 (84%)] Loss: 6489.078125\n",
      "Train Epoch: 130 [192768/225000 (86%)] Loss: 6528.626953\n",
      "Train Epoch: 130 [196864/225000 (87%)] Loss: 6577.072266\n",
      "Train Epoch: 130 [200960/225000 (89%)] Loss: 12916.138672\n",
      "Train Epoch: 130 [205056/225000 (91%)] Loss: 6402.892578\n",
      "Train Epoch: 130 [209152/225000 (93%)] Loss: 6587.453125\n",
      "Train Epoch: 130 [213248/225000 (95%)] Loss: 6580.513672\n",
      "Train Epoch: 130 [217344/225000 (97%)] Loss: 6591.677734\n",
      "Train Epoch: 130 [221440/225000 (98%)] Loss: 6514.083984\n",
      "    epoch          : 130\n",
      "    loss           : 6604.439430905148\n",
      "    val_loss       : 6617.199187901555\n",
      "Train Epoch: 131 [256/225000 (0%)] Loss: 6577.195312\n",
      "Train Epoch: 131 [4352/225000 (2%)] Loss: 6465.417969\n",
      "Train Epoch: 131 [8448/225000 (4%)] Loss: 6529.076172\n",
      "Train Epoch: 131 [12544/225000 (6%)] Loss: 6439.765625\n",
      "Train Epoch: 131 [16640/225000 (7%)] Loss: 6594.128906\n",
      "Train Epoch: 131 [20736/225000 (9%)] Loss: 6580.357422\n",
      "Train Epoch: 131 [24832/225000 (11%)] Loss: 6507.558594\n",
      "Train Epoch: 131 [28928/225000 (13%)] Loss: 6516.378906\n",
      "Train Epoch: 131 [33024/225000 (15%)] Loss: 6671.818359\n",
      "Train Epoch: 131 [37120/225000 (16%)] Loss: 6647.730469\n",
      "Train Epoch: 131 [41216/225000 (18%)] Loss: 6589.324219\n",
      "Train Epoch: 131 [45312/225000 (20%)] Loss: 6506.162109\n",
      "Train Epoch: 131 [49408/225000 (22%)] Loss: 6565.947266\n",
      "Train Epoch: 131 [53504/225000 (24%)] Loss: 6512.900391\n",
      "Train Epoch: 131 [57600/225000 (26%)] Loss: 6481.791016\n",
      "Train Epoch: 131 [61696/225000 (27%)] Loss: 6601.314453\n",
      "Train Epoch: 131 [65792/225000 (29%)] Loss: 6464.980469\n",
      "Train Epoch: 131 [69888/225000 (31%)] Loss: 6540.044922\n",
      "Train Epoch: 131 [73984/225000 (33%)] Loss: 6473.232422\n",
      "Train Epoch: 131 [78080/225000 (35%)] Loss: 6524.488281\n",
      "Train Epoch: 131 [82176/225000 (37%)] Loss: 6441.554688\n",
      "Train Epoch: 131 [86272/225000 (38%)] Loss: 6563.234375\n",
      "Train Epoch: 131 [90368/225000 (40%)] Loss: 6556.933594\n",
      "Train Epoch: 131 [94464/225000 (42%)] Loss: 6409.607422\n",
      "Train Epoch: 131 [98560/225000 (44%)] Loss: 6554.806641\n",
      "Train Epoch: 131 [102656/225000 (46%)] Loss: 6538.818359\n",
      "Train Epoch: 131 [106752/225000 (47%)] Loss: 6642.869141\n",
      "Train Epoch: 131 [110848/225000 (49%)] Loss: 6570.242188\n",
      "Train Epoch: 131 [114944/225000 (51%)] Loss: 6546.488281\n",
      "Train Epoch: 131 [119040/225000 (53%)] Loss: 6589.859375\n",
      "Train Epoch: 131 [123136/225000 (55%)] Loss: 6526.156250\n",
      "Train Epoch: 131 [127232/225000 (57%)] Loss: 6426.279297\n",
      "Train Epoch: 131 [131328/225000 (58%)] Loss: 6429.468750\n",
      "Train Epoch: 131 [135424/225000 (60%)] Loss: 6463.863281\n",
      "Train Epoch: 131 [139520/225000 (62%)] Loss: 6619.537109\n",
      "Train Epoch: 131 [143616/225000 (64%)] Loss: 6622.892578\n",
      "Train Epoch: 131 [147712/225000 (66%)] Loss: 6601.003906\n",
      "Train Epoch: 131 [151808/225000 (67%)] Loss: 6442.914062\n",
      "Train Epoch: 131 [155904/225000 (69%)] Loss: 6448.406250\n",
      "Train Epoch: 131 [160000/225000 (71%)] Loss: 6573.507812\n",
      "Train Epoch: 131 [164096/225000 (73%)] Loss: 6498.859375\n",
      "Train Epoch: 131 [168192/225000 (75%)] Loss: 6447.101562\n",
      "Train Epoch: 131 [172288/225000 (77%)] Loss: 6505.123047\n",
      "Train Epoch: 131 [176384/225000 (78%)] Loss: 6656.080078\n",
      "Train Epoch: 131 [180480/225000 (80%)] Loss: 6511.244141\n",
      "Train Epoch: 131 [184576/225000 (82%)] Loss: 6675.117188\n",
      "Train Epoch: 131 [188672/225000 (84%)] Loss: 6569.914062\n",
      "Train Epoch: 131 [192768/225000 (86%)] Loss: 6738.156250\n",
      "Train Epoch: 131 [196864/225000 (87%)] Loss: 6558.683594\n",
      "Train Epoch: 131 [200960/225000 (89%)] Loss: 6507.261719\n",
      "Train Epoch: 131 [205056/225000 (91%)] Loss: 6577.464844\n",
      "Train Epoch: 131 [209152/225000 (93%)] Loss: 6719.941406\n",
      "Train Epoch: 131 [213248/225000 (95%)] Loss: 6508.666016\n",
      "Train Epoch: 131 [217344/225000 (97%)] Loss: 6545.273438\n",
      "Train Epoch: 131 [221440/225000 (98%)] Loss: 6545.111328\n",
      "    epoch          : 131\n",
      "    loss           : 6599.245403823592\n",
      "    val_loss       : 6792.440365366181\n",
      "Train Epoch: 132 [256/225000 (0%)] Loss: 6686.755859\n",
      "Train Epoch: 132 [4352/225000 (2%)] Loss: 6484.767578\n",
      "Train Epoch: 132 [8448/225000 (4%)] Loss: 6501.429688\n",
      "Train Epoch: 132 [12544/225000 (6%)] Loss: 6503.048828\n",
      "Train Epoch: 132 [16640/225000 (7%)] Loss: 6535.191406\n",
      "Train Epoch: 132 [20736/225000 (9%)] Loss: 6686.558594\n",
      "Train Epoch: 132 [24832/225000 (11%)] Loss: 6586.435547\n",
      "Train Epoch: 132 [28928/225000 (13%)] Loss: 6556.132812\n",
      "Train Epoch: 132 [33024/225000 (15%)] Loss: 6569.154297\n",
      "Train Epoch: 132 [37120/225000 (16%)] Loss: 6431.603516\n",
      "Train Epoch: 132 [41216/225000 (18%)] Loss: 6518.142578\n",
      "Train Epoch: 132 [45312/225000 (20%)] Loss: 6442.527344\n",
      "Train Epoch: 132 [49408/225000 (22%)] Loss: 6507.560547\n",
      "Train Epoch: 132 [53504/225000 (24%)] Loss: 6621.390625\n",
      "Train Epoch: 132 [57600/225000 (26%)] Loss: 6669.072266\n",
      "Train Epoch: 132 [61696/225000 (27%)] Loss: 6506.203125\n",
      "Train Epoch: 132 [65792/225000 (29%)] Loss: 6572.025391\n",
      "Train Epoch: 132 [69888/225000 (31%)] Loss: 6478.054688\n",
      "Train Epoch: 132 [73984/225000 (33%)] Loss: 6610.833984\n",
      "Train Epoch: 132 [78080/225000 (35%)] Loss: 6544.833984\n",
      "Train Epoch: 132 [82176/225000 (37%)] Loss: 6479.109375\n",
      "Train Epoch: 132 [86272/225000 (38%)] Loss: 6525.078125\n",
      "Train Epoch: 132 [90368/225000 (40%)] Loss: 6385.080078\n",
      "Train Epoch: 132 [94464/225000 (42%)] Loss: 6467.519531\n",
      "Train Epoch: 132 [98560/225000 (44%)] Loss: 6460.996094\n",
      "Train Epoch: 132 [102656/225000 (46%)] Loss: 6541.568359\n",
      "Train Epoch: 132 [106752/225000 (47%)] Loss: 6612.232422\n",
      "Train Epoch: 132 [110848/225000 (49%)] Loss: 6335.603516\n",
      "Train Epoch: 132 [114944/225000 (51%)] Loss: 6587.509766\n",
      "Train Epoch: 132 [119040/225000 (53%)] Loss: 6458.812500\n",
      "Train Epoch: 132 [123136/225000 (55%)] Loss: 6528.962891\n",
      "Train Epoch: 132 [127232/225000 (57%)] Loss: 6671.093750\n",
      "Train Epoch: 132 [131328/225000 (58%)] Loss: 6568.142578\n",
      "Train Epoch: 132 [135424/225000 (60%)] Loss: 6555.318359\n",
      "Train Epoch: 132 [139520/225000 (62%)] Loss: 6575.082031\n",
      "Train Epoch: 132 [143616/225000 (64%)] Loss: 6470.869141\n",
      "Train Epoch: 132 [147712/225000 (66%)] Loss: 6476.076172\n",
      "Train Epoch: 132 [151808/225000 (67%)] Loss: 6443.751953\n",
      "Train Epoch: 132 [155904/225000 (69%)] Loss: 6460.052734\n",
      "Train Epoch: 132 [160000/225000 (71%)] Loss: 6623.554688\n",
      "Train Epoch: 132 [164096/225000 (73%)] Loss: 6433.433594\n",
      "Train Epoch: 132 [168192/225000 (75%)] Loss: 6467.757812\n",
      "Train Epoch: 132 [172288/225000 (77%)] Loss: 6491.423828\n",
      "Train Epoch: 132 [176384/225000 (78%)] Loss: 6432.033203\n",
      "Train Epoch: 132 [180480/225000 (80%)] Loss: 6812.859375\n",
      "Train Epoch: 132 [184576/225000 (82%)] Loss: 6610.712891\n",
      "Train Epoch: 132 [188672/225000 (84%)] Loss: 6641.753906\n",
      "Train Epoch: 132 [192768/225000 (86%)] Loss: 6583.421875\n",
      "Train Epoch: 132 [196864/225000 (87%)] Loss: 6389.898438\n",
      "Train Epoch: 132 [200960/225000 (89%)] Loss: 6576.421875\n",
      "Train Epoch: 132 [205056/225000 (91%)] Loss: 6616.279297\n",
      "Train Epoch: 132 [209152/225000 (93%)] Loss: 6516.060547\n",
      "Train Epoch: 132 [213248/225000 (95%)] Loss: 6642.558594\n",
      "Train Epoch: 132 [217344/225000 (97%)] Loss: 6551.201172\n",
      "Train Epoch: 132 [221440/225000 (98%)] Loss: 6542.794922\n",
      "    epoch          : 132\n",
      "    loss           : 6577.709192130617\n",
      "    val_loss       : 6634.51144080746\n",
      "Train Epoch: 133 [256/225000 (0%)] Loss: 6525.105469\n",
      "Train Epoch: 133 [4352/225000 (2%)] Loss: 6499.769531\n",
      "Train Epoch: 133 [8448/225000 (4%)] Loss: 6521.611328\n",
      "Train Epoch: 133 [12544/225000 (6%)] Loss: 6465.099609\n",
      "Train Epoch: 133 [16640/225000 (7%)] Loss: 6484.234375\n",
      "Train Epoch: 133 [20736/225000 (9%)] Loss: 6647.003906\n",
      "Train Epoch: 133 [24832/225000 (11%)] Loss: 6536.898438\n",
      "Train Epoch: 133 [28928/225000 (13%)] Loss: 6539.599609\n",
      "Train Epoch: 133 [33024/225000 (15%)] Loss: 6359.746094\n",
      "Train Epoch: 133 [37120/225000 (16%)] Loss: 6658.134766\n",
      "Train Epoch: 133 [41216/225000 (18%)] Loss: 19003.636719\n",
      "Train Epoch: 133 [45312/225000 (20%)] Loss: 6620.513672\n",
      "Train Epoch: 133 [49408/225000 (22%)] Loss: 6625.761719\n",
      "Train Epoch: 133 [53504/225000 (24%)] Loss: 6618.685547\n",
      "Train Epoch: 133 [57600/225000 (26%)] Loss: 6549.667969\n",
      "Train Epoch: 133 [61696/225000 (27%)] Loss: 6649.859375\n",
      "Train Epoch: 133 [65792/225000 (29%)] Loss: 6589.089844\n",
      "Train Epoch: 133 [69888/225000 (31%)] Loss: 6525.082031\n",
      "Train Epoch: 133 [73984/225000 (33%)] Loss: 6393.585938\n",
      "Train Epoch: 133 [78080/225000 (35%)] Loss: 6669.660156\n",
      "Train Epoch: 133 [82176/225000 (37%)] Loss: 6572.195312\n",
      "Train Epoch: 133 [86272/225000 (38%)] Loss: 6520.488281\n",
      "Train Epoch: 133 [90368/225000 (40%)] Loss: 6488.027344\n",
      "Train Epoch: 133 [94464/225000 (42%)] Loss: 6422.339844\n",
      "Train Epoch: 133 [98560/225000 (44%)] Loss: 6400.808594\n",
      "Train Epoch: 133 [102656/225000 (46%)] Loss: 6531.451172\n",
      "Train Epoch: 133 [106752/225000 (47%)] Loss: 6677.699219\n",
      "Train Epoch: 133 [110848/225000 (49%)] Loss: 6560.191406\n",
      "Train Epoch: 133 [114944/225000 (51%)] Loss: 6408.267578\n",
      "Train Epoch: 133 [119040/225000 (53%)] Loss: 6499.802734\n",
      "Train Epoch: 133 [123136/225000 (55%)] Loss: 6435.451172\n",
      "Train Epoch: 133 [127232/225000 (57%)] Loss: 6519.087891\n",
      "Train Epoch: 133 [131328/225000 (58%)] Loss: 6690.404297\n",
      "Train Epoch: 133 [135424/225000 (60%)] Loss: 6556.109375\n",
      "Train Epoch: 133 [139520/225000 (62%)] Loss: 6490.980469\n",
      "Train Epoch: 133 [143616/225000 (64%)] Loss: 6452.724609\n",
      "Train Epoch: 133 [147712/225000 (66%)] Loss: 6469.060547\n",
      "Train Epoch: 133 [151808/225000 (67%)] Loss: 6553.892578\n",
      "Train Epoch: 133 [155904/225000 (69%)] Loss: 6499.953125\n",
      "Train Epoch: 133 [160000/225000 (71%)] Loss: 6546.070312\n",
      "Train Epoch: 133 [164096/225000 (73%)] Loss: 6544.839844\n",
      "Train Epoch: 133 [168192/225000 (75%)] Loss: 6440.046875\n",
      "Train Epoch: 133 [172288/225000 (77%)] Loss: 6586.683594\n",
      "Train Epoch: 133 [176384/225000 (78%)] Loss: 6490.978516\n",
      "Train Epoch: 133 [180480/225000 (80%)] Loss: 6442.574219\n",
      "Train Epoch: 133 [184576/225000 (82%)] Loss: 6566.423828\n",
      "Train Epoch: 133 [188672/225000 (84%)] Loss: 6544.066406\n",
      "Train Epoch: 133 [192768/225000 (86%)] Loss: 6425.458984\n",
      "Train Epoch: 133 [196864/225000 (87%)] Loss: 6422.369141\n",
      "Train Epoch: 133 [200960/225000 (89%)] Loss: 6518.978516\n",
      "Train Epoch: 133 [205056/225000 (91%)] Loss: 6548.015625\n",
      "Train Epoch: 133 [209152/225000 (93%)] Loss: 6476.408203\n",
      "Train Epoch: 133 [213248/225000 (95%)] Loss: 6575.675781\n",
      "Train Epoch: 133 [217344/225000 (97%)] Loss: 6463.234375\n",
      "Train Epoch: 133 [221440/225000 (98%)] Loss: 6461.976562\n",
      "    epoch          : 133\n",
      "    loss           : 6660.547183855944\n",
      "    val_loss       : 6577.698298529703\n",
      "Train Epoch: 134 [256/225000 (0%)] Loss: 6356.425781\n",
      "Train Epoch: 134 [4352/225000 (2%)] Loss: 8300.871094\n",
      "Train Epoch: 134 [8448/225000 (4%)] Loss: 6502.759766\n",
      "Train Epoch: 134 [12544/225000 (6%)] Loss: 6718.638672\n",
      "Train Epoch: 134 [16640/225000 (7%)] Loss: 6615.972656\n",
      "Train Epoch: 134 [20736/225000 (9%)] Loss: 6458.699219\n",
      "Train Epoch: 134 [24832/225000 (11%)] Loss: 6553.328125\n",
      "Train Epoch: 134 [28928/225000 (13%)] Loss: 6455.447266\n",
      "Train Epoch: 134 [33024/225000 (15%)] Loss: 6470.208984\n",
      "Train Epoch: 134 [37120/225000 (16%)] Loss: 6632.554688\n",
      "Train Epoch: 134 [41216/225000 (18%)] Loss: 6547.472656\n",
      "Train Epoch: 134 [45312/225000 (20%)] Loss: 6535.947266\n",
      "Train Epoch: 134 [49408/225000 (22%)] Loss: 6459.890625\n",
      "Train Epoch: 134 [53504/225000 (24%)] Loss: 6343.515625\n",
      "Train Epoch: 134 [57600/225000 (26%)] Loss: 6444.652344\n",
      "Train Epoch: 134 [61696/225000 (27%)] Loss: 6699.693359\n",
      "Train Epoch: 134 [65792/225000 (29%)] Loss: 6478.490234\n",
      "Train Epoch: 134 [69888/225000 (31%)] Loss: 6492.445312\n",
      "Train Epoch: 134 [73984/225000 (33%)] Loss: 6570.392578\n",
      "Train Epoch: 134 [78080/225000 (35%)] Loss: 6535.058594\n",
      "Train Epoch: 134 [82176/225000 (37%)] Loss: 13075.191406\n",
      "Train Epoch: 134 [86272/225000 (38%)] Loss: 6482.517578\n",
      "Train Epoch: 134 [90368/225000 (40%)] Loss: 6497.353516\n",
      "Train Epoch: 134 [94464/225000 (42%)] Loss: 6595.177734\n",
      "Train Epoch: 134 [98560/225000 (44%)] Loss: 6632.667969\n",
      "Train Epoch: 134 [102656/225000 (46%)] Loss: 6348.257812\n",
      "Train Epoch: 134 [106752/225000 (47%)] Loss: 6465.367188\n",
      "Train Epoch: 134 [110848/225000 (49%)] Loss: 6545.943359\n",
      "Train Epoch: 134 [114944/225000 (51%)] Loss: 6413.759766\n",
      "Train Epoch: 134 [119040/225000 (53%)] Loss: 6470.630859\n",
      "Train Epoch: 134 [123136/225000 (55%)] Loss: 6392.921875\n",
      "Train Epoch: 134 [127232/225000 (57%)] Loss: 6383.646484\n",
      "Train Epoch: 134 [131328/225000 (58%)] Loss: 6647.580078\n",
      "Train Epoch: 134 [135424/225000 (60%)] Loss: 6588.154297\n",
      "Train Epoch: 134 [139520/225000 (62%)] Loss: 6417.673828\n",
      "Train Epoch: 134 [143616/225000 (64%)] Loss: 6501.152344\n",
      "Train Epoch: 134 [147712/225000 (66%)] Loss: 6445.855469\n",
      "Train Epoch: 134 [151808/225000 (67%)] Loss: 6607.619141\n",
      "Train Epoch: 134 [155904/225000 (69%)] Loss: 6612.542969\n",
      "Train Epoch: 134 [160000/225000 (71%)] Loss: 6482.392578\n",
      "Train Epoch: 134 [164096/225000 (73%)] Loss: 6530.435547\n",
      "Train Epoch: 134 [168192/225000 (75%)] Loss: 6469.255859\n",
      "Train Epoch: 134 [172288/225000 (77%)] Loss: 6368.738281\n",
      "Train Epoch: 134 [176384/225000 (78%)] Loss: 6575.404297\n",
      "Train Epoch: 134 [180480/225000 (80%)] Loss: 6467.121094\n",
      "Train Epoch: 134 [184576/225000 (82%)] Loss: 6518.763672\n",
      "Train Epoch: 134 [188672/225000 (84%)] Loss: 6461.779297\n",
      "Train Epoch: 134 [192768/225000 (86%)] Loss: 18376.582031\n",
      "Train Epoch: 134 [196864/225000 (87%)] Loss: 6388.839844\n",
      "Train Epoch: 134 [200960/225000 (89%)] Loss: 6426.140625\n",
      "Train Epoch: 134 [205056/225000 (91%)] Loss: 6526.673828\n",
      "Train Epoch: 134 [209152/225000 (93%)] Loss: 6462.746094\n",
      "Train Epoch: 134 [213248/225000 (95%)] Loss: 6590.787109\n",
      "Train Epoch: 134 [217344/225000 (97%)] Loss: 6552.537109\n",
      "Train Epoch: 134 [221440/225000 (98%)] Loss: 6532.738281\n",
      "    epoch          : 134\n",
      "    loss           : 6619.119930540743\n",
      "    val_loss       : 6778.946144616118\n",
      "Train Epoch: 135 [256/225000 (0%)] Loss: 6433.425781\n",
      "Train Epoch: 135 [4352/225000 (2%)] Loss: 6449.146484\n",
      "Train Epoch: 135 [8448/225000 (4%)] Loss: 6508.798828\n",
      "Train Epoch: 135 [12544/225000 (6%)] Loss: 6474.126953\n",
      "Train Epoch: 135 [16640/225000 (7%)] Loss: 6486.115234\n",
      "Train Epoch: 135 [20736/225000 (9%)] Loss: 6396.998047\n",
      "Train Epoch: 135 [24832/225000 (11%)] Loss: 6542.498047\n",
      "Train Epoch: 135 [28928/225000 (13%)] Loss: 6475.255859\n",
      "Train Epoch: 135 [33024/225000 (15%)] Loss: 6476.798828\n",
      "Train Epoch: 135 [37120/225000 (16%)] Loss: 6532.855469\n",
      "Train Epoch: 135 [41216/225000 (18%)] Loss: 6606.759766\n",
      "Train Epoch: 135 [45312/225000 (20%)] Loss: 6625.810547\n",
      "Train Epoch: 135 [49408/225000 (22%)] Loss: 6540.076172\n",
      "Train Epoch: 135 [53504/225000 (24%)] Loss: 6605.507812\n",
      "Train Epoch: 135 [57600/225000 (26%)] Loss: 6447.035156\n",
      "Train Epoch: 135 [61696/225000 (27%)] Loss: 6457.736328\n",
      "Train Epoch: 135 [65792/225000 (29%)] Loss: 6522.386719\n",
      "Train Epoch: 135 [69888/225000 (31%)] Loss: 6487.621094\n",
      "Train Epoch: 135 [73984/225000 (33%)] Loss: 6438.384766\n",
      "Train Epoch: 135 [78080/225000 (35%)] Loss: 6487.214844\n",
      "Train Epoch: 135 [82176/225000 (37%)] Loss: 6556.429688\n",
      "Train Epoch: 135 [86272/225000 (38%)] Loss: 6554.130859\n",
      "Train Epoch: 135 [90368/225000 (40%)] Loss: 18438.191406\n",
      "Train Epoch: 135 [94464/225000 (42%)] Loss: 6580.220703\n",
      "Train Epoch: 135 [98560/225000 (44%)] Loss: 6471.912109\n",
      "Train Epoch: 135 [102656/225000 (46%)] Loss: 6464.171875\n",
      "Train Epoch: 135 [106752/225000 (47%)] Loss: 6420.142578\n",
      "Train Epoch: 135 [110848/225000 (49%)] Loss: 6397.664062\n",
      "Train Epoch: 135 [114944/225000 (51%)] Loss: 6366.068359\n",
      "Train Epoch: 135 [119040/225000 (53%)] Loss: 6569.687500\n",
      "Train Epoch: 135 [123136/225000 (55%)] Loss: 6465.867188\n",
      "Train Epoch: 135 [127232/225000 (57%)] Loss: 6817.572266\n",
      "Train Epoch: 135 [131328/225000 (58%)] Loss: 6495.816406\n",
      "Train Epoch: 135 [135424/225000 (60%)] Loss: 6575.937500\n",
      "Train Epoch: 135 [139520/225000 (62%)] Loss: 8254.845703\n",
      "Train Epoch: 135 [143616/225000 (64%)] Loss: 6500.791016\n",
      "Train Epoch: 135 [147712/225000 (66%)] Loss: 6736.101562\n",
      "Train Epoch: 135 [151808/225000 (67%)] Loss: 6521.169922\n",
      "Train Epoch: 135 [155904/225000 (69%)] Loss: 6509.501953\n",
      "Train Epoch: 135 [160000/225000 (71%)] Loss: 6418.138672\n",
      "Train Epoch: 135 [164096/225000 (73%)] Loss: 6645.347656\n",
      "Train Epoch: 135 [168192/225000 (75%)] Loss: 6461.484375\n",
      "Train Epoch: 135 [172288/225000 (77%)] Loss: 6548.720703\n",
      "Train Epoch: 135 [176384/225000 (78%)] Loss: 6491.169922\n",
      "Train Epoch: 135 [180480/225000 (80%)] Loss: 6416.912109\n",
      "Train Epoch: 135 [184576/225000 (82%)] Loss: 6529.710938\n",
      "Train Epoch: 135 [188672/225000 (84%)] Loss: 6441.980469\n",
      "Train Epoch: 135 [192768/225000 (86%)] Loss: 6511.125000\n",
      "Train Epoch: 135 [196864/225000 (87%)] Loss: 6463.773438\n",
      "Train Epoch: 135 [200960/225000 (89%)] Loss: 6501.283203\n",
      "Train Epoch: 135 [205056/225000 (91%)] Loss: 6561.486328\n",
      "Train Epoch: 135 [209152/225000 (93%)] Loss: 6467.902344\n",
      "Train Epoch: 135 [213248/225000 (95%)] Loss: 6547.128906\n",
      "Train Epoch: 135 [217344/225000 (97%)] Loss: 6711.871094\n",
      "Train Epoch: 135 [221440/225000 (98%)] Loss: 6501.980469\n",
      "    epoch          : 135\n",
      "    loss           : 6603.93264274033\n",
      "    val_loss       : 6857.208538314518\n",
      "Train Epoch: 136 [256/225000 (0%)] Loss: 6516.806641\n",
      "Train Epoch: 136 [4352/225000 (2%)] Loss: 6556.937500\n",
      "Train Epoch: 136 [8448/225000 (4%)] Loss: 8313.054688\n",
      "Train Epoch: 136 [12544/225000 (6%)] Loss: 6551.957031\n",
      "Train Epoch: 136 [16640/225000 (7%)] Loss: 6556.230469\n",
      "Train Epoch: 136 [20736/225000 (9%)] Loss: 6531.074219\n",
      "Train Epoch: 136 [24832/225000 (11%)] Loss: 6428.957031\n",
      "Train Epoch: 136 [28928/225000 (13%)] Loss: 6533.904297\n",
      "Train Epoch: 136 [33024/225000 (15%)] Loss: 6399.435547\n",
      "Train Epoch: 136 [37120/225000 (16%)] Loss: 6609.994141\n",
      "Train Epoch: 136 [41216/225000 (18%)] Loss: 6659.414062\n",
      "Train Epoch: 136 [45312/225000 (20%)] Loss: 8324.531250\n",
      "Train Epoch: 136 [49408/225000 (22%)] Loss: 6460.980469\n",
      "Train Epoch: 136 [53504/225000 (24%)] Loss: 6428.429688\n",
      "Train Epoch: 136 [57600/225000 (26%)] Loss: 6506.611328\n",
      "Train Epoch: 136 [61696/225000 (27%)] Loss: 6438.093750\n",
      "Train Epoch: 136 [65792/225000 (29%)] Loss: 6475.472656\n",
      "Train Epoch: 136 [69888/225000 (31%)] Loss: 6611.048828\n",
      "Train Epoch: 136 [73984/225000 (33%)] Loss: 6491.976562\n",
      "Train Epoch: 136 [78080/225000 (35%)] Loss: 6386.861328\n",
      "Train Epoch: 136 [82176/225000 (37%)] Loss: 6452.841797\n",
      "Train Epoch: 136 [86272/225000 (38%)] Loss: 6530.835938\n",
      "Train Epoch: 136 [90368/225000 (40%)] Loss: 6541.232422\n",
      "Train Epoch: 136 [94464/225000 (42%)] Loss: 6481.814453\n",
      "Train Epoch: 136 [98560/225000 (44%)] Loss: 6433.820312\n",
      "Train Epoch: 136 [102656/225000 (46%)] Loss: 6537.519531\n",
      "Train Epoch: 136 [106752/225000 (47%)] Loss: 6555.613281\n",
      "Train Epoch: 136 [110848/225000 (49%)] Loss: 6411.107422\n",
      "Train Epoch: 136 [114944/225000 (51%)] Loss: 6510.042969\n",
      "Train Epoch: 136 [119040/225000 (53%)] Loss: 13133.623047\n",
      "Train Epoch: 136 [123136/225000 (55%)] Loss: 6501.468750\n",
      "Train Epoch: 136 [127232/225000 (57%)] Loss: 6473.621094\n",
      "Train Epoch: 136 [131328/225000 (58%)] Loss: 6470.111328\n",
      "Train Epoch: 136 [135424/225000 (60%)] Loss: 6511.455078\n",
      "Train Epoch: 136 [139520/225000 (62%)] Loss: 6402.253906\n",
      "Train Epoch: 136 [143616/225000 (64%)] Loss: 6460.041016\n",
      "Train Epoch: 136 [147712/225000 (66%)] Loss: 6438.400391\n",
      "Train Epoch: 136 [151808/225000 (67%)] Loss: 6469.779297\n",
      "Train Epoch: 136 [155904/225000 (69%)] Loss: 6541.435547\n",
      "Train Epoch: 136 [160000/225000 (71%)] Loss: 6543.279297\n",
      "Train Epoch: 136 [164096/225000 (73%)] Loss: 6480.595703\n",
      "Train Epoch: 136 [168192/225000 (75%)] Loss: 6566.308594\n",
      "Train Epoch: 136 [172288/225000 (77%)] Loss: 6547.818359\n",
      "Train Epoch: 136 [176384/225000 (78%)] Loss: 6523.662109\n",
      "Train Epoch: 136 [180480/225000 (80%)] Loss: 6424.927734\n",
      "Train Epoch: 136 [184576/225000 (82%)] Loss: 6611.939453\n",
      "Train Epoch: 136 [188672/225000 (84%)] Loss: 8334.730469\n",
      "Train Epoch: 136 [192768/225000 (86%)] Loss: 6493.962891\n",
      "Train Epoch: 136 [196864/225000 (87%)] Loss: 6479.083984\n",
      "Train Epoch: 136 [200960/225000 (89%)] Loss: 6374.595703\n",
      "Train Epoch: 136 [205056/225000 (91%)] Loss: 6580.005859\n",
      "Train Epoch: 136 [209152/225000 (93%)] Loss: 6467.347656\n",
      "Train Epoch: 136 [213248/225000 (95%)] Loss: 6544.121094\n",
      "Train Epoch: 136 [217344/225000 (97%)] Loss: 6404.310547\n",
      "Train Epoch: 136 [221440/225000 (98%)] Loss: 6604.742188\n",
      "    epoch          : 136\n",
      "    loss           : 6581.585053149886\n",
      "    val_loss       : 6550.202616500611\n",
      "Train Epoch: 137 [256/225000 (0%)] Loss: 6477.337891\n",
      "Train Epoch: 137 [4352/225000 (2%)] Loss: 6479.115234\n",
      "Train Epoch: 137 [8448/225000 (4%)] Loss: 6380.587891\n",
      "Train Epoch: 137 [12544/225000 (6%)] Loss: 6503.863281\n",
      "Train Epoch: 137 [16640/225000 (7%)] Loss: 6340.724609\n",
      "Train Epoch: 137 [20736/225000 (9%)] Loss: 6595.873047\n",
      "Train Epoch: 137 [24832/225000 (11%)] Loss: 6535.849609\n",
      "Train Epoch: 137 [28928/225000 (13%)] Loss: 6617.220703\n",
      "Train Epoch: 137 [33024/225000 (15%)] Loss: 6550.974609\n",
      "Train Epoch: 137 [37120/225000 (16%)] Loss: 6556.003906\n",
      "Train Epoch: 137 [41216/225000 (18%)] Loss: 6385.115234\n",
      "Train Epoch: 137 [45312/225000 (20%)] Loss: 6445.597656\n",
      "Train Epoch: 137 [49408/225000 (22%)] Loss: 6613.378906\n",
      "Train Epoch: 137 [53504/225000 (24%)] Loss: 6520.923828\n",
      "Train Epoch: 137 [57600/225000 (26%)] Loss: 6529.281250\n",
      "Train Epoch: 137 [61696/225000 (27%)] Loss: 6490.423828\n",
      "Train Epoch: 137 [65792/225000 (29%)] Loss: 6565.519531\n",
      "Train Epoch: 137 [69888/225000 (31%)] Loss: 6446.316406\n",
      "Train Epoch: 137 [73984/225000 (33%)] Loss: 6605.812500\n",
      "Train Epoch: 137 [78080/225000 (35%)] Loss: 8237.150391\n",
      "Train Epoch: 137 [82176/225000 (37%)] Loss: 6526.275391\n",
      "Train Epoch: 137 [86272/225000 (38%)] Loss: 6434.798828\n",
      "Train Epoch: 137 [90368/225000 (40%)] Loss: 6510.875000\n",
      "Train Epoch: 137 [94464/225000 (42%)] Loss: 6440.845703\n",
      "Train Epoch: 137 [98560/225000 (44%)] Loss: 6535.968750\n",
      "Train Epoch: 137 [102656/225000 (46%)] Loss: 6594.712891\n",
      "Train Epoch: 137 [106752/225000 (47%)] Loss: 6610.476562\n",
      "Train Epoch: 137 [110848/225000 (49%)] Loss: 6465.005859\n",
      "Train Epoch: 137 [114944/225000 (51%)] Loss: 6350.619141\n",
      "Train Epoch: 137 [119040/225000 (53%)] Loss: 6472.162109\n",
      "Train Epoch: 137 [123136/225000 (55%)] Loss: 6228.371094\n",
      "Train Epoch: 137 [127232/225000 (57%)] Loss: 6521.613281\n",
      "Train Epoch: 137 [131328/225000 (58%)] Loss: 6612.587891\n",
      "Train Epoch: 137 [135424/225000 (60%)] Loss: 6411.439453\n",
      "Train Epoch: 137 [139520/225000 (62%)] Loss: 6558.103516\n",
      "Train Epoch: 137 [143616/225000 (64%)] Loss: 6535.830078\n",
      "Train Epoch: 137 [147712/225000 (66%)] Loss: 6514.337891\n",
      "Train Epoch: 137 [151808/225000 (67%)] Loss: 6509.248047\n",
      "Train Epoch: 137 [155904/225000 (69%)] Loss: 6665.804688\n",
      "Train Epoch: 137 [160000/225000 (71%)] Loss: 6524.621094\n",
      "Train Epoch: 137 [164096/225000 (73%)] Loss: 6407.740234\n",
      "Train Epoch: 137 [168192/225000 (75%)] Loss: 6428.490234\n",
      "Train Epoch: 137 [172288/225000 (77%)] Loss: 6444.554688\n",
      "Train Epoch: 137 [176384/225000 (78%)] Loss: 6517.687500\n",
      "Train Epoch: 137 [180480/225000 (80%)] Loss: 6691.917969\n",
      "Train Epoch: 137 [184576/225000 (82%)] Loss: 6436.599609\n",
      "Train Epoch: 137 [188672/225000 (84%)] Loss: 6482.181641\n",
      "Train Epoch: 137 [192768/225000 (86%)] Loss: 6445.599609\n",
      "Train Epoch: 137 [196864/225000 (87%)] Loss: 6417.900391\n",
      "Train Epoch: 137 [200960/225000 (89%)] Loss: 6501.738281\n",
      "Train Epoch: 137 [205056/225000 (91%)] Loss: 6407.894531\n",
      "Train Epoch: 137 [209152/225000 (93%)] Loss: 6575.689453\n",
      "Train Epoch: 137 [213248/225000 (95%)] Loss: 6460.576172\n",
      "Train Epoch: 137 [217344/225000 (97%)] Loss: 6588.066406\n",
      "Train Epoch: 137 [221440/225000 (98%)] Loss: 6573.537109\n",
      "    epoch          : 137\n",
      "    loss           : 6589.753415191269\n",
      "    val_loss       : 6581.784955360452\n",
      "Train Epoch: 138 [256/225000 (0%)] Loss: 6477.636719\n",
      "Train Epoch: 138 [4352/225000 (2%)] Loss: 6411.037109\n",
      "Train Epoch: 138 [8448/225000 (4%)] Loss: 6383.441406\n",
      "Train Epoch: 138 [12544/225000 (6%)] Loss: 6507.343750\n",
      "Train Epoch: 138 [16640/225000 (7%)] Loss: 6427.654297\n",
      "Train Epoch: 138 [20736/225000 (9%)] Loss: 6399.667969\n",
      "Train Epoch: 138 [24832/225000 (11%)] Loss: 6518.628906\n",
      "Train Epoch: 138 [28928/225000 (13%)] Loss: 6467.193359\n",
      "Train Epoch: 138 [33024/225000 (15%)] Loss: 6539.976562\n",
      "Train Epoch: 138 [37120/225000 (16%)] Loss: 6559.685547\n",
      "Train Epoch: 138 [41216/225000 (18%)] Loss: 6493.388672\n",
      "Train Epoch: 138 [45312/225000 (20%)] Loss: 6503.996094\n",
      "Train Epoch: 138 [49408/225000 (22%)] Loss: 6441.972656\n",
      "Train Epoch: 138 [53504/225000 (24%)] Loss: 6464.648438\n",
      "Train Epoch: 138 [57600/225000 (26%)] Loss: 6602.400391\n",
      "Train Epoch: 138 [61696/225000 (27%)] Loss: 6444.287109\n",
      "Train Epoch: 138 [65792/225000 (29%)] Loss: 6466.367188\n",
      "Train Epoch: 138 [69888/225000 (31%)] Loss: 6624.662109\n",
      "Train Epoch: 138 [73984/225000 (33%)] Loss: 6503.580078\n",
      "Train Epoch: 138 [78080/225000 (35%)] Loss: 6565.074219\n",
      "Train Epoch: 138 [82176/225000 (37%)] Loss: 6464.390625\n",
      "Train Epoch: 138 [86272/225000 (38%)] Loss: 6586.390625\n",
      "Train Epoch: 138 [90368/225000 (40%)] Loss: 6606.265625\n",
      "Train Epoch: 138 [94464/225000 (42%)] Loss: 6448.796875\n",
      "Train Epoch: 138 [98560/225000 (44%)] Loss: 6608.705078\n",
      "Train Epoch: 138 [102656/225000 (46%)] Loss: 6659.310547\n",
      "Train Epoch: 138 [106752/225000 (47%)] Loss: 6686.673828\n",
      "Train Epoch: 138 [110848/225000 (49%)] Loss: 6481.970703\n",
      "Train Epoch: 138 [114944/225000 (51%)] Loss: 6589.119141\n",
      "Train Epoch: 138 [119040/225000 (53%)] Loss: 6492.074219\n",
      "Train Epoch: 138 [123136/225000 (55%)] Loss: 6520.484375\n",
      "Train Epoch: 138 [127232/225000 (57%)] Loss: 6532.929688\n",
      "Train Epoch: 138 [131328/225000 (58%)] Loss: 6311.617188\n",
      "Train Epoch: 138 [135424/225000 (60%)] Loss: 6562.722656\n",
      "Train Epoch: 138 [139520/225000 (62%)] Loss: 6580.207031\n",
      "Train Epoch: 138 [143616/225000 (64%)] Loss: 6601.841797\n",
      "Train Epoch: 138 [147712/225000 (66%)] Loss: 6521.027344\n",
      "Train Epoch: 138 [151808/225000 (67%)] Loss: 6486.470703\n",
      "Train Epoch: 138 [155904/225000 (69%)] Loss: 6470.115234\n",
      "Train Epoch: 138 [160000/225000 (71%)] Loss: 6463.144531\n",
      "Train Epoch: 138 [164096/225000 (73%)] Loss: 6574.261719\n",
      "Train Epoch: 138 [168192/225000 (75%)] Loss: 6473.658203\n",
      "Train Epoch: 138 [172288/225000 (77%)] Loss: 6423.650391\n",
      "Train Epoch: 138 [176384/225000 (78%)] Loss: 6533.742188\n",
      "Train Epoch: 138 [180480/225000 (80%)] Loss: 6475.457031\n",
      "Train Epoch: 138 [184576/225000 (82%)] Loss: 6572.136719\n",
      "Train Epoch: 138 [188672/225000 (84%)] Loss: 6435.796875\n",
      "Train Epoch: 138 [192768/225000 (86%)] Loss: 6389.158203\n",
      "Train Epoch: 138 [196864/225000 (87%)] Loss: 6545.453125\n",
      "Train Epoch: 138 [200960/225000 (89%)] Loss: 6463.552734\n",
      "Train Epoch: 138 [205056/225000 (91%)] Loss: 6660.837891\n",
      "Train Epoch: 138 [209152/225000 (93%)] Loss: 6536.982422\n",
      "Train Epoch: 138 [213248/225000 (95%)] Loss: 6420.287109\n",
      "Train Epoch: 138 [217344/225000 (97%)] Loss: 6456.330078\n",
      "Train Epoch: 138 [221440/225000 (98%)] Loss: 6470.880859\n",
      "    epoch          : 138\n",
      "    loss           : 6575.072340061504\n",
      "    val_loss       : 6935.802098481023\n",
      "Train Epoch: 139 [256/225000 (0%)] Loss: 6665.042969\n",
      "Train Epoch: 139 [4352/225000 (2%)] Loss: 6463.343750\n",
      "Train Epoch: 139 [8448/225000 (4%)] Loss: 6589.750000\n",
      "Train Epoch: 139 [12544/225000 (6%)] Loss: 6583.710938\n",
      "Train Epoch: 139 [16640/225000 (7%)] Loss: 6393.064453\n",
      "Train Epoch: 139 [20736/225000 (9%)] Loss: 6585.412109\n",
      "Train Epoch: 139 [24832/225000 (11%)] Loss: 6484.728516\n",
      "Train Epoch: 139 [28928/225000 (13%)] Loss: 6458.339844\n",
      "Train Epoch: 139 [33024/225000 (15%)] Loss: 6523.472656\n",
      "Train Epoch: 139 [37120/225000 (16%)] Loss: 6492.066406\n",
      "Train Epoch: 139 [41216/225000 (18%)] Loss: 6519.992188\n",
      "Train Epoch: 139 [45312/225000 (20%)] Loss: 6551.316406\n",
      "Train Epoch: 139 [49408/225000 (22%)] Loss: 6490.890625\n",
      "Train Epoch: 139 [53504/225000 (24%)] Loss: 6644.246094\n",
      "Train Epoch: 139 [57600/225000 (26%)] Loss: 6560.833984\n",
      "Train Epoch: 139 [61696/225000 (27%)] Loss: 6392.009766\n",
      "Train Epoch: 139 [65792/225000 (29%)] Loss: 6599.908203\n",
      "Train Epoch: 139 [69888/225000 (31%)] Loss: 6446.361328\n",
      "Train Epoch: 139 [73984/225000 (33%)] Loss: 6447.326172\n",
      "Train Epoch: 139 [78080/225000 (35%)] Loss: 6531.785156\n",
      "Train Epoch: 139 [82176/225000 (37%)] Loss: 6515.976562\n",
      "Train Epoch: 139 [86272/225000 (38%)] Loss: 6634.650391\n",
      "Train Epoch: 139 [90368/225000 (40%)] Loss: 6411.849609\n",
      "Train Epoch: 139 [94464/225000 (42%)] Loss: 6489.283203\n",
      "Train Epoch: 139 [98560/225000 (44%)] Loss: 6600.503906\n",
      "Train Epoch: 139 [102656/225000 (46%)] Loss: 6405.673828\n",
      "Train Epoch: 139 [106752/225000 (47%)] Loss: 6502.228516\n",
      "Train Epoch: 139 [110848/225000 (49%)] Loss: 6575.181641\n",
      "Train Epoch: 139 [114944/225000 (51%)] Loss: 6475.658203\n",
      "Train Epoch: 139 [119040/225000 (53%)] Loss: 6512.162109\n",
      "Train Epoch: 139 [123136/225000 (55%)] Loss: 6556.988281\n",
      "Train Epoch: 139 [127232/225000 (57%)] Loss: 6504.908203\n",
      "Train Epoch: 139 [131328/225000 (58%)] Loss: 6472.630859\n",
      "Train Epoch: 139 [135424/225000 (60%)] Loss: 6506.914062\n",
      "Train Epoch: 139 [139520/225000 (62%)] Loss: 6511.330078\n",
      "Train Epoch: 139 [143616/225000 (64%)] Loss: 6497.933594\n",
      "Train Epoch: 139 [147712/225000 (66%)] Loss: 6680.607422\n",
      "Train Epoch: 139 [151808/225000 (67%)] Loss: 6503.435547\n",
      "Train Epoch: 139 [155904/225000 (69%)] Loss: 6568.953125\n",
      "Train Epoch: 139 [160000/225000 (71%)] Loss: 6376.158203\n",
      "Train Epoch: 139 [164096/225000 (73%)] Loss: 6448.814453\n",
      "Train Epoch: 139 [168192/225000 (75%)] Loss: 6665.386719\n",
      "Train Epoch: 139 [172288/225000 (77%)] Loss: 6470.945312\n",
      "Train Epoch: 139 [176384/225000 (78%)] Loss: 6443.023438\n",
      "Train Epoch: 139 [180480/225000 (80%)] Loss: 6617.271484\n",
      "Train Epoch: 139 [184576/225000 (82%)] Loss: 6388.955078\n",
      "Train Epoch: 139 [188672/225000 (84%)] Loss: 6408.738281\n",
      "Train Epoch: 139 [192768/225000 (86%)] Loss: 6564.214844\n",
      "Train Epoch: 139 [196864/225000 (87%)] Loss: 6411.384766\n",
      "Train Epoch: 139 [200960/225000 (89%)] Loss: 6522.693359\n",
      "Train Epoch: 139 [205056/225000 (91%)] Loss: 6368.560547\n",
      "Train Epoch: 139 [209152/225000 (93%)] Loss: 6500.242188\n",
      "Train Epoch: 139 [213248/225000 (95%)] Loss: 6475.605469\n",
      "Train Epoch: 139 [217344/225000 (97%)] Loss: 6409.220703\n",
      "Train Epoch: 139 [221440/225000 (98%)] Loss: 6535.763672\n",
      "    epoch          : 139\n",
      "    loss           : 6579.525080658063\n",
      "    val_loss       : 6532.425450253243\n",
      "Train Epoch: 140 [256/225000 (0%)] Loss: 6351.710938\n",
      "Train Epoch: 140 [4352/225000 (2%)] Loss: 6535.802734\n",
      "Train Epoch: 140 [8448/225000 (4%)] Loss: 6417.269531\n",
      "Train Epoch: 140 [12544/225000 (6%)] Loss: 6537.044922\n",
      "Train Epoch: 140 [16640/225000 (7%)] Loss: 6225.777344\n",
      "Train Epoch: 140 [20736/225000 (9%)] Loss: 6499.742188\n",
      "Train Epoch: 140 [24832/225000 (11%)] Loss: 6618.341797\n",
      "Train Epoch: 140 [28928/225000 (13%)] Loss: 6487.603516\n",
      "Train Epoch: 140 [33024/225000 (15%)] Loss: 6395.804688\n",
      "Train Epoch: 140 [37120/225000 (16%)] Loss: 6447.851562\n",
      "Train Epoch: 140 [41216/225000 (18%)] Loss: 6447.550781\n",
      "Train Epoch: 140 [45312/225000 (20%)] Loss: 6695.330078\n",
      "Train Epoch: 140 [49408/225000 (22%)] Loss: 6579.185547\n",
      "Train Epoch: 140 [53504/225000 (24%)] Loss: 6508.746094\n",
      "Train Epoch: 140 [57600/225000 (26%)] Loss: 6461.216797\n",
      "Train Epoch: 140 [61696/225000 (27%)] Loss: 6526.546875\n",
      "Train Epoch: 140 [65792/225000 (29%)] Loss: 6464.876953\n",
      "Train Epoch: 140 [69888/225000 (31%)] Loss: 6386.156250\n",
      "Train Epoch: 140 [73984/225000 (33%)] Loss: 6373.789062\n",
      "Train Epoch: 140 [78080/225000 (35%)] Loss: 6443.103516\n",
      "Train Epoch: 140 [82176/225000 (37%)] Loss: 6509.556641\n",
      "Train Epoch: 140 [86272/225000 (38%)] Loss: 6579.019531\n",
      "Train Epoch: 140 [90368/225000 (40%)] Loss: 17848.412109\n",
      "Train Epoch: 140 [94464/225000 (42%)] Loss: 6486.566406\n",
      "Train Epoch: 140 [98560/225000 (44%)] Loss: 6471.736328\n",
      "Train Epoch: 140 [102656/225000 (46%)] Loss: 17888.283203\n",
      "Train Epoch: 140 [106752/225000 (47%)] Loss: 6376.416016\n",
      "Train Epoch: 140 [110848/225000 (49%)] Loss: 6568.880859\n",
      "Train Epoch: 140 [114944/225000 (51%)] Loss: 6453.400391\n",
      "Train Epoch: 140 [119040/225000 (53%)] Loss: 6375.507812\n",
      "Train Epoch: 140 [123136/225000 (55%)] Loss: 6348.781250\n",
      "Train Epoch: 140 [127232/225000 (57%)] Loss: 6239.500000\n",
      "Train Epoch: 140 [131328/225000 (58%)] Loss: 6634.613281\n",
      "Train Epoch: 140 [135424/225000 (60%)] Loss: 6587.787109\n",
      "Train Epoch: 140 [139520/225000 (62%)] Loss: 6411.917969\n",
      "Train Epoch: 140 [143616/225000 (64%)] Loss: 6381.537109\n",
      "Train Epoch: 140 [147712/225000 (66%)] Loss: 6479.119141\n",
      "Train Epoch: 140 [151808/225000 (67%)] Loss: 6568.738281\n",
      "Train Epoch: 140 [155904/225000 (69%)] Loss: 6535.263672\n",
      "Train Epoch: 140 [160000/225000 (71%)] Loss: 6419.828125\n",
      "Train Epoch: 140 [164096/225000 (73%)] Loss: 6359.000000\n",
      "Train Epoch: 140 [168192/225000 (75%)] Loss: 6506.992188\n",
      "Train Epoch: 140 [172288/225000 (77%)] Loss: 6467.205078\n",
      "Train Epoch: 140 [176384/225000 (78%)] Loss: 6497.945312\n",
      "Train Epoch: 140 [180480/225000 (80%)] Loss: 6500.830078\n",
      "Train Epoch: 140 [184576/225000 (82%)] Loss: 6487.207031\n",
      "Train Epoch: 140 [188672/225000 (84%)] Loss: 6401.011719\n",
      "Train Epoch: 140 [192768/225000 (86%)] Loss: 6464.503906\n",
      "Train Epoch: 140 [196864/225000 (87%)] Loss: 8478.183594\n",
      "Train Epoch: 140 [200960/225000 (89%)] Loss: 6525.373047\n",
      "Train Epoch: 140 [205056/225000 (91%)] Loss: 6473.552734\n",
      "Train Epoch: 140 [209152/225000 (93%)] Loss: 6452.486328\n",
      "Train Epoch: 140 [213248/225000 (95%)] Loss: 6570.619141\n",
      "Train Epoch: 140 [217344/225000 (97%)] Loss: 6415.916016\n",
      "Train Epoch: 140 [221440/225000 (98%)] Loss: 6489.339844\n",
      "    epoch          : 140\n",
      "    loss           : 6597.613537889291\n",
      "    val_loss       : 6771.58306778937\n",
      "Train Epoch: 141 [256/225000 (0%)] Loss: 6434.583984\n",
      "Train Epoch: 141 [4352/225000 (2%)] Loss: 6484.439453\n",
      "Train Epoch: 141 [8448/225000 (4%)] Loss: 6536.705078\n",
      "Train Epoch: 141 [12544/225000 (6%)] Loss: 6377.337891\n",
      "Train Epoch: 141 [16640/225000 (7%)] Loss: 6575.349609\n",
      "Train Epoch: 141 [20736/225000 (9%)] Loss: 6583.484375\n",
      "Train Epoch: 141 [24832/225000 (11%)] Loss: 6382.234375\n",
      "Train Epoch: 141 [28928/225000 (13%)] Loss: 6497.738281\n",
      "Train Epoch: 141 [33024/225000 (15%)] Loss: 6411.339844\n",
      "Train Epoch: 141 [37120/225000 (16%)] Loss: 6555.074219\n",
      "Train Epoch: 141 [41216/225000 (18%)] Loss: 6600.449219\n",
      "Train Epoch: 141 [45312/225000 (20%)] Loss: 6463.613281\n",
      "Train Epoch: 141 [49408/225000 (22%)] Loss: 6499.601562\n",
      "Train Epoch: 141 [53504/225000 (24%)] Loss: 6485.980469\n",
      "Train Epoch: 141 [57600/225000 (26%)] Loss: 6525.839844\n",
      "Train Epoch: 141 [61696/225000 (27%)] Loss: 6531.417969\n",
      "Train Epoch: 141 [65792/225000 (29%)] Loss: 6626.886719\n",
      "Train Epoch: 141 [69888/225000 (31%)] Loss: 6485.138672\n",
      "Train Epoch: 141 [73984/225000 (33%)] Loss: 6363.099609\n",
      "Train Epoch: 141 [78080/225000 (35%)] Loss: 6492.384766\n",
      "Train Epoch: 141 [82176/225000 (37%)] Loss: 6465.617188\n",
      "Train Epoch: 141 [86272/225000 (38%)] Loss: 6613.785156\n",
      "Train Epoch: 141 [90368/225000 (40%)] Loss: 6446.765625\n",
      "Train Epoch: 141 [94464/225000 (42%)] Loss: 6558.380859\n",
      "Train Epoch: 141 [98560/225000 (44%)] Loss: 6443.927734\n",
      "Train Epoch: 141 [102656/225000 (46%)] Loss: 6364.773438\n",
      "Train Epoch: 141 [106752/225000 (47%)] Loss: 6366.990234\n",
      "Train Epoch: 141 [110848/225000 (49%)] Loss: 6513.777344\n",
      "Train Epoch: 141 [114944/225000 (51%)] Loss: 6505.427734\n",
      "Train Epoch: 141 [119040/225000 (53%)] Loss: 6290.630859\n",
      "Train Epoch: 141 [123136/225000 (55%)] Loss: 6482.556641\n",
      "Train Epoch: 141 [127232/225000 (57%)] Loss: 6554.794922\n",
      "Train Epoch: 141 [131328/225000 (58%)] Loss: 6538.898438\n",
      "Train Epoch: 141 [135424/225000 (60%)] Loss: 6626.039062\n",
      "Train Epoch: 141 [139520/225000 (62%)] Loss: 6489.330078\n",
      "Train Epoch: 141 [143616/225000 (64%)] Loss: 6428.699219\n",
      "Train Epoch: 141 [147712/225000 (66%)] Loss: 6465.699219\n",
      "Train Epoch: 141 [151808/225000 (67%)] Loss: 6429.064453\n",
      "Train Epoch: 141 [155904/225000 (69%)] Loss: 6571.544922\n",
      "Train Epoch: 141 [160000/225000 (71%)] Loss: 6510.126953\n",
      "Train Epoch: 141 [164096/225000 (73%)] Loss: 6478.839844\n",
      "Train Epoch: 141 [168192/225000 (75%)] Loss: 6488.216797\n",
      "Train Epoch: 141 [172288/225000 (77%)] Loss: 6485.052734\n",
      "Train Epoch: 141 [176384/225000 (78%)] Loss: 6492.560547\n",
      "Train Epoch: 141 [180480/225000 (80%)] Loss: 6547.152344\n",
      "Train Epoch: 141 [184576/225000 (82%)] Loss: 6534.287109\n",
      "Train Epoch: 141 [188672/225000 (84%)] Loss: 6498.611328\n",
      "Train Epoch: 141 [192768/225000 (86%)] Loss: 6549.595703\n",
      "Train Epoch: 141 [196864/225000 (87%)] Loss: 6473.132812\n",
      "Train Epoch: 141 [200960/225000 (89%)] Loss: 6408.076172\n",
      "Train Epoch: 141 [205056/225000 (91%)] Loss: 6607.906250\n",
      "Train Epoch: 141 [209152/225000 (93%)] Loss: 6523.302734\n",
      "Train Epoch: 141 [213248/225000 (95%)] Loss: 6457.648438\n",
      "Train Epoch: 141 [217344/225000 (97%)] Loss: 6619.041016\n",
      "Train Epoch: 141 [221440/225000 (98%)] Loss: 8317.632812\n",
      "    epoch          : 141\n",
      "    loss           : 6564.44819108184\n",
      "    val_loss       : 6562.932897595727\n",
      "Train Epoch: 142 [256/225000 (0%)] Loss: 6552.285156\n",
      "Train Epoch: 142 [4352/225000 (2%)] Loss: 6465.210938\n",
      "Train Epoch: 142 [8448/225000 (4%)] Loss: 6315.886719\n",
      "Train Epoch: 142 [12544/225000 (6%)] Loss: 6349.128906\n",
      "Train Epoch: 142 [16640/225000 (7%)] Loss: 6425.564453\n",
      "Train Epoch: 142 [20736/225000 (9%)] Loss: 6436.843750\n",
      "Train Epoch: 142 [24832/225000 (11%)] Loss: 12850.396484\n",
      "Train Epoch: 142 [28928/225000 (13%)] Loss: 6427.742188\n",
      "Train Epoch: 142 [33024/225000 (15%)] Loss: 6384.925781\n",
      "Train Epoch: 142 [37120/225000 (16%)] Loss: 8329.560547\n",
      "Train Epoch: 142 [41216/225000 (18%)] Loss: 6464.355469\n",
      "Train Epoch: 142 [45312/225000 (20%)] Loss: 6537.085938\n",
      "Train Epoch: 142 [49408/225000 (22%)] Loss: 6392.427734\n",
      "Train Epoch: 142 [53504/225000 (24%)] Loss: 6537.708984\n",
      "Train Epoch: 142 [57600/225000 (26%)] Loss: 6522.560547\n",
      "Train Epoch: 142 [61696/225000 (27%)] Loss: 6374.353516\n",
      "Train Epoch: 142 [65792/225000 (29%)] Loss: 6546.388672\n",
      "Train Epoch: 142 [69888/225000 (31%)] Loss: 17641.486328\n",
      "Train Epoch: 142 [73984/225000 (33%)] Loss: 6560.552734\n",
      "Train Epoch: 142 [78080/225000 (35%)] Loss: 6233.478516\n",
      "Train Epoch: 142 [82176/225000 (37%)] Loss: 6626.433594\n",
      "Train Epoch: 142 [86272/225000 (38%)] Loss: 6547.101562\n",
      "Train Epoch: 142 [90368/225000 (40%)] Loss: 8370.964844\n",
      "Train Epoch: 142 [94464/225000 (42%)] Loss: 6398.255859\n",
      "Train Epoch: 142 [98560/225000 (44%)] Loss: 6436.861328\n",
      "Train Epoch: 142 [102656/225000 (46%)] Loss: 6549.908203\n",
      "Train Epoch: 142 [106752/225000 (47%)] Loss: 6445.609375\n",
      "Train Epoch: 142 [110848/225000 (49%)] Loss: 6521.816406\n",
      "Train Epoch: 142 [114944/225000 (51%)] Loss: 6434.037109\n",
      "Train Epoch: 142 [119040/225000 (53%)] Loss: 6395.490234\n",
      "Train Epoch: 142 [123136/225000 (55%)] Loss: 6570.992188\n",
      "Train Epoch: 142 [127232/225000 (57%)] Loss: 6514.908203\n",
      "Train Epoch: 142 [131328/225000 (58%)] Loss: 6589.623047\n",
      "Train Epoch: 142 [135424/225000 (60%)] Loss: 6550.228516\n",
      "Train Epoch: 142 [139520/225000 (62%)] Loss: 6359.062500\n",
      "Train Epoch: 142 [143616/225000 (64%)] Loss: 6600.205078\n",
      "Train Epoch: 142 [147712/225000 (66%)] Loss: 6501.515625\n",
      "Train Epoch: 142 [151808/225000 (67%)] Loss: 6522.394531\n",
      "Train Epoch: 142 [155904/225000 (69%)] Loss: 6521.619141\n",
      "Train Epoch: 142 [160000/225000 (71%)] Loss: 6550.582031\n",
      "Train Epoch: 142 [164096/225000 (73%)] Loss: 6540.490234\n",
      "Train Epoch: 142 [168192/225000 (75%)] Loss: 6425.525391\n",
      "Train Epoch: 142 [172288/225000 (77%)] Loss: 6440.800781\n",
      "Train Epoch: 142 [176384/225000 (78%)] Loss: 6453.585938\n",
      "Train Epoch: 142 [180480/225000 (80%)] Loss: 6545.744141\n",
      "Train Epoch: 142 [184576/225000 (82%)] Loss: 6536.330078\n",
      "Train Epoch: 142 [188672/225000 (84%)] Loss: 6635.115234\n",
      "Train Epoch: 142 [192768/225000 (86%)] Loss: 6432.236328\n",
      "Train Epoch: 142 [196864/225000 (87%)] Loss: 6405.445312\n",
      "Train Epoch: 142 [200960/225000 (89%)] Loss: 6631.599609\n",
      "Train Epoch: 142 [205056/225000 (91%)] Loss: 6515.230469\n",
      "Train Epoch: 142 [209152/225000 (93%)] Loss: 6510.912109\n",
      "Train Epoch: 142 [213248/225000 (95%)] Loss: 6457.994141\n",
      "Train Epoch: 142 [217344/225000 (97%)] Loss: 6489.421875\n",
      "Train Epoch: 142 [221440/225000 (98%)] Loss: 6514.765625\n",
      "    epoch          : 142\n",
      "    loss           : 6585.524466279153\n",
      "    val_loss       : 6676.528253607604\n",
      "Train Epoch: 143 [256/225000 (0%)] Loss: 6374.369141\n",
      "Train Epoch: 143 [4352/225000 (2%)] Loss: 6429.826172\n",
      "Train Epoch: 143 [8448/225000 (4%)] Loss: 6414.556641\n",
      "Train Epoch: 143 [12544/225000 (6%)] Loss: 6630.087891\n",
      "Train Epoch: 143 [16640/225000 (7%)] Loss: 6550.642578\n",
      "Train Epoch: 143 [20736/225000 (9%)] Loss: 6475.146484\n",
      "Train Epoch: 143 [24832/225000 (11%)] Loss: 6461.251953\n",
      "Train Epoch: 143 [28928/225000 (13%)] Loss: 6523.087891\n",
      "Train Epoch: 143 [33024/225000 (15%)] Loss: 12770.130859\n",
      "Train Epoch: 143 [37120/225000 (16%)] Loss: 6560.748047\n",
      "Train Epoch: 143 [41216/225000 (18%)] Loss: 6484.623047\n",
      "Train Epoch: 143 [45312/225000 (20%)] Loss: 6507.339844\n",
      "Train Epoch: 143 [49408/225000 (22%)] Loss: 6380.621094\n",
      "Train Epoch: 143 [53504/225000 (24%)] Loss: 6487.701172\n",
      "Train Epoch: 143 [57600/225000 (26%)] Loss: 6451.167969\n",
      "Train Epoch: 143 [61696/225000 (27%)] Loss: 6416.988281\n",
      "Train Epoch: 143 [65792/225000 (29%)] Loss: 6561.248047\n",
      "Train Epoch: 143 [69888/225000 (31%)] Loss: 6607.404297\n",
      "Train Epoch: 143 [73984/225000 (33%)] Loss: 6548.740234\n",
      "Train Epoch: 143 [78080/225000 (35%)] Loss: 17905.214844\n",
      "Train Epoch: 143 [82176/225000 (37%)] Loss: 6619.875000\n",
      "Train Epoch: 143 [86272/225000 (38%)] Loss: 6485.751953\n",
      "Train Epoch: 143 [90368/225000 (40%)] Loss: 6704.960938\n",
      "Train Epoch: 143 [94464/225000 (42%)] Loss: 6451.693359\n",
      "Train Epoch: 143 [98560/225000 (44%)] Loss: 6723.150391\n",
      "Train Epoch: 143 [102656/225000 (46%)] Loss: 6397.390625\n",
      "Train Epoch: 143 [106752/225000 (47%)] Loss: 6426.644531\n",
      "Train Epoch: 143 [110848/225000 (49%)] Loss: 6474.042969\n",
      "Train Epoch: 143 [114944/225000 (51%)] Loss: 6407.558594\n",
      "Train Epoch: 143 [119040/225000 (53%)] Loss: 6543.679688\n",
      "Train Epoch: 143 [123136/225000 (55%)] Loss: 6502.812500\n",
      "Train Epoch: 143 [127232/225000 (57%)] Loss: 6493.839844\n",
      "Train Epoch: 143 [131328/225000 (58%)] Loss: 6397.308594\n",
      "Train Epoch: 143 [135424/225000 (60%)] Loss: 6541.271484\n",
      "Train Epoch: 143 [139520/225000 (62%)] Loss: 6439.984375\n",
      "Train Epoch: 143 [143616/225000 (64%)] Loss: 6534.589844\n",
      "Train Epoch: 143 [147712/225000 (66%)] Loss: 6579.824219\n",
      "Train Epoch: 143 [151808/225000 (67%)] Loss: 6469.617188\n",
      "Train Epoch: 143 [155904/225000 (69%)] Loss: 6484.515625\n",
      "Train Epoch: 143 [160000/225000 (71%)] Loss: 6454.068359\n",
      "Train Epoch: 143 [164096/225000 (73%)] Loss: 6506.648438\n",
      "Train Epoch: 143 [168192/225000 (75%)] Loss: 6434.644531\n",
      "Train Epoch: 143 [172288/225000 (77%)] Loss: 6533.914062\n",
      "Train Epoch: 143 [176384/225000 (78%)] Loss: 6427.646484\n",
      "Train Epoch: 143 [180480/225000 (80%)] Loss: 6557.115234\n",
      "Train Epoch: 143 [184576/225000 (82%)] Loss: 6472.667969\n",
      "Train Epoch: 143 [188672/225000 (84%)] Loss: 6486.849609\n",
      "Train Epoch: 143 [192768/225000 (86%)] Loss: 6248.449219\n",
      "Train Epoch: 143 [196864/225000 (87%)] Loss: 6413.529297\n",
      "Train Epoch: 143 [200960/225000 (89%)] Loss: 6469.859375\n",
      "Train Epoch: 143 [205056/225000 (91%)] Loss: 6512.099609\n",
      "Train Epoch: 143 [209152/225000 (93%)] Loss: 6519.812500\n",
      "Train Epoch: 143 [213248/225000 (95%)] Loss: 6491.412109\n",
      "Train Epoch: 143 [217344/225000 (97%)] Loss: 6349.697266\n",
      "Train Epoch: 143 [221440/225000 (98%)] Loss: 6356.226562\n",
      "    epoch          : 143\n",
      "    loss           : 6538.4839639327365\n",
      "    val_loss       : 6753.919782997394\n",
      "Train Epoch: 144 [256/225000 (0%)] Loss: 6468.960938\n",
      "Train Epoch: 144 [4352/225000 (2%)] Loss: 6501.359375\n",
      "Train Epoch: 144 [8448/225000 (4%)] Loss: 6477.681641\n",
      "Train Epoch: 144 [12544/225000 (6%)] Loss: 6573.580078\n",
      "Train Epoch: 144 [16640/225000 (7%)] Loss: 6460.902344\n",
      "Train Epoch: 144 [20736/225000 (9%)] Loss: 6350.845703\n",
      "Train Epoch: 144 [24832/225000 (11%)] Loss: 6388.019531\n",
      "Train Epoch: 144 [28928/225000 (13%)] Loss: 6561.208984\n",
      "Train Epoch: 144 [33024/225000 (15%)] Loss: 6408.767578\n",
      "Train Epoch: 144 [37120/225000 (16%)] Loss: 6466.072266\n",
      "Train Epoch: 144 [41216/225000 (18%)] Loss: 6454.820312\n",
      "Train Epoch: 144 [45312/225000 (20%)] Loss: 6446.576172\n",
      "Train Epoch: 144 [49408/225000 (22%)] Loss: 6393.197266\n",
      "Train Epoch: 144 [53504/225000 (24%)] Loss: 6451.806641\n",
      "Train Epoch: 144 [57600/225000 (26%)] Loss: 6410.105469\n",
      "Train Epoch: 144 [61696/225000 (27%)] Loss: 6563.021484\n",
      "Train Epoch: 144 [65792/225000 (29%)] Loss: 6397.873047\n",
      "Train Epoch: 144 [69888/225000 (31%)] Loss: 6615.212891\n",
      "Train Epoch: 144 [73984/225000 (33%)] Loss: 6397.984375\n",
      "Train Epoch: 144 [78080/225000 (35%)] Loss: 6500.279297\n",
      "Train Epoch: 144 [82176/225000 (37%)] Loss: 6340.560547\n",
      "Train Epoch: 144 [86272/225000 (38%)] Loss: 6461.708984\n",
      "Train Epoch: 144 [90368/225000 (40%)] Loss: 6478.294922\n",
      "Train Epoch: 144 [94464/225000 (42%)] Loss: 6362.363281\n",
      "Train Epoch: 144 [98560/225000 (44%)] Loss: 6440.083984\n",
      "Train Epoch: 144 [102656/225000 (46%)] Loss: 6578.111328\n",
      "Train Epoch: 144 [106752/225000 (47%)] Loss: 6460.445312\n",
      "Train Epoch: 144 [110848/225000 (49%)] Loss: 6523.476562\n",
      "Train Epoch: 144 [114944/225000 (51%)] Loss: 6404.962891\n",
      "Train Epoch: 144 [119040/225000 (53%)] Loss: 6576.789062\n",
      "Train Epoch: 144 [123136/225000 (55%)] Loss: 6443.263672\n",
      "Train Epoch: 144 [127232/225000 (57%)] Loss: 6456.130859\n",
      "Train Epoch: 144 [131328/225000 (58%)] Loss: 6463.949219\n",
      "Train Epoch: 144 [135424/225000 (60%)] Loss: 6506.939453\n",
      "Train Epoch: 144 [139520/225000 (62%)] Loss: 6376.445312\n",
      "Train Epoch: 144 [143616/225000 (64%)] Loss: 6558.195312\n",
      "Train Epoch: 144 [147712/225000 (66%)] Loss: 6441.203125\n",
      "Train Epoch: 144 [151808/225000 (67%)] Loss: 6378.099609\n",
      "Train Epoch: 144 [155904/225000 (69%)] Loss: 6382.044922\n",
      "Train Epoch: 144 [160000/225000 (71%)] Loss: 6444.285156\n",
      "Train Epoch: 144 [164096/225000 (73%)] Loss: 6517.328125\n",
      "Train Epoch: 144 [168192/225000 (75%)] Loss: 6514.587891\n",
      "Train Epoch: 144 [172288/225000 (77%)] Loss: 6507.107422\n",
      "Train Epoch: 144 [176384/225000 (78%)] Loss: 6527.984375\n",
      "Train Epoch: 144 [180480/225000 (80%)] Loss: 6560.105469\n",
      "Train Epoch: 144 [184576/225000 (82%)] Loss: 6393.335938\n",
      "Train Epoch: 144 [188672/225000 (84%)] Loss: 6431.783203\n",
      "Train Epoch: 144 [192768/225000 (86%)] Loss: 6353.845703\n",
      "Train Epoch: 144 [196864/225000 (87%)] Loss: 6652.650391\n",
      "Train Epoch: 144 [200960/225000 (89%)] Loss: 6407.750000\n",
      "Train Epoch: 144 [205056/225000 (91%)] Loss: 6456.316406\n",
      "Train Epoch: 144 [209152/225000 (93%)] Loss: 6348.480469\n",
      "Train Epoch: 144 [213248/225000 (95%)] Loss: 6462.550781\n",
      "Train Epoch: 144 [217344/225000 (97%)] Loss: 6235.654297\n",
      "Train Epoch: 144 [221440/225000 (98%)] Loss: 6403.230469\n",
      "    epoch          : 144\n",
      "    loss           : 6484.821706795719\n",
      "    val_loss       : 6586.8638271433965\n",
      "Train Epoch: 145 [256/225000 (0%)] Loss: 6387.132812\n",
      "Train Epoch: 145 [4352/225000 (2%)] Loss: 6363.757812\n",
      "Train Epoch: 145 [8448/225000 (4%)] Loss: 6583.800781\n",
      "Train Epoch: 145 [12544/225000 (6%)] Loss: 6500.484375\n",
      "Train Epoch: 145 [16640/225000 (7%)] Loss: 6412.718750\n",
      "Train Epoch: 145 [20736/225000 (9%)] Loss: 6477.658203\n",
      "Train Epoch: 145 [24832/225000 (11%)] Loss: 6429.275391\n",
      "Train Epoch: 145 [28928/225000 (13%)] Loss: 6399.468750\n",
      "Train Epoch: 145 [33024/225000 (15%)] Loss: 6453.976562\n",
      "Train Epoch: 145 [37120/225000 (16%)] Loss: 6507.556641\n",
      "Train Epoch: 145 [41216/225000 (18%)] Loss: 6370.583984\n",
      "Train Epoch: 145 [45312/225000 (20%)] Loss: 6414.173828\n",
      "Train Epoch: 145 [49408/225000 (22%)] Loss: 6460.927734\n",
      "Train Epoch: 145 [53504/225000 (24%)] Loss: 6576.619141\n",
      "Train Epoch: 145 [57600/225000 (26%)] Loss: 6488.847656\n",
      "Train Epoch: 145 [61696/225000 (27%)] Loss: 6419.185547\n",
      "Train Epoch: 145 [65792/225000 (29%)] Loss: 6377.810547\n",
      "Train Epoch: 145 [69888/225000 (31%)] Loss: 6603.691406\n",
      "Train Epoch: 145 [73984/225000 (33%)] Loss: 6427.314453\n",
      "Train Epoch: 145 [78080/225000 (35%)] Loss: 6539.615234\n",
      "Train Epoch: 145 [82176/225000 (37%)] Loss: 6552.753906\n",
      "Train Epoch: 145 [86272/225000 (38%)] Loss: 6504.753906\n",
      "Train Epoch: 145 [90368/225000 (40%)] Loss: 6478.115234\n",
      "Train Epoch: 145 [94464/225000 (42%)] Loss: 6316.492188\n",
      "Train Epoch: 145 [98560/225000 (44%)] Loss: 6456.570312\n",
      "Train Epoch: 145 [102656/225000 (46%)] Loss: 6426.345703\n",
      "Train Epoch: 145 [106752/225000 (47%)] Loss: 6434.201172\n",
      "Train Epoch: 145 [110848/225000 (49%)] Loss: 6567.945312\n",
      "Train Epoch: 145 [114944/225000 (51%)] Loss: 6494.716797\n",
      "Train Epoch: 145 [119040/225000 (53%)] Loss: 6408.376953\n",
      "Train Epoch: 145 [123136/225000 (55%)] Loss: 6367.230469\n",
      "Train Epoch: 145 [127232/225000 (57%)] Loss: 6420.291016\n",
      "Train Epoch: 145 [131328/225000 (58%)] Loss: 6464.101562\n",
      "Train Epoch: 145 [135424/225000 (60%)] Loss: 6500.203125\n",
      "Train Epoch: 145 [139520/225000 (62%)] Loss: 6517.791016\n",
      "Train Epoch: 145 [143616/225000 (64%)] Loss: 6546.496094\n",
      "Train Epoch: 145 [147712/225000 (66%)] Loss: 6559.302734\n",
      "Train Epoch: 145 [151808/225000 (67%)] Loss: 6363.033203\n",
      "Train Epoch: 145 [155904/225000 (69%)] Loss: 6521.613281\n",
      "Train Epoch: 145 [160000/225000 (71%)] Loss: 6533.285156\n",
      "Train Epoch: 145 [164096/225000 (73%)] Loss: 6456.410156\n",
      "Train Epoch: 145 [168192/225000 (75%)] Loss: 6407.816406\n",
      "Train Epoch: 145 [172288/225000 (77%)] Loss: 6387.599609\n",
      "Train Epoch: 145 [176384/225000 (78%)] Loss: 6564.763672\n",
      "Train Epoch: 145 [180480/225000 (80%)] Loss: 6325.976562\n",
      "Train Epoch: 145 [184576/225000 (82%)] Loss: 6563.019531\n",
      "Train Epoch: 145 [188672/225000 (84%)] Loss: 6308.488281\n",
      "Train Epoch: 145 [192768/225000 (86%)] Loss: 6547.757812\n",
      "Train Epoch: 145 [196864/225000 (87%)] Loss: 6543.386719\n",
      "Train Epoch: 145 [200960/225000 (89%)] Loss: 6478.582031\n",
      "Train Epoch: 145 [205056/225000 (91%)] Loss: 6491.978516\n",
      "Train Epoch: 145 [209152/225000 (93%)] Loss: 6545.458984\n",
      "Train Epoch: 145 [213248/225000 (95%)] Loss: 6444.261719\n",
      "Train Epoch: 145 [217344/225000 (97%)] Loss: 6367.822266\n",
      "Train Epoch: 145 [221440/225000 (98%)] Loss: 6363.296875\n",
      "    epoch          : 145\n",
      "    loss           : 6572.226022557594\n",
      "    val_loss       : 6531.374658996961\n",
      "Train Epoch: 146 [256/225000 (0%)] Loss: 6401.287109\n",
      "Train Epoch: 146 [4352/225000 (2%)] Loss: 6461.849609\n",
      "Train Epoch: 146 [8448/225000 (4%)] Loss: 6518.361328\n",
      "Train Epoch: 146 [12544/225000 (6%)] Loss: 6438.773438\n",
      "Train Epoch: 146 [16640/225000 (7%)] Loss: 6538.128906\n",
      "Train Epoch: 146 [20736/225000 (9%)] Loss: 6602.462891\n",
      "Train Epoch: 146 [24832/225000 (11%)] Loss: 6413.550781\n",
      "Train Epoch: 146 [28928/225000 (13%)] Loss: 6350.474609\n",
      "Train Epoch: 146 [33024/225000 (15%)] Loss: 6422.185547\n",
      "Train Epoch: 146 [37120/225000 (16%)] Loss: 6405.492188\n",
      "Train Epoch: 146 [41216/225000 (18%)] Loss: 6402.074219\n",
      "Train Epoch: 146 [45312/225000 (20%)] Loss: 6400.824219\n",
      "Train Epoch: 146 [49408/225000 (22%)] Loss: 6593.285156\n",
      "Train Epoch: 146 [53504/225000 (24%)] Loss: 6540.554688\n",
      "Train Epoch: 146 [57600/225000 (26%)] Loss: 6473.912109\n",
      "Train Epoch: 146 [61696/225000 (27%)] Loss: 6316.785156\n",
      "Train Epoch: 146 [65792/225000 (29%)] Loss: 6492.802734\n",
      "Train Epoch: 146 [69888/225000 (31%)] Loss: 6495.355469\n",
      "Train Epoch: 146 [73984/225000 (33%)] Loss: 6381.984375\n",
      "Train Epoch: 146 [78080/225000 (35%)] Loss: 6452.794922\n",
      "Train Epoch: 146 [82176/225000 (37%)] Loss: 6406.724609\n",
      "Train Epoch: 146 [86272/225000 (38%)] Loss: 6444.769531\n",
      "Train Epoch: 146 [90368/225000 (40%)] Loss: 6491.042969\n",
      "Train Epoch: 146 [94464/225000 (42%)] Loss: 6551.857422\n",
      "Train Epoch: 146 [98560/225000 (44%)] Loss: 6593.568359\n",
      "Train Epoch: 146 [102656/225000 (46%)] Loss: 8092.767578\n",
      "Train Epoch: 146 [106752/225000 (47%)] Loss: 6471.027344\n",
      "Train Epoch: 146 [110848/225000 (49%)] Loss: 6386.603516\n",
      "Train Epoch: 146 [114944/225000 (51%)] Loss: 6455.554688\n",
      "Train Epoch: 146 [119040/225000 (53%)] Loss: 6368.156250\n",
      "Train Epoch: 146 [123136/225000 (55%)] Loss: 6456.439453\n",
      "Train Epoch: 146 [127232/225000 (57%)] Loss: 6426.375000\n",
      "Train Epoch: 146 [131328/225000 (58%)] Loss: 6481.427734\n",
      "Train Epoch: 146 [135424/225000 (60%)] Loss: 6457.638672\n",
      "Train Epoch: 146 [139520/225000 (62%)] Loss: 6395.156250\n",
      "Train Epoch: 146 [143616/225000 (64%)] Loss: 6594.761719\n",
      "Train Epoch: 146 [147712/225000 (66%)] Loss: 6450.144531\n",
      "Train Epoch: 146 [151808/225000 (67%)] Loss: 6417.984375\n",
      "Train Epoch: 146 [155904/225000 (69%)] Loss: 6545.003906\n",
      "Train Epoch: 146 [160000/225000 (71%)] Loss: 6539.505859\n",
      "Train Epoch: 146 [164096/225000 (73%)] Loss: 6539.798828\n",
      "Train Epoch: 146 [168192/225000 (75%)] Loss: 6371.923828\n",
      "Train Epoch: 146 [172288/225000 (77%)] Loss: 6635.449219\n",
      "Train Epoch: 146 [176384/225000 (78%)] Loss: 6447.357422\n",
      "Train Epoch: 146 [180480/225000 (80%)] Loss: 6448.916016\n",
      "Train Epoch: 146 [184576/225000 (82%)] Loss: 6408.861328\n",
      "Train Epoch: 146 [188672/225000 (84%)] Loss: 6318.541016\n",
      "Train Epoch: 146 [192768/225000 (86%)] Loss: 6392.921875\n",
      "Train Epoch: 146 [196864/225000 (87%)] Loss: 6426.316406\n",
      "Train Epoch: 146 [200960/225000 (89%)] Loss: 6521.955078\n",
      "Train Epoch: 146 [205056/225000 (91%)] Loss: 6471.267578\n",
      "Train Epoch: 146 [209152/225000 (93%)] Loss: 6447.697266\n",
      "Train Epoch: 146 [213248/225000 (95%)] Loss: 6339.859375\n",
      "Train Epoch: 146 [217344/225000 (97%)] Loss: 6471.255859\n",
      "Train Epoch: 146 [221440/225000 (98%)] Loss: 6484.414062\n",
      "    epoch          : 146\n",
      "    loss           : 6530.420646242179\n",
      "    val_loss       : 6798.4615025520325\n",
      "Train Epoch: 147 [256/225000 (0%)] Loss: 6468.228516\n",
      "Train Epoch: 147 [4352/225000 (2%)] Loss: 6388.576172\n",
      "Train Epoch: 147 [8448/225000 (4%)] Loss: 6525.238281\n",
      "Train Epoch: 147 [12544/225000 (6%)] Loss: 6357.234375\n",
      "Train Epoch: 147 [16640/225000 (7%)] Loss: 6486.285156\n",
      "Train Epoch: 147 [20736/225000 (9%)] Loss: 6483.507812\n",
      "Train Epoch: 147 [24832/225000 (11%)] Loss: 6527.931641\n",
      "Train Epoch: 147 [28928/225000 (13%)] Loss: 6375.599609\n",
      "Train Epoch: 147 [33024/225000 (15%)] Loss: 6459.429688\n",
      "Train Epoch: 147 [37120/225000 (16%)] Loss: 6402.796875\n",
      "Train Epoch: 147 [41216/225000 (18%)] Loss: 6520.183594\n",
      "Train Epoch: 147 [45312/225000 (20%)] Loss: 6552.402344\n",
      "Train Epoch: 147 [49408/225000 (22%)] Loss: 6461.646484\n",
      "Train Epoch: 147 [53504/225000 (24%)] Loss: 6449.708984\n",
      "Train Epoch: 147 [57600/225000 (26%)] Loss: 6441.496094\n",
      "Train Epoch: 147 [61696/225000 (27%)] Loss: 6251.244141\n",
      "Train Epoch: 147 [65792/225000 (29%)] Loss: 6535.156250\n",
      "Train Epoch: 147 [69888/225000 (31%)] Loss: 6315.892578\n",
      "Train Epoch: 147 [73984/225000 (33%)] Loss: 8187.087891\n",
      "Train Epoch: 147 [78080/225000 (35%)] Loss: 6411.326172\n",
      "Train Epoch: 147 [82176/225000 (37%)] Loss: 6461.730469\n",
      "Train Epoch: 147 [86272/225000 (38%)] Loss: 6389.673828\n",
      "Train Epoch: 147 [90368/225000 (40%)] Loss: 6356.585938\n",
      "Train Epoch: 147 [94464/225000 (42%)] Loss: 6469.746094\n",
      "Train Epoch: 147 [98560/225000 (44%)] Loss: 6393.648438\n",
      "Train Epoch: 147 [102656/225000 (46%)] Loss: 6461.634766\n",
      "Train Epoch: 147 [106752/225000 (47%)] Loss: 6508.802734\n",
      "Train Epoch: 147 [110848/225000 (49%)] Loss: 6279.482422\n",
      "Train Epoch: 147 [114944/225000 (51%)] Loss: 6368.640625\n",
      "Train Epoch: 147 [119040/225000 (53%)] Loss: 6339.869141\n",
      "Train Epoch: 147 [123136/225000 (55%)] Loss: 6533.279297\n",
      "Train Epoch: 147 [127232/225000 (57%)] Loss: 6464.382812\n",
      "Train Epoch: 147 [131328/225000 (58%)] Loss: 6471.365234\n",
      "Train Epoch: 147 [135424/225000 (60%)] Loss: 6444.439453\n",
      "Train Epoch: 147 [139520/225000 (62%)] Loss: 6454.289062\n",
      "Train Epoch: 147 [143616/225000 (64%)] Loss: 6474.289062\n",
      "Train Epoch: 147 [147712/225000 (66%)] Loss: 6483.134766\n",
      "Train Epoch: 147 [151808/225000 (67%)] Loss: 6535.031250\n",
      "Train Epoch: 147 [155904/225000 (69%)] Loss: 6546.285156\n",
      "Train Epoch: 147 [160000/225000 (71%)] Loss: 6246.650391\n",
      "Train Epoch: 147 [164096/225000 (73%)] Loss: 6543.113281\n",
      "Train Epoch: 147 [168192/225000 (75%)] Loss: 6398.433594\n",
      "Train Epoch: 147 [172288/225000 (77%)] Loss: 6440.673828\n",
      "Train Epoch: 147 [176384/225000 (78%)] Loss: 6398.824219\n",
      "Train Epoch: 147 [180480/225000 (80%)] Loss: 6549.431641\n",
      "Train Epoch: 147 [184576/225000 (82%)] Loss: 6454.013672\n",
      "Train Epoch: 147 [188672/225000 (84%)] Loss: 6535.337891\n",
      "Train Epoch: 147 [192768/225000 (86%)] Loss: 6491.539062\n",
      "Train Epoch: 147 [196864/225000 (87%)] Loss: 6511.628906\n",
      "Train Epoch: 147 [200960/225000 (89%)] Loss: 6511.316406\n",
      "Train Epoch: 147 [205056/225000 (91%)] Loss: 6317.939453\n",
      "Train Epoch: 147 [209152/225000 (93%)] Loss: 6323.949219\n",
      "Train Epoch: 147 [213248/225000 (95%)] Loss: 6635.835938\n",
      "Train Epoch: 147 [217344/225000 (97%)] Loss: 6444.908203\n",
      "Train Epoch: 147 [221440/225000 (98%)] Loss: 6472.957031\n",
      "    epoch          : 147\n",
      "    loss           : 6477.810570205845\n",
      "    val_loss       : 6679.993245004392\n",
      "Train Epoch: 148 [256/225000 (0%)] Loss: 6521.687500\n",
      "Train Epoch: 148 [4352/225000 (2%)] Loss: 6554.605469\n",
      "Train Epoch: 148 [8448/225000 (4%)] Loss: 6285.824219\n",
      "Train Epoch: 148 [12544/225000 (6%)] Loss: 6350.839844\n",
      "Train Epoch: 148 [16640/225000 (7%)] Loss: 6475.265625\n",
      "Train Epoch: 148 [20736/225000 (9%)] Loss: 6522.011719\n",
      "Train Epoch: 148 [24832/225000 (11%)] Loss: 6487.197266\n",
      "Train Epoch: 148 [28928/225000 (13%)] Loss: 6480.906250\n",
      "Train Epoch: 148 [33024/225000 (15%)] Loss: 6387.781250\n",
      "Train Epoch: 148 [37120/225000 (16%)] Loss: 6589.703125\n",
      "Train Epoch: 148 [41216/225000 (18%)] Loss: 6393.521484\n",
      "Train Epoch: 148 [45312/225000 (20%)] Loss: 6426.197266\n",
      "Train Epoch: 148 [49408/225000 (22%)] Loss: 6421.695312\n",
      "Train Epoch: 148 [53504/225000 (24%)] Loss: 6532.875000\n",
      "Train Epoch: 148 [57600/225000 (26%)] Loss: 6486.455078\n",
      "Train Epoch: 148 [61696/225000 (27%)] Loss: 6438.529297\n",
      "Train Epoch: 148 [65792/225000 (29%)] Loss: 6434.882812\n",
      "Train Epoch: 148 [69888/225000 (31%)] Loss: 6562.302734\n",
      "Train Epoch: 148 [73984/225000 (33%)] Loss: 6469.169922\n",
      "Train Epoch: 148 [78080/225000 (35%)] Loss: 6390.935547\n",
      "Train Epoch: 148 [82176/225000 (37%)] Loss: 6560.042969\n",
      "Train Epoch: 148 [86272/225000 (38%)] Loss: 6621.232422\n",
      "Train Epoch: 148 [90368/225000 (40%)] Loss: 6404.376953\n",
      "Train Epoch: 148 [94464/225000 (42%)] Loss: 6461.789062\n",
      "Train Epoch: 148 [98560/225000 (44%)] Loss: 6396.134766\n",
      "Train Epoch: 148 [102656/225000 (46%)] Loss: 6357.828125\n",
      "Train Epoch: 148 [106752/225000 (47%)] Loss: 6444.330078\n",
      "Train Epoch: 148 [110848/225000 (49%)] Loss: 6321.958984\n",
      "Train Epoch: 148 [114944/225000 (51%)] Loss: 6586.222656\n",
      "Train Epoch: 148 [119040/225000 (53%)] Loss: 6391.300781\n",
      "Train Epoch: 148 [123136/225000 (55%)] Loss: 6497.285156\n",
      "Train Epoch: 148 [127232/225000 (57%)] Loss: 6451.582031\n",
      "Train Epoch: 148 [131328/225000 (58%)] Loss: 6444.763672\n",
      "Train Epoch: 148 [135424/225000 (60%)] Loss: 6521.406250\n",
      "Train Epoch: 148 [139520/225000 (62%)] Loss: 6396.667969\n",
      "Train Epoch: 148 [143616/225000 (64%)] Loss: 6459.423828\n",
      "Train Epoch: 148 [147712/225000 (66%)] Loss: 6639.076172\n",
      "Train Epoch: 148 [151808/225000 (67%)] Loss: 6523.207031\n",
      "Train Epoch: 148 [155904/225000 (69%)] Loss: 6509.429688\n",
      "Train Epoch: 148 [160000/225000 (71%)] Loss: 6445.736328\n",
      "Train Epoch: 148 [164096/225000 (73%)] Loss: 6484.615234\n",
      "Train Epoch: 148 [168192/225000 (75%)] Loss: 6459.882812\n",
      "Train Epoch: 148 [172288/225000 (77%)] Loss: 6390.867188\n",
      "Train Epoch: 148 [176384/225000 (78%)] Loss: 6477.765625\n",
      "Train Epoch: 148 [180480/225000 (80%)] Loss: 6424.927734\n",
      "Train Epoch: 148 [184576/225000 (82%)] Loss: 6501.181641\n",
      "Train Epoch: 148 [188672/225000 (84%)] Loss: 6571.535156\n",
      "Train Epoch: 148 [192768/225000 (86%)] Loss: 6398.845703\n",
      "Train Epoch: 148 [196864/225000 (87%)] Loss: 6379.113281\n",
      "Train Epoch: 148 [200960/225000 (89%)] Loss: 6578.789062\n",
      "Train Epoch: 148 [205056/225000 (91%)] Loss: 6290.791016\n",
      "Train Epoch: 148 [209152/225000 (93%)] Loss: 6337.486328\n",
      "Train Epoch: 148 [213248/225000 (95%)] Loss: 6415.466797\n",
      "Train Epoch: 148 [217344/225000 (97%)] Loss: 6359.130859\n",
      "Train Epoch: 148 [221440/225000 (98%)] Loss: 6581.375000\n",
      "    epoch          : 148\n",
      "    loss           : 6589.2890747209185\n",
      "    val_loss       : 6491.8144144415855\n",
      "Train Epoch: 149 [256/225000 (0%)] Loss: 6302.023438\n",
      "Train Epoch: 149 [4352/225000 (2%)] Loss: 6423.953125\n",
      "Train Epoch: 149 [8448/225000 (4%)] Loss: 6415.658203\n",
      "Train Epoch: 149 [12544/225000 (6%)] Loss: 6520.482422\n",
      "Train Epoch: 149 [16640/225000 (7%)] Loss: 6541.841797\n",
      "Train Epoch: 149 [20736/225000 (9%)] Loss: 6466.166016\n",
      "Train Epoch: 149 [24832/225000 (11%)] Loss: 6574.693359\n",
      "Train Epoch: 149 [28928/225000 (13%)] Loss: 6283.599609\n",
      "Train Epoch: 149 [33024/225000 (15%)] Loss: 6491.953125\n",
      "Train Epoch: 149 [37120/225000 (16%)] Loss: 6492.888672\n",
      "Train Epoch: 149 [41216/225000 (18%)] Loss: 6517.019531\n",
      "Train Epoch: 149 [45312/225000 (20%)] Loss: 6385.298828\n",
      "Train Epoch: 149 [49408/225000 (22%)] Loss: 8323.865234\n",
      "Train Epoch: 149 [53504/225000 (24%)] Loss: 6414.558594\n",
      "Train Epoch: 149 [57600/225000 (26%)] Loss: 6348.960938\n",
      "Train Epoch: 149 [61696/225000 (27%)] Loss: 6555.267578\n",
      "Train Epoch: 149 [65792/225000 (29%)] Loss: 6512.222656\n",
      "Train Epoch: 149 [69888/225000 (31%)] Loss: 6285.058594\n",
      "Train Epoch: 149 [73984/225000 (33%)] Loss: 6488.691406\n",
      "Train Epoch: 149 [78080/225000 (35%)] Loss: 6581.201172\n",
      "Train Epoch: 149 [82176/225000 (37%)] Loss: 6298.824219\n",
      "Train Epoch: 149 [86272/225000 (38%)] Loss: 6502.408203\n",
      "Train Epoch: 149 [90368/225000 (40%)] Loss: 6456.025391\n",
      "Train Epoch: 149 [94464/225000 (42%)] Loss: 6583.619141\n",
      "Train Epoch: 149 [98560/225000 (44%)] Loss: 6539.369141\n",
      "Train Epoch: 149 [102656/225000 (46%)] Loss: 6537.662109\n",
      "Train Epoch: 149 [106752/225000 (47%)] Loss: 6463.101562\n",
      "Train Epoch: 149 [110848/225000 (49%)] Loss: 6513.185547\n",
      "Train Epoch: 149 [114944/225000 (51%)] Loss: 6413.789062\n",
      "Train Epoch: 149 [119040/225000 (53%)] Loss: 6323.015625\n",
      "Train Epoch: 149 [123136/225000 (55%)] Loss: 6355.644531\n",
      "Train Epoch: 149 [127232/225000 (57%)] Loss: 6401.412109\n",
      "Train Epoch: 149 [131328/225000 (58%)] Loss: 6411.529297\n",
      "Train Epoch: 149 [135424/225000 (60%)] Loss: 6408.382812\n",
      "Train Epoch: 149 [139520/225000 (62%)] Loss: 6383.066406\n",
      "Train Epoch: 149 [143616/225000 (64%)] Loss: 6401.345703\n",
      "Train Epoch: 149 [147712/225000 (66%)] Loss: 6470.080078\n",
      "Train Epoch: 149 [151808/225000 (67%)] Loss: 6465.419922\n",
      "Train Epoch: 149 [155904/225000 (69%)] Loss: 6435.205078\n",
      "Train Epoch: 149 [160000/225000 (71%)] Loss: 6488.929688\n",
      "Train Epoch: 149 [164096/225000 (73%)] Loss: 6528.695312\n",
      "Train Epoch: 149 [168192/225000 (75%)] Loss: 6501.314453\n",
      "Train Epoch: 149 [172288/225000 (77%)] Loss: 6536.433594\n",
      "Train Epoch: 149 [176384/225000 (78%)] Loss: 6606.646484\n",
      "Train Epoch: 149 [180480/225000 (80%)] Loss: 6483.314453\n",
      "Train Epoch: 149 [184576/225000 (82%)] Loss: 6355.667969\n",
      "Train Epoch: 149 [188672/225000 (84%)] Loss: 6293.093750\n",
      "Train Epoch: 149 [192768/225000 (86%)] Loss: 6473.167969\n",
      "Train Epoch: 149 [196864/225000 (87%)] Loss: 6401.548828\n",
      "Train Epoch: 149 [200960/225000 (89%)] Loss: 6558.996094\n",
      "Train Epoch: 149 [205056/225000 (91%)] Loss: 6380.513672\n",
      "Train Epoch: 149 [209152/225000 (93%)] Loss: 6461.558594\n",
      "Train Epoch: 149 [213248/225000 (95%)] Loss: 6301.980469\n",
      "Train Epoch: 149 [217344/225000 (97%)] Loss: 17299.449219\n",
      "Train Epoch: 149 [221440/225000 (98%)] Loss: 6581.302734\n",
      "    epoch          : 149\n",
      "    loss           : 6535.989398908561\n",
      "    val_loss       : 6570.422496706247\n",
      "Train Epoch: 150 [256/225000 (0%)] Loss: 6557.662109\n",
      "Train Epoch: 150 [4352/225000 (2%)] Loss: 6340.537109\n",
      "Train Epoch: 150 [8448/225000 (4%)] Loss: 6302.642578\n",
      "Train Epoch: 150 [12544/225000 (6%)] Loss: 6565.472656\n",
      "Train Epoch: 150 [16640/225000 (7%)] Loss: 6303.123047\n",
      "Train Epoch: 150 [20736/225000 (9%)] Loss: 6513.324219\n",
      "Train Epoch: 150 [24832/225000 (11%)] Loss: 6521.701172\n",
      "Train Epoch: 150 [28928/225000 (13%)] Loss: 6533.732422\n",
      "Train Epoch: 150 [33024/225000 (15%)] Loss: 6499.308594\n",
      "Train Epoch: 150 [37120/225000 (16%)] Loss: 6240.007812\n",
      "Train Epoch: 150 [41216/225000 (18%)] Loss: 6478.398438\n",
      "Train Epoch: 150 [45312/225000 (20%)] Loss: 6467.005859\n",
      "Train Epoch: 150 [49408/225000 (22%)] Loss: 6403.550781\n",
      "Train Epoch: 150 [53504/225000 (24%)] Loss: 6301.517578\n",
      "Train Epoch: 150 [57600/225000 (26%)] Loss: 6516.292969\n",
      "Train Epoch: 150 [61696/225000 (27%)] Loss: 6458.929688\n",
      "Train Epoch: 150 [65792/225000 (29%)] Loss: 6369.058594\n",
      "Train Epoch: 150 [69888/225000 (31%)] Loss: 6563.279297\n",
      "Train Epoch: 150 [73984/225000 (33%)] Loss: 6400.072266\n",
      "Train Epoch: 150 [78080/225000 (35%)] Loss: 6363.714844\n",
      "Train Epoch: 150 [82176/225000 (37%)] Loss: 6338.923828\n",
      "Train Epoch: 150 [86272/225000 (38%)] Loss: 17363.613281\n",
      "Train Epoch: 150 [90368/225000 (40%)] Loss: 6458.291016\n",
      "Train Epoch: 150 [94464/225000 (42%)] Loss: 6426.392578\n",
      "Train Epoch: 150 [98560/225000 (44%)] Loss: 6444.873047\n",
      "Train Epoch: 150 [102656/225000 (46%)] Loss: 6350.958984\n",
      "Train Epoch: 150 [106752/225000 (47%)] Loss: 6517.123047\n",
      "Train Epoch: 150 [110848/225000 (49%)] Loss: 6365.363281\n",
      "Train Epoch: 150 [114944/225000 (51%)] Loss: 6453.349609\n",
      "Train Epoch: 150 [119040/225000 (53%)] Loss: 6395.718750\n",
      "Train Epoch: 150 [123136/225000 (55%)] Loss: 6393.839844\n",
      "Train Epoch: 150 [127232/225000 (57%)] Loss: 6415.107422\n",
      "Train Epoch: 150 [131328/225000 (58%)] Loss: 6404.269531\n",
      "Train Epoch: 150 [135424/225000 (60%)] Loss: 6391.822266\n",
      "Train Epoch: 150 [139520/225000 (62%)] Loss: 6392.804688\n",
      "Train Epoch: 150 [143616/225000 (64%)] Loss: 6261.681641\n",
      "Train Epoch: 150 [147712/225000 (66%)] Loss: 6459.750000\n",
      "Train Epoch: 150 [151808/225000 (67%)] Loss: 6277.916016\n",
      "Train Epoch: 150 [155904/225000 (69%)] Loss: 6394.091797\n",
      "Train Epoch: 150 [160000/225000 (71%)] Loss: 6283.396484\n",
      "Train Epoch: 150 [164096/225000 (73%)] Loss: 6424.882812\n",
      "Train Epoch: 150 [168192/225000 (75%)] Loss: 6487.162109\n",
      "Train Epoch: 150 [172288/225000 (77%)] Loss: 6328.435547\n",
      "Train Epoch: 150 [176384/225000 (78%)] Loss: 6440.021484\n",
      "Train Epoch: 150 [180480/225000 (80%)] Loss: 6427.082031\n",
      "Train Epoch: 150 [184576/225000 (82%)] Loss: 6466.189453\n",
      "Train Epoch: 150 [188672/225000 (84%)] Loss: 6496.966797\n",
      "Train Epoch: 150 [192768/225000 (86%)] Loss: 6552.835938\n",
      "Train Epoch: 150 [196864/225000 (87%)] Loss: 6351.417969\n",
      "Train Epoch: 150 [200960/225000 (89%)] Loss: 6402.199219\n",
      "Train Epoch: 150 [205056/225000 (91%)] Loss: 6349.753906\n",
      "Train Epoch: 150 [209152/225000 (93%)] Loss: 6618.281250\n",
      "Train Epoch: 150 [213248/225000 (95%)] Loss: 6371.324219\n",
      "Train Epoch: 150 [217344/225000 (97%)] Loss: 6495.619141\n",
      "Train Epoch: 150 [221440/225000 (98%)] Loss: 6451.738281\n",
      "    epoch          : 150\n",
      "    loss           : 6485.411579431528\n",
      "    val_loss       : 6644.788285180014\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0815_160624/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [256/225000 (0%)] Loss: 6404.679688\n",
      "Train Epoch: 151 [4352/225000 (2%)] Loss: 6448.736328\n",
      "Train Epoch: 151 [8448/225000 (4%)] Loss: 6492.746094\n",
      "Train Epoch: 151 [12544/225000 (6%)] Loss: 6349.257812\n",
      "Train Epoch: 151 [16640/225000 (7%)] Loss: 6348.968750\n",
      "Train Epoch: 151 [20736/225000 (9%)] Loss: 6463.429688\n",
      "Train Epoch: 151 [24832/225000 (11%)] Loss: 6496.675781\n",
      "Train Epoch: 151 [28928/225000 (13%)] Loss: 6329.289062\n",
      "Train Epoch: 151 [33024/225000 (15%)] Loss: 6305.652344\n",
      "Train Epoch: 151 [37120/225000 (16%)] Loss: 6457.640625\n",
      "Train Epoch: 151 [41216/225000 (18%)] Loss: 6501.744141\n",
      "Train Epoch: 151 [45312/225000 (20%)] Loss: 6423.146484\n",
      "Train Epoch: 151 [49408/225000 (22%)] Loss: 6542.615234\n",
      "Train Epoch: 151 [53504/225000 (24%)] Loss: 6386.427734\n",
      "Train Epoch: 151 [57600/225000 (26%)] Loss: 6334.281250\n",
      "Train Epoch: 151 [61696/225000 (27%)] Loss: 6635.910156\n",
      "Train Epoch: 151 [65792/225000 (29%)] Loss: 6604.978516\n",
      "Train Epoch: 151 [69888/225000 (31%)] Loss: 6356.494141\n",
      "Train Epoch: 151 [73984/225000 (33%)] Loss: 6292.005859\n",
      "Train Epoch: 151 [78080/225000 (35%)] Loss: 6388.630859\n",
      "Train Epoch: 151 [82176/225000 (37%)] Loss: 6501.716797\n",
      "Train Epoch: 151 [86272/225000 (38%)] Loss: 6581.123047\n",
      "Train Epoch: 151 [90368/225000 (40%)] Loss: 6260.443359\n",
      "Train Epoch: 151 [94464/225000 (42%)] Loss: 6344.365234\n",
      "Train Epoch: 151 [98560/225000 (44%)] Loss: 6403.931641\n",
      "Train Epoch: 151 [102656/225000 (46%)] Loss: 6372.222656\n",
      "Train Epoch: 151 [106752/225000 (47%)] Loss: 6411.900391\n",
      "Train Epoch: 151 [110848/225000 (49%)] Loss: 6537.914062\n",
      "Train Epoch: 151 [114944/225000 (51%)] Loss: 6428.289062\n",
      "Train Epoch: 151 [119040/225000 (53%)] Loss: 6413.992188\n",
      "Train Epoch: 151 [123136/225000 (55%)] Loss: 6399.583984\n",
      "Train Epoch: 151 [127232/225000 (57%)] Loss: 6494.773438\n",
      "Train Epoch: 151 [131328/225000 (58%)] Loss: 6421.449219\n",
      "Train Epoch: 151 [135424/225000 (60%)] Loss: 6361.111328\n",
      "Train Epoch: 151 [139520/225000 (62%)] Loss: 6573.609375\n",
      "Train Epoch: 151 [143616/225000 (64%)] Loss: 6458.927734\n",
      "Train Epoch: 151 [147712/225000 (66%)] Loss: 6348.882812\n",
      "Train Epoch: 151 [151808/225000 (67%)] Loss: 6317.945312\n",
      "Train Epoch: 151 [155904/225000 (69%)] Loss: 6398.095703\n",
      "Train Epoch: 151 [160000/225000 (71%)] Loss: 6526.429688\n",
      "Train Epoch: 151 [164096/225000 (73%)] Loss: 6442.169922\n",
      "Train Epoch: 151 [168192/225000 (75%)] Loss: 6454.380859\n",
      "Train Epoch: 151 [172288/225000 (77%)] Loss: 6340.037109\n",
      "Train Epoch: 151 [176384/225000 (78%)] Loss: 6537.140625\n",
      "Train Epoch: 151 [180480/225000 (80%)] Loss: 6388.146484\n",
      "Train Epoch: 151 [184576/225000 (82%)] Loss: 6460.640625\n",
      "Train Epoch: 151 [188672/225000 (84%)] Loss: 6360.746094\n",
      "Train Epoch: 151 [192768/225000 (86%)] Loss: 6314.457031\n",
      "Train Epoch: 151 [196864/225000 (87%)] Loss: 6341.205078\n",
      "Train Epoch: 151 [200960/225000 (89%)] Loss: 6401.056641\n",
      "Train Epoch: 151 [205056/225000 (91%)] Loss: 6424.783203\n",
      "Train Epoch: 151 [209152/225000 (93%)] Loss: 6379.656250\n",
      "Train Epoch: 151 [213248/225000 (95%)] Loss: 6302.455078\n",
      "Train Epoch: 151 [217344/225000 (97%)] Loss: 6542.453125\n",
      "Train Epoch: 151 [221440/225000 (98%)] Loss: 6357.587891\n",
      "    epoch          : 151\n",
      "    loss           : 6492.273715248151\n",
      "    val_loss       : 6612.133983096298\n",
      "Train Epoch: 152 [256/225000 (0%)] Loss: 6466.154297\n",
      "Train Epoch: 152 [4352/225000 (2%)] Loss: 6329.507812\n",
      "Train Epoch: 152 [8448/225000 (4%)] Loss: 12554.410156\n",
      "Train Epoch: 152 [12544/225000 (6%)] Loss: 6436.306641\n",
      "Train Epoch: 152 [16640/225000 (7%)] Loss: 6443.552734\n",
      "Train Epoch: 152 [20736/225000 (9%)] Loss: 6423.435547\n",
      "Train Epoch: 152 [24832/225000 (11%)] Loss: 6418.775391\n",
      "Train Epoch: 152 [28928/225000 (13%)] Loss: 6415.021484\n",
      "Train Epoch: 152 [33024/225000 (15%)] Loss: 6386.716797\n",
      "Train Epoch: 152 [37120/225000 (16%)] Loss: 6392.978516\n",
      "Train Epoch: 152 [41216/225000 (18%)] Loss: 6338.603516\n",
      "Train Epoch: 152 [45312/225000 (20%)] Loss: 6429.351562\n",
      "Train Epoch: 152 [49408/225000 (22%)] Loss: 6356.906250\n",
      "Train Epoch: 152 [53504/225000 (24%)] Loss: 6492.322266\n",
      "Train Epoch: 152 [57600/225000 (26%)] Loss: 6322.314453\n",
      "Train Epoch: 152 [61696/225000 (27%)] Loss: 6367.873047\n",
      "Train Epoch: 152 [65792/225000 (29%)] Loss: 6535.783203\n",
      "Train Epoch: 152 [69888/225000 (31%)] Loss: 6472.513672\n",
      "Train Epoch: 152 [73984/225000 (33%)] Loss: 8299.083984\n",
      "Train Epoch: 152 [78080/225000 (35%)] Loss: 6383.810547\n",
      "Train Epoch: 152 [82176/225000 (37%)] Loss: 6414.732422\n",
      "Train Epoch: 152 [86272/225000 (38%)] Loss: 6448.265625\n",
      "Train Epoch: 152 [90368/225000 (40%)] Loss: 6305.769531\n",
      "Train Epoch: 152 [94464/225000 (42%)] Loss: 6505.037109\n",
      "Train Epoch: 152 [98560/225000 (44%)] Loss: 6576.835938\n",
      "Train Epoch: 152 [102656/225000 (46%)] Loss: 6412.126953\n",
      "Train Epoch: 152 [106752/225000 (47%)] Loss: 6422.031250\n",
      "Train Epoch: 152 [110848/225000 (49%)] Loss: 6387.908203\n",
      "Train Epoch: 152 [114944/225000 (51%)] Loss: 6349.318359\n",
      "Train Epoch: 152 [119040/225000 (53%)] Loss: 6418.576172\n",
      "Train Epoch: 152 [123136/225000 (55%)] Loss: 6430.855469\n",
      "Train Epoch: 152 [127232/225000 (57%)] Loss: 6368.935547\n",
      "Train Epoch: 152 [131328/225000 (58%)] Loss: 6539.917969\n",
      "Train Epoch: 152 [135424/225000 (60%)] Loss: 6431.318359\n",
      "Train Epoch: 152 [139520/225000 (62%)] Loss: 6438.789062\n",
      "Train Epoch: 152 [143616/225000 (64%)] Loss: 6312.458984\n",
      "Train Epoch: 152 [147712/225000 (66%)] Loss: 6555.519531\n",
      "Train Epoch: 152 [151808/225000 (67%)] Loss: 6369.929688\n",
      "Train Epoch: 152 [155904/225000 (69%)] Loss: 6469.298828\n",
      "Train Epoch: 152 [160000/225000 (71%)] Loss: 6445.033203\n",
      "Train Epoch: 152 [164096/225000 (73%)] Loss: 8257.802734\n",
      "Train Epoch: 152 [168192/225000 (75%)] Loss: 6395.642578\n",
      "Train Epoch: 152 [172288/225000 (77%)] Loss: 6535.419922\n",
      "Train Epoch: 152 [176384/225000 (78%)] Loss: 6446.125000\n",
      "Train Epoch: 152 [180480/225000 (80%)] Loss: 6481.947266\n",
      "Train Epoch: 152 [184576/225000 (82%)] Loss: 6440.130859\n",
      "Train Epoch: 152 [188672/225000 (84%)] Loss: 6320.359375\n",
      "Train Epoch: 152 [192768/225000 (86%)] Loss: 6417.132812\n",
      "Train Epoch: 152 [196864/225000 (87%)] Loss: 6443.750000\n",
      "Train Epoch: 152 [200960/225000 (89%)] Loss: 6380.800781\n",
      "Train Epoch: 152 [205056/225000 (91%)] Loss: 6569.300781\n",
      "Train Epoch: 152 [209152/225000 (93%)] Loss: 6511.048828\n",
      "Train Epoch: 152 [213248/225000 (95%)] Loss: 6341.453125\n",
      "Train Epoch: 152 [217344/225000 (97%)] Loss: 6543.328125\n",
      "Train Epoch: 152 [221440/225000 (98%)] Loss: 6317.201172\n",
      "    epoch          : 152\n",
      "    loss           : 6557.250633265785\n",
      "    val_loss       : 6477.003808633405\n",
      "Train Epoch: 153 [256/225000 (0%)] Loss: 6431.496094\n",
      "Train Epoch: 153 [4352/225000 (2%)] Loss: 6346.517578\n",
      "Train Epoch: 153 [8448/225000 (4%)] Loss: 6706.439453\n",
      "Train Epoch: 153 [12544/225000 (6%)] Loss: 6356.210938\n",
      "Train Epoch: 153 [16640/225000 (7%)] Loss: 6492.976562\n",
      "Train Epoch: 153 [20736/225000 (9%)] Loss: 6387.937500\n",
      "Train Epoch: 153 [24832/225000 (11%)] Loss: 6332.099609\n",
      "Train Epoch: 153 [28928/225000 (13%)] Loss: 6488.419922\n",
      "Train Epoch: 153 [33024/225000 (15%)] Loss: 6408.873047\n",
      "Train Epoch: 153 [37120/225000 (16%)] Loss: 6344.089844\n",
      "Train Epoch: 153 [41216/225000 (18%)] Loss: 6492.718750\n",
      "Train Epoch: 153 [45312/225000 (20%)] Loss: 6426.564453\n",
      "Train Epoch: 153 [49408/225000 (22%)] Loss: 6485.933594\n",
      "Train Epoch: 153 [53504/225000 (24%)] Loss: 6302.957031\n",
      "Train Epoch: 153 [57600/225000 (26%)] Loss: 6430.769531\n",
      "Train Epoch: 153 [61696/225000 (27%)] Loss: 6387.363281\n",
      "Train Epoch: 153 [65792/225000 (29%)] Loss: 6432.267578\n",
      "Train Epoch: 153 [69888/225000 (31%)] Loss: 6485.785156\n",
      "Train Epoch: 153 [73984/225000 (33%)] Loss: 6451.537109\n",
      "Train Epoch: 153 [78080/225000 (35%)] Loss: 6386.130859\n",
      "Train Epoch: 153 [82176/225000 (37%)] Loss: 6284.601562\n",
      "Train Epoch: 153 [86272/225000 (38%)] Loss: 6378.601562\n",
      "Train Epoch: 153 [90368/225000 (40%)] Loss: 6591.425781\n",
      "Train Epoch: 153 [94464/225000 (42%)] Loss: 6371.185547\n",
      "Train Epoch: 153 [98560/225000 (44%)] Loss: 6369.871094\n",
      "Train Epoch: 153 [102656/225000 (46%)] Loss: 6424.884766\n",
      "Train Epoch: 153 [106752/225000 (47%)] Loss: 6449.257812\n",
      "Train Epoch: 153 [110848/225000 (49%)] Loss: 6307.857422\n",
      "Train Epoch: 153 [114944/225000 (51%)] Loss: 6401.650391\n",
      "Train Epoch: 153 [119040/225000 (53%)] Loss: 6537.373047\n",
      "Train Epoch: 153 [123136/225000 (55%)] Loss: 6599.587891\n",
      "Train Epoch: 153 [127232/225000 (57%)] Loss: 6478.082031\n",
      "Train Epoch: 153 [131328/225000 (58%)] Loss: 6515.537109\n",
      "Train Epoch: 153 [135424/225000 (60%)] Loss: 6371.318359\n",
      "Train Epoch: 153 [139520/225000 (62%)] Loss: 6514.644531\n",
      "Train Epoch: 153 [143616/225000 (64%)] Loss: 6392.933594\n",
      "Train Epoch: 153 [147712/225000 (66%)] Loss: 6366.361328\n",
      "Train Epoch: 153 [151808/225000 (67%)] Loss: 6369.076172\n",
      "Train Epoch: 153 [155904/225000 (69%)] Loss: 6409.025391\n",
      "Train Epoch: 153 [160000/225000 (71%)] Loss: 6332.113281\n",
      "Train Epoch: 153 [164096/225000 (73%)] Loss: 6461.904297\n",
      "Train Epoch: 153 [168192/225000 (75%)] Loss: 6402.496094\n",
      "Train Epoch: 153 [172288/225000 (77%)] Loss: 6383.537109\n",
      "Train Epoch: 153 [176384/225000 (78%)] Loss: 6442.457031\n",
      "Train Epoch: 153 [180480/225000 (80%)] Loss: 6412.998047\n",
      "Train Epoch: 153 [184576/225000 (82%)] Loss: 6462.837891\n",
      "Train Epoch: 153 [188672/225000 (84%)] Loss: 6418.404297\n",
      "Train Epoch: 153 [192768/225000 (86%)] Loss: 6370.230469\n",
      "Train Epoch: 153 [196864/225000 (87%)] Loss: 6410.841797\n",
      "Train Epoch: 153 [200960/225000 (89%)] Loss: 6427.126953\n",
      "Train Epoch: 153 [205056/225000 (91%)] Loss: 6441.960938\n",
      "Train Epoch: 153 [209152/225000 (93%)] Loss: 6511.833984\n",
      "Train Epoch: 153 [213248/225000 (95%)] Loss: 6410.003906\n",
      "Train Epoch: 153 [217344/225000 (97%)] Loss: 6456.167969\n",
      "Train Epoch: 153 [221440/225000 (98%)] Loss: 17271.441406\n",
      "    epoch          : 153\n",
      "    loss           : 6503.995126075441\n",
      "    val_loss       : 6975.353445706319\n",
      "Train Epoch: 154 [256/225000 (0%)] Loss: 6448.570312\n",
      "Train Epoch: 154 [4352/225000 (2%)] Loss: 6285.570312\n",
      "Train Epoch: 154 [8448/225000 (4%)] Loss: 6496.677734\n",
      "Train Epoch: 154 [12544/225000 (6%)] Loss: 6438.554688\n",
      "Train Epoch: 154 [16640/225000 (7%)] Loss: 6557.699219\n",
      "Train Epoch: 154 [20736/225000 (9%)] Loss: 6401.400391\n",
      "Train Epoch: 154 [24832/225000 (11%)] Loss: 6436.150391\n",
      "Train Epoch: 154 [28928/225000 (13%)] Loss: 6435.380859\n",
      "Train Epoch: 154 [33024/225000 (15%)] Loss: 6365.888672\n",
      "Train Epoch: 154 [37120/225000 (16%)] Loss: 6357.939453\n",
      "Train Epoch: 154 [41216/225000 (18%)] Loss: 6301.849609\n",
      "Train Epoch: 154 [45312/225000 (20%)] Loss: 6468.595703\n",
      "Train Epoch: 154 [49408/225000 (22%)] Loss: 6581.333984\n",
      "Train Epoch: 154 [53504/225000 (24%)] Loss: 6461.035156\n",
      "Train Epoch: 154 [57600/225000 (26%)] Loss: 6435.162109\n",
      "Train Epoch: 154 [61696/225000 (27%)] Loss: 6514.548828\n",
      "Train Epoch: 154 [65792/225000 (29%)] Loss: 6405.718750\n",
      "Train Epoch: 154 [69888/225000 (31%)] Loss: 6398.845703\n",
      "Train Epoch: 154 [73984/225000 (33%)] Loss: 6492.257812\n",
      "Train Epoch: 154 [78080/225000 (35%)] Loss: 6466.146484\n",
      "Train Epoch: 154 [82176/225000 (37%)] Loss: 6513.931641\n",
      "Train Epoch: 154 [86272/225000 (38%)] Loss: 6424.183594\n",
      "Train Epoch: 154 [90368/225000 (40%)] Loss: 6561.757812\n",
      "Train Epoch: 154 [94464/225000 (42%)] Loss: 6376.810547\n",
      "Train Epoch: 154 [98560/225000 (44%)] Loss: 6426.917969\n",
      "Train Epoch: 154 [102656/225000 (46%)] Loss: 6503.298828\n",
      "Train Epoch: 154 [106752/225000 (47%)] Loss: 17036.458984\n",
      "Train Epoch: 154 [110848/225000 (49%)] Loss: 6618.664062\n",
      "Train Epoch: 154 [114944/225000 (51%)] Loss: 6528.244141\n",
      "Train Epoch: 154 [119040/225000 (53%)] Loss: 6336.750000\n",
      "Train Epoch: 154 [123136/225000 (55%)] Loss: 6487.453125\n",
      "Train Epoch: 154 [127232/225000 (57%)] Loss: 6407.855469\n",
      "Train Epoch: 154 [131328/225000 (58%)] Loss: 6394.023438\n",
      "Train Epoch: 154 [135424/225000 (60%)] Loss: 6486.189453\n",
      "Train Epoch: 154 [139520/225000 (62%)] Loss: 6394.011719\n",
      "Train Epoch: 154 [143616/225000 (64%)] Loss: 6441.121094\n",
      "Train Epoch: 154 [147712/225000 (66%)] Loss: 6394.560547\n",
      "Train Epoch: 154 [151808/225000 (67%)] Loss: 6521.105469\n",
      "Train Epoch: 154 [155904/225000 (69%)] Loss: 6374.001953\n",
      "Train Epoch: 154 [160000/225000 (71%)] Loss: 6383.429688\n",
      "Train Epoch: 154 [164096/225000 (73%)] Loss: 6531.193359\n",
      "Train Epoch: 154 [168192/225000 (75%)] Loss: 6539.705078\n",
      "Train Epoch: 154 [172288/225000 (77%)] Loss: 6500.966797\n",
      "Train Epoch: 154 [176384/225000 (78%)] Loss: 6394.859375\n",
      "Train Epoch: 154 [180480/225000 (80%)] Loss: 6473.087891\n",
      "Train Epoch: 154 [184576/225000 (82%)] Loss: 6439.804688\n",
      "Train Epoch: 154 [188672/225000 (84%)] Loss: 6562.939453\n",
      "Train Epoch: 154 [192768/225000 (86%)] Loss: 6409.265625\n",
      "Train Epoch: 154 [196864/225000 (87%)] Loss: 6395.634766\n",
      "Train Epoch: 154 [200960/225000 (89%)] Loss: 8157.158203\n",
      "Train Epoch: 154 [205056/225000 (91%)] Loss: 6409.371094\n",
      "Train Epoch: 154 [209152/225000 (93%)] Loss: 6405.224609\n",
      "Train Epoch: 154 [213248/225000 (95%)] Loss: 6576.798828\n",
      "Train Epoch: 154 [217344/225000 (97%)] Loss: 6504.109375\n",
      "Train Epoch: 154 [221440/225000 (98%)] Loss: 6464.958984\n",
      "    epoch          : 154\n",
      "    loss           : 6545.040782316553\n",
      "    val_loss       : 6475.666268851076\n",
      "Train Epoch: 155 [256/225000 (0%)] Loss: 6475.691406\n",
      "Train Epoch: 155 [4352/225000 (2%)] Loss: 6436.947266\n",
      "Train Epoch: 155 [8448/225000 (4%)] Loss: 6406.755859\n",
      "Train Epoch: 155 [12544/225000 (6%)] Loss: 6456.181641\n",
      "Train Epoch: 155 [16640/225000 (7%)] Loss: 6414.669922\n",
      "Train Epoch: 155 [20736/225000 (9%)] Loss: 6411.179688\n",
      "Train Epoch: 155 [24832/225000 (11%)] Loss: 6561.005859\n",
      "Train Epoch: 155 [28928/225000 (13%)] Loss: 6474.710938\n",
      "Train Epoch: 155 [33024/225000 (15%)] Loss: 6407.947266\n",
      "Train Epoch: 155 [37120/225000 (16%)] Loss: 6357.126953\n",
      "Train Epoch: 155 [41216/225000 (18%)] Loss: 6306.677734\n",
      "Train Epoch: 155 [45312/225000 (20%)] Loss: 6332.164062\n",
      "Train Epoch: 155 [49408/225000 (22%)] Loss: 6384.595703\n",
      "Train Epoch: 155 [53504/225000 (24%)] Loss: 6385.101562\n",
      "Train Epoch: 155 [57600/225000 (26%)] Loss: 6492.515625\n",
      "Train Epoch: 155 [61696/225000 (27%)] Loss: 6544.396484\n",
      "Train Epoch: 155 [65792/225000 (29%)] Loss: 6358.273438\n",
      "Train Epoch: 155 [69888/225000 (31%)] Loss: 6357.894531\n",
      "Train Epoch: 155 [73984/225000 (33%)] Loss: 6473.087891\n",
      "Train Epoch: 155 [78080/225000 (35%)] Loss: 6496.802734\n",
      "Train Epoch: 155 [82176/225000 (37%)] Loss: 6540.394531\n",
      "Train Epoch: 155 [86272/225000 (38%)] Loss: 6354.166016\n",
      "Train Epoch: 155 [90368/225000 (40%)] Loss: 6395.917969\n",
      "Train Epoch: 155 [94464/225000 (42%)] Loss: 6470.406250\n",
      "Train Epoch: 155 [98560/225000 (44%)] Loss: 6522.130859\n",
      "Train Epoch: 155 [102656/225000 (46%)] Loss: 6385.150391\n",
      "Train Epoch: 155 [106752/225000 (47%)] Loss: 6521.304688\n",
      "Train Epoch: 155 [110848/225000 (49%)] Loss: 6476.101562\n",
      "Train Epoch: 155 [114944/225000 (51%)] Loss: 6370.687500\n",
      "Train Epoch: 155 [119040/225000 (53%)] Loss: 6487.876953\n",
      "Train Epoch: 155 [123136/225000 (55%)] Loss: 6405.443359\n",
      "Train Epoch: 155 [127232/225000 (57%)] Loss: 6459.857422\n",
      "Train Epoch: 155 [131328/225000 (58%)] Loss: 6457.175781\n",
      "Train Epoch: 155 [135424/225000 (60%)] Loss: 6353.693359\n",
      "Train Epoch: 155 [139520/225000 (62%)] Loss: 6422.609375\n",
      "Train Epoch: 155 [143616/225000 (64%)] Loss: 6483.931641\n",
      "Train Epoch: 155 [147712/225000 (66%)] Loss: 6481.675781\n",
      "Train Epoch: 155 [151808/225000 (67%)] Loss: 6373.984375\n",
      "Train Epoch: 155 [155904/225000 (69%)] Loss: 6477.109375\n",
      "Train Epoch: 155 [160000/225000 (71%)] Loss: 6526.470703\n",
      "Train Epoch: 155 [164096/225000 (73%)] Loss: 6436.169922\n",
      "Train Epoch: 155 [168192/225000 (75%)] Loss: 6422.626953\n",
      "Train Epoch: 155 [172288/225000 (77%)] Loss: 6484.990234\n",
      "Train Epoch: 155 [176384/225000 (78%)] Loss: 6432.689453\n",
      "Train Epoch: 155 [180480/225000 (80%)] Loss: 6373.992188\n",
      "Train Epoch: 155 [184576/225000 (82%)] Loss: 6466.210938\n",
      "Train Epoch: 155 [188672/225000 (84%)] Loss: 6415.263672\n",
      "Train Epoch: 155 [192768/225000 (86%)] Loss: 6545.101562\n",
      "Train Epoch: 155 [196864/225000 (87%)] Loss: 6347.552734\n",
      "Train Epoch: 155 [200960/225000 (89%)] Loss: 6380.101562\n",
      "Train Epoch: 155 [205056/225000 (91%)] Loss: 6316.523438\n",
      "Train Epoch: 155 [209152/225000 (93%)] Loss: 6341.080078\n",
      "Train Epoch: 155 [213248/225000 (95%)] Loss: 6336.117188\n",
      "Train Epoch: 155 [217344/225000 (97%)] Loss: 6452.093750\n",
      "Train Epoch: 155 [221440/225000 (98%)] Loss: 6402.515625\n",
      "    epoch          : 155\n",
      "    loss           : 6545.58076027446\n",
      "    val_loss       : 6537.849731294476\n",
      "Train Epoch: 156 [256/225000 (0%)] Loss: 6288.681641\n",
      "Train Epoch: 156 [4352/225000 (2%)] Loss: 6275.601562\n",
      "Train Epoch: 156 [8448/225000 (4%)] Loss: 6493.912109\n",
      "Train Epoch: 156 [12544/225000 (6%)] Loss: 6390.837891\n",
      "Train Epoch: 156 [16640/225000 (7%)] Loss: 6387.156250\n",
      "Train Epoch: 156 [20736/225000 (9%)] Loss: 6410.718750\n",
      "Train Epoch: 156 [24832/225000 (11%)] Loss: 6529.521484\n",
      "Train Epoch: 156 [28928/225000 (13%)] Loss: 6373.578125\n",
      "Train Epoch: 156 [33024/225000 (15%)] Loss: 6487.505859\n",
      "Train Epoch: 156 [37120/225000 (16%)] Loss: 6291.792969\n",
      "Train Epoch: 156 [41216/225000 (18%)] Loss: 6492.642578\n",
      "Train Epoch: 156 [45312/225000 (20%)] Loss: 8213.304688\n",
      "Train Epoch: 156 [49408/225000 (22%)] Loss: 6297.449219\n",
      "Train Epoch: 156 [53504/225000 (24%)] Loss: 6360.859375\n",
      "Train Epoch: 156 [57600/225000 (26%)] Loss: 6302.232422\n",
      "Train Epoch: 156 [61696/225000 (27%)] Loss: 6421.771484\n",
      "Train Epoch: 156 [65792/225000 (29%)] Loss: 6339.917969\n",
      "Train Epoch: 156 [69888/225000 (31%)] Loss: 6394.980469\n",
      "Train Epoch: 156 [73984/225000 (33%)] Loss: 6436.478516\n",
      "Train Epoch: 156 [78080/225000 (35%)] Loss: 6372.908203\n",
      "Train Epoch: 156 [82176/225000 (37%)] Loss: 6378.552734\n",
      "Train Epoch: 156 [86272/225000 (38%)] Loss: 6445.658203\n",
      "Train Epoch: 156 [90368/225000 (40%)] Loss: 6388.917969\n",
      "Train Epoch: 156 [94464/225000 (42%)] Loss: 6343.451172\n",
      "Train Epoch: 156 [98560/225000 (44%)] Loss: 6399.859375\n",
      "Train Epoch: 156 [102656/225000 (46%)] Loss: 6398.736328\n",
      "Train Epoch: 156 [106752/225000 (47%)] Loss: 6432.583984\n",
      "Train Epoch: 156 [110848/225000 (49%)] Loss: 6477.806641\n",
      "Train Epoch: 156 [114944/225000 (51%)] Loss: 6396.951172\n",
      "Train Epoch: 156 [119040/225000 (53%)] Loss: 6422.722656\n",
      "Train Epoch: 156 [123136/225000 (55%)] Loss: 6465.626953\n",
      "Train Epoch: 156 [127232/225000 (57%)] Loss: 8200.539062\n",
      "Train Epoch: 156 [131328/225000 (58%)] Loss: 6458.417969\n",
      "Train Epoch: 156 [135424/225000 (60%)] Loss: 6484.417969\n",
      "Train Epoch: 156 [139520/225000 (62%)] Loss: 6455.101562\n",
      "Train Epoch: 156 [143616/225000 (64%)] Loss: 6432.656250\n",
      "Train Epoch: 156 [147712/225000 (66%)] Loss: 12539.017578\n",
      "Train Epoch: 156 [151808/225000 (67%)] Loss: 6351.435547\n",
      "Train Epoch: 156 [155904/225000 (69%)] Loss: 6461.542969\n",
      "Train Epoch: 156 [160000/225000 (71%)] Loss: 6464.792969\n",
      "Train Epoch: 156 [164096/225000 (73%)] Loss: 6314.906250\n",
      "Train Epoch: 156 [168192/225000 (75%)] Loss: 6558.296875\n",
      "Train Epoch: 156 [172288/225000 (77%)] Loss: 6406.361328\n",
      "Train Epoch: 156 [176384/225000 (78%)] Loss: 6311.068359\n",
      "Train Epoch: 156 [180480/225000 (80%)] Loss: 6426.386719\n",
      "Train Epoch: 156 [184576/225000 (82%)] Loss: 6317.156250\n",
      "Train Epoch: 156 [188672/225000 (84%)] Loss: 6364.033203\n",
      "Train Epoch: 156 [192768/225000 (86%)] Loss: 6389.501953\n",
      "Train Epoch: 156 [196864/225000 (87%)] Loss: 6274.812500\n",
      "Train Epoch: 156 [200960/225000 (89%)] Loss: 6494.650391\n",
      "Train Epoch: 156 [205056/225000 (91%)] Loss: 6339.492188\n",
      "Train Epoch: 156 [209152/225000 (93%)] Loss: 6341.333984\n",
      "Train Epoch: 156 [213248/225000 (95%)] Loss: 6477.886719\n",
      "Train Epoch: 156 [217344/225000 (97%)] Loss: 6386.851562\n",
      "Train Epoch: 156 [221440/225000 (98%)] Loss: 6451.742188\n",
      "    epoch          : 156\n",
      "    loss           : 6502.001233201791\n",
      "    val_loss       : 6460.1105478843865\n",
      "Train Epoch: 157 [256/225000 (0%)] Loss: 6472.361328\n",
      "Train Epoch: 157 [4352/225000 (2%)] Loss: 6396.908203\n",
      "Train Epoch: 157 [8448/225000 (4%)] Loss: 6232.113281\n",
      "Train Epoch: 157 [12544/225000 (6%)] Loss: 6300.164062\n",
      "Train Epoch: 157 [16640/225000 (7%)] Loss: 6390.123047\n",
      "Train Epoch: 157 [20736/225000 (9%)] Loss: 6559.294922\n",
      "Train Epoch: 157 [24832/225000 (11%)] Loss: 6393.966797\n",
      "Train Epoch: 157 [28928/225000 (13%)] Loss: 6370.068359\n",
      "Train Epoch: 157 [33024/225000 (15%)] Loss: 6312.195312\n",
      "Train Epoch: 157 [37120/225000 (16%)] Loss: 6357.480469\n",
      "Train Epoch: 157 [41216/225000 (18%)] Loss: 6259.441406\n",
      "Train Epoch: 157 [45312/225000 (20%)] Loss: 6317.732422\n",
      "Train Epoch: 157 [49408/225000 (22%)] Loss: 6430.310547\n",
      "Train Epoch: 157 [53504/225000 (24%)] Loss: 6343.546875\n",
      "Train Epoch: 157 [57600/225000 (26%)] Loss: 6414.232422\n",
      "Train Epoch: 157 [61696/225000 (27%)] Loss: 6446.455078\n",
      "Train Epoch: 157 [65792/225000 (29%)] Loss: 6381.210938\n",
      "Train Epoch: 157 [69888/225000 (31%)] Loss: 6368.396484\n",
      "Train Epoch: 157 [73984/225000 (33%)] Loss: 6391.976562\n",
      "Train Epoch: 157 [78080/225000 (35%)] Loss: 6406.556641\n",
      "Train Epoch: 157 [82176/225000 (37%)] Loss: 6305.216797\n",
      "Train Epoch: 157 [86272/225000 (38%)] Loss: 6534.871094\n",
      "Train Epoch: 157 [90368/225000 (40%)] Loss: 6566.597656\n",
      "Train Epoch: 157 [94464/225000 (42%)] Loss: 6328.191406\n",
      "Train Epoch: 157 [98560/225000 (44%)] Loss: 6441.738281\n",
      "Train Epoch: 157 [102656/225000 (46%)] Loss: 6554.769531\n",
      "Train Epoch: 157 [106752/225000 (47%)] Loss: 6510.042969\n",
      "Train Epoch: 157 [110848/225000 (49%)] Loss: 6455.986328\n",
      "Train Epoch: 157 [114944/225000 (51%)] Loss: 6346.798828\n",
      "Train Epoch: 157 [119040/225000 (53%)] Loss: 6330.173828\n",
      "Train Epoch: 157 [123136/225000 (55%)] Loss: 6375.394531\n",
      "Train Epoch: 157 [127232/225000 (57%)] Loss: 6446.746094\n",
      "Train Epoch: 157 [131328/225000 (58%)] Loss: 6472.490234\n",
      "Train Epoch: 157 [135424/225000 (60%)] Loss: 6468.033203\n",
      "Train Epoch: 157 [139520/225000 (62%)] Loss: 6253.982422\n",
      "Train Epoch: 157 [143616/225000 (64%)] Loss: 6449.630859\n",
      "Train Epoch: 157 [147712/225000 (66%)] Loss: 6381.246094\n",
      "Train Epoch: 157 [151808/225000 (67%)] Loss: 6342.769531\n",
      "Train Epoch: 157 [155904/225000 (69%)] Loss: 6388.505859\n",
      "Train Epoch: 157 [160000/225000 (71%)] Loss: 6477.056641\n",
      "Train Epoch: 157 [164096/225000 (73%)] Loss: 6368.537109\n",
      "Train Epoch: 157 [168192/225000 (75%)] Loss: 6321.259766\n",
      "Train Epoch: 157 [172288/225000 (77%)] Loss: 6326.464844\n",
      "Train Epoch: 157 [176384/225000 (78%)] Loss: 6452.876953\n",
      "Train Epoch: 157 [180480/225000 (80%)] Loss: 6340.550781\n",
      "Train Epoch: 157 [184576/225000 (82%)] Loss: 6513.841797\n",
      "Train Epoch: 157 [188672/225000 (84%)] Loss: 6286.382812\n",
      "Train Epoch: 157 [192768/225000 (86%)] Loss: 6516.847656\n",
      "Train Epoch: 157 [196864/225000 (87%)] Loss: 6387.058594\n",
      "Train Epoch: 157 [200960/225000 (89%)] Loss: 6455.796875\n",
      "Train Epoch: 157 [205056/225000 (91%)] Loss: 6397.046875\n",
      "Train Epoch: 157 [209152/225000 (93%)] Loss: 6409.449219\n",
      "Train Epoch: 157 [213248/225000 (95%)] Loss: 6547.722656\n",
      "Train Epoch: 157 [217344/225000 (97%)] Loss: 6337.525391\n",
      "Train Epoch: 157 [221440/225000 (98%)] Loss: 6450.496094\n",
      "    epoch          : 157\n",
      "    loss           : 6462.193541577787\n",
      "    val_loss       : 6477.601331323994\n",
      "Train Epoch: 158 [256/225000 (0%)] Loss: 6494.964844\n",
      "Train Epoch: 158 [4352/225000 (2%)] Loss: 6391.658203\n",
      "Train Epoch: 158 [8448/225000 (4%)] Loss: 6464.730469\n",
      "Train Epoch: 158 [12544/225000 (6%)] Loss: 6304.908203\n",
      "Train Epoch: 158 [16640/225000 (7%)] Loss: 6365.119141\n",
      "Train Epoch: 158 [20736/225000 (9%)] Loss: 6419.486328\n",
      "Train Epoch: 158 [24832/225000 (11%)] Loss: 6321.564453\n",
      "Train Epoch: 158 [28928/225000 (13%)] Loss: 6324.666016\n",
      "Train Epoch: 158 [33024/225000 (15%)] Loss: 6497.396484\n",
      "Train Epoch: 158 [37120/225000 (16%)] Loss: 6367.277344\n",
      "Train Epoch: 158 [41216/225000 (18%)] Loss: 6429.001953\n",
      "Train Epoch: 158 [45312/225000 (20%)] Loss: 6352.541016\n",
      "Train Epoch: 158 [49408/225000 (22%)] Loss: 6481.308594\n",
      "Train Epoch: 158 [53504/225000 (24%)] Loss: 6491.630859\n",
      "Train Epoch: 158 [57600/225000 (26%)] Loss: 6529.925781\n",
      "Train Epoch: 158 [61696/225000 (27%)] Loss: 6443.919922\n",
      "Train Epoch: 158 [65792/225000 (29%)] Loss: 6441.115234\n",
      "Train Epoch: 158 [69888/225000 (31%)] Loss: 6295.496094\n",
      "Train Epoch: 158 [73984/225000 (33%)] Loss: 6436.367188\n",
      "Train Epoch: 158 [78080/225000 (35%)] Loss: 6367.216797\n",
      "Train Epoch: 158 [82176/225000 (37%)] Loss: 6318.248047\n",
      "Train Epoch: 158 [86272/225000 (38%)] Loss: 6412.099609\n",
      "Train Epoch: 158 [90368/225000 (40%)] Loss: 6365.021484\n",
      "Train Epoch: 158 [94464/225000 (42%)] Loss: 6212.830078\n",
      "Train Epoch: 158 [98560/225000 (44%)] Loss: 6431.789062\n",
      "Train Epoch: 158 [102656/225000 (46%)] Loss: 6412.535156\n",
      "Train Epoch: 158 [106752/225000 (47%)] Loss: 6525.384766\n",
      "Train Epoch: 158 [110848/225000 (49%)] Loss: 6471.986328\n",
      "Train Epoch: 158 [114944/225000 (51%)] Loss: 6249.449219\n",
      "Train Epoch: 158 [119040/225000 (53%)] Loss: 6395.613281\n",
      "Train Epoch: 158 [123136/225000 (55%)] Loss: 6469.310547\n",
      "Train Epoch: 158 [127232/225000 (57%)] Loss: 6425.353516\n",
      "Train Epoch: 158 [131328/225000 (58%)] Loss: 6426.041016\n",
      "Train Epoch: 158 [135424/225000 (60%)] Loss: 6407.527344\n",
      "Train Epoch: 158 [139520/225000 (62%)] Loss: 6475.351562\n",
      "Train Epoch: 158 [143616/225000 (64%)] Loss: 6400.994141\n",
      "Train Epoch: 158 [147712/225000 (66%)] Loss: 6286.203125\n",
      "Train Epoch: 158 [151808/225000 (67%)] Loss: 6355.615234\n",
      "Train Epoch: 158 [155904/225000 (69%)] Loss: 6441.779297\n",
      "Train Epoch: 158 [160000/225000 (71%)] Loss: 6479.708984\n",
      "Train Epoch: 158 [164096/225000 (73%)] Loss: 6405.181641\n",
      "Train Epoch: 158 [168192/225000 (75%)] Loss: 6326.994141\n",
      "Train Epoch: 158 [172288/225000 (77%)] Loss: 6305.550781\n",
      "Train Epoch: 158 [176384/225000 (78%)] Loss: 6547.167969\n",
      "Train Epoch: 158 [180480/225000 (80%)] Loss: 6480.232422\n",
      "Train Epoch: 158 [184576/225000 (82%)] Loss: 6486.703125\n",
      "Train Epoch: 158 [188672/225000 (84%)] Loss: 6360.558594\n",
      "Train Epoch: 158 [192768/225000 (86%)] Loss: 6440.990234\n",
      "Train Epoch: 158 [196864/225000 (87%)] Loss: 6394.613281\n",
      "Train Epoch: 158 [200960/225000 (89%)] Loss: 6371.337891\n",
      "Train Epoch: 158 [205056/225000 (91%)] Loss: 6405.044922\n",
      "Train Epoch: 158 [209152/225000 (93%)] Loss: 6390.068359\n",
      "Train Epoch: 158 [213248/225000 (95%)] Loss: 6442.746094\n",
      "Train Epoch: 158 [217344/225000 (97%)] Loss: 6315.994141\n",
      "Train Epoch: 158 [221440/225000 (98%)] Loss: 6390.708984\n",
      "    epoch          : 158\n",
      "    loss           : 6470.100285969496\n",
      "    val_loss       : 6455.720141078136\n",
      "Train Epoch: 159 [256/225000 (0%)] Loss: 6346.285156\n",
      "Train Epoch: 159 [4352/225000 (2%)] Loss: 6275.369141\n",
      "Train Epoch: 159 [8448/225000 (4%)] Loss: 6343.380859\n",
      "Train Epoch: 159 [12544/225000 (6%)] Loss: 6416.468750\n",
      "Train Epoch: 159 [16640/225000 (7%)] Loss: 6418.376953\n",
      "Train Epoch: 159 [20736/225000 (9%)] Loss: 6523.617188\n",
      "Train Epoch: 159 [24832/225000 (11%)] Loss: 6489.082031\n",
      "Train Epoch: 159 [28928/225000 (13%)] Loss: 6443.000000\n",
      "Train Epoch: 159 [33024/225000 (15%)] Loss: 6619.988281\n",
      "Train Epoch: 159 [37120/225000 (16%)] Loss: 6210.259766\n",
      "Train Epoch: 159 [41216/225000 (18%)] Loss: 6408.085938\n",
      "Train Epoch: 159 [45312/225000 (20%)] Loss: 6303.742188\n",
      "Train Epoch: 159 [49408/225000 (22%)] Loss: 8200.687500\n",
      "Train Epoch: 159 [53504/225000 (24%)] Loss: 6331.818359\n",
      "Train Epoch: 159 [57600/225000 (26%)] Loss: 6295.541016\n",
      "Train Epoch: 159 [61696/225000 (27%)] Loss: 6340.970703\n",
      "Train Epoch: 159 [65792/225000 (29%)] Loss: 6351.802734\n",
      "Train Epoch: 159 [69888/225000 (31%)] Loss: 6309.439453\n",
      "Train Epoch: 159 [73984/225000 (33%)] Loss: 6454.687500\n",
      "Train Epoch: 159 [78080/225000 (35%)] Loss: 6478.324219\n",
      "Train Epoch: 159 [82176/225000 (37%)] Loss: 6426.552734\n",
      "Train Epoch: 159 [86272/225000 (38%)] Loss: 6375.822266\n",
      "Train Epoch: 159 [90368/225000 (40%)] Loss: 6453.650391\n",
      "Train Epoch: 159 [94464/225000 (42%)] Loss: 6426.402344\n",
      "Train Epoch: 159 [98560/225000 (44%)] Loss: 6378.529297\n",
      "Train Epoch: 159 [102656/225000 (46%)] Loss: 6459.484375\n",
      "Train Epoch: 159 [106752/225000 (47%)] Loss: 6309.154297\n",
      "Train Epoch: 159 [110848/225000 (49%)] Loss: 6421.109375\n",
      "Train Epoch: 159 [114944/225000 (51%)] Loss: 6286.775391\n",
      "Train Epoch: 159 [119040/225000 (53%)] Loss: 6420.787109\n",
      "Train Epoch: 159 [123136/225000 (55%)] Loss: 6338.160156\n",
      "Train Epoch: 159 [127232/225000 (57%)] Loss: 6431.396484\n",
      "Train Epoch: 159 [131328/225000 (58%)] Loss: 17170.486328\n",
      "Train Epoch: 159 [135424/225000 (60%)] Loss: 6514.833984\n",
      "Train Epoch: 159 [139520/225000 (62%)] Loss: 6476.394531\n",
      "Train Epoch: 159 [143616/225000 (64%)] Loss: 6379.474609\n",
      "Train Epoch: 159 [147712/225000 (66%)] Loss: 6339.033203\n",
      "Train Epoch: 159 [151808/225000 (67%)] Loss: 6383.468750\n",
      "Train Epoch: 159 [155904/225000 (69%)] Loss: 6307.544922\n",
      "Train Epoch: 159 [160000/225000 (71%)] Loss: 6382.724609\n",
      "Train Epoch: 159 [164096/225000 (73%)] Loss: 6358.394531\n",
      "Train Epoch: 159 [168192/225000 (75%)] Loss: 6381.134766\n",
      "Train Epoch: 159 [172288/225000 (77%)] Loss: 6387.703125\n",
      "Train Epoch: 159 [176384/225000 (78%)] Loss: 6538.888672\n",
      "Train Epoch: 159 [180480/225000 (80%)] Loss: 6518.748047\n",
      "Train Epoch: 159 [184576/225000 (82%)] Loss: 6587.765625\n",
      "Train Epoch: 159 [188672/225000 (84%)] Loss: 6333.244141\n",
      "Train Epoch: 159 [192768/225000 (86%)] Loss: 6337.148438\n",
      "Train Epoch: 159 [196864/225000 (87%)] Loss: 6487.542969\n",
      "Train Epoch: 159 [200960/225000 (89%)] Loss: 6535.849609\n",
      "Train Epoch: 159 [205056/225000 (91%)] Loss: 6303.384766\n",
      "Train Epoch: 159 [209152/225000 (93%)] Loss: 6524.794922\n",
      "Train Epoch: 159 [213248/225000 (95%)] Loss: 6307.058594\n",
      "Train Epoch: 159 [217344/225000 (97%)] Loss: 6164.671875\n",
      "Train Epoch: 159 [221440/225000 (98%)] Loss: 6461.998047\n",
      "    epoch          : 159\n",
      "    loss           : 6471.636019935651\n",
      "    val_loss       : 6581.6751754016295\n",
      "Train Epoch: 160 [256/225000 (0%)] Loss: 6323.054688\n",
      "Train Epoch: 160 [4352/225000 (2%)] Loss: 6308.748047\n",
      "Train Epoch: 160 [8448/225000 (4%)] Loss: 6457.982422\n",
      "Train Epoch: 160 [12544/225000 (6%)] Loss: 6376.238281\n",
      "Train Epoch: 160 [16640/225000 (7%)] Loss: 6356.900391\n",
      "Train Epoch: 160 [20736/225000 (9%)] Loss: 6458.609375\n",
      "Train Epoch: 160 [24832/225000 (11%)] Loss: 6357.357422\n",
      "Train Epoch: 160 [28928/225000 (13%)] Loss: 6289.859375\n",
      "Train Epoch: 160 [33024/225000 (15%)] Loss: 6392.277344\n",
      "Train Epoch: 160 [37120/225000 (16%)] Loss: 6273.701172\n",
      "Train Epoch: 160 [41216/225000 (18%)] Loss: 6350.095703\n",
      "Train Epoch: 160 [45312/225000 (20%)] Loss: 6479.419922\n",
      "Train Epoch: 160 [49408/225000 (22%)] Loss: 6306.314453\n",
      "Train Epoch: 160 [53504/225000 (24%)] Loss: 6402.818359\n",
      "Train Epoch: 160 [57600/225000 (26%)] Loss: 6358.222656\n",
      "Train Epoch: 160 [61696/225000 (27%)] Loss: 6478.875000\n",
      "Train Epoch: 160 [65792/225000 (29%)] Loss: 6399.423828\n",
      "Train Epoch: 160 [69888/225000 (31%)] Loss: 6462.529297\n",
      "Train Epoch: 160 [73984/225000 (33%)] Loss: 6466.322266\n",
      "Train Epoch: 160 [78080/225000 (35%)] Loss: 6428.054688\n",
      "Train Epoch: 160 [82176/225000 (37%)] Loss: 6266.185547\n",
      "Train Epoch: 160 [86272/225000 (38%)] Loss: 6336.292969\n",
      "Train Epoch: 160 [90368/225000 (40%)] Loss: 6371.490234\n",
      "Train Epoch: 160 [94464/225000 (42%)] Loss: 6338.166016\n",
      "Train Epoch: 160 [98560/225000 (44%)] Loss: 6303.875000\n",
      "Train Epoch: 160 [102656/225000 (46%)] Loss: 6396.259766\n",
      "Train Epoch: 160 [106752/225000 (47%)] Loss: 6333.386719\n",
      "Train Epoch: 160 [110848/225000 (49%)] Loss: 6532.244141\n",
      "Train Epoch: 160 [114944/225000 (51%)] Loss: 6443.181641\n",
      "Train Epoch: 160 [119040/225000 (53%)] Loss: 6313.595703\n",
      "Train Epoch: 160 [123136/225000 (55%)] Loss: 6389.660156\n",
      "Train Epoch: 160 [127232/225000 (57%)] Loss: 6301.371094\n",
      "Train Epoch: 160 [131328/225000 (58%)] Loss: 6371.574219\n",
      "Train Epoch: 160 [135424/225000 (60%)] Loss: 6400.226562\n",
      "Train Epoch: 160 [139520/225000 (62%)] Loss: 6559.482422\n",
      "Train Epoch: 160 [143616/225000 (64%)] Loss: 6370.261719\n",
      "Train Epoch: 160 [147712/225000 (66%)] Loss: 6480.064453\n",
      "Train Epoch: 160 [151808/225000 (67%)] Loss: 6294.140625\n",
      "Train Epoch: 160 [155904/225000 (69%)] Loss: 6481.564453\n",
      "Train Epoch: 160 [160000/225000 (71%)] Loss: 6570.261719\n",
      "Train Epoch: 160 [164096/225000 (73%)] Loss: 6369.921875\n",
      "Train Epoch: 160 [168192/225000 (75%)] Loss: 6416.646484\n",
      "Train Epoch: 160 [172288/225000 (77%)] Loss: 6345.138672\n",
      "Train Epoch: 160 [176384/225000 (78%)] Loss: 6337.863281\n",
      "Train Epoch: 160 [180480/225000 (80%)] Loss: 6326.861328\n",
      "Train Epoch: 160 [184576/225000 (82%)] Loss: 6380.873047\n",
      "Train Epoch: 160 [188672/225000 (84%)] Loss: 6397.707031\n",
      "Train Epoch: 160 [192768/225000 (86%)] Loss: 6184.212891\n",
      "Train Epoch: 160 [196864/225000 (87%)] Loss: 6458.242188\n",
      "Train Epoch: 160 [200960/225000 (89%)] Loss: 6489.298828\n",
      "Train Epoch: 160 [205056/225000 (91%)] Loss: 6365.300781\n",
      "Train Epoch: 160 [209152/225000 (93%)] Loss: 6462.363281\n",
      "Train Epoch: 160 [213248/225000 (95%)] Loss: 6341.441406\n",
      "Train Epoch: 160 [217344/225000 (97%)] Loss: 6457.380859\n",
      "Train Epoch: 160 [221440/225000 (98%)] Loss: 6324.080078\n",
      "    epoch          : 160\n",
      "    loss           : 6434.383467985637\n",
      "    val_loss       : 6534.4798368361535\n",
      "Train Epoch: 161 [256/225000 (0%)] Loss: 6294.222656\n",
      "Train Epoch: 161 [4352/225000 (2%)] Loss: 6477.642578\n",
      "Train Epoch: 161 [8448/225000 (4%)] Loss: 6373.062500\n",
      "Train Epoch: 161 [12544/225000 (6%)] Loss: 6346.335938\n",
      "Train Epoch: 161 [16640/225000 (7%)] Loss: 6573.701172\n",
      "Train Epoch: 161 [20736/225000 (9%)] Loss: 6290.406250\n",
      "Train Epoch: 161 [24832/225000 (11%)] Loss: 6416.839844\n",
      "Train Epoch: 161 [28928/225000 (13%)] Loss: 6282.830078\n",
      "Train Epoch: 161 [33024/225000 (15%)] Loss: 6177.187500\n",
      "Train Epoch: 161 [37120/225000 (16%)] Loss: 6338.519531\n",
      "Train Epoch: 161 [41216/225000 (18%)] Loss: 6419.441406\n",
      "Train Epoch: 161 [45312/225000 (20%)] Loss: 6463.195312\n",
      "Train Epoch: 161 [49408/225000 (22%)] Loss: 6304.773438\n",
      "Train Epoch: 161 [53504/225000 (24%)] Loss: 6432.083984\n",
      "Train Epoch: 161 [57600/225000 (26%)] Loss: 6293.177734\n",
      "Train Epoch: 161 [61696/225000 (27%)] Loss: 6457.517578\n",
      "Train Epoch: 161 [65792/225000 (29%)] Loss: 6320.056641\n",
      "Train Epoch: 161 [69888/225000 (31%)] Loss: 6445.460938\n",
      "Train Epoch: 161 [73984/225000 (33%)] Loss: 6420.541016\n",
      "Train Epoch: 161 [78080/225000 (35%)] Loss: 6296.447266\n",
      "Train Epoch: 161 [82176/225000 (37%)] Loss: 6375.484375\n",
      "Train Epoch: 161 [86272/225000 (38%)] Loss: 6392.335938\n",
      "Train Epoch: 161 [90368/225000 (40%)] Loss: 6404.886719\n",
      "Train Epoch: 161 [94464/225000 (42%)] Loss: 6357.126953\n",
      "Train Epoch: 161 [98560/225000 (44%)] Loss: 6483.847656\n",
      "Train Epoch: 161 [102656/225000 (46%)] Loss: 6385.199219\n",
      "Train Epoch: 161 [106752/225000 (47%)] Loss: 6337.763672\n",
      "Train Epoch: 161 [110848/225000 (49%)] Loss: 6315.255859\n",
      "Train Epoch: 161 [114944/225000 (51%)] Loss: 6420.404297\n",
      "Train Epoch: 161 [119040/225000 (53%)] Loss: 6562.216797\n",
      "Train Epoch: 161 [123136/225000 (55%)] Loss: 6415.431641\n",
      "Train Epoch: 161 [127232/225000 (57%)] Loss: 6386.484375\n",
      "Train Epoch: 161 [131328/225000 (58%)] Loss: 6347.816406\n",
      "Train Epoch: 161 [135424/225000 (60%)] Loss: 6421.970703\n",
      "Train Epoch: 161 [139520/225000 (62%)] Loss: 6331.841797\n",
      "Train Epoch: 161 [143616/225000 (64%)] Loss: 6411.736328\n",
      "Train Epoch: 161 [147712/225000 (66%)] Loss: 6465.841797\n",
      "Train Epoch: 161 [151808/225000 (67%)] Loss: 6479.357422\n",
      "Train Epoch: 161 [155904/225000 (69%)] Loss: 6302.482422\n",
      "Train Epoch: 161 [160000/225000 (71%)] Loss: 6428.582031\n",
      "Train Epoch: 161 [164096/225000 (73%)] Loss: 6415.697266\n",
      "Train Epoch: 161 [168192/225000 (75%)] Loss: 6449.017578\n",
      "Train Epoch: 161 [172288/225000 (77%)] Loss: 6413.517578\n",
      "Train Epoch: 161 [176384/225000 (78%)] Loss: 6443.197266\n",
      "Train Epoch: 161 [180480/225000 (80%)] Loss: 6419.876953\n",
      "Train Epoch: 161 [184576/225000 (82%)] Loss: 6443.611328\n",
      "Train Epoch: 161 [188672/225000 (84%)] Loss: 6389.886719\n",
      "Train Epoch: 161 [192768/225000 (86%)] Loss: 6385.531250\n",
      "Train Epoch: 161 [196864/225000 (87%)] Loss: 6543.591797\n",
      "Train Epoch: 161 [200960/225000 (89%)] Loss: 6279.853516\n",
      "Train Epoch: 161 [205056/225000 (91%)] Loss: 6529.312500\n",
      "Train Epoch: 161 [209152/225000 (93%)] Loss: 6294.798828\n",
      "Train Epoch: 161 [213248/225000 (95%)] Loss: 6397.027344\n",
      "Train Epoch: 161 [217344/225000 (97%)] Loss: 6382.837891\n",
      "Train Epoch: 161 [221440/225000 (98%)] Loss: 6405.613281\n",
      "    epoch          : 161\n",
      "    loss           : 6427.4349080542515\n",
      "    val_loss       : 6497.841317315491\n",
      "Train Epoch: 162 [256/225000 (0%)] Loss: 6421.666016\n",
      "Train Epoch: 162 [4352/225000 (2%)] Loss: 6329.449219\n",
      "Train Epoch: 162 [8448/225000 (4%)] Loss: 6379.962891\n",
      "Train Epoch: 162 [12544/225000 (6%)] Loss: 6470.445312\n",
      "Train Epoch: 162 [16640/225000 (7%)] Loss: 6438.294922\n",
      "Train Epoch: 162 [20736/225000 (9%)] Loss: 6295.169922\n",
      "Train Epoch: 162 [24832/225000 (11%)] Loss: 6524.162109\n",
      "Train Epoch: 162 [28928/225000 (13%)] Loss: 6416.875000\n",
      "Train Epoch: 162 [33024/225000 (15%)] Loss: 6334.625000\n",
      "Train Epoch: 162 [37120/225000 (16%)] Loss: 6369.158203\n",
      "Train Epoch: 162 [41216/225000 (18%)] Loss: 6529.513672\n",
      "Train Epoch: 162 [45312/225000 (20%)] Loss: 6363.828125\n",
      "Train Epoch: 162 [49408/225000 (22%)] Loss: 6469.535156\n",
      "Train Epoch: 162 [53504/225000 (24%)] Loss: 6278.611328\n",
      "Train Epoch: 162 [57600/225000 (26%)] Loss: 6424.726562\n",
      "Train Epoch: 162 [61696/225000 (27%)] Loss: 6273.300781\n",
      "Train Epoch: 162 [65792/225000 (29%)] Loss: 6413.587891\n",
      "Train Epoch: 162 [69888/225000 (31%)] Loss: 6412.263672\n",
      "Train Epoch: 162 [73984/225000 (33%)] Loss: 6457.773438\n",
      "Train Epoch: 162 [78080/225000 (35%)] Loss: 6481.656250\n",
      "Train Epoch: 162 [82176/225000 (37%)] Loss: 6353.333984\n",
      "Train Epoch: 162 [86272/225000 (38%)] Loss: 6424.693359\n",
      "Train Epoch: 162 [90368/225000 (40%)] Loss: 6302.208984\n",
      "Train Epoch: 162 [94464/225000 (42%)] Loss: 6250.496094\n",
      "Train Epoch: 162 [98560/225000 (44%)] Loss: 6433.150391\n",
      "Train Epoch: 162 [102656/225000 (46%)] Loss: 6262.646484\n",
      "Train Epoch: 162 [106752/225000 (47%)] Loss: 6280.347656\n",
      "Train Epoch: 162 [110848/225000 (49%)] Loss: 6358.291016\n",
      "Train Epoch: 162 [114944/225000 (51%)] Loss: 6251.292969\n",
      "Train Epoch: 162 [119040/225000 (53%)] Loss: 6347.832031\n",
      "Train Epoch: 162 [123136/225000 (55%)] Loss: 6351.367188\n",
      "Train Epoch: 162 [127232/225000 (57%)] Loss: 6297.566406\n",
      "Train Epoch: 162 [131328/225000 (58%)] Loss: 6404.285156\n",
      "Train Epoch: 162 [135424/225000 (60%)] Loss: 6433.519531\n",
      "Train Epoch: 162 [139520/225000 (62%)] Loss: 6512.054688\n",
      "Train Epoch: 162 [143616/225000 (64%)] Loss: 6267.898438\n",
      "Train Epoch: 162 [147712/225000 (66%)] Loss: 6407.923828\n",
      "Train Epoch: 162 [151808/225000 (67%)] Loss: 6599.937500\n",
      "Train Epoch: 162 [155904/225000 (69%)] Loss: 6377.792969\n",
      "Train Epoch: 162 [160000/225000 (71%)] Loss: 6271.031250\n",
      "Train Epoch: 162 [164096/225000 (73%)] Loss: 6409.603516\n",
      "Train Epoch: 162 [168192/225000 (75%)] Loss: 6415.886719\n",
      "Train Epoch: 162 [172288/225000 (77%)] Loss: 6610.939453\n",
      "Train Epoch: 162 [176384/225000 (78%)] Loss: 6279.417969\n",
      "Train Epoch: 162 [180480/225000 (80%)] Loss: 6305.884766\n",
      "Train Epoch: 162 [184576/225000 (82%)] Loss: 6367.832031\n",
      "Train Epoch: 162 [188672/225000 (84%)] Loss: 6367.123047\n",
      "Train Epoch: 162 [192768/225000 (86%)] Loss: 6319.283203\n",
      "Train Epoch: 162 [196864/225000 (87%)] Loss: 6390.667969\n",
      "Train Epoch: 162 [200960/225000 (89%)] Loss: 6554.078125\n",
      "Train Epoch: 162 [205056/225000 (91%)] Loss: 6334.306641\n",
      "Train Epoch: 162 [209152/225000 (93%)] Loss: 6240.890625\n",
      "Train Epoch: 162 [213248/225000 (95%)] Loss: 6472.193359\n",
      "Train Epoch: 162 [217344/225000 (97%)] Loss: 6389.275391\n",
      "Train Epoch: 162 [221440/225000 (98%)] Loss: 6344.234375\n",
      "    epoch          : 162\n",
      "    loss           : 6411.0620500479945\n",
      "    val_loss       : 6447.766189649397\n",
      "Train Epoch: 163 [256/225000 (0%)] Loss: 6393.816406\n",
      "Train Epoch: 163 [4352/225000 (2%)] Loss: 6470.191406\n",
      "Train Epoch: 163 [8448/225000 (4%)] Loss: 6334.150391\n",
      "Train Epoch: 163 [12544/225000 (6%)] Loss: 6538.162109\n",
      "Train Epoch: 163 [16640/225000 (7%)] Loss: 6327.455078\n",
      "Train Epoch: 163 [20736/225000 (9%)] Loss: 6392.179688\n",
      "Train Epoch: 163 [24832/225000 (11%)] Loss: 6306.935547\n",
      "Train Epoch: 163 [28928/225000 (13%)] Loss: 6393.082031\n",
      "Train Epoch: 163 [33024/225000 (15%)] Loss: 6274.591797\n",
      "Train Epoch: 163 [37120/225000 (16%)] Loss: 6351.958984\n",
      "Train Epoch: 163 [41216/225000 (18%)] Loss: 6310.835938\n",
      "Train Epoch: 163 [45312/225000 (20%)] Loss: 6350.560547\n",
      "Train Epoch: 163 [49408/225000 (22%)] Loss: 6299.574219\n",
      "Train Epoch: 163 [53504/225000 (24%)] Loss: 6510.884766\n",
      "Train Epoch: 163 [57600/225000 (26%)] Loss: 6326.615234\n",
      "Train Epoch: 163 [61696/225000 (27%)] Loss: 6295.724609\n",
      "Train Epoch: 163 [65792/225000 (29%)] Loss: 6393.939453\n",
      "Train Epoch: 163 [69888/225000 (31%)] Loss: 6337.873047\n",
      "Train Epoch: 163 [73984/225000 (33%)] Loss: 6488.734375\n",
      "Train Epoch: 163 [78080/225000 (35%)] Loss: 6375.951172\n",
      "Train Epoch: 163 [82176/225000 (37%)] Loss: 6425.044922\n",
      "Train Epoch: 163 [86272/225000 (38%)] Loss: 6310.669922\n",
      "Train Epoch: 163 [90368/225000 (40%)] Loss: 6343.251953\n",
      "Train Epoch: 163 [94464/225000 (42%)] Loss: 6382.970703\n",
      "Train Epoch: 163 [98560/225000 (44%)] Loss: 6372.902344\n",
      "Train Epoch: 163 [102656/225000 (46%)] Loss: 6250.380859\n",
      "Train Epoch: 163 [106752/225000 (47%)] Loss: 6437.365234\n",
      "Train Epoch: 163 [110848/225000 (49%)] Loss: 6349.800781\n",
      "Train Epoch: 163 [114944/225000 (51%)] Loss: 6364.677734\n",
      "Train Epoch: 163 [119040/225000 (53%)] Loss: 6330.433594\n",
      "Train Epoch: 163 [123136/225000 (55%)] Loss: 6338.638672\n",
      "Train Epoch: 163 [127232/225000 (57%)] Loss: 6317.513672\n",
      "Train Epoch: 163 [131328/225000 (58%)] Loss: 6282.207031\n",
      "Train Epoch: 163 [135424/225000 (60%)] Loss: 6462.662109\n",
      "Train Epoch: 163 [139520/225000 (62%)] Loss: 6359.980469\n",
      "Train Epoch: 163 [143616/225000 (64%)] Loss: 6334.625000\n",
      "Train Epoch: 163 [147712/225000 (66%)] Loss: 6362.601562\n",
      "Train Epoch: 163 [151808/225000 (67%)] Loss: 6510.019531\n",
      "Train Epoch: 163 [155904/225000 (69%)] Loss: 6505.279297\n",
      "Train Epoch: 163 [160000/225000 (71%)] Loss: 6522.226562\n",
      "Train Epoch: 163 [164096/225000 (73%)] Loss: 6327.236328\n",
      "Train Epoch: 163 [168192/225000 (75%)] Loss: 6431.386719\n",
      "Train Epoch: 163 [172288/225000 (77%)] Loss: 6335.455078\n",
      "Train Epoch: 163 [176384/225000 (78%)] Loss: 6267.195312\n",
      "Train Epoch: 163 [180480/225000 (80%)] Loss: 6337.875000\n",
      "Train Epoch: 163 [184576/225000 (82%)] Loss: 6417.759766\n",
      "Train Epoch: 163 [188672/225000 (84%)] Loss: 6430.822266\n",
      "Train Epoch: 163 [192768/225000 (86%)] Loss: 6362.255859\n",
      "Train Epoch: 163 [196864/225000 (87%)] Loss: 6324.050781\n",
      "Train Epoch: 163 [200960/225000 (89%)] Loss: 6461.185547\n",
      "Train Epoch: 163 [205056/225000 (91%)] Loss: 6301.193359\n",
      "Train Epoch: 163 [209152/225000 (93%)] Loss: 6302.251953\n",
      "Train Epoch: 163 [213248/225000 (95%)] Loss: 6336.218750\n",
      "Train Epoch: 163 [217344/225000 (97%)] Loss: 6339.164062\n",
      "Train Epoch: 163 [221440/225000 (98%)] Loss: 6318.048828\n",
      "    epoch          : 163\n",
      "    loss           : 6426.884145691126\n",
      "    val_loss       : 6564.6911197955515\n",
      "Train Epoch: 164 [256/225000 (0%)] Loss: 6473.707031\n",
      "Train Epoch: 164 [4352/225000 (2%)] Loss: 6516.126953\n",
      "Train Epoch: 164 [8448/225000 (4%)] Loss: 6452.349609\n",
      "Train Epoch: 164 [12544/225000 (6%)] Loss: 6358.458984\n",
      "Train Epoch: 164 [16640/225000 (7%)] Loss: 6388.351562\n",
      "Train Epoch: 164 [20736/225000 (9%)] Loss: 6408.328125\n",
      "Train Epoch: 164 [24832/225000 (11%)] Loss: 6336.906250\n",
      "Train Epoch: 164 [28928/225000 (13%)] Loss: 6487.298828\n",
      "Train Epoch: 164 [33024/225000 (15%)] Loss: 6463.416016\n",
      "Train Epoch: 164 [37120/225000 (16%)] Loss: 6399.992188\n",
      "Train Epoch: 164 [41216/225000 (18%)] Loss: 6309.236328\n",
      "Train Epoch: 164 [45312/225000 (20%)] Loss: 6257.279297\n",
      "Train Epoch: 164 [49408/225000 (22%)] Loss: 6368.576172\n",
      "Train Epoch: 164 [53504/225000 (24%)] Loss: 6302.363281\n",
      "Train Epoch: 164 [57600/225000 (26%)] Loss: 6417.177734\n",
      "Train Epoch: 164 [61696/225000 (27%)] Loss: 6250.876953\n",
      "Train Epoch: 164 [65792/225000 (29%)] Loss: 6503.365234\n",
      "Train Epoch: 164 [69888/225000 (31%)] Loss: 6330.531250\n",
      "Train Epoch: 164 [73984/225000 (33%)] Loss: 6356.230469\n",
      "Train Epoch: 164 [78080/225000 (35%)] Loss: 6409.101562\n",
      "Train Epoch: 164 [82176/225000 (37%)] Loss: 6225.123047\n",
      "Train Epoch: 164 [86272/225000 (38%)] Loss: 6406.058594\n",
      "Train Epoch: 164 [90368/225000 (40%)] Loss: 6362.585938\n",
      "Train Epoch: 164 [94464/225000 (42%)] Loss: 6318.191406\n",
      "Train Epoch: 164 [98560/225000 (44%)] Loss: 6269.275391\n",
      "Train Epoch: 164 [102656/225000 (46%)] Loss: 6452.406250\n",
      "Train Epoch: 164 [106752/225000 (47%)] Loss: 6407.541016\n",
      "Train Epoch: 164 [110848/225000 (49%)] Loss: 6408.841797\n",
      "Train Epoch: 164 [114944/225000 (51%)] Loss: 6457.513672\n",
      "Train Epoch: 164 [119040/225000 (53%)] Loss: 6423.753906\n",
      "Train Epoch: 164 [123136/225000 (55%)] Loss: 6306.945312\n",
      "Train Epoch: 164 [127232/225000 (57%)] Loss: 6370.988281\n",
      "Train Epoch: 164 [131328/225000 (58%)] Loss: 6353.208984\n",
      "Train Epoch: 164 [135424/225000 (60%)] Loss: 6463.982422\n",
      "Train Epoch: 164 [139520/225000 (62%)] Loss: 6476.220703\n",
      "Train Epoch: 164 [143616/225000 (64%)] Loss: 6452.607422\n",
      "Train Epoch: 164 [147712/225000 (66%)] Loss: 6230.257812\n",
      "Train Epoch: 164 [151808/225000 (67%)] Loss: 6351.144531\n",
      "Train Epoch: 164 [155904/225000 (69%)] Loss: 6341.001953\n",
      "Train Epoch: 164 [160000/225000 (71%)] Loss: 6451.109375\n",
      "Train Epoch: 164 [164096/225000 (73%)] Loss: 6303.091797\n",
      "Train Epoch: 164 [168192/225000 (75%)] Loss: 6325.406250\n",
      "Train Epoch: 164 [172288/225000 (77%)] Loss: 6411.835938\n",
      "Train Epoch: 164 [176384/225000 (78%)] Loss: 6432.164062\n",
      "Train Epoch: 164 [180480/225000 (80%)] Loss: 6612.642578\n",
      "Train Epoch: 164 [184576/225000 (82%)] Loss: 6407.111328\n",
      "Train Epoch: 164 [188672/225000 (84%)] Loss: 6354.554688\n",
      "Train Epoch: 164 [192768/225000 (86%)] Loss: 6340.320312\n",
      "Train Epoch: 164 [196864/225000 (87%)] Loss: 6440.707031\n",
      "Train Epoch: 164 [200960/225000 (89%)] Loss: 6310.568359\n",
      "Train Epoch: 164 [205056/225000 (91%)] Loss: 6387.628906\n",
      "Train Epoch: 164 [209152/225000 (93%)] Loss: 6434.960938\n",
      "Train Epoch: 164 [213248/225000 (95%)] Loss: 6439.394531\n",
      "Train Epoch: 164 [217344/225000 (97%)] Loss: 6469.371094\n",
      "Train Epoch: 164 [221440/225000 (98%)] Loss: 6314.771484\n",
      "    epoch          : 164\n",
      "    loss           : 6390.056100682594\n",
      "    val_loss       : 6495.375987724382\n",
      "Train Epoch: 165 [256/225000 (0%)] Loss: 6285.734375\n",
      "Train Epoch: 165 [4352/225000 (2%)] Loss: 6311.345703\n",
      "Train Epoch: 165 [8448/225000 (4%)] Loss: 6475.839844\n",
      "Train Epoch: 165 [12544/225000 (6%)] Loss: 6304.125000\n",
      "Train Epoch: 165 [16640/225000 (7%)] Loss: 6260.746094\n",
      "Train Epoch: 165 [20736/225000 (9%)] Loss: 6332.464844\n",
      "Train Epoch: 165 [24832/225000 (11%)] Loss: 6435.740234\n",
      "Train Epoch: 165 [28928/225000 (13%)] Loss: 6321.830078\n",
      "Train Epoch: 165 [33024/225000 (15%)] Loss: 6412.466797\n",
      "Train Epoch: 165 [37120/225000 (16%)] Loss: 6386.244141\n",
      "Train Epoch: 165 [41216/225000 (18%)] Loss: 6446.423828\n",
      "Train Epoch: 165 [45312/225000 (20%)] Loss: 6358.667969\n",
      "Train Epoch: 165 [49408/225000 (22%)] Loss: 6320.226562\n",
      "Train Epoch: 165 [53504/225000 (24%)] Loss: 6366.285156\n",
      "Train Epoch: 165 [57600/225000 (26%)] Loss: 6451.404297\n",
      "Train Epoch: 165 [61696/225000 (27%)] Loss: 6433.910156\n",
      "Train Epoch: 165 [65792/225000 (29%)] Loss: 6238.326172\n",
      "Train Epoch: 165 [69888/225000 (31%)] Loss: 6357.970703\n",
      "Train Epoch: 165 [73984/225000 (33%)] Loss: 6234.847656\n",
      "Train Epoch: 165 [78080/225000 (35%)] Loss: 6346.648438\n",
      "Train Epoch: 165 [82176/225000 (37%)] Loss: 6265.712891\n",
      "Train Epoch: 165 [86272/225000 (38%)] Loss: 6351.242188\n",
      "Train Epoch: 165 [90368/225000 (40%)] Loss: 6328.501953\n",
      "Train Epoch: 165 [94464/225000 (42%)] Loss: 6359.402344\n",
      "Train Epoch: 165 [98560/225000 (44%)] Loss: 6265.281250\n",
      "Train Epoch: 165 [102656/225000 (46%)] Loss: 6369.552734\n",
      "Train Epoch: 165 [106752/225000 (47%)] Loss: 6346.703125\n",
      "Train Epoch: 165 [110848/225000 (49%)] Loss: 6442.996094\n",
      "Train Epoch: 165 [114944/225000 (51%)] Loss: 6426.949219\n",
      "Train Epoch: 165 [119040/225000 (53%)] Loss: 6362.902344\n",
      "Train Epoch: 165 [123136/225000 (55%)] Loss: 6303.732422\n",
      "Train Epoch: 165 [127232/225000 (57%)] Loss: 6355.843750\n",
      "Train Epoch: 165 [131328/225000 (58%)] Loss: 6578.308594\n",
      "Train Epoch: 165 [135424/225000 (60%)] Loss: 6474.271484\n",
      "Train Epoch: 165 [139520/225000 (62%)] Loss: 6478.162109\n",
      "Train Epoch: 165 [143616/225000 (64%)] Loss: 6475.650391\n",
      "Train Epoch: 165 [147712/225000 (66%)] Loss: 6331.449219\n",
      "Train Epoch: 165 [151808/225000 (67%)] Loss: 6397.083984\n",
      "Train Epoch: 165 [155904/225000 (69%)] Loss: 6425.611328\n",
      "Train Epoch: 165 [160000/225000 (71%)] Loss: 6313.070312\n",
      "Train Epoch: 165 [164096/225000 (73%)] Loss: 6346.082031\n",
      "Train Epoch: 165 [168192/225000 (75%)] Loss: 6226.111328\n",
      "Train Epoch: 165 [172288/225000 (77%)] Loss: 6319.994141\n",
      "Train Epoch: 165 [176384/225000 (78%)] Loss: 6331.494141\n",
      "Train Epoch: 165 [180480/225000 (80%)] Loss: 6351.046875\n",
      "Train Epoch: 165 [184576/225000 (82%)] Loss: 6484.388672\n",
      "Train Epoch: 165 [188672/225000 (84%)] Loss: 6319.687500\n",
      "Train Epoch: 165 [192768/225000 (86%)] Loss: 6310.080078\n",
      "Train Epoch: 165 [196864/225000 (87%)] Loss: 6360.240234\n",
      "Train Epoch: 165 [200960/225000 (89%)] Loss: 6373.039062\n",
      "Train Epoch: 165 [205056/225000 (91%)] Loss: 6250.593750\n",
      "Train Epoch: 165 [209152/225000 (93%)] Loss: 6391.091797\n",
      "Train Epoch: 165 [213248/225000 (95%)] Loss: 6198.638672\n",
      "Train Epoch: 165 [217344/225000 (97%)] Loss: 6295.378906\n",
      "Train Epoch: 165 [221440/225000 (98%)] Loss: 6282.701172\n",
      "    epoch          : 165\n",
      "    loss           : 6428.428529845705\n",
      "    val_loss       : 6636.213254884798\n",
      "Train Epoch: 166 [256/225000 (0%)] Loss: 6410.597656\n",
      "Train Epoch: 166 [4352/225000 (2%)] Loss: 6387.449219\n",
      "Train Epoch: 166 [8448/225000 (4%)] Loss: 6289.687500\n",
      "Train Epoch: 166 [12544/225000 (6%)] Loss: 6362.773438\n",
      "Train Epoch: 166 [16640/225000 (7%)] Loss: 6437.501953\n",
      "Train Epoch: 166 [20736/225000 (9%)] Loss: 6365.019531\n",
      "Train Epoch: 166 [24832/225000 (11%)] Loss: 6275.472656\n",
      "Train Epoch: 166 [28928/225000 (13%)] Loss: 6372.648438\n",
      "Train Epoch: 166 [33024/225000 (15%)] Loss: 6496.003906\n",
      "Train Epoch: 166 [37120/225000 (16%)] Loss: 6391.955078\n",
      "Train Epoch: 166 [41216/225000 (18%)] Loss: 6391.070312\n",
      "Train Epoch: 166 [45312/225000 (20%)] Loss: 6319.656250\n",
      "Train Epoch: 166 [49408/225000 (22%)] Loss: 6353.414062\n",
      "Train Epoch: 166 [53504/225000 (24%)] Loss: 6361.457031\n",
      "Train Epoch: 166 [57600/225000 (26%)] Loss: 6359.031250\n",
      "Train Epoch: 166 [61696/225000 (27%)] Loss: 6176.230469\n",
      "Train Epoch: 166 [65792/225000 (29%)] Loss: 6422.367188\n",
      "Train Epoch: 166 [69888/225000 (31%)] Loss: 6363.597656\n",
      "Train Epoch: 166 [73984/225000 (33%)] Loss: 6351.945312\n",
      "Train Epoch: 166 [78080/225000 (35%)] Loss: 6371.435547\n",
      "Train Epoch: 166 [82176/225000 (37%)] Loss: 6358.275391\n",
      "Train Epoch: 166 [86272/225000 (38%)] Loss: 6272.882812\n",
      "Train Epoch: 166 [90368/225000 (40%)] Loss: 6392.246094\n",
      "Train Epoch: 166 [94464/225000 (42%)] Loss: 6359.939453\n",
      "Train Epoch: 166 [98560/225000 (44%)] Loss: 6293.636719\n",
      "Train Epoch: 166 [102656/225000 (46%)] Loss: 6276.580078\n",
      "Train Epoch: 166 [106752/225000 (47%)] Loss: 6274.501953\n",
      "Train Epoch: 166 [110848/225000 (49%)] Loss: 6293.908203\n",
      "Train Epoch: 166 [114944/225000 (51%)] Loss: 6281.722656\n",
      "Train Epoch: 166 [119040/225000 (53%)] Loss: 6286.796875\n",
      "Train Epoch: 166 [123136/225000 (55%)] Loss: 6302.984375\n",
      "Train Epoch: 166 [127232/225000 (57%)] Loss: 6303.000000\n",
      "Train Epoch: 166 [131328/225000 (58%)] Loss: 6388.126953\n",
      "Train Epoch: 166 [135424/225000 (60%)] Loss: 6305.332031\n",
      "Train Epoch: 166 [139520/225000 (62%)] Loss: 6402.443359\n",
      "Train Epoch: 166 [143616/225000 (64%)] Loss: 6360.501953\n",
      "Train Epoch: 166 [147712/225000 (66%)] Loss: 6310.435547\n",
      "Train Epoch: 166 [151808/225000 (67%)] Loss: 6365.255859\n",
      "Train Epoch: 166 [155904/225000 (69%)] Loss: 6369.175781\n",
      "Train Epoch: 166 [160000/225000 (71%)] Loss: 6347.250000\n",
      "Train Epoch: 166 [164096/225000 (73%)] Loss: 6367.826172\n",
      "Train Epoch: 166 [168192/225000 (75%)] Loss: 6399.986328\n",
      "Train Epoch: 166 [172288/225000 (77%)] Loss: 6230.240234\n",
      "Train Epoch: 166 [176384/225000 (78%)] Loss: 6493.605469\n",
      "Train Epoch: 166 [180480/225000 (80%)] Loss: 6404.998047\n",
      "Train Epoch: 166 [184576/225000 (82%)] Loss: 6334.070312\n",
      "Train Epoch: 166 [188672/225000 (84%)] Loss: 6440.767578\n",
      "Train Epoch: 166 [192768/225000 (86%)] Loss: 6464.113281\n",
      "Train Epoch: 166 [196864/225000 (87%)] Loss: 6194.462891\n",
      "Train Epoch: 166 [200960/225000 (89%)] Loss: 6273.533203\n",
      "Train Epoch: 166 [205056/225000 (91%)] Loss: 8245.933594\n",
      "Train Epoch: 166 [209152/225000 (93%)] Loss: 6261.902344\n",
      "Train Epoch: 166 [213248/225000 (95%)] Loss: 6446.025391\n",
      "Train Epoch: 166 [217344/225000 (97%)] Loss: 6486.662109\n",
      "Train Epoch: 166 [221440/225000 (98%)] Loss: 6277.345703\n",
      "    epoch          : 166\n",
      "    loss           : 6423.013782974261\n",
      "    val_loss       : 6651.44719599826\n",
      "Train Epoch: 167 [256/225000 (0%)] Loss: 6465.781250\n",
      "Train Epoch: 167 [4352/225000 (2%)] Loss: 6286.697266\n",
      "Train Epoch: 167 [8448/225000 (4%)] Loss: 6425.005859\n",
      "Train Epoch: 167 [12544/225000 (6%)] Loss: 6322.869141\n",
      "Train Epoch: 167 [16640/225000 (7%)] Loss: 6288.162109\n",
      "Train Epoch: 167 [20736/225000 (9%)] Loss: 6445.027344\n",
      "Train Epoch: 167 [24832/225000 (11%)] Loss: 6275.269531\n",
      "Train Epoch: 167 [28928/225000 (13%)] Loss: 6436.794922\n",
      "Train Epoch: 167 [33024/225000 (15%)] Loss: 6425.730469\n",
      "Train Epoch: 167 [37120/225000 (16%)] Loss: 6304.931641\n",
      "Train Epoch: 167 [41216/225000 (18%)] Loss: 6284.580078\n",
      "Train Epoch: 167 [45312/225000 (20%)] Loss: 6274.031250\n",
      "Train Epoch: 167 [49408/225000 (22%)] Loss: 6313.882812\n",
      "Train Epoch: 167 [53504/225000 (24%)] Loss: 6375.386719\n",
      "Train Epoch: 167 [57600/225000 (26%)] Loss: 6304.021484\n",
      "Train Epoch: 167 [61696/225000 (27%)] Loss: 6222.845703\n",
      "Train Epoch: 167 [65792/225000 (29%)] Loss: 6174.976562\n",
      "Train Epoch: 167 [69888/225000 (31%)] Loss: 6480.390625\n",
      "Train Epoch: 167 [73984/225000 (33%)] Loss: 6461.257812\n",
      "Train Epoch: 167 [78080/225000 (35%)] Loss: 6329.230469\n",
      "Train Epoch: 167 [82176/225000 (37%)] Loss: 6427.337891\n",
      "Train Epoch: 167 [86272/225000 (38%)] Loss: 6367.667969\n",
      "Train Epoch: 167 [90368/225000 (40%)] Loss: 6495.466797\n",
      "Train Epoch: 167 [94464/225000 (42%)] Loss: 6335.765625\n",
      "Train Epoch: 167 [98560/225000 (44%)] Loss: 6488.228516\n",
      "Train Epoch: 167 [102656/225000 (46%)] Loss: 6370.656250\n",
      "Train Epoch: 167 [106752/225000 (47%)] Loss: 6333.994141\n",
      "Train Epoch: 167 [110848/225000 (49%)] Loss: 6479.277344\n",
      "Train Epoch: 167 [114944/225000 (51%)] Loss: 6312.728516\n",
      "Train Epoch: 167 [119040/225000 (53%)] Loss: 6470.390625\n",
      "Train Epoch: 167 [123136/225000 (55%)] Loss: 6495.101562\n",
      "Train Epoch: 167 [127232/225000 (57%)] Loss: 6202.615234\n",
      "Train Epoch: 167 [131328/225000 (58%)] Loss: 6513.160156\n",
      "Train Epoch: 167 [135424/225000 (60%)] Loss: 6336.435547\n",
      "Train Epoch: 167 [139520/225000 (62%)] Loss: 6465.832031\n",
      "Train Epoch: 167 [143616/225000 (64%)] Loss: 6413.681641\n",
      "Train Epoch: 167 [147712/225000 (66%)] Loss: 6314.937500\n",
      "Train Epoch: 167 [151808/225000 (67%)] Loss: 6319.888672\n",
      "Train Epoch: 167 [155904/225000 (69%)] Loss: 6431.771484\n",
      "Train Epoch: 167 [160000/225000 (71%)] Loss: 6438.689453\n",
      "Train Epoch: 167 [164096/225000 (73%)] Loss: 6343.244141\n",
      "Train Epoch: 167 [168192/225000 (75%)] Loss: 6294.199219\n",
      "Train Epoch: 167 [172288/225000 (77%)] Loss: 6394.574219\n",
      "Train Epoch: 167 [176384/225000 (78%)] Loss: 6428.037109\n",
      "Train Epoch: 167 [180480/225000 (80%)] Loss: 6332.505859\n",
      "Train Epoch: 167 [184576/225000 (82%)] Loss: 6391.921875\n",
      "Train Epoch: 167 [188672/225000 (84%)] Loss: 6449.083984\n",
      "Train Epoch: 167 [192768/225000 (86%)] Loss: 6503.990234\n",
      "Train Epoch: 167 [196864/225000 (87%)] Loss: 6430.654297\n",
      "Train Epoch: 167 [200960/225000 (89%)] Loss: 6379.652344\n",
      "Train Epoch: 167 [205056/225000 (91%)] Loss: 6404.140625\n",
      "Train Epoch: 167 [209152/225000 (93%)] Loss: 6504.203125\n",
      "Train Epoch: 167 [213248/225000 (95%)] Loss: 6352.367188\n",
      "Train Epoch: 167 [217344/225000 (97%)] Loss: 6460.816406\n",
      "Train Epoch: 167 [221440/225000 (98%)] Loss: 6383.875000\n",
      "    epoch          : 167\n",
      "    loss           : 6406.7802856584185\n",
      "    val_loss       : 6530.979909069684\n",
      "Train Epoch: 168 [256/225000 (0%)] Loss: 6320.261719\n",
      "Train Epoch: 168 [4352/225000 (2%)] Loss: 6372.759766\n",
      "Train Epoch: 168 [8448/225000 (4%)] Loss: 6432.816406\n",
      "Train Epoch: 168 [12544/225000 (6%)] Loss: 6311.224609\n",
      "Train Epoch: 168 [16640/225000 (7%)] Loss: 6430.111328\n",
      "Train Epoch: 168 [20736/225000 (9%)] Loss: 6417.904297\n",
      "Train Epoch: 168 [24832/225000 (11%)] Loss: 6307.294922\n",
      "Train Epoch: 168 [28928/225000 (13%)] Loss: 6247.910156\n",
      "Train Epoch: 168 [33024/225000 (15%)] Loss: 6313.609375\n",
      "Train Epoch: 168 [37120/225000 (16%)] Loss: 6218.107422\n",
      "Train Epoch: 168 [41216/225000 (18%)] Loss: 6340.492188\n",
      "Train Epoch: 168 [45312/225000 (20%)] Loss: 6321.453125\n",
      "Train Epoch: 168 [49408/225000 (22%)] Loss: 6407.093750\n",
      "Train Epoch: 168 [53504/225000 (24%)] Loss: 6396.960938\n",
      "Train Epoch: 168 [57600/225000 (26%)] Loss: 6296.880859\n",
      "Train Epoch: 168 [61696/225000 (27%)] Loss: 6305.820312\n",
      "Train Epoch: 168 [65792/225000 (29%)] Loss: 7958.410156\n",
      "Train Epoch: 168 [69888/225000 (31%)] Loss: 6376.410156\n",
      "Train Epoch: 168 [73984/225000 (33%)] Loss: 6432.955078\n",
      "Train Epoch: 168 [78080/225000 (35%)] Loss: 6357.414062\n",
      "Train Epoch: 168 [82176/225000 (37%)] Loss: 6369.351562\n",
      "Train Epoch: 168 [86272/225000 (38%)] Loss: 6493.300781\n",
      "Train Epoch: 168 [90368/225000 (40%)] Loss: 6370.041016\n",
      "Train Epoch: 168 [94464/225000 (42%)] Loss: 6340.291016\n",
      "Train Epoch: 168 [98560/225000 (44%)] Loss: 6371.398438\n",
      "Train Epoch: 168 [102656/225000 (46%)] Loss: 6412.216797\n",
      "Train Epoch: 168 [106752/225000 (47%)] Loss: 6311.990234\n",
      "Train Epoch: 168 [110848/225000 (49%)] Loss: 6346.589844\n",
      "Train Epoch: 168 [114944/225000 (51%)] Loss: 6295.572266\n",
      "Train Epoch: 168 [119040/225000 (53%)] Loss: 6264.533203\n",
      "Train Epoch: 168 [123136/225000 (55%)] Loss: 6286.113281\n",
      "Train Epoch: 168 [127232/225000 (57%)] Loss: 6307.927734\n",
      "Train Epoch: 168 [131328/225000 (58%)] Loss: 6315.722656\n",
      "Train Epoch: 168 [135424/225000 (60%)] Loss: 6436.267578\n",
      "Train Epoch: 168 [139520/225000 (62%)] Loss: 6289.109375\n",
      "Train Epoch: 168 [143616/225000 (64%)] Loss: 6258.599609\n",
      "Train Epoch: 168 [147712/225000 (66%)] Loss: 6423.091797\n",
      "Train Epoch: 168 [151808/225000 (67%)] Loss: 6362.166016\n",
      "Train Epoch: 168 [155904/225000 (69%)] Loss: 6331.308594\n",
      "Train Epoch: 168 [160000/225000 (71%)] Loss: 6373.761719\n",
      "Train Epoch: 168 [164096/225000 (73%)] Loss: 6354.214844\n",
      "Train Epoch: 168 [168192/225000 (75%)] Loss: 6397.949219\n",
      "Train Epoch: 168 [172288/225000 (77%)] Loss: 6349.832031\n",
      "Train Epoch: 168 [176384/225000 (78%)] Loss: 6345.427734\n",
      "Train Epoch: 168 [180480/225000 (80%)] Loss: 6324.121094\n",
      "Train Epoch: 168 [184576/225000 (82%)] Loss: 6350.984375\n",
      "Train Epoch: 168 [188672/225000 (84%)] Loss: 6511.775391\n",
      "Train Epoch: 168 [192768/225000 (86%)] Loss: 6221.804688\n",
      "Train Epoch: 168 [196864/225000 (87%)] Loss: 6448.894531\n",
      "Train Epoch: 168 [200960/225000 (89%)] Loss: 6427.654297\n",
      "Train Epoch: 168 [205056/225000 (91%)] Loss: 6339.894531\n",
      "Train Epoch: 168 [209152/225000 (93%)] Loss: 6356.267578\n",
      "Train Epoch: 168 [213248/225000 (95%)] Loss: 6381.035156\n",
      "Train Epoch: 168 [217344/225000 (97%)] Loss: 6317.595703\n",
      "Train Epoch: 168 [221440/225000 (98%)] Loss: 6436.574219\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "    epoch          : 168\n",
      "    loss           : 6409.77950574161\n",
      "    val_loss       : 6487.408951233844\n",
      "Train Epoch: 169 [256/225000 (0%)] Loss: 6305.236328\n",
      "Train Epoch: 169 [4352/225000 (2%)] Loss: 6397.113281\n",
      "Train Epoch: 169 [8448/225000 (4%)] Loss: 6212.156250\n",
      "Train Epoch: 169 [12544/225000 (6%)] Loss: 6331.640625\n",
      "Train Epoch: 169 [16640/225000 (7%)] Loss: 6322.541016\n",
      "Train Epoch: 169 [20736/225000 (9%)] Loss: 6272.408203\n",
      "Train Epoch: 169 [24832/225000 (11%)] Loss: 6383.003906\n",
      "Train Epoch: 169 [28928/225000 (13%)] Loss: 6339.857422\n",
      "Train Epoch: 169 [33024/225000 (15%)] Loss: 6370.767578\n",
      "Train Epoch: 169 [37120/225000 (16%)] Loss: 6309.123047\n",
      "Train Epoch: 169 [41216/225000 (18%)] Loss: 6323.683594\n",
      "Train Epoch: 169 [45312/225000 (20%)] Loss: 6426.876953\n",
      "Train Epoch: 169 [49408/225000 (22%)] Loss: 6337.556641\n",
      "Train Epoch: 169 [53504/225000 (24%)] Loss: 6375.294922\n",
      "Train Epoch: 169 [57600/225000 (26%)] Loss: 6354.652344\n",
      "Train Epoch: 169 [61696/225000 (27%)] Loss: 6335.660156\n",
      "Train Epoch: 169 [65792/225000 (29%)] Loss: 6335.638672\n",
      "Train Epoch: 169 [69888/225000 (31%)] Loss: 6334.560547\n",
      "Train Epoch: 169 [73984/225000 (33%)] Loss: 6342.064453\n",
      "Train Epoch: 169 [78080/225000 (35%)] Loss: 6262.583984\n",
      "Train Epoch: 169 [82176/225000 (37%)] Loss: 6351.943359\n",
      "Train Epoch: 169 [86272/225000 (38%)] Loss: 6361.019531\n",
      "Train Epoch: 169 [90368/225000 (40%)] Loss: 6295.968750\n",
      "Train Epoch: 169 [94464/225000 (42%)] Loss: 6343.533203\n",
      "Train Epoch: 169 [98560/225000 (44%)] Loss: 6269.875000\n",
      "Train Epoch: 169 [102656/225000 (46%)] Loss: 6318.417969\n",
      "Train Epoch: 169 [106752/225000 (47%)] Loss: 6319.847656\n",
      "Train Epoch: 169 [110848/225000 (49%)] Loss: 6321.531250\n",
      "Train Epoch: 169 [114944/225000 (51%)] Loss: 6425.480469\n",
      "Train Epoch: 169 [119040/225000 (53%)] Loss: 6345.677734\n",
      "Train Epoch: 169 [123136/225000 (55%)] Loss: 6171.246094\n",
      "Train Epoch: 169 [127232/225000 (57%)] Loss: 6302.255859\n",
      "Train Epoch: 169 [131328/225000 (58%)] Loss: 6260.148438\n",
      "Train Epoch: 169 [135424/225000 (60%)] Loss: 6333.183594\n",
      "Train Epoch: 169 [139520/225000 (62%)] Loss: 6465.611328\n",
      "Train Epoch: 169 [143616/225000 (64%)] Loss: 6228.072266\n",
      "Train Epoch: 169 [147712/225000 (66%)] Loss: 6241.576172\n",
      "Train Epoch: 169 [151808/225000 (67%)] Loss: 6391.281250\n",
      "Train Epoch: 169 [155904/225000 (69%)] Loss: 6332.134766\n",
      "Train Epoch: 169 [160000/225000 (71%)] Loss: 6270.740234\n",
      "Train Epoch: 169 [164096/225000 (73%)] Loss: 6431.572266\n",
      "Train Epoch: 169 [168192/225000 (75%)] Loss: 6304.171875\n",
      "Train Epoch: 169 [172288/225000 (77%)] Loss: 6296.705078\n",
      "Train Epoch: 169 [176384/225000 (78%)] Loss: 6299.570312\n",
      "Train Epoch: 169 [180480/225000 (80%)] Loss: 6276.582031\n",
      "Train Epoch: 169 [184576/225000 (82%)] Loss: 6301.953125\n",
      "Train Epoch: 169 [188672/225000 (84%)] Loss: 6454.578125\n",
      "Train Epoch: 169 [192768/225000 (86%)] Loss: 6309.566406\n",
      "Train Epoch: 169 [196864/225000 (87%)] Loss: 8149.228516\n",
      "Train Epoch: 169 [200960/225000 (89%)] Loss: 6305.437500\n",
      "Train Epoch: 169 [205056/225000 (91%)] Loss: 6283.255859\n",
      "Train Epoch: 169 [209152/225000 (93%)] Loss: 6455.605469\n",
      "Train Epoch: 169 [213248/225000 (95%)] Loss: 6383.480469\n",
      "Train Epoch: 169 [217344/225000 (97%)] Loss: 6421.199219\n",
      "Train Epoch: 169 [221440/225000 (98%)] Loss: 6425.416016\n",
      "    epoch          : 169\n",
      "    loss           : 6392.940628555177\n",
      "    val_loss       : 6405.852386643692\n",
      "Train Epoch: 170 [256/225000 (0%)] Loss: 6240.871094\n",
      "Train Epoch: 170 [4352/225000 (2%)] Loss: 6332.343750\n",
      "Train Epoch: 170 [8448/225000 (4%)] Loss: 6417.011719\n",
      "Train Epoch: 170 [12544/225000 (6%)] Loss: 6280.943359\n",
      "Train Epoch: 170 [16640/225000 (7%)] Loss: 6414.769531\n",
      "Train Epoch: 170 [20736/225000 (9%)] Loss: 6412.992188\n",
      "Train Epoch: 170 [24832/225000 (11%)] Loss: 6338.650391\n",
      "Train Epoch: 170 [28928/225000 (13%)] Loss: 6309.996094\n",
      "Train Epoch: 170 [33024/225000 (15%)] Loss: 6315.820312\n",
      "Train Epoch: 170 [37120/225000 (16%)] Loss: 8193.634766\n",
      "Train Epoch: 170 [41216/225000 (18%)] Loss: 6210.882812\n",
      "Train Epoch: 170 [45312/225000 (20%)] Loss: 6307.183594\n",
      "Train Epoch: 170 [49408/225000 (22%)] Loss: 8361.255859\n",
      "Train Epoch: 170 [53504/225000 (24%)] Loss: 6495.392578\n",
      "Train Epoch: 170 [57600/225000 (26%)] Loss: 6419.236328\n",
      "Train Epoch: 170 [61696/225000 (27%)] Loss: 6252.570312\n",
      "Train Epoch: 170 [65792/225000 (29%)] Loss: 6196.302734\n",
      "Train Epoch: 170 [69888/225000 (31%)] Loss: 6445.009766\n",
      "Train Epoch: 170 [73984/225000 (33%)] Loss: 6274.312500\n",
      "Train Epoch: 170 [78080/225000 (35%)] Loss: 6417.863281\n",
      "Train Epoch: 170 [82176/225000 (37%)] Loss: 6389.843750\n",
      "Train Epoch: 170 [86272/225000 (38%)] Loss: 6417.589844\n",
      "Train Epoch: 170 [90368/225000 (40%)] Loss: 6258.478516\n",
      "Train Epoch: 170 [94464/225000 (42%)] Loss: 6410.033203\n",
      "Train Epoch: 170 [98560/225000 (44%)] Loss: 6333.101562\n",
      "Train Epoch: 170 [102656/225000 (46%)] Loss: 6406.654297\n",
      "Train Epoch: 170 [106752/225000 (47%)] Loss: 6261.542969\n",
      "Train Epoch: 170 [110848/225000 (49%)] Loss: 6295.429688\n",
      "Train Epoch: 170 [114944/225000 (51%)] Loss: 6290.833984\n",
      "Train Epoch: 170 [119040/225000 (53%)] Loss: 6265.037109\n",
      "Train Epoch: 170 [123136/225000 (55%)] Loss: 6328.572266\n",
      "Train Epoch: 170 [127232/225000 (57%)] Loss: 6256.869141\n",
      "Train Epoch: 170 [131328/225000 (58%)] Loss: 6268.462891\n",
      "Train Epoch: 170 [135424/225000 (60%)] Loss: 6257.593750\n",
      "Train Epoch: 170 [139520/225000 (62%)] Loss: 6220.611328\n",
      "Train Epoch: 170 [143616/225000 (64%)] Loss: 6415.021484\n",
      "Train Epoch: 170 [147712/225000 (66%)] Loss: 6353.355469\n",
      "Train Epoch: 170 [151808/225000 (67%)] Loss: 6296.787109\n",
      "Train Epoch: 170 [155904/225000 (69%)] Loss: 6289.642578\n",
      "Train Epoch: 170 [160000/225000 (71%)] Loss: 6279.646484\n",
      "Train Epoch: 170 [164096/225000 (73%)] Loss: 6450.236328\n",
      "Train Epoch: 170 [168192/225000 (75%)] Loss: 6360.435547\n",
      "Train Epoch: 170 [172288/225000 (77%)] Loss: 6357.824219\n",
      "Train Epoch: 170 [176384/225000 (78%)] Loss: 6435.207031\n",
      "Train Epoch: 170 [180480/225000 (80%)] Loss: 6405.724609\n",
      "Train Epoch: 170 [184576/225000 (82%)] Loss: 6453.847656\n",
      "Train Epoch: 170 [188672/225000 (84%)] Loss: 6248.431641\n",
      "Train Epoch: 170 [192768/225000 (86%)] Loss: 6368.589844\n",
      "Train Epoch: 170 [196864/225000 (87%)] Loss: 6341.320312\n",
      "Train Epoch: 170 [200960/225000 (89%)] Loss: 6292.416016\n",
      "Train Epoch: 170 [205056/225000 (91%)] Loss: 6333.080078\n",
      "Train Epoch: 170 [209152/225000 (93%)] Loss: 6416.156250\n",
      "Train Epoch: 170 [213248/225000 (95%)] Loss: 6284.025391\n",
      "Train Epoch: 170 [217344/225000 (97%)] Loss: 6324.578125\n",
      "Train Epoch: 170 [221440/225000 (98%)] Loss: 6268.304688\n",
      "    epoch          : 170\n",
      "    loss           : 6414.272626475398\n",
      "    val_loss       : 6441.054163808725\n",
      "Train Epoch: 171 [256/225000 (0%)] Loss: 6332.244141\n",
      "Train Epoch: 171 [4352/225000 (2%)] Loss: 6325.892578\n",
      "Train Epoch: 171 [8448/225000 (4%)] Loss: 6320.597656\n",
      "Train Epoch: 171 [12544/225000 (6%)] Loss: 6258.458984\n",
      "Train Epoch: 171 [16640/225000 (7%)] Loss: 6353.580078\n",
      "Train Epoch: 171 [20736/225000 (9%)] Loss: 6267.769531\n",
      "Train Epoch: 171 [24832/225000 (11%)] Loss: 6359.601562\n",
      "Train Epoch: 171 [28928/225000 (13%)] Loss: 6354.859375\n",
      "Train Epoch: 171 [33024/225000 (15%)] Loss: 6396.589844\n",
      "Train Epoch: 171 [37120/225000 (16%)] Loss: 6338.244141\n",
      "Train Epoch: 171 [41216/225000 (18%)] Loss: 6266.142578\n",
      "Train Epoch: 171 [45312/225000 (20%)] Loss: 6342.128906\n",
      "Train Epoch: 171 [49408/225000 (22%)] Loss: 6364.830078\n",
      "Train Epoch: 171 [53504/225000 (24%)] Loss: 6234.292969\n",
      "Train Epoch: 171 [57600/225000 (26%)] Loss: 6305.759766\n",
      "Train Epoch: 171 [61696/225000 (27%)] Loss: 6219.605469\n",
      "Train Epoch: 171 [65792/225000 (29%)] Loss: 6381.390625\n",
      "Train Epoch: 171 [69888/225000 (31%)] Loss: 6355.085938\n",
      "Train Epoch: 171 [73984/225000 (33%)] Loss: 6328.355469\n",
      "Train Epoch: 171 [78080/225000 (35%)] Loss: 6203.238281\n",
      "Train Epoch: 171 [82176/225000 (37%)] Loss: 6401.738281\n",
      "Train Epoch: 171 [86272/225000 (38%)] Loss: 6516.753906\n",
      "Train Epoch: 171 [90368/225000 (40%)] Loss: 6386.982422\n",
      "Train Epoch: 171 [94464/225000 (42%)] Loss: 6394.871094\n",
      "Train Epoch: 171 [98560/225000 (44%)] Loss: 6211.240234\n",
      "Train Epoch: 171 [102656/225000 (46%)] Loss: 6321.652344\n",
      "Train Epoch: 171 [106752/225000 (47%)] Loss: 6348.396484\n",
      "Train Epoch: 171 [110848/225000 (49%)] Loss: 6357.769531\n",
      "Train Epoch: 171 [114944/225000 (51%)] Loss: 6393.564453\n",
      "Train Epoch: 171 [119040/225000 (53%)] Loss: 6284.396484\n",
      "Train Epoch: 171 [123136/225000 (55%)] Loss: 6285.654297\n",
      "Train Epoch: 171 [127232/225000 (57%)] Loss: 6352.369141\n",
      "Train Epoch: 171 [131328/225000 (58%)] Loss: 6361.371094\n",
      "Train Epoch: 171 [135424/225000 (60%)] Loss: 6470.326172\n",
      "Train Epoch: 171 [139520/225000 (62%)] Loss: 6430.314453\n",
      "Train Epoch: 171 [143616/225000 (64%)] Loss: 6399.962891\n",
      "Train Epoch: 171 [147712/225000 (66%)] Loss: 6403.824219\n",
      "Train Epoch: 171 [151808/225000 (67%)] Loss: 6276.501953\n",
      "Train Epoch: 171 [155904/225000 (69%)] Loss: 6311.039062\n",
      "Train Epoch: 171 [160000/225000 (71%)] Loss: 6434.964844\n",
      "Train Epoch: 171 [164096/225000 (73%)] Loss: 6287.328125\n",
      "Train Epoch: 171 [168192/225000 (75%)] Loss: 6398.310547\n",
      "Train Epoch: 171 [172288/225000 (77%)] Loss: 6315.310547\n",
      "Train Epoch: 171 [176384/225000 (78%)] Loss: 6277.470703\n",
      "Train Epoch: 171 [180480/225000 (80%)] Loss: 6340.291016\n",
      "Train Epoch: 171 [184576/225000 (82%)] Loss: 6327.292969\n",
      "Train Epoch: 171 [188672/225000 (84%)] Loss: 6322.287109\n",
      "Train Epoch: 171 [192768/225000 (86%)] Loss: 6341.160156\n",
      "Train Epoch: 171 [196864/225000 (87%)] Loss: 6256.714844\n",
      "Train Epoch: 171 [200960/225000 (89%)] Loss: 6343.742188\n",
      "Train Epoch: 171 [205056/225000 (91%)] Loss: 6398.433594\n",
      "Train Epoch: 171 [209152/225000 (93%)] Loss: 6293.953125\n",
      "Train Epoch: 171 [213248/225000 (95%)] Loss: 6379.580078\n",
      "Train Epoch: 171 [217344/225000 (97%)] Loss: 6255.798828\n",
      "Train Epoch: 171 [221440/225000 (98%)] Loss: 6398.441406\n",
      "    epoch          : 171\n",
      "    loss           : 6352.308993707338\n",
      "    val_loss       : 6440.822732340316\n",
      "Train Epoch: 172 [256/225000 (0%)] Loss: 6240.662109\n",
      "Train Epoch: 172 [4352/225000 (2%)] Loss: 6324.585938\n",
      "Train Epoch: 172 [8448/225000 (4%)] Loss: 6218.177734\n",
      "Train Epoch: 172 [12544/225000 (6%)] Loss: 6469.464844\n",
      "Train Epoch: 172 [16640/225000 (7%)] Loss: 6281.046875\n",
      "Train Epoch: 172 [20736/225000 (9%)] Loss: 6385.906250\n",
      "Train Epoch: 172 [24832/225000 (11%)] Loss: 6199.814453\n",
      "Train Epoch: 172 [28928/225000 (13%)] Loss: 6318.648438\n",
      "Train Epoch: 172 [33024/225000 (15%)] Loss: 6195.269531\n",
      "Train Epoch: 172 [37120/225000 (16%)] Loss: 6382.394531\n",
      "Train Epoch: 172 [41216/225000 (18%)] Loss: 6319.044922\n",
      "Train Epoch: 172 [45312/225000 (20%)] Loss: 6401.394531\n",
      "Train Epoch: 172 [49408/225000 (22%)] Loss: 6272.017578\n",
      "Train Epoch: 172 [53504/225000 (24%)] Loss: 6337.458984\n",
      "Train Epoch: 172 [57600/225000 (26%)] Loss: 6364.445312\n",
      "Train Epoch: 172 [61696/225000 (27%)] Loss: 6239.587891\n",
      "Train Epoch: 172 [65792/225000 (29%)] Loss: 6391.216797\n",
      "Train Epoch: 172 [69888/225000 (31%)] Loss: 6451.976562\n",
      "Train Epoch: 172 [73984/225000 (33%)] Loss: 6299.845703\n",
      "Train Epoch: 172 [78080/225000 (35%)] Loss: 6485.044922\n",
      "Train Epoch: 172 [82176/225000 (37%)] Loss: 6256.878906\n",
      "Train Epoch: 172 [86272/225000 (38%)] Loss: 6423.923828\n",
      "Train Epoch: 172 [90368/225000 (40%)] Loss: 6423.964844\n",
      "Train Epoch: 172 [94464/225000 (42%)] Loss: 6307.076172\n",
      "Train Epoch: 172 [98560/225000 (44%)] Loss: 6346.630859\n",
      "Train Epoch: 172 [102656/225000 (46%)] Loss: 6320.992188\n",
      "Train Epoch: 172 [106752/225000 (47%)] Loss: 6322.853516\n",
      "Train Epoch: 172 [110848/225000 (49%)] Loss: 6459.542969\n",
      "Train Epoch: 172 [114944/225000 (51%)] Loss: 6304.951172\n",
      "Train Epoch: 172 [119040/225000 (53%)] Loss: 6305.355469\n",
      "Train Epoch: 172 [123136/225000 (55%)] Loss: 6194.287109\n",
      "Train Epoch: 172 [127232/225000 (57%)] Loss: 6266.771484\n",
      "Train Epoch: 172 [131328/225000 (58%)] Loss: 6510.822266\n",
      "Train Epoch: 172 [135424/225000 (60%)] Loss: 6313.160156\n",
      "Train Epoch: 172 [139520/225000 (62%)] Loss: 6334.755859\n",
      "Train Epoch: 172 [143616/225000 (64%)] Loss: 6345.783203\n",
      "Train Epoch: 172 [147712/225000 (66%)] Loss: 6457.001953\n",
      "Train Epoch: 172 [151808/225000 (67%)] Loss: 6384.111328\n",
      "Train Epoch: 172 [155904/225000 (69%)] Loss: 6236.705078\n",
      "Train Epoch: 172 [160000/225000 (71%)] Loss: 6378.818359\n",
      "Train Epoch: 172 [164096/225000 (73%)] Loss: 6308.074219\n",
      "Train Epoch: 172 [168192/225000 (75%)] Loss: 6486.976562\n",
      "Train Epoch: 172 [172288/225000 (77%)] Loss: 6467.712891\n",
      "Train Epoch: 172 [176384/225000 (78%)] Loss: 6383.902344\n",
      "Train Epoch: 172 [180480/225000 (80%)] Loss: 6319.542969\n",
      "Train Epoch: 172 [184576/225000 (82%)] Loss: 6325.802734\n",
      "Train Epoch: 172 [188672/225000 (84%)] Loss: 6241.171875\n",
      "Train Epoch: 172 [192768/225000 (86%)] Loss: 6291.527344\n",
      "Train Epoch: 172 [196864/225000 (87%)] Loss: 6289.222656\n",
      "Train Epoch: 172 [200960/225000 (89%)] Loss: 6179.900391\n",
      "Train Epoch: 172 [205056/225000 (91%)] Loss: 6239.167969\n",
      "Train Epoch: 172 [209152/225000 (93%)] Loss: 6319.421875\n",
      "Train Epoch: 172 [213248/225000 (95%)] Loss: 6241.505859\n",
      "Train Epoch: 172 [217344/225000 (97%)] Loss: 6236.302734\n",
      "Train Epoch: 172 [221440/225000 (98%)] Loss: 6234.570312\n",
      "    epoch          : 172\n",
      "    loss           : 6364.664063610992\n",
      "    val_loss       : 6486.061709501306\n",
      "Train Epoch: 173 [256/225000 (0%)] Loss: 6356.646484\n",
      "Train Epoch: 173 [4352/225000 (2%)] Loss: 6384.343750\n",
      "Train Epoch: 173 [8448/225000 (4%)] Loss: 6450.755859\n",
      "Train Epoch: 173 [12544/225000 (6%)] Loss: 6393.984375\n",
      "Train Epoch: 173 [16640/225000 (7%)] Loss: 6476.884766\n",
      "Train Epoch: 173 [20736/225000 (9%)] Loss: 6227.703125\n",
      "Train Epoch: 173 [24832/225000 (11%)] Loss: 6243.509766\n",
      "Train Epoch: 173 [28928/225000 (13%)] Loss: 6418.480469\n",
      "Train Epoch: 173 [33024/225000 (15%)] Loss: 6226.021484\n",
      "Train Epoch: 173 [37120/225000 (16%)] Loss: 6269.738281\n",
      "Train Epoch: 173 [41216/225000 (18%)] Loss: 6400.787109\n",
      "Train Epoch: 173 [45312/225000 (20%)] Loss: 6347.212891\n",
      "Train Epoch: 173 [49408/225000 (22%)] Loss: 6286.251953\n",
      "Train Epoch: 173 [53504/225000 (24%)] Loss: 6352.927734\n",
      "Train Epoch: 173 [57600/225000 (26%)] Loss: 6234.748047\n",
      "Train Epoch: 173 [61696/225000 (27%)] Loss: 6339.642578\n",
      "Train Epoch: 173 [65792/225000 (29%)] Loss: 6343.166016\n",
      "Train Epoch: 173 [69888/225000 (31%)] Loss: 6465.728516\n",
      "Train Epoch: 173 [73984/225000 (33%)] Loss: 6210.517578\n",
      "Train Epoch: 173 [78080/225000 (35%)] Loss: 6433.759766\n",
      "Train Epoch: 173 [82176/225000 (37%)] Loss: 6299.119141\n",
      "Train Epoch: 173 [86272/225000 (38%)] Loss: 6257.376953\n",
      "Train Epoch: 173 [90368/225000 (40%)] Loss: 6283.541016\n",
      "Train Epoch: 173 [94464/225000 (42%)] Loss: 6297.568359\n",
      "Train Epoch: 173 [98560/225000 (44%)] Loss: 6385.378906\n",
      "Train Epoch: 173 [102656/225000 (46%)] Loss: 6312.396484\n",
      "Train Epoch: 173 [106752/225000 (47%)] Loss: 6328.935547\n",
      "Train Epoch: 173 [110848/225000 (49%)] Loss: 6222.466797\n",
      "Train Epoch: 173 [114944/225000 (51%)] Loss: 6306.046875\n",
      "Train Epoch: 173 [119040/225000 (53%)] Loss: 6306.740234\n",
      "Train Epoch: 173 [123136/225000 (55%)] Loss: 6396.435547\n",
      "Train Epoch: 173 [127232/225000 (57%)] Loss: 6307.964844\n",
      "Train Epoch: 173 [131328/225000 (58%)] Loss: 6382.859375\n",
      "Train Epoch: 173 [135424/225000 (60%)] Loss: 6340.179688\n",
      "Train Epoch: 173 [139520/225000 (62%)] Loss: 6268.027344\n",
      "Train Epoch: 173 [143616/225000 (64%)] Loss: 6385.861328\n",
      "Train Epoch: 173 [147712/225000 (66%)] Loss: 6272.816406\n",
      "Train Epoch: 173 [151808/225000 (67%)] Loss: 6543.750000\n",
      "Train Epoch: 173 [155904/225000 (69%)] Loss: 6299.841797\n",
      "Train Epoch: 173 [160000/225000 (71%)] Loss: 6347.681641\n",
      "Train Epoch: 173 [164096/225000 (73%)] Loss: 6326.037109\n",
      "Train Epoch: 173 [168192/225000 (75%)] Loss: 6311.560547\n",
      "Train Epoch: 173 [172288/225000 (77%)] Loss: 6280.007812\n",
      "Train Epoch: 173 [176384/225000 (78%)] Loss: 6316.277344\n",
      "Train Epoch: 173 [180480/225000 (80%)] Loss: 6268.697266\n",
      "Train Epoch: 173 [184576/225000 (82%)] Loss: 6483.996094\n",
      "Train Epoch: 173 [188672/225000 (84%)] Loss: 6217.027344\n",
      "Train Epoch: 173 [192768/225000 (86%)] Loss: 6263.644531\n",
      "Train Epoch: 173 [196864/225000 (87%)] Loss: 6417.167969\n",
      "Train Epoch: 173 [200960/225000 (89%)] Loss: 6364.474609\n",
      "Train Epoch: 173 [205056/225000 (91%)] Loss: 6258.564453\n",
      "Train Epoch: 173 [209152/225000 (93%)] Loss: 6311.982422\n",
      "Train Epoch: 173 [213248/225000 (95%)] Loss: 6523.111328\n",
      "Train Epoch: 173 [217344/225000 (97%)] Loss: 6440.687500\n",
      "Train Epoch: 173 [221440/225000 (98%)] Loss: 6411.919922\n",
      "    epoch          : 173\n",
      "    loss           : 6395.72057091688\n",
      "    val_loss       : 6422.1155968405765\n",
      "Train Epoch: 174 [256/225000 (0%)] Loss: 6317.929688\n",
      "Train Epoch: 174 [4352/225000 (2%)] Loss: 6246.208984\n",
      "Train Epoch: 174 [8448/225000 (4%)] Loss: 6282.419922\n",
      "Train Epoch: 174 [12544/225000 (6%)] Loss: 6233.152344\n",
      "Train Epoch: 174 [16640/225000 (7%)] Loss: 6434.789062\n",
      "Train Epoch: 174 [20736/225000 (9%)] Loss: 6299.224609\n",
      "Train Epoch: 174 [24832/225000 (11%)] Loss: 6426.273438\n",
      "Train Epoch: 174 [28928/225000 (13%)] Loss: 6219.863281\n",
      "Train Epoch: 174 [33024/225000 (15%)] Loss: 6400.193359\n",
      "Train Epoch: 174 [37120/225000 (16%)] Loss: 6439.925781\n",
      "Train Epoch: 174 [41216/225000 (18%)] Loss: 6312.337891\n",
      "Train Epoch: 174 [45312/225000 (20%)] Loss: 6329.462891\n",
      "Train Epoch: 174 [49408/225000 (22%)] Loss: 6226.134766\n",
      "Train Epoch: 174 [53504/225000 (24%)] Loss: 6306.710938\n",
      "Train Epoch: 174 [57600/225000 (26%)] Loss: 6313.572266\n",
      "Train Epoch: 174 [61696/225000 (27%)] Loss: 6378.923828\n",
      "Train Epoch: 174 [65792/225000 (29%)] Loss: 6246.873047\n",
      "Train Epoch: 174 [69888/225000 (31%)] Loss: 6354.845703\n",
      "Train Epoch: 174 [73984/225000 (33%)] Loss: 6249.806641\n",
      "Train Epoch: 174 [78080/225000 (35%)] Loss: 6406.605469\n",
      "Train Epoch: 174 [82176/225000 (37%)] Loss: 6339.582031\n",
      "Train Epoch: 174 [86272/225000 (38%)] Loss: 6348.373047\n",
      "Train Epoch: 174 [90368/225000 (40%)] Loss: 6275.082031\n",
      "Train Epoch: 174 [94464/225000 (42%)] Loss: 6326.251953\n",
      "Train Epoch: 174 [98560/225000 (44%)] Loss: 6325.224609\n",
      "Train Epoch: 174 [102656/225000 (46%)] Loss: 6386.251953\n",
      "Train Epoch: 174 [106752/225000 (47%)] Loss: 6219.658203\n",
      "Train Epoch: 174 [110848/225000 (49%)] Loss: 6333.705078\n",
      "Train Epoch: 174 [114944/225000 (51%)] Loss: 6413.970703\n",
      "Train Epoch: 174 [119040/225000 (53%)] Loss: 6309.513672\n",
      "Train Epoch: 174 [123136/225000 (55%)] Loss: 6464.625000\n",
      "Train Epoch: 174 [127232/225000 (57%)] Loss: 6320.087891\n",
      "Train Epoch: 174 [131328/225000 (58%)] Loss: 6378.517578\n",
      "Train Epoch: 174 [135424/225000 (60%)] Loss: 6409.292969\n",
      "Train Epoch: 174 [139520/225000 (62%)] Loss: 6332.123047\n",
      "Train Epoch: 174 [143616/225000 (64%)] Loss: 6359.671875\n",
      "Train Epoch: 174 [147712/225000 (66%)] Loss: 6405.501953\n",
      "Train Epoch: 174 [151808/225000 (67%)] Loss: 6287.857422\n",
      "Train Epoch: 174 [155904/225000 (69%)] Loss: 6350.318359\n",
      "Train Epoch: 174 [160000/225000 (71%)] Loss: 6374.804688\n",
      "Train Epoch: 174 [164096/225000 (73%)] Loss: 6283.093750\n",
      "Train Epoch: 174 [168192/225000 (75%)] Loss: 6388.300781\n",
      "Train Epoch: 174 [172288/225000 (77%)] Loss: 6319.470703\n",
      "Train Epoch: 174 [176384/225000 (78%)] Loss: 6456.423828\n",
      "Train Epoch: 174 [180480/225000 (80%)] Loss: 6250.398438\n",
      "Train Epoch: 174 [184576/225000 (82%)] Loss: 6273.535156\n",
      "Train Epoch: 174 [188672/225000 (84%)] Loss: 6295.421875\n",
      "Train Epoch: 174 [192768/225000 (86%)] Loss: 6252.435547\n",
      "Train Epoch: 174 [196864/225000 (87%)] Loss: 6376.115234\n",
      "Train Epoch: 174 [200960/225000 (89%)] Loss: 6379.414062\n",
      "Train Epoch: 174 [205056/225000 (91%)] Loss: 6435.947266\n",
      "Train Epoch: 174 [209152/225000 (93%)] Loss: 6429.945312\n",
      "Train Epoch: 174 [213248/225000 (95%)] Loss: 6394.898438\n",
      "Train Epoch: 174 [217344/225000 (97%)] Loss: 6301.101562\n",
      "Train Epoch: 174 [221440/225000 (98%)] Loss: 6268.089844\n",
      "    epoch          : 174\n",
      "    loss           : 6360.705358095137\n",
      "    val_loss       : 6422.010260316791\n",
      "Train Epoch: 175 [256/225000 (0%)] Loss: 6312.912109\n",
      "Train Epoch: 175 [4352/225000 (2%)] Loss: 6401.492188\n",
      "Train Epoch: 175 [8448/225000 (4%)] Loss: 6358.037109\n",
      "Train Epoch: 175 [12544/225000 (6%)] Loss: 6323.560547\n",
      "Train Epoch: 175 [16640/225000 (7%)] Loss: 6356.250000\n",
      "Train Epoch: 175 [20736/225000 (9%)] Loss: 6194.093750\n",
      "Train Epoch: 175 [24832/225000 (11%)] Loss: 6253.421875\n",
      "Train Epoch: 175 [28928/225000 (13%)] Loss: 6244.175781\n",
      "Train Epoch: 175 [33024/225000 (15%)] Loss: 6322.880859\n",
      "Train Epoch: 175 [37120/225000 (16%)] Loss: 6407.234375\n",
      "Train Epoch: 175 [41216/225000 (18%)] Loss: 6281.412109\n",
      "Train Epoch: 175 [45312/225000 (20%)] Loss: 6270.488281\n",
      "Train Epoch: 175 [49408/225000 (22%)] Loss: 6333.111328\n",
      "Train Epoch: 175 [53504/225000 (24%)] Loss: 6204.664062\n",
      "Train Epoch: 175 [57600/225000 (26%)] Loss: 6346.210938\n",
      "Train Epoch: 175 [61696/225000 (27%)] Loss: 6393.974609\n",
      "Train Epoch: 175 [65792/225000 (29%)] Loss: 6425.089844\n",
      "Train Epoch: 175 [69888/225000 (31%)] Loss: 6363.685547\n",
      "Train Epoch: 175 [73984/225000 (33%)] Loss: 6465.828125\n",
      "Train Epoch: 175 [78080/225000 (35%)] Loss: 6434.117188\n",
      "Train Epoch: 175 [82176/225000 (37%)] Loss: 6333.769531\n",
      "Train Epoch: 175 [86272/225000 (38%)] Loss: 6294.531250\n",
      "Train Epoch: 175 [90368/225000 (40%)] Loss: 6311.300781\n",
      "Train Epoch: 175 [94464/225000 (42%)] Loss: 6315.275391\n",
      "Train Epoch: 175 [98560/225000 (44%)] Loss: 6294.994141\n",
      "Train Epoch: 175 [102656/225000 (46%)] Loss: 6319.650391\n",
      "Train Epoch: 175 [106752/225000 (47%)] Loss: 6413.464844\n",
      "Train Epoch: 175 [110848/225000 (49%)] Loss: 6318.707031\n",
      "Train Epoch: 175 [114944/225000 (51%)] Loss: 6213.109375\n",
      "Train Epoch: 175 [119040/225000 (53%)] Loss: 6230.257812\n",
      "Train Epoch: 175 [123136/225000 (55%)] Loss: 6282.742188\n",
      "Train Epoch: 175 [127232/225000 (57%)] Loss: 6368.287109\n",
      "Train Epoch: 175 [131328/225000 (58%)] Loss: 6331.906250\n",
      "Train Epoch: 175 [135424/225000 (60%)] Loss: 6431.277344\n",
      "Train Epoch: 175 [139520/225000 (62%)] Loss: 6299.496094\n",
      "Train Epoch: 175 [143616/225000 (64%)] Loss: 6242.808594\n",
      "Train Epoch: 175 [147712/225000 (66%)] Loss: 6387.455078\n",
      "Train Epoch: 175 [151808/225000 (67%)] Loss: 6271.121094\n",
      "Train Epoch: 175 [155904/225000 (69%)] Loss: 6261.494141\n",
      "Train Epoch: 175 [160000/225000 (71%)] Loss: 6306.482422\n",
      "Train Epoch: 175 [164096/225000 (73%)] Loss: 6315.308594\n",
      "Train Epoch: 175 [168192/225000 (75%)] Loss: 6212.431641\n",
      "Train Epoch: 175 [172288/225000 (77%)] Loss: 6322.488281\n",
      "Train Epoch: 175 [176384/225000 (78%)] Loss: 6282.246094\n",
      "Train Epoch: 175 [180480/225000 (80%)] Loss: 6281.953125\n",
      "Train Epoch: 175 [184576/225000 (82%)] Loss: 6355.634766\n",
      "Train Epoch: 175 [188672/225000 (84%)] Loss: 6324.908203\n",
      "Train Epoch: 175 [192768/225000 (86%)] Loss: 6390.179688\n",
      "Train Epoch: 175 [196864/225000 (87%)] Loss: 6397.154297\n",
      "Train Epoch: 175 [200960/225000 (89%)] Loss: 6453.416016\n",
      "Train Epoch: 175 [205056/225000 (91%)] Loss: 6394.986328\n",
      "Train Epoch: 175 [209152/225000 (93%)] Loss: 6406.197266\n",
      "Train Epoch: 175 [213248/225000 (95%)] Loss: 6340.181641\n",
      "Train Epoch: 175 [217344/225000 (97%)] Loss: 6264.064453\n",
      "Train Epoch: 175 [221440/225000 (98%)] Loss: 6353.265625\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "    epoch          : 175\n",
      "    loss           : 6399.941786209471\n",
      "    val_loss       : 6464.919315246903\n",
      "Train Epoch: 176 [256/225000 (0%)] Loss: 6173.492188\n",
      "Train Epoch: 176 [4352/225000 (2%)] Loss: 6217.611328\n",
      "Train Epoch: 176 [8448/225000 (4%)] Loss: 6297.449219\n",
      "Train Epoch: 176 [12544/225000 (6%)] Loss: 6332.476562\n",
      "Train Epoch: 176 [16640/225000 (7%)] Loss: 6235.458984\n",
      "Train Epoch: 176 [20736/225000 (9%)] Loss: 6158.146484\n",
      "Train Epoch: 176 [24832/225000 (11%)] Loss: 6247.181641\n",
      "Train Epoch: 176 [28928/225000 (13%)] Loss: 6406.738281\n",
      "Train Epoch: 176 [33024/225000 (15%)] Loss: 6421.001953\n",
      "Train Epoch: 176 [37120/225000 (16%)] Loss: 6342.398438\n",
      "Train Epoch: 176 [41216/225000 (18%)] Loss: 6432.425781\n",
      "Train Epoch: 176 [45312/225000 (20%)] Loss: 6448.933594\n",
      "Train Epoch: 176 [49408/225000 (22%)] Loss: 6405.406250\n",
      "Train Epoch: 176 [53504/225000 (24%)] Loss: 6313.947266\n",
      "Train Epoch: 176 [57600/225000 (26%)] Loss: 6454.894531\n",
      "Train Epoch: 176 [61696/225000 (27%)] Loss: 6272.095703\n",
      "Train Epoch: 176 [65792/225000 (29%)] Loss: 6286.921875\n",
      "Train Epoch: 176 [69888/225000 (31%)] Loss: 6347.457031\n",
      "Train Epoch: 176 [73984/225000 (33%)] Loss: 6254.828125\n",
      "Train Epoch: 176 [78080/225000 (35%)] Loss: 6344.201172\n",
      "Train Epoch: 176 [82176/225000 (37%)] Loss: 6225.875000\n",
      "Train Epoch: 176 [86272/225000 (38%)] Loss: 6192.613281\n",
      "Train Epoch: 176 [90368/225000 (40%)] Loss: 6334.966797\n",
      "Train Epoch: 176 [94464/225000 (42%)] Loss: 6297.269531\n",
      "Train Epoch: 176 [98560/225000 (44%)] Loss: 6319.373047\n",
      "Train Epoch: 176 [102656/225000 (46%)] Loss: 6353.814453\n",
      "Train Epoch: 176 [106752/225000 (47%)] Loss: 6233.123047\n",
      "Train Epoch: 176 [110848/225000 (49%)] Loss: 6224.779297\n",
      "Train Epoch: 176 [114944/225000 (51%)] Loss: 6399.078125\n",
      "Train Epoch: 176 [119040/225000 (53%)] Loss: 6376.398438\n",
      "Train Epoch: 176 [123136/225000 (55%)] Loss: 6422.660156\n",
      "Train Epoch: 176 [127232/225000 (57%)] Loss: 6288.490234\n",
      "Train Epoch: 176 [131328/225000 (58%)] Loss: 6390.261719\n",
      "Train Epoch: 176 [135424/225000 (60%)] Loss: 6419.074219\n",
      "Train Epoch: 176 [139520/225000 (62%)] Loss: 6152.617188\n",
      "Train Epoch: 176 [143616/225000 (64%)] Loss: 6200.398438\n",
      "Train Epoch: 176 [147712/225000 (66%)] Loss: 6369.083984\n",
      "Train Epoch: 176 [151808/225000 (67%)] Loss: 6442.781250\n",
      "Train Epoch: 176 [155904/225000 (69%)] Loss: 6288.363281\n",
      "Train Epoch: 176 [160000/225000 (71%)] Loss: 6333.121094\n",
      "Train Epoch: 176 [164096/225000 (73%)] Loss: 6282.732422\n",
      "Train Epoch: 176 [168192/225000 (75%)] Loss: 6372.363281\n",
      "Train Epoch: 176 [172288/225000 (77%)] Loss: 6407.660156\n",
      "Train Epoch: 176 [176384/225000 (78%)] Loss: 6345.513672\n",
      "Train Epoch: 176 [180480/225000 (80%)] Loss: 6258.986328\n",
      "Train Epoch: 176 [184576/225000 (82%)] Loss: 6413.292969\n",
      "Train Epoch: 176 [188672/225000 (84%)] Loss: 6355.464844\n",
      "Train Epoch: 176 [192768/225000 (86%)] Loss: 6460.634766\n",
      "Train Epoch: 176 [196864/225000 (87%)] Loss: 6354.742188\n",
      "Train Epoch: 176 [200960/225000 (89%)] Loss: 6172.675781\n",
      "Train Epoch: 176 [205056/225000 (91%)] Loss: 6186.396484\n",
      "Train Epoch: 176 [209152/225000 (93%)] Loss: 6303.466797\n",
      "Train Epoch: 176 [213248/225000 (95%)] Loss: 6342.705078\n",
      "Train Epoch: 176 [217344/225000 (97%)] Loss: 8152.050781\n",
      "Train Epoch: 176 [221440/225000 (98%)] Loss: 6343.626953\n",
      "    epoch          : 176\n",
      "    loss           : 6463.92509910054\n",
      "    val_loss       : 6401.632876825576\n",
      "Train Epoch: 177 [256/225000 (0%)] Loss: 6334.539062\n",
      "Train Epoch: 177 [4352/225000 (2%)] Loss: 6373.400391\n",
      "Train Epoch: 177 [8448/225000 (4%)] Loss: 6375.160156\n",
      "Train Epoch: 177 [12544/225000 (6%)] Loss: 6293.693359\n",
      "Train Epoch: 177 [16640/225000 (7%)] Loss: 6368.386719\n",
      "Train Epoch: 177 [20736/225000 (9%)] Loss: 6447.277344\n",
      "Train Epoch: 177 [24832/225000 (11%)] Loss: 6382.845703\n",
      "Train Epoch: 177 [28928/225000 (13%)] Loss: 6418.896484\n",
      "Train Epoch: 177 [33024/225000 (15%)] Loss: 6236.552734\n",
      "Train Epoch: 177 [37120/225000 (16%)] Loss: 6530.515625\n",
      "Train Epoch: 177 [41216/225000 (18%)] Loss: 6329.121094\n",
      "Train Epoch: 177 [45312/225000 (20%)] Loss: 6202.218750\n",
      "Train Epoch: 177 [49408/225000 (22%)] Loss: 6294.902344\n",
      "Train Epoch: 177 [53504/225000 (24%)] Loss: 6426.853516\n",
      "Train Epoch: 177 [57600/225000 (26%)] Loss: 6338.677734\n",
      "Train Epoch: 177 [61696/225000 (27%)] Loss: 6276.003906\n",
      "Train Epoch: 177 [65792/225000 (29%)] Loss: 6364.890625\n",
      "Train Epoch: 177 [69888/225000 (31%)] Loss: 6341.908203\n",
      "Train Epoch: 177 [73984/225000 (33%)] Loss: 6236.767578\n",
      "Train Epoch: 177 [78080/225000 (35%)] Loss: 6312.886719\n",
      "Train Epoch: 177 [82176/225000 (37%)] Loss: 6328.357422\n",
      "Train Epoch: 177 [86272/225000 (38%)] Loss: 6404.427734\n",
      "Train Epoch: 177 [90368/225000 (40%)] Loss: 6390.839844\n",
      "Train Epoch: 177 [94464/225000 (42%)] Loss: 6302.009766\n",
      "Train Epoch: 177 [98560/225000 (44%)] Loss: 6254.595703\n",
      "Train Epoch: 177 [102656/225000 (46%)] Loss: 6249.914062\n",
      "Train Epoch: 177 [106752/225000 (47%)] Loss: 6330.263672\n",
      "Train Epoch: 177 [110848/225000 (49%)] Loss: 6465.085938\n",
      "Train Epoch: 177 [114944/225000 (51%)] Loss: 6379.986328\n",
      "Train Epoch: 177 [119040/225000 (53%)] Loss: 6366.242188\n",
      "Train Epoch: 177 [123136/225000 (55%)] Loss: 6263.953125\n",
      "Train Epoch: 177 [127232/225000 (57%)] Loss: 6316.462891\n",
      "Train Epoch: 177 [131328/225000 (58%)] Loss: 6402.953125\n",
      "Train Epoch: 177 [135424/225000 (60%)] Loss: 6350.310547\n",
      "Train Epoch: 177 [139520/225000 (62%)] Loss: 6302.554688\n",
      "Train Epoch: 177 [143616/225000 (64%)] Loss: 6294.369141\n",
      "Train Epoch: 177 [147712/225000 (66%)] Loss: 6396.490234\n",
      "Train Epoch: 177 [151808/225000 (67%)] Loss: 6415.886719\n",
      "Train Epoch: 177 [155904/225000 (69%)] Loss: 6351.269531\n",
      "Train Epoch: 177 [160000/225000 (71%)] Loss: 6385.781250\n",
      "Train Epoch: 177 [164096/225000 (73%)] Loss: 6336.560547\n",
      "Train Epoch: 177 [168192/225000 (75%)] Loss: 6347.316406\n",
      "Train Epoch: 177 [172288/225000 (77%)] Loss: 6339.636719\n",
      "Train Epoch: 177 [176384/225000 (78%)] Loss: 6308.976562\n",
      "Train Epoch: 177 [180480/225000 (80%)] Loss: 6186.451172\n",
      "Train Epoch: 177 [184576/225000 (82%)] Loss: 6284.031250\n",
      "Train Epoch: 177 [188672/225000 (84%)] Loss: 6305.662109\n",
      "Train Epoch: 177 [192768/225000 (86%)] Loss: 6221.246094\n",
      "Train Epoch: 177 [196864/225000 (87%)] Loss: 6376.078125\n",
      "Train Epoch: 177 [200960/225000 (89%)] Loss: 6486.839844\n",
      "Train Epoch: 177 [205056/225000 (91%)] Loss: 6344.441406\n",
      "Train Epoch: 177 [209152/225000 (93%)] Loss: 6273.666016\n",
      "Train Epoch: 177 [213248/225000 (95%)] Loss: 6404.562500\n",
      "Train Epoch: 177 [217344/225000 (97%)] Loss: 6260.373047\n",
      "Train Epoch: 177 [221440/225000 (98%)] Loss: 6396.115234\n",
      "    epoch          : 177\n",
      "    loss           : 6447.2321063531\n",
      "    val_loss       : 6527.783101488133\n",
      "Train Epoch: 178 [256/225000 (0%)] Loss: 6265.324219\n",
      "Train Epoch: 178 [4352/225000 (2%)] Loss: 6365.103516\n",
      "Train Epoch: 178 [8448/225000 (4%)] Loss: 6223.646484\n",
      "Train Epoch: 178 [12544/225000 (6%)] Loss: 6327.941406\n",
      "Train Epoch: 178 [16640/225000 (7%)] Loss: 6204.833984\n",
      "Train Epoch: 178 [20736/225000 (9%)] Loss: 6322.099609\n",
      "Train Epoch: 178 [24832/225000 (11%)] Loss: 6221.921875\n",
      "Train Epoch: 178 [28928/225000 (13%)] Loss: 6357.226562\n",
      "Train Epoch: 178 [33024/225000 (15%)] Loss: 6335.871094\n",
      "Train Epoch: 178 [37120/225000 (16%)] Loss: 6278.269531\n",
      "Train Epoch: 178 [41216/225000 (18%)] Loss: 6337.263672\n",
      "Train Epoch: 178 [45312/225000 (20%)] Loss: 6274.611328\n",
      "Train Epoch: 178 [49408/225000 (22%)] Loss: 6343.513672\n",
      "Train Epoch: 178 [53504/225000 (24%)] Loss: 6352.160156\n",
      "Train Epoch: 178 [57600/225000 (26%)] Loss: 6428.105469\n",
      "Train Epoch: 178 [61696/225000 (27%)] Loss: 6256.343750\n",
      "Train Epoch: 178 [65792/225000 (29%)] Loss: 6295.273438\n",
      "Train Epoch: 178 [69888/225000 (31%)] Loss: 6237.939453\n",
      "Train Epoch: 178 [73984/225000 (33%)] Loss: 6366.601562\n",
      "Train Epoch: 178 [78080/225000 (35%)] Loss: 6467.451172\n",
      "Train Epoch: 178 [82176/225000 (37%)] Loss: 6312.777344\n",
      "Train Epoch: 178 [86272/225000 (38%)] Loss: 6301.449219\n",
      "Train Epoch: 178 [90368/225000 (40%)] Loss: 6284.494141\n",
      "Train Epoch: 178 [94464/225000 (42%)] Loss: 6265.023438\n",
      "Train Epoch: 178 [98560/225000 (44%)] Loss: 6369.068359\n",
      "Train Epoch: 178 [102656/225000 (46%)] Loss: 6329.835938\n",
      "Train Epoch: 178 [106752/225000 (47%)] Loss: 6342.384766\n",
      "Train Epoch: 178 [110848/225000 (49%)] Loss: 6311.257812\n",
      "Train Epoch: 178 [114944/225000 (51%)] Loss: 6324.343750\n",
      "Train Epoch: 178 [119040/225000 (53%)] Loss: 6315.595703\n",
      "Train Epoch: 178 [123136/225000 (55%)] Loss: 6335.955078\n",
      "Train Epoch: 178 [127232/225000 (57%)] Loss: 6347.208984\n",
      "Train Epoch: 178 [131328/225000 (58%)] Loss: 6277.673828\n",
      "Train Epoch: 178 [135424/225000 (60%)] Loss: 6241.697266\n",
      "Train Epoch: 178 [139520/225000 (62%)] Loss: 6259.664062\n",
      "Train Epoch: 178 [143616/225000 (64%)] Loss: 6215.667969\n",
      "Train Epoch: 178 [147712/225000 (66%)] Loss: 6369.511719\n",
      "Train Epoch: 178 [151808/225000 (67%)] Loss: 6344.332031\n",
      "Train Epoch: 178 [155904/225000 (69%)] Loss: 6338.839844\n",
      "Train Epoch: 178 [160000/225000 (71%)] Loss: 6411.582031\n",
      "Train Epoch: 178 [164096/225000 (73%)] Loss: 6315.546875\n",
      "Train Epoch: 178 [168192/225000 (75%)] Loss: 6281.429688\n",
      "Train Epoch: 178 [172288/225000 (77%)] Loss: 6229.673828\n",
      "Train Epoch: 178 [176384/225000 (78%)] Loss: 6468.201172\n",
      "Train Epoch: 178 [180480/225000 (80%)] Loss: 6353.078125\n",
      "Train Epoch: 178 [184576/225000 (82%)] Loss: 6341.828125\n",
      "Train Epoch: 178 [188672/225000 (84%)] Loss: 6370.835938\n",
      "Train Epoch: 178 [192768/225000 (86%)] Loss: 6596.269531\n",
      "Train Epoch: 178 [196864/225000 (87%)] Loss: 6309.958984\n",
      "Train Epoch: 178 [200960/225000 (89%)] Loss: 6361.271484\n",
      "Train Epoch: 178 [205056/225000 (91%)] Loss: 6369.001953\n",
      "Train Epoch: 178 [209152/225000 (93%)] Loss: 8147.285156\n",
      "Train Epoch: 178 [213248/225000 (95%)] Loss: 6428.255859\n",
      "Train Epoch: 178 [217344/225000 (97%)] Loss: 6369.230469\n",
      "Train Epoch: 178 [221440/225000 (98%)] Loss: 6353.642578\n",
      "    epoch          : 178\n",
      "    loss           : 6389.532118796217\n",
      "    val_loss       : 6401.49160857225\n",
      "Train Epoch: 179 [256/225000 (0%)] Loss: 6332.355469\n",
      "Train Epoch: 179 [4352/225000 (2%)] Loss: 6225.724609\n",
      "Train Epoch: 179 [8448/225000 (4%)] Loss: 6317.996094\n",
      "Train Epoch: 179 [12544/225000 (6%)] Loss: 6392.794922\n",
      "Train Epoch: 179 [16640/225000 (7%)] Loss: 6141.644531\n",
      "Train Epoch: 179 [20736/225000 (9%)] Loss: 8040.384766\n",
      "Train Epoch: 179 [24832/225000 (11%)] Loss: 6265.419922\n",
      "Train Epoch: 179 [28928/225000 (13%)] Loss: 6334.166016\n",
      "Train Epoch: 179 [33024/225000 (15%)] Loss: 6291.117188\n",
      "Train Epoch: 179 [37120/225000 (16%)] Loss: 6388.804688\n",
      "Train Epoch: 179 [41216/225000 (18%)] Loss: 6304.923828\n",
      "Train Epoch: 179 [45312/225000 (20%)] Loss: 6318.785156\n",
      "Train Epoch: 179 [49408/225000 (22%)] Loss: 6238.871094\n",
      "Train Epoch: 179 [53504/225000 (24%)] Loss: 6198.542969\n",
      "Train Epoch: 179 [57600/225000 (26%)] Loss: 6335.449219\n",
      "Train Epoch: 179 [61696/225000 (27%)] Loss: 6350.207031\n",
      "Train Epoch: 179 [65792/225000 (29%)] Loss: 6342.609375\n",
      "Train Epoch: 179 [69888/225000 (31%)] Loss: 6405.421875\n",
      "Train Epoch: 179 [73984/225000 (33%)] Loss: 6335.376953\n",
      "Train Epoch: 179 [78080/225000 (35%)] Loss: 6265.447266\n",
      "Train Epoch: 179 [82176/225000 (37%)] Loss: 6285.078125\n",
      "Train Epoch: 179 [86272/225000 (38%)] Loss: 6230.546875\n",
      "Train Epoch: 179 [90368/225000 (40%)] Loss: 6328.484375\n",
      "Train Epoch: 179 [94464/225000 (42%)] Loss: 6322.392578\n",
      "Train Epoch: 179 [98560/225000 (44%)] Loss: 6342.167969\n",
      "Train Epoch: 179 [102656/225000 (46%)] Loss: 6272.212891\n",
      "Train Epoch: 179 [106752/225000 (47%)] Loss: 6327.251953\n",
      "Train Epoch: 179 [110848/225000 (49%)] Loss: 6208.216797\n",
      "Train Epoch: 179 [114944/225000 (51%)] Loss: 6232.314453\n",
      "Train Epoch: 179 [119040/225000 (53%)] Loss: 6501.830078\n",
      "Train Epoch: 179 [123136/225000 (55%)] Loss: 6299.812500\n",
      "Train Epoch: 179 [127232/225000 (57%)] Loss: 6373.851562\n",
      "Train Epoch: 179 [131328/225000 (58%)] Loss: 6296.000000\n",
      "Train Epoch: 179 [135424/225000 (60%)] Loss: 6144.191406\n",
      "Train Epoch: 179 [139520/225000 (62%)] Loss: 6339.837891\n",
      "Train Epoch: 179 [143616/225000 (64%)] Loss: 6420.173828\n",
      "Train Epoch: 179 [147712/225000 (66%)] Loss: 6324.597656\n",
      "Train Epoch: 179 [151808/225000 (67%)] Loss: 6289.953125\n",
      "Train Epoch: 179 [155904/225000 (69%)] Loss: 6442.664062\n",
      "Train Epoch: 179 [160000/225000 (71%)] Loss: 6369.599609\n",
      "Train Epoch: 179 [164096/225000 (73%)] Loss: 6315.347656\n",
      "Train Epoch: 179 [168192/225000 (75%)] Loss: 6401.439453\n",
      "Train Epoch: 179 [172288/225000 (77%)] Loss: 6266.017578\n",
      "Train Epoch: 179 [176384/225000 (78%)] Loss: 6425.037109\n",
      "Train Epoch: 179 [180480/225000 (80%)] Loss: 6317.357422\n",
      "Train Epoch: 179 [184576/225000 (82%)] Loss: 6369.703125\n",
      "Train Epoch: 179 [188672/225000 (84%)] Loss: 6296.255859\n",
      "Train Epoch: 179 [192768/225000 (86%)] Loss: 6286.605469\n",
      "Train Epoch: 179 [196864/225000 (87%)] Loss: 6310.248047\n",
      "Train Epoch: 179 [200960/225000 (89%)] Loss: 6222.789062\n",
      "Train Epoch: 179 [205056/225000 (91%)] Loss: 6194.761719\n",
      "Train Epoch: 179 [209152/225000 (93%)] Loss: 6276.314453\n",
      "Train Epoch: 179 [213248/225000 (95%)] Loss: 6312.406250\n",
      "Train Epoch: 179 [217344/225000 (97%)] Loss: 6336.904297\n",
      "Train Epoch: 179 [221440/225000 (98%)] Loss: 6341.554688\n",
      "    epoch          : 179\n",
      "    loss           : 6411.334344336604\n",
      "    val_loss       : 6437.751515454175\n",
      "Train Epoch: 180 [256/225000 (0%)] Loss: 6265.429688\n",
      "Train Epoch: 180 [4352/225000 (2%)] Loss: 6420.597656\n",
      "Train Epoch: 180 [8448/225000 (4%)] Loss: 6294.310547\n",
      "Train Epoch: 180 [12544/225000 (6%)] Loss: 6257.335938\n",
      "Train Epoch: 180 [16640/225000 (7%)] Loss: 6277.667969\n",
      "Train Epoch: 180 [20736/225000 (9%)] Loss: 6242.738281\n",
      "Train Epoch: 180 [24832/225000 (11%)] Loss: 6248.703125\n",
      "Train Epoch: 180 [28928/225000 (13%)] Loss: 6454.162109\n",
      "Train Epoch: 180 [33024/225000 (15%)] Loss: 6262.531250\n",
      "Train Epoch: 180 [37120/225000 (16%)] Loss: 6329.572266\n",
      "Train Epoch: 180 [41216/225000 (18%)] Loss: 6267.943359\n",
      "Train Epoch: 180 [45312/225000 (20%)] Loss: 6436.017578\n",
      "Train Epoch: 180 [49408/225000 (22%)] Loss: 6392.224609\n",
      "Train Epoch: 180 [53504/225000 (24%)] Loss: 6398.031250\n",
      "Train Epoch: 180 [57600/225000 (26%)] Loss: 6301.132812\n",
      "Train Epoch: 180 [61696/225000 (27%)] Loss: 6178.150391\n",
      "Train Epoch: 180 [65792/225000 (29%)] Loss: 6383.619141\n",
      "Train Epoch: 180 [69888/225000 (31%)] Loss: 6428.130859\n",
      "Train Epoch: 180 [73984/225000 (33%)] Loss: 6487.275391\n",
      "Train Epoch: 180 [78080/225000 (35%)] Loss: 6171.349609\n",
      "Train Epoch: 180 [82176/225000 (37%)] Loss: 6340.634766\n",
      "Train Epoch: 180 [86272/225000 (38%)] Loss: 6429.257812\n",
      "Train Epoch: 180 [90368/225000 (40%)] Loss: 6364.353516\n",
      "Train Epoch: 180 [94464/225000 (42%)] Loss: 6240.759766\n",
      "Train Epoch: 180 [98560/225000 (44%)] Loss: 6236.302734\n",
      "Train Epoch: 180 [102656/225000 (46%)] Loss: 6356.835938\n",
      "Train Epoch: 180 [106752/225000 (47%)] Loss: 6427.162109\n",
      "Train Epoch: 180 [110848/225000 (49%)] Loss: 6316.470703\n",
      "Train Epoch: 180 [114944/225000 (51%)] Loss: 6441.611328\n",
      "Train Epoch: 180 [119040/225000 (53%)] Loss: 6319.603516\n",
      "Train Epoch: 180 [123136/225000 (55%)] Loss: 6356.025391\n",
      "Train Epoch: 180 [127232/225000 (57%)] Loss: 6239.173828\n",
      "Train Epoch: 180 [131328/225000 (58%)] Loss: 6369.750000\n",
      "Train Epoch: 180 [135424/225000 (60%)] Loss: 6350.628906\n",
      "Train Epoch: 180 [139520/225000 (62%)] Loss: 6197.484375\n",
      "Train Epoch: 180 [143616/225000 (64%)] Loss: 6248.294922\n",
      "Train Epoch: 180 [147712/225000 (66%)] Loss: 6259.472656\n",
      "Train Epoch: 180 [151808/225000 (67%)] Loss: 6308.355469\n",
      "Train Epoch: 180 [155904/225000 (69%)] Loss: 6313.769531\n",
      "Train Epoch: 180 [160000/225000 (71%)] Loss: 6347.769531\n",
      "Train Epoch: 180 [164096/225000 (73%)] Loss: 6338.949219\n",
      "Train Epoch: 180 [168192/225000 (75%)] Loss: 6339.324219\n",
      "Train Epoch: 180 [172288/225000 (77%)] Loss: 6320.587891\n",
      "Train Epoch: 180 [176384/225000 (78%)] Loss: 6297.027344\n",
      "Train Epoch: 180 [180480/225000 (80%)] Loss: 6331.279297\n",
      "Train Epoch: 180 [184576/225000 (82%)] Loss: 6428.843750\n",
      "Train Epoch: 180 [188672/225000 (84%)] Loss: 6364.457031\n",
      "Train Epoch: 180 [192768/225000 (86%)] Loss: 6437.265625\n",
      "Train Epoch: 180 [196864/225000 (87%)] Loss: 6303.966797\n",
      "Train Epoch: 180 [200960/225000 (89%)] Loss: 6351.875000\n",
      "Train Epoch: 180 [205056/225000 (91%)] Loss: 6317.298828\n",
      "Train Epoch: 180 [209152/225000 (93%)] Loss: 6250.376953\n",
      "Train Epoch: 180 [213248/225000 (95%)] Loss: 6357.931641\n",
      "Train Epoch: 180 [217344/225000 (97%)] Loss: 6334.400391\n",
      "Train Epoch: 180 [221440/225000 (98%)] Loss: 6230.269531\n",
      "    epoch          : 180\n",
      "    loss           : 6357.5276326080775\n",
      "    val_loss       : 6713.8792616542505\n",
      "Train Epoch: 181 [256/225000 (0%)] Loss: 6345.984375\n",
      "Train Epoch: 181 [4352/225000 (2%)] Loss: 6271.638672\n",
      "Train Epoch: 181 [8448/225000 (4%)] Loss: 6337.623047\n",
      "Train Epoch: 181 [12544/225000 (6%)] Loss: 6263.876953\n",
      "Train Epoch: 181 [16640/225000 (7%)] Loss: 6278.867188\n",
      "Train Epoch: 181 [20736/225000 (9%)] Loss: 6560.068359\n",
      "Train Epoch: 181 [24832/225000 (11%)] Loss: 6240.525391\n",
      "Train Epoch: 181 [28928/225000 (13%)] Loss: 6326.068359\n",
      "Train Epoch: 181 [33024/225000 (15%)] Loss: 6310.123047\n",
      "Train Epoch: 181 [37120/225000 (16%)] Loss: 6373.521484\n",
      "Train Epoch: 181 [41216/225000 (18%)] Loss: 6348.923828\n",
      "Train Epoch: 181 [45312/225000 (20%)] Loss: 6402.855469\n",
      "Train Epoch: 181 [49408/225000 (22%)] Loss: 6421.658203\n",
      "Train Epoch: 181 [53504/225000 (24%)] Loss: 6366.138672\n",
      "Train Epoch: 181 [57600/225000 (26%)] Loss: 6429.732422\n",
      "Train Epoch: 181 [61696/225000 (27%)] Loss: 6094.246094\n",
      "Train Epoch: 181 [65792/225000 (29%)] Loss: 6273.814453\n",
      "Train Epoch: 181 [69888/225000 (31%)] Loss: 6231.517578\n",
      "Train Epoch: 181 [73984/225000 (33%)] Loss: 6388.941406\n",
      "Train Epoch: 181 [78080/225000 (35%)] Loss: 6536.486328\n",
      "Train Epoch: 181 [82176/225000 (37%)] Loss: 6248.257812\n",
      "Train Epoch: 181 [86272/225000 (38%)] Loss: 6492.281250\n",
      "Train Epoch: 181 [90368/225000 (40%)] Loss: 6402.806641\n",
      "Train Epoch: 181 [94464/225000 (42%)] Loss: 6381.939453\n",
      "Train Epoch: 181 [98560/225000 (44%)] Loss: 6278.658203\n",
      "Train Epoch: 181 [102656/225000 (46%)] Loss: 6539.804688\n",
      "Train Epoch: 181 [106752/225000 (47%)] Loss: 6205.892578\n",
      "Train Epoch: 181 [110848/225000 (49%)] Loss: 6426.458984\n",
      "Train Epoch: 181 [114944/225000 (51%)] Loss: 6330.564453\n",
      "Train Epoch: 181 [119040/225000 (53%)] Loss: 6290.238281\n",
      "Train Epoch: 181 [123136/225000 (55%)] Loss: 6404.634766\n",
      "Train Epoch: 181 [127232/225000 (57%)] Loss: 6314.093750\n",
      "Train Epoch: 181 [131328/225000 (58%)] Loss: 8207.601562\n",
      "Train Epoch: 181 [135424/225000 (60%)] Loss: 6361.455078\n",
      "Train Epoch: 181 [139520/225000 (62%)] Loss: 6319.728516\n",
      "Train Epoch: 181 [143616/225000 (64%)] Loss: 6416.093750\n",
      "Train Epoch: 181 [147712/225000 (66%)] Loss: 6234.878906\n",
      "Train Epoch: 181 [151808/225000 (67%)] Loss: 6223.824219\n",
      "Train Epoch: 181 [155904/225000 (69%)] Loss: 6275.623047\n",
      "Train Epoch: 181 [160000/225000 (71%)] Loss: 6421.128906\n",
      "Train Epoch: 181 [164096/225000 (73%)] Loss: 6292.201172\n",
      "Train Epoch: 181 [168192/225000 (75%)] Loss: 6402.396484\n",
      "Train Epoch: 181 [172288/225000 (77%)] Loss: 6264.494141\n",
      "Train Epoch: 181 [176384/225000 (78%)] Loss: 6267.134766\n",
      "Train Epoch: 181 [180480/225000 (80%)] Loss: 6335.453125\n",
      "Train Epoch: 181 [184576/225000 (82%)] Loss: 6370.105469\n",
      "Train Epoch: 181 [188672/225000 (84%)] Loss: 6373.945312\n",
      "Train Epoch: 181 [192768/225000 (86%)] Loss: 6264.037109\n",
      "Train Epoch: 181 [196864/225000 (87%)] Loss: 6301.558594\n",
      "Train Epoch: 181 [200960/225000 (89%)] Loss: 6311.984375\n",
      "Train Epoch: 181 [205056/225000 (91%)] Loss: 6325.343750\n",
      "Train Epoch: 181 [209152/225000 (93%)] Loss: 6370.970703\n",
      "Train Epoch: 181 [213248/225000 (95%)] Loss: 6324.054688\n",
      "Train Epoch: 181 [217344/225000 (97%)] Loss: 6453.205078\n",
      "Train Epoch: 181 [221440/225000 (98%)] Loss: 6336.890625\n",
      "    epoch          : 181\n",
      "    loss           : 6359.341235823735\n",
      "    val_loss       : 6463.925549246827\n",
      "Train Epoch: 182 [256/225000 (0%)] Loss: 6553.738281\n",
      "Train Epoch: 182 [4352/225000 (2%)] Loss: 6239.574219\n",
      "Train Epoch: 182 [8448/225000 (4%)] Loss: 6324.007812\n",
      "Train Epoch: 182 [12544/225000 (6%)] Loss: 6366.861328\n",
      "Train Epoch: 182 [16640/225000 (7%)] Loss: 6336.820312\n",
      "Train Epoch: 182 [20736/225000 (9%)] Loss: 6356.929688\n",
      "Train Epoch: 182 [24832/225000 (11%)] Loss: 6313.486328\n",
      "Train Epoch: 182 [28928/225000 (13%)] Loss: 6310.664062\n",
      "Train Epoch: 182 [33024/225000 (15%)] Loss: 6492.431641\n",
      "Train Epoch: 182 [37120/225000 (16%)] Loss: 6327.535156\n",
      "Train Epoch: 182 [41216/225000 (18%)] Loss: 6333.691406\n",
      "Train Epoch: 182 [45312/225000 (20%)] Loss: 6388.558594\n",
      "Train Epoch: 182 [49408/225000 (22%)] Loss: 6195.470703\n",
      "Train Epoch: 182 [53504/225000 (24%)] Loss: 6411.273438\n",
      "Train Epoch: 182 [57600/225000 (26%)] Loss: 6305.011719\n",
      "Train Epoch: 182 [61696/225000 (27%)] Loss: 6370.181641\n",
      "Train Epoch: 182 [65792/225000 (29%)] Loss: 6235.626953\n",
      "Train Epoch: 182 [69888/225000 (31%)] Loss: 6338.339844\n",
      "Train Epoch: 182 [73984/225000 (33%)] Loss: 6246.839844\n",
      "Train Epoch: 182 [78080/225000 (35%)] Loss: 6321.583984\n",
      "Train Epoch: 182 [82176/225000 (37%)] Loss: 6403.931641\n",
      "Train Epoch: 182 [86272/225000 (38%)] Loss: 6350.152344\n",
      "Train Epoch: 182 [90368/225000 (40%)] Loss: 6306.560547\n",
      "Train Epoch: 182 [94464/225000 (42%)] Loss: 6211.839844\n",
      "Train Epoch: 182 [98560/225000 (44%)] Loss: 6420.826172\n",
      "Train Epoch: 182 [102656/225000 (46%)] Loss: 6488.587891\n",
      "Train Epoch: 182 [106752/225000 (47%)] Loss: 6271.937500\n",
      "Train Epoch: 182 [110848/225000 (49%)] Loss: 6301.470703\n",
      "Train Epoch: 182 [114944/225000 (51%)] Loss: 6300.779297\n",
      "Train Epoch: 182 [119040/225000 (53%)] Loss: 6289.265625\n",
      "Train Epoch: 182 [123136/225000 (55%)] Loss: 6320.593750\n",
      "Train Epoch: 182 [127232/225000 (57%)] Loss: 6391.755859\n",
      "Train Epoch: 182 [131328/225000 (58%)] Loss: 6309.957031\n",
      "Train Epoch: 182 [135424/225000 (60%)] Loss: 6265.333984\n",
      "Train Epoch: 182 [139520/225000 (62%)] Loss: 6388.666016\n",
      "Train Epoch: 182 [143616/225000 (64%)] Loss: 6293.617188\n",
      "Train Epoch: 182 [147712/225000 (66%)] Loss: 6433.009766\n",
      "Train Epoch: 182 [151808/225000 (67%)] Loss: 6526.189453\n",
      "Train Epoch: 182 [155904/225000 (69%)] Loss: 6249.105469\n",
      "Train Epoch: 182 [160000/225000 (71%)] Loss: 6384.378906\n",
      "Train Epoch: 182 [164096/225000 (73%)] Loss: 6220.113281\n",
      "Train Epoch: 182 [168192/225000 (75%)] Loss: 6250.951172\n",
      "Train Epoch: 182 [172288/225000 (77%)] Loss: 6468.501953\n",
      "Train Epoch: 182 [176384/225000 (78%)] Loss: 6374.732422\n",
      "Train Epoch: 182 [180480/225000 (80%)] Loss: 6269.869141\n",
      "Train Epoch: 182 [184576/225000 (82%)] Loss: 6339.701172\n",
      "Train Epoch: 182 [188672/225000 (84%)] Loss: 6174.849609\n",
      "Train Epoch: 182 [192768/225000 (86%)] Loss: 6361.511719\n",
      "Train Epoch: 182 [196864/225000 (87%)] Loss: 6389.291016\n",
      "Train Epoch: 182 [200960/225000 (89%)] Loss: 6316.871094\n",
      "Train Epoch: 182 [205056/225000 (91%)] Loss: 6239.201172\n",
      "Train Epoch: 182 [209152/225000 (93%)] Loss: 6303.033203\n",
      "Train Epoch: 182 [213248/225000 (95%)] Loss: 6347.068359\n",
      "Train Epoch: 182 [217344/225000 (97%)] Loss: 6264.667969\n",
      "Train Epoch: 182 [221440/225000 (98%)] Loss: 6336.503906\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "    epoch          : 182\n",
      "    loss           : 6405.3175316855095\n",
      "    val_loss       : 6419.473621020512\n",
      "Train Epoch: 183 [256/225000 (0%)] Loss: 6340.630859\n",
      "Train Epoch: 183 [4352/225000 (2%)] Loss: 8105.123047\n",
      "Train Epoch: 183 [8448/225000 (4%)] Loss: 6503.125000\n",
      "Train Epoch: 183 [12544/225000 (6%)] Loss: 6498.419922\n",
      "Train Epoch: 183 [16640/225000 (7%)] Loss: 6415.771484\n",
      "Train Epoch: 183 [20736/225000 (9%)] Loss: 6381.916016\n",
      "Train Epoch: 183 [24832/225000 (11%)] Loss: 6356.705078\n",
      "Train Epoch: 183 [28928/225000 (13%)] Loss: 6343.181641\n",
      "Train Epoch: 183 [33024/225000 (15%)] Loss: 6371.667969\n",
      "Train Epoch: 183 [37120/225000 (16%)] Loss: 6393.880859\n",
      "Train Epoch: 183 [41216/225000 (18%)] Loss: 6283.078125\n",
      "Train Epoch: 183 [45312/225000 (20%)] Loss: 6294.130859\n",
      "Train Epoch: 183 [49408/225000 (22%)] Loss: 6397.337891\n",
      "Train Epoch: 183 [53504/225000 (24%)] Loss: 6287.000000\n",
      "Train Epoch: 183 [57600/225000 (26%)] Loss: 6281.007812\n",
      "Train Epoch: 183 [61696/225000 (27%)] Loss: 6263.005859\n",
      "Train Epoch: 183 [65792/225000 (29%)] Loss: 6250.000000\n",
      "Train Epoch: 183 [69888/225000 (31%)] Loss: 6419.429688\n",
      "Train Epoch: 183 [73984/225000 (33%)] Loss: 6438.152344\n",
      "Train Epoch: 183 [78080/225000 (35%)] Loss: 6405.755859\n",
      "Train Epoch: 183 [82176/225000 (37%)] Loss: 6474.509766\n",
      "Train Epoch: 183 [86272/225000 (38%)] Loss: 6366.578125\n",
      "Train Epoch: 183 [90368/225000 (40%)] Loss: 6226.201172\n",
      "Train Epoch: 183 [94464/225000 (42%)] Loss: 6401.416016\n",
      "Train Epoch: 183 [98560/225000 (44%)] Loss: 6419.107422\n",
      "Train Epoch: 183 [102656/225000 (46%)] Loss: 6479.283203\n",
      "Train Epoch: 183 [106752/225000 (47%)] Loss: 6237.021484\n",
      "Train Epoch: 183 [110848/225000 (49%)] Loss: 6299.648438\n",
      "Train Epoch: 183 [114944/225000 (51%)] Loss: 6382.521484\n",
      "Train Epoch: 183 [119040/225000 (53%)] Loss: 6238.996094\n",
      "Train Epoch: 183 [123136/225000 (55%)] Loss: 6414.113281\n",
      "Train Epoch: 183 [127232/225000 (57%)] Loss: 6304.091797\n",
      "Train Epoch: 183 [131328/225000 (58%)] Loss: 6241.177734\n",
      "Train Epoch: 183 [135424/225000 (60%)] Loss: 6255.482422\n",
      "Train Epoch: 183 [139520/225000 (62%)] Loss: 6387.873047\n",
      "Train Epoch: 183 [143616/225000 (64%)] Loss: 6246.050781\n",
      "Train Epoch: 183 [147712/225000 (66%)] Loss: 6381.644531\n",
      "Train Epoch: 183 [151808/225000 (67%)] Loss: 6290.376953\n",
      "Train Epoch: 183 [155904/225000 (69%)] Loss: 6270.382812\n",
      "Train Epoch: 183 [160000/225000 (71%)] Loss: 6305.464844\n",
      "Train Epoch: 183 [164096/225000 (73%)] Loss: 6395.394531\n",
      "Train Epoch: 183 [168192/225000 (75%)] Loss: 6349.660156\n",
      "Train Epoch: 183 [172288/225000 (77%)] Loss: 6366.220703\n",
      "Train Epoch: 183 [176384/225000 (78%)] Loss: 6432.701172\n",
      "Train Epoch: 183 [180480/225000 (80%)] Loss: 6340.804688\n",
      "Train Epoch: 183 [184576/225000 (82%)] Loss: 6301.609375\n",
      "Train Epoch: 183 [188672/225000 (84%)] Loss: 6257.857422\n",
      "Train Epoch: 183 [192768/225000 (86%)] Loss: 6456.435547\n",
      "Train Epoch: 183 [196864/225000 (87%)] Loss: 6487.308594\n",
      "Train Epoch: 183 [200960/225000 (89%)] Loss: 6380.833984\n",
      "Train Epoch: 183 [205056/225000 (91%)] Loss: 6380.970703\n",
      "Train Epoch: 183 [209152/225000 (93%)] Loss: 6248.388672\n",
      "Train Epoch: 183 [213248/225000 (95%)] Loss: 6489.025391\n",
      "Train Epoch: 183 [217344/225000 (97%)] Loss: 6370.773438\n",
      "Train Epoch: 183 [221440/225000 (98%)] Loss: 6360.507812\n",
      "    epoch          : 183\n",
      "    loss           : 6419.706684620307\n",
      "    val_loss       : 6482.474382057482\n",
      "Train Epoch: 184 [256/225000 (0%)] Loss: 6265.890625\n",
      "Train Epoch: 184 [4352/225000 (2%)] Loss: 6346.523438\n",
      "Train Epoch: 184 [8448/225000 (4%)] Loss: 6414.962891\n",
      "Train Epoch: 184 [12544/225000 (6%)] Loss: 6359.230469\n",
      "Train Epoch: 184 [16640/225000 (7%)] Loss: 6363.650391\n",
      "Train Epoch: 184 [20736/225000 (9%)] Loss: 6498.554688\n",
      "Train Epoch: 184 [24832/225000 (11%)] Loss: 6382.775391\n",
      "Train Epoch: 184 [28928/225000 (13%)] Loss: 6395.476562\n",
      "Train Epoch: 184 [33024/225000 (15%)] Loss: 6372.027344\n",
      "Train Epoch: 184 [37120/225000 (16%)] Loss: 6297.605469\n",
      "Train Epoch: 184 [41216/225000 (18%)] Loss: 6208.115234\n",
      "Train Epoch: 184 [45312/225000 (20%)] Loss: 6477.583984\n",
      "Train Epoch: 184 [49408/225000 (22%)] Loss: 6405.978516\n",
      "Train Epoch: 184 [53504/225000 (24%)] Loss: 6297.660156\n",
      "Train Epoch: 184 [57600/225000 (26%)] Loss: 6283.228516\n",
      "Train Epoch: 184 [61696/225000 (27%)] Loss: 6308.136719\n",
      "Train Epoch: 184 [65792/225000 (29%)] Loss: 6307.257812\n",
      "Train Epoch: 184 [69888/225000 (31%)] Loss: 6293.380859\n",
      "Train Epoch: 184 [73984/225000 (33%)] Loss: 6230.759766\n",
      "Train Epoch: 184 [78080/225000 (35%)] Loss: 6433.503906\n",
      "Train Epoch: 184 [82176/225000 (37%)] Loss: 6269.792969\n",
      "Train Epoch: 184 [86272/225000 (38%)] Loss: 6309.244141\n",
      "Train Epoch: 184 [90368/225000 (40%)] Loss: 6317.984375\n",
      "Train Epoch: 184 [94464/225000 (42%)] Loss: 6488.437500\n",
      "Train Epoch: 184 [98560/225000 (44%)] Loss: 6307.683594\n",
      "Train Epoch: 184 [102656/225000 (46%)] Loss: 6318.333984\n",
      "Train Epoch: 184 [106752/225000 (47%)] Loss: 6400.007812\n",
      "Train Epoch: 184 [110848/225000 (49%)] Loss: 6309.861328\n",
      "Train Epoch: 184 [114944/225000 (51%)] Loss: 6316.062500\n",
      "Train Epoch: 184 [119040/225000 (53%)] Loss: 6250.197266\n",
      "Train Epoch: 184 [123136/225000 (55%)] Loss: 6266.869141\n",
      "Train Epoch: 184 [127232/225000 (57%)] Loss: 6287.914062\n",
      "Train Epoch: 184 [131328/225000 (58%)] Loss: 6320.685547\n",
      "Train Epoch: 184 [135424/225000 (60%)] Loss: 6276.429688\n",
      "Train Epoch: 184 [139520/225000 (62%)] Loss: 6319.328125\n",
      "Train Epoch: 184 [143616/225000 (64%)] Loss: 6341.134766\n",
      "Train Epoch: 184 [147712/225000 (66%)] Loss: 6290.205078\n",
      "Train Epoch: 184 [151808/225000 (67%)] Loss: 6270.675781\n",
      "Train Epoch: 184 [155904/225000 (69%)] Loss: 6364.990234\n",
      "Train Epoch: 184 [160000/225000 (71%)] Loss: 6468.343750\n",
      "Train Epoch: 184 [164096/225000 (73%)] Loss: 6377.398438\n",
      "Train Epoch: 184 [168192/225000 (75%)] Loss: 6397.291016\n",
      "Train Epoch: 184 [172288/225000 (77%)] Loss: 6246.681641\n",
      "Train Epoch: 184 [176384/225000 (78%)] Loss: 6318.267578\n",
      "Train Epoch: 184 [180480/225000 (80%)] Loss: 6437.535156\n",
      "Train Epoch: 184 [184576/225000 (82%)] Loss: 6286.566406\n",
      "Train Epoch: 184 [188672/225000 (84%)] Loss: 6332.832031\n",
      "Train Epoch: 184 [192768/225000 (86%)] Loss: 6229.822266\n",
      "Train Epoch: 184 [196864/225000 (87%)] Loss: 6394.728516\n",
      "Train Epoch: 184 [200960/225000 (89%)] Loss: 6331.011719\n",
      "Train Epoch: 184 [205056/225000 (91%)] Loss: 6258.466797\n",
      "Train Epoch: 184 [209152/225000 (93%)] Loss: 6327.654297\n",
      "Train Epoch: 184 [213248/225000 (95%)] Loss: 6234.656250\n",
      "Train Epoch: 184 [217344/225000 (97%)] Loss: 6402.113281\n",
      "Train Epoch: 184 [221440/225000 (98%)] Loss: 6363.681641\n",
      "    epoch          : 184\n",
      "    loss           : 6337.985462661761\n",
      "    val_loss       : 6420.148002455429\n",
      "Train Epoch: 185 [256/225000 (0%)] Loss: 6249.490234\n",
      "Train Epoch: 185 [4352/225000 (2%)] Loss: 6378.585938\n",
      "Train Epoch: 185 [8448/225000 (4%)] Loss: 6389.087891\n",
      "Train Epoch: 185 [12544/225000 (6%)] Loss: 6425.539062\n",
      "Train Epoch: 185 [16640/225000 (7%)] Loss: 6227.087891\n",
      "Train Epoch: 185 [20736/225000 (9%)] Loss: 6306.134766\n",
      "Train Epoch: 185 [24832/225000 (11%)] Loss: 6307.763672\n",
      "Train Epoch: 185 [28928/225000 (13%)] Loss: 6168.414062\n",
      "Train Epoch: 185 [33024/225000 (15%)] Loss: 6275.240234\n",
      "Train Epoch: 185 [37120/225000 (16%)] Loss: 6513.363281\n",
      "Train Epoch: 185 [41216/225000 (18%)] Loss: 6273.001953\n",
      "Train Epoch: 185 [45312/225000 (20%)] Loss: 6302.662109\n",
      "Train Epoch: 185 [49408/225000 (22%)] Loss: 6288.130859\n",
      "Train Epoch: 185 [53504/225000 (24%)] Loss: 6389.933594\n",
      "Train Epoch: 185 [57600/225000 (26%)] Loss: 6322.964844\n",
      "Train Epoch: 185 [61696/225000 (27%)] Loss: 6212.421875\n",
      "Train Epoch: 185 [65792/225000 (29%)] Loss: 6343.931641\n",
      "Train Epoch: 185 [69888/225000 (31%)] Loss: 6265.511719\n",
      "Train Epoch: 185 [73984/225000 (33%)] Loss: 6347.351562\n",
      "Train Epoch: 185 [78080/225000 (35%)] Loss: 6301.404297\n",
      "Train Epoch: 185 [82176/225000 (37%)] Loss: 6405.992188\n",
      "Train Epoch: 185 [86272/225000 (38%)] Loss: 6313.115234\n",
      "Train Epoch: 185 [90368/225000 (40%)] Loss: 6318.220703\n",
      "Train Epoch: 185 [94464/225000 (42%)] Loss: 6424.507812\n",
      "Train Epoch: 185 [98560/225000 (44%)] Loss: 6373.283203\n",
      "Train Epoch: 185 [102656/225000 (46%)] Loss: 6356.486328\n",
      "Train Epoch: 185 [106752/225000 (47%)] Loss: 6489.808594\n",
      "Train Epoch: 185 [110848/225000 (49%)] Loss: 6331.605469\n",
      "Train Epoch: 185 [114944/225000 (51%)] Loss: 6375.451172\n",
      "Train Epoch: 185 [119040/225000 (53%)] Loss: 6340.916016\n",
      "Train Epoch: 185 [123136/225000 (55%)] Loss: 6314.548828\n",
      "Train Epoch: 185 [127232/225000 (57%)] Loss: 6346.746094\n",
      "Train Epoch: 185 [131328/225000 (58%)] Loss: 6314.330078\n",
      "Train Epoch: 185 [135424/225000 (60%)] Loss: 6302.970703\n",
      "Train Epoch: 185 [139520/225000 (62%)] Loss: 6345.048828\n",
      "Train Epoch: 185 [143616/225000 (64%)] Loss: 6288.007812\n",
      "Train Epoch: 185 [147712/225000 (66%)] Loss: 6359.128906\n",
      "Train Epoch: 185 [151808/225000 (67%)] Loss: 6398.818359\n",
      "Train Epoch: 185 [155904/225000 (69%)] Loss: 6417.779297\n",
      "Train Epoch: 185 [160000/225000 (71%)] Loss: 6260.314453\n",
      "Train Epoch: 185 [164096/225000 (73%)] Loss: 6287.033203\n",
      "Train Epoch: 185 [168192/225000 (75%)] Loss: 6301.767578\n",
      "Train Epoch: 185 [172288/225000 (77%)] Loss: 6489.410156\n",
      "Train Epoch: 185 [176384/225000 (78%)] Loss: 6270.503906\n",
      "Train Epoch: 185 [180480/225000 (80%)] Loss: 6331.496094\n",
      "Train Epoch: 185 [184576/225000 (82%)] Loss: 6283.328125\n",
      "Train Epoch: 185 [188672/225000 (84%)] Loss: 6337.185547\n",
      "Train Epoch: 185 [192768/225000 (86%)] Loss: 6291.158203\n",
      "Train Epoch: 185 [196864/225000 (87%)] Loss: 6336.355469\n",
      "Train Epoch: 185 [200960/225000 (89%)] Loss: 6311.437500\n",
      "Train Epoch: 185 [205056/225000 (91%)] Loss: 6399.562500\n",
      "Train Epoch: 185 [209152/225000 (93%)] Loss: 6394.712891\n",
      "Train Epoch: 185 [213248/225000 (95%)] Loss: 6304.968750\n",
      "Train Epoch: 185 [217344/225000 (97%)] Loss: 6332.509766\n",
      "Train Epoch: 185 [221440/225000 (98%)] Loss: 6343.220703\n",
      "    epoch          : 185\n",
      "    loss           : 6379.148109757181\n",
      "    val_loss       : 6438.5777798453155\n",
      "Train Epoch: 186 [256/225000 (0%)] Loss: 6389.632812\n",
      "Train Epoch: 186 [4352/225000 (2%)] Loss: 6384.410156\n",
      "Train Epoch: 186 [8448/225000 (4%)] Loss: 6301.562500\n",
      "Train Epoch: 186 [12544/225000 (6%)] Loss: 6248.578125\n",
      "Train Epoch: 186 [16640/225000 (7%)] Loss: 6315.812500\n",
      "Train Epoch: 186 [20736/225000 (9%)] Loss: 6278.013672\n",
      "Train Epoch: 186 [24832/225000 (11%)] Loss: 6383.972656\n",
      "Train Epoch: 186 [28928/225000 (13%)] Loss: 6174.714844\n",
      "Train Epoch: 186 [33024/225000 (15%)] Loss: 6361.835938\n",
      "Train Epoch: 186 [37120/225000 (16%)] Loss: 6305.550781\n",
      "Train Epoch: 186 [41216/225000 (18%)] Loss: 6274.474609\n",
      "Train Epoch: 186 [45312/225000 (20%)] Loss: 6297.296875\n",
      "Train Epoch: 186 [49408/225000 (22%)] Loss: 6360.275391\n",
      "Train Epoch: 186 [53504/225000 (24%)] Loss: 6391.093750\n",
      "Train Epoch: 186 [57600/225000 (26%)] Loss: 6395.453125\n",
      "Train Epoch: 186 [61696/225000 (27%)] Loss: 6373.773438\n",
      "Train Epoch: 186 [65792/225000 (29%)] Loss: 6202.677734\n",
      "Train Epoch: 186 [69888/225000 (31%)] Loss: 6358.330078\n",
      "Train Epoch: 186 [73984/225000 (33%)] Loss: 6264.408203\n",
      "Train Epoch: 186 [78080/225000 (35%)] Loss: 6288.099609\n",
      "Train Epoch: 186 [82176/225000 (37%)] Loss: 6224.580078\n",
      "Train Epoch: 186 [86272/225000 (38%)] Loss: 6362.015625\n",
      "Train Epoch: 186 [90368/225000 (40%)] Loss: 6368.646484\n",
      "Train Epoch: 186 [94464/225000 (42%)] Loss: 6364.390625\n",
      "Train Epoch: 186 [98560/225000 (44%)] Loss: 6336.632812\n",
      "Train Epoch: 186 [102656/225000 (46%)] Loss: 6307.324219\n",
      "Train Epoch: 186 [106752/225000 (47%)] Loss: 6285.763672\n",
      "Train Epoch: 186 [110848/225000 (49%)] Loss: 6313.304688\n",
      "Train Epoch: 186 [114944/225000 (51%)] Loss: 6287.480469\n",
      "Train Epoch: 186 [119040/225000 (53%)] Loss: 6300.595703\n",
      "Train Epoch: 186 [123136/225000 (55%)] Loss: 6206.255859\n",
      "Train Epoch: 186 [127232/225000 (57%)] Loss: 6231.021484\n",
      "Train Epoch: 186 [131328/225000 (58%)] Loss: 6363.230469\n",
      "Train Epoch: 186 [135424/225000 (60%)] Loss: 6312.544922\n",
      "Train Epoch: 186 [139520/225000 (62%)] Loss: 6278.843750\n",
      "Train Epoch: 186 [143616/225000 (64%)] Loss: 6303.410156\n",
      "Train Epoch: 186 [147712/225000 (66%)] Loss: 6359.222656\n",
      "Train Epoch: 186 [151808/225000 (67%)] Loss: 6360.501953\n",
      "Train Epoch: 186 [155904/225000 (69%)] Loss: 6209.798828\n",
      "Train Epoch: 186 [160000/225000 (71%)] Loss: 6357.566406\n",
      "Train Epoch: 186 [164096/225000 (73%)] Loss: 6318.730469\n",
      "Train Epoch: 186 [168192/225000 (75%)] Loss: 6218.201172\n",
      "Train Epoch: 186 [172288/225000 (77%)] Loss: 6284.933594\n",
      "Train Epoch: 186 [176384/225000 (78%)] Loss: 6297.587891\n",
      "Train Epoch: 186 [180480/225000 (80%)] Loss: 6240.462891\n",
      "Train Epoch: 186 [184576/225000 (82%)] Loss: 6305.888672\n",
      "Train Epoch: 186 [188672/225000 (84%)] Loss: 6353.843750\n",
      "Train Epoch: 186 [192768/225000 (86%)] Loss: 6251.908203\n",
      "Train Epoch: 186 [196864/225000 (87%)] Loss: 6330.447266\n",
      "Train Epoch: 186 [200960/225000 (89%)] Loss: 6340.708984\n",
      "Train Epoch: 186 [205056/225000 (91%)] Loss: 6298.441406\n",
      "Train Epoch: 186 [209152/225000 (93%)] Loss: 6181.718750\n",
      "Train Epoch: 186 [213248/225000 (95%)] Loss: 6300.492188\n",
      "Train Epoch: 186 [217344/225000 (97%)] Loss: 6407.978516\n",
      "Train Epoch: 186 [221440/225000 (98%)] Loss: 6270.798828\n",
      "    epoch          : 186\n",
      "    loss           : 6387.183962599545\n",
      "    val_loss       : 6506.487796418521\n",
      "Train Epoch: 187 [256/225000 (0%)] Loss: 6324.337891\n",
      "Train Epoch: 187 [4352/225000 (2%)] Loss: 6275.265625\n",
      "Train Epoch: 187 [8448/225000 (4%)] Loss: 6322.949219\n",
      "Train Epoch: 187 [12544/225000 (6%)] Loss: 6270.302734\n",
      "Train Epoch: 187 [16640/225000 (7%)] Loss: 6137.134766\n",
      "Train Epoch: 187 [20736/225000 (9%)] Loss: 6282.521484\n",
      "Train Epoch: 187 [24832/225000 (11%)] Loss: 6344.205078\n",
      "Train Epoch: 187 [28928/225000 (13%)] Loss: 6354.025391\n",
      "Train Epoch: 187 [33024/225000 (15%)] Loss: 6315.517578\n",
      "Train Epoch: 187 [37120/225000 (16%)] Loss: 6285.177734\n",
      "Train Epoch: 187 [41216/225000 (18%)] Loss: 6315.015625\n",
      "Train Epoch: 187 [45312/225000 (20%)] Loss: 6359.123047\n",
      "Train Epoch: 187 [49408/225000 (22%)] Loss: 6276.791016\n",
      "Train Epoch: 187 [53504/225000 (24%)] Loss: 6242.320312\n",
      "Train Epoch: 187 [57600/225000 (26%)] Loss: 6400.898438\n",
      "Train Epoch: 187 [61696/225000 (27%)] Loss: 6282.681641\n",
      "Train Epoch: 187 [65792/225000 (29%)] Loss: 6381.259766\n",
      "Train Epoch: 187 [69888/225000 (31%)] Loss: 6329.988281\n",
      "Train Epoch: 187 [73984/225000 (33%)] Loss: 6208.050781\n",
      "Train Epoch: 187 [78080/225000 (35%)] Loss: 6354.066406\n",
      "Train Epoch: 187 [82176/225000 (37%)] Loss: 6231.367188\n",
      "Train Epoch: 187 [86272/225000 (38%)] Loss: 6312.677734\n",
      "Train Epoch: 187 [90368/225000 (40%)] Loss: 6365.738281\n",
      "Train Epoch: 187 [94464/225000 (42%)] Loss: 6343.300781\n",
      "Train Epoch: 187 [98560/225000 (44%)] Loss: 6326.675781\n",
      "Train Epoch: 187 [102656/225000 (46%)] Loss: 6397.830078\n",
      "Train Epoch: 187 [106752/225000 (47%)] Loss: 6411.244141\n",
      "Train Epoch: 187 [110848/225000 (49%)] Loss: 6399.470703\n",
      "Train Epoch: 187 [114944/225000 (51%)] Loss: 6397.578125\n",
      "Train Epoch: 187 [119040/225000 (53%)] Loss: 6328.115234\n",
      "Train Epoch: 187 [123136/225000 (55%)] Loss: 6437.320312\n",
      "Train Epoch: 187 [127232/225000 (57%)] Loss: 6345.017578\n",
      "Train Epoch: 187 [131328/225000 (58%)] Loss: 6237.849609\n",
      "Train Epoch: 187 [135424/225000 (60%)] Loss: 6427.548828\n",
      "Train Epoch: 187 [139520/225000 (62%)] Loss: 6342.796875\n",
      "Train Epoch: 187 [143616/225000 (64%)] Loss: 6287.585938\n",
      "Train Epoch: 187 [147712/225000 (66%)] Loss: 6397.158203\n",
      "Train Epoch: 187 [151808/225000 (67%)] Loss: 6314.439453\n",
      "Train Epoch: 187 [155904/225000 (69%)] Loss: 6311.968750\n",
      "Train Epoch: 187 [160000/225000 (71%)] Loss: 6316.419922\n",
      "Train Epoch: 187 [164096/225000 (73%)] Loss: 6359.814453\n",
      "Train Epoch: 187 [168192/225000 (75%)] Loss: 6257.632812\n",
      "Train Epoch: 187 [172288/225000 (77%)] Loss: 6375.054688\n",
      "Train Epoch: 187 [176384/225000 (78%)] Loss: 6442.121094\n",
      "Train Epoch: 187 [180480/225000 (80%)] Loss: 6279.906250\n",
      "Train Epoch: 187 [184576/225000 (82%)] Loss: 6290.365234\n",
      "Train Epoch: 187 [188672/225000 (84%)] Loss: 6384.355469\n",
      "Train Epoch: 187 [192768/225000 (86%)] Loss: 6397.228516\n",
      "Train Epoch: 187 [196864/225000 (87%)] Loss: 6344.375000\n",
      "Train Epoch: 187 [200960/225000 (89%)] Loss: 6365.792969\n",
      "Train Epoch: 187 [205056/225000 (91%)] Loss: 6431.019531\n",
      "Train Epoch: 187 [209152/225000 (93%)] Loss: 6368.847656\n",
      "Train Epoch: 187 [213248/225000 (95%)] Loss: 6403.070312\n",
      "Train Epoch: 187 [217344/225000 (97%)] Loss: 6396.732422\n",
      "Train Epoch: 187 [221440/225000 (98%)] Loss: 6406.308594\n",
      "    epoch          : 187\n",
      "    loss           : 6402.776728260097\n",
      "    val_loss       : 6401.240526007146\n",
      "Train Epoch: 188 [256/225000 (0%)] Loss: 6397.658203\n",
      "Train Epoch: 188 [4352/225000 (2%)] Loss: 6362.802734\n",
      "Train Epoch: 188 [8448/225000 (4%)] Loss: 6451.503906\n",
      "Train Epoch: 188 [12544/225000 (6%)] Loss: 6266.966797\n",
      "Train Epoch: 188 [16640/225000 (7%)] Loss: 6270.062500\n",
      "Train Epoch: 188 [20736/225000 (9%)] Loss: 6343.474609\n",
      "Train Epoch: 188 [24832/225000 (11%)] Loss: 6431.203125\n",
      "Train Epoch: 188 [28928/225000 (13%)] Loss: 6465.646484\n",
      "Train Epoch: 188 [33024/225000 (15%)] Loss: 6469.371094\n",
      "Train Epoch: 188 [37120/225000 (16%)] Loss: 6420.613281\n",
      "Train Epoch: 188 [41216/225000 (18%)] Loss: 6347.990234\n",
      "Train Epoch: 188 [45312/225000 (20%)] Loss: 6300.060547\n",
      "Train Epoch: 188 [49408/225000 (22%)] Loss: 6358.332031\n",
      "Train Epoch: 188 [53504/225000 (24%)] Loss: 6458.777344\n",
      "Train Epoch: 188 [57600/225000 (26%)] Loss: 6237.921875\n",
      "Train Epoch: 188 [61696/225000 (27%)] Loss: 6368.306641\n",
      "Train Epoch: 188 [65792/225000 (29%)] Loss: 6222.486328\n",
      "Train Epoch: 188 [69888/225000 (31%)] Loss: 6335.730469\n",
      "Train Epoch: 188 [73984/225000 (33%)] Loss: 6253.513672\n",
      "Train Epoch: 188 [78080/225000 (35%)] Loss: 6335.562500\n",
      "Train Epoch: 188 [82176/225000 (37%)] Loss: 6202.437500\n",
      "Train Epoch: 188 [86272/225000 (38%)] Loss: 6336.689453\n",
      "Train Epoch: 188 [90368/225000 (40%)] Loss: 6236.021484\n",
      "Train Epoch: 188 [94464/225000 (42%)] Loss: 6328.070312\n",
      "Train Epoch: 188 [98560/225000 (44%)] Loss: 6445.070312\n",
      "Train Epoch: 188 [102656/225000 (46%)] Loss: 6361.541016\n",
      "Train Epoch: 188 [106752/225000 (47%)] Loss: 6324.160156\n",
      "Train Epoch: 188 [110848/225000 (49%)] Loss: 6402.537109\n",
      "Train Epoch: 188 [114944/225000 (51%)] Loss: 6202.349609\n",
      "Train Epoch: 188 [119040/225000 (53%)] Loss: 6336.326172\n",
      "Train Epoch: 188 [123136/225000 (55%)] Loss: 6391.505859\n",
      "Train Epoch: 188 [127232/225000 (57%)] Loss: 6307.042969\n",
      "Train Epoch: 188 [131328/225000 (58%)] Loss: 6317.285156\n",
      "Train Epoch: 188 [135424/225000 (60%)] Loss: 6239.107422\n",
      "Train Epoch: 188 [139520/225000 (62%)] Loss: 6323.496094\n",
      "Train Epoch: 188 [143616/225000 (64%)] Loss: 6421.265625\n",
      "Train Epoch: 188 [147712/225000 (66%)] Loss: 6312.275391\n",
      "Train Epoch: 188 [151808/225000 (67%)] Loss: 6411.226562\n",
      "Train Epoch: 188 [155904/225000 (69%)] Loss: 6405.628906\n",
      "Train Epoch: 188 [160000/225000 (71%)] Loss: 6460.238281\n",
      "Train Epoch: 188 [164096/225000 (73%)] Loss: 6364.462891\n",
      "Train Epoch: 188 [168192/225000 (75%)] Loss: 6144.261719\n",
      "Train Epoch: 188 [172288/225000 (77%)] Loss: 6324.949219\n",
      "Train Epoch: 188 [176384/225000 (78%)] Loss: 6389.029297\n",
      "Train Epoch: 188 [180480/225000 (80%)] Loss: 6260.101562\n",
      "Train Epoch: 188 [184576/225000 (82%)] Loss: 6263.644531\n",
      "Train Epoch: 188 [188672/225000 (84%)] Loss: 6344.025391\n",
      "Train Epoch: 188 [192768/225000 (86%)] Loss: 6217.677734\n",
      "Train Epoch: 188 [196864/225000 (87%)] Loss: 6351.501953\n",
      "Train Epoch: 188 [200960/225000 (89%)] Loss: 6291.703125\n",
      "Train Epoch: 188 [205056/225000 (91%)] Loss: 6274.437500\n",
      "Train Epoch: 188 [209152/225000 (93%)] Loss: 6409.568359\n",
      "Train Epoch: 188 [213248/225000 (95%)] Loss: 6280.253906\n",
      "Train Epoch: 188 [217344/225000 (97%)] Loss: 6479.744141\n",
      "Train Epoch: 188 [221440/225000 (98%)] Loss: 6190.992188\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "    epoch          : 188\n",
      "    loss           : 6345.884534538538\n",
      "    val_loss       : 6401.22408446852\n",
      "Train Epoch: 189 [256/225000 (0%)] Loss: 6324.646484\n",
      "Train Epoch: 189 [4352/225000 (2%)] Loss: 6239.433594\n",
      "Train Epoch: 189 [8448/225000 (4%)] Loss: 6438.273438\n",
      "Train Epoch: 189 [12544/225000 (6%)] Loss: 6275.716797\n",
      "Train Epoch: 189 [16640/225000 (7%)] Loss: 6353.683594\n",
      "Train Epoch: 189 [20736/225000 (9%)] Loss: 6311.011719\n",
      "Train Epoch: 189 [24832/225000 (11%)] Loss: 26727.099609\n",
      "Train Epoch: 189 [28928/225000 (13%)] Loss: 6414.285156\n",
      "Train Epoch: 189 [33024/225000 (15%)] Loss: 6296.380859\n",
      "Train Epoch: 189 [37120/225000 (16%)] Loss: 6376.451172\n",
      "Train Epoch: 189 [41216/225000 (18%)] Loss: 6430.457031\n",
      "Train Epoch: 189 [45312/225000 (20%)] Loss: 6310.017578\n",
      "Train Epoch: 189 [49408/225000 (22%)] Loss: 6317.343750\n",
      "Train Epoch: 189 [53504/225000 (24%)] Loss: 6413.863281\n",
      "Train Epoch: 189 [57600/225000 (26%)] Loss: 6259.529297\n",
      "Train Epoch: 189 [61696/225000 (27%)] Loss: 6335.146484\n",
      "Train Epoch: 189 [65792/225000 (29%)] Loss: 6329.273438\n",
      "Train Epoch: 189 [69888/225000 (31%)] Loss: 6295.310547\n",
      "Train Epoch: 189 [73984/225000 (33%)] Loss: 6284.476562\n",
      "Train Epoch: 189 [78080/225000 (35%)] Loss: 6206.921875\n",
      "Train Epoch: 189 [82176/225000 (37%)] Loss: 6366.804688\n",
      "Train Epoch: 189 [86272/225000 (38%)] Loss: 6327.349609\n",
      "Train Epoch: 189 [90368/225000 (40%)] Loss: 6271.300781\n",
      "Train Epoch: 189 [94464/225000 (42%)] Loss: 6413.835938\n",
      "Train Epoch: 189 [98560/225000 (44%)] Loss: 6328.525391\n",
      "Train Epoch: 189 [102656/225000 (46%)] Loss: 6349.216797\n",
      "Train Epoch: 189 [106752/225000 (47%)] Loss: 6260.658203\n",
      "Train Epoch: 189 [110848/225000 (49%)] Loss: 6294.453125\n",
      "Train Epoch: 189 [114944/225000 (51%)] Loss: 6334.400391\n",
      "Train Epoch: 189 [119040/225000 (53%)] Loss: 6213.156250\n",
      "Train Epoch: 189 [123136/225000 (55%)] Loss: 6440.800781\n",
      "Train Epoch: 189 [127232/225000 (57%)] Loss: 6339.652344\n",
      "Train Epoch: 189 [131328/225000 (58%)] Loss: 6230.867188\n",
      "Train Epoch: 189 [135424/225000 (60%)] Loss: 6356.675781\n",
      "Train Epoch: 189 [139520/225000 (62%)] Loss: 6324.064453\n",
      "Train Epoch: 189 [143616/225000 (64%)] Loss: 6320.308594\n",
      "Train Epoch: 189 [147712/225000 (66%)] Loss: 6221.822266\n",
      "Train Epoch: 189 [151808/225000 (67%)] Loss: 6318.378906\n",
      "Train Epoch: 189 [155904/225000 (69%)] Loss: 6468.769531\n",
      "Train Epoch: 189 [160000/225000 (71%)] Loss: 6436.472656\n",
      "Train Epoch: 189 [164096/225000 (73%)] Loss: 6307.382812\n",
      "Train Epoch: 189 [168192/225000 (75%)] Loss: 6353.554688\n",
      "Train Epoch: 189 [172288/225000 (77%)] Loss: 6346.410156\n",
      "Train Epoch: 189 [176384/225000 (78%)] Loss: 6300.273438\n",
      "Train Epoch: 189 [180480/225000 (80%)] Loss: 6356.882812\n",
      "Train Epoch: 189 [184576/225000 (82%)] Loss: 6366.470703\n",
      "Train Epoch: 189 [188672/225000 (84%)] Loss: 6221.210938\n",
      "Train Epoch: 189 [192768/225000 (86%)] Loss: 6341.021484\n",
      "Train Epoch: 189 [196864/225000 (87%)] Loss: 6038.460938\n",
      "Train Epoch: 189 [200960/225000 (89%)] Loss: 6317.855469\n",
      "Train Epoch: 189 [205056/225000 (91%)] Loss: 6406.554688\n",
      "Train Epoch: 189 [209152/225000 (93%)] Loss: 6214.121094\n",
      "Train Epoch: 189 [213248/225000 (95%)] Loss: 6341.033203\n",
      "Train Epoch: 189 [217344/225000 (97%)] Loss: 6269.937500\n",
      "Train Epoch: 189 [221440/225000 (98%)] Loss: 6308.582031\n",
      "    epoch          : 189\n",
      "    loss           : 6373.140967185723\n",
      "    val_loss       : 6401.228575039883\n",
      "Train Epoch: 190 [256/225000 (0%)] Loss: 6376.007812\n",
      "Train Epoch: 190 [4352/225000 (2%)] Loss: 6471.994141\n",
      "Train Epoch: 190 [8448/225000 (4%)] Loss: 6483.705078\n",
      "Train Epoch: 190 [12544/225000 (6%)] Loss: 6319.828125\n",
      "Train Epoch: 190 [16640/225000 (7%)] Loss: 6271.687500\n",
      "Train Epoch: 190 [20736/225000 (9%)] Loss: 6374.087891\n",
      "Train Epoch: 190 [24832/225000 (11%)] Loss: 6322.537109\n",
      "Train Epoch: 190 [28928/225000 (13%)] Loss: 6438.228516\n",
      "Train Epoch: 190 [33024/225000 (15%)] Loss: 6379.964844\n",
      "Train Epoch: 190 [37120/225000 (16%)] Loss: 6409.447266\n",
      "Train Epoch: 190 [41216/225000 (18%)] Loss: 6246.742188\n",
      "Train Epoch: 190 [45312/225000 (20%)] Loss: 6294.691406\n",
      "Train Epoch: 190 [49408/225000 (22%)] Loss: 6361.078125\n",
      "Train Epoch: 190 [53504/225000 (24%)] Loss: 6459.541016\n",
      "Train Epoch: 190 [57600/225000 (26%)] Loss: 6219.042969\n",
      "Train Epoch: 190 [61696/225000 (27%)] Loss: 6354.492188\n",
      "Train Epoch: 190 [65792/225000 (29%)] Loss: 6317.453125\n",
      "Train Epoch: 190 [69888/225000 (31%)] Loss: 6309.298828\n",
      "Train Epoch: 190 [73984/225000 (33%)] Loss: 6224.894531\n",
      "Train Epoch: 190 [78080/225000 (35%)] Loss: 6241.205078\n",
      "Train Epoch: 190 [82176/225000 (37%)] Loss: 6435.375000\n",
      "Train Epoch: 190 [86272/225000 (38%)] Loss: 6376.970703\n",
      "Train Epoch: 190 [90368/225000 (40%)] Loss: 6477.333984\n",
      "Train Epoch: 190 [94464/225000 (42%)] Loss: 6356.435547\n",
      "Train Epoch: 190 [98560/225000 (44%)] Loss: 6350.652344\n",
      "Train Epoch: 190 [102656/225000 (46%)] Loss: 6359.044922\n",
      "Train Epoch: 190 [106752/225000 (47%)] Loss: 6363.917969\n",
      "Train Epoch: 190 [110848/225000 (49%)] Loss: 6547.142578\n",
      "Train Epoch: 190 [114944/225000 (51%)] Loss: 6306.826172\n",
      "Train Epoch: 190 [119040/225000 (53%)] Loss: 6248.728516\n",
      "Train Epoch: 190 [123136/225000 (55%)] Loss: 6298.900391\n",
      "Train Epoch: 190 [127232/225000 (57%)] Loss: 6245.482422\n",
      "Train Epoch: 190 [131328/225000 (58%)] Loss: 6235.662109\n",
      "Train Epoch: 190 [135424/225000 (60%)] Loss: 6465.714844\n",
      "Train Epoch: 190 [139520/225000 (62%)] Loss: 6315.792969\n",
      "Train Epoch: 190 [143616/225000 (64%)] Loss: 6315.515625\n",
      "Train Epoch: 190 [147712/225000 (66%)] Loss: 6287.777344\n",
      "Train Epoch: 190 [151808/225000 (67%)] Loss: 6233.689453\n",
      "Train Epoch: 190 [155904/225000 (69%)] Loss: 6289.449219\n",
      "Train Epoch: 190 [160000/225000 (71%)] Loss: 6484.335938\n",
      "Train Epoch: 190 [164096/225000 (73%)] Loss: 6301.496094\n",
      "Train Epoch: 190 [168192/225000 (75%)] Loss: 6190.242188\n",
      "Train Epoch: 190 [172288/225000 (77%)] Loss: 6330.513672\n",
      "Train Epoch: 190 [176384/225000 (78%)] Loss: 6364.662109\n",
      "Train Epoch: 190 [180480/225000 (80%)] Loss: 6275.208984\n",
      "Train Epoch: 190 [184576/225000 (82%)] Loss: 6298.621094\n",
      "Train Epoch: 190 [188672/225000 (84%)] Loss: 6300.994141\n",
      "Train Epoch: 190 [192768/225000 (86%)] Loss: 6308.472656\n",
      "Train Epoch: 190 [196864/225000 (87%)] Loss: 6233.316406\n",
      "Train Epoch: 190 [200960/225000 (89%)] Loss: 6427.105469\n",
      "Train Epoch: 190 [205056/225000 (91%)] Loss: 6311.052734\n",
      "Train Epoch: 190 [209152/225000 (93%)] Loss: 6336.419922\n",
      "Train Epoch: 190 [213248/225000 (95%)] Loss: 6284.107422\n",
      "Train Epoch: 190 [217344/225000 (97%)] Loss: 6376.794922\n",
      "Train Epoch: 190 [221440/225000 (98%)] Loss: 6418.638672\n",
      "    epoch          : 190\n",
      "    loss           : 6417.58504203996\n",
      "    val_loss       : 6463.582406509896\n",
      "Train Epoch: 191 [256/225000 (0%)] Loss: 6373.437500\n",
      "Train Epoch: 191 [4352/225000 (2%)] Loss: 6416.615234\n",
      "Train Epoch: 191 [8448/225000 (4%)] Loss: 6420.968750\n",
      "Train Epoch: 191 [12544/225000 (6%)] Loss: 6155.195312\n",
      "Train Epoch: 191 [16640/225000 (7%)] Loss: 6175.359375\n",
      "Train Epoch: 191 [20736/225000 (9%)] Loss: 6448.621094\n",
      "Train Epoch: 191 [24832/225000 (11%)] Loss: 6223.566406\n",
      "Train Epoch: 191 [28928/225000 (13%)] Loss: 6290.181641\n",
      "Train Epoch: 191 [33024/225000 (15%)] Loss: 6374.585938\n",
      "Train Epoch: 191 [37120/225000 (16%)] Loss: 6497.201172\n",
      "Train Epoch: 191 [41216/225000 (18%)] Loss: 6372.396484\n",
      "Train Epoch: 191 [45312/225000 (20%)] Loss: 6259.748047\n",
      "Train Epoch: 191 [49408/225000 (22%)] Loss: 6242.898438\n",
      "Train Epoch: 191 [53504/225000 (24%)] Loss: 6289.900391\n",
      "Train Epoch: 191 [57600/225000 (26%)] Loss: 6220.160156\n",
      "Train Epoch: 191 [61696/225000 (27%)] Loss: 6416.644531\n",
      "Train Epoch: 191 [65792/225000 (29%)] Loss: 6180.121094\n",
      "Train Epoch: 191 [69888/225000 (31%)] Loss: 6374.677734\n",
      "Train Epoch: 191 [73984/225000 (33%)] Loss: 6298.738281\n",
      "Train Epoch: 191 [78080/225000 (35%)] Loss: 6229.777344\n",
      "Train Epoch: 191 [82176/225000 (37%)] Loss: 6341.400391\n",
      "Train Epoch: 191 [86272/225000 (38%)] Loss: 6262.623047\n",
      "Train Epoch: 191 [90368/225000 (40%)] Loss: 6428.720703\n",
      "Train Epoch: 191 [94464/225000 (42%)] Loss: 6419.833984\n",
      "Train Epoch: 191 [98560/225000 (44%)] Loss: 6422.746094\n",
      "Train Epoch: 191 [102656/225000 (46%)] Loss: 6270.621094\n",
      "Train Epoch: 191 [106752/225000 (47%)] Loss: 6314.712891\n",
      "Train Epoch: 191 [110848/225000 (49%)] Loss: 6321.548828\n",
      "Train Epoch: 191 [114944/225000 (51%)] Loss: 6240.466797\n",
      "Train Epoch: 191 [119040/225000 (53%)] Loss: 6280.570312\n",
      "Train Epoch: 191 [123136/225000 (55%)] Loss: 6281.792969\n",
      "Train Epoch: 191 [127232/225000 (57%)] Loss: 6328.589844\n",
      "Train Epoch: 191 [131328/225000 (58%)] Loss: 6282.365234\n",
      "Train Epoch: 191 [135424/225000 (60%)] Loss: 6132.443359\n",
      "Train Epoch: 191 [139520/225000 (62%)] Loss: 6325.714844\n",
      "Train Epoch: 191 [143616/225000 (64%)] Loss: 6451.373047\n",
      "Train Epoch: 191 [147712/225000 (66%)] Loss: 6375.773438\n",
      "Train Epoch: 191 [151808/225000 (67%)] Loss: 6472.890625\n",
      "Train Epoch: 191 [155904/225000 (69%)] Loss: 6388.875000\n",
      "Train Epoch: 191 [160000/225000 (71%)] Loss: 6362.294922\n",
      "Train Epoch: 191 [164096/225000 (73%)] Loss: 6332.099609\n",
      "Train Epoch: 191 [168192/225000 (75%)] Loss: 6273.154297\n",
      "Train Epoch: 191 [172288/225000 (77%)] Loss: 6475.328125\n",
      "Train Epoch: 191 [176384/225000 (78%)] Loss: 6325.960938\n",
      "Train Epoch: 191 [180480/225000 (80%)] Loss: 6203.390625\n",
      "Train Epoch: 191 [184576/225000 (82%)] Loss: 6448.029297\n",
      "Train Epoch: 191 [188672/225000 (84%)] Loss: 6378.541016\n",
      "Train Epoch: 191 [192768/225000 (86%)] Loss: 6456.464844\n",
      "Train Epoch: 191 [196864/225000 (87%)] Loss: 6397.224609\n",
      "Train Epoch: 191 [200960/225000 (89%)] Loss: 6376.287109\n",
      "Train Epoch: 191 [205056/225000 (91%)] Loss: 6346.884766\n",
      "Train Epoch: 191 [209152/225000 (93%)] Loss: 6433.855469\n",
      "Train Epoch: 191 [213248/225000 (95%)] Loss: 6408.982422\n",
      "Train Epoch: 191 [217344/225000 (97%)] Loss: 6277.427734\n",
      "Train Epoch: 191 [221440/225000 (98%)] Loss: 6425.320312\n",
      "    epoch          : 191\n",
      "    loss           : 6364.9145613356795\n",
      "    val_loss       : 6627.517396853895\n",
      "Train Epoch: 192 [256/225000 (0%)] Loss: 6536.804688\n",
      "Train Epoch: 192 [4352/225000 (2%)] Loss: 6187.390625\n",
      "Train Epoch: 192 [8448/225000 (4%)] Loss: 6234.101562\n",
      "Train Epoch: 192 [12544/225000 (6%)] Loss: 6281.986328\n",
      "Train Epoch: 192 [16640/225000 (7%)] Loss: 6287.939453\n",
      "Train Epoch: 192 [20736/225000 (9%)] Loss: 6298.531250\n",
      "Train Epoch: 192 [24832/225000 (11%)] Loss: 6429.013672\n",
      "Train Epoch: 192 [28928/225000 (13%)] Loss: 6233.675781\n",
      "Train Epoch: 192 [33024/225000 (15%)] Loss: 6391.199219\n",
      "Train Epoch: 192 [37120/225000 (16%)] Loss: 6265.927734\n",
      "Train Epoch: 192 [41216/225000 (18%)] Loss: 6414.958984\n",
      "Train Epoch: 192 [45312/225000 (20%)] Loss: 6493.242188\n",
      "Train Epoch: 192 [49408/225000 (22%)] Loss: 6593.994141\n",
      "Train Epoch: 192 [53504/225000 (24%)] Loss: 6423.673828\n",
      "Train Epoch: 192 [57600/225000 (26%)] Loss: 6303.962891\n",
      "Train Epoch: 192 [61696/225000 (27%)] Loss: 6285.964844\n",
      "Train Epoch: 192 [65792/225000 (29%)] Loss: 6430.474609\n",
      "Train Epoch: 192 [69888/225000 (31%)] Loss: 6267.355469\n",
      "Train Epoch: 192 [73984/225000 (33%)] Loss: 6454.740234\n",
      "Train Epoch: 192 [78080/225000 (35%)] Loss: 6330.003906\n",
      "Train Epoch: 192 [82176/225000 (37%)] Loss: 6368.074219\n",
      "Train Epoch: 192 [86272/225000 (38%)] Loss: 6392.902344\n",
      "Train Epoch: 192 [90368/225000 (40%)] Loss: 6322.501953\n",
      "Train Epoch: 192 [94464/225000 (42%)] Loss: 6337.269531\n",
      "Train Epoch: 192 [98560/225000 (44%)] Loss: 6332.582031\n",
      "Train Epoch: 192 [102656/225000 (46%)] Loss: 6495.250000\n",
      "Train Epoch: 192 [106752/225000 (47%)] Loss: 6245.351562\n",
      "Train Epoch: 192 [110848/225000 (49%)] Loss: 6333.914062\n",
      "Train Epoch: 192 [114944/225000 (51%)] Loss: 6346.023438\n",
      "Train Epoch: 192 [119040/225000 (53%)] Loss: 6240.177734\n",
      "Train Epoch: 192 [123136/225000 (55%)] Loss: 6429.806641\n",
      "Train Epoch: 192 [127232/225000 (57%)] Loss: 6335.992188\n",
      "Train Epoch: 192 [131328/225000 (58%)] Loss: 6417.880859\n",
      "Train Epoch: 192 [135424/225000 (60%)] Loss: 6294.449219\n",
      "Train Epoch: 192 [139520/225000 (62%)] Loss: 6435.478516\n",
      "Train Epoch: 192 [143616/225000 (64%)] Loss: 6322.087891\n",
      "Train Epoch: 192 [147712/225000 (66%)] Loss: 6264.843750\n",
      "Train Epoch: 192 [151808/225000 (67%)] Loss: 6408.929688\n",
      "Train Epoch: 192 [155904/225000 (69%)] Loss: 6244.712891\n",
      "Train Epoch: 192 [160000/225000 (71%)] Loss: 6266.228516\n",
      "Train Epoch: 192 [164096/225000 (73%)] Loss: 6401.826172\n",
      "Train Epoch: 192 [168192/225000 (75%)] Loss: 6446.712891\n",
      "Train Epoch: 192 [172288/225000 (77%)] Loss: 6356.871094\n",
      "Train Epoch: 192 [176384/225000 (78%)] Loss: 6368.511719\n",
      "Train Epoch: 192 [180480/225000 (80%)] Loss: 6275.240234\n",
      "Train Epoch: 192 [184576/225000 (82%)] Loss: 6405.562500\n",
      "Train Epoch: 192 [188672/225000 (84%)] Loss: 6395.783203\n",
      "Train Epoch: 192 [192768/225000 (86%)] Loss: 6293.572266\n",
      "Train Epoch: 192 [196864/225000 (87%)] Loss: 6294.000000\n",
      "Train Epoch: 192 [200960/225000 (89%)] Loss: 6457.583984\n",
      "Train Epoch: 192 [205056/225000 (91%)] Loss: 6364.480469\n",
      "Train Epoch: 192 [209152/225000 (93%)] Loss: 6378.476562\n",
      "Train Epoch: 192 [213248/225000 (95%)] Loss: 6311.031250\n",
      "Train Epoch: 192 [217344/225000 (97%)] Loss: 6259.955078\n",
      "Train Epoch: 192 [221440/225000 (98%)] Loss: 6306.068359\n",
      "    epoch          : 192\n",
      "    loss           : 6377.176701151877\n",
      "    val_loss       : 6412.70168135604\n",
      "Train Epoch: 193 [256/225000 (0%)] Loss: 6323.412109\n",
      "Train Epoch: 193 [4352/225000 (2%)] Loss: 6410.896484\n",
      "Train Epoch: 193 [8448/225000 (4%)] Loss: 6346.144531\n",
      "Train Epoch: 193 [12544/225000 (6%)] Loss: 6269.390625\n",
      "Train Epoch: 193 [16640/225000 (7%)] Loss: 6411.683594\n",
      "Train Epoch: 193 [20736/225000 (9%)] Loss: 6296.291016\n",
      "Train Epoch: 193 [24832/225000 (11%)] Loss: 6376.445312\n",
      "Train Epoch: 193 [28928/225000 (13%)] Loss: 6363.906250\n",
      "Train Epoch: 193 [33024/225000 (15%)] Loss: 6395.068359\n",
      "Train Epoch: 193 [37120/225000 (16%)] Loss: 6274.707031\n",
      "Train Epoch: 193 [41216/225000 (18%)] Loss: 6335.900391\n",
      "Train Epoch: 193 [45312/225000 (20%)] Loss: 6362.595703\n",
      "Train Epoch: 193 [49408/225000 (22%)] Loss: 6257.238281\n",
      "Train Epoch: 193 [53504/225000 (24%)] Loss: 6284.738281\n",
      "Train Epoch: 193 [57600/225000 (26%)] Loss: 12516.923828\n",
      "Train Epoch: 193 [61696/225000 (27%)] Loss: 6259.953125\n",
      "Train Epoch: 193 [65792/225000 (29%)] Loss: 6413.189453\n",
      "Train Epoch: 193 [69888/225000 (31%)] Loss: 6404.685547\n",
      "Train Epoch: 193 [73984/225000 (33%)] Loss: 6273.439453\n",
      "Train Epoch: 193 [78080/225000 (35%)] Loss: 6264.281250\n",
      "Train Epoch: 193 [82176/225000 (37%)] Loss: 6308.740234\n",
      "Train Epoch: 193 [86272/225000 (38%)] Loss: 6585.097656\n",
      "Train Epoch: 193 [90368/225000 (40%)] Loss: 6407.757812\n",
      "Train Epoch: 193 [94464/225000 (42%)] Loss: 8210.392578\n",
      "Train Epoch: 193 [98560/225000 (44%)] Loss: 6206.326172\n",
      "Train Epoch: 193 [102656/225000 (46%)] Loss: 6292.125000\n",
      "Train Epoch: 193 [106752/225000 (47%)] Loss: 6229.464844\n",
      "Train Epoch: 193 [110848/225000 (49%)] Loss: 6249.583984\n",
      "Train Epoch: 193 [114944/225000 (51%)] Loss: 6369.658203\n",
      "Train Epoch: 193 [119040/225000 (53%)] Loss: 6414.787109\n",
      "Train Epoch: 193 [123136/225000 (55%)] Loss: 6329.771484\n",
      "Train Epoch: 193 [127232/225000 (57%)] Loss: 6308.398438\n",
      "Train Epoch: 193 [131328/225000 (58%)] Loss: 6323.523438\n",
      "Train Epoch: 193 [135424/225000 (60%)] Loss: 6190.162109\n",
      "Train Epoch: 193 [139520/225000 (62%)] Loss: 6212.464844\n",
      "Train Epoch: 193 [143616/225000 (64%)] Loss: 6305.835938\n",
      "Train Epoch: 193 [147712/225000 (66%)] Loss: 6492.080078\n",
      "Train Epoch: 193 [151808/225000 (67%)] Loss: 6458.677734\n",
      "Train Epoch: 193 [155904/225000 (69%)] Loss: 6317.685547\n",
      "Train Epoch: 193 [160000/225000 (71%)] Loss: 6359.841797\n",
      "Train Epoch: 193 [164096/225000 (73%)] Loss: 6290.138672\n",
      "Train Epoch: 193 [168192/225000 (75%)] Loss: 6334.843750\n",
      "Train Epoch: 193 [172288/225000 (77%)] Loss: 6312.251953\n",
      "Train Epoch: 193 [176384/225000 (78%)] Loss: 6192.867188\n",
      "Train Epoch: 193 [180480/225000 (80%)] Loss: 6428.982422\n",
      "Train Epoch: 193 [184576/225000 (82%)] Loss: 6363.546875\n",
      "Train Epoch: 193 [188672/225000 (84%)] Loss: 6257.478516\n",
      "Train Epoch: 193 [192768/225000 (86%)] Loss: 6476.054688\n",
      "Train Epoch: 193 [196864/225000 (87%)] Loss: 6414.060547\n",
      "Train Epoch: 193 [200960/225000 (89%)] Loss: 6347.734375\n",
      "Train Epoch: 193 [205056/225000 (91%)] Loss: 6293.398438\n",
      "Train Epoch: 193 [209152/225000 (93%)] Loss: 6269.494141\n",
      "Train Epoch: 193 [213248/225000 (95%)] Loss: 6366.027344\n",
      "Train Epoch: 193 [217344/225000 (97%)] Loss: 6424.710938\n",
      "Train Epoch: 193 [221440/225000 (98%)] Loss: 6188.890625\n",
      "    epoch          : 193\n",
      "    loss           : 6428.200903014789\n",
      "    val_loss       : 6437.943667708611\n",
      "Train Epoch: 194 [256/225000 (0%)] Loss: 16628.699219\n",
      "Train Epoch: 194 [4352/225000 (2%)] Loss: 6170.404297\n",
      "Train Epoch: 194 [8448/225000 (4%)] Loss: 6326.949219\n",
      "Train Epoch: 194 [12544/225000 (6%)] Loss: 6285.369141\n",
      "Train Epoch: 194 [16640/225000 (7%)] Loss: 6423.718750\n",
      "Train Epoch: 194 [20736/225000 (9%)] Loss: 6328.900391\n",
      "Train Epoch: 194 [24832/225000 (11%)] Loss: 6263.886719\n",
      "Train Epoch: 194 [28928/225000 (13%)] Loss: 6396.468750\n",
      "Train Epoch: 194 [33024/225000 (15%)] Loss: 6244.726562\n",
      "Train Epoch: 194 [37120/225000 (16%)] Loss: 6314.457031\n",
      "Train Epoch: 194 [41216/225000 (18%)] Loss: 6346.974609\n",
      "Train Epoch: 194 [45312/225000 (20%)] Loss: 6373.490234\n",
      "Train Epoch: 194 [49408/225000 (22%)] Loss: 6299.263672\n",
      "Train Epoch: 194 [53504/225000 (24%)] Loss: 6270.662109\n",
      "Train Epoch: 194 [57600/225000 (26%)] Loss: 6437.816406\n",
      "Train Epoch: 194 [61696/225000 (27%)] Loss: 6299.078125\n",
      "Train Epoch: 194 [65792/225000 (29%)] Loss: 6311.314453\n",
      "Train Epoch: 194 [69888/225000 (31%)] Loss: 6277.419922\n",
      "Train Epoch: 194 [73984/225000 (33%)] Loss: 6664.125000\n",
      "Train Epoch: 194 [78080/225000 (35%)] Loss: 6328.037109\n",
      "Train Epoch: 194 [82176/225000 (37%)] Loss: 6334.154297\n",
      "Train Epoch: 194 [86272/225000 (38%)] Loss: 6160.906250\n",
      "Train Epoch: 194 [90368/225000 (40%)] Loss: 6434.859375\n",
      "Train Epoch: 194 [94464/225000 (42%)] Loss: 6357.224609\n",
      "Train Epoch: 194 [98560/225000 (44%)] Loss: 6268.199219\n",
      "Train Epoch: 194 [102656/225000 (46%)] Loss: 6198.369141\n",
      "Train Epoch: 194 [106752/225000 (47%)] Loss: 6233.148438\n",
      "Train Epoch: 194 [110848/225000 (49%)] Loss: 6349.011719\n",
      "Train Epoch: 194 [114944/225000 (51%)] Loss: 6392.763672\n",
      "Train Epoch: 194 [119040/225000 (53%)] Loss: 6304.906250\n",
      "Train Epoch: 194 [123136/225000 (55%)] Loss: 6269.689453\n",
      "Train Epoch: 194 [127232/225000 (57%)] Loss: 6179.503906\n",
      "Train Epoch: 194 [131328/225000 (58%)] Loss: 6183.517578\n",
      "Train Epoch: 194 [135424/225000 (60%)] Loss: 12602.527344\n",
      "Train Epoch: 194 [139520/225000 (62%)] Loss: 6387.707031\n",
      "Train Epoch: 194 [143616/225000 (64%)] Loss: 6218.708984\n",
      "Train Epoch: 194 [147712/225000 (66%)] Loss: 6301.755859\n",
      "Train Epoch: 194 [151808/225000 (67%)] Loss: 6299.898438\n",
      "Train Epoch: 194 [155904/225000 (69%)] Loss: 6260.941406\n",
      "Train Epoch: 194 [160000/225000 (71%)] Loss: 6280.951172\n",
      "Train Epoch: 194 [164096/225000 (73%)] Loss: 6279.287109\n",
      "Train Epoch: 194 [168192/225000 (75%)] Loss: 6326.015625\n",
      "Train Epoch: 194 [172288/225000 (77%)] Loss: 6264.257812\n",
      "Train Epoch: 194 [176384/225000 (78%)] Loss: 6300.951172\n",
      "Train Epoch: 194 [180480/225000 (80%)] Loss: 6261.382812\n",
      "Train Epoch: 194 [184576/225000 (82%)] Loss: 6316.783203\n",
      "Train Epoch: 194 [188672/225000 (84%)] Loss: 6197.355469\n",
      "Train Epoch: 194 [192768/225000 (86%)] Loss: 6373.457031\n",
      "Train Epoch: 194 [196864/225000 (87%)] Loss: 6354.539062\n",
      "Train Epoch: 194 [200960/225000 (89%)] Loss: 6395.205078\n",
      "Train Epoch: 194 [205056/225000 (91%)] Loss: 6312.941406\n",
      "Train Epoch: 194 [209152/225000 (93%)] Loss: 6354.226562\n",
      "Train Epoch: 194 [213248/225000 (95%)] Loss: 6310.763672\n",
      "Train Epoch: 194 [217344/225000 (97%)] Loss: 6337.910156\n",
      "Train Epoch: 194 [221440/225000 (98%)] Loss: 8077.544922\n",
      "    epoch          : 194\n",
      "    loss           : 6406.638157485424\n",
      "    val_loss       : 6599.439553665871\n",
      "Train Epoch: 195 [256/225000 (0%)] Loss: 6364.335938\n",
      "Train Epoch: 195 [4352/225000 (2%)] Loss: 6497.583984\n",
      "Train Epoch: 195 [8448/225000 (4%)] Loss: 6393.517578\n",
      "Train Epoch: 195 [12544/225000 (6%)] Loss: 6297.251953\n",
      "Train Epoch: 195 [16640/225000 (7%)] Loss: 6412.896484\n",
      "Train Epoch: 195 [20736/225000 (9%)] Loss: 6285.894531\n",
      "Train Epoch: 195 [24832/225000 (11%)] Loss: 6251.007812\n",
      "Train Epoch: 195 [28928/225000 (13%)] Loss: 6360.306641\n",
      "Train Epoch: 195 [33024/225000 (15%)] Loss: 6426.945312\n",
      "Train Epoch: 195 [37120/225000 (16%)] Loss: 6410.003906\n",
      "Train Epoch: 195 [41216/225000 (18%)] Loss: 6311.806641\n",
      "Train Epoch: 195 [45312/225000 (20%)] Loss: 6354.792969\n",
      "Train Epoch: 195 [49408/225000 (22%)] Loss: 6265.912109\n",
      "Train Epoch: 195 [53504/225000 (24%)] Loss: 6247.683594\n",
      "Train Epoch: 195 [57600/225000 (26%)] Loss: 6306.574219\n",
      "Train Epoch: 195 [61696/225000 (27%)] Loss: 6361.310547\n",
      "Train Epoch: 195 [65792/225000 (29%)] Loss: 6221.972656\n",
      "Train Epoch: 195 [69888/225000 (31%)] Loss: 6235.181641\n",
      "Train Epoch: 195 [73984/225000 (33%)] Loss: 6267.662109\n",
      "Train Epoch: 195 [78080/225000 (35%)] Loss: 6317.826172\n",
      "Train Epoch: 195 [82176/225000 (37%)] Loss: 6456.572266\n",
      "Train Epoch: 195 [86272/225000 (38%)] Loss: 6286.041016\n",
      "Train Epoch: 195 [90368/225000 (40%)] Loss: 6221.460938\n",
      "Train Epoch: 195 [94464/225000 (42%)] Loss: 6227.728516\n",
      "Train Epoch: 195 [98560/225000 (44%)] Loss: 6489.515625\n",
      "Train Epoch: 195 [102656/225000 (46%)] Loss: 6329.949219\n",
      "Train Epoch: 195 [106752/225000 (47%)] Loss: 6215.126953\n",
      "Train Epoch: 195 [110848/225000 (49%)] Loss: 6250.013672\n",
      "Train Epoch: 195 [114944/225000 (51%)] Loss: 6333.789062\n",
      "Train Epoch: 195 [119040/225000 (53%)] Loss: 6361.857422\n",
      "Train Epoch: 195 [123136/225000 (55%)] Loss: 6431.212891\n",
      "Train Epoch: 195 [127232/225000 (57%)] Loss: 6346.714844\n",
      "Train Epoch: 195 [131328/225000 (58%)] Loss: 6265.617188\n",
      "Train Epoch: 195 [135424/225000 (60%)] Loss: 6494.757812\n",
      "Train Epoch: 195 [139520/225000 (62%)] Loss: 6161.039062\n",
      "Train Epoch: 195 [143616/225000 (64%)] Loss: 6290.917969\n",
      "Train Epoch: 195 [147712/225000 (66%)] Loss: 6274.074219\n",
      "Train Epoch: 195 [151808/225000 (67%)] Loss: 6410.542969\n",
      "Train Epoch: 195 [155904/225000 (69%)] Loss: 6189.449219\n",
      "Train Epoch: 195 [160000/225000 (71%)] Loss: 6203.990234\n",
      "Train Epoch: 195 [164096/225000 (73%)] Loss: 6340.419922\n",
      "Train Epoch: 195 [168192/225000 (75%)] Loss: 6386.615234\n",
      "Train Epoch: 195 [172288/225000 (77%)] Loss: 6426.851562\n",
      "Train Epoch: 195 [176384/225000 (78%)] Loss: 6344.210938\n",
      "Train Epoch: 195 [180480/225000 (80%)] Loss: 6397.794922\n",
      "Train Epoch: 195 [184576/225000 (82%)] Loss: 6342.644531\n",
      "Train Epoch: 195 [188672/225000 (84%)] Loss: 6396.310547\n",
      "Train Epoch: 195 [192768/225000 (86%)] Loss: 6324.007812\n",
      "Train Epoch: 195 [196864/225000 (87%)] Loss: 6347.662109\n",
      "Train Epoch: 195 [200960/225000 (89%)] Loss: 6200.328125\n",
      "Train Epoch: 195 [205056/225000 (91%)] Loss: 6285.412109\n",
      "Train Epoch: 195 [209152/225000 (93%)] Loss: 6280.968750\n",
      "Train Epoch: 195 [213248/225000 (95%)] Loss: 6292.671875\n",
      "Train Epoch: 195 [217344/225000 (97%)] Loss: 6196.375000\n",
      "Train Epoch: 195 [221440/225000 (98%)] Loss: 6291.013672\n",
      "    epoch          : 195\n",
      "    loss           : 6412.0771884332335\n",
      "    val_loss       : 6527.071544344328\n",
      "Train Epoch: 196 [256/225000 (0%)] Loss: 6245.302734\n",
      "Train Epoch: 196 [4352/225000 (2%)] Loss: 6367.376953\n",
      "Train Epoch: 196 [8448/225000 (4%)] Loss: 6358.167969\n",
      "Train Epoch: 196 [12544/225000 (6%)] Loss: 6400.302734\n",
      "Train Epoch: 196 [16640/225000 (7%)] Loss: 6323.373047\n",
      "Train Epoch: 196 [20736/225000 (9%)] Loss: 6314.728516\n",
      "Train Epoch: 196 [24832/225000 (11%)] Loss: 6354.626953\n",
      "Train Epoch: 196 [28928/225000 (13%)] Loss: 6472.630859\n",
      "Train Epoch: 196 [33024/225000 (15%)] Loss: 6418.728516\n",
      "Train Epoch: 196 [37120/225000 (16%)] Loss: 6474.054688\n",
      "Train Epoch: 196 [41216/225000 (18%)] Loss: 6366.271484\n",
      "Train Epoch: 196 [45312/225000 (20%)] Loss: 6282.087891\n",
      "Train Epoch: 196 [49408/225000 (22%)] Loss: 6433.230469\n",
      "Train Epoch: 196 [53504/225000 (24%)] Loss: 6267.873047\n",
      "Train Epoch: 196 [57600/225000 (26%)] Loss: 6354.216797\n",
      "Train Epoch: 196 [61696/225000 (27%)] Loss: 6338.531250\n",
      "Train Epoch: 196 [65792/225000 (29%)] Loss: 6367.761719\n",
      "Train Epoch: 196 [69888/225000 (31%)] Loss: 6300.474609\n",
      "Train Epoch: 196 [73984/225000 (33%)] Loss: 6291.052734\n",
      "Train Epoch: 196 [78080/225000 (35%)] Loss: 6317.033203\n",
      "Train Epoch: 196 [82176/225000 (37%)] Loss: 6314.326172\n",
      "Train Epoch: 196 [86272/225000 (38%)] Loss: 6290.439453\n",
      "Train Epoch: 196 [90368/225000 (40%)] Loss: 6359.792969\n",
      "Train Epoch: 196 [94464/225000 (42%)] Loss: 6299.250000\n",
      "Train Epoch: 196 [98560/225000 (44%)] Loss: 6364.718750\n",
      "Train Epoch: 196 [102656/225000 (46%)] Loss: 6438.580078\n",
      "Train Epoch: 196 [106752/225000 (47%)] Loss: 6271.158203\n",
      "Train Epoch: 196 [110848/225000 (49%)] Loss: 6225.859375\n",
      "Train Epoch: 196 [114944/225000 (51%)] Loss: 6270.130859\n",
      "Train Epoch: 196 [119040/225000 (53%)] Loss: 6388.869141\n",
      "Train Epoch: 196 [123136/225000 (55%)] Loss: 6236.511719\n",
      "Train Epoch: 196 [127232/225000 (57%)] Loss: 6275.751953\n",
      "Train Epoch: 196 [131328/225000 (58%)] Loss: 6351.121094\n",
      "Train Epoch: 196 [135424/225000 (60%)] Loss: 6212.533203\n",
      "Train Epoch: 196 [139520/225000 (62%)] Loss: 6391.236328\n",
      "Train Epoch: 196 [143616/225000 (64%)] Loss: 6288.263672\n",
      "Train Epoch: 196 [147712/225000 (66%)] Loss: 6186.734375\n",
      "Train Epoch: 196 [151808/225000 (67%)] Loss: 6334.558594\n",
      "Train Epoch: 196 [155904/225000 (69%)] Loss: 6351.626953\n",
      "Train Epoch: 196 [160000/225000 (71%)] Loss: 6271.994141\n",
      "Train Epoch: 196 [164096/225000 (73%)] Loss: 6258.619141\n",
      "Train Epoch: 196 [168192/225000 (75%)] Loss: 6292.607422\n",
      "Train Epoch: 196 [172288/225000 (77%)] Loss: 6424.332031\n",
      "Train Epoch: 196 [176384/225000 (78%)] Loss: 6317.027344\n",
      "Train Epoch: 196 [180480/225000 (80%)] Loss: 6341.199219\n",
      "Train Epoch: 196 [184576/225000 (82%)] Loss: 6315.113281\n",
      "Train Epoch: 196 [188672/225000 (84%)] Loss: 6405.648438\n",
      "Train Epoch: 196 [192768/225000 (86%)] Loss: 6300.707031\n",
      "Train Epoch: 196 [196864/225000 (87%)] Loss: 6443.234375\n",
      "Train Epoch: 196 [200960/225000 (89%)] Loss: 6194.314453\n",
      "Train Epoch: 196 [205056/225000 (91%)] Loss: 6423.357422\n",
      "Train Epoch: 196 [209152/225000 (93%)] Loss: 6357.443359\n",
      "Train Epoch: 196 [213248/225000 (95%)] Loss: 6259.308594\n",
      "Train Epoch: 196 [217344/225000 (97%)] Loss: 6310.312500\n",
      "Train Epoch: 196 [221440/225000 (98%)] Loss: 6254.722656\n",
      "    epoch          : 196\n",
      "    loss           : 6432.412939286476\n",
      "    val_loss       : 6401.232468083197\n",
      "Train Epoch: 197 [256/225000 (0%)] Loss: 6297.482422\n",
      "Train Epoch: 197 [4352/225000 (2%)] Loss: 6307.113281\n",
      "Train Epoch: 197 [8448/225000 (4%)] Loss: 6330.892578\n",
      "Train Epoch: 197 [12544/225000 (6%)] Loss: 6295.675781\n",
      "Train Epoch: 197 [16640/225000 (7%)] Loss: 6286.431641\n",
      "Train Epoch: 197 [20736/225000 (9%)] Loss: 6341.992188\n",
      "Train Epoch: 197 [24832/225000 (11%)] Loss: 6288.248047\n",
      "Train Epoch: 197 [28928/225000 (13%)] Loss: 6373.746094\n",
      "Train Epoch: 197 [33024/225000 (15%)] Loss: 6374.519531\n",
      "Train Epoch: 197 [37120/225000 (16%)] Loss: 6458.216797\n",
      "Train Epoch: 197 [41216/225000 (18%)] Loss: 6448.541016\n",
      "Train Epoch: 197 [45312/225000 (20%)] Loss: 6371.416016\n",
      "Train Epoch: 197 [49408/225000 (22%)] Loss: 6469.527344\n",
      "Train Epoch: 197 [53504/225000 (24%)] Loss: 6365.775391\n",
      "Train Epoch: 197 [57600/225000 (26%)] Loss: 6357.542969\n",
      "Train Epoch: 197 [61696/225000 (27%)] Loss: 6404.296875\n",
      "Train Epoch: 197 [65792/225000 (29%)] Loss: 6453.203125\n",
      "Train Epoch: 197 [69888/225000 (31%)] Loss: 6317.732422\n",
      "Train Epoch: 197 [73984/225000 (33%)] Loss: 6435.408203\n",
      "Train Epoch: 197 [78080/225000 (35%)] Loss: 6349.296875\n",
      "Train Epoch: 197 [82176/225000 (37%)] Loss: 6363.789062\n",
      "Train Epoch: 197 [86272/225000 (38%)] Loss: 6272.673828\n",
      "Train Epoch: 197 [90368/225000 (40%)] Loss: 6289.027344\n",
      "Train Epoch: 197 [94464/225000 (42%)] Loss: 6374.158203\n",
      "Train Epoch: 197 [98560/225000 (44%)] Loss: 6298.697266\n",
      "Train Epoch: 197 [102656/225000 (46%)] Loss: 6195.896484\n",
      "Train Epoch: 197 [106752/225000 (47%)] Loss: 6388.916016\n",
      "Train Epoch: 197 [110848/225000 (49%)] Loss: 6350.042969\n",
      "Train Epoch: 197 [114944/225000 (51%)] Loss: 6293.107422\n",
      "Train Epoch: 197 [119040/225000 (53%)] Loss: 6422.912109\n",
      "Train Epoch: 197 [123136/225000 (55%)] Loss: 6332.636719\n",
      "Train Epoch: 197 [127232/225000 (57%)] Loss: 6250.259766\n",
      "Train Epoch: 197 [131328/225000 (58%)] Loss: 6270.621094\n",
      "Train Epoch: 197 [135424/225000 (60%)] Loss: 6348.023438\n",
      "Train Epoch: 197 [139520/225000 (62%)] Loss: 6509.292969\n",
      "Train Epoch: 197 [143616/225000 (64%)] Loss: 6325.849609\n",
      "Train Epoch: 197 [147712/225000 (66%)] Loss: 6258.275391\n",
      "Train Epoch: 197 [151808/225000 (67%)] Loss: 6332.568359\n",
      "Train Epoch: 197 [155904/225000 (69%)] Loss: 6461.101562\n",
      "Train Epoch: 197 [160000/225000 (71%)] Loss: 6340.376953\n",
      "Train Epoch: 197 [164096/225000 (73%)] Loss: 6280.447266\n",
      "Train Epoch: 197 [168192/225000 (75%)] Loss: 6234.263672\n",
      "Train Epoch: 197 [172288/225000 (77%)] Loss: 6280.785156\n",
      "Train Epoch: 197 [176384/225000 (78%)] Loss: 6432.232422\n",
      "Train Epoch: 197 [180480/225000 (80%)] Loss: 6306.121094\n",
      "Train Epoch: 197 [184576/225000 (82%)] Loss: 6201.093750\n",
      "Train Epoch: 197 [188672/225000 (84%)] Loss: 6369.982422\n",
      "Train Epoch: 197 [192768/225000 (86%)] Loss: 6280.287109\n",
      "Train Epoch: 197 [196864/225000 (87%)] Loss: 6356.789062\n",
      "Train Epoch: 197 [200960/225000 (89%)] Loss: 6311.296875\n",
      "Train Epoch: 197 [205056/225000 (91%)] Loss: 6352.224609\n",
      "Train Epoch: 197 [209152/225000 (93%)] Loss: 6457.386719\n",
      "Train Epoch: 197 [213248/225000 (95%)] Loss: 6423.476562\n",
      "Train Epoch: 197 [217344/225000 (97%)] Loss: 6347.953125\n",
      "Train Epoch: 197 [221440/225000 (98%)] Loss: 6412.726562\n",
      "    epoch          : 197\n",
      "    loss           : 6421.043463141709\n",
      "    val_loss       : 6401.251555473221\n",
      "Train Epoch: 198 [256/225000 (0%)] Loss: 6285.794922\n",
      "Train Epoch: 198 [4352/225000 (2%)] Loss: 6203.042969\n",
      "Train Epoch: 198 [8448/225000 (4%)] Loss: 6376.949219\n",
      "Train Epoch: 198 [12544/225000 (6%)] Loss: 6347.589844\n",
      "Train Epoch: 198 [16640/225000 (7%)] Loss: 6252.464844\n",
      "Train Epoch: 198 [20736/225000 (9%)] Loss: 6333.685547\n",
      "Train Epoch: 198 [24832/225000 (11%)] Loss: 6423.283203\n",
      "Train Epoch: 198 [28928/225000 (13%)] Loss: 6250.164062\n",
      "Train Epoch: 198 [33024/225000 (15%)] Loss: 6321.294922\n",
      "Train Epoch: 198 [37120/225000 (16%)] Loss: 6462.939453\n",
      "Train Epoch: 198 [41216/225000 (18%)] Loss: 6361.837891\n",
      "Train Epoch: 198 [45312/225000 (20%)] Loss: 6267.984375\n",
      "Train Epoch: 198 [49408/225000 (22%)] Loss: 6367.923828\n",
      "Train Epoch: 198 [53504/225000 (24%)] Loss: 6373.537109\n",
      "Train Epoch: 198 [57600/225000 (26%)] Loss: 6431.294922\n",
      "Train Epoch: 198 [61696/225000 (27%)] Loss: 6298.232422\n",
      "Train Epoch: 198 [65792/225000 (29%)] Loss: 6344.675781\n",
      "Train Epoch: 198 [69888/225000 (31%)] Loss: 6363.810547\n",
      "Train Epoch: 198 [73984/225000 (33%)] Loss: 6353.396484\n",
      "Train Epoch: 198 [78080/225000 (35%)] Loss: 6268.603516\n",
      "Train Epoch: 198 [82176/225000 (37%)] Loss: 6434.591797\n",
      "Train Epoch: 198 [86272/225000 (38%)] Loss: 6288.748047\n",
      "Train Epoch: 198 [90368/225000 (40%)] Loss: 6232.277344\n",
      "Train Epoch: 198 [94464/225000 (42%)] Loss: 6322.281250\n",
      "Train Epoch: 198 [98560/225000 (44%)] Loss: 6346.761719\n",
      "Train Epoch: 198 [102656/225000 (46%)] Loss: 6375.759766\n",
      "Train Epoch: 198 [106752/225000 (47%)] Loss: 6352.349609\n",
      "Train Epoch: 198 [110848/225000 (49%)] Loss: 6308.507812\n",
      "Train Epoch: 198 [114944/225000 (51%)] Loss: 6538.607422\n",
      "Train Epoch: 198 [119040/225000 (53%)] Loss: 6244.242188\n",
      "Train Epoch: 198 [123136/225000 (55%)] Loss: 6351.617188\n",
      "Train Epoch: 198 [127232/225000 (57%)] Loss: 6417.615234\n",
      "Train Epoch: 198 [131328/225000 (58%)] Loss: 6321.500000\n",
      "Train Epoch: 198 [135424/225000 (60%)] Loss: 6399.804688\n",
      "Train Epoch: 198 [139520/225000 (62%)] Loss: 6437.949219\n",
      "Train Epoch: 198 [143616/225000 (64%)] Loss: 6316.929688\n",
      "Train Epoch: 198 [147712/225000 (66%)] Loss: 6430.775391\n",
      "Train Epoch: 198 [151808/225000 (67%)] Loss: 6495.369141\n",
      "Train Epoch: 198 [155904/225000 (69%)] Loss: 6162.603516\n",
      "Train Epoch: 198 [160000/225000 (71%)] Loss: 6364.750000\n",
      "Train Epoch: 198 [164096/225000 (73%)] Loss: 6288.056641\n",
      "Train Epoch: 198 [168192/225000 (75%)] Loss: 6493.826172\n",
      "Train Epoch: 198 [172288/225000 (77%)] Loss: 6470.220703\n",
      "Train Epoch: 198 [176384/225000 (78%)] Loss: 6376.710938\n",
      "Train Epoch: 198 [180480/225000 (80%)] Loss: 6302.193359\n",
      "Train Epoch: 198 [184576/225000 (82%)] Loss: 6384.882812\n",
      "Train Epoch: 198 [188672/225000 (84%)] Loss: 6375.785156\n",
      "Train Epoch: 198 [192768/225000 (86%)] Loss: 6312.119141\n",
      "Train Epoch: 198 [196864/225000 (87%)] Loss: 6468.568359\n",
      "Train Epoch: 198 [200960/225000 (89%)] Loss: 6278.050781\n",
      "Train Epoch: 198 [205056/225000 (91%)] Loss: 6418.527344\n",
      "Train Epoch: 198 [209152/225000 (93%)] Loss: 6290.669922\n",
      "Train Epoch: 198 [213248/225000 (95%)] Loss: 6287.087891\n",
      "Train Epoch: 198 [217344/225000 (97%)] Loss: 6299.632812\n",
      "Train Epoch: 198 [221440/225000 (98%)] Loss: 6325.246094\n",
      "    epoch          : 198\n",
      "    loss           : 6376.668455364761\n",
      "    val_loss       : 6401.1090248859655\n",
      "Train Epoch: 199 [256/225000 (0%)] Loss: 6381.169922\n",
      "Train Epoch: 199 [4352/225000 (2%)] Loss: 6193.791016\n",
      "Train Epoch: 199 [8448/225000 (4%)] Loss: 6383.019531\n",
      "Train Epoch: 199 [12544/225000 (6%)] Loss: 6398.882812\n",
      "Train Epoch: 199 [16640/225000 (7%)] Loss: 6318.703125\n",
      "Train Epoch: 199 [20736/225000 (9%)] Loss: 6273.841797\n",
      "Train Epoch: 199 [24832/225000 (11%)] Loss: 6313.318359\n",
      "Train Epoch: 199 [28928/225000 (13%)] Loss: 6399.185547\n",
      "Train Epoch: 199 [33024/225000 (15%)] Loss: 6319.052734\n",
      "Train Epoch: 199 [37120/225000 (16%)] Loss: 6437.632812\n",
      "Train Epoch: 199 [41216/225000 (18%)] Loss: 6275.628906\n",
      "Train Epoch: 199 [45312/225000 (20%)] Loss: 6322.113281\n",
      "Train Epoch: 199 [49408/225000 (22%)] Loss: 6217.912109\n",
      "Train Epoch: 199 [53504/225000 (24%)] Loss: 6249.005859\n",
      "Train Epoch: 199 [57600/225000 (26%)] Loss: 6210.140625\n",
      "Train Epoch: 199 [61696/225000 (27%)] Loss: 6340.091797\n",
      "Train Epoch: 199 [65792/225000 (29%)] Loss: 6292.941406\n",
      "Train Epoch: 199 [69888/225000 (31%)] Loss: 6334.250000\n",
      "Train Epoch: 199 [73984/225000 (33%)] Loss: 6301.427734\n",
      "Train Epoch: 199 [78080/225000 (35%)] Loss: 6286.783203\n",
      "Train Epoch: 199 [82176/225000 (37%)] Loss: 6267.253906\n",
      "Train Epoch: 199 [86272/225000 (38%)] Loss: 6410.156250\n",
      "Train Epoch: 199 [90368/225000 (40%)] Loss: 6395.380859\n",
      "Train Epoch: 199 [94464/225000 (42%)] Loss: 6269.443359\n",
      "Train Epoch: 199 [98560/225000 (44%)] Loss: 6318.615234\n",
      "Train Epoch: 199 [102656/225000 (46%)] Loss: 6233.287109\n",
      "Train Epoch: 199 [106752/225000 (47%)] Loss: 6365.849609\n",
      "Train Epoch: 199 [110848/225000 (49%)] Loss: 6274.068359\n",
      "Train Epoch: 199 [114944/225000 (51%)] Loss: 6375.281250\n",
      "Train Epoch: 199 [119040/225000 (53%)] Loss: 6420.945312\n",
      "Train Epoch: 199 [123136/225000 (55%)] Loss: 6436.166016\n",
      "Train Epoch: 199 [127232/225000 (57%)] Loss: 6281.433594\n",
      "Train Epoch: 199 [131328/225000 (58%)] Loss: 6313.722656\n",
      "Train Epoch: 199 [135424/225000 (60%)] Loss: 6318.976562\n",
      "Train Epoch: 199 [139520/225000 (62%)] Loss: 6305.697266\n",
      "Train Epoch: 199 [143616/225000 (64%)] Loss: 6421.945312\n",
      "Train Epoch: 199 [147712/225000 (66%)] Loss: 6267.767578\n",
      "Train Epoch: 199 [151808/225000 (67%)] Loss: 6343.916016\n",
      "Train Epoch: 199 [155904/225000 (69%)] Loss: 6342.041016\n",
      "Train Epoch: 199 [160000/225000 (71%)] Loss: 6289.132812\n",
      "Train Epoch: 199 [164096/225000 (73%)] Loss: 6492.263672\n",
      "Train Epoch: 199 [168192/225000 (75%)] Loss: 6300.777344\n",
      "Train Epoch: 199 [172288/225000 (77%)] Loss: 6311.908203\n",
      "Train Epoch: 199 [176384/225000 (78%)] Loss: 6388.341797\n",
      "Train Epoch: 199 [180480/225000 (80%)] Loss: 6317.921875\n",
      "Train Epoch: 199 [184576/225000 (82%)] Loss: 6177.580078\n",
      "Train Epoch: 199 [188672/225000 (84%)] Loss: 6453.113281\n",
      "Train Epoch: 199 [192768/225000 (86%)] Loss: 6400.720703\n",
      "Train Epoch: 199 [196864/225000 (87%)] Loss: 6278.416016\n",
      "Train Epoch: 199 [200960/225000 (89%)] Loss: 6285.855469\n",
      "Train Epoch: 199 [205056/225000 (91%)] Loss: 6331.066406\n",
      "Train Epoch: 199 [209152/225000 (93%)] Loss: 6170.908203\n",
      "Train Epoch: 199 [213248/225000 (95%)] Loss: 6309.699219\n",
      "Train Epoch: 199 [217344/225000 (97%)] Loss: 6284.664062\n",
      "Train Epoch: 199 [221440/225000 (98%)] Loss: 6271.587891\n",
      "    epoch          : 199\n",
      "    loss           : 6376.361222580703\n",
      "    val_loss       : 6419.780053963466\n",
      "Train Epoch: 200 [256/225000 (0%)] Loss: 6316.017578\n",
      "Train Epoch: 200 [4352/225000 (2%)] Loss: 6394.880859\n",
      "Train Epoch: 200 [8448/225000 (4%)] Loss: 6333.761719\n",
      "Train Epoch: 200 [12544/225000 (6%)] Loss: 6375.283203\n",
      "Train Epoch: 200 [16640/225000 (7%)] Loss: 6379.244141\n",
      "Train Epoch: 200 [20736/225000 (9%)] Loss: 6340.685547\n",
      "Train Epoch: 200 [24832/225000 (11%)] Loss: 6317.269531\n",
      "Train Epoch: 200 [28928/225000 (13%)] Loss: 6254.730469\n",
      "Train Epoch: 200 [33024/225000 (15%)] Loss: 6429.013672\n",
      "Train Epoch: 200 [37120/225000 (16%)] Loss: 6320.207031\n",
      "Train Epoch: 200 [41216/225000 (18%)] Loss: 6379.181641\n",
      "Train Epoch: 200 [45312/225000 (20%)] Loss: 6338.542969\n",
      "Train Epoch: 200 [49408/225000 (22%)] Loss: 6295.261719\n",
      "Train Epoch: 200 [53504/225000 (24%)] Loss: 6162.210938\n",
      "Train Epoch: 200 [57600/225000 (26%)] Loss: 6284.757812\n",
      "Train Epoch: 200 [61696/225000 (27%)] Loss: 6276.435547\n",
      "Train Epoch: 200 [65792/225000 (29%)] Loss: 6385.535156\n",
      "Train Epoch: 200 [69888/225000 (31%)] Loss: 6313.923828\n",
      "Train Epoch: 200 [73984/225000 (33%)] Loss: 6341.539062\n",
      "Train Epoch: 200 [78080/225000 (35%)] Loss: 6218.875000\n",
      "Train Epoch: 200 [82176/225000 (37%)] Loss: 6410.041016\n",
      "Train Epoch: 200 [86272/225000 (38%)] Loss: 6383.332031\n",
      "Train Epoch: 200 [90368/225000 (40%)] Loss: 6384.939453\n",
      "Train Epoch: 200 [94464/225000 (42%)] Loss: 6405.005859\n",
      "Train Epoch: 200 [98560/225000 (44%)] Loss: 6504.728516\n",
      "Train Epoch: 200 [102656/225000 (46%)] Loss: 6377.640625\n",
      "Train Epoch: 200 [106752/225000 (47%)] Loss: 6217.660156\n",
      "Train Epoch: 200 [110848/225000 (49%)] Loss: 6300.294922\n",
      "Train Epoch: 200 [114944/225000 (51%)] Loss: 6275.695312\n",
      "Train Epoch: 200 [119040/225000 (53%)] Loss: 6301.652344\n",
      "Train Epoch: 200 [123136/225000 (55%)] Loss: 6257.802734\n",
      "Train Epoch: 200 [127232/225000 (57%)] Loss: 6376.527344\n",
      "Train Epoch: 200 [131328/225000 (58%)] Loss: 6366.929688\n",
      "Train Epoch: 200 [135424/225000 (60%)] Loss: 6478.148438\n",
      "Train Epoch: 200 [139520/225000 (62%)] Loss: 6449.763672\n",
      "Train Epoch: 200 [143616/225000 (64%)] Loss: 6411.910156\n",
      "Train Epoch: 200 [147712/225000 (66%)] Loss: 6291.748047\n",
      "Train Epoch: 200 [151808/225000 (67%)] Loss: 6311.648438\n",
      "Train Epoch: 200 [155904/225000 (69%)] Loss: 6255.628906\n",
      "Train Epoch: 200 [160000/225000 (71%)] Loss: 6308.675781\n",
      "Train Epoch: 200 [164096/225000 (73%)] Loss: 6249.392578\n",
      "Train Epoch: 200 [168192/225000 (75%)] Loss: 6257.568359\n",
      "Train Epoch: 200 [172288/225000 (77%)] Loss: 6314.353516\n",
      "Train Epoch: 200 [176384/225000 (78%)] Loss: 6242.384766\n",
      "Train Epoch: 200 [180480/225000 (80%)] Loss: 6329.488281\n",
      "Train Epoch: 200 [184576/225000 (82%)] Loss: 6273.277344\n",
      "Train Epoch: 200 [188672/225000 (84%)] Loss: 6221.875000\n",
      "Train Epoch: 200 [192768/225000 (86%)] Loss: 6384.189453\n",
      "Train Epoch: 200 [196864/225000 (87%)] Loss: 6373.451172\n",
      "Train Epoch: 200 [200960/225000 (89%)] Loss: 6280.888672\n",
      "Train Epoch: 200 [205056/225000 (91%)] Loss: 6350.408203\n",
      "Train Epoch: 200 [209152/225000 (93%)] Loss: 6285.386719\n",
      "Train Epoch: 200 [213248/225000 (95%)] Loss: 6208.578125\n",
      "Train Epoch: 200 [217344/225000 (97%)] Loss: 6331.964844\n",
      "Train Epoch: 200 [221440/225000 (98%)] Loss: 6497.251953\n",
      "    epoch          : 200\n",
      "    loss           : 6403.42301376742\n",
      "    val_loss       : 6401.248670215509\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0815_160624/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [256/225000 (0%)] Loss: 6203.419922\n",
      "Train Epoch: 201 [4352/225000 (2%)] Loss: 6366.894531\n",
      "Train Epoch: 201 [8448/225000 (4%)] Loss: 6311.136719\n",
      "Train Epoch: 201 [12544/225000 (6%)] Loss: 6260.408203\n",
      "Train Epoch: 201 [16640/225000 (7%)] Loss: 6285.498047\n",
      "Train Epoch: 201 [20736/225000 (9%)] Loss: 6339.185547\n",
      "Train Epoch: 201 [24832/225000 (11%)] Loss: 6307.494141\n",
      "Train Epoch: 201 [28928/225000 (13%)] Loss: 6287.210938\n",
      "Train Epoch: 201 [33024/225000 (15%)] Loss: 6361.623047\n",
      "Train Epoch: 201 [37120/225000 (16%)] Loss: 6257.789062\n",
      "Train Epoch: 201 [41216/225000 (18%)] Loss: 6283.482422\n",
      "Train Epoch: 201 [45312/225000 (20%)] Loss: 6370.228516\n",
      "Train Epoch: 201 [49408/225000 (22%)] Loss: 6320.650391\n",
      "Train Epoch: 201 [53504/225000 (24%)] Loss: 6280.015625\n",
      "Train Epoch: 201 [57600/225000 (26%)] Loss: 6328.968750\n",
      "Train Epoch: 201 [61696/225000 (27%)] Loss: 6348.009766\n",
      "Train Epoch: 201 [65792/225000 (29%)] Loss: 6363.771484\n",
      "Train Epoch: 201 [69888/225000 (31%)] Loss: 6453.328125\n",
      "Train Epoch: 201 [73984/225000 (33%)] Loss: 6363.570312\n",
      "Train Epoch: 201 [78080/225000 (35%)] Loss: 6296.634766\n",
      "Train Epoch: 201 [82176/225000 (37%)] Loss: 6334.853516\n",
      "Train Epoch: 201 [86272/225000 (38%)] Loss: 6353.523438\n",
      "Train Epoch: 201 [90368/225000 (40%)] Loss: 6364.822266\n",
      "Train Epoch: 201 [94464/225000 (42%)] Loss: 6351.646484\n",
      "Train Epoch: 201 [98560/225000 (44%)] Loss: 6283.917969\n",
      "Train Epoch: 201 [102656/225000 (46%)] Loss: 6370.167969\n",
      "Train Epoch: 201 [106752/225000 (47%)] Loss: 6295.738281\n",
      "Train Epoch: 201 [110848/225000 (49%)] Loss: 6474.513672\n",
      "Train Epoch: 201 [114944/225000 (51%)] Loss: 6306.048828\n",
      "Train Epoch: 201 [119040/225000 (53%)] Loss: 6226.990234\n",
      "Train Epoch: 201 [123136/225000 (55%)] Loss: 6339.324219\n",
      "Train Epoch: 201 [127232/225000 (57%)] Loss: 6390.238281\n",
      "Train Epoch: 201 [131328/225000 (58%)] Loss: 6226.943359\n",
      "Train Epoch: 201 [135424/225000 (60%)] Loss: 6391.794922\n",
      "Train Epoch: 201 [139520/225000 (62%)] Loss: 6333.476562\n",
      "Train Epoch: 201 [143616/225000 (64%)] Loss: 6384.763672\n",
      "Train Epoch: 201 [147712/225000 (66%)] Loss: 6431.787109\n",
      "Train Epoch: 201 [151808/225000 (67%)] Loss: 6195.326172\n",
      "Train Epoch: 201 [155904/225000 (69%)] Loss: 6212.343750\n",
      "Train Epoch: 201 [160000/225000 (71%)] Loss: 6163.357422\n",
      "Train Epoch: 201 [164096/225000 (73%)] Loss: 6345.978516\n",
      "Train Epoch: 201 [168192/225000 (75%)] Loss: 6384.529297\n",
      "Train Epoch: 201 [172288/225000 (77%)] Loss: 6238.570312\n",
      "Train Epoch: 201 [176384/225000 (78%)] Loss: 6395.189453\n",
      "Train Epoch: 201 [180480/225000 (80%)] Loss: 6385.195312\n",
      "Train Epoch: 201 [184576/225000 (82%)] Loss: 6204.314453\n",
      "Train Epoch: 201 [188672/225000 (84%)] Loss: 6250.630859\n",
      "Train Epoch: 201 [192768/225000 (86%)] Loss: 6268.728516\n",
      "Train Epoch: 201 [196864/225000 (87%)] Loss: 6439.343750\n",
      "Train Epoch: 201 [200960/225000 (89%)] Loss: 6347.958984\n",
      "Train Epoch: 201 [205056/225000 (91%)] Loss: 6202.763672\n",
      "Train Epoch: 201 [209152/225000 (93%)] Loss: 6470.779297\n",
      "Train Epoch: 201 [213248/225000 (95%)] Loss: 6389.779297\n",
      "Train Epoch: 201 [217344/225000 (97%)] Loss: 6207.085938\n",
      "Train Epoch: 201 [221440/225000 (98%)] Loss: 6265.828125\n",
      "    epoch          : 201\n",
      "    loss           : 6406.304194219283\n",
      "    val_loss       : 6525.298996115217\n",
      "Train Epoch: 202 [256/225000 (0%)] Loss: 6236.867188\n",
      "Train Epoch: 202 [4352/225000 (2%)] Loss: 6369.109375\n",
      "Train Epoch: 202 [8448/225000 (4%)] Loss: 6497.654297\n",
      "Train Epoch: 202 [12544/225000 (6%)] Loss: 6183.751953\n",
      "Train Epoch: 202 [16640/225000 (7%)] Loss: 6199.935547\n",
      "Train Epoch: 202 [20736/225000 (9%)] Loss: 6238.800781\n",
      "Train Epoch: 202 [24832/225000 (11%)] Loss: 6243.558594\n",
      "Train Epoch: 202 [28928/225000 (13%)] Loss: 6328.382812\n",
      "Train Epoch: 202 [33024/225000 (15%)] Loss: 6239.494141\n",
      "Train Epoch: 202 [37120/225000 (16%)] Loss: 6297.085938\n",
      "Train Epoch: 202 [41216/225000 (18%)] Loss: 6234.320312\n",
      "Train Epoch: 202 [45312/225000 (20%)] Loss: 6282.910156\n",
      "Train Epoch: 202 [49408/225000 (22%)] Loss: 6443.900391\n",
      "Train Epoch: 202 [53504/225000 (24%)] Loss: 6377.562500\n",
      "Train Epoch: 202 [57600/225000 (26%)] Loss: 6342.607422\n",
      "Train Epoch: 202 [61696/225000 (27%)] Loss: 6313.564453\n",
      "Train Epoch: 202 [65792/225000 (29%)] Loss: 6316.089844\n",
      "Train Epoch: 202 [69888/225000 (31%)] Loss: 6211.365234\n",
      "Train Epoch: 202 [73984/225000 (33%)] Loss: 6323.855469\n",
      "Train Epoch: 202 [78080/225000 (35%)] Loss: 6292.710938\n",
      "Train Epoch: 202 [82176/225000 (37%)] Loss: 6364.927734\n",
      "Train Epoch: 202 [86272/225000 (38%)] Loss: 6415.259766\n",
      "Train Epoch: 202 [90368/225000 (40%)] Loss: 6373.449219\n",
      "Train Epoch: 202 [94464/225000 (42%)] Loss: 6304.048828\n",
      "Train Epoch: 202 [98560/225000 (44%)] Loss: 6363.384766\n",
      "Train Epoch: 202 [102656/225000 (46%)] Loss: 6262.695312\n",
      "Train Epoch: 202 [106752/225000 (47%)] Loss: 6232.111328\n",
      "Train Epoch: 202 [110848/225000 (49%)] Loss: 6209.166016\n",
      "Train Epoch: 202 [114944/225000 (51%)] Loss: 6274.259766\n",
      "Train Epoch: 202 [119040/225000 (53%)] Loss: 6361.871094\n",
      "Train Epoch: 202 [123136/225000 (55%)] Loss: 8155.996094\n",
      "Train Epoch: 202 [127232/225000 (57%)] Loss: 6250.662109\n",
      "Train Epoch: 202 [131328/225000 (58%)] Loss: 6261.148438\n",
      "Train Epoch: 202 [135424/225000 (60%)] Loss: 6314.873047\n",
      "Train Epoch: 202 [139520/225000 (62%)] Loss: 6264.195312\n",
      "Train Epoch: 202 [143616/225000 (64%)] Loss: 6243.375000\n",
      "Train Epoch: 202 [147712/225000 (66%)] Loss: 6314.814453\n",
      "Train Epoch: 202 [151808/225000 (67%)] Loss: 6325.343750\n",
      "Train Epoch: 202 [155904/225000 (69%)] Loss: 6353.046875\n",
      "Train Epoch: 202 [160000/225000 (71%)] Loss: 6276.177734\n",
      "Train Epoch: 202 [164096/225000 (73%)] Loss: 6261.050781\n",
      "Train Epoch: 202 [168192/225000 (75%)] Loss: 6176.765625\n",
      "Train Epoch: 202 [172288/225000 (77%)] Loss: 6369.351562\n",
      "Train Epoch: 202 [176384/225000 (78%)] Loss: 6249.857422\n",
      "Train Epoch: 202 [180480/225000 (80%)] Loss: 6343.544922\n",
      "Train Epoch: 202 [184576/225000 (82%)] Loss: 6347.517578\n",
      "Train Epoch: 202 [188672/225000 (84%)] Loss: 6300.683594\n",
      "Train Epoch: 202 [192768/225000 (86%)] Loss: 6183.734375\n",
      "Train Epoch: 202 [196864/225000 (87%)] Loss: 6288.386719\n",
      "Train Epoch: 202 [200960/225000 (89%)] Loss: 6285.839844\n",
      "Train Epoch: 202 [205056/225000 (91%)] Loss: 6317.677734\n",
      "Train Epoch: 202 [209152/225000 (93%)] Loss: 6253.251953\n",
      "Train Epoch: 202 [213248/225000 (95%)] Loss: 6298.210938\n",
      "Train Epoch: 202 [217344/225000 (97%)] Loss: 6379.943359\n",
      "Train Epoch: 202 [221440/225000 (98%)] Loss: 6233.167969\n",
      "    epoch          : 202\n",
      "    loss           : 6364.721610805958\n",
      "    val_loss       : 6401.190086313656\n",
      "Train Epoch: 203 [256/225000 (0%)] Loss: 6190.107422\n",
      "Train Epoch: 203 [4352/225000 (2%)] Loss: 6248.054688\n",
      "Train Epoch: 203 [8448/225000 (4%)] Loss: 6301.255859\n",
      "Train Epoch: 203 [12544/225000 (6%)] Loss: 6398.099609\n",
      "Train Epoch: 203 [16640/225000 (7%)] Loss: 6255.466797\n",
      "Train Epoch: 203 [20736/225000 (9%)] Loss: 6264.298828\n",
      "Train Epoch: 203 [24832/225000 (11%)] Loss: 6354.410156\n",
      "Train Epoch: 203 [28928/225000 (13%)] Loss: 6522.050781\n",
      "Train Epoch: 203 [33024/225000 (15%)] Loss: 6362.345703\n",
      "Train Epoch: 203 [37120/225000 (16%)] Loss: 6420.191406\n",
      "Train Epoch: 203 [41216/225000 (18%)] Loss: 6337.673828\n",
      "Train Epoch: 203 [45312/225000 (20%)] Loss: 6214.404297\n",
      "Train Epoch: 203 [49408/225000 (22%)] Loss: 6472.222656\n",
      "Train Epoch: 203 [53504/225000 (24%)] Loss: 6252.308594\n",
      "Train Epoch: 203 [57600/225000 (26%)] Loss: 6355.185547\n",
      "Train Epoch: 203 [61696/225000 (27%)] Loss: 6328.845703\n",
      "Train Epoch: 203 [65792/225000 (29%)] Loss: 6281.070312\n",
      "Train Epoch: 203 [69888/225000 (31%)] Loss: 6268.109375\n",
      "Train Epoch: 203 [73984/225000 (33%)] Loss: 6420.148438\n",
      "Train Epoch: 203 [78080/225000 (35%)] Loss: 6225.082031\n",
      "Train Epoch: 203 [82176/225000 (37%)] Loss: 6472.550781\n",
      "Train Epoch: 203 [86272/225000 (38%)] Loss: 6283.142578\n",
      "Train Epoch: 203 [90368/225000 (40%)] Loss: 6281.068359\n",
      "Train Epoch: 203 [94464/225000 (42%)] Loss: 6250.046875\n",
      "Train Epoch: 203 [98560/225000 (44%)] Loss: 6317.810547\n",
      "Train Epoch: 203 [102656/225000 (46%)] Loss: 6177.552734\n",
      "Train Epoch: 203 [106752/225000 (47%)] Loss: 6284.984375\n",
      "Train Epoch: 203 [110848/225000 (49%)] Loss: 6344.064453\n",
      "Train Epoch: 203 [114944/225000 (51%)] Loss: 6372.929688\n",
      "Train Epoch: 203 [119040/225000 (53%)] Loss: 6333.353516\n",
      "Train Epoch: 203 [123136/225000 (55%)] Loss: 6369.833984\n",
      "Train Epoch: 203 [127232/225000 (57%)] Loss: 6330.783203\n",
      "Train Epoch: 203 [131328/225000 (58%)] Loss: 6326.242188\n",
      "Train Epoch: 203 [135424/225000 (60%)] Loss: 6344.400391\n",
      "Train Epoch: 203 [139520/225000 (62%)] Loss: 6315.787109\n",
      "Train Epoch: 203 [143616/225000 (64%)] Loss: 6265.931641\n",
      "Train Epoch: 203 [147712/225000 (66%)] Loss: 6329.753906\n",
      "Train Epoch: 203 [151808/225000 (67%)] Loss: 6344.373047\n",
      "Train Epoch: 203 [155904/225000 (69%)] Loss: 6287.121094\n",
      "Train Epoch: 203 [160000/225000 (71%)] Loss: 6252.234375\n",
      "Train Epoch: 203 [164096/225000 (73%)] Loss: 6168.482422\n",
      "Train Epoch: 203 [168192/225000 (75%)] Loss: 6378.464844\n",
      "Train Epoch: 203 [172288/225000 (77%)] Loss: 6304.404297\n",
      "Train Epoch: 203 [176384/225000 (78%)] Loss: 6227.042969\n",
      "Train Epoch: 203 [180480/225000 (80%)] Loss: 6302.003906\n",
      "Train Epoch: 203 [184576/225000 (82%)] Loss: 6323.861328\n",
      "Train Epoch: 203 [188672/225000 (84%)] Loss: 6478.160156\n",
      "Train Epoch: 203 [192768/225000 (86%)] Loss: 6267.195312\n",
      "Train Epoch: 203 [196864/225000 (87%)] Loss: 6182.222656\n",
      "Train Epoch: 203 [200960/225000 (89%)] Loss: 6389.466797\n",
      "Train Epoch: 203 [205056/225000 (91%)] Loss: 6271.259766\n",
      "Train Epoch: 203 [209152/225000 (93%)] Loss: 6354.169922\n",
      "Train Epoch: 203 [213248/225000 (95%)] Loss: 6382.013672\n",
      "Train Epoch: 203 [217344/225000 (97%)] Loss: 6301.810547\n",
      "Train Epoch: 203 [221440/225000 (98%)] Loss: 6185.865234\n",
      "    epoch          : 203\n",
      "    loss           : 6412.727582391211\n",
      "    val_loss       : 6419.6786700024895\n",
      "Train Epoch: 204 [256/225000 (0%)] Loss: 6232.304688\n",
      "Train Epoch: 204 [4352/225000 (2%)] Loss: 6298.408203\n",
      "Train Epoch: 204 [8448/225000 (4%)] Loss: 6395.779297\n",
      "Train Epoch: 204 [12544/225000 (6%)] Loss: 6343.339844\n",
      "Train Epoch: 204 [16640/225000 (7%)] Loss: 6435.662109\n",
      "Train Epoch: 204 [20736/225000 (9%)] Loss: 6210.775391\n",
      "Train Epoch: 204 [24832/225000 (11%)] Loss: 6351.890625\n",
      "Train Epoch: 204 [28928/225000 (13%)] Loss: 6395.562500\n",
      "Train Epoch: 204 [33024/225000 (15%)] Loss: 6343.314453\n",
      "Train Epoch: 204 [37120/225000 (16%)] Loss: 6407.175781\n",
      "Train Epoch: 204 [41216/225000 (18%)] Loss: 6431.701172\n",
      "Train Epoch: 204 [45312/225000 (20%)] Loss: 6245.281250\n",
      "Train Epoch: 204 [49408/225000 (22%)] Loss: 6293.841797\n",
      "Train Epoch: 204 [53504/225000 (24%)] Loss: 6220.201172\n",
      "Train Epoch: 204 [57600/225000 (26%)] Loss: 6385.572266\n",
      "Train Epoch: 204 [61696/225000 (27%)] Loss: 6318.746094\n",
      "Train Epoch: 204 [65792/225000 (29%)] Loss: 6258.691406\n",
      "Train Epoch: 204 [69888/225000 (31%)] Loss: 6308.720703\n",
      "Train Epoch: 204 [73984/225000 (33%)] Loss: 6344.052734\n",
      "Train Epoch: 204 [78080/225000 (35%)] Loss: 6397.486328\n",
      "Train Epoch: 204 [82176/225000 (37%)] Loss: 6315.513672\n",
      "Train Epoch: 204 [86272/225000 (38%)] Loss: 6373.615234\n",
      "Train Epoch: 204 [90368/225000 (40%)] Loss: 6507.935547\n",
      "Train Epoch: 204 [94464/225000 (42%)] Loss: 6339.488281\n",
      "Train Epoch: 204 [98560/225000 (44%)] Loss: 6388.214844\n",
      "Train Epoch: 204 [102656/225000 (46%)] Loss: 6463.789062\n",
      "Train Epoch: 204 [106752/225000 (47%)] Loss: 6248.322266\n",
      "Train Epoch: 204 [110848/225000 (49%)] Loss: 6294.976562\n",
      "Train Epoch: 204 [114944/225000 (51%)] Loss: 6417.478516\n",
      "Train Epoch: 204 [119040/225000 (53%)] Loss: 6270.162109\n",
      "Train Epoch: 204 [123136/225000 (55%)] Loss: 6223.304688\n",
      "Train Epoch: 204 [127232/225000 (57%)] Loss: 6405.369141\n",
      "Train Epoch: 204 [131328/225000 (58%)] Loss: 6347.001953\n",
      "Train Epoch: 204 [135424/225000 (60%)] Loss: 6400.308594\n",
      "Train Epoch: 204 [139520/225000 (62%)] Loss: 6318.605469\n",
      "Train Epoch: 204 [143616/225000 (64%)] Loss: 6408.935547\n",
      "Train Epoch: 204 [147712/225000 (66%)] Loss: 6377.785156\n",
      "Train Epoch: 204 [151808/225000 (67%)] Loss: 6351.576172\n",
      "Train Epoch: 204 [155904/225000 (69%)] Loss: 6395.154297\n",
      "Train Epoch: 204 [160000/225000 (71%)] Loss: 6307.400391\n",
      "Train Epoch: 204 [164096/225000 (73%)] Loss: 6173.943359\n",
      "Train Epoch: 204 [168192/225000 (75%)] Loss: 6133.966797\n",
      "Train Epoch: 204 [172288/225000 (77%)] Loss: 6376.474609\n",
      "Train Epoch: 204 [176384/225000 (78%)] Loss: 6197.591797\n",
      "Train Epoch: 204 [180480/225000 (80%)] Loss: 6135.404297\n",
      "Train Epoch: 204 [184576/225000 (82%)] Loss: 8198.421875\n",
      "Train Epoch: 204 [188672/225000 (84%)] Loss: 6447.070312\n",
      "Train Epoch: 204 [192768/225000 (86%)] Loss: 6406.896484\n",
      "Train Epoch: 204 [196864/225000 (87%)] Loss: 6408.689453\n",
      "Train Epoch: 204 [200960/225000 (89%)] Loss: 6437.232422\n",
      "Train Epoch: 204 [205056/225000 (91%)] Loss: 6411.675781\n",
      "Train Epoch: 204 [209152/225000 (93%)] Loss: 6323.052734\n",
      "Train Epoch: 204 [213248/225000 (95%)] Loss: 6348.027344\n",
      "Train Epoch: 204 [217344/225000 (97%)] Loss: 6335.683594\n",
      "Train Epoch: 204 [221440/225000 (98%)] Loss: 6361.433594\n",
      "    epoch          : 204\n",
      "    loss           : 6397.4394653459185\n",
      "    val_loss       : 6455.34046171636\n",
      "Train Epoch: 205 [256/225000 (0%)] Loss: 6290.167969\n",
      "Train Epoch: 205 [4352/225000 (2%)] Loss: 6256.414062\n",
      "Train Epoch: 205 [8448/225000 (4%)] Loss: 6346.726562\n",
      "Train Epoch: 205 [12544/225000 (6%)] Loss: 6348.626953\n",
      "Train Epoch: 205 [16640/225000 (7%)] Loss: 6506.105469\n",
      "Train Epoch: 205 [20736/225000 (9%)] Loss: 6301.359375\n",
      "Train Epoch: 205 [24832/225000 (11%)] Loss: 6415.902344\n",
      "Train Epoch: 205 [28928/225000 (13%)] Loss: 6284.925781\n",
      "Train Epoch: 205 [33024/225000 (15%)] Loss: 6351.730469\n",
      "Train Epoch: 205 [37120/225000 (16%)] Loss: 6307.091797\n",
      "Train Epoch: 205 [41216/225000 (18%)] Loss: 6262.412109\n",
      "Train Epoch: 205 [45312/225000 (20%)] Loss: 6396.412109\n",
      "Train Epoch: 205 [49408/225000 (22%)] Loss: 6335.287109\n",
      "Train Epoch: 205 [53504/225000 (24%)] Loss: 6461.275391\n",
      "Train Epoch: 205 [57600/225000 (26%)] Loss: 6393.458984\n",
      "Train Epoch: 205 [61696/225000 (27%)] Loss: 6365.511719\n",
      "Train Epoch: 205 [65792/225000 (29%)] Loss: 6395.115234\n",
      "Train Epoch: 205 [69888/225000 (31%)] Loss: 6327.281250\n",
      "Train Epoch: 205 [73984/225000 (33%)] Loss: 6388.962891\n",
      "Train Epoch: 205 [78080/225000 (35%)] Loss: 6346.744141\n",
      "Train Epoch: 205 [82176/225000 (37%)] Loss: 6396.000000\n",
      "Train Epoch: 205 [86272/225000 (38%)] Loss: 6375.126953\n",
      "Train Epoch: 205 [90368/225000 (40%)] Loss: 6308.703125\n",
      "Train Epoch: 205 [94464/225000 (42%)] Loss: 6309.701172\n",
      "Train Epoch: 205 [98560/225000 (44%)] Loss: 6298.658203\n",
      "Train Epoch: 205 [102656/225000 (46%)] Loss: 6284.791016\n",
      "Train Epoch: 205 [106752/225000 (47%)] Loss: 6352.609375\n",
      "Train Epoch: 205 [110848/225000 (49%)] Loss: 6361.960938\n",
      "Train Epoch: 205 [114944/225000 (51%)] Loss: 6292.220703\n",
      "Train Epoch: 205 [119040/225000 (53%)] Loss: 6460.757812\n",
      "Train Epoch: 205 [123136/225000 (55%)] Loss: 6313.984375\n",
      "Train Epoch: 205 [127232/225000 (57%)] Loss: 6335.636719\n",
      "Train Epoch: 205 [131328/225000 (58%)] Loss: 6262.792969\n",
      "Train Epoch: 205 [135424/225000 (60%)] Loss: 6350.082031\n",
      "Train Epoch: 205 [139520/225000 (62%)] Loss: 6373.304688\n",
      "Train Epoch: 205 [143616/225000 (64%)] Loss: 6344.835938\n",
      "Train Epoch: 205 [147712/225000 (66%)] Loss: 6254.300781\n",
      "Train Epoch: 205 [151808/225000 (67%)] Loss: 6289.490234\n",
      "Train Epoch: 205 [155904/225000 (69%)] Loss: 6286.244141\n",
      "Train Epoch: 205 [160000/225000 (71%)] Loss: 6395.183594\n",
      "Train Epoch: 205 [164096/225000 (73%)] Loss: 6341.546875\n",
      "Train Epoch: 205 [168192/225000 (75%)] Loss: 6351.576172\n",
      "Train Epoch: 205 [172288/225000 (77%)] Loss: 6375.490234\n",
      "Train Epoch: 205 [176384/225000 (78%)] Loss: 6301.978516\n",
      "Train Epoch: 205 [180480/225000 (80%)] Loss: 6207.126953\n",
      "Train Epoch: 205 [184576/225000 (82%)] Loss: 6282.666016\n",
      "Train Epoch: 205 [188672/225000 (84%)] Loss: 6222.931641\n",
      "Train Epoch: 205 [192768/225000 (86%)] Loss: 6468.197266\n",
      "Train Epoch: 205 [196864/225000 (87%)] Loss: 6240.451172\n",
      "Train Epoch: 205 [200960/225000 (89%)] Loss: 6280.835938\n",
      "Train Epoch: 205 [205056/225000 (91%)] Loss: 6311.919922\n",
      "Train Epoch: 205 [209152/225000 (93%)] Loss: 6184.185547\n",
      "Train Epoch: 205 [213248/225000 (95%)] Loss: 6468.638672\n",
      "Train Epoch: 205 [217344/225000 (97%)] Loss: 6475.103516\n",
      "Train Epoch: 205 [221440/225000 (98%)] Loss: 6256.318359\n",
      "    epoch          : 205\n",
      "    loss           : 6370.045171848336\n",
      "    val_loss       : 6438.012623666501\n",
      "Train Epoch: 206 [256/225000 (0%)] Loss: 6304.375000\n",
      "Train Epoch: 206 [4352/225000 (2%)] Loss: 6255.279297\n",
      "Train Epoch: 206 [8448/225000 (4%)] Loss: 6270.195312\n",
      "Train Epoch: 206 [12544/225000 (6%)] Loss: 6295.304688\n",
      "Train Epoch: 206 [16640/225000 (7%)] Loss: 6274.943359\n",
      "Train Epoch: 206 [20736/225000 (9%)] Loss: 6253.253906\n",
      "Train Epoch: 206 [24832/225000 (11%)] Loss: 6147.289062\n",
      "Train Epoch: 206 [28928/225000 (13%)] Loss: 6484.882812\n",
      "Train Epoch: 206 [33024/225000 (15%)] Loss: 6372.726562\n",
      "Train Epoch: 206 [37120/225000 (16%)] Loss: 6234.380859\n",
      "Train Epoch: 206 [41216/225000 (18%)] Loss: 6223.789062\n",
      "Train Epoch: 206 [45312/225000 (20%)] Loss: 6373.292969\n",
      "Train Epoch: 206 [49408/225000 (22%)] Loss: 6273.445312\n",
      "Train Epoch: 206 [53504/225000 (24%)] Loss: 6416.275391\n",
      "Train Epoch: 206 [57600/225000 (26%)] Loss: 6357.417969\n",
      "Train Epoch: 206 [61696/225000 (27%)] Loss: 6368.480469\n",
      "Train Epoch: 206 [65792/225000 (29%)] Loss: 6423.437500\n",
      "Train Epoch: 206 [69888/225000 (31%)] Loss: 6335.568359\n",
      "Train Epoch: 206 [73984/225000 (33%)] Loss: 6287.576172\n",
      "Train Epoch: 206 [78080/225000 (35%)] Loss: 6160.552734\n",
      "Train Epoch: 206 [82176/225000 (37%)] Loss: 6285.015625\n",
      "Train Epoch: 206 [86272/225000 (38%)] Loss: 6412.058594\n",
      "Train Epoch: 206 [90368/225000 (40%)] Loss: 6346.228516\n",
      "Train Epoch: 206 [94464/225000 (42%)] Loss: 6449.166016\n",
      "Train Epoch: 206 [98560/225000 (44%)] Loss: 6356.947266\n",
      "Train Epoch: 206 [102656/225000 (46%)] Loss: 6279.792969\n",
      "Train Epoch: 206 [106752/225000 (47%)] Loss: 6368.019531\n",
      "Train Epoch: 206 [110848/225000 (49%)] Loss: 6248.115234\n",
      "Train Epoch: 206 [114944/225000 (51%)] Loss: 6284.138672\n",
      "Train Epoch: 206 [119040/225000 (53%)] Loss: 6414.781250\n",
      "Train Epoch: 206 [123136/225000 (55%)] Loss: 6250.486328\n",
      "Train Epoch: 206 [127232/225000 (57%)] Loss: 6274.359375\n",
      "Train Epoch: 206 [131328/225000 (58%)] Loss: 6320.535156\n",
      "Train Epoch: 206 [135424/225000 (60%)] Loss: 6283.308594\n",
      "Train Epoch: 206 [139520/225000 (62%)] Loss: 6343.861328\n",
      "Train Epoch: 206 [143616/225000 (64%)] Loss: 6238.820312\n",
      "Train Epoch: 206 [147712/225000 (66%)] Loss: 6197.273438\n",
      "Train Epoch: 206 [151808/225000 (67%)] Loss: 6391.910156\n",
      "Train Epoch: 206 [155904/225000 (69%)] Loss: 6543.265625\n",
      "Train Epoch: 206 [160000/225000 (71%)] Loss: 6208.953125\n",
      "Train Epoch: 206 [164096/225000 (73%)] Loss: 6454.134766\n",
      "Train Epoch: 206 [168192/225000 (75%)] Loss: 6300.761719\n",
      "Train Epoch: 206 [172288/225000 (77%)] Loss: 6257.046875\n",
      "Train Epoch: 206 [176384/225000 (78%)] Loss: 6250.382812\n",
      "Train Epoch: 206 [180480/225000 (80%)] Loss: 6336.521484\n",
      "Train Epoch: 206 [184576/225000 (82%)] Loss: 6313.769531\n",
      "Train Epoch: 206 [188672/225000 (84%)] Loss: 6470.818359\n",
      "Train Epoch: 206 [192768/225000 (86%)] Loss: 6303.164062\n",
      "Train Epoch: 206 [196864/225000 (87%)] Loss: 6380.740234\n",
      "Train Epoch: 206 [200960/225000 (89%)] Loss: 6312.783203\n",
      "Train Epoch: 206 [205056/225000 (91%)] Loss: 6247.613281\n",
      "Train Epoch: 206 [209152/225000 (93%)] Loss: 6253.634766\n",
      "Train Epoch: 206 [213248/225000 (95%)] Loss: 6256.146484\n",
      "Train Epoch: 206 [217344/225000 (97%)] Loss: 6340.312500\n",
      "Train Epoch: 206 [221440/225000 (98%)] Loss: 6316.287109\n",
      "    epoch          : 206\n",
      "    loss           : 6402.16731215337\n",
      "    val_loss       : 6418.808043842413\n",
      "Train Epoch: 207 [256/225000 (0%)] Loss: 6422.863281\n",
      "Train Epoch: 207 [4352/225000 (2%)] Loss: 6323.658203\n",
      "Train Epoch: 207 [8448/225000 (4%)] Loss: 6338.164062\n",
      "Train Epoch: 207 [12544/225000 (6%)] Loss: 6355.503906\n",
      "Train Epoch: 207 [16640/225000 (7%)] Loss: 6350.195312\n",
      "Train Epoch: 207 [20736/225000 (9%)] Loss: 6225.882812\n",
      "Train Epoch: 207 [24832/225000 (11%)] Loss: 6341.771484\n",
      "Train Epoch: 207 [28928/225000 (13%)] Loss: 6345.216797\n",
      "Train Epoch: 207 [33024/225000 (15%)] Loss: 6441.255859\n",
      "Train Epoch: 207 [37120/225000 (16%)] Loss: 6319.029297\n",
      "Train Epoch: 207 [41216/225000 (18%)] Loss: 6324.308594\n",
      "Train Epoch: 207 [45312/225000 (20%)] Loss: 6303.457031\n",
      "Train Epoch: 207 [49408/225000 (22%)] Loss: 6381.560547\n",
      "Train Epoch: 207 [53504/225000 (24%)] Loss: 6378.939453\n",
      "Train Epoch: 207 [57600/225000 (26%)] Loss: 6263.160156\n",
      "Train Epoch: 207 [61696/225000 (27%)] Loss: 6229.087891\n",
      "Train Epoch: 207 [65792/225000 (29%)] Loss: 6305.789062\n",
      "Train Epoch: 207 [69888/225000 (31%)] Loss: 6354.802734\n",
      "Train Epoch: 207 [73984/225000 (33%)] Loss: 6457.082031\n",
      "Train Epoch: 207 [78080/225000 (35%)] Loss: 6456.470703\n",
      "Train Epoch: 207 [82176/225000 (37%)] Loss: 6439.017578\n",
      "Train Epoch: 207 [86272/225000 (38%)] Loss: 6351.621094\n",
      "Train Epoch: 207 [90368/225000 (40%)] Loss: 6291.283203\n",
      "Train Epoch: 207 [94464/225000 (42%)] Loss: 8111.935547\n",
      "Train Epoch: 207 [98560/225000 (44%)] Loss: 6404.480469\n",
      "Train Epoch: 207 [102656/225000 (46%)] Loss: 6217.728516\n",
      "Train Epoch: 207 [106752/225000 (47%)] Loss: 6418.980469\n",
      "Train Epoch: 207 [110848/225000 (49%)] Loss: 6153.896484\n",
      "Train Epoch: 207 [114944/225000 (51%)] Loss: 6515.611328\n",
      "Train Epoch: 207 [119040/225000 (53%)] Loss: 6409.582031\n",
      "Train Epoch: 207 [123136/225000 (55%)] Loss: 6368.988281\n",
      "Train Epoch: 207 [127232/225000 (57%)] Loss: 6337.185547\n",
      "Train Epoch: 207 [131328/225000 (58%)] Loss: 6221.792969\n",
      "Train Epoch: 207 [135424/225000 (60%)] Loss: 6264.052734\n",
      "Train Epoch: 207 [139520/225000 (62%)] Loss: 6300.773438\n",
      "Train Epoch: 207 [143616/225000 (64%)] Loss: 6312.943359\n",
      "Train Epoch: 207 [147712/225000 (66%)] Loss: 6280.330078\n",
      "Train Epoch: 207 [151808/225000 (67%)] Loss: 6334.591797\n",
      "Train Epoch: 207 [155904/225000 (69%)] Loss: 6260.097656\n",
      "Train Epoch: 207 [160000/225000 (71%)] Loss: 6233.347656\n",
      "Train Epoch: 207 [164096/225000 (73%)] Loss: 6438.646484\n",
      "Train Epoch: 207 [168192/225000 (75%)] Loss: 6355.085938\n",
      "Train Epoch: 207 [172288/225000 (77%)] Loss: 6218.992188\n",
      "Train Epoch: 207 [176384/225000 (78%)] Loss: 6270.501953\n",
      "Train Epoch: 207 [180480/225000 (80%)] Loss: 6249.568359\n",
      "Train Epoch: 207 [184576/225000 (82%)] Loss: 6378.179688\n",
      "Train Epoch: 207 [188672/225000 (84%)] Loss: 6316.050781\n",
      "Train Epoch: 207 [192768/225000 (86%)] Loss: 6288.148438\n",
      "Train Epoch: 207 [196864/225000 (87%)] Loss: 6331.912109\n",
      "Train Epoch: 207 [200960/225000 (89%)] Loss: 6315.462891\n",
      "Train Epoch: 207 [205056/225000 (91%)] Loss: 6356.367188\n",
      "Train Epoch: 207 [209152/225000 (93%)] Loss: 6359.585938\n",
      "Train Epoch: 207 [213248/225000 (95%)] Loss: 6305.945312\n",
      "Train Epoch: 207 [217344/225000 (97%)] Loss: 6357.419922\n",
      "Train Epoch: 207 [221440/225000 (98%)] Loss: 6313.611328\n",
      "    epoch          : 207\n",
      "    loss           : 6365.892265936078\n",
      "    val_loss       : 6419.468504456841\n",
      "Train Epoch: 208 [256/225000 (0%)] Loss: 6347.474609\n",
      "Train Epoch: 208 [4352/225000 (2%)] Loss: 6378.699219\n",
      "Train Epoch: 208 [8448/225000 (4%)] Loss: 6227.892578\n",
      "Train Epoch: 208 [12544/225000 (6%)] Loss: 6389.578125\n",
      "Train Epoch: 208 [16640/225000 (7%)] Loss: 6337.193359\n",
      "Train Epoch: 208 [20736/225000 (9%)] Loss: 6292.005859\n",
      "Train Epoch: 208 [24832/225000 (11%)] Loss: 6296.599609\n",
      "Train Epoch: 208 [28928/225000 (13%)] Loss: 6390.636719\n",
      "Train Epoch: 208 [33024/225000 (15%)] Loss: 6237.716797\n",
      "Train Epoch: 208 [37120/225000 (16%)] Loss: 6361.402344\n",
      "Train Epoch: 208 [41216/225000 (18%)] Loss: 6288.119141\n",
      "Train Epoch: 208 [45312/225000 (20%)] Loss: 6458.269531\n",
      "Train Epoch: 208 [49408/225000 (22%)] Loss: 6295.902344\n",
      "Train Epoch: 208 [53504/225000 (24%)] Loss: 6352.363281\n",
      "Train Epoch: 208 [57600/225000 (26%)] Loss: 6354.789062\n",
      "Train Epoch: 208 [61696/225000 (27%)] Loss: 6337.662109\n",
      "Train Epoch: 208 [65792/225000 (29%)] Loss: 6295.978516\n",
      "Train Epoch: 208 [69888/225000 (31%)] Loss: 6298.574219\n",
      "Train Epoch: 208 [73984/225000 (33%)] Loss: 6158.900391\n",
      "Train Epoch: 208 [78080/225000 (35%)] Loss: 6361.291016\n",
      "Train Epoch: 208 [82176/225000 (37%)] Loss: 6310.724609\n",
      "Train Epoch: 208 [86272/225000 (38%)] Loss: 6397.431641\n",
      "Train Epoch: 208 [90368/225000 (40%)] Loss: 6259.458984\n",
      "Train Epoch: 208 [94464/225000 (42%)] Loss: 6276.646484\n",
      "Train Epoch: 208 [98560/225000 (44%)] Loss: 6217.066406\n",
      "Train Epoch: 208 [102656/225000 (46%)] Loss: 6426.601562\n",
      "Train Epoch: 208 [106752/225000 (47%)] Loss: 6323.228516\n",
      "Train Epoch: 208 [110848/225000 (49%)] Loss: 6456.181641\n",
      "Train Epoch: 208 [114944/225000 (51%)] Loss: 6390.046875\n",
      "Train Epoch: 208 [119040/225000 (53%)] Loss: 6298.292969\n",
      "Train Epoch: 208 [123136/225000 (55%)] Loss: 6235.597656\n",
      "Train Epoch: 208 [127232/225000 (57%)] Loss: 6352.675781\n",
      "Train Epoch: 208 [131328/225000 (58%)] Loss: 6420.960938\n",
      "Train Epoch: 208 [135424/225000 (60%)] Loss: 6345.806641\n",
      "Train Epoch: 208 [139520/225000 (62%)] Loss: 6343.072266\n",
      "Train Epoch: 208 [143616/225000 (64%)] Loss: 6226.445312\n",
      "Train Epoch: 208 [147712/225000 (66%)] Loss: 6431.929688\n",
      "Train Epoch: 208 [151808/225000 (67%)] Loss: 6481.312500\n",
      "Train Epoch: 208 [155904/225000 (69%)] Loss: 6250.138672\n",
      "Train Epoch: 208 [160000/225000 (71%)] Loss: 6322.162109\n",
      "Train Epoch: 208 [164096/225000 (73%)] Loss: 6315.599609\n",
      "Train Epoch: 208 [168192/225000 (75%)] Loss: 6252.933594\n",
      "Train Epoch: 208 [172288/225000 (77%)] Loss: 6363.220703\n",
      "Train Epoch: 208 [176384/225000 (78%)] Loss: 6284.609375\n",
      "Train Epoch: 208 [180480/225000 (80%)] Loss: 6325.300781\n",
      "Train Epoch: 208 [184576/225000 (82%)] Loss: 6320.593750\n",
      "Train Epoch: 208 [188672/225000 (84%)] Loss: 6319.904297\n",
      "Train Epoch: 208 [192768/225000 (86%)] Loss: 6375.976562\n",
      "Train Epoch: 208 [196864/225000 (87%)] Loss: 6343.072266\n",
      "Train Epoch: 208 [200960/225000 (89%)] Loss: 6289.255859\n",
      "Train Epoch: 208 [205056/225000 (91%)] Loss: 6328.521484\n",
      "Train Epoch: 208 [209152/225000 (93%)] Loss: 6313.230469\n",
      "Train Epoch: 208 [213248/225000 (95%)] Loss: 6319.259766\n",
      "Train Epoch: 208 [217344/225000 (97%)] Loss: 6178.755859\n",
      "Train Epoch: 208 [221440/225000 (98%)] Loss: 6301.494141\n",
      "    epoch          : 208\n",
      "    loss           : 6407.900219532139\n",
      "    val_loss       : 6689.301541378303\n",
      "Train Epoch: 209 [256/225000 (0%)] Loss: 6419.927734\n",
      "Train Epoch: 209 [4352/225000 (2%)] Loss: 6268.683594\n",
      "Train Epoch: 209 [8448/225000 (4%)] Loss: 6315.169922\n",
      "Train Epoch: 209 [12544/225000 (6%)] Loss: 6327.376953\n",
      "Train Epoch: 209 [16640/225000 (7%)] Loss: 6395.267578\n",
      "Train Epoch: 209 [20736/225000 (9%)] Loss: 6418.691406\n",
      "Train Epoch: 209 [24832/225000 (11%)] Loss: 6335.341797\n",
      "Train Epoch: 209 [28928/225000 (13%)] Loss: 6471.544922\n",
      "Train Epoch: 209 [33024/225000 (15%)] Loss: 6349.306641\n",
      "Train Epoch: 209 [37120/225000 (16%)] Loss: 6329.128906\n",
      "Train Epoch: 209 [41216/225000 (18%)] Loss: 6368.875000\n",
      "Train Epoch: 209 [45312/225000 (20%)] Loss: 6163.017578\n",
      "Train Epoch: 209 [49408/225000 (22%)] Loss: 6274.269531\n",
      "Train Epoch: 209 [53504/225000 (24%)] Loss: 6361.964844\n",
      "Train Epoch: 209 [57600/225000 (26%)] Loss: 6349.279297\n",
      "Train Epoch: 209 [61696/225000 (27%)] Loss: 6279.228516\n",
      "Train Epoch: 209 [65792/225000 (29%)] Loss: 6366.941406\n",
      "Train Epoch: 209 [69888/225000 (31%)] Loss: 6322.476562\n",
      "Train Epoch: 209 [73984/225000 (33%)] Loss: 6454.091797\n",
      "Train Epoch: 209 [78080/225000 (35%)] Loss: 6242.673828\n",
      "Train Epoch: 209 [82176/225000 (37%)] Loss: 6426.978516\n",
      "Train Epoch: 209 [86272/225000 (38%)] Loss: 6280.666016\n",
      "Train Epoch: 209 [90368/225000 (40%)] Loss: 6338.984375\n",
      "Train Epoch: 209 [94464/225000 (42%)] Loss: 6276.689453\n",
      "Train Epoch: 209 [98560/225000 (44%)] Loss: 6329.187500\n",
      "Train Epoch: 209 [102656/225000 (46%)] Loss: 6419.191406\n",
      "Train Epoch: 209 [106752/225000 (47%)] Loss: 6384.025391\n",
      "Train Epoch: 209 [110848/225000 (49%)] Loss: 6247.109375\n",
      "Train Epoch: 209 [114944/225000 (51%)] Loss: 6407.302734\n",
      "Train Epoch: 209 [119040/225000 (53%)] Loss: 6317.818359\n",
      "Train Epoch: 209 [123136/225000 (55%)] Loss: 6334.734375\n",
      "Train Epoch: 209 [127232/225000 (57%)] Loss: 6246.839844\n",
      "Train Epoch: 209 [131328/225000 (58%)] Loss: 6550.562500\n",
      "Train Epoch: 209 [135424/225000 (60%)] Loss: 6260.265625\n",
      "Train Epoch: 209 [139520/225000 (62%)] Loss: 6263.158203\n",
      "Train Epoch: 209 [143616/225000 (64%)] Loss: 6337.021484\n",
      "Train Epoch: 209 [147712/225000 (66%)] Loss: 6384.529297\n",
      "Train Epoch: 209 [151808/225000 (67%)] Loss: 6295.431641\n",
      "Train Epoch: 209 [155904/225000 (69%)] Loss: 6397.085938\n",
      "Train Epoch: 209 [160000/225000 (71%)] Loss: 6406.037109\n",
      "Train Epoch: 209 [164096/225000 (73%)] Loss: 6405.900391\n",
      "Train Epoch: 209 [168192/225000 (75%)] Loss: 6385.054688\n",
      "Train Epoch: 209 [172288/225000 (77%)] Loss: 6304.869141\n",
      "Train Epoch: 209 [176384/225000 (78%)] Loss: 6422.826172\n",
      "Train Epoch: 209 [180480/225000 (80%)] Loss: 6332.843750\n",
      "Train Epoch: 209 [184576/225000 (82%)] Loss: 6508.742188\n",
      "Train Epoch: 209 [188672/225000 (84%)] Loss: 6360.873047\n",
      "Train Epoch: 209 [192768/225000 (86%)] Loss: 6291.783203\n",
      "Train Epoch: 209 [196864/225000 (87%)] Loss: 6230.267578\n",
      "Train Epoch: 209 [200960/225000 (89%)] Loss: 6475.697266\n",
      "Train Epoch: 209 [205056/225000 (91%)] Loss: 6378.863281\n",
      "Train Epoch: 209 [209152/225000 (93%)] Loss: 6263.097656\n",
      "Train Epoch: 209 [213248/225000 (95%)] Loss: 6390.431641\n",
      "Train Epoch: 209 [217344/225000 (97%)] Loss: 6328.271484\n",
      "Train Epoch: 209 [221440/225000 (98%)] Loss: 6250.705078\n",
      "    epoch          : 209\n",
      "    loss           : 6358.251964234926\n",
      "    val_loss       : 6419.882652333805\n",
      "Train Epoch: 210 [256/225000 (0%)] Loss: 6352.146484\n",
      "Train Epoch: 210 [4352/225000 (2%)] Loss: 6142.058594\n",
      "Train Epoch: 210 [8448/225000 (4%)] Loss: 6277.517578\n",
      "Train Epoch: 210 [12544/225000 (6%)] Loss: 6213.128906\n",
      "Train Epoch: 210 [16640/225000 (7%)] Loss: 6104.894531\n",
      "Train Epoch: 210 [20736/225000 (9%)] Loss: 6251.005859\n",
      "Train Epoch: 210 [24832/225000 (11%)] Loss: 6464.507812\n",
      "Train Epoch: 210 [28928/225000 (13%)] Loss: 6435.748047\n",
      "Train Epoch: 210 [33024/225000 (15%)] Loss: 6351.238281\n",
      "Train Epoch: 210 [37120/225000 (16%)] Loss: 6299.322266\n",
      "Train Epoch: 210 [41216/225000 (18%)] Loss: 6286.412109\n",
      "Train Epoch: 210 [45312/225000 (20%)] Loss: 6180.746094\n",
      "Train Epoch: 210 [49408/225000 (22%)] Loss: 6321.910156\n",
      "Train Epoch: 210 [53504/225000 (24%)] Loss: 6391.447266\n",
      "Train Epoch: 210 [57600/225000 (26%)] Loss: 6312.937500\n",
      "Train Epoch: 210 [61696/225000 (27%)] Loss: 6303.917969\n",
      "Train Epoch: 210 [65792/225000 (29%)] Loss: 6261.500000\n",
      "Train Epoch: 210 [69888/225000 (31%)] Loss: 6400.826172\n",
      "Train Epoch: 210 [73984/225000 (33%)] Loss: 6321.730469\n",
      "Train Epoch: 210 [78080/225000 (35%)] Loss: 6234.046875\n",
      "Train Epoch: 210 [82176/225000 (37%)] Loss: 6297.328125\n",
      "Train Epoch: 210 [86272/225000 (38%)] Loss: 6427.142578\n",
      "Train Epoch: 210 [90368/225000 (40%)] Loss: 6375.964844\n",
      "Train Epoch: 210 [94464/225000 (42%)] Loss: 6248.699219\n",
      "Train Epoch: 210 [98560/225000 (44%)] Loss: 6318.509766\n",
      "Train Epoch: 210 [102656/225000 (46%)] Loss: 6307.132812\n",
      "Train Epoch: 210 [106752/225000 (47%)] Loss: 6400.304688\n",
      "Train Epoch: 210 [110848/225000 (49%)] Loss: 6132.500000\n",
      "Train Epoch: 210 [114944/225000 (51%)] Loss: 6272.218750\n",
      "Train Epoch: 210 [119040/225000 (53%)] Loss: 6231.042969\n",
      "Train Epoch: 210 [123136/225000 (55%)] Loss: 6391.625000\n",
      "Train Epoch: 210 [127232/225000 (57%)] Loss: 6250.062500\n",
      "Train Epoch: 210 [131328/225000 (58%)] Loss: 6329.066406\n",
      "Train Epoch: 210 [135424/225000 (60%)] Loss: 6432.027344\n",
      "Train Epoch: 210 [139520/225000 (62%)] Loss: 6329.085938\n",
      "Train Epoch: 210 [143616/225000 (64%)] Loss: 6223.052734\n",
      "Train Epoch: 210 [147712/225000 (66%)] Loss: 6389.804688\n",
      "Train Epoch: 210 [151808/225000 (67%)] Loss: 6360.162109\n",
      "Train Epoch: 210 [155904/225000 (69%)] Loss: 6374.583984\n",
      "Train Epoch: 210 [160000/225000 (71%)] Loss: 6348.042969\n",
      "Train Epoch: 210 [164096/225000 (73%)] Loss: 6394.652344\n",
      "Train Epoch: 210 [168192/225000 (75%)] Loss: 6245.777344\n",
      "Train Epoch: 210 [172288/225000 (77%)] Loss: 6351.695312\n",
      "Train Epoch: 210 [176384/225000 (78%)] Loss: 6371.908203\n",
      "Train Epoch: 210 [180480/225000 (80%)] Loss: 6434.560547\n",
      "Train Epoch: 210 [184576/225000 (82%)] Loss: 6428.425781\n",
      "Train Epoch: 210 [188672/225000 (84%)] Loss: 6472.542969\n",
      "Train Epoch: 210 [192768/225000 (86%)] Loss: 6335.742188\n",
      "Train Epoch: 210 [196864/225000 (87%)] Loss: 6265.396484\n",
      "Train Epoch: 210 [200960/225000 (89%)] Loss: 6451.533203\n",
      "Train Epoch: 210 [205056/225000 (91%)] Loss: 6359.166016\n",
      "Train Epoch: 210 [209152/225000 (93%)] Loss: 6335.851562\n",
      "Train Epoch: 210 [213248/225000 (95%)] Loss: 6276.414062\n",
      "Train Epoch: 210 [217344/225000 (97%)] Loss: 6299.505859\n",
      "Train Epoch: 210 [221440/225000 (98%)] Loss: 6365.330078\n",
      "    epoch          : 210\n",
      "    loss           : 6437.24031547746\n",
      "    val_loss       : 6500.690595136005\n",
      "Train Epoch: 211 [256/225000 (0%)] Loss: 6423.992188\n",
      "Train Epoch: 211 [4352/225000 (2%)] Loss: 6318.248047\n",
      "Train Epoch: 211 [8448/225000 (4%)] Loss: 6389.115234\n",
      "Train Epoch: 211 [12544/225000 (6%)] Loss: 6421.253906\n",
      "Train Epoch: 211 [16640/225000 (7%)] Loss: 6319.931641\n",
      "Train Epoch: 211 [20736/225000 (9%)] Loss: 6268.529297\n",
      "Train Epoch: 211 [24832/225000 (11%)] Loss: 6468.089844\n",
      "Train Epoch: 211 [28928/225000 (13%)] Loss: 6389.574219\n",
      "Train Epoch: 211 [33024/225000 (15%)] Loss: 6479.611328\n",
      "Train Epoch: 211 [37120/225000 (16%)] Loss: 6384.697266\n",
      "Train Epoch: 211 [41216/225000 (18%)] Loss: 6383.255859\n",
      "Train Epoch: 211 [45312/225000 (20%)] Loss: 6204.941406\n",
      "Train Epoch: 211 [49408/225000 (22%)] Loss: 6363.171875\n",
      "Train Epoch: 211 [53504/225000 (24%)] Loss: 6389.427734\n",
      "Train Epoch: 211 [57600/225000 (26%)] Loss: 6314.099609\n",
      "Train Epoch: 211 [61696/225000 (27%)] Loss: 6310.878906\n",
      "Train Epoch: 211 [65792/225000 (29%)] Loss: 6492.437500\n",
      "Train Epoch: 211 [69888/225000 (31%)] Loss: 6419.550781\n",
      "Train Epoch: 211 [73984/225000 (33%)] Loss: 6403.855469\n",
      "Train Epoch: 211 [78080/225000 (35%)] Loss: 6274.371094\n",
      "Train Epoch: 211 [82176/225000 (37%)] Loss: 6366.347656\n",
      "Train Epoch: 211 [86272/225000 (38%)] Loss: 6230.863281\n",
      "Train Epoch: 211 [90368/225000 (40%)] Loss: 6273.001953\n",
      "Train Epoch: 211 [94464/225000 (42%)] Loss: 6193.638672\n",
      "Train Epoch: 211 [98560/225000 (44%)] Loss: 6219.732422\n",
      "Train Epoch: 211 [102656/225000 (46%)] Loss: 6366.921875\n",
      "Train Epoch: 211 [106752/225000 (47%)] Loss: 6342.253906\n",
      "Train Epoch: 211 [110848/225000 (49%)] Loss: 6372.458984\n",
      "Train Epoch: 211 [114944/225000 (51%)] Loss: 6265.507812\n",
      "Train Epoch: 211 [119040/225000 (53%)] Loss: 6391.667969\n",
      "Train Epoch: 211 [123136/225000 (55%)] Loss: 6273.123047\n",
      "Train Epoch: 211 [127232/225000 (57%)] Loss: 6438.605469\n",
      "Train Epoch: 211 [131328/225000 (58%)] Loss: 6459.392578\n",
      "Train Epoch: 211 [135424/225000 (60%)] Loss: 6228.136719\n",
      "Train Epoch: 211 [139520/225000 (62%)] Loss: 6298.214844\n",
      "Train Epoch: 211 [143616/225000 (64%)] Loss: 6237.003906\n",
      "Train Epoch: 211 [147712/225000 (66%)] Loss: 6194.824219\n",
      "Train Epoch: 211 [151808/225000 (67%)] Loss: 6362.853516\n",
      "Train Epoch: 211 [155904/225000 (69%)] Loss: 6581.427734\n",
      "Train Epoch: 211 [160000/225000 (71%)] Loss: 6385.953125\n",
      "Train Epoch: 211 [164096/225000 (73%)] Loss: 6336.982422\n",
      "Train Epoch: 211 [168192/225000 (75%)] Loss: 6322.416016\n",
      "Train Epoch: 211 [172288/225000 (77%)] Loss: 6354.685547\n",
      "Train Epoch: 211 [176384/225000 (78%)] Loss: 6389.068359\n",
      "Train Epoch: 211 [180480/225000 (80%)] Loss: 6378.197266\n",
      "Train Epoch: 211 [184576/225000 (82%)] Loss: 6419.949219\n",
      "Train Epoch: 211 [188672/225000 (84%)] Loss: 6487.869141\n",
      "Train Epoch: 211 [192768/225000 (86%)] Loss: 6170.966797\n",
      "Train Epoch: 211 [196864/225000 (87%)] Loss: 6393.990234\n",
      "Train Epoch: 211 [200960/225000 (89%)] Loss: 6340.820312\n",
      "Train Epoch: 211 [205056/225000 (91%)] Loss: 6413.419922\n",
      "Train Epoch: 211 [209152/225000 (93%)] Loss: 6410.500000\n",
      "Train Epoch: 211 [213248/225000 (95%)] Loss: 6348.783203\n",
      "Train Epoch: 211 [217344/225000 (97%)] Loss: 6455.027344\n",
      "Train Epoch: 211 [221440/225000 (98%)] Loss: 6355.912109\n",
      "    epoch          : 211\n",
      "    loss           : 6392.224158312002\n",
      "    val_loss       : 6419.913361948364\n",
      "Train Epoch: 212 [256/225000 (0%)] Loss: 6327.246094\n",
      "Train Epoch: 212 [4352/225000 (2%)] Loss: 6274.841797\n",
      "Train Epoch: 212 [8448/225000 (4%)] Loss: 6308.210938\n",
      "Train Epoch: 212 [12544/225000 (6%)] Loss: 6315.292969\n",
      "Train Epoch: 212 [16640/225000 (7%)] Loss: 6302.615234\n",
      "Train Epoch: 212 [20736/225000 (9%)] Loss: 6415.712891\n",
      "Train Epoch: 212 [24832/225000 (11%)] Loss: 6398.984375\n",
      "Train Epoch: 212 [28928/225000 (13%)] Loss: 6214.667969\n",
      "Train Epoch: 212 [33024/225000 (15%)] Loss: 6185.148438\n",
      "Train Epoch: 212 [37120/225000 (16%)] Loss: 6439.257812\n",
      "Train Epoch: 212 [41216/225000 (18%)] Loss: 6448.001953\n",
      "Train Epoch: 212 [45312/225000 (20%)] Loss: 6301.910156\n",
      "Train Epoch: 212 [49408/225000 (22%)] Loss: 6196.865234\n",
      "Train Epoch: 212 [53504/225000 (24%)] Loss: 6392.447266\n",
      "Train Epoch: 212 [57600/225000 (26%)] Loss: 6338.085938\n",
      "Train Epoch: 212 [61696/225000 (27%)] Loss: 6192.001953\n",
      "Train Epoch: 212 [65792/225000 (29%)] Loss: 6354.451172\n",
      "Train Epoch: 212 [69888/225000 (31%)] Loss: 6329.355469\n",
      "Train Epoch: 212 [73984/225000 (33%)] Loss: 6287.248047\n",
      "Train Epoch: 212 [78080/225000 (35%)] Loss: 6252.089844\n",
      "Train Epoch: 212 [82176/225000 (37%)] Loss: 6162.130859\n",
      "Train Epoch: 212 [86272/225000 (38%)] Loss: 6280.662109\n",
      "Train Epoch: 212 [90368/225000 (40%)] Loss: 6422.169922\n",
      "Train Epoch: 212 [94464/225000 (42%)] Loss: 6271.548828\n",
      "Train Epoch: 212 [98560/225000 (44%)] Loss: 6393.023438\n",
      "Train Epoch: 212 [102656/225000 (46%)] Loss: 6267.767578\n",
      "Train Epoch: 212 [106752/225000 (47%)] Loss: 6351.197266\n",
      "Train Epoch: 212 [110848/225000 (49%)] Loss: 6242.464844\n",
      "Train Epoch: 212 [114944/225000 (51%)] Loss: 6246.847656\n",
      "Train Epoch: 212 [119040/225000 (53%)] Loss: 6306.056641\n",
      "Train Epoch: 212 [123136/225000 (55%)] Loss: 6269.736328\n",
      "Train Epoch: 212 [127232/225000 (57%)] Loss: 6293.298828\n",
      "Train Epoch: 212 [131328/225000 (58%)] Loss: 6245.207031\n",
      "Train Epoch: 212 [135424/225000 (60%)] Loss: 6213.767578\n",
      "Train Epoch: 212 [139520/225000 (62%)] Loss: 6331.626953\n",
      "Train Epoch: 212 [143616/225000 (64%)] Loss: 6280.138672\n",
      "Train Epoch: 212 [147712/225000 (66%)] Loss: 6359.585938\n",
      "Train Epoch: 212 [151808/225000 (67%)] Loss: 6258.843750\n",
      "Train Epoch: 212 [155904/225000 (69%)] Loss: 6398.673828\n",
      "Train Epoch: 212 [160000/225000 (71%)] Loss: 6227.021484\n",
      "Train Epoch: 212 [164096/225000 (73%)] Loss: 6311.070312\n",
      "Train Epoch: 212 [168192/225000 (75%)] Loss: 6434.451172\n",
      "Train Epoch: 212 [172288/225000 (77%)] Loss: 6294.460938\n",
      "Train Epoch: 212 [176384/225000 (78%)] Loss: 16773.845703\n",
      "Train Epoch: 212 [180480/225000 (80%)] Loss: 6303.021484\n",
      "Train Epoch: 212 [184576/225000 (82%)] Loss: 6336.947266\n",
      "Train Epoch: 212 [188672/225000 (84%)] Loss: 6358.572266\n",
      "Train Epoch: 212 [192768/225000 (86%)] Loss: 6380.771484\n",
      "Train Epoch: 212 [196864/225000 (87%)] Loss: 6356.500000\n",
      "Train Epoch: 212 [200960/225000 (89%)] Loss: 6262.902344\n",
      "Train Epoch: 212 [205056/225000 (91%)] Loss: 6157.433594\n",
      "Train Epoch: 212 [209152/225000 (93%)] Loss: 6368.367188\n",
      "Train Epoch: 212 [213248/225000 (95%)] Loss: 6363.394531\n",
      "Train Epoch: 212 [217344/225000 (97%)] Loss: 6339.066406\n",
      "Train Epoch: 212 [221440/225000 (98%)] Loss: 6414.367188\n",
      "    epoch          : 212\n",
      "    loss           : 6391.4811109037255\n",
      "    val_loss       : 6401.1402965334\n",
      "Train Epoch: 213 [256/225000 (0%)] Loss: 6305.251953\n",
      "Train Epoch: 213 [4352/225000 (2%)] Loss: 6213.466797\n",
      "Train Epoch: 213 [8448/225000 (4%)] Loss: 6340.458984\n",
      "Train Epoch: 213 [12544/225000 (6%)] Loss: 6300.275391\n",
      "Train Epoch: 213 [16640/225000 (7%)] Loss: 6403.693359\n",
      "Train Epoch: 213 [20736/225000 (9%)] Loss: 6301.193359\n",
      "Train Epoch: 213 [24832/225000 (11%)] Loss: 6267.300781\n",
      "Train Epoch: 213 [28928/225000 (13%)] Loss: 6444.896484\n",
      "Train Epoch: 213 [33024/225000 (15%)] Loss: 6224.128906\n",
      "Train Epoch: 213 [37120/225000 (16%)] Loss: 6321.888672\n",
      "Train Epoch: 213 [41216/225000 (18%)] Loss: 6355.962891\n",
      "Train Epoch: 213 [45312/225000 (20%)] Loss: 6280.572266\n",
      "Train Epoch: 213 [49408/225000 (22%)] Loss: 6403.244141\n",
      "Train Epoch: 213 [53504/225000 (24%)] Loss: 6430.566406\n",
      "Train Epoch: 213 [57600/225000 (26%)] Loss: 6385.263672\n",
      "Train Epoch: 213 [61696/225000 (27%)] Loss: 6291.505859\n",
      "Train Epoch: 213 [65792/225000 (29%)] Loss: 6139.251953\n",
      "Train Epoch: 213 [69888/225000 (31%)] Loss: 6288.509766\n",
      "Train Epoch: 213 [73984/225000 (33%)] Loss: 6322.308594\n",
      "Train Epoch: 213 [78080/225000 (35%)] Loss: 6291.519531\n",
      "Train Epoch: 213 [82176/225000 (37%)] Loss: 6293.880859\n",
      "Train Epoch: 213 [86272/225000 (38%)] Loss: 6373.269531\n",
      "Train Epoch: 213 [90368/225000 (40%)] Loss: 6221.849609\n",
      "Train Epoch: 213 [94464/225000 (42%)] Loss: 6543.050781\n",
      "Train Epoch: 213 [98560/225000 (44%)] Loss: 6349.826172\n",
      "Train Epoch: 213 [102656/225000 (46%)] Loss: 6299.433594\n",
      "Train Epoch: 213 [106752/225000 (47%)] Loss: 6449.335938\n",
      "Train Epoch: 213 [110848/225000 (49%)] Loss: 6249.496094\n",
      "Train Epoch: 213 [114944/225000 (51%)] Loss: 6288.007812\n",
      "Train Epoch: 213 [119040/225000 (53%)] Loss: 6327.515625\n",
      "Train Epoch: 213 [123136/225000 (55%)] Loss: 6405.837891\n",
      "Train Epoch: 213 [127232/225000 (57%)] Loss: 6294.376953\n",
      "Train Epoch: 213 [131328/225000 (58%)] Loss: 6314.853516\n",
      "Train Epoch: 213 [135424/225000 (60%)] Loss: 6327.117188\n",
      "Train Epoch: 213 [139520/225000 (62%)] Loss: 6243.337891\n",
      "Train Epoch: 213 [143616/225000 (64%)] Loss: 6164.292969\n",
      "Train Epoch: 213 [147712/225000 (66%)] Loss: 6364.955078\n",
      "Train Epoch: 213 [151808/225000 (67%)] Loss: 6324.431641\n",
      "Train Epoch: 213 [155904/225000 (69%)] Loss: 6350.429688\n",
      "Train Epoch: 213 [160000/225000 (71%)] Loss: 6416.441406\n",
      "Train Epoch: 213 [164096/225000 (73%)] Loss: 6199.990234\n",
      "Train Epoch: 213 [168192/225000 (75%)] Loss: 6208.472656\n",
      "Train Epoch: 213 [172288/225000 (77%)] Loss: 6281.173828\n",
      "Train Epoch: 213 [176384/225000 (78%)] Loss: 6316.343750\n",
      "Train Epoch: 213 [180480/225000 (80%)] Loss: 6454.275391\n",
      "Train Epoch: 213 [184576/225000 (82%)] Loss: 6295.875000\n",
      "Train Epoch: 213 [188672/225000 (84%)] Loss: 6403.080078\n",
      "Train Epoch: 213 [192768/225000 (86%)] Loss: 6230.750000\n",
      "Train Epoch: 213 [196864/225000 (87%)] Loss: 6216.544922\n",
      "Train Epoch: 213 [200960/225000 (89%)] Loss: 6440.732422\n",
      "Train Epoch: 213 [205056/225000 (91%)] Loss: 6206.259766\n",
      "Train Epoch: 213 [209152/225000 (93%)] Loss: 6336.140625\n",
      "Train Epoch: 213 [213248/225000 (95%)] Loss: 6348.480469\n",
      "Train Epoch: 213 [217344/225000 (97%)] Loss: 6271.675781\n",
      "Train Epoch: 213 [221440/225000 (98%)] Loss: 6237.162109\n",
      "    epoch          : 213\n",
      "    loss           : 6428.415626777588\n",
      "    val_loss       : 6714.8749508164365\n",
      "Train Epoch: 214 [256/225000 (0%)] Loss: 6336.294922\n",
      "Train Epoch: 214 [4352/225000 (2%)] Loss: 6401.261719\n",
      "Train Epoch: 214 [8448/225000 (4%)] Loss: 12423.679688\n",
      "Train Epoch: 214 [12544/225000 (6%)] Loss: 6250.142578\n",
      "Train Epoch: 214 [16640/225000 (7%)] Loss: 6333.642578\n",
      "Train Epoch: 214 [20736/225000 (9%)] Loss: 6290.394531\n",
      "Train Epoch: 214 [24832/225000 (11%)] Loss: 6271.501953\n",
      "Train Epoch: 214 [28928/225000 (13%)] Loss: 6433.056641\n",
      "Train Epoch: 214 [33024/225000 (15%)] Loss: 6252.925781\n",
      "Train Epoch: 214 [37120/225000 (16%)] Loss: 6370.041016\n",
      "Train Epoch: 214 [41216/225000 (18%)] Loss: 6277.771484\n",
      "Train Epoch: 214 [45312/225000 (20%)] Loss: 6280.986328\n",
      "Train Epoch: 214 [49408/225000 (22%)] Loss: 6347.134766\n",
      "Train Epoch: 214 [53504/225000 (24%)] Loss: 6239.099609\n",
      "Train Epoch: 214 [57600/225000 (26%)] Loss: 6299.861328\n",
      "Train Epoch: 214 [61696/225000 (27%)] Loss: 6310.296875\n",
      "Train Epoch: 214 [65792/225000 (29%)] Loss: 6429.648438\n",
      "Train Epoch: 214 [69888/225000 (31%)] Loss: 6393.867188\n",
      "Train Epoch: 214 [73984/225000 (33%)] Loss: 6240.667969\n",
      "Train Epoch: 214 [78080/225000 (35%)] Loss: 6465.589844\n",
      "Train Epoch: 214 [82176/225000 (37%)] Loss: 6342.572266\n",
      "Train Epoch: 214 [86272/225000 (38%)] Loss: 6253.300781\n",
      "Train Epoch: 214 [90368/225000 (40%)] Loss: 6314.244141\n",
      "Train Epoch: 214 [94464/225000 (42%)] Loss: 6213.562500\n",
      "Train Epoch: 214 [98560/225000 (44%)] Loss: 6404.287109\n",
      "Train Epoch: 214 [102656/225000 (46%)] Loss: 6348.593750\n",
      "Train Epoch: 214 [106752/225000 (47%)] Loss: 6398.123047\n",
      "Train Epoch: 214 [110848/225000 (49%)] Loss: 6428.669922\n",
      "Train Epoch: 214 [114944/225000 (51%)] Loss: 6348.773438\n",
      "Train Epoch: 214 [119040/225000 (53%)] Loss: 6193.802734\n",
      "Train Epoch: 214 [123136/225000 (55%)] Loss: 6356.986328\n",
      "Train Epoch: 214 [127232/225000 (57%)] Loss: 6376.400391\n",
      "Train Epoch: 214 [131328/225000 (58%)] Loss: 6353.150391\n",
      "Train Epoch: 214 [135424/225000 (60%)] Loss: 6231.072266\n",
      "Train Epoch: 214 [139520/225000 (62%)] Loss: 6424.832031\n",
      "Train Epoch: 214 [143616/225000 (64%)] Loss: 6385.687500\n",
      "Train Epoch: 214 [147712/225000 (66%)] Loss: 6359.474609\n",
      "Train Epoch: 214 [151808/225000 (67%)] Loss: 6290.439453\n",
      "Train Epoch: 214 [155904/225000 (69%)] Loss: 6222.568359\n",
      "Train Epoch: 214 [160000/225000 (71%)] Loss: 6211.832031\n",
      "Train Epoch: 214 [164096/225000 (73%)] Loss: 6348.205078\n",
      "Train Epoch: 214 [168192/225000 (75%)] Loss: 6211.798828\n",
      "Train Epoch: 214 [172288/225000 (77%)] Loss: 6334.515625\n",
      "Train Epoch: 214 [176384/225000 (78%)] Loss: 6358.826172\n",
      "Train Epoch: 214 [180480/225000 (80%)] Loss: 6292.968750\n",
      "Train Epoch: 214 [184576/225000 (82%)] Loss: 6255.492188\n",
      "Train Epoch: 214 [188672/225000 (84%)] Loss: 6299.976562\n",
      "Train Epoch: 214 [192768/225000 (86%)] Loss: 6377.794922\n",
      "Train Epoch: 214 [196864/225000 (87%)] Loss: 6340.224609\n",
      "Train Epoch: 214 [200960/225000 (89%)] Loss: 6289.994141\n",
      "Train Epoch: 214 [205056/225000 (91%)] Loss: 6371.984375\n",
      "Train Epoch: 214 [209152/225000 (93%)] Loss: 6356.902344\n",
      "Train Epoch: 214 [213248/225000 (95%)] Loss: 6224.724609\n",
      "Train Epoch: 214 [217344/225000 (97%)] Loss: 6374.529297\n",
      "Train Epoch: 214 [221440/225000 (98%)] Loss: 6257.806641\n",
      "    epoch          : 214\n",
      "    loss           : 6351.167245493814\n",
      "    val_loss       : 6608.707964352199\n",
      "Train Epoch: 215 [256/225000 (0%)] Loss: 6367.423828\n",
      "Train Epoch: 215 [4352/225000 (2%)] Loss: 6468.353516\n",
      "Train Epoch: 215 [8448/225000 (4%)] Loss: 6322.708984\n",
      "Train Epoch: 215 [12544/225000 (6%)] Loss: 6274.359375\n",
      "Train Epoch: 215 [16640/225000 (7%)] Loss: 6431.048828\n",
      "Train Epoch: 215 [20736/225000 (9%)] Loss: 6313.738281\n",
      "Train Epoch: 215 [24832/225000 (11%)] Loss: 6198.552734\n",
      "Train Epoch: 215 [28928/225000 (13%)] Loss: 6391.480469\n",
      "Train Epoch: 215 [33024/225000 (15%)] Loss: 6268.707031\n",
      "Train Epoch: 215 [37120/225000 (16%)] Loss: 6322.896484\n",
      "Train Epoch: 215 [41216/225000 (18%)] Loss: 6394.634766\n",
      "Train Epoch: 215 [45312/225000 (20%)] Loss: 6349.044922\n",
      "Train Epoch: 215 [49408/225000 (22%)] Loss: 6297.580078\n",
      "Train Epoch: 215 [53504/225000 (24%)] Loss: 6287.865234\n",
      "Train Epoch: 215 [57600/225000 (26%)] Loss: 6172.037109\n",
      "Train Epoch: 215 [61696/225000 (27%)] Loss: 6364.498047\n",
      "Train Epoch: 215 [65792/225000 (29%)] Loss: 6352.810547\n",
      "Train Epoch: 215 [69888/225000 (31%)] Loss: 6373.351562\n",
      "Train Epoch: 215 [73984/225000 (33%)] Loss: 6231.552734\n",
      "Train Epoch: 215 [78080/225000 (35%)] Loss: 6448.875000\n",
      "Train Epoch: 215 [82176/225000 (37%)] Loss: 6332.154297\n",
      "Train Epoch: 215 [86272/225000 (38%)] Loss: 6300.253906\n",
      "Train Epoch: 215 [90368/225000 (40%)] Loss: 6350.343750\n",
      "Train Epoch: 215 [94464/225000 (42%)] Loss: 6311.744141\n",
      "Train Epoch: 215 [98560/225000 (44%)] Loss: 6410.853516\n",
      "Train Epoch: 215 [102656/225000 (46%)] Loss: 6439.500000\n",
      "Train Epoch: 215 [106752/225000 (47%)] Loss: 6361.619141\n",
      "Train Epoch: 215 [110848/225000 (49%)] Loss: 6214.974609\n",
      "Train Epoch: 215 [114944/225000 (51%)] Loss: 6181.119141\n",
      "Train Epoch: 215 [119040/225000 (53%)] Loss: 6248.394531\n",
      "Train Epoch: 215 [123136/225000 (55%)] Loss: 6428.378906\n",
      "Train Epoch: 215 [127232/225000 (57%)] Loss: 6237.511719\n",
      "Train Epoch: 215 [131328/225000 (58%)] Loss: 6342.992188\n",
      "Train Epoch: 215 [135424/225000 (60%)] Loss: 6328.029297\n",
      "Train Epoch: 215 [139520/225000 (62%)] Loss: 6348.570312\n",
      "Train Epoch: 215 [143616/225000 (64%)] Loss: 6428.115234\n",
      "Train Epoch: 215 [147712/225000 (66%)] Loss: 6202.216797\n",
      "Train Epoch: 215 [151808/225000 (67%)] Loss: 6357.525391\n",
      "Train Epoch: 215 [155904/225000 (69%)] Loss: 6255.980469\n",
      "Train Epoch: 215 [160000/225000 (71%)] Loss: 6330.107422\n",
      "Train Epoch: 215 [164096/225000 (73%)] Loss: 6446.128906\n",
      "Train Epoch: 215 [168192/225000 (75%)] Loss: 6431.218750\n",
      "Train Epoch: 215 [172288/225000 (77%)] Loss: 6327.656250\n",
      "Train Epoch: 215 [176384/225000 (78%)] Loss: 6140.710938\n",
      "Train Epoch: 215 [180480/225000 (80%)] Loss: 6393.923828\n",
      "Train Epoch: 215 [184576/225000 (82%)] Loss: 6307.031250\n",
      "Train Epoch: 215 [188672/225000 (84%)] Loss: 6363.455078\n",
      "Train Epoch: 215 [192768/225000 (86%)] Loss: 6406.519531\n",
      "Train Epoch: 215 [196864/225000 (87%)] Loss: 6358.095703\n",
      "Train Epoch: 215 [200960/225000 (89%)] Loss: 6496.808594\n",
      "Train Epoch: 215 [205056/225000 (91%)] Loss: 6486.714844\n",
      "Train Epoch: 215 [209152/225000 (93%)] Loss: 6217.060547\n",
      "Train Epoch: 215 [213248/225000 (95%)] Loss: 6138.888672\n",
      "Train Epoch: 215 [217344/225000 (97%)] Loss: 6419.414062\n",
      "Train Epoch: 215 [221440/225000 (98%)] Loss: 6486.341797\n",
      "    epoch          : 215\n",
      "    loss           : 6374.760143362486\n",
      "    val_loss       : 6646.28603044578\n",
      "Train Epoch: 216 [256/225000 (0%)] Loss: 6381.322266\n",
      "Train Epoch: 216 [4352/225000 (2%)] Loss: 6255.691406\n",
      "Train Epoch: 216 [8448/225000 (4%)] Loss: 6315.900391\n",
      "Train Epoch: 216 [12544/225000 (6%)] Loss: 6252.847656\n",
      "Train Epoch: 216 [16640/225000 (7%)] Loss: 6254.832031\n",
      "Train Epoch: 216 [20736/225000 (9%)] Loss: 6410.826172\n",
      "Train Epoch: 216 [24832/225000 (11%)] Loss: 6377.226562\n",
      "Train Epoch: 216 [28928/225000 (13%)] Loss: 6372.380859\n",
      "Train Epoch: 216 [33024/225000 (15%)] Loss: 6315.230469\n",
      "Train Epoch: 216 [37120/225000 (16%)] Loss: 6411.085938\n",
      "Train Epoch: 216 [41216/225000 (18%)] Loss: 6464.201172\n",
      "Train Epoch: 216 [45312/225000 (20%)] Loss: 6430.537109\n",
      "Train Epoch: 216 [49408/225000 (22%)] Loss: 6296.255859\n",
      "Train Epoch: 216 [53504/225000 (24%)] Loss: 6311.224609\n",
      "Train Epoch: 216 [57600/225000 (26%)] Loss: 6274.193359\n",
      "Train Epoch: 216 [61696/225000 (27%)] Loss: 6461.337891\n",
      "Train Epoch: 216 [65792/225000 (29%)] Loss: 6217.740234\n",
      "Train Epoch: 216 [69888/225000 (31%)] Loss: 6262.697266\n",
      "Train Epoch: 216 [73984/225000 (33%)] Loss: 6304.271484\n",
      "Train Epoch: 216 [78080/225000 (35%)] Loss: 6291.511719\n",
      "Train Epoch: 216 [82176/225000 (37%)] Loss: 6214.187500\n",
      "Train Epoch: 216 [86272/225000 (38%)] Loss: 6376.906250\n",
      "Train Epoch: 216 [90368/225000 (40%)] Loss: 6303.171875\n",
      "Train Epoch: 216 [94464/225000 (42%)] Loss: 6337.496094\n",
      "Train Epoch: 216 [98560/225000 (44%)] Loss: 6407.039062\n",
      "Train Epoch: 216 [102656/225000 (46%)] Loss: 6377.480469\n",
      "Train Epoch: 216 [106752/225000 (47%)] Loss: 6547.017578\n",
      "Train Epoch: 216 [110848/225000 (49%)] Loss: 6436.529297\n",
      "Train Epoch: 216 [114944/225000 (51%)] Loss: 6344.460938\n",
      "Train Epoch: 216 [119040/225000 (53%)] Loss: 6418.730469\n",
      "Train Epoch: 216 [123136/225000 (55%)] Loss: 6364.146484\n",
      "Train Epoch: 216 [127232/225000 (57%)] Loss: 6476.503906\n",
      "Train Epoch: 216 [131328/225000 (58%)] Loss: 6389.955078\n",
      "Train Epoch: 216 [135424/225000 (60%)] Loss: 6209.794922\n",
      "Train Epoch: 216 [139520/225000 (62%)] Loss: 6278.753906\n",
      "Train Epoch: 216 [143616/225000 (64%)] Loss: 6297.636719\n",
      "Train Epoch: 216 [147712/225000 (66%)] Loss: 6168.669922\n",
      "Train Epoch: 216 [151808/225000 (67%)] Loss: 6433.531250\n",
      "Train Epoch: 216 [155904/225000 (69%)] Loss: 6198.308594\n",
      "Train Epoch: 216 [160000/225000 (71%)] Loss: 6311.882812\n",
      "Train Epoch: 216 [164096/225000 (73%)] Loss: 6187.244141\n",
      "Train Epoch: 216 [168192/225000 (75%)] Loss: 6286.335938\n",
      "Train Epoch: 216 [172288/225000 (77%)] Loss: 6325.210938\n",
      "Train Epoch: 216 [176384/225000 (78%)] Loss: 6331.566406\n",
      "Train Epoch: 216 [180480/225000 (80%)] Loss: 6329.115234\n",
      "Train Epoch: 216 [184576/225000 (82%)] Loss: 6355.199219\n",
      "Train Epoch: 216 [188672/225000 (84%)] Loss: 6369.857422\n",
      "Train Epoch: 216 [192768/225000 (86%)] Loss: 6418.728516\n",
      "Train Epoch: 216 [196864/225000 (87%)] Loss: 6343.951172\n",
      "Train Epoch: 216 [200960/225000 (89%)] Loss: 6333.615234\n",
      "Train Epoch: 216 [205056/225000 (91%)] Loss: 6211.789062\n",
      "Train Epoch: 216 [209152/225000 (93%)] Loss: 6285.767578\n",
      "Train Epoch: 216 [213248/225000 (95%)] Loss: 6403.638672\n",
      "Train Epoch: 216 [217344/225000 (97%)] Loss: 6290.037109\n",
      "Train Epoch: 216 [221440/225000 (98%)] Loss: 6269.201172\n",
      "    epoch          : 216\n",
      "    loss           : 6358.871660356229\n",
      "    val_loss       : 6419.704697736672\n",
      "Train Epoch: 217 [256/225000 (0%)] Loss: 6371.625000\n",
      "Train Epoch: 217 [4352/225000 (2%)] Loss: 6300.078125\n",
      "Train Epoch: 217 [8448/225000 (4%)] Loss: 6258.019531\n",
      "Train Epoch: 217 [12544/225000 (6%)] Loss: 6390.414062\n",
      "Train Epoch: 217 [16640/225000 (7%)] Loss: 6423.152344\n",
      "Train Epoch: 217 [20736/225000 (9%)] Loss: 6310.074219\n",
      "Train Epoch: 217 [24832/225000 (11%)] Loss: 6272.246094\n",
      "Train Epoch: 217 [28928/225000 (13%)] Loss: 6353.751953\n",
      "Train Epoch: 217 [33024/225000 (15%)] Loss: 6371.990234\n",
      "Train Epoch: 217 [37120/225000 (16%)] Loss: 6322.283203\n",
      "Train Epoch: 217 [41216/225000 (18%)] Loss: 6316.775391\n",
      "Train Epoch: 217 [45312/225000 (20%)] Loss: 6300.515625\n",
      "Train Epoch: 217 [49408/225000 (22%)] Loss: 6329.457031\n",
      "Train Epoch: 217 [53504/225000 (24%)] Loss: 6327.419922\n",
      "Train Epoch: 217 [57600/225000 (26%)] Loss: 6305.208984\n",
      "Train Epoch: 217 [61696/225000 (27%)] Loss: 6523.250000\n",
      "Train Epoch: 217 [65792/225000 (29%)] Loss: 6379.097656\n",
      "Train Epoch: 217 [69888/225000 (31%)] Loss: 6307.978516\n",
      "Train Epoch: 217 [73984/225000 (33%)] Loss: 6288.853516\n",
      "Train Epoch: 217 [78080/225000 (35%)] Loss: 6377.505859\n",
      "Train Epoch: 217 [82176/225000 (37%)] Loss: 6452.722656\n",
      "Train Epoch: 217 [86272/225000 (38%)] Loss: 6413.759766\n",
      "Train Epoch: 217 [90368/225000 (40%)] Loss: 6260.361328\n",
      "Train Epoch: 217 [94464/225000 (42%)] Loss: 6260.111328\n",
      "Train Epoch: 217 [98560/225000 (44%)] Loss: 6239.998047\n",
      "Train Epoch: 217 [102656/225000 (46%)] Loss: 6208.052734\n",
      "Train Epoch: 217 [106752/225000 (47%)] Loss: 6350.767578\n",
      "Train Epoch: 217 [110848/225000 (49%)] Loss: 6338.607422\n",
      "Train Epoch: 217 [114944/225000 (51%)] Loss: 6268.564453\n",
      "Train Epoch: 217 [119040/225000 (53%)] Loss: 6436.527344\n",
      "Train Epoch: 217 [123136/225000 (55%)] Loss: 6190.496094\n",
      "Train Epoch: 217 [127232/225000 (57%)] Loss: 6357.746094\n",
      "Train Epoch: 217 [131328/225000 (58%)] Loss: 6236.451172\n",
      "Train Epoch: 217 [135424/225000 (60%)] Loss: 6296.742188\n",
      "Train Epoch: 217 [139520/225000 (62%)] Loss: 6384.671875\n",
      "Train Epoch: 217 [143616/225000 (64%)] Loss: 6412.533203\n",
      "Train Epoch: 217 [147712/225000 (66%)] Loss: 6149.992188\n",
      "Train Epoch: 217 [151808/225000 (67%)] Loss: 6195.250000\n",
      "Train Epoch: 217 [155904/225000 (69%)] Loss: 8116.310547\n",
      "Train Epoch: 217 [160000/225000 (71%)] Loss: 6229.939453\n",
      "Train Epoch: 217 [164096/225000 (73%)] Loss: 6179.275391\n",
      "Train Epoch: 217 [168192/225000 (75%)] Loss: 6480.777344\n",
      "Train Epoch: 217 [172288/225000 (77%)] Loss: 6339.017578\n",
      "Train Epoch: 217 [176384/225000 (78%)] Loss: 6412.894531\n",
      "Train Epoch: 217 [180480/225000 (80%)] Loss: 6420.208984\n",
      "Train Epoch: 217 [184576/225000 (82%)] Loss: 6359.716797\n",
      "Train Epoch: 217 [188672/225000 (84%)] Loss: 6371.433594\n",
      "Train Epoch: 217 [192768/225000 (86%)] Loss: 6341.400391\n",
      "Train Epoch: 217 [196864/225000 (87%)] Loss: 6319.396484\n",
      "Train Epoch: 217 [200960/225000 (89%)] Loss: 6235.542969\n",
      "Train Epoch: 217 [205056/225000 (91%)] Loss: 6353.962891\n",
      "Train Epoch: 217 [209152/225000 (93%)] Loss: 6330.578125\n",
      "Train Epoch: 217 [213248/225000 (95%)] Loss: 6380.222656\n",
      "Train Epoch: 217 [217344/225000 (97%)] Loss: 6301.742188\n",
      "Train Epoch: 217 [221440/225000 (98%)] Loss: 6239.783203\n",
      "    epoch          : 217\n",
      "    loss           : 6369.368798439277\n",
      "    val_loss       : 6525.412187775787\n",
      "Train Epoch: 218 [256/225000 (0%)] Loss: 6444.072266\n",
      "Train Epoch: 218 [4352/225000 (2%)] Loss: 6474.699219\n",
      "Train Epoch: 218 [8448/225000 (4%)] Loss: 6269.892578\n",
      "Train Epoch: 218 [12544/225000 (6%)] Loss: 6324.472656\n",
      "Train Epoch: 218 [16640/225000 (7%)] Loss: 6329.687500\n",
      "Train Epoch: 218 [20736/225000 (9%)] Loss: 6392.453125\n",
      "Train Epoch: 218 [24832/225000 (11%)] Loss: 6393.476562\n",
      "Train Epoch: 218 [28928/225000 (13%)] Loss: 6252.908203\n",
      "Train Epoch: 218 [33024/225000 (15%)] Loss: 6313.308594\n",
      "Train Epoch: 218 [37120/225000 (16%)] Loss: 6336.824219\n",
      "Train Epoch: 218 [41216/225000 (18%)] Loss: 6307.976562\n",
      "Train Epoch: 218 [45312/225000 (20%)] Loss: 6226.703125\n",
      "Train Epoch: 218 [49408/225000 (22%)] Loss: 6292.367188\n",
      "Train Epoch: 218 [53504/225000 (24%)] Loss: 6309.921875\n",
      "Train Epoch: 218 [57600/225000 (26%)] Loss: 6358.373047\n",
      "Train Epoch: 218 [61696/225000 (27%)] Loss: 6280.947266\n",
      "Train Epoch: 218 [65792/225000 (29%)] Loss: 6460.857422\n",
      "Train Epoch: 218 [69888/225000 (31%)] Loss: 6498.343750\n",
      "Train Epoch: 218 [73984/225000 (33%)] Loss: 6407.453125\n",
      "Train Epoch: 218 [78080/225000 (35%)] Loss: 6251.392578\n",
      "Train Epoch: 218 [82176/225000 (37%)] Loss: 6357.730469\n",
      "Train Epoch: 218 [86272/225000 (38%)] Loss: 6195.925781\n",
      "Train Epoch: 218 [90368/225000 (40%)] Loss: 6323.498047\n",
      "Train Epoch: 218 [94464/225000 (42%)] Loss: 6228.136719\n",
      "Train Epoch: 218 [98560/225000 (44%)] Loss: 6343.468750\n",
      "Train Epoch: 218 [102656/225000 (46%)] Loss: 6313.060547\n",
      "Train Epoch: 218 [106752/225000 (47%)] Loss: 6225.076172\n",
      "Train Epoch: 218 [110848/225000 (49%)] Loss: 6331.095703\n",
      "Train Epoch: 218 [114944/225000 (51%)] Loss: 6258.619141\n",
      "Train Epoch: 218 [119040/225000 (53%)] Loss: 6290.488281\n",
      "Train Epoch: 218 [123136/225000 (55%)] Loss: 6211.697266\n",
      "Train Epoch: 218 [127232/225000 (57%)] Loss: 6290.511719\n",
      "Train Epoch: 218 [131328/225000 (58%)] Loss: 6264.117188\n",
      "Train Epoch: 218 [135424/225000 (60%)] Loss: 6340.789062\n",
      "Train Epoch: 218 [139520/225000 (62%)] Loss: 6211.910156\n",
      "Train Epoch: 218 [143616/225000 (64%)] Loss: 6351.976562\n",
      "Train Epoch: 218 [147712/225000 (66%)] Loss: 6306.369141\n",
      "Train Epoch: 218 [151808/225000 (67%)] Loss: 6272.355469\n",
      "Train Epoch: 218 [155904/225000 (69%)] Loss: 6360.626953\n",
      "Train Epoch: 218 [160000/225000 (71%)] Loss: 6293.265625\n",
      "Train Epoch: 218 [164096/225000 (73%)] Loss: 6210.212891\n",
      "Train Epoch: 218 [168192/225000 (75%)] Loss: 6314.130859\n",
      "Train Epoch: 218 [172288/225000 (77%)] Loss: 6315.269531\n",
      "Train Epoch: 218 [176384/225000 (78%)] Loss: 6320.111328\n",
      "Train Epoch: 218 [180480/225000 (80%)] Loss: 6240.886719\n",
      "Train Epoch: 218 [184576/225000 (82%)] Loss: 6495.320312\n",
      "Train Epoch: 218 [188672/225000 (84%)] Loss: 6197.822266\n",
      "Train Epoch: 218 [192768/225000 (86%)] Loss: 6298.160156\n",
      "Train Epoch: 218 [196864/225000 (87%)] Loss: 6355.884766\n",
      "Train Epoch: 218 [200960/225000 (89%)] Loss: 6293.355469\n",
      "Train Epoch: 218 [205056/225000 (91%)] Loss: 6401.189453\n",
      "Train Epoch: 218 [209152/225000 (93%)] Loss: 6322.875000\n",
      "Train Epoch: 218 [213248/225000 (95%)] Loss: 6397.087891\n",
      "Train Epoch: 218 [217344/225000 (97%)] Loss: 6321.570312\n",
      "Train Epoch: 218 [221440/225000 (98%)] Loss: 6359.222656\n",
      "    epoch          : 218\n",
      "    loss           : 6387.171990543231\n",
      "    val_loss       : 6644.399723988407\n",
      "Train Epoch: 219 [256/225000 (0%)] Loss: 6331.500000\n",
      "Train Epoch: 219 [4352/225000 (2%)] Loss: 6374.685547\n",
      "Train Epoch: 219 [8448/225000 (4%)] Loss: 6216.861328\n",
      "Train Epoch: 219 [12544/225000 (6%)] Loss: 8141.431641\n",
      "Train Epoch: 219 [16640/225000 (7%)] Loss: 6253.296875\n",
      "Train Epoch: 219 [20736/225000 (9%)] Loss: 6312.867188\n",
      "Train Epoch: 219 [24832/225000 (11%)] Loss: 6249.925781\n",
      "Train Epoch: 219 [28928/225000 (13%)] Loss: 6302.064453\n",
      "Train Epoch: 219 [33024/225000 (15%)] Loss: 6363.121094\n",
      "Train Epoch: 219 [37120/225000 (16%)] Loss: 6214.857422\n",
      "Train Epoch: 219 [41216/225000 (18%)] Loss: 6361.361328\n",
      "Train Epoch: 219 [45312/225000 (20%)] Loss: 6298.808594\n",
      "Train Epoch: 219 [49408/225000 (22%)] Loss: 6392.892578\n",
      "Train Epoch: 219 [53504/225000 (24%)] Loss: 6432.197266\n",
      "Train Epoch: 219 [57600/225000 (26%)] Loss: 6222.972656\n",
      "Train Epoch: 219 [61696/225000 (27%)] Loss: 6340.580078\n",
      "Train Epoch: 219 [65792/225000 (29%)] Loss: 6331.160156\n",
      "Train Epoch: 219 [69888/225000 (31%)] Loss: 6214.537109\n",
      "Train Epoch: 219 [73984/225000 (33%)] Loss: 6184.833984\n",
      "Train Epoch: 219 [78080/225000 (35%)] Loss: 6239.566406\n",
      "Train Epoch: 219 [82176/225000 (37%)] Loss: 6328.673828\n",
      "Train Epoch: 219 [86272/225000 (38%)] Loss: 6396.910156\n",
      "Train Epoch: 219 [90368/225000 (40%)] Loss: 6400.722656\n",
      "Train Epoch: 219 [94464/225000 (42%)] Loss: 6308.539062\n",
      "Train Epoch: 219 [98560/225000 (44%)] Loss: 6410.210938\n",
      "Train Epoch: 219 [102656/225000 (46%)] Loss: 6275.220703\n",
      "Train Epoch: 219 [106752/225000 (47%)] Loss: 6476.257812\n",
      "Train Epoch: 219 [110848/225000 (49%)] Loss: 6455.121094\n",
      "Train Epoch: 219 [114944/225000 (51%)] Loss: 6257.388672\n",
      "Train Epoch: 219 [119040/225000 (53%)] Loss: 6295.070312\n",
      "Train Epoch: 219 [123136/225000 (55%)] Loss: 6328.964844\n",
      "Train Epoch: 219 [127232/225000 (57%)] Loss: 6487.099609\n",
      "Train Epoch: 219 [131328/225000 (58%)] Loss: 6296.332031\n",
      "Train Epoch: 219 [135424/225000 (60%)] Loss: 6297.955078\n",
      "Train Epoch: 219 [139520/225000 (62%)] Loss: 6395.033203\n",
      "Train Epoch: 219 [143616/225000 (64%)] Loss: 6222.078125\n",
      "Train Epoch: 219 [147712/225000 (66%)] Loss: 6355.787109\n",
      "Train Epoch: 219 [151808/225000 (67%)] Loss: 6240.789062\n",
      "Train Epoch: 219 [155904/225000 (69%)] Loss: 6377.169922\n",
      "Train Epoch: 219 [160000/225000 (71%)] Loss: 6358.830078\n",
      "Train Epoch: 219 [164096/225000 (73%)] Loss: 6395.246094\n",
      "Train Epoch: 219 [168192/225000 (75%)] Loss: 6411.777344\n",
      "Train Epoch: 219 [172288/225000 (77%)] Loss: 6272.144531\n",
      "Train Epoch: 219 [176384/225000 (78%)] Loss: 6242.839844\n",
      "Train Epoch: 219 [180480/225000 (80%)] Loss: 6334.232422\n",
      "Train Epoch: 219 [184576/225000 (82%)] Loss: 6376.582031\n",
      "Train Epoch: 219 [188672/225000 (84%)] Loss: 6331.207031\n",
      "Train Epoch: 219 [192768/225000 (86%)] Loss: 6222.078125\n",
      "Train Epoch: 219 [196864/225000 (87%)] Loss: 6271.419922\n",
      "Train Epoch: 219 [200960/225000 (89%)] Loss: 6462.695312\n",
      "Train Epoch: 219 [205056/225000 (91%)] Loss: 6366.423828\n",
      "Train Epoch: 219 [209152/225000 (93%)] Loss: 6213.859375\n",
      "Train Epoch: 219 [213248/225000 (95%)] Loss: 6388.013672\n",
      "Train Epoch: 219 [217344/225000 (97%)] Loss: 6446.744141\n",
      "Train Epoch: 219 [221440/225000 (98%)] Loss: 6292.921875\n",
      "    epoch          : 219\n",
      "    loss           : 6407.890526121658\n",
      "    val_loss       : 6419.388014648642\n",
      "Train Epoch: 220 [256/225000 (0%)] Loss: 6331.220703\n",
      "Train Epoch: 220 [4352/225000 (2%)] Loss: 6389.195312\n",
      "Train Epoch: 220 [8448/225000 (4%)] Loss: 6216.128906\n",
      "Train Epoch: 220 [12544/225000 (6%)] Loss: 6246.552734\n",
      "Train Epoch: 220 [16640/225000 (7%)] Loss: 6367.830078\n",
      "Train Epoch: 220 [20736/225000 (9%)] Loss: 6398.142578\n",
      "Train Epoch: 220 [24832/225000 (11%)] Loss: 6334.744141\n",
      "Train Epoch: 220 [28928/225000 (13%)] Loss: 6401.796875\n",
      "Train Epoch: 220 [33024/225000 (15%)] Loss: 6230.988281\n",
      "Train Epoch: 220 [37120/225000 (16%)] Loss: 6440.412109\n",
      "Train Epoch: 220 [41216/225000 (18%)] Loss: 6223.224609\n",
      "Train Epoch: 220 [45312/225000 (20%)] Loss: 6279.701172\n",
      "Train Epoch: 220 [49408/225000 (22%)] Loss: 6321.683594\n",
      "Train Epoch: 220 [53504/225000 (24%)] Loss: 6395.972656\n",
      "Train Epoch: 220 [57600/225000 (26%)] Loss: 6228.283203\n",
      "Train Epoch: 220 [61696/225000 (27%)] Loss: 6477.283203\n",
      "Train Epoch: 220 [65792/225000 (29%)] Loss: 6441.341797\n",
      "Train Epoch: 220 [69888/225000 (31%)] Loss: 6347.900391\n",
      "Train Epoch: 220 [73984/225000 (33%)] Loss: 6405.230469\n",
      "Train Epoch: 220 [78080/225000 (35%)] Loss: 6342.517578\n",
      "Train Epoch: 220 [82176/225000 (37%)] Loss: 6362.308594\n",
      "Train Epoch: 220 [86272/225000 (38%)] Loss: 6464.384766\n",
      "Train Epoch: 220 [90368/225000 (40%)] Loss: 6291.884766\n",
      "Train Epoch: 220 [94464/225000 (42%)] Loss: 6288.759766\n",
      "Train Epoch: 220 [98560/225000 (44%)] Loss: 6353.314453\n",
      "Train Epoch: 220 [102656/225000 (46%)] Loss: 6432.603516\n",
      "Train Epoch: 220 [106752/225000 (47%)] Loss: 6298.736328\n",
      "Train Epoch: 220 [110848/225000 (49%)] Loss: 6284.900391\n",
      "Train Epoch: 220 [114944/225000 (51%)] Loss: 6432.607422\n",
      "Train Epoch: 220 [119040/225000 (53%)] Loss: 6467.283203\n",
      "Train Epoch: 220 [123136/225000 (55%)] Loss: 6330.068359\n",
      "Train Epoch: 220 [127232/225000 (57%)] Loss: 6292.203125\n",
      "Train Epoch: 220 [131328/225000 (58%)] Loss: 6414.402344\n",
      "Train Epoch: 220 [135424/225000 (60%)] Loss: 6308.382812\n",
      "Train Epoch: 220 [139520/225000 (62%)] Loss: 6368.763672\n",
      "Train Epoch: 220 [143616/225000 (64%)] Loss: 6309.691406\n",
      "Train Epoch: 220 [147712/225000 (66%)] Loss: 6499.027344\n",
      "Train Epoch: 220 [151808/225000 (67%)] Loss: 6354.072266\n",
      "Train Epoch: 220 [155904/225000 (69%)] Loss: 6423.689453\n",
      "Train Epoch: 220 [160000/225000 (71%)] Loss: 6440.552734\n",
      "Train Epoch: 220 [164096/225000 (73%)] Loss: 6298.216797\n",
      "Train Epoch: 220 [168192/225000 (75%)] Loss: 6178.726562\n",
      "Train Epoch: 220 [172288/225000 (77%)] Loss: 6314.009766\n",
      "Train Epoch: 220 [176384/225000 (78%)] Loss: 6259.916016\n",
      "Train Epoch: 220 [180480/225000 (80%)] Loss: 6301.773438\n",
      "Train Epoch: 220 [184576/225000 (82%)] Loss: 6243.853516\n",
      "Train Epoch: 220 [188672/225000 (84%)] Loss: 8152.417969\n",
      "Train Epoch: 220 [192768/225000 (86%)] Loss: 6254.267578\n",
      "Train Epoch: 220 [196864/225000 (87%)] Loss: 6525.763672\n",
      "Train Epoch: 220 [200960/225000 (89%)] Loss: 6442.296875\n",
      "Train Epoch: 220 [205056/225000 (91%)] Loss: 6345.146484\n",
      "Train Epoch: 220 [209152/225000 (93%)] Loss: 6187.611328\n",
      "Train Epoch: 220 [213248/225000 (95%)] Loss: 6280.914062\n",
      "Train Epoch: 220 [217344/225000 (97%)] Loss: 6357.423828\n",
      "Train Epoch: 220 [221440/225000 (98%)] Loss: 6229.568359\n",
      "    epoch          : 220\n",
      "    loss           : 6387.946657912045\n",
      "    val_loss       : 6626.93353607095\n",
      "Train Epoch: 221 [256/225000 (0%)] Loss: 6345.539062\n",
      "Train Epoch: 221 [4352/225000 (2%)] Loss: 6249.707031\n",
      "Train Epoch: 221 [8448/225000 (4%)] Loss: 6203.099609\n",
      "Train Epoch: 221 [12544/225000 (6%)] Loss: 6399.437500\n",
      "Train Epoch: 221 [16640/225000 (7%)] Loss: 6284.878906\n",
      "Train Epoch: 221 [20736/225000 (9%)] Loss: 6317.125000\n",
      "Train Epoch: 221 [24832/225000 (11%)] Loss: 6461.089844\n",
      "Train Epoch: 221 [28928/225000 (13%)] Loss: 6239.126953\n",
      "Train Epoch: 221 [33024/225000 (15%)] Loss: 6219.289062\n",
      "Train Epoch: 221 [37120/225000 (16%)] Loss: 6287.265625\n",
      "Train Epoch: 221 [41216/225000 (18%)] Loss: 6243.572266\n",
      "Train Epoch: 221 [45312/225000 (20%)] Loss: 6323.664062\n",
      "Train Epoch: 221 [49408/225000 (22%)] Loss: 6402.382812\n",
      "Train Epoch: 221 [53504/225000 (24%)] Loss: 6431.921875\n",
      "Train Epoch: 221 [57600/225000 (26%)] Loss: 6310.935547\n",
      "Train Epoch: 221 [61696/225000 (27%)] Loss: 6333.015625\n",
      "Train Epoch: 221 [65792/225000 (29%)] Loss: 6303.521484\n",
      "Train Epoch: 221 [69888/225000 (31%)] Loss: 6254.015625\n",
      "Train Epoch: 221 [73984/225000 (33%)] Loss: 6434.177734\n",
      "Train Epoch: 221 [78080/225000 (35%)] Loss: 6429.521484\n",
      "Train Epoch: 221 [82176/225000 (37%)] Loss: 6216.998047\n",
      "Train Epoch: 221 [86272/225000 (38%)] Loss: 6351.701172\n",
      "Train Epoch: 221 [90368/225000 (40%)] Loss: 6404.267578\n",
      "Train Epoch: 221 [94464/225000 (42%)] Loss: 6223.636719\n",
      "Train Epoch: 221 [98560/225000 (44%)] Loss: 6274.451172\n",
      "Train Epoch: 221 [102656/225000 (46%)] Loss: 6288.732422\n",
      "Train Epoch: 221 [106752/225000 (47%)] Loss: 6235.662109\n",
      "Train Epoch: 221 [110848/225000 (49%)] Loss: 6362.500000\n",
      "Train Epoch: 221 [114944/225000 (51%)] Loss: 6273.339844\n",
      "Train Epoch: 221 [119040/225000 (53%)] Loss: 6258.417969\n",
      "Train Epoch: 221 [123136/225000 (55%)] Loss: 6346.751953\n",
      "Train Epoch: 221 [127232/225000 (57%)] Loss: 6317.984375\n",
      "Train Epoch: 221 [131328/225000 (58%)] Loss: 6303.914062\n",
      "Train Epoch: 221 [135424/225000 (60%)] Loss: 6217.392578\n",
      "Train Epoch: 221 [139520/225000 (62%)] Loss: 6419.003906\n",
      "Train Epoch: 221 [143616/225000 (64%)] Loss: 6258.023438\n",
      "Train Epoch: 221 [147712/225000 (66%)] Loss: 6166.111328\n",
      "Train Epoch: 221 [151808/225000 (67%)] Loss: 6318.671875\n",
      "Train Epoch: 221 [155904/225000 (69%)] Loss: 6389.341797\n",
      "Train Epoch: 221 [160000/225000 (71%)] Loss: 6277.214844\n",
      "Train Epoch: 221 [164096/225000 (73%)] Loss: 6318.419922\n",
      "Train Epoch: 221 [168192/225000 (75%)] Loss: 6361.605469\n",
      "Train Epoch: 221 [172288/225000 (77%)] Loss: 6326.964844\n",
      "Train Epoch: 221 [176384/225000 (78%)] Loss: 6247.806641\n",
      "Train Epoch: 221 [180480/225000 (80%)] Loss: 6398.177734\n",
      "Train Epoch: 221 [184576/225000 (82%)] Loss: 6308.990234\n",
      "Train Epoch: 221 [188672/225000 (84%)] Loss: 6367.785156\n",
      "Train Epoch: 221 [192768/225000 (86%)] Loss: 6354.718750\n",
      "Train Epoch: 221 [196864/225000 (87%)] Loss: 6315.781250\n",
      "Train Epoch: 221 [200960/225000 (89%)] Loss: 6283.916016\n",
      "Train Epoch: 221 [205056/225000 (91%)] Loss: 6232.189453\n",
      "Train Epoch: 221 [209152/225000 (93%)] Loss: 6420.583984\n",
      "Train Epoch: 221 [213248/225000 (95%)] Loss: 6477.396484\n",
      "Train Epoch: 221 [217344/225000 (97%)] Loss: 6387.541016\n",
      "Train Epoch: 221 [221440/225000 (98%)] Loss: 6381.160156\n",
      "    epoch          : 221\n",
      "    loss           : 6438.220519811221\n",
      "    val_loss       : 6401.222094403238\n",
      "Train Epoch: 222 [256/225000 (0%)] Loss: 6280.697266\n",
      "Train Epoch: 222 [4352/225000 (2%)] Loss: 6307.904297\n",
      "Train Epoch: 222 [8448/225000 (4%)] Loss: 6309.250000\n",
      "Train Epoch: 222 [12544/225000 (6%)] Loss: 6226.396484\n",
      "Train Epoch: 222 [16640/225000 (7%)] Loss: 6468.931641\n",
      "Train Epoch: 222 [20736/225000 (9%)] Loss: 6308.318359\n",
      "Train Epoch: 222 [24832/225000 (11%)] Loss: 6403.402344\n",
      "Train Epoch: 222 [28928/225000 (13%)] Loss: 8121.667969\n",
      "Train Epoch: 222 [33024/225000 (15%)] Loss: 6245.966797\n",
      "Train Epoch: 222 [37120/225000 (16%)] Loss: 6403.201172\n",
      "Train Epoch: 222 [41216/225000 (18%)] Loss: 6485.146484\n",
      "Train Epoch: 222 [45312/225000 (20%)] Loss: 6355.365234\n",
      "Train Epoch: 222 [49408/225000 (22%)] Loss: 6196.472656\n",
      "Train Epoch: 222 [53504/225000 (24%)] Loss: 6269.853516\n",
      "Train Epoch: 222 [57600/225000 (26%)] Loss: 6270.544922\n",
      "Train Epoch: 222 [61696/225000 (27%)] Loss: 6306.910156\n",
      "Train Epoch: 222 [65792/225000 (29%)] Loss: 8165.279297\n",
      "Train Epoch: 222 [69888/225000 (31%)] Loss: 6299.136719\n",
      "Train Epoch: 222 [73984/225000 (33%)] Loss: 6404.189453\n",
      "Train Epoch: 222 [78080/225000 (35%)] Loss: 6242.544922\n",
      "Train Epoch: 222 [82176/225000 (37%)] Loss: 6246.560547\n",
      "Train Epoch: 222 [86272/225000 (38%)] Loss: 6331.429688\n",
      "Train Epoch: 222 [90368/225000 (40%)] Loss: 6184.826172\n",
      "Train Epoch: 222 [94464/225000 (42%)] Loss: 6467.677734\n",
      "Train Epoch: 222 [98560/225000 (44%)] Loss: 6354.828125\n",
      "Train Epoch: 222 [102656/225000 (46%)] Loss: 6412.189453\n",
      "Train Epoch: 222 [106752/225000 (47%)] Loss: 6480.087891\n",
      "Train Epoch: 222 [110848/225000 (49%)] Loss: 6377.638672\n",
      "Train Epoch: 222 [114944/225000 (51%)] Loss: 6282.031250\n",
      "Train Epoch: 222 [119040/225000 (53%)] Loss: 6465.294922\n",
      "Train Epoch: 222 [123136/225000 (55%)] Loss: 6309.423828\n",
      "Train Epoch: 222 [127232/225000 (57%)] Loss: 6356.152344\n",
      "Train Epoch: 222 [131328/225000 (58%)] Loss: 6298.363281\n",
      "Train Epoch: 222 [135424/225000 (60%)] Loss: 6314.560547\n",
      "Train Epoch: 222 [139520/225000 (62%)] Loss: 6323.851562\n",
      "Train Epoch: 222 [143616/225000 (64%)] Loss: 6421.222656\n",
      "Train Epoch: 222 [147712/225000 (66%)] Loss: 6334.046875\n",
      "Train Epoch: 222 [151808/225000 (67%)] Loss: 6347.347656\n",
      "Train Epoch: 222 [155904/225000 (69%)] Loss: 6261.105469\n",
      "Train Epoch: 222 [160000/225000 (71%)] Loss: 6425.748047\n",
      "Train Epoch: 222 [164096/225000 (73%)] Loss: 6236.310547\n",
      "Train Epoch: 222 [168192/225000 (75%)] Loss: 6462.453125\n",
      "Train Epoch: 222 [172288/225000 (77%)] Loss: 6242.326172\n",
      "Train Epoch: 222 [176384/225000 (78%)] Loss: 6397.582031\n",
      "Train Epoch: 222 [180480/225000 (80%)] Loss: 6475.798828\n",
      "Train Epoch: 222 [184576/225000 (82%)] Loss: 6370.482422\n",
      "Train Epoch: 222 [188672/225000 (84%)] Loss: 6326.402344\n",
      "Train Epoch: 222 [192768/225000 (86%)] Loss: 6398.964844\n",
      "Train Epoch: 222 [196864/225000 (87%)] Loss: 6357.380859\n",
      "Train Epoch: 222 [200960/225000 (89%)] Loss: 6435.984375\n",
      "Train Epoch: 222 [205056/225000 (91%)] Loss: 6154.679688\n",
      "Train Epoch: 222 [209152/225000 (93%)] Loss: 8165.541016\n",
      "Train Epoch: 222 [213248/225000 (95%)] Loss: 6317.445312\n",
      "Train Epoch: 222 [217344/225000 (97%)] Loss: 6378.888672\n",
      "Train Epoch: 222 [221440/225000 (98%)] Loss: 6296.623047\n",
      "    epoch          : 222\n",
      "    loss           : 6377.877367525241\n",
      "    val_loss       : 6401.214230733259\n",
      "Train Epoch: 223 [256/225000 (0%)] Loss: 6447.593750\n",
      "Train Epoch: 223 [4352/225000 (2%)] Loss: 6420.105469\n",
      "Train Epoch: 223 [8448/225000 (4%)] Loss: 6302.744141\n",
      "Train Epoch: 223 [12544/225000 (6%)] Loss: 6197.375000\n",
      "Train Epoch: 223 [16640/225000 (7%)] Loss: 6434.531250\n",
      "Train Epoch: 223 [20736/225000 (9%)] Loss: 6377.148438\n",
      "Train Epoch: 223 [24832/225000 (11%)] Loss: 6579.335938\n",
      "Train Epoch: 223 [28928/225000 (13%)] Loss: 6410.660156\n",
      "Train Epoch: 223 [33024/225000 (15%)] Loss: 6464.666016\n",
      "Train Epoch: 223 [37120/225000 (16%)] Loss: 6324.349609\n",
      "Train Epoch: 223 [41216/225000 (18%)] Loss: 6333.236328\n",
      "Train Epoch: 223 [45312/225000 (20%)] Loss: 6308.818359\n",
      "Train Epoch: 223 [49408/225000 (22%)] Loss: 6196.941406\n",
      "Train Epoch: 223 [53504/225000 (24%)] Loss: 6237.681641\n",
      "Train Epoch: 223 [57600/225000 (26%)] Loss: 6365.609375\n",
      "Train Epoch: 223 [61696/225000 (27%)] Loss: 6260.455078\n",
      "Train Epoch: 223 [65792/225000 (29%)] Loss: 6306.775391\n",
      "Train Epoch: 223 [69888/225000 (31%)] Loss: 6354.013672\n",
      "Train Epoch: 223 [73984/225000 (33%)] Loss: 6304.638672\n",
      "Train Epoch: 223 [78080/225000 (35%)] Loss: 6431.865234\n",
      "Train Epoch: 223 [82176/225000 (37%)] Loss: 6397.582031\n",
      "Train Epoch: 223 [86272/225000 (38%)] Loss: 6196.486328\n",
      "Train Epoch: 223 [90368/225000 (40%)] Loss: 6295.748047\n",
      "Train Epoch: 223 [94464/225000 (42%)] Loss: 6436.283203\n",
      "Train Epoch: 223 [98560/225000 (44%)] Loss: 6335.376953\n",
      "Train Epoch: 223 [102656/225000 (46%)] Loss: 6334.439453\n",
      "Train Epoch: 223 [106752/225000 (47%)] Loss: 6273.298828\n",
      "Train Epoch: 223 [110848/225000 (49%)] Loss: 6394.681641\n",
      "Train Epoch: 223 [114944/225000 (51%)] Loss: 6330.326172\n",
      "Train Epoch: 223 [119040/225000 (53%)] Loss: 6250.714844\n",
      "Train Epoch: 223 [123136/225000 (55%)] Loss: 6338.771484\n",
      "Train Epoch: 223 [127232/225000 (57%)] Loss: 6299.955078\n",
      "Train Epoch: 223 [131328/225000 (58%)] Loss: 6269.794922\n",
      "Train Epoch: 223 [135424/225000 (60%)] Loss: 6329.632812\n",
      "Train Epoch: 223 [139520/225000 (62%)] Loss: 6258.701172\n",
      "Train Epoch: 223 [143616/225000 (64%)] Loss: 6263.640625\n",
      "Train Epoch: 223 [147712/225000 (66%)] Loss: 6371.476562\n",
      "Train Epoch: 223 [151808/225000 (67%)] Loss: 6293.753906\n",
      "Train Epoch: 223 [155904/225000 (69%)] Loss: 6350.537109\n",
      "Train Epoch: 223 [160000/225000 (71%)] Loss: 6403.904297\n",
      "Train Epoch: 223 [164096/225000 (73%)] Loss: 6421.833984\n",
      "Train Epoch: 223 [168192/225000 (75%)] Loss: 6429.210938\n",
      "Train Epoch: 223 [172288/225000 (77%)] Loss: 6275.056641\n",
      "Train Epoch: 223 [176384/225000 (78%)] Loss: 6245.884766\n",
      "Train Epoch: 223 [180480/225000 (80%)] Loss: 6252.308594\n",
      "Train Epoch: 223 [184576/225000 (82%)] Loss: 6264.851562\n",
      "Train Epoch: 223 [188672/225000 (84%)] Loss: 6388.550781\n",
      "Train Epoch: 223 [192768/225000 (86%)] Loss: 6369.376953\n",
      "Train Epoch: 223 [196864/225000 (87%)] Loss: 6273.318359\n",
      "Train Epoch: 223 [200960/225000 (89%)] Loss: 6286.746094\n",
      "Train Epoch: 223 [205056/225000 (91%)] Loss: 6341.195312\n",
      "Train Epoch: 223 [209152/225000 (93%)] Loss: 6333.880859\n",
      "Train Epoch: 223 [213248/225000 (95%)] Loss: 6390.654297\n",
      "Train Epoch: 223 [217344/225000 (97%)] Loss: 6431.892578\n",
      "Train Epoch: 223 [221440/225000 (98%)] Loss: 6392.408203\n",
      "    epoch          : 223\n",
      "    loss           : 6401.22197410054\n",
      "    val_loss       : 6419.715427384693\n",
      "Train Epoch: 224 [256/225000 (0%)] Loss: 6252.310547\n",
      "Train Epoch: 224 [4352/225000 (2%)] Loss: 6262.517578\n",
      "Train Epoch: 224 [8448/225000 (4%)] Loss: 6355.675781\n",
      "Train Epoch: 224 [12544/225000 (6%)] Loss: 6376.013672\n",
      "Train Epoch: 224 [16640/225000 (7%)] Loss: 6282.861328\n",
      "Train Epoch: 224 [20736/225000 (9%)] Loss: 6598.812500\n",
      "Train Epoch: 224 [24832/225000 (11%)] Loss: 6359.275391\n",
      "Train Epoch: 224 [28928/225000 (13%)] Loss: 6279.566406\n",
      "Train Epoch: 224 [33024/225000 (15%)] Loss: 6282.263672\n",
      "Train Epoch: 224 [37120/225000 (16%)] Loss: 6389.089844\n",
      "Train Epoch: 224 [41216/225000 (18%)] Loss: 6326.873047\n",
      "Train Epoch: 224 [45312/225000 (20%)] Loss: 6312.509766\n",
      "Train Epoch: 224 [49408/225000 (22%)] Loss: 6396.544922\n",
      "Train Epoch: 224 [53504/225000 (24%)] Loss: 6396.814453\n",
      "Train Epoch: 224 [57600/225000 (26%)] Loss: 6378.835938\n",
      "Train Epoch: 224 [61696/225000 (27%)] Loss: 6290.757812\n",
      "Train Epoch: 224 [65792/225000 (29%)] Loss: 6367.871094\n",
      "Train Epoch: 224 [69888/225000 (31%)] Loss: 6141.191406\n",
      "Train Epoch: 224 [73984/225000 (33%)] Loss: 6317.541016\n",
      "Train Epoch: 224 [78080/225000 (35%)] Loss: 6409.853516\n",
      "Train Epoch: 224 [82176/225000 (37%)] Loss: 6272.490234\n",
      "Train Epoch: 224 [86272/225000 (38%)] Loss: 6193.937500\n",
      "Train Epoch: 224 [90368/225000 (40%)] Loss: 6289.867188\n",
      "Train Epoch: 224 [94464/225000 (42%)] Loss: 6209.587891\n",
      "Train Epoch: 224 [98560/225000 (44%)] Loss: 6307.312500\n",
      "Train Epoch: 224 [102656/225000 (46%)] Loss: 6355.142578\n",
      "Train Epoch: 224 [106752/225000 (47%)] Loss: 6387.382812\n",
      "Train Epoch: 224 [110848/225000 (49%)] Loss: 6303.294922\n",
      "Train Epoch: 224 [114944/225000 (51%)] Loss: 6275.712891\n",
      "Train Epoch: 224 [119040/225000 (53%)] Loss: 6354.548828\n",
      "Train Epoch: 224 [123136/225000 (55%)] Loss: 6441.806641\n",
      "Train Epoch: 224 [127232/225000 (57%)] Loss: 6396.431641\n",
      "Train Epoch: 224 [131328/225000 (58%)] Loss: 6365.326172\n",
      "Train Epoch: 224 [135424/225000 (60%)] Loss: 6258.042969\n",
      "Train Epoch: 224 [139520/225000 (62%)] Loss: 6349.246094\n",
      "Train Epoch: 224 [143616/225000 (64%)] Loss: 6305.755859\n",
      "Train Epoch: 224 [147712/225000 (66%)] Loss: 6235.796875\n",
      "Train Epoch: 224 [151808/225000 (67%)] Loss: 6258.138672\n",
      "Train Epoch: 224 [155904/225000 (69%)] Loss: 8237.585938\n",
      "Train Epoch: 224 [160000/225000 (71%)] Loss: 6406.464844\n",
      "Train Epoch: 224 [164096/225000 (73%)] Loss: 6301.966797\n",
      "Train Epoch: 224 [168192/225000 (75%)] Loss: 6206.564453\n",
      "Train Epoch: 224 [172288/225000 (77%)] Loss: 6341.486328\n",
      "Train Epoch: 224 [176384/225000 (78%)] Loss: 6508.849609\n",
      "Train Epoch: 224 [180480/225000 (80%)] Loss: 6285.484375\n",
      "Train Epoch: 224 [184576/225000 (82%)] Loss: 6316.277344\n",
      "Train Epoch: 224 [188672/225000 (84%)] Loss: 6348.080078\n",
      "Train Epoch: 224 [192768/225000 (86%)] Loss: 6461.244141\n",
      "Train Epoch: 224 [196864/225000 (87%)] Loss: 6317.611328\n",
      "Train Epoch: 224 [200960/225000 (89%)] Loss: 6378.550781\n",
      "Train Epoch: 224 [205056/225000 (91%)] Loss: 6366.759766\n",
      "Train Epoch: 224 [209152/225000 (93%)] Loss: 6273.007812\n",
      "Train Epoch: 224 [213248/225000 (95%)] Loss: 6217.466797\n",
      "Train Epoch: 224 [217344/225000 (97%)] Loss: 6387.828125\n",
      "Train Epoch: 224 [221440/225000 (98%)] Loss: 6330.710938\n",
      "    epoch          : 224\n",
      "    loss           : 6353.187055602958\n",
      "    val_loss       : 6608.854440881281\n",
      "Train Epoch: 225 [256/225000 (0%)] Loss: 6445.074219\n",
      "Train Epoch: 225 [4352/225000 (2%)] Loss: 6307.328125\n",
      "Train Epoch: 225 [8448/225000 (4%)] Loss: 6218.785156\n",
      "Train Epoch: 225 [12544/225000 (6%)] Loss: 6339.654297\n",
      "Train Epoch: 225 [16640/225000 (7%)] Loss: 6279.369141\n",
      "Train Epoch: 225 [20736/225000 (9%)] Loss: 6494.843750\n",
      "Train Epoch: 225 [24832/225000 (11%)] Loss: 6299.841797\n",
      "Train Epoch: 225 [28928/225000 (13%)] Loss: 6225.535156\n",
      "Train Epoch: 225 [33024/225000 (15%)] Loss: 6289.919922\n",
      "Train Epoch: 225 [37120/225000 (16%)] Loss: 6213.146484\n",
      "Train Epoch: 225 [41216/225000 (18%)] Loss: 6243.435547\n",
      "Train Epoch: 225 [45312/225000 (20%)] Loss: 6534.408203\n",
      "Train Epoch: 225 [49408/225000 (22%)] Loss: 6268.587891\n",
      "Train Epoch: 225 [53504/225000 (24%)] Loss: 6304.572266\n",
      "Train Epoch: 225 [57600/225000 (26%)] Loss: 6369.648438\n",
      "Train Epoch: 225 [61696/225000 (27%)] Loss: 6226.396484\n",
      "Train Epoch: 225 [65792/225000 (29%)] Loss: 6278.394531\n",
      "Train Epoch: 225 [69888/225000 (31%)] Loss: 6206.873047\n",
      "Train Epoch: 225 [73984/225000 (33%)] Loss: 6242.115234\n",
      "Train Epoch: 225 [78080/225000 (35%)] Loss: 6359.355469\n",
      "Train Epoch: 225 [82176/225000 (37%)] Loss: 6466.998047\n",
      "Train Epoch: 225 [86272/225000 (38%)] Loss: 6420.607422\n",
      "Train Epoch: 225 [90368/225000 (40%)] Loss: 6492.306641\n",
      "Train Epoch: 225 [94464/225000 (42%)] Loss: 6383.500000\n",
      "Train Epoch: 225 [98560/225000 (44%)] Loss: 6361.789062\n",
      "Train Epoch: 225 [102656/225000 (46%)] Loss: 6280.972656\n",
      "Train Epoch: 225 [106752/225000 (47%)] Loss: 6310.322266\n",
      "Train Epoch: 225 [110848/225000 (49%)] Loss: 6326.648438\n",
      "Train Epoch: 225 [114944/225000 (51%)] Loss: 6299.572266\n",
      "Train Epoch: 225 [119040/225000 (53%)] Loss: 6357.371094\n",
      "Train Epoch: 225 [123136/225000 (55%)] Loss: 6496.451172\n",
      "Train Epoch: 225 [127232/225000 (57%)] Loss: 6356.189453\n",
      "Train Epoch: 225 [131328/225000 (58%)] Loss: 6327.677734\n",
      "Train Epoch: 225 [135424/225000 (60%)] Loss: 6439.085938\n",
      "Train Epoch: 225 [139520/225000 (62%)] Loss: 6359.179688\n",
      "Train Epoch: 225 [143616/225000 (64%)] Loss: 6411.910156\n",
      "Train Epoch: 225 [147712/225000 (66%)] Loss: 6286.470703\n",
      "Train Epoch: 225 [151808/225000 (67%)] Loss: 6359.113281\n",
      "Train Epoch: 225 [155904/225000 (69%)] Loss: 6427.203125\n",
      "Train Epoch: 225 [160000/225000 (71%)] Loss: 6258.453125\n",
      "Train Epoch: 225 [164096/225000 (73%)] Loss: 6517.800781\n",
      "Train Epoch: 225 [168192/225000 (75%)] Loss: 6342.306641\n",
      "Train Epoch: 225 [172288/225000 (77%)] Loss: 6362.904297\n",
      "Train Epoch: 225 [176384/225000 (78%)] Loss: 6403.003906\n",
      "Train Epoch: 225 [180480/225000 (80%)] Loss: 6379.667969\n",
      "Train Epoch: 225 [184576/225000 (82%)] Loss: 6300.867188\n",
      "Train Epoch: 225 [188672/225000 (84%)] Loss: 6305.773438\n",
      "Train Epoch: 225 [192768/225000 (86%)] Loss: 6350.000000\n",
      "Train Epoch: 225 [196864/225000 (87%)] Loss: 8166.527344\n",
      "Train Epoch: 225 [200960/225000 (89%)] Loss: 6317.521484\n",
      "Train Epoch: 225 [205056/225000 (91%)] Loss: 6335.751953\n",
      "Train Epoch: 225 [209152/225000 (93%)] Loss: 6306.740234\n",
      "Train Epoch: 225 [213248/225000 (95%)] Loss: 6368.750000\n",
      "Train Epoch: 225 [217344/225000 (97%)] Loss: 6331.650391\n",
      "Train Epoch: 225 [221440/225000 (98%)] Loss: 6384.150391\n",
      "    epoch          : 225\n",
      "    loss           : 6394.056204004906\n",
      "    val_loss       : 6401.235749772009\n",
      "Train Epoch: 226 [256/225000 (0%)] Loss: 6393.246094\n",
      "Train Epoch: 226 [4352/225000 (2%)] Loss: 6316.732422\n",
      "Train Epoch: 226 [8448/225000 (4%)] Loss: 6432.990234\n",
      "Train Epoch: 226 [12544/225000 (6%)] Loss: 6404.007812\n",
      "Train Epoch: 226 [16640/225000 (7%)] Loss: 6438.806641\n",
      "Train Epoch: 226 [20736/225000 (9%)] Loss: 6265.234375\n",
      "Train Epoch: 226 [24832/225000 (11%)] Loss: 6341.199219\n",
      "Train Epoch: 226 [28928/225000 (13%)] Loss: 6257.316406\n",
      "Train Epoch: 226 [33024/225000 (15%)] Loss: 6318.746094\n",
      "Train Epoch: 226 [37120/225000 (16%)] Loss: 6481.064453\n",
      "Train Epoch: 226 [41216/225000 (18%)] Loss: 6280.546875\n",
      "Train Epoch: 226 [45312/225000 (20%)] Loss: 6309.980469\n",
      "Train Epoch: 226 [49408/225000 (22%)] Loss: 6423.578125\n",
      "Train Epoch: 226 [53504/225000 (24%)] Loss: 6380.537109\n",
      "Train Epoch: 226 [57600/225000 (26%)] Loss: 6314.242188\n",
      "Train Epoch: 226 [61696/225000 (27%)] Loss: 6324.482422\n",
      "Train Epoch: 226 [65792/225000 (29%)] Loss: 6375.007812\n",
      "Train Epoch: 226 [69888/225000 (31%)] Loss: 6386.998047\n",
      "Train Epoch: 226 [73984/225000 (33%)] Loss: 6383.625000\n",
      "Train Epoch: 226 [78080/225000 (35%)] Loss: 6525.708984\n",
      "Train Epoch: 226 [82176/225000 (37%)] Loss: 6317.759766\n",
      "Train Epoch: 226 [86272/225000 (38%)] Loss: 6388.250000\n",
      "Train Epoch: 226 [90368/225000 (40%)] Loss: 6279.509766\n",
      "Train Epoch: 226 [94464/225000 (42%)] Loss: 6427.380859\n",
      "Train Epoch: 226 [98560/225000 (44%)] Loss: 6317.607422\n",
      "Train Epoch: 226 [102656/225000 (46%)] Loss: 6311.121094\n",
      "Train Epoch: 226 [106752/225000 (47%)] Loss: 6350.820312\n",
      "Train Epoch: 226 [110848/225000 (49%)] Loss: 6290.716797\n",
      "Train Epoch: 226 [114944/225000 (51%)] Loss: 6371.421875\n",
      "Train Epoch: 226 [119040/225000 (53%)] Loss: 6484.240234\n",
      "Train Epoch: 226 [123136/225000 (55%)] Loss: 6305.666016\n",
      "Train Epoch: 226 [127232/225000 (57%)] Loss: 6361.337891\n",
      "Train Epoch: 226 [131328/225000 (58%)] Loss: 6277.771484\n",
      "Train Epoch: 226 [135424/225000 (60%)] Loss: 6234.326172\n",
      "Train Epoch: 226 [139520/225000 (62%)] Loss: 6345.714844\n",
      "Train Epoch: 226 [143616/225000 (64%)] Loss: 6348.304688\n",
      "Train Epoch: 226 [147712/225000 (66%)] Loss: 6274.644531\n",
      "Train Epoch: 226 [151808/225000 (67%)] Loss: 6378.535156\n",
      "Train Epoch: 226 [155904/225000 (69%)] Loss: 6254.855469\n",
      "Train Epoch: 226 [160000/225000 (71%)] Loss: 6275.285156\n",
      "Train Epoch: 226 [164096/225000 (73%)] Loss: 6379.816406\n",
      "Train Epoch: 226 [168192/225000 (75%)] Loss: 6269.859375\n",
      "Train Epoch: 226 [172288/225000 (77%)] Loss: 6292.976562\n",
      "Train Epoch: 226 [176384/225000 (78%)] Loss: 6343.263672\n",
      "Train Epoch: 226 [180480/225000 (80%)] Loss: 6251.695312\n",
      "Train Epoch: 226 [184576/225000 (82%)] Loss: 6307.623047\n",
      "Train Epoch: 226 [188672/225000 (84%)] Loss: 6383.257812\n",
      "Train Epoch: 226 [192768/225000 (86%)] Loss: 6263.255859\n",
      "Train Epoch: 226 [196864/225000 (87%)] Loss: 6369.166016\n",
      "Train Epoch: 226 [200960/225000 (89%)] Loss: 6251.687500\n",
      "Train Epoch: 226 [205056/225000 (91%)] Loss: 6341.767578\n",
      "Train Epoch: 226 [209152/225000 (93%)] Loss: 6175.251953\n",
      "Train Epoch: 226 [213248/225000 (95%)] Loss: 6169.466797\n",
      "Train Epoch: 226 [217344/225000 (97%)] Loss: 6477.636719\n",
      "Train Epoch: 226 [221440/225000 (98%)] Loss: 6338.148438\n",
      "    epoch          : 226\n",
      "    loss           : 6386.3380639398465\n",
      "    val_loss       : 6401.2318085127945\n",
      "Train Epoch: 227 [256/225000 (0%)] Loss: 6313.880859\n",
      "Train Epoch: 227 [4352/225000 (2%)] Loss: 6232.667969\n",
      "Train Epoch: 227 [8448/225000 (4%)] Loss: 6296.849609\n",
      "Train Epoch: 227 [12544/225000 (6%)] Loss: 6373.632812\n",
      "Train Epoch: 227 [16640/225000 (7%)] Loss: 6274.789062\n",
      "Train Epoch: 227 [20736/225000 (9%)] Loss: 6235.863281\n",
      "Train Epoch: 227 [24832/225000 (11%)] Loss: 6529.091797\n",
      "Train Epoch: 227 [28928/225000 (13%)] Loss: 6412.810547\n",
      "Train Epoch: 227 [33024/225000 (15%)] Loss: 6353.304688\n",
      "Train Epoch: 227 [37120/225000 (16%)] Loss: 6223.140625\n",
      "Train Epoch: 227 [41216/225000 (18%)] Loss: 6254.185547\n",
      "Train Epoch: 227 [45312/225000 (20%)] Loss: 6288.429688\n",
      "Train Epoch: 227 [49408/225000 (22%)] Loss: 6462.265625\n",
      "Train Epoch: 227 [53504/225000 (24%)] Loss: 6381.710938\n",
      "Train Epoch: 227 [57600/225000 (26%)] Loss: 6430.609375\n",
      "Train Epoch: 227 [61696/225000 (27%)] Loss: 6238.347656\n",
      "Train Epoch: 227 [65792/225000 (29%)] Loss: 6454.341797\n",
      "Train Epoch: 227 [69888/225000 (31%)] Loss: 6454.265625\n",
      "Train Epoch: 227 [73984/225000 (33%)] Loss: 6387.904297\n",
      "Train Epoch: 227 [78080/225000 (35%)] Loss: 6318.298828\n",
      "Train Epoch: 227 [82176/225000 (37%)] Loss: 6362.576172\n",
      "Train Epoch: 227 [86272/225000 (38%)] Loss: 6367.921875\n",
      "Train Epoch: 227 [90368/225000 (40%)] Loss: 6350.761719\n",
      "Train Epoch: 227 [94464/225000 (42%)] Loss: 6334.371094\n",
      "Train Epoch: 227 [98560/225000 (44%)] Loss: 6377.308594\n",
      "Train Epoch: 227 [102656/225000 (46%)] Loss: 6337.195312\n",
      "Train Epoch: 227 [106752/225000 (47%)] Loss: 6326.285156\n",
      "Train Epoch: 227 [110848/225000 (49%)] Loss: 6380.187500\n",
      "Train Epoch: 227 [114944/225000 (51%)] Loss: 8215.582031\n",
      "Train Epoch: 227 [119040/225000 (53%)] Loss: 6388.384766\n",
      "Train Epoch: 227 [123136/225000 (55%)] Loss: 6350.539062\n",
      "Train Epoch: 227 [127232/225000 (57%)] Loss: 6377.310547\n",
      "Train Epoch: 227 [131328/225000 (58%)] Loss: 6345.470703\n",
      "Train Epoch: 227 [135424/225000 (60%)] Loss: 6376.949219\n",
      "Train Epoch: 227 [139520/225000 (62%)] Loss: 6397.271484\n",
      "Train Epoch: 227 [143616/225000 (64%)] Loss: 6269.189453\n",
      "Train Epoch: 227 [147712/225000 (66%)] Loss: 6318.796875\n",
      "Train Epoch: 227 [151808/225000 (67%)] Loss: 6352.582031\n",
      "Train Epoch: 227 [155904/225000 (69%)] Loss: 6394.457031\n",
      "Train Epoch: 227 [160000/225000 (71%)] Loss: 6351.382812\n",
      "Train Epoch: 227 [164096/225000 (73%)] Loss: 6372.486328\n",
      "Train Epoch: 227 [168192/225000 (75%)] Loss: 6401.949219\n",
      "Train Epoch: 227 [172288/225000 (77%)] Loss: 6352.812500\n",
      "Train Epoch: 227 [176384/225000 (78%)] Loss: 6311.009766\n",
      "Train Epoch: 227 [180480/225000 (80%)] Loss: 6363.808594\n",
      "Train Epoch: 227 [184576/225000 (82%)] Loss: 6234.642578\n",
      "Train Epoch: 227 [188672/225000 (84%)] Loss: 6424.189453\n",
      "Train Epoch: 227 [192768/225000 (86%)] Loss: 6212.017578\n",
      "Train Epoch: 227 [196864/225000 (87%)] Loss: 6310.994141\n",
      "Train Epoch: 227 [200960/225000 (89%)] Loss: 6331.857422\n",
      "Train Epoch: 227 [205056/225000 (91%)] Loss: 6315.410156\n",
      "Train Epoch: 227 [209152/225000 (93%)] Loss: 6351.152344\n",
      "Train Epoch: 227 [213248/225000 (95%)] Loss: 6283.587891\n",
      "Train Epoch: 227 [217344/225000 (97%)] Loss: 6336.392578\n",
      "Train Epoch: 227 [221440/225000 (98%)] Loss: 6201.894531\n",
      "    epoch          : 227\n",
      "    loss           : 6435.7605899815135\n",
      "    val_loss       : 6419.223352840969\n",
      "Train Epoch: 228 [256/225000 (0%)] Loss: 6393.908203\n",
      "Train Epoch: 228 [4352/225000 (2%)] Loss: 6325.550781\n",
      "Train Epoch: 228 [8448/225000 (4%)] Loss: 6252.365234\n",
      "Train Epoch: 228 [12544/225000 (6%)] Loss: 6354.972656\n",
      "Train Epoch: 228 [16640/225000 (7%)] Loss: 6313.630859\n",
      "Train Epoch: 228 [20736/225000 (9%)] Loss: 6359.185547\n",
      "Train Epoch: 228 [24832/225000 (11%)] Loss: 6315.015625\n",
      "Train Epoch: 228 [28928/225000 (13%)] Loss: 6319.753906\n",
      "Train Epoch: 228 [33024/225000 (15%)] Loss: 6245.408203\n",
      "Train Epoch: 228 [37120/225000 (16%)] Loss: 6540.763672\n",
      "Train Epoch: 228 [41216/225000 (18%)] Loss: 6267.673828\n",
      "Train Epoch: 228 [45312/225000 (20%)] Loss: 6230.117188\n",
      "Train Epoch: 228 [49408/225000 (22%)] Loss: 6306.460938\n",
      "Train Epoch: 228 [53504/225000 (24%)] Loss: 6273.156250\n",
      "Train Epoch: 228 [57600/225000 (26%)] Loss: 6339.511719\n",
      "Train Epoch: 228 [61696/225000 (27%)] Loss: 6337.035156\n",
      "Train Epoch: 228 [65792/225000 (29%)] Loss: 6228.326172\n",
      "Train Epoch: 228 [69888/225000 (31%)] Loss: 6337.218750\n",
      "Train Epoch: 228 [73984/225000 (33%)] Loss: 6355.068359\n",
      "Train Epoch: 228 [78080/225000 (35%)] Loss: 6210.886719\n",
      "Train Epoch: 228 [82176/225000 (37%)] Loss: 6328.175781\n",
      "Train Epoch: 228 [86272/225000 (38%)] Loss: 6330.412109\n",
      "Train Epoch: 228 [90368/225000 (40%)] Loss: 6333.021484\n",
      "Train Epoch: 228 [94464/225000 (42%)] Loss: 6317.603516\n",
      "Train Epoch: 228 [98560/225000 (44%)] Loss: 6334.460938\n",
      "Train Epoch: 228 [102656/225000 (46%)] Loss: 6284.832031\n",
      "Train Epoch: 228 [106752/225000 (47%)] Loss: 6486.281250\n",
      "Train Epoch: 228 [110848/225000 (49%)] Loss: 6450.625000\n",
      "Train Epoch: 228 [114944/225000 (51%)] Loss: 6362.312500\n",
      "Train Epoch: 228 [119040/225000 (53%)] Loss: 6253.791016\n",
      "Train Epoch: 228 [123136/225000 (55%)] Loss: 6230.884766\n",
      "Train Epoch: 228 [127232/225000 (57%)] Loss: 6203.765625\n",
      "Train Epoch: 228 [131328/225000 (58%)] Loss: 6347.921875\n",
      "Train Epoch: 228 [135424/225000 (60%)] Loss: 6336.757812\n",
      "Train Epoch: 228 [139520/225000 (62%)] Loss: 6145.333984\n",
      "Train Epoch: 228 [143616/225000 (64%)] Loss: 6456.205078\n",
      "Train Epoch: 228 [147712/225000 (66%)] Loss: 6241.648438\n",
      "Train Epoch: 228 [151808/225000 (67%)] Loss: 6351.478516\n",
      "Train Epoch: 228 [155904/225000 (69%)] Loss: 6480.902344\n",
      "Train Epoch: 228 [160000/225000 (71%)] Loss: 6355.552734\n",
      "Train Epoch: 228 [164096/225000 (73%)] Loss: 6328.863281\n",
      "Train Epoch: 228 [168192/225000 (75%)] Loss: 6243.843750\n",
      "Train Epoch: 228 [172288/225000 (77%)] Loss: 6267.007812\n",
      "Train Epoch: 228 [176384/225000 (78%)] Loss: 6327.958984\n",
      "Train Epoch: 228 [180480/225000 (80%)] Loss: 6295.800781\n",
      "Train Epoch: 228 [184576/225000 (82%)] Loss: 6328.654297\n",
      "Train Epoch: 228 [188672/225000 (84%)] Loss: 6225.757812\n",
      "Train Epoch: 228 [192768/225000 (86%)] Loss: 6465.222656\n",
      "Train Epoch: 228 [196864/225000 (87%)] Loss: 6298.673828\n",
      "Train Epoch: 228 [200960/225000 (89%)] Loss: 6342.070312\n",
      "Train Epoch: 228 [205056/225000 (91%)] Loss: 6337.708984\n",
      "Train Epoch: 228 [209152/225000 (93%)] Loss: 6424.650391\n",
      "Train Epoch: 228 [213248/225000 (95%)] Loss: 6403.298828\n",
      "Train Epoch: 228 [217344/225000 (97%)] Loss: 6338.677734\n",
      "Train Epoch: 228 [221440/225000 (98%)] Loss: 6320.560547\n",
      "    epoch          : 228\n",
      "    loss           : 6351.8785018486915\n",
      "    val_loss       : 6745.497678253116\n",
      "Train Epoch: 229 [256/225000 (0%)] Loss: 6477.250000\n",
      "Train Epoch: 229 [4352/225000 (2%)] Loss: 6298.046875\n",
      "Train Epoch: 229 [8448/225000 (4%)] Loss: 6232.205078\n",
      "Train Epoch: 229 [12544/225000 (6%)] Loss: 6361.548828\n",
      "Train Epoch: 229 [16640/225000 (7%)] Loss: 6277.587891\n",
      "Train Epoch: 229 [20736/225000 (9%)] Loss: 6313.746094\n",
      "Train Epoch: 229 [24832/225000 (11%)] Loss: 6351.869141\n",
      "Train Epoch: 229 [28928/225000 (13%)] Loss: 6275.962891\n",
      "Train Epoch: 229 [33024/225000 (15%)] Loss: 6295.162109\n",
      "Train Epoch: 229 [37120/225000 (16%)] Loss: 6377.605469\n",
      "Train Epoch: 229 [41216/225000 (18%)] Loss: 6401.041016\n",
      "Train Epoch: 229 [45312/225000 (20%)] Loss: 6341.880859\n",
      "Train Epoch: 229 [49408/225000 (22%)] Loss: 6353.740234\n",
      "Train Epoch: 229 [53504/225000 (24%)] Loss: 6403.884766\n",
      "Train Epoch: 229 [57600/225000 (26%)] Loss: 6320.300781\n",
      "Train Epoch: 229 [61696/225000 (27%)] Loss: 6304.117188\n",
      "Train Epoch: 229 [65792/225000 (29%)] Loss: 6512.337891\n",
      "Train Epoch: 229 [69888/225000 (31%)] Loss: 6368.587891\n",
      "Train Epoch: 229 [73984/225000 (33%)] Loss: 6396.255859\n",
      "Train Epoch: 229 [78080/225000 (35%)] Loss: 6319.388672\n",
      "Train Epoch: 229 [82176/225000 (37%)] Loss: 6390.240234\n",
      "Train Epoch: 229 [86272/225000 (38%)] Loss: 6308.923828\n",
      "Train Epoch: 229 [90368/225000 (40%)] Loss: 6276.146484\n",
      "Train Epoch: 229 [94464/225000 (42%)] Loss: 6227.876953\n",
      "Train Epoch: 229 [98560/225000 (44%)] Loss: 6279.345703\n",
      "Train Epoch: 229 [102656/225000 (46%)] Loss: 6305.763672\n",
      "Train Epoch: 229 [106752/225000 (47%)] Loss: 6374.882812\n",
      "Train Epoch: 229 [110848/225000 (49%)] Loss: 6230.513672\n",
      "Train Epoch: 229 [114944/225000 (51%)] Loss: 6351.703125\n",
      "Train Epoch: 229 [119040/225000 (53%)] Loss: 6402.804688\n",
      "Train Epoch: 229 [123136/225000 (55%)] Loss: 6382.074219\n",
      "Train Epoch: 229 [127232/225000 (57%)] Loss: 12578.568359\n",
      "Train Epoch: 229 [131328/225000 (58%)] Loss: 6345.656250\n",
      "Train Epoch: 229 [135424/225000 (60%)] Loss: 6371.554688\n",
      "Train Epoch: 229 [139520/225000 (62%)] Loss: 6312.480469\n",
      "Train Epoch: 229 [143616/225000 (64%)] Loss: 6415.705078\n",
      "Train Epoch: 229 [147712/225000 (66%)] Loss: 6363.744141\n",
      "Train Epoch: 229 [151808/225000 (67%)] Loss: 6308.218750\n",
      "Train Epoch: 229 [155904/225000 (69%)] Loss: 6275.986328\n",
      "Train Epoch: 229 [160000/225000 (71%)] Loss: 6396.777344\n",
      "Train Epoch: 229 [164096/225000 (73%)] Loss: 6341.847656\n",
      "Train Epoch: 229 [168192/225000 (75%)] Loss: 6358.621094\n",
      "Train Epoch: 229 [172288/225000 (77%)] Loss: 6360.513672\n",
      "Train Epoch: 229 [176384/225000 (78%)] Loss: 6510.800781\n",
      "Train Epoch: 229 [180480/225000 (80%)] Loss: 6362.720703\n",
      "Train Epoch: 229 [184576/225000 (82%)] Loss: 6397.359375\n",
      "Train Epoch: 229 [188672/225000 (84%)] Loss: 6296.617188\n",
      "Train Epoch: 229 [192768/225000 (86%)] Loss: 6263.824219\n",
      "Train Epoch: 229 [196864/225000 (87%)] Loss: 6338.447266\n",
      "Train Epoch: 229 [200960/225000 (89%)] Loss: 6363.759766\n",
      "Train Epoch: 229 [205056/225000 (91%)] Loss: 6290.087891\n",
      "Train Epoch: 229 [209152/225000 (93%)] Loss: 6317.164062\n",
      "Train Epoch: 229 [213248/225000 (95%)] Loss: 6399.826172\n",
      "Train Epoch: 229 [217344/225000 (97%)] Loss: 6283.947266\n",
      "Train Epoch: 229 [221440/225000 (98%)] Loss: 6388.078125\n",
      "    epoch          : 229\n",
      "    loss           : 6403.7086710750855\n",
      "    val_loss       : 6419.889476770041\n",
      "Train Epoch: 230 [256/225000 (0%)] Loss: 6299.662109\n",
      "Train Epoch: 230 [4352/225000 (2%)] Loss: 6364.828125\n",
      "Train Epoch: 230 [8448/225000 (4%)] Loss: 6413.048828\n",
      "Train Epoch: 230 [12544/225000 (6%)] Loss: 6363.615234\n",
      "Train Epoch: 230 [16640/225000 (7%)] Loss: 6222.066406\n",
      "Train Epoch: 230 [20736/225000 (9%)] Loss: 6289.513672\n",
      "Train Epoch: 230 [24832/225000 (11%)] Loss: 6331.121094\n",
      "Train Epoch: 230 [28928/225000 (13%)] Loss: 6300.828125\n",
      "Train Epoch: 230 [33024/225000 (15%)] Loss: 6361.419922\n",
      "Train Epoch: 230 [37120/225000 (16%)] Loss: 6296.376953\n",
      "Train Epoch: 230 [41216/225000 (18%)] Loss: 6252.619141\n",
      "Train Epoch: 230 [45312/225000 (20%)] Loss: 6375.554688\n",
      "Train Epoch: 230 [49408/225000 (22%)] Loss: 6272.328125\n",
      "Train Epoch: 230 [53504/225000 (24%)] Loss: 6391.498047\n",
      "Train Epoch: 230 [57600/225000 (26%)] Loss: 6264.123047\n",
      "Train Epoch: 230 [61696/225000 (27%)] Loss: 6344.677734\n",
      "Train Epoch: 230 [65792/225000 (29%)] Loss: 6389.654297\n",
      "Train Epoch: 230 [69888/225000 (31%)] Loss: 6305.259766\n",
      "Train Epoch: 230 [73984/225000 (33%)] Loss: 6333.115234\n",
      "Train Epoch: 230 [78080/225000 (35%)] Loss: 6336.912109\n",
      "Train Epoch: 230 [82176/225000 (37%)] Loss: 6310.134766\n",
      "Train Epoch: 230 [86272/225000 (38%)] Loss: 6278.248047\n",
      "Train Epoch: 230 [90368/225000 (40%)] Loss: 6438.507812\n",
      "Train Epoch: 230 [94464/225000 (42%)] Loss: 6235.890625\n",
      "Train Epoch: 230 [98560/225000 (44%)] Loss: 6349.806641\n",
      "Train Epoch: 230 [102656/225000 (46%)] Loss: 6305.478516\n",
      "Train Epoch: 230 [106752/225000 (47%)] Loss: 6322.332031\n",
      "Train Epoch: 230 [110848/225000 (49%)] Loss: 6307.972656\n",
      "Train Epoch: 230 [114944/225000 (51%)] Loss: 6487.533203\n",
      "Train Epoch: 230 [119040/225000 (53%)] Loss: 6251.923828\n",
      "Train Epoch: 230 [123136/225000 (55%)] Loss: 6300.060547\n",
      "Train Epoch: 230 [127232/225000 (57%)] Loss: 6419.083984\n",
      "Train Epoch: 230 [131328/225000 (58%)] Loss: 6191.230469\n",
      "Train Epoch: 230 [135424/225000 (60%)] Loss: 6199.751953\n",
      "Train Epoch: 230 [139520/225000 (62%)] Loss: 6323.007812\n",
      "Train Epoch: 230 [143616/225000 (64%)] Loss: 6358.503906\n",
      "Train Epoch: 230 [147712/225000 (66%)] Loss: 6214.933594\n",
      "Train Epoch: 230 [151808/225000 (67%)] Loss: 6346.257812\n",
      "Train Epoch: 230 [155904/225000 (69%)] Loss: 6326.572266\n",
      "Train Epoch: 230 [160000/225000 (71%)] Loss: 6427.138672\n",
      "Train Epoch: 230 [164096/225000 (73%)] Loss: 6384.138672\n",
      "Train Epoch: 230 [168192/225000 (75%)] Loss: 6466.900391\n",
      "Train Epoch: 230 [172288/225000 (77%)] Loss: 6323.955078\n",
      "Train Epoch: 230 [176384/225000 (78%)] Loss: 6223.562500\n",
      "Train Epoch: 230 [180480/225000 (80%)] Loss: 6267.343750\n",
      "Train Epoch: 230 [184576/225000 (82%)] Loss: 6346.623047\n",
      "Train Epoch: 230 [188672/225000 (84%)] Loss: 6286.710938\n",
      "Train Epoch: 230 [192768/225000 (86%)] Loss: 6290.826172\n",
      "Train Epoch: 230 [196864/225000 (87%)] Loss: 6286.259766\n",
      "Train Epoch: 230 [200960/225000 (89%)] Loss: 6448.974609\n",
      "Train Epoch: 230 [205056/225000 (91%)] Loss: 6265.664062\n",
      "Train Epoch: 230 [209152/225000 (93%)] Loss: 6161.937500\n",
      "Train Epoch: 230 [213248/225000 (95%)] Loss: 6320.695312\n",
      "Train Epoch: 230 [217344/225000 (97%)] Loss: 6180.822266\n",
      "Train Epoch: 230 [221440/225000 (98%)] Loss: 6237.060547\n",
      "    epoch          : 230\n",
      "    loss           : 6371.23400392847\n",
      "    val_loss       : 6626.214740405278\n",
      "Train Epoch: 231 [256/225000 (0%)] Loss: 6385.582031\n",
      "Train Epoch: 231 [4352/225000 (2%)] Loss: 6263.779297\n",
      "Train Epoch: 231 [8448/225000 (4%)] Loss: 6390.988281\n",
      "Train Epoch: 231 [12544/225000 (6%)] Loss: 6326.994141\n",
      "Train Epoch: 231 [16640/225000 (7%)] Loss: 6258.146484\n",
      "Train Epoch: 231 [20736/225000 (9%)] Loss: 6394.513672\n",
      "Train Epoch: 231 [24832/225000 (11%)] Loss: 6369.134766\n",
      "Train Epoch: 231 [28928/225000 (13%)] Loss: 6328.953125\n",
      "Train Epoch: 231 [33024/225000 (15%)] Loss: 6262.535156\n",
      "Train Epoch: 231 [37120/225000 (16%)] Loss: 6399.470703\n",
      "Train Epoch: 231 [41216/225000 (18%)] Loss: 6545.121094\n",
      "Train Epoch: 231 [45312/225000 (20%)] Loss: 6385.464844\n",
      "Train Epoch: 231 [49408/225000 (22%)] Loss: 6388.263672\n",
      "Train Epoch: 231 [53504/225000 (24%)] Loss: 6264.175781\n",
      "Train Epoch: 231 [57600/225000 (26%)] Loss: 6315.019531\n",
      "Train Epoch: 231 [61696/225000 (27%)] Loss: 6404.273438\n",
      "Train Epoch: 231 [65792/225000 (29%)] Loss: 6353.056641\n",
      "Train Epoch: 231 [69888/225000 (31%)] Loss: 6342.835938\n",
      "Train Epoch: 231 [73984/225000 (33%)] Loss: 6373.429688\n",
      "Train Epoch: 231 [78080/225000 (35%)] Loss: 6328.492188\n",
      "Train Epoch: 231 [82176/225000 (37%)] Loss: 6280.210938\n",
      "Train Epoch: 231 [86272/225000 (38%)] Loss: 6366.476562\n",
      "Train Epoch: 231 [90368/225000 (40%)] Loss: 6343.386719\n",
      "Train Epoch: 231 [94464/225000 (42%)] Loss: 6339.878906\n",
      "Train Epoch: 231 [98560/225000 (44%)] Loss: 6315.314453\n",
      "Train Epoch: 231 [102656/225000 (46%)] Loss: 6243.275391\n",
      "Train Epoch: 231 [106752/225000 (47%)] Loss: 6254.621094\n",
      "Train Epoch: 231 [110848/225000 (49%)] Loss: 6369.160156\n",
      "Train Epoch: 231 [114944/225000 (51%)] Loss: 6272.017578\n",
      "Train Epoch: 231 [119040/225000 (53%)] Loss: 6340.234375\n",
      "Train Epoch: 231 [123136/225000 (55%)] Loss: 6331.902344\n",
      "Train Epoch: 231 [127232/225000 (57%)] Loss: 6405.248047\n",
      "Train Epoch: 231 [131328/225000 (58%)] Loss: 6394.876953\n",
      "Train Epoch: 231 [135424/225000 (60%)] Loss: 6372.718750\n",
      "Train Epoch: 231 [139520/225000 (62%)] Loss: 6273.273438\n",
      "Train Epoch: 231 [143616/225000 (64%)] Loss: 6298.164062\n",
      "Train Epoch: 231 [147712/225000 (66%)] Loss: 6333.128906\n",
      "Train Epoch: 231 [151808/225000 (67%)] Loss: 6282.943359\n",
      "Train Epoch: 231 [155904/225000 (69%)] Loss: 6321.425781\n",
      "Train Epoch: 231 [160000/225000 (71%)] Loss: 6343.371094\n",
      "Train Epoch: 231 [164096/225000 (73%)] Loss: 6216.900391\n",
      "Train Epoch: 231 [168192/225000 (75%)] Loss: 6505.832031\n",
      "Train Epoch: 231 [172288/225000 (77%)] Loss: 6381.001953\n",
      "Train Epoch: 231 [176384/225000 (78%)] Loss: 6371.941406\n",
      "Train Epoch: 231 [180480/225000 (80%)] Loss: 6311.029297\n",
      "Train Epoch: 231 [184576/225000 (82%)] Loss: 6319.929688\n",
      "Train Epoch: 231 [188672/225000 (84%)] Loss: 6237.199219\n",
      "Train Epoch: 231 [192768/225000 (86%)] Loss: 6335.921875\n",
      "Train Epoch: 231 [196864/225000 (87%)] Loss: 6479.970703\n",
      "Train Epoch: 231 [200960/225000 (89%)] Loss: 6341.662109\n",
      "Train Epoch: 231 [205056/225000 (91%)] Loss: 6317.216797\n",
      "Train Epoch: 231 [209152/225000 (93%)] Loss: 6377.392578\n",
      "Train Epoch: 231 [213248/225000 (95%)] Loss: 6414.146484\n",
      "Train Epoch: 231 [217344/225000 (97%)] Loss: 6396.037109\n",
      "Train Epoch: 231 [221440/225000 (98%)] Loss: 6296.582031\n",
      "    epoch          : 231\n",
      "    loss           : 6347.437206697952\n",
      "    val_loss       : 6419.686526697509\n",
      "Train Epoch: 232 [256/225000 (0%)] Loss: 6386.443359\n",
      "Train Epoch: 232 [4352/225000 (2%)] Loss: 6308.640625\n",
      "Train Epoch: 232 [8448/225000 (4%)] Loss: 6334.630859\n",
      "Train Epoch: 232 [12544/225000 (6%)] Loss: 6221.119141\n",
      "Train Epoch: 232 [16640/225000 (7%)] Loss: 6392.128906\n",
      "Train Epoch: 232 [20736/225000 (9%)] Loss: 6283.087891\n",
      "Train Epoch: 232 [24832/225000 (11%)] Loss: 6404.318359\n",
      "Train Epoch: 232 [28928/225000 (13%)] Loss: 6444.966797\n",
      "Train Epoch: 232 [33024/225000 (15%)] Loss: 6330.634766\n",
      "Train Epoch: 232 [37120/225000 (16%)] Loss: 6419.763672\n",
      "Train Epoch: 232 [41216/225000 (18%)] Loss: 6343.826172\n",
      "Train Epoch: 232 [45312/225000 (20%)] Loss: 6315.015625\n",
      "Train Epoch: 232 [49408/225000 (22%)] Loss: 6440.181641\n",
      "Train Epoch: 232 [53504/225000 (24%)] Loss: 6198.654297\n",
      "Train Epoch: 232 [57600/225000 (26%)] Loss: 6226.708984\n",
      "Train Epoch: 232 [61696/225000 (27%)] Loss: 6363.160156\n",
      "Train Epoch: 232 [65792/225000 (29%)] Loss: 6299.619141\n",
      "Train Epoch: 232 [69888/225000 (31%)] Loss: 6256.882812\n",
      "Train Epoch: 232 [73984/225000 (33%)] Loss: 6266.794922\n",
      "Train Epoch: 232 [78080/225000 (35%)] Loss: 6314.218750\n",
      "Train Epoch: 232 [82176/225000 (37%)] Loss: 6256.666016\n",
      "Train Epoch: 232 [86272/225000 (38%)] Loss: 6370.074219\n",
      "Train Epoch: 232 [90368/225000 (40%)] Loss: 6465.664062\n",
      "Train Epoch: 232 [94464/225000 (42%)] Loss: 6236.857422\n",
      "Train Epoch: 232 [98560/225000 (44%)] Loss: 6411.386719\n",
      "Train Epoch: 232 [102656/225000 (46%)] Loss: 6321.064453\n",
      "Train Epoch: 232 [106752/225000 (47%)] Loss: 6231.378906\n",
      "Train Epoch: 232 [110848/225000 (49%)] Loss: 6244.859375\n",
      "Train Epoch: 232 [114944/225000 (51%)] Loss: 6256.035156\n",
      "Train Epoch: 232 [119040/225000 (53%)] Loss: 6244.296875\n",
      "Train Epoch: 232 [123136/225000 (55%)] Loss: 6335.416016\n",
      "Train Epoch: 232 [127232/225000 (57%)] Loss: 6234.443359\n",
      "Train Epoch: 232 [131328/225000 (58%)] Loss: 6337.853516\n",
      "Train Epoch: 232 [135424/225000 (60%)] Loss: 6304.445312\n",
      "Train Epoch: 232 [139520/225000 (62%)] Loss: 6218.263672\n",
      "Train Epoch: 232 [143616/225000 (64%)] Loss: 6245.150391\n",
      "Train Epoch: 232 [147712/225000 (66%)] Loss: 6310.679688\n",
      "Train Epoch: 232 [151808/225000 (67%)] Loss: 6430.234375\n",
      "Train Epoch: 232 [155904/225000 (69%)] Loss: 6466.597656\n",
      "Train Epoch: 232 [160000/225000 (71%)] Loss: 6335.285156\n",
      "Train Epoch: 232 [164096/225000 (73%)] Loss: 6317.349609\n",
      "Train Epoch: 232 [168192/225000 (75%)] Loss: 6336.654297\n",
      "Train Epoch: 232 [172288/225000 (77%)] Loss: 8196.994141\n",
      "Train Epoch: 232 [176384/225000 (78%)] Loss: 6219.812500\n",
      "Train Epoch: 232 [180480/225000 (80%)] Loss: 6262.630859\n",
      "Train Epoch: 232 [184576/225000 (82%)] Loss: 6466.583984\n",
      "Train Epoch: 232 [188672/225000 (84%)] Loss: 6212.250000\n",
      "Train Epoch: 232 [192768/225000 (86%)] Loss: 6244.087891\n",
      "Train Epoch: 232 [196864/225000 (87%)] Loss: 6230.677734\n",
      "Train Epoch: 232 [200960/225000 (89%)] Loss: 6313.630859\n",
      "Train Epoch: 232 [205056/225000 (91%)] Loss: 6268.699219\n",
      "Train Epoch: 232 [209152/225000 (93%)] Loss: 6355.130859\n",
      "Train Epoch: 232 [213248/225000 (95%)] Loss: 6480.658203\n",
      "Train Epoch: 232 [217344/225000 (97%)] Loss: 6554.125000\n",
      "Train Epoch: 232 [221440/225000 (98%)] Loss: 6441.007812\n",
      "    epoch          : 232\n",
      "    loss           : 6392.974067210609\n",
      "    val_loss       : 6401.125610116185\n",
      "Train Epoch: 233 [256/225000 (0%)] Loss: 6255.623047\n",
      "Train Epoch: 233 [4352/225000 (2%)] Loss: 6332.759766\n",
      "Train Epoch: 233 [8448/225000 (4%)] Loss: 6430.097656\n",
      "Train Epoch: 233 [12544/225000 (6%)] Loss: 6365.888672\n",
      "Train Epoch: 233 [16640/225000 (7%)] Loss: 6378.800781\n",
      "Train Epoch: 233 [20736/225000 (9%)] Loss: 6326.027344\n",
      "Train Epoch: 233 [24832/225000 (11%)] Loss: 6342.261719\n",
      "Train Epoch: 233 [28928/225000 (13%)] Loss: 6322.380859\n",
      "Train Epoch: 233 [33024/225000 (15%)] Loss: 6343.167969\n",
      "Train Epoch: 233 [37120/225000 (16%)] Loss: 6486.652344\n",
      "Train Epoch: 233 [41216/225000 (18%)] Loss: 6307.583984\n",
      "Train Epoch: 233 [45312/225000 (20%)] Loss: 6248.576172\n",
      "Train Epoch: 233 [49408/225000 (22%)] Loss: 6317.439453\n",
      "Train Epoch: 233 [53504/225000 (24%)] Loss: 6367.412109\n",
      "Train Epoch: 233 [57600/225000 (26%)] Loss: 6308.589844\n",
      "Train Epoch: 233 [61696/225000 (27%)] Loss: 6297.927734\n",
      "Train Epoch: 233 [65792/225000 (29%)] Loss: 6316.939453\n",
      "Train Epoch: 233 [69888/225000 (31%)] Loss: 6428.751953\n",
      "Train Epoch: 233 [73984/225000 (33%)] Loss: 6327.513672\n",
      "Train Epoch: 233 [78080/225000 (35%)] Loss: 6271.798828\n",
      "Train Epoch: 233 [82176/225000 (37%)] Loss: 6298.871094\n",
      "Train Epoch: 233 [86272/225000 (38%)] Loss: 6318.664062\n",
      "Train Epoch: 233 [90368/225000 (40%)] Loss: 6271.451172\n",
      "Train Epoch: 233 [94464/225000 (42%)] Loss: 6377.433594\n",
      "Train Epoch: 233 [98560/225000 (44%)] Loss: 6326.095703\n",
      "Train Epoch: 233 [102656/225000 (46%)] Loss: 6227.197266\n",
      "Train Epoch: 233 [106752/225000 (47%)] Loss: 6349.402344\n",
      "Train Epoch: 233 [110848/225000 (49%)] Loss: 6368.267578\n",
      "Train Epoch: 233 [114944/225000 (51%)] Loss: 6285.605469\n",
      "Train Epoch: 233 [119040/225000 (53%)] Loss: 6549.699219\n",
      "Train Epoch: 233 [123136/225000 (55%)] Loss: 6309.427734\n",
      "Train Epoch: 233 [127232/225000 (57%)] Loss: 6314.417969\n",
      "Train Epoch: 233 [131328/225000 (58%)] Loss: 6322.289062\n",
      "Train Epoch: 233 [135424/225000 (60%)] Loss: 6288.490234\n",
      "Train Epoch: 233 [139520/225000 (62%)] Loss: 6372.636719\n",
      "Train Epoch: 233 [143616/225000 (64%)] Loss: 6312.707031\n",
      "Train Epoch: 233 [147712/225000 (66%)] Loss: 6282.017578\n",
      "Train Epoch: 233 [151808/225000 (67%)] Loss: 6217.960938\n",
      "Train Epoch: 233 [155904/225000 (69%)] Loss: 6401.779297\n",
      "Train Epoch: 233 [160000/225000 (71%)] Loss: 6441.423828\n",
      "Train Epoch: 233 [164096/225000 (73%)] Loss: 6282.666016\n",
      "Train Epoch: 233 [168192/225000 (75%)] Loss: 6140.736328\n",
      "Train Epoch: 233 [172288/225000 (77%)] Loss: 6376.818359\n",
      "Train Epoch: 233 [176384/225000 (78%)] Loss: 6356.791016\n",
      "Train Epoch: 233 [180480/225000 (80%)] Loss: 6205.441406\n",
      "Train Epoch: 233 [184576/225000 (82%)] Loss: 6308.400391\n",
      "Train Epoch: 233 [188672/225000 (84%)] Loss: 6471.048828\n",
      "Train Epoch: 233 [192768/225000 (86%)] Loss: 6345.658203\n",
      "Train Epoch: 233 [196864/225000 (87%)] Loss: 6399.642578\n",
      "Train Epoch: 233 [200960/225000 (89%)] Loss: 6265.054688\n",
      "Train Epoch: 233 [205056/225000 (91%)] Loss: 6269.105469\n",
      "Train Epoch: 233 [209152/225000 (93%)] Loss: 6413.728516\n",
      "Train Epoch: 233 [213248/225000 (95%)] Loss: 6229.283203\n",
      "Train Epoch: 233 [217344/225000 (97%)] Loss: 6462.277344\n",
      "Train Epoch: 233 [221440/225000 (98%)] Loss: 6339.199219\n",
      "    epoch          : 233\n",
      "    loss           : 6357.210930834044\n",
      "    val_loss       : 6463.380142297064\n",
      "Train Epoch: 234 [256/225000 (0%)] Loss: 6330.871094\n",
      "Train Epoch: 234 [4352/225000 (2%)] Loss: 6268.082031\n",
      "Train Epoch: 234 [8448/225000 (4%)] Loss: 6344.736328\n",
      "Train Epoch: 234 [12544/225000 (6%)] Loss: 6262.439453\n",
      "Train Epoch: 234 [16640/225000 (7%)] Loss: 6329.580078\n",
      "Train Epoch: 234 [20736/225000 (9%)] Loss: 6467.839844\n",
      "Train Epoch: 234 [24832/225000 (11%)] Loss: 6251.056641\n",
      "Train Epoch: 234 [28928/225000 (13%)] Loss: 6259.101562\n",
      "Train Epoch: 234 [33024/225000 (15%)] Loss: 6366.531250\n",
      "Train Epoch: 234 [37120/225000 (16%)] Loss: 6345.904297\n",
      "Train Epoch: 234 [41216/225000 (18%)] Loss: 6176.236328\n",
      "Train Epoch: 234 [45312/225000 (20%)] Loss: 6340.160156\n",
      "Train Epoch: 234 [49408/225000 (22%)] Loss: 6298.917969\n",
      "Train Epoch: 234 [53504/225000 (24%)] Loss: 6244.775391\n",
      "Train Epoch: 234 [57600/225000 (26%)] Loss: 6350.808594\n",
      "Train Epoch: 234 [61696/225000 (27%)] Loss: 26544.048828\n",
      "Train Epoch: 234 [65792/225000 (29%)] Loss: 6424.937500\n",
      "Train Epoch: 234 [69888/225000 (31%)] Loss: 6310.048828\n",
      "Train Epoch: 234 [73984/225000 (33%)] Loss: 6394.216797\n",
      "Train Epoch: 234 [78080/225000 (35%)] Loss: 6440.175781\n",
      "Train Epoch: 234 [82176/225000 (37%)] Loss: 6346.947266\n",
      "Train Epoch: 234 [86272/225000 (38%)] Loss: 6216.027344\n",
      "Train Epoch: 234 [90368/225000 (40%)] Loss: 6378.134766\n",
      "Train Epoch: 234 [94464/225000 (42%)] Loss: 6288.796875\n",
      "Train Epoch: 234 [98560/225000 (44%)] Loss: 6405.882812\n",
      "Train Epoch: 234 [102656/225000 (46%)] Loss: 6270.062500\n",
      "Train Epoch: 234 [106752/225000 (47%)] Loss: 6392.199219\n",
      "Train Epoch: 234 [110848/225000 (49%)] Loss: 6446.941406\n",
      "Train Epoch: 234 [114944/225000 (51%)] Loss: 6328.009766\n",
      "Train Epoch: 234 [119040/225000 (53%)] Loss: 6360.421875\n",
      "Train Epoch: 234 [123136/225000 (55%)] Loss: 6305.185547\n",
      "Train Epoch: 234 [127232/225000 (57%)] Loss: 6353.062500\n",
      "Train Epoch: 234 [131328/225000 (58%)] Loss: 6325.695312\n",
      "Train Epoch: 234 [135424/225000 (60%)] Loss: 6324.164062\n",
      "Train Epoch: 234 [139520/225000 (62%)] Loss: 6370.416016\n",
      "Train Epoch: 234 [143616/225000 (64%)] Loss: 6291.923828\n",
      "Train Epoch: 234 [147712/225000 (66%)] Loss: 6385.910156\n",
      "Train Epoch: 234 [151808/225000 (67%)] Loss: 6292.396484\n",
      "Train Epoch: 234 [155904/225000 (69%)] Loss: 8023.906250\n",
      "Train Epoch: 234 [160000/225000 (71%)] Loss: 6274.673828\n",
      "Train Epoch: 234 [164096/225000 (73%)] Loss: 6226.755859\n",
      "Train Epoch: 234 [168192/225000 (75%)] Loss: 6201.470703\n",
      "Train Epoch: 234 [172288/225000 (77%)] Loss: 6307.396484\n",
      "Train Epoch: 234 [176384/225000 (78%)] Loss: 6355.267578\n",
      "Train Epoch: 234 [180480/225000 (80%)] Loss: 6269.712891\n",
      "Train Epoch: 234 [184576/225000 (82%)] Loss: 6199.037109\n",
      "Train Epoch: 234 [188672/225000 (84%)] Loss: 6428.068359\n",
      "Train Epoch: 234 [192768/225000 (86%)] Loss: 6307.095703\n",
      "Train Epoch: 234 [196864/225000 (87%)] Loss: 6346.482422\n",
      "Train Epoch: 234 [200960/225000 (89%)] Loss: 6323.226562\n",
      "Train Epoch: 234 [205056/225000 (91%)] Loss: 6343.369141\n",
      "Train Epoch: 234 [209152/225000 (93%)] Loss: 6375.507812\n",
      "Train Epoch: 234 [213248/225000 (95%)] Loss: 6252.935547\n",
      "Train Epoch: 234 [217344/225000 (97%)] Loss: 6325.642578\n",
      "Train Epoch: 234 [221440/225000 (98%)] Loss: 6477.271484\n",
      "    epoch          : 234\n",
      "    loss           : 6356.979792155504\n",
      "    val_loss       : 6457.097866701836\n",
      "Train Epoch: 235 [256/225000 (0%)] Loss: 6482.898438\n",
      "Train Epoch: 235 [4352/225000 (2%)] Loss: 6357.880859\n",
      "Train Epoch: 235 [8448/225000 (4%)] Loss: 6343.822266\n",
      "Train Epoch: 235 [12544/225000 (6%)] Loss: 6427.083984\n",
      "Train Epoch: 235 [16640/225000 (7%)] Loss: 6357.275391\n",
      "Train Epoch: 235 [20736/225000 (9%)] Loss: 6343.875000\n",
      "Train Epoch: 235 [24832/225000 (11%)] Loss: 6399.750000\n",
      "Train Epoch: 235 [28928/225000 (13%)] Loss: 6351.835938\n",
      "Train Epoch: 235 [33024/225000 (15%)] Loss: 6280.054688\n",
      "Train Epoch: 235 [37120/225000 (16%)] Loss: 6314.833984\n",
      "Train Epoch: 235 [41216/225000 (18%)] Loss: 6325.013672\n",
      "Train Epoch: 235 [45312/225000 (20%)] Loss: 6249.359375\n",
      "Train Epoch: 235 [49408/225000 (22%)] Loss: 6374.148438\n",
      "Train Epoch: 235 [53504/225000 (24%)] Loss: 6275.292969\n",
      "Train Epoch: 235 [57600/225000 (26%)] Loss: 6589.953125\n",
      "Train Epoch: 235 [61696/225000 (27%)] Loss: 6453.199219\n",
      "Train Epoch: 235 [65792/225000 (29%)] Loss: 6528.150391\n",
      "Train Epoch: 235 [69888/225000 (31%)] Loss: 6290.068359\n",
      "Train Epoch: 235 [73984/225000 (33%)] Loss: 6415.916016\n",
      "Train Epoch: 235 [78080/225000 (35%)] Loss: 6326.183594\n",
      "Train Epoch: 235 [82176/225000 (37%)] Loss: 6345.771484\n",
      "Train Epoch: 235 [86272/225000 (38%)] Loss: 6375.705078\n",
      "Train Epoch: 235 [90368/225000 (40%)] Loss: 6283.283203\n",
      "Train Epoch: 235 [94464/225000 (42%)] Loss: 6474.130859\n",
      "Train Epoch: 235 [98560/225000 (44%)] Loss: 6348.640625\n",
      "Train Epoch: 235 [102656/225000 (46%)] Loss: 6333.179688\n",
      "Train Epoch: 235 [106752/225000 (47%)] Loss: 6395.412109\n",
      "Train Epoch: 235 [110848/225000 (49%)] Loss: 6321.957031\n",
      "Train Epoch: 235 [114944/225000 (51%)] Loss: 6379.302734\n",
      "Train Epoch: 235 [119040/225000 (53%)] Loss: 6441.240234\n",
      "Train Epoch: 235 [123136/225000 (55%)] Loss: 6325.105469\n",
      "Train Epoch: 235 [127232/225000 (57%)] Loss: 6307.148438\n",
      "Train Epoch: 235 [131328/225000 (58%)] Loss: 6379.183594\n",
      "Train Epoch: 235 [135424/225000 (60%)] Loss: 6270.070312\n",
      "Train Epoch: 235 [139520/225000 (62%)] Loss: 6364.328125\n",
      "Train Epoch: 235 [143616/225000 (64%)] Loss: 6356.607422\n",
      "Train Epoch: 235 [147712/225000 (66%)] Loss: 6206.164062\n",
      "Train Epoch: 235 [151808/225000 (67%)] Loss: 6361.548828\n",
      "Train Epoch: 235 [155904/225000 (69%)] Loss: 6368.339844\n",
      "Train Epoch: 235 [160000/225000 (71%)] Loss: 6217.937500\n",
      "Train Epoch: 235 [164096/225000 (73%)] Loss: 6288.521484\n",
      "Train Epoch: 235 [168192/225000 (75%)] Loss: 6251.533203\n",
      "Train Epoch: 235 [172288/225000 (77%)] Loss: 6533.380859\n",
      "Train Epoch: 235 [176384/225000 (78%)] Loss: 6211.154297\n",
      "Train Epoch: 235 [180480/225000 (80%)] Loss: 6350.089844\n",
      "Train Epoch: 235 [184576/225000 (82%)] Loss: 6243.511719\n",
      "Train Epoch: 235 [188672/225000 (84%)] Loss: 6378.087891\n",
      "Train Epoch: 235 [192768/225000 (86%)] Loss: 6271.869141\n",
      "Train Epoch: 235 [196864/225000 (87%)] Loss: 6338.667969\n",
      "Train Epoch: 235 [200960/225000 (89%)] Loss: 6327.146484\n",
      "Train Epoch: 235 [205056/225000 (91%)] Loss: 6373.566406\n",
      "Train Epoch: 235 [209152/225000 (93%)] Loss: 6252.796875\n",
      "Train Epoch: 235 [213248/225000 (95%)] Loss: 6355.677734\n",
      "Train Epoch: 235 [217344/225000 (97%)] Loss: 6223.167969\n",
      "Train Epoch: 235 [221440/225000 (98%)] Loss: 6307.166016\n",
      "    epoch          : 235\n",
      "    loss           : 6386.0524188531\n",
      "    val_loss       : 6542.570076769712\n",
      "Train Epoch: 236 [256/225000 (0%)] Loss: 6354.134766\n",
      "Train Epoch: 236 [4352/225000 (2%)] Loss: 6170.000000\n",
      "Train Epoch: 236 [8448/225000 (4%)] Loss: 6425.578125\n",
      "Train Epoch: 236 [12544/225000 (6%)] Loss: 6360.833984\n",
      "Train Epoch: 236 [16640/225000 (7%)] Loss: 6339.574219\n",
      "Train Epoch: 236 [20736/225000 (9%)] Loss: 6266.703125\n",
      "Train Epoch: 236 [24832/225000 (11%)] Loss: 6296.781250\n",
      "Train Epoch: 236 [28928/225000 (13%)] Loss: 6404.486328\n",
      "Train Epoch: 236 [33024/225000 (15%)] Loss: 6328.074219\n",
      "Train Epoch: 236 [37120/225000 (16%)] Loss: 6430.621094\n",
      "Train Epoch: 236 [41216/225000 (18%)] Loss: 6332.111328\n",
      "Train Epoch: 236 [45312/225000 (20%)] Loss: 6230.382812\n",
      "Train Epoch: 236 [49408/225000 (22%)] Loss: 6428.101562\n",
      "Train Epoch: 236 [53504/225000 (24%)] Loss: 6349.433594\n",
      "Train Epoch: 236 [57600/225000 (26%)] Loss: 6540.193359\n",
      "Train Epoch: 236 [61696/225000 (27%)] Loss: 6308.412109\n",
      "Train Epoch: 236 [65792/225000 (29%)] Loss: 6294.884766\n",
      "Train Epoch: 236 [69888/225000 (31%)] Loss: 6236.757812\n",
      "Train Epoch: 236 [73984/225000 (33%)] Loss: 17000.453125\n",
      "Train Epoch: 236 [78080/225000 (35%)] Loss: 6359.603516\n",
      "Train Epoch: 236 [82176/225000 (37%)] Loss: 6358.804688\n",
      "Train Epoch: 236 [86272/225000 (38%)] Loss: 6376.599609\n",
      "Train Epoch: 236 [90368/225000 (40%)] Loss: 6255.378906\n",
      "Train Epoch: 236 [94464/225000 (42%)] Loss: 6321.449219\n",
      "Train Epoch: 236 [98560/225000 (44%)] Loss: 26722.423828\n",
      "Train Epoch: 236 [102656/225000 (46%)] Loss: 6388.173828\n",
      "Train Epoch: 236 [106752/225000 (47%)] Loss: 6314.546875\n",
      "Train Epoch: 236 [110848/225000 (49%)] Loss: 6317.148438\n",
      "Train Epoch: 236 [114944/225000 (51%)] Loss: 6162.339844\n",
      "Train Epoch: 236 [119040/225000 (53%)] Loss: 6294.867188\n",
      "Train Epoch: 236 [123136/225000 (55%)] Loss: 6444.162109\n",
      "Train Epoch: 236 [127232/225000 (57%)] Loss: 6329.238281\n",
      "Train Epoch: 236 [131328/225000 (58%)] Loss: 6276.808594\n",
      "Train Epoch: 236 [135424/225000 (60%)] Loss: 6383.333984\n",
      "Train Epoch: 236 [139520/225000 (62%)] Loss: 6318.859375\n",
      "Train Epoch: 236 [143616/225000 (64%)] Loss: 6301.228516\n",
      "Train Epoch: 236 [147712/225000 (66%)] Loss: 6369.599609\n",
      "Train Epoch: 236 [151808/225000 (67%)] Loss: 6328.482422\n",
      "Train Epoch: 236 [155904/225000 (69%)] Loss: 6324.160156\n",
      "Train Epoch: 236 [160000/225000 (71%)] Loss: 6218.632812\n",
      "Train Epoch: 236 [164096/225000 (73%)] Loss: 6340.941406\n",
      "Train Epoch: 236 [168192/225000 (75%)] Loss: 6350.662109\n",
      "Train Epoch: 236 [172288/225000 (77%)] Loss: 6248.664062\n",
      "Train Epoch: 236 [176384/225000 (78%)] Loss: 6383.572266\n",
      "Train Epoch: 236 [180480/225000 (80%)] Loss: 6458.851562\n",
      "Train Epoch: 236 [184576/225000 (82%)] Loss: 6275.808594\n",
      "Train Epoch: 236 [188672/225000 (84%)] Loss: 6484.757812\n",
      "Train Epoch: 236 [192768/225000 (86%)] Loss: 6350.433594\n",
      "Train Epoch: 236 [196864/225000 (87%)] Loss: 6377.322266\n",
      "Train Epoch: 236 [200960/225000 (89%)] Loss: 6229.986328\n",
      "Train Epoch: 236 [205056/225000 (91%)] Loss: 6264.300781\n",
      "Train Epoch: 236 [209152/225000 (93%)] Loss: 6306.396484\n",
      "Train Epoch: 236 [213248/225000 (95%)] Loss: 6336.972656\n",
      "Train Epoch: 236 [217344/225000 (97%)] Loss: 6262.753906\n",
      "Train Epoch: 236 [221440/225000 (98%)] Loss: 6291.621094\n",
      "    epoch          : 236\n",
      "    loss           : 6433.326595163183\n",
      "    val_loss       : 6525.449177543727\n",
      "Train Epoch: 237 [256/225000 (0%)] Loss: 6415.238281\n",
      "Train Epoch: 237 [4352/225000 (2%)] Loss: 6333.062500\n",
      "Train Epoch: 237 [8448/225000 (4%)] Loss: 6374.748047\n",
      "Train Epoch: 237 [12544/225000 (6%)] Loss: 6286.216797\n",
      "Train Epoch: 237 [16640/225000 (7%)] Loss: 6347.824219\n",
      "Train Epoch: 237 [20736/225000 (9%)] Loss: 6368.849609\n",
      "Train Epoch: 237 [24832/225000 (11%)] Loss: 6382.908203\n",
      "Train Epoch: 237 [28928/225000 (13%)] Loss: 6300.664062\n",
      "Train Epoch: 237 [33024/225000 (15%)] Loss: 6361.968750\n",
      "Train Epoch: 237 [37120/225000 (16%)] Loss: 6275.648438\n",
      "Train Epoch: 237 [41216/225000 (18%)] Loss: 6358.388672\n",
      "Train Epoch: 237 [45312/225000 (20%)] Loss: 6423.886719\n",
      "Train Epoch: 237 [49408/225000 (22%)] Loss: 6244.308594\n",
      "Train Epoch: 237 [53504/225000 (24%)] Loss: 6322.181641\n",
      "Train Epoch: 237 [57600/225000 (26%)] Loss: 6342.035156\n",
      "Train Epoch: 237 [61696/225000 (27%)] Loss: 6396.570312\n",
      "Train Epoch: 237 [65792/225000 (29%)] Loss: 6275.970703\n",
      "Train Epoch: 237 [69888/225000 (31%)] Loss: 6314.689453\n",
      "Train Epoch: 237 [73984/225000 (33%)] Loss: 6439.017578\n",
      "Train Epoch: 237 [78080/225000 (35%)] Loss: 6278.988281\n",
      "Train Epoch: 237 [82176/225000 (37%)] Loss: 6256.761719\n",
      "Train Epoch: 237 [86272/225000 (38%)] Loss: 6373.062500\n",
      "Train Epoch: 237 [90368/225000 (40%)] Loss: 6243.615234\n",
      "Train Epoch: 237 [94464/225000 (42%)] Loss: 6381.078125\n",
      "Train Epoch: 237 [98560/225000 (44%)] Loss: 6350.542969\n",
      "Train Epoch: 237 [102656/225000 (46%)] Loss: 6336.748047\n",
      "Train Epoch: 237 [106752/225000 (47%)] Loss: 6308.710938\n",
      "Train Epoch: 237 [110848/225000 (49%)] Loss: 6411.701172\n",
      "Train Epoch: 237 [114944/225000 (51%)] Loss: 6300.835938\n",
      "Train Epoch: 237 [119040/225000 (53%)] Loss: 6331.064453\n",
      "Train Epoch: 237 [123136/225000 (55%)] Loss: 6397.814453\n",
      "Train Epoch: 237 [127232/225000 (57%)] Loss: 6247.263672\n",
      "Train Epoch: 237 [131328/225000 (58%)] Loss: 6276.226562\n",
      "Train Epoch: 237 [135424/225000 (60%)] Loss: 6321.142578\n",
      "Train Epoch: 237 [139520/225000 (62%)] Loss: 6220.035156\n",
      "Train Epoch: 237 [143616/225000 (64%)] Loss: 6358.593750\n",
      "Train Epoch: 237 [147712/225000 (66%)] Loss: 6386.210938\n",
      "Train Epoch: 237 [151808/225000 (67%)] Loss: 6243.949219\n",
      "Train Epoch: 237 [155904/225000 (69%)] Loss: 6338.417969\n",
      "Train Epoch: 237 [160000/225000 (71%)] Loss: 6232.976562\n",
      "Train Epoch: 237 [164096/225000 (73%)] Loss: 6260.085938\n",
      "Train Epoch: 237 [168192/225000 (75%)] Loss: 6248.222656\n",
      "Train Epoch: 237 [172288/225000 (77%)] Loss: 6458.470703\n",
      "Train Epoch: 237 [176384/225000 (78%)] Loss: 6325.232422\n",
      "Train Epoch: 237 [180480/225000 (80%)] Loss: 6368.427734\n",
      "Train Epoch: 237 [184576/225000 (82%)] Loss: 6234.001953\n",
      "Train Epoch: 237 [188672/225000 (84%)] Loss: 6258.695312\n",
      "Train Epoch: 237 [192768/225000 (86%)] Loss: 6324.740234\n",
      "Train Epoch: 237 [196864/225000 (87%)] Loss: 6264.005859\n",
      "Train Epoch: 237 [200960/225000 (89%)] Loss: 6421.300781\n",
      "Train Epoch: 237 [205056/225000 (91%)] Loss: 6282.460938\n",
      "Train Epoch: 237 [209152/225000 (93%)] Loss: 6392.943359\n",
      "Train Epoch: 237 [213248/225000 (95%)] Loss: 6306.027344\n",
      "Train Epoch: 237 [217344/225000 (97%)] Loss: 6288.345703\n",
      "Train Epoch: 237 [221440/225000 (98%)] Loss: 6298.685547\n",
      "    epoch          : 237\n",
      "    loss           : 6366.01675487948\n",
      "    val_loss       : 6401.246252682744\n",
      "Train Epoch: 238 [256/225000 (0%)] Loss: 6392.476562\n",
      "Train Epoch: 238 [4352/225000 (2%)] Loss: 6467.736328\n",
      "Train Epoch: 238 [8448/225000 (4%)] Loss: 6263.656250\n",
      "Train Epoch: 238 [12544/225000 (6%)] Loss: 6384.183594\n",
      "Train Epoch: 238 [16640/225000 (7%)] Loss: 6242.462891\n",
      "Train Epoch: 238 [20736/225000 (9%)] Loss: 6261.064453\n",
      "Train Epoch: 238 [24832/225000 (11%)] Loss: 6162.068359\n",
      "Train Epoch: 238 [28928/225000 (13%)] Loss: 6388.074219\n",
      "Train Epoch: 238 [33024/225000 (15%)] Loss: 6252.542969\n",
      "Train Epoch: 238 [37120/225000 (16%)] Loss: 6368.705078\n",
      "Train Epoch: 238 [41216/225000 (18%)] Loss: 6270.250000\n",
      "Train Epoch: 238 [45312/225000 (20%)] Loss: 6254.849609\n",
      "Train Epoch: 238 [49408/225000 (22%)] Loss: 6355.507812\n",
      "Train Epoch: 238 [53504/225000 (24%)] Loss: 6316.996094\n",
      "Train Epoch: 238 [57600/225000 (26%)] Loss: 6331.164062\n",
      "Train Epoch: 238 [61696/225000 (27%)] Loss: 8080.250000\n",
      "Train Epoch: 238 [65792/225000 (29%)] Loss: 6199.304688\n",
      "Train Epoch: 238 [69888/225000 (31%)] Loss: 6347.498047\n",
      "Train Epoch: 238 [73984/225000 (33%)] Loss: 6459.404297\n",
      "Train Epoch: 238 [78080/225000 (35%)] Loss: 6249.601562\n",
      "Train Epoch: 238 [82176/225000 (37%)] Loss: 6347.910156\n",
      "Train Epoch: 238 [86272/225000 (38%)] Loss: 6322.505859\n",
      "Train Epoch: 238 [90368/225000 (40%)] Loss: 6169.726562\n",
      "Train Epoch: 238 [94464/225000 (42%)] Loss: 6237.343750\n",
      "Train Epoch: 238 [98560/225000 (44%)] Loss: 6261.466797\n",
      "Train Epoch: 238 [102656/225000 (46%)] Loss: 6345.033203\n",
      "Train Epoch: 238 [106752/225000 (47%)] Loss: 6291.630859\n",
      "Train Epoch: 238 [110848/225000 (49%)] Loss: 6285.150391\n",
      "Train Epoch: 238 [114944/225000 (51%)] Loss: 6439.439453\n",
      "Train Epoch: 238 [119040/225000 (53%)] Loss: 6506.644531\n",
      "Train Epoch: 238 [123136/225000 (55%)] Loss: 6306.003906\n",
      "Train Epoch: 238 [127232/225000 (57%)] Loss: 6308.083984\n",
      "Train Epoch: 238 [131328/225000 (58%)] Loss: 6451.982422\n",
      "Train Epoch: 238 [135424/225000 (60%)] Loss: 6392.316406\n",
      "Train Epoch: 238 [139520/225000 (62%)] Loss: 6390.101562\n",
      "Train Epoch: 238 [143616/225000 (64%)] Loss: 6350.921875\n",
      "Train Epoch: 238 [147712/225000 (66%)] Loss: 6364.828125\n",
      "Train Epoch: 238 [151808/225000 (67%)] Loss: 6389.949219\n",
      "Train Epoch: 238 [155904/225000 (69%)] Loss: 6246.371094\n",
      "Train Epoch: 238 [160000/225000 (71%)] Loss: 6370.515625\n",
      "Train Epoch: 238 [164096/225000 (73%)] Loss: 6286.662109\n",
      "Train Epoch: 238 [168192/225000 (75%)] Loss: 6309.652344\n",
      "Train Epoch: 238 [172288/225000 (77%)] Loss: 6290.955078\n",
      "Train Epoch: 238 [176384/225000 (78%)] Loss: 6361.404297\n",
      "Train Epoch: 238 [180480/225000 (80%)] Loss: 6478.287109\n",
      "Train Epoch: 238 [184576/225000 (82%)] Loss: 6282.384766\n",
      "Train Epoch: 238 [188672/225000 (84%)] Loss: 6292.742188\n",
      "Train Epoch: 238 [192768/225000 (86%)] Loss: 6436.902344\n",
      "Train Epoch: 238 [196864/225000 (87%)] Loss: 6255.976562\n",
      "Train Epoch: 238 [200960/225000 (89%)] Loss: 6258.164062\n",
      "Train Epoch: 238 [205056/225000 (91%)] Loss: 6294.894531\n",
      "Train Epoch: 238 [209152/225000 (93%)] Loss: 6375.232422\n",
      "Train Epoch: 238 [213248/225000 (95%)] Loss: 6337.882812\n",
      "Train Epoch: 238 [217344/225000 (97%)] Loss: 6244.009766\n",
      "Train Epoch: 238 [221440/225000 (98%)] Loss: 6272.462891\n",
      "    epoch          : 238\n",
      "    loss           : 6396.1404727940135\n",
      "    val_loss       : 6419.4146539459425\n",
      "Train Epoch: 239 [256/225000 (0%)] Loss: 6296.417969\n",
      "Train Epoch: 239 [4352/225000 (2%)] Loss: 6302.589844\n",
      "Train Epoch: 239 [8448/225000 (4%)] Loss: 6293.962891\n",
      "Train Epoch: 239 [12544/225000 (6%)] Loss: 6364.304688\n",
      "Train Epoch: 239 [16640/225000 (7%)] Loss: 6316.445312\n",
      "Train Epoch: 239 [20736/225000 (9%)] Loss: 6169.189453\n",
      "Train Epoch: 239 [24832/225000 (11%)] Loss: 6328.396484\n",
      "Train Epoch: 239 [28928/225000 (13%)] Loss: 6272.275391\n",
      "Train Epoch: 239 [33024/225000 (15%)] Loss: 6145.802734\n",
      "Train Epoch: 239 [37120/225000 (16%)] Loss: 6438.179688\n",
      "Train Epoch: 239 [41216/225000 (18%)] Loss: 6360.857422\n",
      "Train Epoch: 239 [45312/225000 (20%)] Loss: 6314.800781\n",
      "Train Epoch: 239 [49408/225000 (22%)] Loss: 6386.039062\n",
      "Train Epoch: 239 [53504/225000 (24%)] Loss: 6275.720703\n",
      "Train Epoch: 239 [57600/225000 (26%)] Loss: 6384.052734\n",
      "Train Epoch: 239 [61696/225000 (27%)] Loss: 6288.974609\n",
      "Train Epoch: 239 [65792/225000 (29%)] Loss: 6438.224609\n",
      "Train Epoch: 239 [69888/225000 (31%)] Loss: 6299.787109\n",
      "Train Epoch: 239 [73984/225000 (33%)] Loss: 6348.121094\n",
      "Train Epoch: 239 [78080/225000 (35%)] Loss: 6440.333984\n",
      "Train Epoch: 239 [82176/225000 (37%)] Loss: 6413.255859\n",
      "Train Epoch: 239 [86272/225000 (38%)] Loss: 6453.244141\n",
      "Train Epoch: 239 [90368/225000 (40%)] Loss: 6309.720703\n",
      "Train Epoch: 239 [94464/225000 (42%)] Loss: 6327.398438\n",
      "Train Epoch: 239 [98560/225000 (44%)] Loss: 6463.341797\n",
      "Train Epoch: 239 [102656/225000 (46%)] Loss: 6258.625000\n",
      "Train Epoch: 239 [106752/225000 (47%)] Loss: 6246.720703\n",
      "Train Epoch: 239 [110848/225000 (49%)] Loss: 6354.437500\n",
      "Train Epoch: 239 [114944/225000 (51%)] Loss: 6245.003906\n",
      "Train Epoch: 239 [119040/225000 (53%)] Loss: 6308.326172\n",
      "Train Epoch: 239 [123136/225000 (55%)] Loss: 6362.158203\n",
      "Train Epoch: 239 [127232/225000 (57%)] Loss: 6427.460938\n",
      "Train Epoch: 239 [131328/225000 (58%)] Loss: 6233.722656\n",
      "Train Epoch: 239 [135424/225000 (60%)] Loss: 6258.333984\n",
      "Train Epoch: 239 [139520/225000 (62%)] Loss: 6204.828125\n",
      "Train Epoch: 239 [143616/225000 (64%)] Loss: 6340.785156\n",
      "Train Epoch: 239 [147712/225000 (66%)] Loss: 6434.007812\n",
      "Train Epoch: 239 [151808/225000 (67%)] Loss: 6276.671875\n",
      "Train Epoch: 239 [155904/225000 (69%)] Loss: 6259.175781\n",
      "Train Epoch: 239 [160000/225000 (71%)] Loss: 6201.962891\n",
      "Train Epoch: 239 [164096/225000 (73%)] Loss: 6278.173828\n",
      "Train Epoch: 239 [168192/225000 (75%)] Loss: 6549.392578\n",
      "Train Epoch: 239 [172288/225000 (77%)] Loss: 6317.828125\n",
      "Train Epoch: 239 [176384/225000 (78%)] Loss: 6334.998047\n",
      "Train Epoch: 239 [180480/225000 (80%)] Loss: 6185.082031\n",
      "Train Epoch: 239 [184576/225000 (82%)] Loss: 6257.787109\n",
      "Train Epoch: 239 [188672/225000 (84%)] Loss: 6418.123047\n",
      "Train Epoch: 239 [192768/225000 (86%)] Loss: 6379.728516\n",
      "Train Epoch: 239 [196864/225000 (87%)] Loss: 6259.464844\n",
      "Train Epoch: 239 [200960/225000 (89%)] Loss: 6393.888672\n",
      "Train Epoch: 239 [205056/225000 (91%)] Loss: 6443.728516\n",
      "Train Epoch: 239 [209152/225000 (93%)] Loss: 6429.103516\n",
      "Train Epoch: 239 [213248/225000 (95%)] Loss: 6407.736328\n",
      "Train Epoch: 239 [217344/225000 (97%)] Loss: 6389.875000\n",
      "Train Epoch: 239 [221440/225000 (98%)] Loss: 6258.652344\n",
      "    epoch          : 239\n",
      "    loss           : 6416.700325298635\n",
      "    val_loss       : 6497.924441854564\n",
      "Train Epoch: 240 [256/225000 (0%)] Loss: 6337.970703\n",
      "Train Epoch: 240 [4352/225000 (2%)] Loss: 6398.187500\n",
      "Train Epoch: 240 [8448/225000 (4%)] Loss: 6422.539062\n",
      "Train Epoch: 240 [12544/225000 (6%)] Loss: 6326.800781\n",
      "Train Epoch: 240 [16640/225000 (7%)] Loss: 6265.091797\n",
      "Train Epoch: 240 [20736/225000 (9%)] Loss: 6223.240234\n",
      "Train Epoch: 240 [24832/225000 (11%)] Loss: 6338.580078\n",
      "Train Epoch: 240 [28928/225000 (13%)] Loss: 6249.005859\n",
      "Train Epoch: 240 [33024/225000 (15%)] Loss: 6179.525391\n",
      "Train Epoch: 240 [37120/225000 (16%)] Loss: 6321.773438\n",
      "Train Epoch: 240 [41216/225000 (18%)] Loss: 6449.587891\n",
      "Train Epoch: 240 [45312/225000 (20%)] Loss: 6356.708984\n",
      "Train Epoch: 240 [49408/225000 (22%)] Loss: 6265.675781\n",
      "Train Epoch: 240 [53504/225000 (24%)] Loss: 6330.970703\n",
      "Train Epoch: 240 [57600/225000 (26%)] Loss: 6349.181641\n",
      "Train Epoch: 240 [61696/225000 (27%)] Loss: 6395.591797\n",
      "Train Epoch: 240 [65792/225000 (29%)] Loss: 6307.267578\n",
      "Train Epoch: 240 [69888/225000 (31%)] Loss: 6349.689453\n",
      "Train Epoch: 240 [73984/225000 (33%)] Loss: 6249.371094\n",
      "Train Epoch: 240 [78080/225000 (35%)] Loss: 6389.929688\n",
      "Train Epoch: 240 [82176/225000 (37%)] Loss: 6412.396484\n",
      "Train Epoch: 240 [86272/225000 (38%)] Loss: 6354.667969\n",
      "Train Epoch: 240 [90368/225000 (40%)] Loss: 6189.773438\n",
      "Train Epoch: 240 [94464/225000 (42%)] Loss: 6273.177734\n",
      "Train Epoch: 240 [98560/225000 (44%)] Loss: 6388.486328\n",
      "Train Epoch: 240 [102656/225000 (46%)] Loss: 6221.421875\n",
      "Train Epoch: 240 [106752/225000 (47%)] Loss: 6405.533203\n",
      "Train Epoch: 240 [110848/225000 (49%)] Loss: 6261.634766\n",
      "Train Epoch: 240 [114944/225000 (51%)] Loss: 6315.470703\n",
      "Train Epoch: 240 [119040/225000 (53%)] Loss: 6324.355469\n",
      "Train Epoch: 240 [123136/225000 (55%)] Loss: 6204.914062\n",
      "Train Epoch: 240 [127232/225000 (57%)] Loss: 6351.455078\n",
      "Train Epoch: 240 [131328/225000 (58%)] Loss: 6232.544922\n",
      "Train Epoch: 240 [135424/225000 (60%)] Loss: 6268.173828\n",
      "Train Epoch: 240 [139520/225000 (62%)] Loss: 6324.609375\n",
      "Train Epoch: 240 [143616/225000 (64%)] Loss: 6290.060547\n",
      "Train Epoch: 240 [147712/225000 (66%)] Loss: 6316.242188\n",
      "Train Epoch: 240 [151808/225000 (67%)] Loss: 6393.636719\n",
      "Train Epoch: 240 [155904/225000 (69%)] Loss: 6290.482422\n",
      "Train Epoch: 240 [160000/225000 (71%)] Loss: 6380.388672\n",
      "Train Epoch: 240 [164096/225000 (73%)] Loss: 6241.142578\n",
      "Train Epoch: 240 [168192/225000 (75%)] Loss: 6323.677734\n",
      "Train Epoch: 240 [172288/225000 (77%)] Loss: 6335.326172\n",
      "Train Epoch: 240 [176384/225000 (78%)] Loss: 6343.251953\n",
      "Train Epoch: 240 [180480/225000 (80%)] Loss: 6390.019531\n",
      "Train Epoch: 240 [184576/225000 (82%)] Loss: 6388.978516\n",
      "Train Epoch: 240 [188672/225000 (84%)] Loss: 6300.753906\n",
      "Train Epoch: 240 [192768/225000 (86%)] Loss: 6331.177734\n",
      "Train Epoch: 240 [196864/225000 (87%)] Loss: 6246.402344\n",
      "Train Epoch: 240 [200960/225000 (89%)] Loss: 6434.072266\n",
      "Train Epoch: 240 [205056/225000 (91%)] Loss: 6383.365234\n",
      "Train Epoch: 240 [209152/225000 (93%)] Loss: 6316.650391\n",
      "Train Epoch: 240 [213248/225000 (95%)] Loss: 6276.617188\n",
      "Train Epoch: 240 [217344/225000 (97%)] Loss: 6307.923828\n",
      "Train Epoch: 240 [221440/225000 (98%)] Loss: 6236.037109\n",
      "    epoch          : 240\n",
      "    loss           : 6375.324225415956\n",
      "    val_loss       : 6419.4430790069155\n",
      "Train Epoch: 241 [256/225000 (0%)] Loss: 6359.416016\n",
      "Train Epoch: 241 [4352/225000 (2%)] Loss: 6480.005859\n",
      "Train Epoch: 241 [8448/225000 (4%)] Loss: 6360.638672\n",
      "Train Epoch: 241 [12544/225000 (6%)] Loss: 6253.517578\n",
      "Train Epoch: 241 [16640/225000 (7%)] Loss: 26713.873047\n",
      "Train Epoch: 241 [20736/225000 (9%)] Loss: 6378.601562\n",
      "Train Epoch: 241 [24832/225000 (11%)] Loss: 6282.888672\n",
      "Train Epoch: 241 [28928/225000 (13%)] Loss: 6333.425781\n",
      "Train Epoch: 241 [33024/225000 (15%)] Loss: 6350.917969\n",
      "Train Epoch: 241 [37120/225000 (16%)] Loss: 6309.673828\n",
      "Train Epoch: 241 [41216/225000 (18%)] Loss: 6429.181641\n",
      "Train Epoch: 241 [45312/225000 (20%)] Loss: 6326.062500\n",
      "Train Epoch: 241 [49408/225000 (22%)] Loss: 6266.013672\n",
      "Train Epoch: 241 [53504/225000 (24%)] Loss: 6271.271484\n",
      "Train Epoch: 241 [57600/225000 (26%)] Loss: 6435.517578\n",
      "Train Epoch: 241 [61696/225000 (27%)] Loss: 6368.968750\n",
      "Train Epoch: 241 [65792/225000 (29%)] Loss: 6430.353516\n",
      "Train Epoch: 241 [69888/225000 (31%)] Loss: 6314.785156\n",
      "Train Epoch: 241 [73984/225000 (33%)] Loss: 6299.503906\n",
      "Train Epoch: 241 [78080/225000 (35%)] Loss: 6125.093750\n",
      "Train Epoch: 241 [82176/225000 (37%)] Loss: 6452.175781\n",
      "Train Epoch: 241 [86272/225000 (38%)] Loss: 6310.837891\n",
      "Train Epoch: 241 [90368/225000 (40%)] Loss: 6412.322266\n",
      "Train Epoch: 241 [94464/225000 (42%)] Loss: 6131.888672\n",
      "Train Epoch: 241 [98560/225000 (44%)] Loss: 6445.376953\n",
      "Train Epoch: 241 [102656/225000 (46%)] Loss: 6295.005859\n",
      "Train Epoch: 241 [106752/225000 (47%)] Loss: 6429.349609\n",
      "Train Epoch: 241 [110848/225000 (49%)] Loss: 6385.869141\n",
      "Train Epoch: 241 [114944/225000 (51%)] Loss: 6274.855469\n",
      "Train Epoch: 241 [119040/225000 (53%)] Loss: 6383.792969\n",
      "Train Epoch: 241 [123136/225000 (55%)] Loss: 6517.091797\n",
      "Train Epoch: 241 [127232/225000 (57%)] Loss: 6298.564453\n",
      "Train Epoch: 241 [131328/225000 (58%)] Loss: 6323.111328\n",
      "Train Epoch: 241 [135424/225000 (60%)] Loss: 6283.855469\n",
      "Train Epoch: 241 [139520/225000 (62%)] Loss: 6214.947266\n",
      "Train Epoch: 241 [143616/225000 (64%)] Loss: 6194.359375\n",
      "Train Epoch: 241 [147712/225000 (66%)] Loss: 6222.367188\n",
      "Train Epoch: 241 [151808/225000 (67%)] Loss: 6344.169922\n",
      "Train Epoch: 241 [155904/225000 (69%)] Loss: 6391.732422\n",
      "Train Epoch: 241 [160000/225000 (71%)] Loss: 6352.814453\n",
      "Train Epoch: 241 [164096/225000 (73%)] Loss: 6265.964844\n",
      "Train Epoch: 241 [168192/225000 (75%)] Loss: 6310.009766\n",
      "Train Epoch: 241 [172288/225000 (77%)] Loss: 6466.982422\n",
      "Train Epoch: 241 [176384/225000 (78%)] Loss: 6284.968750\n",
      "Train Epoch: 241 [180480/225000 (80%)] Loss: 6302.888672\n",
      "Train Epoch: 241 [184576/225000 (82%)] Loss: 6270.322266\n",
      "Train Epoch: 241 [188672/225000 (84%)] Loss: 6503.664062\n",
      "Train Epoch: 241 [192768/225000 (86%)] Loss: 6363.353516\n",
      "Train Epoch: 241 [196864/225000 (87%)] Loss: 6394.220703\n",
      "Train Epoch: 241 [200960/225000 (89%)] Loss: 6284.082031\n",
      "Train Epoch: 241 [205056/225000 (91%)] Loss: 6364.988281\n",
      "Train Epoch: 241 [209152/225000 (93%)] Loss: 6311.945312\n",
      "Train Epoch: 241 [213248/225000 (95%)] Loss: 6322.205078\n",
      "Train Epoch: 241 [217344/225000 (97%)] Loss: 6295.082031\n",
      "Train Epoch: 241 [221440/225000 (98%)] Loss: 6217.406250\n",
      "    epoch          : 241\n",
      "    loss           : 6394.668394260168\n",
      "    val_loss       : 6608.459213705695\n",
      "Train Epoch: 242 [256/225000 (0%)] Loss: 6361.810547\n",
      "Train Epoch: 242 [4352/225000 (2%)] Loss: 6368.232422\n",
      "Train Epoch: 242 [8448/225000 (4%)] Loss: 6387.556641\n",
      "Train Epoch: 242 [12544/225000 (6%)] Loss: 6335.066406\n",
      "Train Epoch: 242 [16640/225000 (7%)] Loss: 6360.812500\n",
      "Train Epoch: 242 [20736/225000 (9%)] Loss: 6348.820312\n",
      "Train Epoch: 242 [24832/225000 (11%)] Loss: 6343.328125\n",
      "Train Epoch: 242 [28928/225000 (13%)] Loss: 6321.345703\n",
      "Train Epoch: 242 [33024/225000 (15%)] Loss: 6259.347656\n",
      "Train Epoch: 242 [37120/225000 (16%)] Loss: 6438.101562\n",
      "Train Epoch: 242 [41216/225000 (18%)] Loss: 6215.554688\n",
      "Train Epoch: 242 [45312/225000 (20%)] Loss: 6348.621094\n",
      "Train Epoch: 242 [49408/225000 (22%)] Loss: 6247.460938\n",
      "Train Epoch: 242 [53504/225000 (24%)] Loss: 6325.791016\n",
      "Train Epoch: 242 [57600/225000 (26%)] Loss: 6332.212891\n",
      "Train Epoch: 242 [61696/225000 (27%)] Loss: 6332.376953\n",
      "Train Epoch: 242 [65792/225000 (29%)] Loss: 6337.861328\n",
      "Train Epoch: 242 [69888/225000 (31%)] Loss: 6311.509766\n",
      "Train Epoch: 242 [73984/225000 (33%)] Loss: 6328.746094\n",
      "Train Epoch: 242 [78080/225000 (35%)] Loss: 6318.068359\n",
      "Train Epoch: 242 [82176/225000 (37%)] Loss: 6424.945312\n",
      "Train Epoch: 242 [86272/225000 (38%)] Loss: 6415.240234\n",
      "Train Epoch: 242 [90368/225000 (40%)] Loss: 6455.755859\n",
      "Train Epoch: 242 [94464/225000 (42%)] Loss: 6291.607422\n",
      "Train Epoch: 242 [98560/225000 (44%)] Loss: 6445.580078\n",
      "Train Epoch: 242 [102656/225000 (46%)] Loss: 6209.410156\n",
      "Train Epoch: 242 [106752/225000 (47%)] Loss: 6243.300781\n",
      "Train Epoch: 242 [110848/225000 (49%)] Loss: 6207.974609\n",
      "Train Epoch: 242 [114944/225000 (51%)] Loss: 6342.623047\n",
      "Train Epoch: 242 [119040/225000 (53%)] Loss: 6338.072266\n",
      "Train Epoch: 242 [123136/225000 (55%)] Loss: 6269.347656\n",
      "Train Epoch: 242 [127232/225000 (57%)] Loss: 6486.183594\n",
      "Train Epoch: 242 [131328/225000 (58%)] Loss: 6380.544922\n",
      "Train Epoch: 242 [135424/225000 (60%)] Loss: 6298.275391\n",
      "Train Epoch: 242 [139520/225000 (62%)] Loss: 6306.513672\n",
      "Train Epoch: 242 [143616/225000 (64%)] Loss: 6268.830078\n",
      "Train Epoch: 242 [147712/225000 (66%)] Loss: 6416.335938\n",
      "Train Epoch: 242 [151808/225000 (67%)] Loss: 6227.414062\n",
      "Train Epoch: 242 [155904/225000 (69%)] Loss: 6319.687500\n",
      "Train Epoch: 242 [160000/225000 (71%)] Loss: 6163.363281\n",
      "Train Epoch: 242 [164096/225000 (73%)] Loss: 6530.474609\n",
      "Train Epoch: 242 [168192/225000 (75%)] Loss: 6268.447266\n",
      "Train Epoch: 242 [172288/225000 (77%)] Loss: 6278.679688\n",
      "Train Epoch: 242 [176384/225000 (78%)] Loss: 6349.808594\n",
      "Train Epoch: 242 [180480/225000 (80%)] Loss: 6330.201172\n",
      "Train Epoch: 242 [184576/225000 (82%)] Loss: 6368.531250\n",
      "Train Epoch: 242 [188672/225000 (84%)] Loss: 6383.859375\n",
      "Train Epoch: 242 [192768/225000 (86%)] Loss: 6436.712891\n",
      "Train Epoch: 242 [196864/225000 (87%)] Loss: 6285.912109\n",
      "Train Epoch: 242 [200960/225000 (89%)] Loss: 6253.234375\n",
      "Train Epoch: 242 [205056/225000 (91%)] Loss: 6301.035156\n",
      "Train Epoch: 242 [209152/225000 (93%)] Loss: 6433.791016\n",
      "Train Epoch: 242 [213248/225000 (95%)] Loss: 6258.857422\n",
      "Train Epoch: 242 [217344/225000 (97%)] Loss: 6301.250000\n",
      "Train Epoch: 242 [221440/225000 (98%)] Loss: 6351.763672\n",
      "    epoch          : 242\n",
      "    loss           : 6378.273614147824\n",
      "    val_loss       : 6401.177764575092\n",
      "Train Epoch: 243 [256/225000 (0%)] Loss: 6352.572266\n",
      "Train Epoch: 243 [4352/225000 (2%)] Loss: 6213.107422\n",
      "Train Epoch: 243 [8448/225000 (4%)] Loss: 6506.300781\n",
      "Train Epoch: 243 [12544/225000 (6%)] Loss: 6276.880859\n",
      "Train Epoch: 243 [16640/225000 (7%)] Loss: 6331.550781\n",
      "Train Epoch: 243 [20736/225000 (9%)] Loss: 6234.054688\n",
      "Train Epoch: 243 [24832/225000 (11%)] Loss: 6576.626953\n",
      "Train Epoch: 243 [28928/225000 (13%)] Loss: 6333.333984\n",
      "Train Epoch: 243 [33024/225000 (15%)] Loss: 6322.501953\n",
      "Train Epoch: 243 [37120/225000 (16%)] Loss: 6413.367188\n",
      "Train Epoch: 243 [41216/225000 (18%)] Loss: 6231.265625\n",
      "Train Epoch: 243 [45312/225000 (20%)] Loss: 6335.269531\n",
      "Train Epoch: 243 [49408/225000 (22%)] Loss: 6371.701172\n",
      "Train Epoch: 243 [53504/225000 (24%)] Loss: 6304.179688\n",
      "Train Epoch: 243 [57600/225000 (26%)] Loss: 6360.785156\n",
      "Train Epoch: 243 [61696/225000 (27%)] Loss: 6393.390625\n",
      "Train Epoch: 243 [65792/225000 (29%)] Loss: 6323.957031\n",
      "Train Epoch: 243 [69888/225000 (31%)] Loss: 6414.511719\n",
      "Train Epoch: 243 [73984/225000 (33%)] Loss: 6367.595703\n",
      "Train Epoch: 243 [78080/225000 (35%)] Loss: 6443.871094\n",
      "Train Epoch: 243 [82176/225000 (37%)] Loss: 6297.636719\n",
      "Train Epoch: 243 [86272/225000 (38%)] Loss: 6373.671875\n",
      "Train Epoch: 243 [90368/225000 (40%)] Loss: 6373.783203\n",
      "Train Epoch: 243 [94464/225000 (42%)] Loss: 6243.066406\n",
      "Train Epoch: 243 [98560/225000 (44%)] Loss: 6318.466797\n",
      "Train Epoch: 243 [102656/225000 (46%)] Loss: 6307.759766\n",
      "Train Epoch: 243 [106752/225000 (47%)] Loss: 6335.021484\n",
      "Train Epoch: 243 [110848/225000 (49%)] Loss: 6520.427734\n",
      "Train Epoch: 243 [114944/225000 (51%)] Loss: 6258.193359\n",
      "Train Epoch: 243 [119040/225000 (53%)] Loss: 6223.343750\n",
      "Train Epoch: 243 [123136/225000 (55%)] Loss: 6417.367188\n",
      "Train Epoch: 243 [127232/225000 (57%)] Loss: 6395.134766\n",
      "Train Epoch: 243 [131328/225000 (58%)] Loss: 6314.783203\n",
      "Train Epoch: 243 [135424/225000 (60%)] Loss: 6230.242188\n",
      "Train Epoch: 243 [139520/225000 (62%)] Loss: 6442.886719\n",
      "Train Epoch: 243 [143616/225000 (64%)] Loss: 6314.126953\n",
      "Train Epoch: 243 [147712/225000 (66%)] Loss: 6388.628906\n",
      "Train Epoch: 243 [151808/225000 (67%)] Loss: 6393.019531\n",
      "Train Epoch: 243 [155904/225000 (69%)] Loss: 6372.324219\n",
      "Train Epoch: 243 [160000/225000 (71%)] Loss: 6510.607422\n",
      "Train Epoch: 243 [164096/225000 (73%)] Loss: 6440.591797\n",
      "Train Epoch: 243 [168192/225000 (75%)] Loss: 6430.587891\n",
      "Train Epoch: 243 [172288/225000 (77%)] Loss: 6362.492188\n",
      "Train Epoch: 243 [176384/225000 (78%)] Loss: 6229.343750\n",
      "Train Epoch: 243 [180480/225000 (80%)] Loss: 6401.146484\n",
      "Train Epoch: 243 [184576/225000 (82%)] Loss: 6384.929688\n",
      "Train Epoch: 243 [188672/225000 (84%)] Loss: 6242.048828\n",
      "Train Epoch: 243 [192768/225000 (86%)] Loss: 6192.605469\n",
      "Train Epoch: 243 [196864/225000 (87%)] Loss: 6233.763672\n",
      "Train Epoch: 243 [200960/225000 (89%)] Loss: 6368.220703\n",
      "Train Epoch: 243 [205056/225000 (91%)] Loss: 6403.494141\n",
      "Train Epoch: 243 [209152/225000 (93%)] Loss: 6331.089844\n",
      "Train Epoch: 243 [213248/225000 (95%)] Loss: 6424.224609\n",
      "Train Epoch: 243 [217344/225000 (97%)] Loss: 6306.232422\n",
      "Train Epoch: 243 [221440/225000 (98%)] Loss: 6401.183594\n",
      "    epoch          : 243\n",
      "    loss           : 6363.537176034556\n",
      "    val_loss       : 6841.079149821583\n",
      "Train Epoch: 244 [256/225000 (0%)] Loss: 6314.585938\n",
      "Train Epoch: 244 [4352/225000 (2%)] Loss: 6318.943359\n",
      "Train Epoch: 244 [8448/225000 (4%)] Loss: 6256.566406\n",
      "Train Epoch: 244 [12544/225000 (6%)] Loss: 6218.873047\n",
      "Train Epoch: 244 [16640/225000 (7%)] Loss: 6291.066406\n",
      "Train Epoch: 244 [20736/225000 (9%)] Loss: 6394.699219\n",
      "Train Epoch: 244 [24832/225000 (11%)] Loss: 6288.285156\n",
      "Train Epoch: 244 [28928/225000 (13%)] Loss: 6400.166016\n",
      "Train Epoch: 244 [33024/225000 (15%)] Loss: 6349.931641\n",
      "Train Epoch: 244 [37120/225000 (16%)] Loss: 6231.158203\n",
      "Train Epoch: 244 [41216/225000 (18%)] Loss: 6283.818359\n",
      "Train Epoch: 244 [45312/225000 (20%)] Loss: 6476.166016\n",
      "Train Epoch: 244 [49408/225000 (22%)] Loss: 6331.056641\n",
      "Train Epoch: 244 [53504/225000 (24%)] Loss: 6401.505859\n",
      "Train Epoch: 244 [57600/225000 (26%)] Loss: 6318.982422\n",
      "Train Epoch: 244 [61696/225000 (27%)] Loss: 6468.224609\n",
      "Train Epoch: 244 [65792/225000 (29%)] Loss: 6253.224609\n",
      "Train Epoch: 244 [69888/225000 (31%)] Loss: 6424.470703\n",
      "Train Epoch: 244 [73984/225000 (33%)] Loss: 6373.138672\n",
      "Train Epoch: 244 [78080/225000 (35%)] Loss: 6334.935547\n",
      "Train Epoch: 244 [82176/225000 (37%)] Loss: 6266.017578\n",
      "Train Epoch: 244 [86272/225000 (38%)] Loss: 6398.740234\n",
      "Train Epoch: 244 [90368/225000 (40%)] Loss: 6183.861328\n",
      "Train Epoch: 244 [94464/225000 (42%)] Loss: 6413.775391\n",
      "Train Epoch: 244 [98560/225000 (44%)] Loss: 6360.312500\n",
      "Train Epoch: 244 [102656/225000 (46%)] Loss: 6122.375000\n",
      "Train Epoch: 244 [106752/225000 (47%)] Loss: 6370.078125\n",
      "Train Epoch: 244 [110848/225000 (49%)] Loss: 6389.095703\n",
      "Train Epoch: 244 [114944/225000 (51%)] Loss: 6306.251953\n",
      "Train Epoch: 244 [119040/225000 (53%)] Loss: 6395.802734\n",
      "Train Epoch: 244 [123136/225000 (55%)] Loss: 6366.939453\n",
      "Train Epoch: 244 [127232/225000 (57%)] Loss: 6317.490234\n",
      "Train Epoch: 244 [131328/225000 (58%)] Loss: 6430.158203\n",
      "Train Epoch: 244 [135424/225000 (60%)] Loss: 6359.666016\n",
      "Train Epoch: 244 [139520/225000 (62%)] Loss: 6256.074219\n",
      "Train Epoch: 244 [143616/225000 (64%)] Loss: 6374.789062\n",
      "Train Epoch: 244 [147712/225000 (66%)] Loss: 6337.906250\n",
      "Train Epoch: 244 [151808/225000 (67%)] Loss: 6380.488281\n",
      "Train Epoch: 244 [155904/225000 (69%)] Loss: 6439.226562\n",
      "Train Epoch: 244 [160000/225000 (71%)] Loss: 6312.664062\n",
      "Train Epoch: 244 [164096/225000 (73%)] Loss: 6298.201172\n",
      "Train Epoch: 244 [168192/225000 (75%)] Loss: 6532.294922\n",
      "Train Epoch: 244 [172288/225000 (77%)] Loss: 6298.837891\n",
      "Train Epoch: 244 [176384/225000 (78%)] Loss: 6385.011719\n",
      "Train Epoch: 244 [180480/225000 (80%)] Loss: 6324.435547\n",
      "Train Epoch: 244 [184576/225000 (82%)] Loss: 6380.466797\n",
      "Train Epoch: 244 [188672/225000 (84%)] Loss: 6342.421875\n",
      "Train Epoch: 244 [192768/225000 (86%)] Loss: 6287.494141\n",
      "Train Epoch: 244 [196864/225000 (87%)] Loss: 6545.521484\n",
      "Train Epoch: 244 [200960/225000 (89%)] Loss: 6432.964844\n",
      "Train Epoch: 244 [205056/225000 (91%)] Loss: 6373.451172\n",
      "Train Epoch: 244 [209152/225000 (93%)] Loss: 6268.953125\n",
      "Train Epoch: 244 [213248/225000 (95%)] Loss: 6274.562500\n",
      "Train Epoch: 244 [217344/225000 (97%)] Loss: 6273.349609\n",
      "Train Epoch: 244 [221440/225000 (98%)] Loss: 6301.957031\n",
      "    epoch          : 244\n",
      "    loss           : 6365.956533525313\n",
      "    val_loss       : 6418.939547091114\n",
      "Train Epoch: 245 [256/225000 (0%)] Loss: 6395.863281\n",
      "Train Epoch: 245 [4352/225000 (2%)] Loss: 6160.740234\n",
      "Train Epoch: 245 [8448/225000 (4%)] Loss: 6188.283203\n",
      "Train Epoch: 245 [12544/225000 (6%)] Loss: 6426.291016\n",
      "Train Epoch: 245 [16640/225000 (7%)] Loss: 6237.019531\n",
      "Train Epoch: 245 [20736/225000 (9%)] Loss: 6495.091797\n",
      "Train Epoch: 245 [24832/225000 (11%)] Loss: 6293.662109\n",
      "Train Epoch: 245 [28928/225000 (13%)] Loss: 6369.101562\n",
      "Train Epoch: 245 [33024/225000 (15%)] Loss: 6312.484375\n",
      "Train Epoch: 245 [37120/225000 (16%)] Loss: 6336.798828\n",
      "Train Epoch: 245 [41216/225000 (18%)] Loss: 6365.326172\n",
      "Train Epoch: 245 [45312/225000 (20%)] Loss: 6331.300781\n",
      "Train Epoch: 245 [49408/225000 (22%)] Loss: 6389.660156\n",
      "Train Epoch: 245 [53504/225000 (24%)] Loss: 6226.578125\n",
      "Train Epoch: 245 [57600/225000 (26%)] Loss: 6443.296875\n",
      "Train Epoch: 245 [61696/225000 (27%)] Loss: 6409.078125\n",
      "Train Epoch: 245 [65792/225000 (29%)] Loss: 6349.820312\n",
      "Train Epoch: 245 [69888/225000 (31%)] Loss: 6239.371094\n",
      "Train Epoch: 245 [73984/225000 (33%)] Loss: 6272.242188\n",
      "Train Epoch: 245 [78080/225000 (35%)] Loss: 6199.886719\n",
      "Train Epoch: 245 [82176/225000 (37%)] Loss: 6260.906250\n",
      "Train Epoch: 245 [86272/225000 (38%)] Loss: 6372.996094\n",
      "Train Epoch: 245 [90368/225000 (40%)] Loss: 6228.705078\n",
      "Train Epoch: 245 [94464/225000 (42%)] Loss: 6210.544922\n",
      "Train Epoch: 245 [98560/225000 (44%)] Loss: 6380.748047\n",
      "Train Epoch: 245 [102656/225000 (46%)] Loss: 6379.773438\n",
      "Train Epoch: 245 [106752/225000 (47%)] Loss: 6403.212891\n",
      "Train Epoch: 245 [110848/225000 (49%)] Loss: 6223.986328\n",
      "Train Epoch: 245 [114944/225000 (51%)] Loss: 6319.154297\n",
      "Train Epoch: 245 [119040/225000 (53%)] Loss: 6312.972656\n",
      "Train Epoch: 245 [123136/225000 (55%)] Loss: 6507.248047\n",
      "Train Epoch: 245 [127232/225000 (57%)] Loss: 6276.203125\n",
      "Train Epoch: 245 [131328/225000 (58%)] Loss: 6220.626953\n",
      "Train Epoch: 245 [135424/225000 (60%)] Loss: 6246.453125\n",
      "Train Epoch: 245 [139520/225000 (62%)] Loss: 6390.607422\n",
      "Train Epoch: 245 [143616/225000 (64%)] Loss: 6335.535156\n",
      "Train Epoch: 245 [147712/225000 (66%)] Loss: 6423.376953\n",
      "Train Epoch: 245 [151808/225000 (67%)] Loss: 6369.833984\n",
      "Train Epoch: 245 [155904/225000 (69%)] Loss: 6373.791016\n",
      "Train Epoch: 245 [160000/225000 (71%)] Loss: 6235.939453\n",
      "Train Epoch: 245 [164096/225000 (73%)] Loss: 6319.984375\n",
      "Train Epoch: 245 [168192/225000 (75%)] Loss: 6297.496094\n",
      "Train Epoch: 245 [172288/225000 (77%)] Loss: 6410.121094\n",
      "Train Epoch: 245 [176384/225000 (78%)] Loss: 6283.748047\n",
      "Train Epoch: 245 [180480/225000 (80%)] Loss: 6322.929688\n",
      "Train Epoch: 245 [184576/225000 (82%)] Loss: 6377.476562\n",
      "Train Epoch: 245 [188672/225000 (84%)] Loss: 6371.562500\n",
      "Train Epoch: 245 [192768/225000 (86%)] Loss: 6365.246094\n",
      "Train Epoch: 245 [196864/225000 (87%)] Loss: 6346.580078\n",
      "Train Epoch: 245 [200960/225000 (89%)] Loss: 6208.591797\n",
      "Train Epoch: 245 [205056/225000 (91%)] Loss: 6248.517578\n",
      "Train Epoch: 245 [209152/225000 (93%)] Loss: 6322.687500\n",
      "Train Epoch: 245 [213248/225000 (95%)] Loss: 6402.029297\n",
      "Train Epoch: 245 [217344/225000 (97%)] Loss: 6299.996094\n",
      "Train Epoch: 245 [221440/225000 (98%)] Loss: 6216.324219\n",
      "    epoch          : 245\n",
      "    loss           : 6356.036093927759\n",
      "    val_loss       : 6401.214793385291\n",
      "Train Epoch: 246 [256/225000 (0%)] Loss: 8111.191406\n",
      "Train Epoch: 246 [4352/225000 (2%)] Loss: 6400.605469\n",
      "Train Epoch: 246 [8448/225000 (4%)] Loss: 6345.933594\n",
      "Train Epoch: 246 [12544/225000 (6%)] Loss: 6303.806641\n",
      "Train Epoch: 246 [16640/225000 (7%)] Loss: 6295.722656\n",
      "Train Epoch: 246 [20736/225000 (9%)] Loss: 6341.099609\n",
      "Train Epoch: 246 [24832/225000 (11%)] Loss: 6189.541016\n",
      "Train Epoch: 246 [28928/225000 (13%)] Loss: 6450.599609\n",
      "Train Epoch: 246 [33024/225000 (15%)] Loss: 6375.410156\n",
      "Train Epoch: 246 [37120/225000 (16%)] Loss: 6353.099609\n",
      "Train Epoch: 246 [41216/225000 (18%)] Loss: 6365.675781\n",
      "Train Epoch: 246 [45312/225000 (20%)] Loss: 6433.828125\n",
      "Train Epoch: 246 [49408/225000 (22%)] Loss: 6326.650391\n",
      "Train Epoch: 246 [53504/225000 (24%)] Loss: 6296.773438\n",
      "Train Epoch: 246 [57600/225000 (26%)] Loss: 6305.312500\n",
      "Train Epoch: 246 [61696/225000 (27%)] Loss: 6208.443359\n",
      "Train Epoch: 246 [65792/225000 (29%)] Loss: 6429.707031\n",
      "Train Epoch: 246 [69888/225000 (31%)] Loss: 6244.115234\n",
      "Train Epoch: 246 [73984/225000 (33%)] Loss: 6266.373047\n",
      "Train Epoch: 246 [78080/225000 (35%)] Loss: 6480.798828\n",
      "Train Epoch: 246 [82176/225000 (37%)] Loss: 6372.169922\n",
      "Train Epoch: 246 [86272/225000 (38%)] Loss: 6275.292969\n",
      "Train Epoch: 246 [90368/225000 (40%)] Loss: 6377.078125\n",
      "Train Epoch: 246 [94464/225000 (42%)] Loss: 6439.027344\n",
      "Train Epoch: 246 [98560/225000 (44%)] Loss: 6347.164062\n",
      "Train Epoch: 246 [102656/225000 (46%)] Loss: 6328.197266\n",
      "Train Epoch: 246 [106752/225000 (47%)] Loss: 6285.070312\n",
      "Train Epoch: 246 [110848/225000 (49%)] Loss: 6294.101562\n",
      "Train Epoch: 246 [114944/225000 (51%)] Loss: 6255.031250\n",
      "Train Epoch: 246 [119040/225000 (53%)] Loss: 6266.890625\n",
      "Train Epoch: 246 [123136/225000 (55%)] Loss: 6293.281250\n",
      "Train Epoch: 246 [127232/225000 (57%)] Loss: 6339.048828\n",
      "Train Epoch: 246 [131328/225000 (58%)] Loss: 6268.013672\n",
      "Train Epoch: 246 [135424/225000 (60%)] Loss: 6397.281250\n",
      "Train Epoch: 246 [139520/225000 (62%)] Loss: 6376.632812\n",
      "Train Epoch: 246 [143616/225000 (64%)] Loss: 6430.708984\n",
      "Train Epoch: 246 [147712/225000 (66%)] Loss: 6348.748047\n",
      "Train Epoch: 246 [151808/225000 (67%)] Loss: 6191.582031\n",
      "Train Epoch: 246 [155904/225000 (69%)] Loss: 6428.203125\n",
      "Train Epoch: 246 [160000/225000 (71%)] Loss: 6357.765625\n",
      "Train Epoch: 246 [164096/225000 (73%)] Loss: 6387.421875\n",
      "Train Epoch: 246 [168192/225000 (75%)] Loss: 6260.318359\n",
      "Train Epoch: 246 [172288/225000 (77%)] Loss: 6257.082031\n",
      "Train Epoch: 246 [176384/225000 (78%)] Loss: 6362.820312\n",
      "Train Epoch: 246 [180480/225000 (80%)] Loss: 6311.472656\n",
      "Train Epoch: 246 [184576/225000 (82%)] Loss: 6318.468750\n",
      "Train Epoch: 246 [188672/225000 (84%)] Loss: 6406.992188\n",
      "Train Epoch: 246 [192768/225000 (86%)] Loss: 6269.423828\n",
      "Train Epoch: 246 [196864/225000 (87%)] Loss: 6262.863281\n",
      "Train Epoch: 246 [200960/225000 (89%)] Loss: 6311.199219\n",
      "Train Epoch: 246 [205056/225000 (91%)] Loss: 6363.998047\n",
      "Train Epoch: 246 [209152/225000 (93%)] Loss: 6277.273438\n",
      "Train Epoch: 246 [213248/225000 (95%)] Loss: 6209.130859\n",
      "Train Epoch: 246 [217344/225000 (97%)] Loss: 6383.656250\n",
      "Train Epoch: 246 [221440/225000 (98%)] Loss: 6357.318359\n",
      "    epoch          : 246\n",
      "    loss           : 6382.928950911903\n",
      "    val_loss       : 6401.192323864722\n",
      "Train Epoch: 247 [256/225000 (0%)] Loss: 6256.332031\n",
      "Train Epoch: 247 [4352/225000 (2%)] Loss: 6252.173828\n",
      "Train Epoch: 247 [8448/225000 (4%)] Loss: 6361.455078\n",
      "Train Epoch: 247 [12544/225000 (6%)] Loss: 6279.003906\n",
      "Train Epoch: 247 [16640/225000 (7%)] Loss: 6467.794922\n",
      "Train Epoch: 247 [20736/225000 (9%)] Loss: 6285.388672\n",
      "Train Epoch: 247 [24832/225000 (11%)] Loss: 6224.484375\n",
      "Train Epoch: 247 [28928/225000 (13%)] Loss: 6390.611328\n",
      "Train Epoch: 247 [33024/225000 (15%)] Loss: 6242.486328\n",
      "Train Epoch: 247 [37120/225000 (16%)] Loss: 6149.537109\n",
      "Train Epoch: 247 [41216/225000 (18%)] Loss: 6452.863281\n",
      "Train Epoch: 247 [45312/225000 (20%)] Loss: 6406.640625\n",
      "Train Epoch: 247 [49408/225000 (22%)] Loss: 6352.691406\n",
      "Train Epoch: 247 [53504/225000 (24%)] Loss: 6424.341797\n",
      "Train Epoch: 247 [57600/225000 (26%)] Loss: 6300.156250\n",
      "Train Epoch: 247 [61696/225000 (27%)] Loss: 6332.693359\n",
      "Train Epoch: 247 [65792/225000 (29%)] Loss: 6376.970703\n",
      "Train Epoch: 247 [69888/225000 (31%)] Loss: 6237.708984\n",
      "Train Epoch: 247 [73984/225000 (33%)] Loss: 6382.363281\n",
      "Train Epoch: 247 [78080/225000 (35%)] Loss: 6265.751953\n",
      "Train Epoch: 247 [82176/225000 (37%)] Loss: 6354.949219\n",
      "Train Epoch: 247 [86272/225000 (38%)] Loss: 6307.562500\n",
      "Train Epoch: 247 [90368/225000 (40%)] Loss: 6419.181641\n",
      "Train Epoch: 247 [94464/225000 (42%)] Loss: 6304.878906\n",
      "Train Epoch: 247 [98560/225000 (44%)] Loss: 6458.085938\n",
      "Train Epoch: 247 [102656/225000 (46%)] Loss: 6328.072266\n",
      "Train Epoch: 247 [106752/225000 (47%)] Loss: 6266.060547\n",
      "Train Epoch: 247 [110848/225000 (49%)] Loss: 6363.582031\n",
      "Train Epoch: 247 [114944/225000 (51%)] Loss: 6264.748047\n",
      "Train Epoch: 247 [119040/225000 (53%)] Loss: 6264.259766\n",
      "Train Epoch: 247 [123136/225000 (55%)] Loss: 6344.755859\n",
      "Train Epoch: 247 [127232/225000 (57%)] Loss: 6404.564453\n",
      "Train Epoch: 247 [131328/225000 (58%)] Loss: 26679.681641\n",
      "Train Epoch: 247 [135424/225000 (60%)] Loss: 6363.578125\n",
      "Train Epoch: 247 [139520/225000 (62%)] Loss: 6232.503906\n",
      "Train Epoch: 247 [143616/225000 (64%)] Loss: 6400.683594\n",
      "Train Epoch: 247 [147712/225000 (66%)] Loss: 6370.675781\n",
      "Train Epoch: 247 [151808/225000 (67%)] Loss: 6354.066406\n",
      "Train Epoch: 247 [155904/225000 (69%)] Loss: 6417.226562\n",
      "Train Epoch: 247 [160000/225000 (71%)] Loss: 6318.828125\n",
      "Train Epoch: 247 [164096/225000 (73%)] Loss: 6305.234375\n",
      "Train Epoch: 247 [168192/225000 (75%)] Loss: 6416.787109\n",
      "Train Epoch: 247 [172288/225000 (77%)] Loss: 6413.523438\n",
      "Train Epoch: 247 [176384/225000 (78%)] Loss: 6327.896484\n",
      "Train Epoch: 247 [180480/225000 (80%)] Loss: 6201.939453\n",
      "Train Epoch: 247 [184576/225000 (82%)] Loss: 6305.146484\n",
      "Train Epoch: 247 [188672/225000 (84%)] Loss: 6288.273438\n",
      "Train Epoch: 247 [192768/225000 (86%)] Loss: 6284.482422\n",
      "Train Epoch: 247 [196864/225000 (87%)] Loss: 6195.878906\n",
      "Train Epoch: 247 [200960/225000 (89%)] Loss: 6251.656250\n",
      "Train Epoch: 247 [205056/225000 (91%)] Loss: 6305.718750\n",
      "Train Epoch: 247 [209152/225000 (93%)] Loss: 6267.169922\n",
      "Train Epoch: 247 [213248/225000 (95%)] Loss: 6306.685547\n",
      "Train Epoch: 247 [217344/225000 (97%)] Loss: 6388.583984\n",
      "Train Epoch: 247 [221440/225000 (98%)] Loss: 6386.386719\n",
      "    epoch          : 247\n",
      "    loss           : 6371.040439019838\n",
      "    val_loss       : 6542.766068155668\n",
      "Train Epoch: 248 [256/225000 (0%)] Loss: 6338.025391\n",
      "Train Epoch: 248 [4352/225000 (2%)] Loss: 6349.498047\n",
      "Train Epoch: 248 [8448/225000 (4%)] Loss: 6197.210938\n",
      "Train Epoch: 248 [12544/225000 (6%)] Loss: 6250.548828\n",
      "Train Epoch: 248 [16640/225000 (7%)] Loss: 6398.576172\n",
      "Train Epoch: 248 [20736/225000 (9%)] Loss: 6480.662109\n",
      "Train Epoch: 248 [24832/225000 (11%)] Loss: 6518.417969\n",
      "Train Epoch: 248 [28928/225000 (13%)] Loss: 6408.201172\n",
      "Train Epoch: 248 [33024/225000 (15%)] Loss: 6427.931641\n",
      "Train Epoch: 248 [37120/225000 (16%)] Loss: 6246.845703\n",
      "Train Epoch: 248 [41216/225000 (18%)] Loss: 6300.386719\n",
      "Train Epoch: 248 [45312/225000 (20%)] Loss: 6250.003906\n",
      "Train Epoch: 248 [49408/225000 (22%)] Loss: 6250.794922\n",
      "Train Epoch: 248 [53504/225000 (24%)] Loss: 6356.265625\n",
      "Train Epoch: 248 [57600/225000 (26%)] Loss: 6412.193359\n",
      "Train Epoch: 248 [61696/225000 (27%)] Loss: 6189.363281\n",
      "Train Epoch: 248 [65792/225000 (29%)] Loss: 6170.648438\n",
      "Train Epoch: 248 [69888/225000 (31%)] Loss: 6305.566406\n",
      "Train Epoch: 248 [73984/225000 (33%)] Loss: 6284.441406\n",
      "Train Epoch: 248 [78080/225000 (35%)] Loss: 6461.298828\n",
      "Train Epoch: 248 [82176/225000 (37%)] Loss: 6257.843750\n",
      "Train Epoch: 248 [86272/225000 (38%)] Loss: 6408.750000\n",
      "Train Epoch: 248 [90368/225000 (40%)] Loss: 6396.300781\n",
      "Train Epoch: 248 [94464/225000 (42%)] Loss: 6280.863281\n",
      "Train Epoch: 248 [98560/225000 (44%)] Loss: 6277.214844\n",
      "Train Epoch: 248 [102656/225000 (46%)] Loss: 6226.789062\n",
      "Train Epoch: 248 [106752/225000 (47%)] Loss: 6317.853516\n",
      "Train Epoch: 248 [110848/225000 (49%)] Loss: 6389.007812\n",
      "Train Epoch: 248 [114944/225000 (51%)] Loss: 6263.273438\n",
      "Train Epoch: 248 [119040/225000 (53%)] Loss: 6347.519531\n",
      "Train Epoch: 248 [123136/225000 (55%)] Loss: 6353.451172\n",
      "Train Epoch: 248 [127232/225000 (57%)] Loss: 6257.527344\n",
      "Train Epoch: 248 [131328/225000 (58%)] Loss: 6276.460938\n",
      "Train Epoch: 248 [135424/225000 (60%)] Loss: 6291.166016\n",
      "Train Epoch: 248 [139520/225000 (62%)] Loss: 6322.029297\n",
      "Train Epoch: 248 [143616/225000 (64%)] Loss: 6336.496094\n",
      "Train Epoch: 248 [147712/225000 (66%)] Loss: 6275.695312\n",
      "Train Epoch: 248 [151808/225000 (67%)] Loss: 6369.410156\n",
      "Train Epoch: 248 [155904/225000 (69%)] Loss: 6331.234375\n",
      "Train Epoch: 248 [160000/225000 (71%)] Loss: 6409.689453\n",
      "Train Epoch: 248 [164096/225000 (73%)] Loss: 6316.605469\n",
      "Train Epoch: 248 [168192/225000 (75%)] Loss: 6405.076172\n",
      "Train Epoch: 248 [172288/225000 (77%)] Loss: 6301.089844\n",
      "Train Epoch: 248 [176384/225000 (78%)] Loss: 6301.500000\n",
      "Train Epoch: 248 [180480/225000 (80%)] Loss: 6316.906250\n",
      "Train Epoch: 248 [184576/225000 (82%)] Loss: 6295.314453\n",
      "Train Epoch: 248 [188672/225000 (84%)] Loss: 6430.095703\n",
      "Train Epoch: 248 [192768/225000 (86%)] Loss: 6300.095703\n",
      "Train Epoch: 248 [196864/225000 (87%)] Loss: 6269.972656\n",
      "Train Epoch: 248 [200960/225000 (89%)] Loss: 6346.894531\n",
      "Train Epoch: 248 [205056/225000 (91%)] Loss: 6423.611328\n",
      "Train Epoch: 248 [209152/225000 (93%)] Loss: 6432.652344\n",
      "Train Epoch: 248 [213248/225000 (95%)] Loss: 6328.123047\n",
      "Train Epoch: 248 [217344/225000 (97%)] Loss: 6465.158203\n",
      "Train Epoch: 248 [221440/225000 (98%)] Loss: 6203.865234\n",
      "    epoch          : 248\n",
      "    loss           : 6392.162293799773\n",
      "    val_loss       : 6437.519247003964\n",
      "Train Epoch: 249 [256/225000 (0%)] Loss: 6398.468750\n",
      "Train Epoch: 249 [4352/225000 (2%)] Loss: 6200.003906\n",
      "Train Epoch: 249 [8448/225000 (4%)] Loss: 6444.216797\n",
      "Train Epoch: 249 [12544/225000 (6%)] Loss: 6341.892578\n",
      "Train Epoch: 249 [16640/225000 (7%)] Loss: 6181.478516\n",
      "Train Epoch: 249 [20736/225000 (9%)] Loss: 6405.435547\n",
      "Train Epoch: 249 [24832/225000 (11%)] Loss: 6445.464844\n",
      "Train Epoch: 249 [28928/225000 (13%)] Loss: 6359.945312\n",
      "Train Epoch: 249 [33024/225000 (15%)] Loss: 6162.652344\n",
      "Train Epoch: 249 [37120/225000 (16%)] Loss: 6246.542969\n",
      "Train Epoch: 249 [41216/225000 (18%)] Loss: 6433.005859\n",
      "Train Epoch: 249 [45312/225000 (20%)] Loss: 6305.945312\n",
      "Train Epoch: 249 [49408/225000 (22%)] Loss: 6239.384766\n",
      "Train Epoch: 249 [53504/225000 (24%)] Loss: 6215.208984\n",
      "Train Epoch: 249 [57600/225000 (26%)] Loss: 6400.888672\n",
      "Train Epoch: 249 [61696/225000 (27%)] Loss: 6384.990234\n",
      "Train Epoch: 249 [65792/225000 (29%)] Loss: 6337.072266\n",
      "Train Epoch: 249 [69888/225000 (31%)] Loss: 6303.107422\n",
      "Train Epoch: 249 [73984/225000 (33%)] Loss: 6254.021484\n",
      "Train Epoch: 249 [78080/225000 (35%)] Loss: 6265.970703\n",
      "Train Epoch: 249 [82176/225000 (37%)] Loss: 6367.609375\n",
      "Train Epoch: 249 [86272/225000 (38%)] Loss: 6469.068359\n",
      "Train Epoch: 249 [90368/225000 (40%)] Loss: 6277.177734\n",
      "Train Epoch: 249 [94464/225000 (42%)] Loss: 6332.775391\n",
      "Train Epoch: 249 [98560/225000 (44%)] Loss: 6348.763672\n",
      "Train Epoch: 249 [102656/225000 (46%)] Loss: 6257.882812\n",
      "Train Epoch: 249 [106752/225000 (47%)] Loss: 6374.105469\n",
      "Train Epoch: 249 [110848/225000 (49%)] Loss: 6324.187500\n",
      "Train Epoch: 249 [114944/225000 (51%)] Loss: 6271.599609\n",
      "Train Epoch: 249 [119040/225000 (53%)] Loss: 6383.095703\n",
      "Train Epoch: 249 [123136/225000 (55%)] Loss: 6324.615234\n",
      "Train Epoch: 249 [127232/225000 (57%)] Loss: 6326.466797\n",
      "Train Epoch: 249 [131328/225000 (58%)] Loss: 6407.222656\n",
      "Train Epoch: 249 [135424/225000 (60%)] Loss: 8090.841797\n",
      "Train Epoch: 249 [139520/225000 (62%)] Loss: 6292.216797\n",
      "Train Epoch: 249 [143616/225000 (64%)] Loss: 6456.310547\n",
      "Train Epoch: 249 [147712/225000 (66%)] Loss: 6339.638672\n",
      "Train Epoch: 249 [151808/225000 (67%)] Loss: 6244.355469\n",
      "Train Epoch: 249 [155904/225000 (69%)] Loss: 6386.978516\n",
      "Train Epoch: 249 [160000/225000 (71%)] Loss: 6421.808594\n",
      "Train Epoch: 249 [164096/225000 (73%)] Loss: 6405.654297\n",
      "Train Epoch: 249 [168192/225000 (75%)] Loss: 6431.818359\n",
      "Train Epoch: 249 [172288/225000 (77%)] Loss: 6316.859375\n",
      "Train Epoch: 249 [176384/225000 (78%)] Loss: 6238.779297\n",
      "Train Epoch: 249 [180480/225000 (80%)] Loss: 6529.529297\n",
      "Train Epoch: 249 [184576/225000 (82%)] Loss: 6401.847656\n",
      "Train Epoch: 249 [188672/225000 (84%)] Loss: 6477.072266\n",
      "Train Epoch: 249 [192768/225000 (86%)] Loss: 6356.066406\n",
      "Train Epoch: 249 [196864/225000 (87%)] Loss: 6314.314453\n",
      "Train Epoch: 249 [200960/225000 (89%)] Loss: 6375.757812\n",
      "Train Epoch: 249 [205056/225000 (91%)] Loss: 6209.972656\n",
      "Train Epoch: 249 [209152/225000 (93%)] Loss: 6342.933594\n",
      "Train Epoch: 249 [213248/225000 (95%)] Loss: 6324.136719\n",
      "Train Epoch: 249 [217344/225000 (97%)] Loss: 6370.191406\n",
      "Train Epoch: 249 [221440/225000 (98%)] Loss: 6440.812500\n",
      "    epoch          : 249\n",
      "    loss           : 6418.432078356086\n",
      "    val_loss       : 6438.062468975174\n",
      "Train Epoch: 250 [256/225000 (0%)] Loss: 6325.771484\n",
      "Train Epoch: 250 [4352/225000 (2%)] Loss: 6304.070312\n",
      "Train Epoch: 250 [8448/225000 (4%)] Loss: 6270.064453\n",
      "Train Epoch: 250 [12544/225000 (6%)] Loss: 6268.312500\n",
      "Train Epoch: 250 [16640/225000 (7%)] Loss: 6358.349609\n",
      "Train Epoch: 250 [20736/225000 (9%)] Loss: 6278.310547\n",
      "Train Epoch: 250 [24832/225000 (11%)] Loss: 6206.123047\n",
      "Train Epoch: 250 [28928/225000 (13%)] Loss: 6366.894531\n",
      "Train Epoch: 250 [33024/225000 (15%)] Loss: 6368.917969\n",
      "Train Epoch: 250 [37120/225000 (16%)] Loss: 6323.039062\n",
      "Train Epoch: 250 [41216/225000 (18%)] Loss: 6328.421875\n",
      "Train Epoch: 250 [45312/225000 (20%)] Loss: 6309.507812\n",
      "Train Epoch: 250 [49408/225000 (22%)] Loss: 6265.787109\n",
      "Train Epoch: 250 [53504/225000 (24%)] Loss: 6330.472656\n",
      "Train Epoch: 250 [57600/225000 (26%)] Loss: 6410.300781\n",
      "Train Epoch: 250 [61696/225000 (27%)] Loss: 6489.648438\n",
      "Train Epoch: 250 [65792/225000 (29%)] Loss: 6420.666016\n",
      "Train Epoch: 250 [69888/225000 (31%)] Loss: 6389.097656\n",
      "Train Epoch: 250 [73984/225000 (33%)] Loss: 6314.105469\n",
      "Train Epoch: 250 [78080/225000 (35%)] Loss: 6237.226562\n",
      "Train Epoch: 250 [82176/225000 (37%)] Loss: 6330.333984\n",
      "Train Epoch: 250 [86272/225000 (38%)] Loss: 6366.031250\n",
      "Train Epoch: 250 [90368/225000 (40%)] Loss: 6272.603516\n",
      "Train Epoch: 250 [94464/225000 (42%)] Loss: 6214.433594\n",
      "Train Epoch: 250 [98560/225000 (44%)] Loss: 6264.466797\n",
      "Train Epoch: 250 [102656/225000 (46%)] Loss: 6318.140625\n",
      "Train Epoch: 250 [106752/225000 (47%)] Loss: 6216.974609\n",
      "Train Epoch: 250 [110848/225000 (49%)] Loss: 6420.191406\n",
      "Train Epoch: 250 [114944/225000 (51%)] Loss: 6302.976562\n",
      "Train Epoch: 250 [119040/225000 (53%)] Loss: 6232.859375\n",
      "Train Epoch: 250 [123136/225000 (55%)] Loss: 6326.101562\n",
      "Train Epoch: 250 [127232/225000 (57%)] Loss: 6245.748047\n",
      "Train Epoch: 250 [131328/225000 (58%)] Loss: 6420.111328\n",
      "Train Epoch: 250 [135424/225000 (60%)] Loss: 6305.207031\n",
      "Train Epoch: 250 [139520/225000 (62%)] Loss: 6327.138672\n",
      "Train Epoch: 250 [143616/225000 (64%)] Loss: 6179.205078\n",
      "Train Epoch: 250 [147712/225000 (66%)] Loss: 6370.117188\n",
      "Train Epoch: 250 [151808/225000 (67%)] Loss: 6320.464844\n",
      "Train Epoch: 250 [155904/225000 (69%)] Loss: 26726.591797\n",
      "Train Epoch: 250 [160000/225000 (71%)] Loss: 6319.306641\n",
      "Train Epoch: 250 [164096/225000 (73%)] Loss: 6270.175781\n",
      "Train Epoch: 250 [168192/225000 (75%)] Loss: 6287.671875\n",
      "Train Epoch: 250 [172288/225000 (77%)] Loss: 6436.705078\n",
      "Train Epoch: 250 [176384/225000 (78%)] Loss: 6326.236328\n",
      "Train Epoch: 250 [180480/225000 (80%)] Loss: 6407.058594\n",
      "Train Epoch: 250 [184576/225000 (82%)] Loss: 6304.041016\n",
      "Train Epoch: 250 [188672/225000 (84%)] Loss: 6417.169922\n",
      "Train Epoch: 250 [192768/225000 (86%)] Loss: 6272.296875\n",
      "Train Epoch: 250 [196864/225000 (87%)] Loss: 6311.626953\n",
      "Train Epoch: 250 [200960/225000 (89%)] Loss: 6291.310547\n",
      "Train Epoch: 250 [205056/225000 (91%)] Loss: 6239.462891\n",
      "Train Epoch: 250 [209152/225000 (93%)] Loss: 6448.476562\n",
      "Train Epoch: 250 [213248/225000 (95%)] Loss: 6378.753906\n",
      "Train Epoch: 250 [217344/225000 (97%)] Loss: 6470.228516\n",
      "Train Epoch: 250 [221440/225000 (98%)] Loss: 6418.611328\n",
      "    epoch          : 250\n",
      "    loss           : 6413.750345518701\n",
      "    val_loss       : 6401.2500073252895\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0815_160624/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [256/225000 (0%)] Loss: 6264.478516\n",
      "Train Epoch: 251 [4352/225000 (2%)] Loss: 6294.931641\n",
      "Train Epoch: 251 [8448/225000 (4%)] Loss: 6307.421875\n",
      "Train Epoch: 251 [12544/225000 (6%)] Loss: 6319.632812\n",
      "Train Epoch: 251 [16640/225000 (7%)] Loss: 6384.332031\n",
      "Train Epoch: 251 [20736/225000 (9%)] Loss: 6436.244141\n",
      "Train Epoch: 251 [24832/225000 (11%)] Loss: 6430.519531\n",
      "Train Epoch: 251 [28928/225000 (13%)] Loss: 6310.082031\n",
      "Train Epoch: 251 [33024/225000 (15%)] Loss: 6244.251953\n",
      "Train Epoch: 251 [37120/225000 (16%)] Loss: 6409.169922\n",
      "Train Epoch: 251 [41216/225000 (18%)] Loss: 6291.943359\n",
      "Train Epoch: 251 [45312/225000 (20%)] Loss: 6437.222656\n",
      "Train Epoch: 251 [49408/225000 (22%)] Loss: 6364.298828\n",
      "Train Epoch: 251 [53504/225000 (24%)] Loss: 6333.402344\n",
      "Train Epoch: 251 [57600/225000 (26%)] Loss: 6240.316406\n",
      "Train Epoch: 251 [61696/225000 (27%)] Loss: 6210.552734\n",
      "Train Epoch: 251 [65792/225000 (29%)] Loss: 6419.894531\n",
      "Train Epoch: 251 [69888/225000 (31%)] Loss: 6351.800781\n",
      "Train Epoch: 251 [73984/225000 (33%)] Loss: 6331.560547\n",
      "Train Epoch: 251 [78080/225000 (35%)] Loss: 6232.792969\n",
      "Train Epoch: 251 [82176/225000 (37%)] Loss: 6306.876953\n",
      "Train Epoch: 251 [86272/225000 (38%)] Loss: 6374.144531\n",
      "Train Epoch: 251 [90368/225000 (40%)] Loss: 6505.046875\n",
      "Train Epoch: 251 [94464/225000 (42%)] Loss: 6280.796875\n",
      "Train Epoch: 251 [98560/225000 (44%)] Loss: 6386.705078\n",
      "Train Epoch: 251 [102656/225000 (46%)] Loss: 6459.542969\n",
      "Train Epoch: 251 [106752/225000 (47%)] Loss: 6361.542969\n",
      "Train Epoch: 251 [110848/225000 (49%)] Loss: 6323.863281\n",
      "Train Epoch: 251 [114944/225000 (51%)] Loss: 6182.314453\n",
      "Train Epoch: 251 [119040/225000 (53%)] Loss: 6387.880859\n",
      "Train Epoch: 251 [123136/225000 (55%)] Loss: 6437.152344\n",
      "Train Epoch: 251 [127232/225000 (57%)] Loss: 6415.638672\n",
      "Train Epoch: 251 [131328/225000 (58%)] Loss: 6247.035156\n",
      "Train Epoch: 251 [135424/225000 (60%)] Loss: 6317.593750\n",
      "Train Epoch: 251 [139520/225000 (62%)] Loss: 6242.894531\n",
      "Train Epoch: 251 [143616/225000 (64%)] Loss: 6369.804688\n",
      "Train Epoch: 251 [147712/225000 (66%)] Loss: 6446.650391\n",
      "Train Epoch: 251 [151808/225000 (67%)] Loss: 6283.361328\n",
      "Train Epoch: 251 [155904/225000 (69%)] Loss: 6443.054688\n",
      "Train Epoch: 251 [160000/225000 (71%)] Loss: 6313.544922\n",
      "Train Epoch: 251 [164096/225000 (73%)] Loss: 6383.875000\n",
      "Train Epoch: 251 [168192/225000 (75%)] Loss: 6302.839844\n",
      "Train Epoch: 251 [172288/225000 (77%)] Loss: 6245.990234\n",
      "Train Epoch: 251 [176384/225000 (78%)] Loss: 6302.140625\n",
      "Train Epoch: 251 [180480/225000 (80%)] Loss: 6380.750000\n",
      "Train Epoch: 251 [184576/225000 (82%)] Loss: 6437.832031\n",
      "Train Epoch: 251 [188672/225000 (84%)] Loss: 6153.605469\n",
      "Train Epoch: 251 [192768/225000 (86%)] Loss: 6504.783203\n",
      "Train Epoch: 251 [196864/225000 (87%)] Loss: 6327.587891\n",
      "Train Epoch: 251 [200960/225000 (89%)] Loss: 6388.087891\n",
      "Train Epoch: 251 [205056/225000 (91%)] Loss: 6366.333984\n",
      "Train Epoch: 251 [209152/225000 (93%)] Loss: 6243.484375\n",
      "Train Epoch: 251 [213248/225000 (95%)] Loss: 6294.925781\n",
      "Train Epoch: 251 [217344/225000 (97%)] Loss: 6320.591797\n",
      "Train Epoch: 251 [221440/225000 (98%)] Loss: 6275.146484\n",
      "    epoch          : 251\n",
      "    loss           : 6416.668294270833\n",
      "    val_loss       : 6608.688030180882\n",
      "Train Epoch: 252 [256/225000 (0%)] Loss: 6244.033203\n",
      "Train Epoch: 252 [4352/225000 (2%)] Loss: 6380.726562\n",
      "Train Epoch: 252 [8448/225000 (4%)] Loss: 6314.882812\n",
      "Train Epoch: 252 [12544/225000 (6%)] Loss: 6345.080078\n",
      "Train Epoch: 252 [16640/225000 (7%)] Loss: 6315.126953\n",
      "Train Epoch: 252 [20736/225000 (9%)] Loss: 6412.757812\n",
      "Train Epoch: 252 [24832/225000 (11%)] Loss: 6287.423828\n",
      "Train Epoch: 252 [28928/225000 (13%)] Loss: 6328.353516\n",
      "Train Epoch: 252 [33024/225000 (15%)] Loss: 6288.560547\n",
      "Train Epoch: 252 [37120/225000 (16%)] Loss: 6355.519531\n",
      "Train Epoch: 252 [41216/225000 (18%)] Loss: 6258.734375\n",
      "Train Epoch: 252 [45312/225000 (20%)] Loss: 6176.525391\n",
      "Train Epoch: 252 [49408/225000 (22%)] Loss: 6235.208984\n",
      "Train Epoch: 252 [53504/225000 (24%)] Loss: 6220.080078\n",
      "Train Epoch: 252 [57600/225000 (26%)] Loss: 6388.400391\n",
      "Train Epoch: 252 [61696/225000 (27%)] Loss: 6417.615234\n",
      "Train Epoch: 252 [65792/225000 (29%)] Loss: 6358.744141\n",
      "Train Epoch: 252 [69888/225000 (31%)] Loss: 6191.468750\n",
      "Train Epoch: 252 [73984/225000 (33%)] Loss: 6402.271484\n",
      "Train Epoch: 252 [78080/225000 (35%)] Loss: 6268.447266\n",
      "Train Epoch: 252 [82176/225000 (37%)] Loss: 6380.462891\n",
      "Train Epoch: 252 [86272/225000 (38%)] Loss: 6304.718750\n",
      "Train Epoch: 252 [90368/225000 (40%)] Loss: 6197.853516\n",
      "Train Epoch: 252 [94464/225000 (42%)] Loss: 6349.232422\n",
      "Train Epoch: 252 [98560/225000 (44%)] Loss: 6477.121094\n",
      "Train Epoch: 252 [102656/225000 (46%)] Loss: 6395.359375\n",
      "Train Epoch: 252 [106752/225000 (47%)] Loss: 6318.837891\n",
      "Train Epoch: 252 [110848/225000 (49%)] Loss: 6336.328125\n",
      "Train Epoch: 252 [114944/225000 (51%)] Loss: 6390.892578\n",
      "Train Epoch: 252 [119040/225000 (53%)] Loss: 6470.164062\n",
      "Train Epoch: 252 [123136/225000 (55%)] Loss: 6188.742188\n",
      "Train Epoch: 252 [127232/225000 (57%)] Loss: 6357.312500\n",
      "Train Epoch: 252 [131328/225000 (58%)] Loss: 6345.556641\n",
      "Train Epoch: 252 [135424/225000 (60%)] Loss: 6333.611328\n",
      "Train Epoch: 252 [139520/225000 (62%)] Loss: 6340.787109\n",
      "Train Epoch: 252 [143616/225000 (64%)] Loss: 6383.103516\n",
      "Train Epoch: 252 [147712/225000 (66%)] Loss: 6249.433594\n",
      "Train Epoch: 252 [151808/225000 (67%)] Loss: 6460.587891\n",
      "Train Epoch: 252 [155904/225000 (69%)] Loss: 6296.843750\n",
      "Train Epoch: 252 [160000/225000 (71%)] Loss: 6306.531250\n",
      "Train Epoch: 252 [164096/225000 (73%)] Loss: 6456.939453\n",
      "Train Epoch: 252 [168192/225000 (75%)] Loss: 6304.732422\n",
      "Train Epoch: 252 [172288/225000 (77%)] Loss: 6395.804688\n",
      "Train Epoch: 252 [176384/225000 (78%)] Loss: 6389.175781\n",
      "Train Epoch: 252 [180480/225000 (80%)] Loss: 6502.871094\n",
      "Train Epoch: 252 [184576/225000 (82%)] Loss: 8285.964844\n",
      "Train Epoch: 252 [188672/225000 (84%)] Loss: 6428.404297\n",
      "Train Epoch: 252 [192768/225000 (86%)] Loss: 6246.841797\n",
      "Train Epoch: 252 [196864/225000 (87%)] Loss: 6218.544922\n",
      "Train Epoch: 252 [200960/225000 (89%)] Loss: 6285.136719\n",
      "Train Epoch: 252 [205056/225000 (91%)] Loss: 6330.957031\n",
      "Train Epoch: 252 [209152/225000 (93%)] Loss: 6487.982422\n",
      "Train Epoch: 252 [213248/225000 (95%)] Loss: 6368.406250\n",
      "Train Epoch: 252 [217344/225000 (97%)] Loss: 6360.623047\n",
      "Train Epoch: 252 [221440/225000 (98%)] Loss: 6278.246094\n",
      "    epoch          : 252\n",
      "    loss           : 6382.700351962458\n",
      "    val_loss       : 6401.182049960506\n",
      "Train Epoch: 253 [256/225000 (0%)] Loss: 6395.087891\n",
      "Train Epoch: 253 [4352/225000 (2%)] Loss: 6317.005859\n",
      "Train Epoch: 253 [8448/225000 (4%)] Loss: 6354.314453\n",
      "Train Epoch: 253 [12544/225000 (6%)] Loss: 6346.591797\n",
      "Train Epoch: 253 [16640/225000 (7%)] Loss: 6404.683594\n",
      "Train Epoch: 253 [20736/225000 (9%)] Loss: 6405.035156\n",
      "Train Epoch: 253 [24832/225000 (11%)] Loss: 6285.617188\n",
      "Train Epoch: 253 [28928/225000 (13%)] Loss: 6296.814453\n",
      "Train Epoch: 253 [33024/225000 (15%)] Loss: 26716.771484\n",
      "Train Epoch: 253 [37120/225000 (16%)] Loss: 6279.830078\n",
      "Train Epoch: 253 [41216/225000 (18%)] Loss: 6250.191406\n",
      "Train Epoch: 253 [45312/225000 (20%)] Loss: 6375.066406\n",
      "Train Epoch: 253 [49408/225000 (22%)] Loss: 6384.792969\n",
      "Train Epoch: 253 [53504/225000 (24%)] Loss: 6288.080078\n",
      "Train Epoch: 253 [57600/225000 (26%)] Loss: 6260.226562\n",
      "Train Epoch: 253 [61696/225000 (27%)] Loss: 6258.148438\n",
      "Train Epoch: 253 [65792/225000 (29%)] Loss: 6524.029297\n",
      "Train Epoch: 253 [69888/225000 (31%)] Loss: 6306.029297\n",
      "Train Epoch: 253 [73984/225000 (33%)] Loss: 6415.810547\n",
      "Train Epoch: 253 [78080/225000 (35%)] Loss: 6327.724609\n",
      "Train Epoch: 253 [82176/225000 (37%)] Loss: 6348.439453\n",
      "Train Epoch: 253 [86272/225000 (38%)] Loss: 6171.001953\n",
      "Train Epoch: 253 [90368/225000 (40%)] Loss: 6307.324219\n",
      "Train Epoch: 253 [94464/225000 (42%)] Loss: 6290.564453\n",
      "Train Epoch: 253 [98560/225000 (44%)] Loss: 6336.541016\n",
      "Train Epoch: 253 [102656/225000 (46%)] Loss: 6380.345703\n",
      "Train Epoch: 253 [106752/225000 (47%)] Loss: 6270.503906\n",
      "Train Epoch: 253 [110848/225000 (49%)] Loss: 6350.005859\n",
      "Train Epoch: 253 [114944/225000 (51%)] Loss: 6307.904297\n",
      "Train Epoch: 253 [119040/225000 (53%)] Loss: 6237.244141\n",
      "Train Epoch: 253 [123136/225000 (55%)] Loss: 6289.587891\n",
      "Train Epoch: 253 [127232/225000 (57%)] Loss: 6252.832031\n",
      "Train Epoch: 253 [131328/225000 (58%)] Loss: 6253.509766\n",
      "Train Epoch: 253 [135424/225000 (60%)] Loss: 6343.703125\n",
      "Train Epoch: 253 [139520/225000 (62%)] Loss: 6308.939453\n",
      "Train Epoch: 253 [143616/225000 (64%)] Loss: 6505.437500\n",
      "Train Epoch: 253 [147712/225000 (66%)] Loss: 6328.658203\n",
      "Train Epoch: 253 [151808/225000 (67%)] Loss: 6429.757812\n",
      "Train Epoch: 253 [155904/225000 (69%)] Loss: 6229.671875\n",
      "Train Epoch: 253 [160000/225000 (71%)] Loss: 6377.470703\n",
      "Train Epoch: 253 [164096/225000 (73%)] Loss: 6215.894531\n",
      "Train Epoch: 253 [168192/225000 (75%)] Loss: 6294.593750\n",
      "Train Epoch: 253 [172288/225000 (77%)] Loss: 6205.396484\n",
      "Train Epoch: 253 [176384/225000 (78%)] Loss: 6332.222656\n",
      "Train Epoch: 253 [180480/225000 (80%)] Loss: 6290.013672\n",
      "Train Epoch: 253 [184576/225000 (82%)] Loss: 6311.029297\n",
      "Train Epoch: 253 [188672/225000 (84%)] Loss: 6395.869141\n",
      "Train Epoch: 253 [192768/225000 (86%)] Loss: 6266.750000\n",
      "Train Epoch: 253 [196864/225000 (87%)] Loss: 6235.314453\n",
      "Train Epoch: 253 [200960/225000 (89%)] Loss: 6355.767578\n",
      "Train Epoch: 253 [205056/225000 (91%)] Loss: 6460.005859\n",
      "Train Epoch: 253 [209152/225000 (93%)] Loss: 6296.136719\n",
      "Train Epoch: 253 [213248/225000 (95%)] Loss: 6354.710938\n",
      "Train Epoch: 253 [217344/225000 (97%)] Loss: 6254.650391\n",
      "Train Epoch: 253 [221440/225000 (98%)] Loss: 6460.097656\n",
      "    epoch          : 253\n",
      "    loss           : 6388.182756061576\n",
      "    val_loss       : 6525.164198107866\n",
      "Train Epoch: 254 [256/225000 (0%)] Loss: 6235.878906\n",
      "Train Epoch: 254 [4352/225000 (2%)] Loss: 6266.738281\n",
      "Train Epoch: 254 [8448/225000 (4%)] Loss: 6406.505859\n",
      "Train Epoch: 254 [12544/225000 (6%)] Loss: 6393.199219\n",
      "Train Epoch: 254 [16640/225000 (7%)] Loss: 6252.480469\n",
      "Train Epoch: 254 [20736/225000 (9%)] Loss: 6316.257812\n",
      "Train Epoch: 254 [24832/225000 (11%)] Loss: 6285.181641\n",
      "Train Epoch: 254 [28928/225000 (13%)] Loss: 6255.224609\n",
      "Train Epoch: 254 [33024/225000 (15%)] Loss: 8248.027344\n",
      "Train Epoch: 254 [37120/225000 (16%)] Loss: 6339.367188\n",
      "Train Epoch: 254 [41216/225000 (18%)] Loss: 6345.962891\n",
      "Train Epoch: 254 [45312/225000 (20%)] Loss: 6413.722656\n",
      "Train Epoch: 254 [49408/225000 (22%)] Loss: 6361.962891\n",
      "Train Epoch: 254 [53504/225000 (24%)] Loss: 6187.361328\n",
      "Train Epoch: 254 [57600/225000 (26%)] Loss: 6335.234375\n",
      "Train Epoch: 254 [61696/225000 (27%)] Loss: 6324.859375\n",
      "Train Epoch: 254 [65792/225000 (29%)] Loss: 6276.816406\n",
      "Train Epoch: 254 [69888/225000 (31%)] Loss: 6435.062500\n",
      "Train Epoch: 254 [73984/225000 (33%)] Loss: 6213.853516\n",
      "Train Epoch: 254 [78080/225000 (35%)] Loss: 6368.251953\n",
      "Train Epoch: 254 [82176/225000 (37%)] Loss: 6384.216797\n",
      "Train Epoch: 254 [86272/225000 (38%)] Loss: 8286.656250\n",
      "Train Epoch: 254 [90368/225000 (40%)] Loss: 6193.187500\n",
      "Train Epoch: 254 [94464/225000 (42%)] Loss: 6318.080078\n",
      "Train Epoch: 254 [98560/225000 (44%)] Loss: 6386.798828\n",
      "Train Epoch: 254 [102656/225000 (46%)] Loss: 6394.125000\n",
      "Train Epoch: 254 [106752/225000 (47%)] Loss: 6319.324219\n",
      "Train Epoch: 254 [110848/225000 (49%)] Loss: 6278.218750\n",
      "Train Epoch: 254 [114944/225000 (51%)] Loss: 6350.746094\n",
      "Train Epoch: 254 [119040/225000 (53%)] Loss: 6380.587891\n",
      "Train Epoch: 254 [123136/225000 (55%)] Loss: 6312.798828\n",
      "Train Epoch: 254 [127232/225000 (57%)] Loss: 6193.191406\n",
      "Train Epoch: 254 [131328/225000 (58%)] Loss: 6378.919922\n",
      "Train Epoch: 254 [135424/225000 (60%)] Loss: 6287.720703\n",
      "Train Epoch: 254 [139520/225000 (62%)] Loss: 6279.380859\n",
      "Train Epoch: 254 [143616/225000 (64%)] Loss: 6336.144531\n",
      "Train Epoch: 254 [147712/225000 (66%)] Loss: 6461.783203\n",
      "Train Epoch: 254 [151808/225000 (67%)] Loss: 6446.839844\n",
      "Train Epoch: 254 [155904/225000 (69%)] Loss: 6323.445312\n",
      "Train Epoch: 254 [160000/225000 (71%)] Loss: 6271.623047\n",
      "Train Epoch: 254 [164096/225000 (73%)] Loss: 6384.951172\n",
      "Train Epoch: 254 [168192/225000 (75%)] Loss: 6376.082031\n",
      "Train Epoch: 254 [172288/225000 (77%)] Loss: 6317.498047\n",
      "Train Epoch: 254 [176384/225000 (78%)] Loss: 6406.003906\n",
      "Train Epoch: 254 [180480/225000 (80%)] Loss: 6256.710938\n",
      "Train Epoch: 254 [184576/225000 (82%)] Loss: 6396.843750\n",
      "Train Epoch: 254 [188672/225000 (84%)] Loss: 6395.611328\n",
      "Train Epoch: 254 [192768/225000 (86%)] Loss: 6421.429688\n",
      "Train Epoch: 254 [196864/225000 (87%)] Loss: 6394.250000\n",
      "Train Epoch: 254 [200960/225000 (89%)] Loss: 6409.761719\n",
      "Train Epoch: 254 [205056/225000 (91%)] Loss: 6393.677734\n",
      "Train Epoch: 254 [209152/225000 (93%)] Loss: 6388.496094\n",
      "Train Epoch: 254 [213248/225000 (95%)] Loss: 6301.958984\n",
      "Train Epoch: 254 [217344/225000 (97%)] Loss: 6246.619141\n",
      "Train Epoch: 254 [221440/225000 (98%)] Loss: 6247.806641\n",
      "    epoch          : 254\n",
      "    loss           : 6404.318790440131\n",
      "    val_loss       : 6457.098118333184\n",
      "Train Epoch: 255 [256/225000 (0%)] Loss: 6371.234375\n",
      "Train Epoch: 255 [4352/225000 (2%)] Loss: 6299.037109\n",
      "Train Epoch: 255 [8448/225000 (4%)] Loss: 6325.750000\n",
      "Train Epoch: 255 [12544/225000 (6%)] Loss: 6365.525391\n",
      "Train Epoch: 255 [16640/225000 (7%)] Loss: 6324.050781\n",
      "Train Epoch: 255 [20736/225000 (9%)] Loss: 6276.380859\n",
      "Train Epoch: 255 [24832/225000 (11%)] Loss: 6254.283203\n",
      "Train Epoch: 255 [28928/225000 (13%)] Loss: 6275.966797\n",
      "Train Epoch: 255 [33024/225000 (15%)] Loss: 6303.339844\n",
      "Train Epoch: 255 [37120/225000 (16%)] Loss: 6210.375000\n",
      "Train Epoch: 255 [41216/225000 (18%)] Loss: 6458.152344\n",
      "Train Epoch: 255 [45312/225000 (20%)] Loss: 6302.916016\n",
      "Train Epoch: 255 [49408/225000 (22%)] Loss: 6539.144531\n",
      "Train Epoch: 255 [53504/225000 (24%)] Loss: 6384.509766\n",
      "Train Epoch: 255 [57600/225000 (26%)] Loss: 6354.763672\n",
      "Train Epoch: 255 [61696/225000 (27%)] Loss: 6308.085938\n",
      "Train Epoch: 255 [65792/225000 (29%)] Loss: 6299.455078\n",
      "Train Epoch: 255 [69888/225000 (31%)] Loss: 6426.160156\n",
      "Train Epoch: 255 [73984/225000 (33%)] Loss: 6182.353516\n",
      "Train Epoch: 255 [78080/225000 (35%)] Loss: 6370.414062\n",
      "Train Epoch: 255 [82176/225000 (37%)] Loss: 6451.939453\n",
      "Train Epoch: 255 [86272/225000 (38%)] Loss: 6322.164062\n",
      "Train Epoch: 255 [90368/225000 (40%)] Loss: 6294.634766\n",
      "Train Epoch: 255 [94464/225000 (42%)] Loss: 6326.503906\n",
      "Train Epoch: 255 [98560/225000 (44%)] Loss: 6326.589844\n",
      "Train Epoch: 255 [102656/225000 (46%)] Loss: 6411.208984\n",
      "Train Epoch: 255 [106752/225000 (47%)] Loss: 6275.847656\n",
      "Train Epoch: 255 [110848/225000 (49%)] Loss: 6343.136719\n",
      "Train Epoch: 255 [114944/225000 (51%)] Loss: 6303.630859\n",
      "Train Epoch: 255 [119040/225000 (53%)] Loss: 6326.474609\n",
      "Train Epoch: 255 [123136/225000 (55%)] Loss: 6300.882812\n",
      "Train Epoch: 255 [127232/225000 (57%)] Loss: 6428.691406\n",
      "Train Epoch: 255 [131328/225000 (58%)] Loss: 6328.058594\n",
      "Train Epoch: 255 [135424/225000 (60%)] Loss: 6159.632812\n",
      "Train Epoch: 255 [139520/225000 (62%)] Loss: 6257.076172\n",
      "Train Epoch: 255 [143616/225000 (64%)] Loss: 6477.318359\n",
      "Train Epoch: 255 [147712/225000 (66%)] Loss: 6356.708984\n",
      "Train Epoch: 255 [151808/225000 (67%)] Loss: 6285.892578\n",
      "Train Epoch: 255 [155904/225000 (69%)] Loss: 6420.455078\n",
      "Train Epoch: 255 [160000/225000 (71%)] Loss: 6120.738281\n",
      "Train Epoch: 255 [164096/225000 (73%)] Loss: 6219.152344\n",
      "Train Epoch: 255 [168192/225000 (75%)] Loss: 6300.376953\n",
      "Train Epoch: 255 [172288/225000 (77%)] Loss: 6330.966797\n",
      "Train Epoch: 255 [176384/225000 (78%)] Loss: 6250.910156\n",
      "Train Epoch: 255 [180480/225000 (80%)] Loss: 6331.400391\n",
      "Train Epoch: 255 [184576/225000 (82%)] Loss: 6206.410156\n",
      "Train Epoch: 255 [188672/225000 (84%)] Loss: 6491.804688\n",
      "Train Epoch: 255 [192768/225000 (86%)] Loss: 6217.748047\n",
      "Train Epoch: 255 [196864/225000 (87%)] Loss: 6340.771484\n",
      "Train Epoch: 255 [200960/225000 (89%)] Loss: 6457.830078\n",
      "Train Epoch: 255 [205056/225000 (91%)] Loss: 6401.414062\n",
      "Train Epoch: 255 [209152/225000 (93%)] Loss: 6237.273438\n",
      "Train Epoch: 255 [213248/225000 (95%)] Loss: 6272.216797\n",
      "Train Epoch: 255 [217344/225000 (97%)] Loss: 6323.199219\n",
      "Train Epoch: 255 [221440/225000 (98%)] Loss: 6241.236328\n",
      "    epoch          : 255\n",
      "    loss           : 6367.325336408561\n",
      "    val_loss       : 6401.158356456124\n",
      "Train Epoch: 256 [256/225000 (0%)] Loss: 6435.929688\n",
      "Train Epoch: 256 [4352/225000 (2%)] Loss: 6413.757812\n",
      "Train Epoch: 256 [8448/225000 (4%)] Loss: 6333.513672\n",
      "Train Epoch: 256 [12544/225000 (6%)] Loss: 6286.158203\n",
      "Train Epoch: 256 [16640/225000 (7%)] Loss: 6283.601562\n",
      "Train Epoch: 256 [20736/225000 (9%)] Loss: 6454.166016\n",
      "Train Epoch: 256 [24832/225000 (11%)] Loss: 6286.222656\n",
      "Train Epoch: 256 [28928/225000 (13%)] Loss: 6084.958984\n",
      "Train Epoch: 256 [33024/225000 (15%)] Loss: 6583.064453\n",
      "Train Epoch: 256 [37120/225000 (16%)] Loss: 6299.738281\n",
      "Train Epoch: 256 [41216/225000 (18%)] Loss: 6265.679688\n",
      "Train Epoch: 256 [45312/225000 (20%)] Loss: 6166.873047\n",
      "Train Epoch: 256 [49408/225000 (22%)] Loss: 6260.767578\n",
      "Train Epoch: 256 [53504/225000 (24%)] Loss: 6474.353516\n",
      "Train Epoch: 256 [57600/225000 (26%)] Loss: 6323.525391\n",
      "Train Epoch: 256 [61696/225000 (27%)] Loss: 6503.425781\n",
      "Train Epoch: 256 [65792/225000 (29%)] Loss: 6277.548828\n",
      "Train Epoch: 256 [69888/225000 (31%)] Loss: 6289.701172\n",
      "Train Epoch: 256 [73984/225000 (33%)] Loss: 6384.917969\n",
      "Train Epoch: 256 [78080/225000 (35%)] Loss: 6275.986328\n",
      "Train Epoch: 256 [82176/225000 (37%)] Loss: 6347.437500\n",
      "Train Epoch: 256 [86272/225000 (38%)] Loss: 6273.001953\n",
      "Train Epoch: 256 [90368/225000 (40%)] Loss: 6504.373047\n",
      "Train Epoch: 256 [94464/225000 (42%)] Loss: 6309.027344\n",
      "Train Epoch: 256 [98560/225000 (44%)] Loss: 6272.453125\n",
      "Train Epoch: 256 [102656/225000 (46%)] Loss: 6359.914062\n",
      "Train Epoch: 256 [106752/225000 (47%)] Loss: 6358.976562\n",
      "Train Epoch: 256 [110848/225000 (49%)] Loss: 6369.625000\n",
      "Train Epoch: 256 [114944/225000 (51%)] Loss: 6208.820312\n",
      "Train Epoch: 256 [119040/225000 (53%)] Loss: 6331.632812\n",
      "Train Epoch: 256 [123136/225000 (55%)] Loss: 6302.703125\n",
      "Train Epoch: 256 [127232/225000 (57%)] Loss: 6266.427734\n",
      "Train Epoch: 256 [131328/225000 (58%)] Loss: 6227.611328\n",
      "Train Epoch: 256 [135424/225000 (60%)] Loss: 6348.890625\n",
      "Train Epoch: 256 [139520/225000 (62%)] Loss: 6497.820312\n",
      "Train Epoch: 256 [143616/225000 (64%)] Loss: 6358.072266\n",
      "Train Epoch: 256 [147712/225000 (66%)] Loss: 6402.582031\n",
      "Train Epoch: 256 [151808/225000 (67%)] Loss: 6334.806641\n",
      "Train Epoch: 256 [155904/225000 (69%)] Loss: 6423.351562\n",
      "Train Epoch: 256 [160000/225000 (71%)] Loss: 6189.531250\n",
      "Train Epoch: 256 [164096/225000 (73%)] Loss: 7979.625000\n",
      "Train Epoch: 256 [168192/225000 (75%)] Loss: 6356.625000\n",
      "Train Epoch: 256 [172288/225000 (77%)] Loss: 6382.312500\n",
      "Train Epoch: 256 [176384/225000 (78%)] Loss: 6281.431641\n",
      "Train Epoch: 256 [180480/225000 (80%)] Loss: 6263.074219\n",
      "Train Epoch: 256 [184576/225000 (82%)] Loss: 6275.173828\n",
      "Train Epoch: 256 [188672/225000 (84%)] Loss: 6232.902344\n",
      "Train Epoch: 256 [192768/225000 (86%)] Loss: 6434.394531\n",
      "Train Epoch: 256 [196864/225000 (87%)] Loss: 6317.720703\n",
      "Train Epoch: 256 [200960/225000 (89%)] Loss: 6200.615234\n",
      "Train Epoch: 256 [205056/225000 (91%)] Loss: 6374.347656\n",
      "Train Epoch: 256 [209152/225000 (93%)] Loss: 6235.671875\n",
      "Train Epoch: 256 [213248/225000 (95%)] Loss: 6346.037109\n",
      "Train Epoch: 256 [217344/225000 (97%)] Loss: 6291.777344\n",
      "Train Epoch: 256 [221440/225000 (98%)] Loss: 6222.572266\n",
      "    epoch          : 256\n",
      "    loss           : 6350.2699845349825\n",
      "    val_loss       : 6481.738780161556\n",
      "Train Epoch: 257 [256/225000 (0%)] Loss: 6294.349609\n",
      "Train Epoch: 257 [4352/225000 (2%)] Loss: 6326.357422\n",
      "Train Epoch: 257 [8448/225000 (4%)] Loss: 6392.386719\n",
      "Train Epoch: 257 [12544/225000 (6%)] Loss: 6357.480469\n",
      "Train Epoch: 257 [16640/225000 (7%)] Loss: 6338.546875\n",
      "Train Epoch: 257 [20736/225000 (9%)] Loss: 6464.476562\n",
      "Train Epoch: 257 [24832/225000 (11%)] Loss: 6268.531250\n",
      "Train Epoch: 257 [28928/225000 (13%)] Loss: 6402.996094\n",
      "Train Epoch: 257 [33024/225000 (15%)] Loss: 6322.033203\n",
      "Train Epoch: 257 [37120/225000 (16%)] Loss: 6354.111328\n",
      "Train Epoch: 257 [41216/225000 (18%)] Loss: 6215.337891\n",
      "Train Epoch: 257 [45312/225000 (20%)] Loss: 6327.976562\n",
      "Train Epoch: 257 [49408/225000 (22%)] Loss: 6291.798828\n",
      "Train Epoch: 257 [53504/225000 (24%)] Loss: 6327.070312\n",
      "Train Epoch: 257 [57600/225000 (26%)] Loss: 6284.193359\n",
      "Train Epoch: 257 [61696/225000 (27%)] Loss: 6405.955078\n",
      "Train Epoch: 257 [65792/225000 (29%)] Loss: 6267.578125\n",
      "Train Epoch: 257 [69888/225000 (31%)] Loss: 6279.673828\n",
      "Train Epoch: 257 [73984/225000 (33%)] Loss: 6325.152344\n",
      "Train Epoch: 257 [78080/225000 (35%)] Loss: 6212.492188\n",
      "Train Epoch: 257 [82176/225000 (37%)] Loss: 6244.517578\n",
      "Train Epoch: 257 [86272/225000 (38%)] Loss: 6218.927734\n",
      "Train Epoch: 257 [90368/225000 (40%)] Loss: 6284.003906\n",
      "Train Epoch: 257 [94464/225000 (42%)] Loss: 6454.912109\n",
      "Train Epoch: 257 [98560/225000 (44%)] Loss: 6322.306641\n",
      "Train Epoch: 257 [102656/225000 (46%)] Loss: 6313.447266\n",
      "Train Epoch: 257 [106752/225000 (47%)] Loss: 6416.220703\n",
      "Train Epoch: 257 [110848/225000 (49%)] Loss: 6329.498047\n",
      "Train Epoch: 257 [114944/225000 (51%)] Loss: 6378.068359\n",
      "Train Epoch: 257 [119040/225000 (53%)] Loss: 6424.458984\n",
      "Train Epoch: 257 [123136/225000 (55%)] Loss: 6261.185547\n",
      "Train Epoch: 257 [127232/225000 (57%)] Loss: 6356.664062\n",
      "Train Epoch: 257 [131328/225000 (58%)] Loss: 6266.490234\n",
      "Train Epoch: 257 [135424/225000 (60%)] Loss: 6334.449219\n",
      "Train Epoch: 257 [139520/225000 (62%)] Loss: 6452.134766\n",
      "Train Epoch: 257 [143616/225000 (64%)] Loss: 6336.558594\n",
      "Train Epoch: 257 [147712/225000 (66%)] Loss: 6369.947266\n",
      "Train Epoch: 257 [151808/225000 (67%)] Loss: 6260.787109\n",
      "Train Epoch: 257 [155904/225000 (69%)] Loss: 6511.212891\n",
      "Train Epoch: 257 [160000/225000 (71%)] Loss: 6327.931641\n",
      "Train Epoch: 257 [164096/225000 (73%)] Loss: 6325.136719\n",
      "Train Epoch: 257 [168192/225000 (75%)] Loss: 6371.875000\n",
      "Train Epoch: 257 [172288/225000 (77%)] Loss: 6480.529297\n",
      "Train Epoch: 257 [176384/225000 (78%)] Loss: 6246.218750\n",
      "Train Epoch: 257 [180480/225000 (80%)] Loss: 6236.802734\n",
      "Train Epoch: 257 [184576/225000 (82%)] Loss: 6211.544922\n",
      "Train Epoch: 257 [188672/225000 (84%)] Loss: 6277.228516\n",
      "Train Epoch: 257 [192768/225000 (86%)] Loss: 6284.783203\n",
      "Train Epoch: 257 [196864/225000 (87%)] Loss: 6307.421875\n",
      "Train Epoch: 257 [200960/225000 (89%)] Loss: 6407.185547\n",
      "Train Epoch: 257 [205056/225000 (91%)] Loss: 6221.628906\n",
      "Train Epoch: 257 [209152/225000 (93%)] Loss: 6200.150391\n",
      "Train Epoch: 257 [213248/225000 (95%)] Loss: 6358.544922\n",
      "Train Epoch: 257 [217344/225000 (97%)] Loss: 6340.228516\n",
      "Train Epoch: 257 [221440/225000 (98%)] Loss: 6361.150391\n",
      "    epoch          : 257\n",
      "    loss           : 6370.8723380617175\n",
      "    val_loss       : 6419.217378927737\n",
      "Train Epoch: 258 [256/225000 (0%)] Loss: 6262.064453\n",
      "Train Epoch: 258 [4352/225000 (2%)] Loss: 6227.423828\n",
      "Train Epoch: 258 [8448/225000 (4%)] Loss: 6369.033203\n",
      "Train Epoch: 258 [12544/225000 (6%)] Loss: 6275.568359\n",
      "Train Epoch: 258 [16640/225000 (7%)] Loss: 6366.578125\n",
      "Train Epoch: 258 [20736/225000 (9%)] Loss: 6251.767578\n",
      "Train Epoch: 258 [24832/225000 (11%)] Loss: 6377.394531\n",
      "Train Epoch: 258 [28928/225000 (13%)] Loss: 6448.474609\n",
      "Train Epoch: 258 [33024/225000 (15%)] Loss: 6415.875000\n",
      "Train Epoch: 258 [37120/225000 (16%)] Loss: 6373.753906\n",
      "Train Epoch: 258 [41216/225000 (18%)] Loss: 6344.457031\n",
      "Train Epoch: 258 [45312/225000 (20%)] Loss: 6248.023438\n",
      "Train Epoch: 258 [49408/225000 (22%)] Loss: 6444.371094\n",
      "Train Epoch: 258 [53504/225000 (24%)] Loss: 6276.400391\n",
      "Train Epoch: 258 [57600/225000 (26%)] Loss: 6275.089844\n",
      "Train Epoch: 258 [61696/225000 (27%)] Loss: 6184.970703\n",
      "Train Epoch: 258 [65792/225000 (29%)] Loss: 6307.132812\n",
      "Train Epoch: 258 [69888/225000 (31%)] Loss: 6279.900391\n",
      "Train Epoch: 258 [73984/225000 (33%)] Loss: 6360.826172\n",
      "Train Epoch: 258 [78080/225000 (35%)] Loss: 6290.820312\n",
      "Train Epoch: 258 [82176/225000 (37%)] Loss: 6405.449219\n",
      "Train Epoch: 258 [86272/225000 (38%)] Loss: 6344.232422\n",
      "Train Epoch: 258 [90368/225000 (40%)] Loss: 6463.078125\n",
      "Train Epoch: 258 [94464/225000 (42%)] Loss: 6287.785156\n",
      "Train Epoch: 258 [98560/225000 (44%)] Loss: 6376.125000\n",
      "Train Epoch: 258 [102656/225000 (46%)] Loss: 6300.330078\n",
      "Train Epoch: 258 [106752/225000 (47%)] Loss: 6239.740234\n",
      "Train Epoch: 258 [110848/225000 (49%)] Loss: 6305.919922\n",
      "Train Epoch: 258 [114944/225000 (51%)] Loss: 6267.994141\n",
      "Train Epoch: 258 [119040/225000 (53%)] Loss: 6366.550781\n",
      "Train Epoch: 258 [123136/225000 (55%)] Loss: 6481.248047\n",
      "Train Epoch: 258 [127232/225000 (57%)] Loss: 6190.958984\n",
      "Train Epoch: 258 [131328/225000 (58%)] Loss: 6391.710938\n",
      "Train Epoch: 258 [135424/225000 (60%)] Loss: 6243.765625\n",
      "Train Epoch: 258 [139520/225000 (62%)] Loss: 6277.917969\n",
      "Train Epoch: 258 [143616/225000 (64%)] Loss: 6337.369141\n",
      "Train Epoch: 258 [147712/225000 (66%)] Loss: 6414.417969\n",
      "Train Epoch: 258 [151808/225000 (67%)] Loss: 6198.509766\n",
      "Train Epoch: 258 [155904/225000 (69%)] Loss: 6423.462891\n",
      "Train Epoch: 258 [160000/225000 (71%)] Loss: 6384.019531\n",
      "Train Epoch: 258 [164096/225000 (73%)] Loss: 6349.988281\n",
      "Train Epoch: 258 [168192/225000 (75%)] Loss: 6264.750000\n",
      "Train Epoch: 258 [172288/225000 (77%)] Loss: 6246.736328\n",
      "Train Epoch: 258 [176384/225000 (78%)] Loss: 6343.033203\n",
      "Train Epoch: 258 [180480/225000 (80%)] Loss: 6370.646484\n",
      "Train Epoch: 258 [184576/225000 (82%)] Loss: 6286.625000\n",
      "Train Epoch: 258 [188672/225000 (84%)] Loss: 6313.263672\n",
      "Train Epoch: 258 [192768/225000 (86%)] Loss: 16784.439453\n",
      "Train Epoch: 258 [196864/225000 (87%)] Loss: 6320.130859\n",
      "Train Epoch: 258 [200960/225000 (89%)] Loss: 6353.427734\n",
      "Train Epoch: 258 [205056/225000 (91%)] Loss: 6321.408203\n",
      "Train Epoch: 258 [209152/225000 (93%)] Loss: 6418.396484\n",
      "Train Epoch: 258 [213248/225000 (95%)] Loss: 6356.085938\n",
      "Train Epoch: 258 [217344/225000 (97%)] Loss: 6318.697266\n",
      "Train Epoch: 258 [221440/225000 (98%)] Loss: 6260.505859\n",
      "    epoch          : 258\n",
      "    loss           : 6361.609714963737\n",
      "    val_loss       : 6419.165701054797\n",
      "Train Epoch: 259 [256/225000 (0%)] Loss: 6208.343750\n",
      "Train Epoch: 259 [4352/225000 (2%)] Loss: 6291.220703\n",
      "Train Epoch: 259 [8448/225000 (4%)] Loss: 6275.140625\n",
      "Train Epoch: 259 [12544/225000 (6%)] Loss: 6405.748047\n",
      "Train Epoch: 259 [16640/225000 (7%)] Loss: 6336.839844\n",
      "Train Epoch: 259 [20736/225000 (9%)] Loss: 6325.835938\n",
      "Train Epoch: 259 [24832/225000 (11%)] Loss: 6248.859375\n",
      "Train Epoch: 259 [28928/225000 (13%)] Loss: 6362.839844\n",
      "Train Epoch: 259 [33024/225000 (15%)] Loss: 6219.726562\n",
      "Train Epoch: 259 [37120/225000 (16%)] Loss: 6299.328125\n",
      "Train Epoch: 259 [41216/225000 (18%)] Loss: 6329.697266\n",
      "Train Epoch: 259 [45312/225000 (20%)] Loss: 6431.259766\n",
      "Train Epoch: 259 [49408/225000 (22%)] Loss: 6244.615234\n",
      "Train Epoch: 259 [53504/225000 (24%)] Loss: 6216.958984\n",
      "Train Epoch: 259 [57600/225000 (26%)] Loss: 6447.324219\n",
      "Train Epoch: 259 [61696/225000 (27%)] Loss: 6199.812500\n",
      "Train Epoch: 259 [65792/225000 (29%)] Loss: 6377.470703\n",
      "Train Epoch: 259 [69888/225000 (31%)] Loss: 6334.128906\n",
      "Train Epoch: 259 [73984/225000 (33%)] Loss: 6256.828125\n",
      "Train Epoch: 259 [78080/225000 (35%)] Loss: 6326.404297\n",
      "Train Epoch: 259 [82176/225000 (37%)] Loss: 6421.640625\n",
      "Train Epoch: 259 [86272/225000 (38%)] Loss: 6243.222656\n",
      "Train Epoch: 259 [90368/225000 (40%)] Loss: 6430.566406\n",
      "Train Epoch: 259 [94464/225000 (42%)] Loss: 6381.705078\n",
      "Train Epoch: 259 [98560/225000 (44%)] Loss: 6262.455078\n",
      "Train Epoch: 259 [102656/225000 (46%)] Loss: 6518.072266\n",
      "Train Epoch: 259 [106752/225000 (47%)] Loss: 6261.814453\n",
      "Train Epoch: 259 [110848/225000 (49%)] Loss: 6339.902344\n",
      "Train Epoch: 259 [114944/225000 (51%)] Loss: 6290.779297\n",
      "Train Epoch: 259 [119040/225000 (53%)] Loss: 6296.302734\n",
      "Train Epoch: 259 [123136/225000 (55%)] Loss: 6410.644531\n",
      "Train Epoch: 259 [127232/225000 (57%)] Loss: 6376.351562\n",
      "Train Epoch: 259 [131328/225000 (58%)] Loss: 6334.314453\n",
      "Train Epoch: 259 [135424/225000 (60%)] Loss: 6271.193359\n",
      "Train Epoch: 259 [139520/225000 (62%)] Loss: 6363.687500\n",
      "Train Epoch: 259 [143616/225000 (64%)] Loss: 6270.212891\n",
      "Train Epoch: 259 [147712/225000 (66%)] Loss: 6307.611328\n",
      "Train Epoch: 259 [151808/225000 (67%)] Loss: 6229.853516\n",
      "Train Epoch: 259 [155904/225000 (69%)] Loss: 6333.246094\n",
      "Train Epoch: 259 [160000/225000 (71%)] Loss: 6266.669922\n",
      "Train Epoch: 259 [164096/225000 (73%)] Loss: 6236.119141\n",
      "Train Epoch: 259 [168192/225000 (75%)] Loss: 6359.158203\n",
      "Train Epoch: 259 [172288/225000 (77%)] Loss: 6355.412109\n",
      "Train Epoch: 259 [176384/225000 (78%)] Loss: 6327.255859\n",
      "Train Epoch: 259 [180480/225000 (80%)] Loss: 6394.429688\n",
      "Train Epoch: 259 [184576/225000 (82%)] Loss: 6351.656250\n",
      "Train Epoch: 259 [188672/225000 (84%)] Loss: 6282.636719\n",
      "Train Epoch: 259 [192768/225000 (86%)] Loss: 6238.701172\n",
      "Train Epoch: 259 [196864/225000 (87%)] Loss: 6267.261719\n",
      "Train Epoch: 259 [200960/225000 (89%)] Loss: 6351.646484\n",
      "Train Epoch: 259 [205056/225000 (91%)] Loss: 6273.402344\n",
      "Train Epoch: 259 [209152/225000 (93%)] Loss: 6229.656250\n",
      "Train Epoch: 259 [213248/225000 (95%)] Loss: 6331.578125\n",
      "Train Epoch: 259 [217344/225000 (97%)] Loss: 6438.361328\n",
      "Train Epoch: 259 [221440/225000 (98%)] Loss: 6424.494141\n",
      "    epoch          : 259\n",
      "    loss           : 6349.840971407494\n",
      "    val_loss       : 6482.496531554631\n",
      "Train Epoch: 260 [256/225000 (0%)] Loss: 6378.845703\n",
      "Train Epoch: 260 [4352/225000 (2%)] Loss: 6415.376953\n",
      "Train Epoch: 260 [8448/225000 (4%)] Loss: 6276.656250\n",
      "Train Epoch: 260 [12544/225000 (6%)] Loss: 6250.261719\n",
      "Train Epoch: 260 [16640/225000 (7%)] Loss: 6401.302734\n",
      "Train Epoch: 260 [20736/225000 (9%)] Loss: 6435.453125\n",
      "Train Epoch: 260 [24832/225000 (11%)] Loss: 6278.109375\n",
      "Train Epoch: 260 [28928/225000 (13%)] Loss: 6303.494141\n",
      "Train Epoch: 260 [33024/225000 (15%)] Loss: 6350.121094\n",
      "Train Epoch: 260 [37120/225000 (16%)] Loss: 6368.134766\n",
      "Train Epoch: 260 [41216/225000 (18%)] Loss: 6419.107422\n",
      "Train Epoch: 260 [45312/225000 (20%)] Loss: 6361.363281\n",
      "Train Epoch: 260 [49408/225000 (22%)] Loss: 6222.330078\n",
      "Train Epoch: 260 [53504/225000 (24%)] Loss: 6427.439453\n",
      "Train Epoch: 260 [57600/225000 (26%)] Loss: 6398.820312\n",
      "Train Epoch: 260 [61696/225000 (27%)] Loss: 6298.457031\n",
      "Train Epoch: 260 [65792/225000 (29%)] Loss: 6304.857422\n",
      "Train Epoch: 260 [69888/225000 (31%)] Loss: 6234.427734\n",
      "Train Epoch: 260 [73984/225000 (33%)] Loss: 6383.017578\n",
      "Train Epoch: 260 [78080/225000 (35%)] Loss: 6074.353516\n",
      "Train Epoch: 260 [82176/225000 (37%)] Loss: 6350.054688\n",
      "Train Epoch: 260 [86272/225000 (38%)] Loss: 6288.496094\n",
      "Train Epoch: 260 [90368/225000 (40%)] Loss: 6247.076172\n",
      "Train Epoch: 260 [94464/225000 (42%)] Loss: 6285.914062\n",
      "Train Epoch: 260 [98560/225000 (44%)] Loss: 6246.054688\n",
      "Train Epoch: 260 [102656/225000 (46%)] Loss: 6288.695312\n",
      "Train Epoch: 260 [106752/225000 (47%)] Loss: 6391.341797\n",
      "Train Epoch: 260 [110848/225000 (49%)] Loss: 6257.083984\n",
      "Train Epoch: 260 [114944/225000 (51%)] Loss: 6517.072266\n",
      "Train Epoch: 260 [119040/225000 (53%)] Loss: 6334.789062\n",
      "Train Epoch: 260 [123136/225000 (55%)] Loss: 6220.996094\n",
      "Train Epoch: 260 [127232/225000 (57%)] Loss: 6403.035156\n",
      "Train Epoch: 260 [131328/225000 (58%)] Loss: 6510.503906\n",
      "Train Epoch: 260 [135424/225000 (60%)] Loss: 6301.437500\n",
      "Train Epoch: 260 [139520/225000 (62%)] Loss: 6289.933594\n",
      "Train Epoch: 260 [143616/225000 (64%)] Loss: 6273.072266\n",
      "Train Epoch: 260 [147712/225000 (66%)] Loss: 6394.501953\n",
      "Train Epoch: 260 [151808/225000 (67%)] Loss: 6426.791016\n",
      "Train Epoch: 260 [155904/225000 (69%)] Loss: 6262.070312\n",
      "Train Epoch: 260 [160000/225000 (71%)] Loss: 6357.679688\n",
      "Train Epoch: 260 [164096/225000 (73%)] Loss: 6332.751953\n",
      "Train Epoch: 260 [168192/225000 (75%)] Loss: 6386.367188\n",
      "Train Epoch: 260 [172288/225000 (77%)] Loss: 6293.851562\n",
      "Train Epoch: 260 [176384/225000 (78%)] Loss: 6277.863281\n",
      "Train Epoch: 260 [180480/225000 (80%)] Loss: 6250.853516\n",
      "Train Epoch: 260 [184576/225000 (82%)] Loss: 6409.591797\n",
      "Train Epoch: 260 [188672/225000 (84%)] Loss: 6302.349609\n",
      "Train Epoch: 260 [192768/225000 (86%)] Loss: 6272.724609\n",
      "Train Epoch: 260 [196864/225000 (87%)] Loss: 6331.476562\n",
      "Train Epoch: 260 [200960/225000 (89%)] Loss: 6270.515625\n",
      "Train Epoch: 260 [205056/225000 (91%)] Loss: 6284.654297\n",
      "Train Epoch: 260 [209152/225000 (93%)] Loss: 6344.058594\n",
      "Train Epoch: 260 [213248/225000 (95%)] Loss: 6424.783203\n",
      "Train Epoch: 260 [217344/225000 (97%)] Loss: 6509.363281\n",
      "Train Epoch: 260 [221440/225000 (98%)] Loss: 6390.884766\n",
      "    epoch          : 260\n",
      "    loss           : 6448.678194325938\n",
      "    val_loss       : 6505.943609280854\n",
      "Train Epoch: 261 [256/225000 (0%)] Loss: 6341.671875\n",
      "Train Epoch: 261 [4352/225000 (2%)] Loss: 6381.625000\n",
      "Train Epoch: 261 [8448/225000 (4%)] Loss: 6293.546875\n",
      "Train Epoch: 261 [12544/225000 (6%)] Loss: 6315.830078\n",
      "Train Epoch: 261 [16640/225000 (7%)] Loss: 6261.375000\n",
      "Train Epoch: 261 [20736/225000 (9%)] Loss: 6395.109375\n",
      "Train Epoch: 261 [24832/225000 (11%)] Loss: 6317.132812\n",
      "Train Epoch: 261 [28928/225000 (13%)] Loss: 6251.878906\n",
      "Train Epoch: 261 [33024/225000 (15%)] Loss: 6300.218750\n",
      "Train Epoch: 261 [37120/225000 (16%)] Loss: 6274.638672\n",
      "Train Epoch: 261 [41216/225000 (18%)] Loss: 6343.818359\n",
      "Train Epoch: 261 [45312/225000 (20%)] Loss: 6351.580078\n",
      "Train Epoch: 261 [49408/225000 (22%)] Loss: 6371.796875\n",
      "Train Epoch: 261 [53504/225000 (24%)] Loss: 6341.240234\n",
      "Train Epoch: 261 [57600/225000 (26%)] Loss: 6223.390625\n",
      "Train Epoch: 261 [61696/225000 (27%)] Loss: 6362.207031\n",
      "Train Epoch: 261 [65792/225000 (29%)] Loss: 6462.216797\n",
      "Train Epoch: 261 [69888/225000 (31%)] Loss: 6398.011719\n",
      "Train Epoch: 261 [73984/225000 (33%)] Loss: 6359.851562\n",
      "Train Epoch: 261 [78080/225000 (35%)] Loss: 6390.277344\n",
      "Train Epoch: 261 [82176/225000 (37%)] Loss: 6268.628906\n",
      "Train Epoch: 261 [86272/225000 (38%)] Loss: 6330.710938\n",
      "Train Epoch: 261 [90368/225000 (40%)] Loss: 6237.248047\n",
      "Train Epoch: 261 [94464/225000 (42%)] Loss: 6370.349609\n",
      "Train Epoch: 261 [98560/225000 (44%)] Loss: 6367.847656\n",
      "Train Epoch: 261 [102656/225000 (46%)] Loss: 6290.041016\n",
      "Train Epoch: 261 [106752/225000 (47%)] Loss: 6422.697266\n",
      "Train Epoch: 261 [110848/225000 (49%)] Loss: 6482.332031\n",
      "Train Epoch: 261 [114944/225000 (51%)] Loss: 6224.154297\n",
      "Train Epoch: 261 [119040/225000 (53%)] Loss: 6450.341797\n",
      "Train Epoch: 261 [123136/225000 (55%)] Loss: 6336.886719\n",
      "Train Epoch: 261 [127232/225000 (57%)] Loss: 6400.378906\n",
      "Train Epoch: 261 [131328/225000 (58%)] Loss: 6313.324219\n",
      "Train Epoch: 261 [135424/225000 (60%)] Loss: 6338.580078\n",
      "Train Epoch: 261 [139520/225000 (62%)] Loss: 6348.058594\n",
      "Train Epoch: 261 [143616/225000 (64%)] Loss: 6334.912109\n",
      "Train Epoch: 261 [147712/225000 (66%)] Loss: 6391.814453\n",
      "Train Epoch: 261 [151808/225000 (67%)] Loss: 6377.539062\n",
      "Train Epoch: 261 [155904/225000 (69%)] Loss: 6166.822266\n",
      "Train Epoch: 261 [160000/225000 (71%)] Loss: 6344.392578\n",
      "Train Epoch: 261 [164096/225000 (73%)] Loss: 6382.615234\n",
      "Train Epoch: 261 [168192/225000 (75%)] Loss: 6360.695312\n",
      "Train Epoch: 261 [172288/225000 (77%)] Loss: 6390.941406\n",
      "Train Epoch: 261 [176384/225000 (78%)] Loss: 6353.460938\n",
      "Train Epoch: 261 [180480/225000 (80%)] Loss: 6267.875000\n",
      "Train Epoch: 261 [184576/225000 (82%)] Loss: 6303.267578\n",
      "Train Epoch: 261 [188672/225000 (84%)] Loss: 6327.875000\n",
      "Train Epoch: 261 [192768/225000 (86%)] Loss: 6404.501953\n",
      "Train Epoch: 261 [196864/225000 (87%)] Loss: 6321.910156\n",
      "Train Epoch: 261 [200960/225000 (89%)] Loss: 6239.851562\n",
      "Train Epoch: 261 [205056/225000 (91%)] Loss: 6338.880859\n",
      "Train Epoch: 261 [209152/225000 (93%)] Loss: 6337.033203\n",
      "Train Epoch: 261 [213248/225000 (95%)] Loss: 6304.066406\n",
      "Train Epoch: 261 [217344/225000 (97%)] Loss: 6288.837891\n",
      "Train Epoch: 261 [221440/225000 (98%)] Loss: 6306.816406\n",
      "    epoch          : 261\n",
      "    loss           : 6402.14606219781\n",
      "    val_loss       : 6419.361965966468\n",
      "Train Epoch: 262 [256/225000 (0%)] Loss: 6394.613281\n",
      "Train Epoch: 262 [4352/225000 (2%)] Loss: 6356.322266\n",
      "Train Epoch: 262 [8448/225000 (4%)] Loss: 6339.730469\n",
      "Train Epoch: 262 [12544/225000 (6%)] Loss: 6128.517578\n",
      "Train Epoch: 262 [16640/225000 (7%)] Loss: 6383.929688\n",
      "Train Epoch: 262 [20736/225000 (9%)] Loss: 6314.363281\n",
      "Train Epoch: 262 [24832/225000 (11%)] Loss: 6441.701172\n",
      "Train Epoch: 262 [28928/225000 (13%)] Loss: 6357.833984\n",
      "Train Epoch: 262 [33024/225000 (15%)] Loss: 6444.003906\n",
      "Train Epoch: 262 [37120/225000 (16%)] Loss: 6364.242188\n",
      "Train Epoch: 262 [41216/225000 (18%)] Loss: 6417.541016\n",
      "Train Epoch: 262 [45312/225000 (20%)] Loss: 6281.078125\n",
      "Train Epoch: 262 [49408/225000 (22%)] Loss: 6361.873047\n",
      "Train Epoch: 262 [53504/225000 (24%)] Loss: 6312.707031\n",
      "Train Epoch: 262 [57600/225000 (26%)] Loss: 6259.083984\n",
      "Train Epoch: 262 [61696/225000 (27%)] Loss: 6375.103516\n",
      "Train Epoch: 262 [65792/225000 (29%)] Loss: 6407.316406\n",
      "Train Epoch: 262 [69888/225000 (31%)] Loss: 6369.025391\n",
      "Train Epoch: 262 [73984/225000 (33%)] Loss: 6354.908203\n",
      "Train Epoch: 262 [78080/225000 (35%)] Loss: 6337.271484\n",
      "Train Epoch: 262 [82176/225000 (37%)] Loss: 6274.287109\n",
      "Train Epoch: 262 [86272/225000 (38%)] Loss: 6330.212891\n",
      "Train Epoch: 262 [90368/225000 (40%)] Loss: 6320.123047\n",
      "Train Epoch: 262 [94464/225000 (42%)] Loss: 6299.962891\n",
      "Train Epoch: 262 [98560/225000 (44%)] Loss: 6405.003906\n",
      "Train Epoch: 262 [102656/225000 (46%)] Loss: 6217.947266\n",
      "Train Epoch: 262 [106752/225000 (47%)] Loss: 6294.250000\n",
      "Train Epoch: 262 [110848/225000 (49%)] Loss: 6270.724609\n",
      "Train Epoch: 262 [114944/225000 (51%)] Loss: 6346.708984\n",
      "Train Epoch: 262 [119040/225000 (53%)] Loss: 6352.138672\n",
      "Train Epoch: 262 [123136/225000 (55%)] Loss: 6295.916016\n",
      "Train Epoch: 262 [127232/225000 (57%)] Loss: 6319.521484\n",
      "Train Epoch: 262 [131328/225000 (58%)] Loss: 6477.619141\n",
      "Train Epoch: 262 [135424/225000 (60%)] Loss: 6441.863281\n",
      "Train Epoch: 262 [139520/225000 (62%)] Loss: 6367.976562\n",
      "Train Epoch: 262 [143616/225000 (64%)] Loss: 6485.802734\n",
      "Train Epoch: 262 [147712/225000 (66%)] Loss: 6219.425781\n",
      "Train Epoch: 262 [151808/225000 (67%)] Loss: 6230.847656\n",
      "Train Epoch: 262 [155904/225000 (69%)] Loss: 6266.283203\n",
      "Train Epoch: 262 [160000/225000 (71%)] Loss: 6404.802734\n",
      "Train Epoch: 262 [164096/225000 (73%)] Loss: 6243.826172\n",
      "Train Epoch: 262 [168192/225000 (75%)] Loss: 6323.751953\n",
      "Train Epoch: 262 [172288/225000 (77%)] Loss: 6367.849609\n",
      "Train Epoch: 262 [176384/225000 (78%)] Loss: 6387.453125\n",
      "Train Epoch: 262 [180480/225000 (80%)] Loss: 6387.083984\n",
      "Train Epoch: 262 [184576/225000 (82%)] Loss: 6364.494141\n",
      "Train Epoch: 262 [188672/225000 (84%)] Loss: 6400.513672\n",
      "Train Epoch: 262 [192768/225000 (86%)] Loss: 6206.537109\n",
      "Train Epoch: 262 [196864/225000 (87%)] Loss: 6277.849609\n",
      "Train Epoch: 262 [200960/225000 (89%)] Loss: 6311.634766\n",
      "Train Epoch: 262 [205056/225000 (91%)] Loss: 6344.273438\n",
      "Train Epoch: 262 [209152/225000 (93%)] Loss: 6490.736328\n",
      "Train Epoch: 262 [213248/225000 (95%)] Loss: 6274.863281\n",
      "Train Epoch: 262 [217344/225000 (97%)] Loss: 6381.212891\n",
      "Train Epoch: 262 [221440/225000 (98%)] Loss: 6313.337891\n",
      "    epoch          : 262\n",
      "    loss           : 6366.151903796928\n",
      "    val_loss       : 6437.3945679068565\n",
      "Train Epoch: 263 [256/225000 (0%)] Loss: 6312.638672\n",
      "Train Epoch: 263 [4352/225000 (2%)] Loss: 6203.830078\n",
      "Train Epoch: 263 [8448/225000 (4%)] Loss: 6369.289062\n",
      "Train Epoch: 263 [12544/225000 (6%)] Loss: 6246.099609\n",
      "Train Epoch: 263 [16640/225000 (7%)] Loss: 6438.591797\n",
      "Train Epoch: 263 [20736/225000 (9%)] Loss: 6359.476562\n",
      "Train Epoch: 263 [24832/225000 (11%)] Loss: 6485.423828\n",
      "Train Epoch: 263 [28928/225000 (13%)] Loss: 6321.000000\n",
      "Train Epoch: 263 [33024/225000 (15%)] Loss: 6296.621094\n",
      "Train Epoch: 263 [37120/225000 (16%)] Loss: 6334.048828\n",
      "Train Epoch: 263 [41216/225000 (18%)] Loss: 6261.814453\n",
      "Train Epoch: 263 [45312/225000 (20%)] Loss: 6278.855469\n",
      "Train Epoch: 263 [49408/225000 (22%)] Loss: 6192.304688\n",
      "Train Epoch: 263 [53504/225000 (24%)] Loss: 6435.384766\n",
      "Train Epoch: 263 [57600/225000 (26%)] Loss: 6319.402344\n",
      "Train Epoch: 263 [61696/225000 (27%)] Loss: 6310.535156\n",
      "Train Epoch: 263 [65792/225000 (29%)] Loss: 6293.439453\n",
      "Train Epoch: 263 [69888/225000 (31%)] Loss: 6468.025391\n",
      "Train Epoch: 263 [73984/225000 (33%)] Loss: 6464.724609\n",
      "Train Epoch: 263 [78080/225000 (35%)] Loss: 6342.830078\n",
      "Train Epoch: 263 [82176/225000 (37%)] Loss: 6393.781250\n",
      "Train Epoch: 263 [86272/225000 (38%)] Loss: 6392.207031\n",
      "Train Epoch: 263 [90368/225000 (40%)] Loss: 6262.886719\n",
      "Train Epoch: 263 [94464/225000 (42%)] Loss: 6254.291016\n",
      "Train Epoch: 263 [98560/225000 (44%)] Loss: 6290.572266\n",
      "Train Epoch: 263 [102656/225000 (46%)] Loss: 6275.248047\n",
      "Train Epoch: 263 [106752/225000 (47%)] Loss: 6216.062500\n",
      "Train Epoch: 263 [110848/225000 (49%)] Loss: 6384.033203\n",
      "Train Epoch: 263 [114944/225000 (51%)] Loss: 6365.625000\n",
      "Train Epoch: 263 [119040/225000 (53%)] Loss: 6326.041016\n",
      "Train Epoch: 263 [123136/225000 (55%)] Loss: 6237.333984\n",
      "Train Epoch: 263 [127232/225000 (57%)] Loss: 6359.982422\n",
      "Train Epoch: 263 [131328/225000 (58%)] Loss: 6287.417969\n",
      "Train Epoch: 263 [135424/225000 (60%)] Loss: 6270.123047\n",
      "Train Epoch: 263 [139520/225000 (62%)] Loss: 6268.802734\n",
      "Train Epoch: 263 [143616/225000 (64%)] Loss: 6290.265625\n",
      "Train Epoch: 263 [147712/225000 (66%)] Loss: 6345.853516\n",
      "Train Epoch: 263 [151808/225000 (67%)] Loss: 6252.298828\n",
      "Train Epoch: 263 [155904/225000 (69%)] Loss: 6310.380859\n",
      "Train Epoch: 263 [160000/225000 (71%)] Loss: 6380.644531\n",
      "Train Epoch: 263 [164096/225000 (73%)] Loss: 6270.498047\n",
      "Train Epoch: 263 [168192/225000 (75%)] Loss: 6415.886719\n",
      "Train Epoch: 263 [172288/225000 (77%)] Loss: 6318.787109\n",
      "Train Epoch: 263 [176384/225000 (78%)] Loss: 6231.156250\n",
      "Train Epoch: 263 [180480/225000 (80%)] Loss: 6287.626953\n",
      "Train Epoch: 263 [184576/225000 (82%)] Loss: 6418.878906\n",
      "Train Epoch: 263 [188672/225000 (84%)] Loss: 6282.748047\n",
      "Train Epoch: 263 [192768/225000 (86%)] Loss: 6441.921875\n",
      "Train Epoch: 263 [196864/225000 (87%)] Loss: 6290.636719\n",
      "Train Epoch: 263 [200960/225000 (89%)] Loss: 6468.501953\n",
      "Train Epoch: 263 [205056/225000 (91%)] Loss: 6288.267578\n",
      "Train Epoch: 263 [209152/225000 (93%)] Loss: 6242.349609\n",
      "Train Epoch: 263 [213248/225000 (95%)] Loss: 6451.554688\n",
      "Train Epoch: 263 [217344/225000 (97%)] Loss: 6333.195312\n",
      "Train Epoch: 263 [221440/225000 (98%)] Loss: 6310.185547\n",
      "    epoch          : 263\n",
      "    loss           : 6356.602262425341\n",
      "    val_loss       : 6419.290367428137\n",
      "Train Epoch: 264 [256/225000 (0%)] Loss: 6370.496094\n",
      "Train Epoch: 264 [4352/225000 (2%)] Loss: 6226.755859\n",
      "Train Epoch: 264 [8448/225000 (4%)] Loss: 6437.457031\n",
      "Train Epoch: 264 [12544/225000 (6%)] Loss: 6287.316406\n",
      "Train Epoch: 264 [16640/225000 (7%)] Loss: 6325.261719\n",
      "Train Epoch: 264 [20736/225000 (9%)] Loss: 6443.875000\n",
      "Train Epoch: 264 [24832/225000 (11%)] Loss: 6193.970703\n",
      "Train Epoch: 264 [28928/225000 (13%)] Loss: 6234.611328\n",
      "Train Epoch: 264 [33024/225000 (15%)] Loss: 6379.511719\n",
      "Train Epoch: 264 [37120/225000 (16%)] Loss: 6308.472656\n",
      "Train Epoch: 264 [41216/225000 (18%)] Loss: 6180.515625\n",
      "Train Epoch: 264 [45312/225000 (20%)] Loss: 6398.757812\n",
      "Train Epoch: 264 [49408/225000 (22%)] Loss: 6430.363281\n",
      "Train Epoch: 264 [53504/225000 (24%)] Loss: 6535.765625\n",
      "Train Epoch: 264 [57600/225000 (26%)] Loss: 6220.664062\n",
      "Train Epoch: 264 [61696/225000 (27%)] Loss: 6413.835938\n",
      "Train Epoch: 264 [65792/225000 (29%)] Loss: 8039.771484\n",
      "Train Epoch: 264 [69888/225000 (31%)] Loss: 6435.914062\n",
      "Train Epoch: 264 [73984/225000 (33%)] Loss: 6276.556641\n",
      "Train Epoch: 264 [78080/225000 (35%)] Loss: 6310.527344\n",
      "Train Epoch: 264 [82176/225000 (37%)] Loss: 6343.054688\n",
      "Train Epoch: 264 [86272/225000 (38%)] Loss: 6414.333984\n",
      "Train Epoch: 264 [90368/225000 (40%)] Loss: 6368.875000\n",
      "Train Epoch: 264 [94464/225000 (42%)] Loss: 6297.140625\n",
      "Train Epoch: 264 [98560/225000 (44%)] Loss: 6348.615234\n",
      "Train Epoch: 264 [102656/225000 (46%)] Loss: 6373.197266\n",
      "Train Epoch: 264 [106752/225000 (47%)] Loss: 6341.986328\n",
      "Train Epoch: 264 [110848/225000 (49%)] Loss: 6272.201172\n",
      "Train Epoch: 264 [114944/225000 (51%)] Loss: 6267.531250\n",
      "Train Epoch: 264 [119040/225000 (53%)] Loss: 6325.642578\n",
      "Train Epoch: 264 [123136/225000 (55%)] Loss: 6333.857422\n",
      "Train Epoch: 264 [127232/225000 (57%)] Loss: 6386.337891\n",
      "Train Epoch: 264 [131328/225000 (58%)] Loss: 6412.271484\n",
      "Train Epoch: 264 [135424/225000 (60%)] Loss: 6259.568359\n",
      "Train Epoch: 264 [139520/225000 (62%)] Loss: 6199.275391\n",
      "Train Epoch: 264 [143616/225000 (64%)] Loss: 6401.240234\n",
      "Train Epoch: 264 [147712/225000 (66%)] Loss: 6291.900391\n",
      "Train Epoch: 264 [151808/225000 (67%)] Loss: 6386.742188\n",
      "Train Epoch: 264 [155904/225000 (69%)] Loss: 6328.513672\n",
      "Train Epoch: 264 [160000/225000 (71%)] Loss: 6410.429688\n",
      "Train Epoch: 264 [164096/225000 (73%)] Loss: 6449.982422\n",
      "Train Epoch: 264 [168192/225000 (75%)] Loss: 6267.417969\n",
      "Train Epoch: 264 [172288/225000 (77%)] Loss: 6330.531250\n",
      "Train Epoch: 264 [176384/225000 (78%)] Loss: 6510.568359\n",
      "Train Epoch: 264 [180480/225000 (80%)] Loss: 6423.488281\n",
      "Train Epoch: 264 [184576/225000 (82%)] Loss: 6299.074219\n",
      "Train Epoch: 264 [188672/225000 (84%)] Loss: 6374.142578\n",
      "Train Epoch: 264 [192768/225000 (86%)] Loss: 6425.820312\n",
      "Train Epoch: 264 [196864/225000 (87%)] Loss: 6298.904297\n",
      "Train Epoch: 264 [200960/225000 (89%)] Loss: 6324.726562\n",
      "Train Epoch: 264 [205056/225000 (91%)] Loss: 6328.419922\n",
      "Train Epoch: 264 [209152/225000 (93%)] Loss: 6465.720703\n",
      "Train Epoch: 264 [213248/225000 (95%)] Loss: 6312.402344\n",
      "Train Epoch: 264 [217344/225000 (97%)] Loss: 6274.617188\n",
      "Train Epoch: 264 [221440/225000 (98%)] Loss: 6299.308594\n",
      "    epoch          : 264\n",
      "    loss           : 6401.695059193686\n",
      "    val_loss       : 6401.206722159775\n",
      "Train Epoch: 265 [256/225000 (0%)] Loss: 6438.212891\n",
      "Train Epoch: 265 [4352/225000 (2%)] Loss: 6311.636719\n",
      "Train Epoch: 265 [8448/225000 (4%)] Loss: 6291.609375\n",
      "Train Epoch: 265 [12544/225000 (6%)] Loss: 6319.736328\n",
      "Train Epoch: 265 [16640/225000 (7%)] Loss: 6240.447266\n",
      "Train Epoch: 265 [20736/225000 (9%)] Loss: 6466.783203\n",
      "Train Epoch: 265 [24832/225000 (11%)] Loss: 6249.675781\n",
      "Train Epoch: 265 [28928/225000 (13%)] Loss: 6333.458984\n",
      "Train Epoch: 265 [33024/225000 (15%)] Loss: 6272.224609\n",
      "Train Epoch: 265 [37120/225000 (16%)] Loss: 6314.548828\n",
      "Train Epoch: 265 [41216/225000 (18%)] Loss: 8207.642578\n",
      "Train Epoch: 265 [45312/225000 (20%)] Loss: 6364.314453\n",
      "Train Epoch: 265 [49408/225000 (22%)] Loss: 6323.701172\n",
      "Train Epoch: 265 [53504/225000 (24%)] Loss: 6273.349609\n",
      "Train Epoch: 265 [57600/225000 (26%)] Loss: 6242.039062\n",
      "Train Epoch: 265 [61696/225000 (27%)] Loss: 6277.087891\n",
      "Train Epoch: 265 [65792/225000 (29%)] Loss: 6337.593750\n",
      "Train Epoch: 265 [69888/225000 (31%)] Loss: 6390.191406\n",
      "Train Epoch: 265 [73984/225000 (33%)] Loss: 6363.308594\n",
      "Train Epoch: 265 [78080/225000 (35%)] Loss: 6263.167969\n",
      "Train Epoch: 265 [82176/225000 (37%)] Loss: 6249.568359\n",
      "Train Epoch: 265 [86272/225000 (38%)] Loss: 6368.359375\n",
      "Train Epoch: 265 [90368/225000 (40%)] Loss: 6227.169922\n",
      "Train Epoch: 265 [94464/225000 (42%)] Loss: 6310.683594\n",
      "Train Epoch: 265 [98560/225000 (44%)] Loss: 6336.802734\n",
      "Train Epoch: 265 [102656/225000 (46%)] Loss: 6478.142578\n",
      "Train Epoch: 265 [106752/225000 (47%)] Loss: 6290.806641\n",
      "Train Epoch: 265 [110848/225000 (49%)] Loss: 6230.232422\n",
      "Train Epoch: 265 [114944/225000 (51%)] Loss: 6337.062500\n",
      "Train Epoch: 265 [119040/225000 (53%)] Loss: 6276.166016\n",
      "Train Epoch: 265 [123136/225000 (55%)] Loss: 6288.005859\n",
      "Train Epoch: 265 [127232/225000 (57%)] Loss: 6383.021484\n",
      "Train Epoch: 265 [131328/225000 (58%)] Loss: 6402.630859\n",
      "Train Epoch: 265 [135424/225000 (60%)] Loss: 6289.125000\n",
      "Train Epoch: 265 [139520/225000 (62%)] Loss: 6386.529297\n",
      "Train Epoch: 265 [143616/225000 (64%)] Loss: 6285.730469\n",
      "Train Epoch: 265 [147712/225000 (66%)] Loss: 6300.595703\n",
      "Train Epoch: 265 [151808/225000 (67%)] Loss: 6445.701172\n",
      "Train Epoch: 265 [155904/225000 (69%)] Loss: 6221.785156\n",
      "Train Epoch: 265 [160000/225000 (71%)] Loss: 6251.025391\n",
      "Train Epoch: 265 [164096/225000 (73%)] Loss: 6391.958984\n",
      "Train Epoch: 265 [168192/225000 (75%)] Loss: 6455.519531\n",
      "Train Epoch: 265 [172288/225000 (77%)] Loss: 6325.326172\n",
      "Train Epoch: 265 [176384/225000 (78%)] Loss: 6241.017578\n",
      "Train Epoch: 265 [180480/225000 (80%)] Loss: 6282.189453\n",
      "Train Epoch: 265 [184576/225000 (82%)] Loss: 6304.759766\n",
      "Train Epoch: 265 [188672/225000 (84%)] Loss: 6416.462891\n",
      "Train Epoch: 265 [192768/225000 (86%)] Loss: 6258.224609\n",
      "Train Epoch: 265 [196864/225000 (87%)] Loss: 6386.667969\n",
      "Train Epoch: 265 [200960/225000 (89%)] Loss: 6398.195312\n",
      "Train Epoch: 265 [205056/225000 (91%)] Loss: 6294.613281\n",
      "Train Epoch: 265 [209152/225000 (93%)] Loss: 6253.527344\n",
      "Train Epoch: 265 [213248/225000 (95%)] Loss: 6389.935547\n",
      "Train Epoch: 265 [217344/225000 (97%)] Loss: 6244.187500\n",
      "Train Epoch: 265 [221440/225000 (98%)] Loss: 6344.117188\n",
      "    epoch          : 265\n",
      "    loss           : 6445.191081840159\n",
      "    val_loss       : 6419.845296177329\n",
      "Train Epoch: 266 [256/225000 (0%)] Loss: 6351.615234\n",
      "Train Epoch: 266 [4352/225000 (2%)] Loss: 6362.183594\n",
      "Train Epoch: 266 [8448/225000 (4%)] Loss: 6187.214844\n",
      "Train Epoch: 266 [12544/225000 (6%)] Loss: 6279.501953\n",
      "Train Epoch: 266 [16640/225000 (7%)] Loss: 6385.875000\n",
      "Train Epoch: 266 [20736/225000 (9%)] Loss: 6356.273438\n",
      "Train Epoch: 266 [24832/225000 (11%)] Loss: 6355.925781\n",
      "Train Epoch: 266 [28928/225000 (13%)] Loss: 6258.857422\n",
      "Train Epoch: 266 [33024/225000 (15%)] Loss: 6257.298828\n",
      "Train Epoch: 266 [37120/225000 (16%)] Loss: 6343.789062\n",
      "Train Epoch: 266 [41216/225000 (18%)] Loss: 6404.833984\n",
      "Train Epoch: 266 [45312/225000 (20%)] Loss: 6289.437500\n",
      "Train Epoch: 266 [49408/225000 (22%)] Loss: 6145.728516\n",
      "Train Epoch: 266 [53504/225000 (24%)] Loss: 6361.892578\n",
      "Train Epoch: 266 [57600/225000 (26%)] Loss: 6329.285156\n",
      "Train Epoch: 266 [61696/225000 (27%)] Loss: 6453.972656\n",
      "Train Epoch: 266 [65792/225000 (29%)] Loss: 6411.375000\n",
      "Train Epoch: 266 [69888/225000 (31%)] Loss: 6456.525391\n",
      "Train Epoch: 266 [73984/225000 (33%)] Loss: 6273.656250\n",
      "Train Epoch: 266 [78080/225000 (35%)] Loss: 6420.464844\n",
      "Train Epoch: 266 [82176/225000 (37%)] Loss: 6405.074219\n",
      "Train Epoch: 266 [86272/225000 (38%)] Loss: 6265.041016\n",
      "Train Epoch: 266 [90368/225000 (40%)] Loss: 6193.613281\n",
      "Train Epoch: 266 [94464/225000 (42%)] Loss: 6187.339844\n",
      "Train Epoch: 266 [98560/225000 (44%)] Loss: 6286.330078\n",
      "Train Epoch: 266 [102656/225000 (46%)] Loss: 6248.632812\n",
      "Train Epoch: 266 [106752/225000 (47%)] Loss: 6293.654297\n",
      "Train Epoch: 266 [110848/225000 (49%)] Loss: 6365.601562\n",
      "Train Epoch: 266 [114944/225000 (51%)] Loss: 6328.888672\n",
      "Train Epoch: 266 [119040/225000 (53%)] Loss: 6283.953125\n",
      "Train Epoch: 266 [123136/225000 (55%)] Loss: 6374.615234\n",
      "Train Epoch: 266 [127232/225000 (57%)] Loss: 6356.941406\n",
      "Train Epoch: 266 [131328/225000 (58%)] Loss: 6321.126953\n",
      "Train Epoch: 266 [135424/225000 (60%)] Loss: 6381.841797\n",
      "Train Epoch: 266 [139520/225000 (62%)] Loss: 6364.693359\n",
      "Train Epoch: 266 [143616/225000 (64%)] Loss: 6431.830078\n",
      "Train Epoch: 266 [147712/225000 (66%)] Loss: 6304.410156\n",
      "Train Epoch: 266 [151808/225000 (67%)] Loss: 6312.595703\n",
      "Train Epoch: 266 [155904/225000 (69%)] Loss: 6340.818359\n",
      "Train Epoch: 266 [160000/225000 (71%)] Loss: 6335.482422\n",
      "Train Epoch: 266 [164096/225000 (73%)] Loss: 6321.681641\n",
      "Train Epoch: 266 [168192/225000 (75%)] Loss: 6305.955078\n",
      "Train Epoch: 266 [172288/225000 (77%)] Loss: 6226.703125\n",
      "Train Epoch: 266 [176384/225000 (78%)] Loss: 6325.898438\n",
      "Train Epoch: 266 [180480/225000 (80%)] Loss: 6427.697266\n",
      "Train Epoch: 266 [184576/225000 (82%)] Loss: 6304.228516\n",
      "Train Epoch: 266 [188672/225000 (84%)] Loss: 6261.164062\n",
      "Train Epoch: 266 [192768/225000 (86%)] Loss: 6254.427734\n",
      "Train Epoch: 266 [196864/225000 (87%)] Loss: 6284.632812\n",
      "Train Epoch: 266 [200960/225000 (89%)] Loss: 6411.904297\n",
      "Train Epoch: 266 [205056/225000 (91%)] Loss: 6336.003906\n",
      "Train Epoch: 266 [209152/225000 (93%)] Loss: 6412.066406\n",
      "Train Epoch: 266 [213248/225000 (95%)] Loss: 6174.716797\n",
      "Train Epoch: 266 [217344/225000 (97%)] Loss: 6318.164062\n",
      "Train Epoch: 266 [221440/225000 (98%)] Loss: 6195.398438\n",
      "    epoch          : 266\n",
      "    loss           : 6349.718261163253\n",
      "    val_loss       : 6419.132438233921\n",
      "Train Epoch: 267 [256/225000 (0%)] Loss: 6331.980469\n",
      "Train Epoch: 267 [4352/225000 (2%)] Loss: 6355.541016\n",
      "Train Epoch: 267 [8448/225000 (4%)] Loss: 6283.130859\n",
      "Train Epoch: 267 [12544/225000 (6%)] Loss: 6345.814453\n",
      "Train Epoch: 267 [16640/225000 (7%)] Loss: 6284.041016\n",
      "Train Epoch: 267 [20736/225000 (9%)] Loss: 6335.710938\n",
      "Train Epoch: 267 [24832/225000 (11%)] Loss: 6281.771484\n",
      "Train Epoch: 267 [28928/225000 (13%)] Loss: 6281.341797\n",
      "Train Epoch: 267 [33024/225000 (15%)] Loss: 6518.988281\n",
      "Train Epoch: 267 [37120/225000 (16%)] Loss: 6424.246094\n",
      "Train Epoch: 267 [41216/225000 (18%)] Loss: 6334.808594\n",
      "Train Epoch: 267 [45312/225000 (20%)] Loss: 6334.111328\n",
      "Train Epoch: 267 [49408/225000 (22%)] Loss: 6349.521484\n",
      "Train Epoch: 267 [53504/225000 (24%)] Loss: 6301.271484\n",
      "Train Epoch: 267 [57600/225000 (26%)] Loss: 6417.117188\n",
      "Train Epoch: 267 [61696/225000 (27%)] Loss: 6383.593750\n",
      "Train Epoch: 267 [65792/225000 (29%)] Loss: 6276.562500\n",
      "Train Epoch: 267 [69888/225000 (31%)] Loss: 6388.589844\n",
      "Train Epoch: 267 [73984/225000 (33%)] Loss: 6347.740234\n",
      "Train Epoch: 267 [78080/225000 (35%)] Loss: 6408.566406\n",
      "Train Epoch: 267 [82176/225000 (37%)] Loss: 6333.669922\n",
      "Train Epoch: 267 [86272/225000 (38%)] Loss: 6302.421875\n",
      "Train Epoch: 267 [90368/225000 (40%)] Loss: 6432.597656\n",
      "Train Epoch: 267 [94464/225000 (42%)] Loss: 6254.195312\n",
      "Train Epoch: 267 [98560/225000 (44%)] Loss: 6380.195312\n",
      "Train Epoch: 267 [102656/225000 (46%)] Loss: 6440.679688\n",
      "Train Epoch: 267 [106752/225000 (47%)] Loss: 6328.779297\n",
      "Train Epoch: 267 [110848/225000 (49%)] Loss: 6338.111328\n",
      "Train Epoch: 267 [114944/225000 (51%)] Loss: 6432.695312\n",
      "Train Epoch: 267 [119040/225000 (53%)] Loss: 6306.769531\n",
      "Train Epoch: 267 [123136/225000 (55%)] Loss: 6299.337891\n",
      "Train Epoch: 267 [127232/225000 (57%)] Loss: 6321.228516\n",
      "Train Epoch: 267 [131328/225000 (58%)] Loss: 6399.925781\n",
      "Train Epoch: 267 [135424/225000 (60%)] Loss: 6318.076172\n",
      "Train Epoch: 267 [139520/225000 (62%)] Loss: 6426.009766\n",
      "Train Epoch: 267 [143616/225000 (64%)] Loss: 6333.486328\n",
      "Train Epoch: 267 [147712/225000 (66%)] Loss: 6450.308594\n",
      "Train Epoch: 267 [151808/225000 (67%)] Loss: 6233.484375\n",
      "Train Epoch: 267 [155904/225000 (69%)] Loss: 6369.248047\n",
      "Train Epoch: 267 [160000/225000 (71%)] Loss: 6388.884766\n",
      "Train Epoch: 267 [164096/225000 (73%)] Loss: 6411.867188\n",
      "Train Epoch: 267 [168192/225000 (75%)] Loss: 6384.261719\n",
      "Train Epoch: 267 [172288/225000 (77%)] Loss: 6265.214844\n",
      "Train Epoch: 267 [176384/225000 (78%)] Loss: 6294.294922\n",
      "Train Epoch: 267 [180480/225000 (80%)] Loss: 6382.164062\n",
      "Train Epoch: 267 [184576/225000 (82%)] Loss: 6407.949219\n",
      "Train Epoch: 267 [188672/225000 (84%)] Loss: 6357.142578\n",
      "Train Epoch: 267 [192768/225000 (86%)] Loss: 6365.628906\n",
      "Train Epoch: 267 [196864/225000 (87%)] Loss: 6316.835938\n",
      "Train Epoch: 267 [200960/225000 (89%)] Loss: 6357.416016\n",
      "Train Epoch: 267 [205056/225000 (91%)] Loss: 6320.593750\n",
      "Train Epoch: 267 [209152/225000 (93%)] Loss: 6347.667969\n",
      "Train Epoch: 267 [213248/225000 (95%)] Loss: 6265.349609\n",
      "Train Epoch: 267 [217344/225000 (97%)] Loss: 6335.996094\n",
      "Train Epoch: 267 [221440/225000 (98%)] Loss: 6282.750000\n",
      "    epoch          : 267\n",
      "    loss           : 6417.113793417591\n",
      "    val_loss       : 6401.220925484385\n",
      "Train Epoch: 268 [256/225000 (0%)] Loss: 6230.330078\n",
      "Train Epoch: 268 [4352/225000 (2%)] Loss: 6258.724609\n",
      "Train Epoch: 268 [8448/225000 (4%)] Loss: 6289.371094\n",
      "Train Epoch: 268 [12544/225000 (6%)] Loss: 6456.554688\n",
      "Train Epoch: 268 [16640/225000 (7%)] Loss: 6304.806641\n",
      "Train Epoch: 268 [20736/225000 (9%)] Loss: 6425.824219\n",
      "Train Epoch: 268 [24832/225000 (11%)] Loss: 6184.435547\n",
      "Train Epoch: 268 [28928/225000 (13%)] Loss: 6330.054688\n",
      "Train Epoch: 268 [33024/225000 (15%)] Loss: 6435.453125\n",
      "Train Epoch: 268 [37120/225000 (16%)] Loss: 6371.246094\n",
      "Train Epoch: 268 [41216/225000 (18%)] Loss: 6399.416016\n",
      "Train Epoch: 268 [45312/225000 (20%)] Loss: 6407.638672\n",
      "Train Epoch: 268 [49408/225000 (22%)] Loss: 6221.517578\n",
      "Train Epoch: 268 [53504/225000 (24%)] Loss: 6355.945312\n",
      "Train Epoch: 268 [57600/225000 (26%)] Loss: 6237.798828\n",
      "Train Epoch: 268 [61696/225000 (27%)] Loss: 6423.496094\n",
      "Train Epoch: 268 [65792/225000 (29%)] Loss: 6315.673828\n",
      "Train Epoch: 268 [69888/225000 (31%)] Loss: 6421.369141\n",
      "Train Epoch: 268 [73984/225000 (33%)] Loss: 6333.058594\n",
      "Train Epoch: 268 [78080/225000 (35%)] Loss: 6450.439453\n",
      "Train Epoch: 268 [82176/225000 (37%)] Loss: 6345.625000\n",
      "Train Epoch: 268 [86272/225000 (38%)] Loss: 6512.878906\n",
      "Train Epoch: 268 [90368/225000 (40%)] Loss: 6479.890625\n",
      "Train Epoch: 268 [94464/225000 (42%)] Loss: 6315.478516\n",
      "Train Epoch: 268 [98560/225000 (44%)] Loss: 6246.111328\n",
      "Train Epoch: 268 [102656/225000 (46%)] Loss: 6340.494141\n",
      "Train Epoch: 268 [106752/225000 (47%)] Loss: 6323.730469\n",
      "Train Epoch: 268 [110848/225000 (49%)] Loss: 6333.111328\n",
      "Train Epoch: 268 [114944/225000 (51%)] Loss: 6190.597656\n",
      "Train Epoch: 268 [119040/225000 (53%)] Loss: 6234.228516\n",
      "Train Epoch: 268 [123136/225000 (55%)] Loss: 6293.099609\n",
      "Train Epoch: 268 [127232/225000 (57%)] Loss: 6421.074219\n",
      "Train Epoch: 268 [131328/225000 (58%)] Loss: 6243.759766\n",
      "Train Epoch: 268 [135424/225000 (60%)] Loss: 6256.310547\n",
      "Train Epoch: 268 [139520/225000 (62%)] Loss: 6312.419922\n",
      "Train Epoch: 268 [143616/225000 (64%)] Loss: 6352.140625\n",
      "Train Epoch: 268 [147712/225000 (66%)] Loss: 6216.343750\n",
      "Train Epoch: 268 [151808/225000 (67%)] Loss: 18199.953125\n",
      "Train Epoch: 268 [155904/225000 (69%)] Loss: 6361.691406\n",
      "Train Epoch: 268 [160000/225000 (71%)] Loss: 6446.408203\n",
      "Train Epoch: 268 [164096/225000 (73%)] Loss: 6417.414062\n",
      "Train Epoch: 268 [168192/225000 (75%)] Loss: 6349.472656\n",
      "Train Epoch: 268 [172288/225000 (77%)] Loss: 6317.437500\n",
      "Train Epoch: 268 [176384/225000 (78%)] Loss: 6331.267578\n",
      "Train Epoch: 268 [180480/225000 (80%)] Loss: 6402.853516\n",
      "Train Epoch: 268 [184576/225000 (82%)] Loss: 6384.855469\n",
      "Train Epoch: 268 [188672/225000 (84%)] Loss: 6427.998047\n",
      "Train Epoch: 268 [192768/225000 (86%)] Loss: 6194.175781\n",
      "Train Epoch: 268 [196864/225000 (87%)] Loss: 6369.531250\n",
      "Train Epoch: 268 [200960/225000 (89%)] Loss: 6372.982422\n",
      "Train Epoch: 268 [205056/225000 (91%)] Loss: 6369.982422\n",
      "Train Epoch: 268 [209152/225000 (93%)] Loss: 6273.425781\n",
      "Train Epoch: 268 [213248/225000 (95%)] Loss: 6359.023438\n",
      "Train Epoch: 268 [217344/225000 (97%)] Loss: 6363.437500\n",
      "Train Epoch: 268 [221440/225000 (98%)] Loss: 6247.335938\n",
      "    epoch          : 268\n",
      "    loss           : 6367.488706760168\n",
      "    val_loss       : 6419.446768160986\n",
      "Train Epoch: 269 [256/225000 (0%)] Loss: 6350.501953\n",
      "Train Epoch: 269 [4352/225000 (2%)] Loss: 6445.667969\n",
      "Train Epoch: 269 [8448/225000 (4%)] Loss: 6377.638672\n",
      "Train Epoch: 269 [12544/225000 (6%)] Loss: 6359.064453\n",
      "Train Epoch: 269 [16640/225000 (7%)] Loss: 6342.966797\n",
      "Train Epoch: 269 [20736/225000 (9%)] Loss: 6299.800781\n",
      "Train Epoch: 269 [24832/225000 (11%)] Loss: 6280.007812\n",
      "Train Epoch: 269 [28928/225000 (13%)] Loss: 6284.048828\n",
      "Train Epoch: 269 [33024/225000 (15%)] Loss: 6221.152344\n",
      "Train Epoch: 269 [37120/225000 (16%)] Loss: 6424.292969\n",
      "Train Epoch: 269 [41216/225000 (18%)] Loss: 6259.251953\n",
      "Train Epoch: 269 [45312/225000 (20%)] Loss: 6359.607422\n",
      "Train Epoch: 269 [49408/225000 (22%)] Loss: 6235.818359\n",
      "Train Epoch: 269 [53504/225000 (24%)] Loss: 6314.962891\n",
      "Train Epoch: 269 [57600/225000 (26%)] Loss: 6373.761719\n",
      "Train Epoch: 269 [61696/225000 (27%)] Loss: 6276.191406\n",
      "Train Epoch: 269 [65792/225000 (29%)] Loss: 6417.349609\n",
      "Train Epoch: 269 [69888/225000 (31%)] Loss: 6413.996094\n",
      "Train Epoch: 269 [73984/225000 (33%)] Loss: 6384.283203\n",
      "Train Epoch: 269 [78080/225000 (35%)] Loss: 6368.455078\n",
      "Train Epoch: 269 [82176/225000 (37%)] Loss: 6309.832031\n",
      "Train Epoch: 269 [86272/225000 (38%)] Loss: 6402.509766\n",
      "Train Epoch: 269 [90368/225000 (40%)] Loss: 6310.675781\n",
      "Train Epoch: 269 [94464/225000 (42%)] Loss: 6280.464844\n",
      "Train Epoch: 269 [98560/225000 (44%)] Loss: 6363.328125\n",
      "Train Epoch: 269 [102656/225000 (46%)] Loss: 6290.919922\n",
      "Train Epoch: 269 [106752/225000 (47%)] Loss: 6415.781250\n",
      "Train Epoch: 269 [110848/225000 (49%)] Loss: 6395.423828\n",
      "Train Epoch: 269 [114944/225000 (51%)] Loss: 6251.552734\n",
      "Train Epoch: 269 [119040/225000 (53%)] Loss: 6396.972656\n",
      "Train Epoch: 269 [123136/225000 (55%)] Loss: 6287.105469\n",
      "Train Epoch: 269 [127232/225000 (57%)] Loss: 6357.951172\n",
      "Train Epoch: 269 [131328/225000 (58%)] Loss: 6163.593750\n",
      "Train Epoch: 269 [135424/225000 (60%)] Loss: 6412.023438\n",
      "Train Epoch: 269 [139520/225000 (62%)] Loss: 6359.949219\n",
      "Train Epoch: 269 [143616/225000 (64%)] Loss: 6307.916016\n",
      "Train Epoch: 269 [147712/225000 (66%)] Loss: 6231.183594\n",
      "Train Epoch: 269 [151808/225000 (67%)] Loss: 6292.980469\n",
      "Train Epoch: 269 [155904/225000 (69%)] Loss: 6235.371094\n",
      "Train Epoch: 269 [160000/225000 (71%)] Loss: 6277.339844\n",
      "Train Epoch: 269 [164096/225000 (73%)] Loss: 6285.708984\n",
      "Train Epoch: 269 [168192/225000 (75%)] Loss: 6413.896484\n",
      "Train Epoch: 269 [172288/225000 (77%)] Loss: 6404.203125\n",
      "Train Epoch: 269 [176384/225000 (78%)] Loss: 6346.648438\n",
      "Train Epoch: 269 [180480/225000 (80%)] Loss: 6192.398438\n",
      "Train Epoch: 269 [184576/225000 (82%)] Loss: 6285.000000\n",
      "Train Epoch: 269 [188672/225000 (84%)] Loss: 6420.625000\n",
      "Train Epoch: 269 [192768/225000 (86%)] Loss: 6324.748047\n",
      "Train Epoch: 269 [196864/225000 (87%)] Loss: 6316.638672\n",
      "Train Epoch: 269 [200960/225000 (89%)] Loss: 6246.269531\n",
      "Train Epoch: 269 [205056/225000 (91%)] Loss: 6227.824219\n",
      "Train Epoch: 269 [209152/225000 (93%)] Loss: 6306.970703\n",
      "Train Epoch: 269 [213248/225000 (95%)] Loss: 6247.931641\n",
      "Train Epoch: 269 [217344/225000 (97%)] Loss: 6310.539062\n",
      "Train Epoch: 269 [221440/225000 (98%)] Loss: 6297.970703\n",
      "    epoch          : 269\n",
      "    loss           : 6368.1223858344\n",
      "    val_loss       : 6466.412031443751\n",
      "Train Epoch: 270 [256/225000 (0%)] Loss: 6268.712891\n",
      "Train Epoch: 270 [4352/225000 (2%)] Loss: 6333.541016\n",
      "Train Epoch: 270 [8448/225000 (4%)] Loss: 6312.197266\n",
      "Train Epoch: 270 [12544/225000 (6%)] Loss: 6389.548828\n",
      "Train Epoch: 270 [16640/225000 (7%)] Loss: 6357.546875\n",
      "Train Epoch: 270 [20736/225000 (9%)] Loss: 6283.009766\n",
      "Train Epoch: 270 [24832/225000 (11%)] Loss: 6367.894531\n",
      "Train Epoch: 270 [28928/225000 (13%)] Loss: 6248.480469\n",
      "Train Epoch: 270 [33024/225000 (15%)] Loss: 6355.123047\n",
      "Train Epoch: 270 [37120/225000 (16%)] Loss: 6426.273438\n",
      "Train Epoch: 270 [41216/225000 (18%)] Loss: 6215.525391\n",
      "Train Epoch: 270 [45312/225000 (20%)] Loss: 6445.763672\n",
      "Train Epoch: 270 [49408/225000 (22%)] Loss: 6341.359375\n",
      "Train Epoch: 270 [53504/225000 (24%)] Loss: 6212.162109\n",
      "Train Epoch: 270 [57600/225000 (26%)] Loss: 6275.568359\n",
      "Train Epoch: 270 [61696/225000 (27%)] Loss: 6341.705078\n",
      "Train Epoch: 270 [65792/225000 (29%)] Loss: 6313.849609\n",
      "Train Epoch: 270 [69888/225000 (31%)] Loss: 6351.408203\n",
      "Train Epoch: 270 [73984/225000 (33%)] Loss: 6363.226562\n",
      "Train Epoch: 270 [78080/225000 (35%)] Loss: 6296.349609\n",
      "Train Epoch: 270 [82176/225000 (37%)] Loss: 6319.298828\n",
      "Train Epoch: 270 [86272/225000 (38%)] Loss: 6240.687500\n",
      "Train Epoch: 270 [90368/225000 (40%)] Loss: 6259.851562\n",
      "Train Epoch: 270 [94464/225000 (42%)] Loss: 6366.460938\n",
      "Train Epoch: 270 [98560/225000 (44%)] Loss: 6228.878906\n",
      "Train Epoch: 270 [102656/225000 (46%)] Loss: 6302.222656\n",
      "Train Epoch: 270 [106752/225000 (47%)] Loss: 6339.216797\n",
      "Train Epoch: 270 [110848/225000 (49%)] Loss: 6369.316406\n",
      "Train Epoch: 270 [114944/225000 (51%)] Loss: 6356.105469\n",
      "Train Epoch: 270 [119040/225000 (53%)] Loss: 6280.660156\n",
      "Train Epoch: 270 [123136/225000 (55%)] Loss: 6339.595703\n",
      "Train Epoch: 270 [127232/225000 (57%)] Loss: 6337.054688\n",
      "Train Epoch: 270 [131328/225000 (58%)] Loss: 6249.828125\n",
      "Train Epoch: 270 [135424/225000 (60%)] Loss: 6347.816406\n",
      "Train Epoch: 270 [139520/225000 (62%)] Loss: 6432.955078\n",
      "Train Epoch: 270 [143616/225000 (64%)] Loss: 6277.115234\n",
      "Train Epoch: 270 [147712/225000 (66%)] Loss: 6356.699219\n",
      "Train Epoch: 270 [151808/225000 (67%)] Loss: 6419.236328\n",
      "Train Epoch: 270 [155904/225000 (69%)] Loss: 6349.753906\n",
      "Train Epoch: 270 [160000/225000 (71%)] Loss: 6249.771484\n",
      "Train Epoch: 270 [164096/225000 (73%)] Loss: 6382.386719\n",
      "Train Epoch: 270 [168192/225000 (75%)] Loss: 6223.214844\n",
      "Train Epoch: 270 [172288/225000 (77%)] Loss: 6456.513672\n",
      "Train Epoch: 270 [176384/225000 (78%)] Loss: 6294.496094\n",
      "Train Epoch: 270 [180480/225000 (80%)] Loss: 6203.916016\n",
      "Train Epoch: 270 [184576/225000 (82%)] Loss: 6406.390625\n",
      "Train Epoch: 270 [188672/225000 (84%)] Loss: 6262.662109\n",
      "Train Epoch: 270 [192768/225000 (86%)] Loss: 6321.308594\n",
      "Train Epoch: 270 [196864/225000 (87%)] Loss: 6270.701172\n",
      "Train Epoch: 270 [200960/225000 (89%)] Loss: 6370.052734\n",
      "Train Epoch: 270 [205056/225000 (91%)] Loss: 6376.380859\n",
      "Train Epoch: 270 [209152/225000 (93%)] Loss: 6412.621094\n",
      "Train Epoch: 270 [213248/225000 (95%)] Loss: 6372.238281\n",
      "Train Epoch: 270 [217344/225000 (97%)] Loss: 6446.324219\n",
      "Train Epoch: 270 [221440/225000 (98%)] Loss: 6448.974609\n",
      "    epoch          : 270\n",
      "    loss           : 6342.138801861135\n",
      "    val_loss       : 6419.237388192391\n",
      "Train Epoch: 271 [256/225000 (0%)] Loss: 6276.179688\n",
      "Train Epoch: 271 [4352/225000 (2%)] Loss: 6401.281250\n",
      "Train Epoch: 271 [8448/225000 (4%)] Loss: 6329.089844\n",
      "Train Epoch: 271 [12544/225000 (6%)] Loss: 6291.353516\n",
      "Train Epoch: 271 [16640/225000 (7%)] Loss: 6264.617188\n",
      "Train Epoch: 271 [20736/225000 (9%)] Loss: 6302.966797\n",
      "Train Epoch: 271 [24832/225000 (11%)] Loss: 6310.830078\n",
      "Train Epoch: 271 [28928/225000 (13%)] Loss: 6207.806641\n",
      "Train Epoch: 271 [33024/225000 (15%)] Loss: 6389.984375\n",
      "Train Epoch: 271 [37120/225000 (16%)] Loss: 6288.820312\n",
      "Train Epoch: 271 [41216/225000 (18%)] Loss: 6293.164062\n",
      "Train Epoch: 271 [45312/225000 (20%)] Loss: 6315.314453\n",
      "Train Epoch: 271 [49408/225000 (22%)] Loss: 6461.453125\n",
      "Train Epoch: 271 [53504/225000 (24%)] Loss: 6334.753906\n",
      "Train Epoch: 271 [57600/225000 (26%)] Loss: 6337.664062\n",
      "Train Epoch: 271 [61696/225000 (27%)] Loss: 6350.285156\n",
      "Train Epoch: 271 [65792/225000 (29%)] Loss: 6262.443359\n",
      "Train Epoch: 271 [69888/225000 (31%)] Loss: 8250.722656\n",
      "Train Epoch: 271 [73984/225000 (33%)] Loss: 6286.056641\n",
      "Train Epoch: 271 [78080/225000 (35%)] Loss: 6287.214844\n",
      "Train Epoch: 271 [82176/225000 (37%)] Loss: 6264.970703\n",
      "Train Epoch: 271 [86272/225000 (38%)] Loss: 6163.955078\n",
      "Train Epoch: 271 [90368/225000 (40%)] Loss: 6231.751953\n",
      "Train Epoch: 271 [94464/225000 (42%)] Loss: 6283.441406\n",
      "Train Epoch: 271 [98560/225000 (44%)] Loss: 6242.750000\n",
      "Train Epoch: 271 [102656/225000 (46%)] Loss: 6169.910156\n",
      "Train Epoch: 271 [106752/225000 (47%)] Loss: 6372.234375\n",
      "Train Epoch: 271 [110848/225000 (49%)] Loss: 6296.416016\n",
      "Train Epoch: 271 [114944/225000 (51%)] Loss: 6392.710938\n",
      "Train Epoch: 271 [119040/225000 (53%)] Loss: 6249.675781\n",
      "Train Epoch: 271 [123136/225000 (55%)] Loss: 6440.187500\n",
      "Train Epoch: 271 [127232/225000 (57%)] Loss: 6497.425781\n",
      "Train Epoch: 271 [131328/225000 (58%)] Loss: 6345.019531\n",
      "Train Epoch: 271 [135424/225000 (60%)] Loss: 6206.242188\n",
      "Train Epoch: 271 [139520/225000 (62%)] Loss: 6381.005859\n",
      "Train Epoch: 271 [143616/225000 (64%)] Loss: 6294.900391\n",
      "Train Epoch: 271 [147712/225000 (66%)] Loss: 6353.919922\n",
      "Train Epoch: 271 [151808/225000 (67%)] Loss: 6323.580078\n",
      "Train Epoch: 271 [155904/225000 (69%)] Loss: 6305.275391\n",
      "Train Epoch: 271 [160000/225000 (71%)] Loss: 6350.794922\n",
      "Train Epoch: 271 [164096/225000 (73%)] Loss: 6254.246094\n",
      "Train Epoch: 271 [168192/225000 (75%)] Loss: 6276.050781\n",
      "Train Epoch: 271 [172288/225000 (77%)] Loss: 6276.460938\n",
      "Train Epoch: 271 [176384/225000 (78%)] Loss: 6359.382812\n",
      "Train Epoch: 271 [180480/225000 (80%)] Loss: 6376.181641\n",
      "Train Epoch: 271 [184576/225000 (82%)] Loss: 6528.394531\n",
      "Train Epoch: 271 [188672/225000 (84%)] Loss: 6328.498047\n",
      "Train Epoch: 271 [192768/225000 (86%)] Loss: 6340.623047\n",
      "Train Epoch: 271 [196864/225000 (87%)] Loss: 6314.863281\n",
      "Train Epoch: 271 [200960/225000 (89%)] Loss: 6282.804688\n",
      "Train Epoch: 271 [205056/225000 (91%)] Loss: 6302.789062\n",
      "Train Epoch: 271 [209152/225000 (93%)] Loss: 6340.681641\n",
      "Train Epoch: 271 [213248/225000 (95%)] Loss: 6271.011719\n",
      "Train Epoch: 271 [217344/225000 (97%)] Loss: 6332.710938\n",
      "Train Epoch: 271 [221440/225000 (98%)] Loss: 6371.431641\n",
      "    epoch          : 271\n",
      "    loss           : 6409.371250399958\n",
      "    val_loss       : 6436.818012642617\n",
      "Train Epoch: 272 [256/225000 (0%)] Loss: 6185.103516\n",
      "Train Epoch: 272 [4352/225000 (2%)] Loss: 6280.650391\n",
      "Train Epoch: 272 [8448/225000 (4%)] Loss: 6322.035156\n",
      "Train Epoch: 272 [12544/225000 (6%)] Loss: 6249.556641\n",
      "Train Epoch: 272 [16640/225000 (7%)] Loss: 6251.275391\n",
      "Train Epoch: 272 [20736/225000 (9%)] Loss: 6386.894531\n",
      "Train Epoch: 272 [24832/225000 (11%)] Loss: 6459.939453\n",
      "Train Epoch: 272 [28928/225000 (13%)] Loss: 6430.251953\n",
      "Train Epoch: 272 [33024/225000 (15%)] Loss: 6399.914062\n",
      "Train Epoch: 272 [37120/225000 (16%)] Loss: 6413.064453\n",
      "Train Epoch: 272 [41216/225000 (18%)] Loss: 6335.017578\n",
      "Train Epoch: 272 [45312/225000 (20%)] Loss: 6208.669922\n",
      "Train Epoch: 272 [49408/225000 (22%)] Loss: 6398.958984\n",
      "Train Epoch: 272 [53504/225000 (24%)] Loss: 6240.431641\n",
      "Train Epoch: 272 [57600/225000 (26%)] Loss: 6236.394531\n",
      "Train Epoch: 272 [61696/225000 (27%)] Loss: 6223.050781\n",
      "Train Epoch: 272 [65792/225000 (29%)] Loss: 6301.521484\n",
      "Train Epoch: 272 [69888/225000 (31%)] Loss: 6348.121094\n",
      "Train Epoch: 272 [73984/225000 (33%)] Loss: 6293.710938\n",
      "Train Epoch: 272 [78080/225000 (35%)] Loss: 6301.527344\n",
      "Train Epoch: 272 [82176/225000 (37%)] Loss: 6296.187500\n",
      "Train Epoch: 272 [86272/225000 (38%)] Loss: 6357.306641\n",
      "Train Epoch: 272 [90368/225000 (40%)] Loss: 6385.177734\n",
      "Train Epoch: 272 [94464/225000 (42%)] Loss: 6306.820312\n",
      "Train Epoch: 272 [98560/225000 (44%)] Loss: 6320.093750\n",
      "Train Epoch: 272 [102656/225000 (46%)] Loss: 6376.566406\n",
      "Train Epoch: 272 [106752/225000 (47%)] Loss: 6310.019531\n",
      "Train Epoch: 272 [110848/225000 (49%)] Loss: 6489.972656\n",
      "Train Epoch: 272 [114944/225000 (51%)] Loss: 6226.312500\n",
      "Train Epoch: 272 [119040/225000 (53%)] Loss: 6340.845703\n",
      "Train Epoch: 272 [123136/225000 (55%)] Loss: 6362.886719\n",
      "Train Epoch: 272 [127232/225000 (57%)] Loss: 6318.871094\n",
      "Train Epoch: 272 [131328/225000 (58%)] Loss: 6230.074219\n",
      "Train Epoch: 272 [135424/225000 (60%)] Loss: 6319.183594\n",
      "Train Epoch: 272 [139520/225000 (62%)] Loss: 6287.316406\n",
      "Train Epoch: 272 [143616/225000 (64%)] Loss: 6309.996094\n",
      "Train Epoch: 272 [147712/225000 (66%)] Loss: 6256.843750\n",
      "Train Epoch: 272 [151808/225000 (67%)] Loss: 6291.757812\n",
      "Train Epoch: 272 [155904/225000 (69%)] Loss: 6257.857422\n",
      "Train Epoch: 272 [160000/225000 (71%)] Loss: 6236.541016\n",
      "Train Epoch: 272 [164096/225000 (73%)] Loss: 6278.210938\n",
      "Train Epoch: 272 [168192/225000 (75%)] Loss: 6366.318359\n",
      "Train Epoch: 272 [172288/225000 (77%)] Loss: 6469.808594\n",
      "Train Epoch: 272 [176384/225000 (78%)] Loss: 6344.412109\n",
      "Train Epoch: 272 [180480/225000 (80%)] Loss: 6262.693359\n",
      "Train Epoch: 272 [184576/225000 (82%)] Loss: 6282.828125\n",
      "Train Epoch: 272 [188672/225000 (84%)] Loss: 6301.777344\n",
      "Train Epoch: 272 [192768/225000 (86%)] Loss: 6246.878906\n",
      "Train Epoch: 272 [196864/225000 (87%)] Loss: 6384.843750\n",
      "Train Epoch: 272 [200960/225000 (89%)] Loss: 6199.509766\n",
      "Train Epoch: 272 [205056/225000 (91%)] Loss: 6181.126953\n",
      "Train Epoch: 272 [209152/225000 (93%)] Loss: 6200.376953\n",
      "Train Epoch: 272 [213248/225000 (95%)] Loss: 6342.640625\n",
      "Train Epoch: 272 [217344/225000 (97%)] Loss: 6258.876953\n",
      "Train Epoch: 272 [221440/225000 (98%)] Loss: 6327.687500\n",
      "    epoch          : 272\n",
      "    loss           : 6374.921836115259\n",
      "    val_loss       : 6401.240282246044\n",
      "Train Epoch: 273 [256/225000 (0%)] Loss: 6274.658203\n",
      "Train Epoch: 273 [4352/225000 (2%)] Loss: 6383.453125\n",
      "Train Epoch: 273 [8448/225000 (4%)] Loss: 6254.572266\n",
      "Train Epoch: 273 [12544/225000 (6%)] Loss: 6354.533203\n",
      "Train Epoch: 273 [16640/225000 (7%)] Loss: 6314.009766\n",
      "Train Epoch: 273 [20736/225000 (9%)] Loss: 6361.646484\n",
      "Train Epoch: 273 [24832/225000 (11%)] Loss: 6175.548828\n",
      "Train Epoch: 273 [28928/225000 (13%)] Loss: 6312.748047\n",
      "Train Epoch: 273 [33024/225000 (15%)] Loss: 6301.724609\n",
      "Train Epoch: 273 [37120/225000 (16%)] Loss: 6357.185547\n",
      "Train Epoch: 273 [41216/225000 (18%)] Loss: 6336.259766\n",
      "Train Epoch: 273 [45312/225000 (20%)] Loss: 6452.523438\n",
      "Train Epoch: 273 [49408/225000 (22%)] Loss: 6384.626953\n",
      "Train Epoch: 273 [53504/225000 (24%)] Loss: 6270.751953\n",
      "Train Epoch: 273 [57600/225000 (26%)] Loss: 6407.710938\n",
      "Train Epoch: 273 [61696/225000 (27%)] Loss: 6417.986328\n",
      "Train Epoch: 273 [65792/225000 (29%)] Loss: 6389.871094\n",
      "Train Epoch: 273 [69888/225000 (31%)] Loss: 6189.634766\n",
      "Train Epoch: 273 [73984/225000 (33%)] Loss: 6241.269531\n",
      "Train Epoch: 273 [78080/225000 (35%)] Loss: 6228.146484\n",
      "Train Epoch: 273 [82176/225000 (37%)] Loss: 6351.398438\n",
      "Train Epoch: 273 [86272/225000 (38%)] Loss: 6250.855469\n",
      "Train Epoch: 273 [90368/225000 (40%)] Loss: 6368.335938\n",
      "Train Epoch: 273 [94464/225000 (42%)] Loss: 6317.738281\n",
      "Train Epoch: 273 [98560/225000 (44%)] Loss: 6265.806641\n",
      "Train Epoch: 273 [102656/225000 (46%)] Loss: 6209.650391\n",
      "Train Epoch: 273 [106752/225000 (47%)] Loss: 6245.236328\n",
      "Train Epoch: 273 [110848/225000 (49%)] Loss: 6320.093750\n",
      "Train Epoch: 273 [114944/225000 (51%)] Loss: 6246.042969\n",
      "Train Epoch: 273 [119040/225000 (53%)] Loss: 6346.412109\n",
      "Train Epoch: 273 [123136/225000 (55%)] Loss: 6261.404297\n",
      "Train Epoch: 273 [127232/225000 (57%)] Loss: 6334.572266\n",
      "Train Epoch: 273 [131328/225000 (58%)] Loss: 6363.630859\n",
      "Train Epoch: 273 [135424/225000 (60%)] Loss: 6296.431641\n",
      "Train Epoch: 273 [139520/225000 (62%)] Loss: 6371.033203\n",
      "Train Epoch: 273 [143616/225000 (64%)] Loss: 6452.841797\n",
      "Train Epoch: 273 [147712/225000 (66%)] Loss: 6436.171875\n",
      "Train Epoch: 273 [151808/225000 (67%)] Loss: 6376.656250\n",
      "Train Epoch: 273 [155904/225000 (69%)] Loss: 6302.019531\n",
      "Train Epoch: 273 [160000/225000 (71%)] Loss: 6425.154297\n",
      "Train Epoch: 273 [164096/225000 (73%)] Loss: 6200.605469\n",
      "Train Epoch: 273 [168192/225000 (75%)] Loss: 6470.837891\n",
      "Train Epoch: 273 [172288/225000 (77%)] Loss: 6200.033203\n",
      "Train Epoch: 273 [176384/225000 (78%)] Loss: 6263.656250\n",
      "Train Epoch: 273 [180480/225000 (80%)] Loss: 6373.794922\n",
      "Train Epoch: 273 [184576/225000 (82%)] Loss: 6313.890625\n",
      "Train Epoch: 273 [188672/225000 (84%)] Loss: 6274.804688\n",
      "Train Epoch: 273 [192768/225000 (86%)] Loss: 6304.380859\n",
      "Train Epoch: 273 [196864/225000 (87%)] Loss: 6305.343750\n",
      "Train Epoch: 273 [200960/225000 (89%)] Loss: 6350.501953\n",
      "Train Epoch: 273 [205056/225000 (91%)] Loss: 6420.304688\n",
      "Train Epoch: 273 [209152/225000 (93%)] Loss: 6268.341797\n",
      "Train Epoch: 273 [213248/225000 (95%)] Loss: 6221.203125\n",
      "Train Epoch: 273 [217344/225000 (97%)] Loss: 6293.685547\n",
      "Train Epoch: 273 [221440/225000 (98%)] Loss: 6291.929688\n",
      "    epoch          : 273\n",
      "    loss           : 6337.937782192122\n",
      "    val_loss       : 6418.883768993981\n",
      "Train Epoch: 274 [256/225000 (0%)] Loss: 6321.070312\n",
      "Train Epoch: 274 [4352/225000 (2%)] Loss: 6380.236328\n",
      "Train Epoch: 274 [8448/225000 (4%)] Loss: 6319.382812\n",
      "Train Epoch: 274 [12544/225000 (6%)] Loss: 6230.337891\n",
      "Train Epoch: 274 [16640/225000 (7%)] Loss: 6252.521484\n",
      "Train Epoch: 274 [20736/225000 (9%)] Loss: 6373.919922\n",
      "Train Epoch: 274 [24832/225000 (11%)] Loss: 6419.531250\n",
      "Train Epoch: 274 [28928/225000 (13%)] Loss: 6207.015625\n",
      "Train Epoch: 274 [33024/225000 (15%)] Loss: 6342.697266\n",
      "Train Epoch: 274 [37120/225000 (16%)] Loss: 6373.082031\n",
      "Train Epoch: 274 [41216/225000 (18%)] Loss: 6269.966797\n",
      "Train Epoch: 274 [45312/225000 (20%)] Loss: 6246.894531\n",
      "Train Epoch: 274 [49408/225000 (22%)] Loss: 6220.570312\n",
      "Train Epoch: 274 [53504/225000 (24%)] Loss: 6186.408203\n",
      "Train Epoch: 274 [57600/225000 (26%)] Loss: 6302.509766\n",
      "Train Epoch: 274 [61696/225000 (27%)] Loss: 6208.722656\n",
      "Train Epoch: 274 [65792/225000 (29%)] Loss: 6212.970703\n",
      "Train Epoch: 274 [69888/225000 (31%)] Loss: 6253.480469\n",
      "Train Epoch: 274 [73984/225000 (33%)] Loss: 6458.695312\n",
      "Train Epoch: 274 [78080/225000 (35%)] Loss: 6341.859375\n",
      "Train Epoch: 274 [82176/225000 (37%)] Loss: 6421.031250\n",
      "Train Epoch: 274 [86272/225000 (38%)] Loss: 6207.925781\n",
      "Train Epoch: 274 [90368/225000 (40%)] Loss: 6331.183594\n",
      "Train Epoch: 274 [94464/225000 (42%)] Loss: 6405.207031\n",
      "Train Epoch: 274 [98560/225000 (44%)] Loss: 6318.294922\n",
      "Train Epoch: 274 [102656/225000 (46%)] Loss: 6384.951172\n",
      "Train Epoch: 274 [106752/225000 (47%)] Loss: 6275.939453\n",
      "Train Epoch: 274 [110848/225000 (49%)] Loss: 6379.023438\n",
      "Train Epoch: 274 [114944/225000 (51%)] Loss: 6327.255859\n",
      "Train Epoch: 274 [119040/225000 (53%)] Loss: 6128.613281\n",
      "Train Epoch: 274 [123136/225000 (55%)] Loss: 6318.130859\n",
      "Train Epoch: 274 [127232/225000 (57%)] Loss: 6295.578125\n",
      "Train Epoch: 274 [131328/225000 (58%)] Loss: 6381.250000\n",
      "Train Epoch: 274 [135424/225000 (60%)] Loss: 6338.044922\n",
      "Train Epoch: 274 [139520/225000 (62%)] Loss: 6222.636719\n",
      "Train Epoch: 274 [143616/225000 (64%)] Loss: 6373.316406\n",
      "Train Epoch: 274 [147712/225000 (66%)] Loss: 6366.716797\n",
      "Train Epoch: 274 [151808/225000 (67%)] Loss: 6324.910156\n",
      "Train Epoch: 274 [155904/225000 (69%)] Loss: 6373.126953\n",
      "Train Epoch: 274 [160000/225000 (71%)] Loss: 6220.912109\n",
      "Train Epoch: 274 [164096/225000 (73%)] Loss: 6200.478516\n",
      "Train Epoch: 274 [168192/225000 (75%)] Loss: 6323.519531\n",
      "Train Epoch: 274 [172288/225000 (77%)] Loss: 6167.646484\n",
      "Train Epoch: 274 [176384/225000 (78%)] Loss: 6309.642578\n",
      "Train Epoch: 274 [180480/225000 (80%)] Loss: 6357.634766\n",
      "Train Epoch: 274 [184576/225000 (82%)] Loss: 6318.457031\n",
      "Train Epoch: 274 [188672/225000 (84%)] Loss: 6245.958984\n",
      "Train Epoch: 274 [192768/225000 (86%)] Loss: 6417.472656\n",
      "Train Epoch: 274 [196864/225000 (87%)] Loss: 6288.115234\n",
      "Train Epoch: 274 [200960/225000 (89%)] Loss: 6272.826172\n",
      "Train Epoch: 274 [205056/225000 (91%)] Loss: 6387.652344\n",
      "Train Epoch: 274 [209152/225000 (93%)] Loss: 6307.734375\n",
      "Train Epoch: 274 [213248/225000 (95%)] Loss: 6249.851562\n",
      "Train Epoch: 274 [217344/225000 (97%)] Loss: 6349.093750\n",
      "Train Epoch: 274 [221440/225000 (98%)] Loss: 6265.322266\n",
      "    epoch          : 274\n",
      "    loss           : 6400.858777285978\n",
      "    val_loss       : 6419.883317077646\n",
      "Validation performance didn't improve for 75 epochs. Training stops.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MolecularVaeCategoryModel(\n",
       "  (_category): FreeCategory(\n",
       "    (generator_0): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRUCell(34, 50)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=50, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_0_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_1): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRUCell(34, 50)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=50, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_1_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=50, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(50, 196, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_2): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRUCell(34, 488)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=488, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_2_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_3): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRUCell(34, 488)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=488, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_3_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=488, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(488, 196, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_4): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRUCell(34, 501)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=501, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_4_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_5): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRUCell(34, 501)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=501, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_5_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=501, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(501, 196, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_6): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRUCell(34, 50)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=50, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_6_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_7): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRUCell(34, 50)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=50, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_7_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=50, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(50, 292, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_8): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRUCell(34, 488)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=488, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_8_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_9): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRUCell(34, 488)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=488, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_9_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=488, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(488, 292, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_10): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRUCell(34, 501)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=501, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_10_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_11): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRUCell(34, 501)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=501, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_11_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=501, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(501, 292, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_12): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRUCell(34, 50)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=50, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_12_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_13): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRUCell(34, 50)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=50, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_13_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=50, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(50, 435, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_14): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRUCell(34, 488)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=488, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_14_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_15): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRUCell(34, 488)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=488, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_15_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=488, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(488, 435, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_16): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRUCell(34, 501)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=501, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_16_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_17): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRUCell(34, 501)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=501, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_17_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=501, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(501, 435, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=4080, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_weights): Sequential(\n",
       "    (0): Linear(in_features=4080, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=42, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_xs, valid_ys = list(valid_data_loader)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NamePopMessenger' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2f4d46d6e7cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/categorical_bpl/lib/python3.8/site-packages/pyro/nn/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pyro_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/categorical_bpl/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AnacondaProjects/categorical_bpl/model/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, observations, train)\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mobservations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             )\n\u001b[0;32m--> 142\u001b[0;31m             return pyro.poutine.replay(self.model, trace=trace)(\n\u001b[0m\u001b[1;32m    143\u001b[0m                 \u001b[0mobservations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/categorical_bpl/lib/python3.8/site-packages/pyro/poutine/messenger.py\u001b[0m in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_context_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/categorical_bpl/lib/python3.8/site-packages/pyro/nn/module.py\u001b[0m in \u001b[0;36mcached_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcached_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pyro_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcached_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AnacondaProjects/categorical_bpl/model/model.py\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(self, observations, train)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mname_pop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_stack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_variable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_morphism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmorphism\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/categorical_bpl/lib/python3.8/site-packages/discopyro/callable.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(self, *values)\u001b[0m\n\u001b[1;32m     92\u001b[0m )\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mDiagram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_PYTHON_FUNCTOR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/categorical_bpl/lib/python3.8/site-packages/discopy/cartesian.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *values)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_input_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mthen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mothers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/categorical_bpl/lib/python3.8/site-packages/discopy/cartesian.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*vals)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAxiomError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoes_not_compose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         return Function(self.dom, other.cod,\n\u001b[0;32m--> 127\u001b[0;31m                         lambda *vals: other(*tuplify(self(*vals))))\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mothers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/categorical_bpl/lib/python3.8/site-packages/discopy/cartesian.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *values)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_input_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mthen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mothers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/categorical_bpl/lib/python3.8/site-packages/discopy/cartesian.py\u001b[0m in \u001b[0;36mproduct\u001b[0;34m(*vals)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mvals0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuplify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0mvals1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuplify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0muntuplify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvals1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/categorical_bpl/lib/python3.8/site-packages/discopy/cartesian.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *values)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_input_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mthen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mothers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/categorical_bpl/lib/python3.8/site-packages/discopy/cartesian.py\u001b[0m in \u001b[0;36mproduct\u001b[0;34m(*vals)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mvals0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuplify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mvals1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuplify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0muntuplify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvals1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/categorical_bpl/lib/python3.8/site-packages/discopy/cartesian.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *values)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_input_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mthen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mothers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/categorical_bpl/lib/python3.8/site-packages/pyro/nn/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pyro_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/categorical_bpl/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AnacondaProjects/categorical_bpl/model/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, zs)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mteacher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mam_i_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_PYRO_STACK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'$%s$'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smiles_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m                 \u001b[0mteacher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'$%s$'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smiles_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NamePopMessenger' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "m, recons = model(observations=valid_xs, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(reconstructions == validation_data).all(dim=-1).to(dtype=torch.float).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:categorical_bpl] *",
   "language": "python",
   "name": "conda-env-categorical_bpl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
