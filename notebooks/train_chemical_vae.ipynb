{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eli/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [12:31:31] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='chemical_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-4,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 5,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer, log_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [192/225000 (0%)] Loss: 82811.687500\n",
      "Train Epoch: 1 [2688/225000 (1%)] Loss: 79181.625000\n",
      "Train Epoch: 1 [5184/225000 (2%)] Loss: 63810.339844\n",
      "Train Epoch: 1 [7680/225000 (3%)] Loss: 74515.312500\n",
      "Train Epoch: 1 [10176/225000 (5%)] Loss: 68839.460938\n",
      "Train Epoch: 1 [12672/225000 (6%)] Loss: 61265.140625\n",
      "Train Epoch: 1 [15168/225000 (7%)] Loss: 53784.226562\n",
      "Train Epoch: 1 [17664/225000 (8%)] Loss: 49350.484375\n",
      "Train Epoch: 1 [20160/225000 (9%)] Loss: 45318.972656\n",
      "Train Epoch: 1 [22656/225000 (10%)] Loss: 43960.312500\n",
      "Train Epoch: 1 [25152/225000 (11%)] Loss: 42003.164062\n",
      "Train Epoch: 1 [27648/225000 (12%)] Loss: 40534.101562\n",
      "Train Epoch: 1 [30144/225000 (13%)] Loss: 39913.339844\n",
      "Train Epoch: 1 [32640/225000 (15%)] Loss: 39042.605469\n",
      "Train Epoch: 1 [35136/225000 (16%)] Loss: 38234.027344\n",
      "Train Epoch: 1 [37632/225000 (17%)] Loss: 37457.445312\n",
      "Train Epoch: 1 [40128/225000 (18%)] Loss: 37150.691406\n",
      "Train Epoch: 1 [42624/225000 (19%)] Loss: 48165.988281\n",
      "Train Epoch: 1 [45120/225000 (20%)] Loss: 36558.675781\n",
      "Train Epoch: 1 [47616/225000 (21%)] Loss: 35328.949219\n",
      "Train Epoch: 1 [50112/225000 (22%)] Loss: 36001.203125\n",
      "Train Epoch: 1 [52608/225000 (23%)] Loss: 34876.480469\n",
      "Train Epoch: 1 [55104/225000 (24%)] Loss: 34439.421875\n",
      "Train Epoch: 1 [57600/225000 (26%)] Loss: 34660.015625\n",
      "Train Epoch: 1 [60096/225000 (27%)] Loss: 33967.406250\n",
      "Train Epoch: 1 [62592/225000 (28%)] Loss: 33968.546875\n",
      "Train Epoch: 1 [65088/225000 (29%)] Loss: 33151.679688\n",
      "Train Epoch: 1 [67584/225000 (30%)] Loss: 33675.839844\n",
      "Train Epoch: 1 [70080/225000 (31%)] Loss: 33115.050781\n",
      "Train Epoch: 1 [72576/225000 (32%)] Loss: 31966.226562\n",
      "Train Epoch: 1 [75072/225000 (33%)] Loss: 31209.238281\n",
      "Train Epoch: 1 [77568/225000 (34%)] Loss: 30984.273438\n",
      "Train Epoch: 1 [80064/225000 (36%)] Loss: 30072.859375\n",
      "Train Epoch: 1 [82560/225000 (37%)] Loss: 30028.726562\n",
      "Train Epoch: 1 [85056/225000 (38%)] Loss: 29075.816406\n",
      "Train Epoch: 1 [87552/225000 (39%)] Loss: 29126.892578\n",
      "Train Epoch: 1 [90048/225000 (40%)] Loss: 28179.332031\n",
      "Train Epoch: 1 [92544/225000 (41%)] Loss: 27848.484375\n",
      "Train Epoch: 1 [95040/225000 (42%)] Loss: 28297.630859\n",
      "Train Epoch: 1 [97536/225000 (43%)] Loss: 27571.142578\n",
      "Train Epoch: 1 [100032/225000 (44%)] Loss: 27448.687500\n",
      "Train Epoch: 1 [102528/225000 (46%)] Loss: 26971.689453\n",
      "Train Epoch: 1 [105024/225000 (47%)] Loss: 26999.673828\n",
      "Train Epoch: 1 [107520/225000 (48%)] Loss: 26331.880859\n",
      "Train Epoch: 1 [110016/225000 (49%)] Loss: 26899.396484\n",
      "Train Epoch: 1 [112512/225000 (50%)] Loss: 26817.351562\n",
      "Train Epoch: 1 [115008/225000 (51%)] Loss: 26232.322266\n",
      "Train Epoch: 1 [117504/225000 (52%)] Loss: 26275.552734\n",
      "Train Epoch: 1 [120000/225000 (53%)] Loss: 25891.011719\n",
      "Train Epoch: 1 [122496/225000 (54%)] Loss: 26267.222656\n",
      "Train Epoch: 1 [124992/225000 (56%)] Loss: 26219.548828\n",
      "Train Epoch: 1 [127488/225000 (57%)] Loss: 26138.980469\n",
      "Train Epoch: 1 [129984/225000 (58%)] Loss: 26263.912109\n",
      "Train Epoch: 1 [132480/225000 (59%)] Loss: 25831.755859\n",
      "Train Epoch: 1 [134976/225000 (60%)] Loss: 26137.464844\n",
      "Train Epoch: 1 [137472/225000 (61%)] Loss: 25313.335938\n",
      "Train Epoch: 1 [139968/225000 (62%)] Loss: 25957.376953\n",
      "Train Epoch: 1 [142464/225000 (63%)] Loss: 24981.363281\n",
      "Train Epoch: 1 [144960/225000 (64%)] Loss: 25422.347656\n",
      "Train Epoch: 1 [147456/225000 (66%)] Loss: 25639.845703\n",
      "Train Epoch: 1 [149952/225000 (67%)] Loss: 24818.304688\n",
      "Train Epoch: 1 [152448/225000 (68%)] Loss: 25647.871094\n",
      "Train Epoch: 1 [154944/225000 (69%)] Loss: 24978.685547\n",
      "Train Epoch: 1 [157440/225000 (70%)] Loss: 24316.406250\n",
      "Train Epoch: 1 [159936/225000 (71%)] Loss: 24643.431641\n",
      "Train Epoch: 1 [162432/225000 (72%)] Loss: 23947.625000\n",
      "Train Epoch: 1 [164928/225000 (73%)] Loss: 25020.185547\n",
      "Train Epoch: 1 [167424/225000 (74%)] Loss: 25367.933594\n",
      "Train Epoch: 1 [169920/225000 (76%)] Loss: 24808.328125\n",
      "Train Epoch: 1 [172416/225000 (77%)] Loss: 24535.271484\n",
      "Train Epoch: 1 [174912/225000 (78%)] Loss: 24141.343750\n",
      "Train Epoch: 1 [177408/225000 (79%)] Loss: 24633.777344\n",
      "Train Epoch: 1 [179904/225000 (80%)] Loss: 24576.279297\n",
      "Train Epoch: 1 [182400/225000 (81%)] Loss: 24050.039062\n",
      "Train Epoch: 1 [184896/225000 (82%)] Loss: 23916.318359\n",
      "Train Epoch: 1 [187392/225000 (83%)] Loss: 24152.992188\n",
      "Train Epoch: 1 [189888/225000 (84%)] Loss: 23955.906250\n",
      "Train Epoch: 1 [192384/225000 (86%)] Loss: 23427.984375\n",
      "Train Epoch: 1 [194880/225000 (87%)] Loss: 23785.847656\n",
      "Train Epoch: 1 [197376/225000 (88%)] Loss: 24210.960938\n",
      "Train Epoch: 1 [199872/225000 (89%)] Loss: 23872.935547\n",
      "Train Epoch: 1 [202368/225000 (90%)] Loss: 23628.628906\n",
      "Train Epoch: 1 [204864/225000 (91%)] Loss: 24011.173828\n",
      "Train Epoch: 1 [207360/225000 (92%)] Loss: 23560.066406\n",
      "Train Epoch: 1 [209856/225000 (93%)] Loss: 23318.339844\n",
      "Train Epoch: 1 [212352/225000 (94%)] Loss: 23986.873047\n",
      "Train Epoch: 1 [214848/225000 (95%)] Loss: 23418.832031\n",
      "Train Epoch: 1 [217344/225000 (97%)] Loss: 23406.835938\n",
      "Train Epoch: 1 [219840/225000 (98%)] Loss: 23134.435547\n",
      "Train Epoch: 1 [222336/225000 (99%)] Loss: 24132.742188\n",
      "Train Epoch: 1 [224832/225000 (100%)] Loss: 24024.699219\n",
      "    epoch          : 1\n",
      "    loss           : 31931.124110094923\n",
      "    val_loss       : 23639.291135074527\n",
      "Train Epoch: 2 [192/225000 (0%)] Loss: 23588.898438\n",
      "Train Epoch: 2 [2688/225000 (1%)] Loss: 23462.605469\n",
      "Train Epoch: 2 [5184/225000 (2%)] Loss: 23360.632812\n",
      "Train Epoch: 2 [7680/225000 (3%)] Loss: 23802.386719\n",
      "Train Epoch: 2 [10176/225000 (5%)] Loss: 23489.876953\n",
      "Train Epoch: 2 [12672/225000 (6%)] Loss: 23526.894531\n",
      "Train Epoch: 2 [15168/225000 (7%)] Loss: 23741.677734\n",
      "Train Epoch: 2 [17664/225000 (8%)] Loss: 23876.281250\n",
      "Train Epoch: 2 [20160/225000 (9%)] Loss: 22841.261719\n",
      "Train Epoch: 2 [22656/225000 (10%)] Loss: 23592.908203\n",
      "Train Epoch: 2 [25152/225000 (11%)] Loss: 23720.912109\n",
      "Train Epoch: 2 [27648/225000 (12%)] Loss: 22972.119141\n",
      "Train Epoch: 2 [30144/225000 (13%)] Loss: 23045.326172\n",
      "Train Epoch: 2 [32640/225000 (15%)] Loss: 23277.035156\n",
      "Train Epoch: 2 [35136/225000 (16%)] Loss: 23636.583984\n",
      "Train Epoch: 2 [37632/225000 (17%)] Loss: 23061.412109\n",
      "Train Epoch: 2 [40128/225000 (18%)] Loss: 23045.242188\n",
      "Train Epoch: 2 [42624/225000 (19%)] Loss: 23121.396484\n",
      "Train Epoch: 2 [45120/225000 (20%)] Loss: 23679.992188\n",
      "Train Epoch: 2 [47616/225000 (21%)] Loss: 23312.949219\n",
      "Train Epoch: 2 [50112/225000 (22%)] Loss: 22661.550781\n",
      "Train Epoch: 2 [52608/225000 (23%)] Loss: 22978.644531\n",
      "Train Epoch: 2 [55104/225000 (24%)] Loss: 23672.468750\n",
      "Train Epoch: 2 [57600/225000 (26%)] Loss: 23336.179688\n",
      "Train Epoch: 2 [60096/225000 (27%)] Loss: 23135.074219\n",
      "Train Epoch: 2 [62592/225000 (28%)] Loss: 23740.394531\n",
      "Train Epoch: 2 [65088/225000 (29%)] Loss: 23428.832031\n",
      "Train Epoch: 2 [67584/225000 (30%)] Loss: 22625.416016\n",
      "Train Epoch: 2 [70080/225000 (31%)] Loss: 23335.396484\n",
      "Train Epoch: 2 [72576/225000 (32%)] Loss: 22748.707031\n",
      "Train Epoch: 2 [75072/225000 (33%)] Loss: 22968.324219\n",
      "Train Epoch: 2 [77568/225000 (34%)] Loss: 22980.357422\n",
      "Train Epoch: 2 [80064/225000 (36%)] Loss: 23293.619141\n",
      "Train Epoch: 2 [82560/225000 (37%)] Loss: 23421.542969\n",
      "Train Epoch: 2 [85056/225000 (38%)] Loss: 23351.507812\n",
      "Train Epoch: 2 [87552/225000 (39%)] Loss: 22974.298828\n",
      "Train Epoch: 2 [90048/225000 (40%)] Loss: 22630.246094\n",
      "Train Epoch: 2 [92544/225000 (41%)] Loss: 22065.646484\n",
      "Train Epoch: 2 [95040/225000 (42%)] Loss: 23119.886719\n",
      "Train Epoch: 2 [97536/225000 (43%)] Loss: 23111.121094\n",
      "Train Epoch: 2 [100032/225000 (44%)] Loss: 22247.703125\n",
      "Train Epoch: 2 [102528/225000 (46%)] Loss: 22598.736328\n",
      "Train Epoch: 2 [105024/225000 (47%)] Loss: 23100.740234\n",
      "Train Epoch: 2 [107520/225000 (48%)] Loss: 22799.832031\n",
      "Train Epoch: 2 [110016/225000 (49%)] Loss: 22244.531250\n",
      "Train Epoch: 2 [112512/225000 (50%)] Loss: 22123.878906\n",
      "Train Epoch: 2 [115008/225000 (51%)] Loss: 22405.515625\n",
      "Train Epoch: 2 [117504/225000 (52%)] Loss: 22622.804688\n",
      "Train Epoch: 2 [120000/225000 (53%)] Loss: 22712.880859\n",
      "Train Epoch: 2 [122496/225000 (54%)] Loss: 22603.519531\n",
      "Train Epoch: 2 [124992/225000 (56%)] Loss: 22498.394531\n",
      "Train Epoch: 2 [127488/225000 (57%)] Loss: 21783.957031\n",
      "Train Epoch: 2 [129984/225000 (58%)] Loss: 22349.542969\n",
      "Train Epoch: 2 [132480/225000 (59%)] Loss: 22265.380859\n",
      "Train Epoch: 2 [134976/225000 (60%)] Loss: 22022.777344\n",
      "Train Epoch: 2 [137472/225000 (61%)] Loss: 21788.136719\n",
      "Train Epoch: 2 [139968/225000 (62%)] Loss: 22025.078125\n",
      "Train Epoch: 2 [142464/225000 (63%)] Loss: 22144.460938\n",
      "Train Epoch: 2 [144960/225000 (64%)] Loss: 21211.353516\n",
      "Train Epoch: 2 [147456/225000 (66%)] Loss: 21800.470703\n",
      "Train Epoch: 2 [149952/225000 (67%)] Loss: 22449.892578\n",
      "Train Epoch: 2 [152448/225000 (68%)] Loss: 21631.953125\n",
      "Train Epoch: 2 [154944/225000 (69%)] Loss: 22389.730469\n",
      "Train Epoch: 2 [157440/225000 (70%)] Loss: 22320.433594\n",
      "Train Epoch: 2 [159936/225000 (71%)] Loss: 22034.058594\n",
      "Train Epoch: 2 [162432/225000 (72%)] Loss: 22315.689453\n",
      "Train Epoch: 2 [164928/225000 (73%)] Loss: 22363.074219\n",
      "Train Epoch: 2 [167424/225000 (74%)] Loss: 21698.652344\n",
      "Train Epoch: 2 [169920/225000 (76%)] Loss: 21956.460938\n",
      "Train Epoch: 2 [172416/225000 (77%)] Loss: 22185.693359\n",
      "Train Epoch: 2 [174912/225000 (78%)] Loss: 21851.396484\n",
      "Train Epoch: 2 [177408/225000 (79%)] Loss: 21922.736328\n",
      "Train Epoch: 2 [179904/225000 (80%)] Loss: 21679.335938\n",
      "Train Epoch: 2 [182400/225000 (81%)] Loss: 21960.757812\n",
      "Train Epoch: 2 [184896/225000 (82%)] Loss: 22521.900391\n",
      "Train Epoch: 2 [187392/225000 (83%)] Loss: 22038.535156\n",
      "Train Epoch: 2 [189888/225000 (84%)] Loss: 21680.878906\n",
      "Train Epoch: 2 [192384/225000 (86%)] Loss: 21541.296875\n",
      "Train Epoch: 2 [194880/225000 (87%)] Loss: 22211.597656\n",
      "Train Epoch: 2 [197376/225000 (88%)] Loss: 21972.972656\n",
      "Train Epoch: 2 [199872/225000 (89%)] Loss: 21832.082031\n",
      "Train Epoch: 2 [202368/225000 (90%)] Loss: 21773.550781\n",
      "Train Epoch: 2 [204864/225000 (91%)] Loss: 21760.320312\n",
      "Train Epoch: 2 [207360/225000 (92%)] Loss: 21319.447266\n",
      "Train Epoch: 2 [209856/225000 (93%)] Loss: 21798.593750\n",
      "Train Epoch: 2 [212352/225000 (94%)] Loss: 21864.230469\n",
      "Train Epoch: 2 [214848/225000 (95%)] Loss: 21844.406250\n",
      "Train Epoch: 2 [217344/225000 (97%)] Loss: 21662.882812\n",
      "Train Epoch: 2 [219840/225000 (98%)] Loss: 21598.648438\n",
      "Train Epoch: 2 [222336/225000 (99%)] Loss: 21947.603516\n",
      "Train Epoch: 2 [224832/225000 (100%)] Loss: 21952.671875\n",
      "    epoch          : 2\n",
      "    loss           : 22602.933402103776\n",
      "    val_loss       : 21641.625861329885\n",
      "Train Epoch: 3 [192/225000 (0%)] Loss: 21689.000000\n",
      "Train Epoch: 3 [2688/225000 (1%)] Loss: 21962.750000\n",
      "Train Epoch: 3 [5184/225000 (2%)] Loss: 21882.484375\n",
      "Train Epoch: 3 [7680/225000 (3%)] Loss: 22231.929688\n",
      "Train Epoch: 3 [10176/225000 (5%)] Loss: 21969.597656\n",
      "Train Epoch: 3 [12672/225000 (6%)] Loss: 21592.664062\n",
      "Train Epoch: 3 [15168/225000 (7%)] Loss: 21928.410156\n",
      "Train Epoch: 3 [17664/225000 (8%)] Loss: 21320.607422\n",
      "Train Epoch: 3 [20160/225000 (9%)] Loss: 21787.484375\n",
      "Train Epoch: 3 [22656/225000 (10%)] Loss: 21671.007812\n",
      "Train Epoch: 3 [25152/225000 (11%)] Loss: 21862.078125\n",
      "Train Epoch: 3 [27648/225000 (12%)] Loss: 21763.333984\n",
      "Train Epoch: 3 [30144/225000 (13%)] Loss: 21600.878906\n",
      "Train Epoch: 3 [32640/225000 (15%)] Loss: 21857.062500\n",
      "Train Epoch: 3 [35136/225000 (16%)] Loss: 22189.101562\n",
      "Train Epoch: 3 [37632/225000 (17%)] Loss: 21759.703125\n",
      "Train Epoch: 3 [40128/225000 (18%)] Loss: 21798.853516\n",
      "Train Epoch: 3 [42624/225000 (19%)] Loss: 21332.416016\n",
      "Train Epoch: 3 [45120/225000 (20%)] Loss: 21794.027344\n",
      "Train Epoch: 3 [47616/225000 (21%)] Loss: 21786.156250\n",
      "Train Epoch: 3 [50112/225000 (22%)] Loss: 21556.658203\n",
      "Train Epoch: 3 [52608/225000 (23%)] Loss: 21807.011719\n",
      "Train Epoch: 3 [55104/225000 (24%)] Loss: 21784.691406\n",
      "Train Epoch: 3 [57600/225000 (26%)] Loss: 21498.371094\n",
      "Train Epoch: 3 [60096/225000 (27%)] Loss: 21723.751953\n",
      "Train Epoch: 3 [62592/225000 (28%)] Loss: 21452.789062\n",
      "Train Epoch: 3 [65088/225000 (29%)] Loss: 21858.105469\n",
      "Train Epoch: 3 [67584/225000 (30%)] Loss: 21724.121094\n",
      "Train Epoch: 3 [70080/225000 (31%)] Loss: 21941.580078\n",
      "Train Epoch: 3 [72576/225000 (32%)] Loss: 21085.476562\n",
      "Train Epoch: 3 [75072/225000 (33%)] Loss: 21469.884766\n",
      "Train Epoch: 3 [77568/225000 (34%)] Loss: 21471.382812\n",
      "Train Epoch: 3 [80064/225000 (36%)] Loss: 21571.917969\n",
      "Train Epoch: 3 [82560/225000 (37%)] Loss: 21218.039062\n",
      "Train Epoch: 3 [85056/225000 (38%)] Loss: 21863.505859\n",
      "Train Epoch: 3 [87552/225000 (39%)] Loss: 21343.619141\n",
      "Train Epoch: 3 [90048/225000 (40%)] Loss: 21144.531250\n",
      "Train Epoch: 3 [92544/225000 (41%)] Loss: 21173.476562\n",
      "Train Epoch: 3 [95040/225000 (42%)] Loss: 21199.580078\n",
      "Train Epoch: 3 [97536/225000 (43%)] Loss: 21543.019531\n",
      "Train Epoch: 3 [100032/225000 (44%)] Loss: 21963.021484\n",
      "Train Epoch: 3 [102528/225000 (46%)] Loss: 21568.355469\n",
      "Train Epoch: 3 [105024/225000 (47%)] Loss: 21153.132812\n",
      "Train Epoch: 3 [107520/225000 (48%)] Loss: 21543.074219\n",
      "Train Epoch: 3 [110016/225000 (49%)] Loss: 21243.753906\n",
      "Train Epoch: 3 [112512/225000 (50%)] Loss: 21320.238281\n",
      "Train Epoch: 3 [115008/225000 (51%)] Loss: 21424.714844\n",
      "Train Epoch: 3 [117504/225000 (52%)] Loss: 21387.595703\n",
      "Train Epoch: 3 [120000/225000 (53%)] Loss: 21917.152344\n",
      "Train Epoch: 3 [122496/225000 (54%)] Loss: 21583.384766\n",
      "Train Epoch: 3 [124992/225000 (56%)] Loss: 21654.390625\n",
      "Train Epoch: 3 [127488/225000 (57%)] Loss: 21452.041016\n",
      "Train Epoch: 3 [129984/225000 (58%)] Loss: 21077.566406\n",
      "Train Epoch: 3 [132480/225000 (59%)] Loss: 21733.539062\n",
      "Train Epoch: 3 [134976/225000 (60%)] Loss: 21235.265625\n",
      "Train Epoch: 3 [137472/225000 (61%)] Loss: 21571.226562\n",
      "Train Epoch: 3 [139968/225000 (62%)] Loss: 21834.082031\n",
      "Train Epoch: 3 [142464/225000 (63%)] Loss: 21870.246094\n",
      "Train Epoch: 3 [144960/225000 (64%)] Loss: 21453.074219\n",
      "Train Epoch: 3 [147456/225000 (66%)] Loss: 21120.503906\n",
      "Train Epoch: 3 [149952/225000 (67%)] Loss: 21385.750000\n",
      "Train Epoch: 3 [152448/225000 (68%)] Loss: 21714.769531\n",
      "Train Epoch: 3 [154944/225000 (69%)] Loss: 21625.054688\n",
      "Train Epoch: 3 [157440/225000 (70%)] Loss: 21457.183594\n",
      "Train Epoch: 3 [159936/225000 (71%)] Loss: 21255.210938\n",
      "Train Epoch: 3 [162432/225000 (72%)] Loss: 20872.068359\n",
      "Train Epoch: 3 [164928/225000 (73%)] Loss: 21519.781250\n",
      "Train Epoch: 3 [167424/225000 (74%)] Loss: 21500.412109\n",
      "Train Epoch: 3 [169920/225000 (76%)] Loss: 21665.316406\n",
      "Train Epoch: 3 [172416/225000 (77%)] Loss: 21649.134766\n",
      "Train Epoch: 3 [174912/225000 (78%)] Loss: 21167.015625\n",
      "Train Epoch: 3 [177408/225000 (79%)] Loss: 21467.964844\n",
      "Train Epoch: 3 [179904/225000 (80%)] Loss: 21228.976562\n",
      "Train Epoch: 3 [182400/225000 (81%)] Loss: 21557.500000\n",
      "Train Epoch: 3 [184896/225000 (82%)] Loss: 21272.429688\n",
      "Train Epoch: 3 [187392/225000 (83%)] Loss: 21652.716797\n",
      "Train Epoch: 3 [189888/225000 (84%)] Loss: 21751.611328\n",
      "Train Epoch: 3 [192384/225000 (86%)] Loss: 21431.523438\n",
      "Train Epoch: 3 [194880/225000 (87%)] Loss: 21313.664062\n",
      "Train Epoch: 3 [197376/225000 (88%)] Loss: 21455.800781\n",
      "Train Epoch: 3 [199872/225000 (89%)] Loss: 20771.599609\n",
      "Train Epoch: 3 [202368/225000 (90%)] Loss: 22014.203125\n",
      "Train Epoch: 3 [204864/225000 (91%)] Loss: 20752.367188\n",
      "Train Epoch: 3 [207360/225000 (92%)] Loss: 21558.232422\n",
      "Train Epoch: 3 [209856/225000 (93%)] Loss: 21547.902344\n",
      "Train Epoch: 3 [212352/225000 (94%)] Loss: 21347.660156\n",
      "Train Epoch: 3 [214848/225000 (95%)] Loss: 21018.519531\n",
      "Train Epoch: 3 [217344/225000 (97%)] Loss: 21108.242188\n",
      "Train Epoch: 3 [219840/225000 (98%)] Loss: 20943.222656\n",
      "Train Epoch: 3 [222336/225000 (99%)] Loss: 20805.007812\n",
      "Train Epoch: 3 [224832/225000 (100%)] Loss: 21433.216797\n",
      "    epoch          : 3\n",
      "    loss           : 21558.244393931313\n",
      "    val_loss       : 21382.67244413791\n",
      "Train Epoch: 4 [192/225000 (0%)] Loss: 21353.726562\n",
      "Train Epoch: 4 [2688/225000 (1%)] Loss: 21347.148438\n",
      "Train Epoch: 4 [5184/225000 (2%)] Loss: 21578.308594\n",
      "Train Epoch: 4 [7680/225000 (3%)] Loss: 21424.664062\n",
      "Train Epoch: 4 [10176/225000 (5%)] Loss: 21118.910156\n",
      "Train Epoch: 4 [12672/225000 (6%)] Loss: 21247.863281\n",
      "Train Epoch: 4 [15168/225000 (7%)] Loss: 21494.205078\n",
      "Train Epoch: 4 [17664/225000 (8%)] Loss: 21449.765625\n",
      "Train Epoch: 4 [20160/225000 (9%)] Loss: 21336.460938\n",
      "Train Epoch: 4 [22656/225000 (10%)] Loss: 21342.763672\n",
      "Train Epoch: 4 [25152/225000 (11%)] Loss: 21446.535156\n",
      "Train Epoch: 4 [27648/225000 (12%)] Loss: 21682.056641\n",
      "Train Epoch: 4 [30144/225000 (13%)] Loss: 20915.023438\n",
      "Train Epoch: 4 [32640/225000 (15%)] Loss: 20729.000000\n",
      "Train Epoch: 4 [35136/225000 (16%)] Loss: 21471.394531\n",
      "Train Epoch: 4 [37632/225000 (17%)] Loss: 21532.960938\n",
      "Train Epoch: 4 [40128/225000 (18%)] Loss: 21184.509766\n",
      "Train Epoch: 4 [42624/225000 (19%)] Loss: 21077.863281\n",
      "Train Epoch: 4 [45120/225000 (20%)] Loss: 21452.257812\n",
      "Train Epoch: 4 [47616/225000 (21%)] Loss: 21076.492188\n",
      "Train Epoch: 4 [50112/225000 (22%)] Loss: 21398.640625\n",
      "Train Epoch: 4 [52608/225000 (23%)] Loss: 21202.468750\n",
      "Train Epoch: 4 [55104/225000 (24%)] Loss: 20544.925781\n",
      "Train Epoch: 4 [57600/225000 (26%)] Loss: 21628.568359\n",
      "Train Epoch: 4 [60096/225000 (27%)] Loss: 21126.589844\n",
      "Train Epoch: 4 [62592/225000 (28%)] Loss: 21228.925781\n",
      "Train Epoch: 4 [65088/225000 (29%)] Loss: 21362.912109\n",
      "Train Epoch: 4 [67584/225000 (30%)] Loss: 21222.671875\n",
      "Train Epoch: 4 [70080/225000 (31%)] Loss: 21291.796875\n",
      "Train Epoch: 4 [72576/225000 (32%)] Loss: 20967.734375\n",
      "Train Epoch: 4 [75072/225000 (33%)] Loss: 21832.296875\n",
      "Train Epoch: 4 [77568/225000 (34%)] Loss: 21011.007812\n",
      "Train Epoch: 4 [80064/225000 (36%)] Loss: 21059.617188\n",
      "Train Epoch: 4 [82560/225000 (37%)] Loss: 21376.638672\n",
      "Train Epoch: 4 [85056/225000 (38%)] Loss: 21931.917969\n",
      "Train Epoch: 4 [87552/225000 (39%)] Loss: 21223.943359\n",
      "Train Epoch: 4 [90048/225000 (40%)] Loss: 21111.812500\n",
      "Train Epoch: 4 [92544/225000 (41%)] Loss: 21641.546875\n",
      "Train Epoch: 4 [95040/225000 (42%)] Loss: 21383.011719\n",
      "Train Epoch: 4 [97536/225000 (43%)] Loss: 20735.371094\n",
      "Train Epoch: 4 [100032/225000 (44%)] Loss: 20937.728516\n",
      "Train Epoch: 4 [102528/225000 (46%)] Loss: 21324.207031\n",
      "Train Epoch: 4 [105024/225000 (47%)] Loss: 20575.242188\n",
      "Train Epoch: 4 [107520/225000 (48%)] Loss: 21081.183594\n",
      "Train Epoch: 4 [110016/225000 (49%)] Loss: 21226.855469\n",
      "Train Epoch: 4 [112512/225000 (50%)] Loss: 21420.025391\n",
      "Train Epoch: 4 [115008/225000 (51%)] Loss: 21084.496094\n",
      "Train Epoch: 4 [117504/225000 (52%)] Loss: 21157.763672\n",
      "Train Epoch: 4 [120000/225000 (53%)] Loss: 21262.980469\n",
      "Train Epoch: 4 [122496/225000 (54%)] Loss: 21428.214844\n",
      "Train Epoch: 4 [124992/225000 (56%)] Loss: 21364.531250\n",
      "Train Epoch: 4 [127488/225000 (57%)] Loss: 20997.656250\n",
      "Train Epoch: 4 [129984/225000 (58%)] Loss: 21470.615234\n",
      "Train Epoch: 4 [132480/225000 (59%)] Loss: 21428.263672\n",
      "Train Epoch: 4 [134976/225000 (60%)] Loss: 21947.796875\n",
      "Train Epoch: 4 [137472/225000 (61%)] Loss: 21009.492188\n",
      "Train Epoch: 4 [139968/225000 (62%)] Loss: 21043.275391\n",
      "Train Epoch: 4 [142464/225000 (63%)] Loss: 20668.621094\n",
      "Train Epoch: 4 [144960/225000 (64%)] Loss: 21427.078125\n",
      "Train Epoch: 4 [147456/225000 (66%)] Loss: 21003.443359\n",
      "Train Epoch: 4 [149952/225000 (67%)] Loss: 21141.687500\n",
      "Train Epoch: 4 [152448/225000 (68%)] Loss: 21222.332031\n",
      "Train Epoch: 4 [154944/225000 (69%)] Loss: 20991.068359\n",
      "Train Epoch: 4 [157440/225000 (70%)] Loss: 21288.017578\n",
      "Train Epoch: 4 [159936/225000 (71%)] Loss: 21248.339844\n",
      "Train Epoch: 4 [162432/225000 (72%)] Loss: 21424.007812\n",
      "Train Epoch: 4 [164928/225000 (73%)] Loss: 21162.392578\n",
      "Train Epoch: 4 [167424/225000 (74%)] Loss: 20838.437500\n",
      "Train Epoch: 4 [169920/225000 (76%)] Loss: 20786.910156\n",
      "Train Epoch: 4 [172416/225000 (77%)] Loss: 21268.890625\n",
      "Train Epoch: 4 [174912/225000 (78%)] Loss: 21066.937500\n",
      "Train Epoch: 4 [177408/225000 (79%)] Loss: 20698.464844\n",
      "Train Epoch: 4 [179904/225000 (80%)] Loss: 21143.425781\n",
      "Train Epoch: 4 [182400/225000 (81%)] Loss: 21136.617188\n",
      "Train Epoch: 4 [184896/225000 (82%)] Loss: 21346.355469\n",
      "Train Epoch: 4 [187392/225000 (83%)] Loss: 21073.146484\n",
      "Train Epoch: 4 [189888/225000 (84%)] Loss: 20746.304688\n",
      "Train Epoch: 4 [192384/225000 (86%)] Loss: 21057.173828\n",
      "Train Epoch: 4 [194880/225000 (87%)] Loss: 21375.371094\n",
      "Train Epoch: 4 [197376/225000 (88%)] Loss: 20716.574219\n",
      "Train Epoch: 4 [199872/225000 (89%)] Loss: 21784.060547\n",
      "Train Epoch: 4 [202368/225000 (90%)] Loss: 21184.228516\n",
      "Train Epoch: 4 [204864/225000 (91%)] Loss: 20781.949219\n",
      "Train Epoch: 4 [207360/225000 (92%)] Loss: 20878.066406\n",
      "Train Epoch: 4 [209856/225000 (93%)] Loss: 20769.806641\n",
      "Train Epoch: 4 [212352/225000 (94%)] Loss: 20803.699219\n",
      "Train Epoch: 4 [214848/225000 (95%)] Loss: 21051.285156\n",
      "Train Epoch: 4 [217344/225000 (97%)] Loss: 21563.984375\n",
      "Train Epoch: 4 [219840/225000 (98%)] Loss: 20774.707031\n",
      "Train Epoch: 4 [222336/225000 (99%)] Loss: 21097.636719\n",
      "Train Epoch: 4 [224832/225000 (100%)] Loss: 21148.675781\n",
      "    epoch          : 4\n",
      "    loss           : 21281.185568539357\n",
      "    val_loss       : 21153.943108900814\n",
      "Train Epoch: 5 [192/225000 (0%)] Loss: 21194.761719\n",
      "Train Epoch: 5 [2688/225000 (1%)] Loss: 21613.318359\n",
      "Train Epoch: 5 [5184/225000 (2%)] Loss: 21585.058594\n",
      "Train Epoch: 5 [7680/225000 (3%)] Loss: 21403.613281\n",
      "Train Epoch: 5 [10176/225000 (5%)] Loss: 21526.765625\n",
      "Train Epoch: 5 [12672/225000 (6%)] Loss: 20967.283203\n",
      "Train Epoch: 5 [15168/225000 (7%)] Loss: 21662.302734\n",
      "Train Epoch: 5 [17664/225000 (8%)] Loss: 21268.605469\n",
      "Train Epoch: 5 [20160/225000 (9%)] Loss: 20828.652344\n",
      "Train Epoch: 5 [22656/225000 (10%)] Loss: 21182.837891\n",
      "Train Epoch: 5 [25152/225000 (11%)] Loss: 20473.898438\n",
      "Train Epoch: 5 [27648/225000 (12%)] Loss: 22156.757812\n",
      "Train Epoch: 5 [30144/225000 (13%)] Loss: 21247.675781\n",
      "Train Epoch: 5 [32640/225000 (15%)] Loss: 20833.996094\n",
      "Train Epoch: 5 [35136/225000 (16%)] Loss: 21617.187500\n",
      "Train Epoch: 5 [37632/225000 (17%)] Loss: 21289.062500\n",
      "Train Epoch: 5 [40128/225000 (18%)] Loss: 21051.316406\n",
      "Train Epoch: 5 [42624/225000 (19%)] Loss: 20792.046875\n",
      "Train Epoch: 5 [45120/225000 (20%)] Loss: 20749.839844\n",
      "Train Epoch: 5 [47616/225000 (21%)] Loss: 20765.605469\n",
      "Train Epoch: 5 [50112/225000 (22%)] Loss: 21022.914062\n",
      "Train Epoch: 5 [52608/225000 (23%)] Loss: 21234.292969\n",
      "Train Epoch: 5 [55104/225000 (24%)] Loss: 21119.787109\n",
      "Train Epoch: 5 [57600/225000 (26%)] Loss: 21322.515625\n",
      "Train Epoch: 5 [60096/225000 (27%)] Loss: 21171.408203\n",
      "Train Epoch: 5 [62592/225000 (28%)] Loss: 21115.023438\n",
      "Train Epoch: 5 [65088/225000 (29%)] Loss: 21084.775391\n",
      "Train Epoch: 5 [67584/225000 (30%)] Loss: 21219.167969\n",
      "Train Epoch: 5 [70080/225000 (31%)] Loss: 20713.537109\n",
      "Train Epoch: 5 [72576/225000 (32%)] Loss: 20514.558594\n",
      "Train Epoch: 5 [75072/225000 (33%)] Loss: 20932.941406\n",
      "Train Epoch: 5 [77568/225000 (34%)] Loss: 21635.947266\n",
      "Train Epoch: 5 [80064/225000 (36%)] Loss: 20956.890625\n",
      "Train Epoch: 5 [82560/225000 (37%)] Loss: 21278.328125\n",
      "Train Epoch: 5 [85056/225000 (38%)] Loss: 20867.531250\n",
      "Train Epoch: 5 [87552/225000 (39%)] Loss: 20950.667969\n",
      "Train Epoch: 5 [90048/225000 (40%)] Loss: 21169.703125\n",
      "Train Epoch: 5 [92544/225000 (41%)] Loss: 20769.636719\n",
      "Train Epoch: 5 [95040/225000 (42%)] Loss: 20674.742188\n",
      "Train Epoch: 5 [97536/225000 (43%)] Loss: 20756.044922\n",
      "Train Epoch: 5 [100032/225000 (44%)] Loss: 21413.292969\n",
      "Train Epoch: 5 [102528/225000 (46%)] Loss: 20748.347656\n",
      "Train Epoch: 5 [105024/225000 (47%)] Loss: 21078.248047\n",
      "Train Epoch: 5 [107520/225000 (48%)] Loss: 20679.423828\n",
      "Train Epoch: 5 [110016/225000 (49%)] Loss: 20893.488281\n",
      "Train Epoch: 5 [112512/225000 (50%)] Loss: 21272.486328\n",
      "Train Epoch: 5 [115008/225000 (51%)] Loss: 20606.148438\n",
      "Train Epoch: 5 [117504/225000 (52%)] Loss: 20856.136719\n",
      "Train Epoch: 5 [120000/225000 (53%)] Loss: 21207.451172\n",
      "Train Epoch: 5 [122496/225000 (54%)] Loss: 21374.097656\n",
      "Train Epoch: 5 [124992/225000 (56%)] Loss: 21256.664062\n",
      "Train Epoch: 5 [127488/225000 (57%)] Loss: 21332.195312\n",
      "Train Epoch: 5 [129984/225000 (58%)] Loss: 20774.628906\n",
      "Train Epoch: 5 [132480/225000 (59%)] Loss: 20814.578125\n",
      "Train Epoch: 5 [134976/225000 (60%)] Loss: 20737.539062\n",
      "Train Epoch: 5 [137472/225000 (61%)] Loss: 21372.109375\n",
      "Train Epoch: 5 [139968/225000 (62%)] Loss: 20536.716797\n",
      "Train Epoch: 5 [142464/225000 (63%)] Loss: 21531.630859\n",
      "Train Epoch: 5 [144960/225000 (64%)] Loss: 21194.617188\n",
      "Train Epoch: 5 [147456/225000 (66%)] Loss: 21171.367188\n",
      "Train Epoch: 5 [149952/225000 (67%)] Loss: 21620.375000\n",
      "Train Epoch: 5 [152448/225000 (68%)] Loss: 20988.322266\n",
      "Train Epoch: 5 [154944/225000 (69%)] Loss: 21559.830078\n",
      "Train Epoch: 5 [157440/225000 (70%)] Loss: 21041.644531\n",
      "Train Epoch: 5 [159936/225000 (71%)] Loss: 20685.097656\n",
      "Train Epoch: 5 [162432/225000 (72%)] Loss: 20993.794922\n",
      "Train Epoch: 5 [164928/225000 (73%)] Loss: 20924.265625\n",
      "Train Epoch: 5 [167424/225000 (74%)] Loss: 20915.097656\n",
      "Train Epoch: 5 [169920/225000 (76%)] Loss: 20281.191406\n",
      "Train Epoch: 5 [172416/225000 (77%)] Loss: 20685.558594\n",
      "Train Epoch: 5 [174912/225000 (78%)] Loss: 20454.576172\n",
      "Train Epoch: 5 [177408/225000 (79%)] Loss: 21128.109375\n",
      "Train Epoch: 5 [179904/225000 (80%)] Loss: 21259.699219\n",
      "Train Epoch: 5 [182400/225000 (81%)] Loss: 20756.500000\n",
      "Train Epoch: 5 [184896/225000 (82%)] Loss: 20862.730469\n",
      "Train Epoch: 5 [187392/225000 (83%)] Loss: 21159.570312\n",
      "Train Epoch: 5 [189888/225000 (84%)] Loss: 21227.441406\n",
      "Train Epoch: 5 [192384/225000 (86%)] Loss: 21293.484375\n",
      "Train Epoch: 5 [194880/225000 (87%)] Loss: 21412.052734\n",
      "Train Epoch: 5 [197376/225000 (88%)] Loss: 20930.750000\n",
      "Train Epoch: 5 [199872/225000 (89%)] Loss: 21105.812500\n",
      "Train Epoch: 5 [202368/225000 (90%)] Loss: 21878.218750\n",
      "Train Epoch: 5 [204864/225000 (91%)] Loss: 20753.535156\n",
      "Train Epoch: 5 [207360/225000 (92%)] Loss: 20539.621094\n",
      "Train Epoch: 5 [209856/225000 (93%)] Loss: 20880.289062\n",
      "Train Epoch: 5 [212352/225000 (94%)] Loss: 21516.550781\n",
      "Train Epoch: 5 [214848/225000 (95%)] Loss: 20976.626953\n",
      "Train Epoch: 5 [217344/225000 (97%)] Loss: 21247.488281\n",
      "Train Epoch: 5 [219840/225000 (98%)] Loss: 20806.365234\n",
      "Train Epoch: 5 [222336/225000 (99%)] Loss: 21305.312500\n",
      "Train Epoch: 5 [224832/225000 (100%)] Loss: 21058.996094\n",
      "    epoch          : 5\n",
      "    loss           : 21137.57281223336\n",
      "    val_loss       : 21053.74821007684\n",
      "Train Epoch: 6 [192/225000 (0%)] Loss: 21157.169922\n",
      "Train Epoch: 6 [2688/225000 (1%)] Loss: 20789.597656\n",
      "Train Epoch: 6 [5184/225000 (2%)] Loss: 21555.066406\n",
      "Train Epoch: 6 [7680/225000 (3%)] Loss: 21147.013672\n",
      "Train Epoch: 6 [10176/225000 (5%)] Loss: 21237.269531\n",
      "Train Epoch: 6 [12672/225000 (6%)] Loss: 21116.945312\n",
      "Train Epoch: 6 [15168/225000 (7%)] Loss: 21102.355469\n",
      "Train Epoch: 6 [17664/225000 (8%)] Loss: 21175.769531\n",
      "Train Epoch: 6 [20160/225000 (9%)] Loss: 20478.710938\n",
      "Train Epoch: 6 [22656/225000 (10%)] Loss: 20840.015625\n",
      "Train Epoch: 6 [25152/225000 (11%)] Loss: 20962.335938\n",
      "Train Epoch: 6 [27648/225000 (12%)] Loss: 21017.675781\n",
      "Train Epoch: 6 [30144/225000 (13%)] Loss: 20777.707031\n",
      "Train Epoch: 6 [32640/225000 (15%)] Loss: 20455.851562\n",
      "Train Epoch: 6 [35136/225000 (16%)] Loss: 20912.990234\n",
      "Train Epoch: 6 [37632/225000 (17%)] Loss: 21831.042969\n",
      "Train Epoch: 6 [40128/225000 (18%)] Loss: 20973.847656\n",
      "Train Epoch: 6 [42624/225000 (19%)] Loss: 21202.841797\n",
      "Train Epoch: 6 [45120/225000 (20%)] Loss: 20888.865234\n",
      "Train Epoch: 6 [47616/225000 (21%)] Loss: 21218.109375\n",
      "Train Epoch: 6 [50112/225000 (22%)] Loss: 20766.544922\n",
      "Train Epoch: 6 [52608/225000 (23%)] Loss: 21439.457031\n",
      "Train Epoch: 6 [55104/225000 (24%)] Loss: 21068.218750\n",
      "Train Epoch: 6 [57600/225000 (26%)] Loss: 21326.148438\n",
      "Train Epoch: 6 [60096/225000 (27%)] Loss: 21098.273438\n",
      "Train Epoch: 6 [62592/225000 (28%)] Loss: 20484.230469\n",
      "Train Epoch: 6 [65088/225000 (29%)] Loss: 21419.781250\n",
      "Train Epoch: 6 [67584/225000 (30%)] Loss: 20819.703125\n",
      "Train Epoch: 6 [70080/225000 (31%)] Loss: 21986.095703\n",
      "Train Epoch: 6 [72576/225000 (32%)] Loss: 20976.437500\n",
      "Train Epoch: 6 [75072/225000 (33%)] Loss: 20456.296875\n",
      "Train Epoch: 6 [77568/225000 (34%)] Loss: 21103.398438\n",
      "Train Epoch: 6 [80064/225000 (36%)] Loss: 21618.367188\n",
      "Train Epoch: 6 [82560/225000 (37%)] Loss: 20786.429688\n",
      "Train Epoch: 6 [85056/225000 (38%)] Loss: 21248.187500\n",
      "Train Epoch: 6 [87552/225000 (39%)] Loss: 20938.560547\n",
      "Train Epoch: 6 [90048/225000 (40%)] Loss: 20921.894531\n",
      "Train Epoch: 6 [92544/225000 (41%)] Loss: 21078.958984\n",
      "Train Epoch: 6 [95040/225000 (42%)] Loss: 20833.949219\n",
      "Train Epoch: 6 [97536/225000 (43%)] Loss: 20874.355469\n",
      "Train Epoch: 6 [100032/225000 (44%)] Loss: 21028.699219\n",
      "Train Epoch: 6 [102528/225000 (46%)] Loss: 21352.583984\n",
      "Train Epoch: 6 [105024/225000 (47%)] Loss: 20527.867188\n",
      "Train Epoch: 6 [107520/225000 (48%)] Loss: 20819.960938\n",
      "Train Epoch: 6 [110016/225000 (49%)] Loss: 20769.167969\n",
      "Train Epoch: 6 [112512/225000 (50%)] Loss: 20907.275391\n",
      "Train Epoch: 6 [115008/225000 (51%)] Loss: 20754.375000\n",
      "Train Epoch: 6 [117504/225000 (52%)] Loss: 20894.314453\n",
      "Train Epoch: 6 [120000/225000 (53%)] Loss: 21205.855469\n",
      "Train Epoch: 6 [122496/225000 (54%)] Loss: 20883.591797\n",
      "Train Epoch: 6 [124992/225000 (56%)] Loss: 21356.386719\n",
      "Train Epoch: 6 [127488/225000 (57%)] Loss: 20817.464844\n",
      "Train Epoch: 6 [129984/225000 (58%)] Loss: 20971.339844\n",
      "Train Epoch: 6 [132480/225000 (59%)] Loss: 20605.882812\n",
      "Train Epoch: 6 [134976/225000 (60%)] Loss: 21637.466797\n",
      "Train Epoch: 6 [137472/225000 (61%)] Loss: 20778.480469\n",
      "Train Epoch: 6 [139968/225000 (62%)] Loss: 21061.556641\n",
      "Train Epoch: 6 [142464/225000 (63%)] Loss: 21249.523438\n",
      "Train Epoch: 6 [144960/225000 (64%)] Loss: 20940.820312\n",
      "Train Epoch: 6 [147456/225000 (66%)] Loss: 20754.001953\n",
      "Train Epoch: 6 [149952/225000 (67%)] Loss: 20801.908203\n",
      "Train Epoch: 6 [152448/225000 (68%)] Loss: 20687.867188\n",
      "Train Epoch: 6 [154944/225000 (69%)] Loss: 21218.611328\n",
      "Train Epoch: 6 [157440/225000 (70%)] Loss: 21130.648438\n",
      "Train Epoch: 6 [159936/225000 (71%)] Loss: 20719.140625\n",
      "Train Epoch: 6 [162432/225000 (72%)] Loss: 20667.320312\n",
      "Train Epoch: 6 [164928/225000 (73%)] Loss: 20849.191406\n",
      "Train Epoch: 6 [167424/225000 (74%)] Loss: 21215.386719\n",
      "Train Epoch: 6 [169920/225000 (76%)] Loss: 20626.378906\n",
      "Train Epoch: 6 [172416/225000 (77%)] Loss: 21782.765625\n",
      "Train Epoch: 6 [174912/225000 (78%)] Loss: 20928.250000\n",
      "Train Epoch: 6 [177408/225000 (79%)] Loss: 21104.933594\n",
      "Train Epoch: 6 [179904/225000 (80%)] Loss: 20660.007812\n",
      "Train Epoch: 6 [182400/225000 (81%)] Loss: 20301.031250\n",
      "Train Epoch: 6 [184896/225000 (82%)] Loss: 20635.847656\n",
      "Train Epoch: 6 [187392/225000 (83%)] Loss: 20557.984375\n",
      "Train Epoch: 6 [189888/225000 (84%)] Loss: 20746.148438\n",
      "Train Epoch: 6 [192384/225000 (86%)] Loss: 20943.531250\n",
      "Train Epoch: 6 [194880/225000 (87%)] Loss: 21056.470703\n",
      "Train Epoch: 6 [197376/225000 (88%)] Loss: 21412.636719\n",
      "Train Epoch: 6 [199872/225000 (89%)] Loss: 21073.015625\n",
      "Train Epoch: 6 [202368/225000 (90%)] Loss: 20767.810547\n",
      "Train Epoch: 6 [204864/225000 (91%)] Loss: 20698.390625\n",
      "Train Epoch: 6 [207360/225000 (92%)] Loss: 21251.339844\n",
      "Train Epoch: 6 [209856/225000 (93%)] Loss: 20346.000000\n",
      "Train Epoch: 6 [212352/225000 (94%)] Loss: 20808.773438\n",
      "Train Epoch: 6 [214848/225000 (95%)] Loss: 20874.593750\n",
      "Train Epoch: 6 [217344/225000 (97%)] Loss: 20779.367188\n",
      "Train Epoch: 6 [219840/225000 (98%)] Loss: 20666.439453\n",
      "Train Epoch: 6 [222336/225000 (99%)] Loss: 21261.496094\n",
      "Train Epoch: 6 [224832/225000 (100%)] Loss: 20933.398438\n",
      "    epoch          : 6\n",
      "    loss           : 21010.087454004904\n",
      "    val_loss       : 20869.71012652193\n",
      "Train Epoch: 7 [192/225000 (0%)] Loss: 21101.720703\n",
      "Train Epoch: 7 [2688/225000 (1%)] Loss: 21177.531250\n",
      "Train Epoch: 7 [5184/225000 (2%)] Loss: 21111.824219\n",
      "Train Epoch: 7 [7680/225000 (3%)] Loss: 21196.683594\n",
      "Train Epoch: 7 [10176/225000 (5%)] Loss: 21387.617188\n",
      "Train Epoch: 7 [12672/225000 (6%)] Loss: 20545.417969\n",
      "Train Epoch: 7 [15168/225000 (7%)] Loss: 20699.429688\n",
      "Train Epoch: 7 [17664/225000 (8%)] Loss: 20976.587891\n",
      "Train Epoch: 7 [20160/225000 (9%)] Loss: 21080.892578\n",
      "Train Epoch: 7 [22656/225000 (10%)] Loss: 21374.820312\n",
      "Train Epoch: 7 [25152/225000 (11%)] Loss: 20874.605469\n",
      "Train Epoch: 7 [27648/225000 (12%)] Loss: 21275.662109\n",
      "Train Epoch: 7 [30144/225000 (13%)] Loss: 20232.791016\n",
      "Train Epoch: 7 [32640/225000 (15%)] Loss: 20674.132812\n",
      "Train Epoch: 7 [35136/225000 (16%)] Loss: 21130.455078\n",
      "Train Epoch: 7 [37632/225000 (17%)] Loss: 20781.125000\n",
      "Train Epoch: 7 [40128/225000 (18%)] Loss: 20502.595703\n",
      "Train Epoch: 7 [42624/225000 (19%)] Loss: 20948.810547\n",
      "Train Epoch: 7 [45120/225000 (20%)] Loss: 20858.085938\n",
      "Train Epoch: 7 [47616/225000 (21%)] Loss: 20746.449219\n",
      "Train Epoch: 7 [50112/225000 (22%)] Loss: 21083.675781\n",
      "Train Epoch: 7 [52608/225000 (23%)] Loss: 20874.931641\n",
      "Train Epoch: 7 [55104/225000 (24%)] Loss: 21234.644531\n",
      "Train Epoch: 7 [57600/225000 (26%)] Loss: 21636.566406\n",
      "Train Epoch: 7 [60096/225000 (27%)] Loss: 21024.560547\n",
      "Train Epoch: 7 [62592/225000 (28%)] Loss: 20658.449219\n",
      "Train Epoch: 7 [65088/225000 (29%)] Loss: 21212.617188\n",
      "Train Epoch: 7 [67584/225000 (30%)] Loss: 20763.341797\n",
      "Train Epoch: 7 [70080/225000 (31%)] Loss: 20926.173828\n",
      "Train Epoch: 7 [72576/225000 (32%)] Loss: 20459.660156\n",
      "Train Epoch: 7 [75072/225000 (33%)] Loss: 20843.283203\n",
      "Train Epoch: 7 [77568/225000 (34%)] Loss: 20629.785156\n",
      "Train Epoch: 7 [80064/225000 (36%)] Loss: 21070.949219\n",
      "Train Epoch: 7 [82560/225000 (37%)] Loss: 20733.517578\n",
      "Train Epoch: 7 [85056/225000 (38%)] Loss: 20984.625000\n",
      "Train Epoch: 7 [87552/225000 (39%)] Loss: 20824.193359\n",
      "Train Epoch: 7 [90048/225000 (40%)] Loss: 20556.550781\n",
      "Train Epoch: 7 [92544/225000 (41%)] Loss: 20737.269531\n",
      "Train Epoch: 7 [95040/225000 (42%)] Loss: 20752.640625\n",
      "Train Epoch: 7 [97536/225000 (43%)] Loss: 20911.816406\n",
      "Train Epoch: 7 [100032/225000 (44%)] Loss: 20854.214844\n",
      "Train Epoch: 7 [102528/225000 (46%)] Loss: 20838.312500\n",
      "Train Epoch: 7 [105024/225000 (47%)] Loss: 20754.246094\n",
      "Train Epoch: 7 [107520/225000 (48%)] Loss: 20977.720703\n",
      "Train Epoch: 7 [110016/225000 (49%)] Loss: 20652.007812\n",
      "Train Epoch: 7 [112512/225000 (50%)] Loss: 20805.992188\n",
      "Train Epoch: 7 [115008/225000 (51%)] Loss: 20511.900391\n",
      "Train Epoch: 7 [117504/225000 (52%)] Loss: 20545.261719\n",
      "Train Epoch: 7 [120000/225000 (53%)] Loss: 21000.738281\n",
      "Train Epoch: 7 [122496/225000 (54%)] Loss: 20831.402344\n",
      "Train Epoch: 7 [124992/225000 (56%)] Loss: 20640.460938\n",
      "Train Epoch: 7 [127488/225000 (57%)] Loss: 21379.404297\n",
      "Train Epoch: 7 [129984/225000 (58%)] Loss: 20801.824219\n",
      "Train Epoch: 7 [132480/225000 (59%)] Loss: 20408.796875\n",
      "Train Epoch: 7 [134976/225000 (60%)] Loss: 21213.285156\n",
      "Train Epoch: 7 [137472/225000 (61%)] Loss: 21052.716797\n",
      "Train Epoch: 7 [139968/225000 (62%)] Loss: 21157.867188\n",
      "Train Epoch: 7 [142464/225000 (63%)] Loss: 20313.867188\n",
      "Train Epoch: 7 [144960/225000 (64%)] Loss: 20765.558594\n",
      "Train Epoch: 7 [147456/225000 (66%)] Loss: 20967.984375\n",
      "Train Epoch: 7 [149952/225000 (67%)] Loss: 21460.421875\n",
      "Train Epoch: 7 [152448/225000 (68%)] Loss: 21432.533203\n",
      "Train Epoch: 7 [154944/225000 (69%)] Loss: 20961.734375\n",
      "Train Epoch: 7 [157440/225000 (70%)] Loss: 20417.429688\n",
      "Train Epoch: 7 [159936/225000 (71%)] Loss: 20869.126953\n",
      "Train Epoch: 7 [162432/225000 (72%)] Loss: 20728.542969\n",
      "Train Epoch: 7 [164928/225000 (73%)] Loss: 21202.320312\n",
      "Train Epoch: 7 [167424/225000 (74%)] Loss: 20902.640625\n",
      "Train Epoch: 7 [169920/225000 (76%)] Loss: 20596.484375\n",
      "Train Epoch: 7 [172416/225000 (77%)] Loss: 20512.453125\n",
      "Train Epoch: 7 [174912/225000 (78%)] Loss: 21713.125000\n",
      "Train Epoch: 7 [177408/225000 (79%)] Loss: 20512.357422\n",
      "Train Epoch: 7 [179904/225000 (80%)] Loss: 21359.378906\n",
      "Train Epoch: 7 [182400/225000 (81%)] Loss: 20950.509766\n",
      "Train Epoch: 7 [184896/225000 (82%)] Loss: 20951.847656\n",
      "Train Epoch: 7 [187392/225000 (83%)] Loss: 21104.548828\n",
      "Train Epoch: 7 [189888/225000 (84%)] Loss: 20841.332031\n",
      "Train Epoch: 7 [192384/225000 (86%)] Loss: 20527.097656\n",
      "Train Epoch: 7 [194880/225000 (87%)] Loss: 20832.392578\n",
      "Train Epoch: 7 [197376/225000 (88%)] Loss: 20638.568359\n",
      "Train Epoch: 7 [199872/225000 (89%)] Loss: 20891.023438\n",
      "Train Epoch: 7 [202368/225000 (90%)] Loss: 20936.724609\n",
      "Train Epoch: 7 [204864/225000 (91%)] Loss: 20755.328125\n",
      "Train Epoch: 7 [207360/225000 (92%)] Loss: 20842.720703\n",
      "Train Epoch: 7 [209856/225000 (93%)] Loss: 20582.962891\n",
      "Train Epoch: 7 [212352/225000 (94%)] Loss: 21107.216797\n",
      "Train Epoch: 7 [214848/225000 (95%)] Loss: 21123.246094\n",
      "Train Epoch: 7 [217344/225000 (97%)] Loss: 20502.164062\n",
      "Train Epoch: 7 [219840/225000 (98%)] Loss: 20525.785156\n",
      "Train Epoch: 7 [222336/225000 (99%)] Loss: 20934.253906\n",
      "Train Epoch: 7 [224832/225000 (100%)] Loss: 20792.203125\n",
      "    epoch          : 7\n",
      "    loss           : 20909.208757732507\n",
      "    val_loss       : 20762.819365686133\n",
      "Train Epoch: 8 [192/225000 (0%)] Loss: 21807.187500\n",
      "Train Epoch: 8 [2688/225000 (1%)] Loss: 20556.681641\n",
      "Train Epoch: 8 [5184/225000 (2%)] Loss: 20829.312500\n",
      "Train Epoch: 8 [7680/225000 (3%)] Loss: 20841.765625\n",
      "Train Epoch: 8 [10176/225000 (5%)] Loss: 20594.144531\n",
      "Train Epoch: 8 [12672/225000 (6%)] Loss: 21110.714844\n",
      "Train Epoch: 8 [15168/225000 (7%)] Loss: 21008.613281\n",
      "Train Epoch: 8 [17664/225000 (8%)] Loss: 20612.320312\n",
      "Train Epoch: 8 [20160/225000 (9%)] Loss: 20502.048828\n",
      "Train Epoch: 8 [22656/225000 (10%)] Loss: 20711.597656\n",
      "Train Epoch: 8 [25152/225000 (11%)] Loss: 20526.931641\n",
      "Train Epoch: 8 [27648/225000 (12%)] Loss: 20444.042969\n",
      "Train Epoch: 8 [30144/225000 (13%)] Loss: 20885.476562\n",
      "Train Epoch: 8 [32640/225000 (15%)] Loss: 21029.236328\n",
      "Train Epoch: 8 [35136/225000 (16%)] Loss: 20851.183594\n",
      "Train Epoch: 8 [37632/225000 (17%)] Loss: 21124.060547\n",
      "Train Epoch: 8 [40128/225000 (18%)] Loss: 21106.160156\n",
      "Train Epoch: 8 [42624/225000 (19%)] Loss: 20731.074219\n",
      "Train Epoch: 8 [45120/225000 (20%)] Loss: 20909.855469\n",
      "Train Epoch: 8 [47616/225000 (21%)] Loss: 21072.644531\n",
      "Train Epoch: 8 [50112/225000 (22%)] Loss: 20688.107422\n",
      "Train Epoch: 8 [52608/225000 (23%)] Loss: 21142.966797\n",
      "Train Epoch: 8 [55104/225000 (24%)] Loss: 20977.703125\n",
      "Train Epoch: 8 [57600/225000 (26%)] Loss: 21004.589844\n",
      "Train Epoch: 8 [60096/225000 (27%)] Loss: 21121.824219\n",
      "Train Epoch: 8 [62592/225000 (28%)] Loss: 20660.277344\n",
      "Train Epoch: 8 [65088/225000 (29%)] Loss: 20989.056641\n",
      "Train Epoch: 8 [67584/225000 (30%)] Loss: 20974.246094\n",
      "Train Epoch: 8 [70080/225000 (31%)] Loss: 21282.791016\n",
      "Train Epoch: 8 [72576/225000 (32%)] Loss: 20828.210938\n",
      "Train Epoch: 8 [75072/225000 (33%)] Loss: 20624.960938\n",
      "Train Epoch: 8 [77568/225000 (34%)] Loss: 20871.289062\n",
      "Train Epoch: 8 [80064/225000 (36%)] Loss: 20900.996094\n",
      "Train Epoch: 8 [82560/225000 (37%)] Loss: 20338.902344\n",
      "Train Epoch: 8 [85056/225000 (38%)] Loss: 21127.406250\n",
      "Train Epoch: 8 [87552/225000 (39%)] Loss: 20719.289062\n",
      "Train Epoch: 8 [90048/225000 (40%)] Loss: 21230.289062\n",
      "Train Epoch: 8 [92544/225000 (41%)] Loss: 20638.183594\n",
      "Train Epoch: 8 [95040/225000 (42%)] Loss: 20809.593750\n",
      "Train Epoch: 8 [97536/225000 (43%)] Loss: 20727.695312\n",
      "Train Epoch: 8 [100032/225000 (44%)] Loss: 20467.472656\n",
      "Train Epoch: 8 [102528/225000 (46%)] Loss: 20976.734375\n",
      "Train Epoch: 8 [105024/225000 (47%)] Loss: 20712.962891\n",
      "Train Epoch: 8 [107520/225000 (48%)] Loss: 20611.277344\n",
      "Train Epoch: 8 [110016/225000 (49%)] Loss: 20940.054688\n",
      "Train Epoch: 8 [112512/225000 (50%)] Loss: 21284.906250\n",
      "Train Epoch: 8 [115008/225000 (51%)] Loss: 20888.718750\n",
      "Train Epoch: 8 [117504/225000 (52%)] Loss: 20878.730469\n",
      "Train Epoch: 8 [120000/225000 (53%)] Loss: 20824.445312\n",
      "Train Epoch: 8 [122496/225000 (54%)] Loss: 20870.062500\n",
      "Train Epoch: 8 [124992/225000 (56%)] Loss: 21069.449219\n",
      "Train Epoch: 8 [127488/225000 (57%)] Loss: 20702.148438\n",
      "Train Epoch: 8 [129984/225000 (58%)] Loss: 20467.261719\n",
      "Train Epoch: 8 [132480/225000 (59%)] Loss: 20940.265625\n",
      "Train Epoch: 8 [134976/225000 (60%)] Loss: 20868.556641\n",
      "Train Epoch: 8 [137472/225000 (61%)] Loss: 21143.380859\n",
      "Train Epoch: 8 [139968/225000 (62%)] Loss: 20228.214844\n",
      "Train Epoch: 8 [142464/225000 (63%)] Loss: 20593.406250\n",
      "Train Epoch: 8 [144960/225000 (64%)] Loss: 20846.718750\n",
      "Train Epoch: 8 [147456/225000 (66%)] Loss: 20832.101562\n",
      "Train Epoch: 8 [149952/225000 (67%)] Loss: 20547.570312\n",
      "Train Epoch: 8 [152448/225000 (68%)] Loss: 20291.621094\n",
      "Train Epoch: 8 [154944/225000 (69%)] Loss: 20873.625000\n",
      "Train Epoch: 8 [157440/225000 (70%)] Loss: 20081.171875\n",
      "Train Epoch: 8 [159936/225000 (71%)] Loss: 20674.535156\n",
      "Train Epoch: 8 [162432/225000 (72%)] Loss: 21281.042969\n",
      "Train Epoch: 8 [164928/225000 (73%)] Loss: 20861.998047\n",
      "Train Epoch: 8 [167424/225000 (74%)] Loss: 21142.328125\n",
      "Train Epoch: 8 [169920/225000 (76%)] Loss: 20924.623047\n",
      "Train Epoch: 8 [172416/225000 (77%)] Loss: 20598.953125\n",
      "Train Epoch: 8 [174912/225000 (78%)] Loss: 20872.332031\n",
      "Train Epoch: 8 [177408/225000 (79%)] Loss: 20690.066406\n",
      "Train Epoch: 8 [179904/225000 (80%)] Loss: 20372.839844\n",
      "Train Epoch: 8 [182400/225000 (81%)] Loss: 20775.394531\n",
      "Train Epoch: 8 [184896/225000 (82%)] Loss: 20548.523438\n",
      "Train Epoch: 8 [187392/225000 (83%)] Loss: 20718.585938\n",
      "Train Epoch: 8 [189888/225000 (84%)] Loss: 21354.976562\n",
      "Train Epoch: 8 [192384/225000 (86%)] Loss: 20813.277344\n",
      "Train Epoch: 8 [194880/225000 (87%)] Loss: 20481.175781\n",
      "Train Epoch: 8 [197376/225000 (88%)] Loss: 20857.210938\n",
      "Train Epoch: 8 [199872/225000 (89%)] Loss: 20738.039062\n",
      "Train Epoch: 8 [202368/225000 (90%)] Loss: 20667.027344\n",
      "Train Epoch: 8 [204864/225000 (91%)] Loss: 20648.050781\n",
      "Train Epoch: 8 [207360/225000 (92%)] Loss: 21107.179688\n",
      "Train Epoch: 8 [209856/225000 (93%)] Loss: 20870.804688\n",
      "Train Epoch: 8 [212352/225000 (94%)] Loss: 20492.875000\n",
      "Train Epoch: 8 [214848/225000 (95%)] Loss: 20951.222656\n",
      "Train Epoch: 8 [217344/225000 (97%)] Loss: 20823.296875\n",
      "Train Epoch: 8 [219840/225000 (98%)] Loss: 20664.306641\n",
      "Train Epoch: 8 [222336/225000 (99%)] Loss: 20628.195312\n",
      "Train Epoch: 8 [224832/225000 (100%)] Loss: 21003.843750\n",
      "    epoch          : 8\n",
      "    loss           : 20841.687205031463\n",
      "    val_loss       : 20968.142700413257\n",
      "Train Epoch: 9 [192/225000 (0%)] Loss: 20705.787109\n",
      "Train Epoch: 9 [2688/225000 (1%)] Loss: 20858.679688\n",
      "Train Epoch: 9 [5184/225000 (2%)] Loss: 20955.703125\n",
      "Train Epoch: 9 [7680/225000 (3%)] Loss: 20801.091797\n",
      "Train Epoch: 9 [10176/225000 (5%)] Loss: 20682.115234\n",
      "Train Epoch: 9 [12672/225000 (6%)] Loss: 20601.451172\n",
      "Train Epoch: 9 [15168/225000 (7%)] Loss: 20627.105469\n",
      "Train Epoch: 9 [17664/225000 (8%)] Loss: 20944.806641\n",
      "Train Epoch: 9 [20160/225000 (9%)] Loss: 20860.501953\n",
      "Train Epoch: 9 [22656/225000 (10%)] Loss: 21084.996094\n",
      "Train Epoch: 9 [25152/225000 (11%)] Loss: 21178.291016\n",
      "Train Epoch: 9 [27648/225000 (12%)] Loss: 20493.812500\n",
      "Train Epoch: 9 [30144/225000 (13%)] Loss: 21244.476562\n",
      "Train Epoch: 9 [32640/225000 (15%)] Loss: 20571.898438\n",
      "Train Epoch: 9 [35136/225000 (16%)] Loss: 20653.675781\n",
      "Train Epoch: 9 [37632/225000 (17%)] Loss: 20571.236328\n",
      "Train Epoch: 9 [40128/225000 (18%)] Loss: 21094.312500\n",
      "Train Epoch: 9 [42624/225000 (19%)] Loss: 21300.167969\n",
      "Train Epoch: 9 [45120/225000 (20%)] Loss: 20954.746094\n",
      "Train Epoch: 9 [47616/225000 (21%)] Loss: 20709.076172\n",
      "Train Epoch: 9 [50112/225000 (22%)] Loss: 20831.554688\n",
      "Train Epoch: 9 [52608/225000 (23%)] Loss: 21111.089844\n",
      "Train Epoch: 9 [55104/225000 (24%)] Loss: 20923.876953\n",
      "Train Epoch: 9 [57600/225000 (26%)] Loss: 20716.738281\n",
      "Train Epoch: 9 [60096/225000 (27%)] Loss: 20904.878906\n",
      "Train Epoch: 9 [62592/225000 (28%)] Loss: 20715.460938\n",
      "Train Epoch: 9 [65088/225000 (29%)] Loss: 20585.984375\n",
      "Train Epoch: 9 [67584/225000 (30%)] Loss: 21044.312500\n",
      "Train Epoch: 9 [70080/225000 (31%)] Loss: 21090.621094\n",
      "Train Epoch: 9 [72576/225000 (32%)] Loss: 20955.968750\n",
      "Train Epoch: 9 [75072/225000 (33%)] Loss: 21256.761719\n",
      "Train Epoch: 9 [77568/225000 (34%)] Loss: 20587.398438\n",
      "Train Epoch: 9 [80064/225000 (36%)] Loss: 20658.445312\n",
      "Train Epoch: 9 [82560/225000 (37%)] Loss: 20487.832031\n",
      "Train Epoch: 9 [85056/225000 (38%)] Loss: 20554.941406\n",
      "Train Epoch: 9 [87552/225000 (39%)] Loss: 20684.421875\n",
      "Train Epoch: 9 [90048/225000 (40%)] Loss: 21033.765625\n",
      "Train Epoch: 9 [92544/225000 (41%)] Loss: 20643.304688\n",
      "Train Epoch: 9 [95040/225000 (42%)] Loss: 20646.839844\n",
      "Train Epoch: 9 [97536/225000 (43%)] Loss: 20733.597656\n",
      "Train Epoch: 9 [100032/225000 (44%)] Loss: 20803.601562\n",
      "Train Epoch: 9 [102528/225000 (46%)] Loss: 21157.363281\n",
      "Train Epoch: 9 [105024/225000 (47%)] Loss: 21090.890625\n",
      "Train Epoch: 9 [107520/225000 (48%)] Loss: 21182.605469\n",
      "Train Epoch: 9 [110016/225000 (49%)] Loss: 20955.171875\n",
      "Train Epoch: 9 [112512/225000 (50%)] Loss: 19864.656250\n",
      "Train Epoch: 9 [115008/225000 (51%)] Loss: 20767.781250\n",
      "Train Epoch: 9 [117504/225000 (52%)] Loss: 21195.582031\n",
      "Train Epoch: 9 [120000/225000 (53%)] Loss: 20959.244141\n",
      "Train Epoch: 9 [122496/225000 (54%)] Loss: 20658.398438\n",
      "Train Epoch: 9 [124992/225000 (56%)] Loss: 21031.607422\n",
      "Train Epoch: 9 [127488/225000 (57%)] Loss: 20571.917969\n",
      "Train Epoch: 9 [129984/225000 (58%)] Loss: 20763.298828\n",
      "Train Epoch: 9 [132480/225000 (59%)] Loss: 20547.306641\n",
      "Train Epoch: 9 [134976/225000 (60%)] Loss: 20878.904297\n",
      "Train Epoch: 9 [137472/225000 (61%)] Loss: 20547.521484\n",
      "Train Epoch: 9 [139968/225000 (62%)] Loss: 20468.945312\n",
      "Train Epoch: 9 [142464/225000 (63%)] Loss: 20849.445312\n",
      "Train Epoch: 9 [144960/225000 (64%)] Loss: 20545.615234\n",
      "Train Epoch: 9 [147456/225000 (66%)] Loss: 20520.199219\n",
      "Train Epoch: 9 [149952/225000 (67%)] Loss: 20778.451172\n",
      "Train Epoch: 9 [152448/225000 (68%)] Loss: 20707.613281\n",
      "Train Epoch: 9 [154944/225000 (69%)] Loss: 21073.433594\n",
      "Train Epoch: 9 [157440/225000 (70%)] Loss: 20800.382812\n",
      "Train Epoch: 9 [159936/225000 (71%)] Loss: 20951.259766\n",
      "Train Epoch: 9 [162432/225000 (72%)] Loss: 20584.269531\n",
      "Train Epoch: 9 [164928/225000 (73%)] Loss: 21203.175781\n",
      "Train Epoch: 9 [167424/225000 (74%)] Loss: 21113.253906\n",
      "Train Epoch: 9 [169920/225000 (76%)] Loss: 20629.748047\n",
      "Train Epoch: 9 [172416/225000 (77%)] Loss: 20650.916016\n",
      "Train Epoch: 9 [174912/225000 (78%)] Loss: 21003.072266\n",
      "Train Epoch: 9 [177408/225000 (79%)] Loss: 20319.910156\n",
      "Train Epoch: 9 [179904/225000 (80%)] Loss: 20741.154297\n",
      "Train Epoch: 9 [182400/225000 (81%)] Loss: 20541.558594\n",
      "Train Epoch: 9 [184896/225000 (82%)] Loss: 20858.812500\n",
      "Train Epoch: 9 [187392/225000 (83%)] Loss: 20641.410156\n",
      "Train Epoch: 9 [189888/225000 (84%)] Loss: 20752.375000\n",
      "Train Epoch: 9 [192384/225000 (86%)] Loss: 20977.400391\n",
      "Train Epoch: 9 [194880/225000 (87%)] Loss: 21039.988281\n",
      "Train Epoch: 9 [197376/225000 (88%)] Loss: 20602.703125\n",
      "Train Epoch: 9 [199872/225000 (89%)] Loss: 20682.427734\n",
      "Train Epoch: 9 [202368/225000 (90%)] Loss: 20556.152344\n",
      "Train Epoch: 9 [204864/225000 (91%)] Loss: 20580.214844\n",
      "Train Epoch: 9 [207360/225000 (92%)] Loss: 20638.359375\n",
      "Train Epoch: 9 [209856/225000 (93%)] Loss: 20480.238281\n",
      "Train Epoch: 9 [212352/225000 (94%)] Loss: 20700.664062\n",
      "Train Epoch: 9 [214848/225000 (95%)] Loss: 20358.464844\n",
      "Train Epoch: 9 [217344/225000 (97%)] Loss: 21205.923828\n",
      "Train Epoch: 9 [219840/225000 (98%)] Loss: 21363.394531\n",
      "Train Epoch: 9 [222336/225000 (99%)] Loss: 20689.751953\n",
      "Train Epoch: 9 [224832/225000 (100%)] Loss: 21093.742188\n",
      "    epoch          : 9\n",
      "    loss           : 20879.810721856335\n",
      "    val_loss       : 20792.072276242816\n",
      "Train Epoch: 10 [192/225000 (0%)] Loss: 20859.085938\n",
      "Train Epoch: 10 [2688/225000 (1%)] Loss: 20165.753906\n",
      "Train Epoch: 10 [5184/225000 (2%)] Loss: 20859.726562\n",
      "Train Epoch: 10 [7680/225000 (3%)] Loss: 21219.181641\n",
      "Train Epoch: 10 [10176/225000 (5%)] Loss: 20454.578125\n",
      "Train Epoch: 10 [12672/225000 (6%)] Loss: 20549.154297\n",
      "Train Epoch: 10 [15168/225000 (7%)] Loss: 20969.878906\n",
      "Train Epoch: 10 [17664/225000 (8%)] Loss: 20899.101562\n",
      "Train Epoch: 10 [20160/225000 (9%)] Loss: 20359.660156\n",
      "Train Epoch: 10 [22656/225000 (10%)] Loss: 20935.800781\n",
      "Train Epoch: 10 [25152/225000 (11%)] Loss: 20445.458984\n",
      "Train Epoch: 10 [27648/225000 (12%)] Loss: 20172.273438\n",
      "Train Epoch: 10 [30144/225000 (13%)] Loss: 36748.695312\n",
      "Train Epoch: 10 [32640/225000 (15%)] Loss: 20354.925781\n",
      "Train Epoch: 10 [35136/225000 (16%)] Loss: 20959.292969\n",
      "Train Epoch: 10 [37632/225000 (17%)] Loss: 20741.554688\n",
      "Train Epoch: 10 [40128/225000 (18%)] Loss: 20723.431641\n",
      "Train Epoch: 10 [42624/225000 (19%)] Loss: 20905.382812\n",
      "Train Epoch: 10 [45120/225000 (20%)] Loss: 20198.972656\n",
      "Train Epoch: 10 [47616/225000 (21%)] Loss: 20585.419922\n",
      "Train Epoch: 10 [50112/225000 (22%)] Loss: 20569.945312\n",
      "Train Epoch: 10 [52608/225000 (23%)] Loss: 19935.148438\n",
      "Train Epoch: 10 [55104/225000 (24%)] Loss: 20536.636719\n",
      "Train Epoch: 10 [57600/225000 (26%)] Loss: 21204.167969\n",
      "Train Epoch: 10 [60096/225000 (27%)] Loss: 20950.554688\n",
      "Train Epoch: 10 [62592/225000 (28%)] Loss: 21156.945312\n",
      "Train Epoch: 10 [65088/225000 (29%)] Loss: 20733.869141\n",
      "Train Epoch: 10 [67584/225000 (30%)] Loss: 21177.238281\n",
      "Train Epoch: 10 [70080/225000 (31%)] Loss: 20742.998047\n",
      "Train Epoch: 10 [72576/225000 (32%)] Loss: 20615.468750\n",
      "Train Epoch: 10 [75072/225000 (33%)] Loss: 20637.074219\n",
      "Train Epoch: 10 [77568/225000 (34%)] Loss: 20941.570312\n",
      "Train Epoch: 10 [80064/225000 (36%)] Loss: 20988.644531\n",
      "Train Epoch: 10 [82560/225000 (37%)] Loss: 20942.652344\n",
      "Train Epoch: 10 [85056/225000 (38%)] Loss: 20990.605469\n",
      "Train Epoch: 10 [87552/225000 (39%)] Loss: 20431.941406\n",
      "Train Epoch: 10 [90048/225000 (40%)] Loss: 20704.861328\n",
      "Train Epoch: 10 [92544/225000 (41%)] Loss: 20847.808594\n",
      "Train Epoch: 10 [95040/225000 (42%)] Loss: 20922.175781\n",
      "Train Epoch: 10 [97536/225000 (43%)] Loss: 21158.076172\n",
      "Train Epoch: 10 [100032/225000 (44%)] Loss: 20765.595703\n",
      "Train Epoch: 10 [102528/225000 (46%)] Loss: 20821.087891\n",
      "Train Epoch: 10 [105024/225000 (47%)] Loss: 20603.410156\n",
      "Train Epoch: 10 [107520/225000 (48%)] Loss: 20475.691406\n",
      "Train Epoch: 10 [110016/225000 (49%)] Loss: 21046.224609\n",
      "Train Epoch: 10 [112512/225000 (50%)] Loss: 21217.763672\n",
      "Train Epoch: 10 [115008/225000 (51%)] Loss: 20678.996094\n",
      "Train Epoch: 10 [117504/225000 (52%)] Loss: 20431.718750\n",
      "Train Epoch: 10 [120000/225000 (53%)] Loss: 20415.023438\n",
      "Train Epoch: 10 [122496/225000 (54%)] Loss: 20887.433594\n",
      "Train Epoch: 10 [124992/225000 (56%)] Loss: 20709.199219\n",
      "Train Epoch: 10 [127488/225000 (57%)] Loss: 20627.400391\n",
      "Train Epoch: 10 [129984/225000 (58%)] Loss: 21227.837891\n",
      "Train Epoch: 10 [132480/225000 (59%)] Loss: 20240.937500\n",
      "Train Epoch: 10 [134976/225000 (60%)] Loss: 20834.867188\n",
      "Train Epoch: 10 [137472/225000 (61%)] Loss: 21424.669922\n",
      "Train Epoch: 10 [139968/225000 (62%)] Loss: 21208.144531\n",
      "Train Epoch: 10 [142464/225000 (63%)] Loss: 20652.794922\n",
      "Train Epoch: 10 [144960/225000 (64%)] Loss: 20840.673828\n",
      "Train Epoch: 10 [147456/225000 (66%)] Loss: 20820.335938\n",
      "Train Epoch: 10 [149952/225000 (67%)] Loss: 20758.335938\n",
      "Train Epoch: 10 [152448/225000 (68%)] Loss: 20832.800781\n",
      "Train Epoch: 10 [154944/225000 (69%)] Loss: 20491.257812\n",
      "Train Epoch: 10 [157440/225000 (70%)] Loss: 20599.164062\n",
      "Train Epoch: 10 [159936/225000 (71%)] Loss: 21068.160156\n",
      "Train Epoch: 10 [162432/225000 (72%)] Loss: 20733.789062\n",
      "Train Epoch: 10 [164928/225000 (73%)] Loss: 20652.656250\n",
      "Train Epoch: 10 [167424/225000 (74%)] Loss: 20541.957031\n",
      "Train Epoch: 10 [169920/225000 (76%)] Loss: 21251.216797\n",
      "Train Epoch: 10 [172416/225000 (77%)] Loss: 20831.341797\n",
      "Train Epoch: 10 [174912/225000 (78%)] Loss: 20323.134766\n",
      "Train Epoch: 10 [177408/225000 (79%)] Loss: 20862.281250\n",
      "Train Epoch: 10 [179904/225000 (80%)] Loss: 20508.910156\n",
      "Train Epoch: 10 [182400/225000 (81%)] Loss: 20860.628906\n",
      "Train Epoch: 10 [184896/225000 (82%)] Loss: 20982.066406\n",
      "Train Epoch: 10 [187392/225000 (83%)] Loss: 20535.056641\n",
      "Train Epoch: 10 [189888/225000 (84%)] Loss: 20972.775391\n",
      "Train Epoch: 10 [192384/225000 (86%)] Loss: 20686.541016\n",
      "Train Epoch: 10 [194880/225000 (87%)] Loss: 20822.636719\n",
      "Train Epoch: 10 [197376/225000 (88%)] Loss: 20814.125000\n",
      "Train Epoch: 10 [199872/225000 (89%)] Loss: 21027.556641\n",
      "Train Epoch: 10 [202368/225000 (90%)] Loss: 20756.753906\n",
      "Train Epoch: 10 [204864/225000 (91%)] Loss: 20570.613281\n",
      "Train Epoch: 10 [207360/225000 (92%)] Loss: 20836.382812\n",
      "Train Epoch: 10 [209856/225000 (93%)] Loss: 20407.632812\n",
      "Train Epoch: 10 [212352/225000 (94%)] Loss: 20541.677734\n",
      "Train Epoch: 10 [214848/225000 (95%)] Loss: 20454.402344\n",
      "Train Epoch: 10 [217344/225000 (97%)] Loss: 20839.855469\n",
      "Train Epoch: 10 [219840/225000 (98%)] Loss: 20889.318359\n",
      "Train Epoch: 10 [222336/225000 (99%)] Loss: 20775.736328\n",
      "Train Epoch: 10 [224832/225000 (100%)] Loss: 20623.968750\n",
      "    epoch          : 10\n",
      "    loss           : 20806.254887811967\n",
      "    val_loss       : 20645.834268437087\n",
      "Train Epoch: 11 [192/225000 (0%)] Loss: 20343.429688\n",
      "Train Epoch: 11 [2688/225000 (1%)] Loss: 20888.046875\n",
      "Train Epoch: 11 [5184/225000 (2%)] Loss: 20541.841797\n",
      "Train Epoch: 11 [7680/225000 (3%)] Loss: 20699.410156\n",
      "Train Epoch: 11 [10176/225000 (5%)] Loss: 20760.261719\n",
      "Train Epoch: 11 [12672/225000 (6%)] Loss: 20971.394531\n",
      "Train Epoch: 11 [15168/225000 (7%)] Loss: 20941.687500\n",
      "Train Epoch: 11 [17664/225000 (8%)] Loss: 20304.353516\n",
      "Train Epoch: 11 [20160/225000 (9%)] Loss: 20778.521484\n",
      "Train Epoch: 11 [22656/225000 (10%)] Loss: 21133.468750\n",
      "Train Epoch: 11 [25152/225000 (11%)] Loss: 20738.164062\n",
      "Train Epoch: 11 [27648/225000 (12%)] Loss: 20827.625000\n",
      "Train Epoch: 11 [30144/225000 (13%)] Loss: 20843.888672\n",
      "Train Epoch: 11 [32640/225000 (15%)] Loss: 20847.583984\n",
      "Train Epoch: 11 [35136/225000 (16%)] Loss: 20620.355469\n",
      "Train Epoch: 11 [37632/225000 (17%)] Loss: 20727.914062\n",
      "Train Epoch: 11 [40128/225000 (18%)] Loss: 20658.640625\n",
      "Train Epoch: 11 [42624/225000 (19%)] Loss: 20914.906250\n",
      "Train Epoch: 11 [45120/225000 (20%)] Loss: 20576.707031\n",
      "Train Epoch: 11 [47616/225000 (21%)] Loss: 21051.039062\n",
      "Train Epoch: 11 [50112/225000 (22%)] Loss: 20755.550781\n",
      "Train Epoch: 11 [52608/225000 (23%)] Loss: 20431.546875\n",
      "Train Epoch: 11 [55104/225000 (24%)] Loss: 21000.132812\n",
      "Train Epoch: 11 [57600/225000 (26%)] Loss: 20915.083984\n",
      "Train Epoch: 11 [60096/225000 (27%)] Loss: 20531.484375\n",
      "Train Epoch: 11 [62592/225000 (28%)] Loss: 20877.095703\n",
      "Train Epoch: 11 [65088/225000 (29%)] Loss: 20817.263672\n",
      "Train Epoch: 11 [67584/225000 (30%)] Loss: 20563.910156\n",
      "Train Epoch: 11 [70080/225000 (31%)] Loss: 20945.699219\n",
      "Train Epoch: 11 [72576/225000 (32%)] Loss: 20723.433594\n",
      "Train Epoch: 11 [75072/225000 (33%)] Loss: 20482.794922\n",
      "Train Epoch: 11 [77568/225000 (34%)] Loss: 20311.609375\n",
      "Train Epoch: 11 [80064/225000 (36%)] Loss: 20517.406250\n",
      "Train Epoch: 11 [82560/225000 (37%)] Loss: 20390.611328\n",
      "Train Epoch: 11 [85056/225000 (38%)] Loss: 20683.234375\n",
      "Train Epoch: 11 [87552/225000 (39%)] Loss: 20765.773438\n",
      "Train Epoch: 11 [90048/225000 (40%)] Loss: 20969.640625\n",
      "Train Epoch: 11 [92544/225000 (41%)] Loss: 21237.214844\n",
      "Train Epoch: 11 [95040/225000 (42%)] Loss: 20708.500000\n",
      "Train Epoch: 11 [97536/225000 (43%)] Loss: 20660.271484\n",
      "Train Epoch: 11 [100032/225000 (44%)] Loss: 21167.564453\n",
      "Train Epoch: 11 [102528/225000 (46%)] Loss: 20662.234375\n",
      "Train Epoch: 11 [105024/225000 (47%)] Loss: 20581.691406\n",
      "Train Epoch: 11 [107520/225000 (48%)] Loss: 20984.097656\n",
      "Train Epoch: 11 [110016/225000 (49%)] Loss: 20710.488281\n",
      "Train Epoch: 11 [112512/225000 (50%)] Loss: 20749.847656\n",
      "Train Epoch: 11 [115008/225000 (51%)] Loss: 20614.207031\n",
      "Train Epoch: 11 [117504/225000 (52%)] Loss: 20700.677734\n",
      "Train Epoch: 11 [120000/225000 (53%)] Loss: 20727.570312\n",
      "Train Epoch: 11 [122496/225000 (54%)] Loss: 20759.570312\n",
      "Train Epoch: 11 [124992/225000 (56%)] Loss: 20651.574219\n",
      "Train Epoch: 11 [127488/225000 (57%)] Loss: 20687.003906\n",
      "Train Epoch: 11 [129984/225000 (58%)] Loss: 20867.769531\n",
      "Train Epoch: 11 [132480/225000 (59%)] Loss: 20922.724609\n",
      "Train Epoch: 11 [134976/225000 (60%)] Loss: 21428.675781\n",
      "Train Epoch: 11 [137472/225000 (61%)] Loss: 20742.474609\n",
      "Train Epoch: 11 [139968/225000 (62%)] Loss: 20975.757812\n",
      "Train Epoch: 11 [142464/225000 (63%)] Loss: 20637.138672\n",
      "Train Epoch: 11 [144960/225000 (64%)] Loss: 20878.746094\n",
      "Train Epoch: 11 [147456/225000 (66%)] Loss: 20816.878906\n",
      "Train Epoch: 11 [149952/225000 (67%)] Loss: 20879.496094\n",
      "Train Epoch: 11 [152448/225000 (68%)] Loss: 20078.210938\n",
      "Train Epoch: 11 [154944/225000 (69%)] Loss: 20917.181641\n",
      "Train Epoch: 11 [157440/225000 (70%)] Loss: 19808.527344\n",
      "Train Epoch: 11 [159936/225000 (71%)] Loss: 20474.593750\n",
      "Train Epoch: 11 [162432/225000 (72%)] Loss: 20838.949219\n",
      "Train Epoch: 11 [164928/225000 (73%)] Loss: 20360.423828\n",
      "Train Epoch: 11 [167424/225000 (74%)] Loss: 20836.378906\n",
      "Train Epoch: 11 [169920/225000 (76%)] Loss: 21445.558594\n",
      "Train Epoch: 11 [172416/225000 (77%)] Loss: 20458.179688\n",
      "Train Epoch: 11 [174912/225000 (78%)] Loss: 20828.458984\n",
      "Train Epoch: 11 [177408/225000 (79%)] Loss: 20692.806641\n",
      "Train Epoch: 11 [179904/225000 (80%)] Loss: 21327.199219\n",
      "Train Epoch: 11 [182400/225000 (81%)] Loss: 21160.910156\n",
      "Train Epoch: 11 [184896/225000 (82%)] Loss: 20597.820312\n",
      "Train Epoch: 11 [187392/225000 (83%)] Loss: 20824.296875\n",
      "Train Epoch: 11 [189888/225000 (84%)] Loss: 20211.296875\n",
      "Train Epoch: 11 [192384/225000 (86%)] Loss: 20201.742188\n",
      "Train Epoch: 11 [194880/225000 (87%)] Loss: 20649.669922\n",
      "Train Epoch: 11 [197376/225000 (88%)] Loss: 20840.343750\n",
      "Train Epoch: 11 [199872/225000 (89%)] Loss: 21007.296875\n",
      "Train Epoch: 11 [202368/225000 (90%)] Loss: 20811.615234\n",
      "Train Epoch: 11 [204864/225000 (91%)] Loss: 21227.683594\n",
      "Train Epoch: 11 [207360/225000 (92%)] Loss: 21114.664062\n",
      "Train Epoch: 11 [209856/225000 (93%)] Loss: 20467.820312\n",
      "Train Epoch: 11 [212352/225000 (94%)] Loss: 20617.992188\n",
      "Train Epoch: 11 [214848/225000 (95%)] Loss: 20875.324219\n",
      "Train Epoch: 11 [217344/225000 (97%)] Loss: 20736.527344\n",
      "Train Epoch: 11 [219840/225000 (98%)] Loss: 21044.175781\n",
      "Train Epoch: 11 [222336/225000 (99%)] Loss: 20746.351562\n",
      "Train Epoch: 11 [224832/225000 (100%)] Loss: 20871.476562\n",
      "    epoch          : 11\n",
      "    loss           : 20724.35826011892\n",
      "    val_loss       : 20600.261942988134\n",
      "Train Epoch: 12 [192/225000 (0%)] Loss: 20146.390625\n",
      "Train Epoch: 12 [2688/225000 (1%)] Loss: 20507.789062\n",
      "Train Epoch: 12 [5184/225000 (2%)] Loss: 20248.875000\n",
      "Train Epoch: 12 [7680/225000 (3%)] Loss: 21128.457031\n",
      "Train Epoch: 12 [10176/225000 (5%)] Loss: 20342.603516\n",
      "Train Epoch: 12 [12672/225000 (6%)] Loss: 20784.828125\n",
      "Train Epoch: 12 [15168/225000 (7%)] Loss: 20713.894531\n",
      "Train Epoch: 12 [17664/225000 (8%)] Loss: 20466.996094\n",
      "Train Epoch: 12 [20160/225000 (9%)] Loss: 20893.195312\n",
      "Train Epoch: 12 [22656/225000 (10%)] Loss: 21367.433594\n",
      "Train Epoch: 12 [25152/225000 (11%)] Loss: 21304.203125\n",
      "Train Epoch: 12 [27648/225000 (12%)] Loss: 21269.406250\n",
      "Train Epoch: 12 [30144/225000 (13%)] Loss: 20612.265625\n",
      "Train Epoch: 12 [32640/225000 (15%)] Loss: 20749.410156\n",
      "Train Epoch: 12 [35136/225000 (16%)] Loss: 20724.589844\n",
      "Train Epoch: 12 [37632/225000 (17%)] Loss: 20733.871094\n",
      "Train Epoch: 12 [40128/225000 (18%)] Loss: 20831.242188\n",
      "Train Epoch: 12 [42624/225000 (19%)] Loss: 20598.273438\n",
      "Train Epoch: 12 [45120/225000 (20%)] Loss: 20778.816406\n",
      "Train Epoch: 12 [47616/225000 (21%)] Loss: 20313.730469\n",
      "Train Epoch: 12 [50112/225000 (22%)] Loss: 20815.609375\n",
      "Train Epoch: 12 [52608/225000 (23%)] Loss: 20860.531250\n",
      "Train Epoch: 12 [55104/225000 (24%)] Loss: 20512.568359\n",
      "Train Epoch: 12 [57600/225000 (26%)] Loss: 20919.789062\n",
      "Train Epoch: 12 [60096/225000 (27%)] Loss: 20541.712891\n",
      "Train Epoch: 12 [62592/225000 (28%)] Loss: 20981.566406\n",
      "Train Epoch: 12 [65088/225000 (29%)] Loss: 21040.925781\n",
      "Train Epoch: 12 [67584/225000 (30%)] Loss: 20432.011719\n",
      "Train Epoch: 12 [70080/225000 (31%)] Loss: 20527.156250\n",
      "Train Epoch: 12 [72576/225000 (32%)] Loss: 20438.011719\n",
      "Train Epoch: 12 [75072/225000 (33%)] Loss: 21066.078125\n",
      "Train Epoch: 12 [77568/225000 (34%)] Loss: 20941.671875\n",
      "Train Epoch: 12 [80064/225000 (36%)] Loss: 20835.039062\n",
      "Train Epoch: 12 [82560/225000 (37%)] Loss: 20648.347656\n",
      "Train Epoch: 12 [85056/225000 (38%)] Loss: 20455.250000\n",
      "Train Epoch: 12 [87552/225000 (39%)] Loss: 20318.664062\n",
      "Train Epoch: 12 [90048/225000 (40%)] Loss: 20827.480469\n",
      "Train Epoch: 12 [92544/225000 (41%)] Loss: 20774.240234\n",
      "Train Epoch: 12 [95040/225000 (42%)] Loss: 21146.355469\n",
      "Train Epoch: 12 [97536/225000 (43%)] Loss: 20132.828125\n",
      "Train Epoch: 12 [100032/225000 (44%)] Loss: 20604.675781\n",
      "Train Epoch: 12 [102528/225000 (46%)] Loss: 20347.695312\n",
      "Train Epoch: 12 [105024/225000 (47%)] Loss: 21167.003906\n",
      "Train Epoch: 12 [107520/225000 (48%)] Loss: 20101.992188\n",
      "Train Epoch: 12 [110016/225000 (49%)] Loss: 20456.328125\n",
      "Train Epoch: 12 [112512/225000 (50%)] Loss: 20697.308594\n",
      "Train Epoch: 12 [115008/225000 (51%)] Loss: 20731.585938\n",
      "Train Epoch: 12 [117504/225000 (52%)] Loss: 20841.976562\n",
      "Train Epoch: 12 [120000/225000 (53%)] Loss: 20758.416016\n",
      "Train Epoch: 12 [122496/225000 (54%)] Loss: 20905.976562\n",
      "Train Epoch: 12 [124992/225000 (56%)] Loss: 20664.548828\n",
      "Train Epoch: 12 [127488/225000 (57%)] Loss: 20352.095703\n",
      "Train Epoch: 12 [129984/225000 (58%)] Loss: 20862.070312\n",
      "Train Epoch: 12 [132480/225000 (59%)] Loss: 20607.921875\n",
      "Train Epoch: 12 [134976/225000 (60%)] Loss: 20490.292969\n",
      "Train Epoch: 12 [137472/225000 (61%)] Loss: 21169.343750\n",
      "Train Epoch: 12 [139968/225000 (62%)] Loss: 20499.269531\n",
      "Train Epoch: 12 [142464/225000 (63%)] Loss: 20921.470703\n",
      "Train Epoch: 12 [144960/225000 (64%)] Loss: 20960.003906\n",
      "Train Epoch: 12 [147456/225000 (66%)] Loss: 21290.746094\n",
      "Train Epoch: 12 [149952/225000 (67%)] Loss: 20751.621094\n",
      "Train Epoch: 12 [152448/225000 (68%)] Loss: 20482.201172\n",
      "Train Epoch: 12 [154944/225000 (69%)] Loss: 20815.441406\n",
      "Train Epoch: 12 [157440/225000 (70%)] Loss: 21152.835938\n",
      "Train Epoch: 12 [159936/225000 (71%)] Loss: 20887.001953\n",
      "Train Epoch: 12 [162432/225000 (72%)] Loss: 20886.226562\n",
      "Train Epoch: 12 [164928/225000 (73%)] Loss: 36524.367188\n",
      "Train Epoch: 12 [167424/225000 (74%)] Loss: 20853.470703\n",
      "Train Epoch: 12 [169920/225000 (76%)] Loss: 20532.781250\n",
      "Train Epoch: 12 [172416/225000 (77%)] Loss: 20874.164062\n",
      "Train Epoch: 12 [174912/225000 (78%)] Loss: 20536.906250\n",
      "Train Epoch: 12 [177408/225000 (79%)] Loss: 20394.386719\n",
      "Train Epoch: 12 [179904/225000 (80%)] Loss: 20629.949219\n",
      "Train Epoch: 12 [182400/225000 (81%)] Loss: 20532.826172\n",
      "Train Epoch: 12 [184896/225000 (82%)] Loss: 20730.632812\n",
      "Train Epoch: 12 [187392/225000 (83%)] Loss: 20893.949219\n",
      "Train Epoch: 12 [189888/225000 (84%)] Loss: 20683.187500\n",
      "Train Epoch: 12 [192384/225000 (86%)] Loss: 20630.789062\n",
      "Train Epoch: 12 [194880/225000 (87%)] Loss: 20520.480469\n",
      "Train Epoch: 12 [197376/225000 (88%)] Loss: 20893.914062\n",
      "Train Epoch: 12 [199872/225000 (89%)] Loss: 20592.808594\n",
      "Train Epoch: 12 [202368/225000 (90%)] Loss: 20868.789062\n",
      "Train Epoch: 12 [204864/225000 (91%)] Loss: 21163.585938\n",
      "Train Epoch: 12 [207360/225000 (92%)] Loss: 20376.050781\n",
      "Train Epoch: 12 [209856/225000 (93%)] Loss: 20681.755859\n",
      "Train Epoch: 12 [212352/225000 (94%)] Loss: 20315.078125\n",
      "Train Epoch: 12 [214848/225000 (95%)] Loss: 20245.128906\n",
      "Train Epoch: 12 [217344/225000 (97%)] Loss: 20330.179688\n",
      "Train Epoch: 12 [219840/225000 (98%)] Loss: 20350.060547\n",
      "Train Epoch: 12 [222336/225000 (99%)] Loss: 20537.710938\n",
      "Train Epoch: 12 [224832/225000 (100%)] Loss: 20607.722656\n",
      "    epoch          : 12\n",
      "    loss           : 20677.722234628305\n",
      "    val_loss       : 20552.658972171426\n",
      "Train Epoch: 13 [192/225000 (0%)] Loss: 20968.445312\n",
      "Train Epoch: 13 [2688/225000 (1%)] Loss: 20630.644531\n",
      "Train Epoch: 13 [5184/225000 (2%)] Loss: 20778.271484\n",
      "Train Epoch: 13 [7680/225000 (3%)] Loss: 21065.378906\n",
      "Train Epoch: 13 [10176/225000 (5%)] Loss: 20602.210938\n",
      "Train Epoch: 13 [12672/225000 (6%)] Loss: 20906.261719\n",
      "Train Epoch: 13 [15168/225000 (7%)] Loss: 20983.654297\n",
      "Train Epoch: 13 [17664/225000 (8%)] Loss: 21113.058594\n",
      "Train Epoch: 13 [20160/225000 (9%)] Loss: 20442.214844\n",
      "Train Epoch: 13 [22656/225000 (10%)] Loss: 20870.009766\n",
      "Train Epoch: 13 [25152/225000 (11%)] Loss: 20195.628906\n",
      "Train Epoch: 13 [27648/225000 (12%)] Loss: 20361.681641\n",
      "Train Epoch: 13 [30144/225000 (13%)] Loss: 20771.585938\n",
      "Train Epoch: 13 [32640/225000 (15%)] Loss: 21173.460938\n",
      "Train Epoch: 13 [35136/225000 (16%)] Loss: 20269.712891\n",
      "Train Epoch: 13 [37632/225000 (17%)] Loss: 20922.806641\n",
      "Train Epoch: 13 [40128/225000 (18%)] Loss: 20183.292969\n",
      "Train Epoch: 13 [42624/225000 (19%)] Loss: 20490.203125\n",
      "Train Epoch: 13 [45120/225000 (20%)] Loss: 20913.496094\n",
      "Train Epoch: 13 [47616/225000 (21%)] Loss: 20536.402344\n",
      "Train Epoch: 13 [50112/225000 (22%)] Loss: 20482.150391\n",
      "Train Epoch: 13 [52608/225000 (23%)] Loss: 20680.152344\n",
      "Train Epoch: 13 [55104/225000 (24%)] Loss: 20793.164062\n",
      "Train Epoch: 13 [57600/225000 (26%)] Loss: 20434.945312\n",
      "Train Epoch: 13 [60096/225000 (27%)] Loss: 20438.207031\n",
      "Train Epoch: 13 [62592/225000 (28%)] Loss: 20496.742188\n",
      "Train Epoch: 13 [65088/225000 (29%)] Loss: 20655.574219\n",
      "Train Epoch: 13 [67584/225000 (30%)] Loss: 20867.277344\n",
      "Train Epoch: 13 [70080/225000 (31%)] Loss: 20392.335938\n",
      "Train Epoch: 13 [72576/225000 (32%)] Loss: 21000.216797\n",
      "Train Epoch: 13 [75072/225000 (33%)] Loss: 20316.496094\n",
      "Train Epoch: 13 [77568/225000 (34%)] Loss: 20623.851562\n",
      "Train Epoch: 13 [80064/225000 (36%)] Loss: 20815.072266\n",
      "Train Epoch: 13 [82560/225000 (37%)] Loss: 20386.812500\n",
      "Train Epoch: 13 [85056/225000 (38%)] Loss: 21063.894531\n",
      "Train Epoch: 13 [87552/225000 (39%)] Loss: 21150.082031\n",
      "Train Epoch: 13 [90048/225000 (40%)] Loss: 20633.863281\n",
      "Train Epoch: 13 [92544/225000 (41%)] Loss: 20685.398438\n",
      "Train Epoch: 13 [95040/225000 (42%)] Loss: 20193.072266\n",
      "Train Epoch: 13 [97536/225000 (43%)] Loss: 20799.152344\n",
      "Train Epoch: 13 [100032/225000 (44%)] Loss: 20615.804688\n",
      "Train Epoch: 13 [102528/225000 (46%)] Loss: 21125.058594\n",
      "Train Epoch: 13 [105024/225000 (47%)] Loss: 20369.205078\n",
      "Train Epoch: 13 [107520/225000 (48%)] Loss: 21000.132812\n",
      "Train Epoch: 13 [110016/225000 (49%)] Loss: 20423.560547\n",
      "Train Epoch: 13 [112512/225000 (50%)] Loss: 20534.183594\n",
      "Train Epoch: 13 [115008/225000 (51%)] Loss: 20413.535156\n",
      "Train Epoch: 13 [117504/225000 (52%)] Loss: 20201.125000\n",
      "Train Epoch: 13 [120000/225000 (53%)] Loss: 20553.089844\n",
      "Train Epoch: 13 [122496/225000 (54%)] Loss: 20743.529297\n",
      "Train Epoch: 13 [124992/225000 (56%)] Loss: 21156.277344\n",
      "Train Epoch: 13 [127488/225000 (57%)] Loss: 20544.464844\n",
      "Train Epoch: 13 [129984/225000 (58%)] Loss: 20345.873047\n",
      "Train Epoch: 13 [132480/225000 (59%)] Loss: 20332.457031\n",
      "Train Epoch: 13 [134976/225000 (60%)] Loss: 20538.273438\n",
      "Train Epoch: 13 [137472/225000 (61%)] Loss: 20613.312500\n",
      "Train Epoch: 13 [139968/225000 (62%)] Loss: 21120.964844\n",
      "Train Epoch: 13 [142464/225000 (63%)] Loss: 20713.779297\n",
      "Train Epoch: 13 [144960/225000 (64%)] Loss: 20533.390625\n",
      "Train Epoch: 13 [147456/225000 (66%)] Loss: 20734.964844\n",
      "Train Epoch: 13 [149952/225000 (67%)] Loss: 20462.501953\n",
      "Train Epoch: 13 [152448/225000 (68%)] Loss: 20552.675781\n",
      "Train Epoch: 13 [154944/225000 (69%)] Loss: 20350.238281\n",
      "Train Epoch: 13 [157440/225000 (70%)] Loss: 20482.960938\n",
      "Train Epoch: 13 [159936/225000 (71%)] Loss: 21255.742188\n",
      "Train Epoch: 13 [162432/225000 (72%)] Loss: 20470.828125\n",
      "Train Epoch: 13 [164928/225000 (73%)] Loss: 20925.621094\n",
      "Train Epoch: 13 [167424/225000 (74%)] Loss: 20618.785156\n",
      "Train Epoch: 13 [169920/225000 (76%)] Loss: 20640.925781\n",
      "Train Epoch: 13 [172416/225000 (77%)] Loss: 20165.976562\n",
      "Train Epoch: 13 [174912/225000 (78%)] Loss: 20433.425781\n",
      "Train Epoch: 13 [177408/225000 (79%)] Loss: 20445.429688\n",
      "Train Epoch: 13 [179904/225000 (80%)] Loss: 20720.484375\n",
      "Train Epoch: 13 [182400/225000 (81%)] Loss: 20526.449219\n",
      "Train Epoch: 13 [184896/225000 (82%)] Loss: 20947.695312\n",
      "Train Epoch: 13 [187392/225000 (83%)] Loss: 20337.445312\n",
      "Train Epoch: 13 [189888/225000 (84%)] Loss: 20485.560547\n",
      "Train Epoch: 13 [192384/225000 (86%)] Loss: 20719.607422\n",
      "Train Epoch: 13 [194880/225000 (87%)] Loss: 20805.023438\n",
      "Train Epoch: 13 [197376/225000 (88%)] Loss: 20608.306641\n",
      "Train Epoch: 13 [199872/225000 (89%)] Loss: 20831.736328\n",
      "Train Epoch: 13 [202368/225000 (90%)] Loss: 20186.498047\n",
      "Train Epoch: 13 [204864/225000 (91%)] Loss: 20607.339844\n",
      "Train Epoch: 13 [207360/225000 (92%)] Loss: 21253.767578\n",
      "Train Epoch: 13 [209856/225000 (93%)] Loss: 20670.416016\n",
      "Train Epoch: 13 [212352/225000 (94%)] Loss: 20430.664062\n",
      "Train Epoch: 13 [214848/225000 (95%)] Loss: 20430.593750\n",
      "Train Epoch: 13 [217344/225000 (97%)] Loss: 20789.789062\n",
      "Train Epoch: 13 [219840/225000 (98%)] Loss: 21016.160156\n",
      "Train Epoch: 13 [222336/225000 (99%)] Loss: 21013.521484\n",
      "Train Epoch: 13 [224832/225000 (100%)] Loss: 20696.851562\n",
      "    epoch          : 13\n",
      "    loss           : 20646.23622146971\n",
      "    val_loss       : 20577.352355438336\n",
      "Train Epoch: 14 [192/225000 (0%)] Loss: 20771.777344\n",
      "Train Epoch: 14 [2688/225000 (1%)] Loss: 20424.585938\n",
      "Train Epoch: 14 [5184/225000 (2%)] Loss: 20497.191406\n",
      "Train Epoch: 14 [7680/225000 (3%)] Loss: 20440.037109\n",
      "Train Epoch: 14 [10176/225000 (5%)] Loss: 20269.207031\n",
      "Train Epoch: 14 [12672/225000 (6%)] Loss: 20744.255859\n",
      "Train Epoch: 14 [15168/225000 (7%)] Loss: 20751.939453\n",
      "Train Epoch: 14 [17664/225000 (8%)] Loss: 20664.087891\n",
      "Train Epoch: 14 [20160/225000 (9%)] Loss: 20102.816406\n",
      "Train Epoch: 14 [22656/225000 (10%)] Loss: 20746.835938\n",
      "Train Epoch: 14 [25152/225000 (11%)] Loss: 20435.378906\n",
      "Train Epoch: 14 [27648/225000 (12%)] Loss: 20803.515625\n",
      "Train Epoch: 14 [30144/225000 (13%)] Loss: 20643.917969\n",
      "Train Epoch: 14 [32640/225000 (15%)] Loss: 20700.664062\n",
      "Train Epoch: 14 [35136/225000 (16%)] Loss: 20565.107422\n",
      "Train Epoch: 14 [37632/225000 (17%)] Loss: 20383.117188\n",
      "Train Epoch: 14 [40128/225000 (18%)] Loss: 20430.183594\n",
      "Train Epoch: 14 [42624/225000 (19%)] Loss: 20577.062500\n",
      "Train Epoch: 14 [45120/225000 (20%)] Loss: 20592.009766\n",
      "Train Epoch: 14 [47616/225000 (21%)] Loss: 20505.460938\n",
      "Train Epoch: 14 [50112/225000 (22%)] Loss: 20493.509766\n",
      "Train Epoch: 14 [52608/225000 (23%)] Loss: 20541.234375\n",
      "Train Epoch: 14 [55104/225000 (24%)] Loss: 20987.269531\n",
      "Train Epoch: 14 [57600/225000 (26%)] Loss: 20408.611328\n",
      "Train Epoch: 14 [60096/225000 (27%)] Loss: 20054.570312\n",
      "Train Epoch: 14 [62592/225000 (28%)] Loss: 20691.496094\n",
      "Train Epoch: 14 [65088/225000 (29%)] Loss: 20180.457031\n",
      "Train Epoch: 14 [67584/225000 (30%)] Loss: 20370.531250\n",
      "Train Epoch: 14 [70080/225000 (31%)] Loss: 20385.378906\n",
      "Train Epoch: 14 [72576/225000 (32%)] Loss: 20715.871094\n",
      "Train Epoch: 14 [75072/225000 (33%)] Loss: 20498.750000\n",
      "Train Epoch: 14 [77568/225000 (34%)] Loss: 20889.931641\n",
      "Train Epoch: 14 [80064/225000 (36%)] Loss: 20594.828125\n",
      "Train Epoch: 14 [82560/225000 (37%)] Loss: 20467.439453\n",
      "Train Epoch: 14 [85056/225000 (38%)] Loss: 21067.535156\n",
      "Train Epoch: 14 [87552/225000 (39%)] Loss: 20464.496094\n",
      "Train Epoch: 14 [90048/225000 (40%)] Loss: 20425.992188\n",
      "Train Epoch: 14 [92544/225000 (41%)] Loss: 20264.890625\n",
      "Train Epoch: 14 [95040/225000 (42%)] Loss: 20588.669922\n",
      "Train Epoch: 14 [97536/225000 (43%)] Loss: 20370.546875\n",
      "Train Epoch: 14 [100032/225000 (44%)] Loss: 21040.292969\n",
      "Train Epoch: 14 [102528/225000 (46%)] Loss: 20547.091797\n",
      "Train Epoch: 14 [105024/225000 (47%)] Loss: 20738.919922\n",
      "Train Epoch: 14 [107520/225000 (48%)] Loss: 20747.136719\n",
      "Train Epoch: 14 [110016/225000 (49%)] Loss: 20852.125000\n",
      "Train Epoch: 14 [112512/225000 (50%)] Loss: 20147.232422\n",
      "Train Epoch: 14 [115008/225000 (51%)] Loss: 20711.136719\n",
      "Train Epoch: 14 [117504/225000 (52%)] Loss: 20727.621094\n",
      "Train Epoch: 14 [120000/225000 (53%)] Loss: 20411.294922\n",
      "Train Epoch: 14 [122496/225000 (54%)] Loss: 20212.859375\n",
      "Train Epoch: 14 [124992/225000 (56%)] Loss: 20673.496094\n",
      "Train Epoch: 14 [127488/225000 (57%)] Loss: 20583.496094\n",
      "Train Epoch: 14 [129984/225000 (58%)] Loss: 20173.769531\n",
      "Train Epoch: 14 [132480/225000 (59%)] Loss: 20259.039062\n",
      "Train Epoch: 14 [134976/225000 (60%)] Loss: 20574.578125\n",
      "Train Epoch: 14 [137472/225000 (61%)] Loss: 20964.906250\n",
      "Train Epoch: 14 [139968/225000 (62%)] Loss: 20746.082031\n",
      "Train Epoch: 14 [142464/225000 (63%)] Loss: 20441.080078\n",
      "Train Epoch: 14 [144960/225000 (64%)] Loss: 20645.929688\n",
      "Train Epoch: 14 [147456/225000 (66%)] Loss: 20202.583984\n",
      "Train Epoch: 14 [149952/225000 (67%)] Loss: 20539.314453\n",
      "Train Epoch: 14 [152448/225000 (68%)] Loss: 21020.480469\n",
      "Train Epoch: 14 [154944/225000 (69%)] Loss: 20429.722656\n",
      "Train Epoch: 14 [157440/225000 (70%)] Loss: 34882.875000\n",
      "Train Epoch: 14 [159936/225000 (71%)] Loss: 20195.779297\n",
      "Train Epoch: 14 [162432/225000 (72%)] Loss: 20324.589844\n",
      "Train Epoch: 14 [164928/225000 (73%)] Loss: 20560.925781\n",
      "Train Epoch: 14 [167424/225000 (74%)] Loss: 21061.644531\n",
      "Train Epoch: 14 [169920/225000 (76%)] Loss: 20577.935547\n",
      "Train Epoch: 14 [172416/225000 (77%)] Loss: 20528.332031\n",
      "Train Epoch: 14 [174912/225000 (78%)] Loss: 20423.964844\n",
      "Train Epoch: 14 [177408/225000 (79%)] Loss: 20611.519531\n",
      "Train Epoch: 14 [179904/225000 (80%)] Loss: 20802.238281\n",
      "Train Epoch: 14 [182400/225000 (81%)] Loss: 20807.421875\n",
      "Train Epoch: 14 [184896/225000 (82%)] Loss: 20041.308594\n",
      "Train Epoch: 14 [187392/225000 (83%)] Loss: 20246.890625\n",
      "Train Epoch: 14 [189888/225000 (84%)] Loss: 20469.054688\n",
      "Train Epoch: 14 [192384/225000 (86%)] Loss: 20716.910156\n",
      "Train Epoch: 14 [194880/225000 (87%)] Loss: 20694.972656\n",
      "Train Epoch: 14 [197376/225000 (88%)] Loss: 20269.128906\n",
      "Train Epoch: 14 [199872/225000 (89%)] Loss: 20602.833984\n",
      "Train Epoch: 14 [202368/225000 (90%)] Loss: 20609.693359\n",
      "Train Epoch: 14 [204864/225000 (91%)] Loss: 20648.648438\n",
      "Train Epoch: 14 [207360/225000 (92%)] Loss: 20868.669922\n",
      "Train Epoch: 14 [209856/225000 (93%)] Loss: 20603.025391\n",
      "Train Epoch: 14 [212352/225000 (94%)] Loss: 20248.322266\n",
      "Train Epoch: 14 [214848/225000 (95%)] Loss: 20584.132812\n",
      "Train Epoch: 14 [217344/225000 (97%)] Loss: 20736.851562\n",
      "Train Epoch: 14 [219840/225000 (98%)] Loss: 20078.781250\n",
      "Train Epoch: 14 [222336/225000 (99%)] Loss: 20803.296875\n",
      "Train Epoch: 14 [224832/225000 (100%)] Loss: 20684.265625\n",
      "    epoch          : 14\n",
      "    loss           : 20619.98784129693\n",
      "    val_loss       : 20489.480741676485\n",
      "Train Epoch: 15 [192/225000 (0%)] Loss: 20540.593750\n",
      "Train Epoch: 15 [2688/225000 (1%)] Loss: 20377.529297\n",
      "Train Epoch: 15 [5184/225000 (2%)] Loss: 20788.433594\n",
      "Train Epoch: 15 [7680/225000 (3%)] Loss: 20743.511719\n",
      "Train Epoch: 15 [10176/225000 (5%)] Loss: 20423.714844\n",
      "Train Epoch: 15 [12672/225000 (6%)] Loss: 20784.951172\n",
      "Train Epoch: 15 [15168/225000 (7%)] Loss: 21113.072266\n",
      "Train Epoch: 15 [17664/225000 (8%)] Loss: 21030.171875\n",
      "Train Epoch: 15 [20160/225000 (9%)] Loss: 20711.804688\n",
      "Train Epoch: 15 [22656/225000 (10%)] Loss: 20602.587891\n",
      "Train Epoch: 15 [25152/225000 (11%)] Loss: 20754.214844\n",
      "Train Epoch: 15 [27648/225000 (12%)] Loss: 20484.187500\n",
      "Train Epoch: 15 [30144/225000 (13%)] Loss: 20870.691406\n",
      "Train Epoch: 15 [32640/225000 (15%)] Loss: 20661.667969\n",
      "Train Epoch: 15 [35136/225000 (16%)] Loss: 20734.574219\n",
      "Train Epoch: 15 [37632/225000 (17%)] Loss: 20361.775391\n",
      "Train Epoch: 15 [40128/225000 (18%)] Loss: 20839.951172\n",
      "Train Epoch: 15 [42624/225000 (19%)] Loss: 20452.087891\n",
      "Train Epoch: 15 [45120/225000 (20%)] Loss: 20164.746094\n",
      "Train Epoch: 15 [47616/225000 (21%)] Loss: 20246.171875\n",
      "Train Epoch: 15 [50112/225000 (22%)] Loss: 20530.982422\n",
      "Train Epoch: 15 [52608/225000 (23%)] Loss: 20738.416016\n",
      "Train Epoch: 15 [55104/225000 (24%)] Loss: 20520.097656\n",
      "Train Epoch: 15 [57600/225000 (26%)] Loss: 20810.597656\n",
      "Train Epoch: 15 [60096/225000 (27%)] Loss: 20617.884766\n",
      "Train Epoch: 15 [62592/225000 (28%)] Loss: 20314.146484\n",
      "Train Epoch: 15 [65088/225000 (29%)] Loss: 20966.097656\n",
      "Train Epoch: 15 [67584/225000 (30%)] Loss: 20235.542969\n",
      "Train Epoch: 15 [70080/225000 (31%)] Loss: 20916.675781\n",
      "Train Epoch: 15 [72576/225000 (32%)] Loss: 20488.121094\n",
      "Train Epoch: 15 [75072/225000 (33%)] Loss: 20606.035156\n",
      "Train Epoch: 15 [77568/225000 (34%)] Loss: 20532.187500\n",
      "Train Epoch: 15 [80064/225000 (36%)] Loss: 20835.664062\n",
      "Train Epoch: 15 [82560/225000 (37%)] Loss: 20614.250000\n",
      "Train Epoch: 15 [85056/225000 (38%)] Loss: 20405.677734\n",
      "Train Epoch: 15 [87552/225000 (39%)] Loss: 20643.410156\n",
      "Train Epoch: 15 [90048/225000 (40%)] Loss: 20517.421875\n",
      "Train Epoch: 15 [92544/225000 (41%)] Loss: 20647.335938\n",
      "Train Epoch: 15 [95040/225000 (42%)] Loss: 20840.132812\n",
      "Train Epoch: 15 [97536/225000 (43%)] Loss: 20861.531250\n",
      "Train Epoch: 15 [100032/225000 (44%)] Loss: 21030.050781\n",
      "Train Epoch: 15 [102528/225000 (46%)] Loss: 20854.152344\n",
      "Train Epoch: 15 [105024/225000 (47%)] Loss: 20611.675781\n",
      "Train Epoch: 15 [107520/225000 (48%)] Loss: 20009.800781\n",
      "Train Epoch: 15 [110016/225000 (49%)] Loss: 20692.085938\n",
      "Train Epoch: 15 [112512/225000 (50%)] Loss: 20927.455078\n",
      "Train Epoch: 15 [115008/225000 (51%)] Loss: 20335.625000\n",
      "Train Epoch: 15 [117504/225000 (52%)] Loss: 20615.712891\n",
      "Train Epoch: 15 [120000/225000 (53%)] Loss: 20772.156250\n",
      "Train Epoch: 15 [122496/225000 (54%)] Loss: 20595.691406\n",
      "Train Epoch: 15 [124992/225000 (56%)] Loss: 20569.339844\n",
      "Train Epoch: 15 [127488/225000 (57%)] Loss: 20378.707031\n",
      "Train Epoch: 15 [129984/225000 (58%)] Loss: 21134.605469\n",
      "Train Epoch: 15 [132480/225000 (59%)] Loss: 20657.468750\n",
      "Train Epoch: 15 [134976/225000 (60%)] Loss: 20801.078125\n",
      "Train Epoch: 15 [137472/225000 (61%)] Loss: 20415.185547\n",
      "Train Epoch: 15 [139968/225000 (62%)] Loss: 20869.773438\n",
      "Train Epoch: 15 [142464/225000 (63%)] Loss: 20637.011719\n",
      "Train Epoch: 15 [144960/225000 (64%)] Loss: 20485.625000\n",
      "Train Epoch: 15 [147456/225000 (66%)] Loss: 20717.240234\n",
      "Train Epoch: 15 [149952/225000 (67%)] Loss: 20257.826172\n",
      "Train Epoch: 15 [152448/225000 (68%)] Loss: 20495.148438\n",
      "Train Epoch: 15 [154944/225000 (69%)] Loss: 20709.013672\n",
      "Train Epoch: 15 [157440/225000 (70%)] Loss: 20677.371094\n",
      "Train Epoch: 15 [159936/225000 (71%)] Loss: 21055.125000\n",
      "Train Epoch: 15 [162432/225000 (72%)] Loss: 20916.210938\n",
      "Train Epoch: 15 [164928/225000 (73%)] Loss: 20644.832031\n",
      "Train Epoch: 15 [167424/225000 (74%)] Loss: 20731.523438\n",
      "Train Epoch: 15 [169920/225000 (76%)] Loss: 20754.513672\n",
      "Train Epoch: 15 [172416/225000 (77%)] Loss: 20442.574219\n",
      "Train Epoch: 15 [174912/225000 (78%)] Loss: 20746.744141\n",
      "Train Epoch: 15 [177408/225000 (79%)] Loss: 20207.601562\n",
      "Train Epoch: 15 [179904/225000 (80%)] Loss: 20362.937500\n",
      "Train Epoch: 15 [182400/225000 (81%)] Loss: 20857.359375\n",
      "Train Epoch: 15 [184896/225000 (82%)] Loss: 20238.996094\n",
      "Train Epoch: 15 [187392/225000 (83%)] Loss: 20680.976562\n",
      "Train Epoch: 15 [189888/225000 (84%)] Loss: 20361.457031\n",
      "Train Epoch: 15 [192384/225000 (86%)] Loss: 20806.677734\n",
      "Train Epoch: 15 [194880/225000 (87%)] Loss: 20562.917969\n",
      "Train Epoch: 15 [197376/225000 (88%)] Loss: 20514.355469\n",
      "Train Epoch: 15 [199872/225000 (89%)] Loss: 20634.128906\n",
      "Train Epoch: 15 [202368/225000 (90%)] Loss: 20625.087891\n",
      "Train Epoch: 15 [204864/225000 (91%)] Loss: 20412.962891\n",
      "Train Epoch: 15 [207360/225000 (92%)] Loss: 20642.531250\n",
      "Train Epoch: 15 [209856/225000 (93%)] Loss: 20565.710938\n",
      "Train Epoch: 15 [212352/225000 (94%)] Loss: 20645.671875\n",
      "Train Epoch: 15 [214848/225000 (95%)] Loss: 20505.529297\n",
      "Train Epoch: 15 [217344/225000 (97%)] Loss: 20461.914062\n",
      "Train Epoch: 15 [219840/225000 (98%)] Loss: 20345.933594\n",
      "Train Epoch: 15 [222336/225000 (99%)] Loss: 20552.347656\n",
      "Train Epoch: 15 [224832/225000 (100%)] Loss: 20358.980469\n",
      "    epoch          : 15\n",
      "    loss           : 20596.641134945607\n",
      "    val_loss       : 20466.279485934563\n",
      "Train Epoch: 16 [192/225000 (0%)] Loss: 20789.519531\n",
      "Train Epoch: 16 [2688/225000 (1%)] Loss: 20745.849609\n",
      "Train Epoch: 16 [5184/225000 (2%)] Loss: 20688.476562\n",
      "Train Epoch: 16 [7680/225000 (3%)] Loss: 20770.933594\n",
      "Train Epoch: 16 [10176/225000 (5%)] Loss: 20604.167969\n",
      "Train Epoch: 16 [12672/225000 (6%)] Loss: 20503.128906\n",
      "Train Epoch: 16 [15168/225000 (7%)] Loss: 20851.867188\n",
      "Train Epoch: 16 [17664/225000 (8%)] Loss: 20627.832031\n",
      "Train Epoch: 16 [20160/225000 (9%)] Loss: 20121.222656\n",
      "Train Epoch: 16 [22656/225000 (10%)] Loss: 20714.966797\n",
      "Train Epoch: 16 [25152/225000 (11%)] Loss: 20847.187500\n",
      "Train Epoch: 16 [27648/225000 (12%)] Loss: 20222.867188\n",
      "Train Epoch: 16 [30144/225000 (13%)] Loss: 20516.687500\n",
      "Train Epoch: 16 [32640/225000 (15%)] Loss: 20341.542969\n",
      "Train Epoch: 16 [35136/225000 (16%)] Loss: 20653.664062\n",
      "Train Epoch: 16 [37632/225000 (17%)] Loss: 20685.207031\n",
      "Train Epoch: 16 [40128/225000 (18%)] Loss: 20418.724609\n",
      "Train Epoch: 16 [42624/225000 (19%)] Loss: 20437.875000\n",
      "Train Epoch: 16 [45120/225000 (20%)] Loss: 20589.919922\n",
      "Train Epoch: 16 [47616/225000 (21%)] Loss: 20539.203125\n",
      "Train Epoch: 16 [50112/225000 (22%)] Loss: 20204.703125\n",
      "Train Epoch: 16 [52608/225000 (23%)] Loss: 20754.509766\n",
      "Train Epoch: 16 [55104/225000 (24%)] Loss: 21102.597656\n",
      "Train Epoch: 16 [57600/225000 (26%)] Loss: 20191.078125\n",
      "Train Epoch: 16 [60096/225000 (27%)] Loss: 20360.214844\n",
      "Train Epoch: 16 [62592/225000 (28%)] Loss: 20461.779297\n",
      "Train Epoch: 16 [65088/225000 (29%)] Loss: 20279.191406\n",
      "Train Epoch: 16 [67584/225000 (30%)] Loss: 20694.767578\n",
      "Train Epoch: 16 [70080/225000 (31%)] Loss: 20633.777344\n",
      "Train Epoch: 16 [72576/225000 (32%)] Loss: 20472.097656\n",
      "Train Epoch: 16 [75072/225000 (33%)] Loss: 20843.101562\n",
      "Train Epoch: 16 [77568/225000 (34%)] Loss: 20956.437500\n",
      "Train Epoch: 16 [80064/225000 (36%)] Loss: 20535.753906\n",
      "Train Epoch: 16 [82560/225000 (37%)] Loss: 20702.480469\n",
      "Train Epoch: 16 [85056/225000 (38%)] Loss: 20455.800781\n",
      "Train Epoch: 16 [87552/225000 (39%)] Loss: 20541.386719\n",
      "Train Epoch: 16 [90048/225000 (40%)] Loss: 20341.416016\n",
      "Train Epoch: 16 [92544/225000 (41%)] Loss: 20999.419922\n",
      "Train Epoch: 16 [95040/225000 (42%)] Loss: 20297.765625\n",
      "Train Epoch: 16 [97536/225000 (43%)] Loss: 20672.148438\n",
      "Train Epoch: 16 [100032/225000 (44%)] Loss: 20475.292969\n",
      "Train Epoch: 16 [102528/225000 (46%)] Loss: 20387.384766\n",
      "Train Epoch: 16 [105024/225000 (47%)] Loss: 20359.011719\n",
      "Train Epoch: 16 [107520/225000 (48%)] Loss: 20252.660156\n",
      "Train Epoch: 16 [110016/225000 (49%)] Loss: 20127.593750\n",
      "Train Epoch: 16 [112512/225000 (50%)] Loss: 20412.597656\n",
      "Train Epoch: 16 [115008/225000 (51%)] Loss: 20475.406250\n",
      "Train Epoch: 16 [117504/225000 (52%)] Loss: 20569.800781\n",
      "Train Epoch: 16 [120000/225000 (53%)] Loss: 20655.431641\n",
      "Train Epoch: 16 [122496/225000 (54%)] Loss: 20274.703125\n",
      "Train Epoch: 16 [124992/225000 (56%)] Loss: 20538.953125\n",
      "Train Epoch: 16 [127488/225000 (57%)] Loss: 20487.560547\n",
      "Train Epoch: 16 [129984/225000 (58%)] Loss: 20601.984375\n",
      "Train Epoch: 16 [132480/225000 (59%)] Loss: 20519.972656\n",
      "Train Epoch: 16 [134976/225000 (60%)] Loss: 20322.902344\n",
      "Train Epoch: 16 [137472/225000 (61%)] Loss: 20429.796875\n",
      "Train Epoch: 16 [139968/225000 (62%)] Loss: 20667.431641\n",
      "Train Epoch: 16 [142464/225000 (63%)] Loss: 20995.937500\n",
      "Train Epoch: 16 [144960/225000 (64%)] Loss: 20414.156250\n",
      "Train Epoch: 16 [147456/225000 (66%)] Loss: 34785.375000\n",
      "Train Epoch: 16 [149952/225000 (67%)] Loss: 20601.585938\n",
      "Train Epoch: 16 [152448/225000 (68%)] Loss: 20086.187500\n",
      "Train Epoch: 16 [154944/225000 (69%)] Loss: 20125.652344\n",
      "Train Epoch: 16 [157440/225000 (70%)] Loss: 20542.912109\n",
      "Train Epoch: 16 [159936/225000 (71%)] Loss: 20643.214844\n",
      "Train Epoch: 16 [162432/225000 (72%)] Loss: 20478.105469\n",
      "Train Epoch: 16 [164928/225000 (73%)] Loss: 20690.380859\n",
      "Train Epoch: 16 [167424/225000 (74%)] Loss: 20336.199219\n",
      "Train Epoch: 16 [169920/225000 (76%)] Loss: 20028.937500\n",
      "Train Epoch: 16 [172416/225000 (77%)] Loss: 20813.082031\n",
      "Train Epoch: 16 [174912/225000 (78%)] Loss: 20624.968750\n",
      "Train Epoch: 16 [177408/225000 (79%)] Loss: 20786.371094\n",
      "Train Epoch: 16 [179904/225000 (80%)] Loss: 20638.304688\n",
      "Train Epoch: 16 [182400/225000 (81%)] Loss: 20145.832031\n",
      "Train Epoch: 16 [184896/225000 (82%)] Loss: 21225.386719\n",
      "Train Epoch: 16 [187392/225000 (83%)] Loss: 20143.207031\n",
      "Train Epoch: 16 [189888/225000 (84%)] Loss: 20318.453125\n",
      "Train Epoch: 16 [192384/225000 (86%)] Loss: 20636.542969\n",
      "Train Epoch: 16 [194880/225000 (87%)] Loss: 20382.585938\n",
      "Train Epoch: 16 [197376/225000 (88%)] Loss: 20557.347656\n",
      "Train Epoch: 16 [199872/225000 (89%)] Loss: 20897.054688\n",
      "Train Epoch: 16 [202368/225000 (90%)] Loss: 20625.882812\n",
      "Train Epoch: 16 [204864/225000 (91%)] Loss: 20249.931641\n",
      "Train Epoch: 16 [207360/225000 (92%)] Loss: 20270.277344\n",
      "Train Epoch: 16 [209856/225000 (93%)] Loss: 20625.625000\n",
      "Train Epoch: 16 [212352/225000 (94%)] Loss: 20035.976562\n",
      "Train Epoch: 16 [214848/225000 (95%)] Loss: 20512.046875\n",
      "Train Epoch: 16 [217344/225000 (97%)] Loss: 20310.240234\n",
      "Train Epoch: 16 [219840/225000 (98%)] Loss: 20376.996094\n",
      "Train Epoch: 16 [222336/225000 (99%)] Loss: 20806.238281\n",
      "Train Epoch: 16 [224832/225000 (100%)] Loss: 20372.128906\n",
      "    epoch          : 16\n",
      "    loss           : 20580.31902763705\n",
      "    val_loss       : 20444.791617218776\n",
      "Train Epoch: 17 [192/225000 (0%)] Loss: 20594.402344\n",
      "Train Epoch: 17 [2688/225000 (1%)] Loss: 20050.281250\n",
      "Train Epoch: 17 [5184/225000 (2%)] Loss: 20266.296875\n",
      "Train Epoch: 17 [7680/225000 (3%)] Loss: 20538.439453\n",
      "Train Epoch: 17 [10176/225000 (5%)] Loss: 20605.292969\n",
      "Train Epoch: 17 [12672/225000 (6%)] Loss: 20864.095703\n",
      "Train Epoch: 17 [15168/225000 (7%)] Loss: 20898.140625\n",
      "Train Epoch: 17 [17664/225000 (8%)] Loss: 20815.218750\n",
      "Train Epoch: 17 [20160/225000 (9%)] Loss: 20648.238281\n",
      "Train Epoch: 17 [22656/225000 (10%)] Loss: 20488.539062\n",
      "Train Epoch: 17 [25152/225000 (11%)] Loss: 20706.988281\n",
      "Train Epoch: 17 [27648/225000 (12%)] Loss: 20247.675781\n",
      "Train Epoch: 17 [30144/225000 (13%)] Loss: 20452.937500\n",
      "Train Epoch: 17 [32640/225000 (15%)] Loss: 20983.925781\n",
      "Train Epoch: 17 [35136/225000 (16%)] Loss: 20301.925781\n",
      "Train Epoch: 17 [37632/225000 (17%)] Loss: 20120.835938\n",
      "Train Epoch: 17 [40128/225000 (18%)] Loss: 20570.488281\n",
      "Train Epoch: 17 [42624/225000 (19%)] Loss: 20724.261719\n",
      "Train Epoch: 17 [45120/225000 (20%)] Loss: 20944.625000\n",
      "Train Epoch: 17 [47616/225000 (21%)] Loss: 20570.464844\n",
      "Train Epoch: 17 [50112/225000 (22%)] Loss: 20495.283203\n",
      "Train Epoch: 17 [52608/225000 (23%)] Loss: 20619.916016\n",
      "Train Epoch: 17 [55104/225000 (24%)] Loss: 20646.070312\n",
      "Train Epoch: 17 [57600/225000 (26%)] Loss: 20590.140625\n",
      "Train Epoch: 17 [60096/225000 (27%)] Loss: 20778.679688\n",
      "Train Epoch: 17 [62592/225000 (28%)] Loss: 20524.343750\n",
      "Train Epoch: 17 [65088/225000 (29%)] Loss: 20722.179688\n",
      "Train Epoch: 17 [67584/225000 (30%)] Loss: 20226.464844\n",
      "Train Epoch: 17 [70080/225000 (31%)] Loss: 20486.072266\n",
      "Train Epoch: 17 [72576/225000 (32%)] Loss: 20198.808594\n",
      "Train Epoch: 17 [75072/225000 (33%)] Loss: 20444.785156\n",
      "Train Epoch: 17 [77568/225000 (34%)] Loss: 19777.097656\n",
      "Train Epoch: 17 [80064/225000 (36%)] Loss: 20701.882812\n",
      "Train Epoch: 17 [82560/225000 (37%)] Loss: 20212.591797\n",
      "Train Epoch: 17 [85056/225000 (38%)] Loss: 20687.353516\n",
      "Train Epoch: 17 [87552/225000 (39%)] Loss: 21097.136719\n",
      "Train Epoch: 17 [90048/225000 (40%)] Loss: 20443.779297\n",
      "Train Epoch: 17 [92544/225000 (41%)] Loss: 20603.417969\n",
      "Train Epoch: 17 [95040/225000 (42%)] Loss: 20590.195312\n",
      "Train Epoch: 17 [97536/225000 (43%)] Loss: 20468.523438\n",
      "Train Epoch: 17 [100032/225000 (44%)] Loss: 20291.320312\n",
      "Train Epoch: 17 [102528/225000 (46%)] Loss: 20646.587891\n",
      "Train Epoch: 17 [105024/225000 (47%)] Loss: 20195.148438\n",
      "Train Epoch: 17 [107520/225000 (48%)] Loss: 20657.531250\n",
      "Train Epoch: 17 [110016/225000 (49%)] Loss: 20737.005859\n",
      "Train Epoch: 17 [112512/225000 (50%)] Loss: 20084.460938\n",
      "Train Epoch: 17 [115008/225000 (51%)] Loss: 20360.396484\n",
      "Train Epoch: 17 [117504/225000 (52%)] Loss: 20193.906250\n",
      "Train Epoch: 17 [120000/225000 (53%)] Loss: 20987.445312\n",
      "Train Epoch: 17 [122496/225000 (54%)] Loss: 20991.912109\n",
      "Train Epoch: 17 [124992/225000 (56%)] Loss: 20341.343750\n",
      "Train Epoch: 17 [127488/225000 (57%)] Loss: 20171.183594\n",
      "Train Epoch: 17 [129984/225000 (58%)] Loss: 20331.199219\n",
      "Train Epoch: 17 [132480/225000 (59%)] Loss: 25477.992188\n",
      "Train Epoch: 17 [134976/225000 (60%)] Loss: 20614.609375\n",
      "Train Epoch: 17 [137472/225000 (61%)] Loss: 20720.789062\n",
      "Train Epoch: 17 [139968/225000 (62%)] Loss: 20574.958984\n",
      "Train Epoch: 17 [142464/225000 (63%)] Loss: 19930.886719\n",
      "Train Epoch: 17 [144960/225000 (64%)] Loss: 20353.660156\n",
      "Train Epoch: 17 [147456/225000 (66%)] Loss: 20052.167969\n",
      "Train Epoch: 17 [149952/225000 (67%)] Loss: 20375.589844\n",
      "Train Epoch: 17 [152448/225000 (68%)] Loss: 20429.597656\n",
      "Train Epoch: 17 [154944/225000 (69%)] Loss: 20349.806641\n",
      "Train Epoch: 17 [157440/225000 (70%)] Loss: 20276.326172\n",
      "Train Epoch: 17 [159936/225000 (71%)] Loss: 20634.511719\n",
      "Train Epoch: 17 [162432/225000 (72%)] Loss: 20849.660156\n",
      "Train Epoch: 17 [164928/225000 (73%)] Loss: 20120.921875\n",
      "Train Epoch: 17 [167424/225000 (74%)] Loss: 20759.083984\n",
      "Train Epoch: 17 [169920/225000 (76%)] Loss: 20491.320312\n",
      "Train Epoch: 17 [172416/225000 (77%)] Loss: 20528.816406\n",
      "Train Epoch: 17 [174912/225000 (78%)] Loss: 20465.269531\n",
      "Train Epoch: 17 [177408/225000 (79%)] Loss: 20144.169922\n",
      "Train Epoch: 17 [179904/225000 (80%)] Loss: 20607.757812\n",
      "Train Epoch: 17 [182400/225000 (81%)] Loss: 20997.835938\n",
      "Train Epoch: 17 [184896/225000 (82%)] Loss: 21182.644531\n",
      "Train Epoch: 17 [187392/225000 (83%)] Loss: 20917.460938\n",
      "Train Epoch: 17 [189888/225000 (84%)] Loss: 20479.361328\n",
      "Train Epoch: 17 [192384/225000 (86%)] Loss: 20661.390625\n",
      "Train Epoch: 17 [194880/225000 (87%)] Loss: 20657.527344\n",
      "Train Epoch: 17 [197376/225000 (88%)] Loss: 20790.488281\n",
      "Train Epoch: 17 [199872/225000 (89%)] Loss: 20078.308594\n",
      "Train Epoch: 17 [202368/225000 (90%)] Loss: 20252.300781\n",
      "Train Epoch: 17 [204864/225000 (91%)] Loss: 20686.980469\n",
      "Train Epoch: 17 [207360/225000 (92%)] Loss: 19838.335938\n",
      "Train Epoch: 17 [209856/225000 (93%)] Loss: 20744.347656\n",
      "Train Epoch: 17 [212352/225000 (94%)] Loss: 21096.242188\n",
      "Train Epoch: 17 [214848/225000 (95%)] Loss: 20506.132812\n",
      "Train Epoch: 17 [217344/225000 (97%)] Loss: 20765.080078\n",
      "Train Epoch: 17 [219840/225000 (98%)] Loss: 20461.080078\n",
      "Train Epoch: 17 [222336/225000 (99%)] Loss: 21032.369141\n",
      "Train Epoch: 17 [224832/225000 (100%)] Loss: 20542.083984\n",
      "    epoch          : 17\n",
      "    loss           : 20557.082864494452\n",
      "    val_loss       : 20576.218766398102\n",
      "Train Epoch: 18 [192/225000 (0%)] Loss: 20458.800781\n",
      "Train Epoch: 18 [2688/225000 (1%)] Loss: 20818.230469\n",
      "Train Epoch: 18 [5184/225000 (2%)] Loss: 20630.994141\n",
      "Train Epoch: 18 [7680/225000 (3%)] Loss: 20491.601562\n",
      "Train Epoch: 18 [10176/225000 (5%)] Loss: 20242.156250\n",
      "Train Epoch: 18 [12672/225000 (6%)] Loss: 20320.031250\n",
      "Train Epoch: 18 [15168/225000 (7%)] Loss: 20146.619141\n",
      "Train Epoch: 18 [17664/225000 (8%)] Loss: 20333.515625\n",
      "Train Epoch: 18 [20160/225000 (9%)] Loss: 20121.671875\n",
      "Train Epoch: 18 [22656/225000 (10%)] Loss: 20958.419922\n",
      "Train Epoch: 18 [25152/225000 (11%)] Loss: 20410.273438\n",
      "Train Epoch: 18 [27648/225000 (12%)] Loss: 20413.062500\n",
      "Train Epoch: 18 [30144/225000 (13%)] Loss: 20695.000000\n",
      "Train Epoch: 18 [32640/225000 (15%)] Loss: 20490.875000\n",
      "Train Epoch: 18 [35136/225000 (16%)] Loss: 20392.640625\n",
      "Train Epoch: 18 [37632/225000 (17%)] Loss: 20533.642578\n",
      "Train Epoch: 18 [40128/225000 (18%)] Loss: 20780.851562\n",
      "Train Epoch: 18 [42624/225000 (19%)] Loss: 20278.876953\n",
      "Train Epoch: 18 [45120/225000 (20%)] Loss: 20429.427734\n",
      "Train Epoch: 18 [47616/225000 (21%)] Loss: 20335.550781\n",
      "Train Epoch: 18 [50112/225000 (22%)] Loss: 20612.656250\n",
      "Train Epoch: 18 [52608/225000 (23%)] Loss: 19997.050781\n",
      "Train Epoch: 18 [55104/225000 (24%)] Loss: 20467.353516\n",
      "Train Epoch: 18 [57600/225000 (26%)] Loss: 20881.705078\n",
      "Train Epoch: 18 [60096/225000 (27%)] Loss: 20801.050781\n",
      "Train Epoch: 18 [62592/225000 (28%)] Loss: 20796.890625\n",
      "Train Epoch: 18 [65088/225000 (29%)] Loss: 20722.191406\n",
      "Train Epoch: 18 [67584/225000 (30%)] Loss: 20584.074219\n",
      "Train Epoch: 18 [70080/225000 (31%)] Loss: 20097.832031\n",
      "Train Epoch: 18 [72576/225000 (32%)] Loss: 20206.103516\n",
      "Train Epoch: 18 [75072/225000 (33%)] Loss: 20428.939453\n",
      "Train Epoch: 18 [77568/225000 (34%)] Loss: 20874.531250\n",
      "Train Epoch: 18 [80064/225000 (36%)] Loss: 20360.607422\n",
      "Train Epoch: 18 [82560/225000 (37%)] Loss: 20644.267578\n",
      "Train Epoch: 18 [85056/225000 (38%)] Loss: 21000.093750\n",
      "Train Epoch: 18 [87552/225000 (39%)] Loss: 20678.369141\n",
      "Train Epoch: 18 [90048/225000 (40%)] Loss: 20438.933594\n",
      "Train Epoch: 18 [92544/225000 (41%)] Loss: 20752.765625\n",
      "Train Epoch: 18 [95040/225000 (42%)] Loss: 20344.113281\n",
      "Train Epoch: 18 [97536/225000 (43%)] Loss: 20391.453125\n",
      "Train Epoch: 18 [100032/225000 (44%)] Loss: 20416.003906\n",
      "Train Epoch: 18 [102528/225000 (46%)] Loss: 20850.673828\n",
      "Train Epoch: 18 [105024/225000 (47%)] Loss: 20654.591797\n",
      "Train Epoch: 18 [107520/225000 (48%)] Loss: 20787.095703\n",
      "Train Epoch: 18 [110016/225000 (49%)] Loss: 20696.929688\n",
      "Train Epoch: 18 [112512/225000 (50%)] Loss: 20775.035156\n",
      "Train Epoch: 18 [115008/225000 (51%)] Loss: 20369.804688\n",
      "Train Epoch: 18 [117504/225000 (52%)] Loss: 20362.392578\n",
      "Train Epoch: 18 [120000/225000 (53%)] Loss: 20894.613281\n",
      "Train Epoch: 18 [122496/225000 (54%)] Loss: 20575.736328\n",
      "Train Epoch: 18 [124992/225000 (56%)] Loss: 20516.003906\n",
      "Train Epoch: 18 [127488/225000 (57%)] Loss: 21104.191406\n",
      "Train Epoch: 18 [129984/225000 (58%)] Loss: 20650.285156\n",
      "Train Epoch: 18 [132480/225000 (59%)] Loss: 20288.964844\n",
      "Train Epoch: 18 [134976/225000 (60%)] Loss: 20196.792969\n",
      "Train Epoch: 18 [137472/225000 (61%)] Loss: 20585.970703\n",
      "Train Epoch: 18 [139968/225000 (62%)] Loss: 20249.628906\n",
      "Train Epoch: 18 [142464/225000 (63%)] Loss: 20481.310547\n",
      "Train Epoch: 18 [144960/225000 (64%)] Loss: 20137.041016\n",
      "Train Epoch: 18 [147456/225000 (66%)] Loss: 20787.558594\n",
      "Train Epoch: 18 [149952/225000 (67%)] Loss: 20421.660156\n",
      "Train Epoch: 18 [152448/225000 (68%)] Loss: 20127.429688\n",
      "Train Epoch: 18 [154944/225000 (69%)] Loss: 20940.640625\n",
      "Train Epoch: 18 [157440/225000 (70%)] Loss: 20473.767578\n",
      "Train Epoch: 18 [159936/225000 (71%)] Loss: 20297.433594\n",
      "Train Epoch: 18 [162432/225000 (72%)] Loss: 20216.664062\n",
      "Train Epoch: 18 [164928/225000 (73%)] Loss: 20154.183594\n",
      "Train Epoch: 18 [167424/225000 (74%)] Loss: 19963.980469\n",
      "Train Epoch: 18 [169920/225000 (76%)] Loss: 20789.605469\n",
      "Train Epoch: 18 [172416/225000 (77%)] Loss: 20460.964844\n",
      "Train Epoch: 18 [174912/225000 (78%)] Loss: 20265.867188\n",
      "Train Epoch: 18 [177408/225000 (79%)] Loss: 20881.146484\n",
      "Train Epoch: 18 [179904/225000 (80%)] Loss: 20765.589844\n",
      "Train Epoch: 18 [182400/225000 (81%)] Loss: 20424.503906\n",
      "Train Epoch: 18 [184896/225000 (82%)] Loss: 20924.023438\n",
      "Train Epoch: 18 [187392/225000 (83%)] Loss: 20111.617188\n",
      "Train Epoch: 18 [189888/225000 (84%)] Loss: 20554.703125\n",
      "Train Epoch: 18 [192384/225000 (86%)] Loss: 20894.898438\n",
      "Train Epoch: 18 [194880/225000 (87%)] Loss: 20063.978516\n",
      "Train Epoch: 18 [197376/225000 (88%)] Loss: 19973.867188\n",
      "Train Epoch: 18 [199872/225000 (89%)] Loss: 20591.003906\n",
      "Train Epoch: 18 [202368/225000 (90%)] Loss: 20520.519531\n",
      "Train Epoch: 18 [204864/225000 (91%)] Loss: 20740.833984\n",
      "Train Epoch: 18 [207360/225000 (92%)] Loss: 20752.177734\n",
      "Train Epoch: 18 [209856/225000 (93%)] Loss: 20677.804688\n",
      "Train Epoch: 18 [212352/225000 (94%)] Loss: 20421.884766\n",
      "Train Epoch: 18 [214848/225000 (95%)] Loss: 20410.236328\n",
      "Train Epoch: 18 [217344/225000 (97%)] Loss: 21035.955078\n",
      "Train Epoch: 18 [219840/225000 (98%)] Loss: 20717.472656\n",
      "Train Epoch: 18 [222336/225000 (99%)] Loss: 20134.984375\n",
      "Train Epoch: 18 [224832/225000 (100%)] Loss: 20371.761719\n",
      "    epoch          : 18\n",
      "    loss           : 20526.599939339805\n",
      "    val_loss       : 20509.79514004802\n",
      "Train Epoch: 19 [192/225000 (0%)] Loss: 19966.291016\n",
      "Train Epoch: 19 [2688/225000 (1%)] Loss: 20373.712891\n",
      "Train Epoch: 19 [5184/225000 (2%)] Loss: 20530.941406\n",
      "Train Epoch: 19 [7680/225000 (3%)] Loss: 20853.570312\n",
      "Train Epoch: 19 [10176/225000 (5%)] Loss: 20683.785156\n",
      "Train Epoch: 19 [12672/225000 (6%)] Loss: 20442.628906\n",
      "Train Epoch: 19 [15168/225000 (7%)] Loss: 20507.421875\n",
      "Train Epoch: 19 [17664/225000 (8%)] Loss: 20588.761719\n",
      "Train Epoch: 19 [20160/225000 (9%)] Loss: 20366.472656\n",
      "Train Epoch: 19 [22656/225000 (10%)] Loss: 20722.261719\n",
      "Train Epoch: 19 [25152/225000 (11%)] Loss: 19668.320312\n",
      "Train Epoch: 19 [27648/225000 (12%)] Loss: 20894.261719\n",
      "Train Epoch: 19 [30144/225000 (13%)] Loss: 20204.503906\n",
      "Train Epoch: 19 [32640/225000 (15%)] Loss: 20937.339844\n",
      "Train Epoch: 19 [35136/225000 (16%)] Loss: 20685.197266\n",
      "Train Epoch: 19 [37632/225000 (17%)] Loss: 20645.041016\n",
      "Train Epoch: 19 [40128/225000 (18%)] Loss: 20151.195312\n",
      "Train Epoch: 19 [42624/225000 (19%)] Loss: 20830.083984\n",
      "Train Epoch: 19 [45120/225000 (20%)] Loss: 20398.886719\n",
      "Train Epoch: 19 [47616/225000 (21%)] Loss: 20469.769531\n",
      "Train Epoch: 19 [50112/225000 (22%)] Loss: 20758.156250\n",
      "Train Epoch: 19 [52608/225000 (23%)] Loss: 20142.781250\n",
      "Train Epoch: 19 [55104/225000 (24%)] Loss: 20685.728516\n",
      "Train Epoch: 19 [57600/225000 (26%)] Loss: 19851.292969\n",
      "Train Epoch: 19 [60096/225000 (27%)] Loss: 20438.000000\n",
      "Train Epoch: 19 [62592/225000 (28%)] Loss: 20883.167969\n",
      "Train Epoch: 19 [65088/225000 (29%)] Loss: 20865.480469\n",
      "Train Epoch: 19 [67584/225000 (30%)] Loss: 20181.109375\n",
      "Train Epoch: 19 [70080/225000 (31%)] Loss: 20783.316406\n",
      "Train Epoch: 19 [72576/225000 (32%)] Loss: 20386.537109\n",
      "Train Epoch: 19 [75072/225000 (33%)] Loss: 20549.347656\n",
      "Train Epoch: 19 [77568/225000 (34%)] Loss: 20619.843750\n",
      "Train Epoch: 19 [80064/225000 (36%)] Loss: 19740.003906\n",
      "Train Epoch: 19 [82560/225000 (37%)] Loss: 20277.628906\n",
      "Train Epoch: 19 [85056/225000 (38%)] Loss: 20496.976562\n",
      "Train Epoch: 19 [87552/225000 (39%)] Loss: 20477.744141\n",
      "Train Epoch: 19 [90048/225000 (40%)] Loss: 20028.683594\n",
      "Train Epoch: 19 [92544/225000 (41%)] Loss: 20290.589844\n",
      "Train Epoch: 19 [95040/225000 (42%)] Loss: 20354.539062\n",
      "Train Epoch: 19 [97536/225000 (43%)] Loss: 20549.468750\n",
      "Train Epoch: 19 [100032/225000 (44%)] Loss: 20301.720703\n",
      "Train Epoch: 19 [102528/225000 (46%)] Loss: 20659.373047\n",
      "Train Epoch: 19 [105024/225000 (47%)] Loss: 20897.160156\n",
      "Train Epoch: 19 [107520/225000 (48%)] Loss: 20265.626953\n",
      "Train Epoch: 19 [110016/225000 (49%)] Loss: 20409.701172\n",
      "Train Epoch: 19 [112512/225000 (50%)] Loss: 20204.552734\n",
      "Train Epoch: 19 [115008/225000 (51%)] Loss: 20168.914062\n",
      "Train Epoch: 19 [117504/225000 (52%)] Loss: 20509.730469\n",
      "Train Epoch: 19 [120000/225000 (53%)] Loss: 20067.515625\n",
      "Train Epoch: 19 [122496/225000 (54%)] Loss: 20719.388672\n",
      "Train Epoch: 19 [124992/225000 (56%)] Loss: 20425.449219\n",
      "Train Epoch: 19 [127488/225000 (57%)] Loss: 20681.824219\n",
      "Train Epoch: 19 [129984/225000 (58%)] Loss: 20306.001953\n",
      "Train Epoch: 19 [132480/225000 (59%)] Loss: 20800.652344\n",
      "Train Epoch: 19 [134976/225000 (60%)] Loss: 20210.667969\n",
      "Train Epoch: 19 [137472/225000 (61%)] Loss: 20240.273438\n",
      "Train Epoch: 19 [139968/225000 (62%)] Loss: 20669.246094\n",
      "Train Epoch: 19 [142464/225000 (63%)] Loss: 20783.853516\n",
      "Train Epoch: 19 [144960/225000 (64%)] Loss: 20175.152344\n",
      "Train Epoch: 19 [147456/225000 (66%)] Loss: 19834.640625\n",
      "Train Epoch: 19 [149952/225000 (67%)] Loss: 21301.000000\n",
      "Train Epoch: 19 [152448/225000 (68%)] Loss: 20675.753906\n",
      "Train Epoch: 19 [154944/225000 (69%)] Loss: 20068.943359\n",
      "Train Epoch: 19 [157440/225000 (70%)] Loss: 20800.699219\n",
      "Train Epoch: 19 [159936/225000 (71%)] Loss: 20518.849609\n",
      "Train Epoch: 19 [162432/225000 (72%)] Loss: 20591.976562\n",
      "Train Epoch: 19 [164928/225000 (73%)] Loss: 20882.509766\n",
      "Train Epoch: 19 [167424/225000 (74%)] Loss: 20831.142578\n",
      "Train Epoch: 19 [169920/225000 (76%)] Loss: 20659.550781\n",
      "Train Epoch: 19 [172416/225000 (77%)] Loss: 20487.960938\n",
      "Train Epoch: 19 [174912/225000 (78%)] Loss: 20011.375000\n",
      "Train Epoch: 19 [177408/225000 (79%)] Loss: 20557.335938\n",
      "Train Epoch: 19 [179904/225000 (80%)] Loss: 20581.988281\n",
      "Train Epoch: 19 [182400/225000 (81%)] Loss: 21086.207031\n",
      "Train Epoch: 19 [184896/225000 (82%)] Loss: 20502.654297\n",
      "Train Epoch: 19 [187392/225000 (83%)] Loss: 20575.593750\n",
      "Train Epoch: 19 [189888/225000 (84%)] Loss: 20332.804688\n",
      "Train Epoch: 19 [192384/225000 (86%)] Loss: 20265.449219\n",
      "Train Epoch: 19 [194880/225000 (87%)] Loss: 20067.355469\n",
      "Train Epoch: 19 [197376/225000 (88%)] Loss: 20008.843750\n",
      "Train Epoch: 19 [199872/225000 (89%)] Loss: 20526.921875\n",
      "Train Epoch: 19 [202368/225000 (90%)] Loss: 20021.193359\n",
      "Train Epoch: 19 [204864/225000 (91%)] Loss: 20982.675781\n",
      "Train Epoch: 19 [207360/225000 (92%)] Loss: 20826.523438\n",
      "Train Epoch: 19 [209856/225000 (93%)] Loss: 20052.085938\n",
      "Train Epoch: 19 [212352/225000 (94%)] Loss: 20866.888672\n",
      "Train Epoch: 19 [214848/225000 (95%)] Loss: 20165.435547\n",
      "Train Epoch: 19 [217344/225000 (97%)] Loss: 20694.613281\n",
      "Train Epoch: 19 [219840/225000 (98%)] Loss: 20297.605469\n",
      "Train Epoch: 19 [222336/225000 (99%)] Loss: 20312.457031\n",
      "Train Epoch: 19 [224832/225000 (100%)] Loss: 20396.755859\n",
      "    epoch          : 19\n",
      "    loss           : 20528.840395357827\n",
      "    val_loss       : 20376.98676357606\n",
      "Train Epoch: 20 [192/225000 (0%)] Loss: 20733.001953\n",
      "Train Epoch: 20 [2688/225000 (1%)] Loss: 20017.806641\n",
      "Train Epoch: 20 [5184/225000 (2%)] Loss: 20546.498047\n",
      "Train Epoch: 20 [7680/225000 (3%)] Loss: 20395.234375\n",
      "Train Epoch: 20 [10176/225000 (5%)] Loss: 19981.396484\n",
      "Train Epoch: 20 [12672/225000 (6%)] Loss: 20837.570312\n",
      "Train Epoch: 20 [15168/225000 (7%)] Loss: 20924.705078\n",
      "Train Epoch: 20 [17664/225000 (8%)] Loss: 20507.070312\n",
      "Train Epoch: 20 [20160/225000 (9%)] Loss: 20758.091797\n",
      "Train Epoch: 20 [22656/225000 (10%)] Loss: 20796.259766\n",
      "Train Epoch: 20 [25152/225000 (11%)] Loss: 20321.214844\n",
      "Train Epoch: 20 [27648/225000 (12%)] Loss: 20389.054688\n",
      "Train Epoch: 20 [30144/225000 (13%)] Loss: 20645.542969\n",
      "Train Epoch: 20 [32640/225000 (15%)] Loss: 20515.734375\n",
      "Train Epoch: 20 [35136/225000 (16%)] Loss: 20424.511719\n",
      "Train Epoch: 20 [37632/225000 (17%)] Loss: 20732.175781\n",
      "Train Epoch: 20 [40128/225000 (18%)] Loss: 20285.109375\n",
      "Train Epoch: 20 [42624/225000 (19%)] Loss: 20059.845703\n",
      "Train Epoch: 20 [45120/225000 (20%)] Loss: 20792.886719\n",
      "Train Epoch: 20 [47616/225000 (21%)] Loss: 20492.929688\n",
      "Train Epoch: 20 [50112/225000 (22%)] Loss: 20172.308594\n",
      "Train Epoch: 20 [52608/225000 (23%)] Loss: 20329.718750\n",
      "Train Epoch: 20 [55104/225000 (24%)] Loss: 20830.359375\n",
      "Train Epoch: 20 [57600/225000 (26%)] Loss: 20731.593750\n",
      "Train Epoch: 20 [60096/225000 (27%)] Loss: 20595.957031\n",
      "Train Epoch: 20 [62592/225000 (28%)] Loss: 20701.800781\n",
      "Train Epoch: 20 [65088/225000 (29%)] Loss: 20274.785156\n",
      "Train Epoch: 20 [67584/225000 (30%)] Loss: 20537.093750\n",
      "Train Epoch: 20 [70080/225000 (31%)] Loss: 20462.937500\n",
      "Train Epoch: 20 [72576/225000 (32%)] Loss: 20523.671875\n",
      "Train Epoch: 20 [75072/225000 (33%)] Loss: 20253.947266\n",
      "Train Epoch: 20 [77568/225000 (34%)] Loss: 20764.898438\n",
      "Train Epoch: 20 [80064/225000 (36%)] Loss: 20203.902344\n",
      "Train Epoch: 20 [82560/225000 (37%)] Loss: 20535.613281\n",
      "Train Epoch: 20 [85056/225000 (38%)] Loss: 20810.710938\n",
      "Train Epoch: 20 [87552/225000 (39%)] Loss: 20048.343750\n",
      "Train Epoch: 20 [90048/225000 (40%)] Loss: 20905.382812\n",
      "Train Epoch: 20 [92544/225000 (41%)] Loss: 20268.746094\n",
      "Train Epoch: 20 [95040/225000 (42%)] Loss: 19925.996094\n",
      "Train Epoch: 20 [97536/225000 (43%)] Loss: 20430.003906\n",
      "Train Epoch: 20 [100032/225000 (44%)] Loss: 20195.931641\n",
      "Train Epoch: 20 [102528/225000 (46%)] Loss: 20344.437500\n",
      "Train Epoch: 20 [105024/225000 (47%)] Loss: 20176.078125\n",
      "Train Epoch: 20 [107520/225000 (48%)] Loss: 20636.955078\n",
      "Train Epoch: 20 [110016/225000 (49%)] Loss: 20804.148438\n",
      "Train Epoch: 20 [112512/225000 (50%)] Loss: 20341.765625\n",
      "Train Epoch: 20 [115008/225000 (51%)] Loss: 20316.771484\n",
      "Train Epoch: 20 [117504/225000 (52%)] Loss: 20081.679688\n",
      "Train Epoch: 20 [120000/225000 (53%)] Loss: 20506.267578\n",
      "Train Epoch: 20 [122496/225000 (54%)] Loss: 20149.548828\n",
      "Train Epoch: 20 [124992/225000 (56%)] Loss: 20479.089844\n",
      "Train Epoch: 20 [127488/225000 (57%)] Loss: 20115.529297\n",
      "Train Epoch: 20 [129984/225000 (58%)] Loss: 20251.679688\n",
      "Train Epoch: 20 [132480/225000 (59%)] Loss: 20794.830078\n",
      "Train Epoch: 20 [134976/225000 (60%)] Loss: 20452.158203\n",
      "Train Epoch: 20 [137472/225000 (61%)] Loss: 20156.234375\n",
      "Train Epoch: 20 [139968/225000 (62%)] Loss: 20090.871094\n",
      "Train Epoch: 20 [142464/225000 (63%)] Loss: 20826.201172\n",
      "Train Epoch: 20 [144960/225000 (64%)] Loss: 20297.169922\n",
      "Train Epoch: 20 [147456/225000 (66%)] Loss: 20403.648438\n",
      "Train Epoch: 20 [149952/225000 (67%)] Loss: 20463.576172\n",
      "Train Epoch: 20 [152448/225000 (68%)] Loss: 20667.232422\n",
      "Train Epoch: 20 [154944/225000 (69%)] Loss: 20489.371094\n",
      "Train Epoch: 20 [157440/225000 (70%)] Loss: 20168.921875\n",
      "Train Epoch: 20 [159936/225000 (71%)] Loss: 20658.187500\n",
      "Train Epoch: 20 [162432/225000 (72%)] Loss: 19898.703125\n",
      "Train Epoch: 20 [164928/225000 (73%)] Loss: 20388.349609\n",
      "Train Epoch: 20 [167424/225000 (74%)] Loss: 20589.585938\n",
      "Train Epoch: 20 [169920/225000 (76%)] Loss: 20524.904297\n",
      "Train Epoch: 20 [172416/225000 (77%)] Loss: 20278.363281\n",
      "Train Epoch: 20 [174912/225000 (78%)] Loss: 20116.419922\n",
      "Train Epoch: 20 [177408/225000 (79%)] Loss: 20517.945312\n",
      "Train Epoch: 20 [179904/225000 (80%)] Loss: 20611.367188\n",
      "Train Epoch: 20 [182400/225000 (81%)] Loss: 20457.105469\n",
      "Train Epoch: 20 [184896/225000 (82%)] Loss: 20112.140625\n",
      "Train Epoch: 20 [187392/225000 (83%)] Loss: 20299.304688\n",
      "Train Epoch: 20 [189888/225000 (84%)] Loss: 20388.693359\n",
      "Train Epoch: 20 [192384/225000 (86%)] Loss: 20142.392578\n",
      "Train Epoch: 20 [194880/225000 (87%)] Loss: 21261.039062\n",
      "Train Epoch: 20 [197376/225000 (88%)] Loss: 20128.500000\n",
      "Train Epoch: 20 [199872/225000 (89%)] Loss: 20218.146484\n",
      "Train Epoch: 20 [202368/225000 (90%)] Loss: 20506.476562\n",
      "Train Epoch: 20 [204864/225000 (91%)] Loss: 20386.080078\n",
      "Train Epoch: 20 [207360/225000 (92%)] Loss: 20599.310547\n",
      "Train Epoch: 20 [209856/225000 (93%)] Loss: 20472.250000\n",
      "Train Epoch: 20 [212352/225000 (94%)] Loss: 20552.107422\n",
      "Train Epoch: 20 [214848/225000 (95%)] Loss: 20468.050781\n",
      "Train Epoch: 20 [217344/225000 (97%)] Loss: 20768.148438\n",
      "Train Epoch: 20 [219840/225000 (98%)] Loss: 20583.058594\n",
      "Train Epoch: 20 [222336/225000 (99%)] Loss: 20704.203125\n",
      "Train Epoch: 20 [224832/225000 (100%)] Loss: 20507.736328\n",
      "    epoch          : 20\n",
      "    loss           : 20518.102910689526\n",
      "    val_loss       : 20365.991022587732\n",
      "Train Epoch: 21 [192/225000 (0%)] Loss: 20340.398438\n",
      "Train Epoch: 21 [2688/225000 (1%)] Loss: 20510.382812\n",
      "Train Epoch: 21 [5184/225000 (2%)] Loss: 20197.773438\n",
      "Train Epoch: 21 [7680/225000 (3%)] Loss: 20671.771484\n",
      "Train Epoch: 21 [10176/225000 (5%)] Loss: 20507.210938\n",
      "Train Epoch: 21 [12672/225000 (6%)] Loss: 20435.464844\n",
      "Train Epoch: 21 [15168/225000 (7%)] Loss: 20314.083984\n",
      "Train Epoch: 21 [17664/225000 (8%)] Loss: 20669.929688\n",
      "Train Epoch: 21 [20160/225000 (9%)] Loss: 20293.074219\n",
      "Train Epoch: 21 [22656/225000 (10%)] Loss: 20759.750000\n",
      "Train Epoch: 21 [25152/225000 (11%)] Loss: 20398.535156\n",
      "Train Epoch: 21 [27648/225000 (12%)] Loss: 20672.898438\n",
      "Train Epoch: 21 [30144/225000 (13%)] Loss: 19926.101562\n",
      "Train Epoch: 21 [32640/225000 (15%)] Loss: 20797.214844\n",
      "Train Epoch: 21 [35136/225000 (16%)] Loss: 20762.265625\n",
      "Train Epoch: 21 [37632/225000 (17%)] Loss: 20696.179688\n",
      "Train Epoch: 21 [40128/225000 (18%)] Loss: 20272.933594\n",
      "Train Epoch: 21 [42624/225000 (19%)] Loss: 20314.173828\n",
      "Train Epoch: 21 [45120/225000 (20%)] Loss: 20775.964844\n",
      "Train Epoch: 21 [47616/225000 (21%)] Loss: 20560.761719\n",
      "Train Epoch: 21 [50112/225000 (22%)] Loss: 20721.527344\n",
      "Train Epoch: 21 [52608/225000 (23%)] Loss: 20330.054688\n",
      "Train Epoch: 21 [55104/225000 (24%)] Loss: 20000.408203\n",
      "Train Epoch: 21 [57600/225000 (26%)] Loss: 20425.886719\n",
      "Train Epoch: 21 [60096/225000 (27%)] Loss: 20439.082031\n",
      "Train Epoch: 21 [62592/225000 (28%)] Loss: 20543.787109\n",
      "Train Epoch: 21 [65088/225000 (29%)] Loss: 20722.777344\n",
      "Train Epoch: 21 [67584/225000 (30%)] Loss: 20159.546875\n",
      "Train Epoch: 21 [70080/225000 (31%)] Loss: 20668.253906\n",
      "Train Epoch: 21 [72576/225000 (32%)] Loss: 20602.380859\n",
      "Train Epoch: 21 [75072/225000 (33%)] Loss: 20680.816406\n",
      "Train Epoch: 21 [77568/225000 (34%)] Loss: 20030.675781\n",
      "Train Epoch: 21 [80064/225000 (36%)] Loss: 20247.203125\n",
      "Train Epoch: 21 [82560/225000 (37%)] Loss: 20542.773438\n",
      "Train Epoch: 21 [85056/225000 (38%)] Loss: 20383.386719\n",
      "Train Epoch: 21 [87552/225000 (39%)] Loss: 20551.298828\n",
      "Train Epoch: 21 [90048/225000 (40%)] Loss: 20215.074219\n",
      "Train Epoch: 21 [92544/225000 (41%)] Loss: 20289.718750\n",
      "Train Epoch: 21 [95040/225000 (42%)] Loss: 20562.068359\n",
      "Train Epoch: 21 [97536/225000 (43%)] Loss: 20497.851562\n",
      "Train Epoch: 21 [100032/225000 (44%)] Loss: 20151.906250\n",
      "Train Epoch: 21 [102528/225000 (46%)] Loss: 20553.843750\n",
      "Train Epoch: 21 [105024/225000 (47%)] Loss: 20185.906250\n",
      "Train Epoch: 21 [107520/225000 (48%)] Loss: 20674.261719\n",
      "Train Epoch: 21 [110016/225000 (49%)] Loss: 20048.769531\n",
      "Train Epoch: 21 [112512/225000 (50%)] Loss: 20787.072266\n",
      "Train Epoch: 21 [115008/225000 (51%)] Loss: 20390.964844\n",
      "Train Epoch: 21 [117504/225000 (52%)] Loss: 20225.261719\n",
      "Train Epoch: 21 [120000/225000 (53%)] Loss: 20233.093750\n",
      "Train Epoch: 21 [122496/225000 (54%)] Loss: 20191.433594\n",
      "Train Epoch: 21 [124992/225000 (56%)] Loss: 20294.994141\n",
      "Train Epoch: 21 [127488/225000 (57%)] Loss: 20287.636719\n",
      "Train Epoch: 21 [129984/225000 (58%)] Loss: 20343.417969\n",
      "Train Epoch: 21 [132480/225000 (59%)] Loss: 20397.644531\n",
      "Train Epoch: 21 [134976/225000 (60%)] Loss: 20246.839844\n",
      "Train Epoch: 21 [137472/225000 (61%)] Loss: 20820.421875\n",
      "Train Epoch: 21 [139968/225000 (62%)] Loss: 20174.750000\n",
      "Train Epoch: 21 [142464/225000 (63%)] Loss: 20149.257812\n",
      "Train Epoch: 21 [144960/225000 (64%)] Loss: 20257.708984\n",
      "Train Epoch: 21 [147456/225000 (66%)] Loss: 20408.359375\n",
      "Train Epoch: 21 [149952/225000 (67%)] Loss: 20586.472656\n",
      "Train Epoch: 21 [152448/225000 (68%)] Loss: 20927.402344\n",
      "Train Epoch: 21 [154944/225000 (69%)] Loss: 20391.656250\n",
      "Train Epoch: 21 [157440/225000 (70%)] Loss: 20805.873047\n",
      "Train Epoch: 21 [159936/225000 (71%)] Loss: 20389.292969\n",
      "Train Epoch: 21 [162432/225000 (72%)] Loss: 20906.707031\n",
      "Train Epoch: 21 [164928/225000 (73%)] Loss: 20257.181641\n",
      "Train Epoch: 21 [167424/225000 (74%)] Loss: 21000.347656\n",
      "Train Epoch: 21 [169920/225000 (76%)] Loss: 20526.736328\n",
      "Train Epoch: 21 [172416/225000 (77%)] Loss: 19985.808594\n",
      "Train Epoch: 21 [174912/225000 (78%)] Loss: 20236.234375\n",
      "Train Epoch: 21 [177408/225000 (79%)] Loss: 20473.597656\n",
      "Train Epoch: 21 [179904/225000 (80%)] Loss: 20367.230469\n",
      "Train Epoch: 21 [182400/225000 (81%)] Loss: 19951.193359\n",
      "Train Epoch: 21 [184896/225000 (82%)] Loss: 20464.080078\n",
      "Train Epoch: 21 [187392/225000 (83%)] Loss: 20453.181641\n",
      "Train Epoch: 21 [189888/225000 (84%)] Loss: 20441.679688\n",
      "Train Epoch: 21 [192384/225000 (86%)] Loss: 20276.160156\n",
      "Train Epoch: 21 [194880/225000 (87%)] Loss: 20066.869141\n",
      "Train Epoch: 21 [197376/225000 (88%)] Loss: 20574.400391\n",
      "Train Epoch: 21 [199872/225000 (89%)] Loss: 20186.652344\n",
      "Train Epoch: 21 [202368/225000 (90%)] Loss: 20444.636719\n",
      "Train Epoch: 21 [204864/225000 (91%)] Loss: 20320.468750\n",
      "Train Epoch: 21 [207360/225000 (92%)] Loss: 21047.218750\n",
      "Train Epoch: 21 [209856/225000 (93%)] Loss: 20207.464844\n",
      "Train Epoch: 21 [212352/225000 (94%)] Loss: 20444.349609\n",
      "Train Epoch: 21 [214848/225000 (95%)] Loss: 20324.376953\n",
      "Train Epoch: 21 [217344/225000 (97%)] Loss: 20136.121094\n",
      "Train Epoch: 21 [219840/225000 (98%)] Loss: 20018.109375\n",
      "Train Epoch: 21 [222336/225000 (99%)] Loss: 20779.376953\n",
      "Train Epoch: 21 [224832/225000 (100%)] Loss: 20342.740234\n",
      "    epoch          : 21\n",
      "    loss           : 20464.97430440753\n",
      "    val_loss       : 20348.579479766257\n",
      "Train Epoch: 22 [192/225000 (0%)] Loss: 20253.484375\n",
      "Train Epoch: 22 [2688/225000 (1%)] Loss: 20677.906250\n",
      "Train Epoch: 22 [5184/225000 (2%)] Loss: 20566.710938\n",
      "Train Epoch: 22 [7680/225000 (3%)] Loss: 20401.171875\n",
      "Train Epoch: 22 [10176/225000 (5%)] Loss: 20481.119141\n",
      "Train Epoch: 22 [12672/225000 (6%)] Loss: 20249.714844\n",
      "Train Epoch: 22 [15168/225000 (7%)] Loss: 20314.347656\n",
      "Train Epoch: 22 [17664/225000 (8%)] Loss: 20152.761719\n",
      "Train Epoch: 22 [20160/225000 (9%)] Loss: 20470.437500\n",
      "Train Epoch: 22 [22656/225000 (10%)] Loss: 20046.031250\n",
      "Train Epoch: 22 [25152/225000 (11%)] Loss: 20759.273438\n",
      "Train Epoch: 22 [27648/225000 (12%)] Loss: 20155.433594\n",
      "Train Epoch: 22 [30144/225000 (13%)] Loss: 21122.070312\n",
      "Train Epoch: 22 [32640/225000 (15%)] Loss: 20663.148438\n",
      "Train Epoch: 22 [35136/225000 (16%)] Loss: 20998.363281\n",
      "Train Epoch: 22 [37632/225000 (17%)] Loss: 20563.113281\n",
      "Train Epoch: 22 [40128/225000 (18%)] Loss: 20327.132812\n",
      "Train Epoch: 22 [42624/225000 (19%)] Loss: 20562.878906\n",
      "Train Epoch: 22 [45120/225000 (20%)] Loss: 20640.171875\n",
      "Train Epoch: 22 [47616/225000 (21%)] Loss: 20220.363281\n",
      "Train Epoch: 22 [50112/225000 (22%)] Loss: 20216.992188\n",
      "Train Epoch: 22 [52608/225000 (23%)] Loss: 20718.087891\n",
      "Train Epoch: 22 [55104/225000 (24%)] Loss: 20298.017578\n",
      "Train Epoch: 22 [57600/225000 (26%)] Loss: 20247.066406\n",
      "Train Epoch: 22 [60096/225000 (27%)] Loss: 20383.638672\n",
      "Train Epoch: 22 [62592/225000 (28%)] Loss: 20680.414062\n",
      "Train Epoch: 22 [65088/225000 (29%)] Loss: 20280.439453\n",
      "Train Epoch: 22 [67584/225000 (30%)] Loss: 20700.074219\n",
      "Train Epoch: 22 [70080/225000 (31%)] Loss: 20885.609375\n",
      "Train Epoch: 22 [72576/225000 (32%)] Loss: 19959.578125\n",
      "Train Epoch: 22 [75072/225000 (33%)] Loss: 20518.500000\n",
      "Train Epoch: 22 [77568/225000 (34%)] Loss: 20514.867188\n",
      "Train Epoch: 22 [80064/225000 (36%)] Loss: 20792.859375\n",
      "Train Epoch: 22 [82560/225000 (37%)] Loss: 20812.281250\n",
      "Train Epoch: 22 [85056/225000 (38%)] Loss: 20415.648438\n",
      "Train Epoch: 22 [87552/225000 (39%)] Loss: 20272.486328\n",
      "Train Epoch: 22 [90048/225000 (40%)] Loss: 20068.023438\n",
      "Train Epoch: 22 [92544/225000 (41%)] Loss: 20028.839844\n",
      "Train Epoch: 22 [95040/225000 (42%)] Loss: 20422.222656\n",
      "Train Epoch: 22 [97536/225000 (43%)] Loss: 20873.691406\n",
      "Train Epoch: 22 [100032/225000 (44%)] Loss: 20260.798828\n",
      "Train Epoch: 22 [102528/225000 (46%)] Loss: 20600.093750\n",
      "Train Epoch: 22 [105024/225000 (47%)] Loss: 20260.953125\n",
      "Train Epoch: 22 [107520/225000 (48%)] Loss: 20216.796875\n",
      "Train Epoch: 22 [110016/225000 (49%)] Loss: 20323.699219\n",
      "Train Epoch: 22 [112512/225000 (50%)] Loss: 20392.496094\n",
      "Train Epoch: 22 [115008/225000 (51%)] Loss: 20302.011719\n",
      "Train Epoch: 22 [117504/225000 (52%)] Loss: 20780.517578\n",
      "Train Epoch: 22 [120000/225000 (53%)] Loss: 20763.308594\n",
      "Train Epoch: 22 [122496/225000 (54%)] Loss: 20714.730469\n",
      "Train Epoch: 22 [124992/225000 (56%)] Loss: 20567.615234\n",
      "Train Epoch: 22 [127488/225000 (57%)] Loss: 20560.259766\n",
      "Train Epoch: 22 [129984/225000 (58%)] Loss: 20128.712891\n",
      "Train Epoch: 22 [132480/225000 (59%)] Loss: 20604.484375\n",
      "Train Epoch: 22 [134976/225000 (60%)] Loss: 20526.746094\n",
      "Train Epoch: 22 [137472/225000 (61%)] Loss: 20742.455078\n",
      "Train Epoch: 22 [139968/225000 (62%)] Loss: 19937.585938\n",
      "Train Epoch: 22 [142464/225000 (63%)] Loss: 19901.960938\n",
      "Train Epoch: 22 [144960/225000 (64%)] Loss: 20488.636719\n",
      "Train Epoch: 22 [147456/225000 (66%)] Loss: 20650.210938\n",
      "Train Epoch: 22 [149952/225000 (67%)] Loss: 20320.490234\n",
      "Train Epoch: 22 [152448/225000 (68%)] Loss: 20087.775391\n",
      "Train Epoch: 22 [154944/225000 (69%)] Loss: 20713.785156\n",
      "Train Epoch: 22 [157440/225000 (70%)] Loss: 20173.107422\n",
      "Train Epoch: 22 [159936/225000 (71%)] Loss: 20350.468750\n",
      "Train Epoch: 22 [162432/225000 (72%)] Loss: 20551.824219\n",
      "Train Epoch: 22 [164928/225000 (73%)] Loss: 20309.089844\n",
      "Train Epoch: 22 [167424/225000 (74%)] Loss: 20102.765625\n",
      "Train Epoch: 22 [169920/225000 (76%)] Loss: 20314.617188\n",
      "Train Epoch: 22 [172416/225000 (77%)] Loss: 19928.386719\n",
      "Train Epoch: 22 [174912/225000 (78%)] Loss: 20164.820312\n",
      "Train Epoch: 22 [177408/225000 (79%)] Loss: 20449.457031\n",
      "Train Epoch: 22 [179904/225000 (80%)] Loss: 20207.037109\n",
      "Train Epoch: 22 [182400/225000 (81%)] Loss: 20825.675781\n",
      "Train Epoch: 22 [184896/225000 (82%)] Loss: 20287.308594\n",
      "Train Epoch: 22 [187392/225000 (83%)] Loss: 20384.046875\n",
      "Train Epoch: 22 [189888/225000 (84%)] Loss: 20227.992188\n",
      "Train Epoch: 22 [192384/225000 (86%)] Loss: 20560.857422\n",
      "Train Epoch: 22 [194880/225000 (87%)] Loss: 20645.539062\n",
      "Train Epoch: 22 [197376/225000 (88%)] Loss: 20356.080078\n",
      "Train Epoch: 22 [199872/225000 (89%)] Loss: 19880.187500\n",
      "Train Epoch: 22 [202368/225000 (90%)] Loss: 19616.912109\n",
      "Train Epoch: 22 [204864/225000 (91%)] Loss: 19959.875000\n",
      "Train Epoch: 22 [207360/225000 (92%)] Loss: 20372.816406\n",
      "Train Epoch: 22 [209856/225000 (93%)] Loss: 20515.033203\n",
      "Train Epoch: 22 [212352/225000 (94%)] Loss: 19878.455078\n",
      "Train Epoch: 22 [214848/225000 (95%)] Loss: 19915.019531\n",
      "Train Epoch: 22 [217344/225000 (97%)] Loss: 20138.156250\n",
      "Train Epoch: 22 [219840/225000 (98%)] Loss: 20482.031250\n",
      "Train Epoch: 22 [222336/225000 (99%)] Loss: 21081.636719\n",
      "Train Epoch: 22 [224832/225000 (100%)] Loss: 20495.824219\n",
      "    epoch          : 22\n",
      "    loss           : 20443.547098309515\n",
      "    val_loss       : 20417.542681264968\n",
      "Train Epoch: 23 [192/225000 (0%)] Loss: 20877.027344\n",
      "Train Epoch: 23 [2688/225000 (1%)] Loss: 20540.978516\n",
      "Train Epoch: 23 [5184/225000 (2%)] Loss: 20258.824219\n",
      "Train Epoch: 23 [7680/225000 (3%)] Loss: 20408.937500\n",
      "Train Epoch: 23 [10176/225000 (5%)] Loss: 20707.496094\n",
      "Train Epoch: 23 [12672/225000 (6%)] Loss: 20405.343750\n",
      "Train Epoch: 23 [15168/225000 (7%)] Loss: 19860.812500\n",
      "Train Epoch: 23 [17664/225000 (8%)] Loss: 20172.289062\n",
      "Train Epoch: 23 [20160/225000 (9%)] Loss: 20233.251953\n",
      "Train Epoch: 23 [22656/225000 (10%)] Loss: 20304.253906\n",
      "Train Epoch: 23 [25152/225000 (11%)] Loss: 20546.335938\n",
      "Train Epoch: 23 [27648/225000 (12%)] Loss: 20606.806641\n",
      "Train Epoch: 23 [30144/225000 (13%)] Loss: 19912.757812\n",
      "Train Epoch: 23 [32640/225000 (15%)] Loss: 20073.378906\n",
      "Train Epoch: 23 [35136/225000 (16%)] Loss: 20244.861328\n",
      "Train Epoch: 23 [37632/225000 (17%)] Loss: 20510.667969\n",
      "Train Epoch: 23 [40128/225000 (18%)] Loss: 20149.605469\n",
      "Train Epoch: 23 [42624/225000 (19%)] Loss: 20120.308594\n",
      "Train Epoch: 23 [45120/225000 (20%)] Loss: 20468.195312\n",
      "Train Epoch: 23 [47616/225000 (21%)] Loss: 20039.484375\n",
      "Train Epoch: 23 [50112/225000 (22%)] Loss: 20580.296875\n",
      "Train Epoch: 23 [52608/225000 (23%)] Loss: 20367.439453\n",
      "Train Epoch: 23 [55104/225000 (24%)] Loss: 20830.865234\n",
      "Train Epoch: 23 [57600/225000 (26%)] Loss: 20270.777344\n",
      "Train Epoch: 23 [60096/225000 (27%)] Loss: 19899.500000\n",
      "Train Epoch: 23 [62592/225000 (28%)] Loss: 20557.923828\n",
      "Train Epoch: 23 [65088/225000 (29%)] Loss: 20160.511719\n",
      "Train Epoch: 23 [67584/225000 (30%)] Loss: 20199.017578\n",
      "Train Epoch: 23 [70080/225000 (31%)] Loss: 20163.980469\n",
      "Train Epoch: 23 [72576/225000 (32%)] Loss: 20356.785156\n",
      "Train Epoch: 23 [75072/225000 (33%)] Loss: 20194.281250\n",
      "Train Epoch: 23 [77568/225000 (34%)] Loss: 20849.019531\n",
      "Train Epoch: 23 [80064/225000 (36%)] Loss: 20367.140625\n",
      "Train Epoch: 23 [82560/225000 (37%)] Loss: 20316.919922\n",
      "Train Epoch: 23 [85056/225000 (38%)] Loss: 20491.792969\n",
      "Train Epoch: 23 [87552/225000 (39%)] Loss: 19775.519531\n",
      "Train Epoch: 23 [90048/225000 (40%)] Loss: 20411.453125\n",
      "Train Epoch: 23 [92544/225000 (41%)] Loss: 20387.658203\n",
      "Train Epoch: 23 [95040/225000 (42%)] Loss: 20166.605469\n",
      "Train Epoch: 23 [97536/225000 (43%)] Loss: 20838.738281\n",
      "Train Epoch: 23 [100032/225000 (44%)] Loss: 19807.253906\n",
      "Train Epoch: 23 [102528/225000 (46%)] Loss: 20632.292969\n",
      "Train Epoch: 23 [105024/225000 (47%)] Loss: 20778.976562\n",
      "Train Epoch: 23 [107520/225000 (48%)] Loss: 20496.351562\n",
      "Train Epoch: 23 [110016/225000 (49%)] Loss: 20803.636719\n",
      "Train Epoch: 23 [112512/225000 (50%)] Loss: 20246.121094\n",
      "Train Epoch: 23 [115008/225000 (51%)] Loss: 21057.117188\n",
      "Train Epoch: 23 [117504/225000 (52%)] Loss: 20180.951172\n",
      "Train Epoch: 23 [120000/225000 (53%)] Loss: 20628.621094\n",
      "Train Epoch: 23 [122496/225000 (54%)] Loss: 20422.154297\n",
      "Train Epoch: 23 [124992/225000 (56%)] Loss: 20237.628906\n",
      "Train Epoch: 23 [127488/225000 (57%)] Loss: 20509.210938\n",
      "Train Epoch: 23 [129984/225000 (58%)] Loss: 20395.332031\n",
      "Train Epoch: 23 [132480/225000 (59%)] Loss: 20733.980469\n",
      "Train Epoch: 23 [134976/225000 (60%)] Loss: 20273.328125\n",
      "Train Epoch: 23 [137472/225000 (61%)] Loss: 20564.128906\n",
      "Train Epoch: 23 [139968/225000 (62%)] Loss: 20627.388672\n",
      "Train Epoch: 23 [142464/225000 (63%)] Loss: 20412.576172\n",
      "Train Epoch: 23 [144960/225000 (64%)] Loss: 20355.414062\n",
      "Train Epoch: 23 [147456/225000 (66%)] Loss: 19920.031250\n",
      "Train Epoch: 23 [149952/225000 (67%)] Loss: 20404.566406\n",
      "Train Epoch: 23 [152448/225000 (68%)] Loss: 20547.519531\n",
      "Train Epoch: 23 [154944/225000 (69%)] Loss: 20736.667969\n",
      "Train Epoch: 23 [157440/225000 (70%)] Loss: 20117.511719\n",
      "Train Epoch: 23 [159936/225000 (71%)] Loss: 20478.226562\n",
      "Train Epoch: 23 [162432/225000 (72%)] Loss: 19815.134766\n",
      "Train Epoch: 23 [164928/225000 (73%)] Loss: 20636.269531\n",
      "Train Epoch: 23 [167424/225000 (74%)] Loss: 20328.335938\n",
      "Train Epoch: 23 [169920/225000 (76%)] Loss: 20491.580078\n",
      "Train Epoch: 23 [172416/225000 (77%)] Loss: 20440.906250\n",
      "Train Epoch: 23 [174912/225000 (78%)] Loss: 19934.656250\n",
      "Train Epoch: 23 [177408/225000 (79%)] Loss: 20483.644531\n",
      "Train Epoch: 23 [179904/225000 (80%)] Loss: 20425.503906\n",
      "Train Epoch: 23 [182400/225000 (81%)] Loss: 20062.449219\n",
      "Train Epoch: 23 [184896/225000 (82%)] Loss: 20220.589844\n",
      "Train Epoch: 23 [187392/225000 (83%)] Loss: 20550.283203\n",
      "Train Epoch: 23 [189888/225000 (84%)] Loss: 20154.470703\n",
      "Train Epoch: 23 [192384/225000 (86%)] Loss: 20373.011719\n",
      "Train Epoch: 23 [194880/225000 (87%)] Loss: 20871.582031\n",
      "Train Epoch: 23 [197376/225000 (88%)] Loss: 20290.771484\n",
      "Train Epoch: 23 [199872/225000 (89%)] Loss: 20111.621094\n",
      "Train Epoch: 23 [202368/225000 (90%)] Loss: 20394.890625\n",
      "Train Epoch: 23 [204864/225000 (91%)] Loss: 20721.966797\n",
      "Train Epoch: 23 [207360/225000 (92%)] Loss: 20068.451172\n",
      "Train Epoch: 23 [209856/225000 (93%)] Loss: 20165.140625\n",
      "Train Epoch: 23 [212352/225000 (94%)] Loss: 20472.736328\n",
      "Train Epoch: 23 [214848/225000 (95%)] Loss: 20253.386719\n",
      "Train Epoch: 23 [217344/225000 (97%)] Loss: 20572.644531\n",
      "Train Epoch: 23 [219840/225000 (98%)] Loss: 20367.171875\n",
      "Train Epoch: 23 [222336/225000 (99%)] Loss: 20625.550781\n",
      "Train Epoch: 23 [224832/225000 (100%)] Loss: 20159.226562\n",
      "    epoch          : 23\n",
      "    loss           : 20428.804765824978\n",
      "    val_loss       : 20316.44812950469\n",
      "Train Epoch: 24 [192/225000 (0%)] Loss: 20266.267578\n",
      "Train Epoch: 24 [2688/225000 (1%)] Loss: 20647.162109\n",
      "Train Epoch: 24 [5184/225000 (2%)] Loss: 20325.628906\n",
      "Train Epoch: 24 [7680/225000 (3%)] Loss: 20472.480469\n",
      "Train Epoch: 24 [10176/225000 (5%)] Loss: 20766.062500\n",
      "Train Epoch: 24 [12672/225000 (6%)] Loss: 20433.386719\n",
      "Train Epoch: 24 [15168/225000 (7%)] Loss: 20240.492188\n",
      "Train Epoch: 24 [17664/225000 (8%)] Loss: 20131.779297\n",
      "Train Epoch: 24 [20160/225000 (9%)] Loss: 19877.515625\n",
      "Train Epoch: 24 [22656/225000 (10%)] Loss: 20606.730469\n",
      "Train Epoch: 24 [25152/225000 (11%)] Loss: 20271.707031\n",
      "Train Epoch: 24 [27648/225000 (12%)] Loss: 20234.447266\n",
      "Train Epoch: 24 [30144/225000 (13%)] Loss: 19633.460938\n",
      "Train Epoch: 24 [32640/225000 (15%)] Loss: 20629.515625\n",
      "Train Epoch: 24 [35136/225000 (16%)] Loss: 20513.031250\n",
      "Train Epoch: 24 [37632/225000 (17%)] Loss: 20390.328125\n",
      "Train Epoch: 24 [40128/225000 (18%)] Loss: 20689.800781\n",
      "Train Epoch: 24 [42624/225000 (19%)] Loss: 20769.095703\n",
      "Train Epoch: 24 [45120/225000 (20%)] Loss: 20172.789062\n",
      "Train Epoch: 24 [47616/225000 (21%)] Loss: 20318.671875\n",
      "Train Epoch: 24 [50112/225000 (22%)] Loss: 20648.595703\n",
      "Train Epoch: 24 [52608/225000 (23%)] Loss: 20341.710938\n",
      "Train Epoch: 24 [55104/225000 (24%)] Loss: 20671.281250\n",
      "Train Epoch: 24 [57600/225000 (26%)] Loss: 20802.765625\n",
      "Train Epoch: 24 [60096/225000 (27%)] Loss: 20743.554688\n",
      "Train Epoch: 24 [62592/225000 (28%)] Loss: 20378.195312\n",
      "Train Epoch: 24 [65088/225000 (29%)] Loss: 20613.173828\n",
      "Train Epoch: 24 [67584/225000 (30%)] Loss: 20459.099609\n",
      "Train Epoch: 24 [70080/225000 (31%)] Loss: 20429.048828\n",
      "Train Epoch: 24 [72576/225000 (32%)] Loss: 20288.851562\n",
      "Train Epoch: 24 [75072/225000 (33%)] Loss: 20249.705078\n",
      "Train Epoch: 24 [77568/225000 (34%)] Loss: 20480.769531\n",
      "Train Epoch: 24 [80064/225000 (36%)] Loss: 20644.515625\n",
      "Train Epoch: 24 [82560/225000 (37%)] Loss: 20145.265625\n",
      "Train Epoch: 24 [85056/225000 (38%)] Loss: 20849.650391\n",
      "Train Epoch: 24 [87552/225000 (39%)] Loss: 20535.400391\n",
      "Train Epoch: 24 [90048/225000 (40%)] Loss: 20505.935547\n",
      "Train Epoch: 24 [92544/225000 (41%)] Loss: 20530.417969\n",
      "Train Epoch: 24 [95040/225000 (42%)] Loss: 20037.931641\n",
      "Train Epoch: 24 [97536/225000 (43%)] Loss: 20379.550781\n",
      "Train Epoch: 24 [100032/225000 (44%)] Loss: 20232.050781\n",
      "Train Epoch: 24 [102528/225000 (46%)] Loss: 20649.234375\n",
      "Train Epoch: 24 [105024/225000 (47%)] Loss: 20346.388672\n",
      "Train Epoch: 24 [107520/225000 (48%)] Loss: 20222.814453\n",
      "Train Epoch: 24 [110016/225000 (49%)] Loss: 20510.937500\n",
      "Train Epoch: 24 [112512/225000 (50%)] Loss: 20569.808594\n",
      "Train Epoch: 24 [115008/225000 (51%)] Loss: 20665.238281\n",
      "Train Epoch: 24 [117504/225000 (52%)] Loss: 20438.753906\n",
      "Train Epoch: 24 [120000/225000 (53%)] Loss: 20219.710938\n",
      "Train Epoch: 24 [122496/225000 (54%)] Loss: 20367.457031\n",
      "Train Epoch: 24 [124992/225000 (56%)] Loss: 20757.511719\n",
      "Train Epoch: 24 [127488/225000 (57%)] Loss: 20068.408203\n",
      "Train Epoch: 24 [129984/225000 (58%)] Loss: 20105.410156\n",
      "Train Epoch: 24 [132480/225000 (59%)] Loss: 20077.400391\n",
      "Train Epoch: 24 [134976/225000 (60%)] Loss: 20123.753906\n",
      "Train Epoch: 24 [137472/225000 (61%)] Loss: 20374.582031\n",
      "Train Epoch: 24 [139968/225000 (62%)] Loss: 20382.972656\n",
      "Train Epoch: 24 [142464/225000 (63%)] Loss: 20237.191406\n",
      "Train Epoch: 24 [144960/225000 (64%)] Loss: 20249.917969\n",
      "Train Epoch: 24 [147456/225000 (66%)] Loss: 20434.548828\n",
      "Train Epoch: 24 [149952/225000 (67%)] Loss: 20363.341797\n",
      "Train Epoch: 24 [152448/225000 (68%)] Loss: 20747.271484\n",
      "Train Epoch: 24 [154944/225000 (69%)] Loss: 20688.796875\n",
      "Train Epoch: 24 [157440/225000 (70%)] Loss: 19794.890625\n",
      "Train Epoch: 24 [159936/225000 (71%)] Loss: 20195.681641\n",
      "Train Epoch: 24 [162432/225000 (72%)] Loss: 20836.337891\n",
      "Train Epoch: 24 [164928/225000 (73%)] Loss: 20361.746094\n",
      "Train Epoch: 24 [167424/225000 (74%)] Loss: 20157.664062\n",
      "Train Epoch: 24 [169920/225000 (76%)] Loss: 20205.910156\n",
      "Train Epoch: 24 [172416/225000 (77%)] Loss: 20569.496094\n",
      "Train Epoch: 24 [174912/225000 (78%)] Loss: 19872.308594\n",
      "Train Epoch: 24 [177408/225000 (79%)] Loss: 20278.597656\n",
      "Train Epoch: 24 [179904/225000 (80%)] Loss: 20702.597656\n",
      "Train Epoch: 24 [182400/225000 (81%)] Loss: 19860.562500\n",
      "Train Epoch: 24 [184896/225000 (82%)] Loss: 20875.351562\n",
      "Train Epoch: 24 [187392/225000 (83%)] Loss: 19855.109375\n",
      "Train Epoch: 24 [189888/225000 (84%)] Loss: 20283.683594\n",
      "Train Epoch: 24 [192384/225000 (86%)] Loss: 20617.931641\n",
      "Train Epoch: 24 [194880/225000 (87%)] Loss: 20837.351562\n",
      "Train Epoch: 24 [197376/225000 (88%)] Loss: 20147.593750\n",
      "Train Epoch: 24 [199872/225000 (89%)] Loss: 20122.140625\n",
      "Train Epoch: 24 [202368/225000 (90%)] Loss: 19623.558594\n",
      "Train Epoch: 24 [204864/225000 (91%)] Loss: 20354.763672\n",
      "Train Epoch: 24 [207360/225000 (92%)] Loss: 20285.480469\n",
      "Train Epoch: 24 [209856/225000 (93%)] Loss: 20287.414062\n",
      "Train Epoch: 24 [212352/225000 (94%)] Loss: 20750.916016\n",
      "Train Epoch: 24 [214848/225000 (95%)] Loss: 20268.562500\n",
      "Train Epoch: 24 [217344/225000 (97%)] Loss: 19972.050781\n",
      "Train Epoch: 24 [219840/225000 (98%)] Loss: 20671.925781\n",
      "Train Epoch: 24 [222336/225000 (99%)] Loss: 20248.039062\n",
      "Train Epoch: 24 [224832/225000 (100%)] Loss: 20155.988281\n",
      "    epoch          : 24\n",
      "    loss           : 20401.4365834311\n",
      "    val_loss       : 20299.083942032954\n",
      "Train Epoch: 25 [192/225000 (0%)] Loss: 20493.433594\n",
      "Train Epoch: 25 [2688/225000 (1%)] Loss: 20394.107422\n",
      "Train Epoch: 25 [5184/225000 (2%)] Loss: 20438.175781\n",
      "Train Epoch: 25 [7680/225000 (3%)] Loss: 20112.242188\n",
      "Train Epoch: 25 [10176/225000 (5%)] Loss: 20281.601562\n",
      "Train Epoch: 25 [12672/225000 (6%)] Loss: 19651.476562\n",
      "Train Epoch: 25 [15168/225000 (7%)] Loss: 20226.312500\n",
      "Train Epoch: 25 [17664/225000 (8%)] Loss: 21039.054688\n",
      "Train Epoch: 25 [20160/225000 (9%)] Loss: 20533.324219\n",
      "Train Epoch: 25 [22656/225000 (10%)] Loss: 19762.339844\n",
      "Train Epoch: 25 [25152/225000 (11%)] Loss: 20837.472656\n",
      "Train Epoch: 25 [27648/225000 (12%)] Loss: 20578.871094\n",
      "Train Epoch: 25 [30144/225000 (13%)] Loss: 19968.898438\n",
      "Train Epoch: 25 [32640/225000 (15%)] Loss: 20505.042969\n",
      "Train Epoch: 25 [35136/225000 (16%)] Loss: 19974.398438\n",
      "Train Epoch: 25 [37632/225000 (17%)] Loss: 20549.085938\n",
      "Train Epoch: 25 [40128/225000 (18%)] Loss: 20737.681641\n",
      "Train Epoch: 25 [42624/225000 (19%)] Loss: 20163.345703\n",
      "Train Epoch: 25 [45120/225000 (20%)] Loss: 20396.964844\n",
      "Train Epoch: 25 [47616/225000 (21%)] Loss: 20390.539062\n",
      "Train Epoch: 25 [50112/225000 (22%)] Loss: 20417.080078\n",
      "Train Epoch: 25 [52608/225000 (23%)] Loss: 20356.546875\n",
      "Train Epoch: 25 [55104/225000 (24%)] Loss: 20384.753906\n",
      "Train Epoch: 25 [57600/225000 (26%)] Loss: 20988.181641\n",
      "Train Epoch: 25 [60096/225000 (27%)] Loss: 20860.085938\n",
      "Train Epoch: 25 [62592/225000 (28%)] Loss: 20998.376953\n",
      "Train Epoch: 25 [65088/225000 (29%)] Loss: 20248.353516\n",
      "Train Epoch: 25 [67584/225000 (30%)] Loss: 20472.812500\n",
      "Train Epoch: 25 [70080/225000 (31%)] Loss: 20301.648438\n",
      "Train Epoch: 25 [72576/225000 (32%)] Loss: 20309.781250\n",
      "Train Epoch: 25 [75072/225000 (33%)] Loss: 20480.859375\n",
      "Train Epoch: 25 [77568/225000 (34%)] Loss: 20159.396484\n",
      "Train Epoch: 25 [80064/225000 (36%)] Loss: 20090.714844\n",
      "Train Epoch: 25 [82560/225000 (37%)] Loss: 20274.779297\n",
      "Train Epoch: 25 [85056/225000 (38%)] Loss: 20214.396484\n",
      "Train Epoch: 25 [87552/225000 (39%)] Loss: 20667.031250\n",
      "Train Epoch: 25 [90048/225000 (40%)] Loss: 20437.400391\n",
      "Train Epoch: 25 [92544/225000 (41%)] Loss: 19992.066406\n",
      "Train Epoch: 25 [95040/225000 (42%)] Loss: 20135.261719\n",
      "Train Epoch: 25 [97536/225000 (43%)] Loss: 20821.433594\n",
      "Train Epoch: 25 [100032/225000 (44%)] Loss: 20338.785156\n",
      "Train Epoch: 25 [102528/225000 (46%)] Loss: 20729.320312\n",
      "Train Epoch: 25 [105024/225000 (47%)] Loss: 20351.296875\n",
      "Train Epoch: 25 [107520/225000 (48%)] Loss: 20269.078125\n",
      "Train Epoch: 25 [110016/225000 (49%)] Loss: 20290.496094\n",
      "Train Epoch: 25 [112512/225000 (50%)] Loss: 19882.587891\n",
      "Train Epoch: 25 [115008/225000 (51%)] Loss: 20459.679688\n",
      "Train Epoch: 25 [117504/225000 (52%)] Loss: 20309.386719\n",
      "Train Epoch: 25 [120000/225000 (53%)] Loss: 20036.753906\n",
      "Train Epoch: 25 [122496/225000 (54%)] Loss: 20666.105469\n",
      "Train Epoch: 25 [124992/225000 (56%)] Loss: 19831.732422\n",
      "Train Epoch: 25 [127488/225000 (57%)] Loss: 20289.023438\n",
      "Train Epoch: 25 [129984/225000 (58%)] Loss: 20260.423828\n",
      "Train Epoch: 25 [132480/225000 (59%)] Loss: 20294.576172\n",
      "Train Epoch: 25 [134976/225000 (60%)] Loss: 20193.378906\n",
      "Train Epoch: 25 [137472/225000 (61%)] Loss: 20705.177734\n",
      "Train Epoch: 25 [139968/225000 (62%)] Loss: 20204.406250\n",
      "Train Epoch: 25 [142464/225000 (63%)] Loss: 20373.869141\n",
      "Train Epoch: 25 [144960/225000 (64%)] Loss: 20254.857422\n",
      "Train Epoch: 25 [147456/225000 (66%)] Loss: 20773.835938\n",
      "Train Epoch: 25 [149952/225000 (67%)] Loss: 19871.792969\n",
      "Train Epoch: 25 [152448/225000 (68%)] Loss: 20324.140625\n",
      "Train Epoch: 25 [154944/225000 (69%)] Loss: 20593.835938\n",
      "Train Epoch: 25 [157440/225000 (70%)] Loss: 20033.699219\n",
      "Train Epoch: 25 [159936/225000 (71%)] Loss: 20673.507812\n",
      "Train Epoch: 25 [162432/225000 (72%)] Loss: 20341.761719\n",
      "Train Epoch: 25 [164928/225000 (73%)] Loss: 19989.257812\n",
      "Train Epoch: 25 [167424/225000 (74%)] Loss: 20305.769531\n",
      "Train Epoch: 25 [169920/225000 (76%)] Loss: 20319.703125\n",
      "Train Epoch: 25 [172416/225000 (77%)] Loss: 20427.187500\n",
      "Train Epoch: 25 [174912/225000 (78%)] Loss: 20034.015625\n",
      "Train Epoch: 25 [177408/225000 (79%)] Loss: 20492.507812\n",
      "Train Epoch: 25 [179904/225000 (80%)] Loss: 20438.109375\n",
      "Train Epoch: 25 [182400/225000 (81%)] Loss: 20473.761719\n",
      "Train Epoch: 25 [184896/225000 (82%)] Loss: 20219.785156\n",
      "Train Epoch: 25 [187392/225000 (83%)] Loss: 20232.597656\n",
      "Train Epoch: 25 [189888/225000 (84%)] Loss: 21107.386719\n",
      "Train Epoch: 25 [192384/225000 (86%)] Loss: 20495.974609\n",
      "Train Epoch: 25 [194880/225000 (87%)] Loss: 20547.550781\n",
      "Train Epoch: 25 [197376/225000 (88%)] Loss: 20600.017578\n",
      "Train Epoch: 25 [199872/225000 (89%)] Loss: 20724.865234\n",
      "Train Epoch: 25 [202368/225000 (90%)] Loss: 20950.273438\n",
      "Train Epoch: 25 [204864/225000 (91%)] Loss: 20570.337891\n",
      "Train Epoch: 25 [207360/225000 (92%)] Loss: 20634.746094\n",
      "Train Epoch: 25 [209856/225000 (93%)] Loss: 20369.636719\n",
      "Train Epoch: 25 [212352/225000 (94%)] Loss: 20784.367188\n",
      "Train Epoch: 25 [214848/225000 (95%)] Loss: 20649.781250\n",
      "Train Epoch: 25 [217344/225000 (97%)] Loss: 20467.787109\n",
      "Train Epoch: 25 [219840/225000 (98%)] Loss: 20616.433594\n",
      "Train Epoch: 25 [222336/225000 (99%)] Loss: 20235.246094\n",
      "Train Epoch: 25 [224832/225000 (100%)] Loss: 20539.988281\n",
      "    epoch          : 25\n",
      "    loss           : 20428.28915082391\n",
      "    val_loss       : 20289.354559149906\n",
      "Train Epoch: 26 [192/225000 (0%)] Loss: 20907.417969\n",
      "Train Epoch: 26 [2688/225000 (1%)] Loss: 20236.261719\n",
      "Train Epoch: 26 [5184/225000 (2%)] Loss: 20990.113281\n",
      "Train Epoch: 26 [7680/225000 (3%)] Loss: 20522.548828\n",
      "Train Epoch: 26 [10176/225000 (5%)] Loss: 20540.789062\n",
      "Train Epoch: 26 [12672/225000 (6%)] Loss: 20261.578125\n",
      "Train Epoch: 26 [15168/225000 (7%)] Loss: 20692.886719\n",
      "Train Epoch: 26 [17664/225000 (8%)] Loss: 20397.750000\n",
      "Train Epoch: 26 [20160/225000 (9%)] Loss: 20237.324219\n",
      "Train Epoch: 26 [22656/225000 (10%)] Loss: 20116.638672\n",
      "Train Epoch: 26 [25152/225000 (11%)] Loss: 20737.935547\n",
      "Train Epoch: 26 [27648/225000 (12%)] Loss: 20516.736328\n",
      "Train Epoch: 26 [30144/225000 (13%)] Loss: 19922.574219\n",
      "Train Epoch: 26 [32640/225000 (15%)] Loss: 19899.968750\n",
      "Train Epoch: 26 [35136/225000 (16%)] Loss: 20163.298828\n",
      "Train Epoch: 26 [37632/225000 (17%)] Loss: 20858.519531\n",
      "Train Epoch: 26 [40128/225000 (18%)] Loss: 20543.103516\n",
      "Train Epoch: 26 [42624/225000 (19%)] Loss: 20547.300781\n",
      "Train Epoch: 26 [45120/225000 (20%)] Loss: 20790.742188\n",
      "Train Epoch: 26 [47616/225000 (21%)] Loss: 20455.671875\n",
      "Train Epoch: 26 [50112/225000 (22%)] Loss: 20685.500000\n",
      "Train Epoch: 26 [52608/225000 (23%)] Loss: 20620.193359\n",
      "Train Epoch: 26 [55104/225000 (24%)] Loss: 20025.208984\n",
      "Train Epoch: 26 [57600/225000 (26%)] Loss: 20214.843750\n",
      "Train Epoch: 26 [60096/225000 (27%)] Loss: 20029.707031\n",
      "Train Epoch: 26 [62592/225000 (28%)] Loss: 20958.343750\n",
      "Train Epoch: 26 [65088/225000 (29%)] Loss: 20479.472656\n",
      "Train Epoch: 26 [67584/225000 (30%)] Loss: 20399.957031\n",
      "Train Epoch: 26 [70080/225000 (31%)] Loss: 20346.621094\n",
      "Train Epoch: 26 [72576/225000 (32%)] Loss: 20414.773438\n",
      "Train Epoch: 26 [75072/225000 (33%)] Loss: 20680.085938\n",
      "Train Epoch: 26 [77568/225000 (34%)] Loss: 20120.246094\n",
      "Train Epoch: 26 [80064/225000 (36%)] Loss: 20224.027344\n",
      "Train Epoch: 26 [82560/225000 (37%)] Loss: 20612.441406\n",
      "Train Epoch: 26 [85056/225000 (38%)] Loss: 20459.402344\n",
      "Train Epoch: 26 [87552/225000 (39%)] Loss: 20263.097656\n",
      "Train Epoch: 26 [90048/225000 (40%)] Loss: 20183.392578\n",
      "Train Epoch: 26 [92544/225000 (41%)] Loss: 20125.042969\n",
      "Train Epoch: 26 [95040/225000 (42%)] Loss: 20377.683594\n",
      "Train Epoch: 26 [97536/225000 (43%)] Loss: 20714.912109\n",
      "Train Epoch: 26 [100032/225000 (44%)] Loss: 20347.626953\n",
      "Train Epoch: 26 [102528/225000 (46%)] Loss: 20362.355469\n",
      "Train Epoch: 26 [105024/225000 (47%)] Loss: 20242.671875\n",
      "Train Epoch: 26 [107520/225000 (48%)] Loss: 20513.878906\n",
      "Train Epoch: 26 [110016/225000 (49%)] Loss: 20197.384766\n",
      "Train Epoch: 26 [112512/225000 (50%)] Loss: 20494.642578\n",
      "Train Epoch: 26 [115008/225000 (51%)] Loss: 20097.121094\n",
      "Train Epoch: 26 [117504/225000 (52%)] Loss: 20512.750000\n",
      "Train Epoch: 26 [120000/225000 (53%)] Loss: 20611.156250\n",
      "Train Epoch: 26 [122496/225000 (54%)] Loss: 20554.949219\n",
      "Train Epoch: 26 [124992/225000 (56%)] Loss: 20297.226562\n",
      "Train Epoch: 26 [127488/225000 (57%)] Loss: 20684.691406\n",
      "Train Epoch: 26 [129984/225000 (58%)] Loss: 20632.816406\n",
      "Train Epoch: 26 [132480/225000 (59%)] Loss: 19894.531250\n",
      "Train Epoch: 26 [134976/225000 (60%)] Loss: 20574.218750\n",
      "Train Epoch: 26 [137472/225000 (61%)] Loss: 20628.875000\n",
      "Train Epoch: 26 [139968/225000 (62%)] Loss: 20301.945312\n",
      "Train Epoch: 26 [142464/225000 (63%)] Loss: 20031.476562\n",
      "Train Epoch: 26 [144960/225000 (64%)] Loss: 20497.732422\n",
      "Train Epoch: 26 [147456/225000 (66%)] Loss: 20762.767578\n",
      "Train Epoch: 26 [149952/225000 (67%)] Loss: 20670.505859\n",
      "Train Epoch: 26 [152448/225000 (68%)] Loss: 20233.046875\n",
      "Train Epoch: 26 [154944/225000 (69%)] Loss: 20449.880859\n",
      "Train Epoch: 26 [157440/225000 (70%)] Loss: 20403.142578\n",
      "Train Epoch: 26 [159936/225000 (71%)] Loss: 19985.119141\n",
      "Train Epoch: 26 [162432/225000 (72%)] Loss: 20652.457031\n",
      "Train Epoch: 26 [164928/225000 (73%)] Loss: 20181.699219\n",
      "Train Epoch: 26 [167424/225000 (74%)] Loss: 20158.603516\n",
      "Train Epoch: 26 [169920/225000 (76%)] Loss: 20405.757812\n",
      "Train Epoch: 26 [172416/225000 (77%)] Loss: 20007.277344\n",
      "Train Epoch: 26 [174912/225000 (78%)] Loss: 19898.755859\n",
      "Train Epoch: 26 [177408/225000 (79%)] Loss: 20128.937500\n",
      "Train Epoch: 26 [179904/225000 (80%)] Loss: 20367.869141\n",
      "Train Epoch: 26 [182400/225000 (81%)] Loss: 20241.324219\n",
      "Train Epoch: 26 [184896/225000 (82%)] Loss: 21104.179688\n",
      "Train Epoch: 26 [187392/225000 (83%)] Loss: 20690.843750\n",
      "Train Epoch: 26 [189888/225000 (84%)] Loss: 20581.560547\n",
      "Train Epoch: 26 [192384/225000 (86%)] Loss: 20400.837891\n",
      "Train Epoch: 26 [194880/225000 (87%)] Loss: 20060.748047\n",
      "Train Epoch: 26 [197376/225000 (88%)] Loss: 19992.939453\n",
      "Train Epoch: 26 [199872/225000 (89%)] Loss: 20411.033203\n",
      "Train Epoch: 26 [202368/225000 (90%)] Loss: 19953.117188\n",
      "Train Epoch: 26 [204864/225000 (91%)] Loss: 20426.042969\n",
      "Train Epoch: 26 [207360/225000 (92%)] Loss: 20376.363281\n",
      "Train Epoch: 26 [209856/225000 (93%)] Loss: 20252.583984\n",
      "Train Epoch: 26 [212352/225000 (94%)] Loss: 20323.785156\n",
      "Train Epoch: 26 [214848/225000 (95%)] Loss: 20709.386719\n",
      "Train Epoch: 26 [217344/225000 (97%)] Loss: 20261.269531\n",
      "Train Epoch: 26 [219840/225000 (98%)] Loss: 20201.195312\n",
      "Train Epoch: 26 [222336/225000 (99%)] Loss: 20270.568359\n",
      "Train Epoch: 26 [224832/225000 (100%)] Loss: 20777.015625\n",
      "    epoch          : 26\n",
      "    loss           : 20385.178392638118\n",
      "    val_loss       : 20276.094029752352\n",
      "Train Epoch: 27 [192/225000 (0%)] Loss: 20299.074219\n",
      "Train Epoch: 27 [2688/225000 (1%)] Loss: 19831.636719\n",
      "Train Epoch: 27 [5184/225000 (2%)] Loss: 20230.427734\n",
      "Train Epoch: 27 [7680/225000 (3%)] Loss: 20750.445312\n",
      "Train Epoch: 27 [10176/225000 (5%)] Loss: 20217.671875\n",
      "Train Epoch: 27 [12672/225000 (6%)] Loss: 20212.050781\n",
      "Train Epoch: 27 [15168/225000 (7%)] Loss: 20255.781250\n",
      "Train Epoch: 27 [17664/225000 (8%)] Loss: 20695.574219\n",
      "Train Epoch: 27 [20160/225000 (9%)] Loss: 20271.974609\n",
      "Train Epoch: 27 [22656/225000 (10%)] Loss: 20391.441406\n",
      "Train Epoch: 27 [25152/225000 (11%)] Loss: 20633.316406\n",
      "Train Epoch: 27 [27648/225000 (12%)] Loss: 20368.160156\n",
      "Train Epoch: 27 [30144/225000 (13%)] Loss: 20136.792969\n",
      "Train Epoch: 27 [32640/225000 (15%)] Loss: 20181.345703\n",
      "Train Epoch: 27 [35136/225000 (16%)] Loss: 20222.476562\n",
      "Train Epoch: 27 [37632/225000 (17%)] Loss: 20408.234375\n",
      "Train Epoch: 27 [40128/225000 (18%)] Loss: 20154.554688\n",
      "Train Epoch: 27 [42624/225000 (19%)] Loss: 20209.187500\n",
      "Train Epoch: 27 [45120/225000 (20%)] Loss: 20599.882812\n",
      "Train Epoch: 27 [47616/225000 (21%)] Loss: 20190.316406\n",
      "Train Epoch: 27 [50112/225000 (22%)] Loss: 20435.757812\n",
      "Train Epoch: 27 [52608/225000 (23%)] Loss: 20621.730469\n",
      "Train Epoch: 27 [55104/225000 (24%)] Loss: 19913.744141\n",
      "Train Epoch: 27 [57600/225000 (26%)] Loss: 20038.207031\n",
      "Train Epoch: 27 [60096/225000 (27%)] Loss: 20596.757812\n",
      "Train Epoch: 27 [62592/225000 (28%)] Loss: 20545.007812\n",
      "Train Epoch: 27 [65088/225000 (29%)] Loss: 20361.335938\n",
      "Train Epoch: 27 [67584/225000 (30%)] Loss: 20129.718750\n",
      "Train Epoch: 27 [70080/225000 (31%)] Loss: 20551.656250\n",
      "Train Epoch: 27 [72576/225000 (32%)] Loss: 20170.289062\n",
      "Train Epoch: 27 [75072/225000 (33%)] Loss: 20700.664062\n",
      "Train Epoch: 27 [77568/225000 (34%)] Loss: 20339.185547\n",
      "Train Epoch: 27 [80064/225000 (36%)] Loss: 20186.195312\n",
      "Train Epoch: 27 [82560/225000 (37%)] Loss: 20527.582031\n",
      "Train Epoch: 27 [85056/225000 (38%)] Loss: 20846.416016\n",
      "Train Epoch: 27 [87552/225000 (39%)] Loss: 20468.636719\n",
      "Train Epoch: 27 [90048/225000 (40%)] Loss: 20979.238281\n",
      "Train Epoch: 27 [92544/225000 (41%)] Loss: 20132.816406\n",
      "Train Epoch: 27 [95040/225000 (42%)] Loss: 20935.234375\n",
      "Train Epoch: 27 [97536/225000 (43%)] Loss: 20246.468750\n",
      "Train Epoch: 27 [100032/225000 (44%)] Loss: 20253.212891\n",
      "Train Epoch: 27 [102528/225000 (46%)] Loss: 19953.132812\n",
      "Train Epoch: 27 [105024/225000 (47%)] Loss: 20196.037109\n",
      "Train Epoch: 27 [107520/225000 (48%)] Loss: 20315.675781\n",
      "Train Epoch: 27 [110016/225000 (49%)] Loss: 20370.578125\n",
      "Train Epoch: 27 [112512/225000 (50%)] Loss: 20349.535156\n",
      "Train Epoch: 27 [115008/225000 (51%)] Loss: 20070.984375\n",
      "Train Epoch: 27 [117504/225000 (52%)] Loss: 20258.083984\n",
      "Train Epoch: 27 [120000/225000 (53%)] Loss: 20316.808594\n",
      "Train Epoch: 27 [122496/225000 (54%)] Loss: 20599.480469\n",
      "Train Epoch: 27 [124992/225000 (56%)] Loss: 20908.687500\n",
      "Train Epoch: 27 [127488/225000 (57%)] Loss: 20367.125000\n",
      "Train Epoch: 27 [129984/225000 (58%)] Loss: 20779.585938\n",
      "Train Epoch: 27 [132480/225000 (59%)] Loss: 20924.830078\n",
      "Train Epoch: 27 [134976/225000 (60%)] Loss: 20221.714844\n",
      "Train Epoch: 27 [137472/225000 (61%)] Loss: 19644.273438\n",
      "Train Epoch: 27 [139968/225000 (62%)] Loss: 19925.753906\n",
      "Train Epoch: 27 [142464/225000 (63%)] Loss: 20974.960938\n",
      "Train Epoch: 27 [144960/225000 (64%)] Loss: 19895.007812\n",
      "Train Epoch: 27 [147456/225000 (66%)] Loss: 19995.203125\n",
      "Train Epoch: 27 [149952/225000 (67%)] Loss: 20465.271484\n",
      "Train Epoch: 27 [152448/225000 (68%)] Loss: 20145.869141\n",
      "Train Epoch: 27 [154944/225000 (69%)] Loss: 20446.878906\n",
      "Train Epoch: 27 [157440/225000 (70%)] Loss: 19763.800781\n",
      "Train Epoch: 27 [159936/225000 (71%)] Loss: 20093.902344\n",
      "Train Epoch: 27 [162432/225000 (72%)] Loss: 20624.591797\n",
      "Train Epoch: 27 [164928/225000 (73%)] Loss: 20061.156250\n",
      "Train Epoch: 27 [167424/225000 (74%)] Loss: 19889.742188\n",
      "Train Epoch: 27 [169920/225000 (76%)] Loss: 20153.746094\n",
      "Train Epoch: 27 [172416/225000 (77%)] Loss: 20546.550781\n",
      "Train Epoch: 27 [174912/225000 (78%)] Loss: 20138.343750\n",
      "Train Epoch: 27 [177408/225000 (79%)] Loss: 20733.355469\n",
      "Train Epoch: 27 [179904/225000 (80%)] Loss: 20272.320312\n",
      "Train Epoch: 27 [182400/225000 (81%)] Loss: 20664.882812\n",
      "Train Epoch: 27 [184896/225000 (82%)] Loss: 20519.980469\n",
      "Train Epoch: 27 [187392/225000 (83%)] Loss: 20694.722656\n",
      "Train Epoch: 27 [189888/225000 (84%)] Loss: 20128.947266\n",
      "Train Epoch: 27 [192384/225000 (86%)] Loss: 20439.191406\n",
      "Train Epoch: 27 [194880/225000 (87%)] Loss: 20652.769531\n",
      "Train Epoch: 27 [197376/225000 (88%)] Loss: 20331.679688\n",
      "Train Epoch: 27 [199872/225000 (89%)] Loss: 20319.843750\n",
      "Train Epoch: 27 [202368/225000 (90%)] Loss: 20366.916016\n",
      "Train Epoch: 27 [204864/225000 (91%)] Loss: 20473.550781\n",
      "Train Epoch: 27 [207360/225000 (92%)] Loss: 20386.912109\n",
      "Train Epoch: 27 [209856/225000 (93%)] Loss: 20209.349609\n",
      "Train Epoch: 27 [212352/225000 (94%)] Loss: 20392.533203\n",
      "Train Epoch: 27 [214848/225000 (95%)] Loss: 20180.835938\n",
      "Train Epoch: 27 [217344/225000 (97%)] Loss: 20283.109375\n",
      "Train Epoch: 27 [219840/225000 (98%)] Loss: 20387.929688\n",
      "Train Epoch: 27 [222336/225000 (99%)] Loss: 20097.035156\n",
      "Train Epoch: 27 [224832/225000 (100%)] Loss: 20458.132812\n",
      "    epoch          : 27\n",
      "    loss           : 20370.1192006186\n",
      "    val_loss       : 20266.42765058543\n",
      "Train Epoch: 28 [192/225000 (0%)] Loss: 20268.121094\n",
      "Train Epoch: 28 [2688/225000 (1%)] Loss: 20404.662109\n",
      "Train Epoch: 28 [5184/225000 (2%)] Loss: 19799.503906\n",
      "Train Epoch: 28 [7680/225000 (3%)] Loss: 20567.042969\n",
      "Train Epoch: 28 [10176/225000 (5%)] Loss: 20798.792969\n",
      "Train Epoch: 28 [12672/225000 (6%)] Loss: 20630.873047\n",
      "Train Epoch: 28 [15168/225000 (7%)] Loss: 20328.484375\n",
      "Train Epoch: 28 [17664/225000 (8%)] Loss: 20187.925781\n",
      "Train Epoch: 28 [20160/225000 (9%)] Loss: 20297.677734\n",
      "Train Epoch: 28 [22656/225000 (10%)] Loss: 20062.353516\n",
      "Train Epoch: 28 [25152/225000 (11%)] Loss: 20744.007812\n",
      "Train Epoch: 28 [27648/225000 (12%)] Loss: 20573.339844\n",
      "Train Epoch: 28 [30144/225000 (13%)] Loss: 20346.015625\n",
      "Train Epoch: 28 [32640/225000 (15%)] Loss: 20147.578125\n",
      "Train Epoch: 28 [35136/225000 (16%)] Loss: 20279.144531\n",
      "Train Epoch: 28 [37632/225000 (17%)] Loss: 20246.625000\n",
      "Train Epoch: 28 [40128/225000 (18%)] Loss: 20139.089844\n",
      "Train Epoch: 28 [42624/225000 (19%)] Loss: 20519.923828\n",
      "Train Epoch: 28 [45120/225000 (20%)] Loss: 20325.015625\n",
      "Train Epoch: 28 [47616/225000 (21%)] Loss: 21053.396484\n",
      "Train Epoch: 28 [50112/225000 (22%)] Loss: 21007.140625\n",
      "Train Epoch: 28 [52608/225000 (23%)] Loss: 20193.011719\n",
      "Train Epoch: 28 [55104/225000 (24%)] Loss: 20414.529297\n",
      "Train Epoch: 28 [57600/225000 (26%)] Loss: 20383.667969\n",
      "Train Epoch: 28 [60096/225000 (27%)] Loss: 20204.011719\n",
      "Train Epoch: 28 [62592/225000 (28%)] Loss: 20411.771484\n",
      "Train Epoch: 28 [65088/225000 (29%)] Loss: 20737.875000\n",
      "Train Epoch: 28 [67584/225000 (30%)] Loss: 20437.515625\n",
      "Train Epoch: 28 [70080/225000 (31%)] Loss: 20441.617188\n",
      "Train Epoch: 28 [72576/225000 (32%)] Loss: 20874.949219\n",
      "Train Epoch: 28 [75072/225000 (33%)] Loss: 20167.222656\n",
      "Train Epoch: 28 [77568/225000 (34%)] Loss: 20301.857422\n",
      "Train Epoch: 28 [80064/225000 (36%)] Loss: 20266.730469\n",
      "Train Epoch: 28 [82560/225000 (37%)] Loss: 20380.460938\n",
      "Train Epoch: 28 [85056/225000 (38%)] Loss: 20179.988281\n",
      "Train Epoch: 28 [87552/225000 (39%)] Loss: 20236.419922\n",
      "Train Epoch: 28 [90048/225000 (40%)] Loss: 20243.812500\n",
      "Train Epoch: 28 [92544/225000 (41%)] Loss: 20149.695312\n",
      "Train Epoch: 28 [95040/225000 (42%)] Loss: 20845.427734\n",
      "Train Epoch: 28 [97536/225000 (43%)] Loss: 20170.402344\n",
      "Train Epoch: 28 [100032/225000 (44%)] Loss: 20033.642578\n",
      "Train Epoch: 28 [102528/225000 (46%)] Loss: 19921.699219\n",
      "Train Epoch: 28 [105024/225000 (47%)] Loss: 20345.308594\n",
      "Train Epoch: 28 [107520/225000 (48%)] Loss: 20371.093750\n",
      "Train Epoch: 28 [110016/225000 (49%)] Loss: 20157.212891\n",
      "Train Epoch: 28 [112512/225000 (50%)] Loss: 20254.451172\n",
      "Train Epoch: 28 [115008/225000 (51%)] Loss: 20356.083984\n",
      "Train Epoch: 28 [117504/225000 (52%)] Loss: 20726.863281\n",
      "Train Epoch: 28 [120000/225000 (53%)] Loss: 20395.648438\n",
      "Train Epoch: 28 [122496/225000 (54%)] Loss: 20565.988281\n",
      "Train Epoch: 28 [124992/225000 (56%)] Loss: 20242.402344\n",
      "Train Epoch: 28 [127488/225000 (57%)] Loss: 20286.359375\n",
      "Train Epoch: 28 [129984/225000 (58%)] Loss: 21044.277344\n",
      "Train Epoch: 28 [132480/225000 (59%)] Loss: 20160.482422\n",
      "Train Epoch: 28 [134976/225000 (60%)] Loss: 19681.628906\n",
      "Train Epoch: 28 [137472/225000 (61%)] Loss: 20539.138672\n",
      "Train Epoch: 28 [139968/225000 (62%)] Loss: 19944.210938\n",
      "Train Epoch: 28 [142464/225000 (63%)] Loss: 20208.505859\n",
      "Train Epoch: 28 [144960/225000 (64%)] Loss: 19906.804688\n",
      "Train Epoch: 28 [147456/225000 (66%)] Loss: 20457.623047\n",
      "Train Epoch: 28 [149952/225000 (67%)] Loss: 20333.843750\n",
      "Train Epoch: 28 [152448/225000 (68%)] Loss: 20125.638672\n",
      "Train Epoch: 28 [154944/225000 (69%)] Loss: 20053.953125\n",
      "Train Epoch: 28 [157440/225000 (70%)] Loss: 20133.453125\n",
      "Train Epoch: 28 [159936/225000 (71%)] Loss: 21356.449219\n",
      "Train Epoch: 28 [162432/225000 (72%)] Loss: 20258.759766\n",
      "Train Epoch: 28 [164928/225000 (73%)] Loss: 20437.083984\n",
      "Train Epoch: 28 [167424/225000 (74%)] Loss: 20050.449219\n",
      "Train Epoch: 28 [169920/225000 (76%)] Loss: 20057.037109\n",
      "Train Epoch: 28 [172416/225000 (77%)] Loss: 20054.582031\n",
      "Train Epoch: 28 [174912/225000 (78%)] Loss: 20780.578125\n",
      "Train Epoch: 28 [177408/225000 (79%)] Loss: 20167.910156\n",
      "Train Epoch: 28 [179904/225000 (80%)] Loss: 20773.537109\n",
      "Train Epoch: 28 [182400/225000 (81%)] Loss: 19944.742188\n",
      "Train Epoch: 28 [184896/225000 (82%)] Loss: 19830.253906\n",
      "Train Epoch: 28 [187392/225000 (83%)] Loss: 20749.953125\n",
      "Train Epoch: 28 [189888/225000 (84%)] Loss: 20224.894531\n",
      "Train Epoch: 28 [192384/225000 (86%)] Loss: 20434.589844\n",
      "Train Epoch: 28 [194880/225000 (87%)] Loss: 20700.156250\n",
      "Train Epoch: 28 [197376/225000 (88%)] Loss: 20374.779297\n",
      "Train Epoch: 28 [199872/225000 (89%)] Loss: 20271.988281\n",
      "Train Epoch: 28 [202368/225000 (90%)] Loss: 20542.523438\n",
      "Train Epoch: 28 [204864/225000 (91%)] Loss: 20490.046875\n",
      "Train Epoch: 28 [207360/225000 (92%)] Loss: 20069.671875\n",
      "Train Epoch: 28 [209856/225000 (93%)] Loss: 20533.896484\n",
      "Train Epoch: 28 [212352/225000 (94%)] Loss: 20089.843750\n",
      "Train Epoch: 28 [214848/225000 (95%)] Loss: 20275.070312\n",
      "Train Epoch: 28 [217344/225000 (97%)] Loss: 20335.265625\n",
      "Train Epoch: 28 [219840/225000 (98%)] Loss: 20555.779297\n",
      "Train Epoch: 28 [222336/225000 (99%)] Loss: 20299.095703\n",
      "Train Epoch: 28 [224832/225000 (100%)] Loss: 19993.732422\n",
      "    epoch          : 28\n",
      "    loss           : 20368.63845523144\n",
      "    val_loss       : 20346.189747826742\n",
      "Train Epoch: 29 [192/225000 (0%)] Loss: 20267.988281\n",
      "Train Epoch: 29 [2688/225000 (1%)] Loss: 20366.488281\n",
      "Train Epoch: 29 [5184/225000 (2%)] Loss: 20404.052734\n",
      "Train Epoch: 29 [7680/225000 (3%)] Loss: 20110.527344\n",
      "Train Epoch: 29 [10176/225000 (5%)] Loss: 20241.623047\n",
      "Train Epoch: 29 [12672/225000 (6%)] Loss: 20815.962891\n",
      "Train Epoch: 29 [15168/225000 (7%)] Loss: 20438.992188\n",
      "Train Epoch: 29 [17664/225000 (8%)] Loss: 19882.601562\n",
      "Train Epoch: 29 [20160/225000 (9%)] Loss: 20355.859375\n",
      "Train Epoch: 29 [22656/225000 (10%)] Loss: 20246.712891\n",
      "Train Epoch: 29 [25152/225000 (11%)] Loss: 20974.492188\n",
      "Train Epoch: 29 [27648/225000 (12%)] Loss: 20220.554688\n",
      "Train Epoch: 29 [30144/225000 (13%)] Loss: 20204.019531\n",
      "Train Epoch: 29 [32640/225000 (15%)] Loss: 20313.871094\n",
      "Train Epoch: 29 [35136/225000 (16%)] Loss: 20412.191406\n",
      "Train Epoch: 29 [37632/225000 (17%)] Loss: 20294.527344\n",
      "Train Epoch: 29 [40128/225000 (18%)] Loss: 20873.519531\n",
      "Train Epoch: 29 [42624/225000 (19%)] Loss: 20167.804688\n",
      "Train Epoch: 29 [45120/225000 (20%)] Loss: 20467.169922\n",
      "Train Epoch: 29 [47616/225000 (21%)] Loss: 19780.912109\n",
      "Train Epoch: 29 [50112/225000 (22%)] Loss: 20138.753906\n",
      "Train Epoch: 29 [52608/225000 (23%)] Loss: 20425.003906\n",
      "Train Epoch: 29 [55104/225000 (24%)] Loss: 20472.093750\n",
      "Train Epoch: 29 [57600/225000 (26%)] Loss: 19607.326172\n",
      "Train Epoch: 29 [60096/225000 (27%)] Loss: 19765.421875\n",
      "Train Epoch: 29 [62592/225000 (28%)] Loss: 19944.929688\n",
      "Train Epoch: 29 [65088/225000 (29%)] Loss: 19963.169922\n",
      "Train Epoch: 29 [67584/225000 (30%)] Loss: 20496.261719\n",
      "Train Epoch: 29 [70080/225000 (31%)] Loss: 20425.878906\n",
      "Train Epoch: 29 [72576/225000 (32%)] Loss: 20226.513672\n",
      "Train Epoch: 29 [75072/225000 (33%)] Loss: 20320.144531\n",
      "Train Epoch: 29 [77568/225000 (34%)] Loss: 20003.164062\n",
      "Train Epoch: 29 [80064/225000 (36%)] Loss: 20639.396484\n",
      "Train Epoch: 29 [82560/225000 (37%)] Loss: 20283.156250\n",
      "Train Epoch: 29 [85056/225000 (38%)] Loss: 20187.710938\n",
      "Train Epoch: 29 [87552/225000 (39%)] Loss: 20007.677734\n",
      "Train Epoch: 29 [90048/225000 (40%)] Loss: 20667.453125\n",
      "Train Epoch: 29 [92544/225000 (41%)] Loss: 20053.626953\n",
      "Train Epoch: 29 [95040/225000 (42%)] Loss: 20454.539062\n",
      "Train Epoch: 29 [97536/225000 (43%)] Loss: 19888.074219\n",
      "Train Epoch: 29 [100032/225000 (44%)] Loss: 20519.712891\n",
      "Train Epoch: 29 [102528/225000 (46%)] Loss: 20543.679688\n",
      "Train Epoch: 29 [105024/225000 (47%)] Loss: 20392.503906\n",
      "Train Epoch: 29 [107520/225000 (48%)] Loss: 20683.082031\n",
      "Train Epoch: 29 [110016/225000 (49%)] Loss: 20224.712891\n",
      "Train Epoch: 29 [112512/225000 (50%)] Loss: 20790.712891\n",
      "Train Epoch: 29 [115008/225000 (51%)] Loss: 20680.013672\n",
      "Train Epoch: 29 [117504/225000 (52%)] Loss: 20298.394531\n",
      "Train Epoch: 29 [120000/225000 (53%)] Loss: 19741.925781\n",
      "Train Epoch: 29 [122496/225000 (54%)] Loss: 20537.722656\n",
      "Train Epoch: 29 [124992/225000 (56%)] Loss: 20477.621094\n",
      "Train Epoch: 29 [127488/225000 (57%)] Loss: 19872.253906\n",
      "Train Epoch: 29 [129984/225000 (58%)] Loss: 20115.570312\n",
      "Train Epoch: 29 [132480/225000 (59%)] Loss: 20503.203125\n",
      "Train Epoch: 29 [134976/225000 (60%)] Loss: 20496.226562\n",
      "Train Epoch: 29 [137472/225000 (61%)] Loss: 20639.322266\n",
      "Train Epoch: 29 [139968/225000 (62%)] Loss: 20572.109375\n",
      "Train Epoch: 29 [142464/225000 (63%)] Loss: 20571.722656\n",
      "Train Epoch: 29 [144960/225000 (64%)] Loss: 19958.367188\n",
      "Train Epoch: 29 [147456/225000 (66%)] Loss: 20677.140625\n",
      "Train Epoch: 29 [149952/225000 (67%)] Loss: 20075.121094\n",
      "Train Epoch: 29 [152448/225000 (68%)] Loss: 20374.294922\n",
      "Train Epoch: 29 [154944/225000 (69%)] Loss: 20008.121094\n",
      "Train Epoch: 29 [157440/225000 (70%)] Loss: 20251.396484\n",
      "Train Epoch: 29 [159936/225000 (71%)] Loss: 20488.886719\n",
      "Train Epoch: 29 [162432/225000 (72%)] Loss: 20877.667969\n",
      "Train Epoch: 29 [164928/225000 (73%)] Loss: 20767.621094\n",
      "Train Epoch: 29 [167424/225000 (74%)] Loss: 20401.613281\n",
      "Train Epoch: 29 [169920/225000 (76%)] Loss: 20524.453125\n",
      "Train Epoch: 29 [172416/225000 (77%)] Loss: 20401.384766\n",
      "Train Epoch: 29 [174912/225000 (78%)] Loss: 20365.732422\n",
      "Train Epoch: 29 [177408/225000 (79%)] Loss: 20349.304688\n",
      "Train Epoch: 29 [179904/225000 (80%)] Loss: 20648.488281\n",
      "Train Epoch: 29 [182400/225000 (81%)] Loss: 20575.492188\n",
      "Train Epoch: 29 [184896/225000 (82%)] Loss: 20543.566406\n",
      "Train Epoch: 29 [187392/225000 (83%)] Loss: 20401.994141\n",
      "Train Epoch: 29 [189888/225000 (84%)] Loss: 20202.382812\n",
      "Train Epoch: 29 [192384/225000 (86%)] Loss: 20732.777344\n",
      "Train Epoch: 29 [194880/225000 (87%)] Loss: 20244.142578\n",
      "Train Epoch: 29 [197376/225000 (88%)] Loss: 20265.242188\n",
      "Train Epoch: 29 [199872/225000 (89%)] Loss: 20375.392578\n",
      "Train Epoch: 29 [202368/225000 (90%)] Loss: 20266.406250\n",
      "Train Epoch: 29 [204864/225000 (91%)] Loss: 20471.441406\n",
      "Train Epoch: 29 [207360/225000 (92%)] Loss: 20150.728516\n",
      "Train Epoch: 29 [209856/225000 (93%)] Loss: 20069.750000\n",
      "Train Epoch: 29 [212352/225000 (94%)] Loss: 20364.457031\n",
      "Train Epoch: 29 [214848/225000 (95%)] Loss: 20033.882812\n",
      "Train Epoch: 29 [217344/225000 (97%)] Loss: 20227.273438\n",
      "Train Epoch: 29 [219840/225000 (98%)] Loss: 20180.908203\n",
      "Train Epoch: 29 [222336/225000 (99%)] Loss: 20034.687500\n",
      "Train Epoch: 29 [224832/225000 (100%)] Loss: 19847.746094\n",
      "    epoch          : 29\n",
      "    loss           : 20355.024442392812\n",
      "    val_loss       : 20245.045007535973\n",
      "Train Epoch: 30 [192/225000 (0%)] Loss: 19981.513672\n",
      "Train Epoch: 30 [2688/225000 (1%)] Loss: 20643.281250\n",
      "Train Epoch: 30 [5184/225000 (2%)] Loss: 20646.544922\n",
      "Train Epoch: 30 [7680/225000 (3%)] Loss: 20070.433594\n",
      "Train Epoch: 30 [10176/225000 (5%)] Loss: 19918.285156\n",
      "Train Epoch: 30 [12672/225000 (6%)] Loss: 20445.222656\n",
      "Train Epoch: 30 [15168/225000 (7%)] Loss: 20298.791016\n",
      "Train Epoch: 30 [17664/225000 (8%)] Loss: 20045.265625\n",
      "Train Epoch: 30 [20160/225000 (9%)] Loss: 20308.375000\n",
      "Train Epoch: 30 [22656/225000 (10%)] Loss: 20287.658203\n",
      "Train Epoch: 30 [25152/225000 (11%)] Loss: 20203.314453\n",
      "Train Epoch: 30 [27648/225000 (12%)] Loss: 20395.308594\n",
      "Train Epoch: 30 [30144/225000 (13%)] Loss: 20283.578125\n",
      "Train Epoch: 30 [32640/225000 (15%)] Loss: 20063.343750\n",
      "Train Epoch: 30 [35136/225000 (16%)] Loss: 20096.939453\n",
      "Train Epoch: 30 [37632/225000 (17%)] Loss: 20134.572266\n",
      "Train Epoch: 30 [40128/225000 (18%)] Loss: 20508.183594\n",
      "Train Epoch: 30 [42624/225000 (19%)] Loss: 20514.992188\n",
      "Train Epoch: 30 [45120/225000 (20%)] Loss: 21280.826172\n",
      "Train Epoch: 30 [47616/225000 (21%)] Loss: 20770.902344\n",
      "Train Epoch: 30 [50112/225000 (22%)] Loss: 20032.087891\n",
      "Train Epoch: 30 [52608/225000 (23%)] Loss: 20117.585938\n",
      "Train Epoch: 30 [55104/225000 (24%)] Loss: 20249.332031\n",
      "Train Epoch: 30 [57600/225000 (26%)] Loss: 20428.142578\n",
      "Train Epoch: 30 [60096/225000 (27%)] Loss: 19875.097656\n",
      "Train Epoch: 30 [62592/225000 (28%)] Loss: 21014.308594\n",
      "Train Epoch: 30 [65088/225000 (29%)] Loss: 20656.808594\n",
      "Train Epoch: 30 [67584/225000 (30%)] Loss: 20612.414062\n",
      "Train Epoch: 30 [70080/225000 (31%)] Loss: 20085.785156\n",
      "Train Epoch: 30 [72576/225000 (32%)] Loss: 20035.953125\n",
      "Train Epoch: 30 [75072/225000 (33%)] Loss: 20604.244141\n",
      "Train Epoch: 30 [77568/225000 (34%)] Loss: 20604.076172\n",
      "Train Epoch: 30 [80064/225000 (36%)] Loss: 20106.023438\n",
      "Train Epoch: 30 [82560/225000 (37%)] Loss: 20521.691406\n",
      "Train Epoch: 30 [85056/225000 (38%)] Loss: 20527.787109\n",
      "Train Epoch: 30 [87552/225000 (39%)] Loss: 19991.851562\n",
      "Train Epoch: 30 [90048/225000 (40%)] Loss: 20359.207031\n",
      "Train Epoch: 30 [92544/225000 (41%)] Loss: 20085.375000\n",
      "Train Epoch: 30 [95040/225000 (42%)] Loss: 20125.074219\n",
      "Train Epoch: 30 [97536/225000 (43%)] Loss: 20371.980469\n",
      "Train Epoch: 30 [100032/225000 (44%)] Loss: 20420.667969\n",
      "Train Epoch: 30 [102528/225000 (46%)] Loss: 20565.027344\n",
      "Train Epoch: 30 [105024/225000 (47%)] Loss: 20267.640625\n",
      "Train Epoch: 30 [107520/225000 (48%)] Loss: 20448.539062\n",
      "Train Epoch: 30 [110016/225000 (49%)] Loss: 20839.363281\n",
      "Train Epoch: 30 [112512/225000 (50%)] Loss: 19881.705078\n",
      "Train Epoch: 30 [115008/225000 (51%)] Loss: 20336.398438\n",
      "Train Epoch: 30 [117504/225000 (52%)] Loss: 20551.957031\n",
      "Train Epoch: 30 [120000/225000 (53%)] Loss: 20118.656250\n",
      "Train Epoch: 30 [122496/225000 (54%)] Loss: 20672.800781\n",
      "Train Epoch: 30 [124992/225000 (56%)] Loss: 20378.402344\n",
      "Train Epoch: 30 [127488/225000 (57%)] Loss: 20116.203125\n",
      "Train Epoch: 30 [129984/225000 (58%)] Loss: 20119.035156\n",
      "Train Epoch: 30 [132480/225000 (59%)] Loss: 20095.886719\n",
      "Train Epoch: 30 [134976/225000 (60%)] Loss: 20633.496094\n",
      "Train Epoch: 30 [137472/225000 (61%)] Loss: 20684.242188\n",
      "Train Epoch: 30 [139968/225000 (62%)] Loss: 20075.320312\n",
      "Train Epoch: 30 [142464/225000 (63%)] Loss: 20369.179688\n",
      "Train Epoch: 30 [144960/225000 (64%)] Loss: 19879.082031\n",
      "Train Epoch: 30 [147456/225000 (66%)] Loss: 20271.312500\n",
      "Train Epoch: 30 [149952/225000 (67%)] Loss: 20549.144531\n",
      "Train Epoch: 30 [152448/225000 (68%)] Loss: 20266.683594\n",
      "Train Epoch: 30 [154944/225000 (69%)] Loss: 20314.050781\n",
      "Train Epoch: 30 [157440/225000 (70%)] Loss: 20066.636719\n",
      "Train Epoch: 30 [159936/225000 (71%)] Loss: 20042.285156\n",
      "Train Epoch: 30 [162432/225000 (72%)] Loss: 20291.144531\n",
      "Train Epoch: 30 [164928/225000 (73%)] Loss: 20398.390625\n",
      "Train Epoch: 30 [167424/225000 (74%)] Loss: 20605.503906\n",
      "Train Epoch: 30 [169920/225000 (76%)] Loss: 20576.011719\n",
      "Train Epoch: 30 [172416/225000 (77%)] Loss: 20270.066406\n",
      "Train Epoch: 30 [174912/225000 (78%)] Loss: 20243.414062\n",
      "Train Epoch: 30 [177408/225000 (79%)] Loss: 20940.361328\n",
      "Train Epoch: 30 [179904/225000 (80%)] Loss: 20368.949219\n",
      "Train Epoch: 30 [182400/225000 (81%)] Loss: 19901.261719\n",
      "Train Epoch: 30 [184896/225000 (82%)] Loss: 20358.570312\n",
      "Train Epoch: 30 [187392/225000 (83%)] Loss: 20325.980469\n",
      "Train Epoch: 30 [189888/225000 (84%)] Loss: 20611.556641\n",
      "Train Epoch: 30 [192384/225000 (86%)] Loss: 20296.505859\n",
      "Train Epoch: 30 [194880/225000 (87%)] Loss: 20460.406250\n",
      "Train Epoch: 30 [197376/225000 (88%)] Loss: 20794.416016\n",
      "Train Epoch: 30 [199872/225000 (89%)] Loss: 20368.710938\n",
      "Train Epoch: 30 [202368/225000 (90%)] Loss: 20711.433594\n",
      "Train Epoch: 30 [204864/225000 (91%)] Loss: 20307.574219\n",
      "Train Epoch: 30 [207360/225000 (92%)] Loss: 19733.029297\n",
      "Train Epoch: 30 [209856/225000 (93%)] Loss: 20104.894531\n",
      "Train Epoch: 30 [212352/225000 (94%)] Loss: 20600.531250\n",
      "Train Epoch: 30 [214848/225000 (95%)] Loss: 20401.533203\n",
      "Train Epoch: 30 [217344/225000 (97%)] Loss: 20408.785156\n",
      "Train Epoch: 30 [219840/225000 (98%)] Loss: 20270.005859\n",
      "Train Epoch: 30 [222336/225000 (99%)] Loss: 20713.617188\n",
      "Train Epoch: 30 [224832/225000 (100%)] Loss: 20146.183594\n",
      "    epoch          : 30\n",
      "    loss           : 20335.489814419794\n",
      "    val_loss       : 20241.360062311625\n",
      "Train Epoch: 31 [192/225000 (0%)] Loss: 20060.904297\n",
      "Train Epoch: 31 [2688/225000 (1%)] Loss: 20706.054688\n",
      "Train Epoch: 31 [5184/225000 (2%)] Loss: 20749.453125\n",
      "Train Epoch: 31 [7680/225000 (3%)] Loss: 20113.044922\n",
      "Train Epoch: 31 [10176/225000 (5%)] Loss: 20232.980469\n",
      "Train Epoch: 31 [12672/225000 (6%)] Loss: 20360.335938\n",
      "Train Epoch: 31 [15168/225000 (7%)] Loss: 20076.544922\n",
      "Train Epoch: 31 [17664/225000 (8%)] Loss: 20152.894531\n",
      "Train Epoch: 31 [20160/225000 (9%)] Loss: 20719.640625\n",
      "Train Epoch: 31 [22656/225000 (10%)] Loss: 20256.787109\n",
      "Train Epoch: 31 [25152/225000 (11%)] Loss: 20074.312500\n",
      "Train Epoch: 31 [27648/225000 (12%)] Loss: 20422.722656\n",
      "Train Epoch: 31 [30144/225000 (13%)] Loss: 20247.734375\n",
      "Train Epoch: 31 [32640/225000 (15%)] Loss: 20274.628906\n",
      "Train Epoch: 31 [35136/225000 (16%)] Loss: 20371.875000\n",
      "Train Epoch: 31 [37632/225000 (17%)] Loss: 20465.380859\n",
      "Train Epoch: 31 [40128/225000 (18%)] Loss: 20361.132812\n",
      "Train Epoch: 31 [42624/225000 (19%)] Loss: 20482.480469\n",
      "Train Epoch: 31 [45120/225000 (20%)] Loss: 20292.898438\n",
      "Train Epoch: 31 [47616/225000 (21%)] Loss: 20436.410156\n",
      "Train Epoch: 31 [50112/225000 (22%)] Loss: 20693.535156\n",
      "Train Epoch: 31 [52608/225000 (23%)] Loss: 20426.304688\n",
      "Train Epoch: 31 [55104/225000 (24%)] Loss: 20086.578125\n",
      "Train Epoch: 31 [57600/225000 (26%)] Loss: 20149.646484\n",
      "Train Epoch: 31 [60096/225000 (27%)] Loss: 20200.949219\n",
      "Train Epoch: 31 [62592/225000 (28%)] Loss: 19400.457031\n",
      "Train Epoch: 31 [65088/225000 (29%)] Loss: 20454.343750\n",
      "Train Epoch: 31 [67584/225000 (30%)] Loss: 20383.558594\n",
      "Train Epoch: 31 [70080/225000 (31%)] Loss: 20087.746094\n",
      "Train Epoch: 31 [72576/225000 (32%)] Loss: 20011.164062\n",
      "Train Epoch: 31 [75072/225000 (33%)] Loss: 20536.460938\n",
      "Train Epoch: 31 [77568/225000 (34%)] Loss: 20284.169922\n",
      "Train Epoch: 31 [80064/225000 (36%)] Loss: 20329.472656\n",
      "Train Epoch: 31 [82560/225000 (37%)] Loss: 20608.341797\n",
      "Train Epoch: 31 [85056/225000 (38%)] Loss: 20309.109375\n",
      "Train Epoch: 31 [87552/225000 (39%)] Loss: 20483.714844\n",
      "Train Epoch: 31 [90048/225000 (40%)] Loss: 20490.835938\n",
      "Train Epoch: 31 [92544/225000 (41%)] Loss: 20097.730469\n",
      "Train Epoch: 31 [95040/225000 (42%)] Loss: 20377.140625\n",
      "Train Epoch: 31 [97536/225000 (43%)] Loss: 20464.312500\n",
      "Train Epoch: 31 [100032/225000 (44%)] Loss: 20683.066406\n",
      "Train Epoch: 31 [102528/225000 (46%)] Loss: 19699.261719\n",
      "Train Epoch: 31 [105024/225000 (47%)] Loss: 19990.144531\n",
      "Train Epoch: 31 [107520/225000 (48%)] Loss: 20237.201172\n",
      "Train Epoch: 31 [110016/225000 (49%)] Loss: 20676.541016\n",
      "Train Epoch: 31 [112512/225000 (50%)] Loss: 20406.023438\n",
      "Train Epoch: 31 [115008/225000 (51%)] Loss: 20107.476562\n",
      "Train Epoch: 31 [117504/225000 (52%)] Loss: 19967.757812\n",
      "Train Epoch: 31 [120000/225000 (53%)] Loss: 20442.347656\n",
      "Train Epoch: 31 [122496/225000 (54%)] Loss: 20360.585938\n",
      "Train Epoch: 31 [124992/225000 (56%)] Loss: 20359.609375\n",
      "Train Epoch: 31 [127488/225000 (57%)] Loss: 20043.828125\n",
      "Train Epoch: 31 [129984/225000 (58%)] Loss: 20394.015625\n",
      "Train Epoch: 31 [132480/225000 (59%)] Loss: 20781.109375\n",
      "Train Epoch: 31 [134976/225000 (60%)] Loss: 20274.136719\n",
      "Train Epoch: 31 [137472/225000 (61%)] Loss: 20141.011719\n",
      "Train Epoch: 31 [139968/225000 (62%)] Loss: 20207.132812\n",
      "Train Epoch: 31 [142464/225000 (63%)] Loss: 20172.035156\n",
      "Train Epoch: 31 [144960/225000 (64%)] Loss: 20388.101562\n",
      "Train Epoch: 31 [147456/225000 (66%)] Loss: 20360.666016\n",
      "Train Epoch: 31 [149952/225000 (67%)] Loss: 19882.902344\n",
      "Train Epoch: 31 [152448/225000 (68%)] Loss: 20148.060547\n",
      "Train Epoch: 31 [154944/225000 (69%)] Loss: 19739.328125\n",
      "Train Epoch: 31 [157440/225000 (70%)] Loss: 20249.326172\n",
      "Train Epoch: 31 [159936/225000 (71%)] Loss: 20195.386719\n",
      "Train Epoch: 31 [162432/225000 (72%)] Loss: 20845.578125\n",
      "Train Epoch: 31 [164928/225000 (73%)] Loss: 20282.806641\n",
      "Train Epoch: 31 [167424/225000 (74%)] Loss: 20877.914062\n",
      "Train Epoch: 31 [169920/225000 (76%)] Loss: 20308.132812\n",
      "Train Epoch: 31 [172416/225000 (77%)] Loss: 20606.515625\n",
      "Train Epoch: 31 [174912/225000 (78%)] Loss: 20383.490234\n",
      "Train Epoch: 31 [177408/225000 (79%)] Loss: 20423.625000\n",
      "Train Epoch: 31 [179904/225000 (80%)] Loss: 20018.636719\n",
      "Train Epoch: 31 [182400/225000 (81%)] Loss: 20115.210938\n",
      "Train Epoch: 31 [184896/225000 (82%)] Loss: 20494.964844\n",
      "Train Epoch: 31 [187392/225000 (83%)] Loss: 19747.468750\n",
      "Train Epoch: 31 [189888/225000 (84%)] Loss: 20237.078125\n",
      "Train Epoch: 31 [192384/225000 (86%)] Loss: 20675.898438\n",
      "Train Epoch: 31 [194880/225000 (87%)] Loss: 20918.800781\n",
      "Train Epoch: 31 [197376/225000 (88%)] Loss: 20181.654297\n",
      "Train Epoch: 31 [199872/225000 (89%)] Loss: 20096.332031\n",
      "Train Epoch: 31 [202368/225000 (90%)] Loss: 20169.800781\n",
      "Train Epoch: 31 [204864/225000 (91%)] Loss: 20888.796875\n",
      "Train Epoch: 31 [207360/225000 (92%)] Loss: 20685.097656\n",
      "Train Epoch: 31 [209856/225000 (93%)] Loss: 20217.824219\n",
      "Train Epoch: 31 [212352/225000 (94%)] Loss: 20300.238281\n",
      "Train Epoch: 31 [214848/225000 (95%)] Loss: 20404.570312\n",
      "Train Epoch: 31 [217344/225000 (97%)] Loss: 20193.882812\n",
      "Train Epoch: 31 [219840/225000 (98%)] Loss: 20048.058594\n",
      "Train Epoch: 31 [222336/225000 (99%)] Loss: 20392.285156\n",
      "Train Epoch: 31 [224832/225000 (100%)] Loss: 20008.544922\n",
      "    epoch          : 31\n",
      "    loss           : 20328.762631985923\n",
      "    val_loss       : 20229.432509150214\n",
      "Train Epoch: 32 [192/225000 (0%)] Loss: 20396.906250\n",
      "Train Epoch: 32 [2688/225000 (1%)] Loss: 20309.371094\n",
      "Train Epoch: 32 [5184/225000 (2%)] Loss: 20057.587891\n",
      "Train Epoch: 32 [7680/225000 (3%)] Loss: 20112.746094\n",
      "Train Epoch: 32 [10176/225000 (5%)] Loss: 20474.230469\n",
      "Train Epoch: 32 [12672/225000 (6%)] Loss: 20949.669922\n",
      "Train Epoch: 32 [15168/225000 (7%)] Loss: 20326.226562\n",
      "Train Epoch: 32 [17664/225000 (8%)] Loss: 19981.824219\n",
      "Train Epoch: 32 [20160/225000 (9%)] Loss: 19809.718750\n",
      "Train Epoch: 32 [22656/225000 (10%)] Loss: 20442.894531\n",
      "Train Epoch: 32 [25152/225000 (11%)] Loss: 20390.113281\n",
      "Train Epoch: 32 [27648/225000 (12%)] Loss: 20111.148438\n",
      "Train Epoch: 32 [30144/225000 (13%)] Loss: 20139.490234\n",
      "Train Epoch: 32 [32640/225000 (15%)] Loss: 20282.566406\n",
      "Train Epoch: 32 [35136/225000 (16%)] Loss: 20538.019531\n",
      "Train Epoch: 32 [37632/225000 (17%)] Loss: 19692.421875\n",
      "Train Epoch: 32 [40128/225000 (18%)] Loss: 20727.117188\n",
      "Train Epoch: 32 [42624/225000 (19%)] Loss: 20198.960938\n",
      "Train Epoch: 32 [45120/225000 (20%)] Loss: 19877.814453\n",
      "Train Epoch: 32 [47616/225000 (21%)] Loss: 20186.644531\n",
      "Train Epoch: 32 [50112/225000 (22%)] Loss: 20489.839844\n",
      "Train Epoch: 32 [52608/225000 (23%)] Loss: 20126.179688\n",
      "Train Epoch: 32 [55104/225000 (24%)] Loss: 20289.804688\n",
      "Train Epoch: 32 [57600/225000 (26%)] Loss: 20535.269531\n",
      "Train Epoch: 32 [60096/225000 (27%)] Loss: 20362.986328\n",
      "Train Epoch: 32 [62592/225000 (28%)] Loss: 20603.796875\n",
      "Train Epoch: 32 [65088/225000 (29%)] Loss: 20418.449219\n",
      "Train Epoch: 32 [67584/225000 (30%)] Loss: 20276.683594\n",
      "Train Epoch: 32 [70080/225000 (31%)] Loss: 20026.585938\n",
      "Train Epoch: 32 [72576/225000 (32%)] Loss: 20596.708984\n",
      "Train Epoch: 32 [75072/225000 (33%)] Loss: 20202.000000\n",
      "Train Epoch: 32 [77568/225000 (34%)] Loss: 21001.910156\n",
      "Train Epoch: 32 [80064/225000 (36%)] Loss: 20322.503906\n",
      "Train Epoch: 32 [82560/225000 (37%)] Loss: 20150.097656\n",
      "Train Epoch: 32 [85056/225000 (38%)] Loss: 20797.054688\n",
      "Train Epoch: 32 [87552/225000 (39%)] Loss: 19910.658203\n",
      "Train Epoch: 32 [90048/225000 (40%)] Loss: 20187.691406\n",
      "Train Epoch: 32 [92544/225000 (41%)] Loss: 20356.281250\n",
      "Train Epoch: 32 [95040/225000 (42%)] Loss: 20134.712891\n",
      "Train Epoch: 32 [97536/225000 (43%)] Loss: 20317.384766\n",
      "Train Epoch: 32 [100032/225000 (44%)] Loss: 20171.796875\n",
      "Train Epoch: 32 [102528/225000 (46%)] Loss: 20731.238281\n",
      "Train Epoch: 32 [105024/225000 (47%)] Loss: 20530.277344\n",
      "Train Epoch: 32 [107520/225000 (48%)] Loss: 20154.759766\n",
      "Train Epoch: 32 [110016/225000 (49%)] Loss: 20403.824219\n",
      "Train Epoch: 32 [112512/225000 (50%)] Loss: 20615.931641\n",
      "Train Epoch: 32 [115008/225000 (51%)] Loss: 20553.050781\n",
      "Train Epoch: 32 [117504/225000 (52%)] Loss: 20740.679688\n",
      "Train Epoch: 32 [120000/225000 (53%)] Loss: 20460.937500\n",
      "Train Epoch: 32 [122496/225000 (54%)] Loss: 20424.035156\n",
      "Train Epoch: 32 [124992/225000 (56%)] Loss: 20382.027344\n",
      "Train Epoch: 32 [127488/225000 (57%)] Loss: 20579.007812\n",
      "Train Epoch: 32 [129984/225000 (58%)] Loss: 19911.798828\n",
      "Train Epoch: 32 [132480/225000 (59%)] Loss: 19780.378906\n",
      "Train Epoch: 32 [134976/225000 (60%)] Loss: 20800.271484\n",
      "Train Epoch: 32 [137472/225000 (61%)] Loss: 19851.888672\n",
      "Train Epoch: 32 [139968/225000 (62%)] Loss: 20463.324219\n",
      "Train Epoch: 32 [142464/225000 (63%)] Loss: 20481.851562\n",
      "Train Epoch: 32 [144960/225000 (64%)] Loss: 20756.253906\n",
      "Train Epoch: 32 [147456/225000 (66%)] Loss: 20235.189453\n",
      "Train Epoch: 32 [149952/225000 (67%)] Loss: 20335.546875\n",
      "Train Epoch: 32 [152448/225000 (68%)] Loss: 20314.431641\n",
      "Train Epoch: 32 [154944/225000 (69%)] Loss: 20154.562500\n",
      "Train Epoch: 32 [157440/225000 (70%)] Loss: 20257.933594\n",
      "Train Epoch: 32 [159936/225000 (71%)] Loss: 19975.183594\n",
      "Train Epoch: 32 [162432/225000 (72%)] Loss: 20389.222656\n",
      "Train Epoch: 32 [164928/225000 (73%)] Loss: 19830.623047\n",
      "Train Epoch: 32 [167424/225000 (74%)] Loss: 19339.476562\n",
      "Train Epoch: 32 [169920/225000 (76%)] Loss: 20324.042969\n",
      "Train Epoch: 32 [172416/225000 (77%)] Loss: 20090.035156\n",
      "Train Epoch: 32 [174912/225000 (78%)] Loss: 20686.587891\n",
      "Train Epoch: 32 [177408/225000 (79%)] Loss: 20463.070312\n",
      "Train Epoch: 32 [179904/225000 (80%)] Loss: 20363.250000\n",
      "Train Epoch: 32 [182400/225000 (81%)] Loss: 20218.566406\n",
      "Train Epoch: 32 [184896/225000 (82%)] Loss: 20477.042969\n",
      "Train Epoch: 32 [187392/225000 (83%)] Loss: 20676.671875\n",
      "Train Epoch: 32 [189888/225000 (84%)] Loss: 20216.062500\n",
      "Train Epoch: 32 [192384/225000 (86%)] Loss: 20798.283203\n",
      "Train Epoch: 32 [194880/225000 (87%)] Loss: 20519.292969\n",
      "Train Epoch: 32 [197376/225000 (88%)] Loss: 20533.300781\n",
      "Train Epoch: 32 [199872/225000 (89%)] Loss: 20140.429688\n",
      "Train Epoch: 32 [202368/225000 (90%)] Loss: 20405.660156\n",
      "Train Epoch: 32 [204864/225000 (91%)] Loss: 20329.792969\n",
      "Train Epoch: 32 [207360/225000 (92%)] Loss: 20403.562500\n",
      "Train Epoch: 32 [209856/225000 (93%)] Loss: 21018.750000\n",
      "Train Epoch: 32 [212352/225000 (94%)] Loss: 20578.027344\n",
      "Train Epoch: 32 [214848/225000 (95%)] Loss: 20200.156250\n",
      "Train Epoch: 32 [217344/225000 (97%)] Loss: 20770.292969\n",
      "Train Epoch: 32 [219840/225000 (98%)] Loss: 20355.074219\n",
      "Train Epoch: 32 [222336/225000 (99%)] Loss: 21349.750000\n",
      "Train Epoch: 32 [224832/225000 (100%)] Loss: 20207.976562\n",
      "    epoch          : 32\n",
      "    loss           : 20326.175196312393\n",
      "    val_loss       : 20231.138168018282\n",
      "Train Epoch: 33 [192/225000 (0%)] Loss: 20456.566406\n",
      "Train Epoch: 33 [2688/225000 (1%)] Loss: 19978.738281\n",
      "Train Epoch: 33 [5184/225000 (2%)] Loss: 20315.980469\n",
      "Train Epoch: 33 [7680/225000 (3%)] Loss: 20194.546875\n",
      "Train Epoch: 33 [10176/225000 (5%)] Loss: 20555.898438\n",
      "Train Epoch: 33 [12672/225000 (6%)] Loss: 20685.574219\n",
      "Train Epoch: 33 [15168/225000 (7%)] Loss: 20356.388672\n",
      "Train Epoch: 33 [17664/225000 (8%)] Loss: 20253.753906\n",
      "Train Epoch: 33 [20160/225000 (9%)] Loss: 20111.890625\n",
      "Train Epoch: 33 [22656/225000 (10%)] Loss: 20188.949219\n",
      "Train Epoch: 33 [25152/225000 (11%)] Loss: 20042.980469\n",
      "Train Epoch: 33 [27648/225000 (12%)] Loss: 20077.792969\n",
      "Train Epoch: 33 [30144/225000 (13%)] Loss: 19843.351562\n",
      "Train Epoch: 33 [32640/225000 (15%)] Loss: 20464.429688\n",
      "Train Epoch: 33 [35136/225000 (16%)] Loss: 20338.621094\n",
      "Train Epoch: 33 [37632/225000 (17%)] Loss: 20315.417969\n",
      "Train Epoch: 33 [40128/225000 (18%)] Loss: 20491.402344\n",
      "Train Epoch: 33 [42624/225000 (19%)] Loss: 20397.386719\n",
      "Train Epoch: 33 [45120/225000 (20%)] Loss: 20784.560547\n",
      "Train Epoch: 33 [47616/225000 (21%)] Loss: 20332.277344\n",
      "Train Epoch: 33 [50112/225000 (22%)] Loss: 20361.718750\n",
      "Train Epoch: 33 [52608/225000 (23%)] Loss: 20518.876953\n",
      "Train Epoch: 33 [55104/225000 (24%)] Loss: 20172.189453\n",
      "Train Epoch: 33 [57600/225000 (26%)] Loss: 20524.564453\n",
      "Train Epoch: 33 [60096/225000 (27%)] Loss: 20220.566406\n",
      "Train Epoch: 33 [62592/225000 (28%)] Loss: 20212.566406\n",
      "Train Epoch: 33 [65088/225000 (29%)] Loss: 19860.746094\n",
      "Train Epoch: 33 [67584/225000 (30%)] Loss: 19698.554688\n",
      "Train Epoch: 33 [70080/225000 (31%)] Loss: 20537.966797\n",
      "Train Epoch: 33 [72576/225000 (32%)] Loss: 21067.281250\n",
      "Train Epoch: 33 [75072/225000 (33%)] Loss: 19937.552734\n",
      "Train Epoch: 33 [77568/225000 (34%)] Loss: 20229.742188\n",
      "Train Epoch: 33 [80064/225000 (36%)] Loss: 20281.593750\n",
      "Train Epoch: 33 [82560/225000 (37%)] Loss: 20286.160156\n",
      "Train Epoch: 33 [85056/225000 (38%)] Loss: 20339.457031\n",
      "Train Epoch: 33 [87552/225000 (39%)] Loss: 20405.236328\n",
      "Train Epoch: 33 [90048/225000 (40%)] Loss: 19959.945312\n",
      "Train Epoch: 33 [92544/225000 (41%)] Loss: 20114.111328\n",
      "Train Epoch: 33 [95040/225000 (42%)] Loss: 20218.503906\n",
      "Train Epoch: 33 [97536/225000 (43%)] Loss: 20471.121094\n",
      "Train Epoch: 33 [100032/225000 (44%)] Loss: 20052.703125\n",
      "Train Epoch: 33 [102528/225000 (46%)] Loss: 20371.757812\n",
      "Train Epoch: 33 [105024/225000 (47%)] Loss: 19925.769531\n",
      "Train Epoch: 33 [107520/225000 (48%)] Loss: 20091.457031\n",
      "Train Epoch: 33 [110016/225000 (49%)] Loss: 20528.197266\n",
      "Train Epoch: 33 [112512/225000 (50%)] Loss: 19975.630859\n",
      "Train Epoch: 33 [115008/225000 (51%)] Loss: 20104.359375\n",
      "Train Epoch: 33 [117504/225000 (52%)] Loss: 20357.363281\n",
      "Train Epoch: 33 [120000/225000 (53%)] Loss: 20364.824219\n",
      "Train Epoch: 33 [122496/225000 (54%)] Loss: 20208.628906\n",
      "Train Epoch: 33 [124992/225000 (56%)] Loss: 20354.570312\n",
      "Train Epoch: 33 [127488/225000 (57%)] Loss: 20523.353516\n",
      "Train Epoch: 33 [129984/225000 (58%)] Loss: 19976.031250\n",
      "Train Epoch: 33 [132480/225000 (59%)] Loss: 21016.171875\n",
      "Train Epoch: 33 [134976/225000 (60%)] Loss: 20302.726562\n",
      "Train Epoch: 33 [137472/225000 (61%)] Loss: 20700.742188\n",
      "Train Epoch: 33 [139968/225000 (62%)] Loss: 20330.750000\n",
      "Train Epoch: 33 [142464/225000 (63%)] Loss: 20038.474609\n",
      "Train Epoch: 33 [144960/225000 (64%)] Loss: 20421.130859\n",
      "Train Epoch: 33 [147456/225000 (66%)] Loss: 20398.646484\n",
      "Train Epoch: 33 [149952/225000 (67%)] Loss: 20465.861328\n",
      "Train Epoch: 33 [152448/225000 (68%)] Loss: 20567.070312\n",
      "Train Epoch: 33 [154944/225000 (69%)] Loss: 19795.242188\n",
      "Train Epoch: 33 [157440/225000 (70%)] Loss: 20549.539062\n",
      "Train Epoch: 33 [159936/225000 (71%)] Loss: 20176.148438\n",
      "Train Epoch: 33 [162432/225000 (72%)] Loss: 20005.625000\n",
      "Train Epoch: 33 [164928/225000 (73%)] Loss: 20509.564453\n",
      "Train Epoch: 33 [167424/225000 (74%)] Loss: 20256.837891\n",
      "Train Epoch: 33 [169920/225000 (76%)] Loss: 19879.121094\n",
      "Train Epoch: 33 [172416/225000 (77%)] Loss: 20361.396484\n",
      "Train Epoch: 33 [174912/225000 (78%)] Loss: 20382.470703\n",
      "Train Epoch: 33 [177408/225000 (79%)] Loss: 20499.527344\n",
      "Train Epoch: 33 [179904/225000 (80%)] Loss: 20110.832031\n",
      "Train Epoch: 33 [182400/225000 (81%)] Loss: 20559.742188\n",
      "Train Epoch: 33 [184896/225000 (82%)] Loss: 20223.425781\n",
      "Train Epoch: 33 [187392/225000 (83%)] Loss: 20682.052734\n",
      "Train Epoch: 33 [189888/225000 (84%)] Loss: 20696.093750\n",
      "Train Epoch: 33 [192384/225000 (86%)] Loss: 20484.285156\n",
      "Train Epoch: 33 [194880/225000 (87%)] Loss: 20764.140625\n",
      "Train Epoch: 33 [197376/225000 (88%)] Loss: 20456.796875\n",
      "Train Epoch: 33 [199872/225000 (89%)] Loss: 20574.335938\n",
      "Train Epoch: 33 [202368/225000 (90%)] Loss: 20113.644531\n",
      "Train Epoch: 33 [204864/225000 (91%)] Loss: 20151.835938\n",
      "Train Epoch: 33 [207360/225000 (92%)] Loss: 20155.425781\n",
      "Train Epoch: 33 [209856/225000 (93%)] Loss: 20249.013672\n",
      "Train Epoch: 33 [212352/225000 (94%)] Loss: 20574.113281\n",
      "Train Epoch: 33 [214848/225000 (95%)] Loss: 20485.570312\n",
      "Train Epoch: 33 [217344/225000 (97%)] Loss: 20605.558594\n",
      "Train Epoch: 33 [219840/225000 (98%)] Loss: 20134.738281\n",
      "Train Epoch: 33 [222336/225000 (99%)] Loss: 20406.419922\n",
      "Train Epoch: 33 [224832/225000 (100%)] Loss: 20612.361328\n",
      "    epoch          : 33\n",
      "    loss           : 20325.375939899743\n",
      "    val_loss       : 20215.819059460217\n",
      "Train Epoch: 34 [192/225000 (0%)] Loss: 20490.923828\n",
      "Train Epoch: 34 [2688/225000 (1%)] Loss: 20071.140625\n",
      "Train Epoch: 34 [5184/225000 (2%)] Loss: 20505.507812\n",
      "Train Epoch: 34 [7680/225000 (3%)] Loss: 20449.892578\n",
      "Train Epoch: 34 [10176/225000 (5%)] Loss: 20545.480469\n",
      "Train Epoch: 34 [12672/225000 (6%)] Loss: 20374.228516\n",
      "Train Epoch: 34 [15168/225000 (7%)] Loss: 19985.542969\n",
      "Train Epoch: 34 [17664/225000 (8%)] Loss: 20377.800781\n",
      "Train Epoch: 34 [20160/225000 (9%)] Loss: 19710.404297\n",
      "Train Epoch: 34 [22656/225000 (10%)] Loss: 19900.136719\n",
      "Train Epoch: 34 [25152/225000 (11%)] Loss: 20319.707031\n",
      "Train Epoch: 34 [27648/225000 (12%)] Loss: 20229.781250\n",
      "Train Epoch: 34 [30144/225000 (13%)] Loss: 19812.218750\n",
      "Train Epoch: 34 [32640/225000 (15%)] Loss: 20085.593750\n",
      "Train Epoch: 34 [35136/225000 (16%)] Loss: 20158.453125\n",
      "Train Epoch: 34 [37632/225000 (17%)] Loss: 20822.761719\n",
      "Train Epoch: 34 [40128/225000 (18%)] Loss: 20242.126953\n",
      "Train Epoch: 34 [42624/225000 (19%)] Loss: 20831.820312\n",
      "Train Epoch: 34 [45120/225000 (20%)] Loss: 20804.804688\n",
      "Train Epoch: 34 [47616/225000 (21%)] Loss: 20227.802734\n",
      "Train Epoch: 34 [50112/225000 (22%)] Loss: 20467.710938\n",
      "Train Epoch: 34 [52608/225000 (23%)] Loss: 19951.531250\n",
      "Train Epoch: 34 [55104/225000 (24%)] Loss: 20456.996094\n",
      "Train Epoch: 34 [57600/225000 (26%)] Loss: 19814.345703\n",
      "Train Epoch: 34 [60096/225000 (27%)] Loss: 20371.167969\n",
      "Train Epoch: 34 [62592/225000 (28%)] Loss: 19858.957031\n",
      "Train Epoch: 34 [65088/225000 (29%)] Loss: 20506.009766\n",
      "Train Epoch: 34 [67584/225000 (30%)] Loss: 20342.574219\n",
      "Train Epoch: 34 [70080/225000 (31%)] Loss: 20324.675781\n",
      "Train Epoch: 34 [72576/225000 (32%)] Loss: 20801.580078\n",
      "Train Epoch: 34 [75072/225000 (33%)] Loss: 20714.986328\n",
      "Train Epoch: 34 [77568/225000 (34%)] Loss: 20005.785156\n",
      "Train Epoch: 34 [80064/225000 (36%)] Loss: 20598.972656\n",
      "Train Epoch: 34 [82560/225000 (37%)] Loss: 20273.363281\n",
      "Train Epoch: 34 [85056/225000 (38%)] Loss: 19924.230469\n",
      "Train Epoch: 34 [87552/225000 (39%)] Loss: 20493.896484\n",
      "Train Epoch: 34 [90048/225000 (40%)] Loss: 20184.117188\n",
      "Train Epoch: 34 [92544/225000 (41%)] Loss: 20701.251953\n",
      "Train Epoch: 34 [95040/225000 (42%)] Loss: 20440.136719\n",
      "Train Epoch: 34 [97536/225000 (43%)] Loss: 20000.373047\n",
      "Train Epoch: 34 [100032/225000 (44%)] Loss: 20210.464844\n",
      "Train Epoch: 34 [102528/225000 (46%)] Loss: 20037.304688\n",
      "Train Epoch: 34 [105024/225000 (47%)] Loss: 20176.017578\n",
      "Train Epoch: 34 [107520/225000 (48%)] Loss: 20281.457031\n",
      "Train Epoch: 34 [110016/225000 (49%)] Loss: 20580.949219\n",
      "Train Epoch: 34 [112512/225000 (50%)] Loss: 19938.578125\n",
      "Train Epoch: 34 [115008/225000 (51%)] Loss: 20159.130859\n",
      "Train Epoch: 34 [117504/225000 (52%)] Loss: 19947.042969\n",
      "Train Epoch: 34 [120000/225000 (53%)] Loss: 20520.173828\n",
      "Train Epoch: 34 [122496/225000 (54%)] Loss: 20022.437500\n",
      "Train Epoch: 34 [124992/225000 (56%)] Loss: 20302.017578\n",
      "Train Epoch: 34 [127488/225000 (57%)] Loss: 20278.929688\n",
      "Train Epoch: 34 [129984/225000 (58%)] Loss: 20307.375000\n",
      "Train Epoch: 34 [132480/225000 (59%)] Loss: 20089.947266\n",
      "Train Epoch: 34 [134976/225000 (60%)] Loss: 20396.582031\n",
      "Train Epoch: 34 [137472/225000 (61%)] Loss: 20391.406250\n",
      "Train Epoch: 34 [139968/225000 (62%)] Loss: 20240.703125\n",
      "Train Epoch: 34 [142464/225000 (63%)] Loss: 20639.335938\n",
      "Train Epoch: 34 [144960/225000 (64%)] Loss: 19962.605469\n",
      "Train Epoch: 34 [147456/225000 (66%)] Loss: 20534.984375\n",
      "Train Epoch: 34 [149952/225000 (67%)] Loss: 20631.480469\n",
      "Train Epoch: 34 [152448/225000 (68%)] Loss: 20000.761719\n",
      "Train Epoch: 34 [154944/225000 (69%)] Loss: 20474.027344\n",
      "Train Epoch: 34 [157440/225000 (70%)] Loss: 20245.617188\n",
      "Train Epoch: 34 [159936/225000 (71%)] Loss: 19669.613281\n",
      "Train Epoch: 34 [162432/225000 (72%)] Loss: 20629.199219\n",
      "Train Epoch: 34 [164928/225000 (73%)] Loss: 20305.220703\n",
      "Train Epoch: 34 [167424/225000 (74%)] Loss: 19885.128906\n",
      "Train Epoch: 34 [169920/225000 (76%)] Loss: 19831.435547\n",
      "Train Epoch: 34 [172416/225000 (77%)] Loss: 20437.429688\n",
      "Train Epoch: 34 [174912/225000 (78%)] Loss: 19690.398438\n",
      "Train Epoch: 34 [177408/225000 (79%)] Loss: 19919.728516\n",
      "Train Epoch: 34 [179904/225000 (80%)] Loss: 20636.441406\n",
      "Train Epoch: 34 [182400/225000 (81%)] Loss: 20673.343750\n",
      "Train Epoch: 34 [184896/225000 (82%)] Loss: 20675.113281\n",
      "Train Epoch: 34 [187392/225000 (83%)] Loss: 20046.695312\n",
      "Train Epoch: 34 [189888/225000 (84%)] Loss: 20514.226562\n",
      "Train Epoch: 34 [192384/225000 (86%)] Loss: 20126.738281\n",
      "Train Epoch: 34 [194880/225000 (87%)] Loss: 19900.169922\n",
      "Train Epoch: 34 [197376/225000 (88%)] Loss: 19806.423828\n",
      "Train Epoch: 34 [199872/225000 (89%)] Loss: 20154.257812\n",
      "Train Epoch: 34 [202368/225000 (90%)] Loss: 20076.642578\n",
      "Train Epoch: 34 [204864/225000 (91%)] Loss: 20735.302734\n",
      "Train Epoch: 34 [207360/225000 (92%)] Loss: 20446.636719\n",
      "Train Epoch: 34 [209856/225000 (93%)] Loss: 20344.242188\n",
      "Train Epoch: 34 [212352/225000 (94%)] Loss: 20265.937500\n",
      "Train Epoch: 34 [214848/225000 (95%)] Loss: 20353.511719\n",
      "Train Epoch: 34 [217344/225000 (97%)] Loss: 20552.355469\n",
      "Train Epoch: 34 [219840/225000 (98%)] Loss: 19611.707031\n",
      "Train Epoch: 34 [222336/225000 (99%)] Loss: 19823.011719\n",
      "Train Epoch: 34 [224832/225000 (100%)] Loss: 20288.265625\n",
      "    epoch          : 34\n",
      "    loss           : 20329.484551647824\n",
      "    val_loss       : 20248.77476262682\n",
      "Train Epoch: 35 [192/225000 (0%)] Loss: 20334.078125\n",
      "Train Epoch: 35 [2688/225000 (1%)] Loss: 20496.953125\n",
      "Train Epoch: 35 [5184/225000 (2%)] Loss: 20194.660156\n",
      "Train Epoch: 35 [7680/225000 (3%)] Loss: 20134.191406\n",
      "Train Epoch: 35 [10176/225000 (5%)] Loss: 20561.490234\n",
      "Train Epoch: 35 [12672/225000 (6%)] Loss: 20372.855469\n",
      "Train Epoch: 35 [15168/225000 (7%)] Loss: 19829.585938\n",
      "Train Epoch: 35 [17664/225000 (8%)] Loss: 20347.294922\n",
      "Train Epoch: 35 [20160/225000 (9%)] Loss: 20506.660156\n",
      "Train Epoch: 35 [22656/225000 (10%)] Loss: 20852.130859\n",
      "Train Epoch: 35 [25152/225000 (11%)] Loss: 20353.292969\n",
      "Train Epoch: 35 [27648/225000 (12%)] Loss: 20593.191406\n",
      "Train Epoch: 35 [30144/225000 (13%)] Loss: 20127.746094\n",
      "Train Epoch: 35 [32640/225000 (15%)] Loss: 20736.082031\n",
      "Train Epoch: 35 [35136/225000 (16%)] Loss: 20236.755859\n",
      "Train Epoch: 35 [37632/225000 (17%)] Loss: 20353.750000\n",
      "Train Epoch: 35 [40128/225000 (18%)] Loss: 20246.019531\n",
      "Train Epoch: 35 [42624/225000 (19%)] Loss: 20335.083984\n",
      "Train Epoch: 35 [45120/225000 (20%)] Loss: 20198.136719\n",
      "Train Epoch: 35 [47616/225000 (21%)] Loss: 20577.625000\n",
      "Train Epoch: 35 [50112/225000 (22%)] Loss: 19829.521484\n",
      "Train Epoch: 35 [52608/225000 (23%)] Loss: 20268.525391\n",
      "Train Epoch: 35 [55104/225000 (24%)] Loss: 20524.390625\n",
      "Train Epoch: 35 [57600/225000 (26%)] Loss: 20967.199219\n",
      "Train Epoch: 35 [60096/225000 (27%)] Loss: 19940.878906\n",
      "Train Epoch: 35 [62592/225000 (28%)] Loss: 20367.449219\n",
      "Train Epoch: 35 [65088/225000 (29%)] Loss: 20366.462891\n",
      "Train Epoch: 35 [67584/225000 (30%)] Loss: 20291.326172\n",
      "Train Epoch: 35 [70080/225000 (31%)] Loss: 20361.101562\n",
      "Train Epoch: 35 [72576/225000 (32%)] Loss: 19798.171875\n",
      "Train Epoch: 35 [75072/225000 (33%)] Loss: 20578.222656\n",
      "Train Epoch: 35 [77568/225000 (34%)] Loss: 19863.320312\n",
      "Train Epoch: 35 [80064/225000 (36%)] Loss: 20529.703125\n",
      "Train Epoch: 35 [82560/225000 (37%)] Loss: 20790.667969\n",
      "Train Epoch: 35 [85056/225000 (38%)] Loss: 20449.142578\n",
      "Train Epoch: 35 [87552/225000 (39%)] Loss: 19728.101562\n",
      "Train Epoch: 35 [90048/225000 (40%)] Loss: 20454.207031\n",
      "Train Epoch: 35 [92544/225000 (41%)] Loss: 20234.320312\n",
      "Train Epoch: 35 [95040/225000 (42%)] Loss: 20695.964844\n",
      "Train Epoch: 35 [97536/225000 (43%)] Loss: 20012.773438\n",
      "Train Epoch: 35 [100032/225000 (44%)] Loss: 20388.673828\n",
      "Train Epoch: 35 [102528/225000 (46%)] Loss: 20277.753906\n",
      "Train Epoch: 35 [105024/225000 (47%)] Loss: 20085.732422\n",
      "Train Epoch: 35 [107520/225000 (48%)] Loss: 20305.697266\n",
      "Train Epoch: 35 [110016/225000 (49%)] Loss: 20106.042969\n",
      "Train Epoch: 35 [112512/225000 (50%)] Loss: 19993.976562\n",
      "Train Epoch: 35 [115008/225000 (51%)] Loss: 19952.855469\n",
      "Train Epoch: 35 [117504/225000 (52%)] Loss: 20475.613281\n",
      "Train Epoch: 35 [120000/225000 (53%)] Loss: 20497.238281\n",
      "Train Epoch: 35 [122496/225000 (54%)] Loss: 20105.185547\n",
      "Train Epoch: 35 [124992/225000 (56%)] Loss: 20565.527344\n",
      "Train Epoch: 35 [127488/225000 (57%)] Loss: 20209.996094\n",
      "Train Epoch: 35 [129984/225000 (58%)] Loss: 20362.140625\n",
      "Train Epoch: 35 [132480/225000 (59%)] Loss: 20049.386719\n",
      "Train Epoch: 35 [134976/225000 (60%)] Loss: 21095.613281\n",
      "Train Epoch: 35 [137472/225000 (61%)] Loss: 20166.187500\n",
      "Train Epoch: 35 [139968/225000 (62%)] Loss: 19852.365234\n",
      "Train Epoch: 35 [142464/225000 (63%)] Loss: 20145.800781\n",
      "Train Epoch: 35 [144960/225000 (64%)] Loss: 20256.394531\n",
      "Train Epoch: 35 [147456/225000 (66%)] Loss: 20924.218750\n",
      "Train Epoch: 35 [149952/225000 (67%)] Loss: 20196.980469\n",
      "Train Epoch: 35 [152448/225000 (68%)] Loss: 20288.140625\n",
      "Train Epoch: 35 [154944/225000 (69%)] Loss: 20487.689453\n",
      "Train Epoch: 35 [157440/225000 (70%)] Loss: 20750.937500\n",
      "Train Epoch: 35 [159936/225000 (71%)] Loss: 20144.003906\n",
      "Train Epoch: 35 [162432/225000 (72%)] Loss: 20539.783203\n",
      "Train Epoch: 35 [164928/225000 (73%)] Loss: 19914.599609\n",
      "Train Epoch: 35 [167424/225000 (74%)] Loss: 20249.212891\n",
      "Train Epoch: 35 [169920/225000 (76%)] Loss: 20635.578125\n",
      "Train Epoch: 35 [172416/225000 (77%)] Loss: 20894.988281\n",
      "Train Epoch: 35 [174912/225000 (78%)] Loss: 20017.958984\n",
      "Train Epoch: 35 [177408/225000 (79%)] Loss: 20658.988281\n",
      "Train Epoch: 35 [179904/225000 (80%)] Loss: 20318.966797\n",
      "Train Epoch: 35 [182400/225000 (81%)] Loss: 20633.925781\n",
      "Train Epoch: 35 [184896/225000 (82%)] Loss: 20347.203125\n",
      "Train Epoch: 35 [187392/225000 (83%)] Loss: 20431.710938\n",
      "Train Epoch: 35 [189888/225000 (84%)] Loss: 20376.169922\n",
      "Train Epoch: 35 [192384/225000 (86%)] Loss: 20344.869141\n",
      "Train Epoch: 35 [194880/225000 (87%)] Loss: 20342.992188\n",
      "Train Epoch: 35 [197376/225000 (88%)] Loss: 20245.957031\n",
      "Train Epoch: 35 [199872/225000 (89%)] Loss: 20331.847656\n",
      "Train Epoch: 35 [202368/225000 (90%)] Loss: 20669.140625\n",
      "Train Epoch: 35 [204864/225000 (91%)] Loss: 20386.000000\n",
      "Train Epoch: 35 [207360/225000 (92%)] Loss: 20585.304688\n",
      "Train Epoch: 35 [209856/225000 (93%)] Loss: 20344.734375\n",
      "Train Epoch: 35 [212352/225000 (94%)] Loss: 20334.445312\n",
      "Train Epoch: 35 [214848/225000 (95%)] Loss: 19985.628906\n",
      "Train Epoch: 35 [217344/225000 (97%)] Loss: 20256.535156\n",
      "Train Epoch: 35 [219840/225000 (98%)] Loss: 20548.886719\n",
      "Train Epoch: 35 [222336/225000 (99%)] Loss: 20174.898438\n",
      "Train Epoch: 35 [224832/225000 (100%)] Loss: 20427.902344\n",
      "    epoch          : 35\n",
      "    loss           : 20322.297279956805\n",
      "    val_loss       : 20215.774625918337\n",
      "Train Epoch: 36 [192/225000 (0%)] Loss: 20110.863281\n",
      "Train Epoch: 36 [2688/225000 (1%)] Loss: 20412.974609\n",
      "Train Epoch: 36 [5184/225000 (2%)] Loss: 20399.664062\n",
      "Train Epoch: 36 [7680/225000 (3%)] Loss: 20501.453125\n",
      "Train Epoch: 36 [10176/225000 (5%)] Loss: 20098.031250\n",
      "Train Epoch: 36 [12672/225000 (6%)] Loss: 20259.691406\n",
      "Train Epoch: 36 [15168/225000 (7%)] Loss: 20255.093750\n",
      "Train Epoch: 36 [17664/225000 (8%)] Loss: 20524.085938\n",
      "Train Epoch: 36 [20160/225000 (9%)] Loss: 20746.921875\n",
      "Train Epoch: 36 [22656/225000 (10%)] Loss: 20421.128906\n",
      "Train Epoch: 36 [25152/225000 (11%)] Loss: 20053.843750\n",
      "Train Epoch: 36 [27648/225000 (12%)] Loss: 20114.505859\n",
      "Train Epoch: 36 [30144/225000 (13%)] Loss: 20263.103516\n",
      "Train Epoch: 36 [32640/225000 (15%)] Loss: 20332.517578\n",
      "Train Epoch: 36 [35136/225000 (16%)] Loss: 19968.625000\n",
      "Train Epoch: 36 [37632/225000 (17%)] Loss: 20112.527344\n",
      "Train Epoch: 36 [40128/225000 (18%)] Loss: 20793.828125\n",
      "Train Epoch: 36 [42624/225000 (19%)] Loss: 20010.222656\n",
      "Train Epoch: 36 [45120/225000 (20%)] Loss: 19801.117188\n",
      "Train Epoch: 36 [47616/225000 (21%)] Loss: 20437.474609\n",
      "Train Epoch: 36 [50112/225000 (22%)] Loss: 20322.785156\n",
      "Train Epoch: 36 [52608/225000 (23%)] Loss: 20620.714844\n",
      "Train Epoch: 36 [55104/225000 (24%)] Loss: 20343.027344\n",
      "Train Epoch: 36 [57600/225000 (26%)] Loss: 20742.695312\n",
      "Train Epoch: 36 [60096/225000 (27%)] Loss: 20477.914062\n",
      "Train Epoch: 36 [62592/225000 (28%)] Loss: 19599.531250\n",
      "Train Epoch: 36 [65088/225000 (29%)] Loss: 19607.765625\n",
      "Train Epoch: 36 [67584/225000 (30%)] Loss: 20418.058594\n",
      "Train Epoch: 36 [70080/225000 (31%)] Loss: 20523.837891\n",
      "Train Epoch: 36 [72576/225000 (32%)] Loss: 19960.015625\n",
      "Train Epoch: 36 [75072/225000 (33%)] Loss: 20380.042969\n",
      "Train Epoch: 36 [77568/225000 (34%)] Loss: 20286.562500\n",
      "Train Epoch: 36 [80064/225000 (36%)] Loss: 20479.417969\n",
      "Train Epoch: 36 [82560/225000 (37%)] Loss: 20848.457031\n",
      "Train Epoch: 36 [85056/225000 (38%)] Loss: 20056.851562\n",
      "Train Epoch: 36 [87552/225000 (39%)] Loss: 20423.949219\n",
      "Train Epoch: 36 [90048/225000 (40%)] Loss: 20308.867188\n",
      "Train Epoch: 36 [92544/225000 (41%)] Loss: 20183.300781\n",
      "Train Epoch: 36 [95040/225000 (42%)] Loss: 20318.679688\n",
      "Train Epoch: 36 [97536/225000 (43%)] Loss: 20750.046875\n",
      "Train Epoch: 36 [100032/225000 (44%)] Loss: 20491.332031\n",
      "Train Epoch: 36 [102528/225000 (46%)] Loss: 20032.718750\n",
      "Train Epoch: 36 [105024/225000 (47%)] Loss: 20246.386719\n",
      "Train Epoch: 36 [107520/225000 (48%)] Loss: 20719.720703\n",
      "Train Epoch: 36 [110016/225000 (49%)] Loss: 20067.970703\n",
      "Train Epoch: 36 [112512/225000 (50%)] Loss: 20306.052734\n",
      "Train Epoch: 36 [115008/225000 (51%)] Loss: 20402.339844\n",
      "Train Epoch: 36 [117504/225000 (52%)] Loss: 20308.230469\n",
      "Train Epoch: 36 [120000/225000 (53%)] Loss: 20010.363281\n",
      "Train Epoch: 36 [122496/225000 (54%)] Loss: 20036.988281\n",
      "Train Epoch: 36 [124992/225000 (56%)] Loss: 20303.890625\n",
      "Train Epoch: 36 [127488/225000 (57%)] Loss: 20324.271484\n",
      "Train Epoch: 36 [129984/225000 (58%)] Loss: 20182.632812\n",
      "Train Epoch: 36 [132480/225000 (59%)] Loss: 20147.378906\n",
      "Train Epoch: 36 [134976/225000 (60%)] Loss: 20718.398438\n",
      "Train Epoch: 36 [137472/225000 (61%)] Loss: 20829.214844\n",
      "Train Epoch: 36 [139968/225000 (62%)] Loss: 19993.605469\n",
      "Train Epoch: 36 [142464/225000 (63%)] Loss: 20478.769531\n",
      "Train Epoch: 36 [144960/225000 (64%)] Loss: 19902.656250\n",
      "Train Epoch: 36 [147456/225000 (66%)] Loss: 20374.255859\n",
      "Train Epoch: 36 [149952/225000 (67%)] Loss: 21052.140625\n",
      "Train Epoch: 36 [152448/225000 (68%)] Loss: 20120.417969\n",
      "Train Epoch: 36 [154944/225000 (69%)] Loss: 20023.281250\n",
      "Train Epoch: 36 [157440/225000 (70%)] Loss: 20226.132812\n",
      "Train Epoch: 36 [159936/225000 (71%)] Loss: 20233.218750\n",
      "Train Epoch: 36 [162432/225000 (72%)] Loss: 19852.535156\n",
      "Train Epoch: 36 [164928/225000 (73%)] Loss: 20084.480469\n",
      "Train Epoch: 36 [167424/225000 (74%)] Loss: 20017.105469\n",
      "Train Epoch: 36 [169920/225000 (76%)] Loss: 21087.957031\n",
      "Train Epoch: 36 [172416/225000 (77%)] Loss: 20831.476562\n",
      "Train Epoch: 36 [174912/225000 (78%)] Loss: 20230.281250\n",
      "Train Epoch: 36 [177408/225000 (79%)] Loss: 20402.062500\n",
      "Train Epoch: 36 [179904/225000 (80%)] Loss: 19663.080078\n",
      "Train Epoch: 36 [182400/225000 (81%)] Loss: 20298.769531\n",
      "Train Epoch: 36 [184896/225000 (82%)] Loss: 20505.189453\n",
      "Train Epoch: 36 [187392/225000 (83%)] Loss: 19954.294922\n",
      "Train Epoch: 36 [189888/225000 (84%)] Loss: 19939.582031\n",
      "Train Epoch: 36 [192384/225000 (86%)] Loss: 20335.640625\n",
      "Train Epoch: 36 [194880/225000 (87%)] Loss: 20228.535156\n",
      "Train Epoch: 36 [197376/225000 (88%)] Loss: 20360.779297\n",
      "Train Epoch: 36 [199872/225000 (89%)] Loss: 19905.095703\n",
      "Train Epoch: 36 [202368/225000 (90%)] Loss: 20549.255859\n",
      "Train Epoch: 36 [204864/225000 (91%)] Loss: 20070.494141\n",
      "Train Epoch: 36 [207360/225000 (92%)] Loss: 19805.048828\n",
      "Train Epoch: 36 [209856/225000 (93%)] Loss: 20247.089844\n",
      "Train Epoch: 36 [212352/225000 (94%)] Loss: 20153.902344\n",
      "Train Epoch: 36 [214848/225000 (95%)] Loss: 20118.730469\n",
      "Train Epoch: 36 [217344/225000 (97%)] Loss: 19855.835938\n",
      "Train Epoch: 36 [219840/225000 (98%)] Loss: 20246.871094\n",
      "Train Epoch: 36 [222336/225000 (99%)] Loss: 20317.921875\n",
      "Train Epoch: 36 [224832/225000 (100%)] Loss: 19766.990234\n",
      "    epoch          : 36\n",
      "    loss           : 20305.01855302101\n",
      "    val_loss       : 20199.76046186185\n",
      "Train Epoch: 37 [192/225000 (0%)] Loss: 20503.611328\n",
      "Train Epoch: 37 [2688/225000 (1%)] Loss: 20192.191406\n",
      "Train Epoch: 37 [5184/225000 (2%)] Loss: 20315.777344\n",
      "Train Epoch: 37 [7680/225000 (3%)] Loss: 20461.535156\n",
      "Train Epoch: 37 [10176/225000 (5%)] Loss: 20686.648438\n",
      "Train Epoch: 37 [12672/225000 (6%)] Loss: 20446.470703\n",
      "Train Epoch: 37 [15168/225000 (7%)] Loss: 20480.796875\n",
      "Train Epoch: 37 [17664/225000 (8%)] Loss: 19885.986328\n",
      "Train Epoch: 37 [20160/225000 (9%)] Loss: 20583.410156\n",
      "Train Epoch: 37 [22656/225000 (10%)] Loss: 20884.144531\n",
      "Train Epoch: 37 [25152/225000 (11%)] Loss: 20426.238281\n",
      "Train Epoch: 37 [27648/225000 (12%)] Loss: 20225.906250\n",
      "Train Epoch: 37 [30144/225000 (13%)] Loss: 19765.115234\n",
      "Train Epoch: 37 [32640/225000 (15%)] Loss: 20705.808594\n",
      "Train Epoch: 37 [35136/225000 (16%)] Loss: 20406.246094\n",
      "Train Epoch: 37 [37632/225000 (17%)] Loss: 20957.812500\n",
      "Train Epoch: 37 [40128/225000 (18%)] Loss: 20181.617188\n",
      "Train Epoch: 37 [42624/225000 (19%)] Loss: 20218.210938\n",
      "Train Epoch: 37 [45120/225000 (20%)] Loss: 20152.339844\n",
      "Train Epoch: 37 [47616/225000 (21%)] Loss: 20155.429688\n",
      "Train Epoch: 37 [50112/225000 (22%)] Loss: 20112.132812\n",
      "Train Epoch: 37 [52608/225000 (23%)] Loss: 20089.992188\n",
      "Train Epoch: 37 [55104/225000 (24%)] Loss: 20186.750000\n",
      "Train Epoch: 37 [57600/225000 (26%)] Loss: 20220.441406\n",
      "Train Epoch: 37 [60096/225000 (27%)] Loss: 20210.554688\n",
      "Train Epoch: 37 [62592/225000 (28%)] Loss: 20378.541016\n",
      "Train Epoch: 37 [65088/225000 (29%)] Loss: 19951.845703\n",
      "Train Epoch: 37 [67584/225000 (30%)] Loss: 20100.042969\n",
      "Train Epoch: 37 [70080/225000 (31%)] Loss: 19712.378906\n",
      "Train Epoch: 37 [72576/225000 (32%)] Loss: 20295.943359\n",
      "Train Epoch: 37 [75072/225000 (33%)] Loss: 20589.378906\n",
      "Train Epoch: 37 [77568/225000 (34%)] Loss: 20561.328125\n",
      "Train Epoch: 37 [80064/225000 (36%)] Loss: 20347.031250\n",
      "Train Epoch: 37 [82560/225000 (37%)] Loss: 20004.687500\n",
      "Train Epoch: 37 [85056/225000 (38%)] Loss: 20190.917969\n",
      "Train Epoch: 37 [87552/225000 (39%)] Loss: 20432.691406\n",
      "Train Epoch: 37 [90048/225000 (40%)] Loss: 20112.421875\n",
      "Train Epoch: 37 [92544/225000 (41%)] Loss: 19693.203125\n",
      "Train Epoch: 37 [95040/225000 (42%)] Loss: 20292.306641\n",
      "Train Epoch: 37 [97536/225000 (43%)] Loss: 20134.925781\n",
      "Train Epoch: 37 [100032/225000 (44%)] Loss: 20333.082031\n",
      "Train Epoch: 37 [102528/225000 (46%)] Loss: 20446.515625\n",
      "Train Epoch: 37 [105024/225000 (47%)] Loss: 20011.894531\n",
      "Train Epoch: 37 [107520/225000 (48%)] Loss: 19627.398438\n",
      "Train Epoch: 37 [110016/225000 (49%)] Loss: 19931.333984\n",
      "Train Epoch: 37 [112512/225000 (50%)] Loss: 20256.128906\n",
      "Train Epoch: 37 [115008/225000 (51%)] Loss: 19976.175781\n",
      "Train Epoch: 37 [117504/225000 (52%)] Loss: 19996.718750\n",
      "Train Epoch: 37 [120000/225000 (53%)] Loss: 20557.769531\n",
      "Train Epoch: 37 [122496/225000 (54%)] Loss: 20375.738281\n",
      "Train Epoch: 37 [124992/225000 (56%)] Loss: 20328.468750\n",
      "Train Epoch: 37 [127488/225000 (57%)] Loss: 20327.613281\n",
      "Train Epoch: 37 [129984/225000 (58%)] Loss: 19964.378906\n",
      "Train Epoch: 37 [132480/225000 (59%)] Loss: 20084.298828\n",
      "Train Epoch: 37 [134976/225000 (60%)] Loss: 19779.468750\n",
      "Train Epoch: 37 [137472/225000 (61%)] Loss: 20382.970703\n",
      "Train Epoch: 37 [139968/225000 (62%)] Loss: 20304.480469\n",
      "Train Epoch: 37 [142464/225000 (63%)] Loss: 20850.416016\n",
      "Train Epoch: 37 [144960/225000 (64%)] Loss: 20558.906250\n",
      "Train Epoch: 37 [147456/225000 (66%)] Loss: 20221.335938\n",
      "Train Epoch: 37 [149952/225000 (67%)] Loss: 20764.566406\n",
      "Train Epoch: 37 [152448/225000 (68%)] Loss: 20271.296875\n",
      "Train Epoch: 37 [154944/225000 (69%)] Loss: 19978.257812\n",
      "Train Epoch: 37 [157440/225000 (70%)] Loss: 20285.292969\n",
      "Train Epoch: 37 [159936/225000 (71%)] Loss: 20110.078125\n",
      "Train Epoch: 37 [162432/225000 (72%)] Loss: 20152.878906\n",
      "Train Epoch: 37 [164928/225000 (73%)] Loss: 20381.308594\n",
      "Train Epoch: 37 [167424/225000 (74%)] Loss: 19896.591797\n",
      "Train Epoch: 37 [169920/225000 (76%)] Loss: 20693.890625\n",
      "Train Epoch: 37 [172416/225000 (77%)] Loss: 20096.496094\n",
      "Train Epoch: 37 [174912/225000 (78%)] Loss: 20621.376953\n",
      "Train Epoch: 37 [177408/225000 (79%)] Loss: 20366.935547\n",
      "Train Epoch: 37 [179904/225000 (80%)] Loss: 20311.359375\n",
      "Train Epoch: 37 [182400/225000 (81%)] Loss: 20321.718750\n",
      "Train Epoch: 37 [184896/225000 (82%)] Loss: 20614.554688\n",
      "Train Epoch: 37 [187392/225000 (83%)] Loss: 20276.539062\n",
      "Train Epoch: 37 [189888/225000 (84%)] Loss: 20234.398438\n",
      "Train Epoch: 37 [192384/225000 (86%)] Loss: 20852.806641\n",
      "Train Epoch: 37 [194880/225000 (87%)] Loss: 20348.007812\n",
      "Train Epoch: 37 [197376/225000 (88%)] Loss: 20239.542969\n",
      "Train Epoch: 37 [199872/225000 (89%)] Loss: 20569.695312\n",
      "Train Epoch: 37 [202368/225000 (90%)] Loss: 20518.390625\n",
      "Train Epoch: 37 [204864/225000 (91%)] Loss: 20309.800781\n",
      "Train Epoch: 37 [207360/225000 (92%)] Loss: 20652.718750\n",
      "Train Epoch: 37 [209856/225000 (93%)] Loss: 20408.632812\n",
      "Train Epoch: 37 [212352/225000 (94%)] Loss: 19946.074219\n",
      "Train Epoch: 37 [214848/225000 (95%)] Loss: 20365.925781\n",
      "Train Epoch: 37 [217344/225000 (97%)] Loss: 20156.261719\n",
      "Train Epoch: 37 [219840/225000 (98%)] Loss: 20220.078125\n",
      "Train Epoch: 37 [222336/225000 (99%)] Loss: 20366.974609\n",
      "Train Epoch: 37 [224832/225000 (100%)] Loss: 20388.281250\n",
      "    epoch          : 37\n",
      "    loss           : 20298.98645144518\n",
      "    val_loss       : 20195.65824703133\n",
      "Train Epoch: 38 [192/225000 (0%)] Loss: 19583.347656\n",
      "Train Epoch: 38 [2688/225000 (1%)] Loss: 20070.964844\n",
      "Train Epoch: 38 [5184/225000 (2%)] Loss: 20608.457031\n",
      "Train Epoch: 38 [7680/225000 (3%)] Loss: 20445.898438\n",
      "Train Epoch: 38 [10176/225000 (5%)] Loss: 20205.087891\n",
      "Train Epoch: 38 [12672/225000 (6%)] Loss: 20505.050781\n",
      "Train Epoch: 38 [15168/225000 (7%)] Loss: 20436.730469\n",
      "Train Epoch: 38 [17664/225000 (8%)] Loss: 20576.160156\n",
      "Train Epoch: 38 [20160/225000 (9%)] Loss: 20508.349609\n",
      "Train Epoch: 38 [22656/225000 (10%)] Loss: 19416.599609\n",
      "Train Epoch: 38 [25152/225000 (11%)] Loss: 20147.574219\n",
      "Train Epoch: 38 [27648/225000 (12%)] Loss: 20216.070312\n",
      "Train Epoch: 38 [30144/225000 (13%)] Loss: 20100.339844\n",
      "Train Epoch: 38 [32640/225000 (15%)] Loss: 20207.607422\n",
      "Train Epoch: 38 [35136/225000 (16%)] Loss: 20362.648438\n",
      "Train Epoch: 38 [37632/225000 (17%)] Loss: 20325.312500\n",
      "Train Epoch: 38 [40128/225000 (18%)] Loss: 20489.925781\n",
      "Train Epoch: 38 [42624/225000 (19%)] Loss: 20201.660156\n",
      "Train Epoch: 38 [45120/225000 (20%)] Loss: 20325.453125\n",
      "Train Epoch: 38 [47616/225000 (21%)] Loss: 20056.128906\n",
      "Train Epoch: 38 [50112/225000 (22%)] Loss: 20585.296875\n",
      "Train Epoch: 38 [52608/225000 (23%)] Loss: 19841.914062\n",
      "Train Epoch: 38 [55104/225000 (24%)] Loss: 20058.816406\n",
      "Train Epoch: 38 [57600/225000 (26%)] Loss: 20731.710938\n",
      "Train Epoch: 38 [60096/225000 (27%)] Loss: 20090.373047\n",
      "Train Epoch: 38 [62592/225000 (28%)] Loss: 20794.707031\n",
      "Train Epoch: 38 [65088/225000 (29%)] Loss: 20235.929688\n",
      "Train Epoch: 38 [67584/225000 (30%)] Loss: 20631.369141\n",
      "Train Epoch: 38 [70080/225000 (31%)] Loss: 20313.216797\n",
      "Train Epoch: 38 [72576/225000 (32%)] Loss: 20084.117188\n",
      "Train Epoch: 38 [75072/225000 (33%)] Loss: 20381.082031\n",
      "Train Epoch: 38 [77568/225000 (34%)] Loss: 19845.384766\n",
      "Train Epoch: 38 [80064/225000 (36%)] Loss: 20452.888672\n",
      "Train Epoch: 38 [82560/225000 (37%)] Loss: 20794.117188\n",
      "Train Epoch: 38 [85056/225000 (38%)] Loss: 20261.162109\n",
      "Train Epoch: 38 [87552/225000 (39%)] Loss: 20329.156250\n",
      "Train Epoch: 38 [90048/225000 (40%)] Loss: 20364.082031\n",
      "Train Epoch: 38 [92544/225000 (41%)] Loss: 20226.390625\n",
      "Train Epoch: 38 [95040/225000 (42%)] Loss: 20450.361328\n",
      "Train Epoch: 38 [97536/225000 (43%)] Loss: 19933.376953\n",
      "Train Epoch: 38 [100032/225000 (44%)] Loss: 20031.673828\n",
      "Train Epoch: 38 [102528/225000 (46%)] Loss: 20029.164062\n",
      "Train Epoch: 38 [105024/225000 (47%)] Loss: 20501.886719\n",
      "Train Epoch: 38 [107520/225000 (48%)] Loss: 20358.994141\n",
      "Train Epoch: 38 [110016/225000 (49%)] Loss: 20668.730469\n",
      "Train Epoch: 38 [112512/225000 (50%)] Loss: 20178.167969\n",
      "Train Epoch: 38 [115008/225000 (51%)] Loss: 20117.355469\n",
      "Train Epoch: 38 [117504/225000 (52%)] Loss: 20633.816406\n",
      "Train Epoch: 38 [120000/225000 (53%)] Loss: 20381.052734\n",
      "Train Epoch: 38 [122496/225000 (54%)] Loss: 20561.685547\n",
      "Train Epoch: 38 [124992/225000 (56%)] Loss: 19874.662109\n",
      "Train Epoch: 38 [127488/225000 (57%)] Loss: 20422.740234\n",
      "Train Epoch: 38 [129984/225000 (58%)] Loss: 20035.242188\n",
      "Train Epoch: 38 [132480/225000 (59%)] Loss: 20704.398438\n",
      "Train Epoch: 38 [134976/225000 (60%)] Loss: 20231.365234\n",
      "Train Epoch: 38 [137472/225000 (61%)] Loss: 20647.335938\n",
      "Train Epoch: 38 [139968/225000 (62%)] Loss: 19931.832031\n",
      "Train Epoch: 38 [142464/225000 (63%)] Loss: 20416.283203\n",
      "Train Epoch: 38 [144960/225000 (64%)] Loss: 20645.605469\n",
      "Train Epoch: 38 [147456/225000 (66%)] Loss: 20427.382812\n",
      "Train Epoch: 38 [149952/225000 (67%)] Loss: 20252.314453\n",
      "Train Epoch: 38 [152448/225000 (68%)] Loss: 20063.029297\n",
      "Train Epoch: 38 [154944/225000 (69%)] Loss: 20051.314453\n",
      "Train Epoch: 38 [157440/225000 (70%)] Loss: 19996.320312\n",
      "Train Epoch: 38 [159936/225000 (71%)] Loss: 20038.175781\n",
      "Train Epoch: 38 [162432/225000 (72%)] Loss: 19911.554688\n",
      "Train Epoch: 38 [164928/225000 (73%)] Loss: 20327.347656\n",
      "Train Epoch: 38 [167424/225000 (74%)] Loss: 20125.111328\n",
      "Train Epoch: 38 [169920/225000 (76%)] Loss: 20940.296875\n",
      "Train Epoch: 38 [172416/225000 (77%)] Loss: 20526.585938\n",
      "Train Epoch: 38 [174912/225000 (78%)] Loss: 20422.611328\n",
      "Train Epoch: 38 [177408/225000 (79%)] Loss: 20431.656250\n",
      "Train Epoch: 38 [179904/225000 (80%)] Loss: 20142.148438\n",
      "Train Epoch: 38 [182400/225000 (81%)] Loss: 20391.541016\n",
      "Train Epoch: 38 [184896/225000 (82%)] Loss: 20182.222656\n",
      "Train Epoch: 38 [187392/225000 (83%)] Loss: 19916.820312\n",
      "Train Epoch: 38 [189888/225000 (84%)] Loss: 20537.666016\n",
      "Train Epoch: 38 [192384/225000 (86%)] Loss: 20201.015625\n",
      "Train Epoch: 38 [194880/225000 (87%)] Loss: 20434.232422\n",
      "Train Epoch: 38 [197376/225000 (88%)] Loss: 20482.611328\n",
      "Train Epoch: 38 [199872/225000 (89%)] Loss: 20341.878906\n",
      "Train Epoch: 38 [202368/225000 (90%)] Loss: 20963.757812\n",
      "Train Epoch: 38 [204864/225000 (91%)] Loss: 20228.818359\n",
      "Train Epoch: 38 [207360/225000 (92%)] Loss: 20079.828125\n",
      "Train Epoch: 38 [209856/225000 (93%)] Loss: 20297.812500\n",
      "Train Epoch: 38 [212352/225000 (94%)] Loss: 19923.890625\n",
      "Train Epoch: 38 [214848/225000 (95%)] Loss: 20293.925781\n",
      "Train Epoch: 38 [217344/225000 (97%)] Loss: 20143.339844\n",
      "Train Epoch: 38 [219840/225000 (98%)] Loss: 20272.953125\n",
      "Train Epoch: 38 [222336/225000 (99%)] Loss: 20221.691406\n",
      "Train Epoch: 38 [224832/225000 (100%)] Loss: 19941.369141\n",
      "    epoch          : 38\n",
      "    loss           : 20290.278057007254\n",
      "    val_loss       : 20188.35770404657\n",
      "Train Epoch: 39 [192/225000 (0%)] Loss: 20582.941406\n",
      "Train Epoch: 39 [2688/225000 (1%)] Loss: 20618.697266\n",
      "Train Epoch: 39 [5184/225000 (2%)] Loss: 20223.890625\n",
      "Train Epoch: 39 [7680/225000 (3%)] Loss: 20581.441406\n",
      "Train Epoch: 39 [10176/225000 (5%)] Loss: 20259.875000\n",
      "Train Epoch: 39 [12672/225000 (6%)] Loss: 20919.144531\n",
      "Train Epoch: 39 [15168/225000 (7%)] Loss: 20472.207031\n",
      "Train Epoch: 39 [17664/225000 (8%)] Loss: 20002.261719\n",
      "Train Epoch: 39 [20160/225000 (9%)] Loss: 20384.972656\n",
      "Train Epoch: 39 [22656/225000 (10%)] Loss: 20559.542969\n",
      "Train Epoch: 39 [25152/225000 (11%)] Loss: 20116.183594\n",
      "Train Epoch: 39 [27648/225000 (12%)] Loss: 20288.386719\n",
      "Train Epoch: 39 [30144/225000 (13%)] Loss: 20023.591797\n",
      "Train Epoch: 39 [32640/225000 (15%)] Loss: 20138.367188\n",
      "Train Epoch: 39 [35136/225000 (16%)] Loss: 20657.345703\n",
      "Train Epoch: 39 [37632/225000 (17%)] Loss: 20427.763672\n",
      "Train Epoch: 39 [40128/225000 (18%)] Loss: 19746.970703\n",
      "Train Epoch: 39 [42624/225000 (19%)] Loss: 20404.914062\n",
      "Train Epoch: 39 [45120/225000 (20%)] Loss: 20592.314453\n",
      "Train Epoch: 39 [47616/225000 (21%)] Loss: 20258.566406\n",
      "Train Epoch: 39 [50112/225000 (22%)] Loss: 20138.673828\n",
      "Train Epoch: 39 [52608/225000 (23%)] Loss: 20140.902344\n",
      "Train Epoch: 39 [55104/225000 (24%)] Loss: 20509.923828\n",
      "Train Epoch: 39 [57600/225000 (26%)] Loss: 20323.425781\n",
      "Train Epoch: 39 [60096/225000 (27%)] Loss: 20372.464844\n",
      "Train Epoch: 39 [62592/225000 (28%)] Loss: 20843.923828\n",
      "Train Epoch: 39 [65088/225000 (29%)] Loss: 20583.394531\n",
      "Train Epoch: 39 [67584/225000 (30%)] Loss: 20552.367188\n",
      "Train Epoch: 39 [70080/225000 (31%)] Loss: 20268.566406\n",
      "Train Epoch: 39 [72576/225000 (32%)] Loss: 20015.968750\n",
      "Train Epoch: 39 [75072/225000 (33%)] Loss: 20272.515625\n",
      "Train Epoch: 39 [77568/225000 (34%)] Loss: 20099.466797\n",
      "Train Epoch: 39 [80064/225000 (36%)] Loss: 20516.160156\n",
      "Train Epoch: 39 [82560/225000 (37%)] Loss: 20426.625000\n",
      "Train Epoch: 39 [85056/225000 (38%)] Loss: 20655.207031\n",
      "Train Epoch: 39 [87552/225000 (39%)] Loss: 20318.324219\n",
      "Train Epoch: 39 [90048/225000 (40%)] Loss: 20263.980469\n",
      "Train Epoch: 39 [92544/225000 (41%)] Loss: 20314.500000\n",
      "Train Epoch: 39 [95040/225000 (42%)] Loss: 20099.849609\n",
      "Train Epoch: 39 [97536/225000 (43%)] Loss: 20235.273438\n",
      "Train Epoch: 39 [100032/225000 (44%)] Loss: 20178.371094\n",
      "Train Epoch: 39 [102528/225000 (46%)] Loss: 20238.046875\n",
      "Train Epoch: 39 [105024/225000 (47%)] Loss: 20875.679688\n",
      "Train Epoch: 39 [107520/225000 (48%)] Loss: 20765.902344\n",
      "Train Epoch: 39 [110016/225000 (49%)] Loss: 20075.447266\n",
      "Train Epoch: 39 [112512/225000 (50%)] Loss: 19459.626953\n",
      "Train Epoch: 39 [115008/225000 (51%)] Loss: 20466.953125\n",
      "Train Epoch: 39 [117504/225000 (52%)] Loss: 20404.796875\n",
      "Train Epoch: 39 [120000/225000 (53%)] Loss: 20487.878906\n",
      "Train Epoch: 39 [122496/225000 (54%)] Loss: 20092.468750\n",
      "Train Epoch: 39 [124992/225000 (56%)] Loss: 20577.724609\n",
      "Train Epoch: 39 [127488/225000 (57%)] Loss: 20399.298828\n",
      "Train Epoch: 39 [129984/225000 (58%)] Loss: 19898.943359\n",
      "Train Epoch: 39 [132480/225000 (59%)] Loss: 20510.646484\n",
      "Train Epoch: 39 [134976/225000 (60%)] Loss: 20034.814453\n",
      "Train Epoch: 39 [137472/225000 (61%)] Loss: 20159.429688\n",
      "Train Epoch: 39 [139968/225000 (62%)] Loss: 20239.164062\n",
      "Train Epoch: 39 [142464/225000 (63%)] Loss: 20088.378906\n",
      "Train Epoch: 39 [144960/225000 (64%)] Loss: 19934.242188\n",
      "Train Epoch: 39 [147456/225000 (66%)] Loss: 20273.285156\n",
      "Train Epoch: 39 [149952/225000 (67%)] Loss: 19959.019531\n",
      "Train Epoch: 39 [152448/225000 (68%)] Loss: 20209.892578\n",
      "Train Epoch: 39 [154944/225000 (69%)] Loss: 19438.136719\n",
      "Train Epoch: 39 [157440/225000 (70%)] Loss: 20233.691406\n",
      "Train Epoch: 39 [159936/225000 (71%)] Loss: 19970.226562\n",
      "Train Epoch: 39 [162432/225000 (72%)] Loss: 20039.367188\n",
      "Train Epoch: 39 [164928/225000 (73%)] Loss: 19797.599609\n",
      "Train Epoch: 39 [167424/225000 (74%)] Loss: 19653.593750\n",
      "Train Epoch: 39 [169920/225000 (76%)] Loss: 20373.212891\n",
      "Train Epoch: 39 [172416/225000 (77%)] Loss: 20188.500000\n",
      "Train Epoch: 39 [174912/225000 (78%)] Loss: 20252.289062\n",
      "Train Epoch: 39 [177408/225000 (79%)] Loss: 20002.328125\n",
      "Train Epoch: 39 [179904/225000 (80%)] Loss: 20618.197266\n",
      "Train Epoch: 39 [182400/225000 (81%)] Loss: 20705.226562\n",
      "Train Epoch: 39 [184896/225000 (82%)] Loss: 19962.097656\n",
      "Train Epoch: 39 [187392/225000 (83%)] Loss: 19959.539062\n",
      "Train Epoch: 39 [189888/225000 (84%)] Loss: 20739.488281\n",
      "Train Epoch: 39 [192384/225000 (86%)] Loss: 20567.167969\n",
      "Train Epoch: 39 [194880/225000 (87%)] Loss: 20435.417969\n",
      "Train Epoch: 39 [197376/225000 (88%)] Loss: 20297.162109\n",
      "Train Epoch: 39 [199872/225000 (89%)] Loss: 19751.236328\n",
      "Train Epoch: 39 [202368/225000 (90%)] Loss: 20517.544922\n",
      "Train Epoch: 39 [204864/225000 (91%)] Loss: 20677.601562\n",
      "Train Epoch: 39 [207360/225000 (92%)] Loss: 20167.691406\n",
      "Train Epoch: 39 [209856/225000 (93%)] Loss: 20539.117188\n",
      "Train Epoch: 39 [212352/225000 (94%)] Loss: 20204.792969\n",
      "Train Epoch: 39 [214848/225000 (95%)] Loss: 20173.593750\n",
      "Train Epoch: 39 [217344/225000 (97%)] Loss: 19858.156250\n",
      "Train Epoch: 39 [219840/225000 (98%)] Loss: 20277.519531\n",
      "Train Epoch: 39 [222336/225000 (99%)] Loss: 19926.281250\n",
      "Train Epoch: 39 [224832/225000 (100%)] Loss: 20439.076172\n",
      "    epoch          : 39\n",
      "    loss           : 20278.391638225257\n",
      "    val_loss       : 20179.79235273827\n",
      "Train Epoch: 40 [192/225000 (0%)] Loss: 20446.001953\n",
      "Train Epoch: 40 [2688/225000 (1%)] Loss: 20672.347656\n",
      "Train Epoch: 40 [5184/225000 (2%)] Loss: 20133.156250\n",
      "Train Epoch: 40 [7680/225000 (3%)] Loss: 20039.855469\n",
      "Train Epoch: 40 [10176/225000 (5%)] Loss: 20436.507812\n",
      "Train Epoch: 40 [12672/225000 (6%)] Loss: 19879.556641\n",
      "Train Epoch: 40 [15168/225000 (7%)] Loss: 20105.070312\n",
      "Train Epoch: 40 [17664/225000 (8%)] Loss: 20055.300781\n",
      "Train Epoch: 40 [20160/225000 (9%)] Loss: 20352.007812\n",
      "Train Epoch: 40 [22656/225000 (10%)] Loss: 20207.171875\n",
      "Train Epoch: 40 [25152/225000 (11%)] Loss: 20880.167969\n",
      "Train Epoch: 40 [27648/225000 (12%)] Loss: 20240.894531\n",
      "Train Epoch: 40 [30144/225000 (13%)] Loss: 20340.757812\n",
      "Train Epoch: 40 [32640/225000 (15%)] Loss: 20019.472656\n",
      "Train Epoch: 40 [35136/225000 (16%)] Loss: 20401.843750\n",
      "Train Epoch: 40 [37632/225000 (17%)] Loss: 20528.792969\n",
      "Train Epoch: 40 [40128/225000 (18%)] Loss: 20968.023438\n",
      "Train Epoch: 40 [42624/225000 (19%)] Loss: 20525.468750\n",
      "Train Epoch: 40 [45120/225000 (20%)] Loss: 20996.371094\n",
      "Train Epoch: 40 [47616/225000 (21%)] Loss: 20430.929688\n",
      "Train Epoch: 40 [50112/225000 (22%)] Loss: 20561.820312\n",
      "Train Epoch: 40 [52608/225000 (23%)] Loss: 19976.273438\n",
      "Train Epoch: 40 [55104/225000 (24%)] Loss: 20106.212891\n",
      "Train Epoch: 40 [57600/225000 (26%)] Loss: 19919.664062\n",
      "Train Epoch: 40 [60096/225000 (27%)] Loss: 19944.539062\n",
      "Train Epoch: 40 [62592/225000 (28%)] Loss: 19916.148438\n",
      "Train Epoch: 40 [65088/225000 (29%)] Loss: 20173.539062\n",
      "Train Epoch: 40 [67584/225000 (30%)] Loss: 20490.431641\n",
      "Train Epoch: 40 [70080/225000 (31%)] Loss: 19939.906250\n",
      "Train Epoch: 40 [72576/225000 (32%)] Loss: 19980.601562\n",
      "Train Epoch: 40 [75072/225000 (33%)] Loss: 20552.332031\n",
      "Train Epoch: 40 [77568/225000 (34%)] Loss: 19968.347656\n",
      "Train Epoch: 40 [80064/225000 (36%)] Loss: 20104.832031\n",
      "Train Epoch: 40 [82560/225000 (37%)] Loss: 20180.642578\n",
      "Train Epoch: 40 [85056/225000 (38%)] Loss: 20405.392578\n",
      "Train Epoch: 40 [87552/225000 (39%)] Loss: 20493.511719\n",
      "Train Epoch: 40 [90048/225000 (40%)] Loss: 20604.894531\n",
      "Train Epoch: 40 [92544/225000 (41%)] Loss: 20451.734375\n",
      "Train Epoch: 40 [95040/225000 (42%)] Loss: 20032.859375\n",
      "Train Epoch: 40 [97536/225000 (43%)] Loss: 19911.562500\n",
      "Train Epoch: 40 [100032/225000 (44%)] Loss: 20573.203125\n",
      "Train Epoch: 40 [102528/225000 (46%)] Loss: 20292.101562\n",
      "Train Epoch: 40 [105024/225000 (47%)] Loss: 20793.968750\n",
      "Train Epoch: 40 [107520/225000 (48%)] Loss: 20539.761719\n",
      "Train Epoch: 40 [110016/225000 (49%)] Loss: 20437.148438\n",
      "Train Epoch: 40 [112512/225000 (50%)] Loss: 20273.503906\n",
      "Train Epoch: 40 [115008/225000 (51%)] Loss: 20670.390625\n",
      "Train Epoch: 40 [117504/225000 (52%)] Loss: 19954.007812\n",
      "Train Epoch: 40 [120000/225000 (53%)] Loss: 20226.796875\n",
      "Train Epoch: 40 [122496/225000 (54%)] Loss: 20597.648438\n",
      "Train Epoch: 40 [124992/225000 (56%)] Loss: 20353.089844\n",
      "Train Epoch: 40 [127488/225000 (57%)] Loss: 19950.111328\n",
      "Train Epoch: 40 [129984/225000 (58%)] Loss: 20139.699219\n",
      "Train Epoch: 40 [132480/225000 (59%)] Loss: 20017.101562\n",
      "Train Epoch: 40 [134976/225000 (60%)] Loss: 20444.066406\n",
      "Train Epoch: 40 [137472/225000 (61%)] Loss: 19965.113281\n",
      "Train Epoch: 40 [139968/225000 (62%)] Loss: 20406.742188\n",
      "Train Epoch: 40 [142464/225000 (63%)] Loss: 20164.066406\n",
      "Train Epoch: 40 [144960/225000 (64%)] Loss: 20403.734375\n",
      "Train Epoch: 40 [147456/225000 (66%)] Loss: 20878.388672\n",
      "Train Epoch: 40 [149952/225000 (67%)] Loss: 19772.843750\n",
      "Train Epoch: 40 [152448/225000 (68%)] Loss: 20222.816406\n",
      "Train Epoch: 40 [154944/225000 (69%)] Loss: 20329.765625\n",
      "Train Epoch: 40 [157440/225000 (70%)] Loss: 20527.294922\n",
      "Train Epoch: 40 [159936/225000 (71%)] Loss: 20385.791016\n",
      "Train Epoch: 40 [162432/225000 (72%)] Loss: 21161.863281\n",
      "Train Epoch: 40 [164928/225000 (73%)] Loss: 20018.146484\n",
      "Train Epoch: 40 [167424/225000 (74%)] Loss: 20141.423828\n",
      "Train Epoch: 40 [169920/225000 (76%)] Loss: 20695.748047\n",
      "Train Epoch: 40 [172416/225000 (77%)] Loss: 20167.441406\n",
      "Train Epoch: 40 [174912/225000 (78%)] Loss: 20067.941406\n",
      "Train Epoch: 40 [177408/225000 (79%)] Loss: 20185.988281\n",
      "Train Epoch: 40 [179904/225000 (80%)] Loss: 20312.164062\n",
      "Train Epoch: 40 [182400/225000 (81%)] Loss: 20032.214844\n",
      "Train Epoch: 40 [184896/225000 (82%)] Loss: 20254.966797\n",
      "Train Epoch: 40 [187392/225000 (83%)] Loss: 20158.792969\n",
      "Train Epoch: 40 [189888/225000 (84%)] Loss: 20102.947266\n",
      "Train Epoch: 40 [192384/225000 (86%)] Loss: 20317.880859\n",
      "Train Epoch: 40 [194880/225000 (87%)] Loss: 20082.289062\n",
      "Train Epoch: 40 [197376/225000 (88%)] Loss: 19910.201172\n",
      "Train Epoch: 40 [199872/225000 (89%)] Loss: 20218.955078\n",
      "Train Epoch: 40 [202368/225000 (90%)] Loss: 20252.166016\n",
      "Train Epoch: 40 [204864/225000 (91%)] Loss: 20344.808594\n",
      "Train Epoch: 40 [207360/225000 (92%)] Loss: 19830.320312\n",
      "Train Epoch: 40 [209856/225000 (93%)] Loss: 20348.566406\n",
      "Train Epoch: 40 [212352/225000 (94%)] Loss: 20726.841797\n",
      "Train Epoch: 40 [214848/225000 (95%)] Loss: 20345.523438\n",
      "Train Epoch: 40 [217344/225000 (97%)] Loss: 20102.445312\n",
      "Train Epoch: 40 [219840/225000 (98%)] Loss: 19878.156250\n",
      "Train Epoch: 40 [222336/225000 (99%)] Loss: 20048.832031\n",
      "Train Epoch: 40 [224832/225000 (100%)] Loss: 20259.466797\n",
      "    epoch          : 40\n",
      "    loss           : 20279.51551501173\n",
      "    val_loss       : 20183.991216839724\n",
      "Train Epoch: 41 [192/225000 (0%)] Loss: 19533.550781\n",
      "Train Epoch: 41 [2688/225000 (1%)] Loss: 20289.619141\n",
      "Train Epoch: 41 [5184/225000 (2%)] Loss: 20510.058594\n",
      "Train Epoch: 41 [7680/225000 (3%)] Loss: 20561.031250\n",
      "Train Epoch: 41 [10176/225000 (5%)] Loss: 20668.210938\n",
      "Train Epoch: 41 [12672/225000 (6%)] Loss: 20945.800781\n",
      "Train Epoch: 41 [15168/225000 (7%)] Loss: 20113.976562\n",
      "Train Epoch: 41 [17664/225000 (8%)] Loss: 19963.841797\n",
      "Train Epoch: 41 [20160/225000 (9%)] Loss: 20078.816406\n",
      "Train Epoch: 41 [22656/225000 (10%)] Loss: 20082.683594\n",
      "Train Epoch: 41 [25152/225000 (11%)] Loss: 20335.683594\n",
      "Train Epoch: 41 [27648/225000 (12%)] Loss: 20765.191406\n",
      "Train Epoch: 41 [30144/225000 (13%)] Loss: 20278.865234\n",
      "Train Epoch: 41 [32640/225000 (15%)] Loss: 20303.914062\n",
      "Train Epoch: 41 [35136/225000 (16%)] Loss: 19991.230469\n",
      "Train Epoch: 41 [37632/225000 (17%)] Loss: 20088.720703\n",
      "Train Epoch: 41 [40128/225000 (18%)] Loss: 20252.253906\n",
      "Train Epoch: 41 [42624/225000 (19%)] Loss: 20226.988281\n",
      "Train Epoch: 41 [45120/225000 (20%)] Loss: 20173.349609\n",
      "Train Epoch: 41 [47616/225000 (21%)] Loss: 19720.724609\n",
      "Train Epoch: 41 [50112/225000 (22%)] Loss: 20365.972656\n",
      "Train Epoch: 41 [52608/225000 (23%)] Loss: 20349.746094\n",
      "Train Epoch: 41 [55104/225000 (24%)] Loss: 20792.136719\n",
      "Train Epoch: 41 [57600/225000 (26%)] Loss: 20396.578125\n",
      "Train Epoch: 41 [60096/225000 (27%)] Loss: 19771.023438\n",
      "Train Epoch: 41 [62592/225000 (28%)] Loss: 19734.697266\n",
      "Train Epoch: 41 [65088/225000 (29%)] Loss: 20656.218750\n",
      "Train Epoch: 41 [67584/225000 (30%)] Loss: 20633.701172\n",
      "Train Epoch: 41 [70080/225000 (31%)] Loss: 20279.273438\n",
      "Train Epoch: 41 [72576/225000 (32%)] Loss: 19737.765625\n",
      "Train Epoch: 41 [75072/225000 (33%)] Loss: 20308.886719\n",
      "Train Epoch: 41 [77568/225000 (34%)] Loss: 20029.414062\n",
      "Train Epoch: 41 [80064/225000 (36%)] Loss: 20642.435547\n",
      "Train Epoch: 41 [82560/225000 (37%)] Loss: 19865.244141\n",
      "Train Epoch: 41 [85056/225000 (38%)] Loss: 20410.675781\n",
      "Train Epoch: 41 [87552/225000 (39%)] Loss: 20361.589844\n",
      "Train Epoch: 41 [90048/225000 (40%)] Loss: 20176.316406\n",
      "Train Epoch: 41 [92544/225000 (41%)] Loss: 20615.871094\n",
      "Train Epoch: 41 [95040/225000 (42%)] Loss: 20479.746094\n",
      "Train Epoch: 41 [97536/225000 (43%)] Loss: 20371.222656\n",
      "Train Epoch: 41 [100032/225000 (44%)] Loss: 20189.939453\n",
      "Train Epoch: 41 [102528/225000 (46%)] Loss: 20404.128906\n",
      "Train Epoch: 41 [105024/225000 (47%)] Loss: 20103.015625\n",
      "Train Epoch: 41 [107520/225000 (48%)] Loss: 20157.115234\n",
      "Train Epoch: 41 [110016/225000 (49%)] Loss: 20494.914062\n",
      "Train Epoch: 41 [112512/225000 (50%)] Loss: 20337.488281\n",
      "Train Epoch: 41 [115008/225000 (51%)] Loss: 20333.974609\n",
      "Train Epoch: 41 [117504/225000 (52%)] Loss: 19834.832031\n",
      "Train Epoch: 41 [120000/225000 (53%)] Loss: 20134.738281\n",
      "Train Epoch: 41 [122496/225000 (54%)] Loss: 20078.964844\n",
      "Train Epoch: 41 [124992/225000 (56%)] Loss: 20724.539062\n",
      "Train Epoch: 41 [127488/225000 (57%)] Loss: 20097.558594\n",
      "Train Epoch: 41 [129984/225000 (58%)] Loss: 20070.710938\n",
      "Train Epoch: 41 [132480/225000 (59%)] Loss: 19818.992188\n",
      "Train Epoch: 41 [134976/225000 (60%)] Loss: 20503.562500\n",
      "Train Epoch: 41 [137472/225000 (61%)] Loss: 20217.253906\n",
      "Train Epoch: 41 [139968/225000 (62%)] Loss: 19962.972656\n",
      "Train Epoch: 41 [142464/225000 (63%)] Loss: 20334.011719\n",
      "Train Epoch: 41 [144960/225000 (64%)] Loss: 20289.701172\n",
      "Train Epoch: 41 [147456/225000 (66%)] Loss: 20188.392578\n",
      "Train Epoch: 41 [149952/225000 (67%)] Loss: 20365.876953\n",
      "Train Epoch: 41 [152448/225000 (68%)] Loss: 19956.585938\n",
      "Train Epoch: 41 [154944/225000 (69%)] Loss: 19910.074219\n",
      "Train Epoch: 41 [157440/225000 (70%)] Loss: 19937.402344\n",
      "Train Epoch: 41 [159936/225000 (71%)] Loss: 20595.691406\n",
      "Train Epoch: 41 [162432/225000 (72%)] Loss: 20266.910156\n",
      "Train Epoch: 41 [164928/225000 (73%)] Loss: 20664.726562\n",
      "Train Epoch: 41 [167424/225000 (74%)] Loss: 20277.023438\n",
      "Train Epoch: 41 [169920/225000 (76%)] Loss: 20841.968750\n",
      "Train Epoch: 41 [172416/225000 (77%)] Loss: 20435.414062\n",
      "Train Epoch: 41 [174912/225000 (78%)] Loss: 20318.250000\n",
      "Train Epoch: 41 [177408/225000 (79%)] Loss: 20397.173828\n",
      "Train Epoch: 41 [179904/225000 (80%)] Loss: 19863.183594\n",
      "Train Epoch: 41 [182400/225000 (81%)] Loss: 20445.265625\n",
      "Train Epoch: 41 [184896/225000 (82%)] Loss: 19819.511719\n",
      "Train Epoch: 41 [187392/225000 (83%)] Loss: 20267.207031\n",
      "Train Epoch: 41 [189888/225000 (84%)] Loss: 19876.076172\n",
      "Train Epoch: 41 [192384/225000 (86%)] Loss: 20014.046875\n",
      "Train Epoch: 41 [194880/225000 (87%)] Loss: 20070.414062\n",
      "Train Epoch: 41 [197376/225000 (88%)] Loss: 20380.769531\n",
      "Train Epoch: 41 [199872/225000 (89%)] Loss: 20316.726562\n",
      "Train Epoch: 41 [202368/225000 (90%)] Loss: 20246.103516\n",
      "Train Epoch: 41 [204864/225000 (91%)] Loss: 20503.007812\n",
      "Train Epoch: 41 [207360/225000 (92%)] Loss: 20248.613281\n",
      "Train Epoch: 41 [209856/225000 (93%)] Loss: 20288.998047\n",
      "Train Epoch: 41 [212352/225000 (94%)] Loss: 20019.900391\n",
      "Train Epoch: 41 [214848/225000 (95%)] Loss: 20741.289062\n",
      "Train Epoch: 41 [217344/225000 (97%)] Loss: 20587.523438\n",
      "Train Epoch: 41 [219840/225000 (98%)] Loss: 20135.292969\n",
      "Train Epoch: 41 [222336/225000 (99%)] Loss: 20102.007812\n",
      "Train Epoch: 41 [224832/225000 (100%)] Loss: 20416.300781\n",
      "    epoch          : 41\n",
      "    loss           : 20276.080719723228\n",
      "    val_loss       : 20172.98808616764\n",
      "Train Epoch: 42 [192/225000 (0%)] Loss: 20180.837891\n",
      "Train Epoch: 42 [2688/225000 (1%)] Loss: 20664.298828\n",
      "Train Epoch: 42 [5184/225000 (2%)] Loss: 20620.875000\n",
      "Train Epoch: 42 [7680/225000 (3%)] Loss: 20525.468750\n",
      "Train Epoch: 42 [10176/225000 (5%)] Loss: 20226.478516\n",
      "Train Epoch: 42 [12672/225000 (6%)] Loss: 20480.757812\n",
      "Train Epoch: 42 [15168/225000 (7%)] Loss: 19735.064453\n",
      "Train Epoch: 42 [17664/225000 (8%)] Loss: 20599.527344\n",
      "Train Epoch: 42 [20160/225000 (9%)] Loss: 20565.546875\n",
      "Train Epoch: 42 [22656/225000 (10%)] Loss: 20039.742188\n",
      "Train Epoch: 42 [25152/225000 (11%)] Loss: 19960.500000\n",
      "Train Epoch: 42 [27648/225000 (12%)] Loss: 20127.460938\n",
      "Train Epoch: 42 [30144/225000 (13%)] Loss: 19946.994141\n",
      "Train Epoch: 42 [32640/225000 (15%)] Loss: 20333.800781\n",
      "Train Epoch: 42 [35136/225000 (16%)] Loss: 20012.578125\n",
      "Train Epoch: 42 [37632/225000 (17%)] Loss: 20023.886719\n",
      "Train Epoch: 42 [40128/225000 (18%)] Loss: 20526.738281\n",
      "Train Epoch: 42 [42624/225000 (19%)] Loss: 20497.523438\n",
      "Train Epoch: 42 [45120/225000 (20%)] Loss: 19707.718750\n",
      "Train Epoch: 42 [47616/225000 (21%)] Loss: 20320.070312\n",
      "Train Epoch: 42 [50112/225000 (22%)] Loss: 19959.359375\n",
      "Train Epoch: 42 [52608/225000 (23%)] Loss: 20248.675781\n",
      "Train Epoch: 42 [55104/225000 (24%)] Loss: 20115.589844\n",
      "Train Epoch: 42 [57600/225000 (26%)] Loss: 20184.648438\n",
      "Train Epoch: 42 [60096/225000 (27%)] Loss: 20320.970703\n",
      "Train Epoch: 42 [62592/225000 (28%)] Loss: 20579.636719\n",
      "Train Epoch: 42 [65088/225000 (29%)] Loss: 20086.531250\n",
      "Train Epoch: 42 [67584/225000 (30%)] Loss: 20550.480469\n",
      "Train Epoch: 42 [70080/225000 (31%)] Loss: 20511.210938\n",
      "Train Epoch: 42 [72576/225000 (32%)] Loss: 20242.390625\n",
      "Train Epoch: 42 [75072/225000 (33%)] Loss: 20326.837891\n",
      "Train Epoch: 42 [77568/225000 (34%)] Loss: 20294.785156\n",
      "Train Epoch: 42 [80064/225000 (36%)] Loss: 19649.972656\n",
      "Train Epoch: 42 [82560/225000 (37%)] Loss: 19718.851562\n",
      "Train Epoch: 42 [85056/225000 (38%)] Loss: 20215.421875\n",
      "Train Epoch: 42 [87552/225000 (39%)] Loss: 20243.804688\n",
      "Train Epoch: 42 [90048/225000 (40%)] Loss: 20359.394531\n",
      "Train Epoch: 42 [92544/225000 (41%)] Loss: 19911.185547\n",
      "Train Epoch: 42 [95040/225000 (42%)] Loss: 19879.308594\n",
      "Train Epoch: 42 [97536/225000 (43%)] Loss: 20268.339844\n",
      "Train Epoch: 42 [100032/225000 (44%)] Loss: 20838.484375\n",
      "Train Epoch: 42 [102528/225000 (46%)] Loss: 20720.197266\n",
      "Train Epoch: 42 [105024/225000 (47%)] Loss: 19858.226562\n",
      "Train Epoch: 42 [107520/225000 (48%)] Loss: 20087.320312\n",
      "Train Epoch: 42 [110016/225000 (49%)] Loss: 20039.863281\n",
      "Train Epoch: 42 [112512/225000 (50%)] Loss: 19928.160156\n",
      "Train Epoch: 42 [115008/225000 (51%)] Loss: 20277.242188\n",
      "Train Epoch: 42 [117504/225000 (52%)] Loss: 20191.574219\n",
      "Train Epoch: 42 [120000/225000 (53%)] Loss: 20247.367188\n",
      "Train Epoch: 42 [122496/225000 (54%)] Loss: 20462.406250\n",
      "Train Epoch: 42 [124992/225000 (56%)] Loss: 20323.691406\n",
      "Train Epoch: 42 [127488/225000 (57%)] Loss: 19912.468750\n",
      "Train Epoch: 42 [129984/225000 (58%)] Loss: 20154.417969\n",
      "Train Epoch: 42 [132480/225000 (59%)] Loss: 20175.398438\n",
      "Train Epoch: 42 [134976/225000 (60%)] Loss: 20217.468750\n",
      "Train Epoch: 42 [137472/225000 (61%)] Loss: 20167.445312\n",
      "Train Epoch: 42 [139968/225000 (62%)] Loss: 20317.542969\n",
      "Train Epoch: 42 [142464/225000 (63%)] Loss: 20231.472656\n",
      "Train Epoch: 42 [144960/225000 (64%)] Loss: 20000.152344\n",
      "Train Epoch: 42 [147456/225000 (66%)] Loss: 20659.550781\n",
      "Train Epoch: 42 [149952/225000 (67%)] Loss: 20131.062500\n",
      "Train Epoch: 42 [152448/225000 (68%)] Loss: 20550.824219\n",
      "Train Epoch: 42 [154944/225000 (69%)] Loss: 20312.214844\n",
      "Train Epoch: 42 [157440/225000 (70%)] Loss: 20011.408203\n",
      "Train Epoch: 42 [159936/225000 (71%)] Loss: 20126.242188\n",
      "Train Epoch: 42 [162432/225000 (72%)] Loss: 20482.503906\n",
      "Train Epoch: 42 [164928/225000 (73%)] Loss: 20121.515625\n",
      "Train Epoch: 42 [167424/225000 (74%)] Loss: 20116.535156\n",
      "Train Epoch: 42 [169920/225000 (76%)] Loss: 20229.691406\n",
      "Train Epoch: 42 [172416/225000 (77%)] Loss: 20094.367188\n",
      "Train Epoch: 42 [174912/225000 (78%)] Loss: 20153.894531\n",
      "Train Epoch: 42 [177408/225000 (79%)] Loss: 20414.242188\n",
      "Train Epoch: 42 [179904/225000 (80%)] Loss: 20088.068359\n",
      "Train Epoch: 42 [182400/225000 (81%)] Loss: 20281.195312\n",
      "Train Epoch: 42 [184896/225000 (82%)] Loss: 20218.679688\n",
      "Train Epoch: 42 [187392/225000 (83%)] Loss: 20503.425781\n",
      "Train Epoch: 42 [189888/225000 (84%)] Loss: 20123.238281\n",
      "Train Epoch: 42 [192384/225000 (86%)] Loss: 20246.091797\n",
      "Train Epoch: 42 [194880/225000 (87%)] Loss: 20284.285156\n",
      "Train Epoch: 42 [197376/225000 (88%)] Loss: 20115.583984\n",
      "Train Epoch: 42 [199872/225000 (89%)] Loss: 19784.941406\n",
      "Train Epoch: 42 [202368/225000 (90%)] Loss: 20427.589844\n",
      "Train Epoch: 42 [204864/225000 (91%)] Loss: 20754.468750\n",
      "Train Epoch: 42 [207360/225000 (92%)] Loss: 20058.285156\n",
      "Train Epoch: 42 [209856/225000 (93%)] Loss: 19832.876953\n",
      "Train Epoch: 42 [212352/225000 (94%)] Loss: 19876.121094\n",
      "Train Epoch: 42 [214848/225000 (95%)] Loss: 20153.304688\n",
      "Train Epoch: 42 [217344/225000 (97%)] Loss: 20229.892578\n",
      "Train Epoch: 42 [219840/225000 (98%)] Loss: 20638.634766\n",
      "Train Epoch: 42 [222336/225000 (99%)] Loss: 20605.279297\n",
      "Train Epoch: 42 [224832/225000 (100%)] Loss: 20411.197266\n",
      "    epoch          : 42\n",
      "    loss           : 20278.46840170382\n",
      "    val_loss       : 20168.489492000514\n",
      "Train Epoch: 43 [192/225000 (0%)] Loss: 19701.257812\n",
      "Train Epoch: 43 [2688/225000 (1%)] Loss: 20716.281250\n",
      "Train Epoch: 43 [5184/225000 (2%)] Loss: 20327.582031\n",
      "Train Epoch: 43 [7680/225000 (3%)] Loss: 19882.800781\n",
      "Train Epoch: 43 [10176/225000 (5%)] Loss: 20384.390625\n",
      "Train Epoch: 43 [12672/225000 (6%)] Loss: 20656.414062\n",
      "Train Epoch: 43 [15168/225000 (7%)] Loss: 20083.390625\n",
      "Train Epoch: 43 [17664/225000 (8%)] Loss: 20213.953125\n",
      "Train Epoch: 43 [20160/225000 (9%)] Loss: 19847.472656\n",
      "Train Epoch: 43 [22656/225000 (10%)] Loss: 20063.652344\n",
      "Train Epoch: 43 [25152/225000 (11%)] Loss: 20292.015625\n",
      "Train Epoch: 43 [27648/225000 (12%)] Loss: 19785.906250\n",
      "Train Epoch: 43 [30144/225000 (13%)] Loss: 19941.708984\n",
      "Train Epoch: 43 [32640/225000 (15%)] Loss: 20221.003906\n",
      "Train Epoch: 43 [35136/225000 (16%)] Loss: 20222.589844\n",
      "Train Epoch: 43 [37632/225000 (17%)] Loss: 20483.039062\n",
      "Train Epoch: 43 [40128/225000 (18%)] Loss: 20072.021484\n",
      "Train Epoch: 43 [42624/225000 (19%)] Loss: 20249.373047\n",
      "Train Epoch: 43 [45120/225000 (20%)] Loss: 20187.236328\n",
      "Train Epoch: 43 [47616/225000 (21%)] Loss: 20551.828125\n",
      "Train Epoch: 43 [50112/225000 (22%)] Loss: 20151.113281\n",
      "Train Epoch: 43 [52608/225000 (23%)] Loss: 20196.046875\n",
      "Train Epoch: 43 [55104/225000 (24%)] Loss: 20146.179688\n",
      "Train Epoch: 43 [57600/225000 (26%)] Loss: 19869.607422\n",
      "Train Epoch: 43 [60096/225000 (27%)] Loss: 20459.609375\n",
      "Train Epoch: 43 [62592/225000 (28%)] Loss: 20420.935547\n",
      "Train Epoch: 43 [65088/225000 (29%)] Loss: 20454.449219\n",
      "Train Epoch: 43 [67584/225000 (30%)] Loss: 20072.828125\n",
      "Train Epoch: 43 [70080/225000 (31%)] Loss: 20290.857422\n",
      "Train Epoch: 43 [72576/225000 (32%)] Loss: 20529.146484\n",
      "Train Epoch: 43 [75072/225000 (33%)] Loss: 20665.015625\n",
      "Train Epoch: 43 [77568/225000 (34%)] Loss: 19711.273438\n",
      "Train Epoch: 43 [80064/225000 (36%)] Loss: 20009.820312\n",
      "Train Epoch: 43 [82560/225000 (37%)] Loss: 20393.320312\n",
      "Train Epoch: 43 [85056/225000 (38%)] Loss: 20177.074219\n",
      "Train Epoch: 43 [87552/225000 (39%)] Loss: 20403.539062\n",
      "Train Epoch: 43 [90048/225000 (40%)] Loss: 20739.148438\n",
      "Train Epoch: 43 [92544/225000 (41%)] Loss: 19987.269531\n",
      "Train Epoch: 43 [95040/225000 (42%)] Loss: 20283.441406\n",
      "Train Epoch: 43 [97536/225000 (43%)] Loss: 20201.843750\n",
      "Train Epoch: 43 [100032/225000 (44%)] Loss: 20161.539062\n",
      "Train Epoch: 43 [102528/225000 (46%)] Loss: 20439.882812\n",
      "Train Epoch: 43 [105024/225000 (47%)] Loss: 20046.132812\n",
      "Train Epoch: 43 [107520/225000 (48%)] Loss: 20122.482422\n",
      "Train Epoch: 43 [110016/225000 (49%)] Loss: 21063.041016\n",
      "Train Epoch: 43 [112512/225000 (50%)] Loss: 20128.882812\n",
      "Train Epoch: 43 [115008/225000 (51%)] Loss: 20457.273438\n",
      "Train Epoch: 43 [117504/225000 (52%)] Loss: 20024.355469\n",
      "Train Epoch: 43 [120000/225000 (53%)] Loss: 19950.484375\n",
      "Train Epoch: 43 [122496/225000 (54%)] Loss: 20544.027344\n",
      "Train Epoch: 43 [124992/225000 (56%)] Loss: 20440.789062\n",
      "Train Epoch: 43 [127488/225000 (57%)] Loss: 20565.074219\n",
      "Train Epoch: 43 [129984/225000 (58%)] Loss: 20249.968750\n",
      "Train Epoch: 43 [132480/225000 (59%)] Loss: 20395.011719\n",
      "Train Epoch: 43 [134976/225000 (60%)] Loss: 20031.621094\n",
      "Train Epoch: 43 [137472/225000 (61%)] Loss: 20737.300781\n",
      "Train Epoch: 43 [139968/225000 (62%)] Loss: 19915.908203\n",
      "Train Epoch: 43 [142464/225000 (63%)] Loss: 20756.046875\n",
      "Train Epoch: 43 [144960/225000 (64%)] Loss: 20673.554688\n",
      "Train Epoch: 43 [147456/225000 (66%)] Loss: 20634.800781\n",
      "Train Epoch: 43 [149952/225000 (67%)] Loss: 20370.363281\n",
      "Train Epoch: 43 [152448/225000 (68%)] Loss: 20517.515625\n",
      "Train Epoch: 43 [154944/225000 (69%)] Loss: 20402.421875\n",
      "Train Epoch: 43 [157440/225000 (70%)] Loss: 20407.011719\n",
      "Train Epoch: 43 [159936/225000 (71%)] Loss: 20479.656250\n",
      "Train Epoch: 43 [162432/225000 (72%)] Loss: 20271.685547\n",
      "Train Epoch: 43 [164928/225000 (73%)] Loss: 20551.636719\n",
      "Train Epoch: 43 [167424/225000 (74%)] Loss: 20288.851562\n",
      "Train Epoch: 43 [169920/225000 (76%)] Loss: 19959.687500\n",
      "Train Epoch: 43 [172416/225000 (77%)] Loss: 20607.109375\n",
      "Train Epoch: 43 [174912/225000 (78%)] Loss: 20397.281250\n",
      "Train Epoch: 43 [177408/225000 (79%)] Loss: 19946.361328\n",
      "Train Epoch: 43 [179904/225000 (80%)] Loss: 20144.355469\n",
      "Train Epoch: 43 [182400/225000 (81%)] Loss: 20161.470703\n",
      "Train Epoch: 43 [184896/225000 (82%)] Loss: 20733.988281\n",
      "Train Epoch: 43 [187392/225000 (83%)] Loss: 19942.875000\n",
      "Train Epoch: 43 [189888/225000 (84%)] Loss: 19976.812500\n",
      "Train Epoch: 43 [192384/225000 (86%)] Loss: 20004.976562\n",
      "Train Epoch: 43 [194880/225000 (87%)] Loss: 20824.697266\n",
      "Train Epoch: 43 [197376/225000 (88%)] Loss: 20130.388672\n",
      "Train Epoch: 43 [199872/225000 (89%)] Loss: 20381.285156\n",
      "Train Epoch: 43 [202368/225000 (90%)] Loss: 20616.054688\n",
      "Train Epoch: 43 [204864/225000 (91%)] Loss: 20241.218750\n",
      "Train Epoch: 43 [207360/225000 (92%)] Loss: 20290.480469\n",
      "Train Epoch: 43 [209856/225000 (93%)] Loss: 20637.863281\n",
      "Train Epoch: 43 [212352/225000 (94%)] Loss: 20173.496094\n",
      "Train Epoch: 43 [214848/225000 (95%)] Loss: 20311.728516\n",
      "Train Epoch: 43 [217344/225000 (97%)] Loss: 20439.832031\n",
      "Train Epoch: 43 [219840/225000 (98%)] Loss: 20153.480469\n",
      "Train Epoch: 43 [222336/225000 (99%)] Loss: 20140.992188\n",
      "Train Epoch: 43 [224832/225000 (100%)] Loss: 20883.222656\n",
      "    epoch          : 43\n",
      "    loss           : 20268.752156436647\n",
      "    val_loss       : 20169.36233079206\n",
      "Train Epoch: 44 [192/225000 (0%)] Loss: 20377.615234\n",
      "Train Epoch: 44 [2688/225000 (1%)] Loss: 20446.851562\n",
      "Train Epoch: 44 [5184/225000 (2%)] Loss: 20347.968750\n",
      "Train Epoch: 44 [7680/225000 (3%)] Loss: 20521.949219\n",
      "Train Epoch: 44 [10176/225000 (5%)] Loss: 20753.603516\n",
      "Train Epoch: 44 [12672/225000 (6%)] Loss: 20546.501953\n",
      "Train Epoch: 44 [15168/225000 (7%)] Loss: 20015.265625\n",
      "Train Epoch: 44 [17664/225000 (8%)] Loss: 19941.539062\n",
      "Train Epoch: 44 [20160/225000 (9%)] Loss: 20024.849609\n",
      "Train Epoch: 44 [22656/225000 (10%)] Loss: 20053.281250\n",
      "Train Epoch: 44 [25152/225000 (11%)] Loss: 20393.105469\n",
      "Train Epoch: 44 [27648/225000 (12%)] Loss: 20265.564453\n",
      "Train Epoch: 44 [30144/225000 (13%)] Loss: 20219.068359\n",
      "Train Epoch: 44 [32640/225000 (15%)] Loss: 20798.818359\n",
      "Train Epoch: 44 [35136/225000 (16%)] Loss: 20047.544922\n",
      "Train Epoch: 44 [37632/225000 (17%)] Loss: 20178.191406\n",
      "Train Epoch: 44 [40128/225000 (18%)] Loss: 20254.837891\n",
      "Train Epoch: 44 [42624/225000 (19%)] Loss: 20668.500000\n",
      "Train Epoch: 44 [45120/225000 (20%)] Loss: 19940.392578\n",
      "Train Epoch: 44 [47616/225000 (21%)] Loss: 19801.093750\n",
      "Train Epoch: 44 [50112/225000 (22%)] Loss: 19971.931641\n",
      "Train Epoch: 44 [52608/225000 (23%)] Loss: 20176.234375\n",
      "Train Epoch: 44 [55104/225000 (24%)] Loss: 20331.187500\n",
      "Train Epoch: 44 [57600/225000 (26%)] Loss: 20734.443359\n",
      "Train Epoch: 44 [60096/225000 (27%)] Loss: 19870.121094\n",
      "Train Epoch: 44 [62592/225000 (28%)] Loss: 20460.468750\n",
      "Train Epoch: 44 [65088/225000 (29%)] Loss: 20177.078125\n",
      "Train Epoch: 44 [67584/225000 (30%)] Loss: 20335.078125\n",
      "Train Epoch: 44 [70080/225000 (31%)] Loss: 20365.691406\n",
      "Train Epoch: 44 [72576/225000 (32%)] Loss: 20522.945312\n",
      "Train Epoch: 44 [75072/225000 (33%)] Loss: 19969.671875\n",
      "Train Epoch: 44 [77568/225000 (34%)] Loss: 19767.009766\n",
      "Train Epoch: 44 [80064/225000 (36%)] Loss: 20387.324219\n",
      "Train Epoch: 44 [82560/225000 (37%)] Loss: 20114.062500\n",
      "Train Epoch: 44 [85056/225000 (38%)] Loss: 20356.078125\n",
      "Train Epoch: 44 [87552/225000 (39%)] Loss: 20264.367188\n",
      "Train Epoch: 44 [90048/225000 (40%)] Loss: 20243.925781\n",
      "Train Epoch: 44 [92544/225000 (41%)] Loss: 20198.289062\n",
      "Train Epoch: 44 [95040/225000 (42%)] Loss: 19993.425781\n",
      "Train Epoch: 44 [97536/225000 (43%)] Loss: 19911.191406\n",
      "Train Epoch: 44 [100032/225000 (44%)] Loss: 19899.203125\n",
      "Train Epoch: 44 [102528/225000 (46%)] Loss: 20217.851562\n",
      "Train Epoch: 44 [105024/225000 (47%)] Loss: 20220.125000\n",
      "Train Epoch: 44 [107520/225000 (48%)] Loss: 20205.574219\n",
      "Train Epoch: 44 [110016/225000 (49%)] Loss: 20226.156250\n",
      "Train Epoch: 44 [112512/225000 (50%)] Loss: 20439.103516\n",
      "Train Epoch: 44 [115008/225000 (51%)] Loss: 19943.242188\n",
      "Train Epoch: 44 [117504/225000 (52%)] Loss: 20096.957031\n",
      "Train Epoch: 44 [120000/225000 (53%)] Loss: 19931.628906\n",
      "Train Epoch: 44 [122496/225000 (54%)] Loss: 20254.632812\n",
      "Train Epoch: 44 [124992/225000 (56%)] Loss: 20434.746094\n",
      "Train Epoch: 44 [127488/225000 (57%)] Loss: 20602.130859\n",
      "Train Epoch: 44 [129984/225000 (58%)] Loss: 20397.751953\n",
      "Train Epoch: 44 [132480/225000 (59%)] Loss: 20378.361328\n",
      "Train Epoch: 44 [134976/225000 (60%)] Loss: 20343.292969\n",
      "Train Epoch: 44 [137472/225000 (61%)] Loss: 20182.101562\n",
      "Train Epoch: 44 [139968/225000 (62%)] Loss: 20117.128906\n",
      "Train Epoch: 44 [142464/225000 (63%)] Loss: 20021.449219\n",
      "Train Epoch: 44 [144960/225000 (64%)] Loss: 20164.457031\n",
      "Train Epoch: 44 [147456/225000 (66%)] Loss: 20340.933594\n",
      "Train Epoch: 44 [149952/225000 (67%)] Loss: 20427.691406\n",
      "Train Epoch: 44 [152448/225000 (68%)] Loss: 20217.654297\n",
      "Train Epoch: 44 [154944/225000 (69%)] Loss: 20158.773438\n",
      "Train Epoch: 44 [157440/225000 (70%)] Loss: 20668.726562\n",
      "Train Epoch: 44 [159936/225000 (71%)] Loss: 20636.603516\n",
      "Train Epoch: 44 [162432/225000 (72%)] Loss: 19789.000000\n",
      "Train Epoch: 44 [164928/225000 (73%)] Loss: 20252.750000\n",
      "Train Epoch: 44 [167424/225000 (74%)] Loss: 20549.062500\n",
      "Train Epoch: 44 [169920/225000 (76%)] Loss: 20543.976562\n",
      "Train Epoch: 44 [172416/225000 (77%)] Loss: 20098.384766\n",
      "Train Epoch: 44 [174912/225000 (78%)] Loss: 20305.189453\n",
      "Train Epoch: 44 [177408/225000 (79%)] Loss: 20103.267578\n",
      "Train Epoch: 44 [179904/225000 (80%)] Loss: 20166.324219\n",
      "Train Epoch: 44 [182400/225000 (81%)] Loss: 20389.753906\n",
      "Train Epoch: 44 [184896/225000 (82%)] Loss: 20567.177734\n",
      "Train Epoch: 44 [187392/225000 (83%)] Loss: 20280.744141\n",
      "Train Epoch: 44 [189888/225000 (84%)] Loss: 20224.148438\n",
      "Train Epoch: 44 [192384/225000 (86%)] Loss: 20387.722656\n",
      "Train Epoch: 44 [194880/225000 (87%)] Loss: 20464.953125\n",
      "Train Epoch: 44 [197376/225000 (88%)] Loss: 20334.496094\n",
      "Train Epoch: 44 [199872/225000 (89%)] Loss: 20254.281250\n",
      "Train Epoch: 44 [202368/225000 (90%)] Loss: 20373.783203\n",
      "Train Epoch: 44 [204864/225000 (91%)] Loss: 20307.968750\n",
      "Train Epoch: 44 [207360/225000 (92%)] Loss: 20705.164062\n",
      "Train Epoch: 44 [209856/225000 (93%)] Loss: 20273.945312\n",
      "Train Epoch: 44 [212352/225000 (94%)] Loss: 20184.480469\n",
      "Train Epoch: 44 [214848/225000 (95%)] Loss: 20624.800781\n",
      "Train Epoch: 44 [217344/225000 (97%)] Loss: 20398.390625\n",
      "Train Epoch: 44 [219840/225000 (98%)] Loss: 20396.898438\n",
      "Train Epoch: 44 [222336/225000 (99%)] Loss: 20047.699219\n",
      "Train Epoch: 44 [224832/225000 (100%)] Loss: 20811.386719\n",
      "    epoch          : 44\n",
      "    loss           : 20264.180345763118\n",
      "    val_loss       : 20159.63170656266\n",
      "Train Epoch: 45 [192/225000 (0%)] Loss: 20197.175781\n",
      "Train Epoch: 45 [2688/225000 (1%)] Loss: 20115.013672\n",
      "Train Epoch: 45 [5184/225000 (2%)] Loss: 20315.908203\n",
      "Train Epoch: 45 [7680/225000 (3%)] Loss: 20080.023438\n",
      "Train Epoch: 45 [10176/225000 (5%)] Loss: 20570.546875\n",
      "Train Epoch: 45 [12672/225000 (6%)] Loss: 20086.238281\n",
      "Train Epoch: 45 [15168/225000 (7%)] Loss: 20668.554688\n",
      "Train Epoch: 45 [17664/225000 (8%)] Loss: 20425.460938\n",
      "Train Epoch: 45 [20160/225000 (9%)] Loss: 20174.619141\n",
      "Train Epoch: 45 [22656/225000 (10%)] Loss: 20185.937500\n",
      "Train Epoch: 45 [25152/225000 (11%)] Loss: 20077.746094\n",
      "Train Epoch: 45 [27648/225000 (12%)] Loss: 20172.552734\n",
      "Train Epoch: 45 [30144/225000 (13%)] Loss: 20414.113281\n",
      "Train Epoch: 45 [32640/225000 (15%)] Loss: 20634.462891\n",
      "Train Epoch: 45 [35136/225000 (16%)] Loss: 20301.945312\n",
      "Train Epoch: 45 [37632/225000 (17%)] Loss: 20080.666016\n",
      "Train Epoch: 45 [40128/225000 (18%)] Loss: 20144.484375\n",
      "Train Epoch: 45 [42624/225000 (19%)] Loss: 19860.003906\n",
      "Train Epoch: 45 [45120/225000 (20%)] Loss: 20195.027344\n",
      "Train Epoch: 45 [47616/225000 (21%)] Loss: 20420.222656\n",
      "Train Epoch: 45 [50112/225000 (22%)] Loss: 20036.439453\n",
      "Train Epoch: 45 [52608/225000 (23%)] Loss: 20384.845703\n",
      "Train Epoch: 45 [55104/225000 (24%)] Loss: 20015.007812\n",
      "Train Epoch: 45 [57600/225000 (26%)] Loss: 19978.593750\n",
      "Train Epoch: 45 [60096/225000 (27%)] Loss: 19948.605469\n",
      "Train Epoch: 45 [62592/225000 (28%)] Loss: 20614.023438\n",
      "Train Epoch: 45 [65088/225000 (29%)] Loss: 20289.625000\n",
      "Train Epoch: 45 [67584/225000 (30%)] Loss: 19949.523438\n",
      "Train Epoch: 45 [70080/225000 (31%)] Loss: 20351.605469\n",
      "Train Epoch: 45 [72576/225000 (32%)] Loss: 19939.445312\n",
      "Train Epoch: 45 [75072/225000 (33%)] Loss: 20347.515625\n",
      "Train Epoch: 45 [77568/225000 (34%)] Loss: 20296.578125\n",
      "Train Epoch: 45 [80064/225000 (36%)] Loss: 20209.875000\n",
      "Train Epoch: 45 [82560/225000 (37%)] Loss: 20046.146484\n",
      "Train Epoch: 45 [85056/225000 (38%)] Loss: 20350.912109\n",
      "Train Epoch: 45 [87552/225000 (39%)] Loss: 20305.183594\n",
      "Train Epoch: 45 [90048/225000 (40%)] Loss: 20341.523438\n",
      "Train Epoch: 45 [92544/225000 (41%)] Loss: 20638.492188\n",
      "Train Epoch: 45 [95040/225000 (42%)] Loss: 20210.898438\n",
      "Train Epoch: 45 [97536/225000 (43%)] Loss: 20132.492188\n",
      "Train Epoch: 45 [100032/225000 (44%)] Loss: 19874.996094\n",
      "Train Epoch: 45 [102528/225000 (46%)] Loss: 20357.761719\n",
      "Train Epoch: 45 [105024/225000 (47%)] Loss: 20706.679688\n",
      "Train Epoch: 45 [107520/225000 (48%)] Loss: 20619.386719\n",
      "Train Epoch: 45 [110016/225000 (49%)] Loss: 20510.369141\n",
      "Train Epoch: 45 [112512/225000 (50%)] Loss: 20411.308594\n",
      "Train Epoch: 45 [115008/225000 (51%)] Loss: 20694.380859\n",
      "Train Epoch: 45 [117504/225000 (52%)] Loss: 20052.771484\n",
      "Train Epoch: 45 [120000/225000 (53%)] Loss: 19973.992188\n",
      "Train Epoch: 45 [122496/225000 (54%)] Loss: 20429.339844\n",
      "Train Epoch: 45 [124992/225000 (56%)] Loss: 20048.371094\n",
      "Train Epoch: 45 [127488/225000 (57%)] Loss: 20333.675781\n",
      "Train Epoch: 45 [129984/225000 (58%)] Loss: 20024.835938\n",
      "Train Epoch: 45 [132480/225000 (59%)] Loss: 20470.128906\n",
      "Train Epoch: 45 [134976/225000 (60%)] Loss: 20476.839844\n",
      "Train Epoch: 45 [137472/225000 (61%)] Loss: 20209.697266\n",
      "Train Epoch: 45 [139968/225000 (62%)] Loss: 20623.515625\n",
      "Train Epoch: 45 [142464/225000 (63%)] Loss: 20248.941406\n",
      "Train Epoch: 45 [144960/225000 (64%)] Loss: 20135.283203\n",
      "Train Epoch: 45 [147456/225000 (66%)] Loss: 20723.251953\n",
      "Train Epoch: 45 [149952/225000 (67%)] Loss: 20208.218750\n",
      "Train Epoch: 45 [152448/225000 (68%)] Loss: 20127.980469\n",
      "Train Epoch: 45 [154944/225000 (69%)] Loss: 20574.236328\n",
      "Train Epoch: 45 [157440/225000 (70%)] Loss: 19960.705078\n",
      "Train Epoch: 45 [159936/225000 (71%)] Loss: 20793.308594\n",
      "Train Epoch: 45 [162432/225000 (72%)] Loss: 20331.292969\n",
      "Train Epoch: 45 [164928/225000 (73%)] Loss: 20340.183594\n",
      "Train Epoch: 45 [167424/225000 (74%)] Loss: 35277.394531\n",
      "Train Epoch: 45 [169920/225000 (76%)] Loss: 20092.070312\n",
      "Train Epoch: 45 [172416/225000 (77%)] Loss: 20153.597656\n",
      "Train Epoch: 45 [174912/225000 (78%)] Loss: 20084.753906\n",
      "Train Epoch: 45 [177408/225000 (79%)] Loss: 20400.136719\n",
      "Train Epoch: 45 [179904/225000 (80%)] Loss: 20551.437500\n",
      "Train Epoch: 45 [182400/225000 (81%)] Loss: 20497.414062\n",
      "Train Epoch: 45 [184896/225000 (82%)] Loss: 20336.332031\n",
      "Train Epoch: 45 [187392/225000 (83%)] Loss: 20024.451172\n",
      "Train Epoch: 45 [189888/225000 (84%)] Loss: 19885.035156\n",
      "Train Epoch: 45 [192384/225000 (86%)] Loss: 20001.488281\n",
      "Train Epoch: 45 [194880/225000 (87%)] Loss: 20255.343750\n",
      "Train Epoch: 45 [197376/225000 (88%)] Loss: 20306.003906\n",
      "Train Epoch: 45 [199872/225000 (89%)] Loss: 20040.464844\n",
      "Train Epoch: 45 [202368/225000 (90%)] Loss: 20358.597656\n",
      "Train Epoch: 45 [204864/225000 (91%)] Loss: 19789.144531\n",
      "Train Epoch: 45 [207360/225000 (92%)] Loss: 19900.871094\n",
      "Train Epoch: 45 [209856/225000 (93%)] Loss: 20324.644531\n",
      "Train Epoch: 45 [212352/225000 (94%)] Loss: 19957.515625\n",
      "Train Epoch: 45 [214848/225000 (95%)] Loss: 20614.414062\n",
      "Train Epoch: 45 [217344/225000 (97%)] Loss: 19991.066406\n",
      "Train Epoch: 45 [219840/225000 (98%)] Loss: 20131.335938\n",
      "Train Epoch: 45 [222336/225000 (99%)] Loss: 20107.398438\n",
      "Train Epoch: 45 [224832/225000 (100%)] Loss: 20023.318359\n",
      "    epoch          : 45\n",
      "    loss           : 20266.07306054021\n",
      "    val_loss       : 20166.923360861896\n",
      "Train Epoch: 46 [192/225000 (0%)] Loss: 20234.050781\n",
      "Train Epoch: 46 [2688/225000 (1%)] Loss: 20410.449219\n",
      "Train Epoch: 46 [5184/225000 (2%)] Loss: 20404.199219\n",
      "Train Epoch: 46 [7680/225000 (3%)] Loss: 20546.746094\n",
      "Train Epoch: 46 [10176/225000 (5%)] Loss: 20116.046875\n",
      "Train Epoch: 46 [12672/225000 (6%)] Loss: 20661.976562\n",
      "Train Epoch: 46 [15168/225000 (7%)] Loss: 20414.287109\n",
      "Train Epoch: 46 [17664/225000 (8%)] Loss: 20087.792969\n",
      "Train Epoch: 46 [20160/225000 (9%)] Loss: 20022.378906\n",
      "Train Epoch: 46 [22656/225000 (10%)] Loss: 20380.652344\n",
      "Train Epoch: 46 [25152/225000 (11%)] Loss: 20281.736328\n",
      "Train Epoch: 46 [27648/225000 (12%)] Loss: 20120.351562\n",
      "Train Epoch: 46 [30144/225000 (13%)] Loss: 20280.195312\n",
      "Train Epoch: 46 [32640/225000 (15%)] Loss: 20157.363281\n",
      "Train Epoch: 46 [35136/225000 (16%)] Loss: 20551.792969\n",
      "Train Epoch: 46 [37632/225000 (17%)] Loss: 20389.552734\n",
      "Train Epoch: 46 [40128/225000 (18%)] Loss: 19894.367188\n",
      "Train Epoch: 46 [42624/225000 (19%)] Loss: 19923.242188\n",
      "Train Epoch: 46 [45120/225000 (20%)] Loss: 20509.777344\n",
      "Train Epoch: 46 [47616/225000 (21%)] Loss: 20668.343750\n",
      "Train Epoch: 46 [50112/225000 (22%)] Loss: 20130.226562\n",
      "Train Epoch: 46 [52608/225000 (23%)] Loss: 20259.031250\n",
      "Train Epoch: 46 [55104/225000 (24%)] Loss: 20664.175781\n",
      "Train Epoch: 46 [57600/225000 (26%)] Loss: 20413.843750\n",
      "Train Epoch: 46 [60096/225000 (27%)] Loss: 21064.166016\n",
      "Train Epoch: 46 [62592/225000 (28%)] Loss: 19951.542969\n",
      "Train Epoch: 46 [65088/225000 (29%)] Loss: 20321.382812\n",
      "Train Epoch: 46 [67584/225000 (30%)] Loss: 20661.839844\n",
      "Train Epoch: 46 [70080/225000 (31%)] Loss: 20711.820312\n",
      "Train Epoch: 46 [72576/225000 (32%)] Loss: 20033.277344\n",
      "Train Epoch: 46 [75072/225000 (33%)] Loss: 20702.574219\n",
      "Train Epoch: 46 [77568/225000 (34%)] Loss: 20503.712891\n",
      "Train Epoch: 46 [80064/225000 (36%)] Loss: 20288.302734\n",
      "Train Epoch: 46 [82560/225000 (37%)] Loss: 20334.058594\n",
      "Train Epoch: 46 [85056/225000 (38%)] Loss: 20254.894531\n",
      "Train Epoch: 46 [87552/225000 (39%)] Loss: 20529.171875\n",
      "Train Epoch: 46 [90048/225000 (40%)] Loss: 19967.378906\n",
      "Train Epoch: 46 [92544/225000 (41%)] Loss: 20356.697266\n",
      "Train Epoch: 46 [95040/225000 (42%)] Loss: 19735.570312\n",
      "Train Epoch: 46 [97536/225000 (43%)] Loss: 20281.593750\n",
      "Train Epoch: 46 [100032/225000 (44%)] Loss: 20631.207031\n",
      "Train Epoch: 46 [102528/225000 (46%)] Loss: 20298.875000\n",
      "Train Epoch: 46 [105024/225000 (47%)] Loss: 20129.531250\n",
      "Train Epoch: 46 [107520/225000 (48%)] Loss: 20239.640625\n",
      "Train Epoch: 46 [110016/225000 (49%)] Loss: 20495.507812\n",
      "Train Epoch: 46 [112512/225000 (50%)] Loss: 20497.414062\n",
      "Train Epoch: 46 [115008/225000 (51%)] Loss: 20361.531250\n",
      "Train Epoch: 46 [117504/225000 (52%)] Loss: 20510.693359\n",
      "Train Epoch: 46 [120000/225000 (53%)] Loss: 20384.929688\n",
      "Train Epoch: 46 [122496/225000 (54%)] Loss: 19159.156250\n",
      "Train Epoch: 46 [124992/225000 (56%)] Loss: 19959.992188\n",
      "Train Epoch: 46 [127488/225000 (57%)] Loss: 20004.947266\n",
      "Train Epoch: 46 [129984/225000 (58%)] Loss: 20576.917969\n",
      "Train Epoch: 46 [132480/225000 (59%)] Loss: 20318.152344\n",
      "Train Epoch: 46 [134976/225000 (60%)] Loss: 20002.093750\n",
      "Train Epoch: 46 [137472/225000 (61%)] Loss: 19691.693359\n",
      "Train Epoch: 46 [139968/225000 (62%)] Loss: 19945.820312\n",
      "Train Epoch: 46 [142464/225000 (63%)] Loss: 19755.984375\n",
      "Train Epoch: 46 [144960/225000 (64%)] Loss: 20660.066406\n",
      "Train Epoch: 46 [147456/225000 (66%)] Loss: 20055.457031\n",
      "Train Epoch: 46 [149952/225000 (67%)] Loss: 20573.082031\n",
      "Train Epoch: 46 [152448/225000 (68%)] Loss: 20060.777344\n",
      "Train Epoch: 46 [154944/225000 (69%)] Loss: 20260.791016\n",
      "Train Epoch: 46 [157440/225000 (70%)] Loss: 20608.511719\n",
      "Train Epoch: 46 [159936/225000 (71%)] Loss: 20117.503906\n",
      "Train Epoch: 46 [162432/225000 (72%)] Loss: 20065.023438\n",
      "Train Epoch: 46 [164928/225000 (73%)] Loss: 20263.656250\n",
      "Train Epoch: 46 [167424/225000 (74%)] Loss: 19763.921875\n",
      "Train Epoch: 46 [169920/225000 (76%)] Loss: 20310.621094\n",
      "Train Epoch: 46 [172416/225000 (77%)] Loss: 20260.880859\n",
      "Train Epoch: 46 [174912/225000 (78%)] Loss: 20020.429688\n",
      "Train Epoch: 46 [177408/225000 (79%)] Loss: 20298.757812\n",
      "Train Epoch: 46 [179904/225000 (80%)] Loss: 20054.333984\n",
      "Train Epoch: 46 [182400/225000 (81%)] Loss: 20467.101562\n",
      "Train Epoch: 46 [184896/225000 (82%)] Loss: 20417.015625\n",
      "Train Epoch: 46 [187392/225000 (83%)] Loss: 20560.972656\n",
      "Train Epoch: 46 [189888/225000 (84%)] Loss: 20179.960938\n",
      "Train Epoch: 46 [192384/225000 (86%)] Loss: 19698.503906\n",
      "Train Epoch: 46 [194880/225000 (87%)] Loss: 20469.896484\n",
      "Train Epoch: 46 [197376/225000 (88%)] Loss: 19972.171875\n",
      "Train Epoch: 46 [199872/225000 (89%)] Loss: 20771.773438\n",
      "Train Epoch: 46 [202368/225000 (90%)] Loss: 19796.355469\n",
      "Train Epoch: 46 [204864/225000 (91%)] Loss: 20297.527344\n",
      "Train Epoch: 46 [207360/225000 (92%)] Loss: 20015.966797\n",
      "Train Epoch: 46 [209856/225000 (93%)] Loss: 19963.875000\n",
      "Train Epoch: 46 [212352/225000 (94%)] Loss: 20087.541016\n",
      "Train Epoch: 46 [214848/225000 (95%)] Loss: 20615.710938\n",
      "Train Epoch: 46 [217344/225000 (97%)] Loss: 19579.121094\n",
      "Train Epoch: 46 [219840/225000 (98%)] Loss: 19778.529297\n",
      "Train Epoch: 46 [222336/225000 (99%)] Loss: 19716.732422\n",
      "Train Epoch: 46 [224832/225000 (100%)] Loss: 20304.421875\n",
      "    epoch          : 46\n",
      "    loss           : 20251.305545741787\n",
      "    val_loss       : 20157.158851457916\n",
      "Train Epoch: 47 [192/225000 (0%)] Loss: 20183.078125\n",
      "Train Epoch: 47 [2688/225000 (1%)] Loss: 19970.011719\n",
      "Train Epoch: 47 [5184/225000 (2%)] Loss: 20436.123047\n",
      "Train Epoch: 47 [7680/225000 (3%)] Loss: 20086.058594\n",
      "Train Epoch: 47 [10176/225000 (5%)] Loss: 20439.761719\n",
      "Train Epoch: 47 [12672/225000 (6%)] Loss: 20444.708984\n",
      "Train Epoch: 47 [15168/225000 (7%)] Loss: 20265.937500\n",
      "Train Epoch: 47 [17664/225000 (8%)] Loss: 20111.632812\n",
      "Train Epoch: 47 [20160/225000 (9%)] Loss: 20000.371094\n",
      "Train Epoch: 47 [22656/225000 (10%)] Loss: 20471.617188\n",
      "Train Epoch: 47 [25152/225000 (11%)] Loss: 19753.375000\n",
      "Train Epoch: 47 [27648/225000 (12%)] Loss: 20003.996094\n",
      "Train Epoch: 47 [30144/225000 (13%)] Loss: 20308.113281\n",
      "Train Epoch: 47 [32640/225000 (15%)] Loss: 20032.679688\n",
      "Train Epoch: 47 [35136/225000 (16%)] Loss: 20288.253906\n",
      "Train Epoch: 47 [37632/225000 (17%)] Loss: 20495.519531\n",
      "Train Epoch: 47 [40128/225000 (18%)] Loss: 20081.765625\n",
      "Train Epoch: 47 [42624/225000 (19%)] Loss: 20665.863281\n",
      "Train Epoch: 47 [45120/225000 (20%)] Loss: 20390.869141\n",
      "Train Epoch: 47 [47616/225000 (21%)] Loss: 20402.496094\n",
      "Train Epoch: 47 [50112/225000 (22%)] Loss: 20435.472656\n",
      "Train Epoch: 47 [52608/225000 (23%)] Loss: 20261.083984\n",
      "Train Epoch: 47 [55104/225000 (24%)] Loss: 20395.871094\n",
      "Train Epoch: 47 [57600/225000 (26%)] Loss: 20165.511719\n",
      "Train Epoch: 47 [60096/225000 (27%)] Loss: 20700.062500\n",
      "Train Epoch: 47 [62592/225000 (28%)] Loss: 19861.660156\n",
      "Train Epoch: 47 [65088/225000 (29%)] Loss: 20184.990234\n",
      "Train Epoch: 47 [67584/225000 (30%)] Loss: 20225.914062\n",
      "Train Epoch: 47 [70080/225000 (31%)] Loss: 20720.058594\n",
      "Train Epoch: 47 [72576/225000 (32%)] Loss: 20730.988281\n",
      "Train Epoch: 47 [75072/225000 (33%)] Loss: 19885.990234\n",
      "Train Epoch: 47 [77568/225000 (34%)] Loss: 20714.085938\n",
      "Train Epoch: 47 [80064/225000 (36%)] Loss: 19799.671875\n",
      "Train Epoch: 47 [82560/225000 (37%)] Loss: 20501.296875\n",
      "Train Epoch: 47 [85056/225000 (38%)] Loss: 20389.695312\n",
      "Train Epoch: 47 [87552/225000 (39%)] Loss: 19855.265625\n",
      "Train Epoch: 47 [90048/225000 (40%)] Loss: 19636.884766\n",
      "Train Epoch: 47 [92544/225000 (41%)] Loss: 20301.152344\n",
      "Train Epoch: 47 [95040/225000 (42%)] Loss: 20721.343750\n",
      "Train Epoch: 47 [97536/225000 (43%)] Loss: 20597.462891\n",
      "Train Epoch: 47 [100032/225000 (44%)] Loss: 20708.974609\n",
      "Train Epoch: 47 [102528/225000 (46%)] Loss: 20213.074219\n",
      "Train Epoch: 47 [105024/225000 (47%)] Loss: 20019.980469\n",
      "Train Epoch: 47 [107520/225000 (48%)] Loss: 20308.179688\n",
      "Train Epoch: 47 [110016/225000 (49%)] Loss: 20196.634766\n",
      "Train Epoch: 47 [112512/225000 (50%)] Loss: 20353.039062\n",
      "Train Epoch: 47 [115008/225000 (51%)] Loss: 19415.230469\n",
      "Train Epoch: 47 [117504/225000 (52%)] Loss: 20059.714844\n",
      "Train Epoch: 47 [120000/225000 (53%)] Loss: 20126.142578\n",
      "Train Epoch: 47 [122496/225000 (54%)] Loss: 20565.546875\n",
      "Train Epoch: 47 [124992/225000 (56%)] Loss: 19979.507812\n",
      "Train Epoch: 47 [127488/225000 (57%)] Loss: 20170.445312\n",
      "Train Epoch: 47 [129984/225000 (58%)] Loss: 20562.736328\n",
      "Train Epoch: 47 [132480/225000 (59%)] Loss: 20632.597656\n",
      "Train Epoch: 47 [134976/225000 (60%)] Loss: 20695.550781\n",
      "Train Epoch: 47 [137472/225000 (61%)] Loss: 20808.527344\n",
      "Train Epoch: 47 [139968/225000 (62%)] Loss: 20203.246094\n",
      "Train Epoch: 47 [142464/225000 (63%)] Loss: 20076.802734\n",
      "Train Epoch: 47 [144960/225000 (64%)] Loss: 20600.781250\n",
      "Train Epoch: 47 [147456/225000 (66%)] Loss: 20126.691406\n",
      "Train Epoch: 47 [149952/225000 (67%)] Loss: 19971.593750\n",
      "Train Epoch: 47 [152448/225000 (68%)] Loss: 20606.966797\n",
      "Train Epoch: 47 [154944/225000 (69%)] Loss: 20094.375000\n",
      "Train Epoch: 47 [157440/225000 (70%)] Loss: 20495.529297\n",
      "Train Epoch: 47 [159936/225000 (71%)] Loss: 19895.000000\n",
      "Train Epoch: 47 [162432/225000 (72%)] Loss: 20450.609375\n",
      "Train Epoch: 47 [164928/225000 (73%)] Loss: 20147.613281\n",
      "Train Epoch: 47 [167424/225000 (74%)] Loss: 20444.501953\n",
      "Train Epoch: 47 [169920/225000 (76%)] Loss: 20013.552734\n",
      "Train Epoch: 47 [172416/225000 (77%)] Loss: 20105.222656\n",
      "Train Epoch: 47 [174912/225000 (78%)] Loss: 19897.570312\n",
      "Train Epoch: 47 [177408/225000 (79%)] Loss: 20406.189453\n",
      "Train Epoch: 47 [179904/225000 (80%)] Loss: 20282.105469\n",
      "Train Epoch: 47 [182400/225000 (81%)] Loss: 19982.298828\n",
      "Train Epoch: 47 [184896/225000 (82%)] Loss: 19942.191406\n",
      "Train Epoch: 47 [187392/225000 (83%)] Loss: 20114.142578\n",
      "Train Epoch: 47 [189888/225000 (84%)] Loss: 20592.328125\n",
      "Train Epoch: 47 [192384/225000 (86%)] Loss: 20556.523438\n",
      "Train Epoch: 47 [194880/225000 (87%)] Loss: 20185.753906\n",
      "Train Epoch: 47 [197376/225000 (88%)] Loss: 20437.433594\n",
      "Train Epoch: 47 [199872/225000 (89%)] Loss: 20652.085938\n",
      "Train Epoch: 47 [202368/225000 (90%)] Loss: 20081.429688\n",
      "Train Epoch: 47 [204864/225000 (91%)] Loss: 20472.371094\n",
      "Train Epoch: 47 [207360/225000 (92%)] Loss: 20667.240234\n",
      "Train Epoch: 47 [209856/225000 (93%)] Loss: 20138.578125\n",
      "Train Epoch: 47 [212352/225000 (94%)] Loss: 20092.687500\n",
      "Train Epoch: 47 [214848/225000 (95%)] Loss: 20374.666016\n",
      "Train Epoch: 47 [217344/225000 (97%)] Loss: 20096.433594\n",
      "Train Epoch: 47 [219840/225000 (98%)] Loss: 20239.054688\n",
      "Train Epoch: 47 [222336/225000 (99%)] Loss: 20369.736328\n",
      "Train Epoch: 47 [224832/225000 (100%)] Loss: 20045.326172\n",
      "    epoch          : 47\n",
      "    loss           : 20247.923684806952\n",
      "    val_loss       : 20267.057221894047\n",
      "Train Epoch: 48 [192/225000 (0%)] Loss: 20837.296875\n",
      "Train Epoch: 48 [2688/225000 (1%)] Loss: 19895.148438\n",
      "Train Epoch: 48 [5184/225000 (2%)] Loss: 20064.296875\n",
      "Train Epoch: 48 [7680/225000 (3%)] Loss: 20596.421875\n",
      "Train Epoch: 48 [10176/225000 (5%)] Loss: 20454.701172\n",
      "Train Epoch: 48 [12672/225000 (6%)] Loss: 20536.164062\n",
      "Train Epoch: 48 [15168/225000 (7%)] Loss: 20677.433594\n",
      "Train Epoch: 48 [17664/225000 (8%)] Loss: 20471.820312\n",
      "Train Epoch: 48 [20160/225000 (9%)] Loss: 20230.865234\n",
      "Train Epoch: 48 [22656/225000 (10%)] Loss: 20000.373047\n",
      "Train Epoch: 48 [25152/225000 (11%)] Loss: 20541.046875\n",
      "Train Epoch: 48 [27648/225000 (12%)] Loss: 20623.193359\n",
      "Train Epoch: 48 [30144/225000 (13%)] Loss: 20760.562500\n",
      "Train Epoch: 48 [32640/225000 (15%)] Loss: 19988.269531\n",
      "Train Epoch: 48 [35136/225000 (16%)] Loss: 19987.167969\n",
      "Train Epoch: 48 [37632/225000 (17%)] Loss: 20707.214844\n",
      "Train Epoch: 48 [40128/225000 (18%)] Loss: 20411.074219\n",
      "Train Epoch: 48 [42624/225000 (19%)] Loss: 19951.091797\n",
      "Train Epoch: 48 [45120/225000 (20%)] Loss: 20556.103516\n",
      "Train Epoch: 48 [47616/225000 (21%)] Loss: 19894.019531\n",
      "Train Epoch: 48 [50112/225000 (22%)] Loss: 19826.863281\n",
      "Train Epoch: 48 [52608/225000 (23%)] Loss: 20478.007812\n",
      "Train Epoch: 48 [55104/225000 (24%)] Loss: 20423.730469\n",
      "Train Epoch: 48 [57600/225000 (26%)] Loss: 19905.708984\n",
      "Train Epoch: 48 [60096/225000 (27%)] Loss: 20375.917969\n",
      "Train Epoch: 48 [62592/225000 (28%)] Loss: 19822.437500\n",
      "Train Epoch: 48 [65088/225000 (29%)] Loss: 20408.425781\n",
      "Train Epoch: 48 [67584/225000 (30%)] Loss: 20035.062500\n",
      "Train Epoch: 48 [70080/225000 (31%)] Loss: 20157.632812\n",
      "Train Epoch: 48 [72576/225000 (32%)] Loss: 20457.136719\n",
      "Train Epoch: 48 [75072/225000 (33%)] Loss: 20395.298828\n",
      "Train Epoch: 48 [77568/225000 (34%)] Loss: 19949.289062\n",
      "Train Epoch: 48 [80064/225000 (36%)] Loss: 20583.765625\n",
      "Train Epoch: 48 [82560/225000 (37%)] Loss: 20435.175781\n",
      "Train Epoch: 48 [85056/225000 (38%)] Loss: 20042.898438\n",
      "Train Epoch: 48 [87552/225000 (39%)] Loss: 20303.621094\n",
      "Train Epoch: 48 [90048/225000 (40%)] Loss: 20294.824219\n",
      "Train Epoch: 48 [92544/225000 (41%)] Loss: 20833.066406\n",
      "Train Epoch: 48 [95040/225000 (42%)] Loss: 20255.265625\n",
      "Train Epoch: 48 [97536/225000 (43%)] Loss: 19951.433594\n",
      "Train Epoch: 48 [100032/225000 (44%)] Loss: 20571.882812\n",
      "Train Epoch: 48 [102528/225000 (46%)] Loss: 20181.714844\n",
      "Train Epoch: 48 [105024/225000 (47%)] Loss: 20219.984375\n",
      "Train Epoch: 48 [107520/225000 (48%)] Loss: 20259.195312\n",
      "Train Epoch: 48 [110016/225000 (49%)] Loss: 20286.353516\n",
      "Train Epoch: 48 [112512/225000 (50%)] Loss: 20651.937500\n",
      "Train Epoch: 48 [115008/225000 (51%)] Loss: 20511.523438\n",
      "Train Epoch: 48 [117504/225000 (52%)] Loss: 20110.074219\n",
      "Train Epoch: 48 [120000/225000 (53%)] Loss: 20610.128906\n",
      "Train Epoch: 48 [122496/225000 (54%)] Loss: 20601.011719\n",
      "Train Epoch: 48 [124992/225000 (56%)] Loss: 20406.876953\n",
      "Train Epoch: 48 [127488/225000 (57%)] Loss: 20572.101562\n",
      "Train Epoch: 48 [129984/225000 (58%)] Loss: 19961.953125\n",
      "Train Epoch: 48 [132480/225000 (59%)] Loss: 20620.097656\n",
      "Train Epoch: 48 [134976/225000 (60%)] Loss: 19860.968750\n",
      "Train Epoch: 48 [137472/225000 (61%)] Loss: 19805.113281\n",
      "Train Epoch: 48 [139968/225000 (62%)] Loss: 19948.976562\n",
      "Train Epoch: 48 [142464/225000 (63%)] Loss: 20164.517578\n",
      "Train Epoch: 48 [144960/225000 (64%)] Loss: 20210.166016\n",
      "Train Epoch: 48 [147456/225000 (66%)] Loss: 19879.183594\n",
      "Train Epoch: 48 [149952/225000 (67%)] Loss: 20235.476562\n",
      "Train Epoch: 48 [152448/225000 (68%)] Loss: 20372.460938\n",
      "Train Epoch: 48 [154944/225000 (69%)] Loss: 19774.128906\n",
      "Train Epoch: 48 [157440/225000 (70%)] Loss: 19921.511719\n",
      "Train Epoch: 48 [159936/225000 (71%)] Loss: 20296.841797\n",
      "Train Epoch: 48 [162432/225000 (72%)] Loss: 20605.888672\n",
      "Train Epoch: 48 [164928/225000 (73%)] Loss: 20317.636719\n",
      "Train Epoch: 48 [167424/225000 (74%)] Loss: 20036.871094\n",
      "Train Epoch: 48 [169920/225000 (76%)] Loss: 19808.472656\n",
      "Train Epoch: 48 [172416/225000 (77%)] Loss: 20180.710938\n",
      "Train Epoch: 48 [174912/225000 (78%)] Loss: 19915.644531\n",
      "Train Epoch: 48 [177408/225000 (79%)] Loss: 20162.425781\n",
      "Train Epoch: 48 [179904/225000 (80%)] Loss: 20585.927734\n",
      "Train Epoch: 48 [182400/225000 (81%)] Loss: 20104.242188\n",
      "Train Epoch: 48 [184896/225000 (82%)] Loss: 20003.830078\n",
      "Train Epoch: 48 [187392/225000 (83%)] Loss: 20046.523438\n",
      "Train Epoch: 48 [189888/225000 (84%)] Loss: 20131.531250\n",
      "Train Epoch: 48 [192384/225000 (86%)] Loss: 19812.800781\n",
      "Train Epoch: 48 [194880/225000 (87%)] Loss: 20105.523438\n",
      "Train Epoch: 48 [197376/225000 (88%)] Loss: 20049.648438\n",
      "Train Epoch: 48 [199872/225000 (89%)] Loss: 20139.070312\n",
      "Train Epoch: 48 [202368/225000 (90%)] Loss: 20276.824219\n",
      "Train Epoch: 48 [204864/225000 (91%)] Loss: 19828.402344\n",
      "Train Epoch: 48 [207360/225000 (92%)] Loss: 20544.962891\n",
      "Train Epoch: 48 [209856/225000 (93%)] Loss: 20384.941406\n",
      "Train Epoch: 48 [212352/225000 (94%)] Loss: 20549.894531\n",
      "Train Epoch: 48 [214848/225000 (95%)] Loss: 19847.910156\n",
      "Train Epoch: 48 [217344/225000 (97%)] Loss: 19888.539062\n",
      "Train Epoch: 48 [219840/225000 (98%)] Loss: 20068.742188\n",
      "Train Epoch: 48 [222336/225000 (99%)] Loss: 20068.150391\n",
      "Train Epoch: 48 [224832/225000 (100%)] Loss: 20480.625000\n",
      "    epoch          : 48\n",
      "    loss           : 20246.434455324765\n",
      "    val_loss       : 20154.434922069082\n",
      "Train Epoch: 49 [192/225000 (0%)] Loss: 20368.769531\n",
      "Train Epoch: 49 [2688/225000 (1%)] Loss: 20840.281250\n",
      "Train Epoch: 49 [5184/225000 (2%)] Loss: 20344.339844\n",
      "Train Epoch: 49 [7680/225000 (3%)] Loss: 20201.669922\n",
      "Train Epoch: 49 [10176/225000 (5%)] Loss: 20662.750000\n",
      "Train Epoch: 49 [12672/225000 (6%)] Loss: 20477.183594\n",
      "Train Epoch: 49 [15168/225000 (7%)] Loss: 20582.314453\n",
      "Train Epoch: 49 [17664/225000 (8%)] Loss: 20058.007812\n",
      "Train Epoch: 49 [20160/225000 (9%)] Loss: 20103.208984\n",
      "Train Epoch: 49 [22656/225000 (10%)] Loss: 20261.199219\n",
      "Train Epoch: 49 [25152/225000 (11%)] Loss: 19783.363281\n",
      "Train Epoch: 49 [27648/225000 (12%)] Loss: 20439.062500\n",
      "Train Epoch: 49 [30144/225000 (13%)] Loss: 20392.265625\n",
      "Train Epoch: 49 [32640/225000 (15%)] Loss: 19987.820312\n",
      "Train Epoch: 49 [35136/225000 (16%)] Loss: 19502.417969\n",
      "Train Epoch: 49 [37632/225000 (17%)] Loss: 20146.769531\n",
      "Train Epoch: 49 [40128/225000 (18%)] Loss: 19804.312500\n",
      "Train Epoch: 49 [42624/225000 (19%)] Loss: 20085.710938\n",
      "Train Epoch: 49 [45120/225000 (20%)] Loss: 19949.824219\n",
      "Train Epoch: 49 [47616/225000 (21%)] Loss: 19781.767578\n",
      "Train Epoch: 49 [50112/225000 (22%)] Loss: 20228.484375\n",
      "Train Epoch: 49 [52608/225000 (23%)] Loss: 20029.179688\n",
      "Train Epoch: 49 [55104/225000 (24%)] Loss: 20400.207031\n",
      "Train Epoch: 49 [57600/225000 (26%)] Loss: 20537.322266\n",
      "Train Epoch: 49 [60096/225000 (27%)] Loss: 20233.484375\n",
      "Train Epoch: 49 [62592/225000 (28%)] Loss: 20018.742188\n",
      "Train Epoch: 49 [65088/225000 (29%)] Loss: 20377.224609\n",
      "Train Epoch: 49 [67584/225000 (30%)] Loss: 20212.451172\n",
      "Train Epoch: 49 [70080/225000 (31%)] Loss: 20521.468750\n",
      "Train Epoch: 49 [72576/225000 (32%)] Loss: 20301.847656\n",
      "Train Epoch: 49 [75072/225000 (33%)] Loss: 20293.210938\n",
      "Train Epoch: 49 [77568/225000 (34%)] Loss: 20279.789062\n",
      "Train Epoch: 49 [80064/225000 (36%)] Loss: 20051.523438\n",
      "Train Epoch: 49 [82560/225000 (37%)] Loss: 20358.375000\n",
      "Train Epoch: 49 [85056/225000 (38%)] Loss: 20127.687500\n",
      "Train Epoch: 49 [87552/225000 (39%)] Loss: 20342.544922\n",
      "Train Epoch: 49 [90048/225000 (40%)] Loss: 20349.746094\n",
      "Train Epoch: 49 [92544/225000 (41%)] Loss: 20281.361328\n",
      "Train Epoch: 49 [95040/225000 (42%)] Loss: 20645.783203\n",
      "Train Epoch: 49 [97536/225000 (43%)] Loss: 20632.464844\n",
      "Train Epoch: 49 [100032/225000 (44%)] Loss: 20045.587891\n",
      "Train Epoch: 49 [102528/225000 (46%)] Loss: 20387.554688\n",
      "Train Epoch: 49 [105024/225000 (47%)] Loss: 20046.314453\n",
      "Train Epoch: 49 [107520/225000 (48%)] Loss: 20352.238281\n",
      "Train Epoch: 49 [110016/225000 (49%)] Loss: 20728.074219\n",
      "Train Epoch: 49 [112512/225000 (50%)] Loss: 20222.261719\n",
      "Train Epoch: 49 [115008/225000 (51%)] Loss: 21011.435547\n",
      "Train Epoch: 49 [117504/225000 (52%)] Loss: 20346.066406\n",
      "Train Epoch: 49 [120000/225000 (53%)] Loss: 20138.599609\n",
      "Train Epoch: 49 [122496/225000 (54%)] Loss: 20197.849609\n",
      "Train Epoch: 49 [124992/225000 (56%)] Loss: 20433.333984\n",
      "Train Epoch: 49 [127488/225000 (57%)] Loss: 19811.710938\n",
      "Train Epoch: 49 [129984/225000 (58%)] Loss: 19922.656250\n",
      "Train Epoch: 49 [132480/225000 (59%)] Loss: 20033.812500\n",
      "Train Epoch: 49 [134976/225000 (60%)] Loss: 20840.144531\n",
      "Train Epoch: 49 [137472/225000 (61%)] Loss: 20188.937500\n",
      "Train Epoch: 49 [139968/225000 (62%)] Loss: 20085.515625\n",
      "Train Epoch: 49 [142464/225000 (63%)] Loss: 20334.414062\n",
      "Train Epoch: 49 [144960/225000 (64%)] Loss: 20619.085938\n",
      "Train Epoch: 49 [147456/225000 (66%)] Loss: 20575.068359\n",
      "Train Epoch: 49 [149952/225000 (67%)] Loss: 20317.164062\n",
      "Train Epoch: 49 [152448/225000 (68%)] Loss: 20323.839844\n",
      "Train Epoch: 49 [154944/225000 (69%)] Loss: 20179.316406\n",
      "Train Epoch: 49 [157440/225000 (70%)] Loss: 20242.621094\n",
      "Train Epoch: 49 [159936/225000 (71%)] Loss: 20201.208984\n",
      "Train Epoch: 49 [162432/225000 (72%)] Loss: 20073.464844\n",
      "Train Epoch: 49 [164928/225000 (73%)] Loss: 20000.722656\n",
      "Train Epoch: 49 [167424/225000 (74%)] Loss: 20070.882812\n",
      "Train Epoch: 49 [169920/225000 (76%)] Loss: 19983.636719\n",
      "Train Epoch: 49 [172416/225000 (77%)] Loss: 19929.679688\n",
      "Train Epoch: 49 [174912/225000 (78%)] Loss: 20487.474609\n",
      "Train Epoch: 49 [177408/225000 (79%)] Loss: 20367.953125\n",
      "Train Epoch: 49 [179904/225000 (80%)] Loss: 19903.556641\n",
      "Train Epoch: 49 [182400/225000 (81%)] Loss: 20201.757812\n",
      "Train Epoch: 49 [184896/225000 (82%)] Loss: 20050.666016\n",
      "Train Epoch: 49 [187392/225000 (83%)] Loss: 20247.658203\n",
      "Train Epoch: 49 [189888/225000 (84%)] Loss: 20368.677734\n",
      "Train Epoch: 49 [192384/225000 (86%)] Loss: 19918.617188\n",
      "Train Epoch: 49 [194880/225000 (87%)] Loss: 20344.824219\n",
      "Train Epoch: 49 [197376/225000 (88%)] Loss: 20300.343750\n",
      "Train Epoch: 49 [199872/225000 (89%)] Loss: 20270.816406\n",
      "Train Epoch: 49 [202368/225000 (90%)] Loss: 20273.441406\n",
      "Train Epoch: 49 [204864/225000 (91%)] Loss: 19843.527344\n",
      "Train Epoch: 49 [207360/225000 (92%)] Loss: 19774.869141\n",
      "Train Epoch: 49 [209856/225000 (93%)] Loss: 20111.191406\n",
      "Train Epoch: 49 [212352/225000 (94%)] Loss: 20401.335938\n",
      "Train Epoch: 49 [214848/225000 (95%)] Loss: 20953.480469\n",
      "Train Epoch: 49 [217344/225000 (97%)] Loss: 20184.003906\n",
      "Train Epoch: 49 [219840/225000 (98%)] Loss: 20205.449219\n",
      "Train Epoch: 49 [222336/225000 (99%)] Loss: 20618.521484\n",
      "Train Epoch: 49 [224832/225000 (100%)] Loss: 19926.875000\n",
      "    epoch          : 49\n",
      "    loss           : 20256.16214103829\n",
      "    val_loss       : 20143.5452929262\n",
      "Train Epoch: 50 [192/225000 (0%)] Loss: 20380.937500\n",
      "Train Epoch: 50 [2688/225000 (1%)] Loss: 20037.605469\n",
      "Train Epoch: 50 [5184/225000 (2%)] Loss: 20386.078125\n",
      "Train Epoch: 50 [7680/225000 (3%)] Loss: 20387.449219\n",
      "Train Epoch: 50 [10176/225000 (5%)] Loss: 20129.746094\n",
      "Train Epoch: 50 [12672/225000 (6%)] Loss: 20591.218750\n",
      "Train Epoch: 50 [15168/225000 (7%)] Loss: 20073.800781\n",
      "Train Epoch: 50 [17664/225000 (8%)] Loss: 20328.492188\n",
      "Train Epoch: 50 [20160/225000 (9%)] Loss: 20414.951172\n",
      "Train Epoch: 50 [22656/225000 (10%)] Loss: 20307.449219\n",
      "Train Epoch: 50 [25152/225000 (11%)] Loss: 20413.589844\n",
      "Train Epoch: 50 [27648/225000 (12%)] Loss: 19972.765625\n",
      "Train Epoch: 50 [30144/225000 (13%)] Loss: 20195.382812\n",
      "Train Epoch: 50 [32640/225000 (15%)] Loss: 20070.179688\n",
      "Train Epoch: 50 [35136/225000 (16%)] Loss: 19988.015625\n",
      "Train Epoch: 50 [37632/225000 (17%)] Loss: 20372.345703\n",
      "Train Epoch: 50 [40128/225000 (18%)] Loss: 20445.603516\n",
      "Train Epoch: 50 [42624/225000 (19%)] Loss: 20574.917969\n",
      "Train Epoch: 50 [45120/225000 (20%)] Loss: 20081.542969\n",
      "Train Epoch: 50 [47616/225000 (21%)] Loss: 19703.679688\n",
      "Train Epoch: 50 [50112/225000 (22%)] Loss: 20844.962891\n",
      "Train Epoch: 50 [52608/225000 (23%)] Loss: 20513.023438\n",
      "Train Epoch: 50 [55104/225000 (24%)] Loss: 20588.523438\n",
      "Train Epoch: 50 [57600/225000 (26%)] Loss: 19559.183594\n",
      "Train Epoch: 50 [60096/225000 (27%)] Loss: 20521.988281\n",
      "Train Epoch: 50 [62592/225000 (28%)] Loss: 20103.707031\n",
      "Train Epoch: 50 [65088/225000 (29%)] Loss: 20103.007812\n",
      "Train Epoch: 50 [67584/225000 (30%)] Loss: 20048.804688\n",
      "Train Epoch: 50 [70080/225000 (31%)] Loss: 20121.177734\n",
      "Train Epoch: 50 [72576/225000 (32%)] Loss: 19944.355469\n",
      "Train Epoch: 50 [75072/225000 (33%)] Loss: 19766.480469\n",
      "Train Epoch: 50 [77568/225000 (34%)] Loss: 20191.847656\n",
      "Train Epoch: 50 [80064/225000 (36%)] Loss: 20357.724609\n",
      "Train Epoch: 50 [82560/225000 (37%)] Loss: 20912.898438\n",
      "Train Epoch: 50 [85056/225000 (38%)] Loss: 20755.386719\n",
      "Train Epoch: 50 [87552/225000 (39%)] Loss: 20489.644531\n",
      "Train Epoch: 50 [90048/225000 (40%)] Loss: 20480.242188\n",
      "Train Epoch: 50 [92544/225000 (41%)] Loss: 19788.007812\n",
      "Train Epoch: 50 [95040/225000 (42%)] Loss: 19845.113281\n",
      "Train Epoch: 50 [97536/225000 (43%)] Loss: 19898.949219\n",
      "Train Epoch: 50 [100032/225000 (44%)] Loss: 20200.330078\n",
      "Train Epoch: 50 [102528/225000 (46%)] Loss: 20114.005859\n",
      "Train Epoch: 50 [105024/225000 (47%)] Loss: 19894.138672\n",
      "Train Epoch: 50 [107520/225000 (48%)] Loss: 20261.777344\n",
      "Train Epoch: 50 [110016/225000 (49%)] Loss: 20578.552734\n",
      "Train Epoch: 50 [112512/225000 (50%)] Loss: 20103.324219\n",
      "Train Epoch: 50 [115008/225000 (51%)] Loss: 20828.107422\n",
      "Train Epoch: 50 [117504/225000 (52%)] Loss: 19921.646484\n",
      "Train Epoch: 50 [120000/225000 (53%)] Loss: 19974.710938\n",
      "Train Epoch: 50 [122496/225000 (54%)] Loss: 20338.773438\n",
      "Train Epoch: 50 [124992/225000 (56%)] Loss: 20127.826172\n",
      "Train Epoch: 50 [127488/225000 (57%)] Loss: 20343.355469\n",
      "Train Epoch: 50 [129984/225000 (58%)] Loss: 20158.023438\n",
      "Train Epoch: 50 [132480/225000 (59%)] Loss: 19767.496094\n",
      "Train Epoch: 50 [134976/225000 (60%)] Loss: 19912.677734\n",
      "Train Epoch: 50 [137472/225000 (61%)] Loss: 20275.792969\n",
      "Train Epoch: 50 [139968/225000 (62%)] Loss: 20737.566406\n",
      "Train Epoch: 50 [142464/225000 (63%)] Loss: 20240.023438\n",
      "Train Epoch: 50 [144960/225000 (64%)] Loss: 20155.255859\n",
      "Train Epoch: 50 [147456/225000 (66%)] Loss: 19503.591797\n",
      "Train Epoch: 50 [149952/225000 (67%)] Loss: 20280.455078\n",
      "Train Epoch: 50 [152448/225000 (68%)] Loss: 20432.113281\n",
      "Train Epoch: 50 [154944/225000 (69%)] Loss: 19827.277344\n",
      "Train Epoch: 50 [157440/225000 (70%)] Loss: 19916.337891\n",
      "Train Epoch: 50 [159936/225000 (71%)] Loss: 19819.328125\n",
      "Train Epoch: 50 [162432/225000 (72%)] Loss: 20191.494141\n",
      "Train Epoch: 50 [164928/225000 (73%)] Loss: 19950.421875\n",
      "Train Epoch: 50 [167424/225000 (74%)] Loss: 20448.855469\n",
      "Train Epoch: 50 [169920/225000 (76%)] Loss: 20347.328125\n",
      "Train Epoch: 50 [172416/225000 (77%)] Loss: 20201.091797\n",
      "Train Epoch: 50 [174912/225000 (78%)] Loss: 20323.164062\n",
      "Train Epoch: 50 [177408/225000 (79%)] Loss: 19903.955078\n",
      "Train Epoch: 50 [179904/225000 (80%)] Loss: 20201.642578\n",
      "Train Epoch: 50 [182400/225000 (81%)] Loss: 20460.570312\n",
      "Train Epoch: 50 [184896/225000 (82%)] Loss: 20474.501953\n",
      "Train Epoch: 50 [187392/225000 (83%)] Loss: 20228.046875\n",
      "Train Epoch: 50 [189888/225000 (84%)] Loss: 20807.898438\n",
      "Train Epoch: 50 [192384/225000 (86%)] Loss: 19874.027344\n",
      "Train Epoch: 50 [194880/225000 (87%)] Loss: 20319.558594\n",
      "Train Epoch: 50 [197376/225000 (88%)] Loss: 19786.677734\n",
      "Train Epoch: 50 [199872/225000 (89%)] Loss: 20169.152344\n",
      "Train Epoch: 50 [202368/225000 (90%)] Loss: 20524.343750\n",
      "Train Epoch: 50 [204864/225000 (91%)] Loss: 20504.984375\n",
      "Train Epoch: 50 [207360/225000 (92%)] Loss: 19892.005859\n",
      "Train Epoch: 50 [209856/225000 (93%)] Loss: 20139.855469\n",
      "Train Epoch: 50 [212352/225000 (94%)] Loss: 20145.265625\n",
      "Train Epoch: 50 [214848/225000 (95%)] Loss: 19964.808594\n",
      "Train Epoch: 50 [217344/225000 (97%)] Loss: 20041.277344\n",
      "Train Epoch: 50 [219840/225000 (98%)] Loss: 20078.800781\n",
      "Train Epoch: 50 [222336/225000 (99%)] Loss: 20480.382812\n",
      "Train Epoch: 50 [224832/225000 (100%)] Loss: 20215.558594\n",
      "    epoch          : 50\n",
      "    loss           : 20254.083579418195\n",
      "    val_loss       : 20129.925958857282\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0804_123131/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [192/225000 (0%)] Loss: 20347.597656\n",
      "Train Epoch: 51 [2688/225000 (1%)] Loss: 20063.138672\n",
      "Train Epoch: 51 [5184/225000 (2%)] Loss: 20235.945312\n",
      "Train Epoch: 51 [7680/225000 (3%)] Loss: 19951.753906\n",
      "Train Epoch: 51 [10176/225000 (5%)] Loss: 20156.773438\n",
      "Train Epoch: 51 [12672/225000 (6%)] Loss: 20180.980469\n",
      "Train Epoch: 51 [15168/225000 (7%)] Loss: 20418.058594\n",
      "Train Epoch: 51 [17664/225000 (8%)] Loss: 19651.722656\n",
      "Train Epoch: 51 [20160/225000 (9%)] Loss: 20086.281250\n",
      "Train Epoch: 51 [22656/225000 (10%)] Loss: 20030.152344\n",
      "Train Epoch: 51 [25152/225000 (11%)] Loss: 20397.287109\n",
      "Train Epoch: 51 [27648/225000 (12%)] Loss: 20969.035156\n",
      "Train Epoch: 51 [30144/225000 (13%)] Loss: 20599.437500\n",
      "Train Epoch: 51 [32640/225000 (15%)] Loss: 19930.625000\n",
      "Train Epoch: 51 [35136/225000 (16%)] Loss: 19738.427734\n",
      "Train Epoch: 51 [37632/225000 (17%)] Loss: 20063.667969\n",
      "Train Epoch: 51 [40128/225000 (18%)] Loss: 19933.324219\n",
      "Train Epoch: 51 [42624/225000 (19%)] Loss: 20033.933594\n",
      "Train Epoch: 51 [45120/225000 (20%)] Loss: 20342.992188\n",
      "Train Epoch: 51 [47616/225000 (21%)] Loss: 20182.470703\n",
      "Train Epoch: 51 [50112/225000 (22%)] Loss: 20112.572266\n",
      "Train Epoch: 51 [52608/225000 (23%)] Loss: 20160.294922\n",
      "Train Epoch: 51 [55104/225000 (24%)] Loss: 20400.378906\n",
      "Train Epoch: 51 [57600/225000 (26%)] Loss: 19804.386719\n",
      "Train Epoch: 51 [60096/225000 (27%)] Loss: 20283.574219\n",
      "Train Epoch: 51 [62592/225000 (28%)] Loss: 20017.312500\n",
      "Train Epoch: 51 [65088/225000 (29%)] Loss: 20204.060547\n",
      "Train Epoch: 51 [67584/225000 (30%)] Loss: 20773.156250\n",
      "Train Epoch: 51 [70080/225000 (31%)] Loss: 19880.574219\n",
      "Train Epoch: 51 [72576/225000 (32%)] Loss: 19908.435547\n",
      "Train Epoch: 51 [75072/225000 (33%)] Loss: 20049.816406\n",
      "Train Epoch: 51 [77568/225000 (34%)] Loss: 20517.468750\n",
      "Train Epoch: 51 [80064/225000 (36%)] Loss: 20001.341797\n",
      "Train Epoch: 51 [82560/225000 (37%)] Loss: 20462.398438\n",
      "Train Epoch: 51 [85056/225000 (38%)] Loss: 20368.449219\n",
      "Train Epoch: 51 [87552/225000 (39%)] Loss: 20182.919922\n",
      "Train Epoch: 51 [90048/225000 (40%)] Loss: 19571.152344\n",
      "Train Epoch: 51 [92544/225000 (41%)] Loss: 20371.148438\n",
      "Train Epoch: 51 [95040/225000 (42%)] Loss: 20422.992188\n",
      "Train Epoch: 51 [97536/225000 (43%)] Loss: 20180.234375\n",
      "Train Epoch: 51 [100032/225000 (44%)] Loss: 20464.507812\n",
      "Train Epoch: 51 [102528/225000 (46%)] Loss: 20422.484375\n",
      "Train Epoch: 51 [105024/225000 (47%)] Loss: 20112.894531\n",
      "Train Epoch: 51 [107520/225000 (48%)] Loss: 20264.851562\n",
      "Train Epoch: 51 [110016/225000 (49%)] Loss: 19970.710938\n",
      "Train Epoch: 51 [112512/225000 (50%)] Loss: 19770.730469\n",
      "Train Epoch: 51 [115008/225000 (51%)] Loss: 20290.574219\n",
      "Train Epoch: 51 [117504/225000 (52%)] Loss: 20132.660156\n",
      "Train Epoch: 51 [120000/225000 (53%)] Loss: 20346.136719\n",
      "Train Epoch: 51 [122496/225000 (54%)] Loss: 20301.613281\n",
      "Train Epoch: 51 [124992/225000 (56%)] Loss: 20279.269531\n",
      "Train Epoch: 51 [127488/225000 (57%)] Loss: 20052.277344\n",
      "Train Epoch: 51 [129984/225000 (58%)] Loss: 20030.255859\n",
      "Train Epoch: 51 [132480/225000 (59%)] Loss: 20534.773438\n",
      "Train Epoch: 51 [134976/225000 (60%)] Loss: 20271.021484\n",
      "Train Epoch: 51 [137472/225000 (61%)] Loss: 20773.136719\n",
      "Train Epoch: 51 [139968/225000 (62%)] Loss: 20193.867188\n",
      "Train Epoch: 51 [142464/225000 (63%)] Loss: 20429.527344\n",
      "Train Epoch: 51 [144960/225000 (64%)] Loss: 20262.867188\n",
      "Train Epoch: 51 [147456/225000 (66%)] Loss: 20040.101562\n",
      "Train Epoch: 51 [149952/225000 (67%)] Loss: 20314.574219\n",
      "Train Epoch: 51 [152448/225000 (68%)] Loss: 20459.191406\n",
      "Train Epoch: 51 [154944/225000 (69%)] Loss: 20891.392578\n",
      "Train Epoch: 51 [157440/225000 (70%)] Loss: 20203.074219\n",
      "Train Epoch: 51 [159936/225000 (71%)] Loss: 20189.111328\n",
      "Train Epoch: 51 [162432/225000 (72%)] Loss: 20297.826172\n",
      "Train Epoch: 51 [164928/225000 (73%)] Loss: 19871.119141\n",
      "Train Epoch: 51 [167424/225000 (74%)] Loss: 20185.738281\n",
      "Train Epoch: 51 [169920/225000 (76%)] Loss: 20243.341797\n",
      "Train Epoch: 51 [172416/225000 (77%)] Loss: 20220.568359\n",
      "Train Epoch: 51 [174912/225000 (78%)] Loss: 20259.410156\n",
      "Train Epoch: 51 [177408/225000 (79%)] Loss: 20082.740234\n",
      "Train Epoch: 51 [179904/225000 (80%)] Loss: 20084.304688\n",
      "Train Epoch: 51 [182400/225000 (81%)] Loss: 20512.542969\n",
      "Train Epoch: 51 [184896/225000 (82%)] Loss: 20468.500000\n",
      "Train Epoch: 51 [187392/225000 (83%)] Loss: 19858.285156\n",
      "Train Epoch: 51 [189888/225000 (84%)] Loss: 20284.746094\n",
      "Train Epoch: 51 [192384/225000 (86%)] Loss: 20348.748047\n",
      "Train Epoch: 51 [194880/225000 (87%)] Loss: 19874.091797\n",
      "Train Epoch: 51 [197376/225000 (88%)] Loss: 20801.244141\n",
      "Train Epoch: 51 [199872/225000 (89%)] Loss: 20124.263672\n",
      "Train Epoch: 51 [202368/225000 (90%)] Loss: 19853.476562\n",
      "Train Epoch: 51 [204864/225000 (91%)] Loss: 20597.128906\n",
      "Train Epoch: 51 [207360/225000 (92%)] Loss: 20151.263672\n",
      "Train Epoch: 51 [209856/225000 (93%)] Loss: 20568.046875\n",
      "Train Epoch: 51 [212352/225000 (94%)] Loss: 20192.027344\n",
      "Train Epoch: 51 [214848/225000 (95%)] Loss: 20166.355469\n",
      "Train Epoch: 51 [217344/225000 (97%)] Loss: 20743.134766\n",
      "Train Epoch: 51 [219840/225000 (98%)] Loss: 20212.478516\n",
      "Train Epoch: 51 [222336/225000 (99%)] Loss: 19787.089844\n",
      "Train Epoch: 51 [224832/225000 (100%)] Loss: 20204.554688\n",
      "    epoch          : 51\n",
      "    loss           : 20247.78973409503\n",
      "    val_loss       : 20113.9083173457\n",
      "Train Epoch: 52 [192/225000 (0%)] Loss: 19893.181641\n",
      "Train Epoch: 52 [2688/225000 (1%)] Loss: 19746.097656\n",
      "Train Epoch: 52 [5184/225000 (2%)] Loss: 20147.558594\n",
      "Train Epoch: 52 [7680/225000 (3%)] Loss: 20268.755859\n",
      "Train Epoch: 52 [10176/225000 (5%)] Loss: 20461.769531\n",
      "Train Epoch: 52 [12672/225000 (6%)] Loss: 20268.023438\n",
      "Train Epoch: 52 [15168/225000 (7%)] Loss: 20228.222656\n",
      "Train Epoch: 52 [17664/225000 (8%)] Loss: 20103.757812\n",
      "Train Epoch: 52 [20160/225000 (9%)] Loss: 20230.312500\n",
      "Train Epoch: 52 [22656/225000 (10%)] Loss: 20699.263672\n",
      "Train Epoch: 52 [25152/225000 (11%)] Loss: 20338.859375\n",
      "Train Epoch: 52 [27648/225000 (12%)] Loss: 20487.976562\n",
      "Train Epoch: 52 [30144/225000 (13%)] Loss: 20437.541016\n",
      "Train Epoch: 52 [32640/225000 (15%)] Loss: 19764.984375\n",
      "Train Epoch: 52 [35136/225000 (16%)] Loss: 20502.546875\n",
      "Train Epoch: 52 [37632/225000 (17%)] Loss: 20293.593750\n",
      "Train Epoch: 52 [40128/225000 (18%)] Loss: 19974.800781\n",
      "Train Epoch: 52 [42624/225000 (19%)] Loss: 20375.660156\n",
      "Train Epoch: 52 [45120/225000 (20%)] Loss: 20068.687500\n",
      "Train Epoch: 52 [47616/225000 (21%)] Loss: 20183.757812\n",
      "Train Epoch: 52 [50112/225000 (22%)] Loss: 20473.214844\n",
      "Train Epoch: 52 [52608/225000 (23%)] Loss: 20581.464844\n",
      "Train Epoch: 52 [55104/225000 (24%)] Loss: 20055.619141\n",
      "Train Epoch: 52 [57600/225000 (26%)] Loss: 20349.441406\n",
      "Train Epoch: 52 [60096/225000 (27%)] Loss: 19978.394531\n",
      "Train Epoch: 52 [62592/225000 (28%)] Loss: 20226.285156\n",
      "Train Epoch: 52 [65088/225000 (29%)] Loss: 20377.615234\n",
      "Train Epoch: 52 [67584/225000 (30%)] Loss: 20190.078125\n",
      "Train Epoch: 52 [70080/225000 (31%)] Loss: 20164.220703\n",
      "Train Epoch: 52 [72576/225000 (32%)] Loss: 20802.929688\n",
      "Train Epoch: 52 [75072/225000 (33%)] Loss: 20143.951172\n",
      "Train Epoch: 52 [77568/225000 (34%)] Loss: 20298.902344\n",
      "Train Epoch: 52 [80064/225000 (36%)] Loss: 20201.156250\n",
      "Train Epoch: 52 [82560/225000 (37%)] Loss: 20274.621094\n",
      "Train Epoch: 52 [85056/225000 (38%)] Loss: 19991.570312\n",
      "Train Epoch: 52 [87552/225000 (39%)] Loss: 20135.183594\n",
      "Train Epoch: 52 [90048/225000 (40%)] Loss: 20571.144531\n",
      "Train Epoch: 52 [92544/225000 (41%)] Loss: 20638.925781\n",
      "Train Epoch: 52 [95040/225000 (42%)] Loss: 19922.570312\n",
      "Train Epoch: 52 [97536/225000 (43%)] Loss: 20329.371094\n",
      "Train Epoch: 52 [100032/225000 (44%)] Loss: 20200.457031\n",
      "Train Epoch: 52 [102528/225000 (46%)] Loss: 20232.009766\n",
      "Train Epoch: 52 [105024/225000 (47%)] Loss: 19971.802734\n",
      "Train Epoch: 52 [107520/225000 (48%)] Loss: 20025.083984\n",
      "Train Epoch: 52 [110016/225000 (49%)] Loss: 20400.845703\n",
      "Train Epoch: 52 [112512/225000 (50%)] Loss: 20143.990234\n",
      "Train Epoch: 52 [115008/225000 (51%)] Loss: 19935.261719\n",
      "Train Epoch: 52 [117504/225000 (52%)] Loss: 20204.781250\n",
      "Train Epoch: 52 [120000/225000 (53%)] Loss: 19948.539062\n",
      "Train Epoch: 52 [122496/225000 (54%)] Loss: 20069.556641\n",
      "Train Epoch: 52 [124992/225000 (56%)] Loss: 20761.955078\n",
      "Train Epoch: 52 [127488/225000 (57%)] Loss: 19947.017578\n",
      "Train Epoch: 52 [129984/225000 (58%)] Loss: 19827.814453\n",
      "Train Epoch: 52 [132480/225000 (59%)] Loss: 19996.404297\n",
      "Train Epoch: 52 [134976/225000 (60%)] Loss: 20148.183594\n",
      "Train Epoch: 52 [137472/225000 (61%)] Loss: 20260.146484\n",
      "Train Epoch: 52 [139968/225000 (62%)] Loss: 20380.265625\n",
      "Train Epoch: 52 [142464/225000 (63%)] Loss: 20362.121094\n",
      "Train Epoch: 52 [144960/225000 (64%)] Loss: 20273.623047\n",
      "Train Epoch: 52 [147456/225000 (66%)] Loss: 20501.457031\n",
      "Train Epoch: 52 [149952/225000 (67%)] Loss: 20095.060547\n",
      "Train Epoch: 52 [152448/225000 (68%)] Loss: 20188.273438\n",
      "Train Epoch: 52 [154944/225000 (69%)] Loss: 20349.876953\n",
      "Train Epoch: 52 [157440/225000 (70%)] Loss: 19820.894531\n",
      "Train Epoch: 52 [159936/225000 (71%)] Loss: 20302.937500\n",
      "Train Epoch: 52 [162432/225000 (72%)] Loss: 19978.992188\n",
      "Train Epoch: 52 [164928/225000 (73%)] Loss: 20112.412109\n",
      "Train Epoch: 52 [167424/225000 (74%)] Loss: 20319.253906\n",
      "Train Epoch: 52 [169920/225000 (76%)] Loss: 19704.937500\n",
      "Train Epoch: 52 [172416/225000 (77%)] Loss: 20015.412109\n",
      "Train Epoch: 52 [174912/225000 (78%)] Loss: 20230.980469\n",
      "Train Epoch: 52 [177408/225000 (79%)] Loss: 20007.689453\n",
      "Train Epoch: 52 [179904/225000 (80%)] Loss: 19730.462891\n",
      "Train Epoch: 52 [182400/225000 (81%)] Loss: 20319.644531\n",
      "Train Epoch: 52 [184896/225000 (82%)] Loss: 20268.576172\n",
      "Train Epoch: 52 [187392/225000 (83%)] Loss: 20010.080078\n",
      "Train Epoch: 52 [189888/225000 (84%)] Loss: 19964.363281\n",
      "Train Epoch: 52 [192384/225000 (86%)] Loss: 20161.832031\n",
      "Train Epoch: 52 [194880/225000 (87%)] Loss: 20099.210938\n",
      "Train Epoch: 52 [197376/225000 (88%)] Loss: 20295.429688\n",
      "Train Epoch: 52 [199872/225000 (89%)] Loss: 20444.453125\n",
      "Train Epoch: 52 [202368/225000 (90%)] Loss: 20490.281250\n",
      "Train Epoch: 52 [204864/225000 (91%)] Loss: 20407.380859\n",
      "Train Epoch: 52 [207360/225000 (92%)] Loss: 20147.121094\n",
      "Train Epoch: 52 [209856/225000 (93%)] Loss: 19978.796875\n",
      "Train Epoch: 52 [212352/225000 (94%)] Loss: 36379.832031\n",
      "Train Epoch: 52 [214848/225000 (95%)] Loss: 20431.746094\n",
      "Train Epoch: 52 [217344/225000 (97%)] Loss: 20156.992188\n",
      "Train Epoch: 52 [219840/225000 (98%)] Loss: 20352.429688\n",
      "Train Epoch: 52 [222336/225000 (99%)] Loss: 20446.890625\n",
      "Train Epoch: 52 [224832/225000 (100%)] Loss: 20507.429688\n",
      "    epoch          : 52\n",
      "    loss           : 20218.487061380118\n",
      "    val_loss       : 20089.621385434657\n",
      "Train Epoch: 53 [192/225000 (0%)] Loss: 19869.246094\n",
      "Train Epoch: 53 [2688/225000 (1%)] Loss: 19829.312500\n",
      "Train Epoch: 53 [5184/225000 (2%)] Loss: 19985.753906\n",
      "Train Epoch: 53 [7680/225000 (3%)] Loss: 20148.125000\n",
      "Train Epoch: 53 [10176/225000 (5%)] Loss: 20410.035156\n",
      "Train Epoch: 53 [12672/225000 (6%)] Loss: 20323.917969\n",
      "Train Epoch: 53 [15168/225000 (7%)] Loss: 20167.546875\n",
      "Train Epoch: 53 [17664/225000 (8%)] Loss: 20456.355469\n",
      "Train Epoch: 53 [20160/225000 (9%)] Loss: 20475.535156\n",
      "Train Epoch: 53 [22656/225000 (10%)] Loss: 20078.906250\n",
      "Train Epoch: 53 [25152/225000 (11%)] Loss: 20588.242188\n",
      "Train Epoch: 53 [27648/225000 (12%)] Loss: 20294.425781\n",
      "Train Epoch: 53 [30144/225000 (13%)] Loss: 20482.214844\n",
      "Train Epoch: 53 [32640/225000 (15%)] Loss: 20292.824219\n",
      "Train Epoch: 53 [35136/225000 (16%)] Loss: 20254.142578\n",
      "Train Epoch: 53 [37632/225000 (17%)] Loss: 20178.074219\n",
      "Train Epoch: 53 [40128/225000 (18%)] Loss: 19974.218750\n",
      "Train Epoch: 53 [42624/225000 (19%)] Loss: 20140.115234\n",
      "Train Epoch: 53 [45120/225000 (20%)] Loss: 20316.886719\n",
      "Train Epoch: 53 [47616/225000 (21%)] Loss: 20230.910156\n",
      "Train Epoch: 53 [50112/225000 (22%)] Loss: 20153.412109\n",
      "Train Epoch: 53 [52608/225000 (23%)] Loss: 20354.447266\n",
      "Train Epoch: 53 [55104/225000 (24%)] Loss: 20259.582031\n",
      "Train Epoch: 53 [57600/225000 (26%)] Loss: 20462.902344\n",
      "Train Epoch: 53 [60096/225000 (27%)] Loss: 20332.310547\n",
      "Train Epoch: 53 [62592/225000 (28%)] Loss: 20116.832031\n",
      "Train Epoch: 53 [65088/225000 (29%)] Loss: 19921.328125\n",
      "Train Epoch: 53 [67584/225000 (30%)] Loss: 20279.009766\n",
      "Train Epoch: 53 [70080/225000 (31%)] Loss: 20343.697266\n",
      "Train Epoch: 53 [72576/225000 (32%)] Loss: 20515.710938\n",
      "Train Epoch: 53 [75072/225000 (33%)] Loss: 20113.015625\n",
      "Train Epoch: 53 [77568/225000 (34%)] Loss: 19938.722656\n",
      "Train Epoch: 53 [80064/225000 (36%)] Loss: 20126.878906\n",
      "Train Epoch: 53 [82560/225000 (37%)] Loss: 20251.777344\n",
      "Train Epoch: 53 [85056/225000 (38%)] Loss: 20462.089844\n",
      "Train Epoch: 53 [87552/225000 (39%)] Loss: 20036.183594\n",
      "Train Epoch: 53 [90048/225000 (40%)] Loss: 20167.267578\n",
      "Train Epoch: 53 [92544/225000 (41%)] Loss: 20045.093750\n",
      "Train Epoch: 53 [95040/225000 (42%)] Loss: 20945.265625\n",
      "Train Epoch: 53 [97536/225000 (43%)] Loss: 19952.714844\n",
      "Train Epoch: 53 [100032/225000 (44%)] Loss: 20314.074219\n",
      "Train Epoch: 53 [102528/225000 (46%)] Loss: 19783.238281\n",
      "Train Epoch: 53 [105024/225000 (47%)] Loss: 19845.132812\n",
      "Train Epoch: 53 [107520/225000 (48%)] Loss: 20116.498047\n",
      "Train Epoch: 53 [110016/225000 (49%)] Loss: 20055.390625\n",
      "Train Epoch: 53 [112512/225000 (50%)] Loss: 20076.464844\n",
      "Train Epoch: 53 [115008/225000 (51%)] Loss: 19828.742188\n",
      "Train Epoch: 53 [117504/225000 (52%)] Loss: 20771.931641\n",
      "Train Epoch: 53 [120000/225000 (53%)] Loss: 19994.285156\n",
      "Train Epoch: 53 [122496/225000 (54%)] Loss: 20160.406250\n",
      "Train Epoch: 53 [124992/225000 (56%)] Loss: 20067.804688\n",
      "Train Epoch: 53 [127488/225000 (57%)] Loss: 20027.101562\n",
      "Train Epoch: 53 [129984/225000 (58%)] Loss: 20357.296875\n",
      "Train Epoch: 53 [132480/225000 (59%)] Loss: 20137.144531\n",
      "Train Epoch: 53 [134976/225000 (60%)] Loss: 20114.261719\n",
      "Train Epoch: 53 [137472/225000 (61%)] Loss: 20254.187500\n",
      "Train Epoch: 53 [139968/225000 (62%)] Loss: 20297.175781\n",
      "Train Epoch: 53 [142464/225000 (63%)] Loss: 19887.714844\n",
      "Train Epoch: 53 [144960/225000 (64%)] Loss: 20391.214844\n",
      "Train Epoch: 53 [147456/225000 (66%)] Loss: 20137.414062\n",
      "Train Epoch: 53 [149952/225000 (67%)] Loss: 20245.253906\n",
      "Train Epoch: 53 [152448/225000 (68%)] Loss: 20694.826172\n",
      "Train Epoch: 53 [154944/225000 (69%)] Loss: 20376.308594\n",
      "Train Epoch: 53 [157440/225000 (70%)] Loss: 19915.421875\n",
      "Train Epoch: 53 [159936/225000 (71%)] Loss: 19726.666016\n",
      "Train Epoch: 53 [162432/225000 (72%)] Loss: 20390.044922\n",
      "Train Epoch: 53 [164928/225000 (73%)] Loss: 20695.824219\n",
      "Train Epoch: 53 [167424/225000 (74%)] Loss: 19819.078125\n",
      "Train Epoch: 53 [169920/225000 (76%)] Loss: 20055.429688\n",
      "Train Epoch: 53 [172416/225000 (77%)] Loss: 20503.146484\n",
      "Train Epoch: 53 [174912/225000 (78%)] Loss: 20798.679688\n",
      "Train Epoch: 53 [177408/225000 (79%)] Loss: 20270.593750\n",
      "Train Epoch: 53 [179904/225000 (80%)] Loss: 19964.402344\n",
      "Train Epoch: 53 [182400/225000 (81%)] Loss: 19952.039062\n",
      "Train Epoch: 53 [184896/225000 (82%)] Loss: 20089.953125\n",
      "Train Epoch: 53 [187392/225000 (83%)] Loss: 20187.785156\n",
      "Train Epoch: 53 [189888/225000 (84%)] Loss: 20561.410156\n",
      "Train Epoch: 53 [192384/225000 (86%)] Loss: 20353.046875\n",
      "Train Epoch: 53 [194880/225000 (87%)] Loss: 20148.962891\n",
      "Train Epoch: 53 [197376/225000 (88%)] Loss: 20504.582031\n",
      "Train Epoch: 53 [199872/225000 (89%)] Loss: 19991.880859\n",
      "Train Epoch: 53 [202368/225000 (90%)] Loss: 20195.650391\n",
      "Train Epoch: 53 [204864/225000 (91%)] Loss: 20508.896484\n",
      "Train Epoch: 53 [207360/225000 (92%)] Loss: 20919.183594\n",
      "Train Epoch: 53 [209856/225000 (93%)] Loss: 20419.453125\n",
      "Train Epoch: 53 [212352/225000 (94%)] Loss: 20223.257812\n",
      "Train Epoch: 53 [214848/225000 (95%)] Loss: 20272.531250\n",
      "Train Epoch: 53 [217344/225000 (97%)] Loss: 20032.613281\n",
      "Train Epoch: 53 [219840/225000 (98%)] Loss: 20388.046875\n",
      "Train Epoch: 53 [222336/225000 (99%)] Loss: 19716.978516\n",
      "Train Epoch: 53 [224832/225000 (100%)] Loss: 19755.455078\n",
      "    epoch          : 53\n",
      "    loss           : 20183.795281836603\n",
      "    val_loss       : 20076.837777173245\n",
      "Train Epoch: 54 [192/225000 (0%)] Loss: 20114.166016\n",
      "Train Epoch: 54 [2688/225000 (1%)] Loss: 20023.921875\n",
      "Train Epoch: 54 [5184/225000 (2%)] Loss: 20232.378906\n",
      "Train Epoch: 54 [7680/225000 (3%)] Loss: 20570.544922\n",
      "Train Epoch: 54 [10176/225000 (5%)] Loss: 19847.921875\n",
      "Train Epoch: 54 [12672/225000 (6%)] Loss: 20283.574219\n",
      "Train Epoch: 54 [15168/225000 (7%)] Loss: 20243.324219\n",
      "Train Epoch: 54 [17664/225000 (8%)] Loss: 20181.398438\n",
      "Train Epoch: 54 [20160/225000 (9%)] Loss: 20230.656250\n",
      "Train Epoch: 54 [22656/225000 (10%)] Loss: 20260.789062\n",
      "Train Epoch: 54 [25152/225000 (11%)] Loss: 19941.640625\n",
      "Train Epoch: 54 [27648/225000 (12%)] Loss: 20187.166016\n",
      "Train Epoch: 54 [30144/225000 (13%)] Loss: 20030.386719\n",
      "Train Epoch: 54 [32640/225000 (15%)] Loss: 20078.488281\n",
      "Train Epoch: 54 [35136/225000 (16%)] Loss: 20661.226562\n",
      "Train Epoch: 54 [37632/225000 (17%)] Loss: 20479.832031\n",
      "Train Epoch: 54 [40128/225000 (18%)] Loss: 20167.867188\n",
      "Train Epoch: 54 [42624/225000 (19%)] Loss: 20116.691406\n",
      "Train Epoch: 54 [45120/225000 (20%)] Loss: 19896.312500\n",
      "Train Epoch: 54 [47616/225000 (21%)] Loss: 20261.734375\n",
      "Train Epoch: 54 [50112/225000 (22%)] Loss: 19986.746094\n",
      "Train Epoch: 54 [52608/225000 (23%)] Loss: 19829.082031\n",
      "Train Epoch: 54 [55104/225000 (24%)] Loss: 20171.207031\n",
      "Train Epoch: 54 [57600/225000 (26%)] Loss: 20347.394531\n",
      "Train Epoch: 54 [60096/225000 (27%)] Loss: 20132.792969\n",
      "Train Epoch: 54 [62592/225000 (28%)] Loss: 20301.037109\n",
      "Train Epoch: 54 [65088/225000 (29%)] Loss: 20274.572266\n",
      "Train Epoch: 54 [67584/225000 (30%)] Loss: 20510.810547\n",
      "Train Epoch: 54 [70080/225000 (31%)] Loss: 19872.480469\n",
      "Train Epoch: 54 [72576/225000 (32%)] Loss: 20293.265625\n",
      "Train Epoch: 54 [75072/225000 (33%)] Loss: 20423.175781\n",
      "Train Epoch: 54 [77568/225000 (34%)] Loss: 19880.539062\n",
      "Train Epoch: 54 [80064/225000 (36%)] Loss: 19503.371094\n",
      "Train Epoch: 54 [82560/225000 (37%)] Loss: 19924.466797\n",
      "Train Epoch: 54 [85056/225000 (38%)] Loss: 20331.750000\n",
      "Train Epoch: 54 [87552/225000 (39%)] Loss: 20553.277344\n",
      "Train Epoch: 54 [90048/225000 (40%)] Loss: 19884.945312\n",
      "Train Epoch: 54 [92544/225000 (41%)] Loss: 20204.035156\n",
      "Train Epoch: 54 [95040/225000 (42%)] Loss: 20223.826172\n",
      "Train Epoch: 54 [97536/225000 (43%)] Loss: 20193.169922\n",
      "Train Epoch: 54 [100032/225000 (44%)] Loss: 20201.763672\n",
      "Train Epoch: 54 [102528/225000 (46%)] Loss: 19981.281250\n",
      "Train Epoch: 54 [105024/225000 (47%)] Loss: 20261.730469\n",
      "Train Epoch: 54 [107520/225000 (48%)] Loss: 20242.986328\n",
      "Train Epoch: 54 [110016/225000 (49%)] Loss: 20387.738281\n",
      "Train Epoch: 54 [112512/225000 (50%)] Loss: 20259.656250\n",
      "Train Epoch: 54 [115008/225000 (51%)] Loss: 20121.664062\n",
      "Train Epoch: 54 [117504/225000 (52%)] Loss: 19952.519531\n",
      "Train Epoch: 54 [120000/225000 (53%)] Loss: 19935.113281\n",
      "Train Epoch: 54 [122496/225000 (54%)] Loss: 20011.505859\n",
      "Train Epoch: 54 [124992/225000 (56%)] Loss: 19791.621094\n",
      "Train Epoch: 54 [127488/225000 (57%)] Loss: 20186.492188\n",
      "Train Epoch: 54 [129984/225000 (58%)] Loss: 20201.914062\n",
      "Train Epoch: 54 [132480/225000 (59%)] Loss: 20069.968750\n",
      "Train Epoch: 54 [134976/225000 (60%)] Loss: 20201.000000\n",
      "Train Epoch: 54 [137472/225000 (61%)] Loss: 20199.429688\n",
      "Train Epoch: 54 [139968/225000 (62%)] Loss: 19767.349609\n",
      "Train Epoch: 54 [142464/225000 (63%)] Loss: 20454.378906\n",
      "Train Epoch: 54 [144960/225000 (64%)] Loss: 19700.242188\n",
      "Train Epoch: 54 [147456/225000 (66%)] Loss: 19826.714844\n",
      "Train Epoch: 54 [149952/225000 (67%)] Loss: 20264.000000\n",
      "Train Epoch: 54 [152448/225000 (68%)] Loss: 20166.964844\n",
      "Train Epoch: 54 [154944/225000 (69%)] Loss: 20202.701172\n",
      "Train Epoch: 54 [157440/225000 (70%)] Loss: 20024.955078\n",
      "Train Epoch: 54 [159936/225000 (71%)] Loss: 20055.125000\n",
      "Train Epoch: 54 [162432/225000 (72%)] Loss: 20651.582031\n",
      "Train Epoch: 54 [164928/225000 (73%)] Loss: 20222.753906\n",
      "Train Epoch: 54 [167424/225000 (74%)] Loss: 20614.345703\n",
      "Train Epoch: 54 [169920/225000 (76%)] Loss: 20233.574219\n",
      "Train Epoch: 54 [172416/225000 (77%)] Loss: 19987.167969\n",
      "Train Epoch: 54 [174912/225000 (78%)] Loss: 19994.593750\n",
      "Train Epoch: 54 [177408/225000 (79%)] Loss: 20115.609375\n",
      "Train Epoch: 54 [179904/225000 (80%)] Loss: 20329.708984\n",
      "Train Epoch: 54 [182400/225000 (81%)] Loss: 20275.636719\n",
      "Train Epoch: 54 [184896/225000 (82%)] Loss: 19838.570312\n",
      "Train Epoch: 54 [187392/225000 (83%)] Loss: 20340.326172\n",
      "Train Epoch: 54 [189888/225000 (84%)] Loss: 20077.056641\n",
      "Train Epoch: 54 [192384/225000 (86%)] Loss: 20237.769531\n",
      "Train Epoch: 54 [194880/225000 (87%)] Loss: 20416.712891\n",
      "Train Epoch: 54 [197376/225000 (88%)] Loss: 19975.925781\n",
      "Train Epoch: 54 [199872/225000 (89%)] Loss: 20317.484375\n",
      "Train Epoch: 54 [202368/225000 (90%)] Loss: 20114.923828\n",
      "Train Epoch: 54 [204864/225000 (91%)] Loss: 20609.062500\n",
      "Train Epoch: 54 [207360/225000 (92%)] Loss: 20214.500000\n",
      "Train Epoch: 54 [209856/225000 (93%)] Loss: 20064.628906\n",
      "Train Epoch: 54 [212352/225000 (94%)] Loss: 19666.320312\n",
      "Train Epoch: 54 [214848/225000 (95%)] Loss: 20409.169922\n",
      "Train Epoch: 54 [217344/225000 (97%)] Loss: 19963.699219\n",
      "Train Epoch: 54 [219840/225000 (98%)] Loss: 20419.556641\n",
      "Train Epoch: 54 [222336/225000 (99%)] Loss: 19894.609375\n",
      "Train Epoch: 54 [224832/225000 (100%)] Loss: 20117.121094\n",
      "    epoch          : 54\n",
      "    loss           : 20175.495335497548\n",
      "    val_loss       : 20172.30140907255\n",
      "Train Epoch: 55 [192/225000 (0%)] Loss: 20034.769531\n",
      "Train Epoch: 55 [2688/225000 (1%)] Loss: 19988.253906\n",
      "Train Epoch: 55 [5184/225000 (2%)] Loss: 19895.591797\n",
      "Train Epoch: 55 [7680/225000 (3%)] Loss: 20024.285156\n",
      "Train Epoch: 55 [10176/225000 (5%)] Loss: 19996.345703\n",
      "Train Epoch: 55 [12672/225000 (6%)] Loss: 20204.839844\n",
      "Train Epoch: 55 [15168/225000 (7%)] Loss: 20153.062500\n",
      "Train Epoch: 55 [17664/225000 (8%)] Loss: 20204.492188\n",
      "Train Epoch: 55 [20160/225000 (9%)] Loss: 20438.664062\n",
      "Train Epoch: 55 [22656/225000 (10%)] Loss: 20788.876953\n",
      "Train Epoch: 55 [25152/225000 (11%)] Loss: 20391.117188\n",
      "Train Epoch: 55 [27648/225000 (12%)] Loss: 20514.730469\n",
      "Train Epoch: 55 [30144/225000 (13%)] Loss: 20371.035156\n",
      "Train Epoch: 55 [32640/225000 (15%)] Loss: 20380.751953\n",
      "Train Epoch: 55 [35136/225000 (16%)] Loss: 20350.365234\n",
      "Train Epoch: 55 [37632/225000 (17%)] Loss: 19666.765625\n",
      "Train Epoch: 55 [40128/225000 (18%)] Loss: 19911.287109\n",
      "Train Epoch: 55 [42624/225000 (19%)] Loss: 20732.593750\n",
      "Train Epoch: 55 [45120/225000 (20%)] Loss: 20476.820312\n",
      "Train Epoch: 55 [47616/225000 (21%)] Loss: 20194.511719\n",
      "Train Epoch: 55 [50112/225000 (22%)] Loss: 20341.898438\n",
      "Train Epoch: 55 [52608/225000 (23%)] Loss: 20326.492188\n",
      "Train Epoch: 55 [55104/225000 (24%)] Loss: 19986.832031\n",
      "Train Epoch: 55 [57600/225000 (26%)] Loss: 20714.400391\n",
      "Train Epoch: 55 [60096/225000 (27%)] Loss: 20142.312500\n",
      "Train Epoch: 55 [62592/225000 (28%)] Loss: 19956.445312\n",
      "Train Epoch: 55 [65088/225000 (29%)] Loss: 19886.166016\n",
      "Train Epoch: 55 [67584/225000 (30%)] Loss: 20238.330078\n",
      "Train Epoch: 55 [70080/225000 (31%)] Loss: 20144.406250\n",
      "Train Epoch: 55 [72576/225000 (32%)] Loss: 20082.078125\n",
      "Train Epoch: 55 [75072/225000 (33%)] Loss: 20114.023438\n",
      "Train Epoch: 55 [77568/225000 (34%)] Loss: 20774.751953\n",
      "Train Epoch: 55 [80064/225000 (36%)] Loss: 19454.593750\n",
      "Train Epoch: 55 [82560/225000 (37%)] Loss: 20241.437500\n",
      "Train Epoch: 55 [85056/225000 (38%)] Loss: 20068.853516\n",
      "Train Epoch: 55 [87552/225000 (39%)] Loss: 20050.335938\n",
      "Train Epoch: 55 [90048/225000 (40%)] Loss: 19656.917969\n",
      "Train Epoch: 55 [92544/225000 (41%)] Loss: 19730.093750\n",
      "Train Epoch: 55 [95040/225000 (42%)] Loss: 19840.406250\n",
      "Train Epoch: 55 [97536/225000 (43%)] Loss: 19654.273438\n",
      "Train Epoch: 55 [100032/225000 (44%)] Loss: 20438.648438\n",
      "Train Epoch: 55 [102528/225000 (46%)] Loss: 20128.300781\n",
      "Train Epoch: 55 [105024/225000 (47%)] Loss: 20404.693359\n",
      "Train Epoch: 55 [107520/225000 (48%)] Loss: 20055.863281\n",
      "Train Epoch: 55 [110016/225000 (49%)] Loss: 20647.699219\n",
      "Train Epoch: 55 [112512/225000 (50%)] Loss: 20342.070312\n",
      "Train Epoch: 55 [115008/225000 (51%)] Loss: 20191.000000\n",
      "Train Epoch: 55 [117504/225000 (52%)] Loss: 20635.339844\n",
      "Train Epoch: 55 [120000/225000 (53%)] Loss: 20557.210938\n",
      "Train Epoch: 55 [122496/225000 (54%)] Loss: 19832.355469\n",
      "Train Epoch: 55 [124992/225000 (56%)] Loss: 20183.298828\n",
      "Train Epoch: 55 [127488/225000 (57%)] Loss: 20380.160156\n",
      "Train Epoch: 55 [129984/225000 (58%)] Loss: 20167.595703\n",
      "Train Epoch: 55 [132480/225000 (59%)] Loss: 19919.972656\n",
      "Train Epoch: 55 [134976/225000 (60%)] Loss: 20721.183594\n",
      "Train Epoch: 55 [137472/225000 (61%)] Loss: 20283.406250\n",
      "Train Epoch: 55 [139968/225000 (62%)] Loss: 20228.201172\n",
      "Train Epoch: 55 [142464/225000 (63%)] Loss: 20371.546875\n",
      "Train Epoch: 55 [144960/225000 (64%)] Loss: 20024.121094\n",
      "Train Epoch: 55 [147456/225000 (66%)] Loss: 20775.285156\n",
      "Train Epoch: 55 [149952/225000 (67%)] Loss: 19945.824219\n",
      "Train Epoch: 55 [152448/225000 (68%)] Loss: 20132.031250\n",
      "Train Epoch: 55 [154944/225000 (69%)] Loss: 19982.875000\n",
      "Train Epoch: 55 [157440/225000 (70%)] Loss: 19841.289062\n",
      "Train Epoch: 55 [159936/225000 (71%)] Loss: 20575.890625\n",
      "Train Epoch: 55 [162432/225000 (72%)] Loss: 20129.132812\n",
      "Train Epoch: 55 [164928/225000 (73%)] Loss: 20454.955078\n",
      "Train Epoch: 55 [167424/225000 (74%)] Loss: 19679.947266\n",
      "Train Epoch: 55 [169920/225000 (76%)] Loss: 19888.246094\n",
      "Train Epoch: 55 [172416/225000 (77%)] Loss: 19893.734375\n",
      "Train Epoch: 55 [174912/225000 (78%)] Loss: 19838.671875\n",
      "Train Epoch: 55 [177408/225000 (79%)] Loss: 20302.199219\n",
      "Train Epoch: 55 [179904/225000 (80%)] Loss: 20366.597656\n",
      "Train Epoch: 55 [182400/225000 (81%)] Loss: 20517.121094\n",
      "Train Epoch: 55 [184896/225000 (82%)] Loss: 20136.101562\n",
      "Train Epoch: 55 [187392/225000 (83%)] Loss: 20138.046875\n",
      "Train Epoch: 55 [189888/225000 (84%)] Loss: 20097.910156\n",
      "Train Epoch: 55 [192384/225000 (86%)] Loss: 20423.597656\n",
      "Train Epoch: 55 [194880/225000 (87%)] Loss: 19558.750000\n",
      "Train Epoch: 55 [197376/225000 (88%)] Loss: 20519.937500\n",
      "Train Epoch: 55 [199872/225000 (89%)] Loss: 19808.035156\n",
      "Train Epoch: 55 [202368/225000 (90%)] Loss: 19588.035156\n",
      "Train Epoch: 55 [204864/225000 (91%)] Loss: 19925.787109\n",
      "Train Epoch: 55 [207360/225000 (92%)] Loss: 19991.447266\n",
      "Train Epoch: 55 [209856/225000 (93%)] Loss: 19828.503906\n",
      "Train Epoch: 55 [212352/225000 (94%)] Loss: 20521.728516\n",
      "Train Epoch: 55 [214848/225000 (95%)] Loss: 20314.015625\n",
      "Train Epoch: 55 [217344/225000 (97%)] Loss: 20378.707031\n",
      "Train Epoch: 55 [219840/225000 (98%)] Loss: 20564.960938\n",
      "Train Epoch: 55 [222336/225000 (99%)] Loss: 20309.908203\n",
      "Train Epoch: 55 [224832/225000 (100%)] Loss: 20301.117188\n",
      "    epoch          : 55\n",
      "    loss           : 20142.309441992853\n",
      "    val_loss       : 20093.59723822397\n",
      "Train Epoch: 56 [192/225000 (0%)] Loss: 20167.246094\n",
      "Train Epoch: 56 [2688/225000 (1%)] Loss: 19684.398438\n",
      "Train Epoch: 56 [5184/225000 (2%)] Loss: 20475.232422\n",
      "Train Epoch: 56 [7680/225000 (3%)] Loss: 20588.437500\n",
      "Train Epoch: 56 [10176/225000 (5%)] Loss: 20123.076172\n",
      "Train Epoch: 56 [12672/225000 (6%)] Loss: 19963.656250\n",
      "Train Epoch: 56 [15168/225000 (7%)] Loss: 20685.326172\n",
      "Train Epoch: 56 [17664/225000 (8%)] Loss: 20603.287109\n",
      "Train Epoch: 56 [20160/225000 (9%)] Loss: 20258.175781\n",
      "Train Epoch: 56 [22656/225000 (10%)] Loss: 20336.082031\n",
      "Train Epoch: 56 [25152/225000 (11%)] Loss: 20089.792969\n",
      "Train Epoch: 56 [27648/225000 (12%)] Loss: 19633.378906\n",
      "Train Epoch: 56 [30144/225000 (13%)] Loss: 20071.757812\n",
      "Train Epoch: 56 [32640/225000 (15%)] Loss: 20233.669922\n",
      "Train Epoch: 56 [35136/225000 (16%)] Loss: 19686.511719\n",
      "Train Epoch: 56 [37632/225000 (17%)] Loss: 19433.406250\n",
      "Train Epoch: 56 [40128/225000 (18%)] Loss: 20073.222656\n",
      "Train Epoch: 56 [42624/225000 (19%)] Loss: 20124.890625\n",
      "Train Epoch: 56 [45120/225000 (20%)] Loss: 20602.082031\n",
      "Train Epoch: 56 [47616/225000 (21%)] Loss: 20129.367188\n",
      "Train Epoch: 56 [50112/225000 (22%)] Loss: 20433.587891\n",
      "Train Epoch: 56 [52608/225000 (23%)] Loss: 19416.107422\n",
      "Train Epoch: 56 [55104/225000 (24%)] Loss: 20110.562500\n",
      "Train Epoch: 56 [57600/225000 (26%)] Loss: 20201.402344\n",
      "Train Epoch: 56 [60096/225000 (27%)] Loss: 20208.210938\n",
      "Train Epoch: 56 [62592/225000 (28%)] Loss: 20176.519531\n",
      "Train Epoch: 56 [65088/225000 (29%)] Loss: 20001.921875\n",
      "Train Epoch: 56 [67584/225000 (30%)] Loss: 20031.113281\n",
      "Train Epoch: 56 [70080/225000 (31%)] Loss: 19921.347656\n",
      "Train Epoch: 56 [72576/225000 (32%)] Loss: 20430.195312\n",
      "Train Epoch: 56 [75072/225000 (33%)] Loss: 20516.507812\n",
      "Train Epoch: 56 [77568/225000 (34%)] Loss: 19841.984375\n",
      "Train Epoch: 56 [80064/225000 (36%)] Loss: 20479.429688\n",
      "Train Epoch: 56 [82560/225000 (37%)] Loss: 19650.003906\n",
      "Train Epoch: 56 [85056/225000 (38%)] Loss: 20118.613281\n",
      "Train Epoch: 56 [87552/225000 (39%)] Loss: 20298.847656\n",
      "Train Epoch: 56 [90048/225000 (40%)] Loss: 19984.437500\n",
      "Train Epoch: 56 [92544/225000 (41%)] Loss: 19818.597656\n",
      "Train Epoch: 56 [95040/225000 (42%)] Loss: 20246.582031\n",
      "Train Epoch: 56 [97536/225000 (43%)] Loss: 20337.281250\n",
      "Train Epoch: 56 [100032/225000 (44%)] Loss: 20744.357422\n",
      "Train Epoch: 56 [102528/225000 (46%)] Loss: 20056.128906\n",
      "Train Epoch: 56 [105024/225000 (47%)] Loss: 19644.007812\n",
      "Train Epoch: 56 [107520/225000 (48%)] Loss: 20014.566406\n",
      "Train Epoch: 56 [110016/225000 (49%)] Loss: 20268.574219\n",
      "Train Epoch: 56 [112512/225000 (50%)] Loss: 20242.207031\n",
      "Train Epoch: 56 [115008/225000 (51%)] Loss: 19735.121094\n",
      "Train Epoch: 56 [117504/225000 (52%)] Loss: 20301.070312\n",
      "Train Epoch: 56 [120000/225000 (53%)] Loss: 20305.890625\n",
      "Train Epoch: 56 [122496/225000 (54%)] Loss: 19810.642578\n",
      "Train Epoch: 56 [124992/225000 (56%)] Loss: 20149.894531\n",
      "Train Epoch: 56 [127488/225000 (57%)] Loss: 20003.011719\n",
      "Train Epoch: 56 [129984/225000 (58%)] Loss: 20064.099609\n",
      "Train Epoch: 56 [132480/225000 (59%)] Loss: 19934.244141\n",
      "Train Epoch: 56 [134976/225000 (60%)] Loss: 20063.835938\n",
      "Train Epoch: 56 [137472/225000 (61%)] Loss: 19801.101562\n",
      "Train Epoch: 56 [139968/225000 (62%)] Loss: 20175.351562\n",
      "Train Epoch: 56 [142464/225000 (63%)] Loss: 19891.593750\n",
      "Train Epoch: 56 [144960/225000 (64%)] Loss: 20166.390625\n",
      "Train Epoch: 56 [147456/225000 (66%)] Loss: 19847.554688\n",
      "Train Epoch: 56 [149952/225000 (67%)] Loss: 20016.968750\n",
      "Train Epoch: 56 [152448/225000 (68%)] Loss: 20287.480469\n",
      "Train Epoch: 56 [154944/225000 (69%)] Loss: 20270.017578\n",
      "Train Epoch: 56 [157440/225000 (70%)] Loss: 20568.332031\n",
      "Train Epoch: 56 [159936/225000 (71%)] Loss: 19821.082031\n",
      "Train Epoch: 56 [162432/225000 (72%)] Loss: 19936.480469\n",
      "Train Epoch: 56 [164928/225000 (73%)] Loss: 20593.320312\n",
      "Train Epoch: 56 [167424/225000 (74%)] Loss: 19930.902344\n",
      "Train Epoch: 56 [169920/225000 (76%)] Loss: 19522.843750\n",
      "Train Epoch: 56 [172416/225000 (77%)] Loss: 19731.951172\n",
      "Train Epoch: 56 [174912/225000 (78%)] Loss: 19954.400391\n",
      "Train Epoch: 56 [177408/225000 (79%)] Loss: 20055.062500\n",
      "Train Epoch: 56 [179904/225000 (80%)] Loss: 20035.025391\n",
      "Train Epoch: 56 [182400/225000 (81%)] Loss: 20285.472656\n",
      "Train Epoch: 56 [184896/225000 (82%)] Loss: 19941.921875\n",
      "Train Epoch: 56 [187392/225000 (83%)] Loss: 19931.292969\n",
      "Train Epoch: 56 [189888/225000 (84%)] Loss: 20160.111328\n",
      "Train Epoch: 56 [192384/225000 (86%)] Loss: 20474.800781\n",
      "Train Epoch: 56 [194880/225000 (87%)] Loss: 20133.552734\n",
      "Train Epoch: 56 [197376/225000 (88%)] Loss: 20085.312500\n",
      "Train Epoch: 56 [199872/225000 (89%)] Loss: 20387.605469\n",
      "Train Epoch: 56 [202368/225000 (90%)] Loss: 20317.292969\n",
      "Train Epoch: 56 [204864/225000 (91%)] Loss: 20249.761719\n",
      "Train Epoch: 56 [207360/225000 (92%)] Loss: 20299.369141\n",
      "Train Epoch: 56 [209856/225000 (93%)] Loss: 20313.925781\n",
      "Train Epoch: 56 [212352/225000 (94%)] Loss: 20174.173828\n",
      "Train Epoch: 56 [214848/225000 (95%)] Loss: 19800.457031\n",
      "Train Epoch: 56 [217344/225000 (97%)] Loss: 20479.183594\n",
      "Train Epoch: 56 [219840/225000 (98%)] Loss: 20240.257812\n",
      "Train Epoch: 56 [222336/225000 (99%)] Loss: 19620.175781\n",
      "Train Epoch: 56 [224832/225000 (100%)] Loss: 20170.904297\n",
      "    epoch          : 56\n",
      "    loss           : 20126.740346029757\n",
      "    val_loss       : 20021.87927673518\n",
      "Train Epoch: 57 [192/225000 (0%)] Loss: 20167.660156\n",
      "Train Epoch: 57 [2688/225000 (1%)] Loss: 19914.339844\n",
      "Train Epoch: 57 [5184/225000 (2%)] Loss: 20171.285156\n",
      "Train Epoch: 57 [7680/225000 (3%)] Loss: 20190.871094\n",
      "Train Epoch: 57 [10176/225000 (5%)] Loss: 19868.482422\n",
      "Train Epoch: 57 [12672/225000 (6%)] Loss: 19559.710938\n",
      "Train Epoch: 57 [15168/225000 (7%)] Loss: 19988.072266\n",
      "Train Epoch: 57 [17664/225000 (8%)] Loss: 19941.880859\n",
      "Train Epoch: 57 [20160/225000 (9%)] Loss: 20245.964844\n",
      "Train Epoch: 57 [22656/225000 (10%)] Loss: 19779.082031\n",
      "Train Epoch: 57 [25152/225000 (11%)] Loss: 20339.449219\n",
      "Train Epoch: 57 [27648/225000 (12%)] Loss: 20064.923828\n",
      "Train Epoch: 57 [30144/225000 (13%)] Loss: 20514.355469\n",
      "Train Epoch: 57 [32640/225000 (15%)] Loss: 20071.179688\n",
      "Train Epoch: 57 [35136/225000 (16%)] Loss: 20112.806641\n",
      "Train Epoch: 57 [37632/225000 (17%)] Loss: 19712.765625\n",
      "Train Epoch: 57 [40128/225000 (18%)] Loss: 19935.714844\n",
      "Train Epoch: 57 [42624/225000 (19%)] Loss: 20610.492188\n",
      "Train Epoch: 57 [45120/225000 (20%)] Loss: 20478.089844\n",
      "Train Epoch: 57 [47616/225000 (21%)] Loss: 20046.753906\n",
      "Train Epoch: 57 [50112/225000 (22%)] Loss: 19883.898438\n",
      "Train Epoch: 57 [52608/225000 (23%)] Loss: 20246.570312\n",
      "Train Epoch: 57 [55104/225000 (24%)] Loss: 19842.539062\n",
      "Train Epoch: 57 [57600/225000 (26%)] Loss: 19656.312500\n",
      "Train Epoch: 57 [60096/225000 (27%)] Loss: 20035.632812\n",
      "Train Epoch: 57 [62592/225000 (28%)] Loss: 20017.601562\n",
      "Train Epoch: 57 [65088/225000 (29%)] Loss: 20103.689453\n",
      "Train Epoch: 57 [67584/225000 (30%)] Loss: 19965.269531\n",
      "Train Epoch: 57 [70080/225000 (31%)] Loss: 20443.484375\n",
      "Train Epoch: 57 [72576/225000 (32%)] Loss: 20066.064453\n",
      "Train Epoch: 57 [75072/225000 (33%)] Loss: 20426.832031\n",
      "Train Epoch: 57 [77568/225000 (34%)] Loss: 20313.656250\n",
      "Train Epoch: 57 [80064/225000 (36%)] Loss: 19857.289062\n",
      "Train Epoch: 57 [82560/225000 (37%)] Loss: 20175.039062\n",
      "Train Epoch: 57 [85056/225000 (38%)] Loss: 20356.015625\n",
      "Train Epoch: 57 [87552/225000 (39%)] Loss: 20151.130859\n",
      "Train Epoch: 57 [90048/225000 (40%)] Loss: 20285.082031\n",
      "Train Epoch: 57 [92544/225000 (41%)] Loss: 20241.265625\n",
      "Train Epoch: 57 [95040/225000 (42%)] Loss: 20603.824219\n",
      "Train Epoch: 57 [97536/225000 (43%)] Loss: 20436.742188\n",
      "Train Epoch: 57 [100032/225000 (44%)] Loss: 19919.597656\n",
      "Train Epoch: 57 [102528/225000 (46%)] Loss: 20232.074219\n",
      "Train Epoch: 57 [105024/225000 (47%)] Loss: 20636.375000\n",
      "Train Epoch: 57 [107520/225000 (48%)] Loss: 20992.603516\n",
      "Train Epoch: 57 [110016/225000 (49%)] Loss: 20535.597656\n",
      "Train Epoch: 57 [112512/225000 (50%)] Loss: 19662.597656\n",
      "Train Epoch: 57 [115008/225000 (51%)] Loss: 20060.845703\n",
      "Train Epoch: 57 [117504/225000 (52%)] Loss: 19879.382812\n",
      "Train Epoch: 57 [120000/225000 (53%)] Loss: 20327.458984\n",
      "Train Epoch: 57 [122496/225000 (54%)] Loss: 20287.515625\n",
      "Train Epoch: 57 [124992/225000 (56%)] Loss: 20143.875000\n",
      "Train Epoch: 57 [127488/225000 (57%)] Loss: 20328.443359\n",
      "Train Epoch: 57 [129984/225000 (58%)] Loss: 20279.500000\n",
      "Train Epoch: 57 [132480/225000 (59%)] Loss: 19993.966797\n",
      "Train Epoch: 57 [134976/225000 (60%)] Loss: 20345.839844\n",
      "Train Epoch: 57 [137472/225000 (61%)] Loss: 20542.285156\n",
      "Train Epoch: 57 [139968/225000 (62%)] Loss: 20513.691406\n",
      "Train Epoch: 57 [142464/225000 (63%)] Loss: 20134.439453\n",
      "Train Epoch: 57 [144960/225000 (64%)] Loss: 19855.033203\n",
      "Train Epoch: 57 [147456/225000 (66%)] Loss: 20287.031250\n",
      "Train Epoch: 57 [149952/225000 (67%)] Loss: 20188.173828\n",
      "Train Epoch: 57 [152448/225000 (68%)] Loss: 20261.816406\n",
      "Train Epoch: 57 [154944/225000 (69%)] Loss: 20289.226562\n",
      "Train Epoch: 57 [157440/225000 (70%)] Loss: 20129.835938\n",
      "Train Epoch: 57 [159936/225000 (71%)] Loss: 19821.574219\n",
      "Train Epoch: 57 [162432/225000 (72%)] Loss: 20401.574219\n",
      "Train Epoch: 57 [164928/225000 (73%)] Loss: 19465.589844\n",
      "Train Epoch: 57 [167424/225000 (74%)] Loss: 20070.496094\n",
      "Train Epoch: 57 [169920/225000 (76%)] Loss: 20025.640625\n",
      "Train Epoch: 57 [172416/225000 (77%)] Loss: 20303.500000\n",
      "Train Epoch: 57 [174912/225000 (78%)] Loss: 20025.035156\n",
      "Train Epoch: 57 [177408/225000 (79%)] Loss: 19975.484375\n",
      "Train Epoch: 57 [179904/225000 (80%)] Loss: 19902.542969\n",
      "Train Epoch: 57 [182400/225000 (81%)] Loss: 19975.664062\n",
      "Train Epoch: 57 [184896/225000 (82%)] Loss: 20086.859375\n",
      "Train Epoch: 57 [187392/225000 (83%)] Loss: 20319.488281\n",
      "Train Epoch: 57 [189888/225000 (84%)] Loss: 20506.720703\n",
      "Train Epoch: 57 [192384/225000 (86%)] Loss: 20511.240234\n",
      "Train Epoch: 57 [194880/225000 (87%)] Loss: 19890.359375\n",
      "Train Epoch: 57 [197376/225000 (88%)] Loss: 19906.554688\n",
      "Train Epoch: 57 [199872/225000 (89%)] Loss: 19631.892578\n",
      "Train Epoch: 57 [202368/225000 (90%)] Loss: 20425.404297\n",
      "Train Epoch: 57 [204864/225000 (91%)] Loss: 19947.925781\n",
      "Train Epoch: 57 [207360/225000 (92%)] Loss: 19885.941406\n",
      "Train Epoch: 57 [209856/225000 (93%)] Loss: 19988.753906\n",
      "Train Epoch: 57 [212352/225000 (94%)] Loss: 20300.164062\n",
      "Train Epoch: 57 [214848/225000 (95%)] Loss: 20310.421875\n",
      "Train Epoch: 57 [217344/225000 (97%)] Loss: 20284.660156\n",
      "Train Epoch: 57 [219840/225000 (98%)] Loss: 20381.699219\n",
      "Train Epoch: 57 [222336/225000 (99%)] Loss: 20480.175781\n",
      "Train Epoch: 57 [224832/225000 (100%)] Loss: 19905.134766\n",
      "    epoch          : 57\n",
      "    loss           : 20119.744207284555\n",
      "    val_loss       : 20022.737475115835\n",
      "Train Epoch: 58 [192/225000 (0%)] Loss: 19892.968750\n",
      "Train Epoch: 58 [2688/225000 (1%)] Loss: 20238.214844\n",
      "Train Epoch: 58 [5184/225000 (2%)] Loss: 20094.972656\n",
      "Train Epoch: 58 [7680/225000 (3%)] Loss: 19822.640625\n",
      "Train Epoch: 58 [10176/225000 (5%)] Loss: 20202.324219\n",
      "Train Epoch: 58 [12672/225000 (6%)] Loss: 20037.976562\n",
      "Train Epoch: 58 [15168/225000 (7%)] Loss: 20257.359375\n",
      "Train Epoch: 58 [17664/225000 (8%)] Loss: 20095.533203\n",
      "Train Epoch: 58 [20160/225000 (9%)] Loss: 20329.427734\n",
      "Train Epoch: 58 [22656/225000 (10%)] Loss: 19984.488281\n",
      "Train Epoch: 58 [25152/225000 (11%)] Loss: 19991.093750\n",
      "Train Epoch: 58 [27648/225000 (12%)] Loss: 19795.492188\n",
      "Train Epoch: 58 [30144/225000 (13%)] Loss: 20280.945312\n",
      "Train Epoch: 58 [32640/225000 (15%)] Loss: 20536.593750\n",
      "Train Epoch: 58 [35136/225000 (16%)] Loss: 20681.865234\n",
      "Train Epoch: 58 [37632/225000 (17%)] Loss: 19906.726562\n",
      "Train Epoch: 58 [40128/225000 (18%)] Loss: 20158.343750\n",
      "Train Epoch: 58 [42624/225000 (19%)] Loss: 20054.628906\n",
      "Train Epoch: 58 [45120/225000 (20%)] Loss: 20883.160156\n",
      "Train Epoch: 58 [47616/225000 (21%)] Loss: 20257.039062\n",
      "Train Epoch: 58 [50112/225000 (22%)] Loss: 20128.273438\n",
      "Train Epoch: 58 [52608/225000 (23%)] Loss: 20197.833984\n",
      "Train Epoch: 58 [55104/225000 (24%)] Loss: 20009.851562\n",
      "Train Epoch: 58 [57600/225000 (26%)] Loss: 20122.921875\n",
      "Train Epoch: 58 [60096/225000 (27%)] Loss: 20438.656250\n",
      "Train Epoch: 58 [62592/225000 (28%)] Loss: 20291.992188\n",
      "Train Epoch: 58 [65088/225000 (29%)] Loss: 19646.560547\n",
      "Train Epoch: 58 [67584/225000 (30%)] Loss: 20270.658203\n",
      "Train Epoch: 58 [70080/225000 (31%)] Loss: 20289.183594\n",
      "Train Epoch: 58 [72576/225000 (32%)] Loss: 20420.839844\n",
      "Train Epoch: 58 [75072/225000 (33%)] Loss: 19797.480469\n",
      "Train Epoch: 58 [77568/225000 (34%)] Loss: 19781.708984\n",
      "Train Epoch: 58 [80064/225000 (36%)] Loss: 20388.843750\n",
      "Train Epoch: 58 [82560/225000 (37%)] Loss: 20037.050781\n",
      "Train Epoch: 58 [85056/225000 (38%)] Loss: 19648.697266\n",
      "Train Epoch: 58 [87552/225000 (39%)] Loss: 20153.306641\n",
      "Train Epoch: 58 [90048/225000 (40%)] Loss: 19832.878906\n",
      "Train Epoch: 58 [92544/225000 (41%)] Loss: 20063.908203\n",
      "Train Epoch: 58 [95040/225000 (42%)] Loss: 19966.898438\n",
      "Train Epoch: 58 [97536/225000 (43%)] Loss: 20241.980469\n",
      "Train Epoch: 58 [100032/225000 (44%)] Loss: 19980.347656\n",
      "Train Epoch: 58 [102528/225000 (46%)] Loss: 19803.660156\n",
      "Train Epoch: 58 [105024/225000 (47%)] Loss: 20026.574219\n",
      "Train Epoch: 58 [107520/225000 (48%)] Loss: 19884.769531\n",
      "Train Epoch: 58 [110016/225000 (49%)] Loss: 19616.984375\n",
      "Train Epoch: 58 [112512/225000 (50%)] Loss: 20414.726562\n",
      "Train Epoch: 58 [115008/225000 (51%)] Loss: 19985.199219\n",
      "Train Epoch: 58 [117504/225000 (52%)] Loss: 20291.421875\n",
      "Train Epoch: 58 [120000/225000 (53%)] Loss: 19969.667969\n",
      "Train Epoch: 58 [122496/225000 (54%)] Loss: 20299.304688\n",
      "Train Epoch: 58 [124992/225000 (56%)] Loss: 20007.808594\n",
      "Train Epoch: 58 [127488/225000 (57%)] Loss: 19770.425781\n",
      "Train Epoch: 58 [129984/225000 (58%)] Loss: 20311.617188\n",
      "Train Epoch: 58 [132480/225000 (59%)] Loss: 20205.496094\n",
      "Train Epoch: 58 [134976/225000 (60%)] Loss: 20111.029297\n",
      "Train Epoch: 58 [137472/225000 (61%)] Loss: 19932.169922\n",
      "Train Epoch: 58 [139968/225000 (62%)] Loss: 20211.480469\n",
      "Train Epoch: 58 [142464/225000 (63%)] Loss: 20099.062500\n",
      "Train Epoch: 58 [144960/225000 (64%)] Loss: 20262.808594\n",
      "Train Epoch: 58 [147456/225000 (66%)] Loss: 20448.667969\n",
      "Train Epoch: 58 [149952/225000 (67%)] Loss: 20245.007812\n",
      "Train Epoch: 58 [152448/225000 (68%)] Loss: 20046.777344\n",
      "Train Epoch: 58 [154944/225000 (69%)] Loss: 19904.789062\n",
      "Train Epoch: 58 [157440/225000 (70%)] Loss: 19953.835938\n",
      "Train Epoch: 58 [159936/225000 (71%)] Loss: 19790.142578\n",
      "Train Epoch: 58 [162432/225000 (72%)] Loss: 20387.636719\n",
      "Train Epoch: 58 [164928/225000 (73%)] Loss: 20040.656250\n",
      "Train Epoch: 58 [167424/225000 (74%)] Loss: 20415.246094\n",
      "Train Epoch: 58 [169920/225000 (76%)] Loss: 20247.613281\n",
      "Train Epoch: 58 [172416/225000 (77%)] Loss: 20374.246094\n",
      "Train Epoch: 58 [174912/225000 (78%)] Loss: 20390.271484\n",
      "Train Epoch: 58 [177408/225000 (79%)] Loss: 20596.267578\n",
      "Train Epoch: 58 [179904/225000 (80%)] Loss: 19556.378906\n",
      "Train Epoch: 58 [182400/225000 (81%)] Loss: 20367.189453\n",
      "Train Epoch: 58 [184896/225000 (82%)] Loss: 20041.482422\n",
      "Train Epoch: 58 [187392/225000 (83%)] Loss: 19573.906250\n",
      "Train Epoch: 58 [189888/225000 (84%)] Loss: 20354.312500\n",
      "Train Epoch: 58 [192384/225000 (86%)] Loss: 20210.343750\n",
      "Train Epoch: 58 [194880/225000 (87%)] Loss: 20265.222656\n",
      "Train Epoch: 58 [197376/225000 (88%)] Loss: 19938.710938\n",
      "Train Epoch: 58 [199872/225000 (89%)] Loss: 19804.886719\n",
      "Train Epoch: 58 [202368/225000 (90%)] Loss: 20161.111328\n",
      "Train Epoch: 58 [204864/225000 (91%)] Loss: 19925.429688\n",
      "Train Epoch: 58 [207360/225000 (92%)] Loss: 20296.968750\n",
      "Train Epoch: 58 [209856/225000 (93%)] Loss: 19953.457031\n",
      "Train Epoch: 58 [212352/225000 (94%)] Loss: 19305.785156\n",
      "Train Epoch: 58 [214848/225000 (95%)] Loss: 20264.548828\n",
      "Train Epoch: 58 [217344/225000 (97%)] Loss: 20186.525391\n",
      "Train Epoch: 58 [219840/225000 (98%)] Loss: 20111.523438\n",
      "Train Epoch: 58 [222336/225000 (99%)] Loss: 20436.849609\n",
      "Train Epoch: 58 [224832/225000 (100%)] Loss: 20027.039062\n",
      "    epoch          : 58\n",
      "    loss           : 20106.757624186754\n",
      "    val_loss       : 20016.01089705038\n",
      "Train Epoch: 59 [192/225000 (0%)] Loss: 19820.308594\n",
      "Train Epoch: 59 [2688/225000 (1%)] Loss: 20036.183594\n",
      "Train Epoch: 59 [5184/225000 (2%)] Loss: 19812.837891\n",
      "Train Epoch: 59 [7680/225000 (3%)] Loss: 20149.359375\n",
      "Train Epoch: 59 [10176/225000 (5%)] Loss: 20613.091797\n",
      "Train Epoch: 59 [12672/225000 (6%)] Loss: 20003.097656\n",
      "Train Epoch: 59 [15168/225000 (7%)] Loss: 19959.662109\n",
      "Train Epoch: 59 [17664/225000 (8%)] Loss: 19484.359375\n",
      "Train Epoch: 59 [20160/225000 (9%)] Loss: 20020.261719\n",
      "Train Epoch: 59 [22656/225000 (10%)] Loss: 20273.875000\n",
      "Train Epoch: 59 [25152/225000 (11%)] Loss: 19960.816406\n",
      "Train Epoch: 59 [27648/225000 (12%)] Loss: 19918.972656\n",
      "Train Epoch: 59 [30144/225000 (13%)] Loss: 20154.144531\n",
      "Train Epoch: 59 [32640/225000 (15%)] Loss: 20041.875000\n",
      "Train Epoch: 59 [35136/225000 (16%)] Loss: 20224.400391\n",
      "Train Epoch: 59 [37632/225000 (17%)] Loss: 20296.541016\n",
      "Train Epoch: 59 [40128/225000 (18%)] Loss: 19903.656250\n",
      "Train Epoch: 59 [42624/225000 (19%)] Loss: 20397.439453\n",
      "Train Epoch: 59 [45120/225000 (20%)] Loss: 20808.167969\n",
      "Train Epoch: 59 [47616/225000 (21%)] Loss: 19918.785156\n",
      "Train Epoch: 59 [50112/225000 (22%)] Loss: 19971.144531\n",
      "Train Epoch: 59 [52608/225000 (23%)] Loss: 19916.003906\n",
      "Train Epoch: 59 [55104/225000 (24%)] Loss: 20183.378906\n",
      "Train Epoch: 59 [57600/225000 (26%)] Loss: 19851.462891\n",
      "Train Epoch: 59 [60096/225000 (27%)] Loss: 20077.597656\n",
      "Train Epoch: 59 [62592/225000 (28%)] Loss: 20134.066406\n",
      "Train Epoch: 59 [65088/225000 (29%)] Loss: 20212.017578\n",
      "Train Epoch: 59 [67584/225000 (30%)] Loss: 20171.191406\n",
      "Train Epoch: 59 [70080/225000 (31%)] Loss: 20408.470703\n",
      "Train Epoch: 59 [72576/225000 (32%)] Loss: 20029.203125\n",
      "Train Epoch: 59 [75072/225000 (33%)] Loss: 20099.476562\n",
      "Train Epoch: 59 [77568/225000 (34%)] Loss: 20425.613281\n",
      "Train Epoch: 59 [80064/225000 (36%)] Loss: 20001.832031\n",
      "Train Epoch: 59 [82560/225000 (37%)] Loss: 20090.671875\n",
      "Train Epoch: 59 [85056/225000 (38%)] Loss: 20282.388672\n",
      "Train Epoch: 59 [87552/225000 (39%)] Loss: 20392.570312\n",
      "Train Epoch: 59 [90048/225000 (40%)] Loss: 20184.355469\n",
      "Train Epoch: 59 [92544/225000 (41%)] Loss: 20121.816406\n",
      "Train Epoch: 59 [95040/225000 (42%)] Loss: 19734.488281\n",
      "Train Epoch: 59 [97536/225000 (43%)] Loss: 20184.781250\n",
      "Train Epoch: 59 [100032/225000 (44%)] Loss: 20068.251953\n",
      "Train Epoch: 59 [102528/225000 (46%)] Loss: 19961.167969\n",
      "Train Epoch: 59 [105024/225000 (47%)] Loss: 19874.785156\n",
      "Train Epoch: 59 [107520/225000 (48%)] Loss: 20154.062500\n",
      "Train Epoch: 59 [110016/225000 (49%)] Loss: 19282.179688\n",
      "Train Epoch: 59 [112512/225000 (50%)] Loss: 20411.291016\n",
      "Train Epoch: 59 [115008/225000 (51%)] Loss: 20271.986328\n",
      "Train Epoch: 59 [117504/225000 (52%)] Loss: 20197.382812\n",
      "Train Epoch: 59 [120000/225000 (53%)] Loss: 20229.218750\n",
      "Train Epoch: 59 [122496/225000 (54%)] Loss: 19683.457031\n",
      "Train Epoch: 59 [124992/225000 (56%)] Loss: 20320.728516\n",
      "Train Epoch: 59 [127488/225000 (57%)] Loss: 20334.494141\n",
      "Train Epoch: 59 [129984/225000 (58%)] Loss: 20402.378906\n",
      "Train Epoch: 59 [132480/225000 (59%)] Loss: 20247.000000\n",
      "Train Epoch: 59 [134976/225000 (60%)] Loss: 19953.712891\n",
      "Train Epoch: 59 [137472/225000 (61%)] Loss: 19478.804688\n",
      "Train Epoch: 59 [139968/225000 (62%)] Loss: 20210.296875\n",
      "Train Epoch: 59 [142464/225000 (63%)] Loss: 20333.677734\n",
      "Train Epoch: 59 [144960/225000 (64%)] Loss: 19774.236328\n",
      "Train Epoch: 59 [147456/225000 (66%)] Loss: 19695.552734\n",
      "Train Epoch: 59 [149952/225000 (67%)] Loss: 20314.306641\n",
      "Train Epoch: 59 [152448/225000 (68%)] Loss: 19764.445312\n",
      "Train Epoch: 59 [154944/225000 (69%)] Loss: 20019.425781\n",
      "Train Epoch: 59 [157440/225000 (70%)] Loss: 19976.910156\n",
      "Train Epoch: 59 [159936/225000 (71%)] Loss: 19924.339844\n",
      "Train Epoch: 59 [162432/225000 (72%)] Loss: 20017.771484\n",
      "Train Epoch: 59 [164928/225000 (73%)] Loss: 20066.099609\n",
      "Train Epoch: 59 [167424/225000 (74%)] Loss: 19774.707031\n",
      "Train Epoch: 59 [169920/225000 (76%)] Loss: 20040.242188\n",
      "Train Epoch: 59 [172416/225000 (77%)] Loss: 19753.660156\n",
      "Train Epoch: 59 [174912/225000 (78%)] Loss: 20071.818359\n",
      "Train Epoch: 59 [177408/225000 (79%)] Loss: 19926.566406\n",
      "Train Epoch: 59 [179904/225000 (80%)] Loss: 19695.177734\n",
      "Train Epoch: 59 [182400/225000 (81%)] Loss: 19814.398438\n",
      "Train Epoch: 59 [184896/225000 (82%)] Loss: 19849.625000\n",
      "Train Epoch: 59 [187392/225000 (83%)] Loss: 20123.054688\n",
      "Train Epoch: 59 [189888/225000 (84%)] Loss: 20118.152344\n",
      "Train Epoch: 59 [192384/225000 (86%)] Loss: 20043.960938\n",
      "Train Epoch: 59 [194880/225000 (87%)] Loss: 20229.037109\n",
      "Train Epoch: 59 [197376/225000 (88%)] Loss: 20024.496094\n",
      "Train Epoch: 59 [199872/225000 (89%)] Loss: 20205.113281\n",
      "Train Epoch: 59 [202368/225000 (90%)] Loss: 19597.285156\n",
      "Train Epoch: 59 [204864/225000 (91%)] Loss: 20046.533203\n",
      "Train Epoch: 59 [207360/225000 (92%)] Loss: 20006.085938\n",
      "Train Epoch: 59 [209856/225000 (93%)] Loss: 20042.046875\n",
      "Train Epoch: 59 [212352/225000 (94%)] Loss: 19989.871094\n",
      "Train Epoch: 59 [214848/225000 (95%)] Loss: 20540.072266\n",
      "Train Epoch: 59 [217344/225000 (97%)] Loss: 19796.050781\n",
      "Train Epoch: 59 [219840/225000 (98%)] Loss: 20076.929688\n",
      "Train Epoch: 59 [222336/225000 (99%)] Loss: 19925.402344\n",
      "Train Epoch: 59 [224832/225000 (100%)] Loss: 20431.041016\n",
      "    epoch          : 59\n",
      "    loss           : 20093.982126906463\n",
      "    val_loss       : 19989.365207741277\n",
      "Train Epoch: 60 [192/225000 (0%)] Loss: 19803.015625\n",
      "Train Epoch: 60 [2688/225000 (1%)] Loss: 20202.710938\n",
      "Train Epoch: 60 [5184/225000 (2%)] Loss: 20035.632812\n",
      "Train Epoch: 60 [7680/225000 (3%)] Loss: 20105.855469\n",
      "Train Epoch: 60 [10176/225000 (5%)] Loss: 20427.490234\n",
      "Train Epoch: 60 [12672/225000 (6%)] Loss: 19437.035156\n",
      "Train Epoch: 60 [15168/225000 (7%)] Loss: 20188.179688\n",
      "Train Epoch: 60 [17664/225000 (8%)] Loss: 20381.916016\n",
      "Train Epoch: 60 [20160/225000 (9%)] Loss: 20094.988281\n",
      "Train Epoch: 60 [22656/225000 (10%)] Loss: 20256.980469\n",
      "Train Epoch: 60 [25152/225000 (11%)] Loss: 20172.654297\n",
      "Train Epoch: 60 [27648/225000 (12%)] Loss: 19776.070312\n",
      "Train Epoch: 60 [30144/225000 (13%)] Loss: 20087.476562\n",
      "Train Epoch: 60 [32640/225000 (15%)] Loss: 20332.707031\n",
      "Train Epoch: 60 [35136/225000 (16%)] Loss: 20516.703125\n",
      "Train Epoch: 60 [37632/225000 (17%)] Loss: 20188.824219\n",
      "Train Epoch: 60 [40128/225000 (18%)] Loss: 20530.171875\n",
      "Train Epoch: 60 [42624/225000 (19%)] Loss: 19641.792969\n",
      "Train Epoch: 60 [45120/225000 (20%)] Loss: 20049.890625\n",
      "Train Epoch: 60 [47616/225000 (21%)] Loss: 20672.304688\n",
      "Train Epoch: 60 [50112/225000 (22%)] Loss: 20090.226562\n",
      "Train Epoch: 60 [52608/225000 (23%)] Loss: 20397.480469\n",
      "Train Epoch: 60 [55104/225000 (24%)] Loss: 20447.585938\n",
      "Train Epoch: 60 [57600/225000 (26%)] Loss: 19918.453125\n",
      "Train Epoch: 60 [60096/225000 (27%)] Loss: 20016.972656\n",
      "Train Epoch: 60 [62592/225000 (28%)] Loss: 19981.671875\n",
      "Train Epoch: 60 [65088/225000 (29%)] Loss: 20064.578125\n",
      "Train Epoch: 60 [67584/225000 (30%)] Loss: 19595.781250\n",
      "Train Epoch: 60 [70080/225000 (31%)] Loss: 20058.882812\n",
      "Train Epoch: 60 [72576/225000 (32%)] Loss: 20067.740234\n",
      "Train Epoch: 60 [75072/225000 (33%)] Loss: 20392.685547\n",
      "Train Epoch: 60 [77568/225000 (34%)] Loss: 20383.894531\n",
      "Train Epoch: 60 [80064/225000 (36%)] Loss: 20233.921875\n",
      "Train Epoch: 60 [82560/225000 (37%)] Loss: 20529.921875\n",
      "Train Epoch: 60 [85056/225000 (38%)] Loss: 20300.484375\n",
      "Train Epoch: 60 [87552/225000 (39%)] Loss: 19961.685547\n",
      "Train Epoch: 60 [90048/225000 (40%)] Loss: 20160.175781\n",
      "Train Epoch: 60 [92544/225000 (41%)] Loss: 20160.882812\n",
      "Train Epoch: 60 [95040/225000 (42%)] Loss: 19515.197266\n",
      "Train Epoch: 60 [97536/225000 (43%)] Loss: 20395.404297\n",
      "Train Epoch: 60 [100032/225000 (44%)] Loss: 20357.490234\n",
      "Train Epoch: 60 [102528/225000 (46%)] Loss: 20356.099609\n",
      "Train Epoch: 60 [105024/225000 (47%)] Loss: 20057.470703\n",
      "Train Epoch: 60 [107520/225000 (48%)] Loss: 20147.425781\n",
      "Train Epoch: 60 [110016/225000 (49%)] Loss: 20145.371094\n",
      "Train Epoch: 60 [112512/225000 (50%)] Loss: 20073.769531\n",
      "Train Epoch: 60 [115008/225000 (51%)] Loss: 20332.755859\n",
      "Train Epoch: 60 [117504/225000 (52%)] Loss: 20125.443359\n",
      "Train Epoch: 60 [120000/225000 (53%)] Loss: 19957.472656\n",
      "Train Epoch: 60 [122496/225000 (54%)] Loss: 20433.578125\n",
      "Train Epoch: 60 [124992/225000 (56%)] Loss: 20409.941406\n",
      "Train Epoch: 60 [127488/225000 (57%)] Loss: 20115.886719\n",
      "Train Epoch: 60 [129984/225000 (58%)] Loss: 20311.609375\n",
      "Train Epoch: 60 [132480/225000 (59%)] Loss: 20214.953125\n",
      "Train Epoch: 60 [134976/225000 (60%)] Loss: 20257.675781\n",
      "Train Epoch: 60 [137472/225000 (61%)] Loss: 20190.406250\n",
      "Train Epoch: 60 [139968/225000 (62%)] Loss: 20265.035156\n",
      "Train Epoch: 60 [142464/225000 (63%)] Loss: 19757.869141\n",
      "Train Epoch: 60 [144960/225000 (64%)] Loss: 20144.980469\n",
      "Train Epoch: 60 [147456/225000 (66%)] Loss: 19788.416016\n",
      "Train Epoch: 60 [149952/225000 (67%)] Loss: 20325.980469\n",
      "Train Epoch: 60 [152448/225000 (68%)] Loss: 20125.595703\n",
      "Train Epoch: 60 [154944/225000 (69%)] Loss: 19584.828125\n",
      "Train Epoch: 60 [157440/225000 (70%)] Loss: 19919.810547\n",
      "Train Epoch: 60 [159936/225000 (71%)] Loss: 20578.750000\n",
      "Train Epoch: 60 [162432/225000 (72%)] Loss: 19878.099609\n",
      "Train Epoch: 60 [164928/225000 (73%)] Loss: 20245.851562\n",
      "Train Epoch: 60 [167424/225000 (74%)] Loss: 20290.392578\n",
      "Train Epoch: 60 [169920/225000 (76%)] Loss: 20087.820312\n",
      "Train Epoch: 60 [172416/225000 (77%)] Loss: 20619.191406\n",
      "Train Epoch: 60 [174912/225000 (78%)] Loss: 20159.750000\n",
      "Train Epoch: 60 [177408/225000 (79%)] Loss: 20117.406250\n",
      "Train Epoch: 60 [179904/225000 (80%)] Loss: 19974.031250\n",
      "Train Epoch: 60 [182400/225000 (81%)] Loss: 20517.230469\n",
      "Train Epoch: 60 [184896/225000 (82%)] Loss: 20529.417969\n",
      "Train Epoch: 60 [187392/225000 (83%)] Loss: 19601.650391\n",
      "Train Epoch: 60 [189888/225000 (84%)] Loss: 20302.933594\n",
      "Train Epoch: 60 [192384/225000 (86%)] Loss: 19826.154297\n",
      "Train Epoch: 60 [194880/225000 (87%)] Loss: 20143.531250\n",
      "Train Epoch: 60 [197376/225000 (88%)] Loss: 20257.328125\n",
      "Train Epoch: 60 [199872/225000 (89%)] Loss: 19813.996094\n",
      "Train Epoch: 60 [202368/225000 (90%)] Loss: 20020.238281\n",
      "Train Epoch: 60 [204864/225000 (91%)] Loss: 19499.523438\n",
      "Train Epoch: 60 [207360/225000 (92%)] Loss: 19882.648438\n",
      "Train Epoch: 60 [209856/225000 (93%)] Loss: 19971.832031\n",
      "Train Epoch: 60 [212352/225000 (94%)] Loss: 20416.699219\n",
      "Train Epoch: 60 [214848/225000 (95%)] Loss: 20652.808594\n",
      "Train Epoch: 60 [217344/225000 (97%)] Loss: 20131.714844\n",
      "Train Epoch: 60 [219840/225000 (98%)] Loss: 19810.679688\n",
      "Train Epoch: 60 [222336/225000 (99%)] Loss: 19937.939453\n",
      "Train Epoch: 60 [224832/225000 (100%)] Loss: 20587.148438\n",
      "    epoch          : 60\n",
      "    loss           : 20080.79783489761\n",
      "    val_loss       : 19978.880370578692\n",
      "Train Epoch: 61 [192/225000 (0%)] Loss: 19857.302734\n",
      "Train Epoch: 61 [2688/225000 (1%)] Loss: 20169.826172\n",
      "Train Epoch: 61 [5184/225000 (2%)] Loss: 19983.458984\n",
      "Train Epoch: 61 [7680/225000 (3%)] Loss: 19950.464844\n",
      "Train Epoch: 61 [10176/225000 (5%)] Loss: 20553.718750\n",
      "Train Epoch: 61 [12672/225000 (6%)] Loss: 19682.871094\n",
      "Train Epoch: 61 [15168/225000 (7%)] Loss: 19821.566406\n",
      "Train Epoch: 61 [17664/225000 (8%)] Loss: 19950.757812\n",
      "Train Epoch: 61 [20160/225000 (9%)] Loss: 20336.736328\n",
      "Train Epoch: 61 [22656/225000 (10%)] Loss: 19977.414062\n",
      "Train Epoch: 61 [25152/225000 (11%)] Loss: 20322.353516\n",
      "Train Epoch: 61 [27648/225000 (12%)] Loss: 19972.498047\n",
      "Train Epoch: 61 [30144/225000 (13%)] Loss: 20535.308594\n",
      "Train Epoch: 61 [32640/225000 (15%)] Loss: 20549.675781\n",
      "Train Epoch: 61 [35136/225000 (16%)] Loss: 20078.318359\n",
      "Train Epoch: 61 [37632/225000 (17%)] Loss: 20425.193359\n",
      "Train Epoch: 61 [40128/225000 (18%)] Loss: 19531.214844\n",
      "Train Epoch: 61 [42624/225000 (19%)] Loss: 20245.378906\n",
      "Train Epoch: 61 [45120/225000 (20%)] Loss: 20551.667969\n",
      "Train Epoch: 61 [47616/225000 (21%)] Loss: 19885.343750\n",
      "Train Epoch: 61 [50112/225000 (22%)] Loss: 20593.144531\n",
      "Train Epoch: 61 [52608/225000 (23%)] Loss: 19917.105469\n",
      "Train Epoch: 61 [55104/225000 (24%)] Loss: 20473.011719\n",
      "Train Epoch: 61 [57600/225000 (26%)] Loss: 19929.964844\n",
      "Train Epoch: 61 [60096/225000 (27%)] Loss: 20056.826172\n",
      "Train Epoch: 61 [62592/225000 (28%)] Loss: 19911.302734\n",
      "Train Epoch: 61 [65088/225000 (29%)] Loss: 20044.550781\n",
      "Train Epoch: 61 [67584/225000 (30%)] Loss: 20091.468750\n",
      "Train Epoch: 61 [70080/225000 (31%)] Loss: 19597.533203\n",
      "Train Epoch: 61 [72576/225000 (32%)] Loss: 20060.164062\n",
      "Train Epoch: 61 [75072/225000 (33%)] Loss: 19775.226562\n",
      "Train Epoch: 61 [77568/225000 (34%)] Loss: 19990.097656\n",
      "Train Epoch: 61 [80064/225000 (36%)] Loss: 19990.066406\n",
      "Train Epoch: 61 [82560/225000 (37%)] Loss: 20106.138672\n",
      "Train Epoch: 61 [85056/225000 (38%)] Loss: 20239.716797\n",
      "Train Epoch: 61 [87552/225000 (39%)] Loss: 19897.707031\n",
      "Train Epoch: 61 [90048/225000 (40%)] Loss: 19546.646484\n",
      "Train Epoch: 61 [92544/225000 (41%)] Loss: 19841.833984\n",
      "Train Epoch: 61 [95040/225000 (42%)] Loss: 20098.550781\n",
      "Train Epoch: 61 [97536/225000 (43%)] Loss: 20359.683594\n",
      "Train Epoch: 61 [100032/225000 (44%)] Loss: 20136.546875\n",
      "Train Epoch: 61 [102528/225000 (46%)] Loss: 20191.457031\n",
      "Train Epoch: 61 [105024/225000 (47%)] Loss: 20152.957031\n",
      "Train Epoch: 61 [107520/225000 (48%)] Loss: 19583.945312\n",
      "Train Epoch: 61 [110016/225000 (49%)] Loss: 19814.080078\n",
      "Train Epoch: 61 [112512/225000 (50%)] Loss: 19736.468750\n",
      "Train Epoch: 61 [115008/225000 (51%)] Loss: 20250.406250\n",
      "Train Epoch: 61 [117504/225000 (52%)] Loss: 19967.289062\n",
      "Train Epoch: 61 [120000/225000 (53%)] Loss: 19705.871094\n",
      "Train Epoch: 61 [122496/225000 (54%)] Loss: 19931.589844\n",
      "Train Epoch: 61 [124992/225000 (56%)] Loss: 20019.511719\n",
      "Train Epoch: 61 [127488/225000 (57%)] Loss: 19800.066406\n",
      "Train Epoch: 61 [129984/225000 (58%)] Loss: 20207.732422\n",
      "Train Epoch: 61 [132480/225000 (59%)] Loss: 19862.179688\n",
      "Train Epoch: 61 [134976/225000 (60%)] Loss: 19775.125000\n",
      "Train Epoch: 61 [137472/225000 (61%)] Loss: 20348.464844\n",
      "Train Epoch: 61 [139968/225000 (62%)] Loss: 20331.285156\n",
      "Train Epoch: 61 [142464/225000 (63%)] Loss: 20223.917969\n",
      "Train Epoch: 61 [144960/225000 (64%)] Loss: 20300.722656\n",
      "Train Epoch: 61 [147456/225000 (66%)] Loss: 20791.537109\n",
      "Train Epoch: 61 [149952/225000 (67%)] Loss: 20597.281250\n",
      "Train Epoch: 61 [152448/225000 (68%)] Loss: 20288.191406\n",
      "Train Epoch: 61 [154944/225000 (69%)] Loss: 20437.697266\n",
      "Train Epoch: 61 [157440/225000 (70%)] Loss: 19900.195312\n",
      "Train Epoch: 61 [159936/225000 (71%)] Loss: 19708.021484\n",
      "Train Epoch: 61 [162432/225000 (72%)] Loss: 20069.062500\n",
      "Train Epoch: 61 [164928/225000 (73%)] Loss: 20021.550781\n",
      "Train Epoch: 61 [167424/225000 (74%)] Loss: 19994.080078\n",
      "Train Epoch: 61 [169920/225000 (76%)] Loss: 19790.882812\n",
      "Train Epoch: 61 [172416/225000 (77%)] Loss: 19860.105469\n",
      "Train Epoch: 61 [174912/225000 (78%)] Loss: 19424.458984\n",
      "Train Epoch: 61 [177408/225000 (79%)] Loss: 19900.839844\n",
      "Train Epoch: 61 [179904/225000 (80%)] Loss: 19482.972656\n",
      "Train Epoch: 61 [182400/225000 (81%)] Loss: 20527.912109\n",
      "Train Epoch: 61 [184896/225000 (82%)] Loss: 19932.666016\n",
      "Train Epoch: 61 [187392/225000 (83%)] Loss: 19899.601562\n",
      "Train Epoch: 61 [189888/225000 (84%)] Loss: 19948.607422\n",
      "Train Epoch: 61 [192384/225000 (86%)] Loss: 20307.158203\n",
      "Train Epoch: 61 [194880/225000 (87%)] Loss: 19861.863281\n",
      "Train Epoch: 61 [197376/225000 (88%)] Loss: 20082.960938\n",
      "Train Epoch: 61 [199872/225000 (89%)] Loss: 19807.041016\n",
      "Train Epoch: 61 [202368/225000 (90%)] Loss: 19854.175781\n",
      "Train Epoch: 61 [204864/225000 (91%)] Loss: 19763.214844\n",
      "Train Epoch: 61 [207360/225000 (92%)] Loss: 19425.859375\n",
      "Train Epoch: 61 [209856/225000 (93%)] Loss: 20345.910156\n",
      "Train Epoch: 61 [212352/225000 (94%)] Loss: 20066.216797\n",
      "Train Epoch: 61 [214848/225000 (95%)] Loss: 20573.738281\n",
      "Train Epoch: 61 [217344/225000 (97%)] Loss: 19978.839844\n",
      "Train Epoch: 61 [219840/225000 (98%)] Loss: 20291.617188\n",
      "Train Epoch: 61 [222336/225000 (99%)] Loss: 20353.296875\n",
      "Train Epoch: 61 [224832/225000 (100%)] Loss: 19378.429688\n",
      "    epoch          : 61\n",
      "    loss           : 20077.682643851324\n",
      "    val_loss       : 19995.062565055512\n",
      "Train Epoch: 62 [192/225000 (0%)] Loss: 19990.457031\n",
      "Train Epoch: 62 [2688/225000 (1%)] Loss: 19902.531250\n",
      "Train Epoch: 62 [5184/225000 (2%)] Loss: 20104.580078\n",
      "Train Epoch: 62 [7680/225000 (3%)] Loss: 19725.363281\n",
      "Train Epoch: 62 [10176/225000 (5%)] Loss: 20387.572266\n",
      "Train Epoch: 62 [12672/225000 (6%)] Loss: 20277.187500\n",
      "Train Epoch: 62 [15168/225000 (7%)] Loss: 19537.480469\n",
      "Train Epoch: 62 [17664/225000 (8%)] Loss: 19553.181641\n",
      "Train Epoch: 62 [20160/225000 (9%)] Loss: 19966.292969\n",
      "Train Epoch: 62 [22656/225000 (10%)] Loss: 20635.511719\n",
      "Train Epoch: 62 [25152/225000 (11%)] Loss: 20045.443359\n",
      "Train Epoch: 62 [27648/225000 (12%)] Loss: 20258.589844\n",
      "Train Epoch: 62 [30144/225000 (13%)] Loss: 19936.765625\n",
      "Train Epoch: 62 [32640/225000 (15%)] Loss: 20202.027344\n",
      "Train Epoch: 62 [35136/225000 (16%)] Loss: 19665.257812\n",
      "Train Epoch: 62 [37632/225000 (17%)] Loss: 19699.593750\n",
      "Train Epoch: 62 [40128/225000 (18%)] Loss: 20129.296875\n",
      "Train Epoch: 62 [42624/225000 (19%)] Loss: 20200.140625\n",
      "Train Epoch: 62 [45120/225000 (20%)] Loss: 19869.044922\n",
      "Train Epoch: 62 [47616/225000 (21%)] Loss: 20018.492188\n",
      "Train Epoch: 62 [50112/225000 (22%)] Loss: 19985.841797\n",
      "Train Epoch: 62 [52608/225000 (23%)] Loss: 20136.410156\n",
      "Train Epoch: 62 [55104/225000 (24%)] Loss: 19897.658203\n",
      "Train Epoch: 62 [57600/225000 (26%)] Loss: 20063.562500\n",
      "Train Epoch: 62 [60096/225000 (27%)] Loss: 20285.304688\n",
      "Train Epoch: 62 [62592/225000 (28%)] Loss: 20343.714844\n",
      "Train Epoch: 62 [65088/225000 (29%)] Loss: 20317.443359\n",
      "Train Epoch: 62 [67584/225000 (30%)] Loss: 19580.222656\n",
      "Train Epoch: 62 [70080/225000 (31%)] Loss: 20013.375000\n",
      "Train Epoch: 62 [72576/225000 (32%)] Loss: 20354.458984\n",
      "Train Epoch: 62 [75072/225000 (33%)] Loss: 20240.689453\n",
      "Train Epoch: 62 [77568/225000 (34%)] Loss: 20348.449219\n",
      "Train Epoch: 62 [80064/225000 (36%)] Loss: 20036.199219\n",
      "Train Epoch: 62 [82560/225000 (37%)] Loss: 19980.199219\n",
      "Train Epoch: 62 [85056/225000 (38%)] Loss: 20397.152344\n",
      "Train Epoch: 62 [87552/225000 (39%)] Loss: 19879.017578\n",
      "Train Epoch: 62 [90048/225000 (40%)] Loss: 19818.445312\n",
      "Train Epoch: 62 [92544/225000 (41%)] Loss: 20434.513672\n",
      "Train Epoch: 62 [95040/225000 (42%)] Loss: 20609.507812\n",
      "Train Epoch: 62 [97536/225000 (43%)] Loss: 19779.199219\n",
      "Train Epoch: 62 [100032/225000 (44%)] Loss: 20539.679688\n",
      "Train Epoch: 62 [102528/225000 (46%)] Loss: 19906.609375\n",
      "Train Epoch: 62 [105024/225000 (47%)] Loss: 20241.058594\n",
      "Train Epoch: 62 [107520/225000 (48%)] Loss: 19609.839844\n",
      "Train Epoch: 62 [110016/225000 (49%)] Loss: 20234.871094\n",
      "Train Epoch: 62 [112512/225000 (50%)] Loss: 19842.734375\n",
      "Train Epoch: 62 [115008/225000 (51%)] Loss: 20352.781250\n",
      "Train Epoch: 62 [117504/225000 (52%)] Loss: 19717.523438\n",
      "Train Epoch: 62 [120000/225000 (53%)] Loss: 20035.890625\n",
      "Train Epoch: 62 [122496/225000 (54%)] Loss: 20445.091797\n",
      "Train Epoch: 62 [124992/225000 (56%)] Loss: 19736.750000\n",
      "Train Epoch: 62 [127488/225000 (57%)] Loss: 20177.871094\n",
      "Train Epoch: 62 [129984/225000 (58%)] Loss: 20340.113281\n",
      "Train Epoch: 62 [132480/225000 (59%)] Loss: 19841.679688\n",
      "Train Epoch: 62 [134976/225000 (60%)] Loss: 20055.929688\n",
      "Train Epoch: 62 [137472/225000 (61%)] Loss: 20164.714844\n",
      "Train Epoch: 62 [139968/225000 (62%)] Loss: 19803.121094\n",
      "Train Epoch: 62 [142464/225000 (63%)] Loss: 19735.285156\n",
      "Train Epoch: 62 [144960/225000 (64%)] Loss: 19745.105469\n",
      "Train Epoch: 62 [147456/225000 (66%)] Loss: 20168.625000\n",
      "Train Epoch: 62 [149952/225000 (67%)] Loss: 19598.867188\n",
      "Train Epoch: 62 [152448/225000 (68%)] Loss: 20007.695312\n",
      "Train Epoch: 62 [154944/225000 (69%)] Loss: 19599.843750\n",
      "Train Epoch: 62 [157440/225000 (70%)] Loss: 20739.029297\n",
      "Train Epoch: 62 [159936/225000 (71%)] Loss: 20191.197266\n",
      "Train Epoch: 62 [162432/225000 (72%)] Loss: 20518.169922\n",
      "Train Epoch: 62 [164928/225000 (73%)] Loss: 20039.037109\n",
      "Train Epoch: 62 [167424/225000 (74%)] Loss: 19940.687500\n",
      "Train Epoch: 62 [169920/225000 (76%)] Loss: 20285.880859\n",
      "Train Epoch: 62 [172416/225000 (77%)] Loss: 19820.097656\n",
      "Train Epoch: 62 [174912/225000 (78%)] Loss: 20328.775391\n",
      "Train Epoch: 62 [177408/225000 (79%)] Loss: 19931.974609\n",
      "Train Epoch: 62 [179904/225000 (80%)] Loss: 20372.466797\n",
      "Train Epoch: 62 [182400/225000 (81%)] Loss: 20366.042969\n",
      "Train Epoch: 62 [184896/225000 (82%)] Loss: 20217.617188\n",
      "Train Epoch: 62 [187392/225000 (83%)] Loss: 20364.478516\n",
      "Train Epoch: 62 [189888/225000 (84%)] Loss: 20266.720703\n",
      "Train Epoch: 62 [192384/225000 (86%)] Loss: 20016.679688\n",
      "Train Epoch: 62 [194880/225000 (87%)] Loss: 19924.953125\n",
      "Train Epoch: 62 [197376/225000 (88%)] Loss: 20179.972656\n",
      "Train Epoch: 62 [199872/225000 (89%)] Loss: 20427.931641\n",
      "Train Epoch: 62 [202368/225000 (90%)] Loss: 20334.910156\n",
      "Train Epoch: 62 [204864/225000 (91%)] Loss: 19816.953125\n",
      "Train Epoch: 62 [207360/225000 (92%)] Loss: 20137.906250\n",
      "Train Epoch: 62 [209856/225000 (93%)] Loss: 19821.535156\n",
      "Train Epoch: 62 [212352/225000 (94%)] Loss: 19858.236328\n",
      "Train Epoch: 62 [214848/225000 (95%)] Loss: 19705.011719\n",
      "Train Epoch: 62 [217344/225000 (97%)] Loss: 20348.488281\n",
      "Train Epoch: 62 [219840/225000 (98%)] Loss: 19646.404297\n",
      "Train Epoch: 62 [222336/225000 (99%)] Loss: 19832.894531\n",
      "Train Epoch: 62 [224832/225000 (100%)] Loss: 19859.660156\n",
      "    epoch          : 62\n",
      "    loss           : 20055.121793675342\n",
      "    val_loss       : 19971.817452567226\n",
      "Train Epoch: 63 [192/225000 (0%)] Loss: 19855.177734\n",
      "Train Epoch: 63 [2688/225000 (1%)] Loss: 20439.964844\n",
      "Train Epoch: 63 [5184/225000 (2%)] Loss: 19931.265625\n",
      "Train Epoch: 63 [7680/225000 (3%)] Loss: 20034.031250\n",
      "Train Epoch: 63 [10176/225000 (5%)] Loss: 19927.964844\n",
      "Train Epoch: 63 [12672/225000 (6%)] Loss: 20113.371094\n",
      "Train Epoch: 63 [15168/225000 (7%)] Loss: 19852.033203\n",
      "Train Epoch: 63 [17664/225000 (8%)] Loss: 20910.132812\n",
      "Train Epoch: 63 [20160/225000 (9%)] Loss: 19742.421875\n",
      "Train Epoch: 63 [22656/225000 (10%)] Loss: 20293.933594\n",
      "Train Epoch: 63 [25152/225000 (11%)] Loss: 20226.601562\n",
      "Train Epoch: 63 [27648/225000 (12%)] Loss: 20327.796875\n",
      "Train Epoch: 63 [30144/225000 (13%)] Loss: 19777.789062\n",
      "Train Epoch: 63 [32640/225000 (15%)] Loss: 19947.810547\n",
      "Train Epoch: 63 [35136/225000 (16%)] Loss: 20494.162109\n",
      "Train Epoch: 63 [37632/225000 (17%)] Loss: 20025.220703\n",
      "Train Epoch: 63 [40128/225000 (18%)] Loss: 20337.191406\n",
      "Train Epoch: 63 [42624/225000 (19%)] Loss: 20250.435547\n",
      "Train Epoch: 63 [45120/225000 (20%)] Loss: 20235.378906\n",
      "Train Epoch: 63 [47616/225000 (21%)] Loss: 20286.703125\n",
      "Train Epoch: 63 [50112/225000 (22%)] Loss: 20004.535156\n",
      "Train Epoch: 63 [52608/225000 (23%)] Loss: 19705.060547\n",
      "Train Epoch: 63 [55104/225000 (24%)] Loss: 20257.255859\n",
      "Train Epoch: 63 [57600/225000 (26%)] Loss: 20151.785156\n",
      "Train Epoch: 63 [60096/225000 (27%)] Loss: 20012.195312\n",
      "Train Epoch: 63 [62592/225000 (28%)] Loss: 20533.349609\n",
      "Train Epoch: 63 [65088/225000 (29%)] Loss: 20452.781250\n",
      "Train Epoch: 63 [67584/225000 (30%)] Loss: 20289.232422\n",
      "Train Epoch: 63 [70080/225000 (31%)] Loss: 19985.582031\n",
      "Train Epoch: 63 [72576/225000 (32%)] Loss: 20276.960938\n",
      "Train Epoch: 63 [75072/225000 (33%)] Loss: 19935.132812\n",
      "Train Epoch: 63 [77568/225000 (34%)] Loss: 20184.375000\n",
      "Train Epoch: 63 [80064/225000 (36%)] Loss: 19744.728516\n",
      "Train Epoch: 63 [82560/225000 (37%)] Loss: 19585.496094\n",
      "Train Epoch: 63 [85056/225000 (38%)] Loss: 19520.787109\n",
      "Train Epoch: 63 [87552/225000 (39%)] Loss: 20085.628906\n",
      "Train Epoch: 63 [90048/225000 (40%)] Loss: 19689.656250\n",
      "Train Epoch: 63 [92544/225000 (41%)] Loss: 20045.175781\n",
      "Train Epoch: 63 [95040/225000 (42%)] Loss: 19782.921875\n",
      "Train Epoch: 63 [97536/225000 (43%)] Loss: 20379.164062\n",
      "Train Epoch: 63 [100032/225000 (44%)] Loss: 19960.767578\n",
      "Train Epoch: 63 [102528/225000 (46%)] Loss: 19832.578125\n",
      "Train Epoch: 63 [105024/225000 (47%)] Loss: 19977.705078\n",
      "Train Epoch: 63 [107520/225000 (48%)] Loss: 19685.875000\n",
      "Train Epoch: 63 [110016/225000 (49%)] Loss: 20174.126953\n",
      "Train Epoch: 63 [112512/225000 (50%)] Loss: 19851.570312\n",
      "Train Epoch: 63 [115008/225000 (51%)] Loss: 20071.669922\n",
      "Train Epoch: 63 [117504/225000 (52%)] Loss: 19707.757812\n",
      "Train Epoch: 63 [120000/225000 (53%)] Loss: 19993.742188\n",
      "Train Epoch: 63 [122496/225000 (54%)] Loss: 20543.808594\n",
      "Train Epoch: 63 [124992/225000 (56%)] Loss: 19701.519531\n",
      "Train Epoch: 63 [127488/225000 (57%)] Loss: 20029.232422\n",
      "Train Epoch: 63 [129984/225000 (58%)] Loss: 19996.597656\n",
      "Train Epoch: 63 [132480/225000 (59%)] Loss: 20061.960938\n",
      "Train Epoch: 63 [134976/225000 (60%)] Loss: 20027.921875\n",
      "Train Epoch: 63 [137472/225000 (61%)] Loss: 19784.644531\n",
      "Train Epoch: 63 [139968/225000 (62%)] Loss: 19718.486328\n",
      "Train Epoch: 63 [142464/225000 (63%)] Loss: 20125.802734\n",
      "Train Epoch: 63 [144960/225000 (64%)] Loss: 19735.300781\n",
      "Train Epoch: 63 [147456/225000 (66%)] Loss: 20634.175781\n",
      "Train Epoch: 63 [149952/225000 (67%)] Loss: 19726.666016\n",
      "Train Epoch: 63 [152448/225000 (68%)] Loss: 19661.804688\n",
      "Train Epoch: 63 [154944/225000 (69%)] Loss: 20526.904297\n",
      "Train Epoch: 63 [157440/225000 (70%)] Loss: 20134.386719\n",
      "Train Epoch: 63 [159936/225000 (71%)] Loss: 20412.748047\n",
      "Train Epoch: 63 [162432/225000 (72%)] Loss: 20034.876953\n",
      "Train Epoch: 63 [164928/225000 (73%)] Loss: 19866.255859\n",
      "Train Epoch: 63 [167424/225000 (74%)] Loss: 19830.843750\n",
      "Train Epoch: 63 [169920/225000 (76%)] Loss: 20111.134766\n",
      "Train Epoch: 63 [172416/225000 (77%)] Loss: 19669.091797\n",
      "Train Epoch: 63 [174912/225000 (78%)] Loss: 19872.419922\n",
      "Train Epoch: 63 [177408/225000 (79%)] Loss: 19895.910156\n",
      "Train Epoch: 63 [179904/225000 (80%)] Loss: 19637.218750\n",
      "Train Epoch: 63 [182400/225000 (81%)] Loss: 19767.300781\n",
      "Train Epoch: 63 [184896/225000 (82%)] Loss: 19785.466797\n",
      "Train Epoch: 63 [187392/225000 (83%)] Loss: 20118.750000\n",
      "Train Epoch: 63 [189888/225000 (84%)] Loss: 20459.498047\n",
      "Train Epoch: 63 [192384/225000 (86%)] Loss: 20261.437500\n",
      "Train Epoch: 63 [194880/225000 (87%)] Loss: 20121.476562\n",
      "Train Epoch: 63 [197376/225000 (88%)] Loss: 19669.312500\n",
      "Train Epoch: 63 [199872/225000 (89%)] Loss: 19940.816406\n",
      "Train Epoch: 63 [202368/225000 (90%)] Loss: 20052.490234\n",
      "Train Epoch: 63 [204864/225000 (91%)] Loss: 20114.187500\n",
      "Train Epoch: 63 [207360/225000 (92%)] Loss: 20202.728516\n",
      "Train Epoch: 63 [209856/225000 (93%)] Loss: 20032.144531\n",
      "Train Epoch: 63 [212352/225000 (94%)] Loss: 20342.843750\n",
      "Train Epoch: 63 [214848/225000 (95%)] Loss: 20024.816406\n",
      "Train Epoch: 63 [217344/225000 (97%)] Loss: 19515.207031\n",
      "Train Epoch: 63 [219840/225000 (98%)] Loss: 20354.486328\n",
      "Train Epoch: 63 [222336/225000 (99%)] Loss: 20117.925781\n",
      "Train Epoch: 63 [224832/225000 (100%)] Loss: 20380.144531\n",
      "    epoch          : 63\n",
      "    loss           : 20032.782109908276\n",
      "    val_loss       : 19919.032272646444\n",
      "Train Epoch: 64 [192/225000 (0%)] Loss: 19946.203125\n",
      "Train Epoch: 64 [2688/225000 (1%)] Loss: 20438.324219\n",
      "Train Epoch: 64 [5184/225000 (2%)] Loss: 19935.199219\n",
      "Train Epoch: 64 [7680/225000 (3%)] Loss: 19900.957031\n",
      "Train Epoch: 64 [10176/225000 (5%)] Loss: 19809.484375\n",
      "Train Epoch: 64 [12672/225000 (6%)] Loss: 20341.458984\n",
      "Train Epoch: 64 [15168/225000 (7%)] Loss: 20359.070312\n",
      "Train Epoch: 64 [17664/225000 (8%)] Loss: 19901.140625\n",
      "Train Epoch: 64 [20160/225000 (9%)] Loss: 20341.257812\n",
      "Train Epoch: 64 [22656/225000 (10%)] Loss: 19915.128906\n",
      "Train Epoch: 64 [25152/225000 (11%)] Loss: 19796.248047\n",
      "Train Epoch: 64 [27648/225000 (12%)] Loss: 19612.414062\n",
      "Train Epoch: 64 [30144/225000 (13%)] Loss: 20089.451172\n",
      "Train Epoch: 64 [32640/225000 (15%)] Loss: 19642.890625\n",
      "Train Epoch: 64 [35136/225000 (16%)] Loss: 20092.085938\n",
      "Train Epoch: 64 [37632/225000 (17%)] Loss: 20049.292969\n",
      "Train Epoch: 64 [40128/225000 (18%)] Loss: 19962.417969\n",
      "Train Epoch: 64 [42624/225000 (19%)] Loss: 19542.925781\n",
      "Train Epoch: 64 [45120/225000 (20%)] Loss: 19867.031250\n",
      "Train Epoch: 64 [47616/225000 (21%)] Loss: 19443.228516\n",
      "Train Epoch: 64 [50112/225000 (22%)] Loss: 19994.539062\n",
      "Train Epoch: 64 [52608/225000 (23%)] Loss: 20181.082031\n",
      "Train Epoch: 64 [55104/225000 (24%)] Loss: 19625.394531\n",
      "Train Epoch: 64 [57600/225000 (26%)] Loss: 19620.589844\n",
      "Train Epoch: 64 [60096/225000 (27%)] Loss: 20026.488281\n",
      "Train Epoch: 64 [62592/225000 (28%)] Loss: 19911.878906\n",
      "Train Epoch: 64 [65088/225000 (29%)] Loss: 19972.923828\n",
      "Train Epoch: 64 [67584/225000 (30%)] Loss: 19627.062500\n",
      "Train Epoch: 64 [70080/225000 (31%)] Loss: 19616.980469\n",
      "Train Epoch: 64 [72576/225000 (32%)] Loss: 19606.632812\n",
      "Train Epoch: 64 [75072/225000 (33%)] Loss: 20364.593750\n",
      "Train Epoch: 64 [77568/225000 (34%)] Loss: 19864.304688\n",
      "Train Epoch: 64 [80064/225000 (36%)] Loss: 20077.019531\n",
      "Train Epoch: 64 [82560/225000 (37%)] Loss: 19544.714844\n",
      "Train Epoch: 64 [85056/225000 (38%)] Loss: 19756.390625\n",
      "Train Epoch: 64 [87552/225000 (39%)] Loss: 20019.691406\n",
      "Train Epoch: 64 [90048/225000 (40%)] Loss: 19482.548828\n",
      "Train Epoch: 64 [92544/225000 (41%)] Loss: 20185.853516\n",
      "Train Epoch: 64 [95040/225000 (42%)] Loss: 20269.238281\n",
      "Train Epoch: 64 [97536/225000 (43%)] Loss: 34691.203125\n",
      "Train Epoch: 64 [100032/225000 (44%)] Loss: 19784.316406\n",
      "Train Epoch: 64 [102528/225000 (46%)] Loss: 20090.001953\n",
      "Train Epoch: 64 [105024/225000 (47%)] Loss: 20180.005859\n",
      "Train Epoch: 64 [107520/225000 (48%)] Loss: 19755.417969\n",
      "Train Epoch: 64 [110016/225000 (49%)] Loss: 19649.238281\n",
      "Train Epoch: 64 [112512/225000 (50%)] Loss: 19675.828125\n",
      "Train Epoch: 64 [115008/225000 (51%)] Loss: 19979.791016\n",
      "Train Epoch: 64 [117504/225000 (52%)] Loss: 20028.031250\n",
      "Train Epoch: 64 [120000/225000 (53%)] Loss: 19590.751953\n",
      "Train Epoch: 64 [122496/225000 (54%)] Loss: 19891.761719\n",
      "Train Epoch: 64 [124992/225000 (56%)] Loss: 20146.328125\n",
      "Train Epoch: 64 [127488/225000 (57%)] Loss: 19786.921875\n",
      "Train Epoch: 64 [129984/225000 (58%)] Loss: 20030.136719\n",
      "Train Epoch: 64 [132480/225000 (59%)] Loss: 19636.804688\n",
      "Train Epoch: 64 [134976/225000 (60%)] Loss: 19934.349609\n",
      "Train Epoch: 64 [137472/225000 (61%)] Loss: 20107.453125\n",
      "Train Epoch: 64 [139968/225000 (62%)] Loss: 20489.648438\n",
      "Train Epoch: 64 [142464/225000 (63%)] Loss: 20018.847656\n",
      "Train Epoch: 64 [144960/225000 (64%)] Loss: 19710.515625\n",
      "Train Epoch: 64 [147456/225000 (66%)] Loss: 19829.429688\n",
      "Train Epoch: 64 [149952/225000 (67%)] Loss: 19729.843750\n",
      "Train Epoch: 64 [152448/225000 (68%)] Loss: 19856.300781\n",
      "Train Epoch: 64 [154944/225000 (69%)] Loss: 19965.691406\n",
      "Train Epoch: 64 [157440/225000 (70%)] Loss: 20105.406250\n",
      "Train Epoch: 64 [159936/225000 (71%)] Loss: 19631.250000\n",
      "Train Epoch: 64 [162432/225000 (72%)] Loss: 19324.796875\n",
      "Train Epoch: 64 [164928/225000 (73%)] Loss: 19812.597656\n",
      "Train Epoch: 64 [167424/225000 (74%)] Loss: 19898.554688\n",
      "Train Epoch: 64 [169920/225000 (76%)] Loss: 19750.033203\n",
      "Train Epoch: 64 [172416/225000 (77%)] Loss: 19564.808594\n",
      "Train Epoch: 64 [174912/225000 (78%)] Loss: 19910.203125\n",
      "Train Epoch: 64 [177408/225000 (79%)] Loss: 20514.726562\n",
      "Train Epoch: 64 [179904/225000 (80%)] Loss: 20310.136719\n",
      "Train Epoch: 64 [182400/225000 (81%)] Loss: 20101.511719\n",
      "Train Epoch: 64 [184896/225000 (82%)] Loss: 20141.105469\n",
      "Train Epoch: 64 [187392/225000 (83%)] Loss: 20309.949219\n",
      "Train Epoch: 64 [189888/225000 (84%)] Loss: 20526.066406\n",
      "Train Epoch: 64 [192384/225000 (86%)] Loss: 19836.644531\n",
      "Train Epoch: 64 [194880/225000 (87%)] Loss: 20210.078125\n",
      "Train Epoch: 64 [197376/225000 (88%)] Loss: 19441.136719\n",
      "Train Epoch: 64 [199872/225000 (89%)] Loss: 20084.636719\n",
      "Train Epoch: 64 [202368/225000 (90%)] Loss: 19832.630859\n",
      "Train Epoch: 64 [204864/225000 (91%)] Loss: 19652.779297\n",
      "Train Epoch: 64 [207360/225000 (92%)] Loss: 19764.007812\n",
      "Train Epoch: 64 [209856/225000 (93%)] Loss: 19854.447266\n",
      "Train Epoch: 64 [212352/225000 (94%)] Loss: 20167.902344\n",
      "Train Epoch: 64 [214848/225000 (95%)] Loss: 19935.007812\n",
      "Train Epoch: 64 [217344/225000 (97%)] Loss: 19583.732422\n",
      "Train Epoch: 64 [219840/225000 (98%)] Loss: 20051.265625\n",
      "Train Epoch: 64 [222336/225000 (99%)] Loss: 19658.164062\n",
      "Train Epoch: 64 [224832/225000 (100%)] Loss: 19887.880859\n",
      "    epoch          : 64\n",
      "    loss           : 19997.91774044102\n",
      "    val_loss       : 19868.80508492648\n",
      "Train Epoch: 65 [192/225000 (0%)] Loss: 19713.765625\n",
      "Train Epoch: 65 [2688/225000 (1%)] Loss: 20146.679688\n",
      "Train Epoch: 65 [5184/225000 (2%)] Loss: 19966.886719\n",
      "Train Epoch: 65 [7680/225000 (3%)] Loss: 19549.927734\n",
      "Train Epoch: 65 [10176/225000 (5%)] Loss: 20290.464844\n",
      "Train Epoch: 65 [12672/225000 (6%)] Loss: 19842.677734\n",
      "Train Epoch: 65 [15168/225000 (7%)] Loss: 19735.921875\n",
      "Train Epoch: 65 [17664/225000 (8%)] Loss: 20013.656250\n",
      "Train Epoch: 65 [20160/225000 (9%)] Loss: 20293.136719\n",
      "Train Epoch: 65 [22656/225000 (10%)] Loss: 19895.363281\n",
      "Train Epoch: 65 [25152/225000 (11%)] Loss: 19524.242188\n",
      "Train Epoch: 65 [27648/225000 (12%)] Loss: 20294.578125\n",
      "Train Epoch: 65 [30144/225000 (13%)] Loss: 19976.148438\n",
      "Train Epoch: 65 [32640/225000 (15%)] Loss: 20108.273438\n",
      "Train Epoch: 65 [35136/225000 (16%)] Loss: 19711.214844\n",
      "Train Epoch: 65 [37632/225000 (17%)] Loss: 19489.582031\n",
      "Train Epoch: 65 [40128/225000 (18%)] Loss: 20004.109375\n",
      "Train Epoch: 65 [42624/225000 (19%)] Loss: 20153.906250\n",
      "Train Epoch: 65 [45120/225000 (20%)] Loss: 19892.363281\n",
      "Train Epoch: 65 [47616/225000 (21%)] Loss: 20017.800781\n",
      "Train Epoch: 65 [50112/225000 (22%)] Loss: 19765.492188\n",
      "Train Epoch: 65 [52608/225000 (23%)] Loss: 19622.089844\n",
      "Train Epoch: 65 [55104/225000 (24%)] Loss: 19510.484375\n",
      "Train Epoch: 65 [57600/225000 (26%)] Loss: 19963.914062\n",
      "Train Epoch: 65 [60096/225000 (27%)] Loss: 19828.355469\n",
      "Train Epoch: 65 [62592/225000 (28%)] Loss: 19873.080078\n",
      "Train Epoch: 65 [65088/225000 (29%)] Loss: 19756.183594\n",
      "Train Epoch: 65 [67584/225000 (30%)] Loss: 19683.929688\n",
      "Train Epoch: 65 [70080/225000 (31%)] Loss: 19717.937500\n",
      "Train Epoch: 65 [72576/225000 (32%)] Loss: 20009.285156\n",
      "Train Epoch: 65 [75072/225000 (33%)] Loss: 19884.273438\n",
      "Train Epoch: 65 [77568/225000 (34%)] Loss: 19733.148438\n",
      "Train Epoch: 65 [80064/225000 (36%)] Loss: 19839.529297\n",
      "Train Epoch: 65 [82560/225000 (37%)] Loss: 20052.679688\n",
      "Train Epoch: 65 [85056/225000 (38%)] Loss: 19891.410156\n",
      "Train Epoch: 65 [87552/225000 (39%)] Loss: 20511.412109\n",
      "Train Epoch: 65 [90048/225000 (40%)] Loss: 19753.726562\n",
      "Train Epoch: 65 [92544/225000 (41%)] Loss: 19657.355469\n",
      "Train Epoch: 65 [95040/225000 (42%)] Loss: 19365.621094\n",
      "Train Epoch: 65 [97536/225000 (43%)] Loss: 20114.312500\n",
      "Train Epoch: 65 [100032/225000 (44%)] Loss: 19469.597656\n",
      "Train Epoch: 65 [102528/225000 (46%)] Loss: 19784.365234\n",
      "Train Epoch: 65 [105024/225000 (47%)] Loss: 19745.128906\n",
      "Train Epoch: 65 [107520/225000 (48%)] Loss: 19939.388672\n",
      "Train Epoch: 65 [110016/225000 (49%)] Loss: 20242.605469\n",
      "Train Epoch: 65 [112512/225000 (50%)] Loss: 19904.355469\n",
      "Train Epoch: 65 [115008/225000 (51%)] Loss: 19596.796875\n",
      "Train Epoch: 65 [117504/225000 (52%)] Loss: 19674.753906\n",
      "Train Epoch: 65 [120000/225000 (53%)] Loss: 20191.505859\n",
      "Train Epoch: 65 [122496/225000 (54%)] Loss: 20185.699219\n",
      "Train Epoch: 65 [124992/225000 (56%)] Loss: 19634.216797\n",
      "Train Epoch: 65 [127488/225000 (57%)] Loss: 20133.818359\n",
      "Train Epoch: 65 [129984/225000 (58%)] Loss: 19633.902344\n",
      "Train Epoch: 65 [132480/225000 (59%)] Loss: 20357.218750\n",
      "Train Epoch: 65 [134976/225000 (60%)] Loss: 19629.101562\n",
      "Train Epoch: 65 [137472/225000 (61%)] Loss: 20174.763672\n",
      "Train Epoch: 65 [139968/225000 (62%)] Loss: 19924.078125\n",
      "Train Epoch: 65 [142464/225000 (63%)] Loss: 19965.062500\n",
      "Train Epoch: 65 [144960/225000 (64%)] Loss: 19908.640625\n",
      "Train Epoch: 65 [147456/225000 (66%)] Loss: 20148.906250\n",
      "Train Epoch: 65 [149952/225000 (67%)] Loss: 20114.248047\n",
      "Train Epoch: 65 [152448/225000 (68%)] Loss: 20081.070312\n",
      "Train Epoch: 65 [154944/225000 (69%)] Loss: 19584.507812\n",
      "Train Epoch: 65 [157440/225000 (70%)] Loss: 19789.146484\n",
      "Train Epoch: 65 [159936/225000 (71%)] Loss: 19868.105469\n",
      "Train Epoch: 65 [162432/225000 (72%)] Loss: 20462.718750\n",
      "Train Epoch: 65 [164928/225000 (73%)] Loss: 20092.835938\n",
      "Train Epoch: 65 [167424/225000 (74%)] Loss: 19948.503906\n",
      "Train Epoch: 65 [169920/225000 (76%)] Loss: 19895.699219\n",
      "Train Epoch: 65 [172416/225000 (77%)] Loss: 20091.121094\n",
      "Train Epoch: 65 [174912/225000 (78%)] Loss: 19277.398438\n",
      "Train Epoch: 65 [177408/225000 (79%)] Loss: 20085.023438\n",
      "Train Epoch: 65 [179904/225000 (80%)] Loss: 19830.611328\n",
      "Train Epoch: 65 [182400/225000 (81%)] Loss: 19658.339844\n",
      "Train Epoch: 65 [184896/225000 (82%)] Loss: 19815.767578\n",
      "Train Epoch: 65 [187392/225000 (83%)] Loss: 19672.173828\n",
      "Train Epoch: 65 [189888/225000 (84%)] Loss: 19619.972656\n",
      "Train Epoch: 65 [192384/225000 (86%)] Loss: 19605.007812\n",
      "Train Epoch: 65 [194880/225000 (87%)] Loss: 20207.707031\n",
      "Train Epoch: 65 [197376/225000 (88%)] Loss: 20269.675781\n",
      "Train Epoch: 65 [199872/225000 (89%)] Loss: 19821.556641\n",
      "Train Epoch: 65 [202368/225000 (90%)] Loss: 19376.082031\n",
      "Train Epoch: 65 [204864/225000 (91%)] Loss: 20015.453125\n",
      "Train Epoch: 65 [207360/225000 (92%)] Loss: 19670.226562\n",
      "Train Epoch: 65 [209856/225000 (93%)] Loss: 19930.185547\n",
      "Train Epoch: 65 [212352/225000 (94%)] Loss: 20479.539062\n",
      "Train Epoch: 65 [214848/225000 (95%)] Loss: 19830.039062\n",
      "Train Epoch: 65 [217344/225000 (97%)] Loss: 19418.734375\n",
      "Train Epoch: 65 [219840/225000 (98%)] Loss: 20222.660156\n",
      "Train Epoch: 65 [222336/225000 (99%)] Loss: 20625.847656\n",
      "Train Epoch: 65 [224832/225000 (100%)] Loss: 19813.328125\n",
      "    epoch          : 65\n",
      "    loss           : 19952.924701365188\n",
      "    val_loss       : 19829.61776775986\n",
      "Train Epoch: 66 [192/225000 (0%)] Loss: 19130.757812\n",
      "Train Epoch: 66 [2688/225000 (1%)] Loss: 19840.785156\n",
      "Train Epoch: 66 [5184/225000 (2%)] Loss: 19171.095703\n",
      "Train Epoch: 66 [7680/225000 (3%)] Loss: 20024.849609\n",
      "Train Epoch: 66 [10176/225000 (5%)] Loss: 20107.218750\n",
      "Train Epoch: 66 [12672/225000 (6%)] Loss: 19426.093750\n",
      "Train Epoch: 66 [15168/225000 (7%)] Loss: 20007.822266\n",
      "Train Epoch: 66 [17664/225000 (8%)] Loss: 19806.972656\n",
      "Train Epoch: 66 [20160/225000 (9%)] Loss: 20373.304688\n",
      "Train Epoch: 66 [22656/225000 (10%)] Loss: 19543.316406\n",
      "Train Epoch: 66 [25152/225000 (11%)] Loss: 19954.425781\n",
      "Train Epoch: 66 [27648/225000 (12%)] Loss: 19819.150391\n",
      "Train Epoch: 66 [30144/225000 (13%)] Loss: 20040.132812\n",
      "Train Epoch: 66 [32640/225000 (15%)] Loss: 20089.541016\n",
      "Train Epoch: 66 [35136/225000 (16%)] Loss: 20080.078125\n",
      "Train Epoch: 66 [37632/225000 (17%)] Loss: 20076.859375\n",
      "Train Epoch: 66 [40128/225000 (18%)] Loss: 19983.843750\n",
      "Train Epoch: 66 [42624/225000 (19%)] Loss: 19827.111328\n",
      "Train Epoch: 66 [45120/225000 (20%)] Loss: 20056.541016\n",
      "Train Epoch: 66 [47616/225000 (21%)] Loss: 19954.484375\n",
      "Train Epoch: 66 [50112/225000 (22%)] Loss: 19817.578125\n",
      "Train Epoch: 66 [52608/225000 (23%)] Loss: 19881.849609\n",
      "Train Epoch: 66 [55104/225000 (24%)] Loss: 19574.765625\n",
      "Train Epoch: 66 [57600/225000 (26%)] Loss: 19813.500000\n",
      "Train Epoch: 66 [60096/225000 (27%)] Loss: 19494.753906\n",
      "Train Epoch: 66 [62592/225000 (28%)] Loss: 20015.189453\n",
      "Train Epoch: 66 [65088/225000 (29%)] Loss: 19809.513672\n",
      "Train Epoch: 66 [67584/225000 (30%)] Loss: 19708.367188\n",
      "Train Epoch: 66 [70080/225000 (31%)] Loss: 20344.460938\n",
      "Train Epoch: 66 [72576/225000 (32%)] Loss: 19566.083984\n",
      "Train Epoch: 66 [75072/225000 (33%)] Loss: 19600.519531\n",
      "Train Epoch: 66 [77568/225000 (34%)] Loss: 19852.488281\n",
      "Train Epoch: 66 [80064/225000 (36%)] Loss: 19861.031250\n",
      "Train Epoch: 66 [82560/225000 (37%)] Loss: 19973.357422\n",
      "Train Epoch: 66 [85056/225000 (38%)] Loss: 19920.593750\n",
      "Train Epoch: 66 [87552/225000 (39%)] Loss: 19578.580078\n",
      "Train Epoch: 66 [90048/225000 (40%)] Loss: 19919.039062\n",
      "Train Epoch: 66 [92544/225000 (41%)] Loss: 19731.738281\n",
      "Train Epoch: 66 [95040/225000 (42%)] Loss: 20010.355469\n",
      "Train Epoch: 66 [97536/225000 (43%)] Loss: 20006.695312\n",
      "Train Epoch: 66 [100032/225000 (44%)] Loss: 19561.246094\n",
      "Train Epoch: 66 [102528/225000 (46%)] Loss: 19952.753906\n",
      "Train Epoch: 66 [105024/225000 (47%)] Loss: 19429.621094\n",
      "Train Epoch: 66 [107520/225000 (48%)] Loss: 20093.914062\n",
      "Train Epoch: 66 [110016/225000 (49%)] Loss: 19954.734375\n",
      "Train Epoch: 66 [112512/225000 (50%)] Loss: 20052.529297\n",
      "Train Epoch: 66 [115008/225000 (51%)] Loss: 19924.253906\n",
      "Train Epoch: 66 [117504/225000 (52%)] Loss: 20508.486328\n",
      "Train Epoch: 66 [120000/225000 (53%)] Loss: 19823.388672\n",
      "Train Epoch: 66 [122496/225000 (54%)] Loss: 20287.476562\n",
      "Train Epoch: 66 [124992/225000 (56%)] Loss: 20104.828125\n",
      "Train Epoch: 66 [127488/225000 (57%)] Loss: 20109.421875\n",
      "Train Epoch: 66 [129984/225000 (58%)] Loss: 19660.783203\n",
      "Train Epoch: 66 [132480/225000 (59%)] Loss: 19611.441406\n",
      "Train Epoch: 66 [134976/225000 (60%)] Loss: 19988.847656\n",
      "Train Epoch: 66 [137472/225000 (61%)] Loss: 20504.765625\n",
      "Train Epoch: 66 [139968/225000 (62%)] Loss: 19642.324219\n",
      "Train Epoch: 66 [142464/225000 (63%)] Loss: 20145.199219\n",
      "Train Epoch: 66 [144960/225000 (64%)] Loss: 20027.175781\n",
      "Train Epoch: 66 [147456/225000 (66%)] Loss: 20042.957031\n",
      "Train Epoch: 66 [149952/225000 (67%)] Loss: 19473.156250\n",
      "Train Epoch: 66 [152448/225000 (68%)] Loss: 19824.066406\n",
      "Train Epoch: 66 [154944/225000 (69%)] Loss: 19715.066406\n",
      "Train Epoch: 66 [157440/225000 (70%)] Loss: 19774.156250\n",
      "Train Epoch: 66 [159936/225000 (71%)] Loss: 19699.339844\n",
      "Train Epoch: 66 [162432/225000 (72%)] Loss: 19443.191406\n",
      "Train Epoch: 66 [164928/225000 (73%)] Loss: 19884.599609\n",
      "Train Epoch: 66 [167424/225000 (74%)] Loss: 19422.277344\n",
      "Train Epoch: 66 [169920/225000 (76%)] Loss: 19730.105469\n",
      "Train Epoch: 66 [172416/225000 (77%)] Loss: 19569.214844\n",
      "Train Epoch: 66 [174912/225000 (78%)] Loss: 20177.402344\n",
      "Train Epoch: 66 [177408/225000 (79%)] Loss: 19771.554688\n",
      "Train Epoch: 66 [179904/225000 (80%)] Loss: 19654.595703\n",
      "Train Epoch: 66 [182400/225000 (81%)] Loss: 19515.042969\n",
      "Train Epoch: 66 [184896/225000 (82%)] Loss: 20253.230469\n",
      "Train Epoch: 66 [187392/225000 (83%)] Loss: 19861.253906\n",
      "Train Epoch: 66 [189888/225000 (84%)] Loss: 19912.429688\n",
      "Train Epoch: 66 [192384/225000 (86%)] Loss: 19977.429688\n",
      "Train Epoch: 66 [194880/225000 (87%)] Loss: 19675.144531\n",
      "Train Epoch: 66 [197376/225000 (88%)] Loss: 19755.207031\n",
      "Train Epoch: 66 [199872/225000 (89%)] Loss: 20073.074219\n",
      "Train Epoch: 66 [202368/225000 (90%)] Loss: 19710.726562\n",
      "Train Epoch: 66 [204864/225000 (91%)] Loss: 19934.804688\n",
      "Train Epoch: 66 [207360/225000 (92%)] Loss: 19850.343750\n",
      "Train Epoch: 66 [209856/225000 (93%)] Loss: 19703.859375\n",
      "Train Epoch: 66 [212352/225000 (94%)] Loss: 20170.242188\n",
      "Train Epoch: 66 [214848/225000 (95%)] Loss: 19687.671875\n",
      "Train Epoch: 66 [217344/225000 (97%)] Loss: 19984.447266\n",
      "Train Epoch: 66 [219840/225000 (98%)] Loss: 20085.304688\n",
      "Train Epoch: 66 [222336/225000 (99%)] Loss: 19913.796875\n",
      "Train Epoch: 66 [224832/225000 (100%)] Loss: 19334.636719\n",
      "    epoch          : 66\n",
      "    loss           : 19924.58626246534\n",
      "    val_loss       : 19813.64559366139\n",
      "Train Epoch: 67 [192/225000 (0%)] Loss: 19549.060547\n",
      "Train Epoch: 67 [2688/225000 (1%)] Loss: 19973.132812\n",
      "Train Epoch: 67 [5184/225000 (2%)] Loss: 20013.371094\n",
      "Train Epoch: 67 [7680/225000 (3%)] Loss: 20342.640625\n",
      "Train Epoch: 67 [10176/225000 (5%)] Loss: 19903.714844\n",
      "Train Epoch: 67 [12672/225000 (6%)] Loss: 20040.511719\n",
      "Train Epoch: 67 [15168/225000 (7%)] Loss: 20242.636719\n",
      "Train Epoch: 67 [17664/225000 (8%)] Loss: 19762.365234\n",
      "Train Epoch: 67 [20160/225000 (9%)] Loss: 20080.515625\n",
      "Train Epoch: 67 [22656/225000 (10%)] Loss: 20101.347656\n",
      "Train Epoch: 67 [25152/225000 (11%)] Loss: 19940.667969\n",
      "Train Epoch: 67 [27648/225000 (12%)] Loss: 20069.468750\n",
      "Train Epoch: 67 [30144/225000 (13%)] Loss: 20142.515625\n",
      "Train Epoch: 67 [32640/225000 (15%)] Loss: 20156.234375\n",
      "Train Epoch: 67 [35136/225000 (16%)] Loss: 19891.714844\n",
      "Train Epoch: 67 [37632/225000 (17%)] Loss: 20000.261719\n",
      "Train Epoch: 67 [40128/225000 (18%)] Loss: 19839.369141\n",
      "Train Epoch: 67 [42624/225000 (19%)] Loss: 19746.552734\n",
      "Train Epoch: 67 [45120/225000 (20%)] Loss: 20204.816406\n",
      "Train Epoch: 67 [47616/225000 (21%)] Loss: 19705.542969\n",
      "Train Epoch: 67 [50112/225000 (22%)] Loss: 19688.886719\n",
      "Train Epoch: 67 [52608/225000 (23%)] Loss: 20347.609375\n",
      "Train Epoch: 67 [55104/225000 (24%)] Loss: 19567.292969\n",
      "Train Epoch: 67 [57600/225000 (26%)] Loss: 19523.339844\n",
      "Train Epoch: 67 [60096/225000 (27%)] Loss: 19839.285156\n",
      "Train Epoch: 67 [62592/225000 (28%)] Loss: 19906.318359\n",
      "Train Epoch: 67 [65088/225000 (29%)] Loss: 19726.857422\n",
      "Train Epoch: 67 [67584/225000 (30%)] Loss: 19870.550781\n",
      "Train Epoch: 67 [70080/225000 (31%)] Loss: 20893.398438\n",
      "Train Epoch: 67 [72576/225000 (32%)] Loss: 19901.496094\n",
      "Train Epoch: 67 [75072/225000 (33%)] Loss: 19592.726562\n",
      "Train Epoch: 67 [77568/225000 (34%)] Loss: 19965.982422\n",
      "Train Epoch: 67 [80064/225000 (36%)] Loss: 20234.419922\n",
      "Train Epoch: 67 [82560/225000 (37%)] Loss: 19739.480469\n",
      "Train Epoch: 67 [85056/225000 (38%)] Loss: 19689.027344\n",
      "Train Epoch: 67 [87552/225000 (39%)] Loss: 19854.085938\n",
      "Train Epoch: 67 [90048/225000 (40%)] Loss: 20056.562500\n",
      "Train Epoch: 67 [92544/225000 (41%)] Loss: 20389.248047\n",
      "Train Epoch: 67 [95040/225000 (42%)] Loss: 19535.953125\n",
      "Train Epoch: 67 [97536/225000 (43%)] Loss: 20005.781250\n",
      "Train Epoch: 67 [100032/225000 (44%)] Loss: 19955.464844\n",
      "Train Epoch: 67 [102528/225000 (46%)] Loss: 19732.042969\n",
      "Train Epoch: 67 [105024/225000 (47%)] Loss: 20216.765625\n",
      "Train Epoch: 67 [107520/225000 (48%)] Loss: 19689.566406\n",
      "Train Epoch: 67 [110016/225000 (49%)] Loss: 19741.105469\n",
      "Train Epoch: 67 [112512/225000 (50%)] Loss: 20085.765625\n",
      "Train Epoch: 67 [115008/225000 (51%)] Loss: 19437.384766\n",
      "Train Epoch: 67 [117504/225000 (52%)] Loss: 20277.080078\n",
      "Train Epoch: 67 [120000/225000 (53%)] Loss: 19859.041016\n",
      "Train Epoch: 67 [122496/225000 (54%)] Loss: 19769.175781\n",
      "Train Epoch: 67 [124992/225000 (56%)] Loss: 19623.023438\n",
      "Train Epoch: 67 [127488/225000 (57%)] Loss: 19724.291016\n",
      "Train Epoch: 67 [129984/225000 (58%)] Loss: 19990.777344\n",
      "Train Epoch: 67 [132480/225000 (59%)] Loss: 19986.093750\n",
      "Train Epoch: 67 [134976/225000 (60%)] Loss: 20131.542969\n",
      "Train Epoch: 67 [137472/225000 (61%)] Loss: 19864.992188\n",
      "Train Epoch: 67 [139968/225000 (62%)] Loss: 20114.722656\n",
      "Train Epoch: 67 [142464/225000 (63%)] Loss: 19604.507812\n",
      "Train Epoch: 67 [144960/225000 (64%)] Loss: 19605.167969\n",
      "Train Epoch: 67 [147456/225000 (66%)] Loss: 19677.746094\n",
      "Train Epoch: 67 [149952/225000 (67%)] Loss: 20451.882812\n",
      "Train Epoch: 67 [152448/225000 (68%)] Loss: 19815.675781\n",
      "Train Epoch: 67 [154944/225000 (69%)] Loss: 19817.708984\n",
      "Train Epoch: 67 [157440/225000 (70%)] Loss: 19778.761719\n",
      "Train Epoch: 67 [159936/225000 (71%)] Loss: 19753.949219\n",
      "Train Epoch: 67 [162432/225000 (72%)] Loss: 19840.535156\n",
      "Train Epoch: 67 [164928/225000 (73%)] Loss: 20132.062500\n",
      "Train Epoch: 67 [167424/225000 (74%)] Loss: 19884.382812\n",
      "Train Epoch: 67 [169920/225000 (76%)] Loss: 20004.826172\n",
      "Train Epoch: 67 [172416/225000 (77%)] Loss: 20035.580078\n",
      "Train Epoch: 67 [174912/225000 (78%)] Loss: 19866.621094\n",
      "Train Epoch: 67 [177408/225000 (79%)] Loss: 19787.714844\n",
      "Train Epoch: 67 [179904/225000 (80%)] Loss: 19876.394531\n",
      "Train Epoch: 67 [182400/225000 (81%)] Loss: 19754.894531\n",
      "Train Epoch: 67 [184896/225000 (82%)] Loss: 20050.394531\n",
      "Train Epoch: 67 [187392/225000 (83%)] Loss: 19868.226562\n",
      "Train Epoch: 67 [189888/225000 (84%)] Loss: 20150.535156\n",
      "Train Epoch: 67 [192384/225000 (86%)] Loss: 19718.972656\n",
      "Train Epoch: 67 [194880/225000 (87%)] Loss: 19993.664062\n",
      "Train Epoch: 67 [197376/225000 (88%)] Loss: 19898.912109\n",
      "Train Epoch: 67 [199872/225000 (89%)] Loss: 19495.265625\n",
      "Train Epoch: 67 [202368/225000 (90%)] Loss: 20078.273438\n",
      "Train Epoch: 67 [204864/225000 (91%)] Loss: 20097.353516\n",
      "Train Epoch: 67 [207360/225000 (92%)] Loss: 20112.589844\n",
      "Train Epoch: 67 [209856/225000 (93%)] Loss: 19710.775391\n",
      "Train Epoch: 67 [212352/225000 (94%)] Loss: 20064.521484\n",
      "Train Epoch: 67 [214848/225000 (95%)] Loss: 20163.179688\n",
      "Train Epoch: 67 [217344/225000 (97%)] Loss: 24163.988281\n",
      "Train Epoch: 67 [219840/225000 (98%)] Loss: 19694.046875\n",
      "Train Epoch: 67 [222336/225000 (99%)] Loss: 19854.222656\n",
      "Train Epoch: 67 [224832/225000 (100%)] Loss: 19582.105469\n",
      "    epoch          : 67\n",
      "    loss           : 19899.367924088096\n",
      "    val_loss       : 19788.721988060093\n",
      "Train Epoch: 68 [192/225000 (0%)] Loss: 19748.906250\n",
      "Train Epoch: 68 [2688/225000 (1%)] Loss: 19756.958984\n",
      "Train Epoch: 68 [5184/225000 (2%)] Loss: 19624.220703\n",
      "Train Epoch: 68 [7680/225000 (3%)] Loss: 20290.554688\n",
      "Train Epoch: 68 [10176/225000 (5%)] Loss: 20202.896484\n",
      "Train Epoch: 68 [12672/225000 (6%)] Loss: 19800.285156\n",
      "Train Epoch: 68 [15168/225000 (7%)] Loss: 19963.796875\n",
      "Train Epoch: 68 [17664/225000 (8%)] Loss: 19561.353516\n",
      "Train Epoch: 68 [20160/225000 (9%)] Loss: 20028.515625\n",
      "Train Epoch: 68 [22656/225000 (10%)] Loss: 20137.996094\n",
      "Train Epoch: 68 [25152/225000 (11%)] Loss: 19778.833984\n",
      "Train Epoch: 68 [27648/225000 (12%)] Loss: 19707.906250\n",
      "Train Epoch: 68 [30144/225000 (13%)] Loss: 19512.521484\n",
      "Train Epoch: 68 [32640/225000 (15%)] Loss: 20214.187500\n",
      "Train Epoch: 68 [35136/225000 (16%)] Loss: 20366.896484\n",
      "Train Epoch: 68 [37632/225000 (17%)] Loss: 19434.794922\n",
      "Train Epoch: 68 [40128/225000 (18%)] Loss: 19867.500000\n",
      "Train Epoch: 68 [42624/225000 (19%)] Loss: 20198.511719\n",
      "Train Epoch: 68 [45120/225000 (20%)] Loss: 19868.835938\n",
      "Train Epoch: 68 [47616/225000 (21%)] Loss: 19802.251953\n",
      "Train Epoch: 68 [50112/225000 (22%)] Loss: 19479.054688\n",
      "Train Epoch: 68 [52608/225000 (23%)] Loss: 19650.087891\n",
      "Train Epoch: 68 [55104/225000 (24%)] Loss: 19482.550781\n",
      "Train Epoch: 68 [57600/225000 (26%)] Loss: 20271.183594\n",
      "Train Epoch: 68 [60096/225000 (27%)] Loss: 19846.287109\n",
      "Train Epoch: 68 [62592/225000 (28%)] Loss: 20090.845703\n",
      "Train Epoch: 68 [65088/225000 (29%)] Loss: 19524.445312\n",
      "Train Epoch: 68 [67584/225000 (30%)] Loss: 20305.796875\n",
      "Train Epoch: 68 [70080/225000 (31%)] Loss: 20337.904297\n",
      "Train Epoch: 68 [72576/225000 (32%)] Loss: 19860.722656\n",
      "Train Epoch: 68 [75072/225000 (33%)] Loss: 20014.460938\n",
      "Train Epoch: 68 [77568/225000 (34%)] Loss: 19297.697266\n",
      "Train Epoch: 68 [80064/225000 (36%)] Loss: 19848.710938\n",
      "Train Epoch: 68 [82560/225000 (37%)] Loss: 20444.779297\n",
      "Train Epoch: 68 [85056/225000 (38%)] Loss: 19674.062500\n",
      "Train Epoch: 68 [87552/225000 (39%)] Loss: 20209.771484\n",
      "Train Epoch: 68 [90048/225000 (40%)] Loss: 20080.878906\n",
      "Train Epoch: 68 [92544/225000 (41%)] Loss: 19800.859375\n",
      "Train Epoch: 68 [95040/225000 (42%)] Loss: 19778.695312\n",
      "Train Epoch: 68 [97536/225000 (43%)] Loss: 20156.734375\n",
      "Train Epoch: 68 [100032/225000 (44%)] Loss: 20225.324219\n",
      "Train Epoch: 68 [102528/225000 (46%)] Loss: 20062.453125\n",
      "Train Epoch: 68 [105024/225000 (47%)] Loss: 19791.707031\n",
      "Train Epoch: 68 [107520/225000 (48%)] Loss: 19960.076172\n",
      "Train Epoch: 68 [110016/225000 (49%)] Loss: 19839.511719\n",
      "Train Epoch: 68 [112512/225000 (50%)] Loss: 19789.269531\n",
      "Train Epoch: 68 [115008/225000 (51%)] Loss: 19591.226562\n",
      "Train Epoch: 68 [117504/225000 (52%)] Loss: 19626.927734\n",
      "Train Epoch: 68 [120000/225000 (53%)] Loss: 20093.945312\n",
      "Train Epoch: 68 [122496/225000 (54%)] Loss: 19222.611328\n",
      "Train Epoch: 68 [124992/225000 (56%)] Loss: 19894.447266\n",
      "Train Epoch: 68 [127488/225000 (57%)] Loss: 20181.714844\n",
      "Train Epoch: 68 [129984/225000 (58%)] Loss: 19650.847656\n",
      "Train Epoch: 68 [132480/225000 (59%)] Loss: 19965.029297\n",
      "Train Epoch: 68 [134976/225000 (60%)] Loss: 19783.408203\n",
      "Train Epoch: 68 [137472/225000 (61%)] Loss: 19394.105469\n",
      "Train Epoch: 68 [139968/225000 (62%)] Loss: 20148.265625\n",
      "Train Epoch: 68 [142464/225000 (63%)] Loss: 18924.541016\n",
      "Train Epoch: 68 [144960/225000 (64%)] Loss: 20089.210938\n",
      "Train Epoch: 68 [147456/225000 (66%)] Loss: 19421.917969\n",
      "Train Epoch: 68 [149952/225000 (67%)] Loss: 20218.101562\n",
      "Train Epoch: 68 [152448/225000 (68%)] Loss: 19708.136719\n",
      "Train Epoch: 68 [154944/225000 (69%)] Loss: 19743.222656\n",
      "Train Epoch: 68 [157440/225000 (70%)] Loss: 20077.066406\n",
      "Train Epoch: 68 [159936/225000 (71%)] Loss: 19653.306641\n",
      "Train Epoch: 68 [162432/225000 (72%)] Loss: 19925.486328\n",
      "Train Epoch: 68 [164928/225000 (73%)] Loss: 19569.558594\n",
      "Train Epoch: 68 [167424/225000 (74%)] Loss: 19723.685547\n",
      "Train Epoch: 68 [169920/225000 (76%)] Loss: 19530.410156\n",
      "Train Epoch: 68 [172416/225000 (77%)] Loss: 19770.171875\n",
      "Train Epoch: 68 [174912/225000 (78%)] Loss: 19713.550781\n",
      "Train Epoch: 68 [177408/225000 (79%)] Loss: 20573.738281\n",
      "Train Epoch: 68 [179904/225000 (80%)] Loss: 20024.144531\n",
      "Train Epoch: 68 [182400/225000 (81%)] Loss: 19729.201172\n",
      "Train Epoch: 68 [184896/225000 (82%)] Loss: 19822.310547\n",
      "Train Epoch: 68 [187392/225000 (83%)] Loss: 19621.355469\n",
      "Train Epoch: 68 [189888/225000 (84%)] Loss: 20178.289062\n",
      "Train Epoch: 68 [192384/225000 (86%)] Loss: 19580.802734\n",
      "Train Epoch: 68 [194880/225000 (87%)] Loss: 19469.617188\n",
      "Train Epoch: 68 [197376/225000 (88%)] Loss: 20399.671875\n",
      "Train Epoch: 68 [199872/225000 (89%)] Loss: 19435.703125\n",
      "Train Epoch: 68 [202368/225000 (90%)] Loss: 20091.398438\n",
      "Train Epoch: 68 [204864/225000 (91%)] Loss: 19946.246094\n",
      "Train Epoch: 68 [207360/225000 (92%)] Loss: 20466.574219\n",
      "Train Epoch: 68 [209856/225000 (93%)] Loss: 19282.121094\n",
      "Train Epoch: 68 [212352/225000 (94%)] Loss: 20173.195312\n",
      "Train Epoch: 68 [214848/225000 (95%)] Loss: 19241.089844\n",
      "Train Epoch: 68 [217344/225000 (97%)] Loss: 19976.625000\n",
      "Train Epoch: 68 [219840/225000 (98%)] Loss: 20096.679688\n",
      "Train Epoch: 68 [222336/225000 (99%)] Loss: 19910.636719\n",
      "Train Epoch: 68 [224832/225000 (100%)] Loss: 19405.941406\n",
      "    epoch          : 68\n",
      "    loss           : 19868.756464310474\n",
      "    val_loss       : 19814.466197080284\n",
      "Train Epoch: 69 [192/225000 (0%)] Loss: 19979.029297\n",
      "Train Epoch: 69 [2688/225000 (1%)] Loss: 19723.562500\n",
      "Train Epoch: 69 [5184/225000 (2%)] Loss: 19942.212891\n",
      "Train Epoch: 69 [7680/225000 (3%)] Loss: 20221.050781\n",
      "Train Epoch: 69 [10176/225000 (5%)] Loss: 19781.181641\n",
      "Train Epoch: 69 [12672/225000 (6%)] Loss: 19863.750000\n",
      "Train Epoch: 69 [15168/225000 (7%)] Loss: 19835.052734\n",
      "Train Epoch: 69 [17664/225000 (8%)] Loss: 19796.613281\n",
      "Train Epoch: 69 [20160/225000 (9%)] Loss: 19617.304688\n",
      "Train Epoch: 69 [22656/225000 (10%)] Loss: 19836.492188\n",
      "Train Epoch: 69 [25152/225000 (11%)] Loss: 19853.121094\n",
      "Train Epoch: 69 [27648/225000 (12%)] Loss: 19973.164062\n",
      "Train Epoch: 69 [30144/225000 (13%)] Loss: 20134.179688\n",
      "Train Epoch: 69 [32640/225000 (15%)] Loss: 19333.449219\n",
      "Train Epoch: 69 [35136/225000 (16%)] Loss: 19458.089844\n",
      "Train Epoch: 69 [37632/225000 (17%)] Loss: 20000.593750\n",
      "Train Epoch: 69 [40128/225000 (18%)] Loss: 19315.523438\n",
      "Train Epoch: 69 [42624/225000 (19%)] Loss: 19855.941406\n",
      "Train Epoch: 69 [45120/225000 (20%)] Loss: 19530.269531\n",
      "Train Epoch: 69 [47616/225000 (21%)] Loss: 20205.679688\n",
      "Train Epoch: 69 [50112/225000 (22%)] Loss: 19926.152344\n",
      "Train Epoch: 69 [52608/225000 (23%)] Loss: 20059.683594\n",
      "Train Epoch: 69 [55104/225000 (24%)] Loss: 19747.248047\n",
      "Train Epoch: 69 [57600/225000 (26%)] Loss: 19873.886719\n",
      "Train Epoch: 69 [60096/225000 (27%)] Loss: 19435.919922\n",
      "Train Epoch: 69 [62592/225000 (28%)] Loss: 19640.791016\n",
      "Train Epoch: 69 [65088/225000 (29%)] Loss: 20110.855469\n",
      "Train Epoch: 69 [67584/225000 (30%)] Loss: 19717.515625\n",
      "Train Epoch: 69 [70080/225000 (31%)] Loss: 19905.693359\n",
      "Train Epoch: 69 [72576/225000 (32%)] Loss: 19577.279297\n",
      "Train Epoch: 69 [75072/225000 (33%)] Loss: 19704.640625\n",
      "Train Epoch: 69 [77568/225000 (34%)] Loss: 20373.699219\n",
      "Train Epoch: 69 [80064/225000 (36%)] Loss: 19481.875000\n",
      "Train Epoch: 69 [82560/225000 (37%)] Loss: 20324.195312\n",
      "Train Epoch: 69 [85056/225000 (38%)] Loss: 19922.214844\n",
      "Train Epoch: 69 [87552/225000 (39%)] Loss: 19923.140625\n",
      "Train Epoch: 69 [90048/225000 (40%)] Loss: 19651.480469\n",
      "Train Epoch: 69 [92544/225000 (41%)] Loss: 20041.042969\n",
      "Train Epoch: 69 [95040/225000 (42%)] Loss: 19504.625000\n",
      "Train Epoch: 69 [97536/225000 (43%)] Loss: 19639.679688\n",
      "Train Epoch: 69 [100032/225000 (44%)] Loss: 19516.410156\n",
      "Train Epoch: 69 [102528/225000 (46%)] Loss: 19793.414062\n",
      "Train Epoch: 69 [105024/225000 (47%)] Loss: 19665.980469\n",
      "Train Epoch: 69 [107520/225000 (48%)] Loss: 20006.123047\n",
      "Train Epoch: 69 [110016/225000 (49%)] Loss: 19952.070312\n",
      "Train Epoch: 69 [112512/225000 (50%)] Loss: 19796.072266\n",
      "Train Epoch: 69 [115008/225000 (51%)] Loss: 19961.167969\n",
      "Train Epoch: 69 [117504/225000 (52%)] Loss: 19624.957031\n",
      "Train Epoch: 69 [120000/225000 (53%)] Loss: 20131.255859\n",
      "Train Epoch: 69 [122496/225000 (54%)] Loss: 20581.371094\n",
      "Train Epoch: 69 [124992/225000 (56%)] Loss: 19509.839844\n",
      "Train Epoch: 69 [127488/225000 (57%)] Loss: 19565.167969\n",
      "Train Epoch: 69 [129984/225000 (58%)] Loss: 20303.822266\n",
      "Train Epoch: 69 [132480/225000 (59%)] Loss: 19377.634766\n",
      "Train Epoch: 69 [134976/225000 (60%)] Loss: 19521.500000\n",
      "Train Epoch: 69 [137472/225000 (61%)] Loss: 20552.382812\n",
      "Train Epoch: 69 [139968/225000 (62%)] Loss: 19950.929688\n",
      "Train Epoch: 69 [142464/225000 (63%)] Loss: 19403.371094\n",
      "Train Epoch: 69 [144960/225000 (64%)] Loss: 20108.927734\n",
      "Train Epoch: 69 [147456/225000 (66%)] Loss: 20122.894531\n",
      "Train Epoch: 69 [149952/225000 (67%)] Loss: 20139.265625\n",
      "Train Epoch: 69 [152448/225000 (68%)] Loss: 20307.710938\n",
      "Train Epoch: 69 [154944/225000 (69%)] Loss: 20345.128906\n",
      "Train Epoch: 69 [157440/225000 (70%)] Loss: 19964.126953\n",
      "Train Epoch: 69 [159936/225000 (71%)] Loss: 20093.156250\n",
      "Train Epoch: 69 [162432/225000 (72%)] Loss: 20012.070312\n",
      "Train Epoch: 69 [164928/225000 (73%)] Loss: 19800.585938\n",
      "Train Epoch: 69 [167424/225000 (74%)] Loss: 19469.109375\n",
      "Train Epoch: 69 [169920/225000 (76%)] Loss: 20287.859375\n",
      "Train Epoch: 69 [172416/225000 (77%)] Loss: 19895.230469\n",
      "Train Epoch: 69 [174912/225000 (78%)] Loss: 20232.191406\n",
      "Train Epoch: 69 [177408/225000 (79%)] Loss: 19542.181641\n",
      "Train Epoch: 69 [179904/225000 (80%)] Loss: 19291.281250\n",
      "Train Epoch: 69 [182400/225000 (81%)] Loss: 19796.582031\n",
      "Train Epoch: 69 [184896/225000 (82%)] Loss: 19558.216797\n",
      "Train Epoch: 69 [187392/225000 (83%)] Loss: 19703.699219\n",
      "Train Epoch: 69 [189888/225000 (84%)] Loss: 20081.644531\n",
      "Train Epoch: 69 [192384/225000 (86%)] Loss: 19761.351562\n",
      "Train Epoch: 69 [194880/225000 (87%)] Loss: 20299.859375\n",
      "Train Epoch: 69 [197376/225000 (88%)] Loss: 20384.570312\n",
      "Train Epoch: 69 [199872/225000 (89%)] Loss: 19804.789062\n",
      "Train Epoch: 69 [202368/225000 (90%)] Loss: 20168.187500\n",
      "Train Epoch: 69 [204864/225000 (91%)] Loss: 20359.910156\n",
      "Train Epoch: 69 [207360/225000 (92%)] Loss: 20124.589844\n",
      "Train Epoch: 69 [209856/225000 (93%)] Loss: 20371.542969\n",
      "Train Epoch: 69 [212352/225000 (94%)] Loss: 19935.281250\n",
      "Train Epoch: 69 [214848/225000 (95%)] Loss: 19829.363281\n",
      "Train Epoch: 69 [217344/225000 (97%)] Loss: 19894.488281\n",
      "Train Epoch: 69 [219840/225000 (98%)] Loss: 19680.164062\n",
      "Train Epoch: 69 [222336/225000 (99%)] Loss: 19296.712891\n",
      "Train Epoch: 69 [224832/225000 (100%)] Loss: 19957.923828\n",
      "    epoch          : 69\n",
      "    loss           : 19863.595639798423\n",
      "    val_loss       : 19794.74078816949\n",
      "Train Epoch: 70 [192/225000 (0%)] Loss: 19407.058594\n",
      "Train Epoch: 70 [2688/225000 (1%)] Loss: 20210.164062\n",
      "Train Epoch: 70 [5184/225000 (2%)] Loss: 19412.878906\n",
      "Train Epoch: 70 [7680/225000 (3%)] Loss: 19764.781250\n",
      "Train Epoch: 70 [10176/225000 (5%)] Loss: 19776.990234\n",
      "Train Epoch: 70 [12672/225000 (6%)] Loss: 19521.710938\n",
      "Train Epoch: 70 [15168/225000 (7%)] Loss: 19955.654297\n",
      "Train Epoch: 70 [17664/225000 (8%)] Loss: 20053.847656\n",
      "Train Epoch: 70 [20160/225000 (9%)] Loss: 19609.941406\n",
      "Train Epoch: 70 [22656/225000 (10%)] Loss: 19762.371094\n",
      "Train Epoch: 70 [25152/225000 (11%)] Loss: 19676.199219\n",
      "Train Epoch: 70 [27648/225000 (12%)] Loss: 19898.154297\n",
      "Train Epoch: 70 [30144/225000 (13%)] Loss: 19811.718750\n",
      "Train Epoch: 70 [32640/225000 (15%)] Loss: 19783.570312\n",
      "Train Epoch: 70 [35136/225000 (16%)] Loss: 19894.296875\n",
      "Train Epoch: 70 [37632/225000 (17%)] Loss: 19515.664062\n",
      "Train Epoch: 70 [40128/225000 (18%)] Loss: 20277.244141\n",
      "Train Epoch: 70 [42624/225000 (19%)] Loss: 19593.906250\n",
      "Train Epoch: 70 [45120/225000 (20%)] Loss: 19943.843750\n",
      "Train Epoch: 70 [47616/225000 (21%)] Loss: 20276.320312\n",
      "Train Epoch: 70 [50112/225000 (22%)] Loss: 20206.425781\n",
      "Train Epoch: 70 [52608/225000 (23%)] Loss: 19686.718750\n",
      "Train Epoch: 70 [55104/225000 (24%)] Loss: 19782.910156\n",
      "Train Epoch: 70 [57600/225000 (26%)] Loss: 19632.439453\n",
      "Train Epoch: 70 [60096/225000 (27%)] Loss: 19608.914062\n",
      "Train Epoch: 70 [62592/225000 (28%)] Loss: 19573.484375\n",
      "Train Epoch: 70 [65088/225000 (29%)] Loss: 20136.478516\n",
      "Train Epoch: 70 [67584/225000 (30%)] Loss: 19517.390625\n",
      "Train Epoch: 70 [70080/225000 (31%)] Loss: 19523.937500\n",
      "Train Epoch: 70 [72576/225000 (32%)] Loss: 20035.357422\n",
      "Train Epoch: 70 [75072/225000 (33%)] Loss: 19370.636719\n",
      "Train Epoch: 70 [77568/225000 (34%)] Loss: 19792.554688\n",
      "Train Epoch: 70 [80064/225000 (36%)] Loss: 19831.464844\n",
      "Train Epoch: 70 [82560/225000 (37%)] Loss: 19357.011719\n",
      "Train Epoch: 70 [85056/225000 (38%)] Loss: 20051.820312\n",
      "Train Epoch: 70 [87552/225000 (39%)] Loss: 19703.109375\n",
      "Train Epoch: 70 [90048/225000 (40%)] Loss: 19833.933594\n",
      "Train Epoch: 70 [92544/225000 (41%)] Loss: 19375.511719\n",
      "Train Epoch: 70 [95040/225000 (42%)] Loss: 19595.707031\n",
      "Train Epoch: 70 [97536/225000 (43%)] Loss: 20043.894531\n",
      "Train Epoch: 70 [100032/225000 (44%)] Loss: 19646.119141\n",
      "Train Epoch: 70 [102528/225000 (46%)] Loss: 19644.917969\n",
      "Train Epoch: 70 [105024/225000 (47%)] Loss: 20030.042969\n",
      "Train Epoch: 70 [107520/225000 (48%)] Loss: 19827.445312\n",
      "Train Epoch: 70 [110016/225000 (49%)] Loss: 19759.179688\n",
      "Train Epoch: 70 [112512/225000 (50%)] Loss: 19816.951172\n",
      "Train Epoch: 70 [115008/225000 (51%)] Loss: 19810.072266\n",
      "Train Epoch: 70 [117504/225000 (52%)] Loss: 20200.316406\n",
      "Train Epoch: 70 [120000/225000 (53%)] Loss: 19936.164062\n",
      "Train Epoch: 70 [122496/225000 (54%)] Loss: 19489.945312\n",
      "Train Epoch: 70 [124992/225000 (56%)] Loss: 19427.011719\n",
      "Train Epoch: 70 [127488/225000 (57%)] Loss: 20091.597656\n",
      "Train Epoch: 70 [129984/225000 (58%)] Loss: 19846.828125\n",
      "Train Epoch: 70 [132480/225000 (59%)] Loss: 19852.310547\n",
      "Train Epoch: 70 [134976/225000 (60%)] Loss: 19503.914062\n",
      "Train Epoch: 70 [137472/225000 (61%)] Loss: 20031.906250\n",
      "Train Epoch: 70 [139968/225000 (62%)] Loss: 19726.556641\n",
      "Train Epoch: 70 [142464/225000 (63%)] Loss: 20020.289062\n",
      "Train Epoch: 70 [144960/225000 (64%)] Loss: 20196.726562\n",
      "Train Epoch: 70 [147456/225000 (66%)] Loss: 19872.257812\n",
      "Train Epoch: 70 [149952/225000 (67%)] Loss: 20264.664062\n",
      "Train Epoch: 70 [152448/225000 (68%)] Loss: 19447.687500\n",
      "Train Epoch: 70 [154944/225000 (69%)] Loss: 19527.269531\n",
      "Train Epoch: 70 [157440/225000 (70%)] Loss: 20097.933594\n",
      "Train Epoch: 70 [159936/225000 (71%)] Loss: 20583.666016\n",
      "Train Epoch: 70 [162432/225000 (72%)] Loss: 19393.367188\n",
      "Train Epoch: 70 [164928/225000 (73%)] Loss: 20220.199219\n",
      "Train Epoch: 70 [167424/225000 (74%)] Loss: 19604.632812\n",
      "Train Epoch: 70 [169920/225000 (76%)] Loss: 19635.763672\n",
      "Train Epoch: 70 [172416/225000 (77%)] Loss: 19855.507812\n",
      "Train Epoch: 70 [174912/225000 (78%)] Loss: 19722.556641\n",
      "Train Epoch: 70 [177408/225000 (79%)] Loss: 19278.939453\n",
      "Train Epoch: 70 [179904/225000 (80%)] Loss: 19883.283203\n",
      "Train Epoch: 70 [182400/225000 (81%)] Loss: 19310.160156\n",
      "Train Epoch: 70 [184896/225000 (82%)] Loss: 20142.476562\n",
      "Train Epoch: 70 [187392/225000 (83%)] Loss: 19971.212891\n",
      "Train Epoch: 70 [189888/225000 (84%)] Loss: 19856.570312\n",
      "Train Epoch: 70 [192384/225000 (86%)] Loss: 19664.445312\n",
      "Train Epoch: 70 [194880/225000 (87%)] Loss: 20153.763672\n",
      "Train Epoch: 70 [197376/225000 (88%)] Loss: 19505.087891\n",
      "Train Epoch: 70 [199872/225000 (89%)] Loss: 19680.191406\n",
      "Train Epoch: 70 [202368/225000 (90%)] Loss: 19796.117188\n",
      "Train Epoch: 70 [204864/225000 (91%)] Loss: 20004.171875\n",
      "Train Epoch: 70 [207360/225000 (92%)] Loss: 19430.347656\n",
      "Train Epoch: 70 [209856/225000 (93%)] Loss: 20098.550781\n",
      "Train Epoch: 70 [212352/225000 (94%)] Loss: 19781.328125\n",
      "Train Epoch: 70 [214848/225000 (95%)] Loss: 20266.882812\n",
      "Train Epoch: 70 [217344/225000 (97%)] Loss: 19733.572266\n",
      "Train Epoch: 70 [219840/225000 (98%)] Loss: 20312.269531\n",
      "Train Epoch: 70 [222336/225000 (99%)] Loss: 19855.835938\n",
      "Train Epoch: 70 [224832/225000 (100%)] Loss: 19253.039062\n",
      "    epoch          : 70\n",
      "    loss           : 19823.96403383639\n",
      "    val_loss       : 19728.806258915945\n",
      "Train Epoch: 71 [192/225000 (0%)] Loss: 19855.417969\n",
      "Train Epoch: 71 [2688/225000 (1%)] Loss: 19978.226562\n",
      "Train Epoch: 71 [5184/225000 (2%)] Loss: 19910.894531\n",
      "Train Epoch: 71 [7680/225000 (3%)] Loss: 19901.382812\n",
      "Train Epoch: 71 [10176/225000 (5%)] Loss: 19985.025391\n",
      "Train Epoch: 71 [12672/225000 (6%)] Loss: 19615.101562\n",
      "Train Epoch: 71 [15168/225000 (7%)] Loss: 19933.773438\n",
      "Train Epoch: 71 [17664/225000 (8%)] Loss: 19980.363281\n",
      "Train Epoch: 71 [20160/225000 (9%)] Loss: 19652.785156\n",
      "Train Epoch: 71 [22656/225000 (10%)] Loss: 19873.351562\n",
      "Train Epoch: 71 [25152/225000 (11%)] Loss: 19789.365234\n",
      "Train Epoch: 71 [27648/225000 (12%)] Loss: 19468.500000\n",
      "Train Epoch: 71 [30144/225000 (13%)] Loss: 19240.648438\n",
      "Train Epoch: 71 [32640/225000 (15%)] Loss: 20043.828125\n",
      "Train Epoch: 71 [35136/225000 (16%)] Loss: 19719.097656\n",
      "Train Epoch: 71 [37632/225000 (17%)] Loss: 19795.093750\n",
      "Train Epoch: 71 [40128/225000 (18%)] Loss: 20045.269531\n",
      "Train Epoch: 71 [42624/225000 (19%)] Loss: 19746.878906\n",
      "Train Epoch: 71 [45120/225000 (20%)] Loss: 19670.496094\n",
      "Train Epoch: 71 [47616/225000 (21%)] Loss: 19922.638672\n",
      "Train Epoch: 71 [50112/225000 (22%)] Loss: 19865.968750\n",
      "Train Epoch: 71 [52608/225000 (23%)] Loss: 19433.375000\n",
      "Train Epoch: 71 [55104/225000 (24%)] Loss: 19912.671875\n",
      "Train Epoch: 71 [57600/225000 (26%)] Loss: 20039.199219\n",
      "Train Epoch: 71 [60096/225000 (27%)] Loss: 19932.755859\n",
      "Train Epoch: 71 [62592/225000 (28%)] Loss: 20369.582031\n",
      "Train Epoch: 71 [65088/225000 (29%)] Loss: 19576.226562\n",
      "Train Epoch: 71 [67584/225000 (30%)] Loss: 19653.800781\n",
      "Train Epoch: 71 [70080/225000 (31%)] Loss: 19966.515625\n",
      "Train Epoch: 71 [72576/225000 (32%)] Loss: 19574.578125\n",
      "Train Epoch: 71 [75072/225000 (33%)] Loss: 20046.902344\n",
      "Train Epoch: 71 [77568/225000 (34%)] Loss: 19900.109375\n",
      "Train Epoch: 71 [80064/225000 (36%)] Loss: 19810.427734\n",
      "Train Epoch: 71 [82560/225000 (37%)] Loss: 19923.984375\n",
      "Train Epoch: 71 [85056/225000 (38%)] Loss: 19719.320312\n",
      "Train Epoch: 71 [87552/225000 (39%)] Loss: 20111.230469\n",
      "Train Epoch: 71 [90048/225000 (40%)] Loss: 19740.289062\n",
      "Train Epoch: 71 [92544/225000 (41%)] Loss: 20041.375000\n",
      "Train Epoch: 71 [95040/225000 (42%)] Loss: 19856.941406\n",
      "Train Epoch: 71 [97536/225000 (43%)] Loss: 19989.044922\n",
      "Train Epoch: 71 [100032/225000 (44%)] Loss: 19915.257812\n",
      "Train Epoch: 71 [102528/225000 (46%)] Loss: 20048.894531\n",
      "Train Epoch: 71 [105024/225000 (47%)] Loss: 20455.277344\n",
      "Train Epoch: 71 [107520/225000 (48%)] Loss: 19609.734375\n",
      "Train Epoch: 71 [110016/225000 (49%)] Loss: 19616.875000\n",
      "Train Epoch: 71 [112512/225000 (50%)] Loss: 19774.482422\n",
      "Train Epoch: 71 [115008/225000 (51%)] Loss: 20285.511719\n",
      "Train Epoch: 71 [117504/225000 (52%)] Loss: 19096.910156\n",
      "Train Epoch: 71 [120000/225000 (53%)] Loss: 19492.511719\n",
      "Train Epoch: 71 [122496/225000 (54%)] Loss: 20008.433594\n",
      "Train Epoch: 71 [124992/225000 (56%)] Loss: 20087.269531\n",
      "Train Epoch: 71 [127488/225000 (57%)] Loss: 19676.203125\n",
      "Train Epoch: 71 [129984/225000 (58%)] Loss: 19606.351562\n",
      "Train Epoch: 71 [132480/225000 (59%)] Loss: 20085.792969\n",
      "Train Epoch: 71 [134976/225000 (60%)] Loss: 19721.253906\n",
      "Train Epoch: 71 [137472/225000 (61%)] Loss: 19845.519531\n",
      "Train Epoch: 71 [139968/225000 (62%)] Loss: 19319.494141\n",
      "Train Epoch: 71 [142464/225000 (63%)] Loss: 19937.009766\n",
      "Train Epoch: 71 [144960/225000 (64%)] Loss: 19741.974609\n",
      "Train Epoch: 71 [147456/225000 (66%)] Loss: 20098.890625\n",
      "Train Epoch: 71 [149952/225000 (67%)] Loss: 19577.482422\n",
      "Train Epoch: 71 [152448/225000 (68%)] Loss: 19645.679688\n",
      "Train Epoch: 71 [154944/225000 (69%)] Loss: 19565.929688\n",
      "Train Epoch: 71 [157440/225000 (70%)] Loss: 19983.089844\n",
      "Train Epoch: 71 [159936/225000 (71%)] Loss: 19908.105469\n",
      "Train Epoch: 71 [162432/225000 (72%)] Loss: 19816.375000\n",
      "Train Epoch: 71 [164928/225000 (73%)] Loss: 20051.453125\n",
      "Train Epoch: 71 [167424/225000 (74%)] Loss: 19518.121094\n",
      "Train Epoch: 71 [169920/225000 (76%)] Loss: 19694.427734\n",
      "Train Epoch: 71 [172416/225000 (77%)] Loss: 19854.457031\n",
      "Train Epoch: 71 [174912/225000 (78%)] Loss: 19751.750000\n",
      "Train Epoch: 71 [177408/225000 (79%)] Loss: 19873.269531\n",
      "Train Epoch: 71 [179904/225000 (80%)] Loss: 19726.500000\n",
      "Train Epoch: 71 [182400/225000 (81%)] Loss: 20042.925781\n",
      "Train Epoch: 71 [184896/225000 (82%)] Loss: 20067.792969\n",
      "Train Epoch: 71 [187392/225000 (83%)] Loss: 19892.419922\n",
      "Train Epoch: 71 [189888/225000 (84%)] Loss: 19786.105469\n",
      "Train Epoch: 71 [192384/225000 (86%)] Loss: 19930.222656\n",
      "Train Epoch: 71 [194880/225000 (87%)] Loss: 19464.679688\n",
      "Train Epoch: 71 [197376/225000 (88%)] Loss: 19698.529297\n",
      "Train Epoch: 71 [199872/225000 (89%)] Loss: 20159.333984\n",
      "Train Epoch: 71 [202368/225000 (90%)] Loss: 19829.011719\n",
      "Train Epoch: 71 [204864/225000 (91%)] Loss: 19995.023438\n",
      "Train Epoch: 71 [207360/225000 (92%)] Loss: 19895.267578\n",
      "Train Epoch: 71 [209856/225000 (93%)] Loss: 19886.277344\n",
      "Train Epoch: 71 [212352/225000 (94%)] Loss: 19901.285156\n",
      "Train Epoch: 71 [214848/225000 (95%)] Loss: 19737.746094\n",
      "Train Epoch: 71 [217344/225000 (97%)] Loss: 19590.937500\n",
      "Train Epoch: 71 [219840/225000 (98%)] Loss: 19932.251953\n",
      "Train Epoch: 71 [222336/225000 (99%)] Loss: 19482.234375\n",
      "Train Epoch: 71 [224832/225000 (100%)] Loss: 19610.576172\n",
      "    epoch          : 71\n",
      "    loss           : 19818.267603122335\n",
      "    val_loss       : 19747.62282515482\n",
      "Train Epoch: 72 [192/225000 (0%)] Loss: 19408.769531\n",
      "Train Epoch: 72 [2688/225000 (1%)] Loss: 19632.689453\n",
      "Train Epoch: 72 [5184/225000 (2%)] Loss: 20048.457031\n",
      "Train Epoch: 72 [7680/225000 (3%)] Loss: 19990.982422\n",
      "Train Epoch: 72 [10176/225000 (5%)] Loss: 19581.173828\n",
      "Train Epoch: 72 [12672/225000 (6%)] Loss: 20442.800781\n",
      "Train Epoch: 72 [15168/225000 (7%)] Loss: 19243.503906\n",
      "Train Epoch: 72 [17664/225000 (8%)] Loss: 19672.916016\n",
      "Train Epoch: 72 [20160/225000 (9%)] Loss: 20023.507812\n",
      "Train Epoch: 72 [22656/225000 (10%)] Loss: 19700.980469\n",
      "Train Epoch: 72 [25152/225000 (11%)] Loss: 19966.703125\n",
      "Train Epoch: 72 [27648/225000 (12%)] Loss: 19669.687500\n",
      "Train Epoch: 72 [30144/225000 (13%)] Loss: 19758.359375\n",
      "Train Epoch: 72 [32640/225000 (15%)] Loss: 19648.017578\n",
      "Train Epoch: 72 [35136/225000 (16%)] Loss: 19851.787109\n",
      "Train Epoch: 72 [37632/225000 (17%)] Loss: 20142.597656\n",
      "Train Epoch: 72 [40128/225000 (18%)] Loss: 19649.791016\n",
      "Train Epoch: 72 [42624/225000 (19%)] Loss: 19721.988281\n",
      "Train Epoch: 72 [45120/225000 (20%)] Loss: 19577.746094\n",
      "Train Epoch: 72 [47616/225000 (21%)] Loss: 19484.294922\n",
      "Train Epoch: 72 [50112/225000 (22%)] Loss: 20273.986328\n",
      "Train Epoch: 72 [52608/225000 (23%)] Loss: 19555.910156\n",
      "Train Epoch: 72 [55104/225000 (24%)] Loss: 19553.201172\n",
      "Train Epoch: 72 [57600/225000 (26%)] Loss: 19322.828125\n",
      "Train Epoch: 72 [60096/225000 (27%)] Loss: 19417.238281\n",
      "Train Epoch: 72 [62592/225000 (28%)] Loss: 19526.720703\n",
      "Train Epoch: 72 [65088/225000 (29%)] Loss: 19849.238281\n",
      "Train Epoch: 72 [67584/225000 (30%)] Loss: 19605.804688\n",
      "Train Epoch: 72 [70080/225000 (31%)] Loss: 20150.812500\n",
      "Train Epoch: 72 [72576/225000 (32%)] Loss: 19149.039062\n",
      "Train Epoch: 72 [75072/225000 (33%)] Loss: 20033.693359\n",
      "Train Epoch: 72 [77568/225000 (34%)] Loss: 19545.390625\n",
      "Train Epoch: 72 [80064/225000 (36%)] Loss: 19731.744141\n",
      "Train Epoch: 72 [82560/225000 (37%)] Loss: 20050.042969\n",
      "Train Epoch: 72 [85056/225000 (38%)] Loss: 19727.167969\n",
      "Train Epoch: 72 [87552/225000 (39%)] Loss: 19548.906250\n",
      "Train Epoch: 72 [90048/225000 (40%)] Loss: 19788.679688\n",
      "Train Epoch: 72 [92544/225000 (41%)] Loss: 19886.478516\n",
      "Train Epoch: 72 [95040/225000 (42%)] Loss: 20242.287109\n",
      "Train Epoch: 72 [97536/225000 (43%)] Loss: 19932.328125\n",
      "Train Epoch: 72 [100032/225000 (44%)] Loss: 19971.488281\n",
      "Train Epoch: 72 [102528/225000 (46%)] Loss: 19652.007812\n",
      "Train Epoch: 72 [105024/225000 (47%)] Loss: 19889.101562\n",
      "Train Epoch: 72 [107520/225000 (48%)] Loss: 19839.207031\n",
      "Train Epoch: 72 [110016/225000 (49%)] Loss: 19687.230469\n",
      "Train Epoch: 72 [112512/225000 (50%)] Loss: 19360.960938\n",
      "Train Epoch: 72 [115008/225000 (51%)] Loss: 19605.511719\n",
      "Train Epoch: 72 [117504/225000 (52%)] Loss: 19891.044922\n",
      "Train Epoch: 72 [120000/225000 (53%)] Loss: 19746.363281\n",
      "Train Epoch: 72 [122496/225000 (54%)] Loss: 19808.283203\n",
      "Train Epoch: 72 [124992/225000 (56%)] Loss: 19765.320312\n",
      "Train Epoch: 72 [127488/225000 (57%)] Loss: 19539.916016\n",
      "Train Epoch: 72 [129984/225000 (58%)] Loss: 19888.679688\n",
      "Train Epoch: 72 [132480/225000 (59%)] Loss: 19615.164062\n",
      "Train Epoch: 72 [134976/225000 (60%)] Loss: 19959.488281\n",
      "Train Epoch: 72 [137472/225000 (61%)] Loss: 19930.281250\n",
      "Train Epoch: 72 [139968/225000 (62%)] Loss: 19922.519531\n",
      "Train Epoch: 72 [142464/225000 (63%)] Loss: 19780.320312\n",
      "Train Epoch: 72 [144960/225000 (64%)] Loss: 19964.742188\n",
      "Train Epoch: 72 [147456/225000 (66%)] Loss: 19894.158203\n",
      "Train Epoch: 72 [149952/225000 (67%)] Loss: 19861.349609\n",
      "Train Epoch: 72 [152448/225000 (68%)] Loss: 19597.933594\n",
      "Train Epoch: 72 [154944/225000 (69%)] Loss: 19568.441406\n",
      "Train Epoch: 72 [157440/225000 (70%)] Loss: 19378.257812\n",
      "Train Epoch: 72 [159936/225000 (71%)] Loss: 20215.175781\n",
      "Train Epoch: 72 [162432/225000 (72%)] Loss: 19884.685547\n",
      "Train Epoch: 72 [164928/225000 (73%)] Loss: 19512.748047\n",
      "Train Epoch: 72 [167424/225000 (74%)] Loss: 19836.687500\n",
      "Train Epoch: 72 [169920/225000 (76%)] Loss: 20076.449219\n",
      "Train Epoch: 72 [172416/225000 (77%)] Loss: 19860.210938\n",
      "Train Epoch: 72 [174912/225000 (78%)] Loss: 19709.312500\n",
      "Train Epoch: 72 [177408/225000 (79%)] Loss: 19764.121094\n",
      "Train Epoch: 72 [179904/225000 (80%)] Loss: 19905.376953\n",
      "Train Epoch: 72 [182400/225000 (81%)] Loss: 19759.654297\n",
      "Train Epoch: 72 [184896/225000 (82%)] Loss: 19971.007812\n",
      "Train Epoch: 72 [187392/225000 (83%)] Loss: 19409.613281\n",
      "Train Epoch: 72 [189888/225000 (84%)] Loss: 20164.136719\n",
      "Train Epoch: 72 [192384/225000 (86%)] Loss: 20234.406250\n",
      "Train Epoch: 72 [194880/225000 (87%)] Loss: 20007.513672\n",
      "Train Epoch: 72 [197376/225000 (88%)] Loss: 19395.984375\n",
      "Train Epoch: 72 [199872/225000 (89%)] Loss: 20553.125000\n",
      "Train Epoch: 72 [202368/225000 (90%)] Loss: 19759.156250\n",
      "Train Epoch: 72 [204864/225000 (91%)] Loss: 19909.443359\n",
      "Train Epoch: 72 [207360/225000 (92%)] Loss: 19467.808594\n",
      "Train Epoch: 72 [209856/225000 (93%)] Loss: 19690.894531\n",
      "Train Epoch: 72 [212352/225000 (94%)] Loss: 19718.126953\n",
      "Train Epoch: 72 [214848/225000 (95%)] Loss: 19894.531250\n",
      "Train Epoch: 72 [217344/225000 (97%)] Loss: 19815.949219\n",
      "Train Epoch: 72 [219840/225000 (98%)] Loss: 19590.222656\n",
      "Train Epoch: 72 [222336/225000 (99%)] Loss: 19473.220703\n",
      "Train Epoch: 72 [224832/225000 (100%)] Loss: 19419.535156\n",
      "    epoch          : 72\n",
      "    loss           : 19786.9757509199\n",
      "    val_loss       : 19698.377766461774\n",
      "Train Epoch: 73 [192/225000 (0%)] Loss: 19862.748047\n",
      "Train Epoch: 73 [2688/225000 (1%)] Loss: 20092.755859\n",
      "Train Epoch: 73 [5184/225000 (2%)] Loss: 20254.632812\n",
      "Train Epoch: 73 [7680/225000 (3%)] Loss: 19891.988281\n",
      "Train Epoch: 73 [10176/225000 (5%)] Loss: 19699.617188\n",
      "Train Epoch: 73 [12672/225000 (6%)] Loss: 19739.289062\n",
      "Train Epoch: 73 [15168/225000 (7%)] Loss: 20134.792969\n",
      "Train Epoch: 73 [17664/225000 (8%)] Loss: 20035.238281\n",
      "Train Epoch: 73 [20160/225000 (9%)] Loss: 19411.687500\n",
      "Train Epoch: 73 [22656/225000 (10%)] Loss: 19727.625000\n",
      "Train Epoch: 73 [25152/225000 (11%)] Loss: 19879.003906\n",
      "Train Epoch: 73 [27648/225000 (12%)] Loss: 20125.382812\n",
      "Train Epoch: 73 [30144/225000 (13%)] Loss: 20418.968750\n",
      "Train Epoch: 73 [32640/225000 (15%)] Loss: 20044.917969\n",
      "Train Epoch: 73 [35136/225000 (16%)] Loss: 20041.843750\n",
      "Train Epoch: 73 [37632/225000 (17%)] Loss: 19791.994141\n",
      "Train Epoch: 73 [40128/225000 (18%)] Loss: 19614.800781\n",
      "Train Epoch: 73 [42624/225000 (19%)] Loss: 19642.019531\n",
      "Train Epoch: 73 [45120/225000 (20%)] Loss: 19691.132812\n",
      "Train Epoch: 73 [47616/225000 (21%)] Loss: 19747.859375\n",
      "Train Epoch: 73 [50112/225000 (22%)] Loss: 19571.226562\n",
      "Train Epoch: 73 [52608/225000 (23%)] Loss: 19855.226562\n",
      "Train Epoch: 73 [55104/225000 (24%)] Loss: 19779.707031\n",
      "Train Epoch: 73 [57600/225000 (26%)] Loss: 19680.582031\n",
      "Train Epoch: 73 [60096/225000 (27%)] Loss: 20049.806641\n",
      "Train Epoch: 73 [62592/225000 (28%)] Loss: 19522.824219\n",
      "Train Epoch: 73 [65088/225000 (29%)] Loss: 20280.787109\n",
      "Train Epoch: 73 [67584/225000 (30%)] Loss: 19705.593750\n",
      "Train Epoch: 73 [70080/225000 (31%)] Loss: 19669.933594\n",
      "Train Epoch: 73 [72576/225000 (32%)] Loss: 19891.804688\n",
      "Train Epoch: 73 [75072/225000 (33%)] Loss: 19714.861328\n",
      "Train Epoch: 73 [77568/225000 (34%)] Loss: 20111.191406\n",
      "Train Epoch: 73 [80064/225000 (36%)] Loss: 20193.261719\n",
      "Train Epoch: 73 [82560/225000 (37%)] Loss: 19843.019531\n",
      "Train Epoch: 73 [85056/225000 (38%)] Loss: 19807.062500\n",
      "Train Epoch: 73 [87552/225000 (39%)] Loss: 20120.097656\n",
      "Train Epoch: 73 [90048/225000 (40%)] Loss: 19453.710938\n",
      "Train Epoch: 73 [92544/225000 (41%)] Loss: 19358.257812\n",
      "Train Epoch: 73 [95040/225000 (42%)] Loss: 19506.070312\n",
      "Train Epoch: 73 [97536/225000 (43%)] Loss: 20242.226562\n",
      "Train Epoch: 73 [100032/225000 (44%)] Loss: 20126.218750\n",
      "Train Epoch: 73 [102528/225000 (46%)] Loss: 19865.617188\n",
      "Train Epoch: 73 [105024/225000 (47%)] Loss: 19796.498047\n",
      "Train Epoch: 73 [107520/225000 (48%)] Loss: 19847.539062\n",
      "Train Epoch: 73 [110016/225000 (49%)] Loss: 19965.589844\n",
      "Train Epoch: 73 [112512/225000 (50%)] Loss: 19602.435547\n",
      "Train Epoch: 73 [115008/225000 (51%)] Loss: 19860.820312\n",
      "Train Epoch: 73 [117504/225000 (52%)] Loss: 19680.701172\n",
      "Train Epoch: 73 [120000/225000 (53%)] Loss: 19591.230469\n",
      "Train Epoch: 73 [122496/225000 (54%)] Loss: 19565.226562\n",
      "Train Epoch: 73 [124992/225000 (56%)] Loss: 19834.386719\n",
      "Train Epoch: 73 [127488/225000 (57%)] Loss: 19490.164062\n",
      "Train Epoch: 73 [129984/225000 (58%)] Loss: 19234.031250\n",
      "Train Epoch: 73 [132480/225000 (59%)] Loss: 19998.527344\n",
      "Train Epoch: 73 [134976/225000 (60%)] Loss: 19324.136719\n",
      "Train Epoch: 73 [137472/225000 (61%)] Loss: 20141.046875\n",
      "Train Epoch: 73 [139968/225000 (62%)] Loss: 19490.042969\n",
      "Train Epoch: 73 [142464/225000 (63%)] Loss: 19916.027344\n",
      "Train Epoch: 73 [144960/225000 (64%)] Loss: 19690.753906\n",
      "Train Epoch: 73 [147456/225000 (66%)] Loss: 19744.855469\n",
      "Train Epoch: 73 [149952/225000 (67%)] Loss: 19782.587891\n",
      "Train Epoch: 73 [152448/225000 (68%)] Loss: 19809.941406\n",
      "Train Epoch: 73 [154944/225000 (69%)] Loss: 19685.193359\n",
      "Train Epoch: 73 [157440/225000 (70%)] Loss: 19739.421875\n",
      "Train Epoch: 73 [159936/225000 (71%)] Loss: 19739.851562\n",
      "Train Epoch: 73 [162432/225000 (72%)] Loss: 19483.054688\n",
      "Train Epoch: 73 [164928/225000 (73%)] Loss: 19471.945312\n",
      "Train Epoch: 73 [167424/225000 (74%)] Loss: 19455.582031\n",
      "Train Epoch: 73 [169920/225000 (76%)] Loss: 19711.248047\n",
      "Train Epoch: 73 [172416/225000 (77%)] Loss: 19260.357422\n",
      "Train Epoch: 73 [174912/225000 (78%)] Loss: 19725.039062\n",
      "Train Epoch: 73 [177408/225000 (79%)] Loss: 20080.556641\n",
      "Train Epoch: 73 [179904/225000 (80%)] Loss: 19663.914062\n",
      "Train Epoch: 73 [182400/225000 (81%)] Loss: 19922.734375\n",
      "Train Epoch: 73 [184896/225000 (82%)] Loss: 19443.144531\n",
      "Train Epoch: 73 [187392/225000 (83%)] Loss: 19861.263672\n",
      "Train Epoch: 73 [189888/225000 (84%)] Loss: 19847.347656\n",
      "Train Epoch: 73 [192384/225000 (86%)] Loss: 19828.601562\n",
      "Train Epoch: 73 [194880/225000 (87%)] Loss: 19890.798828\n",
      "Train Epoch: 73 [197376/225000 (88%)] Loss: 19974.378906\n",
      "Train Epoch: 73 [199872/225000 (89%)] Loss: 19772.533203\n",
      "Train Epoch: 73 [202368/225000 (90%)] Loss: 20162.726562\n",
      "Train Epoch: 73 [204864/225000 (91%)] Loss: 19848.673828\n",
      "Train Epoch: 73 [207360/225000 (92%)] Loss: 19646.195312\n",
      "Train Epoch: 73 [209856/225000 (93%)] Loss: 19926.189453\n",
      "Train Epoch: 73 [212352/225000 (94%)] Loss: 20211.429688\n",
      "Train Epoch: 73 [214848/225000 (95%)] Loss: 19590.203125\n",
      "Train Epoch: 73 [217344/225000 (97%)] Loss: 19896.837891\n",
      "Train Epoch: 73 [219840/225000 (98%)] Loss: 19868.220703\n",
      "Train Epoch: 73 [222336/225000 (99%)] Loss: 19601.742188\n",
      "Train Epoch: 73 [224832/225000 (100%)] Loss: 19620.382812\n",
      "    epoch          : 73\n",
      "    loss           : 19767.37610154917\n",
      "    val_loss       : 19668.679984868028\n",
      "Train Epoch: 74 [192/225000 (0%)] Loss: 19516.175781\n",
      "Train Epoch: 74 [2688/225000 (1%)] Loss: 19896.027344\n",
      "Train Epoch: 74 [5184/225000 (2%)] Loss: 19978.062500\n",
      "Train Epoch: 74 [7680/225000 (3%)] Loss: 19431.546875\n",
      "Train Epoch: 74 [10176/225000 (5%)] Loss: 19736.773438\n",
      "Train Epoch: 74 [12672/225000 (6%)] Loss: 19094.613281\n",
      "Train Epoch: 74 [15168/225000 (7%)] Loss: 19467.003906\n",
      "Train Epoch: 74 [17664/225000 (8%)] Loss: 19359.503906\n",
      "Train Epoch: 74 [20160/225000 (9%)] Loss: 20234.261719\n",
      "Train Epoch: 74 [22656/225000 (10%)] Loss: 19839.441406\n",
      "Train Epoch: 74 [25152/225000 (11%)] Loss: 20064.320312\n",
      "Train Epoch: 74 [27648/225000 (12%)] Loss: 19626.324219\n",
      "Train Epoch: 74 [30144/225000 (13%)] Loss: 19820.816406\n",
      "Train Epoch: 74 [32640/225000 (15%)] Loss: 19509.761719\n",
      "Train Epoch: 74 [35136/225000 (16%)] Loss: 19409.414062\n",
      "Train Epoch: 74 [37632/225000 (17%)] Loss: 19725.853516\n",
      "Train Epoch: 74 [40128/225000 (18%)] Loss: 19592.039062\n",
      "Train Epoch: 74 [42624/225000 (19%)] Loss: 19827.617188\n",
      "Train Epoch: 74 [45120/225000 (20%)] Loss: 19681.464844\n",
      "Train Epoch: 74 [47616/225000 (21%)] Loss: 19739.753906\n",
      "Train Epoch: 74 [50112/225000 (22%)] Loss: 19447.322266\n",
      "Train Epoch: 74 [52608/225000 (23%)] Loss: 19532.808594\n",
      "Train Epoch: 74 [55104/225000 (24%)] Loss: 19586.113281\n",
      "Train Epoch: 74 [57600/225000 (26%)] Loss: 20143.708984\n",
      "Train Epoch: 74 [60096/225000 (27%)] Loss: 19671.960938\n",
      "Train Epoch: 74 [62592/225000 (28%)] Loss: 19565.027344\n",
      "Train Epoch: 74 [65088/225000 (29%)] Loss: 19364.246094\n",
      "Train Epoch: 74 [67584/225000 (30%)] Loss: 19965.021484\n",
      "Train Epoch: 74 [70080/225000 (31%)] Loss: 20016.775391\n",
      "Train Epoch: 74 [72576/225000 (32%)] Loss: 19824.326172\n",
      "Train Epoch: 74 [75072/225000 (33%)] Loss: 19632.666016\n",
      "Train Epoch: 74 [77568/225000 (34%)] Loss: 20139.283203\n",
      "Train Epoch: 74 [80064/225000 (36%)] Loss: 19431.048828\n",
      "Train Epoch: 74 [82560/225000 (37%)] Loss: 19807.958984\n",
      "Train Epoch: 74 [85056/225000 (38%)] Loss: 19344.369141\n",
      "Train Epoch: 74 [87552/225000 (39%)] Loss: 19293.433594\n",
      "Train Epoch: 74 [90048/225000 (40%)] Loss: 19712.964844\n",
      "Train Epoch: 74 [92544/225000 (41%)] Loss: 19580.886719\n",
      "Train Epoch: 74 [95040/225000 (42%)] Loss: 19571.339844\n",
      "Train Epoch: 74 [97536/225000 (43%)] Loss: 19379.189453\n",
      "Train Epoch: 74 [100032/225000 (44%)] Loss: 19749.261719\n",
      "Train Epoch: 74 [102528/225000 (46%)] Loss: 19540.648438\n",
      "Train Epoch: 74 [105024/225000 (47%)] Loss: 19461.792969\n",
      "Train Epoch: 74 [107520/225000 (48%)] Loss: 19675.675781\n",
      "Train Epoch: 74 [110016/225000 (49%)] Loss: 19602.775391\n",
      "Train Epoch: 74 [112512/225000 (50%)] Loss: 19957.660156\n",
      "Train Epoch: 74 [115008/225000 (51%)] Loss: 19504.707031\n",
      "Train Epoch: 74 [117504/225000 (52%)] Loss: 19710.216797\n",
      "Train Epoch: 74 [120000/225000 (53%)] Loss: 19636.179688\n",
      "Train Epoch: 74 [122496/225000 (54%)] Loss: 19487.828125\n",
      "Train Epoch: 74 [124992/225000 (56%)] Loss: 19687.566406\n",
      "Train Epoch: 74 [127488/225000 (57%)] Loss: 20129.830078\n",
      "Train Epoch: 74 [129984/225000 (58%)] Loss: 19839.519531\n",
      "Train Epoch: 74 [132480/225000 (59%)] Loss: 19705.187500\n",
      "Train Epoch: 74 [134976/225000 (60%)] Loss: 19854.734375\n",
      "Train Epoch: 74 [137472/225000 (61%)] Loss: 19523.732422\n",
      "Train Epoch: 74 [139968/225000 (62%)] Loss: 20019.335938\n",
      "Train Epoch: 74 [142464/225000 (63%)] Loss: 19532.123047\n",
      "Train Epoch: 74 [144960/225000 (64%)] Loss: 19635.917969\n",
      "Train Epoch: 74 [147456/225000 (66%)] Loss: 19889.250000\n",
      "Train Epoch: 74 [149952/225000 (67%)] Loss: 20049.998047\n",
      "Train Epoch: 74 [152448/225000 (68%)] Loss: 19461.804688\n",
      "Train Epoch: 74 [154944/225000 (69%)] Loss: 19595.546875\n",
      "Train Epoch: 74 [157440/225000 (70%)] Loss: 19794.501953\n",
      "Train Epoch: 74 [159936/225000 (71%)] Loss: 20221.791016\n",
      "Train Epoch: 74 [162432/225000 (72%)] Loss: 20034.626953\n",
      "Train Epoch: 74 [164928/225000 (73%)] Loss: 19679.230469\n",
      "Train Epoch: 74 [167424/225000 (74%)] Loss: 19730.880859\n",
      "Train Epoch: 74 [169920/225000 (76%)] Loss: 20334.082031\n",
      "Train Epoch: 74 [172416/225000 (77%)] Loss: 19361.410156\n",
      "Train Epoch: 74 [174912/225000 (78%)] Loss: 19865.791016\n",
      "Train Epoch: 74 [177408/225000 (79%)] Loss: 19807.382812\n",
      "Train Epoch: 74 [179904/225000 (80%)] Loss: 20218.261719\n",
      "Train Epoch: 74 [182400/225000 (81%)] Loss: 19572.503906\n",
      "Train Epoch: 74 [184896/225000 (82%)] Loss: 19497.300781\n",
      "Train Epoch: 74 [187392/225000 (83%)] Loss: 20158.996094\n",
      "Train Epoch: 74 [189888/225000 (84%)] Loss: 20020.587891\n",
      "Train Epoch: 74 [192384/225000 (86%)] Loss: 19988.505859\n",
      "Train Epoch: 74 [194880/225000 (87%)] Loss: 19992.580078\n",
      "Train Epoch: 74 [197376/225000 (88%)] Loss: 19765.585938\n",
      "Train Epoch: 74 [199872/225000 (89%)] Loss: 19800.181641\n",
      "Train Epoch: 74 [202368/225000 (90%)] Loss: 19433.824219\n",
      "Train Epoch: 74 [204864/225000 (91%)] Loss: 19721.980469\n",
      "Train Epoch: 74 [207360/225000 (92%)] Loss: 19264.333984\n",
      "Train Epoch: 74 [209856/225000 (93%)] Loss: 19529.457031\n",
      "Train Epoch: 74 [212352/225000 (94%)] Loss: 19906.248047\n",
      "Train Epoch: 74 [214848/225000 (95%)] Loss: 19126.500000\n",
      "Train Epoch: 74 [217344/225000 (97%)] Loss: 19471.970703\n",
      "Train Epoch: 74 [219840/225000 (98%)] Loss: 19756.441406\n",
      "Train Epoch: 74 [222336/225000 (99%)] Loss: 19955.537109\n",
      "Train Epoch: 74 [224832/225000 (100%)] Loss: 19887.843750\n",
      "    epoch          : 74\n",
      "    loss           : 19755.92770104522\n",
      "    val_loss       : 19744.568580573752\n",
      "Train Epoch: 75 [192/225000 (0%)] Loss: 20053.093750\n",
      "Train Epoch: 75 [2688/225000 (1%)] Loss: 19755.433594\n",
      "Train Epoch: 75 [5184/225000 (2%)] Loss: 20012.753906\n",
      "Train Epoch: 75 [7680/225000 (3%)] Loss: 19344.423828\n",
      "Train Epoch: 75 [10176/225000 (5%)] Loss: 19535.912109\n",
      "Train Epoch: 75 [12672/225000 (6%)] Loss: 19846.433594\n",
      "Train Epoch: 75 [15168/225000 (7%)] Loss: 19919.916016\n",
      "Train Epoch: 75 [17664/225000 (8%)] Loss: 19675.574219\n",
      "Train Epoch: 75 [20160/225000 (9%)] Loss: 19409.998047\n",
      "Train Epoch: 75 [22656/225000 (10%)] Loss: 19622.660156\n",
      "Train Epoch: 75 [25152/225000 (11%)] Loss: 19636.875000\n",
      "Train Epoch: 75 [27648/225000 (12%)] Loss: 19821.923828\n",
      "Train Epoch: 75 [30144/225000 (13%)] Loss: 19547.537109\n",
      "Train Epoch: 75 [32640/225000 (15%)] Loss: 19666.931641\n",
      "Train Epoch: 75 [35136/225000 (16%)] Loss: 19203.958984\n",
      "Train Epoch: 75 [37632/225000 (17%)] Loss: 19631.945312\n",
      "Train Epoch: 75 [40128/225000 (18%)] Loss: 19906.703125\n",
      "Train Epoch: 75 [42624/225000 (19%)] Loss: 19061.617188\n",
      "Train Epoch: 75 [45120/225000 (20%)] Loss: 19706.710938\n",
      "Train Epoch: 75 [47616/225000 (21%)] Loss: 19762.056641\n",
      "Train Epoch: 75 [50112/225000 (22%)] Loss: 19886.580078\n",
      "Train Epoch: 75 [52608/225000 (23%)] Loss: 19887.183594\n",
      "Train Epoch: 75 [55104/225000 (24%)] Loss: 20042.023438\n",
      "Train Epoch: 75 [57600/225000 (26%)] Loss: 20082.052734\n",
      "Train Epoch: 75 [60096/225000 (27%)] Loss: 19823.617188\n",
      "Train Epoch: 75 [62592/225000 (28%)] Loss: 19638.535156\n",
      "Train Epoch: 75 [65088/225000 (29%)] Loss: 19466.640625\n",
      "Train Epoch: 75 [67584/225000 (30%)] Loss: 19598.916016\n",
      "Train Epoch: 75 [70080/225000 (31%)] Loss: 19551.279297\n",
      "Train Epoch: 75 [72576/225000 (32%)] Loss: 19606.324219\n",
      "Train Epoch: 75 [75072/225000 (33%)] Loss: 19828.558594\n",
      "Train Epoch: 75 [77568/225000 (34%)] Loss: 19722.406250\n",
      "Train Epoch: 75 [80064/225000 (36%)] Loss: 19648.960938\n",
      "Train Epoch: 75 [82560/225000 (37%)] Loss: 19711.097656\n",
      "Train Epoch: 75 [85056/225000 (38%)] Loss: 19723.062500\n",
      "Train Epoch: 75 [87552/225000 (39%)] Loss: 19295.214844\n",
      "Train Epoch: 75 [90048/225000 (40%)] Loss: 19945.527344\n",
      "Train Epoch: 75 [92544/225000 (41%)] Loss: 19398.359375\n",
      "Train Epoch: 75 [95040/225000 (42%)] Loss: 19955.142578\n",
      "Train Epoch: 75 [97536/225000 (43%)] Loss: 19444.867188\n",
      "Train Epoch: 75 [100032/225000 (44%)] Loss: 19466.224609\n",
      "Train Epoch: 75 [102528/225000 (46%)] Loss: 19903.175781\n",
      "Train Epoch: 75 [105024/225000 (47%)] Loss: 20147.804688\n",
      "Train Epoch: 75 [107520/225000 (48%)] Loss: 20016.222656\n",
      "Train Epoch: 75 [110016/225000 (49%)] Loss: 20229.347656\n",
      "Train Epoch: 75 [112512/225000 (50%)] Loss: 19633.171875\n",
      "Train Epoch: 75 [115008/225000 (51%)] Loss: 19875.041016\n",
      "Train Epoch: 75 [117504/225000 (52%)] Loss: 19894.224609\n",
      "Train Epoch: 75 [120000/225000 (53%)] Loss: 19779.843750\n",
      "Train Epoch: 75 [122496/225000 (54%)] Loss: 19854.199219\n",
      "Train Epoch: 75 [124992/225000 (56%)] Loss: 20107.484375\n",
      "Train Epoch: 75 [127488/225000 (57%)] Loss: 19649.992188\n",
      "Train Epoch: 75 [129984/225000 (58%)] Loss: 19700.000000\n",
      "Train Epoch: 75 [132480/225000 (59%)] Loss: 19914.570312\n",
      "Train Epoch: 75 [134976/225000 (60%)] Loss: 19886.945312\n",
      "Train Epoch: 75 [137472/225000 (61%)] Loss: 20212.140625\n",
      "Train Epoch: 75 [139968/225000 (62%)] Loss: 20006.625000\n",
      "Train Epoch: 75 [142464/225000 (63%)] Loss: 19559.197266\n",
      "Train Epoch: 75 [144960/225000 (64%)] Loss: 18802.445312\n",
      "Train Epoch: 75 [147456/225000 (66%)] Loss: 19445.562500\n",
      "Train Epoch: 75 [149952/225000 (67%)] Loss: 19934.515625\n",
      "Train Epoch: 75 [152448/225000 (68%)] Loss: 20188.150391\n",
      "Train Epoch: 75 [154944/225000 (69%)] Loss: 19785.828125\n",
      "Train Epoch: 75 [157440/225000 (70%)] Loss: 19768.443359\n",
      "Train Epoch: 75 [159936/225000 (71%)] Loss: 20246.044922\n",
      "Train Epoch: 75 [162432/225000 (72%)] Loss: 19426.496094\n",
      "Train Epoch: 75 [164928/225000 (73%)] Loss: 19651.035156\n",
      "Train Epoch: 75 [167424/225000 (74%)] Loss: 19459.759766\n",
      "Train Epoch: 75 [169920/225000 (76%)] Loss: 19317.847656\n",
      "Train Epoch: 75 [172416/225000 (77%)] Loss: 19567.398438\n",
      "Train Epoch: 75 [174912/225000 (78%)] Loss: 19576.039062\n",
      "Train Epoch: 75 [177408/225000 (79%)] Loss: 20006.378906\n",
      "Train Epoch: 75 [179904/225000 (80%)] Loss: 19301.619141\n",
      "Train Epoch: 75 [182400/225000 (81%)] Loss: 19324.824219\n",
      "Train Epoch: 75 [184896/225000 (82%)] Loss: 20203.214844\n",
      "Train Epoch: 75 [187392/225000 (83%)] Loss: 20607.433594\n",
      "Train Epoch: 75 [189888/225000 (84%)] Loss: 19718.949219\n",
      "Train Epoch: 75 [192384/225000 (86%)] Loss: 19484.535156\n",
      "Train Epoch: 75 [194880/225000 (87%)] Loss: 19412.251953\n",
      "Train Epoch: 75 [197376/225000 (88%)] Loss: 19670.519531\n",
      "Train Epoch: 75 [199872/225000 (89%)] Loss: 19750.261719\n",
      "Train Epoch: 75 [202368/225000 (90%)] Loss: 20197.716797\n",
      "Train Epoch: 75 [204864/225000 (91%)] Loss: 20091.001953\n",
      "Train Epoch: 75 [207360/225000 (92%)] Loss: 20025.724609\n",
      "Train Epoch: 75 [209856/225000 (93%)] Loss: 19643.265625\n",
      "Train Epoch: 75 [212352/225000 (94%)] Loss: 19705.835938\n",
      "Train Epoch: 75 [214848/225000 (95%)] Loss: 19753.507812\n",
      "Train Epoch: 75 [217344/225000 (97%)] Loss: 19862.681641\n",
      "Train Epoch: 75 [219840/225000 (98%)] Loss: 19945.156250\n",
      "Train Epoch: 75 [222336/225000 (99%)] Loss: 19861.554688\n",
      "Train Epoch: 75 [224832/225000 (100%)] Loss: 19515.945312\n",
      "    epoch          : 75\n",
      "    loss           : 19739.779015238375\n",
      "    val_loss       : 19650.450103441268\n",
      "Train Epoch: 76 [192/225000 (0%)] Loss: 20140.707031\n",
      "Train Epoch: 76 [2688/225000 (1%)] Loss: 19377.265625\n",
      "Train Epoch: 76 [5184/225000 (2%)] Loss: 20100.121094\n",
      "Train Epoch: 76 [7680/225000 (3%)] Loss: 19820.625000\n",
      "Train Epoch: 76 [10176/225000 (5%)] Loss: 19995.574219\n",
      "Train Epoch: 76 [12672/225000 (6%)] Loss: 20032.177734\n",
      "Train Epoch: 76 [15168/225000 (7%)] Loss: 19397.583984\n",
      "Train Epoch: 76 [17664/225000 (8%)] Loss: 20105.816406\n",
      "Train Epoch: 76 [20160/225000 (9%)] Loss: 19521.085938\n",
      "Train Epoch: 76 [22656/225000 (10%)] Loss: 19240.185547\n",
      "Train Epoch: 76 [25152/225000 (11%)] Loss: 19825.953125\n",
      "Train Epoch: 76 [27648/225000 (12%)] Loss: 19495.546875\n",
      "Train Epoch: 76 [30144/225000 (13%)] Loss: 19497.156250\n",
      "Train Epoch: 76 [32640/225000 (15%)] Loss: 19340.960938\n",
      "Train Epoch: 76 [35136/225000 (16%)] Loss: 19918.656250\n",
      "Train Epoch: 76 [37632/225000 (17%)] Loss: 20126.156250\n",
      "Train Epoch: 76 [40128/225000 (18%)] Loss: 19761.757812\n",
      "Train Epoch: 76 [42624/225000 (19%)] Loss: 18964.726562\n",
      "Train Epoch: 76 [45120/225000 (20%)] Loss: 19897.238281\n",
      "Train Epoch: 76 [47616/225000 (21%)] Loss: 19766.925781\n",
      "Train Epoch: 76 [50112/225000 (22%)] Loss: 19783.410156\n",
      "Train Epoch: 76 [52608/225000 (23%)] Loss: 20102.144531\n",
      "Train Epoch: 76 [55104/225000 (24%)] Loss: 19777.523438\n",
      "Train Epoch: 76 [57600/225000 (26%)] Loss: 19890.054688\n",
      "Train Epoch: 76 [60096/225000 (27%)] Loss: 19484.605469\n",
      "Train Epoch: 76 [62592/225000 (28%)] Loss: 19519.726562\n",
      "Train Epoch: 76 [65088/225000 (29%)] Loss: 19892.527344\n",
      "Train Epoch: 76 [67584/225000 (30%)] Loss: 19652.755859\n",
      "Train Epoch: 76 [70080/225000 (31%)] Loss: 19975.695312\n",
      "Train Epoch: 76 [72576/225000 (32%)] Loss: 19113.906250\n",
      "Train Epoch: 76 [75072/225000 (33%)] Loss: 19906.021484\n",
      "Train Epoch: 76 [77568/225000 (34%)] Loss: 19600.427734\n",
      "Train Epoch: 76 [80064/225000 (36%)] Loss: 20328.183594\n",
      "Train Epoch: 76 [82560/225000 (37%)] Loss: 19662.898438\n",
      "Train Epoch: 76 [85056/225000 (38%)] Loss: 19432.939453\n",
      "Train Epoch: 76 [87552/225000 (39%)] Loss: 20181.035156\n",
      "Train Epoch: 76 [90048/225000 (40%)] Loss: 19723.984375\n",
      "Train Epoch: 76 [92544/225000 (41%)] Loss: 20111.974609\n",
      "Train Epoch: 76 [95040/225000 (42%)] Loss: 19570.500000\n",
      "Train Epoch: 76 [97536/225000 (43%)] Loss: 19604.960938\n",
      "Train Epoch: 76 [100032/225000 (44%)] Loss: 19881.378906\n",
      "Train Epoch: 76 [102528/225000 (46%)] Loss: 19835.763672\n",
      "Train Epoch: 76 [105024/225000 (47%)] Loss: 19785.644531\n",
      "Train Epoch: 76 [107520/225000 (48%)] Loss: 19610.199219\n",
      "Train Epoch: 76 [110016/225000 (49%)] Loss: 19843.066406\n",
      "Train Epoch: 76 [112512/225000 (50%)] Loss: 20149.041016\n",
      "Train Epoch: 76 [115008/225000 (51%)] Loss: 20086.531250\n",
      "Train Epoch: 76 [117504/225000 (52%)] Loss: 19569.941406\n",
      "Train Epoch: 76 [120000/225000 (53%)] Loss: 19549.410156\n",
      "Train Epoch: 76 [122496/225000 (54%)] Loss: 19645.734375\n",
      "Train Epoch: 76 [124992/225000 (56%)] Loss: 20127.851562\n",
      "Train Epoch: 76 [127488/225000 (57%)] Loss: 18935.802734\n",
      "Train Epoch: 76 [129984/225000 (58%)] Loss: 19501.875000\n",
      "Train Epoch: 76 [132480/225000 (59%)] Loss: 20166.707031\n",
      "Train Epoch: 76 [134976/225000 (60%)] Loss: 19601.759766\n",
      "Train Epoch: 76 [137472/225000 (61%)] Loss: 19708.863281\n",
      "Train Epoch: 76 [139968/225000 (62%)] Loss: 19917.203125\n",
      "Train Epoch: 76 [142464/225000 (63%)] Loss: 20357.689453\n",
      "Train Epoch: 76 [144960/225000 (64%)] Loss: 19808.816406\n",
      "Train Epoch: 76 [147456/225000 (66%)] Loss: 19828.367188\n",
      "Train Epoch: 76 [149952/225000 (67%)] Loss: 19696.386719\n",
      "Train Epoch: 76 [152448/225000 (68%)] Loss: 19639.828125\n",
      "Train Epoch: 76 [154944/225000 (69%)] Loss: 20049.535156\n",
      "Train Epoch: 76 [157440/225000 (70%)] Loss: 19686.931641\n",
      "Train Epoch: 76 [159936/225000 (71%)] Loss: 19309.294922\n",
      "Train Epoch: 76 [162432/225000 (72%)] Loss: 20043.210938\n",
      "Train Epoch: 76 [164928/225000 (73%)] Loss: 19629.601562\n",
      "Train Epoch: 76 [167424/225000 (74%)] Loss: 20164.789062\n",
      "Train Epoch: 76 [169920/225000 (76%)] Loss: 19378.044922\n",
      "Train Epoch: 76 [172416/225000 (77%)] Loss: 19650.195312\n",
      "Train Epoch: 76 [174912/225000 (78%)] Loss: 19973.701172\n",
      "Train Epoch: 76 [177408/225000 (79%)] Loss: 19610.441406\n",
      "Train Epoch: 76 [179904/225000 (80%)] Loss: 19808.351562\n",
      "Train Epoch: 76 [182400/225000 (81%)] Loss: 19333.367188\n",
      "Train Epoch: 76 [184896/225000 (82%)] Loss: 19316.730469\n",
      "Train Epoch: 76 [187392/225000 (83%)] Loss: 20291.046875\n",
      "Train Epoch: 76 [189888/225000 (84%)] Loss: 19896.519531\n",
      "Train Epoch: 76 [192384/225000 (86%)] Loss: 20156.265625\n",
      "Train Epoch: 76 [194880/225000 (87%)] Loss: 19708.550781\n",
      "Train Epoch: 76 [197376/225000 (88%)] Loss: 19548.988281\n",
      "Train Epoch: 76 [199872/225000 (89%)] Loss: 19680.005859\n",
      "Train Epoch: 76 [202368/225000 (90%)] Loss: 19999.708984\n",
      "Train Epoch: 76 [204864/225000 (91%)] Loss: 19653.740234\n",
      "Train Epoch: 76 [207360/225000 (92%)] Loss: 19418.378906\n",
      "Train Epoch: 76 [209856/225000 (93%)] Loss: 19529.601562\n",
      "Train Epoch: 76 [212352/225000 (94%)] Loss: 19667.251953\n",
      "Train Epoch: 76 [214848/225000 (95%)] Loss: 19331.250000\n",
      "Train Epoch: 76 [217344/225000 (97%)] Loss: 19401.148438\n",
      "Train Epoch: 76 [219840/225000 (98%)] Loss: 19472.980469\n",
      "Train Epoch: 76 [222336/225000 (99%)] Loss: 19650.484375\n",
      "Train Epoch: 76 [224832/225000 (100%)] Loss: 20165.357422\n",
      "    epoch          : 76\n",
      "    loss           : 19732.591786876066\n",
      "    val_loss       : 19655.37252277454\n",
      "Train Epoch: 77 [192/225000 (0%)] Loss: 20119.080078\n",
      "Train Epoch: 77 [2688/225000 (1%)] Loss: 19665.054688\n",
      "Train Epoch: 77 [5184/225000 (2%)] Loss: 19763.597656\n",
      "Train Epoch: 77 [7680/225000 (3%)] Loss: 19648.429688\n",
      "Train Epoch: 77 [10176/225000 (5%)] Loss: 20071.808594\n",
      "Train Epoch: 77 [12672/225000 (6%)] Loss: 19513.285156\n",
      "Train Epoch: 77 [15168/225000 (7%)] Loss: 20127.574219\n",
      "Train Epoch: 77 [17664/225000 (8%)] Loss: 20017.007812\n",
      "Train Epoch: 77 [20160/225000 (9%)] Loss: 19903.304688\n",
      "Train Epoch: 77 [22656/225000 (10%)] Loss: 19901.767578\n",
      "Train Epoch: 77 [25152/225000 (11%)] Loss: 19772.458984\n",
      "Train Epoch: 77 [27648/225000 (12%)] Loss: 19566.929688\n",
      "Train Epoch: 77 [30144/225000 (13%)] Loss: 19804.847656\n",
      "Train Epoch: 77 [32640/225000 (15%)] Loss: 19944.023438\n",
      "Train Epoch: 77 [35136/225000 (16%)] Loss: 19458.597656\n",
      "Train Epoch: 77 [37632/225000 (17%)] Loss: 19705.546875\n",
      "Train Epoch: 77 [40128/225000 (18%)] Loss: 19305.238281\n",
      "Train Epoch: 77 [42624/225000 (19%)] Loss: 19559.820312\n",
      "Train Epoch: 77 [45120/225000 (20%)] Loss: 19783.109375\n",
      "Train Epoch: 77 [47616/225000 (21%)] Loss: 20003.810547\n",
      "Train Epoch: 77 [50112/225000 (22%)] Loss: 19779.515625\n",
      "Train Epoch: 77 [52608/225000 (23%)] Loss: 19808.050781\n",
      "Train Epoch: 77 [55104/225000 (24%)] Loss: 19682.015625\n",
      "Train Epoch: 77 [57600/225000 (26%)] Loss: 19314.263672\n",
      "Train Epoch: 77 [60096/225000 (27%)] Loss: 20143.531250\n",
      "Train Epoch: 77 [62592/225000 (28%)] Loss: 19425.500000\n",
      "Train Epoch: 77 [65088/225000 (29%)] Loss: 20174.781250\n",
      "Train Epoch: 77 [67584/225000 (30%)] Loss: 19949.441406\n",
      "Train Epoch: 77 [70080/225000 (31%)] Loss: 19449.613281\n",
      "Train Epoch: 77 [72576/225000 (32%)] Loss: 19689.527344\n",
      "Train Epoch: 77 [75072/225000 (33%)] Loss: 20176.304688\n",
      "Train Epoch: 77 [77568/225000 (34%)] Loss: 19901.351562\n",
      "Train Epoch: 77 [80064/225000 (36%)] Loss: 20105.513672\n",
      "Train Epoch: 77 [82560/225000 (37%)] Loss: 19752.031250\n",
      "Train Epoch: 77 [85056/225000 (38%)] Loss: 19425.091797\n",
      "Train Epoch: 77 [87552/225000 (39%)] Loss: 19684.500000\n",
      "Train Epoch: 77 [90048/225000 (40%)] Loss: 19777.906250\n",
      "Train Epoch: 77 [92544/225000 (41%)] Loss: 19731.769531\n",
      "Train Epoch: 77 [95040/225000 (42%)] Loss: 19660.726562\n",
      "Train Epoch: 77 [97536/225000 (43%)] Loss: 19456.265625\n",
      "Train Epoch: 77 [100032/225000 (44%)] Loss: 19925.480469\n",
      "Train Epoch: 77 [102528/225000 (46%)] Loss: 20043.714844\n",
      "Train Epoch: 77 [105024/225000 (47%)] Loss: 20232.000000\n",
      "Train Epoch: 77 [107520/225000 (48%)] Loss: 19373.082031\n",
      "Train Epoch: 77 [110016/225000 (49%)] Loss: 19918.648438\n",
      "Train Epoch: 77 [112512/225000 (50%)] Loss: 19741.902344\n",
      "Train Epoch: 77 [115008/225000 (51%)] Loss: 19928.523438\n",
      "Train Epoch: 77 [117504/225000 (52%)] Loss: 20030.023438\n",
      "Train Epoch: 77 [120000/225000 (53%)] Loss: 19716.210938\n",
      "Train Epoch: 77 [122496/225000 (54%)] Loss: 19991.351562\n",
      "Train Epoch: 77 [124992/225000 (56%)] Loss: 19406.369141\n",
      "Train Epoch: 77 [127488/225000 (57%)] Loss: 19297.371094\n",
      "Train Epoch: 77 [129984/225000 (58%)] Loss: 20002.611328\n",
      "Train Epoch: 77 [132480/225000 (59%)] Loss: 19917.878906\n",
      "Train Epoch: 77 [134976/225000 (60%)] Loss: 19857.125000\n",
      "Train Epoch: 77 [137472/225000 (61%)] Loss: 20137.751953\n",
      "Train Epoch: 77 [139968/225000 (62%)] Loss: 19551.921875\n",
      "Train Epoch: 77 [142464/225000 (63%)] Loss: 19301.275391\n",
      "Train Epoch: 77 [144960/225000 (64%)] Loss: 19849.007812\n",
      "Train Epoch: 77 [147456/225000 (66%)] Loss: 20105.802734\n",
      "Train Epoch: 77 [149952/225000 (67%)] Loss: 19666.843750\n",
      "Train Epoch: 77 [152448/225000 (68%)] Loss: 19781.851562\n",
      "Train Epoch: 77 [154944/225000 (69%)] Loss: 19749.710938\n",
      "Train Epoch: 77 [157440/225000 (70%)] Loss: 19943.427734\n",
      "Train Epoch: 77 [159936/225000 (71%)] Loss: 19707.902344\n",
      "Train Epoch: 77 [162432/225000 (72%)] Loss: 19749.082031\n",
      "Train Epoch: 77 [164928/225000 (73%)] Loss: 19461.796875\n",
      "Train Epoch: 77 [167424/225000 (74%)] Loss: 19767.042969\n",
      "Train Epoch: 77 [169920/225000 (76%)] Loss: 19682.886719\n",
      "Train Epoch: 77 [172416/225000 (77%)] Loss: 19313.250000\n",
      "Train Epoch: 77 [174912/225000 (78%)] Loss: 19443.238281\n",
      "Train Epoch: 77 [177408/225000 (79%)] Loss: 19451.781250\n",
      "Train Epoch: 77 [179904/225000 (80%)] Loss: 19576.191406\n",
      "Train Epoch: 77 [182400/225000 (81%)] Loss: 19295.925781\n",
      "Train Epoch: 77 [184896/225000 (82%)] Loss: 19570.312500\n",
      "Train Epoch: 77 [187392/225000 (83%)] Loss: 19851.945312\n",
      "Train Epoch: 77 [189888/225000 (84%)] Loss: 19344.927734\n",
      "Train Epoch: 77 [192384/225000 (86%)] Loss: 19961.935547\n",
      "Train Epoch: 77 [194880/225000 (87%)] Loss: 19374.000000\n",
      "Train Epoch: 77 [197376/225000 (88%)] Loss: 19920.371094\n",
      "Train Epoch: 77 [199872/225000 (89%)] Loss: 19262.628906\n",
      "Train Epoch: 77 [202368/225000 (90%)] Loss: 19724.375000\n",
      "Train Epoch: 77 [204864/225000 (91%)] Loss: 19803.847656\n",
      "Train Epoch: 77 [207360/225000 (92%)] Loss: 19618.277344\n",
      "Train Epoch: 77 [209856/225000 (93%)] Loss: 19885.367188\n",
      "Train Epoch: 77 [212352/225000 (94%)] Loss: 19732.718750\n",
      "Train Epoch: 77 [214848/225000 (95%)] Loss: 19741.943359\n",
      "Train Epoch: 77 [217344/225000 (97%)] Loss: 19648.232422\n",
      "Train Epoch: 77 [219840/225000 (98%)] Loss: 19505.750000\n",
      "Train Epoch: 77 [222336/225000 (99%)] Loss: 19783.824219\n",
      "Train Epoch: 77 [224832/225000 (100%)] Loss: 19957.402344\n",
      "    epoch          : 77\n",
      "    loss           : 19710.991762545327\n",
      "    val_loss       : 19610.478863387616\n",
      "Train Epoch: 78 [192/225000 (0%)] Loss: 19884.324219\n",
      "Train Epoch: 78 [2688/225000 (1%)] Loss: 19415.125000\n",
      "Train Epoch: 78 [5184/225000 (2%)] Loss: 19860.849609\n",
      "Train Epoch: 78 [7680/225000 (3%)] Loss: 19441.640625\n",
      "Train Epoch: 78 [10176/225000 (5%)] Loss: 20024.339844\n",
      "Train Epoch: 78 [12672/225000 (6%)] Loss: 20016.191406\n",
      "Train Epoch: 78 [15168/225000 (7%)] Loss: 19886.417969\n",
      "Train Epoch: 78 [17664/225000 (8%)] Loss: 19278.593750\n",
      "Train Epoch: 78 [20160/225000 (9%)] Loss: 19833.937500\n",
      "Train Epoch: 78 [22656/225000 (10%)] Loss: 19730.964844\n",
      "Train Epoch: 78 [25152/225000 (11%)] Loss: 19219.701172\n",
      "Train Epoch: 78 [27648/225000 (12%)] Loss: 20330.718750\n",
      "Train Epoch: 78 [30144/225000 (13%)] Loss: 19564.296875\n",
      "Train Epoch: 78 [32640/225000 (15%)] Loss: 19480.726562\n",
      "Train Epoch: 78 [35136/225000 (16%)] Loss: 19291.679688\n",
      "Train Epoch: 78 [37632/225000 (17%)] Loss: 19778.359375\n",
      "Train Epoch: 78 [40128/225000 (18%)] Loss: 19742.689453\n",
      "Train Epoch: 78 [42624/225000 (19%)] Loss: 19803.808594\n",
      "Train Epoch: 78 [45120/225000 (20%)] Loss: 19926.734375\n",
      "Train Epoch: 78 [47616/225000 (21%)] Loss: 19777.572266\n",
      "Train Epoch: 78 [50112/225000 (22%)] Loss: 19725.652344\n",
      "Train Epoch: 78 [52608/225000 (23%)] Loss: 19850.511719\n",
      "Train Epoch: 78 [55104/225000 (24%)] Loss: 19466.119141\n",
      "Train Epoch: 78 [57600/225000 (26%)] Loss: 19765.078125\n",
      "Train Epoch: 78 [60096/225000 (27%)] Loss: 19818.613281\n",
      "Train Epoch: 78 [62592/225000 (28%)] Loss: 19771.605469\n",
      "Train Epoch: 78 [65088/225000 (29%)] Loss: 19431.152344\n",
      "Train Epoch: 78 [67584/225000 (30%)] Loss: 19745.339844\n",
      "Train Epoch: 78 [70080/225000 (31%)] Loss: 19759.480469\n",
      "Train Epoch: 78 [72576/225000 (32%)] Loss: 19594.726562\n",
      "Train Epoch: 78 [75072/225000 (33%)] Loss: 20358.503906\n",
      "Train Epoch: 78 [77568/225000 (34%)] Loss: 19969.419922\n",
      "Train Epoch: 78 [80064/225000 (36%)] Loss: 19541.037109\n",
      "Train Epoch: 78 [82560/225000 (37%)] Loss: 19725.054688\n",
      "Train Epoch: 78 [85056/225000 (38%)] Loss: 19395.693359\n",
      "Train Epoch: 78 [87552/225000 (39%)] Loss: 19644.046875\n",
      "Train Epoch: 78 [90048/225000 (40%)] Loss: 20052.130859\n",
      "Train Epoch: 78 [92544/225000 (41%)] Loss: 19960.382812\n",
      "Train Epoch: 78 [95040/225000 (42%)] Loss: 19701.119141\n",
      "Train Epoch: 78 [97536/225000 (43%)] Loss: 19432.902344\n",
      "Train Epoch: 78 [100032/225000 (44%)] Loss: 19951.265625\n",
      "Train Epoch: 78 [102528/225000 (46%)] Loss: 19493.453125\n",
      "Train Epoch: 78 [105024/225000 (47%)] Loss: 19956.894531\n",
      "Train Epoch: 78 [107520/225000 (48%)] Loss: 19574.925781\n",
      "Train Epoch: 78 [110016/225000 (49%)] Loss: 19322.011719\n",
      "Train Epoch: 78 [112512/225000 (50%)] Loss: 19978.001953\n",
      "Train Epoch: 78 [115008/225000 (51%)] Loss: 19427.343750\n",
      "Train Epoch: 78 [117504/225000 (52%)] Loss: 19366.207031\n",
      "Train Epoch: 78 [120000/225000 (53%)] Loss: 19643.763672\n",
      "Train Epoch: 78 [122496/225000 (54%)] Loss: 20256.042969\n",
      "Train Epoch: 78 [124992/225000 (56%)] Loss: 19876.697266\n",
      "Train Epoch: 78 [127488/225000 (57%)] Loss: 19400.654297\n",
      "Train Epoch: 78 [129984/225000 (58%)] Loss: 20066.593750\n",
      "Train Epoch: 78 [132480/225000 (59%)] Loss: 19638.093750\n",
      "Train Epoch: 78 [134976/225000 (60%)] Loss: 20075.718750\n",
      "Train Epoch: 78 [137472/225000 (61%)] Loss: 19573.439453\n",
      "Train Epoch: 78 [139968/225000 (62%)] Loss: 20098.685547\n",
      "Train Epoch: 78 [142464/225000 (63%)] Loss: 19497.832031\n",
      "Train Epoch: 78 [144960/225000 (64%)] Loss: 19505.578125\n",
      "Train Epoch: 78 [147456/225000 (66%)] Loss: 19718.308594\n",
      "Train Epoch: 78 [149952/225000 (67%)] Loss: 19845.281250\n",
      "Train Epoch: 78 [152448/225000 (68%)] Loss: 19232.109375\n",
      "Train Epoch: 78 [154944/225000 (69%)] Loss: 20027.955078\n",
      "Train Epoch: 78 [157440/225000 (70%)] Loss: 19210.089844\n",
      "Train Epoch: 78 [159936/225000 (71%)] Loss: 19844.000000\n",
      "Train Epoch: 78 [162432/225000 (72%)] Loss: 19804.210938\n",
      "Train Epoch: 78 [164928/225000 (73%)] Loss: 20107.312500\n",
      "Train Epoch: 78 [167424/225000 (74%)] Loss: 19450.257812\n",
      "Train Epoch: 78 [169920/225000 (76%)] Loss: 19681.726562\n",
      "Train Epoch: 78 [172416/225000 (77%)] Loss: 19455.230469\n",
      "Train Epoch: 78 [174912/225000 (78%)] Loss: 19967.853516\n",
      "Train Epoch: 78 [177408/225000 (79%)] Loss: 19322.121094\n",
      "Train Epoch: 78 [179904/225000 (80%)] Loss: 19986.972656\n",
      "Train Epoch: 78 [182400/225000 (81%)] Loss: 19369.537109\n",
      "Train Epoch: 78 [184896/225000 (82%)] Loss: 19537.173828\n",
      "Train Epoch: 78 [187392/225000 (83%)] Loss: 19527.273438\n",
      "Train Epoch: 78 [189888/225000 (84%)] Loss: 19544.865234\n",
      "Train Epoch: 78 [192384/225000 (86%)] Loss: 19147.519531\n",
      "Train Epoch: 78 [194880/225000 (87%)] Loss: 19715.439453\n",
      "Train Epoch: 78 [197376/225000 (88%)] Loss: 19417.519531\n",
      "Train Epoch: 78 [199872/225000 (89%)] Loss: 19505.892578\n",
      "Train Epoch: 78 [202368/225000 (90%)] Loss: 19544.224609\n",
      "Train Epoch: 78 [204864/225000 (91%)] Loss: 19871.904297\n",
      "Train Epoch: 78 [207360/225000 (92%)] Loss: 19300.085938\n",
      "Train Epoch: 78 [209856/225000 (93%)] Loss: 19178.835938\n",
      "Train Epoch: 78 [212352/225000 (94%)] Loss: 19059.367188\n",
      "Train Epoch: 78 [214848/225000 (95%)] Loss: 19409.699219\n",
      "Train Epoch: 78 [217344/225000 (97%)] Loss: 19322.775391\n",
      "Train Epoch: 78 [219840/225000 (98%)] Loss: 19662.992188\n",
      "Train Epoch: 78 [222336/225000 (99%)] Loss: 20041.933594\n",
      "Train Epoch: 78 [224832/225000 (100%)] Loss: 19630.269531\n",
      "    epoch          : 78\n",
      "    loss           : 19707.813761532103\n",
      "    val_loss       : 19603.871453893094\n",
      "Train Epoch: 79 [192/225000 (0%)] Loss: 20087.480469\n",
      "Train Epoch: 79 [2688/225000 (1%)] Loss: 19670.730469\n",
      "Train Epoch: 79 [5184/225000 (2%)] Loss: 19718.656250\n",
      "Train Epoch: 79 [7680/225000 (3%)] Loss: 19738.308594\n",
      "Train Epoch: 79 [10176/225000 (5%)] Loss: 19761.144531\n",
      "Train Epoch: 79 [12672/225000 (6%)] Loss: 19833.773438\n",
      "Train Epoch: 79 [15168/225000 (7%)] Loss: 19493.832031\n",
      "Train Epoch: 79 [17664/225000 (8%)] Loss: 19854.556641\n",
      "Train Epoch: 79 [20160/225000 (9%)] Loss: 19596.078125\n",
      "Train Epoch: 79 [22656/225000 (10%)] Loss: 19443.259766\n",
      "Train Epoch: 79 [25152/225000 (11%)] Loss: 19673.781250\n",
      "Train Epoch: 79 [27648/225000 (12%)] Loss: 19859.300781\n",
      "Train Epoch: 79 [30144/225000 (13%)] Loss: 19527.755859\n",
      "Train Epoch: 79 [32640/225000 (15%)] Loss: 20026.113281\n",
      "Train Epoch: 79 [35136/225000 (16%)] Loss: 19729.929688\n",
      "Train Epoch: 79 [37632/225000 (17%)] Loss: 19788.281250\n",
      "Train Epoch: 79 [40128/225000 (18%)] Loss: 19526.824219\n",
      "Train Epoch: 79 [42624/225000 (19%)] Loss: 19375.851562\n",
      "Train Epoch: 79 [45120/225000 (20%)] Loss: 19417.181641\n",
      "Train Epoch: 79 [47616/225000 (21%)] Loss: 19536.851562\n",
      "Train Epoch: 79 [50112/225000 (22%)] Loss: 19727.296875\n",
      "Train Epoch: 79 [52608/225000 (23%)] Loss: 19823.171875\n",
      "Train Epoch: 79 [55104/225000 (24%)] Loss: 19440.433594\n",
      "Train Epoch: 79 [57600/225000 (26%)] Loss: 19560.835938\n",
      "Train Epoch: 79 [60096/225000 (27%)] Loss: 19879.687500\n",
      "Train Epoch: 79 [62592/225000 (28%)] Loss: 19693.242188\n",
      "Train Epoch: 79 [65088/225000 (29%)] Loss: 20137.320312\n",
      "Train Epoch: 79 [67584/225000 (30%)] Loss: 19764.343750\n",
      "Train Epoch: 79 [70080/225000 (31%)] Loss: 19495.449219\n",
      "Train Epoch: 79 [72576/225000 (32%)] Loss: 19472.527344\n",
      "Train Epoch: 79 [75072/225000 (33%)] Loss: 19514.638672\n",
      "Train Epoch: 79 [77568/225000 (34%)] Loss: 19523.195312\n",
      "Train Epoch: 79 [80064/225000 (36%)] Loss: 19469.289062\n",
      "Train Epoch: 79 [82560/225000 (37%)] Loss: 19934.406250\n",
      "Train Epoch: 79 [85056/225000 (38%)] Loss: 19499.140625\n",
      "Train Epoch: 79 [87552/225000 (39%)] Loss: 19167.285156\n",
      "Train Epoch: 79 [90048/225000 (40%)] Loss: 20035.156250\n",
      "Train Epoch: 79 [92544/225000 (41%)] Loss: 19867.820312\n",
      "Train Epoch: 79 [95040/225000 (42%)] Loss: 19802.025391\n",
      "Train Epoch: 79 [97536/225000 (43%)] Loss: 19394.781250\n",
      "Train Epoch: 79 [100032/225000 (44%)] Loss: 19669.095703\n",
      "Train Epoch: 79 [102528/225000 (46%)] Loss: 20102.732422\n",
      "Train Epoch: 79 [105024/225000 (47%)] Loss: 19755.351562\n",
      "Train Epoch: 79 [107520/225000 (48%)] Loss: 19984.363281\n",
      "Train Epoch: 79 [110016/225000 (49%)] Loss: 19490.886719\n",
      "Train Epoch: 79 [112512/225000 (50%)] Loss: 19313.625000\n",
      "Train Epoch: 79 [115008/225000 (51%)] Loss: 19917.289062\n",
      "Train Epoch: 79 [117504/225000 (52%)] Loss: 20135.017578\n",
      "Train Epoch: 79 [120000/225000 (53%)] Loss: 19863.542969\n",
      "Train Epoch: 79 [122496/225000 (54%)] Loss: 19408.654297\n",
      "Train Epoch: 79 [124992/225000 (56%)] Loss: 19626.250000\n",
      "Train Epoch: 79 [127488/225000 (57%)] Loss: 19068.937500\n",
      "Train Epoch: 79 [129984/225000 (58%)] Loss: 19725.160156\n",
      "Train Epoch: 79 [132480/225000 (59%)] Loss: 19981.976562\n",
      "Train Epoch: 79 [134976/225000 (60%)] Loss: 19797.416016\n",
      "Train Epoch: 79 [137472/225000 (61%)] Loss: 19371.958984\n",
      "Train Epoch: 79 [139968/225000 (62%)] Loss: 19564.894531\n",
      "Train Epoch: 79 [142464/225000 (63%)] Loss: 19650.230469\n",
      "Train Epoch: 79 [144960/225000 (64%)] Loss: 19826.564453\n",
      "Train Epoch: 79 [147456/225000 (66%)] Loss: 19592.722656\n",
      "Train Epoch: 79 [149952/225000 (67%)] Loss: 19886.564453\n",
      "Train Epoch: 79 [152448/225000 (68%)] Loss: 19485.312500\n",
      "Train Epoch: 79 [154944/225000 (69%)] Loss: 20154.976562\n",
      "Train Epoch: 79 [157440/225000 (70%)] Loss: 19319.675781\n",
      "Train Epoch: 79 [159936/225000 (71%)] Loss: 19618.531250\n",
      "Train Epoch: 79 [162432/225000 (72%)] Loss: 19573.880859\n",
      "Train Epoch: 79 [164928/225000 (73%)] Loss: 20013.902344\n",
      "Train Epoch: 79 [167424/225000 (74%)] Loss: 19790.138672\n",
      "Train Epoch: 79 [169920/225000 (76%)] Loss: 19451.462891\n",
      "Train Epoch: 79 [172416/225000 (77%)] Loss: 19947.515625\n",
      "Train Epoch: 79 [174912/225000 (78%)] Loss: 19256.390625\n",
      "Train Epoch: 79 [177408/225000 (79%)] Loss: 19494.503906\n",
      "Train Epoch: 79 [179904/225000 (80%)] Loss: 19489.810547\n",
      "Train Epoch: 79 [182400/225000 (81%)] Loss: 19670.246094\n",
      "Train Epoch: 79 [184896/225000 (82%)] Loss: 19511.867188\n",
      "Train Epoch: 79 [187392/225000 (83%)] Loss: 19342.023438\n",
      "Train Epoch: 79 [189888/225000 (84%)] Loss: 19730.964844\n",
      "Train Epoch: 79 [192384/225000 (86%)] Loss: 19527.375000\n",
      "Train Epoch: 79 [194880/225000 (87%)] Loss: 19977.511719\n",
      "Train Epoch: 79 [197376/225000 (88%)] Loss: 19722.894531\n",
      "Train Epoch: 79 [199872/225000 (89%)] Loss: 19538.140625\n",
      "Train Epoch: 79 [202368/225000 (90%)] Loss: 19734.152344\n",
      "Train Epoch: 79 [204864/225000 (91%)] Loss: 19713.382812\n",
      "Train Epoch: 79 [207360/225000 (92%)] Loss: 19621.648438\n",
      "Train Epoch: 79 [209856/225000 (93%)] Loss: 20158.041016\n",
      "Train Epoch: 79 [212352/225000 (94%)] Loss: 19611.988281\n",
      "Train Epoch: 79 [214848/225000 (95%)] Loss: 19719.410156\n",
      "Train Epoch: 79 [217344/225000 (97%)] Loss: 19538.363281\n",
      "Train Epoch: 79 [219840/225000 (98%)] Loss: 19524.109375\n",
      "Train Epoch: 79 [222336/225000 (99%)] Loss: 19217.187500\n",
      "Train Epoch: 79 [224832/225000 (100%)] Loss: 19244.644531\n",
      "    epoch          : 79\n",
      "    loss           : 19702.898152530397\n",
      "    val_loss       : 19592.182392959377\n",
      "Train Epoch: 80 [192/225000 (0%)] Loss: 19904.503906\n",
      "Train Epoch: 80 [2688/225000 (1%)] Loss: 19779.847656\n",
      "Train Epoch: 80 [5184/225000 (2%)] Loss: 19787.265625\n",
      "Train Epoch: 80 [7680/225000 (3%)] Loss: 20036.343750\n",
      "Train Epoch: 80 [10176/225000 (5%)] Loss: 19817.578125\n",
      "Train Epoch: 80 [12672/225000 (6%)] Loss: 19720.982422\n",
      "Train Epoch: 80 [15168/225000 (7%)] Loss: 19719.246094\n",
      "Train Epoch: 80 [17664/225000 (8%)] Loss: 19542.398438\n",
      "Train Epoch: 80 [20160/225000 (9%)] Loss: 19397.328125\n",
      "Train Epoch: 80 [22656/225000 (10%)] Loss: 19794.234375\n",
      "Train Epoch: 80 [25152/225000 (11%)] Loss: 19659.033203\n",
      "Train Epoch: 80 [27648/225000 (12%)] Loss: 19554.285156\n",
      "Train Epoch: 80 [30144/225000 (13%)] Loss: 19803.812500\n",
      "Train Epoch: 80 [32640/225000 (15%)] Loss: 19524.439453\n",
      "Train Epoch: 80 [35136/225000 (16%)] Loss: 19833.181641\n",
      "Train Epoch: 80 [37632/225000 (17%)] Loss: 19197.083984\n",
      "Train Epoch: 80 [40128/225000 (18%)] Loss: 19695.863281\n",
      "Train Epoch: 80 [42624/225000 (19%)] Loss: 19423.945312\n",
      "Train Epoch: 80 [45120/225000 (20%)] Loss: 19398.343750\n",
      "Train Epoch: 80 [47616/225000 (21%)] Loss: 19861.419922\n",
      "Train Epoch: 80 [50112/225000 (22%)] Loss: 19646.820312\n",
      "Train Epoch: 80 [52608/225000 (23%)] Loss: 19544.591797\n",
      "Train Epoch: 80 [55104/225000 (24%)] Loss: 19893.378906\n",
      "Train Epoch: 80 [57600/225000 (26%)] Loss: 19812.355469\n",
      "Train Epoch: 80 [60096/225000 (27%)] Loss: 19759.839844\n",
      "Train Epoch: 80 [62592/225000 (28%)] Loss: 19576.916016\n",
      "Train Epoch: 80 [65088/225000 (29%)] Loss: 19744.746094\n",
      "Train Epoch: 80 [67584/225000 (30%)] Loss: 19569.855469\n",
      "Train Epoch: 80 [70080/225000 (31%)] Loss: 19897.660156\n",
      "Train Epoch: 80 [72576/225000 (32%)] Loss: 19953.513672\n",
      "Train Epoch: 80 [75072/225000 (33%)] Loss: 19122.789062\n",
      "Train Epoch: 80 [77568/225000 (34%)] Loss: 19496.740234\n",
      "Train Epoch: 80 [80064/225000 (36%)] Loss: 19643.779297\n",
      "Train Epoch: 80 [82560/225000 (37%)] Loss: 19618.535156\n",
      "Train Epoch: 80 [85056/225000 (38%)] Loss: 19506.902344\n",
      "Train Epoch: 80 [87552/225000 (39%)] Loss: 20203.175781\n",
      "Train Epoch: 80 [90048/225000 (40%)] Loss: 19248.031250\n",
      "Train Epoch: 80 [92544/225000 (41%)] Loss: 19513.507812\n",
      "Train Epoch: 80 [95040/225000 (42%)] Loss: 19602.128906\n",
      "Train Epoch: 80 [97536/225000 (43%)] Loss: 19095.296875\n",
      "Train Epoch: 80 [100032/225000 (44%)] Loss: 19610.582031\n",
      "Train Epoch: 80 [102528/225000 (46%)] Loss: 19272.851562\n",
      "Train Epoch: 80 [105024/225000 (47%)] Loss: 19718.828125\n",
      "Train Epoch: 80 [107520/225000 (48%)] Loss: 19662.402344\n",
      "Train Epoch: 80 [110016/225000 (49%)] Loss: 19881.783203\n",
      "Train Epoch: 80 [112512/225000 (50%)] Loss: 19657.996094\n",
      "Train Epoch: 80 [115008/225000 (51%)] Loss: 19373.421875\n",
      "Train Epoch: 80 [117504/225000 (52%)] Loss: 18886.488281\n",
      "Train Epoch: 80 [120000/225000 (53%)] Loss: 19794.236328\n",
      "Train Epoch: 80 [122496/225000 (54%)] Loss: 19691.998047\n",
      "Train Epoch: 80 [124992/225000 (56%)] Loss: 19285.308594\n",
      "Train Epoch: 80 [127488/225000 (57%)] Loss: 19523.964844\n",
      "Train Epoch: 80 [129984/225000 (58%)] Loss: 19660.703125\n",
      "Train Epoch: 80 [132480/225000 (59%)] Loss: 19359.492188\n",
      "Train Epoch: 80 [134976/225000 (60%)] Loss: 19635.460938\n",
      "Train Epoch: 80 [137472/225000 (61%)] Loss: 19316.126953\n",
      "Train Epoch: 80 [139968/225000 (62%)] Loss: 19500.982422\n",
      "Train Epoch: 80 [142464/225000 (63%)] Loss: 19526.140625\n",
      "Train Epoch: 80 [144960/225000 (64%)] Loss: 19884.984375\n",
      "Train Epoch: 80 [147456/225000 (66%)] Loss: 20169.699219\n",
      "Train Epoch: 80 [149952/225000 (67%)] Loss: 19350.474609\n",
      "Train Epoch: 80 [152448/225000 (68%)] Loss: 19690.783203\n",
      "Train Epoch: 80 [154944/225000 (69%)] Loss: 19755.533203\n",
      "Train Epoch: 80 [157440/225000 (70%)] Loss: 19366.171875\n",
      "Train Epoch: 80 [159936/225000 (71%)] Loss: 19605.669922\n",
      "Train Epoch: 80 [162432/225000 (72%)] Loss: 19560.734375\n",
      "Train Epoch: 80 [164928/225000 (73%)] Loss: 19393.695312\n",
      "Train Epoch: 80 [167424/225000 (74%)] Loss: 19251.933594\n",
      "Train Epoch: 80 [169920/225000 (76%)] Loss: 19923.097656\n",
      "Train Epoch: 80 [172416/225000 (77%)] Loss: 19318.824219\n",
      "Train Epoch: 80 [174912/225000 (78%)] Loss: 19737.320312\n",
      "Train Epoch: 80 [177408/225000 (79%)] Loss: 20075.679688\n",
      "Train Epoch: 80 [179904/225000 (80%)] Loss: 19737.058594\n",
      "Train Epoch: 80 [182400/225000 (81%)] Loss: 19503.238281\n",
      "Train Epoch: 80 [184896/225000 (82%)] Loss: 19472.054688\n",
      "Train Epoch: 80 [187392/225000 (83%)] Loss: 19547.265625\n",
      "Train Epoch: 80 [189888/225000 (84%)] Loss: 19466.160156\n",
      "Train Epoch: 80 [192384/225000 (86%)] Loss: 19365.521484\n",
      "Train Epoch: 80 [194880/225000 (87%)] Loss: 19880.472656\n",
      "Train Epoch: 80 [197376/225000 (88%)] Loss: 19693.078125\n",
      "Train Epoch: 80 [199872/225000 (89%)] Loss: 19009.199219\n",
      "Train Epoch: 80 [202368/225000 (90%)] Loss: 19681.824219\n",
      "Train Epoch: 80 [204864/225000 (91%)] Loss: 20201.750000\n",
      "Train Epoch: 80 [207360/225000 (92%)] Loss: 19684.910156\n",
      "Train Epoch: 80 [209856/225000 (93%)] Loss: 19578.818359\n",
      "Train Epoch: 80 [212352/225000 (94%)] Loss: 18955.724609\n",
      "Train Epoch: 80 [214848/225000 (95%)] Loss: 19458.191406\n",
      "Train Epoch: 80 [217344/225000 (97%)] Loss: 19677.611328\n",
      "Train Epoch: 80 [219840/225000 (98%)] Loss: 19388.251953\n",
      "Train Epoch: 80 [222336/225000 (99%)] Loss: 19431.050781\n",
      "Train Epoch: 80 [224832/225000 (100%)] Loss: 19530.414062\n",
      "    epoch          : 80\n",
      "    loss           : 19694.793463697206\n",
      "    val_loss       : 19582.969044435114\n",
      "Train Epoch: 81 [192/225000 (0%)] Loss: 19008.681641\n",
      "Train Epoch: 81 [2688/225000 (1%)] Loss: 19636.736328\n",
      "Train Epoch: 81 [5184/225000 (2%)] Loss: 20006.800781\n",
      "Train Epoch: 81 [7680/225000 (3%)] Loss: 19615.167969\n",
      "Train Epoch: 81 [10176/225000 (5%)] Loss: 19798.429688\n",
      "Train Epoch: 81 [12672/225000 (6%)] Loss: 19390.000000\n",
      "Train Epoch: 81 [15168/225000 (7%)] Loss: 19461.498047\n",
      "Train Epoch: 81 [17664/225000 (8%)] Loss: 19865.230469\n",
      "Train Epoch: 81 [20160/225000 (9%)] Loss: 19895.234375\n",
      "Train Epoch: 81 [22656/225000 (10%)] Loss: 19559.236328\n",
      "Train Epoch: 81 [25152/225000 (11%)] Loss: 19633.396484\n",
      "Train Epoch: 81 [27648/225000 (12%)] Loss: 19637.787109\n",
      "Train Epoch: 81 [30144/225000 (13%)] Loss: 19365.027344\n",
      "Train Epoch: 81 [32640/225000 (15%)] Loss: 19967.248047\n",
      "Train Epoch: 81 [35136/225000 (16%)] Loss: 19823.972656\n",
      "Train Epoch: 81 [37632/225000 (17%)] Loss: 19760.916016\n",
      "Train Epoch: 81 [40128/225000 (18%)] Loss: 19303.322266\n",
      "Train Epoch: 81 [42624/225000 (19%)] Loss: 19225.078125\n",
      "Train Epoch: 81 [45120/225000 (20%)] Loss: 19777.402344\n",
      "Train Epoch: 81 [47616/225000 (21%)] Loss: 19273.281250\n",
      "Train Epoch: 81 [50112/225000 (22%)] Loss: 19893.492188\n",
      "Train Epoch: 81 [52608/225000 (23%)] Loss: 19953.285156\n",
      "Train Epoch: 81 [55104/225000 (24%)] Loss: 19715.017578\n",
      "Train Epoch: 81 [57600/225000 (26%)] Loss: 19399.275391\n",
      "Train Epoch: 81 [60096/225000 (27%)] Loss: 19293.271484\n",
      "Train Epoch: 81 [62592/225000 (28%)] Loss: 19629.664062\n",
      "Train Epoch: 81 [65088/225000 (29%)] Loss: 19546.888672\n",
      "Train Epoch: 81 [67584/225000 (30%)] Loss: 19385.492188\n",
      "Train Epoch: 81 [70080/225000 (31%)] Loss: 19426.589844\n",
      "Train Epoch: 81 [72576/225000 (32%)] Loss: 19818.716797\n",
      "Train Epoch: 81 [75072/225000 (33%)] Loss: 20020.484375\n",
      "Train Epoch: 81 [77568/225000 (34%)] Loss: 19949.820312\n",
      "Train Epoch: 81 [80064/225000 (36%)] Loss: 19351.583984\n",
      "Train Epoch: 81 [82560/225000 (37%)] Loss: 19452.095703\n",
      "Train Epoch: 81 [85056/225000 (38%)] Loss: 19600.339844\n",
      "Train Epoch: 81 [87552/225000 (39%)] Loss: 19754.966797\n",
      "Train Epoch: 81 [90048/225000 (40%)] Loss: 19012.484375\n",
      "Train Epoch: 81 [92544/225000 (41%)] Loss: 19476.703125\n",
      "Train Epoch: 81 [95040/225000 (42%)] Loss: 19810.117188\n",
      "Train Epoch: 81 [97536/225000 (43%)] Loss: 19933.691406\n",
      "Train Epoch: 81 [100032/225000 (44%)] Loss: 19027.230469\n",
      "Train Epoch: 81 [102528/225000 (46%)] Loss: 19612.992188\n",
      "Train Epoch: 81 [105024/225000 (47%)] Loss: 19036.662109\n",
      "Train Epoch: 81 [107520/225000 (48%)] Loss: 19576.802734\n",
      "Train Epoch: 81 [110016/225000 (49%)] Loss: 19874.394531\n",
      "Train Epoch: 81 [112512/225000 (50%)] Loss: 19569.851562\n",
      "Train Epoch: 81 [115008/225000 (51%)] Loss: 19766.724609\n",
      "Train Epoch: 81 [117504/225000 (52%)] Loss: 19934.873047\n",
      "Train Epoch: 81 [120000/225000 (53%)] Loss: 19771.435547\n",
      "Train Epoch: 81 [122496/225000 (54%)] Loss: 19686.207031\n",
      "Train Epoch: 81 [124992/225000 (56%)] Loss: 19261.515625\n",
      "Train Epoch: 81 [127488/225000 (57%)] Loss: 19371.929688\n",
      "Train Epoch: 81 [129984/225000 (58%)] Loss: 19923.158203\n",
      "Train Epoch: 81 [132480/225000 (59%)] Loss: 20373.794922\n",
      "Train Epoch: 81 [134976/225000 (60%)] Loss: 19708.089844\n",
      "Train Epoch: 81 [137472/225000 (61%)] Loss: 20022.115234\n",
      "Train Epoch: 81 [139968/225000 (62%)] Loss: 19842.611328\n",
      "Train Epoch: 81 [142464/225000 (63%)] Loss: 19617.800781\n",
      "Train Epoch: 81 [144960/225000 (64%)] Loss: 19856.535156\n",
      "Train Epoch: 81 [147456/225000 (66%)] Loss: 19526.322266\n",
      "Train Epoch: 81 [149952/225000 (67%)] Loss: 20246.052734\n",
      "Train Epoch: 81 [152448/225000 (68%)] Loss: 19576.837891\n",
      "Train Epoch: 81 [154944/225000 (69%)] Loss: 19621.269531\n",
      "Train Epoch: 81 [157440/225000 (70%)] Loss: 19870.378906\n",
      "Train Epoch: 81 [159936/225000 (71%)] Loss: 19647.769531\n",
      "Train Epoch: 81 [162432/225000 (72%)] Loss: 19508.242188\n",
      "Train Epoch: 81 [164928/225000 (73%)] Loss: 19413.224609\n",
      "Train Epoch: 81 [167424/225000 (74%)] Loss: 19581.632812\n",
      "Train Epoch: 81 [169920/225000 (76%)] Loss: 19640.601562\n",
      "Train Epoch: 81 [172416/225000 (77%)] Loss: 19152.156250\n",
      "Train Epoch: 81 [174912/225000 (78%)] Loss: 19763.472656\n",
      "Train Epoch: 81 [177408/225000 (79%)] Loss: 19609.042969\n",
      "Train Epoch: 81 [179904/225000 (80%)] Loss: 19461.927734\n",
      "Train Epoch: 81 [182400/225000 (81%)] Loss: 19760.902344\n",
      "Train Epoch: 81 [184896/225000 (82%)] Loss: 19463.535156\n",
      "Train Epoch: 81 [187392/225000 (83%)] Loss: 19214.025391\n",
      "Train Epoch: 81 [189888/225000 (84%)] Loss: 19808.039062\n",
      "Train Epoch: 81 [192384/225000 (86%)] Loss: 19646.183594\n",
      "Train Epoch: 81 [194880/225000 (87%)] Loss: 19753.125000\n",
      "Train Epoch: 81 [197376/225000 (88%)] Loss: 20062.890625\n",
      "Train Epoch: 81 [199872/225000 (89%)] Loss: 19788.093750\n",
      "Train Epoch: 81 [202368/225000 (90%)] Loss: 19214.304688\n",
      "Train Epoch: 81 [204864/225000 (91%)] Loss: 19403.546875\n",
      "Train Epoch: 81 [207360/225000 (92%)] Loss: 19697.912109\n",
      "Train Epoch: 81 [209856/225000 (93%)] Loss: 19723.994141\n",
      "Train Epoch: 81 [212352/225000 (94%)] Loss: 19459.609375\n",
      "Train Epoch: 81 [214848/225000 (95%)] Loss: 19845.628906\n",
      "Train Epoch: 81 [217344/225000 (97%)] Loss: 19418.927734\n",
      "Train Epoch: 81 [219840/225000 (98%)] Loss: 19376.933594\n",
      "Train Epoch: 81 [222336/225000 (99%)] Loss: 19744.732422\n",
      "Train Epoch: 81 [224832/225000 (100%)] Loss: 18891.572266\n",
      "    epoch          : 81\n",
      "    loss           : 19665.750079991467\n",
      "    val_loss       : 19560.757710578786\n",
      "Train Epoch: 82 [192/225000 (0%)] Loss: 19599.642578\n",
      "Train Epoch: 82 [2688/225000 (1%)] Loss: 19583.210938\n",
      "Train Epoch: 82 [5184/225000 (2%)] Loss: 20382.980469\n",
      "Train Epoch: 82 [7680/225000 (3%)] Loss: 19610.417969\n",
      "Train Epoch: 82 [10176/225000 (5%)] Loss: 19544.175781\n",
      "Train Epoch: 82 [12672/225000 (6%)] Loss: 19908.875000\n",
      "Train Epoch: 82 [15168/225000 (7%)] Loss: 19599.730469\n",
      "Train Epoch: 82 [17664/225000 (8%)] Loss: 19512.320312\n",
      "Train Epoch: 82 [20160/225000 (9%)] Loss: 19593.621094\n",
      "Train Epoch: 82 [22656/225000 (10%)] Loss: 19447.167969\n",
      "Train Epoch: 82 [25152/225000 (11%)] Loss: 19853.796875\n",
      "Train Epoch: 82 [27648/225000 (12%)] Loss: 20357.146484\n",
      "Train Epoch: 82 [30144/225000 (13%)] Loss: 19466.001953\n",
      "Train Epoch: 82 [32640/225000 (15%)] Loss: 19713.242188\n",
      "Train Epoch: 82 [35136/225000 (16%)] Loss: 19713.414062\n",
      "Train Epoch: 82 [37632/225000 (17%)] Loss: 19563.429688\n",
      "Train Epoch: 82 [40128/225000 (18%)] Loss: 19746.902344\n",
      "Train Epoch: 82 [42624/225000 (19%)] Loss: 19511.933594\n",
      "Train Epoch: 82 [45120/225000 (20%)] Loss: 19726.144531\n",
      "Train Epoch: 82 [47616/225000 (21%)] Loss: 19503.941406\n",
      "Train Epoch: 82 [50112/225000 (22%)] Loss: 19700.332031\n",
      "Train Epoch: 82 [52608/225000 (23%)] Loss: 19292.410156\n",
      "Train Epoch: 82 [55104/225000 (24%)] Loss: 19623.984375\n",
      "Train Epoch: 82 [57600/225000 (26%)] Loss: 19564.880859\n",
      "Train Epoch: 82 [60096/225000 (27%)] Loss: 20003.369141\n",
      "Train Epoch: 82 [62592/225000 (28%)] Loss: 19482.433594\n",
      "Train Epoch: 82 [65088/225000 (29%)] Loss: 19877.207031\n",
      "Train Epoch: 82 [67584/225000 (30%)] Loss: 19695.917969\n",
      "Train Epoch: 82 [70080/225000 (31%)] Loss: 18918.437500\n",
      "Train Epoch: 82 [72576/225000 (32%)] Loss: 19452.648438\n",
      "Train Epoch: 82 [75072/225000 (33%)] Loss: 19461.816406\n",
      "Train Epoch: 82 [77568/225000 (34%)] Loss: 19060.273438\n",
      "Train Epoch: 82 [80064/225000 (36%)] Loss: 19631.156250\n",
      "Train Epoch: 82 [82560/225000 (37%)] Loss: 19643.373047\n",
      "Train Epoch: 82 [85056/225000 (38%)] Loss: 19468.837891\n",
      "Train Epoch: 82 [87552/225000 (39%)] Loss: 19493.476562\n",
      "Train Epoch: 82 [90048/225000 (40%)] Loss: 19564.164062\n",
      "Train Epoch: 82 [92544/225000 (41%)] Loss: 19622.216797\n",
      "Train Epoch: 82 [95040/225000 (42%)] Loss: 19599.863281\n",
      "Train Epoch: 82 [97536/225000 (43%)] Loss: 19522.814453\n",
      "Train Epoch: 82 [100032/225000 (44%)] Loss: 19694.316406\n",
      "Train Epoch: 82 [102528/225000 (46%)] Loss: 19744.683594\n",
      "Train Epoch: 82 [105024/225000 (47%)] Loss: 19700.265625\n",
      "Train Epoch: 82 [107520/225000 (48%)] Loss: 19711.378906\n",
      "Train Epoch: 82 [110016/225000 (49%)] Loss: 19493.527344\n",
      "Train Epoch: 82 [112512/225000 (50%)] Loss: 19302.875000\n",
      "Train Epoch: 82 [115008/225000 (51%)] Loss: 19844.011719\n",
      "Train Epoch: 82 [117504/225000 (52%)] Loss: 19691.316406\n",
      "Train Epoch: 82 [120000/225000 (53%)] Loss: 19412.007812\n",
      "Train Epoch: 82 [122496/225000 (54%)] Loss: 20019.167969\n",
      "Train Epoch: 82 [124992/225000 (56%)] Loss: 19767.968750\n",
      "Train Epoch: 82 [127488/225000 (57%)] Loss: 19622.494141\n",
      "Train Epoch: 82 [129984/225000 (58%)] Loss: 19851.837891\n",
      "Train Epoch: 82 [132480/225000 (59%)] Loss: 19918.396484\n",
      "Train Epoch: 82 [134976/225000 (60%)] Loss: 19409.058594\n",
      "Train Epoch: 82 [137472/225000 (61%)] Loss: 19779.943359\n",
      "Train Epoch: 82 [139968/225000 (62%)] Loss: 19528.742188\n",
      "Train Epoch: 82 [142464/225000 (63%)] Loss: 19731.117188\n",
      "Train Epoch: 82 [144960/225000 (64%)] Loss: 20128.734375\n",
      "Train Epoch: 82 [147456/225000 (66%)] Loss: 19746.226562\n",
      "Train Epoch: 82 [149952/225000 (67%)] Loss: 19816.828125\n",
      "Train Epoch: 82 [152448/225000 (68%)] Loss: 19366.765625\n",
      "Train Epoch: 82 [154944/225000 (69%)] Loss: 19707.160156\n",
      "Train Epoch: 82 [157440/225000 (70%)] Loss: 19768.529297\n",
      "Train Epoch: 82 [159936/225000 (71%)] Loss: 19389.699219\n",
      "Train Epoch: 82 [162432/225000 (72%)] Loss: 19960.302734\n",
      "Train Epoch: 82 [164928/225000 (73%)] Loss: 20113.578125\n",
      "Train Epoch: 82 [167424/225000 (74%)] Loss: 19894.361328\n",
      "Train Epoch: 82 [169920/225000 (76%)] Loss: 19634.902344\n",
      "Train Epoch: 82 [172416/225000 (77%)] Loss: 19737.066406\n",
      "Train Epoch: 82 [174912/225000 (78%)] Loss: 19537.031250\n",
      "Train Epoch: 82 [177408/225000 (79%)] Loss: 19528.300781\n",
      "Train Epoch: 82 [179904/225000 (80%)] Loss: 19304.513672\n",
      "Train Epoch: 82 [182400/225000 (81%)] Loss: 20015.373047\n",
      "Train Epoch: 82 [184896/225000 (82%)] Loss: 19363.230469\n",
      "Train Epoch: 82 [187392/225000 (83%)] Loss: 19253.562500\n",
      "Train Epoch: 82 [189888/225000 (84%)] Loss: 19506.023438\n",
      "Train Epoch: 82 [192384/225000 (86%)] Loss: 19602.656250\n",
      "Train Epoch: 82 [194880/225000 (87%)] Loss: 19950.750000\n",
      "Train Epoch: 82 [197376/225000 (88%)] Loss: 20262.687500\n",
      "Train Epoch: 82 [199872/225000 (89%)] Loss: 19398.746094\n",
      "Train Epoch: 82 [202368/225000 (90%)] Loss: 19238.033203\n",
      "Train Epoch: 82 [204864/225000 (91%)] Loss: 19777.343750\n",
      "Train Epoch: 82 [207360/225000 (92%)] Loss: 19522.566406\n",
      "Train Epoch: 82 [209856/225000 (93%)] Loss: 19556.257812\n",
      "Train Epoch: 82 [212352/225000 (94%)] Loss: 19624.230469\n",
      "Train Epoch: 82 [214848/225000 (95%)] Loss: 19691.097656\n",
      "Train Epoch: 82 [217344/225000 (97%)] Loss: 19662.644531\n",
      "Train Epoch: 82 [219840/225000 (98%)] Loss: 19536.468750\n",
      "Train Epoch: 82 [222336/225000 (99%)] Loss: 19565.082031\n",
      "Train Epoch: 82 [224832/225000 (100%)] Loss: 19923.839844\n",
      "    epoch          : 82\n",
      "    loss           : 19667.338828858254\n",
      "    val_loss       : 19670.10195211964\n",
      "Train Epoch: 83 [192/225000 (0%)] Loss: 19877.867188\n",
      "Train Epoch: 83 [2688/225000 (1%)] Loss: 19413.466797\n",
      "Train Epoch: 83 [5184/225000 (2%)] Loss: 20110.826172\n",
      "Train Epoch: 83 [7680/225000 (3%)] Loss: 19464.820312\n",
      "Train Epoch: 83 [10176/225000 (5%)] Loss: 19507.339844\n",
      "Train Epoch: 83 [12672/225000 (6%)] Loss: 19602.019531\n",
      "Train Epoch: 83 [15168/225000 (7%)] Loss: 19496.988281\n",
      "Train Epoch: 83 [17664/225000 (8%)] Loss: 19881.722656\n",
      "Train Epoch: 83 [20160/225000 (9%)] Loss: 19799.140625\n",
      "Train Epoch: 83 [22656/225000 (10%)] Loss: 19758.121094\n",
      "Train Epoch: 83 [25152/225000 (11%)] Loss: 19666.992188\n",
      "Train Epoch: 83 [27648/225000 (12%)] Loss: 20272.804688\n",
      "Train Epoch: 83 [30144/225000 (13%)] Loss: 19746.757812\n",
      "Train Epoch: 83 [32640/225000 (15%)] Loss: 20064.765625\n",
      "Train Epoch: 83 [35136/225000 (16%)] Loss: 19849.425781\n",
      "Train Epoch: 83 [37632/225000 (17%)] Loss: 19428.718750\n",
      "Train Epoch: 83 [40128/225000 (18%)] Loss: 19719.988281\n",
      "Train Epoch: 83 [42624/225000 (19%)] Loss: 19110.800781\n",
      "Train Epoch: 83 [45120/225000 (20%)] Loss: 19511.378906\n",
      "Train Epoch: 83 [47616/225000 (21%)] Loss: 19616.425781\n",
      "Train Epoch: 83 [50112/225000 (22%)] Loss: 19430.855469\n",
      "Train Epoch: 83 [52608/225000 (23%)] Loss: 19578.351562\n",
      "Train Epoch: 83 [55104/225000 (24%)] Loss: 19196.582031\n",
      "Train Epoch: 83 [57600/225000 (26%)] Loss: 20051.343750\n",
      "Train Epoch: 83 [60096/225000 (27%)] Loss: 19294.851562\n",
      "Train Epoch: 83 [62592/225000 (28%)] Loss: 20070.960938\n",
      "Train Epoch: 83 [65088/225000 (29%)] Loss: 19969.011719\n",
      "Train Epoch: 83 [67584/225000 (30%)] Loss: 20189.462891\n",
      "Train Epoch: 83 [70080/225000 (31%)] Loss: 19621.378906\n",
      "Train Epoch: 83 [72576/225000 (32%)] Loss: 19658.011719\n",
      "Train Epoch: 83 [75072/225000 (33%)] Loss: 20002.683594\n",
      "Train Epoch: 83 [77568/225000 (34%)] Loss: 19101.933594\n",
      "Train Epoch: 83 [80064/225000 (36%)] Loss: 19572.503906\n",
      "Train Epoch: 83 [82560/225000 (37%)] Loss: 20013.308594\n",
      "Train Epoch: 83 [85056/225000 (38%)] Loss: 19462.585938\n",
      "Train Epoch: 83 [87552/225000 (39%)] Loss: 19821.085938\n",
      "Train Epoch: 83 [90048/225000 (40%)] Loss: 18822.939453\n",
      "Train Epoch: 83 [92544/225000 (41%)] Loss: 19823.332031\n",
      "Train Epoch: 83 [95040/225000 (42%)] Loss: 19549.626953\n",
      "Train Epoch: 83 [97536/225000 (43%)] Loss: 19469.757812\n",
      "Train Epoch: 83 [100032/225000 (44%)] Loss: 19844.265625\n",
      "Train Epoch: 83 [102528/225000 (46%)] Loss: 19549.722656\n",
      "Train Epoch: 83 [105024/225000 (47%)] Loss: 19485.808594\n",
      "Train Epoch: 83 [107520/225000 (48%)] Loss: 19598.218750\n",
      "Train Epoch: 83 [110016/225000 (49%)] Loss: 19439.816406\n",
      "Train Epoch: 83 [112512/225000 (50%)] Loss: 19156.453125\n",
      "Train Epoch: 83 [115008/225000 (51%)] Loss: 19588.601562\n",
      "Train Epoch: 83 [117504/225000 (52%)] Loss: 19787.791016\n",
      "Train Epoch: 83 [120000/225000 (53%)] Loss: 19765.683594\n",
      "Train Epoch: 83 [122496/225000 (54%)] Loss: 19572.925781\n",
      "Train Epoch: 83 [124992/225000 (56%)] Loss: 19659.664062\n",
      "Train Epoch: 83 [127488/225000 (57%)] Loss: 18866.166016\n",
      "Train Epoch: 83 [129984/225000 (58%)] Loss: 19730.402344\n",
      "Train Epoch: 83 [132480/225000 (59%)] Loss: 19853.960938\n",
      "Train Epoch: 83 [134976/225000 (60%)] Loss: 19836.007812\n",
      "Train Epoch: 83 [137472/225000 (61%)] Loss: 19232.238281\n",
      "Train Epoch: 83 [139968/225000 (62%)] Loss: 20035.253906\n",
      "Train Epoch: 83 [142464/225000 (63%)] Loss: 19442.783203\n",
      "Train Epoch: 83 [144960/225000 (64%)] Loss: 19794.355469\n",
      "Train Epoch: 83 [147456/225000 (66%)] Loss: 19870.003906\n",
      "Train Epoch: 83 [149952/225000 (67%)] Loss: 19289.457031\n",
      "Train Epoch: 83 [152448/225000 (68%)] Loss: 19565.230469\n",
      "Train Epoch: 83 [154944/225000 (69%)] Loss: 19741.500000\n",
      "Train Epoch: 83 [157440/225000 (70%)] Loss: 19392.347656\n",
      "Train Epoch: 83 [159936/225000 (71%)] Loss: 19829.410156\n",
      "Train Epoch: 83 [162432/225000 (72%)] Loss: 19862.718750\n",
      "Train Epoch: 83 [164928/225000 (73%)] Loss: 19900.675781\n",
      "Train Epoch: 83 [167424/225000 (74%)] Loss: 19500.425781\n",
      "Train Epoch: 83 [169920/225000 (76%)] Loss: 19497.523438\n",
      "Train Epoch: 83 [172416/225000 (77%)] Loss: 19375.835938\n",
      "Train Epoch: 83 [174912/225000 (78%)] Loss: 19908.359375\n",
      "Train Epoch: 83 [177408/225000 (79%)] Loss: 19798.255859\n",
      "Train Epoch: 83 [179904/225000 (80%)] Loss: 19669.339844\n",
      "Train Epoch: 83 [182400/225000 (81%)] Loss: 19916.441406\n",
      "Train Epoch: 83 [184896/225000 (82%)] Loss: 19619.578125\n",
      "Train Epoch: 83 [187392/225000 (83%)] Loss: 19501.417969\n",
      "Train Epoch: 83 [189888/225000 (84%)] Loss: 19856.824219\n",
      "Train Epoch: 83 [192384/225000 (86%)] Loss: 19563.902344\n",
      "Train Epoch: 83 [194880/225000 (87%)] Loss: 19781.167969\n",
      "Train Epoch: 83 [197376/225000 (88%)] Loss: 19910.535156\n",
      "Train Epoch: 83 [199872/225000 (89%)] Loss: 19244.652344\n",
      "Train Epoch: 83 [202368/225000 (90%)] Loss: 19311.101562\n",
      "Train Epoch: 83 [204864/225000 (91%)] Loss: 19541.843750\n",
      "Train Epoch: 83 [207360/225000 (92%)] Loss: 19314.162109\n",
      "Train Epoch: 83 [209856/225000 (93%)] Loss: 19837.847656\n",
      "Train Epoch: 83 [212352/225000 (94%)] Loss: 19722.250000\n",
      "Train Epoch: 83 [214848/225000 (95%)] Loss: 19443.169922\n",
      "Train Epoch: 83 [217344/225000 (97%)] Loss: 19983.425781\n",
      "Train Epoch: 83 [219840/225000 (98%)] Loss: 20238.478516\n",
      "Train Epoch: 83 [222336/225000 (99%)] Loss: 19442.367188\n",
      "Train Epoch: 83 [224832/225000 (100%)] Loss: 19616.839844\n",
      "    epoch          : 83\n",
      "    loss           : 19651.666927194434\n",
      "    val_loss       : 19544.809414092822\n",
      "Train Epoch: 84 [192/225000 (0%)] Loss: 19403.521484\n",
      "Train Epoch: 84 [2688/225000 (1%)] Loss: 19607.675781\n",
      "Train Epoch: 84 [5184/225000 (2%)] Loss: 19288.324219\n",
      "Train Epoch: 84 [7680/225000 (3%)] Loss: 19480.212891\n",
      "Train Epoch: 84 [10176/225000 (5%)] Loss: 19695.523438\n",
      "Train Epoch: 84 [12672/225000 (6%)] Loss: 19137.585938\n",
      "Train Epoch: 84 [15168/225000 (7%)] Loss: 19583.332031\n",
      "Train Epoch: 84 [17664/225000 (8%)] Loss: 19720.062500\n",
      "Train Epoch: 84 [20160/225000 (9%)] Loss: 19741.777344\n",
      "Train Epoch: 84 [22656/225000 (10%)] Loss: 19253.781250\n",
      "Train Epoch: 84 [25152/225000 (11%)] Loss: 19467.626953\n",
      "Train Epoch: 84 [27648/225000 (12%)] Loss: 19941.406250\n",
      "Train Epoch: 84 [30144/225000 (13%)] Loss: 19586.789062\n",
      "Train Epoch: 84 [32640/225000 (15%)] Loss: 20153.148438\n",
      "Train Epoch: 84 [35136/225000 (16%)] Loss: 19750.134766\n",
      "Train Epoch: 84 [37632/225000 (17%)] Loss: 19185.312500\n",
      "Train Epoch: 84 [40128/225000 (18%)] Loss: 19824.832031\n",
      "Train Epoch: 84 [42624/225000 (19%)] Loss: 20025.000000\n",
      "Train Epoch: 84 [45120/225000 (20%)] Loss: 19524.966797\n",
      "Train Epoch: 84 [47616/225000 (21%)] Loss: 19453.769531\n",
      "Train Epoch: 84 [50112/225000 (22%)] Loss: 19965.277344\n",
      "Train Epoch: 84 [52608/225000 (23%)] Loss: 19383.173828\n",
      "Train Epoch: 84 [55104/225000 (24%)] Loss: 19725.488281\n",
      "Train Epoch: 84 [57600/225000 (26%)] Loss: 19417.626953\n",
      "Train Epoch: 84 [60096/225000 (27%)] Loss: 19772.585938\n",
      "Train Epoch: 84 [62592/225000 (28%)] Loss: 20176.859375\n",
      "Train Epoch: 84 [65088/225000 (29%)] Loss: 19748.111328\n",
      "Train Epoch: 84 [67584/225000 (30%)] Loss: 20080.578125\n",
      "Train Epoch: 84 [70080/225000 (31%)] Loss: 19032.726562\n",
      "Train Epoch: 84 [72576/225000 (32%)] Loss: 19545.621094\n",
      "Train Epoch: 84 [75072/225000 (33%)] Loss: 19911.687500\n",
      "Train Epoch: 84 [77568/225000 (34%)] Loss: 19733.033203\n",
      "Train Epoch: 84 [80064/225000 (36%)] Loss: 18958.839844\n",
      "Train Epoch: 84 [82560/225000 (37%)] Loss: 19748.921875\n",
      "Train Epoch: 84 [85056/225000 (38%)] Loss: 19667.429688\n",
      "Train Epoch: 84 [87552/225000 (39%)] Loss: 19772.277344\n",
      "Train Epoch: 84 [90048/225000 (40%)] Loss: 20327.683594\n",
      "Train Epoch: 84 [92544/225000 (41%)] Loss: 19807.406250\n",
      "Train Epoch: 84 [95040/225000 (42%)] Loss: 19810.828125\n",
      "Train Epoch: 84 [97536/225000 (43%)] Loss: 19053.001953\n",
      "Train Epoch: 84 [100032/225000 (44%)] Loss: 19811.830078\n",
      "Train Epoch: 84 [102528/225000 (46%)] Loss: 19553.242188\n",
      "Train Epoch: 84 [105024/225000 (47%)] Loss: 19751.636719\n",
      "Train Epoch: 84 [107520/225000 (48%)] Loss: 19297.259766\n",
      "Train Epoch: 84 [110016/225000 (49%)] Loss: 19978.193359\n",
      "Train Epoch: 84 [112512/225000 (50%)] Loss: 19512.208984\n",
      "Train Epoch: 84 [115008/225000 (51%)] Loss: 19529.039062\n",
      "Train Epoch: 84 [117504/225000 (52%)] Loss: 19859.396484\n",
      "Train Epoch: 84 [120000/225000 (53%)] Loss: 19725.449219\n",
      "Train Epoch: 84 [122496/225000 (54%)] Loss: 19388.308594\n",
      "Train Epoch: 84 [124992/225000 (56%)] Loss: 19626.867188\n",
      "Train Epoch: 84 [127488/225000 (57%)] Loss: 19407.191406\n",
      "Train Epoch: 84 [129984/225000 (58%)] Loss: 19643.642578\n",
      "Train Epoch: 84 [132480/225000 (59%)] Loss: 19520.921875\n",
      "Train Epoch: 84 [134976/225000 (60%)] Loss: 19734.621094\n",
      "Train Epoch: 84 [137472/225000 (61%)] Loss: 19408.140625\n",
      "Train Epoch: 84 [139968/225000 (62%)] Loss: 19172.861328\n",
      "Train Epoch: 84 [142464/225000 (63%)] Loss: 19559.162109\n",
      "Train Epoch: 84 [144960/225000 (64%)] Loss: 19720.445312\n",
      "Train Epoch: 84 [147456/225000 (66%)] Loss: 19901.494141\n",
      "Train Epoch: 84 [149952/225000 (67%)] Loss: 19059.921875\n",
      "Train Epoch: 84 [152448/225000 (68%)] Loss: 19344.953125\n",
      "Train Epoch: 84 [154944/225000 (69%)] Loss: 19946.445312\n",
      "Train Epoch: 84 [157440/225000 (70%)] Loss: 19390.992188\n",
      "Train Epoch: 84 [159936/225000 (71%)] Loss: 19499.531250\n",
      "Train Epoch: 84 [162432/225000 (72%)] Loss: 19794.996094\n",
      "Train Epoch: 84 [164928/225000 (73%)] Loss: 19511.062500\n",
      "Train Epoch: 84 [167424/225000 (74%)] Loss: 20087.785156\n",
      "Train Epoch: 84 [169920/225000 (76%)] Loss: 19522.041016\n",
      "Train Epoch: 84 [172416/225000 (77%)] Loss: 19783.628906\n",
      "Train Epoch: 84 [174912/225000 (78%)] Loss: 20135.248047\n",
      "Train Epoch: 84 [177408/225000 (79%)] Loss: 19066.666016\n",
      "Train Epoch: 84 [179904/225000 (80%)] Loss: 20111.343750\n",
      "Train Epoch: 84 [182400/225000 (81%)] Loss: 19761.574219\n",
      "Train Epoch: 84 [184896/225000 (82%)] Loss: 19741.722656\n",
      "Train Epoch: 84 [187392/225000 (83%)] Loss: 19376.648438\n",
      "Train Epoch: 84 [189888/225000 (84%)] Loss: 19199.132812\n",
      "Train Epoch: 84 [192384/225000 (86%)] Loss: 19164.876953\n",
      "Train Epoch: 84 [194880/225000 (87%)] Loss: 19233.216797\n",
      "Train Epoch: 84 [197376/225000 (88%)] Loss: 19527.156250\n",
      "Train Epoch: 84 [199872/225000 (89%)] Loss: 19493.521484\n",
      "Train Epoch: 84 [202368/225000 (90%)] Loss: 19634.580078\n",
      "Train Epoch: 84 [204864/225000 (91%)] Loss: 19452.441406\n",
      "Train Epoch: 84 [207360/225000 (92%)] Loss: 19697.056641\n",
      "Train Epoch: 84 [209856/225000 (93%)] Loss: 20035.224609\n",
      "Train Epoch: 84 [212352/225000 (94%)] Loss: 19633.296875\n",
      "Train Epoch: 84 [214848/225000 (95%)] Loss: 20094.707031\n",
      "Train Epoch: 84 [217344/225000 (97%)] Loss: 19834.957031\n",
      "Train Epoch: 84 [219840/225000 (98%)] Loss: 19692.039062\n",
      "Train Epoch: 84 [222336/225000 (99%)] Loss: 19880.925781\n",
      "Train Epoch: 84 [224832/225000 (100%)] Loss: 19822.324219\n",
      "    epoch          : 84\n",
      "    loss           : 19632.04470856442\n",
      "    val_loss       : 19540.946734406567\n",
      "Train Epoch: 85 [192/225000 (0%)] Loss: 19386.960938\n",
      "Train Epoch: 85 [2688/225000 (1%)] Loss: 19677.806641\n",
      "Train Epoch: 85 [5184/225000 (2%)] Loss: 19372.152344\n",
      "Train Epoch: 85 [7680/225000 (3%)] Loss: 19194.117188\n",
      "Train Epoch: 85 [10176/225000 (5%)] Loss: 19407.841797\n",
      "Train Epoch: 85 [12672/225000 (6%)] Loss: 19277.369141\n",
      "Train Epoch: 85 [15168/225000 (7%)] Loss: 19778.486328\n",
      "Train Epoch: 85 [17664/225000 (8%)] Loss: 19770.806641\n",
      "Train Epoch: 85 [20160/225000 (9%)] Loss: 19376.044922\n",
      "Train Epoch: 85 [22656/225000 (10%)] Loss: 19487.269531\n",
      "Train Epoch: 85 [25152/225000 (11%)] Loss: 19446.115234\n",
      "Train Epoch: 85 [27648/225000 (12%)] Loss: 19949.703125\n",
      "Train Epoch: 85 [30144/225000 (13%)] Loss: 19451.796875\n",
      "Train Epoch: 85 [32640/225000 (15%)] Loss: 19499.681641\n",
      "Train Epoch: 85 [35136/225000 (16%)] Loss: 19219.015625\n",
      "Train Epoch: 85 [37632/225000 (17%)] Loss: 19767.378906\n",
      "Train Epoch: 85 [40128/225000 (18%)] Loss: 19410.470703\n",
      "Train Epoch: 85 [42624/225000 (19%)] Loss: 19840.121094\n",
      "Train Epoch: 85 [45120/225000 (20%)] Loss: 19551.164062\n",
      "Train Epoch: 85 [47616/225000 (21%)] Loss: 19444.242188\n",
      "Train Epoch: 85 [50112/225000 (22%)] Loss: 19051.320312\n",
      "Train Epoch: 85 [52608/225000 (23%)] Loss: 19611.207031\n",
      "Train Epoch: 85 [55104/225000 (24%)] Loss: 19581.177734\n",
      "Train Epoch: 85 [57600/225000 (26%)] Loss: 19408.957031\n",
      "Train Epoch: 85 [60096/225000 (27%)] Loss: 19711.742188\n",
      "Train Epoch: 85 [62592/225000 (28%)] Loss: 19259.007812\n",
      "Train Epoch: 85 [65088/225000 (29%)] Loss: 20009.921875\n",
      "Train Epoch: 85 [67584/225000 (30%)] Loss: 19672.476562\n",
      "Train Epoch: 85 [70080/225000 (31%)] Loss: 20251.386719\n",
      "Train Epoch: 85 [72576/225000 (32%)] Loss: 20188.781250\n",
      "Train Epoch: 85 [75072/225000 (33%)] Loss: 19373.882812\n",
      "Train Epoch: 85 [77568/225000 (34%)] Loss: 19913.308594\n",
      "Train Epoch: 85 [80064/225000 (36%)] Loss: 19463.587891\n",
      "Train Epoch: 85 [82560/225000 (37%)] Loss: 20140.992188\n",
      "Train Epoch: 85 [85056/225000 (38%)] Loss: 19803.117188\n",
      "Train Epoch: 85 [87552/225000 (39%)] Loss: 19478.503906\n",
      "Train Epoch: 85 [90048/225000 (40%)] Loss: 19641.080078\n",
      "Train Epoch: 85 [92544/225000 (41%)] Loss: 19288.382812\n",
      "Train Epoch: 85 [95040/225000 (42%)] Loss: 19384.222656\n",
      "Train Epoch: 85 [97536/225000 (43%)] Loss: 19736.582031\n",
      "Train Epoch: 85 [100032/225000 (44%)] Loss: 19473.960938\n",
      "Train Epoch: 85 [102528/225000 (46%)] Loss: 20356.554688\n",
      "Train Epoch: 85 [105024/225000 (47%)] Loss: 19370.205078\n",
      "Train Epoch: 85 [107520/225000 (48%)] Loss: 19760.843750\n",
      "Train Epoch: 85 [110016/225000 (49%)] Loss: 19715.968750\n",
      "Train Epoch: 85 [112512/225000 (50%)] Loss: 20125.478516\n",
      "Train Epoch: 85 [115008/225000 (51%)] Loss: 19569.074219\n",
      "Train Epoch: 85 [117504/225000 (52%)] Loss: 19655.976562\n",
      "Train Epoch: 85 [120000/225000 (53%)] Loss: 19726.343750\n",
      "Train Epoch: 85 [122496/225000 (54%)] Loss: 19430.101562\n",
      "Train Epoch: 85 [124992/225000 (56%)] Loss: 20027.994141\n",
      "Train Epoch: 85 [127488/225000 (57%)] Loss: 20055.949219\n",
      "Train Epoch: 85 [129984/225000 (58%)] Loss: 18770.289062\n",
      "Train Epoch: 85 [132480/225000 (59%)] Loss: 19574.222656\n",
      "Train Epoch: 85 [134976/225000 (60%)] Loss: 20019.076172\n",
      "Train Epoch: 85 [137472/225000 (61%)] Loss: 19485.562500\n",
      "Train Epoch: 85 [139968/225000 (62%)] Loss: 19604.199219\n",
      "Train Epoch: 85 [142464/225000 (63%)] Loss: 19345.050781\n",
      "Train Epoch: 85 [144960/225000 (64%)] Loss: 19514.472656\n",
      "Train Epoch: 85 [147456/225000 (66%)] Loss: 19961.171875\n",
      "Train Epoch: 85 [149952/225000 (67%)] Loss: 19345.105469\n",
      "Train Epoch: 85 [152448/225000 (68%)] Loss: 20279.789062\n",
      "Train Epoch: 85 [154944/225000 (69%)] Loss: 19618.312500\n",
      "Train Epoch: 85 [157440/225000 (70%)] Loss: 19919.496094\n",
      "Train Epoch: 85 [159936/225000 (71%)] Loss: 19522.257812\n",
      "Train Epoch: 85 [162432/225000 (72%)] Loss: 19703.472656\n",
      "Train Epoch: 85 [164928/225000 (73%)] Loss: 19863.425781\n",
      "Train Epoch: 85 [167424/225000 (74%)] Loss: 19978.607422\n",
      "Train Epoch: 85 [169920/225000 (76%)] Loss: 19818.144531\n",
      "Train Epoch: 85 [172416/225000 (77%)] Loss: 19662.378906\n",
      "Train Epoch: 85 [174912/225000 (78%)] Loss: 20414.636719\n",
      "Train Epoch: 85 [177408/225000 (79%)] Loss: 19663.777344\n",
      "Train Epoch: 85 [179904/225000 (80%)] Loss: 19469.664062\n",
      "Train Epoch: 85 [182400/225000 (81%)] Loss: 19987.097656\n",
      "Train Epoch: 85 [184896/225000 (82%)] Loss: 19179.917969\n",
      "Train Epoch: 85 [187392/225000 (83%)] Loss: 19547.144531\n",
      "Train Epoch: 85 [189888/225000 (84%)] Loss: 19347.312500\n",
      "Train Epoch: 85 [192384/225000 (86%)] Loss: 19664.148438\n",
      "Train Epoch: 85 [194880/225000 (87%)] Loss: 19706.408203\n",
      "Train Epoch: 85 [197376/225000 (88%)] Loss: 19341.683594\n",
      "Train Epoch: 85 [199872/225000 (89%)] Loss: 19601.998047\n",
      "Train Epoch: 85 [202368/225000 (90%)] Loss: 19700.964844\n",
      "Train Epoch: 85 [204864/225000 (91%)] Loss: 19504.144531\n",
      "Train Epoch: 85 [207360/225000 (92%)] Loss: 19302.402344\n",
      "Train Epoch: 85 [209856/225000 (93%)] Loss: 19625.781250\n",
      "Train Epoch: 85 [212352/225000 (94%)] Loss: 19866.132812\n",
      "Train Epoch: 85 [214848/225000 (95%)] Loss: 19530.546875\n",
      "Train Epoch: 85 [217344/225000 (97%)] Loss: 19748.095703\n",
      "Train Epoch: 85 [219840/225000 (98%)] Loss: 20104.160156\n",
      "Train Epoch: 85 [222336/225000 (99%)] Loss: 19739.083984\n",
      "Train Epoch: 85 [224832/225000 (100%)] Loss: 19193.517578\n",
      "    epoch          : 85\n",
      "    loss           : 19633.17527463737\n",
      "    val_loss       : 19528.255709533474\n",
      "Train Epoch: 86 [192/225000 (0%)] Loss: 19638.470703\n",
      "Train Epoch: 86 [2688/225000 (1%)] Loss: 19876.089844\n",
      "Train Epoch: 86 [5184/225000 (2%)] Loss: 19271.429688\n",
      "Train Epoch: 86 [7680/225000 (3%)] Loss: 19130.945312\n",
      "Train Epoch: 86 [10176/225000 (5%)] Loss: 19652.921875\n",
      "Train Epoch: 86 [12672/225000 (6%)] Loss: 19534.175781\n",
      "Train Epoch: 86 [15168/225000 (7%)] Loss: 20212.453125\n",
      "Train Epoch: 86 [17664/225000 (8%)] Loss: 19431.242188\n",
      "Train Epoch: 86 [20160/225000 (9%)] Loss: 19339.898438\n",
      "Train Epoch: 86 [22656/225000 (10%)] Loss: 19810.503906\n",
      "Train Epoch: 86 [25152/225000 (11%)] Loss: 19198.835938\n",
      "Train Epoch: 86 [27648/225000 (12%)] Loss: 19708.945312\n",
      "Train Epoch: 86 [30144/225000 (13%)] Loss: 19172.693359\n",
      "Train Epoch: 86 [32640/225000 (15%)] Loss: 19373.316406\n",
      "Train Epoch: 86 [35136/225000 (16%)] Loss: 19644.914062\n",
      "Train Epoch: 86 [37632/225000 (17%)] Loss: 19082.601562\n",
      "Train Epoch: 86 [40128/225000 (18%)] Loss: 20085.726562\n",
      "Train Epoch: 86 [42624/225000 (19%)] Loss: 19558.046875\n",
      "Train Epoch: 86 [45120/225000 (20%)] Loss: 19952.867188\n",
      "Train Epoch: 86 [47616/225000 (21%)] Loss: 19941.746094\n",
      "Train Epoch: 86 [50112/225000 (22%)] Loss: 19962.712891\n",
      "Train Epoch: 86 [52608/225000 (23%)] Loss: 19231.531250\n",
      "Train Epoch: 86 [55104/225000 (24%)] Loss: 19607.515625\n",
      "Train Epoch: 86 [57600/225000 (26%)] Loss: 19794.484375\n",
      "Train Epoch: 86 [60096/225000 (27%)] Loss: 19427.179688\n",
      "Train Epoch: 86 [62592/225000 (28%)] Loss: 19773.421875\n",
      "Train Epoch: 86 [65088/225000 (29%)] Loss: 19706.546875\n",
      "Train Epoch: 86 [67584/225000 (30%)] Loss: 19350.613281\n",
      "Train Epoch: 86 [70080/225000 (31%)] Loss: 19473.367188\n",
      "Train Epoch: 86 [72576/225000 (32%)] Loss: 20079.046875\n",
      "Train Epoch: 86 [75072/225000 (33%)] Loss: 19337.851562\n",
      "Train Epoch: 86 [77568/225000 (34%)] Loss: 19620.863281\n",
      "Train Epoch: 86 [80064/225000 (36%)] Loss: 19133.308594\n",
      "Train Epoch: 86 [82560/225000 (37%)] Loss: 19590.000000\n",
      "Train Epoch: 86 [85056/225000 (38%)] Loss: 19718.664062\n",
      "Train Epoch: 86 [87552/225000 (39%)] Loss: 19151.824219\n",
      "Train Epoch: 86 [90048/225000 (40%)] Loss: 19680.203125\n",
      "Train Epoch: 86 [92544/225000 (41%)] Loss: 19630.226562\n",
      "Train Epoch: 86 [95040/225000 (42%)] Loss: 19330.226562\n",
      "Train Epoch: 86 [97536/225000 (43%)] Loss: 19537.562500\n",
      "Train Epoch: 86 [100032/225000 (44%)] Loss: 19643.960938\n",
      "Train Epoch: 86 [102528/225000 (46%)] Loss: 19480.380859\n",
      "Train Epoch: 86 [105024/225000 (47%)] Loss: 19672.339844\n",
      "Train Epoch: 86 [107520/225000 (48%)] Loss: 19356.984375\n",
      "Train Epoch: 86 [110016/225000 (49%)] Loss: 19411.011719\n",
      "Train Epoch: 86 [112512/225000 (50%)] Loss: 19540.708984\n",
      "Train Epoch: 86 [115008/225000 (51%)] Loss: 19452.166016\n",
      "Train Epoch: 86 [117504/225000 (52%)] Loss: 19870.291016\n",
      "Train Epoch: 86 [120000/225000 (53%)] Loss: 19675.351562\n",
      "Train Epoch: 86 [122496/225000 (54%)] Loss: 19433.384766\n",
      "Train Epoch: 86 [124992/225000 (56%)] Loss: 19346.238281\n",
      "Train Epoch: 86 [127488/225000 (57%)] Loss: 19677.962891\n",
      "Train Epoch: 86 [129984/225000 (58%)] Loss: 19292.878906\n",
      "Train Epoch: 86 [132480/225000 (59%)] Loss: 19602.062500\n",
      "Train Epoch: 86 [134976/225000 (60%)] Loss: 19801.886719\n",
      "Train Epoch: 86 [137472/225000 (61%)] Loss: 20015.570312\n",
      "Train Epoch: 86 [139968/225000 (62%)] Loss: 19573.074219\n",
      "Train Epoch: 86 [142464/225000 (63%)] Loss: 19691.937500\n",
      "Train Epoch: 86 [144960/225000 (64%)] Loss: 19761.816406\n",
      "Train Epoch: 86 [147456/225000 (66%)] Loss: 19287.742188\n",
      "Train Epoch: 86 [149952/225000 (67%)] Loss: 19929.033203\n",
      "Train Epoch: 86 [152448/225000 (68%)] Loss: 19291.347656\n",
      "Train Epoch: 86 [154944/225000 (69%)] Loss: 20195.683594\n",
      "Train Epoch: 86 [157440/225000 (70%)] Loss: 19525.660156\n",
      "Train Epoch: 86 [159936/225000 (71%)] Loss: 19481.921875\n",
      "Train Epoch: 86 [162432/225000 (72%)] Loss: 19456.460938\n",
      "Train Epoch: 86 [164928/225000 (73%)] Loss: 19295.373047\n",
      "Train Epoch: 86 [167424/225000 (74%)] Loss: 19600.722656\n",
      "Train Epoch: 86 [169920/225000 (76%)] Loss: 19548.792969\n",
      "Train Epoch: 86 [172416/225000 (77%)] Loss: 20104.558594\n",
      "Train Epoch: 86 [174912/225000 (78%)] Loss: 19759.363281\n",
      "Train Epoch: 86 [177408/225000 (79%)] Loss: 19765.265625\n",
      "Train Epoch: 86 [179904/225000 (80%)] Loss: 19977.285156\n",
      "Train Epoch: 86 [182400/225000 (81%)] Loss: 19605.617188\n",
      "Train Epoch: 86 [184896/225000 (82%)] Loss: 19187.316406\n",
      "Train Epoch: 86 [187392/225000 (83%)] Loss: 19990.746094\n",
      "Train Epoch: 86 [189888/225000 (84%)] Loss: 19838.873047\n",
      "Train Epoch: 86 [192384/225000 (86%)] Loss: 20057.015625\n",
      "Train Epoch: 86 [194880/225000 (87%)] Loss: 19703.378906\n",
      "Train Epoch: 86 [197376/225000 (88%)] Loss: 19674.312500\n",
      "Train Epoch: 86 [199872/225000 (89%)] Loss: 19316.222656\n",
      "Train Epoch: 86 [202368/225000 (90%)] Loss: 19629.173828\n",
      "Train Epoch: 86 [204864/225000 (91%)] Loss: 19643.511719\n",
      "Train Epoch: 86 [207360/225000 (92%)] Loss: 19819.947266\n",
      "Train Epoch: 86 [209856/225000 (93%)] Loss: 19534.220703\n",
      "Train Epoch: 86 [212352/225000 (94%)] Loss: 19303.609375\n",
      "Train Epoch: 86 [214848/225000 (95%)] Loss: 19508.128906\n",
      "Train Epoch: 86 [217344/225000 (97%)] Loss: 19459.554688\n",
      "Train Epoch: 86 [219840/225000 (98%)] Loss: 19826.871094\n",
      "Train Epoch: 86 [222336/225000 (99%)] Loss: 19293.902344\n",
      "Train Epoch: 86 [224832/225000 (100%)] Loss: 19571.539062\n",
      "    epoch          : 86\n",
      "    loss           : 19613.920755119452\n",
      "    val_loss       : 19517.996763928248\n",
      "Train Epoch: 87 [192/225000 (0%)] Loss: 19408.089844\n",
      "Train Epoch: 87 [2688/225000 (1%)] Loss: 19889.451172\n",
      "Train Epoch: 87 [5184/225000 (2%)] Loss: 19496.527344\n",
      "Train Epoch: 87 [7680/225000 (3%)] Loss: 19224.507812\n",
      "Train Epoch: 87 [10176/225000 (5%)] Loss: 19510.023438\n",
      "Train Epoch: 87 [12672/225000 (6%)] Loss: 19002.517578\n",
      "Train Epoch: 87 [15168/225000 (7%)] Loss: 19078.898438\n",
      "Train Epoch: 87 [17664/225000 (8%)] Loss: 18814.515625\n",
      "Train Epoch: 87 [20160/225000 (9%)] Loss: 19891.556641\n",
      "Train Epoch: 87 [22656/225000 (10%)] Loss: 19896.773438\n",
      "Train Epoch: 87 [25152/225000 (11%)] Loss: 19194.976562\n",
      "Train Epoch: 87 [27648/225000 (12%)] Loss: 18982.292969\n",
      "Train Epoch: 87 [30144/225000 (13%)] Loss: 19348.703125\n",
      "Train Epoch: 87 [32640/225000 (15%)] Loss: 19937.867188\n",
      "Train Epoch: 87 [35136/225000 (16%)] Loss: 19655.468750\n",
      "Train Epoch: 87 [37632/225000 (17%)] Loss: 19626.787109\n",
      "Train Epoch: 87 [40128/225000 (18%)] Loss: 19700.412109\n",
      "Train Epoch: 87 [42624/225000 (19%)] Loss: 19165.632812\n",
      "Train Epoch: 87 [45120/225000 (20%)] Loss: 19583.710938\n",
      "Train Epoch: 87 [47616/225000 (21%)] Loss: 19707.337891\n",
      "Train Epoch: 87 [50112/225000 (22%)] Loss: 19106.453125\n",
      "Train Epoch: 87 [52608/225000 (23%)] Loss: 19555.789062\n",
      "Train Epoch: 87 [55104/225000 (24%)] Loss: 19400.732422\n",
      "Train Epoch: 87 [57600/225000 (26%)] Loss: 19512.746094\n",
      "Train Epoch: 87 [60096/225000 (27%)] Loss: 19737.492188\n",
      "Train Epoch: 87 [62592/225000 (28%)] Loss: 19651.351562\n",
      "Train Epoch: 87 [65088/225000 (29%)] Loss: 19406.550781\n",
      "Train Epoch: 87 [67584/225000 (30%)] Loss: 19943.996094\n",
      "Train Epoch: 87 [70080/225000 (31%)] Loss: 19355.041016\n",
      "Train Epoch: 87 [72576/225000 (32%)] Loss: 19551.085938\n",
      "Train Epoch: 87 [75072/225000 (33%)] Loss: 19779.082031\n",
      "Train Epoch: 87 [77568/225000 (34%)] Loss: 19399.781250\n",
      "Train Epoch: 87 [80064/225000 (36%)] Loss: 19638.962891\n",
      "Train Epoch: 87 [82560/225000 (37%)] Loss: 19718.115234\n",
      "Train Epoch: 87 [85056/225000 (38%)] Loss: 19266.574219\n",
      "Train Epoch: 87 [87552/225000 (39%)] Loss: 20251.382812\n",
      "Train Epoch: 87 [90048/225000 (40%)] Loss: 19570.939453\n",
      "Train Epoch: 87 [92544/225000 (41%)] Loss: 18813.785156\n",
      "Train Epoch: 87 [95040/225000 (42%)] Loss: 19487.906250\n",
      "Train Epoch: 87 [97536/225000 (43%)] Loss: 20149.691406\n",
      "Train Epoch: 87 [100032/225000 (44%)] Loss: 19370.343750\n",
      "Train Epoch: 87 [102528/225000 (46%)] Loss: 19451.308594\n",
      "Train Epoch: 87 [105024/225000 (47%)] Loss: 19217.222656\n",
      "Train Epoch: 87 [107520/225000 (48%)] Loss: 19830.521484\n",
      "Train Epoch: 87 [110016/225000 (49%)] Loss: 19939.328125\n",
      "Train Epoch: 87 [112512/225000 (50%)] Loss: 20137.265625\n",
      "Train Epoch: 87 [115008/225000 (51%)] Loss: 19698.636719\n",
      "Train Epoch: 87 [117504/225000 (52%)] Loss: 19273.460938\n",
      "Train Epoch: 87 [120000/225000 (53%)] Loss: 20178.986328\n",
      "Train Epoch: 87 [122496/225000 (54%)] Loss: 19314.857422\n",
      "Train Epoch: 87 [124992/225000 (56%)] Loss: 19338.621094\n",
      "Train Epoch: 87 [127488/225000 (57%)] Loss: 19500.757812\n",
      "Train Epoch: 87 [129984/225000 (58%)] Loss: 19631.996094\n",
      "Train Epoch: 87 [132480/225000 (59%)] Loss: 19396.037109\n",
      "Train Epoch: 87 [134976/225000 (60%)] Loss: 19469.910156\n",
      "Train Epoch: 87 [137472/225000 (61%)] Loss: 19806.343750\n",
      "Train Epoch: 87 [139968/225000 (62%)] Loss: 19623.230469\n",
      "Train Epoch: 87 [142464/225000 (63%)] Loss: 19602.187500\n",
      "Train Epoch: 87 [144960/225000 (64%)] Loss: 19730.060547\n",
      "Train Epoch: 87 [147456/225000 (66%)] Loss: 19618.867188\n",
      "Train Epoch: 87 [149952/225000 (67%)] Loss: 19696.683594\n",
      "Train Epoch: 87 [152448/225000 (68%)] Loss: 20175.695312\n",
      "Train Epoch: 87 [154944/225000 (69%)] Loss: 19818.828125\n",
      "Train Epoch: 87 [157440/225000 (70%)] Loss: 19656.683594\n",
      "Train Epoch: 87 [159936/225000 (71%)] Loss: 20121.910156\n",
      "Train Epoch: 87 [162432/225000 (72%)] Loss: 19385.359375\n",
      "Train Epoch: 87 [164928/225000 (73%)] Loss: 19480.353516\n",
      "Train Epoch: 87 [167424/225000 (74%)] Loss: 19005.757812\n",
      "Train Epoch: 87 [169920/225000 (76%)] Loss: 19334.429688\n",
      "Train Epoch: 87 [172416/225000 (77%)] Loss: 19814.507812\n",
      "Train Epoch: 87 [174912/225000 (78%)] Loss: 19527.687500\n",
      "Train Epoch: 87 [177408/225000 (79%)] Loss: 19444.621094\n",
      "Train Epoch: 87 [179904/225000 (80%)] Loss: 19149.554688\n",
      "Train Epoch: 87 [182400/225000 (81%)] Loss: 19294.974609\n",
      "Train Epoch: 87 [184896/225000 (82%)] Loss: 19099.253906\n",
      "Train Epoch: 87 [187392/225000 (83%)] Loss: 19407.814453\n",
      "Train Epoch: 87 [189888/225000 (84%)] Loss: 19830.876953\n",
      "Train Epoch: 87 [192384/225000 (86%)] Loss: 19399.925781\n",
      "Train Epoch: 87 [194880/225000 (87%)] Loss: 19336.492188\n",
      "Train Epoch: 87 [197376/225000 (88%)] Loss: 19657.968750\n",
      "Train Epoch: 87 [199872/225000 (89%)] Loss: 19573.277344\n",
      "Train Epoch: 87 [202368/225000 (90%)] Loss: 20253.855469\n",
      "Train Epoch: 87 [204864/225000 (91%)] Loss: 19571.953125\n",
      "Train Epoch: 87 [207360/225000 (92%)] Loss: 19503.789062\n",
      "Train Epoch: 87 [209856/225000 (93%)] Loss: 19743.511719\n",
      "Train Epoch: 87 [212352/225000 (94%)] Loss: 19365.125000\n",
      "Train Epoch: 87 [214848/225000 (95%)] Loss: 19302.324219\n",
      "Train Epoch: 87 [217344/225000 (97%)] Loss: 19616.878906\n",
      "Train Epoch: 87 [219840/225000 (98%)] Loss: 19649.230469\n",
      "Train Epoch: 87 [222336/225000 (99%)] Loss: 19520.800781\n",
      "Train Epoch: 87 [224832/225000 (100%)] Loss: 19017.972656\n",
      "    epoch          : 87\n",
      "    loss           : 19624.15266538236\n",
      "    val_loss       : 19511.055648057514\n",
      "Train Epoch: 88 [192/225000 (0%)] Loss: 19455.080078\n",
      "Train Epoch: 88 [2688/225000 (1%)] Loss: 19860.480469\n",
      "Train Epoch: 88 [5184/225000 (2%)] Loss: 19201.734375\n",
      "Train Epoch: 88 [7680/225000 (3%)] Loss: 19228.464844\n",
      "Train Epoch: 88 [10176/225000 (5%)] Loss: 19892.789062\n",
      "Train Epoch: 88 [12672/225000 (6%)] Loss: 20141.650391\n",
      "Train Epoch: 88 [15168/225000 (7%)] Loss: 19799.718750\n",
      "Train Epoch: 88 [17664/225000 (8%)] Loss: 19836.515625\n",
      "Train Epoch: 88 [20160/225000 (9%)] Loss: 19682.246094\n",
      "Train Epoch: 88 [22656/225000 (10%)] Loss: 19400.269531\n",
      "Train Epoch: 88 [25152/225000 (11%)] Loss: 19323.109375\n",
      "Train Epoch: 88 [27648/225000 (12%)] Loss: 19309.755859\n",
      "Train Epoch: 88 [30144/225000 (13%)] Loss: 19760.136719\n",
      "Train Epoch: 88 [32640/225000 (15%)] Loss: 19939.515625\n",
      "Train Epoch: 88 [35136/225000 (16%)] Loss: 20226.585938\n",
      "Train Epoch: 88 [37632/225000 (17%)] Loss: 19784.742188\n",
      "Train Epoch: 88 [40128/225000 (18%)] Loss: 20196.437500\n",
      "Train Epoch: 88 [42624/225000 (19%)] Loss: 19680.703125\n",
      "Train Epoch: 88 [45120/225000 (20%)] Loss: 19582.777344\n",
      "Train Epoch: 88 [47616/225000 (21%)] Loss: 19361.308594\n",
      "Train Epoch: 88 [50112/225000 (22%)] Loss: 19117.476562\n",
      "Train Epoch: 88 [52608/225000 (23%)] Loss: 19607.865234\n",
      "Train Epoch: 88 [55104/225000 (24%)] Loss: 20053.878906\n",
      "Train Epoch: 88 [57600/225000 (26%)] Loss: 19346.628906\n",
      "Train Epoch: 88 [60096/225000 (27%)] Loss: 19614.191406\n",
      "Train Epoch: 88 [62592/225000 (28%)] Loss: 19551.472656\n",
      "Train Epoch: 88 [65088/225000 (29%)] Loss: 18905.363281\n",
      "Train Epoch: 88 [67584/225000 (30%)] Loss: 19848.802734\n",
      "Train Epoch: 88 [70080/225000 (31%)] Loss: 19816.710938\n",
      "Train Epoch: 88 [72576/225000 (32%)] Loss: 19481.802734\n",
      "Train Epoch: 88 [75072/225000 (33%)] Loss: 19787.410156\n",
      "Train Epoch: 88 [77568/225000 (34%)] Loss: 19576.128906\n",
      "Train Epoch: 88 [80064/225000 (36%)] Loss: 19811.167969\n",
      "Train Epoch: 88 [82560/225000 (37%)] Loss: 19426.156250\n",
      "Train Epoch: 88 [85056/225000 (38%)] Loss: 19329.009766\n",
      "Train Epoch: 88 [87552/225000 (39%)] Loss: 20190.460938\n",
      "Train Epoch: 88 [90048/225000 (40%)] Loss: 19949.625000\n",
      "Train Epoch: 88 [92544/225000 (41%)] Loss: 19410.519531\n",
      "Train Epoch: 88 [95040/225000 (42%)] Loss: 19410.076172\n",
      "Train Epoch: 88 [97536/225000 (43%)] Loss: 19324.121094\n",
      "Train Epoch: 88 [100032/225000 (44%)] Loss: 19682.101562\n",
      "Train Epoch: 88 [102528/225000 (46%)] Loss: 19293.261719\n",
      "Train Epoch: 88 [105024/225000 (47%)] Loss: 19995.234375\n",
      "Train Epoch: 88 [107520/225000 (48%)] Loss: 19990.236328\n",
      "Train Epoch: 88 [110016/225000 (49%)] Loss: 19702.070312\n",
      "Train Epoch: 88 [112512/225000 (50%)] Loss: 19670.273438\n",
      "Train Epoch: 88 [115008/225000 (51%)] Loss: 19776.351562\n",
      "Train Epoch: 88 [117504/225000 (52%)] Loss: 19895.250000\n",
      "Train Epoch: 88 [120000/225000 (53%)] Loss: 19832.169922\n",
      "Train Epoch: 88 [122496/225000 (54%)] Loss: 19554.810547\n",
      "Train Epoch: 88 [124992/225000 (56%)] Loss: 19416.792969\n",
      "Train Epoch: 88 [127488/225000 (57%)] Loss: 19522.050781\n",
      "Train Epoch: 88 [129984/225000 (58%)] Loss: 19147.238281\n",
      "Train Epoch: 88 [132480/225000 (59%)] Loss: 19223.847656\n",
      "Train Epoch: 88 [134976/225000 (60%)] Loss: 19860.851562\n",
      "Train Epoch: 88 [137472/225000 (61%)] Loss: 19928.976562\n",
      "Train Epoch: 88 [139968/225000 (62%)] Loss: 19275.296875\n",
      "Train Epoch: 88 [142464/225000 (63%)] Loss: 19414.011719\n",
      "Train Epoch: 88 [144960/225000 (64%)] Loss: 19263.623047\n",
      "Train Epoch: 88 [147456/225000 (66%)] Loss: 20034.697266\n",
      "Train Epoch: 88 [149952/225000 (67%)] Loss: 19633.863281\n",
      "Train Epoch: 88 [152448/225000 (68%)] Loss: 19192.246094\n",
      "Train Epoch: 88 [154944/225000 (69%)] Loss: 19312.261719\n",
      "Train Epoch: 88 [157440/225000 (70%)] Loss: 19048.298828\n",
      "Train Epoch: 88 [159936/225000 (71%)] Loss: 19682.578125\n",
      "Train Epoch: 88 [162432/225000 (72%)] Loss: 19659.015625\n",
      "Train Epoch: 88 [164928/225000 (73%)] Loss: 19346.562500\n",
      "Train Epoch: 88 [167424/225000 (74%)] Loss: 19633.730469\n",
      "Train Epoch: 88 [169920/225000 (76%)] Loss: 19594.515625\n",
      "Train Epoch: 88 [172416/225000 (77%)] Loss: 19998.460938\n",
      "Train Epoch: 88 [174912/225000 (78%)] Loss: 19578.712891\n",
      "Train Epoch: 88 [177408/225000 (79%)] Loss: 19223.402344\n",
      "Train Epoch: 88 [179904/225000 (80%)] Loss: 19259.007812\n",
      "Train Epoch: 88 [182400/225000 (81%)] Loss: 19985.355469\n",
      "Train Epoch: 88 [184896/225000 (82%)] Loss: 19664.378906\n",
      "Train Epoch: 88 [187392/225000 (83%)] Loss: 19497.656250\n",
      "Train Epoch: 88 [189888/225000 (84%)] Loss: 19373.445312\n",
      "Train Epoch: 88 [192384/225000 (86%)] Loss: 19312.648438\n",
      "Train Epoch: 88 [194880/225000 (87%)] Loss: 19295.646484\n",
      "Train Epoch: 88 [197376/225000 (88%)] Loss: 19209.048828\n",
      "Train Epoch: 88 [199872/225000 (89%)] Loss: 19112.619141\n",
      "Train Epoch: 88 [202368/225000 (90%)] Loss: 19476.865234\n",
      "Train Epoch: 88 [204864/225000 (91%)] Loss: 19560.687500\n",
      "Train Epoch: 88 [207360/225000 (92%)] Loss: 19362.863281\n",
      "Train Epoch: 88 [209856/225000 (93%)] Loss: 20287.914062\n",
      "Train Epoch: 88 [212352/225000 (94%)] Loss: 19171.099609\n",
      "Train Epoch: 88 [214848/225000 (95%)] Loss: 19423.787109\n",
      "Train Epoch: 88 [217344/225000 (97%)] Loss: 19805.234375\n",
      "Train Epoch: 88 [219840/225000 (98%)] Loss: 19233.013672\n",
      "Train Epoch: 88 [222336/225000 (99%)] Loss: 19954.046875\n",
      "Train Epoch: 88 [224832/225000 (100%)] Loss: 20088.671875\n",
      "    epoch          : 88\n",
      "    loss           : 19605.30499580045\n",
      "    val_loss       : 19490.79204581166\n",
      "Train Epoch: 89 [192/225000 (0%)] Loss: 19624.560547\n",
      "Train Epoch: 89 [2688/225000 (1%)] Loss: 19187.859375\n",
      "Train Epoch: 89 [5184/225000 (2%)] Loss: 19939.289062\n",
      "Train Epoch: 89 [7680/225000 (3%)] Loss: 19850.046875\n",
      "Train Epoch: 89 [10176/225000 (5%)] Loss: 19659.683594\n",
      "Train Epoch: 89 [12672/225000 (6%)] Loss: 19540.714844\n",
      "Train Epoch: 89 [15168/225000 (7%)] Loss: 19092.500000\n",
      "Train Epoch: 89 [17664/225000 (8%)] Loss: 19425.855469\n",
      "Train Epoch: 89 [20160/225000 (9%)] Loss: 19268.183594\n",
      "Train Epoch: 89 [22656/225000 (10%)] Loss: 20059.419922\n",
      "Train Epoch: 89 [25152/225000 (11%)] Loss: 19490.988281\n",
      "Train Epoch: 89 [27648/225000 (12%)] Loss: 19042.085938\n",
      "Train Epoch: 89 [30144/225000 (13%)] Loss: 19495.238281\n",
      "Train Epoch: 89 [32640/225000 (15%)] Loss: 19285.566406\n",
      "Train Epoch: 89 [35136/225000 (16%)] Loss: 19188.333984\n",
      "Train Epoch: 89 [37632/225000 (17%)] Loss: 19301.300781\n",
      "Train Epoch: 89 [40128/225000 (18%)] Loss: 19800.589844\n",
      "Train Epoch: 89 [42624/225000 (19%)] Loss: 19237.800781\n",
      "Train Epoch: 89 [45120/225000 (20%)] Loss: 19109.125000\n",
      "Train Epoch: 89 [47616/225000 (21%)] Loss: 19538.195312\n",
      "Train Epoch: 89 [50112/225000 (22%)] Loss: 19454.714844\n",
      "Train Epoch: 89 [52608/225000 (23%)] Loss: 19944.179688\n",
      "Train Epoch: 89 [55104/225000 (24%)] Loss: 19491.132812\n",
      "Train Epoch: 89 [57600/225000 (26%)] Loss: 19773.609375\n",
      "Train Epoch: 89 [60096/225000 (27%)] Loss: 19511.222656\n",
      "Train Epoch: 89 [62592/225000 (28%)] Loss: 19329.800781\n",
      "Train Epoch: 89 [65088/225000 (29%)] Loss: 20086.789062\n",
      "Train Epoch: 89 [67584/225000 (30%)] Loss: 19455.250000\n",
      "Train Epoch: 89 [70080/225000 (31%)] Loss: 19779.542969\n",
      "Train Epoch: 89 [72576/225000 (32%)] Loss: 20081.464844\n",
      "Train Epoch: 89 [75072/225000 (33%)] Loss: 19481.460938\n",
      "Train Epoch: 89 [77568/225000 (34%)] Loss: 19424.894531\n",
      "Train Epoch: 89 [80064/225000 (36%)] Loss: 19244.587891\n",
      "Train Epoch: 89 [82560/225000 (37%)] Loss: 19314.955078\n",
      "Train Epoch: 89 [85056/225000 (38%)] Loss: 19237.886719\n",
      "Train Epoch: 89 [87552/225000 (39%)] Loss: 19656.496094\n",
      "Train Epoch: 89 [90048/225000 (40%)] Loss: 19624.265625\n",
      "Train Epoch: 89 [92544/225000 (41%)] Loss: 19608.035156\n",
      "Train Epoch: 89 [95040/225000 (42%)] Loss: 19387.820312\n",
      "Train Epoch: 89 [97536/225000 (43%)] Loss: 19900.873047\n",
      "Train Epoch: 89 [100032/225000 (44%)] Loss: 19203.042969\n",
      "Train Epoch: 89 [102528/225000 (46%)] Loss: 19466.050781\n",
      "Train Epoch: 89 [105024/225000 (47%)] Loss: 19886.820312\n",
      "Train Epoch: 89 [107520/225000 (48%)] Loss: 20119.054688\n",
      "Train Epoch: 89 [110016/225000 (49%)] Loss: 19377.773438\n",
      "Train Epoch: 89 [112512/225000 (50%)] Loss: 20278.875000\n",
      "Train Epoch: 89 [115008/225000 (51%)] Loss: 19359.792969\n",
      "Train Epoch: 89 [117504/225000 (52%)] Loss: 19945.203125\n",
      "Train Epoch: 89 [120000/225000 (53%)] Loss: 19781.937500\n",
      "Train Epoch: 89 [122496/225000 (54%)] Loss: 19783.890625\n",
      "Train Epoch: 89 [124992/225000 (56%)] Loss: 19795.103516\n",
      "Train Epoch: 89 [127488/225000 (57%)] Loss: 19359.554688\n",
      "Train Epoch: 89 [129984/225000 (58%)] Loss: 19390.984375\n",
      "Train Epoch: 89 [132480/225000 (59%)] Loss: 19432.980469\n",
      "Train Epoch: 89 [134976/225000 (60%)] Loss: 20053.679688\n",
      "Train Epoch: 89 [137472/225000 (61%)] Loss: 19754.648438\n",
      "Train Epoch: 89 [139968/225000 (62%)] Loss: 19475.621094\n",
      "Train Epoch: 89 [142464/225000 (63%)] Loss: 20025.003906\n",
      "Train Epoch: 89 [144960/225000 (64%)] Loss: 19630.472656\n",
      "Train Epoch: 89 [147456/225000 (66%)] Loss: 19761.724609\n",
      "Train Epoch: 89 [149952/225000 (67%)] Loss: 19606.683594\n",
      "Train Epoch: 89 [152448/225000 (68%)] Loss: 19009.820312\n",
      "Train Epoch: 89 [154944/225000 (69%)] Loss: 19764.664062\n",
      "Train Epoch: 89 [157440/225000 (70%)] Loss: 19976.031250\n",
      "Train Epoch: 89 [159936/225000 (71%)] Loss: 19733.197266\n",
      "Train Epoch: 89 [162432/225000 (72%)] Loss: 19657.847656\n",
      "Train Epoch: 89 [164928/225000 (73%)] Loss: 19656.021484\n",
      "Train Epoch: 89 [167424/225000 (74%)] Loss: 19573.189453\n",
      "Train Epoch: 89 [169920/225000 (76%)] Loss: 19692.601562\n",
      "Train Epoch: 89 [172416/225000 (77%)] Loss: 19645.242188\n",
      "Train Epoch: 89 [174912/225000 (78%)] Loss: 19755.847656\n",
      "Train Epoch: 89 [177408/225000 (79%)] Loss: 19702.167969\n",
      "Train Epoch: 89 [179904/225000 (80%)] Loss: 19484.220703\n",
      "Train Epoch: 89 [182400/225000 (81%)] Loss: 19532.027344\n",
      "Train Epoch: 89 [184896/225000 (82%)] Loss: 19599.845703\n",
      "Train Epoch: 89 [187392/225000 (83%)] Loss: 19484.162109\n",
      "Train Epoch: 89 [189888/225000 (84%)] Loss: 20061.894531\n",
      "Train Epoch: 89 [192384/225000 (86%)] Loss: 19491.166016\n",
      "Train Epoch: 89 [194880/225000 (87%)] Loss: 19108.898438\n",
      "Train Epoch: 89 [197376/225000 (88%)] Loss: 19626.720703\n",
      "Train Epoch: 89 [199872/225000 (89%)] Loss: 20005.544922\n",
      "Train Epoch: 89 [202368/225000 (90%)] Loss: 19721.240234\n",
      "Train Epoch: 89 [204864/225000 (91%)] Loss: 20335.730469\n",
      "Train Epoch: 89 [207360/225000 (92%)] Loss: 19764.812500\n",
      "Train Epoch: 89 [209856/225000 (93%)] Loss: 19596.197266\n",
      "Train Epoch: 89 [212352/225000 (94%)] Loss: 19533.736328\n",
      "Train Epoch: 89 [214848/225000 (95%)] Loss: 19460.351562\n",
      "Train Epoch: 89 [217344/225000 (97%)] Loss: 20131.417969\n",
      "Train Epoch: 89 [219840/225000 (98%)] Loss: 19342.732422\n",
      "Train Epoch: 89 [222336/225000 (99%)] Loss: 19512.152344\n",
      "Train Epoch: 89 [224832/225000 (100%)] Loss: 19535.673828\n",
      "    epoch          : 89\n",
      "    loss           : 19585.5663912516\n",
      "    val_loss       : 19485.905604508087\n",
      "Train Epoch: 90 [192/225000 (0%)] Loss: 19491.431641\n",
      "Train Epoch: 90 [2688/225000 (1%)] Loss: 19871.011719\n",
      "Train Epoch: 90 [5184/225000 (2%)] Loss: 19824.953125\n",
      "Train Epoch: 90 [7680/225000 (3%)] Loss: 19159.384766\n",
      "Train Epoch: 90 [10176/225000 (5%)] Loss: 19946.109375\n",
      "Train Epoch: 90 [12672/225000 (6%)] Loss: 19246.720703\n",
      "Train Epoch: 90 [15168/225000 (7%)] Loss: 19381.781250\n",
      "Train Epoch: 90 [17664/225000 (8%)] Loss: 19514.945312\n",
      "Train Epoch: 90 [20160/225000 (9%)] Loss: 19914.082031\n",
      "Train Epoch: 90 [22656/225000 (10%)] Loss: 20026.605469\n",
      "Train Epoch: 90 [25152/225000 (11%)] Loss: 19296.615234\n",
      "Train Epoch: 90 [27648/225000 (12%)] Loss: 19768.908203\n",
      "Train Epoch: 90 [30144/225000 (13%)] Loss: 19446.357422\n",
      "Train Epoch: 90 [32640/225000 (15%)] Loss: 19472.636719\n",
      "Train Epoch: 90 [35136/225000 (16%)] Loss: 19727.785156\n",
      "Train Epoch: 90 [37632/225000 (17%)] Loss: 19294.386719\n",
      "Train Epoch: 90 [40128/225000 (18%)] Loss: 19740.960938\n",
      "Train Epoch: 90 [42624/225000 (19%)] Loss: 19909.451172\n",
      "Train Epoch: 90 [45120/225000 (20%)] Loss: 19488.203125\n",
      "Train Epoch: 90 [47616/225000 (21%)] Loss: 19380.466797\n",
      "Train Epoch: 90 [50112/225000 (22%)] Loss: 19527.171875\n",
      "Train Epoch: 90 [52608/225000 (23%)] Loss: 19541.800781\n",
      "Train Epoch: 90 [55104/225000 (24%)] Loss: 19751.835938\n",
      "Train Epoch: 90 [57600/225000 (26%)] Loss: 19343.140625\n",
      "Train Epoch: 90 [60096/225000 (27%)] Loss: 19705.394531\n",
      "Train Epoch: 90 [62592/225000 (28%)] Loss: 19858.074219\n",
      "Train Epoch: 90 [65088/225000 (29%)] Loss: 19152.585938\n",
      "Train Epoch: 90 [67584/225000 (30%)] Loss: 19596.080078\n",
      "Train Epoch: 90 [70080/225000 (31%)] Loss: 19697.843750\n",
      "Train Epoch: 90 [72576/225000 (32%)] Loss: 20024.640625\n",
      "Train Epoch: 90 [75072/225000 (33%)] Loss: 19332.322266\n",
      "Train Epoch: 90 [77568/225000 (34%)] Loss: 19240.279297\n",
      "Train Epoch: 90 [80064/225000 (36%)] Loss: 19215.664062\n",
      "Train Epoch: 90 [82560/225000 (37%)] Loss: 19563.091797\n",
      "Train Epoch: 90 [85056/225000 (38%)] Loss: 19255.804688\n",
      "Train Epoch: 90 [87552/225000 (39%)] Loss: 20071.748047\n",
      "Train Epoch: 90 [90048/225000 (40%)] Loss: 19393.234375\n",
      "Train Epoch: 90 [92544/225000 (41%)] Loss: 19896.679688\n",
      "Train Epoch: 90 [95040/225000 (42%)] Loss: 19276.859375\n",
      "Train Epoch: 90 [97536/225000 (43%)] Loss: 20020.554688\n",
      "Train Epoch: 90 [100032/225000 (44%)] Loss: 19731.205078\n",
      "Train Epoch: 90 [102528/225000 (46%)] Loss: 19897.990234\n",
      "Train Epoch: 90 [105024/225000 (47%)] Loss: 19120.574219\n",
      "Train Epoch: 90 [107520/225000 (48%)] Loss: 19413.642578\n",
      "Train Epoch: 90 [110016/225000 (49%)] Loss: 19756.587891\n",
      "Train Epoch: 90 [112512/225000 (50%)] Loss: 19193.566406\n",
      "Train Epoch: 90 [115008/225000 (51%)] Loss: 19255.636719\n",
      "Train Epoch: 90 [117504/225000 (52%)] Loss: 19140.808594\n",
      "Train Epoch: 90 [120000/225000 (53%)] Loss: 19714.238281\n",
      "Train Epoch: 90 [122496/225000 (54%)] Loss: 19605.816406\n",
      "Train Epoch: 90 [124992/225000 (56%)] Loss: 19869.078125\n",
      "Train Epoch: 90 [127488/225000 (57%)] Loss: 19665.191406\n",
      "Train Epoch: 90 [129984/225000 (58%)] Loss: 19893.652344\n",
      "Train Epoch: 90 [132480/225000 (59%)] Loss: 19452.539062\n",
      "Train Epoch: 90 [134976/225000 (60%)] Loss: 19592.539062\n",
      "Train Epoch: 90 [137472/225000 (61%)] Loss: 19629.187500\n",
      "Train Epoch: 90 [139968/225000 (62%)] Loss: 19767.460938\n",
      "Train Epoch: 90 [142464/225000 (63%)] Loss: 19287.503906\n",
      "Train Epoch: 90 [144960/225000 (64%)] Loss: 19321.593750\n",
      "Train Epoch: 90 [147456/225000 (66%)] Loss: 19780.605469\n",
      "Train Epoch: 90 [149952/225000 (67%)] Loss: 19535.652344\n",
      "Train Epoch: 90 [152448/225000 (68%)] Loss: 19816.492188\n",
      "Train Epoch: 90 [154944/225000 (69%)] Loss: 19316.777344\n",
      "Train Epoch: 90 [157440/225000 (70%)] Loss: 19644.509766\n",
      "Train Epoch: 90 [159936/225000 (71%)] Loss: 19879.328125\n",
      "Train Epoch: 90 [162432/225000 (72%)] Loss: 19524.050781\n",
      "Train Epoch: 90 [164928/225000 (73%)] Loss: 19976.833984\n",
      "Train Epoch: 90 [167424/225000 (74%)] Loss: 20076.511719\n",
      "Train Epoch: 90 [169920/225000 (76%)] Loss: 19347.148438\n",
      "Train Epoch: 90 [172416/225000 (77%)] Loss: 19743.125000\n",
      "Train Epoch: 90 [174912/225000 (78%)] Loss: 19638.470703\n",
      "Train Epoch: 90 [177408/225000 (79%)] Loss: 19755.902344\n",
      "Train Epoch: 90 [179904/225000 (80%)] Loss: 19682.421875\n",
      "Train Epoch: 90 [182400/225000 (81%)] Loss: 19571.531250\n",
      "Train Epoch: 90 [184896/225000 (82%)] Loss: 19265.714844\n",
      "Train Epoch: 90 [187392/225000 (83%)] Loss: 19776.746094\n",
      "Train Epoch: 90 [189888/225000 (84%)] Loss: 19679.597656\n",
      "Train Epoch: 90 [192384/225000 (86%)] Loss: 19153.761719\n",
      "Train Epoch: 90 [194880/225000 (87%)] Loss: 19443.484375\n",
      "Train Epoch: 90 [197376/225000 (88%)] Loss: 19790.179688\n",
      "Train Epoch: 90 [199872/225000 (89%)] Loss: 24294.128906\n",
      "Train Epoch: 90 [202368/225000 (90%)] Loss: 19886.126953\n",
      "Train Epoch: 90 [204864/225000 (91%)] Loss: 19452.365234\n",
      "Train Epoch: 90 [207360/225000 (92%)] Loss: 19595.089844\n",
      "Train Epoch: 90 [209856/225000 (93%)] Loss: 20133.658203\n",
      "Train Epoch: 90 [212352/225000 (94%)] Loss: 20240.148438\n",
      "Train Epoch: 90 [214848/225000 (95%)] Loss: 19588.128906\n",
      "Train Epoch: 90 [217344/225000 (97%)] Loss: 19360.265625\n",
      "Train Epoch: 90 [219840/225000 (98%)] Loss: 19469.218750\n",
      "Train Epoch: 90 [222336/225000 (99%)] Loss: 19371.453125\n",
      "Train Epoch: 90 [224832/225000 (100%)] Loss: 19419.183594\n",
      "    epoch          : 90\n",
      "    loss           : 19586.190987961283\n",
      "    val_loss       : 19475.924868764767\n",
      "Train Epoch: 91 [192/225000 (0%)] Loss: 19548.363281\n",
      "Train Epoch: 91 [2688/225000 (1%)] Loss: 19724.523438\n",
      "Train Epoch: 91 [5184/225000 (2%)] Loss: 19621.562500\n",
      "Train Epoch: 91 [7680/225000 (3%)] Loss: 19407.367188\n",
      "Train Epoch: 91 [10176/225000 (5%)] Loss: 19726.242188\n",
      "Train Epoch: 91 [12672/225000 (6%)] Loss: 19484.886719\n",
      "Train Epoch: 91 [15168/225000 (7%)] Loss: 19896.781250\n",
      "Train Epoch: 91 [17664/225000 (8%)] Loss: 19667.128906\n",
      "Train Epoch: 91 [20160/225000 (9%)] Loss: 19478.886719\n",
      "Train Epoch: 91 [22656/225000 (10%)] Loss: 19309.394531\n",
      "Train Epoch: 91 [25152/225000 (11%)] Loss: 19447.882812\n",
      "Train Epoch: 91 [27648/225000 (12%)] Loss: 19761.341797\n",
      "Train Epoch: 91 [30144/225000 (13%)] Loss: 19550.755859\n",
      "Train Epoch: 91 [32640/225000 (15%)] Loss: 19994.820312\n",
      "Train Epoch: 91 [35136/225000 (16%)] Loss: 19735.923828\n",
      "Train Epoch: 91 [37632/225000 (17%)] Loss: 19409.632812\n",
      "Train Epoch: 91 [40128/225000 (18%)] Loss: 19742.601562\n",
      "Train Epoch: 91 [42624/225000 (19%)] Loss: 19426.183594\n",
      "Train Epoch: 91 [45120/225000 (20%)] Loss: 19380.931641\n",
      "Train Epoch: 91 [47616/225000 (21%)] Loss: 19546.320312\n",
      "Train Epoch: 91 [50112/225000 (22%)] Loss: 19472.328125\n",
      "Train Epoch: 91 [52608/225000 (23%)] Loss: 19524.199219\n",
      "Train Epoch: 91 [55104/225000 (24%)] Loss: 19512.304688\n",
      "Train Epoch: 91 [57600/225000 (26%)] Loss: 19569.765625\n",
      "Train Epoch: 91 [60096/225000 (27%)] Loss: 19567.623047\n",
      "Train Epoch: 91 [62592/225000 (28%)] Loss: 19437.404297\n",
      "Train Epoch: 91 [65088/225000 (29%)] Loss: 19454.740234\n",
      "Train Epoch: 91 [67584/225000 (30%)] Loss: 19405.435547\n",
      "Train Epoch: 91 [70080/225000 (31%)] Loss: 19793.902344\n",
      "Train Epoch: 91 [72576/225000 (32%)] Loss: 19862.816406\n",
      "Train Epoch: 91 [75072/225000 (33%)] Loss: 19359.539062\n",
      "Train Epoch: 91 [77568/225000 (34%)] Loss: 19543.031250\n",
      "Train Epoch: 91 [80064/225000 (36%)] Loss: 19486.939453\n",
      "Train Epoch: 91 [82560/225000 (37%)] Loss: 19728.148438\n",
      "Train Epoch: 91 [85056/225000 (38%)] Loss: 18855.591797\n",
      "Train Epoch: 91 [87552/225000 (39%)] Loss: 19807.064453\n",
      "Train Epoch: 91 [90048/225000 (40%)] Loss: 19853.878906\n",
      "Train Epoch: 91 [92544/225000 (41%)] Loss: 19572.267578\n",
      "Train Epoch: 91 [95040/225000 (42%)] Loss: 19299.697266\n",
      "Train Epoch: 91 [97536/225000 (43%)] Loss: 19404.226562\n",
      "Train Epoch: 91 [100032/225000 (44%)] Loss: 19681.470703\n",
      "Train Epoch: 91 [102528/225000 (46%)] Loss: 19649.542969\n",
      "Train Epoch: 91 [105024/225000 (47%)] Loss: 20107.125000\n",
      "Train Epoch: 91 [107520/225000 (48%)] Loss: 19121.976562\n",
      "Train Epoch: 91 [110016/225000 (49%)] Loss: 19548.269531\n",
      "Train Epoch: 91 [112512/225000 (50%)] Loss: 19306.695312\n",
      "Train Epoch: 91 [115008/225000 (51%)] Loss: 19464.578125\n",
      "Train Epoch: 91 [117504/225000 (52%)] Loss: 19962.515625\n",
      "Train Epoch: 91 [120000/225000 (53%)] Loss: 19426.708984\n",
      "Train Epoch: 91 [122496/225000 (54%)] Loss: 19411.605469\n",
      "Train Epoch: 91 [124992/225000 (56%)] Loss: 19573.261719\n",
      "Train Epoch: 91 [127488/225000 (57%)] Loss: 19908.212891\n",
      "Train Epoch: 91 [129984/225000 (58%)] Loss: 19154.000000\n",
      "Train Epoch: 91 [132480/225000 (59%)] Loss: 19513.111328\n",
      "Train Epoch: 91 [134976/225000 (60%)] Loss: 19760.988281\n",
      "Train Epoch: 91 [137472/225000 (61%)] Loss: 20131.443359\n",
      "Train Epoch: 91 [139968/225000 (62%)] Loss: 19312.132812\n",
      "Train Epoch: 91 [142464/225000 (63%)] Loss: 19435.376953\n",
      "Train Epoch: 91 [144960/225000 (64%)] Loss: 19581.220703\n",
      "Train Epoch: 91 [147456/225000 (66%)] Loss: 18989.011719\n",
      "Train Epoch: 91 [149952/225000 (67%)] Loss: 19845.140625\n",
      "Train Epoch: 91 [152448/225000 (68%)] Loss: 19356.167969\n",
      "Train Epoch: 91 [154944/225000 (69%)] Loss: 20049.414062\n",
      "Train Epoch: 91 [157440/225000 (70%)] Loss: 19646.658203\n",
      "Train Epoch: 91 [159936/225000 (71%)] Loss: 18892.876953\n",
      "Train Epoch: 91 [162432/225000 (72%)] Loss: 19490.880859\n",
      "Train Epoch: 91 [164928/225000 (73%)] Loss: 19347.761719\n",
      "Train Epoch: 91 [167424/225000 (74%)] Loss: 19584.976562\n",
      "Train Epoch: 91 [169920/225000 (76%)] Loss: 19743.623047\n",
      "Train Epoch: 91 [172416/225000 (77%)] Loss: 19556.820312\n",
      "Train Epoch: 91 [174912/225000 (78%)] Loss: 19706.773438\n",
      "Train Epoch: 91 [177408/225000 (79%)] Loss: 19714.904297\n",
      "Train Epoch: 91 [179904/225000 (80%)] Loss: 20011.804688\n",
      "Train Epoch: 91 [182400/225000 (81%)] Loss: 19822.457031\n",
      "Train Epoch: 91 [184896/225000 (82%)] Loss: 19447.558594\n",
      "Train Epoch: 91 [187392/225000 (83%)] Loss: 19726.410156\n",
      "Train Epoch: 91 [189888/225000 (84%)] Loss: 19738.421875\n",
      "Train Epoch: 91 [192384/225000 (86%)] Loss: 19433.132812\n",
      "Train Epoch: 91 [194880/225000 (87%)] Loss: 20265.683594\n",
      "Train Epoch: 91 [197376/225000 (88%)] Loss: 19664.783203\n",
      "Train Epoch: 91 [199872/225000 (89%)] Loss: 19939.103516\n",
      "Train Epoch: 91 [202368/225000 (90%)] Loss: 19920.777344\n",
      "Train Epoch: 91 [204864/225000 (91%)] Loss: 19452.009766\n",
      "Train Epoch: 91 [207360/225000 (92%)] Loss: 19686.796875\n",
      "Train Epoch: 91 [209856/225000 (93%)] Loss: 19670.796875\n",
      "Train Epoch: 91 [212352/225000 (94%)] Loss: 19196.673828\n",
      "Train Epoch: 91 [214848/225000 (95%)] Loss: 19609.093750\n",
      "Train Epoch: 91 [217344/225000 (97%)] Loss: 19744.214844\n",
      "Train Epoch: 91 [219840/225000 (98%)] Loss: 19702.796875\n",
      "Train Epoch: 91 [222336/225000 (99%)] Loss: 19923.607422\n",
      "Train Epoch: 91 [224832/225000 (100%)] Loss: 19491.062500\n",
      "    epoch          : 91\n",
      "    loss           : 19574.224981002026\n",
      "    val_loss       : 19465.856796714186\n",
      "Train Epoch: 92 [192/225000 (0%)] Loss: 20055.242188\n",
      "Train Epoch: 92 [2688/225000 (1%)] Loss: 19790.695312\n",
      "Train Epoch: 92 [5184/225000 (2%)] Loss: 19528.410156\n",
      "Train Epoch: 92 [7680/225000 (3%)] Loss: 19584.425781\n",
      "Train Epoch: 92 [10176/225000 (5%)] Loss: 20092.056641\n",
      "Train Epoch: 92 [12672/225000 (6%)] Loss: 19609.578125\n",
      "Train Epoch: 92 [15168/225000 (7%)] Loss: 19673.785156\n",
      "Train Epoch: 92 [17664/225000 (8%)] Loss: 19968.419922\n",
      "Train Epoch: 92 [20160/225000 (9%)] Loss: 20026.593750\n",
      "Train Epoch: 92 [22656/225000 (10%)] Loss: 19366.400391\n",
      "Train Epoch: 92 [25152/225000 (11%)] Loss: 19995.871094\n",
      "Train Epoch: 92 [27648/225000 (12%)] Loss: 19625.695312\n",
      "Train Epoch: 92 [30144/225000 (13%)] Loss: 19651.300781\n",
      "Train Epoch: 92 [32640/225000 (15%)] Loss: 19639.736328\n",
      "Train Epoch: 92 [35136/225000 (16%)] Loss: 19727.558594\n",
      "Train Epoch: 92 [37632/225000 (17%)] Loss: 19655.152344\n",
      "Train Epoch: 92 [40128/225000 (18%)] Loss: 19817.039062\n",
      "Train Epoch: 92 [42624/225000 (19%)] Loss: 19907.710938\n",
      "Train Epoch: 92 [45120/225000 (20%)] Loss: 19545.201172\n",
      "Train Epoch: 92 [47616/225000 (21%)] Loss: 19659.689453\n",
      "Train Epoch: 92 [50112/225000 (22%)] Loss: 19265.958984\n",
      "Train Epoch: 92 [52608/225000 (23%)] Loss: 19939.039062\n",
      "Train Epoch: 92 [55104/225000 (24%)] Loss: 19734.160156\n",
      "Train Epoch: 92 [57600/225000 (26%)] Loss: 19286.500000\n",
      "Train Epoch: 92 [60096/225000 (27%)] Loss: 19983.144531\n",
      "Train Epoch: 92 [62592/225000 (28%)] Loss: 19685.021484\n",
      "Train Epoch: 92 [65088/225000 (29%)] Loss: 19225.859375\n",
      "Train Epoch: 92 [67584/225000 (30%)] Loss: 19539.285156\n",
      "Train Epoch: 92 [70080/225000 (31%)] Loss: 19586.906250\n",
      "Train Epoch: 92 [72576/225000 (32%)] Loss: 19316.011719\n",
      "Train Epoch: 92 [75072/225000 (33%)] Loss: 19174.261719\n",
      "Train Epoch: 92 [77568/225000 (34%)] Loss: 19081.958984\n",
      "Train Epoch: 92 [80064/225000 (36%)] Loss: 19753.429688\n",
      "Train Epoch: 92 [82560/225000 (37%)] Loss: 19729.324219\n",
      "Train Epoch: 92 [85056/225000 (38%)] Loss: 19212.714844\n",
      "Train Epoch: 92 [87552/225000 (39%)] Loss: 19826.226562\n",
      "Train Epoch: 92 [90048/225000 (40%)] Loss: 19411.728516\n",
      "Train Epoch: 92 [92544/225000 (41%)] Loss: 19403.425781\n",
      "Train Epoch: 92 [95040/225000 (42%)] Loss: 19990.017578\n",
      "Train Epoch: 92 [97536/225000 (43%)] Loss: 19832.589844\n",
      "Train Epoch: 92 [100032/225000 (44%)] Loss: 19098.921875\n",
      "Train Epoch: 92 [102528/225000 (46%)] Loss: 19697.605469\n",
      "Train Epoch: 92 [105024/225000 (47%)] Loss: 19403.839844\n",
      "Train Epoch: 92 [107520/225000 (48%)] Loss: 19467.367188\n",
      "Train Epoch: 92 [110016/225000 (49%)] Loss: 19459.787109\n",
      "Train Epoch: 92 [112512/225000 (50%)] Loss: 19827.320312\n",
      "Train Epoch: 92 [115008/225000 (51%)] Loss: 19672.746094\n",
      "Train Epoch: 92 [117504/225000 (52%)] Loss: 19733.242188\n",
      "Train Epoch: 92 [120000/225000 (53%)] Loss: 19843.203125\n",
      "Train Epoch: 92 [122496/225000 (54%)] Loss: 19436.417969\n",
      "Train Epoch: 92 [124992/225000 (56%)] Loss: 19801.593750\n",
      "Train Epoch: 92 [127488/225000 (57%)] Loss: 19880.523438\n",
      "Train Epoch: 92 [129984/225000 (58%)] Loss: 19741.093750\n",
      "Train Epoch: 92 [132480/225000 (59%)] Loss: 19182.406250\n",
      "Train Epoch: 92 [134976/225000 (60%)] Loss: 19459.972656\n",
      "Train Epoch: 92 [137472/225000 (61%)] Loss: 19916.609375\n",
      "Train Epoch: 92 [139968/225000 (62%)] Loss: 19240.785156\n",
      "Train Epoch: 92 [142464/225000 (63%)] Loss: 19968.980469\n",
      "Train Epoch: 92 [144960/225000 (64%)] Loss: 19117.218750\n",
      "Train Epoch: 92 [147456/225000 (66%)] Loss: 19588.054688\n",
      "Train Epoch: 92 [149952/225000 (67%)] Loss: 19943.828125\n",
      "Train Epoch: 92 [152448/225000 (68%)] Loss: 19195.214844\n",
      "Train Epoch: 92 [154944/225000 (69%)] Loss: 19480.296875\n",
      "Train Epoch: 92 [157440/225000 (70%)] Loss: 19467.574219\n",
      "Train Epoch: 92 [159936/225000 (71%)] Loss: 19822.687500\n",
      "Train Epoch: 92 [162432/225000 (72%)] Loss: 19474.296875\n",
      "Train Epoch: 92 [164928/225000 (73%)] Loss: 19353.238281\n",
      "Train Epoch: 92 [167424/225000 (74%)] Loss: 19520.761719\n",
      "Train Epoch: 92 [169920/225000 (76%)] Loss: 19691.132812\n",
      "Train Epoch: 92 [172416/225000 (77%)] Loss: 19763.599609\n",
      "Train Epoch: 92 [174912/225000 (78%)] Loss: 19218.884766\n",
      "Train Epoch: 92 [177408/225000 (79%)] Loss: 19023.441406\n",
      "Train Epoch: 92 [179904/225000 (80%)] Loss: 19203.248047\n",
      "Train Epoch: 92 [182400/225000 (81%)] Loss: 19474.695312\n",
      "Train Epoch: 92 [184896/225000 (82%)] Loss: 19097.656250\n",
      "Train Epoch: 92 [187392/225000 (83%)] Loss: 19341.154297\n",
      "Train Epoch: 92 [189888/225000 (84%)] Loss: 19140.332031\n",
      "Train Epoch: 92 [192384/225000 (86%)] Loss: 19693.292969\n",
      "Train Epoch: 92 [194880/225000 (87%)] Loss: 19545.972656\n",
      "Train Epoch: 92 [197376/225000 (88%)] Loss: 19233.978516\n",
      "Train Epoch: 92 [199872/225000 (89%)] Loss: 19612.265625\n",
      "Train Epoch: 92 [202368/225000 (90%)] Loss: 19448.396484\n",
      "Train Epoch: 92 [204864/225000 (91%)] Loss: 19726.847656\n",
      "Train Epoch: 92 [207360/225000 (92%)] Loss: 19888.695312\n",
      "Train Epoch: 92 [209856/225000 (93%)] Loss: 19050.998047\n",
      "Train Epoch: 92 [212352/225000 (94%)] Loss: 19418.386719\n",
      "Train Epoch: 92 [214848/225000 (95%)] Loss: 19221.039062\n",
      "Train Epoch: 92 [217344/225000 (97%)] Loss: 19741.484375\n",
      "Train Epoch: 92 [219840/225000 (98%)] Loss: 19720.000000\n",
      "Train Epoch: 92 [222336/225000 (99%)] Loss: 19986.042969\n",
      "Train Epoch: 92 [224832/225000 (100%)] Loss: 19481.667969\n",
      "    epoch          : 92\n",
      "    loss           : 19559.137425341298\n",
      "    val_loss       : 19470.220948577837\n",
      "Train Epoch: 93 [192/225000 (0%)] Loss: 19573.070312\n",
      "Train Epoch: 93 [2688/225000 (1%)] Loss: 19203.650391\n",
      "Train Epoch: 93 [5184/225000 (2%)] Loss: 18980.742188\n",
      "Train Epoch: 93 [7680/225000 (3%)] Loss: 19430.242188\n",
      "Train Epoch: 93 [10176/225000 (5%)] Loss: 19646.351562\n",
      "Train Epoch: 93 [12672/225000 (6%)] Loss: 19283.521484\n",
      "Train Epoch: 93 [15168/225000 (7%)] Loss: 19850.878906\n",
      "Train Epoch: 93 [17664/225000 (8%)] Loss: 19281.875000\n",
      "Train Epoch: 93 [20160/225000 (9%)] Loss: 19295.556641\n",
      "Train Epoch: 93 [22656/225000 (10%)] Loss: 19708.021484\n",
      "Train Epoch: 93 [25152/225000 (11%)] Loss: 19485.744141\n",
      "Train Epoch: 93 [27648/225000 (12%)] Loss: 19624.289062\n",
      "Train Epoch: 93 [30144/225000 (13%)] Loss: 19333.351562\n",
      "Train Epoch: 93 [32640/225000 (15%)] Loss: 19453.023438\n",
      "Train Epoch: 93 [35136/225000 (16%)] Loss: 19630.851562\n",
      "Train Epoch: 93 [37632/225000 (17%)] Loss: 19546.812500\n",
      "Train Epoch: 93 [40128/225000 (18%)] Loss: 19689.910156\n",
      "Train Epoch: 93 [42624/225000 (19%)] Loss: 20000.314453\n",
      "Train Epoch: 93 [45120/225000 (20%)] Loss: 18948.710938\n",
      "Train Epoch: 93 [47616/225000 (21%)] Loss: 19829.953125\n",
      "Train Epoch: 93 [50112/225000 (22%)] Loss: 20035.550781\n",
      "Train Epoch: 93 [52608/225000 (23%)] Loss: 19320.021484\n",
      "Train Epoch: 93 [55104/225000 (24%)] Loss: 20101.023438\n",
      "Train Epoch: 93 [57600/225000 (26%)] Loss: 19848.015625\n",
      "Train Epoch: 93 [60096/225000 (27%)] Loss: 19627.640625\n",
      "Train Epoch: 93 [62592/225000 (28%)] Loss: 19651.843750\n",
      "Train Epoch: 93 [65088/225000 (29%)] Loss: 19586.324219\n",
      "Train Epoch: 93 [67584/225000 (30%)] Loss: 19753.111328\n",
      "Train Epoch: 93 [70080/225000 (31%)] Loss: 19506.468750\n",
      "Train Epoch: 93 [72576/225000 (32%)] Loss: 19084.722656\n",
      "Train Epoch: 93 [75072/225000 (33%)] Loss: 19668.289062\n",
      "Train Epoch: 93 [77568/225000 (34%)] Loss: 19724.070312\n",
      "Train Epoch: 93 [80064/225000 (36%)] Loss: 19532.003906\n",
      "Train Epoch: 93 [82560/225000 (37%)] Loss: 19996.757812\n",
      "Train Epoch: 93 [85056/225000 (38%)] Loss: 19921.033203\n",
      "Train Epoch: 93 [87552/225000 (39%)] Loss: 19473.691406\n",
      "Train Epoch: 93 [90048/225000 (40%)] Loss: 19557.046875\n",
      "Train Epoch: 93 [92544/225000 (41%)] Loss: 19161.734375\n",
      "Train Epoch: 93 [95040/225000 (42%)] Loss: 19028.519531\n",
      "Train Epoch: 93 [97536/225000 (43%)] Loss: 19522.492188\n",
      "Train Epoch: 93 [100032/225000 (44%)] Loss: 19934.714844\n",
      "Train Epoch: 93 [102528/225000 (46%)] Loss: 19620.234375\n",
      "Train Epoch: 93 [105024/225000 (47%)] Loss: 19907.703125\n",
      "Train Epoch: 93 [107520/225000 (48%)] Loss: 19768.281250\n",
      "Train Epoch: 93 [110016/225000 (49%)] Loss: 19747.070312\n",
      "Train Epoch: 93 [112512/225000 (50%)] Loss: 19389.218750\n",
      "Train Epoch: 93 [115008/225000 (51%)] Loss: 19802.871094\n",
      "Train Epoch: 93 [117504/225000 (52%)] Loss: 19224.464844\n",
      "Train Epoch: 93 [120000/225000 (53%)] Loss: 19027.578125\n",
      "Train Epoch: 93 [122496/225000 (54%)] Loss: 19502.316406\n",
      "Train Epoch: 93 [124992/225000 (56%)] Loss: 19510.359375\n",
      "Train Epoch: 93 [127488/225000 (57%)] Loss: 19718.302734\n",
      "Train Epoch: 93 [129984/225000 (58%)] Loss: 19658.828125\n",
      "Train Epoch: 93 [132480/225000 (59%)] Loss: 19611.363281\n",
      "Train Epoch: 93 [134976/225000 (60%)] Loss: 19580.187500\n",
      "Train Epoch: 93 [137472/225000 (61%)] Loss: 19630.519531\n",
      "Train Epoch: 93 [139968/225000 (62%)] Loss: 19816.539062\n",
      "Train Epoch: 93 [142464/225000 (63%)] Loss: 19526.972656\n",
      "Train Epoch: 93 [144960/225000 (64%)] Loss: 19637.910156\n",
      "Train Epoch: 93 [147456/225000 (66%)] Loss: 19741.445312\n",
      "Train Epoch: 93 [149952/225000 (67%)] Loss: 19489.771484\n",
      "Train Epoch: 93 [152448/225000 (68%)] Loss: 19617.257812\n",
      "Train Epoch: 93 [154944/225000 (69%)] Loss: 19770.316406\n",
      "Train Epoch: 93 [157440/225000 (70%)] Loss: 19593.507812\n",
      "Train Epoch: 93 [159936/225000 (71%)] Loss: 18849.214844\n",
      "Train Epoch: 93 [162432/225000 (72%)] Loss: 19498.750000\n",
      "Train Epoch: 93 [164928/225000 (73%)] Loss: 19758.769531\n",
      "Train Epoch: 93 [167424/225000 (74%)] Loss: 19967.394531\n",
      "Train Epoch: 93 [169920/225000 (76%)] Loss: 19957.703125\n",
      "Train Epoch: 93 [172416/225000 (77%)] Loss: 19365.226562\n",
      "Train Epoch: 93 [174912/225000 (78%)] Loss: 19343.109375\n",
      "Train Epoch: 93 [177408/225000 (79%)] Loss: 19350.410156\n",
      "Train Epoch: 93 [179904/225000 (80%)] Loss: 19043.076172\n",
      "Train Epoch: 93 [182400/225000 (81%)] Loss: 19433.824219\n",
      "Train Epoch: 93 [184896/225000 (82%)] Loss: 19357.070312\n",
      "Train Epoch: 93 [187392/225000 (83%)] Loss: 19318.488281\n",
      "Train Epoch: 93 [189888/225000 (84%)] Loss: 19549.378906\n",
      "Train Epoch: 93 [192384/225000 (86%)] Loss: 19447.367188\n",
      "Train Epoch: 93 [194880/225000 (87%)] Loss: 19289.566406\n",
      "Train Epoch: 93 [197376/225000 (88%)] Loss: 19351.376953\n",
      "Train Epoch: 93 [199872/225000 (89%)] Loss: 19539.964844\n",
      "Train Epoch: 93 [202368/225000 (90%)] Loss: 19424.943359\n",
      "Train Epoch: 93 [204864/225000 (91%)] Loss: 19593.882812\n",
      "Train Epoch: 93 [207360/225000 (92%)] Loss: 19926.230469\n",
      "Train Epoch: 93 [209856/225000 (93%)] Loss: 19312.359375\n",
      "Train Epoch: 93 [212352/225000 (94%)] Loss: 20189.333984\n",
      "Train Epoch: 93 [214848/225000 (95%)] Loss: 19277.367188\n",
      "Train Epoch: 93 [217344/225000 (97%)] Loss: 19636.683594\n",
      "Train Epoch: 93 [219840/225000 (98%)] Loss: 19402.046875\n",
      "Train Epoch: 93 [222336/225000 (99%)] Loss: 19749.132812\n",
      "Train Epoch: 93 [224832/225000 (100%)] Loss: 19986.882812\n",
      "    epoch          : 93\n",
      "    loss           : 19549.839193819327\n",
      "    val_loss       : 19448.58067354901\n",
      "Train Epoch: 94 [192/225000 (0%)] Loss: 19834.421875\n",
      "Train Epoch: 94 [2688/225000 (1%)] Loss: 19685.822266\n",
      "Train Epoch: 94 [5184/225000 (2%)] Loss: 19495.078125\n",
      "Train Epoch: 94 [7680/225000 (3%)] Loss: 19562.843750\n",
      "Train Epoch: 94 [10176/225000 (5%)] Loss: 19573.527344\n",
      "Train Epoch: 94 [12672/225000 (6%)] Loss: 19417.929688\n",
      "Train Epoch: 94 [15168/225000 (7%)] Loss: 20018.791016\n",
      "Train Epoch: 94 [17664/225000 (8%)] Loss: 19362.355469\n",
      "Train Epoch: 94 [20160/225000 (9%)] Loss: 19313.695312\n",
      "Train Epoch: 94 [22656/225000 (10%)] Loss: 19631.255859\n",
      "Train Epoch: 94 [25152/225000 (11%)] Loss: 19486.457031\n",
      "Train Epoch: 94 [27648/225000 (12%)] Loss: 19670.050781\n",
      "Train Epoch: 94 [30144/225000 (13%)] Loss: 19634.359375\n",
      "Train Epoch: 94 [32640/225000 (15%)] Loss: 19561.199219\n",
      "Train Epoch: 94 [35136/225000 (16%)] Loss: 19684.734375\n",
      "Train Epoch: 94 [37632/225000 (17%)] Loss: 19293.714844\n",
      "Train Epoch: 94 [40128/225000 (18%)] Loss: 19534.824219\n",
      "Train Epoch: 94 [42624/225000 (19%)] Loss: 19406.878906\n",
      "Train Epoch: 94 [45120/225000 (20%)] Loss: 19414.042969\n",
      "Train Epoch: 94 [47616/225000 (21%)] Loss: 19893.472656\n",
      "Train Epoch: 94 [50112/225000 (22%)] Loss: 19926.656250\n",
      "Train Epoch: 94 [52608/225000 (23%)] Loss: 19325.371094\n",
      "Train Epoch: 94 [55104/225000 (24%)] Loss: 19511.384766\n",
      "Train Epoch: 94 [57600/225000 (26%)] Loss: 19169.988281\n",
      "Train Epoch: 94 [60096/225000 (27%)] Loss: 19798.265625\n",
      "Train Epoch: 94 [62592/225000 (28%)] Loss: 19477.449219\n",
      "Train Epoch: 94 [65088/225000 (29%)] Loss: 19479.123047\n",
      "Train Epoch: 94 [67584/225000 (30%)] Loss: 19567.341797\n",
      "Train Epoch: 94 [70080/225000 (31%)] Loss: 19785.150391\n",
      "Train Epoch: 94 [72576/225000 (32%)] Loss: 19871.613281\n",
      "Train Epoch: 94 [75072/225000 (33%)] Loss: 19590.595703\n",
      "Train Epoch: 94 [77568/225000 (34%)] Loss: 19095.923828\n",
      "Train Epoch: 94 [80064/225000 (36%)] Loss: 19268.312500\n",
      "Train Epoch: 94 [82560/225000 (37%)] Loss: 19781.121094\n",
      "Train Epoch: 94 [85056/225000 (38%)] Loss: 19570.482422\n",
      "Train Epoch: 94 [87552/225000 (39%)] Loss: 19493.265625\n",
      "Train Epoch: 94 [90048/225000 (40%)] Loss: 20029.255859\n",
      "Train Epoch: 94 [92544/225000 (41%)] Loss: 19394.919922\n",
      "Train Epoch: 94 [95040/225000 (42%)] Loss: 19654.367188\n",
      "Train Epoch: 94 [97536/225000 (43%)] Loss: 19495.703125\n",
      "Train Epoch: 94 [100032/225000 (44%)] Loss: 19604.919922\n",
      "Train Epoch: 94 [102528/225000 (46%)] Loss: 19731.941406\n",
      "Train Epoch: 94 [105024/225000 (47%)] Loss: 19376.527344\n",
      "Train Epoch: 94 [107520/225000 (48%)] Loss: 19666.738281\n",
      "Train Epoch: 94 [110016/225000 (49%)] Loss: 19222.839844\n",
      "Train Epoch: 94 [112512/225000 (50%)] Loss: 20035.417969\n",
      "Train Epoch: 94 [115008/225000 (51%)] Loss: 19391.726562\n",
      "Train Epoch: 94 [117504/225000 (52%)] Loss: 19605.757812\n",
      "Train Epoch: 94 [120000/225000 (53%)] Loss: 19833.300781\n",
      "Train Epoch: 94 [122496/225000 (54%)] Loss: 19514.421875\n",
      "Train Epoch: 94 [124992/225000 (56%)] Loss: 19398.058594\n",
      "Train Epoch: 94 [127488/225000 (57%)] Loss: 19267.123047\n",
      "Train Epoch: 94 [129984/225000 (58%)] Loss: 19445.585938\n",
      "Train Epoch: 94 [132480/225000 (59%)] Loss: 19759.320312\n",
      "Train Epoch: 94 [134976/225000 (60%)] Loss: 19743.386719\n",
      "Train Epoch: 94 [137472/225000 (61%)] Loss: 19016.376953\n",
      "Train Epoch: 94 [139968/225000 (62%)] Loss: 19667.320312\n",
      "Train Epoch: 94 [142464/225000 (63%)] Loss: 19605.998047\n",
      "Train Epoch: 94 [144960/225000 (64%)] Loss: 19833.082031\n",
      "Train Epoch: 94 [147456/225000 (66%)] Loss: 19739.769531\n",
      "Train Epoch: 94 [149952/225000 (67%)] Loss: 19470.880859\n",
      "Train Epoch: 94 [152448/225000 (68%)] Loss: 19479.234375\n",
      "Train Epoch: 94 [154944/225000 (69%)] Loss: 19265.609375\n",
      "Train Epoch: 94 [157440/225000 (70%)] Loss: 19349.023438\n",
      "Train Epoch: 94 [159936/225000 (71%)] Loss: 19130.558594\n",
      "Train Epoch: 94 [162432/225000 (72%)] Loss: 19674.716797\n",
      "Train Epoch: 94 [164928/225000 (73%)] Loss: 19392.185547\n",
      "Train Epoch: 94 [167424/225000 (74%)] Loss: 19479.412109\n",
      "Train Epoch: 94 [169920/225000 (76%)] Loss: 19718.597656\n",
      "Train Epoch: 94 [172416/225000 (77%)] Loss: 19760.304688\n",
      "Train Epoch: 94 [174912/225000 (78%)] Loss: 19991.152344\n",
      "Train Epoch: 94 [177408/225000 (79%)] Loss: 19209.310547\n",
      "Train Epoch: 94 [179904/225000 (80%)] Loss: 19564.605469\n",
      "Train Epoch: 94 [182400/225000 (81%)] Loss: 19101.093750\n",
      "Train Epoch: 94 [184896/225000 (82%)] Loss: 19998.074219\n",
      "Train Epoch: 94 [187392/225000 (83%)] Loss: 18996.789062\n",
      "Train Epoch: 94 [189888/225000 (84%)] Loss: 19161.695312\n",
      "Train Epoch: 94 [192384/225000 (86%)] Loss: 19507.019531\n",
      "Train Epoch: 94 [194880/225000 (87%)] Loss: 19206.101562\n",
      "Train Epoch: 94 [197376/225000 (88%)] Loss: 19618.308594\n",
      "Train Epoch: 94 [199872/225000 (89%)] Loss: 19101.214844\n",
      "Train Epoch: 94 [202368/225000 (90%)] Loss: 19576.871094\n",
      "Train Epoch: 94 [204864/225000 (91%)] Loss: 19815.128906\n",
      "Train Epoch: 94 [207360/225000 (92%)] Loss: 19292.558594\n",
      "Train Epoch: 94 [209856/225000 (93%)] Loss: 19703.955078\n",
      "Train Epoch: 94 [212352/225000 (94%)] Loss: 19717.982422\n",
      "Train Epoch: 94 [214848/225000 (95%)] Loss: 19792.058594\n",
      "Train Epoch: 94 [217344/225000 (97%)] Loss: 19580.785156\n",
      "Train Epoch: 94 [219840/225000 (98%)] Loss: 19536.916016\n",
      "Train Epoch: 94 [222336/225000 (99%)] Loss: 19215.656250\n",
      "Train Epoch: 94 [224832/225000 (100%)] Loss: 20095.000000\n",
      "    epoch          : 94\n",
      "    loss           : 19544.61091650224\n",
      "    val_loss       : 19510.66735934119\n",
      "Train Epoch: 95 [192/225000 (0%)] Loss: 20026.156250\n",
      "Train Epoch: 95 [2688/225000 (1%)] Loss: 19869.453125\n",
      "Train Epoch: 95 [5184/225000 (2%)] Loss: 19261.697266\n",
      "Train Epoch: 95 [7680/225000 (3%)] Loss: 19463.046875\n",
      "Train Epoch: 95 [10176/225000 (5%)] Loss: 20067.376953\n",
      "Train Epoch: 95 [12672/225000 (6%)] Loss: 19476.302734\n",
      "Train Epoch: 95 [15168/225000 (7%)] Loss: 20068.765625\n",
      "Train Epoch: 95 [17664/225000 (8%)] Loss: 19560.070312\n",
      "Train Epoch: 95 [20160/225000 (9%)] Loss: 19493.685547\n",
      "Train Epoch: 95 [22656/225000 (10%)] Loss: 19443.570312\n",
      "Train Epoch: 95 [25152/225000 (11%)] Loss: 19488.714844\n",
      "Train Epoch: 95 [27648/225000 (12%)] Loss: 19034.777344\n",
      "Train Epoch: 95 [30144/225000 (13%)] Loss: 19502.015625\n",
      "Train Epoch: 95 [32640/225000 (15%)] Loss: 19685.890625\n",
      "Train Epoch: 95 [35136/225000 (16%)] Loss: 19379.136719\n",
      "Train Epoch: 95 [37632/225000 (17%)] Loss: 19729.531250\n",
      "Train Epoch: 95 [40128/225000 (18%)] Loss: 19749.011719\n",
      "Train Epoch: 95 [42624/225000 (19%)] Loss: 19499.046875\n",
      "Train Epoch: 95 [45120/225000 (20%)] Loss: 19632.464844\n",
      "Train Epoch: 95 [47616/225000 (21%)] Loss: 19682.164062\n",
      "Train Epoch: 95 [50112/225000 (22%)] Loss: 19079.300781\n",
      "Train Epoch: 95 [52608/225000 (23%)] Loss: 19341.910156\n",
      "Train Epoch: 95 [55104/225000 (24%)] Loss: 19641.656250\n",
      "Train Epoch: 95 [57600/225000 (26%)] Loss: 19817.507812\n",
      "Train Epoch: 95 [60096/225000 (27%)] Loss: 19948.330078\n",
      "Train Epoch: 95 [62592/225000 (28%)] Loss: 19006.935547\n",
      "Train Epoch: 95 [65088/225000 (29%)] Loss: 19056.994141\n",
      "Train Epoch: 95 [67584/225000 (30%)] Loss: 19887.406250\n",
      "Train Epoch: 95 [70080/225000 (31%)] Loss: 19175.906250\n",
      "Train Epoch: 95 [72576/225000 (32%)] Loss: 19289.781250\n",
      "Train Epoch: 95 [75072/225000 (33%)] Loss: 19401.074219\n",
      "Train Epoch: 95 [77568/225000 (34%)] Loss: 19753.007812\n",
      "Train Epoch: 95 [80064/225000 (36%)] Loss: 19532.921875\n",
      "Train Epoch: 95 [82560/225000 (37%)] Loss: 19285.183594\n",
      "Train Epoch: 95 [85056/225000 (38%)] Loss: 19445.402344\n",
      "Train Epoch: 95 [87552/225000 (39%)] Loss: 19676.750000\n",
      "Train Epoch: 95 [90048/225000 (40%)] Loss: 19676.519531\n",
      "Train Epoch: 95 [92544/225000 (41%)] Loss: 19898.421875\n",
      "Train Epoch: 95 [95040/225000 (42%)] Loss: 19416.218750\n",
      "Train Epoch: 95 [97536/225000 (43%)] Loss: 19286.386719\n",
      "Train Epoch: 95 [100032/225000 (44%)] Loss: 19543.429688\n",
      "Train Epoch: 95 [102528/225000 (46%)] Loss: 19281.441406\n",
      "Train Epoch: 95 [105024/225000 (47%)] Loss: 19850.294922\n",
      "Train Epoch: 95 [107520/225000 (48%)] Loss: 19481.593750\n",
      "Train Epoch: 95 [110016/225000 (49%)] Loss: 19508.191406\n",
      "Train Epoch: 95 [112512/225000 (50%)] Loss: 19578.324219\n",
      "Train Epoch: 95 [115008/225000 (51%)] Loss: 20002.539062\n",
      "Train Epoch: 95 [117504/225000 (52%)] Loss: 19514.769531\n",
      "Train Epoch: 95 [120000/225000 (53%)] Loss: 19574.658203\n",
      "Train Epoch: 95 [122496/225000 (54%)] Loss: 19323.818359\n",
      "Train Epoch: 95 [124992/225000 (56%)] Loss: 19775.390625\n",
      "Train Epoch: 95 [127488/225000 (57%)] Loss: 19721.308594\n",
      "Train Epoch: 95 [129984/225000 (58%)] Loss: 19627.447266\n",
      "Train Epoch: 95 [132480/225000 (59%)] Loss: 19525.773438\n",
      "Train Epoch: 95 [134976/225000 (60%)] Loss: 19178.400391\n",
      "Train Epoch: 95 [137472/225000 (61%)] Loss: 19045.949219\n",
      "Train Epoch: 95 [139968/225000 (62%)] Loss: 19096.070312\n",
      "Train Epoch: 95 [142464/225000 (63%)] Loss: 20267.320312\n",
      "Train Epoch: 95 [144960/225000 (64%)] Loss: 19811.332031\n",
      "Train Epoch: 95 [147456/225000 (66%)] Loss: 19368.773438\n",
      "Train Epoch: 95 [149952/225000 (67%)] Loss: 19427.007812\n",
      "Train Epoch: 95 [152448/225000 (68%)] Loss: 19663.628906\n",
      "Train Epoch: 95 [154944/225000 (69%)] Loss: 19650.390625\n",
      "Train Epoch: 95 [157440/225000 (70%)] Loss: 19359.748047\n",
      "Train Epoch: 95 [159936/225000 (71%)] Loss: 19485.710938\n",
      "Train Epoch: 95 [162432/225000 (72%)] Loss: 19683.689453\n",
      "Train Epoch: 95 [164928/225000 (73%)] Loss: 19789.660156\n",
      "Train Epoch: 95 [167424/225000 (74%)] Loss: 20054.666016\n",
      "Train Epoch: 95 [169920/225000 (76%)] Loss: 19541.410156\n",
      "Train Epoch: 95 [172416/225000 (77%)] Loss: 19789.382812\n",
      "Train Epoch: 95 [174912/225000 (78%)] Loss: 19232.050781\n",
      "Train Epoch: 95 [177408/225000 (79%)] Loss: 19498.097656\n",
      "Train Epoch: 95 [179904/225000 (80%)] Loss: 19467.974609\n",
      "Train Epoch: 95 [182400/225000 (81%)] Loss: 20070.535156\n",
      "Train Epoch: 95 [184896/225000 (82%)] Loss: 19459.705078\n",
      "Train Epoch: 95 [187392/225000 (83%)] Loss: 19718.187500\n",
      "Train Epoch: 95 [189888/225000 (84%)] Loss: 19683.667969\n",
      "Train Epoch: 95 [192384/225000 (86%)] Loss: 19379.734375\n",
      "Train Epoch: 95 [194880/225000 (87%)] Loss: 19493.509766\n",
      "Train Epoch: 95 [197376/225000 (88%)] Loss: 19985.203125\n",
      "Train Epoch: 95 [199872/225000 (89%)] Loss: 19295.250000\n",
      "Train Epoch: 95 [202368/225000 (90%)] Loss: 19396.414062\n",
      "Train Epoch: 95 [204864/225000 (91%)] Loss: 19398.289062\n",
      "Train Epoch: 95 [207360/225000 (92%)] Loss: 19258.941406\n",
      "Train Epoch: 95 [209856/225000 (93%)] Loss: 19465.019531\n",
      "Train Epoch: 95 [212352/225000 (94%)] Loss: 19535.869141\n",
      "Train Epoch: 95 [214848/225000 (95%)] Loss: 19478.326172\n",
      "Train Epoch: 95 [217344/225000 (97%)] Loss: 19389.753906\n",
      "Train Epoch: 95 [219840/225000 (98%)] Loss: 19545.234375\n",
      "Train Epoch: 95 [222336/225000 (99%)] Loss: 19521.781250\n",
      "Train Epoch: 95 [224832/225000 (100%)] Loss: 19836.750000\n",
      "    epoch          : 95\n",
      "    loss           : 19535.296715017066\n",
      "    val_loss       : 19429.0050721578\n",
      "Train Epoch: 96 [192/225000 (0%)] Loss: 19516.763672\n",
      "Train Epoch: 96 [2688/225000 (1%)] Loss: 19563.373047\n",
      "Train Epoch: 96 [5184/225000 (2%)] Loss: 20086.660156\n",
      "Train Epoch: 96 [7680/225000 (3%)] Loss: 19341.593750\n",
      "Train Epoch: 96 [10176/225000 (5%)] Loss: 19156.734375\n",
      "Train Epoch: 96 [12672/225000 (6%)] Loss: 19434.292969\n",
      "Train Epoch: 96 [15168/225000 (7%)] Loss: 19462.996094\n",
      "Train Epoch: 96 [17664/225000 (8%)] Loss: 19409.937500\n",
      "Train Epoch: 96 [20160/225000 (9%)] Loss: 19521.636719\n",
      "Train Epoch: 96 [22656/225000 (10%)] Loss: 19768.507812\n",
      "Train Epoch: 96 [25152/225000 (11%)] Loss: 19817.140625\n",
      "Train Epoch: 96 [27648/225000 (12%)] Loss: 19134.798828\n",
      "Train Epoch: 96 [30144/225000 (13%)] Loss: 19311.949219\n",
      "Train Epoch: 96 [32640/225000 (15%)] Loss: 20000.345703\n",
      "Train Epoch: 96 [35136/225000 (16%)] Loss: 19382.480469\n",
      "Train Epoch: 96 [37632/225000 (17%)] Loss: 19561.847656\n",
      "Train Epoch: 96 [40128/225000 (18%)] Loss: 19236.244141\n",
      "Train Epoch: 96 [42624/225000 (19%)] Loss: 19666.337891\n",
      "Train Epoch: 96 [45120/225000 (20%)] Loss: 19504.085938\n",
      "Train Epoch: 96 [47616/225000 (21%)] Loss: 19614.119141\n",
      "Train Epoch: 96 [50112/225000 (22%)] Loss: 19506.304688\n",
      "Train Epoch: 96 [52608/225000 (23%)] Loss: 19431.417969\n",
      "Train Epoch: 96 [55104/225000 (24%)] Loss: 19801.568359\n",
      "Train Epoch: 96 [57600/225000 (26%)] Loss: 19684.751953\n",
      "Train Epoch: 96 [60096/225000 (27%)] Loss: 19683.003906\n",
      "Train Epoch: 96 [62592/225000 (28%)] Loss: 19568.523438\n",
      "Train Epoch: 96 [65088/225000 (29%)] Loss: 19560.125000\n",
      "Train Epoch: 96 [67584/225000 (30%)] Loss: 19549.242188\n",
      "Train Epoch: 96 [70080/225000 (31%)] Loss: 19794.894531\n",
      "Train Epoch: 96 [72576/225000 (32%)] Loss: 19116.523438\n",
      "Train Epoch: 96 [75072/225000 (33%)] Loss: 19701.839844\n",
      "Train Epoch: 96 [77568/225000 (34%)] Loss: 19507.687500\n",
      "Train Epoch: 96 [80064/225000 (36%)] Loss: 19601.050781\n",
      "Train Epoch: 96 [82560/225000 (37%)] Loss: 19267.529297\n",
      "Train Epoch: 96 [85056/225000 (38%)] Loss: 19417.412109\n",
      "Train Epoch: 96 [87552/225000 (39%)] Loss: 19835.412109\n",
      "Train Epoch: 96 [90048/225000 (40%)] Loss: 19642.527344\n",
      "Train Epoch: 96 [92544/225000 (41%)] Loss: 19733.042969\n",
      "Train Epoch: 96 [95040/225000 (42%)] Loss: 19641.070312\n",
      "Train Epoch: 96 [97536/225000 (43%)] Loss: 19471.523438\n",
      "Train Epoch: 96 [100032/225000 (44%)] Loss: 19427.761719\n",
      "Train Epoch: 96 [102528/225000 (46%)] Loss: 19845.484375\n",
      "Train Epoch: 96 [105024/225000 (47%)] Loss: 19516.808594\n",
      "Train Epoch: 96 [107520/225000 (48%)] Loss: 19620.677734\n",
      "Train Epoch: 96 [110016/225000 (49%)] Loss: 19314.730469\n",
      "Train Epoch: 96 [112512/225000 (50%)] Loss: 19087.150391\n",
      "Train Epoch: 96 [115008/225000 (51%)] Loss: 19418.763672\n",
      "Train Epoch: 96 [117504/225000 (52%)] Loss: 19764.937500\n",
      "Train Epoch: 96 [120000/225000 (53%)] Loss: 19816.818359\n",
      "Train Epoch: 96 [122496/225000 (54%)] Loss: 19317.496094\n",
      "Train Epoch: 96 [124992/225000 (56%)] Loss: 19077.410156\n",
      "Train Epoch: 96 [127488/225000 (57%)] Loss: 19300.011719\n",
      "Train Epoch: 96 [129984/225000 (58%)] Loss: 19570.023438\n",
      "Train Epoch: 96 [132480/225000 (59%)] Loss: 19349.777344\n",
      "Train Epoch: 96 [134976/225000 (60%)] Loss: 19545.832031\n",
      "Train Epoch: 96 [137472/225000 (61%)] Loss: 19706.105469\n",
      "Train Epoch: 96 [139968/225000 (62%)] Loss: 19142.986328\n",
      "Train Epoch: 96 [142464/225000 (63%)] Loss: 19677.171875\n",
      "Train Epoch: 96 [144960/225000 (64%)] Loss: 19620.435547\n",
      "Train Epoch: 96 [147456/225000 (66%)] Loss: 19457.820312\n",
      "Train Epoch: 96 [149952/225000 (67%)] Loss: 19766.394531\n",
      "Train Epoch: 96 [152448/225000 (68%)] Loss: 19637.552734\n",
      "Train Epoch: 96 [154944/225000 (69%)] Loss: 19425.031250\n",
      "Train Epoch: 96 [157440/225000 (70%)] Loss: 19197.919922\n",
      "Train Epoch: 96 [159936/225000 (71%)] Loss: 19742.210938\n",
      "Train Epoch: 96 [162432/225000 (72%)] Loss: 19378.960938\n",
      "Train Epoch: 96 [164928/225000 (73%)] Loss: 19726.046875\n",
      "Train Epoch: 96 [167424/225000 (74%)] Loss: 19426.320312\n",
      "Train Epoch: 96 [169920/225000 (76%)] Loss: 19417.283203\n",
      "Train Epoch: 96 [172416/225000 (77%)] Loss: 19140.986328\n",
      "Train Epoch: 96 [174912/225000 (78%)] Loss: 20053.648438\n",
      "Train Epoch: 96 [177408/225000 (79%)] Loss: 19410.601562\n",
      "Train Epoch: 96 [179904/225000 (80%)] Loss: 19510.626953\n",
      "Train Epoch: 96 [182400/225000 (81%)] Loss: 19530.011719\n",
      "Train Epoch: 96 [184896/225000 (82%)] Loss: 19248.220703\n",
      "Train Epoch: 96 [187392/225000 (83%)] Loss: 19806.228516\n",
      "Train Epoch: 96 [189888/225000 (84%)] Loss: 19606.546875\n",
      "Train Epoch: 96 [192384/225000 (86%)] Loss: 19431.246094\n",
      "Train Epoch: 96 [194880/225000 (87%)] Loss: 19893.318359\n",
      "Train Epoch: 96 [197376/225000 (88%)] Loss: 19731.189453\n",
      "Train Epoch: 96 [199872/225000 (89%)] Loss: 19152.927734\n",
      "Train Epoch: 96 [202368/225000 (90%)] Loss: 19193.699219\n",
      "Train Epoch: 96 [204864/225000 (91%)] Loss: 19548.527344\n",
      "Train Epoch: 96 [207360/225000 (92%)] Loss: 19294.519531\n",
      "Train Epoch: 96 [209856/225000 (93%)] Loss: 19789.652344\n",
      "Train Epoch: 96 [212352/225000 (94%)] Loss: 19541.810547\n",
      "Train Epoch: 96 [214848/225000 (95%)] Loss: 18928.818359\n",
      "Train Epoch: 96 [217344/225000 (97%)] Loss: 19536.130859\n",
      "Train Epoch: 96 [219840/225000 (98%)] Loss: 19359.023438\n",
      "Train Epoch: 96 [222336/225000 (99%)] Loss: 19349.710938\n",
      "Train Epoch: 96 [224832/225000 (100%)] Loss: 19605.101562\n",
      "    epoch          : 96\n",
      "    loss           : 19528.649344069967\n",
      "    val_loss       : 19443.82794223578\n",
      "Train Epoch: 97 [192/225000 (0%)] Loss: 19385.853516\n",
      "Train Epoch: 97 [2688/225000 (1%)] Loss: 19750.416016\n",
      "Train Epoch: 97 [5184/225000 (2%)] Loss: 19162.585938\n",
      "Train Epoch: 97 [7680/225000 (3%)] Loss: 19449.158203\n",
      "Train Epoch: 97 [10176/225000 (5%)] Loss: 19447.980469\n",
      "Train Epoch: 97 [12672/225000 (6%)] Loss: 19518.111328\n",
      "Train Epoch: 97 [15168/225000 (7%)] Loss: 19468.566406\n",
      "Train Epoch: 97 [17664/225000 (8%)] Loss: 19467.277344\n",
      "Train Epoch: 97 [20160/225000 (9%)] Loss: 19285.402344\n",
      "Train Epoch: 97 [22656/225000 (10%)] Loss: 19641.824219\n",
      "Train Epoch: 97 [25152/225000 (11%)] Loss: 19619.707031\n",
      "Train Epoch: 97 [27648/225000 (12%)] Loss: 19495.333984\n",
      "Train Epoch: 97 [30144/225000 (13%)] Loss: 19360.048828\n",
      "Train Epoch: 97 [32640/225000 (15%)] Loss: 19036.316406\n",
      "Train Epoch: 97 [35136/225000 (16%)] Loss: 19559.058594\n",
      "Train Epoch: 97 [37632/225000 (17%)] Loss: 19084.113281\n",
      "Train Epoch: 97 [40128/225000 (18%)] Loss: 19368.843750\n",
      "Train Epoch: 97 [42624/225000 (19%)] Loss: 19001.199219\n",
      "Train Epoch: 97 [45120/225000 (20%)] Loss: 19671.132812\n",
      "Train Epoch: 97 [47616/225000 (21%)] Loss: 19306.042969\n",
      "Train Epoch: 97 [50112/225000 (22%)] Loss: 19344.195312\n",
      "Train Epoch: 97 [52608/225000 (23%)] Loss: 19034.347656\n",
      "Train Epoch: 97 [55104/225000 (24%)] Loss: 19652.537109\n",
      "Train Epoch: 97 [57600/225000 (26%)] Loss: 19086.427734\n",
      "Train Epoch: 97 [60096/225000 (27%)] Loss: 19359.296875\n",
      "Train Epoch: 97 [62592/225000 (28%)] Loss: 19711.140625\n",
      "Train Epoch: 97 [65088/225000 (29%)] Loss: 19657.628906\n",
      "Train Epoch: 97 [67584/225000 (30%)] Loss: 19387.937500\n",
      "Train Epoch: 97 [70080/225000 (31%)] Loss: 19487.443359\n",
      "Train Epoch: 97 [72576/225000 (32%)] Loss: 19179.160156\n",
      "Train Epoch: 97 [75072/225000 (33%)] Loss: 19600.777344\n",
      "Train Epoch: 97 [77568/225000 (34%)] Loss: 19404.007812\n",
      "Train Epoch: 97 [80064/225000 (36%)] Loss: 18781.863281\n",
      "Train Epoch: 97 [82560/225000 (37%)] Loss: 18976.308594\n",
      "Train Epoch: 97 [85056/225000 (38%)] Loss: 19764.646484\n",
      "Train Epoch: 97 [87552/225000 (39%)] Loss: 19362.496094\n",
      "Train Epoch: 97 [90048/225000 (40%)] Loss: 19308.402344\n",
      "Train Epoch: 97 [92544/225000 (41%)] Loss: 19306.347656\n",
      "Train Epoch: 97 [95040/225000 (42%)] Loss: 19537.625000\n",
      "Train Epoch: 97 [97536/225000 (43%)] Loss: 19796.537109\n",
      "Train Epoch: 97 [100032/225000 (44%)] Loss: 19650.324219\n",
      "Train Epoch: 97 [102528/225000 (46%)] Loss: 19455.742188\n",
      "Train Epoch: 97 [105024/225000 (47%)] Loss: 19957.527344\n",
      "Train Epoch: 97 [107520/225000 (48%)] Loss: 19484.224609\n",
      "Train Epoch: 97 [110016/225000 (49%)] Loss: 19692.300781\n",
      "Train Epoch: 97 [112512/225000 (50%)] Loss: 19785.914062\n",
      "Train Epoch: 97 [115008/225000 (51%)] Loss: 19343.500000\n",
      "Train Epoch: 97 [117504/225000 (52%)] Loss: 19564.902344\n",
      "Train Epoch: 97 [120000/225000 (53%)] Loss: 19584.794922\n",
      "Train Epoch: 97 [122496/225000 (54%)] Loss: 19758.542969\n",
      "Train Epoch: 97 [124992/225000 (56%)] Loss: 19664.328125\n",
      "Train Epoch: 97 [127488/225000 (57%)] Loss: 19122.679688\n",
      "Train Epoch: 97 [129984/225000 (58%)] Loss: 19579.199219\n",
      "Train Epoch: 97 [132480/225000 (59%)] Loss: 19932.300781\n",
      "Train Epoch: 97 [134976/225000 (60%)] Loss: 19317.328125\n",
      "Train Epoch: 97 [137472/225000 (61%)] Loss: 19274.261719\n",
      "Train Epoch: 97 [139968/225000 (62%)] Loss: 19484.320312\n",
      "Train Epoch: 97 [142464/225000 (63%)] Loss: 19571.039062\n",
      "Train Epoch: 97 [144960/225000 (64%)] Loss: 19485.421875\n",
      "Train Epoch: 97 [147456/225000 (66%)] Loss: 19499.626953\n",
      "Train Epoch: 97 [149952/225000 (67%)] Loss: 20147.355469\n",
      "Train Epoch: 97 [152448/225000 (68%)] Loss: 19459.578125\n",
      "Train Epoch: 97 [154944/225000 (69%)] Loss: 19571.023438\n",
      "Train Epoch: 97 [157440/225000 (70%)] Loss: 19434.324219\n",
      "Train Epoch: 97 [159936/225000 (71%)] Loss: 19490.361328\n",
      "Train Epoch: 97 [162432/225000 (72%)] Loss: 19699.888672\n",
      "Train Epoch: 97 [164928/225000 (73%)] Loss: 19862.695312\n",
      "Train Epoch: 97 [167424/225000 (74%)] Loss: 19749.222656\n",
      "Train Epoch: 97 [169920/225000 (76%)] Loss: 19457.042969\n",
      "Train Epoch: 97 [172416/225000 (77%)] Loss: 19733.750000\n",
      "Train Epoch: 97 [174912/225000 (78%)] Loss: 19016.285156\n",
      "Train Epoch: 97 [177408/225000 (79%)] Loss: 19265.984375\n",
      "Train Epoch: 97 [179904/225000 (80%)] Loss: 19401.562500\n",
      "Train Epoch: 97 [182400/225000 (81%)] Loss: 19219.390625\n",
      "Train Epoch: 97 [184896/225000 (82%)] Loss: 19630.185547\n",
      "Train Epoch: 97 [187392/225000 (83%)] Loss: 19582.320312\n",
      "Train Epoch: 97 [189888/225000 (84%)] Loss: 19468.812500\n",
      "Train Epoch: 97 [192384/225000 (86%)] Loss: 19685.341797\n",
      "Train Epoch: 97 [194880/225000 (87%)] Loss: 19861.035156\n",
      "Train Epoch: 97 [197376/225000 (88%)] Loss: 19253.966797\n",
      "Train Epoch: 97 [199872/225000 (89%)] Loss: 19094.378906\n",
      "Train Epoch: 97 [202368/225000 (90%)] Loss: 19821.191406\n",
      "Train Epoch: 97 [204864/225000 (91%)] Loss: 20193.138672\n",
      "Train Epoch: 97 [207360/225000 (92%)] Loss: 19350.691406\n",
      "Train Epoch: 97 [209856/225000 (93%)] Loss: 19615.232422\n",
      "Train Epoch: 97 [212352/225000 (94%)] Loss: 19153.966797\n",
      "Train Epoch: 97 [214848/225000 (95%)] Loss: 19066.402344\n",
      "Train Epoch: 97 [217344/225000 (97%)] Loss: 19628.589844\n",
      "Train Epoch: 97 [219840/225000 (98%)] Loss: 19276.406250\n",
      "Train Epoch: 97 [222336/225000 (99%)] Loss: 19258.195312\n",
      "Train Epoch: 97 [224832/225000 (100%)] Loss: 19360.339844\n",
      "    epoch          : 97\n",
      "    loss           : 19516.722711244132\n",
      "    val_loss       : 19412.407438615806\n",
      "Train Epoch: 98 [192/225000 (0%)] Loss: 19165.921875\n",
      "Train Epoch: 98 [2688/225000 (1%)] Loss: 19373.101562\n",
      "Train Epoch: 98 [5184/225000 (2%)] Loss: 19378.363281\n",
      "Train Epoch: 98 [7680/225000 (3%)] Loss: 19754.576172\n",
      "Train Epoch: 98 [10176/225000 (5%)] Loss: 19346.968750\n",
      "Train Epoch: 98 [12672/225000 (6%)] Loss: 19121.640625\n",
      "Train Epoch: 98 [15168/225000 (7%)] Loss: 19258.292969\n",
      "Train Epoch: 98 [17664/225000 (8%)] Loss: 19217.974609\n",
      "Train Epoch: 98 [20160/225000 (9%)] Loss: 19832.960938\n",
      "Train Epoch: 98 [22656/225000 (10%)] Loss: 19522.914062\n",
      "Train Epoch: 98 [25152/225000 (11%)] Loss: 19479.316406\n",
      "Train Epoch: 98 [27648/225000 (12%)] Loss: 19123.025391\n",
      "Train Epoch: 98 [30144/225000 (13%)] Loss: 19631.296875\n",
      "Train Epoch: 98 [32640/225000 (15%)] Loss: 19379.851562\n",
      "Train Epoch: 98 [35136/225000 (16%)] Loss: 20008.394531\n",
      "Train Epoch: 98 [37632/225000 (17%)] Loss: 19355.587891\n",
      "Train Epoch: 98 [40128/225000 (18%)] Loss: 19327.742188\n",
      "Train Epoch: 98 [42624/225000 (19%)] Loss: 19349.636719\n",
      "Train Epoch: 98 [45120/225000 (20%)] Loss: 19604.957031\n",
      "Train Epoch: 98 [47616/225000 (21%)] Loss: 19423.156250\n",
      "Train Epoch: 98 [50112/225000 (22%)] Loss: 19939.160156\n",
      "Train Epoch: 98 [52608/225000 (23%)] Loss: 19559.324219\n",
      "Train Epoch: 98 [55104/225000 (24%)] Loss: 19156.472656\n",
      "Train Epoch: 98 [57600/225000 (26%)] Loss: 18804.738281\n",
      "Train Epoch: 98 [60096/225000 (27%)] Loss: 19807.537109\n",
      "Train Epoch: 98 [62592/225000 (28%)] Loss: 19698.804688\n",
      "Train Epoch: 98 [65088/225000 (29%)] Loss: 19570.093750\n",
      "Train Epoch: 98 [67584/225000 (30%)] Loss: 19436.019531\n",
      "Train Epoch: 98 [70080/225000 (31%)] Loss: 19441.625000\n",
      "Train Epoch: 98 [72576/225000 (32%)] Loss: 20115.269531\n",
      "Train Epoch: 98 [75072/225000 (33%)] Loss: 19636.109375\n",
      "Train Epoch: 98 [77568/225000 (34%)] Loss: 19181.027344\n",
      "Train Epoch: 98 [80064/225000 (36%)] Loss: 19602.828125\n",
      "Train Epoch: 98 [82560/225000 (37%)] Loss: 19382.281250\n",
      "Train Epoch: 98 [85056/225000 (38%)] Loss: 19821.761719\n",
      "Train Epoch: 98 [87552/225000 (39%)] Loss: 19773.279297\n",
      "Train Epoch: 98 [90048/225000 (40%)] Loss: 19377.453125\n",
      "Train Epoch: 98 [92544/225000 (41%)] Loss: 19244.753906\n",
      "Train Epoch: 98 [95040/225000 (42%)] Loss: 19531.183594\n",
      "Train Epoch: 98 [97536/225000 (43%)] Loss: 19199.199219\n",
      "Train Epoch: 98 [100032/225000 (44%)] Loss: 19428.369141\n",
      "Train Epoch: 98 [102528/225000 (46%)] Loss: 19078.734375\n",
      "Train Epoch: 98 [105024/225000 (47%)] Loss: 19362.070312\n",
      "Train Epoch: 98 [107520/225000 (48%)] Loss: 18865.035156\n",
      "Train Epoch: 98 [110016/225000 (49%)] Loss: 19267.755859\n",
      "Train Epoch: 98 [112512/225000 (50%)] Loss: 18481.757812\n",
      "Train Epoch: 98 [115008/225000 (51%)] Loss: 19268.367188\n",
      "Train Epoch: 98 [117504/225000 (52%)] Loss: 19460.251953\n",
      "Train Epoch: 98 [120000/225000 (53%)] Loss: 19728.466797\n",
      "Train Epoch: 98 [122496/225000 (54%)] Loss: 19778.642578\n",
      "Train Epoch: 98 [124992/225000 (56%)] Loss: 19662.015625\n",
      "Train Epoch: 98 [127488/225000 (57%)] Loss: 19771.371094\n",
      "Train Epoch: 98 [129984/225000 (58%)] Loss: 19883.285156\n",
      "Train Epoch: 98 [132480/225000 (59%)] Loss: 18827.853516\n",
      "Train Epoch: 98 [134976/225000 (60%)] Loss: 19436.417969\n",
      "Train Epoch: 98 [137472/225000 (61%)] Loss: 19685.617188\n",
      "Train Epoch: 98 [139968/225000 (62%)] Loss: 19594.058594\n",
      "Train Epoch: 98 [142464/225000 (63%)] Loss: 19232.585938\n",
      "Train Epoch: 98 [144960/225000 (64%)] Loss: 19160.839844\n",
      "Train Epoch: 98 [147456/225000 (66%)] Loss: 19756.628906\n",
      "Train Epoch: 98 [149952/225000 (67%)] Loss: 19400.128906\n",
      "Train Epoch: 98 [152448/225000 (68%)] Loss: 19493.195312\n",
      "Train Epoch: 98 [154944/225000 (69%)] Loss: 19686.222656\n",
      "Train Epoch: 98 [157440/225000 (70%)] Loss: 19744.523438\n",
      "Train Epoch: 98 [159936/225000 (71%)] Loss: 19464.277344\n",
      "Train Epoch: 98 [162432/225000 (72%)] Loss: 19819.902344\n",
      "Train Epoch: 98 [164928/225000 (73%)] Loss: 19441.769531\n",
      "Train Epoch: 98 [167424/225000 (74%)] Loss: 19352.308594\n",
      "Train Epoch: 98 [169920/225000 (76%)] Loss: 19351.601562\n",
      "Train Epoch: 98 [172416/225000 (77%)] Loss: 19626.453125\n",
      "Train Epoch: 98 [174912/225000 (78%)] Loss: 19471.503906\n",
      "Train Epoch: 98 [177408/225000 (79%)] Loss: 19388.095703\n",
      "Train Epoch: 98 [179904/225000 (80%)] Loss: 19440.472656\n",
      "Train Epoch: 98 [182400/225000 (81%)] Loss: 19387.265625\n",
      "Train Epoch: 98 [184896/225000 (82%)] Loss: 19376.792969\n",
      "Train Epoch: 98 [187392/225000 (83%)] Loss: 19623.890625\n",
      "Train Epoch: 98 [189888/225000 (84%)] Loss: 19225.664062\n",
      "Train Epoch: 98 [192384/225000 (86%)] Loss: 19646.398438\n",
      "Train Epoch: 98 [194880/225000 (87%)] Loss: 19693.843750\n",
      "Train Epoch: 98 [197376/225000 (88%)] Loss: 19715.361328\n",
      "Train Epoch: 98 [199872/225000 (89%)] Loss: 19632.882812\n",
      "Train Epoch: 98 [202368/225000 (90%)] Loss: 19499.357422\n",
      "Train Epoch: 98 [204864/225000 (91%)] Loss: 19426.285156\n",
      "Train Epoch: 98 [207360/225000 (92%)] Loss: 19481.050781\n",
      "Train Epoch: 98 [209856/225000 (93%)] Loss: 19476.000000\n",
      "Train Epoch: 98 [212352/225000 (94%)] Loss: 19601.976562\n",
      "Train Epoch: 98 [214848/225000 (95%)] Loss: 19355.835938\n",
      "Train Epoch: 98 [217344/225000 (97%)] Loss: 19203.601562\n",
      "Train Epoch: 98 [219840/225000 (98%)] Loss: 19747.296875\n",
      "Train Epoch: 98 [222336/225000 (99%)] Loss: 19453.107422\n",
      "Train Epoch: 98 [224832/225000 (100%)] Loss: 19247.978516\n",
      "    epoch          : 98\n",
      "    loss           : 19520.043865321033\n",
      "    val_loss       : 19409.7286094418\n",
      "Train Epoch: 99 [192/225000 (0%)] Loss: 19373.677734\n",
      "Train Epoch: 99 [2688/225000 (1%)] Loss: 19688.349609\n",
      "Train Epoch: 99 [5184/225000 (2%)] Loss: 19900.468750\n",
      "Train Epoch: 99 [7680/225000 (3%)] Loss: 19528.113281\n",
      "Train Epoch: 99 [10176/225000 (5%)] Loss: 19680.671875\n",
      "Train Epoch: 99 [12672/225000 (6%)] Loss: 19581.417969\n",
      "Train Epoch: 99 [15168/225000 (7%)] Loss: 19037.257812\n",
      "Train Epoch: 99 [17664/225000 (8%)] Loss: 19764.759766\n",
      "Train Epoch: 99 [20160/225000 (9%)] Loss: 20276.236328\n",
      "Train Epoch: 99 [22656/225000 (10%)] Loss: 19136.119141\n",
      "Train Epoch: 99 [25152/225000 (11%)] Loss: 19771.558594\n",
      "Train Epoch: 99 [27648/225000 (12%)] Loss: 19370.380859\n",
      "Train Epoch: 99 [30144/225000 (13%)] Loss: 18883.792969\n",
      "Train Epoch: 99 [32640/225000 (15%)] Loss: 19915.820312\n",
      "Train Epoch: 99 [35136/225000 (16%)] Loss: 19842.748047\n",
      "Train Epoch: 99 [37632/225000 (17%)] Loss: 19655.320312\n",
      "Train Epoch: 99 [40128/225000 (18%)] Loss: 19801.232422\n",
      "Train Epoch: 99 [42624/225000 (19%)] Loss: 19587.595703\n",
      "Train Epoch: 99 [45120/225000 (20%)] Loss: 18988.566406\n",
      "Train Epoch: 99 [47616/225000 (21%)] Loss: 19903.808594\n",
      "Train Epoch: 99 [50112/225000 (22%)] Loss: 19287.148438\n",
      "Train Epoch: 99 [52608/225000 (23%)] Loss: 19150.863281\n",
      "Train Epoch: 99 [55104/225000 (24%)] Loss: 20006.773438\n",
      "Train Epoch: 99 [57600/225000 (26%)] Loss: 20066.542969\n",
      "Train Epoch: 99 [60096/225000 (27%)] Loss: 19676.417969\n",
      "Train Epoch: 99 [62592/225000 (28%)] Loss: 19990.941406\n",
      "Train Epoch: 99 [65088/225000 (29%)] Loss: 19367.126953\n",
      "Train Epoch: 99 [67584/225000 (30%)] Loss: 19572.820312\n",
      "Train Epoch: 99 [70080/225000 (31%)] Loss: 19542.281250\n",
      "Train Epoch: 99 [72576/225000 (32%)] Loss: 19623.445312\n",
      "Train Epoch: 99 [75072/225000 (33%)] Loss: 19781.226562\n",
      "Train Epoch: 99 [77568/225000 (34%)] Loss: 19595.242188\n",
      "Train Epoch: 99 [80064/225000 (36%)] Loss: 20168.560547\n",
      "Train Epoch: 99 [82560/225000 (37%)] Loss: 19942.117188\n",
      "Train Epoch: 99 [85056/225000 (38%)] Loss: 19486.113281\n",
      "Train Epoch: 99 [87552/225000 (39%)] Loss: 19845.931641\n",
      "Train Epoch: 99 [90048/225000 (40%)] Loss: 19258.640625\n",
      "Train Epoch: 99 [92544/225000 (41%)] Loss: 20008.140625\n",
      "Train Epoch: 99 [95040/225000 (42%)] Loss: 19716.238281\n",
      "Train Epoch: 99 [97536/225000 (43%)] Loss: 18971.132812\n",
      "Train Epoch: 99 [100032/225000 (44%)] Loss: 19590.767578\n",
      "Train Epoch: 99 [102528/225000 (46%)] Loss: 19843.945312\n",
      "Train Epoch: 99 [105024/225000 (47%)] Loss: 19434.964844\n",
      "Train Epoch: 99 [107520/225000 (48%)] Loss: 19526.742188\n",
      "Train Epoch: 99 [110016/225000 (49%)] Loss: 19605.878906\n",
      "Train Epoch: 99 [112512/225000 (50%)] Loss: 19638.822266\n",
      "Train Epoch: 99 [115008/225000 (51%)] Loss: 19426.843750\n",
      "Train Epoch: 99 [117504/225000 (52%)] Loss: 19968.378906\n",
      "Train Epoch: 99 [120000/225000 (53%)] Loss: 19590.046875\n",
      "Train Epoch: 99 [122496/225000 (54%)] Loss: 19918.531250\n",
      "Train Epoch: 99 [124992/225000 (56%)] Loss: 19095.117188\n",
      "Train Epoch: 99 [127488/225000 (57%)] Loss: 19386.824219\n",
      "Train Epoch: 99 [129984/225000 (58%)] Loss: 19359.312500\n",
      "Train Epoch: 99 [132480/225000 (59%)] Loss: 19791.824219\n",
      "Train Epoch: 99 [134976/225000 (60%)] Loss: 19544.683594\n",
      "Train Epoch: 99 [137472/225000 (61%)] Loss: 19600.781250\n",
      "Train Epoch: 99 [139968/225000 (62%)] Loss: 19002.539062\n",
      "Train Epoch: 99 [142464/225000 (63%)] Loss: 19970.962891\n",
      "Train Epoch: 99 [144960/225000 (64%)] Loss: 18979.566406\n",
      "Train Epoch: 99 [147456/225000 (66%)] Loss: 19684.613281\n",
      "Train Epoch: 99 [149952/225000 (67%)] Loss: 20218.417969\n",
      "Train Epoch: 99 [152448/225000 (68%)] Loss: 19368.451172\n",
      "Train Epoch: 99 [154944/225000 (69%)] Loss: 19540.304688\n",
      "Train Epoch: 99 [157440/225000 (70%)] Loss: 19332.437500\n",
      "Train Epoch: 99 [159936/225000 (71%)] Loss: 19559.789062\n",
      "Train Epoch: 99 [162432/225000 (72%)] Loss: 19114.339844\n",
      "Train Epoch: 99 [164928/225000 (73%)] Loss: 19557.968750\n",
      "Train Epoch: 99 [167424/225000 (74%)] Loss: 19145.347656\n",
      "Train Epoch: 99 [169920/225000 (76%)] Loss: 19760.964844\n",
      "Train Epoch: 99 [172416/225000 (77%)] Loss: 19554.544922\n",
      "Train Epoch: 99 [174912/225000 (78%)] Loss: 19818.285156\n",
      "Train Epoch: 99 [177408/225000 (79%)] Loss: 19814.425781\n",
      "Train Epoch: 99 [179904/225000 (80%)] Loss: 19824.835938\n",
      "Train Epoch: 99 [182400/225000 (81%)] Loss: 19781.570312\n",
      "Train Epoch: 99 [184896/225000 (82%)] Loss: 19166.980469\n",
      "Train Epoch: 99 [187392/225000 (83%)] Loss: 19058.517578\n",
      "Train Epoch: 99 [189888/225000 (84%)] Loss: 19228.687500\n",
      "Train Epoch: 99 [192384/225000 (86%)] Loss: 19252.490234\n",
      "Train Epoch: 99 [194880/225000 (87%)] Loss: 19431.142578\n",
      "Train Epoch: 99 [197376/225000 (88%)] Loss: 19545.386719\n",
      "Train Epoch: 99 [199872/225000 (89%)] Loss: 19973.828125\n",
      "Train Epoch: 99 [202368/225000 (90%)] Loss: 19370.265625\n",
      "Train Epoch: 99 [204864/225000 (91%)] Loss: 19635.091797\n",
      "Train Epoch: 99 [207360/225000 (92%)] Loss: 19481.607422\n",
      "Train Epoch: 99 [209856/225000 (93%)] Loss: 20022.328125\n",
      "Train Epoch: 99 [212352/225000 (94%)] Loss: 20128.503906\n",
      "Train Epoch: 99 [214848/225000 (95%)] Loss: 19634.445312\n",
      "Train Epoch: 99 [217344/225000 (97%)] Loss: 19826.875000\n",
      "Train Epoch: 99 [219840/225000 (98%)] Loss: 19844.097656\n",
      "Train Epoch: 99 [222336/225000 (99%)] Loss: 19364.546875\n",
      "Train Epoch: 99 [224832/225000 (100%)] Loss: 19292.218750\n",
      "    epoch          : 99\n",
      "    loss           : 19504.862939619772\n",
      "    val_loss       : 19411.33794424552\n",
      "Train Epoch: 100 [192/225000 (0%)] Loss: 19240.906250\n",
      "Train Epoch: 100 [2688/225000 (1%)] Loss: 19211.369141\n",
      "Train Epoch: 100 [5184/225000 (2%)] Loss: 19946.859375\n",
      "Train Epoch: 100 [7680/225000 (3%)] Loss: 19665.564453\n",
      "Train Epoch: 100 [10176/225000 (5%)] Loss: 19430.906250\n",
      "Train Epoch: 100 [12672/225000 (6%)] Loss: 19498.531250\n",
      "Train Epoch: 100 [15168/225000 (7%)] Loss: 19194.312500\n",
      "Train Epoch: 100 [17664/225000 (8%)] Loss: 19321.378906\n",
      "Train Epoch: 100 [20160/225000 (9%)] Loss: 19617.486328\n",
      "Train Epoch: 100 [22656/225000 (10%)] Loss: 20149.066406\n",
      "Train Epoch: 100 [25152/225000 (11%)] Loss: 19594.996094\n",
      "Train Epoch: 100 [27648/225000 (12%)] Loss: 20091.261719\n",
      "Train Epoch: 100 [30144/225000 (13%)] Loss: 19774.841797\n",
      "Train Epoch: 100 [32640/225000 (15%)] Loss: 20188.820312\n",
      "Train Epoch: 100 [35136/225000 (16%)] Loss: 20188.919922\n",
      "Train Epoch: 100 [37632/225000 (17%)] Loss: 19217.255859\n",
      "Train Epoch: 100 [40128/225000 (18%)] Loss: 19608.296875\n",
      "Train Epoch: 100 [42624/225000 (19%)] Loss: 19467.712891\n",
      "Train Epoch: 100 [45120/225000 (20%)] Loss: 19343.279297\n",
      "Train Epoch: 100 [47616/225000 (21%)] Loss: 19031.316406\n",
      "Train Epoch: 100 [50112/225000 (22%)] Loss: 19045.070312\n",
      "Train Epoch: 100 [52608/225000 (23%)] Loss: 19660.447266\n",
      "Train Epoch: 100 [55104/225000 (24%)] Loss: 19134.011719\n",
      "Train Epoch: 100 [57600/225000 (26%)] Loss: 19962.271484\n",
      "Train Epoch: 100 [60096/225000 (27%)] Loss: 19246.458984\n",
      "Train Epoch: 100 [62592/225000 (28%)] Loss: 19098.546875\n",
      "Train Epoch: 100 [65088/225000 (29%)] Loss: 19671.949219\n",
      "Train Epoch: 100 [67584/225000 (30%)] Loss: 19653.412109\n",
      "Train Epoch: 100 [70080/225000 (31%)] Loss: 19344.199219\n",
      "Train Epoch: 100 [72576/225000 (32%)] Loss: 19899.210938\n",
      "Train Epoch: 100 [75072/225000 (33%)] Loss: 19840.960938\n",
      "Train Epoch: 100 [77568/225000 (34%)] Loss: 19461.328125\n",
      "Train Epoch: 100 [80064/225000 (36%)] Loss: 19421.476562\n",
      "Train Epoch: 100 [82560/225000 (37%)] Loss: 19081.050781\n",
      "Train Epoch: 100 [85056/225000 (38%)] Loss: 19569.412109\n",
      "Train Epoch: 100 [87552/225000 (39%)] Loss: 19468.105469\n",
      "Train Epoch: 100 [90048/225000 (40%)] Loss: 19340.042969\n",
      "Train Epoch: 100 [92544/225000 (41%)] Loss: 19468.187500\n",
      "Train Epoch: 100 [95040/225000 (42%)] Loss: 19714.421875\n",
      "Train Epoch: 100 [97536/225000 (43%)] Loss: 19733.468750\n",
      "Train Epoch: 100 [100032/225000 (44%)] Loss: 19398.546875\n",
      "Train Epoch: 100 [102528/225000 (46%)] Loss: 19898.861328\n",
      "Train Epoch: 100 [105024/225000 (47%)] Loss: 19705.478516\n",
      "Train Epoch: 100 [107520/225000 (48%)] Loss: 19578.970703\n",
      "Train Epoch: 100 [110016/225000 (49%)] Loss: 19668.675781\n",
      "Train Epoch: 100 [112512/225000 (50%)] Loss: 20037.171875\n",
      "Train Epoch: 100 [115008/225000 (51%)] Loss: 19434.373047\n",
      "Train Epoch: 100 [117504/225000 (52%)] Loss: 19311.337891\n",
      "Train Epoch: 100 [120000/225000 (53%)] Loss: 19613.744141\n",
      "Train Epoch: 100 [122496/225000 (54%)] Loss: 19516.253906\n",
      "Train Epoch: 100 [124992/225000 (56%)] Loss: 19823.843750\n",
      "Train Epoch: 100 [127488/225000 (57%)] Loss: 19539.375000\n",
      "Train Epoch: 100 [129984/225000 (58%)] Loss: 19631.714844\n",
      "Train Epoch: 100 [132480/225000 (59%)] Loss: 19102.078125\n",
      "Train Epoch: 100 [134976/225000 (60%)] Loss: 19011.410156\n",
      "Train Epoch: 100 [137472/225000 (61%)] Loss: 19765.640625\n",
      "Train Epoch: 100 [139968/225000 (62%)] Loss: 19943.246094\n",
      "Train Epoch: 100 [142464/225000 (63%)] Loss: 19083.183594\n",
      "Train Epoch: 100 [144960/225000 (64%)] Loss: 19277.787109\n",
      "Train Epoch: 100 [147456/225000 (66%)] Loss: 19376.093750\n",
      "Train Epoch: 100 [149952/225000 (67%)] Loss: 19539.863281\n",
      "Train Epoch: 100 [152448/225000 (68%)] Loss: 19486.382812\n",
      "Train Epoch: 100 [154944/225000 (69%)] Loss: 19899.832031\n",
      "Train Epoch: 100 [157440/225000 (70%)] Loss: 19511.283203\n",
      "Train Epoch: 100 [159936/225000 (71%)] Loss: 19802.914062\n",
      "Train Epoch: 100 [162432/225000 (72%)] Loss: 19411.916016\n",
      "Train Epoch: 100 [164928/225000 (73%)] Loss: 19800.695312\n",
      "Train Epoch: 100 [167424/225000 (74%)] Loss: 19550.488281\n",
      "Train Epoch: 100 [169920/225000 (76%)] Loss: 19493.068359\n",
      "Train Epoch: 100 [172416/225000 (77%)] Loss: 19416.394531\n",
      "Train Epoch: 100 [174912/225000 (78%)] Loss: 19490.398438\n",
      "Train Epoch: 100 [177408/225000 (79%)] Loss: 19345.794922\n",
      "Train Epoch: 100 [179904/225000 (80%)] Loss: 19488.261719\n",
      "Train Epoch: 100 [182400/225000 (81%)] Loss: 19199.433594\n",
      "Train Epoch: 100 [184896/225000 (82%)] Loss: 18821.605469\n",
      "Train Epoch: 100 [187392/225000 (83%)] Loss: 19421.523438\n",
      "Train Epoch: 100 [189888/225000 (84%)] Loss: 19210.712891\n",
      "Train Epoch: 100 [192384/225000 (86%)] Loss: 18839.384766\n",
      "Train Epoch: 100 [194880/225000 (87%)] Loss: 18711.996094\n",
      "Train Epoch: 100 [197376/225000 (88%)] Loss: 19132.126953\n",
      "Train Epoch: 100 [199872/225000 (89%)] Loss: 19695.097656\n",
      "Train Epoch: 100 [202368/225000 (90%)] Loss: 19814.613281\n",
      "Train Epoch: 100 [204864/225000 (91%)] Loss: 19350.035156\n",
      "Train Epoch: 100 [207360/225000 (92%)] Loss: 19484.388672\n",
      "Train Epoch: 100 [209856/225000 (93%)] Loss: 19325.986328\n",
      "Train Epoch: 100 [212352/225000 (94%)] Loss: 19067.287109\n",
      "Train Epoch: 100 [214848/225000 (95%)] Loss: 19713.566406\n",
      "Train Epoch: 100 [217344/225000 (97%)] Loss: 19527.246094\n",
      "Train Epoch: 100 [219840/225000 (98%)] Loss: 19398.656250\n",
      "Train Epoch: 100 [222336/225000 (99%)] Loss: 19434.796875\n",
      "Train Epoch: 100 [224832/225000 (100%)] Loss: 20035.662109\n",
      "    epoch          : 100\n",
      "    loss           : 19493.053155996695\n",
      "    val_loss       : 19406.047886959015\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0804_123131/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 101 [192/225000 (0%)] Loss: 19409.101562\n",
      "Train Epoch: 101 [2688/225000 (1%)] Loss: 19671.144531\n",
      "Train Epoch: 101 [5184/225000 (2%)] Loss: 19372.269531\n",
      "Train Epoch: 101 [7680/225000 (3%)] Loss: 19845.128906\n",
      "Train Epoch: 101 [10176/225000 (5%)] Loss: 19092.501953\n",
      "Train Epoch: 101 [12672/225000 (6%)] Loss: 19794.365234\n",
      "Train Epoch: 101 [15168/225000 (7%)] Loss: 19237.246094\n",
      "Train Epoch: 101 [17664/225000 (8%)] Loss: 18926.460938\n",
      "Train Epoch: 101 [20160/225000 (9%)] Loss: 19540.011719\n",
      "Train Epoch: 101 [22656/225000 (10%)] Loss: 19178.578125\n",
      "Train Epoch: 101 [25152/225000 (11%)] Loss: 19412.615234\n",
      "Train Epoch: 101 [27648/225000 (12%)] Loss: 19741.101562\n",
      "Train Epoch: 101 [30144/225000 (13%)] Loss: 19709.433594\n",
      "Train Epoch: 101 [32640/225000 (15%)] Loss: 19417.107422\n",
      "Train Epoch: 101 [35136/225000 (16%)] Loss: 19577.238281\n",
      "Train Epoch: 101 [37632/225000 (17%)] Loss: 19195.250000\n",
      "Train Epoch: 101 [40128/225000 (18%)] Loss: 19346.378906\n",
      "Train Epoch: 101 [42624/225000 (19%)] Loss: 19940.027344\n",
      "Train Epoch: 101 [45120/225000 (20%)] Loss: 19481.718750\n",
      "Train Epoch: 101 [47616/225000 (21%)] Loss: 19211.210938\n",
      "Train Epoch: 101 [50112/225000 (22%)] Loss: 19786.183594\n",
      "Train Epoch: 101 [52608/225000 (23%)] Loss: 19262.019531\n",
      "Train Epoch: 101 [55104/225000 (24%)] Loss: 19203.332031\n",
      "Train Epoch: 101 [57600/225000 (26%)] Loss: 19874.750000\n",
      "Train Epoch: 101 [60096/225000 (27%)] Loss: 19264.539062\n",
      "Train Epoch: 101 [62592/225000 (28%)] Loss: 19412.839844\n",
      "Train Epoch: 101 [65088/225000 (29%)] Loss: 19653.453125\n",
      "Train Epoch: 101 [67584/225000 (30%)] Loss: 19891.898438\n",
      "Train Epoch: 101 [70080/225000 (31%)] Loss: 19383.183594\n",
      "Train Epoch: 101 [72576/225000 (32%)] Loss: 19861.050781\n",
      "Train Epoch: 101 [75072/225000 (33%)] Loss: 19540.476562\n",
      "Train Epoch: 101 [77568/225000 (34%)] Loss: 19653.023438\n",
      "Train Epoch: 101 [80064/225000 (36%)] Loss: 19226.242188\n",
      "Train Epoch: 101 [82560/225000 (37%)] Loss: 19819.867188\n",
      "Train Epoch: 101 [85056/225000 (38%)] Loss: 19108.152344\n",
      "Train Epoch: 101 [87552/225000 (39%)] Loss: 19039.582031\n",
      "Train Epoch: 101 [90048/225000 (40%)] Loss: 19691.886719\n",
      "Train Epoch: 101 [92544/225000 (41%)] Loss: 19684.542969\n",
      "Train Epoch: 101 [95040/225000 (42%)] Loss: 19558.312500\n",
      "Train Epoch: 101 [97536/225000 (43%)] Loss: 19687.488281\n",
      "Train Epoch: 101 [100032/225000 (44%)] Loss: 19232.863281\n",
      "Train Epoch: 101 [102528/225000 (46%)] Loss: 19679.925781\n",
      "Train Epoch: 101 [105024/225000 (47%)] Loss: 19543.261719\n",
      "Train Epoch: 101 [107520/225000 (48%)] Loss: 19075.312500\n",
      "Train Epoch: 101 [110016/225000 (49%)] Loss: 19219.769531\n",
      "Train Epoch: 101 [112512/225000 (50%)] Loss: 19242.945312\n",
      "Train Epoch: 101 [115008/225000 (51%)] Loss: 19041.652344\n",
      "Train Epoch: 101 [117504/225000 (52%)] Loss: 19388.027344\n",
      "Train Epoch: 101 [120000/225000 (53%)] Loss: 19483.523438\n",
      "Train Epoch: 101 [122496/225000 (54%)] Loss: 19449.250000\n",
      "Train Epoch: 101 [124992/225000 (56%)] Loss: 19680.257812\n",
      "Train Epoch: 101 [127488/225000 (57%)] Loss: 19358.603516\n",
      "Train Epoch: 101 [129984/225000 (58%)] Loss: 19520.789062\n",
      "Train Epoch: 101 [132480/225000 (59%)] Loss: 19553.255859\n",
      "Train Epoch: 101 [134976/225000 (60%)] Loss: 19169.515625\n",
      "Train Epoch: 101 [137472/225000 (61%)] Loss: 18939.242188\n",
      "Train Epoch: 101 [139968/225000 (62%)] Loss: 19401.785156\n",
      "Train Epoch: 101 [142464/225000 (63%)] Loss: 19427.423828\n",
      "Train Epoch: 101 [144960/225000 (64%)] Loss: 19456.433594\n",
      "Train Epoch: 101 [147456/225000 (66%)] Loss: 19349.771484\n",
      "Train Epoch: 101 [149952/225000 (67%)] Loss: 19761.056641\n",
      "Train Epoch: 101 [152448/225000 (68%)] Loss: 19689.210938\n",
      "Train Epoch: 101 [154944/225000 (69%)] Loss: 19922.363281\n",
      "Train Epoch: 101 [157440/225000 (70%)] Loss: 19891.238281\n",
      "Train Epoch: 101 [159936/225000 (71%)] Loss: 19394.500000\n",
      "Train Epoch: 101 [162432/225000 (72%)] Loss: 19012.539062\n",
      "Train Epoch: 101 [164928/225000 (73%)] Loss: 19372.654297\n",
      "Train Epoch: 101 [167424/225000 (74%)] Loss: 19694.457031\n",
      "Train Epoch: 101 [169920/225000 (76%)] Loss: 19405.835938\n",
      "Train Epoch: 101 [172416/225000 (77%)] Loss: 19374.900391\n",
      "Train Epoch: 101 [174912/225000 (78%)] Loss: 19221.220703\n",
      "Train Epoch: 101 [177408/225000 (79%)] Loss: 19654.070312\n",
      "Train Epoch: 101 [179904/225000 (80%)] Loss: 19657.615234\n",
      "Train Epoch: 101 [182400/225000 (81%)] Loss: 19399.900391\n",
      "Train Epoch: 101 [184896/225000 (82%)] Loss: 19036.609375\n",
      "Train Epoch: 101 [187392/225000 (83%)] Loss: 19608.078125\n",
      "Train Epoch: 101 [189888/225000 (84%)] Loss: 19231.957031\n",
      "Train Epoch: 101 [192384/225000 (86%)] Loss: 19371.464844\n",
      "Train Epoch: 101 [194880/225000 (87%)] Loss: 19838.804688\n",
      "Train Epoch: 101 [197376/225000 (88%)] Loss: 19902.984375\n",
      "Train Epoch: 101 [199872/225000 (89%)] Loss: 19355.464844\n",
      "Train Epoch: 101 [202368/225000 (90%)] Loss: 19363.421875\n",
      "Train Epoch: 101 [204864/225000 (91%)] Loss: 19343.130859\n",
      "Train Epoch: 101 [207360/225000 (92%)] Loss: 19360.031250\n",
      "Train Epoch: 101 [209856/225000 (93%)] Loss: 19173.976562\n",
      "Train Epoch: 101 [212352/225000 (94%)] Loss: 19646.753906\n",
      "Train Epoch: 101 [214848/225000 (95%)] Loss: 19214.785156\n",
      "Train Epoch: 101 [217344/225000 (97%)] Loss: 19290.119141\n",
      "Train Epoch: 101 [219840/225000 (98%)] Loss: 19202.287109\n",
      "Train Epoch: 101 [222336/225000 (99%)] Loss: 19462.863281\n",
      "Train Epoch: 101 [224832/225000 (100%)] Loss: 19789.250000\n",
      "    epoch          : 101\n",
      "    loss           : 19490.385532209897\n",
      "    val_loss       : 19387.97823185684\n",
      "Train Epoch: 102 [192/225000 (0%)] Loss: 18934.101562\n",
      "Train Epoch: 102 [2688/225000 (1%)] Loss: 19555.226562\n",
      "Train Epoch: 102 [5184/225000 (2%)] Loss: 18877.130859\n",
      "Train Epoch: 102 [7680/225000 (3%)] Loss: 19723.984375\n",
      "Train Epoch: 102 [10176/225000 (5%)] Loss: 19779.658203\n",
      "Train Epoch: 102 [12672/225000 (6%)] Loss: 19259.484375\n",
      "Train Epoch: 102 [15168/225000 (7%)] Loss: 19790.421875\n",
      "Train Epoch: 102 [17664/225000 (8%)] Loss: 19745.007812\n",
      "Train Epoch: 102 [20160/225000 (9%)] Loss: 19399.535156\n",
      "Train Epoch: 102 [22656/225000 (10%)] Loss: 19044.785156\n",
      "Train Epoch: 102 [25152/225000 (11%)] Loss: 19550.960938\n",
      "Train Epoch: 102 [27648/225000 (12%)] Loss: 19687.417969\n",
      "Train Epoch: 102 [30144/225000 (13%)] Loss: 19067.773438\n",
      "Train Epoch: 102 [32640/225000 (15%)] Loss: 19629.412109\n",
      "Train Epoch: 102 [35136/225000 (16%)] Loss: 18938.087891\n",
      "Train Epoch: 102 [37632/225000 (17%)] Loss: 18947.636719\n",
      "Train Epoch: 102 [40128/225000 (18%)] Loss: 19589.984375\n",
      "Train Epoch: 102 [42624/225000 (19%)] Loss: 19202.400391\n",
      "Train Epoch: 102 [45120/225000 (20%)] Loss: 19372.404297\n",
      "Train Epoch: 102 [47616/225000 (21%)] Loss: 19335.990234\n",
      "Train Epoch: 102 [50112/225000 (22%)] Loss: 19359.728516\n",
      "Train Epoch: 102 [52608/225000 (23%)] Loss: 18991.994141\n",
      "Train Epoch: 102 [55104/225000 (24%)] Loss: 19369.019531\n",
      "Train Epoch: 102 [57600/225000 (26%)] Loss: 19139.255859\n",
      "Train Epoch: 102 [60096/225000 (27%)] Loss: 19386.609375\n",
      "Train Epoch: 102 [62592/225000 (28%)] Loss: 19641.152344\n",
      "Train Epoch: 102 [65088/225000 (29%)] Loss: 19478.015625\n",
      "Train Epoch: 102 [67584/225000 (30%)] Loss: 20125.722656\n",
      "Train Epoch: 102 [70080/225000 (31%)] Loss: 19157.884766\n",
      "Train Epoch: 102 [72576/225000 (32%)] Loss: 19368.777344\n",
      "Train Epoch: 102 [75072/225000 (33%)] Loss: 19026.156250\n",
      "Train Epoch: 102 [77568/225000 (34%)] Loss: 19638.457031\n",
      "Train Epoch: 102 [80064/225000 (36%)] Loss: 19580.921875\n",
      "Train Epoch: 102 [82560/225000 (37%)] Loss: 19059.289062\n",
      "Train Epoch: 102 [85056/225000 (38%)] Loss: 19431.585938\n",
      "Train Epoch: 102 [87552/225000 (39%)] Loss: 19704.732422\n",
      "Train Epoch: 102 [90048/225000 (40%)] Loss: 19152.878906\n",
      "Train Epoch: 102 [92544/225000 (41%)] Loss: 19770.062500\n",
      "Train Epoch: 102 [95040/225000 (42%)] Loss: 19753.761719\n",
      "Train Epoch: 102 [97536/225000 (43%)] Loss: 19400.935547\n",
      "Train Epoch: 102 [100032/225000 (44%)] Loss: 19284.875000\n",
      "Train Epoch: 102 [102528/225000 (46%)] Loss: 19527.179688\n",
      "Train Epoch: 102 [105024/225000 (47%)] Loss: 19788.107422\n",
      "Train Epoch: 102 [107520/225000 (48%)] Loss: 19343.919922\n",
      "Train Epoch: 102 [110016/225000 (49%)] Loss: 19455.585938\n",
      "Train Epoch: 102 [112512/225000 (50%)] Loss: 19425.871094\n",
      "Train Epoch: 102 [115008/225000 (51%)] Loss: 19571.894531\n",
      "Train Epoch: 102 [117504/225000 (52%)] Loss: 19313.697266\n",
      "Train Epoch: 102 [120000/225000 (53%)] Loss: 19734.011719\n",
      "Train Epoch: 102 [122496/225000 (54%)] Loss: 19855.414062\n",
      "Train Epoch: 102 [124992/225000 (56%)] Loss: 19624.117188\n",
      "Train Epoch: 102 [127488/225000 (57%)] Loss: 19381.414062\n",
      "Train Epoch: 102 [129984/225000 (58%)] Loss: 19831.650391\n",
      "Train Epoch: 102 [132480/225000 (59%)] Loss: 19940.650391\n",
      "Train Epoch: 102 [134976/225000 (60%)] Loss: 19411.679688\n",
      "Train Epoch: 102 [137472/225000 (61%)] Loss: 19246.132812\n",
      "Train Epoch: 102 [139968/225000 (62%)] Loss: 19188.410156\n",
      "Train Epoch: 102 [142464/225000 (63%)] Loss: 19959.078125\n",
      "Train Epoch: 102 [144960/225000 (64%)] Loss: 19136.753906\n",
      "Train Epoch: 102 [147456/225000 (66%)] Loss: 19295.750000\n",
      "Train Epoch: 102 [149952/225000 (67%)] Loss: 19541.982422\n",
      "Train Epoch: 102 [152448/225000 (68%)] Loss: 19302.992188\n",
      "Train Epoch: 102 [154944/225000 (69%)] Loss: 19587.720703\n",
      "Train Epoch: 102 [157440/225000 (70%)] Loss: 19767.566406\n",
      "Train Epoch: 102 [159936/225000 (71%)] Loss: 19528.171875\n",
      "Train Epoch: 102 [162432/225000 (72%)] Loss: 19416.417969\n",
      "Train Epoch: 102 [164928/225000 (73%)] Loss: 19531.710938\n",
      "Train Epoch: 102 [167424/225000 (74%)] Loss: 19081.757812\n",
      "Train Epoch: 102 [169920/225000 (76%)] Loss: 19701.757812\n",
      "Train Epoch: 102 [172416/225000 (77%)] Loss: 19011.378906\n",
      "Train Epoch: 102 [174912/225000 (78%)] Loss: 19412.777344\n",
      "Train Epoch: 102 [177408/225000 (79%)] Loss: 19716.640625\n",
      "Train Epoch: 102 [179904/225000 (80%)] Loss: 19664.667969\n",
      "Train Epoch: 102 [182400/225000 (81%)] Loss: 19200.925781\n",
      "Train Epoch: 102 [184896/225000 (82%)] Loss: 19567.667969\n",
      "Train Epoch: 102 [187392/225000 (83%)] Loss: 19157.890625\n",
      "Train Epoch: 102 [189888/225000 (84%)] Loss: 19850.541016\n",
      "Train Epoch: 102 [192384/225000 (86%)] Loss: 19206.435547\n",
      "Train Epoch: 102 [194880/225000 (87%)] Loss: 19853.875000\n",
      "Train Epoch: 102 [197376/225000 (88%)] Loss: 19309.054688\n",
      "Train Epoch: 102 [199872/225000 (89%)] Loss: 19470.453125\n",
      "Train Epoch: 102 [202368/225000 (90%)] Loss: 19559.824219\n",
      "Train Epoch: 102 [204864/225000 (91%)] Loss: 20037.650391\n",
      "Train Epoch: 102 [207360/225000 (92%)] Loss: 19802.087891\n",
      "Train Epoch: 102 [209856/225000 (93%)] Loss: 19411.054688\n",
      "Train Epoch: 102 [212352/225000 (94%)] Loss: 19109.982422\n",
      "Train Epoch: 102 [214848/225000 (95%)] Loss: 19168.640625\n",
      "Train Epoch: 102 [217344/225000 (97%)] Loss: 19644.285156\n",
      "Train Epoch: 102 [219840/225000 (98%)] Loss: 19551.072266\n",
      "Train Epoch: 102 [222336/225000 (99%)] Loss: 19664.902344\n",
      "Train Epoch: 102 [224832/225000 (100%)] Loss: 18972.132812\n",
      "    epoch          : 102\n",
      "    loss           : 19480.145916102283\n",
      "    val_loss       : 19391.595206858547\n",
      "Train Epoch: 103 [192/225000 (0%)] Loss: 19241.388672\n",
      "Train Epoch: 103 [2688/225000 (1%)] Loss: 19412.261719\n",
      "Train Epoch: 103 [5184/225000 (2%)] Loss: 19493.027344\n",
      "Train Epoch: 103 [7680/225000 (3%)] Loss: 19254.394531\n",
      "Train Epoch: 103 [10176/225000 (5%)] Loss: 19111.394531\n",
      "Train Epoch: 103 [12672/225000 (6%)] Loss: 18995.531250\n",
      "Train Epoch: 103 [15168/225000 (7%)] Loss: 19370.703125\n",
      "Train Epoch: 103 [17664/225000 (8%)] Loss: 19627.808594\n",
      "Train Epoch: 103 [20160/225000 (9%)] Loss: 19345.425781\n",
      "Train Epoch: 103 [22656/225000 (10%)] Loss: 19541.589844\n",
      "Train Epoch: 103 [25152/225000 (11%)] Loss: 19233.777344\n",
      "Train Epoch: 103 [27648/225000 (12%)] Loss: 19584.269531\n",
      "Train Epoch: 103 [30144/225000 (13%)] Loss: 19536.972656\n",
      "Train Epoch: 103 [32640/225000 (15%)] Loss: 19585.041016\n",
      "Train Epoch: 103 [35136/225000 (16%)] Loss: 19304.246094\n",
      "Train Epoch: 103 [37632/225000 (17%)] Loss: 19163.654297\n",
      "Train Epoch: 103 [40128/225000 (18%)] Loss: 19155.613281\n",
      "Train Epoch: 103 [42624/225000 (19%)] Loss: 19890.583984\n",
      "Train Epoch: 103 [45120/225000 (20%)] Loss: 19699.285156\n",
      "Train Epoch: 103 [47616/225000 (21%)] Loss: 19439.341797\n",
      "Train Epoch: 103 [50112/225000 (22%)] Loss: 19364.330078\n",
      "Train Epoch: 103 [52608/225000 (23%)] Loss: 19711.279297\n",
      "Train Epoch: 103 [55104/225000 (24%)] Loss: 19458.199219\n",
      "Train Epoch: 103 [57600/225000 (26%)] Loss: 19460.023438\n",
      "Train Epoch: 103 [60096/225000 (27%)] Loss: 19639.291016\n",
      "Train Epoch: 103 [62592/225000 (28%)] Loss: 19707.421875\n",
      "Train Epoch: 103 [65088/225000 (29%)] Loss: 19309.591797\n",
      "Train Epoch: 103 [67584/225000 (30%)] Loss: 19390.523438\n",
      "Train Epoch: 103 [70080/225000 (31%)] Loss: 19937.005859\n",
      "Train Epoch: 103 [72576/225000 (32%)] Loss: 19363.574219\n",
      "Train Epoch: 103 [75072/225000 (33%)] Loss: 19689.421875\n",
      "Train Epoch: 103 [77568/225000 (34%)] Loss: 19978.833984\n",
      "Train Epoch: 103 [80064/225000 (36%)] Loss: 19722.191406\n",
      "Train Epoch: 103 [82560/225000 (37%)] Loss: 20030.171875\n",
      "Train Epoch: 103 [85056/225000 (38%)] Loss: 19509.140625\n",
      "Train Epoch: 103 [87552/225000 (39%)] Loss: 19839.644531\n",
      "Train Epoch: 103 [90048/225000 (40%)] Loss: 19858.332031\n",
      "Train Epoch: 103 [92544/225000 (41%)] Loss: 19067.757812\n",
      "Train Epoch: 103 [95040/225000 (42%)] Loss: 19351.367188\n",
      "Train Epoch: 103 [97536/225000 (43%)] Loss: 19766.929688\n",
      "Train Epoch: 103 [100032/225000 (44%)] Loss: 19705.068359\n",
      "Train Epoch: 103 [102528/225000 (46%)] Loss: 19100.722656\n",
      "Train Epoch: 103 [105024/225000 (47%)] Loss: 19328.570312\n",
      "Train Epoch: 103 [107520/225000 (48%)] Loss: 19401.701172\n",
      "Train Epoch: 103 [110016/225000 (49%)] Loss: 19338.427734\n",
      "Train Epoch: 103 [112512/225000 (50%)] Loss: 19099.062500\n",
      "Train Epoch: 103 [115008/225000 (51%)] Loss: 19418.664062\n",
      "Train Epoch: 103 [117504/225000 (52%)] Loss: 19353.605469\n",
      "Train Epoch: 103 [120000/225000 (53%)] Loss: 19386.582031\n",
      "Train Epoch: 103 [122496/225000 (54%)] Loss: 19577.751953\n",
      "Train Epoch: 103 [124992/225000 (56%)] Loss: 19271.134766\n",
      "Train Epoch: 103 [127488/225000 (57%)] Loss: 19589.667969\n",
      "Train Epoch: 103 [129984/225000 (58%)] Loss: 19434.251953\n",
      "Train Epoch: 103 [132480/225000 (59%)] Loss: 19122.480469\n",
      "Train Epoch: 103 [134976/225000 (60%)] Loss: 19570.349609\n",
      "Train Epoch: 103 [137472/225000 (61%)] Loss: 19311.900391\n",
      "Train Epoch: 103 [139968/225000 (62%)] Loss: 19457.480469\n",
      "Train Epoch: 103 [142464/225000 (63%)] Loss: 19651.419922\n",
      "Train Epoch: 103 [144960/225000 (64%)] Loss: 19148.951172\n",
      "Train Epoch: 103 [147456/225000 (66%)] Loss: 19496.039062\n",
      "Train Epoch: 103 [149952/225000 (67%)] Loss: 18894.082031\n",
      "Train Epoch: 103 [152448/225000 (68%)] Loss: 19185.884766\n",
      "Train Epoch: 103 [154944/225000 (69%)] Loss: 19920.695312\n",
      "Train Epoch: 103 [157440/225000 (70%)] Loss: 19377.173828\n",
      "Train Epoch: 103 [159936/225000 (71%)] Loss: 19710.082031\n",
      "Train Epoch: 103 [162432/225000 (72%)] Loss: 19246.105469\n",
      "Train Epoch: 103 [164928/225000 (73%)] Loss: 19303.287109\n",
      "Train Epoch: 103 [167424/225000 (74%)] Loss: 19262.656250\n",
      "Train Epoch: 103 [169920/225000 (76%)] Loss: 18852.425781\n",
      "Train Epoch: 103 [172416/225000 (77%)] Loss: 19576.597656\n",
      "Train Epoch: 103 [174912/225000 (78%)] Loss: 19308.335938\n",
      "Train Epoch: 103 [177408/225000 (79%)] Loss: 19609.931641\n",
      "Train Epoch: 103 [179904/225000 (80%)] Loss: 19260.710938\n",
      "Train Epoch: 103 [182400/225000 (81%)] Loss: 19521.027344\n",
      "Train Epoch: 103 [184896/225000 (82%)] Loss: 19183.873047\n",
      "Train Epoch: 103 [187392/225000 (83%)] Loss: 19350.523438\n",
      "Train Epoch: 103 [189888/225000 (84%)] Loss: 19683.417969\n",
      "Train Epoch: 103 [192384/225000 (86%)] Loss: 19500.550781\n",
      "Train Epoch: 103 [194880/225000 (87%)] Loss: 19193.644531\n",
      "Train Epoch: 103 [197376/225000 (88%)] Loss: 19670.894531\n",
      "Train Epoch: 103 [199872/225000 (89%)] Loss: 19559.853516\n",
      "Train Epoch: 103 [202368/225000 (90%)] Loss: 19725.750000\n",
      "Train Epoch: 103 [204864/225000 (91%)] Loss: 19585.960938\n",
      "Train Epoch: 103 [207360/225000 (92%)] Loss: 19404.121094\n",
      "Train Epoch: 103 [209856/225000 (93%)] Loss: 19432.437500\n",
      "Train Epoch: 103 [212352/225000 (94%)] Loss: 19330.132812\n",
      "Train Epoch: 103 [214848/225000 (95%)] Loss: 19979.214844\n",
      "Train Epoch: 103 [217344/225000 (97%)] Loss: 19519.130859\n",
      "Train Epoch: 103 [219840/225000 (98%)] Loss: 19642.464844\n",
      "Train Epoch: 103 [222336/225000 (99%)] Loss: 19557.035156\n",
      "Train Epoch: 103 [224832/225000 (100%)] Loss: 19855.015625\n",
      "    epoch          : 103\n",
      "    loss           : 19479.040672328287\n",
      "    val_loss       : 19408.32003183037\n",
      "Train Epoch: 104 [192/225000 (0%)] Loss: 19408.664062\n",
      "Train Epoch: 104 [2688/225000 (1%)] Loss: 18511.605469\n",
      "Train Epoch: 104 [5184/225000 (2%)] Loss: 19252.554688\n",
      "Train Epoch: 104 [7680/225000 (3%)] Loss: 19437.269531\n",
      "Train Epoch: 104 [10176/225000 (5%)] Loss: 19497.126953\n",
      "Train Epoch: 104 [12672/225000 (6%)] Loss: 19383.851562\n",
      "Train Epoch: 104 [15168/225000 (7%)] Loss: 19401.998047\n",
      "Train Epoch: 104 [17664/225000 (8%)] Loss: 19520.644531\n",
      "Train Epoch: 104 [20160/225000 (9%)] Loss: 19512.031250\n",
      "Train Epoch: 104 [22656/225000 (10%)] Loss: 19559.875000\n",
      "Train Epoch: 104 [25152/225000 (11%)] Loss: 20020.238281\n",
      "Train Epoch: 104 [27648/225000 (12%)] Loss: 19801.083984\n",
      "Train Epoch: 104 [30144/225000 (13%)] Loss: 19204.669922\n",
      "Train Epoch: 104 [32640/225000 (15%)] Loss: 19791.281250\n",
      "Train Epoch: 104 [35136/225000 (16%)] Loss: 20003.621094\n",
      "Train Epoch: 104 [37632/225000 (17%)] Loss: 19051.044922\n",
      "Train Epoch: 104 [40128/225000 (18%)] Loss: 19931.457031\n",
      "Train Epoch: 104 [42624/225000 (19%)] Loss: 19558.984375\n",
      "Train Epoch: 104 [45120/225000 (20%)] Loss: 19296.847656\n",
      "Train Epoch: 104 [47616/225000 (21%)] Loss: 19285.746094\n",
      "Train Epoch: 104 [50112/225000 (22%)] Loss: 19544.824219\n",
      "Train Epoch: 104 [52608/225000 (23%)] Loss: 19431.734375\n",
      "Train Epoch: 104 [55104/225000 (24%)] Loss: 19787.408203\n",
      "Train Epoch: 104 [57600/225000 (26%)] Loss: 19792.986328\n",
      "Train Epoch: 104 [60096/225000 (27%)] Loss: 19549.984375\n",
      "Train Epoch: 104 [62592/225000 (28%)] Loss: 19703.292969\n",
      "Train Epoch: 104 [65088/225000 (29%)] Loss: 19954.373047\n",
      "Train Epoch: 104 [67584/225000 (30%)] Loss: 19519.746094\n",
      "Train Epoch: 104 [70080/225000 (31%)] Loss: 19006.589844\n",
      "Train Epoch: 104 [72576/225000 (32%)] Loss: 19894.335938\n",
      "Train Epoch: 104 [75072/225000 (33%)] Loss: 19596.675781\n",
      "Train Epoch: 104 [77568/225000 (34%)] Loss: 18919.625000\n",
      "Train Epoch: 104 [80064/225000 (36%)] Loss: 18976.015625\n",
      "Train Epoch: 104 [82560/225000 (37%)] Loss: 19787.023438\n",
      "Train Epoch: 104 [85056/225000 (38%)] Loss: 19315.695312\n",
      "Train Epoch: 104 [87552/225000 (39%)] Loss: 19505.492188\n",
      "Train Epoch: 104 [90048/225000 (40%)] Loss: 19621.691406\n",
      "Train Epoch: 104 [92544/225000 (41%)] Loss: 19151.941406\n",
      "Train Epoch: 104 [95040/225000 (42%)] Loss: 19656.914062\n",
      "Train Epoch: 104 [97536/225000 (43%)] Loss: 18951.726562\n",
      "Train Epoch: 104 [100032/225000 (44%)] Loss: 19144.941406\n",
      "Train Epoch: 104 [102528/225000 (46%)] Loss: 19700.314453\n",
      "Train Epoch: 104 [105024/225000 (47%)] Loss: 19038.335938\n",
      "Train Epoch: 104 [107520/225000 (48%)] Loss: 19267.078125\n",
      "Train Epoch: 104 [110016/225000 (49%)] Loss: 19654.722656\n",
      "Train Epoch: 104 [112512/225000 (50%)] Loss: 19080.617188\n",
      "Train Epoch: 104 [115008/225000 (51%)] Loss: 19518.914062\n",
      "Train Epoch: 104 [117504/225000 (52%)] Loss: 19442.257812\n",
      "Train Epoch: 104 [120000/225000 (53%)] Loss: 19662.882812\n",
      "Train Epoch: 104 [122496/225000 (54%)] Loss: 19367.531250\n",
      "Train Epoch: 104 [124992/225000 (56%)] Loss: 19866.824219\n",
      "Train Epoch: 104 [127488/225000 (57%)] Loss: 19143.132812\n",
      "Train Epoch: 104 [129984/225000 (58%)] Loss: 19884.699219\n",
      "Train Epoch: 104 [132480/225000 (59%)] Loss: 19164.128906\n",
      "Train Epoch: 104 [134976/225000 (60%)] Loss: 19241.400391\n",
      "Train Epoch: 104 [137472/225000 (61%)] Loss: 19430.003906\n",
      "Train Epoch: 104 [139968/225000 (62%)] Loss: 19327.580078\n",
      "Train Epoch: 104 [142464/225000 (63%)] Loss: 19431.982422\n",
      "Train Epoch: 104 [144960/225000 (64%)] Loss: 19184.503906\n",
      "Train Epoch: 104 [147456/225000 (66%)] Loss: 19779.421875\n",
      "Train Epoch: 104 [149952/225000 (67%)] Loss: 19187.406250\n",
      "Train Epoch: 104 [152448/225000 (68%)] Loss: 19600.832031\n",
      "Train Epoch: 104 [154944/225000 (69%)] Loss: 19489.789062\n",
      "Train Epoch: 104 [157440/225000 (70%)] Loss: 19745.634766\n",
      "Train Epoch: 104 [159936/225000 (71%)] Loss: 19490.609375\n",
      "Train Epoch: 104 [162432/225000 (72%)] Loss: 19133.490234\n",
      "Train Epoch: 104 [164928/225000 (73%)] Loss: 19202.972656\n",
      "Train Epoch: 104 [167424/225000 (74%)] Loss: 19335.378906\n",
      "Train Epoch: 104 [169920/225000 (76%)] Loss: 19803.455078\n",
      "Train Epoch: 104 [172416/225000 (77%)] Loss: 19905.683594\n",
      "Train Epoch: 104 [174912/225000 (78%)] Loss: 19136.539062\n",
      "Train Epoch: 104 [177408/225000 (79%)] Loss: 19099.210938\n",
      "Train Epoch: 104 [179904/225000 (80%)] Loss: 19456.212891\n",
      "Train Epoch: 104 [182400/225000 (81%)] Loss: 19438.406250\n",
      "Train Epoch: 104 [184896/225000 (82%)] Loss: 19237.783203\n",
      "Train Epoch: 104 [187392/225000 (83%)] Loss: 19176.066406\n",
      "Train Epoch: 104 [189888/225000 (84%)] Loss: 19262.464844\n",
      "Train Epoch: 104 [192384/225000 (86%)] Loss: 18862.166016\n",
      "Train Epoch: 104 [194880/225000 (87%)] Loss: 19610.457031\n",
      "Train Epoch: 104 [197376/225000 (88%)] Loss: 19144.576172\n",
      "Train Epoch: 104 [199872/225000 (89%)] Loss: 19693.878906\n",
      "Train Epoch: 104 [202368/225000 (90%)] Loss: 19862.646484\n",
      "Train Epoch: 104 [204864/225000 (91%)] Loss: 19416.425781\n",
      "Train Epoch: 104 [207360/225000 (92%)] Loss: 20141.785156\n",
      "Train Epoch: 104 [209856/225000 (93%)] Loss: 19149.746094\n",
      "Train Epoch: 104 [212352/225000 (94%)] Loss: 19118.175781\n",
      "Train Epoch: 104 [214848/225000 (95%)] Loss: 19306.726562\n",
      "Train Epoch: 104 [217344/225000 (97%)] Loss: 19134.230469\n",
      "Train Epoch: 104 [219840/225000 (98%)] Loss: 19542.402344\n",
      "Train Epoch: 104 [222336/225000 (99%)] Loss: 19881.900391\n",
      "Train Epoch: 104 [224832/225000 (100%)] Loss: 19663.216797\n",
      "    epoch          : 104\n",
      "    loss           : 19474.547523264184\n",
      "    val_loss       : 19366.427338034144\n",
      "Train Epoch: 105 [192/225000 (0%)] Loss: 19618.253906\n",
      "Train Epoch: 105 [2688/225000 (1%)] Loss: 19671.806641\n",
      "Train Epoch: 105 [5184/225000 (2%)] Loss: 19767.664062\n",
      "Train Epoch: 105 [7680/225000 (3%)] Loss: 19109.644531\n",
      "Train Epoch: 105 [10176/225000 (5%)] Loss: 19374.984375\n",
      "Train Epoch: 105 [12672/225000 (6%)] Loss: 19019.425781\n",
      "Train Epoch: 105 [15168/225000 (7%)] Loss: 19528.273438\n",
      "Train Epoch: 105 [17664/225000 (8%)] Loss: 19058.550781\n",
      "Train Epoch: 105 [20160/225000 (9%)] Loss: 19867.367188\n",
      "Train Epoch: 105 [22656/225000 (10%)] Loss: 19982.351562\n",
      "Train Epoch: 105 [25152/225000 (11%)] Loss: 19548.085938\n",
      "Train Epoch: 105 [27648/225000 (12%)] Loss: 19452.203125\n",
      "Train Epoch: 105 [30144/225000 (13%)] Loss: 19613.171875\n",
      "Train Epoch: 105 [32640/225000 (15%)] Loss: 19675.613281\n",
      "Train Epoch: 105 [35136/225000 (16%)] Loss: 19357.898438\n",
      "Train Epoch: 105 [37632/225000 (17%)] Loss: 19253.425781\n",
      "Train Epoch: 105 [40128/225000 (18%)] Loss: 19515.593750\n",
      "Train Epoch: 105 [42624/225000 (19%)] Loss: 19539.902344\n",
      "Train Epoch: 105 [45120/225000 (20%)] Loss: 19739.726562\n",
      "Train Epoch: 105 [47616/225000 (21%)] Loss: 19604.707031\n",
      "Train Epoch: 105 [50112/225000 (22%)] Loss: 19463.917969\n",
      "Train Epoch: 105 [52608/225000 (23%)] Loss: 19350.857422\n",
      "Train Epoch: 105 [55104/225000 (24%)] Loss: 19365.248047\n",
      "Train Epoch: 105 [57600/225000 (26%)] Loss: 19021.648438\n",
      "Train Epoch: 105 [60096/225000 (27%)] Loss: 19680.687500\n",
      "Train Epoch: 105 [62592/225000 (28%)] Loss: 19584.906250\n",
      "Train Epoch: 105 [65088/225000 (29%)] Loss: 19446.953125\n",
      "Train Epoch: 105 [67584/225000 (30%)] Loss: 18948.054688\n",
      "Train Epoch: 105 [70080/225000 (31%)] Loss: 19563.277344\n",
      "Train Epoch: 105 [72576/225000 (32%)] Loss: 19672.816406\n",
      "Train Epoch: 105 [75072/225000 (33%)] Loss: 19543.277344\n",
      "Train Epoch: 105 [77568/225000 (34%)] Loss: 19595.033203\n",
      "Train Epoch: 105 [80064/225000 (36%)] Loss: 19804.667969\n",
      "Train Epoch: 105 [82560/225000 (37%)] Loss: 19842.123047\n",
      "Train Epoch: 105 [85056/225000 (38%)] Loss: 19394.066406\n",
      "Train Epoch: 105 [87552/225000 (39%)] Loss: 19570.734375\n",
      "Train Epoch: 105 [90048/225000 (40%)] Loss: 19914.308594\n",
      "Train Epoch: 105 [92544/225000 (41%)] Loss: 19106.775391\n",
      "Train Epoch: 105 [95040/225000 (42%)] Loss: 19120.109375\n",
      "Train Epoch: 105 [97536/225000 (43%)] Loss: 19153.277344\n",
      "Train Epoch: 105 [100032/225000 (44%)] Loss: 19359.927734\n",
      "Train Epoch: 105 [102528/225000 (46%)] Loss: 19565.248047\n",
      "Train Epoch: 105 [105024/225000 (47%)] Loss: 19449.404297\n",
      "Train Epoch: 105 [107520/225000 (48%)] Loss: 19685.718750\n",
      "Train Epoch: 105 [110016/225000 (49%)] Loss: 19413.125000\n",
      "Train Epoch: 105 [112512/225000 (50%)] Loss: 19623.710938\n",
      "Train Epoch: 105 [115008/225000 (51%)] Loss: 20007.107422\n",
      "Train Epoch: 105 [117504/225000 (52%)] Loss: 19301.429688\n",
      "Train Epoch: 105 [120000/225000 (53%)] Loss: 18658.089844\n",
      "Train Epoch: 105 [122496/225000 (54%)] Loss: 19495.179688\n",
      "Train Epoch: 105 [124992/225000 (56%)] Loss: 19240.742188\n",
      "Train Epoch: 105 [127488/225000 (57%)] Loss: 19972.191406\n",
      "Train Epoch: 105 [129984/225000 (58%)] Loss: 19384.703125\n",
      "Train Epoch: 105 [132480/225000 (59%)] Loss: 19363.699219\n",
      "Train Epoch: 105 [134976/225000 (60%)] Loss: 19238.285156\n",
      "Train Epoch: 105 [137472/225000 (61%)] Loss: 19851.568359\n",
      "Train Epoch: 105 [139968/225000 (62%)] Loss: 19475.925781\n",
      "Train Epoch: 105 [142464/225000 (63%)] Loss: 19097.611328\n",
      "Train Epoch: 105 [144960/225000 (64%)] Loss: 19084.195312\n",
      "Train Epoch: 105 [147456/225000 (66%)] Loss: 18841.964844\n",
      "Train Epoch: 105 [149952/225000 (67%)] Loss: 19343.140625\n",
      "Train Epoch: 105 [152448/225000 (68%)] Loss: 19986.517578\n",
      "Train Epoch: 105 [154944/225000 (69%)] Loss: 19145.003906\n",
      "Train Epoch: 105 [157440/225000 (70%)] Loss: 19776.601562\n",
      "Train Epoch: 105 [159936/225000 (71%)] Loss: 19956.640625\n",
      "Train Epoch: 105 [162432/225000 (72%)] Loss: 19308.250000\n",
      "Train Epoch: 105 [164928/225000 (73%)] Loss: 19328.187500\n",
      "Train Epoch: 105 [167424/225000 (74%)] Loss: 19928.765625\n",
      "Train Epoch: 105 [169920/225000 (76%)] Loss: 19389.031250\n",
      "Train Epoch: 105 [172416/225000 (77%)] Loss: 19614.675781\n",
      "Train Epoch: 105 [174912/225000 (78%)] Loss: 19715.085938\n",
      "Train Epoch: 105 [177408/225000 (79%)] Loss: 19154.382812\n",
      "Train Epoch: 105 [179904/225000 (80%)] Loss: 19814.042969\n",
      "Train Epoch: 105 [182400/225000 (81%)] Loss: 19685.564453\n",
      "Train Epoch: 105 [184896/225000 (82%)] Loss: 19313.429688\n",
      "Train Epoch: 105 [187392/225000 (83%)] Loss: 19207.097656\n",
      "Train Epoch: 105 [189888/225000 (84%)] Loss: 19876.195312\n",
      "Train Epoch: 105 [192384/225000 (86%)] Loss: 19527.308594\n",
      "Train Epoch: 105 [194880/225000 (87%)] Loss: 19084.056641\n",
      "Train Epoch: 105 [197376/225000 (88%)] Loss: 18965.335938\n",
      "Train Epoch: 105 [199872/225000 (89%)] Loss: 19077.695312\n",
      "Train Epoch: 105 [202368/225000 (90%)] Loss: 19524.628906\n",
      "Train Epoch: 105 [204864/225000 (91%)] Loss: 19981.439453\n",
      "Train Epoch: 105 [207360/225000 (92%)] Loss: 19346.123047\n",
      "Train Epoch: 105 [209856/225000 (93%)] Loss: 18963.113281\n",
      "Train Epoch: 105 [212352/225000 (94%)] Loss: 19554.544922\n",
      "Train Epoch: 105 [214848/225000 (95%)] Loss: 19168.781250\n",
      "Train Epoch: 105 [217344/225000 (97%)] Loss: 19541.298828\n",
      "Train Epoch: 105 [219840/225000 (98%)] Loss: 19699.007812\n",
      "Train Epoch: 105 [222336/225000 (99%)] Loss: 19120.261719\n",
      "Train Epoch: 105 [224832/225000 (100%)] Loss: 18961.972656\n",
      "    epoch          : 105\n",
      "    loss           : 19461.356711950724\n",
      "    val_loss       : 19366.807368680722\n",
      "Train Epoch: 106 [192/225000 (0%)] Loss: 19650.246094\n",
      "Train Epoch: 106 [2688/225000 (1%)] Loss: 19201.367188\n",
      "Train Epoch: 106 [5184/225000 (2%)] Loss: 19632.753906\n",
      "Train Epoch: 106 [7680/225000 (3%)] Loss: 19629.953125\n",
      "Train Epoch: 106 [10176/225000 (5%)] Loss: 19136.550781\n",
      "Train Epoch: 106 [12672/225000 (6%)] Loss: 19615.615234\n",
      "Train Epoch: 106 [15168/225000 (7%)] Loss: 19446.785156\n",
      "Train Epoch: 106 [17664/225000 (8%)] Loss: 19073.730469\n",
      "Train Epoch: 106 [20160/225000 (9%)] Loss: 19712.419922\n",
      "Train Epoch: 106 [22656/225000 (10%)] Loss: 19014.558594\n",
      "Train Epoch: 106 [25152/225000 (11%)] Loss: 20018.650391\n",
      "Train Epoch: 106 [27648/225000 (12%)] Loss: 19057.015625\n",
      "Train Epoch: 106 [30144/225000 (13%)] Loss: 19717.171875\n",
      "Train Epoch: 106 [32640/225000 (15%)] Loss: 19890.697266\n",
      "Train Epoch: 106 [35136/225000 (16%)] Loss: 19178.208984\n",
      "Train Epoch: 106 [37632/225000 (17%)] Loss: 19781.046875\n",
      "Train Epoch: 106 [40128/225000 (18%)] Loss: 19484.640625\n",
      "Train Epoch: 106 [42624/225000 (19%)] Loss: 19587.988281\n",
      "Train Epoch: 106 [45120/225000 (20%)] Loss: 19097.824219\n",
      "Train Epoch: 106 [47616/225000 (21%)] Loss: 19235.886719\n",
      "Train Epoch: 106 [50112/225000 (22%)] Loss: 19713.757812\n",
      "Train Epoch: 106 [52608/225000 (23%)] Loss: 19408.353516\n",
      "Train Epoch: 106 [55104/225000 (24%)] Loss: 19323.753906\n",
      "Train Epoch: 106 [57600/225000 (26%)] Loss: 19609.462891\n",
      "Train Epoch: 106 [60096/225000 (27%)] Loss: 19305.484375\n",
      "Train Epoch: 106 [62592/225000 (28%)] Loss: 19769.406250\n",
      "Train Epoch: 106 [65088/225000 (29%)] Loss: 19907.759766\n",
      "Train Epoch: 106 [67584/225000 (30%)] Loss: 19338.439453\n",
      "Train Epoch: 106 [70080/225000 (31%)] Loss: 19458.138672\n",
      "Train Epoch: 106 [72576/225000 (32%)] Loss: 19508.480469\n",
      "Train Epoch: 106 [75072/225000 (33%)] Loss: 19975.609375\n",
      "Train Epoch: 106 [77568/225000 (34%)] Loss: 19624.632812\n",
      "Train Epoch: 106 [80064/225000 (36%)] Loss: 19244.093750\n",
      "Train Epoch: 106 [82560/225000 (37%)] Loss: 19825.638672\n",
      "Train Epoch: 106 [85056/225000 (38%)] Loss: 19411.521484\n",
      "Train Epoch: 106 [87552/225000 (39%)] Loss: 19470.261719\n",
      "Train Epoch: 106 [90048/225000 (40%)] Loss: 19432.910156\n",
      "Train Epoch: 106 [92544/225000 (41%)] Loss: 19762.289062\n",
      "Train Epoch: 106 [95040/225000 (42%)] Loss: 19782.414062\n",
      "Train Epoch: 106 [97536/225000 (43%)] Loss: 19060.498047\n",
      "Train Epoch: 106 [100032/225000 (44%)] Loss: 19654.187500\n",
      "Train Epoch: 106 [102528/225000 (46%)] Loss: 19440.933594\n",
      "Train Epoch: 106 [105024/225000 (47%)] Loss: 19530.777344\n",
      "Train Epoch: 106 [107520/225000 (48%)] Loss: 19466.863281\n",
      "Train Epoch: 106 [110016/225000 (49%)] Loss: 19520.664062\n",
      "Train Epoch: 106 [112512/225000 (50%)] Loss: 19493.673828\n",
      "Train Epoch: 106 [115008/225000 (51%)] Loss: 19704.714844\n",
      "Train Epoch: 106 [117504/225000 (52%)] Loss: 19458.201172\n",
      "Train Epoch: 106 [120000/225000 (53%)] Loss: 19280.382812\n",
      "Train Epoch: 106 [122496/225000 (54%)] Loss: 19929.085938\n",
      "Train Epoch: 106 [124992/225000 (56%)] Loss: 19964.796875\n",
      "Train Epoch: 106 [127488/225000 (57%)] Loss: 19203.933594\n",
      "Train Epoch: 106 [129984/225000 (58%)] Loss: 19661.074219\n",
      "Train Epoch: 106 [132480/225000 (59%)] Loss: 19479.804688\n",
      "Train Epoch: 106 [134976/225000 (60%)] Loss: 19604.062500\n",
      "Train Epoch: 106 [137472/225000 (61%)] Loss: 19036.560547\n",
      "Train Epoch: 106 [139968/225000 (62%)] Loss: 19604.068359\n",
      "Train Epoch: 106 [142464/225000 (63%)] Loss: 19253.054688\n",
      "Train Epoch: 106 [144960/225000 (64%)] Loss: 19385.013672\n",
      "Train Epoch: 106 [147456/225000 (66%)] Loss: 19071.472656\n",
      "Train Epoch: 106 [149952/225000 (67%)] Loss: 19763.953125\n",
      "Train Epoch: 106 [152448/225000 (68%)] Loss: 19323.445312\n",
      "Train Epoch: 106 [154944/225000 (69%)] Loss: 19503.095703\n",
      "Train Epoch: 106 [157440/225000 (70%)] Loss: 19225.876953\n",
      "Train Epoch: 106 [159936/225000 (71%)] Loss: 19992.197266\n",
      "Train Epoch: 106 [162432/225000 (72%)] Loss: 19469.986328\n",
      "Train Epoch: 106 [164928/225000 (73%)] Loss: 19465.085938\n",
      "Train Epoch: 106 [167424/225000 (74%)] Loss: 19438.183594\n",
      "Train Epoch: 106 [169920/225000 (76%)] Loss: 19382.324219\n",
      "Train Epoch: 106 [172416/225000 (77%)] Loss: 19485.626953\n",
      "Train Epoch: 106 [174912/225000 (78%)] Loss: 19235.363281\n",
      "Train Epoch: 106 [177408/225000 (79%)] Loss: 19946.027344\n",
      "Train Epoch: 106 [179904/225000 (80%)] Loss: 19761.039062\n",
      "Train Epoch: 106 [182400/225000 (81%)] Loss: 18899.738281\n",
      "Train Epoch: 106 [184896/225000 (82%)] Loss: 19776.349609\n",
      "Train Epoch: 106 [187392/225000 (83%)] Loss: 20029.875000\n",
      "Train Epoch: 106 [189888/225000 (84%)] Loss: 19782.712891\n",
      "Train Epoch: 106 [192384/225000 (86%)] Loss: 19704.144531\n",
      "Train Epoch: 106 [194880/225000 (87%)] Loss: 19159.593750\n",
      "Train Epoch: 106 [197376/225000 (88%)] Loss: 19626.955078\n",
      "Train Epoch: 106 [199872/225000 (89%)] Loss: 19500.394531\n",
      "Train Epoch: 106 [202368/225000 (90%)] Loss: 18715.136719\n",
      "Train Epoch: 106 [204864/225000 (91%)] Loss: 18774.085938\n",
      "Train Epoch: 106 [207360/225000 (92%)] Loss: 19570.199219\n",
      "Train Epoch: 106 [209856/225000 (93%)] Loss: 19446.330078\n",
      "Train Epoch: 106 [212352/225000 (94%)] Loss: 19476.949219\n",
      "Train Epoch: 106 [214848/225000 (95%)] Loss: 19842.167969\n",
      "Train Epoch: 106 [217344/225000 (97%)] Loss: 19304.433594\n",
      "Train Epoch: 106 [219840/225000 (98%)] Loss: 19334.574219\n",
      "Train Epoch: 106 [222336/225000 (99%)] Loss: 19436.222656\n",
      "Train Epoch: 106 [224832/225000 (100%)] Loss: 19404.537109\n",
      "    epoch          : 106\n",
      "    loss           : 19455.143772997548\n",
      "    val_loss       : 19361.504189918058\n",
      "Train Epoch: 107 [192/225000 (0%)] Loss: 19395.238281\n",
      "Train Epoch: 107 [2688/225000 (1%)] Loss: 19191.761719\n",
      "Train Epoch: 107 [5184/225000 (2%)] Loss: 19528.503906\n",
      "Train Epoch: 107 [7680/225000 (3%)] Loss: 19687.746094\n",
      "Train Epoch: 107 [10176/225000 (5%)] Loss: 19172.863281\n",
      "Train Epoch: 107 [12672/225000 (6%)] Loss: 19639.828125\n",
      "Train Epoch: 107 [15168/225000 (7%)] Loss: 19386.070312\n",
      "Train Epoch: 107 [17664/225000 (8%)] Loss: 19230.332031\n",
      "Train Epoch: 107 [20160/225000 (9%)] Loss: 19336.253906\n",
      "Train Epoch: 107 [22656/225000 (10%)] Loss: 19151.457031\n",
      "Train Epoch: 107 [25152/225000 (11%)] Loss: 19374.843750\n",
      "Train Epoch: 107 [27648/225000 (12%)] Loss: 19037.628906\n",
      "Train Epoch: 107 [30144/225000 (13%)] Loss: 19551.070312\n",
      "Train Epoch: 107 [32640/225000 (15%)] Loss: 19390.617188\n",
      "Train Epoch: 107 [35136/225000 (16%)] Loss: 19480.289062\n",
      "Train Epoch: 107 [37632/225000 (17%)] Loss: 19665.945312\n",
      "Train Epoch: 107 [40128/225000 (18%)] Loss: 19403.679688\n",
      "Train Epoch: 107 [42624/225000 (19%)] Loss: 19229.925781\n",
      "Train Epoch: 107 [45120/225000 (20%)] Loss: 19271.089844\n",
      "Train Epoch: 107 [47616/225000 (21%)] Loss: 19150.281250\n",
      "Train Epoch: 107 [50112/225000 (22%)] Loss: 19261.062500\n",
      "Train Epoch: 107 [52608/225000 (23%)] Loss: 19283.640625\n",
      "Train Epoch: 107 [55104/225000 (24%)] Loss: 19202.820312\n",
      "Train Epoch: 107 [57600/225000 (26%)] Loss: 19236.251953\n",
      "Train Epoch: 107 [60096/225000 (27%)] Loss: 19685.230469\n",
      "Train Epoch: 107 [62592/225000 (28%)] Loss: 19889.371094\n",
      "Train Epoch: 107 [65088/225000 (29%)] Loss: 19516.880859\n",
      "Train Epoch: 107 [67584/225000 (30%)] Loss: 19283.556641\n",
      "Train Epoch: 107 [70080/225000 (31%)] Loss: 19116.078125\n",
      "Train Epoch: 107 [72576/225000 (32%)] Loss: 19899.425781\n",
      "Train Epoch: 107 [75072/225000 (33%)] Loss: 19141.400391\n",
      "Train Epoch: 107 [77568/225000 (34%)] Loss: 19949.244141\n",
      "Train Epoch: 107 [80064/225000 (36%)] Loss: 19647.984375\n",
      "Train Epoch: 107 [82560/225000 (37%)] Loss: 19172.123047\n",
      "Train Epoch: 107 [85056/225000 (38%)] Loss: 19779.722656\n",
      "Train Epoch: 107 [87552/225000 (39%)] Loss: 19108.392578\n",
      "Train Epoch: 107 [90048/225000 (40%)] Loss: 19345.525391\n",
      "Train Epoch: 107 [92544/225000 (41%)] Loss: 19393.533203\n",
      "Train Epoch: 107 [95040/225000 (42%)] Loss: 19043.820312\n",
      "Train Epoch: 107 [97536/225000 (43%)] Loss: 19589.814453\n",
      "Train Epoch: 107 [100032/225000 (44%)] Loss: 19799.361328\n",
      "Train Epoch: 107 [102528/225000 (46%)] Loss: 19353.693359\n",
      "Train Epoch: 107 [105024/225000 (47%)] Loss: 19442.664062\n",
      "Train Epoch: 107 [107520/225000 (48%)] Loss: 19330.433594\n",
      "Train Epoch: 107 [110016/225000 (49%)] Loss: 19884.947266\n",
      "Train Epoch: 107 [112512/225000 (50%)] Loss: 19646.699219\n",
      "Train Epoch: 107 [115008/225000 (51%)] Loss: 19748.658203\n",
      "Train Epoch: 107 [117504/225000 (52%)] Loss: 19237.992188\n",
      "Train Epoch: 107 [120000/225000 (53%)] Loss: 19640.863281\n",
      "Train Epoch: 107 [122496/225000 (54%)] Loss: 19058.406250\n",
      "Train Epoch: 107 [124992/225000 (56%)] Loss: 19600.466797\n",
      "Train Epoch: 107 [127488/225000 (57%)] Loss: 19383.685547\n",
      "Train Epoch: 107 [129984/225000 (58%)] Loss: 19289.546875\n",
      "Train Epoch: 107 [132480/225000 (59%)] Loss: 19183.490234\n",
      "Train Epoch: 107 [134976/225000 (60%)] Loss: 19343.162109\n",
      "Train Epoch: 107 [137472/225000 (61%)] Loss: 19073.041016\n",
      "Train Epoch: 107 [139968/225000 (62%)] Loss: 19477.998047\n",
      "Train Epoch: 107 [142464/225000 (63%)] Loss: 19625.156250\n",
      "Train Epoch: 107 [144960/225000 (64%)] Loss: 19287.978516\n",
      "Train Epoch: 107 [147456/225000 (66%)] Loss: 19162.304688\n",
      "Train Epoch: 107 [149952/225000 (67%)] Loss: 19607.935547\n",
      "Train Epoch: 107 [152448/225000 (68%)] Loss: 19562.597656\n",
      "Train Epoch: 107 [154944/225000 (69%)] Loss: 18939.072266\n",
      "Train Epoch: 107 [157440/225000 (70%)] Loss: 19192.878906\n",
      "Train Epoch: 107 [159936/225000 (71%)] Loss: 19749.933594\n",
      "Train Epoch: 107 [162432/225000 (72%)] Loss: 19459.253906\n",
      "Train Epoch: 107 [164928/225000 (73%)] Loss: 19454.046875\n",
      "Train Epoch: 107 [167424/225000 (74%)] Loss: 19502.867188\n",
      "Train Epoch: 107 [169920/225000 (76%)] Loss: 19779.511719\n",
      "Train Epoch: 107 [172416/225000 (77%)] Loss: 19500.855469\n",
      "Train Epoch: 107 [174912/225000 (78%)] Loss: 19652.593750\n",
      "Train Epoch: 107 [177408/225000 (79%)] Loss: 18863.767578\n",
      "Train Epoch: 107 [179904/225000 (80%)] Loss: 19687.208984\n",
      "Train Epoch: 107 [182400/225000 (81%)] Loss: 19343.171875\n",
      "Train Epoch: 107 [184896/225000 (82%)] Loss: 19881.167969\n",
      "Train Epoch: 107 [187392/225000 (83%)] Loss: 18926.240234\n",
      "Train Epoch: 107 [189888/225000 (84%)] Loss: 19438.773438\n",
      "Train Epoch: 107 [192384/225000 (86%)] Loss: 19709.650391\n",
      "Train Epoch: 107 [194880/225000 (87%)] Loss: 19288.421875\n",
      "Train Epoch: 107 [197376/225000 (88%)] Loss: 19216.113281\n",
      "Train Epoch: 107 [199872/225000 (89%)] Loss: 19466.386719\n",
      "Train Epoch: 107 [202368/225000 (90%)] Loss: 19445.898438\n",
      "Train Epoch: 107 [204864/225000 (91%)] Loss: 19845.919922\n",
      "Train Epoch: 107 [207360/225000 (92%)] Loss: 18985.898438\n",
      "Train Epoch: 107 [209856/225000 (93%)] Loss: 19295.605469\n",
      "Train Epoch: 107 [212352/225000 (94%)] Loss: 19301.164062\n",
      "Train Epoch: 107 [214848/225000 (95%)] Loss: 19672.277344\n",
      "Train Epoch: 107 [217344/225000 (97%)] Loss: 19440.992188\n",
      "Train Epoch: 107 [219840/225000 (98%)] Loss: 19123.167969\n",
      "Train Epoch: 107 [222336/225000 (99%)] Loss: 19421.519531\n",
      "Train Epoch: 107 [224832/225000 (100%)] Loss: 19174.445312\n",
      "    epoch          : 107\n",
      "    loss           : 19451.761892064846\n",
      "    val_loss       : 19344.420214184367\n",
      "Train Epoch: 108 [192/225000 (0%)] Loss: 19722.646484\n",
      "Train Epoch: 108 [2688/225000 (1%)] Loss: 19363.355469\n",
      "Train Epoch: 108 [5184/225000 (2%)] Loss: 19356.496094\n",
      "Train Epoch: 108 [7680/225000 (3%)] Loss: 19424.269531\n",
      "Train Epoch: 108 [10176/225000 (5%)] Loss: 19167.609375\n",
      "Train Epoch: 108 [12672/225000 (6%)] Loss: 19502.519531\n",
      "Train Epoch: 108 [15168/225000 (7%)] Loss: 19623.714844\n",
      "Train Epoch: 108 [17664/225000 (8%)] Loss: 19701.238281\n",
      "Train Epoch: 108 [20160/225000 (9%)] Loss: 19455.742188\n",
      "Train Epoch: 108 [22656/225000 (10%)] Loss: 18986.640625\n",
      "Train Epoch: 108 [25152/225000 (11%)] Loss: 19481.097656\n",
      "Train Epoch: 108 [27648/225000 (12%)] Loss: 19491.566406\n",
      "Train Epoch: 108 [30144/225000 (13%)] Loss: 19680.238281\n",
      "Train Epoch: 108 [32640/225000 (15%)] Loss: 19657.490234\n",
      "Train Epoch: 108 [35136/225000 (16%)] Loss: 19238.884766\n",
      "Train Epoch: 108 [37632/225000 (17%)] Loss: 20029.820312\n",
      "Train Epoch: 108 [40128/225000 (18%)] Loss: 19241.837891\n",
      "Train Epoch: 108 [42624/225000 (19%)] Loss: 19325.746094\n",
      "Train Epoch: 108 [45120/225000 (20%)] Loss: 19791.144531\n",
      "Train Epoch: 108 [47616/225000 (21%)] Loss: 19785.978516\n",
      "Train Epoch: 108 [50112/225000 (22%)] Loss: 19938.679688\n",
      "Train Epoch: 108 [52608/225000 (23%)] Loss: 19036.871094\n",
      "Train Epoch: 108 [55104/225000 (24%)] Loss: 19451.082031\n",
      "Train Epoch: 108 [57600/225000 (26%)] Loss: 19348.531250\n",
      "Train Epoch: 108 [60096/225000 (27%)] Loss: 19895.839844\n",
      "Train Epoch: 108 [62592/225000 (28%)] Loss: 19953.824219\n",
      "Train Epoch: 108 [65088/225000 (29%)] Loss: 19269.507812\n",
      "Train Epoch: 108 [67584/225000 (30%)] Loss: 19286.048828\n",
      "Train Epoch: 108 [70080/225000 (31%)] Loss: 19571.250000\n",
      "Train Epoch: 108 [72576/225000 (32%)] Loss: 19362.500000\n",
      "Train Epoch: 108 [75072/225000 (33%)] Loss: 19662.906250\n",
      "Train Epoch: 108 [77568/225000 (34%)] Loss: 19565.804688\n",
      "Train Epoch: 108 [80064/225000 (36%)] Loss: 19080.441406\n",
      "Train Epoch: 108 [82560/225000 (37%)] Loss: 19761.957031\n",
      "Train Epoch: 108 [85056/225000 (38%)] Loss: 19509.097656\n",
      "Train Epoch: 108 [87552/225000 (39%)] Loss: 19529.515625\n",
      "Train Epoch: 108 [90048/225000 (40%)] Loss: 19319.902344\n",
      "Train Epoch: 108 [92544/225000 (41%)] Loss: 19218.179688\n",
      "Train Epoch: 108 [95040/225000 (42%)] Loss: 19476.742188\n",
      "Train Epoch: 108 [97536/225000 (43%)] Loss: 19222.960938\n",
      "Train Epoch: 108 [100032/225000 (44%)] Loss: 19535.744141\n",
      "Train Epoch: 108 [102528/225000 (46%)] Loss: 19797.367188\n",
      "Train Epoch: 108 [105024/225000 (47%)] Loss: 19766.664062\n",
      "Train Epoch: 108 [107520/225000 (48%)] Loss: 19814.421875\n",
      "Train Epoch: 108 [110016/225000 (49%)] Loss: 19317.578125\n",
      "Train Epoch: 108 [112512/225000 (50%)] Loss: 19769.160156\n",
      "Train Epoch: 108 [115008/225000 (51%)] Loss: 19390.246094\n",
      "Train Epoch: 108 [117504/225000 (52%)] Loss: 19658.859375\n",
      "Train Epoch: 108 [120000/225000 (53%)] Loss: 19352.234375\n",
      "Train Epoch: 108 [122496/225000 (54%)] Loss: 19494.974609\n",
      "Train Epoch: 108 [124992/225000 (56%)] Loss: 19321.083984\n",
      "Train Epoch: 108 [127488/225000 (57%)] Loss: 18865.841797\n",
      "Train Epoch: 108 [129984/225000 (58%)] Loss: 19616.667969\n",
      "Train Epoch: 108 [132480/225000 (59%)] Loss: 19296.914062\n",
      "Train Epoch: 108 [134976/225000 (60%)] Loss: 19326.056641\n",
      "Train Epoch: 108 [137472/225000 (61%)] Loss: 19125.601562\n",
      "Train Epoch: 108 [139968/225000 (62%)] Loss: 18976.990234\n",
      "Train Epoch: 108 [142464/225000 (63%)] Loss: 19167.683594\n",
      "Train Epoch: 108 [144960/225000 (64%)] Loss: 19542.429688\n",
      "Train Epoch: 108 [147456/225000 (66%)] Loss: 19253.093750\n",
      "Train Epoch: 108 [149952/225000 (67%)] Loss: 19408.671875\n",
      "Train Epoch: 108 [152448/225000 (68%)] Loss: 19018.283203\n",
      "Train Epoch: 108 [154944/225000 (69%)] Loss: 19740.939453\n",
      "Train Epoch: 108 [157440/225000 (70%)] Loss: 19330.480469\n",
      "Train Epoch: 108 [159936/225000 (71%)] Loss: 19681.648438\n",
      "Train Epoch: 108 [162432/225000 (72%)] Loss: 19552.675781\n",
      "Train Epoch: 108 [164928/225000 (73%)] Loss: 19601.527344\n",
      "Train Epoch: 108 [167424/225000 (74%)] Loss: 19502.082031\n",
      "Train Epoch: 108 [169920/225000 (76%)] Loss: 18957.660156\n",
      "Train Epoch: 108 [172416/225000 (77%)] Loss: 19359.164062\n",
      "Train Epoch: 108 [174912/225000 (78%)] Loss: 19262.478516\n",
      "Train Epoch: 108 [177408/225000 (79%)] Loss: 19441.621094\n",
      "Train Epoch: 108 [179904/225000 (80%)] Loss: 19432.398438\n",
      "Train Epoch: 108 [182400/225000 (81%)] Loss: 19806.179688\n",
      "Train Epoch: 108 [184896/225000 (82%)] Loss: 19540.265625\n",
      "Train Epoch: 108 [187392/225000 (83%)] Loss: 19293.503906\n",
      "Train Epoch: 108 [189888/225000 (84%)] Loss: 19368.562500\n",
      "Train Epoch: 108 [192384/225000 (86%)] Loss: 19721.041016\n",
      "Train Epoch: 108 [194880/225000 (87%)] Loss: 19356.039062\n",
      "Train Epoch: 108 [197376/225000 (88%)] Loss: 19317.177734\n",
      "Train Epoch: 108 [199872/225000 (89%)] Loss: 19028.082031\n",
      "Train Epoch: 108 [202368/225000 (90%)] Loss: 19416.421875\n",
      "Train Epoch: 108 [204864/225000 (91%)] Loss: 19201.822266\n",
      "Train Epoch: 108 [207360/225000 (92%)] Loss: 19876.951172\n",
      "Train Epoch: 108 [209856/225000 (93%)] Loss: 19678.082031\n",
      "Train Epoch: 108 [212352/225000 (94%)] Loss: 19164.701172\n",
      "Train Epoch: 108 [214848/225000 (95%)] Loss: 19358.152344\n",
      "Train Epoch: 108 [217344/225000 (97%)] Loss: 19152.900391\n",
      "Train Epoch: 108 [219840/225000 (98%)] Loss: 19540.683594\n",
      "Train Epoch: 108 [222336/225000 (99%)] Loss: 19537.007812\n",
      "Train Epoch: 108 [224832/225000 (100%)] Loss: 19231.962891\n",
      "    epoch          : 108\n",
      "    loss           : 19440.103707271224\n",
      "    val_loss       : 19384.77122038603\n",
      "Train Epoch: 109 [192/225000 (0%)] Loss: 19898.988281\n",
      "Train Epoch: 109 [2688/225000 (1%)] Loss: 19246.958984\n",
      "Train Epoch: 109 [5184/225000 (2%)] Loss: 19549.710938\n",
      "Train Epoch: 109 [7680/225000 (3%)] Loss: 19246.751953\n",
      "Train Epoch: 109 [10176/225000 (5%)] Loss: 19437.908203\n",
      "Train Epoch: 109 [12672/225000 (6%)] Loss: 19898.349609\n",
      "Train Epoch: 109 [15168/225000 (7%)] Loss: 19701.162109\n",
      "Train Epoch: 109 [17664/225000 (8%)] Loss: 19588.207031\n",
      "Train Epoch: 109 [20160/225000 (9%)] Loss: 19488.960938\n",
      "Train Epoch: 109 [22656/225000 (10%)] Loss: 19555.496094\n",
      "Train Epoch: 109 [25152/225000 (11%)] Loss: 19700.863281\n",
      "Train Epoch: 109 [27648/225000 (12%)] Loss: 19728.277344\n",
      "Train Epoch: 109 [30144/225000 (13%)] Loss: 19270.873047\n",
      "Train Epoch: 109 [32640/225000 (15%)] Loss: 19225.806641\n",
      "Train Epoch: 109 [35136/225000 (16%)] Loss: 19542.976562\n",
      "Train Epoch: 109 [37632/225000 (17%)] Loss: 19670.558594\n",
      "Train Epoch: 109 [40128/225000 (18%)] Loss: 19209.691406\n",
      "Train Epoch: 109 [42624/225000 (19%)] Loss: 19468.222656\n",
      "Train Epoch: 109 [45120/225000 (20%)] Loss: 19353.636719\n",
      "Train Epoch: 109 [47616/225000 (21%)] Loss: 19119.289062\n",
      "Train Epoch: 109 [50112/225000 (22%)] Loss: 19509.445312\n",
      "Train Epoch: 109 [52608/225000 (23%)] Loss: 19750.164062\n",
      "Train Epoch: 109 [55104/225000 (24%)] Loss: 19666.019531\n",
      "Train Epoch: 109 [57600/225000 (26%)] Loss: 19377.972656\n",
      "Train Epoch: 109 [60096/225000 (27%)] Loss: 19437.503906\n",
      "Train Epoch: 109 [62592/225000 (28%)] Loss: 19009.369141\n",
      "Train Epoch: 109 [65088/225000 (29%)] Loss: 19514.898438\n",
      "Train Epoch: 109 [67584/225000 (30%)] Loss: 19261.226562\n",
      "Train Epoch: 109 [70080/225000 (31%)] Loss: 19609.568359\n",
      "Train Epoch: 109 [72576/225000 (32%)] Loss: 19806.453125\n",
      "Train Epoch: 109 [75072/225000 (33%)] Loss: 19285.244141\n",
      "Train Epoch: 109 [77568/225000 (34%)] Loss: 19607.503906\n",
      "Train Epoch: 109 [80064/225000 (36%)] Loss: 19427.734375\n",
      "Train Epoch: 109 [82560/225000 (37%)] Loss: 19541.503906\n",
      "Train Epoch: 109 [85056/225000 (38%)] Loss: 19271.492188\n",
      "Train Epoch: 109 [87552/225000 (39%)] Loss: 19696.757812\n",
      "Train Epoch: 109 [90048/225000 (40%)] Loss: 19335.308594\n",
      "Train Epoch: 109 [92544/225000 (41%)] Loss: 19149.914062\n",
      "Train Epoch: 109 [95040/225000 (42%)] Loss: 19296.386719\n",
      "Train Epoch: 109 [97536/225000 (43%)] Loss: 19352.671875\n",
      "Train Epoch: 109 [100032/225000 (44%)] Loss: 19518.027344\n",
      "Train Epoch: 109 [102528/225000 (46%)] Loss: 19169.429688\n",
      "Train Epoch: 109 [105024/225000 (47%)] Loss: 19195.253906\n",
      "Train Epoch: 109 [107520/225000 (48%)] Loss: 18792.148438\n",
      "Train Epoch: 109 [110016/225000 (49%)] Loss: 19086.511719\n",
      "Train Epoch: 109 [112512/225000 (50%)] Loss: 19208.968750\n",
      "Train Epoch: 109 [115008/225000 (51%)] Loss: 19335.218750\n",
      "Train Epoch: 109 [117504/225000 (52%)] Loss: 19701.351562\n",
      "Train Epoch: 109 [120000/225000 (53%)] Loss: 19281.185547\n",
      "Train Epoch: 109 [122496/225000 (54%)] Loss: 19804.609375\n",
      "Train Epoch: 109 [124992/225000 (56%)] Loss: 19701.230469\n",
      "Train Epoch: 109 [127488/225000 (57%)] Loss: 19842.761719\n",
      "Train Epoch: 109 [129984/225000 (58%)] Loss: 19300.687500\n",
      "Train Epoch: 109 [132480/225000 (59%)] Loss: 19051.097656\n",
      "Train Epoch: 109 [134976/225000 (60%)] Loss: 19323.880859\n",
      "Train Epoch: 109 [137472/225000 (61%)] Loss: 19314.865234\n",
      "Train Epoch: 109 [139968/225000 (62%)] Loss: 19613.578125\n",
      "Train Epoch: 109 [142464/225000 (63%)] Loss: 18759.890625\n",
      "Train Epoch: 109 [144960/225000 (64%)] Loss: 19670.582031\n",
      "Train Epoch: 109 [147456/225000 (66%)] Loss: 19761.011719\n",
      "Train Epoch: 109 [149952/225000 (67%)] Loss: 19188.464844\n",
      "Train Epoch: 109 [152448/225000 (68%)] Loss: 19224.892578\n",
      "Train Epoch: 109 [154944/225000 (69%)] Loss: 19799.933594\n",
      "Train Epoch: 109 [157440/225000 (70%)] Loss: 19640.773438\n",
      "Train Epoch: 109 [159936/225000 (71%)] Loss: 19632.062500\n",
      "Train Epoch: 109 [162432/225000 (72%)] Loss: 18788.839844\n",
      "Train Epoch: 109 [164928/225000 (73%)] Loss: 19905.042969\n",
      "Train Epoch: 109 [167424/225000 (74%)] Loss: 19211.347656\n",
      "Train Epoch: 109 [169920/225000 (76%)] Loss: 19561.460938\n",
      "Train Epoch: 109 [172416/225000 (77%)] Loss: 19396.353516\n",
      "Train Epoch: 109 [174912/225000 (78%)] Loss: 19446.947266\n",
      "Train Epoch: 109 [177408/225000 (79%)] Loss: 19092.300781\n",
      "Train Epoch: 109 [179904/225000 (80%)] Loss: 19776.119141\n",
      "Train Epoch: 109 [182400/225000 (81%)] Loss: 19096.023438\n",
      "Train Epoch: 109 [184896/225000 (82%)] Loss: 19278.921875\n",
      "Train Epoch: 109 [187392/225000 (83%)] Loss: 18911.359375\n",
      "Train Epoch: 109 [189888/225000 (84%)] Loss: 19956.597656\n",
      "Train Epoch: 109 [192384/225000 (86%)] Loss: 19169.914062\n",
      "Train Epoch: 109 [194880/225000 (87%)] Loss: 19425.828125\n",
      "Train Epoch: 109 [197376/225000 (88%)] Loss: 19803.699219\n",
      "Train Epoch: 109 [199872/225000 (89%)] Loss: 18909.515625\n",
      "Train Epoch: 109 [202368/225000 (90%)] Loss: 19161.691406\n",
      "Train Epoch: 109 [204864/225000 (91%)] Loss: 19332.351562\n",
      "Train Epoch: 109 [207360/225000 (92%)] Loss: 18826.357422\n",
      "Train Epoch: 109 [209856/225000 (93%)] Loss: 19333.238281\n",
      "Train Epoch: 109 [212352/225000 (94%)] Loss: 19511.296875\n",
      "Train Epoch: 109 [214848/225000 (95%)] Loss: 19330.019531\n",
      "Train Epoch: 109 [217344/225000 (97%)] Loss: 19476.437500\n",
      "Train Epoch: 109 [219840/225000 (98%)] Loss: 19430.398438\n",
      "Train Epoch: 109 [222336/225000 (99%)] Loss: 19692.628906\n",
      "Train Epoch: 109 [224832/225000 (100%)] Loss: 19478.437500\n",
      "    epoch          : 109\n",
      "    loss           : 19435.71856501973\n",
      "    val_loss       : 19340.2229185573\n",
      "Train Epoch: 110 [192/225000 (0%)] Loss: 19662.218750\n",
      "Train Epoch: 110 [2688/225000 (1%)] Loss: 18921.994141\n",
      "Train Epoch: 110 [5184/225000 (2%)] Loss: 19517.917969\n",
      "Train Epoch: 110 [7680/225000 (3%)] Loss: 19737.626953\n",
      "Train Epoch: 110 [10176/225000 (5%)] Loss: 19467.109375\n",
      "Train Epoch: 110 [12672/225000 (6%)] Loss: 19114.691406\n",
      "Train Epoch: 110 [15168/225000 (7%)] Loss: 19708.691406\n",
      "Train Epoch: 110 [17664/225000 (8%)] Loss: 19142.761719\n",
      "Train Epoch: 110 [20160/225000 (9%)] Loss: 20006.255859\n",
      "Train Epoch: 110 [22656/225000 (10%)] Loss: 19594.894531\n",
      "Train Epoch: 110 [25152/225000 (11%)] Loss: 19411.433594\n",
      "Train Epoch: 110 [27648/225000 (12%)] Loss: 18967.380859\n",
      "Train Epoch: 110 [30144/225000 (13%)] Loss: 19730.800781\n",
      "Train Epoch: 110 [32640/225000 (15%)] Loss: 19678.769531\n",
      "Train Epoch: 110 [35136/225000 (16%)] Loss: 19705.900391\n",
      "Train Epoch: 110 [37632/225000 (17%)] Loss: 19517.468750\n",
      "Train Epoch: 110 [40128/225000 (18%)] Loss: 19778.904297\n",
      "Train Epoch: 110 [42624/225000 (19%)] Loss: 19125.710938\n",
      "Train Epoch: 110 [45120/225000 (20%)] Loss: 19594.437500\n",
      "Train Epoch: 110 [47616/225000 (21%)] Loss: 19513.962891\n",
      "Train Epoch: 110 [50112/225000 (22%)] Loss: 19460.714844\n",
      "Train Epoch: 110 [52608/225000 (23%)] Loss: 19544.730469\n",
      "Train Epoch: 110 [55104/225000 (24%)] Loss: 19301.857422\n",
      "Train Epoch: 110 [57600/225000 (26%)] Loss: 19507.253906\n",
      "Train Epoch: 110 [60096/225000 (27%)] Loss: 19290.296875\n",
      "Train Epoch: 110 [62592/225000 (28%)] Loss: 19887.828125\n",
      "Train Epoch: 110 [65088/225000 (29%)] Loss: 19535.023438\n",
      "Train Epoch: 110 [67584/225000 (30%)] Loss: 19616.046875\n",
      "Train Epoch: 110 [70080/225000 (31%)] Loss: 19660.902344\n",
      "Train Epoch: 110 [72576/225000 (32%)] Loss: 19224.074219\n",
      "Train Epoch: 110 [75072/225000 (33%)] Loss: 19576.484375\n",
      "Train Epoch: 110 [77568/225000 (34%)] Loss: 18962.273438\n",
      "Train Epoch: 110 [80064/225000 (36%)] Loss: 19337.568359\n",
      "Train Epoch: 110 [82560/225000 (37%)] Loss: 19712.230469\n",
      "Train Epoch: 110 [85056/225000 (38%)] Loss: 19259.546875\n",
      "Train Epoch: 110 [87552/225000 (39%)] Loss: 19098.230469\n",
      "Train Epoch: 110 [90048/225000 (40%)] Loss: 19622.531250\n",
      "Train Epoch: 110 [92544/225000 (41%)] Loss: 19581.546875\n",
      "Train Epoch: 110 [95040/225000 (42%)] Loss: 19560.769531\n",
      "Train Epoch: 110 [97536/225000 (43%)] Loss: 19158.234375\n",
      "Train Epoch: 110 [100032/225000 (44%)] Loss: 19456.646484\n",
      "Train Epoch: 110 [102528/225000 (46%)] Loss: 19212.537109\n",
      "Train Epoch: 110 [105024/225000 (47%)] Loss: 19487.312500\n",
      "Train Epoch: 110 [107520/225000 (48%)] Loss: 19645.171875\n",
      "Train Epoch: 110 [110016/225000 (49%)] Loss: 19464.875000\n",
      "Train Epoch: 110 [112512/225000 (50%)] Loss: 19427.732422\n",
      "Train Epoch: 110 [115008/225000 (51%)] Loss: 19485.287109\n",
      "Train Epoch: 110 [117504/225000 (52%)] Loss: 19056.125000\n",
      "Train Epoch: 110 [120000/225000 (53%)] Loss: 19529.568359\n",
      "Train Epoch: 110 [122496/225000 (54%)] Loss: 19387.500000\n",
      "Train Epoch: 110 [124992/225000 (56%)] Loss: 19351.441406\n",
      "Train Epoch: 110 [127488/225000 (57%)] Loss: 19343.632812\n",
      "Train Epoch: 110 [129984/225000 (58%)] Loss: 19339.351562\n",
      "Train Epoch: 110 [132480/225000 (59%)] Loss: 19505.427734\n",
      "Train Epoch: 110 [134976/225000 (60%)] Loss: 19381.869141\n",
      "Train Epoch: 110 [137472/225000 (61%)] Loss: 19100.107422\n",
      "Train Epoch: 110 [139968/225000 (62%)] Loss: 19511.060547\n",
      "Train Epoch: 110 [142464/225000 (63%)] Loss: 19175.035156\n",
      "Train Epoch: 110 [144960/225000 (64%)] Loss: 19746.839844\n",
      "Train Epoch: 110 [147456/225000 (66%)] Loss: 19616.039062\n",
      "Train Epoch: 110 [149952/225000 (67%)] Loss: 18903.998047\n",
      "Train Epoch: 110 [152448/225000 (68%)] Loss: 19591.863281\n",
      "Train Epoch: 110 [154944/225000 (69%)] Loss: 19725.453125\n",
      "Train Epoch: 110 [157440/225000 (70%)] Loss: 19476.675781\n",
      "Train Epoch: 110 [159936/225000 (71%)] Loss: 19747.472656\n",
      "Train Epoch: 110 [162432/225000 (72%)] Loss: 19618.257812\n",
      "Train Epoch: 110 [164928/225000 (73%)] Loss: 19752.175781\n",
      "Train Epoch: 110 [167424/225000 (74%)] Loss: 19302.085938\n",
      "Train Epoch: 110 [169920/225000 (76%)] Loss: 18659.761719\n",
      "Train Epoch: 110 [172416/225000 (77%)] Loss: 19455.080078\n",
      "Train Epoch: 110 [174912/225000 (78%)] Loss: 19311.378906\n",
      "Train Epoch: 110 [177408/225000 (79%)] Loss: 19716.912109\n",
      "Train Epoch: 110 [179904/225000 (80%)] Loss: 19288.527344\n",
      "Train Epoch: 110 [182400/225000 (81%)] Loss: 19638.019531\n",
      "Train Epoch: 110 [184896/225000 (82%)] Loss: 19424.279297\n",
      "Train Epoch: 110 [187392/225000 (83%)] Loss: 19074.195312\n",
      "Train Epoch: 110 [189888/225000 (84%)] Loss: 19659.636719\n",
      "Train Epoch: 110 [192384/225000 (86%)] Loss: 19438.816406\n",
      "Train Epoch: 110 [194880/225000 (87%)] Loss: 19212.744141\n",
      "Train Epoch: 110 [197376/225000 (88%)] Loss: 19839.638672\n",
      "Train Epoch: 110 [199872/225000 (89%)] Loss: 19666.166016\n",
      "Train Epoch: 110 [202368/225000 (90%)] Loss: 19296.988281\n",
      "Train Epoch: 110 [204864/225000 (91%)] Loss: 19454.656250\n",
      "Train Epoch: 110 [207360/225000 (92%)] Loss: 19641.804688\n",
      "Train Epoch: 110 [209856/225000 (93%)] Loss: 19461.535156\n",
      "Train Epoch: 110 [212352/225000 (94%)] Loss: 19339.683594\n",
      "Train Epoch: 110 [214848/225000 (95%)] Loss: 19495.375000\n",
      "Train Epoch: 110 [217344/225000 (97%)] Loss: 19785.619141\n",
      "Train Epoch: 110 [219840/225000 (98%)] Loss: 19219.796875\n",
      "Train Epoch: 110 [222336/225000 (99%)] Loss: 19645.164062\n",
      "Train Epoch: 110 [224832/225000 (100%)] Loss: 19499.464844\n",
      "    epoch          : 110\n",
      "    loss           : 19432.153758599085\n",
      "    val_loss       : 19345.3859200878\n",
      "Train Epoch: 111 [192/225000 (0%)] Loss: 19218.181641\n",
      "Train Epoch: 111 [2688/225000 (1%)] Loss: 19415.259766\n",
      "Train Epoch: 111 [5184/225000 (2%)] Loss: 18689.222656\n",
      "Train Epoch: 111 [7680/225000 (3%)] Loss: 19164.255859\n",
      "Train Epoch: 111 [10176/225000 (5%)] Loss: 19421.572266\n",
      "Train Epoch: 111 [12672/225000 (6%)] Loss: 19588.527344\n",
      "Train Epoch: 111 [15168/225000 (7%)] Loss: 19488.734375\n",
      "Train Epoch: 111 [17664/225000 (8%)] Loss: 19530.910156\n",
      "Train Epoch: 111 [20160/225000 (9%)] Loss: 19418.480469\n",
      "Train Epoch: 111 [22656/225000 (10%)] Loss: 19430.332031\n",
      "Train Epoch: 111 [25152/225000 (11%)] Loss: 19127.734375\n",
      "Train Epoch: 111 [27648/225000 (12%)] Loss: 19189.154297\n",
      "Train Epoch: 111 [30144/225000 (13%)] Loss: 19524.113281\n",
      "Train Epoch: 111 [32640/225000 (15%)] Loss: 19651.253906\n",
      "Train Epoch: 111 [35136/225000 (16%)] Loss: 19638.599609\n",
      "Train Epoch: 111 [37632/225000 (17%)] Loss: 19376.851562\n",
      "Train Epoch: 111 [40128/225000 (18%)] Loss: 18649.033203\n",
      "Train Epoch: 111 [42624/225000 (19%)] Loss: 19519.281250\n",
      "Train Epoch: 111 [45120/225000 (20%)] Loss: 19242.335938\n",
      "Train Epoch: 111 [47616/225000 (21%)] Loss: 19290.820312\n",
      "Train Epoch: 111 [50112/225000 (22%)] Loss: 19289.812500\n",
      "Train Epoch: 111 [52608/225000 (23%)] Loss: 19227.851562\n",
      "Train Epoch: 111 [55104/225000 (24%)] Loss: 19255.003906\n",
      "Train Epoch: 111 [57600/225000 (26%)] Loss: 19112.722656\n",
      "Train Epoch: 111 [60096/225000 (27%)] Loss: 19653.019531\n",
      "Train Epoch: 111 [62592/225000 (28%)] Loss: 19095.976562\n",
      "Train Epoch: 111 [65088/225000 (29%)] Loss: 19570.910156\n",
      "Train Epoch: 111 [67584/225000 (30%)] Loss: 18660.480469\n",
      "Train Epoch: 111 [70080/225000 (31%)] Loss: 19832.187500\n",
      "Train Epoch: 111 [72576/225000 (32%)] Loss: 19327.753906\n",
      "Train Epoch: 111 [75072/225000 (33%)] Loss: 19426.740234\n",
      "Train Epoch: 111 [77568/225000 (34%)] Loss: 19696.363281\n",
      "Train Epoch: 111 [80064/225000 (36%)] Loss: 19687.632812\n",
      "Train Epoch: 111 [82560/225000 (37%)] Loss: 19547.166016\n",
      "Train Epoch: 111 [85056/225000 (38%)] Loss: 19305.441406\n",
      "Train Epoch: 111 [87552/225000 (39%)] Loss: 19653.144531\n",
      "Train Epoch: 111 [90048/225000 (40%)] Loss: 19766.816406\n",
      "Train Epoch: 111 [92544/225000 (41%)] Loss: 19094.777344\n",
      "Train Epoch: 111 [95040/225000 (42%)] Loss: 19349.876953\n",
      "Train Epoch: 111 [97536/225000 (43%)] Loss: 19613.777344\n",
      "Train Epoch: 111 [100032/225000 (44%)] Loss: 19071.458984\n",
      "Train Epoch: 111 [102528/225000 (46%)] Loss: 19173.347656\n",
      "Train Epoch: 111 [105024/225000 (47%)] Loss: 19086.191406\n",
      "Train Epoch: 111 [107520/225000 (48%)] Loss: 19541.777344\n",
      "Train Epoch: 111 [110016/225000 (49%)] Loss: 19406.285156\n",
      "Train Epoch: 111 [112512/225000 (50%)] Loss: 19396.507812\n",
      "Train Epoch: 111 [115008/225000 (51%)] Loss: 19547.794922\n",
      "Train Epoch: 111 [117504/225000 (52%)] Loss: 19713.597656\n",
      "Train Epoch: 111 [120000/225000 (53%)] Loss: 19803.421875\n",
      "Train Epoch: 111 [122496/225000 (54%)] Loss: 19327.167969\n",
      "Train Epoch: 111 [124992/225000 (56%)] Loss: 19315.884766\n",
      "Train Epoch: 111 [127488/225000 (57%)] Loss: 19539.675781\n",
      "Train Epoch: 111 [129984/225000 (58%)] Loss: 19530.000000\n",
      "Train Epoch: 111 [132480/225000 (59%)] Loss: 19080.218750\n",
      "Train Epoch: 111 [134976/225000 (60%)] Loss: 19565.839844\n",
      "Train Epoch: 111 [137472/225000 (61%)] Loss: 19252.423828\n",
      "Train Epoch: 111 [139968/225000 (62%)] Loss: 19529.742188\n",
      "Train Epoch: 111 [142464/225000 (63%)] Loss: 19283.574219\n",
      "Train Epoch: 111 [144960/225000 (64%)] Loss: 19147.984375\n",
      "Train Epoch: 111 [147456/225000 (66%)] Loss: 19536.761719\n",
      "Train Epoch: 111 [149952/225000 (67%)] Loss: 19634.359375\n",
      "Train Epoch: 111 [152448/225000 (68%)] Loss: 19499.375000\n",
      "Train Epoch: 111 [154944/225000 (69%)] Loss: 18868.699219\n",
      "Train Epoch: 111 [157440/225000 (70%)] Loss: 19287.082031\n",
      "Train Epoch: 111 [159936/225000 (71%)] Loss: 19597.222656\n",
      "Train Epoch: 111 [162432/225000 (72%)] Loss: 19397.470703\n",
      "Train Epoch: 111 [164928/225000 (73%)] Loss: 19413.410156\n",
      "Train Epoch: 111 [167424/225000 (74%)] Loss: 19581.148438\n",
      "Train Epoch: 111 [169920/225000 (76%)] Loss: 19197.111328\n",
      "Train Epoch: 111 [172416/225000 (77%)] Loss: 18731.994141\n",
      "Train Epoch: 111 [174912/225000 (78%)] Loss: 19111.785156\n",
      "Train Epoch: 111 [177408/225000 (79%)] Loss: 19686.716797\n",
      "Train Epoch: 111 [179904/225000 (80%)] Loss: 19460.482422\n",
      "Train Epoch: 111 [182400/225000 (81%)] Loss: 19230.322266\n",
      "Train Epoch: 111 [184896/225000 (82%)] Loss: 19692.267578\n",
      "Train Epoch: 111 [187392/225000 (83%)] Loss: 19071.617188\n",
      "Train Epoch: 111 [189888/225000 (84%)] Loss: 19281.394531\n",
      "Train Epoch: 111 [192384/225000 (86%)] Loss: 19423.281250\n",
      "Train Epoch: 111 [194880/225000 (87%)] Loss: 19578.873047\n",
      "Train Epoch: 111 [197376/225000 (88%)] Loss: 19365.343750\n",
      "Train Epoch: 111 [199872/225000 (89%)] Loss: 19454.583984\n",
      "Train Epoch: 111 [202368/225000 (90%)] Loss: 19330.019531\n",
      "Train Epoch: 111 [204864/225000 (91%)] Loss: 19491.574219\n",
      "Train Epoch: 111 [207360/225000 (92%)] Loss: 18976.166016\n",
      "Train Epoch: 111 [209856/225000 (93%)] Loss: 18924.066406\n",
      "Train Epoch: 111 [212352/225000 (94%)] Loss: 19337.675781\n",
      "Train Epoch: 111 [214848/225000 (95%)] Loss: 19658.746094\n",
      "Train Epoch: 111 [217344/225000 (97%)] Loss: 19676.171875\n",
      "Train Epoch: 111 [219840/225000 (98%)] Loss: 19568.246094\n",
      "Train Epoch: 111 [222336/225000 (99%)] Loss: 19473.082031\n",
      "Train Epoch: 111 [224832/225000 (100%)] Loss: 19481.695312\n",
      "    epoch          : 111\n",
      "    loss           : 19420.664182487202\n",
      "    val_loss       : 19345.37752357876\n",
      "Train Epoch: 112 [192/225000 (0%)] Loss: 19781.142578\n",
      "Train Epoch: 112 [2688/225000 (1%)] Loss: 19288.863281\n",
      "Train Epoch: 112 [5184/225000 (2%)] Loss: 19127.074219\n",
      "Train Epoch: 112 [7680/225000 (3%)] Loss: 19119.656250\n",
      "Train Epoch: 112 [10176/225000 (5%)] Loss: 19483.244141\n",
      "Train Epoch: 112 [12672/225000 (6%)] Loss: 19368.193359\n",
      "Train Epoch: 112 [15168/225000 (7%)] Loss: 18782.642578\n",
      "Train Epoch: 112 [17664/225000 (8%)] Loss: 19411.519531\n",
      "Train Epoch: 112 [20160/225000 (9%)] Loss: 19298.474609\n",
      "Train Epoch: 112 [22656/225000 (10%)] Loss: 19257.214844\n",
      "Train Epoch: 112 [25152/225000 (11%)] Loss: 18882.875000\n",
      "Train Epoch: 112 [27648/225000 (12%)] Loss: 19019.185547\n",
      "Train Epoch: 112 [30144/225000 (13%)] Loss: 19567.847656\n",
      "Train Epoch: 112 [32640/225000 (15%)] Loss: 19550.718750\n",
      "Train Epoch: 112 [35136/225000 (16%)] Loss: 19497.847656\n",
      "Train Epoch: 112 [37632/225000 (17%)] Loss: 19791.658203\n",
      "Train Epoch: 112 [40128/225000 (18%)] Loss: 19389.589844\n",
      "Train Epoch: 112 [42624/225000 (19%)] Loss: 19107.238281\n",
      "Train Epoch: 112 [45120/225000 (20%)] Loss: 19871.277344\n",
      "Train Epoch: 112 [47616/225000 (21%)] Loss: 19405.355469\n",
      "Train Epoch: 112 [50112/225000 (22%)] Loss: 19636.265625\n",
      "Train Epoch: 112 [52608/225000 (23%)] Loss: 19213.253906\n",
      "Train Epoch: 112 [55104/225000 (24%)] Loss: 19502.558594\n",
      "Train Epoch: 112 [57600/225000 (26%)] Loss: 19344.597656\n",
      "Train Epoch: 112 [60096/225000 (27%)] Loss: 19649.871094\n",
      "Train Epoch: 112 [62592/225000 (28%)] Loss: 18956.566406\n",
      "Train Epoch: 112 [65088/225000 (29%)] Loss: 19456.781250\n",
      "Train Epoch: 112 [67584/225000 (30%)] Loss: 18960.648438\n",
      "Train Epoch: 112 [70080/225000 (31%)] Loss: 19363.917969\n",
      "Train Epoch: 112 [72576/225000 (32%)] Loss: 18999.634766\n",
      "Train Epoch: 112 [75072/225000 (33%)] Loss: 19938.183594\n",
      "Train Epoch: 112 [77568/225000 (34%)] Loss: 19155.490234\n",
      "Train Epoch: 112 [80064/225000 (36%)] Loss: 19465.097656\n",
      "Train Epoch: 112 [82560/225000 (37%)] Loss: 19860.263672\n",
      "Train Epoch: 112 [85056/225000 (38%)] Loss: 19541.802734\n",
      "Train Epoch: 112 [87552/225000 (39%)] Loss: 19049.205078\n",
      "Train Epoch: 112 [90048/225000 (40%)] Loss: 19539.564453\n",
      "Train Epoch: 112 [92544/225000 (41%)] Loss: 19561.585938\n",
      "Train Epoch: 112 [95040/225000 (42%)] Loss: 19619.103516\n",
      "Train Epoch: 112 [97536/225000 (43%)] Loss: 20111.896484\n",
      "Train Epoch: 112 [100032/225000 (44%)] Loss: 19588.363281\n",
      "Train Epoch: 112 [102528/225000 (46%)] Loss: 19855.531250\n",
      "Train Epoch: 112 [105024/225000 (47%)] Loss: 19775.628906\n",
      "Train Epoch: 112 [107520/225000 (48%)] Loss: 19221.789062\n",
      "Train Epoch: 112 [110016/225000 (49%)] Loss: 19826.414062\n",
      "Train Epoch: 112 [112512/225000 (50%)] Loss: 19341.433594\n",
      "Train Epoch: 112 [115008/225000 (51%)] Loss: 19196.203125\n",
      "Train Epoch: 112 [117504/225000 (52%)] Loss: 19487.445312\n",
      "Train Epoch: 112 [120000/225000 (53%)] Loss: 18984.343750\n",
      "Train Epoch: 112 [122496/225000 (54%)] Loss: 19182.134766\n",
      "Train Epoch: 112 [124992/225000 (56%)] Loss: 19433.087891\n",
      "Train Epoch: 112 [127488/225000 (57%)] Loss: 19052.449219\n",
      "Train Epoch: 112 [129984/225000 (58%)] Loss: 19421.972656\n",
      "Train Epoch: 112 [132480/225000 (59%)] Loss: 19334.316406\n",
      "Train Epoch: 112 [134976/225000 (60%)] Loss: 19130.089844\n",
      "Train Epoch: 112 [137472/225000 (61%)] Loss: 19208.892578\n",
      "Train Epoch: 112 [139968/225000 (62%)] Loss: 19902.734375\n",
      "Train Epoch: 112 [142464/225000 (63%)] Loss: 19110.375000\n",
      "Train Epoch: 112 [144960/225000 (64%)] Loss: 19462.710938\n",
      "Train Epoch: 112 [147456/225000 (66%)] Loss: 19481.574219\n",
      "Train Epoch: 112 [149952/225000 (67%)] Loss: 19525.648438\n",
      "Train Epoch: 112 [152448/225000 (68%)] Loss: 19720.601562\n",
      "Train Epoch: 112 [154944/225000 (69%)] Loss: 19843.105469\n",
      "Train Epoch: 112 [157440/225000 (70%)] Loss: 19068.398438\n",
      "Train Epoch: 112 [159936/225000 (71%)] Loss: 19072.748047\n",
      "Train Epoch: 112 [162432/225000 (72%)] Loss: 19825.210938\n",
      "Train Epoch: 112 [164928/225000 (73%)] Loss: 19067.261719\n",
      "Train Epoch: 112 [167424/225000 (74%)] Loss: 19326.306641\n",
      "Train Epoch: 112 [169920/225000 (76%)] Loss: 19709.822266\n",
      "Train Epoch: 112 [172416/225000 (77%)] Loss: 19583.005859\n",
      "Train Epoch: 112 [174912/225000 (78%)] Loss: 19412.916016\n",
      "Train Epoch: 112 [177408/225000 (79%)] Loss: 19355.332031\n",
      "Train Epoch: 112 [179904/225000 (80%)] Loss: 19293.662109\n",
      "Train Epoch: 112 [182400/225000 (81%)] Loss: 19558.003906\n",
      "Train Epoch: 112 [184896/225000 (82%)] Loss: 18898.453125\n",
      "Train Epoch: 112 [187392/225000 (83%)] Loss: 19219.886719\n",
      "Train Epoch: 112 [189888/225000 (84%)] Loss: 19341.832031\n",
      "Train Epoch: 112 [192384/225000 (86%)] Loss: 19231.070312\n",
      "Train Epoch: 112 [194880/225000 (87%)] Loss: 19437.859375\n",
      "Train Epoch: 112 [197376/225000 (88%)] Loss: 19256.531250\n",
      "Train Epoch: 112 [199872/225000 (89%)] Loss: 19803.566406\n",
      "Train Epoch: 112 [202368/225000 (90%)] Loss: 18930.531250\n",
      "Train Epoch: 112 [204864/225000 (91%)] Loss: 19611.250000\n",
      "Train Epoch: 112 [207360/225000 (92%)] Loss: 19533.835938\n",
      "Train Epoch: 112 [209856/225000 (93%)] Loss: 19393.660156\n",
      "Train Epoch: 112 [212352/225000 (94%)] Loss: 19512.191406\n",
      "Train Epoch: 112 [214848/225000 (95%)] Loss: 19421.488281\n",
      "Train Epoch: 112 [217344/225000 (97%)] Loss: 19349.515625\n",
      "Train Epoch: 112 [219840/225000 (98%)] Loss: 19068.982422\n",
      "Train Epoch: 112 [222336/225000 (99%)] Loss: 19715.734375\n",
      "Train Epoch: 112 [224832/225000 (100%)] Loss: 19209.601562\n",
      "    epoch          : 112\n",
      "    loss           : 19418.512931953926\n",
      "    val_loss       : 19324.598220569485\n",
      "Train Epoch: 113 [192/225000 (0%)] Loss: 19052.988281\n",
      "Train Epoch: 113 [2688/225000 (1%)] Loss: 19134.365234\n",
      "Train Epoch: 113 [5184/225000 (2%)] Loss: 19758.806641\n",
      "Train Epoch: 113 [7680/225000 (3%)] Loss: 19485.724609\n",
      "Train Epoch: 113 [10176/225000 (5%)] Loss: 20138.205078\n",
      "Train Epoch: 113 [12672/225000 (6%)] Loss: 19164.820312\n",
      "Train Epoch: 113 [15168/225000 (7%)] Loss: 18824.851562\n",
      "Train Epoch: 113 [17664/225000 (8%)] Loss: 19179.609375\n",
      "Train Epoch: 113 [20160/225000 (9%)] Loss: 18998.101562\n",
      "Train Epoch: 113 [22656/225000 (10%)] Loss: 19936.207031\n",
      "Train Epoch: 113 [25152/225000 (11%)] Loss: 19759.777344\n",
      "Train Epoch: 113 [27648/225000 (12%)] Loss: 19405.187500\n",
      "Train Epoch: 113 [30144/225000 (13%)] Loss: 19441.658203\n",
      "Train Epoch: 113 [32640/225000 (15%)] Loss: 19640.726562\n",
      "Train Epoch: 113 [35136/225000 (16%)] Loss: 19816.625000\n",
      "Train Epoch: 113 [37632/225000 (17%)] Loss: 19529.953125\n",
      "Train Epoch: 113 [40128/225000 (18%)] Loss: 19822.042969\n",
      "Train Epoch: 113 [42624/225000 (19%)] Loss: 19638.257812\n",
      "Train Epoch: 113 [45120/225000 (20%)] Loss: 19329.136719\n",
      "Train Epoch: 113 [47616/225000 (21%)] Loss: 19118.292969\n",
      "Train Epoch: 113 [50112/225000 (22%)] Loss: 19685.691406\n",
      "Train Epoch: 113 [52608/225000 (23%)] Loss: 20289.382812\n",
      "Train Epoch: 113 [55104/225000 (24%)] Loss: 19527.250000\n",
      "Train Epoch: 113 [57600/225000 (26%)] Loss: 19399.156250\n",
      "Train Epoch: 113 [60096/225000 (27%)] Loss: 19338.914062\n",
      "Train Epoch: 113 [62592/225000 (28%)] Loss: 19559.775391\n",
      "Train Epoch: 113 [65088/225000 (29%)] Loss: 19437.230469\n",
      "Train Epoch: 113 [67584/225000 (30%)] Loss: 19256.085938\n",
      "Train Epoch: 113 [70080/225000 (31%)] Loss: 18931.630859\n",
      "Train Epoch: 113 [72576/225000 (32%)] Loss: 19432.875000\n",
      "Train Epoch: 113 [75072/225000 (33%)] Loss: 19729.093750\n",
      "Train Epoch: 113 [77568/225000 (34%)] Loss: 19930.238281\n",
      "Train Epoch: 113 [80064/225000 (36%)] Loss: 19505.492188\n",
      "Train Epoch: 113 [82560/225000 (37%)] Loss: 19406.912109\n",
      "Train Epoch: 113 [85056/225000 (38%)] Loss: 19470.041016\n",
      "Train Epoch: 113 [87552/225000 (39%)] Loss: 19557.544922\n",
      "Train Epoch: 113 [90048/225000 (40%)] Loss: 19387.164062\n",
      "Train Epoch: 113 [92544/225000 (41%)] Loss: 19406.818359\n",
      "Train Epoch: 113 [95040/225000 (42%)] Loss: 19942.617188\n",
      "Train Epoch: 113 [97536/225000 (43%)] Loss: 19015.208984\n",
      "Train Epoch: 113 [100032/225000 (44%)] Loss: 19871.671875\n",
      "Train Epoch: 113 [102528/225000 (46%)] Loss: 19544.914062\n",
      "Train Epoch: 113 [105024/225000 (47%)] Loss: 19281.394531\n",
      "Train Epoch: 113 [107520/225000 (48%)] Loss: 19325.666016\n",
      "Train Epoch: 113 [110016/225000 (49%)] Loss: 19332.429688\n",
      "Train Epoch: 113 [112512/225000 (50%)] Loss: 19196.814453\n",
      "Train Epoch: 113 [115008/225000 (51%)] Loss: 19728.171875\n",
      "Train Epoch: 113 [117504/225000 (52%)] Loss: 19139.769531\n",
      "Train Epoch: 113 [120000/225000 (53%)] Loss: 19570.195312\n",
      "Train Epoch: 113 [122496/225000 (54%)] Loss: 19734.503906\n",
      "Train Epoch: 113 [124992/225000 (56%)] Loss: 19049.625000\n",
      "Train Epoch: 113 [127488/225000 (57%)] Loss: 19698.480469\n",
      "Train Epoch: 113 [129984/225000 (58%)] Loss: 19156.417969\n",
      "Train Epoch: 113 [132480/225000 (59%)] Loss: 19569.187500\n",
      "Train Epoch: 113 [134976/225000 (60%)] Loss: 19379.037109\n",
      "Train Epoch: 113 [137472/225000 (61%)] Loss: 19660.580078\n",
      "Train Epoch: 113 [139968/225000 (62%)] Loss: 19375.037109\n",
      "Train Epoch: 113 [142464/225000 (63%)] Loss: 19646.013672\n",
      "Train Epoch: 113 [144960/225000 (64%)] Loss: 19530.960938\n",
      "Train Epoch: 113 [147456/225000 (66%)] Loss: 19476.031250\n",
      "Train Epoch: 113 [149952/225000 (67%)] Loss: 19126.394531\n",
      "Train Epoch: 113 [152448/225000 (68%)] Loss: 19133.933594\n",
      "Train Epoch: 113 [154944/225000 (69%)] Loss: 19574.996094\n",
      "Train Epoch: 113 [157440/225000 (70%)] Loss: 19042.089844\n",
      "Train Epoch: 113 [159936/225000 (71%)] Loss: 19375.988281\n",
      "Train Epoch: 113 [162432/225000 (72%)] Loss: 19749.029297\n",
      "Train Epoch: 113 [164928/225000 (73%)] Loss: 19767.150391\n",
      "Train Epoch: 113 [167424/225000 (74%)] Loss: 19715.468750\n",
      "Train Epoch: 113 [169920/225000 (76%)] Loss: 19541.996094\n",
      "Train Epoch: 113 [172416/225000 (77%)] Loss: 19616.912109\n",
      "Train Epoch: 113 [174912/225000 (78%)] Loss: 19413.048828\n",
      "Train Epoch: 113 [177408/225000 (79%)] Loss: 19334.218750\n",
      "Train Epoch: 113 [179904/225000 (80%)] Loss: 18965.037109\n",
      "Train Epoch: 113 [182400/225000 (81%)] Loss: 19209.011719\n",
      "Train Epoch: 113 [184896/225000 (82%)] Loss: 19462.136719\n",
      "Train Epoch: 113 [187392/225000 (83%)] Loss: 18759.835938\n",
      "Train Epoch: 113 [189888/225000 (84%)] Loss: 19449.363281\n",
      "Train Epoch: 113 [192384/225000 (86%)] Loss: 19678.796875\n",
      "Train Epoch: 113 [194880/225000 (87%)] Loss: 19502.339844\n",
      "Train Epoch: 113 [197376/225000 (88%)] Loss: 19676.662109\n",
      "Train Epoch: 113 [199872/225000 (89%)] Loss: 19529.183594\n",
      "Train Epoch: 113 [202368/225000 (90%)] Loss: 20219.109375\n",
      "Train Epoch: 113 [204864/225000 (91%)] Loss: 19528.742188\n",
      "Train Epoch: 113 [207360/225000 (92%)] Loss: 19218.511719\n",
      "Train Epoch: 113 [209856/225000 (93%)] Loss: 19026.613281\n",
      "Train Epoch: 113 [212352/225000 (94%)] Loss: 19463.312500\n",
      "Train Epoch: 113 [214848/225000 (95%)] Loss: 19515.203125\n",
      "Train Epoch: 113 [217344/225000 (97%)] Loss: 19659.902344\n",
      "Train Epoch: 113 [219840/225000 (98%)] Loss: 19446.480469\n",
      "Train Epoch: 113 [222336/225000 (99%)] Loss: 19137.648438\n",
      "Train Epoch: 113 [224832/225000 (100%)] Loss: 19429.859375\n",
      "    epoch          : 113\n",
      "    loss           : 19413.437425008\n",
      "    val_loss       : 19312.87087508422\n",
      "Train Epoch: 114 [192/225000 (0%)] Loss: 19652.320312\n",
      "Train Epoch: 114 [2688/225000 (1%)] Loss: 19629.031250\n",
      "Train Epoch: 114 [5184/225000 (2%)] Loss: 19493.199219\n",
      "Train Epoch: 114 [7680/225000 (3%)] Loss: 19332.742188\n",
      "Train Epoch: 114 [10176/225000 (5%)] Loss: 19254.667969\n",
      "Train Epoch: 114 [12672/225000 (6%)] Loss: 19085.919922\n",
      "Train Epoch: 114 [15168/225000 (7%)] Loss: 19690.125000\n",
      "Train Epoch: 114 [17664/225000 (8%)] Loss: 19707.378906\n",
      "Train Epoch: 114 [20160/225000 (9%)] Loss: 19014.320312\n",
      "Train Epoch: 114 [22656/225000 (10%)] Loss: 19246.863281\n",
      "Train Epoch: 114 [25152/225000 (11%)] Loss: 19378.310547\n",
      "Train Epoch: 114 [27648/225000 (12%)] Loss: 19321.722656\n",
      "Train Epoch: 114 [30144/225000 (13%)] Loss: 19584.728516\n",
      "Train Epoch: 114 [32640/225000 (15%)] Loss: 19552.681641\n",
      "Train Epoch: 114 [35136/225000 (16%)] Loss: 19620.203125\n",
      "Train Epoch: 114 [37632/225000 (17%)] Loss: 19672.837891\n",
      "Train Epoch: 114 [40128/225000 (18%)] Loss: 18985.214844\n",
      "Train Epoch: 114 [42624/225000 (19%)] Loss: 19424.125000\n",
      "Train Epoch: 114 [45120/225000 (20%)] Loss: 19215.789062\n",
      "Train Epoch: 114 [47616/225000 (21%)] Loss: 19224.064453\n",
      "Train Epoch: 114 [50112/225000 (22%)] Loss: 19692.871094\n",
      "Train Epoch: 114 [52608/225000 (23%)] Loss: 19358.263672\n",
      "Train Epoch: 114 [55104/225000 (24%)] Loss: 19094.375000\n",
      "Train Epoch: 114 [57600/225000 (26%)] Loss: 19604.597656\n",
      "Train Epoch: 114 [60096/225000 (27%)] Loss: 19142.632812\n",
      "Train Epoch: 114 [62592/225000 (28%)] Loss: 19184.320312\n",
      "Train Epoch: 114 [65088/225000 (29%)] Loss: 19262.755859\n",
      "Train Epoch: 114 [67584/225000 (30%)] Loss: 19630.023438\n",
      "Train Epoch: 114 [70080/225000 (31%)] Loss: 19311.425781\n",
      "Train Epoch: 114 [72576/225000 (32%)] Loss: 19345.904297\n",
      "Train Epoch: 114 [75072/225000 (33%)] Loss: 19740.675781\n",
      "Train Epoch: 114 [77568/225000 (34%)] Loss: 19457.585938\n",
      "Train Epoch: 114 [80064/225000 (36%)] Loss: 19127.654297\n",
      "Train Epoch: 114 [82560/225000 (37%)] Loss: 19678.142578\n",
      "Train Epoch: 114 [85056/225000 (38%)] Loss: 19599.812500\n",
      "Train Epoch: 114 [87552/225000 (39%)] Loss: 19621.882812\n",
      "Train Epoch: 114 [90048/225000 (40%)] Loss: 19240.632812\n",
      "Train Epoch: 114 [92544/225000 (41%)] Loss: 19196.175781\n",
      "Train Epoch: 114 [95040/225000 (42%)] Loss: 19739.693359\n",
      "Train Epoch: 114 [97536/225000 (43%)] Loss: 19695.033203\n",
      "Train Epoch: 114 [100032/225000 (44%)] Loss: 19653.767578\n",
      "Train Epoch: 114 [102528/225000 (46%)] Loss: 19437.968750\n",
      "Train Epoch: 114 [105024/225000 (47%)] Loss: 19366.031250\n",
      "Train Epoch: 114 [107520/225000 (48%)] Loss: 19819.142578\n",
      "Train Epoch: 114 [110016/225000 (49%)] Loss: 19596.691406\n",
      "Train Epoch: 114 [112512/225000 (50%)] Loss: 19123.187500\n",
      "Train Epoch: 114 [115008/225000 (51%)] Loss: 19856.742188\n",
      "Train Epoch: 114 [117504/225000 (52%)] Loss: 19219.908203\n",
      "Train Epoch: 114 [120000/225000 (53%)] Loss: 19777.675781\n",
      "Train Epoch: 114 [122496/225000 (54%)] Loss: 19853.105469\n",
      "Train Epoch: 114 [124992/225000 (56%)] Loss: 19502.433594\n",
      "Train Epoch: 114 [127488/225000 (57%)] Loss: 19423.501953\n",
      "Train Epoch: 114 [129984/225000 (58%)] Loss: 19304.111328\n",
      "Train Epoch: 114 [132480/225000 (59%)] Loss: 19141.666016\n",
      "Train Epoch: 114 [134976/225000 (60%)] Loss: 19415.480469\n",
      "Train Epoch: 114 [137472/225000 (61%)] Loss: 18764.109375\n",
      "Train Epoch: 114 [139968/225000 (62%)] Loss: 19107.162109\n",
      "Train Epoch: 114 [142464/225000 (63%)] Loss: 19353.593750\n",
      "Train Epoch: 114 [144960/225000 (64%)] Loss: 19725.523438\n",
      "Train Epoch: 114 [147456/225000 (66%)] Loss: 19431.496094\n",
      "Train Epoch: 114 [149952/225000 (67%)] Loss: 19405.234375\n",
      "Train Epoch: 114 [152448/225000 (68%)] Loss: 19351.910156\n",
      "Train Epoch: 114 [154944/225000 (69%)] Loss: 19312.664062\n",
      "Train Epoch: 114 [157440/225000 (70%)] Loss: 19163.347656\n",
      "Train Epoch: 114 [159936/225000 (71%)] Loss: 19450.355469\n",
      "Train Epoch: 114 [162432/225000 (72%)] Loss: 19229.242188\n",
      "Train Epoch: 114 [164928/225000 (73%)] Loss: 19651.289062\n",
      "Train Epoch: 114 [167424/225000 (74%)] Loss: 19390.347656\n",
      "Train Epoch: 114 [169920/225000 (76%)] Loss: 19447.041016\n",
      "Train Epoch: 114 [172416/225000 (77%)] Loss: 19121.417969\n",
      "Train Epoch: 114 [174912/225000 (78%)] Loss: 19837.619141\n",
      "Train Epoch: 114 [177408/225000 (79%)] Loss: 19859.308594\n",
      "Train Epoch: 114 [179904/225000 (80%)] Loss: 20057.751953\n",
      "Train Epoch: 114 [182400/225000 (81%)] Loss: 19405.673828\n",
      "Train Epoch: 114 [184896/225000 (82%)] Loss: 19071.367188\n",
      "Train Epoch: 114 [187392/225000 (83%)] Loss: 18925.414062\n",
      "Train Epoch: 114 [189888/225000 (84%)] Loss: 19461.554688\n",
      "Train Epoch: 114 [192384/225000 (86%)] Loss: 19770.265625\n",
      "Train Epoch: 114 [194880/225000 (87%)] Loss: 19460.843750\n",
      "Train Epoch: 114 [197376/225000 (88%)] Loss: 19491.898438\n",
      "Train Epoch: 114 [199872/225000 (89%)] Loss: 19140.078125\n",
      "Train Epoch: 114 [202368/225000 (90%)] Loss: 19598.865234\n",
      "Train Epoch: 114 [204864/225000 (91%)] Loss: 19384.480469\n",
      "Train Epoch: 114 [207360/225000 (92%)] Loss: 19162.277344\n",
      "Train Epoch: 114 [209856/225000 (93%)] Loss: 19209.722656\n",
      "Train Epoch: 114 [212352/225000 (94%)] Loss: 19265.042969\n",
      "Train Epoch: 114 [214848/225000 (95%)] Loss: 19689.433594\n",
      "Train Epoch: 114 [217344/225000 (97%)] Loss: 19476.728516\n",
      "Train Epoch: 114 [219840/225000 (98%)] Loss: 20092.943359\n",
      "Train Epoch: 114 [222336/225000 (99%)] Loss: 19979.679688\n",
      "Train Epoch: 114 [224832/225000 (100%)] Loss: 19556.744141\n",
      "    epoch          : 114\n",
      "    loss           : 19400.845906436647\n",
      "    val_loss       : 19327.155789707453\n",
      "Train Epoch: 115 [192/225000 (0%)] Loss: 19688.351562\n",
      "Train Epoch: 115 [2688/225000 (1%)] Loss: 19599.117188\n",
      "Train Epoch: 115 [5184/225000 (2%)] Loss: 19495.042969\n",
      "Train Epoch: 115 [7680/225000 (3%)] Loss: 19749.185547\n",
      "Train Epoch: 115 [10176/225000 (5%)] Loss: 19421.101562\n",
      "Train Epoch: 115 [12672/225000 (6%)] Loss: 19210.949219\n",
      "Train Epoch: 115 [15168/225000 (7%)] Loss: 19037.181641\n",
      "Train Epoch: 115 [17664/225000 (8%)] Loss: 19120.832031\n",
      "Train Epoch: 115 [20160/225000 (9%)] Loss: 19388.957031\n",
      "Train Epoch: 115 [22656/225000 (10%)] Loss: 19537.417969\n",
      "Train Epoch: 115 [25152/225000 (11%)] Loss: 19311.375000\n",
      "Train Epoch: 115 [27648/225000 (12%)] Loss: 19437.605469\n",
      "Train Epoch: 115 [30144/225000 (13%)] Loss: 19724.867188\n",
      "Train Epoch: 115 [32640/225000 (15%)] Loss: 19144.621094\n",
      "Train Epoch: 115 [35136/225000 (16%)] Loss: 19262.562500\n",
      "Train Epoch: 115 [37632/225000 (17%)] Loss: 19796.125000\n",
      "Train Epoch: 115 [40128/225000 (18%)] Loss: 18950.125000\n",
      "Train Epoch: 115 [42624/225000 (19%)] Loss: 19398.541016\n",
      "Train Epoch: 115 [45120/225000 (20%)] Loss: 19539.300781\n",
      "Train Epoch: 115 [47616/225000 (21%)] Loss: 19243.099609\n",
      "Train Epoch: 115 [50112/225000 (22%)] Loss: 19290.121094\n",
      "Train Epoch: 115 [52608/225000 (23%)] Loss: 19445.292969\n",
      "Train Epoch: 115 [55104/225000 (24%)] Loss: 19984.984375\n",
      "Train Epoch: 115 [57600/225000 (26%)] Loss: 19744.912109\n",
      "Train Epoch: 115 [60096/225000 (27%)] Loss: 19294.357422\n",
      "Train Epoch: 115 [62592/225000 (28%)] Loss: 19575.656250\n",
      "Train Epoch: 115 [65088/225000 (29%)] Loss: 19522.367188\n",
      "Train Epoch: 115 [67584/225000 (30%)] Loss: 19786.566406\n",
      "Train Epoch: 115 [70080/225000 (31%)] Loss: 19735.152344\n",
      "Train Epoch: 115 [72576/225000 (32%)] Loss: 19155.509766\n",
      "Train Epoch: 115 [75072/225000 (33%)] Loss: 19719.941406\n",
      "Train Epoch: 115 [77568/225000 (34%)] Loss: 20095.097656\n",
      "Train Epoch: 115 [80064/225000 (36%)] Loss: 19329.937500\n",
      "Train Epoch: 115 [82560/225000 (37%)] Loss: 19508.871094\n",
      "Train Epoch: 115 [85056/225000 (38%)] Loss: 19162.488281\n",
      "Train Epoch: 115 [87552/225000 (39%)] Loss: 19136.199219\n",
      "Train Epoch: 115 [90048/225000 (40%)] Loss: 19318.425781\n",
      "Train Epoch: 115 [92544/225000 (41%)] Loss: 19333.806641\n",
      "Train Epoch: 115 [95040/225000 (42%)] Loss: 19418.941406\n",
      "Train Epoch: 115 [97536/225000 (43%)] Loss: 19512.380859\n",
      "Train Epoch: 115 [100032/225000 (44%)] Loss: 19751.019531\n",
      "Train Epoch: 115 [102528/225000 (46%)] Loss: 19318.261719\n",
      "Train Epoch: 115 [105024/225000 (47%)] Loss: 18833.585938\n",
      "Train Epoch: 115 [107520/225000 (48%)] Loss: 19452.742188\n",
      "Train Epoch: 115 [110016/225000 (49%)] Loss: 19036.259766\n",
      "Train Epoch: 115 [112512/225000 (50%)] Loss: 19324.894531\n",
      "Train Epoch: 115 [115008/225000 (51%)] Loss: 19634.585938\n",
      "Train Epoch: 115 [117504/225000 (52%)] Loss: 19299.572266\n",
      "Train Epoch: 115 [120000/225000 (53%)] Loss: 19392.367188\n",
      "Train Epoch: 115 [122496/225000 (54%)] Loss: 19468.158203\n",
      "Train Epoch: 115 [124992/225000 (56%)] Loss: 19478.000000\n",
      "Train Epoch: 115 [127488/225000 (57%)] Loss: 19152.794922\n",
      "Train Epoch: 115 [129984/225000 (58%)] Loss: 19292.734375\n",
      "Train Epoch: 115 [132480/225000 (59%)] Loss: 19515.150391\n",
      "Train Epoch: 115 [134976/225000 (60%)] Loss: 19112.765625\n",
      "Train Epoch: 115 [137472/225000 (61%)] Loss: 19335.158203\n",
      "Train Epoch: 115 [139968/225000 (62%)] Loss: 19191.640625\n",
      "Train Epoch: 115 [142464/225000 (63%)] Loss: 19422.576172\n",
      "Train Epoch: 115 [144960/225000 (64%)] Loss: 18946.675781\n",
      "Train Epoch: 115 [147456/225000 (66%)] Loss: 19448.519531\n",
      "Train Epoch: 115 [149952/225000 (67%)] Loss: 19116.921875\n",
      "Train Epoch: 115 [152448/225000 (68%)] Loss: 19413.843750\n",
      "Train Epoch: 115 [154944/225000 (69%)] Loss: 19265.058594\n",
      "Train Epoch: 115 [157440/225000 (70%)] Loss: 19306.335938\n",
      "Train Epoch: 115 [159936/225000 (71%)] Loss: 19450.318359\n",
      "Train Epoch: 115 [162432/225000 (72%)] Loss: 19655.166016\n",
      "Train Epoch: 115 [164928/225000 (73%)] Loss: 19463.765625\n",
      "Train Epoch: 115 [167424/225000 (74%)] Loss: 19353.148438\n",
      "Train Epoch: 115 [169920/225000 (76%)] Loss: 19011.578125\n",
      "Train Epoch: 115 [172416/225000 (77%)] Loss: 19044.865234\n",
      "Train Epoch: 115 [174912/225000 (78%)] Loss: 19679.750000\n",
      "Train Epoch: 115 [177408/225000 (79%)] Loss: 19455.433594\n",
      "Train Epoch: 115 [179904/225000 (80%)] Loss: 19494.156250\n",
      "Train Epoch: 115 [182400/225000 (81%)] Loss: 19863.007812\n",
      "Train Epoch: 115 [184896/225000 (82%)] Loss: 19147.941406\n",
      "Train Epoch: 115 [187392/225000 (83%)] Loss: 19235.640625\n",
      "Train Epoch: 115 [189888/225000 (84%)] Loss: 19171.875000\n",
      "Train Epoch: 115 [192384/225000 (86%)] Loss: 19639.814453\n",
      "Train Epoch: 115 [194880/225000 (87%)] Loss: 19660.382812\n",
      "Train Epoch: 115 [197376/225000 (88%)] Loss: 19283.144531\n",
      "Train Epoch: 115 [199872/225000 (89%)] Loss: 19424.701172\n",
      "Train Epoch: 115 [202368/225000 (90%)] Loss: 19634.814453\n",
      "Train Epoch: 115 [204864/225000 (91%)] Loss: 19497.414062\n",
      "Train Epoch: 115 [207360/225000 (92%)] Loss: 19372.410156\n",
      "Train Epoch: 115 [209856/225000 (93%)] Loss: 20013.378906\n",
      "Train Epoch: 115 [212352/225000 (94%)] Loss: 19258.054688\n",
      "Train Epoch: 115 [214848/225000 (95%)] Loss: 19592.335938\n",
      "Train Epoch: 115 [217344/225000 (97%)] Loss: 19418.644531\n",
      "Train Epoch: 115 [219840/225000 (98%)] Loss: 19646.097656\n",
      "Train Epoch: 115 [222336/225000 (99%)] Loss: 19471.716797\n",
      "Train Epoch: 115 [224832/225000 (100%)] Loss: 19630.572266\n",
      "    epoch          : 115\n",
      "    loss           : 19401.324333737735\n",
      "    val_loss       : 19297.36298399754\n",
      "Train Epoch: 116 [192/225000 (0%)] Loss: 19491.269531\n",
      "Train Epoch: 116 [2688/225000 (1%)] Loss: 19574.693359\n",
      "Train Epoch: 116 [5184/225000 (2%)] Loss: 20091.433594\n",
      "Train Epoch: 116 [7680/225000 (3%)] Loss: 19542.796875\n",
      "Train Epoch: 116 [10176/225000 (5%)] Loss: 19238.056641\n",
      "Train Epoch: 116 [12672/225000 (6%)] Loss: 19799.396484\n",
      "Train Epoch: 116 [15168/225000 (7%)] Loss: 19307.324219\n",
      "Train Epoch: 116 [17664/225000 (8%)] Loss: 19024.523438\n",
      "Train Epoch: 116 [20160/225000 (9%)] Loss: 19524.599609\n",
      "Train Epoch: 116 [22656/225000 (10%)] Loss: 19385.111328\n",
      "Train Epoch: 116 [25152/225000 (11%)] Loss: 20088.824219\n",
      "Train Epoch: 116 [27648/225000 (12%)] Loss: 20100.298828\n",
      "Train Epoch: 116 [30144/225000 (13%)] Loss: 19611.937500\n",
      "Train Epoch: 116 [32640/225000 (15%)] Loss: 19617.082031\n",
      "Train Epoch: 116 [35136/225000 (16%)] Loss: 19624.328125\n",
      "Train Epoch: 116 [37632/225000 (17%)] Loss: 19385.714844\n",
      "Train Epoch: 116 [40128/225000 (18%)] Loss: 18970.289062\n",
      "Train Epoch: 116 [42624/225000 (19%)] Loss: 19437.326172\n",
      "Train Epoch: 116 [45120/225000 (20%)] Loss: 19202.382812\n",
      "Train Epoch: 116 [47616/225000 (21%)] Loss: 19234.429688\n",
      "Train Epoch: 116 [50112/225000 (22%)] Loss: 19242.050781\n",
      "Train Epoch: 116 [52608/225000 (23%)] Loss: 19287.337891\n",
      "Train Epoch: 116 [55104/225000 (24%)] Loss: 19437.134766\n",
      "Train Epoch: 116 [57600/225000 (26%)] Loss: 19256.556641\n",
      "Train Epoch: 116 [60096/225000 (27%)] Loss: 19862.148438\n",
      "Train Epoch: 116 [62592/225000 (28%)] Loss: 19333.744141\n",
      "Train Epoch: 116 [65088/225000 (29%)] Loss: 19239.984375\n",
      "Train Epoch: 116 [67584/225000 (30%)] Loss: 19688.748047\n",
      "Train Epoch: 116 [70080/225000 (31%)] Loss: 19410.525391\n",
      "Train Epoch: 116 [72576/225000 (32%)] Loss: 19694.701172\n",
      "Train Epoch: 116 [75072/225000 (33%)] Loss: 19545.306641\n",
      "Train Epoch: 116 [77568/225000 (34%)] Loss: 19448.291016\n",
      "Train Epoch: 116 [80064/225000 (36%)] Loss: 19118.449219\n",
      "Train Epoch: 116 [82560/225000 (37%)] Loss: 19534.578125\n",
      "Train Epoch: 116 [85056/225000 (38%)] Loss: 19698.324219\n",
      "Train Epoch: 116 [87552/225000 (39%)] Loss: 19413.232422\n",
      "Train Epoch: 116 [90048/225000 (40%)] Loss: 19058.941406\n",
      "Train Epoch: 116 [92544/225000 (41%)] Loss: 19258.880859\n",
      "Train Epoch: 116 [95040/225000 (42%)] Loss: 19758.152344\n",
      "Train Epoch: 116 [97536/225000 (43%)] Loss: 19074.511719\n",
      "Train Epoch: 116 [100032/225000 (44%)] Loss: 19364.753906\n",
      "Train Epoch: 116 [102528/225000 (46%)] Loss: 19496.214844\n",
      "Train Epoch: 116 [105024/225000 (47%)] Loss: 19640.878906\n",
      "Train Epoch: 116 [107520/225000 (48%)] Loss: 19514.894531\n",
      "Train Epoch: 116 [110016/225000 (49%)] Loss: 19631.855469\n",
      "Train Epoch: 116 [112512/225000 (50%)] Loss: 19859.351562\n",
      "Train Epoch: 116 [115008/225000 (51%)] Loss: 19607.859375\n",
      "Train Epoch: 116 [117504/225000 (52%)] Loss: 19438.570312\n",
      "Train Epoch: 116 [120000/225000 (53%)] Loss: 19267.531250\n",
      "Train Epoch: 116 [122496/225000 (54%)] Loss: 19330.839844\n",
      "Train Epoch: 116 [124992/225000 (56%)] Loss: 19502.562500\n",
      "Train Epoch: 116 [127488/225000 (57%)] Loss: 19202.605469\n",
      "Train Epoch: 116 [129984/225000 (58%)] Loss: 19362.859375\n",
      "Train Epoch: 116 [132480/225000 (59%)] Loss: 19533.808594\n",
      "Train Epoch: 116 [134976/225000 (60%)] Loss: 19399.816406\n",
      "Train Epoch: 116 [137472/225000 (61%)] Loss: 19517.406250\n",
      "Train Epoch: 116 [139968/225000 (62%)] Loss: 19473.941406\n",
      "Train Epoch: 116 [142464/225000 (63%)] Loss: 19244.320312\n",
      "Train Epoch: 116 [144960/225000 (64%)] Loss: 19622.380859\n",
      "Train Epoch: 116 [147456/225000 (66%)] Loss: 19608.242188\n",
      "Train Epoch: 116 [149952/225000 (67%)] Loss: 18961.943359\n",
      "Train Epoch: 116 [152448/225000 (68%)] Loss: 19682.625000\n",
      "Train Epoch: 116 [154944/225000 (69%)] Loss: 19661.535156\n",
      "Train Epoch: 116 [157440/225000 (70%)] Loss: 19502.056641\n",
      "Train Epoch: 116 [159936/225000 (71%)] Loss: 19538.652344\n",
      "Train Epoch: 116 [162432/225000 (72%)] Loss: 19553.546875\n",
      "Train Epoch: 116 [164928/225000 (73%)] Loss: 19653.695312\n",
      "Train Epoch: 116 [167424/225000 (74%)] Loss: 18853.769531\n",
      "Train Epoch: 116 [169920/225000 (76%)] Loss: 19143.691406\n",
      "Train Epoch: 116 [172416/225000 (77%)] Loss: 19417.757812\n",
      "Train Epoch: 116 [174912/225000 (78%)] Loss: 20230.750000\n",
      "Train Epoch: 116 [177408/225000 (79%)] Loss: 19589.718750\n",
      "Train Epoch: 116 [179904/225000 (80%)] Loss: 19879.804688\n",
      "Train Epoch: 116 [182400/225000 (81%)] Loss: 19526.925781\n",
      "Train Epoch: 116 [184896/225000 (82%)] Loss: 19397.820312\n",
      "Train Epoch: 116 [187392/225000 (83%)] Loss: 19516.468750\n",
      "Train Epoch: 116 [189888/225000 (84%)] Loss: 19928.835938\n",
      "Train Epoch: 116 [192384/225000 (86%)] Loss: 19781.207031\n",
      "Train Epoch: 116 [194880/225000 (87%)] Loss: 19386.728516\n",
      "Train Epoch: 116 [197376/225000 (88%)] Loss: 19523.105469\n",
      "Train Epoch: 116 [199872/225000 (89%)] Loss: 18825.437500\n",
      "Train Epoch: 116 [202368/225000 (90%)] Loss: 19546.878906\n",
      "Train Epoch: 116 [204864/225000 (91%)] Loss: 19721.029297\n",
      "Train Epoch: 116 [207360/225000 (92%)] Loss: 19627.130859\n",
      "Train Epoch: 116 [209856/225000 (93%)] Loss: 18954.886719\n",
      "Train Epoch: 116 [212352/225000 (94%)] Loss: 19343.726562\n",
      "Train Epoch: 116 [214848/225000 (95%)] Loss: 19419.656250\n",
      "Train Epoch: 116 [217344/225000 (97%)] Loss: 18991.812500\n",
      "Train Epoch: 116 [219840/225000 (98%)] Loss: 19815.929688\n",
      "Train Epoch: 116 [222336/225000 (99%)] Loss: 19578.109375\n",
      "Train Epoch: 116 [224832/225000 (100%)] Loss: 19803.593750\n",
      "    epoch          : 116\n",
      "    loss           : 19397.29751326525\n",
      "    val_loss       : 19317.278138480113\n",
      "Train Epoch: 117 [192/225000 (0%)] Loss: 19666.419922\n",
      "Train Epoch: 117 [2688/225000 (1%)] Loss: 19500.843750\n",
      "Train Epoch: 117 [5184/225000 (2%)] Loss: 19663.796875\n",
      "Train Epoch: 117 [7680/225000 (3%)] Loss: 19372.416016\n",
      "Train Epoch: 117 [10176/225000 (5%)] Loss: 19417.693359\n",
      "Train Epoch: 117 [12672/225000 (6%)] Loss: 19166.849609\n",
      "Train Epoch: 117 [15168/225000 (7%)] Loss: 19204.283203\n",
      "Train Epoch: 117 [17664/225000 (8%)] Loss: 19410.394531\n",
      "Train Epoch: 117 [20160/225000 (9%)] Loss: 19399.191406\n",
      "Train Epoch: 117 [22656/225000 (10%)] Loss: 19185.228516\n",
      "Train Epoch: 117 [25152/225000 (11%)] Loss: 18914.648438\n",
      "Train Epoch: 117 [27648/225000 (12%)] Loss: 19525.328125\n",
      "Train Epoch: 117 [30144/225000 (13%)] Loss: 19261.111328\n",
      "Train Epoch: 117 [32640/225000 (15%)] Loss: 18667.058594\n",
      "Train Epoch: 117 [35136/225000 (16%)] Loss: 19306.886719\n",
      "Train Epoch: 117 [37632/225000 (17%)] Loss: 19339.980469\n",
      "Train Epoch: 117 [40128/225000 (18%)] Loss: 19672.519531\n",
      "Train Epoch: 117 [42624/225000 (19%)] Loss: 19561.449219\n",
      "Train Epoch: 117 [45120/225000 (20%)] Loss: 19383.152344\n",
      "Train Epoch: 117 [47616/225000 (21%)] Loss: 19674.519531\n",
      "Train Epoch: 117 [50112/225000 (22%)] Loss: 19521.394531\n",
      "Train Epoch: 117 [52608/225000 (23%)] Loss: 19042.072266\n",
      "Train Epoch: 117 [55104/225000 (24%)] Loss: 19646.816406\n",
      "Train Epoch: 117 [57600/225000 (26%)] Loss: 19482.953125\n",
      "Train Epoch: 117 [60096/225000 (27%)] Loss: 19760.976562\n",
      "Train Epoch: 117 [62592/225000 (28%)] Loss: 19358.082031\n",
      "Train Epoch: 117 [65088/225000 (29%)] Loss: 19357.113281\n",
      "Train Epoch: 117 [67584/225000 (30%)] Loss: 19417.755859\n",
      "Train Epoch: 117 [70080/225000 (31%)] Loss: 19571.273438\n",
      "Train Epoch: 117 [72576/225000 (32%)] Loss: 19828.203125\n",
      "Train Epoch: 117 [75072/225000 (33%)] Loss: 18935.542969\n",
      "Train Epoch: 117 [77568/225000 (34%)] Loss: 19591.316406\n",
      "Train Epoch: 117 [80064/225000 (36%)] Loss: 19284.667969\n",
      "Train Epoch: 117 [82560/225000 (37%)] Loss: 19516.587891\n",
      "Train Epoch: 117 [85056/225000 (38%)] Loss: 18955.416016\n",
      "Train Epoch: 117 [87552/225000 (39%)] Loss: 19376.863281\n",
      "Train Epoch: 117 [90048/225000 (40%)] Loss: 19321.058594\n",
      "Train Epoch: 117 [92544/225000 (41%)] Loss: 18897.498047\n",
      "Train Epoch: 117 [95040/225000 (42%)] Loss: 19294.101562\n",
      "Train Epoch: 117 [97536/225000 (43%)] Loss: 19535.703125\n",
      "Train Epoch: 117 [100032/225000 (44%)] Loss: 19647.789062\n",
      "Train Epoch: 117 [102528/225000 (46%)] Loss: 19176.689453\n",
      "Train Epoch: 117 [105024/225000 (47%)] Loss: 19162.992188\n",
      "Train Epoch: 117 [107520/225000 (48%)] Loss: 19504.785156\n",
      "Train Epoch: 117 [110016/225000 (49%)] Loss: 19717.355469\n",
      "Train Epoch: 117 [112512/225000 (50%)] Loss: 19247.019531\n",
      "Train Epoch: 117 [115008/225000 (51%)] Loss: 19744.566406\n",
      "Train Epoch: 117 [117504/225000 (52%)] Loss: 19135.332031\n",
      "Train Epoch: 117 [120000/225000 (53%)] Loss: 18898.466797\n",
      "Train Epoch: 117 [122496/225000 (54%)] Loss: 19451.781250\n",
      "Train Epoch: 117 [124992/225000 (56%)] Loss: 19357.683594\n",
      "Train Epoch: 117 [127488/225000 (57%)] Loss: 18664.333984\n",
      "Train Epoch: 117 [129984/225000 (58%)] Loss: 19346.308594\n",
      "Train Epoch: 117 [132480/225000 (59%)] Loss: 19054.238281\n",
      "Train Epoch: 117 [134976/225000 (60%)] Loss: 19852.328125\n",
      "Train Epoch: 117 [137472/225000 (61%)] Loss: 19286.542969\n",
      "Train Epoch: 117 [139968/225000 (62%)] Loss: 19365.751953\n",
      "Train Epoch: 117 [142464/225000 (63%)] Loss: 19548.117188\n",
      "Train Epoch: 117 [144960/225000 (64%)] Loss: 19204.035156\n",
      "Train Epoch: 117 [147456/225000 (66%)] Loss: 19154.500000\n",
      "Train Epoch: 117 [149952/225000 (67%)] Loss: 19380.240234\n",
      "Train Epoch: 117 [152448/225000 (68%)] Loss: 19710.753906\n",
      "Train Epoch: 117 [154944/225000 (69%)] Loss: 19092.107422\n",
      "Train Epoch: 117 [157440/225000 (70%)] Loss: 19474.269531\n",
      "Train Epoch: 117 [159936/225000 (71%)] Loss: 18888.457031\n",
      "Train Epoch: 117 [162432/225000 (72%)] Loss: 19161.675781\n",
      "Train Epoch: 117 [164928/225000 (73%)] Loss: 19342.777344\n",
      "Train Epoch: 117 [167424/225000 (74%)] Loss: 19627.083984\n",
      "Train Epoch: 117 [169920/225000 (76%)] Loss: 19731.691406\n",
      "Train Epoch: 117 [172416/225000 (77%)] Loss: 19481.972656\n",
      "Train Epoch: 117 [174912/225000 (78%)] Loss: 19555.035156\n",
      "Train Epoch: 117 [177408/225000 (79%)] Loss: 19151.656250\n",
      "Train Epoch: 117 [179904/225000 (80%)] Loss: 18983.609375\n",
      "Train Epoch: 117 [182400/225000 (81%)] Loss: 19603.406250\n",
      "Train Epoch: 117 [184896/225000 (82%)] Loss: 18967.470703\n",
      "Train Epoch: 117 [187392/225000 (83%)] Loss: 19081.826172\n",
      "Train Epoch: 117 [189888/225000 (84%)] Loss: 19618.113281\n",
      "Train Epoch: 117 [192384/225000 (86%)] Loss: 19032.529297\n",
      "Train Epoch: 117 [194880/225000 (87%)] Loss: 19843.964844\n",
      "Train Epoch: 117 [197376/225000 (88%)] Loss: 19572.574219\n",
      "Train Epoch: 117 [199872/225000 (89%)] Loss: 19169.587891\n",
      "Train Epoch: 117 [202368/225000 (90%)] Loss: 18895.671875\n",
      "Train Epoch: 117 [204864/225000 (91%)] Loss: 19570.541016\n",
      "Train Epoch: 117 [207360/225000 (92%)] Loss: 19604.371094\n",
      "Train Epoch: 117 [209856/225000 (93%)] Loss: 19549.482422\n",
      "Train Epoch: 117 [212352/225000 (94%)] Loss: 19106.445312\n",
      "Train Epoch: 117 [214848/225000 (95%)] Loss: 19386.531250\n",
      "Train Epoch: 117 [217344/225000 (97%)] Loss: 19638.199219\n",
      "Train Epoch: 117 [219840/225000 (98%)] Loss: 19594.460938\n",
      "Train Epoch: 117 [222336/225000 (99%)] Loss: 19712.546875\n",
      "Train Epoch: 117 [224832/225000 (100%)] Loss: 19031.312500\n",
      "    epoch          : 117\n",
      "    loss           : 19398.944204284875\n",
      "    val_loss       : 19339.678817823642\n",
      "Train Epoch: 118 [192/225000 (0%)] Loss: 19585.505859\n",
      "Train Epoch: 118 [2688/225000 (1%)] Loss: 18985.919922\n",
      "Train Epoch: 118 [5184/225000 (2%)] Loss: 19123.816406\n",
      "Train Epoch: 118 [7680/225000 (3%)] Loss: 19363.882812\n",
      "Train Epoch: 118 [10176/225000 (5%)] Loss: 19207.537109\n",
      "Train Epoch: 118 [12672/225000 (6%)] Loss: 18893.468750\n",
      "Train Epoch: 118 [15168/225000 (7%)] Loss: 19300.867188\n",
      "Train Epoch: 118 [17664/225000 (8%)] Loss: 19832.294922\n",
      "Train Epoch: 118 [20160/225000 (9%)] Loss: 19673.095703\n",
      "Train Epoch: 118 [22656/225000 (10%)] Loss: 19555.507812\n",
      "Train Epoch: 118 [25152/225000 (11%)] Loss: 19746.507812\n",
      "Train Epoch: 118 [27648/225000 (12%)] Loss: 19225.644531\n",
      "Train Epoch: 118 [30144/225000 (13%)] Loss: 19581.875000\n",
      "Train Epoch: 118 [32640/225000 (15%)] Loss: 19415.238281\n",
      "Train Epoch: 118 [35136/225000 (16%)] Loss: 19316.181641\n",
      "Train Epoch: 118 [37632/225000 (17%)] Loss: 19520.031250\n",
      "Train Epoch: 118 [40128/225000 (18%)] Loss: 18913.296875\n",
      "Train Epoch: 118 [42624/225000 (19%)] Loss: 19609.074219\n",
      "Train Epoch: 118 [45120/225000 (20%)] Loss: 19157.468750\n",
      "Train Epoch: 118 [47616/225000 (21%)] Loss: 19229.691406\n",
      "Train Epoch: 118 [50112/225000 (22%)] Loss: 19129.863281\n",
      "Train Epoch: 118 [52608/225000 (23%)] Loss: 18996.533203\n",
      "Train Epoch: 118 [55104/225000 (24%)] Loss: 19605.750000\n",
      "Train Epoch: 118 [57600/225000 (26%)] Loss: 19591.207031\n",
      "Train Epoch: 118 [60096/225000 (27%)] Loss: 19538.941406\n",
      "Train Epoch: 118 [62592/225000 (28%)] Loss: 19685.335938\n",
      "Train Epoch: 118 [65088/225000 (29%)] Loss: 19524.800781\n",
      "Train Epoch: 118 [67584/225000 (30%)] Loss: 19008.617188\n",
      "Train Epoch: 118 [70080/225000 (31%)] Loss: 19747.544922\n",
      "Train Epoch: 118 [72576/225000 (32%)] Loss: 19219.248047\n",
      "Train Epoch: 118 [75072/225000 (33%)] Loss: 18960.175781\n",
      "Train Epoch: 118 [77568/225000 (34%)] Loss: 19673.371094\n",
      "Train Epoch: 118 [80064/225000 (36%)] Loss: 19782.781250\n",
      "Train Epoch: 118 [82560/225000 (37%)] Loss: 19715.218750\n",
      "Train Epoch: 118 [85056/225000 (38%)] Loss: 19395.578125\n",
      "Train Epoch: 118 [87552/225000 (39%)] Loss: 19389.675781\n",
      "Train Epoch: 118 [90048/225000 (40%)] Loss: 19261.449219\n",
      "Train Epoch: 118 [92544/225000 (41%)] Loss: 19643.056641\n",
      "Train Epoch: 118 [95040/225000 (42%)] Loss: 19282.949219\n",
      "Train Epoch: 118 [97536/225000 (43%)] Loss: 19379.960938\n",
      "Train Epoch: 118 [100032/225000 (44%)] Loss: 19075.373047\n",
      "Train Epoch: 118 [102528/225000 (46%)] Loss: 19704.515625\n",
      "Train Epoch: 118 [105024/225000 (47%)] Loss: 19019.150391\n",
      "Train Epoch: 118 [107520/225000 (48%)] Loss: 19473.345703\n",
      "Train Epoch: 118 [110016/225000 (49%)] Loss: 19334.964844\n",
      "Train Epoch: 118 [112512/225000 (50%)] Loss: 19118.238281\n",
      "Train Epoch: 118 [115008/225000 (51%)] Loss: 19461.140625\n",
      "Train Epoch: 118 [117504/225000 (52%)] Loss: 19035.480469\n",
      "Train Epoch: 118 [120000/225000 (53%)] Loss: 19755.880859\n",
      "Train Epoch: 118 [122496/225000 (54%)] Loss: 19350.775391\n",
      "Train Epoch: 118 [124992/225000 (56%)] Loss: 19359.894531\n",
      "Train Epoch: 118 [127488/225000 (57%)] Loss: 19233.576172\n",
      "Train Epoch: 118 [129984/225000 (58%)] Loss: 19537.072266\n",
      "Train Epoch: 118 [132480/225000 (59%)] Loss: 19289.210938\n",
      "Train Epoch: 118 [134976/225000 (60%)] Loss: 18753.066406\n",
      "Train Epoch: 118 [137472/225000 (61%)] Loss: 19706.382812\n",
      "Train Epoch: 118 [139968/225000 (62%)] Loss: 19429.810547\n",
      "Train Epoch: 118 [142464/225000 (63%)] Loss: 19396.375000\n",
      "Train Epoch: 118 [144960/225000 (64%)] Loss: 19499.988281\n",
      "Train Epoch: 118 [147456/225000 (66%)] Loss: 19231.164062\n",
      "Train Epoch: 118 [149952/225000 (67%)] Loss: 19397.974609\n",
      "Train Epoch: 118 [152448/225000 (68%)] Loss: 19492.402344\n",
      "Train Epoch: 118 [154944/225000 (69%)] Loss: 19794.242188\n",
      "Train Epoch: 118 [157440/225000 (70%)] Loss: 19552.806641\n",
      "Train Epoch: 118 [159936/225000 (71%)] Loss: 19396.457031\n",
      "Train Epoch: 118 [162432/225000 (72%)] Loss: 19192.292969\n",
      "Train Epoch: 118 [164928/225000 (73%)] Loss: 19119.703125\n",
      "Train Epoch: 118 [167424/225000 (74%)] Loss: 19308.365234\n",
      "Train Epoch: 118 [169920/225000 (76%)] Loss: 19497.042969\n",
      "Train Epoch: 118 [172416/225000 (77%)] Loss: 19646.705078\n",
      "Train Epoch: 118 [174912/225000 (78%)] Loss: 19507.031250\n",
      "Train Epoch: 118 [177408/225000 (79%)] Loss: 19341.480469\n",
      "Train Epoch: 118 [179904/225000 (80%)] Loss: 19669.578125\n",
      "Train Epoch: 118 [182400/225000 (81%)] Loss: 19210.656250\n",
      "Train Epoch: 118 [184896/225000 (82%)] Loss: 19592.087891\n",
      "Train Epoch: 118 [187392/225000 (83%)] Loss: 19721.384766\n",
      "Train Epoch: 118 [189888/225000 (84%)] Loss: 18849.359375\n",
      "Train Epoch: 118 [192384/225000 (86%)] Loss: 19777.384766\n",
      "Train Epoch: 118 [194880/225000 (87%)] Loss: 19543.917969\n",
      "Train Epoch: 118 [197376/225000 (88%)] Loss: 19335.550781\n",
      "Train Epoch: 118 [199872/225000 (89%)] Loss: 18983.179688\n",
      "Train Epoch: 118 [202368/225000 (90%)] Loss: 19540.867188\n",
      "Train Epoch: 118 [204864/225000 (91%)] Loss: 19914.093750\n",
      "Train Epoch: 118 [207360/225000 (92%)] Loss: 19341.183594\n",
      "Train Epoch: 118 [209856/225000 (93%)] Loss: 19014.152344\n",
      "Train Epoch: 118 [212352/225000 (94%)] Loss: 19323.773438\n",
      "Train Epoch: 118 [214848/225000 (95%)] Loss: 19338.314453\n",
      "Train Epoch: 118 [217344/225000 (97%)] Loss: 19090.847656\n",
      "Train Epoch: 118 [219840/225000 (98%)] Loss: 19545.193359\n",
      "Train Epoch: 118 [222336/225000 (99%)] Loss: 19153.324219\n",
      "Train Epoch: 118 [224832/225000 (100%)] Loss: 19602.009766\n",
      "    epoch          : 118\n",
      "    loss           : 19387.0654596843\n",
      "    val_loss       : 19286.487844755633\n",
      "Train Epoch: 119 [192/225000 (0%)] Loss: 19555.093750\n",
      "Train Epoch: 119 [2688/225000 (1%)] Loss: 19605.792969\n",
      "Train Epoch: 119 [5184/225000 (2%)] Loss: 18582.480469\n",
      "Train Epoch: 119 [7680/225000 (3%)] Loss: 19242.773438\n",
      "Train Epoch: 119 [10176/225000 (5%)] Loss: 19258.841797\n",
      "Train Epoch: 119 [12672/225000 (6%)] Loss: 18877.927734\n",
      "Train Epoch: 119 [15168/225000 (7%)] Loss: 19121.855469\n",
      "Train Epoch: 119 [17664/225000 (8%)] Loss: 19796.980469\n",
      "Train Epoch: 119 [20160/225000 (9%)] Loss: 19162.189453\n",
      "Train Epoch: 119 [22656/225000 (10%)] Loss: 18998.566406\n",
      "Train Epoch: 119 [25152/225000 (11%)] Loss: 18953.910156\n",
      "Train Epoch: 119 [27648/225000 (12%)] Loss: 19426.746094\n",
      "Train Epoch: 119 [30144/225000 (13%)] Loss: 19583.031250\n",
      "Train Epoch: 119 [32640/225000 (15%)] Loss: 19588.414062\n",
      "Train Epoch: 119 [35136/225000 (16%)] Loss: 19329.841797\n",
      "Train Epoch: 119 [37632/225000 (17%)] Loss: 19429.093750\n",
      "Train Epoch: 119 [40128/225000 (18%)] Loss: 18730.386719\n",
      "Train Epoch: 119 [42624/225000 (19%)] Loss: 19519.062500\n",
      "Train Epoch: 119 [45120/225000 (20%)] Loss: 19217.550781\n",
      "Train Epoch: 119 [47616/225000 (21%)] Loss: 19118.480469\n",
      "Train Epoch: 119 [50112/225000 (22%)] Loss: 19372.392578\n",
      "Train Epoch: 119 [52608/225000 (23%)] Loss: 18773.783203\n",
      "Train Epoch: 119 [55104/225000 (24%)] Loss: 19405.462891\n",
      "Train Epoch: 119 [57600/225000 (26%)] Loss: 19485.447266\n",
      "Train Epoch: 119 [60096/225000 (27%)] Loss: 19552.576172\n",
      "Train Epoch: 119 [62592/225000 (28%)] Loss: 19912.683594\n",
      "Train Epoch: 119 [65088/225000 (29%)] Loss: 19866.185547\n",
      "Train Epoch: 119 [67584/225000 (30%)] Loss: 19365.078125\n",
      "Train Epoch: 119 [70080/225000 (31%)] Loss: 19268.082031\n",
      "Train Epoch: 119 [72576/225000 (32%)] Loss: 19883.529297\n",
      "Train Epoch: 119 [75072/225000 (33%)] Loss: 19543.888672\n",
      "Train Epoch: 119 [77568/225000 (34%)] Loss: 19509.765625\n",
      "Train Epoch: 119 [80064/225000 (36%)] Loss: 19070.535156\n",
      "Train Epoch: 119 [82560/225000 (37%)] Loss: 19924.076172\n",
      "Train Epoch: 119 [85056/225000 (38%)] Loss: 19408.753906\n",
      "Train Epoch: 119 [87552/225000 (39%)] Loss: 19724.056641\n",
      "Train Epoch: 119 [90048/225000 (40%)] Loss: 19461.003906\n",
      "Train Epoch: 119 [92544/225000 (41%)] Loss: 19424.634766\n",
      "Train Epoch: 119 [95040/225000 (42%)] Loss: 19298.726562\n",
      "Train Epoch: 119 [97536/225000 (43%)] Loss: 19794.753906\n",
      "Train Epoch: 119 [100032/225000 (44%)] Loss: 19175.242188\n",
      "Train Epoch: 119 [102528/225000 (46%)] Loss: 19030.498047\n",
      "Train Epoch: 119 [105024/225000 (47%)] Loss: 19216.535156\n",
      "Train Epoch: 119 [107520/225000 (48%)] Loss: 19513.650391\n",
      "Train Epoch: 119 [110016/225000 (49%)] Loss: 19383.257812\n",
      "Train Epoch: 119 [112512/225000 (50%)] Loss: 19086.972656\n",
      "Train Epoch: 119 [115008/225000 (51%)] Loss: 19182.082031\n",
      "Train Epoch: 119 [117504/225000 (52%)] Loss: 19470.455078\n",
      "Train Epoch: 119 [120000/225000 (53%)] Loss: 18921.308594\n",
      "Train Epoch: 119 [122496/225000 (54%)] Loss: 19379.035156\n",
      "Train Epoch: 119 [124992/225000 (56%)] Loss: 19283.546875\n",
      "Train Epoch: 119 [127488/225000 (57%)] Loss: 19264.292969\n",
      "Train Epoch: 119 [129984/225000 (58%)] Loss: 19808.394531\n",
      "Train Epoch: 119 [132480/225000 (59%)] Loss: 19021.511719\n",
      "Train Epoch: 119 [134976/225000 (60%)] Loss: 19433.945312\n",
      "Train Epoch: 119 [137472/225000 (61%)] Loss: 19436.833984\n",
      "Train Epoch: 119 [139968/225000 (62%)] Loss: 19288.160156\n",
      "Train Epoch: 119 [142464/225000 (63%)] Loss: 19990.238281\n",
      "Train Epoch: 119 [144960/225000 (64%)] Loss: 19176.521484\n",
      "Train Epoch: 119 [147456/225000 (66%)] Loss: 19379.722656\n",
      "Train Epoch: 119 [149952/225000 (67%)] Loss: 19713.578125\n",
      "Train Epoch: 119 [152448/225000 (68%)] Loss: 18550.023438\n",
      "Train Epoch: 119 [154944/225000 (69%)] Loss: 19556.408203\n",
      "Train Epoch: 119 [157440/225000 (70%)] Loss: 19342.501953\n",
      "Train Epoch: 119 [159936/225000 (71%)] Loss: 18915.363281\n",
      "Train Epoch: 119 [162432/225000 (72%)] Loss: 19500.339844\n",
      "Train Epoch: 119 [164928/225000 (73%)] Loss: 19591.365234\n",
      "Train Epoch: 119 [167424/225000 (74%)] Loss: 19442.289062\n",
      "Train Epoch: 119 [169920/225000 (76%)] Loss: 19351.304688\n",
      "Train Epoch: 119 [172416/225000 (77%)] Loss: 19461.507812\n",
      "Train Epoch: 119 [174912/225000 (78%)] Loss: 19560.712891\n",
      "Train Epoch: 119 [177408/225000 (79%)] Loss: 19873.156250\n",
      "Train Epoch: 119 [179904/225000 (80%)] Loss: 19179.503906\n",
      "Train Epoch: 119 [182400/225000 (81%)] Loss: 19324.144531\n",
      "Train Epoch: 119 [184896/225000 (82%)] Loss: 19549.730469\n",
      "Train Epoch: 119 [187392/225000 (83%)] Loss: 19403.250000\n",
      "Train Epoch: 119 [189888/225000 (84%)] Loss: 18863.691406\n",
      "Train Epoch: 119 [192384/225000 (86%)] Loss: 19400.351562\n",
      "Train Epoch: 119 [194880/225000 (87%)] Loss: 19944.257812\n",
      "Train Epoch: 119 [197376/225000 (88%)] Loss: 19512.183594\n",
      "Train Epoch: 119 [199872/225000 (89%)] Loss: 19021.445312\n",
      "Train Epoch: 119 [202368/225000 (90%)] Loss: 19334.089844\n",
      "Train Epoch: 119 [204864/225000 (91%)] Loss: 19354.269531\n",
      "Train Epoch: 119 [207360/225000 (92%)] Loss: 19526.648438\n",
      "Train Epoch: 119 [209856/225000 (93%)] Loss: 19710.273438\n",
      "Train Epoch: 119 [212352/225000 (94%)] Loss: 19484.867188\n",
      "Train Epoch: 119 [214848/225000 (95%)] Loss: 18843.757812\n",
      "Train Epoch: 119 [217344/225000 (97%)] Loss: 18526.703125\n",
      "Train Epoch: 119 [219840/225000 (98%)] Loss: 19049.093750\n",
      "Train Epoch: 119 [222336/225000 (99%)] Loss: 19625.273438\n",
      "Train Epoch: 119 [224832/225000 (100%)] Loss: 19741.181641\n",
      "    epoch          : 119\n",
      "    loss           : 19385.8953278317\n",
      "    val_loss       : 19288.47548664526\n",
      "Train Epoch: 120 [192/225000 (0%)] Loss: 19208.921875\n",
      "Train Epoch: 120 [2688/225000 (1%)] Loss: 19101.441406\n",
      "Train Epoch: 120 [5184/225000 (2%)] Loss: 19552.156250\n",
      "Train Epoch: 120 [7680/225000 (3%)] Loss: 18839.738281\n",
      "Train Epoch: 120 [10176/225000 (5%)] Loss: 19882.492188\n",
      "Train Epoch: 120 [12672/225000 (6%)] Loss: 19250.226562\n",
      "Train Epoch: 120 [15168/225000 (7%)] Loss: 19542.265625\n",
      "Train Epoch: 120 [17664/225000 (8%)] Loss: 19178.019531\n",
      "Train Epoch: 120 [20160/225000 (9%)] Loss: 19503.742188\n",
      "Train Epoch: 120 [22656/225000 (10%)] Loss: 19142.601562\n",
      "Train Epoch: 120 [25152/225000 (11%)] Loss: 19186.367188\n",
      "Train Epoch: 120 [27648/225000 (12%)] Loss: 19275.533203\n",
      "Train Epoch: 120 [30144/225000 (13%)] Loss: 19670.085938\n",
      "Train Epoch: 120 [32640/225000 (15%)] Loss: 19304.855469\n",
      "Train Epoch: 120 [35136/225000 (16%)] Loss: 19744.732422\n",
      "Train Epoch: 120 [37632/225000 (17%)] Loss: 19620.531250\n",
      "Train Epoch: 120 [40128/225000 (18%)] Loss: 19360.753906\n",
      "Train Epoch: 120 [42624/225000 (19%)] Loss: 19232.728516\n",
      "Train Epoch: 120 [45120/225000 (20%)] Loss: 19363.007812\n",
      "Train Epoch: 120 [47616/225000 (21%)] Loss: 19501.730469\n",
      "Train Epoch: 120 [50112/225000 (22%)] Loss: 18864.066406\n",
      "Train Epoch: 120 [52608/225000 (23%)] Loss: 19221.341797\n",
      "Train Epoch: 120 [55104/225000 (24%)] Loss: 18931.990234\n",
      "Train Epoch: 120 [57600/225000 (26%)] Loss: 19283.250000\n",
      "Train Epoch: 120 [60096/225000 (27%)] Loss: 18904.445312\n",
      "Train Epoch: 120 [62592/225000 (28%)] Loss: 19233.142578\n",
      "Train Epoch: 120 [65088/225000 (29%)] Loss: 19179.634766\n",
      "Train Epoch: 120 [67584/225000 (30%)] Loss: 19297.957031\n",
      "Train Epoch: 120 [70080/225000 (31%)] Loss: 19197.886719\n",
      "Train Epoch: 120 [72576/225000 (32%)] Loss: 19096.851562\n",
      "Train Epoch: 120 [75072/225000 (33%)] Loss: 18949.585938\n",
      "Train Epoch: 120 [77568/225000 (34%)] Loss: 19202.638672\n",
      "Train Epoch: 120 [80064/225000 (36%)] Loss: 19264.207031\n",
      "Train Epoch: 120 [82560/225000 (37%)] Loss: 19090.171875\n",
      "Train Epoch: 120 [85056/225000 (38%)] Loss: 19629.000000\n",
      "Train Epoch: 120 [87552/225000 (39%)] Loss: 19117.570312\n",
      "Train Epoch: 120 [90048/225000 (40%)] Loss: 20084.716797\n",
      "Train Epoch: 120 [92544/225000 (41%)] Loss: 19263.675781\n",
      "Train Epoch: 120 [95040/225000 (42%)] Loss: 19169.390625\n",
      "Train Epoch: 120 [97536/225000 (43%)] Loss: 19101.250000\n",
      "Train Epoch: 120 [100032/225000 (44%)] Loss: 19742.355469\n",
      "Train Epoch: 120 [102528/225000 (46%)] Loss: 19233.273438\n",
      "Train Epoch: 120 [105024/225000 (47%)] Loss: 19236.804688\n",
      "Train Epoch: 120 [107520/225000 (48%)] Loss: 19299.062500\n",
      "Train Epoch: 120 [110016/225000 (49%)] Loss: 19277.996094\n",
      "Train Epoch: 120 [112512/225000 (50%)] Loss: 19493.376953\n",
      "Train Epoch: 120 [115008/225000 (51%)] Loss: 19147.859375\n",
      "Train Epoch: 120 [117504/225000 (52%)] Loss: 19252.738281\n",
      "Train Epoch: 120 [120000/225000 (53%)] Loss: 19008.410156\n",
      "Train Epoch: 120 [122496/225000 (54%)] Loss: 19340.533203\n",
      "Train Epoch: 120 [124992/225000 (56%)] Loss: 19555.093750\n",
      "Train Epoch: 120 [127488/225000 (57%)] Loss: 18882.861328\n",
      "Train Epoch: 120 [129984/225000 (58%)] Loss: 19322.607422\n",
      "Train Epoch: 120 [132480/225000 (59%)] Loss: 19564.707031\n",
      "Train Epoch: 120 [134976/225000 (60%)] Loss: 19226.035156\n",
      "Train Epoch: 120 [137472/225000 (61%)] Loss: 19586.089844\n",
      "Train Epoch: 120 [139968/225000 (62%)] Loss: 19415.687500\n",
      "Train Epoch: 120 [142464/225000 (63%)] Loss: 19952.296875\n",
      "Train Epoch: 120 [144960/225000 (64%)] Loss: 19202.447266\n",
      "Train Epoch: 120 [147456/225000 (66%)] Loss: 19556.558594\n",
      "Train Epoch: 120 [149952/225000 (67%)] Loss: 19622.396484\n",
      "Train Epoch: 120 [152448/225000 (68%)] Loss: 19258.625000\n",
      "Train Epoch: 120 [154944/225000 (69%)] Loss: 19582.667969\n",
      "Train Epoch: 120 [157440/225000 (70%)] Loss: 19257.195312\n",
      "Train Epoch: 120 [159936/225000 (71%)] Loss: 19503.132812\n",
      "Train Epoch: 120 [162432/225000 (72%)] Loss: 18969.160156\n",
      "Train Epoch: 120 [164928/225000 (73%)] Loss: 19356.957031\n",
      "Train Epoch: 120 [167424/225000 (74%)] Loss: 19543.960938\n",
      "Train Epoch: 120 [169920/225000 (76%)] Loss: 19522.546875\n",
      "Train Epoch: 120 [172416/225000 (77%)] Loss: 19009.507812\n",
      "Train Epoch: 120 [174912/225000 (78%)] Loss: 19396.652344\n",
      "Train Epoch: 120 [177408/225000 (79%)] Loss: 18892.902344\n",
      "Train Epoch: 120 [179904/225000 (80%)] Loss: 19192.332031\n",
      "Train Epoch: 120 [182400/225000 (81%)] Loss: 19360.871094\n",
      "Train Epoch: 120 [184896/225000 (82%)] Loss: 19672.007812\n",
      "Train Epoch: 120 [187392/225000 (83%)] Loss: 19374.656250\n",
      "Train Epoch: 120 [189888/225000 (84%)] Loss: 19119.115234\n",
      "Train Epoch: 120 [192384/225000 (86%)] Loss: 19599.425781\n",
      "Train Epoch: 120 [194880/225000 (87%)] Loss: 19874.621094\n",
      "Train Epoch: 120 [197376/225000 (88%)] Loss: 19113.873047\n",
      "Train Epoch: 120 [199872/225000 (89%)] Loss: 19731.929688\n",
      "Train Epoch: 120 [202368/225000 (90%)] Loss: 19151.105469\n",
      "Train Epoch: 120 [204864/225000 (91%)] Loss: 19462.792969\n",
      "Train Epoch: 120 [207360/225000 (92%)] Loss: 19992.285156\n",
      "Train Epoch: 120 [209856/225000 (93%)] Loss: 19495.531250\n",
      "Train Epoch: 120 [212352/225000 (94%)] Loss: 19211.226562\n",
      "Train Epoch: 120 [214848/225000 (95%)] Loss: 18496.154297\n",
      "Train Epoch: 120 [217344/225000 (97%)] Loss: 19538.156250\n",
      "Train Epoch: 120 [219840/225000 (98%)] Loss: 19211.644531\n",
      "Train Epoch: 120 [222336/225000 (99%)] Loss: 18871.666016\n",
      "Train Epoch: 120 [224832/225000 (100%)] Loss: 18983.457031\n",
      "    epoch          : 120\n",
      "    loss           : 19369.518458031143\n",
      "    val_loss       : 19271.898421814425\n",
      "Train Epoch: 121 [192/225000 (0%)] Loss: 19472.898438\n",
      "Train Epoch: 121 [2688/225000 (1%)] Loss: 19883.453125\n",
      "Train Epoch: 121 [5184/225000 (2%)] Loss: 19423.873047\n",
      "Train Epoch: 121 [7680/225000 (3%)] Loss: 19762.835938\n",
      "Train Epoch: 121 [10176/225000 (5%)] Loss: 19304.544922\n",
      "Train Epoch: 121 [12672/225000 (6%)] Loss: 19354.333984\n",
      "Train Epoch: 121 [15168/225000 (7%)] Loss: 19019.521484\n",
      "Train Epoch: 121 [17664/225000 (8%)] Loss: 19259.837891\n",
      "Train Epoch: 121 [20160/225000 (9%)] Loss: 19391.919922\n",
      "Train Epoch: 121 [22656/225000 (10%)] Loss: 18967.150391\n",
      "Train Epoch: 121 [25152/225000 (11%)] Loss: 19483.962891\n",
      "Train Epoch: 121 [27648/225000 (12%)] Loss: 19314.687500\n",
      "Train Epoch: 121 [30144/225000 (13%)] Loss: 19423.402344\n",
      "Train Epoch: 121 [32640/225000 (15%)] Loss: 19232.007812\n",
      "Train Epoch: 121 [35136/225000 (16%)] Loss: 19173.031250\n",
      "Train Epoch: 121 [37632/225000 (17%)] Loss: 19885.988281\n",
      "Train Epoch: 121 [40128/225000 (18%)] Loss: 19642.753906\n",
      "Train Epoch: 121 [42624/225000 (19%)] Loss: 19176.707031\n",
      "Train Epoch: 121 [45120/225000 (20%)] Loss: 19819.332031\n",
      "Train Epoch: 121 [47616/225000 (21%)] Loss: 18480.382812\n",
      "Train Epoch: 121 [50112/225000 (22%)] Loss: 19021.347656\n",
      "Train Epoch: 121 [52608/225000 (23%)] Loss: 19565.431641\n",
      "Train Epoch: 121 [55104/225000 (24%)] Loss: 19222.285156\n",
      "Train Epoch: 121 [57600/225000 (26%)] Loss: 19421.621094\n",
      "Train Epoch: 121 [60096/225000 (27%)] Loss: 19285.914062\n",
      "Train Epoch: 121 [62592/225000 (28%)] Loss: 19236.267578\n",
      "Train Epoch: 121 [65088/225000 (29%)] Loss: 19297.623047\n",
      "Train Epoch: 121 [67584/225000 (30%)] Loss: 19536.714844\n",
      "Train Epoch: 121 [70080/225000 (31%)] Loss: 19503.451172\n",
      "Train Epoch: 121 [72576/225000 (32%)] Loss: 19304.826172\n",
      "Train Epoch: 121 [75072/225000 (33%)] Loss: 19162.371094\n",
      "Train Epoch: 121 [77568/225000 (34%)] Loss: 19160.580078\n",
      "Train Epoch: 121 [80064/225000 (36%)] Loss: 19550.042969\n",
      "Train Epoch: 121 [82560/225000 (37%)] Loss: 19365.769531\n",
      "Train Epoch: 121 [85056/225000 (38%)] Loss: 19595.171875\n",
      "Train Epoch: 121 [87552/225000 (39%)] Loss: 19200.533203\n",
      "Train Epoch: 121 [90048/225000 (40%)] Loss: 19341.386719\n",
      "Train Epoch: 121 [92544/225000 (41%)] Loss: 19368.367188\n",
      "Train Epoch: 121 [95040/225000 (42%)] Loss: 19471.871094\n",
      "Train Epoch: 121 [97536/225000 (43%)] Loss: 19509.765625\n",
      "Train Epoch: 121 [100032/225000 (44%)] Loss: 19113.679688\n",
      "Train Epoch: 121 [102528/225000 (46%)] Loss: 19406.500000\n",
      "Train Epoch: 121 [105024/225000 (47%)] Loss: 19037.242188\n",
      "Train Epoch: 121 [107520/225000 (48%)] Loss: 19399.970703\n",
      "Train Epoch: 121 [110016/225000 (49%)] Loss: 19168.935547\n",
      "Train Epoch: 121 [112512/225000 (50%)] Loss: 19225.503906\n",
      "Train Epoch: 121 [115008/225000 (51%)] Loss: 19385.796875\n",
      "Train Epoch: 121 [117504/225000 (52%)] Loss: 19349.669922\n",
      "Train Epoch: 121 [120000/225000 (53%)] Loss: 18813.589844\n",
      "Train Epoch: 121 [122496/225000 (54%)] Loss: 18948.462891\n",
      "Train Epoch: 121 [124992/225000 (56%)] Loss: 19806.886719\n",
      "Train Epoch: 121 [127488/225000 (57%)] Loss: 19298.148438\n",
      "Train Epoch: 121 [129984/225000 (58%)] Loss: 19150.304688\n",
      "Train Epoch: 121 [132480/225000 (59%)] Loss: 19242.224609\n",
      "Train Epoch: 121 [134976/225000 (60%)] Loss: 19386.601562\n",
      "Train Epoch: 121 [137472/225000 (61%)] Loss: 19709.050781\n",
      "Train Epoch: 121 [139968/225000 (62%)] Loss: 19302.253906\n",
      "Train Epoch: 121 [142464/225000 (63%)] Loss: 19265.789062\n",
      "Train Epoch: 121 [144960/225000 (64%)] Loss: 19379.437500\n",
      "Train Epoch: 121 [147456/225000 (66%)] Loss: 19377.509766\n",
      "Train Epoch: 121 [149952/225000 (67%)] Loss: 19872.947266\n",
      "Train Epoch: 121 [152448/225000 (68%)] Loss: 19341.626953\n",
      "Train Epoch: 121 [154944/225000 (69%)] Loss: 19680.039062\n",
      "Train Epoch: 121 [157440/225000 (70%)] Loss: 19602.054688\n",
      "Train Epoch: 121 [159936/225000 (71%)] Loss: 19032.199219\n",
      "Train Epoch: 121 [162432/225000 (72%)] Loss: 19473.804688\n",
      "Train Epoch: 121 [164928/225000 (73%)] Loss: 19340.302734\n",
      "Train Epoch: 121 [167424/225000 (74%)] Loss: 19008.697266\n",
      "Train Epoch: 121 [169920/225000 (76%)] Loss: 19581.154297\n",
      "Train Epoch: 121 [172416/225000 (77%)] Loss: 19103.593750\n",
      "Train Epoch: 121 [174912/225000 (78%)] Loss: 19070.648438\n",
      "Train Epoch: 121 [177408/225000 (79%)] Loss: 19206.488281\n",
      "Train Epoch: 121 [179904/225000 (80%)] Loss: 19620.925781\n",
      "Train Epoch: 121 [182400/225000 (81%)] Loss: 19609.046875\n",
      "Train Epoch: 121 [184896/225000 (82%)] Loss: 19294.496094\n",
      "Train Epoch: 121 [187392/225000 (83%)] Loss: 19462.496094\n",
      "Train Epoch: 121 [189888/225000 (84%)] Loss: 19559.271484\n",
      "Train Epoch: 121 [192384/225000 (86%)] Loss: 19359.478516\n",
      "Train Epoch: 121 [194880/225000 (87%)] Loss: 19436.671875\n",
      "Train Epoch: 121 [197376/225000 (88%)] Loss: 19028.998047\n",
      "Train Epoch: 121 [199872/225000 (89%)] Loss: 19223.064453\n",
      "Train Epoch: 121 [202368/225000 (90%)] Loss: 19175.707031\n",
      "Train Epoch: 121 [204864/225000 (91%)] Loss: 19433.562500\n",
      "Train Epoch: 121 [207360/225000 (92%)] Loss: 19395.535156\n",
      "Train Epoch: 121 [209856/225000 (93%)] Loss: 19137.140625\n",
      "Train Epoch: 121 [212352/225000 (94%)] Loss: 19358.189453\n",
      "Train Epoch: 121 [214848/225000 (95%)] Loss: 19181.785156\n",
      "Train Epoch: 121 [217344/225000 (97%)] Loss: 19344.441406\n",
      "Train Epoch: 121 [219840/225000 (98%)] Loss: 19601.976562\n",
      "Train Epoch: 121 [222336/225000 (99%)] Loss: 19608.968750\n",
      "Train Epoch: 121 [224832/225000 (100%)] Loss: 19495.113281\n",
      "    epoch          : 121\n",
      "    loss           : 19381.720818112735\n",
      "    val_loss       : 19306.692409486262\n",
      "Train Epoch: 122 [192/225000 (0%)] Loss: 19205.853516\n",
      "Train Epoch: 122 [2688/225000 (1%)] Loss: 19314.595703\n",
      "Train Epoch: 122 [5184/225000 (2%)] Loss: 19392.392578\n",
      "Train Epoch: 122 [7680/225000 (3%)] Loss: 19115.531250\n",
      "Train Epoch: 122 [10176/225000 (5%)] Loss: 19763.726562\n",
      "Train Epoch: 122 [12672/225000 (6%)] Loss: 19683.062500\n",
      "Train Epoch: 122 [15168/225000 (7%)] Loss: 19496.449219\n",
      "Train Epoch: 122 [17664/225000 (8%)] Loss: 19692.957031\n",
      "Train Epoch: 122 [20160/225000 (9%)] Loss: 19512.572266\n",
      "Train Epoch: 122 [22656/225000 (10%)] Loss: 19323.841797\n",
      "Train Epoch: 122 [25152/225000 (11%)] Loss: 19318.480469\n",
      "Train Epoch: 122 [27648/225000 (12%)] Loss: 19614.839844\n",
      "Train Epoch: 122 [30144/225000 (13%)] Loss: 19416.121094\n",
      "Train Epoch: 122 [32640/225000 (15%)] Loss: 19669.037109\n",
      "Train Epoch: 122 [35136/225000 (16%)] Loss: 19820.761719\n",
      "Train Epoch: 122 [37632/225000 (17%)] Loss: 19424.666016\n",
      "Train Epoch: 122 [40128/225000 (18%)] Loss: 19245.167969\n",
      "Train Epoch: 122 [42624/225000 (19%)] Loss: 18909.156250\n",
      "Train Epoch: 122 [45120/225000 (20%)] Loss: 19260.968750\n",
      "Train Epoch: 122 [47616/225000 (21%)] Loss: 19190.628906\n",
      "Train Epoch: 122 [50112/225000 (22%)] Loss: 18872.843750\n",
      "Train Epoch: 122 [52608/225000 (23%)] Loss: 19680.753906\n",
      "Train Epoch: 122 [55104/225000 (24%)] Loss: 19311.691406\n",
      "Train Epoch: 122 [57600/225000 (26%)] Loss: 18868.531250\n",
      "Train Epoch: 122 [60096/225000 (27%)] Loss: 19746.507812\n",
      "Train Epoch: 122 [62592/225000 (28%)] Loss: 18857.816406\n",
      "Train Epoch: 122 [65088/225000 (29%)] Loss: 19471.574219\n",
      "Train Epoch: 122 [67584/225000 (30%)] Loss: 19145.583984\n",
      "Train Epoch: 122 [70080/225000 (31%)] Loss: 19411.177734\n",
      "Train Epoch: 122 [72576/225000 (32%)] Loss: 19554.718750\n",
      "Train Epoch: 122 [75072/225000 (33%)] Loss: 19418.925781\n",
      "Train Epoch: 122 [77568/225000 (34%)] Loss: 19239.259766\n",
      "Train Epoch: 122 [80064/225000 (36%)] Loss: 19254.056641\n",
      "Train Epoch: 122 [82560/225000 (37%)] Loss: 19634.652344\n",
      "Train Epoch: 122 [85056/225000 (38%)] Loss: 19296.164062\n",
      "Train Epoch: 122 [87552/225000 (39%)] Loss: 19303.183594\n",
      "Train Epoch: 122 [90048/225000 (40%)] Loss: 19698.574219\n",
      "Train Epoch: 122 [92544/225000 (41%)] Loss: 19375.789062\n",
      "Train Epoch: 122 [95040/225000 (42%)] Loss: 19267.865234\n",
      "Train Epoch: 122 [97536/225000 (43%)] Loss: 19088.398438\n",
      "Train Epoch: 122 [100032/225000 (44%)] Loss: 19121.191406\n",
      "Train Epoch: 122 [102528/225000 (46%)] Loss: 19126.957031\n",
      "Train Epoch: 122 [105024/225000 (47%)] Loss: 19283.281250\n",
      "Train Epoch: 122 [107520/225000 (48%)] Loss: 19399.548828\n",
      "Train Epoch: 122 [110016/225000 (49%)] Loss: 19352.300781\n",
      "Train Epoch: 122 [112512/225000 (50%)] Loss: 19403.476562\n",
      "Train Epoch: 122 [115008/225000 (51%)] Loss: 19539.919922\n",
      "Train Epoch: 122 [117504/225000 (52%)] Loss: 19180.439453\n",
      "Train Epoch: 122 [120000/225000 (53%)] Loss: 19340.244141\n",
      "Train Epoch: 122 [122496/225000 (54%)] Loss: 19186.140625\n",
      "Train Epoch: 122 [124992/225000 (56%)] Loss: 19257.970703\n",
      "Train Epoch: 122 [127488/225000 (57%)] Loss: 19377.617188\n",
      "Train Epoch: 122 [129984/225000 (58%)] Loss: 19862.447266\n",
      "Train Epoch: 122 [132480/225000 (59%)] Loss: 18944.732422\n",
      "Train Epoch: 122 [134976/225000 (60%)] Loss: 19233.333984\n",
      "Train Epoch: 122 [137472/225000 (61%)] Loss: 19320.886719\n",
      "Train Epoch: 122 [139968/225000 (62%)] Loss: 19292.523438\n",
      "Train Epoch: 122 [142464/225000 (63%)] Loss: 19711.988281\n",
      "Train Epoch: 122 [144960/225000 (64%)] Loss: 19369.640625\n",
      "Train Epoch: 122 [147456/225000 (66%)] Loss: 19029.726562\n",
      "Train Epoch: 122 [149952/225000 (67%)] Loss: 19240.337891\n",
      "Train Epoch: 122 [152448/225000 (68%)] Loss: 19551.300781\n",
      "Train Epoch: 122 [154944/225000 (69%)] Loss: 19556.744141\n",
      "Train Epoch: 122 [157440/225000 (70%)] Loss: 18922.544922\n",
      "Train Epoch: 122 [159936/225000 (71%)] Loss: 19237.925781\n",
      "Train Epoch: 122 [162432/225000 (72%)] Loss: 19049.869141\n",
      "Train Epoch: 122 [164928/225000 (73%)] Loss: 19473.699219\n",
      "Train Epoch: 122 [167424/225000 (74%)] Loss: 19105.816406\n",
      "Train Epoch: 122 [169920/225000 (76%)] Loss: 19787.740234\n",
      "Train Epoch: 122 [172416/225000 (77%)] Loss: 19476.900391\n",
      "Train Epoch: 122 [174912/225000 (78%)] Loss: 19128.464844\n",
      "Train Epoch: 122 [177408/225000 (79%)] Loss: 19636.630859\n",
      "Train Epoch: 122 [179904/225000 (80%)] Loss: 19489.005859\n",
      "Train Epoch: 122 [182400/225000 (81%)] Loss: 19496.175781\n",
      "Train Epoch: 122 [184896/225000 (82%)] Loss: 19040.332031\n",
      "Train Epoch: 122 [187392/225000 (83%)] Loss: 19094.917969\n",
      "Train Epoch: 122 [189888/225000 (84%)] Loss: 19607.722656\n",
      "Train Epoch: 122 [192384/225000 (86%)] Loss: 19297.070312\n",
      "Train Epoch: 122 [194880/225000 (87%)] Loss: 19762.500000\n",
      "Train Epoch: 122 [197376/225000 (88%)] Loss: 19460.855469\n",
      "Train Epoch: 122 [199872/225000 (89%)] Loss: 18897.753906\n",
      "Train Epoch: 122 [202368/225000 (90%)] Loss: 19156.835938\n",
      "Train Epoch: 122 [204864/225000 (91%)] Loss: 19095.878906\n",
      "Train Epoch: 122 [207360/225000 (92%)] Loss: 18754.941406\n",
      "Train Epoch: 122 [209856/225000 (93%)] Loss: 19359.654297\n",
      "Train Epoch: 122 [212352/225000 (94%)] Loss: 19166.242188\n",
      "Train Epoch: 122 [214848/225000 (95%)] Loss: 19737.003906\n",
      "Train Epoch: 122 [217344/225000 (97%)] Loss: 19445.414062\n",
      "Train Epoch: 122 [219840/225000 (98%)] Loss: 19439.328125\n",
      "Train Epoch: 122 [222336/225000 (99%)] Loss: 19245.250000\n",
      "Train Epoch: 122 [224832/225000 (100%)] Loss: 19719.867188\n",
      "    epoch          : 122\n",
      "    loss           : 19360.723352842364\n",
      "    val_loss       : 19262.45069230786\n",
      "Train Epoch: 123 [192/225000 (0%)] Loss: 19804.919922\n",
      "Train Epoch: 123 [2688/225000 (1%)] Loss: 19483.480469\n",
      "Train Epoch: 123 [5184/225000 (2%)] Loss: 19256.492188\n",
      "Train Epoch: 123 [7680/225000 (3%)] Loss: 19454.232422\n",
      "Train Epoch: 123 [10176/225000 (5%)] Loss: 19533.257812\n",
      "Train Epoch: 123 [12672/225000 (6%)] Loss: 19521.003906\n",
      "Train Epoch: 123 [15168/225000 (7%)] Loss: 19299.335938\n",
      "Train Epoch: 123 [17664/225000 (8%)] Loss: 18747.371094\n",
      "Train Epoch: 123 [20160/225000 (9%)] Loss: 18776.179688\n",
      "Train Epoch: 123 [22656/225000 (10%)] Loss: 19234.271484\n",
      "Train Epoch: 123 [25152/225000 (11%)] Loss: 19252.242188\n",
      "Train Epoch: 123 [27648/225000 (12%)] Loss: 19568.632812\n",
      "Train Epoch: 123 [30144/225000 (13%)] Loss: 19368.406250\n",
      "Train Epoch: 123 [32640/225000 (15%)] Loss: 19378.402344\n",
      "Train Epoch: 123 [35136/225000 (16%)] Loss: 19370.035156\n",
      "Train Epoch: 123 [37632/225000 (17%)] Loss: 19641.169922\n",
      "Train Epoch: 123 [40128/225000 (18%)] Loss: 18667.025391\n",
      "Train Epoch: 123 [42624/225000 (19%)] Loss: 19572.074219\n",
      "Train Epoch: 123 [45120/225000 (20%)] Loss: 19118.134766\n",
      "Train Epoch: 123 [47616/225000 (21%)] Loss: 19437.371094\n",
      "Train Epoch: 123 [50112/225000 (22%)] Loss: 19294.519531\n",
      "Train Epoch: 123 [52608/225000 (23%)] Loss: 18700.421875\n",
      "Train Epoch: 123 [55104/225000 (24%)] Loss: 19685.166016\n",
      "Train Epoch: 123 [57600/225000 (26%)] Loss: 19368.777344\n",
      "Train Epoch: 123 [60096/225000 (27%)] Loss: 19416.203125\n",
      "Train Epoch: 123 [62592/225000 (28%)] Loss: 19813.562500\n",
      "Train Epoch: 123 [65088/225000 (29%)] Loss: 19377.101562\n",
      "Train Epoch: 123 [67584/225000 (30%)] Loss: 19362.861328\n",
      "Train Epoch: 123 [70080/225000 (31%)] Loss: 19369.199219\n",
      "Train Epoch: 123 [72576/225000 (32%)] Loss: 19583.703125\n",
      "Train Epoch: 123 [75072/225000 (33%)] Loss: 19528.234375\n",
      "Train Epoch: 123 [77568/225000 (34%)] Loss: 19637.593750\n",
      "Train Epoch: 123 [80064/225000 (36%)] Loss: 19463.593750\n",
      "Train Epoch: 123 [82560/225000 (37%)] Loss: 19146.734375\n",
      "Train Epoch: 123 [85056/225000 (38%)] Loss: 19428.667969\n",
      "Train Epoch: 123 [87552/225000 (39%)] Loss: 19487.816406\n",
      "Train Epoch: 123 [90048/225000 (40%)] Loss: 19144.847656\n",
      "Train Epoch: 123 [92544/225000 (41%)] Loss: 19612.158203\n",
      "Train Epoch: 123 [95040/225000 (42%)] Loss: 18907.871094\n",
      "Train Epoch: 123 [97536/225000 (43%)] Loss: 18894.621094\n",
      "Train Epoch: 123 [100032/225000 (44%)] Loss: 19827.714844\n",
      "Train Epoch: 123 [102528/225000 (46%)] Loss: 19434.685547\n",
      "Train Epoch: 123 [105024/225000 (47%)] Loss: 19364.550781\n",
      "Train Epoch: 123 [107520/225000 (48%)] Loss: 19150.113281\n",
      "Train Epoch: 123 [110016/225000 (49%)] Loss: 19755.962891\n",
      "Train Epoch: 123 [112512/225000 (50%)] Loss: 19484.931641\n",
      "Train Epoch: 123 [115008/225000 (51%)] Loss: 19325.992188\n",
      "Train Epoch: 123 [117504/225000 (52%)] Loss: 19169.378906\n",
      "Train Epoch: 123 [120000/225000 (53%)] Loss: 19371.351562\n",
      "Train Epoch: 123 [122496/225000 (54%)] Loss: 19431.917969\n",
      "Train Epoch: 123 [124992/225000 (56%)] Loss: 18994.484375\n",
      "Train Epoch: 123 [127488/225000 (57%)] Loss: 19174.843750\n",
      "Train Epoch: 123 [129984/225000 (58%)] Loss: 19610.824219\n",
      "Train Epoch: 123 [132480/225000 (59%)] Loss: 19499.152344\n",
      "Train Epoch: 123 [134976/225000 (60%)] Loss: 19299.402344\n",
      "Train Epoch: 123 [137472/225000 (61%)] Loss: 19741.488281\n",
      "Train Epoch: 123 [139968/225000 (62%)] Loss: 19628.296875\n",
      "Train Epoch: 123 [142464/225000 (63%)] Loss: 19142.789062\n",
      "Train Epoch: 123 [144960/225000 (64%)] Loss: 19215.638672\n",
      "Train Epoch: 123 [147456/225000 (66%)] Loss: 19004.580078\n",
      "Train Epoch: 123 [149952/225000 (67%)] Loss: 19258.287109\n",
      "Train Epoch: 123 [152448/225000 (68%)] Loss: 19126.984375\n",
      "Train Epoch: 123 [154944/225000 (69%)] Loss: 19219.191406\n",
      "Train Epoch: 123 [157440/225000 (70%)] Loss: 19655.173828\n",
      "Train Epoch: 123 [159936/225000 (71%)] Loss: 19028.320312\n",
      "Train Epoch: 123 [162432/225000 (72%)] Loss: 19531.330078\n",
      "Train Epoch: 123 [164928/225000 (73%)] Loss: 19209.085938\n",
      "Train Epoch: 123 [167424/225000 (74%)] Loss: 19305.851562\n",
      "Train Epoch: 123 [169920/225000 (76%)] Loss: 19198.953125\n",
      "Train Epoch: 123 [172416/225000 (77%)] Loss: 19370.492188\n",
      "Train Epoch: 123 [174912/225000 (78%)] Loss: 19416.988281\n",
      "Train Epoch: 123 [177408/225000 (79%)] Loss: 19162.824219\n",
      "Train Epoch: 123 [179904/225000 (80%)] Loss: 19536.279297\n",
      "Train Epoch: 123 [182400/225000 (81%)] Loss: 19288.978516\n",
      "Train Epoch: 123 [184896/225000 (82%)] Loss: 19449.851562\n",
      "Train Epoch: 123 [187392/225000 (83%)] Loss: 19364.125000\n",
      "Train Epoch: 123 [189888/225000 (84%)] Loss: 18942.578125\n",
      "Train Epoch: 123 [192384/225000 (86%)] Loss: 19257.275391\n",
      "Train Epoch: 123 [194880/225000 (87%)] Loss: 19673.445312\n",
      "Train Epoch: 123 [197376/225000 (88%)] Loss: 19105.164062\n",
      "Train Epoch: 123 [199872/225000 (89%)] Loss: 19323.025391\n",
      "Train Epoch: 123 [202368/225000 (90%)] Loss: 19373.794922\n",
      "Train Epoch: 123 [204864/225000 (91%)] Loss: 19695.367188\n",
      "Train Epoch: 123 [207360/225000 (92%)] Loss: 19134.781250\n",
      "Train Epoch: 123 [209856/225000 (93%)] Loss: 19207.351562\n",
      "Train Epoch: 123 [212352/225000 (94%)] Loss: 18809.750000\n",
      "Train Epoch: 123 [214848/225000 (95%)] Loss: 19175.757812\n",
      "Train Epoch: 123 [217344/225000 (97%)] Loss: 19544.320312\n",
      "Train Epoch: 123 [219840/225000 (98%)] Loss: 19765.726562\n",
      "Train Epoch: 123 [222336/225000 (99%)] Loss: 19435.019531\n",
      "Train Epoch: 123 [224832/225000 (100%)] Loss: 19852.673828\n",
      "    epoch          : 123\n",
      "    loss           : 19363.874426727816\n",
      "    val_loss       : 19256.98437963096\n",
      "Train Epoch: 124 [192/225000 (0%)] Loss: 19731.111328\n",
      "Train Epoch: 124 [2688/225000 (1%)] Loss: 19317.015625\n",
      "Train Epoch: 124 [5184/225000 (2%)] Loss: 19423.800781\n",
      "Train Epoch: 124 [7680/225000 (3%)] Loss: 19557.679688\n",
      "Train Epoch: 124 [10176/225000 (5%)] Loss: 19739.435547\n",
      "Train Epoch: 124 [12672/225000 (6%)] Loss: 19385.732422\n",
      "Train Epoch: 124 [15168/225000 (7%)] Loss: 19490.369141\n",
      "Train Epoch: 124 [17664/225000 (8%)] Loss: 18883.453125\n",
      "Train Epoch: 124 [20160/225000 (9%)] Loss: 19164.605469\n",
      "Train Epoch: 124 [22656/225000 (10%)] Loss: 19205.894531\n",
      "Train Epoch: 124 [25152/225000 (11%)] Loss: 19519.593750\n",
      "Train Epoch: 124 [27648/225000 (12%)] Loss: 19837.175781\n",
      "Train Epoch: 124 [30144/225000 (13%)] Loss: 19194.830078\n",
      "Train Epoch: 124 [32640/225000 (15%)] Loss: 19270.283203\n",
      "Train Epoch: 124 [35136/225000 (16%)] Loss: 18426.705078\n",
      "Train Epoch: 124 [37632/225000 (17%)] Loss: 19365.250000\n",
      "Train Epoch: 124 [40128/225000 (18%)] Loss: 19727.152344\n",
      "Train Epoch: 124 [42624/225000 (19%)] Loss: 19598.878906\n",
      "Train Epoch: 124 [45120/225000 (20%)] Loss: 19099.757812\n",
      "Train Epoch: 124 [47616/225000 (21%)] Loss: 19102.035156\n",
      "Train Epoch: 124 [50112/225000 (22%)] Loss: 18884.972656\n",
      "Train Epoch: 124 [52608/225000 (23%)] Loss: 19496.927734\n",
      "Train Epoch: 124 [55104/225000 (24%)] Loss: 19633.335938\n",
      "Train Epoch: 124 [57600/225000 (26%)] Loss: 19525.781250\n",
      "Train Epoch: 124 [60096/225000 (27%)] Loss: 19376.847656\n",
      "Train Epoch: 124 [62592/225000 (28%)] Loss: 19337.427734\n",
      "Train Epoch: 124 [65088/225000 (29%)] Loss: 19103.597656\n",
      "Train Epoch: 124 [67584/225000 (30%)] Loss: 19277.250000\n",
      "Train Epoch: 124 [70080/225000 (31%)] Loss: 18885.416016\n",
      "Train Epoch: 124 [72576/225000 (32%)] Loss: 18984.408203\n",
      "Train Epoch: 124 [75072/225000 (33%)] Loss: 19439.359375\n",
      "Train Epoch: 124 [77568/225000 (34%)] Loss: 18701.746094\n",
      "Train Epoch: 124 [80064/225000 (36%)] Loss: 19269.923828\n",
      "Train Epoch: 124 [82560/225000 (37%)] Loss: 19498.044922\n",
      "Train Epoch: 124 [85056/225000 (38%)] Loss: 19014.859375\n",
      "Train Epoch: 124 [87552/225000 (39%)] Loss: 19163.074219\n",
      "Train Epoch: 124 [90048/225000 (40%)] Loss: 19335.451172\n",
      "Train Epoch: 124 [92544/225000 (41%)] Loss: 19638.984375\n",
      "Train Epoch: 124 [95040/225000 (42%)] Loss: 18924.195312\n",
      "Train Epoch: 124 [97536/225000 (43%)] Loss: 19918.244141\n",
      "Train Epoch: 124 [100032/225000 (44%)] Loss: 19380.699219\n",
      "Train Epoch: 124 [102528/225000 (46%)] Loss: 19388.777344\n",
      "Train Epoch: 124 [105024/225000 (47%)] Loss: 19635.181641\n",
      "Train Epoch: 124 [107520/225000 (48%)] Loss: 19309.507812\n",
      "Train Epoch: 124 [110016/225000 (49%)] Loss: 18602.458984\n",
      "Train Epoch: 124 [112512/225000 (50%)] Loss: 19320.664062\n",
      "Train Epoch: 124 [115008/225000 (51%)] Loss: 19616.521484\n",
      "Train Epoch: 124 [117504/225000 (52%)] Loss: 19459.738281\n",
      "Train Epoch: 124 [120000/225000 (53%)] Loss: 19412.888672\n",
      "Train Epoch: 124 [122496/225000 (54%)] Loss: 19591.750000\n",
      "Train Epoch: 124 [124992/225000 (56%)] Loss: 19367.220703\n",
      "Train Epoch: 124 [127488/225000 (57%)] Loss: 19945.763672\n",
      "Train Epoch: 124 [129984/225000 (58%)] Loss: 19293.675781\n",
      "Train Epoch: 124 [132480/225000 (59%)] Loss: 19001.468750\n",
      "Train Epoch: 124 [134976/225000 (60%)] Loss: 19626.814453\n",
      "Train Epoch: 124 [137472/225000 (61%)] Loss: 19607.308594\n",
      "Train Epoch: 124 [139968/225000 (62%)] Loss: 19355.289062\n",
      "Train Epoch: 124 [142464/225000 (63%)] Loss: 19743.414062\n",
      "Train Epoch: 124 [144960/225000 (64%)] Loss: 19217.804688\n",
      "Train Epoch: 124 [147456/225000 (66%)] Loss: 19334.537109\n",
      "Train Epoch: 124 [149952/225000 (67%)] Loss: 19132.712891\n",
      "Train Epoch: 124 [152448/225000 (68%)] Loss: 19240.566406\n",
      "Train Epoch: 124 [154944/225000 (69%)] Loss: 19796.806641\n",
      "Train Epoch: 124 [157440/225000 (70%)] Loss: 19453.554688\n",
      "Train Epoch: 124 [159936/225000 (71%)] Loss: 19405.117188\n",
      "Train Epoch: 124 [162432/225000 (72%)] Loss: 19064.957031\n",
      "Train Epoch: 124 [164928/225000 (73%)] Loss: 19534.990234\n",
      "Train Epoch: 124 [167424/225000 (74%)] Loss: 20055.273438\n",
      "Train Epoch: 124 [169920/225000 (76%)] Loss: 19204.726562\n",
      "Train Epoch: 124 [172416/225000 (77%)] Loss: 19064.351562\n",
      "Train Epoch: 124 [174912/225000 (78%)] Loss: 19536.011719\n",
      "Train Epoch: 124 [177408/225000 (79%)] Loss: 19532.234375\n",
      "Train Epoch: 124 [179904/225000 (80%)] Loss: 19119.519531\n",
      "Train Epoch: 124 [182400/225000 (81%)] Loss: 19284.656250\n",
      "Train Epoch: 124 [184896/225000 (82%)] Loss: 19416.824219\n",
      "Train Epoch: 124 [187392/225000 (83%)] Loss: 19554.875000\n",
      "Train Epoch: 124 [189888/225000 (84%)] Loss: 19173.257812\n",
      "Train Epoch: 124 [192384/225000 (86%)] Loss: 19261.273438\n",
      "Train Epoch: 124 [194880/225000 (87%)] Loss: 19654.625000\n",
      "Train Epoch: 124 [197376/225000 (88%)] Loss: 19251.375000\n",
      "Train Epoch: 124 [199872/225000 (89%)] Loss: 19200.925781\n",
      "Train Epoch: 124 [202368/225000 (90%)] Loss: 19669.394531\n",
      "Train Epoch: 124 [204864/225000 (91%)] Loss: 19469.253906\n",
      "Train Epoch: 124 [207360/225000 (92%)] Loss: 19587.347656\n",
      "Train Epoch: 124 [209856/225000 (93%)] Loss: 19310.425781\n",
      "Train Epoch: 124 [212352/225000 (94%)] Loss: 19351.800781\n",
      "Train Epoch: 124 [214848/225000 (95%)] Loss: 19058.460938\n",
      "Train Epoch: 124 [217344/225000 (97%)] Loss: 19339.421875\n",
      "Train Epoch: 124 [219840/225000 (98%)] Loss: 19507.238281\n",
      "Train Epoch: 124 [222336/225000 (99%)] Loss: 19590.804688\n",
      "Train Epoch: 124 [224832/225000 (100%)] Loss: 19175.089844\n",
      "    epoch          : 124\n",
      "    loss           : 19349.298424834684\n",
      "    val_loss       : 19260.215925427794\n",
      "Train Epoch: 125 [192/225000 (0%)] Loss: 19211.339844\n",
      "Train Epoch: 125 [2688/225000 (1%)] Loss: 19081.822266\n",
      "Train Epoch: 125 [5184/225000 (2%)] Loss: 18860.933594\n",
      "Train Epoch: 125 [7680/225000 (3%)] Loss: 19240.925781\n",
      "Train Epoch: 125 [10176/225000 (5%)] Loss: 19844.589844\n",
      "Train Epoch: 125 [12672/225000 (6%)] Loss: 19820.958984\n",
      "Train Epoch: 125 [15168/225000 (7%)] Loss: 19298.957031\n",
      "Train Epoch: 125 [17664/225000 (8%)] Loss: 19450.488281\n",
      "Train Epoch: 125 [20160/225000 (9%)] Loss: 19581.527344\n",
      "Train Epoch: 125 [22656/225000 (10%)] Loss: 19019.687500\n",
      "Train Epoch: 125 [25152/225000 (11%)] Loss: 18808.066406\n",
      "Train Epoch: 125 [27648/225000 (12%)] Loss: 19137.007812\n",
      "Train Epoch: 125 [30144/225000 (13%)] Loss: 20170.857422\n",
      "Train Epoch: 125 [32640/225000 (15%)] Loss: 18967.486328\n",
      "Train Epoch: 125 [35136/225000 (16%)] Loss: 19609.611328\n",
      "Train Epoch: 125 [37632/225000 (17%)] Loss: 19325.708984\n",
      "Train Epoch: 125 [40128/225000 (18%)] Loss: 19109.433594\n",
      "Train Epoch: 125 [42624/225000 (19%)] Loss: 19380.992188\n",
      "Train Epoch: 125 [45120/225000 (20%)] Loss: 19072.933594\n",
      "Train Epoch: 125 [47616/225000 (21%)] Loss: 19793.566406\n",
      "Train Epoch: 125 [50112/225000 (22%)] Loss: 19515.070312\n",
      "Train Epoch: 125 [52608/225000 (23%)] Loss: 19405.574219\n",
      "Train Epoch: 125 [55104/225000 (24%)] Loss: 19405.326172\n",
      "Train Epoch: 125 [57600/225000 (26%)] Loss: 19386.050781\n",
      "Train Epoch: 125 [60096/225000 (27%)] Loss: 19222.917969\n",
      "Train Epoch: 125 [62592/225000 (28%)] Loss: 19487.382812\n",
      "Train Epoch: 125 [65088/225000 (29%)] Loss: 19193.820312\n",
      "Train Epoch: 125 [67584/225000 (30%)] Loss: 19351.750000\n",
      "Train Epoch: 125 [70080/225000 (31%)] Loss: 19765.900391\n",
      "Train Epoch: 125 [72576/225000 (32%)] Loss: 19286.357422\n",
      "Train Epoch: 125 [75072/225000 (33%)] Loss: 18983.222656\n",
      "Train Epoch: 125 [77568/225000 (34%)] Loss: 19074.070312\n",
      "Train Epoch: 125 [80064/225000 (36%)] Loss: 19203.839844\n",
      "Train Epoch: 125 [82560/225000 (37%)] Loss: 19205.968750\n",
      "Train Epoch: 125 [85056/225000 (38%)] Loss: 19512.230469\n",
      "Train Epoch: 125 [87552/225000 (39%)] Loss: 19357.023438\n",
      "Train Epoch: 125 [90048/225000 (40%)] Loss: 19416.384766\n",
      "Train Epoch: 125 [92544/225000 (41%)] Loss: 19404.316406\n",
      "Train Epoch: 125 [95040/225000 (42%)] Loss: 19212.953125\n",
      "Train Epoch: 125 [97536/225000 (43%)] Loss: 19217.560547\n",
      "Train Epoch: 125 [100032/225000 (44%)] Loss: 19075.302734\n",
      "Train Epoch: 125 [102528/225000 (46%)] Loss: 19450.503906\n",
      "Train Epoch: 125 [105024/225000 (47%)] Loss: 19510.519531\n",
      "Train Epoch: 125 [107520/225000 (48%)] Loss: 19243.425781\n",
      "Train Epoch: 125 [110016/225000 (49%)] Loss: 19624.464844\n",
      "Train Epoch: 125 [112512/225000 (50%)] Loss: 19606.781250\n",
      "Train Epoch: 125 [115008/225000 (51%)] Loss: 19380.257812\n",
      "Train Epoch: 125 [117504/225000 (52%)] Loss: 19027.886719\n",
      "Train Epoch: 125 [120000/225000 (53%)] Loss: 19353.167969\n",
      "Train Epoch: 125 [122496/225000 (54%)] Loss: 19026.861328\n",
      "Train Epoch: 125 [124992/225000 (56%)] Loss: 19164.343750\n",
      "Train Epoch: 125 [127488/225000 (57%)] Loss: 19655.542969\n",
      "Train Epoch: 125 [129984/225000 (58%)] Loss: 19143.605469\n",
      "Train Epoch: 125 [132480/225000 (59%)] Loss: 19370.964844\n",
      "Train Epoch: 125 [134976/225000 (60%)] Loss: 19425.824219\n",
      "Train Epoch: 125 [137472/225000 (61%)] Loss: 19472.748047\n",
      "Train Epoch: 125 [139968/225000 (62%)] Loss: 19344.812500\n",
      "Train Epoch: 125 [142464/225000 (63%)] Loss: 19791.214844\n",
      "Train Epoch: 125 [144960/225000 (64%)] Loss: 19544.720703\n",
      "Train Epoch: 125 [147456/225000 (66%)] Loss: 19635.562500\n",
      "Train Epoch: 125 [149952/225000 (67%)] Loss: 19382.988281\n",
      "Train Epoch: 125 [152448/225000 (68%)] Loss: 19525.816406\n",
      "Train Epoch: 125 [154944/225000 (69%)] Loss: 19595.265625\n",
      "Train Epoch: 125 [157440/225000 (70%)] Loss: 19309.859375\n",
      "Train Epoch: 125 [159936/225000 (71%)] Loss: 19040.710938\n",
      "Train Epoch: 125 [162432/225000 (72%)] Loss: 19371.351562\n",
      "Train Epoch: 125 [164928/225000 (73%)] Loss: 19457.718750\n",
      "Train Epoch: 125 [167424/225000 (74%)] Loss: 19517.765625\n",
      "Train Epoch: 125 [169920/225000 (76%)] Loss: 18845.378906\n",
      "Train Epoch: 125 [172416/225000 (77%)] Loss: 19577.183594\n",
      "Train Epoch: 125 [174912/225000 (78%)] Loss: 19338.005859\n",
      "Train Epoch: 125 [177408/225000 (79%)] Loss: 19450.058594\n",
      "Train Epoch: 125 [179904/225000 (80%)] Loss: 18732.865234\n",
      "Train Epoch: 125 [182400/225000 (81%)] Loss: 19570.550781\n",
      "Train Epoch: 125 [184896/225000 (82%)] Loss: 19550.390625\n",
      "Train Epoch: 125 [187392/225000 (83%)] Loss: 19343.755859\n",
      "Train Epoch: 125 [189888/225000 (84%)] Loss: 18884.638672\n",
      "Train Epoch: 125 [192384/225000 (86%)] Loss: 19369.593750\n",
      "Train Epoch: 125 [194880/225000 (87%)] Loss: 19448.765625\n",
      "Train Epoch: 125 [197376/225000 (88%)] Loss: 20060.330078\n",
      "Train Epoch: 125 [199872/225000 (89%)] Loss: 19229.359375\n",
      "Train Epoch: 125 [202368/225000 (90%)] Loss: 19951.910156\n",
      "Train Epoch: 125 [204864/225000 (91%)] Loss: 18546.757812\n",
      "Train Epoch: 125 [207360/225000 (92%)] Loss: 19390.390625\n",
      "Train Epoch: 125 [209856/225000 (93%)] Loss: 19481.312500\n",
      "Train Epoch: 125 [212352/225000 (94%)] Loss: 19250.496094\n",
      "Train Epoch: 125 [214848/225000 (95%)] Loss: 19554.472656\n",
      "Train Epoch: 125 [217344/225000 (97%)] Loss: 19082.054688\n",
      "Train Epoch: 125 [219840/225000 (98%)] Loss: 19191.484375\n",
      "Train Epoch: 125 [222336/225000 (99%)] Loss: 19136.765625\n",
      "Train Epoch: 125 [224832/225000 (100%)] Loss: 19727.343750\n",
      "    epoch          : 125\n",
      "    loss           : 19345.89476455845\n",
      "    val_loss       : 19277.273218604445\n",
      "Train Epoch: 126 [192/225000 (0%)] Loss: 19284.457031\n",
      "Train Epoch: 126 [2688/225000 (1%)] Loss: 19490.890625\n",
      "Train Epoch: 126 [5184/225000 (2%)] Loss: 19739.187500\n",
      "Train Epoch: 126 [7680/225000 (3%)] Loss: 19001.546875\n",
      "Train Epoch: 126 [10176/225000 (5%)] Loss: 19315.513672\n",
      "Train Epoch: 126 [12672/225000 (6%)] Loss: 19309.296875\n",
      "Train Epoch: 126 [15168/225000 (7%)] Loss: 19459.578125\n",
      "Train Epoch: 126 [17664/225000 (8%)] Loss: 19662.734375\n",
      "Train Epoch: 126 [20160/225000 (9%)] Loss: 19279.437500\n",
      "Train Epoch: 126 [22656/225000 (10%)] Loss: 19254.615234\n",
      "Train Epoch: 126 [25152/225000 (11%)] Loss: 19326.792969\n",
      "Train Epoch: 126 [27648/225000 (12%)] Loss: 19493.085938\n",
      "Train Epoch: 126 [30144/225000 (13%)] Loss: 19087.855469\n",
      "Train Epoch: 126 [32640/225000 (15%)] Loss: 19189.820312\n",
      "Train Epoch: 126 [35136/225000 (16%)] Loss: 19398.439453\n",
      "Train Epoch: 126 [37632/225000 (17%)] Loss: 19240.203125\n",
      "Train Epoch: 126 [40128/225000 (18%)] Loss: 19215.367188\n",
      "Train Epoch: 126 [42624/225000 (19%)] Loss: 19183.722656\n",
      "Train Epoch: 126 [45120/225000 (20%)] Loss: 19266.501953\n",
      "Train Epoch: 126 [47616/225000 (21%)] Loss: 19181.246094\n",
      "Train Epoch: 126 [50112/225000 (22%)] Loss: 19366.558594\n",
      "Train Epoch: 126 [52608/225000 (23%)] Loss: 19497.816406\n",
      "Train Epoch: 126 [55104/225000 (24%)] Loss: 19714.980469\n",
      "Train Epoch: 126 [57600/225000 (26%)] Loss: 19270.875000\n",
      "Train Epoch: 126 [60096/225000 (27%)] Loss: 19429.185547\n",
      "Train Epoch: 126 [62592/225000 (28%)] Loss: 19579.285156\n",
      "Train Epoch: 126 [65088/225000 (29%)] Loss: 19006.501953\n",
      "Train Epoch: 126 [67584/225000 (30%)] Loss: 19691.183594\n",
      "Train Epoch: 126 [70080/225000 (31%)] Loss: 19245.218750\n",
      "Train Epoch: 126 [72576/225000 (32%)] Loss: 18839.394531\n",
      "Train Epoch: 126 [75072/225000 (33%)] Loss: 19363.144531\n",
      "Train Epoch: 126 [77568/225000 (34%)] Loss: 19559.203125\n",
      "Train Epoch: 126 [80064/225000 (36%)] Loss: 19396.742188\n",
      "Train Epoch: 126 [82560/225000 (37%)] Loss: 19663.746094\n",
      "Train Epoch: 126 [85056/225000 (38%)] Loss: 19112.220703\n",
      "Train Epoch: 126 [87552/225000 (39%)] Loss: 19119.222656\n",
      "Train Epoch: 126 [90048/225000 (40%)] Loss: 19882.187500\n",
      "Train Epoch: 126 [92544/225000 (41%)] Loss: 19400.591797\n",
      "Train Epoch: 126 [95040/225000 (42%)] Loss: 19229.027344\n",
      "Train Epoch: 126 [97536/225000 (43%)] Loss: 19134.255859\n",
      "Train Epoch: 126 [100032/225000 (44%)] Loss: 19434.642578\n",
      "Train Epoch: 126 [102528/225000 (46%)] Loss: 19217.126953\n",
      "Train Epoch: 126 [105024/225000 (47%)] Loss: 19539.482422\n",
      "Train Epoch: 126 [107520/225000 (48%)] Loss: 19390.195312\n",
      "Train Epoch: 126 [110016/225000 (49%)] Loss: 19874.683594\n",
      "Train Epoch: 126 [112512/225000 (50%)] Loss: 19333.560547\n",
      "Train Epoch: 126 [115008/225000 (51%)] Loss: 18982.683594\n",
      "Train Epoch: 126 [117504/225000 (52%)] Loss: 19094.761719\n",
      "Train Epoch: 126 [120000/225000 (53%)] Loss: 19164.968750\n",
      "Train Epoch: 126 [122496/225000 (54%)] Loss: 18903.859375\n",
      "Train Epoch: 126 [124992/225000 (56%)] Loss: 19333.269531\n",
      "Train Epoch: 126 [127488/225000 (57%)] Loss: 19552.802734\n",
      "Train Epoch: 126 [129984/225000 (58%)] Loss: 19268.189453\n",
      "Train Epoch: 126 [132480/225000 (59%)] Loss: 19428.746094\n",
      "Train Epoch: 126 [134976/225000 (60%)] Loss: 19528.855469\n",
      "Train Epoch: 126 [137472/225000 (61%)] Loss: 19140.078125\n",
      "Train Epoch: 126 [139968/225000 (62%)] Loss: 19380.785156\n",
      "Train Epoch: 126 [142464/225000 (63%)] Loss: 19509.191406\n",
      "Train Epoch: 126 [144960/225000 (64%)] Loss: 19284.226562\n",
      "Train Epoch: 126 [147456/225000 (66%)] Loss: 19472.861328\n",
      "Train Epoch: 126 [149952/225000 (67%)] Loss: 19463.603516\n",
      "Train Epoch: 126 [152448/225000 (68%)] Loss: 19355.472656\n",
      "Train Epoch: 126 [154944/225000 (69%)] Loss: 19359.849609\n",
      "Train Epoch: 126 [157440/225000 (70%)] Loss: 19109.351562\n",
      "Train Epoch: 126 [159936/225000 (71%)] Loss: 19081.058594\n",
      "Train Epoch: 126 [162432/225000 (72%)] Loss: 19465.410156\n",
      "Train Epoch: 126 [164928/225000 (73%)] Loss: 19679.835938\n",
      "Train Epoch: 126 [167424/225000 (74%)] Loss: 19369.082031\n",
      "Train Epoch: 126 [169920/225000 (76%)] Loss: 19679.353516\n",
      "Train Epoch: 126 [172416/225000 (77%)] Loss: 19400.304688\n",
      "Train Epoch: 126 [174912/225000 (78%)] Loss: 19025.248047\n",
      "Train Epoch: 126 [177408/225000 (79%)] Loss: 19960.435547\n",
      "Train Epoch: 126 [179904/225000 (80%)] Loss: 19486.970703\n",
      "Train Epoch: 126 [182400/225000 (81%)] Loss: 19337.261719\n",
      "Train Epoch: 126 [184896/225000 (82%)] Loss: 19655.296875\n",
      "Train Epoch: 126 [187392/225000 (83%)] Loss: 19137.511719\n",
      "Train Epoch: 126 [189888/225000 (84%)] Loss: 19293.746094\n",
      "Train Epoch: 126 [192384/225000 (86%)] Loss: 18659.148438\n",
      "Train Epoch: 126 [194880/225000 (87%)] Loss: 19400.035156\n",
      "Train Epoch: 126 [197376/225000 (88%)] Loss: 19074.871094\n",
      "Train Epoch: 126 [199872/225000 (89%)] Loss: 19347.359375\n",
      "Train Epoch: 126 [202368/225000 (90%)] Loss: 19321.082031\n",
      "Train Epoch: 126 [204864/225000 (91%)] Loss: 19137.539062\n",
      "Train Epoch: 126 [207360/225000 (92%)] Loss: 19113.466797\n",
      "Train Epoch: 126 [209856/225000 (93%)] Loss: 19678.945312\n",
      "Train Epoch: 126 [212352/225000 (94%)] Loss: 19510.640625\n",
      "Train Epoch: 126 [214848/225000 (95%)] Loss: 19374.476562\n",
      "Train Epoch: 126 [217344/225000 (97%)] Loss: 19394.578125\n",
      "Train Epoch: 126 [219840/225000 (98%)] Loss: 19289.394531\n",
      "Train Epoch: 126 [222336/225000 (99%)] Loss: 19360.320312\n",
      "Train Epoch: 126 [224832/225000 (100%)] Loss: 19003.378906\n",
      "    epoch          : 126\n",
      "    loss           : 19351.213045608467\n",
      "    val_loss       : 19248.259266990743\n",
      "Train Epoch: 127 [192/225000 (0%)] Loss: 19310.988281\n",
      "Train Epoch: 127 [2688/225000 (1%)] Loss: 19089.089844\n",
      "Train Epoch: 127 [5184/225000 (2%)] Loss: 19183.111328\n",
      "Train Epoch: 127 [7680/225000 (3%)] Loss: 18838.941406\n",
      "Train Epoch: 127 [10176/225000 (5%)] Loss: 19107.009766\n",
      "Train Epoch: 127 [12672/225000 (6%)] Loss: 18665.394531\n",
      "Train Epoch: 127 [15168/225000 (7%)] Loss: 19470.630859\n",
      "Train Epoch: 127 [17664/225000 (8%)] Loss: 19487.488281\n",
      "Train Epoch: 127 [20160/225000 (9%)] Loss: 19267.152344\n",
      "Train Epoch: 127 [22656/225000 (10%)] Loss: 19078.257812\n",
      "Train Epoch: 127 [25152/225000 (11%)] Loss: 19444.242188\n",
      "Train Epoch: 127 [27648/225000 (12%)] Loss: 19677.578125\n",
      "Train Epoch: 127 [30144/225000 (13%)] Loss: 19610.548828\n",
      "Train Epoch: 127 [32640/225000 (15%)] Loss: 18996.205078\n",
      "Train Epoch: 127 [35136/225000 (16%)] Loss: 19283.953125\n",
      "Train Epoch: 127 [37632/225000 (17%)] Loss: 19389.808594\n",
      "Train Epoch: 127 [40128/225000 (18%)] Loss: 18901.552734\n",
      "Train Epoch: 127 [42624/225000 (19%)] Loss: 18897.722656\n",
      "Train Epoch: 127 [45120/225000 (20%)] Loss: 19086.429688\n",
      "Train Epoch: 127 [47616/225000 (21%)] Loss: 19281.796875\n",
      "Train Epoch: 127 [50112/225000 (22%)] Loss: 19334.226562\n",
      "Train Epoch: 127 [52608/225000 (23%)] Loss: 19474.917969\n",
      "Train Epoch: 127 [55104/225000 (24%)] Loss: 19081.109375\n",
      "Train Epoch: 127 [57600/225000 (26%)] Loss: 19578.873047\n",
      "Train Epoch: 127 [60096/225000 (27%)] Loss: 19356.412109\n",
      "Train Epoch: 127 [62592/225000 (28%)] Loss: 18757.078125\n",
      "Train Epoch: 127 [65088/225000 (29%)] Loss: 19542.292969\n",
      "Train Epoch: 127 [67584/225000 (30%)] Loss: 19334.746094\n",
      "Train Epoch: 127 [70080/225000 (31%)] Loss: 19074.222656\n",
      "Train Epoch: 127 [72576/225000 (32%)] Loss: 19524.777344\n",
      "Train Epoch: 127 [75072/225000 (33%)] Loss: 20088.398438\n",
      "Train Epoch: 127 [77568/225000 (34%)] Loss: 19939.734375\n",
      "Train Epoch: 127 [80064/225000 (36%)] Loss: 19518.367188\n",
      "Train Epoch: 127 [82560/225000 (37%)] Loss: 19156.626953\n",
      "Train Epoch: 127 [85056/225000 (38%)] Loss: 19002.281250\n",
      "Train Epoch: 127 [87552/225000 (39%)] Loss: 19165.453125\n",
      "Train Epoch: 127 [90048/225000 (40%)] Loss: 19758.857422\n",
      "Train Epoch: 127 [92544/225000 (41%)] Loss: 19696.646484\n",
      "Train Epoch: 127 [95040/225000 (42%)] Loss: 19330.027344\n",
      "Train Epoch: 127 [97536/225000 (43%)] Loss: 18922.833984\n",
      "Train Epoch: 127 [100032/225000 (44%)] Loss: 19139.652344\n",
      "Train Epoch: 127 [102528/225000 (46%)] Loss: 19722.289062\n",
      "Train Epoch: 127 [105024/225000 (47%)] Loss: 19400.906250\n",
      "Train Epoch: 127 [107520/225000 (48%)] Loss: 19638.238281\n",
      "Train Epoch: 127 [110016/225000 (49%)] Loss: 19160.523438\n",
      "Train Epoch: 127 [112512/225000 (50%)] Loss: 19609.398438\n",
      "Train Epoch: 127 [115008/225000 (51%)] Loss: 19485.250000\n",
      "Train Epoch: 127 [117504/225000 (52%)] Loss: 19312.613281\n",
      "Train Epoch: 127 [120000/225000 (53%)] Loss: 19036.246094\n",
      "Train Epoch: 127 [122496/225000 (54%)] Loss: 19142.509766\n",
      "Train Epoch: 127 [124992/225000 (56%)] Loss: 19538.937500\n",
      "Train Epoch: 127 [127488/225000 (57%)] Loss: 19463.335938\n",
      "Train Epoch: 127 [129984/225000 (58%)] Loss: 19605.486328\n",
      "Train Epoch: 127 [132480/225000 (59%)] Loss: 19375.632812\n",
      "Train Epoch: 127 [134976/225000 (60%)] Loss: 19122.263672\n",
      "Train Epoch: 127 [137472/225000 (61%)] Loss: 19222.376953\n",
      "Train Epoch: 127 [139968/225000 (62%)] Loss: 19464.910156\n",
      "Train Epoch: 127 [142464/225000 (63%)] Loss: 19099.935547\n",
      "Train Epoch: 127 [144960/225000 (64%)] Loss: 19499.316406\n",
      "Train Epoch: 127 [147456/225000 (66%)] Loss: 19188.710938\n",
      "Train Epoch: 127 [149952/225000 (67%)] Loss: 19177.992188\n",
      "Train Epoch: 127 [152448/225000 (68%)] Loss: 19270.158203\n",
      "Train Epoch: 127 [154944/225000 (69%)] Loss: 19691.117188\n",
      "Train Epoch: 127 [157440/225000 (70%)] Loss: 19412.156250\n",
      "Train Epoch: 127 [159936/225000 (71%)] Loss: 19518.802734\n",
      "Train Epoch: 127 [162432/225000 (72%)] Loss: 19042.675781\n",
      "Train Epoch: 127 [164928/225000 (73%)] Loss: 19524.476562\n",
      "Train Epoch: 127 [167424/225000 (74%)] Loss: 19649.363281\n",
      "Train Epoch: 127 [169920/225000 (76%)] Loss: 18892.523438\n",
      "Train Epoch: 127 [172416/225000 (77%)] Loss: 19451.425781\n",
      "Train Epoch: 127 [174912/225000 (78%)] Loss: 19352.937500\n",
      "Train Epoch: 127 [177408/225000 (79%)] Loss: 19242.476562\n",
      "Train Epoch: 127 [179904/225000 (80%)] Loss: 19440.960938\n",
      "Train Epoch: 127 [182400/225000 (81%)] Loss: 18683.210938\n",
      "Train Epoch: 127 [184896/225000 (82%)] Loss: 19694.818359\n",
      "Train Epoch: 127 [187392/225000 (83%)] Loss: 19588.167969\n",
      "Train Epoch: 127 [189888/225000 (84%)] Loss: 19516.566406\n",
      "Train Epoch: 127 [192384/225000 (86%)] Loss: 19580.691406\n",
      "Train Epoch: 127 [194880/225000 (87%)] Loss: 19381.335938\n",
      "Train Epoch: 127 [197376/225000 (88%)] Loss: 19400.109375\n",
      "Train Epoch: 127 [199872/225000 (89%)] Loss: 19248.867188\n",
      "Train Epoch: 127 [202368/225000 (90%)] Loss: 19571.265625\n",
      "Train Epoch: 127 [204864/225000 (91%)] Loss: 19065.085938\n",
      "Train Epoch: 127 [207360/225000 (92%)] Loss: 19083.875000\n",
      "Train Epoch: 127 [209856/225000 (93%)] Loss: 19039.386719\n",
      "Train Epoch: 127 [212352/225000 (94%)] Loss: 19632.419922\n",
      "Train Epoch: 127 [214848/225000 (95%)] Loss: 18973.416016\n",
      "Train Epoch: 127 [217344/225000 (97%)] Loss: 19270.083984\n",
      "Train Epoch: 127 [219840/225000 (98%)] Loss: 19558.392578\n",
      "Train Epoch: 127 [222336/225000 (99%)] Loss: 19370.960938\n",
      "Train Epoch: 127 [224832/225000 (100%)] Loss: 19145.683594\n",
      "    epoch          : 127\n",
      "    loss           : 19341.51002226429\n",
      "    val_loss       : 19244.06972690757\n",
      "Train Epoch: 128 [192/225000 (0%)] Loss: 19492.062500\n",
      "Train Epoch: 128 [2688/225000 (1%)] Loss: 19382.160156\n",
      "Train Epoch: 128 [5184/225000 (2%)] Loss: 19380.951172\n",
      "Train Epoch: 128 [7680/225000 (3%)] Loss: 19790.503906\n",
      "Train Epoch: 128 [10176/225000 (5%)] Loss: 19929.371094\n",
      "Train Epoch: 128 [12672/225000 (6%)] Loss: 20134.029297\n",
      "Train Epoch: 128 [15168/225000 (7%)] Loss: 19451.789062\n",
      "Train Epoch: 128 [17664/225000 (8%)] Loss: 19612.296875\n",
      "Train Epoch: 128 [20160/225000 (9%)] Loss: 19730.570312\n",
      "Train Epoch: 128 [22656/225000 (10%)] Loss: 19321.242188\n",
      "Train Epoch: 128 [25152/225000 (11%)] Loss: 20019.664062\n",
      "Train Epoch: 128 [27648/225000 (12%)] Loss: 19251.740234\n",
      "Train Epoch: 128 [30144/225000 (13%)] Loss: 19189.886719\n",
      "Train Epoch: 128 [32640/225000 (15%)] Loss: 18889.703125\n",
      "Train Epoch: 128 [35136/225000 (16%)] Loss: 19718.464844\n",
      "Train Epoch: 128 [37632/225000 (17%)] Loss: 19148.476562\n",
      "Train Epoch: 128 [40128/225000 (18%)] Loss: 18679.675781\n",
      "Train Epoch: 128 [42624/225000 (19%)] Loss: 19242.335938\n",
      "Train Epoch: 128 [45120/225000 (20%)] Loss: 19102.664062\n",
      "Train Epoch: 128 [47616/225000 (21%)] Loss: 19332.617188\n",
      "Train Epoch: 128 [50112/225000 (22%)] Loss: 19459.511719\n",
      "Train Epoch: 128 [52608/225000 (23%)] Loss: 19304.992188\n",
      "Train Epoch: 128 [55104/225000 (24%)] Loss: 19094.324219\n",
      "Train Epoch: 128 [57600/225000 (26%)] Loss: 19141.421875\n",
      "Train Epoch: 128 [60096/225000 (27%)] Loss: 19630.968750\n",
      "Train Epoch: 128 [62592/225000 (28%)] Loss: 18809.542969\n",
      "Train Epoch: 128 [65088/225000 (29%)] Loss: 19486.294922\n",
      "Train Epoch: 128 [67584/225000 (30%)] Loss: 19284.029297\n",
      "Train Epoch: 128 [70080/225000 (31%)] Loss: 19659.253906\n",
      "Train Epoch: 128 [72576/225000 (32%)] Loss: 19501.054688\n",
      "Train Epoch: 128 [75072/225000 (33%)] Loss: 18987.035156\n",
      "Train Epoch: 128 [77568/225000 (34%)] Loss: 19901.812500\n",
      "Train Epoch: 128 [80064/225000 (36%)] Loss: 19596.314453\n",
      "Train Epoch: 128 [82560/225000 (37%)] Loss: 19482.990234\n",
      "Train Epoch: 128 [85056/225000 (38%)] Loss: 19140.023438\n",
      "Train Epoch: 128 [87552/225000 (39%)] Loss: 19245.089844\n",
      "Train Epoch: 128 [90048/225000 (40%)] Loss: 18898.804688\n",
      "Train Epoch: 128 [92544/225000 (41%)] Loss: 19275.375000\n",
      "Train Epoch: 128 [95040/225000 (42%)] Loss: 19286.976562\n",
      "Train Epoch: 128 [97536/225000 (43%)] Loss: 19514.371094\n",
      "Train Epoch: 128 [100032/225000 (44%)] Loss: 18954.453125\n",
      "Train Epoch: 128 [102528/225000 (46%)] Loss: 19105.003906\n",
      "Train Epoch: 128 [105024/225000 (47%)] Loss: 18892.425781\n",
      "Train Epoch: 128 [107520/225000 (48%)] Loss: 19157.210938\n",
      "Train Epoch: 128 [110016/225000 (49%)] Loss: 19058.453125\n",
      "Train Epoch: 128 [112512/225000 (50%)] Loss: 19249.824219\n",
      "Train Epoch: 128 [115008/225000 (51%)] Loss: 19267.867188\n",
      "Train Epoch: 128 [117504/225000 (52%)] Loss: 19479.972656\n",
      "Train Epoch: 128 [120000/225000 (53%)] Loss: 19629.765625\n",
      "Train Epoch: 128 [122496/225000 (54%)] Loss: 19253.005859\n",
      "Train Epoch: 128 [124992/225000 (56%)] Loss: 19190.472656\n",
      "Train Epoch: 128 [127488/225000 (57%)] Loss: 19532.171875\n",
      "Train Epoch: 128 [129984/225000 (58%)] Loss: 19918.171875\n",
      "Train Epoch: 128 [132480/225000 (59%)] Loss: 19452.433594\n",
      "Train Epoch: 128 [134976/225000 (60%)] Loss: 19426.626953\n",
      "Train Epoch: 128 [137472/225000 (61%)] Loss: 18905.148438\n",
      "Train Epoch: 128 [139968/225000 (62%)] Loss: 19649.261719\n",
      "Train Epoch: 128 [142464/225000 (63%)] Loss: 18905.376953\n",
      "Train Epoch: 128 [144960/225000 (64%)] Loss: 18869.585938\n",
      "Train Epoch: 128 [147456/225000 (66%)] Loss: 19074.269531\n",
      "Train Epoch: 128 [149952/225000 (67%)] Loss: 19193.873047\n",
      "Train Epoch: 128 [152448/225000 (68%)] Loss: 19356.076172\n",
      "Train Epoch: 128 [154944/225000 (69%)] Loss: 19441.910156\n",
      "Train Epoch: 128 [157440/225000 (70%)] Loss: 19647.351562\n",
      "Train Epoch: 128 [159936/225000 (71%)] Loss: 19201.390625\n",
      "Train Epoch: 128 [162432/225000 (72%)] Loss: 19623.757812\n",
      "Train Epoch: 128 [164928/225000 (73%)] Loss: 19403.792969\n",
      "Train Epoch: 128 [167424/225000 (74%)] Loss: 19212.142578\n",
      "Train Epoch: 128 [169920/225000 (76%)] Loss: 19183.851562\n",
      "Train Epoch: 128 [172416/225000 (77%)] Loss: 19400.830078\n",
      "Train Epoch: 128 [174912/225000 (78%)] Loss: 19399.425781\n",
      "Train Epoch: 128 [177408/225000 (79%)] Loss: 19253.351562\n",
      "Train Epoch: 128 [179904/225000 (80%)] Loss: 19440.662109\n",
      "Train Epoch: 128 [182400/225000 (81%)] Loss: 18930.078125\n",
      "Train Epoch: 128 [184896/225000 (82%)] Loss: 18745.736328\n",
      "Train Epoch: 128 [187392/225000 (83%)] Loss: 19120.171875\n",
      "Train Epoch: 128 [189888/225000 (84%)] Loss: 19656.337891\n",
      "Train Epoch: 128 [192384/225000 (86%)] Loss: 19586.234375\n",
      "Train Epoch: 128 [194880/225000 (87%)] Loss: 19483.562500\n",
      "Train Epoch: 128 [197376/225000 (88%)] Loss: 19411.177734\n",
      "Train Epoch: 128 [199872/225000 (89%)] Loss: 19054.800781\n",
      "Train Epoch: 128 [202368/225000 (90%)] Loss: 19676.285156\n",
      "Train Epoch: 128 [204864/225000 (91%)] Loss: 18980.148438\n",
      "Train Epoch: 128 [207360/225000 (92%)] Loss: 19671.873047\n",
      "Train Epoch: 128 [209856/225000 (93%)] Loss: 19590.855469\n",
      "Train Epoch: 128 [212352/225000 (94%)] Loss: 19032.896484\n",
      "Train Epoch: 128 [214848/225000 (95%)] Loss: 19631.386719\n",
      "Train Epoch: 128 [217344/225000 (97%)] Loss: 19235.675781\n",
      "Train Epoch: 128 [219840/225000 (98%)] Loss: 19494.546875\n",
      "Train Epoch: 128 [222336/225000 (99%)] Loss: 19363.527344\n",
      "Train Epoch: 128 [224832/225000 (100%)] Loss: 19136.652344\n",
      "    epoch          : 128\n",
      "    loss           : 19340.508992374147\n",
      "    val_loss       : 19244.07581962975\n",
      "Train Epoch: 129 [192/225000 (0%)] Loss: 19316.511719\n",
      "Train Epoch: 129 [2688/225000 (1%)] Loss: 19163.015625\n",
      "Train Epoch: 129 [5184/225000 (2%)] Loss: 19054.281250\n",
      "Train Epoch: 129 [7680/225000 (3%)] Loss: 19371.429688\n",
      "Train Epoch: 129 [10176/225000 (5%)] Loss: 19231.556641\n",
      "Train Epoch: 129 [12672/225000 (6%)] Loss: 19058.570312\n",
      "Train Epoch: 129 [15168/225000 (7%)] Loss: 18896.835938\n",
      "Train Epoch: 129 [17664/225000 (8%)] Loss: 19309.316406\n",
      "Train Epoch: 129 [20160/225000 (9%)] Loss: 19468.755859\n",
      "Train Epoch: 129 [22656/225000 (10%)] Loss: 18953.851562\n",
      "Train Epoch: 129 [25152/225000 (11%)] Loss: 19379.707031\n",
      "Train Epoch: 129 [27648/225000 (12%)] Loss: 19362.773438\n",
      "Train Epoch: 129 [30144/225000 (13%)] Loss: 19340.425781\n",
      "Train Epoch: 129 [32640/225000 (15%)] Loss: 19110.132812\n",
      "Train Epoch: 129 [35136/225000 (16%)] Loss: 19248.994141\n",
      "Train Epoch: 129 [37632/225000 (17%)] Loss: 19140.472656\n",
      "Train Epoch: 129 [40128/225000 (18%)] Loss: 19610.914062\n",
      "Train Epoch: 129 [42624/225000 (19%)] Loss: 19690.179688\n",
      "Train Epoch: 129 [45120/225000 (20%)] Loss: 19384.808594\n",
      "Train Epoch: 129 [47616/225000 (21%)] Loss: 19060.253906\n",
      "Train Epoch: 129 [50112/225000 (22%)] Loss: 18792.951172\n",
      "Train Epoch: 129 [52608/225000 (23%)] Loss: 19115.718750\n",
      "Train Epoch: 129 [55104/225000 (24%)] Loss: 19179.437500\n",
      "Train Epoch: 129 [57600/225000 (26%)] Loss: 19175.656250\n",
      "Train Epoch: 129 [60096/225000 (27%)] Loss: 19227.468750\n",
      "Train Epoch: 129 [62592/225000 (28%)] Loss: 19311.796875\n",
      "Train Epoch: 129 [65088/225000 (29%)] Loss: 19243.183594\n",
      "Train Epoch: 129 [67584/225000 (30%)] Loss: 19675.337891\n",
      "Train Epoch: 129 [70080/225000 (31%)] Loss: 19139.478516\n",
      "Train Epoch: 129 [72576/225000 (32%)] Loss: 19440.000000\n",
      "Train Epoch: 129 [75072/225000 (33%)] Loss: 19049.445312\n",
      "Train Epoch: 129 [77568/225000 (34%)] Loss: 19238.878906\n",
      "Train Epoch: 129 [80064/225000 (36%)] Loss: 18719.761719\n",
      "Train Epoch: 129 [82560/225000 (37%)] Loss: 19225.910156\n",
      "Train Epoch: 129 [85056/225000 (38%)] Loss: 19015.369141\n",
      "Train Epoch: 129 [87552/225000 (39%)] Loss: 19198.738281\n",
      "Train Epoch: 129 [90048/225000 (40%)] Loss: 19152.316406\n",
      "Train Epoch: 129 [92544/225000 (41%)] Loss: 19730.412109\n",
      "Train Epoch: 129 [95040/225000 (42%)] Loss: 19488.513672\n",
      "Train Epoch: 129 [97536/225000 (43%)] Loss: 19139.734375\n",
      "Train Epoch: 129 [100032/225000 (44%)] Loss: 19522.974609\n",
      "Train Epoch: 129 [102528/225000 (46%)] Loss: 19497.257812\n",
      "Train Epoch: 129 [105024/225000 (47%)] Loss: 19283.650391\n",
      "Train Epoch: 129 [107520/225000 (48%)] Loss: 19294.689453\n",
      "Train Epoch: 129 [110016/225000 (49%)] Loss: 19439.527344\n",
      "Train Epoch: 129 [112512/225000 (50%)] Loss: 19100.947266\n",
      "Train Epoch: 129 [115008/225000 (51%)] Loss: 18960.539062\n",
      "Train Epoch: 129 [117504/225000 (52%)] Loss: 19229.250000\n",
      "Train Epoch: 129 [120000/225000 (53%)] Loss: 19027.126953\n",
      "Train Epoch: 129 [122496/225000 (54%)] Loss: 19012.160156\n",
      "Train Epoch: 129 [124992/225000 (56%)] Loss: 19224.781250\n",
      "Train Epoch: 129 [127488/225000 (57%)] Loss: 19334.964844\n",
      "Train Epoch: 129 [129984/225000 (58%)] Loss: 19285.568359\n",
      "Train Epoch: 129 [132480/225000 (59%)] Loss: 19386.593750\n",
      "Train Epoch: 129 [134976/225000 (60%)] Loss: 19311.437500\n",
      "Train Epoch: 129 [137472/225000 (61%)] Loss: 19738.396484\n",
      "Train Epoch: 129 [139968/225000 (62%)] Loss: 19172.269531\n",
      "Train Epoch: 129 [142464/225000 (63%)] Loss: 19475.851562\n",
      "Train Epoch: 129 [144960/225000 (64%)] Loss: 19577.322266\n",
      "Train Epoch: 129 [147456/225000 (66%)] Loss: 19193.507812\n",
      "Train Epoch: 129 [149952/225000 (67%)] Loss: 19728.972656\n",
      "Train Epoch: 129 [152448/225000 (68%)] Loss: 19042.121094\n",
      "Train Epoch: 129 [154944/225000 (69%)] Loss: 19242.058594\n",
      "Train Epoch: 129 [157440/225000 (70%)] Loss: 19370.722656\n",
      "Train Epoch: 129 [159936/225000 (71%)] Loss: 19148.066406\n",
      "Train Epoch: 129 [162432/225000 (72%)] Loss: 19317.332031\n",
      "Train Epoch: 129 [164928/225000 (73%)] Loss: 19496.703125\n",
      "Train Epoch: 129 [167424/225000 (74%)] Loss: 19469.332031\n",
      "Train Epoch: 129 [169920/225000 (76%)] Loss: 19114.070312\n",
      "Train Epoch: 129 [172416/225000 (77%)] Loss: 19243.792969\n",
      "Train Epoch: 129 [174912/225000 (78%)] Loss: 18913.884766\n",
      "Train Epoch: 129 [177408/225000 (79%)] Loss: 19514.097656\n",
      "Train Epoch: 129 [179904/225000 (80%)] Loss: 19549.464844\n",
      "Train Epoch: 129 [182400/225000 (81%)] Loss: 18939.839844\n",
      "Train Epoch: 129 [184896/225000 (82%)] Loss: 18852.771484\n",
      "Train Epoch: 129 [187392/225000 (83%)] Loss: 19243.402344\n",
      "Train Epoch: 129 [189888/225000 (84%)] Loss: 18766.589844\n",
      "Train Epoch: 129 [192384/225000 (86%)] Loss: 19656.214844\n",
      "Train Epoch: 129 [194880/225000 (87%)] Loss: 19796.019531\n",
      "Train Epoch: 129 [197376/225000 (88%)] Loss: 19338.570312\n",
      "Train Epoch: 129 [199872/225000 (89%)] Loss: 19261.972656\n",
      "Train Epoch: 129 [202368/225000 (90%)] Loss: 19554.765625\n",
      "Train Epoch: 129 [204864/225000 (91%)] Loss: 19482.484375\n",
      "Train Epoch: 129 [207360/225000 (92%)] Loss: 19303.035156\n",
      "Train Epoch: 129 [209856/225000 (93%)] Loss: 18948.511719\n",
      "Train Epoch: 129 [212352/225000 (94%)] Loss: 19958.304688\n",
      "Train Epoch: 129 [214848/225000 (95%)] Loss: 18844.640625\n",
      "Train Epoch: 129 [217344/225000 (97%)] Loss: 19012.121094\n",
      "Train Epoch: 129 [219840/225000 (98%)] Loss: 19194.890625\n",
      "Train Epoch: 129 [222336/225000 (99%)] Loss: 19786.277344\n",
      "Train Epoch: 129 [224832/225000 (100%)] Loss: 19795.335938\n",
      "    epoch          : 129\n",
      "    loss           : 19335.817514465125\n",
      "    val_loss       : 19265.38489996568\n",
      "Train Epoch: 130 [192/225000 (0%)] Loss: 19172.660156\n",
      "Train Epoch: 130 [2688/225000 (1%)] Loss: 19057.048828\n",
      "Train Epoch: 130 [5184/225000 (2%)] Loss: 19112.478516\n",
      "Train Epoch: 130 [7680/225000 (3%)] Loss: 19260.685547\n",
      "Train Epoch: 130 [10176/225000 (5%)] Loss: 19840.062500\n",
      "Train Epoch: 130 [12672/225000 (6%)] Loss: 19308.195312\n",
      "Train Epoch: 130 [15168/225000 (7%)] Loss: 19341.929688\n",
      "Train Epoch: 130 [17664/225000 (8%)] Loss: 19034.951172\n",
      "Train Epoch: 130 [20160/225000 (9%)] Loss: 19155.191406\n",
      "Train Epoch: 130 [22656/225000 (10%)] Loss: 19311.658203\n",
      "Train Epoch: 130 [25152/225000 (11%)] Loss: 19526.671875\n",
      "Train Epoch: 130 [27648/225000 (12%)] Loss: 19653.390625\n",
      "Train Epoch: 130 [30144/225000 (13%)] Loss: 19417.843750\n",
      "Train Epoch: 130 [32640/225000 (15%)] Loss: 19000.839844\n",
      "Train Epoch: 130 [35136/225000 (16%)] Loss: 18950.988281\n",
      "Train Epoch: 130 [37632/225000 (17%)] Loss: 19590.910156\n",
      "Train Epoch: 130 [40128/225000 (18%)] Loss: 19438.964844\n",
      "Train Epoch: 130 [42624/225000 (19%)] Loss: 19133.343750\n",
      "Train Epoch: 130 [45120/225000 (20%)] Loss: 19021.894531\n",
      "Train Epoch: 130 [47616/225000 (21%)] Loss: 19329.210938\n",
      "Train Epoch: 130 [50112/225000 (22%)] Loss: 18870.941406\n",
      "Train Epoch: 130 [52608/225000 (23%)] Loss: 19286.218750\n",
      "Train Epoch: 130 [55104/225000 (24%)] Loss: 18835.070312\n",
      "Train Epoch: 130 [57600/225000 (26%)] Loss: 19141.388672\n",
      "Train Epoch: 130 [60096/225000 (27%)] Loss: 19864.347656\n",
      "Train Epoch: 130 [62592/225000 (28%)] Loss: 19451.554688\n",
      "Train Epoch: 130 [65088/225000 (29%)] Loss: 18998.724609\n",
      "Train Epoch: 130 [67584/225000 (30%)] Loss: 18976.083984\n",
      "Train Epoch: 130 [70080/225000 (31%)] Loss: 19442.812500\n",
      "Train Epoch: 130 [72576/225000 (32%)] Loss: 19312.269531\n",
      "Train Epoch: 130 [75072/225000 (33%)] Loss: 19338.074219\n",
      "Train Epoch: 130 [77568/225000 (34%)] Loss: 19664.457031\n",
      "Train Epoch: 130 [80064/225000 (36%)] Loss: 19056.882812\n",
      "Train Epoch: 130 [82560/225000 (37%)] Loss: 19357.203125\n",
      "Train Epoch: 130 [85056/225000 (38%)] Loss: 18994.654297\n",
      "Train Epoch: 130 [87552/225000 (39%)] Loss: 19564.359375\n",
      "Train Epoch: 130 [90048/225000 (40%)] Loss: 19481.125000\n",
      "Train Epoch: 130 [92544/225000 (41%)] Loss: 19315.214844\n",
      "Train Epoch: 130 [95040/225000 (42%)] Loss: 19575.050781\n",
      "Train Epoch: 130 [97536/225000 (43%)] Loss: 19142.074219\n",
      "Train Epoch: 130 [100032/225000 (44%)] Loss: 19651.468750\n",
      "Train Epoch: 130 [102528/225000 (46%)] Loss: 19209.421875\n",
      "Train Epoch: 130 [105024/225000 (47%)] Loss: 19570.593750\n",
      "Train Epoch: 130 [107520/225000 (48%)] Loss: 19631.546875\n",
      "Train Epoch: 130 [110016/225000 (49%)] Loss: 19503.195312\n",
      "Train Epoch: 130 [112512/225000 (50%)] Loss: 18954.865234\n",
      "Train Epoch: 130 [115008/225000 (51%)] Loss: 19537.472656\n",
      "Train Epoch: 130 [117504/225000 (52%)] Loss: 19747.531250\n",
      "Train Epoch: 130 [120000/225000 (53%)] Loss: 19023.550781\n",
      "Train Epoch: 130 [122496/225000 (54%)] Loss: 19469.697266\n",
      "Train Epoch: 130 [124992/225000 (56%)] Loss: 19352.423828\n",
      "Train Epoch: 130 [127488/225000 (57%)] Loss: 18859.318359\n",
      "Train Epoch: 130 [129984/225000 (58%)] Loss: 18929.812500\n",
      "Train Epoch: 130 [132480/225000 (59%)] Loss: 19210.128906\n",
      "Train Epoch: 130 [134976/225000 (60%)] Loss: 19363.992188\n",
      "Train Epoch: 130 [137472/225000 (61%)] Loss: 19418.447266\n",
      "Train Epoch: 130 [139968/225000 (62%)] Loss: 19762.687500\n",
      "Train Epoch: 130 [142464/225000 (63%)] Loss: 19677.642578\n",
      "Train Epoch: 130 [144960/225000 (64%)] Loss: 19702.791016\n",
      "Train Epoch: 130 [147456/225000 (66%)] Loss: 19136.345703\n",
      "Train Epoch: 130 [149952/225000 (67%)] Loss: 19702.792969\n",
      "Train Epoch: 130 [152448/225000 (68%)] Loss: 19105.914062\n",
      "Train Epoch: 130 [154944/225000 (69%)] Loss: 18616.480469\n",
      "Train Epoch: 130 [157440/225000 (70%)] Loss: 19565.968750\n",
      "Train Epoch: 130 [159936/225000 (71%)] Loss: 19738.812500\n",
      "Train Epoch: 130 [162432/225000 (72%)] Loss: 19191.962891\n",
      "Train Epoch: 130 [164928/225000 (73%)] Loss: 19332.074219\n",
      "Train Epoch: 130 [167424/225000 (74%)] Loss: 19369.738281\n",
      "Train Epoch: 130 [169920/225000 (76%)] Loss: 19149.328125\n",
      "Train Epoch: 130 [172416/225000 (77%)] Loss: 19713.226562\n",
      "Train Epoch: 130 [174912/225000 (78%)] Loss: 19315.582031\n",
      "Train Epoch: 130 [177408/225000 (79%)] Loss: 19227.363281\n",
      "Train Epoch: 130 [179904/225000 (80%)] Loss: 19216.875000\n",
      "Train Epoch: 130 [182400/225000 (81%)] Loss: 18595.894531\n",
      "Train Epoch: 130 [184896/225000 (82%)] Loss: 19506.074219\n",
      "Train Epoch: 130 [187392/225000 (83%)] Loss: 19322.029297\n",
      "Train Epoch: 130 [189888/225000 (84%)] Loss: 19139.595703\n",
      "Train Epoch: 130 [192384/225000 (86%)] Loss: 19324.177734\n",
      "Train Epoch: 130 [194880/225000 (87%)] Loss: 18986.921875\n",
      "Train Epoch: 130 [197376/225000 (88%)] Loss: 19214.718750\n",
      "Train Epoch: 130 [199872/225000 (89%)] Loss: 19168.259766\n",
      "Train Epoch: 130 [202368/225000 (90%)] Loss: 19222.255859\n",
      "Train Epoch: 130 [204864/225000 (91%)] Loss: 19310.378906\n",
      "Train Epoch: 130 [207360/225000 (92%)] Loss: 19093.332031\n",
      "Train Epoch: 130 [209856/225000 (93%)] Loss: 19068.914062\n",
      "Train Epoch: 130 [212352/225000 (94%)] Loss: 19658.132812\n",
      "Train Epoch: 130 [214848/225000 (95%)] Loss: 19370.773438\n",
      "Train Epoch: 130 [217344/225000 (97%)] Loss: 19437.968750\n",
      "Train Epoch: 130 [219840/225000 (98%)] Loss: 19353.951172\n",
      "Train Epoch: 130 [222336/225000 (99%)] Loss: 19213.160156\n",
      "Train Epoch: 130 [224832/225000 (100%)] Loss: 19461.957031\n",
      "    epoch          : 130\n",
      "    loss           : 19340.837733975044\n",
      "    val_loss       : 19227.73493724379\n",
      "Train Epoch: 131 [192/225000 (0%)] Loss: 19613.121094\n",
      "Train Epoch: 131 [2688/225000 (1%)] Loss: 19084.554688\n",
      "Train Epoch: 131 [5184/225000 (2%)] Loss: 19402.070312\n",
      "Train Epoch: 131 [7680/225000 (3%)] Loss: 18961.292969\n",
      "Train Epoch: 131 [10176/225000 (5%)] Loss: 19097.392578\n",
      "Train Epoch: 131 [12672/225000 (6%)] Loss: 19470.847656\n",
      "Train Epoch: 131 [15168/225000 (7%)] Loss: 19212.367188\n",
      "Train Epoch: 131 [17664/225000 (8%)] Loss: 18882.062500\n",
      "Train Epoch: 131 [20160/225000 (9%)] Loss: 19966.935547\n",
      "Train Epoch: 131 [22656/225000 (10%)] Loss: 19292.638672\n",
      "Train Epoch: 131 [25152/225000 (11%)] Loss: 19518.943359\n",
      "Train Epoch: 131 [27648/225000 (12%)] Loss: 19361.558594\n",
      "Train Epoch: 131 [30144/225000 (13%)] Loss: 19763.535156\n",
      "Train Epoch: 131 [32640/225000 (15%)] Loss: 19505.080078\n",
      "Train Epoch: 131 [35136/225000 (16%)] Loss: 19298.435547\n",
      "Train Epoch: 131 [37632/225000 (17%)] Loss: 19288.769531\n",
      "Train Epoch: 131 [40128/225000 (18%)] Loss: 19053.031250\n",
      "Train Epoch: 131 [42624/225000 (19%)] Loss: 19503.707031\n",
      "Train Epoch: 131 [45120/225000 (20%)] Loss: 18619.312500\n",
      "Train Epoch: 131 [47616/225000 (21%)] Loss: 18963.804688\n",
      "Train Epoch: 131 [50112/225000 (22%)] Loss: 18812.050781\n",
      "Train Epoch: 131 [52608/225000 (23%)] Loss: 19150.107422\n",
      "Train Epoch: 131 [55104/225000 (24%)] Loss: 19090.515625\n",
      "Train Epoch: 131 [57600/225000 (26%)] Loss: 19281.982422\n",
      "Train Epoch: 131 [60096/225000 (27%)] Loss: 19812.710938\n",
      "Train Epoch: 131 [62592/225000 (28%)] Loss: 19097.599609\n",
      "Train Epoch: 131 [65088/225000 (29%)] Loss: 19016.203125\n",
      "Train Epoch: 131 [67584/225000 (30%)] Loss: 19243.617188\n",
      "Train Epoch: 131 [70080/225000 (31%)] Loss: 19118.150391\n",
      "Train Epoch: 131 [72576/225000 (32%)] Loss: 19360.800781\n",
      "Train Epoch: 131 [75072/225000 (33%)] Loss: 19140.949219\n",
      "Train Epoch: 131 [77568/225000 (34%)] Loss: 19158.888672\n",
      "Train Epoch: 131 [80064/225000 (36%)] Loss: 19256.523438\n",
      "Train Epoch: 131 [82560/225000 (37%)] Loss: 19487.138672\n",
      "Train Epoch: 131 [85056/225000 (38%)] Loss: 19484.634766\n",
      "Train Epoch: 131 [87552/225000 (39%)] Loss: 19991.992188\n",
      "Train Epoch: 131 [90048/225000 (40%)] Loss: 19126.595703\n",
      "Train Epoch: 131 [92544/225000 (41%)] Loss: 19292.927734\n",
      "Train Epoch: 131 [95040/225000 (42%)] Loss: 19464.378906\n",
      "Train Epoch: 131 [97536/225000 (43%)] Loss: 19444.601562\n",
      "Train Epoch: 131 [100032/225000 (44%)] Loss: 18647.101562\n",
      "Train Epoch: 131 [102528/225000 (46%)] Loss: 19370.767578\n",
      "Train Epoch: 131 [105024/225000 (47%)] Loss: 19232.347656\n",
      "Train Epoch: 131 [107520/225000 (48%)] Loss: 19238.875000\n",
      "Train Epoch: 131 [110016/225000 (49%)] Loss: 19831.482422\n",
      "Train Epoch: 131 [112512/225000 (50%)] Loss: 19359.667969\n",
      "Train Epoch: 131 [115008/225000 (51%)] Loss: 19193.810547\n",
      "Train Epoch: 131 [117504/225000 (52%)] Loss: 19294.300781\n",
      "Train Epoch: 131 [120000/225000 (53%)] Loss: 18946.144531\n",
      "Train Epoch: 131 [122496/225000 (54%)] Loss: 19041.824219\n",
      "Train Epoch: 131 [124992/225000 (56%)] Loss: 19273.162109\n",
      "Train Epoch: 131 [127488/225000 (57%)] Loss: 18806.953125\n",
      "Train Epoch: 131 [129984/225000 (58%)] Loss: 19503.964844\n",
      "Train Epoch: 131 [132480/225000 (59%)] Loss: 18694.898438\n",
      "Train Epoch: 131 [134976/225000 (60%)] Loss: 19507.121094\n",
      "Train Epoch: 131 [137472/225000 (61%)] Loss: 18982.574219\n",
      "Train Epoch: 131 [139968/225000 (62%)] Loss: 19458.890625\n",
      "Train Epoch: 131 [142464/225000 (63%)] Loss: 19049.322266\n",
      "Train Epoch: 131 [144960/225000 (64%)] Loss: 19061.031250\n",
      "Train Epoch: 131 [147456/225000 (66%)] Loss: 19416.509766\n",
      "Train Epoch: 131 [149952/225000 (67%)] Loss: 18927.021484\n",
      "Train Epoch: 131 [152448/225000 (68%)] Loss: 19077.076172\n",
      "Train Epoch: 131 [154944/225000 (69%)] Loss: 18980.113281\n",
      "Train Epoch: 131 [157440/225000 (70%)] Loss: 18648.792969\n",
      "Train Epoch: 131 [159936/225000 (71%)] Loss: 19090.042969\n",
      "Train Epoch: 131 [162432/225000 (72%)] Loss: 18852.199219\n",
      "Train Epoch: 131 [164928/225000 (73%)] Loss: 19138.609375\n",
      "Train Epoch: 131 [167424/225000 (74%)] Loss: 19186.156250\n",
      "Train Epoch: 131 [169920/225000 (76%)] Loss: 19436.789062\n",
      "Train Epoch: 131 [172416/225000 (77%)] Loss: 19486.697266\n",
      "Train Epoch: 131 [174912/225000 (78%)] Loss: 19335.539062\n",
      "Train Epoch: 131 [177408/225000 (79%)] Loss: 19354.070312\n",
      "Train Epoch: 131 [179904/225000 (80%)] Loss: 19277.078125\n",
      "Train Epoch: 131 [182400/225000 (81%)] Loss: 19128.476562\n",
      "Train Epoch: 131 [184896/225000 (82%)] Loss: 18678.773438\n",
      "Train Epoch: 131 [187392/225000 (83%)] Loss: 19415.742188\n",
      "Train Epoch: 131 [189888/225000 (84%)] Loss: 19366.945312\n",
      "Train Epoch: 131 [192384/225000 (86%)] Loss: 19380.406250\n",
      "Train Epoch: 131 [194880/225000 (87%)] Loss: 19279.503906\n",
      "Train Epoch: 131 [197376/225000 (88%)] Loss: 19165.517578\n",
      "Train Epoch: 131 [199872/225000 (89%)] Loss: 19931.351562\n",
      "Train Epoch: 131 [202368/225000 (90%)] Loss: 19367.425781\n",
      "Train Epoch: 131 [204864/225000 (91%)] Loss: 19425.910156\n",
      "Train Epoch: 131 [207360/225000 (92%)] Loss: 18650.773438\n",
      "Train Epoch: 131 [209856/225000 (93%)] Loss: 19484.945312\n",
      "Train Epoch: 131 [212352/225000 (94%)] Loss: 19724.757812\n",
      "Train Epoch: 131 [214848/225000 (95%)] Loss: 19366.404297\n",
      "Train Epoch: 131 [217344/225000 (97%)] Loss: 19753.191406\n",
      "Train Epoch: 131 [219840/225000 (98%)] Loss: 19430.164062\n",
      "Train Epoch: 131 [222336/225000 (99%)] Loss: 19423.978516\n",
      "Train Epoch: 131 [224832/225000 (100%)] Loss: 19709.121094\n",
      "    epoch          : 131\n",
      "    loss           : 19329.69412262692\n",
      "    val_loss       : 19252.159875146306\n",
      "Train Epoch: 132 [192/225000 (0%)] Loss: 19140.990234\n",
      "Train Epoch: 132 [2688/225000 (1%)] Loss: 19573.062500\n",
      "Train Epoch: 132 [5184/225000 (2%)] Loss: 19314.458984\n",
      "Train Epoch: 132 [7680/225000 (3%)] Loss: 19676.447266\n",
      "Train Epoch: 132 [10176/225000 (5%)] Loss: 19586.539062\n",
      "Train Epoch: 132 [12672/225000 (6%)] Loss: 19432.128906\n",
      "Train Epoch: 132 [15168/225000 (7%)] Loss: 19300.064453\n",
      "Train Epoch: 132 [17664/225000 (8%)] Loss: 19535.166016\n",
      "Train Epoch: 132 [20160/225000 (9%)] Loss: 19333.433594\n",
      "Train Epoch: 132 [22656/225000 (10%)] Loss: 19817.777344\n",
      "Train Epoch: 132 [25152/225000 (11%)] Loss: 19576.941406\n",
      "Train Epoch: 132 [27648/225000 (12%)] Loss: 20222.603516\n",
      "Train Epoch: 132 [30144/225000 (13%)] Loss: 19577.386719\n",
      "Train Epoch: 132 [32640/225000 (15%)] Loss: 19312.285156\n",
      "Train Epoch: 132 [35136/225000 (16%)] Loss: 19391.087891\n",
      "Train Epoch: 132 [37632/225000 (17%)] Loss: 19875.566406\n",
      "Train Epoch: 132 [40128/225000 (18%)] Loss: 19085.755859\n",
      "Train Epoch: 132 [42624/225000 (19%)] Loss: 18722.101562\n",
      "Train Epoch: 132 [45120/225000 (20%)] Loss: 19244.261719\n",
      "Train Epoch: 132 [47616/225000 (21%)] Loss: 19567.933594\n",
      "Train Epoch: 132 [50112/225000 (22%)] Loss: 18907.482422\n",
      "Train Epoch: 132 [52608/225000 (23%)] Loss: 19474.679688\n",
      "Train Epoch: 132 [55104/225000 (24%)] Loss: 19143.425781\n",
      "Train Epoch: 132 [57600/225000 (26%)] Loss: 19146.212891\n",
      "Train Epoch: 132 [60096/225000 (27%)] Loss: 19332.544922\n",
      "Train Epoch: 132 [62592/225000 (28%)] Loss: 19802.558594\n",
      "Train Epoch: 132 [65088/225000 (29%)] Loss: 19761.644531\n",
      "Train Epoch: 132 [67584/225000 (30%)] Loss: 18811.695312\n",
      "Train Epoch: 132 [70080/225000 (31%)] Loss: 19318.833984\n",
      "Train Epoch: 132 [72576/225000 (32%)] Loss: 19262.236328\n",
      "Train Epoch: 132 [75072/225000 (33%)] Loss: 19252.230469\n",
      "Train Epoch: 132 [77568/225000 (34%)] Loss: 19476.839844\n",
      "Train Epoch: 132 [80064/225000 (36%)] Loss: 19536.593750\n",
      "Train Epoch: 132 [82560/225000 (37%)] Loss: 18939.105469\n",
      "Train Epoch: 132 [85056/225000 (38%)] Loss: 19598.710938\n",
      "Train Epoch: 132 [87552/225000 (39%)] Loss: 19459.164062\n",
      "Train Epoch: 132 [90048/225000 (40%)] Loss: 19247.933594\n",
      "Train Epoch: 132 [92544/225000 (41%)] Loss: 19721.914062\n",
      "Train Epoch: 132 [95040/225000 (42%)] Loss: 19636.835938\n",
      "Train Epoch: 132 [97536/225000 (43%)] Loss: 19626.031250\n",
      "Train Epoch: 132 [100032/225000 (44%)] Loss: 19482.759766\n",
      "Train Epoch: 132 [102528/225000 (46%)] Loss: 19474.152344\n",
      "Train Epoch: 132 [105024/225000 (47%)] Loss: 19099.265625\n",
      "Train Epoch: 132 [107520/225000 (48%)] Loss: 19086.464844\n",
      "Train Epoch: 132 [110016/225000 (49%)] Loss: 19413.376953\n",
      "Train Epoch: 132 [112512/225000 (50%)] Loss: 19350.628906\n",
      "Train Epoch: 132 [115008/225000 (51%)] Loss: 19590.675781\n",
      "Train Epoch: 132 [117504/225000 (52%)] Loss: 19310.970703\n",
      "Train Epoch: 132 [120000/225000 (53%)] Loss: 19476.750000\n",
      "Train Epoch: 132 [122496/225000 (54%)] Loss: 19004.941406\n",
      "Train Epoch: 132 [124992/225000 (56%)] Loss: 20025.460938\n",
      "Train Epoch: 132 [127488/225000 (57%)] Loss: 19253.046875\n",
      "Train Epoch: 132 [129984/225000 (58%)] Loss: 19837.496094\n",
      "Train Epoch: 132 [132480/225000 (59%)] Loss: 18901.742188\n",
      "Train Epoch: 132 [134976/225000 (60%)] Loss: 19134.761719\n",
      "Train Epoch: 132 [137472/225000 (61%)] Loss: 19414.097656\n",
      "Train Epoch: 132 [139968/225000 (62%)] Loss: 19699.656250\n",
      "Train Epoch: 132 [142464/225000 (63%)] Loss: 19431.767578\n",
      "Train Epoch: 132 [144960/225000 (64%)] Loss: 19296.121094\n",
      "Train Epoch: 132 [147456/225000 (66%)] Loss: 19645.138672\n",
      "Train Epoch: 132 [149952/225000 (67%)] Loss: 19230.357422\n",
      "Train Epoch: 132 [152448/225000 (68%)] Loss: 19572.787109\n",
      "Train Epoch: 132 [154944/225000 (69%)] Loss: 19233.191406\n",
      "Train Epoch: 132 [157440/225000 (70%)] Loss: 19730.558594\n",
      "Train Epoch: 132 [159936/225000 (71%)] Loss: 19432.833984\n",
      "Train Epoch: 132 [162432/225000 (72%)] Loss: 19555.699219\n",
      "Train Epoch: 132 [164928/225000 (73%)] Loss: 19078.414062\n",
      "Train Epoch: 132 [167424/225000 (74%)] Loss: 19216.179688\n",
      "Train Epoch: 132 [169920/225000 (76%)] Loss: 19462.537109\n",
      "Train Epoch: 132 [172416/225000 (77%)] Loss: 19579.390625\n",
      "Train Epoch: 132 [174912/225000 (78%)] Loss: 18760.953125\n",
      "Train Epoch: 132 [177408/225000 (79%)] Loss: 19300.757812\n",
      "Train Epoch: 132 [179904/225000 (80%)] Loss: 19094.302734\n",
      "Train Epoch: 132 [182400/225000 (81%)] Loss: 19312.037109\n",
      "Train Epoch: 132 [184896/225000 (82%)] Loss: 19717.087891\n",
      "Train Epoch: 132 [187392/225000 (83%)] Loss: 19179.671875\n",
      "Train Epoch: 132 [189888/225000 (84%)] Loss: 19202.859375\n",
      "Train Epoch: 132 [192384/225000 (86%)] Loss: 19095.957031\n",
      "Train Epoch: 132 [194880/225000 (87%)] Loss: 19276.316406\n",
      "Train Epoch: 132 [197376/225000 (88%)] Loss: 19424.482422\n",
      "Train Epoch: 132 [199872/225000 (89%)] Loss: 19100.724609\n",
      "Train Epoch: 132 [202368/225000 (90%)] Loss: 19543.671875\n",
      "Train Epoch: 132 [204864/225000 (91%)] Loss: 19063.091797\n",
      "Train Epoch: 132 [207360/225000 (92%)] Loss: 19448.017578\n",
      "Train Epoch: 132 [209856/225000 (93%)] Loss: 19203.574219\n",
      "Train Epoch: 132 [212352/225000 (94%)] Loss: 19101.835938\n",
      "Train Epoch: 132 [214848/225000 (95%)] Loss: 19845.404297\n",
      "Train Epoch: 132 [217344/225000 (97%)] Loss: 19803.503906\n",
      "Train Epoch: 132 [219840/225000 (98%)] Loss: 19299.371094\n",
      "Train Epoch: 132 [222336/225000 (99%)] Loss: 18792.625000\n",
      "Train Epoch: 132 [224832/225000 (100%)] Loss: 19542.761719\n",
      "    epoch          : 132\n",
      "    loss           : 19322.859105028798\n",
      "    val_loss       : 19225.298892589926\n",
      "Train Epoch: 133 [192/225000 (0%)] Loss: 19734.484375\n",
      "Train Epoch: 133 [2688/225000 (1%)] Loss: 19528.087891\n",
      "Train Epoch: 133 [5184/225000 (2%)] Loss: 19539.033203\n",
      "Train Epoch: 133 [7680/225000 (3%)] Loss: 19537.757812\n",
      "Train Epoch: 133 [10176/225000 (5%)] Loss: 18833.820312\n",
      "Train Epoch: 133 [12672/225000 (6%)] Loss: 19581.242188\n",
      "Train Epoch: 133 [15168/225000 (7%)] Loss: 19131.978516\n",
      "Train Epoch: 133 [17664/225000 (8%)] Loss: 19421.035156\n",
      "Train Epoch: 133 [20160/225000 (9%)] Loss: 19792.453125\n",
      "Train Epoch: 133 [22656/225000 (10%)] Loss: 19198.929688\n",
      "Train Epoch: 133 [25152/225000 (11%)] Loss: 19297.984375\n",
      "Train Epoch: 133 [27648/225000 (12%)] Loss: 18977.734375\n",
      "Train Epoch: 133 [30144/225000 (13%)] Loss: 19459.238281\n",
      "Train Epoch: 133 [32640/225000 (15%)] Loss: 19647.925781\n",
      "Train Epoch: 133 [35136/225000 (16%)] Loss: 19705.521484\n",
      "Train Epoch: 133 [37632/225000 (17%)] Loss: 19235.042969\n",
      "Train Epoch: 133 [40128/225000 (18%)] Loss: 19470.187500\n",
      "Train Epoch: 133 [42624/225000 (19%)] Loss: 19143.089844\n",
      "Train Epoch: 133 [45120/225000 (20%)] Loss: 19397.777344\n",
      "Train Epoch: 133 [47616/225000 (21%)] Loss: 19347.128906\n",
      "Train Epoch: 133 [50112/225000 (22%)] Loss: 19000.359375\n",
      "Train Epoch: 133 [52608/225000 (23%)] Loss: 19605.125000\n",
      "Train Epoch: 133 [55104/225000 (24%)] Loss: 19094.929688\n",
      "Train Epoch: 133 [57600/225000 (26%)] Loss: 19325.808594\n",
      "Train Epoch: 133 [60096/225000 (27%)] Loss: 19198.941406\n",
      "Train Epoch: 133 [62592/225000 (28%)] Loss: 19077.640625\n",
      "Train Epoch: 133 [65088/225000 (29%)] Loss: 19573.376953\n",
      "Train Epoch: 133 [67584/225000 (30%)] Loss: 19564.617188\n",
      "Train Epoch: 133 [70080/225000 (31%)] Loss: 19147.972656\n",
      "Train Epoch: 133 [72576/225000 (32%)] Loss: 19264.636719\n",
      "Train Epoch: 133 [75072/225000 (33%)] Loss: 18945.619141\n",
      "Train Epoch: 133 [77568/225000 (34%)] Loss: 19392.585938\n",
      "Train Epoch: 133 [80064/225000 (36%)] Loss: 19070.917969\n",
      "Train Epoch: 133 [82560/225000 (37%)] Loss: 19319.742188\n",
      "Train Epoch: 133 [85056/225000 (38%)] Loss: 19142.480469\n",
      "Train Epoch: 133 [87552/225000 (39%)] Loss: 19740.867188\n",
      "Train Epoch: 133 [90048/225000 (40%)] Loss: 19938.314453\n",
      "Train Epoch: 133 [92544/225000 (41%)] Loss: 19554.468750\n",
      "Train Epoch: 133 [95040/225000 (42%)] Loss: 19182.810547\n",
      "Train Epoch: 133 [97536/225000 (43%)] Loss: 19148.226562\n",
      "Train Epoch: 133 [100032/225000 (44%)] Loss: 19670.421875\n",
      "Train Epoch: 133 [102528/225000 (46%)] Loss: 19145.644531\n",
      "Train Epoch: 133 [105024/225000 (47%)] Loss: 19023.408203\n",
      "Train Epoch: 133 [107520/225000 (48%)] Loss: 18887.621094\n",
      "Train Epoch: 133 [110016/225000 (49%)] Loss: 19538.109375\n",
      "Train Epoch: 133 [112512/225000 (50%)] Loss: 19554.675781\n",
      "Train Epoch: 133 [115008/225000 (51%)] Loss: 20004.175781\n",
      "Train Epoch: 133 [117504/225000 (52%)] Loss: 19662.361328\n",
      "Train Epoch: 133 [120000/225000 (53%)] Loss: 19667.806641\n",
      "Train Epoch: 133 [122496/225000 (54%)] Loss: 19664.429688\n",
      "Train Epoch: 133 [124992/225000 (56%)] Loss: 19263.613281\n",
      "Train Epoch: 133 [127488/225000 (57%)] Loss: 19035.130859\n",
      "Train Epoch: 133 [129984/225000 (58%)] Loss: 19046.474609\n",
      "Train Epoch: 133 [132480/225000 (59%)] Loss: 19097.523438\n",
      "Train Epoch: 133 [134976/225000 (60%)] Loss: 19559.304688\n",
      "Train Epoch: 133 [137472/225000 (61%)] Loss: 19830.160156\n",
      "Train Epoch: 133 [139968/225000 (62%)] Loss: 19357.527344\n",
      "Train Epoch: 133 [142464/225000 (63%)] Loss: 19729.687500\n",
      "Train Epoch: 133 [144960/225000 (64%)] Loss: 19659.292969\n",
      "Train Epoch: 133 [147456/225000 (66%)] Loss: 19203.341797\n",
      "Train Epoch: 133 [149952/225000 (67%)] Loss: 18951.943359\n",
      "Train Epoch: 133 [152448/225000 (68%)] Loss: 19160.003906\n",
      "Train Epoch: 133 [154944/225000 (69%)] Loss: 19072.255859\n",
      "Train Epoch: 133 [157440/225000 (70%)] Loss: 19264.291016\n",
      "Train Epoch: 133 [159936/225000 (71%)] Loss: 19079.357422\n",
      "Train Epoch: 133 [162432/225000 (72%)] Loss: 18786.871094\n",
      "Train Epoch: 133 [164928/225000 (73%)] Loss: 19339.558594\n",
      "Train Epoch: 133 [167424/225000 (74%)] Loss: 19098.820312\n",
      "Train Epoch: 133 [169920/225000 (76%)] Loss: 19118.031250\n",
      "Train Epoch: 133 [172416/225000 (77%)] Loss: 19297.070312\n",
      "Train Epoch: 133 [174912/225000 (78%)] Loss: 19641.851562\n",
      "Train Epoch: 133 [177408/225000 (79%)] Loss: 19256.816406\n",
      "Train Epoch: 133 [179904/225000 (80%)] Loss: 19801.031250\n",
      "Train Epoch: 133 [182400/225000 (81%)] Loss: 18903.587891\n",
      "Train Epoch: 133 [184896/225000 (82%)] Loss: 19236.660156\n",
      "Train Epoch: 133 [187392/225000 (83%)] Loss: 19669.160156\n",
      "Train Epoch: 133 [189888/225000 (84%)] Loss: 19378.511719\n",
      "Train Epoch: 133 [192384/225000 (86%)] Loss: 19391.335938\n",
      "Train Epoch: 133 [194880/225000 (87%)] Loss: 19717.992188\n",
      "Train Epoch: 133 [197376/225000 (88%)] Loss: 19216.023438\n",
      "Train Epoch: 133 [199872/225000 (89%)] Loss: 19481.011719\n",
      "Train Epoch: 133 [202368/225000 (90%)] Loss: 19506.072266\n",
      "Train Epoch: 133 [204864/225000 (91%)] Loss: 19419.716797\n",
      "Train Epoch: 133 [207360/225000 (92%)] Loss: 18932.230469\n",
      "Train Epoch: 133 [209856/225000 (93%)] Loss: 19109.578125\n",
      "Train Epoch: 133 [212352/225000 (94%)] Loss: 19345.550781\n",
      "Train Epoch: 133 [214848/225000 (95%)] Loss: 19497.230469\n",
      "Train Epoch: 133 [217344/225000 (97%)] Loss: 19502.281250\n",
      "Train Epoch: 133 [219840/225000 (98%)] Loss: 19135.976562\n",
      "Train Epoch: 133 [222336/225000 (99%)] Loss: 19421.625000\n",
      "Train Epoch: 133 [224832/225000 (100%)] Loss: 19247.791016\n",
      "    epoch          : 133\n",
      "    loss           : 19318.691566232934\n",
      "    val_loss       : 19215.57435974183\n",
      "Train Epoch: 134 [192/225000 (0%)] Loss: 19102.050781\n",
      "Train Epoch: 134 [2688/225000 (1%)] Loss: 18988.492188\n",
      "Train Epoch: 134 [5184/225000 (2%)] Loss: 18793.640625\n",
      "Train Epoch: 134 [7680/225000 (3%)] Loss: 19443.492188\n",
      "Train Epoch: 134 [10176/225000 (5%)] Loss: 19258.517578\n",
      "Train Epoch: 134 [12672/225000 (6%)] Loss: 22563.156250\n",
      "Train Epoch: 134 [15168/225000 (7%)] Loss: 19774.390625\n",
      "Train Epoch: 134 [17664/225000 (8%)] Loss: 19416.113281\n",
      "Train Epoch: 134 [20160/225000 (9%)] Loss: 19598.843750\n",
      "Train Epoch: 134 [22656/225000 (10%)] Loss: 19141.015625\n",
      "Train Epoch: 134 [25152/225000 (11%)] Loss: 19268.957031\n",
      "Train Epoch: 134 [27648/225000 (12%)] Loss: 19251.863281\n",
      "Train Epoch: 134 [30144/225000 (13%)] Loss: 19452.744141\n",
      "Train Epoch: 134 [32640/225000 (15%)] Loss: 19198.265625\n",
      "Train Epoch: 134 [35136/225000 (16%)] Loss: 18937.416016\n",
      "Train Epoch: 134 [37632/225000 (17%)] Loss: 19463.812500\n",
      "Train Epoch: 134 [40128/225000 (18%)] Loss: 18903.054688\n",
      "Train Epoch: 134 [42624/225000 (19%)] Loss: 19455.796875\n",
      "Train Epoch: 134 [45120/225000 (20%)] Loss: 19635.535156\n",
      "Train Epoch: 134 [47616/225000 (21%)] Loss: 19363.980469\n",
      "Train Epoch: 134 [50112/225000 (22%)] Loss: 19141.865234\n",
      "Train Epoch: 134 [52608/225000 (23%)] Loss: 18884.482422\n",
      "Train Epoch: 134 [55104/225000 (24%)] Loss: 19350.197266\n",
      "Train Epoch: 134 [57600/225000 (26%)] Loss: 18866.419922\n",
      "Train Epoch: 134 [60096/225000 (27%)] Loss: 19174.730469\n",
      "Train Epoch: 134 [62592/225000 (28%)] Loss: 18848.925781\n",
      "Train Epoch: 134 [65088/225000 (29%)] Loss: 18953.541016\n",
      "Train Epoch: 134 [67584/225000 (30%)] Loss: 19670.810547\n",
      "Train Epoch: 134 [70080/225000 (31%)] Loss: 19882.240234\n",
      "Train Epoch: 134 [72576/225000 (32%)] Loss: 19246.582031\n",
      "Train Epoch: 134 [75072/225000 (33%)] Loss: 19488.652344\n",
      "Train Epoch: 134 [77568/225000 (34%)] Loss: 19253.117188\n",
      "Train Epoch: 134 [80064/225000 (36%)] Loss: 19656.308594\n",
      "Train Epoch: 134 [82560/225000 (37%)] Loss: 19575.230469\n",
      "Train Epoch: 134 [85056/225000 (38%)] Loss: 19282.669922\n",
      "Train Epoch: 134 [87552/225000 (39%)] Loss: 19324.392578\n",
      "Train Epoch: 134 [90048/225000 (40%)] Loss: 19066.601562\n",
      "Train Epoch: 134 [92544/225000 (41%)] Loss: 19168.605469\n",
      "Train Epoch: 134 [95040/225000 (42%)] Loss: 19395.695312\n",
      "Train Epoch: 134 [97536/225000 (43%)] Loss: 19757.482422\n",
      "Train Epoch: 134 [100032/225000 (44%)] Loss: 19220.628906\n",
      "Train Epoch: 134 [102528/225000 (46%)] Loss: 19084.847656\n",
      "Train Epoch: 134 [105024/225000 (47%)] Loss: 19752.408203\n",
      "Train Epoch: 134 [107520/225000 (48%)] Loss: 18846.707031\n",
      "Train Epoch: 134 [110016/225000 (49%)] Loss: 19178.988281\n",
      "Train Epoch: 134 [112512/225000 (50%)] Loss: 19060.066406\n",
      "Train Epoch: 134 [115008/225000 (51%)] Loss: 19726.054688\n",
      "Train Epoch: 134 [117504/225000 (52%)] Loss: 18653.173828\n",
      "Train Epoch: 134 [120000/225000 (53%)] Loss: 19020.669922\n",
      "Train Epoch: 134 [122496/225000 (54%)] Loss: 19624.800781\n",
      "Train Epoch: 134 [124992/225000 (56%)] Loss: 19186.746094\n",
      "Train Epoch: 134 [127488/225000 (57%)] Loss: 19214.087891\n",
      "Train Epoch: 134 [129984/225000 (58%)] Loss: 19573.394531\n",
      "Train Epoch: 134 [132480/225000 (59%)] Loss: 19476.722656\n",
      "Train Epoch: 134 [134976/225000 (60%)] Loss: 19259.359375\n",
      "Train Epoch: 134 [137472/225000 (61%)] Loss: 19616.843750\n",
      "Train Epoch: 134 [139968/225000 (62%)] Loss: 19234.083984\n",
      "Train Epoch: 134 [142464/225000 (63%)] Loss: 19540.818359\n",
      "Train Epoch: 134 [144960/225000 (64%)] Loss: 19176.722656\n",
      "Train Epoch: 134 [147456/225000 (66%)] Loss: 18683.238281\n",
      "Train Epoch: 134 [149952/225000 (67%)] Loss: 19038.277344\n",
      "Train Epoch: 134 [152448/225000 (68%)] Loss: 18826.583984\n",
      "Train Epoch: 134 [154944/225000 (69%)] Loss: 19904.925781\n",
      "Train Epoch: 134 [157440/225000 (70%)] Loss: 18862.789062\n",
      "Train Epoch: 134 [159936/225000 (71%)] Loss: 19177.292969\n",
      "Train Epoch: 134 [162432/225000 (72%)] Loss: 19192.730469\n",
      "Train Epoch: 134 [164928/225000 (73%)] Loss: 19427.271484\n",
      "Train Epoch: 134 [167424/225000 (74%)] Loss: 19005.246094\n",
      "Train Epoch: 134 [169920/225000 (76%)] Loss: 19544.480469\n",
      "Train Epoch: 134 [172416/225000 (77%)] Loss: 19182.265625\n",
      "Train Epoch: 134 [174912/225000 (78%)] Loss: 18824.218750\n",
      "Train Epoch: 134 [177408/225000 (79%)] Loss: 19256.339844\n",
      "Train Epoch: 134 [179904/225000 (80%)] Loss: 19542.765625\n",
      "Train Epoch: 134 [182400/225000 (81%)] Loss: 19235.457031\n",
      "Train Epoch: 134 [184896/225000 (82%)] Loss: 19391.662109\n",
      "Train Epoch: 134 [187392/225000 (83%)] Loss: 19353.105469\n",
      "Train Epoch: 134 [189888/225000 (84%)] Loss: 19216.757812\n",
      "Train Epoch: 134 [192384/225000 (86%)] Loss: 19252.287109\n",
      "Train Epoch: 134 [194880/225000 (87%)] Loss: 19461.000000\n",
      "Train Epoch: 134 [197376/225000 (88%)] Loss: 19385.140625\n",
      "Train Epoch: 134 [199872/225000 (89%)] Loss: 18941.445312\n",
      "Train Epoch: 134 [202368/225000 (90%)] Loss: 19261.623047\n",
      "Train Epoch: 134 [204864/225000 (91%)] Loss: 19293.419922\n",
      "Train Epoch: 134 [207360/225000 (92%)] Loss: 19120.304688\n",
      "Train Epoch: 134 [209856/225000 (93%)] Loss: 18741.005859\n",
      "Train Epoch: 134 [212352/225000 (94%)] Loss: 19432.355469\n",
      "Train Epoch: 134 [214848/225000 (95%)] Loss: 19713.250000\n",
      "Train Epoch: 134 [217344/225000 (97%)] Loss: 19059.322266\n",
      "Train Epoch: 134 [219840/225000 (98%)] Loss: 18904.828125\n",
      "Train Epoch: 134 [222336/225000 (99%)] Loss: 19247.941406\n",
      "Train Epoch: 134 [224832/225000 (100%)] Loss: 18885.800781\n",
      "    epoch          : 134\n",
      "    loss           : 19313.72026150544\n",
      "    val_loss       : 19256.98793377767\n",
      "Train Epoch: 135 [192/225000 (0%)] Loss: 19427.605469\n",
      "Train Epoch: 135 [2688/225000 (1%)] Loss: 19503.148438\n",
      "Train Epoch: 135 [5184/225000 (2%)] Loss: 19251.847656\n",
      "Train Epoch: 135 [7680/225000 (3%)] Loss: 18899.574219\n",
      "Train Epoch: 135 [10176/225000 (5%)] Loss: 19215.263672\n",
      "Train Epoch: 135 [12672/225000 (6%)] Loss: 19657.535156\n",
      "Train Epoch: 135 [15168/225000 (7%)] Loss: 19895.242188\n",
      "Train Epoch: 135 [17664/225000 (8%)] Loss: 19795.355469\n",
      "Train Epoch: 135 [20160/225000 (9%)] Loss: 18936.367188\n",
      "Train Epoch: 135 [22656/225000 (10%)] Loss: 18750.871094\n",
      "Train Epoch: 135 [25152/225000 (11%)] Loss: 19374.085938\n",
      "Train Epoch: 135 [27648/225000 (12%)] Loss: 18861.509766\n",
      "Train Epoch: 135 [30144/225000 (13%)] Loss: 19598.738281\n",
      "Train Epoch: 135 [32640/225000 (15%)] Loss: 19364.777344\n",
      "Train Epoch: 135 [35136/225000 (16%)] Loss: 19454.931641\n",
      "Train Epoch: 135 [37632/225000 (17%)] Loss: 19485.453125\n",
      "Train Epoch: 135 [40128/225000 (18%)] Loss: 18984.878906\n",
      "Train Epoch: 135 [42624/225000 (19%)] Loss: 19056.296875\n",
      "Train Epoch: 135 [45120/225000 (20%)] Loss: 19500.880859\n",
      "Train Epoch: 135 [47616/225000 (21%)] Loss: 18904.425781\n",
      "Train Epoch: 135 [50112/225000 (22%)] Loss: 19085.507812\n",
      "Train Epoch: 135 [52608/225000 (23%)] Loss: 19214.101562\n",
      "Train Epoch: 135 [55104/225000 (24%)] Loss: 19294.943359\n",
      "Train Epoch: 135 [57600/225000 (26%)] Loss: 19383.294922\n",
      "Train Epoch: 135 [60096/225000 (27%)] Loss: 19311.167969\n",
      "Train Epoch: 135 [62592/225000 (28%)] Loss: 19020.951172\n",
      "Train Epoch: 135 [65088/225000 (29%)] Loss: 19175.460938\n",
      "Train Epoch: 135 [67584/225000 (30%)] Loss: 19310.218750\n",
      "Train Epoch: 135 [70080/225000 (31%)] Loss: 19449.714844\n",
      "Train Epoch: 135 [72576/225000 (32%)] Loss: 18850.191406\n",
      "Train Epoch: 135 [75072/225000 (33%)] Loss: 19521.283203\n",
      "Train Epoch: 135 [77568/225000 (34%)] Loss: 19246.236328\n",
      "Train Epoch: 135 [80064/225000 (36%)] Loss: 19828.875000\n",
      "Train Epoch: 135 [82560/225000 (37%)] Loss: 20005.384766\n",
      "Train Epoch: 135 [85056/225000 (38%)] Loss: 19389.675781\n",
      "Train Epoch: 135 [87552/225000 (39%)] Loss: 18977.908203\n",
      "Train Epoch: 135 [90048/225000 (40%)] Loss: 19002.097656\n",
      "Train Epoch: 135 [92544/225000 (41%)] Loss: 19093.048828\n",
      "Train Epoch: 135 [95040/225000 (42%)] Loss: 18697.746094\n",
      "Train Epoch: 135 [97536/225000 (43%)] Loss: 19032.355469\n",
      "Train Epoch: 135 [100032/225000 (44%)] Loss: 19250.861328\n",
      "Train Epoch: 135 [102528/225000 (46%)] Loss: 18886.101562\n",
      "Train Epoch: 135 [105024/225000 (47%)] Loss: 19530.490234\n",
      "Train Epoch: 135 [107520/225000 (48%)] Loss: 19715.263672\n",
      "Train Epoch: 135 [110016/225000 (49%)] Loss: 18998.912109\n",
      "Train Epoch: 135 [112512/225000 (50%)] Loss: 19793.406250\n",
      "Train Epoch: 135 [115008/225000 (51%)] Loss: 19443.876953\n",
      "Train Epoch: 135 [117504/225000 (52%)] Loss: 19250.605469\n",
      "Train Epoch: 135 [120000/225000 (53%)] Loss: 19055.455078\n",
      "Train Epoch: 135 [122496/225000 (54%)] Loss: 19452.210938\n",
      "Train Epoch: 135 [124992/225000 (56%)] Loss: 19437.058594\n",
      "Train Epoch: 135 [127488/225000 (57%)] Loss: 19390.343750\n",
      "Train Epoch: 135 [129984/225000 (58%)] Loss: 19253.292969\n",
      "Train Epoch: 135 [132480/225000 (59%)] Loss: 19093.390625\n",
      "Train Epoch: 135 [134976/225000 (60%)] Loss: 19602.011719\n",
      "Train Epoch: 135 [137472/225000 (61%)] Loss: 19169.585938\n",
      "Train Epoch: 135 [139968/225000 (62%)] Loss: 19138.738281\n",
      "Train Epoch: 135 [142464/225000 (63%)] Loss: 19257.033203\n",
      "Train Epoch: 135 [144960/225000 (64%)] Loss: 19412.664062\n",
      "Train Epoch: 135 [147456/225000 (66%)] Loss: 19569.962891\n",
      "Train Epoch: 135 [149952/225000 (67%)] Loss: 19142.019531\n",
      "Train Epoch: 135 [152448/225000 (68%)] Loss: 19390.113281\n",
      "Train Epoch: 135 [154944/225000 (69%)] Loss: 19132.425781\n",
      "Train Epoch: 135 [157440/225000 (70%)] Loss: 19214.248047\n",
      "Train Epoch: 135 [159936/225000 (71%)] Loss: 18854.027344\n",
      "Train Epoch: 135 [162432/225000 (72%)] Loss: 19809.578125\n",
      "Train Epoch: 135 [164928/225000 (73%)] Loss: 18945.845703\n",
      "Train Epoch: 135 [167424/225000 (74%)] Loss: 19713.755859\n",
      "Train Epoch: 135 [169920/225000 (76%)] Loss: 18683.425781\n",
      "Train Epoch: 135 [172416/225000 (77%)] Loss: 19109.917969\n",
      "Train Epoch: 135 [174912/225000 (78%)] Loss: 19380.437500\n",
      "Train Epoch: 135 [177408/225000 (79%)] Loss: 19281.574219\n",
      "Train Epoch: 135 [179904/225000 (80%)] Loss: 19698.439453\n",
      "Train Epoch: 135 [182400/225000 (81%)] Loss: 19334.259766\n",
      "Train Epoch: 135 [184896/225000 (82%)] Loss: 19360.421875\n",
      "Train Epoch: 135 [187392/225000 (83%)] Loss: 19654.816406\n",
      "Train Epoch: 135 [189888/225000 (84%)] Loss: 19192.308594\n",
      "Train Epoch: 135 [192384/225000 (86%)] Loss: 19422.529297\n",
      "Train Epoch: 135 [194880/225000 (87%)] Loss: 19388.205078\n",
      "Train Epoch: 135 [197376/225000 (88%)] Loss: 19374.585938\n",
      "Train Epoch: 135 [199872/225000 (89%)] Loss: 19313.312500\n",
      "Train Epoch: 135 [202368/225000 (90%)] Loss: 19783.777344\n",
      "Train Epoch: 135 [204864/225000 (91%)] Loss: 20000.652344\n",
      "Train Epoch: 135 [207360/225000 (92%)] Loss: 19276.441406\n",
      "Train Epoch: 135 [209856/225000 (93%)] Loss: 19617.089844\n",
      "Train Epoch: 135 [212352/225000 (94%)] Loss: 19190.929688\n",
      "Train Epoch: 135 [214848/225000 (95%)] Loss: 19409.187500\n",
      "Train Epoch: 135 [217344/225000 (97%)] Loss: 19529.246094\n",
      "Train Epoch: 135 [219840/225000 (98%)] Loss: 19225.921875\n",
      "Train Epoch: 135 [222336/225000 (99%)] Loss: 18875.775391\n",
      "Train Epoch: 135 [224832/225000 (100%)] Loss: 19541.167969\n",
      "    epoch          : 135\n",
      "    loss           : 19310.671633359107\n",
      "    val_loss       : 19210.34551998677\n",
      "Train Epoch: 136 [192/225000 (0%)] Loss: 19203.707031\n",
      "Train Epoch: 136 [2688/225000 (1%)] Loss: 19031.804688\n",
      "Train Epoch: 136 [5184/225000 (2%)] Loss: 19383.898438\n",
      "Train Epoch: 136 [7680/225000 (3%)] Loss: 19223.091797\n",
      "Train Epoch: 136 [10176/225000 (5%)] Loss: 19576.242188\n",
      "Train Epoch: 136 [12672/225000 (6%)] Loss: 19001.296875\n",
      "Train Epoch: 136 [15168/225000 (7%)] Loss: 19525.914062\n",
      "Train Epoch: 136 [17664/225000 (8%)] Loss: 19641.023438\n",
      "Train Epoch: 136 [20160/225000 (9%)] Loss: 19858.933594\n",
      "Train Epoch: 136 [22656/225000 (10%)] Loss: 19751.613281\n",
      "Train Epoch: 136 [25152/225000 (11%)] Loss: 19089.976562\n",
      "Train Epoch: 136 [27648/225000 (12%)] Loss: 19202.781250\n",
      "Train Epoch: 136 [30144/225000 (13%)] Loss: 18935.753906\n",
      "Train Epoch: 136 [32640/225000 (15%)] Loss: 19463.308594\n",
      "Train Epoch: 136 [35136/225000 (16%)] Loss: 19696.318359\n",
      "Train Epoch: 136 [37632/225000 (17%)] Loss: 19376.617188\n",
      "Train Epoch: 136 [40128/225000 (18%)] Loss: 19115.398438\n",
      "Train Epoch: 136 [42624/225000 (19%)] Loss: 19262.650391\n",
      "Train Epoch: 136 [45120/225000 (20%)] Loss: 19335.140625\n",
      "Train Epoch: 136 [47616/225000 (21%)] Loss: 19213.371094\n",
      "Train Epoch: 136 [50112/225000 (22%)] Loss: 19619.488281\n",
      "Train Epoch: 136 [52608/225000 (23%)] Loss: 18984.882812\n",
      "Train Epoch: 136 [55104/225000 (24%)] Loss: 19065.824219\n",
      "Train Epoch: 136 [57600/225000 (26%)] Loss: 19280.335938\n",
      "Train Epoch: 136 [60096/225000 (27%)] Loss: 19287.916016\n",
      "Train Epoch: 136 [62592/225000 (28%)] Loss: 18925.113281\n",
      "Train Epoch: 136 [65088/225000 (29%)] Loss: 19607.031250\n",
      "Train Epoch: 136 [67584/225000 (30%)] Loss: 19259.457031\n",
      "Train Epoch: 136 [70080/225000 (31%)] Loss: 19876.984375\n",
      "Train Epoch: 136 [72576/225000 (32%)] Loss: 19486.394531\n",
      "Train Epoch: 136 [75072/225000 (33%)] Loss: 19114.171875\n",
      "Train Epoch: 136 [77568/225000 (34%)] Loss: 19511.804688\n",
      "Train Epoch: 136 [80064/225000 (36%)] Loss: 19196.992188\n",
      "Train Epoch: 136 [82560/225000 (37%)] Loss: 19162.957031\n",
      "Train Epoch: 136 [85056/225000 (38%)] Loss: 18848.035156\n",
      "Train Epoch: 136 [87552/225000 (39%)] Loss: 19896.201172\n",
      "Train Epoch: 136 [90048/225000 (40%)] Loss: 19566.324219\n",
      "Train Epoch: 136 [92544/225000 (41%)] Loss: 19224.667969\n",
      "Train Epoch: 136 [95040/225000 (42%)] Loss: 18882.152344\n",
      "Train Epoch: 136 [97536/225000 (43%)] Loss: 19427.917969\n",
      "Train Epoch: 136 [100032/225000 (44%)] Loss: 19305.800781\n",
      "Train Epoch: 136 [102528/225000 (46%)] Loss: 19378.617188\n",
      "Train Epoch: 136 [105024/225000 (47%)] Loss: 19666.742188\n",
      "Train Epoch: 136 [107520/225000 (48%)] Loss: 19037.335938\n",
      "Train Epoch: 136 [110016/225000 (49%)] Loss: 19350.421875\n",
      "Train Epoch: 136 [112512/225000 (50%)] Loss: 18814.519531\n",
      "Train Epoch: 136 [115008/225000 (51%)] Loss: 18813.566406\n",
      "Train Epoch: 136 [117504/225000 (52%)] Loss: 19516.101562\n",
      "Train Epoch: 136 [120000/225000 (53%)] Loss: 18979.976562\n",
      "Train Epoch: 136 [122496/225000 (54%)] Loss: 19661.363281\n",
      "Train Epoch: 136 [124992/225000 (56%)] Loss: 19146.867188\n",
      "Train Epoch: 136 [127488/225000 (57%)] Loss: 19267.044922\n",
      "Train Epoch: 136 [129984/225000 (58%)] Loss: 19725.945312\n",
      "Train Epoch: 136 [132480/225000 (59%)] Loss: 19167.277344\n",
      "Train Epoch: 136 [134976/225000 (60%)] Loss: 19567.316406\n",
      "Train Epoch: 136 [137472/225000 (61%)] Loss: 19489.542969\n",
      "Train Epoch: 136 [139968/225000 (62%)] Loss: 18842.859375\n",
      "Train Epoch: 136 [142464/225000 (63%)] Loss: 19227.566406\n",
      "Train Epoch: 136 [144960/225000 (64%)] Loss: 19143.058594\n",
      "Train Epoch: 136 [147456/225000 (66%)] Loss: 19192.550781\n",
      "Train Epoch: 136 [149952/225000 (67%)] Loss: 19130.328125\n",
      "Train Epoch: 136 [152448/225000 (68%)] Loss: 20027.757812\n",
      "Train Epoch: 136 [154944/225000 (69%)] Loss: 19318.773438\n",
      "Train Epoch: 136 [157440/225000 (70%)] Loss: 19393.257812\n",
      "Train Epoch: 136 [159936/225000 (71%)] Loss: 18906.289062\n",
      "Train Epoch: 136 [162432/225000 (72%)] Loss: 18964.503906\n",
      "Train Epoch: 136 [164928/225000 (73%)] Loss: 19462.939453\n",
      "Train Epoch: 136 [167424/225000 (74%)] Loss: 19130.369141\n",
      "Train Epoch: 136 [169920/225000 (76%)] Loss: 18946.480469\n",
      "Train Epoch: 136 [172416/225000 (77%)] Loss: 19368.085938\n",
      "Train Epoch: 136 [174912/225000 (78%)] Loss: 19145.398438\n",
      "Train Epoch: 136 [177408/225000 (79%)] Loss: 19750.326172\n",
      "Train Epoch: 136 [179904/225000 (80%)] Loss: 18962.814453\n",
      "Train Epoch: 136 [182400/225000 (81%)] Loss: 19729.835938\n",
      "Train Epoch: 136 [184896/225000 (82%)] Loss: 19695.740234\n",
      "Train Epoch: 136 [187392/225000 (83%)] Loss: 19016.708984\n",
      "Train Epoch: 136 [189888/225000 (84%)] Loss: 19529.906250\n",
      "Train Epoch: 136 [192384/225000 (86%)] Loss: 19569.968750\n",
      "Train Epoch: 136 [194880/225000 (87%)] Loss: 19405.595703\n",
      "Train Epoch: 136 [197376/225000 (88%)] Loss: 19818.441406\n",
      "Train Epoch: 136 [199872/225000 (89%)] Loss: 19406.703125\n",
      "Train Epoch: 136 [202368/225000 (90%)] Loss: 18983.617188\n",
      "Train Epoch: 136 [204864/225000 (91%)] Loss: 19384.453125\n",
      "Train Epoch: 136 [207360/225000 (92%)] Loss: 19153.152344\n",
      "Train Epoch: 136 [209856/225000 (93%)] Loss: 19142.742188\n",
      "Train Epoch: 136 [212352/225000 (94%)] Loss: 19260.066406\n",
      "Train Epoch: 136 [214848/225000 (95%)] Loss: 19069.359375\n",
      "Train Epoch: 136 [217344/225000 (97%)] Loss: 19282.304688\n",
      "Train Epoch: 136 [219840/225000 (98%)] Loss: 19318.855469\n",
      "Train Epoch: 136 [222336/225000 (99%)] Loss: 18903.027344\n",
      "Train Epoch: 136 [224832/225000 (100%)] Loss: 19215.503906\n",
      "    epoch          : 136\n",
      "    loss           : 19306.02772204298\n",
      "    val_loss       : 19209.731348470876\n",
      "Train Epoch: 137 [192/225000 (0%)] Loss: 19337.203125\n",
      "Train Epoch: 137 [2688/225000 (1%)] Loss: 19169.960938\n",
      "Train Epoch: 137 [5184/225000 (2%)] Loss: 19408.119141\n",
      "Train Epoch: 137 [7680/225000 (3%)] Loss: 19423.933594\n",
      "Train Epoch: 137 [10176/225000 (5%)] Loss: 19142.599609\n",
      "Train Epoch: 137 [12672/225000 (6%)] Loss: 19081.150391\n",
      "Train Epoch: 137 [15168/225000 (7%)] Loss: 19083.691406\n",
      "Train Epoch: 137 [17664/225000 (8%)] Loss: 19507.015625\n",
      "Train Epoch: 137 [20160/225000 (9%)] Loss: 18711.279297\n",
      "Train Epoch: 137 [22656/225000 (10%)] Loss: 18843.132812\n",
      "Train Epoch: 137 [25152/225000 (11%)] Loss: 19110.603516\n",
      "Train Epoch: 137 [27648/225000 (12%)] Loss: 19025.732422\n",
      "Train Epoch: 137 [30144/225000 (13%)] Loss: 19430.054688\n",
      "Train Epoch: 137 [32640/225000 (15%)] Loss: 19411.400391\n",
      "Train Epoch: 137 [35136/225000 (16%)] Loss: 19247.832031\n",
      "Train Epoch: 137 [37632/225000 (17%)] Loss: 19349.009766\n",
      "Train Epoch: 137 [40128/225000 (18%)] Loss: 19790.101562\n",
      "Train Epoch: 137 [42624/225000 (19%)] Loss: 19407.646484\n",
      "Train Epoch: 137 [45120/225000 (20%)] Loss: 19468.964844\n",
      "Train Epoch: 137 [47616/225000 (21%)] Loss: 19423.578125\n",
      "Train Epoch: 137 [50112/225000 (22%)] Loss: 19493.796875\n",
      "Train Epoch: 137 [52608/225000 (23%)] Loss: 19270.878906\n",
      "Train Epoch: 137 [55104/225000 (24%)] Loss: 19507.871094\n",
      "Train Epoch: 137 [57600/225000 (26%)] Loss: 18958.980469\n",
      "Train Epoch: 137 [60096/225000 (27%)] Loss: 19094.660156\n",
      "Train Epoch: 137 [62592/225000 (28%)] Loss: 19233.472656\n",
      "Train Epoch: 137 [65088/225000 (29%)] Loss: 19669.261719\n",
      "Train Epoch: 137 [67584/225000 (30%)] Loss: 19485.548828\n",
      "Train Epoch: 137 [70080/225000 (31%)] Loss: 19291.718750\n",
      "Train Epoch: 137 [72576/225000 (32%)] Loss: 19252.597656\n",
      "Train Epoch: 137 [75072/225000 (33%)] Loss: 18962.695312\n",
      "Train Epoch: 137 [77568/225000 (34%)] Loss: 19224.003906\n",
      "Train Epoch: 137 [80064/225000 (36%)] Loss: 19400.093750\n",
      "Train Epoch: 137 [82560/225000 (37%)] Loss: 19487.679688\n",
      "Train Epoch: 137 [85056/225000 (38%)] Loss: 19438.054688\n",
      "Train Epoch: 137 [87552/225000 (39%)] Loss: 19481.027344\n",
      "Train Epoch: 137 [90048/225000 (40%)] Loss: 19102.878906\n",
      "Train Epoch: 137 [92544/225000 (41%)] Loss: 19674.218750\n",
      "Train Epoch: 137 [95040/225000 (42%)] Loss: 19344.019531\n",
      "Train Epoch: 137 [97536/225000 (43%)] Loss: 19442.404297\n",
      "Train Epoch: 137 [100032/225000 (44%)] Loss: 18728.304688\n",
      "Train Epoch: 137 [102528/225000 (46%)] Loss: 19290.433594\n",
      "Train Epoch: 137 [105024/225000 (47%)] Loss: 19315.308594\n",
      "Train Epoch: 137 [107520/225000 (48%)] Loss: 19291.632812\n",
      "Train Epoch: 137 [110016/225000 (49%)] Loss: 19722.667969\n",
      "Train Epoch: 137 [112512/225000 (50%)] Loss: 19455.976562\n",
      "Train Epoch: 137 [115008/225000 (51%)] Loss: 19195.019531\n",
      "Train Epoch: 137 [117504/225000 (52%)] Loss: 19271.847656\n",
      "Train Epoch: 137 [120000/225000 (53%)] Loss: 19550.066406\n",
      "Train Epoch: 137 [122496/225000 (54%)] Loss: 19331.789062\n",
      "Train Epoch: 137 [124992/225000 (56%)] Loss: 19447.136719\n",
      "Train Epoch: 137 [127488/225000 (57%)] Loss: 19318.972656\n",
      "Train Epoch: 137 [129984/225000 (58%)] Loss: 19418.449219\n",
      "Train Epoch: 137 [132480/225000 (59%)] Loss: 19042.281250\n",
      "Train Epoch: 137 [134976/225000 (60%)] Loss: 19594.619141\n",
      "Train Epoch: 137 [137472/225000 (61%)] Loss: 18946.863281\n",
      "Train Epoch: 137 [139968/225000 (62%)] Loss: 18989.800781\n",
      "Train Epoch: 137 [142464/225000 (63%)] Loss: 19057.800781\n",
      "Train Epoch: 137 [144960/225000 (64%)] Loss: 19560.964844\n",
      "Train Epoch: 137 [147456/225000 (66%)] Loss: 19311.906250\n",
      "Train Epoch: 137 [149952/225000 (67%)] Loss: 19443.822266\n",
      "Train Epoch: 137 [152448/225000 (68%)] Loss: 19056.648438\n",
      "Train Epoch: 137 [154944/225000 (69%)] Loss: 19479.054688\n",
      "Train Epoch: 137 [157440/225000 (70%)] Loss: 18700.101562\n",
      "Train Epoch: 137 [159936/225000 (71%)] Loss: 19267.242188\n",
      "Train Epoch: 137 [162432/225000 (72%)] Loss: 19117.898438\n",
      "Train Epoch: 137 [164928/225000 (73%)] Loss: 19117.574219\n",
      "Train Epoch: 137 [167424/225000 (74%)] Loss: 19118.105469\n",
      "Train Epoch: 137 [169920/225000 (76%)] Loss: 19661.003906\n",
      "Train Epoch: 137 [172416/225000 (77%)] Loss: 19353.875000\n",
      "Train Epoch: 137 [174912/225000 (78%)] Loss: 19427.667969\n",
      "Train Epoch: 137 [177408/225000 (79%)] Loss: 19228.970703\n",
      "Train Epoch: 137 [179904/225000 (80%)] Loss: 19189.138672\n",
      "Train Epoch: 137 [182400/225000 (81%)] Loss: 19425.792969\n",
      "Train Epoch: 137 [184896/225000 (82%)] Loss: 19422.554688\n",
      "Train Epoch: 137 [187392/225000 (83%)] Loss: 19064.804688\n",
      "Train Epoch: 137 [189888/225000 (84%)] Loss: 19383.289062\n",
      "Train Epoch: 137 [192384/225000 (86%)] Loss: 19225.232422\n",
      "Train Epoch: 137 [194880/225000 (87%)] Loss: 19620.023438\n",
      "Train Epoch: 137 [197376/225000 (88%)] Loss: 18823.175781\n",
      "Train Epoch: 137 [199872/225000 (89%)] Loss: 19203.291016\n",
      "Train Epoch: 137 [202368/225000 (90%)] Loss: 19309.355469\n",
      "Train Epoch: 137 [204864/225000 (91%)] Loss: 19214.453125\n",
      "Train Epoch: 137 [207360/225000 (92%)] Loss: 19500.988281\n",
      "Train Epoch: 137 [209856/225000 (93%)] Loss: 19170.199219\n",
      "Train Epoch: 137 [212352/225000 (94%)] Loss: 18593.708984\n",
      "Train Epoch: 137 [214848/225000 (95%)] Loss: 18998.667969\n",
      "Train Epoch: 137 [217344/225000 (97%)] Loss: 19365.953125\n",
      "Train Epoch: 137 [219840/225000 (98%)] Loss: 18794.406250\n",
      "Train Epoch: 137 [222336/225000 (99%)] Loss: 19135.460938\n",
      "Train Epoch: 137 [224832/225000 (100%)] Loss: 19700.296875\n",
      "    epoch          : 137\n",
      "    loss           : 19306.214517118173\n",
      "    val_loss       : 19213.991111496023\n",
      "Train Epoch: 138 [192/225000 (0%)] Loss: 19167.566406\n",
      "Train Epoch: 138 [2688/225000 (1%)] Loss: 19773.833984\n",
      "Train Epoch: 138 [5184/225000 (2%)] Loss: 19623.353516\n",
      "Train Epoch: 138 [7680/225000 (3%)] Loss: 19709.658203\n",
      "Train Epoch: 138 [10176/225000 (5%)] Loss: 19414.589844\n",
      "Train Epoch: 138 [12672/225000 (6%)] Loss: 19757.761719\n",
      "Train Epoch: 138 [15168/225000 (7%)] Loss: 19653.240234\n",
      "Train Epoch: 138 [17664/225000 (8%)] Loss: 18982.531250\n",
      "Train Epoch: 138 [20160/225000 (9%)] Loss: 19554.777344\n",
      "Train Epoch: 138 [22656/225000 (10%)] Loss: 19348.679688\n",
      "Train Epoch: 138 [25152/225000 (11%)] Loss: 19072.876953\n",
      "Train Epoch: 138 [27648/225000 (12%)] Loss: 19042.994141\n",
      "Train Epoch: 138 [30144/225000 (13%)] Loss: 19208.789062\n",
      "Train Epoch: 138 [32640/225000 (15%)] Loss: 18501.916016\n",
      "Train Epoch: 138 [35136/225000 (16%)] Loss: 19607.953125\n",
      "Train Epoch: 138 [37632/225000 (17%)] Loss: 19004.824219\n",
      "Train Epoch: 138 [40128/225000 (18%)] Loss: 19043.355469\n",
      "Train Epoch: 138 [42624/225000 (19%)] Loss: 19098.408203\n",
      "Train Epoch: 138 [45120/225000 (20%)] Loss: 19384.132812\n",
      "Train Epoch: 138 [47616/225000 (21%)] Loss: 18851.722656\n",
      "Train Epoch: 138 [50112/225000 (22%)] Loss: 18841.371094\n",
      "Train Epoch: 138 [52608/225000 (23%)] Loss: 19358.863281\n",
      "Train Epoch: 138 [55104/225000 (24%)] Loss: 18913.992188\n",
      "Train Epoch: 138 [57600/225000 (26%)] Loss: 19190.320312\n",
      "Train Epoch: 138 [60096/225000 (27%)] Loss: 19076.906250\n",
      "Train Epoch: 138 [62592/225000 (28%)] Loss: 19218.404297\n",
      "Train Epoch: 138 [65088/225000 (29%)] Loss: 18859.013672\n",
      "Train Epoch: 138 [67584/225000 (30%)] Loss: 19401.867188\n",
      "Train Epoch: 138 [70080/225000 (31%)] Loss: 19667.906250\n",
      "Train Epoch: 138 [72576/225000 (32%)] Loss: 19113.914062\n",
      "Train Epoch: 138 [75072/225000 (33%)] Loss: 18616.078125\n",
      "Train Epoch: 138 [77568/225000 (34%)] Loss: 18889.189453\n",
      "Train Epoch: 138 [80064/225000 (36%)] Loss: 19299.414062\n",
      "Train Epoch: 138 [82560/225000 (37%)] Loss: 19332.843750\n",
      "Train Epoch: 138 [85056/225000 (38%)] Loss: 19384.554688\n",
      "Train Epoch: 138 [87552/225000 (39%)] Loss: 19157.005859\n",
      "Train Epoch: 138 [90048/225000 (40%)] Loss: 19461.917969\n",
      "Train Epoch: 138 [92544/225000 (41%)] Loss: 19239.697266\n",
      "Train Epoch: 138 [95040/225000 (42%)] Loss: 19415.556641\n",
      "Train Epoch: 138 [97536/225000 (43%)] Loss: 19251.607422\n",
      "Train Epoch: 138 [100032/225000 (44%)] Loss: 19233.763672\n",
      "Train Epoch: 138 [102528/225000 (46%)] Loss: 19738.421875\n",
      "Train Epoch: 138 [105024/225000 (47%)] Loss: 19024.482422\n",
      "Train Epoch: 138 [107520/225000 (48%)] Loss: 19107.554688\n",
      "Train Epoch: 138 [110016/225000 (49%)] Loss: 19015.820312\n",
      "Train Epoch: 138 [112512/225000 (50%)] Loss: 19503.699219\n",
      "Train Epoch: 138 [115008/225000 (51%)] Loss: 19153.437500\n",
      "Train Epoch: 138 [117504/225000 (52%)] Loss: 19388.351562\n",
      "Train Epoch: 138 [120000/225000 (53%)] Loss: 19225.117188\n",
      "Train Epoch: 138 [122496/225000 (54%)] Loss: 19328.613281\n",
      "Train Epoch: 138 [124992/225000 (56%)] Loss: 19254.453125\n",
      "Train Epoch: 138 [127488/225000 (57%)] Loss: 19481.644531\n",
      "Train Epoch: 138 [129984/225000 (58%)] Loss: 19066.707031\n",
      "Train Epoch: 138 [132480/225000 (59%)] Loss: 19395.794922\n",
      "Train Epoch: 138 [134976/225000 (60%)] Loss: 19140.792969\n",
      "Train Epoch: 138 [137472/225000 (61%)] Loss: 19343.941406\n",
      "Train Epoch: 138 [139968/225000 (62%)] Loss: 19353.513672\n",
      "Train Epoch: 138 [142464/225000 (63%)] Loss: 19027.367188\n",
      "Train Epoch: 138 [144960/225000 (64%)] Loss: 19003.941406\n",
      "Train Epoch: 138 [147456/225000 (66%)] Loss: 19090.214844\n",
      "Train Epoch: 138 [149952/225000 (67%)] Loss: 19178.128906\n",
      "Train Epoch: 138 [152448/225000 (68%)] Loss: 18961.488281\n",
      "Train Epoch: 138 [154944/225000 (69%)] Loss: 19231.683594\n",
      "Train Epoch: 138 [157440/225000 (70%)] Loss: 19508.763672\n",
      "Train Epoch: 138 [159936/225000 (71%)] Loss: 19382.347656\n",
      "Train Epoch: 138 [162432/225000 (72%)] Loss: 19389.281250\n",
      "Train Epoch: 138 [164928/225000 (73%)] Loss: 19415.171875\n",
      "Train Epoch: 138 [167424/225000 (74%)] Loss: 19481.937500\n",
      "Train Epoch: 138 [169920/225000 (76%)] Loss: 19464.093750\n",
      "Train Epoch: 138 [172416/225000 (77%)] Loss: 19243.535156\n",
      "Train Epoch: 138 [174912/225000 (78%)] Loss: 19351.769531\n",
      "Train Epoch: 138 [177408/225000 (79%)] Loss: 19755.474609\n",
      "Train Epoch: 138 [179904/225000 (80%)] Loss: 19038.636719\n",
      "Train Epoch: 138 [182400/225000 (81%)] Loss: 19527.054688\n",
      "Train Epoch: 138 [184896/225000 (82%)] Loss: 19080.375000\n",
      "Train Epoch: 138 [187392/225000 (83%)] Loss: 19160.703125\n",
      "Train Epoch: 138 [189888/225000 (84%)] Loss: 19804.394531\n",
      "Train Epoch: 138 [192384/225000 (86%)] Loss: 19165.968750\n",
      "Train Epoch: 138 [194880/225000 (87%)] Loss: 19838.531250\n",
      "Train Epoch: 138 [197376/225000 (88%)] Loss: 19175.644531\n",
      "Train Epoch: 138 [199872/225000 (89%)] Loss: 19456.437500\n",
      "Train Epoch: 138 [202368/225000 (90%)] Loss: 18988.417969\n",
      "Train Epoch: 138 [204864/225000 (91%)] Loss: 19144.708984\n",
      "Train Epoch: 138 [207360/225000 (92%)] Loss: 19483.298828\n",
      "Train Epoch: 138 [209856/225000 (93%)] Loss: 19638.601562\n",
      "Train Epoch: 138 [212352/225000 (94%)] Loss: 18851.031250\n",
      "Train Epoch: 138 [214848/225000 (95%)] Loss: 19234.537109\n",
      "Train Epoch: 138 [217344/225000 (97%)] Loss: 19378.658203\n",
      "Train Epoch: 138 [219840/225000 (98%)] Loss: 19961.890625\n",
      "Train Epoch: 138 [222336/225000 (99%)] Loss: 19122.425781\n",
      "Train Epoch: 138 [224832/225000 (100%)] Loss: 19690.429688\n",
      "    epoch          : 138\n",
      "    loss           : 19298.991600895904\n",
      "    val_loss       : 19212.291722246708\n",
      "Train Epoch: 139 [192/225000 (0%)] Loss: 19479.476562\n",
      "Train Epoch: 139 [2688/225000 (1%)] Loss: 19744.261719\n",
      "Train Epoch: 139 [5184/225000 (2%)] Loss: 19528.546875\n",
      "Train Epoch: 139 [7680/225000 (3%)] Loss: 19480.681641\n",
      "Train Epoch: 139 [10176/225000 (5%)] Loss: 19579.035156\n",
      "Train Epoch: 139 [12672/225000 (6%)] Loss: 19810.429688\n",
      "Train Epoch: 139 [15168/225000 (7%)] Loss: 19487.078125\n",
      "Train Epoch: 139 [17664/225000 (8%)] Loss: 19767.921875\n",
      "Train Epoch: 139 [20160/225000 (9%)] Loss: 19615.566406\n",
      "Train Epoch: 139 [22656/225000 (10%)] Loss: 19260.507812\n",
      "Train Epoch: 139 [25152/225000 (11%)] Loss: 19202.480469\n",
      "Train Epoch: 139 [27648/225000 (12%)] Loss: 19287.480469\n",
      "Train Epoch: 139 [30144/225000 (13%)] Loss: 19445.820312\n",
      "Train Epoch: 139 [32640/225000 (15%)] Loss: 19175.708984\n",
      "Train Epoch: 139 [35136/225000 (16%)] Loss: 19523.683594\n",
      "Train Epoch: 139 [37632/225000 (17%)] Loss: 19366.703125\n",
      "Train Epoch: 139 [40128/225000 (18%)] Loss: 18794.832031\n",
      "Train Epoch: 139 [42624/225000 (19%)] Loss: 19502.906250\n",
      "Train Epoch: 139 [45120/225000 (20%)] Loss: 19247.695312\n",
      "Train Epoch: 139 [47616/225000 (21%)] Loss: 18913.531250\n",
      "Train Epoch: 139 [50112/225000 (22%)] Loss: 19255.451172\n",
      "Train Epoch: 139 [52608/225000 (23%)] Loss: 19371.324219\n",
      "Train Epoch: 139 [55104/225000 (24%)] Loss: 19490.695312\n",
      "Train Epoch: 139 [57600/225000 (26%)] Loss: 19024.941406\n",
      "Train Epoch: 139 [60096/225000 (27%)] Loss: 19575.707031\n",
      "Train Epoch: 139 [62592/225000 (28%)] Loss: 19089.898438\n",
      "Train Epoch: 139 [65088/225000 (29%)] Loss: 19278.828125\n",
      "Train Epoch: 139 [67584/225000 (30%)] Loss: 19607.974609\n",
      "Train Epoch: 139 [70080/225000 (31%)] Loss: 18941.296875\n",
      "Train Epoch: 139 [72576/225000 (32%)] Loss: 19561.455078\n",
      "Train Epoch: 139 [75072/225000 (33%)] Loss: 19163.414062\n",
      "Train Epoch: 139 [77568/225000 (34%)] Loss: 19349.144531\n",
      "Train Epoch: 139 [80064/225000 (36%)] Loss: 19426.765625\n",
      "Train Epoch: 139 [82560/225000 (37%)] Loss: 19510.298828\n",
      "Train Epoch: 139 [85056/225000 (38%)] Loss: 19317.751953\n",
      "Train Epoch: 139 [87552/225000 (39%)] Loss: 19148.507812\n",
      "Train Epoch: 139 [90048/225000 (40%)] Loss: 19137.500000\n",
      "Train Epoch: 139 [92544/225000 (41%)] Loss: 19600.867188\n",
      "Train Epoch: 139 [95040/225000 (42%)] Loss: 19489.611328\n",
      "Train Epoch: 139 [97536/225000 (43%)] Loss: 19091.218750\n",
      "Train Epoch: 139 [100032/225000 (44%)] Loss: 19391.683594\n",
      "Train Epoch: 139 [102528/225000 (46%)] Loss: 19195.460938\n",
      "Train Epoch: 139 [105024/225000 (47%)] Loss: 19220.585938\n",
      "Train Epoch: 139 [107520/225000 (48%)] Loss: 18999.300781\n",
      "Train Epoch: 139 [110016/225000 (49%)] Loss: 19239.527344\n",
      "Train Epoch: 139 [112512/225000 (50%)] Loss: 19231.099609\n",
      "Train Epoch: 139 [115008/225000 (51%)] Loss: 19457.582031\n",
      "Train Epoch: 139 [117504/225000 (52%)] Loss: 18787.828125\n",
      "Train Epoch: 139 [120000/225000 (53%)] Loss: 19348.626953\n",
      "Train Epoch: 139 [122496/225000 (54%)] Loss: 19340.601562\n",
      "Train Epoch: 139 [124992/225000 (56%)] Loss: 19556.816406\n",
      "Train Epoch: 139 [127488/225000 (57%)] Loss: 19068.613281\n",
      "Train Epoch: 139 [129984/225000 (58%)] Loss: 18989.984375\n",
      "Train Epoch: 139 [132480/225000 (59%)] Loss: 19877.019531\n",
      "Train Epoch: 139 [134976/225000 (60%)] Loss: 19158.720703\n",
      "Train Epoch: 139 [137472/225000 (61%)] Loss: 19310.484375\n",
      "Train Epoch: 139 [139968/225000 (62%)] Loss: 19509.285156\n",
      "Train Epoch: 139 [142464/225000 (63%)] Loss: 18963.054688\n",
      "Train Epoch: 139 [144960/225000 (64%)] Loss: 18652.816406\n",
      "Train Epoch: 139 [147456/225000 (66%)] Loss: 19564.796875\n",
      "Train Epoch: 139 [149952/225000 (67%)] Loss: 19367.779297\n",
      "Train Epoch: 139 [152448/225000 (68%)] Loss: 18972.406250\n",
      "Train Epoch: 139 [154944/225000 (69%)] Loss: 19186.351562\n",
      "Train Epoch: 139 [157440/225000 (70%)] Loss: 19399.980469\n",
      "Train Epoch: 139 [159936/225000 (71%)] Loss: 19155.714844\n",
      "Train Epoch: 139 [162432/225000 (72%)] Loss: 18948.675781\n",
      "Train Epoch: 139 [164928/225000 (73%)] Loss: 19501.234375\n",
      "Train Epoch: 139 [167424/225000 (74%)] Loss: 18908.750000\n",
      "Train Epoch: 139 [169920/225000 (76%)] Loss: 19146.560547\n",
      "Train Epoch: 139 [172416/225000 (77%)] Loss: 18740.113281\n",
      "Train Epoch: 139 [174912/225000 (78%)] Loss: 19521.970703\n",
      "Train Epoch: 139 [177408/225000 (79%)] Loss: 19708.703125\n",
      "Train Epoch: 139 [179904/225000 (80%)] Loss: 19059.441406\n",
      "Train Epoch: 139 [182400/225000 (81%)] Loss: 19494.753906\n",
      "Train Epoch: 139 [184896/225000 (82%)] Loss: 18994.099609\n",
      "Train Epoch: 139 [187392/225000 (83%)] Loss: 19350.876953\n",
      "Train Epoch: 139 [189888/225000 (84%)] Loss: 19264.042969\n",
      "Train Epoch: 139 [192384/225000 (86%)] Loss: 18896.558594\n",
      "Train Epoch: 139 [194880/225000 (87%)] Loss: 18925.898438\n",
      "Train Epoch: 139 [197376/225000 (88%)] Loss: 18854.207031\n",
      "Train Epoch: 139 [199872/225000 (89%)] Loss: 19235.281250\n",
      "Train Epoch: 139 [202368/225000 (90%)] Loss: 19250.691406\n",
      "Train Epoch: 139 [204864/225000 (91%)] Loss: 18880.845703\n",
      "Train Epoch: 139 [207360/225000 (92%)] Loss: 19091.816406\n",
      "Train Epoch: 139 [209856/225000 (93%)] Loss: 19343.832031\n",
      "Train Epoch: 139 [212352/225000 (94%)] Loss: 19247.894531\n",
      "Train Epoch: 139 [214848/225000 (95%)] Loss: 19456.818359\n",
      "Train Epoch: 139 [217344/225000 (97%)] Loss: 19157.976562\n",
      "Train Epoch: 139 [219840/225000 (98%)] Loss: 19280.917969\n",
      "Train Epoch: 139 [222336/225000 (99%)] Loss: 19447.210938\n",
      "Train Epoch: 139 [224832/225000 (100%)] Loss: 19798.390625\n",
      "    epoch          : 139\n",
      "    loss           : 19304.090366194272\n",
      "    val_loss       : 19235.868312074937\n",
      "Train Epoch: 140 [192/225000 (0%)] Loss: 19503.744141\n",
      "Train Epoch: 140 [2688/225000 (1%)] Loss: 19167.148438\n",
      "Train Epoch: 140 [5184/225000 (2%)] Loss: 18892.382812\n",
      "Train Epoch: 140 [7680/225000 (3%)] Loss: 19046.169922\n",
      "Train Epoch: 140 [10176/225000 (5%)] Loss: 19219.429688\n",
      "Train Epoch: 140 [12672/225000 (6%)] Loss: 19307.541016\n",
      "Train Epoch: 140 [15168/225000 (7%)] Loss: 19168.800781\n",
      "Train Epoch: 140 [17664/225000 (8%)] Loss: 19249.835938\n",
      "Train Epoch: 140 [20160/225000 (9%)] Loss: 19147.636719\n",
      "Train Epoch: 140 [22656/225000 (10%)] Loss: 19254.185547\n",
      "Train Epoch: 140 [25152/225000 (11%)] Loss: 18916.371094\n",
      "Train Epoch: 140 [27648/225000 (12%)] Loss: 19154.603516\n",
      "Train Epoch: 140 [30144/225000 (13%)] Loss: 19099.007812\n",
      "Train Epoch: 140 [32640/225000 (15%)] Loss: 19003.314453\n",
      "Train Epoch: 140 [35136/225000 (16%)] Loss: 19577.708984\n",
      "Train Epoch: 140 [37632/225000 (17%)] Loss: 19448.121094\n",
      "Train Epoch: 140 [40128/225000 (18%)] Loss: 19529.816406\n",
      "Train Epoch: 140 [42624/225000 (19%)] Loss: 19316.949219\n",
      "Train Epoch: 140 [45120/225000 (20%)] Loss: 19497.544922\n",
      "Train Epoch: 140 [47616/225000 (21%)] Loss: 19116.126953\n",
      "Train Epoch: 140 [50112/225000 (22%)] Loss: 19498.359375\n",
      "Train Epoch: 140 [52608/225000 (23%)] Loss: 19270.871094\n",
      "Train Epoch: 140 [55104/225000 (24%)] Loss: 19508.765625\n",
      "Train Epoch: 140 [57600/225000 (26%)] Loss: 19699.027344\n",
      "Train Epoch: 140 [60096/225000 (27%)] Loss: 19363.687500\n",
      "Train Epoch: 140 [62592/225000 (28%)] Loss: 19405.376953\n",
      "Train Epoch: 140 [65088/225000 (29%)] Loss: 19144.183594\n",
      "Train Epoch: 140 [67584/225000 (30%)] Loss: 19332.244141\n",
      "Train Epoch: 140 [70080/225000 (31%)] Loss: 19540.453125\n",
      "Train Epoch: 140 [72576/225000 (32%)] Loss: 19400.050781\n",
      "Train Epoch: 140 [75072/225000 (33%)] Loss: 19497.847656\n",
      "Train Epoch: 140 [77568/225000 (34%)] Loss: 19180.287109\n",
      "Train Epoch: 140 [80064/225000 (36%)] Loss: 19318.320312\n",
      "Train Epoch: 140 [82560/225000 (37%)] Loss: 18812.501953\n",
      "Train Epoch: 140 [85056/225000 (38%)] Loss: 19211.707031\n",
      "Train Epoch: 140 [87552/225000 (39%)] Loss: 19409.326172\n",
      "Train Epoch: 140 [90048/225000 (40%)] Loss: 19090.025391\n",
      "Train Epoch: 140 [92544/225000 (41%)] Loss: 19562.730469\n",
      "Train Epoch: 140 [95040/225000 (42%)] Loss: 19345.433594\n",
      "Train Epoch: 140 [97536/225000 (43%)] Loss: 19644.148438\n",
      "Train Epoch: 140 [100032/225000 (44%)] Loss: 19383.806641\n",
      "Train Epoch: 140 [102528/225000 (46%)] Loss: 19134.113281\n",
      "Train Epoch: 140 [105024/225000 (47%)] Loss: 19550.406250\n",
      "Train Epoch: 140 [107520/225000 (48%)] Loss: 19259.496094\n",
      "Train Epoch: 140 [110016/225000 (49%)] Loss: 19605.308594\n",
      "Train Epoch: 140 [112512/225000 (50%)] Loss: 19408.328125\n",
      "Train Epoch: 140 [115008/225000 (51%)] Loss: 19317.406250\n",
      "Train Epoch: 140 [117504/225000 (52%)] Loss: 19554.875000\n",
      "Train Epoch: 140 [120000/225000 (53%)] Loss: 18993.691406\n",
      "Train Epoch: 140 [122496/225000 (54%)] Loss: 19628.324219\n",
      "Train Epoch: 140 [124992/225000 (56%)] Loss: 19252.656250\n",
      "Train Epoch: 140 [127488/225000 (57%)] Loss: 19291.677734\n",
      "Train Epoch: 140 [129984/225000 (58%)] Loss: 19203.658203\n",
      "Train Epoch: 140 [132480/225000 (59%)] Loss: 19243.570312\n",
      "Train Epoch: 140 [134976/225000 (60%)] Loss: 19024.542969\n",
      "Train Epoch: 140 [137472/225000 (61%)] Loss: 19322.625000\n",
      "Train Epoch: 140 [139968/225000 (62%)] Loss: 18946.865234\n",
      "Train Epoch: 140 [142464/225000 (63%)] Loss: 18660.585938\n",
      "Train Epoch: 140 [144960/225000 (64%)] Loss: 19452.027344\n",
      "Train Epoch: 140 [147456/225000 (66%)] Loss: 19428.730469\n",
      "Train Epoch: 140 [149952/225000 (67%)] Loss: 19176.511719\n",
      "Train Epoch: 140 [152448/225000 (68%)] Loss: 18844.935547\n",
      "Train Epoch: 140 [154944/225000 (69%)] Loss: 19432.964844\n",
      "Train Epoch: 140 [157440/225000 (70%)] Loss: 19440.894531\n",
      "Train Epoch: 140 [159936/225000 (71%)] Loss: 19650.666016\n",
      "Train Epoch: 140 [162432/225000 (72%)] Loss: 19416.076172\n",
      "Train Epoch: 140 [164928/225000 (73%)] Loss: 19628.359375\n",
      "Train Epoch: 140 [167424/225000 (74%)] Loss: 19548.015625\n",
      "Train Epoch: 140 [169920/225000 (76%)] Loss: 19492.550781\n",
      "Train Epoch: 140 [172416/225000 (77%)] Loss: 19399.330078\n",
      "Train Epoch: 140 [174912/225000 (78%)] Loss: 19024.792969\n",
      "Train Epoch: 140 [177408/225000 (79%)] Loss: 19257.525391\n",
      "Train Epoch: 140 [179904/225000 (80%)] Loss: 19265.818359\n",
      "Train Epoch: 140 [182400/225000 (81%)] Loss: 19140.347656\n",
      "Train Epoch: 140 [184896/225000 (82%)] Loss: 23402.101562\n",
      "Train Epoch: 140 [187392/225000 (83%)] Loss: 19509.156250\n",
      "Train Epoch: 140 [189888/225000 (84%)] Loss: 19144.070312\n",
      "Train Epoch: 140 [192384/225000 (86%)] Loss: 18973.947266\n",
      "Train Epoch: 140 [194880/225000 (87%)] Loss: 19316.955078\n",
      "Train Epoch: 140 [197376/225000 (88%)] Loss: 19236.328125\n",
      "Train Epoch: 140 [199872/225000 (89%)] Loss: 18814.105469\n",
      "Train Epoch: 140 [202368/225000 (90%)] Loss: 19212.492188\n",
      "Train Epoch: 140 [204864/225000 (91%)] Loss: 19071.867188\n",
      "Train Epoch: 140 [207360/225000 (92%)] Loss: 19532.906250\n",
      "Train Epoch: 140 [209856/225000 (93%)] Loss: 18971.660156\n",
      "Train Epoch: 140 [212352/225000 (94%)] Loss: 19229.027344\n",
      "Train Epoch: 140 [214848/225000 (95%)] Loss: 19461.113281\n",
      "Train Epoch: 140 [217344/225000 (97%)] Loss: 19303.957031\n",
      "Train Epoch: 140 [219840/225000 (98%)] Loss: 19380.089844\n",
      "Train Epoch: 140 [222336/225000 (99%)] Loss: 19271.421875\n",
      "Train Epoch: 140 [224832/225000 (100%)] Loss: 19362.833984\n",
      "    epoch          : 140\n",
      "    loss           : 19295.764090163717\n",
      "    val_loss       : 19243.425047549583\n",
      "Train Epoch: 141 [192/225000 (0%)] Loss: 18973.044922\n",
      "Train Epoch: 141 [2688/225000 (1%)] Loss: 19740.630859\n",
      "Train Epoch: 141 [5184/225000 (2%)] Loss: 19458.169922\n",
      "Train Epoch: 141 [7680/225000 (3%)] Loss: 19554.296875\n",
      "Train Epoch: 141 [10176/225000 (5%)] Loss: 19722.671875\n",
      "Train Epoch: 141 [12672/225000 (6%)] Loss: 19294.724609\n",
      "Train Epoch: 141 [15168/225000 (7%)] Loss: 19094.587891\n",
      "Train Epoch: 141 [17664/225000 (8%)] Loss: 19487.111328\n",
      "Train Epoch: 141 [20160/225000 (9%)] Loss: 19382.009766\n",
      "Train Epoch: 141 [22656/225000 (10%)] Loss: 19711.648438\n",
      "Train Epoch: 141 [25152/225000 (11%)] Loss: 19344.742188\n",
      "Train Epoch: 141 [27648/225000 (12%)] Loss: 18738.042969\n",
      "Train Epoch: 141 [30144/225000 (13%)] Loss: 18990.378906\n",
      "Train Epoch: 141 [32640/225000 (15%)] Loss: 19094.863281\n",
      "Train Epoch: 141 [35136/225000 (16%)] Loss: 18984.132812\n",
      "Train Epoch: 141 [37632/225000 (17%)] Loss: 19568.986328\n",
      "Train Epoch: 141 [40128/225000 (18%)] Loss: 19645.021484\n",
      "Train Epoch: 141 [42624/225000 (19%)] Loss: 19064.814453\n",
      "Train Epoch: 141 [45120/225000 (20%)] Loss: 19187.683594\n",
      "Train Epoch: 141 [47616/225000 (21%)] Loss: 19156.115234\n",
      "Train Epoch: 141 [50112/225000 (22%)] Loss: 19798.480469\n",
      "Train Epoch: 141 [52608/225000 (23%)] Loss: 19416.472656\n",
      "Train Epoch: 141 [55104/225000 (24%)] Loss: 19441.220703\n",
      "Train Epoch: 141 [57600/225000 (26%)] Loss: 19181.257812\n",
      "Train Epoch: 141 [60096/225000 (27%)] Loss: 19165.587891\n",
      "Train Epoch: 141 [62592/225000 (28%)] Loss: 19815.185547\n",
      "Train Epoch: 141 [65088/225000 (29%)] Loss: 19836.816406\n",
      "Train Epoch: 141 [67584/225000 (30%)] Loss: 19451.148438\n",
      "Train Epoch: 141 [70080/225000 (31%)] Loss: 18805.906250\n",
      "Train Epoch: 141 [72576/225000 (32%)] Loss: 19060.320312\n",
      "Train Epoch: 141 [75072/225000 (33%)] Loss: 19689.656250\n",
      "Train Epoch: 141 [77568/225000 (34%)] Loss: 19688.968750\n",
      "Train Epoch: 141 [80064/225000 (36%)] Loss: 19527.585938\n",
      "Train Epoch: 141 [82560/225000 (37%)] Loss: 19406.269531\n",
      "Train Epoch: 141 [85056/225000 (38%)] Loss: 19237.869141\n",
      "Train Epoch: 141 [87552/225000 (39%)] Loss: 18634.277344\n",
      "Train Epoch: 141 [90048/225000 (40%)] Loss: 18852.039062\n",
      "Train Epoch: 141 [92544/225000 (41%)] Loss: 18963.333984\n",
      "Train Epoch: 141 [95040/225000 (42%)] Loss: 19588.558594\n",
      "Train Epoch: 141 [97536/225000 (43%)] Loss: 19728.699219\n",
      "Train Epoch: 141 [100032/225000 (44%)] Loss: 19144.925781\n",
      "Train Epoch: 141 [102528/225000 (46%)] Loss: 19076.140625\n",
      "Train Epoch: 141 [105024/225000 (47%)] Loss: 19412.537109\n",
      "Train Epoch: 141 [107520/225000 (48%)] Loss: 19126.576172\n",
      "Train Epoch: 141 [110016/225000 (49%)] Loss: 19508.203125\n",
      "Train Epoch: 141 [112512/225000 (50%)] Loss: 18625.578125\n",
      "Train Epoch: 141 [115008/225000 (51%)] Loss: 19101.078125\n",
      "Train Epoch: 141 [117504/225000 (52%)] Loss: 19398.894531\n",
      "Train Epoch: 141 [120000/225000 (53%)] Loss: 19308.517578\n",
      "Train Epoch: 141 [122496/225000 (54%)] Loss: 19209.062500\n",
      "Train Epoch: 141 [124992/225000 (56%)] Loss: 19386.976562\n",
      "Train Epoch: 141 [127488/225000 (57%)] Loss: 18951.546875\n",
      "Train Epoch: 141 [129984/225000 (58%)] Loss: 19397.753906\n",
      "Train Epoch: 141 [132480/225000 (59%)] Loss: 19501.402344\n",
      "Train Epoch: 141 [134976/225000 (60%)] Loss: 19379.716797\n",
      "Train Epoch: 141 [137472/225000 (61%)] Loss: 19246.382812\n",
      "Train Epoch: 141 [139968/225000 (62%)] Loss: 19226.498047\n",
      "Train Epoch: 141 [142464/225000 (63%)] Loss: 19300.986328\n",
      "Train Epoch: 141 [144960/225000 (64%)] Loss: 18924.363281\n",
      "Train Epoch: 141 [147456/225000 (66%)] Loss: 19477.285156\n",
      "Train Epoch: 141 [149952/225000 (67%)] Loss: 19429.130859\n",
      "Train Epoch: 141 [152448/225000 (68%)] Loss: 20063.167969\n",
      "Train Epoch: 141 [154944/225000 (69%)] Loss: 19180.601562\n",
      "Train Epoch: 141 [157440/225000 (70%)] Loss: 19350.283203\n",
      "Train Epoch: 141 [159936/225000 (71%)] Loss: 19503.947266\n",
      "Train Epoch: 141 [162432/225000 (72%)] Loss: 19149.447266\n",
      "Train Epoch: 141 [164928/225000 (73%)] Loss: 19046.511719\n",
      "Train Epoch: 141 [167424/225000 (74%)] Loss: 18705.691406\n",
      "Train Epoch: 141 [169920/225000 (76%)] Loss: 19000.931641\n",
      "Train Epoch: 141 [172416/225000 (77%)] Loss: 19301.824219\n",
      "Train Epoch: 141 [174912/225000 (78%)] Loss: 19039.792969\n",
      "Train Epoch: 141 [177408/225000 (79%)] Loss: 19435.000000\n",
      "Train Epoch: 141 [179904/225000 (80%)] Loss: 19169.460938\n",
      "Train Epoch: 141 [182400/225000 (81%)] Loss: 18982.578125\n",
      "Train Epoch: 141 [184896/225000 (82%)] Loss: 19397.001953\n",
      "Train Epoch: 141 [187392/225000 (83%)] Loss: 19175.896484\n",
      "Train Epoch: 141 [189888/225000 (84%)] Loss: 19661.736328\n",
      "Train Epoch: 141 [192384/225000 (86%)] Loss: 19869.046875\n",
      "Train Epoch: 141 [194880/225000 (87%)] Loss: 19446.535156\n",
      "Train Epoch: 141 [197376/225000 (88%)] Loss: 19738.388672\n",
      "Train Epoch: 141 [199872/225000 (89%)] Loss: 19132.726562\n",
      "Train Epoch: 141 [202368/225000 (90%)] Loss: 19378.871094\n",
      "Train Epoch: 141 [204864/225000 (91%)] Loss: 19782.369141\n",
      "Train Epoch: 141 [207360/225000 (92%)] Loss: 19135.515625\n",
      "Train Epoch: 141 [209856/225000 (93%)] Loss: 18958.148438\n",
      "Train Epoch: 141 [212352/225000 (94%)] Loss: 19971.087891\n",
      "Train Epoch: 141 [214848/225000 (95%)] Loss: 19407.972656\n",
      "Train Epoch: 141 [217344/225000 (97%)] Loss: 19509.230469\n",
      "Train Epoch: 141 [219840/225000 (98%)] Loss: 19562.859375\n",
      "Train Epoch: 141 [222336/225000 (99%)] Loss: 19323.695312\n",
      "Train Epoch: 141 [224832/225000 (100%)] Loss: 19135.267578\n",
      "    epoch          : 141\n",
      "    loss           : 19293.20540309034\n",
      "    val_loss       : 19191.084313657448\n",
      "Train Epoch: 142 [192/225000 (0%)] Loss: 19482.718750\n",
      "Train Epoch: 142 [2688/225000 (1%)] Loss: 19877.574219\n",
      "Train Epoch: 142 [5184/225000 (2%)] Loss: 19018.496094\n",
      "Train Epoch: 142 [7680/225000 (3%)] Loss: 19162.533203\n",
      "Train Epoch: 142 [10176/225000 (5%)] Loss: 19383.371094\n",
      "Train Epoch: 142 [12672/225000 (6%)] Loss: 19300.929688\n",
      "Train Epoch: 142 [15168/225000 (7%)] Loss: 18651.007812\n",
      "Train Epoch: 142 [17664/225000 (8%)] Loss: 19059.878906\n",
      "Train Epoch: 142 [20160/225000 (9%)] Loss: 19446.035156\n",
      "Train Epoch: 142 [22656/225000 (10%)] Loss: 19231.011719\n",
      "Train Epoch: 142 [25152/225000 (11%)] Loss: 19080.937500\n",
      "Train Epoch: 142 [27648/225000 (12%)] Loss: 18897.964844\n",
      "Train Epoch: 142 [30144/225000 (13%)] Loss: 19768.671875\n",
      "Train Epoch: 142 [32640/225000 (15%)] Loss: 19297.121094\n",
      "Train Epoch: 142 [35136/225000 (16%)] Loss: 19380.074219\n",
      "Train Epoch: 142 [37632/225000 (17%)] Loss: 19303.605469\n",
      "Train Epoch: 142 [40128/225000 (18%)] Loss: 19195.353516\n",
      "Train Epoch: 142 [42624/225000 (19%)] Loss: 19854.136719\n",
      "Train Epoch: 142 [45120/225000 (20%)] Loss: 18921.328125\n",
      "Train Epoch: 142 [47616/225000 (21%)] Loss: 19143.171875\n",
      "Train Epoch: 142 [50112/225000 (22%)] Loss: 19283.480469\n",
      "Train Epoch: 142 [52608/225000 (23%)] Loss: 18935.070312\n",
      "Train Epoch: 142 [55104/225000 (24%)] Loss: 18706.500000\n",
      "Train Epoch: 142 [57600/225000 (26%)] Loss: 19165.498047\n",
      "Train Epoch: 142 [60096/225000 (27%)] Loss: 19168.544922\n",
      "Train Epoch: 142 [62592/225000 (28%)] Loss: 18992.519531\n",
      "Train Epoch: 142 [65088/225000 (29%)] Loss: 19514.500000\n",
      "Train Epoch: 142 [67584/225000 (30%)] Loss: 19855.697266\n",
      "Train Epoch: 142 [70080/225000 (31%)] Loss: 19313.332031\n",
      "Train Epoch: 142 [72576/225000 (32%)] Loss: 19347.136719\n",
      "Train Epoch: 142 [75072/225000 (33%)] Loss: 19385.630859\n",
      "Train Epoch: 142 [77568/225000 (34%)] Loss: 19294.437500\n",
      "Train Epoch: 142 [80064/225000 (36%)] Loss: 19782.029297\n",
      "Train Epoch: 142 [82560/225000 (37%)] Loss: 19083.210938\n",
      "Train Epoch: 142 [85056/225000 (38%)] Loss: 19356.925781\n",
      "Train Epoch: 142 [87552/225000 (39%)] Loss: 19150.476562\n",
      "Train Epoch: 142 [90048/225000 (40%)] Loss: 19256.765625\n",
      "Train Epoch: 142 [92544/225000 (41%)] Loss: 19684.468750\n",
      "Train Epoch: 142 [95040/225000 (42%)] Loss: 18852.046875\n",
      "Train Epoch: 142 [97536/225000 (43%)] Loss: 19252.269531\n",
      "Train Epoch: 142 [100032/225000 (44%)] Loss: 19201.976562\n",
      "Train Epoch: 142 [102528/225000 (46%)] Loss: 19009.636719\n",
      "Train Epoch: 142 [105024/225000 (47%)] Loss: 19335.978516\n",
      "Train Epoch: 142 [107520/225000 (48%)] Loss: 19087.035156\n",
      "Train Epoch: 142 [110016/225000 (49%)] Loss: 18576.019531\n",
      "Train Epoch: 142 [112512/225000 (50%)] Loss: 19038.583984\n",
      "Train Epoch: 142 [115008/225000 (51%)] Loss: 19497.785156\n",
      "Train Epoch: 142 [117504/225000 (52%)] Loss: 19525.816406\n",
      "Train Epoch: 142 [120000/225000 (53%)] Loss: 19531.257812\n",
      "Train Epoch: 142 [122496/225000 (54%)] Loss: 18871.691406\n",
      "Train Epoch: 142 [124992/225000 (56%)] Loss: 19373.808594\n",
      "Train Epoch: 142 [127488/225000 (57%)] Loss: 19576.046875\n",
      "Train Epoch: 142 [129984/225000 (58%)] Loss: 19300.666016\n",
      "Train Epoch: 142 [132480/225000 (59%)] Loss: 19095.408203\n",
      "Train Epoch: 142 [134976/225000 (60%)] Loss: 19338.664062\n",
      "Train Epoch: 142 [137472/225000 (61%)] Loss: 19559.111328\n",
      "Train Epoch: 142 [139968/225000 (62%)] Loss: 18921.773438\n",
      "Train Epoch: 142 [142464/225000 (63%)] Loss: 19335.085938\n",
      "Train Epoch: 142 [144960/225000 (64%)] Loss: 19372.634766\n",
      "Train Epoch: 142 [147456/225000 (66%)] Loss: 19754.818359\n",
      "Train Epoch: 142 [149952/225000 (67%)] Loss: 19371.648438\n",
      "Train Epoch: 142 [152448/225000 (68%)] Loss: 19346.335938\n",
      "Train Epoch: 142 [154944/225000 (69%)] Loss: 19258.316406\n",
      "Train Epoch: 142 [157440/225000 (70%)] Loss: 18910.611328\n",
      "Train Epoch: 142 [159936/225000 (71%)] Loss: 19306.839844\n",
      "Train Epoch: 142 [162432/225000 (72%)] Loss: 19053.621094\n",
      "Train Epoch: 142 [164928/225000 (73%)] Loss: 19377.130859\n",
      "Train Epoch: 142 [167424/225000 (74%)] Loss: 19339.667969\n",
      "Train Epoch: 142 [169920/225000 (76%)] Loss: 19771.757812\n",
      "Train Epoch: 142 [172416/225000 (77%)] Loss: 19795.273438\n",
      "Train Epoch: 142 [174912/225000 (78%)] Loss: 18613.058594\n",
      "Train Epoch: 142 [177408/225000 (79%)] Loss: 18869.734375\n",
      "Train Epoch: 142 [179904/225000 (80%)] Loss: 19048.638672\n",
      "Train Epoch: 142 [182400/225000 (81%)] Loss: 19466.566406\n",
      "Train Epoch: 142 [184896/225000 (82%)] Loss: 19465.687500\n",
      "Train Epoch: 142 [187392/225000 (83%)] Loss: 19155.828125\n",
      "Train Epoch: 142 [189888/225000 (84%)] Loss: 19218.142578\n",
      "Train Epoch: 142 [192384/225000 (86%)] Loss: 19289.261719\n",
      "Train Epoch: 142 [194880/225000 (87%)] Loss: 19375.423828\n",
      "Train Epoch: 142 [197376/225000 (88%)] Loss: 19072.207031\n",
      "Train Epoch: 142 [199872/225000 (89%)] Loss: 19420.679688\n",
      "Train Epoch: 142 [202368/225000 (90%)] Loss: 19295.964844\n",
      "Train Epoch: 142 [204864/225000 (91%)] Loss: 19055.765625\n",
      "Train Epoch: 142 [207360/225000 (92%)] Loss: 18776.750000\n",
      "Train Epoch: 142 [209856/225000 (93%)] Loss: 19204.492188\n",
      "Train Epoch: 142 [212352/225000 (94%)] Loss: 19640.843750\n",
      "Train Epoch: 142 [214848/225000 (95%)] Loss: 19711.224609\n",
      "Train Epoch: 142 [217344/225000 (97%)] Loss: 19722.351562\n",
      "Train Epoch: 142 [219840/225000 (98%)] Loss: 19395.085938\n",
      "Train Epoch: 142 [222336/225000 (99%)] Loss: 19404.480469\n",
      "Train Epoch: 142 [224832/225000 (100%)] Loss: 19492.716797\n",
      "    epoch          : 142\n",
      "    loss           : 19287.821279063566\n",
      "    val_loss       : 19201.209771938906\n",
      "Train Epoch: 143 [192/225000 (0%)] Loss: 19037.707031\n",
      "Train Epoch: 143 [2688/225000 (1%)] Loss: 19717.894531\n",
      "Train Epoch: 143 [5184/225000 (2%)] Loss: 19564.238281\n",
      "Train Epoch: 143 [7680/225000 (3%)] Loss: 19361.269531\n",
      "Train Epoch: 143 [10176/225000 (5%)] Loss: 19051.925781\n",
      "Train Epoch: 143 [12672/225000 (6%)] Loss: 19521.261719\n",
      "Train Epoch: 143 [15168/225000 (7%)] Loss: 19559.333984\n",
      "Train Epoch: 143 [17664/225000 (8%)] Loss: 19229.382812\n",
      "Train Epoch: 143 [20160/225000 (9%)] Loss: 18952.855469\n",
      "Train Epoch: 143 [22656/225000 (10%)] Loss: 19517.072266\n",
      "Train Epoch: 143 [25152/225000 (11%)] Loss: 19152.949219\n",
      "Train Epoch: 143 [27648/225000 (12%)] Loss: 19670.109375\n",
      "Train Epoch: 143 [30144/225000 (13%)] Loss: 18917.109375\n",
      "Train Epoch: 143 [32640/225000 (15%)] Loss: 19024.699219\n",
      "Train Epoch: 143 [35136/225000 (16%)] Loss: 19018.171875\n",
      "Train Epoch: 143 [37632/225000 (17%)] Loss: 19572.160156\n",
      "Train Epoch: 143 [40128/225000 (18%)] Loss: 19359.078125\n",
      "Train Epoch: 143 [42624/225000 (19%)] Loss: 19149.445312\n",
      "Train Epoch: 143 [45120/225000 (20%)] Loss: 19914.785156\n",
      "Train Epoch: 143 [47616/225000 (21%)] Loss: 19146.718750\n",
      "Train Epoch: 143 [50112/225000 (22%)] Loss: 19099.429688\n",
      "Train Epoch: 143 [52608/225000 (23%)] Loss: 20139.175781\n",
      "Train Epoch: 143 [55104/225000 (24%)] Loss: 19063.707031\n",
      "Train Epoch: 143 [57600/225000 (26%)] Loss: 19468.410156\n",
      "Train Epoch: 143 [60096/225000 (27%)] Loss: 18704.710938\n",
      "Train Epoch: 143 [62592/225000 (28%)] Loss: 19370.134766\n",
      "Train Epoch: 143 [65088/225000 (29%)] Loss: 19165.718750\n",
      "Train Epoch: 143 [67584/225000 (30%)] Loss: 19174.884766\n",
      "Train Epoch: 143 [70080/225000 (31%)] Loss: 18944.078125\n",
      "Train Epoch: 143 [72576/225000 (32%)] Loss: 18991.839844\n",
      "Train Epoch: 143 [75072/225000 (33%)] Loss: 19051.318359\n",
      "Train Epoch: 143 [77568/225000 (34%)] Loss: 19902.511719\n",
      "Train Epoch: 143 [80064/225000 (36%)] Loss: 19377.087891\n",
      "Train Epoch: 143 [82560/225000 (37%)] Loss: 19353.708984\n",
      "Train Epoch: 143 [85056/225000 (38%)] Loss: 19324.994141\n",
      "Train Epoch: 143 [87552/225000 (39%)] Loss: 19327.355469\n",
      "Train Epoch: 143 [90048/225000 (40%)] Loss: 19307.804688\n",
      "Train Epoch: 143 [92544/225000 (41%)] Loss: 18912.113281\n",
      "Train Epoch: 143 [95040/225000 (42%)] Loss: 19392.769531\n",
      "Train Epoch: 143 [97536/225000 (43%)] Loss: 19659.085938\n",
      "Train Epoch: 143 [100032/225000 (44%)] Loss: 19485.539062\n",
      "Train Epoch: 143 [102528/225000 (46%)] Loss: 18847.097656\n",
      "Train Epoch: 143 [105024/225000 (47%)] Loss: 19247.003906\n",
      "Train Epoch: 143 [107520/225000 (48%)] Loss: 19156.988281\n",
      "Train Epoch: 143 [110016/225000 (49%)] Loss: 19378.376953\n",
      "Train Epoch: 143 [112512/225000 (50%)] Loss: 19363.425781\n",
      "Train Epoch: 143 [115008/225000 (51%)] Loss: 18951.976562\n",
      "Train Epoch: 143 [117504/225000 (52%)] Loss: 19479.537109\n",
      "Train Epoch: 143 [120000/225000 (53%)] Loss: 19164.109375\n",
      "Train Epoch: 143 [122496/225000 (54%)] Loss: 18909.660156\n",
      "Train Epoch: 143 [124992/225000 (56%)] Loss: 19106.908203\n",
      "Train Epoch: 143 [127488/225000 (57%)] Loss: 19358.773438\n",
      "Train Epoch: 143 [129984/225000 (58%)] Loss: 19111.292969\n",
      "Train Epoch: 143 [132480/225000 (59%)] Loss: 19374.390625\n",
      "Train Epoch: 143 [134976/225000 (60%)] Loss: 19404.007812\n",
      "Train Epoch: 143 [137472/225000 (61%)] Loss: 19363.074219\n",
      "Train Epoch: 143 [139968/225000 (62%)] Loss: 19682.722656\n",
      "Train Epoch: 143 [142464/225000 (63%)] Loss: 19147.300781\n",
      "Train Epoch: 143 [144960/225000 (64%)] Loss: 18841.494141\n",
      "Train Epoch: 143 [147456/225000 (66%)] Loss: 19559.943359\n",
      "Train Epoch: 143 [149952/225000 (67%)] Loss: 19404.468750\n",
      "Train Epoch: 143 [152448/225000 (68%)] Loss: 19504.082031\n",
      "Train Epoch: 143 [154944/225000 (69%)] Loss: 19233.453125\n",
      "Train Epoch: 143 [157440/225000 (70%)] Loss: 19809.015625\n",
      "Train Epoch: 143 [159936/225000 (71%)] Loss: 19513.853516\n",
      "Train Epoch: 143 [162432/225000 (72%)] Loss: 19334.623047\n",
      "Train Epoch: 143 [164928/225000 (73%)] Loss: 19169.970703\n",
      "Train Epoch: 143 [167424/225000 (74%)] Loss: 19343.673828\n",
      "Train Epoch: 143 [169920/225000 (76%)] Loss: 19338.652344\n",
      "Train Epoch: 143 [172416/225000 (77%)] Loss: 18717.304688\n",
      "Train Epoch: 143 [174912/225000 (78%)] Loss: 19104.839844\n",
      "Train Epoch: 143 [177408/225000 (79%)] Loss: 18836.058594\n",
      "Train Epoch: 143 [179904/225000 (80%)] Loss: 19495.451172\n",
      "Train Epoch: 143 [182400/225000 (81%)] Loss: 18832.035156\n",
      "Train Epoch: 143 [184896/225000 (82%)] Loss: 19609.269531\n",
      "Train Epoch: 143 [187392/225000 (83%)] Loss: 19217.978516\n",
      "Train Epoch: 143 [189888/225000 (84%)] Loss: 19388.949219\n",
      "Train Epoch: 143 [192384/225000 (86%)] Loss: 19027.558594\n",
      "Train Epoch: 143 [194880/225000 (87%)] Loss: 18966.457031\n",
      "Train Epoch: 143 [197376/225000 (88%)] Loss: 19126.574219\n",
      "Train Epoch: 143 [199872/225000 (89%)] Loss: 19448.855469\n",
      "Train Epoch: 143 [202368/225000 (90%)] Loss: 19217.701172\n",
      "Train Epoch: 143 [204864/225000 (91%)] Loss: 19393.158203\n",
      "Train Epoch: 143 [207360/225000 (92%)] Loss: 19262.113281\n",
      "Train Epoch: 143 [209856/225000 (93%)] Loss: 19284.355469\n",
      "Train Epoch: 143 [212352/225000 (94%)] Loss: 19233.406250\n",
      "Train Epoch: 143 [214848/225000 (95%)] Loss: 19144.847656\n",
      "Train Epoch: 143 [217344/225000 (97%)] Loss: 19076.660156\n",
      "Train Epoch: 143 [219840/225000 (98%)] Loss: 18862.240234\n",
      "Train Epoch: 143 [222336/225000 (99%)] Loss: 19037.349609\n",
      "Train Epoch: 143 [224832/225000 (100%)] Loss: 19454.822266\n",
      "    epoch          : 143\n",
      "    loss           : 19277.565186380118\n",
      "    val_loss       : 19179.528079650783\n",
      "Train Epoch: 144 [192/225000 (0%)] Loss: 19033.230469\n",
      "Train Epoch: 144 [2688/225000 (1%)] Loss: 19174.994141\n",
      "Train Epoch: 144 [5184/225000 (2%)] Loss: 19478.511719\n",
      "Train Epoch: 144 [7680/225000 (3%)] Loss: 19079.519531\n",
      "Train Epoch: 144 [10176/225000 (5%)] Loss: 19269.687500\n",
      "Train Epoch: 144 [12672/225000 (6%)] Loss: 19242.609375\n",
      "Train Epoch: 144 [15168/225000 (7%)] Loss: 18914.261719\n",
      "Train Epoch: 144 [17664/225000 (8%)] Loss: 19098.142578\n",
      "Train Epoch: 144 [20160/225000 (9%)] Loss: 19193.734375\n",
      "Train Epoch: 144 [22656/225000 (10%)] Loss: 19308.472656\n",
      "Train Epoch: 144 [25152/225000 (11%)] Loss: 19207.179688\n",
      "Train Epoch: 144 [27648/225000 (12%)] Loss: 19243.234375\n",
      "Train Epoch: 144 [30144/225000 (13%)] Loss: 19316.718750\n",
      "Train Epoch: 144 [32640/225000 (15%)] Loss: 19329.076172\n",
      "Train Epoch: 144 [35136/225000 (16%)] Loss: 19459.953125\n",
      "Train Epoch: 144 [37632/225000 (17%)] Loss: 19072.863281\n",
      "Train Epoch: 144 [40128/225000 (18%)] Loss: 19406.158203\n",
      "Train Epoch: 144 [42624/225000 (19%)] Loss: 19116.078125\n",
      "Train Epoch: 144 [45120/225000 (20%)] Loss: 19447.123047\n",
      "Train Epoch: 144 [47616/225000 (21%)] Loss: 19245.576172\n",
      "Train Epoch: 144 [50112/225000 (22%)] Loss: 19302.466797\n",
      "Train Epoch: 144 [52608/225000 (23%)] Loss: 19173.675781\n",
      "Train Epoch: 144 [55104/225000 (24%)] Loss: 19075.984375\n",
      "Train Epoch: 144 [57600/225000 (26%)] Loss: 19312.968750\n",
      "Train Epoch: 144 [60096/225000 (27%)] Loss: 19750.117188\n",
      "Train Epoch: 144 [62592/225000 (28%)] Loss: 19419.886719\n",
      "Train Epoch: 144 [65088/225000 (29%)] Loss: 18999.894531\n",
      "Train Epoch: 144 [67584/225000 (30%)] Loss: 19484.335938\n",
      "Train Epoch: 144 [70080/225000 (31%)] Loss: 19845.378906\n",
      "Train Epoch: 144 [72576/225000 (32%)] Loss: 19329.787109\n",
      "Train Epoch: 144 [75072/225000 (33%)] Loss: 19303.203125\n",
      "Train Epoch: 144 [77568/225000 (34%)] Loss: 19562.796875\n",
      "Train Epoch: 144 [80064/225000 (36%)] Loss: 19574.650391\n",
      "Train Epoch: 144 [82560/225000 (37%)] Loss: 19504.392578\n",
      "Train Epoch: 144 [85056/225000 (38%)] Loss: 18645.734375\n",
      "Train Epoch: 144 [87552/225000 (39%)] Loss: 18979.796875\n",
      "Train Epoch: 144 [90048/225000 (40%)] Loss: 19077.863281\n",
      "Train Epoch: 144 [92544/225000 (41%)] Loss: 19363.599609\n",
      "Train Epoch: 144 [95040/225000 (42%)] Loss: 19333.664062\n",
      "Train Epoch: 144 [97536/225000 (43%)] Loss: 19237.175781\n",
      "Train Epoch: 144 [100032/225000 (44%)] Loss: 19033.921875\n",
      "Train Epoch: 144 [102528/225000 (46%)] Loss: 19428.359375\n",
      "Train Epoch: 144 [105024/225000 (47%)] Loss: 19283.414062\n",
      "Train Epoch: 144 [107520/225000 (48%)] Loss: 19534.972656\n",
      "Train Epoch: 144 [110016/225000 (49%)] Loss: 19258.496094\n",
      "Train Epoch: 144 [112512/225000 (50%)] Loss: 19621.136719\n",
      "Train Epoch: 144 [115008/225000 (51%)] Loss: 19337.892578\n",
      "Train Epoch: 144 [117504/225000 (52%)] Loss: 19412.816406\n",
      "Train Epoch: 144 [120000/225000 (53%)] Loss: 19436.669922\n",
      "Train Epoch: 144 [122496/225000 (54%)] Loss: 19169.933594\n",
      "Train Epoch: 144 [124992/225000 (56%)] Loss: 19537.960938\n",
      "Train Epoch: 144 [127488/225000 (57%)] Loss: 18949.115234\n",
      "Train Epoch: 144 [129984/225000 (58%)] Loss: 19299.882812\n",
      "Train Epoch: 144 [132480/225000 (59%)] Loss: 19421.929688\n",
      "Train Epoch: 144 [134976/225000 (60%)] Loss: 19471.378906\n",
      "Train Epoch: 144 [137472/225000 (61%)] Loss: 19529.027344\n",
      "Train Epoch: 144 [139968/225000 (62%)] Loss: 19623.789062\n",
      "Train Epoch: 144 [142464/225000 (63%)] Loss: 19097.132812\n",
      "Train Epoch: 144 [144960/225000 (64%)] Loss: 19207.390625\n",
      "Train Epoch: 144 [147456/225000 (66%)] Loss: 19512.269531\n",
      "Train Epoch: 144 [149952/225000 (67%)] Loss: 19337.183594\n",
      "Train Epoch: 144 [152448/225000 (68%)] Loss: 19234.878906\n",
      "Train Epoch: 144 [154944/225000 (69%)] Loss: 19629.773438\n",
      "Train Epoch: 144 [157440/225000 (70%)] Loss: 19398.675781\n",
      "Train Epoch: 144 [159936/225000 (71%)] Loss: 19289.230469\n",
      "Train Epoch: 144 [162432/225000 (72%)] Loss: 19417.652344\n",
      "Train Epoch: 144 [164928/225000 (73%)] Loss: 19212.218750\n",
      "Train Epoch: 144 [167424/225000 (74%)] Loss: 19046.644531\n",
      "Train Epoch: 144 [169920/225000 (76%)] Loss: 19589.886719\n",
      "Train Epoch: 144 [172416/225000 (77%)] Loss: 19147.007812\n",
      "Train Epoch: 144 [174912/225000 (78%)] Loss: 18802.804688\n",
      "Train Epoch: 144 [177408/225000 (79%)] Loss: 19311.609375\n",
      "Train Epoch: 144 [179904/225000 (80%)] Loss: 19207.488281\n",
      "Train Epoch: 144 [182400/225000 (81%)] Loss: 19377.976562\n",
      "Train Epoch: 144 [184896/225000 (82%)] Loss: 19261.185547\n",
      "Train Epoch: 144 [187392/225000 (83%)] Loss: 19060.953125\n",
      "Train Epoch: 144 [189888/225000 (84%)] Loss: 18795.851562\n",
      "Train Epoch: 144 [192384/225000 (86%)] Loss: 18961.583984\n",
      "Train Epoch: 144 [194880/225000 (87%)] Loss: 19782.808594\n",
      "Train Epoch: 144 [197376/225000 (88%)] Loss: 19499.078125\n",
      "Train Epoch: 144 [199872/225000 (89%)] Loss: 19382.267578\n",
      "Train Epoch: 144 [202368/225000 (90%)] Loss: 19754.154297\n",
      "Train Epoch: 144 [204864/225000 (91%)] Loss: 19432.429688\n",
      "Train Epoch: 144 [207360/225000 (92%)] Loss: 19285.953125\n",
      "Train Epoch: 144 [209856/225000 (93%)] Loss: 18962.363281\n",
      "Train Epoch: 144 [212352/225000 (94%)] Loss: 19325.875000\n",
      "Train Epoch: 144 [214848/225000 (95%)] Loss: 18754.722656\n",
      "Train Epoch: 144 [217344/225000 (97%)] Loss: 19501.363281\n",
      "Train Epoch: 144 [219840/225000 (98%)] Loss: 19229.230469\n",
      "Train Epoch: 144 [222336/225000 (99%)] Loss: 18922.929688\n",
      "Train Epoch: 144 [224832/225000 (100%)] Loss: 19088.476562\n",
      "    epoch          : 144\n",
      "    loss           : 19281.917657116574\n",
      "    val_loss       : 19245.13705095535\n",
      "Train Epoch: 145 [192/225000 (0%)] Loss: 19642.800781\n",
      "Train Epoch: 145 [2688/225000 (1%)] Loss: 19293.769531\n",
      "Train Epoch: 145 [5184/225000 (2%)] Loss: 19270.179688\n",
      "Train Epoch: 145 [7680/225000 (3%)] Loss: 19125.578125\n",
      "Train Epoch: 145 [10176/225000 (5%)] Loss: 19799.972656\n",
      "Train Epoch: 145 [12672/225000 (6%)] Loss: 19844.771484\n",
      "Train Epoch: 145 [15168/225000 (7%)] Loss: 19424.648438\n",
      "Train Epoch: 145 [17664/225000 (8%)] Loss: 19686.007812\n",
      "Train Epoch: 145 [20160/225000 (9%)] Loss: 18939.628906\n",
      "Train Epoch: 145 [22656/225000 (10%)] Loss: 19149.882812\n",
      "Train Epoch: 145 [25152/225000 (11%)] Loss: 19612.531250\n",
      "Train Epoch: 145 [27648/225000 (12%)] Loss: 19252.511719\n",
      "Train Epoch: 145 [30144/225000 (13%)] Loss: 18979.953125\n",
      "Train Epoch: 145 [32640/225000 (15%)] Loss: 19648.816406\n",
      "Train Epoch: 145 [35136/225000 (16%)] Loss: 19126.304688\n",
      "Train Epoch: 145 [37632/225000 (17%)] Loss: 19104.769531\n",
      "Train Epoch: 145 [40128/225000 (18%)] Loss: 19494.464844\n",
      "Train Epoch: 145 [42624/225000 (19%)] Loss: 19346.027344\n",
      "Train Epoch: 145 [45120/225000 (20%)] Loss: 19351.816406\n",
      "Train Epoch: 145 [47616/225000 (21%)] Loss: 19195.324219\n",
      "Train Epoch: 145 [50112/225000 (22%)] Loss: 19078.718750\n",
      "Train Epoch: 145 [52608/225000 (23%)] Loss: 19530.960938\n",
      "Train Epoch: 145 [55104/225000 (24%)] Loss: 19211.322266\n",
      "Train Epoch: 145 [57600/225000 (26%)] Loss: 19457.929688\n",
      "Train Epoch: 145 [60096/225000 (27%)] Loss: 19291.294922\n",
      "Train Epoch: 145 [62592/225000 (28%)] Loss: 19490.064453\n",
      "Train Epoch: 145 [65088/225000 (29%)] Loss: 18923.138672\n",
      "Train Epoch: 145 [67584/225000 (30%)] Loss: 18969.197266\n",
      "Train Epoch: 145 [70080/225000 (31%)] Loss: 19128.691406\n",
      "Train Epoch: 145 [72576/225000 (32%)] Loss: 19643.949219\n",
      "Train Epoch: 145 [75072/225000 (33%)] Loss: 18987.564453\n",
      "Train Epoch: 145 [77568/225000 (34%)] Loss: 19335.773438\n",
      "Train Epoch: 145 [80064/225000 (36%)] Loss: 18631.208984\n",
      "Train Epoch: 145 [82560/225000 (37%)] Loss: 19447.960938\n",
      "Train Epoch: 145 [85056/225000 (38%)] Loss: 19602.925781\n",
      "Train Epoch: 145 [87552/225000 (39%)] Loss: 19156.015625\n",
      "Train Epoch: 145 [90048/225000 (40%)] Loss: 19224.304688\n",
      "Train Epoch: 145 [92544/225000 (41%)] Loss: 19342.375000\n",
      "Train Epoch: 145 [95040/225000 (42%)] Loss: 18745.402344\n",
      "Train Epoch: 145 [97536/225000 (43%)] Loss: 19229.929688\n",
      "Train Epoch: 145 [100032/225000 (44%)] Loss: 19255.906250\n",
      "Train Epoch: 145 [102528/225000 (46%)] Loss: 19291.214844\n",
      "Train Epoch: 145 [105024/225000 (47%)] Loss: 19272.691406\n",
      "Train Epoch: 145 [107520/225000 (48%)] Loss: 18763.945312\n",
      "Train Epoch: 145 [110016/225000 (49%)] Loss: 19737.960938\n",
      "Train Epoch: 145 [112512/225000 (50%)] Loss: 19281.949219\n",
      "Train Epoch: 145 [115008/225000 (51%)] Loss: 19132.136719\n",
      "Train Epoch: 145 [117504/225000 (52%)] Loss: 19448.019531\n",
      "Train Epoch: 145 [120000/225000 (53%)] Loss: 19587.542969\n",
      "Train Epoch: 145 [122496/225000 (54%)] Loss: 19132.195312\n",
      "Train Epoch: 145 [124992/225000 (56%)] Loss: 19979.382812\n",
      "Train Epoch: 145 [127488/225000 (57%)] Loss: 19022.880859\n",
      "Train Epoch: 145 [129984/225000 (58%)] Loss: 19365.720703\n",
      "Train Epoch: 145 [132480/225000 (59%)] Loss: 19618.904297\n",
      "Train Epoch: 145 [134976/225000 (60%)] Loss: 19467.105469\n",
      "Train Epoch: 145 [137472/225000 (61%)] Loss: 19276.218750\n",
      "Train Epoch: 145 [139968/225000 (62%)] Loss: 19459.789062\n",
      "Train Epoch: 145 [142464/225000 (63%)] Loss: 19125.050781\n",
      "Train Epoch: 145 [144960/225000 (64%)] Loss: 18980.703125\n",
      "Train Epoch: 145 [147456/225000 (66%)] Loss: 19416.093750\n",
      "Train Epoch: 145 [149952/225000 (67%)] Loss: 19672.548828\n",
      "Train Epoch: 145 [152448/225000 (68%)] Loss: 19501.261719\n",
      "Train Epoch: 145 [154944/225000 (69%)] Loss: 19511.464844\n",
      "Train Epoch: 145 [157440/225000 (70%)] Loss: 19491.599609\n",
      "Train Epoch: 145 [159936/225000 (71%)] Loss: 19099.302734\n",
      "Train Epoch: 145 [162432/225000 (72%)] Loss: 19836.861328\n",
      "Train Epoch: 145 [164928/225000 (73%)] Loss: 19399.396484\n",
      "Train Epoch: 145 [167424/225000 (74%)] Loss: 19672.386719\n",
      "Train Epoch: 145 [169920/225000 (76%)] Loss: 19636.882812\n",
      "Train Epoch: 145 [172416/225000 (77%)] Loss: 19078.035156\n",
      "Train Epoch: 145 [174912/225000 (78%)] Loss: 19145.197266\n",
      "Train Epoch: 145 [177408/225000 (79%)] Loss: 19133.619141\n",
      "Train Epoch: 145 [179904/225000 (80%)] Loss: 19122.433594\n",
      "Train Epoch: 145 [182400/225000 (81%)] Loss: 19593.628906\n",
      "Train Epoch: 145 [184896/225000 (82%)] Loss: 19134.242188\n",
      "Train Epoch: 145 [187392/225000 (83%)] Loss: 19204.773438\n",
      "Train Epoch: 145 [189888/225000 (84%)] Loss: 19563.005859\n",
      "Train Epoch: 145 [192384/225000 (86%)] Loss: 18959.716797\n",
      "Train Epoch: 145 [194880/225000 (87%)] Loss: 19766.748047\n",
      "Train Epoch: 145 [197376/225000 (88%)] Loss: 19093.082031\n",
      "Train Epoch: 145 [199872/225000 (89%)] Loss: 19434.285156\n",
      "Train Epoch: 145 [202368/225000 (90%)] Loss: 18974.800781\n",
      "Train Epoch: 145 [204864/225000 (91%)] Loss: 19414.318359\n",
      "Train Epoch: 145 [207360/225000 (92%)] Loss: 19196.644531\n",
      "Train Epoch: 145 [209856/225000 (93%)] Loss: 19186.587891\n",
      "Train Epoch: 145 [212352/225000 (94%)] Loss: 18840.564453\n",
      "Train Epoch: 145 [214848/225000 (95%)] Loss: 19126.394531\n",
      "Train Epoch: 145 [217344/225000 (97%)] Loss: 19492.800781\n",
      "Train Epoch: 145 [219840/225000 (98%)] Loss: 19609.191406\n",
      "Train Epoch: 145 [222336/225000 (99%)] Loss: 19454.667969\n",
      "Train Epoch: 145 [224832/225000 (100%)] Loss: 18930.406250\n",
      "    epoch          : 145\n",
      "    loss           : 19279.6320159183\n",
      "    val_loss       : 19205.869158026828\n",
      "Train Epoch: 146 [192/225000 (0%)] Loss: 19303.406250\n",
      "Train Epoch: 146 [2688/225000 (1%)] Loss: 19099.398438\n",
      "Train Epoch: 146 [5184/225000 (2%)] Loss: 19416.136719\n",
      "Train Epoch: 146 [7680/225000 (3%)] Loss: 19666.916016\n",
      "Train Epoch: 146 [10176/225000 (5%)] Loss: 19588.910156\n",
      "Train Epoch: 146 [12672/225000 (6%)] Loss: 19419.595703\n",
      "Train Epoch: 146 [15168/225000 (7%)] Loss: 19171.373047\n",
      "Train Epoch: 146 [17664/225000 (8%)] Loss: 19211.851562\n",
      "Train Epoch: 146 [20160/225000 (9%)] Loss: 19216.589844\n",
      "Train Epoch: 146 [22656/225000 (10%)] Loss: 19326.890625\n",
      "Train Epoch: 146 [25152/225000 (11%)] Loss: 18999.023438\n",
      "Train Epoch: 146 [27648/225000 (12%)] Loss: 19338.332031\n",
      "Train Epoch: 146 [30144/225000 (13%)] Loss: 19278.384766\n",
      "Train Epoch: 146 [32640/225000 (15%)] Loss: 19209.599609\n",
      "Train Epoch: 146 [35136/225000 (16%)] Loss: 19311.589844\n",
      "Train Epoch: 146 [37632/225000 (17%)] Loss: 19209.898438\n",
      "Train Epoch: 146 [40128/225000 (18%)] Loss: 19534.347656\n",
      "Train Epoch: 146 [42624/225000 (19%)] Loss: 19501.058594\n",
      "Train Epoch: 146 [45120/225000 (20%)] Loss: 19233.677734\n",
      "Train Epoch: 146 [47616/225000 (21%)] Loss: 19455.841797\n",
      "Train Epoch: 146 [50112/225000 (22%)] Loss: 19617.617188\n",
      "Train Epoch: 146 [52608/225000 (23%)] Loss: 19780.992188\n",
      "Train Epoch: 146 [55104/225000 (24%)] Loss: 19538.152344\n",
      "Train Epoch: 146 [57600/225000 (26%)] Loss: 18751.347656\n",
      "Train Epoch: 146 [60096/225000 (27%)] Loss: 19271.541016\n",
      "Train Epoch: 146 [62592/225000 (28%)] Loss: 19126.144531\n",
      "Train Epoch: 146 [65088/225000 (29%)] Loss: 19397.230469\n",
      "Train Epoch: 146 [67584/225000 (30%)] Loss: 18968.558594\n",
      "Train Epoch: 146 [70080/225000 (31%)] Loss: 19206.187500\n",
      "Train Epoch: 146 [72576/225000 (32%)] Loss: 18702.505859\n",
      "Train Epoch: 146 [75072/225000 (33%)] Loss: 19231.429688\n",
      "Train Epoch: 146 [77568/225000 (34%)] Loss: 19264.574219\n",
      "Train Epoch: 146 [80064/225000 (36%)] Loss: 18958.996094\n",
      "Train Epoch: 146 [82560/225000 (37%)] Loss: 19485.082031\n",
      "Train Epoch: 146 [85056/225000 (38%)] Loss: 19988.451172\n",
      "Train Epoch: 146 [87552/225000 (39%)] Loss: 19628.800781\n",
      "Train Epoch: 146 [90048/225000 (40%)] Loss: 19433.304688\n",
      "Train Epoch: 146 [92544/225000 (41%)] Loss: 19366.447266\n",
      "Train Epoch: 146 [95040/225000 (42%)] Loss: 19585.101562\n",
      "Train Epoch: 146 [97536/225000 (43%)] Loss: 19179.826172\n",
      "Train Epoch: 146 [100032/225000 (44%)] Loss: 19601.537109\n",
      "Train Epoch: 146 [102528/225000 (46%)] Loss: 19444.152344\n",
      "Train Epoch: 146 [105024/225000 (47%)] Loss: 18952.691406\n",
      "Train Epoch: 146 [107520/225000 (48%)] Loss: 19515.982422\n",
      "Train Epoch: 146 [110016/225000 (49%)] Loss: 19115.855469\n",
      "Train Epoch: 146 [112512/225000 (50%)] Loss: 18689.914062\n",
      "Train Epoch: 146 [115008/225000 (51%)] Loss: 19121.537109\n",
      "Train Epoch: 146 [117504/225000 (52%)] Loss: 19756.183594\n",
      "Train Epoch: 146 [120000/225000 (53%)] Loss: 18939.941406\n",
      "Train Epoch: 146 [122496/225000 (54%)] Loss: 19478.761719\n",
      "Train Epoch: 146 [124992/225000 (56%)] Loss: 19188.511719\n",
      "Train Epoch: 146 [127488/225000 (57%)] Loss: 19157.128906\n",
      "Train Epoch: 146 [129984/225000 (58%)] Loss: 19127.591797\n",
      "Train Epoch: 146 [132480/225000 (59%)] Loss: 19386.849609\n",
      "Train Epoch: 146 [134976/225000 (60%)] Loss: 19787.378906\n",
      "Train Epoch: 146 [137472/225000 (61%)] Loss: 19077.099609\n",
      "Train Epoch: 146 [139968/225000 (62%)] Loss: 19260.554688\n",
      "Train Epoch: 146 [142464/225000 (63%)] Loss: 19423.578125\n",
      "Train Epoch: 146 [144960/225000 (64%)] Loss: 18869.228516\n",
      "Train Epoch: 146 [147456/225000 (66%)] Loss: 19588.867188\n",
      "Train Epoch: 146 [149952/225000 (67%)] Loss: 19286.652344\n",
      "Train Epoch: 146 [152448/225000 (68%)] Loss: 19420.570312\n",
      "Train Epoch: 146 [154944/225000 (69%)] Loss: 18979.527344\n",
      "Train Epoch: 146 [157440/225000 (70%)] Loss: 19893.648438\n",
      "Train Epoch: 146 [159936/225000 (71%)] Loss: 18795.566406\n",
      "Train Epoch: 146 [162432/225000 (72%)] Loss: 19505.511719\n",
      "Train Epoch: 146 [164928/225000 (73%)] Loss: 19426.996094\n",
      "Train Epoch: 146 [167424/225000 (74%)] Loss: 19712.570312\n",
      "Train Epoch: 146 [169920/225000 (76%)] Loss: 19685.740234\n",
      "Train Epoch: 146 [172416/225000 (77%)] Loss: 19387.730469\n",
      "Train Epoch: 146 [174912/225000 (78%)] Loss: 18946.273438\n",
      "Train Epoch: 146 [177408/225000 (79%)] Loss: 19505.650391\n",
      "Train Epoch: 146 [179904/225000 (80%)] Loss: 18986.757812\n",
      "Train Epoch: 146 [182400/225000 (81%)] Loss: 18919.996094\n",
      "Train Epoch: 146 [184896/225000 (82%)] Loss: 19424.849609\n",
      "Train Epoch: 146 [187392/225000 (83%)] Loss: 19214.667969\n",
      "Train Epoch: 146 [189888/225000 (84%)] Loss: 19778.224609\n",
      "Train Epoch: 146 [192384/225000 (86%)] Loss: 19451.707031\n",
      "Train Epoch: 146 [194880/225000 (87%)] Loss: 19689.279297\n",
      "Train Epoch: 146 [197376/225000 (88%)] Loss: 19520.066406\n",
      "Train Epoch: 146 [199872/225000 (89%)] Loss: 19208.457031\n",
      "Train Epoch: 146 [202368/225000 (90%)] Loss: 19748.962891\n",
      "Train Epoch: 146 [204864/225000 (91%)] Loss: 19734.742188\n",
      "Train Epoch: 146 [207360/225000 (92%)] Loss: 18622.355469\n",
      "Train Epoch: 146 [209856/225000 (93%)] Loss: 19104.765625\n",
      "Train Epoch: 146 [212352/225000 (94%)] Loss: 19075.515625\n",
      "Train Epoch: 146 [214848/225000 (95%)] Loss: 19346.853516\n",
      "Train Epoch: 146 [217344/225000 (97%)] Loss: 19488.580078\n",
      "Train Epoch: 146 [219840/225000 (98%)] Loss: 19281.917969\n",
      "Train Epoch: 146 [222336/225000 (99%)] Loss: 19281.763672\n",
      "Train Epoch: 146 [224832/225000 (100%)] Loss: 19095.871094\n",
      "    epoch          : 146\n",
      "    loss           : 19274.61393784663\n",
      "    val_loss       : 19180.236514939606\n",
      "Train Epoch: 147 [192/225000 (0%)] Loss: 19528.583984\n",
      "Train Epoch: 147 [2688/225000 (1%)] Loss: 19444.750000\n",
      "Train Epoch: 147 [5184/225000 (2%)] Loss: 19396.451172\n",
      "Train Epoch: 147 [7680/225000 (3%)] Loss: 19407.609375\n",
      "Train Epoch: 147 [10176/225000 (5%)] Loss: 19336.757812\n",
      "Train Epoch: 147 [12672/225000 (6%)] Loss: 19306.982422\n",
      "Train Epoch: 147 [15168/225000 (7%)] Loss: 19350.332031\n",
      "Train Epoch: 147 [17664/225000 (8%)] Loss: 19572.320312\n",
      "Train Epoch: 147 [20160/225000 (9%)] Loss: 19263.855469\n",
      "Train Epoch: 147 [22656/225000 (10%)] Loss: 19109.068359\n",
      "Train Epoch: 147 [25152/225000 (11%)] Loss: 19449.683594\n",
      "Train Epoch: 147 [27648/225000 (12%)] Loss: 18865.128906\n",
      "Train Epoch: 147 [30144/225000 (13%)] Loss: 19367.099609\n",
      "Train Epoch: 147 [32640/225000 (15%)] Loss: 19338.773438\n",
      "Train Epoch: 147 [35136/225000 (16%)] Loss: 19214.474609\n",
      "Train Epoch: 147 [37632/225000 (17%)] Loss: 19217.470703\n",
      "Train Epoch: 147 [40128/225000 (18%)] Loss: 19519.445312\n",
      "Train Epoch: 147 [42624/225000 (19%)] Loss: 19101.503906\n",
      "Train Epoch: 147 [45120/225000 (20%)] Loss: 19256.410156\n",
      "Train Epoch: 147 [47616/225000 (21%)] Loss: 19403.958984\n",
      "Train Epoch: 147 [50112/225000 (22%)] Loss: 19301.843750\n",
      "Train Epoch: 147 [52608/225000 (23%)] Loss: 19170.082031\n",
      "Train Epoch: 147 [55104/225000 (24%)] Loss: 18714.757812\n",
      "Train Epoch: 147 [57600/225000 (26%)] Loss: 19016.871094\n",
      "Train Epoch: 147 [60096/225000 (27%)] Loss: 19435.033203\n",
      "Train Epoch: 147 [62592/225000 (28%)] Loss: 19667.154297\n",
      "Train Epoch: 147 [65088/225000 (29%)] Loss: 19323.867188\n",
      "Train Epoch: 147 [67584/225000 (30%)] Loss: 19498.576172\n",
      "Train Epoch: 147 [70080/225000 (31%)] Loss: 18869.007812\n",
      "Train Epoch: 147 [72576/225000 (32%)] Loss: 19256.273438\n",
      "Train Epoch: 147 [75072/225000 (33%)] Loss: 18791.574219\n",
      "Train Epoch: 147 [77568/225000 (34%)] Loss: 19192.980469\n",
      "Train Epoch: 147 [80064/225000 (36%)] Loss: 19209.623047\n",
      "Train Epoch: 147 [82560/225000 (37%)] Loss: 19198.875000\n",
      "Train Epoch: 147 [85056/225000 (38%)] Loss: 19559.324219\n",
      "Train Epoch: 147 [87552/225000 (39%)] Loss: 19230.855469\n",
      "Train Epoch: 147 [90048/225000 (40%)] Loss: 19082.746094\n",
      "Train Epoch: 147 [92544/225000 (41%)] Loss: 18992.914062\n",
      "Train Epoch: 147 [95040/225000 (42%)] Loss: 19720.222656\n",
      "Train Epoch: 147 [97536/225000 (43%)] Loss: 19372.292969\n",
      "Train Epoch: 147 [100032/225000 (44%)] Loss: 19702.000000\n",
      "Train Epoch: 147 [102528/225000 (46%)] Loss: 18744.996094\n",
      "Train Epoch: 147 [105024/225000 (47%)] Loss: 19373.476562\n",
      "Train Epoch: 147 [107520/225000 (48%)] Loss: 19264.001953\n",
      "Train Epoch: 147 [110016/225000 (49%)] Loss: 19000.390625\n",
      "Train Epoch: 147 [112512/225000 (50%)] Loss: 19572.177734\n",
      "Train Epoch: 147 [115008/225000 (51%)] Loss: 19071.166016\n",
      "Train Epoch: 147 [117504/225000 (52%)] Loss: 19525.125000\n",
      "Train Epoch: 147 [120000/225000 (53%)] Loss: 19515.070312\n",
      "Train Epoch: 147 [122496/225000 (54%)] Loss: 19467.539062\n",
      "Train Epoch: 147 [124992/225000 (56%)] Loss: 18683.750000\n",
      "Train Epoch: 147 [127488/225000 (57%)] Loss: 19721.046875\n",
      "Train Epoch: 147 [129984/225000 (58%)] Loss: 18937.406250\n",
      "Train Epoch: 147 [132480/225000 (59%)] Loss: 19146.933594\n",
      "Train Epoch: 147 [134976/225000 (60%)] Loss: 19219.976562\n",
      "Train Epoch: 147 [137472/225000 (61%)] Loss: 18887.621094\n",
      "Train Epoch: 147 [139968/225000 (62%)] Loss: 18910.781250\n",
      "Train Epoch: 147 [142464/225000 (63%)] Loss: 19638.236328\n",
      "Train Epoch: 147 [144960/225000 (64%)] Loss: 18705.312500\n",
      "Train Epoch: 147 [147456/225000 (66%)] Loss: 19340.628906\n",
      "Train Epoch: 147 [149952/225000 (67%)] Loss: 19274.076172\n",
      "Train Epoch: 147 [152448/225000 (68%)] Loss: 19346.349609\n",
      "Train Epoch: 147 [154944/225000 (69%)] Loss: 19228.273438\n",
      "Train Epoch: 147 [157440/225000 (70%)] Loss: 19005.750000\n",
      "Train Epoch: 147 [159936/225000 (71%)] Loss: 19140.992188\n",
      "Train Epoch: 147 [162432/225000 (72%)] Loss: 19170.480469\n",
      "Train Epoch: 147 [164928/225000 (73%)] Loss: 19416.683594\n",
      "Train Epoch: 147 [167424/225000 (74%)] Loss: 19201.355469\n",
      "Train Epoch: 147 [169920/225000 (76%)] Loss: 18952.814453\n",
      "Train Epoch: 147 [172416/225000 (77%)] Loss: 19063.994141\n",
      "Train Epoch: 147 [174912/225000 (78%)] Loss: 19401.691406\n",
      "Train Epoch: 147 [177408/225000 (79%)] Loss: 19492.195312\n",
      "Train Epoch: 147 [179904/225000 (80%)] Loss: 19031.792969\n",
      "Train Epoch: 147 [182400/225000 (81%)] Loss: 19293.921875\n",
      "Train Epoch: 147 [184896/225000 (82%)] Loss: 19354.125000\n",
      "Train Epoch: 147 [187392/225000 (83%)] Loss: 19051.171875\n",
      "Train Epoch: 147 [189888/225000 (84%)] Loss: 19187.917969\n",
      "Train Epoch: 147 [192384/225000 (86%)] Loss: 19302.630859\n",
      "Train Epoch: 147 [194880/225000 (87%)] Loss: 19022.703125\n",
      "Train Epoch: 147 [197376/225000 (88%)] Loss: 18732.800781\n",
      "Train Epoch: 147 [199872/225000 (89%)] Loss: 19289.498047\n",
      "Train Epoch: 147 [202368/225000 (90%)] Loss: 19551.224609\n",
      "Train Epoch: 147 [204864/225000 (91%)] Loss: 19634.164062\n",
      "Train Epoch: 147 [207360/225000 (92%)] Loss: 19612.406250\n",
      "Train Epoch: 147 [209856/225000 (93%)] Loss: 18983.205078\n",
      "Train Epoch: 147 [212352/225000 (94%)] Loss: 19118.632812\n",
      "Train Epoch: 147 [214848/225000 (95%)] Loss: 19538.437500\n",
      "Train Epoch: 147 [217344/225000 (97%)] Loss: 19464.156250\n",
      "Train Epoch: 147 [219840/225000 (98%)] Loss: 19259.759766\n",
      "Train Epoch: 147 [222336/225000 (99%)] Loss: 19366.662109\n",
      "Train Epoch: 147 [224832/225000 (100%)] Loss: 18478.255859\n",
      "    epoch          : 147\n",
      "    loss           : 19270.61445612468\n",
      "    val_loss       : 19185.467670111255\n",
      "Train Epoch: 148 [192/225000 (0%)] Loss: 18968.929688\n",
      "Train Epoch: 148 [2688/225000 (1%)] Loss: 18981.113281\n",
      "Train Epoch: 148 [5184/225000 (2%)] Loss: 19095.341797\n",
      "Train Epoch: 148 [7680/225000 (3%)] Loss: 19160.550781\n",
      "Train Epoch: 148 [10176/225000 (5%)] Loss: 19283.816406\n",
      "Train Epoch: 148 [12672/225000 (6%)] Loss: 19249.007812\n",
      "Train Epoch: 148 [15168/225000 (7%)] Loss: 19274.453125\n",
      "Train Epoch: 148 [17664/225000 (8%)] Loss: 19086.265625\n",
      "Train Epoch: 148 [20160/225000 (9%)] Loss: 19278.730469\n",
      "Train Epoch: 148 [22656/225000 (10%)] Loss: 19643.699219\n",
      "Train Epoch: 148 [25152/225000 (11%)] Loss: 18813.982422\n",
      "Train Epoch: 148 [27648/225000 (12%)] Loss: 19332.781250\n",
      "Train Epoch: 148 [30144/225000 (13%)] Loss: 19545.515625\n",
      "Train Epoch: 148 [32640/225000 (15%)] Loss: 19247.027344\n",
      "Train Epoch: 148 [35136/225000 (16%)] Loss: 18997.843750\n",
      "Train Epoch: 148 [37632/225000 (17%)] Loss: 19315.888672\n",
      "Train Epoch: 148 [40128/225000 (18%)] Loss: 18974.371094\n",
      "Train Epoch: 148 [42624/225000 (19%)] Loss: 19186.472656\n",
      "Train Epoch: 148 [45120/225000 (20%)] Loss: 18978.671875\n",
      "Train Epoch: 148 [47616/225000 (21%)] Loss: 18507.222656\n",
      "Train Epoch: 148 [50112/225000 (22%)] Loss: 19517.269531\n",
      "Train Epoch: 148 [52608/225000 (23%)] Loss: 19543.101562\n",
      "Train Epoch: 148 [55104/225000 (24%)] Loss: 19449.414062\n",
      "Train Epoch: 148 [57600/225000 (26%)] Loss: 19366.328125\n",
      "Train Epoch: 148 [60096/225000 (27%)] Loss: 19156.300781\n",
      "Train Epoch: 148 [62592/225000 (28%)] Loss: 18941.466797\n",
      "Train Epoch: 148 [65088/225000 (29%)] Loss: 19169.660156\n",
      "Train Epoch: 148 [67584/225000 (30%)] Loss: 19362.882812\n",
      "Train Epoch: 148 [70080/225000 (31%)] Loss: 19518.349609\n",
      "Train Epoch: 148 [72576/225000 (32%)] Loss: 19436.082031\n",
      "Train Epoch: 148 [75072/225000 (33%)] Loss: 19320.148438\n",
      "Train Epoch: 148 [77568/225000 (34%)] Loss: 19362.224609\n",
      "Train Epoch: 148 [80064/225000 (36%)] Loss: 19072.302734\n",
      "Train Epoch: 148 [82560/225000 (37%)] Loss: 19661.546875\n",
      "Train Epoch: 148 [85056/225000 (38%)] Loss: 19031.494141\n",
      "Train Epoch: 148 [87552/225000 (39%)] Loss: 19285.822266\n",
      "Train Epoch: 148 [90048/225000 (40%)] Loss: 19715.373047\n",
      "Train Epoch: 148 [92544/225000 (41%)] Loss: 19140.644531\n",
      "Train Epoch: 148 [95040/225000 (42%)] Loss: 19079.582031\n",
      "Train Epoch: 148 [97536/225000 (43%)] Loss: 19595.841797\n",
      "Train Epoch: 148 [100032/225000 (44%)] Loss: 19451.769531\n",
      "Train Epoch: 148 [102528/225000 (46%)] Loss: 19207.423828\n",
      "Train Epoch: 148 [105024/225000 (47%)] Loss: 19850.457031\n",
      "Train Epoch: 148 [107520/225000 (48%)] Loss: 19359.269531\n",
      "Train Epoch: 148 [110016/225000 (49%)] Loss: 19202.501953\n",
      "Train Epoch: 148 [112512/225000 (50%)] Loss: 18997.269531\n",
      "Train Epoch: 148 [115008/225000 (51%)] Loss: 19151.734375\n",
      "Train Epoch: 148 [117504/225000 (52%)] Loss: 18797.082031\n",
      "Train Epoch: 148 [120000/225000 (53%)] Loss: 19356.550781\n",
      "Train Epoch: 148 [122496/225000 (54%)] Loss: 19108.654297\n",
      "Train Epoch: 148 [124992/225000 (56%)] Loss: 19210.019531\n",
      "Train Epoch: 148 [127488/225000 (57%)] Loss: 19075.248047\n",
      "Train Epoch: 148 [129984/225000 (58%)] Loss: 19422.126953\n",
      "Train Epoch: 148 [132480/225000 (59%)] Loss: 19540.625000\n",
      "Train Epoch: 148 [134976/225000 (60%)] Loss: 19389.144531\n",
      "Train Epoch: 148 [137472/225000 (61%)] Loss: 18977.531250\n",
      "Train Epoch: 148 [139968/225000 (62%)] Loss: 19795.460938\n",
      "Train Epoch: 148 [142464/225000 (63%)] Loss: 19242.050781\n",
      "Train Epoch: 148 [144960/225000 (64%)] Loss: 19079.142578\n",
      "Train Epoch: 148 [147456/225000 (66%)] Loss: 19665.425781\n",
      "Train Epoch: 148 [149952/225000 (67%)] Loss: 18897.015625\n",
      "Train Epoch: 148 [152448/225000 (68%)] Loss: 19146.275391\n",
      "Train Epoch: 148 [154944/225000 (69%)] Loss: 19240.078125\n",
      "Train Epoch: 148 [157440/225000 (70%)] Loss: 19720.722656\n",
      "Train Epoch: 148 [159936/225000 (71%)] Loss: 19283.677734\n",
      "Train Epoch: 148 [162432/225000 (72%)] Loss: 19024.015625\n",
      "Train Epoch: 148 [164928/225000 (73%)] Loss: 19556.835938\n",
      "Train Epoch: 148 [167424/225000 (74%)] Loss: 19569.488281\n",
      "Train Epoch: 148 [169920/225000 (76%)] Loss: 19461.201172\n",
      "Train Epoch: 148 [172416/225000 (77%)] Loss: 19460.716797\n",
      "Train Epoch: 148 [174912/225000 (78%)] Loss: 19271.187500\n",
      "Train Epoch: 148 [177408/225000 (79%)] Loss: 19413.625000\n",
      "Train Epoch: 148 [179904/225000 (80%)] Loss: 19409.539062\n",
      "Train Epoch: 148 [182400/225000 (81%)] Loss: 19622.140625\n",
      "Train Epoch: 148 [184896/225000 (82%)] Loss: 19257.705078\n",
      "Train Epoch: 148 [187392/225000 (83%)] Loss: 19462.332031\n",
      "Train Epoch: 148 [189888/225000 (84%)] Loss: 19008.199219\n",
      "Train Epoch: 148 [192384/225000 (86%)] Loss: 19442.644531\n",
      "Train Epoch: 148 [194880/225000 (87%)] Loss: 19147.550781\n",
      "Train Epoch: 148 [197376/225000 (88%)] Loss: 18686.796875\n",
      "Train Epoch: 148 [199872/225000 (89%)] Loss: 19956.199219\n",
      "Train Epoch: 148 [202368/225000 (90%)] Loss: 19069.152344\n",
      "Train Epoch: 148 [204864/225000 (91%)] Loss: 19179.496094\n",
      "Train Epoch: 148 [207360/225000 (92%)] Loss: 19091.753906\n",
      "Train Epoch: 148 [209856/225000 (93%)] Loss: 19179.351562\n",
      "Train Epoch: 148 [212352/225000 (94%)] Loss: 19153.761719\n",
      "Train Epoch: 148 [214848/225000 (95%)] Loss: 18912.605469\n",
      "Train Epoch: 148 [217344/225000 (97%)] Loss: 19142.132812\n",
      "Train Epoch: 148 [219840/225000 (98%)] Loss: 19649.113281\n",
      "Train Epoch: 148 [222336/225000 (99%)] Loss: 19213.511719\n",
      "Train Epoch: 148 [224832/225000 (100%)] Loss: 19471.042969\n",
      "    epoch          : 148\n",
      "    loss           : 19262.19175454618\n",
      "    val_loss       : 19177.163182517044\n",
      "Train Epoch: 149 [192/225000 (0%)] Loss: 19131.054688\n",
      "Train Epoch: 149 [2688/225000 (1%)] Loss: 19758.951172\n",
      "Train Epoch: 149 [5184/225000 (2%)] Loss: 19162.595703\n",
      "Train Epoch: 149 [7680/225000 (3%)] Loss: 18996.117188\n",
      "Train Epoch: 149 [10176/225000 (5%)] Loss: 18831.375000\n",
      "Train Epoch: 149 [12672/225000 (6%)] Loss: 19763.466797\n",
      "Train Epoch: 149 [15168/225000 (7%)] Loss: 19214.134766\n",
      "Train Epoch: 149 [17664/225000 (8%)] Loss: 18716.691406\n",
      "Train Epoch: 149 [20160/225000 (9%)] Loss: 19215.650391\n",
      "Train Epoch: 149 [22656/225000 (10%)] Loss: 19204.996094\n",
      "Train Epoch: 149 [25152/225000 (11%)] Loss: 19680.687500\n",
      "Train Epoch: 149 [27648/225000 (12%)] Loss: 19187.804688\n",
      "Train Epoch: 149 [30144/225000 (13%)] Loss: 19569.441406\n",
      "Train Epoch: 149 [32640/225000 (15%)] Loss: 19231.695312\n",
      "Train Epoch: 149 [35136/225000 (16%)] Loss: 19397.724609\n",
      "Train Epoch: 149 [37632/225000 (17%)] Loss: 19123.199219\n",
      "Train Epoch: 149 [40128/225000 (18%)] Loss: 19787.492188\n",
      "Train Epoch: 149 [42624/225000 (19%)] Loss: 19496.617188\n",
      "Train Epoch: 149 [45120/225000 (20%)] Loss: 19811.300781\n",
      "Train Epoch: 149 [47616/225000 (21%)] Loss: 19550.767578\n",
      "Train Epoch: 149 [50112/225000 (22%)] Loss: 19759.902344\n",
      "Train Epoch: 149 [52608/225000 (23%)] Loss: 19878.005859\n",
      "Train Epoch: 149 [55104/225000 (24%)] Loss: 19220.833984\n",
      "Train Epoch: 149 [57600/225000 (26%)] Loss: 19138.279297\n",
      "Train Epoch: 149 [60096/225000 (27%)] Loss: 19798.160156\n",
      "Train Epoch: 149 [62592/225000 (28%)] Loss: 19344.146484\n",
      "Train Epoch: 149 [65088/225000 (29%)] Loss: 19206.878906\n",
      "Train Epoch: 149 [67584/225000 (30%)] Loss: 19068.388672\n",
      "Train Epoch: 149 [70080/225000 (31%)] Loss: 19446.517578\n",
      "Train Epoch: 149 [72576/225000 (32%)] Loss: 18632.359375\n",
      "Train Epoch: 149 [75072/225000 (33%)] Loss: 19219.787109\n",
      "Train Epoch: 149 [77568/225000 (34%)] Loss: 19010.144531\n",
      "Train Epoch: 149 [80064/225000 (36%)] Loss: 19208.851562\n",
      "Train Epoch: 149 [82560/225000 (37%)] Loss: 19177.515625\n",
      "Train Epoch: 149 [85056/225000 (38%)] Loss: 18841.824219\n",
      "Train Epoch: 149 [87552/225000 (39%)] Loss: 19240.492188\n",
      "Train Epoch: 149 [90048/225000 (40%)] Loss: 19523.048828\n",
      "Train Epoch: 149 [92544/225000 (41%)] Loss: 19276.691406\n",
      "Train Epoch: 149 [95040/225000 (42%)] Loss: 19273.128906\n",
      "Train Epoch: 149 [97536/225000 (43%)] Loss: 19142.748047\n",
      "Train Epoch: 149 [100032/225000 (44%)] Loss: 19176.257812\n",
      "Train Epoch: 149 [102528/225000 (46%)] Loss: 19484.007812\n",
      "Train Epoch: 149 [105024/225000 (47%)] Loss: 18935.978516\n",
      "Train Epoch: 149 [107520/225000 (48%)] Loss: 19342.656250\n",
      "Train Epoch: 149 [110016/225000 (49%)] Loss: 19241.437500\n",
      "Train Epoch: 149 [112512/225000 (50%)] Loss: 19045.906250\n",
      "Train Epoch: 149 [115008/225000 (51%)] Loss: 19532.798828\n",
      "Train Epoch: 149 [117504/225000 (52%)] Loss: 18992.535156\n",
      "Train Epoch: 149 [120000/225000 (53%)] Loss: 19030.462891\n",
      "Train Epoch: 149 [122496/225000 (54%)] Loss: 19615.671875\n",
      "Train Epoch: 149 [124992/225000 (56%)] Loss: 19478.617188\n",
      "Train Epoch: 149 [127488/225000 (57%)] Loss: 19351.089844\n",
      "Train Epoch: 149 [129984/225000 (58%)] Loss: 19477.382812\n",
      "Train Epoch: 149 [132480/225000 (59%)] Loss: 19015.939453\n",
      "Train Epoch: 149 [134976/225000 (60%)] Loss: 19208.453125\n",
      "Train Epoch: 149 [137472/225000 (61%)] Loss: 19467.390625\n",
      "Train Epoch: 149 [139968/225000 (62%)] Loss: 19332.472656\n",
      "Train Epoch: 149 [142464/225000 (63%)] Loss: 19251.353516\n",
      "Train Epoch: 149 [144960/225000 (64%)] Loss: 19689.279297\n",
      "Train Epoch: 149 [147456/225000 (66%)] Loss: 19352.140625\n",
      "Train Epoch: 149 [149952/225000 (67%)] Loss: 18708.470703\n",
      "Train Epoch: 149 [152448/225000 (68%)] Loss: 19265.632812\n",
      "Train Epoch: 149 [154944/225000 (69%)] Loss: 19573.699219\n",
      "Train Epoch: 149 [157440/225000 (70%)] Loss: 19409.957031\n",
      "Train Epoch: 149 [159936/225000 (71%)] Loss: 19737.121094\n",
      "Train Epoch: 149 [162432/225000 (72%)] Loss: 19356.628906\n",
      "Train Epoch: 149 [164928/225000 (73%)] Loss: 19160.468750\n",
      "Train Epoch: 149 [167424/225000 (74%)] Loss: 18863.912109\n",
      "Train Epoch: 149 [169920/225000 (76%)] Loss: 18744.300781\n",
      "Train Epoch: 149 [172416/225000 (77%)] Loss: 19079.769531\n",
      "Train Epoch: 149 [174912/225000 (78%)] Loss: 19492.871094\n",
      "Train Epoch: 149 [177408/225000 (79%)] Loss: 19441.378906\n",
      "Train Epoch: 149 [179904/225000 (80%)] Loss: 19296.193359\n",
      "Train Epoch: 149 [182400/225000 (81%)] Loss: 19175.570312\n",
      "Train Epoch: 149 [184896/225000 (82%)] Loss: 19335.541016\n",
      "Train Epoch: 149 [187392/225000 (83%)] Loss: 18847.621094\n",
      "Train Epoch: 149 [189888/225000 (84%)] Loss: 19505.019531\n",
      "Train Epoch: 149 [192384/225000 (86%)] Loss: 18816.958984\n",
      "Train Epoch: 149 [194880/225000 (87%)] Loss: 19267.390625\n",
      "Train Epoch: 149 [197376/225000 (88%)] Loss: 19223.089844\n",
      "Train Epoch: 149 [199872/225000 (89%)] Loss: 19555.310547\n",
      "Train Epoch: 149 [202368/225000 (90%)] Loss: 19186.558594\n",
      "Train Epoch: 149 [204864/225000 (91%)] Loss: 18994.841797\n",
      "Train Epoch: 149 [207360/225000 (92%)] Loss: 19248.179688\n",
      "Train Epoch: 149 [209856/225000 (93%)] Loss: 18873.734375\n",
      "Train Epoch: 149 [212352/225000 (94%)] Loss: 19276.863281\n",
      "Train Epoch: 149 [214848/225000 (95%)] Loss: 19243.392578\n",
      "Train Epoch: 149 [217344/225000 (97%)] Loss: 19673.621094\n",
      "Train Epoch: 149 [219840/225000 (98%)] Loss: 19221.078125\n",
      "Train Epoch: 149 [222336/225000 (99%)] Loss: 19648.294922\n",
      "Train Epoch: 149 [224832/225000 (100%)] Loss: 19107.107422\n",
      "    epoch          : 149\n",
      "    loss           : 19264.628552954353\n",
      "    val_loss       : 19163.287039054714\n",
      "Train Epoch: 150 [192/225000 (0%)] Loss: 19088.464844\n",
      "Train Epoch: 150 [2688/225000 (1%)] Loss: 19223.179688\n",
      "Train Epoch: 150 [5184/225000 (2%)] Loss: 19506.072266\n",
      "Train Epoch: 150 [7680/225000 (3%)] Loss: 19794.550781\n",
      "Train Epoch: 150 [10176/225000 (5%)] Loss: 19120.658203\n",
      "Train Epoch: 150 [12672/225000 (6%)] Loss: 19552.271484\n",
      "Train Epoch: 150 [15168/225000 (7%)] Loss: 19009.677734\n",
      "Train Epoch: 150 [17664/225000 (8%)] Loss: 18931.781250\n",
      "Train Epoch: 150 [20160/225000 (9%)] Loss: 20051.972656\n",
      "Train Epoch: 150 [22656/225000 (10%)] Loss: 19089.814453\n",
      "Train Epoch: 150 [25152/225000 (11%)] Loss: 19064.933594\n",
      "Train Epoch: 150 [27648/225000 (12%)] Loss: 19510.478516\n",
      "Train Epoch: 150 [30144/225000 (13%)] Loss: 19555.503906\n",
      "Train Epoch: 150 [32640/225000 (15%)] Loss: 18988.138672\n",
      "Train Epoch: 150 [35136/225000 (16%)] Loss: 19660.753906\n",
      "Train Epoch: 150 [37632/225000 (17%)] Loss: 19339.558594\n",
      "Train Epoch: 150 [40128/225000 (18%)] Loss: 19070.687500\n",
      "Train Epoch: 150 [42624/225000 (19%)] Loss: 19506.664062\n",
      "Train Epoch: 150 [45120/225000 (20%)] Loss: 19345.476562\n",
      "Train Epoch: 150 [47616/225000 (21%)] Loss: 19016.916016\n",
      "Train Epoch: 150 [50112/225000 (22%)] Loss: 18983.796875\n",
      "Train Epoch: 150 [52608/225000 (23%)] Loss: 18876.884766\n",
      "Train Epoch: 150 [55104/225000 (24%)] Loss: 19368.332031\n",
      "Train Epoch: 150 [57600/225000 (26%)] Loss: 19039.871094\n",
      "Train Epoch: 150 [60096/225000 (27%)] Loss: 19833.906250\n",
      "Train Epoch: 150 [62592/225000 (28%)] Loss: 18915.718750\n",
      "Train Epoch: 150 [65088/225000 (29%)] Loss: 19421.519531\n",
      "Train Epoch: 150 [67584/225000 (30%)] Loss: 18971.703125\n",
      "Train Epoch: 150 [70080/225000 (31%)] Loss: 19378.716797\n",
      "Train Epoch: 150 [72576/225000 (32%)] Loss: 19123.214844\n",
      "Train Epoch: 150 [75072/225000 (33%)] Loss: 19360.093750\n",
      "Train Epoch: 150 [77568/225000 (34%)] Loss: 18859.730469\n",
      "Train Epoch: 150 [80064/225000 (36%)] Loss: 19218.066406\n",
      "Train Epoch: 150 [82560/225000 (37%)] Loss: 18856.992188\n",
      "Train Epoch: 150 [85056/225000 (38%)] Loss: 19096.763672\n",
      "Train Epoch: 150 [87552/225000 (39%)] Loss: 19075.751953\n",
      "Train Epoch: 150 [90048/225000 (40%)] Loss: 19013.812500\n",
      "Train Epoch: 150 [92544/225000 (41%)] Loss: 19149.701172\n",
      "Train Epoch: 150 [95040/225000 (42%)] Loss: 19212.335938\n",
      "Train Epoch: 150 [97536/225000 (43%)] Loss: 19154.792969\n",
      "Train Epoch: 150 [100032/225000 (44%)] Loss: 19345.882812\n",
      "Train Epoch: 150 [102528/225000 (46%)] Loss: 19373.195312\n",
      "Train Epoch: 150 [105024/225000 (47%)] Loss: 19669.384766\n",
      "Train Epoch: 150 [107520/225000 (48%)] Loss: 19171.789062\n",
      "Train Epoch: 150 [110016/225000 (49%)] Loss: 18475.222656\n",
      "Train Epoch: 150 [112512/225000 (50%)] Loss: 19219.933594\n",
      "Train Epoch: 150 [115008/225000 (51%)] Loss: 19206.726562\n",
      "Train Epoch: 150 [117504/225000 (52%)] Loss: 18950.527344\n",
      "Train Epoch: 150 [120000/225000 (53%)] Loss: 19015.828125\n",
      "Train Epoch: 150 [122496/225000 (54%)] Loss: 18985.164062\n",
      "Train Epoch: 150 [124992/225000 (56%)] Loss: 19129.132812\n",
      "Train Epoch: 150 [127488/225000 (57%)] Loss: 19169.376953\n",
      "Train Epoch: 150 [129984/225000 (58%)] Loss: 19187.185547\n",
      "Train Epoch: 150 [132480/225000 (59%)] Loss: 19205.242188\n",
      "Train Epoch: 150 [134976/225000 (60%)] Loss: 19132.470703\n",
      "Train Epoch: 150 [137472/225000 (61%)] Loss: 19449.710938\n",
      "Train Epoch: 150 [139968/225000 (62%)] Loss: 19645.236328\n",
      "Train Epoch: 150 [142464/225000 (63%)] Loss: 19393.992188\n",
      "Train Epoch: 150 [144960/225000 (64%)] Loss: 19064.527344\n",
      "Train Epoch: 150 [147456/225000 (66%)] Loss: 19330.472656\n",
      "Train Epoch: 150 [149952/225000 (67%)] Loss: 19315.648438\n",
      "Train Epoch: 150 [152448/225000 (68%)] Loss: 18718.898438\n",
      "Train Epoch: 150 [154944/225000 (69%)] Loss: 19690.414062\n",
      "Train Epoch: 150 [157440/225000 (70%)] Loss: 18694.847656\n",
      "Train Epoch: 150 [159936/225000 (71%)] Loss: 18883.894531\n",
      "Train Epoch: 150 [162432/225000 (72%)] Loss: 19235.929688\n",
      "Train Epoch: 150 [164928/225000 (73%)] Loss: 19013.140625\n",
      "Train Epoch: 150 [167424/225000 (74%)] Loss: 19158.835938\n",
      "Train Epoch: 150 [169920/225000 (76%)] Loss: 19324.033203\n",
      "Train Epoch: 150 [172416/225000 (77%)] Loss: 19304.605469\n",
      "Train Epoch: 150 [174912/225000 (78%)] Loss: 19439.601562\n",
      "Train Epoch: 150 [177408/225000 (79%)] Loss: 19022.980469\n",
      "Train Epoch: 150 [179904/225000 (80%)] Loss: 19934.078125\n",
      "Train Epoch: 150 [182400/225000 (81%)] Loss: 19106.308594\n",
      "Train Epoch: 150 [184896/225000 (82%)] Loss: 19220.265625\n",
      "Train Epoch: 150 [187392/225000 (83%)] Loss: 19642.390625\n",
      "Train Epoch: 150 [189888/225000 (84%)] Loss: 19443.076172\n",
      "Train Epoch: 150 [192384/225000 (86%)] Loss: 18858.367188\n",
      "Train Epoch: 150 [194880/225000 (87%)] Loss: 18987.310547\n",
      "Train Epoch: 150 [197376/225000 (88%)] Loss: 18847.496094\n",
      "Train Epoch: 150 [199872/225000 (89%)] Loss: 19287.796875\n",
      "Train Epoch: 150 [202368/225000 (90%)] Loss: 19443.378906\n",
      "Train Epoch: 150 [204864/225000 (91%)] Loss: 19262.117188\n",
      "Train Epoch: 150 [207360/225000 (92%)] Loss: 18850.380859\n",
      "Train Epoch: 150 [209856/225000 (93%)] Loss: 19532.632812\n",
      "Train Epoch: 150 [212352/225000 (94%)] Loss: 19102.613281\n",
      "Train Epoch: 150 [214848/225000 (95%)] Loss: 18901.437500\n",
      "Train Epoch: 150 [217344/225000 (97%)] Loss: 18836.664062\n",
      "Train Epoch: 150 [219840/225000 (98%)] Loss: 19583.005859\n",
      "Train Epoch: 150 [222336/225000 (99%)] Loss: 18735.769531\n",
      "Train Epoch: 150 [224832/225000 (100%)] Loss: 18911.431641\n",
      "    epoch          : 150\n",
      "    loss           : 19263.57088910516\n",
      "    val_loss       : 19160.009107536942\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0804_123131/checkpoint-epoch150.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 151 [192/225000 (0%)] Loss: 19433.130859\n",
      "Train Epoch: 151 [2688/225000 (1%)] Loss: 19167.376953\n",
      "Train Epoch: 151 [5184/225000 (2%)] Loss: 18840.138672\n",
      "Train Epoch: 151 [7680/225000 (3%)] Loss: 19219.687500\n",
      "Train Epoch: 151 [10176/225000 (5%)] Loss: 19399.523438\n",
      "Train Epoch: 151 [12672/225000 (6%)] Loss: 18842.949219\n",
      "Train Epoch: 151 [15168/225000 (7%)] Loss: 19373.906250\n",
      "Train Epoch: 151 [17664/225000 (8%)] Loss: 18919.328125\n",
      "Train Epoch: 151 [20160/225000 (9%)] Loss: 19409.851562\n",
      "Train Epoch: 151 [22656/225000 (10%)] Loss: 19265.111328\n",
      "Train Epoch: 151 [25152/225000 (11%)] Loss: 19193.376953\n",
      "Train Epoch: 151 [27648/225000 (12%)] Loss: 19506.152344\n",
      "Train Epoch: 151 [30144/225000 (13%)] Loss: 19599.734375\n",
      "Train Epoch: 151 [32640/225000 (15%)] Loss: 19116.927734\n",
      "Train Epoch: 151 [35136/225000 (16%)] Loss: 19383.478516\n",
      "Train Epoch: 151 [37632/225000 (17%)] Loss: 19139.621094\n",
      "Train Epoch: 151 [40128/225000 (18%)] Loss: 19345.291016\n",
      "Train Epoch: 151 [42624/225000 (19%)] Loss: 19290.630859\n",
      "Train Epoch: 151 [45120/225000 (20%)] Loss: 19090.326172\n",
      "Train Epoch: 151 [47616/225000 (21%)] Loss: 19255.832031\n",
      "Train Epoch: 151 [50112/225000 (22%)] Loss: 19380.609375\n",
      "Train Epoch: 151 [52608/225000 (23%)] Loss: 19107.554688\n",
      "Train Epoch: 151 [55104/225000 (24%)] Loss: 18746.328125\n",
      "Train Epoch: 151 [57600/225000 (26%)] Loss: 19107.027344\n",
      "Train Epoch: 151 [60096/225000 (27%)] Loss: 19343.484375\n",
      "Train Epoch: 151 [62592/225000 (28%)] Loss: 19218.974609\n",
      "Train Epoch: 151 [65088/225000 (29%)] Loss: 19545.734375\n",
      "Train Epoch: 151 [67584/225000 (30%)] Loss: 19475.191406\n",
      "Train Epoch: 151 [70080/225000 (31%)] Loss: 19199.841797\n",
      "Train Epoch: 151 [72576/225000 (32%)] Loss: 19620.585938\n",
      "Train Epoch: 151 [75072/225000 (33%)] Loss: 19679.056641\n",
      "Train Epoch: 151 [77568/225000 (34%)] Loss: 18966.699219\n",
      "Train Epoch: 151 [80064/225000 (36%)] Loss: 19053.285156\n",
      "Train Epoch: 151 [82560/225000 (37%)] Loss: 19582.054688\n",
      "Train Epoch: 151 [85056/225000 (38%)] Loss: 19363.541016\n",
      "Train Epoch: 151 [87552/225000 (39%)] Loss: 19328.542969\n",
      "Train Epoch: 151 [90048/225000 (40%)] Loss: 19498.351562\n",
      "Train Epoch: 151 [92544/225000 (41%)] Loss: 19653.453125\n",
      "Train Epoch: 151 [95040/225000 (42%)] Loss: 19005.251953\n",
      "Train Epoch: 151 [97536/225000 (43%)] Loss: 19153.771484\n",
      "Train Epoch: 151 [100032/225000 (44%)] Loss: 19312.914062\n",
      "Train Epoch: 151 [102528/225000 (46%)] Loss: 18827.062500\n",
      "Train Epoch: 151 [105024/225000 (47%)] Loss: 18725.113281\n",
      "Train Epoch: 151 [107520/225000 (48%)] Loss: 19772.507812\n",
      "Train Epoch: 151 [110016/225000 (49%)] Loss: 19826.398438\n",
      "Train Epoch: 151 [112512/225000 (50%)] Loss: 19047.585938\n",
      "Train Epoch: 151 [115008/225000 (51%)] Loss: 19289.416016\n",
      "Train Epoch: 151 [117504/225000 (52%)] Loss: 18950.798828\n",
      "Train Epoch: 151 [120000/225000 (53%)] Loss: 18948.404297\n",
      "Train Epoch: 151 [122496/225000 (54%)] Loss: 19614.867188\n",
      "Train Epoch: 151 [124992/225000 (56%)] Loss: 19327.978516\n",
      "Train Epoch: 151 [127488/225000 (57%)] Loss: 19491.597656\n",
      "Train Epoch: 151 [129984/225000 (58%)] Loss: 19135.345703\n",
      "Train Epoch: 151 [132480/225000 (59%)] Loss: 19470.253906\n",
      "Train Epoch: 151 [134976/225000 (60%)] Loss: 19070.511719\n",
      "Train Epoch: 151 [137472/225000 (61%)] Loss: 19152.691406\n",
      "Train Epoch: 151 [139968/225000 (62%)] Loss: 18874.595703\n",
      "Train Epoch: 151 [142464/225000 (63%)] Loss: 19042.789062\n",
      "Train Epoch: 151 [144960/225000 (64%)] Loss: 19294.656250\n",
      "Train Epoch: 151 [147456/225000 (66%)] Loss: 19590.777344\n",
      "Train Epoch: 151 [149952/225000 (67%)] Loss: 19443.406250\n",
      "Train Epoch: 151 [152448/225000 (68%)] Loss: 19526.074219\n",
      "Train Epoch: 151 [154944/225000 (69%)] Loss: 19629.107422\n",
      "Train Epoch: 151 [157440/225000 (70%)] Loss: 19824.796875\n",
      "Train Epoch: 151 [159936/225000 (71%)] Loss: 19262.230469\n",
      "Train Epoch: 151 [162432/225000 (72%)] Loss: 19360.841797\n",
      "Train Epoch: 151 [164928/225000 (73%)] Loss: 19032.335938\n",
      "Train Epoch: 151 [167424/225000 (74%)] Loss: 19649.894531\n",
      "Train Epoch: 151 [169920/225000 (76%)] Loss: 18669.503906\n",
      "Train Epoch: 151 [172416/225000 (77%)] Loss: 19045.105469\n",
      "Train Epoch: 151 [174912/225000 (78%)] Loss: 18707.714844\n",
      "Train Epoch: 151 [177408/225000 (79%)] Loss: 19036.570312\n",
      "Train Epoch: 151 [179904/225000 (80%)] Loss: 19296.080078\n",
      "Train Epoch: 151 [182400/225000 (81%)] Loss: 19158.308594\n",
      "Train Epoch: 151 [184896/225000 (82%)] Loss: 19266.896484\n",
      "Train Epoch: 151 [187392/225000 (83%)] Loss: 18848.617188\n",
      "Train Epoch: 151 [189888/225000 (84%)] Loss: 19354.910156\n",
      "Train Epoch: 151 [192384/225000 (86%)] Loss: 19111.496094\n",
      "Train Epoch: 151 [194880/225000 (87%)] Loss: 19768.345703\n",
      "Train Epoch: 151 [197376/225000 (88%)] Loss: 19432.949219\n",
      "Train Epoch: 151 [199872/225000 (89%)] Loss: 18971.214844\n",
      "Train Epoch: 151 [202368/225000 (90%)] Loss: 18862.332031\n",
      "Train Epoch: 151 [204864/225000 (91%)] Loss: 19677.894531\n",
      "Train Epoch: 151 [207360/225000 (92%)] Loss: 19393.898438\n",
      "Train Epoch: 151 [209856/225000 (93%)] Loss: 19155.769531\n",
      "Train Epoch: 151 [212352/225000 (94%)] Loss: 19651.175781\n",
      "Train Epoch: 151 [214848/225000 (95%)] Loss: 19791.763672\n",
      "Train Epoch: 151 [217344/225000 (97%)] Loss: 19202.857422\n",
      "Train Epoch: 151 [219840/225000 (98%)] Loss: 19496.535156\n",
      "Train Epoch: 151 [222336/225000 (99%)] Loss: 19249.828125\n",
      "Train Epoch: 151 [224832/225000 (100%)] Loss: 18855.363281\n",
      "    epoch          : 151\n",
      "    loss           : 19260.27533229789\n",
      "    val_loss       : 19162.57050186441\n",
      "Train Epoch: 152 [192/225000 (0%)] Loss: 18652.841797\n",
      "Train Epoch: 152 [2688/225000 (1%)] Loss: 19264.791016\n",
      "Train Epoch: 152 [5184/225000 (2%)] Loss: 19579.976562\n",
      "Train Epoch: 152 [7680/225000 (3%)] Loss: 19156.003906\n",
      "Train Epoch: 152 [10176/225000 (5%)] Loss: 19362.671875\n",
      "Train Epoch: 152 [12672/225000 (6%)] Loss: 18964.136719\n",
      "Train Epoch: 152 [15168/225000 (7%)] Loss: 19122.171875\n",
      "Train Epoch: 152 [17664/225000 (8%)] Loss: 18800.011719\n",
      "Train Epoch: 152 [20160/225000 (9%)] Loss: 19381.398438\n",
      "Train Epoch: 152 [22656/225000 (10%)] Loss: 19420.074219\n",
      "Train Epoch: 152 [25152/225000 (11%)] Loss: 19524.640625\n",
      "Train Epoch: 152 [27648/225000 (12%)] Loss: 19415.337891\n",
      "Train Epoch: 152 [30144/225000 (13%)] Loss: 19503.779297\n",
      "Train Epoch: 152 [32640/225000 (15%)] Loss: 19101.400391\n",
      "Train Epoch: 152 [35136/225000 (16%)] Loss: 19405.996094\n",
      "Train Epoch: 152 [37632/225000 (17%)] Loss: 19429.152344\n",
      "Train Epoch: 152 [40128/225000 (18%)] Loss: 18900.625000\n",
      "Train Epoch: 152 [42624/225000 (19%)] Loss: 19378.234375\n",
      "Train Epoch: 152 [45120/225000 (20%)] Loss: 19163.625000\n",
      "Train Epoch: 152 [47616/225000 (21%)] Loss: 19569.835938\n",
      "Train Epoch: 152 [50112/225000 (22%)] Loss: 19035.085938\n",
      "Train Epoch: 152 [52608/225000 (23%)] Loss: 19253.480469\n",
      "Train Epoch: 152 [55104/225000 (24%)] Loss: 19454.759766\n",
      "Train Epoch: 152 [57600/225000 (26%)] Loss: 19238.500000\n",
      "Train Epoch: 152 [60096/225000 (27%)] Loss: 19484.070312\n",
      "Train Epoch: 152 [62592/225000 (28%)] Loss: 19331.761719\n",
      "Train Epoch: 152 [65088/225000 (29%)] Loss: 19239.855469\n",
      "Train Epoch: 152 [67584/225000 (30%)] Loss: 19151.933594\n",
      "Train Epoch: 152 [70080/225000 (31%)] Loss: 19099.992188\n",
      "Train Epoch: 152 [72576/225000 (32%)] Loss: 19084.138672\n",
      "Train Epoch: 152 [75072/225000 (33%)] Loss: 19468.851562\n",
      "Train Epoch: 152 [77568/225000 (34%)] Loss: 19354.710938\n",
      "Train Epoch: 152 [80064/225000 (36%)] Loss: 19427.218750\n",
      "Train Epoch: 152 [82560/225000 (37%)] Loss: 19694.587891\n",
      "Train Epoch: 152 [85056/225000 (38%)] Loss: 18985.699219\n",
      "Train Epoch: 152 [87552/225000 (39%)] Loss: 19378.890625\n",
      "Train Epoch: 152 [90048/225000 (40%)] Loss: 19554.441406\n",
      "Train Epoch: 152 [92544/225000 (41%)] Loss: 19218.296875\n",
      "Train Epoch: 152 [95040/225000 (42%)] Loss: 18821.925781\n",
      "Train Epoch: 152 [97536/225000 (43%)] Loss: 19062.949219\n",
      "Train Epoch: 152 [100032/225000 (44%)] Loss: 18697.210938\n",
      "Train Epoch: 152 [102528/225000 (46%)] Loss: 19198.964844\n",
      "Train Epoch: 152 [105024/225000 (47%)] Loss: 19361.031250\n",
      "Train Epoch: 152 [107520/225000 (48%)] Loss: 19112.343750\n",
      "Train Epoch: 152 [110016/225000 (49%)] Loss: 19207.925781\n",
      "Train Epoch: 152 [112512/225000 (50%)] Loss: 19468.101562\n",
      "Train Epoch: 152 [115008/225000 (51%)] Loss: 19255.621094\n",
      "Train Epoch: 152 [117504/225000 (52%)] Loss: 19464.740234\n",
      "Train Epoch: 152 [120000/225000 (53%)] Loss: 18954.222656\n",
      "Train Epoch: 152 [122496/225000 (54%)] Loss: 19287.230469\n",
      "Train Epoch: 152 [124992/225000 (56%)] Loss: 19545.316406\n",
      "Train Epoch: 152 [127488/225000 (57%)] Loss: 18751.900391\n",
      "Train Epoch: 152 [129984/225000 (58%)] Loss: 19392.615234\n",
      "Train Epoch: 152 [132480/225000 (59%)] Loss: 19511.488281\n",
      "Train Epoch: 152 [134976/225000 (60%)] Loss: 19156.744141\n",
      "Train Epoch: 152 [137472/225000 (61%)] Loss: 18961.328125\n",
      "Train Epoch: 152 [139968/225000 (62%)] Loss: 19585.962891\n",
      "Train Epoch: 152 [142464/225000 (63%)] Loss: 19880.925781\n",
      "Train Epoch: 152 [144960/225000 (64%)] Loss: 19413.943359\n",
      "Train Epoch: 152 [147456/225000 (66%)] Loss: 18962.773438\n",
      "Train Epoch: 152 [149952/225000 (67%)] Loss: 19455.111328\n",
      "Train Epoch: 152 [152448/225000 (68%)] Loss: 19367.921875\n",
      "Train Epoch: 152 [154944/225000 (69%)] Loss: 19005.789062\n",
      "Train Epoch: 152 [157440/225000 (70%)] Loss: 18975.949219\n",
      "Train Epoch: 152 [159936/225000 (71%)] Loss: 19259.128906\n",
      "Train Epoch: 152 [162432/225000 (72%)] Loss: 19676.599609\n",
      "Train Epoch: 152 [164928/225000 (73%)] Loss: 19377.332031\n",
      "Train Epoch: 152 [167424/225000 (74%)] Loss: 19060.546875\n",
      "Train Epoch: 152 [169920/225000 (76%)] Loss: 19254.656250\n",
      "Train Epoch: 152 [172416/225000 (77%)] Loss: 19197.125000\n",
      "Train Epoch: 152 [174912/225000 (78%)] Loss: 18767.837891\n",
      "Train Epoch: 152 [177408/225000 (79%)] Loss: 19313.304688\n",
      "Train Epoch: 152 [179904/225000 (80%)] Loss: 19025.492188\n",
      "Train Epoch: 152 [182400/225000 (81%)] Loss: 18982.498047\n",
      "Train Epoch: 152 [184896/225000 (82%)] Loss: 18879.263672\n",
      "Train Epoch: 152 [187392/225000 (83%)] Loss: 19436.996094\n",
      "Train Epoch: 152 [189888/225000 (84%)] Loss: 19125.832031\n",
      "Train Epoch: 152 [192384/225000 (86%)] Loss: 19057.261719\n",
      "Train Epoch: 152 [194880/225000 (87%)] Loss: 19051.113281\n",
      "Train Epoch: 152 [197376/225000 (88%)] Loss: 19143.042969\n",
      "Train Epoch: 152 [199872/225000 (89%)] Loss: 18947.855469\n",
      "Train Epoch: 152 [202368/225000 (90%)] Loss: 19068.597656\n",
      "Train Epoch: 152 [204864/225000 (91%)] Loss: 19585.480469\n",
      "Train Epoch: 152 [207360/225000 (92%)] Loss: 19047.613281\n",
      "Train Epoch: 152 [209856/225000 (93%)] Loss: 19495.109375\n",
      "Train Epoch: 152 [212352/225000 (94%)] Loss: 19485.744141\n",
      "Train Epoch: 152 [214848/225000 (95%)] Loss: 19533.070312\n",
      "Train Epoch: 152 [217344/225000 (97%)] Loss: 19239.677734\n",
      "Train Epoch: 152 [219840/225000 (98%)] Loss: 19092.033203\n",
      "Train Epoch: 152 [222336/225000 (99%)] Loss: 19598.265625\n",
      "Train Epoch: 152 [224832/225000 (100%)] Loss: 19037.751953\n",
      "    epoch          : 152\n",
      "    loss           : 19254.09118360708\n",
      "    val_loss       : 19156.843274152914\n",
      "Train Epoch: 153 [192/225000 (0%)] Loss: 19728.937500\n",
      "Train Epoch: 153 [2688/225000 (1%)] Loss: 19331.972656\n",
      "Train Epoch: 153 [5184/225000 (2%)] Loss: 18730.568359\n",
      "Train Epoch: 153 [7680/225000 (3%)] Loss: 19499.781250\n",
      "Train Epoch: 153 [10176/225000 (5%)] Loss: 19254.533203\n",
      "Train Epoch: 153 [12672/225000 (6%)] Loss: 19369.144531\n",
      "Train Epoch: 153 [15168/225000 (7%)] Loss: 19569.523438\n",
      "Train Epoch: 153 [17664/225000 (8%)] Loss: 19384.722656\n",
      "Train Epoch: 153 [20160/225000 (9%)] Loss: 19479.054688\n",
      "Train Epoch: 153 [22656/225000 (10%)] Loss: 18627.988281\n",
      "Train Epoch: 153 [25152/225000 (11%)] Loss: 19082.613281\n",
      "Train Epoch: 153 [27648/225000 (12%)] Loss: 19319.453125\n",
      "Train Epoch: 153 [30144/225000 (13%)] Loss: 19279.097656\n",
      "Train Epoch: 153 [32640/225000 (15%)] Loss: 19324.972656\n",
      "Train Epoch: 153 [35136/225000 (16%)] Loss: 19703.222656\n",
      "Train Epoch: 153 [37632/225000 (17%)] Loss: 18916.382812\n",
      "Train Epoch: 153 [40128/225000 (18%)] Loss: 19311.613281\n",
      "Train Epoch: 153 [42624/225000 (19%)] Loss: 19052.031250\n",
      "Train Epoch: 153 [45120/225000 (20%)] Loss: 19157.773438\n",
      "Train Epoch: 153 [47616/225000 (21%)] Loss: 19356.673828\n",
      "Train Epoch: 153 [50112/225000 (22%)] Loss: 18844.042969\n",
      "Train Epoch: 153 [52608/225000 (23%)] Loss: 19175.863281\n",
      "Train Epoch: 153 [55104/225000 (24%)] Loss: 19110.468750\n",
      "Train Epoch: 153 [57600/225000 (26%)] Loss: 19193.203125\n",
      "Train Epoch: 153 [60096/225000 (27%)] Loss: 24101.281250\n",
      "Train Epoch: 153 [62592/225000 (28%)] Loss: 19516.261719\n",
      "Train Epoch: 153 [65088/225000 (29%)] Loss: 19500.593750\n",
      "Train Epoch: 153 [67584/225000 (30%)] Loss: 19629.685547\n",
      "Train Epoch: 153 [70080/225000 (31%)] Loss: 19732.335938\n",
      "Train Epoch: 153 [72576/225000 (32%)] Loss: 19776.218750\n",
      "Train Epoch: 153 [75072/225000 (33%)] Loss: 19081.757812\n",
      "Train Epoch: 153 [77568/225000 (34%)] Loss: 22675.925781\n",
      "Train Epoch: 153 [80064/225000 (36%)] Loss: 19779.167969\n",
      "Train Epoch: 153 [82560/225000 (37%)] Loss: 19006.500000\n",
      "Train Epoch: 153 [85056/225000 (38%)] Loss: 19247.730469\n",
      "Train Epoch: 153 [87552/225000 (39%)] Loss: 19439.167969\n",
      "Train Epoch: 153 [90048/225000 (40%)] Loss: 19464.982422\n",
      "Train Epoch: 153 [92544/225000 (41%)] Loss: 19171.072266\n",
      "Train Epoch: 153 [95040/225000 (42%)] Loss: 18915.210938\n",
      "Train Epoch: 153 [97536/225000 (43%)] Loss: 19161.550781\n",
      "Train Epoch: 153 [100032/225000 (44%)] Loss: 19609.214844\n",
      "Train Epoch: 153 [102528/225000 (46%)] Loss: 19616.562500\n",
      "Train Epoch: 153 [105024/225000 (47%)] Loss: 19206.671875\n",
      "Train Epoch: 153 [107520/225000 (48%)] Loss: 18720.339844\n",
      "Train Epoch: 153 [110016/225000 (49%)] Loss: 19006.386719\n",
      "Train Epoch: 153 [112512/225000 (50%)] Loss: 19046.488281\n",
      "Train Epoch: 153 [115008/225000 (51%)] Loss: 19504.058594\n",
      "Train Epoch: 153 [117504/225000 (52%)] Loss: 19482.785156\n",
      "Train Epoch: 153 [120000/225000 (53%)] Loss: 19118.974609\n",
      "Train Epoch: 153 [122496/225000 (54%)] Loss: 19409.765625\n",
      "Train Epoch: 153 [124992/225000 (56%)] Loss: 19505.218750\n",
      "Train Epoch: 153 [127488/225000 (57%)] Loss: 19764.574219\n",
      "Train Epoch: 153 [129984/225000 (58%)] Loss: 19293.808594\n",
      "Train Epoch: 153 [132480/225000 (59%)] Loss: 19392.835938\n",
      "Train Epoch: 153 [134976/225000 (60%)] Loss: 19726.773438\n",
      "Train Epoch: 153 [137472/225000 (61%)] Loss: 19692.574219\n",
      "Train Epoch: 153 [139968/225000 (62%)] Loss: 19080.472656\n",
      "Train Epoch: 153 [142464/225000 (63%)] Loss: 19080.873047\n",
      "Train Epoch: 153 [144960/225000 (64%)] Loss: 19081.347656\n",
      "Train Epoch: 153 [147456/225000 (66%)] Loss: 19316.244141\n",
      "Train Epoch: 153 [149952/225000 (67%)] Loss: 19164.117188\n",
      "Train Epoch: 153 [152448/225000 (68%)] Loss: 19225.183594\n",
      "Train Epoch: 153 [154944/225000 (69%)] Loss: 18587.039062\n",
      "Train Epoch: 153 [157440/225000 (70%)] Loss: 19835.761719\n",
      "Train Epoch: 153 [159936/225000 (71%)] Loss: 19306.935547\n",
      "Train Epoch: 153 [162432/225000 (72%)] Loss: 19370.578125\n",
      "Train Epoch: 153 [164928/225000 (73%)] Loss: 19053.476562\n",
      "Train Epoch: 153 [167424/225000 (74%)] Loss: 19631.054688\n",
      "Train Epoch: 153 [169920/225000 (76%)] Loss: 19144.253906\n",
      "Train Epoch: 153 [172416/225000 (77%)] Loss: 19114.257812\n",
      "Train Epoch: 153 [174912/225000 (78%)] Loss: 19082.464844\n",
      "Train Epoch: 153 [177408/225000 (79%)] Loss: 19352.101562\n",
      "Train Epoch: 153 [179904/225000 (80%)] Loss: 19442.339844\n",
      "Train Epoch: 153 [182400/225000 (81%)] Loss: 19247.513672\n",
      "Train Epoch: 153 [184896/225000 (82%)] Loss: 19191.978516\n",
      "Train Epoch: 153 [187392/225000 (83%)] Loss: 19056.718750\n",
      "Train Epoch: 153 [189888/225000 (84%)] Loss: 18985.203125\n",
      "Train Epoch: 153 [192384/225000 (86%)] Loss: 18914.583984\n",
      "Train Epoch: 153 [194880/225000 (87%)] Loss: 19565.910156\n",
      "Train Epoch: 153 [197376/225000 (88%)] Loss: 19482.271484\n",
      "Train Epoch: 153 [199872/225000 (89%)] Loss: 19336.783203\n",
      "Train Epoch: 153 [202368/225000 (90%)] Loss: 19359.367188\n",
      "Train Epoch: 153 [204864/225000 (91%)] Loss: 19331.671875\n",
      "Train Epoch: 153 [207360/225000 (92%)] Loss: 19259.445312\n",
      "Train Epoch: 153 [209856/225000 (93%)] Loss: 19646.960938\n",
      "Train Epoch: 153 [212352/225000 (94%)] Loss: 19152.261719\n",
      "Train Epoch: 153 [214848/225000 (95%)] Loss: 19015.691406\n",
      "Train Epoch: 153 [217344/225000 (97%)] Loss: 19001.392578\n",
      "Train Epoch: 153 [219840/225000 (98%)] Loss: 18616.687500\n",
      "Train Epoch: 153 [222336/225000 (99%)] Loss: 19039.777344\n",
      "Train Epoch: 153 [224832/225000 (100%)] Loss: 19331.455078\n",
      "    epoch          : 153\n",
      "    loss           : 19274.223569485923\n",
      "    val_loss       : 19157.637584163942\n",
      "Train Epoch: 154 [192/225000 (0%)] Loss: 19144.085938\n",
      "Train Epoch: 154 [2688/225000 (1%)] Loss: 19149.890625\n",
      "Train Epoch: 154 [5184/225000 (2%)] Loss: 19867.582031\n",
      "Train Epoch: 154 [7680/225000 (3%)] Loss: 19189.302734\n",
      "Train Epoch: 154 [10176/225000 (5%)] Loss: 19353.281250\n",
      "Train Epoch: 154 [12672/225000 (6%)] Loss: 19220.849609\n",
      "Train Epoch: 154 [15168/225000 (7%)] Loss: 19366.585938\n",
      "Train Epoch: 154 [17664/225000 (8%)] Loss: 19081.527344\n",
      "Train Epoch: 154 [20160/225000 (9%)] Loss: 19220.812500\n",
      "Train Epoch: 154 [22656/225000 (10%)] Loss: 18859.009766\n",
      "Train Epoch: 154 [25152/225000 (11%)] Loss: 19328.777344\n",
      "Train Epoch: 154 [27648/225000 (12%)] Loss: 19675.898438\n",
      "Train Epoch: 154 [30144/225000 (13%)] Loss: 18776.359375\n",
      "Train Epoch: 154 [32640/225000 (15%)] Loss: 19051.455078\n",
      "Train Epoch: 154 [35136/225000 (16%)] Loss: 18957.433594\n",
      "Train Epoch: 154 [37632/225000 (17%)] Loss: 19104.603516\n",
      "Train Epoch: 154 [40128/225000 (18%)] Loss: 19107.291016\n",
      "Train Epoch: 154 [42624/225000 (19%)] Loss: 19605.777344\n",
      "Train Epoch: 154 [45120/225000 (20%)] Loss: 19744.972656\n",
      "Train Epoch: 154 [47616/225000 (21%)] Loss: 19300.363281\n",
      "Train Epoch: 154 [50112/225000 (22%)] Loss: 19060.669922\n",
      "Train Epoch: 154 [52608/225000 (23%)] Loss: 18717.597656\n",
      "Train Epoch: 154 [55104/225000 (24%)] Loss: 18859.398438\n",
      "Train Epoch: 154 [57600/225000 (26%)] Loss: 19078.042969\n",
      "Train Epoch: 154 [60096/225000 (27%)] Loss: 19355.986328\n",
      "Train Epoch: 154 [62592/225000 (28%)] Loss: 19244.947266\n",
      "Train Epoch: 154 [65088/225000 (29%)] Loss: 19206.058594\n",
      "Train Epoch: 154 [67584/225000 (30%)] Loss: 19191.082031\n",
      "Train Epoch: 154 [70080/225000 (31%)] Loss: 18986.753906\n",
      "Train Epoch: 154 [72576/225000 (32%)] Loss: 18902.646484\n",
      "Train Epoch: 154 [75072/225000 (33%)] Loss: 19633.845703\n",
      "Train Epoch: 154 [77568/225000 (34%)] Loss: 19347.904297\n",
      "Train Epoch: 154 [80064/225000 (36%)] Loss: 19390.199219\n",
      "Train Epoch: 154 [82560/225000 (37%)] Loss: 19112.492188\n",
      "Train Epoch: 154 [85056/225000 (38%)] Loss: 18833.695312\n",
      "Train Epoch: 154 [87552/225000 (39%)] Loss: 19144.683594\n",
      "Train Epoch: 154 [90048/225000 (40%)] Loss: 19760.039062\n",
      "Train Epoch: 154 [92544/225000 (41%)] Loss: 18862.705078\n",
      "Train Epoch: 154 [95040/225000 (42%)] Loss: 19052.185547\n",
      "Train Epoch: 154 [97536/225000 (43%)] Loss: 18957.953125\n",
      "Train Epoch: 154 [100032/225000 (44%)] Loss: 19045.480469\n",
      "Train Epoch: 154 [102528/225000 (46%)] Loss: 19361.613281\n",
      "Train Epoch: 154 [105024/225000 (47%)] Loss: 19307.847656\n",
      "Train Epoch: 154 [107520/225000 (48%)] Loss: 19015.802734\n",
      "Train Epoch: 154 [110016/225000 (49%)] Loss: 19187.435547\n",
      "Train Epoch: 154 [112512/225000 (50%)] Loss: 19767.632812\n",
      "Train Epoch: 154 [115008/225000 (51%)] Loss: 19434.460938\n",
      "Train Epoch: 154 [117504/225000 (52%)] Loss: 19102.437500\n",
      "Train Epoch: 154 [120000/225000 (53%)] Loss: 19045.173828\n",
      "Train Epoch: 154 [122496/225000 (54%)] Loss: 19318.730469\n",
      "Train Epoch: 154 [124992/225000 (56%)] Loss: 19411.050781\n",
      "Train Epoch: 154 [127488/225000 (57%)] Loss: 19291.156250\n",
      "Train Epoch: 154 [129984/225000 (58%)] Loss: 19490.712891\n",
      "Train Epoch: 154 [132480/225000 (59%)] Loss: 18976.480469\n",
      "Train Epoch: 154 [134976/225000 (60%)] Loss: 19652.140625\n",
      "Train Epoch: 154 [137472/225000 (61%)] Loss: 19982.539062\n",
      "Train Epoch: 154 [139968/225000 (62%)] Loss: 19660.773438\n",
      "Train Epoch: 154 [142464/225000 (63%)] Loss: 18970.925781\n",
      "Train Epoch: 154 [144960/225000 (64%)] Loss: 19115.320312\n",
      "Train Epoch: 154 [147456/225000 (66%)] Loss: 18866.003906\n",
      "Train Epoch: 154 [149952/225000 (67%)] Loss: 19364.884766\n",
      "Train Epoch: 154 [152448/225000 (68%)] Loss: 18853.707031\n",
      "Train Epoch: 154 [154944/225000 (69%)] Loss: 19324.445312\n",
      "Train Epoch: 154 [157440/225000 (70%)] Loss: 18733.949219\n",
      "Train Epoch: 154 [159936/225000 (71%)] Loss: 18878.800781\n",
      "Train Epoch: 154 [162432/225000 (72%)] Loss: 19654.125000\n",
      "Train Epoch: 154 [164928/225000 (73%)] Loss: 19664.308594\n",
      "Train Epoch: 154 [167424/225000 (74%)] Loss: 18915.767578\n",
      "Train Epoch: 154 [169920/225000 (76%)] Loss: 19326.779297\n",
      "Train Epoch: 154 [172416/225000 (77%)] Loss: 19069.367188\n",
      "Train Epoch: 154 [174912/225000 (78%)] Loss: 18591.656250\n",
      "Train Epoch: 154 [177408/225000 (79%)] Loss: 19286.392578\n",
      "Train Epoch: 154 [179904/225000 (80%)] Loss: 18927.673828\n",
      "Train Epoch: 154 [182400/225000 (81%)] Loss: 19289.767578\n",
      "Train Epoch: 154 [184896/225000 (82%)] Loss: 19147.701172\n",
      "Train Epoch: 154 [187392/225000 (83%)] Loss: 19325.417969\n",
      "Train Epoch: 154 [189888/225000 (84%)] Loss: 18916.857422\n",
      "Train Epoch: 154 [192384/225000 (86%)] Loss: 19440.269531\n",
      "Train Epoch: 154 [194880/225000 (87%)] Loss: 19148.269531\n",
      "Train Epoch: 154 [197376/225000 (88%)] Loss: 19306.365234\n",
      "Train Epoch: 154 [199872/225000 (89%)] Loss: 18569.531250\n",
      "Train Epoch: 154 [202368/225000 (90%)] Loss: 18865.664062\n",
      "Train Epoch: 154 [204864/225000 (91%)] Loss: 19143.171875\n",
      "Train Epoch: 154 [207360/225000 (92%)] Loss: 19249.390625\n",
      "Train Epoch: 154 [209856/225000 (93%)] Loss: 19831.958984\n",
      "Train Epoch: 154 [212352/225000 (94%)] Loss: 19159.900391\n",
      "Train Epoch: 154 [214848/225000 (95%)] Loss: 19549.671875\n",
      "Train Epoch: 154 [217344/225000 (97%)] Loss: 19292.886719\n",
      "Train Epoch: 154 [219840/225000 (98%)] Loss: 18875.386719\n",
      "Train Epoch: 154 [222336/225000 (99%)] Loss: 19708.746094\n",
      "Train Epoch: 154 [224832/225000 (100%)] Loss: 19313.505859\n",
      "    epoch          : 154\n",
      "    loss           : 19256.116677554393\n",
      "    val_loss       : 19149.90193782326\n",
      "Train Epoch: 155 [192/225000 (0%)] Loss: 19086.742188\n",
      "Train Epoch: 155 [2688/225000 (1%)] Loss: 19045.453125\n",
      "Train Epoch: 155 [5184/225000 (2%)] Loss: 24584.511719\n",
      "Train Epoch: 155 [7680/225000 (3%)] Loss: 18946.445312\n",
      "Train Epoch: 155 [10176/225000 (5%)] Loss: 19025.410156\n",
      "Train Epoch: 155 [12672/225000 (6%)] Loss: 19153.339844\n",
      "Train Epoch: 155 [15168/225000 (7%)] Loss: 19280.087891\n",
      "Train Epoch: 155 [17664/225000 (8%)] Loss: 19318.289062\n",
      "Train Epoch: 155 [20160/225000 (9%)] Loss: 18988.046875\n",
      "Train Epoch: 155 [22656/225000 (10%)] Loss: 18993.130859\n",
      "Train Epoch: 155 [25152/225000 (11%)] Loss: 19297.210938\n",
      "Train Epoch: 155 [27648/225000 (12%)] Loss: 19226.726562\n",
      "Train Epoch: 155 [30144/225000 (13%)] Loss: 19154.541016\n",
      "Train Epoch: 155 [32640/225000 (15%)] Loss: 18985.203125\n",
      "Train Epoch: 155 [35136/225000 (16%)] Loss: 19343.628906\n",
      "Train Epoch: 155 [37632/225000 (17%)] Loss: 19076.478516\n",
      "Train Epoch: 155 [40128/225000 (18%)] Loss: 19255.826172\n",
      "Train Epoch: 155 [42624/225000 (19%)] Loss: 18738.683594\n",
      "Train Epoch: 155 [45120/225000 (20%)] Loss: 19076.041016\n",
      "Train Epoch: 155 [47616/225000 (21%)] Loss: 19205.427734\n",
      "Train Epoch: 155 [50112/225000 (22%)] Loss: 19484.662109\n",
      "Train Epoch: 155 [52608/225000 (23%)] Loss: 19332.828125\n",
      "Train Epoch: 155 [55104/225000 (24%)] Loss: 19721.347656\n",
      "Train Epoch: 155 [57600/225000 (26%)] Loss: 19122.224609\n",
      "Train Epoch: 155 [60096/225000 (27%)] Loss: 19063.775391\n",
      "Train Epoch: 155 [62592/225000 (28%)] Loss: 19636.605469\n",
      "Train Epoch: 155 [65088/225000 (29%)] Loss: 19094.457031\n",
      "Train Epoch: 155 [67584/225000 (30%)] Loss: 19081.556641\n",
      "Train Epoch: 155 [70080/225000 (31%)] Loss: 19671.996094\n",
      "Train Epoch: 155 [72576/225000 (32%)] Loss: 19406.921875\n",
      "Train Epoch: 155 [75072/225000 (33%)] Loss: 19256.771484\n",
      "Train Epoch: 155 [77568/225000 (34%)] Loss: 19475.378906\n",
      "Train Epoch: 155 [80064/225000 (36%)] Loss: 19304.537109\n",
      "Train Epoch: 155 [82560/225000 (37%)] Loss: 19864.892578\n",
      "Train Epoch: 155 [85056/225000 (38%)] Loss: 19479.660156\n",
      "Train Epoch: 155 [87552/225000 (39%)] Loss: 20214.355469\n",
      "Train Epoch: 155 [90048/225000 (40%)] Loss: 19598.408203\n",
      "Train Epoch: 155 [92544/225000 (41%)] Loss: 19698.640625\n",
      "Train Epoch: 155 [95040/225000 (42%)] Loss: 19172.648438\n",
      "Train Epoch: 155 [97536/225000 (43%)] Loss: 19321.521484\n",
      "Train Epoch: 155 [100032/225000 (44%)] Loss: 19128.800781\n",
      "Train Epoch: 155 [102528/225000 (46%)] Loss: 19348.683594\n",
      "Train Epoch: 155 [105024/225000 (47%)] Loss: 19357.398438\n",
      "Train Epoch: 155 [107520/225000 (48%)] Loss: 19175.365234\n",
      "Train Epoch: 155 [110016/225000 (49%)] Loss: 19045.765625\n",
      "Train Epoch: 155 [112512/225000 (50%)] Loss: 19374.882812\n",
      "Train Epoch: 155 [115008/225000 (51%)] Loss: 19521.503906\n",
      "Train Epoch: 155 [117504/225000 (52%)] Loss: 19403.283203\n",
      "Train Epoch: 155 [120000/225000 (53%)] Loss: 19634.384766\n",
      "Train Epoch: 155 [122496/225000 (54%)] Loss: 19707.000000\n",
      "Train Epoch: 155 [124992/225000 (56%)] Loss: 18992.835938\n",
      "Train Epoch: 155 [127488/225000 (57%)] Loss: 19279.267578\n",
      "Train Epoch: 155 [129984/225000 (58%)] Loss: 19665.933594\n",
      "Train Epoch: 155 [132480/225000 (59%)] Loss: 19275.263672\n",
      "Train Epoch: 155 [134976/225000 (60%)] Loss: 18875.707031\n",
      "Train Epoch: 155 [137472/225000 (61%)] Loss: 19082.734375\n",
      "Train Epoch: 155 [139968/225000 (62%)] Loss: 19070.421875\n",
      "Train Epoch: 155 [142464/225000 (63%)] Loss: 19014.201172\n",
      "Train Epoch: 155 [144960/225000 (64%)] Loss: 19123.166016\n",
      "Train Epoch: 155 [147456/225000 (66%)] Loss: 18866.373047\n",
      "Train Epoch: 155 [149952/225000 (67%)] Loss: 19272.623047\n",
      "Train Epoch: 155 [152448/225000 (68%)] Loss: 19628.941406\n",
      "Train Epoch: 155 [154944/225000 (69%)] Loss: 19414.716797\n",
      "Train Epoch: 155 [157440/225000 (70%)] Loss: 19456.296875\n",
      "Train Epoch: 155 [159936/225000 (71%)] Loss: 19206.812500\n",
      "Train Epoch: 155 [162432/225000 (72%)] Loss: 19507.527344\n",
      "Train Epoch: 155 [164928/225000 (73%)] Loss: 19527.710938\n",
      "Train Epoch: 155 [167424/225000 (74%)] Loss: 19356.808594\n",
      "Train Epoch: 155 [169920/225000 (76%)] Loss: 19652.259766\n",
      "Train Epoch: 155 [172416/225000 (77%)] Loss: 19494.683594\n",
      "Train Epoch: 155 [174912/225000 (78%)] Loss: 18942.767578\n",
      "Train Epoch: 155 [177408/225000 (79%)] Loss: 19568.148438\n",
      "Train Epoch: 155 [179904/225000 (80%)] Loss: 19214.011719\n",
      "Train Epoch: 155 [182400/225000 (81%)] Loss: 18958.564453\n",
      "Train Epoch: 155 [184896/225000 (82%)] Loss: 18876.593750\n",
      "Train Epoch: 155 [187392/225000 (83%)] Loss: 19598.357422\n",
      "Train Epoch: 155 [189888/225000 (84%)] Loss: 18800.445312\n",
      "Train Epoch: 155 [192384/225000 (86%)] Loss: 19016.449219\n",
      "Train Epoch: 155 [194880/225000 (87%)] Loss: 18697.031250\n",
      "Train Epoch: 155 [197376/225000 (88%)] Loss: 19127.388672\n",
      "Train Epoch: 155 [199872/225000 (89%)] Loss: 19118.851562\n",
      "Train Epoch: 155 [202368/225000 (90%)] Loss: 19153.521484\n",
      "Train Epoch: 155 [204864/225000 (91%)] Loss: 19671.871094\n",
      "Train Epoch: 155 [207360/225000 (92%)] Loss: 19720.507812\n",
      "Train Epoch: 155 [209856/225000 (93%)] Loss: 19136.484375\n",
      "Train Epoch: 155 [212352/225000 (94%)] Loss: 19585.929688\n",
      "Train Epoch: 155 [214848/225000 (95%)] Loss: 19311.892578\n",
      "Train Epoch: 155 [217344/225000 (97%)] Loss: 19215.734375\n",
      "Train Epoch: 155 [219840/225000 (98%)] Loss: 19139.515625\n",
      "Train Epoch: 155 [222336/225000 (99%)] Loss: 19127.599609\n",
      "Train Epoch: 155 [224832/225000 (100%)] Loss: 18736.417969\n",
      "    epoch          : 155\n",
      "    loss           : 19248.68874986668\n",
      "    val_loss       : 19145.501430653432\n",
      "Train Epoch: 156 [192/225000 (0%)] Loss: 19139.789062\n",
      "Train Epoch: 156 [2688/225000 (1%)] Loss: 19859.933594\n",
      "Train Epoch: 156 [5184/225000 (2%)] Loss: 19651.765625\n",
      "Train Epoch: 156 [7680/225000 (3%)] Loss: 19448.714844\n",
      "Train Epoch: 156 [10176/225000 (5%)] Loss: 19698.589844\n",
      "Train Epoch: 156 [12672/225000 (6%)] Loss: 19416.101562\n",
      "Train Epoch: 156 [15168/225000 (7%)] Loss: 19448.148438\n",
      "Train Epoch: 156 [17664/225000 (8%)] Loss: 19941.142578\n",
      "Train Epoch: 156 [20160/225000 (9%)] Loss: 19371.316406\n",
      "Train Epoch: 156 [22656/225000 (10%)] Loss: 19142.136719\n",
      "Train Epoch: 156 [25152/225000 (11%)] Loss: 19461.078125\n",
      "Train Epoch: 156 [27648/225000 (12%)] Loss: 19150.445312\n",
      "Train Epoch: 156 [30144/225000 (13%)] Loss: 19424.078125\n",
      "Train Epoch: 156 [32640/225000 (15%)] Loss: 19136.085938\n",
      "Train Epoch: 156 [35136/225000 (16%)] Loss: 18789.691406\n",
      "Train Epoch: 156 [37632/225000 (17%)] Loss: 19696.246094\n",
      "Train Epoch: 156 [40128/225000 (18%)] Loss: 19033.951172\n",
      "Train Epoch: 156 [42624/225000 (19%)] Loss: 19046.267578\n",
      "Train Epoch: 156 [45120/225000 (20%)] Loss: 19238.464844\n",
      "Train Epoch: 156 [47616/225000 (21%)] Loss: 19261.351562\n",
      "Train Epoch: 156 [50112/225000 (22%)] Loss: 19261.605469\n",
      "Train Epoch: 156 [52608/225000 (23%)] Loss: 19508.191406\n",
      "Train Epoch: 156 [55104/225000 (24%)] Loss: 19111.917969\n",
      "Train Epoch: 156 [57600/225000 (26%)] Loss: 19434.974609\n",
      "Train Epoch: 156 [60096/225000 (27%)] Loss: 19121.998047\n",
      "Train Epoch: 156 [62592/225000 (28%)] Loss: 19390.181641\n",
      "Train Epoch: 156 [65088/225000 (29%)] Loss: 19335.421875\n",
      "Train Epoch: 156 [67584/225000 (30%)] Loss: 19527.488281\n",
      "Train Epoch: 156 [70080/225000 (31%)] Loss: 19451.697266\n",
      "Train Epoch: 156 [72576/225000 (32%)] Loss: 18919.382812\n",
      "Train Epoch: 156 [75072/225000 (33%)] Loss: 19222.390625\n",
      "Train Epoch: 156 [77568/225000 (34%)] Loss: 19205.892578\n",
      "Train Epoch: 156 [80064/225000 (36%)] Loss: 19733.867188\n",
      "Train Epoch: 156 [82560/225000 (37%)] Loss: 19497.207031\n",
      "Train Epoch: 156 [85056/225000 (38%)] Loss: 19605.109375\n",
      "Train Epoch: 156 [87552/225000 (39%)] Loss: 19487.398438\n",
      "Train Epoch: 156 [90048/225000 (40%)] Loss: 19196.730469\n",
      "Train Epoch: 156 [92544/225000 (41%)] Loss: 19123.128906\n",
      "Train Epoch: 156 [95040/225000 (42%)] Loss: 19291.714844\n",
      "Train Epoch: 156 [97536/225000 (43%)] Loss: 19073.941406\n",
      "Train Epoch: 156 [100032/225000 (44%)] Loss: 19101.070312\n",
      "Train Epoch: 156 [102528/225000 (46%)] Loss: 19543.246094\n",
      "Train Epoch: 156 [105024/225000 (47%)] Loss: 19313.505859\n",
      "Train Epoch: 156 [107520/225000 (48%)] Loss: 19571.597656\n",
      "Train Epoch: 156 [110016/225000 (49%)] Loss: 19212.835938\n",
      "Train Epoch: 156 [112512/225000 (50%)] Loss: 18929.730469\n",
      "Train Epoch: 156 [115008/225000 (51%)] Loss: 19106.396484\n",
      "Train Epoch: 156 [117504/225000 (52%)] Loss: 19211.417969\n",
      "Train Epoch: 156 [120000/225000 (53%)] Loss: 18815.917969\n",
      "Train Epoch: 156 [122496/225000 (54%)] Loss: 19423.910156\n",
      "Train Epoch: 156 [124992/225000 (56%)] Loss: 19415.156250\n",
      "Train Epoch: 156 [127488/225000 (57%)] Loss: 19072.671875\n",
      "Train Epoch: 156 [129984/225000 (58%)] Loss: 18975.820312\n",
      "Train Epoch: 156 [132480/225000 (59%)] Loss: 19233.013672\n",
      "Train Epoch: 156 [134976/225000 (60%)] Loss: 19144.945312\n",
      "Train Epoch: 156 [137472/225000 (61%)] Loss: 19432.412109\n",
      "Train Epoch: 156 [139968/225000 (62%)] Loss: 19534.287109\n",
      "Train Epoch: 156 [142464/225000 (63%)] Loss: 19151.636719\n",
      "Train Epoch: 156 [144960/225000 (64%)] Loss: 18617.246094\n",
      "Train Epoch: 156 [147456/225000 (66%)] Loss: 19202.609375\n",
      "Train Epoch: 156 [149952/225000 (67%)] Loss: 19647.808594\n",
      "Train Epoch: 156 [152448/225000 (68%)] Loss: 19358.699219\n",
      "Train Epoch: 156 [154944/225000 (69%)] Loss: 19483.210938\n",
      "Train Epoch: 156 [157440/225000 (70%)] Loss: 19511.593750\n",
      "Train Epoch: 156 [159936/225000 (71%)] Loss: 19158.015625\n",
      "Train Epoch: 156 [162432/225000 (72%)] Loss: 19683.339844\n",
      "Train Epoch: 156 [164928/225000 (73%)] Loss: 19024.500000\n",
      "Train Epoch: 156 [167424/225000 (74%)] Loss: 19077.214844\n",
      "Train Epoch: 156 [169920/225000 (76%)] Loss: 19070.734375\n",
      "Train Epoch: 156 [172416/225000 (77%)] Loss: 18611.105469\n",
      "Train Epoch: 156 [174912/225000 (78%)] Loss: 19541.289062\n",
      "Train Epoch: 156 [177408/225000 (79%)] Loss: 19470.126953\n",
      "Train Epoch: 156 [179904/225000 (80%)] Loss: 19436.886719\n",
      "Train Epoch: 156 [182400/225000 (81%)] Loss: 19238.992188\n",
      "Train Epoch: 156 [184896/225000 (82%)] Loss: 18898.675781\n",
      "Train Epoch: 156 [187392/225000 (83%)] Loss: 19377.332031\n",
      "Train Epoch: 156 [189888/225000 (84%)] Loss: 18677.246094\n",
      "Train Epoch: 156 [192384/225000 (86%)] Loss: 19164.945312\n",
      "Train Epoch: 156 [194880/225000 (87%)] Loss: 19910.775391\n",
      "Train Epoch: 156 [197376/225000 (88%)] Loss: 19087.425781\n",
      "Train Epoch: 156 [199872/225000 (89%)] Loss: 19098.781250\n",
      "Train Epoch: 156 [202368/225000 (90%)] Loss: 19391.121094\n",
      "Train Epoch: 156 [204864/225000 (91%)] Loss: 18888.960938\n",
      "Train Epoch: 156 [207360/225000 (92%)] Loss: 19638.167969\n",
      "Train Epoch: 156 [209856/225000 (93%)] Loss: 19062.832031\n",
      "Train Epoch: 156 [212352/225000 (94%)] Loss: 19229.566406\n",
      "Train Epoch: 156 [214848/225000 (95%)] Loss: 18892.726562\n",
      "Train Epoch: 156 [217344/225000 (97%)] Loss: 19117.236328\n",
      "Train Epoch: 156 [219840/225000 (98%)] Loss: 19272.785156\n",
      "Train Epoch: 156 [222336/225000 (99%)] Loss: 19442.121094\n",
      "Train Epoch: 156 [224832/225000 (100%)] Loss: 19065.443359\n",
      "    epoch          : 156\n",
      "    loss           : 19241.325406956592\n",
      "    val_loss       : 19212.588582790533\n",
      "Train Epoch: 157 [192/225000 (0%)] Loss: 19515.623047\n",
      "Train Epoch: 157 [2688/225000 (1%)] Loss: 19019.429688\n",
      "Train Epoch: 157 [5184/225000 (2%)] Loss: 19643.242188\n",
      "Train Epoch: 157 [7680/225000 (3%)] Loss: 19840.710938\n",
      "Train Epoch: 157 [10176/225000 (5%)] Loss: 18832.550781\n",
      "Train Epoch: 157 [12672/225000 (6%)] Loss: 19032.492188\n",
      "Train Epoch: 157 [15168/225000 (7%)] Loss: 19495.255859\n",
      "Train Epoch: 157 [17664/225000 (8%)] Loss: 19051.207031\n",
      "Train Epoch: 157 [20160/225000 (9%)] Loss: 19415.675781\n",
      "Train Epoch: 157 [22656/225000 (10%)] Loss: 19234.152344\n",
      "Train Epoch: 157 [25152/225000 (11%)] Loss: 19769.699219\n",
      "Train Epoch: 157 [27648/225000 (12%)] Loss: 18909.730469\n",
      "Train Epoch: 157 [30144/225000 (13%)] Loss: 19079.541016\n",
      "Train Epoch: 157 [32640/225000 (15%)] Loss: 19200.761719\n",
      "Train Epoch: 157 [35136/225000 (16%)] Loss: 19797.949219\n",
      "Train Epoch: 157 [37632/225000 (17%)] Loss: 19198.710938\n",
      "Train Epoch: 157 [40128/225000 (18%)] Loss: 19403.554688\n",
      "Train Epoch: 157 [42624/225000 (19%)] Loss: 19254.541016\n",
      "Train Epoch: 157 [45120/225000 (20%)] Loss: 18484.228516\n",
      "Train Epoch: 157 [47616/225000 (21%)] Loss: 18932.648438\n",
      "Train Epoch: 157 [50112/225000 (22%)] Loss: 19460.179688\n",
      "Train Epoch: 157 [52608/225000 (23%)] Loss: 18805.859375\n",
      "Train Epoch: 157 [55104/225000 (24%)] Loss: 19808.490234\n",
      "Train Epoch: 157 [57600/225000 (26%)] Loss: 19119.890625\n",
      "Train Epoch: 157 [60096/225000 (27%)] Loss: 19637.031250\n",
      "Train Epoch: 157 [62592/225000 (28%)] Loss: 19327.289062\n",
      "Train Epoch: 157 [65088/225000 (29%)] Loss: 19448.457031\n",
      "Train Epoch: 157 [67584/225000 (30%)] Loss: 18871.615234\n",
      "Train Epoch: 157 [70080/225000 (31%)] Loss: 19510.947266\n",
      "Train Epoch: 157 [72576/225000 (32%)] Loss: 18859.625000\n",
      "Train Epoch: 157 [75072/225000 (33%)] Loss: 19692.806641\n",
      "Train Epoch: 157 [77568/225000 (34%)] Loss: 19106.875000\n",
      "Train Epoch: 157 [80064/225000 (36%)] Loss: 19239.964844\n",
      "Train Epoch: 157 [82560/225000 (37%)] Loss: 19374.412109\n",
      "Train Epoch: 157 [85056/225000 (38%)] Loss: 19402.447266\n",
      "Train Epoch: 157 [87552/225000 (39%)] Loss: 19208.896484\n",
      "Train Epoch: 157 [90048/225000 (40%)] Loss: 18762.806641\n",
      "Train Epoch: 157 [92544/225000 (41%)] Loss: 19195.714844\n",
      "Train Epoch: 157 [95040/225000 (42%)] Loss: 19082.644531\n",
      "Train Epoch: 157 [97536/225000 (43%)] Loss: 19281.480469\n",
      "Train Epoch: 157 [100032/225000 (44%)] Loss: 19601.773438\n",
      "Train Epoch: 157 [102528/225000 (46%)] Loss: 19147.789062\n",
      "Train Epoch: 157 [105024/225000 (47%)] Loss: 20010.615234\n",
      "Train Epoch: 157 [107520/225000 (48%)] Loss: 19516.289062\n",
      "Train Epoch: 157 [110016/225000 (49%)] Loss: 19108.507812\n",
      "Train Epoch: 157 [112512/225000 (50%)] Loss: 19200.101562\n",
      "Train Epoch: 157 [115008/225000 (51%)] Loss: 19183.351562\n",
      "Train Epoch: 157 [117504/225000 (52%)] Loss: 19040.072266\n",
      "Train Epoch: 157 [120000/225000 (53%)] Loss: 19372.488281\n",
      "Train Epoch: 157 [122496/225000 (54%)] Loss: 19470.125000\n",
      "Train Epoch: 157 [124992/225000 (56%)] Loss: 19105.937500\n",
      "Train Epoch: 157 [127488/225000 (57%)] Loss: 19149.884766\n",
      "Train Epoch: 157 [129984/225000 (58%)] Loss: 19305.218750\n",
      "Train Epoch: 157 [132480/225000 (59%)] Loss: 19502.638672\n",
      "Train Epoch: 157 [134976/225000 (60%)] Loss: 19614.425781\n",
      "Train Epoch: 157 [137472/225000 (61%)] Loss: 18823.025391\n",
      "Train Epoch: 157 [139968/225000 (62%)] Loss: 19135.144531\n",
      "Train Epoch: 157 [142464/225000 (63%)] Loss: 19556.841797\n",
      "Train Epoch: 157 [144960/225000 (64%)] Loss: 18867.923828\n",
      "Train Epoch: 157 [147456/225000 (66%)] Loss: 19813.298828\n",
      "Train Epoch: 157 [149952/225000 (67%)] Loss: 19386.871094\n",
      "Train Epoch: 157 [152448/225000 (68%)] Loss: 19182.927734\n",
      "Train Epoch: 157 [154944/225000 (69%)] Loss: 19201.083984\n",
      "Train Epoch: 157 [157440/225000 (70%)] Loss: 19300.125000\n",
      "Train Epoch: 157 [159936/225000 (71%)] Loss: 19463.832031\n",
      "Train Epoch: 157 [162432/225000 (72%)] Loss: 19292.656250\n",
      "Train Epoch: 157 [164928/225000 (73%)] Loss: 19350.445312\n",
      "Train Epoch: 157 [167424/225000 (74%)] Loss: 19563.759766\n",
      "Train Epoch: 157 [169920/225000 (76%)] Loss: 19070.343750\n",
      "Train Epoch: 157 [172416/225000 (77%)] Loss: 19157.253906\n",
      "Train Epoch: 157 [174912/225000 (78%)] Loss: 19284.962891\n",
      "Train Epoch: 157 [177408/225000 (79%)] Loss: 18973.015625\n",
      "Train Epoch: 157 [179904/225000 (80%)] Loss: 19044.636719\n",
      "Train Epoch: 157 [182400/225000 (81%)] Loss: 19097.693359\n",
      "Train Epoch: 157 [184896/225000 (82%)] Loss: 19087.130859\n",
      "Train Epoch: 157 [187392/225000 (83%)] Loss: 19176.349609\n",
      "Train Epoch: 157 [189888/225000 (84%)] Loss: 19372.343750\n",
      "Train Epoch: 157 [192384/225000 (86%)] Loss: 19169.187500\n",
      "Train Epoch: 157 [194880/225000 (87%)] Loss: 19674.826172\n",
      "Train Epoch: 157 [197376/225000 (88%)] Loss: 18943.029297\n",
      "Train Epoch: 157 [199872/225000 (89%)] Loss: 19308.384766\n",
      "Train Epoch: 157 [202368/225000 (90%)] Loss: 19114.136719\n",
      "Train Epoch: 157 [204864/225000 (91%)] Loss: 19456.857422\n",
      "Train Epoch: 157 [207360/225000 (92%)] Loss: 19568.445312\n",
      "Train Epoch: 157 [209856/225000 (93%)] Loss: 19509.416016\n",
      "Train Epoch: 157 [212352/225000 (94%)] Loss: 19320.855469\n",
      "Train Epoch: 157 [214848/225000 (95%)] Loss: 19215.824219\n",
      "Train Epoch: 157 [217344/225000 (97%)] Loss: 18917.347656\n",
      "Train Epoch: 157 [219840/225000 (98%)] Loss: 19160.925781\n",
      "Train Epoch: 157 [222336/225000 (99%)] Loss: 19551.769531\n",
      "Train Epoch: 157 [224832/225000 (100%)] Loss: 19001.988281\n",
      "    epoch          : 157\n",
      "    loss           : 19248.776570499147\n",
      "    val_loss       : 19146.568368835302\n",
      "Train Epoch: 158 [192/225000 (0%)] Loss: 19330.369141\n",
      "Train Epoch: 158 [2688/225000 (1%)] Loss: 19242.417969\n",
      "Train Epoch: 158 [5184/225000 (2%)] Loss: 18901.847656\n",
      "Train Epoch: 158 [7680/225000 (3%)] Loss: 19477.738281\n",
      "Train Epoch: 158 [10176/225000 (5%)] Loss: 18990.164062\n",
      "Train Epoch: 158 [12672/225000 (6%)] Loss: 19181.402344\n",
      "Train Epoch: 158 [15168/225000 (7%)] Loss: 19244.683594\n",
      "Train Epoch: 158 [17664/225000 (8%)] Loss: 19215.160156\n",
      "Train Epoch: 158 [20160/225000 (9%)] Loss: 19640.105469\n",
      "Train Epoch: 158 [22656/225000 (10%)] Loss: 19264.773438\n",
      "Train Epoch: 158 [25152/225000 (11%)] Loss: 19284.328125\n",
      "Train Epoch: 158 [27648/225000 (12%)] Loss: 19294.064453\n",
      "Train Epoch: 158 [30144/225000 (13%)] Loss: 19002.179688\n",
      "Train Epoch: 158 [32640/225000 (15%)] Loss: 19847.777344\n",
      "Train Epoch: 158 [35136/225000 (16%)] Loss: 19454.833984\n",
      "Train Epoch: 158 [37632/225000 (17%)] Loss: 19389.941406\n",
      "Train Epoch: 158 [40128/225000 (18%)] Loss: 19236.210938\n",
      "Train Epoch: 158 [42624/225000 (19%)] Loss: 19225.742188\n",
      "Train Epoch: 158 [45120/225000 (20%)] Loss: 19476.878906\n",
      "Train Epoch: 158 [47616/225000 (21%)] Loss: 19012.251953\n",
      "Train Epoch: 158 [50112/225000 (22%)] Loss: 19061.121094\n",
      "Train Epoch: 158 [52608/225000 (23%)] Loss: 19362.871094\n",
      "Train Epoch: 158 [55104/225000 (24%)] Loss: 19289.804688\n",
      "Train Epoch: 158 [57600/225000 (26%)] Loss: 19337.667969\n",
      "Train Epoch: 158 [60096/225000 (27%)] Loss: 19085.134766\n",
      "Train Epoch: 158 [62592/225000 (28%)] Loss: 19585.513672\n",
      "Train Epoch: 158 [65088/225000 (29%)] Loss: 19152.939453\n",
      "Train Epoch: 158 [67584/225000 (30%)] Loss: 19034.601562\n",
      "Train Epoch: 158 [70080/225000 (31%)] Loss: 19574.359375\n",
      "Train Epoch: 158 [72576/225000 (32%)] Loss: 19333.527344\n",
      "Train Epoch: 158 [75072/225000 (33%)] Loss: 19018.929688\n",
      "Train Epoch: 158 [77568/225000 (34%)] Loss: 19262.392578\n",
      "Train Epoch: 158 [80064/225000 (36%)] Loss: 18991.664062\n",
      "Train Epoch: 158 [82560/225000 (37%)] Loss: 19411.333984\n",
      "Train Epoch: 158 [85056/225000 (38%)] Loss: 19619.359375\n",
      "Train Epoch: 158 [87552/225000 (39%)] Loss: 19141.837891\n",
      "Train Epoch: 158 [90048/225000 (40%)] Loss: 19222.609375\n",
      "Train Epoch: 158 [92544/225000 (41%)] Loss: 19030.046875\n",
      "Train Epoch: 158 [95040/225000 (42%)] Loss: 19462.796875\n",
      "Train Epoch: 158 [97536/225000 (43%)] Loss: 19427.558594\n",
      "Train Epoch: 158 [100032/225000 (44%)] Loss: 19466.691406\n",
      "Train Epoch: 158 [102528/225000 (46%)] Loss: 19433.156250\n",
      "Train Epoch: 158 [105024/225000 (47%)] Loss: 19482.718750\n",
      "Train Epoch: 158 [107520/225000 (48%)] Loss: 19181.863281\n",
      "Train Epoch: 158 [110016/225000 (49%)] Loss: 18891.757812\n",
      "Train Epoch: 158 [112512/225000 (50%)] Loss: 19346.605469\n",
      "Train Epoch: 158 [115008/225000 (51%)] Loss: 19433.898438\n",
      "Train Epoch: 158 [117504/225000 (52%)] Loss: 19295.480469\n",
      "Train Epoch: 158 [120000/225000 (53%)] Loss: 19052.138672\n",
      "Train Epoch: 158 [122496/225000 (54%)] Loss: 18930.808594\n",
      "Train Epoch: 158 [124992/225000 (56%)] Loss: 19290.472656\n",
      "Train Epoch: 158 [127488/225000 (57%)] Loss: 19399.414062\n",
      "Train Epoch: 158 [129984/225000 (58%)] Loss: 19020.472656\n",
      "Train Epoch: 158 [132480/225000 (59%)] Loss: 18959.470703\n",
      "Train Epoch: 158 [134976/225000 (60%)] Loss: 19363.425781\n",
      "Train Epoch: 158 [137472/225000 (61%)] Loss: 19189.753906\n",
      "Train Epoch: 158 [139968/225000 (62%)] Loss: 18942.035156\n",
      "Train Epoch: 158 [142464/225000 (63%)] Loss: 18560.785156\n",
      "Train Epoch: 158 [144960/225000 (64%)] Loss: 18888.898438\n",
      "Train Epoch: 158 [147456/225000 (66%)] Loss: 19085.146484\n",
      "Train Epoch: 158 [149952/225000 (67%)] Loss: 18980.859375\n",
      "Train Epoch: 158 [152448/225000 (68%)] Loss: 18843.962891\n",
      "Train Epoch: 158 [154944/225000 (69%)] Loss: 18816.496094\n",
      "Train Epoch: 158 [157440/225000 (70%)] Loss: 19421.478516\n",
      "Train Epoch: 158 [159936/225000 (71%)] Loss: 19250.468750\n",
      "Train Epoch: 158 [162432/225000 (72%)] Loss: 19099.269531\n",
      "Train Epoch: 158 [164928/225000 (73%)] Loss: 19235.310547\n",
      "Train Epoch: 158 [167424/225000 (74%)] Loss: 19014.431641\n",
      "Train Epoch: 158 [169920/225000 (76%)] Loss: 19186.281250\n",
      "Train Epoch: 158 [172416/225000 (77%)] Loss: 19155.656250\n",
      "Train Epoch: 158 [174912/225000 (78%)] Loss: 19357.390625\n",
      "Train Epoch: 158 [177408/225000 (79%)] Loss: 18909.650391\n",
      "Train Epoch: 158 [179904/225000 (80%)] Loss: 18764.302734\n",
      "Train Epoch: 158 [182400/225000 (81%)] Loss: 19214.722656\n",
      "Train Epoch: 158 [184896/225000 (82%)] Loss: 18907.386719\n",
      "Train Epoch: 158 [187392/225000 (83%)] Loss: 19422.402344\n",
      "Train Epoch: 158 [189888/225000 (84%)] Loss: 19622.875000\n",
      "Train Epoch: 158 [192384/225000 (86%)] Loss: 19021.375000\n",
      "Train Epoch: 158 [194880/225000 (87%)] Loss: 19272.375000\n",
      "Train Epoch: 158 [197376/225000 (88%)] Loss: 19046.824219\n",
      "Train Epoch: 158 [199872/225000 (89%)] Loss: 19004.123047\n",
      "Train Epoch: 158 [202368/225000 (90%)] Loss: 19310.218750\n",
      "Train Epoch: 158 [204864/225000 (91%)] Loss: 19304.390625\n",
      "Train Epoch: 158 [207360/225000 (92%)] Loss: 19288.750000\n",
      "Train Epoch: 158 [209856/225000 (93%)] Loss: 19064.595703\n",
      "Train Epoch: 158 [212352/225000 (94%)] Loss: 19567.980469\n",
      "Train Epoch: 158 [214848/225000 (95%)] Loss: 19306.640625\n",
      "Train Epoch: 158 [217344/225000 (97%)] Loss: 19155.289062\n",
      "Train Epoch: 158 [219840/225000 (98%)] Loss: 19004.304688\n",
      "Train Epoch: 158 [222336/225000 (99%)] Loss: 19322.923828\n",
      "Train Epoch: 158 [224832/225000 (100%)] Loss: 19016.914062\n",
      "    epoch          : 158\n",
      "    loss           : 19238.036767744772\n",
      "    val_loss       : 19145.461611398303\n",
      "Train Epoch: 159 [192/225000 (0%)] Loss: 19436.304688\n",
      "Train Epoch: 159 [2688/225000 (1%)] Loss: 19643.412109\n",
      "Train Epoch: 159 [5184/225000 (2%)] Loss: 19007.625000\n",
      "Train Epoch: 159 [7680/225000 (3%)] Loss: 19436.144531\n",
      "Train Epoch: 159 [10176/225000 (5%)] Loss: 19177.929688\n",
      "Train Epoch: 159 [12672/225000 (6%)] Loss: 19609.921875\n",
      "Train Epoch: 159 [15168/225000 (7%)] Loss: 19553.236328\n",
      "Train Epoch: 159 [17664/225000 (8%)] Loss: 19799.667969\n",
      "Train Epoch: 159 [20160/225000 (9%)] Loss: 19133.171875\n",
      "Train Epoch: 159 [22656/225000 (10%)] Loss: 19347.125000\n",
      "Train Epoch: 159 [25152/225000 (11%)] Loss: 19010.808594\n",
      "Train Epoch: 159 [27648/225000 (12%)] Loss: 19018.955078\n",
      "Train Epoch: 159 [30144/225000 (13%)] Loss: 19285.804688\n",
      "Train Epoch: 159 [32640/225000 (15%)] Loss: 19442.574219\n",
      "Train Epoch: 159 [35136/225000 (16%)] Loss: 19125.343750\n",
      "Train Epoch: 159 [37632/225000 (17%)] Loss: 19312.855469\n",
      "Train Epoch: 159 [40128/225000 (18%)] Loss: 19274.601562\n",
      "Train Epoch: 159 [42624/225000 (19%)] Loss: 19256.007812\n",
      "Train Epoch: 159 [45120/225000 (20%)] Loss: 18670.105469\n",
      "Train Epoch: 159 [47616/225000 (21%)] Loss: 19197.417969\n",
      "Train Epoch: 159 [50112/225000 (22%)] Loss: 19081.578125\n",
      "Train Epoch: 159 [52608/225000 (23%)] Loss: 19180.599609\n",
      "Train Epoch: 159 [55104/225000 (24%)] Loss: 19162.621094\n",
      "Train Epoch: 159 [57600/225000 (26%)] Loss: 19373.751953\n",
      "Train Epoch: 159 [60096/225000 (27%)] Loss: 19594.287109\n",
      "Train Epoch: 159 [62592/225000 (28%)] Loss: 19271.511719\n",
      "Train Epoch: 159 [65088/225000 (29%)] Loss: 19103.105469\n",
      "Train Epoch: 159 [67584/225000 (30%)] Loss: 19314.851562\n",
      "Train Epoch: 159 [70080/225000 (31%)] Loss: 19407.429688\n",
      "Train Epoch: 159 [72576/225000 (32%)] Loss: 19612.296875\n",
      "Train Epoch: 159 [75072/225000 (33%)] Loss: 18895.990234\n",
      "Train Epoch: 159 [77568/225000 (34%)] Loss: 19184.076172\n",
      "Train Epoch: 159 [80064/225000 (36%)] Loss: 19589.972656\n",
      "Train Epoch: 159 [82560/225000 (37%)] Loss: 19229.851562\n",
      "Train Epoch: 159 [85056/225000 (38%)] Loss: 19447.097656\n",
      "Train Epoch: 159 [87552/225000 (39%)] Loss: 19006.814453\n",
      "Train Epoch: 159 [90048/225000 (40%)] Loss: 19325.263672\n",
      "Train Epoch: 159 [92544/225000 (41%)] Loss: 19048.964844\n",
      "Train Epoch: 159 [95040/225000 (42%)] Loss: 19548.878906\n",
      "Train Epoch: 159 [97536/225000 (43%)] Loss: 19479.218750\n",
      "Train Epoch: 159 [100032/225000 (44%)] Loss: 19269.382812\n",
      "Train Epoch: 159 [102528/225000 (46%)] Loss: 19462.144531\n",
      "Train Epoch: 159 [105024/225000 (47%)] Loss: 19601.808594\n",
      "Train Epoch: 159 [107520/225000 (48%)] Loss: 18625.505859\n",
      "Train Epoch: 159 [110016/225000 (49%)] Loss: 19066.253906\n",
      "Train Epoch: 159 [112512/225000 (50%)] Loss: 19091.292969\n",
      "Train Epoch: 159 [115008/225000 (51%)] Loss: 19193.337891\n",
      "Train Epoch: 159 [117504/225000 (52%)] Loss: 19282.292969\n",
      "Train Epoch: 159 [120000/225000 (53%)] Loss: 19382.175781\n",
      "Train Epoch: 159 [122496/225000 (54%)] Loss: 19383.664062\n",
      "Train Epoch: 159 [124992/225000 (56%)] Loss: 19072.632812\n",
      "Train Epoch: 159 [127488/225000 (57%)] Loss: 19416.617188\n",
      "Train Epoch: 159 [129984/225000 (58%)] Loss: 19613.724609\n",
      "Train Epoch: 159 [132480/225000 (59%)] Loss: 19998.066406\n",
      "Train Epoch: 159 [134976/225000 (60%)] Loss: 19584.953125\n",
      "Train Epoch: 159 [137472/225000 (61%)] Loss: 19214.597656\n",
      "Train Epoch: 159 [139968/225000 (62%)] Loss: 18720.230469\n",
      "Train Epoch: 159 [142464/225000 (63%)] Loss: 33334.253906\n",
      "Train Epoch: 159 [144960/225000 (64%)] Loss: 19661.738281\n",
      "Train Epoch: 159 [147456/225000 (66%)] Loss: 19626.978516\n",
      "Train Epoch: 159 [149952/225000 (67%)] Loss: 19567.824219\n",
      "Train Epoch: 159 [152448/225000 (68%)] Loss: 18650.691406\n",
      "Train Epoch: 159 [154944/225000 (69%)] Loss: 18877.472656\n",
      "Train Epoch: 159 [157440/225000 (70%)] Loss: 19328.828125\n",
      "Train Epoch: 159 [159936/225000 (71%)] Loss: 19436.812500\n",
      "Train Epoch: 159 [162432/225000 (72%)] Loss: 19252.578125\n",
      "Train Epoch: 159 [164928/225000 (73%)] Loss: 19309.884766\n",
      "Train Epoch: 159 [167424/225000 (74%)] Loss: 18969.763672\n",
      "Train Epoch: 159 [169920/225000 (76%)] Loss: 19047.501953\n",
      "Train Epoch: 159 [172416/225000 (77%)] Loss: 19045.425781\n",
      "Train Epoch: 159 [174912/225000 (78%)] Loss: 19310.574219\n",
      "Train Epoch: 159 [177408/225000 (79%)] Loss: 19032.699219\n",
      "Train Epoch: 159 [179904/225000 (80%)] Loss: 19264.070312\n",
      "Train Epoch: 159 [182400/225000 (81%)] Loss: 19397.835938\n",
      "Train Epoch: 159 [184896/225000 (82%)] Loss: 19047.605469\n",
      "Train Epoch: 159 [187392/225000 (83%)] Loss: 19244.552734\n",
      "Train Epoch: 159 [189888/225000 (84%)] Loss: 18923.250000\n",
      "Train Epoch: 159 [192384/225000 (86%)] Loss: 19084.882812\n",
      "Train Epoch: 159 [194880/225000 (87%)] Loss: 19207.927734\n",
      "Train Epoch: 159 [197376/225000 (88%)] Loss: 19380.421875\n",
      "Train Epoch: 159 [199872/225000 (89%)] Loss: 18957.884766\n",
      "Train Epoch: 159 [202368/225000 (90%)] Loss: 19472.439453\n",
      "Train Epoch: 159 [204864/225000 (91%)] Loss: 19540.361328\n",
      "Train Epoch: 159 [207360/225000 (92%)] Loss: 19436.207031\n",
      "Train Epoch: 159 [209856/225000 (93%)] Loss: 19012.007812\n",
      "Train Epoch: 159 [212352/225000 (94%)] Loss: 19304.509766\n",
      "Train Epoch: 159 [214848/225000 (95%)] Loss: 18986.966797\n",
      "Train Epoch: 159 [217344/225000 (97%)] Loss: 19248.968750\n",
      "Train Epoch: 159 [219840/225000 (98%)] Loss: 19496.160156\n",
      "Train Epoch: 159 [222336/225000 (99%)] Loss: 19192.908203\n",
      "Train Epoch: 159 [224832/225000 (100%)] Loss: 19397.917969\n",
      "    epoch          : 159\n",
      "    loss           : 19249.42199832018\n",
      "    val_loss       : 19169.466818916888\n",
      "Train Epoch: 160 [192/225000 (0%)] Loss: 19314.054688\n",
      "Train Epoch: 160 [2688/225000 (1%)] Loss: 19280.722656\n",
      "Train Epoch: 160 [5184/225000 (2%)] Loss: 19509.742188\n",
      "Train Epoch: 160 [7680/225000 (3%)] Loss: 19411.949219\n",
      "Train Epoch: 160 [10176/225000 (5%)] Loss: 19382.988281\n",
      "Train Epoch: 160 [12672/225000 (6%)] Loss: 19413.640625\n",
      "Train Epoch: 160 [15168/225000 (7%)] Loss: 19484.437500\n",
      "Train Epoch: 160 [17664/225000 (8%)] Loss: 19255.078125\n",
      "Train Epoch: 160 [20160/225000 (9%)] Loss: 19075.640625\n",
      "Train Epoch: 160 [22656/225000 (10%)] Loss: 19352.546875\n",
      "Train Epoch: 160 [25152/225000 (11%)] Loss: 19239.246094\n",
      "Train Epoch: 160 [27648/225000 (12%)] Loss: 19467.707031\n",
      "Train Epoch: 160 [30144/225000 (13%)] Loss: 19027.816406\n",
      "Train Epoch: 160 [32640/225000 (15%)] Loss: 19266.980469\n",
      "Train Epoch: 160 [35136/225000 (16%)] Loss: 20075.732422\n",
      "Train Epoch: 160 [37632/225000 (17%)] Loss: 19019.582031\n",
      "Train Epoch: 160 [40128/225000 (18%)] Loss: 19882.798828\n",
      "Train Epoch: 160 [42624/225000 (19%)] Loss: 19691.515625\n",
      "Train Epoch: 160 [45120/225000 (20%)] Loss: 18936.570312\n",
      "Train Epoch: 160 [47616/225000 (21%)] Loss: 19215.550781\n",
      "Train Epoch: 160 [50112/225000 (22%)] Loss: 19033.220703\n",
      "Train Epoch: 160 [52608/225000 (23%)] Loss: 19497.748047\n",
      "Train Epoch: 160 [55104/225000 (24%)] Loss: 19186.714844\n",
      "Train Epoch: 160 [57600/225000 (26%)] Loss: 19231.824219\n",
      "Train Epoch: 160 [60096/225000 (27%)] Loss: 19226.109375\n",
      "Train Epoch: 160 [62592/225000 (28%)] Loss: 19231.523438\n",
      "Train Epoch: 160 [65088/225000 (29%)] Loss: 18979.710938\n",
      "Train Epoch: 160 [67584/225000 (30%)] Loss: 18989.558594\n",
      "Train Epoch: 160 [70080/225000 (31%)] Loss: 19601.847656\n",
      "Train Epoch: 160 [72576/225000 (32%)] Loss: 19668.410156\n",
      "Train Epoch: 160 [75072/225000 (33%)] Loss: 19182.160156\n",
      "Train Epoch: 160 [77568/225000 (34%)] Loss: 19401.574219\n",
      "Train Epoch: 160 [80064/225000 (36%)] Loss: 19130.664062\n",
      "Train Epoch: 160 [82560/225000 (37%)] Loss: 19033.990234\n",
      "Train Epoch: 160 [85056/225000 (38%)] Loss: 19308.285156\n",
      "Train Epoch: 160 [87552/225000 (39%)] Loss: 19370.863281\n",
      "Train Epoch: 160 [90048/225000 (40%)] Loss: 18634.916016\n",
      "Train Epoch: 160 [92544/225000 (41%)] Loss: 19295.472656\n",
      "Train Epoch: 160 [95040/225000 (42%)] Loss: 19441.425781\n",
      "Train Epoch: 160 [97536/225000 (43%)] Loss: 19235.083984\n",
      "Train Epoch: 160 [100032/225000 (44%)] Loss: 19021.085938\n",
      "Train Epoch: 160 [102528/225000 (46%)] Loss: 19122.117188\n",
      "Train Epoch: 160 [105024/225000 (47%)] Loss: 19450.208984\n",
      "Train Epoch: 160 [107520/225000 (48%)] Loss: 19458.152344\n",
      "Train Epoch: 160 [110016/225000 (49%)] Loss: 18996.632812\n",
      "Train Epoch: 160 [112512/225000 (50%)] Loss: 19186.121094\n",
      "Train Epoch: 160 [115008/225000 (51%)] Loss: 19025.425781\n",
      "Train Epoch: 160 [117504/225000 (52%)] Loss: 19349.792969\n",
      "Train Epoch: 160 [120000/225000 (53%)] Loss: 19031.746094\n",
      "Train Epoch: 160 [122496/225000 (54%)] Loss: 18619.882812\n",
      "Train Epoch: 160 [124992/225000 (56%)] Loss: 19015.636719\n",
      "Train Epoch: 160 [127488/225000 (57%)] Loss: 19571.775391\n",
      "Train Epoch: 160 [129984/225000 (58%)] Loss: 19486.218750\n",
      "Train Epoch: 160 [132480/225000 (59%)] Loss: 19223.212891\n",
      "Train Epoch: 160 [134976/225000 (60%)] Loss: 19596.261719\n",
      "Train Epoch: 160 [137472/225000 (61%)] Loss: 19281.812500\n",
      "Train Epoch: 160 [139968/225000 (62%)] Loss: 18954.546875\n",
      "Train Epoch: 160 [142464/225000 (63%)] Loss: 18853.126953\n",
      "Train Epoch: 160 [144960/225000 (64%)] Loss: 19413.722656\n",
      "Train Epoch: 160 [147456/225000 (66%)] Loss: 19359.562500\n",
      "Train Epoch: 160 [149952/225000 (67%)] Loss: 19369.355469\n",
      "Train Epoch: 160 [152448/225000 (68%)] Loss: 19117.052734\n",
      "Train Epoch: 160 [154944/225000 (69%)] Loss: 18928.417969\n",
      "Train Epoch: 160 [157440/225000 (70%)] Loss: 19071.466797\n",
      "Train Epoch: 160 [159936/225000 (71%)] Loss: 18736.613281\n",
      "Train Epoch: 160 [162432/225000 (72%)] Loss: 18982.910156\n",
      "Train Epoch: 160 [164928/225000 (73%)] Loss: 18684.375000\n",
      "Train Epoch: 160 [167424/225000 (74%)] Loss: 19177.226562\n",
      "Train Epoch: 160 [169920/225000 (76%)] Loss: 19480.859375\n",
      "Train Epoch: 160 [172416/225000 (77%)] Loss: 19512.359375\n",
      "Train Epoch: 160 [174912/225000 (78%)] Loss: 19553.777344\n",
      "Train Epoch: 160 [177408/225000 (79%)] Loss: 19152.019531\n",
      "Train Epoch: 160 [179904/225000 (80%)] Loss: 19238.972656\n",
      "Train Epoch: 160 [182400/225000 (81%)] Loss: 19628.558594\n",
      "Train Epoch: 160 [184896/225000 (82%)] Loss: 19110.347656\n",
      "Train Epoch: 160 [187392/225000 (83%)] Loss: 19343.875000\n",
      "Train Epoch: 160 [189888/225000 (84%)] Loss: 19423.457031\n",
      "Train Epoch: 160 [192384/225000 (86%)] Loss: 19779.523438\n",
      "Train Epoch: 160 [194880/225000 (87%)] Loss: 19164.691406\n",
      "Train Epoch: 160 [197376/225000 (88%)] Loss: 19084.072266\n",
      "Train Epoch: 160 [199872/225000 (89%)] Loss: 18893.230469\n",
      "Train Epoch: 160 [202368/225000 (90%)] Loss: 19060.808594\n",
      "Train Epoch: 160 [204864/225000 (91%)] Loss: 19316.484375\n",
      "Train Epoch: 160 [207360/225000 (92%)] Loss: 18965.761719\n",
      "Train Epoch: 160 [209856/225000 (93%)] Loss: 18962.447266\n",
      "Train Epoch: 160 [212352/225000 (94%)] Loss: 19066.867188\n",
      "Train Epoch: 160 [214848/225000 (95%)] Loss: 19168.957031\n",
      "Train Epoch: 160 [217344/225000 (97%)] Loss: 18889.996094\n",
      "Train Epoch: 160 [219840/225000 (98%)] Loss: 19142.519531\n",
      "Train Epoch: 160 [222336/225000 (99%)] Loss: 18956.125000\n",
      "Train Epoch: 160 [224832/225000 (100%)] Loss: 19624.738281\n",
      "    epoch          : 160\n",
      "    loss           : 19232.30508579085\n",
      "    val_loss       : 19135.704215007885\n",
      "Train Epoch: 161 [192/225000 (0%)] Loss: 18874.404297\n",
      "Train Epoch: 161 [2688/225000 (1%)] Loss: 18797.046875\n",
      "Train Epoch: 161 [5184/225000 (2%)] Loss: 18795.509766\n",
      "Train Epoch: 161 [7680/225000 (3%)] Loss: 19175.117188\n",
      "Train Epoch: 161 [10176/225000 (5%)] Loss: 18777.855469\n",
      "Train Epoch: 161 [12672/225000 (6%)] Loss: 19150.416016\n",
      "Train Epoch: 161 [15168/225000 (7%)] Loss: 19287.652344\n",
      "Train Epoch: 161 [17664/225000 (8%)] Loss: 19397.671875\n",
      "Train Epoch: 161 [20160/225000 (9%)] Loss: 19180.466797\n",
      "Train Epoch: 161 [22656/225000 (10%)] Loss: 18874.544922\n",
      "Train Epoch: 161 [25152/225000 (11%)] Loss: 19002.824219\n",
      "Train Epoch: 161 [27648/225000 (12%)] Loss: 19752.960938\n",
      "Train Epoch: 161 [30144/225000 (13%)] Loss: 18812.835938\n",
      "Train Epoch: 161 [32640/225000 (15%)] Loss: 19025.576172\n",
      "Train Epoch: 161 [35136/225000 (16%)] Loss: 19219.785156\n",
      "Train Epoch: 161 [37632/225000 (17%)] Loss: 19965.275391\n",
      "Train Epoch: 161 [40128/225000 (18%)] Loss: 19627.429688\n",
      "Train Epoch: 161 [42624/225000 (19%)] Loss: 19454.546875\n",
      "Train Epoch: 161 [45120/225000 (20%)] Loss: 19470.558594\n",
      "Train Epoch: 161 [47616/225000 (21%)] Loss: 19429.027344\n",
      "Train Epoch: 161 [50112/225000 (22%)] Loss: 19749.425781\n",
      "Train Epoch: 161 [52608/225000 (23%)] Loss: 18697.058594\n",
      "Train Epoch: 161 [55104/225000 (24%)] Loss: 19266.132812\n",
      "Train Epoch: 161 [57600/225000 (26%)] Loss: 19500.191406\n",
      "Train Epoch: 161 [60096/225000 (27%)] Loss: 19016.082031\n",
      "Train Epoch: 161 [62592/225000 (28%)] Loss: 19530.742188\n",
      "Train Epoch: 161 [65088/225000 (29%)] Loss: 18459.187500\n",
      "Train Epoch: 161 [67584/225000 (30%)] Loss: 19391.960938\n",
      "Train Epoch: 161 [70080/225000 (31%)] Loss: 18634.375000\n",
      "Train Epoch: 161 [72576/225000 (32%)] Loss: 19242.820312\n",
      "Train Epoch: 161 [75072/225000 (33%)] Loss: 19211.009766\n",
      "Train Epoch: 161 [77568/225000 (34%)] Loss: 19276.808594\n",
      "Train Epoch: 161 [80064/225000 (36%)] Loss: 18940.259766\n",
      "Train Epoch: 161 [82560/225000 (37%)] Loss: 19261.433594\n",
      "Train Epoch: 161 [85056/225000 (38%)] Loss: 19329.111328\n",
      "Train Epoch: 161 [87552/225000 (39%)] Loss: 19323.044922\n",
      "Train Epoch: 161 [90048/225000 (40%)] Loss: 18923.765625\n",
      "Train Epoch: 161 [92544/225000 (41%)] Loss: 19096.890625\n",
      "Train Epoch: 161 [95040/225000 (42%)] Loss: 19057.613281\n",
      "Train Epoch: 161 [97536/225000 (43%)] Loss: 19181.390625\n",
      "Train Epoch: 161 [100032/225000 (44%)] Loss: 19360.882812\n",
      "Train Epoch: 161 [102528/225000 (46%)] Loss: 18993.177734\n",
      "Train Epoch: 161 [105024/225000 (47%)] Loss: 18840.035156\n",
      "Train Epoch: 161 [107520/225000 (48%)] Loss: 19567.818359\n",
      "Train Epoch: 161 [110016/225000 (49%)] Loss: 19047.050781\n",
      "Train Epoch: 161 [112512/225000 (50%)] Loss: 19235.832031\n",
      "Train Epoch: 161 [115008/225000 (51%)] Loss: 19757.496094\n",
      "Train Epoch: 161 [117504/225000 (52%)] Loss: 19529.441406\n",
      "Train Epoch: 161 [120000/225000 (53%)] Loss: 19104.611328\n",
      "Train Epoch: 161 [122496/225000 (54%)] Loss: 19383.460938\n",
      "Train Epoch: 161 [124992/225000 (56%)] Loss: 19559.878906\n",
      "Train Epoch: 161 [127488/225000 (57%)] Loss: 19160.132812\n",
      "Train Epoch: 161 [129984/225000 (58%)] Loss: 19059.966797\n",
      "Train Epoch: 161 [132480/225000 (59%)] Loss: 19202.144531\n",
      "Train Epoch: 161 [134976/225000 (60%)] Loss: 19512.150391\n",
      "Train Epoch: 161 [137472/225000 (61%)] Loss: 19513.039062\n",
      "Train Epoch: 161 [139968/225000 (62%)] Loss: 19201.376953\n",
      "Train Epoch: 161 [142464/225000 (63%)] Loss: 19165.945312\n",
      "Train Epoch: 161 [144960/225000 (64%)] Loss: 19080.816406\n",
      "Train Epoch: 161 [147456/225000 (66%)] Loss: 19047.992188\n",
      "Train Epoch: 161 [149952/225000 (67%)] Loss: 19436.634766\n",
      "Train Epoch: 161 [152448/225000 (68%)] Loss: 18787.056641\n",
      "Train Epoch: 161 [154944/225000 (69%)] Loss: 19204.054688\n",
      "Train Epoch: 161 [157440/225000 (70%)] Loss: 19262.847656\n",
      "Train Epoch: 161 [159936/225000 (71%)] Loss: 18936.519531\n",
      "Train Epoch: 161 [162432/225000 (72%)] Loss: 19747.171875\n",
      "Train Epoch: 161 [164928/225000 (73%)] Loss: 19087.960938\n",
      "Train Epoch: 161 [167424/225000 (74%)] Loss: 19266.167969\n",
      "Train Epoch: 161 [169920/225000 (76%)] Loss: 18862.757812\n",
      "Train Epoch: 161 [172416/225000 (77%)] Loss: 19400.296875\n",
      "Train Epoch: 161 [174912/225000 (78%)] Loss: 19051.632812\n",
      "Train Epoch: 161 [177408/225000 (79%)] Loss: 19329.242188\n",
      "Train Epoch: 161 [179904/225000 (80%)] Loss: 18865.216797\n",
      "Train Epoch: 161 [182400/225000 (81%)] Loss: 19324.035156\n",
      "Train Epoch: 161 [184896/225000 (82%)] Loss: 19372.025391\n",
      "Train Epoch: 161 [187392/225000 (83%)] Loss: 19242.398438\n",
      "Train Epoch: 161 [189888/225000 (84%)] Loss: 18982.224609\n",
      "Train Epoch: 161 [192384/225000 (86%)] Loss: 19137.886719\n",
      "Train Epoch: 161 [194880/225000 (87%)] Loss: 18757.882812\n",
      "Train Epoch: 161 [197376/225000 (88%)] Loss: 19488.882812\n",
      "Train Epoch: 161 [199872/225000 (89%)] Loss: 19369.677734\n",
      "Train Epoch: 161 [202368/225000 (90%)] Loss: 19312.058594\n",
      "Train Epoch: 161 [204864/225000 (91%)] Loss: 18674.244141\n",
      "Train Epoch: 161 [207360/225000 (92%)] Loss: 19170.689453\n",
      "Train Epoch: 161 [209856/225000 (93%)] Loss: 19845.652344\n",
      "Train Epoch: 161 [212352/225000 (94%)] Loss: 19467.816406\n",
      "Train Epoch: 161 [214848/225000 (95%)] Loss: 19377.501953\n",
      "Train Epoch: 161 [217344/225000 (97%)] Loss: 19514.398438\n",
      "Train Epoch: 161 [219840/225000 (98%)] Loss: 19615.669922\n",
      "Train Epoch: 161 [222336/225000 (99%)] Loss: 18936.363281\n",
      "Train Epoch: 161 [224832/225000 (100%)] Loss: 19216.376953\n",
      "    epoch          : 161\n",
      "    loss           : 19231.211529103562\n",
      "    val_loss       : 19143.925486712964\n",
      "Train Epoch: 162 [192/225000 (0%)] Loss: 19250.167969\n",
      "Train Epoch: 162 [2688/225000 (1%)] Loss: 19271.023438\n",
      "Train Epoch: 162 [5184/225000 (2%)] Loss: 19431.460938\n",
      "Train Epoch: 162 [7680/225000 (3%)] Loss: 19125.207031\n",
      "Train Epoch: 162 [10176/225000 (5%)] Loss: 19415.707031\n",
      "Train Epoch: 162 [12672/225000 (6%)] Loss: 18841.253906\n",
      "Train Epoch: 162 [15168/225000 (7%)] Loss: 19525.835938\n",
      "Train Epoch: 162 [17664/225000 (8%)] Loss: 19128.519531\n",
      "Train Epoch: 162 [20160/225000 (9%)] Loss: 19359.603516\n",
      "Train Epoch: 162 [22656/225000 (10%)] Loss: 18926.250000\n",
      "Train Epoch: 162 [25152/225000 (11%)] Loss: 18614.019531\n",
      "Train Epoch: 162 [27648/225000 (12%)] Loss: 19302.796875\n",
      "Train Epoch: 162 [30144/225000 (13%)] Loss: 19736.636719\n",
      "Train Epoch: 162 [32640/225000 (15%)] Loss: 19619.820312\n",
      "Train Epoch: 162 [35136/225000 (16%)] Loss: 19228.677734\n",
      "Train Epoch: 162 [37632/225000 (17%)] Loss: 19004.425781\n",
      "Train Epoch: 162 [40128/225000 (18%)] Loss: 19245.441406\n",
      "Train Epoch: 162 [42624/225000 (19%)] Loss: 18778.882812\n",
      "Train Epoch: 162 [45120/225000 (20%)] Loss: 19656.945312\n",
      "Train Epoch: 162 [47616/225000 (21%)] Loss: 19313.736328\n",
      "Train Epoch: 162 [50112/225000 (22%)] Loss: 24130.271484\n",
      "Train Epoch: 162 [52608/225000 (23%)] Loss: 19072.281250\n",
      "Train Epoch: 162 [55104/225000 (24%)] Loss: 19276.277344\n",
      "Train Epoch: 162 [57600/225000 (26%)] Loss: 19495.257812\n",
      "Train Epoch: 162 [60096/225000 (27%)] Loss: 18823.128906\n",
      "Train Epoch: 162 [62592/225000 (28%)] Loss: 18996.107422\n",
      "Train Epoch: 162 [65088/225000 (29%)] Loss: 18833.890625\n",
      "Train Epoch: 162 [67584/225000 (30%)] Loss: 18908.003906\n",
      "Train Epoch: 162 [70080/225000 (31%)] Loss: 19321.203125\n",
      "Train Epoch: 162 [72576/225000 (32%)] Loss: 19189.292969\n",
      "Train Epoch: 162 [75072/225000 (33%)] Loss: 19253.917969\n",
      "Train Epoch: 162 [77568/225000 (34%)] Loss: 19098.134766\n",
      "Train Epoch: 162 [80064/225000 (36%)] Loss: 19594.738281\n",
      "Train Epoch: 162 [82560/225000 (37%)] Loss: 19823.867188\n",
      "Train Epoch: 162 [85056/225000 (38%)] Loss: 19677.951172\n",
      "Train Epoch: 162 [87552/225000 (39%)] Loss: 19400.882812\n",
      "Train Epoch: 162 [90048/225000 (40%)] Loss: 18668.289062\n",
      "Train Epoch: 162 [92544/225000 (41%)] Loss: 19387.808594\n",
      "Train Epoch: 162 [95040/225000 (42%)] Loss: 19495.722656\n",
      "Train Epoch: 162 [97536/225000 (43%)] Loss: 19241.234375\n",
      "Train Epoch: 162 [100032/225000 (44%)] Loss: 19014.150391\n",
      "Train Epoch: 162 [102528/225000 (46%)] Loss: 19047.550781\n",
      "Train Epoch: 162 [105024/225000 (47%)] Loss: 18696.361328\n",
      "Train Epoch: 162 [107520/225000 (48%)] Loss: 19088.787109\n",
      "Train Epoch: 162 [110016/225000 (49%)] Loss: 19723.777344\n",
      "Train Epoch: 162 [112512/225000 (50%)] Loss: 19141.511719\n",
      "Train Epoch: 162 [115008/225000 (51%)] Loss: 18753.625000\n",
      "Train Epoch: 162 [117504/225000 (52%)] Loss: 19145.359375\n",
      "Train Epoch: 162 [120000/225000 (53%)] Loss: 19235.351562\n",
      "Train Epoch: 162 [122496/225000 (54%)] Loss: 18936.246094\n",
      "Train Epoch: 162 [124992/225000 (56%)] Loss: 18902.382812\n",
      "Train Epoch: 162 [127488/225000 (57%)] Loss: 19537.847656\n",
      "Train Epoch: 162 [129984/225000 (58%)] Loss: 18960.531250\n",
      "Train Epoch: 162 [132480/225000 (59%)] Loss: 19407.707031\n",
      "Train Epoch: 162 [134976/225000 (60%)] Loss: 19092.781250\n",
      "Train Epoch: 162 [137472/225000 (61%)] Loss: 18954.681641\n",
      "Train Epoch: 162 [139968/225000 (62%)] Loss: 19309.117188\n",
      "Train Epoch: 162 [142464/225000 (63%)] Loss: 19631.125000\n",
      "Train Epoch: 162 [144960/225000 (64%)] Loss: 19995.847656\n",
      "Train Epoch: 162 [147456/225000 (66%)] Loss: 19225.941406\n",
      "Train Epoch: 162 [149952/225000 (67%)] Loss: 18808.523438\n",
      "Train Epoch: 162 [152448/225000 (68%)] Loss: 19378.363281\n",
      "Train Epoch: 162 [154944/225000 (69%)] Loss: 19669.115234\n",
      "Train Epoch: 162 [157440/225000 (70%)] Loss: 19369.439453\n",
      "Train Epoch: 162 [159936/225000 (71%)] Loss: 19894.472656\n",
      "Train Epoch: 162 [162432/225000 (72%)] Loss: 19235.128906\n",
      "Train Epoch: 162 [164928/225000 (73%)] Loss: 19144.173828\n",
      "Train Epoch: 162 [167424/225000 (74%)] Loss: 18866.640625\n",
      "Train Epoch: 162 [169920/225000 (76%)] Loss: 18802.099609\n",
      "Train Epoch: 162 [172416/225000 (77%)] Loss: 19355.992188\n",
      "Train Epoch: 162 [174912/225000 (78%)] Loss: 18705.207031\n",
      "Train Epoch: 162 [177408/225000 (79%)] Loss: 19099.062500\n",
      "Train Epoch: 162 [179904/225000 (80%)] Loss: 19217.214844\n",
      "Train Epoch: 162 [182400/225000 (81%)] Loss: 19235.500000\n",
      "Train Epoch: 162 [184896/225000 (82%)] Loss: 19326.843750\n",
      "Train Epoch: 162 [187392/225000 (83%)] Loss: 18652.335938\n",
      "Train Epoch: 162 [189888/225000 (84%)] Loss: 19688.994141\n",
      "Train Epoch: 162 [192384/225000 (86%)] Loss: 19020.693359\n",
      "Train Epoch: 162 [194880/225000 (87%)] Loss: 19431.994141\n",
      "Train Epoch: 162 [197376/225000 (88%)] Loss: 19429.550781\n",
      "Train Epoch: 162 [199872/225000 (89%)] Loss: 19170.273438\n",
      "Train Epoch: 162 [202368/225000 (90%)] Loss: 19115.218750\n",
      "Train Epoch: 162 [204864/225000 (91%)] Loss: 18526.707031\n",
      "Train Epoch: 162 [207360/225000 (92%)] Loss: 19217.722656\n",
      "Train Epoch: 162 [209856/225000 (93%)] Loss: 19091.404297\n",
      "Train Epoch: 162 [212352/225000 (94%)] Loss: 19334.714844\n",
      "Train Epoch: 162 [214848/225000 (95%)] Loss: 19567.585938\n",
      "Train Epoch: 162 [217344/225000 (97%)] Loss: 19500.101562\n",
      "Train Epoch: 162 [219840/225000 (98%)] Loss: 19350.410156\n",
      "Train Epoch: 162 [222336/225000 (99%)] Loss: 19385.421875\n",
      "Train Epoch: 162 [224832/225000 (100%)] Loss: 19591.503906\n",
      "    epoch          : 162\n",
      "    loss           : 19234.584844283276\n",
      "    val_loss       : 19144.71322278212\n",
      "Train Epoch: 163 [192/225000 (0%)] Loss: 19285.539062\n",
      "Train Epoch: 163 [2688/225000 (1%)] Loss: 19249.175781\n",
      "Train Epoch: 163 [5184/225000 (2%)] Loss: 19439.269531\n",
      "Train Epoch: 163 [7680/225000 (3%)] Loss: 18802.699219\n",
      "Train Epoch: 163 [10176/225000 (5%)] Loss: 18749.628906\n",
      "Train Epoch: 163 [12672/225000 (6%)] Loss: 19286.990234\n",
      "Train Epoch: 163 [15168/225000 (7%)] Loss: 19295.539062\n",
      "Train Epoch: 163 [17664/225000 (8%)] Loss: 19634.154297\n",
      "Train Epoch: 163 [20160/225000 (9%)] Loss: 19440.478516\n",
      "Train Epoch: 163 [22656/225000 (10%)] Loss: 19075.607422\n",
      "Train Epoch: 163 [25152/225000 (11%)] Loss: 19757.261719\n",
      "Train Epoch: 163 [27648/225000 (12%)] Loss: 19136.261719\n",
      "Train Epoch: 163 [30144/225000 (13%)] Loss: 18787.177734\n",
      "Train Epoch: 163 [32640/225000 (15%)] Loss: 18604.921875\n",
      "Train Epoch: 163 [35136/225000 (16%)] Loss: 18797.476562\n",
      "Train Epoch: 163 [37632/225000 (17%)] Loss: 19422.496094\n",
      "Train Epoch: 163 [40128/225000 (18%)] Loss: 19528.718750\n",
      "Train Epoch: 163 [42624/225000 (19%)] Loss: 18970.804688\n",
      "Train Epoch: 163 [45120/225000 (20%)] Loss: 18879.298828\n",
      "Train Epoch: 163 [47616/225000 (21%)] Loss: 19215.468750\n",
      "Train Epoch: 163 [50112/225000 (22%)] Loss: 19067.121094\n",
      "Train Epoch: 163 [52608/225000 (23%)] Loss: 19384.914062\n",
      "Train Epoch: 163 [55104/225000 (24%)] Loss: 19359.154297\n",
      "Train Epoch: 163 [57600/225000 (26%)] Loss: 19549.679688\n",
      "Train Epoch: 163 [60096/225000 (27%)] Loss: 19224.132812\n",
      "Train Epoch: 163 [62592/225000 (28%)] Loss: 19025.769531\n",
      "Train Epoch: 163 [65088/225000 (29%)] Loss: 19028.234375\n",
      "Train Epoch: 163 [67584/225000 (30%)] Loss: 19220.851562\n",
      "Train Epoch: 163 [70080/225000 (31%)] Loss: 19055.132812\n",
      "Train Epoch: 163 [72576/225000 (32%)] Loss: 19281.232422\n",
      "Train Epoch: 163 [75072/225000 (33%)] Loss: 19018.414062\n",
      "Train Epoch: 163 [77568/225000 (34%)] Loss: 19145.398438\n",
      "Train Epoch: 163 [80064/225000 (36%)] Loss: 19714.722656\n",
      "Train Epoch: 163 [82560/225000 (37%)] Loss: 19589.390625\n",
      "Train Epoch: 163 [85056/225000 (38%)] Loss: 19239.513672\n",
      "Train Epoch: 163 [87552/225000 (39%)] Loss: 19442.097656\n",
      "Train Epoch: 163 [90048/225000 (40%)] Loss: 19085.144531\n",
      "Train Epoch: 163 [92544/225000 (41%)] Loss: 19453.921875\n",
      "Train Epoch: 163 [95040/225000 (42%)] Loss: 19064.136719\n",
      "Train Epoch: 163 [97536/225000 (43%)] Loss: 19405.400391\n",
      "Train Epoch: 163 [100032/225000 (44%)] Loss: 19120.656250\n",
      "Train Epoch: 163 [102528/225000 (46%)] Loss: 19252.343750\n",
      "Train Epoch: 163 [105024/225000 (47%)] Loss: 19105.222656\n",
      "Train Epoch: 163 [107520/225000 (48%)] Loss: 19068.951172\n",
      "Train Epoch: 163 [110016/225000 (49%)] Loss: 19200.828125\n",
      "Train Epoch: 163 [112512/225000 (50%)] Loss: 19102.285156\n",
      "Train Epoch: 163 [115008/225000 (51%)] Loss: 19487.734375\n",
      "Train Epoch: 163 [117504/225000 (52%)] Loss: 18841.486328\n",
      "Train Epoch: 163 [120000/225000 (53%)] Loss: 18894.945312\n",
      "Train Epoch: 163 [122496/225000 (54%)] Loss: 19196.285156\n",
      "Train Epoch: 163 [124992/225000 (56%)] Loss: 19395.634766\n",
      "Train Epoch: 163 [127488/225000 (57%)] Loss: 19326.208984\n",
      "Train Epoch: 163 [129984/225000 (58%)] Loss: 19277.378906\n",
      "Train Epoch: 163 [132480/225000 (59%)] Loss: 19374.287109\n",
      "Train Epoch: 163 [134976/225000 (60%)] Loss: 19492.800781\n",
      "Train Epoch: 163 [137472/225000 (61%)] Loss: 18697.568359\n",
      "Train Epoch: 163 [139968/225000 (62%)] Loss: 19391.062500\n",
      "Train Epoch: 163 [142464/225000 (63%)] Loss: 18754.007812\n",
      "Train Epoch: 163 [144960/225000 (64%)] Loss: 19221.388672\n",
      "Train Epoch: 163 [147456/225000 (66%)] Loss: 18847.169922\n",
      "Train Epoch: 163 [149952/225000 (67%)] Loss: 19701.707031\n",
      "Train Epoch: 163 [152448/225000 (68%)] Loss: 18544.523438\n",
      "Train Epoch: 163 [154944/225000 (69%)] Loss: 19713.480469\n",
      "Train Epoch: 163 [157440/225000 (70%)] Loss: 19147.023438\n",
      "Train Epoch: 163 [159936/225000 (71%)] Loss: 18963.685547\n",
      "Train Epoch: 163 [162432/225000 (72%)] Loss: 19477.000000\n",
      "Train Epoch: 163 [164928/225000 (73%)] Loss: 18958.855469\n",
      "Train Epoch: 163 [167424/225000 (74%)] Loss: 19166.007812\n",
      "Train Epoch: 163 [169920/225000 (76%)] Loss: 18935.835938\n",
      "Train Epoch: 163 [172416/225000 (77%)] Loss: 19013.011719\n",
      "Train Epoch: 163 [174912/225000 (78%)] Loss: 19453.988281\n",
      "Train Epoch: 163 [177408/225000 (79%)] Loss: 19331.593750\n",
      "Train Epoch: 163 [179904/225000 (80%)] Loss: 19261.910156\n",
      "Train Epoch: 163 [182400/225000 (81%)] Loss: 19464.234375\n",
      "Train Epoch: 163 [184896/225000 (82%)] Loss: 19271.849609\n",
      "Train Epoch: 163 [187392/225000 (83%)] Loss: 19090.511719\n",
      "Train Epoch: 163 [189888/225000 (84%)] Loss: 18973.195312\n",
      "Train Epoch: 163 [192384/225000 (86%)] Loss: 19971.267578\n",
      "Train Epoch: 163 [194880/225000 (87%)] Loss: 19142.181641\n",
      "Train Epoch: 163 [197376/225000 (88%)] Loss: 19131.304688\n",
      "Train Epoch: 163 [199872/225000 (89%)] Loss: 19480.039062\n",
      "Train Epoch: 163 [202368/225000 (90%)] Loss: 19583.488281\n",
      "Train Epoch: 163 [204864/225000 (91%)] Loss: 19305.476562\n",
      "Train Epoch: 163 [207360/225000 (92%)] Loss: 19542.634766\n",
      "Train Epoch: 163 [209856/225000 (93%)] Loss: 18960.171875\n",
      "Train Epoch: 163 [212352/225000 (94%)] Loss: 18995.835938\n",
      "Train Epoch: 163 [214848/225000 (95%)] Loss: 19696.621094\n",
      "Train Epoch: 163 [217344/225000 (97%)] Loss: 19291.742188\n",
      "Train Epoch: 163 [219840/225000 (98%)] Loss: 18892.224609\n",
      "Train Epoch: 163 [222336/225000 (99%)] Loss: 18750.357422\n",
      "Train Epoch: 163 [224832/225000 (100%)] Loss: 19257.949219\n",
      "    epoch          : 163\n",
      "    loss           : 19218.27050614601\n",
      "    val_loss       : 19145.254064569035\n",
      "Train Epoch: 164 [192/225000 (0%)] Loss: 18900.542969\n",
      "Train Epoch: 164 [2688/225000 (1%)] Loss: 19730.628906\n",
      "Train Epoch: 164 [5184/225000 (2%)] Loss: 19429.023438\n",
      "Train Epoch: 164 [7680/225000 (3%)] Loss: 19251.521484\n",
      "Train Epoch: 164 [10176/225000 (5%)] Loss: 19060.738281\n",
      "Train Epoch: 164 [12672/225000 (6%)] Loss: 18974.179688\n",
      "Train Epoch: 164 [15168/225000 (7%)] Loss: 19382.777344\n",
      "Train Epoch: 164 [17664/225000 (8%)] Loss: 19149.515625\n",
      "Train Epoch: 164 [20160/225000 (9%)] Loss: 19276.210938\n",
      "Train Epoch: 164 [22656/225000 (10%)] Loss: 19467.625000\n",
      "Train Epoch: 164 [25152/225000 (11%)] Loss: 18858.726562\n",
      "Train Epoch: 164 [27648/225000 (12%)] Loss: 19974.980469\n",
      "Train Epoch: 164 [30144/225000 (13%)] Loss: 19374.734375\n",
      "Train Epoch: 164 [32640/225000 (15%)] Loss: 19051.007812\n",
      "Train Epoch: 164 [35136/225000 (16%)] Loss: 19029.939453\n",
      "Train Epoch: 164 [37632/225000 (17%)] Loss: 19638.826172\n",
      "Train Epoch: 164 [40128/225000 (18%)] Loss: 19750.156250\n",
      "Train Epoch: 164 [42624/225000 (19%)] Loss: 18909.800781\n",
      "Train Epoch: 164 [45120/225000 (20%)] Loss: 19521.162109\n",
      "Train Epoch: 164 [47616/225000 (21%)] Loss: 18983.562500\n",
      "Train Epoch: 164 [50112/225000 (22%)] Loss: 19590.699219\n",
      "Train Epoch: 164 [52608/225000 (23%)] Loss: 19025.257812\n",
      "Train Epoch: 164 [55104/225000 (24%)] Loss: 18894.408203\n",
      "Train Epoch: 164 [57600/225000 (26%)] Loss: 19239.771484\n",
      "Train Epoch: 164 [60096/225000 (27%)] Loss: 19515.455078\n",
      "Train Epoch: 164 [62592/225000 (28%)] Loss: 19341.443359\n",
      "Train Epoch: 164 [65088/225000 (29%)] Loss: 19010.910156\n",
      "Train Epoch: 164 [67584/225000 (30%)] Loss: 19078.964844\n",
      "Train Epoch: 164 [70080/225000 (31%)] Loss: 19624.109375\n",
      "Train Epoch: 164 [72576/225000 (32%)] Loss: 19440.775391\n",
      "Train Epoch: 164 [75072/225000 (33%)] Loss: 19104.080078\n",
      "Train Epoch: 164 [77568/225000 (34%)] Loss: 19550.671875\n",
      "Train Epoch: 164 [80064/225000 (36%)] Loss: 19577.833984\n",
      "Train Epoch: 164 [82560/225000 (37%)] Loss: 18697.472656\n",
      "Train Epoch: 164 [85056/225000 (38%)] Loss: 18856.150391\n",
      "Train Epoch: 164 [87552/225000 (39%)] Loss: 19139.621094\n",
      "Train Epoch: 164 [90048/225000 (40%)] Loss: 19161.152344\n",
      "Train Epoch: 164 [92544/225000 (41%)] Loss: 19100.003906\n",
      "Train Epoch: 164 [95040/225000 (42%)] Loss: 19322.173828\n",
      "Train Epoch: 164 [97536/225000 (43%)] Loss: 19840.605469\n",
      "Train Epoch: 164 [100032/225000 (44%)] Loss: 18935.257812\n",
      "Train Epoch: 164 [102528/225000 (46%)] Loss: 19571.863281\n",
      "Train Epoch: 164 [105024/225000 (47%)] Loss: 18428.730469\n",
      "Train Epoch: 164 [107520/225000 (48%)] Loss: 19265.189453\n",
      "Train Epoch: 164 [110016/225000 (49%)] Loss: 19774.912109\n",
      "Train Epoch: 164 [112512/225000 (50%)] Loss: 19756.521484\n",
      "Train Epoch: 164 [115008/225000 (51%)] Loss: 19177.812500\n",
      "Train Epoch: 164 [117504/225000 (52%)] Loss: 19021.835938\n",
      "Train Epoch: 164 [120000/225000 (53%)] Loss: 19045.384766\n",
      "Train Epoch: 164 [122496/225000 (54%)] Loss: 19486.328125\n",
      "Train Epoch: 164 [124992/225000 (56%)] Loss: 19109.742188\n",
      "Train Epoch: 164 [127488/225000 (57%)] Loss: 19040.121094\n",
      "Train Epoch: 164 [129984/225000 (58%)] Loss: 19321.835938\n",
      "Train Epoch: 164 [132480/225000 (59%)] Loss: 19472.023438\n",
      "Train Epoch: 164 [134976/225000 (60%)] Loss: 19256.462891\n",
      "Train Epoch: 164 [137472/225000 (61%)] Loss: 19104.531250\n",
      "Train Epoch: 164 [139968/225000 (62%)] Loss: 19499.121094\n",
      "Train Epoch: 164 [142464/225000 (63%)] Loss: 19308.589844\n",
      "Train Epoch: 164 [144960/225000 (64%)] Loss: 19173.861328\n",
      "Train Epoch: 164 [147456/225000 (66%)] Loss: 19392.169922\n",
      "Train Epoch: 164 [149952/225000 (67%)] Loss: 18884.617188\n",
      "Train Epoch: 164 [152448/225000 (68%)] Loss: 19287.367188\n",
      "Train Epoch: 164 [154944/225000 (69%)] Loss: 18854.453125\n",
      "Train Epoch: 164 [157440/225000 (70%)] Loss: 19451.894531\n",
      "Train Epoch: 164 [159936/225000 (71%)] Loss: 18798.222656\n",
      "Train Epoch: 164 [162432/225000 (72%)] Loss: 18938.048828\n",
      "Train Epoch: 164 [164928/225000 (73%)] Loss: 18878.982422\n",
      "Train Epoch: 164 [167424/225000 (74%)] Loss: 19016.546875\n",
      "Train Epoch: 164 [169920/225000 (76%)] Loss: 19250.781250\n",
      "Train Epoch: 164 [172416/225000 (77%)] Loss: 19211.726562\n",
      "Train Epoch: 164 [174912/225000 (78%)] Loss: 19482.859375\n",
      "Train Epoch: 164 [177408/225000 (79%)] Loss: 19031.164062\n",
      "Train Epoch: 164 [179904/225000 (80%)] Loss: 18889.175781\n",
      "Train Epoch: 164 [182400/225000 (81%)] Loss: 19060.269531\n",
      "Train Epoch: 164 [184896/225000 (82%)] Loss: 19187.591797\n",
      "Train Epoch: 164 [187392/225000 (83%)] Loss: 19374.767578\n",
      "Train Epoch: 164 [189888/225000 (84%)] Loss: 19240.498047\n",
      "Train Epoch: 164 [192384/225000 (86%)] Loss: 19106.335938\n",
      "Train Epoch: 164 [194880/225000 (87%)] Loss: 18852.746094\n",
      "Train Epoch: 164 [197376/225000 (88%)] Loss: 19118.289062\n",
      "Train Epoch: 164 [199872/225000 (89%)] Loss: 19013.802734\n",
      "Train Epoch: 164 [202368/225000 (90%)] Loss: 18642.921875\n",
      "Train Epoch: 164 [204864/225000 (91%)] Loss: 18938.382812\n",
      "Train Epoch: 164 [207360/225000 (92%)] Loss: 19511.015625\n",
      "Train Epoch: 164 [209856/225000 (93%)] Loss: 19187.144531\n",
      "Train Epoch: 164 [212352/225000 (94%)] Loss: 19675.273438\n",
      "Train Epoch: 164 [214848/225000 (95%)] Loss: 18833.972656\n",
      "Train Epoch: 164 [217344/225000 (97%)] Loss: 19386.089844\n",
      "Train Epoch: 164 [219840/225000 (98%)] Loss: 18868.417969\n",
      "Train Epoch: 164 [222336/225000 (99%)] Loss: 19418.003906\n",
      "Train Epoch: 164 [224832/225000 (100%)] Loss: 18988.365234\n",
      "    epoch          : 164\n",
      "    loss           : 19218.852227429074\n",
      "    val_loss       : 19119.953449405788\n",
      "Train Epoch: 165 [192/225000 (0%)] Loss: 19534.636719\n",
      "Train Epoch: 165 [2688/225000 (1%)] Loss: 19771.634766\n",
      "Train Epoch: 165 [5184/225000 (2%)] Loss: 19368.451172\n",
      "Train Epoch: 165 [7680/225000 (3%)] Loss: 19684.101562\n",
      "Train Epoch: 165 [10176/225000 (5%)] Loss: 18628.951172\n",
      "Train Epoch: 165 [12672/225000 (6%)] Loss: 19293.160156\n",
      "Train Epoch: 165 [15168/225000 (7%)] Loss: 19345.994141\n",
      "Train Epoch: 165 [17664/225000 (8%)] Loss: 19321.105469\n",
      "Train Epoch: 165 [20160/225000 (9%)] Loss: 19258.621094\n",
      "Train Epoch: 165 [22656/225000 (10%)] Loss: 19215.728516\n",
      "Train Epoch: 165 [25152/225000 (11%)] Loss: 19227.412109\n",
      "Train Epoch: 165 [27648/225000 (12%)] Loss: 18634.191406\n",
      "Train Epoch: 165 [30144/225000 (13%)] Loss: 19246.531250\n",
      "Train Epoch: 165 [32640/225000 (15%)] Loss: 19268.378906\n",
      "Train Epoch: 165 [35136/225000 (16%)] Loss: 19829.966797\n",
      "Train Epoch: 165 [37632/225000 (17%)] Loss: 19047.664062\n",
      "Train Epoch: 165 [40128/225000 (18%)] Loss: 18969.527344\n",
      "Train Epoch: 165 [42624/225000 (19%)] Loss: 19600.992188\n",
      "Train Epoch: 165 [45120/225000 (20%)] Loss: 19505.951172\n",
      "Train Epoch: 165 [47616/225000 (21%)] Loss: 19393.863281\n",
      "Train Epoch: 165 [50112/225000 (22%)] Loss: 19205.535156\n",
      "Train Epoch: 165 [52608/225000 (23%)] Loss: 19438.187500\n",
      "Train Epoch: 165 [55104/225000 (24%)] Loss: 19040.046875\n",
      "Train Epoch: 165 [57600/225000 (26%)] Loss: 18990.156250\n",
      "Train Epoch: 165 [60096/225000 (27%)] Loss: 19020.968750\n",
      "Train Epoch: 165 [62592/225000 (28%)] Loss: 19293.484375\n",
      "Train Epoch: 165 [65088/225000 (29%)] Loss: 18812.269531\n",
      "Train Epoch: 165 [67584/225000 (30%)] Loss: 19335.533203\n",
      "Train Epoch: 165 [70080/225000 (31%)] Loss: 19031.042969\n",
      "Train Epoch: 165 [72576/225000 (32%)] Loss: 19144.808594\n",
      "Train Epoch: 165 [75072/225000 (33%)] Loss: 19391.566406\n",
      "Train Epoch: 165 [77568/225000 (34%)] Loss: 18990.578125\n",
      "Train Epoch: 165 [80064/225000 (36%)] Loss: 19138.597656\n",
      "Train Epoch: 165 [82560/225000 (37%)] Loss: 19558.853516\n",
      "Train Epoch: 165 [85056/225000 (38%)] Loss: 19305.585938\n",
      "Train Epoch: 165 [87552/225000 (39%)] Loss: 19390.810547\n",
      "Train Epoch: 165 [90048/225000 (40%)] Loss: 19587.330078\n",
      "Train Epoch: 165 [92544/225000 (41%)] Loss: 19214.814453\n",
      "Train Epoch: 165 [95040/225000 (42%)] Loss: 19293.113281\n",
      "Train Epoch: 165 [97536/225000 (43%)] Loss: 19208.332031\n",
      "Train Epoch: 165 [100032/225000 (44%)] Loss: 18705.205078\n",
      "Train Epoch: 165 [102528/225000 (46%)] Loss: 19491.539062\n",
      "Train Epoch: 165 [105024/225000 (47%)] Loss: 19461.175781\n",
      "Train Epoch: 165 [107520/225000 (48%)] Loss: 18880.335938\n",
      "Train Epoch: 165 [110016/225000 (49%)] Loss: 19166.335938\n",
      "Train Epoch: 165 [112512/225000 (50%)] Loss: 19237.566406\n",
      "Train Epoch: 165 [115008/225000 (51%)] Loss: 19190.714844\n",
      "Train Epoch: 165 [117504/225000 (52%)] Loss: 19531.087891\n",
      "Train Epoch: 165 [120000/225000 (53%)] Loss: 19610.404297\n",
      "Train Epoch: 165 [122496/225000 (54%)] Loss: 20280.628906\n",
      "Train Epoch: 165 [124992/225000 (56%)] Loss: 19572.449219\n",
      "Train Epoch: 165 [127488/225000 (57%)] Loss: 19079.218750\n",
      "Train Epoch: 165 [129984/225000 (58%)] Loss: 19318.460938\n",
      "Train Epoch: 165 [132480/225000 (59%)] Loss: 19618.121094\n",
      "Train Epoch: 165 [134976/225000 (60%)] Loss: 19481.664062\n",
      "Train Epoch: 165 [137472/225000 (61%)] Loss: 19486.673828\n",
      "Train Epoch: 165 [139968/225000 (62%)] Loss: 19275.333984\n",
      "Train Epoch: 165 [142464/225000 (63%)] Loss: 18967.671875\n",
      "Train Epoch: 165 [144960/225000 (64%)] Loss: 19563.650391\n",
      "Train Epoch: 165 [147456/225000 (66%)] Loss: 19469.322266\n",
      "Train Epoch: 165 [149952/225000 (67%)] Loss: 19045.863281\n",
      "Train Epoch: 165 [152448/225000 (68%)] Loss: 19308.681641\n",
      "Train Epoch: 165 [154944/225000 (69%)] Loss: 19275.787109\n",
      "Train Epoch: 165 [157440/225000 (70%)] Loss: 19589.296875\n",
      "Train Epoch: 165 [159936/225000 (71%)] Loss: 19624.664062\n",
      "Train Epoch: 165 [162432/225000 (72%)] Loss: 19222.875000\n",
      "Train Epoch: 165 [164928/225000 (73%)] Loss: 19324.507812\n",
      "Train Epoch: 165 [167424/225000 (74%)] Loss: 18432.078125\n",
      "Train Epoch: 165 [169920/225000 (76%)] Loss: 19489.355469\n",
      "Train Epoch: 165 [172416/225000 (77%)] Loss: 19022.718750\n",
      "Train Epoch: 165 [174912/225000 (78%)] Loss: 19148.210938\n",
      "Train Epoch: 165 [177408/225000 (79%)] Loss: 18934.445312\n",
      "Train Epoch: 165 [179904/225000 (80%)] Loss: 18810.007812\n",
      "Train Epoch: 165 [182400/225000 (81%)] Loss: 19023.636719\n",
      "Train Epoch: 165 [184896/225000 (82%)] Loss: 19139.343750\n",
      "Train Epoch: 165 [187392/225000 (83%)] Loss: 19393.843750\n",
      "Train Epoch: 165 [189888/225000 (84%)] Loss: 18910.375000\n",
      "Train Epoch: 165 [192384/225000 (86%)] Loss: 19151.072266\n",
      "Train Epoch: 165 [194880/225000 (87%)] Loss: 18932.056641\n",
      "Train Epoch: 165 [197376/225000 (88%)] Loss: 19143.484375\n",
      "Train Epoch: 165 [199872/225000 (89%)] Loss: 19192.314453\n",
      "Train Epoch: 165 [202368/225000 (90%)] Loss: 19027.699219\n",
      "Train Epoch: 165 [204864/225000 (91%)] Loss: 19202.917969\n",
      "Train Epoch: 165 [207360/225000 (92%)] Loss: 19217.134766\n",
      "Train Epoch: 165 [209856/225000 (93%)] Loss: 19534.525391\n",
      "Train Epoch: 165 [212352/225000 (94%)] Loss: 19331.390625\n",
      "Train Epoch: 165 [214848/225000 (95%)] Loss: 19399.279297\n",
      "Train Epoch: 165 [217344/225000 (97%)] Loss: 19091.417969\n",
      "Train Epoch: 165 [219840/225000 (98%)] Loss: 19896.126953\n",
      "Train Epoch: 165 [222336/225000 (99%)] Loss: 19110.990234\n",
      "Train Epoch: 165 [224832/225000 (100%)] Loss: 19488.347656\n",
      "    epoch          : 165\n",
      "    loss           : 19225.990302701044\n",
      "    val_loss       : 19163.279921976664\n",
      "Train Epoch: 166 [192/225000 (0%)] Loss: 18640.207031\n",
      "Train Epoch: 166 [2688/225000 (1%)] Loss: 18772.003906\n",
      "Train Epoch: 166 [5184/225000 (2%)] Loss: 19200.478516\n",
      "Train Epoch: 166 [7680/225000 (3%)] Loss: 19272.896484\n",
      "Train Epoch: 166 [10176/225000 (5%)] Loss: 19746.304688\n",
      "Train Epoch: 166 [12672/225000 (6%)] Loss: 19222.525391\n",
      "Train Epoch: 166 [15168/225000 (7%)] Loss: 19064.134766\n",
      "Train Epoch: 166 [17664/225000 (8%)] Loss: 19000.421875\n",
      "Train Epoch: 166 [20160/225000 (9%)] Loss: 19452.820312\n",
      "Train Epoch: 166 [22656/225000 (10%)] Loss: 18995.273438\n",
      "Train Epoch: 166 [25152/225000 (11%)] Loss: 19334.160156\n",
      "Train Epoch: 166 [27648/225000 (12%)] Loss: 19166.287109\n",
      "Train Epoch: 166 [30144/225000 (13%)] Loss: 19302.345703\n",
      "Train Epoch: 166 [32640/225000 (15%)] Loss: 19155.898438\n",
      "Train Epoch: 166 [35136/225000 (16%)] Loss: 19022.160156\n",
      "Train Epoch: 166 [37632/225000 (17%)] Loss: 19402.718750\n",
      "Train Epoch: 166 [40128/225000 (18%)] Loss: 19645.103516\n",
      "Train Epoch: 166 [42624/225000 (19%)] Loss: 19686.517578\n",
      "Train Epoch: 166 [45120/225000 (20%)] Loss: 19302.927734\n",
      "Train Epoch: 166 [47616/225000 (21%)] Loss: 19430.796875\n",
      "Train Epoch: 166 [50112/225000 (22%)] Loss: 19152.341797\n",
      "Train Epoch: 166 [52608/225000 (23%)] Loss: 19303.554688\n",
      "Train Epoch: 166 [55104/225000 (24%)] Loss: 19666.710938\n",
      "Train Epoch: 166 [57600/225000 (26%)] Loss: 19337.373047\n",
      "Train Epoch: 166 [60096/225000 (27%)] Loss: 19125.841797\n",
      "Train Epoch: 166 [62592/225000 (28%)] Loss: 19024.414062\n",
      "Train Epoch: 166 [65088/225000 (29%)] Loss: 19178.617188\n",
      "Train Epoch: 166 [67584/225000 (30%)] Loss: 19290.054688\n",
      "Train Epoch: 166 [70080/225000 (31%)] Loss: 18968.052734\n",
      "Train Epoch: 166 [72576/225000 (32%)] Loss: 19353.652344\n",
      "Train Epoch: 166 [75072/225000 (33%)] Loss: 19065.097656\n",
      "Train Epoch: 166 [77568/225000 (34%)] Loss: 18888.414062\n",
      "Train Epoch: 166 [80064/225000 (36%)] Loss: 19249.519531\n",
      "Train Epoch: 166 [82560/225000 (37%)] Loss: 19300.394531\n",
      "Train Epoch: 166 [85056/225000 (38%)] Loss: 18664.824219\n",
      "Train Epoch: 166 [87552/225000 (39%)] Loss: 19163.529297\n",
      "Train Epoch: 166 [90048/225000 (40%)] Loss: 19214.187500\n",
      "Train Epoch: 166 [92544/225000 (41%)] Loss: 19284.078125\n",
      "Train Epoch: 166 [95040/225000 (42%)] Loss: 19136.453125\n",
      "Train Epoch: 166 [97536/225000 (43%)] Loss: 18872.808594\n",
      "Train Epoch: 166 [100032/225000 (44%)] Loss: 19544.031250\n",
      "Train Epoch: 166 [102528/225000 (46%)] Loss: 19567.748047\n",
      "Train Epoch: 166 [105024/225000 (47%)] Loss: 18894.855469\n",
      "Train Epoch: 166 [107520/225000 (48%)] Loss: 19313.390625\n",
      "Train Epoch: 166 [110016/225000 (49%)] Loss: 19023.082031\n",
      "Train Epoch: 166 [112512/225000 (50%)] Loss: 19005.320312\n",
      "Train Epoch: 166 [115008/225000 (51%)] Loss: 19127.656250\n",
      "Train Epoch: 166 [117504/225000 (52%)] Loss: 18728.015625\n",
      "Train Epoch: 166 [120000/225000 (53%)] Loss: 19209.406250\n",
      "Train Epoch: 166 [122496/225000 (54%)] Loss: 19315.355469\n",
      "Train Epoch: 166 [124992/225000 (56%)] Loss: 19368.390625\n",
      "Train Epoch: 166 [127488/225000 (57%)] Loss: 19010.689453\n",
      "Train Epoch: 166 [129984/225000 (58%)] Loss: 19037.855469\n",
      "Train Epoch: 166 [132480/225000 (59%)] Loss: 18984.460938\n",
      "Train Epoch: 166 [134976/225000 (60%)] Loss: 19615.898438\n",
      "Train Epoch: 166 [137472/225000 (61%)] Loss: 19132.183594\n",
      "Train Epoch: 166 [139968/225000 (62%)] Loss: 19216.042969\n",
      "Train Epoch: 166 [142464/225000 (63%)] Loss: 18842.515625\n",
      "Train Epoch: 166 [144960/225000 (64%)] Loss: 19453.910156\n",
      "Train Epoch: 166 [147456/225000 (66%)] Loss: 19323.339844\n",
      "Train Epoch: 166 [149952/225000 (67%)] Loss: 19327.560547\n",
      "Train Epoch: 166 [152448/225000 (68%)] Loss: 19342.179688\n",
      "Train Epoch: 166 [154944/225000 (69%)] Loss: 19300.000000\n",
      "Train Epoch: 166 [157440/225000 (70%)] Loss: 19309.906250\n",
      "Train Epoch: 166 [159936/225000 (71%)] Loss: 18990.207031\n",
      "Train Epoch: 166 [162432/225000 (72%)] Loss: 19224.207031\n",
      "Train Epoch: 166 [164928/225000 (73%)] Loss: 19127.382812\n",
      "Train Epoch: 166 [167424/225000 (74%)] Loss: 18805.460938\n",
      "Train Epoch: 166 [169920/225000 (76%)] Loss: 19143.576172\n",
      "Train Epoch: 166 [172416/225000 (77%)] Loss: 18889.746094\n",
      "Train Epoch: 166 [174912/225000 (78%)] Loss: 18983.478516\n",
      "Train Epoch: 166 [177408/225000 (79%)] Loss: 18797.218750\n",
      "Train Epoch: 166 [179904/225000 (80%)] Loss: 19010.130859\n",
      "Train Epoch: 166 [182400/225000 (81%)] Loss: 18925.037109\n",
      "Train Epoch: 166 [184896/225000 (82%)] Loss: 19262.308594\n",
      "Train Epoch: 166 [187392/225000 (83%)] Loss: 19343.115234\n",
      "Train Epoch: 166 [189888/225000 (84%)] Loss: 18975.234375\n",
      "Train Epoch: 166 [192384/225000 (86%)] Loss: 19046.816406\n",
      "Train Epoch: 166 [194880/225000 (87%)] Loss: 18987.847656\n",
      "Train Epoch: 166 [197376/225000 (88%)] Loss: 19245.173828\n",
      "Train Epoch: 166 [199872/225000 (89%)] Loss: 19247.640625\n",
      "Train Epoch: 166 [202368/225000 (90%)] Loss: 19002.285156\n",
      "Train Epoch: 166 [204864/225000 (91%)] Loss: 18945.820312\n",
      "Train Epoch: 166 [207360/225000 (92%)] Loss: 18819.753906\n",
      "Train Epoch: 166 [209856/225000 (93%)] Loss: 19351.652344\n",
      "Train Epoch: 166 [212352/225000 (94%)] Loss: 19317.982422\n",
      "Train Epoch: 166 [214848/225000 (95%)] Loss: 19365.226562\n",
      "Train Epoch: 166 [217344/225000 (97%)] Loss: 19484.404297\n",
      "Train Epoch: 166 [219840/225000 (98%)] Loss: 19318.441406\n",
      "Train Epoch: 166 [222336/225000 (99%)] Loss: 19689.009766\n",
      "Train Epoch: 166 [224832/225000 (100%)] Loss: 19333.226562\n",
      "    epoch          : 166\n",
      "    loss           : 19220.36653090337\n",
      "    val_loss       : 19113.46707072968\n",
      "Train Epoch: 167 [192/225000 (0%)] Loss: 19010.125000\n",
      "Train Epoch: 167 [2688/225000 (1%)] Loss: 19548.210938\n",
      "Train Epoch: 167 [5184/225000 (2%)] Loss: 19562.058594\n",
      "Train Epoch: 167 [7680/225000 (3%)] Loss: 19395.130859\n",
      "Train Epoch: 167 [10176/225000 (5%)] Loss: 19154.849609\n",
      "Train Epoch: 167 [12672/225000 (6%)] Loss: 19229.839844\n",
      "Train Epoch: 167 [15168/225000 (7%)] Loss: 19046.839844\n",
      "Train Epoch: 167 [17664/225000 (8%)] Loss: 19066.343750\n",
      "Train Epoch: 167 [20160/225000 (9%)] Loss: 19078.839844\n",
      "Train Epoch: 167 [22656/225000 (10%)] Loss: 19076.062500\n",
      "Train Epoch: 167 [25152/225000 (11%)] Loss: 19422.722656\n",
      "Train Epoch: 167 [27648/225000 (12%)] Loss: 18927.316406\n",
      "Train Epoch: 167 [30144/225000 (13%)] Loss: 19277.501953\n",
      "Train Epoch: 167 [32640/225000 (15%)] Loss: 19618.453125\n",
      "Train Epoch: 167 [35136/225000 (16%)] Loss: 19516.574219\n",
      "Train Epoch: 167 [37632/225000 (17%)] Loss: 18803.476562\n",
      "Train Epoch: 167 [40128/225000 (18%)] Loss: 19104.949219\n",
      "Train Epoch: 167 [42624/225000 (19%)] Loss: 19074.869141\n",
      "Train Epoch: 167 [45120/225000 (20%)] Loss: 19372.234375\n",
      "Train Epoch: 167 [47616/225000 (21%)] Loss: 19616.578125\n",
      "Train Epoch: 167 [50112/225000 (22%)] Loss: 19049.382812\n",
      "Train Epoch: 167 [52608/225000 (23%)] Loss: 19470.433594\n",
      "Train Epoch: 167 [55104/225000 (24%)] Loss: 18763.089844\n",
      "Train Epoch: 167 [57600/225000 (26%)] Loss: 18888.218750\n",
      "Train Epoch: 167 [60096/225000 (27%)] Loss: 19465.144531\n",
      "Train Epoch: 167 [62592/225000 (28%)] Loss: 19149.828125\n",
      "Train Epoch: 167 [65088/225000 (29%)] Loss: 19273.349609\n",
      "Train Epoch: 167 [67584/225000 (30%)] Loss: 19157.550781\n",
      "Train Epoch: 167 [70080/225000 (31%)] Loss: 18690.925781\n",
      "Train Epoch: 167 [72576/225000 (32%)] Loss: 19854.513672\n",
      "Train Epoch: 167 [75072/225000 (33%)] Loss: 19364.421875\n",
      "Train Epoch: 167 [77568/225000 (34%)] Loss: 19499.908203\n",
      "Train Epoch: 167 [80064/225000 (36%)] Loss: 19076.712891\n",
      "Train Epoch: 167 [82560/225000 (37%)] Loss: 19453.523438\n",
      "Train Epoch: 167 [85056/225000 (38%)] Loss: 19218.984375\n",
      "Train Epoch: 167 [87552/225000 (39%)] Loss: 18941.093750\n",
      "Train Epoch: 167 [90048/225000 (40%)] Loss: 19523.691406\n",
      "Train Epoch: 167 [92544/225000 (41%)] Loss: 19298.144531\n",
      "Train Epoch: 167 [95040/225000 (42%)] Loss: 19511.423828\n",
      "Train Epoch: 167 [97536/225000 (43%)] Loss: 18977.195312\n",
      "Train Epoch: 167 [100032/225000 (44%)] Loss: 18808.888672\n",
      "Train Epoch: 167 [102528/225000 (46%)] Loss: 19145.865234\n",
      "Train Epoch: 167 [105024/225000 (47%)] Loss: 19429.714844\n",
      "Train Epoch: 167 [107520/225000 (48%)] Loss: 18960.722656\n",
      "Train Epoch: 167 [110016/225000 (49%)] Loss: 19099.453125\n",
      "Train Epoch: 167 [112512/225000 (50%)] Loss: 19183.015625\n",
      "Train Epoch: 167 [115008/225000 (51%)] Loss: 19189.066406\n",
      "Train Epoch: 167 [117504/225000 (52%)] Loss: 19247.726562\n",
      "Train Epoch: 167 [120000/225000 (53%)] Loss: 19116.968750\n",
      "Train Epoch: 167 [122496/225000 (54%)] Loss: 19302.898438\n",
      "Train Epoch: 167 [124992/225000 (56%)] Loss: 19015.515625\n",
      "Train Epoch: 167 [127488/225000 (57%)] Loss: 18971.433594\n",
      "Train Epoch: 167 [129984/225000 (58%)] Loss: 18953.062500\n",
      "Train Epoch: 167 [132480/225000 (59%)] Loss: 19101.234375\n",
      "Train Epoch: 167 [134976/225000 (60%)] Loss: 19357.121094\n",
      "Train Epoch: 167 [137472/225000 (61%)] Loss: 19219.277344\n",
      "Train Epoch: 167 [139968/225000 (62%)] Loss: 18966.626953\n",
      "Train Epoch: 167 [142464/225000 (63%)] Loss: 19059.664062\n",
      "Train Epoch: 167 [144960/225000 (64%)] Loss: 19371.312500\n",
      "Train Epoch: 167 [147456/225000 (66%)] Loss: 19281.777344\n",
      "Train Epoch: 167 [149952/225000 (67%)] Loss: 19216.333984\n",
      "Train Epoch: 167 [152448/225000 (68%)] Loss: 19084.634766\n",
      "Train Epoch: 167 [154944/225000 (69%)] Loss: 18972.148438\n",
      "Train Epoch: 167 [157440/225000 (70%)] Loss: 19338.726562\n",
      "Train Epoch: 167 [159936/225000 (71%)] Loss: 19246.433594\n",
      "Train Epoch: 167 [162432/225000 (72%)] Loss: 18793.044922\n",
      "Train Epoch: 167 [164928/225000 (73%)] Loss: 18974.636719\n",
      "Train Epoch: 167 [167424/225000 (74%)] Loss: 19847.382812\n",
      "Train Epoch: 167 [169920/225000 (76%)] Loss: 19295.953125\n",
      "Train Epoch: 167 [172416/225000 (77%)] Loss: 19103.468750\n",
      "Train Epoch: 167 [174912/225000 (78%)] Loss: 19198.687500\n",
      "Train Epoch: 167 [177408/225000 (79%)] Loss: 19721.718750\n",
      "Train Epoch: 167 [179904/225000 (80%)] Loss: 18896.687500\n",
      "Train Epoch: 167 [182400/225000 (81%)] Loss: 19087.269531\n",
      "Train Epoch: 167 [184896/225000 (82%)] Loss: 19521.500000\n",
      "Train Epoch: 167 [187392/225000 (83%)] Loss: 19005.800781\n",
      "Train Epoch: 167 [189888/225000 (84%)] Loss: 19218.000000\n",
      "Train Epoch: 167 [192384/225000 (86%)] Loss: 19343.259766\n",
      "Train Epoch: 167 [194880/225000 (87%)] Loss: 18937.128906\n",
      "Train Epoch: 167 [197376/225000 (88%)] Loss: 19154.431641\n",
      "Train Epoch: 167 [199872/225000 (89%)] Loss: 19475.875000\n",
      "Train Epoch: 167 [202368/225000 (90%)] Loss: 19432.367188\n",
      "Train Epoch: 167 [204864/225000 (91%)] Loss: 19050.960938\n",
      "Train Epoch: 167 [207360/225000 (92%)] Loss: 19162.171875\n",
      "Train Epoch: 167 [209856/225000 (93%)] Loss: 19714.472656\n",
      "Train Epoch: 167 [212352/225000 (94%)] Loss: 19037.542969\n",
      "Train Epoch: 167 [214848/225000 (95%)] Loss: 19413.093750\n",
      "Train Epoch: 167 [217344/225000 (97%)] Loss: 19288.773438\n",
      "Train Epoch: 167 [219840/225000 (98%)] Loss: 19056.289062\n",
      "Train Epoch: 167 [222336/225000 (99%)] Loss: 19185.878906\n",
      "Train Epoch: 167 [224832/225000 (100%)] Loss: 18740.001953\n",
      "    epoch          : 167\n",
      "    loss           : 19211.824047101643\n",
      "    val_loss       : 19123.29339090786\n",
      "Train Epoch: 168 [192/225000 (0%)] Loss: 19051.951172\n",
      "Train Epoch: 168 [2688/225000 (1%)] Loss: 19280.605469\n",
      "Train Epoch: 168 [5184/225000 (2%)] Loss: 19117.027344\n",
      "Train Epoch: 168 [7680/225000 (3%)] Loss: 19137.458984\n",
      "Train Epoch: 168 [10176/225000 (5%)] Loss: 18691.093750\n",
      "Train Epoch: 168 [12672/225000 (6%)] Loss: 19219.207031\n",
      "Train Epoch: 168 [15168/225000 (7%)] Loss: 19006.267578\n",
      "Train Epoch: 168 [17664/225000 (8%)] Loss: 19208.042969\n",
      "Train Epoch: 168 [20160/225000 (9%)] Loss: 19673.259766\n",
      "Train Epoch: 168 [22656/225000 (10%)] Loss: 19427.113281\n",
      "Train Epoch: 168 [25152/225000 (11%)] Loss: 19290.394531\n",
      "Train Epoch: 168 [27648/225000 (12%)] Loss: 19364.542969\n",
      "Train Epoch: 168 [30144/225000 (13%)] Loss: 19363.656250\n",
      "Train Epoch: 168 [32640/225000 (15%)] Loss: 19401.457031\n",
      "Train Epoch: 168 [35136/225000 (16%)] Loss: 19160.703125\n",
      "Train Epoch: 168 [37632/225000 (17%)] Loss: 19432.990234\n",
      "Train Epoch: 168 [40128/225000 (18%)] Loss: 19153.957031\n",
      "Train Epoch: 168 [42624/225000 (19%)] Loss: 19446.593750\n",
      "Train Epoch: 168 [45120/225000 (20%)] Loss: 19494.720703\n",
      "Train Epoch: 168 [47616/225000 (21%)] Loss: 19342.941406\n",
      "Train Epoch: 168 [50112/225000 (22%)] Loss: 19367.785156\n",
      "Train Epoch: 168 [52608/225000 (23%)] Loss: 19096.617188\n",
      "Train Epoch: 168 [55104/225000 (24%)] Loss: 20000.457031\n",
      "Train Epoch: 168 [57600/225000 (26%)] Loss: 19079.628906\n",
      "Train Epoch: 168 [60096/225000 (27%)] Loss: 19286.328125\n",
      "Train Epoch: 168 [62592/225000 (28%)] Loss: 19150.855469\n",
      "Train Epoch: 168 [65088/225000 (29%)] Loss: 19476.871094\n",
      "Train Epoch: 168 [67584/225000 (30%)] Loss: 19188.826172\n",
      "Train Epoch: 168 [70080/225000 (31%)] Loss: 19052.863281\n",
      "Train Epoch: 168 [72576/225000 (32%)] Loss: 19182.804688\n",
      "Train Epoch: 168 [75072/225000 (33%)] Loss: 19421.152344\n",
      "Train Epoch: 168 [77568/225000 (34%)] Loss: 19414.281250\n",
      "Train Epoch: 168 [80064/225000 (36%)] Loss: 18627.777344\n",
      "Train Epoch: 168 [82560/225000 (37%)] Loss: 19065.113281\n",
      "Train Epoch: 168 [85056/225000 (38%)] Loss: 19176.287109\n",
      "Train Epoch: 168 [87552/225000 (39%)] Loss: 18744.583984\n",
      "Train Epoch: 168 [90048/225000 (40%)] Loss: 19226.183594\n",
      "Train Epoch: 168 [92544/225000 (41%)] Loss: 18710.296875\n",
      "Train Epoch: 168 [95040/225000 (42%)] Loss: 18770.476562\n",
      "Train Epoch: 168 [97536/225000 (43%)] Loss: 18690.699219\n",
      "Train Epoch: 168 [100032/225000 (44%)] Loss: 18968.519531\n",
      "Train Epoch: 168 [102528/225000 (46%)] Loss: 19361.695312\n",
      "Train Epoch: 168 [105024/225000 (47%)] Loss: 18822.458984\n",
      "Train Epoch: 168 [107520/225000 (48%)] Loss: 19185.121094\n",
      "Train Epoch: 168 [110016/225000 (49%)] Loss: 19549.121094\n",
      "Train Epoch: 168 [112512/225000 (50%)] Loss: 19349.718750\n",
      "Train Epoch: 168 [115008/225000 (51%)] Loss: 19312.037109\n",
      "Train Epoch: 168 [117504/225000 (52%)] Loss: 19181.009766\n",
      "Train Epoch: 168 [120000/225000 (53%)] Loss: 19032.539062\n",
      "Train Epoch: 168 [122496/225000 (54%)] Loss: 18934.050781\n",
      "Train Epoch: 168 [124992/225000 (56%)] Loss: 18937.734375\n",
      "Train Epoch: 168 [127488/225000 (57%)] Loss: 19353.830078\n",
      "Train Epoch: 168 [129984/225000 (58%)] Loss: 19164.011719\n",
      "Train Epoch: 168 [132480/225000 (59%)] Loss: 19263.562500\n",
      "Train Epoch: 168 [134976/225000 (60%)] Loss: 18896.171875\n",
      "Train Epoch: 168 [137472/225000 (61%)] Loss: 18866.742188\n",
      "Train Epoch: 168 [139968/225000 (62%)] Loss: 19399.105469\n",
      "Train Epoch: 168 [142464/225000 (63%)] Loss: 19028.945312\n",
      "Train Epoch: 168 [144960/225000 (64%)] Loss: 19536.941406\n",
      "Train Epoch: 168 [147456/225000 (66%)] Loss: 19716.089844\n",
      "Train Epoch: 168 [149952/225000 (67%)] Loss: 18581.566406\n",
      "Train Epoch: 168 [152448/225000 (68%)] Loss: 19102.902344\n",
      "Train Epoch: 168 [154944/225000 (69%)] Loss: 18739.771484\n",
      "Train Epoch: 168 [157440/225000 (70%)] Loss: 18992.636719\n",
      "Train Epoch: 168 [159936/225000 (71%)] Loss: 19372.742188\n",
      "Train Epoch: 168 [162432/225000 (72%)] Loss: 19275.919922\n",
      "Train Epoch: 168 [164928/225000 (73%)] Loss: 18935.210938\n",
      "Train Epoch: 168 [167424/225000 (74%)] Loss: 18876.603516\n",
      "Train Epoch: 168 [169920/225000 (76%)] Loss: 19366.937500\n",
      "Train Epoch: 168 [172416/225000 (77%)] Loss: 19451.101562\n",
      "Train Epoch: 168 [174912/225000 (78%)] Loss: 18866.195312\n",
      "Train Epoch: 168 [177408/225000 (79%)] Loss: 19293.705078\n",
      "Train Epoch: 168 [179904/225000 (80%)] Loss: 19246.132812\n",
      "Train Epoch: 168 [182400/225000 (81%)] Loss: 18893.343750\n",
      "Train Epoch: 168 [184896/225000 (82%)] Loss: 19350.162109\n",
      "Train Epoch: 168 [187392/225000 (83%)] Loss: 19219.960938\n",
      "Train Epoch: 168 [189888/225000 (84%)] Loss: 19282.326172\n",
      "Train Epoch: 168 [192384/225000 (86%)] Loss: 19187.105469\n",
      "Train Epoch: 168 [194880/225000 (87%)] Loss: 18906.156250\n",
      "Train Epoch: 168 [197376/225000 (88%)] Loss: 19257.253906\n",
      "Train Epoch: 168 [199872/225000 (89%)] Loss: 19203.191406\n",
      "Train Epoch: 168 [202368/225000 (90%)] Loss: 19673.359375\n",
      "Train Epoch: 168 [204864/225000 (91%)] Loss: 19029.878906\n",
      "Train Epoch: 168 [207360/225000 (92%)] Loss: 19242.671875\n",
      "Train Epoch: 168 [209856/225000 (93%)] Loss: 19651.484375\n",
      "Train Epoch: 168 [212352/225000 (94%)] Loss: 19118.107422\n",
      "Train Epoch: 168 [214848/225000 (95%)] Loss: 18900.582031\n",
      "Train Epoch: 168 [217344/225000 (97%)] Loss: 19145.369141\n",
      "Train Epoch: 168 [219840/225000 (98%)] Loss: 19173.175781\n",
      "Train Epoch: 168 [222336/225000 (99%)] Loss: 19291.433594\n",
      "Train Epoch: 168 [224832/225000 (100%)] Loss: 18997.042969\n",
      "    epoch          : 168\n",
      "    loss           : 19197.600194312607\n",
      "    val_loss       : 19105.602451058745\n",
      "Train Epoch: 169 [192/225000 (0%)] Loss: 18716.687500\n",
      "Train Epoch: 169 [2688/225000 (1%)] Loss: 19423.714844\n",
      "Train Epoch: 169 [5184/225000 (2%)] Loss: 19429.335938\n",
      "Train Epoch: 169 [7680/225000 (3%)] Loss: 18757.285156\n",
      "Train Epoch: 169 [10176/225000 (5%)] Loss: 19015.628906\n",
      "Train Epoch: 169 [12672/225000 (6%)] Loss: 19000.876953\n",
      "Train Epoch: 169 [15168/225000 (7%)] Loss: 19419.894531\n",
      "Train Epoch: 169 [17664/225000 (8%)] Loss: 19093.175781\n",
      "Train Epoch: 169 [20160/225000 (9%)] Loss: 19437.814453\n",
      "Train Epoch: 169 [22656/225000 (10%)] Loss: 19499.972656\n",
      "Train Epoch: 169 [25152/225000 (11%)] Loss: 19340.068359\n",
      "Train Epoch: 169 [27648/225000 (12%)] Loss: 18957.921875\n",
      "Train Epoch: 169 [30144/225000 (13%)] Loss: 19265.167969\n",
      "Train Epoch: 169 [32640/225000 (15%)] Loss: 19078.847656\n",
      "Train Epoch: 169 [35136/225000 (16%)] Loss: 19059.820312\n",
      "Train Epoch: 169 [37632/225000 (17%)] Loss: 19101.566406\n",
      "Train Epoch: 169 [40128/225000 (18%)] Loss: 18738.052734\n",
      "Train Epoch: 169 [42624/225000 (19%)] Loss: 19132.507812\n",
      "Train Epoch: 169 [45120/225000 (20%)] Loss: 19141.728516\n",
      "Train Epoch: 169 [47616/225000 (21%)] Loss: 19213.656250\n",
      "Train Epoch: 169 [50112/225000 (22%)] Loss: 18872.326172\n",
      "Train Epoch: 169 [52608/225000 (23%)] Loss: 19203.968750\n",
      "Train Epoch: 169 [55104/225000 (24%)] Loss: 19087.611328\n",
      "Train Epoch: 169 [57600/225000 (26%)] Loss: 19311.210938\n",
      "Train Epoch: 169 [60096/225000 (27%)] Loss: 19008.792969\n",
      "Train Epoch: 169 [62592/225000 (28%)] Loss: 19276.929688\n",
      "Train Epoch: 169 [65088/225000 (29%)] Loss: 18938.726562\n",
      "Train Epoch: 169 [67584/225000 (30%)] Loss: 19335.613281\n",
      "Train Epoch: 169 [70080/225000 (31%)] Loss: 19379.371094\n",
      "Train Epoch: 169 [72576/225000 (32%)] Loss: 19318.640625\n",
      "Train Epoch: 169 [75072/225000 (33%)] Loss: 19393.994141\n",
      "Train Epoch: 169 [77568/225000 (34%)] Loss: 18718.179688\n",
      "Train Epoch: 169 [80064/225000 (36%)] Loss: 19048.220703\n",
      "Train Epoch: 169 [82560/225000 (37%)] Loss: 19185.472656\n",
      "Train Epoch: 169 [85056/225000 (38%)] Loss: 19586.691406\n",
      "Train Epoch: 169 [87552/225000 (39%)] Loss: 19317.814453\n",
      "Train Epoch: 169 [90048/225000 (40%)] Loss: 19294.617188\n",
      "Train Epoch: 169 [92544/225000 (41%)] Loss: 18838.714844\n",
      "Train Epoch: 169 [95040/225000 (42%)] Loss: 18728.458984\n",
      "Train Epoch: 169 [97536/225000 (43%)] Loss: 19153.503906\n",
      "Train Epoch: 169 [100032/225000 (44%)] Loss: 18910.058594\n",
      "Train Epoch: 169 [102528/225000 (46%)] Loss: 19609.042969\n",
      "Train Epoch: 169 [105024/225000 (47%)] Loss: 19385.757812\n",
      "Train Epoch: 169 [107520/225000 (48%)] Loss: 19543.123047\n",
      "Train Epoch: 169 [110016/225000 (49%)] Loss: 19349.062500\n",
      "Train Epoch: 169 [112512/225000 (50%)] Loss: 19552.414062\n",
      "Train Epoch: 169 [115008/225000 (51%)] Loss: 19098.476562\n",
      "Train Epoch: 169 [117504/225000 (52%)] Loss: 18919.466797\n",
      "Train Epoch: 169 [120000/225000 (53%)] Loss: 19513.136719\n",
      "Train Epoch: 169 [122496/225000 (54%)] Loss: 19153.591797\n",
      "Train Epoch: 169 [124992/225000 (56%)] Loss: 19422.296875\n",
      "Train Epoch: 169 [127488/225000 (57%)] Loss: 18997.052734\n",
      "Train Epoch: 169 [129984/225000 (58%)] Loss: 18928.791016\n",
      "Train Epoch: 169 [132480/225000 (59%)] Loss: 19464.800781\n",
      "Train Epoch: 169 [134976/225000 (60%)] Loss: 19160.109375\n",
      "Train Epoch: 169 [137472/225000 (61%)] Loss: 19442.853516\n",
      "Train Epoch: 169 [139968/225000 (62%)] Loss: 19322.378906\n",
      "Train Epoch: 169 [142464/225000 (63%)] Loss: 19597.632812\n",
      "Train Epoch: 169 [144960/225000 (64%)] Loss: 19263.242188\n",
      "Train Epoch: 169 [147456/225000 (66%)] Loss: 18945.554688\n",
      "Train Epoch: 169 [149952/225000 (67%)] Loss: 19252.773438\n",
      "Train Epoch: 169 [152448/225000 (68%)] Loss: 19556.109375\n",
      "Train Epoch: 169 [154944/225000 (69%)] Loss: 19240.070312\n",
      "Train Epoch: 169 [157440/225000 (70%)] Loss: 19305.851562\n",
      "Train Epoch: 169 [159936/225000 (71%)] Loss: 19469.480469\n",
      "Train Epoch: 169 [162432/225000 (72%)] Loss: 19171.339844\n",
      "Train Epoch: 169 [164928/225000 (73%)] Loss: 19502.720703\n",
      "Train Epoch: 169 [167424/225000 (74%)] Loss: 19218.355469\n",
      "Train Epoch: 169 [169920/225000 (76%)] Loss: 19087.759766\n",
      "Train Epoch: 169 [172416/225000 (77%)] Loss: 19477.830078\n",
      "Train Epoch: 169 [174912/225000 (78%)] Loss: 19294.734375\n",
      "Train Epoch: 169 [177408/225000 (79%)] Loss: 19762.812500\n",
      "Train Epoch: 169 [179904/225000 (80%)] Loss: 19049.587891\n",
      "Train Epoch: 169 [182400/225000 (81%)] Loss: 19508.656250\n",
      "Train Epoch: 169 [184896/225000 (82%)] Loss: 18714.097656\n",
      "Train Epoch: 169 [187392/225000 (83%)] Loss: 18875.978516\n",
      "Train Epoch: 169 [189888/225000 (84%)] Loss: 19358.275391\n",
      "Train Epoch: 169 [192384/225000 (86%)] Loss: 18575.296875\n",
      "Train Epoch: 169 [194880/225000 (87%)] Loss: 19035.175781\n",
      "Train Epoch: 169 [197376/225000 (88%)] Loss: 19007.343750\n",
      "Train Epoch: 169 [199872/225000 (89%)] Loss: 19127.380859\n",
      "Train Epoch: 169 [202368/225000 (90%)] Loss: 19202.191406\n",
      "Train Epoch: 169 [204864/225000 (91%)] Loss: 19352.001953\n",
      "Train Epoch: 169 [207360/225000 (92%)] Loss: 19542.527344\n",
      "Train Epoch: 169 [209856/225000 (93%)] Loss: 18866.539062\n",
      "Train Epoch: 169 [212352/225000 (94%)] Loss: 19799.921875\n",
      "Train Epoch: 169 [214848/225000 (95%)] Loss: 19327.251953\n",
      "Train Epoch: 169 [217344/225000 (97%)] Loss: 19467.609375\n",
      "Train Epoch: 169 [219840/225000 (98%)] Loss: 19276.007812\n",
      "Train Epoch: 169 [222336/225000 (99%)] Loss: 22443.666016\n",
      "Train Epoch: 169 [224832/225000 (100%)] Loss: 19045.078125\n",
      "    epoch          : 169\n",
      "    loss           : 19201.149589043835\n",
      "    val_loss       : 19109.135546299338\n",
      "Train Epoch: 170 [192/225000 (0%)] Loss: 19224.644531\n",
      "Train Epoch: 170 [2688/225000 (1%)] Loss: 19118.935547\n",
      "Train Epoch: 170 [5184/225000 (2%)] Loss: 19394.937500\n",
      "Train Epoch: 170 [7680/225000 (3%)] Loss: 19270.410156\n",
      "Train Epoch: 170 [10176/225000 (5%)] Loss: 19655.718750\n",
      "Train Epoch: 170 [12672/225000 (6%)] Loss: 19018.123047\n",
      "Train Epoch: 170 [15168/225000 (7%)] Loss: 18698.375000\n",
      "Train Epoch: 170 [17664/225000 (8%)] Loss: 18957.402344\n",
      "Train Epoch: 170 [20160/225000 (9%)] Loss: 19097.994141\n",
      "Train Epoch: 170 [22656/225000 (10%)] Loss: 19642.691406\n",
      "Train Epoch: 170 [25152/225000 (11%)] Loss: 19013.097656\n",
      "Train Epoch: 170 [27648/225000 (12%)] Loss: 19094.958984\n",
      "Train Epoch: 170 [30144/225000 (13%)] Loss: 18717.267578\n",
      "Train Epoch: 170 [32640/225000 (15%)] Loss: 19522.925781\n",
      "Train Epoch: 170 [35136/225000 (16%)] Loss: 19761.503906\n",
      "Train Epoch: 170 [37632/225000 (17%)] Loss: 18946.738281\n",
      "Train Epoch: 170 [40128/225000 (18%)] Loss: 18861.722656\n",
      "Train Epoch: 170 [42624/225000 (19%)] Loss: 19193.226562\n",
      "Train Epoch: 170 [45120/225000 (20%)] Loss: 19513.839844\n",
      "Train Epoch: 170 [47616/225000 (21%)] Loss: 18844.882812\n",
      "Train Epoch: 170 [50112/225000 (22%)] Loss: 18863.695312\n",
      "Train Epoch: 170 [52608/225000 (23%)] Loss: 18904.087891\n",
      "Train Epoch: 170 [55104/225000 (24%)] Loss: 18831.968750\n",
      "Train Epoch: 170 [57600/225000 (26%)] Loss: 19419.753906\n",
      "Train Epoch: 170 [60096/225000 (27%)] Loss: 18888.980469\n",
      "Train Epoch: 170 [62592/225000 (28%)] Loss: 19033.523438\n",
      "Train Epoch: 170 [65088/225000 (29%)] Loss: 19056.361328\n",
      "Train Epoch: 170 [67584/225000 (30%)] Loss: 19393.070312\n",
      "Train Epoch: 170 [70080/225000 (31%)] Loss: 19041.193359\n",
      "Train Epoch: 170 [72576/225000 (32%)] Loss: 18829.757812\n",
      "Train Epoch: 170 [75072/225000 (33%)] Loss: 18823.921875\n",
      "Train Epoch: 170 [77568/225000 (34%)] Loss: 18994.304688\n",
      "Train Epoch: 170 [80064/225000 (36%)] Loss: 19043.351562\n",
      "Train Epoch: 170 [82560/225000 (37%)] Loss: 19501.496094\n",
      "Train Epoch: 170 [85056/225000 (38%)] Loss: 19293.480469\n",
      "Train Epoch: 170 [87552/225000 (39%)] Loss: 18967.253906\n",
      "Train Epoch: 170 [90048/225000 (40%)] Loss: 18704.416016\n",
      "Train Epoch: 170 [92544/225000 (41%)] Loss: 19125.710938\n",
      "Train Epoch: 170 [95040/225000 (42%)] Loss: 19543.072266\n",
      "Train Epoch: 170 [97536/225000 (43%)] Loss: 19172.601562\n",
      "Train Epoch: 170 [100032/225000 (44%)] Loss: 19415.621094\n",
      "Train Epoch: 170 [102528/225000 (46%)] Loss: 18974.851562\n",
      "Train Epoch: 170 [105024/225000 (47%)] Loss: 18705.666016\n",
      "Train Epoch: 170 [107520/225000 (48%)] Loss: 19015.320312\n",
      "Train Epoch: 170 [110016/225000 (49%)] Loss: 19297.751953\n",
      "Train Epoch: 170 [112512/225000 (50%)] Loss: 18854.535156\n",
      "Train Epoch: 170 [115008/225000 (51%)] Loss: 18983.501953\n",
      "Train Epoch: 170 [117504/225000 (52%)] Loss: 19270.007812\n",
      "Train Epoch: 170 [120000/225000 (53%)] Loss: 18886.753906\n",
      "Train Epoch: 170 [122496/225000 (54%)] Loss: 19053.027344\n",
      "Train Epoch: 170 [124992/225000 (56%)] Loss: 19749.974609\n",
      "Train Epoch: 170 [127488/225000 (57%)] Loss: 19227.699219\n",
      "Train Epoch: 170 [129984/225000 (58%)] Loss: 19043.449219\n",
      "Train Epoch: 170 [132480/225000 (59%)] Loss: 19425.738281\n",
      "Train Epoch: 170 [134976/225000 (60%)] Loss: 18697.775391\n",
      "Train Epoch: 170 [137472/225000 (61%)] Loss: 19198.203125\n",
      "Train Epoch: 170 [139968/225000 (62%)] Loss: 19374.158203\n",
      "Train Epoch: 170 [142464/225000 (63%)] Loss: 19406.132812\n",
      "Train Epoch: 170 [144960/225000 (64%)] Loss: 19052.593750\n",
      "Train Epoch: 170 [147456/225000 (66%)] Loss: 18949.046875\n",
      "Train Epoch: 170 [149952/225000 (67%)] Loss: 19538.101562\n",
      "Train Epoch: 170 [152448/225000 (68%)] Loss: 19007.152344\n",
      "Train Epoch: 170 [154944/225000 (69%)] Loss: 19074.792969\n",
      "Train Epoch: 170 [157440/225000 (70%)] Loss: 18741.355469\n",
      "Train Epoch: 170 [159936/225000 (71%)] Loss: 19069.382812\n",
      "Train Epoch: 170 [162432/225000 (72%)] Loss: 19231.580078\n",
      "Train Epoch: 170 [164928/225000 (73%)] Loss: 18883.476562\n",
      "Train Epoch: 170 [167424/225000 (74%)] Loss: 19666.810547\n",
      "Train Epoch: 170 [169920/225000 (76%)] Loss: 18680.101562\n",
      "Train Epoch: 170 [172416/225000 (77%)] Loss: 18701.250000\n",
      "Train Epoch: 170 [174912/225000 (78%)] Loss: 19186.640625\n",
      "Train Epoch: 170 [177408/225000 (79%)] Loss: 19544.585938\n",
      "Train Epoch: 170 [179904/225000 (80%)] Loss: 19248.765625\n",
      "Train Epoch: 170 [182400/225000 (81%)] Loss: 19488.544922\n",
      "Train Epoch: 170 [184896/225000 (82%)] Loss: 19564.644531\n",
      "Train Epoch: 170 [187392/225000 (83%)] Loss: 19406.107422\n",
      "Train Epoch: 170 [189888/225000 (84%)] Loss: 19144.695312\n",
      "Train Epoch: 170 [192384/225000 (86%)] Loss: 19329.394531\n",
      "Train Epoch: 170 [194880/225000 (87%)] Loss: 19371.021484\n",
      "Train Epoch: 170 [197376/225000 (88%)] Loss: 19128.386719\n",
      "Train Epoch: 170 [199872/225000 (89%)] Loss: 18929.833984\n",
      "Train Epoch: 170 [202368/225000 (90%)] Loss: 19470.898438\n",
      "Train Epoch: 170 [204864/225000 (91%)] Loss: 18847.457031\n",
      "Train Epoch: 170 [207360/225000 (92%)] Loss: 19511.275391\n",
      "Train Epoch: 170 [209856/225000 (93%)] Loss: 18916.878906\n",
      "Train Epoch: 170 [212352/225000 (94%)] Loss: 19058.589844\n",
      "Train Epoch: 170 [214848/225000 (95%)] Loss: 18983.949219\n",
      "Train Epoch: 170 [217344/225000 (97%)] Loss: 19133.503906\n",
      "Train Epoch: 170 [219840/225000 (98%)] Loss: 18985.257812\n",
      "Train Epoch: 170 [222336/225000 (99%)] Loss: 19333.800781\n",
      "Train Epoch: 170 [224832/225000 (100%)] Loss: 18883.375000\n",
      "    epoch          : 170\n",
      "    loss           : 19190.78190326365\n",
      "    val_loss       : 19129.959786254032\n",
      "Train Epoch: 171 [192/225000 (0%)] Loss: 19179.164062\n",
      "Train Epoch: 171 [2688/225000 (1%)] Loss: 19464.693359\n",
      "Train Epoch: 171 [5184/225000 (2%)] Loss: 19476.484375\n",
      "Train Epoch: 171 [7680/225000 (3%)] Loss: 19274.330078\n",
      "Train Epoch: 171 [10176/225000 (5%)] Loss: 19105.871094\n",
      "Train Epoch: 171 [12672/225000 (6%)] Loss: 18839.890625\n",
      "Train Epoch: 171 [15168/225000 (7%)] Loss: 18863.781250\n",
      "Train Epoch: 171 [17664/225000 (8%)] Loss: 19416.832031\n",
      "Train Epoch: 171 [20160/225000 (9%)] Loss: 19062.015625\n",
      "Train Epoch: 171 [22656/225000 (10%)] Loss: 18868.548828\n",
      "Train Epoch: 171 [25152/225000 (11%)] Loss: 18918.984375\n",
      "Train Epoch: 171 [27648/225000 (12%)] Loss: 18911.150391\n",
      "Train Epoch: 171 [30144/225000 (13%)] Loss: 19017.726562\n",
      "Train Epoch: 171 [32640/225000 (15%)] Loss: 19379.343750\n",
      "Train Epoch: 171 [35136/225000 (16%)] Loss: 18987.115234\n",
      "Train Epoch: 171 [37632/225000 (17%)] Loss: 19374.355469\n",
      "Train Epoch: 171 [40128/225000 (18%)] Loss: 19439.898438\n",
      "Train Epoch: 171 [42624/225000 (19%)] Loss: 19419.363281\n",
      "Train Epoch: 171 [45120/225000 (20%)] Loss: 19297.292969\n",
      "Train Epoch: 171 [47616/225000 (21%)] Loss: 19407.207031\n",
      "Train Epoch: 171 [50112/225000 (22%)] Loss: 19113.054688\n",
      "Train Epoch: 171 [52608/225000 (23%)] Loss: 19085.933594\n",
      "Train Epoch: 171 [55104/225000 (24%)] Loss: 19225.425781\n",
      "Train Epoch: 171 [57600/225000 (26%)] Loss: 18809.458984\n",
      "Train Epoch: 171 [60096/225000 (27%)] Loss: 18996.875000\n",
      "Train Epoch: 171 [62592/225000 (28%)] Loss: 18895.078125\n",
      "Train Epoch: 171 [65088/225000 (29%)] Loss: 18650.007812\n",
      "Train Epoch: 171 [67584/225000 (30%)] Loss: 19441.464844\n",
      "Train Epoch: 171 [70080/225000 (31%)] Loss: 19000.425781\n",
      "Train Epoch: 171 [72576/225000 (32%)] Loss: 19139.478516\n",
      "Train Epoch: 171 [75072/225000 (33%)] Loss: 19350.539062\n",
      "Train Epoch: 171 [77568/225000 (34%)] Loss: 19298.605469\n",
      "Train Epoch: 171 [80064/225000 (36%)] Loss: 19201.349609\n",
      "Train Epoch: 171 [82560/225000 (37%)] Loss: 19196.980469\n",
      "Train Epoch: 171 [85056/225000 (38%)] Loss: 19124.683594\n",
      "Train Epoch: 171 [87552/225000 (39%)] Loss: 18917.058594\n",
      "Train Epoch: 171 [90048/225000 (40%)] Loss: 19243.644531\n",
      "Train Epoch: 171 [92544/225000 (41%)] Loss: 19359.117188\n",
      "Train Epoch: 171 [95040/225000 (42%)] Loss: 19117.162109\n",
      "Train Epoch: 171 [97536/225000 (43%)] Loss: 19497.019531\n",
      "Train Epoch: 171 [100032/225000 (44%)] Loss: 18712.355469\n",
      "Train Epoch: 171 [102528/225000 (46%)] Loss: 19314.335938\n",
      "Train Epoch: 171 [105024/225000 (47%)] Loss: 18455.986328\n",
      "Train Epoch: 171 [107520/225000 (48%)] Loss: 19338.779297\n",
      "Train Epoch: 171 [110016/225000 (49%)] Loss: 19211.376953\n",
      "Train Epoch: 171 [112512/225000 (50%)] Loss: 19285.322266\n",
      "Train Epoch: 171 [115008/225000 (51%)] Loss: 19239.201172\n",
      "Train Epoch: 171 [117504/225000 (52%)] Loss: 19129.855469\n",
      "Train Epoch: 171 [120000/225000 (53%)] Loss: 19098.029297\n",
      "Train Epoch: 171 [122496/225000 (54%)] Loss: 19413.589844\n",
      "Train Epoch: 171 [124992/225000 (56%)] Loss: 18822.273438\n",
      "Train Epoch: 171 [127488/225000 (57%)] Loss: 19074.414062\n",
      "Train Epoch: 171 [129984/225000 (58%)] Loss: 19450.779297\n",
      "Train Epoch: 171 [132480/225000 (59%)] Loss: 19079.773438\n",
      "Train Epoch: 171 [134976/225000 (60%)] Loss: 19548.324219\n",
      "Train Epoch: 171 [137472/225000 (61%)] Loss: 19372.550781\n",
      "Train Epoch: 171 [139968/225000 (62%)] Loss: 19076.189453\n",
      "Train Epoch: 171 [142464/225000 (63%)] Loss: 19463.203125\n",
      "Train Epoch: 171 [144960/225000 (64%)] Loss: 19080.988281\n",
      "Train Epoch: 171 [147456/225000 (66%)] Loss: 18981.378906\n",
      "Train Epoch: 171 [149952/225000 (67%)] Loss: 19299.886719\n",
      "Train Epoch: 171 [152448/225000 (68%)] Loss: 19385.601562\n",
      "Train Epoch: 171 [154944/225000 (69%)] Loss: 19327.585938\n",
      "Train Epoch: 171 [157440/225000 (70%)] Loss: 19322.060547\n",
      "Train Epoch: 171 [159936/225000 (71%)] Loss: 19255.929688\n",
      "Train Epoch: 171 [162432/225000 (72%)] Loss: 19261.488281\n",
      "Train Epoch: 171 [164928/225000 (73%)] Loss: 19103.322266\n",
      "Train Epoch: 171 [167424/225000 (74%)] Loss: 19090.720703\n",
      "Train Epoch: 171 [169920/225000 (76%)] Loss: 19052.562500\n",
      "Train Epoch: 171 [172416/225000 (77%)] Loss: 19370.222656\n",
      "Train Epoch: 171 [174912/225000 (78%)] Loss: 19227.943359\n",
      "Train Epoch: 171 [177408/225000 (79%)] Loss: 19943.117188\n",
      "Train Epoch: 171 [179904/225000 (80%)] Loss: 19227.546875\n",
      "Train Epoch: 171 [182400/225000 (81%)] Loss: 19302.009766\n",
      "Train Epoch: 171 [184896/225000 (82%)] Loss: 19561.753906\n",
      "Train Epoch: 171 [187392/225000 (83%)] Loss: 18960.515625\n",
      "Train Epoch: 171 [189888/225000 (84%)] Loss: 19388.328125\n",
      "Train Epoch: 171 [192384/225000 (86%)] Loss: 18793.746094\n",
      "Train Epoch: 171 [194880/225000 (87%)] Loss: 19017.882812\n",
      "Train Epoch: 171 [197376/225000 (88%)] Loss: 19103.703125\n",
      "Train Epoch: 171 [199872/225000 (89%)] Loss: 19534.121094\n",
      "Train Epoch: 171 [202368/225000 (90%)] Loss: 19269.515625\n",
      "Train Epoch: 171 [204864/225000 (91%)] Loss: 19370.437500\n",
      "Train Epoch: 171 [207360/225000 (92%)] Loss: 19046.113281\n",
      "Train Epoch: 171 [209856/225000 (93%)] Loss: 19128.230469\n",
      "Train Epoch: 171 [212352/225000 (94%)] Loss: 19018.382812\n",
      "Train Epoch: 171 [214848/225000 (95%)] Loss: 19201.703125\n",
      "Train Epoch: 171 [217344/225000 (97%)] Loss: 19782.546875\n",
      "Train Epoch: 171 [219840/225000 (98%)] Loss: 18961.048828\n",
      "Train Epoch: 171 [222336/225000 (99%)] Loss: 19409.742188\n",
      "Train Epoch: 171 [224832/225000 (100%)] Loss: 18859.257812\n",
      "    epoch          : 171\n",
      "    loss           : 19184.648045875107\n",
      "    val_loss       : 19111.28250053411\n",
      "Train Epoch: 172 [192/225000 (0%)] Loss: 19694.660156\n",
      "Train Epoch: 172 [2688/225000 (1%)] Loss: 19426.031250\n",
      "Train Epoch: 172 [5184/225000 (2%)] Loss: 18386.582031\n",
      "Train Epoch: 172 [7680/225000 (3%)] Loss: 18939.189453\n",
      "Train Epoch: 172 [10176/225000 (5%)] Loss: 19096.337891\n",
      "Train Epoch: 172 [12672/225000 (6%)] Loss: 19029.576172\n",
      "Train Epoch: 172 [15168/225000 (7%)] Loss: 19008.007812\n",
      "Train Epoch: 172 [17664/225000 (8%)] Loss: 19020.546875\n",
      "Train Epoch: 172 [20160/225000 (9%)] Loss: 19173.437500\n",
      "Train Epoch: 172 [22656/225000 (10%)] Loss: 19378.433594\n",
      "Train Epoch: 172 [25152/225000 (11%)] Loss: 19064.062500\n",
      "Train Epoch: 172 [27648/225000 (12%)] Loss: 19025.960938\n",
      "Train Epoch: 172 [30144/225000 (13%)] Loss: 19338.169922\n",
      "Train Epoch: 172 [32640/225000 (15%)] Loss: 19300.511719\n",
      "Train Epoch: 172 [35136/225000 (16%)] Loss: 19349.906250\n",
      "Train Epoch: 172 [37632/225000 (17%)] Loss: 19082.041016\n",
      "Train Epoch: 172 [40128/225000 (18%)] Loss: 19392.089844\n",
      "Train Epoch: 172 [42624/225000 (19%)] Loss: 19310.667969\n",
      "Train Epoch: 172 [45120/225000 (20%)] Loss: 19211.222656\n",
      "Train Epoch: 172 [47616/225000 (21%)] Loss: 19327.554688\n",
      "Train Epoch: 172 [50112/225000 (22%)] Loss: 19261.550781\n",
      "Train Epoch: 172 [52608/225000 (23%)] Loss: 19129.154297\n",
      "Train Epoch: 172 [55104/225000 (24%)] Loss: 19010.232422\n",
      "Train Epoch: 172 [57600/225000 (26%)] Loss: 19468.609375\n",
      "Train Epoch: 172 [60096/225000 (27%)] Loss: 19470.882812\n",
      "Train Epoch: 172 [62592/225000 (28%)] Loss: 18930.560547\n",
      "Train Epoch: 172 [65088/225000 (29%)] Loss: 19206.080078\n",
      "Train Epoch: 172 [67584/225000 (30%)] Loss: 19064.875000\n",
      "Train Epoch: 172 [70080/225000 (31%)] Loss: 19201.683594\n",
      "Train Epoch: 172 [72576/225000 (32%)] Loss: 19169.363281\n",
      "Train Epoch: 172 [75072/225000 (33%)] Loss: 19465.339844\n",
      "Train Epoch: 172 [77568/225000 (34%)] Loss: 18952.191406\n",
      "Train Epoch: 172 [80064/225000 (36%)] Loss: 18944.179688\n",
      "Train Epoch: 172 [82560/225000 (37%)] Loss: 18894.400391\n",
      "Train Epoch: 172 [85056/225000 (38%)] Loss: 19163.539062\n",
      "Train Epoch: 172 [87552/225000 (39%)] Loss: 18891.148438\n",
      "Train Epoch: 172 [90048/225000 (40%)] Loss: 19426.777344\n",
      "Train Epoch: 172 [92544/225000 (41%)] Loss: 19417.757812\n",
      "Train Epoch: 172 [95040/225000 (42%)] Loss: 18969.039062\n",
      "Train Epoch: 172 [97536/225000 (43%)] Loss: 18713.207031\n",
      "Train Epoch: 172 [100032/225000 (44%)] Loss: 18495.921875\n",
      "Train Epoch: 172 [102528/225000 (46%)] Loss: 19945.730469\n",
      "Train Epoch: 172 [105024/225000 (47%)] Loss: 19093.464844\n",
      "Train Epoch: 172 [107520/225000 (48%)] Loss: 19025.109375\n",
      "Train Epoch: 172 [110016/225000 (49%)] Loss: 19137.925781\n",
      "Train Epoch: 172 [112512/225000 (50%)] Loss: 19071.007812\n",
      "Train Epoch: 172 [115008/225000 (51%)] Loss: 19242.652344\n",
      "Train Epoch: 172 [117504/225000 (52%)] Loss: 19039.617188\n",
      "Train Epoch: 172 [120000/225000 (53%)] Loss: 18965.218750\n",
      "Train Epoch: 172 [122496/225000 (54%)] Loss: 18962.605469\n",
      "Train Epoch: 172 [124992/225000 (56%)] Loss: 19646.097656\n",
      "Train Epoch: 172 [127488/225000 (57%)] Loss: 19202.998047\n",
      "Train Epoch: 172 [129984/225000 (58%)] Loss: 19201.060547\n",
      "Train Epoch: 172 [132480/225000 (59%)] Loss: 18979.621094\n",
      "Train Epoch: 172 [134976/225000 (60%)] Loss: 18963.996094\n",
      "Train Epoch: 172 [137472/225000 (61%)] Loss: 19805.476562\n",
      "Train Epoch: 172 [139968/225000 (62%)] Loss: 19322.878906\n",
      "Train Epoch: 172 [142464/225000 (63%)] Loss: 19051.996094\n",
      "Train Epoch: 172 [144960/225000 (64%)] Loss: 19106.378906\n",
      "Train Epoch: 172 [147456/225000 (66%)] Loss: 19231.121094\n",
      "Train Epoch: 172 [149952/225000 (67%)] Loss: 18538.718750\n",
      "Train Epoch: 172 [152448/225000 (68%)] Loss: 19307.480469\n",
      "Train Epoch: 172 [154944/225000 (69%)] Loss: 19466.683594\n",
      "Train Epoch: 172 [157440/225000 (70%)] Loss: 18819.464844\n",
      "Train Epoch: 172 [159936/225000 (71%)] Loss: 19556.929688\n",
      "Train Epoch: 172 [162432/225000 (72%)] Loss: 19103.804688\n",
      "Train Epoch: 172 [164928/225000 (73%)] Loss: 19306.136719\n",
      "Train Epoch: 172 [167424/225000 (74%)] Loss: 19282.402344\n",
      "Train Epoch: 172 [169920/225000 (76%)] Loss: 18844.441406\n",
      "Train Epoch: 172 [172416/225000 (77%)] Loss: 19163.875000\n",
      "Train Epoch: 172 [174912/225000 (78%)] Loss: 19005.640625\n",
      "Train Epoch: 172 [177408/225000 (79%)] Loss: 19256.109375\n",
      "Train Epoch: 172 [179904/225000 (80%)] Loss: 19334.453125\n",
      "Train Epoch: 172 [182400/225000 (81%)] Loss: 18699.140625\n",
      "Train Epoch: 172 [184896/225000 (82%)] Loss: 19279.156250\n",
      "Train Epoch: 172 [187392/225000 (83%)] Loss: 19043.468750\n",
      "Train Epoch: 172 [189888/225000 (84%)] Loss: 18995.796875\n",
      "Train Epoch: 172 [192384/225000 (86%)] Loss: 19193.964844\n",
      "Train Epoch: 172 [194880/225000 (87%)] Loss: 18685.880859\n",
      "Train Epoch: 172 [197376/225000 (88%)] Loss: 19324.113281\n",
      "Train Epoch: 172 [199872/225000 (89%)] Loss: 19180.353516\n",
      "Train Epoch: 172 [202368/225000 (90%)] Loss: 19260.917969\n",
      "Train Epoch: 172 [204864/225000 (91%)] Loss: 19229.320312\n",
      "Train Epoch: 172 [207360/225000 (92%)] Loss: 19463.527344\n",
      "Train Epoch: 172 [209856/225000 (93%)] Loss: 19207.820312\n",
      "Train Epoch: 172 [212352/225000 (94%)] Loss: 19377.890625\n",
      "Train Epoch: 172 [214848/225000 (95%)] Loss: 19022.093750\n",
      "Train Epoch: 172 [217344/225000 (97%)] Loss: 19177.343750\n",
      "Train Epoch: 172 [219840/225000 (98%)] Loss: 19282.308594\n",
      "Train Epoch: 172 [222336/225000 (99%)] Loss: 19546.865234\n",
      "Train Epoch: 172 [224832/225000 (100%)] Loss: 19281.941406\n",
      "    epoch          : 172\n",
      "    loss           : 19179.36413615881\n",
      "    val_loss       : 19085.56845159021\n",
      "Train Epoch: 173 [192/225000 (0%)] Loss: 19006.976562\n",
      "Train Epoch: 173 [2688/225000 (1%)] Loss: 19346.929688\n",
      "Train Epoch: 173 [5184/225000 (2%)] Loss: 18941.089844\n",
      "Train Epoch: 173 [7680/225000 (3%)] Loss: 19329.218750\n",
      "Train Epoch: 173 [10176/225000 (5%)] Loss: 18544.082031\n",
      "Train Epoch: 173 [12672/225000 (6%)] Loss: 18575.089844\n",
      "Train Epoch: 173 [15168/225000 (7%)] Loss: 18480.953125\n",
      "Train Epoch: 173 [17664/225000 (8%)] Loss: 19719.105469\n",
      "Train Epoch: 173 [20160/225000 (9%)] Loss: 19032.812500\n",
      "Train Epoch: 173 [22656/225000 (10%)] Loss: 19191.240234\n",
      "Train Epoch: 173 [25152/225000 (11%)] Loss: 18823.568359\n",
      "Train Epoch: 173 [27648/225000 (12%)] Loss: 19271.613281\n",
      "Train Epoch: 173 [30144/225000 (13%)] Loss: 19027.615234\n",
      "Train Epoch: 173 [32640/225000 (15%)] Loss: 19408.693359\n",
      "Train Epoch: 173 [35136/225000 (16%)] Loss: 19201.546875\n",
      "Train Epoch: 173 [37632/225000 (17%)] Loss: 19431.058594\n",
      "Train Epoch: 173 [40128/225000 (18%)] Loss: 19291.626953\n",
      "Train Epoch: 173 [42624/225000 (19%)] Loss: 19582.492188\n",
      "Train Epoch: 173 [45120/225000 (20%)] Loss: 19133.244141\n",
      "Train Epoch: 173 [47616/225000 (21%)] Loss: 19580.000000\n",
      "Train Epoch: 173 [50112/225000 (22%)] Loss: 19082.523438\n",
      "Train Epoch: 173 [52608/225000 (23%)] Loss: 19162.554688\n",
      "Train Epoch: 173 [55104/225000 (24%)] Loss: 19257.017578\n",
      "Train Epoch: 173 [57600/225000 (26%)] Loss: 18545.464844\n",
      "Train Epoch: 173 [60096/225000 (27%)] Loss: 18916.230469\n",
      "Train Epoch: 173 [62592/225000 (28%)] Loss: 19441.878906\n",
      "Train Epoch: 173 [65088/225000 (29%)] Loss: 18849.347656\n",
      "Train Epoch: 173 [67584/225000 (30%)] Loss: 18865.513672\n",
      "Train Epoch: 173 [70080/225000 (31%)] Loss: 19259.816406\n",
      "Train Epoch: 173 [72576/225000 (32%)] Loss: 18978.242188\n",
      "Train Epoch: 173 [75072/225000 (33%)] Loss: 18979.781250\n",
      "Train Epoch: 173 [77568/225000 (34%)] Loss: 19110.386719\n",
      "Train Epoch: 173 [80064/225000 (36%)] Loss: 19100.644531\n",
      "Train Epoch: 173 [82560/225000 (37%)] Loss: 19018.140625\n",
      "Train Epoch: 173 [85056/225000 (38%)] Loss: 19475.958984\n",
      "Train Epoch: 173 [87552/225000 (39%)] Loss: 19517.140625\n",
      "Train Epoch: 173 [90048/225000 (40%)] Loss: 19413.503906\n",
      "Train Epoch: 173 [92544/225000 (41%)] Loss: 19340.896484\n",
      "Train Epoch: 173 [95040/225000 (42%)] Loss: 19465.125000\n",
      "Train Epoch: 173 [97536/225000 (43%)] Loss: 19357.457031\n",
      "Train Epoch: 173 [100032/225000 (44%)] Loss: 19285.332031\n",
      "Train Epoch: 173 [102528/225000 (46%)] Loss: 19513.308594\n",
      "Train Epoch: 173 [105024/225000 (47%)] Loss: 19109.714844\n",
      "Train Epoch: 173 [107520/225000 (48%)] Loss: 19119.755859\n",
      "Train Epoch: 173 [110016/225000 (49%)] Loss: 19642.808594\n",
      "Train Epoch: 173 [112512/225000 (50%)] Loss: 19200.865234\n",
      "Train Epoch: 173 [115008/225000 (51%)] Loss: 19347.699219\n",
      "Train Epoch: 173 [117504/225000 (52%)] Loss: 18845.457031\n",
      "Train Epoch: 173 [120000/225000 (53%)] Loss: 18836.414062\n",
      "Train Epoch: 173 [122496/225000 (54%)] Loss: 19046.585938\n",
      "Train Epoch: 173 [124992/225000 (56%)] Loss: 19079.619141\n",
      "Train Epoch: 173 [127488/225000 (57%)] Loss: 19079.929688\n",
      "Train Epoch: 173 [129984/225000 (58%)] Loss: 19169.316406\n",
      "Train Epoch: 173 [132480/225000 (59%)] Loss: 19391.582031\n",
      "Train Epoch: 173 [134976/225000 (60%)] Loss: 19360.697266\n",
      "Train Epoch: 173 [137472/225000 (61%)] Loss: 19390.585938\n",
      "Train Epoch: 173 [139968/225000 (62%)] Loss: 18988.617188\n",
      "Train Epoch: 173 [142464/225000 (63%)] Loss: 18999.568359\n",
      "Train Epoch: 173 [144960/225000 (64%)] Loss: 19013.693359\n",
      "Train Epoch: 173 [147456/225000 (66%)] Loss: 18737.820312\n",
      "Train Epoch: 173 [149952/225000 (67%)] Loss: 19233.667969\n",
      "Train Epoch: 173 [152448/225000 (68%)] Loss: 18911.482422\n",
      "Train Epoch: 173 [154944/225000 (69%)] Loss: 19857.757812\n",
      "Train Epoch: 173 [157440/225000 (70%)] Loss: 18474.695312\n",
      "Train Epoch: 173 [159936/225000 (71%)] Loss: 19093.759766\n",
      "Train Epoch: 173 [162432/225000 (72%)] Loss: 19077.042969\n",
      "Train Epoch: 173 [164928/225000 (73%)] Loss: 18836.976562\n",
      "Train Epoch: 173 [167424/225000 (74%)] Loss: 18949.781250\n",
      "Train Epoch: 173 [169920/225000 (76%)] Loss: 19366.410156\n",
      "Train Epoch: 173 [172416/225000 (77%)] Loss: 19610.679688\n",
      "Train Epoch: 173 [174912/225000 (78%)] Loss: 19171.990234\n",
      "Train Epoch: 173 [177408/225000 (79%)] Loss: 19394.253906\n",
      "Train Epoch: 173 [179904/225000 (80%)] Loss: 19285.941406\n",
      "Train Epoch: 173 [182400/225000 (81%)] Loss: 18808.773438\n",
      "Train Epoch: 173 [184896/225000 (82%)] Loss: 19342.859375\n",
      "Train Epoch: 173 [187392/225000 (83%)] Loss: 19480.339844\n",
      "Train Epoch: 173 [189888/225000 (84%)] Loss: 19148.648438\n",
      "Train Epoch: 173 [192384/225000 (86%)] Loss: 18948.796875\n",
      "Train Epoch: 173 [194880/225000 (87%)] Loss: 19570.222656\n",
      "Train Epoch: 173 [197376/225000 (88%)] Loss: 18980.019531\n",
      "Train Epoch: 173 [199872/225000 (89%)] Loss: 18867.765625\n",
      "Train Epoch: 173 [202368/225000 (90%)] Loss: 18865.949219\n",
      "Train Epoch: 173 [204864/225000 (91%)] Loss: 19671.402344\n",
      "Train Epoch: 173 [207360/225000 (92%)] Loss: 18992.462891\n",
      "Train Epoch: 173 [209856/225000 (93%)] Loss: 19546.226562\n",
      "Train Epoch: 173 [212352/225000 (94%)] Loss: 19117.425781\n",
      "Train Epoch: 173 [214848/225000 (95%)] Loss: 19394.984375\n",
      "Train Epoch: 173 [217344/225000 (97%)] Loss: 19357.699219\n",
      "Train Epoch: 173 [219840/225000 (98%)] Loss: 19105.017578\n",
      "Train Epoch: 173 [222336/225000 (99%)] Loss: 19023.402344\n",
      "Train Epoch: 173 [224832/225000 (100%)] Loss: 19008.000000\n",
      "    epoch          : 173\n",
      "    loss           : 19171.215122053647\n",
      "    val_loss       : 19082.18542509179\n",
      "Train Epoch: 174 [192/225000 (0%)] Loss: 19165.458984\n",
      "Train Epoch: 174 [2688/225000 (1%)] Loss: 19299.074219\n",
      "Train Epoch: 174 [5184/225000 (2%)] Loss: 19404.751953\n",
      "Train Epoch: 174 [7680/225000 (3%)] Loss: 18860.146484\n",
      "Train Epoch: 174 [10176/225000 (5%)] Loss: 19027.687500\n",
      "Train Epoch: 174 [12672/225000 (6%)] Loss: 19292.283203\n",
      "Train Epoch: 174 [15168/225000 (7%)] Loss: 19814.564453\n",
      "Train Epoch: 174 [17664/225000 (8%)] Loss: 19131.925781\n",
      "Train Epoch: 174 [20160/225000 (9%)] Loss: 19241.175781\n",
      "Train Epoch: 174 [22656/225000 (10%)] Loss: 19273.472656\n",
      "Train Epoch: 174 [25152/225000 (11%)] Loss: 19255.912109\n",
      "Train Epoch: 174 [27648/225000 (12%)] Loss: 19089.748047\n",
      "Train Epoch: 174 [30144/225000 (13%)] Loss: 19035.042969\n",
      "Train Epoch: 174 [32640/225000 (15%)] Loss: 19647.261719\n",
      "Train Epoch: 174 [35136/225000 (16%)] Loss: 22600.576172\n",
      "Train Epoch: 174 [37632/225000 (17%)] Loss: 19478.144531\n",
      "Train Epoch: 174 [40128/225000 (18%)] Loss: 19049.812500\n",
      "Train Epoch: 174 [42624/225000 (19%)] Loss: 19100.738281\n",
      "Train Epoch: 174 [45120/225000 (20%)] Loss: 19178.294922\n",
      "Train Epoch: 174 [47616/225000 (21%)] Loss: 19128.300781\n",
      "Train Epoch: 174 [50112/225000 (22%)] Loss: 18926.828125\n",
      "Train Epoch: 174 [52608/225000 (23%)] Loss: 19240.240234\n",
      "Train Epoch: 174 [55104/225000 (24%)] Loss: 19194.656250\n",
      "Train Epoch: 174 [57600/225000 (26%)] Loss: 19251.128906\n",
      "Train Epoch: 174 [60096/225000 (27%)] Loss: 19274.601562\n",
      "Train Epoch: 174 [62592/225000 (28%)] Loss: 19004.460938\n",
      "Train Epoch: 174 [65088/225000 (29%)] Loss: 19075.414062\n",
      "Train Epoch: 174 [67584/225000 (30%)] Loss: 19500.945312\n",
      "Train Epoch: 174 [70080/225000 (31%)] Loss: 19520.595703\n",
      "Train Epoch: 174 [72576/225000 (32%)] Loss: 18824.250000\n",
      "Train Epoch: 174 [75072/225000 (33%)] Loss: 19100.078125\n",
      "Train Epoch: 174 [77568/225000 (34%)] Loss: 18990.160156\n",
      "Train Epoch: 174 [80064/225000 (36%)] Loss: 19062.654297\n",
      "Train Epoch: 174 [82560/225000 (37%)] Loss: 19504.972656\n",
      "Train Epoch: 174 [85056/225000 (38%)] Loss: 19439.431641\n",
      "Train Epoch: 174 [87552/225000 (39%)] Loss: 19261.746094\n",
      "Train Epoch: 174 [90048/225000 (40%)] Loss: 19199.839844\n",
      "Train Epoch: 174 [92544/225000 (41%)] Loss: 19194.617188\n",
      "Train Epoch: 174 [95040/225000 (42%)] Loss: 19351.640625\n",
      "Train Epoch: 174 [97536/225000 (43%)] Loss: 19354.351562\n",
      "Train Epoch: 174 [100032/225000 (44%)] Loss: 19361.591797\n",
      "Train Epoch: 174 [102528/225000 (46%)] Loss: 19170.976562\n",
      "Train Epoch: 174 [105024/225000 (47%)] Loss: 19301.898438\n",
      "Train Epoch: 174 [107520/225000 (48%)] Loss: 19249.109375\n",
      "Train Epoch: 174 [110016/225000 (49%)] Loss: 18978.449219\n",
      "Train Epoch: 174 [112512/225000 (50%)] Loss: 18415.121094\n",
      "Train Epoch: 174 [115008/225000 (51%)] Loss: 19181.603516\n",
      "Train Epoch: 174 [117504/225000 (52%)] Loss: 19565.283203\n",
      "Train Epoch: 174 [120000/225000 (53%)] Loss: 19594.453125\n",
      "Train Epoch: 174 [122496/225000 (54%)] Loss: 19025.824219\n",
      "Train Epoch: 174 [124992/225000 (56%)] Loss: 19401.697266\n",
      "Train Epoch: 174 [127488/225000 (57%)] Loss: 18809.361328\n",
      "Train Epoch: 174 [129984/225000 (58%)] Loss: 19050.751953\n",
      "Train Epoch: 174 [132480/225000 (59%)] Loss: 19242.746094\n",
      "Train Epoch: 174 [134976/225000 (60%)] Loss: 18777.853516\n",
      "Train Epoch: 174 [137472/225000 (61%)] Loss: 19221.416016\n",
      "Train Epoch: 174 [139968/225000 (62%)] Loss: 18814.636719\n",
      "Train Epoch: 174 [142464/225000 (63%)] Loss: 19087.925781\n",
      "Train Epoch: 174 [144960/225000 (64%)] Loss: 19503.109375\n",
      "Train Epoch: 174 [147456/225000 (66%)] Loss: 19368.091797\n",
      "Train Epoch: 174 [149952/225000 (67%)] Loss: 18932.671875\n",
      "Train Epoch: 174 [152448/225000 (68%)] Loss: 18801.636719\n",
      "Train Epoch: 174 [154944/225000 (69%)] Loss: 18772.148438\n",
      "Train Epoch: 174 [157440/225000 (70%)] Loss: 19159.755859\n",
      "Train Epoch: 174 [159936/225000 (71%)] Loss: 19178.718750\n",
      "Train Epoch: 174 [162432/225000 (72%)] Loss: 19390.486328\n",
      "Train Epoch: 174 [164928/225000 (73%)] Loss: 18873.707031\n",
      "Train Epoch: 174 [167424/225000 (74%)] Loss: 19006.738281\n",
      "Train Epoch: 174 [169920/225000 (76%)] Loss: 18973.806641\n",
      "Train Epoch: 174 [172416/225000 (77%)] Loss: 19542.365234\n",
      "Train Epoch: 174 [174912/225000 (78%)] Loss: 19105.787109\n",
      "Train Epoch: 174 [177408/225000 (79%)] Loss: 19454.761719\n",
      "Train Epoch: 174 [179904/225000 (80%)] Loss: 19381.345703\n",
      "Train Epoch: 174 [182400/225000 (81%)] Loss: 18885.304688\n",
      "Train Epoch: 174 [184896/225000 (82%)] Loss: 18370.992188\n",
      "Train Epoch: 174 [187392/225000 (83%)] Loss: 19611.078125\n",
      "Train Epoch: 174 [189888/225000 (84%)] Loss: 19398.210938\n",
      "Train Epoch: 174 [192384/225000 (86%)] Loss: 18970.613281\n",
      "Train Epoch: 174 [194880/225000 (87%)] Loss: 19125.699219\n",
      "Train Epoch: 174 [197376/225000 (88%)] Loss: 18760.000000\n",
      "Train Epoch: 174 [199872/225000 (89%)] Loss: 19346.867188\n",
      "Train Epoch: 174 [202368/225000 (90%)] Loss: 19195.960938\n",
      "Train Epoch: 174 [204864/225000 (91%)] Loss: 18962.091797\n",
      "Train Epoch: 174 [207360/225000 (92%)] Loss: 18809.332031\n",
      "Train Epoch: 174 [209856/225000 (93%)] Loss: 19084.210938\n",
      "Train Epoch: 174 [212352/225000 (94%)] Loss: 19168.710938\n",
      "Train Epoch: 174 [214848/225000 (95%)] Loss: 19306.152344\n",
      "Train Epoch: 174 [217344/225000 (97%)] Loss: 18967.019531\n",
      "Train Epoch: 174 [219840/225000 (98%)] Loss: 18541.726562\n",
      "Train Epoch: 174 [222336/225000 (99%)] Loss: 19368.982422\n",
      "Train Epoch: 174 [224832/225000 (100%)] Loss: 19370.755859\n",
      "    epoch          : 174\n",
      "    loss           : 19165.112101375853\n",
      "    val_loss       : 19101.74654637675\n",
      "Train Epoch: 175 [192/225000 (0%)] Loss: 19107.724609\n",
      "Train Epoch: 175 [2688/225000 (1%)] Loss: 19387.078125\n",
      "Train Epoch: 175 [5184/225000 (2%)] Loss: 19595.611328\n",
      "Train Epoch: 175 [7680/225000 (3%)] Loss: 19277.296875\n",
      "Train Epoch: 175 [10176/225000 (5%)] Loss: 18916.589844\n",
      "Train Epoch: 175 [12672/225000 (6%)] Loss: 19460.505859\n",
      "Train Epoch: 175 [15168/225000 (7%)] Loss: 19009.316406\n",
      "Train Epoch: 175 [17664/225000 (8%)] Loss: 19450.640625\n",
      "Train Epoch: 175 [20160/225000 (9%)] Loss: 19392.863281\n",
      "Train Epoch: 175 [22656/225000 (10%)] Loss: 19402.187500\n",
      "Train Epoch: 175 [25152/225000 (11%)] Loss: 19059.609375\n",
      "Train Epoch: 175 [27648/225000 (12%)] Loss: 19237.308594\n",
      "Train Epoch: 175 [30144/225000 (13%)] Loss: 18942.570312\n",
      "Train Epoch: 175 [32640/225000 (15%)] Loss: 19092.740234\n",
      "Train Epoch: 175 [35136/225000 (16%)] Loss: 19150.941406\n",
      "Train Epoch: 175 [37632/225000 (17%)] Loss: 19101.583984\n",
      "Train Epoch: 175 [40128/225000 (18%)] Loss: 19236.472656\n",
      "Train Epoch: 175 [42624/225000 (19%)] Loss: 19041.585938\n",
      "Train Epoch: 175 [45120/225000 (20%)] Loss: 19242.000000\n",
      "Train Epoch: 175 [47616/225000 (21%)] Loss: 19326.054688\n",
      "Train Epoch: 175 [50112/225000 (22%)] Loss: 24089.152344\n",
      "Train Epoch: 175 [52608/225000 (23%)] Loss: 19005.648438\n",
      "Train Epoch: 175 [55104/225000 (24%)] Loss: 18600.064453\n",
      "Train Epoch: 175 [57600/225000 (26%)] Loss: 19448.042969\n",
      "Train Epoch: 175 [60096/225000 (27%)] Loss: 19381.078125\n",
      "Train Epoch: 175 [62592/225000 (28%)] Loss: 19046.425781\n",
      "Train Epoch: 175 [65088/225000 (29%)] Loss: 19100.527344\n",
      "Train Epoch: 175 [67584/225000 (30%)] Loss: 19009.871094\n",
      "Train Epoch: 175 [70080/225000 (31%)] Loss: 19202.007812\n",
      "Train Epoch: 175 [72576/225000 (32%)] Loss: 18622.285156\n",
      "Train Epoch: 175 [75072/225000 (33%)] Loss: 19408.701172\n",
      "Train Epoch: 175 [77568/225000 (34%)] Loss: 19423.796875\n",
      "Train Epoch: 175 [80064/225000 (36%)] Loss: 19122.164062\n",
      "Train Epoch: 175 [82560/225000 (37%)] Loss: 19426.046875\n",
      "Train Epoch: 175 [85056/225000 (38%)] Loss: 18842.041016\n",
      "Train Epoch: 175 [87552/225000 (39%)] Loss: 19042.304688\n",
      "Train Epoch: 175 [90048/225000 (40%)] Loss: 19873.984375\n",
      "Train Epoch: 175 [92544/225000 (41%)] Loss: 19160.527344\n",
      "Train Epoch: 175 [95040/225000 (42%)] Loss: 19161.890625\n",
      "Train Epoch: 175 [97536/225000 (43%)] Loss: 19089.554688\n",
      "Train Epoch: 175 [100032/225000 (44%)] Loss: 19298.257812\n",
      "Train Epoch: 175 [102528/225000 (46%)] Loss: 19125.480469\n",
      "Train Epoch: 175 [105024/225000 (47%)] Loss: 19337.195312\n",
      "Train Epoch: 175 [107520/225000 (48%)] Loss: 19146.167969\n",
      "Train Epoch: 175 [110016/225000 (49%)] Loss: 19434.664062\n",
      "Train Epoch: 175 [112512/225000 (50%)] Loss: 19023.214844\n",
      "Train Epoch: 175 [115008/225000 (51%)] Loss: 18984.257812\n",
      "Train Epoch: 175 [117504/225000 (52%)] Loss: 18936.294922\n",
      "Train Epoch: 175 [120000/225000 (53%)] Loss: 19018.843750\n",
      "Train Epoch: 175 [122496/225000 (54%)] Loss: 19368.193359\n",
      "Train Epoch: 175 [124992/225000 (56%)] Loss: 19105.660156\n",
      "Train Epoch: 175 [127488/225000 (57%)] Loss: 18841.070312\n",
      "Train Epoch: 175 [129984/225000 (58%)] Loss: 19265.486328\n",
      "Train Epoch: 175 [132480/225000 (59%)] Loss: 19218.703125\n",
      "Train Epoch: 175 [134976/225000 (60%)] Loss: 19252.191406\n",
      "Train Epoch: 175 [137472/225000 (61%)] Loss: 18936.048828\n",
      "Train Epoch: 175 [139968/225000 (62%)] Loss: 19513.517578\n",
      "Train Epoch: 175 [142464/225000 (63%)] Loss: 18943.675781\n",
      "Train Epoch: 175 [144960/225000 (64%)] Loss: 18782.306641\n",
      "Train Epoch: 175 [147456/225000 (66%)] Loss: 19423.191406\n",
      "Train Epoch: 175 [149952/225000 (67%)] Loss: 19683.601562\n",
      "Train Epoch: 175 [152448/225000 (68%)] Loss: 19384.947266\n",
      "Train Epoch: 175 [154944/225000 (69%)] Loss: 19231.691406\n",
      "Train Epoch: 175 [157440/225000 (70%)] Loss: 18858.779297\n",
      "Train Epoch: 175 [159936/225000 (71%)] Loss: 19346.189453\n",
      "Train Epoch: 175 [162432/225000 (72%)] Loss: 19168.972656\n",
      "Train Epoch: 175 [164928/225000 (73%)] Loss: 19364.333984\n",
      "Train Epoch: 175 [167424/225000 (74%)] Loss: 19073.722656\n",
      "Train Epoch: 175 [169920/225000 (76%)] Loss: 18908.554688\n",
      "Train Epoch: 175 [172416/225000 (77%)] Loss: 19146.193359\n",
      "Train Epoch: 175 [174912/225000 (78%)] Loss: 19495.033203\n",
      "Train Epoch: 175 [177408/225000 (79%)] Loss: 18971.597656\n",
      "Train Epoch: 175 [179904/225000 (80%)] Loss: 19072.490234\n",
      "Train Epoch: 175 [182400/225000 (81%)] Loss: 19325.324219\n",
      "Train Epoch: 175 [184896/225000 (82%)] Loss: 19162.089844\n",
      "Train Epoch: 175 [187392/225000 (83%)] Loss: 19195.791016\n",
      "Train Epoch: 175 [189888/225000 (84%)] Loss: 19082.742188\n",
      "Train Epoch: 175 [192384/225000 (86%)] Loss: 18628.742188\n",
      "Train Epoch: 175 [194880/225000 (87%)] Loss: 19275.617188\n",
      "Train Epoch: 175 [197376/225000 (88%)] Loss: 19124.419922\n",
      "Train Epoch: 175 [199872/225000 (89%)] Loss: 19033.281250\n",
      "Train Epoch: 175 [202368/225000 (90%)] Loss: 18818.171875\n",
      "Train Epoch: 175 [204864/225000 (91%)] Loss: 19017.212891\n",
      "Train Epoch: 175 [207360/225000 (92%)] Loss: 18812.132812\n",
      "Train Epoch: 175 [209856/225000 (93%)] Loss: 18908.621094\n",
      "Train Epoch: 175 [212352/225000 (94%)] Loss: 18990.376953\n",
      "Train Epoch: 175 [214848/225000 (95%)] Loss: 19078.167969\n",
      "Train Epoch: 175 [217344/225000 (97%)] Loss: 19166.722656\n",
      "Train Epoch: 175 [219840/225000 (98%)] Loss: 19305.435547\n",
      "Train Epoch: 175 [222336/225000 (99%)] Loss: 19231.875000\n",
      "Train Epoch: 175 [224832/225000 (100%)] Loss: 19668.828125\n",
      "    epoch          : 175\n",
      "    loss           : 19164.17503632946\n",
      "    val_loss       : 19107.009150774426\n",
      "Train Epoch: 176 [192/225000 (0%)] Loss: 19526.035156\n",
      "Train Epoch: 176 [2688/225000 (1%)] Loss: 19363.791016\n",
      "Train Epoch: 176 [5184/225000 (2%)] Loss: 19055.808594\n",
      "Train Epoch: 176 [7680/225000 (3%)] Loss: 18701.625000\n",
      "Train Epoch: 176 [10176/225000 (5%)] Loss: 19616.316406\n",
      "Train Epoch: 176 [12672/225000 (6%)] Loss: 18940.376953\n",
      "Train Epoch: 176 [15168/225000 (7%)] Loss: 19032.386719\n",
      "Train Epoch: 176 [17664/225000 (8%)] Loss: 19667.332031\n",
      "Train Epoch: 176 [20160/225000 (9%)] Loss: 18984.134766\n",
      "Train Epoch: 176 [22656/225000 (10%)] Loss: 19463.035156\n",
      "Train Epoch: 176 [25152/225000 (11%)] Loss: 19769.169922\n",
      "Train Epoch: 176 [27648/225000 (12%)] Loss: 18712.562500\n",
      "Train Epoch: 176 [30144/225000 (13%)] Loss: 19142.070312\n",
      "Train Epoch: 176 [32640/225000 (15%)] Loss: 19187.085938\n",
      "Train Epoch: 176 [35136/225000 (16%)] Loss: 19428.769531\n",
      "Train Epoch: 176 [37632/225000 (17%)] Loss: 19072.421875\n",
      "Train Epoch: 176 [40128/225000 (18%)] Loss: 18934.542969\n",
      "Train Epoch: 176 [42624/225000 (19%)] Loss: 19061.421875\n",
      "Train Epoch: 176 [45120/225000 (20%)] Loss: 19360.871094\n",
      "Train Epoch: 176 [47616/225000 (21%)] Loss: 18948.056641\n",
      "Train Epoch: 176 [50112/225000 (22%)] Loss: 18951.964844\n",
      "Train Epoch: 176 [52608/225000 (23%)] Loss: 18676.300781\n",
      "Train Epoch: 176 [55104/225000 (24%)] Loss: 19440.207031\n",
      "Train Epoch: 176 [57600/225000 (26%)] Loss: 18692.220703\n",
      "Train Epoch: 176 [60096/225000 (27%)] Loss: 18868.507812\n",
      "Train Epoch: 176 [62592/225000 (28%)] Loss: 19119.789062\n",
      "Train Epoch: 176 [65088/225000 (29%)] Loss: 19367.582031\n",
      "Train Epoch: 176 [67584/225000 (30%)] Loss: 19291.662109\n",
      "Train Epoch: 176 [70080/225000 (31%)] Loss: 19095.218750\n",
      "Train Epoch: 176 [72576/225000 (32%)] Loss: 19333.234375\n",
      "Train Epoch: 176 [75072/225000 (33%)] Loss: 18957.121094\n",
      "Train Epoch: 176 [77568/225000 (34%)] Loss: 18699.066406\n",
      "Train Epoch: 176 [80064/225000 (36%)] Loss: 19169.468750\n",
      "Train Epoch: 176 [82560/225000 (37%)] Loss: 19162.828125\n",
      "Train Epoch: 176 [85056/225000 (38%)] Loss: 19593.927734\n",
      "Train Epoch: 176 [87552/225000 (39%)] Loss: 18872.548828\n",
      "Train Epoch: 176 [90048/225000 (40%)] Loss: 19404.455078\n",
      "Train Epoch: 176 [92544/225000 (41%)] Loss: 18474.953125\n",
      "Train Epoch: 176 [95040/225000 (42%)] Loss: 18910.857422\n",
      "Train Epoch: 176 [97536/225000 (43%)] Loss: 19054.601562\n",
      "Train Epoch: 176 [100032/225000 (44%)] Loss: 18948.894531\n",
      "Train Epoch: 176 [102528/225000 (46%)] Loss: 19837.601562\n",
      "Train Epoch: 176 [105024/225000 (47%)] Loss: 18904.218750\n",
      "Train Epoch: 176 [107520/225000 (48%)] Loss: 18953.548828\n",
      "Train Epoch: 176 [110016/225000 (49%)] Loss: 19387.312500\n",
      "Train Epoch: 176 [112512/225000 (50%)] Loss: 19374.800781\n",
      "Train Epoch: 176 [115008/225000 (51%)] Loss: 19470.085938\n",
      "Train Epoch: 176 [117504/225000 (52%)] Loss: 19101.851562\n",
      "Train Epoch: 176 [120000/225000 (53%)] Loss: 19222.742188\n",
      "Train Epoch: 176 [122496/225000 (54%)] Loss: 18678.296875\n",
      "Train Epoch: 176 [124992/225000 (56%)] Loss: 19043.587891\n",
      "Train Epoch: 176 [127488/225000 (57%)] Loss: 19462.363281\n",
      "Train Epoch: 176 [129984/225000 (58%)] Loss: 19111.587891\n",
      "Train Epoch: 176 [132480/225000 (59%)] Loss: 19329.064453\n",
      "Train Epoch: 176 [134976/225000 (60%)] Loss: 18985.429688\n",
      "Train Epoch: 176 [137472/225000 (61%)] Loss: 18980.992188\n",
      "Train Epoch: 176 [139968/225000 (62%)] Loss: 19637.597656\n",
      "Train Epoch: 176 [142464/225000 (63%)] Loss: 19550.808594\n",
      "Train Epoch: 176 [144960/225000 (64%)] Loss: 19125.337891\n",
      "Train Epoch: 176 [147456/225000 (66%)] Loss: 18891.769531\n",
      "Train Epoch: 176 [149952/225000 (67%)] Loss: 19069.839844\n",
      "Train Epoch: 176 [152448/225000 (68%)] Loss: 19023.242188\n",
      "Train Epoch: 176 [154944/225000 (69%)] Loss: 19250.605469\n",
      "Train Epoch: 176 [157440/225000 (70%)] Loss: 19672.054688\n",
      "Train Epoch: 176 [159936/225000 (71%)] Loss: 18976.910156\n",
      "Train Epoch: 176 [162432/225000 (72%)] Loss: 19275.087891\n",
      "Train Epoch: 176 [164928/225000 (73%)] Loss: 19243.753906\n",
      "Train Epoch: 176 [167424/225000 (74%)] Loss: 19560.199219\n",
      "Train Epoch: 176 [169920/225000 (76%)] Loss: 19111.542969\n",
      "Train Epoch: 176 [172416/225000 (77%)] Loss: 19204.472656\n",
      "Train Epoch: 176 [174912/225000 (78%)] Loss: 19265.812500\n",
      "Train Epoch: 176 [177408/225000 (79%)] Loss: 19190.816406\n",
      "Train Epoch: 176 [179904/225000 (80%)] Loss: 19019.828125\n",
      "Train Epoch: 176 [182400/225000 (81%)] Loss: 19249.667969\n",
      "Train Epoch: 176 [184896/225000 (82%)] Loss: 19395.398438\n",
      "Train Epoch: 176 [187392/225000 (83%)] Loss: 18665.076172\n",
      "Train Epoch: 176 [189888/225000 (84%)] Loss: 18889.285156\n",
      "Train Epoch: 176 [192384/225000 (86%)] Loss: 18477.921875\n",
      "Train Epoch: 176 [194880/225000 (87%)] Loss: 19185.892578\n",
      "Train Epoch: 176 [197376/225000 (88%)] Loss: 19413.261719\n",
      "Train Epoch: 176 [199872/225000 (89%)] Loss: 18760.404297\n",
      "Train Epoch: 176 [202368/225000 (90%)] Loss: 19326.371094\n",
      "Train Epoch: 176 [204864/225000 (91%)] Loss: 19045.847656\n",
      "Train Epoch: 176 [207360/225000 (92%)] Loss: 19359.906250\n",
      "Train Epoch: 176 [209856/225000 (93%)] Loss: 19214.710938\n",
      "Train Epoch: 176 [212352/225000 (94%)] Loss: 19319.417969\n",
      "Train Epoch: 176 [214848/225000 (95%)] Loss: 19063.000000\n",
      "Train Epoch: 176 [217344/225000 (97%)] Loss: 19204.654297\n",
      "Train Epoch: 176 [219840/225000 (98%)] Loss: 18923.138672\n",
      "Train Epoch: 176 [222336/225000 (99%)] Loss: 18997.574219\n",
      "Train Epoch: 176 [224832/225000 (100%)] Loss: 18891.027344\n",
      "    epoch          : 176\n",
      "    loss           : 19151.93609681634\n",
      "    val_loss       : 19066.997080204143\n",
      "Train Epoch: 177 [192/225000 (0%)] Loss: 18932.000000\n",
      "Train Epoch: 177 [2688/225000 (1%)] Loss: 19141.033203\n",
      "Train Epoch: 177 [5184/225000 (2%)] Loss: 18487.214844\n",
      "Train Epoch: 177 [7680/225000 (3%)] Loss: 19176.017578\n",
      "Train Epoch: 177 [10176/225000 (5%)] Loss: 18978.195312\n",
      "Train Epoch: 177 [12672/225000 (6%)] Loss: 19745.986328\n",
      "Train Epoch: 177 [15168/225000 (7%)] Loss: 18833.691406\n",
      "Train Epoch: 177 [17664/225000 (8%)] Loss: 19032.011719\n",
      "Train Epoch: 177 [20160/225000 (9%)] Loss: 18904.527344\n",
      "Train Epoch: 177 [22656/225000 (10%)] Loss: 19534.986328\n",
      "Train Epoch: 177 [25152/225000 (11%)] Loss: 19265.220703\n",
      "Train Epoch: 177 [27648/225000 (12%)] Loss: 19221.453125\n",
      "Train Epoch: 177 [30144/225000 (13%)] Loss: 19417.867188\n",
      "Train Epoch: 177 [32640/225000 (15%)] Loss: 19094.941406\n",
      "Train Epoch: 177 [35136/225000 (16%)] Loss: 19079.117188\n",
      "Train Epoch: 177 [37632/225000 (17%)] Loss: 18941.216797\n",
      "Train Epoch: 177 [40128/225000 (18%)] Loss: 19219.251953\n",
      "Train Epoch: 177 [42624/225000 (19%)] Loss: 19412.632812\n",
      "Train Epoch: 177 [45120/225000 (20%)] Loss: 19060.927734\n",
      "Train Epoch: 177 [47616/225000 (21%)] Loss: 18811.433594\n",
      "Train Epoch: 177 [50112/225000 (22%)] Loss: 19322.179688\n",
      "Train Epoch: 177 [52608/225000 (23%)] Loss: 18861.035156\n",
      "Train Epoch: 177 [55104/225000 (24%)] Loss: 19092.773438\n",
      "Train Epoch: 177 [57600/225000 (26%)] Loss: 18963.253906\n",
      "Train Epoch: 177 [60096/225000 (27%)] Loss: 19086.875000\n",
      "Train Epoch: 177 [62592/225000 (28%)] Loss: 19497.316406\n",
      "Train Epoch: 177 [65088/225000 (29%)] Loss: 18839.841797\n",
      "Train Epoch: 177 [67584/225000 (30%)] Loss: 19105.609375\n",
      "Train Epoch: 177 [70080/225000 (31%)] Loss: 19275.097656\n",
      "Train Epoch: 177 [72576/225000 (32%)] Loss: 19209.519531\n",
      "Train Epoch: 177 [75072/225000 (33%)] Loss: 19324.574219\n",
      "Train Epoch: 177 [77568/225000 (34%)] Loss: 18888.240234\n",
      "Train Epoch: 177 [80064/225000 (36%)] Loss: 18861.011719\n",
      "Train Epoch: 177 [82560/225000 (37%)] Loss: 19576.382812\n",
      "Train Epoch: 177 [85056/225000 (38%)] Loss: 19370.761719\n",
      "Train Epoch: 177 [87552/225000 (39%)] Loss: 19606.703125\n",
      "Train Epoch: 177 [90048/225000 (40%)] Loss: 19697.566406\n",
      "Train Epoch: 177 [92544/225000 (41%)] Loss: 19149.285156\n",
      "Train Epoch: 177 [95040/225000 (42%)] Loss: 19033.625000\n",
      "Train Epoch: 177 [97536/225000 (43%)] Loss: 19148.902344\n",
      "Train Epoch: 177 [100032/225000 (44%)] Loss: 19051.886719\n",
      "Train Epoch: 177 [102528/225000 (46%)] Loss: 18988.746094\n",
      "Train Epoch: 177 [105024/225000 (47%)] Loss: 19075.273438\n",
      "Train Epoch: 177 [107520/225000 (48%)] Loss: 18687.257812\n",
      "Train Epoch: 177 [110016/225000 (49%)] Loss: 19666.009766\n",
      "Train Epoch: 177 [112512/225000 (50%)] Loss: 19612.791016\n",
      "Train Epoch: 177 [115008/225000 (51%)] Loss: 19088.175781\n",
      "Train Epoch: 177 [117504/225000 (52%)] Loss: 19111.199219\n",
      "Train Epoch: 177 [120000/225000 (53%)] Loss: 19141.628906\n",
      "Train Epoch: 177 [122496/225000 (54%)] Loss: 19702.554688\n",
      "Train Epoch: 177 [124992/225000 (56%)] Loss: 19597.433594\n",
      "Train Epoch: 177 [127488/225000 (57%)] Loss: 19017.992188\n",
      "Train Epoch: 177 [129984/225000 (58%)] Loss: 18671.953125\n",
      "Train Epoch: 177 [132480/225000 (59%)] Loss: 19076.947266\n",
      "Train Epoch: 177 [134976/225000 (60%)] Loss: 19087.820312\n",
      "Train Epoch: 177 [137472/225000 (61%)] Loss: 19026.445312\n",
      "Train Epoch: 177 [139968/225000 (62%)] Loss: 19233.332031\n",
      "Train Epoch: 177 [142464/225000 (63%)] Loss: 18932.814453\n",
      "Train Epoch: 177 [144960/225000 (64%)] Loss: 19135.351562\n",
      "Train Epoch: 177 [147456/225000 (66%)] Loss: 18813.837891\n",
      "Train Epoch: 177 [149952/225000 (67%)] Loss: 19376.906250\n",
      "Train Epoch: 177 [152448/225000 (68%)] Loss: 19305.238281\n",
      "Train Epoch: 177 [154944/225000 (69%)] Loss: 19062.816406\n",
      "Train Epoch: 177 [157440/225000 (70%)] Loss: 18876.437500\n",
      "Train Epoch: 177 [159936/225000 (71%)] Loss: 19428.695312\n",
      "Train Epoch: 177 [162432/225000 (72%)] Loss: 19396.421875\n",
      "Train Epoch: 177 [164928/225000 (73%)] Loss: 19304.925781\n",
      "Train Epoch: 177 [167424/225000 (74%)] Loss: 18893.537109\n",
      "Train Epoch: 177 [169920/225000 (76%)] Loss: 18892.902344\n",
      "Train Epoch: 177 [172416/225000 (77%)] Loss: 19128.962891\n",
      "Train Epoch: 177 [174912/225000 (78%)] Loss: 18828.326172\n",
      "Train Epoch: 177 [177408/225000 (79%)] Loss: 19387.080078\n",
      "Train Epoch: 177 [179904/225000 (80%)] Loss: 19266.140625\n",
      "Train Epoch: 177 [182400/225000 (81%)] Loss: 19068.156250\n",
      "Train Epoch: 177 [184896/225000 (82%)] Loss: 19074.621094\n",
      "Train Epoch: 177 [187392/225000 (83%)] Loss: 19025.191406\n",
      "Train Epoch: 177 [189888/225000 (84%)] Loss: 18988.054688\n",
      "Train Epoch: 177 [192384/225000 (86%)] Loss: 18551.626953\n",
      "Train Epoch: 177 [194880/225000 (87%)] Loss: 19076.496094\n",
      "Train Epoch: 177 [197376/225000 (88%)] Loss: 19573.125000\n",
      "Train Epoch: 177 [199872/225000 (89%)] Loss: 19007.093750\n",
      "Train Epoch: 177 [202368/225000 (90%)] Loss: 18926.171875\n",
      "Train Epoch: 177 [204864/225000 (91%)] Loss: 19082.667969\n",
      "Train Epoch: 177 [207360/225000 (92%)] Loss: 19710.462891\n",
      "Train Epoch: 177 [209856/225000 (93%)] Loss: 19081.816406\n",
      "Train Epoch: 177 [212352/225000 (94%)] Loss: 19026.988281\n",
      "Train Epoch: 177 [214848/225000 (95%)] Loss: 19057.101562\n",
      "Train Epoch: 177 [217344/225000 (97%)] Loss: 18963.238281\n",
      "Train Epoch: 177 [219840/225000 (98%)] Loss: 18965.523438\n",
      "Train Epoch: 177 [222336/225000 (99%)] Loss: 19271.925781\n",
      "Train Epoch: 177 [224832/225000 (100%)] Loss: 19250.738281\n",
      "    epoch          : 177\n",
      "    loss           : 19138.526087217364\n",
      "    val_loss       : 19034.671624487593\n",
      "Train Epoch: 178 [192/225000 (0%)] Loss: 18861.457031\n",
      "Train Epoch: 178 [2688/225000 (1%)] Loss: 19696.226562\n",
      "Train Epoch: 178 [5184/225000 (2%)] Loss: 19227.625000\n",
      "Train Epoch: 178 [7680/225000 (3%)] Loss: 19044.304688\n",
      "Train Epoch: 178 [10176/225000 (5%)] Loss: 19491.664062\n",
      "Train Epoch: 178 [12672/225000 (6%)] Loss: 18994.744141\n",
      "Train Epoch: 178 [15168/225000 (7%)] Loss: 19578.859375\n",
      "Train Epoch: 178 [17664/225000 (8%)] Loss: 19674.472656\n",
      "Train Epoch: 178 [20160/225000 (9%)] Loss: 19924.105469\n",
      "Train Epoch: 178 [22656/225000 (10%)] Loss: 19823.675781\n",
      "Train Epoch: 178 [25152/225000 (11%)] Loss: 18857.191406\n",
      "Train Epoch: 178 [27648/225000 (12%)] Loss: 19231.292969\n",
      "Train Epoch: 178 [30144/225000 (13%)] Loss: 19439.875000\n",
      "Train Epoch: 178 [32640/225000 (15%)] Loss: 19253.980469\n",
      "Train Epoch: 178 [35136/225000 (16%)] Loss: 19073.460938\n",
      "Train Epoch: 178 [37632/225000 (17%)] Loss: 18913.537109\n",
      "Train Epoch: 178 [40128/225000 (18%)] Loss: 19159.429688\n",
      "Train Epoch: 178 [42624/225000 (19%)] Loss: 18941.753906\n",
      "Train Epoch: 178 [45120/225000 (20%)] Loss: 19241.011719\n",
      "Train Epoch: 178 [47616/225000 (21%)] Loss: 18946.691406\n",
      "Train Epoch: 178 [50112/225000 (22%)] Loss: 19644.515625\n",
      "Train Epoch: 178 [52608/225000 (23%)] Loss: 18727.722656\n",
      "Train Epoch: 178 [55104/225000 (24%)] Loss: 19189.558594\n",
      "Train Epoch: 178 [57600/225000 (26%)] Loss: 19050.078125\n",
      "Train Epoch: 178 [60096/225000 (27%)] Loss: 19088.972656\n",
      "Train Epoch: 178 [62592/225000 (28%)] Loss: 18838.449219\n",
      "Train Epoch: 178 [65088/225000 (29%)] Loss: 19265.916016\n",
      "Train Epoch: 178 [67584/225000 (30%)] Loss: 19771.164062\n",
      "Train Epoch: 178 [70080/225000 (31%)] Loss: 19282.541016\n",
      "Train Epoch: 178 [72576/225000 (32%)] Loss: 19286.511719\n",
      "Train Epoch: 178 [75072/225000 (33%)] Loss: 19576.859375\n",
      "Train Epoch: 178 [77568/225000 (34%)] Loss: 19498.707031\n",
      "Train Epoch: 178 [80064/225000 (36%)] Loss: 18879.406250\n",
      "Train Epoch: 178 [82560/225000 (37%)] Loss: 19123.484375\n",
      "Train Epoch: 178 [85056/225000 (38%)] Loss: 18871.406250\n",
      "Train Epoch: 178 [87552/225000 (39%)] Loss: 19355.582031\n",
      "Train Epoch: 178 [90048/225000 (40%)] Loss: 18968.855469\n",
      "Train Epoch: 178 [92544/225000 (41%)] Loss: 19292.175781\n",
      "Train Epoch: 178 [95040/225000 (42%)] Loss: 19135.634766\n",
      "Train Epoch: 178 [97536/225000 (43%)] Loss: 19124.806641\n",
      "Train Epoch: 178 [100032/225000 (44%)] Loss: 18971.369141\n",
      "Train Epoch: 178 [102528/225000 (46%)] Loss: 18949.736328\n",
      "Train Epoch: 178 [105024/225000 (47%)] Loss: 18996.277344\n",
      "Train Epoch: 178 [107520/225000 (48%)] Loss: 18750.335938\n",
      "Train Epoch: 178 [110016/225000 (49%)] Loss: 19554.699219\n",
      "Train Epoch: 178 [112512/225000 (50%)] Loss: 18879.273438\n",
      "Train Epoch: 178 [115008/225000 (51%)] Loss: 19143.710938\n",
      "Train Epoch: 178 [117504/225000 (52%)] Loss: 19380.742188\n",
      "Train Epoch: 178 [120000/225000 (53%)] Loss: 19076.277344\n",
      "Train Epoch: 178 [122496/225000 (54%)] Loss: 19292.207031\n",
      "Train Epoch: 178 [124992/225000 (56%)] Loss: 19511.496094\n",
      "Train Epoch: 178 [127488/225000 (57%)] Loss: 19119.902344\n",
      "Train Epoch: 178 [129984/225000 (58%)] Loss: 19773.457031\n",
      "Train Epoch: 178 [132480/225000 (59%)] Loss: 19230.019531\n",
      "Train Epoch: 178 [134976/225000 (60%)] Loss: 18870.093750\n",
      "Train Epoch: 178 [137472/225000 (61%)] Loss: 19094.171875\n",
      "Train Epoch: 178 [139968/225000 (62%)] Loss: 19235.685547\n",
      "Train Epoch: 178 [142464/225000 (63%)] Loss: 19303.437500\n",
      "Train Epoch: 178 [144960/225000 (64%)] Loss: 19277.343750\n",
      "Train Epoch: 178 [147456/225000 (66%)] Loss: 19147.623047\n",
      "Train Epoch: 178 [149952/225000 (67%)] Loss: 18990.214844\n",
      "Train Epoch: 178 [152448/225000 (68%)] Loss: 19202.312500\n",
      "Train Epoch: 178 [154944/225000 (69%)] Loss: 19386.681641\n",
      "Train Epoch: 178 [157440/225000 (70%)] Loss: 19117.281250\n",
      "Train Epoch: 178 [159936/225000 (71%)] Loss: 19007.212891\n",
      "Train Epoch: 178 [162432/225000 (72%)] Loss: 19145.726562\n",
      "Train Epoch: 178 [164928/225000 (73%)] Loss: 18995.552734\n",
      "Train Epoch: 178 [167424/225000 (74%)] Loss: 18968.576172\n",
      "Train Epoch: 178 [169920/225000 (76%)] Loss: 18758.048828\n",
      "Train Epoch: 178 [172416/225000 (77%)] Loss: 19393.134766\n",
      "Train Epoch: 178 [174912/225000 (78%)] Loss: 19041.199219\n",
      "Train Epoch: 178 [177408/225000 (79%)] Loss: 19349.298828\n",
      "Train Epoch: 178 [179904/225000 (80%)] Loss: 19321.589844\n",
      "Train Epoch: 178 [182400/225000 (81%)] Loss: 18817.816406\n",
      "Train Epoch: 178 [184896/225000 (82%)] Loss: 19225.908203\n",
      "Train Epoch: 178 [187392/225000 (83%)] Loss: 19314.091797\n",
      "Train Epoch: 178 [189888/225000 (84%)] Loss: 19613.988281\n",
      "Train Epoch: 178 [192384/225000 (86%)] Loss: 19426.628906\n",
      "Train Epoch: 178 [194880/225000 (87%)] Loss: 18902.447266\n",
      "Train Epoch: 178 [197376/225000 (88%)] Loss: 18887.414062\n",
      "Train Epoch: 178 [199872/225000 (89%)] Loss: 19141.589844\n",
      "Train Epoch: 178 [202368/225000 (90%)] Loss: 19167.574219\n",
      "Train Epoch: 178 [204864/225000 (91%)] Loss: 19144.800781\n",
      "Train Epoch: 178 [207360/225000 (92%)] Loss: 19533.068359\n",
      "Train Epoch: 178 [209856/225000 (93%)] Loss: 19145.835938\n",
      "Train Epoch: 178 [212352/225000 (94%)] Loss: 18961.425781\n",
      "Train Epoch: 178 [214848/225000 (95%)] Loss: 19414.482422\n",
      "Train Epoch: 178 [217344/225000 (97%)] Loss: 18814.148438\n",
      "Train Epoch: 178 [219840/225000 (98%)] Loss: 18993.628906\n",
      "Train Epoch: 178 [222336/225000 (99%)] Loss: 19265.257812\n",
      "Train Epoch: 178 [224832/225000 (100%)] Loss: 18740.253906\n",
      "    epoch          : 178\n",
      "    loss           : 19129.748201858467\n",
      "    val_loss       : 19022.642317049376\n",
      "Train Epoch: 179 [192/225000 (0%)] Loss: 18876.097656\n",
      "Train Epoch: 179 [2688/225000 (1%)] Loss: 19208.615234\n",
      "Train Epoch: 179 [5184/225000 (2%)] Loss: 19197.507812\n",
      "Train Epoch: 179 [7680/225000 (3%)] Loss: 19317.890625\n",
      "Train Epoch: 179 [10176/225000 (5%)] Loss: 18604.468750\n",
      "Train Epoch: 179 [12672/225000 (6%)] Loss: 19621.683594\n",
      "Train Epoch: 179 [15168/225000 (7%)] Loss: 18989.433594\n",
      "Train Epoch: 179 [17664/225000 (8%)] Loss: 19098.011719\n",
      "Train Epoch: 179 [20160/225000 (9%)] Loss: 19378.775391\n",
      "Train Epoch: 179 [22656/225000 (10%)] Loss: 19358.753906\n",
      "Train Epoch: 179 [25152/225000 (11%)] Loss: 19822.037109\n",
      "Train Epoch: 179 [27648/225000 (12%)] Loss: 19021.449219\n",
      "Train Epoch: 179 [30144/225000 (13%)] Loss: 19022.542969\n",
      "Train Epoch: 179 [32640/225000 (15%)] Loss: 19131.283203\n",
      "Train Epoch: 179 [35136/225000 (16%)] Loss: 19351.933594\n",
      "Train Epoch: 179 [37632/225000 (17%)] Loss: 18920.722656\n",
      "Train Epoch: 179 [40128/225000 (18%)] Loss: 18583.242188\n",
      "Train Epoch: 179 [42624/225000 (19%)] Loss: 18940.238281\n",
      "Train Epoch: 179 [45120/225000 (20%)] Loss: 19194.261719\n",
      "Train Epoch: 179 [47616/225000 (21%)] Loss: 19293.009766\n",
      "Train Epoch: 179 [50112/225000 (22%)] Loss: 18863.638672\n",
      "Train Epoch: 179 [52608/225000 (23%)] Loss: 19113.166016\n",
      "Train Epoch: 179 [55104/225000 (24%)] Loss: 19243.310547\n",
      "Train Epoch: 179 [57600/225000 (26%)] Loss: 19311.785156\n",
      "Train Epoch: 179 [60096/225000 (27%)] Loss: 18826.750000\n",
      "Train Epoch: 179 [62592/225000 (28%)] Loss: 19303.162109\n",
      "Train Epoch: 179 [65088/225000 (29%)] Loss: 19275.820312\n",
      "Train Epoch: 179 [67584/225000 (30%)] Loss: 18966.562500\n",
      "Train Epoch: 179 [70080/225000 (31%)] Loss: 18973.519531\n",
      "Train Epoch: 179 [72576/225000 (32%)] Loss: 19140.367188\n",
      "Train Epoch: 179 [75072/225000 (33%)] Loss: 18764.453125\n",
      "Train Epoch: 179 [77568/225000 (34%)] Loss: 19624.972656\n",
      "Train Epoch: 179 [80064/225000 (36%)] Loss: 19555.566406\n",
      "Train Epoch: 179 [82560/225000 (37%)] Loss: 19095.187500\n",
      "Train Epoch: 179 [85056/225000 (38%)] Loss: 18526.650391\n",
      "Train Epoch: 179 [87552/225000 (39%)] Loss: 18538.062500\n",
      "Train Epoch: 179 [90048/225000 (40%)] Loss: 19115.277344\n",
      "Train Epoch: 179 [92544/225000 (41%)] Loss: 19032.679688\n",
      "Train Epoch: 179 [95040/225000 (42%)] Loss: 19098.998047\n",
      "Train Epoch: 179 [97536/225000 (43%)] Loss: 19181.101562\n",
      "Train Epoch: 179 [100032/225000 (44%)] Loss: 18864.613281\n",
      "Train Epoch: 179 [102528/225000 (46%)] Loss: 19284.652344\n",
      "Train Epoch: 179 [105024/225000 (47%)] Loss: 19046.753906\n",
      "Train Epoch: 179 [107520/225000 (48%)] Loss: 18957.212891\n",
      "Train Epoch: 179 [110016/225000 (49%)] Loss: 18958.558594\n",
      "Train Epoch: 179 [112512/225000 (50%)] Loss: 19435.550781\n",
      "Train Epoch: 179 [115008/225000 (51%)] Loss: 19306.691406\n",
      "Train Epoch: 179 [117504/225000 (52%)] Loss: 18584.011719\n",
      "Train Epoch: 179 [120000/225000 (53%)] Loss: 18910.664062\n",
      "Train Epoch: 179 [122496/225000 (54%)] Loss: 19114.808594\n",
      "Train Epoch: 179 [124992/225000 (56%)] Loss: 19084.931641\n",
      "Train Epoch: 179 [127488/225000 (57%)] Loss: 19476.164062\n",
      "Train Epoch: 179 [129984/225000 (58%)] Loss: 19310.341797\n",
      "Train Epoch: 179 [132480/225000 (59%)] Loss: 18886.730469\n",
      "Train Epoch: 179 [134976/225000 (60%)] Loss: 19149.859375\n",
      "Train Epoch: 179 [137472/225000 (61%)] Loss: 19234.105469\n",
      "Train Epoch: 179 [139968/225000 (62%)] Loss: 18989.132812\n",
      "Train Epoch: 179 [142464/225000 (63%)] Loss: 19307.279297\n",
      "Train Epoch: 179 [144960/225000 (64%)] Loss: 18890.082031\n",
      "Train Epoch: 179 [147456/225000 (66%)] Loss: 19244.316406\n",
      "Train Epoch: 179 [149952/225000 (67%)] Loss: 18890.734375\n",
      "Train Epoch: 179 [152448/225000 (68%)] Loss: 19281.333984\n",
      "Train Epoch: 179 [154944/225000 (69%)] Loss: 19289.285156\n",
      "Train Epoch: 179 [157440/225000 (70%)] Loss: 19097.568359\n",
      "Train Epoch: 179 [159936/225000 (71%)] Loss: 19061.947266\n",
      "Train Epoch: 179 [162432/225000 (72%)] Loss: 18888.437500\n",
      "Train Epoch: 179 [164928/225000 (73%)] Loss: 19116.962891\n",
      "Train Epoch: 179 [167424/225000 (74%)] Loss: 19128.882812\n",
      "Train Epoch: 179 [169920/225000 (76%)] Loss: 18950.445312\n",
      "Train Epoch: 179 [172416/225000 (77%)] Loss: 18476.177734\n",
      "Train Epoch: 179 [174912/225000 (78%)] Loss: 18993.533203\n",
      "Train Epoch: 179 [177408/225000 (79%)] Loss: 18747.343750\n",
      "Train Epoch: 179 [179904/225000 (80%)] Loss: 19580.328125\n",
      "Train Epoch: 179 [182400/225000 (81%)] Loss: 19649.968750\n",
      "Train Epoch: 179 [184896/225000 (82%)] Loss: 19414.601562\n",
      "Train Epoch: 179 [187392/225000 (83%)] Loss: 19369.207031\n",
      "Train Epoch: 179 [189888/225000 (84%)] Loss: 19652.273438\n",
      "Train Epoch: 179 [192384/225000 (86%)] Loss: 19164.117188\n",
      "Train Epoch: 179 [194880/225000 (87%)] Loss: 19161.910156\n",
      "Train Epoch: 179 [197376/225000 (88%)] Loss: 19072.542969\n",
      "Train Epoch: 179 [199872/225000 (89%)] Loss: 19389.255859\n",
      "Train Epoch: 179 [202368/225000 (90%)] Loss: 18825.996094\n",
      "Train Epoch: 179 [204864/225000 (91%)] Loss: 19168.753906\n",
      "Train Epoch: 179 [207360/225000 (92%)] Loss: 19065.650391\n",
      "Train Epoch: 179 [209856/225000 (93%)] Loss: 18947.308594\n",
      "Train Epoch: 179 [212352/225000 (94%)] Loss: 19254.769531\n",
      "Train Epoch: 179 [214848/225000 (95%)] Loss: 19273.748047\n",
      "Train Epoch: 179 [217344/225000 (97%)] Loss: 18733.082031\n",
      "Train Epoch: 179 [219840/225000 (98%)] Loss: 18724.367188\n",
      "Train Epoch: 179 [222336/225000 (99%)] Loss: 19262.484375\n",
      "Train Epoch: 179 [224832/225000 (100%)] Loss: 19507.898438\n",
      "    epoch          : 179\n",
      "    loss           : 19130.832384545647\n",
      "    val_loss       : 19068.853146147183\n",
      "Train Epoch: 180 [192/225000 (0%)] Loss: 19236.578125\n",
      "Train Epoch: 180 [2688/225000 (1%)] Loss: 19354.722656\n",
      "Train Epoch: 180 [5184/225000 (2%)] Loss: 19363.332031\n",
      "Train Epoch: 180 [7680/225000 (3%)] Loss: 19182.224609\n",
      "Train Epoch: 180 [10176/225000 (5%)] Loss: 19409.478516\n",
      "Train Epoch: 180 [12672/225000 (6%)] Loss: 19058.330078\n",
      "Train Epoch: 180 [15168/225000 (7%)] Loss: 19181.460938\n",
      "Train Epoch: 180 [17664/225000 (8%)] Loss: 18867.310547\n",
      "Train Epoch: 180 [20160/225000 (9%)] Loss: 19079.023438\n",
      "Train Epoch: 180 [22656/225000 (10%)] Loss: 18778.367188\n",
      "Train Epoch: 180 [25152/225000 (11%)] Loss: 19555.828125\n",
      "Train Epoch: 180 [27648/225000 (12%)] Loss: 19279.925781\n",
      "Train Epoch: 180 [30144/225000 (13%)] Loss: 18892.101562\n",
      "Train Epoch: 180 [32640/225000 (15%)] Loss: 19396.855469\n",
      "Train Epoch: 180 [35136/225000 (16%)] Loss: 19156.800781\n",
      "Train Epoch: 180 [37632/225000 (17%)] Loss: 18659.300781\n",
      "Train Epoch: 180 [40128/225000 (18%)] Loss: 19273.953125\n",
      "Train Epoch: 180 [42624/225000 (19%)] Loss: 18985.691406\n",
      "Train Epoch: 180 [45120/225000 (20%)] Loss: 18653.699219\n",
      "Train Epoch: 180 [47616/225000 (21%)] Loss: 19528.621094\n",
      "Train Epoch: 180 [50112/225000 (22%)] Loss: 19386.025391\n",
      "Train Epoch: 180 [52608/225000 (23%)] Loss: 19325.582031\n",
      "Train Epoch: 180 [55104/225000 (24%)] Loss: 18625.562500\n",
      "Train Epoch: 180 [57600/225000 (26%)] Loss: 18705.597656\n",
      "Train Epoch: 180 [60096/225000 (27%)] Loss: 18717.421875\n",
      "Train Epoch: 180 [62592/225000 (28%)] Loss: 18738.763672\n",
      "Train Epoch: 180 [65088/225000 (29%)] Loss: 19356.847656\n",
      "Train Epoch: 180 [67584/225000 (30%)] Loss: 19432.697266\n",
      "Train Epoch: 180 [70080/225000 (31%)] Loss: 19053.968750\n",
      "Train Epoch: 180 [72576/225000 (32%)] Loss: 19825.265625\n",
      "Train Epoch: 180 [75072/225000 (33%)] Loss: 19345.796875\n",
      "Train Epoch: 180 [77568/225000 (34%)] Loss: 19772.667969\n",
      "Train Epoch: 180 [80064/225000 (36%)] Loss: 19685.566406\n",
      "Train Epoch: 180 [82560/225000 (37%)] Loss: 19161.722656\n",
      "Train Epoch: 180 [85056/225000 (38%)] Loss: 18894.785156\n",
      "Train Epoch: 180 [87552/225000 (39%)] Loss: 18754.199219\n",
      "Train Epoch: 180 [90048/225000 (40%)] Loss: 19312.628906\n",
      "Train Epoch: 180 [92544/225000 (41%)] Loss: 19064.919922\n",
      "Train Epoch: 180 [95040/225000 (42%)] Loss: 18890.972656\n",
      "Train Epoch: 180 [97536/225000 (43%)] Loss: 19086.347656\n",
      "Train Epoch: 180 [100032/225000 (44%)] Loss: 19460.437500\n",
      "Train Epoch: 180 [102528/225000 (46%)] Loss: 18682.445312\n",
      "Train Epoch: 180 [105024/225000 (47%)] Loss: 19343.476562\n",
      "Train Epoch: 180 [107520/225000 (48%)] Loss: 19207.314453\n",
      "Train Epoch: 180 [110016/225000 (49%)] Loss: 19034.634766\n",
      "Train Epoch: 180 [112512/225000 (50%)] Loss: 19258.060547\n",
      "Train Epoch: 180 [115008/225000 (51%)] Loss: 18678.564453\n",
      "Train Epoch: 180 [117504/225000 (52%)] Loss: 19389.119141\n",
      "Train Epoch: 180 [120000/225000 (53%)] Loss: 19239.732422\n",
      "Train Epoch: 180 [122496/225000 (54%)] Loss: 19236.113281\n",
      "Train Epoch: 180 [124992/225000 (56%)] Loss: 18801.261719\n",
      "Train Epoch: 180 [127488/225000 (57%)] Loss: 19116.035156\n",
      "Train Epoch: 180 [129984/225000 (58%)] Loss: 19346.363281\n",
      "Train Epoch: 180 [132480/225000 (59%)] Loss: 19640.750000\n",
      "Train Epoch: 180 [134976/225000 (60%)] Loss: 19318.171875\n",
      "Train Epoch: 180 [137472/225000 (61%)] Loss: 18962.144531\n",
      "Train Epoch: 180 [139968/225000 (62%)] Loss: 19211.871094\n",
      "Train Epoch: 180 [142464/225000 (63%)] Loss: 18931.986328\n",
      "Train Epoch: 180 [144960/225000 (64%)] Loss: 18685.082031\n",
      "Train Epoch: 180 [147456/225000 (66%)] Loss: 19255.324219\n",
      "Train Epoch: 180 [149952/225000 (67%)] Loss: 19076.933594\n",
      "Train Epoch: 180 [152448/225000 (68%)] Loss: 19017.677734\n",
      "Train Epoch: 180 [154944/225000 (69%)] Loss: 18969.507812\n",
      "Train Epoch: 180 [157440/225000 (70%)] Loss: 18908.984375\n",
      "Train Epoch: 180 [159936/225000 (71%)] Loss: 19126.156250\n",
      "Train Epoch: 180 [162432/225000 (72%)] Loss: 19034.792969\n",
      "Train Epoch: 180 [164928/225000 (73%)] Loss: 19446.742188\n",
      "Train Epoch: 180 [167424/225000 (74%)] Loss: 19421.218750\n",
      "Train Epoch: 180 [169920/225000 (76%)] Loss: 18989.695312\n",
      "Train Epoch: 180 [172416/225000 (77%)] Loss: 19301.832031\n",
      "Train Epoch: 180 [174912/225000 (78%)] Loss: 19229.029297\n",
      "Train Epoch: 180 [177408/225000 (79%)] Loss: 19290.550781\n",
      "Train Epoch: 180 [179904/225000 (80%)] Loss: 18653.265625\n",
      "Train Epoch: 180 [182400/225000 (81%)] Loss: 18731.648438\n",
      "Train Epoch: 180 [184896/225000 (82%)] Loss: 19018.390625\n",
      "Train Epoch: 180 [187392/225000 (83%)] Loss: 19325.820312\n",
      "Train Epoch: 180 [189888/225000 (84%)] Loss: 19372.414062\n",
      "Train Epoch: 180 [192384/225000 (86%)] Loss: 19306.910156\n",
      "Train Epoch: 180 [194880/225000 (87%)] Loss: 19345.058594\n",
      "Train Epoch: 180 [197376/225000 (88%)] Loss: 18583.714844\n",
      "Train Epoch: 180 [199872/225000 (89%)] Loss: 19475.416016\n",
      "Train Epoch: 180 [202368/225000 (90%)] Loss: 19373.787109\n",
      "Train Epoch: 180 [204864/225000 (91%)] Loss: 19094.806641\n",
      "Train Epoch: 180 [207360/225000 (92%)] Loss: 19119.488281\n",
      "Train Epoch: 180 [209856/225000 (93%)] Loss: 19048.097656\n",
      "Train Epoch: 180 [212352/225000 (94%)] Loss: 18711.853516\n",
      "Train Epoch: 180 [214848/225000 (95%)] Loss: 18967.699219\n",
      "Train Epoch: 180 [217344/225000 (97%)] Loss: 19151.796875\n",
      "Train Epoch: 180 [219840/225000 (98%)] Loss: 19617.933594\n",
      "Train Epoch: 180 [222336/225000 (99%)] Loss: 18650.630859\n",
      "Train Epoch: 180 [224832/225000 (100%)] Loss: 19292.984375\n",
      "    epoch          : 180\n",
      "    loss           : 19124.414925741254\n",
      "    val_loss       : 19021.44695551978\n",
      "Train Epoch: 181 [192/225000 (0%)] Loss: 18895.054688\n",
      "Train Epoch: 181 [2688/225000 (1%)] Loss: 18787.503906\n",
      "Train Epoch: 181 [5184/225000 (2%)] Loss: 18794.089844\n",
      "Train Epoch: 181 [7680/225000 (3%)] Loss: 19446.656250\n",
      "Train Epoch: 181 [10176/225000 (5%)] Loss: 18861.777344\n",
      "Train Epoch: 181 [12672/225000 (6%)] Loss: 19063.728516\n",
      "Train Epoch: 181 [15168/225000 (7%)] Loss: 19305.535156\n",
      "Train Epoch: 181 [17664/225000 (8%)] Loss: 18957.109375\n",
      "Train Epoch: 181 [20160/225000 (9%)] Loss: 19053.253906\n",
      "Train Epoch: 181 [22656/225000 (10%)] Loss: 19203.351562\n",
      "Train Epoch: 181 [25152/225000 (11%)] Loss: 19208.777344\n",
      "Train Epoch: 181 [27648/225000 (12%)] Loss: 19251.261719\n",
      "Train Epoch: 181 [30144/225000 (13%)] Loss: 19357.255859\n",
      "Train Epoch: 181 [32640/225000 (15%)] Loss: 18763.839844\n",
      "Train Epoch: 181 [35136/225000 (16%)] Loss: 18916.214844\n",
      "Train Epoch: 181 [37632/225000 (17%)] Loss: 19531.769531\n",
      "Train Epoch: 181 [40128/225000 (18%)] Loss: 19430.003906\n",
      "Train Epoch: 181 [42624/225000 (19%)] Loss: 19447.750000\n",
      "Train Epoch: 181 [45120/225000 (20%)] Loss: 18965.779297\n",
      "Train Epoch: 181 [47616/225000 (21%)] Loss: 19040.796875\n",
      "Train Epoch: 181 [50112/225000 (22%)] Loss: 19498.960938\n",
      "Train Epoch: 181 [52608/225000 (23%)] Loss: 19094.308594\n",
      "Train Epoch: 181 [55104/225000 (24%)] Loss: 18790.167969\n",
      "Train Epoch: 181 [57600/225000 (26%)] Loss: 18761.109375\n",
      "Train Epoch: 181 [60096/225000 (27%)] Loss: 19442.332031\n",
      "Train Epoch: 181 [62592/225000 (28%)] Loss: 19082.882812\n",
      "Train Epoch: 181 [65088/225000 (29%)] Loss: 19099.757812\n",
      "Train Epoch: 181 [67584/225000 (30%)] Loss: 19149.791016\n",
      "Train Epoch: 181 [70080/225000 (31%)] Loss: 19132.195312\n",
      "Train Epoch: 181 [72576/225000 (32%)] Loss: 19130.417969\n",
      "Train Epoch: 181 [75072/225000 (33%)] Loss: 19043.781250\n",
      "Train Epoch: 181 [77568/225000 (34%)] Loss: 19048.582031\n",
      "Train Epoch: 181 [80064/225000 (36%)] Loss: 19569.039062\n",
      "Train Epoch: 181 [82560/225000 (37%)] Loss: 19020.527344\n",
      "Train Epoch: 181 [85056/225000 (38%)] Loss: 19328.820312\n",
      "Train Epoch: 181 [87552/225000 (39%)] Loss: 19102.851562\n",
      "Train Epoch: 181 [90048/225000 (40%)] Loss: 18829.375000\n",
      "Train Epoch: 181 [92544/225000 (41%)] Loss: 19431.937500\n",
      "Train Epoch: 181 [95040/225000 (42%)] Loss: 18941.828125\n",
      "Train Epoch: 181 [97536/225000 (43%)] Loss: 18780.328125\n",
      "Train Epoch: 181 [100032/225000 (44%)] Loss: 18944.308594\n",
      "Train Epoch: 181 [102528/225000 (46%)] Loss: 19138.306641\n",
      "Train Epoch: 181 [105024/225000 (47%)] Loss: 18881.320312\n",
      "Train Epoch: 181 [107520/225000 (48%)] Loss: 18881.445312\n",
      "Train Epoch: 181 [110016/225000 (49%)] Loss: 18464.664062\n",
      "Train Epoch: 181 [112512/225000 (50%)] Loss: 18954.523438\n",
      "Train Epoch: 181 [115008/225000 (51%)] Loss: 19423.402344\n",
      "Train Epoch: 181 [117504/225000 (52%)] Loss: 18808.183594\n",
      "Train Epoch: 181 [120000/225000 (53%)] Loss: 18750.382812\n",
      "Train Epoch: 181 [122496/225000 (54%)] Loss: 19013.609375\n",
      "Train Epoch: 181 [124992/225000 (56%)] Loss: 18891.277344\n",
      "Train Epoch: 181 [127488/225000 (57%)] Loss: 19391.869141\n",
      "Train Epoch: 181 [129984/225000 (58%)] Loss: 19262.851562\n",
      "Train Epoch: 181 [132480/225000 (59%)] Loss: 19073.761719\n",
      "Train Epoch: 181 [134976/225000 (60%)] Loss: 19267.140625\n",
      "Train Epoch: 181 [137472/225000 (61%)] Loss: 19550.947266\n",
      "Train Epoch: 181 [139968/225000 (62%)] Loss: 19104.705078\n",
      "Train Epoch: 181 [142464/225000 (63%)] Loss: 18680.855469\n",
      "Train Epoch: 181 [144960/225000 (64%)] Loss: 19039.566406\n",
      "Train Epoch: 181 [147456/225000 (66%)] Loss: 19502.304688\n",
      "Train Epoch: 181 [149952/225000 (67%)] Loss: 19178.027344\n",
      "Train Epoch: 181 [152448/225000 (68%)] Loss: 19175.464844\n",
      "Train Epoch: 181 [154944/225000 (69%)] Loss: 19327.628906\n",
      "Train Epoch: 181 [157440/225000 (70%)] Loss: 19058.304688\n",
      "Train Epoch: 181 [159936/225000 (71%)] Loss: 18868.703125\n",
      "Train Epoch: 181 [162432/225000 (72%)] Loss: 19218.664062\n",
      "Train Epoch: 181 [164928/225000 (73%)] Loss: 19514.722656\n",
      "Train Epoch: 181 [167424/225000 (74%)] Loss: 18917.984375\n",
      "Train Epoch: 181 [169920/225000 (76%)] Loss: 19680.171875\n",
      "Train Epoch: 181 [172416/225000 (77%)] Loss: 19225.869141\n",
      "Train Epoch: 181 [174912/225000 (78%)] Loss: 19669.992188\n",
      "Train Epoch: 181 [177408/225000 (79%)] Loss: 19194.794922\n",
      "Train Epoch: 181 [179904/225000 (80%)] Loss: 19631.187500\n",
      "Train Epoch: 181 [182400/225000 (81%)] Loss: 18604.031250\n",
      "Train Epoch: 181 [184896/225000 (82%)] Loss: 18860.308594\n",
      "Train Epoch: 181 [187392/225000 (83%)] Loss: 18837.787109\n",
      "Train Epoch: 181 [189888/225000 (84%)] Loss: 18661.769531\n",
      "Train Epoch: 181 [192384/225000 (86%)] Loss: 18713.804688\n",
      "Train Epoch: 181 [194880/225000 (87%)] Loss: 18577.531250\n",
      "Train Epoch: 181 [197376/225000 (88%)] Loss: 18479.097656\n",
      "Train Epoch: 181 [199872/225000 (89%)] Loss: 19379.128906\n",
      "Train Epoch: 181 [202368/225000 (90%)] Loss: 19048.238281\n",
      "Train Epoch: 181 [204864/225000 (91%)] Loss: 18975.058594\n",
      "Train Epoch: 181 [207360/225000 (92%)] Loss: 19453.753906\n",
      "Train Epoch: 181 [209856/225000 (93%)] Loss: 19339.964844\n",
      "Train Epoch: 181 [212352/225000 (94%)] Loss: 19330.253906\n",
      "Train Epoch: 181 [214848/225000 (95%)] Loss: 18369.121094\n",
      "Train Epoch: 181 [217344/225000 (97%)] Loss: 19384.851562\n",
      "Train Epoch: 181 [219840/225000 (98%)] Loss: 18928.503906\n",
      "Train Epoch: 181 [222336/225000 (99%)] Loss: 19109.681641\n",
      "Train Epoch: 181 [224832/225000 (100%)] Loss: 18799.101562\n",
      "    epoch          : 181\n",
      "    loss           : 19092.47403276984\n",
      "    val_loss       : 18999.67957667358\n",
      "Train Epoch: 182 [192/225000 (0%)] Loss: 18950.927734\n",
      "Train Epoch: 182 [2688/225000 (1%)] Loss: 19380.492188\n",
      "Train Epoch: 182 [5184/225000 (2%)] Loss: 19295.691406\n",
      "Train Epoch: 182 [7680/225000 (3%)] Loss: 19168.933594\n",
      "Train Epoch: 182 [10176/225000 (5%)] Loss: 19055.978516\n",
      "Train Epoch: 182 [12672/225000 (6%)] Loss: 18839.687500\n",
      "Train Epoch: 182 [15168/225000 (7%)] Loss: 18815.218750\n",
      "Train Epoch: 182 [17664/225000 (8%)] Loss: 18990.140625\n",
      "Train Epoch: 182 [20160/225000 (9%)] Loss: 18981.195312\n",
      "Train Epoch: 182 [22656/225000 (10%)] Loss: 19198.947266\n",
      "Train Epoch: 182 [25152/225000 (11%)] Loss: 19067.947266\n",
      "Train Epoch: 182 [27648/225000 (12%)] Loss: 19157.759766\n",
      "Train Epoch: 182 [30144/225000 (13%)] Loss: 19442.517578\n",
      "Train Epoch: 182 [32640/225000 (15%)] Loss: 19117.671875\n",
      "Train Epoch: 182 [35136/225000 (16%)] Loss: 19464.113281\n",
      "Train Epoch: 182 [37632/225000 (17%)] Loss: 18915.343750\n",
      "Train Epoch: 182 [40128/225000 (18%)] Loss: 19127.789062\n",
      "Train Epoch: 182 [42624/225000 (19%)] Loss: 19262.572266\n",
      "Train Epoch: 182 [45120/225000 (20%)] Loss: 18944.914062\n",
      "Train Epoch: 182 [47616/225000 (21%)] Loss: 18806.082031\n",
      "Train Epoch: 182 [50112/225000 (22%)] Loss: 19248.953125\n",
      "Train Epoch: 182 [52608/225000 (23%)] Loss: 18966.183594\n",
      "Train Epoch: 182 [55104/225000 (24%)] Loss: 19237.841797\n",
      "Train Epoch: 182 [57600/225000 (26%)] Loss: 18639.820312\n",
      "Train Epoch: 182 [60096/225000 (27%)] Loss: 19127.130859\n",
      "Train Epoch: 182 [62592/225000 (28%)] Loss: 19265.658203\n",
      "Train Epoch: 182 [65088/225000 (29%)] Loss: 18977.101562\n",
      "Train Epoch: 182 [67584/225000 (30%)] Loss: 18974.650391\n",
      "Train Epoch: 182 [70080/225000 (31%)] Loss: 19444.513672\n",
      "Train Epoch: 182 [72576/225000 (32%)] Loss: 19402.695312\n",
      "Train Epoch: 182 [75072/225000 (33%)] Loss: 18986.289062\n",
      "Train Epoch: 182 [77568/225000 (34%)] Loss: 18935.964844\n",
      "Train Epoch: 182 [80064/225000 (36%)] Loss: 18958.714844\n",
      "Train Epoch: 182 [82560/225000 (37%)] Loss: 19546.550781\n",
      "Train Epoch: 182 [85056/225000 (38%)] Loss: 19380.035156\n",
      "Train Epoch: 182 [87552/225000 (39%)] Loss: 19318.792969\n",
      "Train Epoch: 182 [90048/225000 (40%)] Loss: 19504.816406\n",
      "Train Epoch: 182 [92544/225000 (41%)] Loss: 18699.144531\n",
      "Train Epoch: 182 [95040/225000 (42%)] Loss: 18939.632812\n",
      "Train Epoch: 182 [97536/225000 (43%)] Loss: 19185.146484\n",
      "Train Epoch: 182 [100032/225000 (44%)] Loss: 19179.123047\n",
      "Train Epoch: 182 [102528/225000 (46%)] Loss: 19015.253906\n",
      "Train Epoch: 182 [105024/225000 (47%)] Loss: 19146.636719\n",
      "Train Epoch: 182 [107520/225000 (48%)] Loss: 19481.464844\n",
      "Train Epoch: 182 [110016/225000 (49%)] Loss: 19790.949219\n",
      "Train Epoch: 182 [112512/225000 (50%)] Loss: 19389.773438\n",
      "Train Epoch: 182 [115008/225000 (51%)] Loss: 19498.437500\n",
      "Train Epoch: 182 [117504/225000 (52%)] Loss: 19279.628906\n",
      "Train Epoch: 182 [120000/225000 (53%)] Loss: 19309.050781\n",
      "Train Epoch: 182 [122496/225000 (54%)] Loss: 19107.255859\n",
      "Train Epoch: 182 [124992/225000 (56%)] Loss: 18424.687500\n",
      "Train Epoch: 182 [127488/225000 (57%)] Loss: 19283.595703\n",
      "Train Epoch: 182 [129984/225000 (58%)] Loss: 19493.980469\n",
      "Train Epoch: 182 [132480/225000 (59%)] Loss: 19112.640625\n",
      "Train Epoch: 182 [134976/225000 (60%)] Loss: 19167.410156\n",
      "Train Epoch: 182 [137472/225000 (61%)] Loss: 18768.501953\n",
      "Train Epoch: 182 [139968/225000 (62%)] Loss: 19234.367188\n",
      "Train Epoch: 182 [142464/225000 (63%)] Loss: 18883.019531\n",
      "Train Epoch: 182 [144960/225000 (64%)] Loss: 19225.060547\n",
      "Train Epoch: 182 [147456/225000 (66%)] Loss: 19554.666016\n",
      "Train Epoch: 182 [149952/225000 (67%)] Loss: 19626.812500\n",
      "Train Epoch: 182 [152448/225000 (68%)] Loss: 19528.742188\n",
      "Train Epoch: 182 [154944/225000 (69%)] Loss: 19114.035156\n",
      "Train Epoch: 182 [157440/225000 (70%)] Loss: 18865.781250\n",
      "Train Epoch: 182 [159936/225000 (71%)] Loss: 19139.375000\n",
      "Train Epoch: 182 [162432/225000 (72%)] Loss: 18884.173828\n",
      "Train Epoch: 182 [164928/225000 (73%)] Loss: 18933.125000\n",
      "Train Epoch: 182 [167424/225000 (74%)] Loss: 19171.013672\n",
      "Train Epoch: 182 [169920/225000 (76%)] Loss: 19121.507812\n",
      "Train Epoch: 182 [172416/225000 (77%)] Loss: 19403.593750\n",
      "Train Epoch: 182 [174912/225000 (78%)] Loss: 18865.843750\n",
      "Train Epoch: 182 [177408/225000 (79%)] Loss: 18638.189453\n",
      "Train Epoch: 182 [179904/225000 (80%)] Loss: 18952.853516\n",
      "Train Epoch: 182 [182400/225000 (81%)] Loss: 19079.392578\n",
      "Train Epoch: 182 [184896/225000 (82%)] Loss: 18769.515625\n",
      "Train Epoch: 182 [187392/225000 (83%)] Loss: 19203.060547\n",
      "Train Epoch: 182 [189888/225000 (84%)] Loss: 18707.626953\n",
      "Train Epoch: 182 [192384/225000 (86%)] Loss: 19297.574219\n",
      "Train Epoch: 182 [194880/225000 (87%)] Loss: 18654.554688\n",
      "Train Epoch: 182 [197376/225000 (88%)] Loss: 19479.207031\n",
      "Train Epoch: 182 [199872/225000 (89%)] Loss: 18885.656250\n",
      "Train Epoch: 182 [202368/225000 (90%)] Loss: 19435.968750\n",
      "Train Epoch: 182 [204864/225000 (91%)] Loss: 18941.207031\n",
      "Train Epoch: 182 [207360/225000 (92%)] Loss: 18843.062500\n",
      "Train Epoch: 182 [209856/225000 (93%)] Loss: 21952.980469\n",
      "Train Epoch: 182 [212352/225000 (94%)] Loss: 18898.925781\n",
      "Train Epoch: 182 [214848/225000 (95%)] Loss: 18997.253906\n",
      "Train Epoch: 182 [217344/225000 (97%)] Loss: 19095.970703\n",
      "Train Epoch: 182 [219840/225000 (98%)] Loss: 19361.736328\n",
      "Train Epoch: 182 [222336/225000 (99%)] Loss: 19369.804688\n",
      "Train Epoch: 182 [224832/225000 (100%)] Loss: 19253.660156\n",
      "    epoch          : 182\n",
      "    loss           : 19112.302724376066\n",
      "    val_loss       : 18985.00472309298\n",
      "Train Epoch: 183 [192/225000 (0%)] Loss: 19315.019531\n",
      "Train Epoch: 183 [2688/225000 (1%)] Loss: 18515.871094\n",
      "Train Epoch: 183 [5184/225000 (2%)] Loss: 19129.912109\n",
      "Train Epoch: 183 [7680/225000 (3%)] Loss: 19666.792969\n",
      "Train Epoch: 183 [10176/225000 (5%)] Loss: 19343.128906\n",
      "Train Epoch: 183 [12672/225000 (6%)] Loss: 18448.064453\n",
      "Train Epoch: 183 [15168/225000 (7%)] Loss: 19620.726562\n",
      "Train Epoch: 183 [17664/225000 (8%)] Loss: 18815.011719\n",
      "Train Epoch: 183 [20160/225000 (9%)] Loss: 19151.714844\n",
      "Train Epoch: 183 [22656/225000 (10%)] Loss: 18688.906250\n",
      "Train Epoch: 183 [25152/225000 (11%)] Loss: 19403.503906\n",
      "Train Epoch: 183 [27648/225000 (12%)] Loss: 18963.632812\n",
      "Train Epoch: 183 [30144/225000 (13%)] Loss: 19207.048828\n",
      "Train Epoch: 183 [32640/225000 (15%)] Loss: 19398.660156\n",
      "Train Epoch: 183 [35136/225000 (16%)] Loss: 19028.156250\n",
      "Train Epoch: 183 [37632/225000 (17%)] Loss: 19140.203125\n",
      "Train Epoch: 183 [40128/225000 (18%)] Loss: 19334.654297\n",
      "Train Epoch: 183 [42624/225000 (19%)] Loss: 19502.332031\n",
      "Train Epoch: 183 [45120/225000 (20%)] Loss: 19449.089844\n",
      "Train Epoch: 183 [47616/225000 (21%)] Loss: 19109.695312\n",
      "Train Epoch: 183 [50112/225000 (22%)] Loss: 18877.707031\n",
      "Train Epoch: 183 [52608/225000 (23%)] Loss: 18996.148438\n",
      "Train Epoch: 183 [55104/225000 (24%)] Loss: 19161.531250\n",
      "Train Epoch: 183 [57600/225000 (26%)] Loss: 19191.851562\n",
      "Train Epoch: 183 [60096/225000 (27%)] Loss: 18920.666016\n",
      "Train Epoch: 183 [62592/225000 (28%)] Loss: 19063.291016\n",
      "Train Epoch: 183 [65088/225000 (29%)] Loss: 19806.640625\n",
      "Train Epoch: 183 [67584/225000 (30%)] Loss: 19404.902344\n",
      "Train Epoch: 183 [70080/225000 (31%)] Loss: 19168.832031\n",
      "Train Epoch: 183 [72576/225000 (32%)] Loss: 19190.208984\n",
      "Train Epoch: 183 [75072/225000 (33%)] Loss: 19275.160156\n",
      "Train Epoch: 183 [77568/225000 (34%)] Loss: 18795.726562\n",
      "Train Epoch: 183 [80064/225000 (36%)] Loss: 18784.683594\n",
      "Train Epoch: 183 [82560/225000 (37%)] Loss: 18857.218750\n",
      "Train Epoch: 183 [85056/225000 (38%)] Loss: 19090.996094\n",
      "Train Epoch: 183 [87552/225000 (39%)] Loss: 18866.572266\n",
      "Train Epoch: 183 [90048/225000 (40%)] Loss: 18718.414062\n",
      "Train Epoch: 183 [92544/225000 (41%)] Loss: 19079.085938\n",
      "Train Epoch: 183 [95040/225000 (42%)] Loss: 18713.060547\n",
      "Train Epoch: 183 [97536/225000 (43%)] Loss: 19118.660156\n",
      "Train Epoch: 183 [100032/225000 (44%)] Loss: 18943.464844\n",
      "Train Epoch: 183 [102528/225000 (46%)] Loss: 18958.160156\n",
      "Train Epoch: 183 [105024/225000 (47%)] Loss: 18992.300781\n",
      "Train Epoch: 183 [107520/225000 (48%)] Loss: 19085.050781\n",
      "Train Epoch: 183 [110016/225000 (49%)] Loss: 19086.931641\n",
      "Train Epoch: 183 [112512/225000 (50%)] Loss: 19260.757812\n",
      "Train Epoch: 183 [115008/225000 (51%)] Loss: 19348.261719\n",
      "Train Epoch: 183 [117504/225000 (52%)] Loss: 19049.505859\n",
      "Train Epoch: 183 [120000/225000 (53%)] Loss: 19155.841797\n",
      "Train Epoch: 183 [122496/225000 (54%)] Loss: 19408.208984\n",
      "Train Epoch: 183 [124992/225000 (56%)] Loss: 19164.578125\n",
      "Train Epoch: 183 [127488/225000 (57%)] Loss: 18615.867188\n",
      "Train Epoch: 183 [129984/225000 (58%)] Loss: 19171.546875\n",
      "Train Epoch: 183 [132480/225000 (59%)] Loss: 19133.138672\n",
      "Train Epoch: 183 [134976/225000 (60%)] Loss: 19191.402344\n",
      "Train Epoch: 183 [137472/225000 (61%)] Loss: 19039.488281\n",
      "Train Epoch: 183 [139968/225000 (62%)] Loss: 19696.605469\n",
      "Train Epoch: 183 [142464/225000 (63%)] Loss: 19052.054688\n",
      "Train Epoch: 183 [144960/225000 (64%)] Loss: 18855.966797\n",
      "Train Epoch: 183 [147456/225000 (66%)] Loss: 19515.164062\n",
      "Train Epoch: 183 [149952/225000 (67%)] Loss: 19364.273438\n",
      "Train Epoch: 183 [152448/225000 (68%)] Loss: 18897.501953\n",
      "Train Epoch: 183 [154944/225000 (69%)] Loss: 18576.466797\n",
      "Train Epoch: 183 [157440/225000 (70%)] Loss: 18764.269531\n",
      "Train Epoch: 183 [159936/225000 (71%)] Loss: 19181.679688\n",
      "Train Epoch: 183 [162432/225000 (72%)] Loss: 19198.435547\n",
      "Train Epoch: 183 [164928/225000 (73%)] Loss: 19352.835938\n",
      "Train Epoch: 183 [167424/225000 (74%)] Loss: 18978.230469\n",
      "Train Epoch: 183 [169920/225000 (76%)] Loss: 19524.777344\n",
      "Train Epoch: 183 [172416/225000 (77%)] Loss: 19006.611328\n",
      "Train Epoch: 183 [174912/225000 (78%)] Loss: 18772.582031\n",
      "Train Epoch: 183 [177408/225000 (79%)] Loss: 18797.830078\n",
      "Train Epoch: 183 [179904/225000 (80%)] Loss: 19489.470703\n",
      "Train Epoch: 183 [182400/225000 (81%)] Loss: 19012.429688\n",
      "Train Epoch: 183 [184896/225000 (82%)] Loss: 19486.527344\n",
      "Train Epoch: 183 [187392/225000 (83%)] Loss: 19178.179688\n",
      "Train Epoch: 183 [189888/225000 (84%)] Loss: 18777.003906\n",
      "Train Epoch: 183 [192384/225000 (86%)] Loss: 18508.515625\n",
      "Train Epoch: 183 [194880/225000 (87%)] Loss: 18923.345703\n",
      "Train Epoch: 183 [197376/225000 (88%)] Loss: 19574.736328\n",
      "Train Epoch: 183 [199872/225000 (89%)] Loss: 19227.785156\n",
      "Train Epoch: 183 [202368/225000 (90%)] Loss: 19219.226562\n",
      "Train Epoch: 183 [204864/225000 (91%)] Loss: 18881.275391\n",
      "Train Epoch: 183 [207360/225000 (92%)] Loss: 18907.800781\n",
      "Train Epoch: 183 [209856/225000 (93%)] Loss: 19130.890625\n",
      "Train Epoch: 183 [212352/225000 (94%)] Loss: 18834.843750\n",
      "Train Epoch: 183 [214848/225000 (95%)] Loss: 19067.849609\n",
      "Train Epoch: 183 [217344/225000 (97%)] Loss: 19213.402344\n",
      "Train Epoch: 183 [219840/225000 (98%)] Loss: 19276.074219\n",
      "Train Epoch: 183 [222336/225000 (99%)] Loss: 19030.259766\n",
      "Train Epoch: 183 [224832/225000 (100%)] Loss: 18904.218750\n",
      "    epoch          : 183\n",
      "    loss           : 19076.611654756827\n",
      "    val_loss       : 18981.410959013545\n",
      "Train Epoch: 184 [192/225000 (0%)] Loss: 18967.886719\n",
      "Train Epoch: 184 [2688/225000 (1%)] Loss: 19208.433594\n",
      "Train Epoch: 184 [5184/225000 (2%)] Loss: 18904.023438\n",
      "Train Epoch: 184 [7680/225000 (3%)] Loss: 19726.037109\n",
      "Train Epoch: 184 [10176/225000 (5%)] Loss: 18883.048828\n",
      "Train Epoch: 184 [12672/225000 (6%)] Loss: 18846.097656\n",
      "Train Epoch: 184 [15168/225000 (7%)] Loss: 18925.425781\n",
      "Train Epoch: 184 [17664/225000 (8%)] Loss: 18893.357422\n",
      "Train Epoch: 184 [20160/225000 (9%)] Loss: 18878.339844\n",
      "Train Epoch: 184 [22656/225000 (10%)] Loss: 18336.771484\n",
      "Train Epoch: 184 [25152/225000 (11%)] Loss: 19062.550781\n",
      "Train Epoch: 184 [27648/225000 (12%)] Loss: 18814.820312\n",
      "Train Epoch: 184 [30144/225000 (13%)] Loss: 18774.080078\n",
      "Train Epoch: 184 [32640/225000 (15%)] Loss: 19493.613281\n",
      "Train Epoch: 184 [35136/225000 (16%)] Loss: 19198.113281\n",
      "Train Epoch: 184 [37632/225000 (17%)] Loss: 19193.480469\n",
      "Train Epoch: 184 [40128/225000 (18%)] Loss: 19143.082031\n",
      "Train Epoch: 184 [42624/225000 (19%)] Loss: 19146.242188\n",
      "Train Epoch: 184 [45120/225000 (20%)] Loss: 19283.630859\n",
      "Train Epoch: 184 [47616/225000 (21%)] Loss: 18621.808594\n",
      "Train Epoch: 184 [50112/225000 (22%)] Loss: 18712.355469\n",
      "Train Epoch: 184 [52608/225000 (23%)] Loss: 19140.677734\n",
      "Train Epoch: 184 [55104/225000 (24%)] Loss: 18919.113281\n",
      "Train Epoch: 184 [57600/225000 (26%)] Loss: 18980.152344\n",
      "Train Epoch: 184 [60096/225000 (27%)] Loss: 19425.275391\n",
      "Train Epoch: 184 [62592/225000 (28%)] Loss: 19564.888672\n",
      "Train Epoch: 184 [65088/225000 (29%)] Loss: 19208.412109\n",
      "Train Epoch: 184 [67584/225000 (30%)] Loss: 19421.636719\n",
      "Train Epoch: 184 [70080/225000 (31%)] Loss: 18956.476562\n",
      "Train Epoch: 184 [72576/225000 (32%)] Loss: 19106.242188\n",
      "Train Epoch: 184 [75072/225000 (33%)] Loss: 18423.933594\n",
      "Train Epoch: 184 [77568/225000 (34%)] Loss: 19500.066406\n",
      "Train Epoch: 184 [80064/225000 (36%)] Loss: 18966.259766\n",
      "Train Epoch: 184 [82560/225000 (37%)] Loss: 18938.927734\n",
      "Train Epoch: 184 [85056/225000 (38%)] Loss: 19255.367188\n",
      "Train Epoch: 184 [87552/225000 (39%)] Loss: 19218.515625\n",
      "Train Epoch: 184 [90048/225000 (40%)] Loss: 19451.031250\n",
      "Train Epoch: 184 [92544/225000 (41%)] Loss: 18914.427734\n",
      "Train Epoch: 184 [95040/225000 (42%)] Loss: 18743.535156\n",
      "Train Epoch: 184 [97536/225000 (43%)] Loss: 18985.203125\n",
      "Train Epoch: 184 [100032/225000 (44%)] Loss: 19266.753906\n",
      "Train Epoch: 184 [102528/225000 (46%)] Loss: 19159.507812\n",
      "Train Epoch: 184 [105024/225000 (47%)] Loss: 19156.562500\n",
      "Train Epoch: 184 [107520/225000 (48%)] Loss: 19011.261719\n",
      "Train Epoch: 184 [110016/225000 (49%)] Loss: 19091.339844\n",
      "Train Epoch: 184 [112512/225000 (50%)] Loss: 19221.527344\n",
      "Train Epoch: 184 [115008/225000 (51%)] Loss: 19445.312500\n",
      "Train Epoch: 184 [117504/225000 (52%)] Loss: 19254.792969\n",
      "Train Epoch: 184 [120000/225000 (53%)] Loss: 18552.599609\n",
      "Train Epoch: 184 [122496/225000 (54%)] Loss: 18816.667969\n",
      "Train Epoch: 184 [124992/225000 (56%)] Loss: 19497.269531\n",
      "Train Epoch: 184 [127488/225000 (57%)] Loss: 18687.796875\n",
      "Train Epoch: 184 [129984/225000 (58%)] Loss: 18737.785156\n",
      "Train Epoch: 184 [132480/225000 (59%)] Loss: 19107.894531\n",
      "Train Epoch: 184 [134976/225000 (60%)] Loss: 19150.625000\n",
      "Train Epoch: 184 [137472/225000 (61%)] Loss: 19138.785156\n",
      "Train Epoch: 184 [139968/225000 (62%)] Loss: 19036.460938\n",
      "Train Epoch: 184 [142464/225000 (63%)] Loss: 19267.054688\n",
      "Train Epoch: 184 [144960/225000 (64%)] Loss: 18956.863281\n",
      "Train Epoch: 184 [147456/225000 (66%)] Loss: 19185.458984\n",
      "Train Epoch: 184 [149952/225000 (67%)] Loss: 19314.855469\n",
      "Train Epoch: 184 [152448/225000 (68%)] Loss: 19249.611328\n",
      "Train Epoch: 184 [154944/225000 (69%)] Loss: 18846.146484\n",
      "Train Epoch: 184 [157440/225000 (70%)] Loss: 19021.636719\n",
      "Train Epoch: 184 [159936/225000 (71%)] Loss: 19271.597656\n",
      "Train Epoch: 184 [162432/225000 (72%)] Loss: 18536.125000\n",
      "Train Epoch: 184 [164928/225000 (73%)] Loss: 18939.931641\n",
      "Train Epoch: 184 [167424/225000 (74%)] Loss: 19158.871094\n",
      "Train Epoch: 184 [169920/225000 (76%)] Loss: 19121.398438\n",
      "Train Epoch: 184 [172416/225000 (77%)] Loss: 19571.894531\n",
      "Train Epoch: 184 [174912/225000 (78%)] Loss: 19246.644531\n",
      "Train Epoch: 184 [177408/225000 (79%)] Loss: 18715.367188\n",
      "Train Epoch: 184 [179904/225000 (80%)] Loss: 19251.679688\n",
      "Train Epoch: 184 [182400/225000 (81%)] Loss: 18900.064453\n",
      "Train Epoch: 184 [184896/225000 (82%)] Loss: 19196.316406\n",
      "Train Epoch: 184 [187392/225000 (83%)] Loss: 19190.750000\n",
      "Train Epoch: 184 [189888/225000 (84%)] Loss: 18704.560547\n",
      "Train Epoch: 184 [192384/225000 (86%)] Loss: 19323.179688\n",
      "Train Epoch: 184 [194880/225000 (87%)] Loss: 19264.585938\n",
      "Train Epoch: 184 [197376/225000 (88%)] Loss: 18510.042969\n",
      "Train Epoch: 184 [199872/225000 (89%)] Loss: 19131.566406\n",
      "Train Epoch: 184 [202368/225000 (90%)] Loss: 18632.539062\n",
      "Train Epoch: 184 [204864/225000 (91%)] Loss: 18714.464844\n",
      "Train Epoch: 184 [207360/225000 (92%)] Loss: 18500.162109\n",
      "Train Epoch: 184 [209856/225000 (93%)] Loss: 19092.386719\n",
      "Train Epoch: 184 [212352/225000 (94%)] Loss: 19294.597656\n",
      "Train Epoch: 184 [214848/225000 (95%)] Loss: 18778.693359\n",
      "Train Epoch: 184 [217344/225000 (97%)] Loss: 19200.976562\n",
      "Train Epoch: 184 [219840/225000 (98%)] Loss: 19336.542969\n",
      "Train Epoch: 184 [222336/225000 (99%)] Loss: 18639.886719\n",
      "Train Epoch: 184 [224832/225000 (100%)] Loss: 18903.968750\n",
      "    epoch          : 184\n",
      "    loss           : 19059.681647290956\n",
      "    val_loss       : 18989.042038075797\n",
      "Train Epoch: 185 [192/225000 (0%)] Loss: 19194.457031\n",
      "Train Epoch: 185 [2688/225000 (1%)] Loss: 19209.734375\n",
      "Train Epoch: 185 [5184/225000 (2%)] Loss: 18738.158203\n",
      "Train Epoch: 185 [7680/225000 (3%)] Loss: 18992.974609\n",
      "Train Epoch: 185 [10176/225000 (5%)] Loss: 19116.738281\n",
      "Train Epoch: 185 [12672/225000 (6%)] Loss: 19209.996094\n",
      "Train Epoch: 185 [15168/225000 (7%)] Loss: 19017.294922\n",
      "Train Epoch: 185 [17664/225000 (8%)] Loss: 18977.726562\n",
      "Train Epoch: 185 [20160/225000 (9%)] Loss: 18928.158203\n",
      "Train Epoch: 185 [22656/225000 (10%)] Loss: 18943.330078\n",
      "Train Epoch: 185 [25152/225000 (11%)] Loss: 19590.734375\n",
      "Train Epoch: 185 [27648/225000 (12%)] Loss: 18988.455078\n",
      "Train Epoch: 185 [30144/225000 (13%)] Loss: 19397.248047\n",
      "Train Epoch: 185 [32640/225000 (15%)] Loss: 18565.406250\n",
      "Train Epoch: 185 [35136/225000 (16%)] Loss: 18944.232422\n",
      "Train Epoch: 185 [37632/225000 (17%)] Loss: 19294.632812\n",
      "Train Epoch: 185 [40128/225000 (18%)] Loss: 19045.888672\n",
      "Train Epoch: 185 [42624/225000 (19%)] Loss: 19585.542969\n",
      "Train Epoch: 185 [45120/225000 (20%)] Loss: 18796.468750\n",
      "Train Epoch: 185 [47616/225000 (21%)] Loss: 18888.433594\n",
      "Train Epoch: 185 [50112/225000 (22%)] Loss: 18752.613281\n",
      "Train Epoch: 185 [52608/225000 (23%)] Loss: 18515.007812\n",
      "Train Epoch: 185 [55104/225000 (24%)] Loss: 19317.671875\n",
      "Train Epoch: 185 [57600/225000 (26%)] Loss: 19133.281250\n",
      "Train Epoch: 185 [60096/225000 (27%)] Loss: 19373.603516\n",
      "Train Epoch: 185 [62592/225000 (28%)] Loss: 19491.636719\n",
      "Train Epoch: 185 [65088/225000 (29%)] Loss: 18978.392578\n",
      "Train Epoch: 185 [67584/225000 (30%)] Loss: 19035.644531\n",
      "Train Epoch: 185 [70080/225000 (31%)] Loss: 18976.765625\n",
      "Train Epoch: 185 [72576/225000 (32%)] Loss: 19191.035156\n",
      "Train Epoch: 185 [75072/225000 (33%)] Loss: 18586.578125\n",
      "Train Epoch: 185 [77568/225000 (34%)] Loss: 18731.042969\n",
      "Train Epoch: 185 [80064/225000 (36%)] Loss: 18984.734375\n",
      "Train Epoch: 185 [82560/225000 (37%)] Loss: 18783.750000\n",
      "Train Epoch: 185 [85056/225000 (38%)] Loss: 18603.082031\n",
      "Train Epoch: 185 [87552/225000 (39%)] Loss: 19348.296875\n",
      "Train Epoch: 185 [90048/225000 (40%)] Loss: 18838.941406\n",
      "Train Epoch: 185 [92544/225000 (41%)] Loss: 18852.371094\n",
      "Train Epoch: 185 [95040/225000 (42%)] Loss: 19277.355469\n",
      "Train Epoch: 185 [97536/225000 (43%)] Loss: 18931.386719\n",
      "Train Epoch: 185 [100032/225000 (44%)] Loss: 19121.718750\n",
      "Train Epoch: 185 [102528/225000 (46%)] Loss: 18908.195312\n",
      "Train Epoch: 185 [105024/225000 (47%)] Loss: 18943.363281\n",
      "Train Epoch: 185 [107520/225000 (48%)] Loss: 18781.851562\n",
      "Train Epoch: 185 [110016/225000 (49%)] Loss: 19239.812500\n",
      "Train Epoch: 185 [112512/225000 (50%)] Loss: 19158.585938\n",
      "Train Epoch: 185 [115008/225000 (51%)] Loss: 19058.316406\n",
      "Train Epoch: 185 [117504/225000 (52%)] Loss: 19256.621094\n",
      "Train Epoch: 185 [120000/225000 (53%)] Loss: 19160.587891\n",
      "Train Epoch: 185 [122496/225000 (54%)] Loss: 19167.281250\n",
      "Train Epoch: 185 [124992/225000 (56%)] Loss: 19185.757812\n",
      "Train Epoch: 185 [127488/225000 (57%)] Loss: 19364.912109\n",
      "Train Epoch: 185 [129984/225000 (58%)] Loss: 18916.093750\n",
      "Train Epoch: 185 [132480/225000 (59%)] Loss: 18590.714844\n",
      "Train Epoch: 185 [134976/225000 (60%)] Loss: 19094.753906\n",
      "Train Epoch: 185 [137472/225000 (61%)] Loss: 19051.425781\n",
      "Train Epoch: 185 [139968/225000 (62%)] Loss: 18853.058594\n",
      "Train Epoch: 185 [142464/225000 (63%)] Loss: 18995.355469\n",
      "Train Epoch: 185 [144960/225000 (64%)] Loss: 19297.810547\n",
      "Train Epoch: 185 [147456/225000 (66%)] Loss: 19330.193359\n",
      "Train Epoch: 185 [149952/225000 (67%)] Loss: 19270.275391\n",
      "Train Epoch: 185 [152448/225000 (68%)] Loss: 19310.310547\n",
      "Train Epoch: 185 [154944/225000 (69%)] Loss: 19119.371094\n",
      "Train Epoch: 185 [157440/225000 (70%)] Loss: 19170.800781\n",
      "Train Epoch: 185 [159936/225000 (71%)] Loss: 18871.978516\n",
      "Train Epoch: 185 [162432/225000 (72%)] Loss: 19114.871094\n",
      "Train Epoch: 185 [164928/225000 (73%)] Loss: 18862.386719\n",
      "Train Epoch: 185 [167424/225000 (74%)] Loss: 18632.933594\n",
      "Train Epoch: 185 [169920/225000 (76%)] Loss: 19184.261719\n",
      "Train Epoch: 185 [172416/225000 (77%)] Loss: 19288.587891\n",
      "Train Epoch: 185 [174912/225000 (78%)] Loss: 18982.798828\n",
      "Train Epoch: 185 [177408/225000 (79%)] Loss: 18763.453125\n",
      "Train Epoch: 185 [179904/225000 (80%)] Loss: 19088.167969\n",
      "Train Epoch: 185 [182400/225000 (81%)] Loss: 18835.996094\n",
      "Train Epoch: 185 [184896/225000 (82%)] Loss: 18956.121094\n",
      "Train Epoch: 185 [187392/225000 (83%)] Loss: 18450.814453\n",
      "Train Epoch: 185 [189888/225000 (84%)] Loss: 19299.984375\n",
      "Train Epoch: 185 [192384/225000 (86%)] Loss: 19215.193359\n",
      "Train Epoch: 185 [194880/225000 (87%)] Loss: 18595.767578\n",
      "Train Epoch: 185 [197376/225000 (88%)] Loss: 18715.505859\n",
      "Train Epoch: 185 [199872/225000 (89%)] Loss: 18813.171875\n",
      "Train Epoch: 185 [202368/225000 (90%)] Loss: 19149.097656\n",
      "Train Epoch: 185 [204864/225000 (91%)] Loss: 18824.052734\n",
      "Train Epoch: 185 [207360/225000 (92%)] Loss: 18725.066406\n",
      "Train Epoch: 185 [209856/225000 (93%)] Loss: 19122.050781\n",
      "Train Epoch: 185 [212352/225000 (94%)] Loss: 18969.058594\n",
      "Train Epoch: 185 [214848/225000 (95%)] Loss: 18413.728516\n",
      "Train Epoch: 185 [217344/225000 (97%)] Loss: 18624.103516\n",
      "Train Epoch: 185 [219840/225000 (98%)] Loss: 19205.355469\n",
      "Train Epoch: 185 [222336/225000 (99%)] Loss: 19263.292969\n",
      "Train Epoch: 185 [224832/225000 (100%)] Loss: 18901.941406\n",
      "    epoch          : 185\n",
      "    loss           : 19044.414802421074\n",
      "    val_loss       : 18943.347220420383\n",
      "Train Epoch: 186 [192/225000 (0%)] Loss: 19120.759766\n",
      "Train Epoch: 186 [2688/225000 (1%)] Loss: 19081.470703\n",
      "Train Epoch: 186 [5184/225000 (2%)] Loss: 19334.531250\n",
      "Train Epoch: 186 [7680/225000 (3%)] Loss: 19289.003906\n",
      "Train Epoch: 186 [10176/225000 (5%)] Loss: 19024.841797\n",
      "Train Epoch: 186 [12672/225000 (6%)] Loss: 19189.966797\n",
      "Train Epoch: 186 [15168/225000 (7%)] Loss: 18855.976562\n",
      "Train Epoch: 186 [17664/225000 (8%)] Loss: 19128.738281\n",
      "Train Epoch: 186 [20160/225000 (9%)] Loss: 18349.703125\n",
      "Train Epoch: 186 [22656/225000 (10%)] Loss: 19032.013672\n",
      "Train Epoch: 186 [25152/225000 (11%)] Loss: 19200.904297\n",
      "Train Epoch: 186 [27648/225000 (12%)] Loss: 19129.501953\n",
      "Train Epoch: 186 [30144/225000 (13%)] Loss: 18698.488281\n",
      "Train Epoch: 186 [32640/225000 (15%)] Loss: 18859.035156\n",
      "Train Epoch: 186 [35136/225000 (16%)] Loss: 19123.574219\n",
      "Train Epoch: 186 [37632/225000 (17%)] Loss: 19286.628906\n",
      "Train Epoch: 186 [40128/225000 (18%)] Loss: 19600.175781\n",
      "Train Epoch: 186 [42624/225000 (19%)] Loss: 19178.345703\n",
      "Train Epoch: 186 [45120/225000 (20%)] Loss: 18828.732422\n",
      "Train Epoch: 186 [47616/225000 (21%)] Loss: 18636.601562\n",
      "Train Epoch: 186 [50112/225000 (22%)] Loss: 18915.433594\n",
      "Train Epoch: 186 [52608/225000 (23%)] Loss: 19556.384766\n",
      "Train Epoch: 186 [55104/225000 (24%)] Loss: 18958.125000\n",
      "Train Epoch: 186 [57600/225000 (26%)] Loss: 18947.978516\n",
      "Train Epoch: 186 [60096/225000 (27%)] Loss: 19248.074219\n",
      "Train Epoch: 186 [62592/225000 (28%)] Loss: 18740.476562\n",
      "Train Epoch: 186 [65088/225000 (29%)] Loss: 18705.730469\n",
      "Train Epoch: 186 [67584/225000 (30%)] Loss: 18951.218750\n",
      "Train Epoch: 186 [70080/225000 (31%)] Loss: 18869.968750\n",
      "Train Epoch: 186 [72576/225000 (32%)] Loss: 18250.949219\n",
      "Train Epoch: 186 [75072/225000 (33%)] Loss: 18937.867188\n",
      "Train Epoch: 186 [77568/225000 (34%)] Loss: 19385.144531\n",
      "Train Epoch: 186 [80064/225000 (36%)] Loss: 18956.751953\n",
      "Train Epoch: 186 [82560/225000 (37%)] Loss: 19575.605469\n",
      "Train Epoch: 186 [85056/225000 (38%)] Loss: 19133.707031\n",
      "Train Epoch: 186 [87552/225000 (39%)] Loss: 19062.996094\n",
      "Train Epoch: 186 [90048/225000 (40%)] Loss: 19010.617188\n",
      "Train Epoch: 186 [92544/225000 (41%)] Loss: 18958.199219\n",
      "Train Epoch: 186 [95040/225000 (42%)] Loss: 18714.183594\n",
      "Train Epoch: 186 [97536/225000 (43%)] Loss: 19004.949219\n",
      "Train Epoch: 186 [100032/225000 (44%)] Loss: 18950.544922\n",
      "Train Epoch: 186 [102528/225000 (46%)] Loss: 19226.308594\n",
      "Train Epoch: 186 [105024/225000 (47%)] Loss: 19368.343750\n",
      "Train Epoch: 186 [107520/225000 (48%)] Loss: 18823.449219\n",
      "Train Epoch: 186 [110016/225000 (49%)] Loss: 19198.783203\n",
      "Train Epoch: 186 [112512/225000 (50%)] Loss: 19328.558594\n",
      "Train Epoch: 186 [115008/225000 (51%)] Loss: 19084.300781\n",
      "Train Epoch: 186 [117504/225000 (52%)] Loss: 19101.837891\n",
      "Train Epoch: 186 [120000/225000 (53%)] Loss: 18769.468750\n",
      "Train Epoch: 186 [122496/225000 (54%)] Loss: 18643.806641\n",
      "Train Epoch: 186 [124992/225000 (56%)] Loss: 18957.703125\n",
      "Train Epoch: 186 [127488/225000 (57%)] Loss: 19158.158203\n",
      "Train Epoch: 186 [129984/225000 (58%)] Loss: 18740.070312\n",
      "Train Epoch: 186 [132480/225000 (59%)] Loss: 19069.796875\n",
      "Train Epoch: 186 [134976/225000 (60%)] Loss: 18944.416016\n",
      "Train Epoch: 186 [137472/225000 (61%)] Loss: 18536.605469\n",
      "Train Epoch: 186 [139968/225000 (62%)] Loss: 19188.837891\n",
      "Train Epoch: 186 [142464/225000 (63%)] Loss: 19070.652344\n",
      "Train Epoch: 186 [144960/225000 (64%)] Loss: 18907.406250\n",
      "Train Epoch: 186 [147456/225000 (66%)] Loss: 18962.406250\n",
      "Train Epoch: 186 [149952/225000 (67%)] Loss: 19114.722656\n",
      "Train Epoch: 186 [152448/225000 (68%)] Loss: 19501.792969\n",
      "Train Epoch: 186 [154944/225000 (69%)] Loss: 19229.777344\n",
      "Train Epoch: 186 [157440/225000 (70%)] Loss: 18714.095703\n",
      "Train Epoch: 186 [159936/225000 (71%)] Loss: 18909.511719\n",
      "Train Epoch: 186 [162432/225000 (72%)] Loss: 18995.009766\n",
      "Train Epoch: 186 [164928/225000 (73%)] Loss: 19120.380859\n",
      "Train Epoch: 186 [167424/225000 (74%)] Loss: 19202.171875\n",
      "Train Epoch: 186 [169920/225000 (76%)] Loss: 18979.189453\n",
      "Train Epoch: 186 [172416/225000 (77%)] Loss: 18401.794922\n",
      "Train Epoch: 186 [174912/225000 (78%)] Loss: 19110.324219\n",
      "Train Epoch: 186 [177408/225000 (79%)] Loss: 18844.078125\n",
      "Train Epoch: 186 [179904/225000 (80%)] Loss: 18687.890625\n",
      "Train Epoch: 186 [182400/225000 (81%)] Loss: 18865.226562\n",
      "Train Epoch: 186 [184896/225000 (82%)] Loss: 18827.220703\n",
      "Train Epoch: 186 [187392/225000 (83%)] Loss: 18982.910156\n",
      "Train Epoch: 186 [189888/225000 (84%)] Loss: 19084.707031\n",
      "Train Epoch: 186 [192384/225000 (86%)] Loss: 18920.292969\n",
      "Train Epoch: 186 [194880/225000 (87%)] Loss: 18718.433594\n",
      "Train Epoch: 186 [197376/225000 (88%)] Loss: 19477.626953\n",
      "Train Epoch: 186 [199872/225000 (89%)] Loss: 19364.050781\n",
      "Train Epoch: 186 [202368/225000 (90%)] Loss: 19533.753906\n",
      "Train Epoch: 186 [204864/225000 (91%)] Loss: 19251.708984\n",
      "Train Epoch: 186 [207360/225000 (92%)] Loss: 18724.824219\n",
      "Train Epoch: 186 [209856/225000 (93%)] Loss: 19195.714844\n",
      "Train Epoch: 186 [212352/225000 (94%)] Loss: 18874.156250\n",
      "Train Epoch: 186 [214848/225000 (95%)] Loss: 19287.734375\n",
      "Train Epoch: 186 [217344/225000 (97%)] Loss: 19286.765625\n",
      "Train Epoch: 186 [219840/225000 (98%)] Loss: 18524.984375\n",
      "Train Epoch: 186 [222336/225000 (99%)] Loss: 19164.476562\n",
      "Train Epoch: 186 [224832/225000 (100%)] Loss: 18830.515625\n",
      "    epoch          : 186\n",
      "    loss           : 19030.048071539037\n",
      "    val_loss       : 18964.189335028634\n",
      "Train Epoch: 187 [192/225000 (0%)] Loss: 19155.011719\n",
      "Train Epoch: 187 [2688/225000 (1%)] Loss: 19198.199219\n",
      "Train Epoch: 187 [5184/225000 (2%)] Loss: 19089.054688\n",
      "Train Epoch: 187 [7680/225000 (3%)] Loss: 19175.554688\n",
      "Train Epoch: 187 [10176/225000 (5%)] Loss: 18878.050781\n",
      "Train Epoch: 187 [12672/225000 (6%)] Loss: 18922.312500\n",
      "Train Epoch: 187 [15168/225000 (7%)] Loss: 18924.359375\n",
      "Train Epoch: 187 [17664/225000 (8%)] Loss: 24092.326172\n",
      "Train Epoch: 187 [20160/225000 (9%)] Loss: 19205.089844\n",
      "Train Epoch: 187 [22656/225000 (10%)] Loss: 18887.232422\n",
      "Train Epoch: 187 [25152/225000 (11%)] Loss: 19036.222656\n",
      "Train Epoch: 187 [27648/225000 (12%)] Loss: 18917.904297\n",
      "Train Epoch: 187 [30144/225000 (13%)] Loss: 19260.232422\n",
      "Train Epoch: 187 [32640/225000 (15%)] Loss: 18815.917969\n",
      "Train Epoch: 187 [35136/225000 (16%)] Loss: 18723.386719\n",
      "Train Epoch: 187 [37632/225000 (17%)] Loss: 18900.951172\n",
      "Train Epoch: 187 [40128/225000 (18%)] Loss: 18717.128906\n",
      "Train Epoch: 187 [42624/225000 (19%)] Loss: 19011.449219\n",
      "Train Epoch: 187 [45120/225000 (20%)] Loss: 18428.689453\n",
      "Train Epoch: 187 [47616/225000 (21%)] Loss: 19182.035156\n",
      "Train Epoch: 187 [50112/225000 (22%)] Loss: 19073.554688\n",
      "Train Epoch: 187 [52608/225000 (23%)] Loss: 19404.416016\n",
      "Train Epoch: 187 [55104/225000 (24%)] Loss: 18872.597656\n",
      "Train Epoch: 187 [57600/225000 (26%)] Loss: 19081.238281\n",
      "Train Epoch: 187 [60096/225000 (27%)] Loss: 18915.992188\n",
      "Train Epoch: 187 [62592/225000 (28%)] Loss: 18759.218750\n",
      "Train Epoch: 187 [65088/225000 (29%)] Loss: 18927.164062\n",
      "Train Epoch: 187 [67584/225000 (30%)] Loss: 18955.113281\n",
      "Train Epoch: 187 [70080/225000 (31%)] Loss: 19459.324219\n",
      "Train Epoch: 187 [72576/225000 (32%)] Loss: 18843.308594\n",
      "Train Epoch: 187 [75072/225000 (33%)] Loss: 19153.113281\n",
      "Train Epoch: 187 [77568/225000 (34%)] Loss: 18966.296875\n",
      "Train Epoch: 187 [80064/225000 (36%)] Loss: 19143.941406\n",
      "Train Epoch: 187 [82560/225000 (37%)] Loss: 19019.791016\n",
      "Train Epoch: 187 [85056/225000 (38%)] Loss: 19127.476562\n",
      "Train Epoch: 187 [87552/225000 (39%)] Loss: 18608.248047\n",
      "Train Epoch: 187 [90048/225000 (40%)] Loss: 19768.039062\n",
      "Train Epoch: 187 [92544/225000 (41%)] Loss: 19246.871094\n",
      "Train Epoch: 187 [95040/225000 (42%)] Loss: 19138.027344\n",
      "Train Epoch: 187 [97536/225000 (43%)] Loss: 18327.550781\n",
      "Train Epoch: 187 [100032/225000 (44%)] Loss: 19170.085938\n",
      "Train Epoch: 187 [102528/225000 (46%)] Loss: 18868.859375\n",
      "Train Epoch: 187 [105024/225000 (47%)] Loss: 19152.398438\n",
      "Train Epoch: 187 [107520/225000 (48%)] Loss: 19325.363281\n",
      "Train Epoch: 187 [110016/225000 (49%)] Loss: 19244.238281\n",
      "Train Epoch: 187 [112512/225000 (50%)] Loss: 19164.740234\n",
      "Train Epoch: 187 [115008/225000 (51%)] Loss: 19107.750000\n",
      "Train Epoch: 187 [117504/225000 (52%)] Loss: 19294.619141\n",
      "Train Epoch: 187 [120000/225000 (53%)] Loss: 18946.982422\n",
      "Train Epoch: 187 [122496/225000 (54%)] Loss: 19005.970703\n",
      "Train Epoch: 187 [124992/225000 (56%)] Loss: 19172.343750\n",
      "Train Epoch: 187 [127488/225000 (57%)] Loss: 19033.953125\n",
      "Train Epoch: 187 [129984/225000 (58%)] Loss: 18968.812500\n",
      "Train Epoch: 187 [132480/225000 (59%)] Loss: 19378.281250\n",
      "Train Epoch: 187 [134976/225000 (60%)] Loss: 18925.474609\n",
      "Train Epoch: 187 [137472/225000 (61%)] Loss: 18874.933594\n",
      "Train Epoch: 187 [139968/225000 (62%)] Loss: 19159.554688\n",
      "Train Epoch: 187 [142464/225000 (63%)] Loss: 18741.058594\n",
      "Train Epoch: 187 [144960/225000 (64%)] Loss: 18836.820312\n",
      "Train Epoch: 187 [147456/225000 (66%)] Loss: 19346.960938\n",
      "Train Epoch: 187 [149952/225000 (67%)] Loss: 18970.175781\n",
      "Train Epoch: 187 [152448/225000 (68%)] Loss: 19123.556641\n",
      "Train Epoch: 187 [154944/225000 (69%)] Loss: 19142.019531\n",
      "Train Epoch: 187 [157440/225000 (70%)] Loss: 18979.582031\n",
      "Train Epoch: 187 [159936/225000 (71%)] Loss: 18881.414062\n",
      "Train Epoch: 187 [162432/225000 (72%)] Loss: 18978.554688\n",
      "Train Epoch: 187 [164928/225000 (73%)] Loss: 19279.472656\n",
      "Train Epoch: 187 [167424/225000 (74%)] Loss: 19083.117188\n",
      "Train Epoch: 187 [169920/225000 (76%)] Loss: 18993.929688\n",
      "Train Epoch: 187 [172416/225000 (77%)] Loss: 18522.628906\n",
      "Train Epoch: 187 [174912/225000 (78%)] Loss: 19169.875000\n",
      "Train Epoch: 187 [177408/225000 (79%)] Loss: 19061.531250\n",
      "Train Epoch: 187 [179904/225000 (80%)] Loss: 18774.945312\n",
      "Train Epoch: 187 [182400/225000 (81%)] Loss: 19271.601562\n",
      "Train Epoch: 187 [184896/225000 (82%)] Loss: 19224.570312\n",
      "Train Epoch: 187 [187392/225000 (83%)] Loss: 19216.755859\n",
      "Train Epoch: 187 [189888/225000 (84%)] Loss: 18428.062500\n",
      "Train Epoch: 187 [192384/225000 (86%)] Loss: 18899.468750\n",
      "Train Epoch: 187 [194880/225000 (87%)] Loss: 19089.630859\n",
      "Train Epoch: 187 [197376/225000 (88%)] Loss: 19509.300781\n",
      "Train Epoch: 187 [199872/225000 (89%)] Loss: 18830.322266\n",
      "Train Epoch: 187 [202368/225000 (90%)] Loss: 19359.398438\n",
      "Train Epoch: 187 [204864/225000 (91%)] Loss: 19172.328125\n",
      "Train Epoch: 187 [207360/225000 (92%)] Loss: 18745.607422\n",
      "Train Epoch: 187 [209856/225000 (93%)] Loss: 18376.207031\n",
      "Train Epoch: 187 [212352/225000 (94%)] Loss: 19066.263672\n",
      "Train Epoch: 187 [214848/225000 (95%)] Loss: 19214.636719\n",
      "Train Epoch: 187 [217344/225000 (97%)] Loss: 19100.917969\n",
      "Train Epoch: 187 [219840/225000 (98%)] Loss: 18953.945312\n",
      "Train Epoch: 187 [222336/225000 (99%)] Loss: 18904.503906\n",
      "Train Epoch: 187 [224832/225000 (100%)] Loss: 18650.972656\n",
      "    epoch          : 187\n",
      "    loss           : 19029.136738747868\n",
      "    val_loss       : 18954.07008118029\n",
      "Train Epoch: 188 [192/225000 (0%)] Loss: 18731.207031\n",
      "Train Epoch: 188 [2688/225000 (1%)] Loss: 19066.816406\n",
      "Train Epoch: 188 [5184/225000 (2%)] Loss: 18765.246094\n",
      "Train Epoch: 188 [7680/225000 (3%)] Loss: 18685.644531\n",
      "Train Epoch: 188 [10176/225000 (5%)] Loss: 18980.113281\n",
      "Train Epoch: 188 [12672/225000 (6%)] Loss: 19269.292969\n",
      "Train Epoch: 188 [15168/225000 (7%)] Loss: 19147.197266\n",
      "Train Epoch: 188 [17664/225000 (8%)] Loss: 18969.869141\n",
      "Train Epoch: 188 [20160/225000 (9%)] Loss: 19620.921875\n",
      "Train Epoch: 188 [22656/225000 (10%)] Loss: 19046.863281\n",
      "Train Epoch: 188 [25152/225000 (11%)] Loss: 19076.623047\n",
      "Train Epoch: 188 [27648/225000 (12%)] Loss: 18509.554688\n",
      "Train Epoch: 188 [30144/225000 (13%)] Loss: 19196.707031\n",
      "Train Epoch: 188 [32640/225000 (15%)] Loss: 19415.181641\n",
      "Train Epoch: 188 [35136/225000 (16%)] Loss: 19336.734375\n",
      "Train Epoch: 188 [37632/225000 (17%)] Loss: 18604.308594\n",
      "Train Epoch: 188 [40128/225000 (18%)] Loss: 18488.613281\n",
      "Train Epoch: 188 [42624/225000 (19%)] Loss: 18997.466797\n",
      "Train Epoch: 188 [45120/225000 (20%)] Loss: 18580.580078\n",
      "Train Epoch: 188 [47616/225000 (21%)] Loss: 19146.087891\n",
      "Train Epoch: 188 [50112/225000 (22%)] Loss: 18740.820312\n",
      "Train Epoch: 188 [52608/225000 (23%)] Loss: 19056.660156\n",
      "Train Epoch: 188 [55104/225000 (24%)] Loss: 19282.300781\n",
      "Train Epoch: 188 [57600/225000 (26%)] Loss: 19042.601562\n",
      "Train Epoch: 188 [60096/225000 (27%)] Loss: 19186.542969\n",
      "Train Epoch: 188 [62592/225000 (28%)] Loss: 19151.023438\n",
      "Train Epoch: 188 [65088/225000 (29%)] Loss: 19132.164062\n",
      "Train Epoch: 188 [67584/225000 (30%)] Loss: 19008.179688\n",
      "Train Epoch: 188 [70080/225000 (31%)] Loss: 19015.312500\n",
      "Train Epoch: 188 [72576/225000 (32%)] Loss: 18997.691406\n",
      "Train Epoch: 188 [75072/225000 (33%)] Loss: 18409.746094\n",
      "Train Epoch: 188 [77568/225000 (34%)] Loss: 18706.578125\n",
      "Train Epoch: 188 [80064/225000 (36%)] Loss: 18554.191406\n",
      "Train Epoch: 188 [82560/225000 (37%)] Loss: 19263.113281\n",
      "Train Epoch: 188 [85056/225000 (38%)] Loss: 19017.273438\n",
      "Train Epoch: 188 [87552/225000 (39%)] Loss: 18227.050781\n",
      "Train Epoch: 188 [90048/225000 (40%)] Loss: 18857.636719\n",
      "Train Epoch: 188 [92544/225000 (41%)] Loss: 18676.718750\n",
      "Train Epoch: 188 [95040/225000 (42%)] Loss: 18672.857422\n",
      "Train Epoch: 188 [97536/225000 (43%)] Loss: 19101.412109\n",
      "Train Epoch: 188 [100032/225000 (44%)] Loss: 18913.101562\n",
      "Train Epoch: 188 [102528/225000 (46%)] Loss: 19140.533203\n",
      "Train Epoch: 188 [105024/225000 (47%)] Loss: 18671.251953\n",
      "Train Epoch: 188 [107520/225000 (48%)] Loss: 19040.250000\n",
      "Train Epoch: 188 [110016/225000 (49%)] Loss: 19251.257812\n",
      "Train Epoch: 188 [112512/225000 (50%)] Loss: 19358.542969\n",
      "Train Epoch: 188 [115008/225000 (51%)] Loss: 19402.615234\n",
      "Train Epoch: 188 [117504/225000 (52%)] Loss: 18926.269531\n",
      "Train Epoch: 188 [120000/225000 (53%)] Loss: 19190.394531\n",
      "Train Epoch: 188 [122496/225000 (54%)] Loss: 19341.992188\n",
      "Train Epoch: 188 [124992/225000 (56%)] Loss: 19175.783203\n",
      "Train Epoch: 188 [127488/225000 (57%)] Loss: 19002.488281\n",
      "Train Epoch: 188 [129984/225000 (58%)] Loss: 19068.539062\n",
      "Train Epoch: 188 [132480/225000 (59%)] Loss: 18439.693359\n",
      "Train Epoch: 188 [134976/225000 (60%)] Loss: 19278.960938\n",
      "Train Epoch: 188 [137472/225000 (61%)] Loss: 18644.000000\n",
      "Train Epoch: 188 [139968/225000 (62%)] Loss: 18746.800781\n",
      "Train Epoch: 188 [142464/225000 (63%)] Loss: 18713.187500\n",
      "Train Epoch: 188 [144960/225000 (64%)] Loss: 19247.841797\n",
      "Train Epoch: 188 [147456/225000 (66%)] Loss: 18876.746094\n",
      "Train Epoch: 188 [149952/225000 (67%)] Loss: 19072.486328\n",
      "Train Epoch: 188 [152448/225000 (68%)] Loss: 19207.125000\n",
      "Train Epoch: 188 [154944/225000 (69%)] Loss: 18919.250000\n",
      "Train Epoch: 188 [157440/225000 (70%)] Loss: 19392.710938\n",
      "Train Epoch: 188 [159936/225000 (71%)] Loss: 18822.312500\n",
      "Train Epoch: 188 [162432/225000 (72%)] Loss: 19117.429688\n",
      "Train Epoch: 188 [164928/225000 (73%)] Loss: 19069.375000\n",
      "Train Epoch: 188 [167424/225000 (74%)] Loss: 18884.046875\n",
      "Train Epoch: 188 [169920/225000 (76%)] Loss: 19336.839844\n",
      "Train Epoch: 188 [172416/225000 (77%)] Loss: 18671.492188\n",
      "Train Epoch: 188 [174912/225000 (78%)] Loss: 19411.382812\n",
      "Train Epoch: 188 [177408/225000 (79%)] Loss: 18883.689453\n",
      "Train Epoch: 188 [179904/225000 (80%)] Loss: 19253.246094\n",
      "Train Epoch: 188 [182400/225000 (81%)] Loss: 19059.031250\n",
      "Train Epoch: 188 [184896/225000 (82%)] Loss: 19135.425781\n",
      "Train Epoch: 188 [187392/225000 (83%)] Loss: 19325.710938\n",
      "Train Epoch: 188 [189888/225000 (84%)] Loss: 19061.833984\n",
      "Train Epoch: 188 [192384/225000 (86%)] Loss: 19032.224609\n",
      "Train Epoch: 188 [194880/225000 (87%)] Loss: 18903.271484\n",
      "Train Epoch: 188 [197376/225000 (88%)] Loss: 19419.210938\n",
      "Train Epoch: 188 [199872/225000 (89%)] Loss: 19400.906250\n",
      "Train Epoch: 188 [202368/225000 (90%)] Loss: 19043.128906\n",
      "Train Epoch: 188 [204864/225000 (91%)] Loss: 19140.402344\n",
      "Train Epoch: 188 [207360/225000 (92%)] Loss: 18746.494141\n",
      "Train Epoch: 188 [209856/225000 (93%)] Loss: 18718.070312\n",
      "Train Epoch: 188 [212352/225000 (94%)] Loss: 18982.412109\n",
      "Train Epoch: 188 [214848/225000 (95%)] Loss: 18977.687500\n",
      "Train Epoch: 188 [217344/225000 (97%)] Loss: 19239.933594\n",
      "Train Epoch: 188 [219840/225000 (98%)] Loss: 18724.597656\n",
      "Train Epoch: 188 [222336/225000 (99%)] Loss: 18765.857422\n",
      "Train Epoch: 188 [224832/225000 (100%)] Loss: 18765.250000\n",
      "    epoch          : 188\n",
      "    loss           : 19013.62665898971\n",
      "    val_loss       : 18917.968311406275\n",
      "Train Epoch: 189 [192/225000 (0%)] Loss: 18870.091797\n",
      "Train Epoch: 189 [2688/225000 (1%)] Loss: 18446.957031\n",
      "Train Epoch: 189 [5184/225000 (2%)] Loss: 18919.333984\n",
      "Train Epoch: 189 [7680/225000 (3%)] Loss: 19349.734375\n",
      "Train Epoch: 189 [10176/225000 (5%)] Loss: 19206.707031\n",
      "Train Epoch: 189 [12672/225000 (6%)] Loss: 18917.595703\n",
      "Train Epoch: 189 [15168/225000 (7%)] Loss: 19541.250000\n",
      "Train Epoch: 189 [17664/225000 (8%)] Loss: 18816.888672\n",
      "Train Epoch: 189 [20160/225000 (9%)] Loss: 18743.562500\n",
      "Train Epoch: 189 [22656/225000 (10%)] Loss: 19129.347656\n",
      "Train Epoch: 189 [25152/225000 (11%)] Loss: 18520.638672\n",
      "Train Epoch: 189 [27648/225000 (12%)] Loss: 19125.962891\n",
      "Train Epoch: 189 [30144/225000 (13%)] Loss: 19116.453125\n",
      "Train Epoch: 189 [32640/225000 (15%)] Loss: 19227.507812\n",
      "Train Epoch: 189 [35136/225000 (16%)] Loss: 18988.039062\n",
      "Train Epoch: 189 [37632/225000 (17%)] Loss: 19034.890625\n",
      "Train Epoch: 189 [40128/225000 (18%)] Loss: 18914.371094\n",
      "Train Epoch: 189 [42624/225000 (19%)] Loss: 19247.265625\n",
      "Train Epoch: 189 [45120/225000 (20%)] Loss: 19099.015625\n",
      "Train Epoch: 189 [47616/225000 (21%)] Loss: 18764.070312\n",
      "Train Epoch: 189 [50112/225000 (22%)] Loss: 19251.082031\n",
      "Train Epoch: 189 [52608/225000 (23%)] Loss: 18946.675781\n",
      "Train Epoch: 189 [55104/225000 (24%)] Loss: 19088.371094\n",
      "Train Epoch: 189 [57600/225000 (26%)] Loss: 19013.035156\n",
      "Train Epoch: 189 [60096/225000 (27%)] Loss: 19372.384766\n",
      "Train Epoch: 189 [62592/225000 (28%)] Loss: 19016.523438\n",
      "Train Epoch: 189 [65088/225000 (29%)] Loss: 18730.273438\n",
      "Train Epoch: 189 [67584/225000 (30%)] Loss: 19068.890625\n",
      "Train Epoch: 189 [70080/225000 (31%)] Loss: 19064.253906\n",
      "Train Epoch: 189 [72576/225000 (32%)] Loss: 19123.568359\n",
      "Train Epoch: 189 [75072/225000 (33%)] Loss: 18982.539062\n",
      "Train Epoch: 189 [77568/225000 (34%)] Loss: 19182.562500\n",
      "Train Epoch: 189 [80064/225000 (36%)] Loss: 18573.253906\n",
      "Train Epoch: 189 [82560/225000 (37%)] Loss: 19191.406250\n",
      "Train Epoch: 189 [85056/225000 (38%)] Loss: 19168.976562\n",
      "Train Epoch: 189 [87552/225000 (39%)] Loss: 19574.808594\n",
      "Train Epoch: 189 [90048/225000 (40%)] Loss: 19175.519531\n",
      "Train Epoch: 189 [92544/225000 (41%)] Loss: 18893.019531\n",
      "Train Epoch: 189 [95040/225000 (42%)] Loss: 19065.941406\n",
      "Train Epoch: 189 [97536/225000 (43%)] Loss: 19359.164062\n",
      "Train Epoch: 189 [100032/225000 (44%)] Loss: 19728.974609\n",
      "Train Epoch: 189 [102528/225000 (46%)] Loss: 18540.621094\n",
      "Train Epoch: 189 [105024/225000 (47%)] Loss: 19494.515625\n",
      "Train Epoch: 189 [107520/225000 (48%)] Loss: 18827.949219\n",
      "Train Epoch: 189 [110016/225000 (49%)] Loss: 19247.574219\n",
      "Train Epoch: 189 [112512/225000 (50%)] Loss: 19282.695312\n",
      "Train Epoch: 189 [115008/225000 (51%)] Loss: 19077.621094\n",
      "Train Epoch: 189 [117504/225000 (52%)] Loss: 19152.640625\n",
      "Train Epoch: 189 [120000/225000 (53%)] Loss: 19266.691406\n",
      "Train Epoch: 189 [122496/225000 (54%)] Loss: 19002.785156\n",
      "Train Epoch: 189 [124992/225000 (56%)] Loss: 19352.425781\n",
      "Train Epoch: 189 [127488/225000 (57%)] Loss: 19490.664062\n",
      "Train Epoch: 189 [129984/225000 (58%)] Loss: 19064.910156\n",
      "Train Epoch: 189 [132480/225000 (59%)] Loss: 18783.554688\n",
      "Train Epoch: 189 [134976/225000 (60%)] Loss: 18914.425781\n",
      "Train Epoch: 189 [137472/225000 (61%)] Loss: 19187.511719\n",
      "Train Epoch: 189 [139968/225000 (62%)] Loss: 19181.361328\n",
      "Train Epoch: 189 [142464/225000 (63%)] Loss: 19288.851562\n",
      "Train Epoch: 189 [144960/225000 (64%)] Loss: 18324.214844\n",
      "Train Epoch: 189 [147456/225000 (66%)] Loss: 18830.244141\n",
      "Train Epoch: 189 [149952/225000 (67%)] Loss: 19282.335938\n",
      "Train Epoch: 189 [152448/225000 (68%)] Loss: 18881.175781\n",
      "Train Epoch: 189 [154944/225000 (69%)] Loss: 18468.347656\n",
      "Train Epoch: 189 [157440/225000 (70%)] Loss: 19652.304688\n",
      "Train Epoch: 189 [159936/225000 (71%)] Loss: 18942.203125\n",
      "Train Epoch: 189 [162432/225000 (72%)] Loss: 18966.789062\n",
      "Train Epoch: 189 [164928/225000 (73%)] Loss: 18869.039062\n",
      "Train Epoch: 189 [167424/225000 (74%)] Loss: 18901.666016\n",
      "Train Epoch: 189 [169920/225000 (76%)] Loss: 19118.125000\n",
      "Train Epoch: 189 [172416/225000 (77%)] Loss: 18984.753906\n",
      "Train Epoch: 189 [174912/225000 (78%)] Loss: 18912.658203\n",
      "Train Epoch: 189 [177408/225000 (79%)] Loss: 19463.273438\n",
      "Train Epoch: 189 [179904/225000 (80%)] Loss: 18550.820312\n",
      "Train Epoch: 189 [182400/225000 (81%)] Loss: 18785.771484\n",
      "Train Epoch: 189 [184896/225000 (82%)] Loss: 19032.460938\n",
      "Train Epoch: 189 [187392/225000 (83%)] Loss: 18583.906250\n",
      "Train Epoch: 189 [189888/225000 (84%)] Loss: 18352.257812\n",
      "Train Epoch: 189 [192384/225000 (86%)] Loss: 19264.769531\n",
      "Train Epoch: 189 [194880/225000 (87%)] Loss: 18851.984375\n",
      "Train Epoch: 189 [197376/225000 (88%)] Loss: 18883.734375\n",
      "Train Epoch: 189 [199872/225000 (89%)] Loss: 18976.906250\n",
      "Train Epoch: 189 [202368/225000 (90%)] Loss: 19339.970703\n",
      "Train Epoch: 189 [204864/225000 (91%)] Loss: 18721.062500\n",
      "Train Epoch: 189 [207360/225000 (92%)] Loss: 18682.460938\n",
      "Train Epoch: 189 [209856/225000 (93%)] Loss: 19358.261719\n",
      "Train Epoch: 189 [212352/225000 (94%)] Loss: 18991.455078\n",
      "Train Epoch: 189 [214848/225000 (95%)] Loss: 19395.421875\n",
      "Train Epoch: 189 [217344/225000 (97%)] Loss: 19284.972656\n",
      "Train Epoch: 189 [219840/225000 (98%)] Loss: 18894.546875\n",
      "Train Epoch: 189 [222336/225000 (99%)] Loss: 18721.933594\n",
      "Train Epoch: 189 [224832/225000 (100%)] Loss: 19699.949219\n",
      "    epoch          : 189\n",
      "    loss           : 18997.57231395318\n",
      "    val_loss       : 18892.923356208183\n",
      "Train Epoch: 190 [192/225000 (0%)] Loss: 19139.197266\n",
      "Train Epoch: 190 [2688/225000 (1%)] Loss: 19227.957031\n",
      "Train Epoch: 190 [5184/225000 (2%)] Loss: 18572.511719\n",
      "Train Epoch: 190 [7680/225000 (3%)] Loss: 18916.781250\n",
      "Train Epoch: 190 [10176/225000 (5%)] Loss: 19326.390625\n",
      "Train Epoch: 190 [12672/225000 (6%)] Loss: 19444.144531\n",
      "Train Epoch: 190 [15168/225000 (7%)] Loss: 18731.009766\n",
      "Train Epoch: 190 [17664/225000 (8%)] Loss: 19339.300781\n",
      "Train Epoch: 190 [20160/225000 (9%)] Loss: 19110.035156\n",
      "Train Epoch: 190 [22656/225000 (10%)] Loss: 18960.480469\n",
      "Train Epoch: 190 [25152/225000 (11%)] Loss: 18601.638672\n",
      "Train Epoch: 190 [27648/225000 (12%)] Loss: 18967.085938\n",
      "Train Epoch: 190 [30144/225000 (13%)] Loss: 18922.960938\n",
      "Train Epoch: 190 [32640/225000 (15%)] Loss: 19383.732422\n",
      "Train Epoch: 190 [35136/225000 (16%)] Loss: 18768.968750\n",
      "Train Epoch: 190 [37632/225000 (17%)] Loss: 18597.621094\n",
      "Train Epoch: 190 [40128/225000 (18%)] Loss: 19470.875000\n",
      "Train Epoch: 190 [42624/225000 (19%)] Loss: 19179.460938\n",
      "Train Epoch: 190 [45120/225000 (20%)] Loss: 18732.007812\n",
      "Train Epoch: 190 [47616/225000 (21%)] Loss: 18857.771484\n",
      "Train Epoch: 190 [50112/225000 (22%)] Loss: 19581.910156\n",
      "Train Epoch: 190 [52608/225000 (23%)] Loss: 19145.468750\n",
      "Train Epoch: 190 [55104/225000 (24%)] Loss: 18744.626953\n",
      "Train Epoch: 190 [57600/225000 (26%)] Loss: 19094.558594\n",
      "Train Epoch: 190 [60096/225000 (27%)] Loss: 19525.500000\n",
      "Train Epoch: 190 [62592/225000 (28%)] Loss: 18440.330078\n",
      "Train Epoch: 190 [65088/225000 (29%)] Loss: 19143.335938\n",
      "Train Epoch: 190 [67584/225000 (30%)] Loss: 18688.796875\n",
      "Train Epoch: 190 [70080/225000 (31%)] Loss: 19195.960938\n",
      "Train Epoch: 190 [72576/225000 (32%)] Loss: 18980.355469\n",
      "Train Epoch: 190 [75072/225000 (33%)] Loss: 19333.896484\n",
      "Train Epoch: 190 [77568/225000 (34%)] Loss: 18970.890625\n",
      "Train Epoch: 190 [80064/225000 (36%)] Loss: 19090.546875\n",
      "Train Epoch: 190 [82560/225000 (37%)] Loss: 18547.167969\n",
      "Train Epoch: 190 [85056/225000 (38%)] Loss: 19385.042969\n",
      "Train Epoch: 190 [87552/225000 (39%)] Loss: 19145.677734\n",
      "Train Epoch: 190 [90048/225000 (40%)] Loss: 18952.097656\n",
      "Train Epoch: 190 [92544/225000 (41%)] Loss: 19125.310547\n",
      "Train Epoch: 190 [95040/225000 (42%)] Loss: 19138.582031\n",
      "Train Epoch: 190 [97536/225000 (43%)] Loss: 18814.234375\n",
      "Train Epoch: 190 [100032/225000 (44%)] Loss: 19447.066406\n",
      "Train Epoch: 190 [102528/225000 (46%)] Loss: 19453.652344\n",
      "Train Epoch: 190 [105024/225000 (47%)] Loss: 19423.402344\n",
      "Train Epoch: 190 [107520/225000 (48%)] Loss: 18839.636719\n",
      "Train Epoch: 190 [110016/225000 (49%)] Loss: 19100.316406\n",
      "Train Epoch: 190 [112512/225000 (50%)] Loss: 18733.515625\n",
      "Train Epoch: 190 [115008/225000 (51%)] Loss: 18759.216797\n",
      "Train Epoch: 190 [117504/225000 (52%)] Loss: 19369.203125\n",
      "Train Epoch: 190 [120000/225000 (53%)] Loss: 19001.863281\n",
      "Train Epoch: 190 [122496/225000 (54%)] Loss: 19144.046875\n",
      "Train Epoch: 190 [124992/225000 (56%)] Loss: 18882.957031\n",
      "Train Epoch: 190 [127488/225000 (57%)] Loss: 18701.585938\n",
      "Train Epoch: 190 [129984/225000 (58%)] Loss: 19272.601562\n",
      "Train Epoch: 190 [132480/225000 (59%)] Loss: 18840.632812\n",
      "Train Epoch: 190 [134976/225000 (60%)] Loss: 19607.982422\n",
      "Train Epoch: 190 [137472/225000 (61%)] Loss: 18918.019531\n",
      "Train Epoch: 190 [139968/225000 (62%)] Loss: 18766.410156\n",
      "Train Epoch: 190 [142464/225000 (63%)] Loss: 19172.156250\n",
      "Train Epoch: 190 [144960/225000 (64%)] Loss: 18843.964844\n",
      "Train Epoch: 190 [147456/225000 (66%)] Loss: 19239.953125\n",
      "Train Epoch: 190 [149952/225000 (67%)] Loss: 18790.437500\n",
      "Train Epoch: 190 [152448/225000 (68%)] Loss: 18983.576172\n",
      "Train Epoch: 190 [154944/225000 (69%)] Loss: 18810.050781\n",
      "Train Epoch: 190 [157440/225000 (70%)] Loss: 18849.707031\n",
      "Train Epoch: 190 [159936/225000 (71%)] Loss: 19078.875000\n",
      "Train Epoch: 190 [162432/225000 (72%)] Loss: 18993.353516\n",
      "Train Epoch: 190 [164928/225000 (73%)] Loss: 18657.003906\n",
      "Train Epoch: 190 [167424/225000 (74%)] Loss: 18788.388672\n",
      "Train Epoch: 190 [169920/225000 (76%)] Loss: 19434.792969\n",
      "Train Epoch: 190 [172416/225000 (77%)] Loss: 18695.408203\n",
      "Train Epoch: 190 [174912/225000 (78%)] Loss: 19335.562500\n",
      "Train Epoch: 190 [177408/225000 (79%)] Loss: 18866.208984\n",
      "Train Epoch: 190 [179904/225000 (80%)] Loss: 18212.292969\n",
      "Train Epoch: 190 [182400/225000 (81%)] Loss: 19145.687500\n",
      "Train Epoch: 190 [184896/225000 (82%)] Loss: 18847.863281\n",
      "Train Epoch: 190 [187392/225000 (83%)] Loss: 18916.634766\n",
      "Train Epoch: 190 [189888/225000 (84%)] Loss: 18988.035156\n",
      "Train Epoch: 190 [192384/225000 (86%)] Loss: 18560.115234\n",
      "Train Epoch: 190 [194880/225000 (87%)] Loss: 18869.671875\n",
      "Train Epoch: 190 [197376/225000 (88%)] Loss: 18950.441406\n",
      "Train Epoch: 190 [199872/225000 (89%)] Loss: 19456.505859\n",
      "Train Epoch: 190 [202368/225000 (90%)] Loss: 18989.570312\n",
      "Train Epoch: 190 [204864/225000 (91%)] Loss: 18884.816406\n",
      "Train Epoch: 190 [207360/225000 (92%)] Loss: 18713.921875\n",
      "Train Epoch: 190 [209856/225000 (93%)] Loss: 18971.019531\n",
      "Train Epoch: 190 [212352/225000 (94%)] Loss: 19015.855469\n",
      "Train Epoch: 190 [214848/225000 (95%)] Loss: 18873.871094\n",
      "Train Epoch: 190 [217344/225000 (97%)] Loss: 18933.062500\n",
      "Train Epoch: 190 [219840/225000 (98%)] Loss: 19137.347656\n",
      "Train Epoch: 190 [222336/225000 (99%)] Loss: 19163.160156\n",
      "Train Epoch: 190 [224832/225000 (100%)] Loss: 18913.949219\n",
      "    epoch          : 190\n",
      "    loss           : 18990.82640351696\n",
      "    val_loss       : 18876.813005350017\n",
      "Train Epoch: 191 [192/225000 (0%)] Loss: 19106.695312\n",
      "Train Epoch: 191 [2688/225000 (1%)] Loss: 19057.343750\n",
      "Train Epoch: 191 [5184/225000 (2%)] Loss: 18901.814453\n",
      "Train Epoch: 191 [7680/225000 (3%)] Loss: 18671.191406\n",
      "Train Epoch: 191 [10176/225000 (5%)] Loss: 18670.570312\n",
      "Train Epoch: 191 [12672/225000 (6%)] Loss: 18976.867188\n",
      "Train Epoch: 191 [15168/225000 (7%)] Loss: 19174.507812\n",
      "Train Epoch: 191 [17664/225000 (8%)] Loss: 19003.570312\n",
      "Train Epoch: 191 [20160/225000 (9%)] Loss: 19228.250000\n",
      "Train Epoch: 191 [22656/225000 (10%)] Loss: 19002.261719\n",
      "Train Epoch: 191 [25152/225000 (11%)] Loss: 19847.308594\n",
      "Train Epoch: 191 [27648/225000 (12%)] Loss: 18804.246094\n",
      "Train Epoch: 191 [30144/225000 (13%)] Loss: 19192.056641\n",
      "Train Epoch: 191 [32640/225000 (15%)] Loss: 19103.369141\n",
      "Train Epoch: 191 [35136/225000 (16%)] Loss: 18826.132812\n",
      "Train Epoch: 191 [37632/225000 (17%)] Loss: 18914.126953\n",
      "Train Epoch: 191 [40128/225000 (18%)] Loss: 19435.914062\n",
      "Train Epoch: 191 [42624/225000 (19%)] Loss: 18721.097656\n",
      "Train Epoch: 191 [45120/225000 (20%)] Loss: 18920.542969\n",
      "Train Epoch: 191 [47616/225000 (21%)] Loss: 18646.898438\n",
      "Train Epoch: 191 [50112/225000 (22%)] Loss: 18977.230469\n",
      "Train Epoch: 191 [52608/225000 (23%)] Loss: 18897.128906\n",
      "Train Epoch: 191 [55104/225000 (24%)] Loss: 18769.171875\n",
      "Train Epoch: 191 [57600/225000 (26%)] Loss: 18678.878906\n",
      "Train Epoch: 191 [60096/225000 (27%)] Loss: 18595.253906\n",
      "Train Epoch: 191 [62592/225000 (28%)] Loss: 18895.191406\n",
      "Train Epoch: 191 [65088/225000 (29%)] Loss: 18734.093750\n",
      "Train Epoch: 191 [67584/225000 (30%)] Loss: 19028.828125\n",
      "Train Epoch: 191 [70080/225000 (31%)] Loss: 18878.550781\n",
      "Train Epoch: 191 [72576/225000 (32%)] Loss: 19111.853516\n",
      "Train Epoch: 191 [75072/225000 (33%)] Loss: 19728.798828\n",
      "Train Epoch: 191 [77568/225000 (34%)] Loss: 19278.335938\n",
      "Train Epoch: 191 [80064/225000 (36%)] Loss: 18610.042969\n",
      "Train Epoch: 191 [82560/225000 (37%)] Loss: 19325.710938\n",
      "Train Epoch: 191 [85056/225000 (38%)] Loss: 19085.187500\n",
      "Train Epoch: 191 [87552/225000 (39%)] Loss: 19203.511719\n",
      "Train Epoch: 191 [90048/225000 (40%)] Loss: 19130.031250\n",
      "Train Epoch: 191 [92544/225000 (41%)] Loss: 19000.136719\n",
      "Train Epoch: 191 [95040/225000 (42%)] Loss: 19443.847656\n",
      "Train Epoch: 191 [97536/225000 (43%)] Loss: 19243.269531\n",
      "Train Epoch: 191 [100032/225000 (44%)] Loss: 19091.646484\n",
      "Train Epoch: 191 [102528/225000 (46%)] Loss: 19320.066406\n",
      "Train Epoch: 191 [105024/225000 (47%)] Loss: 18483.914062\n",
      "Train Epoch: 191 [107520/225000 (48%)] Loss: 18830.226562\n",
      "Train Epoch: 191 [110016/225000 (49%)] Loss: 19570.160156\n",
      "Train Epoch: 191 [112512/225000 (50%)] Loss: 19271.871094\n",
      "Train Epoch: 191 [115008/225000 (51%)] Loss: 18392.871094\n",
      "Train Epoch: 191 [117504/225000 (52%)] Loss: 18897.482422\n",
      "Train Epoch: 191 [120000/225000 (53%)] Loss: 18599.890625\n",
      "Train Epoch: 191 [122496/225000 (54%)] Loss: 18528.738281\n",
      "Train Epoch: 191 [124992/225000 (56%)] Loss: 19011.978516\n",
      "Train Epoch: 191 [127488/225000 (57%)] Loss: 18779.078125\n",
      "Train Epoch: 191 [129984/225000 (58%)] Loss: 19070.800781\n",
      "Train Epoch: 191 [132480/225000 (59%)] Loss: 18493.382812\n",
      "Train Epoch: 191 [134976/225000 (60%)] Loss: 18646.343750\n",
      "Train Epoch: 191 [137472/225000 (61%)] Loss: 18943.164062\n",
      "Train Epoch: 191 [139968/225000 (62%)] Loss: 18884.595703\n",
      "Train Epoch: 191 [142464/225000 (63%)] Loss: 18947.718750\n",
      "Train Epoch: 191 [144960/225000 (64%)] Loss: 19081.705078\n",
      "Train Epoch: 191 [147456/225000 (66%)] Loss: 19184.148438\n",
      "Train Epoch: 191 [149952/225000 (67%)] Loss: 18728.222656\n",
      "Train Epoch: 191 [152448/225000 (68%)] Loss: 18844.281250\n",
      "Train Epoch: 191 [154944/225000 (69%)] Loss: 19256.222656\n",
      "Train Epoch: 191 [157440/225000 (70%)] Loss: 18708.250000\n",
      "Train Epoch: 191 [159936/225000 (71%)] Loss: 18835.130859\n",
      "Train Epoch: 191 [162432/225000 (72%)] Loss: 19089.113281\n",
      "Train Epoch: 191 [164928/225000 (73%)] Loss: 18780.511719\n",
      "Train Epoch: 191 [167424/225000 (74%)] Loss: 19572.773438\n",
      "Train Epoch: 191 [169920/225000 (76%)] Loss: 18699.078125\n",
      "Train Epoch: 191 [172416/225000 (77%)] Loss: 19234.457031\n",
      "Train Epoch: 191 [174912/225000 (78%)] Loss: 19238.027344\n",
      "Train Epoch: 191 [177408/225000 (79%)] Loss: 18898.628906\n",
      "Train Epoch: 191 [179904/225000 (80%)] Loss: 18652.343750\n",
      "Train Epoch: 191 [182400/225000 (81%)] Loss: 19482.160156\n",
      "Train Epoch: 191 [184896/225000 (82%)] Loss: 18748.447266\n",
      "Train Epoch: 191 [187392/225000 (83%)] Loss: 18879.164062\n",
      "Train Epoch: 191 [189888/225000 (84%)] Loss: 18880.082031\n",
      "Train Epoch: 191 [192384/225000 (86%)] Loss: 18650.054688\n",
      "Train Epoch: 191 [194880/225000 (87%)] Loss: 18898.726562\n",
      "Train Epoch: 191 [197376/225000 (88%)] Loss: 18700.849609\n",
      "Train Epoch: 191 [199872/225000 (89%)] Loss: 19363.156250\n",
      "Train Epoch: 191 [202368/225000 (90%)] Loss: 19304.695312\n",
      "Train Epoch: 191 [204864/225000 (91%)] Loss: 19214.128906\n",
      "Train Epoch: 191 [207360/225000 (92%)] Loss: 18619.779297\n",
      "Train Epoch: 191 [209856/225000 (93%)] Loss: 19145.460938\n",
      "Train Epoch: 191 [212352/225000 (94%)] Loss: 18898.341797\n",
      "Train Epoch: 191 [214848/225000 (95%)] Loss: 19093.109375\n",
      "Train Epoch: 191 [217344/225000 (97%)] Loss: 18982.162109\n",
      "Train Epoch: 191 [219840/225000 (98%)] Loss: 19092.378906\n",
      "Train Epoch: 191 [222336/225000 (99%)] Loss: 18859.859375\n",
      "Train Epoch: 191 [224832/225000 (100%)] Loss: 18967.972656\n",
      "    epoch          : 191\n",
      "    loss           : 18973.451373520158\n",
      "    val_loss       : 18874.029200026096\n",
      "Train Epoch: 192 [192/225000 (0%)] Loss: 19335.078125\n",
      "Train Epoch: 192 [2688/225000 (1%)] Loss: 19049.050781\n",
      "Train Epoch: 192 [5184/225000 (2%)] Loss: 18745.621094\n",
      "Train Epoch: 192 [7680/225000 (3%)] Loss: 18804.175781\n",
      "Train Epoch: 192 [10176/225000 (5%)] Loss: 18838.302734\n",
      "Train Epoch: 192 [12672/225000 (6%)] Loss: 18539.871094\n",
      "Train Epoch: 192 [15168/225000 (7%)] Loss: 19180.429688\n",
      "Train Epoch: 192 [17664/225000 (8%)] Loss: 18985.363281\n",
      "Train Epoch: 192 [20160/225000 (9%)] Loss: 18657.187500\n",
      "Train Epoch: 192 [22656/225000 (10%)] Loss: 18888.171875\n",
      "Train Epoch: 192 [25152/225000 (11%)] Loss: 18679.353516\n",
      "Train Epoch: 192 [27648/225000 (12%)] Loss: 18760.957031\n",
      "Train Epoch: 192 [30144/225000 (13%)] Loss: 19590.865234\n",
      "Train Epoch: 192 [32640/225000 (15%)] Loss: 19007.425781\n",
      "Train Epoch: 192 [35136/225000 (16%)] Loss: 19458.523438\n",
      "Train Epoch: 192 [37632/225000 (17%)] Loss: 18820.394531\n",
      "Train Epoch: 192 [40128/225000 (18%)] Loss: 19171.519531\n",
      "Train Epoch: 192 [42624/225000 (19%)] Loss: 18772.765625\n",
      "Train Epoch: 192 [45120/225000 (20%)] Loss: 18774.710938\n",
      "Train Epoch: 192 [47616/225000 (21%)] Loss: 19438.421875\n",
      "Train Epoch: 192 [50112/225000 (22%)] Loss: 18320.179688\n",
      "Train Epoch: 192 [52608/225000 (23%)] Loss: 19015.621094\n",
      "Train Epoch: 192 [55104/225000 (24%)] Loss: 18638.263672\n",
      "Train Epoch: 192 [57600/225000 (26%)] Loss: 19191.367188\n",
      "Train Epoch: 192 [60096/225000 (27%)] Loss: 18799.367188\n",
      "Train Epoch: 192 [62592/225000 (28%)] Loss: 18855.449219\n",
      "Train Epoch: 192 [65088/225000 (29%)] Loss: 18596.015625\n",
      "Train Epoch: 192 [67584/225000 (30%)] Loss: 19212.539062\n",
      "Train Epoch: 192 [70080/225000 (31%)] Loss: 18568.222656\n",
      "Train Epoch: 192 [72576/225000 (32%)] Loss: 18725.998047\n",
      "Train Epoch: 192 [75072/225000 (33%)] Loss: 19257.384766\n",
      "Train Epoch: 192 [77568/225000 (34%)] Loss: 19221.576172\n",
      "Train Epoch: 192 [80064/225000 (36%)] Loss: 19157.359375\n",
      "Train Epoch: 192 [82560/225000 (37%)] Loss: 18844.062500\n",
      "Train Epoch: 192 [85056/225000 (38%)] Loss: 18755.593750\n",
      "Train Epoch: 192 [87552/225000 (39%)] Loss: 18248.552734\n",
      "Train Epoch: 192 [90048/225000 (40%)] Loss: 18481.634766\n",
      "Train Epoch: 192 [92544/225000 (41%)] Loss: 18891.804688\n",
      "Train Epoch: 192 [95040/225000 (42%)] Loss: 19085.611328\n",
      "Train Epoch: 192 [97536/225000 (43%)] Loss: 18970.175781\n",
      "Train Epoch: 192 [100032/225000 (44%)] Loss: 19058.519531\n",
      "Train Epoch: 192 [102528/225000 (46%)] Loss: 18790.212891\n",
      "Train Epoch: 192 [105024/225000 (47%)] Loss: 18773.005859\n",
      "Train Epoch: 192 [107520/225000 (48%)] Loss: 18707.558594\n",
      "Train Epoch: 192 [110016/225000 (49%)] Loss: 19257.535156\n",
      "Train Epoch: 192 [112512/225000 (50%)] Loss: 18664.421875\n",
      "Train Epoch: 192 [115008/225000 (51%)] Loss: 18815.468750\n",
      "Train Epoch: 192 [117504/225000 (52%)] Loss: 18971.884766\n",
      "Train Epoch: 192 [120000/225000 (53%)] Loss: 19133.726562\n",
      "Train Epoch: 192 [122496/225000 (54%)] Loss: 19152.658203\n",
      "Train Epoch: 192 [124992/225000 (56%)] Loss: 18824.226562\n",
      "Train Epoch: 192 [127488/225000 (57%)] Loss: 19183.925781\n",
      "Train Epoch: 192 [129984/225000 (58%)] Loss: 18653.964844\n",
      "Train Epoch: 192 [132480/225000 (59%)] Loss: 18874.492188\n",
      "Train Epoch: 192 [134976/225000 (60%)] Loss: 18891.480469\n",
      "Train Epoch: 192 [137472/225000 (61%)] Loss: 18740.316406\n",
      "Train Epoch: 192 [139968/225000 (62%)] Loss: 18966.455078\n",
      "Train Epoch: 192 [142464/225000 (63%)] Loss: 18840.343750\n",
      "Train Epoch: 192 [144960/225000 (64%)] Loss: 19069.988281\n",
      "Train Epoch: 192 [147456/225000 (66%)] Loss: 18956.335938\n",
      "Train Epoch: 192 [149952/225000 (67%)] Loss: 19179.644531\n",
      "Train Epoch: 192 [152448/225000 (68%)] Loss: 19161.761719\n",
      "Train Epoch: 192 [154944/225000 (69%)] Loss: 19481.986328\n",
      "Train Epoch: 192 [157440/225000 (70%)] Loss: 19147.558594\n",
      "Train Epoch: 192 [159936/225000 (71%)] Loss: 19178.375000\n",
      "Train Epoch: 192 [162432/225000 (72%)] Loss: 19158.068359\n",
      "Train Epoch: 192 [164928/225000 (73%)] Loss: 18494.039062\n",
      "Train Epoch: 192 [167424/225000 (74%)] Loss: 19270.597656\n",
      "Train Epoch: 192 [169920/225000 (76%)] Loss: 19333.214844\n",
      "Train Epoch: 192 [172416/225000 (77%)] Loss: 18952.283203\n",
      "Train Epoch: 192 [174912/225000 (78%)] Loss: 18763.417969\n",
      "Train Epoch: 192 [177408/225000 (79%)] Loss: 19064.056641\n",
      "Train Epoch: 192 [179904/225000 (80%)] Loss: 18863.576172\n",
      "Train Epoch: 192 [182400/225000 (81%)] Loss: 18897.710938\n",
      "Train Epoch: 192 [184896/225000 (82%)] Loss: 18907.531250\n",
      "Train Epoch: 192 [187392/225000 (83%)] Loss: 19248.378906\n",
      "Train Epoch: 192 [189888/225000 (84%)] Loss: 18817.796875\n",
      "Train Epoch: 192 [192384/225000 (86%)] Loss: 18556.890625\n",
      "Train Epoch: 192 [194880/225000 (87%)] Loss: 18978.343750\n",
      "Train Epoch: 192 [197376/225000 (88%)] Loss: 18507.761719\n",
      "Train Epoch: 192 [199872/225000 (89%)] Loss: 18850.878906\n",
      "Train Epoch: 192 [202368/225000 (90%)] Loss: 18991.023438\n",
      "Train Epoch: 192 [204864/225000 (91%)] Loss: 18590.509766\n",
      "Train Epoch: 192 [207360/225000 (92%)] Loss: 19468.210938\n",
      "Train Epoch: 192 [209856/225000 (93%)] Loss: 18976.886719\n",
      "Train Epoch: 192 [212352/225000 (94%)] Loss: 18822.441406\n",
      "Train Epoch: 192 [214848/225000 (95%)] Loss: 18714.841797\n",
      "Train Epoch: 192 [217344/225000 (97%)] Loss: 18876.515625\n",
      "Train Epoch: 192 [219840/225000 (98%)] Loss: 18796.218750\n",
      "Train Epoch: 192 [222336/225000 (99%)] Loss: 19086.125000\n",
      "Train Epoch: 192 [224832/225000 (100%)] Loss: 19395.718750\n",
      "    epoch          : 192\n",
      "    loss           : 18964.336650757254\n",
      "    val_loss       : 18859.979279801137\n",
      "Train Epoch: 193 [192/225000 (0%)] Loss: 18536.185547\n",
      "Train Epoch: 193 [2688/225000 (1%)] Loss: 18929.230469\n",
      "Train Epoch: 193 [5184/225000 (2%)] Loss: 18925.386719\n",
      "Train Epoch: 193 [7680/225000 (3%)] Loss: 18910.960938\n",
      "Train Epoch: 193 [10176/225000 (5%)] Loss: 19031.935547\n",
      "Train Epoch: 193 [12672/225000 (6%)] Loss: 18704.558594\n",
      "Train Epoch: 193 [15168/225000 (7%)] Loss: 18911.847656\n",
      "Train Epoch: 193 [17664/225000 (8%)] Loss: 18784.246094\n",
      "Train Epoch: 193 [20160/225000 (9%)] Loss: 19221.386719\n",
      "Train Epoch: 193 [22656/225000 (10%)] Loss: 18900.496094\n",
      "Train Epoch: 193 [25152/225000 (11%)] Loss: 19454.699219\n",
      "Train Epoch: 193 [27648/225000 (12%)] Loss: 18842.062500\n",
      "Train Epoch: 193 [30144/225000 (13%)] Loss: 18726.320312\n",
      "Train Epoch: 193 [32640/225000 (15%)] Loss: 19225.685547\n",
      "Train Epoch: 193 [35136/225000 (16%)] Loss: 18705.845703\n",
      "Train Epoch: 193 [37632/225000 (17%)] Loss: 19552.566406\n",
      "Train Epoch: 193 [40128/225000 (18%)] Loss: 18737.726562\n",
      "Train Epoch: 193 [42624/225000 (19%)] Loss: 18922.345703\n",
      "Train Epoch: 193 [45120/225000 (20%)] Loss: 18796.812500\n",
      "Train Epoch: 193 [47616/225000 (21%)] Loss: 18906.962891\n",
      "Train Epoch: 193 [50112/225000 (22%)] Loss: 18285.582031\n",
      "Train Epoch: 193 [52608/225000 (23%)] Loss: 19032.330078\n",
      "Train Epoch: 193 [55104/225000 (24%)] Loss: 19110.441406\n",
      "Train Epoch: 193 [57600/225000 (26%)] Loss: 19108.812500\n",
      "Train Epoch: 193 [60096/225000 (27%)] Loss: 19013.726562\n",
      "Train Epoch: 193 [62592/225000 (28%)] Loss: 18977.041016\n",
      "Train Epoch: 193 [65088/225000 (29%)] Loss: 19027.210938\n",
      "Train Epoch: 193 [67584/225000 (30%)] Loss: 18755.597656\n",
      "Train Epoch: 193 [70080/225000 (31%)] Loss: 18655.619141\n",
      "Train Epoch: 193 [72576/225000 (32%)] Loss: 19198.712891\n",
      "Train Epoch: 193 [75072/225000 (33%)] Loss: 18772.552734\n",
      "Train Epoch: 193 [77568/225000 (34%)] Loss: 18773.960938\n",
      "Train Epoch: 193 [80064/225000 (36%)] Loss: 18733.640625\n",
      "Train Epoch: 193 [82560/225000 (37%)] Loss: 19008.910156\n",
      "Train Epoch: 193 [85056/225000 (38%)] Loss: 18712.621094\n",
      "Train Epoch: 193 [87552/225000 (39%)] Loss: 18853.994141\n",
      "Train Epoch: 193 [90048/225000 (40%)] Loss: 18340.136719\n",
      "Train Epoch: 193 [92544/225000 (41%)] Loss: 18910.199219\n",
      "Train Epoch: 193 [95040/225000 (42%)] Loss: 18902.468750\n",
      "Train Epoch: 193 [97536/225000 (43%)] Loss: 19344.000000\n",
      "Train Epoch: 193 [100032/225000 (44%)] Loss: 19234.654297\n",
      "Train Epoch: 193 [102528/225000 (46%)] Loss: 19115.128906\n",
      "Train Epoch: 193 [105024/225000 (47%)] Loss: 18963.910156\n",
      "Train Epoch: 193 [107520/225000 (48%)] Loss: 18870.789062\n",
      "Train Epoch: 193 [110016/225000 (49%)] Loss: 18772.144531\n",
      "Train Epoch: 193 [112512/225000 (50%)] Loss: 18587.277344\n",
      "Train Epoch: 193 [115008/225000 (51%)] Loss: 19265.048828\n",
      "Train Epoch: 193 [117504/225000 (52%)] Loss: 19134.519531\n",
      "Train Epoch: 193 [120000/225000 (53%)] Loss: 18784.207031\n",
      "Train Epoch: 193 [122496/225000 (54%)] Loss: 18487.074219\n",
      "Train Epoch: 193 [124992/225000 (56%)] Loss: 19219.382812\n",
      "Train Epoch: 193 [127488/225000 (57%)] Loss: 19102.714844\n",
      "Train Epoch: 193 [129984/225000 (58%)] Loss: 18792.632812\n",
      "Train Epoch: 193 [132480/225000 (59%)] Loss: 18725.839844\n",
      "Train Epoch: 193 [134976/225000 (60%)] Loss: 18775.201172\n",
      "Train Epoch: 193 [137472/225000 (61%)] Loss: 18757.250000\n",
      "Train Epoch: 193 [139968/225000 (62%)] Loss: 19030.441406\n",
      "Train Epoch: 193 [142464/225000 (63%)] Loss: 19122.953125\n",
      "Train Epoch: 193 [144960/225000 (64%)] Loss: 18916.734375\n",
      "Train Epoch: 193 [147456/225000 (66%)] Loss: 19082.455078\n",
      "Train Epoch: 193 [149952/225000 (67%)] Loss: 18810.511719\n",
      "Train Epoch: 193 [152448/225000 (68%)] Loss: 19107.617188\n",
      "Train Epoch: 193 [154944/225000 (69%)] Loss: 19240.292969\n",
      "Train Epoch: 193 [157440/225000 (70%)] Loss: 19024.242188\n",
      "Train Epoch: 193 [159936/225000 (71%)] Loss: 18738.765625\n",
      "Train Epoch: 193 [162432/225000 (72%)] Loss: 18297.351562\n",
      "Train Epoch: 193 [164928/225000 (73%)] Loss: 18603.117188\n",
      "Train Epoch: 193 [167424/225000 (74%)] Loss: 19176.804688\n",
      "Train Epoch: 193 [169920/225000 (76%)] Loss: 18861.886719\n",
      "Train Epoch: 193 [172416/225000 (77%)] Loss: 19367.468750\n",
      "Train Epoch: 193 [174912/225000 (78%)] Loss: 19011.660156\n",
      "Train Epoch: 193 [177408/225000 (79%)] Loss: 18542.187500\n",
      "Train Epoch: 193 [179904/225000 (80%)] Loss: 18808.523438\n",
      "Train Epoch: 193 [182400/225000 (81%)] Loss: 19052.345703\n",
      "Train Epoch: 193 [184896/225000 (82%)] Loss: 19138.187500\n",
      "Train Epoch: 193 [187392/225000 (83%)] Loss: 18652.714844\n",
      "Train Epoch: 193 [189888/225000 (84%)] Loss: 19113.652344\n",
      "Train Epoch: 193 [192384/225000 (86%)] Loss: 18846.076172\n",
      "Train Epoch: 193 [194880/225000 (87%)] Loss: 19325.230469\n",
      "Train Epoch: 193 [197376/225000 (88%)] Loss: 19293.921875\n",
      "Train Epoch: 193 [199872/225000 (89%)] Loss: 18715.181641\n",
      "Train Epoch: 193 [202368/225000 (90%)] Loss: 18879.589844\n",
      "Train Epoch: 193 [204864/225000 (91%)] Loss: 18729.390625\n",
      "Train Epoch: 193 [207360/225000 (92%)] Loss: 18566.265625\n",
      "Train Epoch: 193 [209856/225000 (93%)] Loss: 18762.593750\n",
      "Train Epoch: 193 [212352/225000 (94%)] Loss: 18522.000000\n",
      "Train Epoch: 193 [214848/225000 (95%)] Loss: 19022.457031\n",
      "Train Epoch: 193 [217344/225000 (97%)] Loss: 18732.648438\n",
      "Train Epoch: 193 [219840/225000 (98%)] Loss: 18610.933594\n",
      "Train Epoch: 193 [222336/225000 (99%)] Loss: 19186.960938\n",
      "Train Epoch: 193 [224832/225000 (100%)] Loss: 18761.027344\n",
      "    epoch          : 193\n",
      "    loss           : 18955.410467883426\n",
      "    val_loss       : 18853.356620265782\n",
      "Train Epoch: 194 [192/225000 (0%)] Loss: 19040.164062\n",
      "Train Epoch: 194 [2688/225000 (1%)] Loss: 19283.105469\n",
      "Train Epoch: 194 [5184/225000 (2%)] Loss: 18874.730469\n",
      "Train Epoch: 194 [7680/225000 (3%)] Loss: 19613.125000\n",
      "Train Epoch: 194 [10176/225000 (5%)] Loss: 19464.812500\n",
      "Train Epoch: 194 [12672/225000 (6%)] Loss: 19312.898438\n",
      "Train Epoch: 194 [15168/225000 (7%)] Loss: 19091.644531\n",
      "Train Epoch: 194 [17664/225000 (8%)] Loss: 18961.914062\n",
      "Train Epoch: 194 [20160/225000 (9%)] Loss: 19210.218750\n",
      "Train Epoch: 194 [22656/225000 (10%)] Loss: 19581.890625\n",
      "Train Epoch: 194 [25152/225000 (11%)] Loss: 19190.109375\n",
      "Train Epoch: 194 [27648/225000 (12%)] Loss: 18869.882812\n",
      "Train Epoch: 194 [30144/225000 (13%)] Loss: 18979.019531\n",
      "Train Epoch: 194 [32640/225000 (15%)] Loss: 18794.988281\n",
      "Train Epoch: 194 [35136/225000 (16%)] Loss: 18724.197266\n",
      "Train Epoch: 194 [37632/225000 (17%)] Loss: 18737.257812\n",
      "Train Epoch: 194 [40128/225000 (18%)] Loss: 18830.503906\n",
      "Train Epoch: 194 [42624/225000 (19%)] Loss: 19222.300781\n",
      "Train Epoch: 194 [45120/225000 (20%)] Loss: 18886.140625\n",
      "Train Epoch: 194 [47616/225000 (21%)] Loss: 19600.154297\n",
      "Train Epoch: 194 [50112/225000 (22%)] Loss: 18853.128906\n",
      "Train Epoch: 194 [52608/225000 (23%)] Loss: 18948.619141\n",
      "Train Epoch: 194 [55104/225000 (24%)] Loss: 19269.734375\n",
      "Train Epoch: 194 [57600/225000 (26%)] Loss: 19379.841797\n",
      "Train Epoch: 194 [60096/225000 (27%)] Loss: 18897.822266\n",
      "Train Epoch: 194 [62592/225000 (28%)] Loss: 18807.917969\n",
      "Train Epoch: 194 [65088/225000 (29%)] Loss: 19276.349609\n",
      "Train Epoch: 194 [67584/225000 (30%)] Loss: 18751.070312\n",
      "Train Epoch: 194 [70080/225000 (31%)] Loss: 18507.224609\n",
      "Train Epoch: 194 [72576/225000 (32%)] Loss: 18650.980469\n",
      "Train Epoch: 194 [75072/225000 (33%)] Loss: 18458.037109\n",
      "Train Epoch: 194 [77568/225000 (34%)] Loss: 19186.484375\n",
      "Train Epoch: 194 [80064/225000 (36%)] Loss: 18756.781250\n",
      "Train Epoch: 194 [82560/225000 (37%)] Loss: 18878.273438\n",
      "Train Epoch: 194 [85056/225000 (38%)] Loss: 18460.703125\n",
      "Train Epoch: 194 [87552/225000 (39%)] Loss: 19148.398438\n",
      "Train Epoch: 194 [90048/225000 (40%)] Loss: 19310.132812\n",
      "Train Epoch: 194 [92544/225000 (41%)] Loss: 19287.109375\n",
      "Train Epoch: 194 [95040/225000 (42%)] Loss: 19359.976562\n",
      "Train Epoch: 194 [97536/225000 (43%)] Loss: 18870.451172\n",
      "Train Epoch: 194 [100032/225000 (44%)] Loss: 18478.785156\n",
      "Train Epoch: 194 [102528/225000 (46%)] Loss: 19277.402344\n",
      "Train Epoch: 194 [105024/225000 (47%)] Loss: 19355.804688\n",
      "Train Epoch: 194 [107520/225000 (48%)] Loss: 18624.732422\n",
      "Train Epoch: 194 [110016/225000 (49%)] Loss: 18737.794922\n",
      "Train Epoch: 194 [112512/225000 (50%)] Loss: 19014.123047\n",
      "Train Epoch: 194 [115008/225000 (51%)] Loss: 18817.599609\n",
      "Train Epoch: 194 [117504/225000 (52%)] Loss: 18813.066406\n",
      "Train Epoch: 194 [120000/225000 (53%)] Loss: 18791.208984\n",
      "Train Epoch: 194 [122496/225000 (54%)] Loss: 18744.486328\n",
      "Train Epoch: 194 [124992/225000 (56%)] Loss: 18999.648438\n",
      "Train Epoch: 194 [127488/225000 (57%)] Loss: 19143.054688\n",
      "Train Epoch: 194 [129984/225000 (58%)] Loss: 18833.576172\n",
      "Train Epoch: 194 [132480/225000 (59%)] Loss: 18878.070312\n",
      "Train Epoch: 194 [134976/225000 (60%)] Loss: 18866.228516\n",
      "Train Epoch: 194 [137472/225000 (61%)] Loss: 18923.433594\n",
      "Train Epoch: 194 [139968/225000 (62%)] Loss: 18466.228516\n",
      "Train Epoch: 194 [142464/225000 (63%)] Loss: 18926.292969\n",
      "Train Epoch: 194 [144960/225000 (64%)] Loss: 18787.207031\n",
      "Train Epoch: 194 [147456/225000 (66%)] Loss: 19334.605469\n",
      "Train Epoch: 194 [149952/225000 (67%)] Loss: 18673.660156\n",
      "Train Epoch: 194 [152448/225000 (68%)] Loss: 19139.630859\n",
      "Train Epoch: 194 [154944/225000 (69%)] Loss: 19268.503906\n",
      "Train Epoch: 194 [157440/225000 (70%)] Loss: 18512.650391\n",
      "Train Epoch: 194 [159936/225000 (71%)] Loss: 18578.177734\n",
      "Train Epoch: 194 [162432/225000 (72%)] Loss: 18943.976562\n",
      "Train Epoch: 194 [164928/225000 (73%)] Loss: 19073.015625\n",
      "Train Epoch: 194 [167424/225000 (74%)] Loss: 18876.515625\n",
      "Train Epoch: 194 [169920/225000 (76%)] Loss: 18667.080078\n",
      "Train Epoch: 194 [172416/225000 (77%)] Loss: 18557.146484\n",
      "Train Epoch: 194 [174912/225000 (78%)] Loss: 18639.578125\n",
      "Train Epoch: 194 [177408/225000 (79%)] Loss: 18963.949219\n",
      "Train Epoch: 194 [179904/225000 (80%)] Loss: 18739.060547\n",
      "Train Epoch: 194 [182400/225000 (81%)] Loss: 19066.482422\n",
      "Train Epoch: 194 [184896/225000 (82%)] Loss: 19313.593750\n",
      "Train Epoch: 194 [187392/225000 (83%)] Loss: 19086.597656\n",
      "Train Epoch: 194 [189888/225000 (84%)] Loss: 18920.980469\n",
      "Train Epoch: 194 [192384/225000 (86%)] Loss: 18417.185547\n",
      "Train Epoch: 194 [194880/225000 (87%)] Loss: 18568.220703\n",
      "Train Epoch: 194 [197376/225000 (88%)] Loss: 18816.617188\n",
      "Train Epoch: 194 [199872/225000 (89%)] Loss: 18848.023438\n",
      "Train Epoch: 194 [202368/225000 (90%)] Loss: 18802.355469\n",
      "Train Epoch: 194 [204864/225000 (91%)] Loss: 18787.656250\n",
      "Train Epoch: 194 [207360/225000 (92%)] Loss: 18673.939453\n",
      "Train Epoch: 194 [209856/225000 (93%)] Loss: 18596.250000\n",
      "Train Epoch: 194 [212352/225000 (94%)] Loss: 19429.947266\n",
      "Train Epoch: 194 [214848/225000 (95%)] Loss: 19210.675781\n",
      "Train Epoch: 194 [217344/225000 (97%)] Loss: 18452.806641\n",
      "Train Epoch: 194 [219840/225000 (98%)] Loss: 19364.500000\n",
      "Train Epoch: 194 [222336/225000 (99%)] Loss: 19398.292969\n",
      "Train Epoch: 194 [224832/225000 (100%)] Loss: 18948.957031\n",
      "    epoch          : 194\n",
      "    loss           : 18931.12839880413\n",
      "    val_loss       : 18824.736107149198\n",
      "Train Epoch: 195 [192/225000 (0%)] Loss: 19097.785156\n",
      "Train Epoch: 195 [2688/225000 (1%)] Loss: 18532.375000\n",
      "Train Epoch: 195 [5184/225000 (2%)] Loss: 18791.046875\n",
      "Train Epoch: 195 [7680/225000 (3%)] Loss: 18902.050781\n",
      "Train Epoch: 195 [10176/225000 (5%)] Loss: 18678.767578\n",
      "Train Epoch: 195 [12672/225000 (6%)] Loss: 19023.601562\n",
      "Train Epoch: 195 [15168/225000 (7%)] Loss: 19038.222656\n",
      "Train Epoch: 195 [17664/225000 (8%)] Loss: 18786.017578\n",
      "Train Epoch: 195 [20160/225000 (9%)] Loss: 18659.529297\n",
      "Train Epoch: 195 [22656/225000 (10%)] Loss: 18909.042969\n",
      "Train Epoch: 195 [25152/225000 (11%)] Loss: 19063.769531\n",
      "Train Epoch: 195 [27648/225000 (12%)] Loss: 18885.750000\n",
      "Train Epoch: 195 [30144/225000 (13%)] Loss: 18847.835938\n",
      "Train Epoch: 195 [32640/225000 (15%)] Loss: 19082.273438\n",
      "Train Epoch: 195 [35136/225000 (16%)] Loss: 19144.039062\n",
      "Train Epoch: 195 [37632/225000 (17%)] Loss: 18765.742188\n",
      "Train Epoch: 195 [40128/225000 (18%)] Loss: 19302.949219\n",
      "Train Epoch: 195 [42624/225000 (19%)] Loss: 18958.535156\n",
      "Train Epoch: 195 [45120/225000 (20%)] Loss: 19302.917969\n",
      "Train Epoch: 195 [47616/225000 (21%)] Loss: 18926.460938\n",
      "Train Epoch: 195 [50112/225000 (22%)] Loss: 18555.667969\n",
      "Train Epoch: 195 [52608/225000 (23%)] Loss: 19272.980469\n",
      "Train Epoch: 195 [55104/225000 (24%)] Loss: 18785.009766\n",
      "Train Epoch: 195 [57600/225000 (26%)] Loss: 18689.703125\n",
      "Train Epoch: 195 [60096/225000 (27%)] Loss: 18943.882812\n",
      "Train Epoch: 195 [62592/225000 (28%)] Loss: 19131.011719\n",
      "Train Epoch: 195 [65088/225000 (29%)] Loss: 19079.314453\n",
      "Train Epoch: 195 [67584/225000 (30%)] Loss: 18425.708984\n",
      "Train Epoch: 195 [70080/225000 (31%)] Loss: 18987.457031\n",
      "Train Epoch: 195 [72576/225000 (32%)] Loss: 18612.974609\n",
      "Train Epoch: 195 [75072/225000 (33%)] Loss: 19038.382812\n",
      "Train Epoch: 195 [77568/225000 (34%)] Loss: 18790.039062\n",
      "Train Epoch: 195 [80064/225000 (36%)] Loss: 18821.279297\n",
      "Train Epoch: 195 [82560/225000 (37%)] Loss: 18928.007812\n",
      "Train Epoch: 195 [85056/225000 (38%)] Loss: 18994.574219\n",
      "Train Epoch: 195 [87552/225000 (39%)] Loss: 18563.306641\n",
      "Train Epoch: 195 [90048/225000 (40%)] Loss: 19241.306641\n",
      "Train Epoch: 195 [92544/225000 (41%)] Loss: 18952.386719\n",
      "Train Epoch: 195 [95040/225000 (42%)] Loss: 19040.726562\n",
      "Train Epoch: 195 [97536/225000 (43%)] Loss: 19247.822266\n",
      "Train Epoch: 195 [100032/225000 (44%)] Loss: 19004.335938\n",
      "Train Epoch: 195 [102528/225000 (46%)] Loss: 18907.296875\n",
      "Train Epoch: 195 [105024/225000 (47%)] Loss: 18887.585938\n",
      "Train Epoch: 195 [107520/225000 (48%)] Loss: 18646.185547\n",
      "Train Epoch: 195 [110016/225000 (49%)] Loss: 18932.652344\n",
      "Train Epoch: 195 [112512/225000 (50%)] Loss: 18688.460938\n",
      "Train Epoch: 195 [115008/225000 (51%)] Loss: 19225.458984\n",
      "Train Epoch: 195 [117504/225000 (52%)] Loss: 19583.570312\n",
      "Train Epoch: 195 [120000/225000 (53%)] Loss: 18888.019531\n",
      "Train Epoch: 195 [122496/225000 (54%)] Loss: 23800.902344\n",
      "Train Epoch: 195 [124992/225000 (56%)] Loss: 18723.824219\n",
      "Train Epoch: 195 [127488/225000 (57%)] Loss: 18916.957031\n",
      "Train Epoch: 195 [129984/225000 (58%)] Loss: 18881.175781\n",
      "Train Epoch: 195 [132480/225000 (59%)] Loss: 19259.132812\n",
      "Train Epoch: 195 [134976/225000 (60%)] Loss: 18558.080078\n",
      "Train Epoch: 195 [137472/225000 (61%)] Loss: 18963.447266\n",
      "Train Epoch: 195 [139968/225000 (62%)] Loss: 18805.476562\n",
      "Train Epoch: 195 [142464/225000 (63%)] Loss: 18965.058594\n",
      "Train Epoch: 195 [144960/225000 (64%)] Loss: 18742.244141\n",
      "Train Epoch: 195 [147456/225000 (66%)] Loss: 18894.402344\n",
      "Train Epoch: 195 [149952/225000 (67%)] Loss: 18933.894531\n",
      "Train Epoch: 195 [152448/225000 (68%)] Loss: 18829.355469\n",
      "Train Epoch: 195 [154944/225000 (69%)] Loss: 19165.050781\n",
      "Train Epoch: 195 [157440/225000 (70%)] Loss: 18905.328125\n",
      "Train Epoch: 195 [159936/225000 (71%)] Loss: 18714.773438\n",
      "Train Epoch: 195 [162432/225000 (72%)] Loss: 18827.562500\n",
      "Train Epoch: 195 [164928/225000 (73%)] Loss: 18980.582031\n",
      "Train Epoch: 195 [167424/225000 (74%)] Loss: 18554.281250\n",
      "Train Epoch: 195 [169920/225000 (76%)] Loss: 18811.910156\n",
      "Train Epoch: 195 [172416/225000 (77%)] Loss: 19034.824219\n",
      "Train Epoch: 195 [174912/225000 (78%)] Loss: 19347.246094\n",
      "Train Epoch: 195 [177408/225000 (79%)] Loss: 18882.318359\n",
      "Train Epoch: 195 [179904/225000 (80%)] Loss: 18783.750000\n",
      "Train Epoch: 195 [182400/225000 (81%)] Loss: 19429.498047\n",
      "Train Epoch: 195 [184896/225000 (82%)] Loss: 18685.060547\n",
      "Train Epoch: 195 [187392/225000 (83%)] Loss: 19160.445312\n",
      "Train Epoch: 195 [189888/225000 (84%)] Loss: 18980.205078\n",
      "Train Epoch: 195 [192384/225000 (86%)] Loss: 18811.175781\n",
      "Train Epoch: 195 [194880/225000 (87%)] Loss: 18907.617188\n",
      "Train Epoch: 195 [197376/225000 (88%)] Loss: 19094.111328\n",
      "Train Epoch: 195 [199872/225000 (89%)] Loss: 18667.888672\n",
      "Train Epoch: 195 [202368/225000 (90%)] Loss: 18815.429688\n",
      "Train Epoch: 195 [204864/225000 (91%)] Loss: 18961.896484\n",
      "Train Epoch: 195 [207360/225000 (92%)] Loss: 19151.062500\n",
      "Train Epoch: 195 [209856/225000 (93%)] Loss: 18744.919922\n",
      "Train Epoch: 195 [212352/225000 (94%)] Loss: 19129.839844\n",
      "Train Epoch: 195 [214848/225000 (95%)] Loss: 18758.650391\n",
      "Train Epoch: 195 [217344/225000 (97%)] Loss: 18759.679688\n",
      "Train Epoch: 195 [219840/225000 (98%)] Loss: 19107.757812\n",
      "Train Epoch: 195 [222336/225000 (99%)] Loss: 19002.070312\n",
      "Train Epoch: 195 [224832/225000 (100%)] Loss: 18774.666016\n",
      "    epoch          : 195\n",
      "    loss           : 18909.50629932807\n",
      "    val_loss       : 18765.379745578946\n",
      "Train Epoch: 196 [192/225000 (0%)] Loss: 19110.123047\n",
      "Train Epoch: 196 [2688/225000 (1%)] Loss: 18763.435547\n",
      "Train Epoch: 196 [5184/225000 (2%)] Loss: 19080.914062\n",
      "Train Epoch: 196 [7680/225000 (3%)] Loss: 19018.531250\n",
      "Train Epoch: 196 [10176/225000 (5%)] Loss: 19021.976562\n",
      "Train Epoch: 196 [12672/225000 (6%)] Loss: 18587.609375\n",
      "Train Epoch: 196 [15168/225000 (7%)] Loss: 18407.675781\n",
      "Train Epoch: 196 [17664/225000 (8%)] Loss: 18805.220703\n",
      "Train Epoch: 196 [20160/225000 (9%)] Loss: 18593.496094\n",
      "Train Epoch: 196 [22656/225000 (10%)] Loss: 18988.812500\n",
      "Train Epoch: 196 [25152/225000 (11%)] Loss: 19196.345703\n",
      "Train Epoch: 196 [27648/225000 (12%)] Loss: 18862.250000\n",
      "Train Epoch: 196 [30144/225000 (13%)] Loss: 18345.984375\n",
      "Train Epoch: 196 [32640/225000 (15%)] Loss: 18841.968750\n",
      "Train Epoch: 196 [35136/225000 (16%)] Loss: 18699.982422\n",
      "Train Epoch: 196 [37632/225000 (17%)] Loss: 18703.464844\n",
      "Train Epoch: 196 [40128/225000 (18%)] Loss: 18789.808594\n",
      "Train Epoch: 196 [42624/225000 (19%)] Loss: 19309.691406\n",
      "Train Epoch: 196 [45120/225000 (20%)] Loss: 18845.691406\n",
      "Train Epoch: 196 [47616/225000 (21%)] Loss: 18986.525391\n",
      "Train Epoch: 196 [50112/225000 (22%)] Loss: 18428.974609\n",
      "Train Epoch: 196 [52608/225000 (23%)] Loss: 18929.269531\n",
      "Train Epoch: 196 [55104/225000 (24%)] Loss: 19081.335938\n",
      "Train Epoch: 196 [57600/225000 (26%)] Loss: 18960.527344\n",
      "Train Epoch: 196 [60096/225000 (27%)] Loss: 18748.097656\n",
      "Train Epoch: 196 [62592/225000 (28%)] Loss: 18744.228516\n",
      "Train Epoch: 196 [65088/225000 (29%)] Loss: 19006.132812\n",
      "Train Epoch: 196 [67584/225000 (30%)] Loss: 18688.722656\n",
      "Train Epoch: 196 [70080/225000 (31%)] Loss: 18194.675781\n",
      "Train Epoch: 196 [72576/225000 (32%)] Loss: 19246.687500\n",
      "Train Epoch: 196 [75072/225000 (33%)] Loss: 18911.398438\n",
      "Train Epoch: 196 [77568/225000 (34%)] Loss: 18860.636719\n",
      "Train Epoch: 196 [80064/225000 (36%)] Loss: 19079.513672\n",
      "Train Epoch: 196 [82560/225000 (37%)] Loss: 18731.804688\n",
      "Train Epoch: 196 [85056/225000 (38%)] Loss: 18911.667969\n",
      "Train Epoch: 196 [87552/225000 (39%)] Loss: 18781.253906\n",
      "Train Epoch: 196 [90048/225000 (40%)] Loss: 19061.531250\n",
      "Train Epoch: 196 [92544/225000 (41%)] Loss: 18858.931641\n",
      "Train Epoch: 196 [95040/225000 (42%)] Loss: 18897.798828\n",
      "Train Epoch: 196 [97536/225000 (43%)] Loss: 18794.287109\n",
      "Train Epoch: 196 [100032/225000 (44%)] Loss: 19015.710938\n",
      "Train Epoch: 196 [102528/225000 (46%)] Loss: 19187.257812\n",
      "Train Epoch: 196 [105024/225000 (47%)] Loss: 18931.455078\n",
      "Train Epoch: 196 [107520/225000 (48%)] Loss: 18944.480469\n",
      "Train Epoch: 196 [110016/225000 (49%)] Loss: 18554.601562\n",
      "Train Epoch: 196 [112512/225000 (50%)] Loss: 18743.679688\n",
      "Train Epoch: 196 [115008/225000 (51%)] Loss: 18811.070312\n",
      "Train Epoch: 196 [117504/225000 (52%)] Loss: 18945.712891\n",
      "Train Epoch: 196 [120000/225000 (53%)] Loss: 18793.031250\n",
      "Train Epoch: 196 [122496/225000 (54%)] Loss: 18972.125000\n",
      "Train Epoch: 196 [124992/225000 (56%)] Loss: 18722.160156\n",
      "Train Epoch: 196 [127488/225000 (57%)] Loss: 18607.054688\n",
      "Train Epoch: 196 [129984/225000 (58%)] Loss: 19177.353516\n",
      "Train Epoch: 196 [132480/225000 (59%)] Loss: 18649.875000\n",
      "Train Epoch: 196 [134976/225000 (60%)] Loss: 18776.228516\n",
      "Train Epoch: 196 [137472/225000 (61%)] Loss: 19034.208984\n",
      "Train Epoch: 196 [139968/225000 (62%)] Loss: 18963.277344\n",
      "Train Epoch: 196 [142464/225000 (63%)] Loss: 19099.062500\n",
      "Train Epoch: 196 [144960/225000 (64%)] Loss: 19117.402344\n",
      "Train Epoch: 196 [147456/225000 (66%)] Loss: 19099.119141\n",
      "Train Epoch: 196 [149952/225000 (67%)] Loss: 18826.207031\n",
      "Train Epoch: 196 [152448/225000 (68%)] Loss: 18653.187500\n",
      "Train Epoch: 196 [154944/225000 (69%)] Loss: 18739.681641\n",
      "Train Epoch: 196 [157440/225000 (70%)] Loss: 18635.160156\n",
      "Train Epoch: 196 [159936/225000 (71%)] Loss: 19258.445312\n",
      "Train Epoch: 196 [162432/225000 (72%)] Loss: 19337.193359\n",
      "Train Epoch: 196 [164928/225000 (73%)] Loss: 18925.121094\n",
      "Train Epoch: 196 [167424/225000 (74%)] Loss: 18844.693359\n",
      "Train Epoch: 196 [169920/225000 (76%)] Loss: 18719.343750\n",
      "Train Epoch: 196 [172416/225000 (77%)] Loss: 18968.482422\n",
      "Train Epoch: 196 [174912/225000 (78%)] Loss: 19054.447266\n",
      "Train Epoch: 196 [177408/225000 (79%)] Loss: 18941.144531\n",
      "Train Epoch: 196 [179904/225000 (80%)] Loss: 19393.035156\n",
      "Train Epoch: 196 [182400/225000 (81%)] Loss: 18934.484375\n",
      "Train Epoch: 196 [184896/225000 (82%)] Loss: 19044.976562\n",
      "Train Epoch: 196 [187392/225000 (83%)] Loss: 18700.453125\n",
      "Train Epoch: 196 [189888/225000 (84%)] Loss: 18613.083984\n",
      "Train Epoch: 196 [192384/225000 (86%)] Loss: 18953.607422\n",
      "Train Epoch: 196 [194880/225000 (87%)] Loss: 18426.175781\n",
      "Train Epoch: 196 [197376/225000 (88%)] Loss: 18884.886719\n",
      "Train Epoch: 196 [199872/225000 (89%)] Loss: 18407.388672\n",
      "Train Epoch: 196 [202368/225000 (90%)] Loss: 18836.353516\n",
      "Train Epoch: 196 [204864/225000 (91%)] Loss: 18731.683594\n",
      "Train Epoch: 196 [207360/225000 (92%)] Loss: 18656.695312\n",
      "Train Epoch: 196 [209856/225000 (93%)] Loss: 18536.160156\n",
      "Train Epoch: 196 [212352/225000 (94%)] Loss: 18543.326172\n",
      "Train Epoch: 196 [214848/225000 (95%)] Loss: 18585.583984\n",
      "Train Epoch: 196 [217344/225000 (97%)] Loss: 18963.882812\n",
      "Train Epoch: 196 [219840/225000 (98%)] Loss: 19114.998047\n",
      "Train Epoch: 196 [222336/225000 (99%)] Loss: 18584.732422\n",
      "Train Epoch: 196 [224832/225000 (100%)] Loss: 19005.808594\n",
      "    epoch          : 196\n",
      "    loss           : 18838.157278223654\n",
      "    val_loss       : 18727.672809184052\n",
      "Train Epoch: 197 [192/225000 (0%)] Loss: 18840.492188\n",
      "Train Epoch: 197 [2688/225000 (1%)] Loss: 18589.316406\n",
      "Train Epoch: 197 [5184/225000 (2%)] Loss: 19047.882812\n",
      "Train Epoch: 197 [7680/225000 (3%)] Loss: 19095.570312\n",
      "Train Epoch: 197 [10176/225000 (5%)] Loss: 19114.382812\n",
      "Train Epoch: 197 [12672/225000 (6%)] Loss: 19011.822266\n",
      "Train Epoch: 197 [15168/225000 (7%)] Loss: 18843.164062\n",
      "Train Epoch: 197 [17664/225000 (8%)] Loss: 18709.058594\n",
      "Train Epoch: 197 [20160/225000 (9%)] Loss: 18496.269531\n",
      "Train Epoch: 197 [22656/225000 (10%)] Loss: 18883.921875\n",
      "Train Epoch: 197 [25152/225000 (11%)] Loss: 18569.457031\n",
      "Train Epoch: 197 [27648/225000 (12%)] Loss: 18807.945312\n",
      "Train Epoch: 197 [30144/225000 (13%)] Loss: 18711.964844\n",
      "Train Epoch: 197 [32640/225000 (15%)] Loss: 19148.228516\n",
      "Train Epoch: 197 [35136/225000 (16%)] Loss: 18556.179688\n",
      "Train Epoch: 197 [37632/225000 (17%)] Loss: 18617.931641\n",
      "Train Epoch: 197 [40128/225000 (18%)] Loss: 18585.367188\n",
      "Train Epoch: 197 [42624/225000 (19%)] Loss: 18626.730469\n",
      "Train Epoch: 197 [45120/225000 (20%)] Loss: 19396.537109\n",
      "Train Epoch: 197 [47616/225000 (21%)] Loss: 18912.320312\n",
      "Train Epoch: 197 [50112/225000 (22%)] Loss: 18773.507812\n",
      "Train Epoch: 197 [52608/225000 (23%)] Loss: 18385.789062\n",
      "Train Epoch: 197 [55104/225000 (24%)] Loss: 18748.296875\n",
      "Train Epoch: 197 [57600/225000 (26%)] Loss: 19124.796875\n",
      "Train Epoch: 197 [60096/225000 (27%)] Loss: 18855.919922\n",
      "Train Epoch: 197 [62592/225000 (28%)] Loss: 18600.445312\n",
      "Train Epoch: 197 [65088/225000 (29%)] Loss: 19225.033203\n",
      "Train Epoch: 197 [67584/225000 (30%)] Loss: 18962.988281\n",
      "Train Epoch: 197 [70080/225000 (31%)] Loss: 18401.300781\n",
      "Train Epoch: 197 [72576/225000 (32%)] Loss: 18938.210938\n",
      "Train Epoch: 197 [75072/225000 (33%)] Loss: 18781.632812\n",
      "Train Epoch: 197 [77568/225000 (34%)] Loss: 18779.630859\n",
      "Train Epoch: 197 [80064/225000 (36%)] Loss: 19067.101562\n",
      "Train Epoch: 197 [82560/225000 (37%)] Loss: 18435.632812\n",
      "Train Epoch: 197 [85056/225000 (38%)] Loss: 19220.203125\n",
      "Train Epoch: 197 [87552/225000 (39%)] Loss: 18371.400391\n",
      "Train Epoch: 197 [90048/225000 (40%)] Loss: 18728.914062\n",
      "Train Epoch: 197 [92544/225000 (41%)] Loss: 19124.937500\n",
      "Train Epoch: 197 [95040/225000 (42%)] Loss: 18705.550781\n",
      "Train Epoch: 197 [97536/225000 (43%)] Loss: 19149.347656\n",
      "Train Epoch: 197 [100032/225000 (44%)] Loss: 18999.716797\n",
      "Train Epoch: 197 [102528/225000 (46%)] Loss: 19044.871094\n",
      "Train Epoch: 197 [105024/225000 (47%)] Loss: 18939.470703\n",
      "Train Epoch: 197 [107520/225000 (48%)] Loss: 18982.500000\n",
      "Train Epoch: 197 [110016/225000 (49%)] Loss: 18413.800781\n",
      "Train Epoch: 197 [112512/225000 (50%)] Loss: 18566.878906\n",
      "Train Epoch: 197 [115008/225000 (51%)] Loss: 18488.269531\n",
      "Train Epoch: 197 [117504/225000 (52%)] Loss: 19128.949219\n",
      "Train Epoch: 197 [120000/225000 (53%)] Loss: 18901.769531\n",
      "Train Epoch: 197 [122496/225000 (54%)] Loss: 18874.773438\n",
      "Train Epoch: 197 [124992/225000 (56%)] Loss: 18526.880859\n",
      "Train Epoch: 197 [127488/225000 (57%)] Loss: 18395.785156\n",
      "Train Epoch: 197 [129984/225000 (58%)] Loss: 18717.246094\n",
      "Train Epoch: 197 [132480/225000 (59%)] Loss: 18720.500000\n",
      "Train Epoch: 197 [134976/225000 (60%)] Loss: 18476.664062\n",
      "Train Epoch: 197 [137472/225000 (61%)] Loss: 18478.384766\n",
      "Train Epoch: 197 [139968/225000 (62%)] Loss: 18672.062500\n",
      "Train Epoch: 197 [142464/225000 (63%)] Loss: 18843.384766\n",
      "Train Epoch: 197 [144960/225000 (64%)] Loss: 17882.882812\n",
      "Train Epoch: 197 [147456/225000 (66%)] Loss: 19056.035156\n",
      "Train Epoch: 197 [149952/225000 (67%)] Loss: 18593.244141\n",
      "Train Epoch: 197 [152448/225000 (68%)] Loss: 18640.505859\n",
      "Train Epoch: 197 [154944/225000 (69%)] Loss: 19362.298828\n",
      "Train Epoch: 197 [157440/225000 (70%)] Loss: 18568.548828\n",
      "Train Epoch: 197 [159936/225000 (71%)] Loss: 18672.070312\n",
      "Train Epoch: 197 [162432/225000 (72%)] Loss: 18299.816406\n",
      "Train Epoch: 197 [164928/225000 (73%)] Loss: 18984.160156\n",
      "Train Epoch: 197 [167424/225000 (74%)] Loss: 18244.476562\n",
      "Train Epoch: 197 [169920/225000 (76%)] Loss: 18713.267578\n",
      "Train Epoch: 197 [172416/225000 (77%)] Loss: 18770.863281\n",
      "Train Epoch: 197 [174912/225000 (78%)] Loss: 18800.789062\n",
      "Train Epoch: 197 [177408/225000 (79%)] Loss: 18618.685547\n",
      "Train Epoch: 197 [179904/225000 (80%)] Loss: 18594.328125\n",
      "Train Epoch: 197 [182400/225000 (81%)] Loss: 19394.320312\n",
      "Train Epoch: 197 [184896/225000 (82%)] Loss: 18596.621094\n",
      "Train Epoch: 197 [187392/225000 (83%)] Loss: 18829.248047\n",
      "Train Epoch: 197 [189888/225000 (84%)] Loss: 18649.839844\n",
      "Train Epoch: 197 [192384/225000 (86%)] Loss: 18869.378906\n",
      "Train Epoch: 197 [194880/225000 (87%)] Loss: 19014.281250\n",
      "Train Epoch: 197 [197376/225000 (88%)] Loss: 18985.195312\n",
      "Train Epoch: 197 [199872/225000 (89%)] Loss: 19338.902344\n",
      "Train Epoch: 197 [202368/225000 (90%)] Loss: 18802.744141\n",
      "Train Epoch: 197 [204864/225000 (91%)] Loss: 18470.949219\n",
      "Train Epoch: 197 [207360/225000 (92%)] Loss: 18669.712891\n",
      "Train Epoch: 197 [209856/225000 (93%)] Loss: 18547.859375\n",
      "Train Epoch: 197 [212352/225000 (94%)] Loss: 19060.554688\n",
      "Train Epoch: 197 [214848/225000 (95%)] Loss: 18867.085938\n",
      "Train Epoch: 197 [217344/225000 (97%)] Loss: 18795.433594\n",
      "Train Epoch: 197 [219840/225000 (98%)] Loss: 18478.394531\n",
      "Train Epoch: 197 [222336/225000 (99%)] Loss: 18949.808594\n",
      "Train Epoch: 197 [224832/225000 (100%)] Loss: 18493.193359\n",
      "    epoch          : 197\n",
      "    loss           : 18796.585027597055\n",
      "    val_loss       : 18664.679550626806\n",
      "Train Epoch: 198 [192/225000 (0%)] Loss: 18869.417969\n",
      "Train Epoch: 198 [2688/225000 (1%)] Loss: 18898.789062\n",
      "Train Epoch: 198 [5184/225000 (2%)] Loss: 18481.816406\n",
      "Train Epoch: 198 [7680/225000 (3%)] Loss: 18591.667969\n",
      "Train Epoch: 198 [10176/225000 (5%)] Loss: 18770.955078\n",
      "Train Epoch: 198 [12672/225000 (6%)] Loss: 18765.445312\n",
      "Train Epoch: 198 [15168/225000 (7%)] Loss: 18892.691406\n",
      "Train Epoch: 198 [17664/225000 (8%)] Loss: 18763.921875\n",
      "Train Epoch: 198 [20160/225000 (9%)] Loss: 18944.417969\n",
      "Train Epoch: 198 [22656/225000 (10%)] Loss: 18510.054688\n",
      "Train Epoch: 198 [25152/225000 (11%)] Loss: 18789.292969\n",
      "Train Epoch: 198 [27648/225000 (12%)] Loss: 18675.941406\n",
      "Train Epoch: 198 [30144/225000 (13%)] Loss: 18911.351562\n",
      "Train Epoch: 198 [32640/225000 (15%)] Loss: 19558.154297\n",
      "Train Epoch: 198 [35136/225000 (16%)] Loss: 18547.488281\n",
      "Train Epoch: 198 [37632/225000 (17%)] Loss: 18478.208984\n",
      "Train Epoch: 198 [40128/225000 (18%)] Loss: 23634.414062\n",
      "Train Epoch: 198 [42624/225000 (19%)] Loss: 18984.140625\n",
      "Train Epoch: 198 [45120/225000 (20%)] Loss: 18384.921875\n",
      "Train Epoch: 198 [47616/225000 (21%)] Loss: 18639.765625\n",
      "Train Epoch: 198 [50112/225000 (22%)] Loss: 18619.658203\n",
      "Train Epoch: 198 [52608/225000 (23%)] Loss: 18808.230469\n",
      "Train Epoch: 198 [55104/225000 (24%)] Loss: 18669.148438\n",
      "Train Epoch: 198 [57600/225000 (26%)] Loss: 19371.312500\n",
      "Train Epoch: 198 [60096/225000 (27%)] Loss: 18705.800781\n",
      "Train Epoch: 198 [62592/225000 (28%)] Loss: 18828.714844\n",
      "Train Epoch: 198 [65088/225000 (29%)] Loss: 18287.445312\n",
      "Train Epoch: 198 [67584/225000 (30%)] Loss: 19182.306641\n",
      "Train Epoch: 198 [70080/225000 (31%)] Loss: 18528.894531\n",
      "Train Epoch: 198 [72576/225000 (32%)] Loss: 19121.871094\n",
      "Train Epoch: 198 [75072/225000 (33%)] Loss: 19386.332031\n",
      "Train Epoch: 198 [77568/225000 (34%)] Loss: 18473.548828\n",
      "Train Epoch: 198 [80064/225000 (36%)] Loss: 19122.402344\n",
      "Train Epoch: 198 [82560/225000 (37%)] Loss: 19005.316406\n",
      "Train Epoch: 198 [85056/225000 (38%)] Loss: 18511.648438\n",
      "Train Epoch: 198 [87552/225000 (39%)] Loss: 18107.839844\n",
      "Train Epoch: 198 [90048/225000 (40%)] Loss: 18832.302734\n",
      "Train Epoch: 198 [92544/225000 (41%)] Loss: 18921.542969\n",
      "Train Epoch: 198 [95040/225000 (42%)] Loss: 19154.683594\n",
      "Train Epoch: 198 [97536/225000 (43%)] Loss: 18359.802734\n",
      "Train Epoch: 198 [100032/225000 (44%)] Loss: 18382.097656\n",
      "Train Epoch: 198 [102528/225000 (46%)] Loss: 18977.609375\n",
      "Train Epoch: 198 [105024/225000 (47%)] Loss: 18629.224609\n",
      "Train Epoch: 198 [107520/225000 (48%)] Loss: 18833.722656\n",
      "Train Epoch: 198 [110016/225000 (49%)] Loss: 18647.664062\n",
      "Train Epoch: 198 [112512/225000 (50%)] Loss: 18690.160156\n",
      "Train Epoch: 198 [115008/225000 (51%)] Loss: 18700.197266\n",
      "Train Epoch: 198 [117504/225000 (52%)] Loss: 19214.671875\n",
      "Train Epoch: 198 [120000/225000 (53%)] Loss: 18364.433594\n",
      "Train Epoch: 198 [122496/225000 (54%)] Loss: 18842.867188\n",
      "Train Epoch: 198 [124992/225000 (56%)] Loss: 18775.585938\n",
      "Train Epoch: 198 [127488/225000 (57%)] Loss: 18801.691406\n",
      "Train Epoch: 198 [129984/225000 (58%)] Loss: 19050.878906\n",
      "Train Epoch: 198 [132480/225000 (59%)] Loss: 18863.111328\n",
      "Train Epoch: 198 [134976/225000 (60%)] Loss: 18372.878906\n",
      "Train Epoch: 198 [137472/225000 (61%)] Loss: 18747.957031\n",
      "Train Epoch: 198 [139968/225000 (62%)] Loss: 18573.300781\n",
      "Train Epoch: 198 [142464/225000 (63%)] Loss: 18944.583984\n",
      "Train Epoch: 198 [144960/225000 (64%)] Loss: 18754.671875\n",
      "Train Epoch: 198 [147456/225000 (66%)] Loss: 18543.685547\n",
      "Train Epoch: 198 [149952/225000 (67%)] Loss: 19059.933594\n",
      "Train Epoch: 198 [152448/225000 (68%)] Loss: 18768.437500\n",
      "Train Epoch: 198 [154944/225000 (69%)] Loss: 18619.134766\n",
      "Train Epoch: 198 [157440/225000 (70%)] Loss: 18511.273438\n",
      "Train Epoch: 198 [159936/225000 (71%)] Loss: 18589.378906\n",
      "Train Epoch: 198 [162432/225000 (72%)] Loss: 18892.500000\n",
      "Train Epoch: 198 [164928/225000 (73%)] Loss: 18609.019531\n",
      "Train Epoch: 198 [167424/225000 (74%)] Loss: 18490.507812\n",
      "Train Epoch: 198 [169920/225000 (76%)] Loss: 18612.175781\n",
      "Train Epoch: 198 [172416/225000 (77%)] Loss: 18741.425781\n",
      "Train Epoch: 198 [174912/225000 (78%)] Loss: 18583.175781\n",
      "Train Epoch: 198 [177408/225000 (79%)] Loss: 19001.214844\n",
      "Train Epoch: 198 [179904/225000 (80%)] Loss: 18379.632812\n",
      "Train Epoch: 198 [182400/225000 (81%)] Loss: 19267.968750\n",
      "Train Epoch: 198 [184896/225000 (82%)] Loss: 18812.408203\n",
      "Train Epoch: 198 [187392/225000 (83%)] Loss: 18454.544922\n",
      "Train Epoch: 198 [189888/225000 (84%)] Loss: 18318.109375\n",
      "Train Epoch: 198 [192384/225000 (86%)] Loss: 18860.488281\n",
      "Train Epoch: 198 [194880/225000 (87%)] Loss: 18870.259766\n",
      "Train Epoch: 198 [197376/225000 (88%)] Loss: 19090.412109\n",
      "Train Epoch: 198 [199872/225000 (89%)] Loss: 18683.279297\n",
      "Train Epoch: 198 [202368/225000 (90%)] Loss: 18588.792969\n",
      "Train Epoch: 198 [204864/225000 (91%)] Loss: 18627.119141\n",
      "Train Epoch: 198 [207360/225000 (92%)] Loss: 18199.089844\n",
      "Train Epoch: 198 [209856/225000 (93%)] Loss: 18881.730469\n",
      "Train Epoch: 198 [212352/225000 (94%)] Loss: 18892.648438\n",
      "Train Epoch: 198 [214848/225000 (95%)] Loss: 18823.271484\n",
      "Train Epoch: 198 [217344/225000 (97%)] Loss: 19303.031250\n",
      "Train Epoch: 198 [219840/225000 (98%)] Loss: 18381.992188\n",
      "Train Epoch: 198 [222336/225000 (99%)] Loss: 18782.531250\n",
      "Train Epoch: 198 [224832/225000 (100%)] Loss: 19257.003906\n",
      "    epoch          : 198\n",
      "    loss           : 18757.24621707018\n",
      "    val_loss       : 18647.240330426746\n",
      "Train Epoch: 199 [192/225000 (0%)] Loss: 18998.890625\n",
      "Train Epoch: 199 [2688/225000 (1%)] Loss: 18700.097656\n",
      "Train Epoch: 199 [5184/225000 (2%)] Loss: 18682.640625\n",
      "Train Epoch: 199 [7680/225000 (3%)] Loss: 18695.167969\n",
      "Train Epoch: 199 [10176/225000 (5%)] Loss: 18817.013672\n",
      "Train Epoch: 199 [12672/225000 (6%)] Loss: 19154.285156\n",
      "Train Epoch: 199 [15168/225000 (7%)] Loss: 18700.462891\n",
      "Train Epoch: 199 [17664/225000 (8%)] Loss: 19146.582031\n",
      "Train Epoch: 199 [20160/225000 (9%)] Loss: 18941.125000\n",
      "Train Epoch: 199 [22656/225000 (10%)] Loss: 19028.039062\n",
      "Train Epoch: 199 [25152/225000 (11%)] Loss: 18800.876953\n",
      "Train Epoch: 199 [27648/225000 (12%)] Loss: 18340.750000\n",
      "Train Epoch: 199 [30144/225000 (13%)] Loss: 18707.367188\n",
      "Train Epoch: 199 [32640/225000 (15%)] Loss: 18604.855469\n",
      "Train Epoch: 199 [35136/225000 (16%)] Loss: 18451.125000\n",
      "Train Epoch: 199 [37632/225000 (17%)] Loss: 18983.498047\n",
      "Train Epoch: 199 [40128/225000 (18%)] Loss: 18783.300781\n",
      "Train Epoch: 199 [42624/225000 (19%)] Loss: 18378.933594\n",
      "Train Epoch: 199 [45120/225000 (20%)] Loss: 18391.775391\n",
      "Train Epoch: 199 [47616/225000 (21%)] Loss: 18305.933594\n",
      "Train Epoch: 199 [50112/225000 (22%)] Loss: 18590.816406\n",
      "Train Epoch: 199 [52608/225000 (23%)] Loss: 18525.207031\n",
      "Train Epoch: 199 [55104/225000 (24%)] Loss: 18706.996094\n",
      "Train Epoch: 199 [57600/225000 (26%)] Loss: 18767.185547\n",
      "Train Epoch: 199 [60096/225000 (27%)] Loss: 19279.777344\n",
      "Train Epoch: 199 [62592/225000 (28%)] Loss: 18398.890625\n",
      "Train Epoch: 199 [65088/225000 (29%)] Loss: 18567.437500\n",
      "Train Epoch: 199 [67584/225000 (30%)] Loss: 18575.445312\n",
      "Train Epoch: 199 [70080/225000 (31%)] Loss: 19053.808594\n",
      "Train Epoch: 199 [72576/225000 (32%)] Loss: 19369.457031\n",
      "Train Epoch: 199 [75072/225000 (33%)] Loss: 18719.609375\n",
      "Train Epoch: 199 [77568/225000 (34%)] Loss: 18332.277344\n",
      "Train Epoch: 199 [80064/225000 (36%)] Loss: 18927.316406\n",
      "Train Epoch: 199 [82560/225000 (37%)] Loss: 18725.171875\n",
      "Train Epoch: 199 [85056/225000 (38%)] Loss: 18577.671875\n",
      "Train Epoch: 199 [87552/225000 (39%)] Loss: 18481.488281\n",
      "Train Epoch: 199 [90048/225000 (40%)] Loss: 18440.570312\n",
      "Train Epoch: 199 [92544/225000 (41%)] Loss: 19150.410156\n",
      "Train Epoch: 199 [95040/225000 (42%)] Loss: 18147.478516\n",
      "Train Epoch: 199 [97536/225000 (43%)] Loss: 18803.976562\n",
      "Train Epoch: 199 [100032/225000 (44%)] Loss: 18595.808594\n",
      "Train Epoch: 199 [102528/225000 (46%)] Loss: 19178.714844\n",
      "Train Epoch: 199 [105024/225000 (47%)] Loss: 18618.050781\n",
      "Train Epoch: 199 [107520/225000 (48%)] Loss: 18711.726562\n",
      "Train Epoch: 199 [110016/225000 (49%)] Loss: 18779.072266\n",
      "Train Epoch: 199 [112512/225000 (50%)] Loss: 18787.039062\n",
      "Train Epoch: 199 [115008/225000 (51%)] Loss: 18860.982422\n",
      "Train Epoch: 199 [117504/225000 (52%)] Loss: 18856.582031\n",
      "Train Epoch: 199 [120000/225000 (53%)] Loss: 18925.412109\n",
      "Train Epoch: 199 [122496/225000 (54%)] Loss: 18884.804688\n",
      "Train Epoch: 199 [124992/225000 (56%)] Loss: 18716.921875\n",
      "Train Epoch: 199 [127488/225000 (57%)] Loss: 18855.712891\n",
      "Train Epoch: 199 [129984/225000 (58%)] Loss: 18657.011719\n",
      "Train Epoch: 199 [132480/225000 (59%)] Loss: 18946.878906\n",
      "Train Epoch: 199 [134976/225000 (60%)] Loss: 18997.246094\n",
      "Train Epoch: 199 [137472/225000 (61%)] Loss: 18947.683594\n",
      "Train Epoch: 199 [139968/225000 (62%)] Loss: 18480.839844\n",
      "Train Epoch: 199 [142464/225000 (63%)] Loss: 18577.863281\n",
      "Train Epoch: 199 [144960/225000 (64%)] Loss: 18455.267578\n",
      "Train Epoch: 199 [147456/225000 (66%)] Loss: 18636.574219\n",
      "Train Epoch: 199 [149952/225000 (67%)] Loss: 18426.500000\n",
      "Train Epoch: 199 [152448/225000 (68%)] Loss: 18773.281250\n",
      "Train Epoch: 199 [154944/225000 (69%)] Loss: 18740.789062\n",
      "Train Epoch: 199 [157440/225000 (70%)] Loss: 19013.281250\n",
      "Train Epoch: 199 [159936/225000 (71%)] Loss: 18871.953125\n",
      "Train Epoch: 199 [162432/225000 (72%)] Loss: 18712.855469\n",
      "Train Epoch: 199 [164928/225000 (73%)] Loss: 19017.078125\n",
      "Train Epoch: 199 [167424/225000 (74%)] Loss: 18520.378906\n",
      "Train Epoch: 199 [169920/225000 (76%)] Loss: 18862.765625\n",
      "Train Epoch: 199 [172416/225000 (77%)] Loss: 18700.445312\n",
      "Train Epoch: 199 [174912/225000 (78%)] Loss: 19068.894531\n",
      "Train Epoch: 199 [177408/225000 (79%)] Loss: 18852.402344\n",
      "Train Epoch: 199 [179904/225000 (80%)] Loss: 18889.654297\n",
      "Train Epoch: 199 [182400/225000 (81%)] Loss: 18745.111328\n",
      "Train Epoch: 199 [184896/225000 (82%)] Loss: 19241.503906\n",
      "Train Epoch: 199 [187392/225000 (83%)] Loss: 18317.324219\n",
      "Train Epoch: 199 [189888/225000 (84%)] Loss: 18492.996094\n",
      "Train Epoch: 199 [192384/225000 (86%)] Loss: 18750.144531\n",
      "Train Epoch: 199 [194880/225000 (87%)] Loss: 18980.105469\n",
      "Train Epoch: 199 [197376/225000 (88%)] Loss: 19166.203125\n",
      "Train Epoch: 199 [199872/225000 (89%)] Loss: 18712.109375\n",
      "Train Epoch: 199 [202368/225000 (90%)] Loss: 18473.613281\n",
      "Train Epoch: 199 [204864/225000 (91%)] Loss: 18985.224609\n",
      "Train Epoch: 199 [207360/225000 (92%)] Loss: 18774.095703\n",
      "Train Epoch: 199 [209856/225000 (93%)] Loss: 18943.734375\n",
      "Train Epoch: 199 [212352/225000 (94%)] Loss: 18549.949219\n",
      "Train Epoch: 199 [214848/225000 (95%)] Loss: 18549.384766\n",
      "Train Epoch: 199 [217344/225000 (97%)] Loss: 18731.691406\n",
      "Train Epoch: 199 [219840/225000 (98%)] Loss: 18970.900391\n",
      "Train Epoch: 199 [222336/225000 (99%)] Loss: 19162.402344\n",
      "Train Epoch: 199 [224832/225000 (100%)] Loss: 18993.337891\n",
      "    epoch          : 199\n",
      "    loss           : 18730.752251426515\n",
      "    val_loss       : 18640.636613048217\n",
      "Train Epoch: 200 [192/225000 (0%)] Loss: 18379.876953\n",
      "Train Epoch: 200 [2688/225000 (1%)] Loss: 18968.902344\n",
      "Train Epoch: 200 [5184/225000 (2%)] Loss: 18476.732422\n",
      "Train Epoch: 200 [7680/225000 (3%)] Loss: 18537.859375\n",
      "Train Epoch: 200 [10176/225000 (5%)] Loss: 18773.675781\n",
      "Train Epoch: 200 [12672/225000 (6%)] Loss: 18691.281250\n",
      "Train Epoch: 200 [15168/225000 (7%)] Loss: 18862.964844\n",
      "Train Epoch: 200 [17664/225000 (8%)] Loss: 18493.404297\n",
      "Train Epoch: 200 [20160/225000 (9%)] Loss: 18700.919922\n",
      "Train Epoch: 200 [22656/225000 (10%)] Loss: 18855.796875\n",
      "Train Epoch: 200 [25152/225000 (11%)] Loss: 18989.406250\n",
      "Train Epoch: 200 [27648/225000 (12%)] Loss: 18957.757812\n",
      "Train Epoch: 200 [30144/225000 (13%)] Loss: 18651.511719\n",
      "Train Epoch: 200 [32640/225000 (15%)] Loss: 17718.771484\n",
      "Train Epoch: 200 [35136/225000 (16%)] Loss: 18579.298828\n",
      "Train Epoch: 200 [37632/225000 (17%)] Loss: 18538.812500\n",
      "Train Epoch: 200 [40128/225000 (18%)] Loss: 18199.373047\n",
      "Train Epoch: 200 [42624/225000 (19%)] Loss: 18943.906250\n",
      "Train Epoch: 200 [45120/225000 (20%)] Loss: 18415.027344\n",
      "Train Epoch: 200 [47616/225000 (21%)] Loss: 18407.853516\n",
      "Train Epoch: 200 [50112/225000 (22%)] Loss: 18383.476562\n",
      "Train Epoch: 200 [52608/225000 (23%)] Loss: 18971.667969\n",
      "Train Epoch: 200 [55104/225000 (24%)] Loss: 19162.097656\n",
      "Train Epoch: 200 [57600/225000 (26%)] Loss: 18506.945312\n",
      "Train Epoch: 200 [60096/225000 (27%)] Loss: 18798.041016\n",
      "Train Epoch: 200 [62592/225000 (28%)] Loss: 18729.066406\n",
      "Train Epoch: 200 [65088/225000 (29%)] Loss: 18367.726562\n",
      "Train Epoch: 200 [67584/225000 (30%)] Loss: 18987.289062\n",
      "Train Epoch: 200 [70080/225000 (31%)] Loss: 18670.980469\n",
      "Train Epoch: 200 [72576/225000 (32%)] Loss: 17976.693359\n",
      "Train Epoch: 200 [75072/225000 (33%)] Loss: 18915.984375\n",
      "Train Epoch: 200 [77568/225000 (34%)] Loss: 18513.488281\n",
      "Train Epoch: 200 [80064/225000 (36%)] Loss: 19094.255859\n",
      "Train Epoch: 200 [82560/225000 (37%)] Loss: 18421.316406\n",
      "Train Epoch: 200 [85056/225000 (38%)] Loss: 19221.812500\n",
      "Train Epoch: 200 [87552/225000 (39%)] Loss: 18529.164062\n",
      "Train Epoch: 200 [90048/225000 (40%)] Loss: 18926.117188\n",
      "Train Epoch: 200 [92544/225000 (41%)] Loss: 18633.839844\n",
      "Train Epoch: 200 [95040/225000 (42%)] Loss: 18641.125000\n",
      "Train Epoch: 200 [97536/225000 (43%)] Loss: 19291.794922\n",
      "Train Epoch: 200 [100032/225000 (44%)] Loss: 19114.103516\n",
      "Train Epoch: 200 [102528/225000 (46%)] Loss: 18828.710938\n",
      "Train Epoch: 200 [105024/225000 (47%)] Loss: 18430.974609\n",
      "Train Epoch: 200 [107520/225000 (48%)] Loss: 18951.699219\n",
      "Train Epoch: 200 [110016/225000 (49%)] Loss: 18237.574219\n",
      "Train Epoch: 200 [112512/225000 (50%)] Loss: 18470.402344\n",
      "Train Epoch: 200 [115008/225000 (51%)] Loss: 18402.755859\n",
      "Train Epoch: 200 [117504/225000 (52%)] Loss: 18774.542969\n",
      "Train Epoch: 200 [120000/225000 (53%)] Loss: 18628.441406\n",
      "Train Epoch: 200 [122496/225000 (54%)] Loss: 18825.960938\n",
      "Train Epoch: 200 [124992/225000 (56%)] Loss: 19004.406250\n",
      "Train Epoch: 200 [127488/225000 (57%)] Loss: 18280.423828\n",
      "Train Epoch: 200 [129984/225000 (58%)] Loss: 18374.326172\n",
      "Train Epoch: 200 [132480/225000 (59%)] Loss: 19041.199219\n",
      "Train Epoch: 200 [134976/225000 (60%)] Loss: 18792.765625\n",
      "Train Epoch: 200 [137472/225000 (61%)] Loss: 18480.867188\n",
      "Train Epoch: 200 [139968/225000 (62%)] Loss: 18298.054688\n",
      "Train Epoch: 200 [142464/225000 (63%)] Loss: 18748.517578\n",
      "Train Epoch: 200 [144960/225000 (64%)] Loss: 18615.275391\n",
      "Train Epoch: 200 [147456/225000 (66%)] Loss: 18221.332031\n",
      "Train Epoch: 200 [149952/225000 (67%)] Loss: 18590.398438\n",
      "Train Epoch: 200 [152448/225000 (68%)] Loss: 18500.220703\n",
      "Train Epoch: 200 [154944/225000 (69%)] Loss: 18844.732422\n",
      "Train Epoch: 200 [157440/225000 (70%)] Loss: 18991.480469\n",
      "Train Epoch: 200 [159936/225000 (71%)] Loss: 18409.468750\n",
      "Train Epoch: 200 [162432/225000 (72%)] Loss: 18487.468750\n",
      "Train Epoch: 200 [164928/225000 (73%)] Loss: 18444.843750\n",
      "Train Epoch: 200 [167424/225000 (74%)] Loss: 18583.427734\n",
      "Train Epoch: 200 [169920/225000 (76%)] Loss: 18569.402344\n",
      "Train Epoch: 200 [172416/225000 (77%)] Loss: 18783.777344\n",
      "Train Epoch: 200 [174912/225000 (78%)] Loss: 18894.640625\n",
      "Train Epoch: 200 [177408/225000 (79%)] Loss: 18755.017578\n",
      "Train Epoch: 200 [179904/225000 (80%)] Loss: 17964.048828\n",
      "Train Epoch: 200 [182400/225000 (81%)] Loss: 18827.449219\n",
      "Train Epoch: 200 [184896/225000 (82%)] Loss: 18632.636719\n",
      "Train Epoch: 200 [187392/225000 (83%)] Loss: 18828.626953\n",
      "Train Epoch: 200 [189888/225000 (84%)] Loss: 18640.628906\n",
      "Train Epoch: 200 [192384/225000 (86%)] Loss: 18326.488281\n",
      "Train Epoch: 200 [194880/225000 (87%)] Loss: 18933.681641\n",
      "Train Epoch: 200 [197376/225000 (88%)] Loss: 18892.671875\n",
      "Train Epoch: 200 [199872/225000 (89%)] Loss: 18265.683594\n",
      "Train Epoch: 200 [202368/225000 (90%)] Loss: 18742.988281\n",
      "Train Epoch: 200 [204864/225000 (91%)] Loss: 18559.308594\n",
      "Train Epoch: 200 [207360/225000 (92%)] Loss: 18203.695312\n",
      "Train Epoch: 200 [209856/225000 (93%)] Loss: 18888.824219\n",
      "Train Epoch: 200 [212352/225000 (94%)] Loss: 18945.246094\n",
      "Train Epoch: 200 [214848/225000 (95%)] Loss: 18657.630859\n",
      "Train Epoch: 200 [217344/225000 (97%)] Loss: 18506.984375\n",
      "Train Epoch: 200 [219840/225000 (98%)] Loss: 18511.951172\n",
      "Train Epoch: 200 [222336/225000 (99%)] Loss: 18486.207031\n",
      "Train Epoch: 200 [224832/225000 (100%)] Loss: 19063.097656\n",
      "    epoch          : 200\n",
      "    loss           : 18699.260668028743\n",
      "    val_loss       : 18590.398063863053\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0804_123131/checkpoint-epoch200.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 201 [192/225000 (0%)] Loss: 18690.900391\n",
      "Train Epoch: 201 [2688/225000 (1%)] Loss: 18638.109375\n",
      "Train Epoch: 201 [5184/225000 (2%)] Loss: 18696.578125\n",
      "Train Epoch: 201 [7680/225000 (3%)] Loss: 18162.654297\n",
      "Train Epoch: 201 [10176/225000 (5%)] Loss: 19150.910156\n",
      "Train Epoch: 201 [12672/225000 (6%)] Loss: 18182.281250\n",
      "Train Epoch: 201 [15168/225000 (7%)] Loss: 18992.267578\n",
      "Train Epoch: 201 [17664/225000 (8%)] Loss: 19189.816406\n",
      "Train Epoch: 201 [20160/225000 (9%)] Loss: 18865.453125\n",
      "Train Epoch: 201 [22656/225000 (10%)] Loss: 18785.078125\n",
      "Train Epoch: 201 [25152/225000 (11%)] Loss: 18783.386719\n",
      "Train Epoch: 201 [27648/225000 (12%)] Loss: 19082.974609\n",
      "Train Epoch: 201 [30144/225000 (13%)] Loss: 18642.031250\n",
      "Train Epoch: 201 [32640/225000 (15%)] Loss: 18327.394531\n",
      "Train Epoch: 201 [35136/225000 (16%)] Loss: 18479.322266\n",
      "Train Epoch: 201 [37632/225000 (17%)] Loss: 19128.238281\n",
      "Train Epoch: 201 [40128/225000 (18%)] Loss: 18718.361328\n",
      "Train Epoch: 201 [42624/225000 (19%)] Loss: 18855.593750\n",
      "Train Epoch: 201 [45120/225000 (20%)] Loss: 18863.658203\n",
      "Train Epoch: 201 [47616/225000 (21%)] Loss: 17908.392578\n",
      "Train Epoch: 201 [50112/225000 (22%)] Loss: 18881.371094\n",
      "Train Epoch: 201 [52608/225000 (23%)] Loss: 18555.654297\n",
      "Train Epoch: 201 [55104/225000 (24%)] Loss: 18759.664062\n",
      "Train Epoch: 201 [57600/225000 (26%)] Loss: 18482.023438\n",
      "Train Epoch: 201 [60096/225000 (27%)] Loss: 18803.953125\n",
      "Train Epoch: 201 [62592/225000 (28%)] Loss: 18674.970703\n",
      "Train Epoch: 201 [65088/225000 (29%)] Loss: 18460.828125\n",
      "Train Epoch: 201 [67584/225000 (30%)] Loss: 18643.066406\n",
      "Train Epoch: 201 [70080/225000 (31%)] Loss: 18557.056641\n",
      "Train Epoch: 201 [72576/225000 (32%)] Loss: 18623.591797\n",
      "Train Epoch: 201 [75072/225000 (33%)] Loss: 18627.568359\n",
      "Train Epoch: 201 [77568/225000 (34%)] Loss: 18057.164062\n",
      "Train Epoch: 201 [80064/225000 (36%)] Loss: 18715.652344\n",
      "Train Epoch: 201 [82560/225000 (37%)] Loss: 18326.648438\n",
      "Train Epoch: 201 [85056/225000 (38%)] Loss: 18601.322266\n",
      "Train Epoch: 201 [87552/225000 (39%)] Loss: 18972.546875\n",
      "Train Epoch: 201 [90048/225000 (40%)] Loss: 18447.019531\n",
      "Train Epoch: 201 [92544/225000 (41%)] Loss: 17971.671875\n",
      "Train Epoch: 201 [95040/225000 (42%)] Loss: 19000.097656\n",
      "Train Epoch: 201 [97536/225000 (43%)] Loss: 18901.554688\n",
      "Train Epoch: 201 [100032/225000 (44%)] Loss: 18699.562500\n",
      "Train Epoch: 201 [102528/225000 (46%)] Loss: 19023.707031\n",
      "Train Epoch: 201 [105024/225000 (47%)] Loss: 18868.457031\n",
      "Train Epoch: 201 [107520/225000 (48%)] Loss: 18637.218750\n",
      "Train Epoch: 201 [110016/225000 (49%)] Loss: 18896.933594\n",
      "Train Epoch: 201 [112512/225000 (50%)] Loss: 18878.363281\n",
      "Train Epoch: 201 [115008/225000 (51%)] Loss: 19140.257812\n",
      "Train Epoch: 201 [117504/225000 (52%)] Loss: 18717.953125\n",
      "Train Epoch: 201 [120000/225000 (53%)] Loss: 18461.242188\n",
      "Train Epoch: 201 [122496/225000 (54%)] Loss: 18905.757812\n",
      "Train Epoch: 201 [124992/225000 (56%)] Loss: 18651.882812\n",
      "Train Epoch: 201 [127488/225000 (57%)] Loss: 18940.837891\n",
      "Train Epoch: 201 [129984/225000 (58%)] Loss: 19182.712891\n",
      "Train Epoch: 201 [132480/225000 (59%)] Loss: 18635.117188\n",
      "Train Epoch: 201 [134976/225000 (60%)] Loss: 18534.281250\n",
      "Train Epoch: 201 [137472/225000 (61%)] Loss: 18676.585938\n",
      "Train Epoch: 201 [139968/225000 (62%)] Loss: 18678.341797\n",
      "Train Epoch: 201 [142464/225000 (63%)] Loss: 18872.046875\n",
      "Train Epoch: 201 [144960/225000 (64%)] Loss: 18437.072266\n",
      "Train Epoch: 201 [147456/225000 (66%)] Loss: 18835.535156\n",
      "Train Epoch: 201 [149952/225000 (67%)] Loss: 18035.343750\n",
      "Train Epoch: 201 [152448/225000 (68%)] Loss: 18553.148438\n",
      "Train Epoch: 201 [154944/225000 (69%)] Loss: 18725.662109\n",
      "Train Epoch: 201 [157440/225000 (70%)] Loss: 18635.980469\n",
      "Train Epoch: 201 [159936/225000 (71%)] Loss: 18591.136719\n",
      "Train Epoch: 201 [162432/225000 (72%)] Loss: 19041.167969\n",
      "Train Epoch: 201 [164928/225000 (73%)] Loss: 19012.800781\n",
      "Train Epoch: 201 [167424/225000 (74%)] Loss: 18131.324219\n",
      "Train Epoch: 201 [169920/225000 (76%)] Loss: 18452.267578\n",
      "Train Epoch: 201 [172416/225000 (77%)] Loss: 18665.820312\n",
      "Train Epoch: 201 [174912/225000 (78%)] Loss: 18582.408203\n",
      "Train Epoch: 201 [177408/225000 (79%)] Loss: 18704.345703\n",
      "Train Epoch: 201 [179904/225000 (80%)] Loss: 18650.066406\n",
      "Train Epoch: 201 [182400/225000 (81%)] Loss: 18508.437500\n",
      "Train Epoch: 201 [184896/225000 (82%)] Loss: 18579.320312\n",
      "Train Epoch: 201 [187392/225000 (83%)] Loss: 18666.716797\n",
      "Train Epoch: 201 [189888/225000 (84%)] Loss: 18891.558594\n",
      "Train Epoch: 201 [192384/225000 (86%)] Loss: 19031.523438\n",
      "Train Epoch: 201 [194880/225000 (87%)] Loss: 18324.679688\n",
      "Train Epoch: 201 [197376/225000 (88%)] Loss: 19061.570312\n",
      "Train Epoch: 201 [199872/225000 (89%)] Loss: 18380.935547\n",
      "Train Epoch: 201 [202368/225000 (90%)] Loss: 18657.175781\n",
      "Train Epoch: 201 [204864/225000 (91%)] Loss: 18251.187500\n",
      "Train Epoch: 201 [207360/225000 (92%)] Loss: 18369.052734\n",
      "Train Epoch: 201 [209856/225000 (93%)] Loss: 18900.882812\n",
      "Train Epoch: 201 [212352/225000 (94%)] Loss: 18839.453125\n",
      "Train Epoch: 201 [214848/225000 (95%)] Loss: 18828.160156\n",
      "Train Epoch: 201 [217344/225000 (97%)] Loss: 18749.613281\n",
      "Train Epoch: 201 [219840/225000 (98%)] Loss: 18418.162109\n",
      "Train Epoch: 201 [222336/225000 (99%)] Loss: 18673.921875\n",
      "Train Epoch: 201 [224832/225000 (100%)] Loss: 18799.085938\n",
      "    epoch          : 201\n",
      "    loss           : 18676.19789389132\n",
      "    val_loss       : 18559.111530165635\n",
      "Train Epoch: 202 [192/225000 (0%)] Loss: 18540.320312\n",
      "Train Epoch: 202 [2688/225000 (1%)] Loss: 18578.878906\n",
      "Train Epoch: 202 [5184/225000 (2%)] Loss: 18630.261719\n",
      "Train Epoch: 202 [7680/225000 (3%)] Loss: 18681.328125\n",
      "Train Epoch: 202 [10176/225000 (5%)] Loss: 19257.279297\n",
      "Train Epoch: 202 [12672/225000 (6%)] Loss: 18100.863281\n",
      "Train Epoch: 202 [15168/225000 (7%)] Loss: 18730.876953\n",
      "Train Epoch: 202 [17664/225000 (8%)] Loss: 18650.968750\n",
      "Train Epoch: 202 [20160/225000 (9%)] Loss: 18539.574219\n",
      "Train Epoch: 202 [22656/225000 (10%)] Loss: 18629.003906\n",
      "Train Epoch: 202 [25152/225000 (11%)] Loss: 18184.464844\n",
      "Train Epoch: 202 [27648/225000 (12%)] Loss: 18592.007812\n",
      "Train Epoch: 202 [30144/225000 (13%)] Loss: 18480.800781\n",
      "Train Epoch: 202 [32640/225000 (15%)] Loss: 18566.607422\n",
      "Train Epoch: 202 [35136/225000 (16%)] Loss: 18655.421875\n",
      "Train Epoch: 202 [37632/225000 (17%)] Loss: 18341.687500\n",
      "Train Epoch: 202 [40128/225000 (18%)] Loss: 18985.802734\n",
      "Train Epoch: 202 [42624/225000 (19%)] Loss: 18555.628906\n",
      "Train Epoch: 202 [45120/225000 (20%)] Loss: 18611.828125\n",
      "Train Epoch: 202 [47616/225000 (21%)] Loss: 18767.597656\n",
      "Train Epoch: 202 [50112/225000 (22%)] Loss: 18876.792969\n",
      "Train Epoch: 202 [52608/225000 (23%)] Loss: 18582.041016\n",
      "Train Epoch: 202 [55104/225000 (24%)] Loss: 18904.117188\n",
      "Train Epoch: 202 [57600/225000 (26%)] Loss: 18171.988281\n",
      "Train Epoch: 202 [60096/225000 (27%)] Loss: 19019.250000\n",
      "Train Epoch: 202 [62592/225000 (28%)] Loss: 19014.507812\n",
      "Train Epoch: 202 [65088/225000 (29%)] Loss: 18320.667969\n",
      "Train Epoch: 202 [67584/225000 (30%)] Loss: 18395.708984\n",
      "Train Epoch: 202 [70080/225000 (31%)] Loss: 18434.017578\n",
      "Train Epoch: 202 [72576/225000 (32%)] Loss: 19028.746094\n",
      "Train Epoch: 202 [75072/225000 (33%)] Loss: 18201.921875\n",
      "Train Epoch: 202 [77568/225000 (34%)] Loss: 19152.759766\n",
      "Train Epoch: 202 [80064/225000 (36%)] Loss: 18662.927734\n",
      "Train Epoch: 202 [82560/225000 (37%)] Loss: 18711.574219\n",
      "Train Epoch: 202 [85056/225000 (38%)] Loss: 18333.132812\n",
      "Train Epoch: 202 [87552/225000 (39%)] Loss: 18462.189453\n",
      "Train Epoch: 202 [90048/225000 (40%)] Loss: 18309.224609\n",
      "Train Epoch: 202 [92544/225000 (41%)] Loss: 18705.753906\n",
      "Train Epoch: 202 [95040/225000 (42%)] Loss: 18586.394531\n",
      "Train Epoch: 202 [97536/225000 (43%)] Loss: 18345.464844\n",
      "Train Epoch: 202 [100032/225000 (44%)] Loss: 18837.701172\n",
      "Train Epoch: 202 [102528/225000 (46%)] Loss: 19172.119141\n",
      "Train Epoch: 202 [105024/225000 (47%)] Loss: 19143.003906\n",
      "Train Epoch: 202 [107520/225000 (48%)] Loss: 18176.886719\n",
      "Train Epoch: 202 [110016/225000 (49%)] Loss: 18215.843750\n",
      "Train Epoch: 202 [112512/225000 (50%)] Loss: 18831.605469\n",
      "Train Epoch: 202 [115008/225000 (51%)] Loss: 18693.511719\n",
      "Train Epoch: 202 [117504/225000 (52%)] Loss: 18551.457031\n",
      "Train Epoch: 202 [120000/225000 (53%)] Loss: 18701.093750\n",
      "Train Epoch: 202 [122496/225000 (54%)] Loss: 18573.726562\n",
      "Train Epoch: 202 [124992/225000 (56%)] Loss: 18763.054688\n",
      "Train Epoch: 202 [127488/225000 (57%)] Loss: 18689.597656\n",
      "Train Epoch: 202 [129984/225000 (58%)] Loss: 19020.060547\n",
      "Train Epoch: 202 [132480/225000 (59%)] Loss: 18678.808594\n",
      "Train Epoch: 202 [134976/225000 (60%)] Loss: 18549.568359\n",
      "Train Epoch: 202 [137472/225000 (61%)] Loss: 18938.601562\n",
      "Train Epoch: 202 [139968/225000 (62%)] Loss: 18505.853516\n",
      "Train Epoch: 202 [142464/225000 (63%)] Loss: 18236.587891\n",
      "Train Epoch: 202 [144960/225000 (64%)] Loss: 18804.513672\n",
      "Train Epoch: 202 [147456/225000 (66%)] Loss: 18862.880859\n",
      "Train Epoch: 202 [149952/225000 (67%)] Loss: 18635.216797\n",
      "Train Epoch: 202 [152448/225000 (68%)] Loss: 18580.585938\n",
      "Train Epoch: 202 [154944/225000 (69%)] Loss: 18518.716797\n",
      "Train Epoch: 202 [157440/225000 (70%)] Loss: 18727.841797\n",
      "Train Epoch: 202 [159936/225000 (71%)] Loss: 18802.927734\n",
      "Train Epoch: 202 [162432/225000 (72%)] Loss: 18745.660156\n",
      "Train Epoch: 202 [164928/225000 (73%)] Loss: 18693.056641\n",
      "Train Epoch: 202 [167424/225000 (74%)] Loss: 18266.064453\n",
      "Train Epoch: 202 [169920/225000 (76%)] Loss: 18381.458984\n",
      "Train Epoch: 202 [172416/225000 (77%)] Loss: 18744.707031\n",
      "Train Epoch: 202 [174912/225000 (78%)] Loss: 18938.914062\n",
      "Train Epoch: 202 [177408/225000 (79%)] Loss: 18380.792969\n",
      "Train Epoch: 202 [179904/225000 (80%)] Loss: 18406.626953\n",
      "Train Epoch: 202 [182400/225000 (81%)] Loss: 18698.505859\n",
      "Train Epoch: 202 [184896/225000 (82%)] Loss: 18607.695312\n",
      "Train Epoch: 202 [187392/225000 (83%)] Loss: 18733.792969\n",
      "Train Epoch: 202 [189888/225000 (84%)] Loss: 18511.861328\n",
      "Train Epoch: 202 [192384/225000 (86%)] Loss: 18490.035156\n",
      "Train Epoch: 202 [194880/225000 (87%)] Loss: 18570.367188\n",
      "Train Epoch: 202 [197376/225000 (88%)] Loss: 18020.849609\n",
      "Train Epoch: 202 [199872/225000 (89%)] Loss: 18413.406250\n",
      "Train Epoch: 202 [202368/225000 (90%)] Loss: 18509.316406\n",
      "Train Epoch: 202 [204864/225000 (91%)] Loss: 18940.724609\n",
      "Train Epoch: 202 [207360/225000 (92%)] Loss: 18668.281250\n",
      "Train Epoch: 202 [209856/225000 (93%)] Loss: 18701.972656\n",
      "Train Epoch: 202 [212352/225000 (94%)] Loss: 18785.515625\n",
      "Train Epoch: 202 [214848/225000 (95%)] Loss: 18747.113281\n",
      "Train Epoch: 202 [217344/225000 (97%)] Loss: 18967.271484\n",
      "Train Epoch: 202 [219840/225000 (98%)] Loss: 18181.724609\n",
      "Train Epoch: 202 [222336/225000 (99%)] Loss: 18824.496094\n",
      "Train Epoch: 202 [224832/225000 (100%)] Loss: 18524.369141\n",
      "    epoch          : 202\n",
      "    loss           : 18639.860983161798\n",
      "    val_loss       : 18533.15288904969\n",
      "Train Epoch: 203 [192/225000 (0%)] Loss: 18615.201172\n",
      "Train Epoch: 203 [2688/225000 (1%)] Loss: 18774.941406\n",
      "Train Epoch: 203 [5184/225000 (2%)] Loss: 18578.066406\n",
      "Train Epoch: 203 [7680/225000 (3%)] Loss: 18440.474609\n",
      "Train Epoch: 203 [10176/225000 (5%)] Loss: 18865.154297\n",
      "Train Epoch: 203 [12672/225000 (6%)] Loss: 18975.369141\n",
      "Train Epoch: 203 [15168/225000 (7%)] Loss: 18801.261719\n",
      "Train Epoch: 203 [17664/225000 (8%)] Loss: 18755.910156\n",
      "Train Epoch: 203 [20160/225000 (9%)] Loss: 19058.214844\n",
      "Train Epoch: 203 [22656/225000 (10%)] Loss: 18773.785156\n",
      "Train Epoch: 203 [25152/225000 (11%)] Loss: 18930.457031\n",
      "Train Epoch: 203 [27648/225000 (12%)] Loss: 18328.425781\n",
      "Train Epoch: 203 [30144/225000 (13%)] Loss: 18674.378906\n",
      "Train Epoch: 203 [32640/225000 (15%)] Loss: 18517.013672\n",
      "Train Epoch: 203 [35136/225000 (16%)] Loss: 18720.876953\n",
      "Train Epoch: 203 [37632/225000 (17%)] Loss: 18711.884766\n",
      "Train Epoch: 203 [40128/225000 (18%)] Loss: 18832.933594\n",
      "Train Epoch: 203 [42624/225000 (19%)] Loss: 18575.304688\n",
      "Train Epoch: 203 [45120/225000 (20%)] Loss: 18657.167969\n",
      "Train Epoch: 203 [47616/225000 (21%)] Loss: 18689.882812\n",
      "Train Epoch: 203 [50112/225000 (22%)] Loss: 18701.402344\n",
      "Train Epoch: 203 [52608/225000 (23%)] Loss: 18764.339844\n",
      "Train Epoch: 203 [55104/225000 (24%)] Loss: 18575.634766\n",
      "Train Epoch: 203 [57600/225000 (26%)] Loss: 18618.214844\n",
      "Train Epoch: 203 [60096/225000 (27%)] Loss: 18686.011719\n",
      "Train Epoch: 203 [62592/225000 (28%)] Loss: 18413.541016\n",
      "Train Epoch: 203 [65088/225000 (29%)] Loss: 18563.531250\n",
      "Train Epoch: 203 [67584/225000 (30%)] Loss: 18526.289062\n",
      "Train Epoch: 203 [70080/225000 (31%)] Loss: 18897.769531\n",
      "Train Epoch: 203 [72576/225000 (32%)] Loss: 18198.664062\n",
      "Train Epoch: 203 [75072/225000 (33%)] Loss: 18456.804688\n",
      "Train Epoch: 203 [77568/225000 (34%)] Loss: 18881.275391\n",
      "Train Epoch: 203 [80064/225000 (36%)] Loss: 18313.808594\n",
      "Train Epoch: 203 [82560/225000 (37%)] Loss: 18988.429688\n",
      "Train Epoch: 203 [85056/225000 (38%)] Loss: 18342.957031\n",
      "Train Epoch: 203 [87552/225000 (39%)] Loss: 18270.402344\n",
      "Train Epoch: 203 [90048/225000 (40%)] Loss: 18594.347656\n",
      "Train Epoch: 203 [92544/225000 (41%)] Loss: 18512.468750\n",
      "Train Epoch: 203 [95040/225000 (42%)] Loss: 18577.640625\n",
      "Train Epoch: 203 [97536/225000 (43%)] Loss: 18901.703125\n",
      "Train Epoch: 203 [100032/225000 (44%)] Loss: 18625.046875\n",
      "Train Epoch: 203 [102528/225000 (46%)] Loss: 18897.923828\n",
      "Train Epoch: 203 [105024/225000 (47%)] Loss: 18573.902344\n",
      "Train Epoch: 203 [107520/225000 (48%)] Loss: 18713.957031\n",
      "Train Epoch: 203 [110016/225000 (49%)] Loss: 18289.937500\n",
      "Train Epoch: 203 [112512/225000 (50%)] Loss: 18587.560547\n",
      "Train Epoch: 203 [115008/225000 (51%)] Loss: 18545.070312\n",
      "Train Epoch: 203 [117504/225000 (52%)] Loss: 19102.033203\n",
      "Train Epoch: 203 [120000/225000 (53%)] Loss: 18169.328125\n",
      "Train Epoch: 203 [122496/225000 (54%)] Loss: 18528.933594\n",
      "Train Epoch: 203 [124992/225000 (56%)] Loss: 18704.359375\n",
      "Train Epoch: 203 [127488/225000 (57%)] Loss: 17937.537109\n",
      "Train Epoch: 203 [129984/225000 (58%)] Loss: 18174.378906\n",
      "Train Epoch: 203 [132480/225000 (59%)] Loss: 18700.310547\n",
      "Train Epoch: 203 [134976/225000 (60%)] Loss: 18498.425781\n",
      "Train Epoch: 203 [137472/225000 (61%)] Loss: 18447.464844\n",
      "Train Epoch: 203 [139968/225000 (62%)] Loss: 18310.824219\n",
      "Train Epoch: 203 [142464/225000 (63%)] Loss: 18371.363281\n",
      "Train Epoch: 203 [144960/225000 (64%)] Loss: 18868.746094\n",
      "Train Epoch: 203 [147456/225000 (66%)] Loss: 18372.308594\n",
      "Train Epoch: 203 [149952/225000 (67%)] Loss: 18399.744141\n",
      "Train Epoch: 203 [152448/225000 (68%)] Loss: 17893.242188\n",
      "Train Epoch: 203 [154944/225000 (69%)] Loss: 18450.578125\n",
      "Train Epoch: 203 [157440/225000 (70%)] Loss: 18698.042969\n",
      "Train Epoch: 203 [159936/225000 (71%)] Loss: 18972.787109\n",
      "Train Epoch: 203 [162432/225000 (72%)] Loss: 19134.894531\n",
      "Train Epoch: 203 [164928/225000 (73%)] Loss: 18090.125000\n",
      "Train Epoch: 203 [167424/225000 (74%)] Loss: 19128.917969\n",
      "Train Epoch: 203 [169920/225000 (76%)] Loss: 18703.796875\n",
      "Train Epoch: 203 [172416/225000 (77%)] Loss: 18529.714844\n",
      "Train Epoch: 203 [174912/225000 (78%)] Loss: 18575.765625\n",
      "Train Epoch: 203 [177408/225000 (79%)] Loss: 18867.371094\n",
      "Train Epoch: 203 [179904/225000 (80%)] Loss: 18745.279297\n",
      "Train Epoch: 203 [182400/225000 (81%)] Loss: 18970.132812\n",
      "Train Epoch: 203 [184896/225000 (82%)] Loss: 18630.667969\n",
      "Train Epoch: 203 [187392/225000 (83%)] Loss: 18795.875000\n",
      "Train Epoch: 203 [189888/225000 (84%)] Loss: 18860.144531\n",
      "Train Epoch: 203 [192384/225000 (86%)] Loss: 18641.917969\n",
      "Train Epoch: 203 [194880/225000 (87%)] Loss: 18612.857422\n",
      "Train Epoch: 203 [197376/225000 (88%)] Loss: 18492.789062\n",
      "Train Epoch: 203 [199872/225000 (89%)] Loss: 18340.207031\n",
      "Train Epoch: 203 [202368/225000 (90%)] Loss: 18377.367188\n",
      "Train Epoch: 203 [204864/225000 (91%)] Loss: 18597.691406\n",
      "Train Epoch: 203 [207360/225000 (92%)] Loss: 19008.771484\n",
      "Train Epoch: 203 [209856/225000 (93%)] Loss: 18735.972656\n",
      "Train Epoch: 203 [212352/225000 (94%)] Loss: 18565.632812\n",
      "Train Epoch: 203 [214848/225000 (95%)] Loss: 18683.984375\n",
      "Train Epoch: 203 [217344/225000 (97%)] Loss: 18654.109375\n",
      "Train Epoch: 203 [219840/225000 (98%)] Loss: 19080.728516\n",
      "Train Epoch: 203 [222336/225000 (99%)] Loss: 18815.923828\n",
      "Train Epoch: 203 [224832/225000 (100%)] Loss: 18316.039062\n",
      "    epoch          : 203\n",
      "    loss           : 18626.655736721415\n",
      "    val_loss       : 18502.772164823444\n",
      "Train Epoch: 204 [192/225000 (0%)] Loss: 19096.476562\n",
      "Train Epoch: 204 [2688/225000 (1%)] Loss: 18662.556641\n",
      "Train Epoch: 204 [5184/225000 (2%)] Loss: 18806.121094\n",
      "Train Epoch: 204 [7680/225000 (3%)] Loss: 18480.175781\n",
      "Train Epoch: 204 [10176/225000 (5%)] Loss: 18745.773438\n",
      "Train Epoch: 204 [12672/225000 (6%)] Loss: 18589.175781\n",
      "Train Epoch: 204 [15168/225000 (7%)] Loss: 18658.984375\n",
      "Train Epoch: 204 [17664/225000 (8%)] Loss: 18802.691406\n",
      "Train Epoch: 204 [20160/225000 (9%)] Loss: 21095.023438\n",
      "Train Epoch: 204 [22656/225000 (10%)] Loss: 18778.318359\n",
      "Train Epoch: 204 [25152/225000 (11%)] Loss: 18453.214844\n",
      "Train Epoch: 204 [27648/225000 (12%)] Loss: 18682.736328\n",
      "Train Epoch: 204 [30144/225000 (13%)] Loss: 18364.056641\n",
      "Train Epoch: 204 [32640/225000 (15%)] Loss: 18514.902344\n",
      "Train Epoch: 204 [35136/225000 (16%)] Loss: 18665.982422\n",
      "Train Epoch: 204 [37632/225000 (17%)] Loss: 18696.154297\n",
      "Train Epoch: 204 [40128/225000 (18%)] Loss: 18253.324219\n",
      "Train Epoch: 204 [42624/225000 (19%)] Loss: 18333.417969\n",
      "Train Epoch: 204 [45120/225000 (20%)] Loss: 18793.082031\n",
      "Train Epoch: 204 [47616/225000 (21%)] Loss: 18597.343750\n",
      "Train Epoch: 204 [50112/225000 (22%)] Loss: 18529.726562\n",
      "Train Epoch: 204 [52608/225000 (23%)] Loss: 18597.503906\n",
      "Train Epoch: 204 [55104/225000 (24%)] Loss: 18547.353516\n",
      "Train Epoch: 204 [57600/225000 (26%)] Loss: 18404.742188\n",
      "Train Epoch: 204 [60096/225000 (27%)] Loss: 18797.712891\n",
      "Train Epoch: 204 [62592/225000 (28%)] Loss: 18644.828125\n",
      "Train Epoch: 204 [65088/225000 (29%)] Loss: 19402.875000\n",
      "Train Epoch: 204 [67584/225000 (30%)] Loss: 18800.343750\n",
      "Train Epoch: 204 [70080/225000 (31%)] Loss: 18874.839844\n",
      "Train Epoch: 204 [72576/225000 (32%)] Loss: 18254.230469\n",
      "Train Epoch: 204 [75072/225000 (33%)] Loss: 18435.312500\n",
      "Train Epoch: 204 [77568/225000 (34%)] Loss: 18704.595703\n",
      "Train Epoch: 204 [80064/225000 (36%)] Loss: 18519.517578\n",
      "Train Epoch: 204 [82560/225000 (37%)] Loss: 18424.564453\n",
      "Train Epoch: 204 [85056/225000 (38%)] Loss: 18593.769531\n",
      "Train Epoch: 204 [87552/225000 (39%)] Loss: 18744.730469\n",
      "Train Epoch: 204 [90048/225000 (40%)] Loss: 18626.777344\n",
      "Train Epoch: 204 [92544/225000 (41%)] Loss: 18705.296875\n",
      "Train Epoch: 204 [95040/225000 (42%)] Loss: 18381.945312\n",
      "Train Epoch: 204 [97536/225000 (43%)] Loss: 18651.236328\n",
      "Train Epoch: 204 [100032/225000 (44%)] Loss: 18077.554688\n",
      "Train Epoch: 204 [102528/225000 (46%)] Loss: 18737.197266\n",
      "Train Epoch: 204 [105024/225000 (47%)] Loss: 18161.607422\n",
      "Train Epoch: 204 [107520/225000 (48%)] Loss: 18367.783203\n",
      "Train Epoch: 204 [110016/225000 (49%)] Loss: 18379.421875\n",
      "Train Epoch: 204 [112512/225000 (50%)] Loss: 18743.656250\n",
      "Train Epoch: 204 [115008/225000 (51%)] Loss: 18454.500000\n",
      "Train Epoch: 204 [117504/225000 (52%)] Loss: 18998.964844\n",
      "Train Epoch: 204 [120000/225000 (53%)] Loss: 18524.218750\n",
      "Train Epoch: 204 [122496/225000 (54%)] Loss: 18387.412109\n",
      "Train Epoch: 204 [124992/225000 (56%)] Loss: 19013.193359\n",
      "Train Epoch: 204 [127488/225000 (57%)] Loss: 18197.222656\n",
      "Train Epoch: 204 [129984/225000 (58%)] Loss: 18222.785156\n",
      "Train Epoch: 204 [132480/225000 (59%)] Loss: 18745.226562\n",
      "Train Epoch: 204 [134976/225000 (60%)] Loss: 18456.964844\n",
      "Train Epoch: 204 [137472/225000 (61%)] Loss: 18806.878906\n",
      "Train Epoch: 204 [139968/225000 (62%)] Loss: 17838.039062\n",
      "Train Epoch: 204 [142464/225000 (63%)] Loss: 18683.228516\n",
      "Train Epoch: 204 [144960/225000 (64%)] Loss: 18499.525391\n",
      "Train Epoch: 204 [147456/225000 (66%)] Loss: 18687.410156\n",
      "Train Epoch: 204 [149952/225000 (67%)] Loss: 18990.958984\n",
      "Train Epoch: 204 [152448/225000 (68%)] Loss: 18053.898438\n",
      "Train Epoch: 204 [154944/225000 (69%)] Loss: 18562.972656\n",
      "Train Epoch: 204 [157440/225000 (70%)] Loss: 18830.513672\n",
      "Train Epoch: 204 [159936/225000 (71%)] Loss: 18347.212891\n",
      "Train Epoch: 204 [162432/225000 (72%)] Loss: 18869.537109\n",
      "Train Epoch: 204 [164928/225000 (73%)] Loss: 18710.199219\n",
      "Train Epoch: 204 [167424/225000 (74%)] Loss: 18462.726562\n",
      "Train Epoch: 204 [169920/225000 (76%)] Loss: 18691.636719\n",
      "Train Epoch: 204 [172416/225000 (77%)] Loss: 18812.082031\n",
      "Train Epoch: 204 [174912/225000 (78%)] Loss: 18264.378906\n",
      "Train Epoch: 204 [177408/225000 (79%)] Loss: 18646.500000\n",
      "Train Epoch: 204 [179904/225000 (80%)] Loss: 18427.701172\n",
      "Train Epoch: 204 [182400/225000 (81%)] Loss: 18651.542969\n",
      "Train Epoch: 204 [184896/225000 (82%)] Loss: 18610.533203\n",
      "Train Epoch: 204 [187392/225000 (83%)] Loss: 19062.554688\n",
      "Train Epoch: 204 [189888/225000 (84%)] Loss: 18385.099609\n",
      "Train Epoch: 204 [192384/225000 (86%)] Loss: 18706.535156\n",
      "Train Epoch: 204 [194880/225000 (87%)] Loss: 17902.214844\n",
      "Train Epoch: 204 [197376/225000 (88%)] Loss: 18974.296875\n",
      "Train Epoch: 204 [199872/225000 (89%)] Loss: 18135.462891\n",
      "Train Epoch: 204 [202368/225000 (90%)] Loss: 18554.093750\n",
      "Train Epoch: 204 [204864/225000 (91%)] Loss: 18514.314453\n",
      "Train Epoch: 204 [207360/225000 (92%)] Loss: 18513.191406\n",
      "Train Epoch: 204 [209856/225000 (93%)] Loss: 18658.871094\n",
      "Train Epoch: 204 [212352/225000 (94%)] Loss: 18504.621094\n",
      "Train Epoch: 204 [214848/225000 (95%)] Loss: 18829.859375\n",
      "Train Epoch: 204 [217344/225000 (97%)] Loss: 18323.449219\n",
      "Train Epoch: 204 [219840/225000 (98%)] Loss: 18689.601562\n",
      "Train Epoch: 204 [222336/225000 (99%)] Loss: 18532.871094\n",
      "Train Epoch: 204 [224832/225000 (100%)] Loss: 18214.226562\n",
      "    epoch          : 204\n",
      "    loss           : 18596.67070179181\n",
      "    val_loss       : 18510.417335243626\n",
      "Train Epoch: 205 [192/225000 (0%)] Loss: 18590.398438\n",
      "Train Epoch: 205 [2688/225000 (1%)] Loss: 18266.691406\n",
      "Train Epoch: 205 [5184/225000 (2%)] Loss: 18892.593750\n",
      "Train Epoch: 205 [7680/225000 (3%)] Loss: 18554.335938\n",
      "Train Epoch: 205 [10176/225000 (5%)] Loss: 18451.287109\n",
      "Train Epoch: 205 [12672/225000 (6%)] Loss: 18299.910156\n",
      "Train Epoch: 205 [15168/225000 (7%)] Loss: 18385.349609\n",
      "Train Epoch: 205 [17664/225000 (8%)] Loss: 18574.267578\n",
      "Train Epoch: 205 [20160/225000 (9%)] Loss: 18459.699219\n",
      "Train Epoch: 205 [22656/225000 (10%)] Loss: 18697.367188\n",
      "Train Epoch: 205 [25152/225000 (11%)] Loss: 18587.892578\n",
      "Train Epoch: 205 [27648/225000 (12%)] Loss: 18469.011719\n",
      "Train Epoch: 205 [30144/225000 (13%)] Loss: 18834.511719\n",
      "Train Epoch: 205 [32640/225000 (15%)] Loss: 19164.343750\n",
      "Train Epoch: 205 [35136/225000 (16%)] Loss: 18733.027344\n",
      "Train Epoch: 205 [37632/225000 (17%)] Loss: 19199.121094\n",
      "Train Epoch: 205 [40128/225000 (18%)] Loss: 18051.919922\n",
      "Train Epoch: 205 [42624/225000 (19%)] Loss: 18585.746094\n",
      "Train Epoch: 205 [45120/225000 (20%)] Loss: 18560.378906\n",
      "Train Epoch: 205 [47616/225000 (21%)] Loss: 18453.535156\n",
      "Train Epoch: 205 [50112/225000 (22%)] Loss: 18833.894531\n",
      "Train Epoch: 205 [52608/225000 (23%)] Loss: 18672.832031\n",
      "Train Epoch: 205 [55104/225000 (24%)] Loss: 18914.968750\n",
      "Train Epoch: 205 [57600/225000 (26%)] Loss: 18487.789062\n",
      "Train Epoch: 205 [60096/225000 (27%)] Loss: 18680.992188\n",
      "Train Epoch: 205 [62592/225000 (28%)] Loss: 18420.460938\n",
      "Train Epoch: 205 [65088/225000 (29%)] Loss: 18916.732422\n",
      "Train Epoch: 205 [67584/225000 (30%)] Loss: 18853.453125\n",
      "Train Epoch: 205 [70080/225000 (31%)] Loss: 18668.718750\n",
      "Train Epoch: 205 [72576/225000 (32%)] Loss: 18447.488281\n",
      "Train Epoch: 205 [75072/225000 (33%)] Loss: 18186.839844\n",
      "Train Epoch: 205 [77568/225000 (34%)] Loss: 19112.111328\n",
      "Train Epoch: 205 [80064/225000 (36%)] Loss: 18708.539062\n",
      "Train Epoch: 205 [82560/225000 (37%)] Loss: 18614.250000\n",
      "Train Epoch: 205 [85056/225000 (38%)] Loss: 18867.398438\n",
      "Train Epoch: 205 [87552/225000 (39%)] Loss: 18753.046875\n",
      "Train Epoch: 205 [90048/225000 (40%)] Loss: 18567.261719\n",
      "Train Epoch: 205 [92544/225000 (41%)] Loss: 18739.195312\n",
      "Train Epoch: 205 [95040/225000 (42%)] Loss: 18346.980469\n",
      "Train Epoch: 205 [97536/225000 (43%)] Loss: 18363.273438\n",
      "Train Epoch: 205 [100032/225000 (44%)] Loss: 18547.199219\n",
      "Train Epoch: 205 [102528/225000 (46%)] Loss: 18108.257812\n",
      "Train Epoch: 205 [105024/225000 (47%)] Loss: 18518.871094\n",
      "Train Epoch: 205 [107520/225000 (48%)] Loss: 18510.474609\n",
      "Train Epoch: 205 [110016/225000 (49%)] Loss: 18379.246094\n",
      "Train Epoch: 205 [112512/225000 (50%)] Loss: 18709.023438\n",
      "Train Epoch: 205 [115008/225000 (51%)] Loss: 18597.097656\n",
      "Train Epoch: 205 [117504/225000 (52%)] Loss: 18756.871094\n",
      "Train Epoch: 205 [120000/225000 (53%)] Loss: 18771.701172\n",
      "Train Epoch: 205 [122496/225000 (54%)] Loss: 18530.117188\n",
      "Train Epoch: 205 [124992/225000 (56%)] Loss: 18698.730469\n",
      "Train Epoch: 205 [127488/225000 (57%)] Loss: 18245.503906\n",
      "Train Epoch: 205 [129984/225000 (58%)] Loss: 18789.281250\n",
      "Train Epoch: 205 [132480/225000 (59%)] Loss: 18912.101562\n",
      "Train Epoch: 205 [134976/225000 (60%)] Loss: 18393.519531\n",
      "Train Epoch: 205 [137472/225000 (61%)] Loss: 18648.554688\n",
      "Train Epoch: 205 [139968/225000 (62%)] Loss: 18630.382812\n",
      "Train Epoch: 205 [142464/225000 (63%)] Loss: 18028.187500\n",
      "Train Epoch: 205 [144960/225000 (64%)] Loss: 18747.359375\n",
      "Train Epoch: 205 [147456/225000 (66%)] Loss: 19018.347656\n",
      "Train Epoch: 205 [149952/225000 (67%)] Loss: 18497.853516\n",
      "Train Epoch: 205 [152448/225000 (68%)] Loss: 18751.785156\n",
      "Train Epoch: 205 [154944/225000 (69%)] Loss: 18777.265625\n",
      "Train Epoch: 205 [157440/225000 (70%)] Loss: 18260.230469\n",
      "Train Epoch: 205 [159936/225000 (71%)] Loss: 18579.300781\n",
      "Train Epoch: 205 [162432/225000 (72%)] Loss: 18418.294922\n",
      "Train Epoch: 205 [164928/225000 (73%)] Loss: 19093.798828\n",
      "Train Epoch: 205 [167424/225000 (74%)] Loss: 18594.203125\n",
      "Train Epoch: 205 [169920/225000 (76%)] Loss: 18635.191406\n",
      "Train Epoch: 205 [172416/225000 (77%)] Loss: 18427.972656\n",
      "Train Epoch: 205 [174912/225000 (78%)] Loss: 18550.207031\n",
      "Train Epoch: 205 [177408/225000 (79%)] Loss: 18578.908203\n",
      "Train Epoch: 205 [179904/225000 (80%)] Loss: 18523.898438\n",
      "Train Epoch: 205 [182400/225000 (81%)] Loss: 18758.726562\n",
      "Train Epoch: 205 [184896/225000 (82%)] Loss: 18615.343750\n",
      "Train Epoch: 205 [187392/225000 (83%)] Loss: 19097.078125\n",
      "Train Epoch: 205 [189888/225000 (84%)] Loss: 18617.468750\n",
      "Train Epoch: 205 [192384/225000 (86%)] Loss: 18622.187500\n",
      "Train Epoch: 205 [194880/225000 (87%)] Loss: 18391.916016\n",
      "Train Epoch: 205 [197376/225000 (88%)] Loss: 18568.691406\n",
      "Train Epoch: 205 [199872/225000 (89%)] Loss: 19467.884766\n",
      "Train Epoch: 205 [202368/225000 (90%)] Loss: 18158.000000\n",
      "Train Epoch: 205 [204864/225000 (91%)] Loss: 18496.001953\n",
      "Train Epoch: 205 [207360/225000 (92%)] Loss: 18619.617188\n",
      "Train Epoch: 205 [209856/225000 (93%)] Loss: 18354.162109\n",
      "Train Epoch: 205 [212352/225000 (94%)] Loss: 18328.828125\n",
      "Train Epoch: 205 [214848/225000 (95%)] Loss: 18660.343750\n",
      "Train Epoch: 205 [217344/225000 (97%)] Loss: 18369.537109\n",
      "Train Epoch: 205 [219840/225000 (98%)] Loss: 18212.269531\n",
      "Train Epoch: 205 [222336/225000 (99%)] Loss: 18142.621094\n",
      "Train Epoch: 205 [224832/225000 (100%)] Loss: 18057.667969\n",
      "    epoch          : 205\n",
      "    loss           : 18572.704336537437\n",
      "    val_loss       : 18467.21081259642\n",
      "Train Epoch: 206 [192/225000 (0%)] Loss: 18475.144531\n",
      "Train Epoch: 206 [2688/225000 (1%)] Loss: 18451.863281\n",
      "Train Epoch: 206 [5184/225000 (2%)] Loss: 18574.050781\n",
      "Train Epoch: 206 [7680/225000 (3%)] Loss: 18551.861328\n",
      "Train Epoch: 206 [10176/225000 (5%)] Loss: 18975.203125\n",
      "Train Epoch: 206 [12672/225000 (6%)] Loss: 18511.980469\n",
      "Train Epoch: 206 [15168/225000 (7%)] Loss: 18216.123047\n",
      "Train Epoch: 206 [17664/225000 (8%)] Loss: 18206.136719\n",
      "Train Epoch: 206 [20160/225000 (9%)] Loss: 18811.519531\n",
      "Train Epoch: 206 [22656/225000 (10%)] Loss: 18643.562500\n",
      "Train Epoch: 206 [25152/225000 (11%)] Loss: 18474.218750\n",
      "Train Epoch: 206 [27648/225000 (12%)] Loss: 18604.199219\n",
      "Train Epoch: 206 [30144/225000 (13%)] Loss: 18967.324219\n",
      "Train Epoch: 206 [32640/225000 (15%)] Loss: 18461.046875\n",
      "Train Epoch: 206 [35136/225000 (16%)] Loss: 18188.019531\n",
      "Train Epoch: 206 [37632/225000 (17%)] Loss: 18739.515625\n",
      "Train Epoch: 206 [40128/225000 (18%)] Loss: 18328.750000\n",
      "Train Epoch: 206 [42624/225000 (19%)] Loss: 18523.597656\n",
      "Train Epoch: 206 [45120/225000 (20%)] Loss: 18187.453125\n",
      "Train Epoch: 206 [47616/225000 (21%)] Loss: 18820.371094\n",
      "Train Epoch: 206 [50112/225000 (22%)] Loss: 18897.708984\n",
      "Train Epoch: 206 [52608/225000 (23%)] Loss: 18920.296875\n",
      "Train Epoch: 206 [55104/225000 (24%)] Loss: 18117.105469\n",
      "Train Epoch: 206 [57600/225000 (26%)] Loss: 18311.369141\n",
      "Train Epoch: 206 [60096/225000 (27%)] Loss: 19041.199219\n",
      "Train Epoch: 206 [62592/225000 (28%)] Loss: 18805.255859\n",
      "Train Epoch: 206 [65088/225000 (29%)] Loss: 18528.994141\n",
      "Train Epoch: 206 [67584/225000 (30%)] Loss: 18727.613281\n",
      "Train Epoch: 206 [70080/225000 (31%)] Loss: 18690.812500\n",
      "Train Epoch: 206 [72576/225000 (32%)] Loss: 18960.644531\n",
      "Train Epoch: 206 [75072/225000 (33%)] Loss: 18618.328125\n",
      "Train Epoch: 206 [77568/225000 (34%)] Loss: 18521.953125\n",
      "Train Epoch: 206 [80064/225000 (36%)] Loss: 18207.214844\n",
      "Train Epoch: 206 [82560/225000 (37%)] Loss: 18482.714844\n",
      "Train Epoch: 206 [85056/225000 (38%)] Loss: 18162.486328\n",
      "Train Epoch: 206 [87552/225000 (39%)] Loss: 18625.566406\n",
      "Train Epoch: 206 [90048/225000 (40%)] Loss: 18515.968750\n",
      "Train Epoch: 206 [92544/225000 (41%)] Loss: 18487.003906\n",
      "Train Epoch: 206 [95040/225000 (42%)] Loss: 18598.593750\n",
      "Train Epoch: 206 [97536/225000 (43%)] Loss: 18386.662109\n",
      "Train Epoch: 206 [100032/225000 (44%)] Loss: 18828.541016\n",
      "Train Epoch: 206 [102528/225000 (46%)] Loss: 18326.439453\n",
      "Train Epoch: 206 [105024/225000 (47%)] Loss: 18226.609375\n",
      "Train Epoch: 206 [107520/225000 (48%)] Loss: 18184.226562\n",
      "Train Epoch: 206 [110016/225000 (49%)] Loss: 18333.060547\n",
      "Train Epoch: 206 [112512/225000 (50%)] Loss: 18637.390625\n",
      "Train Epoch: 206 [115008/225000 (51%)] Loss: 18637.199219\n",
      "Train Epoch: 206 [117504/225000 (52%)] Loss: 18642.435547\n",
      "Train Epoch: 206 [120000/225000 (53%)] Loss: 18185.316406\n",
      "Train Epoch: 206 [122496/225000 (54%)] Loss: 18679.511719\n",
      "Train Epoch: 206 [124992/225000 (56%)] Loss: 18249.474609\n",
      "Train Epoch: 206 [127488/225000 (57%)] Loss: 17955.941406\n",
      "Train Epoch: 206 [129984/225000 (58%)] Loss: 18813.367188\n",
      "Train Epoch: 206 [132480/225000 (59%)] Loss: 18655.871094\n",
      "Train Epoch: 206 [134976/225000 (60%)] Loss: 17912.101562\n",
      "Train Epoch: 206 [137472/225000 (61%)] Loss: 18348.730469\n",
      "Train Epoch: 206 [139968/225000 (62%)] Loss: 18686.824219\n",
      "Train Epoch: 206 [142464/225000 (63%)] Loss: 18700.396484\n",
      "Train Epoch: 206 [144960/225000 (64%)] Loss: 18624.574219\n",
      "Train Epoch: 206 [147456/225000 (66%)] Loss: 18299.640625\n",
      "Train Epoch: 206 [149952/225000 (67%)] Loss: 18689.794922\n",
      "Train Epoch: 206 [152448/225000 (68%)] Loss: 19096.531250\n",
      "Train Epoch: 206 [154944/225000 (69%)] Loss: 18748.171875\n",
      "Train Epoch: 206 [157440/225000 (70%)] Loss: 18095.824219\n",
      "Train Epoch: 206 [159936/225000 (71%)] Loss: 18266.773438\n",
      "Train Epoch: 206 [162432/225000 (72%)] Loss: 18581.974609\n",
      "Train Epoch: 206 [164928/225000 (73%)] Loss: 18636.902344\n",
      "Train Epoch: 206 [167424/225000 (74%)] Loss: 18589.781250\n",
      "Train Epoch: 206 [169920/225000 (76%)] Loss: 18415.134766\n",
      "Train Epoch: 206 [172416/225000 (77%)] Loss: 18787.785156\n",
      "Train Epoch: 206 [174912/225000 (78%)] Loss: 18732.998047\n",
      "Train Epoch: 206 [177408/225000 (79%)] Loss: 18806.980469\n",
      "Train Epoch: 206 [179904/225000 (80%)] Loss: 18401.437500\n",
      "Train Epoch: 206 [182400/225000 (81%)] Loss: 18502.968750\n",
      "Train Epoch: 206 [184896/225000 (82%)] Loss: 18981.343750\n",
      "Train Epoch: 206 [187392/225000 (83%)] Loss: 18829.001953\n",
      "Train Epoch: 206 [189888/225000 (84%)] Loss: 18651.167969\n",
      "Train Epoch: 206 [192384/225000 (86%)] Loss: 18566.621094\n",
      "Train Epoch: 206 [194880/225000 (87%)] Loss: 18603.060547\n",
      "Train Epoch: 206 [197376/225000 (88%)] Loss: 18690.179688\n",
      "Train Epoch: 206 [199872/225000 (89%)] Loss: 18349.910156\n",
      "Train Epoch: 206 [202368/225000 (90%)] Loss: 18430.421875\n",
      "Train Epoch: 206 [204864/225000 (91%)] Loss: 18643.113281\n",
      "Train Epoch: 206 [207360/225000 (92%)] Loss: 18816.494141\n",
      "Train Epoch: 206 [209856/225000 (93%)] Loss: 18750.769531\n",
      "Train Epoch: 206 [212352/225000 (94%)] Loss: 18831.410156\n",
      "Train Epoch: 206 [214848/225000 (95%)] Loss: 18873.097656\n",
      "Train Epoch: 206 [217344/225000 (97%)] Loss: 18432.234375\n",
      "Train Epoch: 206 [219840/225000 (98%)] Loss: 18188.441406\n",
      "Train Epoch: 206 [222336/225000 (99%)] Loss: 18643.449219\n",
      "Train Epoch: 206 [224832/225000 (100%)] Loss: 18248.035156\n",
      "    epoch          : 206\n",
      "    loss           : 18545.21940159716\n",
      "    val_loss       : 18432.35074794429\n",
      "Train Epoch: 207 [192/225000 (0%)] Loss: 18961.837891\n",
      "Train Epoch: 207 [2688/225000 (1%)] Loss: 18433.873047\n",
      "Train Epoch: 207 [5184/225000 (2%)] Loss: 18750.056641\n",
      "Train Epoch: 207 [7680/225000 (3%)] Loss: 18517.878906\n",
      "Train Epoch: 207 [10176/225000 (5%)] Loss: 18509.779297\n",
      "Train Epoch: 207 [12672/225000 (6%)] Loss: 17773.107422\n",
      "Train Epoch: 207 [15168/225000 (7%)] Loss: 18540.863281\n",
      "Train Epoch: 207 [17664/225000 (8%)] Loss: 18443.054688\n",
      "Train Epoch: 207 [20160/225000 (9%)] Loss: 18228.675781\n",
      "Train Epoch: 207 [22656/225000 (10%)] Loss: 18218.068359\n",
      "Train Epoch: 207 [25152/225000 (11%)] Loss: 18098.101562\n",
      "Train Epoch: 207 [27648/225000 (12%)] Loss: 17660.242188\n",
      "Train Epoch: 207 [30144/225000 (13%)] Loss: 18814.914062\n",
      "Train Epoch: 207 [32640/225000 (15%)] Loss: 18741.980469\n",
      "Train Epoch: 207 [35136/225000 (16%)] Loss: 18456.164062\n",
      "Train Epoch: 207 [37632/225000 (17%)] Loss: 18467.421875\n",
      "Train Epoch: 207 [40128/225000 (18%)] Loss: 18561.113281\n",
      "Train Epoch: 207 [42624/225000 (19%)] Loss: 18692.996094\n",
      "Train Epoch: 207 [45120/225000 (20%)] Loss: 18275.054688\n",
      "Train Epoch: 207 [47616/225000 (21%)] Loss: 18674.601562\n",
      "Train Epoch: 207 [50112/225000 (22%)] Loss: 18710.238281\n",
      "Train Epoch: 207 [52608/225000 (23%)] Loss: 18689.283203\n",
      "Train Epoch: 207 [55104/225000 (24%)] Loss: 18752.083984\n",
      "Train Epoch: 207 [57600/225000 (26%)] Loss: 18726.222656\n",
      "Train Epoch: 207 [60096/225000 (27%)] Loss: 18340.578125\n",
      "Train Epoch: 207 [62592/225000 (28%)] Loss: 18419.234375\n",
      "Train Epoch: 207 [65088/225000 (29%)] Loss: 18168.744141\n",
      "Train Epoch: 207 [67584/225000 (30%)] Loss: 18373.347656\n",
      "Train Epoch: 207 [70080/225000 (31%)] Loss: 18952.714844\n",
      "Train Epoch: 207 [72576/225000 (32%)] Loss: 18390.535156\n",
      "Train Epoch: 207 [75072/225000 (33%)] Loss: 18567.058594\n",
      "Train Epoch: 207 [77568/225000 (34%)] Loss: 18567.564453\n",
      "Train Epoch: 207 [80064/225000 (36%)] Loss: 18567.453125\n",
      "Train Epoch: 207 [82560/225000 (37%)] Loss: 18180.027344\n",
      "Train Epoch: 207 [85056/225000 (38%)] Loss: 18421.062500\n",
      "Train Epoch: 207 [87552/225000 (39%)] Loss: 19097.589844\n",
      "Train Epoch: 207 [90048/225000 (40%)] Loss: 18419.218750\n",
      "Train Epoch: 207 [92544/225000 (41%)] Loss: 17903.121094\n",
      "Train Epoch: 207 [95040/225000 (42%)] Loss: 18536.357422\n",
      "Train Epoch: 207 [97536/225000 (43%)] Loss: 18871.353516\n",
      "Train Epoch: 207 [100032/225000 (44%)] Loss: 18641.613281\n",
      "Train Epoch: 207 [102528/225000 (46%)] Loss: 18163.343750\n",
      "Train Epoch: 207 [105024/225000 (47%)] Loss: 18307.101562\n",
      "Train Epoch: 207 [107520/225000 (48%)] Loss: 18448.189453\n",
      "Train Epoch: 207 [110016/225000 (49%)] Loss: 18192.996094\n",
      "Train Epoch: 207 [112512/225000 (50%)] Loss: 18554.068359\n",
      "Train Epoch: 207 [115008/225000 (51%)] Loss: 18255.427734\n",
      "Train Epoch: 207 [117504/225000 (52%)] Loss: 18550.740234\n",
      "Train Epoch: 207 [120000/225000 (53%)] Loss: 18465.455078\n",
      "Train Epoch: 207 [122496/225000 (54%)] Loss: 18115.863281\n",
      "Train Epoch: 207 [124992/225000 (56%)] Loss: 18617.289062\n",
      "Train Epoch: 207 [127488/225000 (57%)] Loss: 18200.871094\n",
      "Train Epoch: 207 [129984/225000 (58%)] Loss: 18639.580078\n",
      "Train Epoch: 207 [132480/225000 (59%)] Loss: 18489.923828\n",
      "Train Epoch: 207 [134976/225000 (60%)] Loss: 17991.798828\n",
      "Train Epoch: 207 [137472/225000 (61%)] Loss: 18051.796875\n",
      "Train Epoch: 207 [139968/225000 (62%)] Loss: 18311.324219\n",
      "Train Epoch: 207 [142464/225000 (63%)] Loss: 18986.287109\n",
      "Train Epoch: 207 [144960/225000 (64%)] Loss: 18327.859375\n",
      "Train Epoch: 207 [147456/225000 (66%)] Loss: 18480.351562\n",
      "Train Epoch: 207 [149952/225000 (67%)] Loss: 18611.210938\n",
      "Train Epoch: 207 [152448/225000 (68%)] Loss: 18914.919922\n",
      "Train Epoch: 207 [154944/225000 (69%)] Loss: 18291.832031\n",
      "Train Epoch: 207 [157440/225000 (70%)] Loss: 18616.855469\n",
      "Train Epoch: 207 [159936/225000 (71%)] Loss: 18284.609375\n",
      "Train Epoch: 207 [162432/225000 (72%)] Loss: 18675.214844\n",
      "Train Epoch: 207 [164928/225000 (73%)] Loss: 18756.265625\n",
      "Train Epoch: 207 [167424/225000 (74%)] Loss: 18133.011719\n",
      "Train Epoch: 207 [169920/225000 (76%)] Loss: 18346.878906\n",
      "Train Epoch: 207 [172416/225000 (77%)] Loss: 18112.925781\n",
      "Train Epoch: 207 [174912/225000 (78%)] Loss: 18762.578125\n",
      "Train Epoch: 207 [177408/225000 (79%)] Loss: 18636.228516\n",
      "Train Epoch: 207 [179904/225000 (80%)] Loss: 18384.425781\n",
      "Train Epoch: 207 [182400/225000 (81%)] Loss: 18399.304688\n",
      "Train Epoch: 207 [184896/225000 (82%)] Loss: 18635.251953\n",
      "Train Epoch: 207 [187392/225000 (83%)] Loss: 18540.613281\n",
      "Train Epoch: 207 [189888/225000 (84%)] Loss: 18680.154297\n",
      "Train Epoch: 207 [192384/225000 (86%)] Loss: 18181.951172\n",
      "Train Epoch: 207 [194880/225000 (87%)] Loss: 18592.488281\n",
      "Train Epoch: 207 [197376/225000 (88%)] Loss: 18490.158203\n",
      "Train Epoch: 207 [199872/225000 (89%)] Loss: 18042.582031\n",
      "Train Epoch: 207 [202368/225000 (90%)] Loss: 18372.054688\n",
      "Train Epoch: 207 [204864/225000 (91%)] Loss: 18414.919922\n",
      "Train Epoch: 207 [207360/225000 (92%)] Loss: 18681.853516\n",
      "Train Epoch: 207 [209856/225000 (93%)] Loss: 18391.503906\n",
      "Train Epoch: 207 [212352/225000 (94%)] Loss: 18427.140625\n",
      "Train Epoch: 207 [214848/225000 (95%)] Loss: 18669.273438\n",
      "Train Epoch: 207 [217344/225000 (97%)] Loss: 18566.912109\n",
      "Train Epoch: 207 [219840/225000 (98%)] Loss: 18215.441406\n",
      "Train Epoch: 207 [222336/225000 (99%)] Loss: 18591.474609\n",
      "Train Epoch: 207 [224832/225000 (100%)] Loss: 18157.869141\n",
      "    epoch          : 207\n",
      "    loss           : 18512.549461390787\n",
      "    val_loss       : 18438.863235980956\n",
      "Train Epoch: 208 [192/225000 (0%)] Loss: 18632.972656\n",
      "Train Epoch: 208 [2688/225000 (1%)] Loss: 18366.554688\n",
      "Train Epoch: 208 [5184/225000 (2%)] Loss: 19024.119141\n",
      "Train Epoch: 208 [7680/225000 (3%)] Loss: 18365.355469\n",
      "Train Epoch: 208 [10176/225000 (5%)] Loss: 18048.794922\n",
      "Train Epoch: 208 [12672/225000 (6%)] Loss: 18581.265625\n",
      "Train Epoch: 208 [15168/225000 (7%)] Loss: 17993.960938\n",
      "Train Epoch: 208 [17664/225000 (8%)] Loss: 18531.177734\n",
      "Train Epoch: 208 [20160/225000 (9%)] Loss: 18513.998047\n",
      "Train Epoch: 208 [22656/225000 (10%)] Loss: 18540.542969\n",
      "Train Epoch: 208 [25152/225000 (11%)] Loss: 18470.824219\n",
      "Train Epoch: 208 [27648/225000 (12%)] Loss: 18389.158203\n",
      "Train Epoch: 208 [30144/225000 (13%)] Loss: 18431.023438\n",
      "Train Epoch: 208 [32640/225000 (15%)] Loss: 18099.648438\n",
      "Train Epoch: 208 [35136/225000 (16%)] Loss: 18335.171875\n",
      "Train Epoch: 208 [37632/225000 (17%)] Loss: 17952.333984\n",
      "Train Epoch: 208 [40128/225000 (18%)] Loss: 17785.144531\n",
      "Train Epoch: 208 [42624/225000 (19%)] Loss: 18522.146484\n",
      "Train Epoch: 208 [45120/225000 (20%)] Loss: 18540.039062\n",
      "Train Epoch: 208 [47616/225000 (21%)] Loss: 18939.023438\n",
      "Train Epoch: 208 [50112/225000 (22%)] Loss: 18867.296875\n",
      "Train Epoch: 208 [52608/225000 (23%)] Loss: 18519.214844\n",
      "Train Epoch: 208 [55104/225000 (24%)] Loss: 18765.269531\n",
      "Train Epoch: 208 [57600/225000 (26%)] Loss: 18747.183594\n",
      "Train Epoch: 208 [60096/225000 (27%)] Loss: 18555.234375\n",
      "Train Epoch: 208 [62592/225000 (28%)] Loss: 18286.273438\n",
      "Train Epoch: 208 [65088/225000 (29%)] Loss: 18388.099609\n",
      "Train Epoch: 208 [67584/225000 (30%)] Loss: 18354.812500\n",
      "Train Epoch: 208 [70080/225000 (31%)] Loss: 18661.787109\n",
      "Train Epoch: 208 [72576/225000 (32%)] Loss: 18369.355469\n",
      "Train Epoch: 208 [75072/225000 (33%)] Loss: 18882.230469\n",
      "Train Epoch: 208 [77568/225000 (34%)] Loss: 18392.316406\n",
      "Train Epoch: 208 [80064/225000 (36%)] Loss: 18246.179688\n",
      "Train Epoch: 208 [82560/225000 (37%)] Loss: 18927.873047\n",
      "Train Epoch: 208 [85056/225000 (38%)] Loss: 18741.828125\n",
      "Train Epoch: 208 [87552/225000 (39%)] Loss: 18640.984375\n",
      "Train Epoch: 208 [90048/225000 (40%)] Loss: 18561.853516\n",
      "Train Epoch: 208 [92544/225000 (41%)] Loss: 18240.238281\n",
      "Train Epoch: 208 [95040/225000 (42%)] Loss: 18627.072266\n",
      "Train Epoch: 208 [97536/225000 (43%)] Loss: 18840.408203\n",
      "Train Epoch: 208 [100032/225000 (44%)] Loss: 18439.810547\n",
      "Train Epoch: 208 [102528/225000 (46%)] Loss: 19339.923828\n",
      "Train Epoch: 208 [105024/225000 (47%)] Loss: 18349.511719\n",
      "Train Epoch: 208 [107520/225000 (48%)] Loss: 18615.386719\n",
      "Train Epoch: 208 [110016/225000 (49%)] Loss: 18287.576172\n",
      "Train Epoch: 208 [112512/225000 (50%)] Loss: 18361.203125\n",
      "Train Epoch: 208 [115008/225000 (51%)] Loss: 17923.076172\n",
      "Train Epoch: 208 [117504/225000 (52%)] Loss: 18311.181641\n",
      "Train Epoch: 208 [120000/225000 (53%)] Loss: 18122.851562\n",
      "Train Epoch: 208 [122496/225000 (54%)] Loss: 18710.402344\n",
      "Train Epoch: 208 [124992/225000 (56%)] Loss: 18320.261719\n",
      "Train Epoch: 208 [127488/225000 (57%)] Loss: 18866.312500\n",
      "Train Epoch: 208 [129984/225000 (58%)] Loss: 18643.316406\n",
      "Train Epoch: 208 [132480/225000 (59%)] Loss: 18655.619141\n",
      "Train Epoch: 208 [134976/225000 (60%)] Loss: 18585.785156\n",
      "Train Epoch: 208 [137472/225000 (61%)] Loss: 18394.142578\n",
      "Train Epoch: 208 [139968/225000 (62%)] Loss: 18898.617188\n",
      "Train Epoch: 208 [142464/225000 (63%)] Loss: 18379.699219\n",
      "Train Epoch: 208 [144960/225000 (64%)] Loss: 18316.582031\n",
      "Train Epoch: 208 [147456/225000 (66%)] Loss: 18383.933594\n",
      "Train Epoch: 208 [149952/225000 (67%)] Loss: 18466.929688\n",
      "Train Epoch: 208 [152448/225000 (68%)] Loss: 18146.855469\n",
      "Train Epoch: 208 [154944/225000 (69%)] Loss: 18572.855469\n",
      "Train Epoch: 208 [157440/225000 (70%)] Loss: 18367.523438\n",
      "Train Epoch: 208 [159936/225000 (71%)] Loss: 18805.558594\n",
      "Train Epoch: 208 [162432/225000 (72%)] Loss: 18812.382812\n",
      "Train Epoch: 208 [164928/225000 (73%)] Loss: 18124.312500\n",
      "Train Epoch: 208 [167424/225000 (74%)] Loss: 18008.804688\n",
      "Train Epoch: 208 [169920/225000 (76%)] Loss: 18490.244141\n",
      "Train Epoch: 208 [172416/225000 (77%)] Loss: 17791.511719\n",
      "Train Epoch: 208 [174912/225000 (78%)] Loss: 19005.062500\n",
      "Train Epoch: 208 [177408/225000 (79%)] Loss: 18691.609375\n",
      "Train Epoch: 208 [179904/225000 (80%)] Loss: 18790.906250\n",
      "Train Epoch: 208 [182400/225000 (81%)] Loss: 18640.619141\n",
      "Train Epoch: 208 [184896/225000 (82%)] Loss: 18420.355469\n",
      "Train Epoch: 208 [187392/225000 (83%)] Loss: 17943.824219\n",
      "Train Epoch: 208 [189888/225000 (84%)] Loss: 18340.542969\n",
      "Train Epoch: 208 [192384/225000 (86%)] Loss: 18386.691406\n",
      "Train Epoch: 208 [194880/225000 (87%)] Loss: 17971.193359\n",
      "Train Epoch: 208 [197376/225000 (88%)] Loss: 18696.302734\n",
      "Train Epoch: 208 [199872/225000 (89%)] Loss: 18303.546875\n",
      "Train Epoch: 208 [202368/225000 (90%)] Loss: 18319.080078\n",
      "Train Epoch: 208 [204864/225000 (91%)] Loss: 18533.523438\n",
      "Train Epoch: 208 [207360/225000 (92%)] Loss: 21115.687500\n",
      "Train Epoch: 208 [209856/225000 (93%)] Loss: 18406.203125\n",
      "Train Epoch: 208 [212352/225000 (94%)] Loss: 18482.320312\n",
      "Train Epoch: 208 [214848/225000 (95%)] Loss: 18177.531250\n",
      "Train Epoch: 208 [217344/225000 (97%)] Loss: 18331.894531\n",
      "Train Epoch: 208 [219840/225000 (98%)] Loss: 18286.367188\n",
      "Train Epoch: 208 [222336/225000 (99%)] Loss: 18591.699219\n",
      "Train Epoch: 208 [224832/225000 (100%)] Loss: 18240.546875\n",
      "    epoch          : 208\n",
      "    loss           : 18493.989144491254\n",
      "    val_loss       : 18408.661455084348\n",
      "Train Epoch: 209 [192/225000 (0%)] Loss: 18356.500000\n",
      "Train Epoch: 209 [2688/225000 (1%)] Loss: 19174.955078\n",
      "Train Epoch: 209 [5184/225000 (2%)] Loss: 18353.056641\n",
      "Train Epoch: 209 [7680/225000 (3%)] Loss: 18307.902344\n",
      "Train Epoch: 209 [10176/225000 (5%)] Loss: 18457.501953\n",
      "Train Epoch: 209 [12672/225000 (6%)] Loss: 18452.771484\n",
      "Train Epoch: 209 [15168/225000 (7%)] Loss: 18814.757812\n",
      "Train Epoch: 209 [17664/225000 (8%)] Loss: 18482.296875\n",
      "Train Epoch: 209 [20160/225000 (9%)] Loss: 18297.640625\n",
      "Train Epoch: 209 [22656/225000 (10%)] Loss: 18443.136719\n",
      "Train Epoch: 209 [25152/225000 (11%)] Loss: 18407.289062\n",
      "Train Epoch: 209 [27648/225000 (12%)] Loss: 18255.644531\n",
      "Train Epoch: 209 [30144/225000 (13%)] Loss: 18552.074219\n",
      "Train Epoch: 209 [32640/225000 (15%)] Loss: 18319.628906\n",
      "Train Epoch: 209 [35136/225000 (16%)] Loss: 18807.769531\n",
      "Train Epoch: 209 [37632/225000 (17%)] Loss: 18375.335938\n",
      "Train Epoch: 209 [40128/225000 (18%)] Loss: 18087.156250\n",
      "Train Epoch: 209 [42624/225000 (19%)] Loss: 18749.976562\n",
      "Train Epoch: 209 [45120/225000 (20%)] Loss: 18537.757812\n",
      "Train Epoch: 209 [47616/225000 (21%)] Loss: 17980.492188\n",
      "Train Epoch: 209 [50112/225000 (22%)] Loss: 18865.792969\n",
      "Train Epoch: 209 [52608/225000 (23%)] Loss: 18068.332031\n",
      "Train Epoch: 209 [55104/225000 (24%)] Loss: 18317.060547\n",
      "Train Epoch: 209 [57600/225000 (26%)] Loss: 18107.701172\n",
      "Train Epoch: 209 [60096/225000 (27%)] Loss: 18020.224609\n",
      "Train Epoch: 209 [62592/225000 (28%)] Loss: 18473.414062\n",
      "Train Epoch: 209 [65088/225000 (29%)] Loss: 18301.578125\n",
      "Train Epoch: 209 [67584/225000 (30%)] Loss: 18245.300781\n",
      "Train Epoch: 209 [70080/225000 (31%)] Loss: 18116.451172\n",
      "Train Epoch: 209 [72576/225000 (32%)] Loss: 17907.425781\n",
      "Train Epoch: 209 [75072/225000 (33%)] Loss: 18278.988281\n",
      "Train Epoch: 209 [77568/225000 (34%)] Loss: 18606.677734\n",
      "Train Epoch: 209 [80064/225000 (36%)] Loss: 18441.763672\n",
      "Train Epoch: 209 [82560/225000 (37%)] Loss: 18272.216797\n",
      "Train Epoch: 209 [85056/225000 (38%)] Loss: 18211.638672\n",
      "Train Epoch: 209 [87552/225000 (39%)] Loss: 18389.421875\n",
      "Train Epoch: 209 [90048/225000 (40%)] Loss: 18839.937500\n",
      "Train Epoch: 209 [92544/225000 (41%)] Loss: 18437.837891\n",
      "Train Epoch: 209 [95040/225000 (42%)] Loss: 18605.921875\n",
      "Train Epoch: 209 [97536/225000 (43%)] Loss: 18469.830078\n",
      "Train Epoch: 209 [100032/225000 (44%)] Loss: 18237.050781\n",
      "Train Epoch: 209 [102528/225000 (46%)] Loss: 18383.812500\n",
      "Train Epoch: 209 [105024/225000 (47%)] Loss: 18347.105469\n",
      "Train Epoch: 209 [107520/225000 (48%)] Loss: 18513.476562\n",
      "Train Epoch: 209 [110016/225000 (49%)] Loss: 18637.777344\n",
      "Train Epoch: 209 [112512/225000 (50%)] Loss: 18410.843750\n",
      "Train Epoch: 209 [115008/225000 (51%)] Loss: 18186.125000\n",
      "Train Epoch: 209 [117504/225000 (52%)] Loss: 18462.296875\n",
      "Train Epoch: 209 [120000/225000 (53%)] Loss: 18222.378906\n",
      "Train Epoch: 209 [122496/225000 (54%)] Loss: 18014.619141\n",
      "Train Epoch: 209 [124992/225000 (56%)] Loss: 18644.070312\n",
      "Train Epoch: 209 [127488/225000 (57%)] Loss: 18192.005859\n",
      "Train Epoch: 209 [129984/225000 (58%)] Loss: 18960.476562\n",
      "Train Epoch: 209 [132480/225000 (59%)] Loss: 18336.218750\n",
      "Train Epoch: 209 [134976/225000 (60%)] Loss: 18081.021484\n",
      "Train Epoch: 209 [137472/225000 (61%)] Loss: 18366.804688\n",
      "Train Epoch: 209 [139968/225000 (62%)] Loss: 18529.335938\n",
      "Train Epoch: 209 [142464/225000 (63%)] Loss: 18875.906250\n",
      "Train Epoch: 209 [144960/225000 (64%)] Loss: 18869.423828\n",
      "Train Epoch: 209 [147456/225000 (66%)] Loss: 18642.218750\n",
      "Train Epoch: 209 [149952/225000 (67%)] Loss: 18567.015625\n",
      "Train Epoch: 209 [152448/225000 (68%)] Loss: 18143.792969\n",
      "Train Epoch: 209 [154944/225000 (69%)] Loss: 18187.160156\n",
      "Train Epoch: 209 [157440/225000 (70%)] Loss: 18596.164062\n",
      "Train Epoch: 209 [159936/225000 (71%)] Loss: 18553.595703\n",
      "Train Epoch: 209 [162432/225000 (72%)] Loss: 18303.878906\n",
      "Train Epoch: 209 [164928/225000 (73%)] Loss: 18532.199219\n",
      "Train Epoch: 209 [167424/225000 (74%)] Loss: 18907.455078\n",
      "Train Epoch: 209 [169920/225000 (76%)] Loss: 18915.875000\n",
      "Train Epoch: 209 [172416/225000 (77%)] Loss: 18131.847656\n",
      "Train Epoch: 209 [174912/225000 (78%)] Loss: 18825.769531\n",
      "Train Epoch: 209 [177408/225000 (79%)] Loss: 18598.621094\n",
      "Train Epoch: 209 [179904/225000 (80%)] Loss: 18960.605469\n",
      "Train Epoch: 209 [182400/225000 (81%)] Loss: 18708.312500\n",
      "Train Epoch: 209 [184896/225000 (82%)] Loss: 18565.085938\n",
      "Train Epoch: 209 [187392/225000 (83%)] Loss: 18333.238281\n",
      "Train Epoch: 209 [189888/225000 (84%)] Loss: 18195.695312\n",
      "Train Epoch: 209 [192384/225000 (86%)] Loss: 18736.080078\n",
      "Train Epoch: 209 [194880/225000 (87%)] Loss: 18372.937500\n",
      "Train Epoch: 209 [197376/225000 (88%)] Loss: 18698.753906\n",
      "Train Epoch: 209 [199872/225000 (89%)] Loss: 18029.195312\n",
      "Train Epoch: 209 [202368/225000 (90%)] Loss: 18324.582031\n",
      "Train Epoch: 209 [204864/225000 (91%)] Loss: 17930.800781\n",
      "Train Epoch: 209 [207360/225000 (92%)] Loss: 18878.542969\n",
      "Train Epoch: 209 [209856/225000 (93%)] Loss: 18500.191406\n",
      "Train Epoch: 209 [212352/225000 (94%)] Loss: 18772.947266\n",
      "Train Epoch: 209 [214848/225000 (95%)] Loss: 18619.298828\n",
      "Train Epoch: 209 [217344/225000 (97%)] Loss: 18410.886719\n",
      "Train Epoch: 209 [219840/225000 (98%)] Loss: 18561.281250\n",
      "Train Epoch: 209 [222336/225000 (99%)] Loss: 18722.195312\n",
      "Train Epoch: 209 [224832/225000 (100%)] Loss: 18293.257812\n",
      "    epoch          : 209\n",
      "    loss           : 18463.974940173048\n",
      "    val_loss       : 18354.363769051684\n",
      "Train Epoch: 210 [192/225000 (0%)] Loss: 18176.707031\n",
      "Train Epoch: 210 [2688/225000 (1%)] Loss: 20982.816406\n",
      "Train Epoch: 210 [5184/225000 (2%)] Loss: 18778.060547\n",
      "Train Epoch: 210 [7680/225000 (3%)] Loss: 18466.802734\n",
      "Train Epoch: 210 [10176/225000 (5%)] Loss: 18710.714844\n",
      "Train Epoch: 210 [12672/225000 (6%)] Loss: 19065.285156\n",
      "Train Epoch: 210 [15168/225000 (7%)] Loss: 18540.300781\n",
      "Train Epoch: 210 [17664/225000 (8%)] Loss: 18674.759766\n",
      "Train Epoch: 210 [20160/225000 (9%)] Loss: 18339.917969\n",
      "Train Epoch: 210 [22656/225000 (10%)] Loss: 18358.306641\n",
      "Train Epoch: 210 [25152/225000 (11%)] Loss: 18250.037109\n",
      "Train Epoch: 210 [27648/225000 (12%)] Loss: 18769.894531\n",
      "Train Epoch: 210 [30144/225000 (13%)] Loss: 18118.531250\n",
      "Train Epoch: 210 [32640/225000 (15%)] Loss: 18487.093750\n",
      "Train Epoch: 210 [35136/225000 (16%)] Loss: 18644.025391\n",
      "Train Epoch: 210 [37632/225000 (17%)] Loss: 18125.957031\n",
      "Train Epoch: 210 [40128/225000 (18%)] Loss: 18154.097656\n",
      "Train Epoch: 210 [42624/225000 (19%)] Loss: 18771.425781\n",
      "Train Epoch: 210 [45120/225000 (20%)] Loss: 18042.925781\n",
      "Train Epoch: 210 [47616/225000 (21%)] Loss: 18545.509766\n",
      "Train Epoch: 210 [50112/225000 (22%)] Loss: 18340.035156\n",
      "Train Epoch: 210 [52608/225000 (23%)] Loss: 18499.820312\n",
      "Train Epoch: 210 [55104/225000 (24%)] Loss: 18628.796875\n",
      "Train Epoch: 210 [57600/225000 (26%)] Loss: 17949.376953\n",
      "Train Epoch: 210 [60096/225000 (27%)] Loss: 19026.082031\n",
      "Train Epoch: 210 [62592/225000 (28%)] Loss: 18873.007812\n",
      "Train Epoch: 210 [65088/225000 (29%)] Loss: 17752.320312\n",
      "Train Epoch: 210 [67584/225000 (30%)] Loss: 18305.769531\n",
      "Train Epoch: 210 [70080/225000 (31%)] Loss: 18588.367188\n",
      "Train Epoch: 210 [72576/225000 (32%)] Loss: 18033.441406\n",
      "Train Epoch: 210 [75072/225000 (33%)] Loss: 18777.871094\n",
      "Train Epoch: 210 [77568/225000 (34%)] Loss: 18373.792969\n",
      "Train Epoch: 210 [80064/225000 (36%)] Loss: 17896.281250\n",
      "Train Epoch: 210 [82560/225000 (37%)] Loss: 18268.148438\n",
      "Train Epoch: 210 [85056/225000 (38%)] Loss: 18579.052734\n",
      "Train Epoch: 210 [87552/225000 (39%)] Loss: 18597.222656\n",
      "Train Epoch: 210 [90048/225000 (40%)] Loss: 18355.039062\n",
      "Train Epoch: 210 [92544/225000 (41%)] Loss: 18234.142578\n",
      "Train Epoch: 210 [95040/225000 (42%)] Loss: 18449.634766\n",
      "Train Epoch: 210 [97536/225000 (43%)] Loss: 18472.207031\n",
      "Train Epoch: 210 [100032/225000 (44%)] Loss: 18592.621094\n",
      "Train Epoch: 210 [102528/225000 (46%)] Loss: 18112.996094\n",
      "Train Epoch: 210 [105024/225000 (47%)] Loss: 17715.722656\n",
      "Train Epoch: 210 [107520/225000 (48%)] Loss: 18362.205078\n",
      "Train Epoch: 210 [110016/225000 (49%)] Loss: 18460.498047\n",
      "Train Epoch: 210 [112512/225000 (50%)] Loss: 17660.640625\n",
      "Train Epoch: 210 [115008/225000 (51%)] Loss: 18635.894531\n",
      "Train Epoch: 210 [117504/225000 (52%)] Loss: 17968.691406\n",
      "Train Epoch: 210 [120000/225000 (53%)] Loss: 18221.441406\n",
      "Train Epoch: 210 [122496/225000 (54%)] Loss: 18316.794922\n",
      "Train Epoch: 210 [124992/225000 (56%)] Loss: 18352.947266\n",
      "Train Epoch: 210 [127488/225000 (57%)] Loss: 18457.736328\n",
      "Train Epoch: 210 [129984/225000 (58%)] Loss: 18229.308594\n",
      "Train Epoch: 210 [132480/225000 (59%)] Loss: 18417.082031\n",
      "Train Epoch: 210 [134976/225000 (60%)] Loss: 18319.546875\n",
      "Train Epoch: 210 [137472/225000 (61%)] Loss: 18269.976562\n",
      "Train Epoch: 210 [139968/225000 (62%)] Loss: 18921.968750\n",
      "Train Epoch: 210 [142464/225000 (63%)] Loss: 18577.277344\n",
      "Train Epoch: 210 [144960/225000 (64%)] Loss: 18552.996094\n",
      "Train Epoch: 210 [147456/225000 (66%)] Loss: 18575.796875\n",
      "Train Epoch: 210 [149952/225000 (67%)] Loss: 18576.257812\n",
      "Train Epoch: 210 [152448/225000 (68%)] Loss: 18493.644531\n",
      "Train Epoch: 210 [154944/225000 (69%)] Loss: 18549.433594\n",
      "Train Epoch: 210 [157440/225000 (70%)] Loss: 18559.605469\n",
      "Train Epoch: 210 [159936/225000 (71%)] Loss: 18647.404297\n",
      "Train Epoch: 210 [162432/225000 (72%)] Loss: 18651.996094\n",
      "Train Epoch: 210 [164928/225000 (73%)] Loss: 19034.728516\n",
      "Train Epoch: 210 [167424/225000 (74%)] Loss: 18127.455078\n",
      "Train Epoch: 210 [169920/225000 (76%)] Loss: 18446.779297\n",
      "Train Epoch: 210 [172416/225000 (77%)] Loss: 18361.703125\n",
      "Train Epoch: 210 [174912/225000 (78%)] Loss: 18804.681641\n",
      "Train Epoch: 210 [177408/225000 (79%)] Loss: 18294.193359\n",
      "Train Epoch: 210 [179904/225000 (80%)] Loss: 18515.628906\n",
      "Train Epoch: 210 [182400/225000 (81%)] Loss: 18592.742188\n",
      "Train Epoch: 210 [184896/225000 (82%)] Loss: 18072.843750\n",
      "Train Epoch: 210 [187392/225000 (83%)] Loss: 18701.285156\n",
      "Train Epoch: 210 [189888/225000 (84%)] Loss: 18584.421875\n",
      "Train Epoch: 210 [192384/225000 (86%)] Loss: 18541.437500\n",
      "Train Epoch: 210 [194880/225000 (87%)] Loss: 18339.730469\n",
      "Train Epoch: 210 [197376/225000 (88%)] Loss: 18573.113281\n",
      "Train Epoch: 210 [199872/225000 (89%)] Loss: 18770.908203\n",
      "Train Epoch: 210 [202368/225000 (90%)] Loss: 18504.734375\n",
      "Train Epoch: 210 [204864/225000 (91%)] Loss: 18100.226562\n",
      "Train Epoch: 210 [207360/225000 (92%)] Loss: 18318.648438\n",
      "Train Epoch: 210 [209856/225000 (93%)] Loss: 18364.333984\n",
      "Train Epoch: 210 [212352/225000 (94%)] Loss: 18498.917969\n",
      "Train Epoch: 210 [214848/225000 (95%)] Loss: 17928.707031\n",
      "Train Epoch: 210 [217344/225000 (97%)] Loss: 18501.181641\n",
      "Train Epoch: 210 [219840/225000 (98%)] Loss: 19323.488281\n",
      "Train Epoch: 210 [222336/225000 (99%)] Loss: 18806.904297\n",
      "Train Epoch: 210 [224832/225000 (100%)] Loss: 18691.517578\n",
      "    epoch          : 210\n",
      "    loss           : 18438.523973276184\n",
      "    val_loss       : 18324.887087560335\n",
      "Train Epoch: 211 [192/225000 (0%)] Loss: 18138.037109\n",
      "Train Epoch: 211 [2688/225000 (1%)] Loss: 18295.414062\n",
      "Train Epoch: 211 [5184/225000 (2%)] Loss: 18848.154297\n",
      "Train Epoch: 211 [7680/225000 (3%)] Loss: 18353.242188\n",
      "Train Epoch: 211 [10176/225000 (5%)] Loss: 18453.421875\n",
      "Train Epoch: 211 [12672/225000 (6%)] Loss: 18025.925781\n",
      "Train Epoch: 211 [15168/225000 (7%)] Loss: 18553.750000\n",
      "Train Epoch: 211 [17664/225000 (8%)] Loss: 17959.316406\n",
      "Train Epoch: 211 [20160/225000 (9%)] Loss: 18848.914062\n",
      "Train Epoch: 211 [22656/225000 (10%)] Loss: 18096.171875\n",
      "Train Epoch: 211 [25152/225000 (11%)] Loss: 18704.595703\n",
      "Train Epoch: 211 [27648/225000 (12%)] Loss: 18181.132812\n",
      "Train Epoch: 211 [30144/225000 (13%)] Loss: 18406.232422\n",
      "Train Epoch: 211 [32640/225000 (15%)] Loss: 18438.000000\n",
      "Train Epoch: 211 [35136/225000 (16%)] Loss: 18325.312500\n",
      "Train Epoch: 211 [37632/225000 (17%)] Loss: 18390.457031\n",
      "Train Epoch: 211 [40128/225000 (18%)] Loss: 18671.878906\n",
      "Train Epoch: 211 [42624/225000 (19%)] Loss: 18379.589844\n",
      "Train Epoch: 211 [45120/225000 (20%)] Loss: 18432.740234\n",
      "Train Epoch: 211 [47616/225000 (21%)] Loss: 18641.058594\n",
      "Train Epoch: 211 [50112/225000 (22%)] Loss: 18173.882812\n",
      "Train Epoch: 211 [52608/225000 (23%)] Loss: 18174.679688\n",
      "Train Epoch: 211 [55104/225000 (24%)] Loss: 18421.605469\n",
      "Train Epoch: 211 [57600/225000 (26%)] Loss: 18751.039062\n",
      "Train Epoch: 211 [60096/225000 (27%)] Loss: 18490.736328\n",
      "Train Epoch: 211 [62592/225000 (28%)] Loss: 18372.316406\n",
      "Train Epoch: 211 [65088/225000 (29%)] Loss: 18449.585938\n",
      "Train Epoch: 211 [67584/225000 (30%)] Loss: 18608.945312\n",
      "Train Epoch: 211 [70080/225000 (31%)] Loss: 18276.552734\n",
      "Train Epoch: 211 [72576/225000 (32%)] Loss: 18624.654297\n",
      "Train Epoch: 211 [75072/225000 (33%)] Loss: 18757.705078\n",
      "Train Epoch: 211 [77568/225000 (34%)] Loss: 17938.169922\n",
      "Train Epoch: 211 [80064/225000 (36%)] Loss: 18156.312500\n",
      "Train Epoch: 211 [82560/225000 (37%)] Loss: 18577.859375\n",
      "Train Epoch: 211 [85056/225000 (38%)] Loss: 17701.660156\n",
      "Train Epoch: 211 [87552/225000 (39%)] Loss: 18476.566406\n",
      "Train Epoch: 211 [90048/225000 (40%)] Loss: 18159.169922\n",
      "Train Epoch: 211 [92544/225000 (41%)] Loss: 18136.234375\n",
      "Train Epoch: 211 [95040/225000 (42%)] Loss: 18188.312500\n",
      "Train Epoch: 211 [97536/225000 (43%)] Loss: 18679.121094\n",
      "Train Epoch: 211 [100032/225000 (44%)] Loss: 18372.802734\n",
      "Train Epoch: 211 [102528/225000 (46%)] Loss: 18416.878906\n",
      "Train Epoch: 211 [105024/225000 (47%)] Loss: 18885.492188\n",
      "Train Epoch: 211 [107520/225000 (48%)] Loss: 18211.847656\n",
      "Train Epoch: 211 [110016/225000 (49%)] Loss: 18590.681641\n",
      "Train Epoch: 211 [112512/225000 (50%)] Loss: 18622.382812\n",
      "Train Epoch: 211 [115008/225000 (51%)] Loss: 18217.783203\n",
      "Train Epoch: 211 [117504/225000 (52%)] Loss: 18382.292969\n",
      "Train Epoch: 211 [120000/225000 (53%)] Loss: 17976.003906\n",
      "Train Epoch: 211 [122496/225000 (54%)] Loss: 19113.751953\n",
      "Train Epoch: 211 [124992/225000 (56%)] Loss: 18266.871094\n",
      "Train Epoch: 211 [127488/225000 (57%)] Loss: 18372.982422\n",
      "Train Epoch: 211 [129984/225000 (58%)] Loss: 18874.402344\n",
      "Train Epoch: 211 [132480/225000 (59%)] Loss: 18333.585938\n",
      "Train Epoch: 211 [134976/225000 (60%)] Loss: 18513.177734\n",
      "Train Epoch: 211 [137472/225000 (61%)] Loss: 18542.365234\n",
      "Train Epoch: 211 [139968/225000 (62%)] Loss: 18087.234375\n",
      "Train Epoch: 211 [142464/225000 (63%)] Loss: 18841.218750\n",
      "Train Epoch: 211 [144960/225000 (64%)] Loss: 17848.187500\n",
      "Train Epoch: 211 [147456/225000 (66%)] Loss: 18714.828125\n",
      "Train Epoch: 211 [149952/225000 (67%)] Loss: 18596.019531\n",
      "Train Epoch: 211 [152448/225000 (68%)] Loss: 17955.378906\n",
      "Train Epoch: 211 [154944/225000 (69%)] Loss: 18413.292969\n",
      "Train Epoch: 211 [157440/225000 (70%)] Loss: 18177.351562\n",
      "Train Epoch: 211 [159936/225000 (71%)] Loss: 18966.335938\n",
      "Train Epoch: 211 [162432/225000 (72%)] Loss: 18218.597656\n",
      "Train Epoch: 211 [164928/225000 (73%)] Loss: 18183.917969\n",
      "Train Epoch: 211 [167424/225000 (74%)] Loss: 18265.572266\n",
      "Train Epoch: 211 [169920/225000 (76%)] Loss: 18428.000000\n",
      "Train Epoch: 211 [172416/225000 (77%)] Loss: 17776.910156\n",
      "Train Epoch: 211 [174912/225000 (78%)] Loss: 18637.310547\n",
      "Train Epoch: 211 [177408/225000 (79%)] Loss: 18181.867188\n",
      "Train Epoch: 211 [179904/225000 (80%)] Loss: 19261.484375\n",
      "Train Epoch: 211 [182400/225000 (81%)] Loss: 18704.371094\n",
      "Train Epoch: 211 [184896/225000 (82%)] Loss: 18635.873047\n",
      "Train Epoch: 211 [187392/225000 (83%)] Loss: 18547.039062\n",
      "Train Epoch: 211 [189888/225000 (84%)] Loss: 18065.966797\n",
      "Train Epoch: 211 [192384/225000 (86%)] Loss: 18423.632812\n",
      "Train Epoch: 211 [194880/225000 (87%)] Loss: 18085.710938\n",
      "Train Epoch: 211 [197376/225000 (88%)] Loss: 18862.398438\n",
      "Train Epoch: 211 [199872/225000 (89%)] Loss: 18237.253906\n",
      "Train Epoch: 211 [202368/225000 (90%)] Loss: 17893.757812\n",
      "Train Epoch: 211 [204864/225000 (91%)] Loss: 18059.046875\n",
      "Train Epoch: 211 [207360/225000 (92%)] Loss: 18275.197266\n",
      "Train Epoch: 211 [209856/225000 (93%)] Loss: 18353.353516\n",
      "Train Epoch: 211 [212352/225000 (94%)] Loss: 18072.453125\n",
      "Train Epoch: 211 [214848/225000 (95%)] Loss: 18341.916016\n",
      "Train Epoch: 211 [217344/225000 (97%)] Loss: 18580.789062\n",
      "Train Epoch: 211 [219840/225000 (98%)] Loss: 18149.074219\n",
      "Train Epoch: 211 [222336/225000 (99%)] Loss: 18043.437500\n",
      "Train Epoch: 211 [224832/225000 (100%)] Loss: 18404.820312\n",
      "    epoch          : 211\n",
      "    loss           : 18403.72745073859\n",
      "    val_loss       : 18298.839443579884\n",
      "Train Epoch: 212 [192/225000 (0%)] Loss: 18240.054688\n",
      "Train Epoch: 212 [2688/225000 (1%)] Loss: 18956.035156\n",
      "Train Epoch: 212 [5184/225000 (2%)] Loss: 18570.066406\n",
      "Train Epoch: 212 [7680/225000 (3%)] Loss: 18859.714844\n",
      "Train Epoch: 212 [10176/225000 (5%)] Loss: 18359.886719\n",
      "Train Epoch: 212 [12672/225000 (6%)] Loss: 18270.224609\n",
      "Train Epoch: 212 [15168/225000 (7%)] Loss: 18446.347656\n",
      "Train Epoch: 212 [17664/225000 (8%)] Loss: 18147.292969\n",
      "Train Epoch: 212 [20160/225000 (9%)] Loss: 18261.648438\n",
      "Train Epoch: 212 [22656/225000 (10%)] Loss: 18357.595703\n",
      "Train Epoch: 212 [25152/225000 (11%)] Loss: 18229.138672\n",
      "Train Epoch: 212 [27648/225000 (12%)] Loss: 18479.210938\n",
      "Train Epoch: 212 [30144/225000 (13%)] Loss: 18485.486328\n",
      "Train Epoch: 212 [32640/225000 (15%)] Loss: 18629.210938\n",
      "Train Epoch: 212 [35136/225000 (16%)] Loss: 18230.890625\n",
      "Train Epoch: 212 [37632/225000 (17%)] Loss: 18231.761719\n",
      "Train Epoch: 212 [40128/225000 (18%)] Loss: 18726.269531\n",
      "Train Epoch: 212 [42624/225000 (19%)] Loss: 18195.351562\n",
      "Train Epoch: 212 [45120/225000 (20%)] Loss: 18508.480469\n",
      "Train Epoch: 212 [47616/225000 (21%)] Loss: 18016.582031\n",
      "Train Epoch: 212 [50112/225000 (22%)] Loss: 18118.083984\n",
      "Train Epoch: 212 [52608/225000 (23%)] Loss: 18641.548828\n",
      "Train Epoch: 212 [55104/225000 (24%)] Loss: 18259.917969\n",
      "Train Epoch: 212 [57600/225000 (26%)] Loss: 18059.910156\n",
      "Train Epoch: 212 [60096/225000 (27%)] Loss: 18188.656250\n",
      "Train Epoch: 212 [62592/225000 (28%)] Loss: 18398.890625\n",
      "Train Epoch: 212 [65088/225000 (29%)] Loss: 18155.214844\n",
      "Train Epoch: 212 [67584/225000 (30%)] Loss: 18137.187500\n",
      "Train Epoch: 212 [70080/225000 (31%)] Loss: 18179.863281\n",
      "Train Epoch: 212 [72576/225000 (32%)] Loss: 19127.187500\n",
      "Train Epoch: 212 [75072/225000 (33%)] Loss: 17808.507812\n",
      "Train Epoch: 212 [77568/225000 (34%)] Loss: 18391.216797\n",
      "Train Epoch: 212 [80064/225000 (36%)] Loss: 18153.234375\n",
      "Train Epoch: 212 [82560/225000 (37%)] Loss: 18173.648438\n",
      "Train Epoch: 212 [85056/225000 (38%)] Loss: 18150.902344\n",
      "Train Epoch: 212 [87552/225000 (39%)] Loss: 18419.667969\n",
      "Train Epoch: 212 [90048/225000 (40%)] Loss: 18093.593750\n",
      "Train Epoch: 212 [92544/225000 (41%)] Loss: 18106.531250\n",
      "Train Epoch: 212 [95040/225000 (42%)] Loss: 17626.207031\n",
      "Train Epoch: 212 [97536/225000 (43%)] Loss: 17803.656250\n",
      "Train Epoch: 212 [100032/225000 (44%)] Loss: 18221.367188\n",
      "Train Epoch: 212 [102528/225000 (46%)] Loss: 18699.160156\n",
      "Train Epoch: 212 [105024/225000 (47%)] Loss: 18821.531250\n",
      "Train Epoch: 212 [107520/225000 (48%)] Loss: 18161.212891\n",
      "Train Epoch: 212 [110016/225000 (49%)] Loss: 18583.277344\n",
      "Train Epoch: 212 [112512/225000 (50%)] Loss: 18499.281250\n",
      "Train Epoch: 212 [115008/225000 (51%)] Loss: 18704.390625\n",
      "Train Epoch: 212 [117504/225000 (52%)] Loss: 18711.507812\n",
      "Train Epoch: 212 [120000/225000 (53%)] Loss: 18380.156250\n",
      "Train Epoch: 212 [122496/225000 (54%)] Loss: 18542.265625\n",
      "Train Epoch: 212 [124992/225000 (56%)] Loss: 18102.128906\n",
      "Train Epoch: 212 [127488/225000 (57%)] Loss: 18523.296875\n",
      "Train Epoch: 212 [129984/225000 (58%)] Loss: 18821.289062\n",
      "Train Epoch: 212 [132480/225000 (59%)] Loss: 18599.607422\n",
      "Train Epoch: 212 [134976/225000 (60%)] Loss: 18914.185547\n",
      "Train Epoch: 212 [137472/225000 (61%)] Loss: 18206.847656\n",
      "Train Epoch: 212 [139968/225000 (62%)] Loss: 18916.234375\n",
      "Train Epoch: 212 [142464/225000 (63%)] Loss: 18388.585938\n",
      "Train Epoch: 212 [144960/225000 (64%)] Loss: 18379.234375\n",
      "Train Epoch: 212 [147456/225000 (66%)] Loss: 18429.800781\n",
      "Train Epoch: 212 [149952/225000 (67%)] Loss: 18378.498047\n",
      "Train Epoch: 212 [152448/225000 (68%)] Loss: 18283.652344\n",
      "Train Epoch: 212 [154944/225000 (69%)] Loss: 18386.101562\n",
      "Train Epoch: 212 [157440/225000 (70%)] Loss: 18405.042969\n",
      "Train Epoch: 212 [159936/225000 (71%)] Loss: 18204.330078\n",
      "Train Epoch: 212 [162432/225000 (72%)] Loss: 18241.707031\n",
      "Train Epoch: 212 [164928/225000 (73%)] Loss: 18151.818359\n",
      "Train Epoch: 212 [167424/225000 (74%)] Loss: 18280.474609\n",
      "Train Epoch: 212 [169920/225000 (76%)] Loss: 18177.958984\n",
      "Train Epoch: 212 [172416/225000 (77%)] Loss: 18531.101562\n",
      "Train Epoch: 212 [174912/225000 (78%)] Loss: 17766.406250\n",
      "Train Epoch: 212 [177408/225000 (79%)] Loss: 18247.312500\n",
      "Train Epoch: 212 [179904/225000 (80%)] Loss: 18445.556641\n",
      "Train Epoch: 212 [182400/225000 (81%)] Loss: 18623.076172\n",
      "Train Epoch: 212 [184896/225000 (82%)] Loss: 18249.425781\n",
      "Train Epoch: 212 [187392/225000 (83%)] Loss: 17897.363281\n",
      "Train Epoch: 212 [189888/225000 (84%)] Loss: 18835.097656\n",
      "Train Epoch: 212 [192384/225000 (86%)] Loss: 18081.455078\n",
      "Train Epoch: 212 [194880/225000 (87%)] Loss: 18217.003906\n",
      "Train Epoch: 212 [197376/225000 (88%)] Loss: 18758.164062\n",
      "Train Epoch: 212 [199872/225000 (89%)] Loss: 18975.101562\n",
      "Train Epoch: 212 [202368/225000 (90%)] Loss: 18025.742188\n",
      "Train Epoch: 212 [204864/225000 (91%)] Loss: 18290.390625\n",
      "Train Epoch: 212 [207360/225000 (92%)] Loss: 18443.355469\n",
      "Train Epoch: 212 [209856/225000 (93%)] Loss: 18715.238281\n",
      "Train Epoch: 212 [212352/225000 (94%)] Loss: 18519.683594\n",
      "Train Epoch: 212 [214848/225000 (95%)] Loss: 18819.312500\n",
      "Train Epoch: 212 [217344/225000 (97%)] Loss: 18030.816406\n",
      "Train Epoch: 212 [219840/225000 (98%)] Loss: 18583.515625\n",
      "Train Epoch: 212 [222336/225000 (99%)] Loss: 18365.160156\n",
      "Train Epoch: 212 [224832/225000 (100%)] Loss: 18834.820312\n",
      "    epoch          : 212\n",
      "    loss           : 18372.615680994026\n",
      "    val_loss       : 18265.807080653787\n",
      "Train Epoch: 213 [192/225000 (0%)] Loss: 18304.324219\n",
      "Train Epoch: 213 [2688/225000 (1%)] Loss: 18447.218750\n",
      "Train Epoch: 213 [5184/225000 (2%)] Loss: 18038.828125\n",
      "Train Epoch: 213 [7680/225000 (3%)] Loss: 18187.748047\n",
      "Train Epoch: 213 [10176/225000 (5%)] Loss: 18063.777344\n",
      "Train Epoch: 213 [12672/225000 (6%)] Loss: 18489.919922\n",
      "Train Epoch: 213 [15168/225000 (7%)] Loss: 18533.787109\n",
      "Train Epoch: 213 [17664/225000 (8%)] Loss: 18491.164062\n",
      "Train Epoch: 213 [20160/225000 (9%)] Loss: 17994.789062\n",
      "Train Epoch: 213 [22656/225000 (10%)] Loss: 18375.869141\n",
      "Train Epoch: 213 [25152/225000 (11%)] Loss: 18476.771484\n",
      "Train Epoch: 213 [27648/225000 (12%)] Loss: 18463.894531\n",
      "Train Epoch: 213 [30144/225000 (13%)] Loss: 18467.875000\n",
      "Train Epoch: 213 [32640/225000 (15%)] Loss: 18528.390625\n",
      "Train Epoch: 213 [35136/225000 (16%)] Loss: 18252.964844\n",
      "Train Epoch: 213 [37632/225000 (17%)] Loss: 18274.535156\n",
      "Train Epoch: 213 [40128/225000 (18%)] Loss: 18605.894531\n",
      "Train Epoch: 213 [42624/225000 (19%)] Loss: 17761.617188\n",
      "Train Epoch: 213 [45120/225000 (20%)] Loss: 18821.853516\n",
      "Train Epoch: 213 [47616/225000 (21%)] Loss: 18342.855469\n",
      "Train Epoch: 213 [50112/225000 (22%)] Loss: 18657.316406\n",
      "Train Epoch: 213 [52608/225000 (23%)] Loss: 18586.291016\n",
      "Train Epoch: 213 [55104/225000 (24%)] Loss: 18818.343750\n",
      "Train Epoch: 213 [57600/225000 (26%)] Loss: 18638.480469\n",
      "Train Epoch: 213 [60096/225000 (27%)] Loss: 18523.433594\n",
      "Train Epoch: 213 [62592/225000 (28%)] Loss: 18175.468750\n",
      "Train Epoch: 213 [65088/225000 (29%)] Loss: 18557.658203\n",
      "Train Epoch: 213 [67584/225000 (30%)] Loss: 18161.011719\n",
      "Train Epoch: 213 [70080/225000 (31%)] Loss: 18865.316406\n",
      "Train Epoch: 213 [72576/225000 (32%)] Loss: 18372.324219\n",
      "Train Epoch: 213 [75072/225000 (33%)] Loss: 18734.515625\n",
      "Train Epoch: 213 [77568/225000 (34%)] Loss: 17982.519531\n",
      "Train Epoch: 213 [80064/225000 (36%)] Loss: 18283.714844\n",
      "Train Epoch: 213 [82560/225000 (37%)] Loss: 18039.732422\n",
      "Train Epoch: 213 [85056/225000 (38%)] Loss: 18138.230469\n",
      "Train Epoch: 213 [87552/225000 (39%)] Loss: 18015.687500\n",
      "Train Epoch: 213 [90048/225000 (40%)] Loss: 18300.312500\n",
      "Train Epoch: 213 [92544/225000 (41%)] Loss: 18275.574219\n",
      "Train Epoch: 213 [95040/225000 (42%)] Loss: 18296.570312\n",
      "Train Epoch: 213 [97536/225000 (43%)] Loss: 18180.244141\n",
      "Train Epoch: 213 [100032/225000 (44%)] Loss: 18505.578125\n",
      "Train Epoch: 213 [102528/225000 (46%)] Loss: 18309.451172\n",
      "Train Epoch: 213 [105024/225000 (47%)] Loss: 17790.691406\n",
      "Train Epoch: 213 [107520/225000 (48%)] Loss: 18414.720703\n",
      "Train Epoch: 213 [110016/225000 (49%)] Loss: 18549.843750\n",
      "Train Epoch: 213 [112512/225000 (50%)] Loss: 18091.228516\n",
      "Train Epoch: 213 [115008/225000 (51%)] Loss: 18325.933594\n",
      "Train Epoch: 213 [117504/225000 (52%)] Loss: 18505.822266\n",
      "Train Epoch: 213 [120000/225000 (53%)] Loss: 18232.417969\n",
      "Train Epoch: 213 [122496/225000 (54%)] Loss: 17874.134766\n",
      "Train Epoch: 213 [124992/225000 (56%)] Loss: 18563.566406\n",
      "Train Epoch: 213 [127488/225000 (57%)] Loss: 18682.140625\n",
      "Train Epoch: 213 [129984/225000 (58%)] Loss: 18227.406250\n",
      "Train Epoch: 213 [132480/225000 (59%)] Loss: 18573.058594\n",
      "Train Epoch: 213 [134976/225000 (60%)] Loss: 17923.378906\n",
      "Train Epoch: 213 [137472/225000 (61%)] Loss: 18675.298828\n",
      "Train Epoch: 213 [139968/225000 (62%)] Loss: 18320.996094\n",
      "Train Epoch: 213 [142464/225000 (63%)] Loss: 18473.095703\n",
      "Train Epoch: 213 [144960/225000 (64%)] Loss: 18603.382812\n",
      "Train Epoch: 213 [147456/225000 (66%)] Loss: 18559.410156\n",
      "Train Epoch: 213 [149952/225000 (67%)] Loss: 18288.695312\n",
      "Train Epoch: 213 [152448/225000 (68%)] Loss: 18200.902344\n",
      "Train Epoch: 213 [154944/225000 (69%)] Loss: 18330.660156\n",
      "Train Epoch: 213 [157440/225000 (70%)] Loss: 18515.574219\n",
      "Train Epoch: 213 [159936/225000 (71%)] Loss: 18177.345703\n",
      "Train Epoch: 213 [162432/225000 (72%)] Loss: 18325.476562\n",
      "Train Epoch: 213 [164928/225000 (73%)] Loss: 18353.285156\n",
      "Train Epoch: 213 [167424/225000 (74%)] Loss: 18689.695312\n",
      "Train Epoch: 213 [169920/225000 (76%)] Loss: 18139.226562\n",
      "Train Epoch: 213 [172416/225000 (77%)] Loss: 18607.386719\n",
      "Train Epoch: 213 [174912/225000 (78%)] Loss: 18017.792969\n",
      "Train Epoch: 213 [177408/225000 (79%)] Loss: 18631.648438\n",
      "Train Epoch: 213 [179904/225000 (80%)] Loss: 18120.070312\n",
      "Train Epoch: 213 [182400/225000 (81%)] Loss: 18543.882812\n",
      "Train Epoch: 213 [184896/225000 (82%)] Loss: 18705.718750\n",
      "Train Epoch: 213 [187392/225000 (83%)] Loss: 18257.636719\n",
      "Train Epoch: 213 [189888/225000 (84%)] Loss: 18538.441406\n",
      "Train Epoch: 213 [192384/225000 (86%)] Loss: 18082.714844\n",
      "Train Epoch: 213 [194880/225000 (87%)] Loss: 18050.753906\n",
      "Train Epoch: 213 [197376/225000 (88%)] Loss: 18076.234375\n",
      "Train Epoch: 213 [199872/225000 (89%)] Loss: 18216.117188\n",
      "Train Epoch: 213 [202368/225000 (90%)] Loss: 18250.218750\n",
      "Train Epoch: 213 [204864/225000 (91%)] Loss: 18284.492188\n",
      "Train Epoch: 213 [207360/225000 (92%)] Loss: 18658.021484\n",
      "Train Epoch: 213 [209856/225000 (93%)] Loss: 18279.960938\n",
      "Train Epoch: 213 [212352/225000 (94%)] Loss: 17816.232422\n",
      "Train Epoch: 213 [214848/225000 (95%)] Loss: 18569.105469\n",
      "Train Epoch: 213 [217344/225000 (97%)] Loss: 18737.970703\n",
      "Train Epoch: 213 [219840/225000 (98%)] Loss: 18074.763672\n",
      "Train Epoch: 213 [222336/225000 (99%)] Loss: 18519.962891\n",
      "Train Epoch: 213 [224832/225000 (100%)] Loss: 18211.644531\n",
      "    epoch          : 213\n",
      "    loss           : 18340.421762512\n",
      "    val_loss       : 18235.557774929144\n",
      "Train Epoch: 214 [192/225000 (0%)] Loss: 18359.121094\n",
      "Train Epoch: 214 [2688/225000 (1%)] Loss: 18311.447266\n",
      "Train Epoch: 214 [5184/225000 (2%)] Loss: 18451.025391\n",
      "Train Epoch: 214 [7680/225000 (3%)] Loss: 18554.125000\n",
      "Train Epoch: 214 [10176/225000 (5%)] Loss: 18458.238281\n",
      "Train Epoch: 214 [12672/225000 (6%)] Loss: 18454.466797\n",
      "Train Epoch: 214 [15168/225000 (7%)] Loss: 18357.042969\n",
      "Train Epoch: 214 [17664/225000 (8%)] Loss: 18707.537109\n",
      "Train Epoch: 214 [20160/225000 (9%)] Loss: 18244.417969\n",
      "Train Epoch: 214 [22656/225000 (10%)] Loss: 18123.222656\n",
      "Train Epoch: 214 [25152/225000 (11%)] Loss: 18145.367188\n",
      "Train Epoch: 214 [27648/225000 (12%)] Loss: 18274.460938\n",
      "Train Epoch: 214 [30144/225000 (13%)] Loss: 17789.667969\n",
      "Train Epoch: 214 [32640/225000 (15%)] Loss: 17888.550781\n",
      "Train Epoch: 214 [35136/225000 (16%)] Loss: 17890.773438\n",
      "Train Epoch: 214 [37632/225000 (17%)] Loss: 18187.460938\n",
      "Train Epoch: 214 [40128/225000 (18%)] Loss: 18643.603516\n",
      "Train Epoch: 214 [42624/225000 (19%)] Loss: 18141.677734\n",
      "Train Epoch: 214 [45120/225000 (20%)] Loss: 17942.388672\n",
      "Train Epoch: 214 [47616/225000 (21%)] Loss: 18339.501953\n",
      "Train Epoch: 214 [50112/225000 (22%)] Loss: 18651.896484\n",
      "Train Epoch: 214 [52608/225000 (23%)] Loss: 18544.406250\n",
      "Train Epoch: 214 [55104/225000 (24%)] Loss: 18620.923828\n",
      "Train Epoch: 214 [57600/225000 (26%)] Loss: 18308.898438\n",
      "Train Epoch: 214 [60096/225000 (27%)] Loss: 18037.964844\n",
      "Train Epoch: 214 [62592/225000 (28%)] Loss: 18107.951172\n",
      "Train Epoch: 214 [65088/225000 (29%)] Loss: 18592.984375\n",
      "Train Epoch: 214 [67584/225000 (30%)] Loss: 18266.242188\n",
      "Train Epoch: 214 [70080/225000 (31%)] Loss: 18641.910156\n",
      "Train Epoch: 214 [72576/225000 (32%)] Loss: 17962.468750\n",
      "Train Epoch: 214 [75072/225000 (33%)] Loss: 18185.150391\n",
      "Train Epoch: 214 [77568/225000 (34%)] Loss: 18191.802734\n",
      "Train Epoch: 214 [80064/225000 (36%)] Loss: 18503.585938\n",
      "Train Epoch: 214 [82560/225000 (37%)] Loss: 18051.027344\n",
      "Train Epoch: 214 [85056/225000 (38%)] Loss: 18215.033203\n",
      "Train Epoch: 214 [87552/225000 (39%)] Loss: 18134.597656\n",
      "Train Epoch: 214 [90048/225000 (40%)] Loss: 18063.404297\n",
      "Train Epoch: 214 [92544/225000 (41%)] Loss: 18681.369141\n",
      "Train Epoch: 214 [95040/225000 (42%)] Loss: 18073.011719\n",
      "Train Epoch: 214 [97536/225000 (43%)] Loss: 18568.050781\n",
      "Train Epoch: 214 [100032/225000 (44%)] Loss: 17617.613281\n",
      "Train Epoch: 214 [102528/225000 (46%)] Loss: 18314.308594\n",
      "Train Epoch: 214 [105024/225000 (47%)] Loss: 18479.496094\n",
      "Train Epoch: 214 [107520/225000 (48%)] Loss: 17958.558594\n",
      "Train Epoch: 214 [110016/225000 (49%)] Loss: 18005.835938\n",
      "Train Epoch: 214 [112512/225000 (50%)] Loss: 18321.507812\n",
      "Train Epoch: 214 [115008/225000 (51%)] Loss: 18616.449219\n",
      "Train Epoch: 214 [117504/225000 (52%)] Loss: 17975.621094\n",
      "Train Epoch: 214 [120000/225000 (53%)] Loss: 18100.037109\n",
      "Train Epoch: 214 [122496/225000 (54%)] Loss: 17801.738281\n",
      "Train Epoch: 214 [124992/225000 (56%)] Loss: 18392.201172\n",
      "Train Epoch: 214 [127488/225000 (57%)] Loss: 18003.062500\n",
      "Train Epoch: 214 [129984/225000 (58%)] Loss: 18149.484375\n",
      "Train Epoch: 214 [132480/225000 (59%)] Loss: 17965.550781\n",
      "Train Epoch: 214 [134976/225000 (60%)] Loss: 18409.148438\n",
      "Train Epoch: 214 [137472/225000 (61%)] Loss: 18219.886719\n",
      "Train Epoch: 214 [139968/225000 (62%)] Loss: 18491.105469\n",
      "Train Epoch: 214 [142464/225000 (63%)] Loss: 18560.425781\n",
      "Train Epoch: 214 [144960/225000 (64%)] Loss: 18676.105469\n",
      "Train Epoch: 214 [147456/225000 (66%)] Loss: 18162.136719\n",
      "Train Epoch: 214 [149952/225000 (67%)] Loss: 18682.488281\n",
      "Train Epoch: 214 [152448/225000 (68%)] Loss: 18339.312500\n",
      "Train Epoch: 214 [154944/225000 (69%)] Loss: 18530.421875\n",
      "Train Epoch: 214 [157440/225000 (70%)] Loss: 18074.128906\n",
      "Train Epoch: 214 [159936/225000 (71%)] Loss: 18431.593750\n",
      "Train Epoch: 214 [162432/225000 (72%)] Loss: 18197.554688\n",
      "Train Epoch: 214 [164928/225000 (73%)] Loss: 18096.332031\n",
      "Train Epoch: 214 [167424/225000 (74%)] Loss: 18155.304688\n",
      "Train Epoch: 214 [169920/225000 (76%)] Loss: 18302.947266\n",
      "Train Epoch: 214 [172416/225000 (77%)] Loss: 18138.605469\n",
      "Train Epoch: 214 [174912/225000 (78%)] Loss: 18419.011719\n",
      "Train Epoch: 214 [177408/225000 (79%)] Loss: 18132.925781\n",
      "Train Epoch: 214 [179904/225000 (80%)] Loss: 19173.917969\n",
      "Train Epoch: 214 [182400/225000 (81%)] Loss: 18183.167969\n",
      "Train Epoch: 214 [184896/225000 (82%)] Loss: 18168.070312\n",
      "Train Epoch: 214 [187392/225000 (83%)] Loss: 18169.746094\n",
      "Train Epoch: 214 [189888/225000 (84%)] Loss: 17969.578125\n",
      "Train Epoch: 214 [192384/225000 (86%)] Loss: 18287.207031\n",
      "Train Epoch: 214 [194880/225000 (87%)] Loss: 18597.820312\n",
      "Train Epoch: 214 [197376/225000 (88%)] Loss: 17830.296875\n",
      "Train Epoch: 214 [199872/225000 (89%)] Loss: 18407.064453\n",
      "Train Epoch: 214 [202368/225000 (90%)] Loss: 18253.386719\n",
      "Train Epoch: 214 [204864/225000 (91%)] Loss: 18227.037109\n",
      "Train Epoch: 214 [207360/225000 (92%)] Loss: 18240.613281\n",
      "Train Epoch: 214 [209856/225000 (93%)] Loss: 18404.734375\n",
      "Train Epoch: 214 [212352/225000 (94%)] Loss: 18483.986328\n",
      "Train Epoch: 214 [214848/225000 (95%)] Loss: 18019.296875\n",
      "Train Epoch: 214 [217344/225000 (97%)] Loss: 18855.597656\n",
      "Train Epoch: 214 [219840/225000 (98%)] Loss: 18301.992188\n",
      "Train Epoch: 214 [222336/225000 (99%)] Loss: 18651.294922\n",
      "Train Epoch: 214 [224832/225000 (100%)] Loss: 18461.042969\n",
      "    epoch          : 214\n",
      "    loss           : 18313.198293848654\n",
      "    val_loss       : 18242.63033117684\n",
      "Train Epoch: 215 [192/225000 (0%)] Loss: 18045.169922\n",
      "Train Epoch: 215 [2688/225000 (1%)] Loss: 18241.072266\n",
      "Train Epoch: 215 [5184/225000 (2%)] Loss: 18365.189453\n",
      "Train Epoch: 215 [7680/225000 (3%)] Loss: 18081.687500\n",
      "Train Epoch: 215 [10176/225000 (5%)] Loss: 18147.781250\n",
      "Train Epoch: 215 [12672/225000 (6%)] Loss: 18120.835938\n",
      "Train Epoch: 215 [15168/225000 (7%)] Loss: 18405.332031\n",
      "Train Epoch: 215 [17664/225000 (8%)] Loss: 18135.310547\n",
      "Train Epoch: 215 [20160/225000 (9%)] Loss: 18801.560547\n",
      "Train Epoch: 215 [22656/225000 (10%)] Loss: 18202.671875\n",
      "Train Epoch: 215 [25152/225000 (11%)] Loss: 18195.083984\n",
      "Train Epoch: 215 [27648/225000 (12%)] Loss: 18117.326172\n",
      "Train Epoch: 215 [30144/225000 (13%)] Loss: 18338.626953\n",
      "Train Epoch: 215 [32640/225000 (15%)] Loss: 18265.769531\n",
      "Train Epoch: 215 [35136/225000 (16%)] Loss: 18480.175781\n",
      "Train Epoch: 215 [37632/225000 (17%)] Loss: 18115.572266\n",
      "Train Epoch: 215 [40128/225000 (18%)] Loss: 18426.076172\n",
      "Train Epoch: 215 [42624/225000 (19%)] Loss: 18763.322266\n",
      "Train Epoch: 215 [45120/225000 (20%)] Loss: 18217.480469\n",
      "Train Epoch: 215 [47616/225000 (21%)] Loss: 18149.382812\n",
      "Train Epoch: 215 [50112/225000 (22%)] Loss: 17818.289062\n",
      "Train Epoch: 215 [52608/225000 (23%)] Loss: 18404.632812\n",
      "Train Epoch: 215 [55104/225000 (24%)] Loss: 18083.082031\n",
      "Train Epoch: 215 [57600/225000 (26%)] Loss: 18542.882812\n",
      "Train Epoch: 215 [60096/225000 (27%)] Loss: 17874.175781\n",
      "Train Epoch: 215 [62592/225000 (28%)] Loss: 18554.771484\n",
      "Train Epoch: 215 [65088/225000 (29%)] Loss: 18546.355469\n",
      "Train Epoch: 215 [67584/225000 (30%)] Loss: 18485.529297\n",
      "Train Epoch: 215 [70080/225000 (31%)] Loss: 17734.417969\n",
      "Train Epoch: 215 [72576/225000 (32%)] Loss: 18886.595703\n",
      "Train Epoch: 215 [75072/225000 (33%)] Loss: 17858.326172\n",
      "Train Epoch: 215 [77568/225000 (34%)] Loss: 17855.925781\n",
      "Train Epoch: 215 [80064/225000 (36%)] Loss: 18307.023438\n",
      "Train Epoch: 215 [82560/225000 (37%)] Loss: 18564.566406\n",
      "Train Epoch: 215 [85056/225000 (38%)] Loss: 18047.689453\n",
      "Train Epoch: 215 [87552/225000 (39%)] Loss: 18529.375000\n",
      "Train Epoch: 215 [90048/225000 (40%)] Loss: 18390.914062\n",
      "Train Epoch: 215 [92544/225000 (41%)] Loss: 17913.271484\n",
      "Train Epoch: 215 [95040/225000 (42%)] Loss: 18093.644531\n",
      "Train Epoch: 215 [97536/225000 (43%)] Loss: 18305.593750\n",
      "Train Epoch: 215 [100032/225000 (44%)] Loss: 18435.824219\n",
      "Train Epoch: 215 [102528/225000 (46%)] Loss: 17957.505859\n",
      "Train Epoch: 215 [105024/225000 (47%)] Loss: 17961.267578\n",
      "Train Epoch: 215 [107520/225000 (48%)] Loss: 18555.476562\n",
      "Train Epoch: 215 [110016/225000 (49%)] Loss: 18209.812500\n",
      "Train Epoch: 215 [112512/225000 (50%)] Loss: 17728.865234\n",
      "Train Epoch: 215 [115008/225000 (51%)] Loss: 18357.082031\n",
      "Train Epoch: 215 [117504/225000 (52%)] Loss: 18617.005859\n",
      "Train Epoch: 215 [120000/225000 (53%)] Loss: 17997.019531\n",
      "Train Epoch: 215 [122496/225000 (54%)] Loss: 18104.816406\n",
      "Train Epoch: 215 [124992/225000 (56%)] Loss: 18010.078125\n",
      "Train Epoch: 215 [127488/225000 (57%)] Loss: 18122.498047\n",
      "Train Epoch: 215 [129984/225000 (58%)] Loss: 17495.605469\n",
      "Train Epoch: 215 [132480/225000 (59%)] Loss: 18191.353516\n",
      "Train Epoch: 215 [134976/225000 (60%)] Loss: 18045.105469\n",
      "Train Epoch: 215 [137472/225000 (61%)] Loss: 18355.535156\n",
      "Train Epoch: 215 [139968/225000 (62%)] Loss: 18416.357422\n",
      "Train Epoch: 215 [142464/225000 (63%)] Loss: 18418.236328\n",
      "Train Epoch: 215 [144960/225000 (64%)] Loss: 18021.542969\n",
      "Train Epoch: 215 [147456/225000 (66%)] Loss: 18213.703125\n",
      "Train Epoch: 215 [149952/225000 (67%)] Loss: 18677.468750\n",
      "Train Epoch: 215 [152448/225000 (68%)] Loss: 18228.748047\n",
      "Train Epoch: 215 [154944/225000 (69%)] Loss: 18296.273438\n",
      "Train Epoch: 215 [157440/225000 (70%)] Loss: 17899.726562\n",
      "Train Epoch: 215 [159936/225000 (71%)] Loss: 18327.351562\n",
      "Train Epoch: 215 [162432/225000 (72%)] Loss: 18246.242188\n",
      "Train Epoch: 215 [164928/225000 (73%)] Loss: 18510.253906\n",
      "Train Epoch: 215 [167424/225000 (74%)] Loss: 17922.205078\n",
      "Train Epoch: 215 [169920/225000 (76%)] Loss: 18490.457031\n",
      "Train Epoch: 215 [172416/225000 (77%)] Loss: 18417.515625\n",
      "Train Epoch: 215 [174912/225000 (78%)] Loss: 18249.515625\n",
      "Train Epoch: 215 [177408/225000 (79%)] Loss: 18323.542969\n",
      "Train Epoch: 215 [179904/225000 (80%)] Loss: 18313.523438\n",
      "Train Epoch: 215 [182400/225000 (81%)] Loss: 18665.761719\n",
      "Train Epoch: 215 [184896/225000 (82%)] Loss: 18116.039062\n",
      "Train Epoch: 215 [187392/225000 (83%)] Loss: 18226.074219\n",
      "Train Epoch: 215 [189888/225000 (84%)] Loss: 17882.835938\n",
      "Train Epoch: 215 [192384/225000 (86%)] Loss: 18544.023438\n",
      "Train Epoch: 215 [194880/225000 (87%)] Loss: 17904.113281\n",
      "Train Epoch: 215 [197376/225000 (88%)] Loss: 18297.816406\n",
      "Train Epoch: 215 [199872/225000 (89%)] Loss: 18350.699219\n",
      "Train Epoch: 215 [202368/225000 (90%)] Loss: 18371.310547\n",
      "Train Epoch: 215 [204864/225000 (91%)] Loss: 18102.316406\n",
      "Train Epoch: 215 [207360/225000 (92%)] Loss: 17747.187500\n",
      "Train Epoch: 215 [209856/225000 (93%)] Loss: 17825.859375\n",
      "Train Epoch: 215 [212352/225000 (94%)] Loss: 18325.417969\n",
      "Train Epoch: 215 [214848/225000 (95%)] Loss: 17967.939453\n",
      "Train Epoch: 215 [217344/225000 (97%)] Loss: 18146.820312\n",
      "Train Epoch: 215 [219840/225000 (98%)] Loss: 17960.789062\n",
      "Train Epoch: 215 [222336/225000 (99%)] Loss: 18034.804688\n",
      "Train Epoch: 215 [224832/225000 (100%)] Loss: 17789.328125\n",
      "    epoch          : 215\n",
      "    loss           : 18282.474804354202\n",
      "    val_loss       : 18234.639663249025\n",
      "Train Epoch: 216 [192/225000 (0%)] Loss: 18400.750000\n",
      "Train Epoch: 216 [2688/225000 (1%)] Loss: 18381.330078\n",
      "Train Epoch: 216 [5184/225000 (2%)] Loss: 18399.380859\n",
      "Train Epoch: 216 [7680/225000 (3%)] Loss: 18044.421875\n",
      "Train Epoch: 216 [10176/225000 (5%)] Loss: 18337.574219\n",
      "Train Epoch: 216 [12672/225000 (6%)] Loss: 18247.603516\n",
      "Train Epoch: 216 [15168/225000 (7%)] Loss: 18232.171875\n",
      "Train Epoch: 216 [17664/225000 (8%)] Loss: 18395.492188\n",
      "Train Epoch: 216 [20160/225000 (9%)] Loss: 18342.148438\n",
      "Train Epoch: 216 [22656/225000 (10%)] Loss: 18134.750000\n",
      "Train Epoch: 216 [25152/225000 (11%)] Loss: 18167.099609\n",
      "Train Epoch: 216 [27648/225000 (12%)] Loss: 18186.988281\n",
      "Train Epoch: 216 [30144/225000 (13%)] Loss: 18311.621094\n",
      "Train Epoch: 216 [32640/225000 (15%)] Loss: 18524.187500\n",
      "Train Epoch: 216 [35136/225000 (16%)] Loss: 17965.968750\n",
      "Train Epoch: 216 [37632/225000 (17%)] Loss: 18285.089844\n",
      "Train Epoch: 216 [40128/225000 (18%)] Loss: 18649.398438\n",
      "Train Epoch: 216 [42624/225000 (19%)] Loss: 18019.373047\n",
      "Train Epoch: 216 [45120/225000 (20%)] Loss: 18752.578125\n",
      "Train Epoch: 216 [47616/225000 (21%)] Loss: 18376.542969\n",
      "Train Epoch: 216 [50112/225000 (22%)] Loss: 18139.785156\n",
      "Train Epoch: 216 [52608/225000 (23%)] Loss: 18009.931641\n",
      "Train Epoch: 216 [55104/225000 (24%)] Loss: 18518.205078\n",
      "Train Epoch: 216 [57600/225000 (26%)] Loss: 18213.550781\n",
      "Train Epoch: 216 [60096/225000 (27%)] Loss: 18063.460938\n",
      "Train Epoch: 216 [62592/225000 (28%)] Loss: 17818.748047\n",
      "Train Epoch: 216 [65088/225000 (29%)] Loss: 18240.562500\n",
      "Train Epoch: 216 [67584/225000 (30%)] Loss: 18465.177734\n",
      "Train Epoch: 216 [70080/225000 (31%)] Loss: 18260.664062\n",
      "Train Epoch: 216 [72576/225000 (32%)] Loss: 18547.421875\n",
      "Train Epoch: 216 [75072/225000 (33%)] Loss: 18410.292969\n",
      "Train Epoch: 216 [77568/225000 (34%)] Loss: 17948.015625\n",
      "Train Epoch: 216 [80064/225000 (36%)] Loss: 18594.855469\n",
      "Train Epoch: 216 [82560/225000 (37%)] Loss: 18066.574219\n",
      "Train Epoch: 216 [85056/225000 (38%)] Loss: 18046.224609\n",
      "Train Epoch: 216 [87552/225000 (39%)] Loss: 18407.496094\n",
      "Train Epoch: 216 [90048/225000 (40%)] Loss: 17849.031250\n",
      "Train Epoch: 216 [92544/225000 (41%)] Loss: 18833.925781\n",
      "Train Epoch: 216 [95040/225000 (42%)] Loss: 17590.917969\n",
      "Train Epoch: 216 [97536/225000 (43%)] Loss: 18346.361328\n",
      "Train Epoch: 216 [100032/225000 (44%)] Loss: 18359.261719\n",
      "Train Epoch: 216 [102528/225000 (46%)] Loss: 18124.816406\n",
      "Train Epoch: 216 [105024/225000 (47%)] Loss: 17778.246094\n",
      "Train Epoch: 216 [107520/225000 (48%)] Loss: 18238.902344\n",
      "Train Epoch: 216 [110016/225000 (49%)] Loss: 18361.832031\n",
      "Train Epoch: 216 [112512/225000 (50%)] Loss: 18523.402344\n",
      "Train Epoch: 216 [115008/225000 (51%)] Loss: 18458.945312\n",
      "Train Epoch: 216 [117504/225000 (52%)] Loss: 18369.085938\n",
      "Train Epoch: 216 [120000/225000 (53%)] Loss: 18086.578125\n",
      "Train Epoch: 216 [122496/225000 (54%)] Loss: 17793.292969\n",
      "Train Epoch: 216 [124992/225000 (56%)] Loss: 18570.738281\n",
      "Train Epoch: 216 [127488/225000 (57%)] Loss: 18196.091797\n",
      "Train Epoch: 216 [129984/225000 (58%)] Loss: 18174.380859\n",
      "Train Epoch: 216 [132480/225000 (59%)] Loss: 18300.816406\n",
      "Train Epoch: 216 [134976/225000 (60%)] Loss: 17677.730469\n",
      "Train Epoch: 216 [137472/225000 (61%)] Loss: 17809.828125\n",
      "Train Epoch: 216 [139968/225000 (62%)] Loss: 18603.605469\n",
      "Train Epoch: 216 [142464/225000 (63%)] Loss: 18256.484375\n",
      "Train Epoch: 216 [144960/225000 (64%)] Loss: 17867.646484\n",
      "Train Epoch: 216 [147456/225000 (66%)] Loss: 18311.697266\n",
      "Train Epoch: 216 [149952/225000 (67%)] Loss: 18174.800781\n",
      "Train Epoch: 216 [152448/225000 (68%)] Loss: 18128.970703\n",
      "Train Epoch: 216 [154944/225000 (69%)] Loss: 18333.574219\n",
      "Train Epoch: 216 [157440/225000 (70%)] Loss: 17992.238281\n",
      "Train Epoch: 216 [159936/225000 (71%)] Loss: 18299.398438\n",
      "Train Epoch: 216 [162432/225000 (72%)] Loss: 18010.273438\n",
      "Train Epoch: 216 [164928/225000 (73%)] Loss: 18332.140625\n",
      "Train Epoch: 216 [167424/225000 (74%)] Loss: 18572.412109\n",
      "Train Epoch: 216 [169920/225000 (76%)] Loss: 18277.332031\n",
      "Train Epoch: 216 [172416/225000 (77%)] Loss: 18107.921875\n",
      "Train Epoch: 216 [174912/225000 (78%)] Loss: 18251.058594\n",
      "Train Epoch: 216 [177408/225000 (79%)] Loss: 18252.828125\n",
      "Train Epoch: 216 [179904/225000 (80%)] Loss: 18567.023438\n",
      "Train Epoch: 216 [182400/225000 (81%)] Loss: 18410.308594\n",
      "Train Epoch: 216 [184896/225000 (82%)] Loss: 18878.558594\n",
      "Train Epoch: 216 [187392/225000 (83%)] Loss: 18528.011719\n",
      "Train Epoch: 216 [189888/225000 (84%)] Loss: 17759.718750\n",
      "Train Epoch: 216 [192384/225000 (86%)] Loss: 18430.806641\n",
      "Train Epoch: 216 [194880/225000 (87%)] Loss: 18532.462891\n",
      "Train Epoch: 216 [197376/225000 (88%)] Loss: 18696.429688\n",
      "Train Epoch: 216 [199872/225000 (89%)] Loss: 18458.578125\n",
      "Train Epoch: 216 [202368/225000 (90%)] Loss: 18590.279297\n",
      "Train Epoch: 216 [204864/225000 (91%)] Loss: 17726.720703\n",
      "Train Epoch: 216 [207360/225000 (92%)] Loss: 18014.605469\n",
      "Train Epoch: 216 [209856/225000 (93%)] Loss: 17657.906250\n",
      "Train Epoch: 216 [212352/225000 (94%)] Loss: 18161.898438\n",
      "Train Epoch: 216 [214848/225000 (95%)] Loss: 18025.312500\n",
      "Train Epoch: 216 [217344/225000 (97%)] Loss: 18375.798828\n",
      "Train Epoch: 216 [219840/225000 (98%)] Loss: 18697.505859\n",
      "Train Epoch: 216 [222336/225000 (99%)] Loss: 18187.097656\n",
      "Train Epoch: 216 [224832/225000 (100%)] Loss: 18143.003906\n",
      "    epoch          : 216\n",
      "    loss           : 18248.45343163396\n",
      "    val_loss       : 18138.96941773309\n",
      "Train Epoch: 217 [192/225000 (0%)] Loss: 18083.367188\n",
      "Train Epoch: 217 [2688/225000 (1%)] Loss: 18421.414062\n",
      "Train Epoch: 217 [5184/225000 (2%)] Loss: 18030.884766\n",
      "Train Epoch: 217 [7680/225000 (3%)] Loss: 18692.597656\n",
      "Train Epoch: 217 [10176/225000 (5%)] Loss: 18206.445312\n",
      "Train Epoch: 217 [12672/225000 (6%)] Loss: 17832.984375\n",
      "Train Epoch: 217 [15168/225000 (7%)] Loss: 18945.923828\n",
      "Train Epoch: 217 [17664/225000 (8%)] Loss: 18259.667969\n",
      "Train Epoch: 217 [20160/225000 (9%)] Loss: 18454.769531\n",
      "Train Epoch: 217 [22656/225000 (10%)] Loss: 17885.312500\n",
      "Train Epoch: 217 [25152/225000 (11%)] Loss: 18334.101562\n",
      "Train Epoch: 217 [27648/225000 (12%)] Loss: 18349.531250\n",
      "Train Epoch: 217 [30144/225000 (13%)] Loss: 17585.296875\n",
      "Train Epoch: 217 [32640/225000 (15%)] Loss: 17905.882812\n",
      "Train Epoch: 217 [35136/225000 (16%)] Loss: 18434.785156\n",
      "Train Epoch: 217 [37632/225000 (17%)] Loss: 18031.970703\n",
      "Train Epoch: 217 [40128/225000 (18%)] Loss: 18430.843750\n",
      "Train Epoch: 217 [42624/225000 (19%)] Loss: 18130.675781\n",
      "Train Epoch: 217 [45120/225000 (20%)] Loss: 18152.277344\n",
      "Train Epoch: 217 [47616/225000 (21%)] Loss: 18655.054688\n",
      "Train Epoch: 217 [50112/225000 (22%)] Loss: 18156.388672\n",
      "Train Epoch: 217 [52608/225000 (23%)] Loss: 18569.673828\n",
      "Train Epoch: 217 [55104/225000 (24%)] Loss: 18207.988281\n",
      "Train Epoch: 217 [57600/225000 (26%)] Loss: 18606.054688\n",
      "Train Epoch: 217 [60096/225000 (27%)] Loss: 17756.826172\n",
      "Train Epoch: 217 [62592/225000 (28%)] Loss: 18268.164062\n",
      "Train Epoch: 217 [65088/225000 (29%)] Loss: 17986.214844\n",
      "Train Epoch: 217 [67584/225000 (30%)] Loss: 18321.789062\n",
      "Train Epoch: 217 [70080/225000 (31%)] Loss: 18067.914062\n",
      "Train Epoch: 217 [72576/225000 (32%)] Loss: 18215.199219\n",
      "Train Epoch: 217 [75072/225000 (33%)] Loss: 18026.687500\n",
      "Train Epoch: 217 [77568/225000 (34%)] Loss: 18256.714844\n",
      "Train Epoch: 217 [80064/225000 (36%)] Loss: 17920.792969\n",
      "Train Epoch: 217 [82560/225000 (37%)] Loss: 18004.562500\n",
      "Train Epoch: 217 [85056/225000 (38%)] Loss: 18022.074219\n",
      "Train Epoch: 217 [87552/225000 (39%)] Loss: 18388.865234\n",
      "Train Epoch: 217 [90048/225000 (40%)] Loss: 17984.511719\n",
      "Train Epoch: 217 [92544/225000 (41%)] Loss: 18249.628906\n",
      "Train Epoch: 217 [95040/225000 (42%)] Loss: 18154.656250\n",
      "Train Epoch: 217 [97536/225000 (43%)] Loss: 17903.968750\n",
      "Train Epoch: 217 [100032/225000 (44%)] Loss: 18102.144531\n",
      "Train Epoch: 217 [102528/225000 (46%)] Loss: 18383.937500\n",
      "Train Epoch: 217 [105024/225000 (47%)] Loss: 18321.851562\n",
      "Train Epoch: 217 [107520/225000 (48%)] Loss: 18252.734375\n",
      "Train Epoch: 217 [110016/225000 (49%)] Loss: 18491.654297\n",
      "Train Epoch: 217 [112512/225000 (50%)] Loss: 18056.228516\n",
      "Train Epoch: 217 [115008/225000 (51%)] Loss: 18020.738281\n",
      "Train Epoch: 217 [117504/225000 (52%)] Loss: 18105.156250\n",
      "Train Epoch: 217 [120000/225000 (53%)] Loss: 18073.470703\n",
      "Train Epoch: 217 [122496/225000 (54%)] Loss: 18015.347656\n",
      "Train Epoch: 217 [124992/225000 (56%)] Loss: 17782.890625\n",
      "Train Epoch: 217 [127488/225000 (57%)] Loss: 18035.457031\n",
      "Train Epoch: 217 [129984/225000 (58%)] Loss: 18544.722656\n",
      "Train Epoch: 217 [132480/225000 (59%)] Loss: 18230.679688\n",
      "Train Epoch: 217 [134976/225000 (60%)] Loss: 18310.843750\n",
      "Train Epoch: 217 [137472/225000 (61%)] Loss: 18678.277344\n",
      "Train Epoch: 217 [139968/225000 (62%)] Loss: 18518.556641\n",
      "Train Epoch: 217 [142464/225000 (63%)] Loss: 18719.296875\n",
      "Train Epoch: 217 [144960/225000 (64%)] Loss: 18233.484375\n",
      "Train Epoch: 217 [147456/225000 (66%)] Loss: 18297.851562\n",
      "Train Epoch: 217 [149952/225000 (67%)] Loss: 18272.542969\n",
      "Train Epoch: 217 [152448/225000 (68%)] Loss: 18164.408203\n",
      "Train Epoch: 217 [154944/225000 (69%)] Loss: 17934.871094\n",
      "Train Epoch: 217 [157440/225000 (70%)] Loss: 18663.335938\n",
      "Train Epoch: 217 [159936/225000 (71%)] Loss: 17939.640625\n",
      "Train Epoch: 217 [162432/225000 (72%)] Loss: 18471.429688\n",
      "Train Epoch: 217 [164928/225000 (73%)] Loss: 18136.505859\n",
      "Train Epoch: 217 [167424/225000 (74%)] Loss: 18414.210938\n",
      "Train Epoch: 217 [169920/225000 (76%)] Loss: 18125.291016\n",
      "Train Epoch: 217 [172416/225000 (77%)] Loss: 18655.300781\n",
      "Train Epoch: 217 [174912/225000 (78%)] Loss: 18291.945312\n",
      "Train Epoch: 217 [177408/225000 (79%)] Loss: 18465.390625\n",
      "Train Epoch: 217 [179904/225000 (80%)] Loss: 18287.724609\n",
      "Train Epoch: 217 [182400/225000 (81%)] Loss: 18297.890625\n",
      "Train Epoch: 217 [184896/225000 (82%)] Loss: 18154.091797\n",
      "Train Epoch: 217 [187392/225000 (83%)] Loss: 17949.156250\n",
      "Train Epoch: 217 [189888/225000 (84%)] Loss: 18636.730469\n",
      "Train Epoch: 217 [192384/225000 (86%)] Loss: 18332.773438\n",
      "Train Epoch: 217 [194880/225000 (87%)] Loss: 17709.093750\n",
      "Train Epoch: 217 [197376/225000 (88%)] Loss: 18181.144531\n",
      "Train Epoch: 217 [199872/225000 (89%)] Loss: 18864.785156\n",
      "Train Epoch: 217 [202368/225000 (90%)] Loss: 17911.777344\n",
      "Train Epoch: 217 [204864/225000 (91%)] Loss: 18145.253906\n",
      "Train Epoch: 217 [207360/225000 (92%)] Loss: 18443.134766\n",
      "Train Epoch: 217 [209856/225000 (93%)] Loss: 17958.648438\n",
      "Train Epoch: 217 [212352/225000 (94%)] Loss: 18062.097656\n",
      "Train Epoch: 217 [214848/225000 (95%)] Loss: 17929.312500\n",
      "Train Epoch: 217 [217344/225000 (97%)] Loss: 18367.445312\n",
      "Train Epoch: 217 [219840/225000 (98%)] Loss: 18084.794922\n",
      "Train Epoch: 217 [222336/225000 (99%)] Loss: 18658.498047\n",
      "Train Epoch: 217 [224832/225000 (100%)] Loss: 18113.253906\n",
      "    epoch          : 217\n",
      "    loss           : 18221.505386925393\n",
      "    val_loss       : 18121.759405984223\n",
      "Train Epoch: 218 [192/225000 (0%)] Loss: 17976.156250\n",
      "Train Epoch: 218 [2688/225000 (1%)] Loss: 17758.453125\n",
      "Train Epoch: 218 [5184/225000 (2%)] Loss: 18469.134766\n",
      "Train Epoch: 218 [7680/225000 (3%)] Loss: 17833.406250\n",
      "Train Epoch: 218 [10176/225000 (5%)] Loss: 17872.339844\n",
      "Train Epoch: 218 [12672/225000 (6%)] Loss: 18055.890625\n",
      "Train Epoch: 218 [15168/225000 (7%)] Loss: 18108.308594\n",
      "Train Epoch: 218 [17664/225000 (8%)] Loss: 18129.712891\n",
      "Train Epoch: 218 [20160/225000 (9%)] Loss: 18453.480469\n",
      "Train Epoch: 218 [22656/225000 (10%)] Loss: 17959.990234\n",
      "Train Epoch: 218 [25152/225000 (11%)] Loss: 17956.636719\n",
      "Train Epoch: 218 [27648/225000 (12%)] Loss: 18459.515625\n",
      "Train Epoch: 218 [30144/225000 (13%)] Loss: 18360.718750\n",
      "Train Epoch: 218 [32640/225000 (15%)] Loss: 18117.142578\n",
      "Train Epoch: 218 [35136/225000 (16%)] Loss: 18115.269531\n",
      "Train Epoch: 218 [37632/225000 (17%)] Loss: 18425.761719\n",
      "Train Epoch: 218 [40128/225000 (18%)] Loss: 18250.343750\n",
      "Train Epoch: 218 [42624/225000 (19%)] Loss: 18124.027344\n",
      "Train Epoch: 218 [45120/225000 (20%)] Loss: 18212.644531\n",
      "Train Epoch: 218 [47616/225000 (21%)] Loss: 17916.392578\n",
      "Train Epoch: 218 [50112/225000 (22%)] Loss: 17865.130859\n",
      "Train Epoch: 218 [52608/225000 (23%)] Loss: 18493.535156\n",
      "Train Epoch: 218 [55104/225000 (24%)] Loss: 18248.080078\n",
      "Train Epoch: 218 [57600/225000 (26%)] Loss: 18300.958984\n",
      "Train Epoch: 218 [60096/225000 (27%)] Loss: 18446.242188\n",
      "Train Epoch: 218 [62592/225000 (28%)] Loss: 18275.263672\n",
      "Train Epoch: 218 [65088/225000 (29%)] Loss: 18478.558594\n",
      "Train Epoch: 218 [67584/225000 (30%)] Loss: 18316.156250\n",
      "Train Epoch: 218 [70080/225000 (31%)] Loss: 18175.751953\n",
      "Train Epoch: 218 [72576/225000 (32%)] Loss: 17717.785156\n",
      "Train Epoch: 218 [75072/225000 (33%)] Loss: 17950.556641\n",
      "Train Epoch: 218 [77568/225000 (34%)] Loss: 18023.765625\n",
      "Train Epoch: 218 [80064/225000 (36%)] Loss: 18679.617188\n",
      "Train Epoch: 218 [82560/225000 (37%)] Loss: 17993.882812\n",
      "Train Epoch: 218 [85056/225000 (38%)] Loss: 18188.335938\n",
      "Train Epoch: 218 [87552/225000 (39%)] Loss: 18496.224609\n",
      "Train Epoch: 218 [90048/225000 (40%)] Loss: 18418.726562\n",
      "Train Epoch: 218 [92544/225000 (41%)] Loss: 18068.703125\n",
      "Train Epoch: 218 [95040/225000 (42%)] Loss: 18231.707031\n",
      "Train Epoch: 218 [97536/225000 (43%)] Loss: 18314.457031\n",
      "Train Epoch: 218 [100032/225000 (44%)] Loss: 18399.851562\n",
      "Train Epoch: 218 [102528/225000 (46%)] Loss: 18213.726562\n",
      "Train Epoch: 218 [105024/225000 (47%)] Loss: 18026.386719\n",
      "Train Epoch: 218 [107520/225000 (48%)] Loss: 18134.023438\n",
      "Train Epoch: 218 [110016/225000 (49%)] Loss: 18230.140625\n",
      "Train Epoch: 218 [112512/225000 (50%)] Loss: 18120.798828\n",
      "Train Epoch: 218 [115008/225000 (51%)] Loss: 18335.105469\n",
      "Train Epoch: 218 [117504/225000 (52%)] Loss: 17721.117188\n",
      "Train Epoch: 218 [120000/225000 (53%)] Loss: 17895.318359\n",
      "Train Epoch: 218 [122496/225000 (54%)] Loss: 18436.050781\n",
      "Train Epoch: 218 [124992/225000 (56%)] Loss: 17994.980469\n",
      "Train Epoch: 218 [127488/225000 (57%)] Loss: 18273.855469\n",
      "Train Epoch: 218 [129984/225000 (58%)] Loss: 17851.250000\n",
      "Train Epoch: 218 [132480/225000 (59%)] Loss: 18171.417969\n",
      "Train Epoch: 218 [134976/225000 (60%)] Loss: 18304.457031\n",
      "Train Epoch: 218 [137472/225000 (61%)] Loss: 17710.863281\n",
      "Train Epoch: 218 [139968/225000 (62%)] Loss: 18223.109375\n",
      "Train Epoch: 218 [142464/225000 (63%)] Loss: 17952.941406\n",
      "Train Epoch: 218 [144960/225000 (64%)] Loss: 17921.546875\n",
      "Train Epoch: 218 [147456/225000 (66%)] Loss: 18220.468750\n",
      "Train Epoch: 218 [149952/225000 (67%)] Loss: 18081.718750\n",
      "Train Epoch: 218 [152448/225000 (68%)] Loss: 18556.621094\n",
      "Train Epoch: 218 [154944/225000 (69%)] Loss: 18229.406250\n",
      "Train Epoch: 218 [157440/225000 (70%)] Loss: 17765.546875\n",
      "Train Epoch: 218 [159936/225000 (71%)] Loss: 18093.140625\n",
      "Train Epoch: 218 [162432/225000 (72%)] Loss: 18165.789062\n",
      "Train Epoch: 218 [164928/225000 (73%)] Loss: 17926.855469\n",
      "Train Epoch: 218 [167424/225000 (74%)] Loss: 18308.281250\n",
      "Train Epoch: 218 [169920/225000 (76%)] Loss: 17922.503906\n",
      "Train Epoch: 218 [172416/225000 (77%)] Loss: 18092.378906\n",
      "Train Epoch: 218 [174912/225000 (78%)] Loss: 18778.375000\n",
      "Train Epoch: 218 [177408/225000 (79%)] Loss: 18347.406250\n",
      "Train Epoch: 218 [179904/225000 (80%)] Loss: 18216.697266\n",
      "Train Epoch: 218 [182400/225000 (81%)] Loss: 18210.789062\n",
      "Train Epoch: 218 [184896/225000 (82%)] Loss: 18472.859375\n",
      "Train Epoch: 218 [187392/225000 (83%)] Loss: 17590.160156\n",
      "Train Epoch: 218 [189888/225000 (84%)] Loss: 18440.683594\n",
      "Train Epoch: 218 [192384/225000 (86%)] Loss: 18610.820312\n",
      "Train Epoch: 218 [194880/225000 (87%)] Loss: 18378.863281\n",
      "Train Epoch: 218 [197376/225000 (88%)] Loss: 18554.708984\n",
      "Train Epoch: 218 [199872/225000 (89%)] Loss: 18401.566406\n",
      "Train Epoch: 218 [202368/225000 (90%)] Loss: 18218.974609\n",
      "Train Epoch: 218 [204864/225000 (91%)] Loss: 18327.164062\n",
      "Train Epoch: 218 [207360/225000 (92%)] Loss: 18365.425781\n",
      "Train Epoch: 218 [209856/225000 (93%)] Loss: 18172.613281\n",
      "Train Epoch: 218 [212352/225000 (94%)] Loss: 18353.162109\n",
      "Train Epoch: 218 [214848/225000 (95%)] Loss: 18206.316406\n",
      "Train Epoch: 218 [217344/225000 (97%)] Loss: 18454.296875\n",
      "Train Epoch: 218 [219840/225000 (98%)] Loss: 17930.259766\n",
      "Train Epoch: 218 [222336/225000 (99%)] Loss: 18046.962891\n",
      "Train Epoch: 218 [224832/225000 (100%)] Loss: 18247.453125\n",
      "    epoch          : 218\n",
      "    loss           : 18205.816307927154\n",
      "    val_loss       : 18122.94150229811\n",
      "Train Epoch: 219 [192/225000 (0%)] Loss: 17641.812500\n",
      "Train Epoch: 219 [2688/225000 (1%)] Loss: 18082.429688\n",
      "Train Epoch: 219 [5184/225000 (2%)] Loss: 18321.222656\n",
      "Train Epoch: 219 [7680/225000 (3%)] Loss: 18039.480469\n",
      "Train Epoch: 219 [10176/225000 (5%)] Loss: 18094.503906\n",
      "Train Epoch: 219 [12672/225000 (6%)] Loss: 18612.089844\n",
      "Train Epoch: 219 [15168/225000 (7%)] Loss: 18556.218750\n",
      "Train Epoch: 219 [17664/225000 (8%)] Loss: 18137.732422\n",
      "Train Epoch: 219 [20160/225000 (9%)] Loss: 18213.320312\n",
      "Train Epoch: 219 [22656/225000 (10%)] Loss: 18515.003906\n",
      "Train Epoch: 219 [25152/225000 (11%)] Loss: 17996.613281\n",
      "Train Epoch: 219 [27648/225000 (12%)] Loss: 18240.578125\n",
      "Train Epoch: 219 [30144/225000 (13%)] Loss: 18228.191406\n",
      "Train Epoch: 219 [32640/225000 (15%)] Loss: 18198.062500\n",
      "Train Epoch: 219 [35136/225000 (16%)] Loss: 17748.921875\n",
      "Train Epoch: 219 [37632/225000 (17%)] Loss: 18689.000000\n",
      "Train Epoch: 219 [40128/225000 (18%)] Loss: 18274.484375\n",
      "Train Epoch: 219 [42624/225000 (19%)] Loss: 18216.316406\n",
      "Train Epoch: 219 [45120/225000 (20%)] Loss: 17931.195312\n",
      "Train Epoch: 219 [47616/225000 (21%)] Loss: 17823.228516\n",
      "Train Epoch: 219 [50112/225000 (22%)] Loss: 18615.369141\n",
      "Train Epoch: 219 [52608/225000 (23%)] Loss: 18211.277344\n",
      "Train Epoch: 219 [55104/225000 (24%)] Loss: 18190.033203\n",
      "Train Epoch: 219 [57600/225000 (26%)] Loss: 18388.177734\n",
      "Train Epoch: 219 [60096/225000 (27%)] Loss: 17836.091797\n",
      "Train Epoch: 219 [62592/225000 (28%)] Loss: 18171.046875\n",
      "Train Epoch: 219 [65088/225000 (29%)] Loss: 18244.273438\n",
      "Train Epoch: 219 [67584/225000 (30%)] Loss: 17775.199219\n",
      "Train Epoch: 219 [70080/225000 (31%)] Loss: 17900.152344\n",
      "Train Epoch: 219 [72576/225000 (32%)] Loss: 18338.306641\n",
      "Train Epoch: 219 [75072/225000 (33%)] Loss: 18851.501953\n",
      "Train Epoch: 219 [77568/225000 (34%)] Loss: 17888.732422\n",
      "Train Epoch: 219 [80064/225000 (36%)] Loss: 17958.466797\n",
      "Train Epoch: 219 [82560/225000 (37%)] Loss: 18288.347656\n",
      "Train Epoch: 219 [85056/225000 (38%)] Loss: 18178.517578\n",
      "Train Epoch: 219 [87552/225000 (39%)] Loss: 18764.138672\n",
      "Train Epoch: 219 [90048/225000 (40%)] Loss: 18062.089844\n",
      "Train Epoch: 219 [92544/225000 (41%)] Loss: 18123.347656\n",
      "Train Epoch: 219 [95040/225000 (42%)] Loss: 17905.337891\n",
      "Train Epoch: 219 [97536/225000 (43%)] Loss: 18577.863281\n",
      "Train Epoch: 219 [100032/225000 (44%)] Loss: 17946.222656\n",
      "Train Epoch: 219 [102528/225000 (46%)] Loss: 17634.210938\n",
      "Train Epoch: 219 [105024/225000 (47%)] Loss: 18106.119141\n",
      "Train Epoch: 219 [107520/225000 (48%)] Loss: 18667.695312\n",
      "Train Epoch: 219 [110016/225000 (49%)] Loss: 18157.041016\n",
      "Train Epoch: 219 [112512/225000 (50%)] Loss: 18040.843750\n",
      "Train Epoch: 219 [115008/225000 (51%)] Loss: 17607.603516\n",
      "Train Epoch: 219 [117504/225000 (52%)] Loss: 18485.919922\n",
      "Train Epoch: 219 [120000/225000 (53%)] Loss: 18091.031250\n",
      "Train Epoch: 219 [122496/225000 (54%)] Loss: 17941.939453\n",
      "Train Epoch: 219 [124992/225000 (56%)] Loss: 18301.738281\n",
      "Train Epoch: 219 [127488/225000 (57%)] Loss: 18390.816406\n",
      "Train Epoch: 219 [129984/225000 (58%)] Loss: 18482.050781\n",
      "Train Epoch: 219 [132480/225000 (59%)] Loss: 18412.210938\n",
      "Train Epoch: 219 [134976/225000 (60%)] Loss: 18642.406250\n",
      "Train Epoch: 219 [137472/225000 (61%)] Loss: 18399.253906\n",
      "Train Epoch: 219 [139968/225000 (62%)] Loss: 18493.728516\n",
      "Train Epoch: 219 [142464/225000 (63%)] Loss: 17979.269531\n",
      "Train Epoch: 219 [144960/225000 (64%)] Loss: 17720.761719\n",
      "Train Epoch: 219 [147456/225000 (66%)] Loss: 18442.318359\n",
      "Train Epoch: 219 [149952/225000 (67%)] Loss: 18025.898438\n",
      "Train Epoch: 219 [152448/225000 (68%)] Loss: 17989.101562\n",
      "Train Epoch: 219 [154944/225000 (69%)] Loss: 18132.480469\n",
      "Train Epoch: 219 [157440/225000 (70%)] Loss: 17962.378906\n",
      "Train Epoch: 219 [159936/225000 (71%)] Loss: 18188.914062\n",
      "Train Epoch: 219 [162432/225000 (72%)] Loss: 18417.640625\n",
      "Train Epoch: 219 [164928/225000 (73%)] Loss: 18195.519531\n",
      "Train Epoch: 219 [167424/225000 (74%)] Loss: 18107.171875\n",
      "Train Epoch: 219 [169920/225000 (76%)] Loss: 17790.980469\n",
      "Train Epoch: 219 [172416/225000 (77%)] Loss: 18087.234375\n",
      "Train Epoch: 219 [174912/225000 (78%)] Loss: 18005.208984\n",
      "Train Epoch: 219 [177408/225000 (79%)] Loss: 18646.158203\n",
      "Train Epoch: 219 [179904/225000 (80%)] Loss: 18274.523438\n",
      "Train Epoch: 219 [182400/225000 (81%)] Loss: 18108.128906\n",
      "Train Epoch: 219 [184896/225000 (82%)] Loss: 17986.484375\n",
      "Train Epoch: 219 [187392/225000 (83%)] Loss: 17665.677734\n",
      "Train Epoch: 219 [189888/225000 (84%)] Loss: 18252.802734\n",
      "Train Epoch: 219 [192384/225000 (86%)] Loss: 18054.601562\n",
      "Train Epoch: 219 [194880/225000 (87%)] Loss: 18020.761719\n",
      "Train Epoch: 219 [197376/225000 (88%)] Loss: 17926.558594\n",
      "Train Epoch: 219 [199872/225000 (89%)] Loss: 18226.998047\n",
      "Train Epoch: 219 [202368/225000 (90%)] Loss: 18289.570312\n",
      "Train Epoch: 219 [204864/225000 (91%)] Loss: 18250.996094\n",
      "Train Epoch: 219 [207360/225000 (92%)] Loss: 18226.046875\n",
      "Train Epoch: 219 [209856/225000 (93%)] Loss: 18132.898438\n",
      "Train Epoch: 219 [212352/225000 (94%)] Loss: 18318.332031\n",
      "Train Epoch: 219 [214848/225000 (95%)] Loss: 18416.355469\n",
      "Train Epoch: 219 [217344/225000 (97%)] Loss: 17683.671875\n",
      "Train Epoch: 219 [219840/225000 (98%)] Loss: 17594.650391\n",
      "Train Epoch: 219 [222336/225000 (99%)] Loss: 18614.937500\n",
      "Train Epoch: 219 [224832/225000 (100%)] Loss: 18449.097656\n",
      "    epoch          : 219\n",
      "    loss           : 18176.415139051835\n",
      "    val_loss       : 18067.564668836938\n",
      "Train Epoch: 220 [192/225000 (0%)] Loss: 17675.058594\n",
      "Train Epoch: 220 [2688/225000 (1%)] Loss: 18251.701172\n",
      "Train Epoch: 220 [5184/225000 (2%)] Loss: 18285.605469\n",
      "Train Epoch: 220 [7680/225000 (3%)] Loss: 17878.242188\n",
      "Train Epoch: 220 [10176/225000 (5%)] Loss: 17915.769531\n",
      "Train Epoch: 220 [12672/225000 (6%)] Loss: 18257.394531\n",
      "Train Epoch: 220 [15168/225000 (7%)] Loss: 18108.789062\n",
      "Train Epoch: 220 [17664/225000 (8%)] Loss: 18302.691406\n",
      "Train Epoch: 220 [20160/225000 (9%)] Loss: 18084.933594\n",
      "Train Epoch: 220 [22656/225000 (10%)] Loss: 18269.812500\n",
      "Train Epoch: 220 [25152/225000 (11%)] Loss: 17605.958984\n",
      "Train Epoch: 220 [27648/225000 (12%)] Loss: 18375.724609\n",
      "Train Epoch: 220 [30144/225000 (13%)] Loss: 18303.261719\n",
      "Train Epoch: 220 [32640/225000 (15%)] Loss: 18140.894531\n",
      "Train Epoch: 220 [35136/225000 (16%)] Loss: 17860.433594\n",
      "Train Epoch: 220 [37632/225000 (17%)] Loss: 18087.910156\n",
      "Train Epoch: 220 [40128/225000 (18%)] Loss: 18446.796875\n",
      "Train Epoch: 220 [42624/225000 (19%)] Loss: 18504.406250\n",
      "Train Epoch: 220 [45120/225000 (20%)] Loss: 18451.380859\n",
      "Train Epoch: 220 [47616/225000 (21%)] Loss: 18380.152344\n",
      "Train Epoch: 220 [50112/225000 (22%)] Loss: 18333.148438\n",
      "Train Epoch: 220 [52608/225000 (23%)] Loss: 18251.169922\n",
      "Train Epoch: 220 [55104/225000 (24%)] Loss: 18402.785156\n",
      "Train Epoch: 220 [57600/225000 (26%)] Loss: 17781.648438\n",
      "Train Epoch: 220 [60096/225000 (27%)] Loss: 18266.042969\n",
      "Train Epoch: 220 [62592/225000 (28%)] Loss: 18429.937500\n",
      "Train Epoch: 220 [65088/225000 (29%)] Loss: 18143.910156\n",
      "Train Epoch: 220 [67584/225000 (30%)] Loss: 18297.916016\n",
      "Train Epoch: 220 [70080/225000 (31%)] Loss: 18031.855469\n",
      "Train Epoch: 220 [72576/225000 (32%)] Loss: 17959.472656\n",
      "Train Epoch: 220 [75072/225000 (33%)] Loss: 18434.699219\n",
      "Train Epoch: 220 [77568/225000 (34%)] Loss: 17938.228516\n",
      "Train Epoch: 220 [80064/225000 (36%)] Loss: 18148.648438\n",
      "Train Epoch: 220 [82560/225000 (37%)] Loss: 18581.417969\n",
      "Train Epoch: 220 [85056/225000 (38%)] Loss: 18391.708984\n",
      "Train Epoch: 220 [87552/225000 (39%)] Loss: 17934.667969\n",
      "Train Epoch: 220 [90048/225000 (40%)] Loss: 18354.005859\n",
      "Train Epoch: 220 [92544/225000 (41%)] Loss: 18211.777344\n",
      "Train Epoch: 220 [95040/225000 (42%)] Loss: 18183.898438\n",
      "Train Epoch: 220 [97536/225000 (43%)] Loss: 18158.798828\n",
      "Train Epoch: 220 [100032/225000 (44%)] Loss: 18417.171875\n",
      "Train Epoch: 220 [102528/225000 (46%)] Loss: 17793.187500\n",
      "Train Epoch: 220 [105024/225000 (47%)] Loss: 18082.677734\n",
      "Train Epoch: 220 [107520/225000 (48%)] Loss: 17979.183594\n",
      "Train Epoch: 220 [110016/225000 (49%)] Loss: 17980.125000\n",
      "Train Epoch: 220 [112512/225000 (50%)] Loss: 17941.003906\n",
      "Train Epoch: 220 [115008/225000 (51%)] Loss: 17880.574219\n",
      "Train Epoch: 220 [117504/225000 (52%)] Loss: 17796.291016\n",
      "Train Epoch: 220 [120000/225000 (53%)] Loss: 18234.562500\n",
      "Train Epoch: 220 [122496/225000 (54%)] Loss: 17984.222656\n",
      "Train Epoch: 220 [124992/225000 (56%)] Loss: 18149.203125\n",
      "Train Epoch: 220 [127488/225000 (57%)] Loss: 18254.000000\n",
      "Train Epoch: 220 [129984/225000 (58%)] Loss: 18005.492188\n",
      "Train Epoch: 220 [132480/225000 (59%)] Loss: 18712.093750\n",
      "Train Epoch: 220 [134976/225000 (60%)] Loss: 17914.914062\n",
      "Train Epoch: 220 [137472/225000 (61%)] Loss: 18321.519531\n",
      "Train Epoch: 220 [139968/225000 (62%)] Loss: 18190.398438\n",
      "Train Epoch: 220 [142464/225000 (63%)] Loss: 18167.546875\n",
      "Train Epoch: 220 [144960/225000 (64%)] Loss: 17902.734375\n",
      "Train Epoch: 220 [147456/225000 (66%)] Loss: 17949.046875\n",
      "Train Epoch: 220 [149952/225000 (67%)] Loss: 18133.386719\n",
      "Train Epoch: 220 [152448/225000 (68%)] Loss: 18394.050781\n",
      "Train Epoch: 220 [154944/225000 (69%)] Loss: 17675.148438\n",
      "Train Epoch: 220 [157440/225000 (70%)] Loss: 18226.656250\n",
      "Train Epoch: 220 [159936/225000 (71%)] Loss: 17711.085938\n",
      "Train Epoch: 220 [162432/225000 (72%)] Loss: 18702.490234\n",
      "Train Epoch: 220 [164928/225000 (73%)] Loss: 18048.738281\n",
      "Train Epoch: 220 [167424/225000 (74%)] Loss: 21405.642578\n",
      "Train Epoch: 220 [169920/225000 (76%)] Loss: 18288.914062\n",
      "Train Epoch: 220 [172416/225000 (77%)] Loss: 17754.769531\n",
      "Train Epoch: 220 [174912/225000 (78%)] Loss: 18706.933594\n",
      "Train Epoch: 220 [177408/225000 (79%)] Loss: 18186.906250\n",
      "Train Epoch: 220 [179904/225000 (80%)] Loss: 18225.570312\n",
      "Train Epoch: 220 [182400/225000 (81%)] Loss: 18763.699219\n",
      "Train Epoch: 220 [184896/225000 (82%)] Loss: 18193.140625\n",
      "Train Epoch: 220 [187392/225000 (83%)] Loss: 17973.105469\n",
      "Train Epoch: 220 [189888/225000 (84%)] Loss: 18647.435547\n",
      "Train Epoch: 220 [192384/225000 (86%)] Loss: 17972.144531\n",
      "Train Epoch: 220 [194880/225000 (87%)] Loss: 17995.906250\n",
      "Train Epoch: 220 [197376/225000 (88%)] Loss: 18104.378906\n",
      "Train Epoch: 220 [199872/225000 (89%)] Loss: 18312.992188\n",
      "Train Epoch: 220 [202368/225000 (90%)] Loss: 18213.347656\n",
      "Train Epoch: 220 [204864/225000 (91%)] Loss: 17786.960938\n",
      "Train Epoch: 220 [207360/225000 (92%)] Loss: 18408.050781\n",
      "Train Epoch: 220 [209856/225000 (93%)] Loss: 18416.041016\n",
      "Train Epoch: 220 [212352/225000 (94%)] Loss: 17745.476562\n",
      "Train Epoch: 220 [214848/225000 (95%)] Loss: 18081.285156\n",
      "Train Epoch: 220 [217344/225000 (97%)] Loss: 18433.554688\n",
      "Train Epoch: 220 [219840/225000 (98%)] Loss: 18584.128906\n",
      "Train Epoch: 220 [222336/225000 (99%)] Loss: 18408.281250\n",
      "Train Epoch: 220 [224832/225000 (100%)] Loss: 18151.437500\n",
      "    epoch          : 220\n",
      "    loss           : 18156.93476862468\n",
      "    val_loss       : 18058.31121339143\n",
      "Train Epoch: 221 [192/225000 (0%)] Loss: 18162.171875\n",
      "Train Epoch: 221 [2688/225000 (1%)] Loss: 17876.017578\n",
      "Train Epoch: 221 [5184/225000 (2%)] Loss: 18123.613281\n",
      "Train Epoch: 221 [7680/225000 (3%)] Loss: 18204.992188\n",
      "Train Epoch: 221 [10176/225000 (5%)] Loss: 17770.007812\n",
      "Train Epoch: 221 [12672/225000 (6%)] Loss: 17718.642578\n",
      "Train Epoch: 221 [15168/225000 (7%)] Loss: 18353.406250\n",
      "Train Epoch: 221 [17664/225000 (8%)] Loss: 18525.312500\n",
      "Train Epoch: 221 [20160/225000 (9%)] Loss: 18902.671875\n",
      "Train Epoch: 221 [22656/225000 (10%)] Loss: 18089.554688\n",
      "Train Epoch: 221 [25152/225000 (11%)] Loss: 17734.017578\n",
      "Train Epoch: 221 [27648/225000 (12%)] Loss: 17997.656250\n",
      "Train Epoch: 221 [30144/225000 (13%)] Loss: 18005.429688\n",
      "Train Epoch: 221 [32640/225000 (15%)] Loss: 18142.623047\n",
      "Train Epoch: 221 [35136/225000 (16%)] Loss: 18135.550781\n",
      "Train Epoch: 221 [37632/225000 (17%)] Loss: 18422.730469\n",
      "Train Epoch: 221 [40128/225000 (18%)] Loss: 18477.681641\n",
      "Train Epoch: 221 [42624/225000 (19%)] Loss: 18165.339844\n",
      "Train Epoch: 221 [45120/225000 (20%)] Loss: 18803.365234\n",
      "Train Epoch: 221 [47616/225000 (21%)] Loss: 18613.808594\n",
      "Train Epoch: 221 [50112/225000 (22%)] Loss: 18610.734375\n",
      "Train Epoch: 221 [52608/225000 (23%)] Loss: 18418.535156\n",
      "Train Epoch: 221 [55104/225000 (24%)] Loss: 18591.599609\n",
      "Train Epoch: 221 [57600/225000 (26%)] Loss: 18231.353516\n",
      "Train Epoch: 221 [60096/225000 (27%)] Loss: 18254.394531\n",
      "Train Epoch: 221 [62592/225000 (28%)] Loss: 18190.113281\n",
      "Train Epoch: 221 [65088/225000 (29%)] Loss: 18655.226562\n",
      "Train Epoch: 221 [67584/225000 (30%)] Loss: 18089.554688\n",
      "Train Epoch: 221 [70080/225000 (31%)] Loss: 18431.937500\n",
      "Train Epoch: 221 [72576/225000 (32%)] Loss: 18018.294922\n",
      "Train Epoch: 221 [75072/225000 (33%)] Loss: 18301.425781\n",
      "Train Epoch: 221 [77568/225000 (34%)] Loss: 18233.667969\n",
      "Train Epoch: 221 [80064/225000 (36%)] Loss: 18529.234375\n",
      "Train Epoch: 221 [82560/225000 (37%)] Loss: 17777.269531\n",
      "Train Epoch: 221 [85056/225000 (38%)] Loss: 17764.957031\n",
      "Train Epoch: 221 [87552/225000 (39%)] Loss: 18277.976562\n",
      "Train Epoch: 221 [90048/225000 (40%)] Loss: 18225.421875\n",
      "Train Epoch: 221 [92544/225000 (41%)] Loss: 17838.820312\n",
      "Train Epoch: 221 [95040/225000 (42%)] Loss: 18338.968750\n",
      "Train Epoch: 221 [97536/225000 (43%)] Loss: 18090.273438\n",
      "Train Epoch: 221 [100032/225000 (44%)] Loss: 18321.066406\n",
      "Train Epoch: 221 [102528/225000 (46%)] Loss: 18141.998047\n",
      "Train Epoch: 221 [105024/225000 (47%)] Loss: 18013.468750\n",
      "Train Epoch: 221 [107520/225000 (48%)] Loss: 17978.474609\n",
      "Train Epoch: 221 [110016/225000 (49%)] Loss: 18288.933594\n",
      "Train Epoch: 221 [112512/225000 (50%)] Loss: 17959.894531\n",
      "Train Epoch: 221 [115008/225000 (51%)] Loss: 18549.921875\n",
      "Train Epoch: 221 [117504/225000 (52%)] Loss: 18207.035156\n",
      "Train Epoch: 221 [120000/225000 (53%)] Loss: 18598.007812\n",
      "Train Epoch: 221 [122496/225000 (54%)] Loss: 18421.423828\n",
      "Train Epoch: 221 [124992/225000 (56%)] Loss: 17946.574219\n",
      "Train Epoch: 221 [127488/225000 (57%)] Loss: 18300.398438\n",
      "Train Epoch: 221 [129984/225000 (58%)] Loss: 17909.828125\n",
      "Train Epoch: 221 [132480/225000 (59%)] Loss: 17957.085938\n",
      "Train Epoch: 221 [134976/225000 (60%)] Loss: 18394.621094\n",
      "Train Epoch: 221 [137472/225000 (61%)] Loss: 18353.507812\n",
      "Train Epoch: 221 [139968/225000 (62%)] Loss: 18225.777344\n",
      "Train Epoch: 221 [142464/225000 (63%)] Loss: 18269.542969\n",
      "Train Epoch: 221 [144960/225000 (64%)] Loss: 18214.734375\n",
      "Train Epoch: 221 [147456/225000 (66%)] Loss: 17910.402344\n",
      "Train Epoch: 221 [149952/225000 (67%)] Loss: 17818.050781\n",
      "Train Epoch: 221 [152448/225000 (68%)] Loss: 18284.500000\n",
      "Train Epoch: 221 [154944/225000 (69%)] Loss: 18371.615234\n",
      "Train Epoch: 221 [157440/225000 (70%)] Loss: 18299.382812\n",
      "Train Epoch: 221 [159936/225000 (71%)] Loss: 17704.582031\n",
      "Train Epoch: 221 [162432/225000 (72%)] Loss: 18150.890625\n",
      "Train Epoch: 221 [164928/225000 (73%)] Loss: 18065.441406\n",
      "Train Epoch: 221 [167424/225000 (74%)] Loss: 18412.621094\n",
      "Train Epoch: 221 [169920/225000 (76%)] Loss: 18195.437500\n",
      "Train Epoch: 221 [172416/225000 (77%)] Loss: 18001.824219\n",
      "Train Epoch: 221 [174912/225000 (78%)] Loss: 17964.707031\n",
      "Train Epoch: 221 [177408/225000 (79%)] Loss: 17857.193359\n",
      "Train Epoch: 221 [179904/225000 (80%)] Loss: 17642.125000\n",
      "Train Epoch: 221 [182400/225000 (81%)] Loss: 18040.990234\n",
      "Train Epoch: 221 [184896/225000 (82%)] Loss: 18102.531250\n",
      "Train Epoch: 221 [187392/225000 (83%)] Loss: 17788.359375\n",
      "Train Epoch: 221 [189888/225000 (84%)] Loss: 18144.439453\n",
      "Train Epoch: 221 [192384/225000 (86%)] Loss: 17750.929688\n",
      "Train Epoch: 221 [194880/225000 (87%)] Loss: 18096.179688\n",
      "Train Epoch: 221 [197376/225000 (88%)] Loss: 17926.988281\n",
      "Train Epoch: 221 [199872/225000 (89%)] Loss: 18460.203125\n",
      "Train Epoch: 221 [202368/225000 (90%)] Loss: 17993.281250\n",
      "Train Epoch: 221 [204864/225000 (91%)] Loss: 18272.859375\n",
      "Train Epoch: 221 [207360/225000 (92%)] Loss: 18135.078125\n",
      "Train Epoch: 221 [209856/225000 (93%)] Loss: 18217.322266\n",
      "Train Epoch: 221 [212352/225000 (94%)] Loss: 17929.318359\n",
      "Train Epoch: 221 [214848/225000 (95%)] Loss: 17887.921875\n",
      "Train Epoch: 221 [217344/225000 (97%)] Loss: 17975.296875\n",
      "Train Epoch: 221 [219840/225000 (98%)] Loss: 17545.781250\n",
      "Train Epoch: 221 [222336/225000 (99%)] Loss: 18100.091797\n",
      "Train Epoch: 221 [224832/225000 (100%)] Loss: 17975.460938\n",
      "    epoch          : 221\n",
      "    loss           : 18139.957957817835\n",
      "    val_loss       : 18031.171373855977\n",
      "Train Epoch: 222 [192/225000 (0%)] Loss: 18128.773438\n",
      "Train Epoch: 222 [2688/225000 (1%)] Loss: 18080.302734\n",
      "Train Epoch: 222 [5184/225000 (2%)] Loss: 18130.035156\n",
      "Train Epoch: 222 [7680/225000 (3%)] Loss: 17674.464844\n",
      "Train Epoch: 222 [10176/225000 (5%)] Loss: 18858.072266\n",
      "Train Epoch: 222 [12672/225000 (6%)] Loss: 17709.621094\n",
      "Train Epoch: 222 [15168/225000 (7%)] Loss: 18042.660156\n",
      "Train Epoch: 222 [17664/225000 (8%)] Loss: 18041.656250\n",
      "Train Epoch: 222 [20160/225000 (9%)] Loss: 17907.226562\n",
      "Train Epoch: 222 [22656/225000 (10%)] Loss: 17922.179688\n",
      "Train Epoch: 222 [25152/225000 (11%)] Loss: 18057.468750\n",
      "Train Epoch: 222 [27648/225000 (12%)] Loss: 17822.625000\n",
      "Train Epoch: 222 [30144/225000 (13%)] Loss: 18220.296875\n",
      "Train Epoch: 222 [32640/225000 (15%)] Loss: 17963.544922\n",
      "Train Epoch: 222 [35136/225000 (16%)] Loss: 17760.093750\n",
      "Train Epoch: 222 [37632/225000 (17%)] Loss: 18456.322266\n",
      "Train Epoch: 222 [40128/225000 (18%)] Loss: 17914.683594\n",
      "Train Epoch: 222 [42624/225000 (19%)] Loss: 18095.927734\n",
      "Train Epoch: 222 [45120/225000 (20%)] Loss: 17998.435547\n",
      "Train Epoch: 222 [47616/225000 (21%)] Loss: 18118.832031\n",
      "Train Epoch: 222 [50112/225000 (22%)] Loss: 17844.162109\n",
      "Train Epoch: 222 [52608/225000 (23%)] Loss: 17709.548828\n",
      "Train Epoch: 222 [55104/225000 (24%)] Loss: 18070.613281\n",
      "Train Epoch: 222 [57600/225000 (26%)] Loss: 17915.404297\n",
      "Train Epoch: 222 [60096/225000 (27%)] Loss: 17753.968750\n",
      "Train Epoch: 222 [62592/225000 (28%)] Loss: 18058.972656\n",
      "Train Epoch: 222 [65088/225000 (29%)] Loss: 18654.054688\n",
      "Train Epoch: 222 [67584/225000 (30%)] Loss: 17826.250000\n",
      "Train Epoch: 222 [70080/225000 (31%)] Loss: 18343.806641\n",
      "Train Epoch: 222 [72576/225000 (32%)] Loss: 17763.585938\n",
      "Train Epoch: 222 [75072/225000 (33%)] Loss: 18161.960938\n",
      "Train Epoch: 222 [77568/225000 (34%)] Loss: 18420.312500\n",
      "Train Epoch: 222 [80064/225000 (36%)] Loss: 18629.800781\n",
      "Train Epoch: 222 [82560/225000 (37%)] Loss: 18353.375000\n",
      "Train Epoch: 222 [85056/225000 (38%)] Loss: 17841.601562\n",
      "Train Epoch: 222 [87552/225000 (39%)] Loss: 18177.437500\n",
      "Train Epoch: 222 [90048/225000 (40%)] Loss: 17658.964844\n",
      "Train Epoch: 222 [92544/225000 (41%)] Loss: 18172.414062\n",
      "Train Epoch: 222 [95040/225000 (42%)] Loss: 18460.730469\n",
      "Train Epoch: 222 [97536/225000 (43%)] Loss: 18238.576172\n",
      "Train Epoch: 222 [100032/225000 (44%)] Loss: 18197.708984\n",
      "Train Epoch: 222 [102528/225000 (46%)] Loss: 18355.425781\n",
      "Train Epoch: 222 [105024/225000 (47%)] Loss: 18151.972656\n",
      "Train Epoch: 222 [107520/225000 (48%)] Loss: 18310.548828\n",
      "Train Epoch: 222 [110016/225000 (49%)] Loss: 18002.707031\n",
      "Train Epoch: 222 [112512/225000 (50%)] Loss: 19048.132812\n",
      "Train Epoch: 222 [115008/225000 (51%)] Loss: 17762.187500\n",
      "Train Epoch: 222 [117504/225000 (52%)] Loss: 18047.773438\n",
      "Train Epoch: 222 [120000/225000 (53%)] Loss: 17880.398438\n",
      "Train Epoch: 222 [122496/225000 (54%)] Loss: 18359.224609\n",
      "Train Epoch: 222 [124992/225000 (56%)] Loss: 17835.699219\n",
      "Train Epoch: 222 [127488/225000 (57%)] Loss: 18161.152344\n",
      "Train Epoch: 222 [129984/225000 (58%)] Loss: 18433.437500\n",
      "Train Epoch: 222 [132480/225000 (59%)] Loss: 18367.281250\n",
      "Train Epoch: 222 [134976/225000 (60%)] Loss: 18187.957031\n",
      "Train Epoch: 222 [137472/225000 (61%)] Loss: 18603.156250\n",
      "Train Epoch: 222 [139968/225000 (62%)] Loss: 18301.423828\n",
      "Train Epoch: 222 [142464/225000 (63%)] Loss: 18513.263672\n",
      "Train Epoch: 222 [144960/225000 (64%)] Loss: 18332.632812\n",
      "Train Epoch: 222 [147456/225000 (66%)] Loss: 17860.625000\n",
      "Train Epoch: 222 [149952/225000 (67%)] Loss: 18195.281250\n",
      "Train Epoch: 222 [152448/225000 (68%)] Loss: 17785.562500\n",
      "Train Epoch: 222 [154944/225000 (69%)] Loss: 18105.736328\n",
      "Train Epoch: 222 [157440/225000 (70%)] Loss: 18050.115234\n",
      "Train Epoch: 222 [159936/225000 (71%)] Loss: 18216.218750\n",
      "Train Epoch: 222 [162432/225000 (72%)] Loss: 18446.082031\n",
      "Train Epoch: 222 [164928/225000 (73%)] Loss: 18413.566406\n",
      "Train Epoch: 222 [167424/225000 (74%)] Loss: 18289.054688\n",
      "Train Epoch: 222 [169920/225000 (76%)] Loss: 18196.972656\n",
      "Train Epoch: 222 [172416/225000 (77%)] Loss: 17884.906250\n",
      "Train Epoch: 222 [174912/225000 (78%)] Loss: 18326.496094\n",
      "Train Epoch: 222 [177408/225000 (79%)] Loss: 17629.500000\n",
      "Train Epoch: 222 [179904/225000 (80%)] Loss: 18310.234375\n",
      "Train Epoch: 222 [182400/225000 (81%)] Loss: 18103.558594\n",
      "Train Epoch: 222 [184896/225000 (82%)] Loss: 18191.134766\n",
      "Train Epoch: 222 [187392/225000 (83%)] Loss: 18349.835938\n",
      "Train Epoch: 222 [189888/225000 (84%)] Loss: 17770.996094\n",
      "Train Epoch: 222 [192384/225000 (86%)] Loss: 18228.082031\n",
      "Train Epoch: 222 [194880/225000 (87%)] Loss: 18220.058594\n",
      "Train Epoch: 222 [197376/225000 (88%)] Loss: 18371.753906\n",
      "Train Epoch: 222 [199872/225000 (89%)] Loss: 18105.097656\n",
      "Train Epoch: 222 [202368/225000 (90%)] Loss: 18291.207031\n",
      "Train Epoch: 222 [204864/225000 (91%)] Loss: 17748.005859\n",
      "Train Epoch: 222 [207360/225000 (92%)] Loss: 17998.230469\n",
      "Train Epoch: 222 [209856/225000 (93%)] Loss: 18551.750000\n",
      "Train Epoch: 222 [212352/225000 (94%)] Loss: 17894.824219\n",
      "Train Epoch: 222 [214848/225000 (95%)] Loss: 17678.287109\n",
      "Train Epoch: 222 [217344/225000 (97%)] Loss: 17638.832031\n",
      "Train Epoch: 222 [219840/225000 (98%)] Loss: 17707.187500\n",
      "Train Epoch: 222 [222336/225000 (99%)] Loss: 18262.527344\n",
      "Train Epoch: 222 [224832/225000 (100%)] Loss: 18479.167969\n",
      "    epoch          : 222\n",
      "    loss           : 18112.10434636972\n",
      "    val_loss       : 18016.934227470225\n",
      "Train Epoch: 223 [192/225000 (0%)] Loss: 17669.291016\n",
      "Train Epoch: 223 [2688/225000 (1%)] Loss: 17751.390625\n",
      "Train Epoch: 223 [5184/225000 (2%)] Loss: 17948.019531\n",
      "Train Epoch: 223 [7680/225000 (3%)] Loss: 17923.906250\n",
      "Train Epoch: 223 [10176/225000 (5%)] Loss: 18187.125000\n",
      "Train Epoch: 223 [12672/225000 (6%)] Loss: 17938.753906\n",
      "Train Epoch: 223 [15168/225000 (7%)] Loss: 17984.039062\n",
      "Train Epoch: 223 [17664/225000 (8%)] Loss: 17666.476562\n",
      "Train Epoch: 223 [20160/225000 (9%)] Loss: 18109.082031\n",
      "Train Epoch: 223 [22656/225000 (10%)] Loss: 17644.128906\n",
      "Train Epoch: 223 [25152/225000 (11%)] Loss: 18231.246094\n",
      "Train Epoch: 223 [27648/225000 (12%)] Loss: 17974.832031\n",
      "Train Epoch: 223 [30144/225000 (13%)] Loss: 18141.550781\n",
      "Train Epoch: 223 [32640/225000 (15%)] Loss: 17863.203125\n",
      "Train Epoch: 223 [35136/225000 (16%)] Loss: 18234.843750\n",
      "Train Epoch: 223 [37632/225000 (17%)] Loss: 17861.005859\n",
      "Train Epoch: 223 [40128/225000 (18%)] Loss: 18071.500000\n",
      "Train Epoch: 223 [42624/225000 (19%)] Loss: 17810.070312\n",
      "Train Epoch: 223 [45120/225000 (20%)] Loss: 18043.814453\n",
      "Train Epoch: 223 [47616/225000 (21%)] Loss: 17757.261719\n",
      "Train Epoch: 223 [50112/225000 (22%)] Loss: 17784.566406\n",
      "Train Epoch: 223 [52608/225000 (23%)] Loss: 17927.707031\n",
      "Train Epoch: 223 [55104/225000 (24%)] Loss: 18230.312500\n",
      "Train Epoch: 223 [57600/225000 (26%)] Loss: 17440.292969\n",
      "Train Epoch: 223 [60096/225000 (27%)] Loss: 18026.148438\n",
      "Train Epoch: 223 [62592/225000 (28%)] Loss: 17928.441406\n",
      "Train Epoch: 223 [65088/225000 (29%)] Loss: 18321.695312\n",
      "Train Epoch: 223 [67584/225000 (30%)] Loss: 18070.958984\n",
      "Train Epoch: 223 [70080/225000 (31%)] Loss: 17743.199219\n",
      "Train Epoch: 223 [72576/225000 (32%)] Loss: 17773.898438\n",
      "Train Epoch: 223 [75072/225000 (33%)] Loss: 17752.851562\n",
      "Train Epoch: 223 [77568/225000 (34%)] Loss: 17893.771484\n",
      "Train Epoch: 223 [80064/225000 (36%)] Loss: 18097.871094\n",
      "Train Epoch: 223 [82560/225000 (37%)] Loss: 18162.785156\n",
      "Train Epoch: 223 [85056/225000 (38%)] Loss: 18490.503906\n",
      "Train Epoch: 223 [87552/225000 (39%)] Loss: 18269.750000\n",
      "Train Epoch: 223 [90048/225000 (40%)] Loss: 18339.619141\n",
      "Train Epoch: 223 [92544/225000 (41%)] Loss: 18054.617188\n",
      "Train Epoch: 223 [95040/225000 (42%)] Loss: 18539.976562\n",
      "Train Epoch: 223 [97536/225000 (43%)] Loss: 18130.628906\n",
      "Train Epoch: 223 [100032/225000 (44%)] Loss: 18075.771484\n",
      "Train Epoch: 223 [102528/225000 (46%)] Loss: 18221.742188\n",
      "Train Epoch: 223 [105024/225000 (47%)] Loss: 18533.292969\n",
      "Train Epoch: 223 [107520/225000 (48%)] Loss: 18331.476562\n",
      "Train Epoch: 223 [110016/225000 (49%)] Loss: 17995.847656\n",
      "Train Epoch: 223 [112512/225000 (50%)] Loss: 18568.107422\n",
      "Train Epoch: 223 [115008/225000 (51%)] Loss: 18191.728516\n",
      "Train Epoch: 223 [117504/225000 (52%)] Loss: 18535.828125\n",
      "Train Epoch: 223 [120000/225000 (53%)] Loss: 17705.710938\n",
      "Train Epoch: 223 [122496/225000 (54%)] Loss: 17510.132812\n",
      "Train Epoch: 223 [124992/225000 (56%)] Loss: 18394.322266\n",
      "Train Epoch: 223 [127488/225000 (57%)] Loss: 18517.041016\n",
      "Train Epoch: 223 [129984/225000 (58%)] Loss: 18074.482422\n",
      "Train Epoch: 223 [132480/225000 (59%)] Loss: 18283.230469\n",
      "Train Epoch: 223 [134976/225000 (60%)] Loss: 18253.371094\n",
      "Train Epoch: 223 [137472/225000 (61%)] Loss: 18719.318359\n",
      "Train Epoch: 223 [139968/225000 (62%)] Loss: 18087.054688\n",
      "Train Epoch: 223 [142464/225000 (63%)] Loss: 18268.802734\n",
      "Train Epoch: 223 [144960/225000 (64%)] Loss: 18260.445312\n",
      "Train Epoch: 223 [147456/225000 (66%)] Loss: 17672.734375\n",
      "Train Epoch: 223 [149952/225000 (67%)] Loss: 17746.562500\n",
      "Train Epoch: 223 [152448/225000 (68%)] Loss: 17653.527344\n",
      "Train Epoch: 223 [154944/225000 (69%)] Loss: 18340.474609\n",
      "Train Epoch: 223 [157440/225000 (70%)] Loss: 17687.058594\n",
      "Train Epoch: 223 [159936/225000 (71%)] Loss: 17656.496094\n",
      "Train Epoch: 223 [162432/225000 (72%)] Loss: 18612.753906\n",
      "Train Epoch: 223 [164928/225000 (73%)] Loss: 18344.214844\n",
      "Train Epoch: 223 [167424/225000 (74%)] Loss: 18251.423828\n",
      "Train Epoch: 223 [169920/225000 (76%)] Loss: 17976.183594\n",
      "Train Epoch: 223 [172416/225000 (77%)] Loss: 17984.529297\n",
      "Train Epoch: 223 [174912/225000 (78%)] Loss: 18099.404297\n",
      "Train Epoch: 223 [177408/225000 (79%)] Loss: 17985.804688\n",
      "Train Epoch: 223 [179904/225000 (80%)] Loss: 18441.742188\n",
      "Train Epoch: 223 [182400/225000 (81%)] Loss: 17667.525391\n",
      "Train Epoch: 223 [184896/225000 (82%)] Loss: 18392.529297\n",
      "Train Epoch: 223 [187392/225000 (83%)] Loss: 18382.882812\n",
      "Train Epoch: 223 [189888/225000 (84%)] Loss: 18012.753906\n",
      "Train Epoch: 223 [192384/225000 (86%)] Loss: 18071.914062\n",
      "Train Epoch: 223 [194880/225000 (87%)] Loss: 17996.277344\n",
      "Train Epoch: 223 [197376/225000 (88%)] Loss: 17840.603516\n",
      "Train Epoch: 223 [199872/225000 (89%)] Loss: 18250.625000\n",
      "Train Epoch: 223 [202368/225000 (90%)] Loss: 18368.826172\n",
      "Train Epoch: 223 [204864/225000 (91%)] Loss: 18350.839844\n",
      "Train Epoch: 223 [207360/225000 (92%)] Loss: 17884.824219\n",
      "Train Epoch: 223 [209856/225000 (93%)] Loss: 18688.886719\n",
      "Train Epoch: 223 [212352/225000 (94%)] Loss: 18262.703125\n",
      "Train Epoch: 223 [214848/225000 (95%)] Loss: 18276.085938\n",
      "Train Epoch: 223 [217344/225000 (97%)] Loss: 17689.521484\n",
      "Train Epoch: 223 [219840/225000 (98%)] Loss: 18035.128906\n",
      "Train Epoch: 223 [222336/225000 (99%)] Loss: 17890.320312\n",
      "Train Epoch: 223 [224832/225000 (100%)] Loss: 18371.283203\n",
      "    epoch          : 223\n",
      "    loss           : 18093.654558513757\n",
      "    val_loss       : 18024.591113554612\n",
      "Train Epoch: 224 [192/225000 (0%)] Loss: 18041.240234\n",
      "Train Epoch: 224 [2688/225000 (1%)] Loss: 17553.308594\n",
      "Train Epoch: 224 [5184/225000 (2%)] Loss: 18119.439453\n",
      "Train Epoch: 224 [7680/225000 (3%)] Loss: 18258.005859\n",
      "Train Epoch: 224 [10176/225000 (5%)] Loss: 17906.185547\n",
      "Train Epoch: 224 [12672/225000 (6%)] Loss: 17920.144531\n",
      "Train Epoch: 224 [15168/225000 (7%)] Loss: 18171.281250\n",
      "Train Epoch: 224 [17664/225000 (8%)] Loss: 18182.593750\n",
      "Train Epoch: 224 [20160/225000 (9%)] Loss: 18338.148438\n",
      "Train Epoch: 224 [22656/225000 (10%)] Loss: 18335.464844\n",
      "Train Epoch: 224 [25152/225000 (11%)] Loss: 18637.230469\n",
      "Train Epoch: 224 [27648/225000 (12%)] Loss: 18403.285156\n",
      "Train Epoch: 224 [30144/225000 (13%)] Loss: 17982.242188\n",
      "Train Epoch: 224 [32640/225000 (15%)] Loss: 17871.359375\n",
      "Train Epoch: 224 [35136/225000 (16%)] Loss: 18199.128906\n",
      "Train Epoch: 224 [37632/225000 (17%)] Loss: 17971.925781\n",
      "Train Epoch: 224 [40128/225000 (18%)] Loss: 17774.714844\n",
      "Train Epoch: 224 [42624/225000 (19%)] Loss: 18617.066406\n",
      "Train Epoch: 224 [45120/225000 (20%)] Loss: 18066.789062\n",
      "Train Epoch: 224 [47616/225000 (21%)] Loss: 18169.144531\n",
      "Train Epoch: 224 [50112/225000 (22%)] Loss: 18215.390625\n",
      "Train Epoch: 224 [52608/225000 (23%)] Loss: 18525.929688\n",
      "Train Epoch: 224 [55104/225000 (24%)] Loss: 17682.101562\n",
      "Train Epoch: 224 [57600/225000 (26%)] Loss: 17988.138672\n",
      "Train Epoch: 224 [60096/225000 (27%)] Loss: 17652.687500\n",
      "Train Epoch: 224 [62592/225000 (28%)] Loss: 18331.861328\n",
      "Train Epoch: 224 [65088/225000 (29%)] Loss: 18437.125000\n",
      "Train Epoch: 224 [67584/225000 (30%)] Loss: 18147.515625\n",
      "Train Epoch: 224 [70080/225000 (31%)] Loss: 18183.753906\n",
      "Train Epoch: 224 [72576/225000 (32%)] Loss: 17676.839844\n",
      "Train Epoch: 224 [75072/225000 (33%)] Loss: 17783.722656\n",
      "Train Epoch: 224 [77568/225000 (34%)] Loss: 18456.814453\n",
      "Train Epoch: 224 [80064/225000 (36%)] Loss: 18315.148438\n",
      "Train Epoch: 224 [82560/225000 (37%)] Loss: 18629.392578\n",
      "Train Epoch: 224 [85056/225000 (38%)] Loss: 17782.757812\n",
      "Train Epoch: 224 [87552/225000 (39%)] Loss: 18297.029297\n",
      "Train Epoch: 224 [90048/225000 (40%)] Loss: 17714.275391\n",
      "Train Epoch: 224 [92544/225000 (41%)] Loss: 18046.056641\n",
      "Train Epoch: 224 [95040/225000 (42%)] Loss: 18028.988281\n",
      "Train Epoch: 224 [97536/225000 (43%)] Loss: 18139.042969\n",
      "Train Epoch: 224 [100032/225000 (44%)] Loss: 17984.527344\n",
      "Train Epoch: 224 [102528/225000 (46%)] Loss: 18217.226562\n",
      "Train Epoch: 224 [105024/225000 (47%)] Loss: 17875.296875\n",
      "Train Epoch: 224 [107520/225000 (48%)] Loss: 18153.882812\n",
      "Train Epoch: 224 [110016/225000 (49%)] Loss: 17995.722656\n",
      "Train Epoch: 224 [112512/225000 (50%)] Loss: 18256.644531\n",
      "Train Epoch: 224 [115008/225000 (51%)] Loss: 17922.449219\n",
      "Train Epoch: 224 [117504/225000 (52%)] Loss: 18007.175781\n",
      "Train Epoch: 224 [120000/225000 (53%)] Loss: 18096.187500\n",
      "Train Epoch: 224 [122496/225000 (54%)] Loss: 18441.851562\n",
      "Train Epoch: 224 [124992/225000 (56%)] Loss: 18062.589844\n",
      "Train Epoch: 224 [127488/225000 (57%)] Loss: 18100.580078\n",
      "Train Epoch: 224 [129984/225000 (58%)] Loss: 18191.019531\n",
      "Train Epoch: 224 [132480/225000 (59%)] Loss: 18400.878906\n",
      "Train Epoch: 224 [134976/225000 (60%)] Loss: 18277.275391\n",
      "Train Epoch: 224 [137472/225000 (61%)] Loss: 17907.128906\n",
      "Train Epoch: 224 [139968/225000 (62%)] Loss: 17797.996094\n",
      "Train Epoch: 224 [142464/225000 (63%)] Loss: 17968.234375\n",
      "Train Epoch: 224 [144960/225000 (64%)] Loss: 18043.203125\n",
      "Train Epoch: 224 [147456/225000 (66%)] Loss: 18231.648438\n",
      "Train Epoch: 224 [149952/225000 (67%)] Loss: 18593.621094\n",
      "Train Epoch: 224 [152448/225000 (68%)] Loss: 18019.406250\n",
      "Train Epoch: 224 [154944/225000 (69%)] Loss: 17996.917969\n",
      "Train Epoch: 224 [157440/225000 (70%)] Loss: 17751.570312\n",
      "Train Epoch: 224 [159936/225000 (71%)] Loss: 17767.687500\n",
      "Train Epoch: 224 [162432/225000 (72%)] Loss: 18006.306641\n",
      "Train Epoch: 224 [164928/225000 (73%)] Loss: 17868.156250\n",
      "Train Epoch: 224 [167424/225000 (74%)] Loss: 18174.550781\n",
      "Train Epoch: 224 [169920/225000 (76%)] Loss: 18396.896484\n",
      "Train Epoch: 224 [172416/225000 (77%)] Loss: 18278.429688\n",
      "Train Epoch: 224 [174912/225000 (78%)] Loss: 17825.302734\n",
      "Train Epoch: 224 [177408/225000 (79%)] Loss: 17907.728516\n",
      "Train Epoch: 224 [179904/225000 (80%)] Loss: 17857.083984\n",
      "Train Epoch: 224 [182400/225000 (81%)] Loss: 18125.261719\n",
      "Train Epoch: 224 [184896/225000 (82%)] Loss: 17933.746094\n",
      "Train Epoch: 224 [187392/225000 (83%)] Loss: 18131.048828\n",
      "Train Epoch: 224 [189888/225000 (84%)] Loss: 17862.558594\n",
      "Train Epoch: 224 [192384/225000 (86%)] Loss: 18093.203125\n",
      "Train Epoch: 224 [194880/225000 (87%)] Loss: 17513.968750\n",
      "Train Epoch: 224 [197376/225000 (88%)] Loss: 17936.410156\n",
      "Train Epoch: 224 [199872/225000 (89%)] Loss: 18370.964844\n",
      "Train Epoch: 224 [202368/225000 (90%)] Loss: 17981.378906\n",
      "Train Epoch: 224 [204864/225000 (91%)] Loss: 18126.894531\n",
      "Train Epoch: 224 [207360/225000 (92%)] Loss: 18164.992188\n",
      "Train Epoch: 224 [209856/225000 (93%)] Loss: 17772.738281\n",
      "Train Epoch: 224 [212352/225000 (94%)] Loss: 18207.414062\n",
      "Train Epoch: 224 [214848/225000 (95%)] Loss: 18212.423828\n",
      "Train Epoch: 224 [217344/225000 (97%)] Loss: 17764.773438\n",
      "Train Epoch: 224 [219840/225000 (98%)] Loss: 17778.277344\n",
      "Train Epoch: 224 [222336/225000 (99%)] Loss: 17772.789062\n",
      "Train Epoch: 224 [224832/225000 (100%)] Loss: 17592.458984\n",
      "    epoch          : 224\n",
      "    loss           : 18085.09768957978\n",
      "    val_loss       : 17993.895134636463\n",
      "Train Epoch: 225 [192/225000 (0%)] Loss: 17841.589844\n",
      "Train Epoch: 225 [2688/225000 (1%)] Loss: 18993.832031\n",
      "Train Epoch: 225 [5184/225000 (2%)] Loss: 17949.490234\n",
      "Train Epoch: 225 [7680/225000 (3%)] Loss: 17808.914062\n",
      "Train Epoch: 225 [10176/225000 (5%)] Loss: 18274.570312\n",
      "Train Epoch: 225 [12672/225000 (6%)] Loss: 18051.187500\n",
      "Train Epoch: 225 [15168/225000 (7%)] Loss: 18448.269531\n",
      "Train Epoch: 225 [17664/225000 (8%)] Loss: 18197.578125\n",
      "Train Epoch: 225 [20160/225000 (9%)] Loss: 18184.007812\n",
      "Train Epoch: 225 [22656/225000 (10%)] Loss: 17752.675781\n",
      "Train Epoch: 225 [25152/225000 (11%)] Loss: 18259.757812\n",
      "Train Epoch: 225 [27648/225000 (12%)] Loss: 17968.812500\n",
      "Train Epoch: 225 [30144/225000 (13%)] Loss: 18182.269531\n",
      "Train Epoch: 225 [32640/225000 (15%)] Loss: 18435.498047\n",
      "Train Epoch: 225 [35136/225000 (16%)] Loss: 18153.929688\n",
      "Train Epoch: 225 [37632/225000 (17%)] Loss: 17931.226562\n",
      "Train Epoch: 225 [40128/225000 (18%)] Loss: 18072.441406\n",
      "Train Epoch: 225 [42624/225000 (19%)] Loss: 17931.183594\n",
      "Train Epoch: 225 [45120/225000 (20%)] Loss: 18289.560547\n",
      "Train Epoch: 225 [47616/225000 (21%)] Loss: 17982.777344\n",
      "Train Epoch: 225 [50112/225000 (22%)] Loss: 17656.332031\n",
      "Train Epoch: 225 [52608/225000 (23%)] Loss: 18052.058594\n",
      "Train Epoch: 225 [55104/225000 (24%)] Loss: 18045.845703\n",
      "Train Epoch: 225 [57600/225000 (26%)] Loss: 18105.515625\n",
      "Train Epoch: 225 [60096/225000 (27%)] Loss: 18105.855469\n",
      "Train Epoch: 225 [62592/225000 (28%)] Loss: 17747.621094\n",
      "Train Epoch: 225 [65088/225000 (29%)] Loss: 18129.429688\n",
      "Train Epoch: 225 [67584/225000 (30%)] Loss: 18164.773438\n",
      "Train Epoch: 225 [70080/225000 (31%)] Loss: 18293.750000\n",
      "Train Epoch: 225 [72576/225000 (32%)] Loss: 18051.648438\n",
      "Train Epoch: 225 [75072/225000 (33%)] Loss: 18160.025391\n",
      "Train Epoch: 225 [77568/225000 (34%)] Loss: 17545.347656\n",
      "Train Epoch: 225 [80064/225000 (36%)] Loss: 17797.966797\n",
      "Train Epoch: 225 [82560/225000 (37%)] Loss: 18104.898438\n",
      "Train Epoch: 225 [85056/225000 (38%)] Loss: 18488.958984\n",
      "Train Epoch: 225 [87552/225000 (39%)] Loss: 18419.351562\n",
      "Train Epoch: 225 [90048/225000 (40%)] Loss: 17777.996094\n",
      "Train Epoch: 225 [92544/225000 (41%)] Loss: 17973.988281\n",
      "Train Epoch: 225 [95040/225000 (42%)] Loss: 18165.865234\n",
      "Train Epoch: 225 [97536/225000 (43%)] Loss: 18503.410156\n",
      "Train Epoch: 225 [100032/225000 (44%)] Loss: 17995.527344\n",
      "Train Epoch: 225 [102528/225000 (46%)] Loss: 17958.572266\n",
      "Train Epoch: 225 [105024/225000 (47%)] Loss: 17885.031250\n",
      "Train Epoch: 225 [107520/225000 (48%)] Loss: 17790.472656\n",
      "Train Epoch: 225 [110016/225000 (49%)] Loss: 18381.884766\n",
      "Train Epoch: 225 [112512/225000 (50%)] Loss: 18120.558594\n",
      "Train Epoch: 225 [115008/225000 (51%)] Loss: 17817.787109\n",
      "Train Epoch: 225 [117504/225000 (52%)] Loss: 18352.617188\n",
      "Train Epoch: 225 [120000/225000 (53%)] Loss: 17958.515625\n",
      "Train Epoch: 225 [122496/225000 (54%)] Loss: 17466.812500\n",
      "Train Epoch: 225 [124992/225000 (56%)] Loss: 17892.080078\n",
      "Train Epoch: 225 [127488/225000 (57%)] Loss: 18315.664062\n",
      "Train Epoch: 225 [129984/225000 (58%)] Loss: 18170.097656\n",
      "Train Epoch: 225 [132480/225000 (59%)] Loss: 17555.757812\n",
      "Train Epoch: 225 [134976/225000 (60%)] Loss: 18183.117188\n",
      "Train Epoch: 225 [137472/225000 (61%)] Loss: 18361.636719\n",
      "Train Epoch: 225 [139968/225000 (62%)] Loss: 18271.433594\n",
      "Train Epoch: 225 [142464/225000 (63%)] Loss: 18000.980469\n",
      "Train Epoch: 225 [144960/225000 (64%)] Loss: 18477.175781\n",
      "Train Epoch: 225 [147456/225000 (66%)] Loss: 17772.765625\n",
      "Train Epoch: 225 [149952/225000 (67%)] Loss: 18144.226562\n",
      "Train Epoch: 225 [152448/225000 (68%)] Loss: 17696.789062\n",
      "Train Epoch: 225 [154944/225000 (69%)] Loss: 18415.720703\n",
      "Train Epoch: 225 [157440/225000 (70%)] Loss: 17774.734375\n",
      "Train Epoch: 225 [159936/225000 (71%)] Loss: 18220.421875\n",
      "Train Epoch: 225 [162432/225000 (72%)] Loss: 18472.648438\n",
      "Train Epoch: 225 [164928/225000 (73%)] Loss: 18058.824219\n",
      "Train Epoch: 225 [167424/225000 (74%)] Loss: 18022.398438\n",
      "Train Epoch: 225 [169920/225000 (76%)] Loss: 18014.621094\n",
      "Train Epoch: 225 [172416/225000 (77%)] Loss: 17447.816406\n",
      "Train Epoch: 225 [174912/225000 (78%)] Loss: 18225.750000\n",
      "Train Epoch: 225 [177408/225000 (79%)] Loss: 18390.441406\n",
      "Train Epoch: 225 [179904/225000 (80%)] Loss: 17877.042969\n",
      "Train Epoch: 225 [182400/225000 (81%)] Loss: 18334.437500\n",
      "Train Epoch: 225 [184896/225000 (82%)] Loss: 18442.707031\n",
      "Train Epoch: 225 [187392/225000 (83%)] Loss: 18518.642578\n",
      "Train Epoch: 225 [189888/225000 (84%)] Loss: 18218.703125\n",
      "Train Epoch: 225 [192384/225000 (86%)] Loss: 17879.890625\n",
      "Train Epoch: 225 [194880/225000 (87%)] Loss: 17901.035156\n",
      "Train Epoch: 225 [197376/225000 (88%)] Loss: 17929.125000\n",
      "Train Epoch: 225 [199872/225000 (89%)] Loss: 17871.033203\n",
      "Train Epoch: 225 [202368/225000 (90%)] Loss: 17636.832031\n",
      "Train Epoch: 225 [204864/225000 (91%)] Loss: 17625.550781\n",
      "Train Epoch: 225 [207360/225000 (92%)] Loss: 17606.367188\n",
      "Train Epoch: 225 [209856/225000 (93%)] Loss: 17896.289062\n",
      "Train Epoch: 225 [212352/225000 (94%)] Loss: 18064.734375\n",
      "Train Epoch: 225 [214848/225000 (95%)] Loss: 18224.015625\n",
      "Train Epoch: 225 [217344/225000 (97%)] Loss: 18197.011719\n",
      "Train Epoch: 225 [219840/225000 (98%)] Loss: 18156.039062\n",
      "Train Epoch: 225 [222336/225000 (99%)] Loss: 17991.117188\n",
      "Train Epoch: 225 [224832/225000 (100%)] Loss: 17996.710938\n",
      "    epoch          : 225\n",
      "    loss           : 18059.78869587244\n",
      "    val_loss       : 17956.122065058193\n",
      "Train Epoch: 226 [192/225000 (0%)] Loss: 17704.847656\n",
      "Train Epoch: 226 [2688/225000 (1%)] Loss: 18051.603516\n",
      "Train Epoch: 226 [5184/225000 (2%)] Loss: 18052.560547\n",
      "Train Epoch: 226 [7680/225000 (3%)] Loss: 18176.792969\n",
      "Train Epoch: 226 [10176/225000 (5%)] Loss: 18403.593750\n",
      "Train Epoch: 226 [12672/225000 (6%)] Loss: 18058.685547\n",
      "Train Epoch: 226 [15168/225000 (7%)] Loss: 18640.554688\n",
      "Train Epoch: 226 [17664/225000 (8%)] Loss: 18271.171875\n",
      "Train Epoch: 226 [20160/225000 (9%)] Loss: 18200.376953\n",
      "Train Epoch: 226 [22656/225000 (10%)] Loss: 17408.773438\n",
      "Train Epoch: 226 [25152/225000 (11%)] Loss: 18359.464844\n",
      "Train Epoch: 226 [27648/225000 (12%)] Loss: 17843.601562\n",
      "Train Epoch: 226 [30144/225000 (13%)] Loss: 18131.976562\n",
      "Train Epoch: 226 [32640/225000 (15%)] Loss: 18122.310547\n",
      "Train Epoch: 226 [35136/225000 (16%)] Loss: 18371.550781\n",
      "Train Epoch: 226 [37632/225000 (17%)] Loss: 18267.917969\n",
      "Train Epoch: 226 [40128/225000 (18%)] Loss: 18232.562500\n",
      "Train Epoch: 226 [42624/225000 (19%)] Loss: 17826.695312\n",
      "Train Epoch: 226 [45120/225000 (20%)] Loss: 18109.300781\n",
      "Train Epoch: 226 [47616/225000 (21%)] Loss: 17681.068359\n",
      "Train Epoch: 226 [50112/225000 (22%)] Loss: 17823.328125\n",
      "Train Epoch: 226 [52608/225000 (23%)] Loss: 18376.875000\n",
      "Train Epoch: 226 [55104/225000 (24%)] Loss: 17828.347656\n",
      "Train Epoch: 226 [57600/225000 (26%)] Loss: 17539.390625\n",
      "Train Epoch: 226 [60096/225000 (27%)] Loss: 18168.742188\n",
      "Train Epoch: 226 [62592/225000 (28%)] Loss: 17967.902344\n",
      "Train Epoch: 226 [65088/225000 (29%)] Loss: 17961.681641\n",
      "Train Epoch: 226 [67584/225000 (30%)] Loss: 17753.408203\n",
      "Train Epoch: 226 [70080/225000 (31%)] Loss: 17814.773438\n",
      "Train Epoch: 226 [72576/225000 (32%)] Loss: 18262.857422\n",
      "Train Epoch: 226 [75072/225000 (33%)] Loss: 18204.039062\n",
      "Train Epoch: 226 [77568/225000 (34%)] Loss: 18309.820312\n",
      "Train Epoch: 226 [80064/225000 (36%)] Loss: 20802.761719\n",
      "Train Epoch: 226 [82560/225000 (37%)] Loss: 18025.265625\n",
      "Train Epoch: 226 [85056/225000 (38%)] Loss: 17593.292969\n",
      "Train Epoch: 226 [87552/225000 (39%)] Loss: 18285.750000\n",
      "Train Epoch: 226 [90048/225000 (40%)] Loss: 18359.806641\n",
      "Train Epoch: 226 [92544/225000 (41%)] Loss: 18359.144531\n",
      "Train Epoch: 226 [95040/225000 (42%)] Loss: 18045.191406\n",
      "Train Epoch: 226 [97536/225000 (43%)] Loss: 17745.570312\n",
      "Train Epoch: 226 [100032/225000 (44%)] Loss: 18451.828125\n",
      "Train Epoch: 226 [102528/225000 (46%)] Loss: 18308.785156\n",
      "Train Epoch: 226 [105024/225000 (47%)] Loss: 17972.066406\n",
      "Train Epoch: 226 [107520/225000 (48%)] Loss: 17798.957031\n",
      "Train Epoch: 226 [110016/225000 (49%)] Loss: 18424.687500\n",
      "Train Epoch: 226 [112512/225000 (50%)] Loss: 18019.927734\n",
      "Train Epoch: 226 [115008/225000 (51%)] Loss: 18196.873047\n",
      "Train Epoch: 226 [117504/225000 (52%)] Loss: 18080.785156\n",
      "Train Epoch: 226 [120000/225000 (53%)] Loss: 17578.035156\n",
      "Train Epoch: 226 [122496/225000 (54%)] Loss: 18350.042969\n",
      "Train Epoch: 226 [124992/225000 (56%)] Loss: 17574.789062\n",
      "Train Epoch: 226 [127488/225000 (57%)] Loss: 18351.562500\n",
      "Train Epoch: 226 [129984/225000 (58%)] Loss: 17840.195312\n",
      "Train Epoch: 226 [132480/225000 (59%)] Loss: 18230.441406\n",
      "Train Epoch: 226 [134976/225000 (60%)] Loss: 18068.480469\n",
      "Train Epoch: 226 [137472/225000 (61%)] Loss: 18136.539062\n",
      "Train Epoch: 226 [139968/225000 (62%)] Loss: 17779.714844\n",
      "Train Epoch: 226 [142464/225000 (63%)] Loss: 18077.234375\n",
      "Train Epoch: 226 [144960/225000 (64%)] Loss: 18192.980469\n",
      "Train Epoch: 226 [147456/225000 (66%)] Loss: 18255.988281\n",
      "Train Epoch: 226 [149952/225000 (67%)] Loss: 17972.167969\n",
      "Train Epoch: 226 [152448/225000 (68%)] Loss: 18132.062500\n",
      "Train Epoch: 226 [154944/225000 (69%)] Loss: 17710.267578\n",
      "Train Epoch: 226 [157440/225000 (70%)] Loss: 18445.097656\n",
      "Train Epoch: 226 [159936/225000 (71%)] Loss: 17609.238281\n",
      "Train Epoch: 226 [162432/225000 (72%)] Loss: 18200.798828\n",
      "Train Epoch: 226 [164928/225000 (73%)] Loss: 17712.175781\n",
      "Train Epoch: 226 [167424/225000 (74%)] Loss: 17893.941406\n",
      "Train Epoch: 226 [169920/225000 (76%)] Loss: 18472.921875\n",
      "Train Epoch: 226 [172416/225000 (77%)] Loss: 18018.726562\n",
      "Train Epoch: 226 [174912/225000 (78%)] Loss: 18487.902344\n",
      "Train Epoch: 226 [177408/225000 (79%)] Loss: 18503.890625\n",
      "Train Epoch: 226 [179904/225000 (80%)] Loss: 18031.132812\n",
      "Train Epoch: 226 [182400/225000 (81%)] Loss: 17891.328125\n",
      "Train Epoch: 226 [184896/225000 (82%)] Loss: 17939.777344\n",
      "Train Epoch: 226 [187392/225000 (83%)] Loss: 18099.441406\n",
      "Train Epoch: 226 [189888/225000 (84%)] Loss: 18141.023438\n",
      "Train Epoch: 226 [192384/225000 (86%)] Loss: 17603.015625\n",
      "Train Epoch: 226 [194880/225000 (87%)] Loss: 18463.296875\n",
      "Train Epoch: 226 [197376/225000 (88%)] Loss: 17943.447266\n",
      "Train Epoch: 226 [199872/225000 (89%)] Loss: 18484.593750\n",
      "Train Epoch: 226 [202368/225000 (90%)] Loss: 18204.863281\n",
      "Train Epoch: 226 [204864/225000 (91%)] Loss: 18071.619141\n",
      "Train Epoch: 226 [207360/225000 (92%)] Loss: 18097.001953\n",
      "Train Epoch: 226 [209856/225000 (93%)] Loss: 17464.304688\n",
      "Train Epoch: 226 [212352/225000 (94%)] Loss: 17560.544922\n",
      "Train Epoch: 226 [214848/225000 (95%)] Loss: 17668.257812\n",
      "Train Epoch: 226 [217344/225000 (97%)] Loss: 17752.910156\n",
      "Train Epoch: 226 [219840/225000 (98%)] Loss: 18087.687500\n",
      "Train Epoch: 226 [222336/225000 (99%)] Loss: 17693.488281\n",
      "Train Epoch: 226 [224832/225000 (100%)] Loss: 17990.884766\n",
      "    epoch          : 226\n",
      "    loss           : 18051.178845089857\n",
      "    val_loss       : 17942.896535895252\n",
      "Train Epoch: 227 [192/225000 (0%)] Loss: 17965.734375\n",
      "Train Epoch: 227 [2688/225000 (1%)] Loss: 18149.875000\n",
      "Train Epoch: 227 [5184/225000 (2%)] Loss: 18290.886719\n",
      "Train Epoch: 227 [7680/225000 (3%)] Loss: 18511.539062\n",
      "Train Epoch: 227 [10176/225000 (5%)] Loss: 18333.925781\n",
      "Train Epoch: 227 [12672/225000 (6%)] Loss: 17935.183594\n",
      "Train Epoch: 227 [15168/225000 (7%)] Loss: 18098.996094\n",
      "Train Epoch: 227 [17664/225000 (8%)] Loss: 17727.164062\n",
      "Train Epoch: 227 [20160/225000 (9%)] Loss: 18232.033203\n",
      "Train Epoch: 227 [22656/225000 (10%)] Loss: 18071.861328\n",
      "Train Epoch: 227 [25152/225000 (11%)] Loss: 18063.978516\n",
      "Train Epoch: 227 [27648/225000 (12%)] Loss: 18024.890625\n",
      "Train Epoch: 227 [30144/225000 (13%)] Loss: 18032.476562\n",
      "Train Epoch: 227 [32640/225000 (15%)] Loss: 17971.390625\n",
      "Train Epoch: 227 [35136/225000 (16%)] Loss: 18076.306641\n",
      "Train Epoch: 227 [37632/225000 (17%)] Loss: 17732.437500\n",
      "Train Epoch: 227 [40128/225000 (18%)] Loss: 18074.089844\n",
      "Train Epoch: 227 [42624/225000 (19%)] Loss: 17693.738281\n",
      "Train Epoch: 227 [45120/225000 (20%)] Loss: 18435.736328\n",
      "Train Epoch: 227 [47616/225000 (21%)] Loss: 18103.960938\n",
      "Train Epoch: 227 [50112/225000 (22%)] Loss: 18513.886719\n",
      "Train Epoch: 227 [52608/225000 (23%)] Loss: 18290.025391\n",
      "Train Epoch: 227 [55104/225000 (24%)] Loss: 18029.828125\n",
      "Train Epoch: 227 [57600/225000 (26%)] Loss: 17468.894531\n",
      "Train Epoch: 227 [60096/225000 (27%)] Loss: 18154.214844\n",
      "Train Epoch: 227 [62592/225000 (28%)] Loss: 18208.917969\n",
      "Train Epoch: 227 [65088/225000 (29%)] Loss: 17642.574219\n",
      "Train Epoch: 227 [67584/225000 (30%)] Loss: 17371.369141\n",
      "Train Epoch: 227 [70080/225000 (31%)] Loss: 18559.140625\n",
      "Train Epoch: 227 [72576/225000 (32%)] Loss: 18432.664062\n",
      "Train Epoch: 227 [75072/225000 (33%)] Loss: 18071.603516\n",
      "Train Epoch: 227 [77568/225000 (34%)] Loss: 18273.857422\n",
      "Train Epoch: 227 [80064/225000 (36%)] Loss: 18285.222656\n",
      "Train Epoch: 227 [82560/225000 (37%)] Loss: 18257.789062\n",
      "Train Epoch: 227 [85056/225000 (38%)] Loss: 17594.562500\n",
      "Train Epoch: 227 [87552/225000 (39%)] Loss: 17565.500000\n",
      "Train Epoch: 227 [90048/225000 (40%)] Loss: 18012.308594\n",
      "Train Epoch: 227 [92544/225000 (41%)] Loss: 18298.832031\n",
      "Train Epoch: 227 [95040/225000 (42%)] Loss: 17859.986328\n",
      "Train Epoch: 227 [97536/225000 (43%)] Loss: 18136.917969\n",
      "Train Epoch: 227 [100032/225000 (44%)] Loss: 18383.144531\n",
      "Train Epoch: 227 [102528/225000 (46%)] Loss: 18189.410156\n",
      "Train Epoch: 227 [105024/225000 (47%)] Loss: 17878.460938\n",
      "Train Epoch: 227 [107520/225000 (48%)] Loss: 17790.867188\n",
      "Train Epoch: 227 [110016/225000 (49%)] Loss: 17924.042969\n",
      "Train Epoch: 227 [112512/225000 (50%)] Loss: 18005.910156\n",
      "Train Epoch: 227 [115008/225000 (51%)] Loss: 18076.832031\n",
      "Train Epoch: 227 [117504/225000 (52%)] Loss: 17918.769531\n",
      "Train Epoch: 227 [120000/225000 (53%)] Loss: 18191.128906\n",
      "Train Epoch: 227 [122496/225000 (54%)] Loss: 17537.984375\n",
      "Train Epoch: 227 [124992/225000 (56%)] Loss: 18036.837891\n",
      "Train Epoch: 227 [127488/225000 (57%)] Loss: 17864.445312\n",
      "Train Epoch: 227 [129984/225000 (58%)] Loss: 17906.859375\n",
      "Train Epoch: 227 [132480/225000 (59%)] Loss: 17860.888672\n",
      "Train Epoch: 227 [134976/225000 (60%)] Loss: 17930.201172\n",
      "Train Epoch: 227 [137472/225000 (61%)] Loss: 18036.722656\n",
      "Train Epoch: 227 [139968/225000 (62%)] Loss: 17751.351562\n",
      "Train Epoch: 227 [142464/225000 (63%)] Loss: 17652.750000\n",
      "Train Epoch: 227 [144960/225000 (64%)] Loss: 18099.068359\n",
      "Train Epoch: 227 [147456/225000 (66%)] Loss: 18015.025391\n",
      "Train Epoch: 227 [149952/225000 (67%)] Loss: 18092.173828\n",
      "Train Epoch: 227 [152448/225000 (68%)] Loss: 18050.148438\n",
      "Train Epoch: 227 [154944/225000 (69%)] Loss: 18189.960938\n",
      "Train Epoch: 227 [157440/225000 (70%)] Loss: 17813.214844\n",
      "Train Epoch: 227 [159936/225000 (71%)] Loss: 17846.375000\n",
      "Train Epoch: 227 [162432/225000 (72%)] Loss: 17755.449219\n",
      "Train Epoch: 227 [164928/225000 (73%)] Loss: 17455.085938\n",
      "Train Epoch: 227 [167424/225000 (74%)] Loss: 17679.328125\n",
      "Train Epoch: 227 [169920/225000 (76%)] Loss: 18252.574219\n",
      "Train Epoch: 227 [172416/225000 (77%)] Loss: 18046.302734\n",
      "Train Epoch: 227 [174912/225000 (78%)] Loss: 18124.738281\n",
      "Train Epoch: 227 [177408/225000 (79%)] Loss: 18257.042969\n",
      "Train Epoch: 227 [179904/225000 (80%)] Loss: 18051.109375\n",
      "Train Epoch: 227 [182400/225000 (81%)] Loss: 17773.716797\n",
      "Train Epoch: 227 [184896/225000 (82%)] Loss: 18076.808594\n",
      "Train Epoch: 227 [187392/225000 (83%)] Loss: 18007.705078\n",
      "Train Epoch: 227 [189888/225000 (84%)] Loss: 17605.416016\n",
      "Train Epoch: 227 [192384/225000 (86%)] Loss: 18180.541016\n",
      "Train Epoch: 227 [194880/225000 (87%)] Loss: 18218.773438\n",
      "Train Epoch: 227 [197376/225000 (88%)] Loss: 18354.039062\n",
      "Train Epoch: 227 [199872/225000 (89%)] Loss: 17948.244141\n",
      "Train Epoch: 227 [202368/225000 (90%)] Loss: 18191.712891\n",
      "Train Epoch: 227 [204864/225000 (91%)] Loss: 17906.902344\n",
      "Train Epoch: 227 [207360/225000 (92%)] Loss: 17994.695312\n",
      "Train Epoch: 227 [209856/225000 (93%)] Loss: 18199.003906\n",
      "Train Epoch: 227 [212352/225000 (94%)] Loss: 17982.845703\n",
      "Train Epoch: 227 [214848/225000 (95%)] Loss: 18018.507812\n",
      "Train Epoch: 227 [217344/225000 (97%)] Loss: 17977.011719\n",
      "Train Epoch: 227 [219840/225000 (98%)] Loss: 18134.419922\n",
      "Train Epoch: 227 [222336/225000 (99%)] Loss: 17816.869141\n",
      "Train Epoch: 227 [224832/225000 (100%)] Loss: 18038.396484\n",
      "    epoch          : 227\n",
      "    loss           : 18025.707929487522\n",
      "    val_loss       : 17939.124574163943\n",
      "Train Epoch: 228 [192/225000 (0%)] Loss: 17666.218750\n",
      "Train Epoch: 228 [2688/225000 (1%)] Loss: 17759.628906\n",
      "Train Epoch: 228 [5184/225000 (2%)] Loss: 18202.867188\n",
      "Train Epoch: 228 [7680/225000 (3%)] Loss: 18236.062500\n",
      "Train Epoch: 228 [10176/225000 (5%)] Loss: 17672.507812\n",
      "Train Epoch: 228 [12672/225000 (6%)] Loss: 17909.503906\n",
      "Train Epoch: 228 [15168/225000 (7%)] Loss: 18343.064453\n",
      "Train Epoch: 228 [17664/225000 (8%)] Loss: 17843.824219\n",
      "Train Epoch: 228 [20160/225000 (9%)] Loss: 18336.039062\n",
      "Train Epoch: 228 [22656/225000 (10%)] Loss: 18606.798828\n",
      "Train Epoch: 228 [25152/225000 (11%)] Loss: 17678.531250\n",
      "Train Epoch: 228 [27648/225000 (12%)] Loss: 21402.500000\n",
      "Train Epoch: 228 [30144/225000 (13%)] Loss: 17372.832031\n",
      "Train Epoch: 228 [32640/225000 (15%)] Loss: 17534.898438\n",
      "Train Epoch: 228 [35136/225000 (16%)] Loss: 18278.873047\n",
      "Train Epoch: 228 [37632/225000 (17%)] Loss: 18157.703125\n",
      "Train Epoch: 228 [40128/225000 (18%)] Loss: 18234.960938\n",
      "Train Epoch: 228 [42624/225000 (19%)] Loss: 17873.134766\n",
      "Train Epoch: 228 [45120/225000 (20%)] Loss: 17897.058594\n",
      "Train Epoch: 228 [47616/225000 (21%)] Loss: 18127.542969\n",
      "Train Epoch: 228 [50112/225000 (22%)] Loss: 17777.511719\n",
      "Train Epoch: 228 [52608/225000 (23%)] Loss: 17623.246094\n",
      "Train Epoch: 228 [55104/225000 (24%)] Loss: 18535.660156\n",
      "Train Epoch: 228 [57600/225000 (26%)] Loss: 17592.097656\n",
      "Train Epoch: 228 [60096/225000 (27%)] Loss: 18655.921875\n",
      "Train Epoch: 228 [62592/225000 (28%)] Loss: 18012.982422\n",
      "Train Epoch: 228 [65088/225000 (29%)] Loss: 17943.953125\n",
      "Train Epoch: 228 [67584/225000 (30%)] Loss: 17913.179688\n",
      "Train Epoch: 228 [70080/225000 (31%)] Loss: 18088.425781\n",
      "Train Epoch: 228 [72576/225000 (32%)] Loss: 17981.636719\n",
      "Train Epoch: 228 [75072/225000 (33%)] Loss: 17874.535156\n",
      "Train Epoch: 228 [77568/225000 (34%)] Loss: 18438.197266\n",
      "Train Epoch: 228 [80064/225000 (36%)] Loss: 18325.300781\n",
      "Train Epoch: 228 [82560/225000 (37%)] Loss: 17909.738281\n",
      "Train Epoch: 228 [85056/225000 (38%)] Loss: 17679.001953\n",
      "Train Epoch: 228 [87552/225000 (39%)] Loss: 17883.855469\n",
      "Train Epoch: 228 [90048/225000 (40%)] Loss: 18273.343750\n",
      "Train Epoch: 228 [92544/225000 (41%)] Loss: 18118.673828\n",
      "Train Epoch: 228 [95040/225000 (42%)] Loss: 18213.121094\n",
      "Train Epoch: 228 [97536/225000 (43%)] Loss: 18048.632812\n",
      "Train Epoch: 228 [100032/225000 (44%)] Loss: 18082.562500\n",
      "Train Epoch: 228 [102528/225000 (46%)] Loss: 18296.591797\n",
      "Train Epoch: 228 [105024/225000 (47%)] Loss: 18030.400391\n",
      "Train Epoch: 228 [107520/225000 (48%)] Loss: 17754.775391\n",
      "Train Epoch: 228 [110016/225000 (49%)] Loss: 18097.335938\n",
      "Train Epoch: 228 [112512/225000 (50%)] Loss: 18092.449219\n",
      "Train Epoch: 228 [115008/225000 (51%)] Loss: 18326.097656\n",
      "Train Epoch: 228 [117504/225000 (52%)] Loss: 17853.894531\n",
      "Train Epoch: 228 [120000/225000 (53%)] Loss: 17826.863281\n",
      "Train Epoch: 228 [122496/225000 (54%)] Loss: 18057.837891\n",
      "Train Epoch: 228 [124992/225000 (56%)] Loss: 18467.742188\n",
      "Train Epoch: 228 [127488/225000 (57%)] Loss: 17944.136719\n",
      "Train Epoch: 228 [129984/225000 (58%)] Loss: 18419.226562\n",
      "Train Epoch: 228 [132480/225000 (59%)] Loss: 18227.011719\n",
      "Train Epoch: 228 [134976/225000 (60%)] Loss: 18254.695312\n",
      "Train Epoch: 228 [137472/225000 (61%)] Loss: 17573.771484\n",
      "Train Epoch: 228 [139968/225000 (62%)] Loss: 17569.224609\n",
      "Train Epoch: 228 [142464/225000 (63%)] Loss: 17510.496094\n",
      "Train Epoch: 228 [144960/225000 (64%)] Loss: 17862.144531\n",
      "Train Epoch: 228 [147456/225000 (66%)] Loss: 17715.984375\n",
      "Train Epoch: 228 [149952/225000 (67%)] Loss: 18174.755859\n",
      "Train Epoch: 228 [152448/225000 (68%)] Loss: 18247.099609\n",
      "Train Epoch: 228 [154944/225000 (69%)] Loss: 18072.003906\n",
      "Train Epoch: 228 [157440/225000 (70%)] Loss: 18056.531250\n",
      "Train Epoch: 228 [159936/225000 (71%)] Loss: 17751.835938\n",
      "Train Epoch: 228 [162432/225000 (72%)] Loss: 17982.796875\n",
      "Train Epoch: 228 [164928/225000 (73%)] Loss: 17782.634766\n",
      "Train Epoch: 228 [167424/225000 (74%)] Loss: 17809.355469\n",
      "Train Epoch: 228 [169920/225000 (76%)] Loss: 17751.226562\n",
      "Train Epoch: 228 [172416/225000 (77%)] Loss: 18179.158203\n",
      "Train Epoch: 228 [174912/225000 (78%)] Loss: 18146.937500\n",
      "Train Epoch: 228 [177408/225000 (79%)] Loss: 17908.810547\n",
      "Train Epoch: 228 [179904/225000 (80%)] Loss: 17703.812500\n",
      "Train Epoch: 228 [182400/225000 (81%)] Loss: 18200.652344\n",
      "Train Epoch: 228 [184896/225000 (82%)] Loss: 18093.343750\n",
      "Train Epoch: 228 [187392/225000 (83%)] Loss: 18020.740234\n",
      "Train Epoch: 228 [189888/225000 (84%)] Loss: 17593.648438\n",
      "Train Epoch: 228 [192384/225000 (86%)] Loss: 18051.281250\n",
      "Train Epoch: 228 [194880/225000 (87%)] Loss: 17762.199219\n",
      "Train Epoch: 228 [197376/225000 (88%)] Loss: 17627.955078\n",
      "Train Epoch: 228 [199872/225000 (89%)] Loss: 17823.609375\n",
      "Train Epoch: 228 [202368/225000 (90%)] Loss: 18107.111328\n",
      "Train Epoch: 228 [204864/225000 (91%)] Loss: 18637.929688\n",
      "Train Epoch: 228 [207360/225000 (92%)] Loss: 18321.175781\n",
      "Train Epoch: 228 [209856/225000 (93%)] Loss: 18276.154297\n",
      "Train Epoch: 228 [212352/225000 (94%)] Loss: 18406.183594\n",
      "Train Epoch: 228 [214848/225000 (95%)] Loss: 17785.789062\n",
      "Train Epoch: 228 [217344/225000 (97%)] Loss: 17490.222656\n",
      "Train Epoch: 228 [219840/225000 (98%)] Loss: 18159.253906\n",
      "Train Epoch: 228 [222336/225000 (99%)] Loss: 18038.050781\n",
      "Train Epoch: 228 [224832/225000 (100%)] Loss: 18314.953125\n",
      "    epoch          : 228\n",
      "    loss           : 18027.755897704246\n",
      "    val_loss       : 17961.541110539256\n",
      "Train Epoch: 229 [192/225000 (0%)] Loss: 17616.794922\n",
      "Train Epoch: 229 [2688/225000 (1%)] Loss: 18146.132812\n",
      "Train Epoch: 229 [5184/225000 (2%)] Loss: 17933.339844\n",
      "Train Epoch: 229 [7680/225000 (3%)] Loss: 17851.160156\n",
      "Train Epoch: 229 [10176/225000 (5%)] Loss: 17907.074219\n",
      "Train Epoch: 229 [12672/225000 (6%)] Loss: 17868.695312\n",
      "Train Epoch: 229 [15168/225000 (7%)] Loss: 18199.371094\n",
      "Train Epoch: 229 [17664/225000 (8%)] Loss: 17739.097656\n",
      "Train Epoch: 229 [20160/225000 (9%)] Loss: 17991.623047\n",
      "Train Epoch: 229 [22656/225000 (10%)] Loss: 18150.816406\n",
      "Train Epoch: 229 [25152/225000 (11%)] Loss: 18309.658203\n",
      "Train Epoch: 229 [27648/225000 (12%)] Loss: 17823.478516\n",
      "Train Epoch: 229 [30144/225000 (13%)] Loss: 17726.820312\n",
      "Train Epoch: 229 [32640/225000 (15%)] Loss: 18157.664062\n",
      "Train Epoch: 229 [35136/225000 (16%)] Loss: 18112.236328\n",
      "Train Epoch: 229 [37632/225000 (17%)] Loss: 18013.457031\n",
      "Train Epoch: 229 [40128/225000 (18%)] Loss: 17780.035156\n",
      "Train Epoch: 229 [42624/225000 (19%)] Loss: 18483.382812\n",
      "Train Epoch: 229 [45120/225000 (20%)] Loss: 18201.375000\n",
      "Train Epoch: 229 [47616/225000 (21%)] Loss: 17995.503906\n",
      "Train Epoch: 229 [50112/225000 (22%)] Loss: 18171.238281\n",
      "Train Epoch: 229 [52608/225000 (23%)] Loss: 18060.273438\n",
      "Train Epoch: 229 [55104/225000 (24%)] Loss: 17784.253906\n",
      "Train Epoch: 229 [57600/225000 (26%)] Loss: 17828.867188\n",
      "Train Epoch: 229 [60096/225000 (27%)] Loss: 18376.521484\n",
      "Train Epoch: 229 [62592/225000 (28%)] Loss: 18237.800781\n",
      "Train Epoch: 229 [65088/225000 (29%)] Loss: 17771.296875\n",
      "Train Epoch: 229 [67584/225000 (30%)] Loss: 17948.363281\n",
      "Train Epoch: 229 [70080/225000 (31%)] Loss: 17697.250000\n",
      "Train Epoch: 229 [72576/225000 (32%)] Loss: 17996.058594\n",
      "Train Epoch: 229 [75072/225000 (33%)] Loss: 18043.560547\n",
      "Train Epoch: 229 [77568/225000 (34%)] Loss: 18311.691406\n",
      "Train Epoch: 229 [80064/225000 (36%)] Loss: 17927.216797\n",
      "Train Epoch: 229 [82560/225000 (37%)] Loss: 18222.316406\n",
      "Train Epoch: 229 [85056/225000 (38%)] Loss: 17600.261719\n",
      "Train Epoch: 229 [87552/225000 (39%)] Loss: 17925.240234\n",
      "Train Epoch: 229 [90048/225000 (40%)] Loss: 17533.957031\n",
      "Train Epoch: 229 [92544/225000 (41%)] Loss: 18140.519531\n",
      "Train Epoch: 229 [95040/225000 (42%)] Loss: 17649.230469\n",
      "Train Epoch: 229 [97536/225000 (43%)] Loss: 17527.669922\n",
      "Train Epoch: 229 [100032/225000 (44%)] Loss: 18144.794922\n",
      "Train Epoch: 229 [102528/225000 (46%)] Loss: 18108.929688\n",
      "Train Epoch: 229 [105024/225000 (47%)] Loss: 17715.894531\n",
      "Train Epoch: 229 [107520/225000 (48%)] Loss: 18079.222656\n",
      "Train Epoch: 229 [110016/225000 (49%)] Loss: 18082.820312\n",
      "Train Epoch: 229 [112512/225000 (50%)] Loss: 18061.656250\n",
      "Train Epoch: 229 [115008/225000 (51%)] Loss: 17617.722656\n",
      "Train Epoch: 229 [117504/225000 (52%)] Loss: 17984.361328\n",
      "Train Epoch: 229 [120000/225000 (53%)] Loss: 18132.179688\n",
      "Train Epoch: 229 [122496/225000 (54%)] Loss: 17630.416016\n",
      "Train Epoch: 229 [124992/225000 (56%)] Loss: 18162.400391\n",
      "Train Epoch: 229 [127488/225000 (57%)] Loss: 17982.347656\n",
      "Train Epoch: 229 [129984/225000 (58%)] Loss: 17795.298828\n",
      "Train Epoch: 229 [132480/225000 (59%)] Loss: 17846.675781\n",
      "Train Epoch: 229 [134976/225000 (60%)] Loss: 18153.074219\n",
      "Train Epoch: 229 [137472/225000 (61%)] Loss: 18203.376953\n",
      "Train Epoch: 229 [139968/225000 (62%)] Loss: 17632.156250\n",
      "Train Epoch: 229 [142464/225000 (63%)] Loss: 18375.699219\n",
      "Train Epoch: 229 [144960/225000 (64%)] Loss: 18256.673828\n",
      "Train Epoch: 229 [147456/225000 (66%)] Loss: 18367.130859\n",
      "Train Epoch: 229 [149952/225000 (67%)] Loss: 18577.603516\n",
      "Train Epoch: 229 [152448/225000 (68%)] Loss: 18141.042969\n",
      "Train Epoch: 229 [154944/225000 (69%)] Loss: 17467.621094\n",
      "Train Epoch: 229 [157440/225000 (70%)] Loss: 17800.687500\n",
      "Train Epoch: 229 [159936/225000 (71%)] Loss: 18216.158203\n",
      "Train Epoch: 229 [162432/225000 (72%)] Loss: 17776.865234\n",
      "Train Epoch: 229 [164928/225000 (73%)] Loss: 18355.691406\n",
      "Train Epoch: 229 [167424/225000 (74%)] Loss: 18462.605469\n",
      "Train Epoch: 229 [169920/225000 (76%)] Loss: 18180.574219\n",
      "Train Epoch: 229 [172416/225000 (77%)] Loss: 17767.570312\n",
      "Train Epoch: 229 [174912/225000 (78%)] Loss: 18023.041016\n",
      "Train Epoch: 229 [177408/225000 (79%)] Loss: 18166.761719\n",
      "Train Epoch: 229 [179904/225000 (80%)] Loss: 18082.453125\n",
      "Train Epoch: 229 [182400/225000 (81%)] Loss: 18145.890625\n",
      "Train Epoch: 229 [184896/225000 (82%)] Loss: 17853.988281\n",
      "Train Epoch: 229 [187392/225000 (83%)] Loss: 17654.591797\n",
      "Train Epoch: 229 [189888/225000 (84%)] Loss: 17956.531250\n",
      "Train Epoch: 229 [192384/225000 (86%)] Loss: 18137.820312\n",
      "Train Epoch: 229 [194880/225000 (87%)] Loss: 17934.058594\n",
      "Train Epoch: 229 [197376/225000 (88%)] Loss: 18314.345703\n",
      "Train Epoch: 229 [199872/225000 (89%)] Loss: 18301.996094\n",
      "Train Epoch: 229 [202368/225000 (90%)] Loss: 18003.363281\n",
      "Train Epoch: 229 [204864/225000 (91%)] Loss: 18418.658203\n",
      "Train Epoch: 229 [207360/225000 (92%)] Loss: 17774.398438\n",
      "Train Epoch: 229 [209856/225000 (93%)] Loss: 17514.695312\n",
      "Train Epoch: 229 [212352/225000 (94%)] Loss: 18467.273438\n",
      "Train Epoch: 229 [214848/225000 (95%)] Loss: 18040.089844\n",
      "Train Epoch: 229 [217344/225000 (97%)] Loss: 18388.308594\n",
      "Train Epoch: 229 [219840/225000 (98%)] Loss: 20994.167969\n",
      "Train Epoch: 229 [222336/225000 (99%)] Loss: 17782.308594\n",
      "Train Epoch: 229 [224832/225000 (100%)] Loss: 18226.910156\n",
      "    epoch          : 229\n",
      "    loss           : 18002.208511925393\n",
      "    val_loss       : 17902.238543508618\n",
      "Train Epoch: 230 [192/225000 (0%)] Loss: 17601.195312\n",
      "Train Epoch: 230 [2688/225000 (1%)] Loss: 18022.066406\n",
      "Train Epoch: 230 [5184/225000 (2%)] Loss: 17903.660156\n",
      "Train Epoch: 230 [7680/225000 (3%)] Loss: 18051.666016\n",
      "Train Epoch: 230 [10176/225000 (5%)] Loss: 18005.523438\n",
      "Train Epoch: 230 [12672/225000 (6%)] Loss: 18132.556641\n",
      "Train Epoch: 230 [15168/225000 (7%)] Loss: 18018.437500\n",
      "Train Epoch: 230 [17664/225000 (8%)] Loss: 17982.593750\n",
      "Train Epoch: 230 [20160/225000 (9%)] Loss: 18298.347656\n",
      "Train Epoch: 230 [22656/225000 (10%)] Loss: 18292.275391\n",
      "Train Epoch: 230 [25152/225000 (11%)] Loss: 17971.947266\n",
      "Train Epoch: 230 [27648/225000 (12%)] Loss: 18400.857422\n",
      "Train Epoch: 230 [30144/225000 (13%)] Loss: 18383.347656\n",
      "Train Epoch: 230 [32640/225000 (15%)] Loss: 17814.917969\n",
      "Train Epoch: 230 [35136/225000 (16%)] Loss: 18388.570312\n",
      "Train Epoch: 230 [37632/225000 (17%)] Loss: 18101.773438\n",
      "Train Epoch: 230 [40128/225000 (18%)] Loss: 18075.398438\n",
      "Train Epoch: 230 [42624/225000 (19%)] Loss: 17485.156250\n",
      "Train Epoch: 230 [45120/225000 (20%)] Loss: 17557.585938\n",
      "Train Epoch: 230 [47616/225000 (21%)] Loss: 18082.185547\n",
      "Train Epoch: 230 [50112/225000 (22%)] Loss: 17962.142578\n",
      "Train Epoch: 230 [52608/225000 (23%)] Loss: 18018.343750\n",
      "Train Epoch: 230 [55104/225000 (24%)] Loss: 17841.056641\n",
      "Train Epoch: 230 [57600/225000 (26%)] Loss: 18518.617188\n",
      "Train Epoch: 230 [60096/225000 (27%)] Loss: 17782.183594\n",
      "Train Epoch: 230 [62592/225000 (28%)] Loss: 18045.394531\n",
      "Train Epoch: 230 [65088/225000 (29%)] Loss: 18069.796875\n",
      "Train Epoch: 230 [67584/225000 (30%)] Loss: 17840.226562\n",
      "Train Epoch: 230 [70080/225000 (31%)] Loss: 18135.117188\n",
      "Train Epoch: 230 [72576/225000 (32%)] Loss: 17419.826172\n",
      "Train Epoch: 230 [75072/225000 (33%)] Loss: 17987.714844\n",
      "Train Epoch: 230 [77568/225000 (34%)] Loss: 17618.150391\n",
      "Train Epoch: 230 [80064/225000 (36%)] Loss: 17752.363281\n",
      "Train Epoch: 230 [82560/225000 (37%)] Loss: 18011.242188\n",
      "Train Epoch: 230 [85056/225000 (38%)] Loss: 17992.826172\n",
      "Train Epoch: 230 [87552/225000 (39%)] Loss: 17565.771484\n",
      "Train Epoch: 230 [90048/225000 (40%)] Loss: 17437.742188\n",
      "Train Epoch: 230 [92544/225000 (41%)] Loss: 18078.417969\n",
      "Train Epoch: 230 [95040/225000 (42%)] Loss: 18380.558594\n",
      "Train Epoch: 230 [97536/225000 (43%)] Loss: 17484.207031\n",
      "Train Epoch: 230 [100032/225000 (44%)] Loss: 18056.000000\n",
      "Train Epoch: 230 [102528/225000 (46%)] Loss: 18281.777344\n",
      "Train Epoch: 230 [105024/225000 (47%)] Loss: 17863.804688\n",
      "Train Epoch: 230 [107520/225000 (48%)] Loss: 17881.492188\n",
      "Train Epoch: 230 [110016/225000 (49%)] Loss: 17886.009766\n",
      "Train Epoch: 230 [112512/225000 (50%)] Loss: 18067.261719\n",
      "Train Epoch: 230 [115008/225000 (51%)] Loss: 18308.515625\n",
      "Train Epoch: 230 [117504/225000 (52%)] Loss: 17775.250000\n",
      "Train Epoch: 230 [120000/225000 (53%)] Loss: 17838.525391\n",
      "Train Epoch: 230 [122496/225000 (54%)] Loss: 17735.902344\n",
      "Train Epoch: 230 [124992/225000 (56%)] Loss: 18053.664062\n",
      "Train Epoch: 230 [127488/225000 (57%)] Loss: 18246.341797\n",
      "Train Epoch: 230 [129984/225000 (58%)] Loss: 17929.283203\n",
      "Train Epoch: 230 [132480/225000 (59%)] Loss: 17958.898438\n",
      "Train Epoch: 230 [134976/225000 (60%)] Loss: 18048.808594\n",
      "Train Epoch: 230 [137472/225000 (61%)] Loss: 18252.726562\n",
      "Train Epoch: 230 [139968/225000 (62%)] Loss: 17745.128906\n",
      "Train Epoch: 230 [142464/225000 (63%)] Loss: 18220.746094\n",
      "Train Epoch: 230 [144960/225000 (64%)] Loss: 18200.232422\n",
      "Train Epoch: 230 [147456/225000 (66%)] Loss: 17521.320312\n",
      "Train Epoch: 230 [149952/225000 (67%)] Loss: 17962.839844\n",
      "Train Epoch: 230 [152448/225000 (68%)] Loss: 18248.980469\n",
      "Train Epoch: 230 [154944/225000 (69%)] Loss: 17975.802734\n",
      "Train Epoch: 230 [157440/225000 (70%)] Loss: 17824.660156\n",
      "Train Epoch: 230 [159936/225000 (71%)] Loss: 17707.628906\n",
      "Train Epoch: 230 [162432/225000 (72%)] Loss: 17802.894531\n",
      "Train Epoch: 230 [164928/225000 (73%)] Loss: 18394.226562\n",
      "Train Epoch: 230 [167424/225000 (74%)] Loss: 18056.689453\n",
      "Train Epoch: 230 [169920/225000 (76%)] Loss: 18362.046875\n",
      "Train Epoch: 230 [172416/225000 (77%)] Loss: 17849.500000\n",
      "Train Epoch: 230 [174912/225000 (78%)] Loss: 17608.568359\n",
      "Train Epoch: 230 [177408/225000 (79%)] Loss: 17843.525391\n",
      "Train Epoch: 230 [179904/225000 (80%)] Loss: 18065.050781\n",
      "Train Epoch: 230 [182400/225000 (81%)] Loss: 17817.339844\n",
      "Train Epoch: 230 [184896/225000 (82%)] Loss: 17878.322266\n",
      "Train Epoch: 230 [187392/225000 (83%)] Loss: 17849.812500\n",
      "Train Epoch: 230 [189888/225000 (84%)] Loss: 18225.457031\n",
      "Train Epoch: 230 [192384/225000 (86%)] Loss: 18146.587891\n",
      "Train Epoch: 230 [194880/225000 (87%)] Loss: 18708.507812\n",
      "Train Epoch: 230 [197376/225000 (88%)] Loss: 18007.150391\n",
      "Train Epoch: 230 [199872/225000 (89%)] Loss: 17895.257812\n",
      "Train Epoch: 230 [202368/225000 (90%)] Loss: 17844.867188\n",
      "Train Epoch: 230 [204864/225000 (91%)] Loss: 18087.347656\n",
      "Train Epoch: 230 [207360/225000 (92%)] Loss: 17971.085938\n",
      "Train Epoch: 230 [209856/225000 (93%)] Loss: 17723.242188\n",
      "Train Epoch: 230 [212352/225000 (94%)] Loss: 17951.427734\n",
      "Train Epoch: 230 [214848/225000 (95%)] Loss: 18251.228516\n",
      "Train Epoch: 230 [217344/225000 (97%)] Loss: 18289.859375\n",
      "Train Epoch: 230 [219840/225000 (98%)] Loss: 18450.169922\n",
      "Train Epoch: 230 [222336/225000 (99%)] Loss: 17742.390625\n",
      "Train Epoch: 230 [224832/225000 (100%)] Loss: 17945.554688\n",
      "    epoch          : 230\n",
      "    loss           : 17995.068892651452\n",
      "    val_loss       : 17962.822059638627\n",
      "Train Epoch: 231 [192/225000 (0%)] Loss: 18218.769531\n",
      "Train Epoch: 231 [2688/225000 (1%)] Loss: 17680.558594\n",
      "Train Epoch: 231 [5184/225000 (2%)] Loss: 17849.375000\n",
      "Train Epoch: 231 [7680/225000 (3%)] Loss: 17905.050781\n",
      "Train Epoch: 231 [10176/225000 (5%)] Loss: 17925.472656\n",
      "Train Epoch: 231 [12672/225000 (6%)] Loss: 17889.384766\n",
      "Train Epoch: 231 [15168/225000 (7%)] Loss: 17707.574219\n",
      "Train Epoch: 231 [17664/225000 (8%)] Loss: 18147.238281\n",
      "Train Epoch: 231 [20160/225000 (9%)] Loss: 17789.681641\n",
      "Train Epoch: 231 [22656/225000 (10%)] Loss: 17355.464844\n",
      "Train Epoch: 231 [25152/225000 (11%)] Loss: 17886.396484\n",
      "Train Epoch: 231 [27648/225000 (12%)] Loss: 17639.531250\n",
      "Train Epoch: 231 [30144/225000 (13%)] Loss: 17380.166016\n",
      "Train Epoch: 231 [32640/225000 (15%)] Loss: 18138.976562\n",
      "Train Epoch: 231 [35136/225000 (16%)] Loss: 17841.136719\n",
      "Train Epoch: 231 [37632/225000 (17%)] Loss: 18275.990234\n",
      "Train Epoch: 231 [40128/225000 (18%)] Loss: 18168.921875\n",
      "Train Epoch: 231 [42624/225000 (19%)] Loss: 18491.066406\n",
      "Train Epoch: 231 [45120/225000 (20%)] Loss: 18077.593750\n",
      "Train Epoch: 231 [47616/225000 (21%)] Loss: 17651.164062\n",
      "Train Epoch: 231 [50112/225000 (22%)] Loss: 18036.300781\n",
      "Train Epoch: 231 [52608/225000 (23%)] Loss: 17882.875000\n",
      "Train Epoch: 231 [55104/225000 (24%)] Loss: 18243.792969\n",
      "Train Epoch: 231 [57600/225000 (26%)] Loss: 18023.177734\n",
      "Train Epoch: 231 [60096/225000 (27%)] Loss: 17597.937500\n",
      "Train Epoch: 231 [62592/225000 (28%)] Loss: 17265.656250\n",
      "Train Epoch: 231 [65088/225000 (29%)] Loss: 18221.585938\n",
      "Train Epoch: 231 [67584/225000 (30%)] Loss: 18178.423828\n",
      "Train Epoch: 231 [70080/225000 (31%)] Loss: 17902.023438\n",
      "Train Epoch: 231 [72576/225000 (32%)] Loss: 17583.798828\n",
      "Train Epoch: 231 [75072/225000 (33%)] Loss: 17938.681641\n",
      "Train Epoch: 231 [77568/225000 (34%)] Loss: 17971.738281\n",
      "Train Epoch: 231 [80064/225000 (36%)] Loss: 18129.132812\n",
      "Train Epoch: 231 [82560/225000 (37%)] Loss: 18086.531250\n",
      "Train Epoch: 231 [85056/225000 (38%)] Loss: 17841.878906\n",
      "Train Epoch: 231 [87552/225000 (39%)] Loss: 18151.796875\n",
      "Train Epoch: 231 [90048/225000 (40%)] Loss: 18325.343750\n",
      "Train Epoch: 231 [92544/225000 (41%)] Loss: 17901.386719\n",
      "Train Epoch: 231 [95040/225000 (42%)] Loss: 18043.632812\n",
      "Train Epoch: 231 [97536/225000 (43%)] Loss: 17933.234375\n",
      "Train Epoch: 231 [100032/225000 (44%)] Loss: 17979.527344\n",
      "Train Epoch: 231 [102528/225000 (46%)] Loss: 18139.115234\n",
      "Train Epoch: 231 [105024/225000 (47%)] Loss: 18037.746094\n",
      "Train Epoch: 231 [107520/225000 (48%)] Loss: 18194.769531\n",
      "Train Epoch: 231 [110016/225000 (49%)] Loss: 17984.320312\n",
      "Train Epoch: 231 [112512/225000 (50%)] Loss: 18010.289062\n",
      "Train Epoch: 231 [115008/225000 (51%)] Loss: 17910.351562\n",
      "Train Epoch: 231 [117504/225000 (52%)] Loss: 17656.525391\n",
      "Train Epoch: 231 [120000/225000 (53%)] Loss: 18192.429688\n",
      "Train Epoch: 231 [122496/225000 (54%)] Loss: 17940.269531\n",
      "Train Epoch: 231 [124992/225000 (56%)] Loss: 17923.843750\n",
      "Train Epoch: 231 [127488/225000 (57%)] Loss: 17916.570312\n",
      "Train Epoch: 231 [129984/225000 (58%)] Loss: 18002.667969\n",
      "Train Epoch: 231 [132480/225000 (59%)] Loss: 18133.171875\n",
      "Train Epoch: 231 [134976/225000 (60%)] Loss: 18006.976562\n",
      "Train Epoch: 231 [137472/225000 (61%)] Loss: 17995.347656\n",
      "Train Epoch: 231 [139968/225000 (62%)] Loss: 17911.533203\n",
      "Train Epoch: 231 [142464/225000 (63%)] Loss: 17635.625000\n",
      "Train Epoch: 231 [144960/225000 (64%)] Loss: 18067.736328\n",
      "Train Epoch: 231 [147456/225000 (66%)] Loss: 17476.968750\n",
      "Train Epoch: 231 [149952/225000 (67%)] Loss: 18099.876953\n",
      "Train Epoch: 231 [152448/225000 (68%)] Loss: 18292.560547\n",
      "Train Epoch: 231 [154944/225000 (69%)] Loss: 18183.460938\n",
      "Train Epoch: 231 [157440/225000 (70%)] Loss: 17555.917969\n",
      "Train Epoch: 231 [159936/225000 (71%)] Loss: 18160.199219\n",
      "Train Epoch: 231 [162432/225000 (72%)] Loss: 18248.347656\n",
      "Train Epoch: 231 [164928/225000 (73%)] Loss: 18230.832031\n",
      "Train Epoch: 231 [167424/225000 (74%)] Loss: 18097.425781\n",
      "Train Epoch: 231 [169920/225000 (76%)] Loss: 17712.675781\n",
      "Train Epoch: 231 [172416/225000 (77%)] Loss: 17921.347656\n",
      "Train Epoch: 231 [174912/225000 (78%)] Loss: 18047.953125\n",
      "Train Epoch: 231 [177408/225000 (79%)] Loss: 17682.375000\n",
      "Train Epoch: 231 [179904/225000 (80%)] Loss: 18060.660156\n",
      "Train Epoch: 231 [182400/225000 (81%)] Loss: 17764.763672\n",
      "Train Epoch: 231 [184896/225000 (82%)] Loss: 17729.998047\n",
      "Train Epoch: 231 [187392/225000 (83%)] Loss: 17649.742188\n",
      "Train Epoch: 231 [189888/225000 (84%)] Loss: 17876.847656\n",
      "Train Epoch: 231 [192384/225000 (86%)] Loss: 18267.716797\n",
      "Train Epoch: 231 [194880/225000 (87%)] Loss: 18101.285156\n",
      "Train Epoch: 231 [197376/225000 (88%)] Loss: 17962.218750\n",
      "Train Epoch: 231 [199872/225000 (89%)] Loss: 18202.777344\n",
      "Train Epoch: 231 [202368/225000 (90%)] Loss: 18057.265625\n",
      "Train Epoch: 231 [204864/225000 (91%)] Loss: 18093.980469\n",
      "Train Epoch: 231 [207360/225000 (92%)] Loss: 18017.039062\n",
      "Train Epoch: 231 [209856/225000 (93%)] Loss: 18653.679688\n",
      "Train Epoch: 231 [212352/225000 (94%)] Loss: 18089.962891\n",
      "Train Epoch: 231 [214848/225000 (95%)] Loss: 18256.453125\n",
      "Train Epoch: 231 [217344/225000 (97%)] Loss: 17924.773438\n",
      "Train Epoch: 231 [219840/225000 (98%)] Loss: 18052.027344\n",
      "Train Epoch: 231 [222336/225000 (99%)] Loss: 17613.718750\n",
      "Train Epoch: 231 [224832/225000 (100%)] Loss: 18038.894531\n",
      "    epoch          : 231\n",
      "    loss           : 17981.459428494294\n",
      "    val_loss       : 17878.947305090554\n",
      "Train Epoch: 232 [192/225000 (0%)] Loss: 18109.869141\n",
      "Train Epoch: 232 [2688/225000 (1%)] Loss: 17819.300781\n",
      "Train Epoch: 232 [5184/225000 (2%)] Loss: 17751.820312\n",
      "Train Epoch: 232 [7680/225000 (3%)] Loss: 18013.511719\n",
      "Train Epoch: 232 [10176/225000 (5%)] Loss: 17694.410156\n",
      "Train Epoch: 232 [12672/225000 (6%)] Loss: 18047.646484\n",
      "Train Epoch: 232 [15168/225000 (7%)] Loss: 17674.289062\n",
      "Train Epoch: 232 [17664/225000 (8%)] Loss: 18115.699219\n",
      "Train Epoch: 232 [20160/225000 (9%)] Loss: 18457.900391\n",
      "Train Epoch: 232 [22656/225000 (10%)] Loss: 18473.769531\n",
      "Train Epoch: 232 [25152/225000 (11%)] Loss: 18296.958984\n",
      "Train Epoch: 232 [27648/225000 (12%)] Loss: 17758.419922\n",
      "Train Epoch: 232 [30144/225000 (13%)] Loss: 18091.246094\n",
      "Train Epoch: 232 [32640/225000 (15%)] Loss: 17391.500000\n",
      "Train Epoch: 232 [35136/225000 (16%)] Loss: 17836.546875\n",
      "Train Epoch: 232 [37632/225000 (17%)] Loss: 17879.994141\n",
      "Train Epoch: 232 [40128/225000 (18%)] Loss: 17779.818359\n",
      "Train Epoch: 232 [42624/225000 (19%)] Loss: 17800.980469\n",
      "Train Epoch: 232 [45120/225000 (20%)] Loss: 17958.011719\n",
      "Train Epoch: 232 [47616/225000 (21%)] Loss: 17711.488281\n",
      "Train Epoch: 232 [50112/225000 (22%)] Loss: 18227.144531\n",
      "Train Epoch: 232 [52608/225000 (23%)] Loss: 18208.382812\n",
      "Train Epoch: 232 [55104/225000 (24%)] Loss: 18283.652344\n",
      "Train Epoch: 232 [57600/225000 (26%)] Loss: 17920.859375\n",
      "Train Epoch: 232 [60096/225000 (27%)] Loss: 18032.308594\n",
      "Train Epoch: 232 [62592/225000 (28%)] Loss: 18002.250000\n",
      "Train Epoch: 232 [65088/225000 (29%)] Loss: 17589.328125\n",
      "Train Epoch: 232 [67584/225000 (30%)] Loss: 17937.712891\n",
      "Train Epoch: 232 [70080/225000 (31%)] Loss: 18268.070312\n",
      "Train Epoch: 232 [72576/225000 (32%)] Loss: 17920.474609\n",
      "Train Epoch: 232 [75072/225000 (33%)] Loss: 17848.660156\n",
      "Train Epoch: 232 [77568/225000 (34%)] Loss: 17946.750000\n",
      "Train Epoch: 232 [80064/225000 (36%)] Loss: 17724.265625\n",
      "Train Epoch: 232 [82560/225000 (37%)] Loss: 17764.574219\n",
      "Train Epoch: 232 [85056/225000 (38%)] Loss: 18155.792969\n",
      "Train Epoch: 232 [87552/225000 (39%)] Loss: 18001.828125\n",
      "Train Epoch: 232 [90048/225000 (40%)] Loss: 18170.478516\n",
      "Train Epoch: 232 [92544/225000 (41%)] Loss: 18039.082031\n",
      "Train Epoch: 232 [95040/225000 (42%)] Loss: 17807.175781\n",
      "Train Epoch: 232 [97536/225000 (43%)] Loss: 18199.166016\n",
      "Train Epoch: 232 [100032/225000 (44%)] Loss: 17930.757812\n",
      "Train Epoch: 232 [102528/225000 (46%)] Loss: 17704.662109\n",
      "Train Epoch: 232 [105024/225000 (47%)] Loss: 18269.646484\n",
      "Train Epoch: 232 [107520/225000 (48%)] Loss: 17658.960938\n",
      "Train Epoch: 232 [110016/225000 (49%)] Loss: 17841.718750\n",
      "Train Epoch: 232 [112512/225000 (50%)] Loss: 17732.250000\n",
      "Train Epoch: 232 [115008/225000 (51%)] Loss: 17942.578125\n",
      "Train Epoch: 232 [117504/225000 (52%)] Loss: 17701.816406\n",
      "Train Epoch: 232 [120000/225000 (53%)] Loss: 17916.619141\n",
      "Train Epoch: 232 [122496/225000 (54%)] Loss: 17912.591797\n",
      "Train Epoch: 232 [124992/225000 (56%)] Loss: 17759.996094\n",
      "Train Epoch: 232 [127488/225000 (57%)] Loss: 18195.218750\n",
      "Train Epoch: 232 [129984/225000 (58%)] Loss: 17678.839844\n",
      "Train Epoch: 232 [132480/225000 (59%)] Loss: 17659.757812\n",
      "Train Epoch: 232 [134976/225000 (60%)] Loss: 17903.488281\n",
      "Train Epoch: 232 [137472/225000 (61%)] Loss: 17856.531250\n",
      "Train Epoch: 232 [139968/225000 (62%)] Loss: 17203.716797\n",
      "Train Epoch: 232 [142464/225000 (63%)] Loss: 17716.558594\n",
      "Train Epoch: 232 [144960/225000 (64%)] Loss: 17977.654297\n",
      "Train Epoch: 232 [147456/225000 (66%)] Loss: 18454.617188\n",
      "Train Epoch: 232 [149952/225000 (67%)] Loss: 18312.023438\n",
      "Train Epoch: 232 [152448/225000 (68%)] Loss: 17940.955078\n",
      "Train Epoch: 232 [154944/225000 (69%)] Loss: 17635.619141\n",
      "Train Epoch: 232 [157440/225000 (70%)] Loss: 17824.976562\n",
      "Train Epoch: 232 [159936/225000 (71%)] Loss: 18229.910156\n",
      "Train Epoch: 232 [162432/225000 (72%)] Loss: 17540.839844\n",
      "Train Epoch: 232 [164928/225000 (73%)] Loss: 17581.089844\n",
      "Train Epoch: 232 [167424/225000 (74%)] Loss: 18070.292969\n",
      "Train Epoch: 232 [169920/225000 (76%)] Loss: 17789.435547\n",
      "Train Epoch: 232 [172416/225000 (77%)] Loss: 18148.744141\n",
      "Train Epoch: 232 [174912/225000 (78%)] Loss: 17662.433594\n",
      "Train Epoch: 232 [177408/225000 (79%)] Loss: 17442.238281\n",
      "Train Epoch: 232 [179904/225000 (80%)] Loss: 18187.835938\n",
      "Train Epoch: 232 [182400/225000 (81%)] Loss: 17920.087891\n",
      "Train Epoch: 232 [184896/225000 (82%)] Loss: 18157.451172\n",
      "Train Epoch: 232 [187392/225000 (83%)] Loss: 17989.195312\n",
      "Train Epoch: 232 [189888/225000 (84%)] Loss: 18203.216797\n",
      "Train Epoch: 232 [192384/225000 (86%)] Loss: 18048.498047\n",
      "Train Epoch: 232 [194880/225000 (87%)] Loss: 17821.765625\n",
      "Train Epoch: 232 [197376/225000 (88%)] Loss: 18297.421875\n",
      "Train Epoch: 232 [199872/225000 (89%)] Loss: 18177.861328\n",
      "Train Epoch: 232 [202368/225000 (90%)] Loss: 17828.734375\n",
      "Train Epoch: 232 [204864/225000 (91%)] Loss: 18018.875000\n",
      "Train Epoch: 232 [207360/225000 (92%)] Loss: 18078.089844\n",
      "Train Epoch: 232 [209856/225000 (93%)] Loss: 17672.113281\n",
      "Train Epoch: 232 [212352/225000 (94%)] Loss: 17878.816406\n",
      "Train Epoch: 232 [214848/225000 (95%)] Loss: 17937.679688\n",
      "Train Epoch: 232 [217344/225000 (97%)] Loss: 17406.074219\n",
      "Train Epoch: 232 [219840/225000 (98%)] Loss: 17670.601562\n",
      "Train Epoch: 232 [222336/225000 (99%)] Loss: 17975.089844\n",
      "Train Epoch: 232 [224832/225000 (100%)] Loss: 17980.125000\n",
      "    epoch          : 232\n",
      "    loss           : 17958.978380639397\n",
      "    val_loss       : 17861.760813036948\n",
      "Train Epoch: 233 [192/225000 (0%)] Loss: 18235.949219\n",
      "Train Epoch: 233 [2688/225000 (1%)] Loss: 17797.492188\n",
      "Train Epoch: 233 [5184/225000 (2%)] Loss: 17971.587891\n",
      "Train Epoch: 233 [7680/225000 (3%)] Loss: 18016.775391\n",
      "Train Epoch: 233 [10176/225000 (5%)] Loss: 17880.937500\n",
      "Train Epoch: 233 [12672/225000 (6%)] Loss: 17437.679688\n",
      "Train Epoch: 233 [15168/225000 (7%)] Loss: 18066.898438\n",
      "Train Epoch: 233 [17664/225000 (8%)] Loss: 17569.742188\n",
      "Train Epoch: 233 [20160/225000 (9%)] Loss: 18069.808594\n",
      "Train Epoch: 233 [22656/225000 (10%)] Loss: 18008.917969\n",
      "Train Epoch: 233 [25152/225000 (11%)] Loss: 18109.218750\n",
      "Train Epoch: 233 [27648/225000 (12%)] Loss: 18097.714844\n",
      "Train Epoch: 233 [30144/225000 (13%)] Loss: 17678.906250\n",
      "Train Epoch: 233 [32640/225000 (15%)] Loss: 17731.414062\n",
      "Train Epoch: 233 [35136/225000 (16%)] Loss: 18068.898438\n",
      "Train Epoch: 233 [37632/225000 (17%)] Loss: 18114.625000\n",
      "Train Epoch: 233 [40128/225000 (18%)] Loss: 17826.582031\n",
      "Train Epoch: 233 [42624/225000 (19%)] Loss: 18458.210938\n",
      "Train Epoch: 233 [45120/225000 (20%)] Loss: 17682.808594\n",
      "Train Epoch: 233 [47616/225000 (21%)] Loss: 18081.351562\n",
      "Train Epoch: 233 [50112/225000 (22%)] Loss: 18237.554688\n",
      "Train Epoch: 233 [52608/225000 (23%)] Loss: 17914.667969\n",
      "Train Epoch: 233 [55104/225000 (24%)] Loss: 17811.703125\n",
      "Train Epoch: 233 [57600/225000 (26%)] Loss: 17917.613281\n",
      "Train Epoch: 233 [60096/225000 (27%)] Loss: 17994.968750\n",
      "Train Epoch: 233 [62592/225000 (28%)] Loss: 17589.195312\n",
      "Train Epoch: 233 [65088/225000 (29%)] Loss: 18171.953125\n",
      "Train Epoch: 233 [67584/225000 (30%)] Loss: 17802.964844\n",
      "Train Epoch: 233 [70080/225000 (31%)] Loss: 17532.835938\n",
      "Train Epoch: 233 [72576/225000 (32%)] Loss: 17872.050781\n",
      "Train Epoch: 233 [75072/225000 (33%)] Loss: 18301.429688\n",
      "Train Epoch: 233 [77568/225000 (34%)] Loss: 18383.117188\n",
      "Train Epoch: 233 [80064/225000 (36%)] Loss: 17846.000000\n",
      "Train Epoch: 233 [82560/225000 (37%)] Loss: 18604.351562\n",
      "Train Epoch: 233 [85056/225000 (38%)] Loss: 17858.302734\n",
      "Train Epoch: 233 [87552/225000 (39%)] Loss: 18005.058594\n",
      "Train Epoch: 233 [90048/225000 (40%)] Loss: 18093.082031\n",
      "Train Epoch: 233 [92544/225000 (41%)] Loss: 18171.160156\n",
      "Train Epoch: 233 [95040/225000 (42%)] Loss: 17953.175781\n",
      "Train Epoch: 233 [97536/225000 (43%)] Loss: 17635.515625\n",
      "Train Epoch: 233 [100032/225000 (44%)] Loss: 17994.160156\n",
      "Train Epoch: 233 [102528/225000 (46%)] Loss: 17729.666016\n",
      "Train Epoch: 233 [105024/225000 (47%)] Loss: 17789.134766\n",
      "Train Epoch: 233 [107520/225000 (48%)] Loss: 17882.195312\n",
      "Train Epoch: 233 [110016/225000 (49%)] Loss: 17202.023438\n",
      "Train Epoch: 233 [112512/225000 (50%)] Loss: 17618.880859\n",
      "Train Epoch: 233 [115008/225000 (51%)] Loss: 17983.427734\n",
      "Train Epoch: 233 [117504/225000 (52%)] Loss: 17844.687500\n",
      "Train Epoch: 233 [120000/225000 (53%)] Loss: 17955.386719\n",
      "Train Epoch: 233 [122496/225000 (54%)] Loss: 18554.285156\n",
      "Train Epoch: 233 [124992/225000 (56%)] Loss: 17662.488281\n",
      "Train Epoch: 233 [127488/225000 (57%)] Loss: 17977.642578\n",
      "Train Epoch: 233 [129984/225000 (58%)] Loss: 17774.958984\n",
      "Train Epoch: 233 [132480/225000 (59%)] Loss: 17766.267578\n",
      "Train Epoch: 233 [134976/225000 (60%)] Loss: 18078.218750\n",
      "Train Epoch: 233 [137472/225000 (61%)] Loss: 18051.175781\n",
      "Train Epoch: 233 [139968/225000 (62%)] Loss: 17871.968750\n",
      "Train Epoch: 233 [142464/225000 (63%)] Loss: 17622.986328\n",
      "Train Epoch: 233 [144960/225000 (64%)] Loss: 17830.011719\n",
      "Train Epoch: 233 [147456/225000 (66%)] Loss: 17895.027344\n",
      "Train Epoch: 233 [149952/225000 (67%)] Loss: 18339.675781\n",
      "Train Epoch: 233 [152448/225000 (68%)] Loss: 17834.251953\n",
      "Train Epoch: 233 [154944/225000 (69%)] Loss: 18185.812500\n",
      "Train Epoch: 233 [157440/225000 (70%)] Loss: 18202.468750\n",
      "Train Epoch: 233 [159936/225000 (71%)] Loss: 17622.339844\n",
      "Train Epoch: 233 [162432/225000 (72%)] Loss: 18140.712891\n",
      "Train Epoch: 233 [164928/225000 (73%)] Loss: 18247.250000\n",
      "Train Epoch: 233 [167424/225000 (74%)] Loss: 17897.197266\n",
      "Train Epoch: 233 [169920/225000 (76%)] Loss: 18545.486328\n",
      "Train Epoch: 233 [172416/225000 (77%)] Loss: 17948.847656\n",
      "Train Epoch: 233 [174912/225000 (78%)] Loss: 18086.482422\n",
      "Train Epoch: 233 [177408/225000 (79%)] Loss: 17912.238281\n",
      "Train Epoch: 233 [179904/225000 (80%)] Loss: 18184.126953\n",
      "Train Epoch: 233 [182400/225000 (81%)] Loss: 18164.839844\n",
      "Train Epoch: 233 [184896/225000 (82%)] Loss: 18337.980469\n",
      "Train Epoch: 233 [187392/225000 (83%)] Loss: 17484.736328\n",
      "Train Epoch: 233 [189888/225000 (84%)] Loss: 18287.914062\n",
      "Train Epoch: 233 [192384/225000 (86%)] Loss: 17242.246094\n",
      "Train Epoch: 233 [194880/225000 (87%)] Loss: 17932.363281\n",
      "Train Epoch: 233 [197376/225000 (88%)] Loss: 17995.066406\n",
      "Train Epoch: 233 [199872/225000 (89%)] Loss: 18038.144531\n",
      "Train Epoch: 233 [202368/225000 (90%)] Loss: 18040.626953\n",
      "Train Epoch: 233 [204864/225000 (91%)] Loss: 18244.404297\n",
      "Train Epoch: 233 [207360/225000 (92%)] Loss: 17864.277344\n",
      "Train Epoch: 233 [209856/225000 (93%)] Loss: 17678.654297\n",
      "Train Epoch: 233 [212352/225000 (94%)] Loss: 18353.078125\n",
      "Train Epoch: 233 [214848/225000 (95%)] Loss: 18201.734375\n",
      "Train Epoch: 233 [217344/225000 (97%)] Loss: 18037.078125\n",
      "Train Epoch: 233 [219840/225000 (98%)] Loss: 17531.337891\n",
      "Train Epoch: 233 [222336/225000 (99%)] Loss: 17904.164062\n",
      "Train Epoch: 233 [224832/225000 (100%)] Loss: 17734.628906\n",
      "    epoch          : 233\n",
      "    loss           : 17947.27537146038\n",
      "    val_loss       : 17940.256553254054\n",
      "Train Epoch: 234 [192/225000 (0%)] Loss: 17862.023438\n",
      "Train Epoch: 234 [2688/225000 (1%)] Loss: 18006.566406\n",
      "Train Epoch: 234 [5184/225000 (2%)] Loss: 18145.214844\n",
      "Train Epoch: 234 [7680/225000 (3%)] Loss: 17811.683594\n",
      "Train Epoch: 234 [10176/225000 (5%)] Loss: 17757.273438\n",
      "Train Epoch: 234 [12672/225000 (6%)] Loss: 17828.511719\n",
      "Train Epoch: 234 [15168/225000 (7%)] Loss: 17770.300781\n",
      "Train Epoch: 234 [17664/225000 (8%)] Loss: 17634.058594\n",
      "Train Epoch: 234 [20160/225000 (9%)] Loss: 18175.285156\n",
      "Train Epoch: 234 [22656/225000 (10%)] Loss: 17866.777344\n",
      "Train Epoch: 234 [25152/225000 (11%)] Loss: 17904.460938\n",
      "Train Epoch: 234 [27648/225000 (12%)] Loss: 17822.029297\n",
      "Train Epoch: 234 [30144/225000 (13%)] Loss: 17522.039062\n",
      "Train Epoch: 234 [32640/225000 (15%)] Loss: 17758.753906\n",
      "Train Epoch: 234 [35136/225000 (16%)] Loss: 18306.332031\n",
      "Train Epoch: 234 [37632/225000 (17%)] Loss: 17554.640625\n",
      "Train Epoch: 234 [40128/225000 (18%)] Loss: 18032.984375\n",
      "Train Epoch: 234 [42624/225000 (19%)] Loss: 17774.060547\n",
      "Train Epoch: 234 [45120/225000 (20%)] Loss: 18229.835938\n",
      "Train Epoch: 234 [47616/225000 (21%)] Loss: 18110.933594\n",
      "Train Epoch: 234 [50112/225000 (22%)] Loss: 17671.332031\n",
      "Train Epoch: 234 [52608/225000 (23%)] Loss: 18294.546875\n",
      "Train Epoch: 234 [55104/225000 (24%)] Loss: 17842.503906\n",
      "Train Epoch: 234 [57600/225000 (26%)] Loss: 17811.583984\n",
      "Train Epoch: 234 [60096/225000 (27%)] Loss: 17821.513672\n",
      "Train Epoch: 234 [62592/225000 (28%)] Loss: 18028.410156\n",
      "Train Epoch: 234 [65088/225000 (29%)] Loss: 17959.898438\n",
      "Train Epoch: 234 [67584/225000 (30%)] Loss: 17302.554688\n",
      "Train Epoch: 234 [70080/225000 (31%)] Loss: 17989.470703\n",
      "Train Epoch: 234 [72576/225000 (32%)] Loss: 18114.384766\n",
      "Train Epoch: 234 [75072/225000 (33%)] Loss: 17591.404297\n",
      "Train Epoch: 234 [77568/225000 (34%)] Loss: 18094.285156\n",
      "Train Epoch: 234 [80064/225000 (36%)] Loss: 17970.046875\n",
      "Train Epoch: 234 [82560/225000 (37%)] Loss: 17546.496094\n",
      "Train Epoch: 234 [85056/225000 (38%)] Loss: 17872.679688\n",
      "Train Epoch: 234 [87552/225000 (39%)] Loss: 17840.121094\n",
      "Train Epoch: 234 [90048/225000 (40%)] Loss: 18011.792969\n",
      "Train Epoch: 234 [92544/225000 (41%)] Loss: 17982.542969\n",
      "Train Epoch: 234 [95040/225000 (42%)] Loss: 17723.093750\n",
      "Train Epoch: 234 [97536/225000 (43%)] Loss: 17395.984375\n",
      "Train Epoch: 234 [100032/225000 (44%)] Loss: 17634.027344\n",
      "Train Epoch: 234 [102528/225000 (46%)] Loss: 17921.382812\n",
      "Train Epoch: 234 [105024/225000 (47%)] Loss: 17989.445312\n",
      "Train Epoch: 234 [107520/225000 (48%)] Loss: 17918.921875\n",
      "Train Epoch: 234 [110016/225000 (49%)] Loss: 17661.656250\n",
      "Train Epoch: 234 [112512/225000 (50%)] Loss: 18007.007812\n",
      "Train Epoch: 234 [115008/225000 (51%)] Loss: 18074.539062\n",
      "Train Epoch: 234 [117504/225000 (52%)] Loss: 18235.701172\n",
      "Train Epoch: 234 [120000/225000 (53%)] Loss: 17734.021484\n",
      "Train Epoch: 234 [122496/225000 (54%)] Loss: 18101.785156\n",
      "Train Epoch: 234 [124992/225000 (56%)] Loss: 17694.531250\n",
      "Train Epoch: 234 [127488/225000 (57%)] Loss: 18061.445312\n",
      "Train Epoch: 234 [129984/225000 (58%)] Loss: 17887.914062\n",
      "Train Epoch: 234 [132480/225000 (59%)] Loss: 17673.804688\n",
      "Train Epoch: 234 [134976/225000 (60%)] Loss: 17338.625000\n",
      "Train Epoch: 234 [137472/225000 (61%)] Loss: 17551.613281\n",
      "Train Epoch: 234 [139968/225000 (62%)] Loss: 17817.925781\n",
      "Train Epoch: 234 [142464/225000 (63%)] Loss: 17715.027344\n",
      "Train Epoch: 234 [144960/225000 (64%)] Loss: 18036.773438\n",
      "Train Epoch: 234 [147456/225000 (66%)] Loss: 17989.195312\n",
      "Train Epoch: 234 [149952/225000 (67%)] Loss: 18121.746094\n",
      "Train Epoch: 234 [152448/225000 (68%)] Loss: 17655.632812\n",
      "Train Epoch: 234 [154944/225000 (69%)] Loss: 18460.384766\n",
      "Train Epoch: 234 [157440/225000 (70%)] Loss: 17722.238281\n",
      "Train Epoch: 234 [159936/225000 (71%)] Loss: 18191.515625\n",
      "Train Epoch: 234 [162432/225000 (72%)] Loss: 17689.675781\n",
      "Train Epoch: 234 [164928/225000 (73%)] Loss: 17738.380859\n",
      "Train Epoch: 234 [167424/225000 (74%)] Loss: 18336.480469\n",
      "Train Epoch: 234 [169920/225000 (76%)] Loss: 17840.710938\n",
      "Train Epoch: 234 [172416/225000 (77%)] Loss: 17735.759766\n",
      "Train Epoch: 234 [174912/225000 (78%)] Loss: 17746.078125\n",
      "Train Epoch: 234 [177408/225000 (79%)] Loss: 17524.056641\n",
      "Train Epoch: 234 [179904/225000 (80%)] Loss: 17840.541016\n",
      "Train Epoch: 234 [182400/225000 (81%)] Loss: 18150.275391\n",
      "Train Epoch: 234 [184896/225000 (82%)] Loss: 17977.732422\n",
      "Train Epoch: 234 [187392/225000 (83%)] Loss: 17692.402344\n",
      "Train Epoch: 234 [189888/225000 (84%)] Loss: 18261.363281\n",
      "Train Epoch: 234 [192384/225000 (86%)] Loss: 17597.525391\n",
      "Train Epoch: 234 [194880/225000 (87%)] Loss: 18156.273438\n",
      "Train Epoch: 234 [197376/225000 (88%)] Loss: 18118.931641\n",
      "Train Epoch: 234 [199872/225000 (89%)] Loss: 17920.630859\n",
      "Train Epoch: 234 [202368/225000 (90%)] Loss: 17604.650391\n",
      "Train Epoch: 234 [204864/225000 (91%)] Loss: 18116.255859\n",
      "Train Epoch: 234 [207360/225000 (92%)] Loss: 17920.222656\n",
      "Train Epoch: 234 [209856/225000 (93%)] Loss: 17740.070312\n",
      "Train Epoch: 234 [212352/225000 (94%)] Loss: 17644.320312\n",
      "Train Epoch: 234 [214848/225000 (95%)] Loss: 18030.226562\n",
      "Train Epoch: 234 [217344/225000 (97%)] Loss: 17963.587891\n",
      "Train Epoch: 234 [219840/225000 (98%)] Loss: 17971.642578\n",
      "Train Epoch: 234 [222336/225000 (99%)] Loss: 17894.449219\n",
      "Train Epoch: 234 [224832/225000 (100%)] Loss: 18115.855469\n",
      "    epoch          : 234\n",
      "    loss           : 17935.296612527996\n",
      "    val_loss       : 17900.349752609058\n",
      "Train Epoch: 235 [192/225000 (0%)] Loss: 17754.230469\n",
      "Train Epoch: 235 [2688/225000 (1%)] Loss: 17754.625000\n",
      "Train Epoch: 235 [5184/225000 (2%)] Loss: 18159.439453\n",
      "Train Epoch: 235 [7680/225000 (3%)] Loss: 17884.082031\n",
      "Train Epoch: 235 [10176/225000 (5%)] Loss: 18325.537109\n",
      "Train Epoch: 235 [12672/225000 (6%)] Loss: 17737.675781\n",
      "Train Epoch: 235 [15168/225000 (7%)] Loss: 18199.875000\n",
      "Train Epoch: 235 [17664/225000 (8%)] Loss: 17471.421875\n",
      "Train Epoch: 235 [20160/225000 (9%)] Loss: 18111.021484\n",
      "Train Epoch: 235 [22656/225000 (10%)] Loss: 18144.873047\n",
      "Train Epoch: 235 [25152/225000 (11%)] Loss: 17705.427734\n",
      "Train Epoch: 235 [27648/225000 (12%)] Loss: 17898.308594\n",
      "Train Epoch: 235 [30144/225000 (13%)] Loss: 17833.269531\n",
      "Train Epoch: 235 [32640/225000 (15%)] Loss: 17788.710938\n",
      "Train Epoch: 235 [35136/225000 (16%)] Loss: 17906.066406\n",
      "Train Epoch: 235 [37632/225000 (17%)] Loss: 17776.957031\n",
      "Train Epoch: 235 [40128/225000 (18%)] Loss: 17612.755859\n",
      "Train Epoch: 235 [42624/225000 (19%)] Loss: 18017.455078\n",
      "Train Epoch: 235 [45120/225000 (20%)] Loss: 17742.843750\n",
      "Train Epoch: 235 [47616/225000 (21%)] Loss: 18125.595703\n",
      "Train Epoch: 235 [50112/225000 (22%)] Loss: 18051.574219\n",
      "Train Epoch: 235 [52608/225000 (23%)] Loss: 18297.050781\n",
      "Train Epoch: 235 [55104/225000 (24%)] Loss: 17612.785156\n",
      "Train Epoch: 235 [57600/225000 (26%)] Loss: 18160.917969\n",
      "Train Epoch: 235 [60096/225000 (27%)] Loss: 17864.880859\n",
      "Train Epoch: 235 [62592/225000 (28%)] Loss: 17729.236328\n",
      "Train Epoch: 235 [65088/225000 (29%)] Loss: 18222.330078\n",
      "Train Epoch: 235 [67584/225000 (30%)] Loss: 18519.093750\n",
      "Train Epoch: 235 [70080/225000 (31%)] Loss: 17933.476562\n",
      "Train Epoch: 235 [72576/225000 (32%)] Loss: 17685.564453\n",
      "Train Epoch: 235 [75072/225000 (33%)] Loss: 18008.291016\n",
      "Train Epoch: 235 [77568/225000 (34%)] Loss: 18185.941406\n",
      "Train Epoch: 235 [80064/225000 (36%)] Loss: 18128.308594\n",
      "Train Epoch: 235 [82560/225000 (37%)] Loss: 18329.199219\n",
      "Train Epoch: 235 [85056/225000 (38%)] Loss: 17862.427734\n",
      "Train Epoch: 235 [87552/225000 (39%)] Loss: 18292.648438\n",
      "Train Epoch: 235 [90048/225000 (40%)] Loss: 17825.181641\n",
      "Train Epoch: 235 [92544/225000 (41%)] Loss: 17799.691406\n",
      "Train Epoch: 235 [95040/225000 (42%)] Loss: 17912.664062\n",
      "Train Epoch: 235 [97536/225000 (43%)] Loss: 17797.679688\n",
      "Train Epoch: 235 [100032/225000 (44%)] Loss: 18089.890625\n",
      "Train Epoch: 235 [102528/225000 (46%)] Loss: 17900.337891\n",
      "Train Epoch: 235 [105024/225000 (47%)] Loss: 18467.812500\n",
      "Train Epoch: 235 [107520/225000 (48%)] Loss: 18025.835938\n",
      "Train Epoch: 235 [110016/225000 (49%)] Loss: 18150.898438\n",
      "Train Epoch: 235 [112512/225000 (50%)] Loss: 17984.775391\n",
      "Train Epoch: 235 [115008/225000 (51%)] Loss: 17969.152344\n",
      "Train Epoch: 235 [117504/225000 (52%)] Loss: 18162.519531\n",
      "Train Epoch: 235 [120000/225000 (53%)] Loss: 17498.296875\n",
      "Train Epoch: 235 [122496/225000 (54%)] Loss: 18065.917969\n",
      "Train Epoch: 235 [124992/225000 (56%)] Loss: 18391.451172\n",
      "Train Epoch: 235 [127488/225000 (57%)] Loss: 17715.082031\n",
      "Train Epoch: 235 [129984/225000 (58%)] Loss: 17760.250000\n",
      "Train Epoch: 235 [132480/225000 (59%)] Loss: 18145.097656\n",
      "Train Epoch: 235 [134976/225000 (60%)] Loss: 17941.958984\n",
      "Train Epoch: 235 [137472/225000 (61%)] Loss: 17593.027344\n",
      "Train Epoch: 235 [139968/225000 (62%)] Loss: 17877.585938\n",
      "Train Epoch: 235 [142464/225000 (63%)] Loss: 18260.349609\n",
      "Train Epoch: 235 [144960/225000 (64%)] Loss: 17739.480469\n",
      "Train Epoch: 235 [147456/225000 (66%)] Loss: 17808.730469\n",
      "Train Epoch: 235 [149952/225000 (67%)] Loss: 17828.679688\n",
      "Train Epoch: 235 [152448/225000 (68%)] Loss: 18167.451172\n",
      "Train Epoch: 235 [154944/225000 (69%)] Loss: 17938.500000\n",
      "Train Epoch: 235 [157440/225000 (70%)] Loss: 18116.210938\n",
      "Train Epoch: 235 [159936/225000 (71%)] Loss: 18030.367188\n",
      "Train Epoch: 235 [162432/225000 (72%)] Loss: 17698.595703\n",
      "Train Epoch: 235 [164928/225000 (73%)] Loss: 17919.445312\n",
      "Train Epoch: 235 [167424/225000 (74%)] Loss: 17853.187500\n",
      "Train Epoch: 235 [169920/225000 (76%)] Loss: 17792.664062\n",
      "Train Epoch: 235 [172416/225000 (77%)] Loss: 17632.988281\n",
      "Train Epoch: 235 [174912/225000 (78%)] Loss: 18247.998047\n",
      "Train Epoch: 235 [177408/225000 (79%)] Loss: 17489.234375\n",
      "Train Epoch: 235 [179904/225000 (80%)] Loss: 17783.455078\n",
      "Train Epoch: 235 [182400/225000 (81%)] Loss: 17967.503906\n",
      "Train Epoch: 235 [184896/225000 (82%)] Loss: 17930.101562\n",
      "Train Epoch: 235 [187392/225000 (83%)] Loss: 17842.414062\n",
      "Train Epoch: 235 [189888/225000 (84%)] Loss: 18048.980469\n",
      "Train Epoch: 235 [192384/225000 (86%)] Loss: 18027.820312\n",
      "Train Epoch: 235 [194880/225000 (87%)] Loss: 17682.792969\n",
      "Train Epoch: 235 [197376/225000 (88%)] Loss: 18209.812500\n",
      "Train Epoch: 235 [199872/225000 (89%)] Loss: 18136.384766\n",
      "Train Epoch: 235 [202368/225000 (90%)] Loss: 17908.521484\n",
      "Train Epoch: 235 [204864/225000 (91%)] Loss: 17990.941406\n",
      "Train Epoch: 235 [207360/225000 (92%)] Loss: 18023.386719\n",
      "Train Epoch: 235 [209856/225000 (93%)] Loss: 17886.367188\n",
      "Train Epoch: 235 [212352/225000 (94%)] Loss: 17767.394531\n",
      "Train Epoch: 235 [214848/225000 (95%)] Loss: 17897.679688\n",
      "Train Epoch: 235 [217344/225000 (97%)] Loss: 18047.826172\n",
      "Train Epoch: 235 [219840/225000 (98%)] Loss: 18392.925781\n",
      "Train Epoch: 235 [222336/225000 (99%)] Loss: 17438.582031\n",
      "Train Epoch: 235 [224832/225000 (100%)] Loss: 18241.240234\n",
      "    epoch          : 235\n",
      "    loss           : 17928.125734088364\n",
      "    val_loss       : 17828.019184573917\n",
      "Train Epoch: 236 [192/225000 (0%)] Loss: 17743.472656\n",
      "Train Epoch: 236 [2688/225000 (1%)] Loss: 17814.197266\n",
      "Train Epoch: 236 [5184/225000 (2%)] Loss: 18045.500000\n",
      "Train Epoch: 236 [7680/225000 (3%)] Loss: 18134.859375\n",
      "Train Epoch: 236 [10176/225000 (5%)] Loss: 17435.751953\n",
      "Train Epoch: 236 [12672/225000 (6%)] Loss: 17271.464844\n",
      "Train Epoch: 236 [15168/225000 (7%)] Loss: 17955.685547\n",
      "Train Epoch: 236 [17664/225000 (8%)] Loss: 18048.812500\n",
      "Train Epoch: 236 [20160/225000 (9%)] Loss: 18047.056641\n",
      "Train Epoch: 236 [22656/225000 (10%)] Loss: 17752.822266\n",
      "Train Epoch: 236 [25152/225000 (11%)] Loss: 18546.867188\n",
      "Train Epoch: 236 [27648/225000 (12%)] Loss: 18125.615234\n",
      "Train Epoch: 236 [30144/225000 (13%)] Loss: 17944.035156\n",
      "Train Epoch: 236 [32640/225000 (15%)] Loss: 17630.650391\n",
      "Train Epoch: 236 [35136/225000 (16%)] Loss: 18336.074219\n",
      "Train Epoch: 236 [37632/225000 (17%)] Loss: 17839.175781\n",
      "Train Epoch: 236 [40128/225000 (18%)] Loss: 17410.878906\n",
      "Train Epoch: 236 [42624/225000 (19%)] Loss: 18214.941406\n",
      "Train Epoch: 236 [45120/225000 (20%)] Loss: 18038.929688\n",
      "Train Epoch: 236 [47616/225000 (21%)] Loss: 18445.859375\n",
      "Train Epoch: 236 [50112/225000 (22%)] Loss: 17857.398438\n",
      "Train Epoch: 236 [52608/225000 (23%)] Loss: 17565.917969\n",
      "Train Epoch: 236 [55104/225000 (24%)] Loss: 17708.402344\n",
      "Train Epoch: 236 [57600/225000 (26%)] Loss: 17453.339844\n",
      "Train Epoch: 236 [60096/225000 (27%)] Loss: 17288.792969\n",
      "Train Epoch: 236 [62592/225000 (28%)] Loss: 17596.019531\n",
      "Train Epoch: 236 [65088/225000 (29%)] Loss: 18152.615234\n",
      "Train Epoch: 236 [67584/225000 (30%)] Loss: 17559.808594\n",
      "Train Epoch: 236 [70080/225000 (31%)] Loss: 17949.505859\n",
      "Train Epoch: 236 [72576/225000 (32%)] Loss: 17997.507812\n",
      "Train Epoch: 236 [75072/225000 (33%)] Loss: 17706.419922\n",
      "Train Epoch: 236 [77568/225000 (34%)] Loss: 18044.384766\n",
      "Train Epoch: 236 [80064/225000 (36%)] Loss: 18089.820312\n",
      "Train Epoch: 236 [82560/225000 (37%)] Loss: 18401.859375\n",
      "Train Epoch: 236 [85056/225000 (38%)] Loss: 17934.027344\n",
      "Train Epoch: 236 [87552/225000 (39%)] Loss: 17906.919922\n",
      "Train Epoch: 236 [90048/225000 (40%)] Loss: 17686.800781\n",
      "Train Epoch: 236 [92544/225000 (41%)] Loss: 17670.523438\n",
      "Train Epoch: 236 [95040/225000 (42%)] Loss: 17894.750000\n",
      "Train Epoch: 236 [97536/225000 (43%)] Loss: 18108.455078\n",
      "Train Epoch: 236 [100032/225000 (44%)] Loss: 18470.140625\n",
      "Train Epoch: 236 [102528/225000 (46%)] Loss: 17784.111328\n",
      "Train Epoch: 236 [105024/225000 (47%)] Loss: 18327.498047\n",
      "Train Epoch: 236 [107520/225000 (48%)] Loss: 18021.695312\n",
      "Train Epoch: 236 [110016/225000 (49%)] Loss: 17906.335938\n",
      "Train Epoch: 236 [112512/225000 (50%)] Loss: 17670.417969\n",
      "Train Epoch: 236 [115008/225000 (51%)] Loss: 18554.968750\n",
      "Train Epoch: 236 [117504/225000 (52%)] Loss: 17551.488281\n",
      "Train Epoch: 236 [120000/225000 (53%)] Loss: 17875.093750\n",
      "Train Epoch: 236 [122496/225000 (54%)] Loss: 17880.048828\n",
      "Train Epoch: 236 [124992/225000 (56%)] Loss: 17558.300781\n",
      "Train Epoch: 236 [127488/225000 (57%)] Loss: 17730.968750\n",
      "Train Epoch: 236 [129984/225000 (58%)] Loss: 17815.144531\n",
      "Train Epoch: 236 [132480/225000 (59%)] Loss: 17858.615234\n",
      "Train Epoch: 236 [134976/225000 (60%)] Loss: 17953.589844\n",
      "Train Epoch: 236 [137472/225000 (61%)] Loss: 17754.460938\n",
      "Train Epoch: 236 [139968/225000 (62%)] Loss: 18144.470703\n",
      "Train Epoch: 236 [142464/225000 (63%)] Loss: 18156.632812\n",
      "Train Epoch: 236 [144960/225000 (64%)] Loss: 18349.257812\n",
      "Train Epoch: 236 [147456/225000 (66%)] Loss: 18330.878906\n",
      "Train Epoch: 236 [149952/225000 (67%)] Loss: 18357.076172\n",
      "Train Epoch: 236 [152448/225000 (68%)] Loss: 18226.835938\n",
      "Train Epoch: 236 [154944/225000 (69%)] Loss: 18394.066406\n",
      "Train Epoch: 236 [157440/225000 (70%)] Loss: 18004.431641\n",
      "Train Epoch: 236 [159936/225000 (71%)] Loss: 17532.826172\n",
      "Train Epoch: 236 [162432/225000 (72%)] Loss: 18264.886719\n",
      "Train Epoch: 236 [164928/225000 (73%)] Loss: 18102.156250\n",
      "Train Epoch: 236 [167424/225000 (74%)] Loss: 17830.207031\n",
      "Train Epoch: 236 [169920/225000 (76%)] Loss: 17775.669922\n",
      "Train Epoch: 236 [172416/225000 (77%)] Loss: 17435.691406\n",
      "Train Epoch: 236 [174912/225000 (78%)] Loss: 18186.640625\n",
      "Train Epoch: 236 [177408/225000 (79%)] Loss: 17713.953125\n",
      "Train Epoch: 236 [179904/225000 (80%)] Loss: 18172.855469\n",
      "Train Epoch: 236 [182400/225000 (81%)] Loss: 17918.054688\n",
      "Train Epoch: 236 [184896/225000 (82%)] Loss: 17960.566406\n",
      "Train Epoch: 236 [187392/225000 (83%)] Loss: 18284.628906\n",
      "Train Epoch: 236 [189888/225000 (84%)] Loss: 17895.542969\n",
      "Train Epoch: 236 [192384/225000 (86%)] Loss: 18142.808594\n",
      "Train Epoch: 236 [194880/225000 (87%)] Loss: 17652.507812\n",
      "Train Epoch: 236 [197376/225000 (88%)] Loss: 18075.832031\n",
      "Train Epoch: 236 [199872/225000 (89%)] Loss: 18248.402344\n",
      "Train Epoch: 236 [202368/225000 (90%)] Loss: 17982.544922\n",
      "Train Epoch: 236 [204864/225000 (91%)] Loss: 17925.800781\n",
      "Train Epoch: 236 [207360/225000 (92%)] Loss: 17304.195312\n",
      "Train Epoch: 236 [209856/225000 (93%)] Loss: 17834.257812\n",
      "Train Epoch: 236 [212352/225000 (94%)] Loss: 17825.285156\n",
      "Train Epoch: 236 [214848/225000 (95%)] Loss: 17462.195312\n",
      "Train Epoch: 236 [217344/225000 (97%)] Loss: 17595.781250\n",
      "Train Epoch: 236 [219840/225000 (98%)] Loss: 17830.470703\n",
      "Train Epoch: 236 [222336/225000 (99%)] Loss: 17981.304688\n",
      "Train Epoch: 236 [224832/225000 (100%)] Loss: 17605.365234\n",
      "    epoch          : 236\n",
      "    loss           : 17910.051479508853\n",
      "    val_loss       : 17880.411210937353\n",
      "Train Epoch: 237 [192/225000 (0%)] Loss: 17332.417969\n",
      "Train Epoch: 237 [2688/225000 (1%)] Loss: 18080.419922\n",
      "Train Epoch: 237 [5184/225000 (2%)] Loss: 18092.500000\n",
      "Train Epoch: 237 [7680/225000 (3%)] Loss: 17989.824219\n",
      "Train Epoch: 237 [10176/225000 (5%)] Loss: 17716.437500\n",
      "Train Epoch: 237 [12672/225000 (6%)] Loss: 18080.789062\n",
      "Train Epoch: 237 [15168/225000 (7%)] Loss: 18084.164062\n",
      "Train Epoch: 237 [17664/225000 (8%)] Loss: 17888.273438\n",
      "Train Epoch: 237 [20160/225000 (9%)] Loss: 18003.751953\n",
      "Train Epoch: 237 [22656/225000 (10%)] Loss: 17488.941406\n",
      "Train Epoch: 237 [25152/225000 (11%)] Loss: 17806.009766\n",
      "Train Epoch: 237 [27648/225000 (12%)] Loss: 18000.015625\n",
      "Train Epoch: 237 [30144/225000 (13%)] Loss: 17916.101562\n",
      "Train Epoch: 237 [32640/225000 (15%)] Loss: 17698.074219\n",
      "Train Epoch: 237 [35136/225000 (16%)] Loss: 17810.546875\n",
      "Train Epoch: 237 [37632/225000 (17%)] Loss: 18253.732422\n",
      "Train Epoch: 237 [40128/225000 (18%)] Loss: 17415.806641\n",
      "Train Epoch: 237 [42624/225000 (19%)] Loss: 18134.388672\n",
      "Train Epoch: 237 [45120/225000 (20%)] Loss: 17607.433594\n",
      "Train Epoch: 237 [47616/225000 (21%)] Loss: 17830.609375\n",
      "Train Epoch: 237 [50112/225000 (22%)] Loss: 18401.394531\n",
      "Train Epoch: 237 [52608/225000 (23%)] Loss: 18206.304688\n",
      "Train Epoch: 237 [55104/225000 (24%)] Loss: 17970.410156\n",
      "Train Epoch: 237 [57600/225000 (26%)] Loss: 17712.582031\n",
      "Train Epoch: 237 [60096/225000 (27%)] Loss: 17949.148438\n",
      "Train Epoch: 237 [62592/225000 (28%)] Loss: 17536.906250\n",
      "Train Epoch: 237 [65088/225000 (29%)] Loss: 18561.875000\n",
      "Train Epoch: 237 [67584/225000 (30%)] Loss: 17848.248047\n",
      "Train Epoch: 237 [70080/225000 (31%)] Loss: 17562.285156\n",
      "Train Epoch: 237 [72576/225000 (32%)] Loss: 17904.867188\n",
      "Train Epoch: 237 [75072/225000 (33%)] Loss: 17402.472656\n",
      "Train Epoch: 237 [77568/225000 (34%)] Loss: 17918.148438\n",
      "Train Epoch: 237 [80064/225000 (36%)] Loss: 17877.714844\n",
      "Train Epoch: 237 [82560/225000 (37%)] Loss: 17654.910156\n",
      "Train Epoch: 237 [85056/225000 (38%)] Loss: 17833.078125\n",
      "Train Epoch: 237 [87552/225000 (39%)] Loss: 17832.587891\n",
      "Train Epoch: 237 [90048/225000 (40%)] Loss: 17993.746094\n",
      "Train Epoch: 237 [92544/225000 (41%)] Loss: 17912.156250\n",
      "Train Epoch: 237 [95040/225000 (42%)] Loss: 17597.318359\n",
      "Train Epoch: 237 [97536/225000 (43%)] Loss: 17596.876953\n",
      "Train Epoch: 237 [100032/225000 (44%)] Loss: 17753.660156\n",
      "Train Epoch: 237 [102528/225000 (46%)] Loss: 17458.507812\n",
      "Train Epoch: 237 [105024/225000 (47%)] Loss: 17936.291016\n",
      "Train Epoch: 237 [107520/225000 (48%)] Loss: 18291.261719\n",
      "Train Epoch: 237 [110016/225000 (49%)] Loss: 17913.945312\n",
      "Train Epoch: 237 [112512/225000 (50%)] Loss: 17865.802734\n",
      "Train Epoch: 237 [115008/225000 (51%)] Loss: 17847.910156\n",
      "Train Epoch: 237 [117504/225000 (52%)] Loss: 17571.113281\n",
      "Train Epoch: 237 [120000/225000 (53%)] Loss: 17294.587891\n",
      "Train Epoch: 237 [122496/225000 (54%)] Loss: 18094.496094\n",
      "Train Epoch: 237 [124992/225000 (56%)] Loss: 17497.011719\n",
      "Train Epoch: 237 [127488/225000 (57%)] Loss: 17589.242188\n",
      "Train Epoch: 237 [129984/225000 (58%)] Loss: 18112.513672\n",
      "Train Epoch: 237 [132480/225000 (59%)] Loss: 18239.953125\n",
      "Train Epoch: 237 [134976/225000 (60%)] Loss: 18114.683594\n",
      "Train Epoch: 237 [137472/225000 (61%)] Loss: 17984.406250\n",
      "Train Epoch: 237 [139968/225000 (62%)] Loss: 17665.355469\n",
      "Train Epoch: 237 [142464/225000 (63%)] Loss: 18000.064453\n",
      "Train Epoch: 237 [144960/225000 (64%)] Loss: 17915.238281\n",
      "Train Epoch: 237 [147456/225000 (66%)] Loss: 17751.570312\n",
      "Train Epoch: 237 [149952/225000 (67%)] Loss: 17823.496094\n",
      "Train Epoch: 237 [152448/225000 (68%)] Loss: 18083.650391\n",
      "Train Epoch: 237 [154944/225000 (69%)] Loss: 17829.781250\n",
      "Train Epoch: 237 [157440/225000 (70%)] Loss: 17886.998047\n",
      "Train Epoch: 237 [159936/225000 (71%)] Loss: 18226.267578\n",
      "Train Epoch: 237 [162432/225000 (72%)] Loss: 17739.380859\n",
      "Train Epoch: 237 [164928/225000 (73%)] Loss: 17880.726562\n",
      "Train Epoch: 237 [167424/225000 (74%)] Loss: 17545.390625\n",
      "Train Epoch: 237 [169920/225000 (76%)] Loss: 17926.826172\n",
      "Train Epoch: 237 [172416/225000 (77%)] Loss: 17707.523438\n",
      "Train Epoch: 237 [174912/225000 (78%)] Loss: 17865.650391\n",
      "Train Epoch: 237 [177408/225000 (79%)] Loss: 17677.152344\n",
      "Train Epoch: 237 [179904/225000 (80%)] Loss: 18291.828125\n",
      "Train Epoch: 237 [182400/225000 (81%)] Loss: 17357.976562\n",
      "Train Epoch: 237 [184896/225000 (82%)] Loss: 17041.050781\n",
      "Train Epoch: 237 [187392/225000 (83%)] Loss: 17940.871094\n",
      "Train Epoch: 237 [189888/225000 (84%)] Loss: 17728.509766\n",
      "Train Epoch: 237 [192384/225000 (86%)] Loss: 18315.050781\n",
      "Train Epoch: 237 [194880/225000 (87%)] Loss: 17765.625000\n",
      "Train Epoch: 237 [197376/225000 (88%)] Loss: 17822.941406\n",
      "Train Epoch: 237 [199872/225000 (89%)] Loss: 18011.800781\n",
      "Train Epoch: 237 [202368/225000 (90%)] Loss: 18220.578125\n",
      "Train Epoch: 237 [204864/225000 (91%)] Loss: 17540.433594\n",
      "Train Epoch: 237 [207360/225000 (92%)] Loss: 17962.929688\n",
      "Train Epoch: 237 [209856/225000 (93%)] Loss: 18113.035156\n",
      "Train Epoch: 237 [212352/225000 (94%)] Loss: 17962.880859\n",
      "Train Epoch: 237 [214848/225000 (95%)] Loss: 18470.097656\n",
      "Train Epoch: 237 [217344/225000 (97%)] Loss: 17677.683594\n",
      "Train Epoch: 237 [219840/225000 (98%)] Loss: 17431.488281\n",
      "Train Epoch: 237 [222336/225000 (99%)] Loss: 17674.548828\n",
      "Train Epoch: 237 [224832/225000 (100%)] Loss: 18222.539062\n",
      "    epoch          : 237\n",
      "    loss           : 17910.84137025384\n",
      "    val_loss       : 17834.9960170957\n",
      "Train Epoch: 238 [192/225000 (0%)] Loss: 17827.390625\n",
      "Train Epoch: 238 [2688/225000 (1%)] Loss: 18162.320312\n",
      "Train Epoch: 238 [5184/225000 (2%)] Loss: 17836.167969\n",
      "Train Epoch: 238 [7680/225000 (3%)] Loss: 18257.128906\n",
      "Train Epoch: 238 [10176/225000 (5%)] Loss: 17778.160156\n",
      "Train Epoch: 238 [12672/225000 (6%)] Loss: 18236.179688\n",
      "Train Epoch: 238 [15168/225000 (7%)] Loss: 18371.304688\n",
      "Train Epoch: 238 [17664/225000 (8%)] Loss: 17501.330078\n",
      "Train Epoch: 238 [20160/225000 (9%)] Loss: 17926.601562\n",
      "Train Epoch: 238 [22656/225000 (10%)] Loss: 17073.666016\n",
      "Train Epoch: 238 [25152/225000 (11%)] Loss: 18158.914062\n",
      "Train Epoch: 238 [27648/225000 (12%)] Loss: 17765.830078\n",
      "Train Epoch: 238 [30144/225000 (13%)] Loss: 17853.660156\n",
      "Train Epoch: 238 [32640/225000 (15%)] Loss: 18062.580078\n",
      "Train Epoch: 238 [35136/225000 (16%)] Loss: 18116.630859\n",
      "Train Epoch: 238 [37632/225000 (17%)] Loss: 18333.687500\n",
      "Train Epoch: 238 [40128/225000 (18%)] Loss: 18319.134766\n",
      "Train Epoch: 238 [42624/225000 (19%)] Loss: 18250.921875\n",
      "Train Epoch: 238 [45120/225000 (20%)] Loss: 17992.527344\n",
      "Train Epoch: 238 [47616/225000 (21%)] Loss: 17689.605469\n",
      "Train Epoch: 238 [50112/225000 (22%)] Loss: 17295.144531\n",
      "Train Epoch: 238 [52608/225000 (23%)] Loss: 17770.376953\n",
      "Train Epoch: 238 [55104/225000 (24%)] Loss: 17927.003906\n",
      "Train Epoch: 238 [57600/225000 (26%)] Loss: 17509.718750\n",
      "Train Epoch: 238 [60096/225000 (27%)] Loss: 18318.154297\n",
      "Train Epoch: 238 [62592/225000 (28%)] Loss: 17308.023438\n",
      "Train Epoch: 238 [65088/225000 (29%)] Loss: 17339.896484\n",
      "Train Epoch: 238 [67584/225000 (30%)] Loss: 17998.849609\n",
      "Train Epoch: 238 [70080/225000 (31%)] Loss: 17864.521484\n",
      "Train Epoch: 238 [72576/225000 (32%)] Loss: 18162.695312\n",
      "Train Epoch: 238 [75072/225000 (33%)] Loss: 18221.863281\n",
      "Train Epoch: 238 [77568/225000 (34%)] Loss: 17733.861328\n",
      "Train Epoch: 238 [80064/225000 (36%)] Loss: 17269.921875\n",
      "Train Epoch: 238 [82560/225000 (37%)] Loss: 18142.871094\n",
      "Train Epoch: 238 [85056/225000 (38%)] Loss: 17793.458984\n",
      "Train Epoch: 238 [87552/225000 (39%)] Loss: 17824.003906\n",
      "Train Epoch: 238 [90048/225000 (40%)] Loss: 18061.199219\n",
      "Train Epoch: 238 [92544/225000 (41%)] Loss: 18056.886719\n",
      "Train Epoch: 238 [95040/225000 (42%)] Loss: 17504.406250\n",
      "Train Epoch: 238 [97536/225000 (43%)] Loss: 17913.679688\n",
      "Train Epoch: 238 [100032/225000 (44%)] Loss: 18153.396484\n",
      "Train Epoch: 238 [102528/225000 (46%)] Loss: 17881.707031\n",
      "Train Epoch: 238 [105024/225000 (47%)] Loss: 17763.503906\n",
      "Train Epoch: 238 [107520/225000 (48%)] Loss: 17881.437500\n",
      "Train Epoch: 238 [110016/225000 (49%)] Loss: 18496.550781\n",
      "Train Epoch: 238 [112512/225000 (50%)] Loss: 17952.314453\n",
      "Train Epoch: 238 [115008/225000 (51%)] Loss: 18241.976562\n",
      "Train Epoch: 238 [117504/225000 (52%)] Loss: 18345.113281\n",
      "Train Epoch: 238 [120000/225000 (53%)] Loss: 17760.988281\n",
      "Train Epoch: 238 [122496/225000 (54%)] Loss: 17696.289062\n",
      "Train Epoch: 238 [124992/225000 (56%)] Loss: 18318.593750\n",
      "Train Epoch: 238 [127488/225000 (57%)] Loss: 17671.697266\n",
      "Train Epoch: 238 [129984/225000 (58%)] Loss: 18317.166016\n",
      "Train Epoch: 238 [132480/225000 (59%)] Loss: 17266.554688\n",
      "Train Epoch: 238 [134976/225000 (60%)] Loss: 18431.609375\n",
      "Train Epoch: 238 [137472/225000 (61%)] Loss: 17645.925781\n",
      "Train Epoch: 238 [139968/225000 (62%)] Loss: 17655.037109\n",
      "Train Epoch: 238 [142464/225000 (63%)] Loss: 18290.734375\n",
      "Train Epoch: 238 [144960/225000 (64%)] Loss: 17835.634766\n",
      "Train Epoch: 238 [147456/225000 (66%)] Loss: 18097.257812\n",
      "Train Epoch: 238 [149952/225000 (67%)] Loss: 17582.228516\n",
      "Train Epoch: 238 [152448/225000 (68%)] Loss: 17621.253906\n",
      "Train Epoch: 238 [154944/225000 (69%)] Loss: 18274.173828\n",
      "Train Epoch: 238 [157440/225000 (70%)] Loss: 18448.988281\n",
      "Train Epoch: 238 [159936/225000 (71%)] Loss: 18081.582031\n",
      "Train Epoch: 238 [162432/225000 (72%)] Loss: 17959.679688\n",
      "Train Epoch: 238 [164928/225000 (73%)] Loss: 17849.968750\n",
      "Train Epoch: 238 [167424/225000 (74%)] Loss: 17732.359375\n",
      "Train Epoch: 238 [169920/225000 (76%)] Loss: 17613.722656\n",
      "Train Epoch: 238 [172416/225000 (77%)] Loss: 17405.671875\n",
      "Train Epoch: 238 [174912/225000 (78%)] Loss: 18183.808594\n",
      "Train Epoch: 238 [177408/225000 (79%)] Loss: 17917.451172\n",
      "Train Epoch: 238 [179904/225000 (80%)] Loss: 17959.949219\n",
      "Train Epoch: 238 [182400/225000 (81%)] Loss: 17785.937500\n",
      "Train Epoch: 238 [184896/225000 (82%)] Loss: 17826.085938\n",
      "Train Epoch: 238 [187392/225000 (83%)] Loss: 17600.078125\n",
      "Train Epoch: 238 [189888/225000 (84%)] Loss: 17815.755859\n",
      "Train Epoch: 238 [192384/225000 (86%)] Loss: 17819.628906\n",
      "Train Epoch: 238 [194880/225000 (87%)] Loss: 18015.765625\n",
      "Train Epoch: 238 [197376/225000 (88%)] Loss: 18379.574219\n",
      "Train Epoch: 238 [199872/225000 (89%)] Loss: 18365.765625\n",
      "Train Epoch: 238 [202368/225000 (90%)] Loss: 17699.285156\n",
      "Train Epoch: 238 [204864/225000 (91%)] Loss: 18007.515625\n",
      "Train Epoch: 238 [207360/225000 (92%)] Loss: 17860.109375\n",
      "Train Epoch: 238 [209856/225000 (93%)] Loss: 17556.242188\n",
      "Train Epoch: 238 [212352/225000 (94%)] Loss: 17685.082031\n",
      "Train Epoch: 238 [214848/225000 (95%)] Loss: 17379.173828\n",
      "Train Epoch: 238 [217344/225000 (97%)] Loss: 17568.757812\n",
      "Train Epoch: 238 [219840/225000 (98%)] Loss: 17859.982422\n",
      "Train Epoch: 238 [222336/225000 (99%)] Loss: 17568.390625\n",
      "Train Epoch: 238 [224832/225000 (100%)] Loss: 18210.570312\n",
      "    epoch          : 238\n",
      "    loss           : 17890.886272964217\n",
      "    val_loss       : 17816.641623143933\n",
      "Train Epoch: 239 [192/225000 (0%)] Loss: 17746.447266\n",
      "Train Epoch: 239 [2688/225000 (1%)] Loss: 17878.859375\n",
      "Train Epoch: 239 [5184/225000 (2%)] Loss: 18140.433594\n",
      "Train Epoch: 239 [7680/225000 (3%)] Loss: 17872.437500\n",
      "Train Epoch: 239 [10176/225000 (5%)] Loss: 18294.687500\n",
      "Train Epoch: 239 [12672/225000 (6%)] Loss: 17826.220703\n",
      "Train Epoch: 239 [15168/225000 (7%)] Loss: 17727.583984\n",
      "Train Epoch: 239 [17664/225000 (8%)] Loss: 17975.687500\n",
      "Train Epoch: 239 [20160/225000 (9%)] Loss: 18240.363281\n",
      "Train Epoch: 239 [22656/225000 (10%)] Loss: 17579.871094\n",
      "Train Epoch: 239 [25152/225000 (11%)] Loss: 18133.890625\n",
      "Train Epoch: 239 [27648/225000 (12%)] Loss: 18103.082031\n",
      "Train Epoch: 239 [30144/225000 (13%)] Loss: 18014.703125\n",
      "Train Epoch: 239 [32640/225000 (15%)] Loss: 18214.101562\n",
      "Train Epoch: 239 [35136/225000 (16%)] Loss: 18053.707031\n",
      "Train Epoch: 239 [37632/225000 (17%)] Loss: 17858.757812\n",
      "Train Epoch: 239 [40128/225000 (18%)] Loss: 17763.089844\n",
      "Train Epoch: 239 [42624/225000 (19%)] Loss: 18087.253906\n",
      "Train Epoch: 239 [45120/225000 (20%)] Loss: 17881.080078\n",
      "Train Epoch: 239 [47616/225000 (21%)] Loss: 17885.335938\n",
      "Train Epoch: 239 [50112/225000 (22%)] Loss: 17939.722656\n",
      "Train Epoch: 239 [52608/225000 (23%)] Loss: 17768.136719\n",
      "Train Epoch: 239 [55104/225000 (24%)] Loss: 17978.093750\n",
      "Train Epoch: 239 [57600/225000 (26%)] Loss: 18277.937500\n",
      "Train Epoch: 239 [60096/225000 (27%)] Loss: 18031.765625\n",
      "Train Epoch: 239 [62592/225000 (28%)] Loss: 17846.902344\n",
      "Train Epoch: 239 [65088/225000 (29%)] Loss: 17871.730469\n",
      "Train Epoch: 239 [67584/225000 (30%)] Loss: 17871.169922\n",
      "Train Epoch: 239 [70080/225000 (31%)] Loss: 17400.757812\n",
      "Train Epoch: 239 [72576/225000 (32%)] Loss: 17438.652344\n",
      "Train Epoch: 239 [75072/225000 (33%)] Loss: 17727.435547\n",
      "Train Epoch: 239 [77568/225000 (34%)] Loss: 17950.109375\n",
      "Train Epoch: 239 [80064/225000 (36%)] Loss: 17418.394531\n",
      "Train Epoch: 239 [82560/225000 (37%)] Loss: 17744.128906\n",
      "Train Epoch: 239 [85056/225000 (38%)] Loss: 17583.507812\n",
      "Train Epoch: 239 [87552/225000 (39%)] Loss: 18189.960938\n",
      "Train Epoch: 239 [90048/225000 (40%)] Loss: 17685.082031\n",
      "Train Epoch: 239 [92544/225000 (41%)] Loss: 17859.953125\n",
      "Train Epoch: 239 [95040/225000 (42%)] Loss: 17939.933594\n",
      "Train Epoch: 239 [97536/225000 (43%)] Loss: 17764.117188\n",
      "Train Epoch: 239 [100032/225000 (44%)] Loss: 17562.765625\n",
      "Train Epoch: 239 [102528/225000 (46%)] Loss: 17732.921875\n",
      "Train Epoch: 239 [105024/225000 (47%)] Loss: 17594.824219\n",
      "Train Epoch: 239 [107520/225000 (48%)] Loss: 17913.847656\n",
      "Train Epoch: 239 [110016/225000 (49%)] Loss: 18078.027344\n",
      "Train Epoch: 239 [112512/225000 (50%)] Loss: 17757.480469\n",
      "Train Epoch: 239 [115008/225000 (51%)] Loss: 17597.978516\n",
      "Train Epoch: 239 [117504/225000 (52%)] Loss: 17988.820312\n",
      "Train Epoch: 239 [120000/225000 (53%)] Loss: 17821.324219\n",
      "Train Epoch: 239 [122496/225000 (54%)] Loss: 17811.511719\n",
      "Train Epoch: 239 [124992/225000 (56%)] Loss: 17676.863281\n",
      "Train Epoch: 239 [127488/225000 (57%)] Loss: 17668.578125\n",
      "Train Epoch: 239 [129984/225000 (58%)] Loss: 17910.201172\n",
      "Train Epoch: 239 [132480/225000 (59%)] Loss: 17752.179688\n",
      "Train Epoch: 239 [134976/225000 (60%)] Loss: 17605.050781\n",
      "Train Epoch: 239 [137472/225000 (61%)] Loss: 17090.507812\n",
      "Train Epoch: 239 [139968/225000 (62%)] Loss: 17636.994141\n",
      "Train Epoch: 239 [142464/225000 (63%)] Loss: 18027.144531\n",
      "Train Epoch: 239 [144960/225000 (64%)] Loss: 18245.175781\n",
      "Train Epoch: 239 [147456/225000 (66%)] Loss: 17654.367188\n",
      "Train Epoch: 239 [149952/225000 (67%)] Loss: 18265.468750\n",
      "Train Epoch: 239 [152448/225000 (68%)] Loss: 17592.066406\n",
      "Train Epoch: 239 [154944/225000 (69%)] Loss: 17816.986328\n",
      "Train Epoch: 239 [157440/225000 (70%)] Loss: 17896.191406\n",
      "Train Epoch: 239 [159936/225000 (71%)] Loss: 17773.201172\n",
      "Train Epoch: 239 [162432/225000 (72%)] Loss: 18231.218750\n",
      "Train Epoch: 239 [164928/225000 (73%)] Loss: 17933.716797\n",
      "Train Epoch: 239 [167424/225000 (74%)] Loss: 17960.335938\n",
      "Train Epoch: 239 [169920/225000 (76%)] Loss: 17792.718750\n",
      "Train Epoch: 239 [172416/225000 (77%)] Loss: 18053.148438\n",
      "Train Epoch: 239 [174912/225000 (78%)] Loss: 17588.757812\n",
      "Train Epoch: 239 [177408/225000 (79%)] Loss: 18215.265625\n",
      "Train Epoch: 239 [179904/225000 (80%)] Loss: 17560.707031\n",
      "Train Epoch: 239 [182400/225000 (81%)] Loss: 17862.468750\n",
      "Train Epoch: 239 [184896/225000 (82%)] Loss: 17924.515625\n",
      "Train Epoch: 239 [187392/225000 (83%)] Loss: 17328.025391\n",
      "Train Epoch: 239 [189888/225000 (84%)] Loss: 18194.574219\n",
      "Train Epoch: 239 [192384/225000 (86%)] Loss: 17478.234375\n",
      "Train Epoch: 239 [194880/225000 (87%)] Loss: 17432.046875\n",
      "Train Epoch: 239 [197376/225000 (88%)] Loss: 17928.925781\n",
      "Train Epoch: 239 [199872/225000 (89%)] Loss: 17869.412109\n",
      "Train Epoch: 239 [202368/225000 (90%)] Loss: 17883.263672\n",
      "Train Epoch: 239 [204864/225000 (91%)] Loss: 17882.037109\n",
      "Train Epoch: 239 [207360/225000 (92%)] Loss: 17425.949219\n",
      "Train Epoch: 239 [209856/225000 (93%)] Loss: 17529.898438\n",
      "Train Epoch: 239 [212352/225000 (94%)] Loss: 17957.732422\n",
      "Train Epoch: 239 [214848/225000 (95%)] Loss: 17633.007812\n",
      "Train Epoch: 239 [217344/225000 (97%)] Loss: 17685.488281\n",
      "Train Epoch: 239 [219840/225000 (98%)] Loss: 17797.117188\n",
      "Train Epoch: 239 [222336/225000 (99%)] Loss: 17982.386719\n",
      "Train Epoch: 239 [224832/225000 (100%)] Loss: 17386.324219\n",
      "    epoch          : 239\n",
      "    loss           : 17873.811320125853\n",
      "    val_loss       : 17813.614545807584\n",
      "Train Epoch: 240 [192/225000 (0%)] Loss: 17842.820312\n",
      "Train Epoch: 240 [2688/225000 (1%)] Loss: 17338.191406\n",
      "Train Epoch: 240 [5184/225000 (2%)] Loss: 17941.673828\n",
      "Train Epoch: 240 [7680/225000 (3%)] Loss: 17905.363281\n",
      "Train Epoch: 240 [10176/225000 (5%)] Loss: 18083.027344\n",
      "Train Epoch: 240 [12672/225000 (6%)] Loss: 17911.769531\n",
      "Train Epoch: 240 [15168/225000 (7%)] Loss: 17605.216797\n",
      "Train Epoch: 240 [17664/225000 (8%)] Loss: 17759.982422\n",
      "Train Epoch: 240 [20160/225000 (9%)] Loss: 17792.875000\n",
      "Train Epoch: 240 [22656/225000 (10%)] Loss: 18048.664062\n",
      "Train Epoch: 240 [25152/225000 (11%)] Loss: 17709.890625\n",
      "Train Epoch: 240 [27648/225000 (12%)] Loss: 17924.097656\n",
      "Train Epoch: 240 [30144/225000 (13%)] Loss: 17814.265625\n",
      "Train Epoch: 240 [32640/225000 (15%)] Loss: 17773.746094\n",
      "Train Epoch: 240 [35136/225000 (16%)] Loss: 17688.875000\n",
      "Train Epoch: 240 [37632/225000 (17%)] Loss: 17849.597656\n",
      "Train Epoch: 240 [40128/225000 (18%)] Loss: 17744.623047\n",
      "Train Epoch: 240 [42624/225000 (19%)] Loss: 17707.677734\n",
      "Train Epoch: 240 [45120/225000 (20%)] Loss: 17873.105469\n",
      "Train Epoch: 240 [47616/225000 (21%)] Loss: 17527.406250\n",
      "Train Epoch: 240 [50112/225000 (22%)] Loss: 18236.222656\n",
      "Train Epoch: 240 [52608/225000 (23%)] Loss: 17345.259766\n",
      "Train Epoch: 240 [55104/225000 (24%)] Loss: 17642.527344\n",
      "Train Epoch: 240 [57600/225000 (26%)] Loss: 17949.275391\n",
      "Train Epoch: 240 [60096/225000 (27%)] Loss: 17829.667969\n",
      "Train Epoch: 240 [62592/225000 (28%)] Loss: 18060.363281\n",
      "Train Epoch: 240 [65088/225000 (29%)] Loss: 17995.582031\n",
      "Train Epoch: 240 [67584/225000 (30%)] Loss: 17730.808594\n",
      "Train Epoch: 240 [70080/225000 (31%)] Loss: 18152.898438\n",
      "Train Epoch: 240 [72576/225000 (32%)] Loss: 17806.839844\n",
      "Train Epoch: 240 [75072/225000 (33%)] Loss: 17971.500000\n",
      "Train Epoch: 240 [77568/225000 (34%)] Loss: 17562.738281\n",
      "Train Epoch: 240 [80064/225000 (36%)] Loss: 17940.707031\n",
      "Train Epoch: 240 [82560/225000 (37%)] Loss: 18319.757812\n",
      "Train Epoch: 240 [85056/225000 (38%)] Loss: 18307.160156\n",
      "Train Epoch: 240 [87552/225000 (39%)] Loss: 17722.019531\n",
      "Train Epoch: 240 [90048/225000 (40%)] Loss: 18148.750000\n",
      "Train Epoch: 240 [92544/225000 (41%)] Loss: 17479.355469\n",
      "Train Epoch: 240 [95040/225000 (42%)] Loss: 17739.750000\n",
      "Train Epoch: 240 [97536/225000 (43%)] Loss: 17551.003906\n",
      "Train Epoch: 240 [100032/225000 (44%)] Loss: 17926.503906\n",
      "Train Epoch: 240 [102528/225000 (46%)] Loss: 18201.441406\n",
      "Train Epoch: 240 [105024/225000 (47%)] Loss: 18035.849609\n",
      "Train Epoch: 240 [107520/225000 (48%)] Loss: 17948.751953\n",
      "Train Epoch: 240 [110016/225000 (49%)] Loss: 17870.144531\n",
      "Train Epoch: 240 [112512/225000 (50%)] Loss: 17939.365234\n",
      "Train Epoch: 240 [115008/225000 (51%)] Loss: 17845.460938\n",
      "Train Epoch: 240 [117504/225000 (52%)] Loss: 17931.927734\n",
      "Train Epoch: 240 [120000/225000 (53%)] Loss: 17700.355469\n",
      "Train Epoch: 240 [122496/225000 (54%)] Loss: 17633.855469\n",
      "Train Epoch: 240 [124992/225000 (56%)] Loss: 18119.865234\n",
      "Train Epoch: 240 [127488/225000 (57%)] Loss: 17950.800781\n",
      "Train Epoch: 240 [129984/225000 (58%)] Loss: 17851.451172\n",
      "Train Epoch: 240 [132480/225000 (59%)] Loss: 17672.876953\n",
      "Train Epoch: 240 [134976/225000 (60%)] Loss: 18016.839844\n",
      "Train Epoch: 240 [137472/225000 (61%)] Loss: 17655.945312\n",
      "Train Epoch: 240 [139968/225000 (62%)] Loss: 17813.429688\n",
      "Train Epoch: 240 [142464/225000 (63%)] Loss: 18263.148438\n",
      "Train Epoch: 240 [144960/225000 (64%)] Loss: 18137.589844\n",
      "Train Epoch: 240 [147456/225000 (66%)] Loss: 17485.164062\n",
      "Train Epoch: 240 [149952/225000 (67%)] Loss: 17527.761719\n",
      "Train Epoch: 240 [152448/225000 (68%)] Loss: 17339.082031\n",
      "Train Epoch: 240 [154944/225000 (69%)] Loss: 18115.322266\n",
      "Train Epoch: 240 [157440/225000 (70%)] Loss: 17789.791016\n",
      "Train Epoch: 240 [159936/225000 (71%)] Loss: 17664.539062\n",
      "Train Epoch: 240 [162432/225000 (72%)] Loss: 17908.230469\n",
      "Train Epoch: 240 [164928/225000 (73%)] Loss: 17699.044922\n",
      "Train Epoch: 240 [167424/225000 (74%)] Loss: 17983.000000\n",
      "Train Epoch: 240 [169920/225000 (76%)] Loss: 18068.562500\n",
      "Train Epoch: 240 [172416/225000 (77%)] Loss: 18245.128906\n",
      "Train Epoch: 240 [174912/225000 (78%)] Loss: 17794.416016\n",
      "Train Epoch: 240 [177408/225000 (79%)] Loss: 17982.343750\n",
      "Train Epoch: 240 [179904/225000 (80%)] Loss: 18084.457031\n",
      "Train Epoch: 240 [182400/225000 (81%)] Loss: 18458.265625\n",
      "Train Epoch: 240 [184896/225000 (82%)] Loss: 17973.589844\n",
      "Train Epoch: 240 [187392/225000 (83%)] Loss: 17777.521484\n",
      "Train Epoch: 240 [189888/225000 (84%)] Loss: 17622.544922\n",
      "Train Epoch: 240 [192384/225000 (86%)] Loss: 18035.515625\n",
      "Train Epoch: 240 [194880/225000 (87%)] Loss: 17520.089844\n",
      "Train Epoch: 240 [197376/225000 (88%)] Loss: 17653.671875\n",
      "Train Epoch: 240 [199872/225000 (89%)] Loss: 17505.765625\n",
      "Train Epoch: 240 [202368/225000 (90%)] Loss: 17988.806641\n",
      "Train Epoch: 240 [204864/225000 (91%)] Loss: 17343.423828\n",
      "Train Epoch: 240 [207360/225000 (92%)] Loss: 17666.697266\n",
      "Train Epoch: 240 [209856/225000 (93%)] Loss: 17861.359375\n",
      "Train Epoch: 240 [212352/225000 (94%)] Loss: 18092.609375\n",
      "Train Epoch: 240 [214848/225000 (95%)] Loss: 17854.964844\n",
      "Train Epoch: 240 [217344/225000 (97%)] Loss: 17823.957031\n",
      "Train Epoch: 240 [219840/225000 (98%)] Loss: 17929.753906\n",
      "Train Epoch: 240 [222336/225000 (99%)] Loss: 17625.392578\n",
      "Train Epoch: 240 [224832/225000 (100%)] Loss: 17676.958984\n",
      "    epoch          : 240\n",
      "    loss           : 17877.502900523945\n",
      "    val_loss       : 17798.035999551983\n",
      "Train Epoch: 241 [192/225000 (0%)] Loss: 17846.298828\n",
      "Train Epoch: 241 [2688/225000 (1%)] Loss: 17705.074219\n",
      "Train Epoch: 241 [5184/225000 (2%)] Loss: 17420.675781\n",
      "Train Epoch: 241 [7680/225000 (3%)] Loss: 18035.695312\n",
      "Train Epoch: 241 [10176/225000 (5%)] Loss: 17965.798828\n",
      "Train Epoch: 241 [12672/225000 (6%)] Loss: 18087.156250\n",
      "Train Epoch: 241 [15168/225000 (7%)] Loss: 18111.265625\n",
      "Train Epoch: 241 [17664/225000 (8%)] Loss: 17618.738281\n",
      "Train Epoch: 241 [20160/225000 (9%)] Loss: 18006.542969\n",
      "Train Epoch: 241 [22656/225000 (10%)] Loss: 17786.019531\n",
      "Train Epoch: 241 [25152/225000 (11%)] Loss: 18204.980469\n",
      "Train Epoch: 241 [27648/225000 (12%)] Loss: 18046.494141\n",
      "Train Epoch: 241 [30144/225000 (13%)] Loss: 17763.660156\n",
      "Train Epoch: 241 [32640/225000 (15%)] Loss: 17951.734375\n",
      "Train Epoch: 241 [35136/225000 (16%)] Loss: 18043.843750\n",
      "Train Epoch: 241 [37632/225000 (17%)] Loss: 18224.257812\n",
      "Train Epoch: 241 [40128/225000 (18%)] Loss: 17301.421875\n",
      "Train Epoch: 241 [42624/225000 (19%)] Loss: 17318.421875\n",
      "Train Epoch: 241 [45120/225000 (20%)] Loss: 18060.761719\n",
      "Train Epoch: 241 [47616/225000 (21%)] Loss: 18129.001953\n",
      "Train Epoch: 241 [50112/225000 (22%)] Loss: 17923.820312\n",
      "Train Epoch: 241 [52608/225000 (23%)] Loss: 18165.183594\n",
      "Train Epoch: 241 [55104/225000 (24%)] Loss: 18001.378906\n",
      "Train Epoch: 241 [57600/225000 (26%)] Loss: 18256.431641\n",
      "Train Epoch: 241 [60096/225000 (27%)] Loss: 17831.585938\n",
      "Train Epoch: 241 [62592/225000 (28%)] Loss: 18238.001953\n",
      "Train Epoch: 241 [65088/225000 (29%)] Loss: 17599.281250\n",
      "Train Epoch: 241 [67584/225000 (30%)] Loss: 17809.121094\n",
      "Train Epoch: 241 [70080/225000 (31%)] Loss: 17881.878906\n",
      "Train Epoch: 241 [72576/225000 (32%)] Loss: 18255.765625\n",
      "Train Epoch: 241 [75072/225000 (33%)] Loss: 17681.054688\n",
      "Train Epoch: 241 [77568/225000 (34%)] Loss: 17946.011719\n",
      "Train Epoch: 241 [80064/225000 (36%)] Loss: 18112.992188\n",
      "Train Epoch: 241 [82560/225000 (37%)] Loss: 17481.994141\n",
      "Train Epoch: 241 [85056/225000 (38%)] Loss: 17865.984375\n",
      "Train Epoch: 241 [87552/225000 (39%)] Loss: 17712.378906\n",
      "Train Epoch: 241 [90048/225000 (40%)] Loss: 17731.167969\n",
      "Train Epoch: 241 [92544/225000 (41%)] Loss: 17965.964844\n",
      "Train Epoch: 241 [95040/225000 (42%)] Loss: 17639.707031\n",
      "Train Epoch: 241 [97536/225000 (43%)] Loss: 17588.833984\n",
      "Train Epoch: 241 [100032/225000 (44%)] Loss: 18282.804688\n",
      "Train Epoch: 241 [102528/225000 (46%)] Loss: 18050.816406\n",
      "Train Epoch: 241 [105024/225000 (47%)] Loss: 17803.574219\n",
      "Train Epoch: 241 [107520/225000 (48%)] Loss: 18280.472656\n",
      "Train Epoch: 241 [110016/225000 (49%)] Loss: 17847.048828\n",
      "Train Epoch: 241 [112512/225000 (50%)] Loss: 18077.287109\n",
      "Train Epoch: 241 [115008/225000 (51%)] Loss: 17891.705078\n",
      "Train Epoch: 241 [117504/225000 (52%)] Loss: 17595.593750\n",
      "Train Epoch: 241 [120000/225000 (53%)] Loss: 18039.894531\n",
      "Train Epoch: 241 [122496/225000 (54%)] Loss: 17941.970703\n",
      "Train Epoch: 241 [124992/225000 (56%)] Loss: 18025.537109\n",
      "Train Epoch: 241 [127488/225000 (57%)] Loss: 17579.396484\n",
      "Train Epoch: 241 [129984/225000 (58%)] Loss: 18169.591797\n",
      "Train Epoch: 241 [132480/225000 (59%)] Loss: 17606.718750\n",
      "Train Epoch: 241 [134976/225000 (60%)] Loss: 17921.863281\n",
      "Train Epoch: 241 [137472/225000 (61%)] Loss: 17557.583984\n",
      "Train Epoch: 241 [139968/225000 (62%)] Loss: 17541.097656\n",
      "Train Epoch: 241 [142464/225000 (63%)] Loss: 17826.746094\n",
      "Train Epoch: 241 [144960/225000 (64%)] Loss: 18489.947266\n",
      "Train Epoch: 241 [147456/225000 (66%)] Loss: 17542.585938\n",
      "Train Epoch: 241 [149952/225000 (67%)] Loss: 17651.898438\n",
      "Train Epoch: 241 [152448/225000 (68%)] Loss: 17483.750000\n",
      "Train Epoch: 241 [154944/225000 (69%)] Loss: 17751.574219\n",
      "Train Epoch: 241 [157440/225000 (70%)] Loss: 17773.570312\n",
      "Train Epoch: 241 [159936/225000 (71%)] Loss: 17690.996094\n",
      "Train Epoch: 241 [162432/225000 (72%)] Loss: 17550.878906\n",
      "Train Epoch: 241 [164928/225000 (73%)] Loss: 17186.687500\n",
      "Train Epoch: 241 [167424/225000 (74%)] Loss: 17308.265625\n",
      "Train Epoch: 241 [169920/225000 (76%)] Loss: 17801.876953\n",
      "Train Epoch: 241 [172416/225000 (77%)] Loss: 17464.695312\n",
      "Train Epoch: 241 [174912/225000 (78%)] Loss: 17640.291016\n",
      "Train Epoch: 241 [177408/225000 (79%)] Loss: 17803.140625\n",
      "Train Epoch: 241 [179904/225000 (80%)] Loss: 17328.064453\n",
      "Train Epoch: 241 [182400/225000 (81%)] Loss: 18039.441406\n",
      "Train Epoch: 241 [184896/225000 (82%)] Loss: 18042.867188\n",
      "Train Epoch: 241 [187392/225000 (83%)] Loss: 17803.851562\n",
      "Train Epoch: 241 [189888/225000 (84%)] Loss: 17543.892578\n",
      "Train Epoch: 241 [192384/225000 (86%)] Loss: 18132.941406\n",
      "Train Epoch: 241 [194880/225000 (87%)] Loss: 17647.988281\n",
      "Train Epoch: 241 [197376/225000 (88%)] Loss: 17787.697266\n",
      "Train Epoch: 241 [199872/225000 (89%)] Loss: 17965.142578\n",
      "Train Epoch: 241 [202368/225000 (90%)] Loss: 17993.386719\n",
      "Train Epoch: 241 [204864/225000 (91%)] Loss: 17629.773438\n",
      "Train Epoch: 241 [207360/225000 (92%)] Loss: 17168.652344\n",
      "Train Epoch: 241 [209856/225000 (93%)] Loss: 18117.230469\n",
      "Train Epoch: 241 [212352/225000 (94%)] Loss: 18231.136719\n",
      "Train Epoch: 241 [214848/225000 (95%)] Loss: 17845.685547\n",
      "Train Epoch: 241 [217344/225000 (97%)] Loss: 17662.097656\n",
      "Train Epoch: 241 [219840/225000 (98%)] Loss: 17519.492188\n",
      "Train Epoch: 241 [222336/225000 (99%)] Loss: 17512.281250\n",
      "Train Epoch: 241 [224832/225000 (100%)] Loss: 18086.050781\n",
      "    epoch          : 241\n",
      "    loss           : 17850.145312000055\n",
      "    val_loss       : 17772.488176247545\n",
      "Train Epoch: 242 [192/225000 (0%)] Loss: 17409.673828\n",
      "Train Epoch: 242 [2688/225000 (1%)] Loss: 18455.113281\n",
      "Train Epoch: 242 [5184/225000 (2%)] Loss: 17997.826172\n",
      "Train Epoch: 242 [7680/225000 (3%)] Loss: 17850.716797\n",
      "Train Epoch: 242 [10176/225000 (5%)] Loss: 18037.101562\n",
      "Train Epoch: 242 [12672/225000 (6%)] Loss: 17279.390625\n",
      "Train Epoch: 242 [15168/225000 (7%)] Loss: 18274.773438\n",
      "Train Epoch: 242 [17664/225000 (8%)] Loss: 17787.302734\n",
      "Train Epoch: 242 [20160/225000 (9%)] Loss: 17636.519531\n",
      "Train Epoch: 242 [22656/225000 (10%)] Loss: 17670.054688\n",
      "Train Epoch: 242 [25152/225000 (11%)] Loss: 17927.115234\n",
      "Train Epoch: 242 [27648/225000 (12%)] Loss: 17549.648438\n",
      "Train Epoch: 242 [30144/225000 (13%)] Loss: 17599.640625\n",
      "Train Epoch: 242 [32640/225000 (15%)] Loss: 18177.681641\n",
      "Train Epoch: 242 [35136/225000 (16%)] Loss: 17825.429688\n",
      "Train Epoch: 242 [37632/225000 (17%)] Loss: 17965.292969\n",
      "Train Epoch: 242 [40128/225000 (18%)] Loss: 18283.742188\n",
      "Train Epoch: 242 [42624/225000 (19%)] Loss: 17941.402344\n",
      "Train Epoch: 242 [45120/225000 (20%)] Loss: 17608.990234\n",
      "Train Epoch: 242 [47616/225000 (21%)] Loss: 17489.687500\n",
      "Train Epoch: 242 [50112/225000 (22%)] Loss: 17842.875000\n",
      "Train Epoch: 242 [52608/225000 (23%)] Loss: 17865.273438\n",
      "Train Epoch: 242 [55104/225000 (24%)] Loss: 17698.199219\n",
      "Train Epoch: 242 [57600/225000 (26%)] Loss: 17353.105469\n",
      "Train Epoch: 242 [60096/225000 (27%)] Loss: 17865.652344\n",
      "Train Epoch: 242 [62592/225000 (28%)] Loss: 17729.906250\n",
      "Train Epoch: 242 [65088/225000 (29%)] Loss: 17539.900391\n",
      "Train Epoch: 242 [67584/225000 (30%)] Loss: 17435.199219\n",
      "Train Epoch: 242 [70080/225000 (31%)] Loss: 17598.007812\n",
      "Train Epoch: 242 [72576/225000 (32%)] Loss: 17746.972656\n",
      "Train Epoch: 242 [75072/225000 (33%)] Loss: 17326.628906\n",
      "Train Epoch: 242 [77568/225000 (34%)] Loss: 17685.134766\n",
      "Train Epoch: 242 [80064/225000 (36%)] Loss: 17507.392578\n",
      "Train Epoch: 242 [82560/225000 (37%)] Loss: 18069.300781\n",
      "Train Epoch: 242 [85056/225000 (38%)] Loss: 18093.966797\n",
      "Train Epoch: 242 [87552/225000 (39%)] Loss: 17791.429688\n",
      "Train Epoch: 242 [90048/225000 (40%)] Loss: 18186.117188\n",
      "Train Epoch: 242 [92544/225000 (41%)] Loss: 17918.408203\n",
      "Train Epoch: 242 [95040/225000 (42%)] Loss: 18250.431641\n",
      "Train Epoch: 242 [97536/225000 (43%)] Loss: 17431.529297\n",
      "Train Epoch: 242 [100032/225000 (44%)] Loss: 17879.740234\n",
      "Train Epoch: 242 [102528/225000 (46%)] Loss: 18545.035156\n",
      "Train Epoch: 242 [105024/225000 (47%)] Loss: 18208.748047\n",
      "Train Epoch: 242 [107520/225000 (48%)] Loss: 17760.539062\n",
      "Train Epoch: 242 [110016/225000 (49%)] Loss: 17737.001953\n",
      "Train Epoch: 242 [112512/225000 (50%)] Loss: 18052.919922\n",
      "Train Epoch: 242 [115008/225000 (51%)] Loss: 18027.066406\n",
      "Train Epoch: 242 [117504/225000 (52%)] Loss: 17839.123047\n",
      "Train Epoch: 242 [120000/225000 (53%)] Loss: 17728.734375\n",
      "Train Epoch: 242 [122496/225000 (54%)] Loss: 17813.980469\n",
      "Train Epoch: 242 [124992/225000 (56%)] Loss: 18127.308594\n",
      "Train Epoch: 242 [127488/225000 (57%)] Loss: 17782.226562\n",
      "Train Epoch: 242 [129984/225000 (58%)] Loss: 18102.269531\n",
      "Train Epoch: 242 [132480/225000 (59%)] Loss: 17374.085938\n",
      "Train Epoch: 242 [134976/225000 (60%)] Loss: 17637.878906\n",
      "Train Epoch: 242 [137472/225000 (61%)] Loss: 18020.246094\n",
      "Train Epoch: 242 [139968/225000 (62%)] Loss: 17974.207031\n",
      "Train Epoch: 242 [142464/225000 (63%)] Loss: 18231.138672\n",
      "Train Epoch: 242 [144960/225000 (64%)] Loss: 17990.851562\n",
      "Train Epoch: 242 [147456/225000 (66%)] Loss: 18554.433594\n",
      "Train Epoch: 242 [149952/225000 (67%)] Loss: 18118.523438\n",
      "Train Epoch: 242 [152448/225000 (68%)] Loss: 17472.218750\n",
      "Train Epoch: 242 [154944/225000 (69%)] Loss: 17730.664062\n",
      "Train Epoch: 242 [157440/225000 (70%)] Loss: 17888.035156\n",
      "Train Epoch: 242 [159936/225000 (71%)] Loss: 18146.939453\n",
      "Train Epoch: 242 [162432/225000 (72%)] Loss: 18231.832031\n",
      "Train Epoch: 242 [164928/225000 (73%)] Loss: 18069.632812\n",
      "Train Epoch: 242 [167424/225000 (74%)] Loss: 18235.675781\n",
      "Train Epoch: 242 [169920/225000 (76%)] Loss: 17641.451172\n",
      "Train Epoch: 242 [172416/225000 (77%)] Loss: 17738.367188\n",
      "Train Epoch: 242 [174912/225000 (78%)] Loss: 17643.500000\n",
      "Train Epoch: 242 [177408/225000 (79%)] Loss: 18070.699219\n",
      "Train Epoch: 242 [179904/225000 (80%)] Loss: 17934.542969\n",
      "Train Epoch: 242 [182400/225000 (81%)] Loss: 17618.078125\n",
      "Train Epoch: 242 [184896/225000 (82%)] Loss: 17742.097656\n",
      "Train Epoch: 242 [187392/225000 (83%)] Loss: 17768.167969\n",
      "Train Epoch: 242 [189888/225000 (84%)] Loss: 17148.964844\n",
      "Train Epoch: 242 [192384/225000 (86%)] Loss: 17781.074219\n",
      "Train Epoch: 242 [194880/225000 (87%)] Loss: 17481.574219\n",
      "Train Epoch: 242 [197376/225000 (88%)] Loss: 18448.953125\n",
      "Train Epoch: 242 [199872/225000 (89%)] Loss: 18138.835938\n",
      "Train Epoch: 242 [202368/225000 (90%)] Loss: 17154.671875\n",
      "Train Epoch: 242 [204864/225000 (91%)] Loss: 17695.320312\n",
      "Train Epoch: 242 [207360/225000 (92%)] Loss: 18119.611328\n",
      "Train Epoch: 242 [209856/225000 (93%)] Loss: 17403.250000\n",
      "Train Epoch: 242 [212352/225000 (94%)] Loss: 17806.019531\n",
      "Train Epoch: 242 [214848/225000 (95%)] Loss: 17457.910156\n",
      "Train Epoch: 242 [217344/225000 (97%)] Loss: 18025.900391\n",
      "Train Epoch: 242 [219840/225000 (98%)] Loss: 17483.019531\n",
      "Train Epoch: 242 [222336/225000 (99%)] Loss: 17945.449219\n",
      "Train Epoch: 242 [224832/225000 (100%)] Loss: 17230.925781\n",
      "    epoch          : 242\n",
      "    loss           : 17839.312804134224\n",
      "    val_loss       : 17763.793026820393\n",
      "Train Epoch: 243 [192/225000 (0%)] Loss: 17620.703125\n",
      "Train Epoch: 243 [2688/225000 (1%)] Loss: 18109.076172\n",
      "Train Epoch: 243 [5184/225000 (2%)] Loss: 17864.988281\n",
      "Train Epoch: 243 [7680/225000 (3%)] Loss: 18073.322266\n",
      "Train Epoch: 243 [10176/225000 (5%)] Loss: 17855.037109\n",
      "Train Epoch: 243 [12672/225000 (6%)] Loss: 17916.248047\n",
      "Train Epoch: 243 [15168/225000 (7%)] Loss: 17639.943359\n",
      "Train Epoch: 243 [17664/225000 (8%)] Loss: 18099.046875\n",
      "Train Epoch: 243 [20160/225000 (9%)] Loss: 18309.941406\n",
      "Train Epoch: 243 [22656/225000 (10%)] Loss: 17979.708984\n",
      "Train Epoch: 243 [25152/225000 (11%)] Loss: 18341.500000\n",
      "Train Epoch: 243 [27648/225000 (12%)] Loss: 20810.007812\n",
      "Train Epoch: 243 [30144/225000 (13%)] Loss: 17463.064453\n",
      "Train Epoch: 243 [32640/225000 (15%)] Loss: 18139.466797\n",
      "Train Epoch: 243 [35136/225000 (16%)] Loss: 17886.882812\n",
      "Train Epoch: 243 [37632/225000 (17%)] Loss: 17411.722656\n",
      "Train Epoch: 243 [40128/225000 (18%)] Loss: 18212.507812\n",
      "Train Epoch: 243 [42624/225000 (19%)] Loss: 17845.281250\n",
      "Train Epoch: 243 [45120/225000 (20%)] Loss: 17719.259766\n",
      "Train Epoch: 243 [47616/225000 (21%)] Loss: 17835.197266\n",
      "Train Epoch: 243 [50112/225000 (22%)] Loss: 17860.923828\n",
      "Train Epoch: 243 [52608/225000 (23%)] Loss: 17942.785156\n",
      "Train Epoch: 243 [55104/225000 (24%)] Loss: 17850.902344\n",
      "Train Epoch: 243 [57600/225000 (26%)] Loss: 17694.699219\n",
      "Train Epoch: 243 [60096/225000 (27%)] Loss: 17754.009766\n",
      "Train Epoch: 243 [62592/225000 (28%)] Loss: 17973.152344\n",
      "Train Epoch: 243 [65088/225000 (29%)] Loss: 17130.984375\n",
      "Train Epoch: 243 [67584/225000 (30%)] Loss: 17569.585938\n",
      "Train Epoch: 243 [70080/225000 (31%)] Loss: 17351.482422\n",
      "Train Epoch: 243 [72576/225000 (32%)] Loss: 18089.277344\n",
      "Train Epoch: 243 [75072/225000 (33%)] Loss: 17613.298828\n",
      "Train Epoch: 243 [77568/225000 (34%)] Loss: 17813.099609\n",
      "Train Epoch: 243 [80064/225000 (36%)] Loss: 18033.410156\n",
      "Train Epoch: 243 [82560/225000 (37%)] Loss: 17467.082031\n",
      "Train Epoch: 243 [85056/225000 (38%)] Loss: 17860.576172\n",
      "Train Epoch: 243 [87552/225000 (39%)] Loss: 17941.605469\n",
      "Train Epoch: 243 [90048/225000 (40%)] Loss: 17458.439453\n",
      "Train Epoch: 243 [92544/225000 (41%)] Loss: 17623.222656\n",
      "Train Epoch: 243 [95040/225000 (42%)] Loss: 17551.023438\n",
      "Train Epoch: 243 [97536/225000 (43%)] Loss: 18432.572266\n",
      "Train Epoch: 243 [100032/225000 (44%)] Loss: 17560.859375\n",
      "Train Epoch: 243 [102528/225000 (46%)] Loss: 17509.457031\n",
      "Train Epoch: 243 [105024/225000 (47%)] Loss: 17942.505859\n",
      "Train Epoch: 243 [107520/225000 (48%)] Loss: 17970.654297\n",
      "Train Epoch: 243 [110016/225000 (49%)] Loss: 17472.740234\n",
      "Train Epoch: 243 [112512/225000 (50%)] Loss: 17737.710938\n",
      "Train Epoch: 243 [115008/225000 (51%)] Loss: 18116.035156\n",
      "Train Epoch: 243 [117504/225000 (52%)] Loss: 18209.382812\n",
      "Train Epoch: 243 [120000/225000 (53%)] Loss: 17876.976562\n",
      "Train Epoch: 243 [122496/225000 (54%)] Loss: 18193.281250\n",
      "Train Epoch: 243 [124992/225000 (56%)] Loss: 18130.439453\n",
      "Train Epoch: 243 [127488/225000 (57%)] Loss: 17533.128906\n",
      "Train Epoch: 243 [129984/225000 (58%)] Loss: 17582.007812\n",
      "Train Epoch: 243 [132480/225000 (59%)] Loss: 17174.218750\n",
      "Train Epoch: 243 [134976/225000 (60%)] Loss: 17513.146484\n",
      "Train Epoch: 243 [137472/225000 (61%)] Loss: 17957.417969\n",
      "Train Epoch: 243 [139968/225000 (62%)] Loss: 18540.144531\n",
      "Train Epoch: 243 [142464/225000 (63%)] Loss: 17556.574219\n",
      "Train Epoch: 243 [144960/225000 (64%)] Loss: 18291.171875\n",
      "Train Epoch: 243 [147456/225000 (66%)] Loss: 17808.814453\n",
      "Train Epoch: 243 [149952/225000 (67%)] Loss: 17876.437500\n",
      "Train Epoch: 243 [152448/225000 (68%)] Loss: 18095.658203\n",
      "Train Epoch: 243 [154944/225000 (69%)] Loss: 17388.951172\n",
      "Train Epoch: 243 [157440/225000 (70%)] Loss: 17811.476562\n",
      "Train Epoch: 243 [159936/225000 (71%)] Loss: 17658.632812\n",
      "Train Epoch: 243 [162432/225000 (72%)] Loss: 17327.285156\n",
      "Train Epoch: 243 [164928/225000 (73%)] Loss: 17961.511719\n",
      "Train Epoch: 243 [167424/225000 (74%)] Loss: 18028.867188\n",
      "Train Epoch: 243 [169920/225000 (76%)] Loss: 17306.699219\n",
      "Train Epoch: 243 [172416/225000 (77%)] Loss: 17879.933594\n",
      "Train Epoch: 243 [174912/225000 (78%)] Loss: 17809.839844\n",
      "Train Epoch: 243 [177408/225000 (79%)] Loss: 17579.302734\n",
      "Train Epoch: 243 [179904/225000 (80%)] Loss: 17631.292969\n",
      "Train Epoch: 243 [182400/225000 (81%)] Loss: 17594.287109\n",
      "Train Epoch: 243 [184896/225000 (82%)] Loss: 18408.843750\n",
      "Train Epoch: 243 [187392/225000 (83%)] Loss: 18026.386719\n",
      "Train Epoch: 243 [189888/225000 (84%)] Loss: 18047.673828\n",
      "Train Epoch: 243 [192384/225000 (86%)] Loss: 17882.035156\n",
      "Train Epoch: 243 [194880/225000 (87%)] Loss: 17780.019531\n",
      "Train Epoch: 243 [197376/225000 (88%)] Loss: 17562.585938\n",
      "Train Epoch: 243 [199872/225000 (89%)] Loss: 17808.066406\n",
      "Train Epoch: 243 [202368/225000 (90%)] Loss: 18088.542969\n",
      "Train Epoch: 243 [204864/225000 (91%)] Loss: 17884.634766\n",
      "Train Epoch: 243 [207360/225000 (92%)] Loss: 17959.316406\n",
      "Train Epoch: 243 [209856/225000 (93%)] Loss: 17806.921875\n",
      "Train Epoch: 243 [212352/225000 (94%)] Loss: 17969.214844\n",
      "Train Epoch: 243 [214848/225000 (95%)] Loss: 17506.951172\n",
      "Train Epoch: 243 [217344/225000 (97%)] Loss: 17639.722656\n",
      "Train Epoch: 243 [219840/225000 (98%)] Loss: 17396.683594\n",
      "Train Epoch: 243 [222336/225000 (99%)] Loss: 18044.406250\n",
      "Train Epoch: 243 [224832/225000 (100%)] Loss: 17551.726562\n",
      "    epoch          : 243\n",
      "    loss           : 17837.834653470298\n",
      "    val_loss       : 17774.704806641766\n",
      "Train Epoch: 244 [192/225000 (0%)] Loss: 17878.652344\n",
      "Train Epoch: 244 [2688/225000 (1%)] Loss: 17529.992188\n",
      "Train Epoch: 244 [5184/225000 (2%)] Loss: 17596.537109\n",
      "Train Epoch: 244 [7680/225000 (3%)] Loss: 18229.630859\n",
      "Train Epoch: 244 [10176/225000 (5%)] Loss: 17506.835938\n",
      "Train Epoch: 244 [12672/225000 (6%)] Loss: 17659.421875\n",
      "Train Epoch: 244 [15168/225000 (7%)] Loss: 17626.500000\n",
      "Train Epoch: 244 [17664/225000 (8%)] Loss: 17773.636719\n",
      "Train Epoch: 244 [20160/225000 (9%)] Loss: 18124.576172\n",
      "Train Epoch: 244 [22656/225000 (10%)] Loss: 17884.785156\n",
      "Train Epoch: 244 [25152/225000 (11%)] Loss: 18001.517578\n",
      "Train Epoch: 244 [27648/225000 (12%)] Loss: 17645.656250\n",
      "Train Epoch: 244 [30144/225000 (13%)] Loss: 17472.011719\n",
      "Train Epoch: 244 [32640/225000 (15%)] Loss: 18252.484375\n",
      "Train Epoch: 244 [35136/225000 (16%)] Loss: 18440.335938\n",
      "Train Epoch: 244 [37632/225000 (17%)] Loss: 18115.382812\n",
      "Train Epoch: 244 [40128/225000 (18%)] Loss: 17885.859375\n",
      "Train Epoch: 244 [42624/225000 (19%)] Loss: 18009.402344\n",
      "Train Epoch: 244 [45120/225000 (20%)] Loss: 17479.605469\n",
      "Train Epoch: 244 [47616/225000 (21%)] Loss: 17980.101562\n",
      "Train Epoch: 244 [50112/225000 (22%)] Loss: 17819.609375\n",
      "Train Epoch: 244 [52608/225000 (23%)] Loss: 18086.804688\n",
      "Train Epoch: 244 [55104/225000 (24%)] Loss: 18082.609375\n",
      "Train Epoch: 244 [57600/225000 (26%)] Loss: 17606.708984\n",
      "Train Epoch: 244 [60096/225000 (27%)] Loss: 17606.335938\n",
      "Train Epoch: 244 [62592/225000 (28%)] Loss: 17719.394531\n",
      "Train Epoch: 244 [65088/225000 (29%)] Loss: 18093.140625\n",
      "Train Epoch: 244 [67584/225000 (30%)] Loss: 17530.984375\n",
      "Train Epoch: 244 [70080/225000 (31%)] Loss: 18133.746094\n",
      "Train Epoch: 244 [72576/225000 (32%)] Loss: 17868.042969\n",
      "Train Epoch: 244 [75072/225000 (33%)] Loss: 17708.966797\n",
      "Train Epoch: 244 [77568/225000 (34%)] Loss: 17750.570312\n",
      "Train Epoch: 244 [80064/225000 (36%)] Loss: 18152.261719\n",
      "Train Epoch: 244 [82560/225000 (37%)] Loss: 17541.460938\n",
      "Train Epoch: 244 [85056/225000 (38%)] Loss: 18093.710938\n",
      "Train Epoch: 244 [87552/225000 (39%)] Loss: 18157.886719\n",
      "Train Epoch: 244 [90048/225000 (40%)] Loss: 17696.857422\n",
      "Train Epoch: 244 [92544/225000 (41%)] Loss: 17662.429688\n",
      "Train Epoch: 244 [95040/225000 (42%)] Loss: 17737.367188\n",
      "Train Epoch: 244 [97536/225000 (43%)] Loss: 17771.048828\n",
      "Train Epoch: 244 [100032/225000 (44%)] Loss: 19730.519531\n",
      "Train Epoch: 244 [102528/225000 (46%)] Loss: 17767.566406\n",
      "Train Epoch: 244 [105024/225000 (47%)] Loss: 17737.566406\n",
      "Train Epoch: 244 [107520/225000 (48%)] Loss: 17896.804688\n",
      "Train Epoch: 244 [110016/225000 (49%)] Loss: 18411.890625\n",
      "Train Epoch: 244 [112512/225000 (50%)] Loss: 17938.326172\n",
      "Train Epoch: 244 [115008/225000 (51%)] Loss: 17973.835938\n",
      "Train Epoch: 244 [117504/225000 (52%)] Loss: 17765.304688\n",
      "Train Epoch: 244 [120000/225000 (53%)] Loss: 17847.082031\n",
      "Train Epoch: 244 [122496/225000 (54%)] Loss: 17932.761719\n",
      "Train Epoch: 244 [124992/225000 (56%)] Loss: 18076.031250\n",
      "Train Epoch: 244 [127488/225000 (57%)] Loss: 17830.089844\n",
      "Train Epoch: 244 [129984/225000 (58%)] Loss: 17601.322266\n",
      "Train Epoch: 244 [132480/225000 (59%)] Loss: 17857.066406\n",
      "Train Epoch: 244 [134976/225000 (60%)] Loss: 17913.892578\n",
      "Train Epoch: 244 [137472/225000 (61%)] Loss: 18012.289062\n",
      "Train Epoch: 244 [139968/225000 (62%)] Loss: 17412.273438\n",
      "Train Epoch: 244 [142464/225000 (63%)] Loss: 17724.095703\n",
      "Train Epoch: 244 [144960/225000 (64%)] Loss: 17651.542969\n",
      "Train Epoch: 244 [147456/225000 (66%)] Loss: 18229.523438\n",
      "Train Epoch: 244 [149952/225000 (67%)] Loss: 17491.011719\n",
      "Train Epoch: 244 [152448/225000 (68%)] Loss: 17486.402344\n",
      "Train Epoch: 244 [154944/225000 (69%)] Loss: 17636.294922\n",
      "Train Epoch: 244 [157440/225000 (70%)] Loss: 18077.804688\n",
      "Train Epoch: 244 [159936/225000 (71%)] Loss: 17206.867188\n",
      "Train Epoch: 244 [162432/225000 (72%)] Loss: 17781.636719\n",
      "Train Epoch: 244 [164928/225000 (73%)] Loss: 17910.476562\n",
      "Train Epoch: 244 [167424/225000 (74%)] Loss: 17721.648438\n",
      "Train Epoch: 244 [169920/225000 (76%)] Loss: 17655.972656\n",
      "Train Epoch: 244 [172416/225000 (77%)] Loss: 18160.222656\n",
      "Train Epoch: 244 [174912/225000 (78%)] Loss: 17190.933594\n",
      "Train Epoch: 244 [177408/225000 (79%)] Loss: 17864.072266\n",
      "Train Epoch: 244 [179904/225000 (80%)] Loss: 17562.062500\n",
      "Train Epoch: 244 [182400/225000 (81%)] Loss: 17648.097656\n",
      "Train Epoch: 244 [184896/225000 (82%)] Loss: 18211.369141\n",
      "Train Epoch: 244 [187392/225000 (83%)] Loss: 17575.212891\n",
      "Train Epoch: 244 [189888/225000 (84%)] Loss: 17686.312500\n",
      "Train Epoch: 244 [192384/225000 (86%)] Loss: 18111.414062\n",
      "Train Epoch: 244 [194880/225000 (87%)] Loss: 18417.535156\n",
      "Train Epoch: 244 [197376/225000 (88%)] Loss: 17676.660156\n",
      "Train Epoch: 244 [199872/225000 (89%)] Loss: 18185.285156\n",
      "Train Epoch: 244 [202368/225000 (90%)] Loss: 18051.441406\n",
      "Train Epoch: 244 [204864/225000 (91%)] Loss: 17152.203125\n",
      "Train Epoch: 244 [207360/225000 (92%)] Loss: 18012.255859\n",
      "Train Epoch: 244 [209856/225000 (93%)] Loss: 17659.214844\n",
      "Train Epoch: 244 [212352/225000 (94%)] Loss: 17333.074219\n",
      "Train Epoch: 244 [214848/225000 (95%)] Loss: 17927.015625\n",
      "Train Epoch: 244 [217344/225000 (97%)] Loss: 18284.804688\n",
      "Train Epoch: 244 [219840/225000 (98%)] Loss: 17747.226562\n",
      "Train Epoch: 244 [222336/225000 (99%)] Loss: 17120.261719\n",
      "Train Epoch: 244 [224832/225000 (100%)] Loss: 18102.078125\n",
      "    epoch          : 244\n",
      "    loss           : 17815.775097322952\n",
      "    val_loss       : 17730.47076412694\n",
      "Train Epoch: 245 [192/225000 (0%)] Loss: 18369.089844\n",
      "Train Epoch: 245 [2688/225000 (1%)] Loss: 17771.953125\n",
      "Train Epoch: 245 [5184/225000 (2%)] Loss: 17615.203125\n",
      "Train Epoch: 245 [7680/225000 (3%)] Loss: 17596.996094\n",
      "Train Epoch: 245 [10176/225000 (5%)] Loss: 17825.777344\n",
      "Train Epoch: 245 [12672/225000 (6%)] Loss: 18335.535156\n",
      "Train Epoch: 245 [15168/225000 (7%)] Loss: 17218.718750\n",
      "Train Epoch: 245 [17664/225000 (8%)] Loss: 17913.066406\n",
      "Train Epoch: 245 [20160/225000 (9%)] Loss: 18385.138672\n",
      "Train Epoch: 245 [22656/225000 (10%)] Loss: 17679.953125\n",
      "Train Epoch: 245 [25152/225000 (11%)] Loss: 17779.822266\n",
      "Train Epoch: 245 [27648/225000 (12%)] Loss: 17665.292969\n",
      "Train Epoch: 245 [30144/225000 (13%)] Loss: 17589.855469\n",
      "Train Epoch: 245 [32640/225000 (15%)] Loss: 17801.402344\n",
      "Train Epoch: 245 [35136/225000 (16%)] Loss: 17819.113281\n",
      "Train Epoch: 245 [37632/225000 (17%)] Loss: 18073.453125\n",
      "Train Epoch: 245 [40128/225000 (18%)] Loss: 17647.787109\n",
      "Train Epoch: 245 [42624/225000 (19%)] Loss: 17882.394531\n",
      "Train Epoch: 245 [45120/225000 (20%)] Loss: 18127.328125\n",
      "Train Epoch: 245 [47616/225000 (21%)] Loss: 18218.902344\n",
      "Train Epoch: 245 [50112/225000 (22%)] Loss: 18005.718750\n",
      "Train Epoch: 245 [52608/225000 (23%)] Loss: 17885.375000\n",
      "Train Epoch: 245 [55104/225000 (24%)] Loss: 18060.705078\n",
      "Train Epoch: 245 [57600/225000 (26%)] Loss: 17333.806641\n",
      "Train Epoch: 245 [60096/225000 (27%)] Loss: 18406.093750\n",
      "Train Epoch: 245 [62592/225000 (28%)] Loss: 18090.898438\n",
      "Train Epoch: 245 [65088/225000 (29%)] Loss: 17885.503906\n",
      "Train Epoch: 245 [67584/225000 (30%)] Loss: 17663.542969\n",
      "Train Epoch: 245 [70080/225000 (31%)] Loss: 18480.429688\n",
      "Train Epoch: 245 [72576/225000 (32%)] Loss: 17308.476562\n",
      "Train Epoch: 245 [75072/225000 (33%)] Loss: 17908.449219\n",
      "Train Epoch: 245 [77568/225000 (34%)] Loss: 18004.179688\n",
      "Train Epoch: 245 [80064/225000 (36%)] Loss: 18143.582031\n",
      "Train Epoch: 245 [82560/225000 (37%)] Loss: 17490.738281\n",
      "Train Epoch: 245 [85056/225000 (38%)] Loss: 18281.943359\n",
      "Train Epoch: 245 [87552/225000 (39%)] Loss: 18309.285156\n",
      "Train Epoch: 245 [90048/225000 (40%)] Loss: 17399.390625\n",
      "Train Epoch: 245 [92544/225000 (41%)] Loss: 18324.712891\n",
      "Train Epoch: 245 [95040/225000 (42%)] Loss: 17635.062500\n",
      "Train Epoch: 245 [97536/225000 (43%)] Loss: 18066.412109\n",
      "Train Epoch: 245 [100032/225000 (44%)] Loss: 17966.765625\n",
      "Train Epoch: 245 [102528/225000 (46%)] Loss: 17590.394531\n",
      "Train Epoch: 245 [105024/225000 (47%)] Loss: 17788.361328\n",
      "Train Epoch: 245 [107520/225000 (48%)] Loss: 17763.238281\n",
      "Train Epoch: 245 [110016/225000 (49%)] Loss: 17627.539062\n",
      "Train Epoch: 245 [112512/225000 (50%)] Loss: 17940.527344\n",
      "Train Epoch: 245 [115008/225000 (51%)] Loss: 17947.009766\n",
      "Train Epoch: 245 [117504/225000 (52%)] Loss: 18173.171875\n",
      "Train Epoch: 245 [120000/225000 (53%)] Loss: 17866.687500\n",
      "Train Epoch: 245 [122496/225000 (54%)] Loss: 17882.386719\n",
      "Train Epoch: 245 [124992/225000 (56%)] Loss: 17423.720703\n",
      "Train Epoch: 245 [127488/225000 (57%)] Loss: 17919.050781\n",
      "Train Epoch: 245 [129984/225000 (58%)] Loss: 17765.414062\n",
      "Train Epoch: 245 [132480/225000 (59%)] Loss: 17746.888672\n",
      "Train Epoch: 245 [134976/225000 (60%)] Loss: 17572.531250\n",
      "Train Epoch: 245 [137472/225000 (61%)] Loss: 17659.464844\n",
      "Train Epoch: 245 [139968/225000 (62%)] Loss: 17481.742188\n",
      "Train Epoch: 245 [142464/225000 (63%)] Loss: 18119.267578\n",
      "Train Epoch: 245 [144960/225000 (64%)] Loss: 17856.500000\n",
      "Train Epoch: 245 [147456/225000 (66%)] Loss: 17703.138672\n",
      "Train Epoch: 245 [149952/225000 (67%)] Loss: 17579.453125\n",
      "Train Epoch: 245 [152448/225000 (68%)] Loss: 18031.597656\n",
      "Train Epoch: 245 [154944/225000 (69%)] Loss: 17763.525391\n",
      "Train Epoch: 245 [157440/225000 (70%)] Loss: 17214.697266\n",
      "Train Epoch: 245 [159936/225000 (71%)] Loss: 17337.964844\n",
      "Train Epoch: 245 [162432/225000 (72%)] Loss: 17962.636719\n",
      "Train Epoch: 245 [164928/225000 (73%)] Loss: 17807.833984\n",
      "Train Epoch: 245 [167424/225000 (74%)] Loss: 17804.095703\n",
      "Train Epoch: 245 [169920/225000 (76%)] Loss: 17682.388672\n",
      "Train Epoch: 245 [172416/225000 (77%)] Loss: 17614.955078\n",
      "Train Epoch: 245 [174912/225000 (78%)] Loss: 18108.316406\n",
      "Train Epoch: 245 [177408/225000 (79%)] Loss: 18090.316406\n",
      "Train Epoch: 245 [179904/225000 (80%)] Loss: 17749.257812\n",
      "Train Epoch: 245 [182400/225000 (81%)] Loss: 17225.503906\n",
      "Train Epoch: 245 [184896/225000 (82%)] Loss: 17715.105469\n",
      "Train Epoch: 245 [187392/225000 (83%)] Loss: 17899.160156\n",
      "Train Epoch: 245 [189888/225000 (84%)] Loss: 17752.066406\n",
      "Train Epoch: 245 [192384/225000 (86%)] Loss: 17835.197266\n",
      "Train Epoch: 245 [194880/225000 (87%)] Loss: 17500.714844\n",
      "Train Epoch: 245 [197376/225000 (88%)] Loss: 17932.833984\n",
      "Train Epoch: 245 [199872/225000 (89%)] Loss: 17249.751953\n",
      "Train Epoch: 245 [202368/225000 (90%)] Loss: 17560.232422\n",
      "Train Epoch: 245 [204864/225000 (91%)] Loss: 18013.921875\n",
      "Train Epoch: 245 [207360/225000 (92%)] Loss: 17755.341797\n",
      "Train Epoch: 245 [209856/225000 (93%)] Loss: 17638.421875\n",
      "Train Epoch: 245 [212352/225000 (94%)] Loss: 17787.951172\n",
      "Train Epoch: 245 [214848/225000 (95%)] Loss: 18096.095703\n",
      "Train Epoch: 245 [217344/225000 (97%)] Loss: 17473.480469\n",
      "Train Epoch: 245 [219840/225000 (98%)] Loss: 17853.062500\n",
      "Train Epoch: 245 [222336/225000 (99%)] Loss: 17351.886719\n",
      "Train Epoch: 245 [224832/225000 (100%)] Loss: 17609.421875\n",
      "    epoch          : 245\n",
      "    loss           : 17809.443618514026\n",
      "    val_loss       : 17714.222875062747\n",
      "Train Epoch: 246 [192/225000 (0%)] Loss: 17731.675781\n",
      "Train Epoch: 246 [2688/225000 (1%)] Loss: 17420.921875\n",
      "Train Epoch: 246 [5184/225000 (2%)] Loss: 18109.345703\n",
      "Train Epoch: 246 [7680/225000 (3%)] Loss: 17654.125000\n",
      "Train Epoch: 246 [10176/225000 (5%)] Loss: 17844.792969\n",
      "Train Epoch: 246 [12672/225000 (6%)] Loss: 17480.523438\n",
      "Train Epoch: 246 [15168/225000 (7%)] Loss: 17563.359375\n",
      "Train Epoch: 246 [17664/225000 (8%)] Loss: 17945.240234\n",
      "Train Epoch: 246 [20160/225000 (9%)] Loss: 17739.253906\n",
      "Train Epoch: 246 [22656/225000 (10%)] Loss: 17725.468750\n",
      "Train Epoch: 246 [25152/225000 (11%)] Loss: 17594.703125\n",
      "Train Epoch: 246 [27648/225000 (12%)] Loss: 17810.685547\n",
      "Train Epoch: 246 [30144/225000 (13%)] Loss: 17170.367188\n",
      "Train Epoch: 246 [32640/225000 (15%)] Loss: 17735.517578\n",
      "Train Epoch: 246 [35136/225000 (16%)] Loss: 18228.789062\n",
      "Train Epoch: 246 [37632/225000 (17%)] Loss: 17729.355469\n",
      "Train Epoch: 246 [40128/225000 (18%)] Loss: 17898.707031\n",
      "Train Epoch: 246 [42624/225000 (19%)] Loss: 17729.371094\n",
      "Train Epoch: 246 [45120/225000 (20%)] Loss: 17638.558594\n",
      "Train Epoch: 246 [47616/225000 (21%)] Loss: 17767.765625\n",
      "Train Epoch: 246 [50112/225000 (22%)] Loss: 18329.527344\n",
      "Train Epoch: 246 [52608/225000 (23%)] Loss: 17767.449219\n",
      "Train Epoch: 246 [55104/225000 (24%)] Loss: 17980.769531\n",
      "Train Epoch: 246 [57600/225000 (26%)] Loss: 17672.962891\n",
      "Train Epoch: 246 [60096/225000 (27%)] Loss: 17835.562500\n",
      "Train Epoch: 246 [62592/225000 (28%)] Loss: 18080.734375\n",
      "Train Epoch: 246 [65088/225000 (29%)] Loss: 17697.949219\n",
      "Train Epoch: 246 [67584/225000 (30%)] Loss: 17565.046875\n",
      "Train Epoch: 246 [70080/225000 (31%)] Loss: 18150.554688\n",
      "Train Epoch: 246 [72576/225000 (32%)] Loss: 18516.279297\n",
      "Train Epoch: 246 [75072/225000 (33%)] Loss: 17625.166016\n",
      "Train Epoch: 246 [77568/225000 (34%)] Loss: 18036.056641\n",
      "Train Epoch: 246 [80064/225000 (36%)] Loss: 17816.671875\n",
      "Train Epoch: 246 [82560/225000 (37%)] Loss: 17925.306641\n",
      "Train Epoch: 246 [85056/225000 (38%)] Loss: 17832.265625\n",
      "Train Epoch: 246 [87552/225000 (39%)] Loss: 18253.708984\n",
      "Train Epoch: 246 [90048/225000 (40%)] Loss: 18013.289062\n",
      "Train Epoch: 246 [92544/225000 (41%)] Loss: 18088.257812\n",
      "Train Epoch: 246 [95040/225000 (42%)] Loss: 17519.703125\n",
      "Train Epoch: 246 [97536/225000 (43%)] Loss: 17948.843750\n",
      "Train Epoch: 246 [100032/225000 (44%)] Loss: 17752.078125\n",
      "Train Epoch: 246 [102528/225000 (46%)] Loss: 17989.042969\n",
      "Train Epoch: 246 [105024/225000 (47%)] Loss: 17906.472656\n",
      "Train Epoch: 246 [107520/225000 (48%)] Loss: 17915.318359\n",
      "Train Epoch: 246 [110016/225000 (49%)] Loss: 17844.037109\n",
      "Train Epoch: 246 [112512/225000 (50%)] Loss: 18042.748047\n",
      "Train Epoch: 246 [115008/225000 (51%)] Loss: 17978.234375\n",
      "Train Epoch: 246 [117504/225000 (52%)] Loss: 17679.970703\n",
      "Train Epoch: 246 [120000/225000 (53%)] Loss: 17906.082031\n",
      "Train Epoch: 246 [122496/225000 (54%)] Loss: 17766.468750\n",
      "Train Epoch: 246 [124992/225000 (56%)] Loss: 17997.656250\n",
      "Train Epoch: 246 [127488/225000 (57%)] Loss: 18104.925781\n",
      "Train Epoch: 246 [129984/225000 (58%)] Loss: 17891.792969\n",
      "Train Epoch: 246 [132480/225000 (59%)] Loss: 18108.863281\n",
      "Train Epoch: 246 [134976/225000 (60%)] Loss: 17493.859375\n",
      "Train Epoch: 246 [137472/225000 (61%)] Loss: 17812.775391\n",
      "Train Epoch: 246 [139968/225000 (62%)] Loss: 17754.644531\n",
      "Train Epoch: 246 [142464/225000 (63%)] Loss: 18024.388672\n",
      "Train Epoch: 246 [144960/225000 (64%)] Loss: 17766.449219\n",
      "Train Epoch: 246 [147456/225000 (66%)] Loss: 17440.011719\n",
      "Train Epoch: 246 [149952/225000 (67%)] Loss: 17111.398438\n",
      "Train Epoch: 246 [152448/225000 (68%)] Loss: 18202.501953\n",
      "Train Epoch: 246 [154944/225000 (69%)] Loss: 17793.855469\n",
      "Train Epoch: 246 [157440/225000 (70%)] Loss: 17973.511719\n",
      "Train Epoch: 246 [159936/225000 (71%)] Loss: 17728.824219\n",
      "Train Epoch: 246 [162432/225000 (72%)] Loss: 17682.496094\n",
      "Train Epoch: 246 [164928/225000 (73%)] Loss: 17826.748047\n",
      "Train Epoch: 246 [167424/225000 (74%)] Loss: 17493.070312\n",
      "Train Epoch: 246 [169920/225000 (76%)] Loss: 17399.800781\n",
      "Train Epoch: 246 [172416/225000 (77%)] Loss: 17291.464844\n",
      "Train Epoch: 246 [174912/225000 (78%)] Loss: 18272.531250\n",
      "Train Epoch: 246 [177408/225000 (79%)] Loss: 17797.722656\n",
      "Train Epoch: 246 [179904/225000 (80%)] Loss: 17893.505859\n",
      "Train Epoch: 246 [182400/225000 (81%)] Loss: 17662.183594\n",
      "Train Epoch: 246 [184896/225000 (82%)] Loss: 17715.652344\n",
      "Train Epoch: 246 [187392/225000 (83%)] Loss: 18183.183594\n",
      "Train Epoch: 246 [189888/225000 (84%)] Loss: 17962.708984\n",
      "Train Epoch: 246 [192384/225000 (86%)] Loss: 17825.843750\n",
      "Train Epoch: 246 [194880/225000 (87%)] Loss: 17165.832031\n",
      "Train Epoch: 246 [197376/225000 (88%)] Loss: 17858.722656\n",
      "Train Epoch: 246 [199872/225000 (89%)] Loss: 18108.650391\n",
      "Train Epoch: 246 [202368/225000 (90%)] Loss: 17770.542969\n",
      "Train Epoch: 246 [204864/225000 (91%)] Loss: 17604.222656\n",
      "Train Epoch: 246 [207360/225000 (92%)] Loss: 18142.886719\n",
      "Train Epoch: 246 [209856/225000 (93%)] Loss: 17654.687500\n",
      "Train Epoch: 246 [212352/225000 (94%)] Loss: 17207.496094\n",
      "Train Epoch: 246 [214848/225000 (95%)] Loss: 18154.275391\n",
      "Train Epoch: 246 [217344/225000 (97%)] Loss: 18166.507812\n",
      "Train Epoch: 246 [219840/225000 (98%)] Loss: 17738.753906\n",
      "Train Epoch: 246 [222336/225000 (99%)] Loss: 17938.013672\n",
      "Train Epoch: 246 [224832/225000 (100%)] Loss: 18225.171875\n",
      "    epoch          : 246\n",
      "    loss           : 17804.651762978614\n",
      "    val_loss       : 17807.538422477155\n",
      "Train Epoch: 247 [192/225000 (0%)] Loss: 17856.402344\n",
      "Train Epoch: 247 [2688/225000 (1%)] Loss: 17843.652344\n",
      "Train Epoch: 247 [5184/225000 (2%)] Loss: 18092.316406\n",
      "Train Epoch: 247 [7680/225000 (3%)] Loss: 17579.769531\n",
      "Train Epoch: 247 [10176/225000 (5%)] Loss: 17928.322266\n",
      "Train Epoch: 247 [12672/225000 (6%)] Loss: 17830.822266\n",
      "Train Epoch: 247 [15168/225000 (7%)] Loss: 17664.738281\n",
      "Train Epoch: 247 [17664/225000 (8%)] Loss: 17730.146484\n",
      "Train Epoch: 247 [20160/225000 (9%)] Loss: 17386.261719\n",
      "Train Epoch: 247 [22656/225000 (10%)] Loss: 17723.496094\n",
      "Train Epoch: 247 [25152/225000 (11%)] Loss: 17692.945312\n",
      "Train Epoch: 247 [27648/225000 (12%)] Loss: 17750.009766\n",
      "Train Epoch: 247 [30144/225000 (13%)] Loss: 18070.910156\n",
      "Train Epoch: 247 [32640/225000 (15%)] Loss: 17523.146484\n",
      "Train Epoch: 247 [35136/225000 (16%)] Loss: 17627.226562\n",
      "Train Epoch: 247 [37632/225000 (17%)] Loss: 17483.968750\n",
      "Train Epoch: 247 [40128/225000 (18%)] Loss: 17762.980469\n",
      "Train Epoch: 247 [42624/225000 (19%)] Loss: 17755.949219\n",
      "Train Epoch: 247 [45120/225000 (20%)] Loss: 17651.798828\n",
      "Train Epoch: 247 [47616/225000 (21%)] Loss: 17437.500000\n",
      "Train Epoch: 247 [50112/225000 (22%)] Loss: 17935.035156\n",
      "Train Epoch: 247 [52608/225000 (23%)] Loss: 17786.210938\n",
      "Train Epoch: 247 [55104/225000 (24%)] Loss: 18038.992188\n",
      "Train Epoch: 247 [57600/225000 (26%)] Loss: 17810.923828\n",
      "Train Epoch: 247 [60096/225000 (27%)] Loss: 17858.039062\n",
      "Train Epoch: 247 [62592/225000 (28%)] Loss: 17797.710938\n",
      "Train Epoch: 247 [65088/225000 (29%)] Loss: 17938.558594\n",
      "Train Epoch: 247 [67584/225000 (30%)] Loss: 17840.531250\n",
      "Train Epoch: 247 [70080/225000 (31%)] Loss: 17670.031250\n",
      "Train Epoch: 247 [72576/225000 (32%)] Loss: 17540.402344\n",
      "Train Epoch: 247 [75072/225000 (33%)] Loss: 18224.269531\n",
      "Train Epoch: 247 [77568/225000 (34%)] Loss: 17795.919922\n",
      "Train Epoch: 247 [80064/225000 (36%)] Loss: 17402.972656\n",
      "Train Epoch: 247 [82560/225000 (37%)] Loss: 17810.046875\n",
      "Train Epoch: 247 [85056/225000 (38%)] Loss: 17617.386719\n",
      "Train Epoch: 247 [87552/225000 (39%)] Loss: 17806.589844\n",
      "Train Epoch: 247 [90048/225000 (40%)] Loss: 17195.693359\n",
      "Train Epoch: 247 [92544/225000 (41%)] Loss: 18318.347656\n",
      "Train Epoch: 247 [95040/225000 (42%)] Loss: 17593.632812\n",
      "Train Epoch: 247 [97536/225000 (43%)] Loss: 17468.013672\n",
      "Train Epoch: 247 [100032/225000 (44%)] Loss: 18146.113281\n",
      "Train Epoch: 247 [102528/225000 (46%)] Loss: 18234.296875\n",
      "Train Epoch: 247 [105024/225000 (47%)] Loss: 17708.619141\n",
      "Train Epoch: 247 [107520/225000 (48%)] Loss: 17968.193359\n",
      "Train Epoch: 247 [110016/225000 (49%)] Loss: 17839.664062\n",
      "Train Epoch: 247 [112512/225000 (50%)] Loss: 17979.449219\n",
      "Train Epoch: 247 [115008/225000 (51%)] Loss: 17971.316406\n",
      "Train Epoch: 247 [117504/225000 (52%)] Loss: 18427.988281\n",
      "Train Epoch: 247 [120000/225000 (53%)] Loss: 18082.687500\n",
      "Train Epoch: 247 [122496/225000 (54%)] Loss: 17814.152344\n",
      "Train Epoch: 247 [124992/225000 (56%)] Loss: 18228.371094\n",
      "Train Epoch: 247 [127488/225000 (57%)] Loss: 17396.984375\n",
      "Train Epoch: 247 [129984/225000 (58%)] Loss: 18082.785156\n",
      "Train Epoch: 247 [132480/225000 (59%)] Loss: 17669.773438\n",
      "Train Epoch: 247 [134976/225000 (60%)] Loss: 17568.318359\n",
      "Train Epoch: 247 [137472/225000 (61%)] Loss: 17619.863281\n",
      "Train Epoch: 247 [139968/225000 (62%)] Loss: 17082.568359\n",
      "Train Epoch: 247 [142464/225000 (63%)] Loss: 17164.601562\n",
      "Train Epoch: 247 [144960/225000 (64%)] Loss: 17711.537109\n",
      "Train Epoch: 247 [147456/225000 (66%)] Loss: 17904.410156\n",
      "Train Epoch: 247 [149952/225000 (67%)] Loss: 17725.289062\n",
      "Train Epoch: 247 [152448/225000 (68%)] Loss: 17695.123047\n",
      "Train Epoch: 247 [154944/225000 (69%)] Loss: 17284.507812\n",
      "Train Epoch: 247 [157440/225000 (70%)] Loss: 18158.501953\n",
      "Train Epoch: 247 [159936/225000 (71%)] Loss: 18443.109375\n",
      "Train Epoch: 247 [162432/225000 (72%)] Loss: 17967.652344\n",
      "Train Epoch: 247 [164928/225000 (73%)] Loss: 17831.382812\n",
      "Train Epoch: 247 [167424/225000 (74%)] Loss: 17811.414062\n",
      "Train Epoch: 247 [169920/225000 (76%)] Loss: 17743.515625\n",
      "Train Epoch: 247 [172416/225000 (77%)] Loss: 17977.367188\n",
      "Train Epoch: 247 [174912/225000 (78%)] Loss: 17215.257812\n",
      "Train Epoch: 247 [177408/225000 (79%)] Loss: 17354.048828\n",
      "Train Epoch: 247 [179904/225000 (80%)] Loss: 17943.367188\n",
      "Train Epoch: 247 [182400/225000 (81%)] Loss: 17658.208984\n",
      "Train Epoch: 247 [184896/225000 (82%)] Loss: 18208.824219\n",
      "Train Epoch: 247 [187392/225000 (83%)] Loss: 17547.818359\n",
      "Train Epoch: 247 [189888/225000 (84%)] Loss: 17760.031250\n",
      "Train Epoch: 247 [192384/225000 (86%)] Loss: 17605.542969\n",
      "Train Epoch: 247 [194880/225000 (87%)] Loss: 18471.914062\n",
      "Train Epoch: 247 [197376/225000 (88%)] Loss: 18253.000000\n",
      "Train Epoch: 247 [199872/225000 (89%)] Loss: 17926.337891\n",
      "Train Epoch: 247 [202368/225000 (90%)] Loss: 17863.671875\n",
      "Train Epoch: 247 [204864/225000 (91%)] Loss: 17980.101562\n",
      "Train Epoch: 247 [207360/225000 (92%)] Loss: 18011.904297\n",
      "Train Epoch: 247 [209856/225000 (93%)] Loss: 17303.666016\n",
      "Train Epoch: 247 [212352/225000 (94%)] Loss: 17649.281250\n",
      "Train Epoch: 247 [214848/225000 (95%)] Loss: 17834.058594\n",
      "Train Epoch: 247 [217344/225000 (97%)] Loss: 17574.607422\n",
      "Train Epoch: 247 [219840/225000 (98%)] Loss: 17681.107422\n",
      "Train Epoch: 247 [222336/225000 (99%)] Loss: 17767.773438\n",
      "Train Epoch: 247 [224832/225000 (100%)] Loss: 18061.207031\n",
      "    epoch          : 247\n",
      "    loss           : 17789.36651923795\n",
      "    val_loss       : 17700.72725551092\n",
      "Train Epoch: 248 [192/225000 (0%)] Loss: 17475.283203\n",
      "Train Epoch: 248 [2688/225000 (1%)] Loss: 17826.539062\n",
      "Train Epoch: 248 [5184/225000 (2%)] Loss: 18287.496094\n",
      "Train Epoch: 248 [7680/225000 (3%)] Loss: 17854.408203\n",
      "Train Epoch: 248 [10176/225000 (5%)] Loss: 17444.796875\n",
      "Train Epoch: 248 [12672/225000 (6%)] Loss: 17630.437500\n",
      "Train Epoch: 248 [15168/225000 (7%)] Loss: 18139.205078\n",
      "Train Epoch: 248 [17664/225000 (8%)] Loss: 17656.013672\n",
      "Train Epoch: 248 [20160/225000 (9%)] Loss: 18024.968750\n",
      "Train Epoch: 248 [22656/225000 (10%)] Loss: 17848.726562\n",
      "Train Epoch: 248 [25152/225000 (11%)] Loss: 17838.449219\n",
      "Train Epoch: 248 [27648/225000 (12%)] Loss: 17610.910156\n",
      "Train Epoch: 248 [30144/225000 (13%)] Loss: 17770.371094\n",
      "Train Epoch: 248 [32640/225000 (15%)] Loss: 18010.175781\n",
      "Train Epoch: 248 [35136/225000 (16%)] Loss: 17803.806641\n",
      "Train Epoch: 248 [37632/225000 (17%)] Loss: 17386.972656\n",
      "Train Epoch: 248 [40128/225000 (18%)] Loss: 17966.765625\n",
      "Train Epoch: 248 [42624/225000 (19%)] Loss: 17930.519531\n",
      "Train Epoch: 248 [45120/225000 (20%)] Loss: 17442.285156\n",
      "Train Epoch: 248 [47616/225000 (21%)] Loss: 17762.851562\n",
      "Train Epoch: 248 [50112/225000 (22%)] Loss: 17973.332031\n",
      "Train Epoch: 248 [52608/225000 (23%)] Loss: 17412.666016\n",
      "Train Epoch: 248 [55104/225000 (24%)] Loss: 17391.957031\n",
      "Train Epoch: 248 [57600/225000 (26%)] Loss: 17545.927734\n",
      "Train Epoch: 248 [60096/225000 (27%)] Loss: 18052.339844\n",
      "Train Epoch: 248 [62592/225000 (28%)] Loss: 17935.292969\n",
      "Train Epoch: 248 [65088/225000 (29%)] Loss: 17834.195312\n",
      "Train Epoch: 248 [67584/225000 (30%)] Loss: 17419.505859\n",
      "Train Epoch: 248 [70080/225000 (31%)] Loss: 17641.421875\n",
      "Train Epoch: 248 [72576/225000 (32%)] Loss: 17457.548828\n",
      "Train Epoch: 248 [75072/225000 (33%)] Loss: 18230.611328\n",
      "Train Epoch: 248 [77568/225000 (34%)] Loss: 18011.992188\n",
      "Train Epoch: 248 [80064/225000 (36%)] Loss: 17376.234375\n",
      "Train Epoch: 248 [82560/225000 (37%)] Loss: 17362.714844\n",
      "Train Epoch: 248 [85056/225000 (38%)] Loss: 17344.296875\n",
      "Train Epoch: 248 [87552/225000 (39%)] Loss: 17744.273438\n",
      "Train Epoch: 248 [90048/225000 (40%)] Loss: 18084.021484\n",
      "Train Epoch: 248 [92544/225000 (41%)] Loss: 17627.839844\n",
      "Train Epoch: 248 [95040/225000 (42%)] Loss: 17961.468750\n",
      "Train Epoch: 248 [97536/225000 (43%)] Loss: 17351.578125\n",
      "Train Epoch: 248 [100032/225000 (44%)] Loss: 17673.605469\n",
      "Train Epoch: 248 [102528/225000 (46%)] Loss: 17982.593750\n",
      "Train Epoch: 248 [105024/225000 (47%)] Loss: 18089.777344\n",
      "Train Epoch: 248 [107520/225000 (48%)] Loss: 17909.496094\n",
      "Train Epoch: 248 [110016/225000 (49%)] Loss: 17843.509766\n",
      "Train Epoch: 248 [112512/225000 (50%)] Loss: 17218.773438\n",
      "Train Epoch: 248 [115008/225000 (51%)] Loss: 17755.308594\n",
      "Train Epoch: 248 [117504/225000 (52%)] Loss: 17588.728516\n",
      "Train Epoch: 248 [120000/225000 (53%)] Loss: 17507.621094\n",
      "Train Epoch: 248 [122496/225000 (54%)] Loss: 18380.453125\n",
      "Train Epoch: 248 [124992/225000 (56%)] Loss: 17993.097656\n",
      "Train Epoch: 248 [127488/225000 (57%)] Loss: 17595.652344\n",
      "Train Epoch: 248 [129984/225000 (58%)] Loss: 18102.625000\n",
      "Train Epoch: 248 [132480/225000 (59%)] Loss: 17670.240234\n",
      "Train Epoch: 248 [134976/225000 (60%)] Loss: 17854.750000\n",
      "Train Epoch: 248 [137472/225000 (61%)] Loss: 17476.640625\n",
      "Train Epoch: 248 [139968/225000 (62%)] Loss: 17839.925781\n",
      "Train Epoch: 248 [142464/225000 (63%)] Loss: 17783.144531\n",
      "Train Epoch: 248 [144960/225000 (64%)] Loss: 18050.167969\n",
      "Train Epoch: 248 [147456/225000 (66%)] Loss: 17586.177734\n",
      "Train Epoch: 248 [149952/225000 (67%)] Loss: 17918.423828\n",
      "Train Epoch: 248 [152448/225000 (68%)] Loss: 17764.105469\n",
      "Train Epoch: 248 [154944/225000 (69%)] Loss: 17914.642578\n",
      "Train Epoch: 248 [157440/225000 (70%)] Loss: 17957.363281\n",
      "Train Epoch: 248 [159936/225000 (71%)] Loss: 17773.861328\n",
      "Train Epoch: 248 [162432/225000 (72%)] Loss: 17832.054688\n",
      "Train Epoch: 248 [164928/225000 (73%)] Loss: 17695.289062\n",
      "Train Epoch: 248 [167424/225000 (74%)] Loss: 18126.273438\n",
      "Train Epoch: 248 [169920/225000 (76%)] Loss: 17782.546875\n",
      "Train Epoch: 248 [172416/225000 (77%)] Loss: 18505.570312\n",
      "Train Epoch: 248 [174912/225000 (78%)] Loss: 18055.060547\n",
      "Train Epoch: 248 [177408/225000 (79%)] Loss: 17454.527344\n",
      "Train Epoch: 248 [179904/225000 (80%)] Loss: 17634.691406\n",
      "Train Epoch: 248 [182400/225000 (81%)] Loss: 17938.052734\n",
      "Train Epoch: 248 [184896/225000 (82%)] Loss: 17697.859375\n",
      "Train Epoch: 248 [187392/225000 (83%)] Loss: 17946.562500\n",
      "Train Epoch: 248 [189888/225000 (84%)] Loss: 17894.212891\n",
      "Train Epoch: 248 [192384/225000 (86%)] Loss: 18069.078125\n",
      "Train Epoch: 248 [194880/225000 (87%)] Loss: 17644.230469\n",
      "Train Epoch: 248 [197376/225000 (88%)] Loss: 17459.089844\n",
      "Train Epoch: 248 [199872/225000 (89%)] Loss: 17626.609375\n",
      "Train Epoch: 248 [202368/225000 (90%)] Loss: 18225.859375\n",
      "Train Epoch: 248 [204864/225000 (91%)] Loss: 17650.179688\n",
      "Train Epoch: 248 [207360/225000 (92%)] Loss: 17728.414062\n",
      "Train Epoch: 248 [209856/225000 (93%)] Loss: 17378.835938\n",
      "Train Epoch: 248 [212352/225000 (94%)] Loss: 17904.822266\n",
      "Train Epoch: 248 [214848/225000 (95%)] Loss: 17767.550781\n",
      "Train Epoch: 248 [217344/225000 (97%)] Loss: 17802.664062\n",
      "Train Epoch: 248 [219840/225000 (98%)] Loss: 17477.839844\n",
      "Train Epoch: 248 [222336/225000 (99%)] Loss: 17682.718750\n",
      "Train Epoch: 248 [224832/225000 (100%)] Loss: 17840.609375\n",
      "    epoch          : 248\n",
      "    loss           : 17794.222262125375\n",
      "    val_loss       : 17684.217003321828\n",
      "Train Epoch: 249 [192/225000 (0%)] Loss: 17904.574219\n",
      "Train Epoch: 249 [2688/225000 (1%)] Loss: 17480.070312\n",
      "Train Epoch: 249 [5184/225000 (2%)] Loss: 17467.792969\n",
      "Train Epoch: 249 [7680/225000 (3%)] Loss: 17550.654297\n",
      "Train Epoch: 249 [10176/225000 (5%)] Loss: 17624.318359\n",
      "Train Epoch: 249 [12672/225000 (6%)] Loss: 17812.992188\n",
      "Train Epoch: 249 [15168/225000 (7%)] Loss: 17995.214844\n",
      "Train Epoch: 249 [17664/225000 (8%)] Loss: 17938.769531\n",
      "Train Epoch: 249 [20160/225000 (9%)] Loss: 17932.167969\n",
      "Train Epoch: 249 [22656/225000 (10%)] Loss: 17556.015625\n",
      "Train Epoch: 249 [25152/225000 (11%)] Loss: 17386.101562\n",
      "Train Epoch: 249 [27648/225000 (12%)] Loss: 17624.964844\n",
      "Train Epoch: 249 [30144/225000 (13%)] Loss: 18017.242188\n",
      "Train Epoch: 249 [32640/225000 (15%)] Loss: 18145.433594\n",
      "Train Epoch: 249 [35136/225000 (16%)] Loss: 18132.167969\n",
      "Train Epoch: 249 [37632/225000 (17%)] Loss: 17748.904297\n",
      "Train Epoch: 249 [40128/225000 (18%)] Loss: 17292.876953\n",
      "Train Epoch: 249 [42624/225000 (19%)] Loss: 18311.718750\n",
      "Train Epoch: 249 [45120/225000 (20%)] Loss: 17621.839844\n",
      "Train Epoch: 249 [47616/225000 (21%)] Loss: 17803.951172\n",
      "Train Epoch: 249 [50112/225000 (22%)] Loss: 17903.541016\n",
      "Train Epoch: 249 [52608/225000 (23%)] Loss: 17841.917969\n",
      "Train Epoch: 249 [55104/225000 (24%)] Loss: 17819.371094\n",
      "Train Epoch: 249 [57600/225000 (26%)] Loss: 17960.300781\n",
      "Train Epoch: 249 [60096/225000 (27%)] Loss: 18254.472656\n",
      "Train Epoch: 249 [62592/225000 (28%)] Loss: 17662.740234\n",
      "Train Epoch: 249 [65088/225000 (29%)] Loss: 17187.765625\n",
      "Train Epoch: 249 [67584/225000 (30%)] Loss: 17421.035156\n",
      "Train Epoch: 249 [70080/225000 (31%)] Loss: 17966.765625\n",
      "Train Epoch: 249 [72576/225000 (32%)] Loss: 17803.535156\n",
      "Train Epoch: 249 [75072/225000 (33%)] Loss: 17370.937500\n",
      "Train Epoch: 249 [77568/225000 (34%)] Loss: 17543.056641\n",
      "Train Epoch: 249 [80064/225000 (36%)] Loss: 18316.417969\n",
      "Train Epoch: 249 [82560/225000 (37%)] Loss: 17836.039062\n",
      "Train Epoch: 249 [85056/225000 (38%)] Loss: 17139.066406\n",
      "Train Epoch: 249 [87552/225000 (39%)] Loss: 17899.972656\n",
      "Train Epoch: 249 [90048/225000 (40%)] Loss: 18060.375000\n",
      "Train Epoch: 249 [92544/225000 (41%)] Loss: 18059.433594\n",
      "Train Epoch: 249 [95040/225000 (42%)] Loss: 17739.468750\n",
      "Train Epoch: 249 [97536/225000 (43%)] Loss: 17858.273438\n",
      "Train Epoch: 249 [100032/225000 (44%)] Loss: 17810.498047\n",
      "Train Epoch: 249 [102528/225000 (46%)] Loss: 17785.970703\n",
      "Train Epoch: 249 [105024/225000 (47%)] Loss: 17477.082031\n",
      "Train Epoch: 249 [107520/225000 (48%)] Loss: 17470.101562\n",
      "Train Epoch: 249 [110016/225000 (49%)] Loss: 17569.734375\n",
      "Train Epoch: 249 [112512/225000 (50%)] Loss: 17700.410156\n",
      "Train Epoch: 249 [115008/225000 (51%)] Loss: 17949.816406\n",
      "Train Epoch: 249 [117504/225000 (52%)] Loss: 17270.802734\n",
      "Train Epoch: 249 [120000/225000 (53%)] Loss: 17826.652344\n",
      "Train Epoch: 249 [122496/225000 (54%)] Loss: 17707.800781\n",
      "Train Epoch: 249 [124992/225000 (56%)] Loss: 18030.011719\n",
      "Train Epoch: 249 [127488/225000 (57%)] Loss: 17836.742188\n",
      "Train Epoch: 249 [129984/225000 (58%)] Loss: 17985.632812\n",
      "Train Epoch: 249 [132480/225000 (59%)] Loss: 17864.320312\n",
      "Train Epoch: 249 [134976/225000 (60%)] Loss: 17863.445312\n",
      "Train Epoch: 249 [137472/225000 (61%)] Loss: 17147.781250\n",
      "Train Epoch: 249 [139968/225000 (62%)] Loss: 18273.564453\n",
      "Train Epoch: 249 [142464/225000 (63%)] Loss: 17524.761719\n",
      "Train Epoch: 249 [144960/225000 (64%)] Loss: 17763.423828\n",
      "Train Epoch: 249 [147456/225000 (66%)] Loss: 17906.488281\n",
      "Train Epoch: 249 [149952/225000 (67%)] Loss: 17746.160156\n",
      "Train Epoch: 249 [152448/225000 (68%)] Loss: 17305.263672\n",
      "Train Epoch: 249 [154944/225000 (69%)] Loss: 18266.425781\n",
      "Train Epoch: 249 [157440/225000 (70%)] Loss: 17363.662109\n",
      "Train Epoch: 249 [159936/225000 (71%)] Loss: 17596.195312\n",
      "Train Epoch: 249 [162432/225000 (72%)] Loss: 17635.935547\n",
      "Train Epoch: 249 [164928/225000 (73%)] Loss: 17794.535156\n",
      "Train Epoch: 249 [167424/225000 (74%)] Loss: 17327.351562\n",
      "Train Epoch: 249 [169920/225000 (76%)] Loss: 18116.027344\n",
      "Train Epoch: 249 [172416/225000 (77%)] Loss: 18058.908203\n",
      "Train Epoch: 249 [174912/225000 (78%)] Loss: 17749.232422\n",
      "Train Epoch: 249 [177408/225000 (79%)] Loss: 17856.343750\n",
      "Train Epoch: 249 [179904/225000 (80%)] Loss: 17672.349609\n",
      "Train Epoch: 249 [182400/225000 (81%)] Loss: 17560.289062\n",
      "Train Epoch: 249 [184896/225000 (82%)] Loss: 17918.605469\n",
      "Train Epoch: 249 [187392/225000 (83%)] Loss: 17783.492188\n",
      "Train Epoch: 249 [189888/225000 (84%)] Loss: 17801.871094\n",
      "Train Epoch: 249 [192384/225000 (86%)] Loss: 17865.843750\n",
      "Train Epoch: 249 [194880/225000 (87%)] Loss: 17624.771484\n",
      "Train Epoch: 249 [197376/225000 (88%)] Loss: 17571.015625\n",
      "Train Epoch: 249 [199872/225000 (89%)] Loss: 17913.572266\n",
      "Train Epoch: 249 [202368/225000 (90%)] Loss: 17914.218750\n",
      "Train Epoch: 249 [204864/225000 (91%)] Loss: 17733.542969\n",
      "Train Epoch: 249 [207360/225000 (92%)] Loss: 17452.480469\n",
      "Train Epoch: 249 [209856/225000 (93%)] Loss: 17664.832031\n",
      "Train Epoch: 249 [212352/225000 (94%)] Loss: 17629.845703\n",
      "Train Epoch: 249 [214848/225000 (95%)] Loss: 17964.730469\n",
      "Train Epoch: 249 [217344/225000 (97%)] Loss: 17168.482422\n",
      "Train Epoch: 249 [219840/225000 (98%)] Loss: 17871.175781\n",
      "Train Epoch: 249 [222336/225000 (99%)] Loss: 17721.101562\n",
      "Train Epoch: 249 [224832/225000 (100%)] Loss: 17667.683594\n",
      "    epoch          : 249\n",
      "    loss           : 17773.20001949792\n",
      "    val_loss       : 17682.34929535771\n",
      "Train Epoch: 250 [192/225000 (0%)] Loss: 17827.662109\n",
      "Train Epoch: 250 [2688/225000 (1%)] Loss: 17604.537109\n",
      "Train Epoch: 250 [5184/225000 (2%)] Loss: 17539.998047\n",
      "Train Epoch: 250 [7680/225000 (3%)] Loss: 17268.976562\n",
      "Train Epoch: 250 [10176/225000 (5%)] Loss: 17581.054688\n",
      "Train Epoch: 250 [12672/225000 (6%)] Loss: 17759.558594\n",
      "Train Epoch: 250 [15168/225000 (7%)] Loss: 17353.515625\n",
      "Train Epoch: 250 [17664/225000 (8%)] Loss: 17928.677734\n",
      "Train Epoch: 250 [20160/225000 (9%)] Loss: 17558.447266\n",
      "Train Epoch: 250 [22656/225000 (10%)] Loss: 17833.300781\n",
      "Train Epoch: 250 [25152/225000 (11%)] Loss: 17798.666016\n",
      "Train Epoch: 250 [27648/225000 (12%)] Loss: 18260.300781\n",
      "Train Epoch: 250 [30144/225000 (13%)] Loss: 17749.429688\n",
      "Train Epoch: 250 [32640/225000 (15%)] Loss: 17470.968750\n",
      "Train Epoch: 250 [35136/225000 (16%)] Loss: 17661.304688\n",
      "Train Epoch: 250 [37632/225000 (17%)] Loss: 17970.017578\n",
      "Train Epoch: 250 [40128/225000 (18%)] Loss: 17997.835938\n",
      "Train Epoch: 250 [42624/225000 (19%)] Loss: 17704.386719\n",
      "Train Epoch: 250 [45120/225000 (20%)] Loss: 18075.843750\n",
      "Train Epoch: 250 [47616/225000 (21%)] Loss: 18007.320312\n",
      "Train Epoch: 250 [50112/225000 (22%)] Loss: 17776.328125\n",
      "Train Epoch: 250 [52608/225000 (23%)] Loss: 17685.851562\n",
      "Train Epoch: 250 [55104/225000 (24%)] Loss: 17825.654297\n",
      "Train Epoch: 250 [57600/225000 (26%)] Loss: 17670.003906\n",
      "Train Epoch: 250 [60096/225000 (27%)] Loss: 18073.886719\n",
      "Train Epoch: 250 [62592/225000 (28%)] Loss: 17856.503906\n",
      "Train Epoch: 250 [65088/225000 (29%)] Loss: 17601.753906\n",
      "Train Epoch: 250 [67584/225000 (30%)] Loss: 17783.210938\n",
      "Train Epoch: 250 [70080/225000 (31%)] Loss: 18160.953125\n",
      "Train Epoch: 250 [72576/225000 (32%)] Loss: 17892.562500\n",
      "Train Epoch: 250 [75072/225000 (33%)] Loss: 17605.261719\n",
      "Train Epoch: 250 [77568/225000 (34%)] Loss: 17805.152344\n",
      "Train Epoch: 250 [80064/225000 (36%)] Loss: 17720.417969\n",
      "Train Epoch: 250 [82560/225000 (37%)] Loss: 17800.550781\n",
      "Train Epoch: 250 [85056/225000 (38%)] Loss: 17030.669922\n",
      "Train Epoch: 250 [87552/225000 (39%)] Loss: 18021.673828\n",
      "Train Epoch: 250 [90048/225000 (40%)] Loss: 17867.876953\n",
      "Train Epoch: 250 [92544/225000 (41%)] Loss: 17697.226562\n",
      "Train Epoch: 250 [95040/225000 (42%)] Loss: 17769.277344\n",
      "Train Epoch: 250 [97536/225000 (43%)] Loss: 18003.332031\n",
      "Train Epoch: 250 [100032/225000 (44%)] Loss: 17760.324219\n",
      "Train Epoch: 250 [102528/225000 (46%)] Loss: 16931.269531\n",
      "Train Epoch: 250 [105024/225000 (47%)] Loss: 18007.132812\n",
      "Train Epoch: 250 [107520/225000 (48%)] Loss: 17692.056641\n",
      "Train Epoch: 250 [110016/225000 (49%)] Loss: 17581.738281\n",
      "Train Epoch: 250 [112512/225000 (50%)] Loss: 17264.757812\n",
      "Train Epoch: 250 [115008/225000 (51%)] Loss: 17812.503906\n",
      "Train Epoch: 250 [117504/225000 (52%)] Loss: 17600.488281\n",
      "Train Epoch: 250 [120000/225000 (53%)] Loss: 17909.871094\n",
      "Train Epoch: 250 [122496/225000 (54%)] Loss: 17927.859375\n",
      "Train Epoch: 250 [124992/225000 (56%)] Loss: 17343.988281\n",
      "Train Epoch: 250 [127488/225000 (57%)] Loss: 17798.472656\n",
      "Train Epoch: 250 [129984/225000 (58%)] Loss: 18236.552734\n",
      "Train Epoch: 250 [132480/225000 (59%)] Loss: 17493.218750\n",
      "Train Epoch: 250 [134976/225000 (60%)] Loss: 17910.523438\n",
      "Train Epoch: 250 [137472/225000 (61%)] Loss: 17797.406250\n",
      "Train Epoch: 250 [139968/225000 (62%)] Loss: 17576.312500\n",
      "Train Epoch: 250 [142464/225000 (63%)] Loss: 17494.138672\n",
      "Train Epoch: 250 [144960/225000 (64%)] Loss: 17585.851562\n",
      "Train Epoch: 250 [147456/225000 (66%)] Loss: 17657.714844\n",
      "Train Epoch: 250 [149952/225000 (67%)] Loss: 17199.425781\n",
      "Train Epoch: 250 [152448/225000 (68%)] Loss: 17900.927734\n",
      "Train Epoch: 250 [154944/225000 (69%)] Loss: 17857.107422\n",
      "Train Epoch: 250 [157440/225000 (70%)] Loss: 17524.015625\n",
      "Train Epoch: 250 [159936/225000 (71%)] Loss: 17977.902344\n",
      "Train Epoch: 250 [162432/225000 (72%)] Loss: 17738.875000\n",
      "Train Epoch: 250 [164928/225000 (73%)] Loss: 17960.853516\n",
      "Train Epoch: 250 [167424/225000 (74%)] Loss: 17526.312500\n",
      "Train Epoch: 250 [169920/225000 (76%)] Loss: 18062.503906\n",
      "Train Epoch: 250 [172416/225000 (77%)] Loss: 17773.406250\n",
      "Train Epoch: 250 [174912/225000 (78%)] Loss: 17683.667969\n",
      "Train Epoch: 250 [177408/225000 (79%)] Loss: 17832.601562\n",
      "Train Epoch: 250 [179904/225000 (80%)] Loss: 17506.949219\n",
      "Train Epoch: 250 [182400/225000 (81%)] Loss: 17378.687500\n",
      "Train Epoch: 250 [184896/225000 (82%)] Loss: 18164.933594\n",
      "Train Epoch: 250 [187392/225000 (83%)] Loss: 17654.457031\n",
      "Train Epoch: 250 [189888/225000 (84%)] Loss: 17354.292969\n",
      "Train Epoch: 250 [192384/225000 (86%)] Loss: 17563.589844\n",
      "Train Epoch: 250 [194880/225000 (87%)] Loss: 17721.095703\n",
      "Train Epoch: 250 [197376/225000 (88%)] Loss: 17929.351562\n",
      "Train Epoch: 250 [199872/225000 (89%)] Loss: 17642.146484\n",
      "Train Epoch: 250 [202368/225000 (90%)] Loss: 17793.708984\n",
      "Train Epoch: 250 [204864/225000 (91%)] Loss: 17795.324219\n",
      "Train Epoch: 250 [207360/225000 (92%)] Loss: 18584.341797\n",
      "Train Epoch: 250 [209856/225000 (93%)] Loss: 17755.464844\n",
      "Train Epoch: 250 [212352/225000 (94%)] Loss: 17950.175781\n",
      "Train Epoch: 250 [214848/225000 (95%)] Loss: 17945.695312\n",
      "Train Epoch: 250 [217344/225000 (97%)] Loss: 17639.044922\n",
      "Train Epoch: 250 [219840/225000 (98%)] Loss: 17945.033203\n",
      "Train Epoch: 250 [222336/225000 (99%)] Loss: 18128.480469\n",
      "Train Epoch: 250 [224832/225000 (100%)] Loss: 17340.492188\n",
      "    epoch          : 250\n",
      "    loss           : 17761.665849809353\n",
      "    val_loss       : 17702.471867196433\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0804_123131/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [192/225000 (0%)] Loss: 17855.429688\n",
      "Train Epoch: 251 [2688/225000 (1%)] Loss: 17561.449219\n",
      "Train Epoch: 251 [5184/225000 (2%)] Loss: 17565.406250\n",
      "Train Epoch: 251 [7680/225000 (3%)] Loss: 17893.753906\n",
      "Train Epoch: 251 [10176/225000 (5%)] Loss: 17680.962891\n",
      "Train Epoch: 251 [12672/225000 (6%)] Loss: 17516.847656\n",
      "Train Epoch: 251 [15168/225000 (7%)] Loss: 18007.269531\n",
      "Train Epoch: 251 [17664/225000 (8%)] Loss: 17592.859375\n",
      "Train Epoch: 251 [20160/225000 (9%)] Loss: 18053.310547\n",
      "Train Epoch: 251 [22656/225000 (10%)] Loss: 18160.007812\n",
      "Train Epoch: 251 [25152/225000 (11%)] Loss: 17523.433594\n",
      "Train Epoch: 251 [27648/225000 (12%)] Loss: 17455.281250\n",
      "Train Epoch: 251 [30144/225000 (13%)] Loss: 17551.564453\n",
      "Train Epoch: 251 [32640/225000 (15%)] Loss: 18112.753906\n",
      "Train Epoch: 251 [35136/225000 (16%)] Loss: 17923.003906\n",
      "Train Epoch: 251 [37632/225000 (17%)] Loss: 17622.062500\n",
      "Train Epoch: 251 [40128/225000 (18%)] Loss: 17779.892578\n",
      "Train Epoch: 251 [42624/225000 (19%)] Loss: 17704.339844\n",
      "Train Epoch: 251 [45120/225000 (20%)] Loss: 17500.921875\n",
      "Train Epoch: 251 [47616/225000 (21%)] Loss: 18057.296875\n",
      "Train Epoch: 251 [50112/225000 (22%)] Loss: 17498.945312\n",
      "Train Epoch: 251 [52608/225000 (23%)] Loss: 17768.949219\n",
      "Train Epoch: 251 [55104/225000 (24%)] Loss: 17686.611328\n",
      "Train Epoch: 251 [57600/225000 (26%)] Loss: 17880.601562\n",
      "Train Epoch: 251 [60096/225000 (27%)] Loss: 17610.011719\n",
      "Train Epoch: 251 [62592/225000 (28%)] Loss: 17769.644531\n",
      "Train Epoch: 251 [65088/225000 (29%)] Loss: 17754.886719\n",
      "Train Epoch: 251 [67584/225000 (30%)] Loss: 20396.347656\n",
      "Train Epoch: 251 [70080/225000 (31%)] Loss: 17337.828125\n",
      "Train Epoch: 251 [72576/225000 (32%)] Loss: 17870.855469\n",
      "Train Epoch: 251 [75072/225000 (33%)] Loss: 17238.937500\n",
      "Train Epoch: 251 [77568/225000 (34%)] Loss: 17566.917969\n",
      "Train Epoch: 251 [80064/225000 (36%)] Loss: 17851.589844\n",
      "Train Epoch: 251 [82560/225000 (37%)] Loss: 17626.826172\n",
      "Train Epoch: 251 [85056/225000 (38%)] Loss: 17673.902344\n",
      "Train Epoch: 251 [87552/225000 (39%)] Loss: 18054.582031\n",
      "Train Epoch: 251 [90048/225000 (40%)] Loss: 17748.500000\n",
      "Train Epoch: 251 [92544/225000 (41%)] Loss: 17566.357422\n",
      "Train Epoch: 251 [95040/225000 (42%)] Loss: 17981.925781\n",
      "Train Epoch: 251 [97536/225000 (43%)] Loss: 17261.892578\n",
      "Train Epoch: 251 [100032/225000 (44%)] Loss: 17246.445312\n",
      "Train Epoch: 251 [102528/225000 (46%)] Loss: 18281.705078\n",
      "Train Epoch: 251 [105024/225000 (47%)] Loss: 18099.585938\n",
      "Train Epoch: 251 [107520/225000 (48%)] Loss: 17952.488281\n",
      "Train Epoch: 251 [110016/225000 (49%)] Loss: 17867.947266\n",
      "Train Epoch: 251 [112512/225000 (50%)] Loss: 17720.449219\n",
      "Train Epoch: 251 [115008/225000 (51%)] Loss: 17835.535156\n",
      "Train Epoch: 251 [117504/225000 (52%)] Loss: 18026.794922\n",
      "Train Epoch: 251 [120000/225000 (53%)] Loss: 17418.824219\n",
      "Train Epoch: 251 [122496/225000 (54%)] Loss: 17719.765625\n",
      "Train Epoch: 251 [124992/225000 (56%)] Loss: 17952.966797\n",
      "Train Epoch: 251 [127488/225000 (57%)] Loss: 17721.037109\n",
      "Train Epoch: 251 [129984/225000 (58%)] Loss: 18002.542969\n",
      "Train Epoch: 251 [132480/225000 (59%)] Loss: 17843.851562\n",
      "Train Epoch: 251 [134976/225000 (60%)] Loss: 18009.636719\n",
      "Train Epoch: 251 [137472/225000 (61%)] Loss: 17707.406250\n",
      "Train Epoch: 251 [139968/225000 (62%)] Loss: 17832.943359\n",
      "Train Epoch: 251 [142464/225000 (63%)] Loss: 17600.064453\n",
      "Train Epoch: 251 [144960/225000 (64%)] Loss: 17504.697266\n",
      "Train Epoch: 251 [147456/225000 (66%)] Loss: 17158.859375\n",
      "Train Epoch: 251 [149952/225000 (67%)] Loss: 17412.996094\n",
      "Train Epoch: 251 [152448/225000 (68%)] Loss: 17990.062500\n",
      "Train Epoch: 251 [154944/225000 (69%)] Loss: 17729.421875\n",
      "Train Epoch: 251 [157440/225000 (70%)] Loss: 17945.111328\n",
      "Train Epoch: 251 [159936/225000 (71%)] Loss: 17743.324219\n",
      "Train Epoch: 251 [162432/225000 (72%)] Loss: 17624.441406\n",
      "Train Epoch: 251 [164928/225000 (73%)] Loss: 17456.500000\n",
      "Train Epoch: 251 [167424/225000 (74%)] Loss: 17566.843750\n",
      "Train Epoch: 251 [169920/225000 (76%)] Loss: 17078.878906\n",
      "Train Epoch: 251 [172416/225000 (77%)] Loss: 17821.031250\n",
      "Train Epoch: 251 [174912/225000 (78%)] Loss: 17282.921875\n",
      "Train Epoch: 251 [177408/225000 (79%)] Loss: 17571.742188\n",
      "Train Epoch: 251 [179904/225000 (80%)] Loss: 18070.654297\n",
      "Train Epoch: 251 [182400/225000 (81%)] Loss: 18140.753906\n",
      "Train Epoch: 251 [184896/225000 (82%)] Loss: 17743.871094\n",
      "Train Epoch: 251 [187392/225000 (83%)] Loss: 17770.062500\n",
      "Train Epoch: 251 [189888/225000 (84%)] Loss: 17719.871094\n",
      "Train Epoch: 251 [192384/225000 (86%)] Loss: 17969.222656\n",
      "Train Epoch: 251 [194880/225000 (87%)] Loss: 17767.498047\n",
      "Train Epoch: 251 [197376/225000 (88%)] Loss: 17158.398438\n",
      "Train Epoch: 251 [199872/225000 (89%)] Loss: 17797.482422\n",
      "Train Epoch: 251 [202368/225000 (90%)] Loss: 17790.632812\n",
      "Train Epoch: 251 [204864/225000 (91%)] Loss: 17541.921875\n",
      "Train Epoch: 251 [207360/225000 (92%)] Loss: 17536.460938\n",
      "Train Epoch: 251 [209856/225000 (93%)] Loss: 17669.033203\n",
      "Train Epoch: 251 [212352/225000 (94%)] Loss: 18491.091797\n",
      "Train Epoch: 251 [214848/225000 (95%)] Loss: 17978.273438\n",
      "Train Epoch: 251 [217344/225000 (97%)] Loss: 18060.250000\n",
      "Train Epoch: 251 [219840/225000 (98%)] Loss: 18009.257812\n",
      "Train Epoch: 251 [222336/225000 (99%)] Loss: 17861.660156\n",
      "Train Epoch: 251 [224832/225000 (100%)] Loss: 17812.587891\n",
      "    epoch          : 251\n",
      "    loss           : 17753.414769091298\n",
      "    val_loss       : 17670.557745905324\n",
      "Train Epoch: 252 [192/225000 (0%)] Loss: 17991.226562\n",
      "Train Epoch: 252 [2688/225000 (1%)] Loss: 18393.789062\n",
      "Train Epoch: 252 [5184/225000 (2%)] Loss: 18022.750000\n",
      "Train Epoch: 252 [7680/225000 (3%)] Loss: 17868.460938\n",
      "Train Epoch: 252 [10176/225000 (5%)] Loss: 17865.636719\n",
      "Train Epoch: 252 [12672/225000 (6%)] Loss: 17752.863281\n",
      "Train Epoch: 252 [15168/225000 (7%)] Loss: 18405.046875\n",
      "Train Epoch: 252 [17664/225000 (8%)] Loss: 17710.345703\n",
      "Train Epoch: 252 [20160/225000 (9%)] Loss: 17436.845703\n",
      "Train Epoch: 252 [22656/225000 (10%)] Loss: 17639.968750\n",
      "Train Epoch: 252 [25152/225000 (11%)] Loss: 17704.246094\n",
      "Train Epoch: 252 [27648/225000 (12%)] Loss: 18044.714844\n",
      "Train Epoch: 252 [30144/225000 (13%)] Loss: 17738.892578\n",
      "Train Epoch: 252 [32640/225000 (15%)] Loss: 17634.787109\n",
      "Train Epoch: 252 [35136/225000 (16%)] Loss: 17952.064453\n",
      "Train Epoch: 252 [37632/225000 (17%)] Loss: 17518.982422\n",
      "Train Epoch: 252 [40128/225000 (18%)] Loss: 17938.675781\n",
      "Train Epoch: 252 [42624/225000 (19%)] Loss: 17726.539062\n",
      "Train Epoch: 252 [45120/225000 (20%)] Loss: 17113.464844\n",
      "Train Epoch: 252 [47616/225000 (21%)] Loss: 17775.402344\n",
      "Train Epoch: 252 [50112/225000 (22%)] Loss: 17553.277344\n",
      "Train Epoch: 252 [52608/225000 (23%)] Loss: 17596.070312\n",
      "Train Epoch: 252 [55104/225000 (24%)] Loss: 17977.929688\n",
      "Train Epoch: 252 [57600/225000 (26%)] Loss: 17823.511719\n",
      "Train Epoch: 252 [60096/225000 (27%)] Loss: 17571.654297\n",
      "Train Epoch: 252 [62592/225000 (28%)] Loss: 17503.539062\n",
      "Train Epoch: 252 [65088/225000 (29%)] Loss: 17862.539062\n",
      "Train Epoch: 252 [67584/225000 (30%)] Loss: 18012.871094\n",
      "Train Epoch: 252 [70080/225000 (31%)] Loss: 21099.039062\n",
      "Train Epoch: 252 [72576/225000 (32%)] Loss: 17837.248047\n",
      "Train Epoch: 252 [75072/225000 (33%)] Loss: 18337.976562\n",
      "Train Epoch: 252 [77568/225000 (34%)] Loss: 17966.318359\n",
      "Train Epoch: 252 [80064/225000 (36%)] Loss: 17802.167969\n",
      "Train Epoch: 252 [82560/225000 (37%)] Loss: 17388.804688\n",
      "Train Epoch: 252 [85056/225000 (38%)] Loss: 17878.964844\n",
      "Train Epoch: 252 [87552/225000 (39%)] Loss: 17924.058594\n",
      "Train Epoch: 252 [90048/225000 (40%)] Loss: 17856.882812\n",
      "Train Epoch: 252 [92544/225000 (41%)] Loss: 17510.115234\n",
      "Train Epoch: 252 [95040/225000 (42%)] Loss: 17671.183594\n",
      "Train Epoch: 252 [97536/225000 (43%)] Loss: 17861.386719\n",
      "Train Epoch: 252 [100032/225000 (44%)] Loss: 17311.486328\n",
      "Train Epoch: 252 [102528/225000 (46%)] Loss: 17730.265625\n",
      "Train Epoch: 252 [105024/225000 (47%)] Loss: 17964.753906\n",
      "Train Epoch: 252 [107520/225000 (48%)] Loss: 17842.390625\n",
      "Train Epoch: 252 [110016/225000 (49%)] Loss: 17938.533203\n",
      "Train Epoch: 252 [112512/225000 (50%)] Loss: 17422.949219\n",
      "Train Epoch: 252 [115008/225000 (51%)] Loss: 17491.582031\n",
      "Train Epoch: 252 [117504/225000 (52%)] Loss: 18115.951172\n",
      "Train Epoch: 252 [120000/225000 (53%)] Loss: 17287.148438\n",
      "Train Epoch: 252 [122496/225000 (54%)] Loss: 17663.042969\n",
      "Train Epoch: 252 [124992/225000 (56%)] Loss: 18299.054688\n",
      "Train Epoch: 252 [127488/225000 (57%)] Loss: 17439.484375\n",
      "Train Epoch: 252 [129984/225000 (58%)] Loss: 17329.445312\n",
      "Train Epoch: 252 [132480/225000 (59%)] Loss: 17492.699219\n",
      "Train Epoch: 252 [134976/225000 (60%)] Loss: 17955.037109\n",
      "Train Epoch: 252 [137472/225000 (61%)] Loss: 17805.955078\n",
      "Train Epoch: 252 [139968/225000 (62%)] Loss: 17880.755859\n",
      "Train Epoch: 252 [142464/225000 (63%)] Loss: 17886.457031\n",
      "Train Epoch: 252 [144960/225000 (64%)] Loss: 17890.998047\n",
      "Train Epoch: 252 [147456/225000 (66%)] Loss: 18040.435547\n",
      "Train Epoch: 252 [149952/225000 (67%)] Loss: 17992.779297\n",
      "Train Epoch: 252 [152448/225000 (68%)] Loss: 17484.269531\n",
      "Train Epoch: 252 [154944/225000 (69%)] Loss: 17659.980469\n",
      "Train Epoch: 252 [157440/225000 (70%)] Loss: 17864.699219\n",
      "Train Epoch: 252 [159936/225000 (71%)] Loss: 17580.093750\n",
      "Train Epoch: 252 [162432/225000 (72%)] Loss: 17760.490234\n",
      "Train Epoch: 252 [164928/225000 (73%)] Loss: 17000.183594\n",
      "Train Epoch: 252 [167424/225000 (74%)] Loss: 18319.876953\n",
      "Train Epoch: 252 [169920/225000 (76%)] Loss: 17679.277344\n",
      "Train Epoch: 252 [172416/225000 (77%)] Loss: 17881.722656\n",
      "Train Epoch: 252 [174912/225000 (78%)] Loss: 17997.003906\n",
      "Train Epoch: 252 [177408/225000 (79%)] Loss: 18331.416016\n",
      "Train Epoch: 252 [179904/225000 (80%)] Loss: 18057.542969\n",
      "Train Epoch: 252 [182400/225000 (81%)] Loss: 17531.375000\n",
      "Train Epoch: 252 [184896/225000 (82%)] Loss: 17708.617188\n",
      "Train Epoch: 252 [187392/225000 (83%)] Loss: 17711.511719\n",
      "Train Epoch: 252 [189888/225000 (84%)] Loss: 17792.570312\n",
      "Train Epoch: 252 [192384/225000 (86%)] Loss: 17607.152344\n",
      "Train Epoch: 252 [194880/225000 (87%)] Loss: 17631.068359\n",
      "Train Epoch: 252 [197376/225000 (88%)] Loss: 17818.835938\n",
      "Train Epoch: 252 [199872/225000 (89%)] Loss: 17746.859375\n",
      "Train Epoch: 252 [202368/225000 (90%)] Loss: 18240.302734\n",
      "Train Epoch: 252 [204864/225000 (91%)] Loss: 17393.386719\n",
      "Train Epoch: 252 [207360/225000 (92%)] Loss: 17893.351562\n",
      "Train Epoch: 252 [209856/225000 (93%)] Loss: 17876.074219\n",
      "Train Epoch: 252 [212352/225000 (94%)] Loss: 17844.929688\n",
      "Train Epoch: 252 [214848/225000 (95%)] Loss: 17631.970703\n",
      "Train Epoch: 252 [217344/225000 (97%)] Loss: 17508.292969\n",
      "Train Epoch: 252 [219840/225000 (98%)] Loss: 17529.982422\n",
      "Train Epoch: 252 [222336/225000 (99%)] Loss: 17578.724609\n",
      "Train Epoch: 252 [224832/225000 (100%)] Loss: 17898.955078\n",
      "    epoch          : 252\n",
      "    loss           : 17757.22363697872\n",
      "    val_loss       : 17705.188543982178\n",
      "Train Epoch: 253 [192/225000 (0%)] Loss: 17970.234375\n",
      "Train Epoch: 253 [2688/225000 (1%)] Loss: 17752.023438\n",
      "Train Epoch: 253 [5184/225000 (2%)] Loss: 18026.474609\n",
      "Train Epoch: 253 [7680/225000 (3%)] Loss: 17972.128906\n",
      "Train Epoch: 253 [10176/225000 (5%)] Loss: 17403.593750\n",
      "Train Epoch: 253 [12672/225000 (6%)] Loss: 17402.062500\n",
      "Train Epoch: 253 [15168/225000 (7%)] Loss: 17793.414062\n",
      "Train Epoch: 253 [17664/225000 (8%)] Loss: 17438.691406\n",
      "Train Epoch: 253 [20160/225000 (9%)] Loss: 18216.335938\n",
      "Train Epoch: 253 [22656/225000 (10%)] Loss: 17554.314453\n",
      "Train Epoch: 253 [25152/225000 (11%)] Loss: 17404.574219\n",
      "Train Epoch: 253 [27648/225000 (12%)] Loss: 18121.312500\n",
      "Train Epoch: 253 [30144/225000 (13%)] Loss: 17448.660156\n",
      "Train Epoch: 253 [32640/225000 (15%)] Loss: 17390.742188\n",
      "Train Epoch: 253 [35136/225000 (16%)] Loss: 17525.148438\n",
      "Train Epoch: 253 [37632/225000 (17%)] Loss: 17960.511719\n",
      "Train Epoch: 253 [40128/225000 (18%)] Loss: 18035.876953\n",
      "Train Epoch: 253 [42624/225000 (19%)] Loss: 17582.667969\n",
      "Train Epoch: 253 [45120/225000 (20%)] Loss: 17444.753906\n",
      "Train Epoch: 253 [47616/225000 (21%)] Loss: 17414.140625\n",
      "Train Epoch: 253 [50112/225000 (22%)] Loss: 17620.257812\n",
      "Train Epoch: 253 [52608/225000 (23%)] Loss: 18103.554688\n",
      "Train Epoch: 253 [55104/225000 (24%)] Loss: 17790.925781\n",
      "Train Epoch: 253 [57600/225000 (26%)] Loss: 17230.449219\n",
      "Train Epoch: 253 [60096/225000 (27%)] Loss: 18216.304688\n",
      "Train Epoch: 253 [62592/225000 (28%)] Loss: 17684.972656\n",
      "Train Epoch: 253 [65088/225000 (29%)] Loss: 17777.457031\n",
      "Train Epoch: 253 [67584/225000 (30%)] Loss: 17934.421875\n",
      "Train Epoch: 253 [70080/225000 (31%)] Loss: 18112.630859\n",
      "Train Epoch: 253 [72576/225000 (32%)] Loss: 17829.429688\n",
      "Train Epoch: 253 [75072/225000 (33%)] Loss: 18029.964844\n",
      "Train Epoch: 253 [77568/225000 (34%)] Loss: 17495.281250\n",
      "Train Epoch: 253 [80064/225000 (36%)] Loss: 17467.320312\n",
      "Train Epoch: 253 [82560/225000 (37%)] Loss: 17828.898438\n",
      "Train Epoch: 253 [85056/225000 (38%)] Loss: 17443.742188\n",
      "Train Epoch: 253 [87552/225000 (39%)] Loss: 17597.791016\n",
      "Train Epoch: 253 [90048/225000 (40%)] Loss: 17601.070312\n",
      "Train Epoch: 253 [92544/225000 (41%)] Loss: 17704.144531\n",
      "Train Epoch: 253 [95040/225000 (42%)] Loss: 17827.845703\n",
      "Train Epoch: 253 [97536/225000 (43%)] Loss: 17957.306641\n",
      "Train Epoch: 253 [100032/225000 (44%)] Loss: 17570.027344\n",
      "Train Epoch: 253 [102528/225000 (46%)] Loss: 17709.250000\n",
      "Train Epoch: 253 [105024/225000 (47%)] Loss: 16949.054688\n",
      "Train Epoch: 253 [107520/225000 (48%)] Loss: 17542.275391\n",
      "Train Epoch: 253 [110016/225000 (49%)] Loss: 17411.671875\n",
      "Train Epoch: 253 [112512/225000 (50%)] Loss: 17562.066406\n",
      "Train Epoch: 253 [115008/225000 (51%)] Loss: 18009.429688\n",
      "Train Epoch: 253 [117504/225000 (52%)] Loss: 17811.710938\n",
      "Train Epoch: 253 [120000/225000 (53%)] Loss: 17647.541016\n",
      "Train Epoch: 253 [122496/225000 (54%)] Loss: 17850.644531\n",
      "Train Epoch: 253 [124992/225000 (56%)] Loss: 18015.683594\n",
      "Train Epoch: 253 [127488/225000 (57%)] Loss: 17858.152344\n",
      "Train Epoch: 253 [129984/225000 (58%)] Loss: 18203.406250\n",
      "Train Epoch: 253 [132480/225000 (59%)] Loss: 17773.542969\n",
      "Train Epoch: 253 [134976/225000 (60%)] Loss: 17666.890625\n",
      "Train Epoch: 253 [137472/225000 (61%)] Loss: 17900.380859\n",
      "Train Epoch: 253 [139968/225000 (62%)] Loss: 18091.166016\n",
      "Train Epoch: 253 [142464/225000 (63%)] Loss: 17472.716797\n",
      "Train Epoch: 253 [144960/225000 (64%)] Loss: 17007.205078\n",
      "Train Epoch: 253 [147456/225000 (66%)] Loss: 17384.271484\n",
      "Train Epoch: 253 [149952/225000 (67%)] Loss: 17775.236328\n",
      "Train Epoch: 253 [152448/225000 (68%)] Loss: 17489.992188\n",
      "Train Epoch: 253 [154944/225000 (69%)] Loss: 17638.085938\n",
      "Train Epoch: 253 [157440/225000 (70%)] Loss: 18011.640625\n",
      "Train Epoch: 253 [159936/225000 (71%)] Loss: 17514.722656\n",
      "Train Epoch: 253 [162432/225000 (72%)] Loss: 17841.816406\n",
      "Train Epoch: 253 [164928/225000 (73%)] Loss: 17921.914062\n",
      "Train Epoch: 253 [167424/225000 (74%)] Loss: 18150.511719\n",
      "Train Epoch: 253 [169920/225000 (76%)] Loss: 17361.796875\n",
      "Train Epoch: 253 [172416/225000 (77%)] Loss: 17405.367188\n",
      "Train Epoch: 253 [174912/225000 (78%)] Loss: 17963.294922\n",
      "Train Epoch: 253 [177408/225000 (79%)] Loss: 17788.853516\n",
      "Train Epoch: 253 [179904/225000 (80%)] Loss: 18076.500000\n",
      "Train Epoch: 253 [182400/225000 (81%)] Loss: 17731.267578\n",
      "Train Epoch: 253 [184896/225000 (82%)] Loss: 17903.875000\n",
      "Train Epoch: 253 [187392/225000 (83%)] Loss: 17737.591797\n",
      "Train Epoch: 253 [189888/225000 (84%)] Loss: 17492.798828\n",
      "Train Epoch: 253 [192384/225000 (86%)] Loss: 18150.218750\n",
      "Train Epoch: 253 [194880/225000 (87%)] Loss: 17305.101562\n",
      "Train Epoch: 253 [197376/225000 (88%)] Loss: 17667.417969\n",
      "Train Epoch: 253 [199872/225000 (89%)] Loss: 18213.601562\n",
      "Train Epoch: 253 [202368/225000 (90%)] Loss: 17532.093750\n",
      "Train Epoch: 253 [204864/225000 (91%)] Loss: 17515.296875\n",
      "Train Epoch: 253 [207360/225000 (92%)] Loss: 17451.152344\n",
      "Train Epoch: 253 [209856/225000 (93%)] Loss: 17438.742188\n",
      "Train Epoch: 253 [212352/225000 (94%)] Loss: 17692.302734\n",
      "Train Epoch: 253 [214848/225000 (95%)] Loss: 17929.496094\n",
      "Train Epoch: 253 [217344/225000 (97%)] Loss: 17074.039062\n",
      "Train Epoch: 253 [219840/225000 (98%)] Loss: 17501.792969\n",
      "Train Epoch: 253 [222336/225000 (99%)] Loss: 17464.523438\n",
      "Train Epoch: 253 [224832/225000 (100%)] Loss: 17486.365234\n",
      "    epoch          : 253\n",
      "    loss           : 17741.492622453607\n",
      "    val_loss       : 17661.499487729474\n",
      "Train Epoch: 254 [192/225000 (0%)] Loss: 17714.152344\n",
      "Train Epoch: 254 [2688/225000 (1%)] Loss: 17598.716797\n",
      "Train Epoch: 254 [5184/225000 (2%)] Loss: 17026.380859\n",
      "Train Epoch: 254 [7680/225000 (3%)] Loss: 17581.341797\n",
      "Train Epoch: 254 [10176/225000 (5%)] Loss: 18006.148438\n",
      "Train Epoch: 254 [12672/225000 (6%)] Loss: 17159.550781\n",
      "Train Epoch: 254 [15168/225000 (7%)] Loss: 17721.957031\n",
      "Train Epoch: 254 [17664/225000 (8%)] Loss: 17888.875000\n",
      "Train Epoch: 254 [20160/225000 (9%)] Loss: 17482.960938\n",
      "Train Epoch: 254 [22656/225000 (10%)] Loss: 18100.572266\n",
      "Train Epoch: 254 [25152/225000 (11%)] Loss: 18369.021484\n",
      "Train Epoch: 254 [27648/225000 (12%)] Loss: 17170.425781\n",
      "Train Epoch: 254 [30144/225000 (13%)] Loss: 17866.605469\n",
      "Train Epoch: 254 [32640/225000 (15%)] Loss: 17724.406250\n",
      "Train Epoch: 254 [35136/225000 (16%)] Loss: 17776.335938\n",
      "Train Epoch: 254 [37632/225000 (17%)] Loss: 17568.144531\n",
      "Train Epoch: 254 [40128/225000 (18%)] Loss: 18006.667969\n",
      "Train Epoch: 254 [42624/225000 (19%)] Loss: 17704.699219\n",
      "Train Epoch: 254 [45120/225000 (20%)] Loss: 17510.398438\n",
      "Train Epoch: 254 [47616/225000 (21%)] Loss: 17814.296875\n",
      "Train Epoch: 254 [50112/225000 (22%)] Loss: 17248.164062\n",
      "Train Epoch: 254 [52608/225000 (23%)] Loss: 17596.847656\n",
      "Train Epoch: 254 [55104/225000 (24%)] Loss: 17380.078125\n",
      "Train Epoch: 254 [57600/225000 (26%)] Loss: 18080.875000\n",
      "Train Epoch: 254 [60096/225000 (27%)] Loss: 17475.445312\n",
      "Train Epoch: 254 [62592/225000 (28%)] Loss: 17709.339844\n",
      "Train Epoch: 254 [65088/225000 (29%)] Loss: 17720.074219\n",
      "Train Epoch: 254 [67584/225000 (30%)] Loss: 17729.250000\n",
      "Train Epoch: 254 [70080/225000 (31%)] Loss: 17731.062500\n",
      "Train Epoch: 254 [72576/225000 (32%)] Loss: 18099.810547\n",
      "Train Epoch: 254 [75072/225000 (33%)] Loss: 18187.664062\n",
      "Train Epoch: 254 [77568/225000 (34%)] Loss: 18162.542969\n",
      "Train Epoch: 254 [80064/225000 (36%)] Loss: 18029.560547\n",
      "Train Epoch: 254 [82560/225000 (37%)] Loss: 17586.152344\n",
      "Train Epoch: 254 [85056/225000 (38%)] Loss: 17872.656250\n",
      "Train Epoch: 254 [87552/225000 (39%)] Loss: 17654.097656\n",
      "Train Epoch: 254 [90048/225000 (40%)] Loss: 17972.414062\n",
      "Train Epoch: 254 [92544/225000 (41%)] Loss: 17165.292969\n",
      "Train Epoch: 254 [95040/225000 (42%)] Loss: 17655.035156\n",
      "Train Epoch: 254 [97536/225000 (43%)] Loss: 17792.560547\n",
      "Train Epoch: 254 [100032/225000 (44%)] Loss: 17542.644531\n",
      "Train Epoch: 254 [102528/225000 (46%)] Loss: 18257.503906\n",
      "Train Epoch: 254 [105024/225000 (47%)] Loss: 17917.132812\n",
      "Train Epoch: 254 [107520/225000 (48%)] Loss: 17296.746094\n",
      "Train Epoch: 254 [110016/225000 (49%)] Loss: 17823.406250\n",
      "Train Epoch: 254 [112512/225000 (50%)] Loss: 17632.312500\n",
      "Train Epoch: 254 [115008/225000 (51%)] Loss: 17525.935547\n",
      "Train Epoch: 254 [117504/225000 (52%)] Loss: 17969.402344\n",
      "Train Epoch: 254 [120000/225000 (53%)] Loss: 18199.464844\n",
      "Train Epoch: 254 [122496/225000 (54%)] Loss: 17681.304688\n",
      "Train Epoch: 254 [124992/225000 (56%)] Loss: 17371.144531\n",
      "Train Epoch: 254 [127488/225000 (57%)] Loss: 17822.111328\n",
      "Train Epoch: 254 [129984/225000 (58%)] Loss: 17739.962891\n",
      "Train Epoch: 254 [132480/225000 (59%)] Loss: 17643.304688\n",
      "Train Epoch: 254 [134976/225000 (60%)] Loss: 17787.218750\n",
      "Train Epoch: 254 [137472/225000 (61%)] Loss: 18096.914062\n",
      "Train Epoch: 254 [139968/225000 (62%)] Loss: 17622.910156\n",
      "Train Epoch: 254 [142464/225000 (63%)] Loss: 18678.865234\n",
      "Train Epoch: 254 [144960/225000 (64%)] Loss: 17844.572266\n",
      "Train Epoch: 254 [147456/225000 (66%)] Loss: 18019.093750\n",
      "Train Epoch: 254 [149952/225000 (67%)] Loss: 17985.265625\n",
      "Train Epoch: 254 [152448/225000 (68%)] Loss: 17934.025391\n",
      "Train Epoch: 254 [154944/225000 (69%)] Loss: 17493.478516\n",
      "Train Epoch: 254 [157440/225000 (70%)] Loss: 17852.589844\n",
      "Train Epoch: 254 [159936/225000 (71%)] Loss: 17600.296875\n",
      "Train Epoch: 254 [162432/225000 (72%)] Loss: 17731.675781\n",
      "Train Epoch: 254 [164928/225000 (73%)] Loss: 17816.250000\n",
      "Train Epoch: 254 [167424/225000 (74%)] Loss: 17527.720703\n",
      "Train Epoch: 254 [169920/225000 (76%)] Loss: 17619.269531\n",
      "Train Epoch: 254 [172416/225000 (77%)] Loss: 17585.691406\n",
      "Train Epoch: 254 [174912/225000 (78%)] Loss: 17867.722656\n",
      "Train Epoch: 254 [177408/225000 (79%)] Loss: 20215.154297\n",
      "Train Epoch: 254 [179904/225000 (80%)] Loss: 17674.128906\n",
      "Train Epoch: 254 [182400/225000 (81%)] Loss: 17596.992188\n",
      "Train Epoch: 254 [184896/225000 (82%)] Loss: 17762.035156\n",
      "Train Epoch: 254 [187392/225000 (83%)] Loss: 17760.367188\n",
      "Train Epoch: 254 [189888/225000 (84%)] Loss: 17850.140625\n",
      "Train Epoch: 254 [192384/225000 (86%)] Loss: 17517.021484\n",
      "Train Epoch: 254 [194880/225000 (87%)] Loss: 17391.242188\n",
      "Train Epoch: 254 [197376/225000 (88%)] Loss: 17286.914062\n",
      "Train Epoch: 254 [199872/225000 (89%)] Loss: 17505.904297\n",
      "Train Epoch: 254 [202368/225000 (90%)] Loss: 17635.292969\n",
      "Train Epoch: 254 [204864/225000 (91%)] Loss: 17796.820312\n",
      "Train Epoch: 254 [207360/225000 (92%)] Loss: 17942.730469\n",
      "Train Epoch: 254 [209856/225000 (93%)] Loss: 17362.382812\n",
      "Train Epoch: 254 [212352/225000 (94%)] Loss: 17368.828125\n",
      "Train Epoch: 254 [214848/225000 (95%)] Loss: 17116.910156\n",
      "Train Epoch: 254 [217344/225000 (97%)] Loss: 18095.474609\n",
      "Train Epoch: 254 [219840/225000 (98%)] Loss: 17718.250000\n",
      "Train Epoch: 254 [222336/225000 (99%)] Loss: 18199.136719\n",
      "Train Epoch: 254 [224832/225000 (100%)] Loss: 17618.613281\n",
      "    epoch          : 254\n",
      "    loss           : 17745.047779903478\n",
      "    val_loss       : 17642.739841095365\n",
      "Train Epoch: 255 [192/225000 (0%)] Loss: 17576.496094\n",
      "Train Epoch: 255 [2688/225000 (1%)] Loss: 17624.578125\n",
      "Train Epoch: 255 [5184/225000 (2%)] Loss: 17821.695312\n",
      "Train Epoch: 255 [7680/225000 (3%)] Loss: 17661.781250\n",
      "Train Epoch: 255 [10176/225000 (5%)] Loss: 18108.351562\n",
      "Train Epoch: 255 [12672/225000 (6%)] Loss: 17779.164062\n",
      "Train Epoch: 255 [15168/225000 (7%)] Loss: 18200.132812\n",
      "Train Epoch: 255 [17664/225000 (8%)] Loss: 17841.226562\n",
      "Train Epoch: 255 [20160/225000 (9%)] Loss: 17850.410156\n",
      "Train Epoch: 255 [22656/225000 (10%)] Loss: 17929.941406\n",
      "Train Epoch: 255 [25152/225000 (11%)] Loss: 18158.027344\n",
      "Train Epoch: 255 [27648/225000 (12%)] Loss: 18078.226562\n",
      "Train Epoch: 255 [30144/225000 (13%)] Loss: 17566.617188\n",
      "Train Epoch: 255 [32640/225000 (15%)] Loss: 18244.976562\n",
      "Train Epoch: 255 [35136/225000 (16%)] Loss: 17411.232422\n",
      "Train Epoch: 255 [37632/225000 (17%)] Loss: 17847.037109\n",
      "Train Epoch: 255 [40128/225000 (18%)] Loss: 17860.511719\n",
      "Train Epoch: 255 [42624/225000 (19%)] Loss: 17515.597656\n",
      "Train Epoch: 255 [45120/225000 (20%)] Loss: 17436.277344\n",
      "Train Epoch: 255 [47616/225000 (21%)] Loss: 17569.632812\n",
      "Train Epoch: 255 [50112/225000 (22%)] Loss: 18085.371094\n",
      "Train Epoch: 255 [52608/225000 (23%)] Loss: 17876.992188\n",
      "Train Epoch: 255 [55104/225000 (24%)] Loss: 17403.984375\n",
      "Train Epoch: 255 [57600/225000 (26%)] Loss: 17662.781250\n",
      "Train Epoch: 255 [60096/225000 (27%)] Loss: 17868.429688\n",
      "Train Epoch: 255 [62592/225000 (28%)] Loss: 17410.675781\n",
      "Train Epoch: 255 [65088/225000 (29%)] Loss: 17624.250000\n",
      "Train Epoch: 255 [67584/225000 (30%)] Loss: 17897.476562\n",
      "Train Epoch: 255 [70080/225000 (31%)] Loss: 18091.609375\n",
      "Train Epoch: 255 [72576/225000 (32%)] Loss: 17607.574219\n",
      "Train Epoch: 255 [75072/225000 (33%)] Loss: 18248.152344\n",
      "Train Epoch: 255 [77568/225000 (34%)] Loss: 17346.882812\n",
      "Train Epoch: 255 [80064/225000 (36%)] Loss: 17285.175781\n",
      "Train Epoch: 255 [82560/225000 (37%)] Loss: 17979.314453\n",
      "Train Epoch: 255 [85056/225000 (38%)] Loss: 17640.542969\n",
      "Train Epoch: 255 [87552/225000 (39%)] Loss: 17778.708984\n",
      "Train Epoch: 255 [90048/225000 (40%)] Loss: 18002.101562\n",
      "Train Epoch: 255 [92544/225000 (41%)] Loss: 17468.193359\n",
      "Train Epoch: 255 [95040/225000 (42%)] Loss: 17557.591797\n",
      "Train Epoch: 255 [97536/225000 (43%)] Loss: 17803.097656\n",
      "Train Epoch: 255 [100032/225000 (44%)] Loss: 17822.373047\n",
      "Train Epoch: 255 [102528/225000 (46%)] Loss: 17577.054688\n",
      "Train Epoch: 255 [105024/225000 (47%)] Loss: 17493.363281\n",
      "Train Epoch: 255 [107520/225000 (48%)] Loss: 18044.656250\n",
      "Train Epoch: 255 [110016/225000 (49%)] Loss: 17991.724609\n",
      "Train Epoch: 255 [112512/225000 (50%)] Loss: 17665.302734\n",
      "Train Epoch: 255 [115008/225000 (51%)] Loss: 17091.306641\n",
      "Train Epoch: 255 [117504/225000 (52%)] Loss: 17993.878906\n",
      "Train Epoch: 255 [120000/225000 (53%)] Loss: 17775.363281\n",
      "Train Epoch: 255 [122496/225000 (54%)] Loss: 17626.792969\n",
      "Train Epoch: 255 [124992/225000 (56%)] Loss: 17753.492188\n",
      "Train Epoch: 255 [127488/225000 (57%)] Loss: 17834.423828\n",
      "Train Epoch: 255 [129984/225000 (58%)] Loss: 17929.472656\n",
      "Train Epoch: 255 [132480/225000 (59%)] Loss: 17736.746094\n",
      "Train Epoch: 255 [134976/225000 (60%)] Loss: 18216.447266\n",
      "Train Epoch: 255 [137472/225000 (61%)] Loss: 17735.167969\n",
      "Train Epoch: 255 [139968/225000 (62%)] Loss: 18053.187500\n",
      "Train Epoch: 255 [142464/225000 (63%)] Loss: 19826.953125\n",
      "Train Epoch: 255 [144960/225000 (64%)] Loss: 17822.640625\n",
      "Train Epoch: 255 [147456/225000 (66%)] Loss: 17877.953125\n",
      "Train Epoch: 255 [149952/225000 (67%)] Loss: 17555.753906\n",
      "Train Epoch: 255 [152448/225000 (68%)] Loss: 17844.031250\n",
      "Train Epoch: 255 [154944/225000 (69%)] Loss: 17475.785156\n",
      "Train Epoch: 255 [157440/225000 (70%)] Loss: 17956.234375\n",
      "Train Epoch: 255 [159936/225000 (71%)] Loss: 17645.691406\n",
      "Train Epoch: 255 [162432/225000 (72%)] Loss: 17597.337891\n",
      "Train Epoch: 255 [164928/225000 (73%)] Loss: 18254.148438\n",
      "Train Epoch: 255 [167424/225000 (74%)] Loss: 17551.601562\n",
      "Train Epoch: 255 [169920/225000 (76%)] Loss: 18065.964844\n",
      "Train Epoch: 255 [172416/225000 (77%)] Loss: 18001.091797\n",
      "Train Epoch: 255 [174912/225000 (78%)] Loss: 17620.105469\n",
      "Train Epoch: 255 [177408/225000 (79%)] Loss: 18042.332031\n",
      "Train Epoch: 255 [179904/225000 (80%)] Loss: 17563.187500\n",
      "Train Epoch: 255 [182400/225000 (81%)] Loss: 17359.894531\n",
      "Train Epoch: 255 [184896/225000 (82%)] Loss: 17818.355469\n",
      "Train Epoch: 255 [187392/225000 (83%)] Loss: 17851.306641\n",
      "Train Epoch: 255 [189888/225000 (84%)] Loss: 17512.261719\n",
      "Train Epoch: 255 [192384/225000 (86%)] Loss: 17739.113281\n",
      "Train Epoch: 255 [194880/225000 (87%)] Loss: 17683.207031\n",
      "Train Epoch: 255 [197376/225000 (88%)] Loss: 17672.486328\n",
      "Train Epoch: 255 [199872/225000 (89%)] Loss: 18400.335938\n",
      "Train Epoch: 255 [202368/225000 (90%)] Loss: 17810.195312\n",
      "Train Epoch: 255 [204864/225000 (91%)] Loss: 17942.464844\n",
      "Train Epoch: 255 [207360/225000 (92%)] Loss: 17664.964844\n",
      "Train Epoch: 255 [209856/225000 (93%)] Loss: 17287.802734\n",
      "Train Epoch: 255 [212352/225000 (94%)] Loss: 17798.378906\n",
      "Train Epoch: 255 [214848/225000 (95%)] Loss: 17153.492188\n",
      "Train Epoch: 255 [217344/225000 (97%)] Loss: 18063.642578\n",
      "Train Epoch: 255 [219840/225000 (98%)] Loss: 17604.335938\n",
      "Train Epoch: 255 [222336/225000 (99%)] Loss: 17883.605469\n",
      "Train Epoch: 255 [224832/225000 (100%)] Loss: 18027.398438\n",
      "    epoch          : 255\n",
      "    loss           : 17712.585050094658\n",
      "    val_loss       : 17693.810449049673\n",
      "Train Epoch: 256 [192/225000 (0%)] Loss: 18078.896484\n",
      "Train Epoch: 256 [2688/225000 (1%)] Loss: 18438.035156\n",
      "Train Epoch: 256 [5184/225000 (2%)] Loss: 17086.449219\n",
      "Train Epoch: 256 [7680/225000 (3%)] Loss: 17428.042969\n",
      "Train Epoch: 256 [10176/225000 (5%)] Loss: 17866.656250\n",
      "Train Epoch: 256 [12672/225000 (6%)] Loss: 18027.431641\n",
      "Train Epoch: 256 [15168/225000 (7%)] Loss: 17597.484375\n",
      "Train Epoch: 256 [17664/225000 (8%)] Loss: 17798.394531\n",
      "Train Epoch: 256 [20160/225000 (9%)] Loss: 17584.656250\n",
      "Train Epoch: 256 [22656/225000 (10%)] Loss: 17746.775391\n",
      "Train Epoch: 256 [25152/225000 (11%)] Loss: 17882.623047\n",
      "Train Epoch: 256 [27648/225000 (12%)] Loss: 18035.681641\n",
      "Train Epoch: 256 [30144/225000 (13%)] Loss: 17451.179688\n",
      "Train Epoch: 256 [32640/225000 (15%)] Loss: 17831.191406\n",
      "Train Epoch: 256 [35136/225000 (16%)] Loss: 17818.062500\n",
      "Train Epoch: 256 [37632/225000 (17%)] Loss: 18056.039062\n",
      "Train Epoch: 256 [40128/225000 (18%)] Loss: 18162.771484\n",
      "Train Epoch: 256 [42624/225000 (19%)] Loss: 17884.025391\n",
      "Train Epoch: 256 [45120/225000 (20%)] Loss: 17276.353516\n",
      "Train Epoch: 256 [47616/225000 (21%)] Loss: 17890.740234\n",
      "Train Epoch: 256 [50112/225000 (22%)] Loss: 17654.394531\n",
      "Train Epoch: 256 [52608/225000 (23%)] Loss: 17923.484375\n",
      "Train Epoch: 256 [55104/225000 (24%)] Loss: 18351.781250\n",
      "Train Epoch: 256 [57600/225000 (26%)] Loss: 17474.671875\n",
      "Train Epoch: 256 [60096/225000 (27%)] Loss: 18183.000000\n",
      "Train Epoch: 256 [62592/225000 (28%)] Loss: 17410.757812\n",
      "Train Epoch: 256 [65088/225000 (29%)] Loss: 17471.128906\n",
      "Train Epoch: 256 [67584/225000 (30%)] Loss: 17600.925781\n",
      "Train Epoch: 256 [70080/225000 (31%)] Loss: 17980.992188\n",
      "Train Epoch: 256 [72576/225000 (32%)] Loss: 17615.361328\n",
      "Train Epoch: 256 [75072/225000 (33%)] Loss: 17315.796875\n",
      "Train Epoch: 256 [77568/225000 (34%)] Loss: 17670.775391\n",
      "Train Epoch: 256 [80064/225000 (36%)] Loss: 17578.664062\n",
      "Train Epoch: 256 [82560/225000 (37%)] Loss: 17697.699219\n",
      "Train Epoch: 256 [85056/225000 (38%)] Loss: 17537.554688\n",
      "Train Epoch: 256 [87552/225000 (39%)] Loss: 18039.695312\n",
      "Train Epoch: 256 [90048/225000 (40%)] Loss: 17468.673828\n",
      "Train Epoch: 256 [92544/225000 (41%)] Loss: 17490.750000\n",
      "Train Epoch: 256 [95040/225000 (42%)] Loss: 17509.410156\n",
      "Train Epoch: 256 [97536/225000 (43%)] Loss: 17190.441406\n",
      "Train Epoch: 256 [100032/225000 (44%)] Loss: 17865.070312\n",
      "Train Epoch: 256 [102528/225000 (46%)] Loss: 17483.023438\n",
      "Train Epoch: 256 [105024/225000 (47%)] Loss: 17243.976562\n",
      "Train Epoch: 256 [107520/225000 (48%)] Loss: 17515.181641\n",
      "Train Epoch: 256 [110016/225000 (49%)] Loss: 17600.304688\n",
      "Train Epoch: 256 [112512/225000 (50%)] Loss: 17770.173828\n",
      "Train Epoch: 256 [115008/225000 (51%)] Loss: 17523.357422\n",
      "Train Epoch: 256 [117504/225000 (52%)] Loss: 17553.804688\n",
      "Train Epoch: 256 [120000/225000 (53%)] Loss: 17448.195312\n",
      "Train Epoch: 256 [122496/225000 (54%)] Loss: 17513.664062\n",
      "Train Epoch: 256 [124992/225000 (56%)] Loss: 17226.199219\n",
      "Train Epoch: 256 [127488/225000 (57%)] Loss: 18367.845703\n",
      "Train Epoch: 256 [129984/225000 (58%)] Loss: 17551.558594\n",
      "Train Epoch: 256 [132480/225000 (59%)] Loss: 17410.498047\n",
      "Train Epoch: 256 [134976/225000 (60%)] Loss: 17453.832031\n",
      "Train Epoch: 256 [137472/225000 (61%)] Loss: 17379.246094\n",
      "Train Epoch: 256 [139968/225000 (62%)] Loss: 17526.601562\n",
      "Train Epoch: 256 [142464/225000 (63%)] Loss: 17873.550781\n",
      "Train Epoch: 256 [144960/225000 (64%)] Loss: 17849.828125\n",
      "Train Epoch: 256 [147456/225000 (66%)] Loss: 17898.046875\n",
      "Train Epoch: 256 [149952/225000 (67%)] Loss: 17996.652344\n",
      "Train Epoch: 256 [152448/225000 (68%)] Loss: 17617.294922\n",
      "Train Epoch: 256 [154944/225000 (69%)] Loss: 18132.753906\n",
      "Train Epoch: 256 [157440/225000 (70%)] Loss: 17265.259766\n",
      "Train Epoch: 256 [159936/225000 (71%)] Loss: 17621.117188\n",
      "Train Epoch: 256 [162432/225000 (72%)] Loss: 17356.761719\n",
      "Train Epoch: 256 [164928/225000 (73%)] Loss: 17391.234375\n",
      "Train Epoch: 256 [167424/225000 (74%)] Loss: 18049.511719\n",
      "Train Epoch: 256 [169920/225000 (76%)] Loss: 17445.564453\n",
      "Train Epoch: 256 [172416/225000 (77%)] Loss: 17276.472656\n",
      "Train Epoch: 256 [174912/225000 (78%)] Loss: 17362.398438\n",
      "Train Epoch: 256 [177408/225000 (79%)] Loss: 17751.201172\n",
      "Train Epoch: 256 [179904/225000 (80%)] Loss: 18139.240234\n",
      "Train Epoch: 256 [182400/225000 (81%)] Loss: 17833.998047\n",
      "Train Epoch: 256 [184896/225000 (82%)] Loss: 17718.107422\n",
      "Train Epoch: 256 [187392/225000 (83%)] Loss: 17613.869141\n",
      "Train Epoch: 256 [189888/225000 (84%)] Loss: 18145.019531\n",
      "Train Epoch: 256 [192384/225000 (86%)] Loss: 17409.671875\n",
      "Train Epoch: 256 [194880/225000 (87%)] Loss: 17979.296875\n",
      "Train Epoch: 256 [197376/225000 (88%)] Loss: 17860.664062\n",
      "Train Epoch: 256 [199872/225000 (89%)] Loss: 17467.976562\n",
      "Train Epoch: 256 [202368/225000 (90%)] Loss: 17880.730469\n",
      "Train Epoch: 256 [204864/225000 (91%)] Loss: 17800.246094\n",
      "Train Epoch: 256 [207360/225000 (92%)] Loss: 17992.949219\n",
      "Train Epoch: 256 [209856/225000 (93%)] Loss: 17405.500000\n",
      "Train Epoch: 256 [212352/225000 (94%)] Loss: 17786.441406\n",
      "Train Epoch: 256 [214848/225000 (95%)] Loss: 17934.750000\n",
      "Train Epoch: 256 [217344/225000 (97%)] Loss: 17716.628906\n",
      "Train Epoch: 256 [219840/225000 (98%)] Loss: 17771.871094\n",
      "Train Epoch: 256 [222336/225000 (99%)] Loss: 17338.539062\n",
      "Train Epoch: 256 [224832/225000 (100%)] Loss: 17924.636719\n",
      "    epoch          : 256\n",
      "    loss           : 17701.60772517598\n",
      "    val_loss       : 17632.11741691418\n",
      "Train Epoch: 257 [192/225000 (0%)] Loss: 17610.648438\n",
      "Train Epoch: 257 [2688/225000 (1%)] Loss: 17666.500000\n",
      "Train Epoch: 257 [5184/225000 (2%)] Loss: 17281.687500\n",
      "Train Epoch: 257 [7680/225000 (3%)] Loss: 17786.611328\n",
      "Train Epoch: 257 [10176/225000 (5%)] Loss: 17587.994141\n",
      "Train Epoch: 257 [12672/225000 (6%)] Loss: 17526.535156\n",
      "Train Epoch: 257 [15168/225000 (7%)] Loss: 17231.806641\n",
      "Train Epoch: 257 [17664/225000 (8%)] Loss: 17421.548828\n",
      "Train Epoch: 257 [20160/225000 (9%)] Loss: 17229.986328\n",
      "Train Epoch: 257 [22656/225000 (10%)] Loss: 18010.828125\n",
      "Train Epoch: 257 [25152/225000 (11%)] Loss: 17555.558594\n",
      "Train Epoch: 257 [27648/225000 (12%)] Loss: 18038.167969\n",
      "Train Epoch: 257 [30144/225000 (13%)] Loss: 17480.308594\n",
      "Train Epoch: 257 [32640/225000 (15%)] Loss: 17158.171875\n",
      "Train Epoch: 257 [35136/225000 (16%)] Loss: 17950.425781\n",
      "Train Epoch: 257 [37632/225000 (17%)] Loss: 17556.261719\n",
      "Train Epoch: 257 [40128/225000 (18%)] Loss: 17997.814453\n",
      "Train Epoch: 257 [42624/225000 (19%)] Loss: 17884.326172\n",
      "Train Epoch: 257 [45120/225000 (20%)] Loss: 17922.921875\n",
      "Train Epoch: 257 [47616/225000 (21%)] Loss: 17836.390625\n",
      "Train Epoch: 257 [50112/225000 (22%)] Loss: 17828.349609\n",
      "Train Epoch: 257 [52608/225000 (23%)] Loss: 17572.082031\n",
      "Train Epoch: 257 [55104/225000 (24%)] Loss: 17701.949219\n",
      "Train Epoch: 257 [57600/225000 (26%)] Loss: 17791.527344\n",
      "Train Epoch: 257 [60096/225000 (27%)] Loss: 17621.386719\n",
      "Train Epoch: 257 [62592/225000 (28%)] Loss: 17709.636719\n",
      "Train Epoch: 257 [65088/225000 (29%)] Loss: 17410.367188\n",
      "Train Epoch: 257 [67584/225000 (30%)] Loss: 17636.121094\n",
      "Train Epoch: 257 [70080/225000 (31%)] Loss: 17772.951172\n",
      "Train Epoch: 257 [72576/225000 (32%)] Loss: 17762.421875\n",
      "Train Epoch: 257 [75072/225000 (33%)] Loss: 17487.978516\n",
      "Train Epoch: 257 [77568/225000 (34%)] Loss: 17098.671875\n",
      "Train Epoch: 257 [80064/225000 (36%)] Loss: 17620.976562\n",
      "Train Epoch: 257 [82560/225000 (37%)] Loss: 17516.062500\n",
      "Train Epoch: 257 [85056/225000 (38%)] Loss: 17720.437500\n",
      "Train Epoch: 257 [87552/225000 (39%)] Loss: 17435.021484\n",
      "Train Epoch: 257 [90048/225000 (40%)] Loss: 18013.449219\n",
      "Train Epoch: 257 [92544/225000 (41%)] Loss: 17625.917969\n",
      "Train Epoch: 257 [95040/225000 (42%)] Loss: 17640.695312\n",
      "Train Epoch: 257 [97536/225000 (43%)] Loss: 17738.029297\n",
      "Train Epoch: 257 [100032/225000 (44%)] Loss: 17814.376953\n",
      "Train Epoch: 257 [102528/225000 (46%)] Loss: 17609.664062\n",
      "Train Epoch: 257 [105024/225000 (47%)] Loss: 17637.984375\n",
      "Train Epoch: 257 [107520/225000 (48%)] Loss: 17743.070312\n",
      "Train Epoch: 257 [110016/225000 (49%)] Loss: 17606.208984\n",
      "Train Epoch: 257 [112512/225000 (50%)] Loss: 17730.464844\n",
      "Train Epoch: 257 [115008/225000 (51%)] Loss: 17523.576172\n",
      "Train Epoch: 257 [117504/225000 (52%)] Loss: 17450.138672\n",
      "Train Epoch: 257 [120000/225000 (53%)] Loss: 17767.890625\n",
      "Train Epoch: 257 [122496/225000 (54%)] Loss: 17742.500000\n",
      "Train Epoch: 257 [124992/225000 (56%)] Loss: 17324.869141\n",
      "Train Epoch: 257 [127488/225000 (57%)] Loss: 17878.726562\n",
      "Train Epoch: 257 [129984/225000 (58%)] Loss: 17371.560547\n",
      "Train Epoch: 257 [132480/225000 (59%)] Loss: 17726.951172\n",
      "Train Epoch: 257 [134976/225000 (60%)] Loss: 17987.890625\n",
      "Train Epoch: 257 [137472/225000 (61%)] Loss: 17767.496094\n",
      "Train Epoch: 257 [139968/225000 (62%)] Loss: 17454.087891\n",
      "Train Epoch: 257 [142464/225000 (63%)] Loss: 17588.457031\n",
      "Train Epoch: 257 [144960/225000 (64%)] Loss: 17656.554688\n",
      "Train Epoch: 257 [147456/225000 (66%)] Loss: 18046.820312\n",
      "Train Epoch: 257 [149952/225000 (67%)] Loss: 18269.453125\n",
      "Train Epoch: 257 [152448/225000 (68%)] Loss: 17837.246094\n",
      "Train Epoch: 257 [154944/225000 (69%)] Loss: 17904.115234\n",
      "Train Epoch: 257 [157440/225000 (70%)] Loss: 17687.535156\n",
      "Train Epoch: 257 [159936/225000 (71%)] Loss: 17684.863281\n",
      "Train Epoch: 257 [162432/225000 (72%)] Loss: 18009.074219\n",
      "Train Epoch: 257 [164928/225000 (73%)] Loss: 17697.250000\n",
      "Train Epoch: 257 [167424/225000 (74%)] Loss: 17759.839844\n",
      "Train Epoch: 257 [169920/225000 (76%)] Loss: 17919.296875\n",
      "Train Epoch: 257 [172416/225000 (77%)] Loss: 17725.593750\n",
      "Train Epoch: 257 [174912/225000 (78%)] Loss: 17621.843750\n",
      "Train Epoch: 257 [177408/225000 (79%)] Loss: 17774.724609\n",
      "Train Epoch: 257 [179904/225000 (80%)] Loss: 17750.187500\n",
      "Train Epoch: 257 [182400/225000 (81%)] Loss: 17189.537109\n",
      "Train Epoch: 257 [184896/225000 (82%)] Loss: 17760.037109\n",
      "Train Epoch: 257 [187392/225000 (83%)] Loss: 17573.531250\n",
      "Train Epoch: 257 [189888/225000 (84%)] Loss: 17460.945312\n",
      "Train Epoch: 257 [192384/225000 (86%)] Loss: 17216.017578\n",
      "Train Epoch: 257 [194880/225000 (87%)] Loss: 17159.984375\n",
      "Train Epoch: 257 [197376/225000 (88%)] Loss: 17835.025391\n",
      "Train Epoch: 257 [199872/225000 (89%)] Loss: 17619.761719\n",
      "Train Epoch: 257 [202368/225000 (90%)] Loss: 17404.945312\n",
      "Train Epoch: 257 [204864/225000 (91%)] Loss: 17646.878906\n",
      "Train Epoch: 257 [207360/225000 (92%)] Loss: 17969.125000\n",
      "Train Epoch: 257 [209856/225000 (93%)] Loss: 17663.582031\n",
      "Train Epoch: 257 [212352/225000 (94%)] Loss: 17939.642578\n",
      "Train Epoch: 257 [214848/225000 (95%)] Loss: 17847.546875\n",
      "Train Epoch: 257 [217344/225000 (97%)] Loss: 17793.882812\n",
      "Train Epoch: 257 [219840/225000 (98%)] Loss: 17818.617188\n",
      "Train Epoch: 257 [222336/225000 (99%)] Loss: 17703.523438\n",
      "Train Epoch: 257 [224832/225000 (100%)] Loss: 17913.851562\n",
      "    epoch          : 257\n",
      "    loss           : 17698.37921121747\n",
      "    val_loss       : 17620.246146898233\n",
      "Train Epoch: 258 [192/225000 (0%)] Loss: 17834.554688\n",
      "Train Epoch: 258 [2688/225000 (1%)] Loss: 17091.048828\n",
      "Train Epoch: 258 [5184/225000 (2%)] Loss: 17586.503906\n",
      "Train Epoch: 258 [7680/225000 (3%)] Loss: 17542.289062\n",
      "Train Epoch: 258 [10176/225000 (5%)] Loss: 17478.144531\n",
      "Train Epoch: 258 [12672/225000 (6%)] Loss: 17651.300781\n",
      "Train Epoch: 258 [15168/225000 (7%)] Loss: 17594.535156\n",
      "Train Epoch: 258 [17664/225000 (8%)] Loss: 17428.501953\n",
      "Train Epoch: 258 [20160/225000 (9%)] Loss: 17364.796875\n",
      "Train Epoch: 258 [22656/225000 (10%)] Loss: 17339.691406\n",
      "Train Epoch: 258 [25152/225000 (11%)] Loss: 17619.359375\n",
      "Train Epoch: 258 [27648/225000 (12%)] Loss: 17983.089844\n",
      "Train Epoch: 258 [30144/225000 (13%)] Loss: 18329.375000\n",
      "Train Epoch: 258 [32640/225000 (15%)] Loss: 17916.402344\n",
      "Train Epoch: 258 [35136/225000 (16%)] Loss: 17411.546875\n",
      "Train Epoch: 258 [37632/225000 (17%)] Loss: 17441.128906\n",
      "Train Epoch: 258 [40128/225000 (18%)] Loss: 17658.039062\n",
      "Train Epoch: 258 [42624/225000 (19%)] Loss: 18055.859375\n",
      "Train Epoch: 258 [45120/225000 (20%)] Loss: 17641.496094\n",
      "Train Epoch: 258 [47616/225000 (21%)] Loss: 17401.679688\n",
      "Train Epoch: 258 [50112/225000 (22%)] Loss: 18106.433594\n",
      "Train Epoch: 258 [52608/225000 (23%)] Loss: 17884.818359\n",
      "Train Epoch: 258 [55104/225000 (24%)] Loss: 17478.765625\n",
      "Train Epoch: 258 [57600/225000 (26%)] Loss: 17276.273438\n",
      "Train Epoch: 258 [60096/225000 (27%)] Loss: 17846.718750\n",
      "Train Epoch: 258 [62592/225000 (28%)] Loss: 17992.910156\n",
      "Train Epoch: 258 [65088/225000 (29%)] Loss: 17582.492188\n",
      "Train Epoch: 258 [67584/225000 (30%)] Loss: 17377.324219\n",
      "Train Epoch: 258 [70080/225000 (31%)] Loss: 17786.035156\n",
      "Train Epoch: 258 [72576/225000 (32%)] Loss: 18053.113281\n",
      "Train Epoch: 258 [75072/225000 (33%)] Loss: 17889.300781\n",
      "Train Epoch: 258 [77568/225000 (34%)] Loss: 18099.445312\n",
      "Train Epoch: 258 [80064/225000 (36%)] Loss: 17660.105469\n",
      "Train Epoch: 258 [82560/225000 (37%)] Loss: 17200.119141\n",
      "Train Epoch: 258 [85056/225000 (38%)] Loss: 17513.855469\n",
      "Train Epoch: 258 [87552/225000 (39%)] Loss: 17561.777344\n",
      "Train Epoch: 258 [90048/225000 (40%)] Loss: 17992.681641\n",
      "Train Epoch: 258 [92544/225000 (41%)] Loss: 17683.925781\n",
      "Train Epoch: 258 [95040/225000 (42%)] Loss: 17427.609375\n",
      "Train Epoch: 258 [97536/225000 (43%)] Loss: 17769.019531\n",
      "Train Epoch: 258 [100032/225000 (44%)] Loss: 17298.636719\n",
      "Train Epoch: 258 [102528/225000 (46%)] Loss: 17218.437500\n",
      "Train Epoch: 258 [105024/225000 (47%)] Loss: 17802.734375\n",
      "Train Epoch: 258 [107520/225000 (48%)] Loss: 17537.882812\n",
      "Train Epoch: 258 [110016/225000 (49%)] Loss: 17151.939453\n",
      "Train Epoch: 258 [112512/225000 (50%)] Loss: 17543.517578\n",
      "Train Epoch: 258 [115008/225000 (51%)] Loss: 17678.613281\n",
      "Train Epoch: 258 [117504/225000 (52%)] Loss: 17601.558594\n",
      "Train Epoch: 258 [120000/225000 (53%)] Loss: 17577.427734\n",
      "Train Epoch: 258 [122496/225000 (54%)] Loss: 16996.673828\n",
      "Train Epoch: 258 [124992/225000 (56%)] Loss: 17375.515625\n",
      "Train Epoch: 258 [127488/225000 (57%)] Loss: 17917.689453\n",
      "Train Epoch: 258 [129984/225000 (58%)] Loss: 17992.171875\n",
      "Train Epoch: 258 [132480/225000 (59%)] Loss: 17459.285156\n",
      "Train Epoch: 258 [134976/225000 (60%)] Loss: 17730.675781\n",
      "Train Epoch: 258 [137472/225000 (61%)] Loss: 17087.195312\n",
      "Train Epoch: 258 [139968/225000 (62%)] Loss: 17202.871094\n",
      "Train Epoch: 258 [142464/225000 (63%)] Loss: 17610.304688\n",
      "Train Epoch: 258 [144960/225000 (64%)] Loss: 17448.523438\n",
      "Train Epoch: 258 [147456/225000 (66%)] Loss: 18081.246094\n",
      "Train Epoch: 258 [149952/225000 (67%)] Loss: 17654.429688\n",
      "Train Epoch: 258 [152448/225000 (68%)] Loss: 17910.564453\n",
      "Train Epoch: 258 [154944/225000 (69%)] Loss: 17826.378906\n",
      "Train Epoch: 258 [157440/225000 (70%)] Loss: 18094.285156\n",
      "Train Epoch: 258 [159936/225000 (71%)] Loss: 17859.390625\n",
      "Train Epoch: 258 [162432/225000 (72%)] Loss: 17317.675781\n",
      "Train Epoch: 258 [164928/225000 (73%)] Loss: 17467.195312\n",
      "Train Epoch: 258 [167424/225000 (74%)] Loss: 17544.910156\n",
      "Train Epoch: 258 [169920/225000 (76%)] Loss: 17284.513672\n",
      "Train Epoch: 258 [172416/225000 (77%)] Loss: 17695.222656\n",
      "Train Epoch: 258 [174912/225000 (78%)] Loss: 17675.734375\n",
      "Train Epoch: 258 [177408/225000 (79%)] Loss: 18044.304688\n",
      "Train Epoch: 258 [179904/225000 (80%)] Loss: 17987.929688\n",
      "Train Epoch: 258 [182400/225000 (81%)] Loss: 17252.726562\n",
      "Train Epoch: 258 [184896/225000 (82%)] Loss: 17467.839844\n",
      "Train Epoch: 258 [187392/225000 (83%)] Loss: 16725.935547\n",
      "Train Epoch: 258 [189888/225000 (84%)] Loss: 18046.845703\n",
      "Train Epoch: 258 [192384/225000 (86%)] Loss: 17378.310547\n",
      "Train Epoch: 258 [194880/225000 (87%)] Loss: 17646.894531\n",
      "Train Epoch: 258 [197376/225000 (88%)] Loss: 17304.677734\n",
      "Train Epoch: 258 [199872/225000 (89%)] Loss: 17619.667969\n",
      "Train Epoch: 258 [202368/225000 (90%)] Loss: 17553.250000\n",
      "Train Epoch: 258 [204864/225000 (91%)] Loss: 17567.839844\n",
      "Train Epoch: 258 [207360/225000 (92%)] Loss: 17202.242188\n",
      "Train Epoch: 258 [209856/225000 (93%)] Loss: 17472.542969\n",
      "Train Epoch: 258 [212352/225000 (94%)] Loss: 18289.765625\n",
      "Train Epoch: 258 [214848/225000 (95%)] Loss: 18063.175781\n",
      "Train Epoch: 258 [217344/225000 (97%)] Loss: 17974.535156\n",
      "Train Epoch: 258 [219840/225000 (98%)] Loss: 18100.765625\n",
      "Train Epoch: 258 [222336/225000 (99%)] Loss: 17880.960938\n",
      "Train Epoch: 258 [224832/225000 (100%)] Loss: 17716.515625\n",
      "    epoch          : 258\n",
      "    loss           : 17689.989404463522\n",
      "    val_loss       : 17645.219355388\n",
      "Train Epoch: 259 [192/225000 (0%)] Loss: 18192.328125\n",
      "Train Epoch: 259 [2688/225000 (1%)] Loss: 17822.140625\n",
      "Train Epoch: 259 [5184/225000 (2%)] Loss: 18190.833984\n",
      "Train Epoch: 259 [7680/225000 (3%)] Loss: 17764.349609\n",
      "Train Epoch: 259 [10176/225000 (5%)] Loss: 17850.556641\n",
      "Train Epoch: 259 [12672/225000 (6%)] Loss: 17728.207031\n",
      "Train Epoch: 259 [15168/225000 (7%)] Loss: 17903.443359\n",
      "Train Epoch: 259 [17664/225000 (8%)] Loss: 17582.332031\n",
      "Train Epoch: 259 [20160/225000 (9%)] Loss: 17523.042969\n",
      "Train Epoch: 259 [22656/225000 (10%)] Loss: 17855.757812\n",
      "Train Epoch: 259 [25152/225000 (11%)] Loss: 17885.062500\n",
      "Train Epoch: 259 [27648/225000 (12%)] Loss: 17645.097656\n",
      "Train Epoch: 259 [30144/225000 (13%)] Loss: 17971.472656\n",
      "Train Epoch: 259 [32640/225000 (15%)] Loss: 17460.515625\n",
      "Train Epoch: 259 [35136/225000 (16%)] Loss: 17634.437500\n",
      "Train Epoch: 259 [37632/225000 (17%)] Loss: 17956.058594\n",
      "Train Epoch: 259 [40128/225000 (18%)] Loss: 17295.957031\n",
      "Train Epoch: 259 [42624/225000 (19%)] Loss: 17963.494141\n",
      "Train Epoch: 259 [45120/225000 (20%)] Loss: 17320.710938\n",
      "Train Epoch: 259 [47616/225000 (21%)] Loss: 18147.650391\n",
      "Train Epoch: 259 [50112/225000 (22%)] Loss: 17711.423828\n",
      "Train Epoch: 259 [52608/225000 (23%)] Loss: 17327.275391\n",
      "Train Epoch: 259 [55104/225000 (24%)] Loss: 17912.660156\n",
      "Train Epoch: 259 [57600/225000 (26%)] Loss: 17904.671875\n",
      "Train Epoch: 259 [60096/225000 (27%)] Loss: 17678.755859\n",
      "Train Epoch: 259 [62592/225000 (28%)] Loss: 17456.625000\n",
      "Train Epoch: 259 [65088/225000 (29%)] Loss: 17950.906250\n",
      "Train Epoch: 259 [67584/225000 (30%)] Loss: 17847.371094\n",
      "Train Epoch: 259 [70080/225000 (31%)] Loss: 17748.085938\n",
      "Train Epoch: 259 [72576/225000 (32%)] Loss: 17402.390625\n",
      "Train Epoch: 259 [75072/225000 (33%)] Loss: 17328.900391\n",
      "Train Epoch: 259 [77568/225000 (34%)] Loss: 17884.066406\n",
      "Train Epoch: 259 [80064/225000 (36%)] Loss: 17907.953125\n",
      "Train Epoch: 259 [82560/225000 (37%)] Loss: 18141.074219\n",
      "Train Epoch: 259 [85056/225000 (38%)] Loss: 17899.298828\n",
      "Train Epoch: 259 [87552/225000 (39%)] Loss: 17249.890625\n",
      "Train Epoch: 259 [90048/225000 (40%)] Loss: 17510.832031\n",
      "Train Epoch: 259 [92544/225000 (41%)] Loss: 17660.187500\n",
      "Train Epoch: 259 [95040/225000 (42%)] Loss: 17669.726562\n",
      "Train Epoch: 259 [97536/225000 (43%)] Loss: 17268.517578\n",
      "Train Epoch: 259 [100032/225000 (44%)] Loss: 17234.134766\n",
      "Train Epoch: 259 [102528/225000 (46%)] Loss: 17476.234375\n",
      "Train Epoch: 259 [105024/225000 (47%)] Loss: 17536.476562\n",
      "Train Epoch: 259 [107520/225000 (48%)] Loss: 17915.167969\n",
      "Train Epoch: 259 [110016/225000 (49%)] Loss: 18022.296875\n",
      "Train Epoch: 259 [112512/225000 (50%)] Loss: 17580.589844\n",
      "Train Epoch: 259 [115008/225000 (51%)] Loss: 18001.519531\n",
      "Train Epoch: 259 [117504/225000 (52%)] Loss: 18008.066406\n",
      "Train Epoch: 259 [120000/225000 (53%)] Loss: 18020.875000\n",
      "Train Epoch: 259 [122496/225000 (54%)] Loss: 17655.417969\n",
      "Train Epoch: 259 [124992/225000 (56%)] Loss: 17964.824219\n",
      "Train Epoch: 259 [127488/225000 (57%)] Loss: 17446.787109\n",
      "Train Epoch: 259 [129984/225000 (58%)] Loss: 17721.437500\n",
      "Train Epoch: 259 [132480/225000 (59%)] Loss: 17374.763672\n",
      "Train Epoch: 259 [134976/225000 (60%)] Loss: 17424.296875\n",
      "Train Epoch: 259 [137472/225000 (61%)] Loss: 18313.371094\n",
      "Train Epoch: 259 [139968/225000 (62%)] Loss: 17249.281250\n",
      "Train Epoch: 259 [142464/225000 (63%)] Loss: 17839.396484\n",
      "Train Epoch: 259 [144960/225000 (64%)] Loss: 17669.113281\n",
      "Train Epoch: 259 [147456/225000 (66%)] Loss: 17513.480469\n",
      "Train Epoch: 259 [149952/225000 (67%)] Loss: 17525.277344\n",
      "Train Epoch: 259 [152448/225000 (68%)] Loss: 17687.601562\n",
      "Train Epoch: 259 [154944/225000 (69%)] Loss: 17701.656250\n",
      "Train Epoch: 259 [157440/225000 (70%)] Loss: 17822.875000\n",
      "Train Epoch: 259 [159936/225000 (71%)] Loss: 17766.927734\n",
      "Train Epoch: 259 [162432/225000 (72%)] Loss: 17567.330078\n",
      "Train Epoch: 259 [164928/225000 (73%)] Loss: 17734.019531\n",
      "Train Epoch: 259 [167424/225000 (74%)] Loss: 18306.263672\n",
      "Train Epoch: 259 [169920/225000 (76%)] Loss: 17572.261719\n",
      "Train Epoch: 259 [172416/225000 (77%)] Loss: 17680.585938\n",
      "Train Epoch: 259 [174912/225000 (78%)] Loss: 17797.806641\n",
      "Train Epoch: 259 [177408/225000 (79%)] Loss: 17858.101562\n",
      "Train Epoch: 259 [179904/225000 (80%)] Loss: 17594.039062\n",
      "Train Epoch: 259 [182400/225000 (81%)] Loss: 17422.230469\n",
      "Train Epoch: 259 [184896/225000 (82%)] Loss: 17349.107422\n",
      "Train Epoch: 259 [187392/225000 (83%)] Loss: 17394.750000\n",
      "Train Epoch: 259 [189888/225000 (84%)] Loss: 17687.121094\n",
      "Train Epoch: 259 [192384/225000 (86%)] Loss: 17744.560547\n",
      "Train Epoch: 259 [194880/225000 (87%)] Loss: 17272.210938\n",
      "Train Epoch: 259 [197376/225000 (88%)] Loss: 17792.980469\n",
      "Train Epoch: 259 [199872/225000 (89%)] Loss: 17620.929688\n",
      "Train Epoch: 259 [202368/225000 (90%)] Loss: 17789.681641\n",
      "Train Epoch: 259 [204864/225000 (91%)] Loss: 17413.279297\n",
      "Train Epoch: 259 [207360/225000 (92%)] Loss: 18117.166016\n",
      "Train Epoch: 259 [209856/225000 (93%)] Loss: 17416.398438\n",
      "Train Epoch: 259 [212352/225000 (94%)] Loss: 18086.792969\n",
      "Train Epoch: 259 [214848/225000 (95%)] Loss: 17592.507812\n",
      "Train Epoch: 259 [217344/225000 (97%)] Loss: 17485.068359\n",
      "Train Epoch: 259 [219840/225000 (98%)] Loss: 17619.785156\n",
      "Train Epoch: 259 [222336/225000 (99%)] Loss: 17395.105469\n",
      "Train Epoch: 259 [224832/225000 (100%)] Loss: 17541.753906\n",
      "    epoch          : 259\n",
      "    loss           : 17676.288755032798\n",
      "    val_loss       : 17606.93051995758\n",
      "Train Epoch: 260 [192/225000 (0%)] Loss: 17671.015625\n",
      "Train Epoch: 260 [2688/225000 (1%)] Loss: 17430.464844\n",
      "Train Epoch: 260 [5184/225000 (2%)] Loss: 17748.029297\n",
      "Train Epoch: 260 [7680/225000 (3%)] Loss: 18158.492188\n",
      "Train Epoch: 260 [10176/225000 (5%)] Loss: 17403.294922\n",
      "Train Epoch: 260 [12672/225000 (6%)] Loss: 17442.410156\n",
      "Train Epoch: 260 [15168/225000 (7%)] Loss: 17348.519531\n",
      "Train Epoch: 260 [17664/225000 (8%)] Loss: 17977.636719\n",
      "Train Epoch: 260 [20160/225000 (9%)] Loss: 18021.714844\n",
      "Train Epoch: 260 [22656/225000 (10%)] Loss: 17579.072266\n",
      "Train Epoch: 260 [25152/225000 (11%)] Loss: 17305.160156\n",
      "Train Epoch: 260 [27648/225000 (12%)] Loss: 17493.982422\n",
      "Train Epoch: 260 [30144/225000 (13%)] Loss: 17912.964844\n",
      "Train Epoch: 260 [32640/225000 (15%)] Loss: 17703.068359\n",
      "Train Epoch: 260 [35136/225000 (16%)] Loss: 17588.712891\n",
      "Train Epoch: 260 [37632/225000 (17%)] Loss: 17854.070312\n",
      "Train Epoch: 260 [40128/225000 (18%)] Loss: 17473.072266\n",
      "Train Epoch: 260 [42624/225000 (19%)] Loss: 17781.201172\n",
      "Train Epoch: 260 [45120/225000 (20%)] Loss: 17903.748047\n",
      "Train Epoch: 260 [47616/225000 (21%)] Loss: 17676.812500\n",
      "Train Epoch: 260 [50112/225000 (22%)] Loss: 17769.601562\n",
      "Train Epoch: 260 [52608/225000 (23%)] Loss: 17652.671875\n",
      "Train Epoch: 260 [55104/225000 (24%)] Loss: 17919.316406\n",
      "Train Epoch: 260 [57600/225000 (26%)] Loss: 17637.015625\n",
      "Train Epoch: 260 [60096/225000 (27%)] Loss: 17901.892578\n",
      "Train Epoch: 260 [62592/225000 (28%)] Loss: 17421.910156\n",
      "Train Epoch: 260 [65088/225000 (29%)] Loss: 17964.046875\n",
      "Train Epoch: 260 [67584/225000 (30%)] Loss: 17752.398438\n",
      "Train Epoch: 260 [70080/225000 (31%)] Loss: 17464.074219\n",
      "Train Epoch: 260 [72576/225000 (32%)] Loss: 17778.187500\n",
      "Train Epoch: 260 [75072/225000 (33%)] Loss: 17991.107422\n",
      "Train Epoch: 260 [77568/225000 (34%)] Loss: 17909.060547\n",
      "Train Epoch: 260 [80064/225000 (36%)] Loss: 17161.837891\n",
      "Train Epoch: 260 [82560/225000 (37%)] Loss: 17851.931641\n",
      "Train Epoch: 260 [85056/225000 (38%)] Loss: 17879.222656\n",
      "Train Epoch: 260 [87552/225000 (39%)] Loss: 17272.119141\n",
      "Train Epoch: 260 [90048/225000 (40%)] Loss: 17346.605469\n",
      "Train Epoch: 260 [92544/225000 (41%)] Loss: 17455.562500\n",
      "Train Epoch: 260 [95040/225000 (42%)] Loss: 17989.179688\n",
      "Train Epoch: 260 [97536/225000 (43%)] Loss: 17812.210938\n",
      "Train Epoch: 260 [100032/225000 (44%)] Loss: 17890.828125\n",
      "Train Epoch: 260 [102528/225000 (46%)] Loss: 17678.380859\n",
      "Train Epoch: 260 [105024/225000 (47%)] Loss: 18092.726562\n",
      "Train Epoch: 260 [107520/225000 (48%)] Loss: 17582.576172\n",
      "Train Epoch: 260 [110016/225000 (49%)] Loss: 17606.351562\n",
      "Train Epoch: 260 [112512/225000 (50%)] Loss: 17986.699219\n",
      "Train Epoch: 260 [115008/225000 (51%)] Loss: 17670.300781\n",
      "Train Epoch: 260 [117504/225000 (52%)] Loss: 17944.990234\n",
      "Train Epoch: 260 [120000/225000 (53%)] Loss: 18075.455078\n",
      "Train Epoch: 260 [122496/225000 (54%)] Loss: 17500.136719\n",
      "Train Epoch: 260 [124992/225000 (56%)] Loss: 17515.996094\n",
      "Train Epoch: 260 [127488/225000 (57%)] Loss: 17970.671875\n",
      "Train Epoch: 260 [129984/225000 (58%)] Loss: 17783.335938\n",
      "Train Epoch: 260 [132480/225000 (59%)] Loss: 17573.103516\n",
      "Train Epoch: 260 [134976/225000 (60%)] Loss: 17772.966797\n",
      "Train Epoch: 260 [137472/225000 (61%)] Loss: 17563.441406\n",
      "Train Epoch: 260 [139968/225000 (62%)] Loss: 17653.832031\n",
      "Train Epoch: 260 [142464/225000 (63%)] Loss: 18179.525391\n",
      "Train Epoch: 260 [144960/225000 (64%)] Loss: 17477.902344\n",
      "Train Epoch: 260 [147456/225000 (66%)] Loss: 17595.289062\n",
      "Train Epoch: 260 [149952/225000 (67%)] Loss: 17594.521484\n",
      "Train Epoch: 260 [152448/225000 (68%)] Loss: 17930.195312\n",
      "Train Epoch: 260 [154944/225000 (69%)] Loss: 17878.429688\n",
      "Train Epoch: 260 [157440/225000 (70%)] Loss: 17687.078125\n",
      "Train Epoch: 260 [159936/225000 (71%)] Loss: 17947.835938\n",
      "Train Epoch: 260 [162432/225000 (72%)] Loss: 17963.523438\n",
      "Train Epoch: 260 [164928/225000 (73%)] Loss: 17659.347656\n",
      "Train Epoch: 260 [167424/225000 (74%)] Loss: 18053.296875\n",
      "Train Epoch: 260 [169920/225000 (76%)] Loss: 17463.562500\n",
      "Train Epoch: 260 [172416/225000 (77%)] Loss: 17366.912109\n",
      "Train Epoch: 260 [174912/225000 (78%)] Loss: 17836.898438\n",
      "Train Epoch: 260 [177408/225000 (79%)] Loss: 17785.292969\n",
      "Train Epoch: 260 [179904/225000 (80%)] Loss: 17743.150391\n",
      "Train Epoch: 260 [182400/225000 (81%)] Loss: 17384.933594\n",
      "Train Epoch: 260 [184896/225000 (82%)] Loss: 17597.179688\n",
      "Train Epoch: 260 [187392/225000 (83%)] Loss: 17826.625000\n",
      "Train Epoch: 260 [189888/225000 (84%)] Loss: 17362.414062\n",
      "Train Epoch: 260 [192384/225000 (86%)] Loss: 17485.259766\n",
      "Train Epoch: 260 [194880/225000 (87%)] Loss: 17849.078125\n",
      "Train Epoch: 260 [197376/225000 (88%)] Loss: 18071.214844\n",
      "Train Epoch: 260 [199872/225000 (89%)] Loss: 17684.642578\n",
      "Train Epoch: 260 [202368/225000 (90%)] Loss: 17784.378906\n",
      "Train Epoch: 260 [204864/225000 (91%)] Loss: 17490.197266\n",
      "Train Epoch: 260 [207360/225000 (92%)] Loss: 17453.767578\n",
      "Train Epoch: 260 [209856/225000 (93%)] Loss: 17813.660156\n",
      "Train Epoch: 260 [212352/225000 (94%)] Loss: 17819.187500\n",
      "Train Epoch: 260 [214848/225000 (95%)] Loss: 17386.597656\n",
      "Train Epoch: 260 [217344/225000 (97%)] Loss: 17045.429688\n",
      "Train Epoch: 260 [219840/225000 (98%)] Loss: 17140.875000\n",
      "Train Epoch: 260 [222336/225000 (99%)] Loss: 17596.855469\n",
      "Train Epoch: 260 [224832/225000 (100%)] Loss: 17879.078125\n",
      "    epoch          : 260\n",
      "    loss           : 17665.9911809407\n",
      "    val_loss       : 17614.581719613256\n",
      "Train Epoch: 261 [192/225000 (0%)] Loss: 17498.816406\n",
      "Train Epoch: 261 [2688/225000 (1%)] Loss: 17669.869141\n",
      "Train Epoch: 261 [5184/225000 (2%)] Loss: 17328.015625\n",
      "Train Epoch: 261 [7680/225000 (3%)] Loss: 17951.687500\n",
      "Train Epoch: 261 [10176/225000 (5%)] Loss: 17653.550781\n",
      "Train Epoch: 261 [12672/225000 (6%)] Loss: 17742.660156\n",
      "Train Epoch: 261 [15168/225000 (7%)] Loss: 17553.253906\n",
      "Train Epoch: 261 [17664/225000 (8%)] Loss: 17189.160156\n",
      "Train Epoch: 261 [20160/225000 (9%)] Loss: 17162.171875\n",
      "Train Epoch: 261 [22656/225000 (10%)] Loss: 17790.626953\n",
      "Train Epoch: 261 [25152/225000 (11%)] Loss: 17224.316406\n",
      "Train Epoch: 261 [27648/225000 (12%)] Loss: 17856.550781\n",
      "Train Epoch: 261 [30144/225000 (13%)] Loss: 17409.667969\n",
      "Train Epoch: 261 [32640/225000 (15%)] Loss: 17463.533203\n",
      "Train Epoch: 261 [35136/225000 (16%)] Loss: 17501.080078\n",
      "Train Epoch: 261 [37632/225000 (17%)] Loss: 17823.406250\n",
      "Train Epoch: 261 [40128/225000 (18%)] Loss: 17181.578125\n",
      "Train Epoch: 261 [42624/225000 (19%)] Loss: 17823.800781\n",
      "Train Epoch: 261 [45120/225000 (20%)] Loss: 18236.464844\n",
      "Train Epoch: 261 [47616/225000 (21%)] Loss: 17542.271484\n",
      "Train Epoch: 261 [50112/225000 (22%)] Loss: 17886.789062\n",
      "Train Epoch: 261 [52608/225000 (23%)] Loss: 17537.613281\n",
      "Train Epoch: 261 [55104/225000 (24%)] Loss: 17510.738281\n",
      "Train Epoch: 261 [57600/225000 (26%)] Loss: 17620.365234\n",
      "Train Epoch: 261 [60096/225000 (27%)] Loss: 17627.039062\n",
      "Train Epoch: 261 [62592/225000 (28%)] Loss: 17989.503906\n",
      "Train Epoch: 261 [65088/225000 (29%)] Loss: 17167.648438\n",
      "Train Epoch: 261 [67584/225000 (30%)] Loss: 17172.042969\n",
      "Train Epoch: 261 [70080/225000 (31%)] Loss: 17366.394531\n",
      "Train Epoch: 261 [72576/225000 (32%)] Loss: 17323.902344\n",
      "Train Epoch: 261 [75072/225000 (33%)] Loss: 17744.529297\n",
      "Train Epoch: 261 [77568/225000 (34%)] Loss: 17668.798828\n",
      "Train Epoch: 261 [80064/225000 (36%)] Loss: 17917.507812\n",
      "Train Epoch: 261 [82560/225000 (37%)] Loss: 17481.779297\n",
      "Train Epoch: 261 [85056/225000 (38%)] Loss: 17723.929688\n",
      "Train Epoch: 261 [87552/225000 (39%)] Loss: 17438.001953\n",
      "Train Epoch: 261 [90048/225000 (40%)] Loss: 17718.914062\n",
      "Train Epoch: 261 [92544/225000 (41%)] Loss: 17600.300781\n",
      "Train Epoch: 261 [95040/225000 (42%)] Loss: 17837.707031\n",
      "Train Epoch: 261 [97536/225000 (43%)] Loss: 17794.863281\n",
      "Train Epoch: 261 [100032/225000 (44%)] Loss: 17561.970703\n",
      "Train Epoch: 261 [102528/225000 (46%)] Loss: 17153.593750\n",
      "Train Epoch: 261 [105024/225000 (47%)] Loss: 18295.738281\n",
      "Train Epoch: 261 [107520/225000 (48%)] Loss: 17154.373047\n",
      "Train Epoch: 261 [110016/225000 (49%)] Loss: 17478.011719\n",
      "Train Epoch: 261 [112512/225000 (50%)] Loss: 17677.130859\n",
      "Train Epoch: 261 [115008/225000 (51%)] Loss: 18158.986328\n",
      "Train Epoch: 261 [117504/225000 (52%)] Loss: 17898.414062\n",
      "Train Epoch: 261 [120000/225000 (53%)] Loss: 18001.078125\n",
      "Train Epoch: 261 [122496/225000 (54%)] Loss: 17892.628906\n",
      "Train Epoch: 261 [124992/225000 (56%)] Loss: 17721.941406\n",
      "Train Epoch: 261 [127488/225000 (57%)] Loss: 17459.462891\n",
      "Train Epoch: 261 [129984/225000 (58%)] Loss: 17432.371094\n",
      "Train Epoch: 261 [132480/225000 (59%)] Loss: 17540.929688\n",
      "Train Epoch: 261 [134976/225000 (60%)] Loss: 17627.535156\n",
      "Train Epoch: 261 [137472/225000 (61%)] Loss: 17351.671875\n",
      "Train Epoch: 261 [139968/225000 (62%)] Loss: 17812.416016\n",
      "Train Epoch: 261 [142464/225000 (63%)] Loss: 17095.117188\n",
      "Train Epoch: 261 [144960/225000 (64%)] Loss: 17293.707031\n",
      "Train Epoch: 261 [147456/225000 (66%)] Loss: 17508.257812\n",
      "Train Epoch: 261 [149952/225000 (67%)] Loss: 18081.769531\n",
      "Train Epoch: 261 [152448/225000 (68%)] Loss: 17541.513672\n",
      "Train Epoch: 261 [154944/225000 (69%)] Loss: 17279.796875\n",
      "Train Epoch: 261 [157440/225000 (70%)] Loss: 17380.763672\n",
      "Train Epoch: 261 [159936/225000 (71%)] Loss: 17144.171875\n",
      "Train Epoch: 261 [162432/225000 (72%)] Loss: 17426.402344\n",
      "Train Epoch: 261 [164928/225000 (73%)] Loss: 17639.185547\n",
      "Train Epoch: 261 [167424/225000 (74%)] Loss: 17667.837891\n",
      "Train Epoch: 261 [169920/225000 (76%)] Loss: 17594.210938\n",
      "Train Epoch: 261 [172416/225000 (77%)] Loss: 17677.960938\n",
      "Train Epoch: 261 [174912/225000 (78%)] Loss: 17267.142578\n",
      "Train Epoch: 261 [177408/225000 (79%)] Loss: 17771.207031\n",
      "Train Epoch: 261 [179904/225000 (80%)] Loss: 17376.343750\n",
      "Train Epoch: 261 [182400/225000 (81%)] Loss: 17752.503906\n",
      "Train Epoch: 261 [184896/225000 (82%)] Loss: 18067.587891\n",
      "Train Epoch: 261 [187392/225000 (83%)] Loss: 17982.222656\n",
      "Train Epoch: 261 [189888/225000 (84%)] Loss: 17764.775391\n",
      "Train Epoch: 261 [192384/225000 (86%)] Loss: 17614.417969\n",
      "Train Epoch: 261 [194880/225000 (87%)] Loss: 17585.982422\n",
      "Train Epoch: 261 [197376/225000 (88%)] Loss: 17612.304688\n",
      "Train Epoch: 261 [199872/225000 (89%)] Loss: 17321.820312\n",
      "Train Epoch: 261 [202368/225000 (90%)] Loss: 17490.970703\n",
      "Train Epoch: 261 [204864/225000 (91%)] Loss: 18015.535156\n",
      "Train Epoch: 261 [207360/225000 (92%)] Loss: 17462.144531\n",
      "Train Epoch: 261 [209856/225000 (93%)] Loss: 17649.156250\n",
      "Train Epoch: 261 [212352/225000 (94%)] Loss: 17064.160156\n",
      "Train Epoch: 261 [214848/225000 (95%)] Loss: 17544.570312\n",
      "Train Epoch: 261 [217344/225000 (97%)] Loss: 17353.355469\n",
      "Train Epoch: 261 [219840/225000 (98%)] Loss: 17504.597656\n",
      "Train Epoch: 261 [222336/225000 (99%)] Loss: 17548.281250\n",
      "Train Epoch: 261 [224832/225000 (100%)] Loss: 18096.447266\n",
      "    epoch          : 261\n",
      "    loss           : 17660.530300101324\n",
      "    val_loss       : 17655.980104056933\n",
      "Train Epoch: 262 [192/225000 (0%)] Loss: 17432.203125\n",
      "Train Epoch: 262 [2688/225000 (1%)] Loss: 17376.302734\n",
      "Train Epoch: 262 [5184/225000 (2%)] Loss: 17336.958984\n",
      "Train Epoch: 262 [7680/225000 (3%)] Loss: 17150.589844\n",
      "Train Epoch: 262 [10176/225000 (5%)] Loss: 17414.406250\n",
      "Train Epoch: 262 [12672/225000 (6%)] Loss: 17480.210938\n",
      "Train Epoch: 262 [15168/225000 (7%)] Loss: 17692.656250\n",
      "Train Epoch: 262 [17664/225000 (8%)] Loss: 17230.839844\n",
      "Train Epoch: 262 [20160/225000 (9%)] Loss: 17464.748047\n",
      "Train Epoch: 262 [22656/225000 (10%)] Loss: 17516.601562\n",
      "Train Epoch: 262 [25152/225000 (11%)] Loss: 17119.710938\n",
      "Train Epoch: 262 [27648/225000 (12%)] Loss: 18075.898438\n",
      "Train Epoch: 262 [30144/225000 (13%)] Loss: 17331.574219\n",
      "Train Epoch: 262 [32640/225000 (15%)] Loss: 17738.503906\n",
      "Train Epoch: 262 [35136/225000 (16%)] Loss: 17661.693359\n",
      "Train Epoch: 262 [37632/225000 (17%)] Loss: 17563.925781\n",
      "Train Epoch: 262 [40128/225000 (18%)] Loss: 17489.355469\n",
      "Train Epoch: 262 [42624/225000 (19%)] Loss: 17856.687500\n",
      "Train Epoch: 262 [45120/225000 (20%)] Loss: 18283.273438\n",
      "Train Epoch: 262 [47616/225000 (21%)] Loss: 17390.699219\n",
      "Train Epoch: 262 [50112/225000 (22%)] Loss: 17432.279297\n",
      "Train Epoch: 262 [52608/225000 (23%)] Loss: 17831.730469\n",
      "Train Epoch: 262 [55104/225000 (24%)] Loss: 17612.636719\n",
      "Train Epoch: 262 [57600/225000 (26%)] Loss: 17402.718750\n",
      "Train Epoch: 262 [60096/225000 (27%)] Loss: 17831.187500\n",
      "Train Epoch: 262 [62592/225000 (28%)] Loss: 17763.078125\n",
      "Train Epoch: 262 [65088/225000 (29%)] Loss: 17863.781250\n",
      "Train Epoch: 262 [67584/225000 (30%)] Loss: 17469.816406\n",
      "Train Epoch: 262 [70080/225000 (31%)] Loss: 17289.498047\n",
      "Train Epoch: 262 [72576/225000 (32%)] Loss: 17603.281250\n",
      "Train Epoch: 262 [75072/225000 (33%)] Loss: 17963.707031\n",
      "Train Epoch: 262 [77568/225000 (34%)] Loss: 17483.832031\n",
      "Train Epoch: 262 [80064/225000 (36%)] Loss: 17294.210938\n",
      "Train Epoch: 262 [82560/225000 (37%)] Loss: 17547.654297\n",
      "Train Epoch: 262 [85056/225000 (38%)] Loss: 17427.750000\n",
      "Train Epoch: 262 [87552/225000 (39%)] Loss: 17687.949219\n",
      "Train Epoch: 262 [90048/225000 (40%)] Loss: 17447.109375\n",
      "Train Epoch: 262 [92544/225000 (41%)] Loss: 18274.617188\n",
      "Train Epoch: 262 [95040/225000 (42%)] Loss: 17601.136719\n",
      "Train Epoch: 262 [97536/225000 (43%)] Loss: 17881.468750\n",
      "Train Epoch: 262 [100032/225000 (44%)] Loss: 17453.632812\n",
      "Train Epoch: 262 [102528/225000 (46%)] Loss: 17290.970703\n",
      "Train Epoch: 262 [105024/225000 (47%)] Loss: 18072.058594\n",
      "Train Epoch: 262 [107520/225000 (48%)] Loss: 17638.777344\n",
      "Train Epoch: 262 [110016/225000 (49%)] Loss: 17651.402344\n",
      "Train Epoch: 262 [112512/225000 (50%)] Loss: 17185.966797\n",
      "Train Epoch: 262 [115008/225000 (51%)] Loss: 17837.320312\n",
      "Train Epoch: 262 [117504/225000 (52%)] Loss: 17572.546875\n",
      "Train Epoch: 262 [120000/225000 (53%)] Loss: 17644.562500\n",
      "Train Epoch: 262 [122496/225000 (54%)] Loss: 17591.554688\n",
      "Train Epoch: 262 [124992/225000 (56%)] Loss: 18074.312500\n",
      "Train Epoch: 262 [127488/225000 (57%)] Loss: 17399.144531\n",
      "Train Epoch: 262 [129984/225000 (58%)] Loss: 17586.318359\n",
      "Train Epoch: 262 [132480/225000 (59%)] Loss: 17405.675781\n",
      "Train Epoch: 262 [134976/225000 (60%)] Loss: 18183.867188\n",
      "Train Epoch: 262 [137472/225000 (61%)] Loss: 17380.431641\n",
      "Train Epoch: 262 [139968/225000 (62%)] Loss: 17627.546875\n",
      "Train Epoch: 262 [142464/225000 (63%)] Loss: 17501.339844\n",
      "Train Epoch: 262 [144960/225000 (64%)] Loss: 16881.816406\n",
      "Train Epoch: 262 [147456/225000 (66%)] Loss: 17426.681641\n",
      "Train Epoch: 262 [149952/225000 (67%)] Loss: 17996.234375\n",
      "Train Epoch: 262 [152448/225000 (68%)] Loss: 18131.269531\n",
      "Train Epoch: 262 [154944/225000 (69%)] Loss: 17469.335938\n",
      "Train Epoch: 262 [157440/225000 (70%)] Loss: 17582.378906\n",
      "Train Epoch: 262 [159936/225000 (71%)] Loss: 17147.187500\n",
      "Train Epoch: 262 [162432/225000 (72%)] Loss: 17943.363281\n",
      "Train Epoch: 262 [164928/225000 (73%)] Loss: 17104.492188\n",
      "Train Epoch: 262 [167424/225000 (74%)] Loss: 17564.990234\n",
      "Train Epoch: 262 [169920/225000 (76%)] Loss: 18055.234375\n",
      "Train Epoch: 262 [172416/225000 (77%)] Loss: 17600.767578\n",
      "Train Epoch: 262 [174912/225000 (78%)] Loss: 17153.238281\n",
      "Train Epoch: 262 [177408/225000 (79%)] Loss: 18125.408203\n",
      "Train Epoch: 262 [179904/225000 (80%)] Loss: 17485.466797\n",
      "Train Epoch: 262 [182400/225000 (81%)] Loss: 17939.078125\n",
      "Train Epoch: 262 [184896/225000 (82%)] Loss: 17845.207031\n",
      "Train Epoch: 262 [187392/225000 (83%)] Loss: 17640.187500\n",
      "Train Epoch: 262 [189888/225000 (84%)] Loss: 17925.410156\n",
      "Train Epoch: 262 [192384/225000 (86%)] Loss: 17598.820312\n",
      "Train Epoch: 262 [194880/225000 (87%)] Loss: 18058.367188\n",
      "Train Epoch: 262 [197376/225000 (88%)] Loss: 17318.050781\n",
      "Train Epoch: 262 [199872/225000 (89%)] Loss: 18442.480469\n",
      "Train Epoch: 262 [202368/225000 (90%)] Loss: 17931.382812\n",
      "Train Epoch: 262 [204864/225000 (91%)] Loss: 17502.037109\n",
      "Train Epoch: 262 [207360/225000 (92%)] Loss: 17682.976562\n",
      "Train Epoch: 262 [209856/225000 (93%)] Loss: 17097.423828\n",
      "Train Epoch: 262 [212352/225000 (94%)] Loss: 17566.181641\n",
      "Train Epoch: 262 [214848/225000 (95%)] Loss: 17920.710938\n",
      "Train Epoch: 262 [217344/225000 (97%)] Loss: 17598.171875\n",
      "Train Epoch: 262 [219840/225000 (98%)] Loss: 17300.361328\n",
      "Train Epoch: 262 [222336/225000 (99%)] Loss: 17883.843750\n",
      "Train Epoch: 262 [224832/225000 (100%)] Loss: 18069.476562\n",
      "    epoch          : 262\n",
      "    loss           : 17656.834326005228\n",
      "    val_loss       : 17598.599504714704\n",
      "Train Epoch: 263 [192/225000 (0%)] Loss: 18144.593750\n",
      "Train Epoch: 263 [2688/225000 (1%)] Loss: 17966.324219\n",
      "Train Epoch: 263 [5184/225000 (2%)] Loss: 17589.388672\n",
      "Train Epoch: 263 [7680/225000 (3%)] Loss: 17456.085938\n",
      "Train Epoch: 263 [10176/225000 (5%)] Loss: 17694.523438\n",
      "Train Epoch: 263 [12672/225000 (6%)] Loss: 17597.804688\n",
      "Train Epoch: 263 [15168/225000 (7%)] Loss: 18010.287109\n",
      "Train Epoch: 263 [17664/225000 (8%)] Loss: 17472.511719\n",
      "Train Epoch: 263 [20160/225000 (9%)] Loss: 17299.158203\n",
      "Train Epoch: 263 [22656/225000 (10%)] Loss: 17954.382812\n",
      "Train Epoch: 263 [25152/225000 (11%)] Loss: 17523.611328\n",
      "Train Epoch: 263 [27648/225000 (12%)] Loss: 18162.472656\n",
      "Train Epoch: 263 [30144/225000 (13%)] Loss: 17247.136719\n",
      "Train Epoch: 263 [32640/225000 (15%)] Loss: 17345.310547\n",
      "Train Epoch: 263 [35136/225000 (16%)] Loss: 17892.183594\n",
      "Train Epoch: 263 [37632/225000 (17%)] Loss: 17615.621094\n",
      "Train Epoch: 263 [40128/225000 (18%)] Loss: 17400.835938\n",
      "Train Epoch: 263 [42624/225000 (19%)] Loss: 17540.523438\n",
      "Train Epoch: 263 [45120/225000 (20%)] Loss: 17831.406250\n",
      "Train Epoch: 263 [47616/225000 (21%)] Loss: 17743.062500\n",
      "Train Epoch: 263 [50112/225000 (22%)] Loss: 17620.488281\n",
      "Train Epoch: 263 [52608/225000 (23%)] Loss: 17657.625000\n",
      "Train Epoch: 263 [55104/225000 (24%)] Loss: 17935.597656\n",
      "Train Epoch: 263 [57600/225000 (26%)] Loss: 17696.671875\n",
      "Train Epoch: 263 [60096/225000 (27%)] Loss: 17625.800781\n",
      "Train Epoch: 263 [62592/225000 (28%)] Loss: 17576.406250\n",
      "Train Epoch: 263 [65088/225000 (29%)] Loss: 17137.609375\n",
      "Train Epoch: 263 [67584/225000 (30%)] Loss: 17342.748047\n",
      "Train Epoch: 263 [70080/225000 (31%)] Loss: 17753.734375\n",
      "Train Epoch: 263 [72576/225000 (32%)] Loss: 17565.539062\n",
      "Train Epoch: 263 [75072/225000 (33%)] Loss: 17582.152344\n",
      "Train Epoch: 263 [77568/225000 (34%)] Loss: 17620.933594\n",
      "Train Epoch: 263 [80064/225000 (36%)] Loss: 17307.152344\n",
      "Train Epoch: 263 [82560/225000 (37%)] Loss: 17668.998047\n",
      "Train Epoch: 263 [85056/225000 (38%)] Loss: 17211.603516\n",
      "Train Epoch: 263 [87552/225000 (39%)] Loss: 17326.375000\n",
      "Train Epoch: 263 [90048/225000 (40%)] Loss: 17784.218750\n",
      "Train Epoch: 263 [92544/225000 (41%)] Loss: 17227.375000\n",
      "Train Epoch: 263 [95040/225000 (42%)] Loss: 17374.339844\n",
      "Train Epoch: 263 [97536/225000 (43%)] Loss: 17763.812500\n",
      "Train Epoch: 263 [100032/225000 (44%)] Loss: 17684.031250\n",
      "Train Epoch: 263 [102528/225000 (46%)] Loss: 17539.052734\n",
      "Train Epoch: 263 [105024/225000 (47%)] Loss: 17987.634766\n",
      "Train Epoch: 263 [107520/225000 (48%)] Loss: 17535.832031\n",
      "Train Epoch: 263 [110016/225000 (49%)] Loss: 17599.972656\n",
      "Train Epoch: 263 [112512/225000 (50%)] Loss: 17501.742188\n",
      "Train Epoch: 263 [115008/225000 (51%)] Loss: 17859.746094\n",
      "Train Epoch: 263 [117504/225000 (52%)] Loss: 17236.734375\n",
      "Train Epoch: 263 [120000/225000 (53%)] Loss: 17758.304688\n",
      "Train Epoch: 263 [122496/225000 (54%)] Loss: 17751.746094\n",
      "Train Epoch: 263 [124992/225000 (56%)] Loss: 18056.533203\n",
      "Train Epoch: 263 [127488/225000 (57%)] Loss: 17503.523438\n",
      "Train Epoch: 263 [129984/225000 (58%)] Loss: 17378.332031\n",
      "Train Epoch: 263 [132480/225000 (59%)] Loss: 17584.263672\n",
      "Train Epoch: 263 [134976/225000 (60%)] Loss: 17491.880859\n",
      "Train Epoch: 263 [137472/225000 (61%)] Loss: 17805.882812\n",
      "Train Epoch: 263 [139968/225000 (62%)] Loss: 17477.046875\n",
      "Train Epoch: 263 [142464/225000 (63%)] Loss: 17463.923828\n",
      "Train Epoch: 263 [144960/225000 (64%)] Loss: 17655.546875\n",
      "Train Epoch: 263 [147456/225000 (66%)] Loss: 17678.998047\n",
      "Train Epoch: 263 [149952/225000 (67%)] Loss: 17532.781250\n",
      "Train Epoch: 263 [152448/225000 (68%)] Loss: 17700.656250\n",
      "Train Epoch: 263 [154944/225000 (69%)] Loss: 17354.345703\n",
      "Train Epoch: 263 [157440/225000 (70%)] Loss: 17576.316406\n",
      "Train Epoch: 263 [159936/225000 (71%)] Loss: 17171.218750\n",
      "Train Epoch: 263 [162432/225000 (72%)] Loss: 17622.947266\n",
      "Train Epoch: 263 [164928/225000 (73%)] Loss: 17697.699219\n",
      "Train Epoch: 263 [167424/225000 (74%)] Loss: 17561.558594\n",
      "Train Epoch: 263 [169920/225000 (76%)] Loss: 18210.742188\n",
      "Train Epoch: 263 [172416/225000 (77%)] Loss: 17492.931641\n",
      "Train Epoch: 263 [174912/225000 (78%)] Loss: 17352.525391\n",
      "Train Epoch: 263 [177408/225000 (79%)] Loss: 17560.925781\n",
      "Train Epoch: 263 [179904/225000 (80%)] Loss: 17918.523438\n",
      "Train Epoch: 263 [182400/225000 (81%)] Loss: 17582.652344\n",
      "Train Epoch: 263 [184896/225000 (82%)] Loss: 17330.902344\n",
      "Train Epoch: 263 [187392/225000 (83%)] Loss: 17786.451172\n",
      "Train Epoch: 263 [189888/225000 (84%)] Loss: 17213.570312\n",
      "Train Epoch: 263 [192384/225000 (86%)] Loss: 17960.472656\n",
      "Train Epoch: 263 [194880/225000 (87%)] Loss: 17513.582031\n",
      "Train Epoch: 263 [197376/225000 (88%)] Loss: 17570.636719\n",
      "Train Epoch: 263 [199872/225000 (89%)] Loss: 17299.410156\n",
      "Train Epoch: 263 [202368/225000 (90%)] Loss: 17836.167969\n",
      "Train Epoch: 263 [204864/225000 (91%)] Loss: 17804.480469\n",
      "Train Epoch: 263 [207360/225000 (92%)] Loss: 17441.003906\n",
      "Train Epoch: 263 [209856/225000 (93%)] Loss: 17425.478516\n",
      "Train Epoch: 263 [212352/225000 (94%)] Loss: 17815.968750\n",
      "Train Epoch: 263 [214848/225000 (95%)] Loss: 18034.867188\n",
      "Train Epoch: 263 [217344/225000 (97%)] Loss: 17679.425781\n",
      "Train Epoch: 263 [219840/225000 (98%)] Loss: 17594.855469\n",
      "Train Epoch: 263 [222336/225000 (99%)] Loss: 17223.703125\n",
      "Train Epoch: 263 [224832/225000 (100%)] Loss: 17381.503906\n",
      "    epoch          : 263\n",
      "    loss           : 17648.650668095404\n",
      "    val_loss       : 17563.613267018594\n",
      "Train Epoch: 264 [192/225000 (0%)] Loss: 18057.074219\n",
      "Train Epoch: 264 [2688/225000 (1%)] Loss: 17903.425781\n",
      "Train Epoch: 264 [5184/225000 (2%)] Loss: 17220.175781\n",
      "Train Epoch: 264 [7680/225000 (3%)] Loss: 17582.957031\n",
      "Train Epoch: 264 [10176/225000 (5%)] Loss: 17758.488281\n",
      "Train Epoch: 264 [12672/225000 (6%)] Loss: 17447.333984\n",
      "Train Epoch: 264 [15168/225000 (7%)] Loss: 18144.667969\n",
      "Train Epoch: 264 [17664/225000 (8%)] Loss: 17333.326172\n",
      "Train Epoch: 264 [20160/225000 (9%)] Loss: 17373.285156\n",
      "Train Epoch: 264 [22656/225000 (10%)] Loss: 17664.189453\n",
      "Train Epoch: 264 [25152/225000 (11%)] Loss: 16948.322266\n",
      "Train Epoch: 264 [27648/225000 (12%)] Loss: 18015.074219\n",
      "Train Epoch: 264 [30144/225000 (13%)] Loss: 17559.488281\n",
      "Train Epoch: 264 [32640/225000 (15%)] Loss: 17862.191406\n",
      "Train Epoch: 264 [35136/225000 (16%)] Loss: 17575.414062\n",
      "Train Epoch: 264 [37632/225000 (17%)] Loss: 17200.654297\n",
      "Train Epoch: 264 [40128/225000 (18%)] Loss: 17692.105469\n",
      "Train Epoch: 264 [42624/225000 (19%)] Loss: 17719.169922\n",
      "Train Epoch: 264 [45120/225000 (20%)] Loss: 17156.478516\n",
      "Train Epoch: 264 [47616/225000 (21%)] Loss: 18095.910156\n",
      "Train Epoch: 264 [50112/225000 (22%)] Loss: 17759.814453\n",
      "Train Epoch: 264 [52608/225000 (23%)] Loss: 17779.980469\n",
      "Train Epoch: 264 [55104/225000 (24%)] Loss: 17518.003906\n",
      "Train Epoch: 264 [57600/225000 (26%)] Loss: 17571.777344\n",
      "Train Epoch: 264 [60096/225000 (27%)] Loss: 17476.199219\n",
      "Train Epoch: 264 [62592/225000 (28%)] Loss: 17506.246094\n",
      "Train Epoch: 264 [65088/225000 (29%)] Loss: 17714.644531\n",
      "Train Epoch: 264 [67584/225000 (30%)] Loss: 17162.179688\n",
      "Train Epoch: 264 [70080/225000 (31%)] Loss: 17054.332031\n",
      "Train Epoch: 264 [72576/225000 (32%)] Loss: 17874.617188\n",
      "Train Epoch: 264 [75072/225000 (33%)] Loss: 17952.994141\n",
      "Train Epoch: 264 [77568/225000 (34%)] Loss: 17498.910156\n",
      "Train Epoch: 264 [80064/225000 (36%)] Loss: 17751.230469\n",
      "Train Epoch: 264 [82560/225000 (37%)] Loss: 17757.074219\n",
      "Train Epoch: 264 [85056/225000 (38%)] Loss: 17706.187500\n",
      "Train Epoch: 264 [87552/225000 (39%)] Loss: 17713.195312\n",
      "Train Epoch: 264 [90048/225000 (40%)] Loss: 17445.632812\n",
      "Train Epoch: 264 [92544/225000 (41%)] Loss: 17921.175781\n",
      "Train Epoch: 264 [95040/225000 (42%)] Loss: 17704.953125\n",
      "Train Epoch: 264 [97536/225000 (43%)] Loss: 17450.750000\n",
      "Train Epoch: 264 [100032/225000 (44%)] Loss: 17346.582031\n",
      "Train Epoch: 264 [102528/225000 (46%)] Loss: 17653.609375\n",
      "Train Epoch: 264 [105024/225000 (47%)] Loss: 17514.759766\n",
      "Train Epoch: 264 [107520/225000 (48%)] Loss: 17693.474609\n",
      "Train Epoch: 264 [110016/225000 (49%)] Loss: 17525.320312\n",
      "Train Epoch: 264 [112512/225000 (50%)] Loss: 17350.976562\n",
      "Train Epoch: 264 [115008/225000 (51%)] Loss: 17293.226562\n",
      "Train Epoch: 264 [117504/225000 (52%)] Loss: 17465.212891\n",
      "Train Epoch: 264 [120000/225000 (53%)] Loss: 17508.343750\n",
      "Train Epoch: 264 [122496/225000 (54%)] Loss: 17291.761719\n",
      "Train Epoch: 264 [124992/225000 (56%)] Loss: 17530.185547\n",
      "Train Epoch: 264 [127488/225000 (57%)] Loss: 17469.894531\n",
      "Train Epoch: 264 [129984/225000 (58%)] Loss: 17059.009766\n",
      "Train Epoch: 264 [132480/225000 (59%)] Loss: 17468.453125\n",
      "Train Epoch: 264 [134976/225000 (60%)] Loss: 19473.224609\n",
      "Train Epoch: 264 [137472/225000 (61%)] Loss: 17502.835938\n",
      "Train Epoch: 264 [139968/225000 (62%)] Loss: 17891.359375\n",
      "Train Epoch: 264 [142464/225000 (63%)] Loss: 17353.847656\n",
      "Train Epoch: 264 [144960/225000 (64%)] Loss: 17494.656250\n",
      "Train Epoch: 264 [147456/225000 (66%)] Loss: 18035.927734\n",
      "Train Epoch: 264 [149952/225000 (67%)] Loss: 17327.355469\n",
      "Train Epoch: 264 [152448/225000 (68%)] Loss: 17447.750000\n",
      "Train Epoch: 264 [154944/225000 (69%)] Loss: 17740.015625\n",
      "Train Epoch: 264 [157440/225000 (70%)] Loss: 18089.191406\n",
      "Train Epoch: 264 [159936/225000 (71%)] Loss: 17369.402344\n",
      "Train Epoch: 264 [162432/225000 (72%)] Loss: 17521.492188\n",
      "Train Epoch: 264 [164928/225000 (73%)] Loss: 17590.785156\n",
      "Train Epoch: 264 [167424/225000 (74%)] Loss: 17157.585938\n",
      "Train Epoch: 264 [169920/225000 (76%)] Loss: 17487.468750\n",
      "Train Epoch: 264 [172416/225000 (77%)] Loss: 17548.925781\n",
      "Train Epoch: 264 [174912/225000 (78%)] Loss: 17288.843750\n",
      "Train Epoch: 264 [177408/225000 (79%)] Loss: 17125.972656\n",
      "Train Epoch: 264 [179904/225000 (80%)] Loss: 17647.132812\n",
      "Train Epoch: 264 [182400/225000 (81%)] Loss: 17643.660156\n",
      "Train Epoch: 264 [184896/225000 (82%)] Loss: 17860.542969\n",
      "Train Epoch: 264 [187392/225000 (83%)] Loss: 17805.818359\n",
      "Train Epoch: 264 [189888/225000 (84%)] Loss: 18229.726562\n",
      "Train Epoch: 264 [192384/225000 (86%)] Loss: 17558.974609\n",
      "Train Epoch: 264 [194880/225000 (87%)] Loss: 17576.654297\n",
      "Train Epoch: 264 [197376/225000 (88%)] Loss: 17850.667969\n",
      "Train Epoch: 264 [199872/225000 (89%)] Loss: 17972.847656\n",
      "Train Epoch: 264 [202368/225000 (90%)] Loss: 17615.800781\n",
      "Train Epoch: 264 [204864/225000 (91%)] Loss: 17983.503906\n",
      "Train Epoch: 264 [207360/225000 (92%)] Loss: 18139.718750\n",
      "Train Epoch: 264 [209856/225000 (93%)] Loss: 17253.652344\n",
      "Train Epoch: 264 [212352/225000 (94%)] Loss: 17385.044922\n",
      "Train Epoch: 264 [214848/225000 (95%)] Loss: 17512.628906\n",
      "Train Epoch: 264 [217344/225000 (97%)] Loss: 17852.005859\n",
      "Train Epoch: 264 [219840/225000 (98%)] Loss: 17947.816406\n",
      "Train Epoch: 264 [222336/225000 (99%)] Loss: 17731.410156\n",
      "Train Epoch: 264 [224832/225000 (100%)] Loss: 17637.009766\n",
      "    epoch          : 264\n",
      "    loss           : 17654.984892444805\n",
      "    val_loss       : 17569.134719184338\n",
      "Train Epoch: 265 [192/225000 (0%)] Loss: 17801.589844\n",
      "Train Epoch: 265 [2688/225000 (1%)] Loss: 17444.417969\n",
      "Train Epoch: 265 [5184/225000 (2%)] Loss: 17775.255859\n",
      "Train Epoch: 265 [7680/225000 (3%)] Loss: 17390.894531\n",
      "Train Epoch: 265 [10176/225000 (5%)] Loss: 17481.066406\n",
      "Train Epoch: 265 [12672/225000 (6%)] Loss: 17656.238281\n",
      "Train Epoch: 265 [15168/225000 (7%)] Loss: 17620.269531\n",
      "Train Epoch: 265 [17664/225000 (8%)] Loss: 17896.640625\n",
      "Train Epoch: 265 [20160/225000 (9%)] Loss: 17135.673828\n",
      "Train Epoch: 265 [22656/225000 (10%)] Loss: 17672.578125\n",
      "Train Epoch: 265 [25152/225000 (11%)] Loss: 17549.660156\n",
      "Train Epoch: 265 [27648/225000 (12%)] Loss: 17753.445312\n",
      "Train Epoch: 265 [30144/225000 (13%)] Loss: 17765.093750\n",
      "Train Epoch: 265 [32640/225000 (15%)] Loss: 17439.214844\n",
      "Train Epoch: 265 [35136/225000 (16%)] Loss: 17765.164062\n",
      "Train Epoch: 265 [37632/225000 (17%)] Loss: 17616.660156\n",
      "Train Epoch: 265 [40128/225000 (18%)] Loss: 17839.035156\n",
      "Train Epoch: 265 [42624/225000 (19%)] Loss: 17710.257812\n",
      "Train Epoch: 265 [45120/225000 (20%)] Loss: 17933.871094\n",
      "Train Epoch: 265 [47616/225000 (21%)] Loss: 17539.400391\n",
      "Train Epoch: 265 [50112/225000 (22%)] Loss: 17903.937500\n",
      "Train Epoch: 265 [52608/225000 (23%)] Loss: 17135.593750\n",
      "Train Epoch: 265 [55104/225000 (24%)] Loss: 17292.894531\n",
      "Train Epoch: 265 [57600/225000 (26%)] Loss: 17551.808594\n",
      "Train Epoch: 265 [60096/225000 (27%)] Loss: 17103.083984\n",
      "Train Epoch: 265 [62592/225000 (28%)] Loss: 18059.046875\n",
      "Train Epoch: 265 [65088/225000 (29%)] Loss: 17582.269531\n",
      "Train Epoch: 265 [67584/225000 (30%)] Loss: 17918.003906\n",
      "Train Epoch: 265 [70080/225000 (31%)] Loss: 17775.119141\n",
      "Train Epoch: 265 [72576/225000 (32%)] Loss: 17656.156250\n",
      "Train Epoch: 265 [75072/225000 (33%)] Loss: 17503.679688\n",
      "Train Epoch: 265 [77568/225000 (34%)] Loss: 17629.304688\n",
      "Train Epoch: 265 [80064/225000 (36%)] Loss: 17883.050781\n",
      "Train Epoch: 265 [82560/225000 (37%)] Loss: 17155.269531\n",
      "Train Epoch: 265 [85056/225000 (38%)] Loss: 17248.609375\n",
      "Train Epoch: 265 [87552/225000 (39%)] Loss: 17511.609375\n",
      "Train Epoch: 265 [90048/225000 (40%)] Loss: 17512.503906\n",
      "Train Epoch: 265 [92544/225000 (41%)] Loss: 17400.250000\n",
      "Train Epoch: 265 [95040/225000 (42%)] Loss: 17526.128906\n",
      "Train Epoch: 265 [97536/225000 (43%)] Loss: 17627.980469\n",
      "Train Epoch: 265 [100032/225000 (44%)] Loss: 17671.458984\n",
      "Train Epoch: 265 [102528/225000 (46%)] Loss: 17579.550781\n",
      "Train Epoch: 265 [105024/225000 (47%)] Loss: 17201.480469\n",
      "Train Epoch: 265 [107520/225000 (48%)] Loss: 18033.330078\n",
      "Train Epoch: 265 [110016/225000 (49%)] Loss: 17670.152344\n",
      "Train Epoch: 265 [112512/225000 (50%)] Loss: 17435.673828\n",
      "Train Epoch: 265 [115008/225000 (51%)] Loss: 16895.238281\n",
      "Train Epoch: 265 [117504/225000 (52%)] Loss: 17358.615234\n",
      "Train Epoch: 265 [120000/225000 (53%)] Loss: 17360.839844\n",
      "Train Epoch: 265 [122496/225000 (54%)] Loss: 17428.921875\n",
      "Train Epoch: 265 [124992/225000 (56%)] Loss: 17406.154297\n",
      "Train Epoch: 265 [127488/225000 (57%)] Loss: 17598.507812\n",
      "Train Epoch: 265 [129984/225000 (58%)] Loss: 17193.574219\n",
      "Train Epoch: 265 [132480/225000 (59%)] Loss: 17508.271484\n",
      "Train Epoch: 265 [134976/225000 (60%)] Loss: 17677.945312\n",
      "Train Epoch: 265 [137472/225000 (61%)] Loss: 17375.509766\n",
      "Train Epoch: 265 [139968/225000 (62%)] Loss: 17532.589844\n",
      "Train Epoch: 265 [142464/225000 (63%)] Loss: 17318.367188\n",
      "Train Epoch: 265 [144960/225000 (64%)] Loss: 17515.496094\n",
      "Train Epoch: 265 [147456/225000 (66%)] Loss: 17562.255859\n",
      "Train Epoch: 265 [149952/225000 (67%)] Loss: 18154.199219\n",
      "Train Epoch: 265 [152448/225000 (68%)] Loss: 17548.812500\n",
      "Train Epoch: 265 [154944/225000 (69%)] Loss: 17527.746094\n",
      "Train Epoch: 265 [157440/225000 (70%)] Loss: 18033.875000\n",
      "Train Epoch: 265 [159936/225000 (71%)] Loss: 17125.585938\n",
      "Train Epoch: 265 [162432/225000 (72%)] Loss: 17750.058594\n",
      "Train Epoch: 265 [164928/225000 (73%)] Loss: 17658.148438\n",
      "Train Epoch: 265 [167424/225000 (74%)] Loss: 17543.730469\n",
      "Train Epoch: 265 [169920/225000 (76%)] Loss: 17151.064453\n",
      "Train Epoch: 265 [172416/225000 (77%)] Loss: 17681.099609\n",
      "Train Epoch: 265 [174912/225000 (78%)] Loss: 17552.794922\n",
      "Train Epoch: 265 [177408/225000 (79%)] Loss: 17842.406250\n",
      "Train Epoch: 265 [179904/225000 (80%)] Loss: 18042.328125\n",
      "Train Epoch: 265 [182400/225000 (81%)] Loss: 17622.496094\n",
      "Train Epoch: 265 [184896/225000 (82%)] Loss: 17264.164062\n",
      "Train Epoch: 265 [187392/225000 (83%)] Loss: 17437.785156\n",
      "Train Epoch: 265 [189888/225000 (84%)] Loss: 17446.427734\n",
      "Train Epoch: 265 [192384/225000 (86%)] Loss: 17316.906250\n",
      "Train Epoch: 265 [194880/225000 (87%)] Loss: 17850.710938\n",
      "Train Epoch: 265 [197376/225000 (88%)] Loss: 17862.050781\n",
      "Train Epoch: 265 [199872/225000 (89%)] Loss: 17670.964844\n",
      "Train Epoch: 265 [202368/225000 (90%)] Loss: 17617.613281\n",
      "Train Epoch: 265 [204864/225000 (91%)] Loss: 17271.582031\n",
      "Train Epoch: 265 [207360/225000 (92%)] Loss: 18069.089844\n",
      "Train Epoch: 265 [209856/225000 (93%)] Loss: 17114.726562\n",
      "Train Epoch: 265 [212352/225000 (94%)] Loss: 17236.937500\n",
      "Train Epoch: 265 [214848/225000 (95%)] Loss: 17581.869141\n",
      "Train Epoch: 265 [217344/225000 (97%)] Loss: 17822.208984\n",
      "Train Epoch: 265 [219840/225000 (98%)] Loss: 17642.593750\n",
      "Train Epoch: 265 [222336/225000 (99%)] Loss: 17993.453125\n",
      "Train Epoch: 265 [224832/225000 (100%)] Loss: 17030.839844\n",
      "    epoch          : 265\n",
      "    loss           : 17628.529078564952\n",
      "    val_loss       : 17551.54675878732\n",
      "Train Epoch: 266 [192/225000 (0%)] Loss: 17455.011719\n",
      "Train Epoch: 266 [2688/225000 (1%)] Loss: 18045.574219\n",
      "Train Epoch: 266 [5184/225000 (2%)] Loss: 17773.076172\n",
      "Train Epoch: 266 [7680/225000 (3%)] Loss: 18018.396484\n",
      "Train Epoch: 266 [10176/225000 (5%)] Loss: 17721.240234\n",
      "Train Epoch: 266 [12672/225000 (6%)] Loss: 17793.873047\n",
      "Train Epoch: 266 [15168/225000 (7%)] Loss: 17016.125000\n",
      "Train Epoch: 266 [17664/225000 (8%)] Loss: 17945.007812\n",
      "Train Epoch: 266 [20160/225000 (9%)] Loss: 17554.031250\n",
      "Train Epoch: 266 [22656/225000 (10%)] Loss: 17296.957031\n",
      "Train Epoch: 266 [25152/225000 (11%)] Loss: 17444.800781\n",
      "Train Epoch: 266 [27648/225000 (12%)] Loss: 17344.150391\n",
      "Train Epoch: 266 [30144/225000 (13%)] Loss: 17299.363281\n",
      "Train Epoch: 266 [32640/225000 (15%)] Loss: 18322.535156\n",
      "Train Epoch: 266 [35136/225000 (16%)] Loss: 17610.796875\n",
      "Train Epoch: 266 [37632/225000 (17%)] Loss: 17573.591797\n",
      "Train Epoch: 266 [40128/225000 (18%)] Loss: 17943.359375\n",
      "Train Epoch: 266 [42624/225000 (19%)] Loss: 17707.269531\n",
      "Train Epoch: 266 [45120/225000 (20%)] Loss: 17138.058594\n",
      "Train Epoch: 266 [47616/225000 (21%)] Loss: 17982.914062\n",
      "Train Epoch: 266 [50112/225000 (22%)] Loss: 17903.632812\n",
      "Train Epoch: 266 [52608/225000 (23%)] Loss: 17758.546875\n",
      "Train Epoch: 266 [55104/225000 (24%)] Loss: 17880.464844\n",
      "Train Epoch: 266 [57600/225000 (26%)] Loss: 17965.947266\n",
      "Train Epoch: 266 [60096/225000 (27%)] Loss: 17883.898438\n",
      "Train Epoch: 266 [62592/225000 (28%)] Loss: 17574.583984\n",
      "Train Epoch: 266 [65088/225000 (29%)] Loss: 17565.576172\n",
      "Train Epoch: 266 [67584/225000 (30%)] Loss: 17943.648438\n",
      "Train Epoch: 266 [70080/225000 (31%)] Loss: 17303.832031\n",
      "Train Epoch: 266 [72576/225000 (32%)] Loss: 17590.503906\n",
      "Train Epoch: 266 [75072/225000 (33%)] Loss: 17505.218750\n",
      "Train Epoch: 266 [77568/225000 (34%)] Loss: 17772.466797\n",
      "Train Epoch: 266 [80064/225000 (36%)] Loss: 17431.105469\n",
      "Train Epoch: 266 [82560/225000 (37%)] Loss: 17316.117188\n",
      "Train Epoch: 266 [85056/225000 (38%)] Loss: 17195.085938\n",
      "Train Epoch: 266 [87552/225000 (39%)] Loss: 17294.695312\n",
      "Train Epoch: 266 [90048/225000 (40%)] Loss: 17535.017578\n",
      "Train Epoch: 266 [92544/225000 (41%)] Loss: 17974.218750\n",
      "Train Epoch: 266 [95040/225000 (42%)] Loss: 17313.871094\n",
      "Train Epoch: 266 [97536/225000 (43%)] Loss: 17888.328125\n",
      "Train Epoch: 266 [100032/225000 (44%)] Loss: 17369.775391\n",
      "Train Epoch: 266 [102528/225000 (46%)] Loss: 17769.462891\n",
      "Train Epoch: 266 [105024/225000 (47%)] Loss: 17264.302734\n",
      "Train Epoch: 266 [107520/225000 (48%)] Loss: 17364.488281\n",
      "Train Epoch: 266 [110016/225000 (49%)] Loss: 17420.996094\n",
      "Train Epoch: 266 [112512/225000 (50%)] Loss: 17621.576172\n",
      "Train Epoch: 266 [115008/225000 (51%)] Loss: 17500.712891\n",
      "Train Epoch: 266 [117504/225000 (52%)] Loss: 17316.164062\n",
      "Train Epoch: 266 [120000/225000 (53%)] Loss: 17823.199219\n",
      "Train Epoch: 266 [122496/225000 (54%)] Loss: 17375.582031\n",
      "Train Epoch: 266 [124992/225000 (56%)] Loss: 17507.054688\n",
      "Train Epoch: 266 [127488/225000 (57%)] Loss: 17765.095703\n",
      "Train Epoch: 266 [129984/225000 (58%)] Loss: 17819.105469\n",
      "Train Epoch: 266 [132480/225000 (59%)] Loss: 17462.435547\n",
      "Train Epoch: 266 [134976/225000 (60%)] Loss: 17651.253906\n",
      "Train Epoch: 266 [137472/225000 (61%)] Loss: 17859.925781\n",
      "Train Epoch: 266 [139968/225000 (62%)] Loss: 17139.757812\n",
      "Train Epoch: 266 [142464/225000 (63%)] Loss: 17554.949219\n",
      "Train Epoch: 266 [144960/225000 (64%)] Loss: 17904.691406\n",
      "Train Epoch: 266 [147456/225000 (66%)] Loss: 17501.359375\n",
      "Train Epoch: 266 [149952/225000 (67%)] Loss: 17727.355469\n",
      "Train Epoch: 266 [152448/225000 (68%)] Loss: 17523.720703\n",
      "Train Epoch: 266 [154944/225000 (69%)] Loss: 17814.751953\n",
      "Train Epoch: 266 [157440/225000 (70%)] Loss: 17621.777344\n",
      "Train Epoch: 266 [159936/225000 (71%)] Loss: 17703.730469\n",
      "Train Epoch: 266 [162432/225000 (72%)] Loss: 17618.632812\n",
      "Train Epoch: 266 [164928/225000 (73%)] Loss: 17406.359375\n",
      "Train Epoch: 266 [167424/225000 (74%)] Loss: 18093.974609\n",
      "Train Epoch: 266 [169920/225000 (76%)] Loss: 17549.519531\n",
      "Train Epoch: 266 [172416/225000 (77%)] Loss: 17891.945312\n",
      "Train Epoch: 266 [174912/225000 (78%)] Loss: 17747.449219\n",
      "Train Epoch: 266 [177408/225000 (79%)] Loss: 17616.753906\n",
      "Train Epoch: 266 [179904/225000 (80%)] Loss: 18025.628906\n",
      "Train Epoch: 266 [182400/225000 (81%)] Loss: 17098.277344\n",
      "Train Epoch: 266 [184896/225000 (82%)] Loss: 17341.943359\n",
      "Train Epoch: 266 [187392/225000 (83%)] Loss: 17458.199219\n",
      "Train Epoch: 266 [189888/225000 (84%)] Loss: 17589.542969\n",
      "Train Epoch: 266 [192384/225000 (86%)] Loss: 17284.841797\n",
      "Train Epoch: 266 [194880/225000 (87%)] Loss: 17884.164062\n",
      "Train Epoch: 266 [197376/225000 (88%)] Loss: 17460.333984\n",
      "Train Epoch: 266 [199872/225000 (89%)] Loss: 17262.406250\n",
      "Train Epoch: 266 [202368/225000 (90%)] Loss: 17708.031250\n",
      "Train Epoch: 266 [204864/225000 (91%)] Loss: 17660.859375\n",
      "Train Epoch: 266 [207360/225000 (92%)] Loss: 17499.695312\n",
      "Train Epoch: 266 [209856/225000 (93%)] Loss: 17344.312500\n",
      "Train Epoch: 266 [212352/225000 (94%)] Loss: 17982.507812\n",
      "Train Epoch: 266 [214848/225000 (95%)] Loss: 17646.835938\n",
      "Train Epoch: 266 [217344/225000 (97%)] Loss: 17689.435547\n",
      "Train Epoch: 266 [219840/225000 (98%)] Loss: 17695.511719\n",
      "Train Epoch: 266 [222336/225000 (99%)] Loss: 16969.199219\n",
      "Train Epoch: 266 [224832/225000 (100%)] Loss: 17744.261719\n",
      "    epoch          : 266\n",
      "    loss           : 17620.271511038824\n",
      "    val_loss       : 17527.486315753624\n",
      "Train Epoch: 267 [192/225000 (0%)] Loss: 17466.218750\n",
      "Train Epoch: 267 [2688/225000 (1%)] Loss: 17443.484375\n",
      "Train Epoch: 267 [5184/225000 (2%)] Loss: 17991.968750\n",
      "Train Epoch: 267 [7680/225000 (3%)] Loss: 18059.126953\n",
      "Train Epoch: 267 [10176/225000 (5%)] Loss: 17456.695312\n",
      "Train Epoch: 267 [12672/225000 (6%)] Loss: 17927.074219\n",
      "Train Epoch: 267 [15168/225000 (7%)] Loss: 17726.986328\n",
      "Train Epoch: 267 [17664/225000 (8%)] Loss: 17900.976562\n",
      "Train Epoch: 267 [20160/225000 (9%)] Loss: 17547.816406\n",
      "Train Epoch: 267 [22656/225000 (10%)] Loss: 17695.531250\n",
      "Train Epoch: 267 [25152/225000 (11%)] Loss: 17706.097656\n",
      "Train Epoch: 267 [27648/225000 (12%)] Loss: 17638.410156\n",
      "Train Epoch: 267 [30144/225000 (13%)] Loss: 17386.755859\n",
      "Train Epoch: 267 [32640/225000 (15%)] Loss: 17587.599609\n",
      "Train Epoch: 267 [35136/225000 (16%)] Loss: 18024.339844\n",
      "Train Epoch: 267 [37632/225000 (17%)] Loss: 17890.351562\n",
      "Train Epoch: 267 [40128/225000 (18%)] Loss: 17590.527344\n",
      "Train Epoch: 267 [42624/225000 (19%)] Loss: 17527.199219\n",
      "Train Epoch: 267 [45120/225000 (20%)] Loss: 17210.085938\n",
      "Train Epoch: 267 [47616/225000 (21%)] Loss: 17682.519531\n",
      "Train Epoch: 267 [50112/225000 (22%)] Loss: 17061.042969\n",
      "Train Epoch: 267 [52608/225000 (23%)] Loss: 17465.150391\n",
      "Train Epoch: 267 [55104/225000 (24%)] Loss: 18305.865234\n",
      "Train Epoch: 267 [57600/225000 (26%)] Loss: 17660.046875\n",
      "Train Epoch: 267 [60096/225000 (27%)] Loss: 17615.890625\n",
      "Train Epoch: 267 [62592/225000 (28%)] Loss: 17858.037109\n",
      "Train Epoch: 267 [65088/225000 (29%)] Loss: 17287.154297\n",
      "Train Epoch: 267 [67584/225000 (30%)] Loss: 17624.292969\n",
      "Train Epoch: 267 [70080/225000 (31%)] Loss: 17771.914062\n",
      "Train Epoch: 267 [72576/225000 (32%)] Loss: 17360.207031\n",
      "Train Epoch: 267 [75072/225000 (33%)] Loss: 17841.898438\n",
      "Train Epoch: 267 [77568/225000 (34%)] Loss: 17377.421875\n",
      "Train Epoch: 267 [80064/225000 (36%)] Loss: 17880.789062\n",
      "Train Epoch: 267 [82560/225000 (37%)] Loss: 17226.361328\n",
      "Train Epoch: 267 [85056/225000 (38%)] Loss: 17743.394531\n",
      "Train Epoch: 267 [87552/225000 (39%)] Loss: 17988.414062\n",
      "Train Epoch: 267 [90048/225000 (40%)] Loss: 17262.273438\n",
      "Train Epoch: 267 [92544/225000 (41%)] Loss: 17775.720703\n",
      "Train Epoch: 267 [95040/225000 (42%)] Loss: 17817.818359\n",
      "Train Epoch: 267 [97536/225000 (43%)] Loss: 17654.351562\n",
      "Train Epoch: 267 [100032/225000 (44%)] Loss: 17236.378906\n",
      "Train Epoch: 267 [102528/225000 (46%)] Loss: 17438.441406\n",
      "Train Epoch: 267 [105024/225000 (47%)] Loss: 17417.849609\n",
      "Train Epoch: 267 [107520/225000 (48%)] Loss: 17457.785156\n",
      "Train Epoch: 267 [110016/225000 (49%)] Loss: 17814.574219\n",
      "Train Epoch: 267 [112512/225000 (50%)] Loss: 17969.449219\n",
      "Train Epoch: 267 [115008/225000 (51%)] Loss: 17604.625000\n",
      "Train Epoch: 267 [117504/225000 (52%)] Loss: 17576.005859\n",
      "Train Epoch: 267 [120000/225000 (53%)] Loss: 17710.097656\n",
      "Train Epoch: 267 [122496/225000 (54%)] Loss: 17522.449219\n",
      "Train Epoch: 267 [124992/225000 (56%)] Loss: 17383.898438\n",
      "Train Epoch: 267 [127488/225000 (57%)] Loss: 18438.320312\n",
      "Train Epoch: 267 [129984/225000 (58%)] Loss: 17665.490234\n",
      "Train Epoch: 267 [132480/225000 (59%)] Loss: 17546.609375\n",
      "Train Epoch: 267 [134976/225000 (60%)] Loss: 17851.589844\n",
      "Train Epoch: 267 [137472/225000 (61%)] Loss: 18194.500000\n",
      "Train Epoch: 267 [139968/225000 (62%)] Loss: 17995.136719\n",
      "Train Epoch: 267 [142464/225000 (63%)] Loss: 17715.792969\n",
      "Train Epoch: 267 [144960/225000 (64%)] Loss: 17904.199219\n",
      "Train Epoch: 267 [147456/225000 (66%)] Loss: 17484.906250\n",
      "Train Epoch: 267 [149952/225000 (67%)] Loss: 17808.671875\n",
      "Train Epoch: 267 [152448/225000 (68%)] Loss: 17659.625000\n",
      "Train Epoch: 267 [154944/225000 (69%)] Loss: 18215.968750\n",
      "Train Epoch: 267 [157440/225000 (70%)] Loss: 18044.414062\n",
      "Train Epoch: 267 [159936/225000 (71%)] Loss: 17882.035156\n",
      "Train Epoch: 267 [162432/225000 (72%)] Loss: 17578.726562\n",
      "Train Epoch: 267 [164928/225000 (73%)] Loss: 17509.642578\n",
      "Train Epoch: 267 [167424/225000 (74%)] Loss: 17983.037109\n",
      "Train Epoch: 267 [169920/225000 (76%)] Loss: 17872.265625\n",
      "Train Epoch: 267 [172416/225000 (77%)] Loss: 17780.460938\n",
      "Train Epoch: 267 [174912/225000 (78%)] Loss: 17649.435547\n",
      "Train Epoch: 267 [177408/225000 (79%)] Loss: 17459.441406\n",
      "Train Epoch: 267 [179904/225000 (80%)] Loss: 17632.466797\n",
      "Train Epoch: 267 [182400/225000 (81%)] Loss: 17590.890625\n",
      "Train Epoch: 267 [184896/225000 (82%)] Loss: 17169.074219\n",
      "Train Epoch: 267 [187392/225000 (83%)] Loss: 17808.687500\n",
      "Train Epoch: 267 [189888/225000 (84%)] Loss: 17241.451172\n",
      "Train Epoch: 267 [192384/225000 (86%)] Loss: 17661.285156\n",
      "Train Epoch: 267 [194880/225000 (87%)] Loss: 17489.355469\n",
      "Train Epoch: 267 [197376/225000 (88%)] Loss: 17474.156250\n",
      "Train Epoch: 267 [199872/225000 (89%)] Loss: 17938.011719\n",
      "Train Epoch: 267 [202368/225000 (90%)] Loss: 17384.857422\n",
      "Train Epoch: 267 [204864/225000 (91%)] Loss: 17246.300781\n",
      "Train Epoch: 267 [207360/225000 (92%)] Loss: 17571.441406\n",
      "Train Epoch: 267 [209856/225000 (93%)] Loss: 18062.558594\n",
      "Train Epoch: 267 [212352/225000 (94%)] Loss: 17475.521484\n",
      "Train Epoch: 267 [214848/225000 (95%)] Loss: 17384.140625\n",
      "Train Epoch: 267 [217344/225000 (97%)] Loss: 18601.302734\n",
      "Train Epoch: 267 [219840/225000 (98%)] Loss: 18182.435547\n",
      "Train Epoch: 267 [222336/225000 (99%)] Loss: 18302.308594\n",
      "Train Epoch: 267 [224832/225000 (100%)] Loss: 17531.792969\n",
      "    epoch          : 267\n",
      "    loss           : 17611.602708211125\n",
      "    val_loss       : 17584.75943317304\n",
      "Train Epoch: 268 [192/225000 (0%)] Loss: 17622.746094\n",
      "Train Epoch: 268 [2688/225000 (1%)] Loss: 17795.621094\n",
      "Train Epoch: 268 [5184/225000 (2%)] Loss: 18082.761719\n",
      "Train Epoch: 268 [7680/225000 (3%)] Loss: 17501.640625\n",
      "Train Epoch: 268 [10176/225000 (5%)] Loss: 17572.808594\n",
      "Train Epoch: 268 [12672/225000 (6%)] Loss: 18440.347656\n",
      "Train Epoch: 268 [15168/225000 (7%)] Loss: 17762.937500\n",
      "Train Epoch: 268 [17664/225000 (8%)] Loss: 17471.001953\n",
      "Train Epoch: 268 [20160/225000 (9%)] Loss: 17457.804688\n",
      "Train Epoch: 268 [22656/225000 (10%)] Loss: 17332.275391\n",
      "Train Epoch: 268 [25152/225000 (11%)] Loss: 17685.406250\n",
      "Train Epoch: 268 [27648/225000 (12%)] Loss: 17744.187500\n",
      "Train Epoch: 268 [30144/225000 (13%)] Loss: 17545.128906\n",
      "Train Epoch: 268 [32640/225000 (15%)] Loss: 17698.898438\n",
      "Train Epoch: 268 [35136/225000 (16%)] Loss: 17396.699219\n",
      "Train Epoch: 268 [37632/225000 (17%)] Loss: 17254.611328\n",
      "Train Epoch: 268 [40128/225000 (18%)] Loss: 17798.304688\n",
      "Train Epoch: 268 [42624/225000 (19%)] Loss: 17653.921875\n",
      "Train Epoch: 268 [45120/225000 (20%)] Loss: 17233.693359\n",
      "Train Epoch: 268 [47616/225000 (21%)] Loss: 17360.375000\n",
      "Train Epoch: 268 [50112/225000 (22%)] Loss: 17327.750000\n",
      "Train Epoch: 268 [52608/225000 (23%)] Loss: 17597.800781\n",
      "Train Epoch: 268 [55104/225000 (24%)] Loss: 17782.339844\n",
      "Train Epoch: 268 [57600/225000 (26%)] Loss: 17540.818359\n",
      "Train Epoch: 268 [60096/225000 (27%)] Loss: 17883.826172\n",
      "Train Epoch: 268 [62592/225000 (28%)] Loss: 17786.453125\n",
      "Train Epoch: 268 [65088/225000 (29%)] Loss: 17229.896484\n",
      "Train Epoch: 268 [67584/225000 (30%)] Loss: 17774.589844\n",
      "Train Epoch: 268 [70080/225000 (31%)] Loss: 17996.080078\n",
      "Train Epoch: 268 [72576/225000 (32%)] Loss: 17725.261719\n",
      "Train Epoch: 268 [75072/225000 (33%)] Loss: 17462.019531\n",
      "Train Epoch: 268 [77568/225000 (34%)] Loss: 18082.226562\n",
      "Train Epoch: 268 [80064/225000 (36%)] Loss: 17564.378906\n",
      "Train Epoch: 268 [82560/225000 (37%)] Loss: 17650.289062\n",
      "Train Epoch: 268 [85056/225000 (38%)] Loss: 17523.542969\n",
      "Train Epoch: 268 [87552/225000 (39%)] Loss: 17217.103516\n",
      "Train Epoch: 268 [90048/225000 (40%)] Loss: 17562.515625\n",
      "Train Epoch: 268 [92544/225000 (41%)] Loss: 17326.609375\n",
      "Train Epoch: 268 [95040/225000 (42%)] Loss: 17292.199219\n",
      "Train Epoch: 268 [97536/225000 (43%)] Loss: 17833.570312\n",
      "Train Epoch: 268 [100032/225000 (44%)] Loss: 17970.335938\n",
      "Train Epoch: 268 [102528/225000 (46%)] Loss: 17165.359375\n",
      "Train Epoch: 268 [105024/225000 (47%)] Loss: 17527.738281\n",
      "Train Epoch: 268 [107520/225000 (48%)] Loss: 17714.888672\n",
      "Train Epoch: 268 [110016/225000 (49%)] Loss: 17847.400391\n",
      "Train Epoch: 268 [112512/225000 (50%)] Loss: 17493.468750\n",
      "Train Epoch: 268 [115008/225000 (51%)] Loss: 17415.636719\n",
      "Train Epoch: 268 [117504/225000 (52%)] Loss: 17600.529297\n",
      "Train Epoch: 268 [120000/225000 (53%)] Loss: 17748.095703\n",
      "Train Epoch: 268 [122496/225000 (54%)] Loss: 17817.781250\n",
      "Train Epoch: 268 [124992/225000 (56%)] Loss: 17531.000000\n",
      "Train Epoch: 268 [127488/225000 (57%)] Loss: 17840.054688\n",
      "Train Epoch: 268 [129984/225000 (58%)] Loss: 17691.089844\n",
      "Train Epoch: 268 [132480/225000 (59%)] Loss: 17594.351562\n",
      "Train Epoch: 268 [134976/225000 (60%)] Loss: 18042.300781\n",
      "Train Epoch: 268 [137472/225000 (61%)] Loss: 17587.816406\n",
      "Train Epoch: 268 [139968/225000 (62%)] Loss: 17623.726562\n",
      "Train Epoch: 268 [142464/225000 (63%)] Loss: 17575.625000\n",
      "Train Epoch: 268 [144960/225000 (64%)] Loss: 17392.230469\n",
      "Train Epoch: 268 [147456/225000 (66%)] Loss: 17432.753906\n",
      "Train Epoch: 268 [149952/225000 (67%)] Loss: 17392.392578\n",
      "Train Epoch: 268 [152448/225000 (68%)] Loss: 17384.464844\n",
      "Train Epoch: 268 [154944/225000 (69%)] Loss: 17874.230469\n",
      "Train Epoch: 268 [157440/225000 (70%)] Loss: 17586.093750\n",
      "Train Epoch: 268 [159936/225000 (71%)] Loss: 17757.796875\n",
      "Train Epoch: 268 [162432/225000 (72%)] Loss: 17852.365234\n",
      "Train Epoch: 268 [164928/225000 (73%)] Loss: 17704.140625\n",
      "Train Epoch: 268 [167424/225000 (74%)] Loss: 17519.050781\n",
      "Train Epoch: 268 [169920/225000 (76%)] Loss: 17719.414062\n",
      "Train Epoch: 268 [172416/225000 (77%)] Loss: 17757.863281\n",
      "Train Epoch: 268 [174912/225000 (78%)] Loss: 17557.974609\n",
      "Train Epoch: 268 [177408/225000 (79%)] Loss: 17539.212891\n",
      "Train Epoch: 268 [179904/225000 (80%)] Loss: 17764.488281\n",
      "Train Epoch: 268 [182400/225000 (81%)] Loss: 17245.003906\n",
      "Train Epoch: 268 [184896/225000 (82%)] Loss: 17657.699219\n",
      "Train Epoch: 268 [187392/225000 (83%)] Loss: 17748.666016\n",
      "Train Epoch: 268 [189888/225000 (84%)] Loss: 17726.488281\n",
      "Train Epoch: 268 [192384/225000 (86%)] Loss: 17707.335938\n",
      "Train Epoch: 268 [194880/225000 (87%)] Loss: 17616.261719\n",
      "Train Epoch: 268 [197376/225000 (88%)] Loss: 17479.187500\n",
      "Train Epoch: 268 [199872/225000 (89%)] Loss: 17834.585938\n",
      "Train Epoch: 268 [202368/225000 (90%)] Loss: 17375.464844\n",
      "Train Epoch: 268 [204864/225000 (91%)] Loss: 17436.386719\n",
      "Train Epoch: 268 [207360/225000 (92%)] Loss: 18006.574219\n",
      "Train Epoch: 268 [209856/225000 (93%)] Loss: 17628.822266\n",
      "Train Epoch: 268 [212352/225000 (94%)] Loss: 17198.613281\n",
      "Train Epoch: 268 [214848/225000 (95%)] Loss: 18093.714844\n",
      "Train Epoch: 268 [217344/225000 (97%)] Loss: 17501.093750\n",
      "Train Epoch: 268 [219840/225000 (98%)] Loss: 18103.226562\n",
      "Train Epoch: 268 [222336/225000 (99%)] Loss: 17381.835938\n",
      "Train Epoch: 268 [224832/225000 (100%)] Loss: 17865.316406\n",
      "    epoch          : 268\n",
      "    loss           : 17611.59388081938\n",
      "    val_loss       : 17627.962535475046\n",
      "Train Epoch: 269 [192/225000 (0%)] Loss: 20072.226562\n",
      "Train Epoch: 269 [2688/225000 (1%)] Loss: 17943.187500\n",
      "Train Epoch: 269 [5184/225000 (2%)] Loss: 17922.902344\n",
      "Train Epoch: 269 [7680/225000 (3%)] Loss: 17238.140625\n",
      "Train Epoch: 269 [10176/225000 (5%)] Loss: 17553.820312\n",
      "Train Epoch: 269 [12672/225000 (6%)] Loss: 17691.554688\n",
      "Train Epoch: 269 [15168/225000 (7%)] Loss: 17658.089844\n",
      "Train Epoch: 269 [17664/225000 (8%)] Loss: 17633.753906\n",
      "Train Epoch: 269 [20160/225000 (9%)] Loss: 17889.666016\n",
      "Train Epoch: 269 [22656/225000 (10%)] Loss: 17603.810547\n",
      "Train Epoch: 269 [25152/225000 (11%)] Loss: 17989.304688\n",
      "Train Epoch: 269 [27648/225000 (12%)] Loss: 17484.183594\n",
      "Train Epoch: 269 [30144/225000 (13%)] Loss: 17648.207031\n",
      "Train Epoch: 269 [32640/225000 (15%)] Loss: 17585.669922\n",
      "Train Epoch: 269 [35136/225000 (16%)] Loss: 17337.417969\n",
      "Train Epoch: 269 [37632/225000 (17%)] Loss: 17606.882812\n",
      "Train Epoch: 269 [40128/225000 (18%)] Loss: 17645.671875\n",
      "Train Epoch: 269 [42624/225000 (19%)] Loss: 17666.796875\n",
      "Train Epoch: 269 [45120/225000 (20%)] Loss: 17899.519531\n",
      "Train Epoch: 269 [47616/225000 (21%)] Loss: 18089.988281\n",
      "Train Epoch: 269 [50112/225000 (22%)] Loss: 17231.093750\n",
      "Train Epoch: 269 [52608/225000 (23%)] Loss: 17707.929688\n",
      "Train Epoch: 269 [55104/225000 (24%)] Loss: 17594.960938\n",
      "Train Epoch: 269 [57600/225000 (26%)] Loss: 17666.437500\n",
      "Train Epoch: 269 [60096/225000 (27%)] Loss: 17272.457031\n",
      "Train Epoch: 269 [62592/225000 (28%)] Loss: 17595.593750\n",
      "Train Epoch: 269 [65088/225000 (29%)] Loss: 17519.292969\n",
      "Train Epoch: 269 [67584/225000 (30%)] Loss: 18079.751953\n",
      "Train Epoch: 269 [70080/225000 (31%)] Loss: 17485.927734\n",
      "Train Epoch: 269 [72576/225000 (32%)] Loss: 17950.058594\n",
      "Train Epoch: 269 [75072/225000 (33%)] Loss: 17654.542969\n",
      "Train Epoch: 269 [77568/225000 (34%)] Loss: 17848.685547\n",
      "Train Epoch: 269 [80064/225000 (36%)] Loss: 17627.328125\n",
      "Train Epoch: 269 [82560/225000 (37%)] Loss: 18110.457031\n",
      "Train Epoch: 269 [85056/225000 (38%)] Loss: 16891.228516\n",
      "Train Epoch: 269 [87552/225000 (39%)] Loss: 17814.949219\n",
      "Train Epoch: 269 [90048/225000 (40%)] Loss: 17779.878906\n",
      "Train Epoch: 269 [92544/225000 (41%)] Loss: 18419.730469\n",
      "Train Epoch: 269 [95040/225000 (42%)] Loss: 17165.023438\n",
      "Train Epoch: 269 [97536/225000 (43%)] Loss: 17832.908203\n",
      "Train Epoch: 269 [100032/225000 (44%)] Loss: 17345.029297\n",
      "Train Epoch: 269 [102528/225000 (46%)] Loss: 17735.634766\n",
      "Train Epoch: 269 [105024/225000 (47%)] Loss: 17165.417969\n",
      "Train Epoch: 269 [107520/225000 (48%)] Loss: 17598.097656\n",
      "Train Epoch: 269 [110016/225000 (49%)] Loss: 17419.375000\n",
      "Train Epoch: 269 [112512/225000 (50%)] Loss: 17931.035156\n",
      "Train Epoch: 269 [115008/225000 (51%)] Loss: 17708.257812\n",
      "Train Epoch: 269 [117504/225000 (52%)] Loss: 17381.695312\n",
      "Train Epoch: 269 [120000/225000 (53%)] Loss: 17868.984375\n",
      "Train Epoch: 269 [122496/225000 (54%)] Loss: 17617.839844\n",
      "Train Epoch: 269 [124992/225000 (56%)] Loss: 17625.875000\n",
      "Train Epoch: 269 [127488/225000 (57%)] Loss: 17979.902344\n",
      "Train Epoch: 269 [129984/225000 (58%)] Loss: 17194.773438\n",
      "Train Epoch: 269 [132480/225000 (59%)] Loss: 18191.929688\n",
      "Train Epoch: 269 [134976/225000 (60%)] Loss: 17404.164062\n",
      "Train Epoch: 269 [137472/225000 (61%)] Loss: 18093.394531\n",
      "Train Epoch: 269 [139968/225000 (62%)] Loss: 17841.410156\n",
      "Train Epoch: 269 [142464/225000 (63%)] Loss: 17630.699219\n",
      "Train Epoch: 269 [144960/225000 (64%)] Loss: 18177.464844\n",
      "Train Epoch: 269 [147456/225000 (66%)] Loss: 17817.578125\n",
      "Train Epoch: 269 [149952/225000 (67%)] Loss: 17142.896484\n",
      "Train Epoch: 269 [152448/225000 (68%)] Loss: 17921.375000\n",
      "Train Epoch: 269 [154944/225000 (69%)] Loss: 17873.796875\n",
      "Train Epoch: 269 [157440/225000 (70%)] Loss: 17357.337891\n",
      "Train Epoch: 269 [159936/225000 (71%)] Loss: 17839.121094\n",
      "Train Epoch: 269 [162432/225000 (72%)] Loss: 16853.109375\n",
      "Train Epoch: 269 [164928/225000 (73%)] Loss: 17280.296875\n",
      "Train Epoch: 269 [167424/225000 (74%)] Loss: 17378.261719\n",
      "Train Epoch: 269 [169920/225000 (76%)] Loss: 17797.949219\n",
      "Train Epoch: 269 [172416/225000 (77%)] Loss: 17177.933594\n",
      "Train Epoch: 269 [174912/225000 (78%)] Loss: 18035.425781\n",
      "Train Epoch: 269 [177408/225000 (79%)] Loss: 17791.224609\n",
      "Train Epoch: 269 [179904/225000 (80%)] Loss: 17150.433594\n",
      "Train Epoch: 269 [182400/225000 (81%)] Loss: 17778.324219\n",
      "Train Epoch: 269 [184896/225000 (82%)] Loss: 17890.402344\n",
      "Train Epoch: 269 [187392/225000 (83%)] Loss: 17732.085938\n",
      "Train Epoch: 269 [189888/225000 (84%)] Loss: 17588.750000\n",
      "Train Epoch: 269 [192384/225000 (86%)] Loss: 18236.125000\n",
      "Train Epoch: 269 [194880/225000 (87%)] Loss: 17351.648438\n",
      "Train Epoch: 269 [197376/225000 (88%)] Loss: 17707.199219\n",
      "Train Epoch: 269 [199872/225000 (89%)] Loss: 17653.000000\n",
      "Train Epoch: 269 [202368/225000 (90%)] Loss: 17600.402344\n",
      "Train Epoch: 269 [204864/225000 (91%)] Loss: 17888.853516\n",
      "Train Epoch: 269 [207360/225000 (92%)] Loss: 17367.796875\n",
      "Train Epoch: 269 [209856/225000 (93%)] Loss: 18316.523438\n",
      "Train Epoch: 269 [212352/225000 (94%)] Loss: 17776.089844\n",
      "Train Epoch: 269 [214848/225000 (95%)] Loss: 17539.343750\n",
      "Train Epoch: 269 [217344/225000 (97%)] Loss: 17627.050781\n",
      "Train Epoch: 269 [219840/225000 (98%)] Loss: 17499.277344\n",
      "Train Epoch: 269 [222336/225000 (99%)] Loss: 17852.078125\n",
      "Train Epoch: 269 [224832/225000 (100%)] Loss: 17731.886719\n",
      "    epoch          : 269\n",
      "    loss           : 17614.94462757306\n",
      "    val_loss       : 17524.473714164196\n",
      "Train Epoch: 270 [192/225000 (0%)] Loss: 17996.982422\n",
      "Train Epoch: 270 [2688/225000 (1%)] Loss: 17759.255859\n",
      "Train Epoch: 270 [5184/225000 (2%)] Loss: 17108.500000\n",
      "Train Epoch: 270 [7680/225000 (3%)] Loss: 17740.050781\n",
      "Train Epoch: 270 [10176/225000 (5%)] Loss: 17304.925781\n",
      "Train Epoch: 270 [12672/225000 (6%)] Loss: 17843.105469\n",
      "Train Epoch: 270 [15168/225000 (7%)] Loss: 17875.562500\n",
      "Train Epoch: 270 [17664/225000 (8%)] Loss: 17691.521484\n",
      "Train Epoch: 270 [20160/225000 (9%)] Loss: 17661.777344\n",
      "Train Epoch: 270 [22656/225000 (10%)] Loss: 17558.396484\n",
      "Train Epoch: 270 [25152/225000 (11%)] Loss: 17372.355469\n",
      "Train Epoch: 270 [27648/225000 (12%)] Loss: 17449.164062\n",
      "Train Epoch: 270 [30144/225000 (13%)] Loss: 17525.511719\n",
      "Train Epoch: 270 [32640/225000 (15%)] Loss: 16912.369141\n",
      "Train Epoch: 270 [35136/225000 (16%)] Loss: 17538.640625\n",
      "Train Epoch: 270 [37632/225000 (17%)] Loss: 17056.326172\n",
      "Train Epoch: 270 [40128/225000 (18%)] Loss: 17313.710938\n",
      "Train Epoch: 270 [42624/225000 (19%)] Loss: 17626.699219\n",
      "Train Epoch: 270 [45120/225000 (20%)] Loss: 17695.933594\n",
      "Train Epoch: 270 [47616/225000 (21%)] Loss: 17880.894531\n",
      "Train Epoch: 270 [50112/225000 (22%)] Loss: 17293.630859\n",
      "Train Epoch: 270 [52608/225000 (23%)] Loss: 17041.968750\n",
      "Train Epoch: 270 [55104/225000 (24%)] Loss: 17536.011719\n",
      "Train Epoch: 270 [57600/225000 (26%)] Loss: 17415.138672\n",
      "Train Epoch: 270 [60096/225000 (27%)] Loss: 17363.716797\n",
      "Train Epoch: 270 [62592/225000 (28%)] Loss: 17786.195312\n",
      "Train Epoch: 270 [65088/225000 (29%)] Loss: 17367.580078\n",
      "Train Epoch: 270 [67584/225000 (30%)] Loss: 17692.132812\n",
      "Train Epoch: 270 [70080/225000 (31%)] Loss: 17733.041016\n",
      "Train Epoch: 270 [72576/225000 (32%)] Loss: 17856.703125\n",
      "Train Epoch: 270 [75072/225000 (33%)] Loss: 17537.832031\n",
      "Train Epoch: 270 [77568/225000 (34%)] Loss: 17467.105469\n",
      "Train Epoch: 270 [80064/225000 (36%)] Loss: 17887.828125\n",
      "Train Epoch: 270 [82560/225000 (37%)] Loss: 17664.984375\n",
      "Train Epoch: 270 [85056/225000 (38%)] Loss: 17766.093750\n",
      "Train Epoch: 270 [87552/225000 (39%)] Loss: 17724.652344\n",
      "Train Epoch: 270 [90048/225000 (40%)] Loss: 17945.515625\n",
      "Train Epoch: 270 [92544/225000 (41%)] Loss: 17371.582031\n",
      "Train Epoch: 270 [95040/225000 (42%)] Loss: 17759.642578\n",
      "Train Epoch: 270 [97536/225000 (43%)] Loss: 17439.566406\n",
      "Train Epoch: 270 [100032/225000 (44%)] Loss: 17448.085938\n",
      "Train Epoch: 270 [102528/225000 (46%)] Loss: 17528.035156\n",
      "Train Epoch: 270 [105024/225000 (47%)] Loss: 17594.359375\n",
      "Train Epoch: 270 [107520/225000 (48%)] Loss: 17788.878906\n",
      "Train Epoch: 270 [110016/225000 (49%)] Loss: 17519.417969\n",
      "Train Epoch: 270 [112512/225000 (50%)] Loss: 17930.566406\n",
      "Train Epoch: 270 [115008/225000 (51%)] Loss: 17730.267578\n",
      "Train Epoch: 270 [117504/225000 (52%)] Loss: 17908.898438\n",
      "Train Epoch: 270 [120000/225000 (53%)] Loss: 17648.068359\n",
      "Train Epoch: 270 [122496/225000 (54%)] Loss: 17573.582031\n",
      "Train Epoch: 270 [124992/225000 (56%)] Loss: 17635.224609\n",
      "Train Epoch: 270 [127488/225000 (57%)] Loss: 17891.027344\n",
      "Train Epoch: 270 [129984/225000 (58%)] Loss: 17952.332031\n",
      "Train Epoch: 270 [132480/225000 (59%)] Loss: 17549.277344\n",
      "Train Epoch: 270 [134976/225000 (60%)] Loss: 17657.052734\n",
      "Train Epoch: 270 [137472/225000 (61%)] Loss: 17622.582031\n",
      "Train Epoch: 270 [139968/225000 (62%)] Loss: 17570.449219\n",
      "Train Epoch: 270 [142464/225000 (63%)] Loss: 17986.783203\n",
      "Train Epoch: 270 [144960/225000 (64%)] Loss: 17818.054688\n",
      "Train Epoch: 270 [147456/225000 (66%)] Loss: 17396.542969\n",
      "Train Epoch: 270 [149952/225000 (67%)] Loss: 17467.597656\n",
      "Train Epoch: 270 [152448/225000 (68%)] Loss: 17713.828125\n",
      "Train Epoch: 270 [154944/225000 (69%)] Loss: 17693.660156\n",
      "Train Epoch: 270 [157440/225000 (70%)] Loss: 17704.562500\n",
      "Train Epoch: 270 [159936/225000 (71%)] Loss: 17447.207031\n",
      "Train Epoch: 270 [162432/225000 (72%)] Loss: 17424.312500\n",
      "Train Epoch: 270 [164928/225000 (73%)] Loss: 17582.976562\n",
      "Train Epoch: 270 [167424/225000 (74%)] Loss: 17478.378906\n",
      "Train Epoch: 270 [169920/225000 (76%)] Loss: 17520.912109\n",
      "Train Epoch: 270 [172416/225000 (77%)] Loss: 17238.414062\n",
      "Train Epoch: 270 [174912/225000 (78%)] Loss: 17823.468750\n",
      "Train Epoch: 270 [177408/225000 (79%)] Loss: 17074.804688\n",
      "Train Epoch: 270 [179904/225000 (80%)] Loss: 17443.371094\n",
      "Train Epoch: 270 [182400/225000 (81%)] Loss: 17452.130859\n",
      "Train Epoch: 270 [184896/225000 (82%)] Loss: 17735.015625\n",
      "Train Epoch: 270 [187392/225000 (83%)] Loss: 17843.039062\n",
      "Train Epoch: 270 [189888/225000 (84%)] Loss: 17533.953125\n",
      "Train Epoch: 270 [192384/225000 (86%)] Loss: 17887.095703\n",
      "Train Epoch: 270 [194880/225000 (87%)] Loss: 17378.976562\n",
      "Train Epoch: 270 [197376/225000 (88%)] Loss: 17342.427734\n",
      "Train Epoch: 270 [199872/225000 (89%)] Loss: 17349.644531\n",
      "Train Epoch: 270 [202368/225000 (90%)] Loss: 17307.310547\n",
      "Train Epoch: 270 [204864/225000 (91%)] Loss: 17409.468750\n",
      "Train Epoch: 270 [207360/225000 (92%)] Loss: 18234.003906\n",
      "Train Epoch: 270 [209856/225000 (93%)] Loss: 17541.386719\n",
      "Train Epoch: 270 [212352/225000 (94%)] Loss: 17791.257812\n",
      "Train Epoch: 270 [214848/225000 (95%)] Loss: 17373.304688\n",
      "Train Epoch: 270 [217344/225000 (97%)] Loss: 16654.222656\n",
      "Train Epoch: 270 [219840/225000 (98%)] Loss: 18045.511719\n",
      "Train Epoch: 270 [222336/225000 (99%)] Loss: 17917.722656\n",
      "Train Epoch: 270 [224832/225000 (100%)] Loss: 17790.101562\n",
      "    epoch          : 270\n",
      "    loss           : 17590.27967266825\n",
      "    val_loss       : 17503.039242579736\n",
      "Train Epoch: 271 [192/225000 (0%)] Loss: 17741.332031\n",
      "Train Epoch: 271 [2688/225000 (1%)] Loss: 17665.425781\n",
      "Train Epoch: 271 [5184/225000 (2%)] Loss: 16632.654297\n",
      "Train Epoch: 271 [7680/225000 (3%)] Loss: 17083.136719\n",
      "Train Epoch: 271 [10176/225000 (5%)] Loss: 17610.359375\n",
      "Train Epoch: 271 [12672/225000 (6%)] Loss: 17537.433594\n",
      "Train Epoch: 271 [15168/225000 (7%)] Loss: 17594.507812\n",
      "Train Epoch: 271 [17664/225000 (8%)] Loss: 17405.429688\n",
      "Train Epoch: 271 [20160/225000 (9%)] Loss: 17722.933594\n",
      "Train Epoch: 271 [22656/225000 (10%)] Loss: 17621.539062\n",
      "Train Epoch: 271 [25152/225000 (11%)] Loss: 17344.917969\n",
      "Train Epoch: 271 [27648/225000 (12%)] Loss: 17402.804688\n",
      "Train Epoch: 271 [30144/225000 (13%)] Loss: 17189.230469\n",
      "Train Epoch: 271 [32640/225000 (15%)] Loss: 17587.476562\n",
      "Train Epoch: 271 [35136/225000 (16%)] Loss: 17722.482422\n",
      "Train Epoch: 271 [37632/225000 (17%)] Loss: 17349.814453\n",
      "Train Epoch: 271 [40128/225000 (18%)] Loss: 17610.488281\n",
      "Train Epoch: 271 [42624/225000 (19%)] Loss: 17784.398438\n",
      "Train Epoch: 271 [45120/225000 (20%)] Loss: 17440.064453\n",
      "Train Epoch: 271 [47616/225000 (21%)] Loss: 17626.488281\n",
      "Train Epoch: 271 [50112/225000 (22%)] Loss: 17273.416016\n",
      "Train Epoch: 271 [52608/225000 (23%)] Loss: 17442.960938\n",
      "Train Epoch: 271 [55104/225000 (24%)] Loss: 17778.310547\n",
      "Train Epoch: 271 [57600/225000 (26%)] Loss: 17782.021484\n",
      "Train Epoch: 271 [60096/225000 (27%)] Loss: 18282.267578\n",
      "Train Epoch: 271 [62592/225000 (28%)] Loss: 17694.679688\n",
      "Train Epoch: 271 [65088/225000 (29%)] Loss: 17570.552734\n",
      "Train Epoch: 271 [67584/225000 (30%)] Loss: 17354.289062\n",
      "Train Epoch: 271 [70080/225000 (31%)] Loss: 17603.746094\n",
      "Train Epoch: 271 [72576/225000 (32%)] Loss: 17768.587891\n",
      "Train Epoch: 271 [75072/225000 (33%)] Loss: 17466.017578\n",
      "Train Epoch: 271 [77568/225000 (34%)] Loss: 17685.820312\n",
      "Train Epoch: 271 [80064/225000 (36%)] Loss: 17786.328125\n",
      "Train Epoch: 271 [82560/225000 (37%)] Loss: 17811.570312\n",
      "Train Epoch: 271 [85056/225000 (38%)] Loss: 17156.855469\n",
      "Train Epoch: 271 [87552/225000 (39%)] Loss: 17477.914062\n",
      "Train Epoch: 271 [90048/225000 (40%)] Loss: 17645.605469\n",
      "Train Epoch: 271 [92544/225000 (41%)] Loss: 17587.449219\n",
      "Train Epoch: 271 [95040/225000 (42%)] Loss: 17938.894531\n",
      "Train Epoch: 271 [97536/225000 (43%)] Loss: 18131.695312\n",
      "Train Epoch: 271 [100032/225000 (44%)] Loss: 17390.832031\n",
      "Train Epoch: 271 [102528/225000 (46%)] Loss: 17848.960938\n",
      "Train Epoch: 271 [105024/225000 (47%)] Loss: 17731.347656\n",
      "Train Epoch: 271 [107520/225000 (48%)] Loss: 17421.214844\n",
      "Train Epoch: 271 [110016/225000 (49%)] Loss: 17435.498047\n",
      "Train Epoch: 271 [112512/225000 (50%)] Loss: 17248.324219\n",
      "Train Epoch: 271 [115008/225000 (51%)] Loss: 18126.886719\n",
      "Train Epoch: 271 [117504/225000 (52%)] Loss: 17655.113281\n",
      "Train Epoch: 271 [120000/225000 (53%)] Loss: 17584.464844\n",
      "Train Epoch: 271 [122496/225000 (54%)] Loss: 16941.167969\n",
      "Train Epoch: 271 [124992/225000 (56%)] Loss: 17432.808594\n",
      "Train Epoch: 271 [127488/225000 (57%)] Loss: 17391.500000\n",
      "Train Epoch: 271 [129984/225000 (58%)] Loss: 17552.773438\n",
      "Train Epoch: 271 [132480/225000 (59%)] Loss: 17472.734375\n",
      "Train Epoch: 271 [134976/225000 (60%)] Loss: 17687.796875\n",
      "Train Epoch: 271 [137472/225000 (61%)] Loss: 17315.027344\n",
      "Train Epoch: 271 [139968/225000 (62%)] Loss: 17975.007812\n",
      "Train Epoch: 271 [142464/225000 (63%)] Loss: 17692.220703\n",
      "Train Epoch: 271 [144960/225000 (64%)] Loss: 17068.058594\n",
      "Train Epoch: 271 [147456/225000 (66%)] Loss: 17429.062500\n",
      "Train Epoch: 271 [149952/225000 (67%)] Loss: 17547.015625\n",
      "Train Epoch: 271 [152448/225000 (68%)] Loss: 17036.824219\n",
      "Train Epoch: 271 [154944/225000 (69%)] Loss: 17953.906250\n",
      "Train Epoch: 271 [157440/225000 (70%)] Loss: 17126.507812\n",
      "Train Epoch: 271 [159936/225000 (71%)] Loss: 17777.382812\n",
      "Train Epoch: 271 [162432/225000 (72%)] Loss: 17885.599609\n",
      "Train Epoch: 271 [164928/225000 (73%)] Loss: 17406.177734\n",
      "Train Epoch: 271 [167424/225000 (74%)] Loss: 17710.496094\n",
      "Train Epoch: 271 [169920/225000 (76%)] Loss: 17239.212891\n",
      "Train Epoch: 271 [172416/225000 (77%)] Loss: 17382.425781\n",
      "Train Epoch: 271 [174912/225000 (78%)] Loss: 17256.484375\n",
      "Train Epoch: 271 [177408/225000 (79%)] Loss: 17312.281250\n",
      "Train Epoch: 271 [179904/225000 (80%)] Loss: 17622.945312\n",
      "Train Epoch: 271 [182400/225000 (81%)] Loss: 17648.220703\n",
      "Train Epoch: 271 [184896/225000 (82%)] Loss: 17353.984375\n",
      "Train Epoch: 271 [187392/225000 (83%)] Loss: 17803.613281\n",
      "Train Epoch: 271 [189888/225000 (84%)] Loss: 17143.830078\n",
      "Train Epoch: 271 [192384/225000 (86%)] Loss: 17455.136719\n",
      "Train Epoch: 271 [194880/225000 (87%)] Loss: 17612.535156\n",
      "Train Epoch: 271 [197376/225000 (88%)] Loss: 17732.363281\n",
      "Train Epoch: 271 [199872/225000 (89%)] Loss: 18022.777344\n",
      "Train Epoch: 271 [202368/225000 (90%)] Loss: 17640.339844\n",
      "Train Epoch: 271 [204864/225000 (91%)] Loss: 17686.476562\n",
      "Train Epoch: 271 [207360/225000 (92%)] Loss: 17828.023438\n",
      "Train Epoch: 271 [209856/225000 (93%)] Loss: 17643.539062\n",
      "Train Epoch: 271 [212352/225000 (94%)] Loss: 17995.035156\n",
      "Train Epoch: 271 [214848/225000 (95%)] Loss: 18150.718750\n",
      "Train Epoch: 271 [217344/225000 (97%)] Loss: 17785.601562\n",
      "Train Epoch: 271 [219840/225000 (98%)] Loss: 17933.578125\n",
      "Train Epoch: 271 [222336/225000 (99%)] Loss: 18094.207031\n",
      "Train Epoch: 271 [224832/225000 (100%)] Loss: 17679.703125\n",
      "    epoch          : 271\n",
      "    loss           : 17594.88742367481\n",
      "    val_loss       : 17487.210539260894\n",
      "Train Epoch: 272 [192/225000 (0%)] Loss: 17665.980469\n",
      "Train Epoch: 272 [2688/225000 (1%)] Loss: 17694.597656\n",
      "Train Epoch: 272 [5184/225000 (2%)] Loss: 17581.859375\n",
      "Train Epoch: 272 [7680/225000 (3%)] Loss: 17292.464844\n",
      "Train Epoch: 272 [10176/225000 (5%)] Loss: 17565.498047\n",
      "Train Epoch: 272 [12672/225000 (6%)] Loss: 17735.183594\n",
      "Train Epoch: 272 [15168/225000 (7%)] Loss: 17713.828125\n",
      "Train Epoch: 272 [17664/225000 (8%)] Loss: 17474.240234\n",
      "Train Epoch: 272 [20160/225000 (9%)] Loss: 18202.042969\n",
      "Train Epoch: 272 [22656/225000 (10%)] Loss: 17754.300781\n",
      "Train Epoch: 272 [25152/225000 (11%)] Loss: 17598.250000\n",
      "Train Epoch: 272 [27648/225000 (12%)] Loss: 17468.048828\n",
      "Train Epoch: 272 [30144/225000 (13%)] Loss: 17564.212891\n",
      "Train Epoch: 272 [32640/225000 (15%)] Loss: 17868.136719\n",
      "Train Epoch: 272 [35136/225000 (16%)] Loss: 17503.515625\n",
      "Train Epoch: 272 [37632/225000 (17%)] Loss: 17524.037109\n",
      "Train Epoch: 272 [40128/225000 (18%)] Loss: 17572.330078\n",
      "Train Epoch: 272 [42624/225000 (19%)] Loss: 18015.382812\n",
      "Train Epoch: 272 [45120/225000 (20%)] Loss: 17499.539062\n",
      "Train Epoch: 272 [47616/225000 (21%)] Loss: 17787.822266\n",
      "Train Epoch: 272 [50112/225000 (22%)] Loss: 17483.773438\n",
      "Train Epoch: 272 [52608/225000 (23%)] Loss: 17624.050781\n",
      "Train Epoch: 272 [55104/225000 (24%)] Loss: 18312.449219\n",
      "Train Epoch: 272 [57600/225000 (26%)] Loss: 17853.640625\n",
      "Train Epoch: 272 [60096/225000 (27%)] Loss: 18531.171875\n",
      "Train Epoch: 272 [62592/225000 (28%)] Loss: 17549.582031\n",
      "Train Epoch: 272 [65088/225000 (29%)] Loss: 17606.466797\n",
      "Train Epoch: 272 [67584/225000 (30%)] Loss: 17689.162109\n",
      "Train Epoch: 272 [70080/225000 (31%)] Loss: 17596.878906\n",
      "Train Epoch: 272 [72576/225000 (32%)] Loss: 17177.281250\n",
      "Train Epoch: 272 [75072/225000 (33%)] Loss: 17623.273438\n",
      "Train Epoch: 272 [77568/225000 (34%)] Loss: 17635.964844\n",
      "Train Epoch: 272 [80064/225000 (36%)] Loss: 17587.023438\n",
      "Train Epoch: 272 [82560/225000 (37%)] Loss: 17580.314453\n",
      "Train Epoch: 272 [85056/225000 (38%)] Loss: 17484.175781\n",
      "Train Epoch: 272 [87552/225000 (39%)] Loss: 17643.529297\n",
      "Train Epoch: 272 [90048/225000 (40%)] Loss: 17818.599609\n",
      "Train Epoch: 272 [92544/225000 (41%)] Loss: 17471.974609\n",
      "Train Epoch: 272 [95040/225000 (42%)] Loss: 17760.328125\n",
      "Train Epoch: 272 [97536/225000 (43%)] Loss: 17256.710938\n",
      "Train Epoch: 272 [100032/225000 (44%)] Loss: 18015.289062\n",
      "Train Epoch: 272 [102528/225000 (46%)] Loss: 17815.378906\n",
      "Train Epoch: 272 [105024/225000 (47%)] Loss: 17616.027344\n",
      "Train Epoch: 272 [107520/225000 (48%)] Loss: 17531.359375\n",
      "Train Epoch: 272 [110016/225000 (49%)] Loss: 17867.699219\n",
      "Train Epoch: 272 [112512/225000 (50%)] Loss: 17361.753906\n",
      "Train Epoch: 272 [115008/225000 (51%)] Loss: 17518.273438\n",
      "Train Epoch: 272 [117504/225000 (52%)] Loss: 17584.121094\n",
      "Train Epoch: 272 [120000/225000 (53%)] Loss: 17772.417969\n",
      "Train Epoch: 272 [122496/225000 (54%)] Loss: 17498.339844\n",
      "Train Epoch: 272 [124992/225000 (56%)] Loss: 17433.707031\n",
      "Train Epoch: 272 [127488/225000 (57%)] Loss: 17646.429688\n",
      "Train Epoch: 272 [129984/225000 (58%)] Loss: 17465.740234\n",
      "Train Epoch: 272 [132480/225000 (59%)] Loss: 17337.406250\n",
      "Train Epoch: 272 [134976/225000 (60%)] Loss: 17411.171875\n",
      "Train Epoch: 272 [137472/225000 (61%)] Loss: 18387.363281\n",
      "Train Epoch: 272 [139968/225000 (62%)] Loss: 17800.640625\n",
      "Train Epoch: 272 [142464/225000 (63%)] Loss: 17379.605469\n",
      "Train Epoch: 272 [144960/225000 (64%)] Loss: 17417.353516\n",
      "Train Epoch: 272 [147456/225000 (66%)] Loss: 17581.054688\n",
      "Train Epoch: 272 [149952/225000 (67%)] Loss: 17401.253906\n",
      "Train Epoch: 272 [152448/225000 (68%)] Loss: 17573.933594\n",
      "Train Epoch: 272 [154944/225000 (69%)] Loss: 17146.593750\n",
      "Train Epoch: 272 [157440/225000 (70%)] Loss: 17426.378906\n",
      "Train Epoch: 272 [159936/225000 (71%)] Loss: 17670.076172\n",
      "Train Epoch: 272 [162432/225000 (72%)] Loss: 17301.281250\n",
      "Train Epoch: 272 [164928/225000 (73%)] Loss: 17209.416016\n",
      "Train Epoch: 272 [167424/225000 (74%)] Loss: 17510.031250\n",
      "Train Epoch: 272 [169920/225000 (76%)] Loss: 17898.761719\n",
      "Train Epoch: 272 [172416/225000 (77%)] Loss: 17812.937500\n",
      "Train Epoch: 272 [174912/225000 (78%)] Loss: 17625.222656\n",
      "Train Epoch: 272 [177408/225000 (79%)] Loss: 17867.699219\n",
      "Train Epoch: 272 [179904/225000 (80%)] Loss: 17164.894531\n",
      "Train Epoch: 272 [182400/225000 (81%)] Loss: 17269.179688\n",
      "Train Epoch: 272 [184896/225000 (82%)] Loss: 17416.839844\n",
      "Train Epoch: 272 [187392/225000 (83%)] Loss: 17620.300781\n",
      "Train Epoch: 272 [189888/225000 (84%)] Loss: 17863.960938\n",
      "Train Epoch: 272 [192384/225000 (86%)] Loss: 17617.460938\n",
      "Train Epoch: 272 [194880/225000 (87%)] Loss: 17383.191406\n",
      "Train Epoch: 272 [197376/225000 (88%)] Loss: 17306.261719\n",
      "Train Epoch: 272 [199872/225000 (89%)] Loss: 17756.173828\n",
      "Train Epoch: 272 [202368/225000 (90%)] Loss: 17398.273438\n",
      "Train Epoch: 272 [204864/225000 (91%)] Loss: 17472.925781\n",
      "Train Epoch: 272 [207360/225000 (92%)] Loss: 17582.617188\n",
      "Train Epoch: 272 [209856/225000 (93%)] Loss: 17326.292969\n",
      "Train Epoch: 272 [212352/225000 (94%)] Loss: 17503.132812\n",
      "Train Epoch: 272 [214848/225000 (95%)] Loss: 18132.148438\n",
      "Train Epoch: 272 [217344/225000 (97%)] Loss: 17764.894531\n",
      "Train Epoch: 272 [219840/225000 (98%)] Loss: 17090.507812\n",
      "Train Epoch: 272 [222336/225000 (99%)] Loss: 17516.390625\n",
      "Train Epoch: 272 [224832/225000 (100%)] Loss: 17635.816406\n",
      "    epoch          : 272\n",
      "    loss           : 17581.976117547463\n",
      "    val_loss       : 17494.661494987613\n",
      "Train Epoch: 273 [192/225000 (0%)] Loss: 17505.884766\n",
      "Train Epoch: 273 [2688/225000 (1%)] Loss: 17206.664062\n",
      "Train Epoch: 273 [5184/225000 (2%)] Loss: 18073.894531\n",
      "Train Epoch: 273 [7680/225000 (3%)] Loss: 17162.337891\n",
      "Train Epoch: 273 [10176/225000 (5%)] Loss: 18230.775391\n",
      "Train Epoch: 273 [12672/225000 (6%)] Loss: 17367.164062\n",
      "Train Epoch: 273 [15168/225000 (7%)] Loss: 17983.089844\n",
      "Train Epoch: 273 [17664/225000 (8%)] Loss: 17716.148438\n",
      "Train Epoch: 273 [20160/225000 (9%)] Loss: 17259.738281\n",
      "Train Epoch: 273 [22656/225000 (10%)] Loss: 17800.494141\n",
      "Train Epoch: 273 [25152/225000 (11%)] Loss: 17804.076172\n",
      "Train Epoch: 273 [27648/225000 (12%)] Loss: 17563.832031\n",
      "Train Epoch: 273 [30144/225000 (13%)] Loss: 17750.439453\n",
      "Train Epoch: 273 [32640/225000 (15%)] Loss: 17923.630859\n",
      "Train Epoch: 273 [35136/225000 (16%)] Loss: 17426.025391\n",
      "Train Epoch: 273 [37632/225000 (17%)] Loss: 17403.574219\n",
      "Train Epoch: 273 [40128/225000 (18%)] Loss: 17442.441406\n",
      "Train Epoch: 273 [42624/225000 (19%)] Loss: 17371.566406\n",
      "Train Epoch: 273 [45120/225000 (20%)] Loss: 17434.685547\n",
      "Train Epoch: 273 [47616/225000 (21%)] Loss: 17966.716797\n",
      "Train Epoch: 273 [50112/225000 (22%)] Loss: 17702.390625\n",
      "Train Epoch: 273 [52608/225000 (23%)] Loss: 17530.832031\n",
      "Train Epoch: 273 [55104/225000 (24%)] Loss: 17400.484375\n",
      "Train Epoch: 273 [57600/225000 (26%)] Loss: 17375.201172\n",
      "Train Epoch: 273 [60096/225000 (27%)] Loss: 17685.445312\n",
      "Train Epoch: 273 [62592/225000 (28%)] Loss: 17574.218750\n",
      "Train Epoch: 273 [65088/225000 (29%)] Loss: 17198.585938\n",
      "Train Epoch: 273 [67584/225000 (30%)] Loss: 17190.345703\n",
      "Train Epoch: 273 [70080/225000 (31%)] Loss: 17713.833984\n",
      "Train Epoch: 273 [72576/225000 (32%)] Loss: 17910.083984\n",
      "Train Epoch: 273 [75072/225000 (33%)] Loss: 17943.552734\n",
      "Train Epoch: 273 [77568/225000 (34%)] Loss: 17812.001953\n",
      "Train Epoch: 273 [80064/225000 (36%)] Loss: 17541.449219\n",
      "Train Epoch: 273 [82560/225000 (37%)] Loss: 17520.882812\n",
      "Train Epoch: 273 [85056/225000 (38%)] Loss: 17497.748047\n",
      "Train Epoch: 273 [87552/225000 (39%)] Loss: 18074.457031\n",
      "Train Epoch: 273 [90048/225000 (40%)] Loss: 17347.894531\n",
      "Train Epoch: 273 [92544/225000 (41%)] Loss: 17544.816406\n",
      "Train Epoch: 273 [95040/225000 (42%)] Loss: 17535.035156\n",
      "Train Epoch: 273 [97536/225000 (43%)] Loss: 17965.582031\n",
      "Train Epoch: 273 [100032/225000 (44%)] Loss: 17323.281250\n",
      "Train Epoch: 273 [102528/225000 (46%)] Loss: 17606.490234\n",
      "Train Epoch: 273 [105024/225000 (47%)] Loss: 18142.164062\n",
      "Train Epoch: 273 [107520/225000 (48%)] Loss: 17359.115234\n",
      "Train Epoch: 273 [110016/225000 (49%)] Loss: 17133.480469\n",
      "Train Epoch: 273 [112512/225000 (50%)] Loss: 17037.962891\n",
      "Train Epoch: 273 [115008/225000 (51%)] Loss: 17273.109375\n",
      "Train Epoch: 273 [117504/225000 (52%)] Loss: 17649.220703\n",
      "Train Epoch: 273 [120000/225000 (53%)] Loss: 17541.742188\n",
      "Train Epoch: 273 [122496/225000 (54%)] Loss: 17520.460938\n",
      "Train Epoch: 273 [124992/225000 (56%)] Loss: 17870.199219\n",
      "Train Epoch: 273 [127488/225000 (57%)] Loss: 17689.242188\n",
      "Train Epoch: 273 [129984/225000 (58%)] Loss: 17510.863281\n",
      "Train Epoch: 273 [132480/225000 (59%)] Loss: 17217.978516\n",
      "Train Epoch: 273 [134976/225000 (60%)] Loss: 17245.117188\n",
      "Train Epoch: 273 [137472/225000 (61%)] Loss: 17500.585938\n",
      "Train Epoch: 273 [139968/225000 (62%)] Loss: 17442.843750\n",
      "Train Epoch: 273 [142464/225000 (63%)] Loss: 17564.820312\n",
      "Train Epoch: 273 [144960/225000 (64%)] Loss: 17692.976562\n",
      "Train Epoch: 273 [147456/225000 (66%)] Loss: 17252.195312\n",
      "Train Epoch: 273 [149952/225000 (67%)] Loss: 17072.261719\n",
      "Train Epoch: 273 [152448/225000 (68%)] Loss: 17299.972656\n",
      "Train Epoch: 273 [154944/225000 (69%)] Loss: 17659.896484\n",
      "Train Epoch: 273 [157440/225000 (70%)] Loss: 17456.232422\n",
      "Train Epoch: 273 [159936/225000 (71%)] Loss: 17901.988281\n",
      "Train Epoch: 273 [162432/225000 (72%)] Loss: 17654.158203\n",
      "Train Epoch: 273 [164928/225000 (73%)] Loss: 17313.324219\n",
      "Train Epoch: 273 [167424/225000 (74%)] Loss: 17710.398438\n",
      "Train Epoch: 273 [169920/225000 (76%)] Loss: 17545.472656\n",
      "Train Epoch: 273 [172416/225000 (77%)] Loss: 17699.121094\n",
      "Train Epoch: 273 [174912/225000 (78%)] Loss: 17681.281250\n",
      "Train Epoch: 273 [177408/225000 (79%)] Loss: 17579.009766\n",
      "Train Epoch: 273 [179904/225000 (80%)] Loss: 17314.558594\n",
      "Train Epoch: 273 [182400/225000 (81%)] Loss: 17254.710938\n",
      "Train Epoch: 273 [184896/225000 (82%)] Loss: 17706.000000\n",
      "Train Epoch: 273 [187392/225000 (83%)] Loss: 17978.300781\n",
      "Train Epoch: 273 [189888/225000 (84%)] Loss: 17831.585938\n",
      "Train Epoch: 273 [192384/225000 (86%)] Loss: 17575.183594\n",
      "Train Epoch: 273 [194880/225000 (87%)] Loss: 17636.054688\n",
      "Train Epoch: 273 [197376/225000 (88%)] Loss: 17657.324219\n",
      "Train Epoch: 273 [199872/225000 (89%)] Loss: 17762.984375\n",
      "Train Epoch: 273 [202368/225000 (90%)] Loss: 17259.453125\n",
      "Train Epoch: 273 [204864/225000 (91%)] Loss: 17240.441406\n",
      "Train Epoch: 273 [207360/225000 (92%)] Loss: 17900.531250\n",
      "Train Epoch: 273 [209856/225000 (93%)] Loss: 17792.417969\n",
      "Train Epoch: 273 [212352/225000 (94%)] Loss: 17441.177734\n",
      "Train Epoch: 273 [214848/225000 (95%)] Loss: 17481.218750\n",
      "Train Epoch: 273 [217344/225000 (97%)] Loss: 17734.453125\n",
      "Train Epoch: 273 [219840/225000 (98%)] Loss: 17717.070312\n",
      "Train Epoch: 273 [222336/225000 (99%)] Loss: 17457.392578\n",
      "Train Epoch: 273 [224832/225000 (100%)] Loss: 17597.773438\n",
      "    epoch          : 273\n",
      "    loss           : 17583.163726702485\n",
      "    val_loss       : 17488.327930935466\n",
      "Train Epoch: 274 [192/225000 (0%)] Loss: 17284.449219\n",
      "Train Epoch: 274 [2688/225000 (1%)] Loss: 17235.414062\n",
      "Train Epoch: 274 [5184/225000 (2%)] Loss: 17776.886719\n",
      "Train Epoch: 274 [7680/225000 (3%)] Loss: 17025.312500\n",
      "Train Epoch: 274 [10176/225000 (5%)] Loss: 17247.458984\n",
      "Train Epoch: 274 [12672/225000 (6%)] Loss: 17431.683594\n",
      "Train Epoch: 274 [15168/225000 (7%)] Loss: 17547.382812\n",
      "Train Epoch: 274 [17664/225000 (8%)] Loss: 17561.144531\n",
      "Train Epoch: 274 [20160/225000 (9%)] Loss: 17183.355469\n",
      "Train Epoch: 274 [22656/225000 (10%)] Loss: 16960.140625\n",
      "Train Epoch: 274 [25152/225000 (11%)] Loss: 17912.941406\n",
      "Train Epoch: 274 [27648/225000 (12%)] Loss: 17739.001953\n",
      "Train Epoch: 274 [30144/225000 (13%)] Loss: 17170.748047\n",
      "Train Epoch: 274 [32640/225000 (15%)] Loss: 17657.617188\n",
      "Train Epoch: 274 [35136/225000 (16%)] Loss: 17630.363281\n",
      "Train Epoch: 274 [37632/225000 (17%)] Loss: 17633.582031\n",
      "Train Epoch: 274 [40128/225000 (18%)] Loss: 17525.992188\n",
      "Train Epoch: 274 [42624/225000 (19%)] Loss: 17561.425781\n",
      "Train Epoch: 274 [45120/225000 (20%)] Loss: 17345.142578\n",
      "Train Epoch: 274 [47616/225000 (21%)] Loss: 17187.210938\n",
      "Train Epoch: 274 [50112/225000 (22%)] Loss: 17266.500000\n",
      "Train Epoch: 274 [52608/225000 (23%)] Loss: 17271.486328\n",
      "Train Epoch: 274 [55104/225000 (24%)] Loss: 17235.052734\n",
      "Train Epoch: 274 [57600/225000 (26%)] Loss: 17418.476562\n",
      "Train Epoch: 274 [60096/225000 (27%)] Loss: 17793.585938\n",
      "Train Epoch: 274 [62592/225000 (28%)] Loss: 17667.236328\n",
      "Train Epoch: 274 [65088/225000 (29%)] Loss: 17352.417969\n",
      "Train Epoch: 274 [67584/225000 (30%)] Loss: 17741.730469\n",
      "Train Epoch: 274 [70080/225000 (31%)] Loss: 17644.810547\n",
      "Train Epoch: 274 [72576/225000 (32%)] Loss: 17306.070312\n",
      "Train Epoch: 274 [75072/225000 (33%)] Loss: 17520.855469\n",
      "Train Epoch: 274 [77568/225000 (34%)] Loss: 17669.753906\n",
      "Train Epoch: 274 [80064/225000 (36%)] Loss: 17162.074219\n",
      "Train Epoch: 274 [82560/225000 (37%)] Loss: 17587.814453\n",
      "Train Epoch: 274 [85056/225000 (38%)] Loss: 17544.359375\n",
      "Train Epoch: 274 [87552/225000 (39%)] Loss: 17352.722656\n",
      "Train Epoch: 274 [90048/225000 (40%)] Loss: 17499.076172\n",
      "Train Epoch: 274 [92544/225000 (41%)] Loss: 18172.304688\n",
      "Train Epoch: 274 [95040/225000 (42%)] Loss: 17260.160156\n",
      "Train Epoch: 274 [97536/225000 (43%)] Loss: 18189.527344\n",
      "Train Epoch: 274 [100032/225000 (44%)] Loss: 17544.839844\n",
      "Train Epoch: 274 [102528/225000 (46%)] Loss: 17429.296875\n",
      "Train Epoch: 274 [105024/225000 (47%)] Loss: 17311.207031\n",
      "Train Epoch: 274 [107520/225000 (48%)] Loss: 17514.691406\n",
      "Train Epoch: 274 [110016/225000 (49%)] Loss: 17432.904297\n",
      "Train Epoch: 274 [112512/225000 (50%)] Loss: 17182.968750\n",
      "Train Epoch: 274 [115008/225000 (51%)] Loss: 17225.597656\n",
      "Train Epoch: 274 [117504/225000 (52%)] Loss: 17617.835938\n",
      "Train Epoch: 274 [120000/225000 (53%)] Loss: 17475.449219\n",
      "Train Epoch: 274 [122496/225000 (54%)] Loss: 17461.634766\n",
      "Train Epoch: 274 [124992/225000 (56%)] Loss: 17640.230469\n",
      "Train Epoch: 274 [127488/225000 (57%)] Loss: 17685.857422\n",
      "Train Epoch: 274 [129984/225000 (58%)] Loss: 17800.517578\n",
      "Train Epoch: 274 [132480/225000 (59%)] Loss: 17518.056641\n",
      "Train Epoch: 274 [134976/225000 (60%)] Loss: 17615.214844\n",
      "Train Epoch: 274 [137472/225000 (61%)] Loss: 17404.957031\n",
      "Train Epoch: 274 [139968/225000 (62%)] Loss: 18036.617188\n",
      "Train Epoch: 274 [142464/225000 (63%)] Loss: 17273.015625\n",
      "Train Epoch: 274 [144960/225000 (64%)] Loss: 17643.074219\n",
      "Train Epoch: 274 [147456/225000 (66%)] Loss: 17509.441406\n",
      "Train Epoch: 274 [149952/225000 (67%)] Loss: 17978.347656\n",
      "Train Epoch: 274 [152448/225000 (68%)] Loss: 17688.226562\n",
      "Train Epoch: 274 [154944/225000 (69%)] Loss: 17561.175781\n",
      "Train Epoch: 274 [157440/225000 (70%)] Loss: 17646.441406\n",
      "Train Epoch: 274 [159936/225000 (71%)] Loss: 17246.859375\n",
      "Train Epoch: 274 [162432/225000 (72%)] Loss: 17235.667969\n",
      "Train Epoch: 274 [164928/225000 (73%)] Loss: 17405.031250\n",
      "Train Epoch: 274 [167424/225000 (74%)] Loss: 17903.902344\n",
      "Train Epoch: 274 [169920/225000 (76%)] Loss: 17500.636719\n",
      "Train Epoch: 274 [172416/225000 (77%)] Loss: 17184.572266\n",
      "Train Epoch: 274 [174912/225000 (78%)] Loss: 17313.972656\n",
      "Train Epoch: 274 [177408/225000 (79%)] Loss: 17586.419922\n",
      "Train Epoch: 274 [179904/225000 (80%)] Loss: 17438.707031\n",
      "Train Epoch: 274 [182400/225000 (81%)] Loss: 17578.089844\n",
      "Train Epoch: 274 [184896/225000 (82%)] Loss: 16818.958984\n",
      "Train Epoch: 274 [187392/225000 (83%)] Loss: 17483.507812\n",
      "Train Epoch: 274 [189888/225000 (84%)] Loss: 17605.005859\n",
      "Train Epoch: 274 [192384/225000 (86%)] Loss: 17133.496094\n",
      "Train Epoch: 274 [194880/225000 (87%)] Loss: 17452.164062\n",
      "Train Epoch: 274 [197376/225000 (88%)] Loss: 17444.044922\n",
      "Train Epoch: 274 [199872/225000 (89%)] Loss: 17990.458984\n",
      "Train Epoch: 274 [202368/225000 (90%)] Loss: 17391.683594\n",
      "Train Epoch: 274 [204864/225000 (91%)] Loss: 17626.142578\n",
      "Train Epoch: 274 [207360/225000 (92%)] Loss: 17197.242188\n",
      "Train Epoch: 274 [209856/225000 (93%)] Loss: 17898.714844\n",
      "Train Epoch: 274 [212352/225000 (94%)] Loss: 17419.271484\n",
      "Train Epoch: 274 [214848/225000 (95%)] Loss: 17587.914062\n",
      "Train Epoch: 274 [217344/225000 (97%)] Loss: 17775.195312\n",
      "Train Epoch: 274 [219840/225000 (98%)] Loss: 17567.171875\n",
      "Train Epoch: 274 [222336/225000 (99%)] Loss: 17581.394531\n",
      "Train Epoch: 274 [224832/225000 (100%)] Loss: 16987.476562\n",
      "    epoch          : 274\n",
      "    loss           : 17575.208966876868\n",
      "    val_loss       : 17495.91864169099\n",
      "Train Epoch: 275 [192/225000 (0%)] Loss: 17189.542969\n",
      "Train Epoch: 275 [2688/225000 (1%)] Loss: 17431.554688\n",
      "Train Epoch: 275 [5184/225000 (2%)] Loss: 17699.621094\n",
      "Train Epoch: 275 [7680/225000 (3%)] Loss: 17765.464844\n",
      "Train Epoch: 275 [10176/225000 (5%)] Loss: 17446.660156\n",
      "Train Epoch: 275 [12672/225000 (6%)] Loss: 17516.277344\n",
      "Train Epoch: 275 [15168/225000 (7%)] Loss: 17461.753906\n",
      "Train Epoch: 275 [17664/225000 (8%)] Loss: 17584.363281\n",
      "Train Epoch: 275 [20160/225000 (9%)] Loss: 17248.033203\n",
      "Train Epoch: 275 [22656/225000 (10%)] Loss: 17529.279297\n",
      "Train Epoch: 275 [25152/225000 (11%)] Loss: 17539.564453\n",
      "Train Epoch: 275 [27648/225000 (12%)] Loss: 17801.564453\n",
      "Train Epoch: 275 [30144/225000 (13%)] Loss: 17546.269531\n",
      "Train Epoch: 275 [32640/225000 (15%)] Loss: 17378.462891\n",
      "Train Epoch: 275 [35136/225000 (16%)] Loss: 17418.468750\n",
      "Train Epoch: 275 [37632/225000 (17%)] Loss: 17313.621094\n",
      "Train Epoch: 275 [40128/225000 (18%)] Loss: 17815.656250\n",
      "Train Epoch: 275 [42624/225000 (19%)] Loss: 17803.460938\n",
      "Train Epoch: 275 [45120/225000 (20%)] Loss: 17083.205078\n",
      "Train Epoch: 275 [47616/225000 (21%)] Loss: 17765.125000\n",
      "Train Epoch: 275 [50112/225000 (22%)] Loss: 17470.929688\n",
      "Train Epoch: 275 [52608/225000 (23%)] Loss: 17978.882812\n",
      "Train Epoch: 275 [55104/225000 (24%)] Loss: 17436.316406\n",
      "Train Epoch: 275 [57600/225000 (26%)] Loss: 17514.773438\n",
      "Train Epoch: 275 [60096/225000 (27%)] Loss: 17306.636719\n",
      "Train Epoch: 275 [62592/225000 (28%)] Loss: 17743.207031\n",
      "Train Epoch: 275 [65088/225000 (29%)] Loss: 17717.224609\n",
      "Train Epoch: 275 [67584/225000 (30%)] Loss: 17248.339844\n",
      "Train Epoch: 275 [70080/225000 (31%)] Loss: 17429.701172\n",
      "Train Epoch: 275 [72576/225000 (32%)] Loss: 16952.207031\n",
      "Train Epoch: 275 [75072/225000 (33%)] Loss: 17439.951172\n",
      "Train Epoch: 275 [77568/225000 (34%)] Loss: 17531.816406\n",
      "Train Epoch: 275 [80064/225000 (36%)] Loss: 17489.476562\n",
      "Train Epoch: 275 [82560/225000 (37%)] Loss: 17617.148438\n",
      "Train Epoch: 275 [85056/225000 (38%)] Loss: 17561.890625\n",
      "Train Epoch: 275 [87552/225000 (39%)] Loss: 17506.546875\n",
      "Train Epoch: 275 [90048/225000 (40%)] Loss: 17505.121094\n",
      "Train Epoch: 275 [92544/225000 (41%)] Loss: 17830.470703\n",
      "Train Epoch: 275 [95040/225000 (42%)] Loss: 18121.578125\n",
      "Train Epoch: 275 [97536/225000 (43%)] Loss: 17973.810547\n",
      "Train Epoch: 275 [100032/225000 (44%)] Loss: 17740.968750\n",
      "Train Epoch: 275 [102528/225000 (46%)] Loss: 17955.128906\n",
      "Train Epoch: 275 [105024/225000 (47%)] Loss: 17226.488281\n",
      "Train Epoch: 275 [107520/225000 (48%)] Loss: 17555.429688\n",
      "Train Epoch: 275 [110016/225000 (49%)] Loss: 17505.423828\n",
      "Train Epoch: 275 [112512/225000 (50%)] Loss: 16892.167969\n",
      "Train Epoch: 275 [115008/225000 (51%)] Loss: 17455.667969\n",
      "Train Epoch: 275 [117504/225000 (52%)] Loss: 18032.476562\n",
      "Train Epoch: 275 [120000/225000 (53%)] Loss: 17978.460938\n",
      "Train Epoch: 275 [122496/225000 (54%)] Loss: 17677.582031\n",
      "Train Epoch: 275 [124992/225000 (56%)] Loss: 17562.873047\n",
      "Train Epoch: 275 [127488/225000 (57%)] Loss: 17387.097656\n",
      "Train Epoch: 275 [129984/225000 (58%)] Loss: 17537.421875\n",
      "Train Epoch: 275 [132480/225000 (59%)] Loss: 17434.601562\n",
      "Train Epoch: 275 [134976/225000 (60%)] Loss: 17737.929688\n",
      "Train Epoch: 275 [137472/225000 (61%)] Loss: 17426.783203\n",
      "Train Epoch: 275 [139968/225000 (62%)] Loss: 17616.935547\n",
      "Train Epoch: 275 [142464/225000 (63%)] Loss: 17574.632812\n",
      "Train Epoch: 275 [144960/225000 (64%)] Loss: 17576.828125\n",
      "Train Epoch: 275 [147456/225000 (66%)] Loss: 17672.906250\n",
      "Train Epoch: 275 [149952/225000 (67%)] Loss: 17655.144531\n",
      "Train Epoch: 275 [152448/225000 (68%)] Loss: 17301.582031\n",
      "Train Epoch: 275 [154944/225000 (69%)] Loss: 17509.105469\n",
      "Train Epoch: 275 [157440/225000 (70%)] Loss: 18136.414062\n",
      "Train Epoch: 275 [159936/225000 (71%)] Loss: 17803.316406\n",
      "Train Epoch: 275 [162432/225000 (72%)] Loss: 17483.859375\n",
      "Train Epoch: 275 [164928/225000 (73%)] Loss: 17572.837891\n",
      "Train Epoch: 275 [167424/225000 (74%)] Loss: 17671.408203\n",
      "Train Epoch: 275 [169920/225000 (76%)] Loss: 17588.019531\n",
      "Train Epoch: 275 [172416/225000 (77%)] Loss: 17696.613281\n",
      "Train Epoch: 275 [174912/225000 (78%)] Loss: 17550.171875\n",
      "Train Epoch: 275 [177408/225000 (79%)] Loss: 17587.078125\n",
      "Train Epoch: 275 [179904/225000 (80%)] Loss: 17223.660156\n",
      "Train Epoch: 275 [182400/225000 (81%)] Loss: 17291.375000\n",
      "Train Epoch: 275 [184896/225000 (82%)] Loss: 17790.769531\n",
      "Train Epoch: 275 [187392/225000 (83%)] Loss: 17633.238281\n",
      "Train Epoch: 275 [189888/225000 (84%)] Loss: 17323.187500\n",
      "Train Epoch: 275 [192384/225000 (86%)] Loss: 17797.330078\n",
      "Train Epoch: 275 [194880/225000 (87%)] Loss: 17010.146484\n",
      "Train Epoch: 275 [197376/225000 (88%)] Loss: 17913.722656\n",
      "Train Epoch: 275 [199872/225000 (89%)] Loss: 17196.519531\n",
      "Train Epoch: 275 [202368/225000 (90%)] Loss: 17327.542969\n",
      "Train Epoch: 275 [204864/225000 (91%)] Loss: 17539.281250\n",
      "Train Epoch: 275 [207360/225000 (92%)] Loss: 17569.144531\n",
      "Train Epoch: 275 [209856/225000 (93%)] Loss: 17967.457031\n",
      "Train Epoch: 275 [212352/225000 (94%)] Loss: 17708.625000\n",
      "Train Epoch: 275 [214848/225000 (95%)] Loss: 17529.054688\n",
      "Train Epoch: 275 [217344/225000 (97%)] Loss: 17511.789062\n",
      "Train Epoch: 275 [219840/225000 (98%)] Loss: 17705.761719\n",
      "Train Epoch: 275 [222336/225000 (99%)] Loss: 17649.410156\n",
      "Train Epoch: 275 [224832/225000 (100%)] Loss: 17117.453125\n",
      "    epoch          : 275\n",
      "    loss           : 17555.046335057596\n",
      "    val_loss       : 17472.676228808992\n",
      "Train Epoch: 276 [192/225000 (0%)] Loss: 17611.968750\n",
      "Train Epoch: 276 [2688/225000 (1%)] Loss: 17513.222656\n",
      "Train Epoch: 276 [5184/225000 (2%)] Loss: 17750.396484\n",
      "Train Epoch: 276 [7680/225000 (3%)] Loss: 17213.707031\n",
      "Train Epoch: 276 [10176/225000 (5%)] Loss: 17255.191406\n",
      "Train Epoch: 276 [12672/225000 (6%)] Loss: 17472.224609\n",
      "Train Epoch: 276 [15168/225000 (7%)] Loss: 17114.617188\n",
      "Train Epoch: 276 [17664/225000 (8%)] Loss: 17446.441406\n",
      "Train Epoch: 276 [20160/225000 (9%)] Loss: 17571.679688\n",
      "Train Epoch: 276 [22656/225000 (10%)] Loss: 17281.429688\n",
      "Train Epoch: 276 [25152/225000 (11%)] Loss: 17114.738281\n",
      "Train Epoch: 276 [27648/225000 (12%)] Loss: 17650.585938\n",
      "Train Epoch: 276 [30144/225000 (13%)] Loss: 17582.558594\n",
      "Train Epoch: 276 [32640/225000 (15%)] Loss: 17535.826172\n",
      "Train Epoch: 276 [35136/225000 (16%)] Loss: 17825.988281\n",
      "Train Epoch: 276 [37632/225000 (17%)] Loss: 17340.476562\n",
      "Train Epoch: 276 [40128/225000 (18%)] Loss: 17798.863281\n",
      "Train Epoch: 276 [42624/225000 (19%)] Loss: 17631.949219\n",
      "Train Epoch: 276 [45120/225000 (20%)] Loss: 17958.593750\n",
      "Train Epoch: 276 [47616/225000 (21%)] Loss: 17718.906250\n",
      "Train Epoch: 276 [50112/225000 (22%)] Loss: 17644.128906\n",
      "Train Epoch: 276 [52608/225000 (23%)] Loss: 17432.873047\n",
      "Train Epoch: 276 [55104/225000 (24%)] Loss: 17633.654297\n",
      "Train Epoch: 276 [57600/225000 (26%)] Loss: 17301.109375\n",
      "Train Epoch: 276 [60096/225000 (27%)] Loss: 17744.162109\n",
      "Train Epoch: 276 [62592/225000 (28%)] Loss: 17324.929688\n",
      "Train Epoch: 276 [65088/225000 (29%)] Loss: 17374.078125\n",
      "Train Epoch: 276 [67584/225000 (30%)] Loss: 17591.882812\n",
      "Train Epoch: 276 [70080/225000 (31%)] Loss: 17609.265625\n",
      "Train Epoch: 276 [72576/225000 (32%)] Loss: 17251.025391\n",
      "Train Epoch: 276 [75072/225000 (33%)] Loss: 17550.191406\n",
      "Train Epoch: 276 [77568/225000 (34%)] Loss: 17531.332031\n",
      "Train Epoch: 276 [80064/225000 (36%)] Loss: 17787.503906\n",
      "Train Epoch: 276 [82560/225000 (37%)] Loss: 17458.753906\n",
      "Train Epoch: 276 [85056/225000 (38%)] Loss: 17690.707031\n",
      "Train Epoch: 276 [87552/225000 (39%)] Loss: 17549.945312\n",
      "Train Epoch: 276 [90048/225000 (40%)] Loss: 17244.902344\n",
      "Train Epoch: 276 [92544/225000 (41%)] Loss: 17147.113281\n",
      "Train Epoch: 276 [95040/225000 (42%)] Loss: 17552.746094\n",
      "Train Epoch: 276 [97536/225000 (43%)] Loss: 17634.132812\n",
      "Train Epoch: 276 [100032/225000 (44%)] Loss: 17416.480469\n",
      "Train Epoch: 276 [102528/225000 (46%)] Loss: 17879.082031\n",
      "Train Epoch: 276 [105024/225000 (47%)] Loss: 17182.003906\n",
      "Train Epoch: 276 [107520/225000 (48%)] Loss: 17253.714844\n",
      "Train Epoch: 276 [110016/225000 (49%)] Loss: 17435.281250\n",
      "Train Epoch: 276 [112512/225000 (50%)] Loss: 17348.394531\n",
      "Train Epoch: 276 [115008/225000 (51%)] Loss: 17625.046875\n",
      "Train Epoch: 276 [117504/225000 (52%)] Loss: 17319.472656\n",
      "Train Epoch: 276 [120000/225000 (53%)] Loss: 17913.443359\n",
      "Train Epoch: 276 [122496/225000 (54%)] Loss: 17806.394531\n",
      "Train Epoch: 276 [124992/225000 (56%)] Loss: 17686.679688\n",
      "Train Epoch: 276 [127488/225000 (57%)] Loss: 17470.787109\n",
      "Train Epoch: 276 [129984/225000 (58%)] Loss: 17336.976562\n",
      "Train Epoch: 276 [132480/225000 (59%)] Loss: 17686.417969\n",
      "Train Epoch: 276 [134976/225000 (60%)] Loss: 17706.791016\n",
      "Train Epoch: 276 [137472/225000 (61%)] Loss: 18005.574219\n",
      "Train Epoch: 276 [139968/225000 (62%)] Loss: 17294.808594\n",
      "Train Epoch: 276 [142464/225000 (63%)] Loss: 17136.816406\n",
      "Train Epoch: 276 [144960/225000 (64%)] Loss: 17815.933594\n",
      "Train Epoch: 276 [147456/225000 (66%)] Loss: 17173.722656\n",
      "Train Epoch: 276 [149952/225000 (67%)] Loss: 17278.105469\n",
      "Train Epoch: 276 [152448/225000 (68%)] Loss: 17417.464844\n",
      "Train Epoch: 276 [154944/225000 (69%)] Loss: 17756.625000\n",
      "Train Epoch: 276 [157440/225000 (70%)] Loss: 17649.837891\n",
      "Train Epoch: 276 [159936/225000 (71%)] Loss: 17804.687500\n",
      "Train Epoch: 276 [162432/225000 (72%)] Loss: 17450.472656\n",
      "Train Epoch: 276 [164928/225000 (73%)] Loss: 16946.960938\n",
      "Train Epoch: 276 [167424/225000 (74%)] Loss: 17495.332031\n",
      "Train Epoch: 276 [169920/225000 (76%)] Loss: 17242.894531\n",
      "Train Epoch: 276 [172416/225000 (77%)] Loss: 17443.011719\n",
      "Train Epoch: 276 [174912/225000 (78%)] Loss: 17974.392578\n",
      "Train Epoch: 276 [177408/225000 (79%)] Loss: 17555.968750\n",
      "Train Epoch: 276 [179904/225000 (80%)] Loss: 17685.675781\n",
      "Train Epoch: 276 [182400/225000 (81%)] Loss: 17341.380859\n",
      "Train Epoch: 276 [184896/225000 (82%)] Loss: 18056.294922\n",
      "Train Epoch: 276 [187392/225000 (83%)] Loss: 17819.687500\n",
      "Train Epoch: 276 [189888/225000 (84%)] Loss: 17266.714844\n",
      "Train Epoch: 276 [192384/225000 (86%)] Loss: 17434.857422\n",
      "Train Epoch: 276 [194880/225000 (87%)] Loss: 17218.953125\n",
      "Train Epoch: 276 [197376/225000 (88%)] Loss: 17619.566406\n",
      "Train Epoch: 276 [199872/225000 (89%)] Loss: 17928.398438\n",
      "Train Epoch: 276 [202368/225000 (90%)] Loss: 17232.750000\n",
      "Train Epoch: 276 [204864/225000 (91%)] Loss: 17626.718750\n",
      "Train Epoch: 276 [207360/225000 (92%)] Loss: 17648.892578\n",
      "Train Epoch: 276 [209856/225000 (93%)] Loss: 17226.593750\n",
      "Train Epoch: 276 [212352/225000 (94%)] Loss: 17143.460938\n",
      "Train Epoch: 276 [214848/225000 (95%)] Loss: 17576.564453\n",
      "Train Epoch: 276 [217344/225000 (97%)] Loss: 17689.431641\n",
      "Train Epoch: 276 [219840/225000 (98%)] Loss: 17113.945312\n",
      "Train Epoch: 276 [222336/225000 (99%)] Loss: 17565.916016\n",
      "Train Epoch: 276 [224832/225000 (100%)] Loss: 17815.845703\n",
      "    epoch          : 276\n",
      "    loss           : 17560.83935130253\n",
      "    val_loss       : 17607.812103343374\n",
      "Train Epoch: 277 [192/225000 (0%)] Loss: 17614.269531\n",
      "Train Epoch: 277 [2688/225000 (1%)] Loss: 17722.507812\n",
      "Train Epoch: 277 [5184/225000 (2%)] Loss: 17721.341797\n",
      "Train Epoch: 277 [7680/225000 (3%)] Loss: 17869.304688\n",
      "Train Epoch: 277 [10176/225000 (5%)] Loss: 17524.867188\n",
      "Train Epoch: 277 [12672/225000 (6%)] Loss: 17538.556641\n",
      "Train Epoch: 277 [15168/225000 (7%)] Loss: 17508.904297\n",
      "Train Epoch: 277 [17664/225000 (8%)] Loss: 18031.013672\n",
      "Train Epoch: 277 [20160/225000 (9%)] Loss: 17486.164062\n",
      "Train Epoch: 277 [22656/225000 (10%)] Loss: 17127.353516\n",
      "Train Epoch: 277 [25152/225000 (11%)] Loss: 17963.818359\n",
      "Train Epoch: 277 [27648/225000 (12%)] Loss: 17622.332031\n",
      "Train Epoch: 277 [30144/225000 (13%)] Loss: 17420.773438\n",
      "Train Epoch: 277 [32640/225000 (15%)] Loss: 17820.214844\n",
      "Train Epoch: 277 [35136/225000 (16%)] Loss: 17315.589844\n",
      "Train Epoch: 277 [37632/225000 (17%)] Loss: 17157.992188\n",
      "Train Epoch: 277 [40128/225000 (18%)] Loss: 17432.927734\n",
      "Train Epoch: 277 [42624/225000 (19%)] Loss: 17090.457031\n",
      "Train Epoch: 277 [45120/225000 (20%)] Loss: 17699.730469\n",
      "Train Epoch: 277 [47616/225000 (21%)] Loss: 17941.449219\n",
      "Train Epoch: 277 [50112/225000 (22%)] Loss: 17372.480469\n",
      "Train Epoch: 277 [52608/225000 (23%)] Loss: 17465.460938\n",
      "Train Epoch: 277 [55104/225000 (24%)] Loss: 17458.941406\n",
      "Train Epoch: 277 [57600/225000 (26%)] Loss: 18328.105469\n",
      "Train Epoch: 277 [60096/225000 (27%)] Loss: 17817.419922\n",
      "Train Epoch: 277 [62592/225000 (28%)] Loss: 17436.347656\n",
      "Train Epoch: 277 [65088/225000 (29%)] Loss: 17702.386719\n",
      "Train Epoch: 277 [67584/225000 (30%)] Loss: 17279.699219\n",
      "Train Epoch: 277 [70080/225000 (31%)] Loss: 17807.177734\n",
      "Train Epoch: 277 [72576/225000 (32%)] Loss: 17736.972656\n",
      "Train Epoch: 277 [75072/225000 (33%)] Loss: 17933.957031\n",
      "Train Epoch: 277 [77568/225000 (34%)] Loss: 17719.242188\n",
      "Train Epoch: 277 [80064/225000 (36%)] Loss: 17498.750000\n",
      "Train Epoch: 277 [82560/225000 (37%)] Loss: 17027.013672\n",
      "Train Epoch: 277 [85056/225000 (38%)] Loss: 17443.070312\n",
      "Train Epoch: 277 [87552/225000 (39%)] Loss: 18118.648438\n",
      "Train Epoch: 277 [90048/225000 (40%)] Loss: 17442.636719\n",
      "Train Epoch: 277 [92544/225000 (41%)] Loss: 17323.320312\n",
      "Train Epoch: 277 [95040/225000 (42%)] Loss: 17191.820312\n",
      "Train Epoch: 277 [97536/225000 (43%)] Loss: 17659.724609\n",
      "Train Epoch: 277 [100032/225000 (44%)] Loss: 17871.648438\n",
      "Train Epoch: 277 [102528/225000 (46%)] Loss: 17600.794922\n",
      "Train Epoch: 277 [105024/225000 (47%)] Loss: 17743.291016\n",
      "Train Epoch: 277 [107520/225000 (48%)] Loss: 17579.707031\n",
      "Train Epoch: 277 [110016/225000 (49%)] Loss: 17224.257812\n",
      "Train Epoch: 277 [112512/225000 (50%)] Loss: 17511.828125\n",
      "Train Epoch: 277 [115008/225000 (51%)] Loss: 16977.199219\n",
      "Train Epoch: 277 [117504/225000 (52%)] Loss: 17424.019531\n",
      "Train Epoch: 277 [120000/225000 (53%)] Loss: 17399.621094\n",
      "Train Epoch: 277 [122496/225000 (54%)] Loss: 17803.351562\n",
      "Train Epoch: 277 [124992/225000 (56%)] Loss: 17588.437500\n",
      "Train Epoch: 277 [127488/225000 (57%)] Loss: 17798.535156\n",
      "Train Epoch: 277 [129984/225000 (58%)] Loss: 17268.107422\n",
      "Train Epoch: 277 [132480/225000 (59%)] Loss: 17462.437500\n",
      "Train Epoch: 277 [134976/225000 (60%)] Loss: 17358.791016\n",
      "Train Epoch: 277 [137472/225000 (61%)] Loss: 17412.955078\n",
      "Train Epoch: 277 [139968/225000 (62%)] Loss: 17717.238281\n",
      "Train Epoch: 277 [142464/225000 (63%)] Loss: 17645.820312\n",
      "Train Epoch: 277 [144960/225000 (64%)] Loss: 17250.457031\n",
      "Train Epoch: 277 [147456/225000 (66%)] Loss: 17134.052734\n",
      "Train Epoch: 277 [149952/225000 (67%)] Loss: 17319.917969\n",
      "Train Epoch: 277 [152448/225000 (68%)] Loss: 17584.355469\n",
      "Train Epoch: 277 [154944/225000 (69%)] Loss: 17541.572266\n",
      "Train Epoch: 277 [157440/225000 (70%)] Loss: 17644.775391\n",
      "Train Epoch: 277 [159936/225000 (71%)] Loss: 17340.792969\n",
      "Train Epoch: 277 [162432/225000 (72%)] Loss: 17287.933594\n",
      "Train Epoch: 277 [164928/225000 (73%)] Loss: 17784.820312\n",
      "Train Epoch: 277 [167424/225000 (74%)] Loss: 17447.113281\n",
      "Train Epoch: 277 [169920/225000 (76%)] Loss: 17170.769531\n",
      "Train Epoch: 277 [172416/225000 (77%)] Loss: 17346.464844\n",
      "Train Epoch: 277 [174912/225000 (78%)] Loss: 17401.414062\n",
      "Train Epoch: 277 [177408/225000 (79%)] Loss: 17507.884766\n",
      "Train Epoch: 277 [179904/225000 (80%)] Loss: 17832.890625\n",
      "Train Epoch: 277 [182400/225000 (81%)] Loss: 17069.353516\n",
      "Train Epoch: 277 [184896/225000 (82%)] Loss: 17233.332031\n",
      "Train Epoch: 277 [187392/225000 (83%)] Loss: 17645.417969\n",
      "Train Epoch: 277 [189888/225000 (84%)] Loss: 17521.328125\n",
      "Train Epoch: 277 [192384/225000 (86%)] Loss: 17348.109375\n",
      "Train Epoch: 277 [194880/225000 (87%)] Loss: 17978.048828\n",
      "Train Epoch: 277 [197376/225000 (88%)] Loss: 18080.392578\n",
      "Train Epoch: 277 [199872/225000 (89%)] Loss: 18291.929688\n",
      "Train Epoch: 277 [202368/225000 (90%)] Loss: 17439.984375\n",
      "Train Epoch: 277 [204864/225000 (91%)] Loss: 17763.505859\n",
      "Train Epoch: 277 [207360/225000 (92%)] Loss: 16546.910156\n",
      "Train Epoch: 277 [209856/225000 (93%)] Loss: 18226.492188\n",
      "Train Epoch: 277 [212352/225000 (94%)] Loss: 17253.417969\n",
      "Train Epoch: 277 [214848/225000 (95%)] Loss: 17917.046875\n",
      "Train Epoch: 277 [217344/225000 (97%)] Loss: 17989.125000\n",
      "Train Epoch: 277 [219840/225000 (98%)] Loss: 17402.812500\n",
      "Train Epoch: 277 [222336/225000 (99%)] Loss: 18024.871094\n",
      "Train Epoch: 277 [224832/225000 (100%)] Loss: 17259.718750\n",
      "    epoch          : 277\n",
      "    loss           : 17553.02080528077\n",
      "    val_loss       : 17530.649506952926\n",
      "Train Epoch: 278 [192/225000 (0%)] Loss: 17759.187500\n",
      "Train Epoch: 278 [2688/225000 (1%)] Loss: 17785.353516\n",
      "Train Epoch: 278 [5184/225000 (2%)] Loss: 17303.308594\n",
      "Train Epoch: 278 [7680/225000 (3%)] Loss: 17961.058594\n",
      "Train Epoch: 278 [10176/225000 (5%)] Loss: 17745.070312\n",
      "Train Epoch: 278 [12672/225000 (6%)] Loss: 17301.246094\n",
      "Train Epoch: 278 [15168/225000 (7%)] Loss: 17360.837891\n",
      "Train Epoch: 278 [17664/225000 (8%)] Loss: 17238.744141\n",
      "Train Epoch: 278 [20160/225000 (9%)] Loss: 17505.388672\n",
      "Train Epoch: 278 [22656/225000 (10%)] Loss: 17710.968750\n",
      "Train Epoch: 278 [25152/225000 (11%)] Loss: 17688.324219\n",
      "Train Epoch: 278 [27648/225000 (12%)] Loss: 17472.628906\n",
      "Train Epoch: 278 [30144/225000 (13%)] Loss: 17802.328125\n",
      "Train Epoch: 278 [32640/225000 (15%)] Loss: 17514.140625\n",
      "Train Epoch: 278 [35136/225000 (16%)] Loss: 17928.191406\n",
      "Train Epoch: 278 [37632/225000 (17%)] Loss: 18117.478516\n",
      "Train Epoch: 278 [40128/225000 (18%)] Loss: 17089.748047\n",
      "Train Epoch: 278 [42624/225000 (19%)] Loss: 17589.355469\n",
      "Train Epoch: 278 [45120/225000 (20%)] Loss: 17333.085938\n",
      "Train Epoch: 278 [47616/225000 (21%)] Loss: 17765.660156\n",
      "Train Epoch: 278 [50112/225000 (22%)] Loss: 17302.699219\n",
      "Train Epoch: 278 [52608/225000 (23%)] Loss: 17438.375000\n",
      "Train Epoch: 278 [55104/225000 (24%)] Loss: 17673.324219\n",
      "Train Epoch: 278 [57600/225000 (26%)] Loss: 17230.625000\n",
      "Train Epoch: 278 [60096/225000 (27%)] Loss: 17302.214844\n",
      "Train Epoch: 278 [62592/225000 (28%)] Loss: 17656.585938\n",
      "Train Epoch: 278 [65088/225000 (29%)] Loss: 17213.593750\n",
      "Train Epoch: 278 [67584/225000 (30%)] Loss: 17430.003906\n",
      "Train Epoch: 278 [70080/225000 (31%)] Loss: 17586.076172\n",
      "Train Epoch: 278 [72576/225000 (32%)] Loss: 17432.779297\n",
      "Train Epoch: 278 [75072/225000 (33%)] Loss: 17789.498047\n",
      "Train Epoch: 278 [77568/225000 (34%)] Loss: 17367.156250\n",
      "Train Epoch: 278 [80064/225000 (36%)] Loss: 17693.800781\n",
      "Train Epoch: 278 [82560/225000 (37%)] Loss: 17416.210938\n",
      "Train Epoch: 278 [85056/225000 (38%)] Loss: 17697.240234\n",
      "Train Epoch: 278 [87552/225000 (39%)] Loss: 17607.191406\n",
      "Train Epoch: 278 [90048/225000 (40%)] Loss: 17708.167969\n",
      "Train Epoch: 278 [92544/225000 (41%)] Loss: 17390.414062\n",
      "Train Epoch: 278 [95040/225000 (42%)] Loss: 18038.597656\n",
      "Train Epoch: 278 [97536/225000 (43%)] Loss: 17685.882812\n",
      "Train Epoch: 278 [100032/225000 (44%)] Loss: 17333.677734\n",
      "Train Epoch: 278 [102528/225000 (46%)] Loss: 17158.031250\n",
      "Train Epoch: 278 [105024/225000 (47%)] Loss: 17708.490234\n",
      "Train Epoch: 278 [107520/225000 (48%)] Loss: 17424.535156\n",
      "Train Epoch: 278 [110016/225000 (49%)] Loss: 17147.234375\n",
      "Train Epoch: 278 [112512/225000 (50%)] Loss: 17521.304688\n",
      "Train Epoch: 278 [115008/225000 (51%)] Loss: 17693.941406\n",
      "Train Epoch: 278 [117504/225000 (52%)] Loss: 17716.279297\n",
      "Train Epoch: 278 [120000/225000 (53%)] Loss: 17657.898438\n",
      "Train Epoch: 278 [122496/225000 (54%)] Loss: 17254.085938\n",
      "Train Epoch: 278 [124992/225000 (56%)] Loss: 17446.753906\n",
      "Train Epoch: 278 [127488/225000 (57%)] Loss: 17730.919922\n",
      "Train Epoch: 278 [129984/225000 (58%)] Loss: 17739.296875\n",
      "Train Epoch: 278 [132480/225000 (59%)] Loss: 17732.677734\n",
      "Train Epoch: 278 [134976/225000 (60%)] Loss: 17340.128906\n",
      "Train Epoch: 278 [137472/225000 (61%)] Loss: 17310.050781\n",
      "Train Epoch: 278 [139968/225000 (62%)] Loss: 17541.257812\n",
      "Train Epoch: 278 [142464/225000 (63%)] Loss: 17465.742188\n",
      "Train Epoch: 278 [144960/225000 (64%)] Loss: 17837.917969\n",
      "Train Epoch: 278 [147456/225000 (66%)] Loss: 17503.152344\n",
      "Train Epoch: 278 [149952/225000 (67%)] Loss: 17890.400391\n",
      "Train Epoch: 278 [152448/225000 (68%)] Loss: 17708.353516\n",
      "Train Epoch: 278 [154944/225000 (69%)] Loss: 17848.158203\n",
      "Train Epoch: 278 [157440/225000 (70%)] Loss: 17791.828125\n",
      "Train Epoch: 278 [159936/225000 (71%)] Loss: 17370.222656\n",
      "Train Epoch: 278 [162432/225000 (72%)] Loss: 17218.324219\n",
      "Train Epoch: 278 [164928/225000 (73%)] Loss: 17419.968750\n",
      "Train Epoch: 278 [167424/225000 (74%)] Loss: 17599.695312\n",
      "Train Epoch: 278 [169920/225000 (76%)] Loss: 17458.630859\n",
      "Train Epoch: 278 [172416/225000 (77%)] Loss: 17301.351562\n",
      "Train Epoch: 278 [174912/225000 (78%)] Loss: 17314.597656\n",
      "Train Epoch: 278 [177408/225000 (79%)] Loss: 17318.250000\n",
      "Train Epoch: 278 [179904/225000 (80%)] Loss: 17243.378906\n",
      "Train Epoch: 278 [182400/225000 (81%)] Loss: 17481.201172\n",
      "Train Epoch: 278 [184896/225000 (82%)] Loss: 17271.691406\n",
      "Train Epoch: 278 [187392/225000 (83%)] Loss: 17227.363281\n",
      "Train Epoch: 278 [189888/225000 (84%)] Loss: 17735.929688\n",
      "Train Epoch: 278 [192384/225000 (86%)] Loss: 17736.417969\n",
      "Train Epoch: 278 [194880/225000 (87%)] Loss: 17505.910156\n",
      "Train Epoch: 278 [197376/225000 (88%)] Loss: 16971.880859\n",
      "Train Epoch: 278 [199872/225000 (89%)] Loss: 17785.535156\n",
      "Train Epoch: 278 [202368/225000 (90%)] Loss: 17643.267578\n",
      "Train Epoch: 278 [204864/225000 (91%)] Loss: 17666.941406\n",
      "Train Epoch: 278 [207360/225000 (92%)] Loss: 17502.953125\n",
      "Train Epoch: 278 [209856/225000 (93%)] Loss: 17589.947266\n",
      "Train Epoch: 278 [212352/225000 (94%)] Loss: 17456.062500\n",
      "Train Epoch: 278 [214848/225000 (95%)] Loss: 17873.378906\n",
      "Train Epoch: 278 [217344/225000 (97%)] Loss: 17311.382812\n",
      "Train Epoch: 278 [219840/225000 (98%)] Loss: 17368.867188\n",
      "Train Epoch: 278 [222336/225000 (99%)] Loss: 17508.089844\n",
      "Train Epoch: 278 [224832/225000 (100%)] Loss: 16934.285156\n",
      "    epoch          : 278\n",
      "    loss           : 17535.068813493228\n",
      "    val_loss       : 17448.782360286204\n",
      "Train Epoch: 279 [192/225000 (0%)] Loss: 17565.833984\n",
      "Train Epoch: 279 [2688/225000 (1%)] Loss: 17180.566406\n",
      "Train Epoch: 279 [5184/225000 (2%)] Loss: 17271.843750\n",
      "Train Epoch: 279 [7680/225000 (3%)] Loss: 17292.656250\n",
      "Train Epoch: 279 [10176/225000 (5%)] Loss: 17958.605469\n",
      "Train Epoch: 279 [12672/225000 (6%)] Loss: 17309.472656\n",
      "Train Epoch: 279 [15168/225000 (7%)] Loss: 17341.283203\n",
      "Train Epoch: 279 [17664/225000 (8%)] Loss: 18193.195312\n",
      "Train Epoch: 279 [20160/225000 (9%)] Loss: 17083.585938\n",
      "Train Epoch: 279 [22656/225000 (10%)] Loss: 17532.828125\n",
      "Train Epoch: 279 [25152/225000 (11%)] Loss: 17831.232422\n",
      "Train Epoch: 279 [27648/225000 (12%)] Loss: 17600.789062\n",
      "Train Epoch: 279 [30144/225000 (13%)] Loss: 17880.808594\n",
      "Train Epoch: 279 [32640/225000 (15%)] Loss: 17316.441406\n",
      "Train Epoch: 279 [35136/225000 (16%)] Loss: 17514.125000\n",
      "Train Epoch: 279 [37632/225000 (17%)] Loss: 17704.097656\n",
      "Train Epoch: 279 [40128/225000 (18%)] Loss: 17558.339844\n",
      "Train Epoch: 279 [42624/225000 (19%)] Loss: 17674.117188\n",
      "Train Epoch: 279 [45120/225000 (20%)] Loss: 16898.503906\n",
      "Train Epoch: 279 [47616/225000 (21%)] Loss: 17902.199219\n",
      "Train Epoch: 279 [50112/225000 (22%)] Loss: 17511.751953\n",
      "Train Epoch: 279 [52608/225000 (23%)] Loss: 17062.914062\n",
      "Train Epoch: 279 [55104/225000 (24%)] Loss: 17210.613281\n",
      "Train Epoch: 279 [57600/225000 (26%)] Loss: 17121.457031\n",
      "Train Epoch: 279 [60096/225000 (27%)] Loss: 17221.806641\n",
      "Train Epoch: 279 [62592/225000 (28%)] Loss: 17391.832031\n",
      "Train Epoch: 279 [65088/225000 (29%)] Loss: 17441.048828\n",
      "Train Epoch: 279 [67584/225000 (30%)] Loss: 18028.636719\n",
      "Train Epoch: 279 [70080/225000 (31%)] Loss: 17268.033203\n",
      "Train Epoch: 279 [72576/225000 (32%)] Loss: 18012.384766\n",
      "Train Epoch: 279 [75072/225000 (33%)] Loss: 17453.242188\n",
      "Train Epoch: 279 [77568/225000 (34%)] Loss: 17513.519531\n",
      "Train Epoch: 279 [80064/225000 (36%)] Loss: 17345.222656\n",
      "Train Epoch: 279 [82560/225000 (37%)] Loss: 17830.494141\n",
      "Train Epoch: 279 [85056/225000 (38%)] Loss: 18191.808594\n",
      "Train Epoch: 279 [87552/225000 (39%)] Loss: 16906.175781\n",
      "Train Epoch: 279 [90048/225000 (40%)] Loss: 17779.562500\n",
      "Train Epoch: 279 [92544/225000 (41%)] Loss: 17876.142578\n",
      "Train Epoch: 279 [95040/225000 (42%)] Loss: 17837.398438\n",
      "Train Epoch: 279 [97536/225000 (43%)] Loss: 17485.144531\n",
      "Train Epoch: 279 [100032/225000 (44%)] Loss: 17611.123047\n",
      "Train Epoch: 279 [102528/225000 (46%)] Loss: 17533.136719\n",
      "Train Epoch: 279 [105024/225000 (47%)] Loss: 17544.539062\n",
      "Train Epoch: 279 [107520/225000 (48%)] Loss: 17740.148438\n",
      "Train Epoch: 279 [110016/225000 (49%)] Loss: 17799.171875\n",
      "Train Epoch: 279 [112512/225000 (50%)] Loss: 17373.396484\n",
      "Train Epoch: 279 [115008/225000 (51%)] Loss: 17267.062500\n",
      "Train Epoch: 279 [117504/225000 (52%)] Loss: 17248.304688\n",
      "Train Epoch: 279 [120000/225000 (53%)] Loss: 17654.509766\n",
      "Train Epoch: 279 [122496/225000 (54%)] Loss: 17659.886719\n",
      "Train Epoch: 279 [124992/225000 (56%)] Loss: 18087.117188\n",
      "Train Epoch: 279 [127488/225000 (57%)] Loss: 17716.074219\n",
      "Train Epoch: 279 [129984/225000 (58%)] Loss: 17923.833984\n",
      "Train Epoch: 279 [132480/225000 (59%)] Loss: 17561.394531\n",
      "Train Epoch: 279 [134976/225000 (60%)] Loss: 17535.744141\n",
      "Train Epoch: 279 [137472/225000 (61%)] Loss: 16834.152344\n",
      "Train Epoch: 279 [139968/225000 (62%)] Loss: 17432.964844\n",
      "Train Epoch: 279 [142464/225000 (63%)] Loss: 17635.382812\n",
      "Train Epoch: 279 [144960/225000 (64%)] Loss: 17656.808594\n",
      "Train Epoch: 279 [147456/225000 (66%)] Loss: 17163.644531\n",
      "Train Epoch: 279 [149952/225000 (67%)] Loss: 17799.328125\n",
      "Train Epoch: 279 [152448/225000 (68%)] Loss: 17444.906250\n",
      "Train Epoch: 279 [154944/225000 (69%)] Loss: 17185.830078\n",
      "Train Epoch: 279 [157440/225000 (70%)] Loss: 17269.218750\n",
      "Train Epoch: 279 [159936/225000 (71%)] Loss: 17131.042969\n",
      "Train Epoch: 279 [162432/225000 (72%)] Loss: 17604.832031\n",
      "Train Epoch: 279 [164928/225000 (73%)] Loss: 17598.718750\n",
      "Train Epoch: 279 [167424/225000 (74%)] Loss: 17626.316406\n",
      "Train Epoch: 279 [169920/225000 (76%)] Loss: 17230.753906\n",
      "Train Epoch: 279 [172416/225000 (77%)] Loss: 17998.587891\n",
      "Train Epoch: 279 [174912/225000 (78%)] Loss: 17107.761719\n",
      "Train Epoch: 279 [177408/225000 (79%)] Loss: 17626.935547\n",
      "Train Epoch: 279 [179904/225000 (80%)] Loss: 17433.050781\n",
      "Train Epoch: 279 [182400/225000 (81%)] Loss: 17398.246094\n",
      "Train Epoch: 279 [184896/225000 (82%)] Loss: 17444.578125\n",
      "Train Epoch: 279 [187392/225000 (83%)] Loss: 17308.171875\n",
      "Train Epoch: 279 [189888/225000 (84%)] Loss: 17872.250000\n",
      "Train Epoch: 279 [192384/225000 (86%)] Loss: 17542.152344\n",
      "Train Epoch: 279 [194880/225000 (87%)] Loss: 18234.708984\n",
      "Train Epoch: 279 [197376/225000 (88%)] Loss: 17450.582031\n",
      "Train Epoch: 279 [199872/225000 (89%)] Loss: 17515.539062\n",
      "Train Epoch: 279 [202368/225000 (90%)] Loss: 17472.882812\n",
      "Train Epoch: 279 [204864/225000 (91%)] Loss: 17030.498047\n",
      "Train Epoch: 279 [207360/225000 (92%)] Loss: 17523.339844\n",
      "Train Epoch: 279 [209856/225000 (93%)] Loss: 17814.082031\n",
      "Train Epoch: 279 [212352/225000 (94%)] Loss: 17760.523438\n",
      "Train Epoch: 279 [214848/225000 (95%)] Loss: 17800.654297\n",
      "Train Epoch: 279 [217344/225000 (97%)] Loss: 17211.183594\n",
      "Train Epoch: 279 [219840/225000 (98%)] Loss: 17747.937500\n",
      "Train Epoch: 279 [222336/225000 (99%)] Loss: 17122.671875\n",
      "Train Epoch: 279 [224832/225000 (100%)] Loss: 18326.460938\n",
      "    epoch          : 279\n",
      "    loss           : 17532.98211690753\n",
      "    val_loss       : 17527.658526416497\n",
      "Train Epoch: 280 [192/225000 (0%)] Loss: 17715.722656\n",
      "Train Epoch: 280 [2688/225000 (1%)] Loss: 17651.726562\n",
      "Train Epoch: 280 [5184/225000 (2%)] Loss: 17783.640625\n",
      "Train Epoch: 280 [7680/225000 (3%)] Loss: 17357.726562\n",
      "Train Epoch: 280 [10176/225000 (5%)] Loss: 17384.761719\n",
      "Train Epoch: 280 [12672/225000 (6%)] Loss: 17607.589844\n",
      "Train Epoch: 280 [15168/225000 (7%)] Loss: 17700.597656\n",
      "Train Epoch: 280 [17664/225000 (8%)] Loss: 17685.953125\n",
      "Train Epoch: 280 [20160/225000 (9%)] Loss: 17387.574219\n",
      "Train Epoch: 280 [22656/225000 (10%)] Loss: 17639.421875\n",
      "Train Epoch: 280 [25152/225000 (11%)] Loss: 17198.244141\n",
      "Train Epoch: 280 [27648/225000 (12%)] Loss: 17849.816406\n",
      "Train Epoch: 280 [30144/225000 (13%)] Loss: 17165.113281\n",
      "Train Epoch: 280 [32640/225000 (15%)] Loss: 17461.523438\n",
      "Train Epoch: 280 [35136/225000 (16%)] Loss: 17877.367188\n",
      "Train Epoch: 280 [37632/225000 (17%)] Loss: 17388.253906\n",
      "Train Epoch: 280 [40128/225000 (18%)] Loss: 17252.638672\n",
      "Train Epoch: 280 [42624/225000 (19%)] Loss: 17652.210938\n",
      "Train Epoch: 280 [45120/225000 (20%)] Loss: 17705.039062\n",
      "Train Epoch: 280 [47616/225000 (21%)] Loss: 17708.738281\n",
      "Train Epoch: 280 [50112/225000 (22%)] Loss: 17481.964844\n",
      "Train Epoch: 280 [52608/225000 (23%)] Loss: 17825.843750\n",
      "Train Epoch: 280 [55104/225000 (24%)] Loss: 17251.144531\n",
      "Train Epoch: 280 [57600/225000 (26%)] Loss: 17503.236328\n",
      "Train Epoch: 280 [60096/225000 (27%)] Loss: 17525.664062\n",
      "Train Epoch: 280 [62592/225000 (28%)] Loss: 17704.800781\n",
      "Train Epoch: 280 [65088/225000 (29%)] Loss: 17391.447266\n",
      "Train Epoch: 280 [67584/225000 (30%)] Loss: 17584.628906\n",
      "Train Epoch: 280 [70080/225000 (31%)] Loss: 17879.042969\n",
      "Train Epoch: 280 [72576/225000 (32%)] Loss: 17854.234375\n",
      "Train Epoch: 280 [75072/225000 (33%)] Loss: 17815.429688\n",
      "Train Epoch: 280 [77568/225000 (34%)] Loss: 17431.640625\n",
      "Train Epoch: 280 [80064/225000 (36%)] Loss: 17572.000000\n",
      "Train Epoch: 280 [82560/225000 (37%)] Loss: 17755.597656\n",
      "Train Epoch: 280 [85056/225000 (38%)] Loss: 17204.195312\n",
      "Train Epoch: 280 [87552/225000 (39%)] Loss: 17182.091797\n",
      "Train Epoch: 280 [90048/225000 (40%)] Loss: 17897.591797\n",
      "Train Epoch: 280 [92544/225000 (41%)] Loss: 17908.140625\n",
      "Train Epoch: 280 [95040/225000 (42%)] Loss: 17662.621094\n",
      "Train Epoch: 280 [97536/225000 (43%)] Loss: 17521.667969\n",
      "Train Epoch: 280 [100032/225000 (44%)] Loss: 17997.529297\n",
      "Train Epoch: 280 [102528/225000 (46%)] Loss: 17318.042969\n",
      "Train Epoch: 280 [105024/225000 (47%)] Loss: 17483.603516\n",
      "Train Epoch: 280 [107520/225000 (48%)] Loss: 17291.488281\n",
      "Train Epoch: 280 [110016/225000 (49%)] Loss: 17527.923828\n",
      "Train Epoch: 280 [112512/225000 (50%)] Loss: 17426.269531\n",
      "Train Epoch: 280 [115008/225000 (51%)] Loss: 17678.230469\n",
      "Train Epoch: 280 [117504/225000 (52%)] Loss: 17117.796875\n",
      "Train Epoch: 280 [120000/225000 (53%)] Loss: 17672.460938\n",
      "Train Epoch: 280 [122496/225000 (54%)] Loss: 17373.886719\n",
      "Train Epoch: 280 [124992/225000 (56%)] Loss: 17159.964844\n",
      "Train Epoch: 280 [127488/225000 (57%)] Loss: 17630.246094\n",
      "Train Epoch: 280 [129984/225000 (58%)] Loss: 17689.814453\n",
      "Train Epoch: 280 [132480/225000 (59%)] Loss: 17548.125000\n",
      "Train Epoch: 280 [134976/225000 (60%)] Loss: 17403.310547\n",
      "Train Epoch: 280 [137472/225000 (61%)] Loss: 17217.726562\n",
      "Train Epoch: 280 [139968/225000 (62%)] Loss: 17397.746094\n",
      "Train Epoch: 280 [142464/225000 (63%)] Loss: 17691.917969\n",
      "Train Epoch: 280 [144960/225000 (64%)] Loss: 17610.230469\n",
      "Train Epoch: 280 [147456/225000 (66%)] Loss: 16990.572266\n",
      "Train Epoch: 280 [149952/225000 (67%)] Loss: 18312.273438\n",
      "Train Epoch: 280 [152448/225000 (68%)] Loss: 17363.814453\n",
      "Train Epoch: 280 [154944/225000 (69%)] Loss: 17305.593750\n",
      "Train Epoch: 280 [157440/225000 (70%)] Loss: 17152.673828\n",
      "Train Epoch: 280 [159936/225000 (71%)] Loss: 17511.992188\n",
      "Train Epoch: 280 [162432/225000 (72%)] Loss: 17159.457031\n",
      "Train Epoch: 280 [164928/225000 (73%)] Loss: 17464.498047\n",
      "Train Epoch: 280 [167424/225000 (74%)] Loss: 17630.767578\n",
      "Train Epoch: 280 [169920/225000 (76%)] Loss: 17960.421875\n",
      "Train Epoch: 280 [172416/225000 (77%)] Loss: 17667.619141\n",
      "Train Epoch: 280 [174912/225000 (78%)] Loss: 17111.718750\n",
      "Train Epoch: 280 [177408/225000 (79%)] Loss: 17637.285156\n",
      "Train Epoch: 280 [179904/225000 (80%)] Loss: 17401.511719\n",
      "Train Epoch: 280 [182400/225000 (81%)] Loss: 17174.976562\n",
      "Train Epoch: 280 [184896/225000 (82%)] Loss: 17538.796875\n",
      "Train Epoch: 280 [187392/225000 (83%)] Loss: 17498.019531\n",
      "Train Epoch: 280 [189888/225000 (84%)] Loss: 17561.027344\n",
      "Train Epoch: 280 [192384/225000 (86%)] Loss: 17464.359375\n",
      "Train Epoch: 280 [194880/225000 (87%)] Loss: 17261.750000\n",
      "Train Epoch: 280 [197376/225000 (88%)] Loss: 17748.542969\n",
      "Train Epoch: 280 [199872/225000 (89%)] Loss: 17534.371094\n",
      "Train Epoch: 280 [202368/225000 (90%)] Loss: 17790.363281\n",
      "Train Epoch: 280 [204864/225000 (91%)] Loss: 17607.890625\n",
      "Train Epoch: 280 [207360/225000 (92%)] Loss: 17556.125000\n",
      "Train Epoch: 280 [209856/225000 (93%)] Loss: 17871.371094\n",
      "Train Epoch: 280 [212352/225000 (94%)] Loss: 17811.621094\n",
      "Train Epoch: 280 [214848/225000 (95%)] Loss: 17244.121094\n",
      "Train Epoch: 280 [217344/225000 (97%)] Loss: 17987.855469\n",
      "Train Epoch: 280 [219840/225000 (98%)] Loss: 17409.621094\n",
      "Train Epoch: 280 [222336/225000 (99%)] Loss: 17298.861328\n",
      "Train Epoch: 280 [224832/225000 (100%)] Loss: 17907.441406\n",
      "    epoch          : 280\n",
      "    loss           : 17518.562802467735\n",
      "    val_loss       : 17444.723647133993\n",
      "Train Epoch: 281 [192/225000 (0%)] Loss: 17496.273438\n",
      "Train Epoch: 281 [2688/225000 (1%)] Loss: 17543.710938\n",
      "Train Epoch: 281 [5184/225000 (2%)] Loss: 17427.648438\n",
      "Train Epoch: 281 [7680/225000 (3%)] Loss: 17515.308594\n",
      "Train Epoch: 281 [10176/225000 (5%)] Loss: 17477.597656\n",
      "Train Epoch: 281 [12672/225000 (6%)] Loss: 17324.814453\n",
      "Train Epoch: 281 [15168/225000 (7%)] Loss: 17530.015625\n",
      "Train Epoch: 281 [17664/225000 (8%)] Loss: 17471.318359\n",
      "Train Epoch: 281 [20160/225000 (9%)] Loss: 17549.394531\n",
      "Train Epoch: 281 [22656/225000 (10%)] Loss: 17252.714844\n",
      "Train Epoch: 281 [25152/225000 (11%)] Loss: 17501.224609\n",
      "Train Epoch: 281 [27648/225000 (12%)] Loss: 17314.638672\n",
      "Train Epoch: 281 [30144/225000 (13%)] Loss: 17329.484375\n",
      "Train Epoch: 281 [32640/225000 (15%)] Loss: 17507.628906\n",
      "Train Epoch: 281 [35136/225000 (16%)] Loss: 17153.589844\n",
      "Train Epoch: 281 [37632/225000 (17%)] Loss: 17423.007812\n",
      "Train Epoch: 281 [40128/225000 (18%)] Loss: 17863.150391\n",
      "Train Epoch: 281 [42624/225000 (19%)] Loss: 17488.105469\n",
      "Train Epoch: 281 [45120/225000 (20%)] Loss: 17927.853516\n",
      "Train Epoch: 281 [47616/225000 (21%)] Loss: 17247.908203\n",
      "Train Epoch: 281 [50112/225000 (22%)] Loss: 17291.031250\n",
      "Train Epoch: 281 [52608/225000 (23%)] Loss: 17288.605469\n",
      "Train Epoch: 281 [55104/225000 (24%)] Loss: 17578.136719\n",
      "Train Epoch: 281 [57600/225000 (26%)] Loss: 17438.687500\n",
      "Train Epoch: 281 [60096/225000 (27%)] Loss: 17712.281250\n",
      "Train Epoch: 281 [62592/225000 (28%)] Loss: 17952.306641\n",
      "Train Epoch: 281 [65088/225000 (29%)] Loss: 17386.074219\n",
      "Train Epoch: 281 [67584/225000 (30%)] Loss: 17441.621094\n",
      "Train Epoch: 281 [70080/225000 (31%)] Loss: 17407.417969\n",
      "Train Epoch: 281 [72576/225000 (32%)] Loss: 17088.515625\n",
      "Train Epoch: 281 [75072/225000 (33%)] Loss: 17846.050781\n",
      "Train Epoch: 281 [77568/225000 (34%)] Loss: 17325.796875\n",
      "Train Epoch: 281 [80064/225000 (36%)] Loss: 16858.734375\n",
      "Train Epoch: 281 [82560/225000 (37%)] Loss: 17640.199219\n",
      "Train Epoch: 281 [85056/225000 (38%)] Loss: 17307.519531\n",
      "Train Epoch: 281 [87552/225000 (39%)] Loss: 17098.451172\n",
      "Train Epoch: 281 [90048/225000 (40%)] Loss: 17598.210938\n",
      "Train Epoch: 281 [92544/225000 (41%)] Loss: 17521.875000\n",
      "Train Epoch: 281 [95040/225000 (42%)] Loss: 17832.628906\n",
      "Train Epoch: 281 [97536/225000 (43%)] Loss: 17724.021484\n",
      "Train Epoch: 281 [100032/225000 (44%)] Loss: 17451.507812\n",
      "Train Epoch: 281 [102528/225000 (46%)] Loss: 17923.968750\n",
      "Train Epoch: 281 [105024/225000 (47%)] Loss: 17436.753906\n",
      "Train Epoch: 281 [107520/225000 (48%)] Loss: 17499.585938\n",
      "Train Epoch: 281 [110016/225000 (49%)] Loss: 17878.876953\n",
      "Train Epoch: 281 [112512/225000 (50%)] Loss: 17268.800781\n",
      "Train Epoch: 281 [115008/225000 (51%)] Loss: 17764.128906\n",
      "Train Epoch: 281 [117504/225000 (52%)] Loss: 18211.597656\n",
      "Train Epoch: 281 [120000/225000 (53%)] Loss: 17658.408203\n",
      "Train Epoch: 281 [122496/225000 (54%)] Loss: 17492.328125\n",
      "Train Epoch: 281 [124992/225000 (56%)] Loss: 17793.054688\n",
      "Train Epoch: 281 [127488/225000 (57%)] Loss: 17693.007812\n",
      "Train Epoch: 281 [129984/225000 (58%)] Loss: 17724.171875\n",
      "Train Epoch: 281 [132480/225000 (59%)] Loss: 17402.603516\n",
      "Train Epoch: 281 [134976/225000 (60%)] Loss: 17229.140625\n",
      "Train Epoch: 281 [137472/225000 (61%)] Loss: 17487.031250\n",
      "Train Epoch: 281 [139968/225000 (62%)] Loss: 17776.658203\n",
      "Train Epoch: 281 [142464/225000 (63%)] Loss: 18007.179688\n",
      "Train Epoch: 281 [144960/225000 (64%)] Loss: 17384.595703\n",
      "Train Epoch: 281 [147456/225000 (66%)] Loss: 17904.183594\n",
      "Train Epoch: 281 [149952/225000 (67%)] Loss: 17188.871094\n",
      "Train Epoch: 281 [152448/225000 (68%)] Loss: 17413.048828\n",
      "Train Epoch: 281 [154944/225000 (69%)] Loss: 17267.669922\n",
      "Train Epoch: 281 [157440/225000 (70%)] Loss: 17292.546875\n",
      "Train Epoch: 281 [159936/225000 (71%)] Loss: 17331.677734\n",
      "Train Epoch: 281 [162432/225000 (72%)] Loss: 17275.191406\n",
      "Train Epoch: 281 [164928/225000 (73%)] Loss: 18061.388672\n",
      "Train Epoch: 281 [167424/225000 (74%)] Loss: 17700.089844\n",
      "Train Epoch: 281 [169920/225000 (76%)] Loss: 17257.058594\n",
      "Train Epoch: 281 [172416/225000 (77%)] Loss: 17179.927734\n",
      "Train Epoch: 281 [174912/225000 (78%)] Loss: 17384.804688\n",
      "Train Epoch: 281 [177408/225000 (79%)] Loss: 17223.316406\n",
      "Train Epoch: 281 [179904/225000 (80%)] Loss: 17638.160156\n",
      "Train Epoch: 281 [182400/225000 (81%)] Loss: 17812.871094\n",
      "Train Epoch: 281 [184896/225000 (82%)] Loss: 17453.613281\n",
      "Train Epoch: 281 [187392/225000 (83%)] Loss: 17877.847656\n",
      "Train Epoch: 281 [189888/225000 (84%)] Loss: 17882.443359\n",
      "Train Epoch: 281 [192384/225000 (86%)] Loss: 17905.433594\n",
      "Train Epoch: 281 [194880/225000 (87%)] Loss: 18129.896484\n",
      "Train Epoch: 281 [197376/225000 (88%)] Loss: 17534.824219\n",
      "Train Epoch: 281 [199872/225000 (89%)] Loss: 17497.957031\n",
      "Train Epoch: 281 [202368/225000 (90%)] Loss: 17440.324219\n",
      "Train Epoch: 281 [204864/225000 (91%)] Loss: 17602.814453\n",
      "Train Epoch: 281 [207360/225000 (92%)] Loss: 17574.402344\n",
      "Train Epoch: 281 [209856/225000 (93%)] Loss: 17318.546875\n",
      "Train Epoch: 281 [212352/225000 (94%)] Loss: 17408.453125\n",
      "Train Epoch: 281 [214848/225000 (95%)] Loss: 18005.671875\n",
      "Train Epoch: 281 [217344/225000 (97%)] Loss: 17541.175781\n",
      "Train Epoch: 281 [219840/225000 (98%)] Loss: 17234.039062\n",
      "Train Epoch: 281 [222336/225000 (99%)] Loss: 17779.140625\n",
      "Train Epoch: 281 [224832/225000 (100%)] Loss: 17349.421875\n",
      "    epoch          : 281\n",
      "    loss           : 17523.559287009386\n",
      "    val_loss       : 17479.91621489925\n",
      "Train Epoch: 282 [192/225000 (0%)] Loss: 17812.683594\n",
      "Train Epoch: 282 [2688/225000 (1%)] Loss: 17600.921875\n",
      "Train Epoch: 282 [5184/225000 (2%)] Loss: 17646.453125\n",
      "Train Epoch: 282 [7680/225000 (3%)] Loss: 17227.048828\n",
      "Train Epoch: 282 [10176/225000 (5%)] Loss: 17392.070312\n",
      "Train Epoch: 282 [12672/225000 (6%)] Loss: 17534.330078\n",
      "Train Epoch: 282 [15168/225000 (7%)] Loss: 17686.589844\n",
      "Train Epoch: 282 [17664/225000 (8%)] Loss: 17171.103516\n",
      "Train Epoch: 282 [20160/225000 (9%)] Loss: 17429.427734\n",
      "Train Epoch: 282 [22656/225000 (10%)] Loss: 17813.097656\n",
      "Train Epoch: 282 [25152/225000 (11%)] Loss: 17664.453125\n",
      "Train Epoch: 282 [27648/225000 (12%)] Loss: 17107.400391\n",
      "Train Epoch: 282 [30144/225000 (13%)] Loss: 17767.277344\n",
      "Train Epoch: 282 [32640/225000 (15%)] Loss: 18048.017578\n",
      "Train Epoch: 282 [35136/225000 (16%)] Loss: 17489.121094\n",
      "Train Epoch: 282 [37632/225000 (17%)] Loss: 17556.000000\n",
      "Train Epoch: 282 [40128/225000 (18%)] Loss: 17187.976562\n",
      "Train Epoch: 282 [42624/225000 (19%)] Loss: 17300.898438\n",
      "Train Epoch: 282 [45120/225000 (20%)] Loss: 17771.068359\n",
      "Train Epoch: 282 [47616/225000 (21%)] Loss: 16823.687500\n",
      "Train Epoch: 282 [50112/225000 (22%)] Loss: 17531.123047\n",
      "Train Epoch: 282 [52608/225000 (23%)] Loss: 16721.058594\n",
      "Train Epoch: 282 [55104/225000 (24%)] Loss: 17314.917969\n",
      "Train Epoch: 282 [57600/225000 (26%)] Loss: 17390.273438\n",
      "Train Epoch: 282 [60096/225000 (27%)] Loss: 17423.562500\n",
      "Train Epoch: 282 [62592/225000 (28%)] Loss: 17563.441406\n",
      "Train Epoch: 282 [65088/225000 (29%)] Loss: 17880.050781\n",
      "Train Epoch: 282 [67584/225000 (30%)] Loss: 17481.857422\n",
      "Train Epoch: 282 [70080/225000 (31%)] Loss: 17298.804688\n",
      "Train Epoch: 282 [72576/225000 (32%)] Loss: 17612.091797\n",
      "Train Epoch: 282 [75072/225000 (33%)] Loss: 17167.351562\n",
      "Train Epoch: 282 [77568/225000 (34%)] Loss: 17065.164062\n",
      "Train Epoch: 282 [80064/225000 (36%)] Loss: 17332.029297\n",
      "Train Epoch: 282 [82560/225000 (37%)] Loss: 17735.488281\n",
      "Train Epoch: 282 [85056/225000 (38%)] Loss: 17058.470703\n",
      "Train Epoch: 282 [87552/225000 (39%)] Loss: 17703.738281\n",
      "Train Epoch: 282 [90048/225000 (40%)] Loss: 17872.492188\n",
      "Train Epoch: 282 [92544/225000 (41%)] Loss: 16891.775391\n",
      "Train Epoch: 282 [95040/225000 (42%)] Loss: 17906.007812\n",
      "Train Epoch: 282 [97536/225000 (43%)] Loss: 17451.113281\n",
      "Train Epoch: 282 [100032/225000 (44%)] Loss: 17561.078125\n",
      "Train Epoch: 282 [102528/225000 (46%)] Loss: 17365.007812\n",
      "Train Epoch: 282 [105024/225000 (47%)] Loss: 17424.339844\n",
      "Train Epoch: 282 [107520/225000 (48%)] Loss: 17290.617188\n",
      "Train Epoch: 282 [110016/225000 (49%)] Loss: 17479.267578\n",
      "Train Epoch: 282 [112512/225000 (50%)] Loss: 17370.791016\n",
      "Train Epoch: 282 [115008/225000 (51%)] Loss: 17283.113281\n",
      "Train Epoch: 282 [117504/225000 (52%)] Loss: 17317.582031\n",
      "Train Epoch: 282 [120000/225000 (53%)] Loss: 17645.830078\n",
      "Train Epoch: 282 [122496/225000 (54%)] Loss: 17267.621094\n",
      "Train Epoch: 282 [124992/225000 (56%)] Loss: 17343.378906\n",
      "Train Epoch: 282 [127488/225000 (57%)] Loss: 17373.617188\n",
      "Train Epoch: 282 [129984/225000 (58%)] Loss: 17381.773438\n",
      "Train Epoch: 282 [132480/225000 (59%)] Loss: 17369.718750\n",
      "Train Epoch: 282 [134976/225000 (60%)] Loss: 18018.375000\n",
      "Train Epoch: 282 [137472/225000 (61%)] Loss: 17491.585938\n",
      "Train Epoch: 282 [139968/225000 (62%)] Loss: 17277.021484\n",
      "Train Epoch: 282 [142464/225000 (63%)] Loss: 17722.664062\n",
      "Train Epoch: 282 [144960/225000 (64%)] Loss: 17804.109375\n",
      "Train Epoch: 282 [147456/225000 (66%)] Loss: 17546.951172\n",
      "Train Epoch: 282 [149952/225000 (67%)] Loss: 17608.988281\n",
      "Train Epoch: 282 [152448/225000 (68%)] Loss: 17706.601562\n",
      "Train Epoch: 282 [154944/225000 (69%)] Loss: 17754.082031\n",
      "Train Epoch: 282 [157440/225000 (70%)] Loss: 17979.140625\n",
      "Train Epoch: 282 [159936/225000 (71%)] Loss: 17585.769531\n",
      "Train Epoch: 282 [162432/225000 (72%)] Loss: 17509.359375\n",
      "Train Epoch: 282 [164928/225000 (73%)] Loss: 17644.931641\n",
      "Train Epoch: 282 [167424/225000 (74%)] Loss: 17252.621094\n",
      "Train Epoch: 282 [169920/225000 (76%)] Loss: 17777.443359\n",
      "Train Epoch: 282 [172416/225000 (77%)] Loss: 17651.910156\n",
      "Train Epoch: 282 [174912/225000 (78%)] Loss: 17476.445312\n",
      "Train Epoch: 282 [177408/225000 (79%)] Loss: 17484.296875\n",
      "Train Epoch: 282 [179904/225000 (80%)] Loss: 17444.386719\n",
      "Train Epoch: 282 [182400/225000 (81%)] Loss: 17364.578125\n",
      "Train Epoch: 282 [184896/225000 (82%)] Loss: 17736.628906\n",
      "Train Epoch: 282 [187392/225000 (83%)] Loss: 17804.269531\n",
      "Train Epoch: 282 [189888/225000 (84%)] Loss: 17414.308594\n",
      "Train Epoch: 282 [192384/225000 (86%)] Loss: 17417.916016\n",
      "Train Epoch: 282 [194880/225000 (87%)] Loss: 17834.611328\n",
      "Train Epoch: 282 [197376/225000 (88%)] Loss: 17382.904297\n",
      "Train Epoch: 282 [199872/225000 (89%)] Loss: 17827.480469\n",
      "Train Epoch: 282 [202368/225000 (90%)] Loss: 17098.320312\n",
      "Train Epoch: 282 [204864/225000 (91%)] Loss: 16940.925781\n",
      "Train Epoch: 282 [207360/225000 (92%)] Loss: 18068.566406\n",
      "Train Epoch: 282 [209856/225000 (93%)] Loss: 17700.109375\n",
      "Train Epoch: 282 [212352/225000 (94%)] Loss: 17423.550781\n",
      "Train Epoch: 282 [214848/225000 (95%)] Loss: 17043.318359\n",
      "Train Epoch: 282 [217344/225000 (97%)] Loss: 17251.460938\n",
      "Train Epoch: 282 [219840/225000 (98%)] Loss: 17183.542969\n",
      "Train Epoch: 282 [222336/225000 (99%)] Loss: 17530.529297\n",
      "Train Epoch: 282 [224832/225000 (100%)] Loss: 17069.277344\n",
      "    epoch          : 282\n",
      "    loss           : 17504.154327705044\n",
      "    val_loss       : 17427.316477829263\n",
      "Train Epoch: 283 [192/225000 (0%)] Loss: 17301.445312\n",
      "Train Epoch: 283 [2688/225000 (1%)] Loss: 17572.621094\n",
      "Train Epoch: 283 [5184/225000 (2%)] Loss: 17593.326172\n",
      "Train Epoch: 283 [7680/225000 (3%)] Loss: 17132.582031\n",
      "Train Epoch: 283 [10176/225000 (5%)] Loss: 17311.244141\n",
      "Train Epoch: 283 [12672/225000 (6%)] Loss: 17165.519531\n",
      "Train Epoch: 283 [15168/225000 (7%)] Loss: 17562.027344\n",
      "Train Epoch: 283 [17664/225000 (8%)] Loss: 17598.042969\n",
      "Train Epoch: 283 [20160/225000 (9%)] Loss: 16971.826172\n",
      "Train Epoch: 283 [22656/225000 (10%)] Loss: 17936.656250\n",
      "Train Epoch: 283 [25152/225000 (11%)] Loss: 17301.380859\n",
      "Train Epoch: 283 [27648/225000 (12%)] Loss: 17374.933594\n",
      "Train Epoch: 283 [30144/225000 (13%)] Loss: 16939.843750\n",
      "Train Epoch: 283 [32640/225000 (15%)] Loss: 17201.269531\n",
      "Train Epoch: 283 [35136/225000 (16%)] Loss: 17847.119141\n",
      "Train Epoch: 283 [37632/225000 (17%)] Loss: 17213.871094\n",
      "Train Epoch: 283 [40128/225000 (18%)] Loss: 17844.128906\n",
      "Train Epoch: 283 [42624/225000 (19%)] Loss: 17164.035156\n",
      "Train Epoch: 283 [45120/225000 (20%)] Loss: 17480.746094\n",
      "Train Epoch: 283 [47616/225000 (21%)] Loss: 17495.175781\n",
      "Train Epoch: 283 [50112/225000 (22%)] Loss: 17557.542969\n",
      "Train Epoch: 283 [52608/225000 (23%)] Loss: 17851.089844\n",
      "Train Epoch: 283 [55104/225000 (24%)] Loss: 17308.738281\n",
      "Train Epoch: 283 [57600/225000 (26%)] Loss: 17169.445312\n",
      "Train Epoch: 283 [60096/225000 (27%)] Loss: 17125.185547\n",
      "Train Epoch: 283 [62592/225000 (28%)] Loss: 17257.566406\n",
      "Train Epoch: 283 [65088/225000 (29%)] Loss: 17314.250000\n",
      "Train Epoch: 283 [67584/225000 (30%)] Loss: 17398.468750\n",
      "Train Epoch: 283 [70080/225000 (31%)] Loss: 17260.410156\n",
      "Train Epoch: 283 [72576/225000 (32%)] Loss: 17595.468750\n",
      "Train Epoch: 283 [75072/225000 (33%)] Loss: 17640.341797\n",
      "Train Epoch: 283 [77568/225000 (34%)] Loss: 17406.580078\n",
      "Train Epoch: 283 [80064/225000 (36%)] Loss: 17555.087891\n",
      "Train Epoch: 283 [82560/225000 (37%)] Loss: 17474.398438\n",
      "Train Epoch: 283 [85056/225000 (38%)] Loss: 17307.328125\n",
      "Train Epoch: 283 [87552/225000 (39%)] Loss: 17374.164062\n",
      "Train Epoch: 283 [90048/225000 (40%)] Loss: 17296.980469\n",
      "Train Epoch: 283 [92544/225000 (41%)] Loss: 17819.671875\n",
      "Train Epoch: 283 [95040/225000 (42%)] Loss: 16997.472656\n",
      "Train Epoch: 283 [97536/225000 (43%)] Loss: 18104.476562\n",
      "Train Epoch: 283 [100032/225000 (44%)] Loss: 17364.140625\n",
      "Train Epoch: 283 [102528/225000 (46%)] Loss: 17027.273438\n",
      "Train Epoch: 283 [105024/225000 (47%)] Loss: 17341.160156\n",
      "Train Epoch: 283 [107520/225000 (48%)] Loss: 16806.062500\n",
      "Train Epoch: 283 [110016/225000 (49%)] Loss: 17184.322266\n",
      "Train Epoch: 283 [112512/225000 (50%)] Loss: 17719.109375\n",
      "Train Epoch: 283 [115008/225000 (51%)] Loss: 16926.082031\n",
      "Train Epoch: 283 [117504/225000 (52%)] Loss: 17953.960938\n",
      "Train Epoch: 283 [120000/225000 (53%)] Loss: 17529.980469\n",
      "Train Epoch: 283 [122496/225000 (54%)] Loss: 17901.738281\n",
      "Train Epoch: 283 [124992/225000 (56%)] Loss: 17532.615234\n",
      "Train Epoch: 283 [127488/225000 (57%)] Loss: 17823.058594\n",
      "Train Epoch: 283 [129984/225000 (58%)] Loss: 17744.191406\n",
      "Train Epoch: 283 [132480/225000 (59%)] Loss: 17850.304688\n",
      "Train Epoch: 283 [134976/225000 (60%)] Loss: 17183.046875\n",
      "Train Epoch: 283 [137472/225000 (61%)] Loss: 17608.791016\n",
      "Train Epoch: 283 [139968/225000 (62%)] Loss: 18017.000000\n",
      "Train Epoch: 283 [142464/225000 (63%)] Loss: 17118.876953\n",
      "Train Epoch: 283 [144960/225000 (64%)] Loss: 17316.873047\n",
      "Train Epoch: 283 [147456/225000 (66%)] Loss: 17375.589844\n",
      "Train Epoch: 283 [149952/225000 (67%)] Loss: 17317.070312\n",
      "Train Epoch: 283 [152448/225000 (68%)] Loss: 17407.628906\n",
      "Train Epoch: 283 [154944/225000 (69%)] Loss: 16975.843750\n",
      "Train Epoch: 283 [157440/225000 (70%)] Loss: 17370.376953\n",
      "Train Epoch: 283 [159936/225000 (71%)] Loss: 17427.914062\n",
      "Train Epoch: 283 [162432/225000 (72%)] Loss: 17625.480469\n",
      "Train Epoch: 283 [164928/225000 (73%)] Loss: 17755.310547\n",
      "Train Epoch: 283 [167424/225000 (74%)] Loss: 17205.943359\n",
      "Train Epoch: 283 [169920/225000 (76%)] Loss: 17942.554688\n",
      "Train Epoch: 283 [172416/225000 (77%)] Loss: 17345.523438\n",
      "Train Epoch: 283 [174912/225000 (78%)] Loss: 17241.552734\n",
      "Train Epoch: 283 [177408/225000 (79%)] Loss: 17683.970703\n",
      "Train Epoch: 283 [179904/225000 (80%)] Loss: 17302.275391\n",
      "Train Epoch: 283 [182400/225000 (81%)] Loss: 17851.691406\n",
      "Train Epoch: 283 [184896/225000 (82%)] Loss: 17706.589844\n",
      "Train Epoch: 283 [187392/225000 (83%)] Loss: 17208.265625\n",
      "Train Epoch: 283 [189888/225000 (84%)] Loss: 17631.625000\n",
      "Train Epoch: 283 [192384/225000 (86%)] Loss: 17409.656250\n",
      "Train Epoch: 283 [194880/225000 (87%)] Loss: 17505.431641\n",
      "Train Epoch: 283 [197376/225000 (88%)] Loss: 17292.722656\n",
      "Train Epoch: 283 [199872/225000 (89%)] Loss: 17239.988281\n",
      "Train Epoch: 283 [202368/225000 (90%)] Loss: 17388.468750\n",
      "Train Epoch: 283 [204864/225000 (91%)] Loss: 16880.939453\n",
      "Train Epoch: 283 [207360/225000 (92%)] Loss: 16934.630859\n",
      "Train Epoch: 283 [209856/225000 (93%)] Loss: 17263.308594\n",
      "Train Epoch: 283 [212352/225000 (94%)] Loss: 17216.328125\n",
      "Train Epoch: 283 [214848/225000 (95%)] Loss: 17421.437500\n",
      "Train Epoch: 283 [217344/225000 (97%)] Loss: 17660.132812\n",
      "Train Epoch: 283 [219840/225000 (98%)] Loss: 17312.312500\n",
      "Train Epoch: 283 [222336/225000 (99%)] Loss: 17395.333984\n",
      "Train Epoch: 283 [224832/225000 (100%)] Loss: 17292.160156\n",
      "    epoch          : 283\n",
      "    loss           : 17491.84852282423\n",
      "    val_loss       : 17383.25957260605\n",
      "Train Epoch: 284 [192/225000 (0%)] Loss: 17531.578125\n",
      "Train Epoch: 284 [2688/225000 (1%)] Loss: 17545.464844\n",
      "Train Epoch: 284 [5184/225000 (2%)] Loss: 17600.722656\n",
      "Train Epoch: 284 [7680/225000 (3%)] Loss: 17284.175781\n",
      "Train Epoch: 284 [10176/225000 (5%)] Loss: 17005.761719\n",
      "Train Epoch: 284 [12672/225000 (6%)] Loss: 17440.841797\n",
      "Train Epoch: 284 [15168/225000 (7%)] Loss: 17892.332031\n",
      "Train Epoch: 284 [17664/225000 (8%)] Loss: 17964.542969\n",
      "Train Epoch: 284 [20160/225000 (9%)] Loss: 17628.933594\n",
      "Train Epoch: 284 [22656/225000 (10%)] Loss: 17567.523438\n",
      "Train Epoch: 284 [25152/225000 (11%)] Loss: 18077.783203\n",
      "Train Epoch: 284 [27648/225000 (12%)] Loss: 17197.062500\n",
      "Train Epoch: 284 [30144/225000 (13%)] Loss: 17245.982422\n",
      "Train Epoch: 284 [32640/225000 (15%)] Loss: 17086.222656\n",
      "Train Epoch: 284 [35136/225000 (16%)] Loss: 17485.421875\n",
      "Train Epoch: 284 [37632/225000 (17%)] Loss: 17232.990234\n",
      "Train Epoch: 284 [40128/225000 (18%)] Loss: 17160.503906\n",
      "Train Epoch: 284 [42624/225000 (19%)] Loss: 17925.855469\n",
      "Train Epoch: 284 [45120/225000 (20%)] Loss: 17766.996094\n",
      "Train Epoch: 284 [47616/225000 (21%)] Loss: 16766.328125\n",
      "Train Epoch: 284 [50112/225000 (22%)] Loss: 17694.746094\n",
      "Train Epoch: 284 [52608/225000 (23%)] Loss: 17611.273438\n",
      "Train Epoch: 284 [55104/225000 (24%)] Loss: 17466.167969\n",
      "Train Epoch: 284 [57600/225000 (26%)] Loss: 18221.921875\n",
      "Train Epoch: 284 [60096/225000 (27%)] Loss: 17712.953125\n",
      "Train Epoch: 284 [62592/225000 (28%)] Loss: 17694.832031\n",
      "Train Epoch: 284 [65088/225000 (29%)] Loss: 17570.658203\n",
      "Train Epoch: 284 [67584/225000 (30%)] Loss: 17320.738281\n",
      "Train Epoch: 284 [70080/225000 (31%)] Loss: 17521.500000\n",
      "Train Epoch: 284 [72576/225000 (32%)] Loss: 17071.972656\n",
      "Train Epoch: 284 [75072/225000 (33%)] Loss: 17649.117188\n",
      "Train Epoch: 284 [77568/225000 (34%)] Loss: 17027.925781\n",
      "Train Epoch: 284 [80064/225000 (36%)] Loss: 17280.564453\n",
      "Train Epoch: 284 [82560/225000 (37%)] Loss: 17692.767578\n",
      "Train Epoch: 284 [85056/225000 (38%)] Loss: 17481.375000\n",
      "Train Epoch: 284 [87552/225000 (39%)] Loss: 17615.892578\n",
      "Train Epoch: 284 [90048/225000 (40%)] Loss: 17309.562500\n",
      "Train Epoch: 284 [92544/225000 (41%)] Loss: 17561.847656\n",
      "Train Epoch: 284 [95040/225000 (42%)] Loss: 16429.056641\n",
      "Train Epoch: 284 [97536/225000 (43%)] Loss: 17331.621094\n",
      "Train Epoch: 284 [100032/225000 (44%)] Loss: 17779.257812\n",
      "Train Epoch: 284 [102528/225000 (46%)] Loss: 17113.185547\n",
      "Train Epoch: 284 [105024/225000 (47%)] Loss: 17261.941406\n",
      "Train Epoch: 284 [107520/225000 (48%)] Loss: 17449.023438\n",
      "Train Epoch: 284 [110016/225000 (49%)] Loss: 17839.515625\n",
      "Train Epoch: 284 [112512/225000 (50%)] Loss: 17551.884766\n",
      "Train Epoch: 284 [115008/225000 (51%)] Loss: 16944.396484\n",
      "Train Epoch: 284 [117504/225000 (52%)] Loss: 18022.359375\n",
      "Train Epoch: 284 [120000/225000 (53%)] Loss: 17296.037109\n",
      "Train Epoch: 284 [122496/225000 (54%)] Loss: 17553.945312\n",
      "Train Epoch: 284 [124992/225000 (56%)] Loss: 17190.496094\n",
      "Train Epoch: 284 [127488/225000 (57%)] Loss: 17791.421875\n",
      "Train Epoch: 284 [129984/225000 (58%)] Loss: 17471.726562\n",
      "Train Epoch: 284 [132480/225000 (59%)] Loss: 17100.742188\n",
      "Train Epoch: 284 [134976/225000 (60%)] Loss: 17484.175781\n",
      "Train Epoch: 284 [137472/225000 (61%)] Loss: 17532.804688\n",
      "Train Epoch: 284 [139968/225000 (62%)] Loss: 17418.652344\n",
      "Train Epoch: 284 [142464/225000 (63%)] Loss: 17976.835938\n",
      "Train Epoch: 284 [144960/225000 (64%)] Loss: 17818.652344\n",
      "Train Epoch: 284 [147456/225000 (66%)] Loss: 17852.515625\n",
      "Train Epoch: 284 [149952/225000 (67%)] Loss: 17854.802734\n",
      "Train Epoch: 284 [152448/225000 (68%)] Loss: 17721.318359\n",
      "Train Epoch: 284 [154944/225000 (69%)] Loss: 17957.025391\n",
      "Train Epoch: 284 [157440/225000 (70%)] Loss: 17138.814453\n",
      "Train Epoch: 284 [159936/225000 (71%)] Loss: 17052.173828\n",
      "Train Epoch: 284 [162432/225000 (72%)] Loss: 17501.441406\n",
      "Train Epoch: 284 [164928/225000 (73%)] Loss: 16966.988281\n",
      "Train Epoch: 284 [167424/225000 (74%)] Loss: 17291.947266\n",
      "Train Epoch: 284 [169920/225000 (76%)] Loss: 17271.283203\n",
      "Train Epoch: 284 [172416/225000 (77%)] Loss: 17152.599609\n",
      "Train Epoch: 284 [174912/225000 (78%)] Loss: 17368.878906\n",
      "Train Epoch: 284 [177408/225000 (79%)] Loss: 17403.503906\n",
      "Train Epoch: 284 [179904/225000 (80%)] Loss: 17744.943359\n",
      "Train Epoch: 284 [182400/225000 (81%)] Loss: 17500.455078\n",
      "Train Epoch: 284 [184896/225000 (82%)] Loss: 17581.484375\n",
      "Train Epoch: 284 [187392/225000 (83%)] Loss: 17403.773438\n",
      "Train Epoch: 284 [189888/225000 (84%)] Loss: 17550.630859\n",
      "Train Epoch: 284 [192384/225000 (86%)] Loss: 17805.375000\n",
      "Train Epoch: 284 [194880/225000 (87%)] Loss: 17472.527344\n",
      "Train Epoch: 284 [197376/225000 (88%)] Loss: 17697.582031\n",
      "Train Epoch: 284 [199872/225000 (89%)] Loss: 17006.785156\n",
      "Train Epoch: 284 [202368/225000 (90%)] Loss: 17339.498047\n",
      "Train Epoch: 284 [204864/225000 (91%)] Loss: 17543.019531\n",
      "Train Epoch: 284 [207360/225000 (92%)] Loss: 17728.429688\n",
      "Train Epoch: 284 [209856/225000 (93%)] Loss: 17318.675781\n",
      "Train Epoch: 284 [212352/225000 (94%)] Loss: 17310.111328\n",
      "Train Epoch: 284 [214848/225000 (95%)] Loss: 17412.769531\n",
      "Train Epoch: 284 [217344/225000 (97%)] Loss: 17432.824219\n",
      "Train Epoch: 284 [219840/225000 (98%)] Loss: 17361.250000\n",
      "Train Epoch: 284 [222336/225000 (99%)] Loss: 17718.894531\n",
      "Train Epoch: 284 [224832/225000 (100%)] Loss: 17669.003906\n",
      "    epoch          : 284\n",
      "    loss           : 17484.33143797995\n",
      "    val_loss       : 17426.798389244625\n",
      "Train Epoch: 285 [192/225000 (0%)] Loss: 17411.203125\n",
      "Train Epoch: 285 [2688/225000 (1%)] Loss: 17332.564453\n",
      "Train Epoch: 285 [5184/225000 (2%)] Loss: 17363.021484\n",
      "Train Epoch: 285 [7680/225000 (3%)] Loss: 17433.808594\n",
      "Train Epoch: 285 [10176/225000 (5%)] Loss: 17499.275391\n",
      "Train Epoch: 285 [12672/225000 (6%)] Loss: 17207.724609\n",
      "Train Epoch: 285 [15168/225000 (7%)] Loss: 17131.160156\n",
      "Train Epoch: 285 [17664/225000 (8%)] Loss: 18129.501953\n",
      "Train Epoch: 285 [20160/225000 (9%)] Loss: 17137.650391\n",
      "Train Epoch: 285 [22656/225000 (10%)] Loss: 17478.222656\n",
      "Train Epoch: 285 [25152/225000 (11%)] Loss: 17744.958984\n",
      "Train Epoch: 285 [27648/225000 (12%)] Loss: 17574.660156\n",
      "Train Epoch: 285 [30144/225000 (13%)] Loss: 17898.566406\n",
      "Train Epoch: 285 [32640/225000 (15%)] Loss: 17922.998047\n",
      "Train Epoch: 285 [35136/225000 (16%)] Loss: 17979.140625\n",
      "Train Epoch: 285 [37632/225000 (17%)] Loss: 17113.789062\n",
      "Train Epoch: 285 [40128/225000 (18%)] Loss: 17618.679688\n",
      "Train Epoch: 285 [42624/225000 (19%)] Loss: 17205.355469\n",
      "Train Epoch: 285 [45120/225000 (20%)] Loss: 17671.835938\n",
      "Train Epoch: 285 [47616/225000 (21%)] Loss: 17400.425781\n",
      "Train Epoch: 285 [50112/225000 (22%)] Loss: 17615.404297\n",
      "Train Epoch: 285 [52608/225000 (23%)] Loss: 17178.113281\n",
      "Train Epoch: 285 [55104/225000 (24%)] Loss: 17175.140625\n",
      "Train Epoch: 285 [57600/225000 (26%)] Loss: 17570.677734\n",
      "Train Epoch: 285 [60096/225000 (27%)] Loss: 17449.437500\n",
      "Train Epoch: 285 [62592/225000 (28%)] Loss: 17634.478516\n",
      "Train Epoch: 285 [65088/225000 (29%)] Loss: 17669.011719\n",
      "Train Epoch: 285 [67584/225000 (30%)] Loss: 17440.925781\n",
      "Train Epoch: 285 [70080/225000 (31%)] Loss: 17507.875000\n",
      "Train Epoch: 285 [72576/225000 (32%)] Loss: 17026.482422\n",
      "Train Epoch: 285 [75072/225000 (33%)] Loss: 17805.703125\n",
      "Train Epoch: 285 [77568/225000 (34%)] Loss: 17404.667969\n",
      "Train Epoch: 285 [80064/225000 (36%)] Loss: 17644.257812\n",
      "Train Epoch: 285 [82560/225000 (37%)] Loss: 17211.689453\n",
      "Train Epoch: 285 [85056/225000 (38%)] Loss: 17285.851562\n",
      "Train Epoch: 285 [87552/225000 (39%)] Loss: 17757.863281\n",
      "Train Epoch: 285 [90048/225000 (40%)] Loss: 17400.103516\n",
      "Train Epoch: 285 [92544/225000 (41%)] Loss: 17604.304688\n",
      "Train Epoch: 285 [95040/225000 (42%)] Loss: 17426.009766\n",
      "Train Epoch: 285 [97536/225000 (43%)] Loss: 18150.644531\n",
      "Train Epoch: 285 [100032/225000 (44%)] Loss: 17440.382812\n",
      "Train Epoch: 285 [102528/225000 (46%)] Loss: 17389.832031\n",
      "Train Epoch: 285 [105024/225000 (47%)] Loss: 17439.378906\n",
      "Train Epoch: 285 [107520/225000 (48%)] Loss: 17363.640625\n",
      "Train Epoch: 285 [110016/225000 (49%)] Loss: 17247.464844\n",
      "Train Epoch: 285 [112512/225000 (50%)] Loss: 17724.779297\n",
      "Train Epoch: 285 [115008/225000 (51%)] Loss: 17772.195312\n",
      "Train Epoch: 285 [117504/225000 (52%)] Loss: 17851.339844\n",
      "Train Epoch: 285 [120000/225000 (53%)] Loss: 17350.001953\n",
      "Train Epoch: 285 [122496/225000 (54%)] Loss: 17797.308594\n",
      "Train Epoch: 285 [124992/225000 (56%)] Loss: 17867.042969\n",
      "Train Epoch: 285 [127488/225000 (57%)] Loss: 16806.566406\n",
      "Train Epoch: 285 [129984/225000 (58%)] Loss: 17550.441406\n",
      "Train Epoch: 285 [132480/225000 (59%)] Loss: 17563.863281\n",
      "Train Epoch: 285 [134976/225000 (60%)] Loss: 17859.560547\n",
      "Train Epoch: 285 [137472/225000 (61%)] Loss: 17149.210938\n",
      "Train Epoch: 285 [139968/225000 (62%)] Loss: 17137.054688\n",
      "Train Epoch: 285 [142464/225000 (63%)] Loss: 17458.597656\n",
      "Train Epoch: 285 [144960/225000 (64%)] Loss: 17400.617188\n",
      "Train Epoch: 285 [147456/225000 (66%)] Loss: 16933.359375\n",
      "Train Epoch: 285 [149952/225000 (67%)] Loss: 17576.095703\n",
      "Train Epoch: 285 [152448/225000 (68%)] Loss: 17205.179688\n",
      "Train Epoch: 285 [154944/225000 (69%)] Loss: 17614.166016\n",
      "Train Epoch: 285 [157440/225000 (70%)] Loss: 17484.664062\n",
      "Train Epoch: 285 [159936/225000 (71%)] Loss: 17992.201172\n",
      "Train Epoch: 285 [162432/225000 (72%)] Loss: 17724.066406\n",
      "Train Epoch: 285 [164928/225000 (73%)] Loss: 17169.246094\n",
      "Train Epoch: 285 [167424/225000 (74%)] Loss: 17155.406250\n",
      "Train Epoch: 285 [169920/225000 (76%)] Loss: 17991.261719\n",
      "Train Epoch: 285 [172416/225000 (77%)] Loss: 17404.919922\n",
      "Train Epoch: 285 [174912/225000 (78%)] Loss: 17205.669922\n",
      "Train Epoch: 285 [177408/225000 (79%)] Loss: 17464.144531\n",
      "Train Epoch: 285 [179904/225000 (80%)] Loss: 17410.779297\n",
      "Train Epoch: 285 [182400/225000 (81%)] Loss: 17336.953125\n",
      "Train Epoch: 285 [184896/225000 (82%)] Loss: 17159.322266\n",
      "Train Epoch: 285 [187392/225000 (83%)] Loss: 17711.621094\n",
      "Train Epoch: 285 [189888/225000 (84%)] Loss: 17436.394531\n",
      "Train Epoch: 285 [192384/225000 (86%)] Loss: 17404.210938\n",
      "Train Epoch: 285 [194880/225000 (87%)] Loss: 17539.292969\n",
      "Train Epoch: 285 [197376/225000 (88%)] Loss: 17800.148438\n",
      "Train Epoch: 285 [199872/225000 (89%)] Loss: 17648.765625\n",
      "Train Epoch: 285 [202368/225000 (90%)] Loss: 17764.046875\n",
      "Train Epoch: 285 [204864/225000 (91%)] Loss: 17412.941406\n",
      "Train Epoch: 285 [207360/225000 (92%)] Loss: 17420.480469\n",
      "Train Epoch: 285 [209856/225000 (93%)] Loss: 16880.695312\n",
      "Train Epoch: 285 [212352/225000 (94%)] Loss: 17327.798828\n",
      "Train Epoch: 285 [214848/225000 (95%)] Loss: 17289.167969\n",
      "Train Epoch: 285 [217344/225000 (97%)] Loss: 17279.234375\n",
      "Train Epoch: 285 [219840/225000 (98%)] Loss: 17746.509766\n",
      "Train Epoch: 285 [222336/225000 (99%)] Loss: 17396.888672\n",
      "Train Epoch: 285 [224832/225000 (100%)] Loss: 17607.515625\n",
      "    epoch          : 285\n",
      "    loss           : 17470.51238284583\n",
      "    val_loss       : 17396.785234838953\n",
      "Train Epoch: 286 [192/225000 (0%)] Loss: 17221.875000\n",
      "Train Epoch: 286 [2688/225000 (1%)] Loss: 17222.828125\n",
      "Train Epoch: 286 [5184/225000 (2%)] Loss: 17841.599609\n",
      "Train Epoch: 286 [7680/225000 (3%)] Loss: 17612.941406\n",
      "Train Epoch: 286 [10176/225000 (5%)] Loss: 17408.402344\n",
      "Train Epoch: 286 [12672/225000 (6%)] Loss: 17609.093750\n",
      "Train Epoch: 286 [15168/225000 (7%)] Loss: 17768.562500\n",
      "Train Epoch: 286 [17664/225000 (8%)] Loss: 17371.181641\n",
      "Train Epoch: 286 [20160/225000 (9%)] Loss: 17326.953125\n",
      "Train Epoch: 286 [22656/225000 (10%)] Loss: 17767.097656\n",
      "Train Epoch: 286 [25152/225000 (11%)] Loss: 17126.570312\n",
      "Train Epoch: 286 [27648/225000 (12%)] Loss: 17685.273438\n",
      "Train Epoch: 286 [30144/225000 (13%)] Loss: 17905.500000\n",
      "Train Epoch: 286 [32640/225000 (15%)] Loss: 17242.167969\n",
      "Train Epoch: 286 [35136/225000 (16%)] Loss: 17924.617188\n",
      "Train Epoch: 286 [37632/225000 (17%)] Loss: 17475.511719\n",
      "Train Epoch: 286 [40128/225000 (18%)] Loss: 17250.685547\n",
      "Train Epoch: 286 [42624/225000 (19%)] Loss: 17161.384766\n",
      "Train Epoch: 286 [45120/225000 (20%)] Loss: 17345.875000\n",
      "Train Epoch: 286 [47616/225000 (21%)] Loss: 17414.890625\n",
      "Train Epoch: 286 [50112/225000 (22%)] Loss: 17517.238281\n",
      "Train Epoch: 286 [52608/225000 (23%)] Loss: 17699.990234\n",
      "Train Epoch: 286 [55104/225000 (24%)] Loss: 17373.179688\n",
      "Train Epoch: 286 [57600/225000 (26%)] Loss: 17738.949219\n",
      "Train Epoch: 286 [60096/225000 (27%)] Loss: 16954.625000\n",
      "Train Epoch: 286 [62592/225000 (28%)] Loss: 17375.958984\n",
      "Train Epoch: 286 [65088/225000 (29%)] Loss: 17501.703125\n",
      "Train Epoch: 286 [67584/225000 (30%)] Loss: 17537.183594\n",
      "Train Epoch: 286 [70080/225000 (31%)] Loss: 17331.160156\n",
      "Train Epoch: 286 [72576/225000 (32%)] Loss: 17104.714844\n",
      "Train Epoch: 286 [75072/225000 (33%)] Loss: 17121.371094\n",
      "Train Epoch: 286 [77568/225000 (34%)] Loss: 17463.281250\n",
      "Train Epoch: 286 [80064/225000 (36%)] Loss: 17462.292969\n",
      "Train Epoch: 286 [82560/225000 (37%)] Loss: 17323.027344\n",
      "Train Epoch: 286 [85056/225000 (38%)] Loss: 17183.996094\n",
      "Train Epoch: 286 [87552/225000 (39%)] Loss: 17410.039062\n",
      "Train Epoch: 286 [90048/225000 (40%)] Loss: 17311.402344\n",
      "Train Epoch: 286 [92544/225000 (41%)] Loss: 17440.210938\n",
      "Train Epoch: 286 [95040/225000 (42%)] Loss: 17542.267578\n",
      "Train Epoch: 286 [97536/225000 (43%)] Loss: 17920.121094\n",
      "Train Epoch: 286 [100032/225000 (44%)] Loss: 17404.953125\n",
      "Train Epoch: 286 [102528/225000 (46%)] Loss: 17319.097656\n",
      "Train Epoch: 286 [105024/225000 (47%)] Loss: 18149.628906\n",
      "Train Epoch: 286 [107520/225000 (48%)] Loss: 17605.929688\n",
      "Train Epoch: 286 [110016/225000 (49%)] Loss: 17170.238281\n",
      "Train Epoch: 286 [112512/225000 (50%)] Loss: 17343.621094\n",
      "Train Epoch: 286 [115008/225000 (51%)] Loss: 17342.757812\n",
      "Train Epoch: 286 [117504/225000 (52%)] Loss: 17079.128906\n",
      "Train Epoch: 286 [120000/225000 (53%)] Loss: 17507.441406\n",
      "Train Epoch: 286 [122496/225000 (54%)] Loss: 17946.644531\n",
      "Train Epoch: 286 [124992/225000 (56%)] Loss: 17107.021484\n",
      "Train Epoch: 286 [127488/225000 (57%)] Loss: 17639.410156\n",
      "Train Epoch: 286 [129984/225000 (58%)] Loss: 17463.585938\n",
      "Train Epoch: 286 [132480/225000 (59%)] Loss: 17177.449219\n",
      "Train Epoch: 286 [134976/225000 (60%)] Loss: 17331.917969\n",
      "Train Epoch: 286 [137472/225000 (61%)] Loss: 17352.734375\n",
      "Train Epoch: 286 [139968/225000 (62%)] Loss: 17161.500000\n",
      "Train Epoch: 286 [142464/225000 (63%)] Loss: 17892.240234\n",
      "Train Epoch: 286 [144960/225000 (64%)] Loss: 17707.554688\n",
      "Train Epoch: 286 [147456/225000 (66%)] Loss: 17354.804688\n",
      "Train Epoch: 286 [149952/225000 (67%)] Loss: 17246.781250\n",
      "Train Epoch: 286 [152448/225000 (68%)] Loss: 17731.089844\n",
      "Train Epoch: 286 [154944/225000 (69%)] Loss: 16881.750000\n",
      "Train Epoch: 286 [157440/225000 (70%)] Loss: 17710.605469\n",
      "Train Epoch: 286 [159936/225000 (71%)] Loss: 17432.125000\n",
      "Train Epoch: 286 [162432/225000 (72%)] Loss: 17153.919922\n",
      "Train Epoch: 286 [164928/225000 (73%)] Loss: 17285.410156\n",
      "Train Epoch: 286 [167424/225000 (74%)] Loss: 17656.574219\n",
      "Train Epoch: 286 [169920/225000 (76%)] Loss: 17141.585938\n",
      "Train Epoch: 286 [172416/225000 (77%)] Loss: 18092.992188\n",
      "Train Epoch: 286 [174912/225000 (78%)] Loss: 17310.007812\n",
      "Train Epoch: 286 [177408/225000 (79%)] Loss: 18009.863281\n",
      "Train Epoch: 286 [179904/225000 (80%)] Loss: 17298.753906\n",
      "Train Epoch: 286 [182400/225000 (81%)] Loss: 17219.103516\n",
      "Train Epoch: 286 [184896/225000 (82%)] Loss: 17220.832031\n",
      "Train Epoch: 286 [187392/225000 (83%)] Loss: 17513.402344\n",
      "Train Epoch: 286 [189888/225000 (84%)] Loss: 17620.535156\n",
      "Train Epoch: 286 [192384/225000 (86%)] Loss: 17616.429688\n",
      "Train Epoch: 286 [194880/225000 (87%)] Loss: 19174.646484\n",
      "Train Epoch: 286 [197376/225000 (88%)] Loss: 17282.503906\n",
      "Train Epoch: 286 [199872/225000 (89%)] Loss: 17563.226562\n",
      "Train Epoch: 286 [202368/225000 (90%)] Loss: 16906.730469\n",
      "Train Epoch: 286 [204864/225000 (91%)] Loss: 17298.806641\n",
      "Train Epoch: 286 [207360/225000 (92%)] Loss: 17579.542969\n",
      "Train Epoch: 286 [209856/225000 (93%)] Loss: 17558.496094\n",
      "Train Epoch: 286 [212352/225000 (94%)] Loss: 17432.542969\n",
      "Train Epoch: 286 [214848/225000 (95%)] Loss: 17247.113281\n",
      "Train Epoch: 286 [217344/225000 (97%)] Loss: 17269.119141\n",
      "Train Epoch: 286 [219840/225000 (98%)] Loss: 17369.509766\n",
      "Train Epoch: 286 [222336/225000 (99%)] Loss: 17593.474609\n",
      "Train Epoch: 286 [224832/225000 (100%)] Loss: 17467.820312\n",
      "    epoch          : 286\n",
      "    loss           : 17453.371247066978\n",
      "    val_loss       : 17491.507706206263\n",
      "Train Epoch: 287 [192/225000 (0%)] Loss: 17747.195312\n",
      "Train Epoch: 287 [2688/225000 (1%)] Loss: 17132.025391\n",
      "Train Epoch: 287 [5184/225000 (2%)] Loss: 17429.208984\n",
      "Train Epoch: 287 [7680/225000 (3%)] Loss: 17472.628906\n",
      "Train Epoch: 287 [10176/225000 (5%)] Loss: 17551.476562\n",
      "Train Epoch: 287 [12672/225000 (6%)] Loss: 17100.945312\n",
      "Train Epoch: 287 [15168/225000 (7%)] Loss: 17660.677734\n",
      "Train Epoch: 287 [17664/225000 (8%)] Loss: 17273.218750\n",
      "Train Epoch: 287 [20160/225000 (9%)] Loss: 17721.917969\n",
      "Train Epoch: 287 [22656/225000 (10%)] Loss: 17330.343750\n",
      "Train Epoch: 287 [25152/225000 (11%)] Loss: 17548.781250\n",
      "Train Epoch: 287 [27648/225000 (12%)] Loss: 17433.109375\n",
      "Train Epoch: 287 [30144/225000 (13%)] Loss: 17336.101562\n",
      "Train Epoch: 287 [32640/225000 (15%)] Loss: 17168.722656\n",
      "Train Epoch: 287 [35136/225000 (16%)] Loss: 17069.388672\n",
      "Train Epoch: 287 [37632/225000 (17%)] Loss: 17039.003906\n",
      "Train Epoch: 287 [40128/225000 (18%)] Loss: 17129.578125\n",
      "Train Epoch: 287 [42624/225000 (19%)] Loss: 17136.511719\n",
      "Train Epoch: 287 [45120/225000 (20%)] Loss: 17542.847656\n",
      "Train Epoch: 287 [47616/225000 (21%)] Loss: 17456.615234\n",
      "Train Epoch: 287 [50112/225000 (22%)] Loss: 17435.363281\n",
      "Train Epoch: 287 [52608/225000 (23%)] Loss: 17182.195312\n",
      "Train Epoch: 287 [55104/225000 (24%)] Loss: 17406.210938\n",
      "Train Epoch: 287 [57600/225000 (26%)] Loss: 17374.548828\n",
      "Train Epoch: 287 [60096/225000 (27%)] Loss: 18004.402344\n",
      "Train Epoch: 287 [62592/225000 (28%)] Loss: 17551.740234\n",
      "Train Epoch: 287 [65088/225000 (29%)] Loss: 17714.941406\n",
      "Train Epoch: 287 [67584/225000 (30%)] Loss: 17225.347656\n",
      "Train Epoch: 287 [70080/225000 (31%)] Loss: 17466.921875\n",
      "Train Epoch: 287 [72576/225000 (32%)] Loss: 17756.689453\n",
      "Train Epoch: 287 [75072/225000 (33%)] Loss: 17272.908203\n",
      "Train Epoch: 287 [77568/225000 (34%)] Loss: 17529.722656\n",
      "Train Epoch: 287 [80064/225000 (36%)] Loss: 17338.115234\n",
      "Train Epoch: 287 [82560/225000 (37%)] Loss: 17539.367188\n",
      "Train Epoch: 287 [85056/225000 (38%)] Loss: 17669.271484\n",
      "Train Epoch: 287 [87552/225000 (39%)] Loss: 17492.906250\n",
      "Train Epoch: 287 [90048/225000 (40%)] Loss: 17883.339844\n",
      "Train Epoch: 287 [92544/225000 (41%)] Loss: 17703.453125\n",
      "Train Epoch: 287 [95040/225000 (42%)] Loss: 17734.013672\n",
      "Train Epoch: 287 [97536/225000 (43%)] Loss: 17290.902344\n",
      "Train Epoch: 287 [100032/225000 (44%)] Loss: 17316.333984\n",
      "Train Epoch: 287 [102528/225000 (46%)] Loss: 17039.583984\n",
      "Train Epoch: 287 [105024/225000 (47%)] Loss: 17544.328125\n",
      "Train Epoch: 287 [107520/225000 (48%)] Loss: 17102.058594\n",
      "Train Epoch: 287 [110016/225000 (49%)] Loss: 17371.175781\n",
      "Train Epoch: 287 [112512/225000 (50%)] Loss: 17201.964844\n",
      "Train Epoch: 287 [115008/225000 (51%)] Loss: 17554.621094\n",
      "Train Epoch: 287 [117504/225000 (52%)] Loss: 17197.375000\n",
      "Train Epoch: 287 [120000/225000 (53%)] Loss: 17655.835938\n",
      "Train Epoch: 287 [122496/225000 (54%)] Loss: 17496.683594\n",
      "Train Epoch: 287 [124992/225000 (56%)] Loss: 17789.488281\n",
      "Train Epoch: 287 [127488/225000 (57%)] Loss: 17587.628906\n",
      "Train Epoch: 287 [129984/225000 (58%)] Loss: 17210.916016\n",
      "Train Epoch: 287 [132480/225000 (59%)] Loss: 17728.099609\n",
      "Train Epoch: 287 [134976/225000 (60%)] Loss: 17846.421875\n",
      "Train Epoch: 287 [137472/225000 (61%)] Loss: 17535.992188\n",
      "Train Epoch: 287 [139968/225000 (62%)] Loss: 17319.757812\n",
      "Train Epoch: 287 [142464/225000 (63%)] Loss: 17962.132812\n",
      "Train Epoch: 287 [144960/225000 (64%)] Loss: 17758.363281\n",
      "Train Epoch: 287 [147456/225000 (66%)] Loss: 17436.035156\n",
      "Train Epoch: 287 [149952/225000 (67%)] Loss: 17427.626953\n",
      "Train Epoch: 287 [152448/225000 (68%)] Loss: 17495.361328\n",
      "Train Epoch: 287 [154944/225000 (69%)] Loss: 17691.601562\n",
      "Train Epoch: 287 [157440/225000 (70%)] Loss: 17081.738281\n",
      "Train Epoch: 287 [159936/225000 (71%)] Loss: 17079.792969\n",
      "Train Epoch: 287 [162432/225000 (72%)] Loss: 17340.927734\n",
      "Train Epoch: 287 [164928/225000 (73%)] Loss: 17474.750000\n",
      "Train Epoch: 287 [167424/225000 (74%)] Loss: 17317.685547\n",
      "Train Epoch: 287 [169920/225000 (76%)] Loss: 17780.781250\n",
      "Train Epoch: 287 [172416/225000 (77%)] Loss: 17425.837891\n",
      "Train Epoch: 287 [174912/225000 (78%)] Loss: 17522.820312\n",
      "Train Epoch: 287 [177408/225000 (79%)] Loss: 17262.515625\n",
      "Train Epoch: 287 [179904/225000 (80%)] Loss: 17256.667969\n",
      "Train Epoch: 287 [182400/225000 (81%)] Loss: 17433.394531\n",
      "Train Epoch: 287 [184896/225000 (82%)] Loss: 17442.460938\n",
      "Train Epoch: 287 [187392/225000 (83%)] Loss: 17597.515625\n",
      "Train Epoch: 287 [189888/225000 (84%)] Loss: 17666.621094\n",
      "Train Epoch: 287 [192384/225000 (86%)] Loss: 17213.712891\n",
      "Train Epoch: 287 [194880/225000 (87%)] Loss: 17403.779297\n",
      "Train Epoch: 287 [197376/225000 (88%)] Loss: 17857.625000\n",
      "Train Epoch: 287 [199872/225000 (89%)] Loss: 17744.097656\n",
      "Train Epoch: 287 [202368/225000 (90%)] Loss: 16908.402344\n",
      "Train Epoch: 287 [204864/225000 (91%)] Loss: 17579.669922\n",
      "Train Epoch: 287 [207360/225000 (92%)] Loss: 17406.242188\n",
      "Train Epoch: 287 [209856/225000 (93%)] Loss: 17238.988281\n",
      "Train Epoch: 287 [212352/225000 (94%)] Loss: 17310.113281\n",
      "Train Epoch: 287 [214848/225000 (95%)] Loss: 17173.101562\n",
      "Train Epoch: 287 [217344/225000 (97%)] Loss: 17750.910156\n",
      "Train Epoch: 287 [219840/225000 (98%)] Loss: 17552.644531\n",
      "Train Epoch: 287 [222336/225000 (99%)] Loss: 18092.214844\n",
      "Train Epoch: 287 [224832/225000 (100%)] Loss: 18034.375000\n",
      "    epoch          : 287\n",
      "    loss           : 17443.590161216136\n",
      "    val_loss       : 17361.904507656134\n",
      "Train Epoch: 288 [192/225000 (0%)] Loss: 17314.214844\n",
      "Train Epoch: 288 [2688/225000 (1%)] Loss: 17778.794922\n",
      "Train Epoch: 288 [5184/225000 (2%)] Loss: 17160.285156\n",
      "Train Epoch: 288 [7680/225000 (3%)] Loss: 17639.347656\n",
      "Train Epoch: 288 [10176/225000 (5%)] Loss: 17594.820312\n",
      "Train Epoch: 288 [12672/225000 (6%)] Loss: 17039.742188\n",
      "Train Epoch: 288 [15168/225000 (7%)] Loss: 17212.652344\n",
      "Train Epoch: 288 [17664/225000 (8%)] Loss: 17256.351562\n",
      "Train Epoch: 288 [20160/225000 (9%)] Loss: 16983.457031\n",
      "Train Epoch: 288 [22656/225000 (10%)] Loss: 17614.925781\n",
      "Train Epoch: 288 [25152/225000 (11%)] Loss: 17237.359375\n",
      "Train Epoch: 288 [27648/225000 (12%)] Loss: 17262.982422\n",
      "Train Epoch: 288 [30144/225000 (13%)] Loss: 17238.363281\n",
      "Train Epoch: 288 [32640/225000 (15%)] Loss: 17758.488281\n",
      "Train Epoch: 288 [35136/225000 (16%)] Loss: 17388.222656\n",
      "Train Epoch: 288 [37632/225000 (17%)] Loss: 17467.765625\n",
      "Train Epoch: 288 [40128/225000 (18%)] Loss: 17254.863281\n",
      "Train Epoch: 288 [42624/225000 (19%)] Loss: 17973.298828\n",
      "Train Epoch: 288 [45120/225000 (20%)] Loss: 17409.718750\n",
      "Train Epoch: 288 [47616/225000 (21%)] Loss: 17230.654297\n",
      "Train Epoch: 288 [50112/225000 (22%)] Loss: 17405.195312\n",
      "Train Epoch: 288 [52608/225000 (23%)] Loss: 17908.203125\n",
      "Train Epoch: 288 [55104/225000 (24%)] Loss: 17260.103516\n",
      "Train Epoch: 288 [57600/225000 (26%)] Loss: 17159.105469\n",
      "Train Epoch: 288 [60096/225000 (27%)] Loss: 17120.109375\n",
      "Train Epoch: 288 [62592/225000 (28%)] Loss: 17632.851562\n",
      "Train Epoch: 288 [65088/225000 (29%)] Loss: 17391.574219\n",
      "Train Epoch: 288 [67584/225000 (30%)] Loss: 17631.160156\n",
      "Train Epoch: 288 [70080/225000 (31%)] Loss: 17329.572266\n",
      "Train Epoch: 288 [72576/225000 (32%)] Loss: 17126.800781\n",
      "Train Epoch: 288 [75072/225000 (33%)] Loss: 17407.740234\n",
      "Train Epoch: 288 [77568/225000 (34%)] Loss: 17811.078125\n",
      "Train Epoch: 288 [80064/225000 (36%)] Loss: 17550.250000\n",
      "Train Epoch: 288 [82560/225000 (37%)] Loss: 17729.019531\n",
      "Train Epoch: 288 [85056/225000 (38%)] Loss: 17346.421875\n",
      "Train Epoch: 288 [87552/225000 (39%)] Loss: 17315.845703\n",
      "Train Epoch: 288 [90048/225000 (40%)] Loss: 17412.105469\n",
      "Train Epoch: 288 [92544/225000 (41%)] Loss: 17737.007812\n",
      "Train Epoch: 288 [95040/225000 (42%)] Loss: 17459.542969\n",
      "Train Epoch: 288 [97536/225000 (43%)] Loss: 17093.406250\n",
      "Train Epoch: 288 [100032/225000 (44%)] Loss: 17570.609375\n",
      "Train Epoch: 288 [102528/225000 (46%)] Loss: 17248.316406\n",
      "Train Epoch: 288 [105024/225000 (47%)] Loss: 17701.767578\n",
      "Train Epoch: 288 [107520/225000 (48%)] Loss: 17363.492188\n",
      "Train Epoch: 288 [110016/225000 (49%)] Loss: 17305.787109\n",
      "Train Epoch: 288 [112512/225000 (50%)] Loss: 17573.535156\n",
      "Train Epoch: 288 [115008/225000 (51%)] Loss: 17555.097656\n",
      "Train Epoch: 288 [117504/225000 (52%)] Loss: 17352.619141\n",
      "Train Epoch: 288 [120000/225000 (53%)] Loss: 17656.890625\n",
      "Train Epoch: 288 [122496/225000 (54%)] Loss: 17157.542969\n",
      "Train Epoch: 288 [124992/225000 (56%)] Loss: 17028.453125\n",
      "Train Epoch: 288 [127488/225000 (57%)] Loss: 17355.580078\n",
      "Train Epoch: 288 [129984/225000 (58%)] Loss: 17503.164062\n",
      "Train Epoch: 288 [132480/225000 (59%)] Loss: 17144.878906\n",
      "Train Epoch: 288 [134976/225000 (60%)] Loss: 17472.873047\n",
      "Train Epoch: 288 [137472/225000 (61%)] Loss: 17610.835938\n",
      "Train Epoch: 288 [139968/225000 (62%)] Loss: 17232.042969\n",
      "Train Epoch: 288 [142464/225000 (63%)] Loss: 17389.406250\n",
      "Train Epoch: 288 [144960/225000 (64%)] Loss: 17514.921875\n",
      "Train Epoch: 288 [147456/225000 (66%)] Loss: 17309.484375\n",
      "Train Epoch: 288 [149952/225000 (67%)] Loss: 17039.037109\n",
      "Train Epoch: 288 [152448/225000 (68%)] Loss: 17322.816406\n",
      "Train Epoch: 288 [154944/225000 (69%)] Loss: 17472.773438\n",
      "Train Epoch: 288 [157440/225000 (70%)] Loss: 17219.984375\n",
      "Train Epoch: 288 [159936/225000 (71%)] Loss: 17359.562500\n",
      "Train Epoch: 288 [162432/225000 (72%)] Loss: 16943.462891\n",
      "Train Epoch: 288 [164928/225000 (73%)] Loss: 17773.035156\n",
      "Train Epoch: 288 [167424/225000 (74%)] Loss: 17010.109375\n",
      "Train Epoch: 288 [169920/225000 (76%)] Loss: 17706.535156\n",
      "Train Epoch: 288 [172416/225000 (77%)] Loss: 17665.222656\n",
      "Train Epoch: 288 [174912/225000 (78%)] Loss: 17318.121094\n",
      "Train Epoch: 288 [177408/225000 (79%)] Loss: 17390.199219\n",
      "Train Epoch: 288 [179904/225000 (80%)] Loss: 17341.859375\n",
      "Train Epoch: 288 [182400/225000 (81%)] Loss: 16955.410156\n",
      "Train Epoch: 288 [184896/225000 (82%)] Loss: 17380.058594\n",
      "Train Epoch: 288 [187392/225000 (83%)] Loss: 17439.255859\n",
      "Train Epoch: 288 [189888/225000 (84%)] Loss: 17385.406250\n",
      "Train Epoch: 288 [192384/225000 (86%)] Loss: 17101.535156\n",
      "Train Epoch: 288 [194880/225000 (87%)] Loss: 17423.285156\n",
      "Train Epoch: 288 [197376/225000 (88%)] Loss: 17266.632812\n",
      "Train Epoch: 288 [199872/225000 (89%)] Loss: 17344.984375\n",
      "Train Epoch: 288 [202368/225000 (90%)] Loss: 17477.662109\n",
      "Train Epoch: 288 [204864/225000 (91%)] Loss: 17399.988281\n",
      "Train Epoch: 288 [207360/225000 (92%)] Loss: 16852.978516\n",
      "Train Epoch: 288 [209856/225000 (93%)] Loss: 17205.687500\n",
      "Train Epoch: 288 [212352/225000 (94%)] Loss: 17183.820312\n",
      "Train Epoch: 288 [214848/225000 (95%)] Loss: 17274.681641\n",
      "Train Epoch: 288 [217344/225000 (97%)] Loss: 18084.375000\n",
      "Train Epoch: 288 [219840/225000 (98%)] Loss: 17543.746094\n",
      "Train Epoch: 288 [222336/225000 (99%)] Loss: 17657.880859\n",
      "Train Epoch: 288 [224832/225000 (100%)] Loss: 17650.931641\n",
      "    epoch          : 288\n",
      "    loss           : 17433.908992207496\n",
      "    val_loss       : 17392.421313919185\n",
      "Train Epoch: 289 [192/225000 (0%)] Loss: 17455.984375\n",
      "Train Epoch: 289 [2688/225000 (1%)] Loss: 16929.097656\n",
      "Train Epoch: 289 [5184/225000 (2%)] Loss: 17092.021484\n",
      "Train Epoch: 289 [7680/225000 (3%)] Loss: 17619.394531\n",
      "Train Epoch: 289 [10176/225000 (5%)] Loss: 17438.375000\n",
      "Train Epoch: 289 [12672/225000 (6%)] Loss: 17508.828125\n",
      "Train Epoch: 289 [15168/225000 (7%)] Loss: 17186.843750\n",
      "Train Epoch: 289 [17664/225000 (8%)] Loss: 17516.207031\n",
      "Train Epoch: 289 [20160/225000 (9%)] Loss: 16918.208984\n",
      "Train Epoch: 289 [22656/225000 (10%)] Loss: 17401.619141\n",
      "Train Epoch: 289 [25152/225000 (11%)] Loss: 17418.294922\n",
      "Train Epoch: 289 [27648/225000 (12%)] Loss: 17556.988281\n",
      "Train Epoch: 289 [30144/225000 (13%)] Loss: 17296.253906\n",
      "Train Epoch: 289 [32640/225000 (15%)] Loss: 17428.335938\n",
      "Train Epoch: 289 [35136/225000 (16%)] Loss: 17401.449219\n",
      "Train Epoch: 289 [37632/225000 (17%)] Loss: 16990.708984\n",
      "Train Epoch: 289 [40128/225000 (18%)] Loss: 17249.484375\n",
      "Train Epoch: 289 [42624/225000 (19%)] Loss: 16722.632812\n",
      "Train Epoch: 289 [45120/225000 (20%)] Loss: 16991.363281\n",
      "Train Epoch: 289 [47616/225000 (21%)] Loss: 16765.675781\n",
      "Train Epoch: 289 [50112/225000 (22%)] Loss: 17245.617188\n",
      "Train Epoch: 289 [52608/225000 (23%)] Loss: 17375.041016\n",
      "Train Epoch: 289 [55104/225000 (24%)] Loss: 17234.734375\n",
      "Train Epoch: 289 [57600/225000 (26%)] Loss: 17246.408203\n",
      "Train Epoch: 289 [60096/225000 (27%)] Loss: 17048.542969\n",
      "Train Epoch: 289 [62592/225000 (28%)] Loss: 17568.517578\n",
      "Train Epoch: 289 [65088/225000 (29%)] Loss: 17427.355469\n",
      "Train Epoch: 289 [67584/225000 (30%)] Loss: 17257.068359\n",
      "Train Epoch: 289 [70080/225000 (31%)] Loss: 17205.214844\n",
      "Train Epoch: 289 [72576/225000 (32%)] Loss: 17505.808594\n",
      "Train Epoch: 289 [75072/225000 (33%)] Loss: 17998.695312\n",
      "Train Epoch: 289 [77568/225000 (34%)] Loss: 17219.636719\n",
      "Train Epoch: 289 [80064/225000 (36%)] Loss: 17636.119141\n",
      "Train Epoch: 289 [82560/225000 (37%)] Loss: 17370.847656\n",
      "Train Epoch: 289 [85056/225000 (38%)] Loss: 17793.337891\n",
      "Train Epoch: 289 [87552/225000 (39%)] Loss: 17391.492188\n",
      "Train Epoch: 289 [90048/225000 (40%)] Loss: 17449.931641\n",
      "Train Epoch: 289 [92544/225000 (41%)] Loss: 17295.335938\n",
      "Train Epoch: 289 [95040/225000 (42%)] Loss: 16952.402344\n",
      "Train Epoch: 289 [97536/225000 (43%)] Loss: 17318.986328\n",
      "Train Epoch: 289 [100032/225000 (44%)] Loss: 17233.628906\n",
      "Train Epoch: 289 [102528/225000 (46%)] Loss: 17406.416016\n",
      "Train Epoch: 289 [105024/225000 (47%)] Loss: 17245.503906\n",
      "Train Epoch: 289 [107520/225000 (48%)] Loss: 17252.269531\n",
      "Train Epoch: 289 [110016/225000 (49%)] Loss: 17115.791016\n",
      "Train Epoch: 289 [112512/225000 (50%)] Loss: 17080.306641\n",
      "Train Epoch: 289 [115008/225000 (51%)] Loss: 17110.685547\n",
      "Train Epoch: 289 [117504/225000 (52%)] Loss: 17525.082031\n",
      "Train Epoch: 289 [120000/225000 (53%)] Loss: 17219.765625\n",
      "Train Epoch: 289 [122496/225000 (54%)] Loss: 18102.511719\n",
      "Train Epoch: 289 [124992/225000 (56%)] Loss: 17364.019531\n",
      "Train Epoch: 289 [127488/225000 (57%)] Loss: 17802.695312\n",
      "Train Epoch: 289 [129984/225000 (58%)] Loss: 17493.613281\n",
      "Train Epoch: 289 [132480/225000 (59%)] Loss: 17641.511719\n",
      "Train Epoch: 289 [134976/225000 (60%)] Loss: 17335.019531\n",
      "Train Epoch: 289 [137472/225000 (61%)] Loss: 17330.496094\n",
      "Train Epoch: 289 [139968/225000 (62%)] Loss: 16953.050781\n",
      "Train Epoch: 289 [142464/225000 (63%)] Loss: 17810.367188\n",
      "Train Epoch: 289 [144960/225000 (64%)] Loss: 17564.617188\n",
      "Train Epoch: 289 [147456/225000 (66%)] Loss: 17681.169922\n",
      "Train Epoch: 289 [149952/225000 (67%)] Loss: 17402.636719\n",
      "Train Epoch: 289 [152448/225000 (68%)] Loss: 17158.195312\n",
      "Train Epoch: 289 [154944/225000 (69%)] Loss: 17372.812500\n",
      "Train Epoch: 289 [157440/225000 (70%)] Loss: 17448.552734\n",
      "Train Epoch: 289 [159936/225000 (71%)] Loss: 17324.462891\n",
      "Train Epoch: 289 [162432/225000 (72%)] Loss: 17382.378906\n",
      "Train Epoch: 289 [164928/225000 (73%)] Loss: 16908.214844\n",
      "Train Epoch: 289 [167424/225000 (74%)] Loss: 17784.886719\n",
      "Train Epoch: 289 [169920/225000 (76%)] Loss: 17039.886719\n",
      "Train Epoch: 289 [172416/225000 (77%)] Loss: 17299.433594\n",
      "Train Epoch: 289 [174912/225000 (78%)] Loss: 17408.867188\n",
      "Train Epoch: 289 [177408/225000 (79%)] Loss: 17450.650391\n",
      "Train Epoch: 289 [179904/225000 (80%)] Loss: 17731.242188\n",
      "Train Epoch: 289 [182400/225000 (81%)] Loss: 17131.339844\n",
      "Train Epoch: 289 [184896/225000 (82%)] Loss: 17475.175781\n",
      "Train Epoch: 289 [187392/225000 (83%)] Loss: 17117.097656\n",
      "Train Epoch: 289 [189888/225000 (84%)] Loss: 17849.142578\n",
      "Train Epoch: 289 [192384/225000 (86%)] Loss: 17195.132812\n",
      "Train Epoch: 289 [194880/225000 (87%)] Loss: 17455.050781\n",
      "Train Epoch: 289 [197376/225000 (88%)] Loss: 16839.277344\n",
      "Train Epoch: 289 [199872/225000 (89%)] Loss: 17284.341797\n",
      "Train Epoch: 289 [202368/225000 (90%)] Loss: 17425.121094\n",
      "Train Epoch: 289 [204864/225000 (91%)] Loss: 17537.130859\n",
      "Train Epoch: 289 [207360/225000 (92%)] Loss: 17643.460938\n",
      "Train Epoch: 289 [209856/225000 (93%)] Loss: 16913.677734\n",
      "Train Epoch: 289 [212352/225000 (94%)] Loss: 17061.218750\n",
      "Train Epoch: 289 [214848/225000 (95%)] Loss: 17913.000000\n",
      "Train Epoch: 289 [217344/225000 (97%)] Loss: 17514.402344\n",
      "Train Epoch: 289 [219840/225000 (98%)] Loss: 17845.394531\n",
      "Train Epoch: 289 [222336/225000 (99%)] Loss: 16802.433594\n",
      "Train Epoch: 289 [224832/225000 (100%)] Loss: 17212.457031\n",
      "    epoch          : 289\n",
      "    loss           : 17439.0320240841\n",
      "    val_loss       : 17391.24581404224\n",
      "Train Epoch: 290 [192/225000 (0%)] Loss: 17311.726562\n",
      "Train Epoch: 290 [2688/225000 (1%)] Loss: 17448.664062\n",
      "Train Epoch: 290 [5184/225000 (2%)] Loss: 17416.796875\n",
      "Train Epoch: 290 [7680/225000 (3%)] Loss: 17227.144531\n",
      "Train Epoch: 290 [10176/225000 (5%)] Loss: 17378.902344\n",
      "Train Epoch: 290 [12672/225000 (6%)] Loss: 17312.941406\n",
      "Train Epoch: 290 [15168/225000 (7%)] Loss: 17370.503906\n",
      "Train Epoch: 290 [17664/225000 (8%)] Loss: 17547.472656\n",
      "Train Epoch: 290 [20160/225000 (9%)] Loss: 17437.226562\n",
      "Train Epoch: 290 [22656/225000 (10%)] Loss: 16900.960938\n",
      "Train Epoch: 290 [25152/225000 (11%)] Loss: 17376.857422\n",
      "Train Epoch: 290 [27648/225000 (12%)] Loss: 17037.326172\n",
      "Train Epoch: 290 [30144/225000 (13%)] Loss: 17101.183594\n",
      "Train Epoch: 290 [32640/225000 (15%)] Loss: 17438.933594\n",
      "Train Epoch: 290 [35136/225000 (16%)] Loss: 17284.148438\n",
      "Train Epoch: 290 [37632/225000 (17%)] Loss: 17425.644531\n",
      "Train Epoch: 290 [40128/225000 (18%)] Loss: 17357.283203\n",
      "Train Epoch: 290 [42624/225000 (19%)] Loss: 18274.832031\n",
      "Train Epoch: 290 [45120/225000 (20%)] Loss: 17401.646484\n",
      "Train Epoch: 290 [47616/225000 (21%)] Loss: 17444.910156\n",
      "Train Epoch: 290 [50112/225000 (22%)] Loss: 17345.076172\n",
      "Train Epoch: 290 [52608/225000 (23%)] Loss: 17355.789062\n",
      "Train Epoch: 290 [55104/225000 (24%)] Loss: 17360.474609\n",
      "Train Epoch: 290 [57600/225000 (26%)] Loss: 17619.779297\n",
      "Train Epoch: 290 [60096/225000 (27%)] Loss: 17478.136719\n",
      "Train Epoch: 290 [62592/225000 (28%)] Loss: 17379.000000\n",
      "Train Epoch: 290 [65088/225000 (29%)] Loss: 17218.533203\n",
      "Train Epoch: 290 [67584/225000 (30%)] Loss: 17165.533203\n",
      "Train Epoch: 290 [70080/225000 (31%)] Loss: 17393.619141\n",
      "Train Epoch: 290 [72576/225000 (32%)] Loss: 17172.519531\n",
      "Train Epoch: 290 [75072/225000 (33%)] Loss: 17406.552734\n",
      "Train Epoch: 290 [77568/225000 (34%)] Loss: 17557.222656\n",
      "Train Epoch: 290 [80064/225000 (36%)] Loss: 17099.031250\n",
      "Train Epoch: 290 [82560/225000 (37%)] Loss: 17456.488281\n",
      "Train Epoch: 290 [85056/225000 (38%)] Loss: 17591.197266\n",
      "Train Epoch: 290 [87552/225000 (39%)] Loss: 17393.240234\n",
      "Train Epoch: 290 [90048/225000 (40%)] Loss: 17330.306641\n",
      "Train Epoch: 290 [92544/225000 (41%)] Loss: 16829.367188\n",
      "Train Epoch: 290 [95040/225000 (42%)] Loss: 17032.433594\n",
      "Train Epoch: 290 [97536/225000 (43%)] Loss: 17194.662109\n",
      "Train Epoch: 290 [100032/225000 (44%)] Loss: 17345.738281\n",
      "Train Epoch: 290 [102528/225000 (46%)] Loss: 17485.064453\n",
      "Train Epoch: 290 [105024/225000 (47%)] Loss: 17660.683594\n",
      "Train Epoch: 290 [107520/225000 (48%)] Loss: 17007.523438\n",
      "Train Epoch: 290 [110016/225000 (49%)] Loss: 17361.476562\n",
      "Train Epoch: 290 [112512/225000 (50%)] Loss: 17362.404297\n",
      "Train Epoch: 290 [115008/225000 (51%)] Loss: 17225.542969\n",
      "Train Epoch: 290 [117504/225000 (52%)] Loss: 17529.832031\n",
      "Train Epoch: 290 [120000/225000 (53%)] Loss: 17979.019531\n",
      "Train Epoch: 290 [122496/225000 (54%)] Loss: 17304.623047\n",
      "Train Epoch: 290 [124992/225000 (56%)] Loss: 17035.390625\n",
      "Train Epoch: 290 [127488/225000 (57%)] Loss: 17499.078125\n",
      "Train Epoch: 290 [129984/225000 (58%)] Loss: 17704.695312\n",
      "Train Epoch: 290 [132480/225000 (59%)] Loss: 17800.437500\n",
      "Train Epoch: 290 [134976/225000 (60%)] Loss: 17426.664062\n",
      "Train Epoch: 290 [137472/225000 (61%)] Loss: 17195.390625\n",
      "Train Epoch: 290 [139968/225000 (62%)] Loss: 17135.304688\n",
      "Train Epoch: 290 [142464/225000 (63%)] Loss: 17633.625000\n",
      "Train Epoch: 290 [144960/225000 (64%)] Loss: 16908.408203\n",
      "Train Epoch: 290 [147456/225000 (66%)] Loss: 17782.875000\n",
      "Train Epoch: 290 [149952/225000 (67%)] Loss: 17077.964844\n",
      "Train Epoch: 290 [152448/225000 (68%)] Loss: 17453.453125\n",
      "Train Epoch: 290 [154944/225000 (69%)] Loss: 17237.671875\n",
      "Train Epoch: 290 [157440/225000 (70%)] Loss: 17052.603516\n",
      "Train Epoch: 290 [159936/225000 (71%)] Loss: 17166.585938\n",
      "Train Epoch: 290 [162432/225000 (72%)] Loss: 17281.908203\n",
      "Train Epoch: 290 [164928/225000 (73%)] Loss: 17326.634766\n",
      "Train Epoch: 290 [167424/225000 (74%)] Loss: 17326.019531\n",
      "Train Epoch: 290 [169920/225000 (76%)] Loss: 17482.166016\n",
      "Train Epoch: 290 [172416/225000 (77%)] Loss: 17296.457031\n",
      "Train Epoch: 290 [174912/225000 (78%)] Loss: 17468.707031\n",
      "Train Epoch: 290 [177408/225000 (79%)] Loss: 17195.494141\n",
      "Train Epoch: 290 [179904/225000 (80%)] Loss: 17504.474609\n",
      "Train Epoch: 290 [182400/225000 (81%)] Loss: 17352.550781\n",
      "Train Epoch: 290 [184896/225000 (82%)] Loss: 17476.683594\n",
      "Train Epoch: 290 [187392/225000 (83%)] Loss: 17177.800781\n",
      "Train Epoch: 290 [189888/225000 (84%)] Loss: 17088.804688\n",
      "Train Epoch: 290 [192384/225000 (86%)] Loss: 17585.750000\n",
      "Train Epoch: 290 [194880/225000 (87%)] Loss: 17309.539062\n",
      "Train Epoch: 290 [197376/225000 (88%)] Loss: 17119.562500\n",
      "Train Epoch: 290 [199872/225000 (89%)] Loss: 17352.394531\n",
      "Train Epoch: 290 [202368/225000 (90%)] Loss: 17089.927734\n",
      "Train Epoch: 290 [204864/225000 (91%)] Loss: 17656.089844\n",
      "Train Epoch: 290 [207360/225000 (92%)] Loss: 17722.734375\n",
      "Train Epoch: 290 [209856/225000 (93%)] Loss: 17552.875000\n",
      "Train Epoch: 290 [212352/225000 (94%)] Loss: 16977.171875\n",
      "Train Epoch: 290 [214848/225000 (95%)] Loss: 17330.648438\n",
      "Train Epoch: 290 [217344/225000 (97%)] Loss: 17013.769531\n",
      "Train Epoch: 290 [219840/225000 (98%)] Loss: 17157.009766\n",
      "Train Epoch: 290 [222336/225000 (99%)] Loss: 19663.925781\n",
      "Train Epoch: 290 [224832/225000 (100%)] Loss: 17207.449219\n",
      "    epoch          : 290\n",
      "    loss           : 17438.945487481335\n",
      "    val_loss       : 17335.737878675678\n",
      "Train Epoch: 291 [192/225000 (0%)] Loss: 16983.558594\n",
      "Train Epoch: 291 [2688/225000 (1%)] Loss: 17608.564453\n",
      "Train Epoch: 291 [5184/225000 (2%)] Loss: 17072.634766\n",
      "Train Epoch: 291 [7680/225000 (3%)] Loss: 17606.554688\n",
      "Train Epoch: 291 [10176/225000 (5%)] Loss: 17374.115234\n",
      "Train Epoch: 291 [12672/225000 (6%)] Loss: 17216.722656\n",
      "Train Epoch: 291 [15168/225000 (7%)] Loss: 17211.351562\n",
      "Train Epoch: 291 [17664/225000 (8%)] Loss: 17060.503906\n",
      "Train Epoch: 291 [20160/225000 (9%)] Loss: 17135.296875\n",
      "Train Epoch: 291 [22656/225000 (10%)] Loss: 17507.960938\n",
      "Train Epoch: 291 [25152/225000 (11%)] Loss: 17568.400391\n",
      "Train Epoch: 291 [27648/225000 (12%)] Loss: 17821.125000\n",
      "Train Epoch: 291 [30144/225000 (13%)] Loss: 17536.380859\n",
      "Train Epoch: 291 [32640/225000 (15%)] Loss: 17233.568359\n",
      "Train Epoch: 291 [35136/225000 (16%)] Loss: 17684.289062\n",
      "Train Epoch: 291 [37632/225000 (17%)] Loss: 17091.386719\n",
      "Train Epoch: 291 [40128/225000 (18%)] Loss: 17676.597656\n",
      "Train Epoch: 291 [42624/225000 (19%)] Loss: 17688.537109\n",
      "Train Epoch: 291 [45120/225000 (20%)] Loss: 17904.636719\n",
      "Train Epoch: 291 [47616/225000 (21%)] Loss: 17833.246094\n",
      "Train Epoch: 291 [50112/225000 (22%)] Loss: 17226.718750\n",
      "Train Epoch: 291 [52608/225000 (23%)] Loss: 17405.646484\n",
      "Train Epoch: 291 [55104/225000 (24%)] Loss: 17468.226562\n",
      "Train Epoch: 291 [57600/225000 (26%)] Loss: 17242.226562\n",
      "Train Epoch: 291 [60096/225000 (27%)] Loss: 17309.720703\n",
      "Train Epoch: 291 [62592/225000 (28%)] Loss: 17439.351562\n",
      "Train Epoch: 291 [65088/225000 (29%)] Loss: 17069.230469\n",
      "Train Epoch: 291 [67584/225000 (30%)] Loss: 18068.972656\n",
      "Train Epoch: 291 [70080/225000 (31%)] Loss: 17301.386719\n",
      "Train Epoch: 291 [72576/225000 (32%)] Loss: 17493.914062\n",
      "Train Epoch: 291 [75072/225000 (33%)] Loss: 17398.382812\n",
      "Train Epoch: 291 [77568/225000 (34%)] Loss: 17364.421875\n",
      "Train Epoch: 291 [80064/225000 (36%)] Loss: 17279.523438\n",
      "Train Epoch: 291 [82560/225000 (37%)] Loss: 17358.687500\n",
      "Train Epoch: 291 [85056/225000 (38%)] Loss: 17684.460938\n",
      "Train Epoch: 291 [87552/225000 (39%)] Loss: 17473.783203\n",
      "Train Epoch: 291 [90048/225000 (40%)] Loss: 17797.660156\n",
      "Train Epoch: 291 [92544/225000 (41%)] Loss: 17154.558594\n",
      "Train Epoch: 291 [95040/225000 (42%)] Loss: 17158.957031\n",
      "Train Epoch: 291 [97536/225000 (43%)] Loss: 17160.710938\n",
      "Train Epoch: 291 [100032/225000 (44%)] Loss: 17250.457031\n",
      "Train Epoch: 291 [102528/225000 (46%)] Loss: 17288.980469\n",
      "Train Epoch: 291 [105024/225000 (47%)] Loss: 17341.496094\n",
      "Train Epoch: 291 [107520/225000 (48%)] Loss: 17276.988281\n",
      "Train Epoch: 291 [110016/225000 (49%)] Loss: 17634.765625\n",
      "Train Epoch: 291 [112512/225000 (50%)] Loss: 16832.279297\n",
      "Train Epoch: 291 [115008/225000 (51%)] Loss: 17399.304688\n",
      "Train Epoch: 291 [117504/225000 (52%)] Loss: 17457.554688\n",
      "Train Epoch: 291 [120000/225000 (53%)] Loss: 17085.308594\n",
      "Train Epoch: 291 [122496/225000 (54%)] Loss: 17370.515625\n",
      "Train Epoch: 291 [124992/225000 (56%)] Loss: 17055.335938\n",
      "Train Epoch: 291 [127488/225000 (57%)] Loss: 17116.306641\n",
      "Train Epoch: 291 [129984/225000 (58%)] Loss: 17194.425781\n",
      "Train Epoch: 291 [132480/225000 (59%)] Loss: 17405.326172\n",
      "Train Epoch: 291 [134976/225000 (60%)] Loss: 17620.314453\n",
      "Train Epoch: 291 [137472/225000 (61%)] Loss: 17399.576172\n",
      "Train Epoch: 291 [139968/225000 (62%)] Loss: 17014.988281\n",
      "Train Epoch: 291 [142464/225000 (63%)] Loss: 17721.998047\n",
      "Train Epoch: 291 [144960/225000 (64%)] Loss: 17210.158203\n",
      "Train Epoch: 291 [147456/225000 (66%)] Loss: 17709.376953\n",
      "Train Epoch: 291 [149952/225000 (67%)] Loss: 17468.105469\n",
      "Train Epoch: 291 [152448/225000 (68%)] Loss: 17085.921875\n",
      "Train Epoch: 291 [154944/225000 (69%)] Loss: 17421.847656\n",
      "Train Epoch: 291 [157440/225000 (70%)] Loss: 16840.248047\n",
      "Train Epoch: 291 [159936/225000 (71%)] Loss: 17103.359375\n",
      "Train Epoch: 291 [162432/225000 (72%)] Loss: 17446.917969\n",
      "Train Epoch: 291 [164928/225000 (73%)] Loss: 16933.585938\n",
      "Train Epoch: 291 [167424/225000 (74%)] Loss: 17314.128906\n",
      "Train Epoch: 291 [169920/225000 (76%)] Loss: 17904.792969\n",
      "Train Epoch: 291 [172416/225000 (77%)] Loss: 17182.009766\n",
      "Train Epoch: 291 [174912/225000 (78%)] Loss: 16793.781250\n",
      "Train Epoch: 291 [177408/225000 (79%)] Loss: 17569.929688\n",
      "Train Epoch: 291 [179904/225000 (80%)] Loss: 17245.130859\n",
      "Train Epoch: 291 [182400/225000 (81%)] Loss: 17513.193359\n",
      "Train Epoch: 291 [184896/225000 (82%)] Loss: 17311.574219\n",
      "Train Epoch: 291 [187392/225000 (83%)] Loss: 17258.601562\n",
      "Train Epoch: 291 [189888/225000 (84%)] Loss: 17125.089844\n",
      "Train Epoch: 291 [192384/225000 (86%)] Loss: 17234.464844\n",
      "Train Epoch: 291 [194880/225000 (87%)] Loss: 17626.531250\n",
      "Train Epoch: 291 [197376/225000 (88%)] Loss: 17838.773438\n",
      "Train Epoch: 291 [199872/225000 (89%)] Loss: 17149.886719\n",
      "Train Epoch: 291 [202368/225000 (90%)] Loss: 17440.285156\n",
      "Train Epoch: 291 [204864/225000 (91%)] Loss: 17279.746094\n",
      "Train Epoch: 291 [207360/225000 (92%)] Loss: 17402.636719\n",
      "Train Epoch: 291 [209856/225000 (93%)] Loss: 17319.162109\n",
      "Train Epoch: 291 [212352/225000 (94%)] Loss: 17125.332031\n",
      "Train Epoch: 291 [214848/225000 (95%)] Loss: 17227.531250\n",
      "Train Epoch: 291 [217344/225000 (97%)] Loss: 17364.238281\n",
      "Train Epoch: 291 [219840/225000 (98%)] Loss: 18101.890625\n",
      "Train Epoch: 291 [222336/225000 (99%)] Loss: 17459.638672\n",
      "Train Epoch: 291 [224832/225000 (100%)] Loss: 17351.882812\n",
      "    epoch          : 291\n",
      "    loss           : 17405.175591270265\n",
      "    val_loss       : 17357.22464622978\n",
      "Train Epoch: 292 [192/225000 (0%)] Loss: 17349.917969\n",
      "Train Epoch: 292 [2688/225000 (1%)] Loss: 17849.281250\n",
      "Train Epoch: 292 [5184/225000 (2%)] Loss: 17010.626953\n",
      "Train Epoch: 292 [7680/225000 (3%)] Loss: 17496.597656\n",
      "Train Epoch: 292 [10176/225000 (5%)] Loss: 17386.759766\n",
      "Train Epoch: 292 [12672/225000 (6%)] Loss: 17408.873047\n",
      "Train Epoch: 292 [15168/225000 (7%)] Loss: 17274.546875\n",
      "Train Epoch: 292 [17664/225000 (8%)] Loss: 17273.574219\n",
      "Train Epoch: 292 [20160/225000 (9%)] Loss: 17646.593750\n",
      "Train Epoch: 292 [22656/225000 (10%)] Loss: 17396.634766\n",
      "Train Epoch: 292 [25152/225000 (11%)] Loss: 17561.851562\n",
      "Train Epoch: 292 [27648/225000 (12%)] Loss: 16987.871094\n",
      "Train Epoch: 292 [30144/225000 (13%)] Loss: 17739.093750\n",
      "Train Epoch: 292 [32640/225000 (15%)] Loss: 17359.683594\n",
      "Train Epoch: 292 [35136/225000 (16%)] Loss: 17105.556641\n",
      "Train Epoch: 292 [37632/225000 (17%)] Loss: 17413.933594\n",
      "Train Epoch: 292 [40128/225000 (18%)] Loss: 19482.675781\n",
      "Train Epoch: 292 [42624/225000 (19%)] Loss: 17312.740234\n",
      "Train Epoch: 292 [45120/225000 (20%)] Loss: 17355.605469\n",
      "Train Epoch: 292 [47616/225000 (21%)] Loss: 17269.710938\n",
      "Train Epoch: 292 [50112/225000 (22%)] Loss: 17173.109375\n",
      "Train Epoch: 292 [52608/225000 (23%)] Loss: 17481.703125\n",
      "Train Epoch: 292 [55104/225000 (24%)] Loss: 17360.246094\n",
      "Train Epoch: 292 [57600/225000 (26%)] Loss: 16941.925781\n",
      "Train Epoch: 292 [60096/225000 (27%)] Loss: 17739.673828\n",
      "Train Epoch: 292 [62592/225000 (28%)] Loss: 17189.921875\n",
      "Train Epoch: 292 [65088/225000 (29%)] Loss: 17034.679688\n",
      "Train Epoch: 292 [67584/225000 (30%)] Loss: 17281.039062\n",
      "Train Epoch: 292 [70080/225000 (31%)] Loss: 17289.382812\n",
      "Train Epoch: 292 [72576/225000 (32%)] Loss: 17271.037109\n",
      "Train Epoch: 292 [75072/225000 (33%)] Loss: 17602.496094\n",
      "Train Epoch: 292 [77568/225000 (34%)] Loss: 17209.558594\n",
      "Train Epoch: 292 [80064/225000 (36%)] Loss: 17279.332031\n",
      "Train Epoch: 292 [82560/225000 (37%)] Loss: 17567.068359\n",
      "Train Epoch: 292 [85056/225000 (38%)] Loss: 17187.910156\n",
      "Train Epoch: 292 [87552/225000 (39%)] Loss: 17826.765625\n",
      "Train Epoch: 292 [90048/225000 (40%)] Loss: 16926.941406\n",
      "Train Epoch: 292 [92544/225000 (41%)] Loss: 17712.177734\n",
      "Train Epoch: 292 [95040/225000 (42%)] Loss: 17375.562500\n",
      "Train Epoch: 292 [97536/225000 (43%)] Loss: 17142.873047\n",
      "Train Epoch: 292 [100032/225000 (44%)] Loss: 17471.980469\n",
      "Train Epoch: 292 [102528/225000 (46%)] Loss: 17745.367188\n",
      "Train Epoch: 292 [105024/225000 (47%)] Loss: 17395.062500\n",
      "Train Epoch: 292 [107520/225000 (48%)] Loss: 17305.488281\n",
      "Train Epoch: 292 [110016/225000 (49%)] Loss: 17767.015625\n",
      "Train Epoch: 292 [112512/225000 (50%)] Loss: 17466.105469\n",
      "Train Epoch: 292 [115008/225000 (51%)] Loss: 17478.218750\n",
      "Train Epoch: 292 [117504/225000 (52%)] Loss: 17572.628906\n",
      "Train Epoch: 292 [120000/225000 (53%)] Loss: 17587.093750\n",
      "Train Epoch: 292 [122496/225000 (54%)] Loss: 16890.785156\n",
      "Train Epoch: 292 [124992/225000 (56%)] Loss: 17572.847656\n",
      "Train Epoch: 292 [127488/225000 (57%)] Loss: 19436.773438\n",
      "Train Epoch: 292 [129984/225000 (58%)] Loss: 17155.691406\n",
      "Train Epoch: 292 [132480/225000 (59%)] Loss: 17626.585938\n",
      "Train Epoch: 292 [134976/225000 (60%)] Loss: 17813.263672\n",
      "Train Epoch: 292 [137472/225000 (61%)] Loss: 17418.433594\n",
      "Train Epoch: 292 [139968/225000 (62%)] Loss: 17415.236328\n",
      "Train Epoch: 292 [142464/225000 (63%)] Loss: 17308.503906\n",
      "Train Epoch: 292 [144960/225000 (64%)] Loss: 17289.324219\n",
      "Train Epoch: 292 [147456/225000 (66%)] Loss: 17665.646484\n",
      "Train Epoch: 292 [149952/225000 (67%)] Loss: 17211.621094\n",
      "Train Epoch: 292 [152448/225000 (68%)] Loss: 17154.208984\n",
      "Train Epoch: 292 [154944/225000 (69%)] Loss: 17775.115234\n",
      "Train Epoch: 292 [157440/225000 (70%)] Loss: 17071.271484\n",
      "Train Epoch: 292 [159936/225000 (71%)] Loss: 17619.707031\n",
      "Train Epoch: 292 [162432/225000 (72%)] Loss: 17476.447266\n",
      "Train Epoch: 292 [164928/225000 (73%)] Loss: 17237.910156\n",
      "Train Epoch: 292 [167424/225000 (74%)] Loss: 17361.841797\n",
      "Train Epoch: 292 [169920/225000 (76%)] Loss: 17802.697266\n",
      "Train Epoch: 292 [172416/225000 (77%)] Loss: 17293.488281\n",
      "Train Epoch: 292 [174912/225000 (78%)] Loss: 17750.490234\n",
      "Train Epoch: 292 [177408/225000 (79%)] Loss: 17049.392578\n",
      "Train Epoch: 292 [179904/225000 (80%)] Loss: 17757.228516\n",
      "Train Epoch: 292 [182400/225000 (81%)] Loss: 17908.580078\n",
      "Train Epoch: 292 [184896/225000 (82%)] Loss: 17369.599609\n",
      "Train Epoch: 292 [187392/225000 (83%)] Loss: 17183.798828\n",
      "Train Epoch: 292 [189888/225000 (84%)] Loss: 17551.441406\n",
      "Train Epoch: 292 [192384/225000 (86%)] Loss: 17118.648438\n",
      "Train Epoch: 292 [194880/225000 (87%)] Loss: 17394.382812\n",
      "Train Epoch: 292 [197376/225000 (88%)] Loss: 17624.574219\n",
      "Train Epoch: 292 [199872/225000 (89%)] Loss: 17491.332031\n",
      "Train Epoch: 292 [202368/225000 (90%)] Loss: 17364.453125\n",
      "Train Epoch: 292 [204864/225000 (91%)] Loss: 17760.246094\n",
      "Train Epoch: 292 [207360/225000 (92%)] Loss: 17554.859375\n",
      "Train Epoch: 292 [209856/225000 (93%)] Loss: 17256.599609\n",
      "Train Epoch: 292 [212352/225000 (94%)] Loss: 17102.646484\n",
      "Train Epoch: 292 [214848/225000 (95%)] Loss: 17309.417969\n",
      "Train Epoch: 292 [217344/225000 (97%)] Loss: 16934.593750\n",
      "Train Epoch: 292 [219840/225000 (98%)] Loss: 17792.945312\n",
      "Train Epoch: 292 [222336/225000 (99%)] Loss: 16962.394531\n",
      "Train Epoch: 292 [224832/225000 (100%)] Loss: 17731.267578\n",
      "    epoch          : 292\n",
      "    loss           : 17417.160248740136\n",
      "    val_loss       : 17369.858765107983\n",
      "Train Epoch: 293 [192/225000 (0%)] Loss: 17818.890625\n",
      "Train Epoch: 293 [2688/225000 (1%)] Loss: 17462.414062\n",
      "Train Epoch: 293 [5184/225000 (2%)] Loss: 17306.488281\n",
      "Train Epoch: 293 [7680/225000 (3%)] Loss: 17125.308594\n",
      "Train Epoch: 293 [10176/225000 (5%)] Loss: 17261.566406\n",
      "Train Epoch: 293 [12672/225000 (6%)] Loss: 17378.314453\n",
      "Train Epoch: 293 [15168/225000 (7%)] Loss: 17417.298828\n",
      "Train Epoch: 293 [17664/225000 (8%)] Loss: 17143.097656\n",
      "Train Epoch: 293 [20160/225000 (9%)] Loss: 17298.476562\n",
      "Train Epoch: 293 [22656/225000 (10%)] Loss: 17345.494141\n",
      "Train Epoch: 293 [25152/225000 (11%)] Loss: 17143.214844\n",
      "Train Epoch: 293 [27648/225000 (12%)] Loss: 17538.425781\n",
      "Train Epoch: 293 [30144/225000 (13%)] Loss: 17983.441406\n",
      "Train Epoch: 293 [32640/225000 (15%)] Loss: 17415.203125\n",
      "Train Epoch: 293 [35136/225000 (16%)] Loss: 17278.599609\n",
      "Train Epoch: 293 [37632/225000 (17%)] Loss: 17326.910156\n",
      "Train Epoch: 293 [40128/225000 (18%)] Loss: 17848.363281\n",
      "Train Epoch: 293 [42624/225000 (19%)] Loss: 17365.253906\n",
      "Train Epoch: 293 [45120/225000 (20%)] Loss: 17543.117188\n",
      "Train Epoch: 293 [47616/225000 (21%)] Loss: 17014.185547\n",
      "Train Epoch: 293 [50112/225000 (22%)] Loss: 17459.210938\n",
      "Train Epoch: 293 [52608/225000 (23%)] Loss: 17238.746094\n",
      "Train Epoch: 293 [55104/225000 (24%)] Loss: 17236.414062\n",
      "Train Epoch: 293 [57600/225000 (26%)] Loss: 17494.523438\n",
      "Train Epoch: 293 [60096/225000 (27%)] Loss: 17088.960938\n",
      "Train Epoch: 293 [62592/225000 (28%)] Loss: 17421.039062\n",
      "Train Epoch: 293 [65088/225000 (29%)] Loss: 16906.437500\n",
      "Train Epoch: 293 [67584/225000 (30%)] Loss: 17608.402344\n",
      "Train Epoch: 293 [70080/225000 (31%)] Loss: 17391.480469\n",
      "Train Epoch: 293 [72576/225000 (32%)] Loss: 17345.783203\n",
      "Train Epoch: 293 [75072/225000 (33%)] Loss: 17664.210938\n",
      "Train Epoch: 293 [77568/225000 (34%)] Loss: 17549.292969\n",
      "Train Epoch: 293 [80064/225000 (36%)] Loss: 17411.027344\n",
      "Train Epoch: 293 [82560/225000 (37%)] Loss: 17030.339844\n",
      "Train Epoch: 293 [85056/225000 (38%)] Loss: 17510.378906\n",
      "Train Epoch: 293 [87552/225000 (39%)] Loss: 17302.011719\n",
      "Train Epoch: 293 [90048/225000 (40%)] Loss: 17599.921875\n",
      "Train Epoch: 293 [92544/225000 (41%)] Loss: 17382.746094\n",
      "Train Epoch: 293 [95040/225000 (42%)] Loss: 17177.378906\n",
      "Train Epoch: 293 [97536/225000 (43%)] Loss: 17545.027344\n",
      "Train Epoch: 293 [100032/225000 (44%)] Loss: 17492.859375\n",
      "Train Epoch: 293 [102528/225000 (46%)] Loss: 17176.367188\n",
      "Train Epoch: 293 [105024/225000 (47%)] Loss: 17613.718750\n",
      "Train Epoch: 293 [107520/225000 (48%)] Loss: 17618.591797\n",
      "Train Epoch: 293 [110016/225000 (49%)] Loss: 17455.289062\n",
      "Train Epoch: 293 [112512/225000 (50%)] Loss: 17062.271484\n",
      "Train Epoch: 293 [115008/225000 (51%)] Loss: 18113.628906\n",
      "Train Epoch: 293 [117504/225000 (52%)] Loss: 17396.609375\n",
      "Train Epoch: 293 [120000/225000 (53%)] Loss: 17312.957031\n",
      "Train Epoch: 293 [122496/225000 (54%)] Loss: 17620.236328\n",
      "Train Epoch: 293 [124992/225000 (56%)] Loss: 17161.775391\n",
      "Train Epoch: 293 [127488/225000 (57%)] Loss: 17691.289062\n",
      "Train Epoch: 293 [129984/225000 (58%)] Loss: 17294.009766\n",
      "Train Epoch: 293 [132480/225000 (59%)] Loss: 16723.906250\n",
      "Train Epoch: 293 [134976/225000 (60%)] Loss: 17449.488281\n",
      "Train Epoch: 293 [137472/225000 (61%)] Loss: 16900.871094\n",
      "Train Epoch: 293 [139968/225000 (62%)] Loss: 17569.732422\n",
      "Train Epoch: 293 [142464/225000 (63%)] Loss: 17507.570312\n",
      "Train Epoch: 293 [144960/225000 (64%)] Loss: 17861.115234\n",
      "Train Epoch: 293 [147456/225000 (66%)] Loss: 17567.617188\n",
      "Train Epoch: 293 [149952/225000 (67%)] Loss: 17270.085938\n",
      "Train Epoch: 293 [152448/225000 (68%)] Loss: 17376.480469\n",
      "Train Epoch: 293 [154944/225000 (69%)] Loss: 17362.554688\n",
      "Train Epoch: 293 [157440/225000 (70%)] Loss: 17157.566406\n",
      "Train Epoch: 293 [159936/225000 (71%)] Loss: 17275.730469\n",
      "Train Epoch: 293 [162432/225000 (72%)] Loss: 16937.039062\n",
      "Train Epoch: 293 [164928/225000 (73%)] Loss: 17513.482422\n",
      "Train Epoch: 293 [167424/225000 (74%)] Loss: 17271.171875\n",
      "Train Epoch: 293 [169920/225000 (76%)] Loss: 17635.058594\n",
      "Train Epoch: 293 [172416/225000 (77%)] Loss: 17251.439453\n",
      "Train Epoch: 293 [174912/225000 (78%)] Loss: 17027.080078\n",
      "Train Epoch: 293 [177408/225000 (79%)] Loss: 17393.605469\n",
      "Train Epoch: 293 [179904/225000 (80%)] Loss: 17493.642578\n",
      "Train Epoch: 293 [182400/225000 (81%)] Loss: 17452.574219\n",
      "Train Epoch: 293 [184896/225000 (82%)] Loss: 17199.763672\n",
      "Train Epoch: 293 [187392/225000 (83%)] Loss: 17154.283203\n",
      "Train Epoch: 293 [189888/225000 (84%)] Loss: 17146.148438\n",
      "Train Epoch: 293 [192384/225000 (86%)] Loss: 17053.500000\n",
      "Train Epoch: 293 [194880/225000 (87%)] Loss: 17357.820312\n",
      "Train Epoch: 293 [197376/225000 (88%)] Loss: 16929.242188\n",
      "Train Epoch: 293 [199872/225000 (89%)] Loss: 17620.968750\n",
      "Train Epoch: 293 [202368/225000 (90%)] Loss: 17674.740234\n",
      "Train Epoch: 293 [204864/225000 (91%)] Loss: 17522.902344\n",
      "Train Epoch: 293 [207360/225000 (92%)] Loss: 17700.660156\n",
      "Train Epoch: 293 [209856/225000 (93%)] Loss: 17865.550781\n",
      "Train Epoch: 293 [212352/225000 (94%)] Loss: 17189.261719\n",
      "Train Epoch: 293 [214848/225000 (95%)] Loss: 17060.746094\n",
      "Train Epoch: 293 [217344/225000 (97%)] Loss: 17184.179688\n",
      "Train Epoch: 293 [219840/225000 (98%)] Loss: 17544.859375\n",
      "Train Epoch: 293 [222336/225000 (99%)] Loss: 17433.558594\n",
      "Train Epoch: 293 [224832/225000 (100%)] Loss: 17144.261719\n",
      "    epoch          : 293\n",
      "    loss           : 17391.66755379426\n",
      "    val_loss       : 17291.225164016694\n",
      "Train Epoch: 294 [192/225000 (0%)] Loss: 16967.203125\n",
      "Train Epoch: 294 [2688/225000 (1%)] Loss: 17288.175781\n",
      "Train Epoch: 294 [5184/225000 (2%)] Loss: 17063.957031\n",
      "Train Epoch: 294 [7680/225000 (3%)] Loss: 18016.257812\n",
      "Train Epoch: 294 [10176/225000 (5%)] Loss: 17525.501953\n",
      "Train Epoch: 294 [12672/225000 (6%)] Loss: 17484.453125\n",
      "Train Epoch: 294 [15168/225000 (7%)] Loss: 17436.044922\n",
      "Train Epoch: 294 [17664/225000 (8%)] Loss: 17377.589844\n",
      "Train Epoch: 294 [20160/225000 (9%)] Loss: 17432.927734\n",
      "Train Epoch: 294 [22656/225000 (10%)] Loss: 17590.423828\n",
      "Train Epoch: 294 [25152/225000 (11%)] Loss: 17552.111328\n",
      "Train Epoch: 294 [27648/225000 (12%)] Loss: 17046.617188\n",
      "Train Epoch: 294 [30144/225000 (13%)] Loss: 17351.529297\n",
      "Train Epoch: 294 [32640/225000 (15%)] Loss: 17442.035156\n",
      "Train Epoch: 294 [35136/225000 (16%)] Loss: 17210.644531\n",
      "Train Epoch: 294 [37632/225000 (17%)] Loss: 17324.621094\n",
      "Train Epoch: 294 [40128/225000 (18%)] Loss: 17612.203125\n",
      "Train Epoch: 294 [42624/225000 (19%)] Loss: 17851.402344\n",
      "Train Epoch: 294 [45120/225000 (20%)] Loss: 17608.011719\n",
      "Train Epoch: 294 [47616/225000 (21%)] Loss: 17244.425781\n",
      "Train Epoch: 294 [50112/225000 (22%)] Loss: 17042.640625\n",
      "Train Epoch: 294 [52608/225000 (23%)] Loss: 17058.687500\n",
      "Train Epoch: 294 [55104/225000 (24%)] Loss: 17116.750000\n",
      "Train Epoch: 294 [57600/225000 (26%)] Loss: 17155.476562\n",
      "Train Epoch: 294 [60096/225000 (27%)] Loss: 17664.824219\n",
      "Train Epoch: 294 [62592/225000 (28%)] Loss: 17404.802734\n",
      "Train Epoch: 294 [65088/225000 (29%)] Loss: 17169.546875\n",
      "Train Epoch: 294 [67584/225000 (30%)] Loss: 17586.191406\n",
      "Train Epoch: 294 [70080/225000 (31%)] Loss: 16954.902344\n",
      "Train Epoch: 294 [72576/225000 (32%)] Loss: 17853.414062\n",
      "Train Epoch: 294 [75072/225000 (33%)] Loss: 17158.718750\n",
      "Train Epoch: 294 [77568/225000 (34%)] Loss: 17241.035156\n",
      "Train Epoch: 294 [80064/225000 (36%)] Loss: 17285.138672\n",
      "Train Epoch: 294 [82560/225000 (37%)] Loss: 17342.158203\n",
      "Train Epoch: 294 [85056/225000 (38%)] Loss: 17412.642578\n",
      "Train Epoch: 294 [87552/225000 (39%)] Loss: 17421.664062\n",
      "Train Epoch: 294 [90048/225000 (40%)] Loss: 17463.849609\n",
      "Train Epoch: 294 [92544/225000 (41%)] Loss: 16946.462891\n",
      "Train Epoch: 294 [95040/225000 (42%)] Loss: 17606.542969\n",
      "Train Epoch: 294 [97536/225000 (43%)] Loss: 16961.152344\n",
      "Train Epoch: 294 [100032/225000 (44%)] Loss: 17169.552734\n",
      "Train Epoch: 294 [102528/225000 (46%)] Loss: 17753.685547\n",
      "Train Epoch: 294 [105024/225000 (47%)] Loss: 17004.558594\n",
      "Train Epoch: 294 [107520/225000 (48%)] Loss: 17211.394531\n",
      "Train Epoch: 294 [110016/225000 (49%)] Loss: 17638.951172\n",
      "Train Epoch: 294 [112512/225000 (50%)] Loss: 17590.738281\n",
      "Train Epoch: 294 [115008/225000 (51%)] Loss: 17269.638672\n",
      "Train Epoch: 294 [117504/225000 (52%)] Loss: 17496.386719\n",
      "Train Epoch: 294 [120000/225000 (53%)] Loss: 17406.333984\n",
      "Train Epoch: 294 [122496/225000 (54%)] Loss: 17097.197266\n",
      "Train Epoch: 294 [124992/225000 (56%)] Loss: 17280.298828\n",
      "Train Epoch: 294 [127488/225000 (57%)] Loss: 17499.664062\n",
      "Train Epoch: 294 [129984/225000 (58%)] Loss: 17096.937500\n",
      "Train Epoch: 294 [132480/225000 (59%)] Loss: 17247.093750\n",
      "Train Epoch: 294 [134976/225000 (60%)] Loss: 17537.060547\n",
      "Train Epoch: 294 [137472/225000 (61%)] Loss: 17030.414062\n",
      "Train Epoch: 294 [139968/225000 (62%)] Loss: 17044.341797\n",
      "Train Epoch: 294 [142464/225000 (63%)] Loss: 17114.660156\n",
      "Train Epoch: 294 [144960/225000 (64%)] Loss: 17416.082031\n",
      "Train Epoch: 294 [147456/225000 (66%)] Loss: 17305.861328\n",
      "Train Epoch: 294 [149952/225000 (67%)] Loss: 17411.292969\n",
      "Train Epoch: 294 [152448/225000 (68%)] Loss: 17196.195312\n",
      "Train Epoch: 294 [154944/225000 (69%)] Loss: 17602.115234\n",
      "Train Epoch: 294 [157440/225000 (70%)] Loss: 17698.810547\n",
      "Train Epoch: 294 [159936/225000 (71%)] Loss: 17584.652344\n",
      "Train Epoch: 294 [162432/225000 (72%)] Loss: 17249.330078\n",
      "Train Epoch: 294 [164928/225000 (73%)] Loss: 17179.816406\n",
      "Train Epoch: 294 [167424/225000 (74%)] Loss: 17373.957031\n",
      "Train Epoch: 294 [169920/225000 (76%)] Loss: 17694.144531\n",
      "Train Epoch: 294 [172416/225000 (77%)] Loss: 17678.373047\n",
      "Train Epoch: 294 [174912/225000 (78%)] Loss: 17413.730469\n",
      "Train Epoch: 294 [177408/225000 (79%)] Loss: 18151.851562\n",
      "Train Epoch: 294 [179904/225000 (80%)] Loss: 17480.787109\n",
      "Train Epoch: 294 [182400/225000 (81%)] Loss: 17582.324219\n",
      "Train Epoch: 294 [184896/225000 (82%)] Loss: 17593.160156\n",
      "Train Epoch: 294 [187392/225000 (83%)] Loss: 17701.914062\n",
      "Train Epoch: 294 [189888/225000 (84%)] Loss: 17162.841797\n",
      "Train Epoch: 294 [192384/225000 (86%)] Loss: 17350.691406\n",
      "Train Epoch: 294 [194880/225000 (87%)] Loss: 17158.917969\n",
      "Train Epoch: 294 [197376/225000 (88%)] Loss: 17494.488281\n",
      "Train Epoch: 294 [199872/225000 (89%)] Loss: 17255.039062\n",
      "Train Epoch: 294 [202368/225000 (90%)] Loss: 17409.914062\n",
      "Train Epoch: 294 [204864/225000 (91%)] Loss: 17456.242188\n",
      "Train Epoch: 294 [207360/225000 (92%)] Loss: 18187.652344\n",
      "Train Epoch: 294 [209856/225000 (93%)] Loss: 17807.304688\n",
      "Train Epoch: 294 [212352/225000 (94%)] Loss: 17177.445312\n",
      "Train Epoch: 294 [214848/225000 (95%)] Loss: 17209.500000\n",
      "Train Epoch: 294 [217344/225000 (97%)] Loss: 17287.640625\n",
      "Train Epoch: 294 [219840/225000 (98%)] Loss: 17346.917969\n",
      "Train Epoch: 294 [222336/225000 (99%)] Loss: 17892.173828\n",
      "Train Epoch: 294 [224832/225000 (100%)] Loss: 17288.312500\n",
      "    epoch          : 294\n",
      "    loss           : 17368.519386265463\n",
      "    val_loss       : 17368.017577534414\n",
      "Train Epoch: 295 [192/225000 (0%)] Loss: 17368.505859\n",
      "Train Epoch: 295 [2688/225000 (1%)] Loss: 17524.363281\n",
      "Train Epoch: 295 [5184/225000 (2%)] Loss: 17292.902344\n",
      "Train Epoch: 295 [7680/225000 (3%)] Loss: 17294.652344\n",
      "Train Epoch: 295 [10176/225000 (5%)] Loss: 17173.167969\n",
      "Train Epoch: 295 [12672/225000 (6%)] Loss: 17606.550781\n",
      "Train Epoch: 295 [15168/225000 (7%)] Loss: 17661.296875\n",
      "Train Epoch: 295 [17664/225000 (8%)] Loss: 17313.316406\n",
      "Train Epoch: 295 [20160/225000 (9%)] Loss: 17439.255859\n",
      "Train Epoch: 295 [22656/225000 (10%)] Loss: 17257.087891\n",
      "Train Epoch: 295 [25152/225000 (11%)] Loss: 17805.648438\n",
      "Train Epoch: 295 [27648/225000 (12%)] Loss: 17680.287109\n",
      "Train Epoch: 295 [30144/225000 (13%)] Loss: 16756.796875\n",
      "Train Epoch: 295 [32640/225000 (15%)] Loss: 17233.464844\n",
      "Train Epoch: 295 [35136/225000 (16%)] Loss: 17412.644531\n",
      "Train Epoch: 295 [37632/225000 (17%)] Loss: 17471.339844\n",
      "Train Epoch: 295 [40128/225000 (18%)] Loss: 17627.439453\n",
      "Train Epoch: 295 [42624/225000 (19%)] Loss: 16810.638672\n",
      "Train Epoch: 295 [45120/225000 (20%)] Loss: 17308.492188\n",
      "Train Epoch: 295 [47616/225000 (21%)] Loss: 16905.445312\n",
      "Train Epoch: 295 [50112/225000 (22%)] Loss: 17495.453125\n",
      "Train Epoch: 295 [52608/225000 (23%)] Loss: 17044.761719\n",
      "Train Epoch: 295 [55104/225000 (24%)] Loss: 17587.347656\n",
      "Train Epoch: 295 [57600/225000 (26%)] Loss: 16777.140625\n",
      "Train Epoch: 295 [60096/225000 (27%)] Loss: 17862.595703\n",
      "Train Epoch: 295 [62592/225000 (28%)] Loss: 17638.511719\n",
      "Train Epoch: 295 [65088/225000 (29%)] Loss: 17701.316406\n",
      "Train Epoch: 295 [67584/225000 (30%)] Loss: 17528.593750\n",
      "Train Epoch: 295 [70080/225000 (31%)] Loss: 17251.064453\n",
      "Train Epoch: 295 [72576/225000 (32%)] Loss: 17065.707031\n",
      "Train Epoch: 295 [75072/225000 (33%)] Loss: 17249.546875\n",
      "Train Epoch: 295 [77568/225000 (34%)] Loss: 17104.312500\n",
      "Train Epoch: 295 [80064/225000 (36%)] Loss: 17355.828125\n",
      "Train Epoch: 295 [82560/225000 (37%)] Loss: 17438.675781\n",
      "Train Epoch: 295 [85056/225000 (38%)] Loss: 17877.800781\n",
      "Train Epoch: 295 [87552/225000 (39%)] Loss: 17202.660156\n",
      "Train Epoch: 295 [90048/225000 (40%)] Loss: 17313.218750\n",
      "Train Epoch: 295 [92544/225000 (41%)] Loss: 16821.644531\n",
      "Train Epoch: 295 [95040/225000 (42%)] Loss: 17797.960938\n",
      "Train Epoch: 295 [97536/225000 (43%)] Loss: 17456.976562\n",
      "Train Epoch: 295 [100032/225000 (44%)] Loss: 17675.015625\n",
      "Train Epoch: 295 [102528/225000 (46%)] Loss: 17380.937500\n",
      "Train Epoch: 295 [105024/225000 (47%)] Loss: 16880.054688\n",
      "Train Epoch: 295 [107520/225000 (48%)] Loss: 17458.867188\n",
      "Train Epoch: 295 [110016/225000 (49%)] Loss: 17673.617188\n",
      "Train Epoch: 295 [112512/225000 (50%)] Loss: 17584.945312\n",
      "Train Epoch: 295 [115008/225000 (51%)] Loss: 16714.957031\n",
      "Train Epoch: 295 [117504/225000 (52%)] Loss: 17541.632812\n",
      "Train Epoch: 295 [120000/225000 (53%)] Loss: 17689.457031\n",
      "Train Epoch: 295 [122496/225000 (54%)] Loss: 17351.769531\n",
      "Train Epoch: 295 [124992/225000 (56%)] Loss: 17514.746094\n",
      "Train Epoch: 295 [127488/225000 (57%)] Loss: 17296.724609\n",
      "Train Epoch: 295 [129984/225000 (58%)] Loss: 20180.031250\n",
      "Train Epoch: 295 [132480/225000 (59%)] Loss: 17345.511719\n",
      "Train Epoch: 295 [134976/225000 (60%)] Loss: 17825.562500\n",
      "Train Epoch: 295 [137472/225000 (61%)] Loss: 17516.367188\n",
      "Train Epoch: 295 [139968/225000 (62%)] Loss: 17740.191406\n",
      "Train Epoch: 295 [142464/225000 (63%)] Loss: 17512.738281\n",
      "Train Epoch: 295 [144960/225000 (64%)] Loss: 16941.039062\n",
      "Train Epoch: 295 [147456/225000 (66%)] Loss: 17165.646484\n",
      "Train Epoch: 295 [149952/225000 (67%)] Loss: 17107.132812\n",
      "Train Epoch: 295 [152448/225000 (68%)] Loss: 17262.550781\n",
      "Train Epoch: 295 [154944/225000 (69%)] Loss: 17502.771484\n",
      "Train Epoch: 295 [157440/225000 (70%)] Loss: 17201.839844\n",
      "Train Epoch: 295 [159936/225000 (71%)] Loss: 17213.085938\n",
      "Train Epoch: 295 [162432/225000 (72%)] Loss: 16976.628906\n",
      "Train Epoch: 295 [164928/225000 (73%)] Loss: 17255.246094\n",
      "Train Epoch: 295 [167424/225000 (74%)] Loss: 17719.369141\n",
      "Train Epoch: 295 [169920/225000 (76%)] Loss: 17853.937500\n",
      "Train Epoch: 295 [172416/225000 (77%)] Loss: 17538.208984\n",
      "Train Epoch: 295 [174912/225000 (78%)] Loss: 17146.386719\n",
      "Train Epoch: 295 [177408/225000 (79%)] Loss: 16923.425781\n",
      "Train Epoch: 295 [179904/225000 (80%)] Loss: 16932.257812\n",
      "Train Epoch: 295 [182400/225000 (81%)] Loss: 17478.076172\n",
      "Train Epoch: 295 [184896/225000 (82%)] Loss: 17376.726562\n",
      "Train Epoch: 295 [187392/225000 (83%)] Loss: 17439.060547\n",
      "Train Epoch: 295 [189888/225000 (84%)] Loss: 17229.820312\n",
      "Train Epoch: 295 [192384/225000 (86%)] Loss: 17347.541016\n",
      "Train Epoch: 295 [194880/225000 (87%)] Loss: 17386.375000\n",
      "Train Epoch: 295 [197376/225000 (88%)] Loss: 17984.265625\n",
      "Train Epoch: 295 [199872/225000 (89%)] Loss: 17436.132812\n",
      "Train Epoch: 295 [202368/225000 (90%)] Loss: 17723.542969\n",
      "Train Epoch: 295 [204864/225000 (91%)] Loss: 17380.328125\n",
      "Train Epoch: 295 [207360/225000 (92%)] Loss: 17353.675781\n",
      "Train Epoch: 295 [209856/225000 (93%)] Loss: 17498.921875\n",
      "Train Epoch: 295 [212352/225000 (94%)] Loss: 17556.835938\n",
      "Train Epoch: 295 [214848/225000 (95%)] Loss: 17252.230469\n",
      "Train Epoch: 295 [217344/225000 (97%)] Loss: 17081.201172\n",
      "Train Epoch: 295 [219840/225000 (98%)] Loss: 17360.970703\n",
      "Train Epoch: 295 [222336/225000 (99%)] Loss: 17181.714844\n",
      "Train Epoch: 295 [224832/225000 (100%)] Loss: 17297.126953\n",
      "    epoch          : 295\n",
      "    loss           : 17388.117296655022\n",
      "    val_loss       : 17384.51766390655\n",
      "Train Epoch: 296 [192/225000 (0%)] Loss: 17032.210938\n",
      "Train Epoch: 296 [2688/225000 (1%)] Loss: 17340.792969\n",
      "Train Epoch: 296 [5184/225000 (2%)] Loss: 17169.910156\n",
      "Train Epoch: 296 [7680/225000 (3%)] Loss: 17911.488281\n",
      "Train Epoch: 296 [10176/225000 (5%)] Loss: 17656.691406\n",
      "Train Epoch: 296 [12672/225000 (6%)] Loss: 17716.083984\n",
      "Train Epoch: 296 [15168/225000 (7%)] Loss: 17164.835938\n",
      "Train Epoch: 296 [17664/225000 (8%)] Loss: 17331.339844\n",
      "Train Epoch: 296 [20160/225000 (9%)] Loss: 17801.251953\n",
      "Train Epoch: 296 [22656/225000 (10%)] Loss: 17308.490234\n",
      "Train Epoch: 296 [25152/225000 (11%)] Loss: 17383.220703\n",
      "Train Epoch: 296 [27648/225000 (12%)] Loss: 17250.523438\n",
      "Train Epoch: 296 [30144/225000 (13%)] Loss: 17996.126953\n",
      "Train Epoch: 296 [32640/225000 (15%)] Loss: 17545.425781\n",
      "Train Epoch: 296 [35136/225000 (16%)] Loss: 17640.662109\n",
      "Train Epoch: 296 [37632/225000 (17%)] Loss: 16927.207031\n",
      "Train Epoch: 296 [40128/225000 (18%)] Loss: 17477.843750\n",
      "Train Epoch: 296 [42624/225000 (19%)] Loss: 17314.347656\n",
      "Train Epoch: 296 [45120/225000 (20%)] Loss: 16880.226562\n",
      "Train Epoch: 296 [47616/225000 (21%)] Loss: 17274.697266\n",
      "Train Epoch: 296 [50112/225000 (22%)] Loss: 17358.769531\n",
      "Train Epoch: 296 [52608/225000 (23%)] Loss: 17031.064453\n",
      "Train Epoch: 296 [55104/225000 (24%)] Loss: 17297.572266\n",
      "Train Epoch: 296 [57600/225000 (26%)] Loss: 16785.789062\n",
      "Train Epoch: 296 [60096/225000 (27%)] Loss: 16919.496094\n",
      "Train Epoch: 296 [62592/225000 (28%)] Loss: 17668.261719\n",
      "Train Epoch: 296 [65088/225000 (29%)] Loss: 17551.294922\n",
      "Train Epoch: 296 [67584/225000 (30%)] Loss: 17143.933594\n",
      "Train Epoch: 296 [70080/225000 (31%)] Loss: 17255.363281\n",
      "Train Epoch: 296 [72576/225000 (32%)] Loss: 17522.761719\n",
      "Train Epoch: 296 [75072/225000 (33%)] Loss: 17234.183594\n",
      "Train Epoch: 296 [77568/225000 (34%)] Loss: 17386.539062\n",
      "Train Epoch: 296 [80064/225000 (36%)] Loss: 17900.861328\n",
      "Train Epoch: 296 [82560/225000 (37%)] Loss: 17432.070312\n",
      "Train Epoch: 296 [85056/225000 (38%)] Loss: 17276.734375\n",
      "Train Epoch: 296 [87552/225000 (39%)] Loss: 17536.644531\n",
      "Train Epoch: 296 [90048/225000 (40%)] Loss: 17437.103516\n",
      "Train Epoch: 296 [92544/225000 (41%)] Loss: 17304.871094\n",
      "Train Epoch: 296 [95040/225000 (42%)] Loss: 17143.921875\n",
      "Train Epoch: 296 [97536/225000 (43%)] Loss: 17828.902344\n",
      "Train Epoch: 296 [100032/225000 (44%)] Loss: 17311.300781\n",
      "Train Epoch: 296 [102528/225000 (46%)] Loss: 17054.476562\n",
      "Train Epoch: 296 [105024/225000 (47%)] Loss: 17374.216797\n",
      "Train Epoch: 296 [107520/225000 (48%)] Loss: 17329.386719\n",
      "Train Epoch: 296 [110016/225000 (49%)] Loss: 16732.003906\n",
      "Train Epoch: 296 [112512/225000 (50%)] Loss: 17428.500000\n",
      "Train Epoch: 296 [115008/225000 (51%)] Loss: 17409.837891\n",
      "Train Epoch: 296 [117504/225000 (52%)] Loss: 17481.437500\n",
      "Train Epoch: 296 [120000/225000 (53%)] Loss: 17184.871094\n",
      "Train Epoch: 296 [122496/225000 (54%)] Loss: 17237.097656\n",
      "Train Epoch: 296 [124992/225000 (56%)] Loss: 17558.312500\n",
      "Train Epoch: 296 [127488/225000 (57%)] Loss: 17509.816406\n",
      "Train Epoch: 296 [129984/225000 (58%)] Loss: 17135.351562\n",
      "Train Epoch: 296 [132480/225000 (59%)] Loss: 16994.894531\n",
      "Train Epoch: 296 [134976/225000 (60%)] Loss: 17805.873047\n",
      "Train Epoch: 296 [137472/225000 (61%)] Loss: 16996.835938\n",
      "Train Epoch: 296 [139968/225000 (62%)] Loss: 17527.707031\n",
      "Train Epoch: 296 [142464/225000 (63%)] Loss: 17495.076172\n",
      "Train Epoch: 296 [144960/225000 (64%)] Loss: 17198.765625\n",
      "Train Epoch: 296 [147456/225000 (66%)] Loss: 17216.601562\n",
      "Train Epoch: 296 [149952/225000 (67%)] Loss: 17476.773438\n",
      "Train Epoch: 296 [152448/225000 (68%)] Loss: 17083.140625\n",
      "Train Epoch: 296 [154944/225000 (69%)] Loss: 17119.875000\n",
      "Train Epoch: 296 [157440/225000 (70%)] Loss: 18175.449219\n",
      "Train Epoch: 296 [159936/225000 (71%)] Loss: 17046.761719\n",
      "Train Epoch: 296 [162432/225000 (72%)] Loss: 17080.128906\n",
      "Train Epoch: 296 [164928/225000 (73%)] Loss: 17858.867188\n",
      "Train Epoch: 296 [167424/225000 (74%)] Loss: 17981.496094\n",
      "Train Epoch: 296 [169920/225000 (76%)] Loss: 17439.763672\n",
      "Train Epoch: 296 [172416/225000 (77%)] Loss: 17296.218750\n",
      "Train Epoch: 296 [174912/225000 (78%)] Loss: 17023.058594\n",
      "Train Epoch: 296 [177408/225000 (79%)] Loss: 16956.777344\n",
      "Train Epoch: 296 [179904/225000 (80%)] Loss: 17237.371094\n",
      "Train Epoch: 296 [182400/225000 (81%)] Loss: 17127.714844\n",
      "Train Epoch: 296 [184896/225000 (82%)] Loss: 17062.207031\n",
      "Train Epoch: 296 [187392/225000 (83%)] Loss: 17000.089844\n",
      "Train Epoch: 296 [189888/225000 (84%)] Loss: 17247.111328\n",
      "Train Epoch: 296 [192384/225000 (86%)] Loss: 17145.699219\n",
      "Train Epoch: 296 [194880/225000 (87%)] Loss: 17239.558594\n",
      "Train Epoch: 296 [197376/225000 (88%)] Loss: 17310.289062\n",
      "Train Epoch: 296 [199872/225000 (89%)] Loss: 17293.939453\n",
      "Train Epoch: 296 [202368/225000 (90%)] Loss: 17416.914062\n",
      "Train Epoch: 296 [204864/225000 (91%)] Loss: 17625.986328\n",
      "Train Epoch: 296 [207360/225000 (92%)] Loss: 17051.679688\n",
      "Train Epoch: 296 [209856/225000 (93%)] Loss: 17447.271484\n",
      "Train Epoch: 296 [212352/225000 (94%)] Loss: 17206.347656\n",
      "Train Epoch: 296 [214848/225000 (95%)] Loss: 17242.267578\n",
      "Train Epoch: 296 [217344/225000 (97%)] Loss: 17660.652344\n",
      "Train Epoch: 296 [219840/225000 (98%)] Loss: 16939.710938\n",
      "Train Epoch: 296 [222336/225000 (99%)] Loss: 17399.777344\n",
      "Train Epoch: 296 [224832/225000 (100%)] Loss: 17051.130859\n",
      "    epoch          : 296\n",
      "    loss           : 17362.979908809728\n",
      "    val_loss       : 17359.60872945986\n",
      "Train Epoch: 297 [192/225000 (0%)] Loss: 17299.054688\n",
      "Train Epoch: 297 [2688/225000 (1%)] Loss: 17462.785156\n",
      "Train Epoch: 297 [5184/225000 (2%)] Loss: 17407.716797\n",
      "Train Epoch: 297 [7680/225000 (3%)] Loss: 17198.722656\n",
      "Train Epoch: 297 [10176/225000 (5%)] Loss: 17691.566406\n",
      "Train Epoch: 297 [12672/225000 (6%)] Loss: 17433.451172\n",
      "Train Epoch: 297 [15168/225000 (7%)] Loss: 17066.722656\n",
      "Train Epoch: 297 [17664/225000 (8%)] Loss: 17216.935547\n",
      "Train Epoch: 297 [20160/225000 (9%)] Loss: 17579.906250\n",
      "Train Epoch: 297 [22656/225000 (10%)] Loss: 17261.445312\n",
      "Train Epoch: 297 [25152/225000 (11%)] Loss: 17206.875000\n",
      "Train Epoch: 297 [27648/225000 (12%)] Loss: 17292.679688\n",
      "Train Epoch: 297 [30144/225000 (13%)] Loss: 17223.648438\n",
      "Train Epoch: 297 [32640/225000 (15%)] Loss: 17041.738281\n",
      "Train Epoch: 297 [35136/225000 (16%)] Loss: 17299.660156\n",
      "Train Epoch: 297 [37632/225000 (17%)] Loss: 17322.203125\n",
      "Train Epoch: 297 [40128/225000 (18%)] Loss: 17481.236328\n",
      "Train Epoch: 297 [42624/225000 (19%)] Loss: 17386.828125\n",
      "Train Epoch: 297 [45120/225000 (20%)] Loss: 17161.804688\n",
      "Train Epoch: 297 [47616/225000 (21%)] Loss: 17622.578125\n",
      "Train Epoch: 297 [50112/225000 (22%)] Loss: 17783.355469\n",
      "Train Epoch: 297 [52608/225000 (23%)] Loss: 17438.582031\n",
      "Train Epoch: 297 [55104/225000 (24%)] Loss: 17615.105469\n",
      "Train Epoch: 297 [57600/225000 (26%)] Loss: 17244.123047\n",
      "Train Epoch: 297 [60096/225000 (27%)] Loss: 17338.337891\n",
      "Train Epoch: 297 [62592/225000 (28%)] Loss: 17405.277344\n",
      "Train Epoch: 297 [65088/225000 (29%)] Loss: 17114.007812\n",
      "Train Epoch: 297 [67584/225000 (30%)] Loss: 17247.164062\n",
      "Train Epoch: 297 [70080/225000 (31%)] Loss: 16592.566406\n",
      "Train Epoch: 297 [72576/225000 (32%)] Loss: 17057.117188\n",
      "Train Epoch: 297 [75072/225000 (33%)] Loss: 17466.507812\n",
      "Train Epoch: 297 [77568/225000 (34%)] Loss: 17330.779297\n",
      "Train Epoch: 297 [80064/225000 (36%)] Loss: 16995.490234\n",
      "Train Epoch: 297 [82560/225000 (37%)] Loss: 17347.609375\n",
      "Train Epoch: 297 [85056/225000 (38%)] Loss: 17282.708984\n",
      "Train Epoch: 297 [87552/225000 (39%)] Loss: 17277.173828\n",
      "Train Epoch: 297 [90048/225000 (40%)] Loss: 17931.099609\n",
      "Train Epoch: 297 [92544/225000 (41%)] Loss: 17440.914062\n",
      "Train Epoch: 297 [95040/225000 (42%)] Loss: 17281.410156\n",
      "Train Epoch: 297 [97536/225000 (43%)] Loss: 17344.949219\n",
      "Train Epoch: 297 [100032/225000 (44%)] Loss: 17611.269531\n",
      "Train Epoch: 297 [102528/225000 (46%)] Loss: 30722.078125\n",
      "Train Epoch: 297 [105024/225000 (47%)] Loss: 17933.132812\n",
      "Train Epoch: 297 [107520/225000 (48%)] Loss: 16809.371094\n",
      "Train Epoch: 297 [110016/225000 (49%)] Loss: 17325.769531\n",
      "Train Epoch: 297 [112512/225000 (50%)] Loss: 17210.039062\n",
      "Train Epoch: 297 [115008/225000 (51%)] Loss: 17555.728516\n",
      "Train Epoch: 297 [117504/225000 (52%)] Loss: 16811.644531\n",
      "Train Epoch: 297 [120000/225000 (53%)] Loss: 16873.947266\n",
      "Train Epoch: 297 [122496/225000 (54%)] Loss: 17625.097656\n",
      "Train Epoch: 297 [124992/225000 (56%)] Loss: 16984.072266\n",
      "Train Epoch: 297 [127488/225000 (57%)] Loss: 17328.484375\n",
      "Train Epoch: 297 [129984/225000 (58%)] Loss: 16994.054688\n",
      "Train Epoch: 297 [132480/225000 (59%)] Loss: 17681.134766\n",
      "Train Epoch: 297 [134976/225000 (60%)] Loss: 17030.886719\n",
      "Train Epoch: 297 [137472/225000 (61%)] Loss: 17013.812500\n",
      "Train Epoch: 297 [139968/225000 (62%)] Loss: 17574.464844\n",
      "Train Epoch: 297 [142464/225000 (63%)] Loss: 17570.474609\n",
      "Train Epoch: 297 [144960/225000 (64%)] Loss: 17725.207031\n",
      "Train Epoch: 297 [147456/225000 (66%)] Loss: 17087.382812\n",
      "Train Epoch: 297 [149952/225000 (67%)] Loss: 17630.625000\n",
      "Train Epoch: 297 [152448/225000 (68%)] Loss: 17459.763672\n",
      "Train Epoch: 297 [154944/225000 (69%)] Loss: 17520.441406\n",
      "Train Epoch: 297 [157440/225000 (70%)] Loss: 17080.113281\n",
      "Train Epoch: 297 [159936/225000 (71%)] Loss: 17483.785156\n",
      "Train Epoch: 297 [162432/225000 (72%)] Loss: 17027.191406\n",
      "Train Epoch: 297 [164928/225000 (73%)] Loss: 17468.593750\n",
      "Train Epoch: 297 [167424/225000 (74%)] Loss: 17039.113281\n",
      "Train Epoch: 297 [169920/225000 (76%)] Loss: 17422.175781\n",
      "Train Epoch: 297 [172416/225000 (77%)] Loss: 16878.402344\n",
      "Train Epoch: 297 [174912/225000 (78%)] Loss: 17276.828125\n",
      "Train Epoch: 297 [177408/225000 (79%)] Loss: 17791.097656\n",
      "Train Epoch: 297 [179904/225000 (80%)] Loss: 17616.152344\n",
      "Train Epoch: 297 [182400/225000 (81%)] Loss: 17530.767578\n",
      "Train Epoch: 297 [184896/225000 (82%)] Loss: 17318.929688\n",
      "Train Epoch: 297 [187392/225000 (83%)] Loss: 17071.695312\n",
      "Train Epoch: 297 [189888/225000 (84%)] Loss: 17224.472656\n",
      "Train Epoch: 297 [192384/225000 (86%)] Loss: 17207.941406\n",
      "Train Epoch: 297 [194880/225000 (87%)] Loss: 17175.730469\n",
      "Train Epoch: 297 [197376/225000 (88%)] Loss: 17628.906250\n",
      "Train Epoch: 297 [199872/225000 (89%)] Loss: 16961.546875\n",
      "Train Epoch: 297 [202368/225000 (90%)] Loss: 17665.525391\n",
      "Train Epoch: 297 [204864/225000 (91%)] Loss: 17406.492188\n",
      "Train Epoch: 297 [207360/225000 (92%)] Loss: 17547.136719\n",
      "Train Epoch: 297 [209856/225000 (93%)] Loss: 17268.410156\n",
      "Train Epoch: 297 [212352/225000 (94%)] Loss: 17480.210938\n",
      "Train Epoch: 297 [214848/225000 (95%)] Loss: 17132.285156\n",
      "Train Epoch: 297 [217344/225000 (97%)] Loss: 17013.468750\n",
      "Train Epoch: 297 [219840/225000 (98%)] Loss: 17708.152344\n",
      "Train Epoch: 297 [222336/225000 (99%)] Loss: 17203.785156\n",
      "Train Epoch: 297 [224832/225000 (100%)] Loss: 17525.046875\n",
      "    epoch          : 297\n",
      "    loss           : 17357.983090970298\n",
      "    val_loss       : 17270.931968607067\n",
      "Train Epoch: 298 [192/225000 (0%)] Loss: 17316.902344\n",
      "Train Epoch: 298 [2688/225000 (1%)] Loss: 17810.613281\n",
      "Train Epoch: 298 [5184/225000 (2%)] Loss: 17010.980469\n",
      "Train Epoch: 298 [7680/225000 (3%)] Loss: 16934.634766\n",
      "Train Epoch: 298 [10176/225000 (5%)] Loss: 17104.896484\n",
      "Train Epoch: 298 [12672/225000 (6%)] Loss: 17027.861328\n",
      "Train Epoch: 298 [15168/225000 (7%)] Loss: 17547.740234\n",
      "Train Epoch: 298 [17664/225000 (8%)] Loss: 17235.335938\n",
      "Train Epoch: 298 [20160/225000 (9%)] Loss: 17618.203125\n",
      "Train Epoch: 298 [22656/225000 (10%)] Loss: 17884.742188\n",
      "Train Epoch: 298 [25152/225000 (11%)] Loss: 17281.111328\n",
      "Train Epoch: 298 [27648/225000 (12%)] Loss: 17231.054688\n",
      "Train Epoch: 298 [30144/225000 (13%)] Loss: 17544.982422\n",
      "Train Epoch: 298 [32640/225000 (15%)] Loss: 17409.904297\n",
      "Train Epoch: 298 [35136/225000 (16%)] Loss: 16970.775391\n",
      "Train Epoch: 298 [37632/225000 (17%)] Loss: 17066.482422\n",
      "Train Epoch: 298 [40128/225000 (18%)] Loss: 16746.626953\n",
      "Train Epoch: 298 [42624/225000 (19%)] Loss: 17326.259766\n",
      "Train Epoch: 298 [45120/225000 (20%)] Loss: 17434.828125\n",
      "Train Epoch: 298 [47616/225000 (21%)] Loss: 17328.767578\n",
      "Train Epoch: 298 [50112/225000 (22%)] Loss: 17228.355469\n",
      "Train Epoch: 298 [52608/225000 (23%)] Loss: 17297.460938\n",
      "Train Epoch: 298 [55104/225000 (24%)] Loss: 17061.878906\n",
      "Train Epoch: 298 [57600/225000 (26%)] Loss: 17246.587891\n",
      "Train Epoch: 298 [60096/225000 (27%)] Loss: 17644.484375\n",
      "Train Epoch: 298 [62592/225000 (28%)] Loss: 17422.156250\n",
      "Train Epoch: 298 [65088/225000 (29%)] Loss: 17717.867188\n",
      "Train Epoch: 298 [67584/225000 (30%)] Loss: 17116.949219\n",
      "Train Epoch: 298 [70080/225000 (31%)] Loss: 16893.058594\n",
      "Train Epoch: 298 [72576/225000 (32%)] Loss: 17056.416016\n",
      "Train Epoch: 298 [75072/225000 (33%)] Loss: 17356.248047\n",
      "Train Epoch: 298 [77568/225000 (34%)] Loss: 17785.871094\n",
      "Train Epoch: 298 [80064/225000 (36%)] Loss: 17081.730469\n",
      "Train Epoch: 298 [82560/225000 (37%)] Loss: 17386.638672\n",
      "Train Epoch: 298 [85056/225000 (38%)] Loss: 17672.826172\n",
      "Train Epoch: 298 [87552/225000 (39%)] Loss: 17480.351562\n",
      "Train Epoch: 298 [90048/225000 (40%)] Loss: 17555.414062\n",
      "Train Epoch: 298 [92544/225000 (41%)] Loss: 17533.359375\n",
      "Train Epoch: 298 [95040/225000 (42%)] Loss: 17511.470703\n",
      "Train Epoch: 298 [97536/225000 (43%)] Loss: 17942.933594\n",
      "Train Epoch: 298 [100032/225000 (44%)] Loss: 17639.160156\n",
      "Train Epoch: 298 [102528/225000 (46%)] Loss: 17031.289062\n",
      "Train Epoch: 298 [105024/225000 (47%)] Loss: 17236.957031\n",
      "Train Epoch: 298 [107520/225000 (48%)] Loss: 16965.429688\n",
      "Train Epoch: 298 [110016/225000 (49%)] Loss: 17274.988281\n",
      "Train Epoch: 298 [112512/225000 (50%)] Loss: 16988.699219\n",
      "Train Epoch: 298 [115008/225000 (51%)] Loss: 17742.337891\n",
      "Train Epoch: 298 [117504/225000 (52%)] Loss: 17340.271484\n",
      "Train Epoch: 298 [120000/225000 (53%)] Loss: 17188.339844\n",
      "Train Epoch: 298 [122496/225000 (54%)] Loss: 17226.218750\n",
      "Train Epoch: 298 [124992/225000 (56%)] Loss: 17508.179688\n",
      "Train Epoch: 298 [127488/225000 (57%)] Loss: 17626.695312\n",
      "Train Epoch: 298 [129984/225000 (58%)] Loss: 17006.441406\n",
      "Train Epoch: 298 [132480/225000 (59%)] Loss: 17582.828125\n",
      "Train Epoch: 298 [134976/225000 (60%)] Loss: 16799.761719\n",
      "Train Epoch: 298 [137472/225000 (61%)] Loss: 17515.343750\n",
      "Train Epoch: 298 [139968/225000 (62%)] Loss: 17473.515625\n",
      "Train Epoch: 298 [142464/225000 (63%)] Loss: 16984.402344\n",
      "Train Epoch: 298 [144960/225000 (64%)] Loss: 17312.925781\n",
      "Train Epoch: 298 [147456/225000 (66%)] Loss: 17628.519531\n",
      "Train Epoch: 298 [149952/225000 (67%)] Loss: 17364.632812\n",
      "Train Epoch: 298 [152448/225000 (68%)] Loss: 17643.765625\n",
      "Train Epoch: 298 [154944/225000 (69%)] Loss: 17291.148438\n",
      "Train Epoch: 298 [157440/225000 (70%)] Loss: 17459.712891\n",
      "Train Epoch: 298 [159936/225000 (71%)] Loss: 17176.398438\n",
      "Train Epoch: 298 [162432/225000 (72%)] Loss: 17537.960938\n",
      "Train Epoch: 298 [164928/225000 (73%)] Loss: 17061.492188\n",
      "Train Epoch: 298 [167424/225000 (74%)] Loss: 17225.136719\n",
      "Train Epoch: 298 [169920/225000 (76%)] Loss: 17333.359375\n",
      "Train Epoch: 298 [172416/225000 (77%)] Loss: 17120.898438\n",
      "Train Epoch: 298 [174912/225000 (78%)] Loss: 18024.091797\n",
      "Train Epoch: 298 [177408/225000 (79%)] Loss: 17273.894531\n",
      "Train Epoch: 298 [179904/225000 (80%)] Loss: 17251.201172\n",
      "Train Epoch: 298 [182400/225000 (81%)] Loss: 17246.546875\n",
      "Train Epoch: 298 [184896/225000 (82%)] Loss: 17734.730469\n",
      "Train Epoch: 298 [187392/225000 (83%)] Loss: 17146.089844\n",
      "Train Epoch: 298 [189888/225000 (84%)] Loss: 16757.425781\n",
      "Train Epoch: 298 [192384/225000 (86%)] Loss: 17497.687500\n",
      "Train Epoch: 298 [194880/225000 (87%)] Loss: 17796.730469\n",
      "Train Epoch: 298 [197376/225000 (88%)] Loss: 17464.410156\n",
      "Train Epoch: 298 [199872/225000 (89%)] Loss: 17363.259766\n",
      "Train Epoch: 298 [202368/225000 (90%)] Loss: 17842.720703\n",
      "Train Epoch: 298 [204864/225000 (91%)] Loss: 17157.914062\n",
      "Train Epoch: 298 [207360/225000 (92%)] Loss: 17651.201172\n",
      "Train Epoch: 298 [209856/225000 (93%)] Loss: 17515.097656\n",
      "Train Epoch: 298 [212352/225000 (94%)] Loss: 17074.160156\n",
      "Train Epoch: 298 [214848/225000 (95%)] Loss: 17662.542969\n",
      "Train Epoch: 298 [217344/225000 (97%)] Loss: 17338.750000\n",
      "Train Epoch: 298 [219840/225000 (98%)] Loss: 18840.033203\n",
      "Train Epoch: 298 [222336/225000 (99%)] Loss: 17420.484375\n",
      "Train Epoch: 298 [224832/225000 (100%)] Loss: 17113.769531\n",
      "    epoch          : 298\n",
      "    loss           : 17358.290460684195\n",
      "    val_loss       : 17324.321470647366\n",
      "Train Epoch: 299 [192/225000 (0%)] Loss: 17354.242188\n",
      "Train Epoch: 299 [2688/225000 (1%)] Loss: 17343.609375\n",
      "Train Epoch: 299 [5184/225000 (2%)] Loss: 17476.126953\n",
      "Train Epoch: 299 [7680/225000 (3%)] Loss: 17503.406250\n",
      "Train Epoch: 299 [10176/225000 (5%)] Loss: 17653.031250\n",
      "Train Epoch: 299 [12672/225000 (6%)] Loss: 17103.373047\n",
      "Train Epoch: 299 [15168/225000 (7%)] Loss: 17691.451172\n",
      "Train Epoch: 299 [17664/225000 (8%)] Loss: 17018.183594\n",
      "Train Epoch: 299 [20160/225000 (9%)] Loss: 17858.605469\n",
      "Train Epoch: 299 [22656/225000 (10%)] Loss: 17536.714844\n",
      "Train Epoch: 299 [25152/225000 (11%)] Loss: 17752.328125\n",
      "Train Epoch: 299 [27648/225000 (12%)] Loss: 16953.019531\n",
      "Train Epoch: 299 [30144/225000 (13%)] Loss: 17477.398438\n",
      "Train Epoch: 299 [32640/225000 (15%)] Loss: 17255.605469\n",
      "Train Epoch: 299 [35136/225000 (16%)] Loss: 17720.261719\n",
      "Train Epoch: 299 [37632/225000 (17%)] Loss: 16655.601562\n",
      "Train Epoch: 299 [40128/225000 (18%)] Loss: 17268.117188\n",
      "Train Epoch: 299 [42624/225000 (19%)] Loss: 17122.296875\n",
      "Train Epoch: 299 [45120/225000 (20%)] Loss: 17247.539062\n",
      "Train Epoch: 299 [47616/225000 (21%)] Loss: 17197.761719\n",
      "Train Epoch: 299 [50112/225000 (22%)] Loss: 16688.638672\n",
      "Train Epoch: 299 [52608/225000 (23%)] Loss: 17326.574219\n",
      "Train Epoch: 299 [55104/225000 (24%)] Loss: 17677.435547\n",
      "Train Epoch: 299 [57600/225000 (26%)] Loss: 17726.677734\n",
      "Train Epoch: 299 [60096/225000 (27%)] Loss: 17203.460938\n",
      "Train Epoch: 299 [62592/225000 (28%)] Loss: 17461.431641\n",
      "Train Epoch: 299 [65088/225000 (29%)] Loss: 17930.476562\n",
      "Train Epoch: 299 [67584/225000 (30%)] Loss: 17545.812500\n",
      "Train Epoch: 299 [70080/225000 (31%)] Loss: 17543.324219\n",
      "Train Epoch: 299 [72576/225000 (32%)] Loss: 17569.894531\n",
      "Train Epoch: 299 [75072/225000 (33%)] Loss: 16800.302734\n",
      "Train Epoch: 299 [77568/225000 (34%)] Loss: 17364.962891\n",
      "Train Epoch: 299 [80064/225000 (36%)] Loss: 17128.281250\n",
      "Train Epoch: 299 [82560/225000 (37%)] Loss: 17681.566406\n",
      "Train Epoch: 299 [85056/225000 (38%)] Loss: 17376.289062\n",
      "Train Epoch: 299 [87552/225000 (39%)] Loss: 16735.308594\n",
      "Train Epoch: 299 [90048/225000 (40%)] Loss: 17526.082031\n",
      "Train Epoch: 299 [92544/225000 (41%)] Loss: 17477.144531\n",
      "Train Epoch: 299 [95040/225000 (42%)] Loss: 17175.863281\n",
      "Train Epoch: 299 [97536/225000 (43%)] Loss: 17140.484375\n",
      "Train Epoch: 299 [100032/225000 (44%)] Loss: 17575.089844\n",
      "Train Epoch: 299 [102528/225000 (46%)] Loss: 17417.890625\n",
      "Train Epoch: 299 [105024/225000 (47%)] Loss: 17596.937500\n",
      "Train Epoch: 299 [107520/225000 (48%)] Loss: 17543.357422\n",
      "Train Epoch: 299 [110016/225000 (49%)] Loss: 16994.445312\n",
      "Train Epoch: 299 [112512/225000 (50%)] Loss: 17370.589844\n",
      "Train Epoch: 299 [115008/225000 (51%)] Loss: 17400.205078\n",
      "Train Epoch: 299 [117504/225000 (52%)] Loss: 16980.642578\n",
      "Train Epoch: 299 [120000/225000 (53%)] Loss: 17446.460938\n",
      "Train Epoch: 299 [122496/225000 (54%)] Loss: 17062.806641\n",
      "Train Epoch: 299 [124992/225000 (56%)] Loss: 17233.035156\n",
      "Train Epoch: 299 [127488/225000 (57%)] Loss: 17840.785156\n",
      "Train Epoch: 299 [129984/225000 (58%)] Loss: 17234.316406\n",
      "Train Epoch: 299 [132480/225000 (59%)] Loss: 16799.158203\n",
      "Train Epoch: 299 [134976/225000 (60%)] Loss: 17614.488281\n",
      "Train Epoch: 299 [137472/225000 (61%)] Loss: 17729.970703\n",
      "Train Epoch: 299 [139968/225000 (62%)] Loss: 17075.507812\n",
      "Train Epoch: 299 [142464/225000 (63%)] Loss: 17454.832031\n",
      "Train Epoch: 299 [144960/225000 (64%)] Loss: 17185.382812\n",
      "Train Epoch: 299 [147456/225000 (66%)] Loss: 17699.421875\n",
      "Train Epoch: 299 [149952/225000 (67%)] Loss: 17384.539062\n",
      "Train Epoch: 299 [152448/225000 (68%)] Loss: 17138.619141\n",
      "Train Epoch: 299 [154944/225000 (69%)] Loss: 17348.527344\n",
      "Train Epoch: 299 [157440/225000 (70%)] Loss: 17546.871094\n",
      "Train Epoch: 299 [159936/225000 (71%)] Loss: 17045.621094\n",
      "Train Epoch: 299 [162432/225000 (72%)] Loss: 17472.257812\n",
      "Train Epoch: 299 [164928/225000 (73%)] Loss: 17525.558594\n",
      "Train Epoch: 299 [167424/225000 (74%)] Loss: 17431.125000\n",
      "Train Epoch: 299 [169920/225000 (76%)] Loss: 17244.031250\n",
      "Train Epoch: 299 [172416/225000 (77%)] Loss: 17354.458984\n",
      "Train Epoch: 299 [174912/225000 (78%)] Loss: 16926.335938\n",
      "Train Epoch: 299 [177408/225000 (79%)] Loss: 17065.406250\n",
      "Train Epoch: 299 [179904/225000 (80%)] Loss: 17423.718750\n",
      "Train Epoch: 299 [182400/225000 (81%)] Loss: 16891.478516\n",
      "Train Epoch: 299 [184896/225000 (82%)] Loss: 17085.589844\n",
      "Train Epoch: 299 [187392/225000 (83%)] Loss: 17172.662109\n",
      "Train Epoch: 299 [189888/225000 (84%)] Loss: 17024.335938\n",
      "Train Epoch: 299 [192384/225000 (86%)] Loss: 17785.992188\n",
      "Train Epoch: 299 [194880/225000 (87%)] Loss: 16761.742188\n",
      "Train Epoch: 299 [197376/225000 (88%)] Loss: 17352.921875\n",
      "Train Epoch: 299 [199872/225000 (89%)] Loss: 17521.058594\n",
      "Train Epoch: 299 [202368/225000 (90%)] Loss: 17120.968750\n",
      "Train Epoch: 299 [204864/225000 (91%)] Loss: 17087.496094\n",
      "Train Epoch: 299 [207360/225000 (92%)] Loss: 17512.230469\n",
      "Train Epoch: 299 [209856/225000 (93%)] Loss: 17527.224609\n",
      "Train Epoch: 299 [212352/225000 (94%)] Loss: 17350.171875\n",
      "Train Epoch: 299 [214848/225000 (95%)] Loss: 17150.521484\n",
      "Train Epoch: 299 [217344/225000 (97%)] Loss: 17194.812500\n",
      "Train Epoch: 299 [219840/225000 (98%)] Loss: 17233.792969\n",
      "Train Epoch: 299 [222336/225000 (99%)] Loss: 16905.494141\n",
      "Train Epoch: 299 [224832/225000 (100%)] Loss: 17781.949219\n",
      "    epoch          : 299\n",
      "    loss           : 17332.439403963577\n",
      "    val_loss       : 17255.359821725437\n",
      "Train Epoch: 300 [192/225000 (0%)] Loss: 17163.328125\n",
      "Train Epoch: 300 [2688/225000 (1%)] Loss: 17117.001953\n",
      "Train Epoch: 300 [5184/225000 (2%)] Loss: 17613.585938\n",
      "Train Epoch: 300 [7680/225000 (3%)] Loss: 17146.242188\n",
      "Train Epoch: 300 [10176/225000 (5%)] Loss: 17435.527344\n",
      "Train Epoch: 300 [12672/225000 (6%)] Loss: 17216.900391\n",
      "Train Epoch: 300 [15168/225000 (7%)] Loss: 17627.398438\n",
      "Train Epoch: 300 [17664/225000 (8%)] Loss: 17318.187500\n",
      "Train Epoch: 300 [20160/225000 (9%)] Loss: 17152.269531\n",
      "Train Epoch: 300 [22656/225000 (10%)] Loss: 17779.113281\n",
      "Train Epoch: 300 [25152/225000 (11%)] Loss: 17413.964844\n",
      "Train Epoch: 300 [27648/225000 (12%)] Loss: 17395.822266\n",
      "Train Epoch: 300 [30144/225000 (13%)] Loss: 17006.054688\n",
      "Train Epoch: 300 [32640/225000 (15%)] Loss: 17971.109375\n",
      "Train Epoch: 300 [35136/225000 (16%)] Loss: 17313.546875\n",
      "Train Epoch: 300 [37632/225000 (17%)] Loss: 17372.203125\n",
      "Train Epoch: 300 [40128/225000 (18%)] Loss: 17833.726562\n",
      "Train Epoch: 300 [42624/225000 (19%)] Loss: 17244.527344\n",
      "Train Epoch: 300 [45120/225000 (20%)] Loss: 17148.468750\n",
      "Train Epoch: 300 [47616/225000 (21%)] Loss: 17264.457031\n",
      "Train Epoch: 300 [50112/225000 (22%)] Loss: 16936.132812\n",
      "Train Epoch: 300 [52608/225000 (23%)] Loss: 16827.300781\n",
      "Train Epoch: 300 [55104/225000 (24%)] Loss: 16741.898438\n",
      "Train Epoch: 300 [57600/225000 (26%)] Loss: 17055.396484\n",
      "Train Epoch: 300 [60096/225000 (27%)] Loss: 17246.804688\n",
      "Train Epoch: 300 [62592/225000 (28%)] Loss: 17761.232422\n",
      "Train Epoch: 300 [65088/225000 (29%)] Loss: 17392.861328\n",
      "Train Epoch: 300 [67584/225000 (30%)] Loss: 17281.937500\n",
      "Train Epoch: 300 [70080/225000 (31%)] Loss: 17205.662109\n",
      "Train Epoch: 300 [72576/225000 (32%)] Loss: 16915.703125\n",
      "Train Epoch: 300 [75072/225000 (33%)] Loss: 17008.652344\n",
      "Train Epoch: 300 [77568/225000 (34%)] Loss: 17358.726562\n",
      "Train Epoch: 300 [80064/225000 (36%)] Loss: 17268.494141\n",
      "Train Epoch: 300 [82560/225000 (37%)] Loss: 17237.263672\n",
      "Train Epoch: 300 [85056/225000 (38%)] Loss: 17348.500000\n",
      "Train Epoch: 300 [87552/225000 (39%)] Loss: 16969.976562\n",
      "Train Epoch: 300 [90048/225000 (40%)] Loss: 17350.490234\n",
      "Train Epoch: 300 [92544/225000 (41%)] Loss: 17342.019531\n",
      "Train Epoch: 300 [95040/225000 (42%)] Loss: 17592.828125\n",
      "Train Epoch: 300 [97536/225000 (43%)] Loss: 17739.519531\n",
      "Train Epoch: 300 [100032/225000 (44%)] Loss: 17118.679688\n",
      "Train Epoch: 300 [102528/225000 (46%)] Loss: 17125.691406\n",
      "Train Epoch: 300 [105024/225000 (47%)] Loss: 16970.824219\n",
      "Train Epoch: 300 [107520/225000 (48%)] Loss: 17209.027344\n",
      "Train Epoch: 300 [110016/225000 (49%)] Loss: 17186.765625\n",
      "Train Epoch: 300 [112512/225000 (50%)] Loss: 16890.460938\n",
      "Train Epoch: 300 [115008/225000 (51%)] Loss: 16910.275391\n",
      "Train Epoch: 300 [117504/225000 (52%)] Loss: 17207.164062\n",
      "Train Epoch: 300 [120000/225000 (53%)] Loss: 17479.324219\n",
      "Train Epoch: 300 [122496/225000 (54%)] Loss: 17718.089844\n",
      "Train Epoch: 300 [124992/225000 (56%)] Loss: 17221.304688\n",
      "Train Epoch: 300 [127488/225000 (57%)] Loss: 17565.832031\n",
      "Train Epoch: 300 [129984/225000 (58%)] Loss: 16770.789062\n",
      "Train Epoch: 300 [132480/225000 (59%)] Loss: 17251.136719\n",
      "Train Epoch: 300 [134976/225000 (60%)] Loss: 17408.726562\n",
      "Train Epoch: 300 [137472/225000 (61%)] Loss: 16972.941406\n",
      "Train Epoch: 300 [139968/225000 (62%)] Loss: 17711.394531\n",
      "Train Epoch: 300 [142464/225000 (63%)] Loss: 17242.177734\n",
      "Train Epoch: 300 [144960/225000 (64%)] Loss: 17139.550781\n",
      "Train Epoch: 300 [147456/225000 (66%)] Loss: 17511.699219\n",
      "Train Epoch: 300 [149952/225000 (67%)] Loss: 17758.359375\n",
      "Train Epoch: 300 [152448/225000 (68%)] Loss: 17606.367188\n",
      "Train Epoch: 300 [154944/225000 (69%)] Loss: 17135.779297\n",
      "Train Epoch: 300 [157440/225000 (70%)] Loss: 17386.482422\n",
      "Train Epoch: 300 [159936/225000 (71%)] Loss: 16993.941406\n",
      "Train Epoch: 300 [162432/225000 (72%)] Loss: 17100.195312\n",
      "Train Epoch: 300 [164928/225000 (73%)] Loss: 18136.519531\n",
      "Train Epoch: 300 [167424/225000 (74%)] Loss: 17314.117188\n",
      "Train Epoch: 300 [169920/225000 (76%)] Loss: 17775.902344\n",
      "Train Epoch: 300 [172416/225000 (77%)] Loss: 17398.144531\n",
      "Train Epoch: 300 [174912/225000 (78%)] Loss: 17221.324219\n",
      "Train Epoch: 300 [177408/225000 (79%)] Loss: 17227.134766\n",
      "Train Epoch: 300 [179904/225000 (80%)] Loss: 17054.148438\n",
      "Train Epoch: 300 [182400/225000 (81%)] Loss: 17163.666016\n",
      "Train Epoch: 300 [184896/225000 (82%)] Loss: 16971.068359\n",
      "Train Epoch: 300 [187392/225000 (83%)] Loss: 17344.820312\n",
      "Train Epoch: 300 [189888/225000 (84%)] Loss: 17523.433594\n",
      "Train Epoch: 300 [192384/225000 (86%)] Loss: 17525.218750\n",
      "Train Epoch: 300 [194880/225000 (87%)] Loss: 17215.828125\n",
      "Train Epoch: 300 [197376/225000 (88%)] Loss: 17281.433594\n",
      "Train Epoch: 300 [199872/225000 (89%)] Loss: 17743.498047\n",
      "Train Epoch: 300 [202368/225000 (90%)] Loss: 17163.623047\n",
      "Train Epoch: 300 [204864/225000 (91%)] Loss: 17154.107422\n",
      "Train Epoch: 300 [207360/225000 (92%)] Loss: 17433.414062\n",
      "Train Epoch: 300 [209856/225000 (93%)] Loss: 17188.144531\n",
      "Train Epoch: 300 [212352/225000 (94%)] Loss: 17420.419922\n",
      "Train Epoch: 300 [214848/225000 (95%)] Loss: 17453.619141\n",
      "Train Epoch: 300 [217344/225000 (97%)] Loss: 17284.273438\n",
      "Train Epoch: 300 [219840/225000 (98%)] Loss: 18058.746094\n",
      "Train Epoch: 300 [222336/225000 (99%)] Loss: 17408.896484\n",
      "Train Epoch: 300 [224832/225000 (100%)] Loss: 17452.435547\n",
      "    epoch          : 300\n",
      "    loss           : 17316.270306167342\n",
      "    val_loss       : 17261.198323358105\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0804_123131/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [192/225000 (0%)] Loss: 16978.085938\n",
      "Train Epoch: 301 [2688/225000 (1%)] Loss: 17537.578125\n",
      "Train Epoch: 301 [5184/225000 (2%)] Loss: 17561.035156\n",
      "Train Epoch: 301 [7680/225000 (3%)] Loss: 17110.167969\n",
      "Train Epoch: 301 [10176/225000 (5%)] Loss: 17067.085938\n",
      "Train Epoch: 301 [12672/225000 (6%)] Loss: 17290.910156\n",
      "Train Epoch: 301 [15168/225000 (7%)] Loss: 17322.027344\n",
      "Train Epoch: 301 [17664/225000 (8%)] Loss: 17766.613281\n",
      "Train Epoch: 301 [20160/225000 (9%)] Loss: 17341.136719\n",
      "Train Epoch: 301 [22656/225000 (10%)] Loss: 17570.716797\n",
      "Train Epoch: 301 [25152/225000 (11%)] Loss: 17302.507812\n",
      "Train Epoch: 301 [27648/225000 (12%)] Loss: 17189.894531\n",
      "Train Epoch: 301 [30144/225000 (13%)] Loss: 17012.160156\n",
      "Train Epoch: 301 [32640/225000 (15%)] Loss: 17277.333984\n",
      "Train Epoch: 301 [35136/225000 (16%)] Loss: 17375.621094\n",
      "Train Epoch: 301 [37632/225000 (17%)] Loss: 17640.371094\n",
      "Train Epoch: 301 [40128/225000 (18%)] Loss: 17690.707031\n",
      "Train Epoch: 301 [42624/225000 (19%)] Loss: 17115.490234\n",
      "Train Epoch: 301 [45120/225000 (20%)] Loss: 17268.652344\n",
      "Train Epoch: 301 [47616/225000 (21%)] Loss: 17692.003906\n",
      "Train Epoch: 301 [50112/225000 (22%)] Loss: 17151.062500\n",
      "Train Epoch: 301 [52608/225000 (23%)] Loss: 17388.808594\n",
      "Train Epoch: 301 [55104/225000 (24%)] Loss: 16896.804688\n",
      "Train Epoch: 301 [57600/225000 (26%)] Loss: 17102.593750\n",
      "Train Epoch: 301 [60096/225000 (27%)] Loss: 17923.755859\n",
      "Train Epoch: 301 [62592/225000 (28%)] Loss: 17467.117188\n",
      "Train Epoch: 301 [65088/225000 (29%)] Loss: 17097.451172\n",
      "Train Epoch: 301 [67584/225000 (30%)] Loss: 17399.292969\n",
      "Train Epoch: 301 [70080/225000 (31%)] Loss: 17176.527344\n",
      "Train Epoch: 301 [72576/225000 (32%)] Loss: 17233.882812\n",
      "Train Epoch: 301 [75072/225000 (33%)] Loss: 17529.710938\n",
      "Train Epoch: 301 [77568/225000 (34%)] Loss: 17100.648438\n",
      "Train Epoch: 301 [80064/225000 (36%)] Loss: 17665.527344\n",
      "Train Epoch: 301 [82560/225000 (37%)] Loss: 17290.171875\n",
      "Train Epoch: 301 [85056/225000 (38%)] Loss: 17509.763672\n",
      "Train Epoch: 301 [87552/225000 (39%)] Loss: 17191.070312\n",
      "Train Epoch: 301 [90048/225000 (40%)] Loss: 17278.359375\n",
      "Train Epoch: 301 [92544/225000 (41%)] Loss: 17169.296875\n",
      "Train Epoch: 301 [95040/225000 (42%)] Loss: 16850.630859\n",
      "Train Epoch: 301 [97536/225000 (43%)] Loss: 17135.916016\n",
      "Train Epoch: 301 [100032/225000 (44%)] Loss: 17569.333984\n",
      "Train Epoch: 301 [102528/225000 (46%)] Loss: 17076.957031\n",
      "Train Epoch: 301 [105024/225000 (47%)] Loss: 17529.339844\n",
      "Train Epoch: 301 [107520/225000 (48%)] Loss: 17180.472656\n",
      "Train Epoch: 301 [110016/225000 (49%)] Loss: 17571.710938\n",
      "Train Epoch: 301 [112512/225000 (50%)] Loss: 17164.480469\n",
      "Train Epoch: 301 [115008/225000 (51%)] Loss: 17069.562500\n",
      "Train Epoch: 301 [117504/225000 (52%)] Loss: 16959.187500\n",
      "Train Epoch: 301 [120000/225000 (53%)] Loss: 17205.382812\n",
      "Train Epoch: 301 [122496/225000 (54%)] Loss: 17146.910156\n",
      "Train Epoch: 301 [124992/225000 (56%)] Loss: 16869.519531\n",
      "Train Epoch: 301 [127488/225000 (57%)] Loss: 17263.939453\n",
      "Train Epoch: 301 [129984/225000 (58%)] Loss: 17589.867188\n",
      "Train Epoch: 301 [132480/225000 (59%)] Loss: 17206.136719\n",
      "Train Epoch: 301 [134976/225000 (60%)] Loss: 17579.646484\n",
      "Train Epoch: 301 [137472/225000 (61%)] Loss: 17284.882812\n",
      "Train Epoch: 301 [139968/225000 (62%)] Loss: 16781.466797\n",
      "Train Epoch: 301 [142464/225000 (63%)] Loss: 17377.671875\n",
      "Train Epoch: 301 [144960/225000 (64%)] Loss: 17508.246094\n",
      "Train Epoch: 301 [147456/225000 (66%)] Loss: 17270.167969\n",
      "Train Epoch: 301 [149952/225000 (67%)] Loss: 17528.773438\n",
      "Train Epoch: 301 [152448/225000 (68%)] Loss: 17214.929688\n",
      "Train Epoch: 301 [154944/225000 (69%)] Loss: 17244.046875\n",
      "Train Epoch: 301 [157440/225000 (70%)] Loss: 17981.148438\n",
      "Train Epoch: 301 [159936/225000 (71%)] Loss: 17158.318359\n",
      "Train Epoch: 301 [162432/225000 (72%)] Loss: 17416.453125\n",
      "Train Epoch: 301 [164928/225000 (73%)] Loss: 17114.367188\n",
      "Train Epoch: 301 [167424/225000 (74%)] Loss: 17433.599609\n",
      "Train Epoch: 301 [169920/225000 (76%)] Loss: 17241.414062\n",
      "Train Epoch: 301 [172416/225000 (77%)] Loss: 17404.507812\n",
      "Train Epoch: 301 [174912/225000 (78%)] Loss: 17379.921875\n",
      "Train Epoch: 301 [177408/225000 (79%)] Loss: 17106.089844\n",
      "Train Epoch: 301 [179904/225000 (80%)] Loss: 17057.503906\n",
      "Train Epoch: 301 [182400/225000 (81%)] Loss: 17165.537109\n",
      "Train Epoch: 301 [184896/225000 (82%)] Loss: 17764.892578\n",
      "Train Epoch: 301 [187392/225000 (83%)] Loss: 17425.857422\n",
      "Train Epoch: 301 [189888/225000 (84%)] Loss: 16947.175781\n",
      "Train Epoch: 301 [192384/225000 (86%)] Loss: 17355.144531\n",
      "Train Epoch: 301 [194880/225000 (87%)] Loss: 17287.621094\n",
      "Train Epoch: 301 [197376/225000 (88%)] Loss: 17018.441406\n",
      "Train Epoch: 301 [199872/225000 (89%)] Loss: 17521.861328\n",
      "Train Epoch: 301 [202368/225000 (90%)] Loss: 16966.570312\n",
      "Train Epoch: 301 [204864/225000 (91%)] Loss: 17197.705078\n",
      "Train Epoch: 301 [207360/225000 (92%)] Loss: 17444.707031\n",
      "Train Epoch: 301 [209856/225000 (93%)] Loss: 17633.335938\n",
      "Train Epoch: 301 [212352/225000 (94%)] Loss: 17202.218750\n",
      "Train Epoch: 301 [214848/225000 (95%)] Loss: 17045.902344\n",
      "Train Epoch: 301 [217344/225000 (97%)] Loss: 17353.507812\n",
      "Train Epoch: 301 [219840/225000 (98%)] Loss: 17284.214844\n",
      "Train Epoch: 301 [222336/225000 (99%)] Loss: 17534.859375\n",
      "Train Epoch: 301 [224832/225000 (100%)] Loss: 17457.867188\n",
      "    epoch          : 301\n",
      "    loss           : 17316.48263851856\n",
      "    val_loss       : 17224.06614444911\n",
      "Train Epoch: 302 [192/225000 (0%)] Loss: 16901.214844\n",
      "Train Epoch: 302 [2688/225000 (1%)] Loss: 17583.414062\n",
      "Train Epoch: 302 [5184/225000 (2%)] Loss: 17031.083984\n",
      "Train Epoch: 302 [7680/225000 (3%)] Loss: 17071.734375\n",
      "Train Epoch: 302 [10176/225000 (5%)] Loss: 17365.386719\n",
      "Train Epoch: 302 [12672/225000 (6%)] Loss: 17026.261719\n",
      "Train Epoch: 302 [15168/225000 (7%)] Loss: 17225.402344\n",
      "Train Epoch: 302 [17664/225000 (8%)] Loss: 16905.753906\n",
      "Train Epoch: 302 [20160/225000 (9%)] Loss: 17188.544922\n",
      "Train Epoch: 302 [22656/225000 (10%)] Loss: 17346.320312\n",
      "Train Epoch: 302 [25152/225000 (11%)] Loss: 17164.910156\n",
      "Train Epoch: 302 [27648/225000 (12%)] Loss: 17237.054688\n",
      "Train Epoch: 302 [30144/225000 (13%)] Loss: 17532.566406\n",
      "Train Epoch: 302 [32640/225000 (15%)] Loss: 17310.068359\n",
      "Train Epoch: 302 [35136/225000 (16%)] Loss: 17077.013672\n",
      "Train Epoch: 302 [37632/225000 (17%)] Loss: 16885.708984\n",
      "Train Epoch: 302 [40128/225000 (18%)] Loss: 17163.404297\n",
      "Train Epoch: 302 [42624/225000 (19%)] Loss: 17272.339844\n",
      "Train Epoch: 302 [45120/225000 (20%)] Loss: 17741.769531\n",
      "Train Epoch: 302 [47616/225000 (21%)] Loss: 16924.908203\n",
      "Train Epoch: 302 [50112/225000 (22%)] Loss: 17640.785156\n",
      "Train Epoch: 302 [52608/225000 (23%)] Loss: 17059.425781\n",
      "Train Epoch: 302 [55104/225000 (24%)] Loss: 17002.402344\n",
      "Train Epoch: 302 [57600/225000 (26%)] Loss: 17249.980469\n",
      "Train Epoch: 302 [60096/225000 (27%)] Loss: 16883.507812\n",
      "Train Epoch: 302 [62592/225000 (28%)] Loss: 16771.035156\n",
      "Train Epoch: 302 [65088/225000 (29%)] Loss: 17102.097656\n",
      "Train Epoch: 302 [67584/225000 (30%)] Loss: 17403.601562\n",
      "Train Epoch: 302 [70080/225000 (31%)] Loss: 17189.320312\n",
      "Train Epoch: 302 [72576/225000 (32%)] Loss: 17167.832031\n",
      "Train Epoch: 302 [75072/225000 (33%)] Loss: 17233.929688\n",
      "Train Epoch: 302 [77568/225000 (34%)] Loss: 17204.306641\n",
      "Train Epoch: 302 [80064/225000 (36%)] Loss: 17008.718750\n",
      "Train Epoch: 302 [82560/225000 (37%)] Loss: 17605.707031\n",
      "Train Epoch: 302 [85056/225000 (38%)] Loss: 17324.039062\n",
      "Train Epoch: 302 [87552/225000 (39%)] Loss: 17688.794922\n",
      "Train Epoch: 302 [90048/225000 (40%)] Loss: 17013.962891\n",
      "Train Epoch: 302 [92544/225000 (41%)] Loss: 17193.091797\n",
      "Train Epoch: 302 [95040/225000 (42%)] Loss: 17436.460938\n",
      "Train Epoch: 302 [97536/225000 (43%)] Loss: 17687.494141\n",
      "Train Epoch: 302 [100032/225000 (44%)] Loss: 17260.367188\n",
      "Train Epoch: 302 [102528/225000 (46%)] Loss: 17622.574219\n",
      "Train Epoch: 302 [105024/225000 (47%)] Loss: 17366.189453\n",
      "Train Epoch: 302 [107520/225000 (48%)] Loss: 17173.851562\n",
      "Train Epoch: 302 [110016/225000 (49%)] Loss: 17388.335938\n",
      "Train Epoch: 302 [112512/225000 (50%)] Loss: 17143.855469\n",
      "Train Epoch: 302 [115008/225000 (51%)] Loss: 17421.960938\n",
      "Train Epoch: 302 [117504/225000 (52%)] Loss: 17200.917969\n",
      "Train Epoch: 302 [120000/225000 (53%)] Loss: 17409.968750\n",
      "Train Epoch: 302 [122496/225000 (54%)] Loss: 17594.453125\n",
      "Train Epoch: 302 [124992/225000 (56%)] Loss: 17230.792969\n",
      "Train Epoch: 302 [127488/225000 (57%)] Loss: 17076.523438\n",
      "Train Epoch: 302 [129984/225000 (58%)] Loss: 17311.128906\n",
      "Train Epoch: 302 [132480/225000 (59%)] Loss: 17165.591797\n",
      "Train Epoch: 302 [134976/225000 (60%)] Loss: 17612.048828\n",
      "Train Epoch: 302 [137472/225000 (61%)] Loss: 16889.623047\n",
      "Train Epoch: 302 [139968/225000 (62%)] Loss: 17369.679688\n",
      "Train Epoch: 302 [142464/225000 (63%)] Loss: 16902.083984\n",
      "Train Epoch: 302 [144960/225000 (64%)] Loss: 17061.492188\n",
      "Train Epoch: 302 [147456/225000 (66%)] Loss: 16982.414062\n",
      "Train Epoch: 302 [149952/225000 (67%)] Loss: 17403.953125\n",
      "Train Epoch: 302 [152448/225000 (68%)] Loss: 17422.007812\n",
      "Train Epoch: 302 [154944/225000 (69%)] Loss: 17597.070312\n",
      "Train Epoch: 302 [157440/225000 (70%)] Loss: 17001.761719\n",
      "Train Epoch: 302 [159936/225000 (71%)] Loss: 17268.703125\n",
      "Train Epoch: 302 [162432/225000 (72%)] Loss: 17133.078125\n",
      "Train Epoch: 302 [164928/225000 (73%)] Loss: 17230.562500\n",
      "Train Epoch: 302 [167424/225000 (74%)] Loss: 16754.617188\n",
      "Train Epoch: 302 [169920/225000 (76%)] Loss: 17511.919922\n",
      "Train Epoch: 302 [172416/225000 (77%)] Loss: 17501.523438\n",
      "Train Epoch: 302 [174912/225000 (78%)] Loss: 17305.554688\n",
      "Train Epoch: 302 [177408/225000 (79%)] Loss: 17261.273438\n",
      "Train Epoch: 302 [179904/225000 (80%)] Loss: 17653.513672\n",
      "Train Epoch: 302 [182400/225000 (81%)] Loss: 16912.738281\n",
      "Train Epoch: 302 [184896/225000 (82%)] Loss: 17222.761719\n",
      "Train Epoch: 302 [187392/225000 (83%)] Loss: 17355.472656\n",
      "Train Epoch: 302 [189888/225000 (84%)] Loss: 16958.722656\n",
      "Train Epoch: 302 [192384/225000 (86%)] Loss: 17156.613281\n",
      "Train Epoch: 302 [194880/225000 (87%)] Loss: 17526.460938\n",
      "Train Epoch: 302 [197376/225000 (88%)] Loss: 17167.447266\n",
      "Train Epoch: 302 [199872/225000 (89%)] Loss: 17270.800781\n",
      "Train Epoch: 302 [202368/225000 (90%)] Loss: 17183.884766\n",
      "Train Epoch: 302 [204864/225000 (91%)] Loss: 17282.019531\n",
      "Train Epoch: 302 [207360/225000 (92%)] Loss: 17264.742188\n",
      "Train Epoch: 302 [209856/225000 (93%)] Loss: 16944.382812\n",
      "Train Epoch: 302 [212352/225000 (94%)] Loss: 19170.994141\n",
      "Train Epoch: 302 [214848/225000 (95%)] Loss: 17261.882812\n",
      "Train Epoch: 302 [217344/225000 (97%)] Loss: 17204.482422\n",
      "Train Epoch: 302 [219840/225000 (98%)] Loss: 17099.765625\n",
      "Train Epoch: 302 [222336/225000 (99%)] Loss: 17305.503906\n",
      "Train Epoch: 302 [224832/225000 (100%)] Loss: 17339.490234\n",
      "    epoch          : 302\n",
      "    loss           : 17324.25597352949\n",
      "    val_loss       : 17285.25066823268\n",
      "Train Epoch: 303 [192/225000 (0%)] Loss: 17179.818359\n",
      "Train Epoch: 303 [2688/225000 (1%)] Loss: 17345.750000\n",
      "Train Epoch: 303 [5184/225000 (2%)] Loss: 17776.636719\n",
      "Train Epoch: 303 [7680/225000 (3%)] Loss: 17200.343750\n",
      "Train Epoch: 303 [10176/225000 (5%)] Loss: 17421.552734\n",
      "Train Epoch: 303 [12672/225000 (6%)] Loss: 17212.630859\n",
      "Train Epoch: 303 [15168/225000 (7%)] Loss: 17479.798828\n",
      "Train Epoch: 303 [17664/225000 (8%)] Loss: 17144.609375\n",
      "Train Epoch: 303 [20160/225000 (9%)] Loss: 17175.480469\n",
      "Train Epoch: 303 [22656/225000 (10%)] Loss: 17086.316406\n",
      "Train Epoch: 303 [25152/225000 (11%)] Loss: 16998.097656\n",
      "Train Epoch: 303 [27648/225000 (12%)] Loss: 17408.117188\n",
      "Train Epoch: 303 [30144/225000 (13%)] Loss: 17377.156250\n",
      "Train Epoch: 303 [32640/225000 (15%)] Loss: 16758.734375\n",
      "Train Epoch: 303 [35136/225000 (16%)] Loss: 17334.500000\n",
      "Train Epoch: 303 [37632/225000 (17%)] Loss: 17103.826172\n",
      "Train Epoch: 303 [40128/225000 (18%)] Loss: 17254.800781\n",
      "Train Epoch: 303 [42624/225000 (19%)] Loss: 17191.179688\n",
      "Train Epoch: 303 [45120/225000 (20%)] Loss: 17819.503906\n",
      "Train Epoch: 303 [47616/225000 (21%)] Loss: 17276.839844\n",
      "Train Epoch: 303 [50112/225000 (22%)] Loss: 17093.855469\n",
      "Train Epoch: 303 [52608/225000 (23%)] Loss: 17385.457031\n",
      "Train Epoch: 303 [55104/225000 (24%)] Loss: 17255.757812\n",
      "Train Epoch: 303 [57600/225000 (26%)] Loss: 17787.488281\n",
      "Train Epoch: 303 [60096/225000 (27%)] Loss: 16930.755859\n",
      "Train Epoch: 303 [62592/225000 (28%)] Loss: 17073.353516\n",
      "Train Epoch: 303 [65088/225000 (29%)] Loss: 17478.007812\n",
      "Train Epoch: 303 [67584/225000 (30%)] Loss: 17011.148438\n",
      "Train Epoch: 303 [70080/225000 (31%)] Loss: 17336.095703\n",
      "Train Epoch: 303 [72576/225000 (32%)] Loss: 17290.074219\n",
      "Train Epoch: 303 [75072/225000 (33%)] Loss: 17201.460938\n",
      "Train Epoch: 303 [77568/225000 (34%)] Loss: 17615.224609\n",
      "Train Epoch: 303 [80064/225000 (36%)] Loss: 17442.263672\n",
      "Train Epoch: 303 [82560/225000 (37%)] Loss: 17117.562500\n",
      "Train Epoch: 303 [85056/225000 (38%)] Loss: 17287.335938\n",
      "Train Epoch: 303 [87552/225000 (39%)] Loss: 17249.148438\n",
      "Train Epoch: 303 [90048/225000 (40%)] Loss: 17511.867188\n",
      "Train Epoch: 303 [92544/225000 (41%)] Loss: 17532.892578\n",
      "Train Epoch: 303 [95040/225000 (42%)] Loss: 16999.890625\n",
      "Train Epoch: 303 [97536/225000 (43%)] Loss: 16857.035156\n",
      "Train Epoch: 303 [100032/225000 (44%)] Loss: 17902.976562\n",
      "Train Epoch: 303 [102528/225000 (46%)] Loss: 17137.914062\n",
      "Train Epoch: 303 [105024/225000 (47%)] Loss: 17515.000000\n",
      "Train Epoch: 303 [107520/225000 (48%)] Loss: 17259.027344\n",
      "Train Epoch: 303 [110016/225000 (49%)] Loss: 17168.968750\n",
      "Train Epoch: 303 [112512/225000 (50%)] Loss: 17032.886719\n",
      "Train Epoch: 303 [115008/225000 (51%)] Loss: 17716.726562\n",
      "Train Epoch: 303 [117504/225000 (52%)] Loss: 17473.236328\n",
      "Train Epoch: 303 [120000/225000 (53%)] Loss: 17238.621094\n",
      "Train Epoch: 303 [122496/225000 (54%)] Loss: 17019.992188\n",
      "Train Epoch: 303 [124992/225000 (56%)] Loss: 17472.958984\n",
      "Train Epoch: 303 [127488/225000 (57%)] Loss: 17421.707031\n",
      "Train Epoch: 303 [129984/225000 (58%)] Loss: 17303.136719\n",
      "Train Epoch: 303 [132480/225000 (59%)] Loss: 17124.746094\n",
      "Train Epoch: 303 [134976/225000 (60%)] Loss: 17181.191406\n",
      "Train Epoch: 303 [137472/225000 (61%)] Loss: 16839.253906\n",
      "Train Epoch: 303 [139968/225000 (62%)] Loss: 17422.074219\n",
      "Train Epoch: 303 [142464/225000 (63%)] Loss: 16988.750000\n",
      "Train Epoch: 303 [144960/225000 (64%)] Loss: 17021.496094\n",
      "Train Epoch: 303 [147456/225000 (66%)] Loss: 17327.007812\n",
      "Train Epoch: 303 [149952/225000 (67%)] Loss: 17216.781250\n",
      "Train Epoch: 303 [152448/225000 (68%)] Loss: 16849.222656\n",
      "Train Epoch: 303 [154944/225000 (69%)] Loss: 17163.496094\n",
      "Train Epoch: 303 [157440/225000 (70%)] Loss: 17480.765625\n",
      "Train Epoch: 303 [159936/225000 (71%)] Loss: 17376.703125\n",
      "Train Epoch: 303 [162432/225000 (72%)] Loss: 16782.667969\n",
      "Train Epoch: 303 [164928/225000 (73%)] Loss: 17005.857422\n",
      "Train Epoch: 303 [167424/225000 (74%)] Loss: 16737.503906\n",
      "Train Epoch: 303 [169920/225000 (76%)] Loss: 17157.421875\n",
      "Train Epoch: 303 [172416/225000 (77%)] Loss: 17340.406250\n",
      "Train Epoch: 303 [174912/225000 (78%)] Loss: 17463.884766\n",
      "Train Epoch: 303 [177408/225000 (79%)] Loss: 17368.326172\n",
      "Train Epoch: 303 [179904/225000 (80%)] Loss: 17444.068359\n",
      "Train Epoch: 303 [182400/225000 (81%)] Loss: 17236.617188\n",
      "Train Epoch: 303 [184896/225000 (82%)] Loss: 17073.539062\n",
      "Train Epoch: 303 [187392/225000 (83%)] Loss: 17821.761719\n",
      "Train Epoch: 303 [189888/225000 (84%)] Loss: 17115.025391\n",
      "Train Epoch: 303 [192384/225000 (86%)] Loss: 16977.246094\n",
      "Train Epoch: 303 [194880/225000 (87%)] Loss: 17578.720703\n",
      "Train Epoch: 303 [197376/225000 (88%)] Loss: 17818.417969\n",
      "Train Epoch: 303 [199872/225000 (89%)] Loss: 16878.871094\n",
      "Train Epoch: 303 [202368/225000 (90%)] Loss: 17700.503906\n",
      "Train Epoch: 303 [204864/225000 (91%)] Loss: 16977.144531\n",
      "Train Epoch: 303 [207360/225000 (92%)] Loss: 17074.707031\n",
      "Train Epoch: 303 [209856/225000 (93%)] Loss: 17597.830078\n",
      "Train Epoch: 303 [212352/225000 (94%)] Loss: 17534.875000\n",
      "Train Epoch: 303 [214848/225000 (95%)] Loss: 17104.812500\n",
      "Train Epoch: 303 [217344/225000 (97%)] Loss: 17179.705078\n",
      "Train Epoch: 303 [219840/225000 (98%)] Loss: 17274.332031\n",
      "Train Epoch: 303 [222336/225000 (99%)] Loss: 17384.423828\n",
      "Train Epoch: 303 [224832/225000 (100%)] Loss: 17080.988281\n",
      "    epoch          : 303\n",
      "    loss           : 17275.2753231322\n",
      "    val_loss       : 17303.047434567496\n",
      "Train Epoch: 304 [192/225000 (0%)] Loss: 17498.900391\n",
      "Train Epoch: 304 [2688/225000 (1%)] Loss: 17331.328125\n",
      "Train Epoch: 304 [5184/225000 (2%)] Loss: 17266.439453\n",
      "Train Epoch: 304 [7680/225000 (3%)] Loss: 17376.941406\n",
      "Train Epoch: 304 [10176/225000 (5%)] Loss: 17576.492188\n",
      "Train Epoch: 304 [12672/225000 (6%)] Loss: 17352.601562\n",
      "Train Epoch: 304 [15168/225000 (7%)] Loss: 17137.716797\n",
      "Train Epoch: 304 [17664/225000 (8%)] Loss: 17132.699219\n",
      "Train Epoch: 304 [20160/225000 (9%)] Loss: 17217.375000\n",
      "Train Epoch: 304 [22656/225000 (10%)] Loss: 17268.441406\n",
      "Train Epoch: 304 [25152/225000 (11%)] Loss: 17319.074219\n",
      "Train Epoch: 304 [27648/225000 (12%)] Loss: 17273.367188\n",
      "Train Epoch: 304 [30144/225000 (13%)] Loss: 17791.035156\n",
      "Train Epoch: 304 [32640/225000 (15%)] Loss: 17430.910156\n",
      "Train Epoch: 304 [35136/225000 (16%)] Loss: 17171.878906\n",
      "Train Epoch: 304 [37632/225000 (17%)] Loss: 17084.707031\n",
      "Train Epoch: 304 [40128/225000 (18%)] Loss: 17550.666016\n",
      "Train Epoch: 304 [42624/225000 (19%)] Loss: 17235.621094\n",
      "Train Epoch: 304 [45120/225000 (20%)] Loss: 19208.089844\n",
      "Train Epoch: 304 [47616/225000 (21%)] Loss: 17419.355469\n",
      "Train Epoch: 304 [50112/225000 (22%)] Loss: 16842.914062\n",
      "Train Epoch: 304 [52608/225000 (23%)] Loss: 17105.867188\n",
      "Train Epoch: 304 [55104/225000 (24%)] Loss: 17566.300781\n",
      "Train Epoch: 304 [57600/225000 (26%)] Loss: 17061.785156\n",
      "Train Epoch: 304 [60096/225000 (27%)] Loss: 17238.562500\n",
      "Train Epoch: 304 [62592/225000 (28%)] Loss: 17197.976562\n",
      "Train Epoch: 304 [65088/225000 (29%)] Loss: 17321.335938\n",
      "Train Epoch: 304 [67584/225000 (30%)] Loss: 16819.705078\n",
      "Train Epoch: 304 [70080/225000 (31%)] Loss: 17326.785156\n",
      "Train Epoch: 304 [72576/225000 (32%)] Loss: 17378.410156\n",
      "Train Epoch: 304 [75072/225000 (33%)] Loss: 17339.050781\n",
      "Train Epoch: 304 [77568/225000 (34%)] Loss: 17127.648438\n",
      "Train Epoch: 304 [80064/225000 (36%)] Loss: 17752.988281\n",
      "Train Epoch: 304 [82560/225000 (37%)] Loss: 16761.472656\n",
      "Train Epoch: 304 [85056/225000 (38%)] Loss: 17192.628906\n",
      "Train Epoch: 304 [87552/225000 (39%)] Loss: 17540.009766\n",
      "Train Epoch: 304 [90048/225000 (40%)] Loss: 17574.085938\n",
      "Train Epoch: 304 [92544/225000 (41%)] Loss: 16926.789062\n",
      "Train Epoch: 304 [95040/225000 (42%)] Loss: 17566.248047\n",
      "Train Epoch: 304 [97536/225000 (43%)] Loss: 17216.414062\n",
      "Train Epoch: 304 [100032/225000 (44%)] Loss: 17030.515625\n",
      "Train Epoch: 304 [102528/225000 (46%)] Loss: 17167.019531\n",
      "Train Epoch: 304 [105024/225000 (47%)] Loss: 17485.732422\n",
      "Train Epoch: 304 [107520/225000 (48%)] Loss: 17384.179688\n",
      "Train Epoch: 304 [110016/225000 (49%)] Loss: 16861.210938\n",
      "Train Epoch: 304 [112512/225000 (50%)] Loss: 17041.449219\n",
      "Train Epoch: 304 [115008/225000 (51%)] Loss: 17081.343750\n",
      "Train Epoch: 304 [117504/225000 (52%)] Loss: 17618.070312\n",
      "Train Epoch: 304 [120000/225000 (53%)] Loss: 16790.507812\n",
      "Train Epoch: 304 [122496/225000 (54%)] Loss: 17492.380859\n",
      "Train Epoch: 304 [124992/225000 (56%)] Loss: 17450.835938\n",
      "Train Epoch: 304 [127488/225000 (57%)] Loss: 17507.031250\n",
      "Train Epoch: 304 [129984/225000 (58%)] Loss: 17356.238281\n",
      "Train Epoch: 304 [132480/225000 (59%)] Loss: 17059.503906\n",
      "Train Epoch: 304 [134976/225000 (60%)] Loss: 17413.046875\n",
      "Train Epoch: 304 [137472/225000 (61%)] Loss: 17440.421875\n",
      "Train Epoch: 304 [139968/225000 (62%)] Loss: 16829.324219\n",
      "Train Epoch: 304 [142464/225000 (63%)] Loss: 16962.755859\n",
      "Train Epoch: 304 [144960/225000 (64%)] Loss: 17658.835938\n",
      "Train Epoch: 304 [147456/225000 (66%)] Loss: 16930.207031\n",
      "Train Epoch: 304 [149952/225000 (67%)] Loss: 17250.128906\n",
      "Train Epoch: 304 [152448/225000 (68%)] Loss: 17636.621094\n",
      "Train Epoch: 304 [154944/225000 (69%)] Loss: 17602.066406\n",
      "Train Epoch: 304 [157440/225000 (70%)] Loss: 17204.585938\n",
      "Train Epoch: 304 [159936/225000 (71%)] Loss: 16978.507812\n",
      "Train Epoch: 304 [162432/225000 (72%)] Loss: 17417.412109\n",
      "Train Epoch: 304 [164928/225000 (73%)] Loss: 17280.535156\n",
      "Train Epoch: 304 [167424/225000 (74%)] Loss: 17573.107422\n",
      "Train Epoch: 304 [169920/225000 (76%)] Loss: 17029.179688\n",
      "Train Epoch: 304 [172416/225000 (77%)] Loss: 16677.925781\n",
      "Train Epoch: 304 [174912/225000 (78%)] Loss: 16854.484375\n",
      "Train Epoch: 304 [177408/225000 (79%)] Loss: 17009.429688\n",
      "Train Epoch: 304 [179904/225000 (80%)] Loss: 17092.554688\n",
      "Train Epoch: 304 [182400/225000 (81%)] Loss: 17291.810547\n",
      "Train Epoch: 304 [184896/225000 (82%)] Loss: 17286.476562\n",
      "Train Epoch: 304 [187392/225000 (83%)] Loss: 17292.724609\n",
      "Train Epoch: 304 [189888/225000 (84%)] Loss: 17592.078125\n",
      "Train Epoch: 304 [192384/225000 (86%)] Loss: 17336.496094\n",
      "Train Epoch: 304 [194880/225000 (87%)] Loss: 17439.570312\n",
      "Train Epoch: 304 [197376/225000 (88%)] Loss: 17733.101562\n",
      "Train Epoch: 304 [199872/225000 (89%)] Loss: 16753.648438\n",
      "Train Epoch: 304 [202368/225000 (90%)] Loss: 17099.939453\n",
      "Train Epoch: 304 [204864/225000 (91%)] Loss: 17620.464844\n",
      "Train Epoch: 304 [207360/225000 (92%)] Loss: 17463.214844\n",
      "Train Epoch: 304 [209856/225000 (93%)] Loss: 17665.953125\n",
      "Train Epoch: 304 [212352/225000 (94%)] Loss: 17407.609375\n",
      "Train Epoch: 304 [214848/225000 (95%)] Loss: 17227.472656\n",
      "Train Epoch: 304 [217344/225000 (97%)] Loss: 17024.230469\n",
      "Train Epoch: 304 [219840/225000 (98%)] Loss: 17005.843750\n",
      "Train Epoch: 304 [222336/225000 (99%)] Loss: 17009.394531\n",
      "Train Epoch: 304 [224832/225000 (100%)] Loss: 16863.000000\n",
      "    epoch          : 304\n",
      "    loss           : 17282.420437653316\n",
      "    val_loss       : 17189.905505523428\n",
      "Train Epoch: 305 [192/225000 (0%)] Loss: 17633.875000\n",
      "Train Epoch: 305 [2688/225000 (1%)] Loss: 16811.941406\n",
      "Train Epoch: 305 [5184/225000 (2%)] Loss: 17433.460938\n",
      "Train Epoch: 305 [7680/225000 (3%)] Loss: 17100.683594\n",
      "Train Epoch: 305 [10176/225000 (5%)] Loss: 17490.712891\n",
      "Train Epoch: 305 [12672/225000 (6%)] Loss: 17570.162109\n",
      "Train Epoch: 305 [15168/225000 (7%)] Loss: 16976.054688\n",
      "Train Epoch: 305 [17664/225000 (8%)] Loss: 17781.435547\n",
      "Train Epoch: 305 [20160/225000 (9%)] Loss: 17456.294922\n",
      "Train Epoch: 305 [22656/225000 (10%)] Loss: 17186.324219\n",
      "Train Epoch: 305 [25152/225000 (11%)] Loss: 16830.699219\n",
      "Train Epoch: 305 [27648/225000 (12%)] Loss: 17485.046875\n",
      "Train Epoch: 305 [30144/225000 (13%)] Loss: 17524.669922\n",
      "Train Epoch: 305 [32640/225000 (15%)] Loss: 17156.898438\n",
      "Train Epoch: 305 [35136/225000 (16%)] Loss: 16808.304688\n",
      "Train Epoch: 305 [37632/225000 (17%)] Loss: 17339.121094\n",
      "Train Epoch: 305 [40128/225000 (18%)] Loss: 17094.087891\n",
      "Train Epoch: 305 [42624/225000 (19%)] Loss: 17535.183594\n",
      "Train Epoch: 305 [45120/225000 (20%)] Loss: 17176.175781\n",
      "Train Epoch: 305 [47616/225000 (21%)] Loss: 17295.906250\n",
      "Train Epoch: 305 [50112/225000 (22%)] Loss: 17121.199219\n",
      "Train Epoch: 305 [52608/225000 (23%)] Loss: 16859.828125\n",
      "Train Epoch: 305 [55104/225000 (24%)] Loss: 17609.433594\n",
      "Train Epoch: 305 [57600/225000 (26%)] Loss: 19243.726562\n",
      "Train Epoch: 305 [60096/225000 (27%)] Loss: 16941.298828\n",
      "Train Epoch: 305 [62592/225000 (28%)] Loss: 17579.816406\n",
      "Train Epoch: 305 [65088/225000 (29%)] Loss: 17295.640625\n",
      "Train Epoch: 305 [67584/225000 (30%)] Loss: 17342.628906\n",
      "Train Epoch: 305 [70080/225000 (31%)] Loss: 17415.861328\n",
      "Train Epoch: 305 [72576/225000 (32%)] Loss: 17946.191406\n",
      "Train Epoch: 305 [75072/225000 (33%)] Loss: 17167.894531\n",
      "Train Epoch: 305 [77568/225000 (34%)] Loss: 17394.757812\n",
      "Train Epoch: 305 [80064/225000 (36%)] Loss: 17564.099609\n",
      "Train Epoch: 305 [82560/225000 (37%)] Loss: 16850.541016\n",
      "Train Epoch: 305 [85056/225000 (38%)] Loss: 16982.300781\n",
      "Train Epoch: 305 [87552/225000 (39%)] Loss: 17591.453125\n",
      "Train Epoch: 305 [90048/225000 (40%)] Loss: 17002.554688\n",
      "Train Epoch: 305 [92544/225000 (41%)] Loss: 17575.294922\n",
      "Train Epoch: 305 [95040/225000 (42%)] Loss: 17008.789062\n",
      "Train Epoch: 305 [97536/225000 (43%)] Loss: 17563.583984\n",
      "Train Epoch: 305 [100032/225000 (44%)] Loss: 17077.609375\n",
      "Train Epoch: 305 [102528/225000 (46%)] Loss: 16766.886719\n",
      "Train Epoch: 305 [105024/225000 (47%)] Loss: 16728.468750\n",
      "Train Epoch: 305 [107520/225000 (48%)] Loss: 17226.281250\n",
      "Train Epoch: 305 [110016/225000 (49%)] Loss: 16922.337891\n",
      "Train Epoch: 305 [112512/225000 (50%)] Loss: 17171.638672\n",
      "Train Epoch: 305 [115008/225000 (51%)] Loss: 17324.738281\n",
      "Train Epoch: 305 [117504/225000 (52%)] Loss: 16874.753906\n",
      "Train Epoch: 305 [120000/225000 (53%)] Loss: 17127.128906\n",
      "Train Epoch: 305 [122496/225000 (54%)] Loss: 17589.949219\n",
      "Train Epoch: 305 [124992/225000 (56%)] Loss: 17069.507812\n",
      "Train Epoch: 305 [127488/225000 (57%)] Loss: 17111.296875\n",
      "Train Epoch: 305 [129984/225000 (58%)] Loss: 17129.998047\n",
      "Train Epoch: 305 [132480/225000 (59%)] Loss: 16935.539062\n",
      "Train Epoch: 305 [134976/225000 (60%)] Loss: 17679.183594\n",
      "Train Epoch: 305 [137472/225000 (61%)] Loss: 17197.398438\n",
      "Train Epoch: 305 [139968/225000 (62%)] Loss: 18037.332031\n",
      "Train Epoch: 305 [142464/225000 (63%)] Loss: 17286.816406\n",
      "Train Epoch: 305 [144960/225000 (64%)] Loss: 16810.054688\n",
      "Train Epoch: 305 [147456/225000 (66%)] Loss: 17614.628906\n",
      "Train Epoch: 305 [149952/225000 (67%)] Loss: 17321.472656\n",
      "Train Epoch: 305 [152448/225000 (68%)] Loss: 17050.707031\n",
      "Train Epoch: 305 [154944/225000 (69%)] Loss: 17587.458984\n",
      "Train Epoch: 305 [157440/225000 (70%)] Loss: 17059.511719\n",
      "Train Epoch: 305 [159936/225000 (71%)] Loss: 17019.625000\n",
      "Train Epoch: 305 [162432/225000 (72%)] Loss: 16981.736328\n",
      "Train Epoch: 305 [164928/225000 (73%)] Loss: 16897.234375\n",
      "Train Epoch: 305 [167424/225000 (74%)] Loss: 17829.121094\n",
      "Train Epoch: 305 [169920/225000 (76%)] Loss: 16950.050781\n",
      "Train Epoch: 305 [172416/225000 (77%)] Loss: 17315.414062\n",
      "Train Epoch: 305 [174912/225000 (78%)] Loss: 17707.269531\n",
      "Train Epoch: 305 [177408/225000 (79%)] Loss: 17399.019531\n",
      "Train Epoch: 305 [179904/225000 (80%)] Loss: 17120.853516\n",
      "Train Epoch: 305 [182400/225000 (81%)] Loss: 17949.347656\n",
      "Train Epoch: 305 [184896/225000 (82%)] Loss: 17498.105469\n",
      "Train Epoch: 305 [187392/225000 (83%)] Loss: 17280.632812\n",
      "Train Epoch: 305 [189888/225000 (84%)] Loss: 17294.359375\n",
      "Train Epoch: 305 [192384/225000 (86%)] Loss: 17116.277344\n",
      "Train Epoch: 305 [194880/225000 (87%)] Loss: 16891.773438\n",
      "Train Epoch: 305 [197376/225000 (88%)] Loss: 17092.324219\n",
      "Train Epoch: 305 [199872/225000 (89%)] Loss: 17235.242188\n",
      "Train Epoch: 305 [202368/225000 (90%)] Loss: 17750.867188\n",
      "Train Epoch: 305 [204864/225000 (91%)] Loss: 17852.355469\n",
      "Train Epoch: 305 [207360/225000 (92%)] Loss: 17157.394531\n",
      "Train Epoch: 305 [209856/225000 (93%)] Loss: 17627.175781\n",
      "Train Epoch: 305 [212352/225000 (94%)] Loss: 17012.339844\n",
      "Train Epoch: 305 [214848/225000 (95%)] Loss: 16879.884766\n",
      "Train Epoch: 305 [217344/225000 (97%)] Loss: 17133.968750\n",
      "Train Epoch: 305 [219840/225000 (98%)] Loss: 16931.914062\n",
      "Train Epoch: 305 [222336/225000 (99%)] Loss: 17282.195312\n",
      "Train Epoch: 305 [224832/225000 (100%)] Loss: 17323.953125\n",
      "    epoch          : 305\n",
      "    loss           : 17283.31203088337\n",
      "    val_loss       : 17199.72968487157\n",
      "Train Epoch: 306 [192/225000 (0%)] Loss: 17080.816406\n",
      "Train Epoch: 306 [2688/225000 (1%)] Loss: 16975.457031\n",
      "Train Epoch: 306 [5184/225000 (2%)] Loss: 17206.425781\n",
      "Train Epoch: 306 [7680/225000 (3%)] Loss: 17267.378906\n",
      "Train Epoch: 306 [10176/225000 (5%)] Loss: 16982.937500\n",
      "Train Epoch: 306 [12672/225000 (6%)] Loss: 16900.111328\n",
      "Train Epoch: 306 [15168/225000 (7%)] Loss: 17519.638672\n",
      "Train Epoch: 306 [17664/225000 (8%)] Loss: 17334.046875\n",
      "Train Epoch: 306 [20160/225000 (9%)] Loss: 17401.283203\n",
      "Train Epoch: 306 [22656/225000 (10%)] Loss: 17127.068359\n",
      "Train Epoch: 306 [25152/225000 (11%)] Loss: 17509.730469\n",
      "Train Epoch: 306 [27648/225000 (12%)] Loss: 17363.953125\n",
      "Train Epoch: 306 [30144/225000 (13%)] Loss: 17054.902344\n",
      "Train Epoch: 306 [32640/225000 (15%)] Loss: 17330.994141\n",
      "Train Epoch: 306 [35136/225000 (16%)] Loss: 17298.281250\n",
      "Train Epoch: 306 [37632/225000 (17%)] Loss: 17188.656250\n",
      "Train Epoch: 306 [40128/225000 (18%)] Loss: 17344.574219\n",
      "Train Epoch: 306 [42624/225000 (19%)] Loss: 17344.623047\n",
      "Train Epoch: 306 [45120/225000 (20%)] Loss: 17258.832031\n",
      "Train Epoch: 306 [47616/225000 (21%)] Loss: 17208.609375\n",
      "Train Epoch: 306 [50112/225000 (22%)] Loss: 17769.535156\n",
      "Train Epoch: 306 [52608/225000 (23%)] Loss: 17382.843750\n",
      "Train Epoch: 306 [55104/225000 (24%)] Loss: 16691.730469\n",
      "Train Epoch: 306 [57600/225000 (26%)] Loss: 17273.722656\n",
      "Train Epoch: 306 [60096/225000 (27%)] Loss: 17057.988281\n",
      "Train Epoch: 306 [62592/225000 (28%)] Loss: 17520.812500\n",
      "Train Epoch: 306 [65088/225000 (29%)] Loss: 17336.843750\n",
      "Train Epoch: 306 [67584/225000 (30%)] Loss: 16725.208984\n",
      "Train Epoch: 306 [70080/225000 (31%)] Loss: 17208.617188\n",
      "Train Epoch: 306 [72576/225000 (32%)] Loss: 17207.253906\n",
      "Train Epoch: 306 [75072/225000 (33%)] Loss: 17417.472656\n",
      "Train Epoch: 306 [77568/225000 (34%)] Loss: 17598.863281\n",
      "Train Epoch: 306 [80064/225000 (36%)] Loss: 17101.701172\n",
      "Train Epoch: 306 [82560/225000 (37%)] Loss: 17402.855469\n",
      "Train Epoch: 306 [85056/225000 (38%)] Loss: 17066.568359\n",
      "Train Epoch: 306 [87552/225000 (39%)] Loss: 16900.097656\n",
      "Train Epoch: 306 [90048/225000 (40%)] Loss: 17223.324219\n",
      "Train Epoch: 306 [92544/225000 (41%)] Loss: 17304.515625\n",
      "Train Epoch: 306 [95040/225000 (42%)] Loss: 17523.626953\n",
      "Train Epoch: 306 [97536/225000 (43%)] Loss: 17331.734375\n",
      "Train Epoch: 306 [100032/225000 (44%)] Loss: 16857.281250\n",
      "Train Epoch: 306 [102528/225000 (46%)] Loss: 16797.693359\n",
      "Train Epoch: 306 [105024/225000 (47%)] Loss: 17772.175781\n",
      "Train Epoch: 306 [107520/225000 (48%)] Loss: 17533.378906\n",
      "Train Epoch: 306 [110016/225000 (49%)] Loss: 17116.972656\n",
      "Train Epoch: 306 [112512/225000 (50%)] Loss: 17336.093750\n",
      "Train Epoch: 306 [115008/225000 (51%)] Loss: 17441.375000\n",
      "Train Epoch: 306 [117504/225000 (52%)] Loss: 17419.244141\n",
      "Train Epoch: 306 [120000/225000 (53%)] Loss: 17407.597656\n",
      "Train Epoch: 306 [122496/225000 (54%)] Loss: 16840.808594\n",
      "Train Epoch: 306 [124992/225000 (56%)] Loss: 17330.847656\n",
      "Train Epoch: 306 [127488/225000 (57%)] Loss: 17715.695312\n",
      "Train Epoch: 306 [129984/225000 (58%)] Loss: 17360.800781\n",
      "Train Epoch: 306 [132480/225000 (59%)] Loss: 17078.847656\n",
      "Train Epoch: 306 [134976/225000 (60%)] Loss: 17594.822266\n",
      "Train Epoch: 306 [137472/225000 (61%)] Loss: 17862.703125\n",
      "Train Epoch: 306 [139968/225000 (62%)] Loss: 16832.257812\n",
      "Train Epoch: 306 [142464/225000 (63%)] Loss: 17178.941406\n",
      "Train Epoch: 306 [144960/225000 (64%)] Loss: 17196.562500\n",
      "Train Epoch: 306 [147456/225000 (66%)] Loss: 17410.007812\n",
      "Train Epoch: 306 [149952/225000 (67%)] Loss: 19092.144531\n",
      "Train Epoch: 306 [152448/225000 (68%)] Loss: 17746.929688\n",
      "Train Epoch: 306 [154944/225000 (69%)] Loss: 16490.785156\n",
      "Train Epoch: 306 [157440/225000 (70%)] Loss: 17260.876953\n",
      "Train Epoch: 306 [159936/225000 (71%)] Loss: 17429.271484\n",
      "Train Epoch: 306 [162432/225000 (72%)] Loss: 17633.490234\n",
      "Train Epoch: 306 [164928/225000 (73%)] Loss: 17091.359375\n",
      "Train Epoch: 306 [167424/225000 (74%)] Loss: 17255.328125\n",
      "Train Epoch: 306 [169920/225000 (76%)] Loss: 17361.914062\n",
      "Train Epoch: 306 [172416/225000 (77%)] Loss: 17395.484375\n",
      "Train Epoch: 306 [174912/225000 (78%)] Loss: 17047.796875\n",
      "Train Epoch: 306 [177408/225000 (79%)] Loss: 18040.761719\n",
      "Train Epoch: 306 [179904/225000 (80%)] Loss: 16989.160156\n",
      "Train Epoch: 306 [182400/225000 (81%)] Loss: 17478.574219\n",
      "Train Epoch: 306 [184896/225000 (82%)] Loss: 17369.833984\n",
      "Train Epoch: 306 [187392/225000 (83%)] Loss: 17199.710938\n",
      "Train Epoch: 306 [189888/225000 (84%)] Loss: 17604.375000\n",
      "Train Epoch: 306 [192384/225000 (86%)] Loss: 16811.058594\n",
      "Train Epoch: 306 [194880/225000 (87%)] Loss: 17659.542969\n",
      "Train Epoch: 306 [197376/225000 (88%)] Loss: 16896.531250\n",
      "Train Epoch: 306 [199872/225000 (89%)] Loss: 17215.324219\n",
      "Train Epoch: 306 [202368/225000 (90%)] Loss: 17076.929688\n",
      "Train Epoch: 306 [204864/225000 (91%)] Loss: 17129.187500\n",
      "Train Epoch: 306 [207360/225000 (92%)] Loss: 17821.453125\n",
      "Train Epoch: 306 [209856/225000 (93%)] Loss: 17252.240234\n",
      "Train Epoch: 306 [212352/225000 (94%)] Loss: 17519.171875\n",
      "Train Epoch: 306 [214848/225000 (95%)] Loss: 17705.378906\n",
      "Train Epoch: 306 [217344/225000 (97%)] Loss: 17098.230469\n",
      "Train Epoch: 306 [219840/225000 (98%)] Loss: 17930.906250\n",
      "Train Epoch: 306 [222336/225000 (99%)] Loss: 17058.634766\n",
      "Train Epoch: 306 [224832/225000 (100%)] Loss: 17192.503906\n",
      "    epoch          : 306\n",
      "    loss           : 17269.041521404382\n",
      "    val_loss       : 17206.640163154094\n",
      "Train Epoch: 307 [192/225000 (0%)] Loss: 17627.394531\n",
      "Train Epoch: 307 [2688/225000 (1%)] Loss: 17240.494141\n",
      "Train Epoch: 307 [5184/225000 (2%)] Loss: 16974.167969\n",
      "Train Epoch: 307 [7680/225000 (3%)] Loss: 17091.093750\n",
      "Train Epoch: 307 [10176/225000 (5%)] Loss: 17420.738281\n",
      "Train Epoch: 307 [12672/225000 (6%)] Loss: 16950.964844\n",
      "Train Epoch: 307 [15168/225000 (7%)] Loss: 17429.208984\n",
      "Train Epoch: 307 [17664/225000 (8%)] Loss: 17120.656250\n",
      "Train Epoch: 307 [20160/225000 (9%)] Loss: 17168.699219\n",
      "Train Epoch: 307 [22656/225000 (10%)] Loss: 17060.673828\n",
      "Train Epoch: 307 [25152/225000 (11%)] Loss: 17148.005859\n",
      "Train Epoch: 307 [27648/225000 (12%)] Loss: 17096.681641\n",
      "Train Epoch: 307 [30144/225000 (13%)] Loss: 17217.765625\n",
      "Train Epoch: 307 [32640/225000 (15%)] Loss: 17707.187500\n",
      "Train Epoch: 307 [35136/225000 (16%)] Loss: 17021.744141\n",
      "Train Epoch: 307 [37632/225000 (17%)] Loss: 17057.328125\n",
      "Train Epoch: 307 [40128/225000 (18%)] Loss: 17138.871094\n",
      "Train Epoch: 307 [42624/225000 (19%)] Loss: 16990.933594\n",
      "Train Epoch: 307 [45120/225000 (20%)] Loss: 17036.417969\n",
      "Train Epoch: 307 [47616/225000 (21%)] Loss: 17181.238281\n",
      "Train Epoch: 307 [50112/225000 (22%)] Loss: 17796.292969\n",
      "Train Epoch: 307 [52608/225000 (23%)] Loss: 17281.644531\n",
      "Train Epoch: 307 [55104/225000 (24%)] Loss: 17003.238281\n",
      "Train Epoch: 307 [57600/225000 (26%)] Loss: 16815.859375\n",
      "Train Epoch: 307 [60096/225000 (27%)] Loss: 17225.777344\n",
      "Train Epoch: 307 [62592/225000 (28%)] Loss: 16765.921875\n",
      "Train Epoch: 307 [65088/225000 (29%)] Loss: 17314.878906\n",
      "Train Epoch: 307 [67584/225000 (30%)] Loss: 16955.433594\n",
      "Train Epoch: 307 [70080/225000 (31%)] Loss: 17105.308594\n",
      "Train Epoch: 307 [72576/225000 (32%)] Loss: 17134.998047\n",
      "Train Epoch: 307 [75072/225000 (33%)] Loss: 17679.626953\n",
      "Train Epoch: 307 [77568/225000 (34%)] Loss: 16739.382812\n",
      "Train Epoch: 307 [80064/225000 (36%)] Loss: 17450.519531\n",
      "Train Epoch: 307 [82560/225000 (37%)] Loss: 17718.726562\n",
      "Train Epoch: 307 [85056/225000 (38%)] Loss: 17271.119141\n",
      "Train Epoch: 307 [87552/225000 (39%)] Loss: 17142.544922\n",
      "Train Epoch: 307 [90048/225000 (40%)] Loss: 17163.417969\n",
      "Train Epoch: 307 [92544/225000 (41%)] Loss: 17132.582031\n",
      "Train Epoch: 307 [95040/225000 (42%)] Loss: 17366.613281\n",
      "Train Epoch: 307 [97536/225000 (43%)] Loss: 16960.382812\n",
      "Train Epoch: 307 [100032/225000 (44%)] Loss: 17542.046875\n",
      "Train Epoch: 307 [102528/225000 (46%)] Loss: 16974.730469\n",
      "Train Epoch: 307 [105024/225000 (47%)] Loss: 17264.263672\n",
      "Train Epoch: 307 [107520/225000 (48%)] Loss: 16929.787109\n",
      "Train Epoch: 307 [110016/225000 (49%)] Loss: 17230.597656\n",
      "Train Epoch: 307 [112512/225000 (50%)] Loss: 17585.445312\n",
      "Train Epoch: 307 [115008/225000 (51%)] Loss: 16790.917969\n",
      "Train Epoch: 307 [117504/225000 (52%)] Loss: 17552.140625\n",
      "Train Epoch: 307 [120000/225000 (53%)] Loss: 17682.589844\n",
      "Train Epoch: 307 [122496/225000 (54%)] Loss: 17196.833984\n",
      "Train Epoch: 307 [124992/225000 (56%)] Loss: 17204.675781\n",
      "Train Epoch: 307 [127488/225000 (57%)] Loss: 16626.277344\n",
      "Train Epoch: 307 [129984/225000 (58%)] Loss: 17008.773438\n",
      "Train Epoch: 307 [132480/225000 (59%)] Loss: 16775.039062\n",
      "Train Epoch: 307 [134976/225000 (60%)] Loss: 17311.164062\n",
      "Train Epoch: 307 [137472/225000 (61%)] Loss: 17603.027344\n",
      "Train Epoch: 307 [139968/225000 (62%)] Loss: 16883.902344\n",
      "Train Epoch: 307 [142464/225000 (63%)] Loss: 17546.380859\n",
      "Train Epoch: 307 [144960/225000 (64%)] Loss: 17393.015625\n",
      "Train Epoch: 307 [147456/225000 (66%)] Loss: 16740.238281\n",
      "Train Epoch: 307 [149952/225000 (67%)] Loss: 19458.117188\n",
      "Train Epoch: 307 [152448/225000 (68%)] Loss: 17025.369141\n",
      "Train Epoch: 307 [154944/225000 (69%)] Loss: 16920.419922\n",
      "Train Epoch: 307 [157440/225000 (70%)] Loss: 17237.371094\n",
      "Train Epoch: 307 [159936/225000 (71%)] Loss: 17428.414062\n",
      "Train Epoch: 307 [162432/225000 (72%)] Loss: 17445.648438\n",
      "Train Epoch: 307 [164928/225000 (73%)] Loss: 17160.734375\n",
      "Train Epoch: 307 [167424/225000 (74%)] Loss: 16949.453125\n",
      "Train Epoch: 307 [169920/225000 (76%)] Loss: 17432.863281\n",
      "Train Epoch: 307 [172416/225000 (77%)] Loss: 16714.888672\n",
      "Train Epoch: 307 [174912/225000 (78%)] Loss: 17109.861328\n",
      "Train Epoch: 307 [177408/225000 (79%)] Loss: 17101.601562\n",
      "Train Epoch: 307 [179904/225000 (80%)] Loss: 17543.683594\n",
      "Train Epoch: 307 [182400/225000 (81%)] Loss: 17556.519531\n",
      "Train Epoch: 307 [184896/225000 (82%)] Loss: 17176.757812\n",
      "Train Epoch: 307 [187392/225000 (83%)] Loss: 16977.875000\n",
      "Train Epoch: 307 [189888/225000 (84%)] Loss: 17034.156250\n",
      "Train Epoch: 307 [192384/225000 (86%)] Loss: 16934.871094\n",
      "Train Epoch: 307 [194880/225000 (87%)] Loss: 17065.953125\n",
      "Train Epoch: 307 [197376/225000 (88%)] Loss: 17025.472656\n",
      "Train Epoch: 307 [199872/225000 (89%)] Loss: 17042.902344\n",
      "Train Epoch: 307 [202368/225000 (90%)] Loss: 16836.140625\n",
      "Train Epoch: 307 [204864/225000 (91%)] Loss: 17443.738281\n",
      "Train Epoch: 307 [207360/225000 (92%)] Loss: 17445.273438\n",
      "Train Epoch: 307 [209856/225000 (93%)] Loss: 17333.228516\n",
      "Train Epoch: 307 [212352/225000 (94%)] Loss: 17349.777344\n",
      "Train Epoch: 307 [214848/225000 (95%)] Loss: 17436.300781\n",
      "Train Epoch: 307 [217344/225000 (97%)] Loss: 17470.910156\n",
      "Train Epoch: 307 [219840/225000 (98%)] Loss: 17366.808594\n",
      "Train Epoch: 307 [222336/225000 (99%)] Loss: 16756.523438\n",
      "Train Epoch: 307 [224832/225000 (100%)] Loss: 17425.632812\n",
      "    epoch          : 307\n",
      "    loss           : 17230.331437146706\n",
      "    val_loss       : 17189.975688514365\n",
      "Train Epoch: 308 [192/225000 (0%)] Loss: 17116.380859\n",
      "Train Epoch: 308 [2688/225000 (1%)] Loss: 17141.871094\n",
      "Train Epoch: 308 [5184/225000 (2%)] Loss: 17831.718750\n",
      "Train Epoch: 308 [7680/225000 (3%)] Loss: 17485.996094\n",
      "Train Epoch: 308 [10176/225000 (5%)] Loss: 16942.677734\n",
      "Train Epoch: 308 [12672/225000 (6%)] Loss: 17643.230469\n",
      "Train Epoch: 308 [15168/225000 (7%)] Loss: 17202.910156\n",
      "Train Epoch: 308 [17664/225000 (8%)] Loss: 17950.417969\n",
      "Train Epoch: 308 [20160/225000 (9%)] Loss: 17005.664062\n",
      "Train Epoch: 308 [22656/225000 (10%)] Loss: 16643.054688\n",
      "Train Epoch: 308 [25152/225000 (11%)] Loss: 17524.302734\n",
      "Train Epoch: 308 [27648/225000 (12%)] Loss: 17512.582031\n",
      "Train Epoch: 308 [30144/225000 (13%)] Loss: 17422.355469\n",
      "Train Epoch: 308 [32640/225000 (15%)] Loss: 17661.789062\n",
      "Train Epoch: 308 [35136/225000 (16%)] Loss: 17544.597656\n",
      "Train Epoch: 308 [37632/225000 (17%)] Loss: 16984.292969\n",
      "Train Epoch: 308 [40128/225000 (18%)] Loss: 17812.773438\n",
      "Train Epoch: 308 [42624/225000 (19%)] Loss: 17147.587891\n",
      "Train Epoch: 308 [45120/225000 (20%)] Loss: 16761.291016\n",
      "Train Epoch: 308 [47616/225000 (21%)] Loss: 17352.251953\n",
      "Train Epoch: 308 [50112/225000 (22%)] Loss: 17311.398438\n",
      "Train Epoch: 308 [52608/225000 (23%)] Loss: 16961.224609\n",
      "Train Epoch: 308 [55104/225000 (24%)] Loss: 17041.720703\n",
      "Train Epoch: 308 [57600/225000 (26%)] Loss: 16811.914062\n",
      "Train Epoch: 308 [60096/225000 (27%)] Loss: 16886.046875\n",
      "Train Epoch: 308 [62592/225000 (28%)] Loss: 17345.839844\n",
      "Train Epoch: 308 [65088/225000 (29%)] Loss: 17006.375000\n",
      "Train Epoch: 308 [67584/225000 (30%)] Loss: 17226.429688\n",
      "Train Epoch: 308 [70080/225000 (31%)] Loss: 16997.453125\n",
      "Train Epoch: 308 [72576/225000 (32%)] Loss: 17099.351562\n",
      "Train Epoch: 308 [75072/225000 (33%)] Loss: 17183.164062\n",
      "Train Epoch: 308 [77568/225000 (34%)] Loss: 17259.843750\n",
      "Train Epoch: 308 [80064/225000 (36%)] Loss: 17076.302734\n",
      "Train Epoch: 308 [82560/225000 (37%)] Loss: 17792.593750\n",
      "Train Epoch: 308 [85056/225000 (38%)] Loss: 17043.375000\n",
      "Train Epoch: 308 [87552/225000 (39%)] Loss: 17516.851562\n",
      "Train Epoch: 308 [90048/225000 (40%)] Loss: 17674.732422\n",
      "Train Epoch: 308 [92544/225000 (41%)] Loss: 17218.632812\n",
      "Train Epoch: 308 [95040/225000 (42%)] Loss: 16787.464844\n",
      "Train Epoch: 308 [97536/225000 (43%)] Loss: 17246.960938\n",
      "Train Epoch: 308 [100032/225000 (44%)] Loss: 16641.929688\n",
      "Train Epoch: 308 [102528/225000 (46%)] Loss: 17507.226562\n",
      "Train Epoch: 308 [105024/225000 (47%)] Loss: 17163.789062\n",
      "Train Epoch: 308 [107520/225000 (48%)] Loss: 17673.562500\n",
      "Train Epoch: 308 [110016/225000 (49%)] Loss: 17131.974609\n",
      "Train Epoch: 308 [112512/225000 (50%)] Loss: 17372.933594\n",
      "Train Epoch: 308 [115008/225000 (51%)] Loss: 17316.511719\n",
      "Train Epoch: 308 [117504/225000 (52%)] Loss: 17346.320312\n",
      "Train Epoch: 308 [120000/225000 (53%)] Loss: 17139.984375\n",
      "Train Epoch: 308 [122496/225000 (54%)] Loss: 17528.773438\n",
      "Train Epoch: 308 [124992/225000 (56%)] Loss: 17337.044922\n",
      "Train Epoch: 308 [127488/225000 (57%)] Loss: 17020.687500\n",
      "Train Epoch: 308 [129984/225000 (58%)] Loss: 17398.902344\n",
      "Train Epoch: 308 [132480/225000 (59%)] Loss: 17300.738281\n",
      "Train Epoch: 308 [134976/225000 (60%)] Loss: 17691.964844\n",
      "Train Epoch: 308 [137472/225000 (61%)] Loss: 17416.906250\n",
      "Train Epoch: 308 [139968/225000 (62%)] Loss: 17133.992188\n",
      "Train Epoch: 308 [142464/225000 (63%)] Loss: 17127.285156\n",
      "Train Epoch: 308 [144960/225000 (64%)] Loss: 17143.828125\n",
      "Train Epoch: 308 [147456/225000 (66%)] Loss: 17365.421875\n",
      "Train Epoch: 308 [149952/225000 (67%)] Loss: 17053.599609\n",
      "Train Epoch: 308 [152448/225000 (68%)] Loss: 17036.839844\n",
      "Train Epoch: 308 [154944/225000 (69%)] Loss: 16973.445312\n",
      "Train Epoch: 308 [157440/225000 (70%)] Loss: 16921.917969\n",
      "Train Epoch: 308 [159936/225000 (71%)] Loss: 17090.152344\n",
      "Train Epoch: 308 [162432/225000 (72%)] Loss: 17554.593750\n",
      "Train Epoch: 308 [164928/225000 (73%)] Loss: 17339.230469\n",
      "Train Epoch: 308 [167424/225000 (74%)] Loss: 17333.046875\n",
      "Train Epoch: 308 [169920/225000 (76%)] Loss: 16813.515625\n",
      "Train Epoch: 308 [172416/225000 (77%)] Loss: 17442.753906\n",
      "Train Epoch: 308 [174912/225000 (78%)] Loss: 17073.519531\n",
      "Train Epoch: 308 [177408/225000 (79%)] Loss: 17235.664062\n",
      "Train Epoch: 308 [179904/225000 (80%)] Loss: 17737.632812\n",
      "Train Epoch: 308 [182400/225000 (81%)] Loss: 16792.117188\n",
      "Train Epoch: 308 [184896/225000 (82%)] Loss: 17040.062500\n",
      "Train Epoch: 308 [187392/225000 (83%)] Loss: 17098.382812\n",
      "Train Epoch: 308 [189888/225000 (84%)] Loss: 16857.179688\n",
      "Train Epoch: 308 [192384/225000 (86%)] Loss: 17255.070312\n",
      "Train Epoch: 308 [194880/225000 (87%)] Loss: 17132.066406\n",
      "Train Epoch: 308 [197376/225000 (88%)] Loss: 17462.593750\n",
      "Train Epoch: 308 [199872/225000 (89%)] Loss: 16956.158203\n",
      "Train Epoch: 308 [202368/225000 (90%)] Loss: 16982.027344\n",
      "Train Epoch: 308 [204864/225000 (91%)] Loss: 17082.324219\n",
      "Train Epoch: 308 [207360/225000 (92%)] Loss: 17247.589844\n",
      "Train Epoch: 308 [209856/225000 (93%)] Loss: 17290.187500\n",
      "Train Epoch: 308 [212352/225000 (94%)] Loss: 17045.392578\n",
      "Train Epoch: 308 [214848/225000 (95%)] Loss: 17162.912109\n",
      "Train Epoch: 308 [217344/225000 (97%)] Loss: 17186.718750\n",
      "Train Epoch: 308 [219840/225000 (98%)] Loss: 17252.636719\n",
      "Train Epoch: 308 [222336/225000 (99%)] Loss: 17067.083984\n",
      "Train Epoch: 308 [224832/225000 (100%)] Loss: 17015.701172\n",
      "    epoch          : 308\n",
      "    loss           : 17222.77766704885\n",
      "    val_loss       : 17309.613361271284\n",
      "Train Epoch: 309 [192/225000 (0%)] Loss: 17615.417969\n",
      "Train Epoch: 309 [2688/225000 (1%)] Loss: 16652.273438\n",
      "Train Epoch: 309 [5184/225000 (2%)] Loss: 17081.015625\n",
      "Train Epoch: 309 [7680/225000 (3%)] Loss: 17302.121094\n",
      "Train Epoch: 309 [10176/225000 (5%)] Loss: 17040.009766\n",
      "Train Epoch: 309 [12672/225000 (6%)] Loss: 17418.453125\n",
      "Train Epoch: 309 [15168/225000 (7%)] Loss: 16681.281250\n",
      "Train Epoch: 309 [17664/225000 (8%)] Loss: 17137.093750\n",
      "Train Epoch: 309 [20160/225000 (9%)] Loss: 17270.496094\n",
      "Train Epoch: 309 [22656/225000 (10%)] Loss: 16853.468750\n",
      "Train Epoch: 309 [25152/225000 (11%)] Loss: 17081.320312\n",
      "Train Epoch: 309 [27648/225000 (12%)] Loss: 17238.898438\n",
      "Train Epoch: 309 [30144/225000 (13%)] Loss: 17352.197266\n",
      "Train Epoch: 309 [32640/225000 (15%)] Loss: 17226.484375\n",
      "Train Epoch: 309 [35136/225000 (16%)] Loss: 17205.541016\n",
      "Train Epoch: 309 [37632/225000 (17%)] Loss: 17275.716797\n",
      "Train Epoch: 309 [40128/225000 (18%)] Loss: 17089.187500\n",
      "Train Epoch: 309 [42624/225000 (19%)] Loss: 17370.246094\n",
      "Train Epoch: 309 [45120/225000 (20%)] Loss: 17391.572266\n",
      "Train Epoch: 309 [47616/225000 (21%)] Loss: 17362.820312\n",
      "Train Epoch: 309 [50112/225000 (22%)] Loss: 17178.890625\n",
      "Train Epoch: 309 [52608/225000 (23%)] Loss: 17492.332031\n",
      "Train Epoch: 309 [55104/225000 (24%)] Loss: 16999.769531\n",
      "Train Epoch: 309 [57600/225000 (26%)] Loss: 17534.218750\n",
      "Train Epoch: 309 [60096/225000 (27%)] Loss: 16937.796875\n",
      "Train Epoch: 309 [62592/225000 (28%)] Loss: 17642.511719\n",
      "Train Epoch: 309 [65088/225000 (29%)] Loss: 16821.988281\n",
      "Train Epoch: 309 [67584/225000 (30%)] Loss: 17680.214844\n",
      "Train Epoch: 309 [70080/225000 (31%)] Loss: 17283.878906\n",
      "Train Epoch: 309 [72576/225000 (32%)] Loss: 17417.005859\n",
      "Train Epoch: 309 [75072/225000 (33%)] Loss: 17643.681641\n",
      "Train Epoch: 309 [77568/225000 (34%)] Loss: 17492.429688\n",
      "Train Epoch: 309 [80064/225000 (36%)] Loss: 16648.384766\n",
      "Train Epoch: 309 [82560/225000 (37%)] Loss: 17357.224609\n",
      "Train Epoch: 309 [85056/225000 (38%)] Loss: 17294.708984\n",
      "Train Epoch: 309 [87552/225000 (39%)] Loss: 16884.371094\n",
      "Train Epoch: 309 [90048/225000 (40%)] Loss: 16920.273438\n",
      "Train Epoch: 309 [92544/225000 (41%)] Loss: 17622.468750\n",
      "Train Epoch: 309 [95040/225000 (42%)] Loss: 17604.929688\n",
      "Train Epoch: 309 [97536/225000 (43%)] Loss: 17254.656250\n",
      "Train Epoch: 309 [100032/225000 (44%)] Loss: 16954.460938\n",
      "Train Epoch: 309 [102528/225000 (46%)] Loss: 17375.404297\n",
      "Train Epoch: 309 [105024/225000 (47%)] Loss: 17693.605469\n",
      "Train Epoch: 309 [107520/225000 (48%)] Loss: 16940.507812\n",
      "Train Epoch: 309 [110016/225000 (49%)] Loss: 17559.343750\n",
      "Train Epoch: 309 [112512/225000 (50%)] Loss: 17266.617188\n",
      "Train Epoch: 309 [115008/225000 (51%)] Loss: 17862.812500\n",
      "Train Epoch: 309 [117504/225000 (52%)] Loss: 17215.044922\n",
      "Train Epoch: 309 [120000/225000 (53%)] Loss: 17482.976562\n",
      "Train Epoch: 309 [122496/225000 (54%)] Loss: 17162.996094\n",
      "Train Epoch: 309 [124992/225000 (56%)] Loss: 17316.527344\n",
      "Train Epoch: 309 [127488/225000 (57%)] Loss: 17288.343750\n",
      "Train Epoch: 309 [129984/225000 (58%)] Loss: 17649.119141\n",
      "Train Epoch: 309 [132480/225000 (59%)] Loss: 17636.453125\n",
      "Train Epoch: 309 [134976/225000 (60%)] Loss: 17002.511719\n",
      "Train Epoch: 309 [137472/225000 (61%)] Loss: 17214.478516\n",
      "Train Epoch: 309 [139968/225000 (62%)] Loss: 17090.144531\n",
      "Train Epoch: 309 [142464/225000 (63%)] Loss: 17476.541016\n",
      "Train Epoch: 309 [144960/225000 (64%)] Loss: 17299.671875\n",
      "Train Epoch: 309 [147456/225000 (66%)] Loss: 16863.732422\n",
      "Train Epoch: 309 [149952/225000 (67%)] Loss: 16615.970703\n",
      "Train Epoch: 309 [152448/225000 (68%)] Loss: 17505.949219\n",
      "Train Epoch: 309 [154944/225000 (69%)] Loss: 16884.300781\n",
      "Train Epoch: 309 [157440/225000 (70%)] Loss: 16906.691406\n",
      "Train Epoch: 309 [159936/225000 (71%)] Loss: 17305.000000\n",
      "Train Epoch: 309 [162432/225000 (72%)] Loss: 17134.154297\n",
      "Train Epoch: 309 [164928/225000 (73%)] Loss: 17203.205078\n",
      "Train Epoch: 309 [167424/225000 (74%)] Loss: 17277.984375\n",
      "Train Epoch: 309 [169920/225000 (76%)] Loss: 16981.277344\n",
      "Train Epoch: 309 [172416/225000 (77%)] Loss: 16986.548828\n",
      "Train Epoch: 309 [174912/225000 (78%)] Loss: 17458.529297\n",
      "Train Epoch: 309 [177408/225000 (79%)] Loss: 17305.167969\n",
      "Train Epoch: 309 [179904/225000 (80%)] Loss: 17086.945312\n",
      "Train Epoch: 309 [182400/225000 (81%)] Loss: 17176.867188\n",
      "Train Epoch: 309 [184896/225000 (82%)] Loss: 17223.482422\n",
      "Train Epoch: 309 [187392/225000 (83%)] Loss: 16906.978516\n",
      "Train Epoch: 309 [189888/225000 (84%)] Loss: 17368.957031\n",
      "Train Epoch: 309 [192384/225000 (86%)] Loss: 17123.550781\n",
      "Train Epoch: 309 [194880/225000 (87%)] Loss: 17331.607422\n",
      "Train Epoch: 309 [197376/225000 (88%)] Loss: 17055.347656\n",
      "Train Epoch: 309 [199872/225000 (89%)] Loss: 17584.257812\n",
      "Train Epoch: 309 [202368/225000 (90%)] Loss: 17399.769531\n",
      "Train Epoch: 309 [204864/225000 (91%)] Loss: 17343.953125\n",
      "Train Epoch: 309 [207360/225000 (92%)] Loss: 17171.748047\n",
      "Train Epoch: 309 [209856/225000 (93%)] Loss: 17250.615234\n",
      "Train Epoch: 309 [212352/225000 (94%)] Loss: 17423.585938\n",
      "Train Epoch: 309 [214848/225000 (95%)] Loss: 17197.281250\n",
      "Train Epoch: 309 [217344/225000 (97%)] Loss: 17029.884766\n",
      "Train Epoch: 309 [219840/225000 (98%)] Loss: 17669.929688\n",
      "Train Epoch: 309 [222336/225000 (99%)] Loss: 17193.425781\n",
      "Train Epoch: 309 [224832/225000 (100%)] Loss: 17166.425781\n",
      "    epoch          : 309\n",
      "    loss           : 17218.139131825938\n",
      "    val_loss       : 17153.787042905355\n",
      "Train Epoch: 310 [192/225000 (0%)] Loss: 16772.242188\n",
      "Train Epoch: 310 [2688/225000 (1%)] Loss: 17295.744141\n",
      "Train Epoch: 310 [5184/225000 (2%)] Loss: 17195.341797\n",
      "Train Epoch: 310 [7680/225000 (3%)] Loss: 17382.289062\n",
      "Train Epoch: 310 [10176/225000 (5%)] Loss: 17616.689453\n",
      "Train Epoch: 310 [12672/225000 (6%)] Loss: 17121.718750\n",
      "Train Epoch: 310 [15168/225000 (7%)] Loss: 17157.234375\n",
      "Train Epoch: 310 [17664/225000 (8%)] Loss: 17215.300781\n",
      "Train Epoch: 310 [20160/225000 (9%)] Loss: 17421.931641\n",
      "Train Epoch: 310 [22656/225000 (10%)] Loss: 17320.339844\n",
      "Train Epoch: 310 [25152/225000 (11%)] Loss: 17480.935547\n",
      "Train Epoch: 310 [27648/225000 (12%)] Loss: 17526.160156\n",
      "Train Epoch: 310 [30144/225000 (13%)] Loss: 17234.609375\n",
      "Train Epoch: 310 [32640/225000 (15%)] Loss: 17539.898438\n",
      "Train Epoch: 310 [35136/225000 (16%)] Loss: 17065.187500\n",
      "Train Epoch: 310 [37632/225000 (17%)] Loss: 17098.796875\n",
      "Train Epoch: 310 [40128/225000 (18%)] Loss: 16742.945312\n",
      "Train Epoch: 310 [42624/225000 (19%)] Loss: 16955.871094\n",
      "Train Epoch: 310 [45120/225000 (20%)] Loss: 17183.941406\n",
      "Train Epoch: 310 [47616/225000 (21%)] Loss: 17132.710938\n",
      "Train Epoch: 310 [50112/225000 (22%)] Loss: 17473.484375\n",
      "Train Epoch: 310 [52608/225000 (23%)] Loss: 17012.972656\n",
      "Train Epoch: 310 [55104/225000 (24%)] Loss: 16645.054688\n",
      "Train Epoch: 310 [57600/225000 (26%)] Loss: 17417.380859\n",
      "Train Epoch: 310 [60096/225000 (27%)] Loss: 17619.265625\n",
      "Train Epoch: 310 [62592/225000 (28%)] Loss: 17218.699219\n",
      "Train Epoch: 310 [65088/225000 (29%)] Loss: 17515.488281\n",
      "Train Epoch: 310 [67584/225000 (30%)] Loss: 16889.521484\n",
      "Train Epoch: 310 [70080/225000 (31%)] Loss: 16843.593750\n",
      "Train Epoch: 310 [72576/225000 (32%)] Loss: 17263.503906\n",
      "Train Epoch: 310 [75072/225000 (33%)] Loss: 17200.111328\n",
      "Train Epoch: 310 [77568/225000 (34%)] Loss: 17298.587891\n",
      "Train Epoch: 310 [80064/225000 (36%)] Loss: 17119.712891\n",
      "Train Epoch: 310 [82560/225000 (37%)] Loss: 16668.189453\n",
      "Train Epoch: 310 [85056/225000 (38%)] Loss: 17241.738281\n",
      "Train Epoch: 310 [87552/225000 (39%)] Loss: 17041.689453\n",
      "Train Epoch: 310 [90048/225000 (40%)] Loss: 16784.144531\n",
      "Train Epoch: 310 [92544/225000 (41%)] Loss: 17028.193359\n",
      "Train Epoch: 310 [95040/225000 (42%)] Loss: 17149.505859\n",
      "Train Epoch: 310 [97536/225000 (43%)] Loss: 16832.695312\n",
      "Train Epoch: 310 [100032/225000 (44%)] Loss: 17080.207031\n",
      "Train Epoch: 310 [102528/225000 (46%)] Loss: 17051.357422\n",
      "Train Epoch: 310 [105024/225000 (47%)] Loss: 17351.875000\n",
      "Train Epoch: 310 [107520/225000 (48%)] Loss: 17530.710938\n",
      "Train Epoch: 310 [110016/225000 (49%)] Loss: 16878.408203\n",
      "Train Epoch: 310 [112512/225000 (50%)] Loss: 16645.052734\n",
      "Train Epoch: 310 [115008/225000 (51%)] Loss: 17325.945312\n",
      "Train Epoch: 310 [117504/225000 (52%)] Loss: 17470.996094\n",
      "Train Epoch: 310 [120000/225000 (53%)] Loss: 16928.457031\n",
      "Train Epoch: 310 [122496/225000 (54%)] Loss: 17510.878906\n",
      "Train Epoch: 310 [124992/225000 (56%)] Loss: 17187.207031\n",
      "Train Epoch: 310 [127488/225000 (57%)] Loss: 17304.925781\n",
      "Train Epoch: 310 [129984/225000 (58%)] Loss: 17096.484375\n",
      "Train Epoch: 310 [132480/225000 (59%)] Loss: 17128.443359\n",
      "Train Epoch: 310 [134976/225000 (60%)] Loss: 17636.062500\n",
      "Train Epoch: 310 [137472/225000 (61%)] Loss: 17386.648438\n",
      "Train Epoch: 310 [139968/225000 (62%)] Loss: 17411.605469\n",
      "Train Epoch: 310 [142464/225000 (63%)] Loss: 17304.580078\n",
      "Train Epoch: 310 [144960/225000 (64%)] Loss: 17388.964844\n",
      "Train Epoch: 310 [147456/225000 (66%)] Loss: 17474.855469\n",
      "Train Epoch: 310 [149952/225000 (67%)] Loss: 17455.699219\n",
      "Train Epoch: 310 [152448/225000 (68%)] Loss: 17262.648438\n",
      "Train Epoch: 310 [154944/225000 (69%)] Loss: 17045.878906\n",
      "Train Epoch: 310 [157440/225000 (70%)] Loss: 17126.402344\n",
      "Train Epoch: 310 [159936/225000 (71%)] Loss: 17154.025391\n",
      "Train Epoch: 310 [162432/225000 (72%)] Loss: 17587.812500\n",
      "Train Epoch: 310 [164928/225000 (73%)] Loss: 17094.335938\n",
      "Train Epoch: 310 [167424/225000 (74%)] Loss: 16957.554688\n",
      "Train Epoch: 310 [169920/225000 (76%)] Loss: 17093.890625\n",
      "Train Epoch: 310 [172416/225000 (77%)] Loss: 16831.017578\n",
      "Train Epoch: 310 [174912/225000 (78%)] Loss: 17215.644531\n",
      "Train Epoch: 310 [177408/225000 (79%)] Loss: 17409.277344\n",
      "Train Epoch: 310 [179904/225000 (80%)] Loss: 17092.296875\n",
      "Train Epoch: 310 [182400/225000 (81%)] Loss: 16999.777344\n",
      "Train Epoch: 310 [184896/225000 (82%)] Loss: 17292.125000\n",
      "Train Epoch: 310 [187392/225000 (83%)] Loss: 16947.773438\n",
      "Train Epoch: 310 [189888/225000 (84%)] Loss: 17029.972656\n",
      "Train Epoch: 310 [192384/225000 (86%)] Loss: 17730.406250\n",
      "Train Epoch: 310 [194880/225000 (87%)] Loss: 16880.722656\n",
      "Train Epoch: 310 [197376/225000 (88%)] Loss: 17002.193359\n",
      "Train Epoch: 310 [199872/225000 (89%)] Loss: 16618.400391\n",
      "Train Epoch: 310 [202368/225000 (90%)] Loss: 17163.105469\n",
      "Train Epoch: 310 [204864/225000 (91%)] Loss: 17509.464844\n",
      "Train Epoch: 310 [207360/225000 (92%)] Loss: 17516.962891\n",
      "Train Epoch: 310 [209856/225000 (93%)] Loss: 17683.193359\n",
      "Train Epoch: 310 [212352/225000 (94%)] Loss: 17194.746094\n",
      "Train Epoch: 310 [214848/225000 (95%)] Loss: 17205.660156\n",
      "Train Epoch: 310 [217344/225000 (97%)] Loss: 17473.730469\n",
      "Train Epoch: 310 [219840/225000 (98%)] Loss: 17209.146484\n",
      "Train Epoch: 310 [222336/225000 (99%)] Loss: 16946.917969\n",
      "Train Epoch: 310 [224832/225000 (100%)] Loss: 16769.443359\n",
      "    epoch          : 310\n",
      "    loss           : 17189.875063326577\n",
      "    val_loss       : 17115.085035190783\n",
      "Train Epoch: 311 [192/225000 (0%)] Loss: 16932.703125\n",
      "Train Epoch: 311 [2688/225000 (1%)] Loss: 17169.058594\n",
      "Train Epoch: 311 [5184/225000 (2%)] Loss: 17109.613281\n",
      "Train Epoch: 311 [7680/225000 (3%)] Loss: 17804.347656\n",
      "Train Epoch: 311 [10176/225000 (5%)] Loss: 17003.373047\n",
      "Train Epoch: 311 [12672/225000 (6%)] Loss: 18832.515625\n",
      "Train Epoch: 311 [15168/225000 (7%)] Loss: 17183.234375\n",
      "Train Epoch: 311 [17664/225000 (8%)] Loss: 16850.261719\n",
      "Train Epoch: 311 [20160/225000 (9%)] Loss: 16835.775391\n",
      "Train Epoch: 311 [22656/225000 (10%)] Loss: 17476.587891\n",
      "Train Epoch: 311 [25152/225000 (11%)] Loss: 17178.007812\n",
      "Train Epoch: 311 [27648/225000 (12%)] Loss: 17145.464844\n",
      "Train Epoch: 311 [30144/225000 (13%)] Loss: 17391.214844\n",
      "Train Epoch: 311 [32640/225000 (15%)] Loss: 17469.679688\n",
      "Train Epoch: 311 [35136/225000 (16%)] Loss: 17144.041016\n",
      "Train Epoch: 311 [37632/225000 (17%)] Loss: 17649.548828\n",
      "Train Epoch: 311 [40128/225000 (18%)] Loss: 17522.187500\n",
      "Train Epoch: 311 [42624/225000 (19%)] Loss: 17333.136719\n",
      "Train Epoch: 311 [45120/225000 (20%)] Loss: 17084.355469\n",
      "Train Epoch: 311 [47616/225000 (21%)] Loss: 17482.398438\n",
      "Train Epoch: 311 [50112/225000 (22%)] Loss: 17476.066406\n",
      "Train Epoch: 311 [52608/225000 (23%)] Loss: 17823.160156\n",
      "Train Epoch: 311 [55104/225000 (24%)] Loss: 17086.484375\n",
      "Train Epoch: 311 [57600/225000 (26%)] Loss: 17019.789062\n",
      "Train Epoch: 311 [60096/225000 (27%)] Loss: 17465.757812\n",
      "Train Epoch: 311 [62592/225000 (28%)] Loss: 16909.613281\n",
      "Train Epoch: 311 [65088/225000 (29%)] Loss: 17294.703125\n",
      "Train Epoch: 311 [67584/225000 (30%)] Loss: 16806.718750\n",
      "Train Epoch: 311 [70080/225000 (31%)] Loss: 16821.578125\n",
      "Train Epoch: 311 [72576/225000 (32%)] Loss: 17675.894531\n",
      "Train Epoch: 311 [75072/225000 (33%)] Loss: 17525.544922\n",
      "Train Epoch: 311 [77568/225000 (34%)] Loss: 16582.998047\n",
      "Train Epoch: 311 [80064/225000 (36%)] Loss: 17446.898438\n",
      "Train Epoch: 311 [82560/225000 (37%)] Loss: 17302.466797\n",
      "Train Epoch: 311 [85056/225000 (38%)] Loss: 17050.890625\n",
      "Train Epoch: 311 [87552/225000 (39%)] Loss: 17231.644531\n",
      "Train Epoch: 311 [90048/225000 (40%)] Loss: 16765.562500\n",
      "Train Epoch: 311 [92544/225000 (41%)] Loss: 17166.394531\n",
      "Train Epoch: 311 [95040/225000 (42%)] Loss: 17285.464844\n",
      "Train Epoch: 311 [97536/225000 (43%)] Loss: 16569.816406\n",
      "Train Epoch: 311 [100032/225000 (44%)] Loss: 17608.218750\n",
      "Train Epoch: 311 [102528/225000 (46%)] Loss: 17262.128906\n",
      "Train Epoch: 311 [105024/225000 (47%)] Loss: 17096.289062\n",
      "Train Epoch: 311 [107520/225000 (48%)] Loss: 17208.519531\n",
      "Train Epoch: 311 [110016/225000 (49%)] Loss: 17417.550781\n",
      "Train Epoch: 311 [112512/225000 (50%)] Loss: 17307.144531\n",
      "Train Epoch: 311 [115008/225000 (51%)] Loss: 17291.630859\n",
      "Train Epoch: 311 [117504/225000 (52%)] Loss: 17399.673828\n",
      "Train Epoch: 311 [120000/225000 (53%)] Loss: 17038.855469\n",
      "Train Epoch: 311 [122496/225000 (54%)] Loss: 17318.726562\n",
      "Train Epoch: 311 [124992/225000 (56%)] Loss: 16774.750000\n",
      "Train Epoch: 311 [127488/225000 (57%)] Loss: 17091.878906\n",
      "Train Epoch: 311 [129984/225000 (58%)] Loss: 17240.185547\n",
      "Train Epoch: 311 [132480/225000 (59%)] Loss: 17452.265625\n",
      "Train Epoch: 311 [134976/225000 (60%)] Loss: 17518.363281\n",
      "Train Epoch: 311 [137472/225000 (61%)] Loss: 17012.228516\n",
      "Train Epoch: 311 [139968/225000 (62%)] Loss: 17178.089844\n",
      "Train Epoch: 311 [142464/225000 (63%)] Loss: 17250.687500\n",
      "Train Epoch: 311 [144960/225000 (64%)] Loss: 19165.480469\n",
      "Train Epoch: 311 [147456/225000 (66%)] Loss: 16755.792969\n",
      "Train Epoch: 311 [149952/225000 (67%)] Loss: 17174.539062\n",
      "Train Epoch: 311 [152448/225000 (68%)] Loss: 16853.287109\n",
      "Train Epoch: 311 [154944/225000 (69%)] Loss: 17210.167969\n",
      "Train Epoch: 311 [157440/225000 (70%)] Loss: 16758.734375\n",
      "Train Epoch: 311 [159936/225000 (71%)] Loss: 17141.773438\n",
      "Train Epoch: 311 [162432/225000 (72%)] Loss: 17532.269531\n",
      "Train Epoch: 311 [164928/225000 (73%)] Loss: 17543.308594\n",
      "Train Epoch: 311 [167424/225000 (74%)] Loss: 17159.046875\n",
      "Train Epoch: 311 [169920/225000 (76%)] Loss: 17237.173828\n",
      "Train Epoch: 311 [172416/225000 (77%)] Loss: 16934.632812\n",
      "Train Epoch: 311 [174912/225000 (78%)] Loss: 17249.349609\n",
      "Train Epoch: 311 [177408/225000 (79%)] Loss: 17011.925781\n",
      "Train Epoch: 311 [179904/225000 (80%)] Loss: 17178.453125\n",
      "Train Epoch: 311 [182400/225000 (81%)] Loss: 17170.531250\n",
      "Train Epoch: 311 [184896/225000 (82%)] Loss: 17228.558594\n",
      "Train Epoch: 311 [187392/225000 (83%)] Loss: 17368.042969\n",
      "Train Epoch: 311 [189888/225000 (84%)] Loss: 17908.292969\n",
      "Train Epoch: 311 [192384/225000 (86%)] Loss: 16786.035156\n",
      "Train Epoch: 311 [194880/225000 (87%)] Loss: 17132.457031\n",
      "Train Epoch: 311 [197376/225000 (88%)] Loss: 16772.265625\n",
      "Train Epoch: 311 [199872/225000 (89%)] Loss: 16779.652344\n",
      "Train Epoch: 311 [202368/225000 (90%)] Loss: 16683.976562\n",
      "Train Epoch: 311 [204864/225000 (91%)] Loss: 16950.705078\n",
      "Train Epoch: 311 [207360/225000 (92%)] Loss: 17091.388672\n",
      "Train Epoch: 311 [209856/225000 (93%)] Loss: 16552.835938\n",
      "Train Epoch: 311 [212352/225000 (94%)] Loss: 16990.111328\n",
      "Train Epoch: 311 [214848/225000 (95%)] Loss: 16984.718750\n",
      "Train Epoch: 311 [217344/225000 (97%)] Loss: 16914.742188\n",
      "Train Epoch: 311 [219840/225000 (98%)] Loss: 17577.277344\n",
      "Train Epoch: 311 [222336/225000 (99%)] Loss: 17444.888672\n",
      "Train Epoch: 311 [224832/225000 (100%)] Loss: 17594.292969\n",
      "    epoch          : 311\n",
      "    loss           : 17193.702521731015\n",
      "    val_loss       : 17122.35389490892\n",
      "Train Epoch: 312 [192/225000 (0%)] Loss: 17207.632812\n",
      "Train Epoch: 312 [2688/225000 (1%)] Loss: 17462.113281\n",
      "Train Epoch: 312 [5184/225000 (2%)] Loss: 16991.191406\n",
      "Train Epoch: 312 [7680/225000 (3%)] Loss: 17271.320312\n",
      "Train Epoch: 312 [10176/225000 (5%)] Loss: 16668.902344\n",
      "Train Epoch: 312 [12672/225000 (6%)] Loss: 16751.035156\n",
      "Train Epoch: 312 [15168/225000 (7%)] Loss: 17060.955078\n",
      "Train Epoch: 312 [17664/225000 (8%)] Loss: 17530.843750\n",
      "Train Epoch: 312 [20160/225000 (9%)] Loss: 17503.777344\n",
      "Train Epoch: 312 [22656/225000 (10%)] Loss: 17466.851562\n",
      "Train Epoch: 312 [25152/225000 (11%)] Loss: 17570.464844\n",
      "Train Epoch: 312 [27648/225000 (12%)] Loss: 17497.419922\n",
      "Train Epoch: 312 [30144/225000 (13%)] Loss: 17331.593750\n",
      "Train Epoch: 312 [32640/225000 (15%)] Loss: 16886.300781\n",
      "Train Epoch: 312 [35136/225000 (16%)] Loss: 17184.414062\n",
      "Train Epoch: 312 [37632/225000 (17%)] Loss: 17093.158203\n",
      "Train Epoch: 312 [40128/225000 (18%)] Loss: 17248.761719\n",
      "Train Epoch: 312 [42624/225000 (19%)] Loss: 17248.595703\n",
      "Train Epoch: 312 [45120/225000 (20%)] Loss: 17313.871094\n",
      "Train Epoch: 312 [47616/225000 (21%)] Loss: 16921.246094\n",
      "Train Epoch: 312 [50112/225000 (22%)] Loss: 16920.408203\n",
      "Train Epoch: 312 [52608/225000 (23%)] Loss: 17087.964844\n",
      "Train Epoch: 312 [55104/225000 (24%)] Loss: 17295.089844\n",
      "Train Epoch: 312 [57600/225000 (26%)] Loss: 17203.671875\n",
      "Train Epoch: 312 [60096/225000 (27%)] Loss: 16985.957031\n",
      "Train Epoch: 312 [62592/225000 (28%)] Loss: 17298.697266\n",
      "Train Epoch: 312 [65088/225000 (29%)] Loss: 17172.050781\n",
      "Train Epoch: 312 [67584/225000 (30%)] Loss: 17199.757812\n",
      "Train Epoch: 312 [70080/225000 (31%)] Loss: 17262.679688\n",
      "Train Epoch: 312 [72576/225000 (32%)] Loss: 17247.613281\n",
      "Train Epoch: 312 [75072/225000 (33%)] Loss: 17441.113281\n",
      "Train Epoch: 312 [77568/225000 (34%)] Loss: 17117.605469\n",
      "Train Epoch: 312 [80064/225000 (36%)] Loss: 17065.675781\n",
      "Train Epoch: 312 [82560/225000 (37%)] Loss: 17306.300781\n",
      "Train Epoch: 312 [85056/225000 (38%)] Loss: 17371.628906\n",
      "Train Epoch: 312 [87552/225000 (39%)] Loss: 17116.947266\n",
      "Train Epoch: 312 [90048/225000 (40%)] Loss: 16936.332031\n",
      "Train Epoch: 312 [92544/225000 (41%)] Loss: 17144.183594\n",
      "Train Epoch: 312 [95040/225000 (42%)] Loss: 17243.621094\n",
      "Train Epoch: 312 [97536/225000 (43%)] Loss: 17173.292969\n",
      "Train Epoch: 312 [100032/225000 (44%)] Loss: 16987.800781\n",
      "Train Epoch: 312 [102528/225000 (46%)] Loss: 16959.925781\n",
      "Train Epoch: 312 [105024/225000 (47%)] Loss: 17299.558594\n",
      "Train Epoch: 312 [107520/225000 (48%)] Loss: 17478.550781\n",
      "Train Epoch: 312 [110016/225000 (49%)] Loss: 16989.875000\n",
      "Train Epoch: 312 [112512/225000 (50%)] Loss: 17259.750000\n",
      "Train Epoch: 312 [115008/225000 (51%)] Loss: 17099.453125\n",
      "Train Epoch: 312 [117504/225000 (52%)] Loss: 17318.347656\n",
      "Train Epoch: 312 [120000/225000 (53%)] Loss: 17113.593750\n",
      "Train Epoch: 312 [122496/225000 (54%)] Loss: 16956.027344\n",
      "Train Epoch: 312 [124992/225000 (56%)] Loss: 17005.425781\n",
      "Train Epoch: 312 [127488/225000 (57%)] Loss: 17025.019531\n",
      "Train Epoch: 312 [129984/225000 (58%)] Loss: 16936.542969\n",
      "Train Epoch: 312 [132480/225000 (59%)] Loss: 16903.718750\n",
      "Train Epoch: 312 [134976/225000 (60%)] Loss: 17236.652344\n",
      "Train Epoch: 312 [137472/225000 (61%)] Loss: 17123.667969\n",
      "Train Epoch: 312 [139968/225000 (62%)] Loss: 17292.636719\n",
      "Train Epoch: 312 [142464/225000 (63%)] Loss: 17581.378906\n",
      "Train Epoch: 312 [144960/225000 (64%)] Loss: 17238.332031\n",
      "Train Epoch: 312 [147456/225000 (66%)] Loss: 17386.517578\n",
      "Train Epoch: 312 [149952/225000 (67%)] Loss: 17115.550781\n",
      "Train Epoch: 312 [152448/225000 (68%)] Loss: 16892.019531\n",
      "Train Epoch: 312 [154944/225000 (69%)] Loss: 17392.398438\n",
      "Train Epoch: 312 [157440/225000 (70%)] Loss: 16982.203125\n",
      "Train Epoch: 312 [159936/225000 (71%)] Loss: 17157.414062\n",
      "Train Epoch: 312 [162432/225000 (72%)] Loss: 17328.253906\n",
      "Train Epoch: 312 [164928/225000 (73%)] Loss: 16788.257812\n",
      "Train Epoch: 312 [167424/225000 (74%)] Loss: 18955.232422\n",
      "Train Epoch: 312 [169920/225000 (76%)] Loss: 17014.732422\n",
      "Train Epoch: 312 [172416/225000 (77%)] Loss: 17435.984375\n",
      "Train Epoch: 312 [174912/225000 (78%)] Loss: 17468.812500\n",
      "Train Epoch: 312 [177408/225000 (79%)] Loss: 17256.710938\n",
      "Train Epoch: 312 [179904/225000 (80%)] Loss: 17082.058594\n",
      "Train Epoch: 312 [182400/225000 (81%)] Loss: 17112.914062\n",
      "Train Epoch: 312 [184896/225000 (82%)] Loss: 17219.480469\n",
      "Train Epoch: 312 [187392/225000 (83%)] Loss: 17212.855469\n",
      "Train Epoch: 312 [189888/225000 (84%)] Loss: 17072.300781\n",
      "Train Epoch: 312 [192384/225000 (86%)] Loss: 17135.144531\n",
      "Train Epoch: 312 [194880/225000 (87%)] Loss: 17034.798828\n",
      "Train Epoch: 312 [197376/225000 (88%)] Loss: 16824.730469\n",
      "Train Epoch: 312 [199872/225000 (89%)] Loss: 17342.871094\n",
      "Train Epoch: 312 [202368/225000 (90%)] Loss: 17380.191406\n",
      "Train Epoch: 312 [204864/225000 (91%)] Loss: 16740.714844\n",
      "Train Epoch: 312 [207360/225000 (92%)] Loss: 16768.367188\n",
      "Train Epoch: 312 [209856/225000 (93%)] Loss: 16688.179688\n",
      "Train Epoch: 312 [212352/225000 (94%)] Loss: 17436.648438\n",
      "Train Epoch: 312 [214848/225000 (95%)] Loss: 17113.343750\n",
      "Train Epoch: 312 [217344/225000 (97%)] Loss: 16644.250000\n",
      "Train Epoch: 312 [219840/225000 (98%)] Loss: 17114.285156\n",
      "Train Epoch: 312 [222336/225000 (99%)] Loss: 16938.636719\n",
      "Train Epoch: 312 [224832/225000 (100%)] Loss: 17049.242188\n",
      "    epoch          : 312\n",
      "    loss           : 17172.826436846735\n",
      "    val_loss       : 17151.11422575885\n",
      "Train Epoch: 313 [192/225000 (0%)] Loss: 17288.939453\n",
      "Train Epoch: 313 [2688/225000 (1%)] Loss: 17022.750000\n",
      "Train Epoch: 313 [5184/225000 (2%)] Loss: 16823.044922\n",
      "Train Epoch: 313 [7680/225000 (3%)] Loss: 17339.330078\n",
      "Train Epoch: 313 [10176/225000 (5%)] Loss: 17050.441406\n",
      "Train Epoch: 313 [12672/225000 (6%)] Loss: 17280.605469\n",
      "Train Epoch: 313 [15168/225000 (7%)] Loss: 17382.445312\n",
      "Train Epoch: 313 [17664/225000 (8%)] Loss: 17606.367188\n",
      "Train Epoch: 313 [20160/225000 (9%)] Loss: 17248.289062\n",
      "Train Epoch: 313 [22656/225000 (10%)] Loss: 17120.701172\n",
      "Train Epoch: 313 [25152/225000 (11%)] Loss: 17399.070312\n",
      "Train Epoch: 313 [27648/225000 (12%)] Loss: 17346.470703\n",
      "Train Epoch: 313 [30144/225000 (13%)] Loss: 17204.054688\n",
      "Train Epoch: 313 [32640/225000 (15%)] Loss: 16881.476562\n",
      "Train Epoch: 313 [35136/225000 (16%)] Loss: 16939.009766\n",
      "Train Epoch: 313 [37632/225000 (17%)] Loss: 17327.285156\n",
      "Train Epoch: 313 [40128/225000 (18%)] Loss: 17307.281250\n",
      "Train Epoch: 313 [42624/225000 (19%)] Loss: 17176.226562\n",
      "Train Epoch: 313 [45120/225000 (20%)] Loss: 17282.978516\n",
      "Train Epoch: 313 [47616/225000 (21%)] Loss: 16996.199219\n",
      "Train Epoch: 313 [50112/225000 (22%)] Loss: 17546.316406\n",
      "Train Epoch: 313 [52608/225000 (23%)] Loss: 16837.449219\n",
      "Train Epoch: 313 [55104/225000 (24%)] Loss: 16605.285156\n",
      "Train Epoch: 313 [57600/225000 (26%)] Loss: 16993.033203\n",
      "Train Epoch: 313 [60096/225000 (27%)] Loss: 17053.003906\n",
      "Train Epoch: 313 [62592/225000 (28%)] Loss: 17268.414062\n",
      "Train Epoch: 313 [65088/225000 (29%)] Loss: 16774.890625\n",
      "Train Epoch: 313 [67584/225000 (30%)] Loss: 16966.023438\n",
      "Train Epoch: 313 [70080/225000 (31%)] Loss: 17132.664062\n",
      "Train Epoch: 313 [72576/225000 (32%)] Loss: 16674.541016\n",
      "Train Epoch: 313 [75072/225000 (33%)] Loss: 16887.398438\n",
      "Train Epoch: 313 [77568/225000 (34%)] Loss: 17643.605469\n",
      "Train Epoch: 313 [80064/225000 (36%)] Loss: 17251.837891\n",
      "Train Epoch: 313 [82560/225000 (37%)] Loss: 17404.771484\n",
      "Train Epoch: 313 [85056/225000 (38%)] Loss: 17037.699219\n",
      "Train Epoch: 313 [87552/225000 (39%)] Loss: 17009.982422\n",
      "Train Epoch: 313 [90048/225000 (40%)] Loss: 17180.578125\n",
      "Train Epoch: 313 [92544/225000 (41%)] Loss: 17044.968750\n",
      "Train Epoch: 313 [95040/225000 (42%)] Loss: 17116.019531\n",
      "Train Epoch: 313 [97536/225000 (43%)] Loss: 16907.138672\n",
      "Train Epoch: 313 [100032/225000 (44%)] Loss: 16779.183594\n",
      "Train Epoch: 313 [102528/225000 (46%)] Loss: 17374.847656\n",
      "Train Epoch: 313 [105024/225000 (47%)] Loss: 17128.753906\n",
      "Train Epoch: 313 [107520/225000 (48%)] Loss: 17163.251953\n",
      "Train Epoch: 313 [110016/225000 (49%)] Loss: 17428.246094\n",
      "Train Epoch: 313 [112512/225000 (50%)] Loss: 17341.093750\n",
      "Train Epoch: 313 [115008/225000 (51%)] Loss: 16776.527344\n",
      "Train Epoch: 313 [117504/225000 (52%)] Loss: 17322.398438\n",
      "Train Epoch: 313 [120000/225000 (53%)] Loss: 17084.445312\n",
      "Train Epoch: 313 [122496/225000 (54%)] Loss: 16736.640625\n",
      "Train Epoch: 313 [124992/225000 (56%)] Loss: 17188.230469\n",
      "Train Epoch: 313 [127488/225000 (57%)] Loss: 17442.214844\n",
      "Train Epoch: 313 [129984/225000 (58%)] Loss: 17858.068359\n",
      "Train Epoch: 313 [132480/225000 (59%)] Loss: 16901.271484\n",
      "Train Epoch: 313 [134976/225000 (60%)] Loss: 17263.980469\n",
      "Train Epoch: 313 [137472/225000 (61%)] Loss: 17323.949219\n",
      "Train Epoch: 313 [139968/225000 (62%)] Loss: 17337.218750\n",
      "Train Epoch: 313 [142464/225000 (63%)] Loss: 16656.757812\n",
      "Train Epoch: 313 [144960/225000 (64%)] Loss: 17244.859375\n",
      "Train Epoch: 313 [147456/225000 (66%)] Loss: 16959.472656\n",
      "Train Epoch: 313 [149952/225000 (67%)] Loss: 17557.468750\n",
      "Train Epoch: 313 [152448/225000 (68%)] Loss: 17179.042969\n",
      "Train Epoch: 313 [154944/225000 (69%)] Loss: 16776.656250\n",
      "Train Epoch: 313 [157440/225000 (70%)] Loss: 17238.375000\n",
      "Train Epoch: 313 [159936/225000 (71%)] Loss: 17670.777344\n",
      "Train Epoch: 313 [162432/225000 (72%)] Loss: 17267.617188\n",
      "Train Epoch: 313 [164928/225000 (73%)] Loss: 16802.628906\n",
      "Train Epoch: 313 [167424/225000 (74%)] Loss: 16780.568359\n",
      "Train Epoch: 313 [169920/225000 (76%)] Loss: 17331.634766\n",
      "Train Epoch: 313 [172416/225000 (77%)] Loss: 17055.261719\n",
      "Train Epoch: 313 [174912/225000 (78%)] Loss: 16590.919922\n",
      "Train Epoch: 313 [177408/225000 (79%)] Loss: 17422.003906\n",
      "Train Epoch: 313 [179904/225000 (80%)] Loss: 16760.904297\n",
      "Train Epoch: 313 [182400/225000 (81%)] Loss: 17234.128906\n",
      "Train Epoch: 313 [184896/225000 (82%)] Loss: 17422.156250\n",
      "Train Epoch: 313 [187392/225000 (83%)] Loss: 18874.453125\n",
      "Train Epoch: 313 [189888/225000 (84%)] Loss: 17434.484375\n",
      "Train Epoch: 313 [192384/225000 (86%)] Loss: 16738.855469\n",
      "Train Epoch: 313 [194880/225000 (87%)] Loss: 19287.431641\n",
      "Train Epoch: 313 [197376/225000 (88%)] Loss: 17646.933594\n",
      "Train Epoch: 313 [199872/225000 (89%)] Loss: 16781.771484\n",
      "Train Epoch: 313 [202368/225000 (90%)] Loss: 17302.808594\n",
      "Train Epoch: 313 [204864/225000 (91%)] Loss: 17450.646484\n",
      "Train Epoch: 313 [207360/225000 (92%)] Loss: 16837.875000\n",
      "Train Epoch: 313 [209856/225000 (93%)] Loss: 17348.972656\n",
      "Train Epoch: 313 [212352/225000 (94%)] Loss: 17429.900391\n",
      "Train Epoch: 313 [214848/225000 (95%)] Loss: 17177.300781\n",
      "Train Epoch: 313 [217344/225000 (97%)] Loss: 17222.218750\n",
      "Train Epoch: 313 [219840/225000 (98%)] Loss: 17157.277344\n",
      "Train Epoch: 313 [222336/225000 (99%)] Loss: 17121.638672\n",
      "Train Epoch: 313 [224832/225000 (100%)] Loss: 17523.640625\n",
      "    epoch          : 313\n",
      "    loss           : 17162.30790132386\n",
      "    val_loss       : 17163.464464753637\n",
      "Train Epoch: 314 [192/225000 (0%)] Loss: 17466.902344\n",
      "Train Epoch: 314 [2688/225000 (1%)] Loss: 16925.933594\n",
      "Train Epoch: 314 [5184/225000 (2%)] Loss: 17226.632812\n",
      "Train Epoch: 314 [7680/225000 (3%)] Loss: 17072.187500\n",
      "Train Epoch: 314 [10176/225000 (5%)] Loss: 17219.265625\n",
      "Train Epoch: 314 [12672/225000 (6%)] Loss: 17127.277344\n",
      "Train Epoch: 314 [15168/225000 (7%)] Loss: 17219.427734\n",
      "Train Epoch: 314 [17664/225000 (8%)] Loss: 17030.365234\n",
      "Train Epoch: 314 [20160/225000 (9%)] Loss: 17173.507812\n",
      "Train Epoch: 314 [22656/225000 (10%)] Loss: 17623.371094\n",
      "Train Epoch: 314 [25152/225000 (11%)] Loss: 17324.619141\n",
      "Train Epoch: 314 [27648/225000 (12%)] Loss: 16865.890625\n",
      "Train Epoch: 314 [30144/225000 (13%)] Loss: 17127.449219\n",
      "Train Epoch: 314 [32640/225000 (15%)] Loss: 17093.896484\n",
      "Train Epoch: 314 [35136/225000 (16%)] Loss: 17263.767578\n",
      "Train Epoch: 314 [37632/225000 (17%)] Loss: 17163.916016\n",
      "Train Epoch: 314 [40128/225000 (18%)] Loss: 17352.503906\n",
      "Train Epoch: 314 [42624/225000 (19%)] Loss: 17042.039062\n",
      "Train Epoch: 314 [45120/225000 (20%)] Loss: 17832.230469\n",
      "Train Epoch: 314 [47616/225000 (21%)] Loss: 17212.527344\n",
      "Train Epoch: 314 [50112/225000 (22%)] Loss: 17455.457031\n",
      "Train Epoch: 314 [52608/225000 (23%)] Loss: 16897.250000\n",
      "Train Epoch: 314 [55104/225000 (24%)] Loss: 17085.597656\n",
      "Train Epoch: 314 [57600/225000 (26%)] Loss: 17565.296875\n",
      "Train Epoch: 314 [60096/225000 (27%)] Loss: 17022.894531\n",
      "Train Epoch: 314 [62592/225000 (28%)] Loss: 17349.134766\n",
      "Train Epoch: 314 [65088/225000 (29%)] Loss: 16962.201172\n",
      "Train Epoch: 314 [67584/225000 (30%)] Loss: 17377.767578\n",
      "Train Epoch: 314 [70080/225000 (31%)] Loss: 16897.000000\n",
      "Train Epoch: 314 [72576/225000 (32%)] Loss: 16888.150391\n",
      "Train Epoch: 314 [75072/225000 (33%)] Loss: 17424.326172\n",
      "Train Epoch: 314 [77568/225000 (34%)] Loss: 17449.826172\n",
      "Train Epoch: 314 [80064/225000 (36%)] Loss: 16833.632812\n",
      "Train Epoch: 314 [82560/225000 (37%)] Loss: 17145.572266\n",
      "Train Epoch: 314 [85056/225000 (38%)] Loss: 16952.953125\n",
      "Train Epoch: 314 [87552/225000 (39%)] Loss: 17230.015625\n",
      "Train Epoch: 314 [90048/225000 (40%)] Loss: 17278.976562\n",
      "Train Epoch: 314 [92544/225000 (41%)] Loss: 17322.488281\n",
      "Train Epoch: 314 [95040/225000 (42%)] Loss: 17347.027344\n",
      "Train Epoch: 314 [97536/225000 (43%)] Loss: 17076.039062\n",
      "Train Epoch: 314 [100032/225000 (44%)] Loss: 16591.732422\n",
      "Train Epoch: 314 [102528/225000 (46%)] Loss: 17488.744141\n",
      "Train Epoch: 314 [105024/225000 (47%)] Loss: 17112.625000\n",
      "Train Epoch: 314 [107520/225000 (48%)] Loss: 17313.195312\n",
      "Train Epoch: 314 [110016/225000 (49%)] Loss: 17231.044922\n",
      "Train Epoch: 314 [112512/225000 (50%)] Loss: 17184.914062\n",
      "Train Epoch: 314 [115008/225000 (51%)] Loss: 17377.523438\n",
      "Train Epoch: 314 [117504/225000 (52%)] Loss: 16719.828125\n",
      "Train Epoch: 314 [120000/225000 (53%)] Loss: 16976.812500\n",
      "Train Epoch: 314 [122496/225000 (54%)] Loss: 17283.500000\n",
      "Train Epoch: 314 [124992/225000 (56%)] Loss: 16818.882812\n",
      "Train Epoch: 314 [127488/225000 (57%)] Loss: 17345.777344\n",
      "Train Epoch: 314 [129984/225000 (58%)] Loss: 16810.822266\n",
      "Train Epoch: 314 [132480/225000 (59%)] Loss: 17089.195312\n",
      "Train Epoch: 314 [134976/225000 (60%)] Loss: 17108.574219\n",
      "Train Epoch: 314 [137472/225000 (61%)] Loss: 17430.363281\n",
      "Train Epoch: 314 [139968/225000 (62%)] Loss: 17460.740234\n",
      "Train Epoch: 314 [142464/225000 (63%)] Loss: 16671.820312\n",
      "Train Epoch: 314 [144960/225000 (64%)] Loss: 17094.121094\n",
      "Train Epoch: 314 [147456/225000 (66%)] Loss: 17183.890625\n",
      "Train Epoch: 314 [149952/225000 (67%)] Loss: 17205.031250\n",
      "Train Epoch: 314 [152448/225000 (68%)] Loss: 17829.136719\n",
      "Train Epoch: 314 [154944/225000 (69%)] Loss: 17068.662109\n",
      "Train Epoch: 314 [157440/225000 (70%)] Loss: 16737.396484\n",
      "Train Epoch: 314 [159936/225000 (71%)] Loss: 17108.757812\n",
      "Train Epoch: 314 [162432/225000 (72%)] Loss: 17059.263672\n",
      "Train Epoch: 314 [164928/225000 (73%)] Loss: 17306.107422\n",
      "Train Epoch: 314 [167424/225000 (74%)] Loss: 17019.722656\n",
      "Train Epoch: 314 [169920/225000 (76%)] Loss: 17327.392578\n",
      "Train Epoch: 314 [172416/225000 (77%)] Loss: 16901.527344\n",
      "Train Epoch: 314 [174912/225000 (78%)] Loss: 16554.066406\n",
      "Train Epoch: 314 [177408/225000 (79%)] Loss: 16974.136719\n",
      "Train Epoch: 314 [179904/225000 (80%)] Loss: 16930.759766\n",
      "Train Epoch: 314 [182400/225000 (81%)] Loss: 17259.769531\n",
      "Train Epoch: 314 [184896/225000 (82%)] Loss: 17333.230469\n",
      "Train Epoch: 314 [187392/225000 (83%)] Loss: 17197.386719\n",
      "Train Epoch: 314 [189888/225000 (84%)] Loss: 16793.820312\n",
      "Train Epoch: 314 [192384/225000 (86%)] Loss: 17260.855469\n",
      "Train Epoch: 314 [194880/225000 (87%)] Loss: 17048.199219\n",
      "Train Epoch: 314 [197376/225000 (88%)] Loss: 17300.828125\n",
      "Train Epoch: 314 [199872/225000 (89%)] Loss: 17519.214844\n",
      "Train Epoch: 314 [202368/225000 (90%)] Loss: 17521.796875\n",
      "Train Epoch: 314 [204864/225000 (91%)] Loss: 16924.191406\n",
      "Train Epoch: 314 [207360/225000 (92%)] Loss: 16651.107422\n",
      "Train Epoch: 314 [209856/225000 (93%)] Loss: 16761.750000\n",
      "Train Epoch: 314 [212352/225000 (94%)] Loss: 16977.896484\n",
      "Train Epoch: 314 [214848/225000 (95%)] Loss: 16861.634766\n",
      "Train Epoch: 314 [217344/225000 (97%)] Loss: 17033.906250\n",
      "Train Epoch: 314 [219840/225000 (98%)] Loss: 17083.859375\n",
      "Train Epoch: 314 [222336/225000 (99%)] Loss: 17054.460938\n",
      "Train Epoch: 314 [224832/225000 (100%)] Loss: 16996.585938\n",
      "    epoch          : 314\n",
      "    loss           : 17134.623328511625\n",
      "    val_loss       : 17071.143135403858\n",
      "Train Epoch: 315 [192/225000 (0%)] Loss: 17269.933594\n",
      "Train Epoch: 315 [2688/225000 (1%)] Loss: 17064.457031\n",
      "Train Epoch: 315 [5184/225000 (2%)] Loss: 17307.359375\n",
      "Train Epoch: 315 [7680/225000 (3%)] Loss: 16824.404297\n",
      "Train Epoch: 315 [10176/225000 (5%)] Loss: 17138.187500\n",
      "Train Epoch: 315 [12672/225000 (6%)] Loss: 16816.138672\n",
      "Train Epoch: 315 [15168/225000 (7%)] Loss: 17475.814453\n",
      "Train Epoch: 315 [17664/225000 (8%)] Loss: 16765.369141\n",
      "Train Epoch: 315 [20160/225000 (9%)] Loss: 17527.980469\n",
      "Train Epoch: 315 [22656/225000 (10%)] Loss: 17188.523438\n",
      "Train Epoch: 315 [25152/225000 (11%)] Loss: 17207.062500\n",
      "Train Epoch: 315 [27648/225000 (12%)] Loss: 17220.179688\n",
      "Train Epoch: 315 [30144/225000 (13%)] Loss: 17088.187500\n",
      "Train Epoch: 315 [32640/225000 (15%)] Loss: 16797.613281\n",
      "Train Epoch: 315 [35136/225000 (16%)] Loss: 16928.968750\n",
      "Train Epoch: 315 [37632/225000 (17%)] Loss: 17195.011719\n",
      "Train Epoch: 315 [40128/225000 (18%)] Loss: 17048.355469\n",
      "Train Epoch: 315 [42624/225000 (19%)] Loss: 17823.125000\n",
      "Train Epoch: 315 [45120/225000 (20%)] Loss: 16851.943359\n",
      "Train Epoch: 315 [47616/225000 (21%)] Loss: 16765.578125\n",
      "Train Epoch: 315 [50112/225000 (22%)] Loss: 17312.208984\n",
      "Train Epoch: 315 [52608/225000 (23%)] Loss: 16786.652344\n",
      "Train Epoch: 315 [55104/225000 (24%)] Loss: 17304.792969\n",
      "Train Epoch: 315 [57600/225000 (26%)] Loss: 16965.458984\n",
      "Train Epoch: 315 [60096/225000 (27%)] Loss: 17025.257812\n",
      "Train Epoch: 315 [62592/225000 (28%)] Loss: 16872.041016\n",
      "Train Epoch: 315 [65088/225000 (29%)] Loss: 17379.919922\n",
      "Train Epoch: 315 [67584/225000 (30%)] Loss: 17147.027344\n",
      "Train Epoch: 315 [70080/225000 (31%)] Loss: 17109.003906\n",
      "Train Epoch: 315 [72576/225000 (32%)] Loss: 17378.363281\n",
      "Train Epoch: 315 [75072/225000 (33%)] Loss: 17229.445312\n",
      "Train Epoch: 315 [77568/225000 (34%)] Loss: 17142.625000\n",
      "Train Epoch: 315 [80064/225000 (36%)] Loss: 16831.660156\n",
      "Train Epoch: 315 [82560/225000 (37%)] Loss: 16758.496094\n",
      "Train Epoch: 315 [85056/225000 (38%)] Loss: 17211.632812\n",
      "Train Epoch: 315 [87552/225000 (39%)] Loss: 17124.414062\n",
      "Train Epoch: 315 [90048/225000 (40%)] Loss: 17403.000000\n",
      "Train Epoch: 315 [92544/225000 (41%)] Loss: 16716.101562\n",
      "Train Epoch: 315 [95040/225000 (42%)] Loss: 16867.544922\n",
      "Train Epoch: 315 [97536/225000 (43%)] Loss: 17224.865234\n",
      "Train Epoch: 315 [100032/225000 (44%)] Loss: 17024.187500\n",
      "Train Epoch: 315 [102528/225000 (46%)] Loss: 17337.652344\n",
      "Train Epoch: 315 [105024/225000 (47%)] Loss: 16961.654297\n",
      "Train Epoch: 315 [107520/225000 (48%)] Loss: 17637.167969\n",
      "Train Epoch: 315 [110016/225000 (49%)] Loss: 16982.050781\n",
      "Train Epoch: 315 [112512/225000 (50%)] Loss: 17209.951172\n",
      "Train Epoch: 315 [115008/225000 (51%)] Loss: 16714.980469\n",
      "Train Epoch: 315 [117504/225000 (52%)] Loss: 16663.388672\n",
      "Train Epoch: 315 [120000/225000 (53%)] Loss: 16765.669922\n",
      "Train Epoch: 315 [122496/225000 (54%)] Loss: 18654.750000\n",
      "Train Epoch: 315 [124992/225000 (56%)] Loss: 17413.125000\n",
      "Train Epoch: 315 [127488/225000 (57%)] Loss: 17434.312500\n",
      "Train Epoch: 315 [129984/225000 (58%)] Loss: 19189.642578\n",
      "Train Epoch: 315 [132480/225000 (59%)] Loss: 16785.486328\n",
      "Train Epoch: 315 [134976/225000 (60%)] Loss: 17480.369141\n",
      "Train Epoch: 315 [137472/225000 (61%)] Loss: 17272.640625\n",
      "Train Epoch: 315 [139968/225000 (62%)] Loss: 16641.666016\n",
      "Train Epoch: 315 [142464/225000 (63%)] Loss: 17352.210938\n",
      "Train Epoch: 315 [144960/225000 (64%)] Loss: 17050.619141\n",
      "Train Epoch: 315 [147456/225000 (66%)] Loss: 16958.085938\n",
      "Train Epoch: 315 [149952/225000 (67%)] Loss: 16814.742188\n",
      "Train Epoch: 315 [152448/225000 (68%)] Loss: 16803.906250\n",
      "Train Epoch: 315 [154944/225000 (69%)] Loss: 16853.515625\n",
      "Train Epoch: 315 [157440/225000 (70%)] Loss: 16614.210938\n",
      "Train Epoch: 315 [159936/225000 (71%)] Loss: 17579.361328\n",
      "Train Epoch: 315 [162432/225000 (72%)] Loss: 16854.398438\n",
      "Train Epoch: 315 [164928/225000 (73%)] Loss: 16845.042969\n",
      "Train Epoch: 315 [167424/225000 (74%)] Loss: 16570.300781\n",
      "Train Epoch: 315 [169920/225000 (76%)] Loss: 17253.640625\n",
      "Train Epoch: 315 [172416/225000 (77%)] Loss: 16589.576172\n",
      "Train Epoch: 315 [174912/225000 (78%)] Loss: 17237.330078\n",
      "Train Epoch: 315 [177408/225000 (79%)] Loss: 17144.199219\n",
      "Train Epoch: 315 [179904/225000 (80%)] Loss: 16989.738281\n",
      "Train Epoch: 315 [182400/225000 (81%)] Loss: 16754.871094\n",
      "Train Epoch: 315 [184896/225000 (82%)] Loss: 16894.181641\n",
      "Train Epoch: 315 [187392/225000 (83%)] Loss: 17370.500000\n",
      "Train Epoch: 315 [189888/225000 (84%)] Loss: 16912.484375\n",
      "Train Epoch: 315 [192384/225000 (86%)] Loss: 16554.332031\n",
      "Train Epoch: 315 [194880/225000 (87%)] Loss: 16817.349609\n",
      "Train Epoch: 315 [197376/225000 (88%)] Loss: 16669.806641\n",
      "Train Epoch: 315 [199872/225000 (89%)] Loss: 17084.730469\n",
      "Train Epoch: 315 [202368/225000 (90%)] Loss: 17168.500000\n",
      "Train Epoch: 315 [204864/225000 (91%)] Loss: 17022.855469\n",
      "Train Epoch: 315 [207360/225000 (92%)] Loss: 17205.250000\n",
      "Train Epoch: 315 [209856/225000 (93%)] Loss: 17180.699219\n",
      "Train Epoch: 315 [212352/225000 (94%)] Loss: 17430.304688\n",
      "Train Epoch: 315 [214848/225000 (95%)] Loss: 17084.765625\n",
      "Train Epoch: 315 [217344/225000 (97%)] Loss: 17782.402344\n",
      "Train Epoch: 315 [219840/225000 (98%)] Loss: 17204.503906\n",
      "Train Epoch: 315 [222336/225000 (99%)] Loss: 17505.191406\n",
      "Train Epoch: 315 [224832/225000 (100%)] Loss: 17174.082031\n",
      "    epoch          : 315\n",
      "    loss           : 17143.371891998187\n",
      "    val_loss       : 17033.714859640324\n",
      "Train Epoch: 316 [192/225000 (0%)] Loss: 17460.378906\n",
      "Train Epoch: 316 [2688/225000 (1%)] Loss: 17673.242188\n",
      "Train Epoch: 316 [5184/225000 (2%)] Loss: 17112.582031\n",
      "Train Epoch: 316 [7680/225000 (3%)] Loss: 16970.058594\n",
      "Train Epoch: 316 [10176/225000 (5%)] Loss: 16885.847656\n",
      "Train Epoch: 316 [12672/225000 (6%)] Loss: 17250.480469\n",
      "Train Epoch: 316 [15168/225000 (7%)] Loss: 17008.292969\n",
      "Train Epoch: 316 [17664/225000 (8%)] Loss: 17050.111328\n",
      "Train Epoch: 316 [20160/225000 (9%)] Loss: 17008.658203\n",
      "Train Epoch: 316 [22656/225000 (10%)] Loss: 17243.457031\n",
      "Train Epoch: 316 [25152/225000 (11%)] Loss: 17161.501953\n",
      "Train Epoch: 316 [27648/225000 (12%)] Loss: 17057.222656\n",
      "Train Epoch: 316 [30144/225000 (13%)] Loss: 17459.453125\n",
      "Train Epoch: 316 [32640/225000 (15%)] Loss: 16708.460938\n",
      "Train Epoch: 316 [35136/225000 (16%)] Loss: 17131.218750\n",
      "Train Epoch: 316 [37632/225000 (17%)] Loss: 16788.525391\n",
      "Train Epoch: 316 [40128/225000 (18%)] Loss: 16936.785156\n",
      "Train Epoch: 316 [42624/225000 (19%)] Loss: 16774.406250\n",
      "Train Epoch: 316 [45120/225000 (20%)] Loss: 17400.998047\n",
      "Train Epoch: 316 [47616/225000 (21%)] Loss: 17092.232422\n",
      "Train Epoch: 316 [50112/225000 (22%)] Loss: 16770.687500\n",
      "Train Epoch: 316 [52608/225000 (23%)] Loss: 16872.234375\n",
      "Train Epoch: 316 [55104/225000 (24%)] Loss: 16674.994141\n",
      "Train Epoch: 316 [57600/225000 (26%)] Loss: 17087.187500\n",
      "Train Epoch: 316 [60096/225000 (27%)] Loss: 16997.968750\n",
      "Train Epoch: 316 [62592/225000 (28%)] Loss: 17299.980469\n",
      "Train Epoch: 316 [65088/225000 (29%)] Loss: 17209.269531\n",
      "Train Epoch: 316 [67584/225000 (30%)] Loss: 16883.480469\n",
      "Train Epoch: 316 [70080/225000 (31%)] Loss: 17028.070312\n",
      "Train Epoch: 316 [72576/225000 (32%)] Loss: 17227.884766\n",
      "Train Epoch: 316 [75072/225000 (33%)] Loss: 17068.640625\n",
      "Train Epoch: 316 [77568/225000 (34%)] Loss: 17280.986328\n",
      "Train Epoch: 316 [80064/225000 (36%)] Loss: 17158.386719\n",
      "Train Epoch: 316 [82560/225000 (37%)] Loss: 17130.855469\n",
      "Train Epoch: 316 [85056/225000 (38%)] Loss: 17078.359375\n",
      "Train Epoch: 316 [87552/225000 (39%)] Loss: 16998.636719\n",
      "Train Epoch: 316 [90048/225000 (40%)] Loss: 17497.542969\n",
      "Train Epoch: 316 [92544/225000 (41%)] Loss: 17371.564453\n",
      "Train Epoch: 316 [95040/225000 (42%)] Loss: 17022.419922\n",
      "Train Epoch: 316 [97536/225000 (43%)] Loss: 16981.351562\n",
      "Train Epoch: 316 [100032/225000 (44%)] Loss: 17239.869141\n",
      "Train Epoch: 316 [102528/225000 (46%)] Loss: 17202.886719\n",
      "Train Epoch: 316 [105024/225000 (47%)] Loss: 17239.345703\n",
      "Train Epoch: 316 [107520/225000 (48%)] Loss: 17235.515625\n",
      "Train Epoch: 316 [110016/225000 (49%)] Loss: 17259.660156\n",
      "Train Epoch: 316 [112512/225000 (50%)] Loss: 17171.507812\n",
      "Train Epoch: 316 [115008/225000 (51%)] Loss: 17379.623047\n",
      "Train Epoch: 316 [117504/225000 (52%)] Loss: 17088.300781\n",
      "Train Epoch: 316 [120000/225000 (53%)] Loss: 16977.712891\n",
      "Train Epoch: 316 [122496/225000 (54%)] Loss: 17619.203125\n",
      "Train Epoch: 316 [124992/225000 (56%)] Loss: 17045.136719\n",
      "Train Epoch: 316 [127488/225000 (57%)] Loss: 17005.628906\n",
      "Train Epoch: 316 [129984/225000 (58%)] Loss: 16995.468750\n",
      "Train Epoch: 316 [132480/225000 (59%)] Loss: 16716.597656\n",
      "Train Epoch: 316 [134976/225000 (60%)] Loss: 16893.011719\n",
      "Train Epoch: 316 [137472/225000 (61%)] Loss: 17019.410156\n",
      "Train Epoch: 316 [139968/225000 (62%)] Loss: 17021.972656\n",
      "Train Epoch: 316 [142464/225000 (63%)] Loss: 17116.822266\n",
      "Train Epoch: 316 [144960/225000 (64%)] Loss: 17509.250000\n",
      "Train Epoch: 316 [147456/225000 (66%)] Loss: 17114.304688\n",
      "Train Epoch: 316 [149952/225000 (67%)] Loss: 17116.875000\n",
      "Train Epoch: 316 [152448/225000 (68%)] Loss: 16867.199219\n",
      "Train Epoch: 316 [154944/225000 (69%)] Loss: 16791.058594\n",
      "Train Epoch: 316 [157440/225000 (70%)] Loss: 17227.148438\n",
      "Train Epoch: 316 [159936/225000 (71%)] Loss: 17140.812500\n",
      "Train Epoch: 316 [162432/225000 (72%)] Loss: 16688.648438\n",
      "Train Epoch: 316 [164928/225000 (73%)] Loss: 17121.101562\n",
      "Train Epoch: 316 [167424/225000 (74%)] Loss: 17235.320312\n",
      "Train Epoch: 316 [169920/225000 (76%)] Loss: 17197.761719\n",
      "Train Epoch: 316 [172416/225000 (77%)] Loss: 17132.451172\n",
      "Train Epoch: 316 [174912/225000 (78%)] Loss: 16558.201172\n",
      "Train Epoch: 316 [177408/225000 (79%)] Loss: 17120.546875\n",
      "Train Epoch: 316 [179904/225000 (80%)] Loss: 16714.210938\n",
      "Train Epoch: 316 [182400/225000 (81%)] Loss: 16778.439453\n",
      "Train Epoch: 316 [184896/225000 (82%)] Loss: 16942.357422\n",
      "Train Epoch: 316 [187392/225000 (83%)] Loss: 17282.427734\n",
      "Train Epoch: 316 [189888/225000 (84%)] Loss: 16506.716797\n",
      "Train Epoch: 316 [192384/225000 (86%)] Loss: 17033.355469\n",
      "Train Epoch: 316 [194880/225000 (87%)] Loss: 17345.017578\n",
      "Train Epoch: 316 [197376/225000 (88%)] Loss: 16626.640625\n",
      "Train Epoch: 316 [199872/225000 (89%)] Loss: 16896.925781\n",
      "Train Epoch: 316 [202368/225000 (90%)] Loss: 17104.373047\n",
      "Train Epoch: 316 [204864/225000 (91%)] Loss: 16967.292969\n",
      "Train Epoch: 316 [207360/225000 (92%)] Loss: 17540.500000\n",
      "Train Epoch: 316 [209856/225000 (93%)] Loss: 17307.003906\n",
      "Train Epoch: 316 [212352/225000 (94%)] Loss: 17311.054688\n",
      "Train Epoch: 316 [214848/225000 (95%)] Loss: 16957.371094\n",
      "Train Epoch: 316 [217344/225000 (97%)] Loss: 16997.035156\n",
      "Train Epoch: 316 [219840/225000 (98%)] Loss: 17722.763672\n",
      "Train Epoch: 316 [222336/225000 (99%)] Loss: 17329.187500\n",
      "Train Epoch: 316 [224832/225000 (100%)] Loss: 17296.255859\n",
      "    epoch          : 316\n",
      "    loss           : 17163.35096589697\n",
      "    val_loss       : 17054.47729366836\n",
      "Train Epoch: 317 [192/225000 (0%)] Loss: 16745.445312\n",
      "Train Epoch: 317 [2688/225000 (1%)] Loss: 16936.078125\n",
      "Train Epoch: 317 [5184/225000 (2%)] Loss: 17253.574219\n",
      "Train Epoch: 317 [7680/225000 (3%)] Loss: 17135.111328\n",
      "Train Epoch: 317 [10176/225000 (5%)] Loss: 17007.908203\n",
      "Train Epoch: 317 [12672/225000 (6%)] Loss: 17492.027344\n",
      "Train Epoch: 317 [15168/225000 (7%)] Loss: 16662.539062\n",
      "Train Epoch: 317 [17664/225000 (8%)] Loss: 17295.162109\n",
      "Train Epoch: 317 [20160/225000 (9%)] Loss: 16967.189453\n",
      "Train Epoch: 317 [22656/225000 (10%)] Loss: 17123.517578\n",
      "Train Epoch: 317 [25152/225000 (11%)] Loss: 16784.656250\n",
      "Train Epoch: 317 [27648/225000 (12%)] Loss: 17013.041016\n",
      "Train Epoch: 317 [30144/225000 (13%)] Loss: 17399.699219\n",
      "Train Epoch: 317 [32640/225000 (15%)] Loss: 16927.021484\n",
      "Train Epoch: 317 [35136/225000 (16%)] Loss: 16728.941406\n",
      "Train Epoch: 317 [37632/225000 (17%)] Loss: 18731.429688\n",
      "Train Epoch: 317 [40128/225000 (18%)] Loss: 17195.964844\n",
      "Train Epoch: 317 [42624/225000 (19%)] Loss: 16964.750000\n",
      "Train Epoch: 317 [45120/225000 (20%)] Loss: 16833.914062\n",
      "Train Epoch: 317 [47616/225000 (21%)] Loss: 17355.748047\n",
      "Train Epoch: 317 [50112/225000 (22%)] Loss: 16638.800781\n",
      "Train Epoch: 317 [52608/225000 (23%)] Loss: 16615.796875\n",
      "Train Epoch: 317 [55104/225000 (24%)] Loss: 17185.593750\n",
      "Train Epoch: 317 [57600/225000 (26%)] Loss: 16825.464844\n",
      "Train Epoch: 317 [60096/225000 (27%)] Loss: 17186.667969\n",
      "Train Epoch: 317 [62592/225000 (28%)] Loss: 17072.025391\n",
      "Train Epoch: 317 [65088/225000 (29%)] Loss: 17297.314453\n",
      "Train Epoch: 317 [67584/225000 (30%)] Loss: 17369.958984\n",
      "Train Epoch: 317 [70080/225000 (31%)] Loss: 16484.820312\n",
      "Train Epoch: 317 [72576/225000 (32%)] Loss: 17324.949219\n",
      "Train Epoch: 317 [75072/225000 (33%)] Loss: 17111.384766\n",
      "Train Epoch: 317 [77568/225000 (34%)] Loss: 16949.195312\n",
      "Train Epoch: 317 [80064/225000 (36%)] Loss: 17835.742188\n",
      "Train Epoch: 317 [82560/225000 (37%)] Loss: 17106.457031\n",
      "Train Epoch: 317 [85056/225000 (38%)] Loss: 17173.587891\n",
      "Train Epoch: 317 [87552/225000 (39%)] Loss: 17365.476562\n",
      "Train Epoch: 317 [90048/225000 (40%)] Loss: 17463.798828\n",
      "Train Epoch: 317 [92544/225000 (41%)] Loss: 16943.757812\n",
      "Train Epoch: 317 [95040/225000 (42%)] Loss: 16837.904297\n",
      "Train Epoch: 317 [97536/225000 (43%)] Loss: 17072.316406\n",
      "Train Epoch: 317 [100032/225000 (44%)] Loss: 16320.937500\n",
      "Train Epoch: 317 [102528/225000 (46%)] Loss: 16938.320312\n",
      "Train Epoch: 317 [105024/225000 (47%)] Loss: 17526.822266\n",
      "Train Epoch: 317 [107520/225000 (48%)] Loss: 16960.816406\n",
      "Train Epoch: 317 [110016/225000 (49%)] Loss: 16957.794922\n",
      "Train Epoch: 317 [112512/225000 (50%)] Loss: 17093.546875\n",
      "Train Epoch: 317 [115008/225000 (51%)] Loss: 16934.820312\n",
      "Train Epoch: 317 [117504/225000 (52%)] Loss: 17130.351562\n",
      "Train Epoch: 317 [120000/225000 (53%)] Loss: 16657.810547\n",
      "Train Epoch: 317 [122496/225000 (54%)] Loss: 16972.591797\n",
      "Train Epoch: 317 [124992/225000 (56%)] Loss: 17426.707031\n",
      "Train Epoch: 317 [127488/225000 (57%)] Loss: 16505.234375\n",
      "Train Epoch: 317 [129984/225000 (58%)] Loss: 16966.410156\n",
      "Train Epoch: 317 [132480/225000 (59%)] Loss: 17430.949219\n",
      "Train Epoch: 317 [134976/225000 (60%)] Loss: 17433.023438\n",
      "Train Epoch: 317 [137472/225000 (61%)] Loss: 17061.820312\n",
      "Train Epoch: 317 [139968/225000 (62%)] Loss: 17023.777344\n",
      "Train Epoch: 317 [142464/225000 (63%)] Loss: 17363.425781\n",
      "Train Epoch: 317 [144960/225000 (64%)] Loss: 16844.132812\n",
      "Train Epoch: 317 [147456/225000 (66%)] Loss: 16967.521484\n",
      "Train Epoch: 317 [149952/225000 (67%)] Loss: 16553.640625\n",
      "Train Epoch: 317 [152448/225000 (68%)] Loss: 17169.375000\n",
      "Train Epoch: 317 [154944/225000 (69%)] Loss: 17546.417969\n",
      "Train Epoch: 317 [157440/225000 (70%)] Loss: 17475.732422\n",
      "Train Epoch: 317 [159936/225000 (71%)] Loss: 16694.140625\n",
      "Train Epoch: 317 [162432/225000 (72%)] Loss: 17313.105469\n",
      "Train Epoch: 317 [164928/225000 (73%)] Loss: 17247.617188\n",
      "Train Epoch: 317 [167424/225000 (74%)] Loss: 16974.128906\n",
      "Train Epoch: 317 [169920/225000 (76%)] Loss: 18960.322266\n",
      "Train Epoch: 317 [172416/225000 (77%)] Loss: 18543.390625\n",
      "Train Epoch: 317 [174912/225000 (78%)] Loss: 17087.261719\n",
      "Train Epoch: 317 [177408/225000 (79%)] Loss: 17533.320312\n",
      "Train Epoch: 317 [179904/225000 (80%)] Loss: 17030.355469\n",
      "Train Epoch: 317 [182400/225000 (81%)] Loss: 17106.736328\n",
      "Train Epoch: 317 [184896/225000 (82%)] Loss: 16691.406250\n",
      "Train Epoch: 317 [187392/225000 (83%)] Loss: 17385.023438\n",
      "Train Epoch: 317 [189888/225000 (84%)] Loss: 17239.347656\n",
      "Train Epoch: 317 [192384/225000 (86%)] Loss: 17134.333984\n",
      "Train Epoch: 317 [194880/225000 (87%)] Loss: 17571.419922\n",
      "Train Epoch: 317 [197376/225000 (88%)] Loss: 17105.515625\n",
      "Train Epoch: 317 [199872/225000 (89%)] Loss: 16918.675781\n",
      "Train Epoch: 317 [202368/225000 (90%)] Loss: 17053.878906\n",
      "Train Epoch: 317 [204864/225000 (91%)] Loss: 17336.191406\n",
      "Train Epoch: 317 [207360/225000 (92%)] Loss: 17320.392578\n",
      "Train Epoch: 317 [209856/225000 (93%)] Loss: 17043.488281\n",
      "Train Epoch: 317 [212352/225000 (94%)] Loss: 17079.931641\n",
      "Train Epoch: 317 [214848/225000 (95%)] Loss: 17329.136719\n",
      "Train Epoch: 317 [217344/225000 (97%)] Loss: 16921.113281\n",
      "Train Epoch: 317 [219840/225000 (98%)] Loss: 16929.753906\n",
      "Train Epoch: 317 [222336/225000 (99%)] Loss: 16903.287109\n",
      "Train Epoch: 317 [224832/225000 (100%)] Loss: 16826.824219\n",
      "    epoch          : 317\n",
      "    loss           : 17125.978233988375\n",
      "    val_loss       : 17100.052713204885\n",
      "Train Epoch: 318 [192/225000 (0%)] Loss: 17127.146484\n",
      "Train Epoch: 318 [2688/225000 (1%)] Loss: 16760.468750\n",
      "Train Epoch: 318 [5184/225000 (2%)] Loss: 17161.990234\n",
      "Train Epoch: 318 [7680/225000 (3%)] Loss: 17243.941406\n",
      "Train Epoch: 318 [10176/225000 (5%)] Loss: 16930.082031\n",
      "Train Epoch: 318 [12672/225000 (6%)] Loss: 17213.681641\n",
      "Train Epoch: 318 [15168/225000 (7%)] Loss: 17313.429688\n",
      "Train Epoch: 318 [17664/225000 (8%)] Loss: 16933.351562\n",
      "Train Epoch: 318 [20160/225000 (9%)] Loss: 17385.281250\n",
      "Train Epoch: 318 [22656/225000 (10%)] Loss: 17066.013672\n",
      "Train Epoch: 318 [25152/225000 (11%)] Loss: 16957.123047\n",
      "Train Epoch: 318 [27648/225000 (12%)] Loss: 16583.916016\n",
      "Train Epoch: 318 [30144/225000 (13%)] Loss: 17200.232422\n",
      "Train Epoch: 318 [32640/225000 (15%)] Loss: 16744.123047\n",
      "Train Epoch: 318 [35136/225000 (16%)] Loss: 16951.097656\n",
      "Train Epoch: 318 [37632/225000 (17%)] Loss: 16994.871094\n",
      "Train Epoch: 318 [40128/225000 (18%)] Loss: 16857.214844\n",
      "Train Epoch: 318 [42624/225000 (19%)] Loss: 17160.044922\n",
      "Train Epoch: 318 [45120/225000 (20%)] Loss: 17662.527344\n",
      "Train Epoch: 318 [47616/225000 (21%)] Loss: 17543.292969\n",
      "Train Epoch: 318 [50112/225000 (22%)] Loss: 18460.980469\n",
      "Train Epoch: 318 [52608/225000 (23%)] Loss: 17069.316406\n",
      "Train Epoch: 318 [55104/225000 (24%)] Loss: 16718.898438\n",
      "Train Epoch: 318 [57600/225000 (26%)] Loss: 17626.343750\n",
      "Train Epoch: 318 [60096/225000 (27%)] Loss: 17017.820312\n",
      "Train Epoch: 318 [62592/225000 (28%)] Loss: 18747.800781\n",
      "Train Epoch: 318 [65088/225000 (29%)] Loss: 17194.992188\n",
      "Train Epoch: 318 [67584/225000 (30%)] Loss: 16979.166016\n",
      "Train Epoch: 318 [70080/225000 (31%)] Loss: 17398.789062\n",
      "Train Epoch: 318 [72576/225000 (32%)] Loss: 17022.746094\n",
      "Train Epoch: 318 [75072/225000 (33%)] Loss: 16762.107422\n",
      "Train Epoch: 318 [77568/225000 (34%)] Loss: 16288.155273\n",
      "Train Epoch: 318 [80064/225000 (36%)] Loss: 17441.970703\n",
      "Train Epoch: 318 [82560/225000 (37%)] Loss: 17380.873047\n",
      "Train Epoch: 318 [85056/225000 (38%)] Loss: 17331.509766\n",
      "Train Epoch: 318 [87552/225000 (39%)] Loss: 17268.271484\n",
      "Train Epoch: 318 [90048/225000 (40%)] Loss: 17043.585938\n",
      "Train Epoch: 318 [92544/225000 (41%)] Loss: 16776.613281\n",
      "Train Epoch: 318 [95040/225000 (42%)] Loss: 16866.910156\n",
      "Train Epoch: 318 [97536/225000 (43%)] Loss: 17509.351562\n",
      "Train Epoch: 318 [100032/225000 (44%)] Loss: 17151.035156\n",
      "Train Epoch: 318 [102528/225000 (46%)] Loss: 17024.988281\n",
      "Train Epoch: 318 [105024/225000 (47%)] Loss: 17080.615234\n",
      "Train Epoch: 318 [107520/225000 (48%)] Loss: 16938.492188\n",
      "Train Epoch: 318 [110016/225000 (49%)] Loss: 17312.539062\n",
      "Train Epoch: 318 [112512/225000 (50%)] Loss: 17244.238281\n",
      "Train Epoch: 318 [115008/225000 (51%)] Loss: 17060.550781\n",
      "Train Epoch: 318 [117504/225000 (52%)] Loss: 16962.910156\n",
      "Train Epoch: 318 [120000/225000 (53%)] Loss: 17114.992188\n",
      "Train Epoch: 318 [122496/225000 (54%)] Loss: 17258.363281\n",
      "Train Epoch: 318 [124992/225000 (56%)] Loss: 16967.572266\n",
      "Train Epoch: 318 [127488/225000 (57%)] Loss: 17150.263672\n",
      "Train Epoch: 318 [129984/225000 (58%)] Loss: 17254.990234\n",
      "Train Epoch: 318 [132480/225000 (59%)] Loss: 16683.666016\n",
      "Train Epoch: 318 [134976/225000 (60%)] Loss: 16944.601562\n",
      "Train Epoch: 318 [137472/225000 (61%)] Loss: 16918.468750\n",
      "Train Epoch: 318 [139968/225000 (62%)] Loss: 16743.253906\n",
      "Train Epoch: 318 [142464/225000 (63%)] Loss: 17579.248047\n",
      "Train Epoch: 318 [144960/225000 (64%)] Loss: 17521.078125\n",
      "Train Epoch: 318 [147456/225000 (66%)] Loss: 17024.585938\n",
      "Train Epoch: 318 [149952/225000 (67%)] Loss: 17136.009766\n",
      "Train Epoch: 318 [152448/225000 (68%)] Loss: 17186.371094\n",
      "Train Epoch: 318 [154944/225000 (69%)] Loss: 16926.445312\n",
      "Train Epoch: 318 [157440/225000 (70%)] Loss: 18973.597656\n",
      "Train Epoch: 318 [159936/225000 (71%)] Loss: 17036.121094\n",
      "Train Epoch: 318 [162432/225000 (72%)] Loss: 16559.060547\n",
      "Train Epoch: 318 [164928/225000 (73%)] Loss: 17355.347656\n",
      "Train Epoch: 318 [167424/225000 (74%)] Loss: 17360.273438\n",
      "Train Epoch: 318 [169920/225000 (76%)] Loss: 16722.003906\n",
      "Train Epoch: 318 [172416/225000 (77%)] Loss: 17535.414062\n",
      "Train Epoch: 318 [174912/225000 (78%)] Loss: 17095.921875\n",
      "Train Epoch: 318 [177408/225000 (79%)] Loss: 17086.148438\n",
      "Train Epoch: 318 [179904/225000 (80%)] Loss: 17419.486328\n",
      "Train Epoch: 318 [182400/225000 (81%)] Loss: 16574.750000\n",
      "Train Epoch: 318 [184896/225000 (82%)] Loss: 17048.515625\n",
      "Train Epoch: 318 [187392/225000 (83%)] Loss: 17139.937500\n",
      "Train Epoch: 318 [189888/225000 (84%)] Loss: 16371.374023\n",
      "Train Epoch: 318 [192384/225000 (86%)] Loss: 16645.171875\n",
      "Train Epoch: 318 [194880/225000 (87%)] Loss: 17338.650391\n",
      "Train Epoch: 318 [197376/225000 (88%)] Loss: 16916.718750\n",
      "Train Epoch: 318 [199872/225000 (89%)] Loss: 17344.308594\n",
      "Train Epoch: 318 [202368/225000 (90%)] Loss: 16954.326172\n",
      "Train Epoch: 318 [204864/225000 (91%)] Loss: 18969.683594\n",
      "Train Epoch: 318 [207360/225000 (92%)] Loss: 17274.312500\n",
      "Train Epoch: 318 [209856/225000 (93%)] Loss: 16688.130859\n",
      "Train Epoch: 318 [212352/225000 (94%)] Loss: 16994.171875\n",
      "Train Epoch: 318 [214848/225000 (95%)] Loss: 16858.031250\n",
      "Train Epoch: 318 [217344/225000 (97%)] Loss: 17405.044922\n",
      "Train Epoch: 318 [219840/225000 (98%)] Loss: 16302.694336\n",
      "Train Epoch: 318 [222336/225000 (99%)] Loss: 17232.882812\n",
      "Train Epoch: 318 [224832/225000 (100%)] Loss: 16916.312500\n",
      "    epoch          : 318\n",
      "    loss           : 17139.887272857562\n",
      "    val_loss       : 17103.27517911769\n",
      "Train Epoch: 319 [192/225000 (0%)] Loss: 16857.093750\n",
      "Train Epoch: 319 [2688/225000 (1%)] Loss: 17435.121094\n",
      "Train Epoch: 319 [5184/225000 (2%)] Loss: 17148.394531\n",
      "Train Epoch: 319 [7680/225000 (3%)] Loss: 17088.019531\n",
      "Train Epoch: 319 [10176/225000 (5%)] Loss: 17026.878906\n",
      "Train Epoch: 319 [12672/225000 (6%)] Loss: 16878.550781\n",
      "Train Epoch: 319 [15168/225000 (7%)] Loss: 17252.263672\n",
      "Train Epoch: 319 [17664/225000 (8%)] Loss: 17347.367188\n",
      "Train Epoch: 319 [20160/225000 (9%)] Loss: 16759.460938\n",
      "Train Epoch: 319 [22656/225000 (10%)] Loss: 16929.593750\n",
      "Train Epoch: 319 [25152/225000 (11%)] Loss: 17601.107422\n",
      "Train Epoch: 319 [27648/225000 (12%)] Loss: 16669.386719\n",
      "Train Epoch: 319 [30144/225000 (13%)] Loss: 17275.373047\n",
      "Train Epoch: 319 [32640/225000 (15%)] Loss: 17134.898438\n",
      "Train Epoch: 319 [35136/225000 (16%)] Loss: 16756.068359\n",
      "Train Epoch: 319 [37632/225000 (17%)] Loss: 16962.646484\n",
      "Train Epoch: 319 [40128/225000 (18%)] Loss: 16938.632812\n",
      "Train Epoch: 319 [42624/225000 (19%)] Loss: 16728.835938\n",
      "Train Epoch: 319 [45120/225000 (20%)] Loss: 16813.470703\n",
      "Train Epoch: 319 [47616/225000 (21%)] Loss: 16998.019531\n",
      "Train Epoch: 319 [50112/225000 (22%)] Loss: 17078.052734\n",
      "Train Epoch: 319 [52608/225000 (23%)] Loss: 17128.671875\n",
      "Train Epoch: 319 [55104/225000 (24%)] Loss: 16730.136719\n",
      "Train Epoch: 319 [57600/225000 (26%)] Loss: 16918.386719\n",
      "Train Epoch: 319 [60096/225000 (27%)] Loss: 18687.576172\n",
      "Train Epoch: 319 [62592/225000 (28%)] Loss: 17147.111328\n",
      "Train Epoch: 319 [65088/225000 (29%)] Loss: 17113.107422\n",
      "Train Epoch: 319 [67584/225000 (30%)] Loss: 17202.507812\n",
      "Train Epoch: 319 [70080/225000 (31%)] Loss: 18049.925781\n",
      "Train Epoch: 319 [72576/225000 (32%)] Loss: 17087.814453\n",
      "Train Epoch: 319 [75072/225000 (33%)] Loss: 16998.976562\n",
      "Train Epoch: 319 [77568/225000 (34%)] Loss: 16630.486328\n",
      "Train Epoch: 319 [80064/225000 (36%)] Loss: 17115.939453\n",
      "Train Epoch: 319 [82560/225000 (37%)] Loss: 17181.238281\n",
      "Train Epoch: 319 [85056/225000 (38%)] Loss: 16907.667969\n",
      "Train Epoch: 319 [87552/225000 (39%)] Loss: 16671.300781\n",
      "Train Epoch: 319 [90048/225000 (40%)] Loss: 17034.496094\n",
      "Train Epoch: 319 [92544/225000 (41%)] Loss: 16429.097656\n",
      "Train Epoch: 319 [95040/225000 (42%)] Loss: 17352.421875\n",
      "Train Epoch: 319 [97536/225000 (43%)] Loss: 16922.472656\n",
      "Train Epoch: 319 [100032/225000 (44%)] Loss: 16858.292969\n",
      "Train Epoch: 319 [102528/225000 (46%)] Loss: 16832.480469\n",
      "Train Epoch: 319 [105024/225000 (47%)] Loss: 17200.621094\n",
      "Train Epoch: 319 [107520/225000 (48%)] Loss: 17353.361328\n",
      "Train Epoch: 319 [110016/225000 (49%)] Loss: 17357.031250\n",
      "Train Epoch: 319 [112512/225000 (50%)] Loss: 17000.402344\n",
      "Train Epoch: 319 [115008/225000 (51%)] Loss: 16551.093750\n",
      "Train Epoch: 319 [117504/225000 (52%)] Loss: 16685.203125\n",
      "Train Epoch: 319 [120000/225000 (53%)] Loss: 16601.300781\n",
      "Train Epoch: 319 [122496/225000 (54%)] Loss: 17258.509766\n",
      "Train Epoch: 319 [124992/225000 (56%)] Loss: 17261.660156\n",
      "Train Epoch: 319 [127488/225000 (57%)] Loss: 19063.419922\n",
      "Train Epoch: 319 [129984/225000 (58%)] Loss: 16683.207031\n",
      "Train Epoch: 319 [132480/225000 (59%)] Loss: 16845.935547\n",
      "Train Epoch: 319 [134976/225000 (60%)] Loss: 17479.679688\n",
      "Train Epoch: 319 [137472/225000 (61%)] Loss: 16884.554688\n",
      "Train Epoch: 319 [139968/225000 (62%)] Loss: 16964.285156\n",
      "Train Epoch: 319 [142464/225000 (63%)] Loss: 16632.492188\n",
      "Train Epoch: 319 [144960/225000 (64%)] Loss: 17078.574219\n",
      "Train Epoch: 319 [147456/225000 (66%)] Loss: 17374.437500\n",
      "Train Epoch: 319 [149952/225000 (67%)] Loss: 17194.544922\n",
      "Train Epoch: 319 [152448/225000 (68%)] Loss: 16678.757812\n",
      "Train Epoch: 319 [154944/225000 (69%)] Loss: 17035.597656\n",
      "Train Epoch: 319 [157440/225000 (70%)] Loss: 17063.201172\n",
      "Train Epoch: 319 [159936/225000 (71%)] Loss: 17149.314453\n",
      "Train Epoch: 319 [162432/225000 (72%)] Loss: 16737.873047\n",
      "Train Epoch: 319 [164928/225000 (73%)] Loss: 18694.919922\n",
      "Train Epoch: 319 [167424/225000 (74%)] Loss: 17459.130859\n",
      "Train Epoch: 319 [169920/225000 (76%)] Loss: 17457.726562\n",
      "Train Epoch: 319 [172416/225000 (77%)] Loss: 16963.535156\n",
      "Train Epoch: 319 [174912/225000 (78%)] Loss: 16773.376953\n",
      "Train Epoch: 319 [177408/225000 (79%)] Loss: 16311.299805\n",
      "Train Epoch: 319 [179904/225000 (80%)] Loss: 16778.123047\n",
      "Train Epoch: 319 [182400/225000 (81%)] Loss: 17015.199219\n",
      "Train Epoch: 319 [184896/225000 (82%)] Loss: 16984.679688\n",
      "Train Epoch: 319 [187392/225000 (83%)] Loss: 16983.740234\n",
      "Train Epoch: 319 [189888/225000 (84%)] Loss: 17144.878906\n",
      "Train Epoch: 319 [192384/225000 (86%)] Loss: 16711.109375\n",
      "Train Epoch: 319 [194880/225000 (87%)] Loss: 17012.400391\n",
      "Train Epoch: 319 [197376/225000 (88%)] Loss: 16983.810547\n",
      "Train Epoch: 319 [199872/225000 (89%)] Loss: 17047.484375\n",
      "Train Epoch: 319 [202368/225000 (90%)] Loss: 17273.503906\n",
      "Train Epoch: 319 [204864/225000 (91%)] Loss: 17662.171875\n",
      "Train Epoch: 319 [207360/225000 (92%)] Loss: 17273.142578\n",
      "Train Epoch: 319 [209856/225000 (93%)] Loss: 18963.019531\n",
      "Train Epoch: 319 [212352/225000 (94%)] Loss: 17270.597656\n",
      "Train Epoch: 319 [214848/225000 (95%)] Loss: 16951.375000\n",
      "Train Epoch: 319 [217344/225000 (97%)] Loss: 17138.265625\n",
      "Train Epoch: 319 [219840/225000 (98%)] Loss: 17350.792969\n",
      "Train Epoch: 319 [222336/225000 (99%)] Loss: 17713.339844\n",
      "Train Epoch: 319 [224832/225000 (100%)] Loss: 17156.576172\n",
      "    epoch          : 319\n",
      "    loss           : 17100.359434160357\n",
      "    val_loss       : 17017.534902504383\n",
      "Train Epoch: 320 [192/225000 (0%)] Loss: 17303.994141\n",
      "Train Epoch: 320 [2688/225000 (1%)] Loss: 17523.316406\n",
      "Train Epoch: 320 [5184/225000 (2%)] Loss: 17151.546875\n",
      "Train Epoch: 320 [7680/225000 (3%)] Loss: 17108.250000\n",
      "Train Epoch: 320 [10176/225000 (5%)] Loss: 16720.841797\n",
      "Train Epoch: 320 [12672/225000 (6%)] Loss: 17222.222656\n",
      "Train Epoch: 320 [15168/225000 (7%)] Loss: 17164.457031\n",
      "Train Epoch: 320 [17664/225000 (8%)] Loss: 16907.082031\n",
      "Train Epoch: 320 [20160/225000 (9%)] Loss: 17516.660156\n",
      "Train Epoch: 320 [22656/225000 (10%)] Loss: 16851.554688\n",
      "Train Epoch: 320 [25152/225000 (11%)] Loss: 17093.107422\n",
      "Train Epoch: 320 [27648/225000 (12%)] Loss: 17182.242188\n",
      "Train Epoch: 320 [30144/225000 (13%)] Loss: 17123.878906\n",
      "Train Epoch: 320 [32640/225000 (15%)] Loss: 17126.767578\n",
      "Train Epoch: 320 [35136/225000 (16%)] Loss: 17280.375000\n",
      "Train Epoch: 320 [37632/225000 (17%)] Loss: 17305.492188\n",
      "Train Epoch: 320 [40128/225000 (18%)] Loss: 16959.142578\n",
      "Train Epoch: 320 [42624/225000 (19%)] Loss: 17161.496094\n",
      "Train Epoch: 320 [45120/225000 (20%)] Loss: 17362.507812\n",
      "Train Epoch: 320 [47616/225000 (21%)] Loss: 17104.269531\n",
      "Train Epoch: 320 [50112/225000 (22%)] Loss: 17095.871094\n",
      "Train Epoch: 320 [52608/225000 (23%)] Loss: 16963.847656\n",
      "Train Epoch: 320 [55104/225000 (24%)] Loss: 17130.652344\n",
      "Train Epoch: 320 [57600/225000 (26%)] Loss: 16741.171875\n",
      "Train Epoch: 320 [60096/225000 (27%)] Loss: 16919.710938\n",
      "Train Epoch: 320 [62592/225000 (28%)] Loss: 16827.695312\n",
      "Train Epoch: 320 [65088/225000 (29%)] Loss: 17038.910156\n",
      "Train Epoch: 320 [67584/225000 (30%)] Loss: 16852.832031\n",
      "Train Epoch: 320 [70080/225000 (31%)] Loss: 17522.234375\n",
      "Train Epoch: 320 [72576/225000 (32%)] Loss: 17191.992188\n",
      "Train Epoch: 320 [75072/225000 (33%)] Loss: 17230.277344\n",
      "Train Epoch: 320 [77568/225000 (34%)] Loss: 17279.175781\n",
      "Train Epoch: 320 [80064/225000 (36%)] Loss: 17185.851562\n",
      "Train Epoch: 320 [82560/225000 (37%)] Loss: 17328.472656\n",
      "Train Epoch: 320 [85056/225000 (38%)] Loss: 17206.169922\n",
      "Train Epoch: 320 [87552/225000 (39%)] Loss: 17431.880859\n",
      "Train Epoch: 320 [90048/225000 (40%)] Loss: 16632.082031\n",
      "Train Epoch: 320 [92544/225000 (41%)] Loss: 16796.355469\n",
      "Train Epoch: 320 [95040/225000 (42%)] Loss: 17356.054688\n",
      "Train Epoch: 320 [97536/225000 (43%)] Loss: 17359.195312\n",
      "Train Epoch: 320 [100032/225000 (44%)] Loss: 16448.460938\n",
      "Train Epoch: 320 [102528/225000 (46%)] Loss: 17292.542969\n",
      "Train Epoch: 320 [105024/225000 (47%)] Loss: 17045.621094\n",
      "Train Epoch: 320 [107520/225000 (48%)] Loss: 16371.954102\n",
      "Train Epoch: 320 [110016/225000 (49%)] Loss: 17211.052734\n",
      "Train Epoch: 320 [112512/225000 (50%)] Loss: 16873.542969\n",
      "Train Epoch: 320 [115008/225000 (51%)] Loss: 17063.054688\n",
      "Train Epoch: 320 [117504/225000 (52%)] Loss: 17023.390625\n",
      "Train Epoch: 320 [120000/225000 (53%)] Loss: 17037.777344\n",
      "Train Epoch: 320 [122496/225000 (54%)] Loss: 17102.558594\n",
      "Train Epoch: 320 [124992/225000 (56%)] Loss: 17222.972656\n",
      "Train Epoch: 320 [127488/225000 (57%)] Loss: 17239.214844\n",
      "Train Epoch: 320 [129984/225000 (58%)] Loss: 17076.167969\n",
      "Train Epoch: 320 [132480/225000 (59%)] Loss: 17151.746094\n",
      "Train Epoch: 320 [134976/225000 (60%)] Loss: 17088.835938\n",
      "Train Epoch: 320 [137472/225000 (61%)] Loss: 17348.425781\n",
      "Train Epoch: 320 [139968/225000 (62%)] Loss: 17047.816406\n",
      "Train Epoch: 320 [142464/225000 (63%)] Loss: 16519.679688\n",
      "Train Epoch: 320 [144960/225000 (64%)] Loss: 16616.062500\n",
      "Train Epoch: 320 [147456/225000 (66%)] Loss: 16790.953125\n",
      "Train Epoch: 320 [149952/225000 (67%)] Loss: 16743.859375\n",
      "Train Epoch: 320 [152448/225000 (68%)] Loss: 16776.636719\n",
      "Train Epoch: 320 [154944/225000 (69%)] Loss: 17396.167969\n",
      "Train Epoch: 320 [157440/225000 (70%)] Loss: 16948.136719\n",
      "Train Epoch: 320 [159936/225000 (71%)] Loss: 16910.785156\n",
      "Train Epoch: 320 [162432/225000 (72%)] Loss: 16519.939453\n",
      "Train Epoch: 320 [164928/225000 (73%)] Loss: 17073.984375\n",
      "Train Epoch: 320 [167424/225000 (74%)] Loss: 17185.503906\n",
      "Train Epoch: 320 [169920/225000 (76%)] Loss: 17270.488281\n",
      "Train Epoch: 320 [172416/225000 (77%)] Loss: 17120.720703\n",
      "Train Epoch: 320 [174912/225000 (78%)] Loss: 16752.589844\n",
      "Train Epoch: 320 [177408/225000 (79%)] Loss: 16940.191406\n",
      "Train Epoch: 320 [179904/225000 (80%)] Loss: 16849.255859\n",
      "Train Epoch: 320 [182400/225000 (81%)] Loss: 17118.990234\n",
      "Train Epoch: 320 [184896/225000 (82%)] Loss: 16572.449219\n",
      "Train Epoch: 320 [187392/225000 (83%)] Loss: 17142.556641\n",
      "Train Epoch: 320 [189888/225000 (84%)] Loss: 16579.562500\n",
      "Train Epoch: 320 [192384/225000 (86%)] Loss: 16781.683594\n",
      "Train Epoch: 320 [194880/225000 (87%)] Loss: 17310.171875\n",
      "Train Epoch: 320 [197376/225000 (88%)] Loss: 17070.281250\n",
      "Train Epoch: 320 [199872/225000 (89%)] Loss: 16935.261719\n",
      "Train Epoch: 320 [202368/225000 (90%)] Loss: 16915.544922\n",
      "Train Epoch: 320 [204864/225000 (91%)] Loss: 17211.000000\n",
      "Train Epoch: 320 [207360/225000 (92%)] Loss: 16817.015625\n",
      "Train Epoch: 320 [209856/225000 (93%)] Loss: 17122.337891\n",
      "Train Epoch: 320 [212352/225000 (94%)] Loss: 17223.841797\n",
      "Train Epoch: 320 [214848/225000 (95%)] Loss: 17121.164062\n",
      "Train Epoch: 320 [217344/225000 (97%)] Loss: 17031.361328\n",
      "Train Epoch: 320 [219840/225000 (98%)] Loss: 17103.089844\n",
      "Train Epoch: 320 [222336/225000 (99%)] Loss: 16735.566406\n",
      "Train Epoch: 320 [224832/225000 (100%)] Loss: 16423.339844\n",
      "    epoch          : 320\n",
      "    loss           : 17073.763568552688\n",
      "    val_loss       : 16917.669060067365\n",
      "Train Epoch: 321 [192/225000 (0%)] Loss: 16798.710938\n",
      "Train Epoch: 321 [2688/225000 (1%)] Loss: 16970.505859\n",
      "Train Epoch: 321 [5184/225000 (2%)] Loss: 17002.078125\n",
      "Train Epoch: 321 [7680/225000 (3%)] Loss: 16728.218750\n",
      "Train Epoch: 321 [10176/225000 (5%)] Loss: 18801.734375\n",
      "Train Epoch: 321 [12672/225000 (6%)] Loss: 16940.970703\n",
      "Train Epoch: 321 [15168/225000 (7%)] Loss: 17257.298828\n",
      "Train Epoch: 321 [17664/225000 (8%)] Loss: 18752.648438\n",
      "Train Epoch: 321 [20160/225000 (9%)] Loss: 16493.013672\n",
      "Train Epoch: 321 [22656/225000 (10%)] Loss: 17095.103516\n",
      "Train Epoch: 321 [25152/225000 (11%)] Loss: 17257.472656\n",
      "Train Epoch: 321 [27648/225000 (12%)] Loss: 17161.003906\n",
      "Train Epoch: 321 [30144/225000 (13%)] Loss: 16936.500000\n",
      "Train Epoch: 321 [32640/225000 (15%)] Loss: 16970.156250\n",
      "Train Epoch: 321 [35136/225000 (16%)] Loss: 17260.296875\n",
      "Train Epoch: 321 [37632/225000 (17%)] Loss: 17289.808594\n",
      "Train Epoch: 321 [40128/225000 (18%)] Loss: 16567.945312\n",
      "Train Epoch: 321 [42624/225000 (19%)] Loss: 16931.859375\n",
      "Train Epoch: 321 [45120/225000 (20%)] Loss: 17245.583984\n",
      "Train Epoch: 321 [47616/225000 (21%)] Loss: 17069.628906\n",
      "Train Epoch: 321 [50112/225000 (22%)] Loss: 16965.039062\n",
      "Train Epoch: 321 [52608/225000 (23%)] Loss: 17151.175781\n",
      "Train Epoch: 321 [55104/225000 (24%)] Loss: 16917.675781\n",
      "Train Epoch: 321 [57600/225000 (26%)] Loss: 17186.117188\n",
      "Train Epoch: 321 [60096/225000 (27%)] Loss: 16993.001953\n",
      "Train Epoch: 321 [62592/225000 (28%)] Loss: 16967.136719\n",
      "Train Epoch: 321 [65088/225000 (29%)] Loss: 16464.269531\n",
      "Train Epoch: 321 [67584/225000 (30%)] Loss: 17113.683594\n",
      "Train Epoch: 321 [70080/225000 (31%)] Loss: 16592.804688\n",
      "Train Epoch: 321 [72576/225000 (32%)] Loss: 17059.683594\n",
      "Train Epoch: 321 [75072/225000 (33%)] Loss: 16986.980469\n",
      "Train Epoch: 321 [77568/225000 (34%)] Loss: 17195.839844\n",
      "Train Epoch: 321 [80064/225000 (36%)] Loss: 17003.501953\n",
      "Train Epoch: 321 [82560/225000 (37%)] Loss: 17427.316406\n",
      "Train Epoch: 321 [85056/225000 (38%)] Loss: 16894.636719\n",
      "Train Epoch: 321 [87552/225000 (39%)] Loss: 17014.320312\n",
      "Train Epoch: 321 [90048/225000 (40%)] Loss: 18784.816406\n",
      "Train Epoch: 321 [92544/225000 (41%)] Loss: 17161.585938\n",
      "Train Epoch: 321 [95040/225000 (42%)] Loss: 17692.832031\n",
      "Train Epoch: 321 [97536/225000 (43%)] Loss: 17080.310547\n",
      "Train Epoch: 321 [100032/225000 (44%)] Loss: 17285.939453\n",
      "Train Epoch: 321 [102528/225000 (46%)] Loss: 16744.675781\n",
      "Train Epoch: 321 [105024/225000 (47%)] Loss: 17226.076172\n",
      "Train Epoch: 321 [107520/225000 (48%)] Loss: 17100.683594\n",
      "Train Epoch: 321 [110016/225000 (49%)] Loss: 17351.753906\n",
      "Train Epoch: 321 [112512/225000 (50%)] Loss: 17034.517578\n",
      "Train Epoch: 321 [115008/225000 (51%)] Loss: 17399.703125\n",
      "Train Epoch: 321 [117504/225000 (52%)] Loss: 17113.388672\n",
      "Train Epoch: 321 [120000/225000 (53%)] Loss: 17075.996094\n",
      "Train Epoch: 321 [122496/225000 (54%)] Loss: 16405.371094\n",
      "Train Epoch: 321 [124992/225000 (56%)] Loss: 17066.109375\n",
      "Train Epoch: 321 [127488/225000 (57%)] Loss: 16659.132812\n",
      "Train Epoch: 321 [129984/225000 (58%)] Loss: 16807.939453\n",
      "Train Epoch: 321 [132480/225000 (59%)] Loss: 16942.273438\n",
      "Train Epoch: 321 [134976/225000 (60%)] Loss: 16700.746094\n",
      "Train Epoch: 321 [137472/225000 (61%)] Loss: 16912.285156\n",
      "Train Epoch: 321 [139968/225000 (62%)] Loss: 17125.625000\n",
      "Train Epoch: 321 [142464/225000 (63%)] Loss: 16663.468750\n",
      "Train Epoch: 321 [144960/225000 (64%)] Loss: 16749.470703\n",
      "Train Epoch: 321 [147456/225000 (66%)] Loss: 16996.142578\n",
      "Train Epoch: 321 [149952/225000 (67%)] Loss: 17181.351562\n",
      "Train Epoch: 321 [152448/225000 (68%)] Loss: 17111.177734\n",
      "Train Epoch: 321 [154944/225000 (69%)] Loss: 16903.171875\n",
      "Train Epoch: 321 [157440/225000 (70%)] Loss: 17395.785156\n",
      "Train Epoch: 321 [159936/225000 (71%)] Loss: 17105.947266\n",
      "Train Epoch: 321 [162432/225000 (72%)] Loss: 17082.546875\n",
      "Train Epoch: 321 [164928/225000 (73%)] Loss: 16996.867188\n",
      "Train Epoch: 321 [167424/225000 (74%)] Loss: 16780.523438\n",
      "Train Epoch: 321 [169920/225000 (76%)] Loss: 16643.414062\n",
      "Train Epoch: 321 [172416/225000 (77%)] Loss: 17119.722656\n",
      "Train Epoch: 321 [174912/225000 (78%)] Loss: 16753.152344\n",
      "Train Epoch: 321 [177408/225000 (79%)] Loss: 18729.843750\n",
      "Train Epoch: 321 [179904/225000 (80%)] Loss: 17133.289062\n",
      "Train Epoch: 321 [182400/225000 (81%)] Loss: 16912.505859\n",
      "Train Epoch: 321 [184896/225000 (82%)] Loss: 17259.011719\n",
      "Train Epoch: 321 [187392/225000 (83%)] Loss: 16967.773438\n",
      "Train Epoch: 321 [189888/225000 (84%)] Loss: 17281.896484\n",
      "Train Epoch: 321 [192384/225000 (86%)] Loss: 17152.410156\n",
      "Train Epoch: 321 [194880/225000 (87%)] Loss: 16962.238281\n",
      "Train Epoch: 321 [197376/225000 (88%)] Loss: 17084.933594\n",
      "Train Epoch: 321 [199872/225000 (89%)] Loss: 17452.363281\n",
      "Train Epoch: 321 [202368/225000 (90%)] Loss: 17038.960938\n",
      "Train Epoch: 321 [204864/225000 (91%)] Loss: 16969.435547\n",
      "Train Epoch: 321 [207360/225000 (92%)] Loss: 16486.662109\n",
      "Train Epoch: 321 [209856/225000 (93%)] Loss: 16788.410156\n",
      "Train Epoch: 321 [212352/225000 (94%)] Loss: 17153.406250\n",
      "Train Epoch: 321 [214848/225000 (95%)] Loss: 17126.058594\n",
      "Train Epoch: 321 [217344/225000 (97%)] Loss: 16911.439453\n",
      "Train Epoch: 321 [219840/225000 (98%)] Loss: 16671.716797\n",
      "Train Epoch: 321 [222336/225000 (99%)] Loss: 17085.660156\n",
      "Train Epoch: 321 [224832/225000 (100%)] Loss: 17032.404297\n",
      "    epoch          : 321\n",
      "    loss           : 17054.880912702643\n",
      "    val_loss       : 17067.7126389478\n",
      "Train Epoch: 322 [192/225000 (0%)] Loss: 17345.279297\n",
      "Train Epoch: 322 [2688/225000 (1%)] Loss: 16773.193359\n",
      "Train Epoch: 322 [5184/225000 (2%)] Loss: 17047.833984\n",
      "Train Epoch: 322 [7680/225000 (3%)] Loss: 17411.238281\n",
      "Train Epoch: 322 [10176/225000 (5%)] Loss: 16839.802734\n",
      "Train Epoch: 322 [12672/225000 (6%)] Loss: 17190.048828\n",
      "Train Epoch: 322 [15168/225000 (7%)] Loss: 16652.984375\n",
      "Train Epoch: 322 [17664/225000 (8%)] Loss: 17021.917969\n",
      "Train Epoch: 322 [20160/225000 (9%)] Loss: 16691.839844\n",
      "Train Epoch: 322 [22656/225000 (10%)] Loss: 16994.960938\n",
      "Train Epoch: 322 [25152/225000 (11%)] Loss: 17295.164062\n",
      "Train Epoch: 322 [27648/225000 (12%)] Loss: 17176.353516\n",
      "Train Epoch: 322 [30144/225000 (13%)] Loss: 16738.542969\n",
      "Train Epoch: 322 [32640/225000 (15%)] Loss: 16895.191406\n",
      "Train Epoch: 322 [35136/225000 (16%)] Loss: 17048.382812\n",
      "Train Epoch: 322 [37632/225000 (17%)] Loss: 16402.160156\n",
      "Train Epoch: 322 [40128/225000 (18%)] Loss: 16860.878906\n",
      "Train Epoch: 322 [42624/225000 (19%)] Loss: 17434.265625\n",
      "Train Epoch: 322 [45120/225000 (20%)] Loss: 16766.707031\n",
      "Train Epoch: 322 [47616/225000 (21%)] Loss: 17303.673828\n",
      "Train Epoch: 322 [50112/225000 (22%)] Loss: 17012.677734\n",
      "Train Epoch: 322 [52608/225000 (23%)] Loss: 17055.765625\n",
      "Train Epoch: 322 [55104/225000 (24%)] Loss: 17069.070312\n",
      "Train Epoch: 322 [57600/225000 (26%)] Loss: 16478.939453\n",
      "Train Epoch: 322 [60096/225000 (27%)] Loss: 17225.343750\n",
      "Train Epoch: 322 [62592/225000 (28%)] Loss: 16928.080078\n",
      "Train Epoch: 322 [65088/225000 (29%)] Loss: 17386.023438\n",
      "Train Epoch: 322 [67584/225000 (30%)] Loss: 17515.003906\n",
      "Train Epoch: 322 [70080/225000 (31%)] Loss: 17137.707031\n",
      "Train Epoch: 322 [72576/225000 (32%)] Loss: 16916.109375\n",
      "Train Epoch: 322 [75072/225000 (33%)] Loss: 16804.984375\n",
      "Train Epoch: 322 [77568/225000 (34%)] Loss: 17204.544922\n",
      "Train Epoch: 322 [80064/225000 (36%)] Loss: 17339.273438\n",
      "Train Epoch: 322 [82560/225000 (37%)] Loss: 16933.621094\n",
      "Train Epoch: 322 [85056/225000 (38%)] Loss: 17058.738281\n",
      "Train Epoch: 322 [87552/225000 (39%)] Loss: 16939.156250\n",
      "Train Epoch: 322 [90048/225000 (40%)] Loss: 17207.248047\n",
      "Train Epoch: 322 [92544/225000 (41%)] Loss: 17014.746094\n",
      "Train Epoch: 322 [95040/225000 (42%)] Loss: 17071.876953\n",
      "Train Epoch: 322 [97536/225000 (43%)] Loss: 16843.074219\n",
      "Train Epoch: 322 [100032/225000 (44%)] Loss: 16775.179688\n",
      "Train Epoch: 322 [102528/225000 (46%)] Loss: 17084.707031\n",
      "Train Epoch: 322 [105024/225000 (47%)] Loss: 16707.876953\n",
      "Train Epoch: 322 [107520/225000 (48%)] Loss: 16468.832031\n",
      "Train Epoch: 322 [110016/225000 (49%)] Loss: 17224.800781\n",
      "Train Epoch: 322 [112512/225000 (50%)] Loss: 16559.908203\n",
      "Train Epoch: 322 [115008/225000 (51%)] Loss: 17231.156250\n",
      "Train Epoch: 322 [117504/225000 (52%)] Loss: 17084.320312\n",
      "Train Epoch: 322 [120000/225000 (53%)] Loss: 18363.078125\n",
      "Train Epoch: 322 [122496/225000 (54%)] Loss: 17331.820312\n",
      "Train Epoch: 322 [124992/225000 (56%)] Loss: 17092.335938\n",
      "Train Epoch: 322 [127488/225000 (57%)] Loss: 16774.919922\n",
      "Train Epoch: 322 [129984/225000 (58%)] Loss: 17006.289062\n",
      "Train Epoch: 322 [132480/225000 (59%)] Loss: 17115.609375\n",
      "Train Epoch: 322 [134976/225000 (60%)] Loss: 16969.921875\n",
      "Train Epoch: 322 [137472/225000 (61%)] Loss: 17026.800781\n",
      "Train Epoch: 322 [139968/225000 (62%)] Loss: 17245.623047\n",
      "Train Epoch: 322 [142464/225000 (63%)] Loss: 16734.199219\n",
      "Train Epoch: 322 [144960/225000 (64%)] Loss: 16994.029297\n",
      "Train Epoch: 322 [147456/225000 (66%)] Loss: 17015.828125\n",
      "Train Epoch: 322 [149952/225000 (67%)] Loss: 17140.675781\n",
      "Train Epoch: 322 [152448/225000 (68%)] Loss: 16931.892578\n",
      "Train Epoch: 322 [154944/225000 (69%)] Loss: 17051.089844\n",
      "Train Epoch: 322 [157440/225000 (70%)] Loss: 17211.269531\n",
      "Train Epoch: 322 [159936/225000 (71%)] Loss: 16963.261719\n",
      "Train Epoch: 322 [162432/225000 (72%)] Loss: 17054.882812\n",
      "Train Epoch: 322 [164928/225000 (73%)] Loss: 16933.753906\n",
      "Train Epoch: 322 [167424/225000 (74%)] Loss: 16762.761719\n",
      "Train Epoch: 322 [169920/225000 (76%)] Loss: 17163.875000\n",
      "Train Epoch: 322 [172416/225000 (77%)] Loss: 16961.464844\n",
      "Train Epoch: 322 [174912/225000 (78%)] Loss: 17255.431641\n",
      "Train Epoch: 322 [177408/225000 (79%)] Loss: 17215.625000\n",
      "Train Epoch: 322 [179904/225000 (80%)] Loss: 17392.449219\n",
      "Train Epoch: 322 [182400/225000 (81%)] Loss: 17117.896484\n",
      "Train Epoch: 322 [184896/225000 (82%)] Loss: 17282.925781\n",
      "Train Epoch: 322 [187392/225000 (83%)] Loss: 16859.343750\n",
      "Train Epoch: 322 [189888/225000 (84%)] Loss: 16996.839844\n",
      "Train Epoch: 322 [192384/225000 (86%)] Loss: 17267.513672\n",
      "Train Epoch: 322 [194880/225000 (87%)] Loss: 17035.455078\n",
      "Train Epoch: 322 [197376/225000 (88%)] Loss: 16988.498047\n",
      "Train Epoch: 322 [199872/225000 (89%)] Loss: 17291.609375\n",
      "Train Epoch: 322 [202368/225000 (90%)] Loss: 17334.046875\n",
      "Train Epoch: 322 [204864/225000 (91%)] Loss: 16925.652344\n",
      "Train Epoch: 322 [207360/225000 (92%)] Loss: 17418.238281\n",
      "Train Epoch: 322 [209856/225000 (93%)] Loss: 16852.371094\n",
      "Train Epoch: 322 [212352/225000 (94%)] Loss: 16914.259766\n",
      "Train Epoch: 322 [214848/225000 (95%)] Loss: 16464.195312\n",
      "Train Epoch: 322 [217344/225000 (97%)] Loss: 17216.250000\n",
      "Train Epoch: 322 [219840/225000 (98%)] Loss: 17065.662109\n",
      "Train Epoch: 322 [222336/225000 (99%)] Loss: 17484.949219\n",
      "Train Epoch: 322 [224832/225000 (100%)] Loss: 16993.015625\n",
      "    epoch          : 322\n",
      "    loss           : 17023.14065832978\n",
      "    val_loss       : 16911.163753240162\n",
      "Train Epoch: 323 [192/225000 (0%)] Loss: 16810.339844\n",
      "Train Epoch: 323 [2688/225000 (1%)] Loss: 17180.152344\n",
      "Train Epoch: 323 [5184/225000 (2%)] Loss: 16592.648438\n",
      "Train Epoch: 323 [7680/225000 (3%)] Loss: 16820.207031\n",
      "Train Epoch: 323 [10176/225000 (5%)] Loss: 16826.742188\n",
      "Train Epoch: 323 [12672/225000 (6%)] Loss: 16931.798828\n",
      "Train Epoch: 323 [15168/225000 (7%)] Loss: 16630.894531\n",
      "Train Epoch: 323 [17664/225000 (8%)] Loss: 16582.710938\n",
      "Train Epoch: 323 [20160/225000 (9%)] Loss: 17025.804688\n",
      "Train Epoch: 323 [22656/225000 (10%)] Loss: 17272.105469\n",
      "Train Epoch: 323 [25152/225000 (11%)] Loss: 16858.871094\n",
      "Train Epoch: 323 [27648/225000 (12%)] Loss: 16599.660156\n",
      "Train Epoch: 323 [30144/225000 (13%)] Loss: 16667.517578\n",
      "Train Epoch: 323 [32640/225000 (15%)] Loss: 16844.328125\n",
      "Train Epoch: 323 [35136/225000 (16%)] Loss: 17379.345703\n",
      "Train Epoch: 323 [37632/225000 (17%)] Loss: 17052.738281\n",
      "Train Epoch: 323 [40128/225000 (18%)] Loss: 16949.488281\n",
      "Train Epoch: 323 [42624/225000 (19%)] Loss: 17273.121094\n",
      "Train Epoch: 323 [45120/225000 (20%)] Loss: 17760.765625\n",
      "Train Epoch: 323 [47616/225000 (21%)] Loss: 17012.591797\n",
      "Train Epoch: 323 [50112/225000 (22%)] Loss: 16860.458984\n",
      "Train Epoch: 323 [52608/225000 (23%)] Loss: 17083.224609\n",
      "Train Epoch: 323 [55104/225000 (24%)] Loss: 16705.562500\n",
      "Train Epoch: 323 [57600/225000 (26%)] Loss: 17293.279297\n",
      "Train Epoch: 323 [60096/225000 (27%)] Loss: 17086.835938\n",
      "Train Epoch: 323 [62592/225000 (28%)] Loss: 16611.335938\n",
      "Train Epoch: 323 [65088/225000 (29%)] Loss: 17084.167969\n",
      "Train Epoch: 323 [67584/225000 (30%)] Loss: 16415.113281\n",
      "Train Epoch: 323 [70080/225000 (31%)] Loss: 16930.671875\n",
      "Train Epoch: 323 [72576/225000 (32%)] Loss: 16559.242188\n",
      "Train Epoch: 323 [75072/225000 (33%)] Loss: 16919.871094\n",
      "Train Epoch: 323 [77568/225000 (34%)] Loss: 16718.742188\n",
      "Train Epoch: 323 [80064/225000 (36%)] Loss: 17233.570312\n",
      "Train Epoch: 323 [82560/225000 (37%)] Loss: 16884.187500\n",
      "Train Epoch: 323 [85056/225000 (38%)] Loss: 16763.400391\n",
      "Train Epoch: 323 [87552/225000 (39%)] Loss: 16852.039062\n",
      "Train Epoch: 323 [90048/225000 (40%)] Loss: 16954.044922\n",
      "Train Epoch: 323 [92544/225000 (41%)] Loss: 17115.687500\n",
      "Train Epoch: 323 [95040/225000 (42%)] Loss: 17121.230469\n",
      "Train Epoch: 323 [97536/225000 (43%)] Loss: 16967.328125\n",
      "Train Epoch: 323 [100032/225000 (44%)] Loss: 17294.949219\n",
      "Train Epoch: 323 [102528/225000 (46%)] Loss: 17306.367188\n",
      "Train Epoch: 323 [105024/225000 (47%)] Loss: 17110.197266\n",
      "Train Epoch: 323 [107520/225000 (48%)] Loss: 17011.951172\n",
      "Train Epoch: 323 [110016/225000 (49%)] Loss: 16545.595703\n",
      "Train Epoch: 323 [112512/225000 (50%)] Loss: 16658.515625\n",
      "Train Epoch: 323 [115008/225000 (51%)] Loss: 17043.406250\n",
      "Train Epoch: 323 [117504/225000 (52%)] Loss: 17061.859375\n",
      "Train Epoch: 323 [120000/225000 (53%)] Loss: 16699.542969\n",
      "Train Epoch: 323 [122496/225000 (54%)] Loss: 16738.863281\n",
      "Train Epoch: 323 [124992/225000 (56%)] Loss: 16304.126953\n",
      "Train Epoch: 323 [127488/225000 (57%)] Loss: 16403.914062\n",
      "Train Epoch: 323 [129984/225000 (58%)] Loss: 17208.224609\n",
      "Train Epoch: 323 [132480/225000 (59%)] Loss: 16365.751953\n",
      "Train Epoch: 323 [134976/225000 (60%)] Loss: 16796.015625\n",
      "Train Epoch: 323 [137472/225000 (61%)] Loss: 16983.625000\n",
      "Train Epoch: 323 [139968/225000 (62%)] Loss: 17191.781250\n",
      "Train Epoch: 323 [142464/225000 (63%)] Loss: 17297.720703\n",
      "Train Epoch: 323 [144960/225000 (64%)] Loss: 16626.871094\n",
      "Train Epoch: 323 [147456/225000 (66%)] Loss: 17075.718750\n",
      "Train Epoch: 323 [149952/225000 (67%)] Loss: 16686.804688\n",
      "Train Epoch: 323 [152448/225000 (68%)] Loss: 16271.418945\n",
      "Train Epoch: 323 [154944/225000 (69%)] Loss: 17168.144531\n",
      "Train Epoch: 323 [157440/225000 (70%)] Loss: 16367.488281\n",
      "Train Epoch: 323 [159936/225000 (71%)] Loss: 17014.265625\n",
      "Train Epoch: 323 [162432/225000 (72%)] Loss: 17217.289062\n",
      "Train Epoch: 323 [164928/225000 (73%)] Loss: 17000.562500\n",
      "Train Epoch: 323 [167424/225000 (74%)] Loss: 16821.664062\n",
      "Train Epoch: 323 [169920/225000 (76%)] Loss: 17171.312500\n",
      "Train Epoch: 323 [172416/225000 (77%)] Loss: 16824.082031\n",
      "Train Epoch: 323 [174912/225000 (78%)] Loss: 17116.869141\n",
      "Train Epoch: 323 [177408/225000 (79%)] Loss: 16803.750000\n",
      "Train Epoch: 323 [179904/225000 (80%)] Loss: 16975.175781\n",
      "Train Epoch: 323 [182400/225000 (81%)] Loss: 17018.832031\n",
      "Train Epoch: 323 [184896/225000 (82%)] Loss: 17263.589844\n",
      "Train Epoch: 323 [187392/225000 (83%)] Loss: 16946.417969\n",
      "Train Epoch: 323 [189888/225000 (84%)] Loss: 16517.001953\n",
      "Train Epoch: 323 [192384/225000 (86%)] Loss: 16820.664062\n",
      "Train Epoch: 323 [194880/225000 (87%)] Loss: 16666.912109\n",
      "Train Epoch: 323 [197376/225000 (88%)] Loss: 16805.652344\n",
      "Train Epoch: 323 [199872/225000 (89%)] Loss: 16968.527344\n",
      "Train Epoch: 323 [202368/225000 (90%)] Loss: 16903.082031\n",
      "Train Epoch: 323 [204864/225000 (91%)] Loss: 17070.972656\n",
      "Train Epoch: 323 [207360/225000 (92%)] Loss: 16894.601562\n",
      "Train Epoch: 323 [209856/225000 (93%)] Loss: 17035.085938\n",
      "Train Epoch: 323 [212352/225000 (94%)] Loss: 16769.351562\n",
      "Train Epoch: 323 [214848/225000 (95%)] Loss: 16924.394531\n",
      "Train Epoch: 323 [217344/225000 (97%)] Loss: 16851.515625\n",
      "Train Epoch: 323 [219840/225000 (98%)] Loss: 16569.169922\n",
      "Train Epoch: 323 [222336/225000 (99%)] Loss: 17075.085938\n",
      "Train Epoch: 323 [224832/225000 (100%)] Loss: 17217.388672\n",
      "    epoch          : 323\n",
      "    loss           : 17000.205755552743\n",
      "    val_loss       : 16868.45582002265\n",
      "Train Epoch: 324 [192/225000 (0%)] Loss: 16574.765625\n",
      "Train Epoch: 324 [2688/225000 (1%)] Loss: 17426.359375\n",
      "Train Epoch: 324 [5184/225000 (2%)] Loss: 16403.451172\n",
      "Train Epoch: 324 [7680/225000 (3%)] Loss: 16825.771484\n",
      "Train Epoch: 324 [10176/225000 (5%)] Loss: 19095.695312\n",
      "Train Epoch: 324 [12672/225000 (6%)] Loss: 16703.810547\n",
      "Train Epoch: 324 [15168/225000 (7%)] Loss: 18658.722656\n",
      "Train Epoch: 324 [17664/225000 (8%)] Loss: 16582.894531\n",
      "Train Epoch: 324 [20160/225000 (9%)] Loss: 17507.835938\n",
      "Train Epoch: 324 [22656/225000 (10%)] Loss: 17260.689453\n",
      "Train Epoch: 324 [25152/225000 (11%)] Loss: 17138.917969\n",
      "Train Epoch: 324 [27648/225000 (12%)] Loss: 18533.531250\n",
      "Train Epoch: 324 [30144/225000 (13%)] Loss: 16414.707031\n",
      "Train Epoch: 324 [32640/225000 (15%)] Loss: 17087.294922\n",
      "Train Epoch: 324 [35136/225000 (16%)] Loss: 17414.542969\n",
      "Train Epoch: 324 [37632/225000 (17%)] Loss: 17025.138672\n",
      "Train Epoch: 324 [40128/225000 (18%)] Loss: 16876.613281\n",
      "Train Epoch: 324 [42624/225000 (19%)] Loss: 17008.240234\n",
      "Train Epoch: 324 [45120/225000 (20%)] Loss: 16701.937500\n",
      "Train Epoch: 324 [47616/225000 (21%)] Loss: 16967.843750\n",
      "Train Epoch: 324 [50112/225000 (22%)] Loss: 17060.652344\n",
      "Train Epoch: 324 [52608/225000 (23%)] Loss: 16441.734375\n",
      "Train Epoch: 324 [55104/225000 (24%)] Loss: 17112.187500\n",
      "Train Epoch: 324 [57600/225000 (26%)] Loss: 16881.765625\n",
      "Train Epoch: 324 [60096/225000 (27%)] Loss: 17336.060547\n",
      "Train Epoch: 324 [62592/225000 (28%)] Loss: 16932.894531\n",
      "Train Epoch: 324 [65088/225000 (29%)] Loss: 16689.910156\n",
      "Train Epoch: 324 [67584/225000 (30%)] Loss: 17188.429688\n",
      "Train Epoch: 324 [70080/225000 (31%)] Loss: 16484.011719\n",
      "Train Epoch: 324 [72576/225000 (32%)] Loss: 17060.550781\n",
      "Train Epoch: 324 [75072/225000 (33%)] Loss: 16880.425781\n",
      "Train Epoch: 324 [77568/225000 (34%)] Loss: 17170.460938\n",
      "Train Epoch: 324 [80064/225000 (36%)] Loss: 16518.917969\n",
      "Train Epoch: 324 [82560/225000 (37%)] Loss: 16953.589844\n",
      "Train Epoch: 324 [85056/225000 (38%)] Loss: 16922.031250\n",
      "Train Epoch: 324 [87552/225000 (39%)] Loss: 17202.464844\n",
      "Train Epoch: 324 [90048/225000 (40%)] Loss: 17073.015625\n",
      "Train Epoch: 324 [92544/225000 (41%)] Loss: 16922.742188\n",
      "Train Epoch: 324 [95040/225000 (42%)] Loss: 16894.802734\n",
      "Train Epoch: 324 [97536/225000 (43%)] Loss: 17245.783203\n",
      "Train Epoch: 324 [100032/225000 (44%)] Loss: 17376.531250\n",
      "Train Epoch: 324 [102528/225000 (46%)] Loss: 16923.085938\n",
      "Train Epoch: 324 [105024/225000 (47%)] Loss: 17035.222656\n",
      "Train Epoch: 324 [107520/225000 (48%)] Loss: 17217.849609\n",
      "Train Epoch: 324 [110016/225000 (49%)] Loss: 16560.574219\n",
      "Train Epoch: 324 [112512/225000 (50%)] Loss: 17352.183594\n",
      "Train Epoch: 324 [115008/225000 (51%)] Loss: 16879.183594\n",
      "Train Epoch: 324 [117504/225000 (52%)] Loss: 16684.003906\n",
      "Train Epoch: 324 [120000/225000 (53%)] Loss: 16407.619141\n",
      "Train Epoch: 324 [122496/225000 (54%)] Loss: 17160.105469\n",
      "Train Epoch: 324 [124992/225000 (56%)] Loss: 16577.925781\n",
      "Train Epoch: 324 [127488/225000 (57%)] Loss: 16847.367188\n",
      "Train Epoch: 324 [129984/225000 (58%)] Loss: 16677.804688\n",
      "Train Epoch: 324 [132480/225000 (59%)] Loss: 17291.742188\n",
      "Train Epoch: 324 [134976/225000 (60%)] Loss: 16830.138672\n",
      "Train Epoch: 324 [137472/225000 (61%)] Loss: 16543.164062\n",
      "Train Epoch: 324 [139968/225000 (62%)] Loss: 16878.400391\n",
      "Train Epoch: 324 [142464/225000 (63%)] Loss: 16542.585938\n",
      "Train Epoch: 324 [144960/225000 (64%)] Loss: 17358.136719\n",
      "Train Epoch: 324 [147456/225000 (66%)] Loss: 16918.535156\n",
      "Train Epoch: 324 [149952/225000 (67%)] Loss: 16875.253906\n",
      "Train Epoch: 324 [152448/225000 (68%)] Loss: 16947.046875\n",
      "Train Epoch: 324 [154944/225000 (69%)] Loss: 16649.710938\n",
      "Train Epoch: 324 [157440/225000 (70%)] Loss: 16779.011719\n",
      "Train Epoch: 324 [159936/225000 (71%)] Loss: 17200.792969\n",
      "Train Epoch: 324 [162432/225000 (72%)] Loss: 16745.675781\n",
      "Train Epoch: 324 [164928/225000 (73%)] Loss: 16976.488281\n",
      "Train Epoch: 324 [167424/225000 (74%)] Loss: 17109.138672\n",
      "Train Epoch: 324 [169920/225000 (76%)] Loss: 17022.859375\n",
      "Train Epoch: 324 [172416/225000 (77%)] Loss: 16893.376953\n",
      "Train Epoch: 324 [174912/225000 (78%)] Loss: 17353.601562\n",
      "Train Epoch: 324 [177408/225000 (79%)] Loss: 17623.933594\n",
      "Train Epoch: 324 [179904/225000 (80%)] Loss: 16911.601562\n",
      "Train Epoch: 324 [182400/225000 (81%)] Loss: 17178.320312\n",
      "Train Epoch: 324 [184896/225000 (82%)] Loss: 17128.560547\n",
      "Train Epoch: 324 [187392/225000 (83%)] Loss: 16931.312500\n",
      "Train Epoch: 324 [189888/225000 (84%)] Loss: 17587.292969\n",
      "Train Epoch: 324 [192384/225000 (86%)] Loss: 17119.917969\n",
      "Train Epoch: 324 [194880/225000 (87%)] Loss: 17548.203125\n",
      "Train Epoch: 324 [197376/225000 (88%)] Loss: 17091.763672\n",
      "Train Epoch: 324 [199872/225000 (89%)] Loss: 17107.431641\n",
      "Train Epoch: 324 [202368/225000 (90%)] Loss: 16692.685547\n",
      "Train Epoch: 324 [204864/225000 (91%)] Loss: 16464.281250\n",
      "Train Epoch: 324 [207360/225000 (92%)] Loss: 17068.320312\n",
      "Train Epoch: 324 [209856/225000 (93%)] Loss: 17133.625000\n",
      "Train Epoch: 324 [212352/225000 (94%)] Loss: 16749.273438\n",
      "Train Epoch: 324 [214848/225000 (95%)] Loss: 16733.720703\n",
      "Train Epoch: 324 [217344/225000 (97%)] Loss: 17100.515625\n",
      "Train Epoch: 324 [219840/225000 (98%)] Loss: 17330.902344\n",
      "Train Epoch: 324 [222336/225000 (99%)] Loss: 16984.652344\n",
      "Train Epoch: 324 [224832/225000 (100%)] Loss: 16859.382812\n",
      "    epoch          : 324\n",
      "    loss           : 16999.83020144518\n",
      "    val_loss       : 16906.571666207023\n",
      "Train Epoch: 325 [192/225000 (0%)] Loss: 16969.089844\n",
      "Train Epoch: 325 [2688/225000 (1%)] Loss: 17314.621094\n",
      "Train Epoch: 325 [5184/225000 (2%)] Loss: 16787.074219\n",
      "Train Epoch: 325 [7680/225000 (3%)] Loss: 16832.349609\n",
      "Train Epoch: 325 [10176/225000 (5%)] Loss: 16616.628906\n",
      "Train Epoch: 325 [12672/225000 (6%)] Loss: 16575.031250\n",
      "Train Epoch: 325 [15168/225000 (7%)] Loss: 16590.703125\n",
      "Train Epoch: 325 [17664/225000 (8%)] Loss: 16601.289062\n",
      "Train Epoch: 325 [20160/225000 (9%)] Loss: 16918.351562\n",
      "Train Epoch: 325 [22656/225000 (10%)] Loss: 18962.679688\n",
      "Train Epoch: 325 [25152/225000 (11%)] Loss: 16802.746094\n",
      "Train Epoch: 325 [27648/225000 (12%)] Loss: 17055.562500\n",
      "Train Epoch: 325 [30144/225000 (13%)] Loss: 16737.472656\n",
      "Train Epoch: 325 [32640/225000 (15%)] Loss: 16942.289062\n",
      "Train Epoch: 325 [35136/225000 (16%)] Loss: 18337.285156\n",
      "Train Epoch: 325 [37632/225000 (17%)] Loss: 17163.767578\n",
      "Train Epoch: 325 [40128/225000 (18%)] Loss: 16569.539062\n",
      "Train Epoch: 325 [42624/225000 (19%)] Loss: 17259.144531\n",
      "Train Epoch: 325 [45120/225000 (20%)] Loss: 16956.718750\n",
      "Train Epoch: 325 [47616/225000 (21%)] Loss: 16769.976562\n",
      "Train Epoch: 325 [50112/225000 (22%)] Loss: 17050.572266\n",
      "Train Epoch: 325 [52608/225000 (23%)] Loss: 17159.273438\n",
      "Train Epoch: 325 [55104/225000 (24%)] Loss: 16960.476562\n",
      "Train Epoch: 325 [57600/225000 (26%)] Loss: 17162.433594\n",
      "Train Epoch: 325 [60096/225000 (27%)] Loss: 17338.378906\n",
      "Train Epoch: 325 [62592/225000 (28%)] Loss: 16732.335938\n",
      "Train Epoch: 325 [65088/225000 (29%)] Loss: 16865.222656\n",
      "Train Epoch: 325 [67584/225000 (30%)] Loss: 16774.785156\n",
      "Train Epoch: 325 [70080/225000 (31%)] Loss: 17145.400391\n",
      "Train Epoch: 325 [72576/225000 (32%)] Loss: 17574.744141\n",
      "Train Epoch: 325 [75072/225000 (33%)] Loss: 16204.525391\n",
      "Train Epoch: 325 [77568/225000 (34%)] Loss: 17102.769531\n",
      "Train Epoch: 325 [80064/225000 (36%)] Loss: 16744.238281\n",
      "Train Epoch: 325 [82560/225000 (37%)] Loss: 16341.835938\n",
      "Train Epoch: 325 [85056/225000 (38%)] Loss: 17064.257812\n",
      "Train Epoch: 325 [87552/225000 (39%)] Loss: 16933.427734\n",
      "Train Epoch: 325 [90048/225000 (40%)] Loss: 16499.607422\n",
      "Train Epoch: 325 [92544/225000 (41%)] Loss: 16598.765625\n",
      "Train Epoch: 325 [95040/225000 (42%)] Loss: 16821.644531\n",
      "Train Epoch: 325 [97536/225000 (43%)] Loss: 16569.689453\n",
      "Train Epoch: 325 [100032/225000 (44%)] Loss: 16479.027344\n",
      "Train Epoch: 325 [102528/225000 (46%)] Loss: 17051.117188\n",
      "Train Epoch: 325 [105024/225000 (47%)] Loss: 17035.855469\n",
      "Train Epoch: 325 [107520/225000 (48%)] Loss: 17098.732422\n",
      "Train Epoch: 325 [110016/225000 (49%)] Loss: 17320.542969\n",
      "Train Epoch: 325 [112512/225000 (50%)] Loss: 16522.000000\n",
      "Train Epoch: 325 [115008/225000 (51%)] Loss: 16706.468750\n",
      "Train Epoch: 325 [117504/225000 (52%)] Loss: 17220.890625\n",
      "Train Epoch: 325 [120000/225000 (53%)] Loss: 17181.722656\n",
      "Train Epoch: 325 [122496/225000 (54%)] Loss: 17290.000000\n",
      "Train Epoch: 325 [124992/225000 (56%)] Loss: 17218.898438\n",
      "Train Epoch: 325 [127488/225000 (57%)] Loss: 16939.937500\n",
      "Train Epoch: 325 [129984/225000 (58%)] Loss: 16901.832031\n",
      "Train Epoch: 325 [132480/225000 (59%)] Loss: 17110.484375\n",
      "Train Epoch: 325 [134976/225000 (60%)] Loss: 17099.445312\n",
      "Train Epoch: 325 [137472/225000 (61%)] Loss: 16542.613281\n",
      "Train Epoch: 325 [139968/225000 (62%)] Loss: 16513.210938\n",
      "Train Epoch: 325 [142464/225000 (63%)] Loss: 16441.695312\n",
      "Train Epoch: 325 [144960/225000 (64%)] Loss: 16891.617188\n",
      "Train Epoch: 325 [147456/225000 (66%)] Loss: 16852.535156\n",
      "Train Epoch: 325 [149952/225000 (67%)] Loss: 17171.488281\n",
      "Train Epoch: 325 [152448/225000 (68%)] Loss: 16434.031250\n",
      "Train Epoch: 325 [154944/225000 (69%)] Loss: 17011.652344\n",
      "Train Epoch: 325 [157440/225000 (70%)] Loss: 17147.115234\n",
      "Train Epoch: 325 [159936/225000 (71%)] Loss: 17008.558594\n",
      "Train Epoch: 325 [162432/225000 (72%)] Loss: 16628.775391\n",
      "Train Epoch: 325 [164928/225000 (73%)] Loss: 16972.335938\n",
      "Train Epoch: 325 [167424/225000 (74%)] Loss: 16648.607422\n",
      "Train Epoch: 325 [169920/225000 (76%)] Loss: 17360.968750\n",
      "Train Epoch: 325 [172416/225000 (77%)] Loss: 16656.695312\n",
      "Train Epoch: 325 [174912/225000 (78%)] Loss: 16962.167969\n",
      "Train Epoch: 325 [177408/225000 (79%)] Loss: 17203.626953\n",
      "Train Epoch: 325 [179904/225000 (80%)] Loss: 16550.478516\n",
      "Train Epoch: 325 [182400/225000 (81%)] Loss: 17191.945312\n",
      "Train Epoch: 325 [184896/225000 (82%)] Loss: 16515.617188\n",
      "Train Epoch: 325 [187392/225000 (83%)] Loss: 16886.828125\n",
      "Train Epoch: 325 [189888/225000 (84%)] Loss: 17223.761719\n",
      "Train Epoch: 325 [192384/225000 (86%)] Loss: 17337.904297\n",
      "Train Epoch: 325 [194880/225000 (87%)] Loss: 16595.910156\n",
      "Train Epoch: 325 [197376/225000 (88%)] Loss: 17340.683594\n",
      "Train Epoch: 325 [199872/225000 (89%)] Loss: 16779.001953\n",
      "Train Epoch: 325 [202368/225000 (90%)] Loss: 16865.708984\n",
      "Train Epoch: 325 [204864/225000 (91%)] Loss: 16733.867188\n",
      "Train Epoch: 325 [207360/225000 (92%)] Loss: 16723.162109\n",
      "Train Epoch: 325 [209856/225000 (93%)] Loss: 16856.908203\n",
      "Train Epoch: 325 [212352/225000 (94%)] Loss: 16969.123047\n",
      "Train Epoch: 325 [214848/225000 (95%)] Loss: 16516.826172\n",
      "Train Epoch: 325 [217344/225000 (97%)] Loss: 16938.773438\n",
      "Train Epoch: 325 [219840/225000 (98%)] Loss: 18472.753906\n",
      "Train Epoch: 325 [222336/225000 (99%)] Loss: 16946.691406\n",
      "Train Epoch: 325 [224832/225000 (100%)] Loss: 17080.847656\n",
      "    epoch          : 325\n",
      "    loss           : 16969.321943159397\n",
      "    val_loss       : 16910.71930258465\n",
      "Train Epoch: 326 [192/225000 (0%)] Loss: 17297.921875\n",
      "Train Epoch: 326 [2688/225000 (1%)] Loss: 16909.085938\n",
      "Train Epoch: 326 [5184/225000 (2%)] Loss: 17066.394531\n",
      "Train Epoch: 326 [7680/225000 (3%)] Loss: 16614.052734\n",
      "Train Epoch: 326 [10176/225000 (5%)] Loss: 17026.880859\n",
      "Train Epoch: 326 [12672/225000 (6%)] Loss: 16717.761719\n",
      "Train Epoch: 326 [15168/225000 (7%)] Loss: 16980.234375\n",
      "Train Epoch: 326 [17664/225000 (8%)] Loss: 16890.242188\n",
      "Train Epoch: 326 [20160/225000 (9%)] Loss: 16790.140625\n",
      "Train Epoch: 326 [22656/225000 (10%)] Loss: 16942.521484\n",
      "Train Epoch: 326 [25152/225000 (11%)] Loss: 17227.757812\n",
      "Train Epoch: 326 [27648/225000 (12%)] Loss: 17138.845703\n",
      "Train Epoch: 326 [30144/225000 (13%)] Loss: 17133.867188\n",
      "Train Epoch: 326 [32640/225000 (15%)] Loss: 16838.707031\n",
      "Train Epoch: 326 [35136/225000 (16%)] Loss: 16905.265625\n",
      "Train Epoch: 326 [37632/225000 (17%)] Loss: 16693.371094\n",
      "Train Epoch: 326 [40128/225000 (18%)] Loss: 17001.640625\n",
      "Train Epoch: 326 [42624/225000 (19%)] Loss: 17082.658203\n",
      "Train Epoch: 326 [45120/225000 (20%)] Loss: 18431.191406\n",
      "Train Epoch: 326 [47616/225000 (21%)] Loss: 16903.023438\n",
      "Train Epoch: 326 [50112/225000 (22%)] Loss: 17109.716797\n",
      "Train Epoch: 326 [52608/225000 (23%)] Loss: 16977.892578\n",
      "Train Epoch: 326 [55104/225000 (24%)] Loss: 16692.431641\n",
      "Train Epoch: 326 [57600/225000 (26%)] Loss: 16831.097656\n",
      "Train Epoch: 326 [60096/225000 (27%)] Loss: 16275.436523\n",
      "Train Epoch: 326 [62592/225000 (28%)] Loss: 16577.800781\n",
      "Train Epoch: 326 [65088/225000 (29%)] Loss: 17433.562500\n",
      "Train Epoch: 326 [67584/225000 (30%)] Loss: 16796.261719\n",
      "Train Epoch: 326 [70080/225000 (31%)] Loss: 16678.351562\n",
      "Train Epoch: 326 [72576/225000 (32%)] Loss: 17498.037109\n",
      "Train Epoch: 326 [75072/225000 (33%)] Loss: 16706.652344\n",
      "Train Epoch: 326 [77568/225000 (34%)] Loss: 16729.933594\n",
      "Train Epoch: 326 [80064/225000 (36%)] Loss: 17151.250000\n",
      "Train Epoch: 326 [82560/225000 (37%)] Loss: 16937.783203\n",
      "Train Epoch: 326 [85056/225000 (38%)] Loss: 17163.160156\n",
      "Train Epoch: 326 [87552/225000 (39%)] Loss: 17221.503906\n",
      "Train Epoch: 326 [90048/225000 (40%)] Loss: 16968.398438\n",
      "Train Epoch: 326 [92544/225000 (41%)] Loss: 17238.701172\n",
      "Train Epoch: 326 [95040/225000 (42%)] Loss: 17257.388672\n",
      "Train Epoch: 326 [97536/225000 (43%)] Loss: 17233.291016\n",
      "Train Epoch: 326 [100032/225000 (44%)] Loss: 16987.855469\n",
      "Train Epoch: 326 [102528/225000 (46%)] Loss: 17246.566406\n",
      "Train Epoch: 326 [105024/225000 (47%)] Loss: 16693.931641\n",
      "Train Epoch: 326 [107520/225000 (48%)] Loss: 16592.552734\n",
      "Train Epoch: 326 [110016/225000 (49%)] Loss: 16658.082031\n",
      "Train Epoch: 326 [112512/225000 (50%)] Loss: 16399.201172\n",
      "Train Epoch: 326 [115008/225000 (51%)] Loss: 17443.386719\n",
      "Train Epoch: 326 [117504/225000 (52%)] Loss: 16523.445312\n",
      "Train Epoch: 326 [120000/225000 (53%)] Loss: 17375.640625\n",
      "Train Epoch: 326 [122496/225000 (54%)] Loss: 16947.406250\n",
      "Train Epoch: 326 [124992/225000 (56%)] Loss: 16585.367188\n",
      "Train Epoch: 326 [127488/225000 (57%)] Loss: 17438.763672\n",
      "Train Epoch: 326 [129984/225000 (58%)] Loss: 16570.283203\n",
      "Train Epoch: 326 [132480/225000 (59%)] Loss: 16760.873047\n",
      "Train Epoch: 326 [134976/225000 (60%)] Loss: 16873.648438\n",
      "Train Epoch: 326 [137472/225000 (61%)] Loss: 17343.498047\n",
      "Train Epoch: 326 [139968/225000 (62%)] Loss: 17047.230469\n",
      "Train Epoch: 326 [142464/225000 (63%)] Loss: 17257.814453\n",
      "Train Epoch: 326 [144960/225000 (64%)] Loss: 16902.236328\n",
      "Train Epoch: 326 [147456/225000 (66%)] Loss: 16675.046875\n",
      "Train Epoch: 326 [149952/225000 (67%)] Loss: 16992.449219\n",
      "Train Epoch: 326 [152448/225000 (68%)] Loss: 16862.718750\n",
      "Train Epoch: 326 [154944/225000 (69%)] Loss: 16909.255859\n",
      "Train Epoch: 326 [157440/225000 (70%)] Loss: 17161.804688\n",
      "Train Epoch: 326 [159936/225000 (71%)] Loss: 16586.462891\n",
      "Train Epoch: 326 [162432/225000 (72%)] Loss: 16848.621094\n",
      "Train Epoch: 326 [164928/225000 (73%)] Loss: 17097.382812\n",
      "Train Epoch: 326 [167424/225000 (74%)] Loss: 16579.664062\n",
      "Train Epoch: 326 [169920/225000 (76%)] Loss: 16728.261719\n",
      "Train Epoch: 326 [172416/225000 (77%)] Loss: 16750.472656\n",
      "Train Epoch: 326 [174912/225000 (78%)] Loss: 16699.398438\n",
      "Train Epoch: 326 [177408/225000 (79%)] Loss: 17334.826172\n",
      "Train Epoch: 326 [179904/225000 (80%)] Loss: 16777.546875\n",
      "Train Epoch: 326 [182400/225000 (81%)] Loss: 16896.503906\n",
      "Train Epoch: 326 [184896/225000 (82%)] Loss: 16720.087891\n",
      "Train Epoch: 326 [187392/225000 (83%)] Loss: 17655.595703\n",
      "Train Epoch: 326 [189888/225000 (84%)] Loss: 16395.312500\n",
      "Train Epoch: 326 [192384/225000 (86%)] Loss: 16523.955078\n",
      "Train Epoch: 326 [194880/225000 (87%)] Loss: 16971.089844\n",
      "Train Epoch: 326 [197376/225000 (88%)] Loss: 17027.089844\n",
      "Train Epoch: 326 [199872/225000 (89%)] Loss: 16867.349609\n",
      "Train Epoch: 326 [202368/225000 (90%)] Loss: 17004.289062\n",
      "Train Epoch: 326 [204864/225000 (91%)] Loss: 16910.451172\n",
      "Train Epoch: 326 [207360/225000 (92%)] Loss: 16949.230469\n",
      "Train Epoch: 326 [209856/225000 (93%)] Loss: 16856.812500\n",
      "Train Epoch: 326 [212352/225000 (94%)] Loss: 16887.136719\n",
      "Train Epoch: 326 [214848/225000 (95%)] Loss: 17115.568359\n",
      "Train Epoch: 326 [217344/225000 (97%)] Loss: 17104.429688\n",
      "Train Epoch: 326 [219840/225000 (98%)] Loss: 16672.585938\n",
      "Train Epoch: 326 [222336/225000 (99%)] Loss: 16824.027344\n",
      "Train Epoch: 326 [224832/225000 (100%)] Loss: 16581.390625\n",
      "    epoch          : 326\n",
      "    loss           : 16938.037269357934\n",
      "    val_loss       : 16860.341418823213\n",
      "Train Epoch: 327 [192/225000 (0%)] Loss: 17122.921875\n",
      "Train Epoch: 327 [2688/225000 (1%)] Loss: 16697.929688\n",
      "Train Epoch: 327 [5184/225000 (2%)] Loss: 16656.871094\n",
      "Train Epoch: 327 [7680/225000 (3%)] Loss: 18067.134766\n",
      "Train Epoch: 327 [10176/225000 (5%)] Loss: 16615.085938\n",
      "Train Epoch: 327 [12672/225000 (6%)] Loss: 16937.429688\n",
      "Train Epoch: 327 [15168/225000 (7%)] Loss: 16836.460938\n",
      "Train Epoch: 327 [17664/225000 (8%)] Loss: 16622.222656\n",
      "Train Epoch: 327 [20160/225000 (9%)] Loss: 16855.710938\n",
      "Train Epoch: 327 [22656/225000 (10%)] Loss: 17374.623047\n",
      "Train Epoch: 327 [25152/225000 (11%)] Loss: 16505.574219\n",
      "Train Epoch: 327 [27648/225000 (12%)] Loss: 17019.828125\n",
      "Train Epoch: 327 [30144/225000 (13%)] Loss: 16806.619141\n",
      "Train Epoch: 327 [32640/225000 (15%)] Loss: 16715.142578\n",
      "Train Epoch: 327 [35136/225000 (16%)] Loss: 16763.449219\n",
      "Train Epoch: 327 [37632/225000 (17%)] Loss: 17186.732422\n",
      "Train Epoch: 327 [40128/225000 (18%)] Loss: 16587.613281\n",
      "Train Epoch: 327 [42624/225000 (19%)] Loss: 16662.402344\n",
      "Train Epoch: 327 [45120/225000 (20%)] Loss: 16551.445312\n",
      "Train Epoch: 327 [47616/225000 (21%)] Loss: 16815.121094\n",
      "Train Epoch: 327 [50112/225000 (22%)] Loss: 16750.253906\n",
      "Train Epoch: 327 [52608/225000 (23%)] Loss: 16687.277344\n",
      "Train Epoch: 327 [55104/225000 (24%)] Loss: 16726.621094\n",
      "Train Epoch: 327 [57600/225000 (26%)] Loss: 16956.220703\n",
      "Train Epoch: 327 [60096/225000 (27%)] Loss: 16725.320312\n",
      "Train Epoch: 327 [62592/225000 (28%)] Loss: 17136.429688\n",
      "Train Epoch: 327 [65088/225000 (29%)] Loss: 16661.664062\n",
      "Train Epoch: 327 [67584/225000 (30%)] Loss: 16755.021484\n",
      "Train Epoch: 327 [70080/225000 (31%)] Loss: 16542.695312\n",
      "Train Epoch: 327 [72576/225000 (32%)] Loss: 16776.380859\n",
      "Train Epoch: 327 [75072/225000 (33%)] Loss: 16494.511719\n",
      "Train Epoch: 327 [77568/225000 (34%)] Loss: 17044.652344\n",
      "Train Epoch: 327 [80064/225000 (36%)] Loss: 17103.761719\n",
      "Train Epoch: 327 [82560/225000 (37%)] Loss: 16974.052734\n",
      "Train Epoch: 327 [85056/225000 (38%)] Loss: 17188.892578\n",
      "Train Epoch: 327 [87552/225000 (39%)] Loss: 16689.248047\n",
      "Train Epoch: 327 [90048/225000 (40%)] Loss: 16880.646484\n",
      "Train Epoch: 327 [92544/225000 (41%)] Loss: 18842.132812\n",
      "Train Epoch: 327 [95040/225000 (42%)] Loss: 16887.544922\n",
      "Train Epoch: 327 [97536/225000 (43%)] Loss: 18732.156250\n",
      "Train Epoch: 327 [100032/225000 (44%)] Loss: 17251.273438\n",
      "Train Epoch: 327 [102528/225000 (46%)] Loss: 17077.195312\n",
      "Train Epoch: 327 [105024/225000 (47%)] Loss: 16950.095703\n",
      "Train Epoch: 327 [107520/225000 (48%)] Loss: 16699.992188\n",
      "Train Epoch: 327 [110016/225000 (49%)] Loss: 17283.199219\n",
      "Train Epoch: 327 [112512/225000 (50%)] Loss: 16955.138672\n",
      "Train Epoch: 327 [115008/225000 (51%)] Loss: 17361.757812\n",
      "Train Epoch: 327 [117504/225000 (52%)] Loss: 16238.550781\n",
      "Train Epoch: 327 [120000/225000 (53%)] Loss: 17294.718750\n",
      "Train Epoch: 327 [122496/225000 (54%)] Loss: 17131.333984\n",
      "Train Epoch: 327 [124992/225000 (56%)] Loss: 16433.226562\n",
      "Train Epoch: 327 [127488/225000 (57%)] Loss: 16814.337891\n",
      "Train Epoch: 327 [129984/225000 (58%)] Loss: 16755.656250\n",
      "Train Epoch: 327 [132480/225000 (59%)] Loss: 16619.513672\n",
      "Train Epoch: 327 [134976/225000 (60%)] Loss: 16752.851562\n",
      "Train Epoch: 327 [137472/225000 (61%)] Loss: 17267.453125\n",
      "Train Epoch: 327 [139968/225000 (62%)] Loss: 16913.919922\n",
      "Train Epoch: 327 [142464/225000 (63%)] Loss: 17048.427734\n",
      "Train Epoch: 327 [144960/225000 (64%)] Loss: 16858.839844\n",
      "Train Epoch: 327 [147456/225000 (66%)] Loss: 17417.566406\n",
      "Train Epoch: 327 [149952/225000 (67%)] Loss: 16476.835938\n",
      "Train Epoch: 327 [152448/225000 (68%)] Loss: 17012.859375\n",
      "Train Epoch: 327 [154944/225000 (69%)] Loss: 16324.094727\n",
      "Train Epoch: 327 [157440/225000 (70%)] Loss: 16956.044922\n",
      "Train Epoch: 327 [159936/225000 (71%)] Loss: 18264.910156\n",
      "Train Epoch: 327 [162432/225000 (72%)] Loss: 17042.562500\n",
      "Train Epoch: 327 [164928/225000 (73%)] Loss: 16829.207031\n",
      "Train Epoch: 327 [167424/225000 (74%)] Loss: 16926.410156\n",
      "Train Epoch: 327 [169920/225000 (76%)] Loss: 17069.511719\n",
      "Train Epoch: 327 [172416/225000 (77%)] Loss: 17240.466797\n",
      "Train Epoch: 327 [174912/225000 (78%)] Loss: 16440.210938\n",
      "Train Epoch: 327 [177408/225000 (79%)] Loss: 16481.968750\n",
      "Train Epoch: 327 [179904/225000 (80%)] Loss: 16373.705078\n",
      "Train Epoch: 327 [182400/225000 (81%)] Loss: 16956.828125\n",
      "Train Epoch: 327 [184896/225000 (82%)] Loss: 16958.042969\n",
      "Train Epoch: 327 [187392/225000 (83%)] Loss: 16254.192383\n",
      "Train Epoch: 327 [189888/225000 (84%)] Loss: 17062.253906\n",
      "Train Epoch: 327 [192384/225000 (86%)] Loss: 16841.849609\n",
      "Train Epoch: 327 [194880/225000 (87%)] Loss: 17029.671875\n",
      "Train Epoch: 327 [197376/225000 (88%)] Loss: 16812.527344\n",
      "Train Epoch: 327 [199872/225000 (89%)] Loss: 16427.472656\n",
      "Train Epoch: 327 [202368/225000 (90%)] Loss: 16854.976562\n",
      "Train Epoch: 327 [204864/225000 (91%)] Loss: 16896.648438\n",
      "Train Epoch: 327 [207360/225000 (92%)] Loss: 16969.556641\n",
      "Train Epoch: 327 [209856/225000 (93%)] Loss: 16849.042969\n",
      "Train Epoch: 327 [212352/225000 (94%)] Loss: 16464.046875\n",
      "Train Epoch: 327 [214848/225000 (95%)] Loss: 16397.183594\n",
      "Train Epoch: 327 [217344/225000 (97%)] Loss: 17155.056641\n",
      "Train Epoch: 327 [219840/225000 (98%)] Loss: 16770.626953\n",
      "Train Epoch: 327 [222336/225000 (99%)] Loss: 16588.703125\n",
      "Train Epoch: 327 [224832/225000 (100%)] Loss: 16758.500000\n",
      "    epoch          : 327\n",
      "    loss           : 16930.08678324312\n",
      "    val_loss       : 16850.822550043806\n",
      "Train Epoch: 328 [192/225000 (0%)] Loss: 16663.658203\n",
      "Train Epoch: 328 [2688/225000 (1%)] Loss: 17072.261719\n",
      "Train Epoch: 328 [5184/225000 (2%)] Loss: 16419.113281\n",
      "Train Epoch: 328 [7680/225000 (3%)] Loss: 16624.578125\n",
      "Train Epoch: 328 [10176/225000 (5%)] Loss: 17028.988281\n",
      "Train Epoch: 328 [12672/225000 (6%)] Loss: 16750.863281\n",
      "Train Epoch: 328 [15168/225000 (7%)] Loss: 16994.533203\n",
      "Train Epoch: 328 [17664/225000 (8%)] Loss: 17029.832031\n",
      "Train Epoch: 328 [20160/225000 (9%)] Loss: 16460.542969\n",
      "Train Epoch: 328 [22656/225000 (10%)] Loss: 16968.966797\n",
      "Train Epoch: 328 [25152/225000 (11%)] Loss: 16806.324219\n",
      "Train Epoch: 328 [27648/225000 (12%)] Loss: 17280.250000\n",
      "Train Epoch: 328 [30144/225000 (13%)] Loss: 16878.957031\n",
      "Train Epoch: 328 [32640/225000 (15%)] Loss: 16921.984375\n",
      "Train Epoch: 328 [35136/225000 (16%)] Loss: 17345.300781\n",
      "Train Epoch: 328 [37632/225000 (17%)] Loss: 16912.878906\n",
      "Train Epoch: 328 [40128/225000 (18%)] Loss: 16648.070312\n",
      "Train Epoch: 328 [42624/225000 (19%)] Loss: 16522.261719\n",
      "Train Epoch: 328 [45120/225000 (20%)] Loss: 18510.109375\n",
      "Train Epoch: 328 [47616/225000 (21%)] Loss: 17009.189453\n",
      "Train Epoch: 328 [50112/225000 (22%)] Loss: 16456.144531\n",
      "Train Epoch: 328 [52608/225000 (23%)] Loss: 17207.472656\n",
      "Train Epoch: 328 [55104/225000 (24%)] Loss: 16946.531250\n",
      "Train Epoch: 328 [57600/225000 (26%)] Loss: 16722.062500\n",
      "Train Epoch: 328 [60096/225000 (27%)] Loss: 16661.269531\n",
      "Train Epoch: 328 [62592/225000 (28%)] Loss: 16321.802734\n",
      "Train Epoch: 328 [65088/225000 (29%)] Loss: 16691.515625\n",
      "Train Epoch: 328 [67584/225000 (30%)] Loss: 16798.511719\n",
      "Train Epoch: 328 [70080/225000 (31%)] Loss: 16787.082031\n",
      "Train Epoch: 328 [72576/225000 (32%)] Loss: 16971.832031\n",
      "Train Epoch: 328 [75072/225000 (33%)] Loss: 16870.177734\n",
      "Train Epoch: 328 [77568/225000 (34%)] Loss: 16898.605469\n",
      "Train Epoch: 328 [80064/225000 (36%)] Loss: 16882.013672\n",
      "Train Epoch: 328 [82560/225000 (37%)] Loss: 16749.503906\n",
      "Train Epoch: 328 [85056/225000 (38%)] Loss: 16790.812500\n",
      "Train Epoch: 328 [87552/225000 (39%)] Loss: 16597.066406\n",
      "Train Epoch: 328 [90048/225000 (40%)] Loss: 16732.699219\n",
      "Train Epoch: 328 [92544/225000 (41%)] Loss: 18309.722656\n",
      "Train Epoch: 328 [95040/225000 (42%)] Loss: 16774.656250\n",
      "Train Epoch: 328 [97536/225000 (43%)] Loss: 17160.960938\n",
      "Train Epoch: 328 [100032/225000 (44%)] Loss: 16584.671875\n",
      "Train Epoch: 328 [102528/225000 (46%)] Loss: 16883.871094\n",
      "Train Epoch: 328 [105024/225000 (47%)] Loss: 16893.890625\n",
      "Train Epoch: 328 [107520/225000 (48%)] Loss: 17092.662109\n",
      "Train Epoch: 328 [110016/225000 (49%)] Loss: 16896.164062\n",
      "Train Epoch: 328 [112512/225000 (50%)] Loss: 16906.175781\n",
      "Train Epoch: 328 [115008/225000 (51%)] Loss: 17059.708984\n",
      "Train Epoch: 328 [117504/225000 (52%)] Loss: 16698.484375\n",
      "Train Epoch: 328 [120000/225000 (53%)] Loss: 18350.468750\n",
      "Train Epoch: 328 [122496/225000 (54%)] Loss: 17191.050781\n",
      "Train Epoch: 328 [124992/225000 (56%)] Loss: 17317.457031\n",
      "Train Epoch: 328 [127488/225000 (57%)] Loss: 16296.791992\n",
      "Train Epoch: 328 [129984/225000 (58%)] Loss: 16629.648438\n",
      "Train Epoch: 328 [132480/225000 (59%)] Loss: 16375.821289\n",
      "Train Epoch: 328 [134976/225000 (60%)] Loss: 16719.011719\n",
      "Train Epoch: 328 [137472/225000 (61%)] Loss: 17014.980469\n",
      "Train Epoch: 328 [139968/225000 (62%)] Loss: 17014.644531\n",
      "Train Epoch: 328 [142464/225000 (63%)] Loss: 16925.724609\n",
      "Train Epoch: 328 [144960/225000 (64%)] Loss: 16621.291016\n",
      "Train Epoch: 328 [147456/225000 (66%)] Loss: 16716.753906\n",
      "Train Epoch: 328 [149952/225000 (67%)] Loss: 16837.675781\n",
      "Train Epoch: 328 [152448/225000 (68%)] Loss: 16682.103516\n",
      "Train Epoch: 328 [154944/225000 (69%)] Loss: 17119.636719\n",
      "Train Epoch: 328 [157440/225000 (70%)] Loss: 16915.470703\n",
      "Train Epoch: 328 [159936/225000 (71%)] Loss: 16557.785156\n",
      "Train Epoch: 328 [162432/225000 (72%)] Loss: 16989.984375\n",
      "Train Epoch: 328 [164928/225000 (73%)] Loss: 17229.621094\n",
      "Train Epoch: 328 [167424/225000 (74%)] Loss: 17182.546875\n",
      "Train Epoch: 328 [169920/225000 (76%)] Loss: 16356.100586\n",
      "Train Epoch: 328 [172416/225000 (77%)] Loss: 16481.859375\n",
      "Train Epoch: 328 [174912/225000 (78%)] Loss: 16878.191406\n",
      "Train Epoch: 328 [177408/225000 (79%)] Loss: 16866.777344\n",
      "Train Epoch: 328 [179904/225000 (80%)] Loss: 17710.234375\n",
      "Train Epoch: 328 [182400/225000 (81%)] Loss: 16981.230469\n",
      "Train Epoch: 328 [184896/225000 (82%)] Loss: 17221.962891\n",
      "Train Epoch: 328 [187392/225000 (83%)] Loss: 16934.953125\n",
      "Train Epoch: 328 [189888/225000 (84%)] Loss: 16797.902344\n",
      "Train Epoch: 328 [192384/225000 (86%)] Loss: 16362.701172\n",
      "Train Epoch: 328 [194880/225000 (87%)] Loss: 16801.279297\n",
      "Train Epoch: 328 [197376/225000 (88%)] Loss: 16856.703125\n",
      "Train Epoch: 328 [199872/225000 (89%)] Loss: 17008.804688\n",
      "Train Epoch: 328 [202368/225000 (90%)] Loss: 16810.425781\n",
      "Train Epoch: 328 [204864/225000 (91%)] Loss: 17108.781250\n",
      "Train Epoch: 328 [207360/225000 (92%)] Loss: 16585.330078\n",
      "Train Epoch: 328 [209856/225000 (93%)] Loss: 16988.755859\n",
      "Train Epoch: 328 [212352/225000 (94%)] Loss: 16596.875000\n",
      "Train Epoch: 328 [214848/225000 (95%)] Loss: 17122.886719\n",
      "Train Epoch: 328 [217344/225000 (97%)] Loss: 16840.820312\n",
      "Train Epoch: 328 [219840/225000 (98%)] Loss: 17073.476562\n",
      "Train Epoch: 328 [222336/225000 (99%)] Loss: 17264.404297\n",
      "Train Epoch: 328 [224832/225000 (100%)] Loss: 16865.607422\n",
      "    epoch          : 328\n",
      "    loss           : 16901.203914082496\n",
      "    val_loss       : 16822.263082205794\n",
      "Train Epoch: 329 [192/225000 (0%)] Loss: 17070.777344\n",
      "Train Epoch: 329 [2688/225000 (1%)] Loss: 17193.304688\n",
      "Train Epoch: 329 [5184/225000 (2%)] Loss: 16426.457031\n",
      "Train Epoch: 329 [7680/225000 (3%)] Loss: 16767.867188\n",
      "Train Epoch: 329 [10176/225000 (5%)] Loss: 17238.687500\n",
      "Train Epoch: 329 [12672/225000 (6%)] Loss: 16525.318359\n",
      "Train Epoch: 329 [15168/225000 (7%)] Loss: 16647.593750\n",
      "Train Epoch: 329 [17664/225000 (8%)] Loss: 16211.247070\n",
      "Train Epoch: 329 [20160/225000 (9%)] Loss: 17075.583984\n",
      "Train Epoch: 329 [22656/225000 (10%)] Loss: 17126.667969\n",
      "Train Epoch: 329 [25152/225000 (11%)] Loss: 16224.576172\n",
      "Train Epoch: 329 [27648/225000 (12%)] Loss: 16892.156250\n",
      "Train Epoch: 329 [30144/225000 (13%)] Loss: 16906.390625\n",
      "Train Epoch: 329 [32640/225000 (15%)] Loss: 16529.734375\n",
      "Train Epoch: 329 [35136/225000 (16%)] Loss: 16692.437500\n",
      "Train Epoch: 329 [37632/225000 (17%)] Loss: 16609.406250\n",
      "Train Epoch: 329 [40128/225000 (18%)] Loss: 17105.906250\n",
      "Train Epoch: 329 [42624/225000 (19%)] Loss: 17108.488281\n",
      "Train Epoch: 329 [45120/225000 (20%)] Loss: 17079.957031\n",
      "Train Epoch: 329 [47616/225000 (21%)] Loss: 17407.253906\n",
      "Train Epoch: 329 [50112/225000 (22%)] Loss: 16536.472656\n",
      "Train Epoch: 329 [52608/225000 (23%)] Loss: 16955.412109\n",
      "Train Epoch: 329 [55104/225000 (24%)] Loss: 17113.824219\n",
      "Train Epoch: 329 [57600/225000 (26%)] Loss: 18300.744141\n",
      "Train Epoch: 329 [60096/225000 (27%)] Loss: 16514.480469\n",
      "Train Epoch: 329 [62592/225000 (28%)] Loss: 17111.828125\n",
      "Train Epoch: 329 [65088/225000 (29%)] Loss: 17348.503906\n",
      "Train Epoch: 329 [67584/225000 (30%)] Loss: 16951.406250\n",
      "Train Epoch: 329 [70080/225000 (31%)] Loss: 17320.324219\n",
      "Train Epoch: 329 [72576/225000 (32%)] Loss: 16940.628906\n",
      "Train Epoch: 329 [75072/225000 (33%)] Loss: 16891.367188\n",
      "Train Epoch: 329 [77568/225000 (34%)] Loss: 16358.098633\n",
      "Train Epoch: 329 [80064/225000 (36%)] Loss: 17414.427734\n",
      "Train Epoch: 329 [82560/225000 (37%)] Loss: 16492.207031\n",
      "Train Epoch: 329 [85056/225000 (38%)] Loss: 16683.664062\n",
      "Train Epoch: 329 [87552/225000 (39%)] Loss: 16643.796875\n",
      "Train Epoch: 329 [90048/225000 (40%)] Loss: 16650.859375\n",
      "Train Epoch: 329 [92544/225000 (41%)] Loss: 16620.378906\n",
      "Train Epoch: 329 [95040/225000 (42%)] Loss: 17127.255859\n",
      "Train Epoch: 329 [97536/225000 (43%)] Loss: 16599.332031\n",
      "Train Epoch: 329 [100032/225000 (44%)] Loss: 17028.554688\n",
      "Train Epoch: 329 [102528/225000 (46%)] Loss: 17009.654297\n",
      "Train Epoch: 329 [105024/225000 (47%)] Loss: 17353.011719\n",
      "Train Epoch: 329 [107520/225000 (48%)] Loss: 16559.917969\n",
      "Train Epoch: 329 [110016/225000 (49%)] Loss: 16939.867188\n",
      "Train Epoch: 329 [112512/225000 (50%)] Loss: 17175.218750\n",
      "Train Epoch: 329 [115008/225000 (51%)] Loss: 17417.089844\n",
      "Train Epoch: 329 [117504/225000 (52%)] Loss: 17049.691406\n",
      "Train Epoch: 329 [120000/225000 (53%)] Loss: 16831.078125\n",
      "Train Epoch: 329 [122496/225000 (54%)] Loss: 16408.894531\n",
      "Train Epoch: 329 [124992/225000 (56%)] Loss: 16701.025391\n",
      "Train Epoch: 329 [127488/225000 (57%)] Loss: 16811.955078\n",
      "Train Epoch: 329 [129984/225000 (58%)] Loss: 17069.750000\n",
      "Train Epoch: 329 [132480/225000 (59%)] Loss: 16798.222656\n",
      "Train Epoch: 329 [134976/225000 (60%)] Loss: 16882.375000\n",
      "Train Epoch: 329 [137472/225000 (61%)] Loss: 16954.134766\n",
      "Train Epoch: 329 [139968/225000 (62%)] Loss: 16978.169922\n",
      "Train Epoch: 329 [142464/225000 (63%)] Loss: 16871.785156\n",
      "Train Epoch: 329 [144960/225000 (64%)] Loss: 16880.640625\n",
      "Train Epoch: 329 [147456/225000 (66%)] Loss: 16987.009766\n",
      "Train Epoch: 329 [149952/225000 (67%)] Loss: 16569.138672\n",
      "Train Epoch: 329 [152448/225000 (68%)] Loss: 16830.726562\n",
      "Train Epoch: 329 [154944/225000 (69%)] Loss: 16484.042969\n",
      "Train Epoch: 329 [157440/225000 (70%)] Loss: 17046.535156\n",
      "Train Epoch: 329 [159936/225000 (71%)] Loss: 16583.992188\n",
      "Train Epoch: 329 [162432/225000 (72%)] Loss: 17139.246094\n",
      "Train Epoch: 329 [164928/225000 (73%)] Loss: 16235.637695\n",
      "Train Epoch: 329 [167424/225000 (74%)] Loss: 16984.769531\n",
      "Train Epoch: 329 [169920/225000 (76%)] Loss: 17177.822266\n",
      "Train Epoch: 329 [172416/225000 (77%)] Loss: 17344.804688\n",
      "Train Epoch: 329 [174912/225000 (78%)] Loss: 17085.210938\n",
      "Train Epoch: 329 [177408/225000 (79%)] Loss: 16631.605469\n",
      "Train Epoch: 329 [179904/225000 (80%)] Loss: 16689.296875\n",
      "Train Epoch: 329 [182400/225000 (81%)] Loss: 17411.984375\n",
      "Train Epoch: 329 [184896/225000 (82%)] Loss: 16899.812500\n",
      "Train Epoch: 329 [187392/225000 (83%)] Loss: 17211.074219\n",
      "Train Epoch: 329 [189888/225000 (84%)] Loss: 16671.126953\n",
      "Train Epoch: 329 [192384/225000 (86%)] Loss: 16787.234375\n",
      "Train Epoch: 329 [194880/225000 (87%)] Loss: 17351.230469\n",
      "Train Epoch: 329 [197376/225000 (88%)] Loss: 16645.238281\n",
      "Train Epoch: 329 [199872/225000 (89%)] Loss: 16896.996094\n",
      "Train Epoch: 329 [202368/225000 (90%)] Loss: 16178.799805\n",
      "Train Epoch: 329 [204864/225000 (91%)] Loss: 16718.207031\n",
      "Train Epoch: 329 [207360/225000 (92%)] Loss: 17040.320312\n",
      "Train Epoch: 329 [209856/225000 (93%)] Loss: 16949.417969\n",
      "Train Epoch: 329 [212352/225000 (94%)] Loss: 16766.957031\n",
      "Train Epoch: 329 [214848/225000 (95%)] Loss: 16927.949219\n",
      "Train Epoch: 329 [217344/225000 (97%)] Loss: 16860.841797\n",
      "Train Epoch: 329 [219840/225000 (98%)] Loss: 17029.523438\n",
      "Train Epoch: 329 [222336/225000 (99%)] Loss: 16867.988281\n",
      "Train Epoch: 329 [224832/225000 (100%)] Loss: 16754.240234\n",
      "    epoch          : 329\n",
      "    loss           : 16894.896479375533\n",
      "    val_loss       : 16809.572036198988\n",
      "Train Epoch: 330 [192/225000 (0%)] Loss: 16822.003906\n",
      "Train Epoch: 330 [2688/225000 (1%)] Loss: 16565.974609\n",
      "Train Epoch: 330 [5184/225000 (2%)] Loss: 17273.753906\n",
      "Train Epoch: 330 [7680/225000 (3%)] Loss: 17139.636719\n",
      "Train Epoch: 330 [10176/225000 (5%)] Loss: 16433.976562\n",
      "Train Epoch: 330 [12672/225000 (6%)] Loss: 16646.062500\n",
      "Train Epoch: 330 [15168/225000 (7%)] Loss: 17069.804688\n",
      "Train Epoch: 330 [17664/225000 (8%)] Loss: 16954.277344\n",
      "Train Epoch: 330 [20160/225000 (9%)] Loss: 16628.968750\n",
      "Train Epoch: 330 [22656/225000 (10%)] Loss: 16818.962891\n",
      "Train Epoch: 330 [25152/225000 (11%)] Loss: 16863.089844\n",
      "Train Epoch: 330 [27648/225000 (12%)] Loss: 16653.626953\n",
      "Train Epoch: 330 [30144/225000 (13%)] Loss: 16610.103516\n",
      "Train Epoch: 330 [32640/225000 (15%)] Loss: 17033.974609\n",
      "Train Epoch: 330 [35136/225000 (16%)] Loss: 16745.894531\n",
      "Train Epoch: 330 [37632/225000 (17%)] Loss: 16895.906250\n",
      "Train Epoch: 330 [40128/225000 (18%)] Loss: 16900.070312\n",
      "Train Epoch: 330 [42624/225000 (19%)] Loss: 17270.765625\n",
      "Train Epoch: 330 [45120/225000 (20%)] Loss: 16835.027344\n",
      "Train Epoch: 330 [47616/225000 (21%)] Loss: 16828.765625\n",
      "Train Epoch: 330 [50112/225000 (22%)] Loss: 18646.031250\n",
      "Train Epoch: 330 [52608/225000 (23%)] Loss: 17314.597656\n",
      "Train Epoch: 330 [55104/225000 (24%)] Loss: 16792.662109\n",
      "Train Epoch: 330 [57600/225000 (26%)] Loss: 16530.757812\n",
      "Train Epoch: 330 [60096/225000 (27%)] Loss: 16922.363281\n",
      "Train Epoch: 330 [62592/225000 (28%)] Loss: 16830.199219\n",
      "Train Epoch: 330 [65088/225000 (29%)] Loss: 17063.005859\n",
      "Train Epoch: 330 [67584/225000 (30%)] Loss: 16931.382812\n",
      "Train Epoch: 330 [70080/225000 (31%)] Loss: 16839.601562\n",
      "Train Epoch: 330 [72576/225000 (32%)] Loss: 17009.779297\n",
      "Train Epoch: 330 [75072/225000 (33%)] Loss: 16604.890625\n",
      "Train Epoch: 330 [77568/225000 (34%)] Loss: 16993.027344\n",
      "Train Epoch: 330 [80064/225000 (36%)] Loss: 16587.179688\n",
      "Train Epoch: 330 [82560/225000 (37%)] Loss: 16539.128906\n",
      "Train Epoch: 330 [85056/225000 (38%)] Loss: 16397.535156\n",
      "Train Epoch: 330 [87552/225000 (39%)] Loss: 16968.867188\n",
      "Train Epoch: 330 [90048/225000 (40%)] Loss: 16749.751953\n",
      "Train Epoch: 330 [92544/225000 (41%)] Loss: 16612.501953\n",
      "Train Epoch: 330 [95040/225000 (42%)] Loss: 16883.316406\n",
      "Train Epoch: 330 [97536/225000 (43%)] Loss: 16888.785156\n",
      "Train Epoch: 330 [100032/225000 (44%)] Loss: 17375.007812\n",
      "Train Epoch: 330 [102528/225000 (46%)] Loss: 17142.869141\n",
      "Train Epoch: 330 [105024/225000 (47%)] Loss: 16855.585938\n",
      "Train Epoch: 330 [107520/225000 (48%)] Loss: 16918.136719\n",
      "Train Epoch: 330 [110016/225000 (49%)] Loss: 16658.353516\n",
      "Train Epoch: 330 [112512/225000 (50%)] Loss: 16918.097656\n",
      "Train Epoch: 330 [115008/225000 (51%)] Loss: 17078.875000\n",
      "Train Epoch: 330 [117504/225000 (52%)] Loss: 16282.739258\n",
      "Train Epoch: 330 [120000/225000 (53%)] Loss: 16340.177734\n",
      "Train Epoch: 330 [122496/225000 (54%)] Loss: 16609.460938\n",
      "Train Epoch: 330 [124992/225000 (56%)] Loss: 16756.593750\n",
      "Train Epoch: 330 [127488/225000 (57%)] Loss: 17016.556641\n",
      "Train Epoch: 330 [129984/225000 (58%)] Loss: 16892.582031\n",
      "Train Epoch: 330 [132480/225000 (59%)] Loss: 17226.054688\n",
      "Train Epoch: 330 [134976/225000 (60%)] Loss: 16710.080078\n",
      "Train Epoch: 330 [137472/225000 (61%)] Loss: 16894.164062\n",
      "Train Epoch: 330 [139968/225000 (62%)] Loss: 17080.656250\n",
      "Train Epoch: 330 [142464/225000 (63%)] Loss: 16732.976562\n",
      "Train Epoch: 330 [144960/225000 (64%)] Loss: 16867.134766\n",
      "Train Epoch: 330 [147456/225000 (66%)] Loss: 16859.550781\n",
      "Train Epoch: 330 [149952/225000 (67%)] Loss: 17037.832031\n",
      "Train Epoch: 330 [152448/225000 (68%)] Loss: 16790.703125\n",
      "Train Epoch: 330 [154944/225000 (69%)] Loss: 16378.387695\n",
      "Train Epoch: 330 [157440/225000 (70%)] Loss: 16681.865234\n",
      "Train Epoch: 330 [159936/225000 (71%)] Loss: 16918.675781\n",
      "Train Epoch: 330 [162432/225000 (72%)] Loss: 16891.009766\n",
      "Train Epoch: 330 [164928/225000 (73%)] Loss: 16476.546875\n",
      "Train Epoch: 330 [167424/225000 (74%)] Loss: 16707.269531\n",
      "Train Epoch: 330 [169920/225000 (76%)] Loss: 16761.781250\n",
      "Train Epoch: 330 [172416/225000 (77%)] Loss: 16860.921875\n",
      "Train Epoch: 330 [174912/225000 (78%)] Loss: 17021.484375\n",
      "Train Epoch: 330 [177408/225000 (79%)] Loss: 16484.242188\n",
      "Train Epoch: 330 [179904/225000 (80%)] Loss: 17157.507812\n",
      "Train Epoch: 330 [182400/225000 (81%)] Loss: 17090.562500\n",
      "Train Epoch: 330 [184896/225000 (82%)] Loss: 17458.546875\n",
      "Train Epoch: 330 [187392/225000 (83%)] Loss: 17047.882812\n",
      "Train Epoch: 330 [189888/225000 (84%)] Loss: 16680.162109\n",
      "Train Epoch: 330 [192384/225000 (86%)] Loss: 16650.878906\n",
      "Train Epoch: 330 [194880/225000 (87%)] Loss: 16983.275391\n",
      "Train Epoch: 330 [197376/225000 (88%)] Loss: 16622.457031\n",
      "Train Epoch: 330 [199872/225000 (89%)] Loss: 16926.425781\n",
      "Train Epoch: 330 [202368/225000 (90%)] Loss: 16867.828125\n",
      "Train Epoch: 330 [204864/225000 (91%)] Loss: 17020.132812\n",
      "Train Epoch: 330 [207360/225000 (92%)] Loss: 16830.992188\n",
      "Train Epoch: 330 [209856/225000 (93%)] Loss: 17000.197266\n",
      "Train Epoch: 330 [212352/225000 (94%)] Loss: 16932.105469\n",
      "Train Epoch: 330 [214848/225000 (95%)] Loss: 16306.207031\n",
      "Train Epoch: 330 [217344/225000 (97%)] Loss: 18757.318359\n",
      "Train Epoch: 330 [219840/225000 (98%)] Loss: 18320.037109\n",
      "Train Epoch: 330 [222336/225000 (99%)] Loss: 17148.837891\n",
      "Train Epoch: 330 [224832/225000 (100%)] Loss: 16627.085938\n",
      "    epoch          : 330\n",
      "    loss           : 16895.1046421715\n",
      "    val_loss       : 16834.492736872828\n",
      "Train Epoch: 331 [192/225000 (0%)] Loss: 16994.398438\n",
      "Train Epoch: 331 [2688/225000 (1%)] Loss: 16496.027344\n",
      "Train Epoch: 331 [5184/225000 (2%)] Loss: 16432.738281\n",
      "Train Epoch: 331 [7680/225000 (3%)] Loss: 16803.957031\n",
      "Train Epoch: 331 [10176/225000 (5%)] Loss: 16648.980469\n",
      "Train Epoch: 331 [12672/225000 (6%)] Loss: 16651.718750\n",
      "Train Epoch: 331 [15168/225000 (7%)] Loss: 16562.367188\n",
      "Train Epoch: 331 [17664/225000 (8%)] Loss: 16765.433594\n",
      "Train Epoch: 331 [20160/225000 (9%)] Loss: 16607.851562\n",
      "Train Epoch: 331 [22656/225000 (10%)] Loss: 17117.542969\n",
      "Train Epoch: 331 [25152/225000 (11%)] Loss: 17029.666016\n",
      "Train Epoch: 331 [27648/225000 (12%)] Loss: 16805.712891\n",
      "Train Epoch: 331 [30144/225000 (13%)] Loss: 16912.630859\n",
      "Train Epoch: 331 [32640/225000 (15%)] Loss: 17083.027344\n",
      "Train Epoch: 331 [35136/225000 (16%)] Loss: 16487.224609\n",
      "Train Epoch: 331 [37632/225000 (17%)] Loss: 17042.968750\n",
      "Train Epoch: 331 [40128/225000 (18%)] Loss: 16810.253906\n",
      "Train Epoch: 331 [42624/225000 (19%)] Loss: 16855.335938\n",
      "Train Epoch: 331 [45120/225000 (20%)] Loss: 16482.003906\n",
      "Train Epoch: 331 [47616/225000 (21%)] Loss: 17053.843750\n",
      "Train Epoch: 331 [50112/225000 (22%)] Loss: 16818.802734\n",
      "Train Epoch: 331 [52608/225000 (23%)] Loss: 16641.339844\n",
      "Train Epoch: 331 [55104/225000 (24%)] Loss: 17055.068359\n",
      "Train Epoch: 331 [57600/225000 (26%)] Loss: 17053.671875\n",
      "Train Epoch: 331 [60096/225000 (27%)] Loss: 17019.324219\n",
      "Train Epoch: 331 [62592/225000 (28%)] Loss: 17187.460938\n",
      "Train Epoch: 331 [65088/225000 (29%)] Loss: 16806.460938\n",
      "Train Epoch: 331 [67584/225000 (30%)] Loss: 16929.339844\n",
      "Train Epoch: 331 [70080/225000 (31%)] Loss: 17248.898438\n",
      "Train Epoch: 331 [72576/225000 (32%)] Loss: 16857.400391\n",
      "Train Epoch: 331 [75072/225000 (33%)] Loss: 17185.681641\n",
      "Train Epoch: 331 [77568/225000 (34%)] Loss: 16604.984375\n",
      "Train Epoch: 331 [80064/225000 (36%)] Loss: 16829.855469\n",
      "Train Epoch: 331 [82560/225000 (37%)] Loss: 16879.751953\n",
      "Train Epoch: 331 [85056/225000 (38%)] Loss: 16918.191406\n",
      "Train Epoch: 331 [87552/225000 (39%)] Loss: 16701.326172\n",
      "Train Epoch: 331 [90048/225000 (40%)] Loss: 16736.498047\n",
      "Train Epoch: 331 [92544/225000 (41%)] Loss: 16696.300781\n",
      "Train Epoch: 331 [95040/225000 (42%)] Loss: 16912.443359\n",
      "Train Epoch: 331 [97536/225000 (43%)] Loss: 16950.496094\n",
      "Train Epoch: 331 [100032/225000 (44%)] Loss: 16598.033203\n",
      "Train Epoch: 331 [102528/225000 (46%)] Loss: 16984.195312\n",
      "Train Epoch: 331 [105024/225000 (47%)] Loss: 16904.578125\n",
      "Train Epoch: 331 [107520/225000 (48%)] Loss: 17115.314453\n",
      "Train Epoch: 331 [110016/225000 (49%)] Loss: 15937.596680\n",
      "Train Epoch: 331 [112512/225000 (50%)] Loss: 17243.757812\n",
      "Train Epoch: 331 [115008/225000 (51%)] Loss: 16729.767578\n",
      "Train Epoch: 331 [117504/225000 (52%)] Loss: 16675.906250\n",
      "Train Epoch: 331 [120000/225000 (53%)] Loss: 18378.492188\n",
      "Train Epoch: 331 [122496/225000 (54%)] Loss: 16619.816406\n",
      "Train Epoch: 331 [124992/225000 (56%)] Loss: 17255.853516\n",
      "Train Epoch: 331 [127488/225000 (57%)] Loss: 16390.492188\n",
      "Train Epoch: 331 [129984/225000 (58%)] Loss: 16770.015625\n",
      "Train Epoch: 331 [132480/225000 (59%)] Loss: 16525.367188\n",
      "Train Epoch: 331 [134976/225000 (60%)] Loss: 16734.144531\n",
      "Train Epoch: 331 [137472/225000 (61%)] Loss: 16796.281250\n",
      "Train Epoch: 331 [139968/225000 (62%)] Loss: 16628.878906\n",
      "Train Epoch: 331 [142464/225000 (63%)] Loss: 17170.585938\n",
      "Train Epoch: 331 [144960/225000 (64%)] Loss: 17382.966797\n",
      "Train Epoch: 331 [147456/225000 (66%)] Loss: 16681.035156\n",
      "Train Epoch: 331 [149952/225000 (67%)] Loss: 16708.917969\n",
      "Train Epoch: 331 [152448/225000 (68%)] Loss: 16589.322266\n",
      "Train Epoch: 331 [154944/225000 (69%)] Loss: 17388.171875\n",
      "Train Epoch: 331 [157440/225000 (70%)] Loss: 16726.935547\n",
      "Train Epoch: 331 [159936/225000 (71%)] Loss: 16928.007812\n",
      "Train Epoch: 331 [162432/225000 (72%)] Loss: 16256.349609\n",
      "Train Epoch: 331 [164928/225000 (73%)] Loss: 16830.855469\n",
      "Train Epoch: 331 [167424/225000 (74%)] Loss: 17667.037109\n",
      "Train Epoch: 331 [169920/225000 (76%)] Loss: 16520.609375\n",
      "Train Epoch: 331 [172416/225000 (77%)] Loss: 16883.537109\n",
      "Train Epoch: 331 [174912/225000 (78%)] Loss: 16370.067383\n",
      "Train Epoch: 331 [177408/225000 (79%)] Loss: 16981.429688\n",
      "Train Epoch: 331 [179904/225000 (80%)] Loss: 16643.261719\n",
      "Train Epoch: 331 [182400/225000 (81%)] Loss: 16881.521484\n",
      "Train Epoch: 331 [184896/225000 (82%)] Loss: 16800.173828\n",
      "Train Epoch: 331 [187392/225000 (83%)] Loss: 16468.839844\n",
      "Train Epoch: 331 [189888/225000 (84%)] Loss: 16881.609375\n",
      "Train Epoch: 331 [192384/225000 (86%)] Loss: 16824.984375\n",
      "Train Epoch: 331 [194880/225000 (87%)] Loss: 16829.783203\n",
      "Train Epoch: 331 [197376/225000 (88%)] Loss: 16660.089844\n",
      "Train Epoch: 331 [199872/225000 (89%)] Loss: 16671.472656\n",
      "Train Epoch: 331 [202368/225000 (90%)] Loss: 18176.730469\n",
      "Train Epoch: 331 [204864/225000 (91%)] Loss: 16902.355469\n",
      "Train Epoch: 331 [207360/225000 (92%)] Loss: 16814.509766\n",
      "Train Epoch: 331 [209856/225000 (93%)] Loss: 17027.919922\n",
      "Train Epoch: 331 [212352/225000 (94%)] Loss: 17038.111328\n",
      "Train Epoch: 331 [214848/225000 (95%)] Loss: 16633.171875\n",
      "Train Epoch: 331 [217344/225000 (97%)] Loss: 16748.849609\n",
      "Train Epoch: 331 [219840/225000 (98%)] Loss: 16837.699219\n",
      "Train Epoch: 331 [222336/225000 (99%)] Loss: 16469.951172\n",
      "Train Epoch: 331 [224832/225000 (100%)] Loss: 16889.052734\n",
      "    epoch          : 331\n",
      "    loss           : 16845.6190131386\n",
      "    val_loss       : 16767.540593385696\n",
      "Train Epoch: 332 [192/225000 (0%)] Loss: 17266.812500\n",
      "Train Epoch: 332 [2688/225000 (1%)] Loss: 16177.052734\n",
      "Train Epoch: 332 [5184/225000 (2%)] Loss: 16463.763672\n",
      "Train Epoch: 332 [7680/225000 (3%)] Loss: 16647.880859\n",
      "Train Epoch: 332 [10176/225000 (5%)] Loss: 16579.619141\n",
      "Train Epoch: 332 [12672/225000 (6%)] Loss: 16602.460938\n",
      "Train Epoch: 332 [15168/225000 (7%)] Loss: 16867.154297\n",
      "Train Epoch: 332 [17664/225000 (8%)] Loss: 16964.144531\n",
      "Train Epoch: 332 [20160/225000 (9%)] Loss: 16797.214844\n",
      "Train Epoch: 332 [22656/225000 (10%)] Loss: 16842.503906\n",
      "Train Epoch: 332 [25152/225000 (11%)] Loss: 16593.914062\n",
      "Train Epoch: 332 [27648/225000 (12%)] Loss: 16555.007812\n",
      "Train Epoch: 332 [30144/225000 (13%)] Loss: 16867.439453\n",
      "Train Epoch: 332 [32640/225000 (15%)] Loss: 17226.164062\n",
      "Train Epoch: 332 [35136/225000 (16%)] Loss: 16759.080078\n",
      "Train Epoch: 332 [37632/225000 (17%)] Loss: 16440.179688\n",
      "Train Epoch: 332 [40128/225000 (18%)] Loss: 16595.941406\n",
      "Train Epoch: 332 [42624/225000 (19%)] Loss: 17082.880859\n",
      "Train Epoch: 332 [45120/225000 (20%)] Loss: 16846.000000\n",
      "Train Epoch: 332 [47616/225000 (21%)] Loss: 16560.460938\n",
      "Train Epoch: 332 [50112/225000 (22%)] Loss: 16816.558594\n",
      "Train Epoch: 332 [52608/225000 (23%)] Loss: 16958.414062\n",
      "Train Epoch: 332 [55104/225000 (24%)] Loss: 17258.570312\n",
      "Train Epoch: 332 [57600/225000 (26%)] Loss: 16485.583984\n",
      "Train Epoch: 332 [60096/225000 (27%)] Loss: 17132.207031\n",
      "Train Epoch: 332 [62592/225000 (28%)] Loss: 16575.863281\n",
      "Train Epoch: 332 [65088/225000 (29%)] Loss: 16811.656250\n",
      "Train Epoch: 332 [67584/225000 (30%)] Loss: 17072.453125\n",
      "Train Epoch: 332 [70080/225000 (31%)] Loss: 16615.474609\n",
      "Train Epoch: 332 [72576/225000 (32%)] Loss: 16881.667969\n",
      "Train Epoch: 332 [75072/225000 (33%)] Loss: 16872.250000\n",
      "Train Epoch: 332 [77568/225000 (34%)] Loss: 16405.433594\n",
      "Train Epoch: 332 [80064/225000 (36%)] Loss: 16691.773438\n",
      "Train Epoch: 332 [82560/225000 (37%)] Loss: 16681.437500\n",
      "Train Epoch: 332 [85056/225000 (38%)] Loss: 16673.308594\n",
      "Train Epoch: 332 [87552/225000 (39%)] Loss: 17093.248047\n",
      "Train Epoch: 332 [90048/225000 (40%)] Loss: 17287.648438\n",
      "Train Epoch: 332 [92544/225000 (41%)] Loss: 16828.957031\n",
      "Train Epoch: 332 [95040/225000 (42%)] Loss: 16572.273438\n",
      "Train Epoch: 332 [97536/225000 (43%)] Loss: 16973.240234\n",
      "Train Epoch: 332 [100032/225000 (44%)] Loss: 17219.130859\n",
      "Train Epoch: 332 [102528/225000 (46%)] Loss: 16734.388672\n",
      "Train Epoch: 332 [105024/225000 (47%)] Loss: 16959.503906\n",
      "Train Epoch: 332 [107520/225000 (48%)] Loss: 16537.519531\n",
      "Train Epoch: 332 [110016/225000 (49%)] Loss: 16805.093750\n",
      "Train Epoch: 332 [112512/225000 (50%)] Loss: 17127.363281\n",
      "Train Epoch: 332 [115008/225000 (51%)] Loss: 17135.652344\n",
      "Train Epoch: 332 [117504/225000 (52%)] Loss: 17026.287109\n",
      "Train Epoch: 332 [120000/225000 (53%)] Loss: 16802.929688\n",
      "Train Epoch: 332 [122496/225000 (54%)] Loss: 16798.652344\n",
      "Train Epoch: 332 [124992/225000 (56%)] Loss: 16870.585938\n",
      "Train Epoch: 332 [127488/225000 (57%)] Loss: 16635.199219\n",
      "Train Epoch: 332 [129984/225000 (58%)] Loss: 16532.902344\n",
      "Train Epoch: 332 [132480/225000 (59%)] Loss: 16578.634766\n",
      "Train Epoch: 332 [134976/225000 (60%)] Loss: 17050.484375\n",
      "Train Epoch: 332 [137472/225000 (61%)] Loss: 16937.722656\n",
      "Train Epoch: 332 [139968/225000 (62%)] Loss: 16801.970703\n",
      "Train Epoch: 332 [142464/225000 (63%)] Loss: 17369.101562\n",
      "Train Epoch: 332 [144960/225000 (64%)] Loss: 16221.862305\n",
      "Train Epoch: 332 [147456/225000 (66%)] Loss: 19700.957031\n",
      "Train Epoch: 332 [149952/225000 (67%)] Loss: 17117.214844\n",
      "Train Epoch: 332 [152448/225000 (68%)] Loss: 16918.503906\n",
      "Train Epoch: 332 [154944/225000 (69%)] Loss: 16948.652344\n",
      "Train Epoch: 332 [157440/225000 (70%)] Loss: 16524.625000\n",
      "Train Epoch: 332 [159936/225000 (71%)] Loss: 16900.742188\n",
      "Train Epoch: 332 [162432/225000 (72%)] Loss: 16450.314453\n",
      "Train Epoch: 332 [164928/225000 (73%)] Loss: 16610.302734\n",
      "Train Epoch: 332 [167424/225000 (74%)] Loss: 16910.093750\n",
      "Train Epoch: 332 [169920/225000 (76%)] Loss: 16817.439453\n",
      "Train Epoch: 332 [172416/225000 (77%)] Loss: 16586.507812\n",
      "Train Epoch: 332 [174912/225000 (78%)] Loss: 16804.074219\n",
      "Train Epoch: 332 [177408/225000 (79%)] Loss: 16655.093750\n",
      "Train Epoch: 332 [179904/225000 (80%)] Loss: 16657.242188\n",
      "Train Epoch: 332 [182400/225000 (81%)] Loss: 17041.681641\n",
      "Train Epoch: 332 [184896/225000 (82%)] Loss: 16646.773438\n",
      "Train Epoch: 332 [187392/225000 (83%)] Loss: 16995.042969\n",
      "Train Epoch: 332 [189888/225000 (84%)] Loss: 18152.218750\n",
      "Train Epoch: 332 [192384/225000 (86%)] Loss: 16682.128906\n",
      "Train Epoch: 332 [194880/225000 (87%)] Loss: 17059.199219\n",
      "Train Epoch: 332 [197376/225000 (88%)] Loss: 17113.917969\n",
      "Train Epoch: 332 [199872/225000 (89%)] Loss: 16967.890625\n",
      "Train Epoch: 332 [202368/225000 (90%)] Loss: 17080.765625\n",
      "Train Epoch: 332 [204864/225000 (91%)] Loss: 16276.383789\n",
      "Train Epoch: 332 [207360/225000 (92%)] Loss: 16546.335938\n",
      "Train Epoch: 332 [209856/225000 (93%)] Loss: 16914.609375\n",
      "Train Epoch: 332 [212352/225000 (94%)] Loss: 16701.371094\n",
      "Train Epoch: 332 [214848/225000 (95%)] Loss: 16676.316406\n",
      "Train Epoch: 332 [217344/225000 (97%)] Loss: 17083.089844\n",
      "Train Epoch: 332 [219840/225000 (98%)] Loss: 16473.492188\n",
      "Train Epoch: 332 [222336/225000 (99%)] Loss: 16654.583984\n",
      "Train Epoch: 332 [224832/225000 (100%)] Loss: 16429.453125\n",
      "    epoch          : 332\n",
      "    loss           : 16877.278345309835\n",
      "    val_loss       : 16786.01399944575\n",
      "Train Epoch: 333 [192/225000 (0%)] Loss: 16856.076172\n",
      "Train Epoch: 333 [2688/225000 (1%)] Loss: 16702.933594\n",
      "Train Epoch: 333 [5184/225000 (2%)] Loss: 16382.281250\n",
      "Train Epoch: 333 [7680/225000 (3%)] Loss: 16243.102539\n",
      "Train Epoch: 333 [10176/225000 (5%)] Loss: 16762.083984\n",
      "Train Epoch: 333 [12672/225000 (6%)] Loss: 16998.179688\n",
      "Train Epoch: 333 [15168/225000 (7%)] Loss: 18793.777344\n",
      "Train Epoch: 333 [17664/225000 (8%)] Loss: 16773.289062\n",
      "Train Epoch: 333 [20160/225000 (9%)] Loss: 17013.828125\n",
      "Train Epoch: 333 [22656/225000 (10%)] Loss: 17365.863281\n",
      "Train Epoch: 333 [25152/225000 (11%)] Loss: 16749.335938\n",
      "Train Epoch: 333 [27648/225000 (12%)] Loss: 16812.441406\n",
      "Train Epoch: 333 [30144/225000 (13%)] Loss: 16825.640625\n",
      "Train Epoch: 333 [32640/225000 (15%)] Loss: 16737.101562\n",
      "Train Epoch: 333 [35136/225000 (16%)] Loss: 16915.455078\n",
      "Train Epoch: 333 [37632/225000 (17%)] Loss: 17240.486328\n",
      "Train Epoch: 333 [40128/225000 (18%)] Loss: 16667.804688\n",
      "Train Epoch: 333 [42624/225000 (19%)] Loss: 16700.066406\n",
      "Train Epoch: 333 [45120/225000 (20%)] Loss: 17083.363281\n",
      "Train Epoch: 333 [47616/225000 (21%)] Loss: 16575.056641\n",
      "Train Epoch: 333 [50112/225000 (22%)] Loss: 16358.455078\n",
      "Train Epoch: 333 [52608/225000 (23%)] Loss: 16733.259766\n",
      "Train Epoch: 333 [55104/225000 (24%)] Loss: 16712.714844\n",
      "Train Epoch: 333 [57600/225000 (26%)] Loss: 16767.214844\n",
      "Train Epoch: 333 [60096/225000 (27%)] Loss: 16409.871094\n",
      "Train Epoch: 333 [62592/225000 (28%)] Loss: 16393.884766\n",
      "Train Epoch: 333 [65088/225000 (29%)] Loss: 16404.531250\n",
      "Train Epoch: 333 [67584/225000 (30%)] Loss: 16729.046875\n",
      "Train Epoch: 333 [70080/225000 (31%)] Loss: 17839.250000\n",
      "Train Epoch: 333 [72576/225000 (32%)] Loss: 16446.585938\n",
      "Train Epoch: 333 [75072/225000 (33%)] Loss: 16533.351562\n",
      "Train Epoch: 333 [77568/225000 (34%)] Loss: 16902.964844\n",
      "Train Epoch: 333 [80064/225000 (36%)] Loss: 17051.667969\n",
      "Train Epoch: 333 [82560/225000 (37%)] Loss: 16588.259766\n",
      "Train Epoch: 333 [85056/225000 (38%)] Loss: 16979.792969\n",
      "Train Epoch: 333 [87552/225000 (39%)] Loss: 16589.875000\n",
      "Train Epoch: 333 [90048/225000 (40%)] Loss: 16940.382812\n",
      "Train Epoch: 333 [92544/225000 (41%)] Loss: 17540.238281\n",
      "Train Epoch: 333 [95040/225000 (42%)] Loss: 17315.816406\n",
      "Train Epoch: 333 [97536/225000 (43%)] Loss: 16746.464844\n",
      "Train Epoch: 333 [100032/225000 (44%)] Loss: 17008.453125\n",
      "Train Epoch: 333 [102528/225000 (46%)] Loss: 16552.839844\n",
      "Train Epoch: 333 [105024/225000 (47%)] Loss: 16763.525391\n",
      "Train Epoch: 333 [107520/225000 (48%)] Loss: 16864.720703\n",
      "Train Epoch: 333 [110016/225000 (49%)] Loss: 17098.582031\n",
      "Train Epoch: 333 [112512/225000 (50%)] Loss: 16532.300781\n",
      "Train Epoch: 333 [115008/225000 (51%)] Loss: 16911.572266\n",
      "Train Epoch: 333 [117504/225000 (52%)] Loss: 17217.742188\n",
      "Train Epoch: 333 [120000/225000 (53%)] Loss: 17174.542969\n",
      "Train Epoch: 333 [122496/225000 (54%)] Loss: 16923.457031\n",
      "Train Epoch: 333 [124992/225000 (56%)] Loss: 17400.195312\n",
      "Train Epoch: 333 [127488/225000 (57%)] Loss: 17118.898438\n",
      "Train Epoch: 333 [129984/225000 (58%)] Loss: 16714.179688\n",
      "Train Epoch: 333 [132480/225000 (59%)] Loss: 16235.594727\n",
      "Train Epoch: 333 [134976/225000 (60%)] Loss: 17045.132812\n",
      "Train Epoch: 333 [137472/225000 (61%)] Loss: 16848.933594\n",
      "Train Epoch: 333 [139968/225000 (62%)] Loss: 16608.746094\n",
      "Train Epoch: 333 [142464/225000 (63%)] Loss: 16693.878906\n",
      "Train Epoch: 333 [144960/225000 (64%)] Loss: 16741.902344\n",
      "Train Epoch: 333 [147456/225000 (66%)] Loss: 16778.355469\n",
      "Train Epoch: 333 [149952/225000 (67%)] Loss: 16387.355469\n",
      "Train Epoch: 333 [152448/225000 (68%)] Loss: 17061.511719\n",
      "Train Epoch: 333 [154944/225000 (69%)] Loss: 16574.722656\n",
      "Train Epoch: 333 [157440/225000 (70%)] Loss: 18566.464844\n",
      "Train Epoch: 333 [159936/225000 (71%)] Loss: 16841.785156\n",
      "Train Epoch: 333 [162432/225000 (72%)] Loss: 16599.828125\n",
      "Train Epoch: 333 [164928/225000 (73%)] Loss: 17008.511719\n",
      "Train Epoch: 333 [167424/225000 (74%)] Loss: 16822.898438\n",
      "Train Epoch: 333 [169920/225000 (76%)] Loss: 16571.353516\n",
      "Train Epoch: 333 [172416/225000 (77%)] Loss: 16640.228516\n",
      "Train Epoch: 333 [174912/225000 (78%)] Loss: 16886.984375\n",
      "Train Epoch: 333 [177408/225000 (79%)] Loss: 16703.056641\n",
      "Train Epoch: 333 [179904/225000 (80%)] Loss: 16630.265625\n",
      "Train Epoch: 333 [182400/225000 (81%)] Loss: 17027.937500\n",
      "Train Epoch: 333 [184896/225000 (82%)] Loss: 17465.406250\n",
      "Train Epoch: 333 [187392/225000 (83%)] Loss: 16767.373047\n",
      "Train Epoch: 333 [189888/225000 (84%)] Loss: 17102.410156\n",
      "Train Epoch: 333 [192384/225000 (86%)] Loss: 17150.066406\n",
      "Train Epoch: 333 [194880/225000 (87%)] Loss: 16939.673828\n",
      "Train Epoch: 333 [197376/225000 (88%)] Loss: 16386.574219\n",
      "Train Epoch: 333 [199872/225000 (89%)] Loss: 16880.183594\n",
      "Train Epoch: 333 [202368/225000 (90%)] Loss: 16555.746094\n",
      "Train Epoch: 333 [204864/225000 (91%)] Loss: 18555.492188\n",
      "Train Epoch: 333 [207360/225000 (92%)] Loss: 18245.042969\n",
      "Train Epoch: 333 [209856/225000 (93%)] Loss: 16857.734375\n",
      "Train Epoch: 333 [212352/225000 (94%)] Loss: 16741.234375\n",
      "Train Epoch: 333 [214848/225000 (95%)] Loss: 16854.792969\n",
      "Train Epoch: 333 [217344/225000 (97%)] Loss: 26391.933594\n",
      "Train Epoch: 333 [219840/225000 (98%)] Loss: 16694.406250\n",
      "Train Epoch: 333 [222336/225000 (99%)] Loss: 17170.287109\n",
      "Train Epoch: 333 [224832/225000 (100%)] Loss: 17117.992188\n",
      "    epoch          : 333\n",
      "    loss           : 16859.495802947684\n",
      "    val_loss       : 16877.241980300605\n",
      "Train Epoch: 334 [192/225000 (0%)] Loss: 16571.664062\n",
      "Train Epoch: 334 [2688/225000 (1%)] Loss: 16691.873047\n",
      "Train Epoch: 334 [5184/225000 (2%)] Loss: 16914.552734\n",
      "Train Epoch: 334 [7680/225000 (3%)] Loss: 16854.087891\n",
      "Train Epoch: 334 [10176/225000 (5%)] Loss: 17149.730469\n",
      "Train Epoch: 334 [12672/225000 (6%)] Loss: 16496.312500\n",
      "Train Epoch: 334 [15168/225000 (7%)] Loss: 16577.023438\n",
      "Train Epoch: 334 [17664/225000 (8%)] Loss: 16516.037109\n",
      "Train Epoch: 334 [20160/225000 (9%)] Loss: 17008.242188\n",
      "Train Epoch: 334 [22656/225000 (10%)] Loss: 18261.455078\n",
      "Train Epoch: 334 [25152/225000 (11%)] Loss: 16739.255859\n",
      "Train Epoch: 334 [27648/225000 (12%)] Loss: 16506.949219\n",
      "Train Epoch: 334 [30144/225000 (13%)] Loss: 16886.273438\n",
      "Train Epoch: 334 [32640/225000 (15%)] Loss: 16739.167969\n",
      "Train Epoch: 334 [35136/225000 (16%)] Loss: 17246.148438\n",
      "Train Epoch: 334 [37632/225000 (17%)] Loss: 16390.527344\n",
      "Train Epoch: 334 [40128/225000 (18%)] Loss: 16764.359375\n",
      "Train Epoch: 334 [42624/225000 (19%)] Loss: 16750.802734\n",
      "Train Epoch: 334 [45120/225000 (20%)] Loss: 16697.177734\n",
      "Train Epoch: 334 [47616/225000 (21%)] Loss: 16547.621094\n",
      "Train Epoch: 334 [50112/225000 (22%)] Loss: 16789.636719\n",
      "Train Epoch: 334 [52608/225000 (23%)] Loss: 16719.164062\n",
      "Train Epoch: 334 [55104/225000 (24%)] Loss: 16820.820312\n",
      "Train Epoch: 334 [57600/225000 (26%)] Loss: 16582.542969\n",
      "Train Epoch: 334 [60096/225000 (27%)] Loss: 17020.142578\n",
      "Train Epoch: 334 [62592/225000 (28%)] Loss: 16598.537109\n",
      "Train Epoch: 334 [65088/225000 (29%)] Loss: 16442.632812\n",
      "Train Epoch: 334 [67584/225000 (30%)] Loss: 16733.566406\n",
      "Train Epoch: 334 [70080/225000 (31%)] Loss: 16569.886719\n",
      "Train Epoch: 334 [72576/225000 (32%)] Loss: 16641.914062\n",
      "Train Epoch: 334 [75072/225000 (33%)] Loss: 18562.720703\n",
      "Train Epoch: 334 [77568/225000 (34%)] Loss: 16451.460938\n",
      "Train Epoch: 334 [80064/225000 (36%)] Loss: 16560.933594\n",
      "Train Epoch: 334 [82560/225000 (37%)] Loss: 16566.492188\n",
      "Train Epoch: 334 [85056/225000 (38%)] Loss: 16668.378906\n",
      "Train Epoch: 334 [87552/225000 (39%)] Loss: 18355.939453\n",
      "Train Epoch: 334 [90048/225000 (40%)] Loss: 16823.945312\n",
      "Train Epoch: 334 [92544/225000 (41%)] Loss: 16955.187500\n",
      "Train Epoch: 334 [95040/225000 (42%)] Loss: 16672.945312\n",
      "Train Epoch: 334 [97536/225000 (43%)] Loss: 16807.806641\n",
      "Train Epoch: 334 [100032/225000 (44%)] Loss: 16587.933594\n",
      "Train Epoch: 334 [102528/225000 (46%)] Loss: 16793.388672\n",
      "Train Epoch: 334 [105024/225000 (47%)] Loss: 17032.625000\n",
      "Train Epoch: 334 [107520/225000 (48%)] Loss: 16530.283203\n",
      "Train Epoch: 334 [110016/225000 (49%)] Loss: 16755.289062\n",
      "Train Epoch: 334 [112512/225000 (50%)] Loss: 16674.609375\n",
      "Train Epoch: 334 [115008/225000 (51%)] Loss: 17188.136719\n",
      "Train Epoch: 334 [117504/225000 (52%)] Loss: 16666.808594\n",
      "Train Epoch: 334 [120000/225000 (53%)] Loss: 16990.148438\n",
      "Train Epoch: 334 [122496/225000 (54%)] Loss: 16735.871094\n",
      "Train Epoch: 334 [124992/225000 (56%)] Loss: 16695.337891\n",
      "Train Epoch: 334 [127488/225000 (57%)] Loss: 16776.324219\n",
      "Train Epoch: 334 [129984/225000 (58%)] Loss: 16518.265625\n",
      "Train Epoch: 334 [132480/225000 (59%)] Loss: 16706.244141\n",
      "Train Epoch: 334 [134976/225000 (60%)] Loss: 16778.904297\n",
      "Train Epoch: 334 [137472/225000 (61%)] Loss: 16831.453125\n",
      "Train Epoch: 334 [139968/225000 (62%)] Loss: 18873.970703\n",
      "Train Epoch: 334 [142464/225000 (63%)] Loss: 16729.376953\n",
      "Train Epoch: 334 [144960/225000 (64%)] Loss: 16584.281250\n",
      "Train Epoch: 334 [147456/225000 (66%)] Loss: 17244.800781\n",
      "Train Epoch: 334 [149952/225000 (67%)] Loss: 17138.125000\n",
      "Train Epoch: 334 [152448/225000 (68%)] Loss: 16463.074219\n",
      "Train Epoch: 334 [154944/225000 (69%)] Loss: 16279.837891\n",
      "Train Epoch: 334 [157440/225000 (70%)] Loss: 17149.039062\n",
      "Train Epoch: 334 [159936/225000 (71%)] Loss: 16817.257812\n",
      "Train Epoch: 334 [162432/225000 (72%)] Loss: 16849.703125\n",
      "Train Epoch: 334 [164928/225000 (73%)] Loss: 16811.554688\n",
      "Train Epoch: 334 [167424/225000 (74%)] Loss: 16608.248047\n",
      "Train Epoch: 334 [169920/225000 (76%)] Loss: 16215.892578\n",
      "Train Epoch: 334 [172416/225000 (77%)] Loss: 16929.160156\n",
      "Train Epoch: 334 [174912/225000 (78%)] Loss: 17107.062500\n",
      "Train Epoch: 334 [177408/225000 (79%)] Loss: 16669.828125\n",
      "Train Epoch: 334 [179904/225000 (80%)] Loss: 16754.050781\n",
      "Train Epoch: 334 [182400/225000 (81%)] Loss: 16979.722656\n",
      "Train Epoch: 334 [184896/225000 (82%)] Loss: 16975.160156\n",
      "Train Epoch: 334 [187392/225000 (83%)] Loss: 16309.900391\n",
      "Train Epoch: 334 [189888/225000 (84%)] Loss: 16995.714844\n",
      "Train Epoch: 334 [192384/225000 (86%)] Loss: 16630.019531\n",
      "Train Epoch: 334 [194880/225000 (87%)] Loss: 16359.791992\n",
      "Train Epoch: 334 [197376/225000 (88%)] Loss: 18185.343750\n",
      "Train Epoch: 334 [199872/225000 (89%)] Loss: 16487.968750\n",
      "Train Epoch: 334 [202368/225000 (90%)] Loss: 17124.164062\n",
      "Train Epoch: 334 [204864/225000 (91%)] Loss: 17062.419922\n",
      "Train Epoch: 334 [207360/225000 (92%)] Loss: 16634.357422\n",
      "Train Epoch: 334 [209856/225000 (93%)] Loss: 16982.089844\n",
      "Train Epoch: 334 [212352/225000 (94%)] Loss: 16800.652344\n",
      "Train Epoch: 334 [214848/225000 (95%)] Loss: 16744.417969\n",
      "Train Epoch: 334 [217344/225000 (97%)] Loss: 16930.257812\n",
      "Train Epoch: 334 [219840/225000 (98%)] Loss: 16842.386719\n",
      "Train Epoch: 334 [222336/225000 (99%)] Loss: 17131.718750\n",
      "Train Epoch: 334 [224832/225000 (100%)] Loss: 17342.843750\n",
      "    epoch          : 334\n",
      "    loss           : 16896.944939206485\n",
      "    val_loss       : 16766.189039173016\n",
      "Train Epoch: 335 [192/225000 (0%)] Loss: 16546.535156\n",
      "Train Epoch: 335 [2688/225000 (1%)] Loss: 16881.390625\n",
      "Train Epoch: 335 [5184/225000 (2%)] Loss: 16565.703125\n",
      "Train Epoch: 335 [7680/225000 (3%)] Loss: 16870.519531\n",
      "Train Epoch: 335 [10176/225000 (5%)] Loss: 16607.939453\n",
      "Train Epoch: 335 [12672/225000 (6%)] Loss: 16782.328125\n",
      "Train Epoch: 335 [15168/225000 (7%)] Loss: 16744.320312\n",
      "Train Epoch: 335 [17664/225000 (8%)] Loss: 16702.093750\n",
      "Train Epoch: 335 [20160/225000 (9%)] Loss: 16842.535156\n",
      "Train Epoch: 335 [22656/225000 (10%)] Loss: 16780.253906\n",
      "Train Epoch: 335 [25152/225000 (11%)] Loss: 16542.164062\n",
      "Train Epoch: 335 [27648/225000 (12%)] Loss: 16973.917969\n",
      "Train Epoch: 335 [30144/225000 (13%)] Loss: 16684.058594\n",
      "Train Epoch: 335 [32640/225000 (15%)] Loss: 16882.921875\n",
      "Train Epoch: 335 [35136/225000 (16%)] Loss: 16471.896484\n",
      "Train Epoch: 335 [37632/225000 (17%)] Loss: 16940.121094\n",
      "Train Epoch: 335 [40128/225000 (18%)] Loss: 16805.099609\n",
      "Train Epoch: 335 [42624/225000 (19%)] Loss: 17060.511719\n",
      "Train Epoch: 335 [45120/225000 (20%)] Loss: 16718.183594\n",
      "Train Epoch: 335 [47616/225000 (21%)] Loss: 16483.238281\n",
      "Train Epoch: 335 [50112/225000 (22%)] Loss: 17214.917969\n",
      "Train Epoch: 335 [52608/225000 (23%)] Loss: 16789.191406\n",
      "Train Epoch: 335 [55104/225000 (24%)] Loss: 17277.585938\n",
      "Train Epoch: 335 [57600/225000 (26%)] Loss: 16446.169922\n",
      "Train Epoch: 335 [60096/225000 (27%)] Loss: 16310.632812\n",
      "Train Epoch: 335 [62592/225000 (28%)] Loss: 17169.833984\n",
      "Train Epoch: 335 [65088/225000 (29%)] Loss: 17115.416016\n",
      "Train Epoch: 335 [67584/225000 (30%)] Loss: 16582.658203\n",
      "Train Epoch: 335 [70080/225000 (31%)] Loss: 16656.457031\n",
      "Train Epoch: 335 [72576/225000 (32%)] Loss: 17356.675781\n",
      "Train Epoch: 335 [75072/225000 (33%)] Loss: 17142.603516\n",
      "Train Epoch: 335 [77568/225000 (34%)] Loss: 16442.351562\n",
      "Train Epoch: 335 [80064/225000 (36%)] Loss: 17089.470703\n",
      "Train Epoch: 335 [82560/225000 (37%)] Loss: 16636.417969\n",
      "Train Epoch: 335 [85056/225000 (38%)] Loss: 16585.982422\n",
      "Train Epoch: 335 [87552/225000 (39%)] Loss: 16868.792969\n",
      "Train Epoch: 335 [90048/225000 (40%)] Loss: 16599.880859\n",
      "Train Epoch: 335 [92544/225000 (41%)] Loss: 17014.906250\n",
      "Train Epoch: 335 [95040/225000 (42%)] Loss: 17342.269531\n",
      "Train Epoch: 335 [97536/225000 (43%)] Loss: 16659.888672\n",
      "Train Epoch: 335 [100032/225000 (44%)] Loss: 16666.410156\n",
      "Train Epoch: 335 [102528/225000 (46%)] Loss: 16885.929688\n",
      "Train Epoch: 335 [105024/225000 (47%)] Loss: 16952.050781\n",
      "Train Epoch: 335 [107520/225000 (48%)] Loss: 17047.271484\n",
      "Train Epoch: 335 [110016/225000 (49%)] Loss: 16255.985352\n",
      "Train Epoch: 335 [112512/225000 (50%)] Loss: 16709.152344\n",
      "Train Epoch: 335 [115008/225000 (51%)] Loss: 17005.718750\n",
      "Train Epoch: 335 [117504/225000 (52%)] Loss: 16624.113281\n",
      "Train Epoch: 335 [120000/225000 (53%)] Loss: 16652.421875\n",
      "Train Epoch: 335 [122496/225000 (54%)] Loss: 16494.039062\n",
      "Train Epoch: 335 [124992/225000 (56%)] Loss: 17100.742188\n",
      "Train Epoch: 335 [127488/225000 (57%)] Loss: 17129.650391\n",
      "Train Epoch: 335 [129984/225000 (58%)] Loss: 16874.789062\n",
      "Train Epoch: 335 [132480/225000 (59%)] Loss: 16775.582031\n",
      "Train Epoch: 335 [134976/225000 (60%)] Loss: 18213.884766\n",
      "Train Epoch: 335 [137472/225000 (61%)] Loss: 16465.707031\n",
      "Train Epoch: 335 [139968/225000 (62%)] Loss: 17175.142578\n",
      "Train Epoch: 335 [142464/225000 (63%)] Loss: 16478.710938\n",
      "Train Epoch: 335 [144960/225000 (64%)] Loss: 16692.699219\n",
      "Train Epoch: 335 [147456/225000 (66%)] Loss: 16567.585938\n",
      "Train Epoch: 335 [149952/225000 (67%)] Loss: 16831.105469\n",
      "Train Epoch: 335 [152448/225000 (68%)] Loss: 17042.746094\n",
      "Train Epoch: 335 [154944/225000 (69%)] Loss: 17034.468750\n",
      "Train Epoch: 335 [157440/225000 (70%)] Loss: 16820.125000\n",
      "Train Epoch: 335 [159936/225000 (71%)] Loss: 16666.906250\n",
      "Train Epoch: 335 [162432/225000 (72%)] Loss: 16961.417969\n",
      "Train Epoch: 335 [164928/225000 (73%)] Loss: 16866.718750\n",
      "Train Epoch: 335 [167424/225000 (74%)] Loss: 16319.089844\n",
      "Train Epoch: 335 [169920/225000 (76%)] Loss: 16771.759766\n",
      "Train Epoch: 335 [172416/225000 (77%)] Loss: 16717.187500\n",
      "Train Epoch: 335 [174912/225000 (78%)] Loss: 16641.322266\n",
      "Train Epoch: 335 [177408/225000 (79%)] Loss: 16931.445312\n",
      "Train Epoch: 335 [179904/225000 (80%)] Loss: 16901.964844\n",
      "Train Epoch: 335 [182400/225000 (81%)] Loss: 17106.535156\n",
      "Train Epoch: 335 [184896/225000 (82%)] Loss: 16589.228516\n",
      "Train Epoch: 335 [187392/225000 (83%)] Loss: 17188.066406\n",
      "Train Epoch: 335 [189888/225000 (84%)] Loss: 17044.732422\n",
      "Train Epoch: 335 [192384/225000 (86%)] Loss: 16333.330078\n",
      "Train Epoch: 335 [194880/225000 (87%)] Loss: 16695.144531\n",
      "Train Epoch: 335 [197376/225000 (88%)] Loss: 17130.248047\n",
      "Train Epoch: 335 [199872/225000 (89%)] Loss: 16669.960938\n",
      "Train Epoch: 335 [202368/225000 (90%)] Loss: 16889.916016\n",
      "Train Epoch: 335 [204864/225000 (91%)] Loss: 16946.335938\n",
      "Train Epoch: 335 [207360/225000 (92%)] Loss: 17150.615234\n",
      "Train Epoch: 335 [209856/225000 (93%)] Loss: 18366.417969\n",
      "Train Epoch: 335 [212352/225000 (94%)] Loss: 16527.605469\n",
      "Train Epoch: 335 [214848/225000 (95%)] Loss: 16351.592773\n",
      "Train Epoch: 335 [217344/225000 (97%)] Loss: 17027.847656\n",
      "Train Epoch: 335 [219840/225000 (98%)] Loss: 16615.054688\n",
      "Train Epoch: 335 [222336/225000 (99%)] Loss: 16810.406250\n",
      "Train Epoch: 335 [224832/225000 (100%)] Loss: 17050.164062\n",
      "    epoch          : 335\n",
      "    loss           : 16855.74988917849\n",
      "    val_loss       : 16772.35525043866\n",
      "Train Epoch: 336 [192/225000 (0%)] Loss: 16793.097656\n",
      "Train Epoch: 336 [2688/225000 (1%)] Loss: 16576.537109\n",
      "Train Epoch: 336 [5184/225000 (2%)] Loss: 16852.875000\n",
      "Train Epoch: 336 [7680/225000 (3%)] Loss: 16809.132812\n",
      "Train Epoch: 336 [10176/225000 (5%)] Loss: 16389.968750\n",
      "Train Epoch: 336 [12672/225000 (6%)] Loss: 16769.300781\n",
      "Train Epoch: 336 [15168/225000 (7%)] Loss: 17044.949219\n",
      "Train Epoch: 336 [17664/225000 (8%)] Loss: 17161.251953\n",
      "Train Epoch: 336 [20160/225000 (9%)] Loss: 16800.167969\n",
      "Train Epoch: 336 [22656/225000 (10%)] Loss: 17212.824219\n",
      "Train Epoch: 336 [25152/225000 (11%)] Loss: 16796.613281\n",
      "Train Epoch: 336 [27648/225000 (12%)] Loss: 16472.261719\n",
      "Train Epoch: 336 [30144/225000 (13%)] Loss: 16878.902344\n",
      "Train Epoch: 336 [32640/225000 (15%)] Loss: 16367.378906\n",
      "Train Epoch: 336 [35136/225000 (16%)] Loss: 16866.726562\n",
      "Train Epoch: 336 [37632/225000 (17%)] Loss: 16565.359375\n",
      "Train Epoch: 336 [40128/225000 (18%)] Loss: 16762.714844\n",
      "Train Epoch: 336 [42624/225000 (19%)] Loss: 16462.394531\n",
      "Train Epoch: 336 [45120/225000 (20%)] Loss: 16841.003906\n",
      "Train Epoch: 336 [47616/225000 (21%)] Loss: 16864.646484\n",
      "Train Epoch: 336 [50112/225000 (22%)] Loss: 16897.097656\n",
      "Train Epoch: 336 [52608/225000 (23%)] Loss: 16917.843750\n",
      "Train Epoch: 336 [55104/225000 (24%)] Loss: 17071.691406\n",
      "Train Epoch: 336 [57600/225000 (26%)] Loss: 16746.347656\n",
      "Train Epoch: 336 [60096/225000 (27%)] Loss: 16636.009766\n",
      "Train Epoch: 336 [62592/225000 (28%)] Loss: 17202.289062\n",
      "Train Epoch: 336 [65088/225000 (29%)] Loss: 16817.087891\n",
      "Train Epoch: 336 [67584/225000 (30%)] Loss: 17246.347656\n",
      "Train Epoch: 336 [70080/225000 (31%)] Loss: 16689.761719\n",
      "Train Epoch: 336 [72576/225000 (32%)] Loss: 16532.585938\n",
      "Train Epoch: 336 [75072/225000 (33%)] Loss: 16383.650391\n",
      "Train Epoch: 336 [77568/225000 (34%)] Loss: 16551.683594\n",
      "Train Epoch: 336 [80064/225000 (36%)] Loss: 16988.210938\n",
      "Train Epoch: 336 [82560/225000 (37%)] Loss: 16873.406250\n",
      "Train Epoch: 336 [85056/225000 (38%)] Loss: 16709.156250\n",
      "Train Epoch: 336 [87552/225000 (39%)] Loss: 16432.480469\n",
      "Train Epoch: 336 [90048/225000 (40%)] Loss: 16813.691406\n",
      "Train Epoch: 336 [92544/225000 (41%)] Loss: 17352.937500\n",
      "Train Epoch: 336 [95040/225000 (42%)] Loss: 18349.835938\n",
      "Train Epoch: 336 [97536/225000 (43%)] Loss: 16055.269531\n",
      "Train Epoch: 336 [100032/225000 (44%)] Loss: 16617.882812\n",
      "Train Epoch: 336 [102528/225000 (46%)] Loss: 16818.160156\n",
      "Train Epoch: 336 [105024/225000 (47%)] Loss: 17094.644531\n",
      "Train Epoch: 336 [107520/225000 (48%)] Loss: 17076.589844\n",
      "Train Epoch: 336 [110016/225000 (49%)] Loss: 16521.875000\n",
      "Train Epoch: 336 [112512/225000 (50%)] Loss: 16808.226562\n",
      "Train Epoch: 336 [115008/225000 (51%)] Loss: 16676.751953\n",
      "Train Epoch: 336 [117504/225000 (52%)] Loss: 16971.806641\n",
      "Train Epoch: 336 [120000/225000 (53%)] Loss: 16992.658203\n",
      "Train Epoch: 336 [122496/225000 (54%)] Loss: 16057.788086\n",
      "Train Epoch: 336 [124992/225000 (56%)] Loss: 16664.679688\n",
      "Train Epoch: 336 [127488/225000 (57%)] Loss: 16422.843750\n",
      "Train Epoch: 336 [129984/225000 (58%)] Loss: 16678.974609\n",
      "Train Epoch: 336 [132480/225000 (59%)] Loss: 17139.281250\n",
      "Train Epoch: 336 [134976/225000 (60%)] Loss: 16416.744141\n",
      "Train Epoch: 336 [137472/225000 (61%)] Loss: 16697.621094\n",
      "Train Epoch: 336 [139968/225000 (62%)] Loss: 17219.304688\n",
      "Train Epoch: 336 [142464/225000 (63%)] Loss: 16802.957031\n",
      "Train Epoch: 336 [144960/225000 (64%)] Loss: 16852.679688\n",
      "Train Epoch: 336 [147456/225000 (66%)] Loss: 17251.269531\n",
      "Train Epoch: 336 [149952/225000 (67%)] Loss: 18651.228516\n",
      "Train Epoch: 336 [152448/225000 (68%)] Loss: 16429.570312\n",
      "Train Epoch: 336 [154944/225000 (69%)] Loss: 17118.542969\n",
      "Train Epoch: 336 [157440/225000 (70%)] Loss: 16476.867188\n",
      "Train Epoch: 336 [159936/225000 (71%)] Loss: 16921.990234\n",
      "Train Epoch: 336 [162432/225000 (72%)] Loss: 16838.703125\n",
      "Train Epoch: 336 [164928/225000 (73%)] Loss: 16594.757812\n",
      "Train Epoch: 336 [167424/225000 (74%)] Loss: 16948.828125\n",
      "Train Epoch: 336 [169920/225000 (76%)] Loss: 17016.488281\n",
      "Train Epoch: 336 [172416/225000 (77%)] Loss: 16745.492188\n",
      "Train Epoch: 336 [174912/225000 (78%)] Loss: 16243.348633\n",
      "Train Epoch: 336 [177408/225000 (79%)] Loss: 16723.992188\n",
      "Train Epoch: 336 [179904/225000 (80%)] Loss: 16941.730469\n",
      "Train Epoch: 336 [182400/225000 (81%)] Loss: 17000.294922\n",
      "Train Epoch: 336 [184896/225000 (82%)] Loss: 16956.808594\n",
      "Train Epoch: 336 [187392/225000 (83%)] Loss: 16893.279297\n",
      "Train Epoch: 336 [189888/225000 (84%)] Loss: 16706.750000\n",
      "Train Epoch: 336 [192384/225000 (86%)] Loss: 16873.585938\n",
      "Train Epoch: 336 [194880/225000 (87%)] Loss: 16399.281250\n",
      "Train Epoch: 336 [197376/225000 (88%)] Loss: 16800.888672\n",
      "Train Epoch: 336 [199872/225000 (89%)] Loss: 16854.333984\n",
      "Train Epoch: 336 [202368/225000 (90%)] Loss: 16302.693359\n",
      "Train Epoch: 336 [204864/225000 (91%)] Loss: 16126.863281\n",
      "Train Epoch: 336 [207360/225000 (92%)] Loss: 16777.632812\n",
      "Train Epoch: 336 [209856/225000 (93%)] Loss: 16849.398438\n",
      "Train Epoch: 336 [212352/225000 (94%)] Loss: 16568.273438\n",
      "Train Epoch: 336 [214848/225000 (95%)] Loss: 16688.687500\n",
      "Train Epoch: 336 [217344/225000 (97%)] Loss: 17257.394531\n",
      "Train Epoch: 336 [219840/225000 (98%)] Loss: 16777.556641\n",
      "Train Epoch: 336 [222336/225000 (99%)] Loss: 16663.878906\n",
      "Train Epoch: 336 [224832/225000 (100%)] Loss: 16861.347656\n",
      "    epoch          : 336\n",
      "    loss           : 16825.537907623187\n",
      "    val_loss       : 16782.92131489925\n",
      "Train Epoch: 337 [192/225000 (0%)] Loss: 16486.558594\n",
      "Train Epoch: 337 [2688/225000 (1%)] Loss: 16981.773438\n",
      "Train Epoch: 337 [5184/225000 (2%)] Loss: 16788.882812\n",
      "Train Epoch: 337 [7680/225000 (3%)] Loss: 16860.816406\n",
      "Train Epoch: 337 [10176/225000 (5%)] Loss: 16660.464844\n",
      "Train Epoch: 337 [12672/225000 (6%)] Loss: 17961.605469\n",
      "Train Epoch: 337 [15168/225000 (7%)] Loss: 16640.187500\n",
      "Train Epoch: 337 [17664/225000 (8%)] Loss: 16506.718750\n",
      "Train Epoch: 337 [20160/225000 (9%)] Loss: 16874.625000\n",
      "Train Epoch: 337 [22656/225000 (10%)] Loss: 16619.414062\n",
      "Train Epoch: 337 [25152/225000 (11%)] Loss: 16493.419922\n",
      "Train Epoch: 337 [27648/225000 (12%)] Loss: 16731.998047\n",
      "Train Epoch: 337 [30144/225000 (13%)] Loss: 16864.871094\n",
      "Train Epoch: 337 [32640/225000 (15%)] Loss: 16825.632812\n",
      "Train Epoch: 337 [35136/225000 (16%)] Loss: 16922.171875\n",
      "Train Epoch: 337 [37632/225000 (17%)] Loss: 17182.042969\n",
      "Train Epoch: 337 [40128/225000 (18%)] Loss: 16307.705078\n",
      "Train Epoch: 337 [42624/225000 (19%)] Loss: 16911.597656\n",
      "Train Epoch: 337 [45120/225000 (20%)] Loss: 16645.632812\n",
      "Train Epoch: 337 [47616/225000 (21%)] Loss: 16776.914062\n",
      "Train Epoch: 337 [50112/225000 (22%)] Loss: 16572.171875\n",
      "Train Epoch: 337 [52608/225000 (23%)] Loss: 16747.183594\n",
      "Train Epoch: 337 [55104/225000 (24%)] Loss: 16580.441406\n",
      "Train Epoch: 337 [57600/225000 (26%)] Loss: 16914.441406\n",
      "Train Epoch: 337 [60096/225000 (27%)] Loss: 16589.996094\n",
      "Train Epoch: 337 [62592/225000 (28%)] Loss: 17197.640625\n",
      "Train Epoch: 337 [65088/225000 (29%)] Loss: 16487.632812\n",
      "Train Epoch: 337 [67584/225000 (30%)] Loss: 16746.058594\n",
      "Train Epoch: 337 [70080/225000 (31%)] Loss: 16769.316406\n",
      "Train Epoch: 337 [72576/225000 (32%)] Loss: 16895.589844\n",
      "Train Epoch: 337 [75072/225000 (33%)] Loss: 16706.355469\n",
      "Train Epoch: 337 [77568/225000 (34%)] Loss: 16404.007812\n",
      "Train Epoch: 337 [80064/225000 (36%)] Loss: 18419.722656\n",
      "Train Epoch: 337 [82560/225000 (37%)] Loss: 16758.222656\n",
      "Train Epoch: 337 [85056/225000 (38%)] Loss: 16819.484375\n",
      "Train Epoch: 337 [87552/225000 (39%)] Loss: 16824.050781\n",
      "Train Epoch: 337 [90048/225000 (40%)] Loss: 16756.361328\n",
      "Train Epoch: 337 [92544/225000 (41%)] Loss: 16833.371094\n",
      "Train Epoch: 337 [95040/225000 (42%)] Loss: 17101.625000\n",
      "Train Epoch: 337 [97536/225000 (43%)] Loss: 16695.371094\n",
      "Train Epoch: 337 [100032/225000 (44%)] Loss: 16648.126953\n",
      "Train Epoch: 337 [102528/225000 (46%)] Loss: 16950.484375\n",
      "Train Epoch: 337 [105024/225000 (47%)] Loss: 17273.447266\n",
      "Train Epoch: 337 [107520/225000 (48%)] Loss: 17063.109375\n",
      "Train Epoch: 337 [110016/225000 (49%)] Loss: 16410.353516\n",
      "Train Epoch: 337 [112512/225000 (50%)] Loss: 16781.449219\n",
      "Train Epoch: 337 [115008/225000 (51%)] Loss: 16966.421875\n",
      "Train Epoch: 337 [117504/225000 (52%)] Loss: 16662.515625\n",
      "Train Epoch: 337 [120000/225000 (53%)] Loss: 17011.822266\n",
      "Train Epoch: 337 [122496/225000 (54%)] Loss: 17437.414062\n",
      "Train Epoch: 337 [124992/225000 (56%)] Loss: 16447.285156\n",
      "Train Epoch: 337 [127488/225000 (57%)] Loss: 16946.558594\n",
      "Train Epoch: 337 [129984/225000 (58%)] Loss: 16604.832031\n",
      "Train Epoch: 337 [132480/225000 (59%)] Loss: 16728.625000\n",
      "Train Epoch: 337 [134976/225000 (60%)] Loss: 16973.308594\n",
      "Train Epoch: 337 [137472/225000 (61%)] Loss: 16713.230469\n",
      "Train Epoch: 337 [139968/225000 (62%)] Loss: 17168.412109\n",
      "Train Epoch: 337 [142464/225000 (63%)] Loss: 17184.476562\n",
      "Train Epoch: 337 [144960/225000 (64%)] Loss: 16560.582031\n",
      "Train Epoch: 337 [147456/225000 (66%)] Loss: 16767.203125\n",
      "Train Epoch: 337 [149952/225000 (67%)] Loss: 16938.224609\n",
      "Train Epoch: 337 [152448/225000 (68%)] Loss: 18323.734375\n",
      "Train Epoch: 337 [154944/225000 (69%)] Loss: 16766.140625\n",
      "Train Epoch: 337 [157440/225000 (70%)] Loss: 16906.845703\n",
      "Train Epoch: 337 [159936/225000 (71%)] Loss: 16874.255859\n",
      "Train Epoch: 337 [162432/225000 (72%)] Loss: 16890.890625\n",
      "Train Epoch: 337 [164928/225000 (73%)] Loss: 18289.132812\n",
      "Train Epoch: 337 [167424/225000 (74%)] Loss: 17275.851562\n",
      "Train Epoch: 337 [169920/225000 (76%)] Loss: 16340.003906\n",
      "Train Epoch: 337 [172416/225000 (77%)] Loss: 16922.667969\n",
      "Train Epoch: 337 [174912/225000 (78%)] Loss: 16999.371094\n",
      "Train Epoch: 337 [177408/225000 (79%)] Loss: 17106.226562\n",
      "Train Epoch: 337 [179904/225000 (80%)] Loss: 16745.271484\n",
      "Train Epoch: 337 [182400/225000 (81%)] Loss: 16685.939453\n",
      "Train Epoch: 337 [184896/225000 (82%)] Loss: 16829.462891\n",
      "Train Epoch: 337 [187392/225000 (83%)] Loss: 16919.314453\n",
      "Train Epoch: 337 [189888/225000 (84%)] Loss: 16979.371094\n",
      "Train Epoch: 337 [192384/225000 (86%)] Loss: 16518.658203\n",
      "Train Epoch: 337 [194880/225000 (87%)] Loss: 16467.792969\n",
      "Train Epoch: 337 [197376/225000 (88%)] Loss: 17020.115234\n",
      "Train Epoch: 337 [199872/225000 (89%)] Loss: 16752.558594\n",
      "Train Epoch: 337 [202368/225000 (90%)] Loss: 16509.320312\n",
      "Train Epoch: 337 [204864/225000 (91%)] Loss: 16569.203125\n",
      "Train Epoch: 337 [207360/225000 (92%)] Loss: 16630.183594\n",
      "Train Epoch: 337 [209856/225000 (93%)] Loss: 16918.855469\n",
      "Train Epoch: 337 [212352/225000 (94%)] Loss: 16481.777344\n",
      "Train Epoch: 337 [214848/225000 (95%)] Loss: 16835.501953\n",
      "Train Epoch: 337 [217344/225000 (97%)] Loss: 16819.914062\n",
      "Train Epoch: 337 [219840/225000 (98%)] Loss: 16682.763672\n",
      "Train Epoch: 337 [222336/225000 (99%)] Loss: 17162.398438\n",
      "Train Epoch: 337 [224832/225000 (100%)] Loss: 16929.333984\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "    epoch          : 337\n",
      "    loss           : 16843.894087130706\n",
      "    val_loss       : 16824.420734753134\n",
      "Train Epoch: 338 [192/225000 (0%)] Loss: 16968.265625\n",
      "Train Epoch: 338 [2688/225000 (1%)] Loss: 17036.037109\n",
      "Train Epoch: 338 [5184/225000 (2%)] Loss: 16494.291016\n",
      "Train Epoch: 338 [7680/225000 (3%)] Loss: 16589.031250\n",
      "Train Epoch: 338 [10176/225000 (5%)] Loss: 16911.724609\n",
      "Train Epoch: 338 [12672/225000 (6%)] Loss: 16912.390625\n",
      "Train Epoch: 338 [15168/225000 (7%)] Loss: 17005.083984\n",
      "Train Epoch: 338 [17664/225000 (8%)] Loss: 16539.746094\n",
      "Train Epoch: 338 [20160/225000 (9%)] Loss: 16681.714844\n",
      "Train Epoch: 338 [22656/225000 (10%)] Loss: 16837.945312\n",
      "Train Epoch: 338 [25152/225000 (11%)] Loss: 16382.793945\n",
      "Train Epoch: 338 [27648/225000 (12%)] Loss: 16498.605469\n",
      "Train Epoch: 338 [30144/225000 (13%)] Loss: 16932.613281\n",
      "Train Epoch: 338 [32640/225000 (15%)] Loss: 16510.777344\n",
      "Train Epoch: 338 [35136/225000 (16%)] Loss: 16707.144531\n",
      "Train Epoch: 338 [37632/225000 (17%)] Loss: 16465.523438\n",
      "Train Epoch: 338 [40128/225000 (18%)] Loss: 16727.142578\n",
      "Train Epoch: 338 [42624/225000 (19%)] Loss: 16843.679688\n",
      "Train Epoch: 338 [45120/225000 (20%)] Loss: 18382.927734\n",
      "Train Epoch: 338 [47616/225000 (21%)] Loss: 16761.324219\n",
      "Train Epoch: 338 [50112/225000 (22%)] Loss: 16714.312500\n",
      "Train Epoch: 338 [52608/225000 (23%)] Loss: 16524.046875\n",
      "Train Epoch: 338 [55104/225000 (24%)] Loss: 16839.566406\n",
      "Train Epoch: 338 [57600/225000 (26%)] Loss: 16849.695312\n",
      "Train Epoch: 338 [60096/225000 (27%)] Loss: 16944.902344\n",
      "Train Epoch: 338 [62592/225000 (28%)] Loss: 15998.301758\n",
      "Train Epoch: 338 [65088/225000 (29%)] Loss: 16862.087891\n",
      "Train Epoch: 338 [67584/225000 (30%)] Loss: 16711.222656\n",
      "Train Epoch: 338 [70080/225000 (31%)] Loss: 16739.748047\n",
      "Train Epoch: 338 [72576/225000 (32%)] Loss: 16599.683594\n",
      "Train Epoch: 338 [75072/225000 (33%)] Loss: 16828.132812\n",
      "Train Epoch: 338 [77568/225000 (34%)] Loss: 16686.343750\n",
      "Train Epoch: 338 [80064/225000 (36%)] Loss: 16311.432617\n",
      "Train Epoch: 338 [82560/225000 (37%)] Loss: 16656.246094\n",
      "Train Epoch: 338 [85056/225000 (38%)] Loss: 16567.214844\n",
      "Train Epoch: 338 [87552/225000 (39%)] Loss: 16651.097656\n",
      "Train Epoch: 338 [90048/225000 (40%)] Loss: 16693.267578\n",
      "Train Epoch: 338 [92544/225000 (41%)] Loss: 16863.880859\n",
      "Train Epoch: 338 [95040/225000 (42%)] Loss: 16887.392578\n",
      "Train Epoch: 338 [97536/225000 (43%)] Loss: 16817.796875\n",
      "Train Epoch: 338 [100032/225000 (44%)] Loss: 17287.044922\n",
      "Train Epoch: 338 [102528/225000 (46%)] Loss: 16424.697266\n",
      "Train Epoch: 338 [105024/225000 (47%)] Loss: 16701.429688\n",
      "Train Epoch: 338 [107520/225000 (48%)] Loss: 16594.931641\n",
      "Train Epoch: 338 [110016/225000 (49%)] Loss: 16405.316406\n",
      "Train Epoch: 338 [112512/225000 (50%)] Loss: 16484.070312\n",
      "Train Epoch: 338 [115008/225000 (51%)] Loss: 16415.615234\n",
      "Train Epoch: 338 [117504/225000 (52%)] Loss: 16451.128906\n",
      "Train Epoch: 338 [120000/225000 (53%)] Loss: 16967.880859\n",
      "Train Epoch: 338 [122496/225000 (54%)] Loss: 16560.695312\n",
      "Train Epoch: 338 [124992/225000 (56%)] Loss: 16973.900391\n",
      "Train Epoch: 338 [127488/225000 (57%)] Loss: 16454.039062\n",
      "Train Epoch: 338 [129984/225000 (58%)] Loss: 17210.322266\n",
      "Train Epoch: 338 [132480/225000 (59%)] Loss: 16578.458984\n",
      "Train Epoch: 338 [134976/225000 (60%)] Loss: 16471.496094\n",
      "Train Epoch: 338 [137472/225000 (61%)] Loss: 16662.660156\n",
      "Train Epoch: 338 [139968/225000 (62%)] Loss: 16482.876953\n",
      "Train Epoch: 338 [142464/225000 (63%)] Loss: 19100.330078\n",
      "Train Epoch: 338 [144960/225000 (64%)] Loss: 16802.878906\n",
      "Train Epoch: 338 [147456/225000 (66%)] Loss: 16853.570312\n",
      "Train Epoch: 338 [149952/225000 (67%)] Loss: 16944.042969\n",
      "Train Epoch: 338 [152448/225000 (68%)] Loss: 16885.812500\n",
      "Train Epoch: 338 [154944/225000 (69%)] Loss: 16934.832031\n",
      "Train Epoch: 338 [157440/225000 (70%)] Loss: 16986.765625\n",
      "Train Epoch: 338 [159936/225000 (71%)] Loss: 16506.298828\n",
      "Train Epoch: 338 [162432/225000 (72%)] Loss: 17014.156250\n",
      "Train Epoch: 338 [164928/225000 (73%)] Loss: 17145.800781\n",
      "Train Epoch: 338 [167424/225000 (74%)] Loss: 16807.457031\n",
      "Train Epoch: 338 [169920/225000 (76%)] Loss: 16708.070312\n",
      "Train Epoch: 338 [172416/225000 (77%)] Loss: 16428.851562\n",
      "Train Epoch: 338 [174912/225000 (78%)] Loss: 16365.453125\n",
      "Train Epoch: 338 [177408/225000 (79%)] Loss: 16392.078125\n",
      "Train Epoch: 338 [179904/225000 (80%)] Loss: 16593.507812\n",
      "Train Epoch: 338 [182400/225000 (81%)] Loss: 17274.072266\n",
      "Train Epoch: 338 [184896/225000 (82%)] Loss: 16692.828125\n",
      "Train Epoch: 338 [187392/225000 (83%)] Loss: 16272.912109\n",
      "Train Epoch: 338 [189888/225000 (84%)] Loss: 18186.066406\n",
      "Train Epoch: 338 [192384/225000 (86%)] Loss: 17094.882812\n",
      "Train Epoch: 338 [194880/225000 (87%)] Loss: 16298.614258\n",
      "Train Epoch: 338 [197376/225000 (88%)] Loss: 16335.840820\n",
      "Train Epoch: 338 [199872/225000 (89%)] Loss: 16873.070312\n",
      "Train Epoch: 338 [202368/225000 (90%)] Loss: 16439.179688\n",
      "Train Epoch: 338 [204864/225000 (91%)] Loss: 16841.449219\n",
      "Train Epoch: 338 [207360/225000 (92%)] Loss: 16618.839844\n",
      "Train Epoch: 338 [209856/225000 (93%)] Loss: 16573.542969\n",
      "Train Epoch: 338 [212352/225000 (94%)] Loss: 17023.332031\n",
      "Train Epoch: 338 [214848/225000 (95%)] Loss: 16453.371094\n",
      "Train Epoch: 338 [217344/225000 (97%)] Loss: 16643.117188\n",
      "Train Epoch: 338 [219840/225000 (98%)] Loss: 16395.904297\n",
      "Train Epoch: 338 [222336/225000 (99%)] Loss: 16785.496094\n",
      "Train Epoch: 338 [224832/225000 (100%)] Loss: 17141.121094\n",
      "    epoch          : 338\n",
      "    loss           : 16756.994315606335\n",
      "    val_loss       : 16676.557244970598\n",
      "Train Epoch: 339 [192/225000 (0%)] Loss: 16685.435547\n",
      "Train Epoch: 339 [2688/225000 (1%)] Loss: 16681.265625\n",
      "Train Epoch: 339 [5184/225000 (2%)] Loss: 16521.410156\n",
      "Train Epoch: 339 [7680/225000 (3%)] Loss: 16602.191406\n",
      "Train Epoch: 339 [10176/225000 (5%)] Loss: 16763.675781\n",
      "Train Epoch: 339 [12672/225000 (6%)] Loss: 16635.242188\n",
      "Train Epoch: 339 [15168/225000 (7%)] Loss: 16869.373047\n",
      "Train Epoch: 339 [17664/225000 (8%)] Loss: 16844.089844\n",
      "Train Epoch: 339 [20160/225000 (9%)] Loss: 17187.648438\n",
      "Train Epoch: 339 [22656/225000 (10%)] Loss: 16893.160156\n",
      "Train Epoch: 339 [25152/225000 (11%)] Loss: 16625.220703\n",
      "Train Epoch: 339 [27648/225000 (12%)] Loss: 16625.425781\n",
      "Train Epoch: 339 [30144/225000 (13%)] Loss: 16660.652344\n",
      "Train Epoch: 339 [32640/225000 (15%)] Loss: 16502.062500\n",
      "Train Epoch: 339 [35136/225000 (16%)] Loss: 16556.093750\n",
      "Train Epoch: 339 [37632/225000 (17%)] Loss: 16551.062500\n",
      "Train Epoch: 339 [40128/225000 (18%)] Loss: 16328.792969\n",
      "Train Epoch: 339 [42624/225000 (19%)] Loss: 16966.789062\n",
      "Train Epoch: 339 [45120/225000 (20%)] Loss: 16779.001953\n",
      "Train Epoch: 339 [47616/225000 (21%)] Loss: 16581.277344\n",
      "Train Epoch: 339 [50112/225000 (22%)] Loss: 16713.955078\n",
      "Train Epoch: 339 [52608/225000 (23%)] Loss: 17159.404297\n",
      "Train Epoch: 339 [55104/225000 (24%)] Loss: 16653.820312\n",
      "Train Epoch: 339 [57600/225000 (26%)] Loss: 16289.901367\n",
      "Train Epoch: 339 [60096/225000 (27%)] Loss: 18233.263672\n",
      "Train Epoch: 339 [62592/225000 (28%)] Loss: 16700.371094\n",
      "Train Epoch: 339 [65088/225000 (29%)] Loss: 16452.546875\n",
      "Train Epoch: 339 [67584/225000 (30%)] Loss: 16116.125977\n",
      "Train Epoch: 339 [70080/225000 (31%)] Loss: 16859.371094\n",
      "Train Epoch: 339 [72576/225000 (32%)] Loss: 16380.033203\n",
      "Train Epoch: 339 [75072/225000 (33%)] Loss: 16868.410156\n",
      "Train Epoch: 339 [77568/225000 (34%)] Loss: 16737.339844\n",
      "Train Epoch: 339 [80064/225000 (36%)] Loss: 16041.410156\n",
      "Train Epoch: 339 [82560/225000 (37%)] Loss: 16443.935547\n",
      "Train Epoch: 339 [85056/225000 (38%)] Loss: 16483.234375\n",
      "Train Epoch: 339 [87552/225000 (39%)] Loss: 16479.517578\n",
      "Train Epoch: 339 [90048/225000 (40%)] Loss: 15832.567383\n",
      "Train Epoch: 339 [92544/225000 (41%)] Loss: 16853.560547\n",
      "Train Epoch: 339 [95040/225000 (42%)] Loss: 16624.488281\n",
      "Train Epoch: 339 [97536/225000 (43%)] Loss: 16392.562500\n",
      "Train Epoch: 339 [100032/225000 (44%)] Loss: 16601.492188\n",
      "Train Epoch: 339 [102528/225000 (46%)] Loss: 16756.662109\n",
      "Train Epoch: 339 [105024/225000 (47%)] Loss: 16278.304688\n",
      "Train Epoch: 339 [107520/225000 (48%)] Loss: 16938.478516\n",
      "Train Epoch: 339 [110016/225000 (49%)] Loss: 18207.849609\n",
      "Train Epoch: 339 [112512/225000 (50%)] Loss: 17138.587891\n",
      "Train Epoch: 339 [115008/225000 (51%)] Loss: 16607.892578\n",
      "Train Epoch: 339 [117504/225000 (52%)] Loss: 16304.279297\n",
      "Train Epoch: 339 [120000/225000 (53%)] Loss: 16673.597656\n",
      "Train Epoch: 339 [122496/225000 (54%)] Loss: 17001.457031\n",
      "Train Epoch: 339 [124992/225000 (56%)] Loss: 16782.449219\n",
      "Train Epoch: 339 [127488/225000 (57%)] Loss: 16367.134766\n",
      "Train Epoch: 339 [129984/225000 (58%)] Loss: 16949.105469\n",
      "Train Epoch: 339 [132480/225000 (59%)] Loss: 16634.820312\n",
      "Train Epoch: 339 [134976/225000 (60%)] Loss: 16480.316406\n",
      "Train Epoch: 339 [137472/225000 (61%)] Loss: 16743.125000\n",
      "Train Epoch: 339 [139968/225000 (62%)] Loss: 16321.236328\n",
      "Train Epoch: 339 [142464/225000 (63%)] Loss: 17136.113281\n",
      "Train Epoch: 339 [144960/225000 (64%)] Loss: 16581.718750\n",
      "Train Epoch: 339 [147456/225000 (66%)] Loss: 17009.113281\n",
      "Train Epoch: 339 [149952/225000 (67%)] Loss: 18105.666016\n",
      "Train Epoch: 339 [152448/225000 (68%)] Loss: 17086.542969\n",
      "Train Epoch: 339 [154944/225000 (69%)] Loss: 16875.863281\n",
      "Train Epoch: 339 [157440/225000 (70%)] Loss: 16428.699219\n",
      "Train Epoch: 339 [159936/225000 (71%)] Loss: 16690.875000\n",
      "Train Epoch: 339 [162432/225000 (72%)] Loss: 17088.224609\n",
      "Train Epoch: 339 [164928/225000 (73%)] Loss: 16708.419922\n",
      "Train Epoch: 339 [167424/225000 (74%)] Loss: 16221.620117\n",
      "Train Epoch: 339 [169920/225000 (76%)] Loss: 16370.938477\n",
      "Train Epoch: 339 [172416/225000 (77%)] Loss: 16603.511719\n",
      "Train Epoch: 339 [174912/225000 (78%)] Loss: 16837.035156\n",
      "Train Epoch: 339 [177408/225000 (79%)] Loss: 16623.042969\n",
      "Train Epoch: 339 [179904/225000 (80%)] Loss: 16507.847656\n",
      "Train Epoch: 339 [182400/225000 (81%)] Loss: 16871.947266\n",
      "Train Epoch: 339 [184896/225000 (82%)] Loss: 16551.396484\n",
      "Train Epoch: 339 [187392/225000 (83%)] Loss: 16651.972656\n",
      "Train Epoch: 339 [189888/225000 (84%)] Loss: 16567.066406\n",
      "Train Epoch: 339 [192384/225000 (86%)] Loss: 16887.156250\n",
      "Train Epoch: 339 [194880/225000 (87%)] Loss: 16876.769531\n",
      "Train Epoch: 339 [197376/225000 (88%)] Loss: 16534.380859\n",
      "Train Epoch: 339 [199872/225000 (89%)] Loss: 18285.968750\n",
      "Train Epoch: 339 [202368/225000 (90%)] Loss: 16186.167969\n",
      "Train Epoch: 339 [204864/225000 (91%)] Loss: 16277.223633\n",
      "Train Epoch: 339 [207360/225000 (92%)] Loss: 17007.894531\n",
      "Train Epoch: 339 [209856/225000 (93%)] Loss: 18454.988281\n",
      "Train Epoch: 339 [212352/225000 (94%)] Loss: 16422.855469\n",
      "Train Epoch: 339 [214848/225000 (95%)] Loss: 16821.033203\n",
      "Train Epoch: 339 [217344/225000 (97%)] Loss: 16751.980469\n",
      "Train Epoch: 339 [219840/225000 (98%)] Loss: 16547.562500\n",
      "Train Epoch: 339 [222336/225000 (99%)] Loss: 16905.464844\n",
      "Train Epoch: 339 [224832/225000 (100%)] Loss: 16272.538086\n",
      "    epoch          : 339\n",
      "    loss           : 16715.760008099136\n",
      "    val_loss       : 16655.95811160466\n",
      "Train Epoch: 340 [192/225000 (0%)] Loss: 16426.531250\n",
      "Train Epoch: 340 [2688/225000 (1%)] Loss: 16498.250000\n",
      "Train Epoch: 340 [5184/225000 (2%)] Loss: 16596.593750\n",
      "Train Epoch: 340 [7680/225000 (3%)] Loss: 16363.342773\n",
      "Train Epoch: 340 [10176/225000 (5%)] Loss: 16558.628906\n",
      "Train Epoch: 340 [12672/225000 (6%)] Loss: 16834.982422\n",
      "Train Epoch: 340 [15168/225000 (7%)] Loss: 17034.820312\n",
      "Train Epoch: 340 [17664/225000 (8%)] Loss: 16549.675781\n",
      "Train Epoch: 340 [20160/225000 (9%)] Loss: 16752.958984\n",
      "Train Epoch: 340 [22656/225000 (10%)] Loss: 16816.781250\n",
      "Train Epoch: 340 [25152/225000 (11%)] Loss: 16938.519531\n",
      "Train Epoch: 340 [27648/225000 (12%)] Loss: 16526.890625\n",
      "Train Epoch: 340 [30144/225000 (13%)] Loss: 16943.365234\n",
      "Train Epoch: 340 [32640/225000 (15%)] Loss: 16527.523438\n",
      "Train Epoch: 340 [35136/225000 (16%)] Loss: 16245.037109\n",
      "Train Epoch: 340 [37632/225000 (17%)] Loss: 16166.537109\n",
      "Train Epoch: 340 [40128/225000 (18%)] Loss: 17060.714844\n",
      "Train Epoch: 340 [42624/225000 (19%)] Loss: 16507.884766\n",
      "Train Epoch: 340 [45120/225000 (20%)] Loss: 18206.832031\n",
      "Train Epoch: 340 [47616/225000 (21%)] Loss: 16761.914062\n",
      "Train Epoch: 340 [50112/225000 (22%)] Loss: 17594.351562\n",
      "Train Epoch: 340 [52608/225000 (23%)] Loss: 16834.929688\n",
      "Train Epoch: 340 [55104/225000 (24%)] Loss: 16783.025391\n",
      "Train Epoch: 340 [57600/225000 (26%)] Loss: 17225.960938\n",
      "Train Epoch: 340 [60096/225000 (27%)] Loss: 16786.353516\n",
      "Train Epoch: 340 [62592/225000 (28%)] Loss: 16255.812500\n",
      "Train Epoch: 340 [65088/225000 (29%)] Loss: 16890.320312\n",
      "Train Epoch: 340 [67584/225000 (30%)] Loss: 16779.164062\n",
      "Train Epoch: 340 [70080/225000 (31%)] Loss: 17116.425781\n",
      "Train Epoch: 340 [72576/225000 (32%)] Loss: 17064.195312\n",
      "Train Epoch: 340 [75072/225000 (33%)] Loss: 16186.039062\n",
      "Train Epoch: 340 [77568/225000 (34%)] Loss: 16917.476562\n",
      "Train Epoch: 340 [80064/225000 (36%)] Loss: 16498.291016\n",
      "Train Epoch: 340 [82560/225000 (37%)] Loss: 18360.878906\n",
      "Train Epoch: 340 [85056/225000 (38%)] Loss: 16791.125000\n",
      "Train Epoch: 340 [87552/225000 (39%)] Loss: 16720.257812\n",
      "Train Epoch: 340 [90048/225000 (40%)] Loss: 16521.355469\n",
      "Train Epoch: 340 [92544/225000 (41%)] Loss: 18153.857422\n",
      "Train Epoch: 340 [95040/225000 (42%)] Loss: 17165.095703\n",
      "Train Epoch: 340 [97536/225000 (43%)] Loss: 16796.839844\n",
      "Train Epoch: 340 [100032/225000 (44%)] Loss: 16437.509766\n",
      "Train Epoch: 340 [102528/225000 (46%)] Loss: 17008.080078\n",
      "Train Epoch: 340 [105024/225000 (47%)] Loss: 16199.260742\n",
      "Train Epoch: 340 [107520/225000 (48%)] Loss: 16464.058594\n",
      "Train Epoch: 340 [110016/225000 (49%)] Loss: 18358.699219\n",
      "Train Epoch: 340 [112512/225000 (50%)] Loss: 17003.951172\n",
      "Train Epoch: 340 [115008/225000 (51%)] Loss: 17263.277344\n",
      "Train Epoch: 340 [117504/225000 (52%)] Loss: 16542.500000\n",
      "Train Epoch: 340 [120000/225000 (53%)] Loss: 16469.148438\n",
      "Train Epoch: 340 [122496/225000 (54%)] Loss: 16909.363281\n",
      "Train Epoch: 340 [124992/225000 (56%)] Loss: 16882.394531\n",
      "Train Epoch: 340 [127488/225000 (57%)] Loss: 15954.415039\n",
      "Train Epoch: 340 [129984/225000 (58%)] Loss: 17179.828125\n",
      "Train Epoch: 340 [132480/225000 (59%)] Loss: 16383.565430\n",
      "Train Epoch: 340 [134976/225000 (60%)] Loss: 16798.925781\n",
      "Train Epoch: 340 [137472/225000 (61%)] Loss: 17001.173828\n",
      "Train Epoch: 340 [139968/225000 (62%)] Loss: 16892.601562\n",
      "Train Epoch: 340 [142464/225000 (63%)] Loss: 16502.652344\n",
      "Train Epoch: 340 [144960/225000 (64%)] Loss: 16405.193359\n",
      "Train Epoch: 340 [147456/225000 (66%)] Loss: 16454.744141\n",
      "Train Epoch: 340 [149952/225000 (67%)] Loss: 16518.003906\n",
      "Train Epoch: 340 [152448/225000 (68%)] Loss: 16646.460938\n",
      "Train Epoch: 340 [154944/225000 (69%)] Loss: 16545.421875\n",
      "Train Epoch: 340 [157440/225000 (70%)] Loss: 16879.410156\n",
      "Train Epoch: 340 [159936/225000 (71%)] Loss: 16923.857422\n",
      "Train Epoch: 340 [162432/225000 (72%)] Loss: 16552.984375\n",
      "Train Epoch: 340 [164928/225000 (73%)] Loss: 16556.058594\n",
      "Train Epoch: 340 [167424/225000 (74%)] Loss: 16783.730469\n",
      "Train Epoch: 340 [169920/225000 (76%)] Loss: 16430.777344\n",
      "Train Epoch: 340 [172416/225000 (77%)] Loss: 16251.315430\n",
      "Train Epoch: 340 [174912/225000 (78%)] Loss: 16744.257812\n",
      "Train Epoch: 340 [177408/225000 (79%)] Loss: 16450.886719\n",
      "Train Epoch: 340 [179904/225000 (80%)] Loss: 17237.933594\n",
      "Train Epoch: 340 [182400/225000 (81%)] Loss: 16903.109375\n",
      "Train Epoch: 340 [184896/225000 (82%)] Loss: 17178.333984\n",
      "Train Epoch: 340 [187392/225000 (83%)] Loss: 16456.593750\n",
      "Train Epoch: 340 [189888/225000 (84%)] Loss: 16800.646484\n",
      "Train Epoch: 340 [192384/225000 (86%)] Loss: 16861.048828\n",
      "Train Epoch: 340 [194880/225000 (87%)] Loss: 16737.910156\n",
      "Train Epoch: 340 [197376/225000 (88%)] Loss: 16451.791016\n",
      "Train Epoch: 340 [199872/225000 (89%)] Loss: 16846.773438\n",
      "Train Epoch: 340 [202368/225000 (90%)] Loss: 16967.078125\n",
      "Train Epoch: 340 [204864/225000 (91%)] Loss: 16246.275391\n",
      "Train Epoch: 340 [207360/225000 (92%)] Loss: 17320.296875\n",
      "Train Epoch: 340 [209856/225000 (93%)] Loss: 16283.950195\n",
      "Train Epoch: 340 [212352/225000 (94%)] Loss: 16595.656250\n",
      "Train Epoch: 340 [214848/225000 (95%)] Loss: 16403.156250\n",
      "Train Epoch: 340 [217344/225000 (97%)] Loss: 16261.311523\n",
      "Train Epoch: 340 [219840/225000 (98%)] Loss: 16993.511719\n",
      "Train Epoch: 340 [222336/225000 (99%)] Loss: 16569.816406\n",
      "Train Epoch: 340 [224832/225000 (100%)] Loss: 16578.304688\n",
      "    epoch          : 340\n",
      "    loss           : 16740.509194019305\n",
      "    val_loss       : 16616.806388531022\n",
      "Train Epoch: 341 [192/225000 (0%)] Loss: 17058.429688\n",
      "Train Epoch: 341 [2688/225000 (1%)] Loss: 16913.742188\n",
      "Train Epoch: 341 [5184/225000 (2%)] Loss: 16399.896484\n",
      "Train Epoch: 341 [7680/225000 (3%)] Loss: 16677.726562\n",
      "Train Epoch: 341 [10176/225000 (5%)] Loss: 16190.113281\n",
      "Train Epoch: 341 [12672/225000 (6%)] Loss: 17118.773438\n",
      "Train Epoch: 341 [15168/225000 (7%)] Loss: 16447.824219\n",
      "Train Epoch: 341 [17664/225000 (8%)] Loss: 16299.507812\n",
      "Train Epoch: 341 [20160/225000 (9%)] Loss: 16309.856445\n",
      "Train Epoch: 341 [22656/225000 (10%)] Loss: 16679.218750\n",
      "Train Epoch: 341 [25152/225000 (11%)] Loss: 17330.269531\n",
      "Train Epoch: 341 [27648/225000 (12%)] Loss: 16684.625000\n",
      "Train Epoch: 341 [30144/225000 (13%)] Loss: 16849.646484\n",
      "Train Epoch: 341 [32640/225000 (15%)] Loss: 18204.019531\n",
      "Train Epoch: 341 [35136/225000 (16%)] Loss: 16801.031250\n",
      "Train Epoch: 341 [37632/225000 (17%)] Loss: 17257.667969\n",
      "Train Epoch: 341 [40128/225000 (18%)] Loss: 16523.720703\n",
      "Train Epoch: 341 [42624/225000 (19%)] Loss: 16673.382812\n",
      "Train Epoch: 341 [45120/225000 (20%)] Loss: 17077.769531\n",
      "Train Epoch: 341 [47616/225000 (21%)] Loss: 16840.416016\n",
      "Train Epoch: 341 [50112/225000 (22%)] Loss: 16375.567383\n",
      "Train Epoch: 341 [52608/225000 (23%)] Loss: 16205.313477\n",
      "Train Epoch: 341 [55104/225000 (24%)] Loss: 17049.378906\n",
      "Train Epoch: 341 [57600/225000 (26%)] Loss: 16713.757812\n",
      "Train Epoch: 341 [60096/225000 (27%)] Loss: 16577.398438\n",
      "Train Epoch: 341 [62592/225000 (28%)] Loss: 16432.515625\n",
      "Train Epoch: 341 [65088/225000 (29%)] Loss: 18191.601562\n",
      "Train Epoch: 341 [67584/225000 (30%)] Loss: 16098.973633\n",
      "Train Epoch: 341 [70080/225000 (31%)] Loss: 16667.875000\n",
      "Train Epoch: 341 [72576/225000 (32%)] Loss: 16328.790039\n",
      "Train Epoch: 341 [75072/225000 (33%)] Loss: 16641.902344\n",
      "Train Epoch: 341 [77568/225000 (34%)] Loss: 16710.867188\n",
      "Train Epoch: 341 [80064/225000 (36%)] Loss: 16765.070312\n",
      "Train Epoch: 341 [82560/225000 (37%)] Loss: 16347.169922\n",
      "Train Epoch: 341 [85056/225000 (38%)] Loss: 16809.013672\n",
      "Train Epoch: 341 [87552/225000 (39%)] Loss: 16934.773438\n",
      "Train Epoch: 341 [90048/225000 (40%)] Loss: 18615.285156\n",
      "Train Epoch: 341 [92544/225000 (41%)] Loss: 16879.181641\n",
      "Train Epoch: 341 [95040/225000 (42%)] Loss: 16673.974609\n",
      "Train Epoch: 341 [97536/225000 (43%)] Loss: 17837.027344\n",
      "Train Epoch: 341 [100032/225000 (44%)] Loss: 16765.269531\n",
      "Train Epoch: 341 [102528/225000 (46%)] Loss: 16582.279297\n",
      "Train Epoch: 341 [105024/225000 (47%)] Loss: 16874.031250\n",
      "Train Epoch: 341 [107520/225000 (48%)] Loss: 16310.666992\n",
      "Train Epoch: 341 [110016/225000 (49%)] Loss: 16226.873047\n",
      "Train Epoch: 341 [112512/225000 (50%)] Loss: 18104.355469\n",
      "Train Epoch: 341 [115008/225000 (51%)] Loss: 16277.495117\n",
      "Train Epoch: 341 [117504/225000 (52%)] Loss: 17008.990234\n",
      "Train Epoch: 341 [120000/225000 (53%)] Loss: 16696.625000\n",
      "Train Epoch: 341 [122496/225000 (54%)] Loss: 16433.718750\n",
      "Train Epoch: 341 [124992/225000 (56%)] Loss: 16767.498047\n",
      "Train Epoch: 341 [127488/225000 (57%)] Loss: 16264.395508\n",
      "Train Epoch: 341 [129984/225000 (58%)] Loss: 18044.253906\n",
      "Train Epoch: 341 [132480/225000 (59%)] Loss: 16643.410156\n",
      "Train Epoch: 341 [134976/225000 (60%)] Loss: 16594.697266\n",
      "Train Epoch: 341 [137472/225000 (61%)] Loss: 16878.431641\n",
      "Train Epoch: 341 [139968/225000 (62%)] Loss: 16671.583984\n",
      "Train Epoch: 341 [142464/225000 (63%)] Loss: 16610.691406\n",
      "Train Epoch: 341 [144960/225000 (64%)] Loss: 16915.468750\n",
      "Train Epoch: 341 [147456/225000 (66%)] Loss: 16624.343750\n",
      "Train Epoch: 341 [149952/225000 (67%)] Loss: 16900.767578\n",
      "Train Epoch: 341 [152448/225000 (68%)] Loss: 18254.888672\n",
      "Train Epoch: 341 [154944/225000 (69%)] Loss: 16743.292969\n",
      "Train Epoch: 341 [157440/225000 (70%)] Loss: 16946.187500\n",
      "Train Epoch: 341 [159936/225000 (71%)] Loss: 16796.933594\n",
      "Train Epoch: 341 [162432/225000 (72%)] Loss: 16427.763672\n",
      "Train Epoch: 341 [164928/225000 (73%)] Loss: 16783.902344\n",
      "Train Epoch: 341 [167424/225000 (74%)] Loss: 16600.466797\n",
      "Train Epoch: 341 [169920/225000 (76%)] Loss: 16249.220703\n",
      "Train Epoch: 341 [172416/225000 (77%)] Loss: 16796.900391\n",
      "Train Epoch: 341 [174912/225000 (78%)] Loss: 16464.359375\n",
      "Train Epoch: 341 [177408/225000 (79%)] Loss: 16628.148438\n",
      "Train Epoch: 341 [179904/225000 (80%)] Loss: 16831.839844\n",
      "Train Epoch: 341 [182400/225000 (81%)] Loss: 16718.230469\n",
      "Train Epoch: 341 [184896/225000 (82%)] Loss: 16800.183594\n",
      "Train Epoch: 341 [187392/225000 (83%)] Loss: 16938.804688\n",
      "Train Epoch: 341 [189888/225000 (84%)] Loss: 16619.912109\n",
      "Train Epoch: 341 [192384/225000 (86%)] Loss: 16969.429688\n",
      "Train Epoch: 341 [194880/225000 (87%)] Loss: 16795.070312\n",
      "Train Epoch: 341 [197376/225000 (88%)] Loss: 16478.412109\n",
      "Train Epoch: 341 [199872/225000 (89%)] Loss: 16237.017578\n",
      "Train Epoch: 341 [202368/225000 (90%)] Loss: 17122.089844\n",
      "Train Epoch: 341 [204864/225000 (91%)] Loss: 17234.488281\n",
      "Train Epoch: 341 [207360/225000 (92%)] Loss: 16516.166016\n",
      "Train Epoch: 341 [209856/225000 (93%)] Loss: 16350.476562\n",
      "Train Epoch: 341 [212352/225000 (94%)] Loss: 16685.384766\n",
      "Train Epoch: 341 [214848/225000 (95%)] Loss: 17035.394531\n",
      "Train Epoch: 341 [217344/225000 (97%)] Loss: 16580.675781\n",
      "Train Epoch: 341 [219840/225000 (98%)] Loss: 16829.986328\n",
      "Train Epoch: 341 [222336/225000 (99%)] Loss: 16562.253906\n",
      "Train Epoch: 341 [224832/225000 (100%)] Loss: 16818.449219\n",
      "    epoch          : 341\n",
      "    loss           : 16755.111373120202\n",
      "    val_loss       : 16636.117770977602\n",
      "Train Epoch: 342 [192/225000 (0%)] Loss: 16815.816406\n",
      "Train Epoch: 342 [2688/225000 (1%)] Loss: 16747.062500\n",
      "Train Epoch: 342 [5184/225000 (2%)] Loss: 16691.609375\n",
      "Train Epoch: 342 [7680/225000 (3%)] Loss: 16829.654297\n",
      "Train Epoch: 342 [10176/225000 (5%)] Loss: 16583.921875\n",
      "Train Epoch: 342 [12672/225000 (6%)] Loss: 16739.730469\n",
      "Train Epoch: 342 [15168/225000 (7%)] Loss: 16643.468750\n",
      "Train Epoch: 342 [17664/225000 (8%)] Loss: 17393.046875\n",
      "Train Epoch: 342 [20160/225000 (9%)] Loss: 17040.531250\n",
      "Train Epoch: 342 [22656/225000 (10%)] Loss: 16918.914062\n",
      "Train Epoch: 342 [25152/225000 (11%)] Loss: 16567.507812\n",
      "Train Epoch: 342 [27648/225000 (12%)] Loss: 16891.792969\n",
      "Train Epoch: 342 [30144/225000 (13%)] Loss: 16618.501953\n",
      "Train Epoch: 342 [32640/225000 (15%)] Loss: 16898.783203\n",
      "Train Epoch: 342 [35136/225000 (16%)] Loss: 16634.246094\n",
      "Train Epoch: 342 [37632/225000 (17%)] Loss: 17134.849609\n",
      "Train Epoch: 342 [40128/225000 (18%)] Loss: 16579.433594\n",
      "Train Epoch: 342 [42624/225000 (19%)] Loss: 16471.230469\n",
      "Train Epoch: 342 [45120/225000 (20%)] Loss: 18690.167969\n",
      "Train Epoch: 342 [47616/225000 (21%)] Loss: 17086.599609\n",
      "Train Epoch: 342 [50112/225000 (22%)] Loss: 16716.105469\n",
      "Train Epoch: 342 [52608/225000 (23%)] Loss: 17082.267578\n",
      "Train Epoch: 342 [55104/225000 (24%)] Loss: 16342.376953\n",
      "Train Epoch: 342 [57600/225000 (26%)] Loss: 16382.460938\n",
      "Train Epoch: 342 [60096/225000 (27%)] Loss: 16288.605469\n",
      "Train Epoch: 342 [62592/225000 (28%)] Loss: 16524.062500\n",
      "Train Epoch: 342 [65088/225000 (29%)] Loss: 16654.996094\n",
      "Train Epoch: 342 [67584/225000 (30%)] Loss: 16770.882812\n",
      "Train Epoch: 342 [70080/225000 (31%)] Loss: 16501.093750\n",
      "Train Epoch: 342 [72576/225000 (32%)] Loss: 16485.332031\n",
      "Train Epoch: 342 [75072/225000 (33%)] Loss: 16736.107422\n",
      "Train Epoch: 342 [77568/225000 (34%)] Loss: 16727.812500\n",
      "Train Epoch: 342 [80064/225000 (36%)] Loss: 16898.527344\n",
      "Train Epoch: 342 [82560/225000 (37%)] Loss: 16702.710938\n",
      "Train Epoch: 342 [85056/225000 (38%)] Loss: 16894.917969\n",
      "Train Epoch: 342 [87552/225000 (39%)] Loss: 16506.267578\n",
      "Train Epoch: 342 [90048/225000 (40%)] Loss: 16977.103516\n",
      "Train Epoch: 342 [92544/225000 (41%)] Loss: 16562.902344\n",
      "Train Epoch: 342 [95040/225000 (42%)] Loss: 16589.808594\n",
      "Train Epoch: 342 [97536/225000 (43%)] Loss: 16678.279297\n",
      "Train Epoch: 342 [100032/225000 (44%)] Loss: 16845.746094\n",
      "Train Epoch: 342 [102528/225000 (46%)] Loss: 16487.691406\n",
      "Train Epoch: 342 [105024/225000 (47%)] Loss: 16603.183594\n",
      "Train Epoch: 342 [107520/225000 (48%)] Loss: 16621.949219\n",
      "Train Epoch: 342 [110016/225000 (49%)] Loss: 16706.433594\n",
      "Train Epoch: 342 [112512/225000 (50%)] Loss: 16820.517578\n",
      "Train Epoch: 342 [115008/225000 (51%)] Loss: 16851.703125\n",
      "Train Epoch: 342 [117504/225000 (52%)] Loss: 16309.639648\n",
      "Train Epoch: 342 [120000/225000 (53%)] Loss: 16626.183594\n",
      "Train Epoch: 342 [122496/225000 (54%)] Loss: 16572.794922\n",
      "Train Epoch: 342 [124992/225000 (56%)] Loss: 16253.362305\n",
      "Train Epoch: 342 [127488/225000 (57%)] Loss: 16532.013672\n",
      "Train Epoch: 342 [129984/225000 (58%)] Loss: 16336.913086\n",
      "Train Epoch: 342 [132480/225000 (59%)] Loss: 16686.964844\n",
      "Train Epoch: 342 [134976/225000 (60%)] Loss: 16621.578125\n",
      "Train Epoch: 342 [137472/225000 (61%)] Loss: 16939.054688\n",
      "Train Epoch: 342 [139968/225000 (62%)] Loss: 16562.851562\n",
      "Train Epoch: 342 [142464/225000 (63%)] Loss: 16913.279297\n",
      "Train Epoch: 342 [144960/225000 (64%)] Loss: 16613.951172\n",
      "Train Epoch: 342 [147456/225000 (66%)] Loss: 16596.125000\n",
      "Train Epoch: 342 [149952/225000 (67%)] Loss: 16810.925781\n",
      "Train Epoch: 342 [152448/225000 (68%)] Loss: 16537.339844\n",
      "Train Epoch: 342 [154944/225000 (69%)] Loss: 16426.039062\n",
      "Train Epoch: 342 [157440/225000 (70%)] Loss: 16723.941406\n",
      "Train Epoch: 342 [159936/225000 (71%)] Loss: 16547.119141\n",
      "Train Epoch: 342 [162432/225000 (72%)] Loss: 16642.867188\n",
      "Train Epoch: 342 [164928/225000 (73%)] Loss: 16565.652344\n",
      "Train Epoch: 342 [167424/225000 (74%)] Loss: 16678.007812\n",
      "Train Epoch: 342 [169920/225000 (76%)] Loss: 16670.191406\n",
      "Train Epoch: 342 [172416/225000 (77%)] Loss: 17677.449219\n",
      "Train Epoch: 342 [174912/225000 (78%)] Loss: 16270.120117\n",
      "Train Epoch: 342 [177408/225000 (79%)] Loss: 17054.060547\n",
      "Train Epoch: 342 [179904/225000 (80%)] Loss: 16609.636719\n",
      "Train Epoch: 342 [182400/225000 (81%)] Loss: 16451.359375\n",
      "Train Epoch: 342 [184896/225000 (82%)] Loss: 16626.023438\n",
      "Train Epoch: 342 [187392/225000 (83%)] Loss: 16317.642578\n",
      "Train Epoch: 342 [189888/225000 (84%)] Loss: 16437.867188\n",
      "Train Epoch: 342 [192384/225000 (86%)] Loss: 16577.199219\n",
      "Train Epoch: 342 [194880/225000 (87%)] Loss: 16688.339844\n",
      "Train Epoch: 342 [197376/225000 (88%)] Loss: 16179.160156\n",
      "Train Epoch: 342 [199872/225000 (89%)] Loss: 17286.710938\n",
      "Train Epoch: 342 [202368/225000 (90%)] Loss: 16640.742188\n",
      "Train Epoch: 342 [204864/225000 (91%)] Loss: 16431.035156\n",
      "Train Epoch: 342 [207360/225000 (92%)] Loss: 16697.496094\n",
      "Train Epoch: 342 [209856/225000 (93%)] Loss: 16203.684570\n",
      "Train Epoch: 342 [212352/225000 (94%)] Loss: 16542.636719\n",
      "Train Epoch: 342 [214848/225000 (95%)] Loss: 16840.796875\n",
      "Train Epoch: 342 [217344/225000 (97%)] Loss: 16902.214844\n",
      "Train Epoch: 342 [219840/225000 (98%)] Loss: 16655.480469\n",
      "Train Epoch: 342 [222336/225000 (99%)] Loss: 16761.126953\n",
      "Train Epoch: 342 [224832/225000 (100%)] Loss: 18428.996094\n",
      "    epoch          : 342\n",
      "    loss           : 16724.299891344923\n",
      "    val_loss       : 16736.76731960555\n",
      "Train Epoch: 343 [192/225000 (0%)] Loss: 17026.062500\n",
      "Train Epoch: 343 [2688/225000 (1%)] Loss: 16803.171875\n",
      "Train Epoch: 343 [5184/225000 (2%)] Loss: 16655.304688\n",
      "Train Epoch: 343 [7680/225000 (3%)] Loss: 16696.710938\n",
      "Train Epoch: 343 [10176/225000 (5%)] Loss: 16252.498047\n",
      "Train Epoch: 343 [12672/225000 (6%)] Loss: 16265.180664\n",
      "Train Epoch: 343 [15168/225000 (7%)] Loss: 16566.279297\n",
      "Train Epoch: 343 [17664/225000 (8%)] Loss: 16881.958984\n",
      "Train Epoch: 343 [20160/225000 (9%)] Loss: 16175.282227\n",
      "Train Epoch: 343 [22656/225000 (10%)] Loss: 16335.460938\n",
      "Train Epoch: 343 [25152/225000 (11%)] Loss: 16633.531250\n",
      "Train Epoch: 343 [27648/225000 (12%)] Loss: 16614.716797\n",
      "Train Epoch: 343 [30144/225000 (13%)] Loss: 16374.082031\n",
      "Train Epoch: 343 [32640/225000 (15%)] Loss: 16612.560547\n",
      "Train Epoch: 343 [35136/225000 (16%)] Loss: 16358.156250\n",
      "Train Epoch: 343 [37632/225000 (17%)] Loss: 16988.652344\n",
      "Train Epoch: 343 [40128/225000 (18%)] Loss: 18151.941406\n",
      "Train Epoch: 343 [42624/225000 (19%)] Loss: 16366.550781\n",
      "Train Epoch: 343 [45120/225000 (20%)] Loss: 17118.917969\n",
      "Train Epoch: 343 [47616/225000 (21%)] Loss: 16662.730469\n",
      "Train Epoch: 343 [50112/225000 (22%)] Loss: 16792.789062\n",
      "Train Epoch: 343 [52608/225000 (23%)] Loss: 16185.708984\n",
      "Train Epoch: 343 [55104/225000 (24%)] Loss: 16671.058594\n",
      "Train Epoch: 343 [57600/225000 (26%)] Loss: 16543.220703\n",
      "Train Epoch: 343 [60096/225000 (27%)] Loss: 16649.703125\n",
      "Train Epoch: 343 [62592/225000 (28%)] Loss: 16209.598633\n",
      "Train Epoch: 343 [65088/225000 (29%)] Loss: 16117.215820\n",
      "Train Epoch: 343 [67584/225000 (30%)] Loss: 18801.160156\n",
      "Train Epoch: 343 [70080/225000 (31%)] Loss: 16523.105469\n",
      "Train Epoch: 343 [72576/225000 (32%)] Loss: 16835.367188\n",
      "Train Epoch: 343 [75072/225000 (33%)] Loss: 17340.222656\n",
      "Train Epoch: 343 [77568/225000 (34%)] Loss: 16946.666016\n",
      "Train Epoch: 343 [80064/225000 (36%)] Loss: 16934.269531\n",
      "Train Epoch: 343 [82560/225000 (37%)] Loss: 16294.669922\n",
      "Train Epoch: 343 [85056/225000 (38%)] Loss: 18144.847656\n",
      "Train Epoch: 343 [87552/225000 (39%)] Loss: 16446.417969\n",
      "Train Epoch: 343 [90048/225000 (40%)] Loss: 16744.191406\n",
      "Train Epoch: 343 [92544/225000 (41%)] Loss: 17199.958984\n",
      "Train Epoch: 343 [95040/225000 (42%)] Loss: 18794.027344\n",
      "Train Epoch: 343 [97536/225000 (43%)] Loss: 16912.949219\n",
      "Train Epoch: 343 [100032/225000 (44%)] Loss: 16921.636719\n",
      "Train Epoch: 343 [102528/225000 (46%)] Loss: 16702.773438\n",
      "Train Epoch: 343 [105024/225000 (47%)] Loss: 16772.568359\n",
      "Train Epoch: 343 [107520/225000 (48%)] Loss: 16779.837891\n",
      "Train Epoch: 343 [110016/225000 (49%)] Loss: 16797.496094\n",
      "Train Epoch: 343 [112512/225000 (50%)] Loss: 16896.816406\n",
      "Train Epoch: 343 [115008/225000 (51%)] Loss: 16840.417969\n",
      "Train Epoch: 343 [117504/225000 (52%)] Loss: 17050.046875\n",
      "Train Epoch: 343 [120000/225000 (53%)] Loss: 16711.886719\n",
      "Train Epoch: 343 [122496/225000 (54%)] Loss: 16645.406250\n",
      "Train Epoch: 343 [124992/225000 (56%)] Loss: 16774.634766\n",
      "Train Epoch: 343 [127488/225000 (57%)] Loss: 16773.894531\n",
      "Train Epoch: 343 [129984/225000 (58%)] Loss: 16314.389648\n",
      "Train Epoch: 343 [132480/225000 (59%)] Loss: 16947.128906\n",
      "Train Epoch: 343 [134976/225000 (60%)] Loss: 16800.751953\n",
      "Train Epoch: 343 [137472/225000 (61%)] Loss: 16697.818359\n",
      "Train Epoch: 343 [139968/225000 (62%)] Loss: 17253.625000\n",
      "Train Epoch: 343 [142464/225000 (63%)] Loss: 16223.316406\n",
      "Train Epoch: 343 [144960/225000 (64%)] Loss: 16943.417969\n",
      "Train Epoch: 343 [147456/225000 (66%)] Loss: 16329.519531\n",
      "Train Epoch: 343 [149952/225000 (67%)] Loss: 16677.296875\n",
      "Train Epoch: 343 [152448/225000 (68%)] Loss: 16639.214844\n",
      "Train Epoch: 343 [154944/225000 (69%)] Loss: 16665.423828\n",
      "Train Epoch: 343 [157440/225000 (70%)] Loss: 16943.722656\n",
      "Train Epoch: 343 [159936/225000 (71%)] Loss: 16920.902344\n",
      "Train Epoch: 343 [162432/225000 (72%)] Loss: 16565.878906\n",
      "Train Epoch: 343 [164928/225000 (73%)] Loss: 16928.437500\n",
      "Train Epoch: 343 [167424/225000 (74%)] Loss: 16761.339844\n",
      "Train Epoch: 343 [169920/225000 (76%)] Loss: 16528.503906\n",
      "Train Epoch: 343 [172416/225000 (77%)] Loss: 16983.378906\n",
      "Train Epoch: 343 [174912/225000 (78%)] Loss: 16574.562500\n",
      "Train Epoch: 343 [177408/225000 (79%)] Loss: 16063.739258\n",
      "Train Epoch: 343 [179904/225000 (80%)] Loss: 16577.265625\n",
      "Train Epoch: 343 [182400/225000 (81%)] Loss: 16673.142578\n",
      "Train Epoch: 343 [184896/225000 (82%)] Loss: 16747.578125\n",
      "Train Epoch: 343 [187392/225000 (83%)] Loss: 16827.773438\n",
      "Train Epoch: 343 [189888/225000 (84%)] Loss: 16728.748047\n",
      "Train Epoch: 343 [192384/225000 (86%)] Loss: 16963.835938\n",
      "Train Epoch: 343 [194880/225000 (87%)] Loss: 16491.121094\n",
      "Train Epoch: 343 [197376/225000 (88%)] Loss: 17201.398438\n",
      "Train Epoch: 343 [199872/225000 (89%)] Loss: 16214.566406\n",
      "Train Epoch: 343 [202368/225000 (90%)] Loss: 16817.820312\n",
      "Train Epoch: 343 [204864/225000 (91%)] Loss: 16909.341797\n",
      "Train Epoch: 343 [207360/225000 (92%)] Loss: 16267.602539\n",
      "Train Epoch: 343 [209856/225000 (93%)] Loss: 16004.832031\n",
      "Train Epoch: 343 [212352/225000 (94%)] Loss: 16427.257812\n",
      "Train Epoch: 343 [214848/225000 (95%)] Loss: 16896.025391\n",
      "Train Epoch: 343 [217344/225000 (97%)] Loss: 16554.167969\n",
      "Train Epoch: 343 [219840/225000 (98%)] Loss: 16767.058594\n",
      "Train Epoch: 343 [222336/225000 (99%)] Loss: 16708.222656\n",
      "Train Epoch: 343 [224832/225000 (100%)] Loss: 16908.007812\n",
      "    epoch          : 343\n",
      "    loss           : 16746.546886665423\n",
      "    val_loss       : 16708.247027297057\n",
      "Train Epoch: 344 [192/225000 (0%)] Loss: 18231.041016\n",
      "Train Epoch: 344 [2688/225000 (1%)] Loss: 16597.701172\n",
      "Train Epoch: 344 [5184/225000 (2%)] Loss: 16960.144531\n",
      "Train Epoch: 344 [7680/225000 (3%)] Loss: 16632.775391\n",
      "Train Epoch: 344 [10176/225000 (5%)] Loss: 16205.289062\n",
      "Train Epoch: 344 [12672/225000 (6%)] Loss: 16402.214844\n",
      "Train Epoch: 344 [15168/225000 (7%)] Loss: 16806.083984\n",
      "Train Epoch: 344 [17664/225000 (8%)] Loss: 16813.138672\n",
      "Train Epoch: 344 [20160/225000 (9%)] Loss: 16767.136719\n",
      "Train Epoch: 344 [22656/225000 (10%)] Loss: 16809.519531\n",
      "Train Epoch: 344 [25152/225000 (11%)] Loss: 16464.703125\n",
      "Train Epoch: 344 [27648/225000 (12%)] Loss: 16599.396484\n",
      "Train Epoch: 344 [30144/225000 (13%)] Loss: 18207.062500\n",
      "Train Epoch: 344 [32640/225000 (15%)] Loss: 16477.195312\n",
      "Train Epoch: 344 [35136/225000 (16%)] Loss: 16335.593750\n",
      "Train Epoch: 344 [37632/225000 (17%)] Loss: 16570.066406\n",
      "Train Epoch: 344 [40128/225000 (18%)] Loss: 16773.585938\n",
      "Train Epoch: 344 [42624/225000 (19%)] Loss: 16657.132812\n",
      "Train Epoch: 344 [45120/225000 (20%)] Loss: 16947.824219\n",
      "Train Epoch: 344 [47616/225000 (21%)] Loss: 16905.058594\n",
      "Train Epoch: 344 [50112/225000 (22%)] Loss: 17045.367188\n",
      "Train Epoch: 344 [52608/225000 (23%)] Loss: 16658.566406\n",
      "Train Epoch: 344 [55104/225000 (24%)] Loss: 16666.144531\n",
      "Train Epoch: 344 [57600/225000 (26%)] Loss: 16663.277344\n",
      "Train Epoch: 344 [60096/225000 (27%)] Loss: 16476.375000\n",
      "Train Epoch: 344 [62592/225000 (28%)] Loss: 16448.574219\n",
      "Train Epoch: 344 [65088/225000 (29%)] Loss: 16264.161133\n",
      "Train Epoch: 344 [67584/225000 (30%)] Loss: 16937.460938\n",
      "Train Epoch: 344 [70080/225000 (31%)] Loss: 16526.171875\n",
      "Train Epoch: 344 [72576/225000 (32%)] Loss: 16671.265625\n",
      "Train Epoch: 344 [75072/225000 (33%)] Loss: 16809.203125\n",
      "Train Epoch: 344 [77568/225000 (34%)] Loss: 16196.366211\n",
      "Train Epoch: 344 [80064/225000 (36%)] Loss: 16282.833008\n",
      "Train Epoch: 344 [82560/225000 (37%)] Loss: 16929.007812\n",
      "Train Epoch: 344 [85056/225000 (38%)] Loss: 16983.349609\n",
      "Train Epoch: 344 [87552/225000 (39%)] Loss: 16823.902344\n",
      "Train Epoch: 344 [90048/225000 (40%)] Loss: 16539.617188\n",
      "Train Epoch: 344 [92544/225000 (41%)] Loss: 16071.383789\n",
      "Train Epoch: 344 [95040/225000 (42%)] Loss: 16840.843750\n",
      "Train Epoch: 344 [97536/225000 (43%)] Loss: 17113.226562\n",
      "Train Epoch: 344 [100032/225000 (44%)] Loss: 16633.052734\n",
      "Train Epoch: 344 [102528/225000 (46%)] Loss: 16514.982422\n",
      "Train Epoch: 344 [105024/225000 (47%)] Loss: 16596.849609\n",
      "Train Epoch: 344 [107520/225000 (48%)] Loss: 16595.710938\n",
      "Train Epoch: 344 [110016/225000 (49%)] Loss: 16635.197266\n",
      "Train Epoch: 344 [112512/225000 (50%)] Loss: 16889.347656\n",
      "Train Epoch: 344 [115008/225000 (51%)] Loss: 16516.953125\n",
      "Train Epoch: 344 [117504/225000 (52%)] Loss: 16454.820312\n",
      "Train Epoch: 344 [120000/225000 (53%)] Loss: 16364.454102\n",
      "Train Epoch: 344 [122496/225000 (54%)] Loss: 16468.566406\n",
      "Train Epoch: 344 [124992/225000 (56%)] Loss: 16473.417969\n",
      "Train Epoch: 344 [127488/225000 (57%)] Loss: 16626.812500\n",
      "Train Epoch: 344 [129984/225000 (58%)] Loss: 15949.002930\n",
      "Train Epoch: 344 [132480/225000 (59%)] Loss: 16892.996094\n",
      "Train Epoch: 344 [134976/225000 (60%)] Loss: 16826.289062\n",
      "Train Epoch: 344 [137472/225000 (61%)] Loss: 16679.763672\n",
      "Train Epoch: 344 [139968/225000 (62%)] Loss: 16522.097656\n",
      "Train Epoch: 344 [142464/225000 (63%)] Loss: 16818.972656\n",
      "Train Epoch: 344 [144960/225000 (64%)] Loss: 16773.152344\n",
      "Train Epoch: 344 [147456/225000 (66%)] Loss: 16457.708984\n",
      "Train Epoch: 344 [149952/225000 (67%)] Loss: 16469.248047\n",
      "Train Epoch: 344 [152448/225000 (68%)] Loss: 16763.716797\n",
      "Train Epoch: 344 [154944/225000 (69%)] Loss: 16468.820312\n",
      "Train Epoch: 344 [157440/225000 (70%)] Loss: 16879.478516\n",
      "Train Epoch: 344 [159936/225000 (71%)] Loss: 16651.185547\n",
      "Train Epoch: 344 [162432/225000 (72%)] Loss: 16478.953125\n",
      "Train Epoch: 344 [164928/225000 (73%)] Loss: 16754.746094\n",
      "Train Epoch: 344 [167424/225000 (74%)] Loss: 16563.693359\n",
      "Train Epoch: 344 [169920/225000 (76%)] Loss: 16852.433594\n",
      "Train Epoch: 344 [172416/225000 (77%)] Loss: 16881.796875\n",
      "Train Epoch: 344 [174912/225000 (78%)] Loss: 16486.613281\n",
      "Train Epoch: 344 [177408/225000 (79%)] Loss: 16235.378906\n",
      "Train Epoch: 344 [179904/225000 (80%)] Loss: 16400.996094\n",
      "Train Epoch: 344 [182400/225000 (81%)] Loss: 16494.867188\n",
      "Train Epoch: 344 [184896/225000 (82%)] Loss: 16610.152344\n",
      "Train Epoch: 344 [187392/225000 (83%)] Loss: 16764.429688\n",
      "Train Epoch: 344 [189888/225000 (84%)] Loss: 17009.292969\n",
      "Train Epoch: 344 [192384/225000 (86%)] Loss: 16520.484375\n",
      "Train Epoch: 344 [194880/225000 (87%)] Loss: 17086.568359\n",
      "Train Epoch: 344 [197376/225000 (88%)] Loss: 16547.455078\n",
      "Train Epoch: 344 [199872/225000 (89%)] Loss: 16597.714844\n",
      "Train Epoch: 344 [202368/225000 (90%)] Loss: 16654.691406\n",
      "Train Epoch: 344 [204864/225000 (91%)] Loss: 16238.046875\n",
      "Train Epoch: 344 [207360/225000 (92%)] Loss: 18072.125000\n",
      "Train Epoch: 344 [209856/225000 (93%)] Loss: 16885.605469\n",
      "Train Epoch: 344 [212352/225000 (94%)] Loss: 16723.277344\n",
      "Train Epoch: 344 [214848/225000 (95%)] Loss: 16871.746094\n",
      "Train Epoch: 344 [217344/225000 (97%)] Loss: 16292.830078\n",
      "Train Epoch: 344 [219840/225000 (98%)] Loss: 18395.695312\n",
      "Train Epoch: 344 [222336/225000 (99%)] Loss: 17121.371094\n",
      "Train Epoch: 344 [224832/225000 (100%)] Loss: 16974.582031\n",
      "    epoch          : 344\n",
      "    loss           : 16723.568728502294\n",
      "    val_loss       : 16641.070244259507\n",
      "Train Epoch: 345 [192/225000 (0%)] Loss: 16177.848633\n",
      "Train Epoch: 345 [2688/225000 (1%)] Loss: 16032.722656\n",
      "Train Epoch: 345 [5184/225000 (2%)] Loss: 16972.925781\n",
      "Train Epoch: 345 [7680/225000 (3%)] Loss: 16937.093750\n",
      "Train Epoch: 345 [10176/225000 (5%)] Loss: 16386.011719\n",
      "Train Epoch: 345 [12672/225000 (6%)] Loss: 17935.287109\n",
      "Train Epoch: 345 [15168/225000 (7%)] Loss: 16420.589844\n",
      "Train Epoch: 345 [17664/225000 (8%)] Loss: 16423.863281\n",
      "Train Epoch: 345 [20160/225000 (9%)] Loss: 16927.433594\n",
      "Train Epoch: 345 [22656/225000 (10%)] Loss: 16614.738281\n",
      "Train Epoch: 345 [25152/225000 (11%)] Loss: 16306.373047\n",
      "Train Epoch: 345 [27648/225000 (12%)] Loss: 16685.304688\n",
      "Train Epoch: 345 [30144/225000 (13%)] Loss: 16643.300781\n",
      "Train Epoch: 345 [32640/225000 (15%)] Loss: 16847.597656\n",
      "Train Epoch: 345 [35136/225000 (16%)] Loss: 16522.902344\n",
      "Train Epoch: 345 [37632/225000 (17%)] Loss: 17019.419922\n",
      "Train Epoch: 345 [40128/225000 (18%)] Loss: 18104.203125\n",
      "Train Epoch: 345 [42624/225000 (19%)] Loss: 16785.300781\n",
      "Train Epoch: 345 [45120/225000 (20%)] Loss: 16239.640625\n",
      "Train Epoch: 345 [47616/225000 (21%)] Loss: 16696.082031\n",
      "Train Epoch: 345 [50112/225000 (22%)] Loss: 16751.373047\n",
      "Train Epoch: 345 [52608/225000 (23%)] Loss: 16778.394531\n",
      "Train Epoch: 345 [55104/225000 (24%)] Loss: 16998.033203\n",
      "Train Epoch: 345 [57600/225000 (26%)] Loss: 16633.353516\n",
      "Train Epoch: 345 [60096/225000 (27%)] Loss: 16883.814453\n",
      "Train Epoch: 345 [62592/225000 (28%)] Loss: 16518.166016\n",
      "Train Epoch: 345 [65088/225000 (29%)] Loss: 16960.220703\n",
      "Train Epoch: 345 [67584/225000 (30%)] Loss: 16691.960938\n",
      "Train Epoch: 345 [70080/225000 (31%)] Loss: 16512.898438\n",
      "Train Epoch: 345 [72576/225000 (32%)] Loss: 16641.121094\n",
      "Train Epoch: 345 [75072/225000 (33%)] Loss: 17192.476562\n",
      "Train Epoch: 345 [77568/225000 (34%)] Loss: 16788.824219\n",
      "Train Epoch: 345 [80064/225000 (36%)] Loss: 16865.509766\n",
      "Train Epoch: 345 [82560/225000 (37%)] Loss: 16519.792969\n",
      "Train Epoch: 345 [85056/225000 (38%)] Loss: 16027.073242\n",
      "Train Epoch: 345 [87552/225000 (39%)] Loss: 16772.968750\n",
      "Train Epoch: 345 [90048/225000 (40%)] Loss: 16603.654297\n",
      "Train Epoch: 345 [92544/225000 (41%)] Loss: 16517.388672\n",
      "Train Epoch: 345 [95040/225000 (42%)] Loss: 16435.408203\n",
      "Train Epoch: 345 [97536/225000 (43%)] Loss: 16507.675781\n",
      "Train Epoch: 345 [100032/225000 (44%)] Loss: 16964.878906\n",
      "Train Epoch: 345 [102528/225000 (46%)] Loss: 16640.326172\n",
      "Train Epoch: 345 [105024/225000 (47%)] Loss: 16190.666992\n",
      "Train Epoch: 345 [107520/225000 (48%)] Loss: 16199.154297\n",
      "Train Epoch: 345 [110016/225000 (49%)] Loss: 16631.003906\n",
      "Train Epoch: 345 [112512/225000 (50%)] Loss: 16461.292969\n",
      "Train Epoch: 345 [115008/225000 (51%)] Loss: 18370.671875\n",
      "Train Epoch: 345 [117504/225000 (52%)] Loss: 17090.476562\n",
      "Train Epoch: 345 [120000/225000 (53%)] Loss: 16739.957031\n",
      "Train Epoch: 345 [122496/225000 (54%)] Loss: 16178.671875\n",
      "Train Epoch: 345 [124992/225000 (56%)] Loss: 16392.523438\n",
      "Train Epoch: 345 [127488/225000 (57%)] Loss: 16682.761719\n",
      "Train Epoch: 345 [129984/225000 (58%)] Loss: 16622.376953\n",
      "Train Epoch: 345 [132480/225000 (59%)] Loss: 16496.515625\n",
      "Train Epoch: 345 [134976/225000 (60%)] Loss: 16835.812500\n",
      "Train Epoch: 345 [137472/225000 (61%)] Loss: 16419.355469\n",
      "Train Epoch: 345 [139968/225000 (62%)] Loss: 16638.484375\n",
      "Train Epoch: 345 [142464/225000 (63%)] Loss: 17054.605469\n",
      "Train Epoch: 345 [144960/225000 (64%)] Loss: 16658.480469\n",
      "Train Epoch: 345 [147456/225000 (66%)] Loss: 16824.318359\n",
      "Train Epoch: 345 [149952/225000 (67%)] Loss: 16985.888672\n",
      "Train Epoch: 345 [152448/225000 (68%)] Loss: 16435.294922\n",
      "Train Epoch: 345 [154944/225000 (69%)] Loss: 16307.734375\n",
      "Train Epoch: 345 [157440/225000 (70%)] Loss: 17693.787109\n",
      "Train Epoch: 345 [159936/225000 (71%)] Loss: 16759.376953\n",
      "Train Epoch: 345 [162432/225000 (72%)] Loss: 16759.636719\n",
      "Train Epoch: 345 [164928/225000 (73%)] Loss: 16491.589844\n",
      "Train Epoch: 345 [167424/225000 (74%)] Loss: 16426.171875\n",
      "Train Epoch: 345 [169920/225000 (76%)] Loss: 16766.480469\n",
      "Train Epoch: 345 [172416/225000 (77%)] Loss: 16352.101562\n",
      "Train Epoch: 345 [174912/225000 (78%)] Loss: 16477.804688\n",
      "Train Epoch: 345 [177408/225000 (79%)] Loss: 16639.275391\n",
      "Train Epoch: 345 [179904/225000 (80%)] Loss: 16814.062500\n",
      "Train Epoch: 345 [182400/225000 (81%)] Loss: 16793.064453\n",
      "Train Epoch: 345 [184896/225000 (82%)] Loss: 16897.039062\n",
      "Train Epoch: 345 [187392/225000 (83%)] Loss: 17133.167969\n",
      "Train Epoch: 345 [189888/225000 (84%)] Loss: 16629.425781\n",
      "Train Epoch: 345 [192384/225000 (86%)] Loss: 18164.335938\n",
      "Train Epoch: 345 [194880/225000 (87%)] Loss: 16426.042969\n",
      "Train Epoch: 345 [197376/225000 (88%)] Loss: 16431.642578\n",
      "Train Epoch: 345 [199872/225000 (89%)] Loss: 16652.566406\n",
      "Train Epoch: 345 [202368/225000 (90%)] Loss: 16578.300781\n",
      "Train Epoch: 345 [204864/225000 (91%)] Loss: 16581.048828\n",
      "Train Epoch: 345 [207360/225000 (92%)] Loss: 16598.123047\n",
      "Train Epoch: 345 [209856/225000 (93%)] Loss: 16370.176758\n",
      "Train Epoch: 345 [212352/225000 (94%)] Loss: 16644.781250\n",
      "Train Epoch: 345 [214848/225000 (95%)] Loss: 16812.707031\n",
      "Train Epoch: 345 [217344/225000 (97%)] Loss: 16203.832031\n",
      "Train Epoch: 345 [219840/225000 (98%)] Loss: 16903.623047\n",
      "Train Epoch: 345 [222336/225000 (99%)] Loss: 16613.324219\n",
      "Train Epoch: 345 [224832/225000 (100%)] Loss: 16564.550781\n",
      "    epoch          : 345\n",
      "    loss           : 16736.895047861562\n",
      "    val_loss       : 16711.04366851672\n",
      "Train Epoch: 346 [192/225000 (0%)] Loss: 16495.546875\n",
      "Train Epoch: 346 [2688/225000 (1%)] Loss: 16361.308594\n",
      "Train Epoch: 346 [5184/225000 (2%)] Loss: 16561.148438\n",
      "Train Epoch: 346 [7680/225000 (3%)] Loss: 16360.307617\n",
      "Train Epoch: 346 [10176/225000 (5%)] Loss: 16515.416016\n",
      "Train Epoch: 346 [12672/225000 (6%)] Loss: 16859.621094\n",
      "Train Epoch: 346 [15168/225000 (7%)] Loss: 16381.289062\n",
      "Train Epoch: 346 [17664/225000 (8%)] Loss: 16389.824219\n",
      "Train Epoch: 346 [20160/225000 (9%)] Loss: 16906.015625\n",
      "Train Epoch: 346 [22656/225000 (10%)] Loss: 16786.523438\n",
      "Train Epoch: 346 [25152/225000 (11%)] Loss: 16734.335938\n",
      "Train Epoch: 346 [27648/225000 (12%)] Loss: 16894.626953\n",
      "Train Epoch: 346 [30144/225000 (13%)] Loss: 17921.945312\n",
      "Train Epoch: 346 [32640/225000 (15%)] Loss: 16609.568359\n",
      "Train Epoch: 346 [35136/225000 (16%)] Loss: 16586.765625\n",
      "Train Epoch: 346 [37632/225000 (17%)] Loss: 16234.202148\n",
      "Train Epoch: 346 [40128/225000 (18%)] Loss: 16797.251953\n",
      "Train Epoch: 346 [42624/225000 (19%)] Loss: 16776.843750\n",
      "Train Epoch: 346 [45120/225000 (20%)] Loss: 16893.359375\n",
      "Train Epoch: 346 [47616/225000 (21%)] Loss: 16439.875000\n",
      "Train Epoch: 346 [50112/225000 (22%)] Loss: 16720.751953\n",
      "Train Epoch: 346 [52608/225000 (23%)] Loss: 16480.361328\n",
      "Train Epoch: 346 [55104/225000 (24%)] Loss: 17137.257812\n",
      "Train Epoch: 346 [57600/225000 (26%)] Loss: 16416.421875\n",
      "Train Epoch: 346 [60096/225000 (27%)] Loss: 16443.511719\n",
      "Train Epoch: 346 [62592/225000 (28%)] Loss: 16731.640625\n",
      "Train Epoch: 346 [65088/225000 (29%)] Loss: 16479.433594\n",
      "Train Epoch: 346 [67584/225000 (30%)] Loss: 17003.574219\n",
      "Train Epoch: 346 [70080/225000 (31%)] Loss: 16765.351562\n",
      "Train Epoch: 346 [72576/225000 (32%)] Loss: 16651.914062\n",
      "Train Epoch: 346 [75072/225000 (33%)] Loss: 16782.535156\n",
      "Train Epoch: 346 [77568/225000 (34%)] Loss: 16373.062500\n",
      "Train Epoch: 346 [80064/225000 (36%)] Loss: 18541.701172\n",
      "Train Epoch: 346 [82560/225000 (37%)] Loss: 18176.132812\n",
      "Train Epoch: 346 [85056/225000 (38%)] Loss: 16798.007812\n",
      "Train Epoch: 346 [87552/225000 (39%)] Loss: 16489.330078\n",
      "Train Epoch: 346 [90048/225000 (40%)] Loss: 16645.574219\n",
      "Train Epoch: 346 [92544/225000 (41%)] Loss: 16519.197266\n",
      "Train Epoch: 346 [95040/225000 (42%)] Loss: 16769.900391\n",
      "Train Epoch: 346 [97536/225000 (43%)] Loss: 17174.681641\n",
      "Train Epoch: 346 [100032/225000 (44%)] Loss: 16741.757812\n",
      "Train Epoch: 346 [102528/225000 (46%)] Loss: 16847.408203\n",
      "Train Epoch: 346 [105024/225000 (47%)] Loss: 16452.027344\n",
      "Train Epoch: 346 [107520/225000 (48%)] Loss: 16720.150391\n",
      "Train Epoch: 346 [110016/225000 (49%)] Loss: 16559.875000\n",
      "Train Epoch: 346 [112512/225000 (50%)] Loss: 16332.186523\n",
      "Train Epoch: 346 [115008/225000 (51%)] Loss: 16602.160156\n",
      "Train Epoch: 346 [117504/225000 (52%)] Loss: 17044.175781\n",
      "Train Epoch: 346 [120000/225000 (53%)] Loss: 16899.816406\n",
      "Train Epoch: 346 [122496/225000 (54%)] Loss: 16749.222656\n",
      "Train Epoch: 346 [124992/225000 (56%)] Loss: 16422.273438\n",
      "Train Epoch: 346 [127488/225000 (57%)] Loss: 16473.851562\n",
      "Train Epoch: 346 [129984/225000 (58%)] Loss: 16736.000000\n",
      "Train Epoch: 346 [132480/225000 (59%)] Loss: 16304.012695\n",
      "Train Epoch: 346 [134976/225000 (60%)] Loss: 16459.683594\n",
      "Train Epoch: 346 [137472/225000 (61%)] Loss: 16449.152344\n",
      "Train Epoch: 346 [139968/225000 (62%)] Loss: 16698.660156\n",
      "Train Epoch: 346 [142464/225000 (63%)] Loss: 16669.003906\n",
      "Train Epoch: 346 [144960/225000 (64%)] Loss: 16699.304688\n",
      "Train Epoch: 346 [147456/225000 (66%)] Loss: 16607.851562\n",
      "Train Epoch: 346 [149952/225000 (67%)] Loss: 16583.265625\n",
      "Train Epoch: 346 [152448/225000 (68%)] Loss: 16160.117188\n",
      "Train Epoch: 346 [154944/225000 (69%)] Loss: 16512.707031\n",
      "Train Epoch: 346 [157440/225000 (70%)] Loss: 16851.728516\n",
      "Train Epoch: 346 [159936/225000 (71%)] Loss: 17261.367188\n",
      "Train Epoch: 346 [162432/225000 (72%)] Loss: 16779.677734\n",
      "Train Epoch: 346 [164928/225000 (73%)] Loss: 21373.199219\n",
      "Train Epoch: 346 [167424/225000 (74%)] Loss: 16936.035156\n",
      "Train Epoch: 346 [169920/225000 (76%)] Loss: 17123.644531\n",
      "Train Epoch: 346 [172416/225000 (77%)] Loss: 17024.513672\n",
      "Train Epoch: 346 [174912/225000 (78%)] Loss: 16585.390625\n",
      "Train Epoch: 346 [177408/225000 (79%)] Loss: 16460.570312\n",
      "Train Epoch: 346 [179904/225000 (80%)] Loss: 16874.039062\n",
      "Train Epoch: 346 [182400/225000 (81%)] Loss: 16634.808594\n",
      "Train Epoch: 346 [184896/225000 (82%)] Loss: 16572.691406\n",
      "Train Epoch: 346 [187392/225000 (83%)] Loss: 16266.781250\n",
      "Train Epoch: 346 [189888/225000 (84%)] Loss: 16372.392578\n",
      "Train Epoch: 346 [192384/225000 (86%)] Loss: 16813.519531\n",
      "Train Epoch: 346 [194880/225000 (87%)] Loss: 17046.539062\n",
      "Train Epoch: 346 [197376/225000 (88%)] Loss: 16058.868164\n",
      "Train Epoch: 346 [199872/225000 (89%)] Loss: 16506.003906\n",
      "Train Epoch: 346 [202368/225000 (90%)] Loss: 17917.449219\n",
      "Train Epoch: 346 [204864/225000 (91%)] Loss: 18288.570312\n",
      "Train Epoch: 346 [207360/225000 (92%)] Loss: 16858.667969\n",
      "Train Epoch: 346 [209856/225000 (93%)] Loss: 16640.222656\n",
      "Train Epoch: 346 [212352/225000 (94%)] Loss: 16660.671875\n",
      "Train Epoch: 346 [214848/225000 (95%)] Loss: 16732.113281\n",
      "Train Epoch: 346 [217344/225000 (97%)] Loss: 16292.614258\n",
      "Train Epoch: 346 [219840/225000 (98%)] Loss: 17236.320312\n",
      "Train Epoch: 346 [222336/225000 (99%)] Loss: 16559.218750\n",
      "Train Epoch: 346 [224832/225000 (100%)] Loss: 16492.972656\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   346: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   335: reducing learning rate of group 0 to 1.0000e-06.\n",
      "    epoch          : 346\n",
      "    loss           : 16719.963893018077\n",
      "    val_loss       : 16675.770186780064\n",
      "Train Epoch: 347 [192/225000 (0%)] Loss: 16561.191406\n",
      "Train Epoch: 347 [2688/225000 (1%)] Loss: 16597.359375\n",
      "Train Epoch: 347 [5184/225000 (2%)] Loss: 16487.396484\n",
      "Train Epoch: 347 [7680/225000 (3%)] Loss: 16734.992188\n",
      "Train Epoch: 347 [10176/225000 (5%)] Loss: 18148.953125\n",
      "Train Epoch: 347 [12672/225000 (6%)] Loss: 16149.832031\n",
      "Train Epoch: 347 [15168/225000 (7%)] Loss: 17033.873047\n",
      "Train Epoch: 347 [17664/225000 (8%)] Loss: 17051.519531\n",
      "Train Epoch: 347 [20160/225000 (9%)] Loss: 16668.800781\n",
      "Train Epoch: 347 [22656/225000 (10%)] Loss: 16908.707031\n",
      "Train Epoch: 347 [25152/225000 (11%)] Loss: 16240.421875\n",
      "Train Epoch: 347 [27648/225000 (12%)] Loss: 16535.535156\n",
      "Train Epoch: 347 [30144/225000 (13%)] Loss: 16823.125000\n",
      "Train Epoch: 347 [32640/225000 (15%)] Loss: 25129.218750\n",
      "Train Epoch: 347 [35136/225000 (16%)] Loss: 16682.820312\n",
      "Train Epoch: 347 [37632/225000 (17%)] Loss: 16390.757812\n",
      "Train Epoch: 347 [40128/225000 (18%)] Loss: 16698.406250\n",
      "Train Epoch: 347 [42624/225000 (19%)] Loss: 16502.289062\n",
      "Train Epoch: 347 [45120/225000 (20%)] Loss: 16562.984375\n",
      "Train Epoch: 347 [47616/225000 (21%)] Loss: 16675.859375\n",
      "Train Epoch: 347 [50112/225000 (22%)] Loss: 16902.498047\n",
      "Train Epoch: 347 [52608/225000 (23%)] Loss: 16778.925781\n",
      "Train Epoch: 347 [55104/225000 (24%)] Loss: 16926.722656\n",
      "Train Epoch: 347 [57600/225000 (26%)] Loss: 16718.466797\n",
      "Train Epoch: 347 [60096/225000 (27%)] Loss: 16729.500000\n",
      "Train Epoch: 347 [62592/225000 (28%)] Loss: 16804.527344\n",
      "Train Epoch: 347 [65088/225000 (29%)] Loss: 16575.218750\n",
      "Train Epoch: 347 [67584/225000 (30%)] Loss: 16983.712891\n",
      "Train Epoch: 347 [70080/225000 (31%)] Loss: 16092.161133\n",
      "Train Epoch: 347 [72576/225000 (32%)] Loss: 18273.851562\n",
      "Train Epoch: 347 [75072/225000 (33%)] Loss: 16559.730469\n",
      "Train Epoch: 347 [77568/225000 (34%)] Loss: 15988.972656\n",
      "Train Epoch: 347 [80064/225000 (36%)] Loss: 16348.049805\n",
      "Train Epoch: 347 [82560/225000 (37%)] Loss: 16956.767578\n",
      "Train Epoch: 347 [85056/225000 (38%)] Loss: 16347.213867\n",
      "Train Epoch: 347 [87552/225000 (39%)] Loss: 16325.431641\n",
      "Train Epoch: 347 [90048/225000 (40%)] Loss: 16711.132812\n",
      "Train Epoch: 347 [92544/225000 (41%)] Loss: 17133.144531\n",
      "Train Epoch: 347 [95040/225000 (42%)] Loss: 16971.402344\n",
      "Train Epoch: 347 [97536/225000 (43%)] Loss: 16625.234375\n",
      "Train Epoch: 347 [100032/225000 (44%)] Loss: 18071.929688\n",
      "Train Epoch: 347 [102528/225000 (46%)] Loss: 16974.273438\n",
      "Train Epoch: 347 [105024/225000 (47%)] Loss: 16848.015625\n",
      "Train Epoch: 347 [107520/225000 (48%)] Loss: 16554.505859\n",
      "Train Epoch: 347 [110016/225000 (49%)] Loss: 16534.757812\n",
      "Train Epoch: 347 [112512/225000 (50%)] Loss: 16881.250000\n",
      "Train Epoch: 347 [115008/225000 (51%)] Loss: 16619.148438\n",
      "Train Epoch: 347 [117504/225000 (52%)] Loss: 16436.496094\n",
      "Train Epoch: 347 [120000/225000 (53%)] Loss: 17821.238281\n",
      "Train Epoch: 347 [122496/225000 (54%)] Loss: 16607.238281\n",
      "Train Epoch: 347 [124992/225000 (56%)] Loss: 16549.740234\n",
      "Train Epoch: 347 [127488/225000 (57%)] Loss: 16443.302734\n",
      "Train Epoch: 347 [129984/225000 (58%)] Loss: 16334.541992\n",
      "Train Epoch: 347 [132480/225000 (59%)] Loss: 17059.800781\n",
      "Train Epoch: 347 [134976/225000 (60%)] Loss: 16398.398438\n",
      "Train Epoch: 347 [137472/225000 (61%)] Loss: 16223.924805\n",
      "Train Epoch: 347 [139968/225000 (62%)] Loss: 16566.953125\n",
      "Train Epoch: 347 [142464/225000 (63%)] Loss: 16317.968750\n",
      "Train Epoch: 347 [144960/225000 (64%)] Loss: 16579.236328\n",
      "Train Epoch: 347 [147456/225000 (66%)] Loss: 16025.136719\n",
      "Train Epoch: 347 [149952/225000 (67%)] Loss: 16944.882812\n",
      "Train Epoch: 347 [152448/225000 (68%)] Loss: 16600.777344\n",
      "Train Epoch: 347 [154944/225000 (69%)] Loss: 16392.501953\n",
      "Train Epoch: 347 [157440/225000 (70%)] Loss: 16355.777344\n",
      "Train Epoch: 347 [159936/225000 (71%)] Loss: 17177.093750\n",
      "Train Epoch: 347 [162432/225000 (72%)] Loss: 16935.113281\n",
      "Train Epoch: 347 [164928/225000 (73%)] Loss: 16667.789062\n",
      "Train Epoch: 347 [167424/225000 (74%)] Loss: 16459.955078\n",
      "Train Epoch: 347 [169920/225000 (76%)] Loss: 17529.103516\n",
      "Train Epoch: 347 [172416/225000 (77%)] Loss: 18097.050781\n",
      "Train Epoch: 347 [174912/225000 (78%)] Loss: 16326.219727\n",
      "Train Epoch: 347 [177408/225000 (79%)] Loss: 16823.218750\n",
      "Train Epoch: 347 [179904/225000 (80%)] Loss: 17020.314453\n",
      "Train Epoch: 347 [182400/225000 (81%)] Loss: 16737.949219\n",
      "Train Epoch: 347 [184896/225000 (82%)] Loss: 16459.861328\n",
      "Train Epoch: 347 [187392/225000 (83%)] Loss: 16004.208984\n",
      "Train Epoch: 347 [189888/225000 (84%)] Loss: 16461.878906\n",
      "Train Epoch: 347 [192384/225000 (86%)] Loss: 16839.585938\n",
      "Train Epoch: 347 [194880/225000 (87%)] Loss: 16684.339844\n",
      "Train Epoch: 347 [197376/225000 (88%)] Loss: 16370.799805\n",
      "Train Epoch: 347 [199872/225000 (89%)] Loss: 16793.558594\n",
      "Train Epoch: 347 [202368/225000 (90%)] Loss: 16895.777344\n",
      "Train Epoch: 347 [204864/225000 (91%)] Loss: 16430.324219\n",
      "Train Epoch: 347 [207360/225000 (92%)] Loss: 16404.984375\n",
      "Train Epoch: 347 [209856/225000 (93%)] Loss: 16376.570312\n",
      "Train Epoch: 347 [212352/225000 (94%)] Loss: 16492.597656\n",
      "Train Epoch: 347 [214848/225000 (95%)] Loss: 15973.457031\n",
      "Train Epoch: 347 [217344/225000 (97%)] Loss: 16231.984375\n",
      "Train Epoch: 347 [219840/225000 (98%)] Loss: 16568.533203\n",
      "Train Epoch: 347 [222336/225000 (99%)] Loss: 16957.640625\n",
      "Train Epoch: 347 [224832/225000 (100%)] Loss: 16377.400391\n",
      "    epoch          : 347\n",
      "    loss           : 16700.771202738375\n",
      "    val_loss       : 16650.331753063747\n",
      "Train Epoch: 348 [192/225000 (0%)] Loss: 16929.417969\n",
      "Train Epoch: 348 [2688/225000 (1%)] Loss: 16477.375000\n",
      "Train Epoch: 348 [5184/225000 (2%)] Loss: 16912.058594\n",
      "Train Epoch: 348 [7680/225000 (3%)] Loss: 17053.046875\n",
      "Train Epoch: 348 [10176/225000 (5%)] Loss: 16649.429688\n",
      "Train Epoch: 348 [12672/225000 (6%)] Loss: 16675.585938\n",
      "Train Epoch: 348 [15168/225000 (7%)] Loss: 16324.708008\n",
      "Train Epoch: 348 [17664/225000 (8%)] Loss: 17026.800781\n",
      "Train Epoch: 348 [20160/225000 (9%)] Loss: 16598.648438\n",
      "Train Epoch: 348 [22656/225000 (10%)] Loss: 16758.267578\n",
      "Train Epoch: 348 [25152/225000 (11%)] Loss: 16580.878906\n",
      "Train Epoch: 348 [27648/225000 (12%)] Loss: 16599.417969\n",
      "Train Epoch: 348 [30144/225000 (13%)] Loss: 16844.781250\n",
      "Train Epoch: 348 [32640/225000 (15%)] Loss: 16430.275391\n",
      "Train Epoch: 348 [35136/225000 (16%)] Loss: 16334.824219\n",
      "Train Epoch: 348 [37632/225000 (17%)] Loss: 17296.566406\n",
      "Train Epoch: 348 [40128/225000 (18%)] Loss: 16117.643555\n",
      "Train Epoch: 348 [42624/225000 (19%)] Loss: 16448.031250\n",
      "Train Epoch: 348 [45120/225000 (20%)] Loss: 16523.789062\n",
      "Train Epoch: 348 [47616/225000 (21%)] Loss: 16599.066406\n",
      "Train Epoch: 348 [50112/225000 (22%)] Loss: 16657.371094\n",
      "Train Epoch: 348 [52608/225000 (23%)] Loss: 16849.617188\n",
      "Train Epoch: 348 [55104/225000 (24%)] Loss: 16085.074219\n",
      "Train Epoch: 348 [57600/225000 (26%)] Loss: 16495.515625\n",
      "Train Epoch: 348 [60096/225000 (27%)] Loss: 16902.835938\n",
      "Train Epoch: 348 [62592/225000 (28%)] Loss: 16427.460938\n",
      "Train Epoch: 348 [65088/225000 (29%)] Loss: 16916.279297\n",
      "Train Epoch: 348 [67584/225000 (30%)] Loss: 16798.607422\n",
      "Train Epoch: 348 [70080/225000 (31%)] Loss: 16455.042969\n",
      "Train Epoch: 348 [72576/225000 (32%)] Loss: 16241.419922\n",
      "Train Epoch: 348 [75072/225000 (33%)] Loss: 16258.428711\n",
      "Train Epoch: 348 [77568/225000 (34%)] Loss: 16675.292969\n",
      "Train Epoch: 348 [80064/225000 (36%)] Loss: 16783.179688\n",
      "Train Epoch: 348 [82560/225000 (37%)] Loss: 16554.902344\n",
      "Train Epoch: 348 [85056/225000 (38%)] Loss: 16393.576172\n",
      "Train Epoch: 348 [87552/225000 (39%)] Loss: 16524.716797\n",
      "Train Epoch: 348 [90048/225000 (40%)] Loss: 16183.799805\n",
      "Train Epoch: 348 [92544/225000 (41%)] Loss: 18125.601562\n",
      "Train Epoch: 348 [95040/225000 (42%)] Loss: 16802.480469\n",
      "Train Epoch: 348 [97536/225000 (43%)] Loss: 16122.099609\n",
      "Train Epoch: 348 [100032/225000 (44%)] Loss: 16997.417969\n",
      "Train Epoch: 348 [102528/225000 (46%)] Loss: 16519.574219\n",
      "Train Epoch: 348 [105024/225000 (47%)] Loss: 16686.753906\n",
      "Train Epoch: 348 [107520/225000 (48%)] Loss: 16684.197266\n",
      "Train Epoch: 348 [110016/225000 (49%)] Loss: 16662.078125\n",
      "Train Epoch: 348 [112512/225000 (50%)] Loss: 16791.208984\n",
      "Train Epoch: 348 [115008/225000 (51%)] Loss: 16524.041016\n",
      "Train Epoch: 348 [117504/225000 (52%)] Loss: 16766.960938\n",
      "Train Epoch: 348 [120000/225000 (53%)] Loss: 16757.128906\n",
      "Train Epoch: 348 [122496/225000 (54%)] Loss: 16542.636719\n",
      "Train Epoch: 348 [124992/225000 (56%)] Loss: 16927.328125\n",
      "Train Epoch: 348 [127488/225000 (57%)] Loss: 16277.138672\n",
      "Train Epoch: 348 [129984/225000 (58%)] Loss: 16675.125000\n",
      "Train Epoch: 348 [132480/225000 (59%)] Loss: 16474.500000\n",
      "Train Epoch: 348 [134976/225000 (60%)] Loss: 16756.466797\n",
      "Train Epoch: 348 [137472/225000 (61%)] Loss: 16488.753906\n",
      "Train Epoch: 348 [139968/225000 (62%)] Loss: 16620.734375\n",
      "Train Epoch: 348 [142464/225000 (63%)] Loss: 16386.585938\n",
      "Train Epoch: 348 [144960/225000 (64%)] Loss: 16621.812500\n",
      "Train Epoch: 348 [147456/225000 (66%)] Loss: 16757.615234\n",
      "Train Epoch: 348 [149952/225000 (67%)] Loss: 16935.109375\n",
      "Train Epoch: 348 [152448/225000 (68%)] Loss: 16261.771484\n",
      "Train Epoch: 348 [154944/225000 (69%)] Loss: 16443.945312\n",
      "Train Epoch: 348 [157440/225000 (70%)] Loss: 16712.804688\n",
      "Train Epoch: 348 [159936/225000 (71%)] Loss: 16254.275391\n",
      "Train Epoch: 348 [162432/225000 (72%)] Loss: 16332.384766\n",
      "Train Epoch: 348 [164928/225000 (73%)] Loss: 16851.128906\n",
      "Train Epoch: 348 [167424/225000 (74%)] Loss: 16705.531250\n",
      "Train Epoch: 348 [169920/225000 (76%)] Loss: 16400.451172\n",
      "Train Epoch: 348 [172416/225000 (77%)] Loss: 16261.494141\n",
      "Train Epoch: 348 [174912/225000 (78%)] Loss: 16477.292969\n",
      "Train Epoch: 348 [177408/225000 (79%)] Loss: 16098.304688\n",
      "Train Epoch: 348 [179904/225000 (80%)] Loss: 16466.960938\n",
      "Train Epoch: 348 [182400/225000 (81%)] Loss: 16902.451172\n",
      "Train Epoch: 348 [184896/225000 (82%)] Loss: 16472.152344\n",
      "Train Epoch: 348 [187392/225000 (83%)] Loss: 16045.180664\n",
      "Train Epoch: 348 [189888/225000 (84%)] Loss: 16652.867188\n",
      "Train Epoch: 348 [192384/225000 (86%)] Loss: 16552.142578\n",
      "Train Epoch: 348 [194880/225000 (87%)] Loss: 17115.035156\n",
      "Train Epoch: 348 [197376/225000 (88%)] Loss: 16324.151367\n",
      "Train Epoch: 348 [199872/225000 (89%)] Loss: 16711.183594\n",
      "Train Epoch: 348 [202368/225000 (90%)] Loss: 16664.601562\n",
      "Train Epoch: 348 [204864/225000 (91%)] Loss: 16544.062500\n",
      "Train Epoch: 348 [207360/225000 (92%)] Loss: 16989.308594\n",
      "Train Epoch: 348 [209856/225000 (93%)] Loss: 16610.900391\n",
      "Train Epoch: 348 [212352/225000 (94%)] Loss: 18246.396484\n",
      "Train Epoch: 348 [214848/225000 (95%)] Loss: 16313.106445\n",
      "Train Epoch: 348 [217344/225000 (97%)] Loss: 16972.062500\n",
      "Train Epoch: 348 [219840/225000 (98%)] Loss: 17131.332031\n",
      "Train Epoch: 348 [222336/225000 (99%)] Loss: 16942.066406\n",
      "Train Epoch: 348 [224832/225000 (100%)] Loss: 17006.123047\n",
      "    epoch          : 348\n",
      "    loss           : 16730.00499280077\n",
      "    val_loss       : 16620.557580772245\n",
      "Train Epoch: 349 [192/225000 (0%)] Loss: 16271.617188\n",
      "Train Epoch: 349 [2688/225000 (1%)] Loss: 16589.824219\n",
      "Train Epoch: 349 [5184/225000 (2%)] Loss: 16756.736328\n",
      "Train Epoch: 349 [7680/225000 (3%)] Loss: 16669.050781\n",
      "Train Epoch: 349 [10176/225000 (5%)] Loss: 16602.445312\n",
      "Train Epoch: 349 [12672/225000 (6%)] Loss: 18141.181641\n",
      "Train Epoch: 349 [15168/225000 (7%)] Loss: 16774.482422\n",
      "Train Epoch: 349 [17664/225000 (8%)] Loss: 16552.263672\n",
      "Train Epoch: 349 [20160/225000 (9%)] Loss: 16285.000000\n",
      "Train Epoch: 349 [22656/225000 (10%)] Loss: 16212.545898\n",
      "Train Epoch: 349 [25152/225000 (11%)] Loss: 16211.929688\n",
      "Train Epoch: 349 [27648/225000 (12%)] Loss: 16576.734375\n",
      "Train Epoch: 349 [30144/225000 (13%)] Loss: 16874.583984\n",
      "Train Epoch: 349 [32640/225000 (15%)] Loss: 16735.634766\n",
      "Train Epoch: 349 [35136/225000 (16%)] Loss: 16494.712891\n",
      "Train Epoch: 349 [37632/225000 (17%)] Loss: 16551.140625\n",
      "Train Epoch: 349 [40128/225000 (18%)] Loss: 16205.320312\n",
      "Train Epoch: 349 [42624/225000 (19%)] Loss: 16405.707031\n",
      "Train Epoch: 349 [45120/225000 (20%)] Loss: 16747.875000\n",
      "Train Epoch: 349 [47616/225000 (21%)] Loss: 16812.242188\n",
      "Train Epoch: 349 [50112/225000 (22%)] Loss: 16568.765625\n",
      "Train Epoch: 349 [52608/225000 (23%)] Loss: 16728.296875\n",
      "Train Epoch: 349 [55104/225000 (24%)] Loss: 16680.976562\n",
      "Train Epoch: 349 [57600/225000 (26%)] Loss: 17212.878906\n",
      "Train Epoch: 349 [60096/225000 (27%)] Loss: 16864.707031\n",
      "Train Epoch: 349 [62592/225000 (28%)] Loss: 16717.265625\n",
      "Train Epoch: 349 [65088/225000 (29%)] Loss: 16865.220703\n",
      "Train Epoch: 349 [67584/225000 (30%)] Loss: 16480.603516\n",
      "Train Epoch: 349 [70080/225000 (31%)] Loss: 16417.390625\n",
      "Train Epoch: 349 [72576/225000 (32%)] Loss: 16845.031250\n",
      "Train Epoch: 349 [75072/225000 (33%)] Loss: 16389.072266\n",
      "Train Epoch: 349 [77568/225000 (34%)] Loss: 16285.448242\n",
      "Train Epoch: 349 [80064/225000 (36%)] Loss: 16375.015625\n",
      "Train Epoch: 349 [82560/225000 (37%)] Loss: 16439.480469\n",
      "Train Epoch: 349 [85056/225000 (38%)] Loss: 16521.384766\n",
      "Train Epoch: 349 [87552/225000 (39%)] Loss: 16507.851562\n",
      "Train Epoch: 349 [90048/225000 (40%)] Loss: 16486.087891\n",
      "Train Epoch: 349 [92544/225000 (41%)] Loss: 16635.914062\n",
      "Train Epoch: 349 [95040/225000 (42%)] Loss: 16594.226562\n",
      "Train Epoch: 349 [97536/225000 (43%)] Loss: 18012.230469\n",
      "Train Epoch: 349 [100032/225000 (44%)] Loss: 16614.431641\n",
      "Train Epoch: 349 [102528/225000 (46%)] Loss: 16832.285156\n",
      "Train Epoch: 349 [105024/225000 (47%)] Loss: 16576.835938\n",
      "Train Epoch: 349 [107520/225000 (48%)] Loss: 16504.988281\n",
      "Train Epoch: 349 [110016/225000 (49%)] Loss: 16641.080078\n",
      "Train Epoch: 349 [112512/225000 (50%)] Loss: 16200.280273\n",
      "Train Epoch: 349 [115008/225000 (51%)] Loss: 16697.689453\n",
      "Train Epoch: 349 [117504/225000 (52%)] Loss: 16671.058594\n",
      "Train Epoch: 349 [120000/225000 (53%)] Loss: 16480.312500\n",
      "Train Epoch: 349 [122496/225000 (54%)] Loss: 16576.914062\n",
      "Train Epoch: 349 [124992/225000 (56%)] Loss: 16561.964844\n",
      "Train Epoch: 349 [127488/225000 (57%)] Loss: 16429.845703\n",
      "Train Epoch: 349 [129984/225000 (58%)] Loss: 16681.687500\n",
      "Train Epoch: 349 [132480/225000 (59%)] Loss: 17093.863281\n",
      "Train Epoch: 349 [134976/225000 (60%)] Loss: 16626.027344\n",
      "Train Epoch: 349 [137472/225000 (61%)] Loss: 17485.011719\n",
      "Train Epoch: 349 [139968/225000 (62%)] Loss: 16488.859375\n",
      "Train Epoch: 349 [142464/225000 (63%)] Loss: 16280.953125\n",
      "Train Epoch: 349 [144960/225000 (64%)] Loss: 16319.071289\n",
      "Train Epoch: 349 [147456/225000 (66%)] Loss: 16453.785156\n",
      "Train Epoch: 349 [149952/225000 (67%)] Loss: 16679.511719\n",
      "Train Epoch: 349 [152448/225000 (68%)] Loss: 16765.175781\n",
      "Train Epoch: 349 [154944/225000 (69%)] Loss: 16393.359375\n",
      "Train Epoch: 349 [157440/225000 (70%)] Loss: 16319.347656\n",
      "Train Epoch: 349 [159936/225000 (71%)] Loss: 16388.980469\n",
      "Train Epoch: 349 [162432/225000 (72%)] Loss: 16603.539062\n",
      "Train Epoch: 349 [164928/225000 (73%)] Loss: 16741.951172\n",
      "Train Epoch: 349 [167424/225000 (74%)] Loss: 16654.546875\n",
      "Train Epoch: 349 [169920/225000 (76%)] Loss: 16419.347656\n",
      "Train Epoch: 349 [172416/225000 (77%)] Loss: 16312.135742\n",
      "Train Epoch: 349 [174912/225000 (78%)] Loss: 16494.496094\n",
      "Train Epoch: 349 [177408/225000 (79%)] Loss: 16547.837891\n",
      "Train Epoch: 349 [179904/225000 (80%)] Loss: 16876.623047\n",
      "Train Epoch: 349 [182400/225000 (81%)] Loss: 15750.282227\n",
      "Train Epoch: 349 [184896/225000 (82%)] Loss: 16795.488281\n",
      "Train Epoch: 349 [187392/225000 (83%)] Loss: 16681.003906\n",
      "Train Epoch: 349 [189888/225000 (84%)] Loss: 16624.554688\n",
      "Train Epoch: 349 [192384/225000 (86%)] Loss: 16694.308594\n",
      "Train Epoch: 349 [194880/225000 (87%)] Loss: 16955.462891\n",
      "Train Epoch: 349 [197376/225000 (88%)] Loss: 18192.791016\n",
      "Train Epoch: 349 [199872/225000 (89%)] Loss: 16534.324219\n",
      "Train Epoch: 349 [202368/225000 (90%)] Loss: 16937.052734\n",
      "Train Epoch: 349 [204864/225000 (91%)] Loss: 17083.523438\n",
      "Train Epoch: 349 [207360/225000 (92%)] Loss: 16659.015625\n",
      "Train Epoch: 349 [209856/225000 (93%)] Loss: 16610.681641\n",
      "Train Epoch: 349 [212352/225000 (94%)] Loss: 16718.957031\n",
      "Train Epoch: 349 [214848/225000 (95%)] Loss: 16619.212891\n",
      "Train Epoch: 349 [217344/225000 (97%)] Loss: 16744.851562\n",
      "Train Epoch: 349 [219840/225000 (98%)] Loss: 19704.734375\n",
      "Train Epoch: 349 [222336/225000 (99%)] Loss: 16580.132812\n",
      "Train Epoch: 349 [224832/225000 (100%)] Loss: 16610.000000\n",
      "    epoch          : 349\n",
      "    loss           : 16715.49273577485\n",
      "    val_loss       : 16669.352569609197\n",
      "Train Epoch: 350 [192/225000 (0%)] Loss: 18371.339844\n",
      "Train Epoch: 350 [2688/225000 (1%)] Loss: 16907.056641\n",
      "Train Epoch: 350 [5184/225000 (2%)] Loss: 16465.156250\n",
      "Train Epoch: 350 [7680/225000 (3%)] Loss: 17235.710938\n",
      "Train Epoch: 350 [10176/225000 (5%)] Loss: 16586.476562\n",
      "Train Epoch: 350 [12672/225000 (6%)] Loss: 16273.912109\n",
      "Train Epoch: 350 [15168/225000 (7%)] Loss: 16170.815430\n",
      "Train Epoch: 350 [17664/225000 (8%)] Loss: 16247.469727\n",
      "Train Epoch: 350 [20160/225000 (9%)] Loss: 16964.210938\n",
      "Train Epoch: 350 [22656/225000 (10%)] Loss: 16892.148438\n",
      "Train Epoch: 350 [25152/225000 (11%)] Loss: 17722.031250\n",
      "Train Epoch: 350 [27648/225000 (12%)] Loss: 16632.054688\n",
      "Train Epoch: 350 [30144/225000 (13%)] Loss: 16539.011719\n",
      "Train Epoch: 350 [32640/225000 (15%)] Loss: 16615.597656\n",
      "Train Epoch: 350 [35136/225000 (16%)] Loss: 16848.974609\n",
      "Train Epoch: 350 [37632/225000 (17%)] Loss: 16421.535156\n",
      "Train Epoch: 350 [40128/225000 (18%)] Loss: 17297.529297\n",
      "Train Epoch: 350 [42624/225000 (19%)] Loss: 18609.457031\n",
      "Train Epoch: 350 [45120/225000 (20%)] Loss: 16446.294922\n",
      "Train Epoch: 350 [47616/225000 (21%)] Loss: 16856.421875\n",
      "Train Epoch: 350 [50112/225000 (22%)] Loss: 17108.320312\n",
      "Train Epoch: 350 [52608/225000 (23%)] Loss: 16893.300781\n",
      "Train Epoch: 350 [55104/225000 (24%)] Loss: 16838.617188\n",
      "Train Epoch: 350 [57600/225000 (26%)] Loss: 17234.640625\n",
      "Train Epoch: 350 [60096/225000 (27%)] Loss: 16454.537109\n",
      "Train Epoch: 350 [62592/225000 (28%)] Loss: 16742.414062\n",
      "Train Epoch: 350 [65088/225000 (29%)] Loss: 16787.750000\n",
      "Train Epoch: 350 [67584/225000 (30%)] Loss: 16235.927734\n",
      "Train Epoch: 350 [70080/225000 (31%)] Loss: 16662.984375\n",
      "Train Epoch: 350 [72576/225000 (32%)] Loss: 18224.574219\n",
      "Train Epoch: 350 [75072/225000 (33%)] Loss: 16474.480469\n",
      "Train Epoch: 350 [77568/225000 (34%)] Loss: 17189.375000\n",
      "Train Epoch: 350 [80064/225000 (36%)] Loss: 16460.386719\n",
      "Train Epoch: 350 [82560/225000 (37%)] Loss: 16381.289062\n",
      "Train Epoch: 350 [85056/225000 (38%)] Loss: 16312.765625\n",
      "Train Epoch: 350 [87552/225000 (39%)] Loss: 16737.683594\n",
      "Train Epoch: 350 [90048/225000 (40%)] Loss: 16438.695312\n",
      "Train Epoch: 350 [92544/225000 (41%)] Loss: 16252.252930\n",
      "Train Epoch: 350 [95040/225000 (42%)] Loss: 16429.996094\n",
      "Train Epoch: 350 [97536/225000 (43%)] Loss: 16933.962891\n",
      "Train Epoch: 350 [100032/225000 (44%)] Loss: 16902.722656\n",
      "Train Epoch: 350 [102528/225000 (46%)] Loss: 16726.373047\n",
      "Train Epoch: 350 [105024/225000 (47%)] Loss: 16474.132812\n",
      "Train Epoch: 350 [107520/225000 (48%)] Loss: 16388.919922\n",
      "Train Epoch: 350 [110016/225000 (49%)] Loss: 16848.611328\n",
      "Train Epoch: 350 [112512/225000 (50%)] Loss: 16357.736328\n",
      "Train Epoch: 350 [115008/225000 (51%)] Loss: 16941.195312\n",
      "Train Epoch: 350 [117504/225000 (52%)] Loss: 16572.253906\n",
      "Train Epoch: 350 [120000/225000 (53%)] Loss: 16762.343750\n",
      "Train Epoch: 350 [122496/225000 (54%)] Loss: 16436.232422\n",
      "Train Epoch: 350 [124992/225000 (56%)] Loss: 16631.808594\n",
      "Train Epoch: 350 [127488/225000 (57%)] Loss: 16922.017578\n",
      "Train Epoch: 350 [129984/225000 (58%)] Loss: 16585.792969\n",
      "Train Epoch: 350 [132480/225000 (59%)] Loss: 16527.107422\n",
      "Train Epoch: 350 [134976/225000 (60%)] Loss: 16555.781250\n",
      "Train Epoch: 350 [137472/225000 (61%)] Loss: 16612.085938\n",
      "Train Epoch: 350 [139968/225000 (62%)] Loss: 16386.376953\n",
      "Train Epoch: 350 [142464/225000 (63%)] Loss: 16657.486328\n",
      "Train Epoch: 350 [144960/225000 (64%)] Loss: 16765.279297\n",
      "Train Epoch: 350 [147456/225000 (66%)] Loss: 16598.183594\n",
      "Train Epoch: 350 [149952/225000 (67%)] Loss: 16717.253906\n",
      "Train Epoch: 350 [152448/225000 (68%)] Loss: 16347.929688\n",
      "Train Epoch: 350 [154944/225000 (69%)] Loss: 16490.169922\n",
      "Train Epoch: 350 [157440/225000 (70%)] Loss: 17043.074219\n",
      "Train Epoch: 350 [159936/225000 (71%)] Loss: 16364.669922\n",
      "Train Epoch: 350 [162432/225000 (72%)] Loss: 16361.079102\n",
      "Train Epoch: 350 [164928/225000 (73%)] Loss: 16185.586914\n",
      "Train Epoch: 350 [167424/225000 (74%)] Loss: 16465.796875\n",
      "Train Epoch: 350 [169920/225000 (76%)] Loss: 16589.175781\n",
      "Train Epoch: 350 [172416/225000 (77%)] Loss: 17132.972656\n",
      "Train Epoch: 350 [174912/225000 (78%)] Loss: 16618.093750\n",
      "Train Epoch: 350 [177408/225000 (79%)] Loss: 16849.750000\n",
      "Train Epoch: 350 [179904/225000 (80%)] Loss: 16492.453125\n",
      "Train Epoch: 350 [182400/225000 (81%)] Loss: 16281.318359\n",
      "Train Epoch: 350 [184896/225000 (82%)] Loss: 16560.078125\n",
      "Train Epoch: 350 [187392/225000 (83%)] Loss: 16727.703125\n",
      "Train Epoch: 350 [189888/225000 (84%)] Loss: 16486.582031\n",
      "Train Epoch: 350 [192384/225000 (86%)] Loss: 16892.398438\n",
      "Train Epoch: 350 [194880/225000 (87%)] Loss: 16526.710938\n",
      "Train Epoch: 350 [197376/225000 (88%)] Loss: 16685.800781\n",
      "Train Epoch: 350 [199872/225000 (89%)] Loss: 16636.810547\n",
      "Train Epoch: 350 [202368/225000 (90%)] Loss: 17009.736328\n",
      "Train Epoch: 350 [204864/225000 (91%)] Loss: 16027.097656\n",
      "Train Epoch: 350 [207360/225000 (92%)] Loss: 17878.933594\n",
      "Train Epoch: 350 [209856/225000 (93%)] Loss: 18219.585938\n",
      "Train Epoch: 350 [212352/225000 (94%)] Loss: 16793.507812\n",
      "Train Epoch: 350 [214848/225000 (95%)] Loss: 16028.326172\n",
      "Train Epoch: 350 [217344/225000 (97%)] Loss: 16515.343750\n",
      "Train Epoch: 350 [219840/225000 (98%)] Loss: 16793.017578\n",
      "Train Epoch: 350 [222336/225000 (99%)] Loss: 16793.617188\n",
      "Train Epoch: 350 [224832/225000 (100%)] Loss: 16839.341797\n",
      "    epoch          : 350\n",
      "    loss           : 16714.751949792022\n",
      "    val_loss       : 16710.1045562129\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0804_123131/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [192/225000 (0%)] Loss: 16421.359375\n",
      "Train Epoch: 351 [2688/225000 (1%)] Loss: 18283.714844\n",
      "Train Epoch: 351 [5184/225000 (2%)] Loss: 16503.453125\n",
      "Train Epoch: 351 [7680/225000 (3%)] Loss: 16646.125000\n",
      "Train Epoch: 351 [10176/225000 (5%)] Loss: 16809.210938\n",
      "Train Epoch: 351 [12672/225000 (6%)] Loss: 18392.740234\n",
      "Train Epoch: 351 [15168/225000 (7%)] Loss: 17023.667969\n",
      "Train Epoch: 351 [17664/225000 (8%)] Loss: 16491.019531\n",
      "Train Epoch: 351 [20160/225000 (9%)] Loss: 16243.217773\n",
      "Train Epoch: 351 [22656/225000 (10%)] Loss: 16867.197266\n",
      "Train Epoch: 351 [25152/225000 (11%)] Loss: 16346.833008\n",
      "Train Epoch: 351 [27648/225000 (12%)] Loss: 16167.061523\n",
      "Train Epoch: 351 [30144/225000 (13%)] Loss: 16598.875000\n",
      "Train Epoch: 351 [32640/225000 (15%)] Loss: 16609.437500\n",
      "Train Epoch: 351 [35136/225000 (16%)] Loss: 16558.347656\n",
      "Train Epoch: 351 [37632/225000 (17%)] Loss: 16828.363281\n",
      "Train Epoch: 351 [40128/225000 (18%)] Loss: 17088.070312\n",
      "Train Epoch: 351 [42624/225000 (19%)] Loss: 16665.558594\n",
      "Train Epoch: 351 [45120/225000 (20%)] Loss: 16919.912109\n",
      "Train Epoch: 351 [47616/225000 (21%)] Loss: 16628.248047\n",
      "Train Epoch: 351 [50112/225000 (22%)] Loss: 16515.808594\n",
      "Train Epoch: 351 [52608/225000 (23%)] Loss: 16411.373047\n",
      "Train Epoch: 351 [55104/225000 (24%)] Loss: 16811.884766\n",
      "Train Epoch: 351 [57600/225000 (26%)] Loss: 17421.566406\n",
      "Train Epoch: 351 [60096/225000 (27%)] Loss: 16730.166016\n",
      "Train Epoch: 351 [62592/225000 (28%)] Loss: 16635.007812\n",
      "Train Epoch: 351 [65088/225000 (29%)] Loss: 16991.689453\n",
      "Train Epoch: 351 [67584/225000 (30%)] Loss: 16538.574219\n",
      "Train Epoch: 351 [70080/225000 (31%)] Loss: 16031.672852\n",
      "Train Epoch: 351 [72576/225000 (32%)] Loss: 16537.730469\n",
      "Train Epoch: 351 [75072/225000 (33%)] Loss: 16453.742188\n",
      "Train Epoch: 351 [77568/225000 (34%)] Loss: 16182.109375\n",
      "Train Epoch: 351 [80064/225000 (36%)] Loss: 17958.871094\n",
      "Train Epoch: 351 [82560/225000 (37%)] Loss: 16713.445312\n",
      "Train Epoch: 351 [85056/225000 (38%)] Loss: 16684.175781\n",
      "Train Epoch: 351 [87552/225000 (39%)] Loss: 16629.484375\n",
      "Train Epoch: 351 [90048/225000 (40%)] Loss: 16701.996094\n",
      "Train Epoch: 351 [92544/225000 (41%)] Loss: 16140.476562\n",
      "Train Epoch: 351 [95040/225000 (42%)] Loss: 16755.058594\n",
      "Train Epoch: 351 [97536/225000 (43%)] Loss: 16768.144531\n",
      "Train Epoch: 351 [100032/225000 (44%)] Loss: 16581.373047\n",
      "Train Epoch: 351 [102528/225000 (46%)] Loss: 16139.067383\n",
      "Train Epoch: 351 [105024/225000 (47%)] Loss: 16920.642578\n",
      "Train Epoch: 351 [107520/225000 (48%)] Loss: 16298.546875\n",
      "Train Epoch: 351 [110016/225000 (49%)] Loss: 16501.539062\n",
      "Train Epoch: 351 [112512/225000 (50%)] Loss: 16667.199219\n",
      "Train Epoch: 351 [115008/225000 (51%)] Loss: 16672.609375\n",
      "Train Epoch: 351 [117504/225000 (52%)] Loss: 16612.183594\n",
      "Train Epoch: 351 [120000/225000 (53%)] Loss: 16433.792969\n",
      "Train Epoch: 351 [122496/225000 (54%)] Loss: 16674.292969\n",
      "Train Epoch: 351 [124992/225000 (56%)] Loss: 16932.087891\n",
      "Train Epoch: 351 [127488/225000 (57%)] Loss: 16747.628906\n",
      "Train Epoch: 351 [129984/225000 (58%)] Loss: 16577.228516\n",
      "Train Epoch: 351 [132480/225000 (59%)] Loss: 16871.591797\n",
      "Train Epoch: 351 [134976/225000 (60%)] Loss: 16114.581055\n",
      "Train Epoch: 351 [137472/225000 (61%)] Loss: 16853.128906\n",
      "Train Epoch: 351 [139968/225000 (62%)] Loss: 16771.929688\n",
      "Train Epoch: 351 [142464/225000 (63%)] Loss: 16726.587891\n",
      "Train Epoch: 351 [144960/225000 (64%)] Loss: 16538.109375\n",
      "Train Epoch: 351 [147456/225000 (66%)] Loss: 16479.197266\n",
      "Train Epoch: 351 [149952/225000 (67%)] Loss: 16342.906250\n",
      "Train Epoch: 351 [152448/225000 (68%)] Loss: 16806.953125\n",
      "Train Epoch: 351 [154944/225000 (69%)] Loss: 16454.404297\n",
      "Train Epoch: 351 [157440/225000 (70%)] Loss: 16437.953125\n",
      "Train Epoch: 351 [159936/225000 (71%)] Loss: 16766.929688\n",
      "Train Epoch: 351 [162432/225000 (72%)] Loss: 16131.517578\n",
      "Train Epoch: 351 [164928/225000 (73%)] Loss: 16789.830078\n",
      "Train Epoch: 351 [167424/225000 (74%)] Loss: 16466.599609\n",
      "Train Epoch: 351 [169920/225000 (76%)] Loss: 16722.363281\n",
      "Train Epoch: 351 [172416/225000 (77%)] Loss: 16750.273438\n",
      "Train Epoch: 351 [174912/225000 (78%)] Loss: 16464.310547\n",
      "Train Epoch: 351 [177408/225000 (79%)] Loss: 16481.589844\n",
      "Train Epoch: 351 [179904/225000 (80%)] Loss: 16361.921875\n",
      "Train Epoch: 351 [182400/225000 (81%)] Loss: 16249.311523\n",
      "Train Epoch: 351 [184896/225000 (82%)] Loss: 16528.027344\n",
      "Train Epoch: 351 [187392/225000 (83%)] Loss: 16439.687500\n",
      "Train Epoch: 351 [189888/225000 (84%)] Loss: 16413.605469\n",
      "Train Epoch: 351 [192384/225000 (86%)] Loss: 15848.601562\n",
      "Train Epoch: 351 [194880/225000 (87%)] Loss: 16817.820312\n",
      "Train Epoch: 351 [197376/225000 (88%)] Loss: 16102.741211\n",
      "Train Epoch: 351 [199872/225000 (89%)] Loss: 16566.085938\n",
      "Train Epoch: 351 [202368/225000 (90%)] Loss: 16828.277344\n",
      "Train Epoch: 351 [204864/225000 (91%)] Loss: 16649.488281\n",
      "Train Epoch: 351 [207360/225000 (92%)] Loss: 18091.232422\n",
      "Train Epoch: 351 [209856/225000 (93%)] Loss: 16068.578125\n",
      "Train Epoch: 351 [212352/225000 (94%)] Loss: 16522.945312\n",
      "Train Epoch: 351 [214848/225000 (95%)] Loss: 16433.998047\n",
      "Train Epoch: 351 [217344/225000 (97%)] Loss: 16984.777344\n",
      "Train Epoch: 351 [219840/225000 (98%)] Loss: 16951.798828\n",
      "Train Epoch: 351 [222336/225000 (99%)] Loss: 16782.710938\n",
      "Train Epoch: 351 [224832/225000 (100%)] Loss: 16543.839844\n",
      "    epoch          : 351\n",
      "    loss           : 16721.616883365776\n",
      "    val_loss       : 16672.755781748823\n",
      "Train Epoch: 352 [192/225000 (0%)] Loss: 17137.917969\n",
      "Train Epoch: 352 [2688/225000 (1%)] Loss: 16610.882812\n",
      "Train Epoch: 352 [5184/225000 (2%)] Loss: 16603.921875\n",
      "Train Epoch: 352 [7680/225000 (3%)] Loss: 16497.177734\n",
      "Train Epoch: 352 [10176/225000 (5%)] Loss: 16857.167969\n",
      "Train Epoch: 352 [12672/225000 (6%)] Loss: 16375.119141\n",
      "Train Epoch: 352 [15168/225000 (7%)] Loss: 16125.500000\n",
      "Train Epoch: 352 [17664/225000 (8%)] Loss: 16330.616211\n",
      "Train Epoch: 352 [20160/225000 (9%)] Loss: 16725.656250\n",
      "Train Epoch: 352 [22656/225000 (10%)] Loss: 16498.511719\n",
      "Train Epoch: 352 [25152/225000 (11%)] Loss: 16513.027344\n",
      "Train Epoch: 352 [27648/225000 (12%)] Loss: 16878.808594\n",
      "Train Epoch: 352 [30144/225000 (13%)] Loss: 16032.400391\n",
      "Train Epoch: 352 [32640/225000 (15%)] Loss: 16831.763672\n",
      "Train Epoch: 352 [35136/225000 (16%)] Loss: 16686.363281\n",
      "Train Epoch: 352 [37632/225000 (17%)] Loss: 16759.988281\n",
      "Train Epoch: 352 [40128/225000 (18%)] Loss: 16788.859375\n",
      "Train Epoch: 352 [42624/225000 (19%)] Loss: 17148.378906\n",
      "Train Epoch: 352 [45120/225000 (20%)] Loss: 16867.320312\n",
      "Train Epoch: 352 [47616/225000 (21%)] Loss: 17159.156250\n",
      "Train Epoch: 352 [50112/225000 (22%)] Loss: 16827.609375\n",
      "Train Epoch: 352 [52608/225000 (23%)] Loss: 16906.275391\n",
      "Train Epoch: 352 [55104/225000 (24%)] Loss: 16555.097656\n",
      "Train Epoch: 352 [57600/225000 (26%)] Loss: 16275.503906\n",
      "Train Epoch: 352 [60096/225000 (27%)] Loss: 16724.312500\n",
      "Train Epoch: 352 [62592/225000 (28%)] Loss: 16846.359375\n",
      "Train Epoch: 352 [65088/225000 (29%)] Loss: 16900.437500\n",
      "Train Epoch: 352 [67584/225000 (30%)] Loss: 16492.066406\n",
      "Train Epoch: 352 [70080/225000 (31%)] Loss: 16213.362305\n",
      "Train Epoch: 352 [72576/225000 (32%)] Loss: 16583.609375\n",
      "Train Epoch: 352 [75072/225000 (33%)] Loss: 16554.080078\n",
      "Train Epoch: 352 [77568/225000 (34%)] Loss: 16576.644531\n",
      "Train Epoch: 352 [80064/225000 (36%)] Loss: 16437.357422\n",
      "Train Epoch: 352 [82560/225000 (37%)] Loss: 16662.181641\n",
      "Train Epoch: 352 [85056/225000 (38%)] Loss: 16676.945312\n",
      "Train Epoch: 352 [87552/225000 (39%)] Loss: 16998.582031\n",
      "Train Epoch: 352 [90048/225000 (40%)] Loss: 16555.666016\n",
      "Train Epoch: 352 [92544/225000 (41%)] Loss: 16519.367188\n",
      "Train Epoch: 352 [95040/225000 (42%)] Loss: 16758.839844\n",
      "Train Epoch: 352 [97536/225000 (43%)] Loss: 16414.755859\n",
      "Train Epoch: 352 [100032/225000 (44%)] Loss: 16546.300781\n",
      "Train Epoch: 352 [102528/225000 (46%)] Loss: 16389.035156\n",
      "Train Epoch: 352 [105024/225000 (47%)] Loss: 16661.597656\n",
      "Train Epoch: 352 [107520/225000 (48%)] Loss: 16448.669922\n",
      "Train Epoch: 352 [110016/225000 (49%)] Loss: 17895.566406\n",
      "Train Epoch: 352 [112512/225000 (50%)] Loss: 16407.248047\n",
      "Train Epoch: 352 [115008/225000 (51%)] Loss: 16225.262695\n",
      "Train Epoch: 352 [117504/225000 (52%)] Loss: 17103.625000\n",
      "Train Epoch: 352 [120000/225000 (53%)] Loss: 17935.757812\n",
      "Train Epoch: 352 [122496/225000 (54%)] Loss: 16514.394531\n",
      "Train Epoch: 352 [124992/225000 (56%)] Loss: 16712.019531\n",
      "Train Epoch: 352 [127488/225000 (57%)] Loss: 16676.914062\n",
      "Train Epoch: 352 [129984/225000 (58%)] Loss: 16959.589844\n",
      "Train Epoch: 352 [132480/225000 (59%)] Loss: 16683.804688\n",
      "Train Epoch: 352 [134976/225000 (60%)] Loss: 18533.214844\n",
      "Train Epoch: 352 [137472/225000 (61%)] Loss: 18748.410156\n",
      "Train Epoch: 352 [139968/225000 (62%)] Loss: 16891.699219\n",
      "Train Epoch: 352 [142464/225000 (63%)] Loss: 17048.232422\n",
      "Train Epoch: 352 [144960/225000 (64%)] Loss: 16305.655273\n",
      "Train Epoch: 352 [147456/225000 (66%)] Loss: 16508.296875\n",
      "Train Epoch: 352 [149952/225000 (67%)] Loss: 16758.332031\n",
      "Train Epoch: 352 [152448/225000 (68%)] Loss: 16363.525391\n",
      "Train Epoch: 352 [154944/225000 (69%)] Loss: 16805.630859\n",
      "Train Epoch: 352 [157440/225000 (70%)] Loss: 17301.832031\n",
      "Train Epoch: 352 [159936/225000 (71%)] Loss: 16063.538086\n",
      "Train Epoch: 352 [162432/225000 (72%)] Loss: 18090.371094\n",
      "Train Epoch: 352 [164928/225000 (73%)] Loss: 16416.669922\n",
      "Train Epoch: 352 [167424/225000 (74%)] Loss: 16864.962891\n",
      "Train Epoch: 352 [169920/225000 (76%)] Loss: 16612.406250\n",
      "Train Epoch: 352 [172416/225000 (77%)] Loss: 16745.765625\n",
      "Train Epoch: 352 [174912/225000 (78%)] Loss: 17179.527344\n",
      "Train Epoch: 352 [177408/225000 (79%)] Loss: 16546.380859\n",
      "Train Epoch: 352 [179904/225000 (80%)] Loss: 16546.097656\n",
      "Train Epoch: 352 [182400/225000 (81%)] Loss: 16247.459961\n",
      "Train Epoch: 352 [184896/225000 (82%)] Loss: 16546.136719\n",
      "Train Epoch: 352 [187392/225000 (83%)] Loss: 16978.576172\n",
      "Train Epoch: 352 [189888/225000 (84%)] Loss: 17961.367188\n",
      "Train Epoch: 352 [192384/225000 (86%)] Loss: 16606.109375\n",
      "Train Epoch: 352 [194880/225000 (87%)] Loss: 16926.816406\n",
      "Train Epoch: 352 [197376/225000 (88%)] Loss: 16483.074219\n",
      "Train Epoch: 352 [199872/225000 (89%)] Loss: 16475.886719\n",
      "Train Epoch: 352 [202368/225000 (90%)] Loss: 16718.171875\n",
      "Train Epoch: 352 [204864/225000 (91%)] Loss: 16412.046875\n",
      "Train Epoch: 352 [207360/225000 (92%)] Loss: 16713.480469\n",
      "Train Epoch: 352 [209856/225000 (93%)] Loss: 16297.181641\n",
      "Train Epoch: 352 [212352/225000 (94%)] Loss: 16250.652344\n",
      "Train Epoch: 352 [214848/225000 (95%)] Loss: 16413.699219\n",
      "Train Epoch: 352 [217344/225000 (97%)] Loss: 17869.068359\n",
      "Train Epoch: 352 [219840/225000 (98%)] Loss: 16885.351562\n",
      "Train Epoch: 352 [222336/225000 (99%)] Loss: 16677.564453\n",
      "Train Epoch: 352 [224832/225000 (100%)] Loss: 16509.498047\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   352: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   341: reducing learning rate of group 0 to 1.0000e-07.\n",
      "    epoch          : 352\n",
      "    loss           : 16728.754010405555\n",
      "    val_loss       : 16668.31685611947\n",
      "Train Epoch: 353 [192/225000 (0%)] Loss: 16570.240234\n",
      "Train Epoch: 353 [2688/225000 (1%)] Loss: 16630.843750\n",
      "Train Epoch: 353 [5184/225000 (2%)] Loss: 16956.162109\n",
      "Train Epoch: 353 [7680/225000 (3%)] Loss: 16628.898438\n",
      "Train Epoch: 353 [10176/225000 (5%)] Loss: 16765.173828\n",
      "Train Epoch: 353 [12672/225000 (6%)] Loss: 16845.636719\n",
      "Train Epoch: 353 [15168/225000 (7%)] Loss: 16732.320312\n",
      "Train Epoch: 353 [17664/225000 (8%)] Loss: 16853.894531\n",
      "Train Epoch: 353 [20160/225000 (9%)] Loss: 16360.581055\n",
      "Train Epoch: 353 [22656/225000 (10%)] Loss: 16477.125000\n",
      "Train Epoch: 353 [25152/225000 (11%)] Loss: 16498.777344\n",
      "Train Epoch: 353 [27648/225000 (12%)] Loss: 16382.288086\n",
      "Train Epoch: 353 [30144/225000 (13%)] Loss: 16831.130859\n",
      "Train Epoch: 353 [32640/225000 (15%)] Loss: 16213.173828\n",
      "Train Epoch: 353 [35136/225000 (16%)] Loss: 16528.015625\n",
      "Train Epoch: 353 [37632/225000 (17%)] Loss: 16598.119141\n",
      "Train Epoch: 353 [40128/225000 (18%)] Loss: 17121.058594\n",
      "Train Epoch: 353 [42624/225000 (19%)] Loss: 16667.062500\n",
      "Train Epoch: 353 [45120/225000 (20%)] Loss: 16506.169922\n",
      "Train Epoch: 353 [47616/225000 (21%)] Loss: 16346.980469\n",
      "Train Epoch: 353 [50112/225000 (22%)] Loss: 16726.363281\n",
      "Train Epoch: 353 [52608/225000 (23%)] Loss: 16609.017578\n",
      "Train Epoch: 353 [55104/225000 (24%)] Loss: 16758.121094\n",
      "Train Epoch: 353 [57600/225000 (26%)] Loss: 16795.208984\n",
      "Train Epoch: 353 [60096/225000 (27%)] Loss: 16371.447266\n",
      "Train Epoch: 353 [62592/225000 (28%)] Loss: 16669.496094\n",
      "Train Epoch: 353 [65088/225000 (29%)] Loss: 16664.277344\n",
      "Train Epoch: 353 [67584/225000 (30%)] Loss: 16080.952148\n",
      "Train Epoch: 353 [70080/225000 (31%)] Loss: 16790.808594\n",
      "Train Epoch: 353 [72576/225000 (32%)] Loss: 16482.437500\n",
      "Train Epoch: 353 [75072/225000 (33%)] Loss: 16506.615234\n",
      "Train Epoch: 353 [77568/225000 (34%)] Loss: 16995.076172\n",
      "Train Epoch: 353 [80064/225000 (36%)] Loss: 16677.035156\n",
      "Train Epoch: 353 [82560/225000 (37%)] Loss: 16575.824219\n",
      "Train Epoch: 353 [85056/225000 (38%)] Loss: 16298.354492\n",
      "Train Epoch: 353 [87552/225000 (39%)] Loss: 16592.441406\n",
      "Train Epoch: 353 [90048/225000 (40%)] Loss: 16588.107422\n",
      "Train Epoch: 353 [92544/225000 (41%)] Loss: 17036.701172\n",
      "Train Epoch: 353 [95040/225000 (42%)] Loss: 16650.085938\n",
      "Train Epoch: 353 [97536/225000 (43%)] Loss: 16762.417969\n",
      "Train Epoch: 353 [100032/225000 (44%)] Loss: 16928.304688\n",
      "Train Epoch: 353 [102528/225000 (46%)] Loss: 16750.156250\n",
      "Train Epoch: 353 [105024/225000 (47%)] Loss: 16633.974609\n",
      "Train Epoch: 353 [107520/225000 (48%)] Loss: 16790.039062\n",
      "Train Epoch: 353 [110016/225000 (49%)] Loss: 17074.179688\n",
      "Train Epoch: 353 [112512/225000 (50%)] Loss: 16413.996094\n",
      "Train Epoch: 353 [115008/225000 (51%)] Loss: 16772.015625\n",
      "Train Epoch: 353 [117504/225000 (52%)] Loss: 16276.669922\n",
      "Train Epoch: 353 [120000/225000 (53%)] Loss: 16493.154297\n",
      "Train Epoch: 353 [122496/225000 (54%)] Loss: 16678.708984\n",
      "Train Epoch: 353 [124992/225000 (56%)] Loss: 16509.519531\n",
      "Train Epoch: 353 [127488/225000 (57%)] Loss: 16481.900391\n",
      "Train Epoch: 353 [129984/225000 (58%)] Loss: 16622.921875\n",
      "Train Epoch: 353 [132480/225000 (59%)] Loss: 16563.066406\n",
      "Train Epoch: 353 [134976/225000 (60%)] Loss: 16731.968750\n",
      "Train Epoch: 353 [137472/225000 (61%)] Loss: 16619.949219\n",
      "Train Epoch: 353 [139968/225000 (62%)] Loss: 16661.203125\n",
      "Train Epoch: 353 [142464/225000 (63%)] Loss: 16235.322266\n",
      "Train Epoch: 353 [144960/225000 (64%)] Loss: 16501.804688\n",
      "Train Epoch: 353 [147456/225000 (66%)] Loss: 16564.597656\n",
      "Train Epoch: 353 [149952/225000 (67%)] Loss: 16708.458984\n",
      "Train Epoch: 353 [152448/225000 (68%)] Loss: 16978.515625\n",
      "Train Epoch: 353 [154944/225000 (69%)] Loss: 16983.796875\n",
      "Train Epoch: 353 [157440/225000 (70%)] Loss: 16644.921875\n",
      "Train Epoch: 353 [159936/225000 (71%)] Loss: 16253.464844\n",
      "Train Epoch: 353 [162432/225000 (72%)] Loss: 16214.928711\n",
      "Train Epoch: 353 [164928/225000 (73%)] Loss: 16436.736328\n",
      "Train Epoch: 353 [167424/225000 (74%)] Loss: 16339.218750\n",
      "Train Epoch: 353 [169920/225000 (76%)] Loss: 17153.417969\n",
      "Train Epoch: 353 [172416/225000 (77%)] Loss: 16816.496094\n",
      "Train Epoch: 353 [174912/225000 (78%)] Loss: 16822.173828\n",
      "Train Epoch: 353 [177408/225000 (79%)] Loss: 16649.802734\n",
      "Train Epoch: 353 [179904/225000 (80%)] Loss: 16815.138672\n",
      "Train Epoch: 353 [182400/225000 (81%)] Loss: 16300.011719\n",
      "Train Epoch: 353 [184896/225000 (82%)] Loss: 16903.080078\n",
      "Train Epoch: 353 [187392/225000 (83%)] Loss: 16839.343750\n",
      "Train Epoch: 353 [189888/225000 (84%)] Loss: 16296.258789\n",
      "Train Epoch: 353 [192384/225000 (86%)] Loss: 16536.648438\n",
      "Train Epoch: 353 [194880/225000 (87%)] Loss: 16121.901367\n",
      "Train Epoch: 353 [197376/225000 (88%)] Loss: 16392.671875\n",
      "Train Epoch: 353 [199872/225000 (89%)] Loss: 16383.831055\n",
      "Train Epoch: 353 [202368/225000 (90%)] Loss: 16797.224609\n",
      "Train Epoch: 353 [204864/225000 (91%)] Loss: 16348.544922\n",
      "Train Epoch: 353 [207360/225000 (92%)] Loss: 16646.796875\n",
      "Train Epoch: 353 [209856/225000 (93%)] Loss: 16255.439453\n",
      "Train Epoch: 353 [212352/225000 (94%)] Loss: 16161.623047\n",
      "Train Epoch: 353 [214848/225000 (95%)] Loss: 16673.562500\n",
      "Train Epoch: 353 [217344/225000 (97%)] Loss: 16915.742188\n",
      "Train Epoch: 353 [219840/225000 (98%)] Loss: 17018.226562\n",
      "Train Epoch: 353 [222336/225000 (99%)] Loss: 16372.222656\n",
      "Train Epoch: 353 [224832/225000 (100%)] Loss: 16912.554688\n",
      "    epoch          : 353\n",
      "    loss           : 16731.022031816607\n",
      "    val_loss       : 16638.11254440373\n",
      "Train Epoch: 354 [192/225000 (0%)] Loss: 16658.996094\n",
      "Train Epoch: 354 [2688/225000 (1%)] Loss: 16930.457031\n",
      "Train Epoch: 354 [5184/225000 (2%)] Loss: 17042.070312\n",
      "Train Epoch: 354 [7680/225000 (3%)] Loss: 16504.015625\n",
      "Train Epoch: 354 [10176/225000 (5%)] Loss: 16669.511719\n",
      "Train Epoch: 354 [12672/225000 (6%)] Loss: 16790.535156\n",
      "Train Epoch: 354 [15168/225000 (7%)] Loss: 16808.462891\n",
      "Train Epoch: 354 [17664/225000 (8%)] Loss: 16786.972656\n",
      "Train Epoch: 354 [20160/225000 (9%)] Loss: 16461.451172\n",
      "Train Epoch: 354 [22656/225000 (10%)] Loss: 16633.748047\n",
      "Train Epoch: 354 [25152/225000 (11%)] Loss: 16941.830078\n",
      "Train Epoch: 354 [27648/225000 (12%)] Loss: 16432.939453\n",
      "Train Epoch: 354 [30144/225000 (13%)] Loss: 17928.175781\n",
      "Train Epoch: 354 [32640/225000 (15%)] Loss: 16443.734375\n",
      "Train Epoch: 354 [35136/225000 (16%)] Loss: 16736.359375\n",
      "Train Epoch: 354 [37632/225000 (17%)] Loss: 16564.003906\n",
      "Train Epoch: 354 [40128/225000 (18%)] Loss: 16682.964844\n",
      "Train Epoch: 354 [42624/225000 (19%)] Loss: 16752.164062\n",
      "Train Epoch: 354 [45120/225000 (20%)] Loss: 16237.081055\n",
      "Train Epoch: 354 [47616/225000 (21%)] Loss: 16522.699219\n",
      "Train Epoch: 354 [50112/225000 (22%)] Loss: 16791.597656\n",
      "Train Epoch: 354 [52608/225000 (23%)] Loss: 16684.226562\n",
      "Train Epoch: 354 [55104/225000 (24%)] Loss: 18454.906250\n",
      "Train Epoch: 354 [57600/225000 (26%)] Loss: 17803.244141\n",
      "Train Epoch: 354 [60096/225000 (27%)] Loss: 16259.360352\n",
      "Train Epoch: 354 [62592/225000 (28%)] Loss: 16518.476562\n",
      "Train Epoch: 354 [65088/225000 (29%)] Loss: 17156.406250\n",
      "Train Epoch: 354 [67584/225000 (30%)] Loss: 16513.226562\n",
      "Train Epoch: 354 [70080/225000 (31%)] Loss: 16972.542969\n",
      "Train Epoch: 354 [72576/225000 (32%)] Loss: 16332.455078\n",
      "Train Epoch: 354 [75072/225000 (33%)] Loss: 16346.399414\n",
      "Train Epoch: 354 [77568/225000 (34%)] Loss: 16814.330078\n",
      "Train Epoch: 354 [80064/225000 (36%)] Loss: 16373.915039\n",
      "Train Epoch: 354 [82560/225000 (37%)] Loss: 16497.931641\n",
      "Train Epoch: 354 [85056/225000 (38%)] Loss: 15877.815430\n",
      "Train Epoch: 354 [87552/225000 (39%)] Loss: 16879.710938\n",
      "Train Epoch: 354 [90048/225000 (40%)] Loss: 16478.375000\n",
      "Train Epoch: 354 [92544/225000 (41%)] Loss: 16639.332031\n",
      "Train Epoch: 354 [95040/225000 (42%)] Loss: 16941.033203\n",
      "Train Epoch: 354 [97536/225000 (43%)] Loss: 16631.078125\n",
      "Train Epoch: 354 [100032/225000 (44%)] Loss: 16738.339844\n",
      "Train Epoch: 354 [102528/225000 (46%)] Loss: 16648.593750\n",
      "Train Epoch: 354 [105024/225000 (47%)] Loss: 16557.814453\n",
      "Train Epoch: 354 [107520/225000 (48%)] Loss: 16685.892578\n",
      "Train Epoch: 354 [110016/225000 (49%)] Loss: 16544.802734\n",
      "Train Epoch: 354 [112512/225000 (50%)] Loss: 16571.421875\n",
      "Train Epoch: 354 [115008/225000 (51%)] Loss: 16682.041016\n",
      "Train Epoch: 354 [117504/225000 (52%)] Loss: 17289.078125\n",
      "Train Epoch: 354 [120000/225000 (53%)] Loss: 16642.500000\n",
      "Train Epoch: 354 [122496/225000 (54%)] Loss: 16728.308594\n",
      "Train Epoch: 354 [124992/225000 (56%)] Loss: 16565.730469\n",
      "Train Epoch: 354 [127488/225000 (57%)] Loss: 16708.173828\n",
      "Train Epoch: 354 [129984/225000 (58%)] Loss: 16692.035156\n",
      "Train Epoch: 354 [132480/225000 (59%)] Loss: 17022.296875\n",
      "Train Epoch: 354 [134976/225000 (60%)] Loss: 16524.062500\n",
      "Train Epoch: 354 [137472/225000 (61%)] Loss: 16718.019531\n",
      "Train Epoch: 354 [139968/225000 (62%)] Loss: 16633.607422\n",
      "Train Epoch: 354 [142464/225000 (63%)] Loss: 16791.433594\n",
      "Train Epoch: 354 [144960/225000 (64%)] Loss: 16661.515625\n",
      "Train Epoch: 354 [147456/225000 (66%)] Loss: 16260.110352\n",
      "Train Epoch: 354 [149952/225000 (67%)] Loss: 17802.283203\n",
      "Train Epoch: 354 [152448/225000 (68%)] Loss: 16698.957031\n",
      "Train Epoch: 354 [154944/225000 (69%)] Loss: 16636.548828\n",
      "Train Epoch: 354 [157440/225000 (70%)] Loss: 16727.355469\n",
      "Train Epoch: 354 [159936/225000 (71%)] Loss: 16670.265625\n",
      "Train Epoch: 354 [162432/225000 (72%)] Loss: 16620.011719\n",
      "Train Epoch: 354 [164928/225000 (73%)] Loss: 16724.160156\n",
      "Train Epoch: 354 [167424/225000 (74%)] Loss: 16198.341797\n",
      "Train Epoch: 354 [169920/225000 (76%)] Loss: 16583.425781\n",
      "Train Epoch: 354 [172416/225000 (77%)] Loss: 16192.997070\n",
      "Train Epoch: 354 [174912/225000 (78%)] Loss: 16971.169922\n",
      "Train Epoch: 354 [177408/225000 (79%)] Loss: 16170.569336\n",
      "Train Epoch: 354 [179904/225000 (80%)] Loss: 16624.625000\n",
      "Train Epoch: 354 [182400/225000 (81%)] Loss: 16660.240234\n",
      "Train Epoch: 354 [184896/225000 (82%)] Loss: 16543.503906\n",
      "Train Epoch: 354 [187392/225000 (83%)] Loss: 16413.462891\n",
      "Train Epoch: 354 [189888/225000 (84%)] Loss: 16189.437500\n",
      "Train Epoch: 354 [192384/225000 (86%)] Loss: 16854.125000\n",
      "Train Epoch: 354 [194880/225000 (87%)] Loss: 17113.544922\n",
      "Train Epoch: 354 [197376/225000 (88%)] Loss: 17013.917969\n",
      "Train Epoch: 354 [199872/225000 (89%)] Loss: 16434.207031\n",
      "Train Epoch: 354 [202368/225000 (90%)] Loss: 16919.970703\n",
      "Train Epoch: 354 [204864/225000 (91%)] Loss: 16402.691406\n",
      "Train Epoch: 354 [207360/225000 (92%)] Loss: 16665.109375\n",
      "Train Epoch: 354 [209856/225000 (93%)] Loss: 16875.177734\n",
      "Train Epoch: 354 [212352/225000 (94%)] Loss: 16555.937500\n",
      "Train Epoch: 354 [214848/225000 (95%)] Loss: 16560.960938\n",
      "Train Epoch: 354 [217344/225000 (97%)] Loss: 16342.647461\n",
      "Train Epoch: 354 [219840/225000 (98%)] Loss: 16667.335938\n",
      "Train Epoch: 354 [222336/225000 (99%)] Loss: 16988.548828\n",
      "Train Epoch: 354 [224832/225000 (100%)] Loss: 16940.621094\n",
      "    epoch          : 354\n",
      "    loss           : 16724.995017198165\n",
      "    val_loss       : 16661.120653118796\n",
      "Train Epoch: 355 [192/225000 (0%)] Loss: 16834.140625\n",
      "Train Epoch: 355 [2688/225000 (1%)] Loss: 16694.814453\n",
      "Train Epoch: 355 [5184/225000 (2%)] Loss: 16536.693359\n",
      "Train Epoch: 355 [7680/225000 (3%)] Loss: 16555.800781\n",
      "Train Epoch: 355 [10176/225000 (5%)] Loss: 16791.960938\n",
      "Train Epoch: 355 [12672/225000 (6%)] Loss: 15817.088867\n",
      "Train Epoch: 355 [15168/225000 (7%)] Loss: 16727.031250\n",
      "Train Epoch: 355 [17664/225000 (8%)] Loss: 16605.511719\n",
      "Train Epoch: 355 [20160/225000 (9%)] Loss: 16689.875000\n",
      "Train Epoch: 355 [22656/225000 (10%)] Loss: 16117.243164\n",
      "Train Epoch: 355 [25152/225000 (11%)] Loss: 17338.902344\n",
      "Train Epoch: 355 [27648/225000 (12%)] Loss: 16538.658203\n",
      "Train Epoch: 355 [30144/225000 (13%)] Loss: 16489.839844\n",
      "Train Epoch: 355 [32640/225000 (15%)] Loss: 16693.886719\n",
      "Train Epoch: 355 [35136/225000 (16%)] Loss: 16419.132812\n",
      "Train Epoch: 355 [37632/225000 (17%)] Loss: 16529.988281\n",
      "Train Epoch: 355 [40128/225000 (18%)] Loss: 16943.443359\n",
      "Train Epoch: 355 [42624/225000 (19%)] Loss: 16850.214844\n",
      "Train Epoch: 355 [45120/225000 (20%)] Loss: 16519.533203\n",
      "Train Epoch: 355 [47616/225000 (21%)] Loss: 16806.187500\n",
      "Train Epoch: 355 [50112/225000 (22%)] Loss: 16275.237305\n",
      "Train Epoch: 355 [52608/225000 (23%)] Loss: 16257.768555\n",
      "Train Epoch: 355 [55104/225000 (24%)] Loss: 16654.921875\n",
      "Train Epoch: 355 [57600/225000 (26%)] Loss: 16690.988281\n",
      "Train Epoch: 355 [60096/225000 (27%)] Loss: 16255.373047\n",
      "Train Epoch: 355 [62592/225000 (28%)] Loss: 16266.770508\n",
      "Train Epoch: 355 [65088/225000 (29%)] Loss: 16306.670898\n",
      "Train Epoch: 355 [67584/225000 (30%)] Loss: 18109.425781\n",
      "Train Epoch: 355 [70080/225000 (31%)] Loss: 16420.304688\n",
      "Train Epoch: 355 [72576/225000 (32%)] Loss: 16437.794922\n",
      "Train Epoch: 355 [75072/225000 (33%)] Loss: 17105.015625\n",
      "Train Epoch: 355 [77568/225000 (34%)] Loss: 16413.324219\n",
      "Train Epoch: 355 [80064/225000 (36%)] Loss: 16662.062500\n",
      "Train Epoch: 355 [82560/225000 (37%)] Loss: 17083.224609\n",
      "Train Epoch: 355 [85056/225000 (38%)] Loss: 16702.320312\n",
      "Train Epoch: 355 [87552/225000 (39%)] Loss: 17577.871094\n",
      "Train Epoch: 355 [90048/225000 (40%)] Loss: 16474.314453\n",
      "Train Epoch: 355 [92544/225000 (41%)] Loss: 16671.113281\n",
      "Train Epoch: 355 [95040/225000 (42%)] Loss: 16861.564453\n",
      "Train Epoch: 355 [97536/225000 (43%)] Loss: 16695.185547\n",
      "Train Epoch: 355 [100032/225000 (44%)] Loss: 16673.884766\n",
      "Train Epoch: 355 [102528/225000 (46%)] Loss: 16097.210938\n",
      "Train Epoch: 355 [105024/225000 (47%)] Loss: 16014.008789\n",
      "Train Epoch: 355 [107520/225000 (48%)] Loss: 16453.388672\n",
      "Train Epoch: 355 [110016/225000 (49%)] Loss: 16802.994141\n",
      "Train Epoch: 355 [112512/225000 (50%)] Loss: 16552.763672\n",
      "Train Epoch: 355 [115008/225000 (51%)] Loss: 16698.623047\n",
      "Train Epoch: 355 [117504/225000 (52%)] Loss: 16051.292969\n",
      "Train Epoch: 355 [120000/225000 (53%)] Loss: 16482.621094\n",
      "Train Epoch: 355 [122496/225000 (54%)] Loss: 16593.427734\n",
      "Train Epoch: 355 [124992/225000 (56%)] Loss: 16812.654297\n",
      "Train Epoch: 355 [127488/225000 (57%)] Loss: 16202.334961\n",
      "Train Epoch: 355 [129984/225000 (58%)] Loss: 16223.548828\n",
      "Train Epoch: 355 [132480/225000 (59%)] Loss: 25166.851562\n",
      "Train Epoch: 355 [134976/225000 (60%)] Loss: 16339.730469\n",
      "Train Epoch: 355 [137472/225000 (61%)] Loss: 16926.832031\n",
      "Train Epoch: 355 [139968/225000 (62%)] Loss: 16657.312500\n",
      "Train Epoch: 355 [142464/225000 (63%)] Loss: 16511.896484\n",
      "Train Epoch: 355 [144960/225000 (64%)] Loss: 16674.957031\n",
      "Train Epoch: 355 [147456/225000 (66%)] Loss: 16222.867188\n",
      "Train Epoch: 355 [149952/225000 (67%)] Loss: 16641.726562\n",
      "Train Epoch: 355 [152448/225000 (68%)] Loss: 16864.480469\n",
      "Train Epoch: 355 [154944/225000 (69%)] Loss: 16582.136719\n",
      "Train Epoch: 355 [157440/225000 (70%)] Loss: 16454.759766\n",
      "Train Epoch: 355 [159936/225000 (71%)] Loss: 16350.498047\n",
      "Train Epoch: 355 [162432/225000 (72%)] Loss: 16269.438477\n",
      "Train Epoch: 355 [164928/225000 (73%)] Loss: 16505.957031\n",
      "Train Epoch: 355 [167424/225000 (74%)] Loss: 16483.328125\n",
      "Train Epoch: 355 [169920/225000 (76%)] Loss: 16565.878906\n",
      "Train Epoch: 355 [172416/225000 (77%)] Loss: 16258.509766\n",
      "Train Epoch: 355 [174912/225000 (78%)] Loss: 16142.002930\n",
      "Train Epoch: 355 [177408/225000 (79%)] Loss: 16677.093750\n",
      "Train Epoch: 355 [179904/225000 (80%)] Loss: 16612.937500\n",
      "Train Epoch: 355 [182400/225000 (81%)] Loss: 17069.277344\n",
      "Train Epoch: 355 [184896/225000 (82%)] Loss: 16708.652344\n",
      "Train Epoch: 355 [187392/225000 (83%)] Loss: 16436.222656\n",
      "Train Epoch: 355 [189888/225000 (84%)] Loss: 16687.515625\n",
      "Train Epoch: 355 [192384/225000 (86%)] Loss: 16442.179688\n",
      "Train Epoch: 355 [194880/225000 (87%)] Loss: 16908.955078\n",
      "Train Epoch: 355 [197376/225000 (88%)] Loss: 16328.656250\n",
      "Train Epoch: 355 [199872/225000 (89%)] Loss: 16948.517578\n",
      "Train Epoch: 355 [202368/225000 (90%)] Loss: 17091.238281\n",
      "Train Epoch: 355 [204864/225000 (91%)] Loss: 16560.388672\n",
      "Train Epoch: 355 [207360/225000 (92%)] Loss: 16924.058594\n",
      "Train Epoch: 355 [209856/225000 (93%)] Loss: 17006.625000\n",
      "Train Epoch: 355 [212352/225000 (94%)] Loss: 16635.613281\n",
      "Train Epoch: 355 [214848/225000 (95%)] Loss: 16939.378906\n",
      "Train Epoch: 355 [217344/225000 (97%)] Loss: 16934.964844\n",
      "Train Epoch: 355 [219840/225000 (98%)] Loss: 17057.609375\n",
      "Train Epoch: 355 [222336/225000 (99%)] Loss: 16639.871094\n",
      "Train Epoch: 355 [224832/225000 (100%)] Loss: 16325.075195\n",
      "    epoch          : 355\n",
      "    loss           : 16720.438309080364\n",
      "    val_loss       : 16612.566699895695\n",
      "Train Epoch: 356 [192/225000 (0%)] Loss: 16520.070312\n",
      "Train Epoch: 356 [2688/225000 (1%)] Loss: 16776.511719\n",
      "Train Epoch: 356 [5184/225000 (2%)] Loss: 16544.695312\n",
      "Train Epoch: 356 [7680/225000 (3%)] Loss: 16908.953125\n",
      "Train Epoch: 356 [10176/225000 (5%)] Loss: 16903.232422\n",
      "Train Epoch: 356 [12672/225000 (6%)] Loss: 16615.722656\n",
      "Train Epoch: 356 [15168/225000 (7%)] Loss: 16350.365234\n",
      "Train Epoch: 356 [17664/225000 (8%)] Loss: 16194.248047\n",
      "Train Epoch: 356 [20160/225000 (9%)] Loss: 16809.226562\n",
      "Train Epoch: 356 [22656/225000 (10%)] Loss: 16626.171875\n",
      "Train Epoch: 356 [25152/225000 (11%)] Loss: 16863.945312\n",
      "Train Epoch: 356 [27648/225000 (12%)] Loss: 16537.218750\n",
      "Train Epoch: 356 [30144/225000 (13%)] Loss: 16680.160156\n",
      "Train Epoch: 356 [32640/225000 (15%)] Loss: 16438.097656\n",
      "Train Epoch: 356 [35136/225000 (16%)] Loss: 16656.250000\n",
      "Train Epoch: 356 [37632/225000 (17%)] Loss: 16664.660156\n",
      "Train Epoch: 356 [40128/225000 (18%)] Loss: 16818.462891\n",
      "Train Epoch: 356 [42624/225000 (19%)] Loss: 17056.113281\n",
      "Train Epoch: 356 [45120/225000 (20%)] Loss: 16327.556641\n",
      "Train Epoch: 356 [47616/225000 (21%)] Loss: 16849.787109\n",
      "Train Epoch: 356 [50112/225000 (22%)] Loss: 16690.328125\n",
      "Train Epoch: 356 [52608/225000 (23%)] Loss: 16300.271484\n",
      "Train Epoch: 356 [55104/225000 (24%)] Loss: 16184.105469\n",
      "Train Epoch: 356 [57600/225000 (26%)] Loss: 16814.441406\n",
      "Train Epoch: 356 [60096/225000 (27%)] Loss: 16133.604492\n",
      "Train Epoch: 356 [62592/225000 (28%)] Loss: 16453.335938\n",
      "Train Epoch: 356 [65088/225000 (29%)] Loss: 16304.790039\n",
      "Train Epoch: 356 [67584/225000 (30%)] Loss: 17992.050781\n",
      "Train Epoch: 356 [70080/225000 (31%)] Loss: 16415.414062\n",
      "Train Epoch: 356 [72576/225000 (32%)] Loss: 16587.964844\n",
      "Train Epoch: 356 [75072/225000 (33%)] Loss: 16779.757812\n",
      "Train Epoch: 356 [77568/225000 (34%)] Loss: 16590.457031\n",
      "Train Epoch: 356 [80064/225000 (36%)] Loss: 18056.894531\n",
      "Train Epoch: 356 [82560/225000 (37%)] Loss: 16365.583008\n",
      "Train Epoch: 356 [85056/225000 (38%)] Loss: 16954.583984\n",
      "Train Epoch: 356 [87552/225000 (39%)] Loss: 16230.964844\n",
      "Train Epoch: 356 [90048/225000 (40%)] Loss: 16718.761719\n",
      "Train Epoch: 356 [92544/225000 (41%)] Loss: 16733.691406\n",
      "Train Epoch: 356 [95040/225000 (42%)] Loss: 17152.691406\n",
      "Train Epoch: 356 [97536/225000 (43%)] Loss: 18431.644531\n",
      "Train Epoch: 356 [100032/225000 (44%)] Loss: 16867.134766\n",
      "Train Epoch: 356 [102528/225000 (46%)] Loss: 16776.388672\n",
      "Train Epoch: 356 [105024/225000 (47%)] Loss: 16386.625000\n",
      "Train Epoch: 356 [107520/225000 (48%)] Loss: 16990.097656\n",
      "Train Epoch: 356 [110016/225000 (49%)] Loss: 16295.666016\n",
      "Train Epoch: 356 [112512/225000 (50%)] Loss: 16572.458984\n",
      "Train Epoch: 356 [115008/225000 (51%)] Loss: 16006.344727\n",
      "Train Epoch: 356 [117504/225000 (52%)] Loss: 16566.333984\n",
      "Train Epoch: 356 [120000/225000 (53%)] Loss: 16682.501953\n",
      "Train Epoch: 356 [122496/225000 (54%)] Loss: 16836.166016\n",
      "Train Epoch: 356 [124992/225000 (56%)] Loss: 16700.128906\n",
      "Train Epoch: 356 [127488/225000 (57%)] Loss: 16035.787109\n",
      "Train Epoch: 356 [129984/225000 (58%)] Loss: 16695.398438\n",
      "Train Epoch: 356 [132480/225000 (59%)] Loss: 15996.995117\n",
      "Train Epoch: 356 [134976/225000 (60%)] Loss: 16178.695312\n",
      "Train Epoch: 356 [137472/225000 (61%)] Loss: 16332.660156\n",
      "Train Epoch: 356 [139968/225000 (62%)] Loss: 16316.083008\n",
      "Train Epoch: 356 [142464/225000 (63%)] Loss: 17000.640625\n",
      "Train Epoch: 356 [144960/225000 (64%)] Loss: 16735.207031\n",
      "Train Epoch: 356 [147456/225000 (66%)] Loss: 17075.343750\n",
      "Train Epoch: 356 [149952/225000 (67%)] Loss: 17019.820312\n",
      "Train Epoch: 356 [152448/225000 (68%)] Loss: 17103.160156\n",
      "Train Epoch: 356 [154944/225000 (69%)] Loss: 16518.738281\n",
      "Train Epoch: 356 [157440/225000 (70%)] Loss: 16620.099609\n",
      "Train Epoch: 356 [159936/225000 (71%)] Loss: 16739.937500\n",
      "Train Epoch: 356 [162432/225000 (72%)] Loss: 16720.679688\n",
      "Train Epoch: 356 [164928/225000 (73%)] Loss: 16389.402344\n",
      "Train Epoch: 356 [167424/225000 (74%)] Loss: 16539.496094\n",
      "Train Epoch: 356 [169920/225000 (76%)] Loss: 16461.751953\n",
      "Train Epoch: 356 [172416/225000 (77%)] Loss: 16264.228516\n",
      "Train Epoch: 356 [174912/225000 (78%)] Loss: 16625.296875\n",
      "Train Epoch: 356 [177408/225000 (79%)] Loss: 16853.486328\n",
      "Train Epoch: 356 [179904/225000 (80%)] Loss: 16181.087891\n",
      "Train Epoch: 356 [182400/225000 (81%)] Loss: 16479.792969\n",
      "Train Epoch: 356 [184896/225000 (82%)] Loss: 16752.917969\n",
      "Train Epoch: 356 [187392/225000 (83%)] Loss: 18037.367188\n",
      "Train Epoch: 356 [189888/225000 (84%)] Loss: 16481.988281\n",
      "Train Epoch: 356 [192384/225000 (86%)] Loss: 16695.177734\n",
      "Train Epoch: 356 [194880/225000 (87%)] Loss: 18439.076172\n",
      "Train Epoch: 356 [197376/225000 (88%)] Loss: 16378.413086\n",
      "Train Epoch: 356 [199872/225000 (89%)] Loss: 16476.554688\n",
      "Train Epoch: 356 [202368/225000 (90%)] Loss: 16645.984375\n",
      "Train Epoch: 356 [204864/225000 (91%)] Loss: 18571.277344\n",
      "Train Epoch: 356 [207360/225000 (92%)] Loss: 16873.177734\n",
      "Train Epoch: 356 [209856/225000 (93%)] Loss: 16278.312500\n",
      "Train Epoch: 356 [212352/225000 (94%)] Loss: 16975.261719\n",
      "Train Epoch: 356 [214848/225000 (95%)] Loss: 16787.253906\n",
      "Train Epoch: 356 [217344/225000 (97%)] Loss: 17207.085938\n",
      "Train Epoch: 356 [219840/225000 (98%)] Loss: 25128.062500\n",
      "Train Epoch: 356 [222336/225000 (99%)] Loss: 16555.847656\n",
      "Train Epoch: 356 [224832/225000 (100%)] Loss: 16628.460938\n",
      "    epoch          : 356\n",
      "    loss           : 16732.411211137478\n",
      "    val_loss       : 16662.416061851814\n",
      "Train Epoch: 357 [192/225000 (0%)] Loss: 16537.355469\n",
      "Train Epoch: 357 [2688/225000 (1%)] Loss: 16520.332031\n",
      "Train Epoch: 357 [5184/225000 (2%)] Loss: 16379.208984\n",
      "Train Epoch: 357 [7680/225000 (3%)] Loss: 16336.013672\n",
      "Train Epoch: 357 [10176/225000 (5%)] Loss: 16045.656250\n",
      "Train Epoch: 357 [12672/225000 (6%)] Loss: 16811.667969\n",
      "Train Epoch: 357 [15168/225000 (7%)] Loss: 16855.958984\n",
      "Train Epoch: 357 [17664/225000 (8%)] Loss: 16604.974609\n",
      "Train Epoch: 357 [20160/225000 (9%)] Loss: 16352.505859\n",
      "Train Epoch: 357 [22656/225000 (10%)] Loss: 16501.650391\n",
      "Train Epoch: 357 [25152/225000 (11%)] Loss: 16983.632812\n",
      "Train Epoch: 357 [27648/225000 (12%)] Loss: 16484.609375\n",
      "Train Epoch: 357 [30144/225000 (13%)] Loss: 16116.289062\n",
      "Train Epoch: 357 [32640/225000 (15%)] Loss: 16491.320312\n",
      "Train Epoch: 357 [35136/225000 (16%)] Loss: 18115.238281\n",
      "Train Epoch: 357 [37632/225000 (17%)] Loss: 16728.101562\n",
      "Train Epoch: 357 [40128/225000 (18%)] Loss: 19296.353516\n",
      "Train Epoch: 357 [42624/225000 (19%)] Loss: 17149.968750\n",
      "Train Epoch: 357 [45120/225000 (20%)] Loss: 16564.484375\n",
      "Train Epoch: 357 [47616/225000 (21%)] Loss: 16508.015625\n",
      "Train Epoch: 357 [50112/225000 (22%)] Loss: 16441.787109\n",
      "Train Epoch: 357 [52608/225000 (23%)] Loss: 16255.586914\n",
      "Train Epoch: 357 [55104/225000 (24%)] Loss: 16515.609375\n",
      "Train Epoch: 357 [57600/225000 (26%)] Loss: 16787.578125\n",
      "Train Epoch: 357 [60096/225000 (27%)] Loss: 16386.132812\n",
      "Train Epoch: 357 [62592/225000 (28%)] Loss: 16923.972656\n",
      "Train Epoch: 357 [65088/225000 (29%)] Loss: 17192.593750\n",
      "Train Epoch: 357 [67584/225000 (30%)] Loss: 16288.093750\n",
      "Train Epoch: 357 [70080/225000 (31%)] Loss: 16905.234375\n",
      "Train Epoch: 357 [72576/225000 (32%)] Loss: 16450.070312\n",
      "Train Epoch: 357 [75072/225000 (33%)] Loss: 16844.050781\n",
      "Train Epoch: 357 [77568/225000 (34%)] Loss: 16867.783203\n",
      "Train Epoch: 357 [80064/225000 (36%)] Loss: 16344.759766\n",
      "Train Epoch: 357 [82560/225000 (37%)] Loss: 16896.986328\n",
      "Train Epoch: 357 [85056/225000 (38%)] Loss: 16025.807617\n",
      "Train Epoch: 357 [87552/225000 (39%)] Loss: 18211.183594\n",
      "Train Epoch: 357 [90048/225000 (40%)] Loss: 18223.191406\n",
      "Train Epoch: 357 [92544/225000 (41%)] Loss: 16625.845703\n",
      "Train Epoch: 357 [95040/225000 (42%)] Loss: 16251.166992\n",
      "Train Epoch: 357 [97536/225000 (43%)] Loss: 16758.759766\n",
      "Train Epoch: 357 [100032/225000 (44%)] Loss: 16204.316406\n",
      "Train Epoch: 357 [102528/225000 (46%)] Loss: 16417.753906\n",
      "Train Epoch: 357 [105024/225000 (47%)] Loss: 16600.734375\n",
      "Train Epoch: 357 [107520/225000 (48%)] Loss: 16781.816406\n",
      "Train Epoch: 357 [110016/225000 (49%)] Loss: 16403.949219\n",
      "Train Epoch: 357 [112512/225000 (50%)] Loss: 16501.777344\n",
      "Train Epoch: 357 [115008/225000 (51%)] Loss: 17003.968750\n",
      "Train Epoch: 357 [117504/225000 (52%)] Loss: 17168.746094\n",
      "Train Epoch: 357 [120000/225000 (53%)] Loss: 17385.980469\n",
      "Train Epoch: 357 [122496/225000 (54%)] Loss: 16815.542969\n",
      "Train Epoch: 357 [124992/225000 (56%)] Loss: 16779.210938\n",
      "Train Epoch: 357 [127488/225000 (57%)] Loss: 16801.941406\n",
      "Train Epoch: 357 [129984/225000 (58%)] Loss: 16459.335938\n",
      "Train Epoch: 357 [132480/225000 (59%)] Loss: 16534.585938\n",
      "Train Epoch: 357 [134976/225000 (60%)] Loss: 16540.472656\n",
      "Train Epoch: 357 [137472/225000 (61%)] Loss: 18274.824219\n",
      "Train Epoch: 357 [139968/225000 (62%)] Loss: 16512.058594\n",
      "Train Epoch: 357 [142464/225000 (63%)] Loss: 16679.080078\n",
      "Train Epoch: 357 [144960/225000 (64%)] Loss: 17001.906250\n",
      "Train Epoch: 357 [147456/225000 (66%)] Loss: 16407.902344\n",
      "Train Epoch: 357 [149952/225000 (67%)] Loss: 16640.476562\n",
      "Train Epoch: 357 [152448/225000 (68%)] Loss: 16962.910156\n",
      "Train Epoch: 357 [154944/225000 (69%)] Loss: 16640.964844\n",
      "Train Epoch: 357 [157440/225000 (70%)] Loss: 16355.774414\n",
      "Train Epoch: 357 [159936/225000 (71%)] Loss: 16621.757812\n",
      "Train Epoch: 357 [162432/225000 (72%)] Loss: 16398.734375\n",
      "Train Epoch: 357 [164928/225000 (73%)] Loss: 16678.160156\n",
      "Train Epoch: 357 [167424/225000 (74%)] Loss: 16814.531250\n",
      "Train Epoch: 357 [169920/225000 (76%)] Loss: 16536.078125\n",
      "Train Epoch: 357 [172416/225000 (77%)] Loss: 17119.179688\n",
      "Train Epoch: 357 [174912/225000 (78%)] Loss: 16730.484375\n",
      "Train Epoch: 357 [177408/225000 (79%)] Loss: 16563.949219\n",
      "Train Epoch: 357 [179904/225000 (80%)] Loss: 16333.280273\n",
      "Train Epoch: 357 [182400/225000 (81%)] Loss: 16445.679688\n",
      "Train Epoch: 357 [184896/225000 (82%)] Loss: 16756.681641\n",
      "Train Epoch: 357 [187392/225000 (83%)] Loss: 16698.648438\n",
      "Train Epoch: 357 [189888/225000 (84%)] Loss: 16358.741211\n",
      "Train Epoch: 357 [192384/225000 (86%)] Loss: 16632.128906\n",
      "Train Epoch: 357 [194880/225000 (87%)] Loss: 18914.691406\n",
      "Train Epoch: 357 [197376/225000 (88%)] Loss: 16277.538086\n",
      "Train Epoch: 357 [199872/225000 (89%)] Loss: 16839.132812\n",
      "Train Epoch: 357 [202368/225000 (90%)] Loss: 16543.269531\n",
      "Train Epoch: 357 [204864/225000 (91%)] Loss: 16302.949219\n",
      "Train Epoch: 357 [207360/225000 (92%)] Loss: 16402.640625\n",
      "Train Epoch: 357 [209856/225000 (93%)] Loss: 16324.089844\n",
      "Train Epoch: 357 [212352/225000 (94%)] Loss: 16799.902344\n",
      "Train Epoch: 357 [214848/225000 (95%)] Loss: 16677.652344\n",
      "Train Epoch: 357 [217344/225000 (97%)] Loss: 16446.597656\n",
      "Train Epoch: 357 [219840/225000 (98%)] Loss: 16772.355469\n",
      "Train Epoch: 357 [222336/225000 (99%)] Loss: 16502.222656\n",
      "Train Epoch: 357 [224832/225000 (100%)] Loss: 16767.390625\n",
      "    epoch          : 357\n",
      "    loss           : 16734.62324352069\n",
      "    val_loss       : 16668.902740319267\n",
      "Train Epoch: 358 [192/225000 (0%)] Loss: 16355.864258\n",
      "Train Epoch: 358 [2688/225000 (1%)] Loss: 16794.437500\n",
      "Train Epoch: 358 [5184/225000 (2%)] Loss: 16477.457031\n",
      "Train Epoch: 358 [7680/225000 (3%)] Loss: 16416.080078\n",
      "Train Epoch: 358 [10176/225000 (5%)] Loss: 16881.406250\n",
      "Train Epoch: 358 [12672/225000 (6%)] Loss: 16353.250977\n",
      "Train Epoch: 358 [15168/225000 (7%)] Loss: 17129.718750\n",
      "Train Epoch: 358 [17664/225000 (8%)] Loss: 16583.058594\n",
      "Train Epoch: 358 [20160/225000 (9%)] Loss: 16872.867188\n",
      "Train Epoch: 358 [22656/225000 (10%)] Loss: 16906.812500\n",
      "Train Epoch: 358 [25152/225000 (11%)] Loss: 16718.011719\n",
      "Train Epoch: 358 [27648/225000 (12%)] Loss: 16936.113281\n",
      "Train Epoch: 358 [30144/225000 (13%)] Loss: 16911.007812\n",
      "Train Epoch: 358 [32640/225000 (15%)] Loss: 16652.863281\n",
      "Train Epoch: 358 [35136/225000 (16%)] Loss: 16617.017578\n",
      "Train Epoch: 358 [37632/225000 (17%)] Loss: 16467.693359\n",
      "Train Epoch: 358 [40128/225000 (18%)] Loss: 17553.804688\n",
      "Train Epoch: 358 [42624/225000 (19%)] Loss: 16446.316406\n",
      "Train Epoch: 358 [45120/225000 (20%)] Loss: 16374.428711\n",
      "Train Epoch: 358 [47616/225000 (21%)] Loss: 16319.341797\n",
      "Train Epoch: 358 [50112/225000 (22%)] Loss: 16143.338867\n",
      "Train Epoch: 358 [52608/225000 (23%)] Loss: 16522.617188\n",
      "Train Epoch: 358 [55104/225000 (24%)] Loss: 16490.851562\n",
      "Train Epoch: 358 [57600/225000 (26%)] Loss: 17103.914062\n",
      "Train Epoch: 358 [60096/225000 (27%)] Loss: 16686.494141\n",
      "Train Epoch: 358 [62592/225000 (28%)] Loss: 16689.105469\n",
      "Train Epoch: 358 [65088/225000 (29%)] Loss: 16709.642578\n",
      "Train Epoch: 358 [67584/225000 (30%)] Loss: 16983.886719\n",
      "Train Epoch: 358 [70080/225000 (31%)] Loss: 18050.042969\n",
      "Train Epoch: 358 [72576/225000 (32%)] Loss: 16509.359375\n",
      "Train Epoch: 358 [75072/225000 (33%)] Loss: 16216.842773\n",
      "Train Epoch: 358 [77568/225000 (34%)] Loss: 16565.919922\n",
      "Train Epoch: 358 [80064/225000 (36%)] Loss: 16718.355469\n",
      "Train Epoch: 358 [82560/225000 (37%)] Loss: 16946.636719\n",
      "Train Epoch: 358 [85056/225000 (38%)] Loss: 16431.062500\n",
      "Train Epoch: 358 [87552/225000 (39%)] Loss: 17065.945312\n",
      "Train Epoch: 358 [90048/225000 (40%)] Loss: 16795.648438\n",
      "Train Epoch: 358 [92544/225000 (41%)] Loss: 17877.757812\n",
      "Train Epoch: 358 [95040/225000 (42%)] Loss: 16411.718750\n",
      "Train Epoch: 358 [97536/225000 (43%)] Loss: 16805.527344\n",
      "Train Epoch: 358 [100032/225000 (44%)] Loss: 16695.169922\n",
      "Train Epoch: 358 [102528/225000 (46%)] Loss: 16396.345703\n",
      "Train Epoch: 358 [105024/225000 (47%)] Loss: 16801.394531\n",
      "Train Epoch: 358 [107520/225000 (48%)] Loss: 16081.838867\n",
      "Train Epoch: 358 [110016/225000 (49%)] Loss: 16532.355469\n",
      "Train Epoch: 358 [112512/225000 (50%)] Loss: 16420.824219\n",
      "Train Epoch: 358 [115008/225000 (51%)] Loss: 16795.632812\n",
      "Train Epoch: 358 [117504/225000 (52%)] Loss: 16578.626953\n",
      "Train Epoch: 358 [120000/225000 (53%)] Loss: 16533.181641\n",
      "Train Epoch: 358 [122496/225000 (54%)] Loss: 16686.822266\n",
      "Train Epoch: 358 [124992/225000 (56%)] Loss: 16715.730469\n",
      "Train Epoch: 358 [127488/225000 (57%)] Loss: 16623.978516\n",
      "Train Epoch: 358 [129984/225000 (58%)] Loss: 16278.315430\n",
      "Train Epoch: 358 [132480/225000 (59%)] Loss: 16832.671875\n",
      "Train Epoch: 358 [134976/225000 (60%)] Loss: 16566.722656\n",
      "Train Epoch: 358 [137472/225000 (61%)] Loss: 16385.000000\n",
      "Train Epoch: 358 [139968/225000 (62%)] Loss: 16591.523438\n",
      "Train Epoch: 358 [142464/225000 (63%)] Loss: 17082.558594\n",
      "Train Epoch: 358 [144960/225000 (64%)] Loss: 16926.548828\n",
      "Train Epoch: 358 [147456/225000 (66%)] Loss: 16214.589844\n",
      "Train Epoch: 358 [149952/225000 (67%)] Loss: 16114.248047\n",
      "Train Epoch: 358 [152448/225000 (68%)] Loss: 16681.525391\n",
      "Train Epoch: 358 [154944/225000 (69%)] Loss: 16932.050781\n",
      "Train Epoch: 358 [157440/225000 (70%)] Loss: 16607.800781\n",
      "Train Epoch: 358 [159936/225000 (71%)] Loss: 16884.056641\n",
      "Train Epoch: 358 [162432/225000 (72%)] Loss: 16319.169922\n",
      "Train Epoch: 358 [164928/225000 (73%)] Loss: 17099.921875\n",
      "Train Epoch: 358 [167424/225000 (74%)] Loss: 17005.007812\n",
      "Train Epoch: 358 [169920/225000 (76%)] Loss: 16542.617188\n",
      "Train Epoch: 358 [172416/225000 (77%)] Loss: 16414.078125\n",
      "Train Epoch: 358 [174912/225000 (78%)] Loss: 16822.623047\n",
      "Train Epoch: 358 [177408/225000 (79%)] Loss: 16484.712891\n",
      "Train Epoch: 358 [179904/225000 (80%)] Loss: 16813.144531\n",
      "Train Epoch: 358 [182400/225000 (81%)] Loss: 16329.581055\n",
      "Train Epoch: 358 [184896/225000 (82%)] Loss: 16704.011719\n",
      "Train Epoch: 358 [187392/225000 (83%)] Loss: 16515.371094\n",
      "Train Epoch: 358 [189888/225000 (84%)] Loss: 17274.869141\n",
      "Train Epoch: 358 [192384/225000 (86%)] Loss: 16768.404297\n",
      "Train Epoch: 358 [194880/225000 (87%)] Loss: 16976.353516\n",
      "Train Epoch: 358 [197376/225000 (88%)] Loss: 16909.667969\n",
      "Train Epoch: 358 [199872/225000 (89%)] Loss: 16698.578125\n",
      "Train Epoch: 358 [202368/225000 (90%)] Loss: 16832.757812\n",
      "Train Epoch: 358 [204864/225000 (91%)] Loss: 16662.240234\n",
      "Train Epoch: 358 [207360/225000 (92%)] Loss: 16486.519531\n",
      "Train Epoch: 358 [209856/225000 (93%)] Loss: 16230.907227\n",
      "Train Epoch: 358 [212352/225000 (94%)] Loss: 16320.370117\n",
      "Train Epoch: 358 [214848/225000 (95%)] Loss: 16978.310547\n",
      "Train Epoch: 358 [217344/225000 (97%)] Loss: 16899.500000\n",
      "Train Epoch: 358 [219840/225000 (98%)] Loss: 16493.730469\n",
      "Train Epoch: 358 [222336/225000 (99%)] Loss: 18068.484375\n",
      "Train Epoch: 358 [224832/225000 (100%)] Loss: 16466.734375\n",
      "    epoch          : 358\n",
      "    loss           : 16744.616220936434\n",
      "    val_loss       : 16612.23158299104\n",
      "Train Epoch: 359 [192/225000 (0%)] Loss: 16548.847656\n",
      "Train Epoch: 359 [2688/225000 (1%)] Loss: 16949.962891\n",
      "Train Epoch: 359 [5184/225000 (2%)] Loss: 16629.890625\n",
      "Train Epoch: 359 [7680/225000 (3%)] Loss: 18328.878906\n",
      "Train Epoch: 359 [10176/225000 (5%)] Loss: 16624.941406\n",
      "Train Epoch: 359 [12672/225000 (6%)] Loss: 17175.398438\n",
      "Train Epoch: 359 [15168/225000 (7%)] Loss: 16574.562500\n",
      "Train Epoch: 359 [17664/225000 (8%)] Loss: 16151.979492\n",
      "Train Epoch: 359 [20160/225000 (9%)] Loss: 16107.848633\n",
      "Train Epoch: 359 [22656/225000 (10%)] Loss: 17118.507812\n",
      "Train Epoch: 359 [25152/225000 (11%)] Loss: 16735.138672\n",
      "Train Epoch: 359 [27648/225000 (12%)] Loss: 16629.642578\n",
      "Train Epoch: 359 [30144/225000 (13%)] Loss: 16674.027344\n",
      "Train Epoch: 359 [32640/225000 (15%)] Loss: 16589.898438\n",
      "Train Epoch: 359 [35136/225000 (16%)] Loss: 16637.222656\n",
      "Train Epoch: 359 [37632/225000 (17%)] Loss: 17084.964844\n",
      "Train Epoch: 359 [40128/225000 (18%)] Loss: 16625.933594\n",
      "Train Epoch: 359 [42624/225000 (19%)] Loss: 16715.128906\n",
      "Train Epoch: 359 [45120/225000 (20%)] Loss: 16279.006836\n",
      "Train Epoch: 359 [47616/225000 (21%)] Loss: 16774.798828\n",
      "Train Epoch: 359 [50112/225000 (22%)] Loss: 16772.453125\n",
      "Train Epoch: 359 [52608/225000 (23%)] Loss: 16824.093750\n",
      "Train Epoch: 359 [55104/225000 (24%)] Loss: 16402.156250\n",
      "Train Epoch: 359 [57600/225000 (26%)] Loss: 16791.986328\n",
      "Train Epoch: 359 [60096/225000 (27%)] Loss: 17100.359375\n",
      "Train Epoch: 359 [62592/225000 (28%)] Loss: 16787.253906\n",
      "Train Epoch: 359 [65088/225000 (29%)] Loss: 16351.377930\n",
      "Train Epoch: 359 [67584/225000 (30%)] Loss: 16399.728516\n",
      "Train Epoch: 359 [70080/225000 (31%)] Loss: 16448.984375\n",
      "Train Epoch: 359 [72576/225000 (32%)] Loss: 16725.234375\n",
      "Train Epoch: 359 [75072/225000 (33%)] Loss: 16395.359375\n",
      "Train Epoch: 359 [77568/225000 (34%)] Loss: 16878.246094\n",
      "Train Epoch: 359 [80064/225000 (36%)] Loss: 16535.742188\n",
      "Train Epoch: 359 [82560/225000 (37%)] Loss: 16629.121094\n",
      "Train Epoch: 359 [85056/225000 (38%)] Loss: 16516.681641\n",
      "Train Epoch: 359 [87552/225000 (39%)] Loss: 16344.117188\n",
      "Train Epoch: 359 [90048/225000 (40%)] Loss: 16804.101562\n",
      "Train Epoch: 359 [92544/225000 (41%)] Loss: 16414.785156\n",
      "Train Epoch: 359 [95040/225000 (42%)] Loss: 16882.238281\n",
      "Train Epoch: 359 [97536/225000 (43%)] Loss: 17049.597656\n",
      "Train Epoch: 359 [100032/225000 (44%)] Loss: 16869.335938\n",
      "Train Epoch: 359 [102528/225000 (46%)] Loss: 16928.005859\n",
      "Train Epoch: 359 [105024/225000 (47%)] Loss: 16771.931641\n",
      "Train Epoch: 359 [107520/225000 (48%)] Loss: 16245.541992\n",
      "Train Epoch: 359 [110016/225000 (49%)] Loss: 16998.353516\n",
      "Train Epoch: 359 [112512/225000 (50%)] Loss: 16780.527344\n",
      "Train Epoch: 359 [115008/225000 (51%)] Loss: 18236.000000\n",
      "Train Epoch: 359 [117504/225000 (52%)] Loss: 16099.671875\n",
      "Train Epoch: 359 [120000/225000 (53%)] Loss: 16532.693359\n",
      "Train Epoch: 359 [122496/225000 (54%)] Loss: 16094.272461\n",
      "Train Epoch: 359 [124992/225000 (56%)] Loss: 16493.121094\n",
      "Train Epoch: 359 [127488/225000 (57%)] Loss: 16791.277344\n",
      "Train Epoch: 359 [129984/225000 (58%)] Loss: 16299.802734\n",
      "Train Epoch: 359 [132480/225000 (59%)] Loss: 16445.015625\n",
      "Train Epoch: 359 [134976/225000 (60%)] Loss: 16814.058594\n",
      "Train Epoch: 359 [137472/225000 (61%)] Loss: 17126.773438\n",
      "Train Epoch: 359 [139968/225000 (62%)] Loss: 16283.805664\n",
      "Train Epoch: 359 [142464/225000 (63%)] Loss: 16605.902344\n",
      "Train Epoch: 359 [144960/225000 (64%)] Loss: 16524.628906\n",
      "Train Epoch: 359 [147456/225000 (66%)] Loss: 16447.097656\n",
      "Train Epoch: 359 [149952/225000 (67%)] Loss: 16435.291016\n",
      "Train Epoch: 359 [152448/225000 (68%)] Loss: 16735.128906\n",
      "Train Epoch: 359 [154944/225000 (69%)] Loss: 16322.533203\n",
      "Train Epoch: 359 [157440/225000 (70%)] Loss: 16313.916992\n",
      "Train Epoch: 359 [159936/225000 (71%)] Loss: 16643.250000\n",
      "Train Epoch: 359 [162432/225000 (72%)] Loss: 16441.824219\n",
      "Train Epoch: 359 [164928/225000 (73%)] Loss: 16951.640625\n",
      "Train Epoch: 359 [167424/225000 (74%)] Loss: 16293.852539\n",
      "Train Epoch: 359 [169920/225000 (76%)] Loss: 16525.199219\n",
      "Train Epoch: 359 [172416/225000 (77%)] Loss: 16766.843750\n",
      "Train Epoch: 359 [174912/225000 (78%)] Loss: 16615.925781\n",
      "Train Epoch: 359 [177408/225000 (79%)] Loss: 16598.585938\n",
      "Train Epoch: 359 [179904/225000 (80%)] Loss: 16573.574219\n",
      "Train Epoch: 359 [182400/225000 (81%)] Loss: 16600.359375\n",
      "Train Epoch: 359 [184896/225000 (82%)] Loss: 16412.119141\n",
      "Train Epoch: 359 [187392/225000 (83%)] Loss: 16925.166016\n",
      "Train Epoch: 359 [189888/225000 (84%)] Loss: 16810.687500\n",
      "Train Epoch: 359 [192384/225000 (86%)] Loss: 16499.863281\n",
      "Train Epoch: 359 [194880/225000 (87%)] Loss: 16312.232422\n",
      "Train Epoch: 359 [197376/225000 (88%)] Loss: 16272.581055\n",
      "Train Epoch: 359 [199872/225000 (89%)] Loss: 16668.191406\n",
      "Train Epoch: 359 [202368/225000 (90%)] Loss: 16634.910156\n",
      "Train Epoch: 359 [204864/225000 (91%)] Loss: 16702.625000\n",
      "Train Epoch: 359 [207360/225000 (92%)] Loss: 16047.024414\n",
      "Train Epoch: 359 [209856/225000 (93%)] Loss: 16614.511719\n",
      "Train Epoch: 359 [212352/225000 (94%)] Loss: 16131.875000\n",
      "Train Epoch: 359 [214848/225000 (95%)] Loss: 16346.489258\n",
      "Train Epoch: 359 [217344/225000 (97%)] Loss: 17149.531250\n",
      "Train Epoch: 359 [219840/225000 (98%)] Loss: 16581.246094\n",
      "Train Epoch: 359 [222336/225000 (99%)] Loss: 16823.599609\n",
      "Train Epoch: 359 [224832/225000 (100%)] Loss: 16930.785156\n",
      "    epoch          : 359\n",
      "    loss           : 16700.383752399743\n",
      "    val_loss       : 16642.273926434173\n",
      "Train Epoch: 360 [192/225000 (0%)] Loss: 17123.316406\n",
      "Train Epoch: 360 [2688/225000 (1%)] Loss: 17184.054688\n",
      "Train Epoch: 360 [5184/225000 (2%)] Loss: 16864.583984\n",
      "Train Epoch: 360 [7680/225000 (3%)] Loss: 16277.198242\n",
      "Train Epoch: 360 [10176/225000 (5%)] Loss: 16897.339844\n",
      "Train Epoch: 360 [12672/225000 (6%)] Loss: 16821.087891\n",
      "Train Epoch: 360 [15168/225000 (7%)] Loss: 16702.298828\n",
      "Train Epoch: 360 [17664/225000 (8%)] Loss: 16921.195312\n",
      "Train Epoch: 360 [20160/225000 (9%)] Loss: 16346.552734\n",
      "Train Epoch: 360 [22656/225000 (10%)] Loss: 16636.212891\n",
      "Train Epoch: 360 [25152/225000 (11%)] Loss: 16511.050781\n",
      "Train Epoch: 360 [27648/225000 (12%)] Loss: 16681.656250\n",
      "Train Epoch: 360 [30144/225000 (13%)] Loss: 16598.236328\n",
      "Train Epoch: 360 [32640/225000 (15%)] Loss: 16555.589844\n",
      "Train Epoch: 360 [35136/225000 (16%)] Loss: 16649.175781\n",
      "Train Epoch: 360 [37632/225000 (17%)] Loss: 17136.460938\n",
      "Train Epoch: 360 [40128/225000 (18%)] Loss: 16083.458008\n",
      "Train Epoch: 360 [42624/225000 (19%)] Loss: 16675.636719\n",
      "Train Epoch: 360 [45120/225000 (20%)] Loss: 16751.800781\n",
      "Train Epoch: 360 [47616/225000 (21%)] Loss: 18266.103516\n",
      "Train Epoch: 360 [50112/225000 (22%)] Loss: 16920.636719\n",
      "Train Epoch: 360 [52608/225000 (23%)] Loss: 17013.175781\n",
      "Train Epoch: 360 [55104/225000 (24%)] Loss: 16773.750000\n",
      "Train Epoch: 360 [57600/225000 (26%)] Loss: 16679.312500\n",
      "Train Epoch: 360 [60096/225000 (27%)] Loss: 18481.867188\n",
      "Train Epoch: 360 [62592/225000 (28%)] Loss: 16763.636719\n",
      "Train Epoch: 360 [65088/225000 (29%)] Loss: 16832.976562\n",
      "Train Epoch: 360 [67584/225000 (30%)] Loss: 18387.701172\n",
      "Train Epoch: 360 [70080/225000 (31%)] Loss: 16951.269531\n",
      "Train Epoch: 360 [72576/225000 (32%)] Loss: 16942.480469\n",
      "Train Epoch: 360 [75072/225000 (33%)] Loss: 16843.080078\n",
      "Train Epoch: 360 [77568/225000 (34%)] Loss: 16982.619141\n",
      "Train Epoch: 360 [80064/225000 (36%)] Loss: 16440.933594\n",
      "Train Epoch: 360 [82560/225000 (37%)] Loss: 16306.247070\n",
      "Train Epoch: 360 [85056/225000 (38%)] Loss: 16927.718750\n",
      "Train Epoch: 360 [87552/225000 (39%)] Loss: 16709.474609\n",
      "Train Epoch: 360 [90048/225000 (40%)] Loss: 17147.281250\n",
      "Train Epoch: 360 [92544/225000 (41%)] Loss: 16654.792969\n",
      "Train Epoch: 360 [95040/225000 (42%)] Loss: 16723.714844\n",
      "Train Epoch: 360 [97536/225000 (43%)] Loss: 16769.042969\n",
      "Train Epoch: 360 [100032/225000 (44%)] Loss: 16642.625000\n",
      "Train Epoch: 360 [102528/225000 (46%)] Loss: 16704.275391\n",
      "Train Epoch: 360 [105024/225000 (47%)] Loss: 16560.050781\n",
      "Train Epoch: 360 [107520/225000 (48%)] Loss: 16440.945312\n",
      "Train Epoch: 360 [110016/225000 (49%)] Loss: 17037.550781\n",
      "Train Epoch: 360 [112512/225000 (50%)] Loss: 25059.371094\n",
      "Train Epoch: 360 [115008/225000 (51%)] Loss: 17740.238281\n",
      "Train Epoch: 360 [117504/225000 (52%)] Loss: 16239.543945\n",
      "Train Epoch: 360 [120000/225000 (53%)] Loss: 16205.919922\n",
      "Train Epoch: 360 [122496/225000 (54%)] Loss: 16578.408203\n",
      "Train Epoch: 360 [124992/225000 (56%)] Loss: 16265.031250\n",
      "Train Epoch: 360 [127488/225000 (57%)] Loss: 16708.019531\n",
      "Train Epoch: 360 [129984/225000 (58%)] Loss: 16705.679688\n",
      "Train Epoch: 360 [132480/225000 (59%)] Loss: 17821.574219\n",
      "Train Epoch: 360 [134976/225000 (60%)] Loss: 16990.960938\n",
      "Train Epoch: 360 [137472/225000 (61%)] Loss: 16761.347656\n",
      "Train Epoch: 360 [139968/225000 (62%)] Loss: 16427.800781\n",
      "Train Epoch: 360 [142464/225000 (63%)] Loss: 16459.458984\n",
      "Train Epoch: 360 [144960/225000 (64%)] Loss: 16219.976562\n",
      "Train Epoch: 360 [147456/225000 (66%)] Loss: 16606.597656\n",
      "Train Epoch: 360 [149952/225000 (67%)] Loss: 16628.595703\n",
      "Train Epoch: 360 [152448/225000 (68%)] Loss: 16210.410156\n",
      "Train Epoch: 360 [154944/225000 (69%)] Loss: 16400.859375\n",
      "Train Epoch: 360 [157440/225000 (70%)] Loss: 16998.835938\n",
      "Train Epoch: 360 [159936/225000 (71%)] Loss: 16502.335938\n",
      "Train Epoch: 360 [162432/225000 (72%)] Loss: 16745.460938\n",
      "Train Epoch: 360 [164928/225000 (73%)] Loss: 16453.363281\n",
      "Train Epoch: 360 [167424/225000 (74%)] Loss: 16965.523438\n",
      "Train Epoch: 360 [169920/225000 (76%)] Loss: 16993.496094\n",
      "Train Epoch: 360 [172416/225000 (77%)] Loss: 16475.339844\n",
      "Train Epoch: 360 [174912/225000 (78%)] Loss: 16743.394531\n",
      "Train Epoch: 360 [177408/225000 (79%)] Loss: 17092.333984\n",
      "Train Epoch: 360 [179904/225000 (80%)] Loss: 16837.566406\n",
      "Train Epoch: 360 [182400/225000 (81%)] Loss: 16527.933594\n",
      "Train Epoch: 360 [184896/225000 (82%)] Loss: 16971.181641\n",
      "Train Epoch: 360 [187392/225000 (83%)] Loss: 16807.089844\n",
      "Train Epoch: 360 [189888/225000 (84%)] Loss: 16764.669922\n",
      "Train Epoch: 360 [192384/225000 (86%)] Loss: 16492.164062\n",
      "Train Epoch: 360 [194880/225000 (87%)] Loss: 16579.947266\n",
      "Train Epoch: 360 [197376/225000 (88%)] Loss: 16772.554688\n",
      "Train Epoch: 360 [199872/225000 (89%)] Loss: 16354.280273\n",
      "Train Epoch: 360 [202368/225000 (90%)] Loss: 16149.844727\n",
      "Train Epoch: 360 [204864/225000 (91%)] Loss: 15891.621094\n",
      "Train Epoch: 360 [207360/225000 (92%)] Loss: 16704.210938\n",
      "Train Epoch: 360 [209856/225000 (93%)] Loss: 16326.434570\n",
      "Train Epoch: 360 [212352/225000 (94%)] Loss: 16878.609375\n",
      "Train Epoch: 360 [214848/225000 (95%)] Loss: 16491.761719\n",
      "Train Epoch: 360 [217344/225000 (97%)] Loss: 17102.261719\n",
      "Train Epoch: 360 [219840/225000 (98%)] Loss: 16535.371094\n",
      "Train Epoch: 360 [222336/225000 (99%)] Loss: 16660.708984\n",
      "Train Epoch: 360 [224832/225000 (100%)] Loss: 16753.414062\n",
      "    epoch          : 360\n",
      "    loss           : 16723.13029443526\n",
      "    val_loss       : 16645.017219698155\n",
      "Train Epoch: 361 [192/225000 (0%)] Loss: 16700.794922\n",
      "Train Epoch: 361 [2688/225000 (1%)] Loss: 16650.646484\n",
      "Train Epoch: 361 [5184/225000 (2%)] Loss: 16369.619141\n",
      "Train Epoch: 361 [7680/225000 (3%)] Loss: 18221.615234\n",
      "Train Epoch: 361 [10176/225000 (5%)] Loss: 16850.761719\n",
      "Train Epoch: 361 [12672/225000 (6%)] Loss: 16821.695312\n",
      "Train Epoch: 361 [15168/225000 (7%)] Loss: 16711.826172\n",
      "Train Epoch: 361 [17664/225000 (8%)] Loss: 18190.275391\n",
      "Train Epoch: 361 [20160/225000 (9%)] Loss: 16302.226562\n",
      "Train Epoch: 361 [22656/225000 (10%)] Loss: 16599.527344\n",
      "Train Epoch: 361 [25152/225000 (11%)] Loss: 16557.816406\n",
      "Train Epoch: 361 [27648/225000 (12%)] Loss: 17090.929688\n",
      "Train Epoch: 361 [30144/225000 (13%)] Loss: 17085.101562\n",
      "Train Epoch: 361 [32640/225000 (15%)] Loss: 16641.554688\n",
      "Train Epoch: 361 [35136/225000 (16%)] Loss: 18305.910156\n",
      "Train Epoch: 361 [37632/225000 (17%)] Loss: 16394.222656\n",
      "Train Epoch: 361 [40128/225000 (18%)] Loss: 16831.625000\n",
      "Train Epoch: 361 [42624/225000 (19%)] Loss: 16672.980469\n",
      "Train Epoch: 361 [45120/225000 (20%)] Loss: 18261.058594\n",
      "Train Epoch: 361 [47616/225000 (21%)] Loss: 16770.111328\n",
      "Train Epoch: 361 [50112/225000 (22%)] Loss: 18311.246094\n",
      "Train Epoch: 361 [52608/225000 (23%)] Loss: 16514.945312\n",
      "Train Epoch: 361 [55104/225000 (24%)] Loss: 16414.464844\n",
      "Train Epoch: 361 [57600/225000 (26%)] Loss: 16850.601562\n",
      "Train Epoch: 361 [60096/225000 (27%)] Loss: 16902.205078\n",
      "Train Epoch: 361 [62592/225000 (28%)] Loss: 16606.433594\n",
      "Train Epoch: 361 [65088/225000 (29%)] Loss: 16580.796875\n",
      "Train Epoch: 361 [67584/225000 (30%)] Loss: 16478.621094\n",
      "Train Epoch: 361 [70080/225000 (31%)] Loss: 16741.826172\n",
      "Train Epoch: 361 [72576/225000 (32%)] Loss: 16729.382812\n",
      "Train Epoch: 361 [75072/225000 (33%)] Loss: 16393.531250\n",
      "Train Epoch: 361 [77568/225000 (34%)] Loss: 16674.335938\n",
      "Train Epoch: 361 [80064/225000 (36%)] Loss: 16168.744141\n",
      "Train Epoch: 361 [82560/225000 (37%)] Loss: 16610.128906\n",
      "Train Epoch: 361 [85056/225000 (38%)] Loss: 16712.759766\n",
      "Train Epoch: 361 [87552/225000 (39%)] Loss: 17032.886719\n",
      "Train Epoch: 361 [90048/225000 (40%)] Loss: 16491.373047\n",
      "Train Epoch: 361 [92544/225000 (41%)] Loss: 17011.585938\n",
      "Train Epoch: 361 [95040/225000 (42%)] Loss: 16644.212891\n",
      "Train Epoch: 361 [97536/225000 (43%)] Loss: 16636.814453\n",
      "Train Epoch: 361 [100032/225000 (44%)] Loss: 16830.167969\n",
      "Train Epoch: 361 [102528/225000 (46%)] Loss: 16520.929688\n",
      "Train Epoch: 361 [105024/225000 (47%)] Loss: 16817.197266\n",
      "Train Epoch: 361 [107520/225000 (48%)] Loss: 16211.016602\n",
      "Train Epoch: 361 [110016/225000 (49%)] Loss: 16671.660156\n",
      "Train Epoch: 361 [112512/225000 (50%)] Loss: 16915.607422\n",
      "Train Epoch: 361 [115008/225000 (51%)] Loss: 16518.851562\n",
      "Train Epoch: 361 [117504/225000 (52%)] Loss: 17047.253906\n",
      "Train Epoch: 361 [120000/225000 (53%)] Loss: 16500.648438\n",
      "Train Epoch: 361 [122496/225000 (54%)] Loss: 17033.445312\n",
      "Train Epoch: 361 [124992/225000 (56%)] Loss: 16713.738281\n",
      "Train Epoch: 361 [127488/225000 (57%)] Loss: 24882.492188\n",
      "Train Epoch: 361 [129984/225000 (58%)] Loss: 17785.484375\n",
      "Train Epoch: 361 [132480/225000 (59%)] Loss: 16409.236328\n",
      "Train Epoch: 361 [134976/225000 (60%)] Loss: 16868.882812\n",
      "Train Epoch: 361 [137472/225000 (61%)] Loss: 16553.460938\n",
      "Train Epoch: 361 [139968/225000 (62%)] Loss: 16837.191406\n",
      "Train Epoch: 361 [142464/225000 (63%)] Loss: 16502.007812\n",
      "Train Epoch: 361 [144960/225000 (64%)] Loss: 16128.986328\n",
      "Train Epoch: 361 [147456/225000 (66%)] Loss: 18126.398438\n",
      "Train Epoch: 361 [149952/225000 (67%)] Loss: 17027.339844\n",
      "Train Epoch: 361 [152448/225000 (68%)] Loss: 16958.382812\n",
      "Train Epoch: 361 [154944/225000 (69%)] Loss: 16857.375000\n",
      "Train Epoch: 361 [157440/225000 (70%)] Loss: 16706.849609\n",
      "Train Epoch: 361 [159936/225000 (71%)] Loss: 16671.765625\n",
      "Train Epoch: 361 [162432/225000 (72%)] Loss: 16268.948242\n",
      "Train Epoch: 361 [164928/225000 (73%)] Loss: 16594.941406\n",
      "Train Epoch: 361 [167424/225000 (74%)] Loss: 17065.660156\n",
      "Train Epoch: 361 [169920/225000 (76%)] Loss: 16731.968750\n",
      "Train Epoch: 361 [172416/225000 (77%)] Loss: 16614.878906\n",
      "Train Epoch: 361 [174912/225000 (78%)] Loss: 16926.267578\n",
      "Train Epoch: 361 [177408/225000 (79%)] Loss: 17029.324219\n",
      "Train Epoch: 361 [179904/225000 (80%)] Loss: 16766.703125\n",
      "Train Epoch: 361 [182400/225000 (81%)] Loss: 16789.949219\n",
      "Train Epoch: 361 [184896/225000 (82%)] Loss: 16946.226562\n",
      "Train Epoch: 361 [187392/225000 (83%)] Loss: 16366.452148\n",
      "Train Epoch: 361 [189888/225000 (84%)] Loss: 16044.168945\n",
      "Train Epoch: 361 [192384/225000 (86%)] Loss: 16242.392578\n",
      "Train Epoch: 361 [194880/225000 (87%)] Loss: 15954.262695\n",
      "Train Epoch: 361 [197376/225000 (88%)] Loss: 16729.664062\n",
      "Train Epoch: 361 [199872/225000 (89%)] Loss: 16448.820312\n",
      "Train Epoch: 361 [202368/225000 (90%)] Loss: 16623.886719\n",
      "Train Epoch: 361 [204864/225000 (91%)] Loss: 16916.230469\n",
      "Train Epoch: 361 [207360/225000 (92%)] Loss: 16482.140625\n",
      "Train Epoch: 361 [209856/225000 (93%)] Loss: 16345.903320\n",
      "Train Epoch: 361 [212352/225000 (94%)] Loss: 16424.636719\n",
      "Train Epoch: 361 [214848/225000 (95%)] Loss: 16988.541016\n",
      "Train Epoch: 361 [217344/225000 (97%)] Loss: 16791.546875\n",
      "Train Epoch: 361 [219840/225000 (98%)] Loss: 16421.441406\n",
      "Train Epoch: 361 [222336/225000 (99%)] Loss: 16321.691406\n",
      "Train Epoch: 361 [224832/225000 (100%)] Loss: 16557.433594\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   350: reducing learning rate of group 0 to 1.0000e-08.\n",
      "    epoch          : 361\n",
      "    loss           : 16726.13539555781\n",
      "    val_loss       : 16694.474397099653\n",
      "Train Epoch: 362 [192/225000 (0%)] Loss: 16253.019531\n",
      "Train Epoch: 362 [2688/225000 (1%)] Loss: 16853.371094\n",
      "Train Epoch: 362 [5184/225000 (2%)] Loss: 16070.359375\n",
      "Train Epoch: 362 [7680/225000 (3%)] Loss: 16285.938477\n",
      "Train Epoch: 362 [10176/225000 (5%)] Loss: 16464.863281\n",
      "Train Epoch: 362 [12672/225000 (6%)] Loss: 16173.180664\n",
      "Train Epoch: 362 [15168/225000 (7%)] Loss: 16851.910156\n",
      "Train Epoch: 362 [17664/225000 (8%)] Loss: 17939.039062\n",
      "Train Epoch: 362 [20160/225000 (9%)] Loss: 17189.917969\n",
      "Train Epoch: 362 [22656/225000 (10%)] Loss: 16703.640625\n",
      "Train Epoch: 362 [25152/225000 (11%)] Loss: 16848.660156\n",
      "Train Epoch: 362 [27648/225000 (12%)] Loss: 16738.609375\n",
      "Train Epoch: 362 [30144/225000 (13%)] Loss: 16662.343750\n",
      "Train Epoch: 362 [32640/225000 (15%)] Loss: 16236.985352\n",
      "Train Epoch: 362 [35136/225000 (16%)] Loss: 16533.859375\n",
      "Train Epoch: 362 [37632/225000 (17%)] Loss: 16887.482422\n",
      "Train Epoch: 362 [40128/225000 (18%)] Loss: 16394.736328\n",
      "Train Epoch: 362 [42624/225000 (19%)] Loss: 16677.460938\n",
      "Train Epoch: 362 [45120/225000 (20%)] Loss: 16465.906250\n",
      "Train Epoch: 362 [47616/225000 (21%)] Loss: 16913.816406\n",
      "Train Epoch: 362 [50112/225000 (22%)] Loss: 17089.472656\n",
      "Train Epoch: 362 [52608/225000 (23%)] Loss: 18460.207031\n",
      "Train Epoch: 362 [55104/225000 (24%)] Loss: 16589.679688\n",
      "Train Epoch: 362 [57600/225000 (26%)] Loss: 16529.376953\n",
      "Train Epoch: 362 [60096/225000 (27%)] Loss: 17147.849609\n",
      "Train Epoch: 362 [62592/225000 (28%)] Loss: 16630.378906\n",
      "Train Epoch: 362 [65088/225000 (29%)] Loss: 16817.337891\n",
      "Train Epoch: 362 [67584/225000 (30%)] Loss: 17025.029297\n",
      "Train Epoch: 362 [70080/225000 (31%)] Loss: 16912.402344\n",
      "Train Epoch: 362 [72576/225000 (32%)] Loss: 16933.597656\n",
      "Train Epoch: 362 [75072/225000 (33%)] Loss: 17048.394531\n",
      "Train Epoch: 362 [77568/225000 (34%)] Loss: 16467.695312\n",
      "Train Epoch: 362 [80064/225000 (36%)] Loss: 16196.222656\n",
      "Train Epoch: 362 [82560/225000 (37%)] Loss: 16802.605469\n",
      "Train Epoch: 362 [85056/225000 (38%)] Loss: 16831.541016\n",
      "Train Epoch: 362 [87552/225000 (39%)] Loss: 16935.722656\n",
      "Train Epoch: 362 [90048/225000 (40%)] Loss: 16841.710938\n",
      "Train Epoch: 362 [92544/225000 (41%)] Loss: 16694.666016\n",
      "Train Epoch: 362 [95040/225000 (42%)] Loss: 16322.450195\n",
      "Train Epoch: 362 [97536/225000 (43%)] Loss: 16812.992188\n",
      "Train Epoch: 362 [100032/225000 (44%)] Loss: 16846.597656\n",
      "Train Epoch: 362 [102528/225000 (46%)] Loss: 16380.160156\n",
      "Train Epoch: 362 [105024/225000 (47%)] Loss: 16682.496094\n",
      "Train Epoch: 362 [107520/225000 (48%)] Loss: 16635.794922\n",
      "Train Epoch: 362 [110016/225000 (49%)] Loss: 16891.548828\n",
      "Train Epoch: 362 [112512/225000 (50%)] Loss: 16359.207031\n",
      "Train Epoch: 362 [115008/225000 (51%)] Loss: 16658.751953\n",
      "Train Epoch: 362 [117504/225000 (52%)] Loss: 17067.570312\n",
      "Train Epoch: 362 [120000/225000 (53%)] Loss: 16786.544922\n",
      "Train Epoch: 362 [122496/225000 (54%)] Loss: 16517.609375\n",
      "Train Epoch: 362 [124992/225000 (56%)] Loss: 16631.322266\n",
      "Train Epoch: 362 [127488/225000 (57%)] Loss: 16672.988281\n",
      "Train Epoch: 362 [129984/225000 (58%)] Loss: 16816.843750\n",
      "Train Epoch: 362 [132480/225000 (59%)] Loss: 16708.755859\n",
      "Train Epoch: 362 [134976/225000 (60%)] Loss: 16878.005859\n",
      "Train Epoch: 362 [137472/225000 (61%)] Loss: 16242.764648\n",
      "Train Epoch: 362 [139968/225000 (62%)] Loss: 16667.736328\n",
      "Train Epoch: 362 [142464/225000 (63%)] Loss: 16513.082031\n",
      "Train Epoch: 362 [144960/225000 (64%)] Loss: 16695.679688\n",
      "Train Epoch: 362 [147456/225000 (66%)] Loss: 16913.515625\n",
      "Train Epoch: 362 [149952/225000 (67%)] Loss: 16481.871094\n",
      "Train Epoch: 362 [152448/225000 (68%)] Loss: 16502.876953\n",
      "Train Epoch: 362 [154944/225000 (69%)] Loss: 16634.445312\n",
      "Train Epoch: 362 [157440/225000 (70%)] Loss: 16950.427734\n",
      "Train Epoch: 362 [159936/225000 (71%)] Loss: 16659.351562\n",
      "Train Epoch: 362 [162432/225000 (72%)] Loss: 16601.285156\n",
      "Train Epoch: 362 [164928/225000 (73%)] Loss: 16621.695312\n",
      "Train Epoch: 362 [167424/225000 (74%)] Loss: 16889.951172\n",
      "Train Epoch: 362 [169920/225000 (76%)] Loss: 16740.820312\n",
      "Train Epoch: 362 [172416/225000 (77%)] Loss: 16699.914062\n",
      "Train Epoch: 362 [174912/225000 (78%)] Loss: 16537.097656\n",
      "Train Epoch: 362 [177408/225000 (79%)] Loss: 16612.095703\n",
      "Train Epoch: 362 [179904/225000 (80%)] Loss: 16533.980469\n",
      "Train Epoch: 362 [182400/225000 (81%)] Loss: 16656.626953\n",
      "Train Epoch: 362 [184896/225000 (82%)] Loss: 16638.011719\n",
      "Train Epoch: 362 [187392/225000 (83%)] Loss: 16252.346680\n",
      "Train Epoch: 362 [189888/225000 (84%)] Loss: 16828.259766\n",
      "Train Epoch: 362 [192384/225000 (86%)] Loss: 16613.656250\n",
      "Train Epoch: 362 [194880/225000 (87%)] Loss: 16997.832031\n",
      "Train Epoch: 362 [197376/225000 (88%)] Loss: 16557.484375\n",
      "Train Epoch: 362 [199872/225000 (89%)] Loss: 16456.033203\n",
      "Train Epoch: 362 [202368/225000 (90%)] Loss: 16582.019531\n",
      "Train Epoch: 362 [204864/225000 (91%)] Loss: 16837.875000\n",
      "Train Epoch: 362 [207360/225000 (92%)] Loss: 16367.360352\n",
      "Train Epoch: 362 [209856/225000 (93%)] Loss: 16537.173828\n",
      "Train Epoch: 362 [212352/225000 (94%)] Loss: 16472.865234\n",
      "Train Epoch: 362 [214848/225000 (95%)] Loss: 16631.886719\n",
      "Train Epoch: 362 [217344/225000 (97%)] Loss: 16847.875000\n",
      "Train Epoch: 362 [219840/225000 (98%)] Loss: 16423.296875\n",
      "Train Epoch: 362 [222336/225000 (99%)] Loss: 16656.847656\n",
      "Train Epoch: 362 [224832/225000 (100%)] Loss: 16463.070312\n",
      "    epoch          : 362\n",
      "    loss           : 16731.6053637612\n",
      "    val_loss       : 16685.892424648955\n",
      "Train Epoch: 363 [192/225000 (0%)] Loss: 16893.867188\n",
      "Train Epoch: 363 [2688/225000 (1%)] Loss: 16094.191406\n",
      "Train Epoch: 363 [5184/225000 (2%)] Loss: 16846.427734\n",
      "Train Epoch: 363 [7680/225000 (3%)] Loss: 16602.294922\n",
      "Train Epoch: 363 [10176/225000 (5%)] Loss: 17910.335938\n",
      "Train Epoch: 363 [12672/225000 (6%)] Loss: 16778.792969\n",
      "Train Epoch: 363 [15168/225000 (7%)] Loss: 16207.249023\n",
      "Train Epoch: 363 [17664/225000 (8%)] Loss: 16413.263672\n",
      "Train Epoch: 363 [20160/225000 (9%)] Loss: 16651.011719\n",
      "Train Epoch: 363 [22656/225000 (10%)] Loss: 16687.382812\n",
      "Train Epoch: 363 [25152/225000 (11%)] Loss: 16641.585938\n",
      "Train Epoch: 363 [27648/225000 (12%)] Loss: 16772.226562\n",
      "Train Epoch: 363 [30144/225000 (13%)] Loss: 17113.230469\n",
      "Train Epoch: 363 [32640/225000 (15%)] Loss: 16642.980469\n",
      "Train Epoch: 363 [35136/225000 (16%)] Loss: 16409.523438\n",
      "Train Epoch: 363 [37632/225000 (17%)] Loss: 16411.031250\n",
      "Train Epoch: 363 [40128/225000 (18%)] Loss: 16366.807617\n",
      "Train Epoch: 363 [42624/225000 (19%)] Loss: 16968.019531\n",
      "Train Epoch: 363 [45120/225000 (20%)] Loss: 17038.601562\n",
      "Train Epoch: 363 [47616/225000 (21%)] Loss: 16901.443359\n",
      "Train Epoch: 363 [50112/225000 (22%)] Loss: 16907.609375\n",
      "Train Epoch: 363 [52608/225000 (23%)] Loss: 16760.507812\n",
      "Train Epoch: 363 [55104/225000 (24%)] Loss: 17078.029297\n",
      "Train Epoch: 363 [57600/225000 (26%)] Loss: 16290.844727\n",
      "Train Epoch: 363 [60096/225000 (27%)] Loss: 16800.361328\n",
      "Train Epoch: 363 [62592/225000 (28%)] Loss: 16645.871094\n",
      "Train Epoch: 363 [65088/225000 (29%)] Loss: 16748.636719\n",
      "Train Epoch: 363 [67584/225000 (30%)] Loss: 16644.617188\n",
      "Train Epoch: 363 [70080/225000 (31%)] Loss: 18325.470703\n",
      "Train Epoch: 363 [72576/225000 (32%)] Loss: 16775.988281\n",
      "Train Epoch: 363 [75072/225000 (33%)] Loss: 16258.740234\n",
      "Train Epoch: 363 [77568/225000 (34%)] Loss: 16852.312500\n",
      "Train Epoch: 363 [80064/225000 (36%)] Loss: 16456.017578\n",
      "Train Epoch: 363 [82560/225000 (37%)] Loss: 16722.962891\n",
      "Train Epoch: 363 [85056/225000 (38%)] Loss: 16519.585938\n",
      "Train Epoch: 363 [87552/225000 (39%)] Loss: 16473.113281\n",
      "Train Epoch: 363 [90048/225000 (40%)] Loss: 16241.472656\n",
      "Train Epoch: 363 [92544/225000 (41%)] Loss: 16615.871094\n",
      "Train Epoch: 363 [95040/225000 (42%)] Loss: 15941.142578\n",
      "Train Epoch: 363 [97536/225000 (43%)] Loss: 16620.472656\n",
      "Train Epoch: 363 [100032/225000 (44%)] Loss: 16679.449219\n",
      "Train Epoch: 363 [102528/225000 (46%)] Loss: 16473.277344\n",
      "Train Epoch: 363 [105024/225000 (47%)] Loss: 16584.912109\n",
      "Train Epoch: 363 [107520/225000 (48%)] Loss: 16789.101562\n",
      "Train Epoch: 363 [110016/225000 (49%)] Loss: 16665.730469\n",
      "Train Epoch: 363 [112512/225000 (50%)] Loss: 16339.342773\n",
      "Train Epoch: 363 [115008/225000 (51%)] Loss: 16209.127930\n",
      "Train Epoch: 363 [117504/225000 (52%)] Loss: 16617.121094\n",
      "Train Epoch: 363 [120000/225000 (53%)] Loss: 16478.425781\n",
      "Train Epoch: 363 [122496/225000 (54%)] Loss: 16572.402344\n",
      "Train Epoch: 363 [124992/225000 (56%)] Loss: 16655.308594\n",
      "Train Epoch: 363 [127488/225000 (57%)] Loss: 16862.152344\n",
      "Train Epoch: 363 [129984/225000 (58%)] Loss: 16459.199219\n",
      "Train Epoch: 363 [132480/225000 (59%)] Loss: 16980.119141\n",
      "Train Epoch: 363 [134976/225000 (60%)] Loss: 16914.667969\n",
      "Train Epoch: 363 [137472/225000 (61%)] Loss: 16967.445312\n",
      "Train Epoch: 363 [139968/225000 (62%)] Loss: 16432.767578\n",
      "Train Epoch: 363 [142464/225000 (63%)] Loss: 16104.894531\n",
      "Train Epoch: 363 [144960/225000 (64%)] Loss: 16577.798828\n",
      "Train Epoch: 363 [147456/225000 (66%)] Loss: 16821.150391\n",
      "Train Epoch: 363 [149952/225000 (67%)] Loss: 17056.046875\n",
      "Train Epoch: 363 [152448/225000 (68%)] Loss: 16200.952148\n",
      "Train Epoch: 363 [154944/225000 (69%)] Loss: 16152.660156\n",
      "Train Epoch: 363 [157440/225000 (70%)] Loss: 16765.609375\n",
      "Train Epoch: 363 [159936/225000 (71%)] Loss: 16965.097656\n",
      "Train Epoch: 363 [162432/225000 (72%)] Loss: 16612.484375\n",
      "Train Epoch: 363 [164928/225000 (73%)] Loss: 16597.437500\n",
      "Train Epoch: 363 [167424/225000 (74%)] Loss: 16367.855469\n",
      "Train Epoch: 363 [169920/225000 (76%)] Loss: 16325.245117\n",
      "Train Epoch: 363 [172416/225000 (77%)] Loss: 16751.554688\n",
      "Train Epoch: 363 [174912/225000 (78%)] Loss: 16668.488281\n",
      "Train Epoch: 363 [177408/225000 (79%)] Loss: 16495.218750\n",
      "Train Epoch: 363 [179904/225000 (80%)] Loss: 16489.150391\n",
      "Train Epoch: 363 [182400/225000 (81%)] Loss: 16860.234375\n",
      "Train Epoch: 363 [184896/225000 (82%)] Loss: 16936.164062\n",
      "Train Epoch: 363 [187392/225000 (83%)] Loss: 16135.531250\n",
      "Train Epoch: 363 [189888/225000 (84%)] Loss: 16682.753906\n",
      "Train Epoch: 363 [192384/225000 (86%)] Loss: 16504.384766\n",
      "Train Epoch: 363 [194880/225000 (87%)] Loss: 16481.441406\n",
      "Train Epoch: 363 [197376/225000 (88%)] Loss: 16834.085938\n",
      "Train Epoch: 363 [199872/225000 (89%)] Loss: 16290.339844\n",
      "Train Epoch: 363 [202368/225000 (90%)] Loss: 16393.253906\n",
      "Train Epoch: 363 [204864/225000 (91%)] Loss: 16637.648438\n",
      "Train Epoch: 363 [207360/225000 (92%)] Loss: 16709.632812\n",
      "Train Epoch: 363 [209856/225000 (93%)] Loss: 16519.535156\n",
      "Train Epoch: 363 [212352/225000 (94%)] Loss: 16529.238281\n",
      "Train Epoch: 363 [214848/225000 (95%)] Loss: 16381.219727\n",
      "Train Epoch: 363 [217344/225000 (97%)] Loss: 16671.691406\n",
      "Train Epoch: 363 [219840/225000 (98%)] Loss: 16913.447266\n",
      "Train Epoch: 363 [222336/225000 (99%)] Loss: 16834.347656\n",
      "Train Epoch: 363 [224832/225000 (100%)] Loss: 16547.458984\n",
      "    epoch          : 363\n",
      "    loss           : 16713.809288675875\n",
      "    val_loss       : 16649.742735354954\n",
      "Train Epoch: 364 [192/225000 (0%)] Loss: 16330.458008\n",
      "Train Epoch: 364 [2688/225000 (1%)] Loss: 16694.986328\n",
      "Train Epoch: 364 [5184/225000 (2%)] Loss: 16841.871094\n",
      "Train Epoch: 364 [7680/225000 (3%)] Loss: 16399.441406\n",
      "Train Epoch: 364 [10176/225000 (5%)] Loss: 16702.109375\n",
      "Train Epoch: 364 [12672/225000 (6%)] Loss: 16487.992188\n",
      "Train Epoch: 364 [15168/225000 (7%)] Loss: 16520.970703\n",
      "Train Epoch: 364 [17664/225000 (8%)] Loss: 17059.494141\n",
      "Train Epoch: 364 [20160/225000 (9%)] Loss: 16806.222656\n",
      "Train Epoch: 364 [22656/225000 (10%)] Loss: 16802.037109\n",
      "Train Epoch: 364 [25152/225000 (11%)] Loss: 16869.597656\n",
      "Train Epoch: 364 [27648/225000 (12%)] Loss: 16869.310547\n",
      "Train Epoch: 364 [30144/225000 (13%)] Loss: 16465.535156\n",
      "Train Epoch: 364 [32640/225000 (15%)] Loss: 17019.050781\n",
      "Train Epoch: 364 [35136/225000 (16%)] Loss: 16281.008789\n",
      "Train Epoch: 364 [37632/225000 (17%)] Loss: 16327.059570\n",
      "Train Epoch: 364 [40128/225000 (18%)] Loss: 16830.376953\n",
      "Train Epoch: 364 [42624/225000 (19%)] Loss: 16605.722656\n",
      "Train Epoch: 364 [45120/225000 (20%)] Loss: 16557.230469\n",
      "Train Epoch: 364 [47616/225000 (21%)] Loss: 16408.863281\n",
      "Train Epoch: 364 [50112/225000 (22%)] Loss: 16676.380859\n",
      "Train Epoch: 364 [52608/225000 (23%)] Loss: 16996.720703\n",
      "Train Epoch: 364 [55104/225000 (24%)] Loss: 16321.895508\n",
      "Train Epoch: 364 [57600/225000 (26%)] Loss: 16839.384766\n",
      "Train Epoch: 364 [60096/225000 (27%)] Loss: 16744.355469\n",
      "Train Epoch: 364 [62592/225000 (28%)] Loss: 17990.667969\n",
      "Train Epoch: 364 [65088/225000 (29%)] Loss: 16455.076172\n",
      "Train Epoch: 364 [67584/225000 (30%)] Loss: 15994.461914\n",
      "Train Epoch: 364 [70080/225000 (31%)] Loss: 16720.398438\n",
      "Train Epoch: 364 [72576/225000 (32%)] Loss: 16848.800781\n",
      "Train Epoch: 364 [75072/225000 (33%)] Loss: 17109.730469\n",
      "Train Epoch: 364 [77568/225000 (34%)] Loss: 17103.138672\n",
      "Train Epoch: 364 [80064/225000 (36%)] Loss: 16513.679688\n",
      "Train Epoch: 364 [82560/225000 (37%)] Loss: 16750.289062\n",
      "Train Epoch: 364 [85056/225000 (38%)] Loss: 17086.300781\n",
      "Train Epoch: 364 [87552/225000 (39%)] Loss: 16298.401367\n",
      "Train Epoch: 364 [90048/225000 (40%)] Loss: 16784.328125\n",
      "Train Epoch: 364 [92544/225000 (41%)] Loss: 16612.441406\n",
      "Train Epoch: 364 [95040/225000 (42%)] Loss: 16504.738281\n",
      "Train Epoch: 364 [97536/225000 (43%)] Loss: 16701.900391\n",
      "Train Epoch: 364 [100032/225000 (44%)] Loss: 16555.859375\n",
      "Train Epoch: 364 [102528/225000 (46%)] Loss: 18349.037109\n",
      "Train Epoch: 364 [105024/225000 (47%)] Loss: 16506.847656\n",
      "Train Epoch: 364 [107520/225000 (48%)] Loss: 16619.167969\n",
      "Train Epoch: 364 [110016/225000 (49%)] Loss: 16849.798828\n",
      "Train Epoch: 364 [112512/225000 (50%)] Loss: 16582.960938\n",
      "Train Epoch: 364 [115008/225000 (51%)] Loss: 17031.169922\n",
      "Train Epoch: 364 [117504/225000 (52%)] Loss: 16654.507812\n",
      "Train Epoch: 364 [120000/225000 (53%)] Loss: 16861.984375\n",
      "Train Epoch: 364 [122496/225000 (54%)] Loss: 16491.546875\n",
      "Train Epoch: 364 [124992/225000 (56%)] Loss: 17883.347656\n",
      "Train Epoch: 364 [127488/225000 (57%)] Loss: 16245.541992\n",
      "Train Epoch: 364 [129984/225000 (58%)] Loss: 16306.259766\n",
      "Train Epoch: 364 [132480/225000 (59%)] Loss: 16876.013672\n",
      "Train Epoch: 364 [134976/225000 (60%)] Loss: 16806.097656\n",
      "Train Epoch: 364 [137472/225000 (61%)] Loss: 16801.859375\n",
      "Train Epoch: 364 [139968/225000 (62%)] Loss: 16778.314453\n",
      "Train Epoch: 364 [142464/225000 (63%)] Loss: 16320.708008\n",
      "Train Epoch: 364 [144960/225000 (64%)] Loss: 16571.925781\n",
      "Train Epoch: 364 [147456/225000 (66%)] Loss: 16654.023438\n",
      "Train Epoch: 364 [149952/225000 (67%)] Loss: 16624.781250\n",
      "Train Epoch: 364 [152448/225000 (68%)] Loss: 16525.773438\n",
      "Train Epoch: 364 [154944/225000 (69%)] Loss: 16933.777344\n",
      "Train Epoch: 364 [157440/225000 (70%)] Loss: 15988.790039\n",
      "Train Epoch: 364 [159936/225000 (71%)] Loss: 18219.529297\n",
      "Train Epoch: 364 [162432/225000 (72%)] Loss: 17344.941406\n",
      "Train Epoch: 364 [164928/225000 (73%)] Loss: 16198.639648\n",
      "Train Epoch: 364 [167424/225000 (74%)] Loss: 16774.714844\n",
      "Train Epoch: 364 [169920/225000 (76%)] Loss: 16513.265625\n",
      "Train Epoch: 364 [172416/225000 (77%)] Loss: 16420.701172\n",
      "Train Epoch: 364 [174912/225000 (78%)] Loss: 16808.296875\n",
      "Train Epoch: 364 [177408/225000 (79%)] Loss: 16432.009766\n",
      "Train Epoch: 364 [179904/225000 (80%)] Loss: 16982.681641\n",
      "Train Epoch: 364 [182400/225000 (81%)] Loss: 16450.113281\n",
      "Train Epoch: 364 [184896/225000 (82%)] Loss: 16553.656250\n",
      "Train Epoch: 364 [187392/225000 (83%)] Loss: 16694.757812\n",
      "Train Epoch: 364 [189888/225000 (84%)] Loss: 16471.974609\n",
      "Train Epoch: 364 [192384/225000 (86%)] Loss: 16857.367188\n",
      "Train Epoch: 364 [194880/225000 (87%)] Loss: 16369.210938\n",
      "Train Epoch: 364 [197376/225000 (88%)] Loss: 16389.082031\n",
      "Train Epoch: 364 [199872/225000 (89%)] Loss: 16232.448242\n",
      "Train Epoch: 364 [202368/225000 (90%)] Loss: 16948.945312\n",
      "Train Epoch: 364 [204864/225000 (91%)] Loss: 16735.218750\n",
      "Train Epoch: 364 [207360/225000 (92%)] Loss: 16645.943359\n",
      "Train Epoch: 364 [209856/225000 (93%)] Loss: 16732.437500\n",
      "Train Epoch: 364 [212352/225000 (94%)] Loss: 16845.683594\n",
      "Train Epoch: 364 [214848/225000 (95%)] Loss: 16120.836914\n",
      "Train Epoch: 364 [217344/225000 (97%)] Loss: 17495.828125\n",
      "Train Epoch: 364 [219840/225000 (98%)] Loss: 16960.619141\n",
      "Train Epoch: 364 [222336/225000 (99%)] Loss: 16993.132812\n",
      "Train Epoch: 364 [224832/225000 (100%)] Loss: 16835.976562\n",
      "    epoch          : 364\n",
      "    loss           : 16693.96840420355\n",
      "    val_loss       : 16696.530437110036\n",
      "Train Epoch: 365 [192/225000 (0%)] Loss: 16760.871094\n",
      "Train Epoch: 365 [2688/225000 (1%)] Loss: 16499.160156\n",
      "Train Epoch: 365 [5184/225000 (2%)] Loss: 16664.650391\n",
      "Train Epoch: 365 [7680/225000 (3%)] Loss: 16573.031250\n",
      "Train Epoch: 365 [10176/225000 (5%)] Loss: 16986.037109\n",
      "Train Epoch: 365 [12672/225000 (6%)] Loss: 16129.852539\n",
      "Train Epoch: 365 [15168/225000 (7%)] Loss: 16935.062500\n",
      "Train Epoch: 365 [17664/225000 (8%)] Loss: 16986.871094\n",
      "Train Epoch: 365 [20160/225000 (9%)] Loss: 16676.953125\n",
      "Train Epoch: 365 [22656/225000 (10%)] Loss: 16244.087891\n",
      "Train Epoch: 365 [25152/225000 (11%)] Loss: 16817.464844\n",
      "Train Epoch: 365 [27648/225000 (12%)] Loss: 16832.269531\n",
      "Train Epoch: 365 [30144/225000 (13%)] Loss: 16595.679688\n",
      "Train Epoch: 365 [32640/225000 (15%)] Loss: 16440.222656\n",
      "Train Epoch: 365 [35136/225000 (16%)] Loss: 16741.314453\n",
      "Train Epoch: 365 [37632/225000 (17%)] Loss: 18337.742188\n",
      "Train Epoch: 365 [40128/225000 (18%)] Loss: 16398.292969\n",
      "Train Epoch: 365 [42624/225000 (19%)] Loss: 16604.703125\n",
      "Train Epoch: 365 [45120/225000 (20%)] Loss: 16679.027344\n",
      "Train Epoch: 365 [47616/225000 (21%)] Loss: 16677.886719\n",
      "Train Epoch: 365 [50112/225000 (22%)] Loss: 16830.552734\n",
      "Train Epoch: 365 [52608/225000 (23%)] Loss: 18595.019531\n",
      "Train Epoch: 365 [55104/225000 (24%)] Loss: 16943.960938\n",
      "Train Epoch: 365 [57600/225000 (26%)] Loss: 16177.550781\n",
      "Train Epoch: 365 [60096/225000 (27%)] Loss: 16230.642578\n",
      "Train Epoch: 365 [62592/225000 (28%)] Loss: 16326.574219\n",
      "Train Epoch: 365 [65088/225000 (29%)] Loss: 16609.324219\n",
      "Train Epoch: 365 [67584/225000 (30%)] Loss: 16810.503906\n",
      "Train Epoch: 365 [70080/225000 (31%)] Loss: 18087.982422\n",
      "Train Epoch: 365 [72576/225000 (32%)] Loss: 16716.976562\n",
      "Train Epoch: 365 [75072/225000 (33%)] Loss: 16692.683594\n",
      "Train Epoch: 365 [77568/225000 (34%)] Loss: 16839.621094\n",
      "Train Epoch: 365 [80064/225000 (36%)] Loss: 16324.150391\n",
      "Train Epoch: 365 [82560/225000 (37%)] Loss: 16508.312500\n",
      "Train Epoch: 365 [85056/225000 (38%)] Loss: 17069.097656\n",
      "Train Epoch: 365 [87552/225000 (39%)] Loss: 16845.281250\n",
      "Train Epoch: 365 [90048/225000 (40%)] Loss: 16872.357422\n",
      "Train Epoch: 365 [92544/225000 (41%)] Loss: 16277.436523\n",
      "Train Epoch: 365 [95040/225000 (42%)] Loss: 16894.107422\n",
      "Train Epoch: 365 [97536/225000 (43%)] Loss: 16392.480469\n",
      "Train Epoch: 365 [100032/225000 (44%)] Loss: 16670.429688\n",
      "Train Epoch: 365 [102528/225000 (46%)] Loss: 16655.070312\n",
      "Train Epoch: 365 [105024/225000 (47%)] Loss: 16921.453125\n",
      "Train Epoch: 365 [107520/225000 (48%)] Loss: 16241.027344\n",
      "Train Epoch: 365 [110016/225000 (49%)] Loss: 17068.539062\n",
      "Train Epoch: 365 [112512/225000 (50%)] Loss: 16191.624023\n",
      "Train Epoch: 365 [115008/225000 (51%)] Loss: 16447.546875\n",
      "Train Epoch: 365 [117504/225000 (52%)] Loss: 16781.033203\n",
      "Train Epoch: 365 [120000/225000 (53%)] Loss: 17245.636719\n",
      "Train Epoch: 365 [122496/225000 (54%)] Loss: 17027.763672\n",
      "Train Epoch: 365 [124992/225000 (56%)] Loss: 16056.488281\n",
      "Train Epoch: 365 [127488/225000 (57%)] Loss: 16807.023438\n",
      "Train Epoch: 365 [129984/225000 (58%)] Loss: 16570.992188\n",
      "Train Epoch: 365 [132480/225000 (59%)] Loss: 16830.339844\n",
      "Train Epoch: 365 [134976/225000 (60%)] Loss: 16728.355469\n",
      "Train Epoch: 365 [137472/225000 (61%)] Loss: 16372.303711\n",
      "Train Epoch: 365 [139968/225000 (62%)] Loss: 16360.926758\n",
      "Train Epoch: 365 [142464/225000 (63%)] Loss: 16729.148438\n",
      "Train Epoch: 365 [144960/225000 (64%)] Loss: 16243.698242\n",
      "Train Epoch: 365 [147456/225000 (66%)] Loss: 16718.027344\n",
      "Train Epoch: 365 [149952/225000 (67%)] Loss: 16675.615234\n",
      "Train Epoch: 365 [152448/225000 (68%)] Loss: 16569.361328\n",
      "Train Epoch: 365 [154944/225000 (69%)] Loss: 16551.707031\n",
      "Train Epoch: 365 [157440/225000 (70%)] Loss: 16394.195312\n",
      "Train Epoch: 365 [159936/225000 (71%)] Loss: 16311.495117\n",
      "Train Epoch: 365 [162432/225000 (72%)] Loss: 17018.419922\n",
      "Train Epoch: 365 [164928/225000 (73%)] Loss: 16477.843750\n",
      "Train Epoch: 365 [167424/225000 (74%)] Loss: 17144.683594\n",
      "Train Epoch: 365 [169920/225000 (76%)] Loss: 16818.816406\n",
      "Train Epoch: 365 [172416/225000 (77%)] Loss: 16766.617188\n",
      "Train Epoch: 365 [174912/225000 (78%)] Loss: 18065.343750\n",
      "Train Epoch: 365 [177408/225000 (79%)] Loss: 16578.140625\n",
      "Train Epoch: 365 [179904/225000 (80%)] Loss: 16649.824219\n",
      "Train Epoch: 365 [182400/225000 (81%)] Loss: 17047.794922\n",
      "Train Epoch: 365 [184896/225000 (82%)] Loss: 16838.160156\n",
      "Train Epoch: 365 [187392/225000 (83%)] Loss: 17054.734375\n",
      "Train Epoch: 365 [189888/225000 (84%)] Loss: 16829.148438\n",
      "Train Epoch: 365 [192384/225000 (86%)] Loss: 16441.708984\n",
      "Train Epoch: 365 [194880/225000 (87%)] Loss: 16512.089844\n",
      "Train Epoch: 365 [197376/225000 (88%)] Loss: 16467.830078\n",
      "Train Epoch: 365 [199872/225000 (89%)] Loss: 16374.154297\n",
      "Train Epoch: 365 [202368/225000 (90%)] Loss: 16658.539062\n",
      "Train Epoch: 365 [204864/225000 (91%)] Loss: 16359.545898\n",
      "Train Epoch: 365 [207360/225000 (92%)] Loss: 16599.050781\n",
      "Train Epoch: 365 [209856/225000 (93%)] Loss: 16446.640625\n",
      "Train Epoch: 365 [212352/225000 (94%)] Loss: 16683.427734\n",
      "Train Epoch: 365 [214848/225000 (95%)] Loss: 16637.621094\n",
      "Train Epoch: 365 [217344/225000 (97%)] Loss: 16160.671875\n",
      "Train Epoch: 365 [219840/225000 (98%)] Loss: 16633.970703\n",
      "Train Epoch: 365 [222336/225000 (99%)] Loss: 16888.410156\n",
      "Train Epoch: 365 [224832/225000 (100%)] Loss: 17827.367188\n",
      "    epoch          : 365\n",
      "    loss           : 16717.421976655824\n",
      "    val_loss       : 16670.53342811297\n",
      "Train Epoch: 366 [192/225000 (0%)] Loss: 16930.015625\n",
      "Train Epoch: 366 [2688/225000 (1%)] Loss: 16740.242188\n",
      "Train Epoch: 366 [5184/225000 (2%)] Loss: 16489.494141\n",
      "Train Epoch: 366 [7680/225000 (3%)] Loss: 16431.304688\n",
      "Train Epoch: 366 [10176/225000 (5%)] Loss: 16427.267578\n",
      "Train Epoch: 366 [12672/225000 (6%)] Loss: 16578.527344\n",
      "Train Epoch: 366 [15168/225000 (7%)] Loss: 16865.058594\n",
      "Train Epoch: 366 [17664/225000 (8%)] Loss: 17032.003906\n",
      "Train Epoch: 366 [20160/225000 (9%)] Loss: 16337.157227\n",
      "Train Epoch: 366 [22656/225000 (10%)] Loss: 16707.863281\n",
      "Train Epoch: 366 [25152/225000 (11%)] Loss: 16861.085938\n",
      "Train Epoch: 366 [27648/225000 (12%)] Loss: 16426.945312\n",
      "Train Epoch: 366 [30144/225000 (13%)] Loss: 17854.880859\n",
      "Train Epoch: 366 [32640/225000 (15%)] Loss: 16715.476562\n",
      "Train Epoch: 366 [35136/225000 (16%)] Loss: 16323.493164\n",
      "Train Epoch: 366 [37632/225000 (17%)] Loss: 16907.660156\n",
      "Train Epoch: 366 [40128/225000 (18%)] Loss: 16788.484375\n",
      "Train Epoch: 366 [42624/225000 (19%)] Loss: 16879.861328\n",
      "Train Epoch: 366 [45120/225000 (20%)] Loss: 16579.917969\n",
      "Train Epoch: 366 [47616/225000 (21%)] Loss: 16333.905273\n",
      "Train Epoch: 366 [50112/225000 (22%)] Loss: 16980.398438\n",
      "Train Epoch: 366 [52608/225000 (23%)] Loss: 16590.853516\n",
      "Train Epoch: 366 [55104/225000 (24%)] Loss: 16487.302734\n",
      "Train Epoch: 366 [57600/225000 (26%)] Loss: 16862.074219\n",
      "Train Epoch: 366 [60096/225000 (27%)] Loss: 16620.542969\n",
      "Train Epoch: 366 [62592/225000 (28%)] Loss: 16695.169922\n",
      "Train Epoch: 366 [65088/225000 (29%)] Loss: 16549.847656\n",
      "Train Epoch: 366 [67584/225000 (30%)] Loss: 16233.633789\n",
      "Train Epoch: 366 [70080/225000 (31%)] Loss: 16521.492188\n",
      "Train Epoch: 366 [72576/225000 (32%)] Loss: 16631.613281\n",
      "Train Epoch: 366 [75072/225000 (33%)] Loss: 16559.308594\n",
      "Train Epoch: 366 [77568/225000 (34%)] Loss: 18101.746094\n",
      "Train Epoch: 366 [80064/225000 (36%)] Loss: 16704.707031\n",
      "Train Epoch: 366 [82560/225000 (37%)] Loss: 16236.040039\n",
      "Train Epoch: 366 [85056/225000 (38%)] Loss: 16137.966797\n",
      "Train Epoch: 366 [87552/225000 (39%)] Loss: 16554.121094\n",
      "Train Epoch: 366 [90048/225000 (40%)] Loss: 18094.429688\n",
      "Train Epoch: 366 [92544/225000 (41%)] Loss: 16811.457031\n",
      "Train Epoch: 366 [95040/225000 (42%)] Loss: 16776.585938\n",
      "Train Epoch: 366 [97536/225000 (43%)] Loss: 16503.976562\n",
      "Train Epoch: 366 [100032/225000 (44%)] Loss: 16262.847656\n",
      "Train Epoch: 366 [102528/225000 (46%)] Loss: 16539.585938\n",
      "Train Epoch: 366 [105024/225000 (47%)] Loss: 18140.378906\n",
      "Train Epoch: 366 [107520/225000 (48%)] Loss: 16622.251953\n",
      "Train Epoch: 366 [110016/225000 (49%)] Loss: 16283.574219\n",
      "Train Epoch: 366 [112512/225000 (50%)] Loss: 16555.326172\n",
      "Train Epoch: 366 [115008/225000 (51%)] Loss: 16937.927734\n",
      "Train Epoch: 366 [117504/225000 (52%)] Loss: 16776.468750\n",
      "Train Epoch: 366 [120000/225000 (53%)] Loss: 16471.259766\n",
      "Train Epoch: 366 [122496/225000 (54%)] Loss: 16152.555664\n",
      "Train Epoch: 366 [124992/225000 (56%)] Loss: 16555.113281\n",
      "Train Epoch: 366 [127488/225000 (57%)] Loss: 16684.300781\n",
      "Train Epoch: 366 [129984/225000 (58%)] Loss: 16903.687500\n",
      "Train Epoch: 366 [132480/225000 (59%)] Loss: 16992.488281\n",
      "Train Epoch: 366 [134976/225000 (60%)] Loss: 17168.248047\n",
      "Train Epoch: 366 [137472/225000 (61%)] Loss: 16535.816406\n",
      "Train Epoch: 366 [139968/225000 (62%)] Loss: 16491.675781\n",
      "Train Epoch: 366 [142464/225000 (63%)] Loss: 16543.855469\n",
      "Train Epoch: 366 [144960/225000 (64%)] Loss: 16417.775391\n",
      "Train Epoch: 366 [147456/225000 (66%)] Loss: 16534.019531\n",
      "Train Epoch: 366 [149952/225000 (67%)] Loss: 16880.990234\n",
      "Train Epoch: 366 [152448/225000 (68%)] Loss: 16603.558594\n",
      "Train Epoch: 366 [154944/225000 (69%)] Loss: 16649.011719\n",
      "Train Epoch: 366 [157440/225000 (70%)] Loss: 16923.744141\n",
      "Train Epoch: 366 [159936/225000 (71%)] Loss: 16368.424805\n",
      "Train Epoch: 366 [162432/225000 (72%)] Loss: 16656.107422\n",
      "Train Epoch: 366 [164928/225000 (73%)] Loss: 16587.312500\n",
      "Train Epoch: 366 [167424/225000 (74%)] Loss: 16813.406250\n",
      "Train Epoch: 366 [169920/225000 (76%)] Loss: 16984.107422\n",
      "Train Epoch: 366 [172416/225000 (77%)] Loss: 17035.455078\n",
      "Train Epoch: 366 [174912/225000 (78%)] Loss: 16446.462891\n",
      "Train Epoch: 366 [177408/225000 (79%)] Loss: 17932.691406\n",
      "Train Epoch: 366 [179904/225000 (80%)] Loss: 16497.265625\n",
      "Train Epoch: 366 [182400/225000 (81%)] Loss: 16582.324219\n",
      "Train Epoch: 366 [184896/225000 (82%)] Loss: 17745.531250\n",
      "Train Epoch: 366 [187392/225000 (83%)] Loss: 16404.048828\n",
      "Train Epoch: 366 [189888/225000 (84%)] Loss: 16547.593750\n",
      "Train Epoch: 366 [192384/225000 (86%)] Loss: 16499.595703\n",
      "Train Epoch: 366 [194880/225000 (87%)] Loss: 17050.273438\n",
      "Train Epoch: 366 [197376/225000 (88%)] Loss: 16315.924805\n",
      "Train Epoch: 366 [199872/225000 (89%)] Loss: 16678.101562\n",
      "Train Epoch: 366 [202368/225000 (90%)] Loss: 16546.277344\n",
      "Train Epoch: 366 [204864/225000 (91%)] Loss: 16854.218750\n",
      "Train Epoch: 366 [207360/225000 (92%)] Loss: 16608.851562\n",
      "Train Epoch: 366 [209856/225000 (93%)] Loss: 16194.219727\n",
      "Train Epoch: 366 [212352/225000 (94%)] Loss: 16227.976562\n",
      "Train Epoch: 366 [214848/225000 (95%)] Loss: 17129.382812\n",
      "Train Epoch: 366 [217344/225000 (97%)] Loss: 16466.408203\n",
      "Train Epoch: 366 [219840/225000 (98%)] Loss: 16607.570312\n",
      "Train Epoch: 366 [222336/225000 (99%)] Loss: 16057.715820\n",
      "Train Epoch: 366 [224832/225000 (100%)] Loss: 16537.988281\n",
      "    epoch          : 366\n",
      "    loss           : 16743.661450278636\n",
      "    val_loss       : 16634.532407145463\n",
      "Train Epoch: 367 [192/225000 (0%)] Loss: 16657.751953\n",
      "Train Epoch: 367 [2688/225000 (1%)] Loss: 16757.160156\n",
      "Train Epoch: 367 [5184/225000 (2%)] Loss: 16656.785156\n",
      "Train Epoch: 367 [7680/225000 (3%)] Loss: 16773.791016\n",
      "Train Epoch: 367 [10176/225000 (5%)] Loss: 16815.605469\n",
      "Train Epoch: 367 [12672/225000 (6%)] Loss: 25011.564453\n",
      "Train Epoch: 367 [15168/225000 (7%)] Loss: 16974.757812\n",
      "Train Epoch: 367 [17664/225000 (8%)] Loss: 16567.890625\n",
      "Train Epoch: 367 [20160/225000 (9%)] Loss: 16646.355469\n",
      "Train Epoch: 367 [22656/225000 (10%)] Loss: 16707.480469\n",
      "Train Epoch: 367 [25152/225000 (11%)] Loss: 16706.890625\n",
      "Train Epoch: 367 [27648/225000 (12%)] Loss: 16785.058594\n",
      "Train Epoch: 367 [30144/225000 (13%)] Loss: 16904.414062\n",
      "Train Epoch: 367 [32640/225000 (15%)] Loss: 16473.542969\n",
      "Train Epoch: 367 [35136/225000 (16%)] Loss: 16423.824219\n",
      "Train Epoch: 367 [37632/225000 (17%)] Loss: 16761.699219\n",
      "Train Epoch: 367 [40128/225000 (18%)] Loss: 16732.701172\n",
      "Train Epoch: 367 [42624/225000 (19%)] Loss: 16602.406250\n",
      "Train Epoch: 367 [45120/225000 (20%)] Loss: 16595.035156\n",
      "Train Epoch: 367 [47616/225000 (21%)] Loss: 16737.437500\n",
      "Train Epoch: 367 [50112/225000 (22%)] Loss: 16412.507812\n",
      "Train Epoch: 367 [52608/225000 (23%)] Loss: 16647.583984\n",
      "Train Epoch: 367 [55104/225000 (24%)] Loss: 16408.068359\n",
      "Train Epoch: 367 [57600/225000 (26%)] Loss: 16852.935547\n",
      "Train Epoch: 367 [60096/225000 (27%)] Loss: 16752.058594\n",
      "Train Epoch: 367 [62592/225000 (28%)] Loss: 16555.718750\n",
      "Train Epoch: 367 [65088/225000 (29%)] Loss: 16381.875977\n",
      "Train Epoch: 367 [67584/225000 (30%)] Loss: 18224.246094\n",
      "Train Epoch: 367 [70080/225000 (31%)] Loss: 16620.128906\n",
      "Train Epoch: 367 [72576/225000 (32%)] Loss: 16567.533203\n",
      "Train Epoch: 367 [75072/225000 (33%)] Loss: 16531.105469\n",
      "Train Epoch: 367 [77568/225000 (34%)] Loss: 16416.923828\n",
      "Train Epoch: 367 [80064/225000 (36%)] Loss: 16240.682617\n",
      "Train Epoch: 367 [82560/225000 (37%)] Loss: 16879.886719\n",
      "Train Epoch: 367 [85056/225000 (38%)] Loss: 18346.589844\n",
      "Train Epoch: 367 [87552/225000 (39%)] Loss: 17982.777344\n",
      "Train Epoch: 367 [90048/225000 (40%)] Loss: 16640.941406\n",
      "Train Epoch: 367 [92544/225000 (41%)] Loss: 17123.089844\n",
      "Train Epoch: 367 [95040/225000 (42%)] Loss: 16609.001953\n",
      "Train Epoch: 367 [97536/225000 (43%)] Loss: 16906.556641\n",
      "Train Epoch: 367 [100032/225000 (44%)] Loss: 16996.669922\n",
      "Train Epoch: 367 [102528/225000 (46%)] Loss: 16623.136719\n",
      "Train Epoch: 367 [105024/225000 (47%)] Loss: 16545.898438\n",
      "Train Epoch: 367 [107520/225000 (48%)] Loss: 16283.879883\n",
      "Train Epoch: 367 [110016/225000 (49%)] Loss: 16919.320312\n",
      "Train Epoch: 367 [112512/225000 (50%)] Loss: 16487.597656\n",
      "Train Epoch: 367 [115008/225000 (51%)] Loss: 16836.667969\n",
      "Train Epoch: 367 [117504/225000 (52%)] Loss: 16820.541016\n",
      "Train Epoch: 367 [120000/225000 (53%)] Loss: 16716.394531\n",
      "Train Epoch: 367 [122496/225000 (54%)] Loss: 16911.695312\n",
      "Train Epoch: 367 [124992/225000 (56%)] Loss: 16094.375977\n",
      "Train Epoch: 367 [127488/225000 (57%)] Loss: 16803.707031\n",
      "Train Epoch: 367 [129984/225000 (58%)] Loss: 16260.550781\n",
      "Train Epoch: 367 [132480/225000 (59%)] Loss: 16682.117188\n",
      "Train Epoch: 367 [134976/225000 (60%)] Loss: 16352.915039\n",
      "Train Epoch: 367 [137472/225000 (61%)] Loss: 16013.134766\n",
      "Train Epoch: 367 [139968/225000 (62%)] Loss: 16758.667969\n",
      "Train Epoch: 367 [142464/225000 (63%)] Loss: 16758.855469\n",
      "Train Epoch: 367 [144960/225000 (64%)] Loss: 16332.050781\n",
      "Train Epoch: 367 [147456/225000 (66%)] Loss: 16630.312500\n",
      "Train Epoch: 367 [149952/225000 (67%)] Loss: 16555.492188\n",
      "Train Epoch: 367 [152448/225000 (68%)] Loss: 16241.542969\n",
      "Train Epoch: 367 [154944/225000 (69%)] Loss: 16644.734375\n",
      "Train Epoch: 367 [157440/225000 (70%)] Loss: 18173.191406\n",
      "Train Epoch: 367 [159936/225000 (71%)] Loss: 16764.742188\n",
      "Train Epoch: 367 [162432/225000 (72%)] Loss: 16443.976562\n",
      "Train Epoch: 367 [164928/225000 (73%)] Loss: 16720.730469\n",
      "Train Epoch: 367 [167424/225000 (74%)] Loss: 18764.410156\n",
      "Train Epoch: 367 [169920/225000 (76%)] Loss: 16470.480469\n",
      "Train Epoch: 367 [172416/225000 (77%)] Loss: 16440.207031\n",
      "Train Epoch: 367 [174912/225000 (78%)] Loss: 16330.744141\n",
      "Train Epoch: 367 [177408/225000 (79%)] Loss: 16911.125000\n",
      "Train Epoch: 367 [179904/225000 (80%)] Loss: 16743.015625\n",
      "Train Epoch: 367 [182400/225000 (81%)] Loss: 16821.488281\n",
      "Train Epoch: 367 [184896/225000 (82%)] Loss: 16552.750000\n",
      "Train Epoch: 367 [187392/225000 (83%)] Loss: 16716.429688\n",
      "Train Epoch: 367 [189888/225000 (84%)] Loss: 16853.640625\n",
      "Train Epoch: 367 [192384/225000 (86%)] Loss: 16785.828125\n",
      "Train Epoch: 367 [194880/225000 (87%)] Loss: 16243.892578\n",
      "Train Epoch: 367 [197376/225000 (88%)] Loss: 16570.052734\n",
      "Train Epoch: 367 [199872/225000 (89%)] Loss: 16616.027344\n",
      "Train Epoch: 367 [202368/225000 (90%)] Loss: 16261.330078\n",
      "Train Epoch: 367 [204864/225000 (91%)] Loss: 16487.755859\n",
      "Train Epoch: 367 [207360/225000 (92%)] Loss: 16956.154297\n",
      "Train Epoch: 367 [209856/225000 (93%)] Loss: 18327.250000\n",
      "Train Epoch: 367 [212352/225000 (94%)] Loss: 17128.794922\n",
      "Train Epoch: 367 [214848/225000 (95%)] Loss: 17135.035156\n",
      "Train Epoch: 367 [217344/225000 (97%)] Loss: 16690.996094\n",
      "Train Epoch: 367 [219840/225000 (98%)] Loss: 16439.666016\n",
      "Train Epoch: 367 [222336/225000 (99%)] Loss: 17224.539062\n",
      "Train Epoch: 367 [224832/225000 (100%)] Loss: 16826.890625\n",
      "    epoch          : 367\n",
      "    loss           : 16730.953466630228\n",
      "    val_loss       : 16663.22051028896\n",
      "Train Epoch: 368 [192/225000 (0%)] Loss: 16706.312500\n",
      "Train Epoch: 368 [2688/225000 (1%)] Loss: 16485.769531\n",
      "Train Epoch: 368 [5184/225000 (2%)] Loss: 16936.771484\n",
      "Train Epoch: 368 [7680/225000 (3%)] Loss: 17209.011719\n",
      "Train Epoch: 368 [10176/225000 (5%)] Loss: 16038.261719\n",
      "Train Epoch: 368 [12672/225000 (6%)] Loss: 16522.591797\n",
      "Train Epoch: 368 [15168/225000 (7%)] Loss: 16736.695312\n",
      "Train Epoch: 368 [17664/225000 (8%)] Loss: 16277.386719\n",
      "Train Epoch: 368 [20160/225000 (9%)] Loss: 16619.710938\n",
      "Train Epoch: 368 [22656/225000 (10%)] Loss: 16718.675781\n",
      "Train Epoch: 368 [25152/225000 (11%)] Loss: 16708.955078\n",
      "Train Epoch: 368 [27648/225000 (12%)] Loss: 18131.560547\n",
      "Train Epoch: 368 [30144/225000 (13%)] Loss: 16285.026367\n",
      "Train Epoch: 368 [32640/225000 (15%)] Loss: 16808.683594\n",
      "Train Epoch: 368 [35136/225000 (16%)] Loss: 16130.860352\n",
      "Train Epoch: 368 [37632/225000 (17%)] Loss: 16510.214844\n",
      "Train Epoch: 368 [40128/225000 (18%)] Loss: 16249.637695\n",
      "Train Epoch: 368 [42624/225000 (19%)] Loss: 16534.253906\n",
      "Train Epoch: 368 [45120/225000 (20%)] Loss: 16124.219727\n",
      "Train Epoch: 368 [47616/225000 (21%)] Loss: 16827.828125\n",
      "Train Epoch: 368 [50112/225000 (22%)] Loss: 17003.562500\n",
      "Train Epoch: 368 [52608/225000 (23%)] Loss: 16243.983398\n",
      "Train Epoch: 368 [55104/225000 (24%)] Loss: 16697.613281\n",
      "Train Epoch: 368 [57600/225000 (26%)] Loss: 17069.931641\n",
      "Train Epoch: 368 [60096/225000 (27%)] Loss: 16330.070312\n",
      "Train Epoch: 368 [62592/225000 (28%)] Loss: 18201.582031\n",
      "Train Epoch: 368 [65088/225000 (29%)] Loss: 16127.673828\n",
      "Train Epoch: 368 [67584/225000 (30%)] Loss: 16260.256836\n",
      "Train Epoch: 368 [70080/225000 (31%)] Loss: 16296.887695\n",
      "Train Epoch: 368 [72576/225000 (32%)] Loss: 16659.191406\n",
      "Train Epoch: 368 [75072/225000 (33%)] Loss: 16629.695312\n",
      "Train Epoch: 368 [77568/225000 (34%)] Loss: 16876.957031\n",
      "Train Epoch: 368 [80064/225000 (36%)] Loss: 16858.683594\n",
      "Train Epoch: 368 [82560/225000 (37%)] Loss: 16835.736328\n",
      "Train Epoch: 368 [85056/225000 (38%)] Loss: 16907.611328\n",
      "Train Epoch: 368 [87552/225000 (39%)] Loss: 16900.812500\n",
      "Train Epoch: 368 [90048/225000 (40%)] Loss: 16450.271484\n",
      "Train Epoch: 368 [92544/225000 (41%)] Loss: 16426.894531\n",
      "Train Epoch: 368 [95040/225000 (42%)] Loss: 16571.718750\n",
      "Train Epoch: 368 [97536/225000 (43%)] Loss: 16092.462891\n",
      "Train Epoch: 368 [100032/225000 (44%)] Loss: 17156.222656\n",
      "Train Epoch: 368 [102528/225000 (46%)] Loss: 16865.121094\n",
      "Train Epoch: 368 [105024/225000 (47%)] Loss: 17021.164062\n",
      "Train Epoch: 368 [107520/225000 (48%)] Loss: 16347.227539\n",
      "Train Epoch: 368 [110016/225000 (49%)] Loss: 16734.250000\n",
      "Train Epoch: 368 [112512/225000 (50%)] Loss: 16593.291016\n",
      "Train Epoch: 368 [115008/225000 (51%)] Loss: 16573.240234\n",
      "Train Epoch: 368 [117504/225000 (52%)] Loss: 16640.523438\n",
      "Train Epoch: 368 [120000/225000 (53%)] Loss: 16547.388672\n",
      "Train Epoch: 368 [122496/225000 (54%)] Loss: 16344.970703\n",
      "Train Epoch: 368 [124992/225000 (56%)] Loss: 16819.429688\n",
      "Train Epoch: 368 [127488/225000 (57%)] Loss: 16022.870117\n",
      "Train Epoch: 368 [129984/225000 (58%)] Loss: 17123.443359\n",
      "Train Epoch: 368 [132480/225000 (59%)] Loss: 16989.179688\n",
      "Train Epoch: 368 [134976/225000 (60%)] Loss: 16782.212891\n",
      "Train Epoch: 368 [137472/225000 (61%)] Loss: 16450.210938\n",
      "Train Epoch: 368 [139968/225000 (62%)] Loss: 17011.843750\n",
      "Train Epoch: 368 [142464/225000 (63%)] Loss: 18051.746094\n",
      "Train Epoch: 368 [144960/225000 (64%)] Loss: 16568.320312\n",
      "Train Epoch: 368 [147456/225000 (66%)] Loss: 17126.240234\n",
      "Train Epoch: 368 [149952/225000 (67%)] Loss: 16902.160156\n",
      "Train Epoch: 368 [152448/225000 (68%)] Loss: 15688.699219\n",
      "Train Epoch: 368 [154944/225000 (69%)] Loss: 16252.562500\n",
      "Train Epoch: 368 [157440/225000 (70%)] Loss: 16227.419922\n",
      "Train Epoch: 368 [159936/225000 (71%)] Loss: 16767.636719\n",
      "Train Epoch: 368 [162432/225000 (72%)] Loss: 16520.261719\n",
      "Train Epoch: 368 [164928/225000 (73%)] Loss: 16733.076172\n",
      "Train Epoch: 368 [167424/225000 (74%)] Loss: 16788.625000\n",
      "Train Epoch: 368 [169920/225000 (76%)] Loss: 17102.179688\n",
      "Train Epoch: 368 [172416/225000 (77%)] Loss: 16235.081055\n",
      "Train Epoch: 368 [174912/225000 (78%)] Loss: 16492.320312\n",
      "Train Epoch: 368 [177408/225000 (79%)] Loss: 16682.708984\n",
      "Train Epoch: 368 [179904/225000 (80%)] Loss: 16790.425781\n",
      "Train Epoch: 368 [182400/225000 (81%)] Loss: 16905.417969\n",
      "Train Epoch: 368 [184896/225000 (82%)] Loss: 16962.960938\n",
      "Train Epoch: 368 [187392/225000 (83%)] Loss: 16232.116211\n",
      "Train Epoch: 368 [189888/225000 (84%)] Loss: 16678.394531\n",
      "Train Epoch: 368 [192384/225000 (86%)] Loss: 16218.469727\n",
      "Train Epoch: 368 [194880/225000 (87%)] Loss: 16577.095703\n",
      "Train Epoch: 368 [197376/225000 (88%)] Loss: 16761.320312\n",
      "Train Epoch: 368 [199872/225000 (89%)] Loss: 16888.437500\n",
      "Train Epoch: 368 [202368/225000 (90%)] Loss: 16182.790039\n",
      "Train Epoch: 368 [204864/225000 (91%)] Loss: 16944.410156\n",
      "Train Epoch: 368 [207360/225000 (92%)] Loss: 16417.339844\n",
      "Train Epoch: 368 [209856/225000 (93%)] Loss: 16769.742188\n",
      "Train Epoch: 368 [212352/225000 (94%)] Loss: 16644.287109\n",
      "Train Epoch: 368 [214848/225000 (95%)] Loss: 16627.812500\n",
      "Train Epoch: 368 [217344/225000 (97%)] Loss: 16644.230469\n",
      "Train Epoch: 368 [219840/225000 (98%)] Loss: 16727.269531\n",
      "Train Epoch: 368 [222336/225000 (99%)] Loss: 16285.482422\n",
      "Train Epoch: 368 [224832/225000 (100%)] Loss: 16522.535156\n",
      "    epoch          : 368\n",
      "    loss           : 16697.360543208724\n",
      "    val_loss       : 16702.210826572114\n",
      "Train Epoch: 369 [192/225000 (0%)] Loss: 16410.056641\n",
      "Train Epoch: 369 [2688/225000 (1%)] Loss: 16493.789062\n",
      "Train Epoch: 369 [5184/225000 (2%)] Loss: 16221.935547\n",
      "Train Epoch: 369 [7680/225000 (3%)] Loss: 16660.605469\n",
      "Train Epoch: 369 [10176/225000 (5%)] Loss: 16675.730469\n",
      "Train Epoch: 369 [12672/225000 (6%)] Loss: 16873.906250\n",
      "Train Epoch: 369 [15168/225000 (7%)] Loss: 17199.921875\n",
      "Train Epoch: 369 [17664/225000 (8%)] Loss: 16396.003906\n",
      "Train Epoch: 369 [20160/225000 (9%)] Loss: 16868.517578\n",
      "Train Epoch: 369 [22656/225000 (10%)] Loss: 16777.132812\n",
      "Train Epoch: 369 [25152/225000 (11%)] Loss: 16774.199219\n",
      "Train Epoch: 369 [27648/225000 (12%)] Loss: 17115.333984\n",
      "Train Epoch: 369 [30144/225000 (13%)] Loss: 16554.089844\n",
      "Train Epoch: 369 [32640/225000 (15%)] Loss: 16565.699219\n",
      "Train Epoch: 369 [35136/225000 (16%)] Loss: 17253.394531\n",
      "Train Epoch: 369 [37632/225000 (17%)] Loss: 16348.446289\n",
      "Train Epoch: 369 [40128/225000 (18%)] Loss: 16190.944336\n",
      "Train Epoch: 369 [42624/225000 (19%)] Loss: 16752.363281\n",
      "Train Epoch: 369 [45120/225000 (20%)] Loss: 17249.080078\n",
      "Train Epoch: 369 [47616/225000 (21%)] Loss: 16966.484375\n",
      "Train Epoch: 369 [50112/225000 (22%)] Loss: 17982.458984\n",
      "Train Epoch: 369 [52608/225000 (23%)] Loss: 16059.865234\n",
      "Train Epoch: 369 [55104/225000 (24%)] Loss: 16346.526367\n",
      "Train Epoch: 369 [57600/225000 (26%)] Loss: 17052.535156\n",
      "Train Epoch: 369 [60096/225000 (27%)] Loss: 16960.900391\n",
      "Train Epoch: 369 [62592/225000 (28%)] Loss: 16595.570312\n",
      "Train Epoch: 369 [65088/225000 (29%)] Loss: 16540.085938\n",
      "Train Epoch: 369 [67584/225000 (30%)] Loss: 16745.378906\n",
      "Train Epoch: 369 [70080/225000 (31%)] Loss: 17041.195312\n",
      "Train Epoch: 369 [72576/225000 (32%)] Loss: 16887.648438\n",
      "Train Epoch: 369 [75072/225000 (33%)] Loss: 16491.820312\n",
      "Train Epoch: 369 [77568/225000 (34%)] Loss: 16215.669922\n",
      "Train Epoch: 369 [80064/225000 (36%)] Loss: 18199.607422\n",
      "Train Epoch: 369 [82560/225000 (37%)] Loss: 16965.441406\n",
      "Train Epoch: 369 [85056/225000 (38%)] Loss: 16255.282227\n",
      "Train Epoch: 369 [87552/225000 (39%)] Loss: 16379.922852\n",
      "Train Epoch: 369 [90048/225000 (40%)] Loss: 18056.917969\n",
      "Train Epoch: 369 [92544/225000 (41%)] Loss: 16763.613281\n",
      "Train Epoch: 369 [95040/225000 (42%)] Loss: 16189.879883\n",
      "Train Epoch: 369 [97536/225000 (43%)] Loss: 16492.984375\n",
      "Train Epoch: 369 [100032/225000 (44%)] Loss: 16332.842773\n",
      "Train Epoch: 369 [102528/225000 (46%)] Loss: 25076.589844\n",
      "Train Epoch: 369 [105024/225000 (47%)] Loss: 16474.843750\n",
      "Train Epoch: 369 [107520/225000 (48%)] Loss: 16588.533203\n",
      "Train Epoch: 369 [110016/225000 (49%)] Loss: 16385.800781\n",
      "Train Epoch: 369 [112512/225000 (50%)] Loss: 17013.304688\n",
      "Train Epoch: 369 [115008/225000 (51%)] Loss: 16725.236328\n",
      "Train Epoch: 369 [117504/225000 (52%)] Loss: 16727.164062\n",
      "Train Epoch: 369 [120000/225000 (53%)] Loss: 16578.806641\n",
      "Train Epoch: 369 [122496/225000 (54%)] Loss: 18583.894531\n",
      "Train Epoch: 369 [124992/225000 (56%)] Loss: 16778.962891\n",
      "Train Epoch: 369 [127488/225000 (57%)] Loss: 16493.570312\n",
      "Train Epoch: 369 [129984/225000 (58%)] Loss: 17762.078125\n",
      "Train Epoch: 369 [132480/225000 (59%)] Loss: 16897.673828\n",
      "Train Epoch: 369 [134976/225000 (60%)] Loss: 16475.654297\n",
      "Train Epoch: 369 [137472/225000 (61%)] Loss: 16647.044922\n",
      "Train Epoch: 369 [139968/225000 (62%)] Loss: 16649.375000\n",
      "Train Epoch: 369 [142464/225000 (63%)] Loss: 16516.531250\n",
      "Train Epoch: 369 [144960/225000 (64%)] Loss: 16695.806641\n",
      "Train Epoch: 369 [147456/225000 (66%)] Loss: 16437.105469\n",
      "Train Epoch: 369 [149952/225000 (67%)] Loss: 16766.242188\n",
      "Train Epoch: 369 [152448/225000 (68%)] Loss: 16517.132812\n",
      "Train Epoch: 369 [154944/225000 (69%)] Loss: 16375.000977\n",
      "Train Epoch: 369 [157440/225000 (70%)] Loss: 16958.556641\n",
      "Train Epoch: 369 [159936/225000 (71%)] Loss: 16494.523438\n",
      "Train Epoch: 369 [162432/225000 (72%)] Loss: 17217.457031\n",
      "Train Epoch: 369 [164928/225000 (73%)] Loss: 16489.167969\n",
      "Train Epoch: 369 [167424/225000 (74%)] Loss: 16817.269531\n",
      "Train Epoch: 369 [169920/225000 (76%)] Loss: 16161.622070\n",
      "Train Epoch: 369 [172416/225000 (77%)] Loss: 17094.210938\n",
      "Train Epoch: 369 [174912/225000 (78%)] Loss: 16549.443359\n",
      "Train Epoch: 369 [177408/225000 (79%)] Loss: 16913.753906\n",
      "Train Epoch: 369 [179904/225000 (80%)] Loss: 17224.019531\n",
      "Train Epoch: 369 [182400/225000 (81%)] Loss: 16830.457031\n",
      "Train Epoch: 369 [184896/225000 (82%)] Loss: 16466.449219\n",
      "Train Epoch: 369 [187392/225000 (83%)] Loss: 16241.585938\n",
      "Train Epoch: 369 [189888/225000 (84%)] Loss: 16791.875000\n",
      "Train Epoch: 369 [192384/225000 (86%)] Loss: 16181.292969\n",
      "Train Epoch: 369 [194880/225000 (87%)] Loss: 16735.687500\n",
      "Train Epoch: 369 [197376/225000 (88%)] Loss: 16484.429688\n",
      "Train Epoch: 369 [199872/225000 (89%)] Loss: 16604.058594\n",
      "Train Epoch: 369 [202368/225000 (90%)] Loss: 16490.109375\n",
      "Train Epoch: 369 [204864/225000 (91%)] Loss: 16698.796875\n",
      "Train Epoch: 369 [207360/225000 (92%)] Loss: 16617.003906\n",
      "Train Epoch: 369 [209856/225000 (93%)] Loss: 16275.500000\n",
      "Train Epoch: 369 [212352/225000 (94%)] Loss: 16562.355469\n",
      "Train Epoch: 369 [214848/225000 (95%)] Loss: 16526.777344\n",
      "Train Epoch: 369 [217344/225000 (97%)] Loss: 16646.601562\n",
      "Train Epoch: 369 [219840/225000 (98%)] Loss: 17038.203125\n",
      "Train Epoch: 369 [222336/225000 (99%)] Loss: 16845.888672\n",
      "Train Epoch: 369 [224832/225000 (100%)] Loss: 17996.089844\n",
      "    epoch          : 369\n",
      "    loss           : 16725.527527897026\n",
      "    val_loss       : 16685.78320514337\n",
      "Train Epoch: 370 [192/225000 (0%)] Loss: 16443.222656\n",
      "Train Epoch: 370 [2688/225000 (1%)] Loss: 16868.583984\n",
      "Train Epoch: 370 [5184/225000 (2%)] Loss: 16391.013672\n",
      "Train Epoch: 370 [7680/225000 (3%)] Loss: 16989.234375\n",
      "Train Epoch: 370 [10176/225000 (5%)] Loss: 18205.062500\n",
      "Train Epoch: 370 [12672/225000 (6%)] Loss: 16785.769531\n",
      "Train Epoch: 370 [15168/225000 (7%)] Loss: 16761.511719\n",
      "Train Epoch: 370 [17664/225000 (8%)] Loss: 16592.787109\n",
      "Train Epoch: 370 [20160/225000 (9%)] Loss: 18326.113281\n",
      "Train Epoch: 370 [22656/225000 (10%)] Loss: 16682.691406\n",
      "Train Epoch: 370 [25152/225000 (11%)] Loss: 16555.007812\n",
      "Train Epoch: 370 [27648/225000 (12%)] Loss: 16036.480469\n",
      "Train Epoch: 370 [30144/225000 (13%)] Loss: 16121.526367\n",
      "Train Epoch: 370 [32640/225000 (15%)] Loss: 16374.957031\n",
      "Train Epoch: 370 [35136/225000 (16%)] Loss: 16815.417969\n",
      "Train Epoch: 370 [37632/225000 (17%)] Loss: 16552.257812\n",
      "Train Epoch: 370 [40128/225000 (18%)] Loss: 17379.962891\n",
      "Train Epoch: 370 [42624/225000 (19%)] Loss: 16720.597656\n",
      "Train Epoch: 370 [45120/225000 (20%)] Loss: 16810.207031\n",
      "Train Epoch: 370 [47616/225000 (21%)] Loss: 16715.156250\n",
      "Train Epoch: 370 [50112/225000 (22%)] Loss: 16852.880859\n",
      "Train Epoch: 370 [52608/225000 (23%)] Loss: 16948.656250\n",
      "Train Epoch: 370 [55104/225000 (24%)] Loss: 16630.765625\n",
      "Train Epoch: 370 [57600/225000 (26%)] Loss: 16379.632812\n",
      "Train Epoch: 370 [60096/225000 (27%)] Loss: 16608.996094\n",
      "Train Epoch: 370 [62592/225000 (28%)] Loss: 16596.460938\n",
      "Train Epoch: 370 [65088/225000 (29%)] Loss: 16394.435547\n",
      "Train Epoch: 370 [67584/225000 (30%)] Loss: 16547.738281\n",
      "Train Epoch: 370 [70080/225000 (31%)] Loss: 16489.519531\n",
      "Train Epoch: 370 [72576/225000 (32%)] Loss: 16867.750000\n",
      "Train Epoch: 370 [75072/225000 (33%)] Loss: 16268.750977\n",
      "Train Epoch: 370 [77568/225000 (34%)] Loss: 16390.171875\n",
      "Train Epoch: 370 [80064/225000 (36%)] Loss: 16332.252930\n",
      "Train Epoch: 370 [82560/225000 (37%)] Loss: 16453.972656\n",
      "Train Epoch: 370 [85056/225000 (38%)] Loss: 16990.171875\n",
      "Train Epoch: 370 [87552/225000 (39%)] Loss: 16582.492188\n",
      "Train Epoch: 370 [90048/225000 (40%)] Loss: 16487.220703\n",
      "Train Epoch: 370 [92544/225000 (41%)] Loss: 16183.233398\n",
      "Train Epoch: 370 [95040/225000 (42%)] Loss: 16919.644531\n",
      "Train Epoch: 370 [97536/225000 (43%)] Loss: 16495.601562\n",
      "Train Epoch: 370 [100032/225000 (44%)] Loss: 16551.054688\n",
      "Train Epoch: 370 [102528/225000 (46%)] Loss: 16175.419922\n",
      "Train Epoch: 370 [105024/225000 (47%)] Loss: 16895.476562\n",
      "Train Epoch: 370 [107520/225000 (48%)] Loss: 16664.242188\n",
      "Train Epoch: 370 [110016/225000 (49%)] Loss: 16734.339844\n",
      "Train Epoch: 370 [112512/225000 (50%)] Loss: 26541.125000\n",
      "Train Epoch: 370 [115008/225000 (51%)] Loss: 16525.537109\n",
      "Train Epoch: 370 [117504/225000 (52%)] Loss: 16823.734375\n",
      "Train Epoch: 370 [120000/225000 (53%)] Loss: 16886.273438\n",
      "Train Epoch: 370 [122496/225000 (54%)] Loss: 16623.583984\n",
      "Train Epoch: 370 [124992/225000 (56%)] Loss: 16263.337891\n",
      "Train Epoch: 370 [127488/225000 (57%)] Loss: 16358.702148\n",
      "Train Epoch: 370 [129984/225000 (58%)] Loss: 16626.218750\n",
      "Train Epoch: 370 [132480/225000 (59%)] Loss: 16739.589844\n",
      "Train Epoch: 370 [134976/225000 (60%)] Loss: 16351.389648\n",
      "Train Epoch: 370 [137472/225000 (61%)] Loss: 17109.152344\n",
      "Train Epoch: 370 [139968/225000 (62%)] Loss: 16690.234375\n",
      "Train Epoch: 370 [142464/225000 (63%)] Loss: 16915.640625\n",
      "Train Epoch: 370 [144960/225000 (64%)] Loss: 16667.685547\n",
      "Train Epoch: 370 [147456/225000 (66%)] Loss: 15943.736328\n",
      "Train Epoch: 370 [149952/225000 (67%)] Loss: 16718.623047\n",
      "Train Epoch: 370 [152448/225000 (68%)] Loss: 16906.376953\n",
      "Train Epoch: 370 [154944/225000 (69%)] Loss: 16943.984375\n",
      "Train Epoch: 370 [157440/225000 (70%)] Loss: 16434.117188\n",
      "Train Epoch: 370 [159936/225000 (71%)] Loss: 16786.945312\n",
      "Train Epoch: 370 [162432/225000 (72%)] Loss: 16882.355469\n",
      "Train Epoch: 370 [164928/225000 (73%)] Loss: 17073.218750\n",
      "Train Epoch: 370 [167424/225000 (74%)] Loss: 16810.193359\n",
      "Train Epoch: 370 [169920/225000 (76%)] Loss: 16530.025391\n",
      "Train Epoch: 370 [172416/225000 (77%)] Loss: 16556.902344\n",
      "Train Epoch: 370 [174912/225000 (78%)] Loss: 16436.472656\n",
      "Train Epoch: 370 [177408/225000 (79%)] Loss: 16913.425781\n",
      "Train Epoch: 370 [179904/225000 (80%)] Loss: 18264.019531\n",
      "Train Epoch: 370 [182400/225000 (81%)] Loss: 16441.039062\n",
      "Train Epoch: 370 [184896/225000 (82%)] Loss: 16230.495117\n",
      "Train Epoch: 370 [187392/225000 (83%)] Loss: 16248.444336\n",
      "Train Epoch: 370 [189888/225000 (84%)] Loss: 16822.466797\n",
      "Train Epoch: 370 [192384/225000 (86%)] Loss: 16177.715820\n",
      "Train Epoch: 370 [194880/225000 (87%)] Loss: 16805.480469\n",
      "Train Epoch: 370 [197376/225000 (88%)] Loss: 16540.433594\n",
      "Train Epoch: 370 [199872/225000 (89%)] Loss: 16708.855469\n",
      "Train Epoch: 370 [202368/225000 (90%)] Loss: 16678.515625\n",
      "Train Epoch: 370 [204864/225000 (91%)] Loss: 16457.816406\n",
      "Train Epoch: 370 [207360/225000 (92%)] Loss: 16931.921875\n",
      "Train Epoch: 370 [209856/225000 (93%)] Loss: 16552.380859\n",
      "Train Epoch: 370 [212352/225000 (94%)] Loss: 16252.740234\n",
      "Train Epoch: 370 [214848/225000 (95%)] Loss: 16756.851562\n",
      "Train Epoch: 370 [217344/225000 (97%)] Loss: 16483.640625\n",
      "Train Epoch: 370 [219840/225000 (98%)] Loss: 16329.578125\n",
      "Train Epoch: 370 [222336/225000 (99%)] Loss: 16746.855469\n",
      "Train Epoch: 370 [224832/225000 (100%)] Loss: 16888.125000\n",
      "    epoch          : 370\n",
      "    loss           : 16732.566085450886\n",
      "    val_loss       : 16658.020722176283\n",
      "Train Epoch: 371 [192/225000 (0%)] Loss: 16669.574219\n",
      "Train Epoch: 371 [2688/225000 (1%)] Loss: 16708.914062\n",
      "Train Epoch: 371 [5184/225000 (2%)] Loss: 17102.085938\n",
      "Train Epoch: 371 [7680/225000 (3%)] Loss: 17900.707031\n",
      "Train Epoch: 371 [10176/225000 (5%)] Loss: 16581.746094\n",
      "Train Epoch: 371 [12672/225000 (6%)] Loss: 16480.359375\n",
      "Train Epoch: 371 [15168/225000 (7%)] Loss: 16601.359375\n",
      "Train Epoch: 371 [17664/225000 (8%)] Loss: 17041.019531\n",
      "Train Epoch: 371 [20160/225000 (9%)] Loss: 16418.558594\n",
      "Train Epoch: 371 [22656/225000 (10%)] Loss: 16501.253906\n",
      "Train Epoch: 371 [25152/225000 (11%)] Loss: 16373.984375\n",
      "Train Epoch: 371 [27648/225000 (12%)] Loss: 16743.068359\n",
      "Train Epoch: 371 [30144/225000 (13%)] Loss: 16258.489258\n",
      "Train Epoch: 371 [32640/225000 (15%)] Loss: 16677.375000\n",
      "Train Epoch: 371 [35136/225000 (16%)] Loss: 16695.960938\n",
      "Train Epoch: 371 [37632/225000 (17%)] Loss: 16211.691406\n",
      "Train Epoch: 371 [40128/225000 (18%)] Loss: 17020.285156\n",
      "Train Epoch: 371 [42624/225000 (19%)] Loss: 16779.460938\n",
      "Train Epoch: 371 [45120/225000 (20%)] Loss: 18600.371094\n",
      "Train Epoch: 371 [47616/225000 (21%)] Loss: 17259.109375\n",
      "Train Epoch: 371 [50112/225000 (22%)] Loss: 16872.117188\n",
      "Train Epoch: 371 [52608/225000 (23%)] Loss: 16452.949219\n",
      "Train Epoch: 371 [55104/225000 (24%)] Loss: 16773.050781\n",
      "Train Epoch: 371 [57600/225000 (26%)] Loss: 16617.246094\n",
      "Train Epoch: 371 [60096/225000 (27%)] Loss: 18106.564453\n",
      "Train Epoch: 371 [62592/225000 (28%)] Loss: 16709.544922\n",
      "Train Epoch: 371 [65088/225000 (29%)] Loss: 16590.031250\n",
      "Train Epoch: 371 [67584/225000 (30%)] Loss: 16550.058594\n",
      "Train Epoch: 371 [70080/225000 (31%)] Loss: 16962.003906\n",
      "Train Epoch: 371 [72576/225000 (32%)] Loss: 16753.140625\n",
      "Train Epoch: 371 [75072/225000 (33%)] Loss: 16583.214844\n",
      "Train Epoch: 371 [77568/225000 (34%)] Loss: 16382.554688\n",
      "Train Epoch: 371 [80064/225000 (36%)] Loss: 17264.388672\n",
      "Train Epoch: 371 [82560/225000 (37%)] Loss: 16283.777344\n",
      "Train Epoch: 371 [85056/225000 (38%)] Loss: 16232.710938\n",
      "Train Epoch: 371 [87552/225000 (39%)] Loss: 16954.212891\n",
      "Train Epoch: 371 [90048/225000 (40%)] Loss: 16779.750000\n",
      "Train Epoch: 371 [92544/225000 (41%)] Loss: 16431.980469\n",
      "Train Epoch: 371 [95040/225000 (42%)] Loss: 16400.386719\n",
      "Train Epoch: 371 [97536/225000 (43%)] Loss: 16437.183594\n",
      "Train Epoch: 371 [100032/225000 (44%)] Loss: 16422.349609\n",
      "Train Epoch: 371 [102528/225000 (46%)] Loss: 16812.515625\n",
      "Train Epoch: 371 [105024/225000 (47%)] Loss: 16561.109375\n",
      "Train Epoch: 371 [107520/225000 (48%)] Loss: 16306.339844\n",
      "Train Epoch: 371 [110016/225000 (49%)] Loss: 16557.851562\n",
      "Train Epoch: 371 [112512/225000 (50%)] Loss: 18255.509766\n",
      "Train Epoch: 371 [115008/225000 (51%)] Loss: 16459.914062\n",
      "Train Epoch: 371 [117504/225000 (52%)] Loss: 16729.167969\n",
      "Train Epoch: 371 [120000/225000 (53%)] Loss: 16712.144531\n",
      "Train Epoch: 371 [122496/225000 (54%)] Loss: 16494.375000\n",
      "Train Epoch: 371 [124992/225000 (56%)] Loss: 16611.148438\n",
      "Train Epoch: 371 [127488/225000 (57%)] Loss: 16488.613281\n",
      "Train Epoch: 371 [129984/225000 (58%)] Loss: 16431.332031\n",
      "Train Epoch: 371 [132480/225000 (59%)] Loss: 16307.920898\n",
      "Train Epoch: 371 [134976/225000 (60%)] Loss: 16576.687500\n",
      "Train Epoch: 371 [137472/225000 (61%)] Loss: 16384.875000\n",
      "Train Epoch: 371 [139968/225000 (62%)] Loss: 16652.974609\n",
      "Train Epoch: 371 [142464/225000 (63%)] Loss: 16679.632812\n",
      "Train Epoch: 371 [144960/225000 (64%)] Loss: 16378.978516\n",
      "Train Epoch: 371 [147456/225000 (66%)] Loss: 16738.839844\n",
      "Train Epoch: 371 [149952/225000 (67%)] Loss: 16900.605469\n",
      "Train Epoch: 371 [152448/225000 (68%)] Loss: 16502.794922\n",
      "Train Epoch: 371 [154944/225000 (69%)] Loss: 16255.608398\n",
      "Train Epoch: 371 [157440/225000 (70%)] Loss: 16751.515625\n",
      "Train Epoch: 371 [159936/225000 (71%)] Loss: 16962.167969\n",
      "Train Epoch: 371 [162432/225000 (72%)] Loss: 16874.523438\n",
      "Train Epoch: 371 [164928/225000 (73%)] Loss: 16701.109375\n",
      "Train Epoch: 371 [167424/225000 (74%)] Loss: 16615.345703\n",
      "Train Epoch: 371 [169920/225000 (76%)] Loss: 17161.238281\n",
      "Train Epoch: 371 [172416/225000 (77%)] Loss: 18074.390625\n",
      "Train Epoch: 371 [174912/225000 (78%)] Loss: 17735.400391\n",
      "Train Epoch: 371 [177408/225000 (79%)] Loss: 16151.290039\n",
      "Train Epoch: 371 [179904/225000 (80%)] Loss: 15915.164062\n",
      "Train Epoch: 371 [182400/225000 (81%)] Loss: 16411.914062\n",
      "Train Epoch: 371 [184896/225000 (82%)] Loss: 16671.785156\n",
      "Train Epoch: 371 [187392/225000 (83%)] Loss: 16922.582031\n",
      "Train Epoch: 371 [189888/225000 (84%)] Loss: 16451.867188\n",
      "Train Epoch: 371 [192384/225000 (86%)] Loss: 16414.505859\n",
      "Train Epoch: 371 [194880/225000 (87%)] Loss: 16806.660156\n",
      "Train Epoch: 371 [197376/225000 (88%)] Loss: 16587.041016\n",
      "Train Epoch: 371 [199872/225000 (89%)] Loss: 16832.636719\n",
      "Train Epoch: 371 [202368/225000 (90%)] Loss: 16112.006836\n",
      "Train Epoch: 371 [204864/225000 (91%)] Loss: 17056.166016\n",
      "Train Epoch: 371 [207360/225000 (92%)] Loss: 16395.117188\n",
      "Train Epoch: 371 [209856/225000 (93%)] Loss: 16744.128906\n",
      "Train Epoch: 371 [212352/225000 (94%)] Loss: 17083.103516\n",
      "Train Epoch: 371 [214848/225000 (95%)] Loss: 16924.929688\n",
      "Train Epoch: 371 [217344/225000 (97%)] Loss: 16518.535156\n",
      "Train Epoch: 371 [219840/225000 (98%)] Loss: 16390.488281\n",
      "Train Epoch: 371 [222336/225000 (99%)] Loss: 16641.310547\n",
      "Train Epoch: 371 [224832/225000 (100%)] Loss: 16355.243164\n",
      "    epoch          : 371\n",
      "    loss           : 16733.0388825192\n",
      "    val_loss       : 16719.0644395042\n",
      "Train Epoch: 372 [192/225000 (0%)] Loss: 16506.707031\n",
      "Train Epoch: 372 [2688/225000 (1%)] Loss: 16802.171875\n",
      "Train Epoch: 372 [5184/225000 (2%)] Loss: 16689.050781\n",
      "Train Epoch: 372 [7680/225000 (3%)] Loss: 16696.375000\n",
      "Train Epoch: 372 [10176/225000 (5%)] Loss: 16745.773438\n",
      "Train Epoch: 372 [12672/225000 (6%)] Loss: 16484.603516\n",
      "Train Epoch: 372 [15168/225000 (7%)] Loss: 16892.792969\n",
      "Train Epoch: 372 [17664/225000 (8%)] Loss: 16482.189453\n",
      "Train Epoch: 372 [20160/225000 (9%)] Loss: 16922.789062\n",
      "Train Epoch: 372 [22656/225000 (10%)] Loss: 16651.519531\n",
      "Train Epoch: 372 [25152/225000 (11%)] Loss: 16381.125000\n",
      "Train Epoch: 372 [27648/225000 (12%)] Loss: 16692.023438\n",
      "Train Epoch: 372 [30144/225000 (13%)] Loss: 18107.861328\n",
      "Train Epoch: 372 [32640/225000 (15%)] Loss: 16487.609375\n",
      "Train Epoch: 372 [35136/225000 (16%)] Loss: 16583.320312\n",
      "Train Epoch: 372 [37632/225000 (17%)] Loss: 16814.968750\n",
      "Train Epoch: 372 [40128/225000 (18%)] Loss: 16337.567383\n",
      "Train Epoch: 372 [42624/225000 (19%)] Loss: 17082.128906\n",
      "Train Epoch: 372 [45120/225000 (20%)] Loss: 16372.652344\n",
      "Train Epoch: 372 [47616/225000 (21%)] Loss: 16777.876953\n",
      "Train Epoch: 372 [50112/225000 (22%)] Loss: 16755.019531\n",
      "Train Epoch: 372 [52608/225000 (23%)] Loss: 16303.123047\n",
      "Train Epoch: 372 [55104/225000 (24%)] Loss: 16820.277344\n",
      "Train Epoch: 372 [57600/225000 (26%)] Loss: 16695.101562\n",
      "Train Epoch: 372 [60096/225000 (27%)] Loss: 18478.742188\n",
      "Train Epoch: 372 [62592/225000 (28%)] Loss: 16762.558594\n",
      "Train Epoch: 372 [65088/225000 (29%)] Loss: 16804.699219\n",
      "Train Epoch: 372 [67584/225000 (30%)] Loss: 21944.861328\n",
      "Train Epoch: 372 [70080/225000 (31%)] Loss: 16763.097656\n",
      "Train Epoch: 372 [72576/225000 (32%)] Loss: 16622.759766\n",
      "Train Epoch: 372 [75072/225000 (33%)] Loss: 16804.304688\n",
      "Train Epoch: 372 [77568/225000 (34%)] Loss: 18534.894531\n",
      "Train Epoch: 372 [80064/225000 (36%)] Loss: 16617.943359\n",
      "Train Epoch: 372 [82560/225000 (37%)] Loss: 16204.949219\n",
      "Train Epoch: 372 [85056/225000 (38%)] Loss: 17260.738281\n",
      "Train Epoch: 372 [87552/225000 (39%)] Loss: 17027.437500\n",
      "Train Epoch: 372 [90048/225000 (40%)] Loss: 16478.150391\n",
      "Train Epoch: 372 [92544/225000 (41%)] Loss: 16691.421875\n",
      "Train Epoch: 372 [95040/225000 (42%)] Loss: 16621.169922\n",
      "Train Epoch: 372 [97536/225000 (43%)] Loss: 17099.074219\n",
      "Train Epoch: 372 [100032/225000 (44%)] Loss: 16488.390625\n",
      "Train Epoch: 372 [102528/225000 (46%)] Loss: 17863.941406\n",
      "Train Epoch: 372 [105024/225000 (47%)] Loss: 17368.562500\n",
      "Train Epoch: 372 [107520/225000 (48%)] Loss: 16506.730469\n",
      "Train Epoch: 372 [110016/225000 (49%)] Loss: 17096.089844\n",
      "Train Epoch: 372 [112512/225000 (50%)] Loss: 16678.199219\n",
      "Train Epoch: 372 [115008/225000 (51%)] Loss: 16623.183594\n",
      "Train Epoch: 372 [117504/225000 (52%)] Loss: 17977.636719\n",
      "Train Epoch: 372 [120000/225000 (53%)] Loss: 16769.808594\n",
      "Train Epoch: 372 [122496/225000 (54%)] Loss: 16760.148438\n",
      "Train Epoch: 372 [124992/225000 (56%)] Loss: 16623.718750\n",
      "Train Epoch: 372 [127488/225000 (57%)] Loss: 16394.796875\n",
      "Train Epoch: 372 [129984/225000 (58%)] Loss: 16533.916016\n",
      "Train Epoch: 372 [132480/225000 (59%)] Loss: 16605.517578\n",
      "Train Epoch: 372 [134976/225000 (60%)] Loss: 16681.939453\n",
      "Train Epoch: 372 [137472/225000 (61%)] Loss: 16543.050781\n",
      "Train Epoch: 372 [139968/225000 (62%)] Loss: 16937.742188\n",
      "Train Epoch: 372 [142464/225000 (63%)] Loss: 16708.027344\n",
      "Train Epoch: 372 [144960/225000 (64%)] Loss: 16497.742188\n",
      "Train Epoch: 372 [147456/225000 (66%)] Loss: 17069.232422\n",
      "Train Epoch: 372 [149952/225000 (67%)] Loss: 16239.851562\n",
      "Train Epoch: 372 [152448/225000 (68%)] Loss: 17000.982422\n",
      "Train Epoch: 372 [154944/225000 (69%)] Loss: 16752.757812\n",
      "Train Epoch: 372 [157440/225000 (70%)] Loss: 17876.126953\n",
      "Train Epoch: 372 [159936/225000 (71%)] Loss: 16433.382812\n",
      "Train Epoch: 372 [162432/225000 (72%)] Loss: 16499.710938\n",
      "Train Epoch: 372 [164928/225000 (73%)] Loss: 16551.187500\n",
      "Train Epoch: 372 [167424/225000 (74%)] Loss: 16807.238281\n",
      "Train Epoch: 372 [169920/225000 (76%)] Loss: 16573.519531\n",
      "Train Epoch: 372 [172416/225000 (77%)] Loss: 17005.128906\n",
      "Train Epoch: 372 [174912/225000 (78%)] Loss: 16426.601562\n",
      "Train Epoch: 372 [177408/225000 (79%)] Loss: 16475.984375\n",
      "Train Epoch: 372 [179904/225000 (80%)] Loss: 16804.462891\n",
      "Train Epoch: 372 [182400/225000 (81%)] Loss: 16791.148438\n",
      "Train Epoch: 372 [184896/225000 (82%)] Loss: 16968.761719\n",
      "Train Epoch: 372 [187392/225000 (83%)] Loss: 16350.498047\n",
      "Train Epoch: 372 [189888/225000 (84%)] Loss: 16685.630859\n",
      "Train Epoch: 372 [192384/225000 (86%)] Loss: 16545.152344\n",
      "Train Epoch: 372 [194880/225000 (87%)] Loss: 15999.493164\n",
      "Train Epoch: 372 [197376/225000 (88%)] Loss: 16767.566406\n",
      "Train Epoch: 372 [199872/225000 (89%)] Loss: 17011.339844\n",
      "Train Epoch: 372 [202368/225000 (90%)] Loss: 16776.791016\n",
      "Train Epoch: 372 [204864/225000 (91%)] Loss: 16844.976562\n",
      "Train Epoch: 372 [207360/225000 (92%)] Loss: 16708.824219\n",
      "Train Epoch: 372 [209856/225000 (93%)] Loss: 16437.320312\n",
      "Train Epoch: 372 [212352/225000 (94%)] Loss: 16649.398438\n",
      "Train Epoch: 372 [214848/225000 (95%)] Loss: 16491.359375\n",
      "Train Epoch: 372 [217344/225000 (97%)] Loss: 17069.455078\n",
      "Train Epoch: 372 [219840/225000 (98%)] Loss: 16684.832031\n",
      "Train Epoch: 372 [222336/225000 (99%)] Loss: 16754.207031\n",
      "Train Epoch: 372 [224832/225000 (100%)] Loss: 16839.525391\n",
      "    epoch          : 372\n",
      "    loss           : 16716.97746240401\n",
      "    val_loss       : 16710.61481515324\n",
      "Train Epoch: 373 [192/225000 (0%)] Loss: 16354.962891\n",
      "Train Epoch: 373 [2688/225000 (1%)] Loss: 16309.017578\n",
      "Train Epoch: 373 [5184/225000 (2%)] Loss: 18113.791016\n",
      "Train Epoch: 373 [7680/225000 (3%)] Loss: 16430.843750\n",
      "Train Epoch: 373 [10176/225000 (5%)] Loss: 16328.972656\n",
      "Train Epoch: 373 [12672/225000 (6%)] Loss: 17068.464844\n",
      "Train Epoch: 373 [15168/225000 (7%)] Loss: 16887.652344\n",
      "Train Epoch: 373 [17664/225000 (8%)] Loss: 16988.363281\n",
      "Train Epoch: 373 [20160/225000 (9%)] Loss: 16445.455078\n",
      "Train Epoch: 373 [22656/225000 (10%)] Loss: 16565.927734\n",
      "Train Epoch: 373 [25152/225000 (11%)] Loss: 16749.125000\n",
      "Train Epoch: 373 [27648/225000 (12%)] Loss: 16724.324219\n",
      "Train Epoch: 373 [30144/225000 (13%)] Loss: 16588.632812\n",
      "Train Epoch: 373 [32640/225000 (15%)] Loss: 16855.699219\n",
      "Train Epoch: 373 [35136/225000 (16%)] Loss: 16306.014648\n",
      "Train Epoch: 373 [37632/225000 (17%)] Loss: 16686.253906\n",
      "Train Epoch: 373 [40128/225000 (18%)] Loss: 16895.316406\n",
      "Train Epoch: 373 [42624/225000 (19%)] Loss: 17011.652344\n",
      "Train Epoch: 373 [45120/225000 (20%)] Loss: 16753.015625\n",
      "Train Epoch: 373 [47616/225000 (21%)] Loss: 16109.770508\n",
      "Train Epoch: 373 [50112/225000 (22%)] Loss: 16763.296875\n",
      "Train Epoch: 373 [52608/225000 (23%)] Loss: 16766.599609\n",
      "Train Epoch: 373 [55104/225000 (24%)] Loss: 16161.738281\n",
      "Train Epoch: 373 [57600/225000 (26%)] Loss: 16933.853516\n",
      "Train Epoch: 373 [60096/225000 (27%)] Loss: 16712.003906\n",
      "Train Epoch: 373 [62592/225000 (28%)] Loss: 16662.546875\n",
      "Train Epoch: 373 [65088/225000 (29%)] Loss: 16024.237305\n",
      "Train Epoch: 373 [67584/225000 (30%)] Loss: 16481.779297\n",
      "Train Epoch: 373 [70080/225000 (31%)] Loss: 16820.865234\n",
      "Train Epoch: 373 [72576/225000 (32%)] Loss: 16384.423828\n",
      "Train Epoch: 373 [75072/225000 (33%)] Loss: 16781.726562\n",
      "Train Epoch: 373 [77568/225000 (34%)] Loss: 16967.074219\n",
      "Train Epoch: 373 [80064/225000 (36%)] Loss: 16489.617188\n",
      "Train Epoch: 373 [82560/225000 (37%)] Loss: 16820.648438\n",
      "Train Epoch: 373 [85056/225000 (38%)] Loss: 17076.027344\n",
      "Train Epoch: 373 [87552/225000 (39%)] Loss: 16620.066406\n",
      "Train Epoch: 373 [90048/225000 (40%)] Loss: 16965.548828\n",
      "Train Epoch: 373 [92544/225000 (41%)] Loss: 16182.993164\n",
      "Train Epoch: 373 [95040/225000 (42%)] Loss: 16492.865234\n",
      "Train Epoch: 373 [97536/225000 (43%)] Loss: 16300.519531\n",
      "Train Epoch: 373 [100032/225000 (44%)] Loss: 15913.342773\n",
      "Train Epoch: 373 [102528/225000 (46%)] Loss: 16394.238281\n",
      "Train Epoch: 373 [105024/225000 (47%)] Loss: 16578.369141\n",
      "Train Epoch: 373 [107520/225000 (48%)] Loss: 16555.900391\n",
      "Train Epoch: 373 [110016/225000 (49%)] Loss: 16610.957031\n",
      "Train Epoch: 373 [112512/225000 (50%)] Loss: 16723.304688\n",
      "Train Epoch: 373 [115008/225000 (51%)] Loss: 16493.591797\n",
      "Train Epoch: 373 [117504/225000 (52%)] Loss: 17339.890625\n",
      "Train Epoch: 373 [120000/225000 (53%)] Loss: 16549.257812\n",
      "Train Epoch: 373 [122496/225000 (54%)] Loss: 16246.127930\n",
      "Train Epoch: 373 [124992/225000 (56%)] Loss: 16908.345703\n",
      "Train Epoch: 373 [127488/225000 (57%)] Loss: 15900.879883\n",
      "Train Epoch: 373 [129984/225000 (58%)] Loss: 16745.687500\n",
      "Train Epoch: 373 [132480/225000 (59%)] Loss: 16810.033203\n",
      "Train Epoch: 373 [134976/225000 (60%)] Loss: 18471.847656\n",
      "Train Epoch: 373 [137472/225000 (61%)] Loss: 16673.566406\n",
      "Train Epoch: 373 [139968/225000 (62%)] Loss: 17120.093750\n",
      "Train Epoch: 373 [142464/225000 (63%)] Loss: 16726.882812\n",
      "Train Epoch: 373 [144960/225000 (64%)] Loss: 16216.341797\n",
      "Train Epoch: 373 [147456/225000 (66%)] Loss: 16378.689453\n",
      "Train Epoch: 373 [149952/225000 (67%)] Loss: 16234.701172\n",
      "Train Epoch: 373 [152448/225000 (68%)] Loss: 17094.242188\n",
      "Train Epoch: 373 [154944/225000 (69%)] Loss: 16818.712891\n",
      "Train Epoch: 373 [157440/225000 (70%)] Loss: 16454.875000\n",
      "Train Epoch: 373 [159936/225000 (71%)] Loss: 16127.937500\n",
      "Train Epoch: 373 [162432/225000 (72%)] Loss: 16295.696289\n",
      "Train Epoch: 373 [164928/225000 (73%)] Loss: 16604.265625\n",
      "Train Epoch: 373 [167424/225000 (74%)] Loss: 16310.648438\n",
      "Train Epoch: 373 [169920/225000 (76%)] Loss: 16574.531250\n",
      "Train Epoch: 373 [172416/225000 (77%)] Loss: 16667.679688\n",
      "Train Epoch: 373 [174912/225000 (78%)] Loss: 16498.363281\n",
      "Train Epoch: 373 [177408/225000 (79%)] Loss: 16694.714844\n",
      "Train Epoch: 373 [179904/225000 (80%)] Loss: 16855.695312\n",
      "Train Epoch: 373 [182400/225000 (81%)] Loss: 16435.408203\n",
      "Train Epoch: 373 [184896/225000 (82%)] Loss: 16772.201172\n",
      "Train Epoch: 373 [187392/225000 (83%)] Loss: 17508.445312\n",
      "Train Epoch: 373 [189888/225000 (84%)] Loss: 16607.662109\n",
      "Train Epoch: 373 [192384/225000 (86%)] Loss: 16889.703125\n",
      "Train Epoch: 373 [194880/225000 (87%)] Loss: 16450.294922\n",
      "Train Epoch: 373 [197376/225000 (88%)] Loss: 16554.750000\n",
      "Train Epoch: 373 [199872/225000 (89%)] Loss: 16829.255859\n",
      "Train Epoch: 373 [202368/225000 (90%)] Loss: 16525.335938\n",
      "Train Epoch: 373 [204864/225000 (91%)] Loss: 16413.519531\n",
      "Train Epoch: 373 [207360/225000 (92%)] Loss: 16640.533203\n",
      "Train Epoch: 373 [209856/225000 (93%)] Loss: 16695.501953\n",
      "Train Epoch: 373 [212352/225000 (94%)] Loss: 16515.986328\n",
      "Train Epoch: 373 [214848/225000 (95%)] Loss: 16653.675781\n",
      "Train Epoch: 373 [217344/225000 (97%)] Loss: 17010.515625\n",
      "Train Epoch: 373 [219840/225000 (98%)] Loss: 17788.683594\n",
      "Train Epoch: 373 [222336/225000 (99%)] Loss: 16755.484375\n",
      "Train Epoch: 373 [224832/225000 (100%)] Loss: 16512.722656\n",
      "    epoch          : 373\n",
      "    loss           : 16714.31752696379\n",
      "    val_loss       : 16701.952144218765\n",
      "Train Epoch: 374 [192/225000 (0%)] Loss: 16350.773438\n",
      "Train Epoch: 374 [2688/225000 (1%)] Loss: 16586.009766\n",
      "Train Epoch: 374 [5184/225000 (2%)] Loss: 16497.738281\n",
      "Train Epoch: 374 [7680/225000 (3%)] Loss: 16520.960938\n",
      "Train Epoch: 374 [10176/225000 (5%)] Loss: 16119.501953\n",
      "Train Epoch: 374 [12672/225000 (6%)] Loss: 16254.130859\n",
      "Train Epoch: 374 [15168/225000 (7%)] Loss: 16591.859375\n",
      "Train Epoch: 374 [17664/225000 (8%)] Loss: 17274.814453\n",
      "Train Epoch: 374 [20160/225000 (9%)] Loss: 17101.511719\n",
      "Train Epoch: 374 [22656/225000 (10%)] Loss: 16549.425781\n",
      "Train Epoch: 374 [25152/225000 (11%)] Loss: 16661.609375\n",
      "Train Epoch: 374 [27648/225000 (12%)] Loss: 16372.499023\n",
      "Train Epoch: 374 [30144/225000 (13%)] Loss: 16708.449219\n",
      "Train Epoch: 374 [32640/225000 (15%)] Loss: 16372.374023\n",
      "Train Epoch: 374 [35136/225000 (16%)] Loss: 16505.550781\n",
      "Train Epoch: 374 [37632/225000 (17%)] Loss: 16503.880859\n",
      "Train Epoch: 374 [40128/225000 (18%)] Loss: 16747.349609\n",
      "Train Epoch: 374 [42624/225000 (19%)] Loss: 18215.251953\n",
      "Train Epoch: 374 [45120/225000 (20%)] Loss: 17987.480469\n",
      "Train Epoch: 374 [47616/225000 (21%)] Loss: 16418.257812\n",
      "Train Epoch: 374 [50112/225000 (22%)] Loss: 16269.221680\n",
      "Train Epoch: 374 [52608/225000 (23%)] Loss: 15852.136719\n",
      "Train Epoch: 374 [55104/225000 (24%)] Loss: 16500.718750\n",
      "Train Epoch: 374 [57600/225000 (26%)] Loss: 18740.203125\n",
      "Train Epoch: 374 [60096/225000 (27%)] Loss: 16476.384766\n",
      "Train Epoch: 374 [62592/225000 (28%)] Loss: 16282.557617\n",
      "Train Epoch: 374 [65088/225000 (29%)] Loss: 16875.363281\n",
      "Train Epoch: 374 [67584/225000 (30%)] Loss: 16326.708008\n",
      "Train Epoch: 374 [70080/225000 (31%)] Loss: 17497.078125\n",
      "Train Epoch: 374 [72576/225000 (32%)] Loss: 17026.269531\n",
      "Train Epoch: 374 [75072/225000 (33%)] Loss: 16602.419922\n",
      "Train Epoch: 374 [77568/225000 (34%)] Loss: 16526.449219\n",
      "Train Epoch: 374 [80064/225000 (36%)] Loss: 16858.580078\n",
      "Train Epoch: 374 [82560/225000 (37%)] Loss: 16789.792969\n",
      "Train Epoch: 374 [85056/225000 (38%)] Loss: 16459.822266\n",
      "Train Epoch: 374 [87552/225000 (39%)] Loss: 16844.212891\n",
      "Train Epoch: 374 [90048/225000 (40%)] Loss: 16538.378906\n",
      "Train Epoch: 374 [92544/225000 (41%)] Loss: 16621.216797\n",
      "Train Epoch: 374 [95040/225000 (42%)] Loss: 16335.411133\n",
      "Train Epoch: 374 [97536/225000 (43%)] Loss: 16426.751953\n",
      "Train Epoch: 374 [100032/225000 (44%)] Loss: 16711.050781\n",
      "Train Epoch: 374 [102528/225000 (46%)] Loss: 16693.410156\n",
      "Train Epoch: 374 [105024/225000 (47%)] Loss: 16604.630859\n",
      "Train Epoch: 374 [107520/225000 (48%)] Loss: 16644.714844\n",
      "Train Epoch: 374 [110016/225000 (49%)] Loss: 16603.808594\n",
      "Train Epoch: 374 [112512/225000 (50%)] Loss: 16810.078125\n",
      "Train Epoch: 374 [115008/225000 (51%)] Loss: 16664.531250\n",
      "Train Epoch: 374 [117504/225000 (52%)] Loss: 16877.169922\n",
      "Train Epoch: 374 [120000/225000 (53%)] Loss: 16761.468750\n",
      "Train Epoch: 374 [122496/225000 (54%)] Loss: 16413.609375\n",
      "Train Epoch: 374 [124992/225000 (56%)] Loss: 18226.726562\n",
      "Train Epoch: 374 [127488/225000 (57%)] Loss: 16559.466797\n",
      "Train Epoch: 374 [129984/225000 (58%)] Loss: 16901.683594\n",
      "Train Epoch: 374 [132480/225000 (59%)] Loss: 16788.199219\n",
      "Train Epoch: 374 [134976/225000 (60%)] Loss: 16903.792969\n",
      "Train Epoch: 374 [137472/225000 (61%)] Loss: 16513.707031\n",
      "Train Epoch: 374 [139968/225000 (62%)] Loss: 16322.048828\n",
      "Train Epoch: 374 [142464/225000 (63%)] Loss: 16542.546875\n",
      "Train Epoch: 374 [144960/225000 (64%)] Loss: 16365.362305\n",
      "Train Epoch: 374 [147456/225000 (66%)] Loss: 16717.558594\n",
      "Train Epoch: 374 [149952/225000 (67%)] Loss: 16945.550781\n",
      "Train Epoch: 374 [152448/225000 (68%)] Loss: 16787.408203\n",
      "Train Epoch: 374 [154944/225000 (69%)] Loss: 16847.636719\n",
      "Train Epoch: 374 [157440/225000 (70%)] Loss: 16613.890625\n",
      "Train Epoch: 374 [159936/225000 (71%)] Loss: 16849.746094\n",
      "Train Epoch: 374 [162432/225000 (72%)] Loss: 16965.679688\n",
      "Train Epoch: 374 [164928/225000 (73%)] Loss: 16562.099609\n",
      "Train Epoch: 374 [167424/225000 (74%)] Loss: 16453.019531\n",
      "Train Epoch: 374 [169920/225000 (76%)] Loss: 16194.495117\n",
      "Train Epoch: 374 [172416/225000 (77%)] Loss: 16808.796875\n",
      "Train Epoch: 374 [174912/225000 (78%)] Loss: 16396.736328\n",
      "Train Epoch: 374 [177408/225000 (79%)] Loss: 16166.086914\n",
      "Train Epoch: 374 [179904/225000 (80%)] Loss: 16488.453125\n",
      "Train Epoch: 374 [182400/225000 (81%)] Loss: 16772.326172\n",
      "Train Epoch: 374 [184896/225000 (82%)] Loss: 17110.718750\n",
      "Train Epoch: 374 [187392/225000 (83%)] Loss: 16726.062500\n",
      "Train Epoch: 374 [189888/225000 (84%)] Loss: 16713.078125\n",
      "Train Epoch: 374 [192384/225000 (86%)] Loss: 17189.992188\n",
      "Train Epoch: 374 [194880/225000 (87%)] Loss: 16769.433594\n",
      "Train Epoch: 374 [197376/225000 (88%)] Loss: 17175.527344\n",
      "Train Epoch: 374 [199872/225000 (89%)] Loss: 16662.970703\n",
      "Train Epoch: 374 [202368/225000 (90%)] Loss: 16907.648438\n",
      "Train Epoch: 374 [204864/225000 (91%)] Loss: 17413.164062\n",
      "Train Epoch: 374 [207360/225000 (92%)] Loss: 16468.832031\n",
      "Train Epoch: 374 [209856/225000 (93%)] Loss: 16924.810547\n",
      "Train Epoch: 374 [212352/225000 (94%)] Loss: 16431.226562\n",
      "Train Epoch: 374 [214848/225000 (95%)] Loss: 18211.150391\n",
      "Train Epoch: 374 [217344/225000 (97%)] Loss: 16888.210938\n",
      "Train Epoch: 374 [219840/225000 (98%)] Loss: 16554.593750\n",
      "Train Epoch: 374 [222336/225000 (99%)] Loss: 16356.292969\n",
      "Train Epoch: 374 [224832/225000 (100%)] Loss: 16459.144531\n",
      "    epoch          : 374\n",
      "    loss           : 16698.375923234857\n",
      "    val_loss       : 16689.248139475592\n",
      "Train Epoch: 375 [192/225000 (0%)] Loss: 16498.437500\n",
      "Train Epoch: 375 [2688/225000 (1%)] Loss: 16730.613281\n",
      "Train Epoch: 375 [5184/225000 (2%)] Loss: 16312.178711\n",
      "Train Epoch: 375 [7680/225000 (3%)] Loss: 16421.808594\n",
      "Train Epoch: 375 [10176/225000 (5%)] Loss: 16678.875000\n",
      "Train Epoch: 375 [12672/225000 (6%)] Loss: 17066.273438\n",
      "Train Epoch: 375 [15168/225000 (7%)] Loss: 16708.335938\n",
      "Train Epoch: 375 [17664/225000 (8%)] Loss: 17121.996094\n",
      "Train Epoch: 375 [20160/225000 (9%)] Loss: 16506.265625\n",
      "Train Epoch: 375 [22656/225000 (10%)] Loss: 17014.677734\n",
      "Train Epoch: 375 [25152/225000 (11%)] Loss: 16194.787109\n",
      "Train Epoch: 375 [27648/225000 (12%)] Loss: 16593.765625\n",
      "Train Epoch: 375 [30144/225000 (13%)] Loss: 16666.324219\n",
      "Train Epoch: 375 [32640/225000 (15%)] Loss: 16565.830078\n",
      "Train Epoch: 375 [35136/225000 (16%)] Loss: 16651.306641\n",
      "Train Epoch: 375 [37632/225000 (17%)] Loss: 16494.980469\n",
      "Train Epoch: 375 [40128/225000 (18%)] Loss: 16545.324219\n",
      "Train Epoch: 375 [42624/225000 (19%)] Loss: 16671.093750\n",
      "Train Epoch: 375 [45120/225000 (20%)] Loss: 16655.113281\n",
      "Train Epoch: 375 [47616/225000 (21%)] Loss: 16954.617188\n",
      "Train Epoch: 375 [50112/225000 (22%)] Loss: 17110.169922\n",
      "Train Epoch: 375 [52608/225000 (23%)] Loss: 16346.320312\n",
      "Train Epoch: 375 [55104/225000 (24%)] Loss: 16531.894531\n",
      "Train Epoch: 375 [57600/225000 (26%)] Loss: 17158.390625\n",
      "Train Epoch: 375 [60096/225000 (27%)] Loss: 16629.371094\n",
      "Train Epoch: 375 [62592/225000 (28%)] Loss: 16636.179688\n",
      "Train Epoch: 375 [65088/225000 (29%)] Loss: 17152.726562\n",
      "Train Epoch: 375 [67584/225000 (30%)] Loss: 16806.263672\n",
      "Train Epoch: 375 [70080/225000 (31%)] Loss: 16865.675781\n",
      "Train Epoch: 375 [72576/225000 (32%)] Loss: 16805.121094\n",
      "Train Epoch: 375 [75072/225000 (33%)] Loss: 16366.711914\n",
      "Train Epoch: 375 [77568/225000 (34%)] Loss: 16995.041016\n",
      "Train Epoch: 375 [80064/225000 (36%)] Loss: 16715.701172\n",
      "Train Epoch: 375 [82560/225000 (37%)] Loss: 16577.412109\n",
      "Train Epoch: 375 [85056/225000 (38%)] Loss: 16559.804688\n",
      "Train Epoch: 375 [87552/225000 (39%)] Loss: 16535.011719\n",
      "Train Epoch: 375 [90048/225000 (40%)] Loss: 16767.640625\n",
      "Train Epoch: 375 [92544/225000 (41%)] Loss: 16103.739258\n",
      "Train Epoch: 375 [95040/225000 (42%)] Loss: 17001.142578\n",
      "Train Epoch: 375 [97536/225000 (43%)] Loss: 16679.671875\n",
      "Train Epoch: 375 [100032/225000 (44%)] Loss: 16902.181641\n",
      "Train Epoch: 375 [102528/225000 (46%)] Loss: 16583.140625\n",
      "Train Epoch: 375 [105024/225000 (47%)] Loss: 16582.142578\n",
      "Train Epoch: 375 [107520/225000 (48%)] Loss: 16636.078125\n",
      "Train Epoch: 375 [110016/225000 (49%)] Loss: 16709.816406\n",
      "Train Epoch: 375 [112512/225000 (50%)] Loss: 16359.458008\n",
      "Train Epoch: 375 [115008/225000 (51%)] Loss: 16740.837891\n",
      "Train Epoch: 375 [117504/225000 (52%)] Loss: 16975.925781\n",
      "Train Epoch: 375 [120000/225000 (53%)] Loss: 16557.335938\n",
      "Train Epoch: 375 [122496/225000 (54%)] Loss: 16706.283203\n",
      "Train Epoch: 375 [124992/225000 (56%)] Loss: 16438.267578\n",
      "Train Epoch: 375 [127488/225000 (57%)] Loss: 16627.050781\n",
      "Train Epoch: 375 [129984/225000 (58%)] Loss: 16584.121094\n",
      "Train Epoch: 375 [132480/225000 (59%)] Loss: 16863.781250\n",
      "Train Epoch: 375 [134976/225000 (60%)] Loss: 16465.486328\n",
      "Train Epoch: 375 [137472/225000 (61%)] Loss: 16704.279297\n",
      "Train Epoch: 375 [139968/225000 (62%)] Loss: 16661.867188\n",
      "Train Epoch: 375 [142464/225000 (63%)] Loss: 16788.597656\n",
      "Train Epoch: 375 [144960/225000 (64%)] Loss: 16387.265625\n",
      "Train Epoch: 375 [147456/225000 (66%)] Loss: 18142.593750\n",
      "Train Epoch: 375 [149952/225000 (67%)] Loss: 17195.957031\n",
      "Train Epoch: 375 [152448/225000 (68%)] Loss: 16729.359375\n",
      "Train Epoch: 375 [154944/225000 (69%)] Loss: 16980.841797\n",
      "Train Epoch: 375 [157440/225000 (70%)] Loss: 16571.007812\n",
      "Train Epoch: 375 [159936/225000 (71%)] Loss: 16505.703125\n",
      "Train Epoch: 375 [162432/225000 (72%)] Loss: 17977.351562\n",
      "Train Epoch: 375 [164928/225000 (73%)] Loss: 16487.656250\n",
      "Train Epoch: 375 [167424/225000 (74%)] Loss: 16654.207031\n",
      "Train Epoch: 375 [169920/225000 (76%)] Loss: 16829.404297\n",
      "Train Epoch: 375 [172416/225000 (77%)] Loss: 16608.511719\n",
      "Train Epoch: 375 [174912/225000 (78%)] Loss: 16871.828125\n",
      "Train Epoch: 375 [177408/225000 (79%)] Loss: 16481.683594\n",
      "Train Epoch: 375 [179904/225000 (80%)] Loss: 16725.818359\n",
      "Train Epoch: 375 [182400/225000 (81%)] Loss: 16448.464844\n",
      "Train Epoch: 375 [184896/225000 (82%)] Loss: 17008.054688\n",
      "Train Epoch: 375 [187392/225000 (83%)] Loss: 16693.378906\n",
      "Train Epoch: 375 [189888/225000 (84%)] Loss: 16327.845703\n",
      "Train Epoch: 375 [192384/225000 (86%)] Loss: 16766.570312\n",
      "Train Epoch: 375 [194880/225000 (87%)] Loss: 16678.292969\n",
      "Train Epoch: 375 [197376/225000 (88%)] Loss: 16518.511719\n",
      "Train Epoch: 375 [199872/225000 (89%)] Loss: 17059.429688\n",
      "Train Epoch: 375 [202368/225000 (90%)] Loss: 16465.187500\n",
      "Train Epoch: 375 [204864/225000 (91%)] Loss: 16659.457031\n",
      "Train Epoch: 375 [207360/225000 (92%)] Loss: 16970.609375\n",
      "Train Epoch: 375 [209856/225000 (93%)] Loss: 16604.457031\n",
      "Train Epoch: 375 [212352/225000 (94%)] Loss: 16774.359375\n",
      "Train Epoch: 375 [214848/225000 (95%)] Loss: 16957.734375\n",
      "Train Epoch: 375 [217344/225000 (97%)] Loss: 17125.101562\n",
      "Train Epoch: 375 [219840/225000 (98%)] Loss: 16456.347656\n",
      "Train Epoch: 375 [222336/225000 (99%)] Loss: 16755.812500\n",
      "Train Epoch: 375 [224832/225000 (100%)] Loss: 16432.791016\n",
      "    epoch          : 375\n",
      "    loss           : 16710.787439339805\n",
      "    val_loss       : 16635.949194381254\n",
      "Train Epoch: 376 [192/225000 (0%)] Loss: 16574.083984\n",
      "Train Epoch: 376 [2688/225000 (1%)] Loss: 16657.359375\n",
      "Train Epoch: 376 [5184/225000 (2%)] Loss: 16596.417969\n",
      "Train Epoch: 376 [7680/225000 (3%)] Loss: 18126.683594\n",
      "Train Epoch: 376 [10176/225000 (5%)] Loss: 16281.543945\n",
      "Train Epoch: 376 [12672/225000 (6%)] Loss: 16818.328125\n",
      "Train Epoch: 376 [15168/225000 (7%)] Loss: 16702.802734\n",
      "Train Epoch: 376 [17664/225000 (8%)] Loss: 16522.546875\n",
      "Train Epoch: 376 [20160/225000 (9%)] Loss: 16669.781250\n",
      "Train Epoch: 376 [22656/225000 (10%)] Loss: 16733.359375\n",
      "Train Epoch: 376 [25152/225000 (11%)] Loss: 16676.189453\n",
      "Train Epoch: 376 [27648/225000 (12%)] Loss: 17397.230469\n",
      "Train Epoch: 376 [30144/225000 (13%)] Loss: 16447.085938\n",
      "Train Epoch: 376 [32640/225000 (15%)] Loss: 16305.356445\n",
      "Train Epoch: 376 [35136/225000 (16%)] Loss: 16312.180664\n",
      "Train Epoch: 376 [37632/225000 (17%)] Loss: 16394.625000\n",
      "Train Epoch: 376 [40128/225000 (18%)] Loss: 16255.642578\n",
      "Train Epoch: 376 [42624/225000 (19%)] Loss: 16759.542969\n",
      "Train Epoch: 376 [45120/225000 (20%)] Loss: 16721.214844\n",
      "Train Epoch: 376 [47616/225000 (21%)] Loss: 18043.757812\n",
      "Train Epoch: 376 [50112/225000 (22%)] Loss: 16658.937500\n",
      "Train Epoch: 376 [52608/225000 (23%)] Loss: 16497.806641\n",
      "Train Epoch: 376 [55104/225000 (24%)] Loss: 16515.421875\n",
      "Train Epoch: 376 [57600/225000 (26%)] Loss: 16696.667969\n",
      "Train Epoch: 376 [60096/225000 (27%)] Loss: 16886.894531\n",
      "Train Epoch: 376 [62592/225000 (28%)] Loss: 16083.571289\n",
      "Train Epoch: 376 [65088/225000 (29%)] Loss: 16886.496094\n",
      "Train Epoch: 376 [67584/225000 (30%)] Loss: 16623.609375\n",
      "Train Epoch: 376 [70080/225000 (31%)] Loss: 17041.742188\n",
      "Train Epoch: 376 [72576/225000 (32%)] Loss: 16939.226562\n",
      "Train Epoch: 376 [75072/225000 (33%)] Loss: 16808.408203\n",
      "Train Epoch: 376 [77568/225000 (34%)] Loss: 16613.087891\n",
      "Train Epoch: 376 [80064/225000 (36%)] Loss: 16608.355469\n",
      "Train Epoch: 376 [82560/225000 (37%)] Loss: 16087.694336\n",
      "Train Epoch: 376 [85056/225000 (38%)] Loss: 16245.359375\n",
      "Train Epoch: 376 [87552/225000 (39%)] Loss: 17011.076172\n",
      "Train Epoch: 376 [90048/225000 (40%)] Loss: 16683.175781\n",
      "Train Epoch: 376 [92544/225000 (41%)] Loss: 16754.216797\n",
      "Train Epoch: 376 [95040/225000 (42%)] Loss: 16515.955078\n",
      "Train Epoch: 376 [97536/225000 (43%)] Loss: 16873.712891\n",
      "Train Epoch: 376 [100032/225000 (44%)] Loss: 16548.601562\n",
      "Train Epoch: 376 [102528/225000 (46%)] Loss: 16433.023438\n",
      "Train Epoch: 376 [105024/225000 (47%)] Loss: 16704.794922\n",
      "Train Epoch: 376 [107520/225000 (48%)] Loss: 16268.356445\n",
      "Train Epoch: 376 [110016/225000 (49%)] Loss: 16440.945312\n",
      "Train Epoch: 376 [112512/225000 (50%)] Loss: 16433.246094\n",
      "Train Epoch: 376 [115008/225000 (51%)] Loss: 16982.134766\n",
      "Train Epoch: 376 [117504/225000 (52%)] Loss: 16602.824219\n",
      "Train Epoch: 376 [120000/225000 (53%)] Loss: 16249.088867\n",
      "Train Epoch: 376 [122496/225000 (54%)] Loss: 16578.785156\n",
      "Train Epoch: 376 [124992/225000 (56%)] Loss: 16490.035156\n",
      "Train Epoch: 376 [127488/225000 (57%)] Loss: 16665.339844\n",
      "Train Epoch: 376 [129984/225000 (58%)] Loss: 16085.540039\n",
      "Train Epoch: 376 [132480/225000 (59%)] Loss: 16511.781250\n",
      "Train Epoch: 376 [134976/225000 (60%)] Loss: 16708.617188\n",
      "Train Epoch: 376 [137472/225000 (61%)] Loss: 16647.210938\n",
      "Train Epoch: 376 [139968/225000 (62%)] Loss: 18071.121094\n",
      "Train Epoch: 376 [142464/225000 (63%)] Loss: 16802.363281\n",
      "Train Epoch: 376 [144960/225000 (64%)] Loss: 16419.953125\n",
      "Train Epoch: 376 [147456/225000 (66%)] Loss: 16451.457031\n",
      "Train Epoch: 376 [149952/225000 (67%)] Loss: 16002.601562\n",
      "Train Epoch: 376 [152448/225000 (68%)] Loss: 16849.623047\n",
      "Train Epoch: 376 [154944/225000 (69%)] Loss: 16567.234375\n",
      "Train Epoch: 376 [157440/225000 (70%)] Loss: 17071.062500\n",
      "Train Epoch: 376 [159936/225000 (71%)] Loss: 16614.511719\n",
      "Train Epoch: 376 [162432/225000 (72%)] Loss: 16870.591797\n",
      "Train Epoch: 376 [164928/225000 (73%)] Loss: 16673.031250\n",
      "Train Epoch: 376 [167424/225000 (74%)] Loss: 16367.661133\n",
      "Train Epoch: 376 [169920/225000 (76%)] Loss: 16706.035156\n",
      "Train Epoch: 376 [172416/225000 (77%)] Loss: 16519.421875\n",
      "Train Epoch: 376 [174912/225000 (78%)] Loss: 16533.253906\n",
      "Train Epoch: 376 [177408/225000 (79%)] Loss: 17002.632812\n",
      "Train Epoch: 376 [179904/225000 (80%)] Loss: 16655.244141\n",
      "Train Epoch: 376 [182400/225000 (81%)] Loss: 18090.941406\n",
      "Train Epoch: 376 [184896/225000 (82%)] Loss: 16929.136719\n",
      "Train Epoch: 376 [187392/225000 (83%)] Loss: 16407.085938\n",
      "Train Epoch: 376 [189888/225000 (84%)] Loss: 16268.466797\n",
      "Train Epoch: 376 [192384/225000 (86%)] Loss: 16622.673828\n",
      "Train Epoch: 376 [194880/225000 (87%)] Loss: 16521.416016\n",
      "Train Epoch: 376 [197376/225000 (88%)] Loss: 16452.341797\n",
      "Train Epoch: 376 [199872/225000 (89%)] Loss: 16958.511719\n",
      "Train Epoch: 376 [202368/225000 (90%)] Loss: 16579.128906\n",
      "Train Epoch: 376 [204864/225000 (91%)] Loss: 16343.733398\n",
      "Train Epoch: 376 [207360/225000 (92%)] Loss: 17277.562500\n",
      "Train Epoch: 376 [209856/225000 (93%)] Loss: 16683.925781\n",
      "Train Epoch: 376 [212352/225000 (94%)] Loss: 16672.675781\n",
      "Train Epoch: 376 [214848/225000 (95%)] Loss: 16454.261719\n",
      "Train Epoch: 376 [217344/225000 (97%)] Loss: 16540.912109\n",
      "Train Epoch: 376 [219840/225000 (98%)] Loss: 16481.632812\n",
      "Train Epoch: 376 [222336/225000 (99%)] Loss: 16297.791992\n",
      "Train Epoch: 376 [224832/225000 (100%)] Loss: 16676.968750\n",
      "    epoch          : 376\n",
      "    loss           : 16715.791133112467\n",
      "    val_loss       : 16637.355738428712\n",
      "Train Epoch: 377 [192/225000 (0%)] Loss: 16849.832031\n",
      "Train Epoch: 377 [2688/225000 (1%)] Loss: 16780.121094\n",
      "Train Epoch: 377 [5184/225000 (2%)] Loss: 16548.332031\n",
      "Train Epoch: 377 [7680/225000 (3%)] Loss: 16664.798828\n",
      "Train Epoch: 377 [10176/225000 (5%)] Loss: 16983.765625\n",
      "Train Epoch: 377 [12672/225000 (6%)] Loss: 17111.337891\n",
      "Train Epoch: 377 [15168/225000 (7%)] Loss: 16522.453125\n",
      "Train Epoch: 377 [17664/225000 (8%)] Loss: 16767.076172\n",
      "Train Epoch: 377 [20160/225000 (9%)] Loss: 16922.046875\n",
      "Train Epoch: 377 [22656/225000 (10%)] Loss: 16838.542969\n",
      "Train Epoch: 377 [25152/225000 (11%)] Loss: 16705.845703\n",
      "Train Epoch: 377 [27648/225000 (12%)] Loss: 16423.046875\n",
      "Train Epoch: 377 [30144/225000 (13%)] Loss: 16375.544922\n",
      "Train Epoch: 377 [32640/225000 (15%)] Loss: 16287.017578\n",
      "Train Epoch: 377 [35136/225000 (16%)] Loss: 16566.109375\n",
      "Train Epoch: 377 [37632/225000 (17%)] Loss: 16789.945312\n",
      "Train Epoch: 377 [40128/225000 (18%)] Loss: 16427.136719\n",
      "Train Epoch: 377 [42624/225000 (19%)] Loss: 16493.824219\n",
      "Train Epoch: 377 [45120/225000 (20%)] Loss: 16988.902344\n",
      "Train Epoch: 377 [47616/225000 (21%)] Loss: 16339.206055\n",
      "Train Epoch: 377 [50112/225000 (22%)] Loss: 16674.042969\n",
      "Train Epoch: 377 [52608/225000 (23%)] Loss: 17102.121094\n",
      "Train Epoch: 377 [55104/225000 (24%)] Loss: 16374.529297\n",
      "Train Epoch: 377 [57600/225000 (26%)] Loss: 16723.935547\n",
      "Train Epoch: 377 [60096/225000 (27%)] Loss: 17219.037109\n",
      "Train Epoch: 377 [62592/225000 (28%)] Loss: 16666.894531\n",
      "Train Epoch: 377 [65088/225000 (29%)] Loss: 16648.207031\n",
      "Train Epoch: 377 [67584/225000 (30%)] Loss: 15958.869141\n",
      "Train Epoch: 377 [70080/225000 (31%)] Loss: 16470.167969\n",
      "Train Epoch: 377 [72576/225000 (32%)] Loss: 16722.542969\n",
      "Train Epoch: 377 [75072/225000 (33%)] Loss: 17014.511719\n",
      "Train Epoch: 377 [77568/225000 (34%)] Loss: 16458.644531\n",
      "Train Epoch: 377 [80064/225000 (36%)] Loss: 17064.375000\n",
      "Train Epoch: 377 [82560/225000 (37%)] Loss: 16218.973633\n",
      "Train Epoch: 377 [85056/225000 (38%)] Loss: 16306.321289\n",
      "Train Epoch: 377 [87552/225000 (39%)] Loss: 16912.511719\n",
      "Train Epoch: 377 [90048/225000 (40%)] Loss: 16399.128906\n",
      "Train Epoch: 377 [92544/225000 (41%)] Loss: 16517.261719\n",
      "Train Epoch: 377 [95040/225000 (42%)] Loss: 16064.739258\n",
      "Train Epoch: 377 [97536/225000 (43%)] Loss: 16543.023438\n",
      "Train Epoch: 377 [100032/225000 (44%)] Loss: 16722.429688\n",
      "Train Epoch: 377 [102528/225000 (46%)] Loss: 16449.628906\n",
      "Train Epoch: 377 [105024/225000 (47%)] Loss: 16644.957031\n",
      "Train Epoch: 377 [107520/225000 (48%)] Loss: 17103.826172\n",
      "Train Epoch: 377 [110016/225000 (49%)] Loss: 17779.917969\n",
      "Train Epoch: 377 [112512/225000 (50%)] Loss: 16327.477539\n",
      "Train Epoch: 377 [115008/225000 (51%)] Loss: 16732.699219\n",
      "Train Epoch: 377 [117504/225000 (52%)] Loss: 16752.441406\n",
      "Train Epoch: 377 [120000/225000 (53%)] Loss: 16752.632812\n",
      "Train Epoch: 377 [122496/225000 (54%)] Loss: 16612.810547\n",
      "Train Epoch: 377 [124992/225000 (56%)] Loss: 16595.335938\n",
      "Train Epoch: 377 [127488/225000 (57%)] Loss: 16386.876953\n",
      "Train Epoch: 377 [129984/225000 (58%)] Loss: 16845.156250\n",
      "Train Epoch: 377 [132480/225000 (59%)] Loss: 16067.191406\n",
      "Train Epoch: 377 [134976/225000 (60%)] Loss: 16724.417969\n",
      "Train Epoch: 377 [137472/225000 (61%)] Loss: 16635.443359\n",
      "Train Epoch: 377 [139968/225000 (62%)] Loss: 16356.820312\n",
      "Train Epoch: 377 [142464/225000 (63%)] Loss: 16882.375000\n",
      "Train Epoch: 377 [144960/225000 (64%)] Loss: 16538.242188\n",
      "Train Epoch: 377 [147456/225000 (66%)] Loss: 16727.257812\n",
      "Train Epoch: 377 [149952/225000 (67%)] Loss: 16515.679688\n",
      "Train Epoch: 377 [152448/225000 (68%)] Loss: 17194.792969\n",
      "Train Epoch: 377 [154944/225000 (69%)] Loss: 18062.054688\n",
      "Train Epoch: 377 [157440/225000 (70%)] Loss: 16738.632812\n",
      "Train Epoch: 377 [159936/225000 (71%)] Loss: 16320.878906\n",
      "Train Epoch: 377 [162432/225000 (72%)] Loss: 16581.359375\n",
      "Train Epoch: 377 [164928/225000 (73%)] Loss: 18403.810547\n",
      "Train Epoch: 377 [167424/225000 (74%)] Loss: 16432.527344\n",
      "Train Epoch: 377 [169920/225000 (76%)] Loss: 16968.322266\n",
      "Train Epoch: 377 [172416/225000 (77%)] Loss: 16683.599609\n",
      "Train Epoch: 377 [174912/225000 (78%)] Loss: 16729.220703\n",
      "Train Epoch: 377 [177408/225000 (79%)] Loss: 17031.976562\n",
      "Train Epoch: 377 [179904/225000 (80%)] Loss: 16543.484375\n",
      "Train Epoch: 377 [182400/225000 (81%)] Loss: 16486.246094\n",
      "Train Epoch: 377 [184896/225000 (82%)] Loss: 16884.667969\n",
      "Train Epoch: 377 [187392/225000 (83%)] Loss: 16725.537109\n",
      "Train Epoch: 377 [189888/225000 (84%)] Loss: 16951.882812\n",
      "Train Epoch: 377 [192384/225000 (86%)] Loss: 16556.785156\n",
      "Train Epoch: 377 [194880/225000 (87%)] Loss: 16254.999023\n",
      "Train Epoch: 377 [197376/225000 (88%)] Loss: 16644.718750\n",
      "Train Epoch: 377 [199872/225000 (89%)] Loss: 16526.417969\n",
      "Train Epoch: 377 [202368/225000 (90%)] Loss: 16398.029297\n",
      "Train Epoch: 377 [204864/225000 (91%)] Loss: 16547.250000\n",
      "Train Epoch: 377 [207360/225000 (92%)] Loss: 17179.458984\n",
      "Train Epoch: 377 [209856/225000 (93%)] Loss: 18602.648438\n",
      "Train Epoch: 377 [212352/225000 (94%)] Loss: 16605.769531\n",
      "Train Epoch: 377 [214848/225000 (95%)] Loss: 16478.820312\n",
      "Train Epoch: 377 [217344/225000 (97%)] Loss: 16725.171875\n",
      "Train Epoch: 377 [219840/225000 (98%)] Loss: 17078.914062\n",
      "Train Epoch: 377 [222336/225000 (99%)] Loss: 17926.156250\n",
      "Train Epoch: 377 [224832/225000 (100%)] Loss: 16598.210938\n",
      "    epoch          : 377\n",
      "    loss           : 16696.249991667555\n",
      "    val_loss       : 16672.590508040583\n",
      "Train Epoch: 378 [192/225000 (0%)] Loss: 16484.660156\n",
      "Train Epoch: 378 [2688/225000 (1%)] Loss: 16679.140625\n",
      "Train Epoch: 378 [5184/225000 (2%)] Loss: 16648.923828\n",
      "Train Epoch: 378 [7680/225000 (3%)] Loss: 16966.535156\n",
      "Train Epoch: 378 [10176/225000 (5%)] Loss: 16592.718750\n",
      "Train Epoch: 378 [12672/225000 (6%)] Loss: 16667.773438\n",
      "Train Epoch: 378 [15168/225000 (7%)] Loss: 16730.996094\n",
      "Train Epoch: 378 [17664/225000 (8%)] Loss: 16790.355469\n",
      "Train Epoch: 378 [20160/225000 (9%)] Loss: 16442.884766\n",
      "Train Epoch: 378 [22656/225000 (10%)] Loss: 16838.132812\n",
      "Train Epoch: 378 [25152/225000 (11%)] Loss: 16574.228516\n",
      "Train Epoch: 378 [27648/225000 (12%)] Loss: 18173.794922\n",
      "Train Epoch: 378 [30144/225000 (13%)] Loss: 16579.078125\n",
      "Train Epoch: 378 [32640/225000 (15%)] Loss: 16596.976562\n",
      "Train Epoch: 378 [35136/225000 (16%)] Loss: 16583.300781\n",
      "Train Epoch: 378 [37632/225000 (17%)] Loss: 16672.656250\n",
      "Train Epoch: 378 [40128/225000 (18%)] Loss: 16862.689453\n",
      "Train Epoch: 378 [42624/225000 (19%)] Loss: 16539.294922\n",
      "Train Epoch: 378 [45120/225000 (20%)] Loss: 16405.417969\n",
      "Train Epoch: 378 [47616/225000 (21%)] Loss: 16382.365234\n",
      "Train Epoch: 378 [50112/225000 (22%)] Loss: 16728.898438\n",
      "Train Epoch: 378 [52608/225000 (23%)] Loss: 16617.343750\n",
      "Train Epoch: 378 [55104/225000 (24%)] Loss: 18002.751953\n",
      "Train Epoch: 378 [57600/225000 (26%)] Loss: 16631.222656\n",
      "Train Epoch: 378 [60096/225000 (27%)] Loss: 16692.441406\n",
      "Train Epoch: 378 [62592/225000 (28%)] Loss: 16133.490234\n",
      "Train Epoch: 378 [65088/225000 (29%)] Loss: 16286.375977\n",
      "Train Epoch: 378 [67584/225000 (30%)] Loss: 16318.444336\n",
      "Train Epoch: 378 [70080/225000 (31%)] Loss: 16895.308594\n",
      "Train Epoch: 378 [72576/225000 (32%)] Loss: 16751.925781\n",
      "Train Epoch: 378 [75072/225000 (33%)] Loss: 16881.250000\n",
      "Train Epoch: 378 [77568/225000 (34%)] Loss: 16476.542969\n",
      "Train Epoch: 378 [80064/225000 (36%)] Loss: 16519.347656\n",
      "Train Epoch: 378 [82560/225000 (37%)] Loss: 17220.113281\n",
      "Train Epoch: 378 [85056/225000 (38%)] Loss: 16564.535156\n",
      "Train Epoch: 378 [87552/225000 (39%)] Loss: 16677.382812\n",
      "Train Epoch: 378 [90048/225000 (40%)] Loss: 18441.621094\n",
      "Train Epoch: 378 [92544/225000 (41%)] Loss: 17104.738281\n",
      "Train Epoch: 378 [95040/225000 (42%)] Loss: 16469.000000\n",
      "Train Epoch: 378 [97536/225000 (43%)] Loss: 16224.341797\n",
      "Train Epoch: 378 [100032/225000 (44%)] Loss: 16737.136719\n",
      "Train Epoch: 378 [102528/225000 (46%)] Loss: 16436.318359\n",
      "Train Epoch: 378 [105024/225000 (47%)] Loss: 16327.650391\n",
      "Train Epoch: 378 [107520/225000 (48%)] Loss: 16725.769531\n",
      "Train Epoch: 378 [110016/225000 (49%)] Loss: 16994.914062\n",
      "Train Epoch: 378 [112512/225000 (50%)] Loss: 17073.363281\n",
      "Train Epoch: 378 [115008/225000 (51%)] Loss: 16748.699219\n",
      "Train Epoch: 378 [117504/225000 (52%)] Loss: 16559.884766\n",
      "Train Epoch: 378 [120000/225000 (53%)] Loss: 17072.429688\n",
      "Train Epoch: 378 [122496/225000 (54%)] Loss: 16262.487305\n",
      "Train Epoch: 378 [124992/225000 (56%)] Loss: 16432.984375\n",
      "Train Epoch: 378 [127488/225000 (57%)] Loss: 16490.001953\n",
      "Train Epoch: 378 [129984/225000 (58%)] Loss: 16266.022461\n",
      "Train Epoch: 378 [132480/225000 (59%)] Loss: 17019.042969\n",
      "Train Epoch: 378 [134976/225000 (60%)] Loss: 16424.240234\n",
      "Train Epoch: 378 [137472/225000 (61%)] Loss: 16282.636719\n",
      "Train Epoch: 378 [139968/225000 (62%)] Loss: 16552.273438\n",
      "Train Epoch: 378 [142464/225000 (63%)] Loss: 16417.191406\n",
      "Train Epoch: 378 [144960/225000 (64%)] Loss: 16461.126953\n",
      "Train Epoch: 378 [147456/225000 (66%)] Loss: 16693.617188\n",
      "Train Epoch: 378 [149952/225000 (67%)] Loss: 16398.080078\n",
      "Train Epoch: 378 [152448/225000 (68%)] Loss: 16062.481445\n",
      "Train Epoch: 378 [154944/225000 (69%)] Loss: 16356.175781\n",
      "Train Epoch: 378 [157440/225000 (70%)] Loss: 16649.710938\n",
      "Train Epoch: 378 [159936/225000 (71%)] Loss: 16647.162109\n",
      "Train Epoch: 378 [162432/225000 (72%)] Loss: 16652.808594\n",
      "Train Epoch: 378 [164928/225000 (73%)] Loss: 16524.304688\n",
      "Train Epoch: 378 [167424/225000 (74%)] Loss: 16632.802734\n",
      "Train Epoch: 378 [169920/225000 (76%)] Loss: 16736.457031\n",
      "Train Epoch: 378 [172416/225000 (77%)] Loss: 16509.593750\n",
      "Train Epoch: 378 [174912/225000 (78%)] Loss: 16312.463867\n",
      "Train Epoch: 378 [177408/225000 (79%)] Loss: 17069.083984\n",
      "Train Epoch: 378 [179904/225000 (80%)] Loss: 16775.152344\n",
      "Train Epoch: 378 [182400/225000 (81%)] Loss: 16454.539062\n",
      "Train Epoch: 378 [184896/225000 (82%)] Loss: 16545.351562\n",
      "Train Epoch: 378 [187392/225000 (83%)] Loss: 16700.156250\n",
      "Train Epoch: 378 [189888/225000 (84%)] Loss: 17915.726562\n",
      "Train Epoch: 378 [192384/225000 (86%)] Loss: 16460.179688\n",
      "Train Epoch: 378 [194880/225000 (87%)] Loss: 16701.421875\n",
      "Train Epoch: 378 [197376/225000 (88%)] Loss: 16925.679688\n",
      "Train Epoch: 378 [199872/225000 (89%)] Loss: 16430.371094\n",
      "Train Epoch: 378 [202368/225000 (90%)] Loss: 16810.800781\n",
      "Train Epoch: 378 [204864/225000 (91%)] Loss: 16785.695312\n",
      "Train Epoch: 378 [207360/225000 (92%)] Loss: 16495.000000\n",
      "Train Epoch: 378 [209856/225000 (93%)] Loss: 16514.664062\n",
      "Train Epoch: 378 [212352/225000 (94%)] Loss: 16479.207031\n",
      "Train Epoch: 378 [214848/225000 (95%)] Loss: 16427.875000\n",
      "Train Epoch: 378 [217344/225000 (97%)] Loss: 16649.837891\n",
      "Train Epoch: 378 [219840/225000 (98%)] Loss: 17051.128906\n",
      "Train Epoch: 378 [222336/225000 (99%)] Loss: 16774.898438\n",
      "Train Epoch: 378 [224832/225000 (100%)] Loss: 16828.494141\n",
      "    epoch          : 378\n",
      "    loss           : 16715.165373193526\n",
      "    val_loss       : 16652.550878135302\n",
      "Train Epoch: 379 [192/225000 (0%)] Loss: 16771.730469\n",
      "Train Epoch: 379 [2688/225000 (1%)] Loss: 16361.001953\n",
      "Train Epoch: 379 [5184/225000 (2%)] Loss: 16626.097656\n",
      "Train Epoch: 379 [7680/225000 (3%)] Loss: 16822.625000\n",
      "Train Epoch: 379 [10176/225000 (5%)] Loss: 16876.519531\n",
      "Train Epoch: 379 [12672/225000 (6%)] Loss: 16569.435547\n",
      "Train Epoch: 379 [15168/225000 (7%)] Loss: 17811.119141\n",
      "Train Epoch: 379 [17664/225000 (8%)] Loss: 17165.171875\n",
      "Train Epoch: 379 [20160/225000 (9%)] Loss: 16502.679688\n",
      "Train Epoch: 379 [22656/225000 (10%)] Loss: 16714.105469\n",
      "Train Epoch: 379 [25152/225000 (11%)] Loss: 16562.210938\n",
      "Train Epoch: 379 [27648/225000 (12%)] Loss: 16569.042969\n",
      "Train Epoch: 379 [30144/225000 (13%)] Loss: 16660.605469\n",
      "Train Epoch: 379 [32640/225000 (15%)] Loss: 16015.192383\n",
      "Train Epoch: 379 [35136/225000 (16%)] Loss: 16543.976562\n",
      "Train Epoch: 379 [37632/225000 (17%)] Loss: 16729.636719\n",
      "Train Epoch: 379 [40128/225000 (18%)] Loss: 18053.390625\n",
      "Train Epoch: 379 [42624/225000 (19%)] Loss: 16826.042969\n",
      "Train Epoch: 379 [45120/225000 (20%)] Loss: 16286.007812\n",
      "Train Epoch: 379 [47616/225000 (21%)] Loss: 16851.148438\n",
      "Train Epoch: 379 [50112/225000 (22%)] Loss: 16604.636719\n",
      "Train Epoch: 379 [52608/225000 (23%)] Loss: 16896.605469\n",
      "Train Epoch: 379 [55104/225000 (24%)] Loss: 16511.808594\n",
      "Train Epoch: 379 [57600/225000 (26%)] Loss: 16781.824219\n",
      "Train Epoch: 379 [60096/225000 (27%)] Loss: 16291.673828\n",
      "Train Epoch: 379 [62592/225000 (28%)] Loss: 16432.478516\n",
      "Train Epoch: 379 [65088/225000 (29%)] Loss: 16331.389648\n",
      "Train Epoch: 379 [67584/225000 (30%)] Loss: 16956.101562\n",
      "Train Epoch: 379 [70080/225000 (31%)] Loss: 17130.980469\n",
      "Train Epoch: 379 [72576/225000 (32%)] Loss: 16156.048828\n",
      "Train Epoch: 379 [75072/225000 (33%)] Loss: 16656.371094\n",
      "Train Epoch: 379 [77568/225000 (34%)] Loss: 16528.085938\n",
      "Train Epoch: 379 [80064/225000 (36%)] Loss: 16725.455078\n",
      "Train Epoch: 379 [82560/225000 (37%)] Loss: 17054.423828\n",
      "Train Epoch: 379 [85056/225000 (38%)] Loss: 16872.789062\n",
      "Train Epoch: 379 [87552/225000 (39%)] Loss: 16760.666016\n",
      "Train Epoch: 379 [90048/225000 (40%)] Loss: 16381.921875\n",
      "Train Epoch: 379 [92544/225000 (41%)] Loss: 16723.625000\n",
      "Train Epoch: 379 [95040/225000 (42%)] Loss: 16813.230469\n",
      "Train Epoch: 379 [97536/225000 (43%)] Loss: 16090.279297\n",
      "Train Epoch: 379 [100032/225000 (44%)] Loss: 16652.925781\n",
      "Train Epoch: 379 [102528/225000 (46%)] Loss: 16408.750000\n",
      "Train Epoch: 379 [105024/225000 (47%)] Loss: 16869.267578\n",
      "Train Epoch: 379 [107520/225000 (48%)] Loss: 16828.404297\n",
      "Train Epoch: 379 [110016/225000 (49%)] Loss: 16998.046875\n",
      "Train Epoch: 379 [112512/225000 (50%)] Loss: 17061.445312\n",
      "Train Epoch: 379 [115008/225000 (51%)] Loss: 16473.207031\n",
      "Train Epoch: 379 [117504/225000 (52%)] Loss: 16806.207031\n",
      "Train Epoch: 379 [120000/225000 (53%)] Loss: 16827.851562\n",
      "Train Epoch: 379 [122496/225000 (54%)] Loss: 16364.675781\n",
      "Train Epoch: 379 [124992/225000 (56%)] Loss: 16816.558594\n",
      "Train Epoch: 379 [127488/225000 (57%)] Loss: 16956.453125\n",
      "Train Epoch: 379 [129984/225000 (58%)] Loss: 16492.976562\n",
      "Train Epoch: 379 [132480/225000 (59%)] Loss: 16458.253906\n",
      "Train Epoch: 379 [134976/225000 (60%)] Loss: 16444.117188\n",
      "Train Epoch: 379 [137472/225000 (61%)] Loss: 16322.852539\n",
      "Train Epoch: 379 [139968/225000 (62%)] Loss: 16477.789062\n",
      "Train Epoch: 379 [142464/225000 (63%)] Loss: 16647.445312\n",
      "Train Epoch: 379 [144960/225000 (64%)] Loss: 16428.001953\n",
      "Train Epoch: 379 [147456/225000 (66%)] Loss: 16438.748047\n",
      "Train Epoch: 379 [149952/225000 (67%)] Loss: 16678.093750\n",
      "Train Epoch: 379 [152448/225000 (68%)] Loss: 16649.550781\n",
      "Train Epoch: 379 [154944/225000 (69%)] Loss: 16773.669922\n",
      "Train Epoch: 379 [157440/225000 (70%)] Loss: 16311.522461\n",
      "Train Epoch: 379 [159936/225000 (71%)] Loss: 16233.448242\n",
      "Train Epoch: 379 [162432/225000 (72%)] Loss: 16728.367188\n",
      "Train Epoch: 379 [164928/225000 (73%)] Loss: 16448.066406\n",
      "Train Epoch: 379 [167424/225000 (74%)] Loss: 16460.451172\n",
      "Train Epoch: 379 [169920/225000 (76%)] Loss: 16792.953125\n",
      "Train Epoch: 379 [172416/225000 (77%)] Loss: 16801.574219\n",
      "Train Epoch: 379 [174912/225000 (78%)] Loss: 16508.531250\n",
      "Train Epoch: 379 [177408/225000 (79%)] Loss: 16394.640625\n",
      "Train Epoch: 379 [179904/225000 (80%)] Loss: 16762.785156\n",
      "Train Epoch: 379 [182400/225000 (81%)] Loss: 16816.976562\n",
      "Train Epoch: 379 [184896/225000 (82%)] Loss: 16435.101562\n",
      "Train Epoch: 379 [187392/225000 (83%)] Loss: 18270.539062\n",
      "Train Epoch: 379 [189888/225000 (84%)] Loss: 16601.218750\n",
      "Train Epoch: 379 [192384/225000 (86%)] Loss: 16951.078125\n",
      "Train Epoch: 379 [194880/225000 (87%)] Loss: 16211.916016\n",
      "Train Epoch: 379 [197376/225000 (88%)] Loss: 16584.574219\n",
      "Train Epoch: 379 [199872/225000 (89%)] Loss: 16427.734375\n",
      "Train Epoch: 379 [202368/225000 (90%)] Loss: 16567.550781\n",
      "Train Epoch: 379 [204864/225000 (91%)] Loss: 16999.296875\n",
      "Train Epoch: 379 [207360/225000 (92%)] Loss: 16996.394531\n",
      "Train Epoch: 379 [209856/225000 (93%)] Loss: 16865.441406\n",
      "Train Epoch: 379 [212352/225000 (94%)] Loss: 16746.718750\n",
      "Train Epoch: 379 [214848/225000 (95%)] Loss: 16356.716797\n",
      "Train Epoch: 379 [217344/225000 (97%)] Loss: 16396.576172\n",
      "Train Epoch: 379 [219840/225000 (98%)] Loss: 16603.226562\n",
      "Train Epoch: 379 [222336/225000 (99%)] Loss: 16522.667969\n",
      "Train Epoch: 379 [224832/225000 (100%)] Loss: 16522.787109\n",
      "    epoch          : 379\n",
      "    loss           : 16736.887002053114\n",
      "    val_loss       : 16608.771321000942\n",
      "Train Epoch: 380 [192/225000 (0%)] Loss: 16738.607422\n",
      "Train Epoch: 380 [2688/225000 (1%)] Loss: 17102.812500\n",
      "Train Epoch: 380 [5184/225000 (2%)] Loss: 16741.414062\n",
      "Train Epoch: 380 [7680/225000 (3%)] Loss: 16297.992188\n",
      "Train Epoch: 380 [10176/225000 (5%)] Loss: 16403.207031\n",
      "Train Epoch: 380 [12672/225000 (6%)] Loss: 15896.104492\n",
      "Train Epoch: 380 [15168/225000 (7%)] Loss: 16646.761719\n",
      "Train Epoch: 380 [17664/225000 (8%)] Loss: 16816.949219\n",
      "Train Epoch: 380 [20160/225000 (9%)] Loss: 16513.144531\n",
      "Train Epoch: 380 [22656/225000 (10%)] Loss: 16255.616211\n",
      "Train Epoch: 380 [25152/225000 (11%)] Loss: 16494.093750\n",
      "Train Epoch: 380 [27648/225000 (12%)] Loss: 16327.334961\n",
      "Train Epoch: 380 [30144/225000 (13%)] Loss: 16853.564453\n",
      "Train Epoch: 380 [32640/225000 (15%)] Loss: 16944.203125\n",
      "Train Epoch: 380 [35136/225000 (16%)] Loss: 16679.355469\n",
      "Train Epoch: 380 [37632/225000 (17%)] Loss: 16308.237305\n",
      "Train Epoch: 380 [40128/225000 (18%)] Loss: 16642.845703\n",
      "Train Epoch: 380 [42624/225000 (19%)] Loss: 16959.074219\n",
      "Train Epoch: 380 [45120/225000 (20%)] Loss: 16968.085938\n",
      "Train Epoch: 380 [47616/225000 (21%)] Loss: 16900.068359\n",
      "Train Epoch: 380 [50112/225000 (22%)] Loss: 16758.367188\n",
      "Train Epoch: 380 [52608/225000 (23%)] Loss: 16201.634766\n",
      "Train Epoch: 380 [55104/225000 (24%)] Loss: 16615.894531\n",
      "Train Epoch: 380 [57600/225000 (26%)] Loss: 16911.808594\n",
      "Train Epoch: 380 [60096/225000 (27%)] Loss: 15988.612305\n",
      "Train Epoch: 380 [62592/225000 (28%)] Loss: 16390.429688\n",
      "Train Epoch: 380 [65088/225000 (29%)] Loss: 16307.936523\n",
      "Train Epoch: 380 [67584/225000 (30%)] Loss: 16518.972656\n",
      "Train Epoch: 380 [70080/225000 (31%)] Loss: 16759.638672\n",
      "Train Epoch: 380 [72576/225000 (32%)] Loss: 16795.572266\n",
      "Train Epoch: 380 [75072/225000 (33%)] Loss: 16711.312500\n",
      "Train Epoch: 380 [77568/225000 (34%)] Loss: 16471.134766\n",
      "Train Epoch: 380 [80064/225000 (36%)] Loss: 16171.185547\n",
      "Train Epoch: 380 [82560/225000 (37%)] Loss: 18027.558594\n",
      "Train Epoch: 380 [85056/225000 (38%)] Loss: 16703.335938\n",
      "Train Epoch: 380 [87552/225000 (39%)] Loss: 16369.958984\n",
      "Train Epoch: 380 [90048/225000 (40%)] Loss: 16787.707031\n",
      "Train Epoch: 380 [92544/225000 (41%)] Loss: 16917.296875\n",
      "Train Epoch: 380 [95040/225000 (42%)] Loss: 16570.496094\n",
      "Train Epoch: 380 [97536/225000 (43%)] Loss: 16661.570312\n",
      "Train Epoch: 380 [100032/225000 (44%)] Loss: 16377.697266\n",
      "Train Epoch: 380 [102528/225000 (46%)] Loss: 16648.882812\n",
      "Train Epoch: 380 [105024/225000 (47%)] Loss: 16661.134766\n",
      "Train Epoch: 380 [107520/225000 (48%)] Loss: 16753.398438\n",
      "Train Epoch: 380 [110016/225000 (49%)] Loss: 16846.402344\n",
      "Train Epoch: 380 [112512/225000 (50%)] Loss: 16868.128906\n",
      "Train Epoch: 380 [115008/225000 (51%)] Loss: 16700.605469\n",
      "Train Epoch: 380 [117504/225000 (52%)] Loss: 16929.031250\n",
      "Train Epoch: 380 [120000/225000 (53%)] Loss: 16590.285156\n",
      "Train Epoch: 380 [122496/225000 (54%)] Loss: 16761.355469\n",
      "Train Epoch: 380 [124992/225000 (56%)] Loss: 16777.134766\n",
      "Train Epoch: 380 [127488/225000 (57%)] Loss: 16287.548828\n",
      "Train Epoch: 380 [129984/225000 (58%)] Loss: 16701.917969\n",
      "Train Epoch: 380 [132480/225000 (59%)] Loss: 17189.664062\n",
      "Train Epoch: 380 [134976/225000 (60%)] Loss: 16623.224609\n",
      "Train Epoch: 380 [137472/225000 (61%)] Loss: 16925.162109\n",
      "Train Epoch: 380 [139968/225000 (62%)] Loss: 16953.478516\n",
      "Train Epoch: 380 [142464/225000 (63%)] Loss: 18336.691406\n",
      "Train Epoch: 380 [144960/225000 (64%)] Loss: 16786.523438\n",
      "Train Epoch: 380 [147456/225000 (66%)] Loss: 16919.851562\n",
      "Train Epoch: 380 [149952/225000 (67%)] Loss: 16440.281250\n",
      "Train Epoch: 380 [152448/225000 (68%)] Loss: 16698.945312\n",
      "Train Epoch: 380 [154944/225000 (69%)] Loss: 16525.933594\n",
      "Train Epoch: 380 [157440/225000 (70%)] Loss: 16567.335938\n",
      "Train Epoch: 380 [159936/225000 (71%)] Loss: 16579.578125\n",
      "Train Epoch: 380 [162432/225000 (72%)] Loss: 16270.697266\n",
      "Train Epoch: 380 [164928/225000 (73%)] Loss: 16537.761719\n",
      "Train Epoch: 380 [167424/225000 (74%)] Loss: 16659.949219\n",
      "Train Epoch: 380 [169920/225000 (76%)] Loss: 16826.757812\n",
      "Train Epoch: 380 [172416/225000 (77%)] Loss: 16793.611328\n",
      "Train Epoch: 380 [174912/225000 (78%)] Loss: 16828.197266\n",
      "Train Epoch: 380 [177408/225000 (79%)] Loss: 16428.210938\n",
      "Train Epoch: 380 [179904/225000 (80%)] Loss: 16539.097656\n",
      "Train Epoch: 380 [182400/225000 (81%)] Loss: 16659.015625\n",
      "Train Epoch: 380 [184896/225000 (82%)] Loss: 16766.650391\n",
      "Train Epoch: 380 [187392/225000 (83%)] Loss: 16359.854492\n",
      "Train Epoch: 380 [189888/225000 (84%)] Loss: 16838.664062\n",
      "Train Epoch: 380 [192384/225000 (86%)] Loss: 16912.640625\n",
      "Train Epoch: 380 [194880/225000 (87%)] Loss: 16674.796875\n",
      "Train Epoch: 380 [197376/225000 (88%)] Loss: 16670.904297\n",
      "Train Epoch: 380 [199872/225000 (89%)] Loss: 16566.335938\n",
      "Train Epoch: 380 [202368/225000 (90%)] Loss: 16887.859375\n",
      "Train Epoch: 380 [204864/225000 (91%)] Loss: 18038.527344\n",
      "Train Epoch: 380 [207360/225000 (92%)] Loss: 16074.262695\n",
      "Train Epoch: 380 [209856/225000 (93%)] Loss: 16658.253906\n",
      "Train Epoch: 380 [212352/225000 (94%)] Loss: 18072.453125\n",
      "Train Epoch: 380 [214848/225000 (95%)] Loss: 16457.316406\n",
      "Train Epoch: 380 [217344/225000 (97%)] Loss: 16883.937500\n",
      "Train Epoch: 380 [219840/225000 (98%)] Loss: 16309.918945\n",
      "Train Epoch: 380 [222336/225000 (99%)] Loss: 16707.021484\n",
      "Train Epoch: 380 [224832/225000 (100%)] Loss: 16819.376953\n",
      "    epoch          : 380\n",
      "    loss           : 16734.753316312926\n",
      "    val_loss       : 16657.985183370023\n",
      "Train Epoch: 381 [192/225000 (0%)] Loss: 16446.613281\n",
      "Train Epoch: 381 [2688/225000 (1%)] Loss: 16507.074219\n",
      "Train Epoch: 381 [5184/225000 (2%)] Loss: 16598.105469\n",
      "Train Epoch: 381 [7680/225000 (3%)] Loss: 16528.113281\n",
      "Train Epoch: 381 [10176/225000 (5%)] Loss: 16544.458984\n",
      "Train Epoch: 381 [12672/225000 (6%)] Loss: 16817.843750\n",
      "Train Epoch: 381 [15168/225000 (7%)] Loss: 16790.314453\n",
      "Train Epoch: 381 [17664/225000 (8%)] Loss: 16436.500000\n",
      "Train Epoch: 381 [20160/225000 (9%)] Loss: 16532.929688\n",
      "Train Epoch: 381 [22656/225000 (10%)] Loss: 16620.660156\n",
      "Train Epoch: 381 [25152/225000 (11%)] Loss: 16621.257812\n",
      "Train Epoch: 381 [27648/225000 (12%)] Loss: 16344.805664\n",
      "Train Epoch: 381 [30144/225000 (13%)] Loss: 16191.064453\n",
      "Train Epoch: 381 [32640/225000 (15%)] Loss: 16173.007812\n",
      "Train Epoch: 381 [35136/225000 (16%)] Loss: 16381.781250\n",
      "Train Epoch: 381 [37632/225000 (17%)] Loss: 16672.591797\n",
      "Train Epoch: 381 [40128/225000 (18%)] Loss: 16680.175781\n",
      "Train Epoch: 381 [42624/225000 (19%)] Loss: 16592.832031\n",
      "Train Epoch: 381 [45120/225000 (20%)] Loss: 16456.476562\n",
      "Train Epoch: 381 [47616/225000 (21%)] Loss: 16693.089844\n",
      "Train Epoch: 381 [50112/225000 (22%)] Loss: 16761.367188\n",
      "Train Epoch: 381 [52608/225000 (23%)] Loss: 16370.944336\n",
      "Train Epoch: 381 [55104/225000 (24%)] Loss: 16417.691406\n",
      "Train Epoch: 381 [57600/225000 (26%)] Loss: 16873.796875\n",
      "Train Epoch: 381 [60096/225000 (27%)] Loss: 16076.653320\n",
      "Train Epoch: 381 [62592/225000 (28%)] Loss: 16741.156250\n",
      "Train Epoch: 381 [65088/225000 (29%)] Loss: 16649.695312\n",
      "Train Epoch: 381 [67584/225000 (30%)] Loss: 16933.039062\n",
      "Train Epoch: 381 [70080/225000 (31%)] Loss: 16800.826172\n",
      "Train Epoch: 381 [72576/225000 (32%)] Loss: 16695.501953\n",
      "Train Epoch: 381 [75072/225000 (33%)] Loss: 16683.753906\n",
      "Train Epoch: 381 [77568/225000 (34%)] Loss: 16493.246094\n",
      "Train Epoch: 381 [80064/225000 (36%)] Loss: 17025.134766\n",
      "Train Epoch: 381 [82560/225000 (37%)] Loss: 16838.103516\n",
      "Train Epoch: 381 [85056/225000 (38%)] Loss: 16821.746094\n",
      "Train Epoch: 381 [87552/225000 (39%)] Loss: 16778.439453\n",
      "Train Epoch: 381 [90048/225000 (40%)] Loss: 16887.507812\n",
      "Train Epoch: 381 [92544/225000 (41%)] Loss: 16394.574219\n",
      "Train Epoch: 381 [95040/225000 (42%)] Loss: 16372.174805\n",
      "Train Epoch: 381 [97536/225000 (43%)] Loss: 16724.195312\n",
      "Train Epoch: 381 [100032/225000 (44%)] Loss: 17920.058594\n",
      "Train Epoch: 381 [102528/225000 (46%)] Loss: 17064.486328\n",
      "Train Epoch: 381 [105024/225000 (47%)] Loss: 16547.691406\n",
      "Train Epoch: 381 [107520/225000 (48%)] Loss: 16595.519531\n",
      "Train Epoch: 381 [110016/225000 (49%)] Loss: 16739.808594\n",
      "Train Epoch: 381 [112512/225000 (50%)] Loss: 16481.080078\n",
      "Train Epoch: 381 [115008/225000 (51%)] Loss: 16632.421875\n",
      "Train Epoch: 381 [117504/225000 (52%)] Loss: 16848.929688\n",
      "Train Epoch: 381 [120000/225000 (53%)] Loss: 16252.922852\n",
      "Train Epoch: 381 [122496/225000 (54%)] Loss: 16482.738281\n",
      "Train Epoch: 381 [124992/225000 (56%)] Loss: 16667.289062\n",
      "Train Epoch: 381 [127488/225000 (57%)] Loss: 16734.134766\n",
      "Train Epoch: 381 [129984/225000 (58%)] Loss: 16596.621094\n",
      "Train Epoch: 381 [132480/225000 (59%)] Loss: 16353.239258\n",
      "Train Epoch: 381 [134976/225000 (60%)] Loss: 16688.585938\n",
      "Train Epoch: 381 [137472/225000 (61%)] Loss: 16629.250000\n",
      "Train Epoch: 381 [139968/225000 (62%)] Loss: 16502.222656\n",
      "Train Epoch: 381 [142464/225000 (63%)] Loss: 16818.636719\n",
      "Train Epoch: 381 [144960/225000 (64%)] Loss: 16560.699219\n",
      "Train Epoch: 381 [147456/225000 (66%)] Loss: 16833.320312\n",
      "Train Epoch: 381 [149952/225000 (67%)] Loss: 16723.886719\n",
      "Train Epoch: 381 [152448/225000 (68%)] Loss: 16208.914062\n",
      "Train Epoch: 381 [154944/225000 (69%)] Loss: 16828.402344\n",
      "Train Epoch: 381 [157440/225000 (70%)] Loss: 16460.449219\n",
      "Train Epoch: 381 [159936/225000 (71%)] Loss: 16576.658203\n",
      "Train Epoch: 381 [162432/225000 (72%)] Loss: 16772.675781\n",
      "Train Epoch: 381 [164928/225000 (73%)] Loss: 17088.183594\n",
      "Train Epoch: 381 [167424/225000 (74%)] Loss: 16862.894531\n",
      "Train Epoch: 381 [169920/225000 (76%)] Loss: 16823.054688\n",
      "Train Epoch: 381 [172416/225000 (77%)] Loss: 16427.066406\n",
      "Train Epoch: 381 [174912/225000 (78%)] Loss: 16735.535156\n",
      "Train Epoch: 381 [177408/225000 (79%)] Loss: 16320.700195\n",
      "Train Epoch: 381 [179904/225000 (80%)] Loss: 16621.351562\n",
      "Train Epoch: 381 [182400/225000 (81%)] Loss: 16480.535156\n",
      "Train Epoch: 381 [184896/225000 (82%)] Loss: 16584.232422\n",
      "Train Epoch: 381 [187392/225000 (83%)] Loss: 16998.789062\n",
      "Train Epoch: 381 [189888/225000 (84%)] Loss: 16522.218750\n",
      "Train Epoch: 381 [192384/225000 (86%)] Loss: 16671.974609\n",
      "Train Epoch: 381 [194880/225000 (87%)] Loss: 16884.351562\n",
      "Train Epoch: 381 [197376/225000 (88%)] Loss: 16873.828125\n",
      "Train Epoch: 381 [199872/225000 (89%)] Loss: 19976.957031\n",
      "Train Epoch: 381 [202368/225000 (90%)] Loss: 17207.925781\n",
      "Train Epoch: 381 [204864/225000 (91%)] Loss: 16250.667969\n",
      "Train Epoch: 381 [207360/225000 (92%)] Loss: 16360.995117\n",
      "Train Epoch: 381 [209856/225000 (93%)] Loss: 16734.951172\n",
      "Train Epoch: 381 [212352/225000 (94%)] Loss: 16623.207031\n",
      "Train Epoch: 381 [214848/225000 (95%)] Loss: 16970.529297\n",
      "Train Epoch: 381 [217344/225000 (97%)] Loss: 17307.001953\n",
      "Train Epoch: 381 [219840/225000 (98%)] Loss: 18223.636719\n",
      "Train Epoch: 381 [222336/225000 (99%)] Loss: 15982.420898\n",
      "Train Epoch: 381 [224832/225000 (100%)] Loss: 16341.167969\n",
      "    epoch          : 381\n",
      "    loss           : 16717.022420941765\n",
      "    val_loss       : 16714.6819045389\n",
      "Train Epoch: 382 [192/225000 (0%)] Loss: 18490.369141\n",
      "Train Epoch: 382 [2688/225000 (1%)] Loss: 16461.656250\n",
      "Train Epoch: 382 [5184/225000 (2%)] Loss: 16946.273438\n",
      "Train Epoch: 382 [7680/225000 (3%)] Loss: 16938.972656\n",
      "Train Epoch: 382 [10176/225000 (5%)] Loss: 18366.806641\n",
      "Train Epoch: 382 [12672/225000 (6%)] Loss: 16545.527344\n",
      "Train Epoch: 382 [15168/225000 (7%)] Loss: 16789.349609\n",
      "Train Epoch: 382 [17664/225000 (8%)] Loss: 16527.101562\n",
      "Train Epoch: 382 [20160/225000 (9%)] Loss: 16707.980469\n",
      "Train Epoch: 382 [22656/225000 (10%)] Loss: 16058.132812\n",
      "Train Epoch: 382 [25152/225000 (11%)] Loss: 16570.500000\n",
      "Train Epoch: 382 [27648/225000 (12%)] Loss: 16490.703125\n",
      "Train Epoch: 382 [30144/225000 (13%)] Loss: 16419.726562\n",
      "Train Epoch: 382 [32640/225000 (15%)] Loss: 16195.062500\n",
      "Train Epoch: 382 [35136/225000 (16%)] Loss: 16826.835938\n",
      "Train Epoch: 382 [37632/225000 (17%)] Loss: 17982.794922\n",
      "Train Epoch: 382 [40128/225000 (18%)] Loss: 16915.697266\n",
      "Train Epoch: 382 [42624/225000 (19%)] Loss: 16795.117188\n",
      "Train Epoch: 382 [45120/225000 (20%)] Loss: 16341.866211\n",
      "Train Epoch: 382 [47616/225000 (21%)] Loss: 16720.781250\n",
      "Train Epoch: 382 [50112/225000 (22%)] Loss: 16472.000000\n",
      "Train Epoch: 382 [52608/225000 (23%)] Loss: 16286.433594\n",
      "Train Epoch: 382 [55104/225000 (24%)] Loss: 16953.019531\n",
      "Train Epoch: 382 [57600/225000 (26%)] Loss: 16622.125000\n",
      "Train Epoch: 382 [60096/225000 (27%)] Loss: 18049.404297\n",
      "Train Epoch: 382 [62592/225000 (28%)] Loss: 16276.738281\n",
      "Train Epoch: 382 [65088/225000 (29%)] Loss: 17233.402344\n",
      "Train Epoch: 382 [67584/225000 (30%)] Loss: 16754.210938\n",
      "Train Epoch: 382 [70080/225000 (31%)] Loss: 16644.324219\n",
      "Train Epoch: 382 [72576/225000 (32%)] Loss: 16072.616211\n",
      "Train Epoch: 382 [75072/225000 (33%)] Loss: 16795.210938\n",
      "Train Epoch: 382 [77568/225000 (34%)] Loss: 16888.488281\n",
      "Train Epoch: 382 [80064/225000 (36%)] Loss: 16586.867188\n",
      "Train Epoch: 382 [82560/225000 (37%)] Loss: 16215.420898\n",
      "Train Epoch: 382 [85056/225000 (38%)] Loss: 16787.775391\n",
      "Train Epoch: 382 [87552/225000 (39%)] Loss: 16540.664062\n",
      "Train Epoch: 382 [90048/225000 (40%)] Loss: 16629.460938\n",
      "Train Epoch: 382 [92544/225000 (41%)] Loss: 16591.566406\n",
      "Train Epoch: 382 [95040/225000 (42%)] Loss: 16372.533203\n",
      "Train Epoch: 382 [97536/225000 (43%)] Loss: 16317.520508\n",
      "Train Epoch: 382 [100032/225000 (44%)] Loss: 16715.060547\n",
      "Train Epoch: 382 [102528/225000 (46%)] Loss: 16642.732422\n",
      "Train Epoch: 382 [105024/225000 (47%)] Loss: 16737.125000\n",
      "Train Epoch: 382 [107520/225000 (48%)] Loss: 16263.469727\n",
      "Train Epoch: 382 [110016/225000 (49%)] Loss: 16539.324219\n",
      "Train Epoch: 382 [112512/225000 (50%)] Loss: 18119.136719\n",
      "Train Epoch: 382 [115008/225000 (51%)] Loss: 16487.027344\n",
      "Train Epoch: 382 [117504/225000 (52%)] Loss: 16829.187500\n",
      "Train Epoch: 382 [120000/225000 (53%)] Loss: 16241.021484\n",
      "Train Epoch: 382 [122496/225000 (54%)] Loss: 17079.632812\n",
      "Train Epoch: 382 [124992/225000 (56%)] Loss: 16551.679688\n",
      "Train Epoch: 382 [127488/225000 (57%)] Loss: 16242.446289\n",
      "Train Epoch: 382 [129984/225000 (58%)] Loss: 16585.886719\n",
      "Train Epoch: 382 [132480/225000 (59%)] Loss: 16279.378906\n",
      "Train Epoch: 382 [134976/225000 (60%)] Loss: 16850.433594\n",
      "Train Epoch: 382 [137472/225000 (61%)] Loss: 16455.847656\n",
      "Train Epoch: 382 [139968/225000 (62%)] Loss: 16536.974609\n",
      "Train Epoch: 382 [142464/225000 (63%)] Loss: 16837.517578\n",
      "Train Epoch: 382 [144960/225000 (64%)] Loss: 16734.675781\n",
      "Train Epoch: 382 [147456/225000 (66%)] Loss: 16255.406250\n",
      "Train Epoch: 382 [149952/225000 (67%)] Loss: 16756.605469\n",
      "Train Epoch: 382 [152448/225000 (68%)] Loss: 16451.611328\n",
      "Train Epoch: 382 [154944/225000 (69%)] Loss: 16614.398438\n",
      "Train Epoch: 382 [157440/225000 (70%)] Loss: 16580.703125\n",
      "Train Epoch: 382 [159936/225000 (71%)] Loss: 17227.804688\n",
      "Train Epoch: 382 [162432/225000 (72%)] Loss: 16740.386719\n",
      "Train Epoch: 382 [164928/225000 (73%)] Loss: 16671.132812\n",
      "Train Epoch: 382 [167424/225000 (74%)] Loss: 17960.148438\n",
      "Train Epoch: 382 [169920/225000 (76%)] Loss: 16563.726562\n",
      "Train Epoch: 382 [172416/225000 (77%)] Loss: 17031.865234\n",
      "Train Epoch: 382 [174912/225000 (78%)] Loss: 16107.380859\n",
      "Train Epoch: 382 [177408/225000 (79%)] Loss: 16685.316406\n",
      "Train Epoch: 382 [179904/225000 (80%)] Loss: 16721.472656\n",
      "Train Epoch: 382 [182400/225000 (81%)] Loss: 16820.294922\n",
      "Train Epoch: 382 [184896/225000 (82%)] Loss: 16927.767578\n",
      "Train Epoch: 382 [187392/225000 (83%)] Loss: 16858.919922\n",
      "Train Epoch: 382 [189888/225000 (84%)] Loss: 16914.091797\n",
      "Train Epoch: 382 [192384/225000 (86%)] Loss: 17009.519531\n",
      "Train Epoch: 382 [194880/225000 (87%)] Loss: 16967.964844\n",
      "Train Epoch: 382 [197376/225000 (88%)] Loss: 16085.266602\n",
      "Train Epoch: 382 [199872/225000 (89%)] Loss: 16620.273438\n",
      "Train Epoch: 382 [202368/225000 (90%)] Loss: 17054.691406\n",
      "Train Epoch: 382 [204864/225000 (91%)] Loss: 16772.726562\n",
      "Train Epoch: 382 [207360/225000 (92%)] Loss: 17169.656250\n",
      "Train Epoch: 382 [209856/225000 (93%)] Loss: 16903.093750\n",
      "Train Epoch: 382 [212352/225000 (94%)] Loss: 17525.513672\n",
      "Train Epoch: 382 [214848/225000 (95%)] Loss: 16695.175781\n",
      "Train Epoch: 382 [217344/225000 (97%)] Loss: 16561.417969\n",
      "Train Epoch: 382 [219840/225000 (98%)] Loss: 16771.916016\n",
      "Train Epoch: 382 [222336/225000 (99%)] Loss: 16636.777344\n",
      "Train Epoch: 382 [224832/225000 (100%)] Loss: 16456.500000\n",
      "    epoch          : 382\n",
      "    loss           : 16729.935714357136\n",
      "    val_loss       : 16639.800891688763\n",
      "Train Epoch: 383 [192/225000 (0%)] Loss: 16711.421875\n",
      "Train Epoch: 383 [2688/225000 (1%)] Loss: 16300.939453\n",
      "Train Epoch: 383 [5184/225000 (2%)] Loss: 16558.398438\n",
      "Train Epoch: 383 [7680/225000 (3%)] Loss: 16624.732422\n",
      "Train Epoch: 383 [10176/225000 (5%)] Loss: 16441.789062\n",
      "Train Epoch: 383 [12672/225000 (6%)] Loss: 16817.191406\n",
      "Train Epoch: 383 [15168/225000 (7%)] Loss: 16676.566406\n",
      "Train Epoch: 383 [17664/225000 (8%)] Loss: 16751.675781\n",
      "Train Epoch: 383 [20160/225000 (9%)] Loss: 16894.453125\n",
      "Train Epoch: 383 [22656/225000 (10%)] Loss: 16702.039062\n",
      "Train Epoch: 383 [25152/225000 (11%)] Loss: 16302.951172\n",
      "Train Epoch: 383 [27648/225000 (12%)] Loss: 16602.833984\n",
      "Train Epoch: 383 [30144/225000 (13%)] Loss: 16441.830078\n",
      "Train Epoch: 383 [32640/225000 (15%)] Loss: 16718.205078\n",
      "Train Epoch: 383 [35136/225000 (16%)] Loss: 18128.005859\n",
      "Train Epoch: 383 [37632/225000 (17%)] Loss: 16537.890625\n",
      "Train Epoch: 383 [40128/225000 (18%)] Loss: 16470.160156\n",
      "Train Epoch: 383 [42624/225000 (19%)] Loss: 16510.566406\n",
      "Train Epoch: 383 [45120/225000 (20%)] Loss: 16616.880859\n",
      "Train Epoch: 383 [47616/225000 (21%)] Loss: 16794.601562\n",
      "Train Epoch: 383 [50112/225000 (22%)] Loss: 16332.846680\n",
      "Train Epoch: 383 [52608/225000 (23%)] Loss: 17260.082031\n",
      "Train Epoch: 383 [55104/225000 (24%)] Loss: 16868.556641\n",
      "Train Epoch: 383 [57600/225000 (26%)] Loss: 17598.091797\n",
      "Train Epoch: 383 [60096/225000 (27%)] Loss: 16942.744141\n",
      "Train Epoch: 383 [62592/225000 (28%)] Loss: 18119.812500\n",
      "Train Epoch: 383 [65088/225000 (29%)] Loss: 16604.031250\n",
      "Train Epoch: 383 [67584/225000 (30%)] Loss: 16525.761719\n",
      "Train Epoch: 383 [70080/225000 (31%)] Loss: 16916.722656\n",
      "Train Epoch: 383 [72576/225000 (32%)] Loss: 17268.660156\n",
      "Train Epoch: 383 [75072/225000 (33%)] Loss: 16781.785156\n",
      "Train Epoch: 383 [77568/225000 (34%)] Loss: 16854.369141\n",
      "Train Epoch: 383 [80064/225000 (36%)] Loss: 16000.539062\n",
      "Train Epoch: 383 [82560/225000 (37%)] Loss: 16280.110352\n",
      "Train Epoch: 383 [85056/225000 (38%)] Loss: 16919.101562\n",
      "Train Epoch: 383 [87552/225000 (39%)] Loss: 16841.865234\n",
      "Train Epoch: 383 [90048/225000 (40%)] Loss: 16965.451172\n",
      "Train Epoch: 383 [92544/225000 (41%)] Loss: 16864.154297\n",
      "Train Epoch: 383 [95040/225000 (42%)] Loss: 16955.263672\n",
      "Train Epoch: 383 [97536/225000 (43%)] Loss: 16627.671875\n",
      "Train Epoch: 383 [100032/225000 (44%)] Loss: 16954.640625\n",
      "Train Epoch: 383 [102528/225000 (46%)] Loss: 16703.769531\n",
      "Train Epoch: 383 [105024/225000 (47%)] Loss: 16910.433594\n",
      "Train Epoch: 383 [107520/225000 (48%)] Loss: 16336.524414\n",
      "Train Epoch: 383 [110016/225000 (49%)] Loss: 17065.023438\n",
      "Train Epoch: 383 [112512/225000 (50%)] Loss: 16744.886719\n",
      "Train Epoch: 383 [115008/225000 (51%)] Loss: 18449.343750\n",
      "Train Epoch: 383 [117504/225000 (52%)] Loss: 16515.855469\n",
      "Train Epoch: 383 [120000/225000 (53%)] Loss: 16854.398438\n",
      "Train Epoch: 383 [122496/225000 (54%)] Loss: 16510.660156\n",
      "Train Epoch: 383 [124992/225000 (56%)] Loss: 16460.160156\n",
      "Train Epoch: 383 [127488/225000 (57%)] Loss: 17979.462891\n",
      "Train Epoch: 383 [129984/225000 (58%)] Loss: 16027.337891\n",
      "Train Epoch: 383 [132480/225000 (59%)] Loss: 16648.195312\n",
      "Train Epoch: 383 [134976/225000 (60%)] Loss: 16430.878906\n",
      "Train Epoch: 383 [137472/225000 (61%)] Loss: 16766.136719\n",
      "Train Epoch: 383 [139968/225000 (62%)] Loss: 17053.636719\n",
      "Train Epoch: 383 [142464/225000 (63%)] Loss: 16094.168945\n",
      "Train Epoch: 383 [144960/225000 (64%)] Loss: 16566.335938\n",
      "Train Epoch: 383 [147456/225000 (66%)] Loss: 16229.558594\n",
      "Train Epoch: 383 [149952/225000 (67%)] Loss: 16667.373047\n",
      "Train Epoch: 383 [152448/225000 (68%)] Loss: 16158.457031\n",
      "Train Epoch: 383 [154944/225000 (69%)] Loss: 17347.765625\n",
      "Train Epoch: 383 [157440/225000 (70%)] Loss: 16886.339844\n",
      "Train Epoch: 383 [159936/225000 (71%)] Loss: 16402.382812\n",
      "Train Epoch: 383 [162432/225000 (72%)] Loss: 16696.689453\n",
      "Train Epoch: 383 [164928/225000 (73%)] Loss: 16086.290039\n",
      "Train Epoch: 383 [167424/225000 (74%)] Loss: 16614.359375\n",
      "Train Epoch: 383 [169920/225000 (76%)] Loss: 17114.068359\n",
      "Train Epoch: 383 [172416/225000 (77%)] Loss: 16998.296875\n",
      "Train Epoch: 383 [174912/225000 (78%)] Loss: 16371.917969\n",
      "Train Epoch: 383 [177408/225000 (79%)] Loss: 16783.542969\n",
      "Train Epoch: 383 [179904/225000 (80%)] Loss: 16238.714844\n",
      "Train Epoch: 383 [182400/225000 (81%)] Loss: 16618.605469\n",
      "Train Epoch: 383 [184896/225000 (82%)] Loss: 16268.610352\n",
      "Train Epoch: 383 [187392/225000 (83%)] Loss: 16331.307617\n",
      "Train Epoch: 383 [189888/225000 (84%)] Loss: 16505.492188\n",
      "Train Epoch: 383 [192384/225000 (86%)] Loss: 16841.447266\n",
      "Train Epoch: 383 [194880/225000 (87%)] Loss: 16574.261719\n",
      "Train Epoch: 383 [197376/225000 (88%)] Loss: 16423.605469\n",
      "Train Epoch: 383 [199872/225000 (89%)] Loss: 16870.945312\n",
      "Train Epoch: 383 [202368/225000 (90%)] Loss: 16169.633789\n",
      "Train Epoch: 383 [204864/225000 (91%)] Loss: 18222.298828\n",
      "Train Epoch: 383 [207360/225000 (92%)] Loss: 17197.662109\n",
      "Train Epoch: 383 [209856/225000 (93%)] Loss: 16333.379883\n",
      "Train Epoch: 383 [212352/225000 (94%)] Loss: 16977.546875\n",
      "Train Epoch: 383 [214848/225000 (95%)] Loss: 16799.457031\n",
      "Train Epoch: 383 [217344/225000 (97%)] Loss: 16800.027344\n",
      "Train Epoch: 383 [219840/225000 (98%)] Loss: 17109.632812\n",
      "Train Epoch: 383 [222336/225000 (99%)] Loss: 16351.933594\n",
      "Train Epoch: 383 [224832/225000 (100%)] Loss: 16629.394531\n",
      "    epoch          : 383\n",
      "    loss           : 16730.84702965017\n",
      "    val_loss       : 16586.609004564412\n",
      "Train Epoch: 384 [192/225000 (0%)] Loss: 16804.902344\n",
      "Train Epoch: 384 [2688/225000 (1%)] Loss: 16490.882812\n",
      "Train Epoch: 384 [5184/225000 (2%)] Loss: 16477.943359\n",
      "Train Epoch: 384 [7680/225000 (3%)] Loss: 16211.042969\n",
      "Train Epoch: 384 [10176/225000 (5%)] Loss: 16874.906250\n",
      "Train Epoch: 384 [12672/225000 (6%)] Loss: 16945.359375\n",
      "Train Epoch: 384 [15168/225000 (7%)] Loss: 16566.746094\n",
      "Train Epoch: 384 [17664/225000 (8%)] Loss: 16519.064453\n",
      "Train Epoch: 384 [20160/225000 (9%)] Loss: 16528.460938\n",
      "Train Epoch: 384 [22656/225000 (10%)] Loss: 16498.121094\n",
      "Train Epoch: 384 [25152/225000 (11%)] Loss: 16793.835938\n",
      "Train Epoch: 384 [27648/225000 (12%)] Loss: 16648.527344\n",
      "Train Epoch: 384 [30144/225000 (13%)] Loss: 16706.753906\n",
      "Train Epoch: 384 [32640/225000 (15%)] Loss: 16254.882812\n",
      "Train Epoch: 384 [35136/225000 (16%)] Loss: 16462.312500\n",
      "Train Epoch: 384 [37632/225000 (17%)] Loss: 16871.781250\n",
      "Train Epoch: 384 [40128/225000 (18%)] Loss: 16956.753906\n",
      "Train Epoch: 384 [42624/225000 (19%)] Loss: 16363.105469\n",
      "Train Epoch: 384 [45120/225000 (20%)] Loss: 16691.318359\n",
      "Train Epoch: 384 [47616/225000 (21%)] Loss: 16145.178711\n",
      "Train Epoch: 384 [50112/225000 (22%)] Loss: 16216.988281\n",
      "Train Epoch: 384 [52608/225000 (23%)] Loss: 17161.771484\n",
      "Train Epoch: 384 [55104/225000 (24%)] Loss: 16588.339844\n",
      "Train Epoch: 384 [57600/225000 (26%)] Loss: 17266.105469\n",
      "Train Epoch: 384 [60096/225000 (27%)] Loss: 16328.912109\n",
      "Train Epoch: 384 [62592/225000 (28%)] Loss: 16435.363281\n",
      "Train Epoch: 384 [65088/225000 (29%)] Loss: 17083.027344\n",
      "Train Epoch: 384 [67584/225000 (30%)] Loss: 16795.835938\n",
      "Train Epoch: 384 [70080/225000 (31%)] Loss: 16651.019531\n",
      "Train Epoch: 384 [72576/225000 (32%)] Loss: 16431.035156\n",
      "Train Epoch: 384 [75072/225000 (33%)] Loss: 17107.408203\n",
      "Train Epoch: 384 [77568/225000 (34%)] Loss: 16526.126953\n",
      "Train Epoch: 384 [80064/225000 (36%)] Loss: 16564.865234\n",
      "Train Epoch: 384 [82560/225000 (37%)] Loss: 17656.871094\n",
      "Train Epoch: 384 [85056/225000 (38%)] Loss: 16477.195312\n",
      "Train Epoch: 384 [87552/225000 (39%)] Loss: 16631.755859\n",
      "Train Epoch: 384 [90048/225000 (40%)] Loss: 16588.046875\n",
      "Train Epoch: 384 [92544/225000 (41%)] Loss: 17856.345703\n",
      "Train Epoch: 384 [95040/225000 (42%)] Loss: 16199.416016\n",
      "Train Epoch: 384 [97536/225000 (43%)] Loss: 16785.070312\n",
      "Train Epoch: 384 [100032/225000 (44%)] Loss: 16654.957031\n",
      "Train Epoch: 384 [102528/225000 (46%)] Loss: 16453.796875\n",
      "Train Epoch: 384 [105024/225000 (47%)] Loss: 16495.636719\n",
      "Train Epoch: 384 [107520/225000 (48%)] Loss: 16158.083984\n",
      "Train Epoch: 384 [110016/225000 (49%)] Loss: 18764.482422\n",
      "Train Epoch: 384 [112512/225000 (50%)] Loss: 16975.722656\n",
      "Train Epoch: 384 [115008/225000 (51%)] Loss: 16223.983398\n",
      "Train Epoch: 384 [117504/225000 (52%)] Loss: 16835.847656\n",
      "Train Epoch: 384 [120000/225000 (53%)] Loss: 16691.591797\n",
      "Train Epoch: 384 [122496/225000 (54%)] Loss: 16490.140625\n",
      "Train Epoch: 384 [124992/225000 (56%)] Loss: 16449.148438\n",
      "Train Epoch: 384 [127488/225000 (57%)] Loss: 16992.605469\n",
      "Train Epoch: 384 [129984/225000 (58%)] Loss: 17042.105469\n",
      "Train Epoch: 384 [132480/225000 (59%)] Loss: 17002.667969\n",
      "Train Epoch: 384 [134976/225000 (60%)] Loss: 16730.332031\n",
      "Train Epoch: 384 [137472/225000 (61%)] Loss: 18298.785156\n",
      "Train Epoch: 384 [139968/225000 (62%)] Loss: 16901.890625\n",
      "Train Epoch: 384 [142464/225000 (63%)] Loss: 16750.945312\n",
      "Train Epoch: 384 [144960/225000 (64%)] Loss: 15938.335938\n",
      "Train Epoch: 384 [147456/225000 (66%)] Loss: 16306.964844\n",
      "Train Epoch: 384 [149952/225000 (67%)] Loss: 16506.851562\n",
      "Train Epoch: 384 [152448/225000 (68%)] Loss: 16632.183594\n",
      "Train Epoch: 384 [154944/225000 (69%)] Loss: 16690.335938\n",
      "Train Epoch: 384 [157440/225000 (70%)] Loss: 16813.914062\n",
      "Train Epoch: 384 [159936/225000 (71%)] Loss: 16852.541016\n",
      "Train Epoch: 384 [162432/225000 (72%)] Loss: 16612.160156\n",
      "Train Epoch: 384 [164928/225000 (73%)] Loss: 16625.039062\n",
      "Train Epoch: 384 [167424/225000 (74%)] Loss: 16464.550781\n",
      "Train Epoch: 384 [169920/225000 (76%)] Loss: 16992.484375\n",
      "Train Epoch: 384 [172416/225000 (77%)] Loss: 16575.210938\n",
      "Train Epoch: 384 [174912/225000 (78%)] Loss: 16592.164062\n",
      "Train Epoch: 384 [177408/225000 (79%)] Loss: 16327.770508\n",
      "Train Epoch: 384 [179904/225000 (80%)] Loss: 16501.300781\n",
      "Train Epoch: 384 [182400/225000 (81%)] Loss: 16575.121094\n",
      "Train Epoch: 384 [184896/225000 (82%)] Loss: 16729.714844\n",
      "Train Epoch: 384 [187392/225000 (83%)] Loss: 16886.876953\n",
      "Train Epoch: 384 [189888/225000 (84%)] Loss: 16668.671875\n",
      "Train Epoch: 384 [192384/225000 (86%)] Loss: 16939.554688\n",
      "Train Epoch: 384 [194880/225000 (87%)] Loss: 16583.896484\n",
      "Train Epoch: 384 [197376/225000 (88%)] Loss: 16688.113281\n",
      "Train Epoch: 384 [199872/225000 (89%)] Loss: 16317.151367\n",
      "Train Epoch: 384 [202368/225000 (90%)] Loss: 16493.287109\n",
      "Train Epoch: 384 [204864/225000 (91%)] Loss: 16608.636719\n",
      "Train Epoch: 384 [207360/225000 (92%)] Loss: 16609.398438\n",
      "Train Epoch: 384 [209856/225000 (93%)] Loss: 16451.730469\n",
      "Train Epoch: 384 [212352/225000 (94%)] Loss: 16812.351562\n",
      "Train Epoch: 384 [214848/225000 (95%)] Loss: 16685.671875\n",
      "Train Epoch: 384 [217344/225000 (97%)] Loss: 16704.171875\n",
      "Train Epoch: 384 [219840/225000 (98%)] Loss: 18168.082031\n",
      "Train Epoch: 384 [222336/225000 (99%)] Loss: 16884.242188\n",
      "Train Epoch: 384 [224832/225000 (100%)] Loss: 15901.534180\n",
      "    epoch          : 384\n",
      "    loss           : 16702.151950458618\n",
      "    val_loss       : 16637.97422794167\n",
      "Train Epoch: 385 [192/225000 (0%)] Loss: 16449.191406\n",
      "Train Epoch: 385 [2688/225000 (1%)] Loss: 16505.890625\n",
      "Train Epoch: 385 [5184/225000 (2%)] Loss: 17059.453125\n",
      "Train Epoch: 385 [7680/225000 (3%)] Loss: 16502.851562\n",
      "Train Epoch: 385 [10176/225000 (5%)] Loss: 16489.308594\n",
      "Train Epoch: 385 [12672/225000 (6%)] Loss: 16628.414062\n",
      "Train Epoch: 385 [15168/225000 (7%)] Loss: 16415.464844\n",
      "Train Epoch: 385 [17664/225000 (8%)] Loss: 16955.734375\n",
      "Train Epoch: 385 [20160/225000 (9%)] Loss: 17021.816406\n",
      "Train Epoch: 385 [22656/225000 (10%)] Loss: 16264.739258\n",
      "Train Epoch: 385 [25152/225000 (11%)] Loss: 16464.716797\n",
      "Train Epoch: 385 [27648/225000 (12%)] Loss: 17275.531250\n",
      "Train Epoch: 385 [30144/225000 (13%)] Loss: 16208.554688\n",
      "Train Epoch: 385 [32640/225000 (15%)] Loss: 16817.082031\n",
      "Train Epoch: 385 [35136/225000 (16%)] Loss: 16863.433594\n",
      "Train Epoch: 385 [37632/225000 (17%)] Loss: 16710.382812\n",
      "Train Epoch: 385 [40128/225000 (18%)] Loss: 16668.277344\n",
      "Train Epoch: 385 [42624/225000 (19%)] Loss: 16850.476562\n",
      "Train Epoch: 385 [45120/225000 (20%)] Loss: 16647.607422\n",
      "Train Epoch: 385 [47616/225000 (21%)] Loss: 17203.277344\n",
      "Train Epoch: 385 [50112/225000 (22%)] Loss: 16721.451172\n",
      "Train Epoch: 385 [52608/225000 (23%)] Loss: 16306.363281\n",
      "Train Epoch: 385 [55104/225000 (24%)] Loss: 16578.214844\n",
      "Train Epoch: 385 [57600/225000 (26%)] Loss: 16555.996094\n",
      "Train Epoch: 385 [60096/225000 (27%)] Loss: 16557.417969\n",
      "Train Epoch: 385 [62592/225000 (28%)] Loss: 16819.808594\n",
      "Train Epoch: 385 [65088/225000 (29%)] Loss: 16240.045898\n",
      "Train Epoch: 385 [67584/225000 (30%)] Loss: 16504.583984\n",
      "Train Epoch: 385 [70080/225000 (31%)] Loss: 16826.636719\n",
      "Train Epoch: 385 [72576/225000 (32%)] Loss: 16924.365234\n",
      "Train Epoch: 385 [75072/225000 (33%)] Loss: 16857.652344\n",
      "Train Epoch: 385 [77568/225000 (34%)] Loss: 17019.187500\n",
      "Train Epoch: 385 [80064/225000 (36%)] Loss: 16732.634766\n",
      "Train Epoch: 385 [82560/225000 (37%)] Loss: 16838.441406\n",
      "Train Epoch: 385 [85056/225000 (38%)] Loss: 16961.255859\n",
      "Train Epoch: 385 [87552/225000 (39%)] Loss: 16509.529297\n",
      "Train Epoch: 385 [90048/225000 (40%)] Loss: 16274.341797\n",
      "Train Epoch: 385 [92544/225000 (41%)] Loss: 17099.148438\n",
      "Train Epoch: 385 [95040/225000 (42%)] Loss: 16499.980469\n",
      "Train Epoch: 385 [97536/225000 (43%)] Loss: 16814.152344\n",
      "Train Epoch: 385 [100032/225000 (44%)] Loss: 16442.281250\n",
      "Train Epoch: 385 [102528/225000 (46%)] Loss: 16601.728516\n",
      "Train Epoch: 385 [105024/225000 (47%)] Loss: 16610.755859\n",
      "Train Epoch: 385 [107520/225000 (48%)] Loss: 16674.898438\n",
      "Train Epoch: 385 [110016/225000 (49%)] Loss: 16584.158203\n",
      "Train Epoch: 385 [112512/225000 (50%)] Loss: 16063.680664\n",
      "Train Epoch: 385 [115008/225000 (51%)] Loss: 18249.359375\n",
      "Train Epoch: 385 [117504/225000 (52%)] Loss: 16988.074219\n",
      "Train Epoch: 385 [120000/225000 (53%)] Loss: 16306.847656\n",
      "Train Epoch: 385 [122496/225000 (54%)] Loss: 16692.027344\n",
      "Train Epoch: 385 [124992/225000 (56%)] Loss: 16526.589844\n",
      "Train Epoch: 385 [127488/225000 (57%)] Loss: 16524.951172\n",
      "Train Epoch: 385 [129984/225000 (58%)] Loss: 16587.746094\n",
      "Train Epoch: 385 [132480/225000 (59%)] Loss: 16563.300781\n",
      "Train Epoch: 385 [134976/225000 (60%)] Loss: 16149.077148\n",
      "Train Epoch: 385 [137472/225000 (61%)] Loss: 16810.437500\n",
      "Train Epoch: 385 [139968/225000 (62%)] Loss: 16592.109375\n",
      "Train Epoch: 385 [142464/225000 (63%)] Loss: 16102.194336\n",
      "Train Epoch: 385 [144960/225000 (64%)] Loss: 16445.101562\n",
      "Train Epoch: 385 [147456/225000 (66%)] Loss: 16686.671875\n",
      "Train Epoch: 385 [149952/225000 (67%)] Loss: 15930.129883\n",
      "Train Epoch: 385 [152448/225000 (68%)] Loss: 18369.560547\n",
      "Train Epoch: 385 [154944/225000 (69%)] Loss: 16305.767578\n",
      "Train Epoch: 385 [157440/225000 (70%)] Loss: 24952.226562\n",
      "Train Epoch: 385 [159936/225000 (71%)] Loss: 16416.441406\n",
      "Train Epoch: 385 [162432/225000 (72%)] Loss: 16604.777344\n",
      "Train Epoch: 385 [164928/225000 (73%)] Loss: 16589.593750\n",
      "Train Epoch: 385 [167424/225000 (74%)] Loss: 16455.679688\n",
      "Train Epoch: 385 [169920/225000 (76%)] Loss: 16063.781250\n",
      "Train Epoch: 385 [172416/225000 (77%)] Loss: 16386.027344\n",
      "Train Epoch: 385 [174912/225000 (78%)] Loss: 16558.476562\n",
      "Train Epoch: 385 [177408/225000 (79%)] Loss: 17925.726562\n",
      "Train Epoch: 385 [179904/225000 (80%)] Loss: 16277.741211\n",
      "Train Epoch: 385 [182400/225000 (81%)] Loss: 16672.914062\n",
      "Train Epoch: 385 [184896/225000 (82%)] Loss: 16645.742188\n",
      "Train Epoch: 385 [187392/225000 (83%)] Loss: 16545.640625\n",
      "Train Epoch: 385 [189888/225000 (84%)] Loss: 16501.097656\n",
      "Train Epoch: 385 [192384/225000 (86%)] Loss: 16696.728516\n",
      "Train Epoch: 385 [194880/225000 (87%)] Loss: 16536.730469\n",
      "Train Epoch: 385 [197376/225000 (88%)] Loss: 16764.556641\n",
      "Train Epoch: 385 [199872/225000 (89%)] Loss: 16211.430664\n",
      "Train Epoch: 385 [202368/225000 (90%)] Loss: 16394.226562\n",
      "Train Epoch: 385 [204864/225000 (91%)] Loss: 16770.662109\n",
      "Train Epoch: 385 [207360/225000 (92%)] Loss: 18275.949219\n",
      "Train Epoch: 385 [209856/225000 (93%)] Loss: 16332.496094\n",
      "Train Epoch: 385 [212352/225000 (94%)] Loss: 16806.722656\n",
      "Train Epoch: 385 [214848/225000 (95%)] Loss: 16045.381836\n",
      "Train Epoch: 385 [217344/225000 (97%)] Loss: 16926.888672\n",
      "Train Epoch: 385 [219840/225000 (98%)] Loss: 16404.863281\n",
      "Train Epoch: 385 [222336/225000 (99%)] Loss: 16718.625000\n",
      "Train Epoch: 385 [224832/225000 (100%)] Loss: 16503.091797\n",
      "    epoch          : 385\n",
      "    loss           : 16718.96371970323\n",
      "    val_loss       : 16622.891392911664\n",
      "Train Epoch: 386 [192/225000 (0%)] Loss: 16717.496094\n",
      "Train Epoch: 386 [2688/225000 (1%)] Loss: 17075.914062\n",
      "Train Epoch: 386 [5184/225000 (2%)] Loss: 16715.300781\n",
      "Train Epoch: 386 [7680/225000 (3%)] Loss: 16588.193359\n",
      "Train Epoch: 386 [10176/225000 (5%)] Loss: 18013.570312\n",
      "Train Epoch: 386 [12672/225000 (6%)] Loss: 16609.888672\n",
      "Train Epoch: 386 [15168/225000 (7%)] Loss: 16817.271484\n",
      "Train Epoch: 386 [17664/225000 (8%)] Loss: 16815.410156\n",
      "Train Epoch: 386 [20160/225000 (9%)] Loss: 16776.367188\n",
      "Train Epoch: 386 [22656/225000 (10%)] Loss: 16978.503906\n",
      "Train Epoch: 386 [25152/225000 (11%)] Loss: 16630.144531\n",
      "Train Epoch: 386 [27648/225000 (12%)] Loss: 16646.953125\n",
      "Train Epoch: 386 [30144/225000 (13%)] Loss: 16612.277344\n",
      "Train Epoch: 386 [32640/225000 (15%)] Loss: 16438.085938\n",
      "Train Epoch: 386 [35136/225000 (16%)] Loss: 16394.988281\n",
      "Train Epoch: 386 [37632/225000 (17%)] Loss: 16564.277344\n",
      "Train Epoch: 386 [40128/225000 (18%)] Loss: 16195.523438\n",
      "Train Epoch: 386 [42624/225000 (19%)] Loss: 16674.203125\n",
      "Train Epoch: 386 [45120/225000 (20%)] Loss: 16238.625977\n",
      "Train Epoch: 386 [47616/225000 (21%)] Loss: 16627.828125\n",
      "Train Epoch: 386 [50112/225000 (22%)] Loss: 17347.232422\n",
      "Train Epoch: 386 [52608/225000 (23%)] Loss: 16482.769531\n",
      "Train Epoch: 386 [55104/225000 (24%)] Loss: 18075.306641\n",
      "Train Epoch: 386 [57600/225000 (26%)] Loss: 17288.070312\n",
      "Train Epoch: 386 [60096/225000 (27%)] Loss: 16341.438477\n",
      "Train Epoch: 386 [62592/225000 (28%)] Loss: 16788.164062\n",
      "Train Epoch: 386 [65088/225000 (29%)] Loss: 16616.296875\n",
      "Train Epoch: 386 [67584/225000 (30%)] Loss: 16649.574219\n",
      "Train Epoch: 386 [70080/225000 (31%)] Loss: 16580.269531\n",
      "Train Epoch: 386 [72576/225000 (32%)] Loss: 18042.671875\n",
      "Train Epoch: 386 [75072/225000 (33%)] Loss: 17100.570312\n",
      "Train Epoch: 386 [77568/225000 (34%)] Loss: 16398.707031\n",
      "Train Epoch: 386 [80064/225000 (36%)] Loss: 17137.048828\n",
      "Train Epoch: 386 [82560/225000 (37%)] Loss: 16635.626953\n",
      "Train Epoch: 386 [85056/225000 (38%)] Loss: 16693.408203\n",
      "Train Epoch: 386 [87552/225000 (39%)] Loss: 16716.851562\n",
      "Train Epoch: 386 [90048/225000 (40%)] Loss: 16764.441406\n",
      "Train Epoch: 386 [92544/225000 (41%)] Loss: 16834.765625\n",
      "Train Epoch: 386 [95040/225000 (42%)] Loss: 16715.500000\n",
      "Train Epoch: 386 [97536/225000 (43%)] Loss: 16480.984375\n",
      "Train Epoch: 386 [100032/225000 (44%)] Loss: 16158.539062\n",
      "Train Epoch: 386 [102528/225000 (46%)] Loss: 16394.929688\n",
      "Train Epoch: 386 [105024/225000 (47%)] Loss: 16410.878906\n",
      "Train Epoch: 386 [107520/225000 (48%)] Loss: 16360.945312\n",
      "Train Epoch: 386 [110016/225000 (49%)] Loss: 16858.546875\n",
      "Train Epoch: 386 [112512/225000 (50%)] Loss: 16901.027344\n",
      "Train Epoch: 386 [115008/225000 (51%)] Loss: 16847.126953\n",
      "Train Epoch: 386 [117504/225000 (52%)] Loss: 18845.957031\n",
      "Train Epoch: 386 [120000/225000 (53%)] Loss: 16613.996094\n",
      "Train Epoch: 386 [122496/225000 (54%)] Loss: 16085.839844\n",
      "Train Epoch: 386 [124992/225000 (56%)] Loss: 16084.097656\n",
      "Train Epoch: 386 [127488/225000 (57%)] Loss: 16501.162109\n",
      "Train Epoch: 386 [129984/225000 (58%)] Loss: 16973.156250\n",
      "Train Epoch: 386 [132480/225000 (59%)] Loss: 16344.759766\n",
      "Train Epoch: 386 [134976/225000 (60%)] Loss: 16731.863281\n",
      "Train Epoch: 386 [137472/225000 (61%)] Loss: 16692.699219\n",
      "Train Epoch: 386 [139968/225000 (62%)] Loss: 16573.669922\n",
      "Train Epoch: 386 [142464/225000 (63%)] Loss: 16498.375000\n",
      "Train Epoch: 386 [144960/225000 (64%)] Loss: 16240.601562\n",
      "Train Epoch: 386 [147456/225000 (66%)] Loss: 16728.757812\n",
      "Train Epoch: 386 [149952/225000 (67%)] Loss: 16827.810547\n",
      "Train Epoch: 386 [152448/225000 (68%)] Loss: 16840.546875\n",
      "Train Epoch: 386 [154944/225000 (69%)] Loss: 16843.138672\n",
      "Train Epoch: 386 [157440/225000 (70%)] Loss: 17049.302734\n",
      "Train Epoch: 386 [159936/225000 (71%)] Loss: 16707.875000\n",
      "Train Epoch: 386 [162432/225000 (72%)] Loss: 16801.449219\n",
      "Train Epoch: 386 [164928/225000 (73%)] Loss: 16457.472656\n",
      "Train Epoch: 386 [167424/225000 (74%)] Loss: 16651.605469\n",
      "Train Epoch: 386 [169920/225000 (76%)] Loss: 16835.929688\n",
      "Train Epoch: 386 [172416/225000 (77%)] Loss: 16954.269531\n",
      "Train Epoch: 386 [174912/225000 (78%)] Loss: 16380.133789\n",
      "Train Epoch: 386 [177408/225000 (79%)] Loss: 16277.021484\n",
      "Train Epoch: 386 [179904/225000 (80%)] Loss: 17223.164062\n",
      "Train Epoch: 386 [182400/225000 (81%)] Loss: 16678.695312\n",
      "Train Epoch: 386 [184896/225000 (82%)] Loss: 16700.441406\n",
      "Train Epoch: 386 [187392/225000 (83%)] Loss: 18351.855469\n",
      "Train Epoch: 386 [189888/225000 (84%)] Loss: 16750.560547\n",
      "Train Epoch: 386 [192384/225000 (86%)] Loss: 16786.328125\n",
      "Train Epoch: 386 [194880/225000 (87%)] Loss: 16407.914062\n",
      "Train Epoch: 386 [197376/225000 (88%)] Loss: 16703.210938\n",
      "Train Epoch: 386 [199872/225000 (89%)] Loss: 16501.894531\n",
      "Train Epoch: 386 [202368/225000 (90%)] Loss: 16761.238281\n",
      "Train Epoch: 386 [204864/225000 (91%)] Loss: 16982.234375\n",
      "Train Epoch: 386 [207360/225000 (92%)] Loss: 16447.191406\n",
      "Train Epoch: 386 [209856/225000 (93%)] Loss: 16613.941406\n",
      "Train Epoch: 386 [212352/225000 (94%)] Loss: 16874.886719\n",
      "Train Epoch: 386 [214848/225000 (95%)] Loss: 16208.605469\n",
      "Train Epoch: 386 [217344/225000 (97%)] Loss: 16817.453125\n",
      "Train Epoch: 386 [219840/225000 (98%)] Loss: 16063.458984\n",
      "Train Epoch: 386 [222336/225000 (99%)] Loss: 16615.464844\n",
      "Train Epoch: 386 [224832/225000 (100%)] Loss: 16504.566406\n",
      "    epoch          : 386\n",
      "    loss           : 16703.066792042184\n",
      "    val_loss       : 16659.20281951118\n",
      "Train Epoch: 387 [192/225000 (0%)] Loss: 16608.761719\n",
      "Train Epoch: 387 [2688/225000 (1%)] Loss: 16242.513672\n",
      "Train Epoch: 387 [5184/225000 (2%)] Loss: 16460.007812\n",
      "Train Epoch: 387 [7680/225000 (3%)] Loss: 16538.464844\n",
      "Train Epoch: 387 [10176/225000 (5%)] Loss: 16874.886719\n",
      "Train Epoch: 387 [12672/225000 (6%)] Loss: 16732.906250\n",
      "Train Epoch: 387 [15168/225000 (7%)] Loss: 16353.855469\n",
      "Train Epoch: 387 [17664/225000 (8%)] Loss: 16718.134766\n",
      "Train Epoch: 387 [20160/225000 (9%)] Loss: 16559.363281\n",
      "Train Epoch: 387 [22656/225000 (10%)] Loss: 17022.480469\n",
      "Train Epoch: 387 [25152/225000 (11%)] Loss: 16810.605469\n",
      "Train Epoch: 387 [27648/225000 (12%)] Loss: 16756.898438\n",
      "Train Epoch: 387 [30144/225000 (13%)] Loss: 16756.998047\n",
      "Train Epoch: 387 [32640/225000 (15%)] Loss: 16567.326172\n",
      "Train Epoch: 387 [35136/225000 (16%)] Loss: 16647.849609\n",
      "Train Epoch: 387 [37632/225000 (17%)] Loss: 16578.515625\n",
      "Train Epoch: 387 [40128/225000 (18%)] Loss: 16795.285156\n",
      "Train Epoch: 387 [42624/225000 (19%)] Loss: 16598.904297\n",
      "Train Epoch: 387 [45120/225000 (20%)] Loss: 16687.847656\n",
      "Train Epoch: 387 [47616/225000 (21%)] Loss: 16932.898438\n",
      "Train Epoch: 387 [50112/225000 (22%)] Loss: 16750.171875\n",
      "Train Epoch: 387 [52608/225000 (23%)] Loss: 16353.329102\n",
      "Train Epoch: 387 [55104/225000 (24%)] Loss: 16511.625000\n",
      "Train Epoch: 387 [57600/225000 (26%)] Loss: 16460.835938\n",
      "Train Epoch: 387 [60096/225000 (27%)] Loss: 16411.812500\n",
      "Train Epoch: 387 [62592/225000 (28%)] Loss: 16845.367188\n",
      "Train Epoch: 387 [65088/225000 (29%)] Loss: 16809.308594\n",
      "Train Epoch: 387 [67584/225000 (30%)] Loss: 16578.378906\n",
      "Train Epoch: 387 [70080/225000 (31%)] Loss: 16274.000000\n",
      "Train Epoch: 387 [72576/225000 (32%)] Loss: 16799.308594\n",
      "Train Epoch: 387 [75072/225000 (33%)] Loss: 16879.242188\n",
      "Train Epoch: 387 [77568/225000 (34%)] Loss: 16505.150391\n",
      "Train Epoch: 387 [80064/225000 (36%)] Loss: 16653.933594\n",
      "Train Epoch: 387 [82560/225000 (37%)] Loss: 16933.476562\n",
      "Train Epoch: 387 [85056/225000 (38%)] Loss: 17231.902344\n",
      "Train Epoch: 387 [87552/225000 (39%)] Loss: 16548.488281\n",
      "Train Epoch: 387 [90048/225000 (40%)] Loss: 17022.480469\n",
      "Train Epoch: 387 [92544/225000 (41%)] Loss: 16544.019531\n",
      "Train Epoch: 387 [95040/225000 (42%)] Loss: 16747.593750\n",
      "Train Epoch: 387 [97536/225000 (43%)] Loss: 16728.535156\n",
      "Train Epoch: 387 [100032/225000 (44%)] Loss: 16907.511719\n",
      "Train Epoch: 387 [102528/225000 (46%)] Loss: 16860.992188\n",
      "Train Epoch: 387 [105024/225000 (47%)] Loss: 16530.207031\n",
      "Train Epoch: 387 [107520/225000 (48%)] Loss: 16227.858398\n",
      "Train Epoch: 387 [110016/225000 (49%)] Loss: 16578.464844\n",
      "Train Epoch: 387 [112512/225000 (50%)] Loss: 16701.742188\n",
      "Train Epoch: 387 [115008/225000 (51%)] Loss: 16597.976562\n",
      "Train Epoch: 387 [117504/225000 (52%)] Loss: 16161.752930\n",
      "Train Epoch: 387 [120000/225000 (53%)] Loss: 16535.373047\n",
      "Train Epoch: 387 [122496/225000 (54%)] Loss: 16044.560547\n",
      "Train Epoch: 387 [124992/225000 (56%)] Loss: 16557.205078\n",
      "Train Epoch: 387 [127488/225000 (57%)] Loss: 16925.289062\n",
      "Train Epoch: 387 [129984/225000 (58%)] Loss: 16741.851562\n",
      "Train Epoch: 387 [132480/225000 (59%)] Loss: 16026.687500\n",
      "Train Epoch: 387 [134976/225000 (60%)] Loss: 16272.295898\n",
      "Train Epoch: 387 [137472/225000 (61%)] Loss: 16212.236328\n",
      "Train Epoch: 387 [139968/225000 (62%)] Loss: 18210.414062\n",
      "Train Epoch: 387 [142464/225000 (63%)] Loss: 17182.824219\n",
      "Train Epoch: 387 [144960/225000 (64%)] Loss: 16837.253906\n",
      "Train Epoch: 387 [147456/225000 (66%)] Loss: 16119.739258\n",
      "Train Epoch: 387 [149952/225000 (67%)] Loss: 16480.185547\n",
      "Train Epoch: 387 [152448/225000 (68%)] Loss: 16372.736328\n",
      "Train Epoch: 387 [154944/225000 (69%)] Loss: 16876.214844\n",
      "Train Epoch: 387 [157440/225000 (70%)] Loss: 16814.402344\n",
      "Train Epoch: 387 [159936/225000 (71%)] Loss: 16556.605469\n",
      "Train Epoch: 387 [162432/225000 (72%)] Loss: 16778.488281\n",
      "Train Epoch: 387 [164928/225000 (73%)] Loss: 16428.123047\n",
      "Train Epoch: 387 [167424/225000 (74%)] Loss: 16258.629883\n",
      "Train Epoch: 387 [169920/225000 (76%)] Loss: 16752.916016\n",
      "Train Epoch: 387 [172416/225000 (77%)] Loss: 16054.776367\n",
      "Train Epoch: 387 [174912/225000 (78%)] Loss: 16238.956055\n",
      "Train Epoch: 387 [177408/225000 (79%)] Loss: 16645.484375\n",
      "Train Epoch: 387 [179904/225000 (80%)] Loss: 16446.289062\n",
      "Train Epoch: 387 [182400/225000 (81%)] Loss: 16429.150391\n",
      "Train Epoch: 387 [184896/225000 (82%)] Loss: 16603.843750\n",
      "Train Epoch: 387 [187392/225000 (83%)] Loss: 18417.613281\n",
      "Train Epoch: 387 [189888/225000 (84%)] Loss: 18118.816406\n",
      "Train Epoch: 387 [192384/225000 (86%)] Loss: 16671.683594\n",
      "Train Epoch: 387 [194880/225000 (87%)] Loss: 16473.783203\n",
      "Train Epoch: 387 [197376/225000 (88%)] Loss: 16068.757812\n",
      "Train Epoch: 387 [199872/225000 (89%)] Loss: 16689.830078\n",
      "Train Epoch: 387 [202368/225000 (90%)] Loss: 16423.355469\n",
      "Train Epoch: 387 [204864/225000 (91%)] Loss: 17334.978516\n",
      "Train Epoch: 387 [207360/225000 (92%)] Loss: 16165.929688\n",
      "Train Epoch: 387 [209856/225000 (93%)] Loss: 16622.085938\n",
      "Train Epoch: 387 [212352/225000 (94%)] Loss: 16389.015625\n",
      "Train Epoch: 387 [214848/225000 (95%)] Loss: 16437.193359\n",
      "Train Epoch: 387 [217344/225000 (97%)] Loss: 18166.072266\n",
      "Train Epoch: 387 [219840/225000 (98%)] Loss: 16974.335938\n",
      "Train Epoch: 387 [222336/225000 (99%)] Loss: 16227.980469\n",
      "Train Epoch: 387 [224832/225000 (100%)] Loss: 16406.457031\n",
      "    epoch          : 387\n",
      "    loss           : 16737.404429360868\n",
      "    val_loss       : 16733.231744534187\n",
      "Train Epoch: 388 [192/225000 (0%)] Loss: 16223.152344\n",
      "Train Epoch: 388 [2688/225000 (1%)] Loss: 16563.578125\n",
      "Train Epoch: 388 [5184/225000 (2%)] Loss: 16749.511719\n",
      "Train Epoch: 388 [7680/225000 (3%)] Loss: 16427.250000\n",
      "Train Epoch: 388 [10176/225000 (5%)] Loss: 16976.062500\n",
      "Train Epoch: 388 [12672/225000 (6%)] Loss: 16611.015625\n",
      "Train Epoch: 388 [15168/225000 (7%)] Loss: 16562.097656\n",
      "Train Epoch: 388 [17664/225000 (8%)] Loss: 17190.210938\n",
      "Train Epoch: 388 [20160/225000 (9%)] Loss: 16656.707031\n",
      "Train Epoch: 388 [22656/225000 (10%)] Loss: 16577.392578\n",
      "Train Epoch: 388 [25152/225000 (11%)] Loss: 16988.152344\n",
      "Train Epoch: 388 [27648/225000 (12%)] Loss: 17024.101562\n",
      "Train Epoch: 388 [30144/225000 (13%)] Loss: 16662.664062\n",
      "Train Epoch: 388 [32640/225000 (15%)] Loss: 16432.812500\n",
      "Train Epoch: 388 [35136/225000 (16%)] Loss: 18174.082031\n",
      "Train Epoch: 388 [37632/225000 (17%)] Loss: 16864.148438\n",
      "Train Epoch: 388 [40128/225000 (18%)] Loss: 16743.085938\n",
      "Train Epoch: 388 [42624/225000 (19%)] Loss: 16882.843750\n",
      "Train Epoch: 388 [45120/225000 (20%)] Loss: 16700.955078\n",
      "Train Epoch: 388 [47616/225000 (21%)] Loss: 16485.644531\n",
      "Train Epoch: 388 [50112/225000 (22%)] Loss: 16676.656250\n",
      "Train Epoch: 388 [52608/225000 (23%)] Loss: 16830.332031\n",
      "Train Epoch: 388 [55104/225000 (24%)] Loss: 16388.382812\n",
      "Train Epoch: 388 [57600/225000 (26%)] Loss: 18144.632812\n",
      "Train Epoch: 388 [60096/225000 (27%)] Loss: 16787.982422\n",
      "Train Epoch: 388 [62592/225000 (28%)] Loss: 16515.523438\n",
      "Train Epoch: 388 [65088/225000 (29%)] Loss: 16912.687500\n",
      "Train Epoch: 388 [67584/225000 (30%)] Loss: 16632.378906\n",
      "Train Epoch: 388 [70080/225000 (31%)] Loss: 16189.087891\n",
      "Train Epoch: 388 [72576/225000 (32%)] Loss: 24731.529297\n",
      "Train Epoch: 388 [75072/225000 (33%)] Loss: 16369.252930\n",
      "Train Epoch: 388 [77568/225000 (34%)] Loss: 16730.410156\n",
      "Train Epoch: 388 [80064/225000 (36%)] Loss: 16497.937500\n",
      "Train Epoch: 388 [82560/225000 (37%)] Loss: 16730.265625\n",
      "Train Epoch: 388 [85056/225000 (38%)] Loss: 16757.382812\n",
      "Train Epoch: 388 [87552/225000 (39%)] Loss: 17118.406250\n",
      "Train Epoch: 388 [90048/225000 (40%)] Loss: 16814.300781\n",
      "Train Epoch: 388 [92544/225000 (41%)] Loss: 16763.429688\n",
      "Train Epoch: 388 [95040/225000 (42%)] Loss: 16704.441406\n",
      "Train Epoch: 388 [97536/225000 (43%)] Loss: 16639.250000\n",
      "Train Epoch: 388 [100032/225000 (44%)] Loss: 17027.896484\n",
      "Train Epoch: 388 [102528/225000 (46%)] Loss: 16220.313477\n",
      "Train Epoch: 388 [105024/225000 (47%)] Loss: 16949.705078\n",
      "Train Epoch: 388 [107520/225000 (48%)] Loss: 16908.371094\n",
      "Train Epoch: 388 [110016/225000 (49%)] Loss: 16606.453125\n",
      "Train Epoch: 388 [112512/225000 (50%)] Loss: 16621.570312\n",
      "Train Epoch: 388 [115008/225000 (51%)] Loss: 16642.511719\n",
      "Train Epoch: 388 [117504/225000 (52%)] Loss: 17056.890625\n",
      "Train Epoch: 388 [120000/225000 (53%)] Loss: 16573.792969\n",
      "Train Epoch: 388 [122496/225000 (54%)] Loss: 16663.761719\n",
      "Train Epoch: 388 [124992/225000 (56%)] Loss: 16813.402344\n",
      "Train Epoch: 388 [127488/225000 (57%)] Loss: 16843.417969\n",
      "Train Epoch: 388 [129984/225000 (58%)] Loss: 16722.792969\n",
      "Train Epoch: 388 [132480/225000 (59%)] Loss: 16993.914062\n",
      "Train Epoch: 388 [134976/225000 (60%)] Loss: 16003.439453\n",
      "Train Epoch: 388 [137472/225000 (61%)] Loss: 16823.449219\n",
      "Train Epoch: 388 [139968/225000 (62%)] Loss: 16971.656250\n",
      "Train Epoch: 388 [142464/225000 (63%)] Loss: 16748.400391\n",
      "Train Epoch: 388 [144960/225000 (64%)] Loss: 16556.718750\n",
      "Train Epoch: 388 [147456/225000 (66%)] Loss: 16640.001953\n",
      "Train Epoch: 388 [149952/225000 (67%)] Loss: 16535.998047\n",
      "Train Epoch: 388 [152448/225000 (68%)] Loss: 16617.929688\n",
      "Train Epoch: 388 [154944/225000 (69%)] Loss: 16604.902344\n",
      "Train Epoch: 388 [157440/225000 (70%)] Loss: 16505.695312\n",
      "Train Epoch: 388 [159936/225000 (71%)] Loss: 16355.137695\n",
      "Train Epoch: 388 [162432/225000 (72%)] Loss: 16720.955078\n",
      "Train Epoch: 388 [164928/225000 (73%)] Loss: 16950.669922\n",
      "Train Epoch: 388 [167424/225000 (74%)] Loss: 16708.484375\n",
      "Train Epoch: 388 [169920/225000 (76%)] Loss: 16327.203125\n",
      "Train Epoch: 388 [172416/225000 (77%)] Loss: 16550.429688\n",
      "Train Epoch: 388 [174912/225000 (78%)] Loss: 16706.033203\n",
      "Train Epoch: 388 [177408/225000 (79%)] Loss: 16273.521484\n",
      "Train Epoch: 388 [179904/225000 (80%)] Loss: 17206.023438\n",
      "Train Epoch: 388 [182400/225000 (81%)] Loss: 16524.523438\n",
      "Train Epoch: 388 [184896/225000 (82%)] Loss: 16323.875000\n",
      "Train Epoch: 388 [187392/225000 (83%)] Loss: 16536.765625\n",
      "Train Epoch: 388 [189888/225000 (84%)] Loss: 16220.764648\n",
      "Train Epoch: 388 [192384/225000 (86%)] Loss: 16413.101562\n",
      "Train Epoch: 388 [194880/225000 (87%)] Loss: 16635.132812\n",
      "Train Epoch: 388 [197376/225000 (88%)] Loss: 16706.451172\n",
      "Train Epoch: 388 [199872/225000 (89%)] Loss: 16951.980469\n",
      "Train Epoch: 388 [202368/225000 (90%)] Loss: 16266.857422\n",
      "Train Epoch: 388 [204864/225000 (91%)] Loss: 16996.244141\n",
      "Train Epoch: 388 [207360/225000 (92%)] Loss: 16761.511719\n",
      "Train Epoch: 388 [209856/225000 (93%)] Loss: 16779.613281\n",
      "Train Epoch: 388 [212352/225000 (94%)] Loss: 16576.398438\n",
      "Train Epoch: 388 [214848/225000 (95%)] Loss: 15975.534180\n",
      "Train Epoch: 388 [217344/225000 (97%)] Loss: 16667.851562\n",
      "Train Epoch: 388 [219840/225000 (98%)] Loss: 18397.300781\n",
      "Train Epoch: 388 [222336/225000 (99%)] Loss: 16751.929688\n",
      "Train Epoch: 388 [224832/225000 (100%)] Loss: 16555.597656\n",
      "    epoch          : 388\n",
      "    loss           : 16736.480953698272\n",
      "    val_loss       : 16609.27374433106\n",
      "Train Epoch: 389 [192/225000 (0%)] Loss: 16586.425781\n",
      "Train Epoch: 389 [2688/225000 (1%)] Loss: 16653.492188\n",
      "Train Epoch: 389 [5184/225000 (2%)] Loss: 18527.525391\n",
      "Train Epoch: 389 [7680/225000 (3%)] Loss: 16874.550781\n",
      "Train Epoch: 389 [10176/225000 (5%)] Loss: 17039.134766\n",
      "Train Epoch: 389 [12672/225000 (6%)] Loss: 16908.267578\n",
      "Train Epoch: 389 [15168/225000 (7%)] Loss: 16502.451172\n",
      "Train Epoch: 389 [17664/225000 (8%)] Loss: 16647.800781\n",
      "Train Epoch: 389 [20160/225000 (9%)] Loss: 16555.044922\n",
      "Train Epoch: 389 [22656/225000 (10%)] Loss: 16642.001953\n",
      "Train Epoch: 389 [25152/225000 (11%)] Loss: 16377.477539\n",
      "Train Epoch: 389 [27648/225000 (12%)] Loss: 16888.394531\n",
      "Train Epoch: 389 [30144/225000 (13%)] Loss: 16819.185547\n",
      "Train Epoch: 389 [32640/225000 (15%)] Loss: 16976.541016\n",
      "Train Epoch: 389 [35136/225000 (16%)] Loss: 16426.705078\n",
      "Train Epoch: 389 [37632/225000 (17%)] Loss: 16666.863281\n",
      "Train Epoch: 389 [40128/225000 (18%)] Loss: 16192.538086\n",
      "Train Epoch: 389 [42624/225000 (19%)] Loss: 16428.212891\n",
      "Train Epoch: 389 [45120/225000 (20%)] Loss: 18290.984375\n",
      "Train Epoch: 389 [47616/225000 (21%)] Loss: 16965.042969\n",
      "Train Epoch: 389 [50112/225000 (22%)] Loss: 16565.416016\n",
      "Train Epoch: 389 [52608/225000 (23%)] Loss: 18266.611328\n",
      "Train Epoch: 389 [55104/225000 (24%)] Loss: 16607.414062\n",
      "Train Epoch: 389 [57600/225000 (26%)] Loss: 16724.054688\n",
      "Train Epoch: 389 [60096/225000 (27%)] Loss: 16605.519531\n",
      "Train Epoch: 389 [62592/225000 (28%)] Loss: 16539.957031\n",
      "Train Epoch: 389 [65088/225000 (29%)] Loss: 17122.664062\n",
      "Train Epoch: 389 [67584/225000 (30%)] Loss: 16583.808594\n",
      "Train Epoch: 389 [70080/225000 (31%)] Loss: 16592.640625\n",
      "Train Epoch: 389 [72576/225000 (32%)] Loss: 16452.240234\n",
      "Train Epoch: 389 [75072/225000 (33%)] Loss: 16671.947266\n",
      "Train Epoch: 389 [77568/225000 (34%)] Loss: 16872.140625\n",
      "Train Epoch: 389 [80064/225000 (36%)] Loss: 16950.654297\n",
      "Train Epoch: 389 [82560/225000 (37%)] Loss: 16180.927734\n",
      "Train Epoch: 389 [85056/225000 (38%)] Loss: 16864.652344\n",
      "Train Epoch: 389 [87552/225000 (39%)] Loss: 16452.128906\n",
      "Train Epoch: 389 [90048/225000 (40%)] Loss: 16404.779297\n",
      "Train Epoch: 389 [92544/225000 (41%)] Loss: 17003.253906\n",
      "Train Epoch: 389 [95040/225000 (42%)] Loss: 17226.367188\n",
      "Train Epoch: 389 [97536/225000 (43%)] Loss: 16771.250000\n",
      "Train Epoch: 389 [100032/225000 (44%)] Loss: 16559.943359\n",
      "Train Epoch: 389 [102528/225000 (46%)] Loss: 18414.675781\n",
      "Train Epoch: 389 [105024/225000 (47%)] Loss: 16580.162109\n",
      "Train Epoch: 389 [107520/225000 (48%)] Loss: 16813.337891\n",
      "Train Epoch: 389 [110016/225000 (49%)] Loss: 16417.640625\n",
      "Train Epoch: 389 [112512/225000 (50%)] Loss: 16478.093750\n",
      "Train Epoch: 389 [115008/225000 (51%)] Loss: 16504.347656\n",
      "Train Epoch: 389 [117504/225000 (52%)] Loss: 16740.820312\n",
      "Train Epoch: 389 [120000/225000 (53%)] Loss: 16713.960938\n",
      "Train Epoch: 389 [122496/225000 (54%)] Loss: 16581.880859\n",
      "Train Epoch: 389 [124992/225000 (56%)] Loss: 16760.558594\n",
      "Train Epoch: 389 [127488/225000 (57%)] Loss: 17014.402344\n",
      "Train Epoch: 389 [129984/225000 (58%)] Loss: 16277.241211\n",
      "Train Epoch: 389 [132480/225000 (59%)] Loss: 16377.735352\n",
      "Train Epoch: 389 [134976/225000 (60%)] Loss: 16476.585938\n",
      "Train Epoch: 389 [137472/225000 (61%)] Loss: 17024.589844\n",
      "Train Epoch: 389 [139968/225000 (62%)] Loss: 16093.684570\n",
      "Train Epoch: 389 [142464/225000 (63%)] Loss: 16477.837891\n",
      "Train Epoch: 389 [144960/225000 (64%)] Loss: 16422.767578\n",
      "Train Epoch: 389 [147456/225000 (66%)] Loss: 16033.804688\n",
      "Train Epoch: 389 [149952/225000 (67%)] Loss: 16424.447266\n",
      "Train Epoch: 389 [152448/225000 (68%)] Loss: 16490.345703\n",
      "Train Epoch: 389 [154944/225000 (69%)] Loss: 16957.458984\n",
      "Train Epoch: 389 [157440/225000 (70%)] Loss: 16585.863281\n",
      "Train Epoch: 389 [159936/225000 (71%)] Loss: 16917.566406\n",
      "Train Epoch: 389 [162432/225000 (72%)] Loss: 16686.390625\n",
      "Train Epoch: 389 [164928/225000 (73%)] Loss: 16639.458984\n",
      "Train Epoch: 389 [167424/225000 (74%)] Loss: 16201.195312\n",
      "Train Epoch: 389 [169920/225000 (76%)] Loss: 16494.242188\n",
      "Train Epoch: 389 [172416/225000 (77%)] Loss: 16905.673828\n",
      "Train Epoch: 389 [174912/225000 (78%)] Loss: 16862.585938\n",
      "Train Epoch: 389 [177408/225000 (79%)] Loss: 16303.853516\n",
      "Train Epoch: 389 [179904/225000 (80%)] Loss: 16710.937500\n",
      "Train Epoch: 389 [182400/225000 (81%)] Loss: 16698.277344\n",
      "Train Epoch: 389 [184896/225000 (82%)] Loss: 16624.144531\n",
      "Train Epoch: 389 [187392/225000 (83%)] Loss: 15913.660156\n",
      "Train Epoch: 389 [189888/225000 (84%)] Loss: 16899.884766\n",
      "Train Epoch: 389 [192384/225000 (86%)] Loss: 16531.578125\n",
      "Train Epoch: 389 [194880/225000 (87%)] Loss: 16464.044922\n",
      "Train Epoch: 389 [197376/225000 (88%)] Loss: 17127.855469\n",
      "Train Epoch: 389 [199872/225000 (89%)] Loss: 16272.343750\n",
      "Train Epoch: 389 [202368/225000 (90%)] Loss: 16569.996094\n",
      "Train Epoch: 389 [204864/225000 (91%)] Loss: 16596.693359\n",
      "Train Epoch: 389 [207360/225000 (92%)] Loss: 16315.772461\n",
      "Train Epoch: 389 [209856/225000 (93%)] Loss: 16689.214844\n",
      "Train Epoch: 389 [212352/225000 (94%)] Loss: 16443.898438\n",
      "Train Epoch: 389 [214848/225000 (95%)] Loss: 16643.919922\n",
      "Train Epoch: 389 [217344/225000 (97%)] Loss: 16366.862305\n",
      "Train Epoch: 389 [219840/225000 (98%)] Loss: 16608.925781\n",
      "Train Epoch: 389 [222336/225000 (99%)] Loss: 16302.627930\n",
      "Train Epoch: 389 [224832/225000 (100%)] Loss: 16752.621094\n",
      "    epoch          : 389\n",
      "    loss           : 16696.178111001493\n",
      "    val_loss       : 16696.643268650725\n",
      "Train Epoch: 390 [192/225000 (0%)] Loss: 16069.180664\n",
      "Train Epoch: 390 [2688/225000 (1%)] Loss: 16595.876953\n",
      "Train Epoch: 390 [5184/225000 (2%)] Loss: 16521.941406\n",
      "Train Epoch: 390 [7680/225000 (3%)] Loss: 16700.179688\n",
      "Train Epoch: 390 [10176/225000 (5%)] Loss: 16614.820312\n",
      "Train Epoch: 390 [12672/225000 (6%)] Loss: 16547.871094\n",
      "Train Epoch: 390 [15168/225000 (7%)] Loss: 16853.595703\n",
      "Train Epoch: 390 [17664/225000 (8%)] Loss: 16478.042969\n",
      "Train Epoch: 390 [20160/225000 (9%)] Loss: 16566.554688\n",
      "Train Epoch: 390 [22656/225000 (10%)] Loss: 16483.460938\n",
      "Train Epoch: 390 [25152/225000 (11%)] Loss: 16916.347656\n",
      "Train Epoch: 390 [27648/225000 (12%)] Loss: 16296.499023\n",
      "Train Epoch: 390 [30144/225000 (13%)] Loss: 16458.281250\n",
      "Train Epoch: 390 [32640/225000 (15%)] Loss: 16754.201172\n",
      "Train Epoch: 390 [35136/225000 (16%)] Loss: 16818.074219\n",
      "Train Epoch: 390 [37632/225000 (17%)] Loss: 16654.726562\n",
      "Train Epoch: 390 [40128/225000 (18%)] Loss: 16482.697266\n",
      "Train Epoch: 390 [42624/225000 (19%)] Loss: 16590.396484\n",
      "Train Epoch: 390 [45120/225000 (20%)] Loss: 16688.007812\n",
      "Train Epoch: 390 [47616/225000 (21%)] Loss: 16668.671875\n",
      "Train Epoch: 390 [50112/225000 (22%)] Loss: 16500.351562\n",
      "Train Epoch: 390 [52608/225000 (23%)] Loss: 16498.148438\n",
      "Train Epoch: 390 [55104/225000 (24%)] Loss: 16583.355469\n",
      "Train Epoch: 390 [57600/225000 (26%)] Loss: 16868.691406\n",
      "Train Epoch: 390 [60096/225000 (27%)] Loss: 16880.871094\n",
      "Train Epoch: 390 [62592/225000 (28%)] Loss: 16616.261719\n",
      "Train Epoch: 390 [65088/225000 (29%)] Loss: 16809.406250\n",
      "Train Epoch: 390 [67584/225000 (30%)] Loss: 16908.136719\n",
      "Train Epoch: 390 [70080/225000 (31%)] Loss: 16150.998047\n",
      "Train Epoch: 390 [72576/225000 (32%)] Loss: 16580.222656\n",
      "Train Epoch: 390 [75072/225000 (33%)] Loss: 16798.294922\n",
      "Train Epoch: 390 [77568/225000 (34%)] Loss: 16893.664062\n",
      "Train Epoch: 390 [80064/225000 (36%)] Loss: 16780.027344\n",
      "Train Epoch: 390 [82560/225000 (37%)] Loss: 16881.132812\n",
      "Train Epoch: 390 [85056/225000 (38%)] Loss: 16951.300781\n",
      "Train Epoch: 390 [87552/225000 (39%)] Loss: 18100.353516\n",
      "Train Epoch: 390 [90048/225000 (40%)] Loss: 16542.894531\n",
      "Train Epoch: 390 [92544/225000 (41%)] Loss: 16521.750000\n",
      "Train Epoch: 390 [95040/225000 (42%)] Loss: 16499.263672\n",
      "Train Epoch: 390 [97536/225000 (43%)] Loss: 16735.292969\n",
      "Train Epoch: 390 [100032/225000 (44%)] Loss: 17857.121094\n",
      "Train Epoch: 390 [102528/225000 (46%)] Loss: 18113.378906\n",
      "Train Epoch: 390 [105024/225000 (47%)] Loss: 16585.269531\n",
      "Train Epoch: 390 [107520/225000 (48%)] Loss: 17026.593750\n",
      "Train Epoch: 390 [110016/225000 (49%)] Loss: 16745.808594\n",
      "Train Epoch: 390 [112512/225000 (50%)] Loss: 16575.351562\n",
      "Train Epoch: 390 [115008/225000 (51%)] Loss: 18339.515625\n",
      "Train Epoch: 390 [117504/225000 (52%)] Loss: 16967.878906\n",
      "Train Epoch: 390 [120000/225000 (53%)] Loss: 16891.027344\n",
      "Train Epoch: 390 [122496/225000 (54%)] Loss: 16968.230469\n",
      "Train Epoch: 390 [124992/225000 (56%)] Loss: 16483.093750\n",
      "Train Epoch: 390 [127488/225000 (57%)] Loss: 16848.667969\n",
      "Train Epoch: 390 [129984/225000 (58%)] Loss: 18183.373047\n",
      "Train Epoch: 390 [132480/225000 (59%)] Loss: 17074.169922\n",
      "Train Epoch: 390 [134976/225000 (60%)] Loss: 16646.734375\n",
      "Train Epoch: 390 [137472/225000 (61%)] Loss: 16559.207031\n",
      "Train Epoch: 390 [139968/225000 (62%)] Loss: 16382.483398\n",
      "Train Epoch: 390 [142464/225000 (63%)] Loss: 16534.453125\n",
      "Train Epoch: 390 [144960/225000 (64%)] Loss: 16706.628906\n",
      "Train Epoch: 390 [147456/225000 (66%)] Loss: 16414.402344\n",
      "Train Epoch: 390 [149952/225000 (67%)] Loss: 16205.600586\n",
      "Train Epoch: 390 [152448/225000 (68%)] Loss: 16548.312500\n",
      "Train Epoch: 390 [154944/225000 (69%)] Loss: 16858.558594\n",
      "Train Epoch: 390 [157440/225000 (70%)] Loss: 16748.628906\n",
      "Train Epoch: 390 [159936/225000 (71%)] Loss: 16320.974609\n",
      "Train Epoch: 390 [162432/225000 (72%)] Loss: 16743.089844\n",
      "Train Epoch: 390 [164928/225000 (73%)] Loss: 16550.226562\n",
      "Train Epoch: 390 [167424/225000 (74%)] Loss: 16774.132812\n",
      "Train Epoch: 390 [169920/225000 (76%)] Loss: 16941.748047\n",
      "Train Epoch: 390 [172416/225000 (77%)] Loss: 16455.296875\n",
      "Train Epoch: 390 [174912/225000 (78%)] Loss: 16618.144531\n",
      "Train Epoch: 390 [177408/225000 (79%)] Loss: 16456.076172\n",
      "Train Epoch: 390 [179904/225000 (80%)] Loss: 16259.551758\n",
      "Train Epoch: 390 [182400/225000 (81%)] Loss: 16720.314453\n",
      "Train Epoch: 390 [184896/225000 (82%)] Loss: 16486.003906\n",
      "Train Epoch: 390 [187392/225000 (83%)] Loss: 16761.134766\n",
      "Train Epoch: 390 [189888/225000 (84%)] Loss: 16993.160156\n",
      "Train Epoch: 390 [192384/225000 (86%)] Loss: 16573.296875\n",
      "Train Epoch: 390 [194880/225000 (87%)] Loss: 16208.282227\n",
      "Train Epoch: 390 [197376/225000 (88%)] Loss: 16496.933594\n",
      "Train Epoch: 390 [199872/225000 (89%)] Loss: 16868.712891\n",
      "Train Epoch: 390 [202368/225000 (90%)] Loss: 16371.797852\n",
      "Train Epoch: 390 [204864/225000 (91%)] Loss: 16679.857422\n",
      "Train Epoch: 390 [207360/225000 (92%)] Loss: 16615.164062\n",
      "Train Epoch: 390 [209856/225000 (93%)] Loss: 16506.515625\n",
      "Train Epoch: 390 [212352/225000 (94%)] Loss: 16570.769531\n",
      "Train Epoch: 390 [214848/225000 (95%)] Loss: 16826.253906\n",
      "Train Epoch: 390 [217344/225000 (97%)] Loss: 17128.484375\n",
      "Train Epoch: 390 [219840/225000 (98%)] Loss: 17101.429688\n",
      "Train Epoch: 390 [222336/225000 (99%)] Loss: 16394.484375\n",
      "Train Epoch: 390 [224832/225000 (100%)] Loss: 16720.363281\n",
      "    epoch          : 390\n",
      "    loss           : 16735.411012825298\n",
      "    val_loss       : 16670.147741771838\n",
      "Train Epoch: 391 [192/225000 (0%)] Loss: 16336.450195\n",
      "Train Epoch: 391 [2688/225000 (1%)] Loss: 16557.287109\n",
      "Train Epoch: 391 [5184/225000 (2%)] Loss: 16250.590820\n",
      "Train Epoch: 391 [7680/225000 (3%)] Loss: 16266.683594\n",
      "Train Epoch: 391 [10176/225000 (5%)] Loss: 16562.851562\n",
      "Train Epoch: 391 [12672/225000 (6%)] Loss: 18112.593750\n",
      "Train Epoch: 391 [15168/225000 (7%)] Loss: 16884.117188\n",
      "Train Epoch: 391 [17664/225000 (8%)] Loss: 16272.547852\n",
      "Train Epoch: 391 [20160/225000 (9%)] Loss: 16734.611328\n",
      "Train Epoch: 391 [22656/225000 (10%)] Loss: 16408.751953\n",
      "Train Epoch: 391 [25152/225000 (11%)] Loss: 16864.722656\n",
      "Train Epoch: 391 [27648/225000 (12%)] Loss: 17079.021484\n",
      "Train Epoch: 391 [30144/225000 (13%)] Loss: 16618.417969\n",
      "Train Epoch: 391 [32640/225000 (15%)] Loss: 16666.654297\n",
      "Train Epoch: 391 [35136/225000 (16%)] Loss: 16824.839844\n",
      "Train Epoch: 391 [37632/225000 (17%)] Loss: 16909.046875\n",
      "Train Epoch: 391 [40128/225000 (18%)] Loss: 16632.353516\n",
      "Train Epoch: 391 [42624/225000 (19%)] Loss: 16920.136719\n",
      "Train Epoch: 391 [45120/225000 (20%)] Loss: 16179.218750\n",
      "Train Epoch: 391 [47616/225000 (21%)] Loss: 16433.253906\n",
      "Train Epoch: 391 [50112/225000 (22%)] Loss: 16558.199219\n",
      "Train Epoch: 391 [52608/225000 (23%)] Loss: 16658.269531\n",
      "Train Epoch: 391 [55104/225000 (24%)] Loss: 16654.003906\n",
      "Train Epoch: 391 [57600/225000 (26%)] Loss: 16815.320312\n",
      "Train Epoch: 391 [60096/225000 (27%)] Loss: 16597.707031\n",
      "Train Epoch: 391 [62592/225000 (28%)] Loss: 17068.201172\n",
      "Train Epoch: 391 [65088/225000 (29%)] Loss: 16623.902344\n",
      "Train Epoch: 391 [67584/225000 (30%)] Loss: 16536.867188\n",
      "Train Epoch: 391 [70080/225000 (31%)] Loss: 16712.876953\n",
      "Train Epoch: 391 [72576/225000 (32%)] Loss: 16522.804688\n",
      "Train Epoch: 391 [75072/225000 (33%)] Loss: 16327.049805\n",
      "Train Epoch: 391 [77568/225000 (34%)] Loss: 16862.472656\n",
      "Train Epoch: 391 [80064/225000 (36%)] Loss: 16728.113281\n",
      "Train Epoch: 391 [82560/225000 (37%)] Loss: 16569.517578\n",
      "Train Epoch: 391 [85056/225000 (38%)] Loss: 16491.160156\n",
      "Train Epoch: 391 [87552/225000 (39%)] Loss: 16132.366211\n",
      "Train Epoch: 391 [90048/225000 (40%)] Loss: 16818.949219\n",
      "Train Epoch: 391 [92544/225000 (41%)] Loss: 16600.519531\n",
      "Train Epoch: 391 [95040/225000 (42%)] Loss: 16651.886719\n",
      "Train Epoch: 391 [97536/225000 (43%)] Loss: 16569.835938\n",
      "Train Epoch: 391 [100032/225000 (44%)] Loss: 17024.648438\n",
      "Train Epoch: 391 [102528/225000 (46%)] Loss: 16870.000000\n",
      "Train Epoch: 391 [105024/225000 (47%)] Loss: 16448.347656\n",
      "Train Epoch: 391 [107520/225000 (48%)] Loss: 16783.667969\n",
      "Train Epoch: 391 [110016/225000 (49%)] Loss: 16688.896484\n",
      "Train Epoch: 391 [112512/225000 (50%)] Loss: 16738.847656\n",
      "Train Epoch: 391 [115008/225000 (51%)] Loss: 16754.308594\n",
      "Train Epoch: 391 [117504/225000 (52%)] Loss: 16360.092773\n",
      "Train Epoch: 391 [120000/225000 (53%)] Loss: 16478.367188\n",
      "Train Epoch: 391 [122496/225000 (54%)] Loss: 16862.890625\n",
      "Train Epoch: 391 [124992/225000 (56%)] Loss: 16369.134766\n",
      "Train Epoch: 391 [127488/225000 (57%)] Loss: 16928.792969\n",
      "Train Epoch: 391 [129984/225000 (58%)] Loss: 16747.613281\n",
      "Train Epoch: 391 [132480/225000 (59%)] Loss: 16645.921875\n",
      "Train Epoch: 391 [134976/225000 (60%)] Loss: 16703.957031\n",
      "Train Epoch: 391 [137472/225000 (61%)] Loss: 16712.171875\n",
      "Train Epoch: 391 [139968/225000 (62%)] Loss: 16664.207031\n",
      "Train Epoch: 391 [142464/225000 (63%)] Loss: 16757.675781\n",
      "Train Epoch: 391 [144960/225000 (64%)] Loss: 16749.093750\n",
      "Train Epoch: 391 [147456/225000 (66%)] Loss: 16505.476562\n",
      "Train Epoch: 391 [149952/225000 (67%)] Loss: 16831.392578\n",
      "Train Epoch: 391 [152448/225000 (68%)] Loss: 16680.179688\n",
      "Train Epoch: 391 [154944/225000 (69%)] Loss: 16652.808594\n",
      "Train Epoch: 391 [157440/225000 (70%)] Loss: 16783.468750\n",
      "Train Epoch: 391 [159936/225000 (71%)] Loss: 16381.141602\n",
      "Train Epoch: 391 [162432/225000 (72%)] Loss: 16393.855469\n",
      "Train Epoch: 391 [164928/225000 (73%)] Loss: 16632.632812\n",
      "Train Epoch: 391 [167424/225000 (74%)] Loss: 16501.187500\n",
      "Train Epoch: 391 [169920/225000 (76%)] Loss: 17512.101562\n",
      "Train Epoch: 391 [172416/225000 (77%)] Loss: 16612.953125\n",
      "Train Epoch: 391 [174912/225000 (78%)] Loss: 16649.339844\n",
      "Train Epoch: 391 [177408/225000 (79%)] Loss: 16254.912109\n",
      "Train Epoch: 391 [179904/225000 (80%)] Loss: 16593.554688\n",
      "Train Epoch: 391 [182400/225000 (81%)] Loss: 17004.589844\n",
      "Train Epoch: 391 [184896/225000 (82%)] Loss: 16386.972656\n",
      "Train Epoch: 391 [187392/225000 (83%)] Loss: 16451.611328\n",
      "Train Epoch: 391 [189888/225000 (84%)] Loss: 16981.957031\n",
      "Train Epoch: 391 [192384/225000 (86%)] Loss: 16607.000000\n",
      "Train Epoch: 391 [194880/225000 (87%)] Loss: 16371.164062\n",
      "Train Epoch: 391 [197376/225000 (88%)] Loss: 16833.771484\n",
      "Train Epoch: 391 [199872/225000 (89%)] Loss: 16633.683594\n",
      "Train Epoch: 391 [202368/225000 (90%)] Loss: 16881.128906\n",
      "Train Epoch: 391 [204864/225000 (91%)] Loss: 15942.155273\n",
      "Train Epoch: 391 [207360/225000 (92%)] Loss: 18880.839844\n",
      "Train Epoch: 391 [209856/225000 (93%)] Loss: 16473.121094\n",
      "Train Epoch: 391 [212352/225000 (94%)] Loss: 16470.031250\n",
      "Train Epoch: 391 [214848/225000 (95%)] Loss: 16392.031250\n",
      "Train Epoch: 391 [217344/225000 (97%)] Loss: 16531.425781\n",
      "Train Epoch: 391 [219840/225000 (98%)] Loss: 15955.059570\n",
      "Train Epoch: 391 [222336/225000 (99%)] Loss: 16626.281250\n",
      "Train Epoch: 391 [224832/225000 (100%)] Loss: 17856.601562\n",
      "    epoch          : 391\n",
      "    loss           : 16705.091246100415\n",
      "    val_loss       : 16624.342599235417\n",
      "Train Epoch: 392 [192/225000 (0%)] Loss: 16467.437500\n",
      "Train Epoch: 392 [2688/225000 (1%)] Loss: 16696.816406\n",
      "Train Epoch: 392 [5184/225000 (2%)] Loss: 16747.218750\n",
      "Train Epoch: 392 [7680/225000 (3%)] Loss: 16835.476562\n",
      "Train Epoch: 392 [10176/225000 (5%)] Loss: 16428.457031\n",
      "Train Epoch: 392 [12672/225000 (6%)] Loss: 16433.714844\n",
      "Train Epoch: 392 [15168/225000 (7%)] Loss: 16626.328125\n",
      "Train Epoch: 392 [17664/225000 (8%)] Loss: 16859.783203\n",
      "Train Epoch: 392 [20160/225000 (9%)] Loss: 16411.566406\n",
      "Train Epoch: 392 [22656/225000 (10%)] Loss: 16290.866211\n",
      "Train Epoch: 392 [25152/225000 (11%)] Loss: 16469.628906\n",
      "Train Epoch: 392 [27648/225000 (12%)] Loss: 16427.070312\n",
      "Train Epoch: 392 [30144/225000 (13%)] Loss: 16785.773438\n",
      "Train Epoch: 392 [32640/225000 (15%)] Loss: 17334.300781\n",
      "Train Epoch: 392 [35136/225000 (16%)] Loss: 16756.501953\n",
      "Train Epoch: 392 [37632/225000 (17%)] Loss: 16571.015625\n",
      "Train Epoch: 392 [40128/225000 (18%)] Loss: 16673.566406\n",
      "Train Epoch: 392 [42624/225000 (19%)] Loss: 16408.775391\n",
      "Train Epoch: 392 [45120/225000 (20%)] Loss: 16052.039062\n",
      "Train Epoch: 392 [47616/225000 (21%)] Loss: 16870.285156\n",
      "Train Epoch: 392 [50112/225000 (22%)] Loss: 16545.562500\n",
      "Train Epoch: 392 [52608/225000 (23%)] Loss: 16817.738281\n",
      "Train Epoch: 392 [55104/225000 (24%)] Loss: 16651.310547\n",
      "Train Epoch: 392 [57600/225000 (26%)] Loss: 16555.089844\n",
      "Train Epoch: 392 [60096/225000 (27%)] Loss: 17147.607422\n",
      "Train Epoch: 392 [62592/225000 (28%)] Loss: 16771.748047\n",
      "Train Epoch: 392 [65088/225000 (29%)] Loss: 16707.964844\n",
      "Train Epoch: 392 [67584/225000 (30%)] Loss: 17025.855469\n",
      "Train Epoch: 392 [70080/225000 (31%)] Loss: 17084.265625\n",
      "Train Epoch: 392 [72576/225000 (32%)] Loss: 16817.958984\n",
      "Train Epoch: 392 [75072/225000 (33%)] Loss: 16730.019531\n",
      "Train Epoch: 392 [77568/225000 (34%)] Loss: 16414.921875\n",
      "Train Epoch: 392 [80064/225000 (36%)] Loss: 16454.843750\n",
      "Train Epoch: 392 [82560/225000 (37%)] Loss: 16936.656250\n",
      "Train Epoch: 392 [85056/225000 (38%)] Loss: 16531.652344\n",
      "Train Epoch: 392 [87552/225000 (39%)] Loss: 16650.597656\n",
      "Train Epoch: 392 [90048/225000 (40%)] Loss: 16730.863281\n",
      "Train Epoch: 392 [92544/225000 (41%)] Loss: 17138.113281\n",
      "Train Epoch: 392 [95040/225000 (42%)] Loss: 16723.882812\n",
      "Train Epoch: 392 [97536/225000 (43%)] Loss: 16491.609375\n",
      "Train Epoch: 392 [100032/225000 (44%)] Loss: 16813.480469\n",
      "Train Epoch: 392 [102528/225000 (46%)] Loss: 16456.128906\n",
      "Train Epoch: 392 [105024/225000 (47%)] Loss: 16658.521484\n",
      "Train Epoch: 392 [107520/225000 (48%)] Loss: 16123.495117\n",
      "Train Epoch: 392 [110016/225000 (49%)] Loss: 16868.574219\n",
      "Train Epoch: 392 [112512/225000 (50%)] Loss: 16665.136719\n",
      "Train Epoch: 392 [115008/225000 (51%)] Loss: 16497.167969\n",
      "Train Epoch: 392 [117504/225000 (52%)] Loss: 16653.189453\n",
      "Train Epoch: 392 [120000/225000 (53%)] Loss: 17089.378906\n",
      "Train Epoch: 392 [122496/225000 (54%)] Loss: 16690.585938\n",
      "Train Epoch: 392 [124992/225000 (56%)] Loss: 16566.539062\n",
      "Train Epoch: 392 [127488/225000 (57%)] Loss: 16304.703125\n",
      "Train Epoch: 392 [129984/225000 (58%)] Loss: 16932.203125\n",
      "Train Epoch: 392 [132480/225000 (59%)] Loss: 16274.497070\n",
      "Train Epoch: 392 [134976/225000 (60%)] Loss: 16793.093750\n",
      "Train Epoch: 392 [137472/225000 (61%)] Loss: 16627.734375\n",
      "Train Epoch: 392 [139968/225000 (62%)] Loss: 16790.246094\n",
      "Train Epoch: 392 [142464/225000 (63%)] Loss: 16551.492188\n",
      "Train Epoch: 392 [144960/225000 (64%)] Loss: 16794.449219\n",
      "Train Epoch: 392 [147456/225000 (66%)] Loss: 16787.824219\n",
      "Train Epoch: 392 [149952/225000 (67%)] Loss: 16593.935547\n",
      "Train Epoch: 392 [152448/225000 (68%)] Loss: 16763.851562\n",
      "Train Epoch: 392 [154944/225000 (69%)] Loss: 16581.628906\n",
      "Train Epoch: 392 [157440/225000 (70%)] Loss: 18672.537109\n",
      "Train Epoch: 392 [159936/225000 (71%)] Loss: 16818.347656\n",
      "Train Epoch: 392 [162432/225000 (72%)] Loss: 18192.410156\n",
      "Train Epoch: 392 [164928/225000 (73%)] Loss: 16467.437500\n",
      "Train Epoch: 392 [167424/225000 (74%)] Loss: 16512.613281\n",
      "Train Epoch: 392 [169920/225000 (76%)] Loss: 16515.687500\n",
      "Train Epoch: 392 [172416/225000 (77%)] Loss: 16703.828125\n",
      "Train Epoch: 392 [174912/225000 (78%)] Loss: 16305.531250\n",
      "Train Epoch: 392 [177408/225000 (79%)] Loss: 17146.722656\n",
      "Train Epoch: 392 [179904/225000 (80%)] Loss: 16167.083008\n",
      "Train Epoch: 392 [182400/225000 (81%)] Loss: 15894.060547\n",
      "Train Epoch: 392 [184896/225000 (82%)] Loss: 16471.777344\n",
      "Train Epoch: 392 [187392/225000 (83%)] Loss: 16611.695312\n",
      "Train Epoch: 392 [189888/225000 (84%)] Loss: 16845.789062\n",
      "Train Epoch: 392 [192384/225000 (86%)] Loss: 17039.601562\n",
      "Train Epoch: 392 [194880/225000 (87%)] Loss: 16437.222656\n",
      "Train Epoch: 392 [197376/225000 (88%)] Loss: 18156.500000\n",
      "Train Epoch: 392 [199872/225000 (89%)] Loss: 16609.621094\n",
      "Train Epoch: 392 [202368/225000 (90%)] Loss: 16923.406250\n",
      "Train Epoch: 392 [204864/225000 (91%)] Loss: 16132.911133\n",
      "Train Epoch: 392 [207360/225000 (92%)] Loss: 16524.546875\n",
      "Train Epoch: 392 [209856/225000 (93%)] Loss: 16665.400391\n",
      "Train Epoch: 392 [212352/225000 (94%)] Loss: 16662.800781\n",
      "Train Epoch: 392 [214848/225000 (95%)] Loss: 16526.822266\n",
      "Train Epoch: 392 [217344/225000 (97%)] Loss: 17804.910156\n",
      "Train Epoch: 392 [219840/225000 (98%)] Loss: 16329.062500\n",
      "Train Epoch: 392 [222336/225000 (99%)] Loss: 16704.876953\n",
      "Train Epoch: 392 [224832/225000 (100%)] Loss: 17013.304688\n",
      "    epoch          : 392\n",
      "    loss           : 16757.985217410143\n",
      "    val_loss       : 16735.646773531236\n",
      "Train Epoch: 393 [192/225000 (0%)] Loss: 16863.753906\n",
      "Train Epoch: 393 [2688/225000 (1%)] Loss: 16487.437500\n",
      "Train Epoch: 393 [5184/225000 (2%)] Loss: 16511.769531\n",
      "Train Epoch: 393 [7680/225000 (3%)] Loss: 16784.175781\n",
      "Train Epoch: 393 [10176/225000 (5%)] Loss: 16502.171875\n",
      "Train Epoch: 393 [12672/225000 (6%)] Loss: 16627.078125\n",
      "Train Epoch: 393 [15168/225000 (7%)] Loss: 16161.959961\n",
      "Train Epoch: 393 [17664/225000 (8%)] Loss: 16716.216797\n",
      "Train Epoch: 393 [20160/225000 (9%)] Loss: 16148.832031\n",
      "Train Epoch: 393 [22656/225000 (10%)] Loss: 16290.294922\n",
      "Train Epoch: 393 [25152/225000 (11%)] Loss: 16930.103516\n",
      "Train Epoch: 393 [27648/225000 (12%)] Loss: 16806.683594\n",
      "Train Epoch: 393 [30144/225000 (13%)] Loss: 16580.734375\n",
      "Train Epoch: 393 [32640/225000 (15%)] Loss: 16083.130859\n",
      "Train Epoch: 393 [35136/225000 (16%)] Loss: 16857.863281\n",
      "Train Epoch: 393 [37632/225000 (17%)] Loss: 16917.574219\n",
      "Train Epoch: 393 [40128/225000 (18%)] Loss: 17071.287109\n",
      "Train Epoch: 393 [42624/225000 (19%)] Loss: 16493.828125\n",
      "Train Epoch: 393 [45120/225000 (20%)] Loss: 16917.050781\n",
      "Train Epoch: 393 [47616/225000 (21%)] Loss: 16434.812500\n",
      "Train Epoch: 393 [50112/225000 (22%)] Loss: 16752.011719\n",
      "Train Epoch: 393 [52608/225000 (23%)] Loss: 16704.125000\n",
      "Train Epoch: 393 [55104/225000 (24%)] Loss: 16352.868164\n",
      "Train Epoch: 393 [57600/225000 (26%)] Loss: 16659.453125\n",
      "Train Epoch: 393 [60096/225000 (27%)] Loss: 16203.734375\n",
      "Train Epoch: 393 [62592/225000 (28%)] Loss: 16792.201172\n",
      "Train Epoch: 393 [65088/225000 (29%)] Loss: 16232.280273\n",
      "Train Epoch: 393 [67584/225000 (30%)] Loss: 16857.039062\n",
      "Train Epoch: 393 [70080/225000 (31%)] Loss: 16842.548828\n",
      "Train Epoch: 393 [72576/225000 (32%)] Loss: 16891.650391\n",
      "Train Epoch: 393 [75072/225000 (33%)] Loss: 16657.970703\n",
      "Train Epoch: 393 [77568/225000 (34%)] Loss: 16653.597656\n",
      "Train Epoch: 393 [80064/225000 (36%)] Loss: 16806.210938\n",
      "Train Epoch: 393 [82560/225000 (37%)] Loss: 16712.244141\n",
      "Train Epoch: 393 [85056/225000 (38%)] Loss: 16729.085938\n",
      "Train Epoch: 393 [87552/225000 (39%)] Loss: 16544.816406\n",
      "Train Epoch: 393 [90048/225000 (40%)] Loss: 18430.285156\n",
      "Train Epoch: 393 [92544/225000 (41%)] Loss: 16440.689453\n",
      "Train Epoch: 393 [95040/225000 (42%)] Loss: 16636.275391\n",
      "Train Epoch: 393 [97536/225000 (43%)] Loss: 16744.175781\n",
      "Train Epoch: 393 [100032/225000 (44%)] Loss: 16931.890625\n",
      "Train Epoch: 393 [102528/225000 (46%)] Loss: 16548.025391\n",
      "Train Epoch: 393 [105024/225000 (47%)] Loss: 17001.212891\n",
      "Train Epoch: 393 [107520/225000 (48%)] Loss: 16034.553711\n",
      "Train Epoch: 393 [110016/225000 (49%)] Loss: 16633.460938\n",
      "Train Epoch: 393 [112512/225000 (50%)] Loss: 16267.989258\n",
      "Train Epoch: 393 [115008/225000 (51%)] Loss: 16771.824219\n",
      "Train Epoch: 393 [117504/225000 (52%)] Loss: 16952.960938\n",
      "Train Epoch: 393 [120000/225000 (53%)] Loss: 17050.556641\n",
      "Train Epoch: 393 [122496/225000 (54%)] Loss: 16991.988281\n",
      "Train Epoch: 393 [124992/225000 (56%)] Loss: 16811.964844\n",
      "Train Epoch: 393 [127488/225000 (57%)] Loss: 16538.765625\n",
      "Train Epoch: 393 [129984/225000 (58%)] Loss: 16205.419922\n",
      "Train Epoch: 393 [132480/225000 (59%)] Loss: 16763.257812\n",
      "Train Epoch: 393 [134976/225000 (60%)] Loss: 16609.347656\n",
      "Train Epoch: 393 [137472/225000 (61%)] Loss: 16843.898438\n",
      "Train Epoch: 393 [139968/225000 (62%)] Loss: 16468.888672\n",
      "Train Epoch: 393 [142464/225000 (63%)] Loss: 16664.148438\n",
      "Train Epoch: 393 [144960/225000 (64%)] Loss: 16885.066406\n",
      "Train Epoch: 393 [147456/225000 (66%)] Loss: 16975.066406\n",
      "Train Epoch: 393 [149952/225000 (67%)] Loss: 17567.300781\n",
      "Train Epoch: 393 [152448/225000 (68%)] Loss: 15956.823242\n",
      "Train Epoch: 393 [154944/225000 (69%)] Loss: 16382.387695\n",
      "Train Epoch: 393 [157440/225000 (70%)] Loss: 16947.957031\n",
      "Train Epoch: 393 [159936/225000 (71%)] Loss: 16427.980469\n",
      "Train Epoch: 393 [162432/225000 (72%)] Loss: 16977.435547\n",
      "Train Epoch: 393 [164928/225000 (73%)] Loss: 16421.833984\n",
      "Train Epoch: 393 [167424/225000 (74%)] Loss: 17004.933594\n",
      "Train Epoch: 393 [169920/225000 (76%)] Loss: 16922.744141\n",
      "Train Epoch: 393 [172416/225000 (77%)] Loss: 16967.921875\n",
      "Train Epoch: 393 [174912/225000 (78%)] Loss: 16751.232422\n",
      "Train Epoch: 393 [177408/225000 (79%)] Loss: 17815.107422\n",
      "Train Epoch: 393 [179904/225000 (80%)] Loss: 16526.720703\n",
      "Train Epoch: 393 [182400/225000 (81%)] Loss: 16371.568359\n",
      "Train Epoch: 393 [184896/225000 (82%)] Loss: 17024.593750\n",
      "Train Epoch: 393 [187392/225000 (83%)] Loss: 16991.648438\n",
      "Train Epoch: 393 [189888/225000 (84%)] Loss: 16859.394531\n",
      "Train Epoch: 393 [192384/225000 (86%)] Loss: 16565.269531\n",
      "Train Epoch: 393 [194880/225000 (87%)] Loss: 16399.675781\n",
      "Train Epoch: 393 [197376/225000 (88%)] Loss: 16668.785156\n",
      "Train Epoch: 393 [199872/225000 (89%)] Loss: 16553.023438\n",
      "Train Epoch: 393 [202368/225000 (90%)] Loss: 16201.470703\n",
      "Train Epoch: 393 [204864/225000 (91%)] Loss: 16769.193359\n",
      "Train Epoch: 393 [207360/225000 (92%)] Loss: 18075.691406\n",
      "Train Epoch: 393 [209856/225000 (93%)] Loss: 16637.695312\n",
      "Train Epoch: 393 [212352/225000 (94%)] Loss: 17136.115234\n",
      "Train Epoch: 393 [214848/225000 (95%)] Loss: 16935.921875\n",
      "Train Epoch: 393 [217344/225000 (97%)] Loss: 16337.128906\n",
      "Train Epoch: 393 [219840/225000 (98%)] Loss: 17032.191406\n",
      "Train Epoch: 393 [222336/225000 (99%)] Loss: 16635.378906\n",
      "Train Epoch: 393 [224832/225000 (100%)] Loss: 16678.134766\n",
      "    epoch          : 393\n",
      "    loss           : 16715.058307113908\n",
      "    val_loss       : 16682.978395748683\n",
      "Train Epoch: 394 [192/225000 (0%)] Loss: 16796.865234\n",
      "Train Epoch: 394 [2688/225000 (1%)] Loss: 17052.839844\n",
      "Train Epoch: 394 [5184/225000 (2%)] Loss: 16971.496094\n",
      "Train Epoch: 394 [7680/225000 (3%)] Loss: 16831.882812\n",
      "Train Epoch: 394 [10176/225000 (5%)] Loss: 16406.402344\n",
      "Train Epoch: 394 [12672/225000 (6%)] Loss: 16454.117188\n",
      "Train Epoch: 394 [15168/225000 (7%)] Loss: 16682.705078\n",
      "Train Epoch: 394 [17664/225000 (8%)] Loss: 16655.484375\n",
      "Train Epoch: 394 [20160/225000 (9%)] Loss: 16813.761719\n",
      "Train Epoch: 394 [22656/225000 (10%)] Loss: 16365.158203\n",
      "Train Epoch: 394 [25152/225000 (11%)] Loss: 16502.699219\n",
      "Train Epoch: 394 [27648/225000 (12%)] Loss: 16391.773438\n",
      "Train Epoch: 394 [30144/225000 (13%)] Loss: 16263.163086\n",
      "Train Epoch: 394 [32640/225000 (15%)] Loss: 16571.261719\n",
      "Train Epoch: 394 [35136/225000 (16%)] Loss: 16558.929688\n",
      "Train Epoch: 394 [37632/225000 (17%)] Loss: 17155.974609\n",
      "Train Epoch: 394 [40128/225000 (18%)] Loss: 17156.695312\n",
      "Train Epoch: 394 [42624/225000 (19%)] Loss: 16882.972656\n",
      "Train Epoch: 394 [45120/225000 (20%)] Loss: 16690.988281\n",
      "Train Epoch: 394 [47616/225000 (21%)] Loss: 16599.667969\n",
      "Train Epoch: 394 [50112/225000 (22%)] Loss: 16635.332031\n",
      "Train Epoch: 394 [52608/225000 (23%)] Loss: 16990.664062\n",
      "Train Epoch: 394 [55104/225000 (24%)] Loss: 16345.159180\n",
      "Train Epoch: 394 [57600/225000 (26%)] Loss: 16983.617188\n",
      "Train Epoch: 394 [60096/225000 (27%)] Loss: 16520.568359\n",
      "Train Epoch: 394 [62592/225000 (28%)] Loss: 16491.730469\n",
      "Train Epoch: 394 [65088/225000 (29%)] Loss: 16700.814453\n",
      "Train Epoch: 394 [67584/225000 (30%)] Loss: 16261.667969\n",
      "Train Epoch: 394 [70080/225000 (31%)] Loss: 16135.460938\n",
      "Train Epoch: 394 [72576/225000 (32%)] Loss: 17051.789062\n",
      "Train Epoch: 394 [75072/225000 (33%)] Loss: 16540.054688\n",
      "Train Epoch: 394 [77568/225000 (34%)] Loss: 16500.929688\n",
      "Train Epoch: 394 [80064/225000 (36%)] Loss: 16417.691406\n",
      "Train Epoch: 394 [82560/225000 (37%)] Loss: 16475.914062\n",
      "Train Epoch: 394 [85056/225000 (38%)] Loss: 16394.408203\n",
      "Train Epoch: 394 [87552/225000 (39%)] Loss: 16327.683594\n",
      "Train Epoch: 394 [90048/225000 (40%)] Loss: 16714.144531\n",
      "Train Epoch: 394 [92544/225000 (41%)] Loss: 16497.777344\n",
      "Train Epoch: 394 [95040/225000 (42%)] Loss: 16224.577148\n",
      "Train Epoch: 394 [97536/225000 (43%)] Loss: 24603.958984\n",
      "Train Epoch: 394 [100032/225000 (44%)] Loss: 17079.097656\n",
      "Train Epoch: 394 [102528/225000 (46%)] Loss: 17071.953125\n",
      "Train Epoch: 394 [105024/225000 (47%)] Loss: 16489.720703\n",
      "Train Epoch: 394 [107520/225000 (48%)] Loss: 16473.437500\n",
      "Train Epoch: 394 [110016/225000 (49%)] Loss: 16702.121094\n",
      "Train Epoch: 394 [112512/225000 (50%)] Loss: 16584.238281\n",
      "Train Epoch: 394 [115008/225000 (51%)] Loss: 16528.878906\n",
      "Train Epoch: 394 [117504/225000 (52%)] Loss: 16149.482422\n",
      "Train Epoch: 394 [120000/225000 (53%)] Loss: 16799.765625\n",
      "Train Epoch: 394 [122496/225000 (54%)] Loss: 16972.339844\n",
      "Train Epoch: 394 [124992/225000 (56%)] Loss: 16579.324219\n",
      "Train Epoch: 394 [127488/225000 (57%)] Loss: 16354.198242\n",
      "Train Epoch: 394 [129984/225000 (58%)] Loss: 16733.484375\n",
      "Train Epoch: 394 [132480/225000 (59%)] Loss: 17036.916016\n",
      "Train Epoch: 394 [134976/225000 (60%)] Loss: 18347.433594\n",
      "Train Epoch: 394 [137472/225000 (61%)] Loss: 16426.042969\n",
      "Train Epoch: 394 [139968/225000 (62%)] Loss: 16894.386719\n",
      "Train Epoch: 394 [142464/225000 (63%)] Loss: 16090.682617\n",
      "Train Epoch: 394 [144960/225000 (64%)] Loss: 16678.070312\n",
      "Train Epoch: 394 [147456/225000 (66%)] Loss: 16582.072266\n",
      "Train Epoch: 394 [149952/225000 (67%)] Loss: 16540.937500\n",
      "Train Epoch: 394 [152448/225000 (68%)] Loss: 16724.085938\n",
      "Train Epoch: 394 [154944/225000 (69%)] Loss: 16366.469727\n",
      "Train Epoch: 394 [157440/225000 (70%)] Loss: 16574.437500\n",
      "Train Epoch: 394 [159936/225000 (71%)] Loss: 16466.335938\n",
      "Train Epoch: 394 [162432/225000 (72%)] Loss: 16501.816406\n",
      "Train Epoch: 394 [164928/225000 (73%)] Loss: 16219.955078\n",
      "Train Epoch: 394 [167424/225000 (74%)] Loss: 16607.042969\n",
      "Train Epoch: 394 [169920/225000 (76%)] Loss: 16514.828125\n",
      "Train Epoch: 394 [172416/225000 (77%)] Loss: 16882.878906\n",
      "Train Epoch: 394 [174912/225000 (78%)] Loss: 16630.025391\n",
      "Train Epoch: 394 [177408/225000 (79%)] Loss: 16346.098633\n",
      "Train Epoch: 394 [179904/225000 (80%)] Loss: 16811.148438\n",
      "Train Epoch: 394 [182400/225000 (81%)] Loss: 16960.095703\n",
      "Train Epoch: 394 [184896/225000 (82%)] Loss: 16610.292969\n",
      "Train Epoch: 394 [187392/225000 (83%)] Loss: 18730.570312\n",
      "Train Epoch: 394 [189888/225000 (84%)] Loss: 16583.984375\n",
      "Train Epoch: 394 [192384/225000 (86%)] Loss: 16309.782227\n",
      "Train Epoch: 394 [194880/225000 (87%)] Loss: 16716.447266\n",
      "Train Epoch: 394 [197376/225000 (88%)] Loss: 16755.636719\n",
      "Train Epoch: 394 [199872/225000 (89%)] Loss: 16327.607422\n",
      "Train Epoch: 394 [202368/225000 (90%)] Loss: 16983.255859\n",
      "Train Epoch: 394 [204864/225000 (91%)] Loss: 16214.232422\n",
      "Train Epoch: 394 [207360/225000 (92%)] Loss: 16644.140625\n",
      "Train Epoch: 394 [209856/225000 (93%)] Loss: 16305.261719\n",
      "Train Epoch: 394 [212352/225000 (94%)] Loss: 16886.148438\n",
      "Train Epoch: 394 [214848/225000 (95%)] Loss: 16780.164062\n",
      "Train Epoch: 394 [217344/225000 (97%)] Loss: 16288.527344\n",
      "Train Epoch: 394 [219840/225000 (98%)] Loss: 16743.511719\n",
      "Train Epoch: 394 [222336/225000 (99%)] Loss: 17340.515625\n",
      "Train Epoch: 394 [224832/225000 (100%)] Loss: 16847.929688\n",
      "    epoch          : 394\n",
      "    loss           : 16742.820200845243\n",
      "    val_loss       : 16645.434099512247\n",
      "Train Epoch: 395 [192/225000 (0%)] Loss: 16803.095703\n",
      "Train Epoch: 395 [2688/225000 (1%)] Loss: 16547.574219\n",
      "Train Epoch: 395 [5184/225000 (2%)] Loss: 25298.880859\n",
      "Train Epoch: 395 [7680/225000 (3%)] Loss: 17041.681641\n",
      "Train Epoch: 395 [10176/225000 (5%)] Loss: 16288.368164\n",
      "Train Epoch: 395 [12672/225000 (6%)] Loss: 16685.804688\n",
      "Train Epoch: 395 [15168/225000 (7%)] Loss: 16482.316406\n",
      "Train Epoch: 395 [17664/225000 (8%)] Loss: 16538.376953\n",
      "Train Epoch: 395 [20160/225000 (9%)] Loss: 18409.210938\n",
      "Train Epoch: 395 [22656/225000 (10%)] Loss: 16380.004883\n",
      "Train Epoch: 395 [25152/225000 (11%)] Loss: 16723.531250\n",
      "Train Epoch: 395 [27648/225000 (12%)] Loss: 16294.294922\n",
      "Train Epoch: 395 [30144/225000 (13%)] Loss: 16486.867188\n",
      "Train Epoch: 395 [32640/225000 (15%)] Loss: 16855.515625\n",
      "Train Epoch: 395 [35136/225000 (16%)] Loss: 17778.453125\n",
      "Train Epoch: 395 [37632/225000 (17%)] Loss: 16752.613281\n",
      "Train Epoch: 395 [40128/225000 (18%)] Loss: 16625.875000\n",
      "Train Epoch: 395 [42624/225000 (19%)] Loss: 16708.128906\n",
      "Train Epoch: 395 [45120/225000 (20%)] Loss: 16505.083984\n",
      "Train Epoch: 395 [47616/225000 (21%)] Loss: 18208.572266\n",
      "Train Epoch: 395 [50112/225000 (22%)] Loss: 16478.964844\n",
      "Train Epoch: 395 [52608/225000 (23%)] Loss: 16291.230469\n",
      "Train Epoch: 395 [55104/225000 (24%)] Loss: 16732.941406\n",
      "Train Epoch: 395 [57600/225000 (26%)] Loss: 16293.332031\n",
      "Train Epoch: 395 [60096/225000 (27%)] Loss: 17212.111328\n",
      "Train Epoch: 395 [62592/225000 (28%)] Loss: 16727.699219\n",
      "Train Epoch: 395 [65088/225000 (29%)] Loss: 16516.093750\n",
      "Train Epoch: 395 [67584/225000 (30%)] Loss: 16531.812500\n",
      "Train Epoch: 395 [70080/225000 (31%)] Loss: 16418.320312\n",
      "Train Epoch: 395 [72576/225000 (32%)] Loss: 16925.074219\n",
      "Train Epoch: 395 [75072/225000 (33%)] Loss: 16567.582031\n",
      "Train Epoch: 395 [77568/225000 (34%)] Loss: 16504.570312\n",
      "Train Epoch: 395 [80064/225000 (36%)] Loss: 16769.347656\n",
      "Train Epoch: 395 [82560/225000 (37%)] Loss: 17310.744141\n",
      "Train Epoch: 395 [85056/225000 (38%)] Loss: 16173.193359\n",
      "Train Epoch: 395 [87552/225000 (39%)] Loss: 16827.871094\n",
      "Train Epoch: 395 [90048/225000 (40%)] Loss: 16653.058594\n",
      "Train Epoch: 395 [92544/225000 (41%)] Loss: 24907.505859\n",
      "Train Epoch: 395 [95040/225000 (42%)] Loss: 16576.216797\n",
      "Train Epoch: 395 [97536/225000 (43%)] Loss: 16831.939453\n",
      "Train Epoch: 395 [100032/225000 (44%)] Loss: 16403.093750\n",
      "Train Epoch: 395 [102528/225000 (46%)] Loss: 16308.806641\n",
      "Train Epoch: 395 [105024/225000 (47%)] Loss: 16517.212891\n",
      "Train Epoch: 395 [107520/225000 (48%)] Loss: 16389.937500\n",
      "Train Epoch: 395 [110016/225000 (49%)] Loss: 16587.246094\n",
      "Train Epoch: 395 [112512/225000 (50%)] Loss: 16832.654297\n",
      "Train Epoch: 395 [115008/225000 (51%)] Loss: 16736.406250\n",
      "Train Epoch: 395 [117504/225000 (52%)] Loss: 15870.728516\n",
      "Train Epoch: 395 [120000/225000 (53%)] Loss: 16680.597656\n",
      "Train Epoch: 395 [122496/225000 (54%)] Loss: 16838.925781\n",
      "Train Epoch: 395 [124992/225000 (56%)] Loss: 16744.427734\n",
      "Train Epoch: 395 [127488/225000 (57%)] Loss: 16881.189453\n",
      "Train Epoch: 395 [129984/225000 (58%)] Loss: 16321.910156\n",
      "Train Epoch: 395 [132480/225000 (59%)] Loss: 16850.070312\n",
      "Train Epoch: 395 [134976/225000 (60%)] Loss: 16691.677734\n",
      "Train Epoch: 395 [137472/225000 (61%)] Loss: 17006.968750\n",
      "Train Epoch: 395 [139968/225000 (62%)] Loss: 16495.109375\n",
      "Train Epoch: 395 [142464/225000 (63%)] Loss: 17018.421875\n",
      "Train Epoch: 395 [144960/225000 (64%)] Loss: 16936.085938\n",
      "Train Epoch: 395 [147456/225000 (66%)] Loss: 16758.480469\n",
      "Train Epoch: 395 [149952/225000 (67%)] Loss: 16884.199219\n",
      "Train Epoch: 395 [152448/225000 (68%)] Loss: 16941.642578\n",
      "Train Epoch: 395 [154944/225000 (69%)] Loss: 18320.583984\n",
      "Train Epoch: 395 [157440/225000 (70%)] Loss: 16371.767578\n",
      "Train Epoch: 395 [159936/225000 (71%)] Loss: 16537.578125\n",
      "Train Epoch: 395 [162432/225000 (72%)] Loss: 16495.328125\n",
      "Train Epoch: 395 [164928/225000 (73%)] Loss: 17134.003906\n",
      "Train Epoch: 395 [167424/225000 (74%)] Loss: 16739.822266\n",
      "Train Epoch: 395 [169920/225000 (76%)] Loss: 16437.808594\n",
      "Train Epoch: 395 [172416/225000 (77%)] Loss: 16606.808594\n",
      "Train Epoch: 395 [174912/225000 (78%)] Loss: 16571.000000\n",
      "Train Epoch: 395 [177408/225000 (79%)] Loss: 17109.234375\n",
      "Train Epoch: 395 [179904/225000 (80%)] Loss: 17206.773438\n",
      "Train Epoch: 395 [182400/225000 (81%)] Loss: 16892.228516\n",
      "Train Epoch: 395 [184896/225000 (82%)] Loss: 16304.492188\n",
      "Train Epoch: 395 [187392/225000 (83%)] Loss: 15967.420898\n",
      "Train Epoch: 395 [189888/225000 (84%)] Loss: 16417.648438\n",
      "Train Epoch: 395 [192384/225000 (86%)] Loss: 16289.258789\n",
      "Train Epoch: 395 [194880/225000 (87%)] Loss: 16550.421875\n",
      "Train Epoch: 395 [197376/225000 (88%)] Loss: 16789.130859\n",
      "Train Epoch: 395 [199872/225000 (89%)] Loss: 16614.027344\n",
      "Train Epoch: 395 [202368/225000 (90%)] Loss: 16182.610352\n",
      "Train Epoch: 395 [204864/225000 (91%)] Loss: 16361.966797\n",
      "Train Epoch: 395 [207360/225000 (92%)] Loss: 16846.328125\n",
      "Train Epoch: 395 [209856/225000 (93%)] Loss: 17080.492188\n",
      "Train Epoch: 395 [212352/225000 (94%)] Loss: 16914.513672\n",
      "Train Epoch: 395 [214848/225000 (95%)] Loss: 16796.265625\n",
      "Train Epoch: 395 [217344/225000 (97%)] Loss: 17198.859375\n",
      "Train Epoch: 395 [219840/225000 (98%)] Loss: 16610.843750\n",
      "Train Epoch: 395 [222336/225000 (99%)] Loss: 16301.233398\n",
      "Train Epoch: 395 [224832/225000 (100%)] Loss: 16016.046875\n",
      "    epoch          : 395\n",
      "    loss           : 16736.807490534342\n",
      "    val_loss       : 16641.501462035507\n",
      "Train Epoch: 396 [192/225000 (0%)] Loss: 16447.357422\n",
      "Train Epoch: 396 [2688/225000 (1%)] Loss: 16707.804688\n",
      "Train Epoch: 396 [5184/225000 (2%)] Loss: 15993.143555\n",
      "Train Epoch: 396 [7680/225000 (3%)] Loss: 16411.234375\n",
      "Train Epoch: 396 [10176/225000 (5%)] Loss: 16866.753906\n",
      "Train Epoch: 396 [12672/225000 (6%)] Loss: 16645.929688\n",
      "Train Epoch: 396 [15168/225000 (7%)] Loss: 16273.400391\n",
      "Train Epoch: 396 [17664/225000 (8%)] Loss: 16591.517578\n",
      "Train Epoch: 396 [20160/225000 (9%)] Loss: 16629.734375\n",
      "Train Epoch: 396 [22656/225000 (10%)] Loss: 16151.294922\n",
      "Train Epoch: 396 [25152/225000 (11%)] Loss: 16402.003906\n",
      "Train Epoch: 396 [27648/225000 (12%)] Loss: 16817.250000\n",
      "Train Epoch: 396 [30144/225000 (13%)] Loss: 16610.964844\n",
      "Train Epoch: 396 [32640/225000 (15%)] Loss: 16609.128906\n",
      "Train Epoch: 396 [35136/225000 (16%)] Loss: 16595.167969\n",
      "Train Epoch: 396 [37632/225000 (17%)] Loss: 16733.449219\n",
      "Train Epoch: 396 [40128/225000 (18%)] Loss: 16554.972656\n",
      "Train Epoch: 396 [42624/225000 (19%)] Loss: 16340.615234\n",
      "Train Epoch: 396 [45120/225000 (20%)] Loss: 18019.287109\n",
      "Train Epoch: 396 [47616/225000 (21%)] Loss: 16675.148438\n",
      "Train Epoch: 396 [50112/225000 (22%)] Loss: 16732.175781\n",
      "Train Epoch: 396 [52608/225000 (23%)] Loss: 17037.937500\n",
      "Train Epoch: 396 [55104/225000 (24%)] Loss: 16938.781250\n",
      "Train Epoch: 396 [57600/225000 (26%)] Loss: 16754.386719\n",
      "Train Epoch: 396 [60096/225000 (27%)] Loss: 16555.847656\n",
      "Train Epoch: 396 [62592/225000 (28%)] Loss: 16607.656250\n",
      "Train Epoch: 396 [65088/225000 (29%)] Loss: 16355.330078\n",
      "Train Epoch: 396 [67584/225000 (30%)] Loss: 16887.830078\n",
      "Train Epoch: 396 [70080/225000 (31%)] Loss: 16735.535156\n",
      "Train Epoch: 396 [72576/225000 (32%)] Loss: 16495.457031\n",
      "Train Epoch: 396 [75072/225000 (33%)] Loss: 16377.689453\n",
      "Train Epoch: 396 [77568/225000 (34%)] Loss: 16688.085938\n",
      "Train Epoch: 396 [80064/225000 (36%)] Loss: 16609.568359\n",
      "Train Epoch: 396 [82560/225000 (37%)] Loss: 16681.277344\n",
      "Train Epoch: 396 [85056/225000 (38%)] Loss: 16651.550781\n",
      "Train Epoch: 396 [87552/225000 (39%)] Loss: 16457.789062\n",
      "Train Epoch: 396 [90048/225000 (40%)] Loss: 16912.808594\n",
      "Train Epoch: 396 [92544/225000 (41%)] Loss: 17124.464844\n",
      "Train Epoch: 396 [95040/225000 (42%)] Loss: 18254.894531\n",
      "Train Epoch: 396 [97536/225000 (43%)] Loss: 16531.447266\n",
      "Train Epoch: 396 [100032/225000 (44%)] Loss: 16887.478516\n",
      "Train Epoch: 396 [102528/225000 (46%)] Loss: 16675.578125\n",
      "Train Epoch: 396 [105024/225000 (47%)] Loss: 16719.144531\n",
      "Train Epoch: 396 [107520/225000 (48%)] Loss: 16601.781250\n",
      "Train Epoch: 396 [110016/225000 (49%)] Loss: 16907.765625\n",
      "Train Epoch: 396 [112512/225000 (50%)] Loss: 16583.296875\n",
      "Train Epoch: 396 [115008/225000 (51%)] Loss: 16567.929688\n",
      "Train Epoch: 396 [117504/225000 (52%)] Loss: 16468.906250\n",
      "Train Epoch: 396 [120000/225000 (53%)] Loss: 16238.717773\n",
      "Train Epoch: 396 [122496/225000 (54%)] Loss: 16951.005859\n",
      "Train Epoch: 396 [124992/225000 (56%)] Loss: 16977.214844\n",
      "Train Epoch: 396 [127488/225000 (57%)] Loss: 16461.386719\n",
      "Train Epoch: 396 [129984/225000 (58%)] Loss: 16733.753906\n",
      "Train Epoch: 396 [132480/225000 (59%)] Loss: 17909.478516\n",
      "Train Epoch: 396 [134976/225000 (60%)] Loss: 16549.998047\n",
      "Train Epoch: 396 [137472/225000 (61%)] Loss: 16847.728516\n",
      "Train Epoch: 396 [139968/225000 (62%)] Loss: 16819.093750\n",
      "Train Epoch: 396 [142464/225000 (63%)] Loss: 16907.240234\n",
      "Train Epoch: 396 [144960/225000 (64%)] Loss: 16897.417969\n",
      "Train Epoch: 396 [147456/225000 (66%)] Loss: 16619.234375\n",
      "Train Epoch: 396 [149952/225000 (67%)] Loss: 16518.855469\n",
      "Train Epoch: 396 [152448/225000 (68%)] Loss: 16566.824219\n",
      "Train Epoch: 396 [154944/225000 (69%)] Loss: 16563.507812\n",
      "Train Epoch: 396 [157440/225000 (70%)] Loss: 16740.863281\n",
      "Train Epoch: 396 [159936/225000 (71%)] Loss: 16772.517578\n",
      "Train Epoch: 396 [162432/225000 (72%)] Loss: 16537.039062\n",
      "Train Epoch: 396 [164928/225000 (73%)] Loss: 16533.996094\n",
      "Train Epoch: 396 [167424/225000 (74%)] Loss: 16701.785156\n",
      "Train Epoch: 396 [169920/225000 (76%)] Loss: 16578.955078\n",
      "Train Epoch: 396 [172416/225000 (77%)] Loss: 16282.325195\n",
      "Train Epoch: 396 [174912/225000 (78%)] Loss: 16076.499023\n",
      "Train Epoch: 396 [177408/225000 (79%)] Loss: 16640.205078\n",
      "Train Epoch: 396 [179904/225000 (80%)] Loss: 16069.611328\n",
      "Train Epoch: 396 [182400/225000 (81%)] Loss: 16876.523438\n",
      "Train Epoch: 396 [184896/225000 (82%)] Loss: 16554.054688\n",
      "Train Epoch: 396 [187392/225000 (83%)] Loss: 16385.613281\n",
      "Train Epoch: 396 [189888/225000 (84%)] Loss: 16306.886719\n",
      "Train Epoch: 396 [192384/225000 (86%)] Loss: 16597.328125\n",
      "Train Epoch: 396 [194880/225000 (87%)] Loss: 16566.742188\n",
      "Train Epoch: 396 [197376/225000 (88%)] Loss: 16244.980469\n",
      "Train Epoch: 396 [199872/225000 (89%)] Loss: 16562.992188\n",
      "Train Epoch: 396 [202368/225000 (90%)] Loss: 16758.792969\n",
      "Train Epoch: 396 [204864/225000 (91%)] Loss: 16497.796875\n",
      "Train Epoch: 396 [207360/225000 (92%)] Loss: 17257.593750\n",
      "Train Epoch: 396 [209856/225000 (93%)] Loss: 17122.175781\n",
      "Train Epoch: 396 [212352/225000 (94%)] Loss: 16604.591797\n",
      "Train Epoch: 396 [214848/225000 (95%)] Loss: 17325.107422\n",
      "Train Epoch: 396 [217344/225000 (97%)] Loss: 16605.802734\n",
      "Train Epoch: 396 [219840/225000 (98%)] Loss: 16750.582031\n",
      "Train Epoch: 396 [222336/225000 (99%)] Loss: 16689.363281\n",
      "Train Epoch: 396 [224832/225000 (100%)] Loss: 16129.131836\n",
      "    epoch          : 396\n",
      "    loss           : 16720.697343116735\n",
      "    val_loss       : 16629.287314168825\n",
      "Train Epoch: 397 [192/225000 (0%)] Loss: 16308.245117\n",
      "Train Epoch: 397 [2688/225000 (1%)] Loss: 16263.997070\n",
      "Train Epoch: 397 [5184/225000 (2%)] Loss: 16557.986328\n",
      "Train Epoch: 397 [7680/225000 (3%)] Loss: 16445.046875\n",
      "Train Epoch: 397 [10176/225000 (5%)] Loss: 18727.994141\n",
      "Train Epoch: 397 [12672/225000 (6%)] Loss: 16664.093750\n",
      "Train Epoch: 397 [15168/225000 (7%)] Loss: 16867.480469\n",
      "Train Epoch: 397 [17664/225000 (8%)] Loss: 16294.792969\n",
      "Train Epoch: 397 [20160/225000 (9%)] Loss: 16827.250000\n",
      "Train Epoch: 397 [22656/225000 (10%)] Loss: 16315.393555\n",
      "Train Epoch: 397 [25152/225000 (11%)] Loss: 16591.947266\n",
      "Train Epoch: 397 [27648/225000 (12%)] Loss: 16727.289062\n",
      "Train Epoch: 397 [30144/225000 (13%)] Loss: 16998.167969\n",
      "Train Epoch: 397 [32640/225000 (15%)] Loss: 16710.082031\n",
      "Train Epoch: 397 [35136/225000 (16%)] Loss: 17186.203125\n",
      "Train Epoch: 397 [37632/225000 (17%)] Loss: 16852.562500\n",
      "Train Epoch: 397 [40128/225000 (18%)] Loss: 18154.326172\n",
      "Train Epoch: 397 [42624/225000 (19%)] Loss: 16626.933594\n",
      "Train Epoch: 397 [45120/225000 (20%)] Loss: 16594.548828\n",
      "Train Epoch: 397 [47616/225000 (21%)] Loss: 16581.986328\n",
      "Train Epoch: 397 [50112/225000 (22%)] Loss: 16344.729492\n",
      "Train Epoch: 397 [52608/225000 (23%)] Loss: 16804.750000\n",
      "Train Epoch: 397 [55104/225000 (24%)] Loss: 16570.001953\n",
      "Train Epoch: 397 [57600/225000 (26%)] Loss: 17000.898438\n",
      "Train Epoch: 397 [60096/225000 (27%)] Loss: 16065.863281\n",
      "Train Epoch: 397 [62592/225000 (28%)] Loss: 16935.500000\n",
      "Train Epoch: 397 [65088/225000 (29%)] Loss: 16520.421875\n",
      "Train Epoch: 397 [67584/225000 (30%)] Loss: 17062.490234\n",
      "Train Epoch: 397 [70080/225000 (31%)] Loss: 16734.261719\n",
      "Train Epoch: 397 [72576/225000 (32%)] Loss: 16812.140625\n",
      "Train Epoch: 397 [75072/225000 (33%)] Loss: 16650.617188\n",
      "Train Epoch: 397 [77568/225000 (34%)] Loss: 16562.730469\n",
      "Train Epoch: 397 [80064/225000 (36%)] Loss: 18510.847656\n",
      "Train Epoch: 397 [82560/225000 (37%)] Loss: 16569.728516\n",
      "Train Epoch: 397 [85056/225000 (38%)] Loss: 16676.875000\n",
      "Train Epoch: 397 [87552/225000 (39%)] Loss: 16333.361328\n",
      "Train Epoch: 397 [90048/225000 (40%)] Loss: 16642.861328\n",
      "Train Epoch: 397 [92544/225000 (41%)] Loss: 16905.558594\n",
      "Train Epoch: 397 [95040/225000 (42%)] Loss: 16664.500000\n",
      "Train Epoch: 397 [97536/225000 (43%)] Loss: 16555.251953\n",
      "Train Epoch: 397 [100032/225000 (44%)] Loss: 16487.169922\n",
      "Train Epoch: 397 [102528/225000 (46%)] Loss: 16444.460938\n",
      "Train Epoch: 397 [105024/225000 (47%)] Loss: 16652.761719\n",
      "Train Epoch: 397 [107520/225000 (48%)] Loss: 16516.794922\n",
      "Train Epoch: 397 [110016/225000 (49%)] Loss: 16766.152344\n",
      "Train Epoch: 397 [112512/225000 (50%)] Loss: 16247.397461\n",
      "Train Epoch: 397 [115008/225000 (51%)] Loss: 16277.806641\n",
      "Train Epoch: 397 [117504/225000 (52%)] Loss: 16499.910156\n",
      "Train Epoch: 397 [120000/225000 (53%)] Loss: 16901.816406\n",
      "Train Epoch: 397 [122496/225000 (54%)] Loss: 16592.162109\n",
      "Train Epoch: 397 [124992/225000 (56%)] Loss: 16419.898438\n",
      "Train Epoch: 397 [127488/225000 (57%)] Loss: 16761.427734\n",
      "Train Epoch: 397 [129984/225000 (58%)] Loss: 16612.654297\n",
      "Train Epoch: 397 [132480/225000 (59%)] Loss: 17366.078125\n",
      "Train Epoch: 397 [134976/225000 (60%)] Loss: 16693.089844\n",
      "Train Epoch: 397 [137472/225000 (61%)] Loss: 17037.324219\n",
      "Train Epoch: 397 [139968/225000 (62%)] Loss: 16505.406250\n",
      "Train Epoch: 397 [142464/225000 (63%)] Loss: 17080.755859\n",
      "Train Epoch: 397 [144960/225000 (64%)] Loss: 16836.748047\n",
      "Train Epoch: 397 [147456/225000 (66%)] Loss: 16316.520508\n",
      "Train Epoch: 397 [149952/225000 (67%)] Loss: 16597.437500\n",
      "Train Epoch: 397 [152448/225000 (68%)] Loss: 18039.367188\n",
      "Train Epoch: 397 [154944/225000 (69%)] Loss: 16684.742188\n",
      "Train Epoch: 397 [157440/225000 (70%)] Loss: 16627.570312\n",
      "Train Epoch: 397 [159936/225000 (71%)] Loss: 16365.037109\n",
      "Train Epoch: 397 [162432/225000 (72%)] Loss: 16875.445312\n",
      "Train Epoch: 397 [164928/225000 (73%)] Loss: 16341.299805\n",
      "Train Epoch: 397 [167424/225000 (74%)] Loss: 16519.333984\n",
      "Train Epoch: 397 [169920/225000 (76%)] Loss: 18003.902344\n",
      "Train Epoch: 397 [172416/225000 (77%)] Loss: 16616.285156\n",
      "Train Epoch: 397 [174912/225000 (78%)] Loss: 16702.722656\n",
      "Train Epoch: 397 [177408/225000 (79%)] Loss: 16372.295898\n",
      "Train Epoch: 397 [179904/225000 (80%)] Loss: 16687.078125\n",
      "Train Epoch: 397 [182400/225000 (81%)] Loss: 16491.250000\n",
      "Train Epoch: 397 [184896/225000 (82%)] Loss: 16693.761719\n",
      "Train Epoch: 397 [187392/225000 (83%)] Loss: 16185.950195\n",
      "Train Epoch: 397 [189888/225000 (84%)] Loss: 17729.689453\n",
      "Train Epoch: 397 [192384/225000 (86%)] Loss: 16445.101562\n",
      "Train Epoch: 397 [194880/225000 (87%)] Loss: 16484.953125\n",
      "Train Epoch: 397 [197376/225000 (88%)] Loss: 16731.968750\n",
      "Train Epoch: 397 [199872/225000 (89%)] Loss: 16519.609375\n",
      "Train Epoch: 397 [202368/225000 (90%)] Loss: 16615.128906\n",
      "Train Epoch: 397 [204864/225000 (91%)] Loss: 16270.005859\n",
      "Train Epoch: 397 [207360/225000 (92%)] Loss: 21593.277344\n",
      "Train Epoch: 397 [209856/225000 (93%)] Loss: 16716.625000\n",
      "Train Epoch: 397 [212352/225000 (94%)] Loss: 17122.707031\n",
      "Train Epoch: 397 [214848/225000 (95%)] Loss: 16387.468750\n",
      "Train Epoch: 397 [217344/225000 (97%)] Loss: 16603.808594\n",
      "Train Epoch: 397 [219840/225000 (98%)] Loss: 16848.435547\n",
      "Train Epoch: 397 [222336/225000 (99%)] Loss: 17152.171875\n",
      "Train Epoch: 397 [224832/225000 (100%)] Loss: 16777.654297\n",
      "    epoch          : 397\n",
      "    loss           : 16758.055637398676\n",
      "    val_loss       : 16649.070828925564\n",
      "Train Epoch: 398 [192/225000 (0%)] Loss: 16921.781250\n",
      "Train Epoch: 398 [2688/225000 (1%)] Loss: 17018.296875\n",
      "Train Epoch: 398 [5184/225000 (2%)] Loss: 16657.453125\n",
      "Train Epoch: 398 [7680/225000 (3%)] Loss: 16541.248047\n",
      "Train Epoch: 398 [10176/225000 (5%)] Loss: 17032.382812\n",
      "Train Epoch: 398 [12672/225000 (6%)] Loss: 17059.500000\n",
      "Train Epoch: 398 [15168/225000 (7%)] Loss: 16827.824219\n",
      "Train Epoch: 398 [17664/225000 (8%)] Loss: 18133.300781\n",
      "Train Epoch: 398 [20160/225000 (9%)] Loss: 16791.042969\n",
      "Train Epoch: 398 [22656/225000 (10%)] Loss: 16581.855469\n",
      "Train Epoch: 398 [25152/225000 (11%)] Loss: 16619.187500\n",
      "Train Epoch: 398 [27648/225000 (12%)] Loss: 17154.181641\n",
      "Train Epoch: 398 [30144/225000 (13%)] Loss: 16664.250000\n",
      "Train Epoch: 398 [32640/225000 (15%)] Loss: 16567.214844\n",
      "Train Epoch: 398 [35136/225000 (16%)] Loss: 16607.085938\n",
      "Train Epoch: 398 [37632/225000 (17%)] Loss: 16525.347656\n",
      "Train Epoch: 398 [40128/225000 (18%)] Loss: 16751.214844\n",
      "Train Epoch: 398 [42624/225000 (19%)] Loss: 16486.886719\n",
      "Train Epoch: 398 [45120/225000 (20%)] Loss: 16937.191406\n",
      "Train Epoch: 398 [47616/225000 (21%)] Loss: 16650.558594\n",
      "Train Epoch: 398 [50112/225000 (22%)] Loss: 16536.728516\n",
      "Train Epoch: 398 [52608/225000 (23%)] Loss: 16952.906250\n",
      "Train Epoch: 398 [55104/225000 (24%)] Loss: 17804.476562\n",
      "Train Epoch: 398 [57600/225000 (26%)] Loss: 16661.437500\n",
      "Train Epoch: 398 [60096/225000 (27%)] Loss: 16536.160156\n",
      "Train Epoch: 398 [62592/225000 (28%)] Loss: 16313.623047\n",
      "Train Epoch: 398 [65088/225000 (29%)] Loss: 16528.449219\n",
      "Train Epoch: 398 [67584/225000 (30%)] Loss: 16324.377930\n",
      "Train Epoch: 398 [70080/225000 (31%)] Loss: 16380.366211\n",
      "Train Epoch: 398 [72576/225000 (32%)] Loss: 16752.804688\n",
      "Train Epoch: 398 [75072/225000 (33%)] Loss: 16733.746094\n",
      "Train Epoch: 398 [77568/225000 (34%)] Loss: 16629.685547\n",
      "Train Epoch: 398 [80064/225000 (36%)] Loss: 16247.665039\n",
      "Train Epoch: 398 [82560/225000 (37%)] Loss: 16691.548828\n",
      "Train Epoch: 398 [85056/225000 (38%)] Loss: 16300.652344\n",
      "Train Epoch: 398 [87552/225000 (39%)] Loss: 16443.835938\n",
      "Train Epoch: 398 [90048/225000 (40%)] Loss: 16669.894531\n",
      "Train Epoch: 398 [92544/225000 (41%)] Loss: 16769.037109\n",
      "Train Epoch: 398 [95040/225000 (42%)] Loss: 16329.605469\n",
      "Train Epoch: 398 [97536/225000 (43%)] Loss: 16386.210938\n",
      "Train Epoch: 398 [100032/225000 (44%)] Loss: 16867.511719\n",
      "Train Epoch: 398 [102528/225000 (46%)] Loss: 16832.777344\n",
      "Train Epoch: 398 [105024/225000 (47%)] Loss: 16583.250000\n",
      "Train Epoch: 398 [107520/225000 (48%)] Loss: 16720.843750\n",
      "Train Epoch: 398 [110016/225000 (49%)] Loss: 16833.876953\n",
      "Train Epoch: 398 [112512/225000 (50%)] Loss: 16529.464844\n",
      "Train Epoch: 398 [115008/225000 (51%)] Loss: 16928.042969\n",
      "Train Epoch: 398 [117504/225000 (52%)] Loss: 16410.976562\n",
      "Train Epoch: 398 [120000/225000 (53%)] Loss: 16731.945312\n",
      "Train Epoch: 398 [122496/225000 (54%)] Loss: 16606.224609\n",
      "Train Epoch: 398 [124992/225000 (56%)] Loss: 16579.910156\n",
      "Train Epoch: 398 [127488/225000 (57%)] Loss: 16817.222656\n",
      "Train Epoch: 398 [129984/225000 (58%)] Loss: 16440.222656\n",
      "Train Epoch: 398 [132480/225000 (59%)] Loss: 16334.859375\n",
      "Train Epoch: 398 [134976/225000 (60%)] Loss: 16337.204102\n",
      "Train Epoch: 398 [137472/225000 (61%)] Loss: 16442.792969\n",
      "Train Epoch: 398 [139968/225000 (62%)] Loss: 16763.085938\n",
      "Train Epoch: 398 [142464/225000 (63%)] Loss: 16284.587891\n",
      "Train Epoch: 398 [144960/225000 (64%)] Loss: 16741.103516\n",
      "Train Epoch: 398 [147456/225000 (66%)] Loss: 16227.238281\n",
      "Train Epoch: 398 [149952/225000 (67%)] Loss: 16401.753906\n",
      "Train Epoch: 398 [152448/225000 (68%)] Loss: 16311.023438\n",
      "Train Epoch: 398 [154944/225000 (69%)] Loss: 16226.195312\n",
      "Train Epoch: 398 [157440/225000 (70%)] Loss: 16765.441406\n",
      "Train Epoch: 398 [159936/225000 (71%)] Loss: 16411.246094\n",
      "Train Epoch: 398 [162432/225000 (72%)] Loss: 16372.028320\n",
      "Train Epoch: 398 [164928/225000 (73%)] Loss: 16546.816406\n",
      "Train Epoch: 398 [167424/225000 (74%)] Loss: 16723.554688\n",
      "Train Epoch: 398 [169920/225000 (76%)] Loss: 17061.675781\n",
      "Train Epoch: 398 [172416/225000 (77%)] Loss: 16923.183594\n",
      "Train Epoch: 398 [174912/225000 (78%)] Loss: 16379.741211\n",
      "Train Epoch: 398 [177408/225000 (79%)] Loss: 18004.310547\n",
      "Train Epoch: 398 [179904/225000 (80%)] Loss: 16943.078125\n",
      "Train Epoch: 398 [182400/225000 (81%)] Loss: 16431.339844\n",
      "Train Epoch: 398 [184896/225000 (82%)] Loss: 16399.318359\n",
      "Train Epoch: 398 [187392/225000 (83%)] Loss: 16304.541992\n",
      "Train Epoch: 398 [189888/225000 (84%)] Loss: 16415.746094\n",
      "Train Epoch: 398 [192384/225000 (86%)] Loss: 16074.603516\n",
      "Train Epoch: 398 [194880/225000 (87%)] Loss: 16397.900391\n",
      "Train Epoch: 398 [197376/225000 (88%)] Loss: 16219.485352\n",
      "Train Epoch: 398 [199872/225000 (89%)] Loss: 16322.387695\n",
      "Train Epoch: 398 [202368/225000 (90%)] Loss: 16206.143555\n",
      "Train Epoch: 398 [204864/225000 (91%)] Loss: 16737.894531\n",
      "Train Epoch: 398 [207360/225000 (92%)] Loss: 16198.783203\n",
      "Train Epoch: 398 [209856/225000 (93%)] Loss: 16745.402344\n",
      "Train Epoch: 398 [212352/225000 (94%)] Loss: 17129.101562\n",
      "Train Epoch: 398 [214848/225000 (95%)] Loss: 17006.214844\n",
      "Train Epoch: 398 [217344/225000 (97%)] Loss: 16492.500000\n",
      "Train Epoch: 398 [219840/225000 (98%)] Loss: 16203.776367\n",
      "Train Epoch: 398 [222336/225000 (99%)] Loss: 16094.653320\n",
      "Train Epoch: 398 [224832/225000 (100%)] Loss: 16417.082031\n",
      "    epoch          : 398\n",
      "    loss           : 16727.62988531223\n",
      "    val_loss       : 16609.977855893492\n",
      "Train Epoch: 399 [192/225000 (0%)] Loss: 16711.113281\n",
      "Train Epoch: 399 [2688/225000 (1%)] Loss: 16824.945312\n",
      "Train Epoch: 399 [5184/225000 (2%)] Loss: 16592.105469\n",
      "Train Epoch: 399 [7680/225000 (3%)] Loss: 16722.652344\n",
      "Train Epoch: 399 [10176/225000 (5%)] Loss: 17225.376953\n",
      "Train Epoch: 399 [12672/225000 (6%)] Loss: 16938.027344\n",
      "Train Epoch: 399 [15168/225000 (7%)] Loss: 16856.279297\n",
      "Train Epoch: 399 [17664/225000 (8%)] Loss: 16619.082031\n",
      "Train Epoch: 399 [20160/225000 (9%)] Loss: 16642.746094\n",
      "Train Epoch: 399 [22656/225000 (10%)] Loss: 16645.533203\n",
      "Train Epoch: 399 [25152/225000 (11%)] Loss: 16475.349609\n",
      "Train Epoch: 399 [27648/225000 (12%)] Loss: 16697.324219\n",
      "Train Epoch: 399 [30144/225000 (13%)] Loss: 16824.804688\n",
      "Train Epoch: 399 [32640/225000 (15%)] Loss: 16992.660156\n",
      "Train Epoch: 399 [35136/225000 (16%)] Loss: 16517.628906\n",
      "Train Epoch: 399 [37632/225000 (17%)] Loss: 16393.753906\n",
      "Train Epoch: 399 [40128/225000 (18%)] Loss: 17049.843750\n",
      "Train Epoch: 399 [42624/225000 (19%)] Loss: 16780.613281\n",
      "Train Epoch: 399 [45120/225000 (20%)] Loss: 16435.789062\n",
      "Train Epoch: 399 [47616/225000 (21%)] Loss: 16376.652344\n",
      "Train Epoch: 399 [50112/225000 (22%)] Loss: 16739.890625\n",
      "Train Epoch: 399 [52608/225000 (23%)] Loss: 16479.335938\n",
      "Train Epoch: 399 [55104/225000 (24%)] Loss: 16883.445312\n",
      "Train Epoch: 399 [57600/225000 (26%)] Loss: 16676.371094\n",
      "Train Epoch: 399 [60096/225000 (27%)] Loss: 16647.496094\n",
      "Train Epoch: 399 [62592/225000 (28%)] Loss: 16265.049805\n",
      "Train Epoch: 399 [65088/225000 (29%)] Loss: 17025.781250\n",
      "Train Epoch: 399 [67584/225000 (30%)] Loss: 16807.972656\n",
      "Train Epoch: 399 [70080/225000 (31%)] Loss: 16816.972656\n",
      "Train Epoch: 399 [72576/225000 (32%)] Loss: 16744.533203\n",
      "Train Epoch: 399 [75072/225000 (33%)] Loss: 16432.980469\n",
      "Train Epoch: 399 [77568/225000 (34%)] Loss: 16827.152344\n",
      "Train Epoch: 399 [80064/225000 (36%)] Loss: 16325.519531\n",
      "Train Epoch: 399 [82560/225000 (37%)] Loss: 16881.773438\n",
      "Train Epoch: 399 [85056/225000 (38%)] Loss: 16417.222656\n",
      "Train Epoch: 399 [87552/225000 (39%)] Loss: 16663.478516\n",
      "Train Epoch: 399 [90048/225000 (40%)] Loss: 18215.445312\n",
      "Train Epoch: 399 [92544/225000 (41%)] Loss: 17141.316406\n",
      "Train Epoch: 399 [95040/225000 (42%)] Loss: 16853.556641\n",
      "Train Epoch: 399 [97536/225000 (43%)] Loss: 17248.285156\n",
      "Train Epoch: 399 [100032/225000 (44%)] Loss: 16496.726562\n",
      "Train Epoch: 399 [102528/225000 (46%)] Loss: 16639.253906\n",
      "Train Epoch: 399 [105024/225000 (47%)] Loss: 16337.015625\n",
      "Train Epoch: 399 [107520/225000 (48%)] Loss: 16849.501953\n",
      "Train Epoch: 399 [110016/225000 (49%)] Loss: 16653.132812\n",
      "Train Epoch: 399 [112512/225000 (50%)] Loss: 16289.243164\n",
      "Train Epoch: 399 [115008/225000 (51%)] Loss: 16523.734375\n",
      "Train Epoch: 399 [117504/225000 (52%)] Loss: 16530.312500\n",
      "Train Epoch: 399 [120000/225000 (53%)] Loss: 16761.554688\n",
      "Train Epoch: 399 [122496/225000 (54%)] Loss: 16872.582031\n",
      "Train Epoch: 399 [124992/225000 (56%)] Loss: 16788.355469\n",
      "Train Epoch: 399 [127488/225000 (57%)] Loss: 16298.518555\n",
      "Train Epoch: 399 [129984/225000 (58%)] Loss: 16808.988281\n",
      "Train Epoch: 399 [132480/225000 (59%)] Loss: 16696.324219\n",
      "Train Epoch: 399 [134976/225000 (60%)] Loss: 16005.297852\n",
      "Train Epoch: 399 [137472/225000 (61%)] Loss: 16857.085938\n",
      "Train Epoch: 399 [139968/225000 (62%)] Loss: 16675.878906\n",
      "Train Epoch: 399 [142464/225000 (63%)] Loss: 16323.430664\n",
      "Train Epoch: 399 [144960/225000 (64%)] Loss: 16513.853516\n",
      "Train Epoch: 399 [147456/225000 (66%)] Loss: 16954.621094\n",
      "Train Epoch: 399 [149952/225000 (67%)] Loss: 16786.988281\n",
      "Train Epoch: 399 [152448/225000 (68%)] Loss: 16945.144531\n",
      "Train Epoch: 399 [154944/225000 (69%)] Loss: 16284.026367\n",
      "Train Epoch: 399 [157440/225000 (70%)] Loss: 16565.210938\n",
      "Train Epoch: 399 [159936/225000 (71%)] Loss: 16368.133789\n",
      "Train Epoch: 399 [162432/225000 (72%)] Loss: 16766.152344\n",
      "Train Epoch: 399 [164928/225000 (73%)] Loss: 16433.548828\n",
      "Train Epoch: 399 [167424/225000 (74%)] Loss: 16464.794922\n",
      "Train Epoch: 399 [169920/225000 (76%)] Loss: 16534.910156\n",
      "Train Epoch: 399 [172416/225000 (77%)] Loss: 16301.068359\n",
      "Train Epoch: 399 [174912/225000 (78%)] Loss: 16847.179688\n",
      "Train Epoch: 399 [177408/225000 (79%)] Loss: 18097.757812\n",
      "Train Epoch: 399 [179904/225000 (80%)] Loss: 16275.508789\n",
      "Train Epoch: 399 [182400/225000 (81%)] Loss: 16347.508789\n",
      "Train Epoch: 399 [184896/225000 (82%)] Loss: 16787.917969\n",
      "Train Epoch: 399 [187392/225000 (83%)] Loss: 16486.527344\n",
      "Train Epoch: 399 [189888/225000 (84%)] Loss: 16598.412109\n",
      "Train Epoch: 399 [192384/225000 (86%)] Loss: 16297.158203\n",
      "Train Epoch: 399 [194880/225000 (87%)] Loss: 16755.931641\n",
      "Train Epoch: 399 [197376/225000 (88%)] Loss: 16781.566406\n",
      "Train Epoch: 399 [199872/225000 (89%)] Loss: 16554.484375\n",
      "Train Epoch: 399 [202368/225000 (90%)] Loss: 16847.212891\n",
      "Train Epoch: 399 [204864/225000 (91%)] Loss: 16770.031250\n",
      "Train Epoch: 399 [207360/225000 (92%)] Loss: 16889.228516\n",
      "Train Epoch: 399 [209856/225000 (93%)] Loss: 17177.300781\n",
      "Train Epoch: 399 [212352/225000 (94%)] Loss: 18032.179688\n",
      "Train Epoch: 399 [214848/225000 (95%)] Loss: 16532.929688\n",
      "Train Epoch: 399 [217344/225000 (97%)] Loss: 16545.943359\n",
      "Train Epoch: 399 [219840/225000 (98%)] Loss: 16265.725586\n",
      "Train Epoch: 399 [222336/225000 (99%)] Loss: 16643.496094\n",
      "Train Epoch: 399 [224832/225000 (100%)] Loss: 15925.717773\n",
      "    epoch          : 399\n",
      "    loss           : 16737.456805440754\n",
      "    val_loss       : 16656.4866823595\n",
      "Train Epoch: 400 [192/225000 (0%)] Loss: 16796.021484\n",
      "Train Epoch: 400 [2688/225000 (1%)] Loss: 16778.988281\n",
      "Train Epoch: 400 [5184/225000 (2%)] Loss: 16790.437500\n",
      "Train Epoch: 400 [7680/225000 (3%)] Loss: 16950.652344\n",
      "Train Epoch: 400 [10176/225000 (5%)] Loss: 17220.068359\n",
      "Train Epoch: 400 [12672/225000 (6%)] Loss: 16451.775391\n",
      "Train Epoch: 400 [15168/225000 (7%)] Loss: 16599.935547\n",
      "Train Epoch: 400 [17664/225000 (8%)] Loss: 16254.224609\n",
      "Train Epoch: 400 [20160/225000 (9%)] Loss: 16950.570312\n",
      "Train Epoch: 400 [22656/225000 (10%)] Loss: 17073.652344\n",
      "Train Epoch: 400 [25152/225000 (11%)] Loss: 16126.722656\n",
      "Train Epoch: 400 [27648/225000 (12%)] Loss: 16894.117188\n",
      "Train Epoch: 400 [30144/225000 (13%)] Loss: 16667.570312\n",
      "Train Epoch: 400 [32640/225000 (15%)] Loss: 17108.652344\n",
      "Train Epoch: 400 [35136/225000 (16%)] Loss: 16548.197266\n",
      "Train Epoch: 400 [37632/225000 (17%)] Loss: 18605.972656\n",
      "Train Epoch: 400 [40128/225000 (18%)] Loss: 16594.750000\n",
      "Train Epoch: 400 [42624/225000 (19%)] Loss: 16687.433594\n",
      "Train Epoch: 400 [45120/225000 (20%)] Loss: 16807.316406\n",
      "Train Epoch: 400 [47616/225000 (21%)] Loss: 16799.521484\n",
      "Train Epoch: 400 [50112/225000 (22%)] Loss: 16365.989258\n",
      "Train Epoch: 400 [52608/225000 (23%)] Loss: 16370.431641\n",
      "Train Epoch: 400 [55104/225000 (24%)] Loss: 16550.785156\n",
      "Train Epoch: 400 [57600/225000 (26%)] Loss: 16409.945312\n",
      "Train Epoch: 400 [60096/225000 (27%)] Loss: 16694.363281\n",
      "Train Epoch: 400 [62592/225000 (28%)] Loss: 16918.623047\n",
      "Train Epoch: 400 [65088/225000 (29%)] Loss: 16754.658203\n",
      "Train Epoch: 400 [67584/225000 (30%)] Loss: 16498.398438\n",
      "Train Epoch: 400 [70080/225000 (31%)] Loss: 16799.996094\n",
      "Train Epoch: 400 [72576/225000 (32%)] Loss: 16402.191406\n",
      "Train Epoch: 400 [75072/225000 (33%)] Loss: 16433.718750\n",
      "Train Epoch: 400 [77568/225000 (34%)] Loss: 16364.073242\n",
      "Train Epoch: 400 [80064/225000 (36%)] Loss: 16733.886719\n",
      "Train Epoch: 400 [82560/225000 (37%)] Loss: 16230.517578\n",
      "Train Epoch: 400 [85056/225000 (38%)] Loss: 16858.011719\n",
      "Train Epoch: 400 [87552/225000 (39%)] Loss: 16647.021484\n",
      "Train Epoch: 400 [90048/225000 (40%)] Loss: 16271.461914\n",
      "Train Epoch: 400 [92544/225000 (41%)] Loss: 16981.187500\n",
      "Train Epoch: 400 [95040/225000 (42%)] Loss: 17875.441406\n",
      "Train Epoch: 400 [97536/225000 (43%)] Loss: 16361.089844\n",
      "Train Epoch: 400 [100032/225000 (44%)] Loss: 18117.410156\n",
      "Train Epoch: 400 [102528/225000 (46%)] Loss: 16489.429688\n",
      "Train Epoch: 400 [105024/225000 (47%)] Loss: 16905.126953\n",
      "Train Epoch: 400 [107520/225000 (48%)] Loss: 16457.812500\n",
      "Train Epoch: 400 [110016/225000 (49%)] Loss: 16652.929688\n",
      "Train Epoch: 400 [112512/225000 (50%)] Loss: 16773.671875\n",
      "Train Epoch: 400 [115008/225000 (51%)] Loss: 16711.003906\n",
      "Train Epoch: 400 [117504/225000 (52%)] Loss: 16744.748047\n",
      "Train Epoch: 400 [120000/225000 (53%)] Loss: 16751.281250\n",
      "Train Epoch: 400 [122496/225000 (54%)] Loss: 16131.274414\n",
      "Train Epoch: 400 [124992/225000 (56%)] Loss: 16830.417969\n",
      "Train Epoch: 400 [127488/225000 (57%)] Loss: 16111.642578\n",
      "Train Epoch: 400 [129984/225000 (58%)] Loss: 16531.867188\n",
      "Train Epoch: 400 [132480/225000 (59%)] Loss: 16760.453125\n",
      "Train Epoch: 400 [134976/225000 (60%)] Loss: 16592.132812\n",
      "Train Epoch: 400 [137472/225000 (61%)] Loss: 16466.128906\n",
      "Train Epoch: 400 [139968/225000 (62%)] Loss: 16572.443359\n",
      "Train Epoch: 400 [142464/225000 (63%)] Loss: 18057.533203\n",
      "Train Epoch: 400 [144960/225000 (64%)] Loss: 16689.101562\n",
      "Train Epoch: 400 [147456/225000 (66%)] Loss: 16561.814453\n",
      "Train Epoch: 400 [149952/225000 (67%)] Loss: 16522.431641\n",
      "Train Epoch: 400 [152448/225000 (68%)] Loss: 16778.667969\n",
      "Train Epoch: 400 [154944/225000 (69%)] Loss: 16666.437500\n",
      "Train Epoch: 400 [157440/225000 (70%)] Loss: 16843.388672\n",
      "Train Epoch: 400 [159936/225000 (71%)] Loss: 16076.194336\n",
      "Train Epoch: 400 [162432/225000 (72%)] Loss: 16227.406250\n",
      "Train Epoch: 400 [164928/225000 (73%)] Loss: 16653.464844\n",
      "Train Epoch: 400 [167424/225000 (74%)] Loss: 16618.996094\n",
      "Train Epoch: 400 [169920/225000 (76%)] Loss: 16933.960938\n",
      "Train Epoch: 400 [172416/225000 (77%)] Loss: 16718.500000\n",
      "Train Epoch: 400 [174912/225000 (78%)] Loss: 16651.337891\n",
      "Train Epoch: 400 [177408/225000 (79%)] Loss: 16367.838867\n",
      "Train Epoch: 400 [179904/225000 (80%)] Loss: 16431.722656\n",
      "Train Epoch: 400 [182400/225000 (81%)] Loss: 16451.945312\n",
      "Train Epoch: 400 [184896/225000 (82%)] Loss: 16840.441406\n",
      "Train Epoch: 400 [187392/225000 (83%)] Loss: 16952.972656\n",
      "Train Epoch: 400 [189888/225000 (84%)] Loss: 24861.648438\n",
      "Train Epoch: 400 [192384/225000 (86%)] Loss: 16549.972656\n",
      "Train Epoch: 400 [194880/225000 (87%)] Loss: 16455.191406\n",
      "Train Epoch: 400 [197376/225000 (88%)] Loss: 16446.390625\n",
      "Train Epoch: 400 [199872/225000 (89%)] Loss: 16735.988281\n",
      "Train Epoch: 400 [202368/225000 (90%)] Loss: 16696.226562\n",
      "Train Epoch: 400 [204864/225000 (91%)] Loss: 16352.743164\n",
      "Train Epoch: 400 [207360/225000 (92%)] Loss: 16864.048828\n",
      "Train Epoch: 400 [209856/225000 (93%)] Loss: 15982.205078\n",
      "Train Epoch: 400 [212352/225000 (94%)] Loss: 16635.755859\n",
      "Train Epoch: 400 [214848/225000 (95%)] Loss: 16786.656250\n",
      "Train Epoch: 400 [217344/225000 (97%)] Loss: 16549.539062\n",
      "Train Epoch: 400 [219840/225000 (98%)] Loss: 16694.144531\n",
      "Train Epoch: 400 [222336/225000 (99%)] Loss: 16760.347656\n",
      "Train Epoch: 400 [224832/225000 (100%)] Loss: 16560.339844\n",
      "    epoch          : 400\n",
      "    loss           : 16719.487709644305\n",
      "    val_loss       : 16642.755916084952\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0804_123131/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [192/225000 (0%)] Loss: 16780.347656\n",
      "Train Epoch: 401 [2688/225000 (1%)] Loss: 18688.472656\n",
      "Train Epoch: 401 [5184/225000 (2%)] Loss: 16176.497070\n",
      "Train Epoch: 401 [7680/225000 (3%)] Loss: 16461.929688\n",
      "Train Epoch: 401 [10176/225000 (5%)] Loss: 16289.326172\n",
      "Train Epoch: 401 [12672/225000 (6%)] Loss: 16969.251953\n",
      "Train Epoch: 401 [15168/225000 (7%)] Loss: 16970.615234\n",
      "Train Epoch: 401 [17664/225000 (8%)] Loss: 16693.105469\n",
      "Train Epoch: 401 [20160/225000 (9%)] Loss: 16543.226562\n",
      "Train Epoch: 401 [22656/225000 (10%)] Loss: 16233.355469\n",
      "Train Epoch: 401 [25152/225000 (11%)] Loss: 16449.644531\n",
      "Train Epoch: 401 [27648/225000 (12%)] Loss: 16580.193359\n",
      "Train Epoch: 401 [30144/225000 (13%)] Loss: 16509.474609\n",
      "Train Epoch: 401 [32640/225000 (15%)] Loss: 17078.128906\n",
      "Train Epoch: 401 [35136/225000 (16%)] Loss: 16646.078125\n",
      "Train Epoch: 401 [37632/225000 (17%)] Loss: 16757.121094\n",
      "Train Epoch: 401 [40128/225000 (18%)] Loss: 16802.328125\n",
      "Train Epoch: 401 [42624/225000 (19%)] Loss: 16448.146484\n",
      "Train Epoch: 401 [45120/225000 (20%)] Loss: 16501.468750\n",
      "Train Epoch: 401 [47616/225000 (21%)] Loss: 16586.728516\n",
      "Train Epoch: 401 [50112/225000 (22%)] Loss: 16687.523438\n",
      "Train Epoch: 401 [52608/225000 (23%)] Loss: 16482.355469\n",
      "Train Epoch: 401 [55104/225000 (24%)] Loss: 16594.585938\n",
      "Train Epoch: 401 [57600/225000 (26%)] Loss: 18427.121094\n",
      "Train Epoch: 401 [60096/225000 (27%)] Loss: 17085.875000\n",
      "Train Epoch: 401 [62592/225000 (28%)] Loss: 16260.284180\n",
      "Train Epoch: 401 [65088/225000 (29%)] Loss: 16713.804688\n",
      "Train Epoch: 401 [67584/225000 (30%)] Loss: 16636.986328\n",
      "Train Epoch: 401 [70080/225000 (31%)] Loss: 16656.296875\n",
      "Train Epoch: 401 [72576/225000 (32%)] Loss: 16437.873047\n",
      "Train Epoch: 401 [75072/225000 (33%)] Loss: 16705.736328\n",
      "Train Epoch: 401 [77568/225000 (34%)] Loss: 16482.492188\n",
      "Train Epoch: 401 [80064/225000 (36%)] Loss: 16342.167969\n",
      "Train Epoch: 401 [82560/225000 (37%)] Loss: 17081.869141\n",
      "Train Epoch: 401 [85056/225000 (38%)] Loss: 16481.787109\n",
      "Train Epoch: 401 [87552/225000 (39%)] Loss: 16369.257812\n",
      "Train Epoch: 401 [90048/225000 (40%)] Loss: 16790.523438\n",
      "Train Epoch: 401 [92544/225000 (41%)] Loss: 16334.475586\n",
      "Train Epoch: 401 [95040/225000 (42%)] Loss: 16618.339844\n",
      "Train Epoch: 401 [97536/225000 (43%)] Loss: 16639.312500\n",
      "Train Epoch: 401 [100032/225000 (44%)] Loss: 16371.985352\n",
      "Train Epoch: 401 [102528/225000 (46%)] Loss: 16490.824219\n",
      "Train Epoch: 401 [105024/225000 (47%)] Loss: 16312.832031\n",
      "Train Epoch: 401 [107520/225000 (48%)] Loss: 16206.476562\n",
      "Train Epoch: 401 [110016/225000 (49%)] Loss: 16968.007812\n",
      "Train Epoch: 401 [112512/225000 (50%)] Loss: 16761.707031\n",
      "Train Epoch: 401 [115008/225000 (51%)] Loss: 16488.089844\n",
      "Train Epoch: 401 [117504/225000 (52%)] Loss: 17234.183594\n",
      "Train Epoch: 401 [120000/225000 (53%)] Loss: 16753.757812\n",
      "Train Epoch: 401 [122496/225000 (54%)] Loss: 16492.429688\n",
      "Train Epoch: 401 [124992/225000 (56%)] Loss: 16641.845703\n",
      "Train Epoch: 401 [127488/225000 (57%)] Loss: 20128.441406\n",
      "Train Epoch: 401 [129984/225000 (58%)] Loss: 16585.984375\n",
      "Train Epoch: 401 [132480/225000 (59%)] Loss: 16476.636719\n",
      "Train Epoch: 401 [134976/225000 (60%)] Loss: 16788.402344\n",
      "Train Epoch: 401 [137472/225000 (61%)] Loss: 16421.714844\n",
      "Train Epoch: 401 [139968/225000 (62%)] Loss: 16309.345703\n",
      "Train Epoch: 401 [142464/225000 (63%)] Loss: 18499.240234\n",
      "Train Epoch: 401 [144960/225000 (64%)] Loss: 16771.988281\n",
      "Train Epoch: 401 [147456/225000 (66%)] Loss: 16710.644531\n",
      "Train Epoch: 401 [149952/225000 (67%)] Loss: 16570.695312\n",
      "Train Epoch: 401 [152448/225000 (68%)] Loss: 16251.055664\n",
      "Train Epoch: 401 [154944/225000 (69%)] Loss: 16490.984375\n",
      "Train Epoch: 401 [157440/225000 (70%)] Loss: 16562.667969\n",
      "Train Epoch: 401 [159936/225000 (71%)] Loss: 17091.093750\n",
      "Train Epoch: 401 [162432/225000 (72%)] Loss: 17041.175781\n",
      "Train Epoch: 401 [164928/225000 (73%)] Loss: 16657.730469\n",
      "Train Epoch: 401 [167424/225000 (74%)] Loss: 16708.408203\n",
      "Train Epoch: 401 [169920/225000 (76%)] Loss: 16564.035156\n",
      "Train Epoch: 401 [172416/225000 (77%)] Loss: 16681.593750\n",
      "Train Epoch: 401 [174912/225000 (78%)] Loss: 16815.882812\n",
      "Train Epoch: 401 [177408/225000 (79%)] Loss: 16495.882812\n",
      "Train Epoch: 401 [179904/225000 (80%)] Loss: 17056.503906\n",
      "Train Epoch: 401 [182400/225000 (81%)] Loss: 16584.097656\n",
      "Train Epoch: 401 [184896/225000 (82%)] Loss: 16473.882812\n",
      "Train Epoch: 401 [187392/225000 (83%)] Loss: 16671.125000\n",
      "Train Epoch: 401 [189888/225000 (84%)] Loss: 16935.107422\n",
      "Train Epoch: 401 [192384/225000 (86%)] Loss: 17046.023438\n",
      "Train Epoch: 401 [194880/225000 (87%)] Loss: 17111.390625\n",
      "Train Epoch: 401 [197376/225000 (88%)] Loss: 16884.617188\n",
      "Train Epoch: 401 [199872/225000 (89%)] Loss: 17126.537109\n",
      "Train Epoch: 401 [202368/225000 (90%)] Loss: 16467.296875\n",
      "Train Epoch: 401 [204864/225000 (91%)] Loss: 16574.322266\n",
      "Train Epoch: 401 [207360/225000 (92%)] Loss: 16292.226562\n",
      "Train Epoch: 401 [209856/225000 (93%)] Loss: 16290.678711\n",
      "Train Epoch: 401 [212352/225000 (94%)] Loss: 16654.197266\n",
      "Train Epoch: 401 [214848/225000 (95%)] Loss: 16430.457031\n",
      "Train Epoch: 401 [217344/225000 (97%)] Loss: 17946.320312\n",
      "Train Epoch: 401 [219840/225000 (98%)] Loss: 16847.515625\n",
      "Train Epoch: 401 [222336/225000 (99%)] Loss: 16611.507812\n",
      "Train Epoch: 401 [224832/225000 (100%)] Loss: 16709.013672\n",
      "    epoch          : 401\n",
      "    loss           : 16726.048050707923\n",
      "    val_loss       : 16668.15467414783\n",
      "Train Epoch: 402 [192/225000 (0%)] Loss: 16452.851562\n",
      "Train Epoch: 402 [2688/225000 (1%)] Loss: 16634.109375\n",
      "Train Epoch: 402 [5184/225000 (2%)] Loss: 16684.246094\n",
      "Train Epoch: 402 [7680/225000 (3%)] Loss: 16828.289062\n",
      "Train Epoch: 402 [10176/225000 (5%)] Loss: 16813.015625\n",
      "Train Epoch: 402 [12672/225000 (6%)] Loss: 16652.144531\n",
      "Train Epoch: 402 [15168/225000 (7%)] Loss: 16183.137695\n",
      "Train Epoch: 402 [17664/225000 (8%)] Loss: 16860.816406\n",
      "Train Epoch: 402 [20160/225000 (9%)] Loss: 16278.081055\n",
      "Train Epoch: 402 [22656/225000 (10%)] Loss: 16837.009766\n",
      "Train Epoch: 402 [25152/225000 (11%)] Loss: 16752.146484\n",
      "Train Epoch: 402 [27648/225000 (12%)] Loss: 18020.687500\n",
      "Train Epoch: 402 [30144/225000 (13%)] Loss: 16083.558594\n",
      "Train Epoch: 402 [32640/225000 (15%)] Loss: 16955.691406\n",
      "Train Epoch: 402 [35136/225000 (16%)] Loss: 16521.142578\n",
      "Train Epoch: 402 [37632/225000 (17%)] Loss: 16906.986328\n",
      "Train Epoch: 402 [40128/225000 (18%)] Loss: 18405.234375\n",
      "Train Epoch: 402 [42624/225000 (19%)] Loss: 16943.294922\n",
      "Train Epoch: 402 [45120/225000 (20%)] Loss: 16722.164062\n",
      "Train Epoch: 402 [47616/225000 (21%)] Loss: 16521.728516\n",
      "Train Epoch: 402 [50112/225000 (22%)] Loss: 16668.695312\n",
      "Train Epoch: 402 [52608/225000 (23%)] Loss: 17026.152344\n",
      "Train Epoch: 402 [55104/225000 (24%)] Loss: 16197.185547\n",
      "Train Epoch: 402 [57600/225000 (26%)] Loss: 16724.500000\n",
      "Train Epoch: 402 [60096/225000 (27%)] Loss: 16923.236328\n",
      "Train Epoch: 402 [62592/225000 (28%)] Loss: 16889.697266\n",
      "Train Epoch: 402 [65088/225000 (29%)] Loss: 17221.257812\n",
      "Train Epoch: 402 [67584/225000 (30%)] Loss: 16821.501953\n",
      "Train Epoch: 402 [70080/225000 (31%)] Loss: 16751.312500\n",
      "Train Epoch: 402 [72576/225000 (32%)] Loss: 16667.332031\n",
      "Train Epoch: 402 [75072/225000 (33%)] Loss: 16532.863281\n",
      "Train Epoch: 402 [77568/225000 (34%)] Loss: 16761.820312\n",
      "Train Epoch: 402 [80064/225000 (36%)] Loss: 16715.601562\n",
      "Train Epoch: 402 [82560/225000 (37%)] Loss: 16441.802734\n",
      "Train Epoch: 402 [85056/225000 (38%)] Loss: 16916.984375\n",
      "Train Epoch: 402 [87552/225000 (39%)] Loss: 16290.215820\n",
      "Train Epoch: 402 [90048/225000 (40%)] Loss: 16721.849609\n",
      "Train Epoch: 402 [92544/225000 (41%)] Loss: 16779.609375\n",
      "Train Epoch: 402 [95040/225000 (42%)] Loss: 16592.718750\n",
      "Train Epoch: 402 [97536/225000 (43%)] Loss: 16314.855469\n",
      "Train Epoch: 402 [100032/225000 (44%)] Loss: 16864.207031\n",
      "Train Epoch: 402 [102528/225000 (46%)] Loss: 16588.851562\n",
      "Train Epoch: 402 [105024/225000 (47%)] Loss: 18201.621094\n",
      "Train Epoch: 402 [107520/225000 (48%)] Loss: 16826.009766\n",
      "Train Epoch: 402 [110016/225000 (49%)] Loss: 16096.783203\n",
      "Train Epoch: 402 [112512/225000 (50%)] Loss: 16097.191406\n",
      "Train Epoch: 402 [115008/225000 (51%)] Loss: 16689.679688\n",
      "Train Epoch: 402 [117504/225000 (52%)] Loss: 16943.958984\n",
      "Train Epoch: 402 [120000/225000 (53%)] Loss: 16526.339844\n",
      "Train Epoch: 402 [122496/225000 (54%)] Loss: 17993.390625\n",
      "Train Epoch: 402 [124992/225000 (56%)] Loss: 16708.617188\n",
      "Train Epoch: 402 [127488/225000 (57%)] Loss: 17040.386719\n",
      "Train Epoch: 402 [129984/225000 (58%)] Loss: 17042.195312\n",
      "Train Epoch: 402 [132480/225000 (59%)] Loss: 16632.298828\n",
      "Train Epoch: 402 [134976/225000 (60%)] Loss: 16410.099609\n",
      "Train Epoch: 402 [137472/225000 (61%)] Loss: 16169.593750\n",
      "Train Epoch: 402 [139968/225000 (62%)] Loss: 16355.743164\n",
      "Train Epoch: 402 [142464/225000 (63%)] Loss: 16287.231445\n",
      "Train Epoch: 402 [144960/225000 (64%)] Loss: 16350.781250\n",
      "Train Epoch: 402 [147456/225000 (66%)] Loss: 16451.335938\n",
      "Train Epoch: 402 [149952/225000 (67%)] Loss: 17062.164062\n",
      "Train Epoch: 402 [152448/225000 (68%)] Loss: 16604.875000\n",
      "Train Epoch: 402 [154944/225000 (69%)] Loss: 16857.343750\n",
      "Train Epoch: 402 [157440/225000 (70%)] Loss: 16213.270508\n",
      "Train Epoch: 402 [159936/225000 (71%)] Loss: 16870.408203\n",
      "Train Epoch: 402 [162432/225000 (72%)] Loss: 16372.614258\n",
      "Train Epoch: 402 [164928/225000 (73%)] Loss: 16897.988281\n",
      "Train Epoch: 402 [167424/225000 (74%)] Loss: 16205.452148\n",
      "Train Epoch: 402 [169920/225000 (76%)] Loss: 16815.583984\n",
      "Train Epoch: 402 [172416/225000 (77%)] Loss: 16807.603516\n",
      "Train Epoch: 402 [174912/225000 (78%)] Loss: 16651.625000\n",
      "Train Epoch: 402 [177408/225000 (79%)] Loss: 17108.201172\n",
      "Train Epoch: 402 [179904/225000 (80%)] Loss: 18134.947266\n",
      "Train Epoch: 402 [182400/225000 (81%)] Loss: 16610.968750\n",
      "Train Epoch: 402 [184896/225000 (82%)] Loss: 16297.923828\n",
      "Train Epoch: 402 [187392/225000 (83%)] Loss: 16490.767578\n",
      "Train Epoch: 402 [189888/225000 (84%)] Loss: 16433.250000\n",
      "Train Epoch: 402 [192384/225000 (86%)] Loss: 16364.842773\n",
      "Train Epoch: 402 [194880/225000 (87%)] Loss: 16700.691406\n",
      "Train Epoch: 402 [197376/225000 (88%)] Loss: 16814.562500\n",
      "Train Epoch: 402 [199872/225000 (89%)] Loss: 16608.031250\n",
      "Train Epoch: 402 [202368/225000 (90%)] Loss: 16148.054688\n",
      "Train Epoch: 402 [204864/225000 (91%)] Loss: 16802.546875\n",
      "Train Epoch: 402 [207360/225000 (92%)] Loss: 16652.960938\n",
      "Train Epoch: 402 [209856/225000 (93%)] Loss: 16581.085938\n",
      "Train Epoch: 402 [212352/225000 (94%)] Loss: 17814.765625\n",
      "Train Epoch: 402 [214848/225000 (95%)] Loss: 18371.828125\n",
      "Train Epoch: 402 [217344/225000 (97%)] Loss: 16543.789062\n",
      "Train Epoch: 402 [219840/225000 (98%)] Loss: 16588.964844\n",
      "Train Epoch: 402 [222336/225000 (99%)] Loss: 16555.460938\n",
      "Train Epoch: 402 [224832/225000 (100%)] Loss: 16327.561523\n",
      "    epoch          : 402\n",
      "    loss           : 16701.839779590176\n",
      "    val_loss       : 16673.15983520937\n",
      "Train Epoch: 403 [192/225000 (0%)] Loss: 16445.660156\n",
      "Train Epoch: 403 [2688/225000 (1%)] Loss: 16968.863281\n",
      "Train Epoch: 403 [5184/225000 (2%)] Loss: 16431.898438\n",
      "Train Epoch: 403 [7680/225000 (3%)] Loss: 18021.775391\n",
      "Train Epoch: 403 [10176/225000 (5%)] Loss: 16553.707031\n",
      "Train Epoch: 403 [12672/225000 (6%)] Loss: 16440.417969\n",
      "Train Epoch: 403 [15168/225000 (7%)] Loss: 16565.724609\n",
      "Train Epoch: 403 [17664/225000 (8%)] Loss: 17953.535156\n",
      "Train Epoch: 403 [20160/225000 (9%)] Loss: 16914.992188\n",
      "Train Epoch: 403 [22656/225000 (10%)] Loss: 16815.394531\n",
      "Train Epoch: 403 [25152/225000 (11%)] Loss: 17092.531250\n",
      "Train Epoch: 403 [27648/225000 (12%)] Loss: 17469.691406\n",
      "Train Epoch: 403 [30144/225000 (13%)] Loss: 16778.472656\n",
      "Train Epoch: 403 [32640/225000 (15%)] Loss: 16977.894531\n",
      "Train Epoch: 403 [35136/225000 (16%)] Loss: 16519.128906\n",
      "Train Epoch: 403 [37632/225000 (17%)] Loss: 16545.091797\n",
      "Train Epoch: 403 [40128/225000 (18%)] Loss: 16613.378906\n",
      "Train Epoch: 403 [42624/225000 (19%)] Loss: 18632.675781\n",
      "Train Epoch: 403 [45120/225000 (20%)] Loss: 16642.585938\n",
      "Train Epoch: 403 [47616/225000 (21%)] Loss: 16268.015625\n",
      "Train Epoch: 403 [50112/225000 (22%)] Loss: 16561.515625\n",
      "Train Epoch: 403 [52608/225000 (23%)] Loss: 16338.629883\n",
      "Train Epoch: 403 [55104/225000 (24%)] Loss: 16838.349609\n",
      "Train Epoch: 403 [57600/225000 (26%)] Loss: 16134.879883\n",
      "Train Epoch: 403 [60096/225000 (27%)] Loss: 16535.734375\n",
      "Train Epoch: 403 [62592/225000 (28%)] Loss: 16161.416016\n",
      "Train Epoch: 403 [65088/225000 (29%)] Loss: 19566.066406\n",
      "Train Epoch: 403 [67584/225000 (30%)] Loss: 16919.009766\n",
      "Train Epoch: 403 [70080/225000 (31%)] Loss: 16698.656250\n",
      "Train Epoch: 403 [72576/225000 (32%)] Loss: 16394.492188\n",
      "Train Epoch: 403 [75072/225000 (33%)] Loss: 17148.701172\n",
      "Train Epoch: 403 [77568/225000 (34%)] Loss: 16744.855469\n",
      "Train Epoch: 403 [80064/225000 (36%)] Loss: 16764.789062\n",
      "Train Epoch: 403 [82560/225000 (37%)] Loss: 16696.289062\n",
      "Train Epoch: 403 [85056/225000 (38%)] Loss: 16410.714844\n",
      "Train Epoch: 403 [87552/225000 (39%)] Loss: 16530.554688\n",
      "Train Epoch: 403 [90048/225000 (40%)] Loss: 16779.087891\n",
      "Train Epoch: 403 [92544/225000 (41%)] Loss: 16811.820312\n",
      "Train Epoch: 403 [95040/225000 (42%)] Loss: 16721.884766\n",
      "Train Epoch: 403 [97536/225000 (43%)] Loss: 16845.367188\n",
      "Train Epoch: 403 [100032/225000 (44%)] Loss: 16745.605469\n",
      "Train Epoch: 403 [102528/225000 (46%)] Loss: 16463.431641\n",
      "Train Epoch: 403 [105024/225000 (47%)] Loss: 16816.515625\n",
      "Train Epoch: 403 [107520/225000 (48%)] Loss: 16970.933594\n",
      "Train Epoch: 403 [110016/225000 (49%)] Loss: 16571.802734\n",
      "Train Epoch: 403 [112512/225000 (50%)] Loss: 16484.910156\n",
      "Train Epoch: 403 [115008/225000 (51%)] Loss: 16485.656250\n",
      "Train Epoch: 403 [117504/225000 (52%)] Loss: 16744.533203\n",
      "Train Epoch: 403 [120000/225000 (53%)] Loss: 18109.189453\n",
      "Train Epoch: 403 [122496/225000 (54%)] Loss: 16734.646484\n",
      "Train Epoch: 403 [124992/225000 (56%)] Loss: 16329.032227\n",
      "Train Epoch: 403 [127488/225000 (57%)] Loss: 16333.497070\n",
      "Train Epoch: 403 [129984/225000 (58%)] Loss: 16248.796875\n",
      "Train Epoch: 403 [132480/225000 (59%)] Loss: 16663.671875\n",
      "Train Epoch: 403 [134976/225000 (60%)] Loss: 16255.607422\n",
      "Train Epoch: 403 [137472/225000 (61%)] Loss: 16831.062500\n",
      "Train Epoch: 403 [139968/225000 (62%)] Loss: 17265.507812\n",
      "Train Epoch: 403 [142464/225000 (63%)] Loss: 16496.121094\n",
      "Train Epoch: 403 [144960/225000 (64%)] Loss: 17022.750000\n",
      "Train Epoch: 403 [147456/225000 (66%)] Loss: 16071.119141\n",
      "Train Epoch: 403 [149952/225000 (67%)] Loss: 16916.615234\n",
      "Train Epoch: 403 [152448/225000 (68%)] Loss: 16988.806641\n",
      "Train Epoch: 403 [154944/225000 (69%)] Loss: 16393.902344\n",
      "Train Epoch: 403 [157440/225000 (70%)] Loss: 16677.109375\n",
      "Train Epoch: 403 [159936/225000 (71%)] Loss: 17029.171875\n",
      "Train Epoch: 403 [162432/225000 (72%)] Loss: 16747.779297\n",
      "Train Epoch: 403 [164928/225000 (73%)] Loss: 16430.753906\n",
      "Train Epoch: 403 [167424/225000 (74%)] Loss: 16693.121094\n",
      "Train Epoch: 403 [169920/225000 (76%)] Loss: 16726.291016\n",
      "Train Epoch: 403 [172416/225000 (77%)] Loss: 16617.736328\n",
      "Train Epoch: 403 [174912/225000 (78%)] Loss: 16040.755859\n",
      "Train Epoch: 403 [177408/225000 (79%)] Loss: 16271.636719\n",
      "Train Epoch: 403 [179904/225000 (80%)] Loss: 16587.166016\n",
      "Train Epoch: 403 [182400/225000 (81%)] Loss: 17112.753906\n",
      "Train Epoch: 403 [184896/225000 (82%)] Loss: 16464.005859\n",
      "Train Epoch: 403 [187392/225000 (83%)] Loss: 17102.886719\n",
      "Train Epoch: 403 [189888/225000 (84%)] Loss: 16434.117188\n",
      "Train Epoch: 403 [192384/225000 (86%)] Loss: 17229.929688\n",
      "Train Epoch: 403 [194880/225000 (87%)] Loss: 16367.480469\n",
      "Train Epoch: 403 [197376/225000 (88%)] Loss: 16639.515625\n",
      "Train Epoch: 403 [199872/225000 (89%)] Loss: 16477.628906\n",
      "Train Epoch: 403 [202368/225000 (90%)] Loss: 16555.214844\n",
      "Train Epoch: 403 [204864/225000 (91%)] Loss: 16688.052734\n",
      "Train Epoch: 403 [207360/225000 (92%)] Loss: 16953.882812\n",
      "Train Epoch: 403 [209856/225000 (93%)] Loss: 16490.412109\n",
      "Train Epoch: 403 [212352/225000 (94%)] Loss: 16903.601562\n",
      "Train Epoch: 403 [214848/225000 (95%)] Loss: 16509.402344\n",
      "Train Epoch: 403 [217344/225000 (97%)] Loss: 16827.003906\n",
      "Train Epoch: 403 [219840/225000 (98%)] Loss: 16109.946289\n",
      "Train Epoch: 403 [222336/225000 (99%)] Loss: 16382.867188\n",
      "Train Epoch: 403 [224832/225000 (100%)] Loss: 16477.158203\n",
      "    epoch          : 403\n",
      "    loss           : 16702.49079931474\n",
      "    val_loss       : 16645.7700142278\n",
      "Train Epoch: 404 [192/225000 (0%)] Loss: 16663.509766\n",
      "Train Epoch: 404 [2688/225000 (1%)] Loss: 16439.509766\n",
      "Train Epoch: 404 [5184/225000 (2%)] Loss: 16889.927734\n",
      "Train Epoch: 404 [7680/225000 (3%)] Loss: 16652.886719\n",
      "Train Epoch: 404 [10176/225000 (5%)] Loss: 16581.171875\n",
      "Train Epoch: 404 [12672/225000 (6%)] Loss: 16943.160156\n",
      "Train Epoch: 404 [15168/225000 (7%)] Loss: 16621.402344\n",
      "Train Epoch: 404 [17664/225000 (8%)] Loss: 16903.855469\n",
      "Train Epoch: 404 [20160/225000 (9%)] Loss: 16703.705078\n",
      "Train Epoch: 404 [22656/225000 (10%)] Loss: 16464.306641\n",
      "Train Epoch: 404 [25152/225000 (11%)] Loss: 16844.958984\n",
      "Train Epoch: 404 [27648/225000 (12%)] Loss: 16411.363281\n",
      "Train Epoch: 404 [30144/225000 (13%)] Loss: 16562.132812\n",
      "Train Epoch: 404 [32640/225000 (15%)] Loss: 16441.892578\n",
      "Train Epoch: 404 [35136/225000 (16%)] Loss: 16984.158203\n",
      "Train Epoch: 404 [37632/225000 (17%)] Loss: 16587.066406\n",
      "Train Epoch: 404 [40128/225000 (18%)] Loss: 16396.058594\n",
      "Train Epoch: 404 [42624/225000 (19%)] Loss: 16287.350586\n",
      "Train Epoch: 404 [45120/225000 (20%)] Loss: 17029.843750\n",
      "Train Epoch: 404 [47616/225000 (21%)] Loss: 16715.839844\n",
      "Train Epoch: 404 [50112/225000 (22%)] Loss: 16802.750000\n",
      "Train Epoch: 404 [52608/225000 (23%)] Loss: 16742.861328\n",
      "Train Epoch: 404 [55104/225000 (24%)] Loss: 16789.595703\n",
      "Train Epoch: 404 [57600/225000 (26%)] Loss: 16795.996094\n",
      "Train Epoch: 404 [60096/225000 (27%)] Loss: 16974.703125\n",
      "Train Epoch: 404 [62592/225000 (28%)] Loss: 16661.802734\n",
      "Train Epoch: 404 [65088/225000 (29%)] Loss: 16640.414062\n",
      "Train Epoch: 404 [67584/225000 (30%)] Loss: 16602.425781\n",
      "Train Epoch: 404 [70080/225000 (31%)] Loss: 16519.773438\n",
      "Train Epoch: 404 [72576/225000 (32%)] Loss: 16631.914062\n",
      "Train Epoch: 404 [75072/225000 (33%)] Loss: 16855.570312\n",
      "Train Epoch: 404 [77568/225000 (34%)] Loss: 16540.191406\n",
      "Train Epoch: 404 [80064/225000 (36%)] Loss: 16408.250000\n",
      "Train Epoch: 404 [82560/225000 (37%)] Loss: 16475.792969\n",
      "Train Epoch: 404 [85056/225000 (38%)] Loss: 16801.705078\n",
      "Train Epoch: 404 [87552/225000 (39%)] Loss: 16491.984375\n",
      "Train Epoch: 404 [90048/225000 (40%)] Loss: 16789.468750\n",
      "Train Epoch: 404 [92544/225000 (41%)] Loss: 16897.857422\n",
      "Train Epoch: 404 [95040/225000 (42%)] Loss: 16325.705078\n",
      "Train Epoch: 404 [97536/225000 (43%)] Loss: 16748.542969\n",
      "Train Epoch: 404 [100032/225000 (44%)] Loss: 16248.571289\n",
      "Train Epoch: 404 [102528/225000 (46%)] Loss: 16492.421875\n",
      "Train Epoch: 404 [105024/225000 (47%)] Loss: 16458.207031\n",
      "Train Epoch: 404 [107520/225000 (48%)] Loss: 16579.384766\n",
      "Train Epoch: 404 [110016/225000 (49%)] Loss: 16400.175781\n",
      "Train Epoch: 404 [112512/225000 (50%)] Loss: 16750.433594\n",
      "Train Epoch: 404 [115008/225000 (51%)] Loss: 16224.218750\n",
      "Train Epoch: 404 [117504/225000 (52%)] Loss: 16116.540039\n",
      "Train Epoch: 404 [120000/225000 (53%)] Loss: 18251.398438\n",
      "Train Epoch: 404 [122496/225000 (54%)] Loss: 16496.156250\n",
      "Train Epoch: 404 [124992/225000 (56%)] Loss: 16940.841797\n",
      "Train Epoch: 404 [127488/225000 (57%)] Loss: 16985.101562\n",
      "Train Epoch: 404 [129984/225000 (58%)] Loss: 16252.253906\n",
      "Train Epoch: 404 [132480/225000 (59%)] Loss: 16482.992188\n",
      "Train Epoch: 404 [134976/225000 (60%)] Loss: 16653.753906\n",
      "Train Epoch: 404 [137472/225000 (61%)] Loss: 16995.574219\n",
      "Train Epoch: 404 [139968/225000 (62%)] Loss: 16152.647461\n",
      "Train Epoch: 404 [142464/225000 (63%)] Loss: 17004.789062\n",
      "Train Epoch: 404 [144960/225000 (64%)] Loss: 16820.546875\n",
      "Train Epoch: 404 [147456/225000 (66%)] Loss: 16469.992188\n",
      "Train Epoch: 404 [149952/225000 (67%)] Loss: 16557.367188\n",
      "Train Epoch: 404 [152448/225000 (68%)] Loss: 16617.277344\n",
      "Train Epoch: 404 [154944/225000 (69%)] Loss: 17197.521484\n",
      "Train Epoch: 404 [157440/225000 (70%)] Loss: 16464.328125\n",
      "Train Epoch: 404 [159936/225000 (71%)] Loss: 16476.923828\n",
      "Train Epoch: 404 [162432/225000 (72%)] Loss: 16564.664062\n",
      "Train Epoch: 404 [164928/225000 (73%)] Loss: 16476.773438\n",
      "Train Epoch: 404 [167424/225000 (74%)] Loss: 16302.150391\n",
      "Train Epoch: 404 [169920/225000 (76%)] Loss: 16463.949219\n",
      "Train Epoch: 404 [172416/225000 (77%)] Loss: 16596.669922\n",
      "Train Epoch: 404 [174912/225000 (78%)] Loss: 16674.398438\n",
      "Train Epoch: 404 [177408/225000 (79%)] Loss: 16508.464844\n",
      "Train Epoch: 404 [179904/225000 (80%)] Loss: 16973.296875\n",
      "Train Epoch: 404 [182400/225000 (81%)] Loss: 16591.691406\n",
      "Train Epoch: 404 [184896/225000 (82%)] Loss: 16586.363281\n",
      "Train Epoch: 404 [187392/225000 (83%)] Loss: 16555.394531\n",
      "Train Epoch: 404 [189888/225000 (84%)] Loss: 16529.572266\n",
      "Train Epoch: 404 [192384/225000 (86%)] Loss: 16708.287109\n",
      "Train Epoch: 404 [194880/225000 (87%)] Loss: 16551.542969\n",
      "Train Epoch: 404 [197376/225000 (88%)] Loss: 16631.871094\n",
      "Train Epoch: 404 [199872/225000 (89%)] Loss: 16580.089844\n",
      "Train Epoch: 404 [202368/225000 (90%)] Loss: 16987.257812\n",
      "Train Epoch: 404 [204864/225000 (91%)] Loss: 16347.190430\n",
      "Train Epoch: 404 [207360/225000 (92%)] Loss: 16554.265625\n",
      "Train Epoch: 404 [209856/225000 (93%)] Loss: 16320.063477\n",
      "Train Epoch: 404 [212352/225000 (94%)] Loss: 16375.261719\n",
      "Train Epoch: 404 [214848/225000 (95%)] Loss: 16307.099609\n",
      "Train Epoch: 404 [217344/225000 (97%)] Loss: 16354.874023\n",
      "Train Epoch: 404 [219840/225000 (98%)] Loss: 16362.107422\n",
      "Train Epoch: 404 [222336/225000 (99%)] Loss: 16607.253906\n",
      "Train Epoch: 404 [224832/225000 (100%)] Loss: 16203.617188\n",
      "    epoch          : 404\n",
      "    loss           : 16725.789484121695\n",
      "    val_loss       : 16680.403779281005\n",
      "Train Epoch: 405 [192/225000 (0%)] Loss: 16663.570312\n",
      "Train Epoch: 405 [2688/225000 (1%)] Loss: 16608.921875\n",
      "Train Epoch: 405 [5184/225000 (2%)] Loss: 16585.710938\n",
      "Train Epoch: 405 [7680/225000 (3%)] Loss: 16570.953125\n",
      "Train Epoch: 405 [10176/225000 (5%)] Loss: 16544.189453\n",
      "Train Epoch: 405 [12672/225000 (6%)] Loss: 16340.046875\n",
      "Train Epoch: 405 [15168/225000 (7%)] Loss: 16810.671875\n",
      "Train Epoch: 405 [17664/225000 (8%)] Loss: 16533.234375\n",
      "Train Epoch: 405 [20160/225000 (9%)] Loss: 16783.082031\n",
      "Train Epoch: 405 [22656/225000 (10%)] Loss: 16873.531250\n",
      "Train Epoch: 405 [25152/225000 (11%)] Loss: 17028.218750\n",
      "Train Epoch: 405 [27648/225000 (12%)] Loss: 16757.986328\n",
      "Train Epoch: 405 [30144/225000 (13%)] Loss: 16528.542969\n",
      "Train Epoch: 405 [32640/225000 (15%)] Loss: 16815.359375\n",
      "Train Epoch: 405 [35136/225000 (16%)] Loss: 16810.250000\n",
      "Train Epoch: 405 [37632/225000 (17%)] Loss: 16890.656250\n",
      "Train Epoch: 405 [40128/225000 (18%)] Loss: 16548.898438\n",
      "Train Epoch: 405 [42624/225000 (19%)] Loss: 16597.480469\n",
      "Train Epoch: 405 [45120/225000 (20%)] Loss: 16943.925781\n",
      "Train Epoch: 405 [47616/225000 (21%)] Loss: 16418.986328\n",
      "Train Epoch: 405 [50112/225000 (22%)] Loss: 16717.757812\n",
      "Train Epoch: 405 [52608/225000 (23%)] Loss: 16348.442383\n",
      "Train Epoch: 405 [55104/225000 (24%)] Loss: 16842.695312\n",
      "Train Epoch: 405 [57600/225000 (26%)] Loss: 17063.798828\n",
      "Train Epoch: 405 [60096/225000 (27%)] Loss: 16866.402344\n",
      "Train Epoch: 405 [62592/225000 (28%)] Loss: 16285.169922\n",
      "Train Epoch: 405 [65088/225000 (29%)] Loss: 16805.671875\n",
      "Train Epoch: 405 [67584/225000 (30%)] Loss: 16028.107422\n",
      "Train Epoch: 405 [70080/225000 (31%)] Loss: 16599.273438\n",
      "Train Epoch: 405 [72576/225000 (32%)] Loss: 16834.019531\n",
      "Train Epoch: 405 [75072/225000 (33%)] Loss: 16664.380859\n",
      "Train Epoch: 405 [77568/225000 (34%)] Loss: 16309.047852\n",
      "Train Epoch: 405 [80064/225000 (36%)] Loss: 16749.679688\n",
      "Train Epoch: 405 [82560/225000 (37%)] Loss: 16768.632812\n",
      "Train Epoch: 405 [85056/225000 (38%)] Loss: 16712.273438\n",
      "Train Epoch: 405 [87552/225000 (39%)] Loss: 16360.824219\n",
      "Train Epoch: 405 [90048/225000 (40%)] Loss: 16957.892578\n",
      "Train Epoch: 405 [92544/225000 (41%)] Loss: 16243.732422\n",
      "Train Epoch: 405 [95040/225000 (42%)] Loss: 16499.125000\n",
      "Train Epoch: 405 [97536/225000 (43%)] Loss: 18270.640625\n",
      "Train Epoch: 405 [100032/225000 (44%)] Loss: 16781.906250\n",
      "Train Epoch: 405 [102528/225000 (46%)] Loss: 16612.964844\n",
      "Train Epoch: 405 [105024/225000 (47%)] Loss: 16774.191406\n",
      "Train Epoch: 405 [107520/225000 (48%)] Loss: 16550.203125\n",
      "Train Epoch: 405 [110016/225000 (49%)] Loss: 17103.312500\n",
      "Train Epoch: 405 [112512/225000 (50%)] Loss: 16996.925781\n",
      "Train Epoch: 405 [115008/225000 (51%)] Loss: 16768.472656\n",
      "Train Epoch: 405 [117504/225000 (52%)] Loss: 17037.652344\n",
      "Train Epoch: 405 [120000/225000 (53%)] Loss: 16156.010742\n",
      "Train Epoch: 405 [122496/225000 (54%)] Loss: 16670.527344\n",
      "Train Epoch: 405 [124992/225000 (56%)] Loss: 17018.230469\n",
      "Train Epoch: 405 [127488/225000 (57%)] Loss: 16540.947266\n",
      "Train Epoch: 405 [129984/225000 (58%)] Loss: 16746.203125\n",
      "Train Epoch: 405 [132480/225000 (59%)] Loss: 16662.632812\n",
      "Train Epoch: 405 [134976/225000 (60%)] Loss: 17014.343750\n",
      "Train Epoch: 405 [137472/225000 (61%)] Loss: 16784.312500\n",
      "Train Epoch: 405 [139968/225000 (62%)] Loss: 17078.628906\n",
      "Train Epoch: 405 [142464/225000 (63%)] Loss: 16609.511719\n",
      "Train Epoch: 405 [144960/225000 (64%)] Loss: 16740.001953\n",
      "Train Epoch: 405 [147456/225000 (66%)] Loss: 16641.253906\n",
      "Train Epoch: 405 [149952/225000 (67%)] Loss: 16343.697266\n",
      "Train Epoch: 405 [152448/225000 (68%)] Loss: 16283.294922\n",
      "Train Epoch: 405 [154944/225000 (69%)] Loss: 16325.144531\n",
      "Train Epoch: 405 [157440/225000 (70%)] Loss: 17478.621094\n",
      "Train Epoch: 405 [159936/225000 (71%)] Loss: 16422.054688\n",
      "Train Epoch: 405 [162432/225000 (72%)] Loss: 16607.833984\n",
      "Train Epoch: 405 [164928/225000 (73%)] Loss: 16769.523438\n",
      "Train Epoch: 405 [167424/225000 (74%)] Loss: 17050.871094\n",
      "Train Epoch: 405 [169920/225000 (76%)] Loss: 16651.105469\n",
      "Train Epoch: 405 [172416/225000 (77%)] Loss: 17024.914062\n",
      "Train Epoch: 405 [174912/225000 (78%)] Loss: 16814.343750\n",
      "Train Epoch: 405 [177408/225000 (79%)] Loss: 16592.542969\n",
      "Train Epoch: 405 [179904/225000 (80%)] Loss: 16463.605469\n",
      "Train Epoch: 405 [182400/225000 (81%)] Loss: 16313.697266\n",
      "Train Epoch: 405 [184896/225000 (82%)] Loss: 16242.648438\n",
      "Train Epoch: 405 [187392/225000 (83%)] Loss: 16972.189453\n",
      "Train Epoch: 405 [189888/225000 (84%)] Loss: 16869.984375\n",
      "Train Epoch: 405 [192384/225000 (86%)] Loss: 16662.335938\n",
      "Train Epoch: 405 [194880/225000 (87%)] Loss: 17181.839844\n",
      "Train Epoch: 405 [197376/225000 (88%)] Loss: 17053.773438\n",
      "Train Epoch: 405 [199872/225000 (89%)] Loss: 16870.832031\n",
      "Train Epoch: 405 [202368/225000 (90%)] Loss: 16987.482422\n",
      "Train Epoch: 405 [204864/225000 (91%)] Loss: 16505.742188\n",
      "Train Epoch: 405 [207360/225000 (92%)] Loss: 16271.651367\n",
      "Train Epoch: 405 [209856/225000 (93%)] Loss: 16569.294922\n",
      "Train Epoch: 405 [212352/225000 (94%)] Loss: 16845.667969\n",
      "Train Epoch: 405 [214848/225000 (95%)] Loss: 16592.601562\n",
      "Train Epoch: 405 [217344/225000 (97%)] Loss: 16315.825195\n",
      "Train Epoch: 405 [219840/225000 (98%)] Loss: 16506.916016\n",
      "Train Epoch: 405 [222336/225000 (99%)] Loss: 17974.109375\n",
      "Train Epoch: 405 [224832/225000 (100%)] Loss: 16705.457031\n",
      "    epoch          : 405\n",
      "    loss           : 16718.490942632787\n",
      "    val_loss       : 16633.282738608257\n",
      "Train Epoch: 406 [192/225000 (0%)] Loss: 16912.527344\n",
      "Train Epoch: 406 [2688/225000 (1%)] Loss: 16468.582031\n",
      "Train Epoch: 406 [5184/225000 (2%)] Loss: 16558.539062\n",
      "Train Epoch: 406 [7680/225000 (3%)] Loss: 16659.000000\n",
      "Train Epoch: 406 [10176/225000 (5%)] Loss: 16834.230469\n",
      "Train Epoch: 406 [12672/225000 (6%)] Loss: 16993.171875\n",
      "Train Epoch: 406 [15168/225000 (7%)] Loss: 16723.707031\n",
      "Train Epoch: 406 [17664/225000 (8%)] Loss: 16641.810547\n",
      "Train Epoch: 406 [20160/225000 (9%)] Loss: 18432.218750\n",
      "Train Epoch: 406 [22656/225000 (10%)] Loss: 17123.332031\n",
      "Train Epoch: 406 [25152/225000 (11%)] Loss: 15992.740234\n",
      "Train Epoch: 406 [27648/225000 (12%)] Loss: 16707.531250\n",
      "Train Epoch: 406 [30144/225000 (13%)] Loss: 16343.057617\n",
      "Train Epoch: 406 [32640/225000 (15%)] Loss: 16685.087891\n",
      "Train Epoch: 406 [35136/225000 (16%)] Loss: 16526.050781\n",
      "Train Epoch: 406 [37632/225000 (17%)] Loss: 16854.435547\n",
      "Train Epoch: 406 [40128/225000 (18%)] Loss: 18065.302734\n",
      "Train Epoch: 406 [42624/225000 (19%)] Loss: 16582.941406\n",
      "Train Epoch: 406 [45120/225000 (20%)] Loss: 16806.480469\n",
      "Train Epoch: 406 [47616/225000 (21%)] Loss: 16667.578125\n",
      "Train Epoch: 406 [50112/225000 (22%)] Loss: 16625.937500\n",
      "Train Epoch: 406 [52608/225000 (23%)] Loss: 16090.328125\n",
      "Train Epoch: 406 [55104/225000 (24%)] Loss: 16448.664062\n",
      "Train Epoch: 406 [57600/225000 (26%)] Loss: 16678.312500\n",
      "Train Epoch: 406 [60096/225000 (27%)] Loss: 16694.957031\n",
      "Train Epoch: 406 [62592/225000 (28%)] Loss: 16682.777344\n",
      "Train Epoch: 406 [65088/225000 (29%)] Loss: 17054.576172\n",
      "Train Epoch: 406 [67584/225000 (30%)] Loss: 16443.791016\n",
      "Train Epoch: 406 [70080/225000 (31%)] Loss: 17191.501953\n",
      "Train Epoch: 406 [72576/225000 (32%)] Loss: 16555.414062\n",
      "Train Epoch: 406 [75072/225000 (33%)] Loss: 16451.765625\n",
      "Train Epoch: 406 [77568/225000 (34%)] Loss: 16969.585938\n",
      "Train Epoch: 406 [80064/225000 (36%)] Loss: 16626.812500\n",
      "Train Epoch: 406 [82560/225000 (37%)] Loss: 16559.689453\n",
      "Train Epoch: 406 [85056/225000 (38%)] Loss: 16594.753906\n",
      "Train Epoch: 406 [87552/225000 (39%)] Loss: 16667.257812\n",
      "Train Epoch: 406 [90048/225000 (40%)] Loss: 16425.875000\n",
      "Train Epoch: 406 [92544/225000 (41%)] Loss: 16680.988281\n",
      "Train Epoch: 406 [95040/225000 (42%)] Loss: 17001.781250\n",
      "Train Epoch: 406 [97536/225000 (43%)] Loss: 16967.236328\n",
      "Train Epoch: 406 [100032/225000 (44%)] Loss: 16914.386719\n",
      "Train Epoch: 406 [102528/225000 (46%)] Loss: 16710.996094\n",
      "Train Epoch: 406 [105024/225000 (47%)] Loss: 16374.337891\n",
      "Train Epoch: 406 [107520/225000 (48%)] Loss: 16965.878906\n",
      "Train Epoch: 406 [110016/225000 (49%)] Loss: 16438.503906\n",
      "Train Epoch: 406 [112512/225000 (50%)] Loss: 17002.402344\n",
      "Train Epoch: 406 [115008/225000 (51%)] Loss: 16871.015625\n",
      "Train Epoch: 406 [117504/225000 (52%)] Loss: 16963.251953\n",
      "Train Epoch: 406 [120000/225000 (53%)] Loss: 16819.884766\n",
      "Train Epoch: 406 [122496/225000 (54%)] Loss: 16769.460938\n",
      "Train Epoch: 406 [124992/225000 (56%)] Loss: 16923.578125\n",
      "Train Epoch: 406 [127488/225000 (57%)] Loss: 15960.169922\n",
      "Train Epoch: 406 [129984/225000 (58%)] Loss: 16710.746094\n",
      "Train Epoch: 406 [132480/225000 (59%)] Loss: 16817.523438\n",
      "Train Epoch: 406 [134976/225000 (60%)] Loss: 16469.794922\n",
      "Train Epoch: 406 [137472/225000 (61%)] Loss: 16496.628906\n",
      "Train Epoch: 406 [139968/225000 (62%)] Loss: 17058.781250\n",
      "Train Epoch: 406 [142464/225000 (63%)] Loss: 16566.476562\n",
      "Train Epoch: 406 [144960/225000 (64%)] Loss: 16672.859375\n",
      "Train Epoch: 406 [147456/225000 (66%)] Loss: 16677.904297\n",
      "Train Epoch: 406 [149952/225000 (67%)] Loss: 18277.378906\n",
      "Train Epoch: 406 [152448/225000 (68%)] Loss: 16946.378906\n",
      "Train Epoch: 406 [154944/225000 (69%)] Loss: 16717.115234\n",
      "Train Epoch: 406 [157440/225000 (70%)] Loss: 16405.152344\n",
      "Train Epoch: 406 [159936/225000 (71%)] Loss: 16245.862305\n",
      "Train Epoch: 406 [162432/225000 (72%)] Loss: 16696.259766\n",
      "Train Epoch: 406 [164928/225000 (73%)] Loss: 16955.318359\n",
      "Train Epoch: 406 [167424/225000 (74%)] Loss: 16884.972656\n",
      "Train Epoch: 406 [169920/225000 (76%)] Loss: 16484.531250\n",
      "Train Epoch: 406 [172416/225000 (77%)] Loss: 16988.224609\n",
      "Train Epoch: 406 [174912/225000 (78%)] Loss: 16495.792969\n",
      "Train Epoch: 406 [177408/225000 (79%)] Loss: 16727.978516\n",
      "Train Epoch: 406 [179904/225000 (80%)] Loss: 16539.027344\n",
      "Train Epoch: 406 [182400/225000 (81%)] Loss: 17055.625000\n",
      "Train Epoch: 406 [184896/225000 (82%)] Loss: 16375.604492\n",
      "Train Epoch: 406 [187392/225000 (83%)] Loss: 16469.500000\n",
      "Train Epoch: 406 [189888/225000 (84%)] Loss: 17073.675781\n",
      "Train Epoch: 406 [192384/225000 (86%)] Loss: 16807.781250\n",
      "Train Epoch: 406 [194880/225000 (87%)] Loss: 16741.640625\n",
      "Train Epoch: 406 [197376/225000 (88%)] Loss: 16497.246094\n",
      "Train Epoch: 406 [199872/225000 (89%)] Loss: 16544.183594\n",
      "Train Epoch: 406 [202368/225000 (90%)] Loss: 16622.833984\n",
      "Train Epoch: 406 [204864/225000 (91%)] Loss: 16973.015625\n",
      "Train Epoch: 406 [207360/225000 (92%)] Loss: 17234.619141\n",
      "Train Epoch: 406 [209856/225000 (93%)] Loss: 16577.453125\n",
      "Train Epoch: 406 [212352/225000 (94%)] Loss: 16821.906250\n",
      "Train Epoch: 406 [214848/225000 (95%)] Loss: 16630.683594\n",
      "Train Epoch: 406 [217344/225000 (97%)] Loss: 16875.460938\n",
      "Train Epoch: 406 [219840/225000 (98%)] Loss: 16827.150391\n",
      "Train Epoch: 406 [222336/225000 (99%)] Loss: 17011.357422\n",
      "Train Epoch: 406 [224832/225000 (100%)] Loss: 16464.468750\n",
      "    epoch          : 406\n",
      "    loss           : 16728.07196232402\n",
      "    val_loss       : 16644.15090244599\n",
      "Train Epoch: 407 [192/225000 (0%)] Loss: 16985.228516\n",
      "Train Epoch: 407 [2688/225000 (1%)] Loss: 16425.484375\n",
      "Train Epoch: 407 [5184/225000 (2%)] Loss: 16673.439453\n",
      "Train Epoch: 407 [7680/225000 (3%)] Loss: 18620.066406\n",
      "Train Epoch: 407 [10176/225000 (5%)] Loss: 16137.562500\n",
      "Train Epoch: 407 [12672/225000 (6%)] Loss: 17097.070312\n",
      "Train Epoch: 407 [15168/225000 (7%)] Loss: 16701.664062\n",
      "Train Epoch: 407 [17664/225000 (8%)] Loss: 16566.691406\n",
      "Train Epoch: 407 [20160/225000 (9%)] Loss: 18153.837891\n",
      "Train Epoch: 407 [22656/225000 (10%)] Loss: 16244.155273\n",
      "Train Epoch: 407 [25152/225000 (11%)] Loss: 16326.286133\n",
      "Train Epoch: 407 [27648/225000 (12%)] Loss: 16681.820312\n",
      "Train Epoch: 407 [30144/225000 (13%)] Loss: 17302.789062\n",
      "Train Epoch: 407 [32640/225000 (15%)] Loss: 16808.982422\n",
      "Train Epoch: 407 [35136/225000 (16%)] Loss: 16065.740234\n",
      "Train Epoch: 407 [37632/225000 (17%)] Loss: 16876.513672\n",
      "Train Epoch: 407 [40128/225000 (18%)] Loss: 17008.199219\n",
      "Train Epoch: 407 [42624/225000 (19%)] Loss: 16755.642578\n",
      "Train Epoch: 407 [45120/225000 (20%)] Loss: 16901.587891\n",
      "Train Epoch: 407 [47616/225000 (21%)] Loss: 16819.167969\n",
      "Train Epoch: 407 [50112/225000 (22%)] Loss: 16745.554688\n",
      "Train Epoch: 407 [52608/225000 (23%)] Loss: 16614.851562\n",
      "Train Epoch: 407 [55104/225000 (24%)] Loss: 17047.248047\n",
      "Train Epoch: 407 [57600/225000 (26%)] Loss: 16664.808594\n",
      "Train Epoch: 407 [60096/225000 (27%)] Loss: 16589.406250\n",
      "Train Epoch: 407 [62592/225000 (28%)] Loss: 16893.257812\n",
      "Train Epoch: 407 [65088/225000 (29%)] Loss: 16027.476562\n",
      "Train Epoch: 407 [67584/225000 (30%)] Loss: 16571.628906\n",
      "Train Epoch: 407 [70080/225000 (31%)] Loss: 16576.779297\n",
      "Train Epoch: 407 [72576/225000 (32%)] Loss: 16659.441406\n",
      "Train Epoch: 407 [75072/225000 (33%)] Loss: 16577.931641\n",
      "Train Epoch: 407 [77568/225000 (34%)] Loss: 16803.408203\n",
      "Train Epoch: 407 [80064/225000 (36%)] Loss: 16456.066406\n",
      "Train Epoch: 407 [82560/225000 (37%)] Loss: 16502.496094\n",
      "Train Epoch: 407 [85056/225000 (38%)] Loss: 17030.691406\n",
      "Train Epoch: 407 [87552/225000 (39%)] Loss: 16694.761719\n",
      "Train Epoch: 407 [90048/225000 (40%)] Loss: 16803.898438\n",
      "Train Epoch: 407 [92544/225000 (41%)] Loss: 16432.759766\n",
      "Train Epoch: 407 [95040/225000 (42%)] Loss: 16944.703125\n",
      "Train Epoch: 407 [97536/225000 (43%)] Loss: 17141.734375\n",
      "Train Epoch: 407 [100032/225000 (44%)] Loss: 16566.574219\n",
      "Train Epoch: 407 [102528/225000 (46%)] Loss: 16858.742188\n",
      "Train Epoch: 407 [105024/225000 (47%)] Loss: 16241.795898\n",
      "Train Epoch: 407 [107520/225000 (48%)] Loss: 17081.326172\n",
      "Train Epoch: 407 [110016/225000 (49%)] Loss: 16598.492188\n",
      "Train Epoch: 407 [112512/225000 (50%)] Loss: 16667.906250\n",
      "Train Epoch: 407 [115008/225000 (51%)] Loss: 16548.378906\n",
      "Train Epoch: 407 [117504/225000 (52%)] Loss: 16864.402344\n",
      "Train Epoch: 407 [120000/225000 (53%)] Loss: 17321.175781\n",
      "Train Epoch: 407 [122496/225000 (54%)] Loss: 16765.890625\n",
      "Train Epoch: 407 [124992/225000 (56%)] Loss: 16775.105469\n",
      "Train Epoch: 407 [127488/225000 (57%)] Loss: 18507.080078\n",
      "Train Epoch: 407 [129984/225000 (58%)] Loss: 16685.199219\n",
      "Train Epoch: 407 [132480/225000 (59%)] Loss: 16649.296875\n",
      "Train Epoch: 407 [134976/225000 (60%)] Loss: 16538.265625\n",
      "Train Epoch: 407 [137472/225000 (61%)] Loss: 16408.804688\n",
      "Train Epoch: 407 [139968/225000 (62%)] Loss: 16325.743164\n",
      "Train Epoch: 407 [142464/225000 (63%)] Loss: 16493.492188\n",
      "Train Epoch: 407 [144960/225000 (64%)] Loss: 16476.820312\n",
      "Train Epoch: 407 [147456/225000 (66%)] Loss: 16822.822266\n",
      "Train Epoch: 407 [149952/225000 (67%)] Loss: 16613.816406\n",
      "Train Epoch: 407 [152448/225000 (68%)] Loss: 16763.664062\n",
      "Train Epoch: 407 [154944/225000 (69%)] Loss: 24898.265625\n",
      "Train Epoch: 407 [157440/225000 (70%)] Loss: 16503.257812\n",
      "Train Epoch: 407 [159936/225000 (71%)] Loss: 16402.460938\n",
      "Train Epoch: 407 [162432/225000 (72%)] Loss: 16563.675781\n",
      "Train Epoch: 407 [164928/225000 (73%)] Loss: 16951.992188\n",
      "Train Epoch: 407 [167424/225000 (74%)] Loss: 16791.841797\n",
      "Train Epoch: 407 [169920/225000 (76%)] Loss: 16736.949219\n",
      "Train Epoch: 407 [172416/225000 (77%)] Loss: 16524.265625\n",
      "Train Epoch: 407 [174912/225000 (78%)] Loss: 16561.765625\n",
      "Train Epoch: 407 [177408/225000 (79%)] Loss: 16327.400391\n",
      "Train Epoch: 407 [179904/225000 (80%)] Loss: 16640.675781\n",
      "Train Epoch: 407 [182400/225000 (81%)] Loss: 16505.253906\n",
      "Train Epoch: 407 [184896/225000 (82%)] Loss: 16682.476562\n",
      "Train Epoch: 407 [187392/225000 (83%)] Loss: 16882.250000\n",
      "Train Epoch: 407 [189888/225000 (84%)] Loss: 16876.435547\n",
      "Train Epoch: 407 [192384/225000 (86%)] Loss: 16747.751953\n",
      "Train Epoch: 407 [194880/225000 (87%)] Loss: 16948.353516\n",
      "Train Epoch: 407 [197376/225000 (88%)] Loss: 16307.667969\n",
      "Train Epoch: 407 [199872/225000 (89%)] Loss: 16580.146484\n",
      "Train Epoch: 407 [202368/225000 (90%)] Loss: 16539.199219\n",
      "Train Epoch: 407 [204864/225000 (91%)] Loss: 16534.158203\n",
      "Train Epoch: 407 [207360/225000 (92%)] Loss: 17032.441406\n",
      "Train Epoch: 407 [209856/225000 (93%)] Loss: 16829.738281\n",
      "Train Epoch: 407 [212352/225000 (94%)] Loss: 16840.484375\n",
      "Train Epoch: 407 [214848/225000 (95%)] Loss: 16323.917969\n",
      "Train Epoch: 407 [217344/225000 (97%)] Loss: 16694.570312\n",
      "Train Epoch: 407 [219840/225000 (98%)] Loss: 16338.643555\n",
      "Train Epoch: 407 [222336/225000 (99%)] Loss: 16655.986328\n",
      "Train Epoch: 407 [224832/225000 (100%)] Loss: 17637.636719\n",
      "    epoch          : 407\n",
      "    loss           : 16715.28975325965\n",
      "    val_loss       : 16718.45414185888\n",
      "Train Epoch: 408 [192/225000 (0%)] Loss: 16541.410156\n",
      "Train Epoch: 408 [2688/225000 (1%)] Loss: 16537.113281\n",
      "Train Epoch: 408 [5184/225000 (2%)] Loss: 16391.675781\n",
      "Train Epoch: 408 [7680/225000 (3%)] Loss: 16728.615234\n",
      "Train Epoch: 408 [10176/225000 (5%)] Loss: 16826.855469\n",
      "Train Epoch: 408 [12672/225000 (6%)] Loss: 16642.628906\n",
      "Train Epoch: 408 [15168/225000 (7%)] Loss: 16529.894531\n",
      "Train Epoch: 408 [17664/225000 (8%)] Loss: 16966.494141\n",
      "Train Epoch: 408 [20160/225000 (9%)] Loss: 16987.554688\n",
      "Train Epoch: 408 [22656/225000 (10%)] Loss: 16305.750000\n",
      "Train Epoch: 408 [25152/225000 (11%)] Loss: 16534.996094\n",
      "Train Epoch: 408 [27648/225000 (12%)] Loss: 16704.812500\n",
      "Train Epoch: 408 [30144/225000 (13%)] Loss: 17933.832031\n",
      "Train Epoch: 408 [32640/225000 (15%)] Loss: 16694.380859\n",
      "Train Epoch: 408 [35136/225000 (16%)] Loss: 16847.988281\n",
      "Train Epoch: 408 [37632/225000 (17%)] Loss: 16444.388672\n",
      "Train Epoch: 408 [40128/225000 (18%)] Loss: 16793.265625\n",
      "Train Epoch: 408 [42624/225000 (19%)] Loss: 16674.878906\n",
      "Train Epoch: 408 [45120/225000 (20%)] Loss: 16693.351562\n",
      "Train Epoch: 408 [47616/225000 (21%)] Loss: 16745.496094\n",
      "Train Epoch: 408 [50112/225000 (22%)] Loss: 16869.636719\n",
      "Train Epoch: 408 [52608/225000 (23%)] Loss: 16769.417969\n",
      "Train Epoch: 408 [55104/225000 (24%)] Loss: 16695.486328\n",
      "Train Epoch: 408 [57600/225000 (26%)] Loss: 17025.949219\n",
      "Train Epoch: 408 [60096/225000 (27%)] Loss: 16659.998047\n",
      "Train Epoch: 408 [62592/225000 (28%)] Loss: 16855.402344\n",
      "Train Epoch: 408 [65088/225000 (29%)] Loss: 17514.863281\n",
      "Train Epoch: 408 [67584/225000 (30%)] Loss: 16983.699219\n",
      "Train Epoch: 408 [70080/225000 (31%)] Loss: 16627.634766\n",
      "Train Epoch: 408 [72576/225000 (32%)] Loss: 16793.585938\n",
      "Train Epoch: 408 [75072/225000 (33%)] Loss: 17015.230469\n",
      "Train Epoch: 408 [77568/225000 (34%)] Loss: 18481.861328\n",
      "Train Epoch: 408 [80064/225000 (36%)] Loss: 16649.988281\n",
      "Train Epoch: 408 [82560/225000 (37%)] Loss: 16324.405273\n",
      "Train Epoch: 408 [85056/225000 (38%)] Loss: 16234.163086\n",
      "Train Epoch: 408 [87552/225000 (39%)] Loss: 16393.609375\n",
      "Train Epoch: 408 [90048/225000 (40%)] Loss: 16192.359375\n",
      "Train Epoch: 408 [92544/225000 (41%)] Loss: 16584.523438\n",
      "Train Epoch: 408 [95040/225000 (42%)] Loss: 16472.707031\n",
      "Train Epoch: 408 [97536/225000 (43%)] Loss: 16878.789062\n",
      "Train Epoch: 408 [100032/225000 (44%)] Loss: 16664.728516\n",
      "Train Epoch: 408 [102528/225000 (46%)] Loss: 16244.462891\n",
      "Train Epoch: 408 [105024/225000 (47%)] Loss: 17211.841797\n",
      "Train Epoch: 408 [107520/225000 (48%)] Loss: 16918.279297\n",
      "Train Epoch: 408 [110016/225000 (49%)] Loss: 16950.302734\n",
      "Train Epoch: 408 [112512/225000 (50%)] Loss: 16875.511719\n",
      "Train Epoch: 408 [115008/225000 (51%)] Loss: 16682.382812\n",
      "Train Epoch: 408 [117504/225000 (52%)] Loss: 16781.855469\n",
      "Train Epoch: 408 [120000/225000 (53%)] Loss: 16675.863281\n",
      "Train Epoch: 408 [122496/225000 (54%)] Loss: 16214.495117\n",
      "Train Epoch: 408 [124992/225000 (56%)] Loss: 16418.562500\n",
      "Train Epoch: 408 [127488/225000 (57%)] Loss: 16589.757812\n",
      "Train Epoch: 408 [129984/225000 (58%)] Loss: 16457.541016\n",
      "Train Epoch: 408 [132480/225000 (59%)] Loss: 16777.164062\n",
      "Train Epoch: 408 [134976/225000 (60%)] Loss: 16987.421875\n",
      "Train Epoch: 408 [137472/225000 (61%)] Loss: 16864.410156\n",
      "Train Epoch: 408 [139968/225000 (62%)] Loss: 17044.156250\n",
      "Train Epoch: 408 [142464/225000 (63%)] Loss: 16302.705078\n",
      "Train Epoch: 408 [144960/225000 (64%)] Loss: 16525.445312\n",
      "Train Epoch: 408 [147456/225000 (66%)] Loss: 16558.156250\n",
      "Train Epoch: 408 [149952/225000 (67%)] Loss: 17874.642578\n",
      "Train Epoch: 408 [152448/225000 (68%)] Loss: 18534.500000\n",
      "Train Epoch: 408 [154944/225000 (69%)] Loss: 16793.580078\n",
      "Train Epoch: 408 [157440/225000 (70%)] Loss: 16577.382812\n",
      "Train Epoch: 408 [159936/225000 (71%)] Loss: 16921.947266\n",
      "Train Epoch: 408 [162432/225000 (72%)] Loss: 16338.090820\n",
      "Train Epoch: 408 [164928/225000 (73%)] Loss: 16732.750000\n",
      "Train Epoch: 408 [167424/225000 (74%)] Loss: 16628.980469\n",
      "Train Epoch: 408 [169920/225000 (76%)] Loss: 16866.113281\n",
      "Train Epoch: 408 [172416/225000 (77%)] Loss: 16977.808594\n",
      "Train Epoch: 408 [174912/225000 (78%)] Loss: 16576.867188\n",
      "Train Epoch: 408 [177408/225000 (79%)] Loss: 16315.709961\n",
      "Train Epoch: 408 [179904/225000 (80%)] Loss: 16239.085938\n",
      "Train Epoch: 408 [182400/225000 (81%)] Loss: 16415.859375\n",
      "Train Epoch: 408 [184896/225000 (82%)] Loss: 16954.289062\n",
      "Train Epoch: 408 [187392/225000 (83%)] Loss: 16444.841797\n",
      "Train Epoch: 408 [189888/225000 (84%)] Loss: 16876.476562\n",
      "Train Epoch: 408 [192384/225000 (86%)] Loss: 16724.152344\n",
      "Train Epoch: 408 [194880/225000 (87%)] Loss: 16384.535156\n",
      "Train Epoch: 408 [197376/225000 (88%)] Loss: 16265.334961\n",
      "Train Epoch: 408 [199872/225000 (89%)] Loss: 16505.369141\n",
      "Train Epoch: 408 [202368/225000 (90%)] Loss: 16566.519531\n",
      "Train Epoch: 408 [204864/225000 (91%)] Loss: 16566.902344\n",
      "Train Epoch: 408 [207360/225000 (92%)] Loss: 16784.500000\n",
      "Train Epoch: 408 [209856/225000 (93%)] Loss: 16993.152344\n",
      "Train Epoch: 408 [212352/225000 (94%)] Loss: 16629.664062\n",
      "Train Epoch: 408 [214848/225000 (95%)] Loss: 16866.808594\n",
      "Train Epoch: 408 [217344/225000 (97%)] Loss: 16374.066406\n",
      "Train Epoch: 408 [219840/225000 (98%)] Loss: 18136.058594\n",
      "Train Epoch: 408 [222336/225000 (99%)] Loss: 17341.347656\n",
      "Train Epoch: 408 [224832/225000 (100%)] Loss: 16990.902344\n",
      "    epoch          : 408\n",
      "    loss           : 16694.937895791114\n",
      "    val_loss       : 16659.14573154559\n",
      "Train Epoch: 409 [192/225000 (0%)] Loss: 16653.234375\n",
      "Train Epoch: 409 [2688/225000 (1%)] Loss: 16050.501953\n",
      "Train Epoch: 409 [5184/225000 (2%)] Loss: 16504.945312\n",
      "Train Epoch: 409 [7680/225000 (3%)] Loss: 17316.152344\n",
      "Train Epoch: 409 [10176/225000 (5%)] Loss: 16759.394531\n",
      "Train Epoch: 409 [12672/225000 (6%)] Loss: 16644.082031\n",
      "Train Epoch: 409 [15168/225000 (7%)] Loss: 16673.320312\n",
      "Train Epoch: 409 [17664/225000 (8%)] Loss: 17060.539062\n",
      "Train Epoch: 409 [20160/225000 (9%)] Loss: 16743.527344\n",
      "Train Epoch: 409 [22656/225000 (10%)] Loss: 16829.208984\n",
      "Train Epoch: 409 [25152/225000 (11%)] Loss: 16978.845703\n",
      "Train Epoch: 409 [27648/225000 (12%)] Loss: 16561.857422\n",
      "Train Epoch: 409 [30144/225000 (13%)] Loss: 16995.859375\n",
      "Train Epoch: 409 [32640/225000 (15%)] Loss: 16723.648438\n",
      "Train Epoch: 409 [35136/225000 (16%)] Loss: 16407.173828\n",
      "Train Epoch: 409 [37632/225000 (17%)] Loss: 16828.621094\n",
      "Train Epoch: 409 [40128/225000 (18%)] Loss: 16440.337891\n",
      "Train Epoch: 409 [42624/225000 (19%)] Loss: 16775.820312\n",
      "Train Epoch: 409 [45120/225000 (20%)] Loss: 16647.978516\n",
      "Train Epoch: 409 [47616/225000 (21%)] Loss: 16765.546875\n",
      "Train Epoch: 409 [50112/225000 (22%)] Loss: 16422.093750\n",
      "Train Epoch: 409 [52608/225000 (23%)] Loss: 16893.132812\n",
      "Train Epoch: 409 [55104/225000 (24%)] Loss: 16711.173828\n",
      "Train Epoch: 409 [57600/225000 (26%)] Loss: 16491.335938\n",
      "Train Epoch: 409 [60096/225000 (27%)] Loss: 17112.890625\n",
      "Train Epoch: 409 [62592/225000 (28%)] Loss: 16853.701172\n",
      "Train Epoch: 409 [65088/225000 (29%)] Loss: 16691.675781\n",
      "Train Epoch: 409 [67584/225000 (30%)] Loss: 16841.535156\n",
      "Train Epoch: 409 [70080/225000 (31%)] Loss: 16810.337891\n",
      "Train Epoch: 409 [72576/225000 (32%)] Loss: 18511.191406\n",
      "Train Epoch: 409 [75072/225000 (33%)] Loss: 16609.378906\n",
      "Train Epoch: 409 [77568/225000 (34%)] Loss: 16395.179688\n",
      "Train Epoch: 409 [80064/225000 (36%)] Loss: 16609.750000\n",
      "Train Epoch: 409 [82560/225000 (37%)] Loss: 17946.833984\n",
      "Train Epoch: 409 [85056/225000 (38%)] Loss: 16814.453125\n",
      "Train Epoch: 409 [87552/225000 (39%)] Loss: 16602.013672\n",
      "Train Epoch: 409 [90048/225000 (40%)] Loss: 17062.220703\n",
      "Train Epoch: 409 [92544/225000 (41%)] Loss: 16532.257812\n",
      "Train Epoch: 409 [95040/225000 (42%)] Loss: 16707.689453\n",
      "Train Epoch: 409 [97536/225000 (43%)] Loss: 16574.605469\n",
      "Train Epoch: 409 [100032/225000 (44%)] Loss: 16981.085938\n",
      "Train Epoch: 409 [102528/225000 (46%)] Loss: 16763.488281\n",
      "Train Epoch: 409 [105024/225000 (47%)] Loss: 16519.888672\n",
      "Train Epoch: 409 [107520/225000 (48%)] Loss: 17059.134766\n",
      "Train Epoch: 409 [110016/225000 (49%)] Loss: 16650.289062\n",
      "Train Epoch: 409 [112512/225000 (50%)] Loss: 16832.312500\n",
      "Train Epoch: 409 [115008/225000 (51%)] Loss: 16837.500000\n",
      "Train Epoch: 409 [117504/225000 (52%)] Loss: 16387.072266\n",
      "Train Epoch: 409 [120000/225000 (53%)] Loss: 16675.896484\n",
      "Train Epoch: 409 [122496/225000 (54%)] Loss: 16894.816406\n",
      "Train Epoch: 409 [124992/225000 (56%)] Loss: 17018.966797\n",
      "Train Epoch: 409 [127488/225000 (57%)] Loss: 16635.394531\n",
      "Train Epoch: 409 [129984/225000 (58%)] Loss: 16661.009766\n",
      "Train Epoch: 409 [132480/225000 (59%)] Loss: 16650.986328\n",
      "Train Epoch: 409 [134976/225000 (60%)] Loss: 16244.762695\n",
      "Train Epoch: 409 [137472/225000 (61%)] Loss: 16007.021484\n",
      "Train Epoch: 409 [139968/225000 (62%)] Loss: 16334.368164\n",
      "Train Epoch: 409 [142464/225000 (63%)] Loss: 16673.593750\n",
      "Train Epoch: 409 [144960/225000 (64%)] Loss: 16692.832031\n",
      "Train Epoch: 409 [147456/225000 (66%)] Loss: 16885.710938\n",
      "Train Epoch: 409 [149952/225000 (67%)] Loss: 16935.257812\n",
      "Train Epoch: 409 [152448/225000 (68%)] Loss: 16097.133789\n",
      "Train Epoch: 409 [154944/225000 (69%)] Loss: 16291.761719\n",
      "Train Epoch: 409 [157440/225000 (70%)] Loss: 16813.929688\n",
      "Train Epoch: 409 [159936/225000 (71%)] Loss: 17154.394531\n",
      "Train Epoch: 409 [162432/225000 (72%)] Loss: 16542.355469\n",
      "Train Epoch: 409 [164928/225000 (73%)] Loss: 16108.514648\n",
      "Train Epoch: 409 [167424/225000 (74%)] Loss: 16567.757812\n",
      "Train Epoch: 409 [169920/225000 (76%)] Loss: 17064.535156\n",
      "Train Epoch: 409 [172416/225000 (77%)] Loss: 16680.171875\n",
      "Train Epoch: 409 [174912/225000 (78%)] Loss: 16446.160156\n",
      "Train Epoch: 409 [177408/225000 (79%)] Loss: 16585.765625\n",
      "Train Epoch: 409 [179904/225000 (80%)] Loss: 16818.820312\n",
      "Train Epoch: 409 [182400/225000 (81%)] Loss: 16341.036133\n",
      "Train Epoch: 409 [184896/225000 (82%)] Loss: 17105.046875\n",
      "Train Epoch: 409 [187392/225000 (83%)] Loss: 16722.003906\n",
      "Train Epoch: 409 [189888/225000 (84%)] Loss: 16729.730469\n",
      "Train Epoch: 409 [192384/225000 (86%)] Loss: 16554.082031\n",
      "Train Epoch: 409 [194880/225000 (87%)] Loss: 16210.350586\n",
      "Train Epoch: 409 [197376/225000 (88%)] Loss: 16344.684570\n",
      "Train Epoch: 409 [199872/225000 (89%)] Loss: 16804.373047\n",
      "Train Epoch: 409 [202368/225000 (90%)] Loss: 16702.457031\n",
      "Train Epoch: 409 [204864/225000 (91%)] Loss: 16428.529297\n",
      "Train Epoch: 409 [207360/225000 (92%)] Loss: 16725.632812\n",
      "Train Epoch: 409 [209856/225000 (93%)] Loss: 16399.365234\n",
      "Train Epoch: 409 [212352/225000 (94%)] Loss: 16861.923828\n",
      "Train Epoch: 409 [214848/225000 (95%)] Loss: 17069.029297\n",
      "Train Epoch: 409 [217344/225000 (97%)] Loss: 16547.992188\n",
      "Train Epoch: 409 [219840/225000 (98%)] Loss: 16694.425781\n",
      "Train Epoch: 409 [222336/225000 (99%)] Loss: 16517.173828\n",
      "Train Epoch: 409 [224832/225000 (100%)] Loss: 16173.001953\n",
      "    epoch          : 409\n",
      "    loss           : 16717.962244860548\n",
      "    val_loss       : 16647.56900175382\n",
      "Train Epoch: 410 [192/225000 (0%)] Loss: 16975.400391\n",
      "Train Epoch: 410 [2688/225000 (1%)] Loss: 16449.066406\n",
      "Train Epoch: 410 [5184/225000 (2%)] Loss: 17167.484375\n",
      "Train Epoch: 410 [7680/225000 (3%)] Loss: 16846.611328\n",
      "Train Epoch: 410 [10176/225000 (5%)] Loss: 17399.019531\n",
      "Train Epoch: 410 [12672/225000 (6%)] Loss: 17104.902344\n",
      "Train Epoch: 410 [15168/225000 (7%)] Loss: 17094.144531\n",
      "Train Epoch: 410 [17664/225000 (8%)] Loss: 16995.230469\n",
      "Train Epoch: 410 [20160/225000 (9%)] Loss: 17043.169922\n",
      "Train Epoch: 410 [22656/225000 (10%)] Loss: 17061.980469\n",
      "Train Epoch: 410 [25152/225000 (11%)] Loss: 16409.097656\n",
      "Train Epoch: 410 [27648/225000 (12%)] Loss: 16484.933594\n",
      "Train Epoch: 410 [30144/225000 (13%)] Loss: 16713.171875\n",
      "Train Epoch: 410 [32640/225000 (15%)] Loss: 16567.986328\n",
      "Train Epoch: 410 [35136/225000 (16%)] Loss: 16770.363281\n",
      "Train Epoch: 410 [37632/225000 (17%)] Loss: 16638.035156\n",
      "Train Epoch: 410 [40128/225000 (18%)] Loss: 16444.009766\n",
      "Train Epoch: 410 [42624/225000 (19%)] Loss: 16804.070312\n",
      "Train Epoch: 410 [45120/225000 (20%)] Loss: 16867.386719\n",
      "Train Epoch: 410 [47616/225000 (21%)] Loss: 16597.429688\n",
      "Train Epoch: 410 [50112/225000 (22%)] Loss: 16402.082031\n",
      "Train Epoch: 410 [52608/225000 (23%)] Loss: 16192.205078\n",
      "Train Epoch: 410 [55104/225000 (24%)] Loss: 16805.269531\n",
      "Train Epoch: 410 [57600/225000 (26%)] Loss: 16749.128906\n",
      "Train Epoch: 410 [60096/225000 (27%)] Loss: 16594.757812\n",
      "Train Epoch: 410 [62592/225000 (28%)] Loss: 18726.164062\n",
      "Train Epoch: 410 [65088/225000 (29%)] Loss: 16435.414062\n",
      "Train Epoch: 410 [67584/225000 (30%)] Loss: 16864.339844\n",
      "Train Epoch: 410 [70080/225000 (31%)] Loss: 16579.271484\n",
      "Train Epoch: 410 [72576/225000 (32%)] Loss: 16332.088867\n",
      "Train Epoch: 410 [75072/225000 (33%)] Loss: 17357.160156\n",
      "Train Epoch: 410 [77568/225000 (34%)] Loss: 17314.640625\n",
      "Train Epoch: 410 [80064/225000 (36%)] Loss: 16797.269531\n",
      "Train Epoch: 410 [82560/225000 (37%)] Loss: 16544.470703\n",
      "Train Epoch: 410 [85056/225000 (38%)] Loss: 16800.523438\n",
      "Train Epoch: 410 [87552/225000 (39%)] Loss: 16565.986328\n",
      "Train Epoch: 410 [90048/225000 (40%)] Loss: 16549.960938\n",
      "Train Epoch: 410 [92544/225000 (41%)] Loss: 16428.394531\n",
      "Train Epoch: 410 [95040/225000 (42%)] Loss: 16276.980469\n",
      "Train Epoch: 410 [97536/225000 (43%)] Loss: 16662.507812\n",
      "Train Epoch: 410 [100032/225000 (44%)] Loss: 16620.972656\n",
      "Train Epoch: 410 [102528/225000 (46%)] Loss: 16753.507812\n",
      "Train Epoch: 410 [105024/225000 (47%)] Loss: 16712.701172\n",
      "Train Epoch: 410 [107520/225000 (48%)] Loss: 16214.197266\n",
      "Train Epoch: 410 [110016/225000 (49%)] Loss: 16904.367188\n",
      "Train Epoch: 410 [112512/225000 (50%)] Loss: 16355.391602\n",
      "Train Epoch: 410 [115008/225000 (51%)] Loss: 16768.910156\n",
      "Train Epoch: 410 [117504/225000 (52%)] Loss: 16724.312500\n",
      "Train Epoch: 410 [120000/225000 (53%)] Loss: 16537.054688\n",
      "Train Epoch: 410 [122496/225000 (54%)] Loss: 16390.263672\n",
      "Train Epoch: 410 [124992/225000 (56%)] Loss: 16348.792969\n",
      "Train Epoch: 410 [127488/225000 (57%)] Loss: 16301.550781\n",
      "Train Epoch: 410 [129984/225000 (58%)] Loss: 16633.199219\n",
      "Train Epoch: 410 [132480/225000 (59%)] Loss: 16847.863281\n",
      "Train Epoch: 410 [134976/225000 (60%)] Loss: 16722.332031\n",
      "Train Epoch: 410 [137472/225000 (61%)] Loss: 16767.550781\n",
      "Train Epoch: 410 [139968/225000 (62%)] Loss: 16916.070312\n",
      "Train Epoch: 410 [142464/225000 (63%)] Loss: 16874.187500\n",
      "Train Epoch: 410 [144960/225000 (64%)] Loss: 16471.511719\n",
      "Train Epoch: 410 [147456/225000 (66%)] Loss: 16243.121094\n",
      "Train Epoch: 410 [149952/225000 (67%)] Loss: 16511.281250\n",
      "Train Epoch: 410 [152448/225000 (68%)] Loss: 17070.033203\n",
      "Train Epoch: 410 [154944/225000 (69%)] Loss: 16678.777344\n",
      "Train Epoch: 410 [157440/225000 (70%)] Loss: 16574.855469\n",
      "Train Epoch: 410 [159936/225000 (71%)] Loss: 16707.242188\n",
      "Train Epoch: 410 [162432/225000 (72%)] Loss: 16641.794922\n",
      "Train Epoch: 410 [164928/225000 (73%)] Loss: 18026.171875\n",
      "Train Epoch: 410 [167424/225000 (74%)] Loss: 16364.992188\n",
      "Train Epoch: 410 [169920/225000 (76%)] Loss: 17015.423828\n",
      "Train Epoch: 410 [172416/225000 (77%)] Loss: 16290.293945\n",
      "Train Epoch: 410 [174912/225000 (78%)] Loss: 16575.179688\n",
      "Train Epoch: 410 [177408/225000 (79%)] Loss: 16648.431641\n",
      "Train Epoch: 410 [179904/225000 (80%)] Loss: 16832.859375\n",
      "Train Epoch: 410 [182400/225000 (81%)] Loss: 16809.046875\n",
      "Train Epoch: 410 [184896/225000 (82%)] Loss: 16822.695312\n",
      "Train Epoch: 410 [187392/225000 (83%)] Loss: 16323.709961\n",
      "Train Epoch: 410 [189888/225000 (84%)] Loss: 16479.539062\n",
      "Train Epoch: 410 [192384/225000 (86%)] Loss: 16588.398438\n",
      "Train Epoch: 410 [194880/225000 (87%)] Loss: 16160.146484\n",
      "Train Epoch: 410 [197376/225000 (88%)] Loss: 17000.632812\n",
      "Train Epoch: 410 [199872/225000 (89%)] Loss: 16628.636719\n",
      "Train Epoch: 410 [202368/225000 (90%)] Loss: 16881.666016\n",
      "Train Epoch: 410 [204864/225000 (91%)] Loss: 16762.410156\n",
      "Train Epoch: 410 [207360/225000 (92%)] Loss: 16782.486328\n",
      "Train Epoch: 410 [209856/225000 (93%)] Loss: 16192.460938\n",
      "Train Epoch: 410 [212352/225000 (94%)] Loss: 16407.777344\n",
      "Train Epoch: 410 [214848/225000 (95%)] Loss: 16553.878906\n",
      "Train Epoch: 410 [217344/225000 (97%)] Loss: 16496.664062\n",
      "Train Epoch: 410 [219840/225000 (98%)] Loss: 16663.566406\n",
      "Train Epoch: 410 [222336/225000 (99%)] Loss: 16552.740234\n",
      "Train Epoch: 410 [224832/225000 (100%)] Loss: 16702.296875\n",
      "    epoch          : 410\n",
      "    loss           : 16718.596782176566\n",
      "    val_loss       : 16646.379820294052\n",
      "Train Epoch: 411 [192/225000 (0%)] Loss: 16755.542969\n",
      "Train Epoch: 411 [2688/225000 (1%)] Loss: 17141.671875\n",
      "Train Epoch: 411 [5184/225000 (2%)] Loss: 16951.074219\n",
      "Train Epoch: 411 [7680/225000 (3%)] Loss: 16370.829102\n",
      "Train Epoch: 411 [10176/225000 (5%)] Loss: 16607.019531\n",
      "Train Epoch: 411 [12672/225000 (6%)] Loss: 16509.062500\n",
      "Train Epoch: 411 [15168/225000 (7%)] Loss: 16573.583984\n",
      "Train Epoch: 411 [17664/225000 (8%)] Loss: 16803.281250\n",
      "Train Epoch: 411 [20160/225000 (9%)] Loss: 16313.772461\n",
      "Train Epoch: 411 [22656/225000 (10%)] Loss: 16709.246094\n",
      "Train Epoch: 411 [25152/225000 (11%)] Loss: 16603.623047\n",
      "Train Epoch: 411 [27648/225000 (12%)] Loss: 16743.871094\n",
      "Train Epoch: 411 [30144/225000 (13%)] Loss: 16590.804688\n",
      "Train Epoch: 411 [32640/225000 (15%)] Loss: 16147.732422\n",
      "Train Epoch: 411 [35136/225000 (16%)] Loss: 16252.680664\n",
      "Train Epoch: 411 [37632/225000 (17%)] Loss: 16471.324219\n",
      "Train Epoch: 411 [40128/225000 (18%)] Loss: 16840.291016\n",
      "Train Epoch: 411 [42624/225000 (19%)] Loss: 16256.296875\n",
      "Train Epoch: 411 [45120/225000 (20%)] Loss: 16587.113281\n",
      "Train Epoch: 411 [47616/225000 (21%)] Loss: 16502.285156\n",
      "Train Epoch: 411 [50112/225000 (22%)] Loss: 16202.422852\n",
      "Train Epoch: 411 [52608/225000 (23%)] Loss: 16254.697266\n",
      "Train Epoch: 411 [55104/225000 (24%)] Loss: 16585.734375\n",
      "Train Epoch: 411 [57600/225000 (26%)] Loss: 16467.300781\n",
      "Train Epoch: 411 [60096/225000 (27%)] Loss: 16857.945312\n",
      "Train Epoch: 411 [62592/225000 (28%)] Loss: 16656.919922\n",
      "Train Epoch: 411 [65088/225000 (29%)] Loss: 16264.945312\n",
      "Train Epoch: 411 [67584/225000 (30%)] Loss: 16302.368164\n",
      "Train Epoch: 411 [70080/225000 (31%)] Loss: 17142.562500\n",
      "Train Epoch: 411 [72576/225000 (32%)] Loss: 16201.885742\n",
      "Train Epoch: 411 [75072/225000 (33%)] Loss: 16441.742188\n",
      "Train Epoch: 411 [77568/225000 (34%)] Loss: 16602.890625\n",
      "Train Epoch: 411 [80064/225000 (36%)] Loss: 16546.003906\n",
      "Train Epoch: 411 [82560/225000 (37%)] Loss: 16616.304688\n",
      "Train Epoch: 411 [85056/225000 (38%)] Loss: 16295.548828\n",
      "Train Epoch: 411 [87552/225000 (39%)] Loss: 16567.281250\n",
      "Train Epoch: 411 [90048/225000 (40%)] Loss: 16584.632812\n",
      "Train Epoch: 411 [92544/225000 (41%)] Loss: 16675.339844\n",
      "Train Epoch: 411 [95040/225000 (42%)] Loss: 16741.445312\n",
      "Train Epoch: 411 [97536/225000 (43%)] Loss: 16850.218750\n",
      "Train Epoch: 411 [100032/225000 (44%)] Loss: 16543.101562\n",
      "Train Epoch: 411 [102528/225000 (46%)] Loss: 16349.040039\n",
      "Train Epoch: 411 [105024/225000 (47%)] Loss: 16483.863281\n",
      "Train Epoch: 411 [107520/225000 (48%)] Loss: 16933.246094\n",
      "Train Epoch: 411 [110016/225000 (49%)] Loss: 18375.710938\n",
      "Train Epoch: 411 [112512/225000 (50%)] Loss: 16847.042969\n",
      "Train Epoch: 411 [115008/225000 (51%)] Loss: 16453.878906\n",
      "Train Epoch: 411 [117504/225000 (52%)] Loss: 16701.835938\n",
      "Train Epoch: 411 [120000/225000 (53%)] Loss: 16584.736328\n",
      "Train Epoch: 411 [122496/225000 (54%)] Loss: 16833.498047\n",
      "Train Epoch: 411 [124992/225000 (56%)] Loss: 16953.044922\n",
      "Train Epoch: 411 [127488/225000 (57%)] Loss: 16381.112305\n",
      "Train Epoch: 411 [129984/225000 (58%)] Loss: 17972.826172\n",
      "Train Epoch: 411 [132480/225000 (59%)] Loss: 16470.335938\n",
      "Train Epoch: 411 [134976/225000 (60%)] Loss: 16543.527344\n",
      "Train Epoch: 411 [137472/225000 (61%)] Loss: 17900.687500\n",
      "Train Epoch: 411 [139968/225000 (62%)] Loss: 16685.500000\n",
      "Train Epoch: 411 [142464/225000 (63%)] Loss: 16596.464844\n",
      "Train Epoch: 411 [144960/225000 (64%)] Loss: 16721.917969\n",
      "Train Epoch: 411 [147456/225000 (66%)] Loss: 16683.523438\n",
      "Train Epoch: 411 [149952/225000 (67%)] Loss: 16789.457031\n",
      "Train Epoch: 411 [152448/225000 (68%)] Loss: 16788.410156\n",
      "Train Epoch: 411 [154944/225000 (69%)] Loss: 16829.548828\n",
      "Train Epoch: 411 [157440/225000 (70%)] Loss: 17093.964844\n",
      "Train Epoch: 411 [159936/225000 (71%)] Loss: 16821.257812\n",
      "Train Epoch: 411 [162432/225000 (72%)] Loss: 16684.361328\n",
      "Train Epoch: 411 [164928/225000 (73%)] Loss: 16280.101562\n",
      "Train Epoch: 411 [167424/225000 (74%)] Loss: 16508.992188\n",
      "Train Epoch: 411 [169920/225000 (76%)] Loss: 16818.835938\n",
      "Train Epoch: 411 [172416/225000 (77%)] Loss: 16679.353516\n",
      "Train Epoch: 411 [174912/225000 (78%)] Loss: 16802.066406\n",
      "Train Epoch: 411 [177408/225000 (79%)] Loss: 16146.141602\n",
      "Train Epoch: 411 [179904/225000 (80%)] Loss: 16705.343750\n",
      "Train Epoch: 411 [182400/225000 (81%)] Loss: 16925.718750\n",
      "Train Epoch: 411 [184896/225000 (82%)] Loss: 17067.388672\n",
      "Train Epoch: 411 [187392/225000 (83%)] Loss: 16339.888672\n",
      "Train Epoch: 411 [189888/225000 (84%)] Loss: 16634.882812\n",
      "Train Epoch: 411 [192384/225000 (86%)] Loss: 16049.656250\n",
      "Train Epoch: 411 [194880/225000 (87%)] Loss: 16653.355469\n",
      "Train Epoch: 411 [197376/225000 (88%)] Loss: 16505.664062\n",
      "Train Epoch: 411 [199872/225000 (89%)] Loss: 16392.984375\n",
      "Train Epoch: 411 [202368/225000 (90%)] Loss: 16218.671875\n",
      "Train Epoch: 411 [204864/225000 (91%)] Loss: 16713.710938\n",
      "Train Epoch: 411 [207360/225000 (92%)] Loss: 16683.839844\n",
      "Train Epoch: 411 [209856/225000 (93%)] Loss: 17745.806641\n",
      "Train Epoch: 411 [212352/225000 (94%)] Loss: 16822.187500\n",
      "Train Epoch: 411 [214848/225000 (95%)] Loss: 16786.398438\n",
      "Train Epoch: 411 [217344/225000 (97%)] Loss: 16068.289062\n",
      "Train Epoch: 411 [219840/225000 (98%)] Loss: 18464.093750\n",
      "Train Epoch: 411 [222336/225000 (99%)] Loss: 18527.855469\n",
      "Train Epoch: 411 [224832/225000 (100%)] Loss: 16762.359375\n",
      "    epoch          : 411\n",
      "    loss           : 16698.27714710431\n",
      "    val_loss       : 16597.05609035037\n",
      "Train Epoch: 412 [192/225000 (0%)] Loss: 16716.425781\n",
      "Train Epoch: 412 [2688/225000 (1%)] Loss: 16773.273438\n",
      "Train Epoch: 412 [5184/225000 (2%)] Loss: 16819.714844\n",
      "Train Epoch: 412 [7680/225000 (3%)] Loss: 18095.414062\n",
      "Train Epoch: 412 [10176/225000 (5%)] Loss: 17303.617188\n",
      "Train Epoch: 412 [12672/225000 (6%)] Loss: 16754.371094\n",
      "Train Epoch: 412 [15168/225000 (7%)] Loss: 16355.911133\n",
      "Train Epoch: 412 [17664/225000 (8%)] Loss: 16921.460938\n",
      "Train Epoch: 412 [20160/225000 (9%)] Loss: 16557.875000\n",
      "Train Epoch: 412 [22656/225000 (10%)] Loss: 16563.753906\n",
      "Train Epoch: 412 [25152/225000 (11%)] Loss: 16615.121094\n",
      "Train Epoch: 412 [27648/225000 (12%)] Loss: 16763.695312\n",
      "Train Epoch: 412 [30144/225000 (13%)] Loss: 16488.843750\n",
      "Train Epoch: 412 [32640/225000 (15%)] Loss: 16271.884766\n",
      "Train Epoch: 412 [35136/225000 (16%)] Loss: 16361.956055\n",
      "Train Epoch: 412 [37632/225000 (17%)] Loss: 16483.109375\n",
      "Train Epoch: 412 [40128/225000 (18%)] Loss: 16238.213867\n",
      "Train Epoch: 412 [42624/225000 (19%)] Loss: 16859.996094\n",
      "Train Epoch: 412 [45120/225000 (20%)] Loss: 16670.648438\n",
      "Train Epoch: 412 [47616/225000 (21%)] Loss: 16935.359375\n",
      "Train Epoch: 412 [50112/225000 (22%)] Loss: 16789.056641\n",
      "Train Epoch: 412 [52608/225000 (23%)] Loss: 16638.359375\n",
      "Train Epoch: 412 [55104/225000 (24%)] Loss: 16683.621094\n",
      "Train Epoch: 412 [57600/225000 (26%)] Loss: 18672.572266\n",
      "Train Epoch: 412 [60096/225000 (27%)] Loss: 17066.128906\n",
      "Train Epoch: 412 [62592/225000 (28%)] Loss: 16458.589844\n",
      "Train Epoch: 412 [65088/225000 (29%)] Loss: 16576.410156\n",
      "Train Epoch: 412 [67584/225000 (30%)] Loss: 17082.281250\n",
      "Train Epoch: 412 [70080/225000 (31%)] Loss: 16280.454102\n",
      "Train Epoch: 412 [72576/225000 (32%)] Loss: 16294.716797\n",
      "Train Epoch: 412 [75072/225000 (33%)] Loss: 16793.183594\n",
      "Train Epoch: 412 [77568/225000 (34%)] Loss: 16677.109375\n",
      "Train Epoch: 412 [80064/225000 (36%)] Loss: 17144.125000\n",
      "Train Epoch: 412 [82560/225000 (37%)] Loss: 16513.855469\n",
      "Train Epoch: 412 [85056/225000 (38%)] Loss: 16465.789062\n",
      "Train Epoch: 412 [87552/225000 (39%)] Loss: 16707.814453\n",
      "Train Epoch: 412 [90048/225000 (40%)] Loss: 16590.812500\n",
      "Train Epoch: 412 [92544/225000 (41%)] Loss: 16269.838867\n",
      "Train Epoch: 412 [95040/225000 (42%)] Loss: 16352.517578\n",
      "Train Epoch: 412 [97536/225000 (43%)] Loss: 16036.262695\n",
      "Train Epoch: 412 [100032/225000 (44%)] Loss: 16964.535156\n",
      "Train Epoch: 412 [102528/225000 (46%)] Loss: 16585.292969\n",
      "Train Epoch: 412 [105024/225000 (47%)] Loss: 16864.951172\n",
      "Train Epoch: 412 [107520/225000 (48%)] Loss: 16580.583984\n",
      "Train Epoch: 412 [110016/225000 (49%)] Loss: 16545.568359\n",
      "Train Epoch: 412 [112512/225000 (50%)] Loss: 16806.121094\n",
      "Train Epoch: 412 [115008/225000 (51%)] Loss: 16767.703125\n",
      "Train Epoch: 412 [117504/225000 (52%)] Loss: 16285.525391\n",
      "Train Epoch: 412 [120000/225000 (53%)] Loss: 16858.613281\n",
      "Train Epoch: 412 [122496/225000 (54%)] Loss: 16837.226562\n",
      "Train Epoch: 412 [124992/225000 (56%)] Loss: 16610.699219\n",
      "Train Epoch: 412 [127488/225000 (57%)] Loss: 16908.265625\n",
      "Train Epoch: 412 [129984/225000 (58%)] Loss: 16356.134766\n",
      "Train Epoch: 412 [132480/225000 (59%)] Loss: 16215.300781\n",
      "Train Epoch: 412 [134976/225000 (60%)] Loss: 16355.269531\n",
      "Train Epoch: 412 [137472/225000 (61%)] Loss: 16313.670898\n",
      "Train Epoch: 412 [139968/225000 (62%)] Loss: 16891.960938\n",
      "Train Epoch: 412 [142464/225000 (63%)] Loss: 17202.828125\n",
      "Train Epoch: 412 [144960/225000 (64%)] Loss: 16524.648438\n",
      "Train Epoch: 412 [147456/225000 (66%)] Loss: 16709.687500\n",
      "Train Epoch: 412 [149952/225000 (67%)] Loss: 16315.522461\n",
      "Train Epoch: 412 [152448/225000 (68%)] Loss: 16633.283203\n",
      "Train Epoch: 412 [154944/225000 (69%)] Loss: 16157.293945\n",
      "Train Epoch: 412 [157440/225000 (70%)] Loss: 16616.289062\n",
      "Train Epoch: 412 [159936/225000 (71%)] Loss: 16707.492188\n",
      "Train Epoch: 412 [162432/225000 (72%)] Loss: 17968.212891\n",
      "Train Epoch: 412 [164928/225000 (73%)] Loss: 16549.269531\n",
      "Train Epoch: 412 [167424/225000 (74%)] Loss: 16427.859375\n",
      "Train Epoch: 412 [169920/225000 (76%)] Loss: 16576.259766\n",
      "Train Epoch: 412 [172416/225000 (77%)] Loss: 16564.634766\n",
      "Train Epoch: 412 [174912/225000 (78%)] Loss: 16861.835938\n",
      "Train Epoch: 412 [177408/225000 (79%)] Loss: 16295.869141\n",
      "Train Epoch: 412 [179904/225000 (80%)] Loss: 16341.179688\n",
      "Train Epoch: 412 [182400/225000 (81%)] Loss: 16704.527344\n",
      "Train Epoch: 412 [184896/225000 (82%)] Loss: 16543.593750\n",
      "Train Epoch: 412 [187392/225000 (83%)] Loss: 16378.065430\n",
      "Train Epoch: 412 [189888/225000 (84%)] Loss: 16978.175781\n",
      "Train Epoch: 412 [192384/225000 (86%)] Loss: 17286.771484\n",
      "Train Epoch: 412 [194880/225000 (87%)] Loss: 16887.238281\n",
      "Train Epoch: 412 [197376/225000 (88%)] Loss: 16554.769531\n",
      "Train Epoch: 412 [199872/225000 (89%)] Loss: 16574.968750\n",
      "Train Epoch: 412 [202368/225000 (90%)] Loss: 16589.820312\n",
      "Train Epoch: 412 [204864/225000 (91%)] Loss: 16325.228516\n",
      "Train Epoch: 412 [207360/225000 (92%)] Loss: 16874.093750\n",
      "Train Epoch: 412 [209856/225000 (93%)] Loss: 17006.015625\n",
      "Train Epoch: 412 [212352/225000 (94%)] Loss: 16360.649414\n",
      "Train Epoch: 412 [214848/225000 (95%)] Loss: 16721.031250\n",
      "Train Epoch: 412 [217344/225000 (97%)] Loss: 16712.914062\n",
      "Train Epoch: 412 [219840/225000 (98%)] Loss: 16533.406250\n",
      "Train Epoch: 412 [222336/225000 (99%)] Loss: 16619.976562\n",
      "Train Epoch: 412 [224832/225000 (100%)] Loss: 17138.117188\n",
      "    epoch          : 412\n",
      "    loss           : 16697.479967970085\n",
      "    val_loss       : 16679.432849085966\n",
      "Train Epoch: 413 [192/225000 (0%)] Loss: 17196.457031\n",
      "Train Epoch: 413 [2688/225000 (1%)] Loss: 16854.718750\n",
      "Train Epoch: 413 [5184/225000 (2%)] Loss: 16551.792969\n",
      "Train Epoch: 413 [7680/225000 (3%)] Loss: 16109.292969\n",
      "Train Epoch: 413 [10176/225000 (5%)] Loss: 16729.775391\n",
      "Train Epoch: 413 [12672/225000 (6%)] Loss: 16069.020508\n",
      "Train Epoch: 413 [15168/225000 (7%)] Loss: 16736.283203\n",
      "Train Epoch: 413 [17664/225000 (8%)] Loss: 16370.206055\n",
      "Train Epoch: 413 [20160/225000 (9%)] Loss: 16312.490234\n",
      "Train Epoch: 413 [22656/225000 (10%)] Loss: 16913.496094\n",
      "Train Epoch: 413 [25152/225000 (11%)] Loss: 16506.574219\n",
      "Train Epoch: 413 [27648/225000 (12%)] Loss: 16715.802734\n",
      "Train Epoch: 413 [30144/225000 (13%)] Loss: 16620.542969\n",
      "Train Epoch: 413 [32640/225000 (15%)] Loss: 16755.716797\n",
      "Train Epoch: 413 [35136/225000 (16%)] Loss: 16581.630859\n",
      "Train Epoch: 413 [37632/225000 (17%)] Loss: 16402.000000\n",
      "Train Epoch: 413 [40128/225000 (18%)] Loss: 16582.750000\n",
      "Train Epoch: 413 [42624/225000 (19%)] Loss: 16728.613281\n",
      "Train Epoch: 413 [45120/225000 (20%)] Loss: 16720.859375\n",
      "Train Epoch: 413 [47616/225000 (21%)] Loss: 16580.923828\n",
      "Train Epoch: 413 [50112/225000 (22%)] Loss: 16378.411133\n",
      "Train Epoch: 413 [52608/225000 (23%)] Loss: 16635.863281\n",
      "Train Epoch: 413 [55104/225000 (24%)] Loss: 16972.746094\n",
      "Train Epoch: 413 [57600/225000 (26%)] Loss: 17494.105469\n",
      "Train Epoch: 413 [60096/225000 (27%)] Loss: 16784.472656\n",
      "Train Epoch: 413 [62592/225000 (28%)] Loss: 16824.451172\n",
      "Train Epoch: 413 [65088/225000 (29%)] Loss: 16337.873047\n",
      "Train Epoch: 413 [67584/225000 (30%)] Loss: 16657.304688\n",
      "Train Epoch: 413 [70080/225000 (31%)] Loss: 16381.072266\n",
      "Train Epoch: 413 [72576/225000 (32%)] Loss: 16655.144531\n",
      "Train Epoch: 413 [75072/225000 (33%)] Loss: 16955.406250\n",
      "Train Epoch: 413 [77568/225000 (34%)] Loss: 16452.667969\n",
      "Train Epoch: 413 [80064/225000 (36%)] Loss: 16863.572266\n",
      "Train Epoch: 413 [82560/225000 (37%)] Loss: 16548.375000\n",
      "Train Epoch: 413 [85056/225000 (38%)] Loss: 16639.535156\n",
      "Train Epoch: 413 [87552/225000 (39%)] Loss: 16480.853516\n",
      "Train Epoch: 413 [90048/225000 (40%)] Loss: 16469.183594\n",
      "Train Epoch: 413 [92544/225000 (41%)] Loss: 16730.017578\n",
      "Train Epoch: 413 [95040/225000 (42%)] Loss: 16501.652344\n",
      "Train Epoch: 413 [97536/225000 (43%)] Loss: 16281.624023\n",
      "Train Epoch: 413 [100032/225000 (44%)] Loss: 16401.703125\n",
      "Train Epoch: 413 [102528/225000 (46%)] Loss: 16448.277344\n",
      "Train Epoch: 413 [105024/225000 (47%)] Loss: 16821.332031\n",
      "Train Epoch: 413 [107520/225000 (48%)] Loss: 16744.476562\n",
      "Train Epoch: 413 [110016/225000 (49%)] Loss: 18111.105469\n",
      "Train Epoch: 413 [112512/225000 (50%)] Loss: 16805.425781\n",
      "Train Epoch: 413 [115008/225000 (51%)] Loss: 16128.704102\n",
      "Train Epoch: 413 [117504/225000 (52%)] Loss: 16892.458984\n",
      "Train Epoch: 413 [120000/225000 (53%)] Loss: 16457.375000\n",
      "Train Epoch: 413 [122496/225000 (54%)] Loss: 16707.402344\n",
      "Train Epoch: 413 [124992/225000 (56%)] Loss: 16427.324219\n",
      "Train Epoch: 413 [127488/225000 (57%)] Loss: 16860.296875\n",
      "Train Epoch: 413 [129984/225000 (58%)] Loss: 17032.703125\n",
      "Train Epoch: 413 [132480/225000 (59%)] Loss: 16977.794922\n",
      "Train Epoch: 413 [134976/225000 (60%)] Loss: 16437.193359\n",
      "Train Epoch: 413 [137472/225000 (61%)] Loss: 16614.929688\n",
      "Train Epoch: 413 [139968/225000 (62%)] Loss: 16772.402344\n",
      "Train Epoch: 413 [142464/225000 (63%)] Loss: 16780.630859\n",
      "Train Epoch: 413 [144960/225000 (64%)] Loss: 16663.091797\n",
      "Train Epoch: 413 [147456/225000 (66%)] Loss: 16331.530273\n",
      "Train Epoch: 413 [149952/225000 (67%)] Loss: 16513.265625\n",
      "Train Epoch: 413 [152448/225000 (68%)] Loss: 16884.371094\n",
      "Train Epoch: 413 [154944/225000 (69%)] Loss: 16439.535156\n",
      "Train Epoch: 413 [157440/225000 (70%)] Loss: 16551.957031\n",
      "Train Epoch: 413 [159936/225000 (71%)] Loss: 16850.322266\n",
      "Train Epoch: 413 [162432/225000 (72%)] Loss: 16871.335938\n",
      "Train Epoch: 413 [164928/225000 (73%)] Loss: 16213.845703\n",
      "Train Epoch: 413 [167424/225000 (74%)] Loss: 16311.635742\n",
      "Train Epoch: 413 [169920/225000 (76%)] Loss: 16644.105469\n",
      "Train Epoch: 413 [172416/225000 (77%)] Loss: 16771.273438\n",
      "Train Epoch: 413 [174912/225000 (78%)] Loss: 16518.203125\n",
      "Train Epoch: 413 [177408/225000 (79%)] Loss: 15839.881836\n",
      "Train Epoch: 413 [179904/225000 (80%)] Loss: 19789.552734\n",
      "Train Epoch: 413 [182400/225000 (81%)] Loss: 16816.833984\n",
      "Train Epoch: 413 [184896/225000 (82%)] Loss: 16931.121094\n",
      "Train Epoch: 413 [187392/225000 (83%)] Loss: 16963.421875\n",
      "Train Epoch: 413 [189888/225000 (84%)] Loss: 16501.853516\n",
      "Train Epoch: 413 [192384/225000 (86%)] Loss: 17013.007812\n",
      "Train Epoch: 413 [194880/225000 (87%)] Loss: 16904.296875\n",
      "Train Epoch: 413 [197376/225000 (88%)] Loss: 16792.195312\n",
      "Train Epoch: 413 [199872/225000 (89%)] Loss: 16361.257812\n",
      "Train Epoch: 413 [202368/225000 (90%)] Loss: 16420.537109\n",
      "Train Epoch: 413 [204864/225000 (91%)] Loss: 16233.666016\n",
      "Train Epoch: 413 [207360/225000 (92%)] Loss: 16379.947266\n",
      "Train Epoch: 413 [209856/225000 (93%)] Loss: 16740.925781\n",
      "Train Epoch: 413 [212352/225000 (94%)] Loss: 16792.501953\n",
      "Train Epoch: 413 [214848/225000 (95%)] Loss: 16933.349609\n",
      "Train Epoch: 413 [217344/225000 (97%)] Loss: 16588.095703\n",
      "Train Epoch: 413 [219840/225000 (98%)] Loss: 16805.257812\n",
      "Train Epoch: 413 [222336/225000 (99%)] Loss: 16571.027344\n",
      "Train Epoch: 413 [224832/225000 (100%)] Loss: 16349.212891\n",
      "    epoch          : 413\n",
      "    loss           : 16715.183060473548\n",
      "    val_loss       : 16652.93558959424\n",
      "Train Epoch: 414 [192/225000 (0%)] Loss: 16898.242188\n",
      "Train Epoch: 414 [2688/225000 (1%)] Loss: 16560.621094\n",
      "Train Epoch: 414 [5184/225000 (2%)] Loss: 16635.621094\n",
      "Train Epoch: 414 [7680/225000 (3%)] Loss: 16897.917969\n",
      "Train Epoch: 414 [10176/225000 (5%)] Loss: 17111.064453\n",
      "Train Epoch: 414 [12672/225000 (6%)] Loss: 16469.039062\n",
      "Train Epoch: 414 [15168/225000 (7%)] Loss: 16846.802734\n",
      "Train Epoch: 414 [17664/225000 (8%)] Loss: 17579.976562\n",
      "Train Epoch: 414 [20160/225000 (9%)] Loss: 16425.382812\n",
      "Train Epoch: 414 [22656/225000 (10%)] Loss: 16643.673828\n",
      "Train Epoch: 414 [25152/225000 (11%)] Loss: 16550.439453\n",
      "Train Epoch: 414 [27648/225000 (12%)] Loss: 16897.742188\n",
      "Train Epoch: 414 [30144/225000 (13%)] Loss: 16807.597656\n",
      "Train Epoch: 414 [32640/225000 (15%)] Loss: 16315.366211\n",
      "Train Epoch: 414 [35136/225000 (16%)] Loss: 16761.023438\n",
      "Train Epoch: 414 [37632/225000 (17%)] Loss: 16703.281250\n",
      "Train Epoch: 414 [40128/225000 (18%)] Loss: 16449.023438\n",
      "Train Epoch: 414 [42624/225000 (19%)] Loss: 16368.194336\n",
      "Train Epoch: 414 [45120/225000 (20%)] Loss: 16348.797852\n",
      "Train Epoch: 414 [47616/225000 (21%)] Loss: 17416.460938\n",
      "Train Epoch: 414 [50112/225000 (22%)] Loss: 16409.589844\n",
      "Train Epoch: 414 [52608/225000 (23%)] Loss: 16451.625000\n",
      "Train Epoch: 414 [55104/225000 (24%)] Loss: 16570.232422\n",
      "Train Epoch: 414 [57600/225000 (26%)] Loss: 16266.470703\n",
      "Train Epoch: 414 [60096/225000 (27%)] Loss: 16807.167969\n",
      "Train Epoch: 414 [62592/225000 (28%)] Loss: 16377.437500\n",
      "Train Epoch: 414 [65088/225000 (29%)] Loss: 16469.125000\n",
      "Train Epoch: 414 [67584/225000 (30%)] Loss: 16142.811523\n",
      "Train Epoch: 414 [70080/225000 (31%)] Loss: 18299.171875\n",
      "Train Epoch: 414 [72576/225000 (32%)] Loss: 16599.324219\n",
      "Train Epoch: 414 [75072/225000 (33%)] Loss: 16589.628906\n",
      "Train Epoch: 414 [77568/225000 (34%)] Loss: 16816.390625\n",
      "Train Epoch: 414 [80064/225000 (36%)] Loss: 16190.884766\n",
      "Train Epoch: 414 [82560/225000 (37%)] Loss: 17239.791016\n",
      "Train Epoch: 414 [85056/225000 (38%)] Loss: 16498.503906\n",
      "Train Epoch: 414 [87552/225000 (39%)] Loss: 17140.746094\n",
      "Train Epoch: 414 [90048/225000 (40%)] Loss: 16668.177734\n",
      "Train Epoch: 414 [92544/225000 (41%)] Loss: 16848.039062\n",
      "Train Epoch: 414 [95040/225000 (42%)] Loss: 17800.671875\n",
      "Train Epoch: 414 [97536/225000 (43%)] Loss: 16318.011719\n",
      "Train Epoch: 414 [100032/225000 (44%)] Loss: 16528.367188\n",
      "Train Epoch: 414 [102528/225000 (46%)] Loss: 16297.049805\n",
      "Train Epoch: 414 [105024/225000 (47%)] Loss: 16456.843750\n",
      "Train Epoch: 414 [107520/225000 (48%)] Loss: 16448.351562\n",
      "Train Epoch: 414 [110016/225000 (49%)] Loss: 16743.681641\n",
      "Train Epoch: 414 [112512/225000 (50%)] Loss: 16509.341797\n",
      "Train Epoch: 414 [115008/225000 (51%)] Loss: 16471.085938\n",
      "Train Epoch: 414 [117504/225000 (52%)] Loss: 16045.777344\n",
      "Train Epoch: 414 [120000/225000 (53%)] Loss: 16839.382812\n",
      "Train Epoch: 414 [122496/225000 (54%)] Loss: 16381.853516\n",
      "Train Epoch: 414 [124992/225000 (56%)] Loss: 16164.635742\n",
      "Train Epoch: 414 [127488/225000 (57%)] Loss: 16772.210938\n",
      "Train Epoch: 414 [129984/225000 (58%)] Loss: 16581.160156\n",
      "Train Epoch: 414 [132480/225000 (59%)] Loss: 16556.054688\n",
      "Train Epoch: 414 [134976/225000 (60%)] Loss: 16548.824219\n",
      "Train Epoch: 414 [137472/225000 (61%)] Loss: 16490.230469\n",
      "Train Epoch: 414 [139968/225000 (62%)] Loss: 16444.750000\n",
      "Train Epoch: 414 [142464/225000 (63%)] Loss: 16555.597656\n",
      "Train Epoch: 414 [144960/225000 (64%)] Loss: 16889.013672\n",
      "Train Epoch: 414 [147456/225000 (66%)] Loss: 16461.968750\n",
      "Train Epoch: 414 [149952/225000 (67%)] Loss: 16668.753906\n",
      "Train Epoch: 414 [152448/225000 (68%)] Loss: 17011.990234\n",
      "Train Epoch: 414 [154944/225000 (69%)] Loss: 16314.007812\n",
      "Train Epoch: 414 [157440/225000 (70%)] Loss: 16760.619141\n",
      "Train Epoch: 414 [159936/225000 (71%)] Loss: 16602.980469\n",
      "Train Epoch: 414 [162432/225000 (72%)] Loss: 16504.621094\n",
      "Train Epoch: 414 [164928/225000 (73%)] Loss: 17073.988281\n",
      "Train Epoch: 414 [167424/225000 (74%)] Loss: 16661.640625\n",
      "Train Epoch: 414 [169920/225000 (76%)] Loss: 16620.027344\n",
      "Train Epoch: 414 [172416/225000 (77%)] Loss: 17035.875000\n",
      "Train Epoch: 414 [174912/225000 (78%)] Loss: 16743.546875\n",
      "Train Epoch: 414 [177408/225000 (79%)] Loss: 16772.613281\n",
      "Train Epoch: 414 [179904/225000 (80%)] Loss: 16659.349609\n",
      "Train Epoch: 414 [182400/225000 (81%)] Loss: 18011.660156\n",
      "Train Epoch: 414 [184896/225000 (82%)] Loss: 17635.890625\n",
      "Train Epoch: 414 [187392/225000 (83%)] Loss: 16630.230469\n",
      "Train Epoch: 414 [189888/225000 (84%)] Loss: 16321.180664\n",
      "Train Epoch: 414 [192384/225000 (86%)] Loss: 16591.976562\n",
      "Train Epoch: 414 [194880/225000 (87%)] Loss: 16913.162109\n",
      "Train Epoch: 414 [197376/225000 (88%)] Loss: 16604.925781\n",
      "Train Epoch: 414 [199872/225000 (89%)] Loss: 16952.148438\n",
      "Train Epoch: 414 [202368/225000 (90%)] Loss: 16595.992188\n",
      "Train Epoch: 414 [204864/225000 (91%)] Loss: 16954.765625\n",
      "Train Epoch: 414 [207360/225000 (92%)] Loss: 16591.824219\n",
      "Train Epoch: 414 [209856/225000 (93%)] Loss: 16920.283203\n",
      "Train Epoch: 414 [212352/225000 (94%)] Loss: 16728.966797\n",
      "Train Epoch: 414 [214848/225000 (95%)] Loss: 16315.890625\n",
      "Train Epoch: 414 [217344/225000 (97%)] Loss: 16924.976562\n",
      "Train Epoch: 414 [219840/225000 (98%)] Loss: 16336.034180\n",
      "Train Epoch: 414 [222336/225000 (99%)] Loss: 16074.754883\n",
      "Train Epoch: 414 [224832/225000 (100%)] Loss: 16385.531250\n",
      "    epoch          : 414\n",
      "    loss           : 16716.693962643985\n",
      "    val_loss       : 16677.377877182633\n",
      "Train Epoch: 415 [192/225000 (0%)] Loss: 16824.699219\n",
      "Train Epoch: 415 [2688/225000 (1%)] Loss: 17032.394531\n",
      "Train Epoch: 415 [5184/225000 (2%)] Loss: 16846.568359\n",
      "Train Epoch: 415 [7680/225000 (3%)] Loss: 16424.957031\n",
      "Train Epoch: 415 [10176/225000 (5%)] Loss: 16137.244141\n",
      "Train Epoch: 415 [12672/225000 (6%)] Loss: 16748.371094\n",
      "Train Epoch: 415 [15168/225000 (7%)] Loss: 16794.724609\n",
      "Train Epoch: 415 [17664/225000 (8%)] Loss: 16697.433594\n",
      "Train Epoch: 415 [20160/225000 (9%)] Loss: 16854.521484\n",
      "Train Epoch: 415 [22656/225000 (10%)] Loss: 17202.664062\n",
      "Train Epoch: 415 [25152/225000 (11%)] Loss: 16280.178711\n",
      "Train Epoch: 415 [27648/225000 (12%)] Loss: 16416.044922\n",
      "Train Epoch: 415 [30144/225000 (13%)] Loss: 16138.100586\n",
      "Train Epoch: 415 [32640/225000 (15%)] Loss: 16207.104492\n",
      "Train Epoch: 415 [35136/225000 (16%)] Loss: 16565.439453\n",
      "Train Epoch: 415 [37632/225000 (17%)] Loss: 16626.191406\n",
      "Train Epoch: 415 [40128/225000 (18%)] Loss: 18148.828125\n",
      "Train Epoch: 415 [42624/225000 (19%)] Loss: 16242.949219\n",
      "Train Epoch: 415 [45120/225000 (20%)] Loss: 16786.570312\n",
      "Train Epoch: 415 [47616/225000 (21%)] Loss: 16396.675781\n",
      "Train Epoch: 415 [50112/225000 (22%)] Loss: 16412.601562\n",
      "Train Epoch: 415 [52608/225000 (23%)] Loss: 16379.649414\n",
      "Train Epoch: 415 [55104/225000 (24%)] Loss: 18378.484375\n",
      "Train Epoch: 415 [57600/225000 (26%)] Loss: 16457.662109\n",
      "Train Epoch: 415 [60096/225000 (27%)] Loss: 16433.792969\n",
      "Train Epoch: 415 [62592/225000 (28%)] Loss: 16683.212891\n",
      "Train Epoch: 415 [65088/225000 (29%)] Loss: 17008.257812\n",
      "Train Epoch: 415 [67584/225000 (30%)] Loss: 16490.529297\n",
      "Train Epoch: 415 [70080/225000 (31%)] Loss: 16542.289062\n",
      "Train Epoch: 415 [72576/225000 (32%)] Loss: 16582.777344\n",
      "Train Epoch: 415 [75072/225000 (33%)] Loss: 16453.826172\n",
      "Train Epoch: 415 [77568/225000 (34%)] Loss: 16708.562500\n",
      "Train Epoch: 415 [80064/225000 (36%)] Loss: 16584.265625\n",
      "Train Epoch: 415 [82560/225000 (37%)] Loss: 18018.777344\n",
      "Train Epoch: 415 [85056/225000 (38%)] Loss: 16144.899414\n",
      "Train Epoch: 415 [87552/225000 (39%)] Loss: 16838.369141\n",
      "Train Epoch: 415 [90048/225000 (40%)] Loss: 16817.886719\n",
      "Train Epoch: 415 [92544/225000 (41%)] Loss: 16157.643555\n",
      "Train Epoch: 415 [95040/225000 (42%)] Loss: 16655.802734\n",
      "Train Epoch: 415 [97536/225000 (43%)] Loss: 16502.875000\n",
      "Train Epoch: 415 [100032/225000 (44%)] Loss: 16405.171875\n",
      "Train Epoch: 415 [102528/225000 (46%)] Loss: 16873.703125\n",
      "Train Epoch: 415 [105024/225000 (47%)] Loss: 16689.808594\n",
      "Train Epoch: 415 [107520/225000 (48%)] Loss: 16788.921875\n",
      "Train Epoch: 415 [110016/225000 (49%)] Loss: 16596.957031\n",
      "Train Epoch: 415 [112512/225000 (50%)] Loss: 16231.646484\n",
      "Train Epoch: 415 [115008/225000 (51%)] Loss: 16554.367188\n",
      "Train Epoch: 415 [117504/225000 (52%)] Loss: 16308.598633\n",
      "Train Epoch: 415 [120000/225000 (53%)] Loss: 16633.617188\n",
      "Train Epoch: 415 [122496/225000 (54%)] Loss: 16563.507812\n",
      "Train Epoch: 415 [124992/225000 (56%)] Loss: 16676.523438\n",
      "Train Epoch: 415 [127488/225000 (57%)] Loss: 17215.212891\n",
      "Train Epoch: 415 [129984/225000 (58%)] Loss: 16871.435547\n",
      "Train Epoch: 415 [132480/225000 (59%)] Loss: 16332.032227\n",
      "Train Epoch: 415 [134976/225000 (60%)] Loss: 17078.488281\n",
      "Train Epoch: 415 [137472/225000 (61%)] Loss: 16864.585938\n",
      "Train Epoch: 415 [139968/225000 (62%)] Loss: 16599.777344\n",
      "Train Epoch: 415 [142464/225000 (63%)] Loss: 16778.576172\n",
      "Train Epoch: 415 [144960/225000 (64%)] Loss: 16991.582031\n",
      "Train Epoch: 415 [147456/225000 (66%)] Loss: 16730.117188\n",
      "Train Epoch: 415 [149952/225000 (67%)] Loss: 16768.765625\n",
      "Train Epoch: 415 [152448/225000 (68%)] Loss: 16041.615234\n",
      "Train Epoch: 415 [154944/225000 (69%)] Loss: 16848.183594\n",
      "Train Epoch: 415 [157440/225000 (70%)] Loss: 16723.843750\n",
      "Train Epoch: 415 [159936/225000 (71%)] Loss: 16135.475586\n",
      "Train Epoch: 415 [162432/225000 (72%)] Loss: 16641.195312\n",
      "Train Epoch: 415 [164928/225000 (73%)] Loss: 16717.525391\n",
      "Train Epoch: 415 [167424/225000 (74%)] Loss: 16365.732422\n",
      "Train Epoch: 415 [169920/225000 (76%)] Loss: 16493.097656\n",
      "Train Epoch: 415 [172416/225000 (77%)] Loss: 16435.384766\n",
      "Train Epoch: 415 [174912/225000 (78%)] Loss: 16435.441406\n",
      "Train Epoch: 415 [177408/225000 (79%)] Loss: 16984.078125\n",
      "Train Epoch: 415 [179904/225000 (80%)] Loss: 16617.796875\n",
      "Train Epoch: 415 [182400/225000 (81%)] Loss: 17662.804688\n",
      "Train Epoch: 415 [184896/225000 (82%)] Loss: 16581.396484\n",
      "Train Epoch: 415 [187392/225000 (83%)] Loss: 16871.712891\n",
      "Train Epoch: 415 [189888/225000 (84%)] Loss: 16734.146484\n",
      "Train Epoch: 415 [192384/225000 (86%)] Loss: 16566.208984\n",
      "Train Epoch: 415 [194880/225000 (87%)] Loss: 16488.980469\n",
      "Train Epoch: 415 [197376/225000 (88%)] Loss: 16506.162109\n",
      "Train Epoch: 415 [199872/225000 (89%)] Loss: 16759.636719\n",
      "Train Epoch: 415 [202368/225000 (90%)] Loss: 16133.787109\n",
      "Train Epoch: 415 [204864/225000 (91%)] Loss: 16333.657227\n",
      "Train Epoch: 415 [207360/225000 (92%)] Loss: 16467.164062\n",
      "Train Epoch: 415 [209856/225000 (93%)] Loss: 16973.853516\n",
      "Train Epoch: 415 [212352/225000 (94%)] Loss: 16514.656250\n",
      "Train Epoch: 415 [214848/225000 (95%)] Loss: 16821.886719\n",
      "Train Epoch: 415 [217344/225000 (97%)] Loss: 16516.750000\n",
      "Train Epoch: 415 [219840/225000 (98%)] Loss: 16746.179688\n",
      "Train Epoch: 415 [222336/225000 (99%)] Loss: 16818.445312\n",
      "Train Epoch: 415 [224832/225000 (100%)] Loss: 16557.689453\n",
      "    epoch          : 415\n",
      "    loss           : 16723.779725995893\n",
      "    val_loss       : 16771.604858660517\n",
      "Train Epoch: 416 [192/225000 (0%)] Loss: 16929.082031\n",
      "Train Epoch: 416 [2688/225000 (1%)] Loss: 16742.550781\n",
      "Train Epoch: 416 [5184/225000 (2%)] Loss: 16364.711914\n",
      "Train Epoch: 416 [7680/225000 (3%)] Loss: 17233.865234\n",
      "Train Epoch: 416 [10176/225000 (5%)] Loss: 16843.175781\n",
      "Train Epoch: 416 [12672/225000 (6%)] Loss: 16703.218750\n",
      "Train Epoch: 416 [15168/225000 (7%)] Loss: 16541.175781\n",
      "Train Epoch: 416 [17664/225000 (8%)] Loss: 16324.534180\n",
      "Train Epoch: 416 [20160/225000 (9%)] Loss: 18478.500000\n",
      "Train Epoch: 416 [22656/225000 (10%)] Loss: 16521.289062\n",
      "Train Epoch: 416 [25152/225000 (11%)] Loss: 16668.787109\n",
      "Train Epoch: 416 [27648/225000 (12%)] Loss: 16433.132812\n",
      "Train Epoch: 416 [30144/225000 (13%)] Loss: 16670.242188\n",
      "Train Epoch: 416 [32640/225000 (15%)] Loss: 16574.312500\n",
      "Train Epoch: 416 [35136/225000 (16%)] Loss: 16618.585938\n",
      "Train Epoch: 416 [37632/225000 (17%)] Loss: 16287.342773\n",
      "Train Epoch: 416 [40128/225000 (18%)] Loss: 16417.539062\n",
      "Train Epoch: 416 [42624/225000 (19%)] Loss: 16741.103516\n",
      "Train Epoch: 416 [45120/225000 (20%)] Loss: 16660.289062\n",
      "Train Epoch: 416 [47616/225000 (21%)] Loss: 16588.843750\n",
      "Train Epoch: 416 [50112/225000 (22%)] Loss: 16645.617188\n",
      "Train Epoch: 416 [52608/225000 (23%)] Loss: 16586.144531\n",
      "Train Epoch: 416 [55104/225000 (24%)] Loss: 16576.781250\n",
      "Train Epoch: 416 [57600/225000 (26%)] Loss: 16632.496094\n",
      "Train Epoch: 416 [60096/225000 (27%)] Loss: 16583.992188\n",
      "Train Epoch: 416 [62592/225000 (28%)] Loss: 17575.207031\n",
      "Train Epoch: 416 [65088/225000 (29%)] Loss: 15955.149414\n",
      "Train Epoch: 416 [67584/225000 (30%)] Loss: 17195.957031\n",
      "Train Epoch: 416 [70080/225000 (31%)] Loss: 16667.828125\n",
      "Train Epoch: 416 [72576/225000 (32%)] Loss: 16274.680664\n",
      "Train Epoch: 416 [75072/225000 (33%)] Loss: 16818.250000\n",
      "Train Epoch: 416 [77568/225000 (34%)] Loss: 16522.968750\n",
      "Train Epoch: 416 [80064/225000 (36%)] Loss: 16689.707031\n",
      "Train Epoch: 416 [82560/225000 (37%)] Loss: 17115.000000\n",
      "Train Epoch: 416 [85056/225000 (38%)] Loss: 17016.466797\n",
      "Train Epoch: 416 [87552/225000 (39%)] Loss: 18051.892578\n",
      "Train Epoch: 416 [90048/225000 (40%)] Loss: 16793.585938\n",
      "Train Epoch: 416 [92544/225000 (41%)] Loss: 16494.728516\n",
      "Train Epoch: 416 [95040/225000 (42%)] Loss: 16675.703125\n",
      "Train Epoch: 416 [97536/225000 (43%)] Loss: 16722.476562\n",
      "Train Epoch: 416 [100032/225000 (44%)] Loss: 16783.294922\n",
      "Train Epoch: 416 [102528/225000 (46%)] Loss: 16792.720703\n",
      "Train Epoch: 416 [105024/225000 (47%)] Loss: 15953.153320\n",
      "Train Epoch: 416 [107520/225000 (48%)] Loss: 16465.535156\n",
      "Train Epoch: 416 [110016/225000 (49%)] Loss: 16890.253906\n",
      "Train Epoch: 416 [112512/225000 (50%)] Loss: 16712.664062\n",
      "Train Epoch: 416 [115008/225000 (51%)] Loss: 16542.386719\n",
      "Train Epoch: 416 [117504/225000 (52%)] Loss: 16624.449219\n",
      "Train Epoch: 416 [120000/225000 (53%)] Loss: 16582.535156\n",
      "Train Epoch: 416 [122496/225000 (54%)] Loss: 17271.779297\n",
      "Train Epoch: 416 [124992/225000 (56%)] Loss: 16825.798828\n",
      "Train Epoch: 416 [127488/225000 (57%)] Loss: 16658.042969\n",
      "Train Epoch: 416 [129984/225000 (58%)] Loss: 16972.708984\n",
      "Train Epoch: 416 [132480/225000 (59%)] Loss: 16674.132812\n",
      "Train Epoch: 416 [134976/225000 (60%)] Loss: 16954.324219\n",
      "Train Epoch: 416 [137472/225000 (61%)] Loss: 16376.771484\n",
      "Train Epoch: 416 [139968/225000 (62%)] Loss: 16832.568359\n",
      "Train Epoch: 416 [142464/225000 (63%)] Loss: 16674.830078\n",
      "Train Epoch: 416 [144960/225000 (64%)] Loss: 16732.945312\n",
      "Train Epoch: 416 [147456/225000 (66%)] Loss: 16764.015625\n",
      "Train Epoch: 416 [149952/225000 (67%)] Loss: 16607.621094\n",
      "Train Epoch: 416 [152448/225000 (68%)] Loss: 16710.886719\n",
      "Train Epoch: 416 [154944/225000 (69%)] Loss: 16519.453125\n",
      "Train Epoch: 416 [157440/225000 (70%)] Loss: 16226.638672\n",
      "Train Epoch: 416 [159936/225000 (71%)] Loss: 18422.853516\n",
      "Train Epoch: 416 [162432/225000 (72%)] Loss: 16440.386719\n",
      "Train Epoch: 416 [164928/225000 (73%)] Loss: 16128.021484\n",
      "Train Epoch: 416 [167424/225000 (74%)] Loss: 16755.234375\n",
      "Train Epoch: 416 [169920/225000 (76%)] Loss: 17033.640625\n",
      "Train Epoch: 416 [172416/225000 (77%)] Loss: 16907.488281\n",
      "Train Epoch: 416 [174912/225000 (78%)] Loss: 17499.132812\n",
      "Train Epoch: 416 [177408/225000 (79%)] Loss: 16596.921875\n",
      "Train Epoch: 416 [179904/225000 (80%)] Loss: 16192.461914\n",
      "Train Epoch: 416 [182400/225000 (81%)] Loss: 16488.886719\n",
      "Train Epoch: 416 [184896/225000 (82%)] Loss: 16558.167969\n",
      "Train Epoch: 416 [187392/225000 (83%)] Loss: 16891.957031\n",
      "Train Epoch: 416 [189888/225000 (84%)] Loss: 16628.273438\n",
      "Train Epoch: 416 [192384/225000 (86%)] Loss: 17496.906250\n",
      "Train Epoch: 416 [194880/225000 (87%)] Loss: 16661.980469\n",
      "Train Epoch: 416 [197376/225000 (88%)] Loss: 16969.589844\n",
      "Train Epoch: 416 [199872/225000 (89%)] Loss: 16410.826172\n",
      "Train Epoch: 416 [202368/225000 (90%)] Loss: 16732.546875\n",
      "Train Epoch: 416 [204864/225000 (91%)] Loss: 16671.664062\n",
      "Train Epoch: 416 [207360/225000 (92%)] Loss: 16655.503906\n",
      "Train Epoch: 416 [209856/225000 (93%)] Loss: 16855.398438\n",
      "Train Epoch: 416 [212352/225000 (94%)] Loss: 16850.597656\n",
      "Train Epoch: 416 [214848/225000 (95%)] Loss: 16646.945312\n",
      "Train Epoch: 416 [217344/225000 (97%)] Loss: 16612.681641\n",
      "Train Epoch: 416 [219840/225000 (98%)] Loss: 17127.394531\n",
      "Train Epoch: 416 [222336/225000 (99%)] Loss: 16409.695312\n",
      "Train Epoch: 416 [224832/225000 (100%)] Loss: 16407.105469\n",
      "    epoch          : 416\n",
      "    loss           : 16714.1162634319\n",
      "    val_loss       : 16610.415607735402\n",
      "Train Epoch: 417 [192/225000 (0%)] Loss: 16693.886719\n",
      "Train Epoch: 417 [2688/225000 (1%)] Loss: 16813.136719\n",
      "Train Epoch: 417 [5184/225000 (2%)] Loss: 16377.104492\n",
      "Train Epoch: 417 [7680/225000 (3%)] Loss: 16490.199219\n",
      "Train Epoch: 417 [10176/225000 (5%)] Loss: 16697.851562\n",
      "Train Epoch: 417 [12672/225000 (6%)] Loss: 16807.136719\n",
      "Train Epoch: 417 [15168/225000 (7%)] Loss: 18350.445312\n",
      "Train Epoch: 417 [17664/225000 (8%)] Loss: 18338.937500\n",
      "Train Epoch: 417 [20160/225000 (9%)] Loss: 16374.052734\n",
      "Train Epoch: 417 [22656/225000 (10%)] Loss: 16635.125000\n",
      "Train Epoch: 417 [25152/225000 (11%)] Loss: 16930.978516\n",
      "Train Epoch: 417 [27648/225000 (12%)] Loss: 16114.156250\n",
      "Train Epoch: 417 [30144/225000 (13%)] Loss: 16647.140625\n",
      "Train Epoch: 417 [32640/225000 (15%)] Loss: 16308.661133\n",
      "Train Epoch: 417 [35136/225000 (16%)] Loss: 16831.541016\n",
      "Train Epoch: 417 [37632/225000 (17%)] Loss: 16596.207031\n",
      "Train Epoch: 417 [40128/225000 (18%)] Loss: 16554.300781\n",
      "Train Epoch: 417 [42624/225000 (19%)] Loss: 16327.066406\n",
      "Train Epoch: 417 [45120/225000 (20%)] Loss: 16535.591797\n",
      "Train Epoch: 417 [47616/225000 (21%)] Loss: 16600.982422\n",
      "Train Epoch: 417 [50112/225000 (22%)] Loss: 16530.345703\n",
      "Train Epoch: 417 [52608/225000 (23%)] Loss: 16452.796875\n",
      "Train Epoch: 417 [55104/225000 (24%)] Loss: 16367.463867\n",
      "Train Epoch: 417 [57600/225000 (26%)] Loss: 16136.215820\n",
      "Train Epoch: 417 [60096/225000 (27%)] Loss: 16145.901367\n",
      "Train Epoch: 417 [62592/225000 (28%)] Loss: 16068.903320\n",
      "Train Epoch: 417 [65088/225000 (29%)] Loss: 16975.636719\n",
      "Train Epoch: 417 [67584/225000 (30%)] Loss: 16380.216797\n",
      "Train Epoch: 417 [70080/225000 (31%)] Loss: 16961.316406\n",
      "Train Epoch: 417 [72576/225000 (32%)] Loss: 16468.625000\n",
      "Train Epoch: 417 [75072/225000 (33%)] Loss: 18225.630859\n",
      "Train Epoch: 417 [77568/225000 (34%)] Loss: 16371.550781\n",
      "Train Epoch: 417 [80064/225000 (36%)] Loss: 16176.320312\n",
      "Train Epoch: 417 [82560/225000 (37%)] Loss: 17766.214844\n",
      "Train Epoch: 417 [85056/225000 (38%)] Loss: 16194.580078\n",
      "Train Epoch: 417 [87552/225000 (39%)] Loss: 16786.468750\n",
      "Train Epoch: 417 [90048/225000 (40%)] Loss: 16414.937500\n",
      "Train Epoch: 417 [92544/225000 (41%)] Loss: 16562.617188\n",
      "Train Epoch: 417 [95040/225000 (42%)] Loss: 16141.899414\n",
      "Train Epoch: 417 [97536/225000 (43%)] Loss: 17040.781250\n",
      "Train Epoch: 417 [100032/225000 (44%)] Loss: 16709.937500\n",
      "Train Epoch: 417 [102528/225000 (46%)] Loss: 16440.539062\n",
      "Train Epoch: 417 [105024/225000 (47%)] Loss: 16494.746094\n",
      "Train Epoch: 417 [107520/225000 (48%)] Loss: 16702.910156\n",
      "Train Epoch: 417 [110016/225000 (49%)] Loss: 16280.035156\n",
      "Train Epoch: 417 [112512/225000 (50%)] Loss: 16345.746094\n",
      "Train Epoch: 417 [115008/225000 (51%)] Loss: 16430.677734\n",
      "Train Epoch: 417 [117504/225000 (52%)] Loss: 16529.087891\n",
      "Train Epoch: 417 [120000/225000 (53%)] Loss: 16372.035156\n",
      "Train Epoch: 417 [122496/225000 (54%)] Loss: 16672.888672\n",
      "Train Epoch: 417 [124992/225000 (56%)] Loss: 16498.953125\n",
      "Train Epoch: 417 [127488/225000 (57%)] Loss: 17700.839844\n",
      "Train Epoch: 417 [129984/225000 (58%)] Loss: 19248.689453\n",
      "Train Epoch: 417 [132480/225000 (59%)] Loss: 16128.652344\n",
      "Train Epoch: 417 [134976/225000 (60%)] Loss: 16962.150391\n",
      "Train Epoch: 417 [137472/225000 (61%)] Loss: 16626.335938\n",
      "Train Epoch: 417 [139968/225000 (62%)] Loss: 16587.179688\n",
      "Train Epoch: 417 [142464/225000 (63%)] Loss: 16248.950195\n",
      "Train Epoch: 417 [144960/225000 (64%)] Loss: 16447.777344\n",
      "Train Epoch: 417 [147456/225000 (66%)] Loss: 16159.072266\n",
      "Train Epoch: 417 [149952/225000 (67%)] Loss: 16850.181641\n",
      "Train Epoch: 417 [152448/225000 (68%)] Loss: 16612.343750\n",
      "Train Epoch: 417 [154944/225000 (69%)] Loss: 16331.129883\n",
      "Train Epoch: 417 [157440/225000 (70%)] Loss: 16716.060547\n",
      "Train Epoch: 417 [159936/225000 (71%)] Loss: 16527.814453\n",
      "Train Epoch: 417 [162432/225000 (72%)] Loss: 16913.589844\n",
      "Train Epoch: 417 [164928/225000 (73%)] Loss: 16805.113281\n",
      "Train Epoch: 417 [167424/225000 (74%)] Loss: 16674.382812\n",
      "Train Epoch: 417 [169920/225000 (76%)] Loss: 16723.937500\n",
      "Train Epoch: 417 [172416/225000 (77%)] Loss: 16653.080078\n",
      "Train Epoch: 417 [174912/225000 (78%)] Loss: 16623.167969\n",
      "Train Epoch: 417 [177408/225000 (79%)] Loss: 16668.472656\n",
      "Train Epoch: 417 [179904/225000 (80%)] Loss: 16434.363281\n",
      "Train Epoch: 417 [182400/225000 (81%)] Loss: 16758.841797\n",
      "Train Epoch: 417 [184896/225000 (82%)] Loss: 16958.369141\n",
      "Train Epoch: 417 [187392/225000 (83%)] Loss: 16848.386719\n",
      "Train Epoch: 417 [189888/225000 (84%)] Loss: 16470.496094\n",
      "Train Epoch: 417 [192384/225000 (86%)] Loss: 16431.652344\n",
      "Train Epoch: 417 [194880/225000 (87%)] Loss: 16468.867188\n",
      "Train Epoch: 417 [197376/225000 (88%)] Loss: 16988.894531\n",
      "Train Epoch: 417 [199872/225000 (89%)] Loss: 16642.542969\n",
      "Train Epoch: 417 [202368/225000 (90%)] Loss: 16517.025391\n",
      "Train Epoch: 417 [204864/225000 (91%)] Loss: 16192.040039\n",
      "Train Epoch: 417 [207360/225000 (92%)] Loss: 16227.477539\n",
      "Train Epoch: 417 [209856/225000 (93%)] Loss: 16646.773438\n",
      "Train Epoch: 417 [212352/225000 (94%)] Loss: 16027.652344\n",
      "Train Epoch: 417 [214848/225000 (95%)] Loss: 16970.792969\n",
      "Train Epoch: 417 [217344/225000 (97%)] Loss: 16398.304688\n",
      "Train Epoch: 417 [219840/225000 (98%)] Loss: 16852.365234\n",
      "Train Epoch: 417 [222336/225000 (99%)] Loss: 16827.781250\n",
      "Train Epoch: 417 [224832/225000 (100%)] Loss: 16433.619141\n",
      "    epoch          : 417\n",
      "    loss           : 16709.970148184195\n",
      "    val_loss       : 16612.08006825611\n",
      "Train Epoch: 418 [192/225000 (0%)] Loss: 16727.572266\n",
      "Train Epoch: 418 [2688/225000 (1%)] Loss: 16469.113281\n",
      "Train Epoch: 418 [5184/225000 (2%)] Loss: 17005.267578\n",
      "Train Epoch: 418 [7680/225000 (3%)] Loss: 16422.007812\n",
      "Train Epoch: 418 [10176/225000 (5%)] Loss: 18006.675781\n",
      "Train Epoch: 418 [12672/225000 (6%)] Loss: 16392.523438\n",
      "Train Epoch: 418 [15168/225000 (7%)] Loss: 16304.954102\n",
      "Train Epoch: 418 [17664/225000 (8%)] Loss: 16778.332031\n",
      "Train Epoch: 418 [20160/225000 (9%)] Loss: 16562.976562\n",
      "Train Epoch: 418 [22656/225000 (10%)] Loss: 16997.437500\n",
      "Train Epoch: 418 [25152/225000 (11%)] Loss: 16387.482422\n",
      "Train Epoch: 418 [27648/225000 (12%)] Loss: 16469.816406\n",
      "Train Epoch: 418 [30144/225000 (13%)] Loss: 16389.224609\n",
      "Train Epoch: 418 [32640/225000 (15%)] Loss: 16944.011719\n",
      "Train Epoch: 418 [35136/225000 (16%)] Loss: 16417.906250\n",
      "Train Epoch: 418 [37632/225000 (17%)] Loss: 16851.539062\n",
      "Train Epoch: 418 [40128/225000 (18%)] Loss: 17082.015625\n",
      "Train Epoch: 418 [42624/225000 (19%)] Loss: 16583.173828\n",
      "Train Epoch: 418 [45120/225000 (20%)] Loss: 16721.152344\n",
      "Train Epoch: 418 [47616/225000 (21%)] Loss: 16689.156250\n",
      "Train Epoch: 418 [50112/225000 (22%)] Loss: 16515.318359\n",
      "Train Epoch: 418 [52608/225000 (23%)] Loss: 16401.777344\n",
      "Train Epoch: 418 [55104/225000 (24%)] Loss: 16865.390625\n",
      "Train Epoch: 418 [57600/225000 (26%)] Loss: 17036.523438\n",
      "Train Epoch: 418 [60096/225000 (27%)] Loss: 16528.380859\n",
      "Train Epoch: 418 [62592/225000 (28%)] Loss: 16897.591797\n",
      "Train Epoch: 418 [65088/225000 (29%)] Loss: 16510.519531\n",
      "Train Epoch: 418 [67584/225000 (30%)] Loss: 15940.658203\n",
      "Train Epoch: 418 [70080/225000 (31%)] Loss: 16637.642578\n",
      "Train Epoch: 418 [72576/225000 (32%)] Loss: 16318.542969\n",
      "Train Epoch: 418 [75072/225000 (33%)] Loss: 16521.593750\n",
      "Train Epoch: 418 [77568/225000 (34%)] Loss: 16449.050781\n",
      "Train Epoch: 418 [80064/225000 (36%)] Loss: 16830.587891\n",
      "Train Epoch: 418 [82560/225000 (37%)] Loss: 16504.546875\n",
      "Train Epoch: 418 [85056/225000 (38%)] Loss: 16414.351562\n",
      "Train Epoch: 418 [87552/225000 (39%)] Loss: 16410.541016\n",
      "Train Epoch: 418 [90048/225000 (40%)] Loss: 16529.074219\n",
      "Train Epoch: 418 [92544/225000 (41%)] Loss: 18776.230469\n",
      "Train Epoch: 418 [95040/225000 (42%)] Loss: 16510.621094\n",
      "Train Epoch: 418 [97536/225000 (43%)] Loss: 16778.253906\n",
      "Train Epoch: 418 [100032/225000 (44%)] Loss: 17206.914062\n",
      "Train Epoch: 418 [102528/225000 (46%)] Loss: 16460.994141\n",
      "Train Epoch: 418 [105024/225000 (47%)] Loss: 17812.107422\n",
      "Train Epoch: 418 [107520/225000 (48%)] Loss: 16227.480469\n",
      "Train Epoch: 418 [110016/225000 (49%)] Loss: 16762.519531\n",
      "Train Epoch: 418 [112512/225000 (50%)] Loss: 16619.507812\n",
      "Train Epoch: 418 [115008/225000 (51%)] Loss: 16906.712891\n",
      "Train Epoch: 418 [117504/225000 (52%)] Loss: 16761.867188\n",
      "Train Epoch: 418 [120000/225000 (53%)] Loss: 16316.723633\n",
      "Train Epoch: 418 [122496/225000 (54%)] Loss: 16542.621094\n",
      "Train Epoch: 418 [124992/225000 (56%)] Loss: 16168.276367\n",
      "Train Epoch: 418 [127488/225000 (57%)] Loss: 18885.460938\n",
      "Train Epoch: 418 [129984/225000 (58%)] Loss: 16864.335938\n",
      "Train Epoch: 418 [132480/225000 (59%)] Loss: 16954.000000\n",
      "Train Epoch: 418 [134976/225000 (60%)] Loss: 16715.271484\n",
      "Train Epoch: 418 [137472/225000 (61%)] Loss: 16495.730469\n",
      "Train Epoch: 418 [139968/225000 (62%)] Loss: 16696.765625\n",
      "Train Epoch: 418 [142464/225000 (63%)] Loss: 16602.796875\n",
      "Train Epoch: 418 [144960/225000 (64%)] Loss: 16621.601562\n",
      "Train Epoch: 418 [147456/225000 (66%)] Loss: 16496.355469\n",
      "Train Epoch: 418 [149952/225000 (67%)] Loss: 16197.131836\n",
      "Train Epoch: 418 [152448/225000 (68%)] Loss: 16449.302734\n",
      "Train Epoch: 418 [154944/225000 (69%)] Loss: 16782.863281\n",
      "Train Epoch: 418 [157440/225000 (70%)] Loss: 16510.246094\n",
      "Train Epoch: 418 [159936/225000 (71%)] Loss: 16840.478516\n",
      "Train Epoch: 418 [162432/225000 (72%)] Loss: 16254.683594\n",
      "Train Epoch: 418 [164928/225000 (73%)] Loss: 16739.166016\n",
      "Train Epoch: 418 [167424/225000 (74%)] Loss: 16323.930664\n",
      "Train Epoch: 418 [169920/225000 (76%)] Loss: 16201.299805\n",
      "Train Epoch: 418 [172416/225000 (77%)] Loss: 16450.033203\n",
      "Train Epoch: 418 [174912/225000 (78%)] Loss: 16591.263672\n",
      "Train Epoch: 418 [177408/225000 (79%)] Loss: 17193.882812\n",
      "Train Epoch: 418 [179904/225000 (80%)] Loss: 16555.894531\n",
      "Train Epoch: 418 [182400/225000 (81%)] Loss: 16711.781250\n",
      "Train Epoch: 418 [184896/225000 (82%)] Loss: 16953.894531\n",
      "Train Epoch: 418 [187392/225000 (83%)] Loss: 18306.804688\n",
      "Train Epoch: 418 [189888/225000 (84%)] Loss: 18390.376953\n",
      "Train Epoch: 418 [192384/225000 (86%)] Loss: 16474.210938\n",
      "Train Epoch: 418 [194880/225000 (87%)] Loss: 16610.609375\n",
      "Train Epoch: 418 [197376/225000 (88%)] Loss: 16807.371094\n",
      "Train Epoch: 418 [199872/225000 (89%)] Loss: 16971.500000\n",
      "Train Epoch: 418 [202368/225000 (90%)] Loss: 16722.259766\n",
      "Train Epoch: 418 [204864/225000 (91%)] Loss: 16512.703125\n",
      "Train Epoch: 418 [207360/225000 (92%)] Loss: 16431.361328\n",
      "Train Epoch: 418 [209856/225000 (93%)] Loss: 16531.582031\n",
      "Train Epoch: 418 [212352/225000 (94%)] Loss: 16771.648438\n",
      "Train Epoch: 418 [214848/225000 (95%)] Loss: 16478.921875\n",
      "Train Epoch: 418 [217344/225000 (97%)] Loss: 17940.875000\n",
      "Train Epoch: 418 [219840/225000 (98%)] Loss: 18560.679688\n",
      "Train Epoch: 418 [222336/225000 (99%)] Loss: 16284.544922\n",
      "Train Epoch: 418 [224832/225000 (100%)] Loss: 16627.347656\n",
      "    epoch          : 418\n",
      "    loss           : 16708.36013241921\n",
      "    val_loss       : 16608.884874420313\n",
      "Train Epoch: 419 [192/225000 (0%)] Loss: 16641.437500\n",
      "Train Epoch: 419 [2688/225000 (1%)] Loss: 16702.871094\n",
      "Train Epoch: 419 [5184/225000 (2%)] Loss: 16550.324219\n",
      "Train Epoch: 419 [7680/225000 (3%)] Loss: 16811.410156\n",
      "Train Epoch: 419 [10176/225000 (5%)] Loss: 17001.287109\n",
      "Train Epoch: 419 [12672/225000 (6%)] Loss: 17021.404297\n",
      "Train Epoch: 419 [15168/225000 (7%)] Loss: 16708.488281\n",
      "Train Epoch: 419 [17664/225000 (8%)] Loss: 16655.257812\n",
      "Train Epoch: 419 [20160/225000 (9%)] Loss: 16609.914062\n",
      "Train Epoch: 419 [22656/225000 (10%)] Loss: 16395.513672\n",
      "Train Epoch: 419 [25152/225000 (11%)] Loss: 16679.742188\n",
      "Train Epoch: 419 [27648/225000 (12%)] Loss: 16333.930664\n",
      "Train Epoch: 419 [30144/225000 (13%)] Loss: 17016.830078\n",
      "Train Epoch: 419 [32640/225000 (15%)] Loss: 16441.218750\n",
      "Train Epoch: 419 [35136/225000 (16%)] Loss: 17039.535156\n",
      "Train Epoch: 419 [37632/225000 (17%)] Loss: 16358.524414\n",
      "Train Epoch: 419 [40128/225000 (18%)] Loss: 16522.363281\n",
      "Train Epoch: 419 [42624/225000 (19%)] Loss: 16693.382812\n",
      "Train Epoch: 419 [45120/225000 (20%)] Loss: 16536.578125\n",
      "Train Epoch: 419 [47616/225000 (21%)] Loss: 17154.890625\n",
      "Train Epoch: 419 [50112/225000 (22%)] Loss: 16814.222656\n",
      "Train Epoch: 419 [52608/225000 (23%)] Loss: 16980.792969\n",
      "Train Epoch: 419 [55104/225000 (24%)] Loss: 17336.988281\n",
      "Train Epoch: 419 [57600/225000 (26%)] Loss: 17030.566406\n",
      "Train Epoch: 419 [60096/225000 (27%)] Loss: 16527.585938\n",
      "Train Epoch: 419 [62592/225000 (28%)] Loss: 16834.855469\n",
      "Train Epoch: 419 [65088/225000 (29%)] Loss: 16483.931641\n",
      "Train Epoch: 419 [67584/225000 (30%)] Loss: 17030.515625\n",
      "Train Epoch: 419 [70080/225000 (31%)] Loss: 16757.318359\n",
      "Train Epoch: 419 [72576/225000 (32%)] Loss: 16199.382812\n",
      "Train Epoch: 419 [75072/225000 (33%)] Loss: 16542.638672\n",
      "Train Epoch: 419 [77568/225000 (34%)] Loss: 16712.386719\n",
      "Train Epoch: 419 [80064/225000 (36%)] Loss: 16593.828125\n",
      "Train Epoch: 419 [82560/225000 (37%)] Loss: 16806.279297\n",
      "Train Epoch: 419 [85056/225000 (38%)] Loss: 16524.500000\n",
      "Train Epoch: 419 [87552/225000 (39%)] Loss: 16255.904297\n",
      "Train Epoch: 419 [90048/225000 (40%)] Loss: 16879.894531\n",
      "Train Epoch: 419 [92544/225000 (41%)] Loss: 16413.255859\n",
      "Train Epoch: 419 [95040/225000 (42%)] Loss: 16748.277344\n",
      "Train Epoch: 419 [97536/225000 (43%)] Loss: 16603.695312\n",
      "Train Epoch: 419 [100032/225000 (44%)] Loss: 17102.871094\n",
      "Train Epoch: 419 [102528/225000 (46%)] Loss: 16656.906250\n",
      "Train Epoch: 419 [105024/225000 (47%)] Loss: 16759.714844\n",
      "Train Epoch: 419 [107520/225000 (48%)] Loss: 16565.781250\n",
      "Train Epoch: 419 [110016/225000 (49%)] Loss: 16734.662109\n",
      "Train Epoch: 419 [112512/225000 (50%)] Loss: 16355.591797\n",
      "Train Epoch: 419 [115008/225000 (51%)] Loss: 18206.804688\n",
      "Train Epoch: 419 [117504/225000 (52%)] Loss: 16702.988281\n",
      "Train Epoch: 419 [120000/225000 (53%)] Loss: 16700.902344\n",
      "Train Epoch: 419 [122496/225000 (54%)] Loss: 16733.445312\n",
      "Train Epoch: 419 [124992/225000 (56%)] Loss: 16097.321289\n",
      "Train Epoch: 419 [127488/225000 (57%)] Loss: 16669.357422\n",
      "Train Epoch: 419 [129984/225000 (58%)] Loss: 16618.564453\n",
      "Train Epoch: 419 [132480/225000 (59%)] Loss: 16738.210938\n",
      "Train Epoch: 419 [134976/225000 (60%)] Loss: 16420.457031\n",
      "Train Epoch: 419 [137472/225000 (61%)] Loss: 16605.775391\n",
      "Train Epoch: 419 [139968/225000 (62%)] Loss: 17154.361328\n",
      "Train Epoch: 419 [142464/225000 (63%)] Loss: 16653.083984\n",
      "Train Epoch: 419 [144960/225000 (64%)] Loss: 16651.375000\n",
      "Train Epoch: 419 [147456/225000 (66%)] Loss: 16624.031250\n",
      "Train Epoch: 419 [149952/225000 (67%)] Loss: 16607.568359\n",
      "Train Epoch: 419 [152448/225000 (68%)] Loss: 16456.058594\n",
      "Train Epoch: 419 [154944/225000 (69%)] Loss: 16590.835938\n",
      "Train Epoch: 419 [157440/225000 (70%)] Loss: 17019.433594\n",
      "Train Epoch: 419 [159936/225000 (71%)] Loss: 16547.367188\n",
      "Train Epoch: 419 [162432/225000 (72%)] Loss: 17922.871094\n",
      "Train Epoch: 419 [164928/225000 (73%)] Loss: 16661.921875\n",
      "Train Epoch: 419 [167424/225000 (74%)] Loss: 16782.156250\n",
      "Train Epoch: 419 [169920/225000 (76%)] Loss: 16708.761719\n",
      "Train Epoch: 419 [172416/225000 (77%)] Loss: 16705.140625\n",
      "Train Epoch: 419 [174912/225000 (78%)] Loss: 17023.775391\n",
      "Train Epoch: 419 [177408/225000 (79%)] Loss: 16524.511719\n",
      "Train Epoch: 419 [179904/225000 (80%)] Loss: 16879.457031\n",
      "Train Epoch: 419 [182400/225000 (81%)] Loss: 16843.898438\n",
      "Train Epoch: 419 [184896/225000 (82%)] Loss: 16346.401367\n",
      "Train Epoch: 419 [187392/225000 (83%)] Loss: 16692.804688\n",
      "Train Epoch: 419 [189888/225000 (84%)] Loss: 16138.807617\n",
      "Train Epoch: 419 [192384/225000 (86%)] Loss: 16291.208008\n",
      "Train Epoch: 419 [194880/225000 (87%)] Loss: 16440.533203\n",
      "Train Epoch: 419 [197376/225000 (88%)] Loss: 16813.042969\n",
      "Train Epoch: 419 [199872/225000 (89%)] Loss: 16434.626953\n",
      "Train Epoch: 419 [202368/225000 (90%)] Loss: 16855.951172\n",
      "Train Epoch: 419 [204864/225000 (91%)] Loss: 16690.621094\n",
      "Train Epoch: 419 [207360/225000 (92%)] Loss: 16471.275391\n",
      "Train Epoch: 419 [209856/225000 (93%)] Loss: 16493.603516\n",
      "Train Epoch: 419 [212352/225000 (94%)] Loss: 16472.824219\n",
      "Train Epoch: 419 [214848/225000 (95%)] Loss: 16410.164062\n",
      "Train Epoch: 419 [217344/225000 (97%)] Loss: 16668.613281\n",
      "Train Epoch: 419 [219840/225000 (98%)] Loss: 16545.410156\n",
      "Train Epoch: 419 [222336/225000 (99%)] Loss: 18241.644531\n",
      "Train Epoch: 419 [224832/225000 (100%)] Loss: 16813.574219\n",
      "    epoch          : 419\n",
      "    loss           : 16701.590410356228\n",
      "    val_loss       : 16709.900038913915\n",
      "Train Epoch: 420 [192/225000 (0%)] Loss: 18532.279297\n",
      "Train Epoch: 420 [2688/225000 (1%)] Loss: 17089.160156\n",
      "Train Epoch: 420 [5184/225000 (2%)] Loss: 16776.929688\n",
      "Train Epoch: 420 [7680/225000 (3%)] Loss: 16875.664062\n",
      "Train Epoch: 420 [10176/225000 (5%)] Loss: 16534.183594\n",
      "Train Epoch: 420 [12672/225000 (6%)] Loss: 16444.042969\n",
      "Train Epoch: 420 [15168/225000 (7%)] Loss: 16566.240234\n",
      "Train Epoch: 420 [17664/225000 (8%)] Loss: 16469.201172\n",
      "Train Epoch: 420 [20160/225000 (9%)] Loss: 17377.736328\n",
      "Train Epoch: 420 [22656/225000 (10%)] Loss: 16594.312500\n",
      "Train Epoch: 420 [25152/225000 (11%)] Loss: 16437.191406\n",
      "Train Epoch: 420 [27648/225000 (12%)] Loss: 16192.112305\n",
      "Train Epoch: 420 [30144/225000 (13%)] Loss: 16352.677734\n",
      "Train Epoch: 420 [32640/225000 (15%)] Loss: 16796.687500\n",
      "Train Epoch: 420 [35136/225000 (16%)] Loss: 16297.405273\n",
      "Train Epoch: 420 [37632/225000 (17%)] Loss: 16642.962891\n",
      "Train Epoch: 420 [40128/225000 (18%)] Loss: 16544.257812\n",
      "Train Epoch: 420 [42624/225000 (19%)] Loss: 16839.693359\n",
      "Train Epoch: 420 [45120/225000 (20%)] Loss: 16701.652344\n",
      "Train Epoch: 420 [47616/225000 (21%)] Loss: 16385.644531\n",
      "Train Epoch: 420 [50112/225000 (22%)] Loss: 17026.972656\n",
      "Train Epoch: 420 [52608/225000 (23%)] Loss: 16514.746094\n",
      "Train Epoch: 420 [55104/225000 (24%)] Loss: 17365.894531\n",
      "Train Epoch: 420 [57600/225000 (26%)] Loss: 16389.998047\n",
      "Train Epoch: 420 [60096/225000 (27%)] Loss: 16257.964844\n",
      "Train Epoch: 420 [62592/225000 (28%)] Loss: 16450.880859\n",
      "Train Epoch: 420 [65088/225000 (29%)] Loss: 16963.929688\n",
      "Train Epoch: 420 [67584/225000 (30%)] Loss: 16319.947266\n",
      "Train Epoch: 420 [70080/225000 (31%)] Loss: 16877.050781\n",
      "Train Epoch: 420 [72576/225000 (32%)] Loss: 16612.992188\n",
      "Train Epoch: 420 [75072/225000 (33%)] Loss: 16336.678711\n",
      "Train Epoch: 420 [77568/225000 (34%)] Loss: 16865.746094\n",
      "Train Epoch: 420 [80064/225000 (36%)] Loss: 16912.097656\n",
      "Train Epoch: 420 [82560/225000 (37%)] Loss: 18245.800781\n",
      "Train Epoch: 420 [85056/225000 (38%)] Loss: 18410.441406\n",
      "Train Epoch: 420 [87552/225000 (39%)] Loss: 16676.093750\n",
      "Train Epoch: 420 [90048/225000 (40%)] Loss: 16352.234375\n",
      "Train Epoch: 420 [92544/225000 (41%)] Loss: 16740.984375\n",
      "Train Epoch: 420 [95040/225000 (42%)] Loss: 16253.145508\n",
      "Train Epoch: 420 [97536/225000 (43%)] Loss: 16712.197266\n",
      "Train Epoch: 420 [100032/225000 (44%)] Loss: 16642.957031\n",
      "Train Epoch: 420 [102528/225000 (46%)] Loss: 16490.992188\n",
      "Train Epoch: 420 [105024/225000 (47%)] Loss: 17107.964844\n",
      "Train Epoch: 420 [107520/225000 (48%)] Loss: 16633.650391\n",
      "Train Epoch: 420 [110016/225000 (49%)] Loss: 16616.156250\n",
      "Train Epoch: 420 [112512/225000 (50%)] Loss: 16787.554688\n",
      "Train Epoch: 420 [115008/225000 (51%)] Loss: 16545.433594\n",
      "Train Epoch: 420 [117504/225000 (52%)] Loss: 16646.099609\n",
      "Train Epoch: 420 [120000/225000 (53%)] Loss: 16413.304688\n",
      "Train Epoch: 420 [122496/225000 (54%)] Loss: 16415.292969\n",
      "Train Epoch: 420 [124992/225000 (56%)] Loss: 16481.937500\n",
      "Train Epoch: 420 [127488/225000 (57%)] Loss: 16437.523438\n",
      "Train Epoch: 420 [129984/225000 (58%)] Loss: 16559.050781\n",
      "Train Epoch: 420 [132480/225000 (59%)] Loss: 16335.353516\n",
      "Train Epoch: 420 [134976/225000 (60%)] Loss: 17082.867188\n",
      "Train Epoch: 420 [137472/225000 (61%)] Loss: 16988.304688\n",
      "Train Epoch: 420 [139968/225000 (62%)] Loss: 16692.023438\n",
      "Train Epoch: 420 [142464/225000 (63%)] Loss: 16944.369141\n",
      "Train Epoch: 420 [144960/225000 (64%)] Loss: 16701.707031\n",
      "Train Epoch: 420 [147456/225000 (66%)] Loss: 16767.576172\n",
      "Train Epoch: 420 [149952/225000 (67%)] Loss: 16599.236328\n",
      "Train Epoch: 420 [152448/225000 (68%)] Loss: 16737.457031\n",
      "Train Epoch: 420 [154944/225000 (69%)] Loss: 16953.650391\n",
      "Train Epoch: 420 [157440/225000 (70%)] Loss: 16372.380859\n",
      "Train Epoch: 420 [159936/225000 (71%)] Loss: 16887.515625\n",
      "Train Epoch: 420 [162432/225000 (72%)] Loss: 16244.264648\n",
      "Train Epoch: 420 [164928/225000 (73%)] Loss: 16414.718750\n",
      "Train Epoch: 420 [167424/225000 (74%)] Loss: 18050.085938\n",
      "Train Epoch: 420 [169920/225000 (76%)] Loss: 16883.796875\n",
      "Train Epoch: 420 [172416/225000 (77%)] Loss: 16870.130859\n",
      "Train Epoch: 420 [174912/225000 (78%)] Loss: 16802.568359\n",
      "Train Epoch: 420 [177408/225000 (79%)] Loss: 17254.746094\n",
      "Train Epoch: 420 [179904/225000 (80%)] Loss: 16942.390625\n",
      "Train Epoch: 420 [182400/225000 (81%)] Loss: 17047.787109\n",
      "Train Epoch: 420 [184896/225000 (82%)] Loss: 16708.914062\n",
      "Train Epoch: 420 [187392/225000 (83%)] Loss: 16419.570312\n",
      "Train Epoch: 420 [189888/225000 (84%)] Loss: 16201.183594\n",
      "Train Epoch: 420 [192384/225000 (86%)] Loss: 16441.646484\n",
      "Train Epoch: 420 [194880/225000 (87%)] Loss: 16819.314453\n",
      "Train Epoch: 420 [197376/225000 (88%)] Loss: 16538.660156\n",
      "Train Epoch: 420 [199872/225000 (89%)] Loss: 16682.933594\n",
      "Train Epoch: 420 [202368/225000 (90%)] Loss: 16300.764648\n",
      "Train Epoch: 420 [204864/225000 (91%)] Loss: 16193.944336\n",
      "Train Epoch: 420 [207360/225000 (92%)] Loss: 16742.121094\n",
      "Train Epoch: 420 [209856/225000 (93%)] Loss: 16606.066406\n",
      "Train Epoch: 420 [212352/225000 (94%)] Loss: 16601.542969\n",
      "Train Epoch: 420 [214848/225000 (95%)] Loss: 16791.849609\n",
      "Train Epoch: 420 [217344/225000 (97%)] Loss: 16863.386719\n",
      "Train Epoch: 420 [219840/225000 (98%)] Loss: 16507.136719\n",
      "Train Epoch: 420 [222336/225000 (99%)] Loss: 16666.781250\n",
      "Train Epoch: 420 [224832/225000 (100%)] Loss: 16778.613281\n",
      "    epoch          : 420\n",
      "    loss           : 16693.43963477229\n",
      "    val_loss       : 16628.18679902026\n",
      "Train Epoch: 421 [192/225000 (0%)] Loss: 17046.503906\n",
      "Train Epoch: 421 [2688/225000 (1%)] Loss: 16008.170898\n",
      "Train Epoch: 421 [5184/225000 (2%)] Loss: 16438.744141\n",
      "Train Epoch: 421 [7680/225000 (3%)] Loss: 15723.493164\n",
      "Train Epoch: 421 [10176/225000 (5%)] Loss: 16758.271484\n",
      "Train Epoch: 421 [12672/225000 (6%)] Loss: 16596.310547\n",
      "Train Epoch: 421 [15168/225000 (7%)] Loss: 16864.593750\n",
      "Train Epoch: 421 [17664/225000 (8%)] Loss: 16583.183594\n",
      "Train Epoch: 421 [20160/225000 (9%)] Loss: 16625.847656\n",
      "Train Epoch: 421 [22656/225000 (10%)] Loss: 16179.350586\n",
      "Train Epoch: 421 [25152/225000 (11%)] Loss: 16856.878906\n",
      "Train Epoch: 421 [27648/225000 (12%)] Loss: 16716.210938\n",
      "Train Epoch: 421 [30144/225000 (13%)] Loss: 16753.597656\n",
      "Train Epoch: 421 [32640/225000 (15%)] Loss: 17358.851562\n",
      "Train Epoch: 421 [35136/225000 (16%)] Loss: 16391.144531\n",
      "Train Epoch: 421 [37632/225000 (17%)] Loss: 17027.742188\n",
      "Train Epoch: 421 [40128/225000 (18%)] Loss: 17093.156250\n",
      "Train Epoch: 421 [42624/225000 (19%)] Loss: 16443.816406\n",
      "Train Epoch: 421 [45120/225000 (20%)] Loss: 16934.308594\n",
      "Train Epoch: 421 [47616/225000 (21%)] Loss: 16204.797852\n",
      "Train Epoch: 421 [50112/225000 (22%)] Loss: 16817.265625\n",
      "Train Epoch: 421 [52608/225000 (23%)] Loss: 16361.660156\n",
      "Train Epoch: 421 [55104/225000 (24%)] Loss: 17008.761719\n",
      "Train Epoch: 421 [57600/225000 (26%)] Loss: 16216.120117\n",
      "Train Epoch: 421 [60096/225000 (27%)] Loss: 16746.031250\n",
      "Train Epoch: 421 [62592/225000 (28%)] Loss: 16304.217773\n",
      "Train Epoch: 421 [65088/225000 (29%)] Loss: 16402.742188\n",
      "Train Epoch: 421 [67584/225000 (30%)] Loss: 16460.664062\n",
      "Train Epoch: 421 [70080/225000 (31%)] Loss: 16417.789062\n",
      "Train Epoch: 421 [72576/225000 (32%)] Loss: 16224.900391\n",
      "Train Epoch: 421 [75072/225000 (33%)] Loss: 16488.927734\n",
      "Train Epoch: 421 [77568/225000 (34%)] Loss: 16763.125000\n",
      "Train Epoch: 421 [80064/225000 (36%)] Loss: 16647.808594\n",
      "Train Epoch: 421 [82560/225000 (37%)] Loss: 16166.961914\n",
      "Train Epoch: 421 [85056/225000 (38%)] Loss: 16571.509766\n",
      "Train Epoch: 421 [87552/225000 (39%)] Loss: 16702.925781\n",
      "Train Epoch: 421 [90048/225000 (40%)] Loss: 16293.158203\n",
      "Train Epoch: 421 [92544/225000 (41%)] Loss: 16601.587891\n",
      "Train Epoch: 421 [95040/225000 (42%)] Loss: 16534.097656\n",
      "Train Epoch: 421 [97536/225000 (43%)] Loss: 16502.490234\n",
      "Train Epoch: 421 [100032/225000 (44%)] Loss: 16984.699219\n",
      "Train Epoch: 421 [102528/225000 (46%)] Loss: 16512.929688\n",
      "Train Epoch: 421 [105024/225000 (47%)] Loss: 16391.902344\n",
      "Train Epoch: 421 [107520/225000 (48%)] Loss: 16440.632812\n",
      "Train Epoch: 421 [110016/225000 (49%)] Loss: 16649.058594\n",
      "Train Epoch: 421 [112512/225000 (50%)] Loss: 16522.605469\n",
      "Train Epoch: 421 [115008/225000 (51%)] Loss: 16834.019531\n",
      "Train Epoch: 421 [117504/225000 (52%)] Loss: 16841.824219\n",
      "Train Epoch: 421 [120000/225000 (53%)] Loss: 16695.078125\n",
      "Train Epoch: 421 [122496/225000 (54%)] Loss: 16598.820312\n",
      "Train Epoch: 421 [124992/225000 (56%)] Loss: 16639.779297\n",
      "Train Epoch: 421 [127488/225000 (57%)] Loss: 16092.633789\n",
      "Train Epoch: 421 [129984/225000 (58%)] Loss: 16606.896484\n",
      "Train Epoch: 421 [132480/225000 (59%)] Loss: 16813.449219\n",
      "Train Epoch: 421 [134976/225000 (60%)] Loss: 16713.820312\n",
      "Train Epoch: 421 [137472/225000 (61%)] Loss: 16461.332031\n",
      "Train Epoch: 421 [139968/225000 (62%)] Loss: 16561.619141\n",
      "Train Epoch: 421 [142464/225000 (63%)] Loss: 16806.167969\n",
      "Train Epoch: 421 [144960/225000 (64%)] Loss: 16240.733398\n",
      "Train Epoch: 421 [147456/225000 (66%)] Loss: 16168.018555\n",
      "Train Epoch: 421 [149952/225000 (67%)] Loss: 16201.826172\n",
      "Train Epoch: 421 [152448/225000 (68%)] Loss: 16885.541016\n",
      "Train Epoch: 421 [154944/225000 (69%)] Loss: 18360.605469\n",
      "Train Epoch: 421 [157440/225000 (70%)] Loss: 18461.417969\n",
      "Train Epoch: 421 [159936/225000 (71%)] Loss: 16417.248047\n",
      "Train Epoch: 421 [162432/225000 (72%)] Loss: 18814.044922\n",
      "Train Epoch: 421 [164928/225000 (73%)] Loss: 16963.476562\n",
      "Train Epoch: 421 [167424/225000 (74%)] Loss: 17292.611328\n",
      "Train Epoch: 421 [169920/225000 (76%)] Loss: 16420.644531\n",
      "Train Epoch: 421 [172416/225000 (77%)] Loss: 16735.148438\n",
      "Train Epoch: 421 [174912/225000 (78%)] Loss: 16868.275391\n",
      "Train Epoch: 421 [177408/225000 (79%)] Loss: 16875.007812\n",
      "Train Epoch: 421 [179904/225000 (80%)] Loss: 16688.500000\n",
      "Train Epoch: 421 [182400/225000 (81%)] Loss: 16819.773438\n",
      "Train Epoch: 421 [184896/225000 (82%)] Loss: 16755.308594\n",
      "Train Epoch: 421 [187392/225000 (83%)] Loss: 16429.728516\n",
      "Train Epoch: 421 [189888/225000 (84%)] Loss: 16316.254883\n",
      "Train Epoch: 421 [192384/225000 (86%)] Loss: 16526.152344\n",
      "Train Epoch: 421 [194880/225000 (87%)] Loss: 16360.702148\n",
      "Train Epoch: 421 [197376/225000 (88%)] Loss: 16521.939453\n",
      "Train Epoch: 421 [199872/225000 (89%)] Loss: 16623.478516\n",
      "Train Epoch: 421 [202368/225000 (90%)] Loss: 16773.445312\n",
      "Train Epoch: 421 [204864/225000 (91%)] Loss: 17291.023438\n",
      "Train Epoch: 421 [207360/225000 (92%)] Loss: 17135.195312\n",
      "Train Epoch: 421 [209856/225000 (93%)] Loss: 16775.675781\n",
      "Train Epoch: 421 [212352/225000 (94%)] Loss: 16409.074219\n",
      "Train Epoch: 421 [214848/225000 (95%)] Loss: 16782.916016\n",
      "Train Epoch: 421 [217344/225000 (97%)] Loss: 16846.808594\n",
      "Train Epoch: 421 [219840/225000 (98%)] Loss: 16923.078125\n",
      "Train Epoch: 421 [222336/225000 (99%)] Loss: 16591.007812\n",
      "Train Epoch: 421 [224832/225000 (100%)] Loss: 16700.205078\n",
      "    epoch          : 421\n",
      "    loss           : 16700.84184520318\n",
      "    val_loss       : 16622.890088294298\n",
      "Train Epoch: 422 [192/225000 (0%)] Loss: 17041.531250\n",
      "Train Epoch: 422 [2688/225000 (1%)] Loss: 16798.914062\n",
      "Train Epoch: 422 [5184/225000 (2%)] Loss: 16354.728516\n",
      "Train Epoch: 422 [7680/225000 (3%)] Loss: 16909.000000\n",
      "Train Epoch: 422 [10176/225000 (5%)] Loss: 17865.603516\n",
      "Train Epoch: 422 [12672/225000 (6%)] Loss: 16339.111328\n",
      "Train Epoch: 422 [15168/225000 (7%)] Loss: 16669.291016\n",
      "Train Epoch: 422 [17664/225000 (8%)] Loss: 16350.692383\n",
      "Train Epoch: 422 [20160/225000 (9%)] Loss: 16534.166016\n",
      "Train Epoch: 422 [22656/225000 (10%)] Loss: 16552.355469\n",
      "Train Epoch: 422 [25152/225000 (11%)] Loss: 16588.531250\n",
      "Train Epoch: 422 [27648/225000 (12%)] Loss: 16652.812500\n",
      "Train Epoch: 422 [30144/225000 (13%)] Loss: 16719.296875\n",
      "Train Epoch: 422 [32640/225000 (15%)] Loss: 16601.552734\n",
      "Train Epoch: 422 [35136/225000 (16%)] Loss: 16740.402344\n",
      "Train Epoch: 422 [37632/225000 (17%)] Loss: 16558.818359\n",
      "Train Epoch: 422 [40128/225000 (18%)] Loss: 16786.968750\n",
      "Train Epoch: 422 [42624/225000 (19%)] Loss: 16590.953125\n",
      "Train Epoch: 422 [45120/225000 (20%)] Loss: 16637.542969\n",
      "Train Epoch: 422 [47616/225000 (21%)] Loss: 16310.296875\n",
      "Train Epoch: 422 [50112/225000 (22%)] Loss: 16332.399414\n",
      "Train Epoch: 422 [52608/225000 (23%)] Loss: 17282.957031\n",
      "Train Epoch: 422 [55104/225000 (24%)] Loss: 16471.697266\n",
      "Train Epoch: 422 [57600/225000 (26%)] Loss: 16326.905273\n",
      "Train Epoch: 422 [60096/225000 (27%)] Loss: 16492.160156\n",
      "Train Epoch: 422 [62592/225000 (28%)] Loss: 16324.146484\n",
      "Train Epoch: 422 [65088/225000 (29%)] Loss: 16547.589844\n",
      "Train Epoch: 422 [67584/225000 (30%)] Loss: 16194.661133\n",
      "Train Epoch: 422 [70080/225000 (31%)] Loss: 16941.894531\n",
      "Train Epoch: 422 [72576/225000 (32%)] Loss: 16996.128906\n",
      "Train Epoch: 422 [75072/225000 (33%)] Loss: 16894.826172\n",
      "Train Epoch: 422 [77568/225000 (34%)] Loss: 16539.472656\n",
      "Train Epoch: 422 [80064/225000 (36%)] Loss: 16604.828125\n",
      "Train Epoch: 422 [82560/225000 (37%)] Loss: 16410.023438\n",
      "Train Epoch: 422 [85056/225000 (38%)] Loss: 16513.195312\n",
      "Train Epoch: 422 [87552/225000 (39%)] Loss: 16339.570312\n",
      "Train Epoch: 422 [90048/225000 (40%)] Loss: 16116.936523\n",
      "Train Epoch: 422 [92544/225000 (41%)] Loss: 16401.136719\n",
      "Train Epoch: 422 [95040/225000 (42%)] Loss: 16713.349609\n",
      "Train Epoch: 422 [97536/225000 (43%)] Loss: 16289.318359\n",
      "Train Epoch: 422 [100032/225000 (44%)] Loss: 16592.074219\n",
      "Train Epoch: 422 [102528/225000 (46%)] Loss: 16434.714844\n",
      "Train Epoch: 422 [105024/225000 (47%)] Loss: 16407.072266\n",
      "Train Epoch: 422 [107520/225000 (48%)] Loss: 16523.037109\n",
      "Train Epoch: 422 [110016/225000 (49%)] Loss: 16885.472656\n",
      "Train Epoch: 422 [112512/225000 (50%)] Loss: 16582.126953\n",
      "Train Epoch: 422 [115008/225000 (51%)] Loss: 16549.328125\n",
      "Train Epoch: 422 [117504/225000 (52%)] Loss: 17955.246094\n",
      "Train Epoch: 422 [120000/225000 (53%)] Loss: 16733.687500\n",
      "Train Epoch: 422 [122496/225000 (54%)] Loss: 16198.015625\n",
      "Train Epoch: 422 [124992/225000 (56%)] Loss: 15925.149414\n",
      "Train Epoch: 422 [127488/225000 (57%)] Loss: 16293.212891\n",
      "Train Epoch: 422 [129984/225000 (58%)] Loss: 16473.589844\n",
      "Train Epoch: 422 [132480/225000 (59%)] Loss: 17023.416016\n",
      "Train Epoch: 422 [134976/225000 (60%)] Loss: 16656.167969\n",
      "Train Epoch: 422 [137472/225000 (61%)] Loss: 16977.261719\n",
      "Train Epoch: 422 [139968/225000 (62%)] Loss: 16458.970703\n",
      "Train Epoch: 422 [142464/225000 (63%)] Loss: 16771.289062\n",
      "Train Epoch: 422 [144960/225000 (64%)] Loss: 16548.765625\n",
      "Train Epoch: 422 [147456/225000 (66%)] Loss: 16588.019531\n",
      "Train Epoch: 422 [149952/225000 (67%)] Loss: 16502.722656\n",
      "Train Epoch: 422 [152448/225000 (68%)] Loss: 16585.775391\n",
      "Train Epoch: 422 [154944/225000 (69%)] Loss: 17176.789062\n",
      "Train Epoch: 422 [157440/225000 (70%)] Loss: 16347.188477\n",
      "Train Epoch: 422 [159936/225000 (71%)] Loss: 16680.632812\n",
      "Train Epoch: 422 [162432/225000 (72%)] Loss: 16461.402344\n",
      "Train Epoch: 422 [164928/225000 (73%)] Loss: 16876.792969\n",
      "Train Epoch: 422 [167424/225000 (74%)] Loss: 16446.332031\n",
      "Train Epoch: 422 [169920/225000 (76%)] Loss: 16178.473633\n",
      "Train Epoch: 422 [172416/225000 (77%)] Loss: 16409.101562\n",
      "Train Epoch: 422 [174912/225000 (78%)] Loss: 16466.027344\n",
      "Train Epoch: 422 [177408/225000 (79%)] Loss: 17941.480469\n",
      "Train Epoch: 422 [179904/225000 (80%)] Loss: 16375.518555\n",
      "Train Epoch: 422 [182400/225000 (81%)] Loss: 16645.146484\n",
      "Train Epoch: 422 [184896/225000 (82%)] Loss: 16294.855469\n",
      "Train Epoch: 422 [187392/225000 (83%)] Loss: 16708.337891\n",
      "Train Epoch: 422 [189888/225000 (84%)] Loss: 16173.226562\n",
      "Train Epoch: 422 [192384/225000 (86%)] Loss: 16346.394531\n",
      "Train Epoch: 422 [194880/225000 (87%)] Loss: 16978.396484\n",
      "Train Epoch: 422 [197376/225000 (88%)] Loss: 16901.527344\n",
      "Train Epoch: 422 [199872/225000 (89%)] Loss: 16409.316406\n",
      "Train Epoch: 422 [202368/225000 (90%)] Loss: 16574.750000\n",
      "Train Epoch: 422 [204864/225000 (91%)] Loss: 16463.605469\n",
      "Train Epoch: 422 [207360/225000 (92%)] Loss: 16318.546875\n",
      "Train Epoch: 422 [209856/225000 (93%)] Loss: 16738.832031\n",
      "Train Epoch: 422 [212352/225000 (94%)] Loss: 17186.539062\n",
      "Train Epoch: 422 [214848/225000 (95%)] Loss: 16230.496094\n",
      "Train Epoch: 422 [217344/225000 (97%)] Loss: 16602.072266\n",
      "Train Epoch: 422 [219840/225000 (98%)] Loss: 17200.298828\n",
      "Train Epoch: 422 [222336/225000 (99%)] Loss: 16787.300781\n",
      "Train Epoch: 422 [224832/225000 (100%)] Loss: 16706.957031\n",
      "    epoch          : 422\n",
      "    loss           : 16716.439317306154\n",
      "    val_loss       : 16670.602251827262\n",
      "Train Epoch: 423 [192/225000 (0%)] Loss: 16217.973633\n",
      "Train Epoch: 423 [2688/225000 (1%)] Loss: 16540.628906\n",
      "Train Epoch: 423 [5184/225000 (2%)] Loss: 16867.222656\n",
      "Train Epoch: 423 [7680/225000 (3%)] Loss: 16867.312500\n",
      "Train Epoch: 423 [10176/225000 (5%)] Loss: 16236.078125\n",
      "Train Epoch: 423 [12672/225000 (6%)] Loss: 16547.804688\n",
      "Train Epoch: 423 [15168/225000 (7%)] Loss: 16361.301758\n",
      "Train Epoch: 423 [17664/225000 (8%)] Loss: 16625.476562\n",
      "Train Epoch: 423 [20160/225000 (9%)] Loss: 16834.908203\n",
      "Train Epoch: 423 [22656/225000 (10%)] Loss: 18037.000000\n",
      "Train Epoch: 423 [25152/225000 (11%)] Loss: 16541.609375\n",
      "Train Epoch: 423 [27648/225000 (12%)] Loss: 16379.574219\n",
      "Train Epoch: 423 [30144/225000 (13%)] Loss: 16431.820312\n",
      "Train Epoch: 423 [32640/225000 (15%)] Loss: 16425.378906\n",
      "Train Epoch: 423 [35136/225000 (16%)] Loss: 16622.615234\n",
      "Train Epoch: 423 [37632/225000 (17%)] Loss: 16192.877930\n",
      "Train Epoch: 423 [40128/225000 (18%)] Loss: 17988.386719\n",
      "Train Epoch: 423 [42624/225000 (19%)] Loss: 16581.101562\n",
      "Train Epoch: 423 [45120/225000 (20%)] Loss: 16761.093750\n",
      "Train Epoch: 423 [47616/225000 (21%)] Loss: 16259.105469\n",
      "Train Epoch: 423 [50112/225000 (22%)] Loss: 16369.959961\n",
      "Train Epoch: 423 [52608/225000 (23%)] Loss: 16367.592773\n",
      "Train Epoch: 423 [55104/225000 (24%)] Loss: 16593.306641\n",
      "Train Epoch: 423 [57600/225000 (26%)] Loss: 17928.242188\n",
      "Train Epoch: 423 [60096/225000 (27%)] Loss: 16854.976562\n",
      "Train Epoch: 423 [62592/225000 (28%)] Loss: 16804.781250\n",
      "Train Epoch: 423 [65088/225000 (29%)] Loss: 16494.250000\n",
      "Train Epoch: 423 [67584/225000 (30%)] Loss: 16564.064453\n",
      "Train Epoch: 423 [70080/225000 (31%)] Loss: 16738.625000\n",
      "Train Epoch: 423 [72576/225000 (32%)] Loss: 18455.123047\n",
      "Train Epoch: 423 [75072/225000 (33%)] Loss: 16236.542969\n",
      "Train Epoch: 423 [77568/225000 (34%)] Loss: 17482.208984\n",
      "Train Epoch: 423 [80064/225000 (36%)] Loss: 16848.242188\n",
      "Train Epoch: 423 [82560/225000 (37%)] Loss: 17166.406250\n",
      "Train Epoch: 423 [85056/225000 (38%)] Loss: 16670.072266\n",
      "Train Epoch: 423 [87552/225000 (39%)] Loss: 16486.246094\n",
      "Train Epoch: 423 [90048/225000 (40%)] Loss: 16747.734375\n",
      "Train Epoch: 423 [92544/225000 (41%)] Loss: 16587.111328\n",
      "Train Epoch: 423 [95040/225000 (42%)] Loss: 16586.058594\n",
      "Train Epoch: 423 [97536/225000 (43%)] Loss: 16789.232422\n",
      "Train Epoch: 423 [100032/225000 (44%)] Loss: 16520.564453\n",
      "Train Epoch: 423 [102528/225000 (46%)] Loss: 17084.076172\n",
      "Train Epoch: 423 [105024/225000 (47%)] Loss: 16527.644531\n",
      "Train Epoch: 423 [107520/225000 (48%)] Loss: 16772.988281\n",
      "Train Epoch: 423 [110016/225000 (49%)] Loss: 16162.845703\n",
      "Train Epoch: 423 [112512/225000 (50%)] Loss: 16581.687500\n",
      "Train Epoch: 423 [115008/225000 (51%)] Loss: 16443.652344\n",
      "Train Epoch: 423 [117504/225000 (52%)] Loss: 16341.155273\n",
      "Train Epoch: 423 [120000/225000 (53%)] Loss: 16523.718750\n",
      "Train Epoch: 423 [122496/225000 (54%)] Loss: 16698.230469\n",
      "Train Epoch: 423 [124992/225000 (56%)] Loss: 16519.619141\n",
      "Train Epoch: 423 [127488/225000 (57%)] Loss: 16686.746094\n",
      "Train Epoch: 423 [129984/225000 (58%)] Loss: 16431.609375\n",
      "Train Epoch: 423 [132480/225000 (59%)] Loss: 16611.052734\n",
      "Train Epoch: 423 [134976/225000 (60%)] Loss: 16238.946289\n",
      "Train Epoch: 423 [137472/225000 (61%)] Loss: 16574.410156\n",
      "Train Epoch: 423 [139968/225000 (62%)] Loss: 16444.525391\n",
      "Train Epoch: 423 [142464/225000 (63%)] Loss: 16468.269531\n",
      "Train Epoch: 423 [144960/225000 (64%)] Loss: 18400.466797\n",
      "Train Epoch: 423 [147456/225000 (66%)] Loss: 16785.048828\n",
      "Train Epoch: 423 [149952/225000 (67%)] Loss: 16619.029297\n",
      "Train Epoch: 423 [152448/225000 (68%)] Loss: 16895.996094\n",
      "Train Epoch: 423 [154944/225000 (69%)] Loss: 16085.606445\n",
      "Train Epoch: 423 [157440/225000 (70%)] Loss: 16384.150391\n",
      "Train Epoch: 423 [159936/225000 (71%)] Loss: 17030.972656\n",
      "Train Epoch: 423 [162432/225000 (72%)] Loss: 16411.660156\n",
      "Train Epoch: 423 [164928/225000 (73%)] Loss: 17141.250000\n",
      "Train Epoch: 423 [167424/225000 (74%)] Loss: 16412.730469\n",
      "Train Epoch: 423 [169920/225000 (76%)] Loss: 16623.796875\n",
      "Train Epoch: 423 [172416/225000 (77%)] Loss: 16802.982422\n",
      "Train Epoch: 423 [174912/225000 (78%)] Loss: 16485.875000\n",
      "Train Epoch: 423 [177408/225000 (79%)] Loss: 17051.189453\n",
      "Train Epoch: 423 [179904/225000 (80%)] Loss: 16610.835938\n",
      "Train Epoch: 423 [182400/225000 (81%)] Loss: 16102.733398\n",
      "Train Epoch: 423 [184896/225000 (82%)] Loss: 16560.070312\n",
      "Train Epoch: 423 [187392/225000 (83%)] Loss: 16576.917969\n",
      "Train Epoch: 423 [189888/225000 (84%)] Loss: 16506.023438\n",
      "Train Epoch: 423 [192384/225000 (86%)] Loss: 17209.005859\n",
      "Train Epoch: 423 [194880/225000 (87%)] Loss: 16303.714844\n",
      "Train Epoch: 423 [197376/225000 (88%)] Loss: 16603.941406\n",
      "Train Epoch: 423 [199872/225000 (89%)] Loss: 16367.654297\n",
      "Train Epoch: 423 [202368/225000 (90%)] Loss: 16549.808594\n",
      "Train Epoch: 423 [204864/225000 (91%)] Loss: 16678.093750\n",
      "Train Epoch: 423 [207360/225000 (92%)] Loss: 17029.460938\n",
      "Train Epoch: 423 [209856/225000 (93%)] Loss: 16586.441406\n",
      "Train Epoch: 423 [212352/225000 (94%)] Loss: 16683.089844\n",
      "Train Epoch: 423 [214848/225000 (95%)] Loss: 16940.632812\n",
      "Train Epoch: 423 [217344/225000 (97%)] Loss: 16891.265625\n",
      "Train Epoch: 423 [219840/225000 (98%)] Loss: 16957.037109\n",
      "Train Epoch: 423 [222336/225000 (99%)] Loss: 16634.779297\n",
      "Train Epoch: 423 [224832/225000 (100%)] Loss: 16383.795898\n",
      "    epoch          : 423\n",
      "    loss           : 16715.86666005626\n",
      "    val_loss       : 16623.672587835154\n",
      "Train Epoch: 424 [192/225000 (0%)] Loss: 18320.585938\n",
      "Train Epoch: 424 [2688/225000 (1%)] Loss: 16915.085938\n",
      "Train Epoch: 424 [5184/225000 (2%)] Loss: 17119.312500\n",
      "Train Epoch: 424 [7680/225000 (3%)] Loss: 16007.718750\n",
      "Train Epoch: 424 [10176/225000 (5%)] Loss: 16746.806641\n",
      "Train Epoch: 424 [12672/225000 (6%)] Loss: 16458.343750\n",
      "Train Epoch: 424 [15168/225000 (7%)] Loss: 16279.715820\n",
      "Train Epoch: 424 [17664/225000 (8%)] Loss: 16507.669922\n",
      "Train Epoch: 424 [20160/225000 (9%)] Loss: 16664.312500\n",
      "Train Epoch: 424 [22656/225000 (10%)] Loss: 16482.621094\n",
      "Train Epoch: 424 [25152/225000 (11%)] Loss: 16861.792969\n",
      "Train Epoch: 424 [27648/225000 (12%)] Loss: 16817.339844\n",
      "Train Epoch: 424 [30144/225000 (13%)] Loss: 16643.244141\n",
      "Train Epoch: 424 [32640/225000 (15%)] Loss: 16667.839844\n",
      "Train Epoch: 424 [35136/225000 (16%)] Loss: 16538.359375\n",
      "Train Epoch: 424 [37632/225000 (17%)] Loss: 16537.283203\n",
      "Train Epoch: 424 [40128/225000 (18%)] Loss: 16382.348633\n",
      "Train Epoch: 424 [42624/225000 (19%)] Loss: 16629.160156\n",
      "Train Epoch: 424 [45120/225000 (20%)] Loss: 17053.080078\n",
      "Train Epoch: 424 [47616/225000 (21%)] Loss: 16858.277344\n",
      "Train Epoch: 424 [50112/225000 (22%)] Loss: 16411.101562\n",
      "Train Epoch: 424 [52608/225000 (23%)] Loss: 16632.091797\n",
      "Train Epoch: 424 [55104/225000 (24%)] Loss: 16633.794922\n",
      "Train Epoch: 424 [57600/225000 (26%)] Loss: 16368.160156\n",
      "Train Epoch: 424 [60096/225000 (27%)] Loss: 16060.510742\n",
      "Train Epoch: 424 [62592/225000 (28%)] Loss: 16332.639648\n",
      "Train Epoch: 424 [65088/225000 (29%)] Loss: 16525.296875\n",
      "Train Epoch: 424 [67584/225000 (30%)] Loss: 16308.166992\n",
      "Train Epoch: 424 [70080/225000 (31%)] Loss: 16723.085938\n",
      "Train Epoch: 424 [72576/225000 (32%)] Loss: 16859.833984\n",
      "Train Epoch: 424 [75072/225000 (33%)] Loss: 16726.671875\n",
      "Train Epoch: 424 [77568/225000 (34%)] Loss: 16896.517578\n",
      "Train Epoch: 424 [80064/225000 (36%)] Loss: 16399.910156\n",
      "Train Epoch: 424 [82560/225000 (37%)] Loss: 16993.175781\n",
      "Train Epoch: 424 [85056/225000 (38%)] Loss: 16708.257812\n",
      "Train Epoch: 424 [87552/225000 (39%)] Loss: 16483.060547\n",
      "Train Epoch: 424 [90048/225000 (40%)] Loss: 16253.188477\n",
      "Train Epoch: 424 [92544/225000 (41%)] Loss: 16864.425781\n",
      "Train Epoch: 424 [95040/225000 (42%)] Loss: 16298.407227\n",
      "Train Epoch: 424 [97536/225000 (43%)] Loss: 16892.316406\n",
      "Train Epoch: 424 [100032/225000 (44%)] Loss: 16973.775391\n",
      "Train Epoch: 424 [102528/225000 (46%)] Loss: 16813.812500\n",
      "Train Epoch: 424 [105024/225000 (47%)] Loss: 16528.117188\n",
      "Train Epoch: 424 [107520/225000 (48%)] Loss: 16393.068359\n",
      "Train Epoch: 424 [110016/225000 (49%)] Loss: 16610.117188\n",
      "Train Epoch: 424 [112512/225000 (50%)] Loss: 16947.250000\n",
      "Train Epoch: 424 [115008/225000 (51%)] Loss: 16578.472656\n",
      "Train Epoch: 424 [117504/225000 (52%)] Loss: 16267.445312\n",
      "Train Epoch: 424 [120000/225000 (53%)] Loss: 16374.869141\n",
      "Train Epoch: 424 [122496/225000 (54%)] Loss: 16732.078125\n",
      "Train Epoch: 424 [124992/225000 (56%)] Loss: 16572.019531\n",
      "Train Epoch: 424 [127488/225000 (57%)] Loss: 16566.648438\n",
      "Train Epoch: 424 [129984/225000 (58%)] Loss: 16836.326172\n",
      "Train Epoch: 424 [132480/225000 (59%)] Loss: 16936.039062\n",
      "Train Epoch: 424 [134976/225000 (60%)] Loss: 16116.853516\n",
      "Train Epoch: 424 [137472/225000 (61%)] Loss: 16540.859375\n",
      "Train Epoch: 424 [139968/225000 (62%)] Loss: 17230.164062\n",
      "Train Epoch: 424 [142464/225000 (63%)] Loss: 16853.238281\n",
      "Train Epoch: 424 [144960/225000 (64%)] Loss: 17036.164062\n",
      "Train Epoch: 424 [147456/225000 (66%)] Loss: 16799.574219\n",
      "Train Epoch: 424 [149952/225000 (67%)] Loss: 16648.533203\n",
      "Train Epoch: 424 [152448/225000 (68%)] Loss: 16269.600586\n",
      "Train Epoch: 424 [154944/225000 (69%)] Loss: 16728.039062\n",
      "Train Epoch: 424 [157440/225000 (70%)] Loss: 16538.238281\n",
      "Train Epoch: 424 [159936/225000 (71%)] Loss: 16941.988281\n",
      "Train Epoch: 424 [162432/225000 (72%)] Loss: 16627.908203\n",
      "Train Epoch: 424 [164928/225000 (73%)] Loss: 16614.263672\n",
      "Train Epoch: 424 [167424/225000 (74%)] Loss: 16606.705078\n",
      "Train Epoch: 424 [169920/225000 (76%)] Loss: 16900.496094\n",
      "Train Epoch: 424 [172416/225000 (77%)] Loss: 16881.347656\n",
      "Train Epoch: 424 [174912/225000 (78%)] Loss: 16648.910156\n",
      "Train Epoch: 424 [177408/225000 (79%)] Loss: 16466.910156\n",
      "Train Epoch: 424 [179904/225000 (80%)] Loss: 17039.755859\n",
      "Train Epoch: 424 [182400/225000 (81%)] Loss: 17109.789062\n",
      "Train Epoch: 424 [184896/225000 (82%)] Loss: 16951.457031\n",
      "Train Epoch: 424 [187392/225000 (83%)] Loss: 16791.015625\n",
      "Train Epoch: 424 [189888/225000 (84%)] Loss: 16952.404297\n",
      "Train Epoch: 424 [192384/225000 (86%)] Loss: 16811.738281\n",
      "Train Epoch: 424 [194880/225000 (87%)] Loss: 16115.233398\n",
      "Train Epoch: 424 [197376/225000 (88%)] Loss: 16623.371094\n",
      "Train Epoch: 424 [199872/225000 (89%)] Loss: 16819.101562\n",
      "Train Epoch: 424 [202368/225000 (90%)] Loss: 16715.785156\n",
      "Train Epoch: 424 [204864/225000 (91%)] Loss: 16496.625000\n",
      "Train Epoch: 424 [207360/225000 (92%)] Loss: 16683.523438\n",
      "Train Epoch: 424 [209856/225000 (93%)] Loss: 16815.683594\n",
      "Train Epoch: 424 [212352/225000 (94%)] Loss: 18240.476562\n",
      "Train Epoch: 424 [214848/225000 (95%)] Loss: 17115.375000\n",
      "Train Epoch: 424 [217344/225000 (97%)] Loss: 16314.436523\n",
      "Train Epoch: 424 [219840/225000 (98%)] Loss: 16419.904297\n",
      "Train Epoch: 424 [222336/225000 (99%)] Loss: 16671.171875\n",
      "Train Epoch: 424 [224832/225000 (100%)] Loss: 16424.187500\n",
      "    epoch          : 424\n",
      "    loss           : 16732.544778556952\n",
      "    val_loss       : 16691.997946688236\n",
      "Train Epoch: 425 [192/225000 (0%)] Loss: 18573.953125\n",
      "Train Epoch: 425 [2688/225000 (1%)] Loss: 16955.453125\n",
      "Train Epoch: 425 [5184/225000 (2%)] Loss: 16700.050781\n",
      "Train Epoch: 425 [7680/225000 (3%)] Loss: 16499.564453\n",
      "Train Epoch: 425 [10176/225000 (5%)] Loss: 16505.070312\n",
      "Train Epoch: 425 [12672/225000 (6%)] Loss: 16551.058594\n",
      "Train Epoch: 425 [15168/225000 (7%)] Loss: 18081.453125\n",
      "Train Epoch: 425 [17664/225000 (8%)] Loss: 16389.443359\n",
      "Train Epoch: 425 [20160/225000 (9%)] Loss: 16476.830078\n",
      "Train Epoch: 425 [22656/225000 (10%)] Loss: 18236.042969\n",
      "Train Epoch: 425 [25152/225000 (11%)] Loss: 16286.865234\n",
      "Train Epoch: 425 [27648/225000 (12%)] Loss: 16556.335938\n",
      "Train Epoch: 425 [30144/225000 (13%)] Loss: 16549.542969\n",
      "Train Epoch: 425 [32640/225000 (15%)] Loss: 16598.242188\n",
      "Train Epoch: 425 [35136/225000 (16%)] Loss: 16253.656250\n",
      "Train Epoch: 425 [37632/225000 (17%)] Loss: 16773.289062\n",
      "Train Epoch: 425 [40128/225000 (18%)] Loss: 16576.445312\n",
      "Train Epoch: 425 [42624/225000 (19%)] Loss: 17052.269531\n",
      "Train Epoch: 425 [45120/225000 (20%)] Loss: 17101.185547\n",
      "Train Epoch: 425 [47616/225000 (21%)] Loss: 16978.617188\n",
      "Train Epoch: 425 [50112/225000 (22%)] Loss: 16508.146484\n",
      "Train Epoch: 425 [52608/225000 (23%)] Loss: 16332.534180\n",
      "Train Epoch: 425 [55104/225000 (24%)] Loss: 16308.242188\n",
      "Train Epoch: 425 [57600/225000 (26%)] Loss: 18016.773438\n",
      "Train Epoch: 425 [60096/225000 (27%)] Loss: 16922.761719\n",
      "Train Epoch: 425 [62592/225000 (28%)] Loss: 16322.405273\n",
      "Train Epoch: 425 [65088/225000 (29%)] Loss: 16641.978516\n",
      "Train Epoch: 425 [67584/225000 (30%)] Loss: 16870.519531\n",
      "Train Epoch: 425 [70080/225000 (31%)] Loss: 16847.945312\n",
      "Train Epoch: 425 [72576/225000 (32%)] Loss: 16666.460938\n",
      "Train Epoch: 425 [75072/225000 (33%)] Loss: 17148.156250\n",
      "Train Epoch: 425 [77568/225000 (34%)] Loss: 16242.025391\n",
      "Train Epoch: 425 [80064/225000 (36%)] Loss: 16081.855469\n",
      "Train Epoch: 425 [82560/225000 (37%)] Loss: 16565.824219\n",
      "Train Epoch: 425 [85056/225000 (38%)] Loss: 16590.250000\n",
      "Train Epoch: 425 [87552/225000 (39%)] Loss: 16939.265625\n",
      "Train Epoch: 425 [90048/225000 (40%)] Loss: 16540.250000\n",
      "Train Epoch: 425 [92544/225000 (41%)] Loss: 17133.791016\n",
      "Train Epoch: 425 [95040/225000 (42%)] Loss: 16761.976562\n",
      "Train Epoch: 425 [97536/225000 (43%)] Loss: 16413.195312\n",
      "Train Epoch: 425 [100032/225000 (44%)] Loss: 18195.871094\n",
      "Train Epoch: 425 [102528/225000 (46%)] Loss: 16356.516602\n",
      "Train Epoch: 425 [105024/225000 (47%)] Loss: 17008.871094\n",
      "Train Epoch: 425 [107520/225000 (48%)] Loss: 16838.480469\n",
      "Train Epoch: 425 [110016/225000 (49%)] Loss: 16411.242188\n",
      "Train Epoch: 425 [112512/225000 (50%)] Loss: 16539.130859\n",
      "Train Epoch: 425 [115008/225000 (51%)] Loss: 16625.882812\n",
      "Train Epoch: 425 [117504/225000 (52%)] Loss: 16466.050781\n",
      "Train Epoch: 425 [120000/225000 (53%)] Loss: 17200.648438\n",
      "Train Epoch: 425 [122496/225000 (54%)] Loss: 16361.860352\n",
      "Train Epoch: 425 [124992/225000 (56%)] Loss: 16564.152344\n",
      "Train Epoch: 425 [127488/225000 (57%)] Loss: 16826.808594\n",
      "Train Epoch: 425 [129984/225000 (58%)] Loss: 16672.617188\n",
      "Train Epoch: 425 [132480/225000 (59%)] Loss: 16897.757812\n",
      "Train Epoch: 425 [134976/225000 (60%)] Loss: 16071.085938\n",
      "Train Epoch: 425 [137472/225000 (61%)] Loss: 16688.433594\n",
      "Train Epoch: 425 [139968/225000 (62%)] Loss: 16633.000000\n",
      "Train Epoch: 425 [142464/225000 (63%)] Loss: 16541.503906\n",
      "Train Epoch: 425 [144960/225000 (64%)] Loss: 16472.265625\n",
      "Train Epoch: 425 [147456/225000 (66%)] Loss: 16394.806641\n",
      "Train Epoch: 425 [149952/225000 (67%)] Loss: 17025.960938\n",
      "Train Epoch: 425 [152448/225000 (68%)] Loss: 16504.082031\n",
      "Train Epoch: 425 [154944/225000 (69%)] Loss: 16046.229492\n",
      "Train Epoch: 425 [157440/225000 (70%)] Loss: 16957.240234\n",
      "Train Epoch: 425 [159936/225000 (71%)] Loss: 16799.976562\n",
      "Train Epoch: 425 [162432/225000 (72%)] Loss: 16426.921875\n",
      "Train Epoch: 425 [164928/225000 (73%)] Loss: 16334.241211\n",
      "Train Epoch: 425 [167424/225000 (74%)] Loss: 16816.625000\n",
      "Train Epoch: 425 [169920/225000 (76%)] Loss: 16538.539062\n",
      "Train Epoch: 425 [172416/225000 (77%)] Loss: 16909.728516\n",
      "Train Epoch: 425 [174912/225000 (78%)] Loss: 16777.851562\n",
      "Train Epoch: 425 [177408/225000 (79%)] Loss: 16364.223633\n",
      "Train Epoch: 425 [179904/225000 (80%)] Loss: 16456.417969\n",
      "Train Epoch: 425 [182400/225000 (81%)] Loss: 16830.285156\n",
      "Train Epoch: 425 [184896/225000 (82%)] Loss: 16574.583984\n",
      "Train Epoch: 425 [187392/225000 (83%)] Loss: 16659.035156\n",
      "Train Epoch: 425 [189888/225000 (84%)] Loss: 16899.488281\n",
      "Train Epoch: 425 [192384/225000 (86%)] Loss: 16607.898438\n",
      "Train Epoch: 425 [194880/225000 (87%)] Loss: 16632.853516\n",
      "Train Epoch: 425 [197376/225000 (88%)] Loss: 16688.500000\n",
      "Train Epoch: 425 [199872/225000 (89%)] Loss: 16907.441406\n",
      "Train Epoch: 425 [202368/225000 (90%)] Loss: 16509.386719\n",
      "Train Epoch: 425 [204864/225000 (91%)] Loss: 16560.320312\n",
      "Train Epoch: 425 [207360/225000 (92%)] Loss: 18412.652344\n",
      "Train Epoch: 425 [209856/225000 (93%)] Loss: 16956.035156\n",
      "Train Epoch: 425 [212352/225000 (94%)] Loss: 16458.888672\n",
      "Train Epoch: 425 [214848/225000 (95%)] Loss: 16657.312500\n",
      "Train Epoch: 425 [217344/225000 (97%)] Loss: 16974.347656\n",
      "Train Epoch: 425 [219840/225000 (98%)] Loss: 16532.843750\n",
      "Train Epoch: 425 [222336/225000 (99%)] Loss: 16655.769531\n",
      "Train Epoch: 425 [224832/225000 (100%)] Loss: 16650.105469\n",
      "    epoch          : 425\n",
      "    loss           : 16724.20840443686\n",
      "    val_loss       : 16658.85709935654\n",
      "Train Epoch: 426 [192/225000 (0%)] Loss: 16576.093750\n",
      "Train Epoch: 426 [2688/225000 (1%)] Loss: 17052.867188\n",
      "Train Epoch: 426 [5184/225000 (2%)] Loss: 16213.882812\n",
      "Train Epoch: 426 [7680/225000 (3%)] Loss: 17087.275391\n",
      "Train Epoch: 426 [10176/225000 (5%)] Loss: 16407.375000\n",
      "Train Epoch: 426 [12672/225000 (6%)] Loss: 16932.708984\n",
      "Train Epoch: 426 [15168/225000 (7%)] Loss: 16103.382812\n",
      "Train Epoch: 426 [17664/225000 (8%)] Loss: 16496.335938\n",
      "Train Epoch: 426 [20160/225000 (9%)] Loss: 17032.148438\n",
      "Train Epoch: 426 [22656/225000 (10%)] Loss: 16616.792969\n",
      "Train Epoch: 426 [25152/225000 (11%)] Loss: 16761.369141\n",
      "Train Epoch: 426 [27648/225000 (12%)] Loss: 16512.851562\n",
      "Train Epoch: 426 [30144/225000 (13%)] Loss: 18078.958984\n",
      "Train Epoch: 426 [32640/225000 (15%)] Loss: 16564.841797\n",
      "Train Epoch: 426 [35136/225000 (16%)] Loss: 16800.207031\n",
      "Train Epoch: 426 [37632/225000 (17%)] Loss: 16459.984375\n",
      "Train Epoch: 426 [40128/225000 (18%)] Loss: 16473.441406\n",
      "Train Epoch: 426 [42624/225000 (19%)] Loss: 18088.687500\n",
      "Train Epoch: 426 [45120/225000 (20%)] Loss: 16825.722656\n",
      "Train Epoch: 426 [47616/225000 (21%)] Loss: 16630.802734\n",
      "Train Epoch: 426 [50112/225000 (22%)] Loss: 16461.904297\n",
      "Train Epoch: 426 [52608/225000 (23%)] Loss: 16747.271484\n",
      "Train Epoch: 426 [55104/225000 (24%)] Loss: 16632.300781\n",
      "Train Epoch: 426 [57600/225000 (26%)] Loss: 16911.003906\n",
      "Train Epoch: 426 [60096/225000 (27%)] Loss: 16800.486328\n",
      "Train Epoch: 426 [62592/225000 (28%)] Loss: 16930.119141\n",
      "Train Epoch: 426 [65088/225000 (29%)] Loss: 16629.517578\n",
      "Train Epoch: 426 [67584/225000 (30%)] Loss: 16841.902344\n",
      "Train Epoch: 426 [70080/225000 (31%)] Loss: 17858.589844\n",
      "Train Epoch: 426 [72576/225000 (32%)] Loss: 16523.542969\n",
      "Train Epoch: 426 [75072/225000 (33%)] Loss: 16553.531250\n",
      "Train Epoch: 426 [77568/225000 (34%)] Loss: 16568.029297\n",
      "Train Epoch: 426 [80064/225000 (36%)] Loss: 16594.917969\n",
      "Train Epoch: 426 [82560/225000 (37%)] Loss: 16944.343750\n",
      "Train Epoch: 426 [85056/225000 (38%)] Loss: 16913.703125\n",
      "Train Epoch: 426 [87552/225000 (39%)] Loss: 16938.386719\n",
      "Train Epoch: 426 [90048/225000 (40%)] Loss: 16691.845703\n",
      "Train Epoch: 426 [92544/225000 (41%)] Loss: 16220.972656\n",
      "Train Epoch: 426 [95040/225000 (42%)] Loss: 17146.400391\n",
      "Train Epoch: 426 [97536/225000 (43%)] Loss: 16820.070312\n",
      "Train Epoch: 426 [100032/225000 (44%)] Loss: 16877.500000\n",
      "Train Epoch: 426 [102528/225000 (46%)] Loss: 16626.875000\n",
      "Train Epoch: 426 [105024/225000 (47%)] Loss: 18383.556641\n",
      "Train Epoch: 426 [107520/225000 (48%)] Loss: 16738.798828\n",
      "Train Epoch: 426 [110016/225000 (49%)] Loss: 16803.839844\n",
      "Train Epoch: 426 [112512/225000 (50%)] Loss: 17138.445312\n",
      "Train Epoch: 426 [115008/225000 (51%)] Loss: 16677.615234\n",
      "Train Epoch: 426 [117504/225000 (52%)] Loss: 16547.267578\n",
      "Train Epoch: 426 [120000/225000 (53%)] Loss: 16834.621094\n",
      "Train Epoch: 426 [122496/225000 (54%)] Loss: 16705.621094\n",
      "Train Epoch: 426 [124992/225000 (56%)] Loss: 16836.822266\n",
      "Train Epoch: 426 [127488/225000 (57%)] Loss: 16661.037109\n",
      "Train Epoch: 426 [129984/225000 (58%)] Loss: 16748.390625\n",
      "Train Epoch: 426 [132480/225000 (59%)] Loss: 16709.289062\n",
      "Train Epoch: 426 [134976/225000 (60%)] Loss: 16903.146484\n",
      "Train Epoch: 426 [137472/225000 (61%)] Loss: 16387.841797\n",
      "Train Epoch: 426 [139968/225000 (62%)] Loss: 16796.527344\n",
      "Train Epoch: 426 [142464/225000 (63%)] Loss: 16546.917969\n",
      "Train Epoch: 426 [144960/225000 (64%)] Loss: 16523.777344\n",
      "Train Epoch: 426 [147456/225000 (66%)] Loss: 16327.202148\n",
      "Train Epoch: 426 [149952/225000 (67%)] Loss: 16606.203125\n",
      "Train Epoch: 426 [152448/225000 (68%)] Loss: 17980.693359\n",
      "Train Epoch: 426 [154944/225000 (69%)] Loss: 16690.718750\n",
      "Train Epoch: 426 [157440/225000 (70%)] Loss: 16404.386719\n",
      "Train Epoch: 426 [159936/225000 (71%)] Loss: 16294.145508\n",
      "Train Epoch: 426 [162432/225000 (72%)] Loss: 17095.951172\n",
      "Train Epoch: 426 [164928/225000 (73%)] Loss: 16936.480469\n",
      "Train Epoch: 426 [167424/225000 (74%)] Loss: 16436.968750\n",
      "Train Epoch: 426 [169920/225000 (76%)] Loss: 17147.132812\n",
      "Train Epoch: 426 [172416/225000 (77%)] Loss: 16968.712891\n",
      "Train Epoch: 426 [174912/225000 (78%)] Loss: 17059.753906\n",
      "Train Epoch: 426 [177408/225000 (79%)] Loss: 16154.038086\n",
      "Train Epoch: 426 [179904/225000 (80%)] Loss: 16882.781250\n",
      "Train Epoch: 426 [182400/225000 (81%)] Loss: 16881.070312\n",
      "Train Epoch: 426 [184896/225000 (82%)] Loss: 15953.380859\n",
      "Train Epoch: 426 [187392/225000 (83%)] Loss: 16792.863281\n",
      "Train Epoch: 426 [189888/225000 (84%)] Loss: 16089.609375\n",
      "Train Epoch: 426 [192384/225000 (86%)] Loss: 16407.187500\n",
      "Train Epoch: 426 [194880/225000 (87%)] Loss: 17041.712891\n",
      "Train Epoch: 426 [197376/225000 (88%)] Loss: 16421.056641\n",
      "Train Epoch: 426 [199872/225000 (89%)] Loss: 16648.621094\n",
      "Train Epoch: 426 [202368/225000 (90%)] Loss: 16790.597656\n",
      "Train Epoch: 426 [204864/225000 (91%)] Loss: 16477.562500\n",
      "Train Epoch: 426 [207360/225000 (92%)] Loss: 16804.656250\n",
      "Train Epoch: 426 [209856/225000 (93%)] Loss: 17123.277344\n",
      "Train Epoch: 426 [212352/225000 (94%)] Loss: 16049.655273\n",
      "Train Epoch: 426 [214848/225000 (95%)] Loss: 17936.417969\n",
      "Train Epoch: 426 [217344/225000 (97%)] Loss: 16860.460938\n",
      "Train Epoch: 426 [219840/225000 (98%)] Loss: 16453.929688\n",
      "Train Epoch: 426 [222336/225000 (99%)] Loss: 16062.471680\n",
      "Train Epoch: 426 [224832/225000 (100%)] Loss: 16262.445312\n",
      "    epoch          : 426\n",
      "    loss           : 16707.599318572684\n",
      "    val_loss       : 16637.9790707499\n",
      "Train Epoch: 427 [192/225000 (0%)] Loss: 16384.724609\n",
      "Train Epoch: 427 [2688/225000 (1%)] Loss: 16517.257812\n",
      "Train Epoch: 427 [5184/225000 (2%)] Loss: 17948.423828\n",
      "Train Epoch: 427 [7680/225000 (3%)] Loss: 16383.971680\n",
      "Train Epoch: 427 [10176/225000 (5%)] Loss: 16817.949219\n",
      "Train Epoch: 427 [12672/225000 (6%)] Loss: 16829.574219\n",
      "Train Epoch: 427 [15168/225000 (7%)] Loss: 17048.035156\n",
      "Train Epoch: 427 [17664/225000 (8%)] Loss: 16645.082031\n",
      "Train Epoch: 427 [20160/225000 (9%)] Loss: 16556.710938\n",
      "Train Epoch: 427 [22656/225000 (10%)] Loss: 16666.835938\n",
      "Train Epoch: 427 [25152/225000 (11%)] Loss: 16795.972656\n",
      "Train Epoch: 427 [27648/225000 (12%)] Loss: 16714.000000\n",
      "Train Epoch: 427 [30144/225000 (13%)] Loss: 16824.910156\n",
      "Train Epoch: 427 [32640/225000 (15%)] Loss: 17024.074219\n",
      "Train Epoch: 427 [35136/225000 (16%)] Loss: 16945.585938\n",
      "Train Epoch: 427 [37632/225000 (17%)] Loss: 16401.330078\n",
      "Train Epoch: 427 [40128/225000 (18%)] Loss: 16526.878906\n",
      "Train Epoch: 427 [42624/225000 (19%)] Loss: 16208.673828\n",
      "Train Epoch: 427 [45120/225000 (20%)] Loss: 16691.636719\n",
      "Train Epoch: 427 [47616/225000 (21%)] Loss: 16464.812500\n",
      "Train Epoch: 427 [50112/225000 (22%)] Loss: 16591.511719\n",
      "Train Epoch: 427 [52608/225000 (23%)] Loss: 16831.529297\n",
      "Train Epoch: 427 [55104/225000 (24%)] Loss: 16422.261719\n",
      "Train Epoch: 427 [57600/225000 (26%)] Loss: 16720.304688\n",
      "Train Epoch: 427 [60096/225000 (27%)] Loss: 16395.107422\n",
      "Train Epoch: 427 [62592/225000 (28%)] Loss: 15909.720703\n",
      "Train Epoch: 427 [65088/225000 (29%)] Loss: 16406.597656\n",
      "Train Epoch: 427 [67584/225000 (30%)] Loss: 16073.227539\n",
      "Train Epoch: 427 [70080/225000 (31%)] Loss: 17073.605469\n",
      "Train Epoch: 427 [72576/225000 (32%)] Loss: 17129.648438\n",
      "Train Epoch: 427 [75072/225000 (33%)] Loss: 16262.698242\n",
      "Train Epoch: 427 [77568/225000 (34%)] Loss: 16054.889648\n",
      "Train Epoch: 427 [80064/225000 (36%)] Loss: 16635.365234\n",
      "Train Epoch: 427 [82560/225000 (37%)] Loss: 17685.496094\n",
      "Train Epoch: 427 [85056/225000 (38%)] Loss: 16580.464844\n",
      "Train Epoch: 427 [87552/225000 (39%)] Loss: 16750.947266\n",
      "Train Epoch: 427 [90048/225000 (40%)] Loss: 17332.906250\n",
      "Train Epoch: 427 [92544/225000 (41%)] Loss: 16786.603516\n",
      "Train Epoch: 427 [95040/225000 (42%)] Loss: 18424.722656\n",
      "Train Epoch: 427 [97536/225000 (43%)] Loss: 16744.886719\n",
      "Train Epoch: 427 [100032/225000 (44%)] Loss: 16345.851562\n",
      "Train Epoch: 427 [102528/225000 (46%)] Loss: 16264.342773\n",
      "Train Epoch: 427 [105024/225000 (47%)] Loss: 16935.160156\n",
      "Train Epoch: 427 [107520/225000 (48%)] Loss: 17101.597656\n",
      "Train Epoch: 427 [110016/225000 (49%)] Loss: 16588.775391\n",
      "Train Epoch: 427 [112512/225000 (50%)] Loss: 16551.160156\n",
      "Train Epoch: 427 [115008/225000 (51%)] Loss: 16506.498047\n",
      "Train Epoch: 427 [117504/225000 (52%)] Loss: 16249.626953\n",
      "Train Epoch: 427 [120000/225000 (53%)] Loss: 16408.328125\n",
      "Train Epoch: 427 [122496/225000 (54%)] Loss: 16747.744141\n",
      "Train Epoch: 427 [124992/225000 (56%)] Loss: 16409.900391\n",
      "Train Epoch: 427 [127488/225000 (57%)] Loss: 16565.917969\n",
      "Train Epoch: 427 [129984/225000 (58%)] Loss: 16738.208984\n",
      "Train Epoch: 427 [132480/225000 (59%)] Loss: 16408.291016\n",
      "Train Epoch: 427 [134976/225000 (60%)] Loss: 16510.919922\n",
      "Train Epoch: 427 [137472/225000 (61%)] Loss: 17132.253906\n",
      "Train Epoch: 427 [139968/225000 (62%)] Loss: 16882.841797\n",
      "Train Epoch: 427 [142464/225000 (63%)] Loss: 16653.589844\n",
      "Train Epoch: 427 [144960/225000 (64%)] Loss: 16796.085938\n",
      "Train Epoch: 427 [147456/225000 (66%)] Loss: 16949.527344\n",
      "Train Epoch: 427 [149952/225000 (67%)] Loss: 16956.841797\n",
      "Train Epoch: 427 [152448/225000 (68%)] Loss: 16786.742188\n",
      "Train Epoch: 427 [154944/225000 (69%)] Loss: 16691.548828\n",
      "Train Epoch: 427 [157440/225000 (70%)] Loss: 16587.964844\n",
      "Train Epoch: 427 [159936/225000 (71%)] Loss: 16681.062500\n",
      "Train Epoch: 427 [162432/225000 (72%)] Loss: 16698.320312\n",
      "Train Epoch: 427 [164928/225000 (73%)] Loss: 16555.767578\n",
      "Train Epoch: 427 [167424/225000 (74%)] Loss: 16544.277344\n",
      "Train Epoch: 427 [169920/225000 (76%)] Loss: 16583.828125\n",
      "Train Epoch: 427 [172416/225000 (77%)] Loss: 16617.316406\n",
      "Train Epoch: 427 [174912/225000 (78%)] Loss: 16764.140625\n",
      "Train Epoch: 427 [177408/225000 (79%)] Loss: 18215.597656\n",
      "Train Epoch: 427 [179904/225000 (80%)] Loss: 16720.074219\n",
      "Train Epoch: 427 [182400/225000 (81%)] Loss: 18426.648438\n",
      "Train Epoch: 427 [184896/225000 (82%)] Loss: 17179.042969\n",
      "Train Epoch: 427 [187392/225000 (83%)] Loss: 16775.824219\n",
      "Train Epoch: 427 [189888/225000 (84%)] Loss: 16414.429688\n",
      "Train Epoch: 427 [192384/225000 (86%)] Loss: 16538.888672\n",
      "Train Epoch: 427 [194880/225000 (87%)] Loss: 16785.812500\n",
      "Train Epoch: 427 [197376/225000 (88%)] Loss: 16756.851562\n",
      "Train Epoch: 427 [199872/225000 (89%)] Loss: 16868.753906\n",
      "Train Epoch: 427 [202368/225000 (90%)] Loss: 16598.957031\n",
      "Train Epoch: 427 [204864/225000 (91%)] Loss: 16918.843750\n",
      "Train Epoch: 427 [207360/225000 (92%)] Loss: 16737.603516\n",
      "Train Epoch: 427 [209856/225000 (93%)] Loss: 16920.509766\n",
      "Train Epoch: 427 [212352/225000 (94%)] Loss: 16625.357422\n",
      "Train Epoch: 427 [214848/225000 (95%)] Loss: 16767.623047\n",
      "Train Epoch: 427 [217344/225000 (97%)] Loss: 16738.978516\n",
      "Train Epoch: 427 [219840/225000 (98%)] Loss: 16707.605469\n",
      "Train Epoch: 427 [222336/225000 (99%)] Loss: 16325.433594\n",
      "Train Epoch: 427 [224832/225000 (100%)] Loss: 16512.187500\n",
      "    epoch          : 427\n",
      "    loss           : 16744.77848362841\n",
      "    val_loss       : 16622.543722790615\n",
      "Train Epoch: 428 [192/225000 (0%)] Loss: 16401.894531\n",
      "Train Epoch: 428 [2688/225000 (1%)] Loss: 16623.861328\n",
      "Train Epoch: 428 [5184/225000 (2%)] Loss: 16730.394531\n",
      "Train Epoch: 428 [7680/225000 (3%)] Loss: 16122.219727\n",
      "Train Epoch: 428 [10176/225000 (5%)] Loss: 16560.443359\n",
      "Train Epoch: 428 [12672/225000 (6%)] Loss: 16518.593750\n",
      "Train Epoch: 428 [15168/225000 (7%)] Loss: 17213.562500\n",
      "Train Epoch: 428 [17664/225000 (8%)] Loss: 16688.380859\n",
      "Train Epoch: 428 [20160/225000 (9%)] Loss: 17182.179688\n",
      "Train Epoch: 428 [22656/225000 (10%)] Loss: 16395.353516\n",
      "Train Epoch: 428 [25152/225000 (11%)] Loss: 16643.341797\n",
      "Train Epoch: 428 [27648/225000 (12%)] Loss: 16311.791016\n",
      "Train Epoch: 428 [30144/225000 (13%)] Loss: 16803.556641\n",
      "Train Epoch: 428 [32640/225000 (15%)] Loss: 16683.816406\n",
      "Train Epoch: 428 [35136/225000 (16%)] Loss: 16748.292969\n",
      "Train Epoch: 428 [37632/225000 (17%)] Loss: 16341.568359\n",
      "Train Epoch: 428 [40128/225000 (18%)] Loss: 16728.476562\n",
      "Train Epoch: 428 [42624/225000 (19%)] Loss: 16882.746094\n",
      "Train Epoch: 428 [45120/225000 (20%)] Loss: 16671.320312\n",
      "Train Epoch: 428 [47616/225000 (21%)] Loss: 16810.500000\n",
      "Train Epoch: 428 [50112/225000 (22%)] Loss: 16515.699219\n",
      "Train Epoch: 428 [52608/225000 (23%)] Loss: 24910.701172\n",
      "Train Epoch: 428 [55104/225000 (24%)] Loss: 16798.531250\n",
      "Train Epoch: 428 [57600/225000 (26%)] Loss: 16945.792969\n",
      "Train Epoch: 428 [60096/225000 (27%)] Loss: 16316.864258\n",
      "Train Epoch: 428 [62592/225000 (28%)] Loss: 16694.574219\n",
      "Train Epoch: 428 [65088/225000 (29%)] Loss: 16436.078125\n",
      "Train Epoch: 428 [67584/225000 (30%)] Loss: 16742.812500\n",
      "Train Epoch: 428 [70080/225000 (31%)] Loss: 16679.644531\n",
      "Train Epoch: 428 [72576/225000 (32%)] Loss: 16720.925781\n",
      "Train Epoch: 428 [75072/225000 (33%)] Loss: 16558.378906\n",
      "Train Epoch: 428 [77568/225000 (34%)] Loss: 16634.716797\n",
      "Train Epoch: 428 [80064/225000 (36%)] Loss: 16575.644531\n",
      "Train Epoch: 428 [82560/225000 (37%)] Loss: 16529.585938\n",
      "Train Epoch: 428 [85056/225000 (38%)] Loss: 17033.765625\n",
      "Train Epoch: 428 [87552/225000 (39%)] Loss: 16144.291016\n",
      "Train Epoch: 428 [90048/225000 (40%)] Loss: 16727.894531\n",
      "Train Epoch: 428 [92544/225000 (41%)] Loss: 16550.351562\n",
      "Train Epoch: 428 [95040/225000 (42%)] Loss: 17032.835938\n",
      "Train Epoch: 428 [97536/225000 (43%)] Loss: 16594.074219\n",
      "Train Epoch: 428 [100032/225000 (44%)] Loss: 16105.457031\n",
      "Train Epoch: 428 [102528/225000 (46%)] Loss: 16761.015625\n",
      "Train Epoch: 428 [105024/225000 (47%)] Loss: 16843.058594\n",
      "Train Epoch: 428 [107520/225000 (48%)] Loss: 16635.394531\n",
      "Train Epoch: 428 [110016/225000 (49%)] Loss: 16390.015625\n",
      "Train Epoch: 428 [112512/225000 (50%)] Loss: 16766.169922\n",
      "Train Epoch: 428 [115008/225000 (51%)] Loss: 16498.134766\n",
      "Train Epoch: 428 [117504/225000 (52%)] Loss: 16712.480469\n",
      "Train Epoch: 428 [120000/225000 (53%)] Loss: 16674.894531\n",
      "Train Epoch: 428 [122496/225000 (54%)] Loss: 16883.644531\n",
      "Train Epoch: 428 [124992/225000 (56%)] Loss: 18061.625000\n",
      "Train Epoch: 428 [127488/225000 (57%)] Loss: 16805.437500\n",
      "Train Epoch: 428 [129984/225000 (58%)] Loss: 16412.273438\n",
      "Train Epoch: 428 [132480/225000 (59%)] Loss: 16783.371094\n",
      "Train Epoch: 428 [134976/225000 (60%)] Loss: 16593.105469\n",
      "Train Epoch: 428 [137472/225000 (61%)] Loss: 16565.154297\n",
      "Train Epoch: 428 [139968/225000 (62%)] Loss: 16913.517578\n",
      "Train Epoch: 428 [142464/225000 (63%)] Loss: 16619.183594\n",
      "Train Epoch: 428 [144960/225000 (64%)] Loss: 16341.034180\n",
      "Train Epoch: 428 [147456/225000 (66%)] Loss: 16780.726562\n",
      "Train Epoch: 428 [149952/225000 (67%)] Loss: 16509.359375\n",
      "Train Epoch: 428 [152448/225000 (68%)] Loss: 16581.664062\n",
      "Train Epoch: 428 [154944/225000 (69%)] Loss: 16911.304688\n",
      "Train Epoch: 428 [157440/225000 (70%)] Loss: 16390.632812\n",
      "Train Epoch: 428 [159936/225000 (71%)] Loss: 16688.738281\n",
      "Train Epoch: 428 [162432/225000 (72%)] Loss: 16837.585938\n",
      "Train Epoch: 428 [164928/225000 (73%)] Loss: 16567.105469\n",
      "Train Epoch: 428 [167424/225000 (74%)] Loss: 16737.796875\n",
      "Train Epoch: 428 [169920/225000 (76%)] Loss: 18207.339844\n",
      "Train Epoch: 428 [172416/225000 (77%)] Loss: 16670.089844\n",
      "Train Epoch: 428 [174912/225000 (78%)] Loss: 16686.574219\n",
      "Train Epoch: 428 [177408/225000 (79%)] Loss: 16999.699219\n",
      "Train Epoch: 428 [179904/225000 (80%)] Loss: 17156.878906\n",
      "Train Epoch: 428 [182400/225000 (81%)] Loss: 18179.525391\n",
      "Train Epoch: 428 [184896/225000 (82%)] Loss: 16125.546875\n",
      "Train Epoch: 428 [187392/225000 (83%)] Loss: 16523.021484\n",
      "Train Epoch: 428 [189888/225000 (84%)] Loss: 16541.332031\n",
      "Train Epoch: 428 [192384/225000 (86%)] Loss: 18239.011719\n",
      "Train Epoch: 428 [194880/225000 (87%)] Loss: 16228.687500\n",
      "Train Epoch: 428 [197376/225000 (88%)] Loss: 16909.492188\n",
      "Train Epoch: 428 [199872/225000 (89%)] Loss: 16606.136719\n",
      "Train Epoch: 428 [202368/225000 (90%)] Loss: 17134.449219\n",
      "Train Epoch: 428 [204864/225000 (91%)] Loss: 16496.185547\n",
      "Train Epoch: 428 [207360/225000 (92%)] Loss: 16444.765625\n",
      "Train Epoch: 428 [209856/225000 (93%)] Loss: 16574.384766\n",
      "Train Epoch: 428 [212352/225000 (94%)] Loss: 16785.089844\n",
      "Train Epoch: 428 [214848/225000 (95%)] Loss: 16854.539062\n",
      "Train Epoch: 428 [217344/225000 (97%)] Loss: 16889.613281\n",
      "Train Epoch: 428 [219840/225000 (98%)] Loss: 16304.275391\n",
      "Train Epoch: 428 [222336/225000 (99%)] Loss: 16250.917969\n",
      "Train Epoch: 428 [224832/225000 (100%)] Loss: 16270.095703\n",
      "    epoch          : 428\n",
      "    loss           : 16716.25168148731\n",
      "    val_loss       : 16677.991009957917\n",
      "Train Epoch: 429 [192/225000 (0%)] Loss: 16556.994141\n",
      "Train Epoch: 429 [2688/225000 (1%)] Loss: 16687.484375\n",
      "Train Epoch: 429 [5184/225000 (2%)] Loss: 16682.199219\n",
      "Train Epoch: 429 [7680/225000 (3%)] Loss: 16346.267578\n",
      "Train Epoch: 429 [10176/225000 (5%)] Loss: 16305.041992\n",
      "Train Epoch: 429 [12672/225000 (6%)] Loss: 16415.574219\n",
      "Train Epoch: 429 [15168/225000 (7%)] Loss: 17009.208984\n",
      "Train Epoch: 429 [17664/225000 (8%)] Loss: 16926.660156\n",
      "Train Epoch: 429 [20160/225000 (9%)] Loss: 16570.472656\n",
      "Train Epoch: 429 [22656/225000 (10%)] Loss: 16107.060547\n",
      "Train Epoch: 429 [25152/225000 (11%)] Loss: 16238.168945\n",
      "Train Epoch: 429 [27648/225000 (12%)] Loss: 16745.812500\n",
      "Train Epoch: 429 [30144/225000 (13%)] Loss: 16630.074219\n",
      "Train Epoch: 429 [32640/225000 (15%)] Loss: 16721.455078\n",
      "Train Epoch: 429 [35136/225000 (16%)] Loss: 16811.503906\n",
      "Train Epoch: 429 [37632/225000 (17%)] Loss: 16976.375000\n",
      "Train Epoch: 429 [40128/225000 (18%)] Loss: 16596.302734\n",
      "Train Epoch: 429 [42624/225000 (19%)] Loss: 16525.925781\n",
      "Train Epoch: 429 [45120/225000 (20%)] Loss: 16744.304688\n",
      "Train Epoch: 429 [47616/225000 (21%)] Loss: 16601.773438\n",
      "Train Epoch: 429 [50112/225000 (22%)] Loss: 16530.750000\n",
      "Train Epoch: 429 [52608/225000 (23%)] Loss: 16809.390625\n",
      "Train Epoch: 429 [55104/225000 (24%)] Loss: 16531.945312\n",
      "Train Epoch: 429 [57600/225000 (26%)] Loss: 16747.691406\n",
      "Train Epoch: 429 [60096/225000 (27%)] Loss: 16486.480469\n",
      "Train Epoch: 429 [62592/225000 (28%)] Loss: 16481.544922\n",
      "Train Epoch: 429 [65088/225000 (29%)] Loss: 16495.152344\n",
      "Train Epoch: 429 [67584/225000 (30%)] Loss: 16820.169922\n",
      "Train Epoch: 429 [70080/225000 (31%)] Loss: 16700.332031\n",
      "Train Epoch: 429 [72576/225000 (32%)] Loss: 16607.230469\n",
      "Train Epoch: 429 [75072/225000 (33%)] Loss: 16964.464844\n",
      "Train Epoch: 429 [77568/225000 (34%)] Loss: 16852.185547\n",
      "Train Epoch: 429 [80064/225000 (36%)] Loss: 16367.106445\n",
      "Train Epoch: 429 [82560/225000 (37%)] Loss: 16659.513672\n",
      "Train Epoch: 429 [85056/225000 (38%)] Loss: 16559.324219\n",
      "Train Epoch: 429 [87552/225000 (39%)] Loss: 17904.378906\n",
      "Train Epoch: 429 [90048/225000 (40%)] Loss: 16666.345703\n",
      "Train Epoch: 429 [92544/225000 (41%)] Loss: 16623.058594\n",
      "Train Epoch: 429 [95040/225000 (42%)] Loss: 16523.699219\n",
      "Train Epoch: 429 [97536/225000 (43%)] Loss: 16995.460938\n",
      "Train Epoch: 429 [100032/225000 (44%)] Loss: 16751.455078\n",
      "Train Epoch: 429 [102528/225000 (46%)] Loss: 16757.968750\n",
      "Train Epoch: 429 [105024/225000 (47%)] Loss: 16417.753906\n",
      "Train Epoch: 429 [107520/225000 (48%)] Loss: 16637.722656\n",
      "Train Epoch: 429 [110016/225000 (49%)] Loss: 25100.402344\n",
      "Train Epoch: 429 [112512/225000 (50%)] Loss: 16163.581055\n",
      "Train Epoch: 429 [115008/225000 (51%)] Loss: 16511.591797\n",
      "Train Epoch: 429 [117504/225000 (52%)] Loss: 16374.281250\n",
      "Train Epoch: 429 [120000/225000 (53%)] Loss: 16607.750000\n",
      "Train Epoch: 429 [122496/225000 (54%)] Loss: 16370.470703\n",
      "Train Epoch: 429 [124992/225000 (56%)] Loss: 17055.800781\n",
      "Train Epoch: 429 [127488/225000 (57%)] Loss: 16659.511719\n",
      "Train Epoch: 429 [129984/225000 (58%)] Loss: 16648.097656\n",
      "Train Epoch: 429 [132480/225000 (59%)] Loss: 16848.035156\n",
      "Train Epoch: 429 [134976/225000 (60%)] Loss: 16614.871094\n",
      "Train Epoch: 429 [137472/225000 (61%)] Loss: 16882.785156\n",
      "Train Epoch: 429 [139968/225000 (62%)] Loss: 16262.113281\n",
      "Train Epoch: 429 [142464/225000 (63%)] Loss: 16597.167969\n",
      "Train Epoch: 429 [144960/225000 (64%)] Loss: 15908.762695\n",
      "Train Epoch: 429 [147456/225000 (66%)] Loss: 16489.761719\n",
      "Train Epoch: 429 [149952/225000 (67%)] Loss: 16507.583984\n",
      "Train Epoch: 429 [152448/225000 (68%)] Loss: 16448.605469\n",
      "Train Epoch: 429 [154944/225000 (69%)] Loss: 16513.423828\n",
      "Train Epoch: 429 [157440/225000 (70%)] Loss: 16764.464844\n",
      "Train Epoch: 429 [159936/225000 (71%)] Loss: 16455.132812\n",
      "Train Epoch: 429 [162432/225000 (72%)] Loss: 16645.261719\n",
      "Train Epoch: 429 [164928/225000 (73%)] Loss: 16630.851562\n",
      "Train Epoch: 429 [167424/225000 (74%)] Loss: 16805.871094\n",
      "Train Epoch: 429 [169920/225000 (76%)] Loss: 16369.938477\n",
      "Train Epoch: 429 [172416/225000 (77%)] Loss: 17569.621094\n",
      "Train Epoch: 429 [174912/225000 (78%)] Loss: 16656.490234\n",
      "Train Epoch: 429 [177408/225000 (79%)] Loss: 16377.500000\n",
      "Train Epoch: 429 [179904/225000 (80%)] Loss: 16856.070312\n",
      "Train Epoch: 429 [182400/225000 (81%)] Loss: 16979.183594\n",
      "Train Epoch: 429 [184896/225000 (82%)] Loss: 16657.363281\n",
      "Train Epoch: 429 [187392/225000 (83%)] Loss: 16768.572266\n",
      "Train Epoch: 429 [189888/225000 (84%)] Loss: 16496.802734\n",
      "Train Epoch: 429 [192384/225000 (86%)] Loss: 16516.906250\n",
      "Train Epoch: 429 [194880/225000 (87%)] Loss: 16864.984375\n",
      "Train Epoch: 429 [197376/225000 (88%)] Loss: 16708.226562\n",
      "Train Epoch: 429 [199872/225000 (89%)] Loss: 16630.580078\n",
      "Train Epoch: 429 [202368/225000 (90%)] Loss: 16730.230469\n",
      "Train Epoch: 429 [204864/225000 (91%)] Loss: 16575.679688\n",
      "Train Epoch: 429 [207360/225000 (92%)] Loss: 16564.949219\n",
      "Train Epoch: 429 [209856/225000 (93%)] Loss: 17092.701172\n",
      "Train Epoch: 429 [212352/225000 (94%)] Loss: 16604.376953\n",
      "Train Epoch: 429 [214848/225000 (95%)] Loss: 16573.974609\n",
      "Train Epoch: 429 [217344/225000 (97%)] Loss: 24946.302734\n",
      "Train Epoch: 429 [219840/225000 (98%)] Loss: 16981.171875\n",
      "Train Epoch: 429 [222336/225000 (99%)] Loss: 16651.031250\n",
      "Train Epoch: 429 [224832/225000 (100%)] Loss: 18044.628906\n",
      "    epoch          : 429\n",
      "    loss           : 16722.75058160463\n",
      "    val_loss       : 16636.675129121497\n",
      "Train Epoch: 430 [192/225000 (0%)] Loss: 16618.298828\n",
      "Train Epoch: 430 [2688/225000 (1%)] Loss: 16237.001953\n",
      "Train Epoch: 430 [5184/225000 (2%)] Loss: 16344.097656\n",
      "Train Epoch: 430 [7680/225000 (3%)] Loss: 16939.625000\n",
      "Train Epoch: 430 [10176/225000 (5%)] Loss: 17946.707031\n",
      "Train Epoch: 430 [12672/225000 (6%)] Loss: 16355.882812\n",
      "Train Epoch: 430 [15168/225000 (7%)] Loss: 16492.734375\n",
      "Train Epoch: 430 [17664/225000 (8%)] Loss: 16849.539062\n",
      "Train Epoch: 430 [20160/225000 (9%)] Loss: 16630.626953\n",
      "Train Epoch: 430 [22656/225000 (10%)] Loss: 16584.220703\n",
      "Train Epoch: 430 [25152/225000 (11%)] Loss: 16431.617188\n",
      "Train Epoch: 430 [27648/225000 (12%)] Loss: 16793.666016\n",
      "Train Epoch: 430 [30144/225000 (13%)] Loss: 16288.254883\n",
      "Train Epoch: 430 [32640/225000 (15%)] Loss: 16938.328125\n",
      "Train Epoch: 430 [35136/225000 (16%)] Loss: 16514.578125\n",
      "Train Epoch: 430 [37632/225000 (17%)] Loss: 16469.253906\n",
      "Train Epoch: 430 [40128/225000 (18%)] Loss: 16845.167969\n",
      "Train Epoch: 430 [42624/225000 (19%)] Loss: 16822.160156\n",
      "Train Epoch: 430 [45120/225000 (20%)] Loss: 18163.976562\n",
      "Train Epoch: 430 [47616/225000 (21%)] Loss: 17052.898438\n",
      "Train Epoch: 430 [50112/225000 (22%)] Loss: 16386.037109\n",
      "Train Epoch: 430 [52608/225000 (23%)] Loss: 17172.679688\n",
      "Train Epoch: 430 [55104/225000 (24%)] Loss: 17326.609375\n",
      "Train Epoch: 430 [57600/225000 (26%)] Loss: 16734.773438\n",
      "Train Epoch: 430 [60096/225000 (27%)] Loss: 16937.568359\n",
      "Train Epoch: 430 [62592/225000 (28%)] Loss: 16360.385742\n",
      "Train Epoch: 430 [65088/225000 (29%)] Loss: 17083.285156\n",
      "Train Epoch: 430 [67584/225000 (30%)] Loss: 16773.937500\n",
      "Train Epoch: 430 [70080/225000 (31%)] Loss: 16775.546875\n",
      "Train Epoch: 430 [72576/225000 (32%)] Loss: 16671.490234\n",
      "Train Epoch: 430 [75072/225000 (33%)] Loss: 16514.996094\n",
      "Train Epoch: 430 [77568/225000 (34%)] Loss: 16843.898438\n",
      "Train Epoch: 430 [80064/225000 (36%)] Loss: 17990.363281\n",
      "Train Epoch: 430 [82560/225000 (37%)] Loss: 16806.601562\n",
      "Train Epoch: 430 [85056/225000 (38%)] Loss: 16271.324219\n",
      "Train Epoch: 430 [87552/225000 (39%)] Loss: 16518.972656\n",
      "Train Epoch: 430 [90048/225000 (40%)] Loss: 16793.042969\n",
      "Train Epoch: 430 [92544/225000 (41%)] Loss: 19247.267578\n",
      "Train Epoch: 430 [95040/225000 (42%)] Loss: 16520.269531\n",
      "Train Epoch: 430 [97536/225000 (43%)] Loss: 16724.148438\n",
      "Train Epoch: 430 [100032/225000 (44%)] Loss: 16816.468750\n",
      "Train Epoch: 430 [102528/225000 (46%)] Loss: 16723.822266\n",
      "Train Epoch: 430 [105024/225000 (47%)] Loss: 16224.963867\n",
      "Train Epoch: 430 [107520/225000 (48%)] Loss: 16925.714844\n",
      "Train Epoch: 430 [110016/225000 (49%)] Loss: 16442.210938\n",
      "Train Epoch: 430 [112512/225000 (50%)] Loss: 17238.667969\n",
      "Train Epoch: 430 [115008/225000 (51%)] Loss: 16740.773438\n",
      "Train Epoch: 430 [117504/225000 (52%)] Loss: 16314.587891\n",
      "Train Epoch: 430 [120000/225000 (53%)] Loss: 16518.408203\n",
      "Train Epoch: 430 [122496/225000 (54%)] Loss: 16934.023438\n",
      "Train Epoch: 430 [124992/225000 (56%)] Loss: 16848.519531\n",
      "Train Epoch: 430 [127488/225000 (57%)] Loss: 16582.275391\n",
      "Train Epoch: 430 [129984/225000 (58%)] Loss: 16729.136719\n",
      "Train Epoch: 430 [132480/225000 (59%)] Loss: 17979.195312\n",
      "Train Epoch: 430 [134976/225000 (60%)] Loss: 16601.355469\n",
      "Train Epoch: 430 [137472/225000 (61%)] Loss: 16829.058594\n",
      "Train Epoch: 430 [139968/225000 (62%)] Loss: 16747.425781\n",
      "Train Epoch: 430 [142464/225000 (63%)] Loss: 16962.107422\n",
      "Train Epoch: 430 [144960/225000 (64%)] Loss: 16822.910156\n",
      "Train Epoch: 430 [147456/225000 (66%)] Loss: 16593.835938\n",
      "Train Epoch: 430 [149952/225000 (67%)] Loss: 16558.992188\n",
      "Train Epoch: 430 [152448/225000 (68%)] Loss: 16990.863281\n",
      "Train Epoch: 430 [154944/225000 (69%)] Loss: 16978.396484\n",
      "Train Epoch: 430 [157440/225000 (70%)] Loss: 16871.505859\n",
      "Train Epoch: 430 [159936/225000 (71%)] Loss: 16978.970703\n",
      "Train Epoch: 430 [162432/225000 (72%)] Loss: 16466.431641\n",
      "Train Epoch: 430 [164928/225000 (73%)] Loss: 16432.148438\n",
      "Train Epoch: 430 [167424/225000 (74%)] Loss: 16318.208984\n",
      "Train Epoch: 430 [169920/225000 (76%)] Loss: 16605.074219\n",
      "Train Epoch: 430 [172416/225000 (77%)] Loss: 16718.289062\n",
      "Train Epoch: 430 [174912/225000 (78%)] Loss: 16734.796875\n",
      "Train Epoch: 430 [177408/225000 (79%)] Loss: 17082.667969\n",
      "Train Epoch: 430 [179904/225000 (80%)] Loss: 16618.621094\n",
      "Train Epoch: 430 [182400/225000 (81%)] Loss: 16635.894531\n",
      "Train Epoch: 430 [184896/225000 (82%)] Loss: 16721.308594\n",
      "Train Epoch: 430 [187392/225000 (83%)] Loss: 16452.140625\n",
      "Train Epoch: 430 [189888/225000 (84%)] Loss: 16514.894531\n",
      "Train Epoch: 430 [192384/225000 (86%)] Loss: 16624.505859\n",
      "Train Epoch: 430 [194880/225000 (87%)] Loss: 16217.759766\n",
      "Train Epoch: 430 [197376/225000 (88%)] Loss: 18411.048828\n",
      "Train Epoch: 430 [199872/225000 (89%)] Loss: 16708.625000\n",
      "Train Epoch: 430 [202368/225000 (90%)] Loss: 16402.605469\n",
      "Train Epoch: 430 [204864/225000 (91%)] Loss: 16675.939453\n",
      "Train Epoch: 430 [207360/225000 (92%)] Loss: 16455.296875\n",
      "Train Epoch: 430 [209856/225000 (93%)] Loss: 16369.018555\n",
      "Train Epoch: 430 [212352/225000 (94%)] Loss: 17018.570312\n",
      "Train Epoch: 430 [214848/225000 (95%)] Loss: 16705.232422\n",
      "Train Epoch: 430 [217344/225000 (97%)] Loss: 16272.305664\n",
      "Train Epoch: 430 [219840/225000 (98%)] Loss: 16818.316406\n",
      "Train Epoch: 430 [222336/225000 (99%)] Loss: 16871.625000\n",
      "Train Epoch: 430 [224832/225000 (100%)] Loss: 16144.227539\n",
      "    epoch          : 430\n",
      "    loss           : 16717.121049588044\n",
      "    val_loss       : 16632.88999579335\n",
      "Train Epoch: 431 [192/225000 (0%)] Loss: 16625.062500\n",
      "Train Epoch: 431 [2688/225000 (1%)] Loss: 17903.853516\n",
      "Train Epoch: 431 [5184/225000 (2%)] Loss: 16722.115234\n",
      "Train Epoch: 431 [7680/225000 (3%)] Loss: 16771.691406\n",
      "Train Epoch: 431 [10176/225000 (5%)] Loss: 16540.871094\n",
      "Train Epoch: 431 [12672/225000 (6%)] Loss: 17317.433594\n",
      "Train Epoch: 431 [15168/225000 (7%)] Loss: 16764.437500\n",
      "Train Epoch: 431 [17664/225000 (8%)] Loss: 17002.099609\n",
      "Train Epoch: 431 [20160/225000 (9%)] Loss: 16431.871094\n",
      "Train Epoch: 431 [22656/225000 (10%)] Loss: 16719.988281\n",
      "Train Epoch: 431 [25152/225000 (11%)] Loss: 16957.560547\n",
      "Train Epoch: 431 [27648/225000 (12%)] Loss: 16321.337891\n",
      "Train Epoch: 431 [30144/225000 (13%)] Loss: 16544.808594\n",
      "Train Epoch: 431 [32640/225000 (15%)] Loss: 16788.111328\n",
      "Train Epoch: 431 [35136/225000 (16%)] Loss: 16373.609375\n",
      "Train Epoch: 431 [37632/225000 (17%)] Loss: 16902.998047\n",
      "Train Epoch: 431 [40128/225000 (18%)] Loss: 16468.197266\n",
      "Train Epoch: 431 [42624/225000 (19%)] Loss: 16749.265625\n",
      "Train Epoch: 431 [45120/225000 (20%)] Loss: 16824.882812\n",
      "Train Epoch: 431 [47616/225000 (21%)] Loss: 16713.031250\n",
      "Train Epoch: 431 [50112/225000 (22%)] Loss: 16853.058594\n",
      "Train Epoch: 431 [52608/225000 (23%)] Loss: 16566.134766\n",
      "Train Epoch: 431 [55104/225000 (24%)] Loss: 16063.899414\n",
      "Train Epoch: 431 [57600/225000 (26%)] Loss: 16407.750000\n",
      "Train Epoch: 431 [60096/225000 (27%)] Loss: 16435.761719\n",
      "Train Epoch: 431 [62592/225000 (28%)] Loss: 16619.402344\n",
      "Train Epoch: 431 [65088/225000 (29%)] Loss: 16618.011719\n",
      "Train Epoch: 431 [67584/225000 (30%)] Loss: 16997.900391\n",
      "Train Epoch: 431 [70080/225000 (31%)] Loss: 15891.837891\n",
      "Train Epoch: 431 [72576/225000 (32%)] Loss: 16902.128906\n",
      "Train Epoch: 431 [75072/225000 (33%)] Loss: 16912.296875\n",
      "Train Epoch: 431 [77568/225000 (34%)] Loss: 16588.843750\n",
      "Train Epoch: 431 [80064/225000 (36%)] Loss: 16550.613281\n",
      "Train Epoch: 431 [82560/225000 (37%)] Loss: 16854.675781\n",
      "Train Epoch: 431 [85056/225000 (38%)] Loss: 16951.167969\n",
      "Train Epoch: 431 [87552/225000 (39%)] Loss: 16582.867188\n",
      "Train Epoch: 431 [90048/225000 (40%)] Loss: 16527.064453\n",
      "Train Epoch: 431 [92544/225000 (41%)] Loss: 16351.645508\n",
      "Train Epoch: 431 [95040/225000 (42%)] Loss: 16949.916016\n",
      "Train Epoch: 431 [97536/225000 (43%)] Loss: 16365.840820\n",
      "Train Epoch: 431 [100032/225000 (44%)] Loss: 16273.288086\n",
      "Train Epoch: 431 [102528/225000 (46%)] Loss: 16844.929688\n",
      "Train Epoch: 431 [105024/225000 (47%)] Loss: 16580.890625\n",
      "Train Epoch: 431 [107520/225000 (48%)] Loss: 17073.716797\n",
      "Train Epoch: 431 [110016/225000 (49%)] Loss: 16470.082031\n",
      "Train Epoch: 431 [112512/225000 (50%)] Loss: 17043.933594\n",
      "Train Epoch: 431 [115008/225000 (51%)] Loss: 16654.960938\n",
      "Train Epoch: 431 [117504/225000 (52%)] Loss: 16205.198242\n",
      "Train Epoch: 431 [120000/225000 (53%)] Loss: 17023.298828\n",
      "Train Epoch: 431 [122496/225000 (54%)] Loss: 16514.927734\n",
      "Train Epoch: 431 [124992/225000 (56%)] Loss: 16539.394531\n",
      "Train Epoch: 431 [127488/225000 (57%)] Loss: 16712.394531\n",
      "Train Epoch: 431 [129984/225000 (58%)] Loss: 16781.804688\n",
      "Train Epoch: 431 [132480/225000 (59%)] Loss: 16464.742188\n",
      "Train Epoch: 431 [134976/225000 (60%)] Loss: 16526.359375\n",
      "Train Epoch: 431 [137472/225000 (61%)] Loss: 16381.980469\n",
      "Train Epoch: 431 [139968/225000 (62%)] Loss: 17758.042969\n",
      "Train Epoch: 431 [142464/225000 (63%)] Loss: 16757.832031\n",
      "Train Epoch: 431 [144960/225000 (64%)] Loss: 16668.578125\n",
      "Train Epoch: 431 [147456/225000 (66%)] Loss: 16944.535156\n",
      "Train Epoch: 431 [149952/225000 (67%)] Loss: 16466.917969\n",
      "Train Epoch: 431 [152448/225000 (68%)] Loss: 16565.880859\n",
      "Train Epoch: 431 [154944/225000 (69%)] Loss: 16445.111328\n",
      "Train Epoch: 431 [157440/225000 (70%)] Loss: 16881.597656\n",
      "Train Epoch: 431 [159936/225000 (71%)] Loss: 16365.085938\n",
      "Train Epoch: 431 [162432/225000 (72%)] Loss: 16219.527344\n",
      "Train Epoch: 431 [164928/225000 (73%)] Loss: 16482.908203\n",
      "Train Epoch: 431 [167424/225000 (74%)] Loss: 17167.064453\n",
      "Train Epoch: 431 [169920/225000 (76%)] Loss: 17142.990234\n",
      "Train Epoch: 431 [172416/225000 (77%)] Loss: 16422.781250\n",
      "Train Epoch: 431 [174912/225000 (78%)] Loss: 16681.378906\n",
      "Train Epoch: 431 [177408/225000 (79%)] Loss: 16588.050781\n",
      "Train Epoch: 431 [179904/225000 (80%)] Loss: 16653.968750\n",
      "Train Epoch: 431 [182400/225000 (81%)] Loss: 16652.097656\n",
      "Train Epoch: 431 [184896/225000 (82%)] Loss: 16845.849609\n",
      "Train Epoch: 431 [187392/225000 (83%)] Loss: 16957.001953\n",
      "Train Epoch: 431 [189888/225000 (84%)] Loss: 16732.152344\n",
      "Train Epoch: 431 [192384/225000 (86%)] Loss: 16783.107422\n",
      "Train Epoch: 431 [194880/225000 (87%)] Loss: 16160.426758\n",
      "Train Epoch: 431 [197376/225000 (88%)] Loss: 16458.121094\n",
      "Train Epoch: 431 [199872/225000 (89%)] Loss: 18230.234375\n",
      "Train Epoch: 431 [202368/225000 (90%)] Loss: 17045.480469\n",
      "Train Epoch: 431 [204864/225000 (91%)] Loss: 16794.023438\n",
      "Train Epoch: 431 [207360/225000 (92%)] Loss: 16535.714844\n",
      "Train Epoch: 431 [209856/225000 (93%)] Loss: 16947.050781\n",
      "Train Epoch: 431 [212352/225000 (94%)] Loss: 17715.214844\n",
      "Train Epoch: 431 [214848/225000 (95%)] Loss: 16820.859375\n",
      "Train Epoch: 431 [217344/225000 (97%)] Loss: 16490.820312\n",
      "Train Epoch: 431 [219840/225000 (98%)] Loss: 15983.045898\n",
      "Train Epoch: 431 [222336/225000 (99%)] Loss: 16192.400391\n",
      "Train Epoch: 431 [224832/225000 (100%)] Loss: 16295.939453\n",
      "    epoch          : 431\n",
      "    loss           : 16710.872541095618\n",
      "    val_loss       : 16700.663058517544\n",
      "Train Epoch: 432 [192/225000 (0%)] Loss: 16692.355469\n",
      "Train Epoch: 432 [2688/225000 (1%)] Loss: 16820.960938\n",
      "Train Epoch: 432 [5184/225000 (2%)] Loss: 16327.937500\n",
      "Train Epoch: 432 [7680/225000 (3%)] Loss: 16721.878906\n",
      "Train Epoch: 432 [10176/225000 (5%)] Loss: 16540.011719\n",
      "Train Epoch: 432 [12672/225000 (6%)] Loss: 17241.978516\n",
      "Train Epoch: 432 [15168/225000 (7%)] Loss: 16515.609375\n",
      "Train Epoch: 432 [17664/225000 (8%)] Loss: 16417.871094\n",
      "Train Epoch: 432 [20160/225000 (9%)] Loss: 16970.378906\n",
      "Train Epoch: 432 [22656/225000 (10%)] Loss: 16437.980469\n",
      "Train Epoch: 432 [25152/225000 (11%)] Loss: 16812.699219\n",
      "Train Epoch: 432 [27648/225000 (12%)] Loss: 16653.476562\n",
      "Train Epoch: 432 [30144/225000 (13%)] Loss: 16279.164062\n",
      "Train Epoch: 432 [32640/225000 (15%)] Loss: 16472.076172\n",
      "Train Epoch: 432 [35136/225000 (16%)] Loss: 16544.062500\n",
      "Train Epoch: 432 [37632/225000 (17%)] Loss: 15872.184570\n",
      "Train Epoch: 432 [40128/225000 (18%)] Loss: 16684.789062\n",
      "Train Epoch: 432 [42624/225000 (19%)] Loss: 16683.835938\n",
      "Train Epoch: 432 [45120/225000 (20%)] Loss: 17056.257812\n",
      "Train Epoch: 432 [47616/225000 (21%)] Loss: 16770.171875\n",
      "Train Epoch: 432 [50112/225000 (22%)] Loss: 16751.339844\n",
      "Train Epoch: 432 [52608/225000 (23%)] Loss: 15776.196289\n",
      "Train Epoch: 432 [55104/225000 (24%)] Loss: 16110.645508\n",
      "Train Epoch: 432 [57600/225000 (26%)] Loss: 16177.485352\n",
      "Train Epoch: 432 [60096/225000 (27%)] Loss: 18341.925781\n",
      "Train Epoch: 432 [62592/225000 (28%)] Loss: 16381.737305\n",
      "Train Epoch: 432 [65088/225000 (29%)] Loss: 16898.121094\n",
      "Train Epoch: 432 [67584/225000 (30%)] Loss: 16582.998047\n",
      "Train Epoch: 432 [70080/225000 (31%)] Loss: 16831.078125\n",
      "Train Epoch: 432 [72576/225000 (32%)] Loss: 17244.609375\n",
      "Train Epoch: 432 [75072/225000 (33%)] Loss: 16840.765625\n",
      "Train Epoch: 432 [77568/225000 (34%)] Loss: 16461.197266\n",
      "Train Epoch: 432 [80064/225000 (36%)] Loss: 16594.074219\n",
      "Train Epoch: 432 [82560/225000 (37%)] Loss: 16455.968750\n",
      "Train Epoch: 432 [85056/225000 (38%)] Loss: 16390.671875\n",
      "Train Epoch: 432 [87552/225000 (39%)] Loss: 17129.896484\n",
      "Train Epoch: 432 [90048/225000 (40%)] Loss: 16643.164062\n",
      "Train Epoch: 432 [92544/225000 (41%)] Loss: 16141.448242\n",
      "Train Epoch: 432 [95040/225000 (42%)] Loss: 16514.105469\n",
      "Train Epoch: 432 [97536/225000 (43%)] Loss: 16293.062500\n",
      "Train Epoch: 432 [100032/225000 (44%)] Loss: 16444.812500\n",
      "Train Epoch: 432 [102528/225000 (46%)] Loss: 17244.455078\n",
      "Train Epoch: 432 [105024/225000 (47%)] Loss: 16484.923828\n",
      "Train Epoch: 432 [107520/225000 (48%)] Loss: 16554.875000\n",
      "Train Epoch: 432 [110016/225000 (49%)] Loss: 16313.521484\n",
      "Train Epoch: 432 [112512/225000 (50%)] Loss: 16457.787109\n",
      "Train Epoch: 432 [115008/225000 (51%)] Loss: 16826.167969\n",
      "Train Epoch: 432 [117504/225000 (52%)] Loss: 16926.949219\n",
      "Train Epoch: 432 [120000/225000 (53%)] Loss: 16161.259766\n",
      "Train Epoch: 432 [122496/225000 (54%)] Loss: 16660.441406\n",
      "Train Epoch: 432 [124992/225000 (56%)] Loss: 16487.599609\n",
      "Train Epoch: 432 [127488/225000 (57%)] Loss: 16715.539062\n",
      "Train Epoch: 432 [129984/225000 (58%)] Loss: 16598.675781\n",
      "Train Epoch: 432 [132480/225000 (59%)] Loss: 16752.367188\n",
      "Train Epoch: 432 [134976/225000 (60%)] Loss: 16717.837891\n",
      "Train Epoch: 432 [137472/225000 (61%)] Loss: 16924.437500\n",
      "Train Epoch: 432 [139968/225000 (62%)] Loss: 16444.683594\n",
      "Train Epoch: 432 [142464/225000 (63%)] Loss: 16900.974609\n",
      "Train Epoch: 432 [144960/225000 (64%)] Loss: 17130.267578\n",
      "Train Epoch: 432 [147456/225000 (66%)] Loss: 16711.835938\n",
      "Train Epoch: 432 [149952/225000 (67%)] Loss: 16411.800781\n",
      "Train Epoch: 432 [152448/225000 (68%)] Loss: 16622.078125\n",
      "Train Epoch: 432 [154944/225000 (69%)] Loss: 16508.806641\n",
      "Train Epoch: 432 [157440/225000 (70%)] Loss: 16227.184570\n",
      "Train Epoch: 432 [159936/225000 (71%)] Loss: 16808.933594\n",
      "Train Epoch: 432 [162432/225000 (72%)] Loss: 17021.910156\n",
      "Train Epoch: 432 [164928/225000 (73%)] Loss: 16820.707031\n",
      "Train Epoch: 432 [167424/225000 (74%)] Loss: 16398.902344\n",
      "Train Epoch: 432 [169920/225000 (76%)] Loss: 16842.515625\n",
      "Train Epoch: 432 [172416/225000 (77%)] Loss: 16616.753906\n",
      "Train Epoch: 432 [174912/225000 (78%)] Loss: 16961.527344\n",
      "Train Epoch: 432 [177408/225000 (79%)] Loss: 16734.763672\n",
      "Train Epoch: 432 [179904/225000 (80%)] Loss: 16582.828125\n",
      "Train Epoch: 432 [182400/225000 (81%)] Loss: 16551.310547\n",
      "Train Epoch: 432 [184896/225000 (82%)] Loss: 17058.042969\n",
      "Train Epoch: 432 [187392/225000 (83%)] Loss: 17874.714844\n",
      "Train Epoch: 432 [189888/225000 (84%)] Loss: 16924.197266\n",
      "Train Epoch: 432 [192384/225000 (86%)] Loss: 16560.519531\n",
      "Train Epoch: 432 [194880/225000 (87%)] Loss: 16838.394531\n",
      "Train Epoch: 432 [197376/225000 (88%)] Loss: 16274.480469\n",
      "Train Epoch: 432 [199872/225000 (89%)] Loss: 16263.655273\n",
      "Train Epoch: 432 [202368/225000 (90%)] Loss: 16238.281250\n",
      "Train Epoch: 432 [204864/225000 (91%)] Loss: 16827.093750\n",
      "Train Epoch: 432 [207360/225000 (92%)] Loss: 16523.710938\n",
      "Train Epoch: 432 [209856/225000 (93%)] Loss: 16794.769531\n",
      "Train Epoch: 432 [212352/225000 (94%)] Loss: 16643.820312\n",
      "Train Epoch: 432 [214848/225000 (95%)] Loss: 16573.343750\n",
      "Train Epoch: 432 [217344/225000 (97%)] Loss: 16678.789062\n",
      "Train Epoch: 432 [219840/225000 (98%)] Loss: 16590.410156\n",
      "Train Epoch: 432 [222336/225000 (99%)] Loss: 16844.634766\n",
      "Train Epoch: 432 [224832/225000 (100%)] Loss: 16539.503906\n",
      "    epoch          : 432\n",
      "    loss           : 16721.996943659342\n",
      "    val_loss       : 16682.875037412607\n",
      "Train Epoch: 433 [192/225000 (0%)] Loss: 17064.277344\n",
      "Train Epoch: 433 [2688/225000 (1%)] Loss: 17017.445312\n",
      "Train Epoch: 433 [5184/225000 (2%)] Loss: 16597.515625\n",
      "Train Epoch: 433 [7680/225000 (3%)] Loss: 16295.936523\n",
      "Train Epoch: 433 [10176/225000 (5%)] Loss: 16693.060547\n",
      "Train Epoch: 433 [12672/225000 (6%)] Loss: 17006.876953\n",
      "Train Epoch: 433 [15168/225000 (7%)] Loss: 16221.930664\n",
      "Train Epoch: 433 [17664/225000 (8%)] Loss: 16673.441406\n",
      "Train Epoch: 433 [20160/225000 (9%)] Loss: 16448.898438\n",
      "Train Epoch: 433 [22656/225000 (10%)] Loss: 16626.750000\n",
      "Train Epoch: 433 [25152/225000 (11%)] Loss: 16444.009766\n",
      "Train Epoch: 433 [27648/225000 (12%)] Loss: 18455.203125\n",
      "Train Epoch: 433 [30144/225000 (13%)] Loss: 16731.148438\n",
      "Train Epoch: 433 [32640/225000 (15%)] Loss: 16646.921875\n",
      "Train Epoch: 433 [35136/225000 (16%)] Loss: 16630.988281\n",
      "Train Epoch: 433 [37632/225000 (17%)] Loss: 17003.988281\n",
      "Train Epoch: 433 [40128/225000 (18%)] Loss: 16883.738281\n",
      "Train Epoch: 433 [42624/225000 (19%)] Loss: 18256.224609\n",
      "Train Epoch: 433 [45120/225000 (20%)] Loss: 16691.957031\n",
      "Train Epoch: 433 [47616/225000 (21%)] Loss: 16512.679688\n",
      "Train Epoch: 433 [50112/225000 (22%)] Loss: 16659.716797\n",
      "Train Epoch: 433 [52608/225000 (23%)] Loss: 16378.268555\n",
      "Train Epoch: 433 [55104/225000 (24%)] Loss: 16703.199219\n",
      "Train Epoch: 433 [57600/225000 (26%)] Loss: 16489.484375\n",
      "Train Epoch: 433 [60096/225000 (27%)] Loss: 16492.187500\n",
      "Train Epoch: 433 [62592/225000 (28%)] Loss: 16607.291016\n",
      "Train Epoch: 433 [65088/225000 (29%)] Loss: 16558.765625\n",
      "Train Epoch: 433 [67584/225000 (30%)] Loss: 16822.050781\n",
      "Train Epoch: 433 [70080/225000 (31%)] Loss: 17018.664062\n",
      "Train Epoch: 433 [72576/225000 (32%)] Loss: 17121.042969\n",
      "Train Epoch: 433 [75072/225000 (33%)] Loss: 17100.488281\n",
      "Train Epoch: 433 [77568/225000 (34%)] Loss: 16428.328125\n",
      "Train Epoch: 433 [80064/225000 (36%)] Loss: 16801.845703\n",
      "Train Epoch: 433 [82560/225000 (37%)] Loss: 16694.570312\n",
      "Train Epoch: 433 [85056/225000 (38%)] Loss: 16753.498047\n",
      "Train Epoch: 433 [87552/225000 (39%)] Loss: 16354.727539\n",
      "Train Epoch: 433 [90048/225000 (40%)] Loss: 16712.589844\n",
      "Train Epoch: 433 [92544/225000 (41%)] Loss: 16234.213867\n",
      "Train Epoch: 433 [95040/225000 (42%)] Loss: 16871.236328\n",
      "Train Epoch: 433 [97536/225000 (43%)] Loss: 16559.285156\n",
      "Train Epoch: 433 [100032/225000 (44%)] Loss: 16753.929688\n",
      "Train Epoch: 433 [102528/225000 (46%)] Loss: 16887.085938\n",
      "Train Epoch: 433 [105024/225000 (47%)] Loss: 16423.945312\n",
      "Train Epoch: 433 [107520/225000 (48%)] Loss: 16634.789062\n",
      "Train Epoch: 433 [110016/225000 (49%)] Loss: 16835.304688\n",
      "Train Epoch: 433 [112512/225000 (50%)] Loss: 16652.796875\n",
      "Train Epoch: 433 [115008/225000 (51%)] Loss: 16360.466797\n",
      "Train Epoch: 433 [117504/225000 (52%)] Loss: 16496.544922\n",
      "Train Epoch: 433 [120000/225000 (53%)] Loss: 16628.121094\n",
      "Train Epoch: 433 [122496/225000 (54%)] Loss: 16387.660156\n",
      "Train Epoch: 433 [124992/225000 (56%)] Loss: 16736.490234\n",
      "Train Epoch: 433 [127488/225000 (57%)] Loss: 16309.853516\n",
      "Train Epoch: 433 [129984/225000 (58%)] Loss: 16577.621094\n",
      "Train Epoch: 433 [132480/225000 (59%)] Loss: 16524.234375\n",
      "Train Epoch: 433 [134976/225000 (60%)] Loss: 16983.835938\n",
      "Train Epoch: 433 [137472/225000 (61%)] Loss: 16969.337891\n",
      "Train Epoch: 433 [139968/225000 (62%)] Loss: 17477.871094\n",
      "Train Epoch: 433 [142464/225000 (63%)] Loss: 16413.667969\n",
      "Train Epoch: 433 [144960/225000 (64%)] Loss: 16606.505859\n",
      "Train Epoch: 433 [147456/225000 (66%)] Loss: 16543.496094\n",
      "Train Epoch: 433 [149952/225000 (67%)] Loss: 16594.441406\n",
      "Train Epoch: 433 [152448/225000 (68%)] Loss: 16755.847656\n",
      "Train Epoch: 433 [154944/225000 (69%)] Loss: 16352.026367\n",
      "Train Epoch: 433 [157440/225000 (70%)] Loss: 16458.847656\n",
      "Train Epoch: 433 [159936/225000 (71%)] Loss: 16737.226562\n",
      "Train Epoch: 433 [162432/225000 (72%)] Loss: 16068.508789\n",
      "Train Epoch: 433 [164928/225000 (73%)] Loss: 18314.951172\n",
      "Train Epoch: 433 [167424/225000 (74%)] Loss: 16620.869141\n",
      "Train Epoch: 433 [169920/225000 (76%)] Loss: 16160.308594\n",
      "Train Epoch: 433 [172416/225000 (77%)] Loss: 16442.531250\n",
      "Train Epoch: 433 [174912/225000 (78%)] Loss: 16498.050781\n",
      "Train Epoch: 433 [177408/225000 (79%)] Loss: 16662.357422\n",
      "Train Epoch: 433 [179904/225000 (80%)] Loss: 16910.308594\n",
      "Train Epoch: 433 [182400/225000 (81%)] Loss: 16593.208984\n",
      "Train Epoch: 433 [184896/225000 (82%)] Loss: 16324.129883\n",
      "Train Epoch: 433 [187392/225000 (83%)] Loss: 16450.925781\n",
      "Train Epoch: 433 [189888/225000 (84%)] Loss: 16339.712891\n",
      "Train Epoch: 433 [192384/225000 (86%)] Loss: 16346.032227\n",
      "Train Epoch: 433 [194880/225000 (87%)] Loss: 16280.714844\n",
      "Train Epoch: 433 [197376/225000 (88%)] Loss: 16832.876953\n",
      "Train Epoch: 433 [199872/225000 (89%)] Loss: 16830.304688\n",
      "Train Epoch: 433 [202368/225000 (90%)] Loss: 16809.835938\n",
      "Train Epoch: 433 [204864/225000 (91%)] Loss: 18225.892578\n",
      "Train Epoch: 433 [207360/225000 (92%)] Loss: 16223.903320\n",
      "Train Epoch: 433 [209856/225000 (93%)] Loss: 16650.386719\n",
      "Train Epoch: 433 [212352/225000 (94%)] Loss: 16714.255859\n",
      "Train Epoch: 433 [214848/225000 (95%)] Loss: 17058.556641\n",
      "Train Epoch: 433 [217344/225000 (97%)] Loss: 17176.509766\n",
      "Train Epoch: 433 [219840/225000 (98%)] Loss: 16419.972656\n",
      "Train Epoch: 433 [222336/225000 (99%)] Loss: 16924.832031\n",
      "Train Epoch: 433 [224832/225000 (100%)] Loss: 16463.205078\n",
      "    epoch          : 433\n",
      "    loss           : 16712.39995317166\n",
      "    val_loss       : 16683.44451203601\n",
      "Train Epoch: 434 [192/225000 (0%)] Loss: 16607.974609\n",
      "Train Epoch: 434 [2688/225000 (1%)] Loss: 16555.496094\n",
      "Train Epoch: 434 [5184/225000 (2%)] Loss: 16154.704102\n",
      "Train Epoch: 434 [7680/225000 (3%)] Loss: 16946.437500\n",
      "Train Epoch: 434 [10176/225000 (5%)] Loss: 16429.054688\n",
      "Train Epoch: 434 [12672/225000 (6%)] Loss: 16725.320312\n",
      "Train Epoch: 434 [15168/225000 (7%)] Loss: 16515.058594\n",
      "Train Epoch: 434 [17664/225000 (8%)] Loss: 16469.789062\n",
      "Train Epoch: 434 [20160/225000 (9%)] Loss: 16686.345703\n",
      "Train Epoch: 434 [22656/225000 (10%)] Loss: 16759.988281\n",
      "Train Epoch: 434 [25152/225000 (11%)] Loss: 16903.386719\n",
      "Train Epoch: 434 [27648/225000 (12%)] Loss: 16042.324219\n",
      "Train Epoch: 434 [30144/225000 (13%)] Loss: 18537.246094\n",
      "Train Epoch: 434 [32640/225000 (15%)] Loss: 16711.074219\n",
      "Train Epoch: 434 [35136/225000 (16%)] Loss: 16441.054688\n",
      "Train Epoch: 434 [37632/225000 (17%)] Loss: 17085.113281\n",
      "Train Epoch: 434 [40128/225000 (18%)] Loss: 16626.861328\n",
      "Train Epoch: 434 [42624/225000 (19%)] Loss: 15993.947266\n",
      "Train Epoch: 434 [45120/225000 (20%)] Loss: 16557.730469\n",
      "Train Epoch: 434 [47616/225000 (21%)] Loss: 16953.726562\n",
      "Train Epoch: 434 [50112/225000 (22%)] Loss: 16685.742188\n",
      "Train Epoch: 434 [52608/225000 (23%)] Loss: 16644.121094\n",
      "Train Epoch: 434 [55104/225000 (24%)] Loss: 17219.978516\n",
      "Train Epoch: 434 [57600/225000 (26%)] Loss: 16906.570312\n",
      "Train Epoch: 434 [60096/225000 (27%)] Loss: 16781.224609\n",
      "Train Epoch: 434 [62592/225000 (28%)] Loss: 16746.890625\n",
      "Train Epoch: 434 [65088/225000 (29%)] Loss: 16585.714844\n",
      "Train Epoch: 434 [67584/225000 (30%)] Loss: 16797.484375\n",
      "Train Epoch: 434 [70080/225000 (31%)] Loss: 16686.441406\n",
      "Train Epoch: 434 [72576/225000 (32%)] Loss: 16618.199219\n",
      "Train Epoch: 434 [75072/225000 (33%)] Loss: 18211.453125\n",
      "Train Epoch: 434 [77568/225000 (34%)] Loss: 16940.460938\n",
      "Train Epoch: 434 [80064/225000 (36%)] Loss: 17064.134766\n",
      "Train Epoch: 434 [82560/225000 (37%)] Loss: 16558.695312\n",
      "Train Epoch: 434 [85056/225000 (38%)] Loss: 16127.536133\n",
      "Train Epoch: 434 [87552/225000 (39%)] Loss: 16512.578125\n",
      "Train Epoch: 434 [90048/225000 (40%)] Loss: 16326.370117\n",
      "Train Epoch: 434 [92544/225000 (41%)] Loss: 16606.308594\n",
      "Train Epoch: 434 [95040/225000 (42%)] Loss: 16291.056641\n",
      "Train Epoch: 434 [97536/225000 (43%)] Loss: 16604.832031\n",
      "Train Epoch: 434 [100032/225000 (44%)] Loss: 16596.039062\n",
      "Train Epoch: 434 [102528/225000 (46%)] Loss: 16481.474609\n",
      "Train Epoch: 434 [105024/225000 (47%)] Loss: 16690.869141\n",
      "Train Epoch: 434 [107520/225000 (48%)] Loss: 16452.519531\n",
      "Train Epoch: 434 [110016/225000 (49%)] Loss: 16783.714844\n",
      "Train Epoch: 434 [112512/225000 (50%)] Loss: 16508.800781\n",
      "Train Epoch: 434 [115008/225000 (51%)] Loss: 16551.136719\n",
      "Train Epoch: 434 [117504/225000 (52%)] Loss: 16661.691406\n",
      "Train Epoch: 434 [120000/225000 (53%)] Loss: 16471.601562\n",
      "Train Epoch: 434 [122496/225000 (54%)] Loss: 16767.609375\n",
      "Train Epoch: 434 [124992/225000 (56%)] Loss: 16368.427734\n",
      "Train Epoch: 434 [127488/225000 (57%)] Loss: 17080.777344\n",
      "Train Epoch: 434 [129984/225000 (58%)] Loss: 16397.111328\n",
      "Train Epoch: 434 [132480/225000 (59%)] Loss: 16659.597656\n",
      "Train Epoch: 434 [134976/225000 (60%)] Loss: 16822.113281\n",
      "Train Epoch: 434 [137472/225000 (61%)] Loss: 16951.507812\n",
      "Train Epoch: 434 [139968/225000 (62%)] Loss: 16631.578125\n",
      "Train Epoch: 434 [142464/225000 (63%)] Loss: 16796.394531\n",
      "Train Epoch: 434 [144960/225000 (64%)] Loss: 16461.667969\n",
      "Train Epoch: 434 [147456/225000 (66%)] Loss: 16458.863281\n",
      "Train Epoch: 434 [149952/225000 (67%)] Loss: 18500.146484\n",
      "Train Epoch: 434 [152448/225000 (68%)] Loss: 16940.916016\n",
      "Train Epoch: 434 [154944/225000 (69%)] Loss: 16511.724609\n",
      "Train Epoch: 434 [157440/225000 (70%)] Loss: 16562.703125\n",
      "Train Epoch: 434 [159936/225000 (71%)] Loss: 16594.896484\n",
      "Train Epoch: 434 [162432/225000 (72%)] Loss: 16636.382812\n",
      "Train Epoch: 434 [164928/225000 (73%)] Loss: 16765.863281\n",
      "Train Epoch: 434 [167424/225000 (74%)] Loss: 16825.187500\n",
      "Train Epoch: 434 [169920/225000 (76%)] Loss: 16594.921875\n",
      "Train Epoch: 434 [172416/225000 (77%)] Loss: 16647.070312\n",
      "Train Epoch: 434 [174912/225000 (78%)] Loss: 16346.595703\n",
      "Train Epoch: 434 [177408/225000 (79%)] Loss: 16119.487305\n",
      "Train Epoch: 434 [179904/225000 (80%)] Loss: 16483.253906\n",
      "Train Epoch: 434 [182400/225000 (81%)] Loss: 16924.738281\n",
      "Train Epoch: 434 [184896/225000 (82%)] Loss: 16247.721680\n",
      "Train Epoch: 434 [187392/225000 (83%)] Loss: 16326.617188\n",
      "Train Epoch: 434 [189888/225000 (84%)] Loss: 16509.773438\n",
      "Train Epoch: 434 [192384/225000 (86%)] Loss: 16834.150391\n",
      "Train Epoch: 434 [194880/225000 (87%)] Loss: 16595.574219\n",
      "Train Epoch: 434 [197376/225000 (88%)] Loss: 17019.453125\n",
      "Train Epoch: 434 [199872/225000 (89%)] Loss: 16867.917969\n",
      "Train Epoch: 434 [202368/225000 (90%)] Loss: 16699.214844\n",
      "Train Epoch: 434 [204864/225000 (91%)] Loss: 16222.385742\n",
      "Train Epoch: 434 [207360/225000 (92%)] Loss: 16628.203125\n",
      "Train Epoch: 434 [209856/225000 (93%)] Loss: 16149.416016\n",
      "Train Epoch: 434 [212352/225000 (94%)] Loss: 17308.423828\n",
      "Train Epoch: 434 [214848/225000 (95%)] Loss: 16556.343750\n",
      "Train Epoch: 434 [217344/225000 (97%)] Loss: 16518.240234\n",
      "Train Epoch: 434 [219840/225000 (98%)] Loss: 16633.582031\n",
      "Train Epoch: 434 [222336/225000 (99%)] Loss: 16339.501953\n",
      "Train Epoch: 434 [224832/225000 (100%)] Loss: 16977.656250\n",
      "    epoch          : 434\n",
      "    loss           : 16719.243048241522\n",
      "    val_loss       : 16642.7486427531\n",
      "Train Epoch: 435 [192/225000 (0%)] Loss: 17163.128906\n",
      "Train Epoch: 435 [2688/225000 (1%)] Loss: 16190.294922\n",
      "Train Epoch: 435 [5184/225000 (2%)] Loss: 16800.818359\n",
      "Train Epoch: 435 [7680/225000 (3%)] Loss: 16588.667969\n",
      "Train Epoch: 435 [10176/225000 (5%)] Loss: 16824.312500\n",
      "Train Epoch: 435 [12672/225000 (6%)] Loss: 16714.437500\n",
      "Train Epoch: 435 [15168/225000 (7%)] Loss: 16814.890625\n",
      "Train Epoch: 435 [17664/225000 (8%)] Loss: 16684.582031\n",
      "Train Epoch: 435 [20160/225000 (9%)] Loss: 17113.449219\n",
      "Train Epoch: 435 [22656/225000 (10%)] Loss: 16121.082031\n",
      "Train Epoch: 435 [25152/225000 (11%)] Loss: 16898.445312\n",
      "Train Epoch: 435 [27648/225000 (12%)] Loss: 15930.111328\n",
      "Train Epoch: 435 [30144/225000 (13%)] Loss: 16378.014648\n",
      "Train Epoch: 435 [32640/225000 (15%)] Loss: 16424.125000\n",
      "Train Epoch: 435 [35136/225000 (16%)] Loss: 16757.550781\n",
      "Train Epoch: 435 [37632/225000 (17%)] Loss: 17951.445312\n",
      "Train Epoch: 435 [40128/225000 (18%)] Loss: 17116.853516\n",
      "Train Epoch: 435 [42624/225000 (19%)] Loss: 16542.357422\n",
      "Train Epoch: 435 [45120/225000 (20%)] Loss: 16930.609375\n",
      "Train Epoch: 435 [47616/225000 (21%)] Loss: 16407.292969\n",
      "Train Epoch: 435 [50112/225000 (22%)] Loss: 18739.812500\n",
      "Train Epoch: 435 [52608/225000 (23%)] Loss: 16713.570312\n",
      "Train Epoch: 435 [55104/225000 (24%)] Loss: 16377.443359\n",
      "Train Epoch: 435 [57600/225000 (26%)] Loss: 16496.853516\n",
      "Train Epoch: 435 [60096/225000 (27%)] Loss: 16687.535156\n",
      "Train Epoch: 435 [62592/225000 (28%)] Loss: 16844.462891\n",
      "Train Epoch: 435 [65088/225000 (29%)] Loss: 18323.972656\n",
      "Train Epoch: 435 [67584/225000 (30%)] Loss: 16412.070312\n",
      "Train Epoch: 435 [70080/225000 (31%)] Loss: 16793.275391\n",
      "Train Epoch: 435 [72576/225000 (32%)] Loss: 16584.253906\n",
      "Train Epoch: 435 [75072/225000 (33%)] Loss: 18324.007812\n",
      "Train Epoch: 435 [77568/225000 (34%)] Loss: 16449.992188\n",
      "Train Epoch: 435 [80064/225000 (36%)] Loss: 16686.335938\n",
      "Train Epoch: 435 [82560/225000 (37%)] Loss: 16734.587891\n",
      "Train Epoch: 435 [85056/225000 (38%)] Loss: 17158.695312\n",
      "Train Epoch: 435 [87552/225000 (39%)] Loss: 16764.919922\n",
      "Train Epoch: 435 [90048/225000 (40%)] Loss: 16572.589844\n",
      "Train Epoch: 435 [92544/225000 (41%)] Loss: 16803.277344\n",
      "Train Epoch: 435 [95040/225000 (42%)] Loss: 16622.330078\n",
      "Train Epoch: 435 [97536/225000 (43%)] Loss: 16488.734375\n",
      "Train Epoch: 435 [100032/225000 (44%)] Loss: 17001.984375\n",
      "Train Epoch: 435 [102528/225000 (46%)] Loss: 16420.835938\n",
      "Train Epoch: 435 [105024/225000 (47%)] Loss: 16959.015625\n",
      "Train Epoch: 435 [107520/225000 (48%)] Loss: 16572.933594\n",
      "Train Epoch: 435 [110016/225000 (49%)] Loss: 16452.078125\n",
      "Train Epoch: 435 [112512/225000 (50%)] Loss: 16691.753906\n",
      "Train Epoch: 435 [115008/225000 (51%)] Loss: 16964.154297\n",
      "Train Epoch: 435 [117504/225000 (52%)] Loss: 16211.749023\n",
      "Train Epoch: 435 [120000/225000 (53%)] Loss: 16506.062500\n",
      "Train Epoch: 435 [122496/225000 (54%)] Loss: 17130.162109\n",
      "Train Epoch: 435 [124992/225000 (56%)] Loss: 16473.023438\n",
      "Train Epoch: 435 [127488/225000 (57%)] Loss: 16431.277344\n",
      "Train Epoch: 435 [129984/225000 (58%)] Loss: 16412.035156\n",
      "Train Epoch: 435 [132480/225000 (59%)] Loss: 15941.473633\n",
      "Train Epoch: 435 [134976/225000 (60%)] Loss: 16556.742188\n",
      "Train Epoch: 435 [137472/225000 (61%)] Loss: 16519.925781\n",
      "Train Epoch: 435 [139968/225000 (62%)] Loss: 17558.779297\n",
      "Train Epoch: 435 [142464/225000 (63%)] Loss: 16982.105469\n",
      "Train Epoch: 435 [144960/225000 (64%)] Loss: 18049.613281\n",
      "Train Epoch: 435 [147456/225000 (66%)] Loss: 16686.937500\n",
      "Train Epoch: 435 [149952/225000 (67%)] Loss: 16289.437500\n",
      "Train Epoch: 435 [152448/225000 (68%)] Loss: 16326.725586\n",
      "Train Epoch: 435 [154944/225000 (69%)] Loss: 16576.710938\n",
      "Train Epoch: 435 [157440/225000 (70%)] Loss: 16413.447266\n",
      "Train Epoch: 435 [159936/225000 (71%)] Loss: 16831.476562\n",
      "Train Epoch: 435 [162432/225000 (72%)] Loss: 16552.179688\n",
      "Train Epoch: 435 [164928/225000 (73%)] Loss: 16723.917969\n",
      "Train Epoch: 435 [167424/225000 (74%)] Loss: 16637.808594\n",
      "Train Epoch: 435 [169920/225000 (76%)] Loss: 16912.371094\n",
      "Train Epoch: 435 [172416/225000 (77%)] Loss: 17518.652344\n",
      "Train Epoch: 435 [174912/225000 (78%)] Loss: 16714.781250\n",
      "Train Epoch: 435 [177408/225000 (79%)] Loss: 16473.863281\n",
      "Train Epoch: 435 [179904/225000 (80%)] Loss: 16795.900391\n",
      "Train Epoch: 435 [182400/225000 (81%)] Loss: 16477.453125\n",
      "Train Epoch: 435 [184896/225000 (82%)] Loss: 16920.031250\n",
      "Train Epoch: 435 [187392/225000 (83%)] Loss: 17860.710938\n",
      "Train Epoch: 435 [189888/225000 (84%)] Loss: 16769.347656\n",
      "Train Epoch: 435 [192384/225000 (86%)] Loss: 16599.585938\n",
      "Train Epoch: 435 [194880/225000 (87%)] Loss: 17346.371094\n",
      "Train Epoch: 435 [197376/225000 (88%)] Loss: 17995.085938\n",
      "Train Epoch: 435 [199872/225000 (89%)] Loss: 17135.183594\n",
      "Train Epoch: 435 [202368/225000 (90%)] Loss: 16536.425781\n",
      "Train Epoch: 435 [204864/225000 (91%)] Loss: 16719.949219\n",
      "Train Epoch: 435 [207360/225000 (92%)] Loss: 16880.765625\n",
      "Train Epoch: 435 [209856/225000 (93%)] Loss: 16413.589844\n",
      "Train Epoch: 435 [212352/225000 (94%)] Loss: 16812.562500\n",
      "Train Epoch: 435 [214848/225000 (95%)] Loss: 16737.201172\n",
      "Train Epoch: 435 [217344/225000 (97%)] Loss: 16514.806641\n",
      "Train Epoch: 435 [219840/225000 (98%)] Loss: 16881.851562\n",
      "Train Epoch: 435 [222336/225000 (99%)] Loss: 16730.832031\n",
      "Train Epoch: 435 [224832/225000 (100%)] Loss: 16428.636719\n",
      "    epoch          : 435\n",
      "    loss           : 16721.85883422435\n",
      "    val_loss       : 16667.539042737648\n",
      "Train Epoch: 436 [192/225000 (0%)] Loss: 16856.152344\n",
      "Train Epoch: 436 [2688/225000 (1%)] Loss: 16514.789062\n",
      "Train Epoch: 436 [5184/225000 (2%)] Loss: 15877.540039\n",
      "Train Epoch: 436 [7680/225000 (3%)] Loss: 16920.804688\n",
      "Train Epoch: 436 [10176/225000 (5%)] Loss: 16802.937500\n",
      "Train Epoch: 436 [12672/225000 (6%)] Loss: 16423.042969\n",
      "Train Epoch: 436 [15168/225000 (7%)] Loss: 16397.199219\n",
      "Train Epoch: 436 [17664/225000 (8%)] Loss: 16698.574219\n",
      "Train Epoch: 436 [20160/225000 (9%)] Loss: 16779.638672\n",
      "Train Epoch: 436 [22656/225000 (10%)] Loss: 16430.296875\n",
      "Train Epoch: 436 [25152/225000 (11%)] Loss: 16387.017578\n",
      "Train Epoch: 436 [27648/225000 (12%)] Loss: 16553.007812\n",
      "Train Epoch: 436 [30144/225000 (13%)] Loss: 16250.856445\n",
      "Train Epoch: 436 [32640/225000 (15%)] Loss: 16330.777344\n",
      "Train Epoch: 436 [35136/225000 (16%)] Loss: 16608.179688\n",
      "Train Epoch: 436 [37632/225000 (17%)] Loss: 18781.582031\n",
      "Train Epoch: 436 [40128/225000 (18%)] Loss: 16709.763672\n",
      "Train Epoch: 436 [42624/225000 (19%)] Loss: 16441.546875\n",
      "Train Epoch: 436 [45120/225000 (20%)] Loss: 16683.621094\n",
      "Train Epoch: 436 [47616/225000 (21%)] Loss: 16461.537109\n",
      "Train Epoch: 436 [50112/225000 (22%)] Loss: 16833.375000\n",
      "Train Epoch: 436 [52608/225000 (23%)] Loss: 16421.179688\n",
      "Train Epoch: 436 [55104/225000 (24%)] Loss: 16417.515625\n",
      "Train Epoch: 436 [57600/225000 (26%)] Loss: 16559.531250\n",
      "Train Epoch: 436 [60096/225000 (27%)] Loss: 20160.082031\n",
      "Train Epoch: 436 [62592/225000 (28%)] Loss: 16777.511719\n",
      "Train Epoch: 436 [65088/225000 (29%)] Loss: 16762.253906\n",
      "Train Epoch: 436 [67584/225000 (30%)] Loss: 17968.269531\n",
      "Train Epoch: 436 [70080/225000 (31%)] Loss: 16812.933594\n",
      "Train Epoch: 436 [72576/225000 (32%)] Loss: 16309.351562\n",
      "Train Epoch: 436 [75072/225000 (33%)] Loss: 16578.648438\n",
      "Train Epoch: 436 [77568/225000 (34%)] Loss: 16189.115234\n",
      "Train Epoch: 436 [80064/225000 (36%)] Loss: 16740.039062\n",
      "Train Epoch: 436 [82560/225000 (37%)] Loss: 17917.107422\n",
      "Train Epoch: 436 [85056/225000 (38%)] Loss: 18406.951172\n",
      "Train Epoch: 436 [87552/225000 (39%)] Loss: 16472.941406\n",
      "Train Epoch: 436 [90048/225000 (40%)] Loss: 16779.908203\n",
      "Train Epoch: 436 [92544/225000 (41%)] Loss: 16797.138672\n",
      "Train Epoch: 436 [95040/225000 (42%)] Loss: 16858.371094\n",
      "Train Epoch: 436 [97536/225000 (43%)] Loss: 16560.246094\n",
      "Train Epoch: 436 [100032/225000 (44%)] Loss: 16590.734375\n",
      "Train Epoch: 436 [102528/225000 (46%)] Loss: 16536.679688\n",
      "Train Epoch: 436 [105024/225000 (47%)] Loss: 16640.722656\n",
      "Train Epoch: 436 [107520/225000 (48%)] Loss: 16568.644531\n",
      "Train Epoch: 436 [110016/225000 (49%)] Loss: 16964.433594\n",
      "Train Epoch: 436 [112512/225000 (50%)] Loss: 16722.990234\n",
      "Train Epoch: 436 [115008/225000 (51%)] Loss: 17037.714844\n",
      "Train Epoch: 436 [117504/225000 (52%)] Loss: 16469.115234\n",
      "Train Epoch: 436 [120000/225000 (53%)] Loss: 16914.968750\n",
      "Train Epoch: 436 [122496/225000 (54%)] Loss: 16971.660156\n",
      "Train Epoch: 436 [124992/225000 (56%)] Loss: 16545.609375\n",
      "Train Epoch: 436 [127488/225000 (57%)] Loss: 16664.285156\n",
      "Train Epoch: 436 [129984/225000 (58%)] Loss: 17096.437500\n",
      "Train Epoch: 436 [132480/225000 (59%)] Loss: 17082.804688\n",
      "Train Epoch: 436 [134976/225000 (60%)] Loss: 16415.988281\n",
      "Train Epoch: 436 [137472/225000 (61%)] Loss: 16516.769531\n",
      "Train Epoch: 436 [139968/225000 (62%)] Loss: 16559.958984\n",
      "Train Epoch: 436 [142464/225000 (63%)] Loss: 17016.554688\n",
      "Train Epoch: 436 [144960/225000 (64%)] Loss: 16551.093750\n",
      "Train Epoch: 436 [147456/225000 (66%)] Loss: 16689.498047\n",
      "Train Epoch: 436 [149952/225000 (67%)] Loss: 16899.230469\n",
      "Train Epoch: 436 [152448/225000 (68%)] Loss: 16467.335938\n",
      "Train Epoch: 436 [154944/225000 (69%)] Loss: 16607.003906\n",
      "Train Epoch: 436 [157440/225000 (70%)] Loss: 16363.532227\n",
      "Train Epoch: 436 [159936/225000 (71%)] Loss: 18232.833984\n",
      "Train Epoch: 436 [162432/225000 (72%)] Loss: 16420.689453\n",
      "Train Epoch: 436 [164928/225000 (73%)] Loss: 16510.111328\n",
      "Train Epoch: 436 [167424/225000 (74%)] Loss: 16802.183594\n",
      "Train Epoch: 436 [169920/225000 (76%)] Loss: 16904.781250\n",
      "Train Epoch: 436 [172416/225000 (77%)] Loss: 16719.261719\n",
      "Train Epoch: 436 [174912/225000 (78%)] Loss: 17021.917969\n",
      "Train Epoch: 436 [177408/225000 (79%)] Loss: 16820.085938\n",
      "Train Epoch: 436 [179904/225000 (80%)] Loss: 16506.511719\n",
      "Train Epoch: 436 [182400/225000 (81%)] Loss: 16806.957031\n",
      "Train Epoch: 436 [184896/225000 (82%)] Loss: 16706.085938\n",
      "Train Epoch: 436 [187392/225000 (83%)] Loss: 16316.734375\n",
      "Train Epoch: 436 [189888/225000 (84%)] Loss: 16752.285156\n",
      "Train Epoch: 436 [192384/225000 (86%)] Loss: 16579.542969\n",
      "Train Epoch: 436 [194880/225000 (87%)] Loss: 16135.535156\n",
      "Train Epoch: 436 [197376/225000 (88%)] Loss: 16676.796875\n",
      "Train Epoch: 436 [199872/225000 (89%)] Loss: 16121.157227\n",
      "Train Epoch: 436 [202368/225000 (90%)] Loss: 16578.699219\n",
      "Train Epoch: 436 [204864/225000 (91%)] Loss: 16812.558594\n",
      "Train Epoch: 436 [207360/225000 (92%)] Loss: 16438.671875\n",
      "Train Epoch: 436 [209856/225000 (93%)] Loss: 17187.882812\n",
      "Train Epoch: 436 [212352/225000 (94%)] Loss: 16556.992188\n",
      "Train Epoch: 436 [214848/225000 (95%)] Loss: 18452.792969\n",
      "Train Epoch: 436 [217344/225000 (97%)] Loss: 18081.105469\n",
      "Train Epoch: 436 [219840/225000 (98%)] Loss: 16493.878906\n",
      "Train Epoch: 436 [222336/225000 (99%)] Loss: 16604.636719\n",
      "Train Epoch: 436 [224832/225000 (100%)] Loss: 16753.195312\n",
      "    epoch          : 436\n",
      "    loss           : 16693.22702828365\n",
      "    val_loss       : 16673.81314044144\n",
      "Train Epoch: 437 [192/225000 (0%)] Loss: 16823.011719\n",
      "Train Epoch: 437 [2688/225000 (1%)] Loss: 18249.878906\n",
      "Train Epoch: 437 [5184/225000 (2%)] Loss: 16990.095703\n",
      "Train Epoch: 437 [7680/225000 (3%)] Loss: 16643.613281\n",
      "Train Epoch: 437 [10176/225000 (5%)] Loss: 16547.890625\n",
      "Train Epoch: 437 [12672/225000 (6%)] Loss: 17220.929688\n",
      "Train Epoch: 437 [15168/225000 (7%)] Loss: 16772.908203\n",
      "Train Epoch: 437 [17664/225000 (8%)] Loss: 16597.833984\n",
      "Train Epoch: 437 [20160/225000 (9%)] Loss: 16449.279297\n",
      "Train Epoch: 437 [22656/225000 (10%)] Loss: 16649.787109\n",
      "Train Epoch: 437 [25152/225000 (11%)] Loss: 16591.617188\n",
      "Train Epoch: 437 [27648/225000 (12%)] Loss: 16529.613281\n",
      "Train Epoch: 437 [30144/225000 (13%)] Loss: 16295.554688\n",
      "Train Epoch: 437 [32640/225000 (15%)] Loss: 16295.259766\n",
      "Train Epoch: 437 [35136/225000 (16%)] Loss: 15877.808594\n",
      "Train Epoch: 437 [37632/225000 (17%)] Loss: 16373.080078\n",
      "Train Epoch: 437 [40128/225000 (18%)] Loss: 16354.442383\n",
      "Train Epoch: 437 [42624/225000 (19%)] Loss: 16287.566406\n",
      "Train Epoch: 437 [45120/225000 (20%)] Loss: 16231.186523\n",
      "Train Epoch: 437 [47616/225000 (21%)] Loss: 16653.025391\n",
      "Train Epoch: 437 [50112/225000 (22%)] Loss: 16943.419922\n",
      "Train Epoch: 437 [52608/225000 (23%)] Loss: 16791.046875\n",
      "Train Epoch: 437 [55104/225000 (24%)] Loss: 16515.566406\n",
      "Train Epoch: 437 [57600/225000 (26%)] Loss: 16546.972656\n",
      "Train Epoch: 437 [60096/225000 (27%)] Loss: 16851.332031\n",
      "Train Epoch: 437 [62592/225000 (28%)] Loss: 16715.640625\n",
      "Train Epoch: 437 [65088/225000 (29%)] Loss: 17102.998047\n",
      "Train Epoch: 437 [67584/225000 (30%)] Loss: 16600.726562\n",
      "Train Epoch: 437 [70080/225000 (31%)] Loss: 16713.222656\n",
      "Train Epoch: 437 [72576/225000 (32%)] Loss: 16269.541016\n",
      "Train Epoch: 437 [75072/225000 (33%)] Loss: 16662.857422\n",
      "Train Epoch: 437 [77568/225000 (34%)] Loss: 16733.619141\n",
      "Train Epoch: 437 [80064/225000 (36%)] Loss: 16252.077148\n",
      "Train Epoch: 437 [82560/225000 (37%)] Loss: 16469.224609\n",
      "Train Epoch: 437 [85056/225000 (38%)] Loss: 16520.261719\n",
      "Train Epoch: 437 [87552/225000 (39%)] Loss: 16579.421875\n",
      "Train Epoch: 437 [90048/225000 (40%)] Loss: 16627.328125\n",
      "Train Epoch: 437 [92544/225000 (41%)] Loss: 16859.078125\n",
      "Train Epoch: 437 [95040/225000 (42%)] Loss: 16801.035156\n",
      "Train Epoch: 437 [97536/225000 (43%)] Loss: 16536.433594\n",
      "Train Epoch: 437 [100032/225000 (44%)] Loss: 17009.080078\n",
      "Train Epoch: 437 [102528/225000 (46%)] Loss: 16742.283203\n",
      "Train Epoch: 437 [105024/225000 (47%)] Loss: 16729.152344\n",
      "Train Epoch: 437 [107520/225000 (48%)] Loss: 16409.845703\n",
      "Train Epoch: 437 [110016/225000 (49%)] Loss: 16283.687500\n",
      "Train Epoch: 437 [112512/225000 (50%)] Loss: 17043.316406\n",
      "Train Epoch: 437 [115008/225000 (51%)] Loss: 16599.406250\n",
      "Train Epoch: 437 [117504/225000 (52%)] Loss: 16281.288086\n",
      "Train Epoch: 437 [120000/225000 (53%)] Loss: 16506.138672\n",
      "Train Epoch: 437 [122496/225000 (54%)] Loss: 16858.156250\n",
      "Train Epoch: 437 [124992/225000 (56%)] Loss: 16268.202148\n",
      "Train Epoch: 437 [127488/225000 (57%)] Loss: 16222.826172\n",
      "Train Epoch: 437 [129984/225000 (58%)] Loss: 16687.570312\n",
      "Train Epoch: 437 [132480/225000 (59%)] Loss: 16760.103516\n",
      "Train Epoch: 437 [134976/225000 (60%)] Loss: 16124.460938\n",
      "Train Epoch: 437 [137472/225000 (61%)] Loss: 16210.463867\n",
      "Train Epoch: 437 [139968/225000 (62%)] Loss: 16958.523438\n",
      "Train Epoch: 437 [142464/225000 (63%)] Loss: 16441.486328\n",
      "Train Epoch: 437 [144960/225000 (64%)] Loss: 16468.386719\n",
      "Train Epoch: 437 [147456/225000 (66%)] Loss: 16777.000000\n",
      "Train Epoch: 437 [149952/225000 (67%)] Loss: 16723.261719\n",
      "Train Epoch: 437 [152448/225000 (68%)] Loss: 16552.675781\n",
      "Train Epoch: 437 [154944/225000 (69%)] Loss: 16823.410156\n",
      "Train Epoch: 437 [157440/225000 (70%)] Loss: 16309.374023\n",
      "Train Epoch: 437 [159936/225000 (71%)] Loss: 16458.992188\n",
      "Train Epoch: 437 [162432/225000 (72%)] Loss: 16566.132812\n",
      "Train Epoch: 437 [164928/225000 (73%)] Loss: 16391.125000\n",
      "Train Epoch: 437 [167424/225000 (74%)] Loss: 16298.967773\n",
      "Train Epoch: 437 [169920/225000 (76%)] Loss: 16829.289062\n",
      "Train Epoch: 437 [172416/225000 (77%)] Loss: 16624.337891\n",
      "Train Epoch: 437 [174912/225000 (78%)] Loss: 16974.875000\n",
      "Train Epoch: 437 [177408/225000 (79%)] Loss: 16606.572266\n",
      "Train Epoch: 437 [179904/225000 (80%)] Loss: 16473.324219\n",
      "Train Epoch: 437 [182400/225000 (81%)] Loss: 18032.660156\n",
      "Train Epoch: 437 [184896/225000 (82%)] Loss: 16802.550781\n",
      "Train Epoch: 437 [187392/225000 (83%)] Loss: 16906.548828\n",
      "Train Epoch: 437 [189888/225000 (84%)] Loss: 16164.632812\n",
      "Train Epoch: 437 [192384/225000 (86%)] Loss: 16544.824219\n",
      "Train Epoch: 437 [194880/225000 (87%)] Loss: 16631.777344\n",
      "Train Epoch: 437 [197376/225000 (88%)] Loss: 16709.738281\n",
      "Train Epoch: 437 [199872/225000 (89%)] Loss: 17029.533203\n",
      "Train Epoch: 437 [202368/225000 (90%)] Loss: 16585.089844\n",
      "Train Epoch: 437 [204864/225000 (91%)] Loss: 16723.742188\n",
      "Train Epoch: 437 [207360/225000 (92%)] Loss: 17188.941406\n",
      "Train Epoch: 437 [209856/225000 (93%)] Loss: 16979.195312\n",
      "Train Epoch: 437 [212352/225000 (94%)] Loss: 16677.554688\n",
      "Train Epoch: 437 [214848/225000 (95%)] Loss: 16881.685547\n",
      "Train Epoch: 437 [217344/225000 (97%)] Loss: 16718.992188\n",
      "Train Epoch: 437 [219840/225000 (98%)] Loss: 16693.082031\n",
      "Train Epoch: 437 [222336/225000 (99%)] Loss: 16983.087891\n",
      "Train Epoch: 437 [224832/225000 (100%)] Loss: 16783.871094\n",
      "    epoch          : 437\n",
      "    loss           : 16715.093780830044\n",
      "    val_loss       : 16733.422140473172\n",
      "Train Epoch: 438 [192/225000 (0%)] Loss: 16548.576172\n",
      "Train Epoch: 438 [2688/225000 (1%)] Loss: 16631.039062\n",
      "Train Epoch: 438 [5184/225000 (2%)] Loss: 17045.187500\n",
      "Train Epoch: 438 [7680/225000 (3%)] Loss: 16294.742188\n",
      "Train Epoch: 438 [10176/225000 (5%)] Loss: 16930.386719\n",
      "Train Epoch: 438 [12672/225000 (6%)] Loss: 16814.265625\n",
      "Train Epoch: 438 [15168/225000 (7%)] Loss: 16531.773438\n",
      "Train Epoch: 438 [17664/225000 (8%)] Loss: 16621.019531\n",
      "Train Epoch: 438 [20160/225000 (9%)] Loss: 16698.777344\n",
      "Train Epoch: 438 [22656/225000 (10%)] Loss: 16481.828125\n",
      "Train Epoch: 438 [25152/225000 (11%)] Loss: 16425.248047\n",
      "Train Epoch: 438 [27648/225000 (12%)] Loss: 16771.695312\n",
      "Train Epoch: 438 [30144/225000 (13%)] Loss: 16520.882812\n",
      "Train Epoch: 438 [32640/225000 (15%)] Loss: 17043.568359\n",
      "Train Epoch: 438 [35136/225000 (16%)] Loss: 16446.476562\n",
      "Train Epoch: 438 [37632/225000 (17%)] Loss: 16728.664062\n",
      "Train Epoch: 438 [40128/225000 (18%)] Loss: 17272.849609\n",
      "Train Epoch: 438 [42624/225000 (19%)] Loss: 16701.320312\n",
      "Train Epoch: 438 [45120/225000 (20%)] Loss: 16347.189453\n",
      "Train Epoch: 438 [47616/225000 (21%)] Loss: 16670.181641\n",
      "Train Epoch: 438 [50112/225000 (22%)] Loss: 16890.000000\n",
      "Train Epoch: 438 [52608/225000 (23%)] Loss: 16400.638672\n",
      "Train Epoch: 438 [55104/225000 (24%)] Loss: 16080.206055\n",
      "Train Epoch: 438 [57600/225000 (26%)] Loss: 16676.949219\n",
      "Train Epoch: 438 [60096/225000 (27%)] Loss: 16975.003906\n",
      "Train Epoch: 438 [62592/225000 (28%)] Loss: 17060.738281\n",
      "Train Epoch: 438 [65088/225000 (29%)] Loss: 16511.167969\n",
      "Train Epoch: 438 [67584/225000 (30%)] Loss: 16745.341797\n",
      "Train Epoch: 438 [70080/225000 (31%)] Loss: 17352.480469\n",
      "Train Epoch: 438 [72576/225000 (32%)] Loss: 16471.328125\n",
      "Train Epoch: 438 [75072/225000 (33%)] Loss: 16292.501953\n",
      "Train Epoch: 438 [77568/225000 (34%)] Loss: 16249.638672\n",
      "Train Epoch: 438 [80064/225000 (36%)] Loss: 16682.099609\n",
      "Train Epoch: 438 [82560/225000 (37%)] Loss: 16275.133789\n",
      "Train Epoch: 438 [85056/225000 (38%)] Loss: 17249.195312\n",
      "Train Epoch: 438 [87552/225000 (39%)] Loss: 16799.703125\n",
      "Train Epoch: 438 [90048/225000 (40%)] Loss: 16564.058594\n",
      "Train Epoch: 438 [92544/225000 (41%)] Loss: 16887.656250\n",
      "Train Epoch: 438 [95040/225000 (42%)] Loss: 16650.218750\n",
      "Train Epoch: 438 [97536/225000 (43%)] Loss: 16463.207031\n",
      "Train Epoch: 438 [100032/225000 (44%)] Loss: 16081.743164\n",
      "Train Epoch: 438 [102528/225000 (46%)] Loss: 16315.449219\n",
      "Train Epoch: 438 [105024/225000 (47%)] Loss: 16526.367188\n",
      "Train Epoch: 438 [107520/225000 (48%)] Loss: 16679.820312\n",
      "Train Epoch: 438 [110016/225000 (49%)] Loss: 16801.562500\n",
      "Train Epoch: 438 [112512/225000 (50%)] Loss: 16669.238281\n",
      "Train Epoch: 438 [115008/225000 (51%)] Loss: 16408.412109\n",
      "Train Epoch: 438 [117504/225000 (52%)] Loss: 17058.150391\n",
      "Train Epoch: 438 [120000/225000 (53%)] Loss: 16354.438477\n",
      "Train Epoch: 438 [122496/225000 (54%)] Loss: 16825.757812\n",
      "Train Epoch: 438 [124992/225000 (56%)] Loss: 16587.886719\n",
      "Train Epoch: 438 [127488/225000 (57%)] Loss: 16566.164062\n",
      "Train Epoch: 438 [129984/225000 (58%)] Loss: 16454.503906\n",
      "Train Epoch: 438 [132480/225000 (59%)] Loss: 16895.156250\n",
      "Train Epoch: 438 [134976/225000 (60%)] Loss: 16395.179688\n",
      "Train Epoch: 438 [137472/225000 (61%)] Loss: 17124.775391\n",
      "Train Epoch: 438 [139968/225000 (62%)] Loss: 16673.486328\n",
      "Train Epoch: 438 [142464/225000 (63%)] Loss: 16971.218750\n",
      "Train Epoch: 438 [144960/225000 (64%)] Loss: 16776.087891\n",
      "Train Epoch: 438 [147456/225000 (66%)] Loss: 16482.246094\n",
      "Train Epoch: 438 [149952/225000 (67%)] Loss: 16548.212891\n",
      "Train Epoch: 438 [152448/225000 (68%)] Loss: 17490.175781\n",
      "Train Epoch: 438 [154944/225000 (69%)] Loss: 16529.486328\n",
      "Train Epoch: 438 [157440/225000 (70%)] Loss: 17150.300781\n",
      "Train Epoch: 438 [159936/225000 (71%)] Loss: 16396.607422\n",
      "Train Epoch: 438 [162432/225000 (72%)] Loss: 17015.058594\n",
      "Train Epoch: 438 [164928/225000 (73%)] Loss: 16708.023438\n",
      "Train Epoch: 438 [167424/225000 (74%)] Loss: 16433.019531\n",
      "Train Epoch: 438 [169920/225000 (76%)] Loss: 16917.765625\n",
      "Train Epoch: 438 [172416/225000 (77%)] Loss: 16781.578125\n",
      "Train Epoch: 438 [174912/225000 (78%)] Loss: 16954.238281\n",
      "Train Epoch: 438 [177408/225000 (79%)] Loss: 16411.146484\n",
      "Train Epoch: 438 [179904/225000 (80%)] Loss: 16657.814453\n",
      "Train Epoch: 438 [182400/225000 (81%)] Loss: 17027.929688\n",
      "Train Epoch: 438 [184896/225000 (82%)] Loss: 16466.906250\n",
      "Train Epoch: 438 [187392/225000 (83%)] Loss: 16619.523438\n",
      "Train Epoch: 438 [189888/225000 (84%)] Loss: 16508.716797\n",
      "Train Epoch: 438 [192384/225000 (86%)] Loss: 16152.047852\n",
      "Train Epoch: 438 [194880/225000 (87%)] Loss: 16683.417969\n",
      "Train Epoch: 438 [197376/225000 (88%)] Loss: 16269.091797\n",
      "Train Epoch: 438 [199872/225000 (89%)] Loss: 16152.697266\n",
      "Train Epoch: 438 [202368/225000 (90%)] Loss: 16131.757812\n",
      "Train Epoch: 438 [204864/225000 (91%)] Loss: 16166.921875\n",
      "Train Epoch: 438 [207360/225000 (92%)] Loss: 16154.889648\n",
      "Train Epoch: 438 [209856/225000 (93%)] Loss: 18025.468750\n",
      "Train Epoch: 438 [212352/225000 (94%)] Loss: 16386.988281\n",
      "Train Epoch: 438 [214848/225000 (95%)] Loss: 16666.792969\n",
      "Train Epoch: 438 [217344/225000 (97%)] Loss: 16373.175781\n",
      "Train Epoch: 438 [219840/225000 (98%)] Loss: 17021.185547\n",
      "Train Epoch: 438 [222336/225000 (99%)] Loss: 16989.710938\n",
      "Train Epoch: 438 [224832/225000 (100%)] Loss: 17048.433594\n",
      "    epoch          : 438\n",
      "    loss           : 16693.0789590777\n",
      "    val_loss       : 16619.798594750522\n",
      "Train Epoch: 439 [192/225000 (0%)] Loss: 16737.554688\n",
      "Train Epoch: 439 [2688/225000 (1%)] Loss: 16313.370117\n",
      "Train Epoch: 439 [5184/225000 (2%)] Loss: 16626.619141\n",
      "Train Epoch: 439 [7680/225000 (3%)] Loss: 16745.855469\n",
      "Train Epoch: 439 [10176/225000 (5%)] Loss: 16503.900391\n",
      "Train Epoch: 439 [12672/225000 (6%)] Loss: 18088.890625\n",
      "Train Epoch: 439 [15168/225000 (7%)] Loss: 16717.738281\n",
      "Train Epoch: 439 [17664/225000 (8%)] Loss: 16610.656250\n",
      "Train Epoch: 439 [20160/225000 (9%)] Loss: 16336.532227\n",
      "Train Epoch: 439 [22656/225000 (10%)] Loss: 16390.667969\n",
      "Train Epoch: 439 [25152/225000 (11%)] Loss: 17949.695312\n",
      "Train Epoch: 439 [27648/225000 (12%)] Loss: 16750.230469\n",
      "Train Epoch: 439 [30144/225000 (13%)] Loss: 16110.873047\n",
      "Train Epoch: 439 [32640/225000 (15%)] Loss: 16552.953125\n",
      "Train Epoch: 439 [35136/225000 (16%)] Loss: 16585.734375\n",
      "Train Epoch: 439 [37632/225000 (17%)] Loss: 16673.962891\n",
      "Train Epoch: 439 [40128/225000 (18%)] Loss: 16717.101562\n",
      "Train Epoch: 439 [42624/225000 (19%)] Loss: 16691.519531\n",
      "Train Epoch: 439 [45120/225000 (20%)] Loss: 16723.113281\n",
      "Train Epoch: 439 [47616/225000 (21%)] Loss: 16565.726562\n",
      "Train Epoch: 439 [50112/225000 (22%)] Loss: 16658.669922\n",
      "Train Epoch: 439 [52608/225000 (23%)] Loss: 17077.003906\n",
      "Train Epoch: 439 [55104/225000 (24%)] Loss: 17002.218750\n",
      "Train Epoch: 439 [57600/225000 (26%)] Loss: 17393.996094\n",
      "Train Epoch: 439 [60096/225000 (27%)] Loss: 16585.027344\n",
      "Train Epoch: 439 [62592/225000 (28%)] Loss: 16757.191406\n",
      "Train Epoch: 439 [65088/225000 (29%)] Loss: 16667.769531\n",
      "Train Epoch: 439 [67584/225000 (30%)] Loss: 16493.289062\n",
      "Train Epoch: 439 [70080/225000 (31%)] Loss: 16359.791992\n",
      "Train Epoch: 439 [72576/225000 (32%)] Loss: 16457.855469\n",
      "Train Epoch: 439 [75072/225000 (33%)] Loss: 16173.774414\n",
      "Train Epoch: 439 [77568/225000 (34%)] Loss: 16872.769531\n",
      "Train Epoch: 439 [80064/225000 (36%)] Loss: 16897.363281\n",
      "Train Epoch: 439 [82560/225000 (37%)] Loss: 16306.975586\n",
      "Train Epoch: 439 [85056/225000 (38%)] Loss: 15970.983398\n",
      "Train Epoch: 439 [87552/225000 (39%)] Loss: 16570.152344\n",
      "Train Epoch: 439 [90048/225000 (40%)] Loss: 16383.107422\n",
      "Train Epoch: 439 [92544/225000 (41%)] Loss: 16738.230469\n",
      "Train Epoch: 439 [95040/225000 (42%)] Loss: 16564.507812\n",
      "Train Epoch: 439 [97536/225000 (43%)] Loss: 16504.111328\n",
      "Train Epoch: 439 [100032/225000 (44%)] Loss: 16479.191406\n",
      "Train Epoch: 439 [102528/225000 (46%)] Loss: 16543.437500\n",
      "Train Epoch: 439 [105024/225000 (47%)] Loss: 16690.511719\n",
      "Train Epoch: 439 [107520/225000 (48%)] Loss: 17039.021484\n",
      "Train Epoch: 439 [110016/225000 (49%)] Loss: 16806.437500\n",
      "Train Epoch: 439 [112512/225000 (50%)] Loss: 16691.957031\n",
      "Train Epoch: 439 [115008/225000 (51%)] Loss: 16695.132812\n",
      "Train Epoch: 439 [117504/225000 (52%)] Loss: 16887.156250\n",
      "Train Epoch: 439 [120000/225000 (53%)] Loss: 16356.621094\n",
      "Train Epoch: 439 [122496/225000 (54%)] Loss: 16546.183594\n",
      "Train Epoch: 439 [124992/225000 (56%)] Loss: 16069.247070\n",
      "Train Epoch: 439 [127488/225000 (57%)] Loss: 16494.871094\n",
      "Train Epoch: 439 [129984/225000 (58%)] Loss: 16412.638672\n",
      "Train Epoch: 439 [132480/225000 (59%)] Loss: 16741.765625\n",
      "Train Epoch: 439 [134976/225000 (60%)] Loss: 16580.000000\n",
      "Train Epoch: 439 [137472/225000 (61%)] Loss: 16319.416992\n",
      "Train Epoch: 439 [139968/225000 (62%)] Loss: 16778.156250\n",
      "Train Epoch: 439 [142464/225000 (63%)] Loss: 16694.578125\n",
      "Train Epoch: 439 [144960/225000 (64%)] Loss: 16783.781250\n",
      "Train Epoch: 439 [147456/225000 (66%)] Loss: 16770.546875\n",
      "Train Epoch: 439 [149952/225000 (67%)] Loss: 16554.414062\n",
      "Train Epoch: 439 [152448/225000 (68%)] Loss: 16320.135742\n",
      "Train Epoch: 439 [154944/225000 (69%)] Loss: 16798.925781\n",
      "Train Epoch: 439 [157440/225000 (70%)] Loss: 16558.851562\n",
      "Train Epoch: 439 [159936/225000 (71%)] Loss: 16599.316406\n",
      "Train Epoch: 439 [162432/225000 (72%)] Loss: 16878.505859\n",
      "Train Epoch: 439 [164928/225000 (73%)] Loss: 17978.755859\n",
      "Train Epoch: 439 [167424/225000 (74%)] Loss: 16286.529297\n",
      "Train Epoch: 439 [169920/225000 (76%)] Loss: 16897.105469\n",
      "Train Epoch: 439 [172416/225000 (77%)] Loss: 17176.109375\n",
      "Train Epoch: 439 [174912/225000 (78%)] Loss: 16413.890625\n",
      "Train Epoch: 439 [177408/225000 (79%)] Loss: 16222.337891\n",
      "Train Epoch: 439 [179904/225000 (80%)] Loss: 16672.816406\n",
      "Train Epoch: 439 [182400/225000 (81%)] Loss: 18049.382812\n",
      "Train Epoch: 439 [184896/225000 (82%)] Loss: 16757.984375\n",
      "Train Epoch: 439 [187392/225000 (83%)] Loss: 17020.236328\n",
      "Train Epoch: 439 [189888/225000 (84%)] Loss: 16347.358398\n",
      "Train Epoch: 439 [192384/225000 (86%)] Loss: 18653.871094\n",
      "Train Epoch: 439 [194880/225000 (87%)] Loss: 16727.654297\n",
      "Train Epoch: 439 [197376/225000 (88%)] Loss: 18525.527344\n",
      "Train Epoch: 439 [199872/225000 (89%)] Loss: 16555.447266\n",
      "Train Epoch: 439 [202368/225000 (90%)] Loss: 16489.726562\n",
      "Train Epoch: 439 [204864/225000 (91%)] Loss: 16661.033203\n",
      "Train Epoch: 439 [207360/225000 (92%)] Loss: 16645.265625\n",
      "Train Epoch: 439 [209856/225000 (93%)] Loss: 16796.806641\n",
      "Train Epoch: 439 [212352/225000 (94%)] Loss: 16711.164062\n",
      "Train Epoch: 439 [214848/225000 (95%)] Loss: 16991.800781\n",
      "Train Epoch: 439 [217344/225000 (97%)] Loss: 16556.304688\n",
      "Train Epoch: 439 [219840/225000 (98%)] Loss: 16790.164062\n",
      "Train Epoch: 439 [222336/225000 (99%)] Loss: 16807.031250\n",
      "Train Epoch: 439 [224832/225000 (100%)] Loss: 16623.816406\n",
      "    epoch          : 439\n",
      "    loss           : 16714.92592873427\n",
      "    val_loss       : 16690.109423054993\n",
      "Train Epoch: 440 [192/225000 (0%)] Loss: 16901.703125\n",
      "Train Epoch: 440 [2688/225000 (1%)] Loss: 16634.308594\n",
      "Train Epoch: 440 [5184/225000 (2%)] Loss: 18872.222656\n",
      "Train Epoch: 440 [7680/225000 (3%)] Loss: 16375.344727\n",
      "Train Epoch: 440 [10176/225000 (5%)] Loss: 16380.985352\n",
      "Train Epoch: 440 [12672/225000 (6%)] Loss: 16490.339844\n",
      "Train Epoch: 440 [15168/225000 (7%)] Loss: 16639.216797\n",
      "Train Epoch: 440 [17664/225000 (8%)] Loss: 16725.316406\n",
      "Train Epoch: 440 [20160/225000 (9%)] Loss: 16416.921875\n",
      "Train Epoch: 440 [22656/225000 (10%)] Loss: 16583.222656\n",
      "Train Epoch: 440 [25152/225000 (11%)] Loss: 16628.941406\n",
      "Train Epoch: 440 [27648/225000 (12%)] Loss: 16280.148438\n",
      "Train Epoch: 440 [30144/225000 (13%)] Loss: 16971.175781\n",
      "Train Epoch: 440 [32640/225000 (15%)] Loss: 16846.552734\n",
      "Train Epoch: 440 [35136/225000 (16%)] Loss: 17999.144531\n",
      "Train Epoch: 440 [37632/225000 (17%)] Loss: 16828.451172\n",
      "Train Epoch: 440 [40128/225000 (18%)] Loss: 16333.096680\n",
      "Train Epoch: 440 [42624/225000 (19%)] Loss: 16579.220703\n",
      "Train Epoch: 440 [45120/225000 (20%)] Loss: 16091.659180\n",
      "Train Epoch: 440 [47616/225000 (21%)] Loss: 17065.312500\n",
      "Train Epoch: 440 [50112/225000 (22%)] Loss: 17022.007812\n",
      "Train Epoch: 440 [52608/225000 (23%)] Loss: 16749.048828\n",
      "Train Epoch: 440 [55104/225000 (24%)] Loss: 17042.742188\n",
      "Train Epoch: 440 [57600/225000 (26%)] Loss: 16669.839844\n",
      "Train Epoch: 440 [60096/225000 (27%)] Loss: 16810.398438\n",
      "Train Epoch: 440 [62592/225000 (28%)] Loss: 16723.390625\n",
      "Train Epoch: 440 [65088/225000 (29%)] Loss: 18360.791016\n",
      "Train Epoch: 440 [67584/225000 (30%)] Loss: 17080.664062\n",
      "Train Epoch: 440 [70080/225000 (31%)] Loss: 16861.464844\n",
      "Train Epoch: 440 [72576/225000 (32%)] Loss: 18265.744141\n",
      "Train Epoch: 440 [75072/225000 (33%)] Loss: 16900.484375\n",
      "Train Epoch: 440 [77568/225000 (34%)] Loss: 16499.511719\n",
      "Train Epoch: 440 [80064/225000 (36%)] Loss: 16440.248047\n",
      "Train Epoch: 440 [82560/225000 (37%)] Loss: 17014.705078\n",
      "Train Epoch: 440 [85056/225000 (38%)] Loss: 16411.582031\n",
      "Train Epoch: 440 [87552/225000 (39%)] Loss: 18374.222656\n",
      "Train Epoch: 440 [90048/225000 (40%)] Loss: 16196.233398\n",
      "Train Epoch: 440 [92544/225000 (41%)] Loss: 16469.216797\n",
      "Train Epoch: 440 [95040/225000 (42%)] Loss: 16878.257812\n",
      "Train Epoch: 440 [97536/225000 (43%)] Loss: 16638.484375\n",
      "Train Epoch: 440 [100032/225000 (44%)] Loss: 16732.062500\n",
      "Train Epoch: 440 [102528/225000 (46%)] Loss: 16601.386719\n",
      "Train Epoch: 440 [105024/225000 (47%)] Loss: 16851.964844\n",
      "Train Epoch: 440 [107520/225000 (48%)] Loss: 16230.314453\n",
      "Train Epoch: 440 [110016/225000 (49%)] Loss: 16554.937500\n",
      "Train Epoch: 440 [112512/225000 (50%)] Loss: 16461.468750\n",
      "Train Epoch: 440 [115008/225000 (51%)] Loss: 16655.992188\n",
      "Train Epoch: 440 [117504/225000 (52%)] Loss: 16097.006836\n",
      "Train Epoch: 440 [120000/225000 (53%)] Loss: 16486.847656\n",
      "Train Epoch: 440 [122496/225000 (54%)] Loss: 17036.562500\n",
      "Train Epoch: 440 [124992/225000 (56%)] Loss: 17027.318359\n",
      "Train Epoch: 440 [127488/225000 (57%)] Loss: 16567.890625\n",
      "Train Epoch: 440 [129984/225000 (58%)] Loss: 16826.164062\n",
      "Train Epoch: 440 [132480/225000 (59%)] Loss: 16497.105469\n",
      "Train Epoch: 440 [134976/225000 (60%)] Loss: 16594.953125\n",
      "Train Epoch: 440 [137472/225000 (61%)] Loss: 16530.121094\n",
      "Train Epoch: 440 [139968/225000 (62%)] Loss: 16616.128906\n",
      "Train Epoch: 440 [142464/225000 (63%)] Loss: 16692.027344\n",
      "Train Epoch: 440 [144960/225000 (64%)] Loss: 16639.933594\n",
      "Train Epoch: 440 [147456/225000 (66%)] Loss: 17155.238281\n",
      "Train Epoch: 440 [149952/225000 (67%)] Loss: 16682.171875\n",
      "Train Epoch: 440 [152448/225000 (68%)] Loss: 16319.408203\n",
      "Train Epoch: 440 [154944/225000 (69%)] Loss: 16631.894531\n",
      "Train Epoch: 440 [157440/225000 (70%)] Loss: 16401.550781\n",
      "Train Epoch: 440 [159936/225000 (71%)] Loss: 18181.433594\n",
      "Train Epoch: 440 [162432/225000 (72%)] Loss: 16427.207031\n",
      "Train Epoch: 440 [164928/225000 (73%)] Loss: 16786.576172\n",
      "Train Epoch: 440 [167424/225000 (74%)] Loss: 16573.376953\n",
      "Train Epoch: 440 [169920/225000 (76%)] Loss: 16856.187500\n",
      "Train Epoch: 440 [172416/225000 (77%)] Loss: 16681.750000\n",
      "Train Epoch: 440 [174912/225000 (78%)] Loss: 16644.722656\n",
      "Train Epoch: 440 [177408/225000 (79%)] Loss: 16399.792969\n",
      "Train Epoch: 440 [179904/225000 (80%)] Loss: 16531.546875\n",
      "Train Epoch: 440 [182400/225000 (81%)] Loss: 16957.433594\n",
      "Train Epoch: 440 [184896/225000 (82%)] Loss: 16325.807617\n",
      "Train Epoch: 440 [187392/225000 (83%)] Loss: 16376.708008\n",
      "Train Epoch: 440 [189888/225000 (84%)] Loss: 16670.980469\n",
      "Train Epoch: 440 [192384/225000 (86%)] Loss: 16822.988281\n",
      "Train Epoch: 440 [194880/225000 (87%)] Loss: 16622.755859\n",
      "Train Epoch: 440 [197376/225000 (88%)] Loss: 16472.617188\n",
      "Train Epoch: 440 [199872/225000 (89%)] Loss: 17119.343750\n",
      "Train Epoch: 440 [202368/225000 (90%)] Loss: 16563.046875\n",
      "Train Epoch: 440 [204864/225000 (91%)] Loss: 16640.029297\n",
      "Train Epoch: 440 [207360/225000 (92%)] Loss: 16366.971680\n",
      "Train Epoch: 440 [209856/225000 (93%)] Loss: 16704.568359\n",
      "Train Epoch: 440 [212352/225000 (94%)] Loss: 16916.054688\n",
      "Train Epoch: 440 [214848/225000 (95%)] Loss: 16761.222656\n",
      "Train Epoch: 440 [217344/225000 (97%)] Loss: 17008.388672\n",
      "Train Epoch: 440 [219840/225000 (98%)] Loss: 16352.033203\n",
      "Train Epoch: 440 [222336/225000 (99%)] Loss: 16284.458984\n",
      "Train Epoch: 440 [224832/225000 (100%)] Loss: 16656.671875\n",
      "    epoch          : 440\n",
      "    loss           : 16699.611682253893\n",
      "    val_loss       : 16620.869461576447\n",
      "Train Epoch: 441 [192/225000 (0%)] Loss: 16693.062500\n",
      "Train Epoch: 441 [2688/225000 (1%)] Loss: 18532.580078\n",
      "Train Epoch: 441 [5184/225000 (2%)] Loss: 16367.221680\n",
      "Train Epoch: 441 [7680/225000 (3%)] Loss: 16471.748047\n",
      "Train Epoch: 441 [10176/225000 (5%)] Loss: 16656.867188\n",
      "Train Epoch: 441 [12672/225000 (6%)] Loss: 16999.714844\n",
      "Train Epoch: 441 [15168/225000 (7%)] Loss: 16747.597656\n",
      "Train Epoch: 441 [17664/225000 (8%)] Loss: 16797.500000\n",
      "Train Epoch: 441 [20160/225000 (9%)] Loss: 16427.996094\n",
      "Train Epoch: 441 [22656/225000 (10%)] Loss: 17124.509766\n",
      "Train Epoch: 441 [25152/225000 (11%)] Loss: 16293.016602\n",
      "Train Epoch: 441 [27648/225000 (12%)] Loss: 16633.890625\n",
      "Train Epoch: 441 [30144/225000 (13%)] Loss: 16497.330078\n",
      "Train Epoch: 441 [32640/225000 (15%)] Loss: 16634.292969\n",
      "Train Epoch: 441 [35136/225000 (16%)] Loss: 16909.978516\n",
      "Train Epoch: 441 [37632/225000 (17%)] Loss: 16888.414062\n",
      "Train Epoch: 441 [40128/225000 (18%)] Loss: 16460.382812\n",
      "Train Epoch: 441 [42624/225000 (19%)] Loss: 16620.226562\n",
      "Train Epoch: 441 [45120/225000 (20%)] Loss: 16629.173828\n",
      "Train Epoch: 441 [47616/225000 (21%)] Loss: 16670.255859\n",
      "Train Epoch: 441 [50112/225000 (22%)] Loss: 16871.048828\n",
      "Train Epoch: 441 [52608/225000 (23%)] Loss: 16257.787109\n",
      "Train Epoch: 441 [55104/225000 (24%)] Loss: 16881.816406\n",
      "Train Epoch: 441 [57600/225000 (26%)] Loss: 18511.507812\n",
      "Train Epoch: 441 [60096/225000 (27%)] Loss: 16798.328125\n",
      "Train Epoch: 441 [62592/225000 (28%)] Loss: 16696.181641\n",
      "Train Epoch: 441 [65088/225000 (29%)] Loss: 16235.627930\n",
      "Train Epoch: 441 [67584/225000 (30%)] Loss: 16916.269531\n",
      "Train Epoch: 441 [70080/225000 (31%)] Loss: 16465.207031\n",
      "Train Epoch: 441 [72576/225000 (32%)] Loss: 16831.726562\n",
      "Train Epoch: 441 [75072/225000 (33%)] Loss: 16809.207031\n",
      "Train Epoch: 441 [77568/225000 (34%)] Loss: 16665.691406\n",
      "Train Epoch: 441 [80064/225000 (36%)] Loss: 17088.386719\n",
      "Train Epoch: 441 [82560/225000 (37%)] Loss: 16530.707031\n",
      "Train Epoch: 441 [85056/225000 (38%)] Loss: 16452.802734\n",
      "Train Epoch: 441 [87552/225000 (39%)] Loss: 16623.410156\n",
      "Train Epoch: 441 [90048/225000 (40%)] Loss: 16674.162109\n",
      "Train Epoch: 441 [92544/225000 (41%)] Loss: 17190.644531\n",
      "Train Epoch: 441 [95040/225000 (42%)] Loss: 16340.027344\n",
      "Train Epoch: 441 [97536/225000 (43%)] Loss: 17920.917969\n",
      "Train Epoch: 441 [100032/225000 (44%)] Loss: 16446.609375\n",
      "Train Epoch: 441 [102528/225000 (46%)] Loss: 16541.517578\n",
      "Train Epoch: 441 [105024/225000 (47%)] Loss: 16496.544922\n",
      "Train Epoch: 441 [107520/225000 (48%)] Loss: 17121.429688\n",
      "Train Epoch: 441 [110016/225000 (49%)] Loss: 16631.015625\n",
      "Train Epoch: 441 [112512/225000 (50%)] Loss: 16356.988281\n",
      "Train Epoch: 441 [115008/225000 (51%)] Loss: 16572.197266\n",
      "Train Epoch: 441 [117504/225000 (52%)] Loss: 17208.246094\n",
      "Train Epoch: 441 [120000/225000 (53%)] Loss: 16519.570312\n",
      "Train Epoch: 441 [122496/225000 (54%)] Loss: 16634.695312\n",
      "Train Epoch: 441 [124992/225000 (56%)] Loss: 16981.457031\n",
      "Train Epoch: 441 [127488/225000 (57%)] Loss: 16508.115234\n",
      "Train Epoch: 441 [129984/225000 (58%)] Loss: 16840.144531\n",
      "Train Epoch: 441 [132480/225000 (59%)] Loss: 16520.171875\n",
      "Train Epoch: 441 [134976/225000 (60%)] Loss: 16594.691406\n",
      "Train Epoch: 441 [137472/225000 (61%)] Loss: 16850.609375\n",
      "Train Epoch: 441 [139968/225000 (62%)] Loss: 16600.222656\n",
      "Train Epoch: 441 [142464/225000 (63%)] Loss: 16573.343750\n",
      "Train Epoch: 441 [144960/225000 (64%)] Loss: 16896.500000\n",
      "Train Epoch: 441 [147456/225000 (66%)] Loss: 16563.554688\n",
      "Train Epoch: 441 [149952/225000 (67%)] Loss: 16457.414062\n",
      "Train Epoch: 441 [152448/225000 (68%)] Loss: 16584.632812\n",
      "Train Epoch: 441 [154944/225000 (69%)] Loss: 17174.681641\n",
      "Train Epoch: 441 [157440/225000 (70%)] Loss: 16341.910156\n",
      "Train Epoch: 441 [159936/225000 (71%)] Loss: 16557.773438\n",
      "Train Epoch: 441 [162432/225000 (72%)] Loss: 16366.193359\n",
      "Train Epoch: 441 [164928/225000 (73%)] Loss: 16501.062500\n",
      "Train Epoch: 441 [167424/225000 (74%)] Loss: 16879.945312\n",
      "Train Epoch: 441 [169920/225000 (76%)] Loss: 16450.363281\n",
      "Train Epoch: 441 [172416/225000 (77%)] Loss: 16659.515625\n",
      "Train Epoch: 441 [174912/225000 (78%)] Loss: 16898.488281\n",
      "Train Epoch: 441 [177408/225000 (79%)] Loss: 16841.417969\n",
      "Train Epoch: 441 [179904/225000 (80%)] Loss: 18701.687500\n",
      "Train Epoch: 441 [182400/225000 (81%)] Loss: 16609.355469\n",
      "Train Epoch: 441 [184896/225000 (82%)] Loss: 16154.323242\n",
      "Train Epoch: 441 [187392/225000 (83%)] Loss: 16612.207031\n",
      "Train Epoch: 441 [189888/225000 (84%)] Loss: 16810.261719\n",
      "Train Epoch: 441 [192384/225000 (86%)] Loss: 16472.484375\n",
      "Train Epoch: 441 [194880/225000 (87%)] Loss: 16763.162109\n",
      "Train Epoch: 441 [197376/225000 (88%)] Loss: 16324.893555\n",
      "Train Epoch: 441 [199872/225000 (89%)] Loss: 16436.128906\n",
      "Train Epoch: 441 [202368/225000 (90%)] Loss: 16181.055664\n",
      "Train Epoch: 441 [204864/225000 (91%)] Loss: 16695.058594\n",
      "Train Epoch: 441 [207360/225000 (92%)] Loss: 16727.519531\n",
      "Train Epoch: 441 [209856/225000 (93%)] Loss: 16588.882812\n",
      "Train Epoch: 441 [212352/225000 (94%)] Loss: 17241.890625\n",
      "Train Epoch: 441 [214848/225000 (95%)] Loss: 16772.683594\n",
      "Train Epoch: 441 [217344/225000 (97%)] Loss: 16734.160156\n",
      "Train Epoch: 441 [219840/225000 (98%)] Loss: 17249.265625\n",
      "Train Epoch: 441 [222336/225000 (99%)] Loss: 16361.329102\n",
      "Train Epoch: 441 [224832/225000 (100%)] Loss: 16756.302734\n",
      "    epoch          : 441\n",
      "    loss           : 16724.2608021811\n",
      "    val_loss       : 16678.755093897573\n",
      "Train Epoch: 442 [192/225000 (0%)] Loss: 16069.839844\n",
      "Train Epoch: 442 [2688/225000 (1%)] Loss: 16794.912109\n",
      "Train Epoch: 442 [5184/225000 (2%)] Loss: 16038.186523\n",
      "Train Epoch: 442 [7680/225000 (3%)] Loss: 16526.386719\n",
      "Train Epoch: 442 [10176/225000 (5%)] Loss: 16523.347656\n",
      "Train Epoch: 442 [12672/225000 (6%)] Loss: 18518.468750\n",
      "Train Epoch: 442 [15168/225000 (7%)] Loss: 16784.433594\n",
      "Train Epoch: 442 [17664/225000 (8%)] Loss: 16494.472656\n",
      "Train Epoch: 442 [20160/225000 (9%)] Loss: 16722.892578\n",
      "Train Epoch: 442 [22656/225000 (10%)] Loss: 16851.193359\n",
      "Train Epoch: 442 [25152/225000 (11%)] Loss: 16811.996094\n",
      "Train Epoch: 442 [27648/225000 (12%)] Loss: 16793.775391\n",
      "Train Epoch: 442 [30144/225000 (13%)] Loss: 16913.716797\n",
      "Train Epoch: 442 [32640/225000 (15%)] Loss: 16754.539062\n",
      "Train Epoch: 442 [35136/225000 (16%)] Loss: 17061.406250\n",
      "Train Epoch: 442 [37632/225000 (17%)] Loss: 16469.660156\n",
      "Train Epoch: 442 [40128/225000 (18%)] Loss: 16184.739258\n",
      "Train Epoch: 442 [42624/225000 (19%)] Loss: 17018.021484\n",
      "Train Epoch: 442 [45120/225000 (20%)] Loss: 16738.960938\n",
      "Train Epoch: 442 [47616/225000 (21%)] Loss: 16678.839844\n",
      "Train Epoch: 442 [50112/225000 (22%)] Loss: 16718.865234\n",
      "Train Epoch: 442 [52608/225000 (23%)] Loss: 16783.876953\n",
      "Train Epoch: 442 [55104/225000 (24%)] Loss: 16448.484375\n",
      "Train Epoch: 442 [57600/225000 (26%)] Loss: 16674.390625\n",
      "Train Epoch: 442 [60096/225000 (27%)] Loss: 16371.406250\n",
      "Train Epoch: 442 [62592/225000 (28%)] Loss: 16467.367188\n",
      "Train Epoch: 442 [65088/225000 (29%)] Loss: 16334.215820\n",
      "Train Epoch: 442 [67584/225000 (30%)] Loss: 16726.140625\n",
      "Train Epoch: 442 [70080/225000 (31%)] Loss: 16742.511719\n",
      "Train Epoch: 442 [72576/225000 (32%)] Loss: 17261.638672\n",
      "Train Epoch: 442 [75072/225000 (33%)] Loss: 16543.490234\n",
      "Train Epoch: 442 [77568/225000 (34%)] Loss: 16468.425781\n",
      "Train Epoch: 442 [80064/225000 (36%)] Loss: 16800.818359\n",
      "Train Epoch: 442 [82560/225000 (37%)] Loss: 16537.847656\n",
      "Train Epoch: 442 [85056/225000 (38%)] Loss: 16421.181641\n",
      "Train Epoch: 442 [87552/225000 (39%)] Loss: 16486.695312\n",
      "Train Epoch: 442 [90048/225000 (40%)] Loss: 16304.802734\n",
      "Train Epoch: 442 [92544/225000 (41%)] Loss: 17118.207031\n",
      "Train Epoch: 442 [95040/225000 (42%)] Loss: 16150.986328\n",
      "Train Epoch: 442 [97536/225000 (43%)] Loss: 16505.441406\n",
      "Train Epoch: 442 [100032/225000 (44%)] Loss: 16352.413086\n",
      "Train Epoch: 442 [102528/225000 (46%)] Loss: 16933.398438\n",
      "Train Epoch: 442 [105024/225000 (47%)] Loss: 16575.515625\n",
      "Train Epoch: 442 [107520/225000 (48%)] Loss: 17220.144531\n",
      "Train Epoch: 442 [110016/225000 (49%)] Loss: 16550.574219\n",
      "Train Epoch: 442 [112512/225000 (50%)] Loss: 16444.289062\n",
      "Train Epoch: 442 [115008/225000 (51%)] Loss: 18105.013672\n",
      "Train Epoch: 442 [117504/225000 (52%)] Loss: 16823.285156\n",
      "Train Epoch: 442 [120000/225000 (53%)] Loss: 16645.121094\n",
      "Train Epoch: 442 [122496/225000 (54%)] Loss: 16462.759766\n",
      "Train Epoch: 442 [124992/225000 (56%)] Loss: 16919.292969\n",
      "Train Epoch: 442 [127488/225000 (57%)] Loss: 16677.818359\n",
      "Train Epoch: 442 [129984/225000 (58%)] Loss: 16417.292969\n",
      "Train Epoch: 442 [132480/225000 (59%)] Loss: 16356.244141\n",
      "Train Epoch: 442 [134976/225000 (60%)] Loss: 16684.148438\n",
      "Train Epoch: 442 [137472/225000 (61%)] Loss: 16309.730469\n",
      "Train Epoch: 442 [139968/225000 (62%)] Loss: 16856.441406\n",
      "Train Epoch: 442 [142464/225000 (63%)] Loss: 16592.369141\n",
      "Train Epoch: 442 [144960/225000 (64%)] Loss: 17909.042969\n",
      "Train Epoch: 442 [147456/225000 (66%)] Loss: 16858.777344\n",
      "Train Epoch: 442 [149952/225000 (67%)] Loss: 16795.992188\n",
      "Train Epoch: 442 [152448/225000 (68%)] Loss: 16793.375000\n",
      "Train Epoch: 442 [154944/225000 (69%)] Loss: 16384.458984\n",
      "Train Epoch: 442 [157440/225000 (70%)] Loss: 16586.816406\n",
      "Train Epoch: 442 [159936/225000 (71%)] Loss: 16519.724609\n",
      "Train Epoch: 442 [162432/225000 (72%)] Loss: 16799.031250\n",
      "Train Epoch: 442 [164928/225000 (73%)] Loss: 17066.527344\n",
      "Train Epoch: 442 [167424/225000 (74%)] Loss: 16891.462891\n",
      "Train Epoch: 442 [169920/225000 (76%)] Loss: 16125.953125\n",
      "Train Epoch: 442 [172416/225000 (77%)] Loss: 16997.505859\n",
      "Train Epoch: 442 [174912/225000 (78%)] Loss: 17178.710938\n",
      "Train Epoch: 442 [177408/225000 (79%)] Loss: 16692.943359\n",
      "Train Epoch: 442 [179904/225000 (80%)] Loss: 17485.189453\n",
      "Train Epoch: 442 [182400/225000 (81%)] Loss: 16600.890625\n",
      "Train Epoch: 442 [184896/225000 (82%)] Loss: 16853.517578\n",
      "Train Epoch: 442 [187392/225000 (83%)] Loss: 16401.050781\n",
      "Train Epoch: 442 [189888/225000 (84%)] Loss: 16684.199219\n",
      "Train Epoch: 442 [192384/225000 (86%)] Loss: 16330.716797\n",
      "Train Epoch: 442 [194880/225000 (87%)] Loss: 16619.015625\n",
      "Train Epoch: 442 [197376/225000 (88%)] Loss: 16316.541992\n",
      "Train Epoch: 442 [199872/225000 (89%)] Loss: 16546.449219\n",
      "Train Epoch: 442 [202368/225000 (90%)] Loss: 17228.808594\n",
      "Train Epoch: 442 [204864/225000 (91%)] Loss: 16514.486328\n",
      "Train Epoch: 442 [207360/225000 (92%)] Loss: 16851.232422\n",
      "Train Epoch: 442 [209856/225000 (93%)] Loss: 16507.869141\n",
      "Train Epoch: 442 [212352/225000 (94%)] Loss: 16454.289062\n",
      "Train Epoch: 442 [214848/225000 (95%)] Loss: 16011.811523\n",
      "Train Epoch: 442 [217344/225000 (97%)] Loss: 16635.169922\n",
      "Train Epoch: 442 [219840/225000 (98%)] Loss: 16360.685547\n",
      "Train Epoch: 442 [222336/225000 (99%)] Loss: 16640.289062\n",
      "Train Epoch: 442 [224832/225000 (100%)] Loss: 16515.937500\n",
      "    epoch          : 442\n",
      "    loss           : 16729.006555967364\n",
      "    val_loss       : 16631.11275611306\n",
      "Train Epoch: 443 [192/225000 (0%)] Loss: 16424.976562\n",
      "Train Epoch: 443 [2688/225000 (1%)] Loss: 16406.113281\n",
      "Train Epoch: 443 [5184/225000 (2%)] Loss: 17173.960938\n",
      "Train Epoch: 443 [7680/225000 (3%)] Loss: 16582.101562\n",
      "Train Epoch: 443 [10176/225000 (5%)] Loss: 16969.878906\n",
      "Train Epoch: 443 [12672/225000 (6%)] Loss: 16599.958984\n",
      "Train Epoch: 443 [15168/225000 (7%)] Loss: 16466.324219\n",
      "Train Epoch: 443 [17664/225000 (8%)] Loss: 16906.468750\n",
      "Train Epoch: 443 [20160/225000 (9%)] Loss: 16610.488281\n",
      "Train Epoch: 443 [22656/225000 (10%)] Loss: 16612.917969\n",
      "Train Epoch: 443 [25152/225000 (11%)] Loss: 16719.533203\n",
      "Train Epoch: 443 [27648/225000 (12%)] Loss: 17054.066406\n",
      "Train Epoch: 443 [30144/225000 (13%)] Loss: 17185.597656\n",
      "Train Epoch: 443 [32640/225000 (15%)] Loss: 16956.078125\n",
      "Train Epoch: 443 [35136/225000 (16%)] Loss: 16633.941406\n",
      "Train Epoch: 443 [37632/225000 (17%)] Loss: 16630.550781\n",
      "Train Epoch: 443 [40128/225000 (18%)] Loss: 16139.435547\n",
      "Train Epoch: 443 [42624/225000 (19%)] Loss: 16768.187500\n",
      "Train Epoch: 443 [45120/225000 (20%)] Loss: 16882.187500\n",
      "Train Epoch: 443 [47616/225000 (21%)] Loss: 16625.601562\n",
      "Train Epoch: 443 [50112/225000 (22%)] Loss: 16809.412109\n",
      "Train Epoch: 443 [52608/225000 (23%)] Loss: 16542.011719\n",
      "Train Epoch: 443 [55104/225000 (24%)] Loss: 16462.919922\n",
      "Train Epoch: 443 [57600/225000 (26%)] Loss: 16713.066406\n",
      "Train Epoch: 443 [60096/225000 (27%)] Loss: 16783.583984\n",
      "Train Epoch: 443 [62592/225000 (28%)] Loss: 16564.863281\n",
      "Train Epoch: 443 [65088/225000 (29%)] Loss: 16757.031250\n",
      "Train Epoch: 443 [67584/225000 (30%)] Loss: 16925.839844\n",
      "Train Epoch: 443 [70080/225000 (31%)] Loss: 16095.050781\n",
      "Train Epoch: 443 [72576/225000 (32%)] Loss: 17092.777344\n",
      "Train Epoch: 443 [75072/225000 (33%)] Loss: 16864.814453\n",
      "Train Epoch: 443 [77568/225000 (34%)] Loss: 16650.371094\n",
      "Train Epoch: 443 [80064/225000 (36%)] Loss: 16819.314453\n",
      "Train Epoch: 443 [82560/225000 (37%)] Loss: 16887.187500\n",
      "Train Epoch: 443 [85056/225000 (38%)] Loss: 16432.970703\n",
      "Train Epoch: 443 [87552/225000 (39%)] Loss: 16496.654297\n",
      "Train Epoch: 443 [90048/225000 (40%)] Loss: 16513.285156\n",
      "Train Epoch: 443 [92544/225000 (41%)] Loss: 17043.746094\n",
      "Train Epoch: 443 [95040/225000 (42%)] Loss: 16544.183594\n",
      "Train Epoch: 443 [97536/225000 (43%)] Loss: 17054.679688\n",
      "Train Epoch: 443 [100032/225000 (44%)] Loss: 16515.761719\n",
      "Train Epoch: 443 [102528/225000 (46%)] Loss: 16460.666016\n",
      "Train Epoch: 443 [105024/225000 (47%)] Loss: 16321.576172\n",
      "Train Epoch: 443 [107520/225000 (48%)] Loss: 16174.449219\n",
      "Train Epoch: 443 [110016/225000 (49%)] Loss: 16877.195312\n",
      "Train Epoch: 443 [112512/225000 (50%)] Loss: 16528.699219\n",
      "Train Epoch: 443 [115008/225000 (51%)] Loss: 17041.992188\n",
      "Train Epoch: 443 [117504/225000 (52%)] Loss: 16644.367188\n",
      "Train Epoch: 443 [120000/225000 (53%)] Loss: 16844.914062\n",
      "Train Epoch: 443 [122496/225000 (54%)] Loss: 16296.800781\n",
      "Train Epoch: 443 [124992/225000 (56%)] Loss: 16902.732422\n",
      "Train Epoch: 443 [127488/225000 (57%)] Loss: 16374.010742\n",
      "Train Epoch: 443 [129984/225000 (58%)] Loss: 18151.128906\n",
      "Train Epoch: 443 [132480/225000 (59%)] Loss: 16532.210938\n",
      "Train Epoch: 443 [134976/225000 (60%)] Loss: 16807.468750\n",
      "Train Epoch: 443 [137472/225000 (61%)] Loss: 16454.203125\n",
      "Train Epoch: 443 [139968/225000 (62%)] Loss: 16086.573242\n",
      "Train Epoch: 443 [142464/225000 (63%)] Loss: 16855.048828\n",
      "Train Epoch: 443 [144960/225000 (64%)] Loss: 16288.391602\n",
      "Train Epoch: 443 [147456/225000 (66%)] Loss: 16909.816406\n",
      "Train Epoch: 443 [149952/225000 (67%)] Loss: 16646.837891\n",
      "Train Epoch: 443 [152448/225000 (68%)] Loss: 16710.367188\n",
      "Train Epoch: 443 [154944/225000 (69%)] Loss: 16288.222656\n",
      "Train Epoch: 443 [157440/225000 (70%)] Loss: 17033.183594\n",
      "Train Epoch: 443 [159936/225000 (71%)] Loss: 16521.996094\n",
      "Train Epoch: 443 [162432/225000 (72%)] Loss: 16411.929688\n",
      "Train Epoch: 443 [164928/225000 (73%)] Loss: 16829.179688\n",
      "Train Epoch: 443 [167424/225000 (74%)] Loss: 16718.242188\n",
      "Train Epoch: 443 [169920/225000 (76%)] Loss: 16796.093750\n",
      "Train Epoch: 443 [172416/225000 (77%)] Loss: 17278.542969\n",
      "Train Epoch: 443 [174912/225000 (78%)] Loss: 16287.219727\n",
      "Train Epoch: 443 [177408/225000 (79%)] Loss: 17463.447266\n",
      "Train Epoch: 443 [179904/225000 (80%)] Loss: 16837.708984\n",
      "Train Epoch: 443 [182400/225000 (81%)] Loss: 16916.710938\n",
      "Train Epoch: 443 [184896/225000 (82%)] Loss: 16515.361328\n",
      "Train Epoch: 443 [187392/225000 (83%)] Loss: 16917.804688\n",
      "Train Epoch: 443 [189888/225000 (84%)] Loss: 17145.613281\n",
      "Train Epoch: 443 [192384/225000 (86%)] Loss: 16067.457031\n",
      "Train Epoch: 443 [194880/225000 (87%)] Loss: 16745.222656\n",
      "Train Epoch: 443 [197376/225000 (88%)] Loss: 16545.453125\n",
      "Train Epoch: 443 [199872/225000 (89%)] Loss: 15890.134766\n",
      "Train Epoch: 443 [202368/225000 (90%)] Loss: 16491.646484\n",
      "Train Epoch: 443 [204864/225000 (91%)] Loss: 16868.437500\n",
      "Train Epoch: 443 [207360/225000 (92%)] Loss: 16771.576172\n",
      "Train Epoch: 443 [209856/225000 (93%)] Loss: 16704.871094\n",
      "Train Epoch: 443 [212352/225000 (94%)] Loss: 16577.027344\n",
      "Train Epoch: 443 [214848/225000 (95%)] Loss: 16370.122070\n",
      "Train Epoch: 443 [217344/225000 (97%)] Loss: 16557.410156\n",
      "Train Epoch: 443 [219840/225000 (98%)] Loss: 16323.622070\n",
      "Train Epoch: 443 [222336/225000 (99%)] Loss: 16714.210938\n",
      "Train Epoch: 443 [224832/225000 (100%)] Loss: 16981.824219\n",
      "    epoch          : 443\n",
      "    loss           : 16704.22189466457\n",
      "    val_loss       : 16637.17045529926\n",
      "Train Epoch: 444 [192/225000 (0%)] Loss: 17182.062500\n",
      "Train Epoch: 444 [2688/225000 (1%)] Loss: 16699.585938\n",
      "Train Epoch: 444 [5184/225000 (2%)] Loss: 16782.125000\n",
      "Train Epoch: 444 [7680/225000 (3%)] Loss: 16582.539062\n",
      "Train Epoch: 444 [10176/225000 (5%)] Loss: 16705.050781\n",
      "Train Epoch: 444 [12672/225000 (6%)] Loss: 16520.705078\n",
      "Train Epoch: 444 [15168/225000 (7%)] Loss: 16683.699219\n",
      "Train Epoch: 444 [17664/225000 (8%)] Loss: 16767.896484\n",
      "Train Epoch: 444 [20160/225000 (9%)] Loss: 16527.394531\n",
      "Train Epoch: 444 [22656/225000 (10%)] Loss: 16828.117188\n",
      "Train Epoch: 444 [25152/225000 (11%)] Loss: 16995.093750\n",
      "Train Epoch: 444 [27648/225000 (12%)] Loss: 17795.976562\n",
      "Train Epoch: 444 [30144/225000 (13%)] Loss: 16529.800781\n",
      "Train Epoch: 444 [32640/225000 (15%)] Loss: 16276.465820\n",
      "Train Epoch: 444 [35136/225000 (16%)] Loss: 17067.673828\n",
      "Train Epoch: 444 [37632/225000 (17%)] Loss: 16499.037109\n",
      "Train Epoch: 444 [40128/225000 (18%)] Loss: 16484.550781\n",
      "Train Epoch: 444 [42624/225000 (19%)] Loss: 16968.128906\n",
      "Train Epoch: 444 [45120/225000 (20%)] Loss: 16503.521484\n",
      "Train Epoch: 444 [47616/225000 (21%)] Loss: 16822.195312\n",
      "Train Epoch: 444 [50112/225000 (22%)] Loss: 16740.753906\n",
      "Train Epoch: 444 [52608/225000 (23%)] Loss: 15997.409180\n",
      "Train Epoch: 444 [55104/225000 (24%)] Loss: 16902.585938\n",
      "Train Epoch: 444 [57600/225000 (26%)] Loss: 16797.332031\n",
      "Train Epoch: 444 [60096/225000 (27%)] Loss: 17116.847656\n",
      "Train Epoch: 444 [62592/225000 (28%)] Loss: 16280.938477\n",
      "Train Epoch: 444 [65088/225000 (29%)] Loss: 16567.728516\n",
      "Train Epoch: 444 [67584/225000 (30%)] Loss: 16515.527344\n",
      "Train Epoch: 444 [70080/225000 (31%)] Loss: 16417.714844\n",
      "Train Epoch: 444 [72576/225000 (32%)] Loss: 16415.751953\n",
      "Train Epoch: 444 [75072/225000 (33%)] Loss: 16652.781250\n",
      "Train Epoch: 444 [77568/225000 (34%)] Loss: 16492.320312\n",
      "Train Epoch: 444 [80064/225000 (36%)] Loss: 16626.371094\n",
      "Train Epoch: 444 [82560/225000 (37%)] Loss: 16766.144531\n",
      "Train Epoch: 444 [85056/225000 (38%)] Loss: 16990.167969\n",
      "Train Epoch: 444 [87552/225000 (39%)] Loss: 16904.406250\n",
      "Train Epoch: 444 [90048/225000 (40%)] Loss: 16283.365234\n",
      "Train Epoch: 444 [92544/225000 (41%)] Loss: 16144.650391\n",
      "Train Epoch: 444 [95040/225000 (42%)] Loss: 16583.324219\n",
      "Train Epoch: 444 [97536/225000 (43%)] Loss: 16341.278320\n",
      "Train Epoch: 444 [100032/225000 (44%)] Loss: 16820.777344\n",
      "Train Epoch: 444 [102528/225000 (46%)] Loss: 16219.603516\n",
      "Train Epoch: 444 [105024/225000 (47%)] Loss: 16443.000000\n",
      "Train Epoch: 444 [107520/225000 (48%)] Loss: 16924.773438\n",
      "Train Epoch: 444 [110016/225000 (49%)] Loss: 16552.052734\n",
      "Train Epoch: 444 [112512/225000 (50%)] Loss: 16616.859375\n",
      "Train Epoch: 444 [115008/225000 (51%)] Loss: 16092.127930\n",
      "Train Epoch: 444 [117504/225000 (52%)] Loss: 16306.720703\n",
      "Train Epoch: 444 [120000/225000 (53%)] Loss: 16675.500000\n",
      "Train Epoch: 444 [122496/225000 (54%)] Loss: 16216.500000\n",
      "Train Epoch: 444 [124992/225000 (56%)] Loss: 16434.964844\n",
      "Train Epoch: 444 [127488/225000 (57%)] Loss: 16396.550781\n",
      "Train Epoch: 444 [129984/225000 (58%)] Loss: 16566.511719\n",
      "Train Epoch: 444 [132480/225000 (59%)] Loss: 16527.312500\n",
      "Train Epoch: 444 [134976/225000 (60%)] Loss: 16587.531250\n",
      "Train Epoch: 444 [137472/225000 (61%)] Loss: 16726.250000\n",
      "Train Epoch: 444 [139968/225000 (62%)] Loss: 16469.039062\n",
      "Train Epoch: 444 [142464/225000 (63%)] Loss: 18244.750000\n",
      "Train Epoch: 444 [144960/225000 (64%)] Loss: 16501.001953\n",
      "Train Epoch: 444 [147456/225000 (66%)] Loss: 16076.357422\n",
      "Train Epoch: 444 [149952/225000 (67%)] Loss: 17251.585938\n",
      "Train Epoch: 444 [152448/225000 (68%)] Loss: 16649.375000\n",
      "Train Epoch: 444 [154944/225000 (69%)] Loss: 16361.502930\n",
      "Train Epoch: 444 [157440/225000 (70%)] Loss: 16544.707031\n",
      "Train Epoch: 444 [159936/225000 (71%)] Loss: 16886.394531\n",
      "Train Epoch: 444 [162432/225000 (72%)] Loss: 16595.718750\n",
      "Train Epoch: 444 [164928/225000 (73%)] Loss: 16396.708984\n",
      "Train Epoch: 444 [167424/225000 (74%)] Loss: 16853.718750\n",
      "Train Epoch: 444 [169920/225000 (76%)] Loss: 16228.206055\n",
      "Train Epoch: 444 [172416/225000 (77%)] Loss: 17358.539062\n",
      "Train Epoch: 444 [174912/225000 (78%)] Loss: 16690.820312\n",
      "Train Epoch: 444 [177408/225000 (79%)] Loss: 16969.027344\n",
      "Train Epoch: 444 [179904/225000 (80%)] Loss: 16445.578125\n",
      "Train Epoch: 444 [182400/225000 (81%)] Loss: 16537.722656\n",
      "Train Epoch: 444 [184896/225000 (82%)] Loss: 16573.722656\n",
      "Train Epoch: 444 [187392/225000 (83%)] Loss: 16606.626953\n",
      "Train Epoch: 444 [189888/225000 (84%)] Loss: 16875.470703\n",
      "Train Epoch: 444 [192384/225000 (86%)] Loss: 16443.742188\n",
      "Train Epoch: 444 [194880/225000 (87%)] Loss: 16112.882812\n",
      "Train Epoch: 444 [197376/225000 (88%)] Loss: 16948.269531\n",
      "Train Epoch: 444 [199872/225000 (89%)] Loss: 16880.728516\n",
      "Train Epoch: 444 [202368/225000 (90%)] Loss: 16972.941406\n",
      "Train Epoch: 444 [204864/225000 (91%)] Loss: 17851.443359\n",
      "Train Epoch: 444 [207360/225000 (92%)] Loss: 16426.027344\n",
      "Train Epoch: 444 [209856/225000 (93%)] Loss: 16379.608398\n",
      "Train Epoch: 444 [212352/225000 (94%)] Loss: 16697.775391\n",
      "Train Epoch: 444 [214848/225000 (95%)] Loss: 16790.593750\n",
      "Train Epoch: 444 [217344/225000 (97%)] Loss: 16630.187500\n",
      "Train Epoch: 444 [219840/225000 (98%)] Loss: 16801.712891\n",
      "Train Epoch: 444 [222336/225000 (99%)] Loss: 16448.875000\n",
      "Train Epoch: 444 [224832/225000 (100%)] Loss: 16798.166016\n",
      "    epoch          : 444\n",
      "    loss           : 16713.460289235816\n",
      "    val_loss       : 16656.558518524387\n",
      "Train Epoch: 445 [192/225000 (0%)] Loss: 16477.648438\n",
      "Train Epoch: 445 [2688/225000 (1%)] Loss: 16577.222656\n",
      "Train Epoch: 445 [5184/225000 (2%)] Loss: 17069.730469\n",
      "Train Epoch: 445 [7680/225000 (3%)] Loss: 16675.488281\n",
      "Train Epoch: 445 [10176/225000 (5%)] Loss: 15989.982422\n",
      "Train Epoch: 445 [12672/225000 (6%)] Loss: 16602.039062\n",
      "Train Epoch: 445 [15168/225000 (7%)] Loss: 16329.918945\n",
      "Train Epoch: 445 [17664/225000 (8%)] Loss: 16882.076172\n",
      "Train Epoch: 445 [20160/225000 (9%)] Loss: 16611.160156\n",
      "Train Epoch: 445 [22656/225000 (10%)] Loss: 16794.794922\n",
      "Train Epoch: 445 [25152/225000 (11%)] Loss: 16445.406250\n",
      "Train Epoch: 445 [27648/225000 (12%)] Loss: 16494.806641\n",
      "Train Epoch: 445 [30144/225000 (13%)] Loss: 16759.605469\n",
      "Train Epoch: 445 [32640/225000 (15%)] Loss: 16781.599609\n",
      "Train Epoch: 445 [35136/225000 (16%)] Loss: 16870.478516\n",
      "Train Epoch: 445 [37632/225000 (17%)] Loss: 16497.437500\n",
      "Train Epoch: 445 [40128/225000 (18%)] Loss: 16923.146484\n",
      "Train Epoch: 445 [42624/225000 (19%)] Loss: 16975.835938\n",
      "Train Epoch: 445 [45120/225000 (20%)] Loss: 16335.474609\n",
      "Train Epoch: 445 [47616/225000 (21%)] Loss: 16540.123047\n",
      "Train Epoch: 445 [50112/225000 (22%)] Loss: 16778.773438\n",
      "Train Epoch: 445 [52608/225000 (23%)] Loss: 16954.121094\n",
      "Train Epoch: 445 [55104/225000 (24%)] Loss: 16517.589844\n",
      "Train Epoch: 445 [57600/225000 (26%)] Loss: 17986.824219\n",
      "Train Epoch: 445 [60096/225000 (27%)] Loss: 16967.265625\n",
      "Train Epoch: 445 [62592/225000 (28%)] Loss: 16337.793945\n",
      "Train Epoch: 445 [65088/225000 (29%)] Loss: 16377.510742\n",
      "Train Epoch: 445 [67584/225000 (30%)] Loss: 16474.695312\n",
      "Train Epoch: 445 [70080/225000 (31%)] Loss: 18180.261719\n",
      "Train Epoch: 445 [72576/225000 (32%)] Loss: 18527.851562\n",
      "Train Epoch: 445 [75072/225000 (33%)] Loss: 17034.902344\n",
      "Train Epoch: 445 [77568/225000 (34%)] Loss: 16640.039062\n",
      "Train Epoch: 445 [80064/225000 (36%)] Loss: 16677.222656\n",
      "Train Epoch: 445 [82560/225000 (37%)] Loss: 16914.300781\n",
      "Train Epoch: 445 [85056/225000 (38%)] Loss: 16745.898438\n",
      "Train Epoch: 445 [87552/225000 (39%)] Loss: 16441.410156\n",
      "Train Epoch: 445 [90048/225000 (40%)] Loss: 16135.719727\n",
      "Train Epoch: 445 [92544/225000 (41%)] Loss: 16854.623047\n",
      "Train Epoch: 445 [95040/225000 (42%)] Loss: 16141.768555\n",
      "Train Epoch: 445 [97536/225000 (43%)] Loss: 18565.957031\n",
      "Train Epoch: 445 [100032/225000 (44%)] Loss: 16985.626953\n",
      "Train Epoch: 445 [102528/225000 (46%)] Loss: 16390.892578\n",
      "Train Epoch: 445 [105024/225000 (47%)] Loss: 16480.593750\n",
      "Train Epoch: 445 [107520/225000 (48%)] Loss: 16792.625000\n",
      "Train Epoch: 445 [110016/225000 (49%)] Loss: 16813.691406\n",
      "Train Epoch: 445 [112512/225000 (50%)] Loss: 16558.238281\n",
      "Train Epoch: 445 [115008/225000 (51%)] Loss: 16379.532227\n",
      "Train Epoch: 445 [117504/225000 (52%)] Loss: 16730.406250\n",
      "Train Epoch: 445 [120000/225000 (53%)] Loss: 16411.029297\n",
      "Train Epoch: 445 [122496/225000 (54%)] Loss: 17944.152344\n",
      "Train Epoch: 445 [124992/225000 (56%)] Loss: 16822.296875\n",
      "Train Epoch: 445 [127488/225000 (57%)] Loss: 16474.875000\n",
      "Train Epoch: 445 [129984/225000 (58%)] Loss: 16375.992188\n",
      "Train Epoch: 445 [132480/225000 (59%)] Loss: 16462.740234\n",
      "Train Epoch: 445 [134976/225000 (60%)] Loss: 16570.599609\n",
      "Train Epoch: 445 [137472/225000 (61%)] Loss: 16908.160156\n",
      "Train Epoch: 445 [139968/225000 (62%)] Loss: 16869.851562\n",
      "Train Epoch: 445 [142464/225000 (63%)] Loss: 16621.236328\n",
      "Train Epoch: 445 [144960/225000 (64%)] Loss: 16558.980469\n",
      "Train Epoch: 445 [147456/225000 (66%)] Loss: 16897.226562\n",
      "Train Epoch: 445 [149952/225000 (67%)] Loss: 16995.242188\n",
      "Train Epoch: 445 [152448/225000 (68%)] Loss: 16510.355469\n",
      "Train Epoch: 445 [154944/225000 (69%)] Loss: 16350.204102\n",
      "Train Epoch: 445 [157440/225000 (70%)] Loss: 17102.365234\n",
      "Train Epoch: 445 [159936/225000 (71%)] Loss: 16861.798828\n",
      "Train Epoch: 445 [162432/225000 (72%)] Loss: 16339.126953\n",
      "Train Epoch: 445 [164928/225000 (73%)] Loss: 16176.119141\n",
      "Train Epoch: 445 [167424/225000 (74%)] Loss: 16668.632812\n",
      "Train Epoch: 445 [169920/225000 (76%)] Loss: 16675.085938\n",
      "Train Epoch: 445 [172416/225000 (77%)] Loss: 16705.406250\n",
      "Train Epoch: 445 [174912/225000 (78%)] Loss: 18161.859375\n",
      "Train Epoch: 445 [177408/225000 (79%)] Loss: 16898.718750\n",
      "Train Epoch: 445 [179904/225000 (80%)] Loss: 16675.314453\n",
      "Train Epoch: 445 [182400/225000 (81%)] Loss: 16213.901367\n",
      "Train Epoch: 445 [184896/225000 (82%)] Loss: 18420.818359\n",
      "Train Epoch: 445 [187392/225000 (83%)] Loss: 16604.201172\n",
      "Train Epoch: 445 [189888/225000 (84%)] Loss: 16671.214844\n",
      "Train Epoch: 445 [192384/225000 (86%)] Loss: 16693.414062\n",
      "Train Epoch: 445 [194880/225000 (87%)] Loss: 16707.000000\n",
      "Train Epoch: 445 [197376/225000 (88%)] Loss: 16644.296875\n",
      "Train Epoch: 445 [199872/225000 (89%)] Loss: 16884.386719\n",
      "Train Epoch: 445 [202368/225000 (90%)] Loss: 18237.929688\n",
      "Train Epoch: 445 [204864/225000 (91%)] Loss: 16552.464844\n",
      "Train Epoch: 445 [207360/225000 (92%)] Loss: 16743.554688\n",
      "Train Epoch: 445 [209856/225000 (93%)] Loss: 16772.554688\n",
      "Train Epoch: 445 [212352/225000 (94%)] Loss: 16892.214844\n",
      "Train Epoch: 445 [214848/225000 (95%)] Loss: 16449.980469\n",
      "Train Epoch: 445 [217344/225000 (97%)] Loss: 16864.820312\n",
      "Train Epoch: 445 [219840/225000 (98%)] Loss: 16668.414062\n",
      "Train Epoch: 445 [222336/225000 (99%)] Loss: 16425.341797\n",
      "Train Epoch: 445 [224832/225000 (100%)] Loss: 16054.678711\n",
      "    epoch          : 445\n",
      "    loss           : 16722.79745077192\n",
      "    val_loss       : 16619.968431966907\n",
      "Train Epoch: 446 [192/225000 (0%)] Loss: 16678.390625\n",
      "Train Epoch: 446 [2688/225000 (1%)] Loss: 16484.962891\n",
      "Train Epoch: 446 [5184/225000 (2%)] Loss: 16517.144531\n",
      "Train Epoch: 446 [7680/225000 (3%)] Loss: 16899.371094\n",
      "Train Epoch: 446 [10176/225000 (5%)] Loss: 16754.441406\n",
      "Train Epoch: 446 [12672/225000 (6%)] Loss: 18065.328125\n",
      "Train Epoch: 446 [15168/225000 (7%)] Loss: 16689.798828\n",
      "Train Epoch: 446 [17664/225000 (8%)] Loss: 16246.685547\n",
      "Train Epoch: 446 [20160/225000 (9%)] Loss: 16697.035156\n",
      "Train Epoch: 446 [22656/225000 (10%)] Loss: 17031.947266\n",
      "Train Epoch: 446 [25152/225000 (11%)] Loss: 16620.384766\n",
      "Train Epoch: 446 [27648/225000 (12%)] Loss: 16941.205078\n",
      "Train Epoch: 446 [30144/225000 (13%)] Loss: 16496.101562\n",
      "Train Epoch: 446 [32640/225000 (15%)] Loss: 16673.878906\n",
      "Train Epoch: 446 [35136/225000 (16%)] Loss: 16142.964844\n",
      "Train Epoch: 446 [37632/225000 (17%)] Loss: 16413.367188\n",
      "Train Epoch: 446 [40128/225000 (18%)] Loss: 16806.097656\n",
      "Train Epoch: 446 [42624/225000 (19%)] Loss: 16782.714844\n",
      "Train Epoch: 446 [45120/225000 (20%)] Loss: 16639.171875\n",
      "Train Epoch: 446 [47616/225000 (21%)] Loss: 16796.007812\n",
      "Train Epoch: 446 [50112/225000 (22%)] Loss: 16686.531250\n",
      "Train Epoch: 446 [52608/225000 (23%)] Loss: 16713.796875\n",
      "Train Epoch: 446 [55104/225000 (24%)] Loss: 16733.851562\n",
      "Train Epoch: 446 [57600/225000 (26%)] Loss: 16481.187500\n",
      "Train Epoch: 446 [60096/225000 (27%)] Loss: 16947.578125\n",
      "Train Epoch: 446 [62592/225000 (28%)] Loss: 18212.734375\n",
      "Train Epoch: 446 [65088/225000 (29%)] Loss: 16397.273438\n",
      "Train Epoch: 446 [67584/225000 (30%)] Loss: 16659.089844\n",
      "Train Epoch: 446 [70080/225000 (31%)] Loss: 16429.503906\n",
      "Train Epoch: 446 [72576/225000 (32%)] Loss: 16455.593750\n",
      "Train Epoch: 446 [75072/225000 (33%)] Loss: 15891.230469\n",
      "Train Epoch: 446 [77568/225000 (34%)] Loss: 16674.265625\n",
      "Train Epoch: 446 [80064/225000 (36%)] Loss: 16854.878906\n",
      "Train Epoch: 446 [82560/225000 (37%)] Loss: 16416.484375\n",
      "Train Epoch: 446 [85056/225000 (38%)] Loss: 16852.470703\n",
      "Train Epoch: 446 [87552/225000 (39%)] Loss: 16290.125000\n",
      "Train Epoch: 446 [90048/225000 (40%)] Loss: 16619.677734\n",
      "Train Epoch: 446 [92544/225000 (41%)] Loss: 16744.824219\n",
      "Train Epoch: 446 [95040/225000 (42%)] Loss: 16573.671875\n",
      "Train Epoch: 446 [97536/225000 (43%)] Loss: 16744.984375\n",
      "Train Epoch: 446 [100032/225000 (44%)] Loss: 16626.464844\n",
      "Train Epoch: 446 [102528/225000 (46%)] Loss: 16603.699219\n",
      "Train Epoch: 446 [105024/225000 (47%)] Loss: 16728.238281\n",
      "Train Epoch: 446 [107520/225000 (48%)] Loss: 16594.976562\n",
      "Train Epoch: 446 [110016/225000 (49%)] Loss: 16704.597656\n",
      "Train Epoch: 446 [112512/225000 (50%)] Loss: 16531.796875\n",
      "Train Epoch: 446 [115008/225000 (51%)] Loss: 16786.738281\n",
      "Train Epoch: 446 [117504/225000 (52%)] Loss: 16556.234375\n",
      "Train Epoch: 446 [120000/225000 (53%)] Loss: 16544.683594\n",
      "Train Epoch: 446 [122496/225000 (54%)] Loss: 16635.628906\n",
      "Train Epoch: 446 [124992/225000 (56%)] Loss: 17068.550781\n",
      "Train Epoch: 446 [127488/225000 (57%)] Loss: 16863.160156\n",
      "Train Epoch: 446 [129984/225000 (58%)] Loss: 16652.132812\n",
      "Train Epoch: 446 [132480/225000 (59%)] Loss: 16535.710938\n",
      "Train Epoch: 446 [134976/225000 (60%)] Loss: 16814.757812\n",
      "Train Epoch: 446 [137472/225000 (61%)] Loss: 16180.299805\n",
      "Train Epoch: 446 [139968/225000 (62%)] Loss: 16856.441406\n",
      "Train Epoch: 446 [142464/225000 (63%)] Loss: 16445.730469\n",
      "Train Epoch: 446 [144960/225000 (64%)] Loss: 16533.601562\n",
      "Train Epoch: 446 [147456/225000 (66%)] Loss: 16154.655273\n",
      "Train Epoch: 446 [149952/225000 (67%)] Loss: 17279.160156\n",
      "Train Epoch: 446 [152448/225000 (68%)] Loss: 16415.175781\n",
      "Train Epoch: 446 [154944/225000 (69%)] Loss: 18381.513672\n",
      "Train Epoch: 446 [157440/225000 (70%)] Loss: 16139.442383\n",
      "Train Epoch: 446 [159936/225000 (71%)] Loss: 16798.019531\n",
      "Train Epoch: 446 [162432/225000 (72%)] Loss: 16931.355469\n",
      "Train Epoch: 446 [164928/225000 (73%)] Loss: 16475.437500\n",
      "Train Epoch: 446 [167424/225000 (74%)] Loss: 16504.308594\n",
      "Train Epoch: 446 [169920/225000 (76%)] Loss: 16619.492188\n",
      "Train Epoch: 446 [172416/225000 (77%)] Loss: 16775.730469\n",
      "Train Epoch: 446 [174912/225000 (78%)] Loss: 16518.082031\n",
      "Train Epoch: 446 [177408/225000 (79%)] Loss: 16229.112305\n",
      "Train Epoch: 446 [179904/225000 (80%)] Loss: 16695.703125\n",
      "Train Epoch: 446 [182400/225000 (81%)] Loss: 16495.601562\n",
      "Train Epoch: 446 [184896/225000 (82%)] Loss: 16699.539062\n",
      "Train Epoch: 446 [187392/225000 (83%)] Loss: 17003.082031\n",
      "Train Epoch: 446 [189888/225000 (84%)] Loss: 16512.126953\n",
      "Train Epoch: 446 [192384/225000 (86%)] Loss: 16401.648438\n",
      "Train Epoch: 446 [194880/225000 (87%)] Loss: 16521.851562\n",
      "Train Epoch: 446 [197376/225000 (88%)] Loss: 16219.254883\n",
      "Train Epoch: 446 [199872/225000 (89%)] Loss: 16738.603516\n",
      "Train Epoch: 446 [202368/225000 (90%)] Loss: 16664.195312\n",
      "Train Epoch: 446 [204864/225000 (91%)] Loss: 16246.857422\n",
      "Train Epoch: 446 [207360/225000 (92%)] Loss: 17084.953125\n",
      "Train Epoch: 446 [209856/225000 (93%)] Loss: 16492.941406\n",
      "Train Epoch: 446 [212352/225000 (94%)] Loss: 16387.378906\n",
      "Train Epoch: 446 [214848/225000 (95%)] Loss: 16767.197266\n",
      "Train Epoch: 446 [217344/225000 (97%)] Loss: 16691.644531\n",
      "Train Epoch: 446 [219840/225000 (98%)] Loss: 17095.503906\n",
      "Train Epoch: 446 [222336/225000 (99%)] Loss: 16472.599609\n",
      "Train Epoch: 446 [224832/225000 (100%)] Loss: 17087.775391\n",
      "    epoch          : 446\n",
      "    loss           : 16725.80098206191\n",
      "    val_loss       : 16636.655123308414\n",
      "Train Epoch: 447 [192/225000 (0%)] Loss: 16705.636719\n",
      "Train Epoch: 447 [2688/225000 (1%)] Loss: 16690.144531\n",
      "Train Epoch: 447 [5184/225000 (2%)] Loss: 16940.992188\n",
      "Train Epoch: 447 [7680/225000 (3%)] Loss: 16434.712891\n",
      "Train Epoch: 447 [10176/225000 (5%)] Loss: 16329.307617\n",
      "Train Epoch: 447 [12672/225000 (6%)] Loss: 16493.937500\n",
      "Train Epoch: 447 [15168/225000 (7%)] Loss: 16490.636719\n",
      "Train Epoch: 447 [17664/225000 (8%)] Loss: 16793.173828\n",
      "Train Epoch: 447 [20160/225000 (9%)] Loss: 16639.314453\n",
      "Train Epoch: 447 [22656/225000 (10%)] Loss: 16514.591797\n",
      "Train Epoch: 447 [25152/225000 (11%)] Loss: 16527.990234\n",
      "Train Epoch: 447 [27648/225000 (12%)] Loss: 16753.562500\n",
      "Train Epoch: 447 [30144/225000 (13%)] Loss: 16926.658203\n",
      "Train Epoch: 447 [32640/225000 (15%)] Loss: 16886.042969\n",
      "Train Epoch: 447 [35136/225000 (16%)] Loss: 16795.562500\n",
      "Train Epoch: 447 [37632/225000 (17%)] Loss: 16324.624023\n",
      "Train Epoch: 447 [40128/225000 (18%)] Loss: 16781.382812\n",
      "Train Epoch: 447 [42624/225000 (19%)] Loss: 16725.703125\n",
      "Train Epoch: 447 [45120/225000 (20%)] Loss: 16710.216797\n",
      "Train Epoch: 447 [47616/225000 (21%)] Loss: 16576.332031\n",
      "Train Epoch: 447 [50112/225000 (22%)] Loss: 17304.484375\n",
      "Train Epoch: 447 [52608/225000 (23%)] Loss: 16225.055664\n",
      "Train Epoch: 447 [55104/225000 (24%)] Loss: 16662.519531\n",
      "Train Epoch: 447 [57600/225000 (26%)] Loss: 16936.921875\n",
      "Train Epoch: 447 [60096/225000 (27%)] Loss: 16316.362305\n",
      "Train Epoch: 447 [62592/225000 (28%)] Loss: 18419.964844\n",
      "Train Epoch: 447 [65088/225000 (29%)] Loss: 16313.139648\n",
      "Train Epoch: 447 [67584/225000 (30%)] Loss: 16583.830078\n",
      "Train Epoch: 447 [70080/225000 (31%)] Loss: 16673.105469\n",
      "Train Epoch: 447 [72576/225000 (32%)] Loss: 17041.375000\n",
      "Train Epoch: 447 [75072/225000 (33%)] Loss: 16432.513672\n",
      "Train Epoch: 447 [77568/225000 (34%)] Loss: 16729.447266\n",
      "Train Epoch: 447 [80064/225000 (36%)] Loss: 16823.871094\n",
      "Train Epoch: 447 [82560/225000 (37%)] Loss: 16348.633789\n",
      "Train Epoch: 447 [85056/225000 (38%)] Loss: 16704.033203\n",
      "Train Epoch: 447 [87552/225000 (39%)] Loss: 16892.144531\n",
      "Train Epoch: 447 [90048/225000 (40%)] Loss: 16448.070312\n",
      "Train Epoch: 447 [92544/225000 (41%)] Loss: 17669.792969\n",
      "Train Epoch: 447 [95040/225000 (42%)] Loss: 16375.914062\n",
      "Train Epoch: 447 [97536/225000 (43%)] Loss: 16521.210938\n",
      "Train Epoch: 447 [100032/225000 (44%)] Loss: 16688.130859\n",
      "Train Epoch: 447 [102528/225000 (46%)] Loss: 16922.599609\n",
      "Train Epoch: 447 [105024/225000 (47%)] Loss: 17811.222656\n",
      "Train Epoch: 447 [107520/225000 (48%)] Loss: 16701.421875\n",
      "Train Epoch: 447 [110016/225000 (49%)] Loss: 16746.001953\n",
      "Train Epoch: 447 [112512/225000 (50%)] Loss: 16176.446289\n",
      "Train Epoch: 447 [115008/225000 (51%)] Loss: 16375.124023\n",
      "Train Epoch: 447 [117504/225000 (52%)] Loss: 17292.601562\n",
      "Train Epoch: 447 [120000/225000 (53%)] Loss: 16449.261719\n",
      "Train Epoch: 447 [122496/225000 (54%)] Loss: 16960.742188\n",
      "Train Epoch: 447 [124992/225000 (56%)] Loss: 16782.792969\n",
      "Train Epoch: 447 [127488/225000 (57%)] Loss: 16760.089844\n",
      "Train Epoch: 447 [129984/225000 (58%)] Loss: 16641.433594\n",
      "Train Epoch: 447 [132480/225000 (59%)] Loss: 17099.808594\n",
      "Train Epoch: 447 [134976/225000 (60%)] Loss: 16554.388672\n",
      "Train Epoch: 447 [137472/225000 (61%)] Loss: 16608.996094\n",
      "Train Epoch: 447 [139968/225000 (62%)] Loss: 16876.099609\n",
      "Train Epoch: 447 [142464/225000 (63%)] Loss: 16534.453125\n",
      "Train Epoch: 447 [144960/225000 (64%)] Loss: 16534.089844\n",
      "Train Epoch: 447 [147456/225000 (66%)] Loss: 16487.783203\n",
      "Train Epoch: 447 [149952/225000 (67%)] Loss: 16335.285156\n",
      "Train Epoch: 447 [152448/225000 (68%)] Loss: 16900.656250\n",
      "Train Epoch: 447 [154944/225000 (69%)] Loss: 16915.644531\n",
      "Train Epoch: 447 [157440/225000 (70%)] Loss: 16617.714844\n",
      "Train Epoch: 447 [159936/225000 (71%)] Loss: 16791.781250\n",
      "Train Epoch: 447 [162432/225000 (72%)] Loss: 16833.746094\n",
      "Train Epoch: 447 [164928/225000 (73%)] Loss: 16791.335938\n",
      "Train Epoch: 447 [167424/225000 (74%)] Loss: 16346.130859\n",
      "Train Epoch: 447 [169920/225000 (76%)] Loss: 17204.007812\n",
      "Train Epoch: 447 [172416/225000 (77%)] Loss: 16290.507812\n",
      "Train Epoch: 447 [174912/225000 (78%)] Loss: 16409.289062\n",
      "Train Epoch: 447 [177408/225000 (79%)] Loss: 16769.050781\n",
      "Train Epoch: 447 [179904/225000 (80%)] Loss: 16397.738281\n",
      "Train Epoch: 447 [182400/225000 (81%)] Loss: 15935.198242\n",
      "Train Epoch: 447 [184896/225000 (82%)] Loss: 16595.355469\n",
      "Train Epoch: 447 [187392/225000 (83%)] Loss: 16586.968750\n",
      "Train Epoch: 447 [189888/225000 (84%)] Loss: 16285.457031\n",
      "Train Epoch: 447 [192384/225000 (86%)] Loss: 16688.900391\n",
      "Train Epoch: 447 [194880/225000 (87%)] Loss: 16710.781250\n",
      "Train Epoch: 447 [197376/225000 (88%)] Loss: 16856.703125\n",
      "Train Epoch: 447 [199872/225000 (89%)] Loss: 17939.953125\n",
      "Train Epoch: 447 [202368/225000 (90%)] Loss: 16963.058594\n",
      "Train Epoch: 447 [204864/225000 (91%)] Loss: 16091.967773\n",
      "Train Epoch: 447 [207360/225000 (92%)] Loss: 16928.894531\n",
      "Train Epoch: 447 [209856/225000 (93%)] Loss: 16505.228516\n",
      "Train Epoch: 447 [212352/225000 (94%)] Loss: 17149.761719\n",
      "Train Epoch: 447 [214848/225000 (95%)] Loss: 16844.804688\n",
      "Train Epoch: 447 [217344/225000 (97%)] Loss: 16358.578125\n",
      "Train Epoch: 447 [219840/225000 (98%)] Loss: 16978.929688\n",
      "Train Epoch: 447 [222336/225000 (99%)] Loss: 16575.753906\n",
      "Train Epoch: 447 [224832/225000 (100%)] Loss: 16600.402344\n",
      "    epoch          : 447\n",
      "    loss           : 16699.988579551515\n",
      "    val_loss       : 16679.042580025798\n",
      "Train Epoch: 448 [192/225000 (0%)] Loss: 16505.771484\n",
      "Train Epoch: 448 [2688/225000 (1%)] Loss: 16162.341797\n",
      "Train Epoch: 448 [5184/225000 (2%)] Loss: 16932.976562\n",
      "Train Epoch: 448 [7680/225000 (3%)] Loss: 16865.398438\n",
      "Train Epoch: 448 [10176/225000 (5%)] Loss: 16798.673828\n",
      "Train Epoch: 448 [12672/225000 (6%)] Loss: 16843.238281\n",
      "Train Epoch: 448 [15168/225000 (7%)] Loss: 16349.865234\n",
      "Train Epoch: 448 [17664/225000 (8%)] Loss: 17102.802734\n",
      "Train Epoch: 448 [20160/225000 (9%)] Loss: 16626.330078\n",
      "Train Epoch: 448 [22656/225000 (10%)] Loss: 16805.546875\n",
      "Train Epoch: 448 [25152/225000 (11%)] Loss: 16495.253906\n",
      "Train Epoch: 448 [27648/225000 (12%)] Loss: 16666.273438\n",
      "Train Epoch: 448 [30144/225000 (13%)] Loss: 16599.378906\n",
      "Train Epoch: 448 [32640/225000 (15%)] Loss: 16814.109375\n",
      "Train Epoch: 448 [35136/225000 (16%)] Loss: 16187.010742\n",
      "Train Epoch: 448 [37632/225000 (17%)] Loss: 16708.578125\n",
      "Train Epoch: 448 [40128/225000 (18%)] Loss: 16639.394531\n",
      "Train Epoch: 448 [42624/225000 (19%)] Loss: 16671.312500\n",
      "Train Epoch: 448 [45120/225000 (20%)] Loss: 16563.765625\n",
      "Train Epoch: 448 [47616/225000 (21%)] Loss: 16797.906250\n",
      "Train Epoch: 448 [50112/225000 (22%)] Loss: 16705.953125\n",
      "Train Epoch: 448 [52608/225000 (23%)] Loss: 16604.955078\n",
      "Train Epoch: 448 [55104/225000 (24%)] Loss: 16929.718750\n",
      "Train Epoch: 448 [57600/225000 (26%)] Loss: 16843.103516\n",
      "Train Epoch: 448 [60096/225000 (27%)] Loss: 16348.131836\n",
      "Train Epoch: 448 [62592/225000 (28%)] Loss: 18072.632812\n",
      "Train Epoch: 448 [65088/225000 (29%)] Loss: 18102.734375\n",
      "Train Epoch: 448 [67584/225000 (30%)] Loss: 16772.130859\n",
      "Train Epoch: 448 [70080/225000 (31%)] Loss: 17075.289062\n",
      "Train Epoch: 448 [72576/225000 (32%)] Loss: 16682.087891\n",
      "Train Epoch: 448 [75072/225000 (33%)] Loss: 16407.156250\n",
      "Train Epoch: 448 [77568/225000 (34%)] Loss: 17347.705078\n",
      "Train Epoch: 448 [80064/225000 (36%)] Loss: 16719.359375\n",
      "Train Epoch: 448 [82560/225000 (37%)] Loss: 16582.537109\n",
      "Train Epoch: 448 [85056/225000 (38%)] Loss: 16568.695312\n",
      "Train Epoch: 448 [87552/225000 (39%)] Loss: 16918.615234\n",
      "Train Epoch: 448 [90048/225000 (40%)] Loss: 16482.691406\n",
      "Train Epoch: 448 [92544/225000 (41%)] Loss: 16923.144531\n",
      "Train Epoch: 448 [95040/225000 (42%)] Loss: 16454.566406\n",
      "Train Epoch: 448 [97536/225000 (43%)] Loss: 16765.511719\n",
      "Train Epoch: 448 [100032/225000 (44%)] Loss: 16621.582031\n",
      "Train Epoch: 448 [102528/225000 (46%)] Loss: 18024.779297\n",
      "Train Epoch: 448 [105024/225000 (47%)] Loss: 16675.933594\n",
      "Train Epoch: 448 [107520/225000 (48%)] Loss: 16627.355469\n",
      "Train Epoch: 448 [110016/225000 (49%)] Loss: 16882.062500\n",
      "Train Epoch: 448 [112512/225000 (50%)] Loss: 16719.011719\n",
      "Train Epoch: 448 [115008/225000 (51%)] Loss: 16889.636719\n",
      "Train Epoch: 448 [117504/225000 (52%)] Loss: 16592.033203\n",
      "Train Epoch: 448 [120000/225000 (53%)] Loss: 16406.062500\n",
      "Train Epoch: 448 [122496/225000 (54%)] Loss: 16509.007812\n",
      "Train Epoch: 448 [124992/225000 (56%)] Loss: 16856.472656\n",
      "Train Epoch: 448 [127488/225000 (57%)] Loss: 16783.335938\n",
      "Train Epoch: 448 [129984/225000 (58%)] Loss: 16920.031250\n",
      "Train Epoch: 448 [132480/225000 (59%)] Loss: 16424.671875\n",
      "Train Epoch: 448 [134976/225000 (60%)] Loss: 16351.609375\n",
      "Train Epoch: 448 [137472/225000 (61%)] Loss: 16615.595703\n",
      "Train Epoch: 448 [139968/225000 (62%)] Loss: 16874.111328\n",
      "Train Epoch: 448 [142464/225000 (63%)] Loss: 17114.023438\n",
      "Train Epoch: 448 [144960/225000 (64%)] Loss: 16627.343750\n",
      "Train Epoch: 448 [147456/225000 (66%)] Loss: 16404.269531\n",
      "Train Epoch: 448 [149952/225000 (67%)] Loss: 16464.929688\n",
      "Train Epoch: 448 [152448/225000 (68%)] Loss: 16486.171875\n",
      "Train Epoch: 448 [154944/225000 (69%)] Loss: 16646.531250\n",
      "Train Epoch: 448 [157440/225000 (70%)] Loss: 16433.042969\n",
      "Train Epoch: 448 [159936/225000 (71%)] Loss: 16316.583984\n",
      "Train Epoch: 448 [162432/225000 (72%)] Loss: 16882.927734\n",
      "Train Epoch: 448 [164928/225000 (73%)] Loss: 16537.804688\n",
      "Train Epoch: 448 [167424/225000 (74%)] Loss: 18099.832031\n",
      "Train Epoch: 448 [169920/225000 (76%)] Loss: 16710.460938\n",
      "Train Epoch: 448 [172416/225000 (77%)] Loss: 16832.222656\n",
      "Train Epoch: 448 [174912/225000 (78%)] Loss: 16942.917969\n",
      "Train Epoch: 448 [177408/225000 (79%)] Loss: 16515.146484\n",
      "Train Epoch: 448 [179904/225000 (80%)] Loss: 16880.859375\n",
      "Train Epoch: 448 [182400/225000 (81%)] Loss: 16512.367188\n",
      "Train Epoch: 448 [184896/225000 (82%)] Loss: 16759.312500\n",
      "Train Epoch: 448 [187392/225000 (83%)] Loss: 16523.935547\n",
      "Train Epoch: 448 [189888/225000 (84%)] Loss: 16948.927734\n",
      "Train Epoch: 448 [192384/225000 (86%)] Loss: 16180.876953\n",
      "Train Epoch: 448 [194880/225000 (87%)] Loss: 16988.042969\n",
      "Train Epoch: 448 [197376/225000 (88%)] Loss: 16966.269531\n",
      "Train Epoch: 448 [199872/225000 (89%)] Loss: 16596.355469\n",
      "Train Epoch: 448 [202368/225000 (90%)] Loss: 16971.785156\n",
      "Train Epoch: 448 [204864/225000 (91%)] Loss: 16732.199219\n",
      "Train Epoch: 448 [207360/225000 (92%)] Loss: 17074.976562\n",
      "Train Epoch: 448 [209856/225000 (93%)] Loss: 15943.087891\n",
      "Train Epoch: 448 [212352/225000 (94%)] Loss: 16956.544922\n",
      "Train Epoch: 448 [214848/225000 (95%)] Loss: 16283.661133\n",
      "Train Epoch: 448 [217344/225000 (97%)] Loss: 16775.273438\n",
      "Train Epoch: 448 [219840/225000 (98%)] Loss: 16727.435547\n",
      "Train Epoch: 448 [222336/225000 (99%)] Loss: 16562.736328\n",
      "Train Epoch: 448 [224832/225000 (100%)] Loss: 17801.410156\n",
      "    epoch          : 448\n",
      "    loss           : 16726.69129959471\n",
      "    val_loss       : 16669.757160275945\n",
      "Train Epoch: 449 [192/225000 (0%)] Loss: 16476.503906\n",
      "Train Epoch: 449 [2688/225000 (1%)] Loss: 16699.699219\n",
      "Train Epoch: 449 [5184/225000 (2%)] Loss: 16790.353516\n",
      "Train Epoch: 449 [7680/225000 (3%)] Loss: 16958.820312\n",
      "Train Epoch: 449 [10176/225000 (5%)] Loss: 16800.111328\n",
      "Train Epoch: 449 [12672/225000 (6%)] Loss: 18224.212891\n",
      "Train Epoch: 449 [15168/225000 (7%)] Loss: 16681.857422\n",
      "Train Epoch: 449 [17664/225000 (8%)] Loss: 16613.218750\n",
      "Train Epoch: 449 [20160/225000 (9%)] Loss: 16839.605469\n",
      "Train Epoch: 449 [22656/225000 (10%)] Loss: 16668.828125\n",
      "Train Epoch: 449 [25152/225000 (11%)] Loss: 16322.191406\n",
      "Train Epoch: 449 [27648/225000 (12%)] Loss: 16335.955078\n",
      "Train Epoch: 449 [30144/225000 (13%)] Loss: 16471.951172\n",
      "Train Epoch: 449 [32640/225000 (15%)] Loss: 16841.222656\n",
      "Train Epoch: 449 [35136/225000 (16%)] Loss: 16810.220703\n",
      "Train Epoch: 449 [37632/225000 (17%)] Loss: 16849.757812\n",
      "Train Epoch: 449 [40128/225000 (18%)] Loss: 16881.640625\n",
      "Train Epoch: 449 [42624/225000 (19%)] Loss: 16790.123047\n",
      "Train Epoch: 449 [45120/225000 (20%)] Loss: 16874.937500\n",
      "Train Epoch: 449 [47616/225000 (21%)] Loss: 17012.191406\n",
      "Train Epoch: 449 [50112/225000 (22%)] Loss: 16783.000000\n",
      "Train Epoch: 449 [52608/225000 (23%)] Loss: 16687.371094\n",
      "Train Epoch: 449 [55104/225000 (24%)] Loss: 16485.451172\n",
      "Train Epoch: 449 [57600/225000 (26%)] Loss: 16272.781250\n",
      "Train Epoch: 449 [60096/225000 (27%)] Loss: 17260.312500\n",
      "Train Epoch: 449 [62592/225000 (28%)] Loss: 16865.089844\n",
      "Train Epoch: 449 [65088/225000 (29%)] Loss: 16573.675781\n",
      "Train Epoch: 449 [67584/225000 (30%)] Loss: 16163.800781\n",
      "Train Epoch: 449 [70080/225000 (31%)] Loss: 17081.228516\n",
      "Train Epoch: 449 [72576/225000 (32%)] Loss: 16424.128906\n",
      "Train Epoch: 449 [75072/225000 (33%)] Loss: 16964.691406\n",
      "Train Epoch: 449 [77568/225000 (34%)] Loss: 16446.906250\n",
      "Train Epoch: 449 [80064/225000 (36%)] Loss: 16350.328125\n",
      "Train Epoch: 449 [82560/225000 (37%)] Loss: 16332.365234\n",
      "Train Epoch: 449 [85056/225000 (38%)] Loss: 16276.745117\n",
      "Train Epoch: 449 [87552/225000 (39%)] Loss: 16546.931641\n",
      "Train Epoch: 449 [90048/225000 (40%)] Loss: 16580.695312\n",
      "Train Epoch: 449 [92544/225000 (41%)] Loss: 16432.050781\n",
      "Train Epoch: 449 [95040/225000 (42%)] Loss: 16560.144531\n",
      "Train Epoch: 449 [97536/225000 (43%)] Loss: 16557.408203\n",
      "Train Epoch: 449 [100032/225000 (44%)] Loss: 16498.951172\n",
      "Train Epoch: 449 [102528/225000 (46%)] Loss: 16159.852539\n",
      "Train Epoch: 449 [105024/225000 (47%)] Loss: 16555.082031\n",
      "Train Epoch: 449 [107520/225000 (48%)] Loss: 16702.710938\n",
      "Train Epoch: 449 [110016/225000 (49%)] Loss: 16383.729492\n",
      "Train Epoch: 449 [112512/225000 (50%)] Loss: 16674.226562\n",
      "Train Epoch: 449 [115008/225000 (51%)] Loss: 16843.517578\n",
      "Train Epoch: 449 [117504/225000 (52%)] Loss: 16239.500000\n",
      "Train Epoch: 449 [120000/225000 (53%)] Loss: 16704.960938\n",
      "Train Epoch: 449 [122496/225000 (54%)] Loss: 16603.425781\n",
      "Train Epoch: 449 [124992/225000 (56%)] Loss: 16478.703125\n",
      "Train Epoch: 449 [127488/225000 (57%)] Loss: 16476.027344\n",
      "Train Epoch: 449 [129984/225000 (58%)] Loss: 16814.386719\n",
      "Train Epoch: 449 [132480/225000 (59%)] Loss: 16832.160156\n",
      "Train Epoch: 449 [134976/225000 (60%)] Loss: 16974.978516\n",
      "Train Epoch: 449 [137472/225000 (61%)] Loss: 16702.046875\n",
      "Train Epoch: 449 [139968/225000 (62%)] Loss: 17042.765625\n",
      "Train Epoch: 449 [142464/225000 (63%)] Loss: 16620.265625\n",
      "Train Epoch: 449 [144960/225000 (64%)] Loss: 16866.023438\n",
      "Train Epoch: 449 [147456/225000 (66%)] Loss: 16339.333008\n",
      "Train Epoch: 449 [149952/225000 (67%)] Loss: 16685.265625\n",
      "Train Epoch: 449 [152448/225000 (68%)] Loss: 15834.934570\n",
      "Train Epoch: 449 [154944/225000 (69%)] Loss: 18013.964844\n",
      "Train Epoch: 449 [157440/225000 (70%)] Loss: 16325.095703\n",
      "Train Epoch: 449 [159936/225000 (71%)] Loss: 16650.179688\n",
      "Train Epoch: 449 [162432/225000 (72%)] Loss: 16823.896484\n",
      "Train Epoch: 449 [164928/225000 (73%)] Loss: 17123.412109\n",
      "Train Epoch: 449 [167424/225000 (74%)] Loss: 16414.669922\n",
      "Train Epoch: 449 [169920/225000 (76%)] Loss: 16513.748047\n",
      "Train Epoch: 449 [172416/225000 (77%)] Loss: 16596.205078\n",
      "Train Epoch: 449 [174912/225000 (78%)] Loss: 16687.203125\n",
      "Train Epoch: 449 [177408/225000 (79%)] Loss: 16844.847656\n",
      "Train Epoch: 449 [179904/225000 (80%)] Loss: 16453.472656\n",
      "Train Epoch: 449 [182400/225000 (81%)] Loss: 17030.894531\n",
      "Train Epoch: 449 [184896/225000 (82%)] Loss: 16448.171875\n",
      "Train Epoch: 449 [187392/225000 (83%)] Loss: 18008.078125\n",
      "Train Epoch: 449 [189888/225000 (84%)] Loss: 16397.285156\n",
      "Train Epoch: 449 [192384/225000 (86%)] Loss: 16567.453125\n",
      "Train Epoch: 449 [194880/225000 (87%)] Loss: 16664.937500\n",
      "Train Epoch: 449 [197376/225000 (88%)] Loss: 17078.667969\n",
      "Train Epoch: 449 [199872/225000 (89%)] Loss: 18000.738281\n",
      "Train Epoch: 449 [202368/225000 (90%)] Loss: 17031.007812\n",
      "Train Epoch: 449 [204864/225000 (91%)] Loss: 16528.785156\n",
      "Train Epoch: 449 [207360/225000 (92%)] Loss: 16437.595703\n",
      "Train Epoch: 449 [209856/225000 (93%)] Loss: 16709.195312\n",
      "Train Epoch: 449 [212352/225000 (94%)] Loss: 16522.296875\n",
      "Train Epoch: 449 [214848/225000 (95%)] Loss: 16592.371094\n",
      "Train Epoch: 449 [217344/225000 (97%)] Loss: 16682.728516\n",
      "Train Epoch: 449 [219840/225000 (98%)] Loss: 16700.960938\n",
      "Train Epoch: 449 [222336/225000 (99%)] Loss: 17119.804688\n",
      "Train Epoch: 449 [224832/225000 (100%)] Loss: 16467.410156\n",
      "    epoch          : 449\n",
      "    loss           : 16724.444199285408\n",
      "    val_loss       : 16642.102280798077\n",
      "Train Epoch: 450 [192/225000 (0%)] Loss: 16696.613281\n",
      "Train Epoch: 450 [2688/225000 (1%)] Loss: 17255.765625\n",
      "Train Epoch: 450 [5184/225000 (2%)] Loss: 16846.759766\n",
      "Train Epoch: 450 [7680/225000 (3%)] Loss: 16258.222656\n",
      "Train Epoch: 450 [10176/225000 (5%)] Loss: 16586.074219\n",
      "Train Epoch: 450 [12672/225000 (6%)] Loss: 16456.441406\n",
      "Train Epoch: 450 [15168/225000 (7%)] Loss: 16401.375000\n",
      "Train Epoch: 450 [17664/225000 (8%)] Loss: 16465.320312\n",
      "Train Epoch: 450 [20160/225000 (9%)] Loss: 18451.472656\n",
      "Train Epoch: 450 [22656/225000 (10%)] Loss: 16763.800781\n",
      "Train Epoch: 450 [25152/225000 (11%)] Loss: 16368.623047\n",
      "Train Epoch: 450 [27648/225000 (12%)] Loss: 16268.334961\n",
      "Train Epoch: 450 [30144/225000 (13%)] Loss: 16814.978516\n",
      "Train Epoch: 450 [32640/225000 (15%)] Loss: 16369.936523\n",
      "Train Epoch: 450 [35136/225000 (16%)] Loss: 16501.753906\n",
      "Train Epoch: 450 [37632/225000 (17%)] Loss: 16150.025391\n",
      "Train Epoch: 450 [40128/225000 (18%)] Loss: 16654.816406\n",
      "Train Epoch: 450 [42624/225000 (19%)] Loss: 16539.085938\n",
      "Train Epoch: 450 [45120/225000 (20%)] Loss: 16761.027344\n",
      "Train Epoch: 450 [47616/225000 (21%)] Loss: 16853.824219\n",
      "Train Epoch: 450 [50112/225000 (22%)] Loss: 17104.109375\n",
      "Train Epoch: 450 [52608/225000 (23%)] Loss: 16220.949219\n",
      "Train Epoch: 450 [55104/225000 (24%)] Loss: 16560.492188\n",
      "Train Epoch: 450 [57600/225000 (26%)] Loss: 16147.935547\n",
      "Train Epoch: 450 [60096/225000 (27%)] Loss: 16552.187500\n",
      "Train Epoch: 450 [62592/225000 (28%)] Loss: 16411.125000\n",
      "Train Epoch: 450 [65088/225000 (29%)] Loss: 16640.929688\n",
      "Train Epoch: 450 [67584/225000 (30%)] Loss: 16463.607422\n",
      "Train Epoch: 450 [70080/225000 (31%)] Loss: 16797.025391\n",
      "Train Epoch: 450 [72576/225000 (32%)] Loss: 16821.562500\n",
      "Train Epoch: 450 [75072/225000 (33%)] Loss: 16624.484375\n",
      "Train Epoch: 450 [77568/225000 (34%)] Loss: 16831.398438\n",
      "Train Epoch: 450 [80064/225000 (36%)] Loss: 17037.351562\n",
      "Train Epoch: 450 [82560/225000 (37%)] Loss: 17897.154297\n",
      "Train Epoch: 450 [85056/225000 (38%)] Loss: 16883.179688\n",
      "Train Epoch: 450 [87552/225000 (39%)] Loss: 16722.878906\n",
      "Train Epoch: 450 [90048/225000 (40%)] Loss: 16400.523438\n",
      "Train Epoch: 450 [92544/225000 (41%)] Loss: 16504.351562\n",
      "Train Epoch: 450 [95040/225000 (42%)] Loss: 16182.975586\n",
      "Train Epoch: 450 [97536/225000 (43%)] Loss: 16752.212891\n",
      "Train Epoch: 450 [100032/225000 (44%)] Loss: 16660.847656\n",
      "Train Epoch: 450 [102528/225000 (46%)] Loss: 16949.167969\n",
      "Train Epoch: 450 [105024/225000 (47%)] Loss: 16530.871094\n",
      "Train Epoch: 450 [107520/225000 (48%)] Loss: 16097.369141\n",
      "Train Epoch: 450 [110016/225000 (49%)] Loss: 16498.867188\n",
      "Train Epoch: 450 [112512/225000 (50%)] Loss: 16247.057617\n",
      "Train Epoch: 450 [115008/225000 (51%)] Loss: 16796.152344\n",
      "Train Epoch: 450 [117504/225000 (52%)] Loss: 16076.614258\n",
      "Train Epoch: 450 [120000/225000 (53%)] Loss: 16866.833984\n",
      "Train Epoch: 450 [122496/225000 (54%)] Loss: 16671.281250\n",
      "Train Epoch: 450 [124992/225000 (56%)] Loss: 16790.568359\n",
      "Train Epoch: 450 [127488/225000 (57%)] Loss: 17041.132812\n",
      "Train Epoch: 450 [129984/225000 (58%)] Loss: 16730.548828\n",
      "Train Epoch: 450 [132480/225000 (59%)] Loss: 16620.343750\n",
      "Train Epoch: 450 [134976/225000 (60%)] Loss: 16536.250000\n",
      "Train Epoch: 450 [137472/225000 (61%)] Loss: 16723.046875\n",
      "Train Epoch: 450 [139968/225000 (62%)] Loss: 16416.929688\n",
      "Train Epoch: 450 [142464/225000 (63%)] Loss: 16763.742188\n",
      "Train Epoch: 450 [144960/225000 (64%)] Loss: 16234.905273\n",
      "Train Epoch: 450 [147456/225000 (66%)] Loss: 17649.667969\n",
      "Train Epoch: 450 [149952/225000 (67%)] Loss: 16893.726562\n",
      "Train Epoch: 450 [152448/225000 (68%)] Loss: 16818.757812\n",
      "Train Epoch: 450 [154944/225000 (69%)] Loss: 16504.648438\n",
      "Train Epoch: 450 [157440/225000 (70%)] Loss: 17125.238281\n",
      "Train Epoch: 450 [159936/225000 (71%)] Loss: 16509.712891\n",
      "Train Epoch: 450 [162432/225000 (72%)] Loss: 16823.828125\n",
      "Train Epoch: 450 [164928/225000 (73%)] Loss: 16560.500000\n",
      "Train Epoch: 450 [167424/225000 (74%)] Loss: 16771.708984\n",
      "Train Epoch: 450 [169920/225000 (76%)] Loss: 17079.486328\n",
      "Train Epoch: 450 [172416/225000 (77%)] Loss: 16602.187500\n",
      "Train Epoch: 450 [174912/225000 (78%)] Loss: 17042.160156\n",
      "Train Epoch: 450 [177408/225000 (79%)] Loss: 16494.373047\n",
      "Train Epoch: 450 [179904/225000 (80%)] Loss: 16687.166016\n",
      "Train Epoch: 450 [182400/225000 (81%)] Loss: 16977.943359\n",
      "Train Epoch: 450 [184896/225000 (82%)] Loss: 16547.355469\n",
      "Train Epoch: 450 [187392/225000 (83%)] Loss: 17141.406250\n",
      "Train Epoch: 450 [189888/225000 (84%)] Loss: 16960.199219\n",
      "Train Epoch: 450 [192384/225000 (86%)] Loss: 16792.328125\n",
      "Train Epoch: 450 [194880/225000 (87%)] Loss: 16740.621094\n",
      "Train Epoch: 450 [197376/225000 (88%)] Loss: 16398.587891\n",
      "Train Epoch: 450 [199872/225000 (89%)] Loss: 16440.968750\n",
      "Train Epoch: 450 [202368/225000 (90%)] Loss: 18107.675781\n",
      "Train Epoch: 450 [204864/225000 (91%)] Loss: 16513.824219\n",
      "Train Epoch: 450 [207360/225000 (92%)] Loss: 16725.058594\n",
      "Train Epoch: 450 [209856/225000 (93%)] Loss: 16767.263672\n",
      "Train Epoch: 450 [212352/225000 (94%)] Loss: 16314.559570\n",
      "Train Epoch: 450 [214848/225000 (95%)] Loss: 16589.080078\n",
      "Train Epoch: 450 [217344/225000 (97%)] Loss: 18328.335938\n",
      "Train Epoch: 450 [219840/225000 (98%)] Loss: 16962.093750\n",
      "Train Epoch: 450 [222336/225000 (99%)] Loss: 16783.679688\n",
      "Train Epoch: 450 [224832/225000 (100%)] Loss: 16685.242188\n",
      "    epoch          : 450\n",
      "    loss           : 16712.049120593805\n",
      "    val_loss       : 16684.195960756475\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0804_123131/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [192/225000 (0%)] Loss: 16907.033203\n",
      "Train Epoch: 451 [2688/225000 (1%)] Loss: 16751.792969\n",
      "Train Epoch: 451 [5184/225000 (2%)] Loss: 17015.031250\n",
      "Train Epoch: 451 [7680/225000 (3%)] Loss: 16611.074219\n",
      "Train Epoch: 451 [10176/225000 (5%)] Loss: 16769.312500\n",
      "Train Epoch: 451 [12672/225000 (6%)] Loss: 17006.318359\n",
      "Train Epoch: 451 [15168/225000 (7%)] Loss: 16983.234375\n",
      "Train Epoch: 451 [17664/225000 (8%)] Loss: 16228.969727\n",
      "Train Epoch: 451 [20160/225000 (9%)] Loss: 16532.748047\n",
      "Train Epoch: 451 [22656/225000 (10%)] Loss: 16501.162109\n",
      "Train Epoch: 451 [25152/225000 (11%)] Loss: 16878.820312\n",
      "Train Epoch: 451 [27648/225000 (12%)] Loss: 16726.023438\n",
      "Train Epoch: 451 [30144/225000 (13%)] Loss: 16555.132812\n",
      "Train Epoch: 451 [32640/225000 (15%)] Loss: 16928.328125\n",
      "Train Epoch: 451 [35136/225000 (16%)] Loss: 16590.617188\n",
      "Train Epoch: 451 [37632/225000 (17%)] Loss: 16749.125000\n",
      "Train Epoch: 451 [40128/225000 (18%)] Loss: 16095.571289\n",
      "Train Epoch: 451 [42624/225000 (19%)] Loss: 16253.470703\n",
      "Train Epoch: 451 [45120/225000 (20%)] Loss: 16399.726562\n",
      "Train Epoch: 451 [47616/225000 (21%)] Loss: 16529.388672\n",
      "Train Epoch: 451 [50112/225000 (22%)] Loss: 16594.724609\n",
      "Train Epoch: 451 [52608/225000 (23%)] Loss: 16608.623047\n",
      "Train Epoch: 451 [55104/225000 (24%)] Loss: 16725.056641\n",
      "Train Epoch: 451 [57600/225000 (26%)] Loss: 16635.378906\n",
      "Train Epoch: 451 [60096/225000 (27%)] Loss: 16633.343750\n",
      "Train Epoch: 451 [62592/225000 (28%)] Loss: 16277.258789\n",
      "Train Epoch: 451 [65088/225000 (29%)] Loss: 16923.617188\n",
      "Train Epoch: 451 [67584/225000 (30%)] Loss: 17943.896484\n",
      "Train Epoch: 451 [70080/225000 (31%)] Loss: 16358.427734\n",
      "Train Epoch: 451 [72576/225000 (32%)] Loss: 17074.363281\n",
      "Train Epoch: 451 [75072/225000 (33%)] Loss: 16559.937500\n",
      "Train Epoch: 451 [77568/225000 (34%)] Loss: 18732.695312\n",
      "Train Epoch: 451 [80064/225000 (36%)] Loss: 17082.617188\n",
      "Train Epoch: 451 [82560/225000 (37%)] Loss: 17021.160156\n",
      "Train Epoch: 451 [85056/225000 (38%)] Loss: 16807.437500\n",
      "Train Epoch: 451 [87552/225000 (39%)] Loss: 16797.636719\n",
      "Train Epoch: 451 [90048/225000 (40%)] Loss: 16898.453125\n",
      "Train Epoch: 451 [92544/225000 (41%)] Loss: 16791.076172\n",
      "Train Epoch: 451 [95040/225000 (42%)] Loss: 16564.468750\n",
      "Train Epoch: 451 [97536/225000 (43%)] Loss: 16757.015625\n",
      "Train Epoch: 451 [100032/225000 (44%)] Loss: 16841.585938\n",
      "Train Epoch: 451 [102528/225000 (46%)] Loss: 17039.011719\n",
      "Train Epoch: 451 [105024/225000 (47%)] Loss: 16762.480469\n",
      "Train Epoch: 451 [107520/225000 (48%)] Loss: 16296.487305\n",
      "Train Epoch: 451 [110016/225000 (49%)] Loss: 16384.974609\n",
      "Train Epoch: 451 [112512/225000 (50%)] Loss: 16286.092773\n",
      "Train Epoch: 451 [115008/225000 (51%)] Loss: 16548.119141\n",
      "Train Epoch: 451 [117504/225000 (52%)] Loss: 15975.710938\n",
      "Train Epoch: 451 [120000/225000 (53%)] Loss: 16605.445312\n",
      "Train Epoch: 451 [122496/225000 (54%)] Loss: 16853.660156\n",
      "Train Epoch: 451 [124992/225000 (56%)] Loss: 16638.210938\n",
      "Train Epoch: 451 [127488/225000 (57%)] Loss: 16320.478516\n",
      "Train Epoch: 451 [129984/225000 (58%)] Loss: 16678.898438\n",
      "Train Epoch: 451 [132480/225000 (59%)] Loss: 16958.714844\n",
      "Train Epoch: 451 [134976/225000 (60%)] Loss: 16530.656250\n",
      "Train Epoch: 451 [137472/225000 (61%)] Loss: 16562.208984\n",
      "Train Epoch: 451 [139968/225000 (62%)] Loss: 16827.027344\n",
      "Train Epoch: 451 [142464/225000 (63%)] Loss: 16505.906250\n",
      "Train Epoch: 451 [144960/225000 (64%)] Loss: 16789.908203\n",
      "Train Epoch: 451 [147456/225000 (66%)] Loss: 17004.031250\n",
      "Train Epoch: 451 [149952/225000 (67%)] Loss: 16616.160156\n",
      "Train Epoch: 451 [152448/225000 (68%)] Loss: 16875.878906\n",
      "Train Epoch: 451 [154944/225000 (69%)] Loss: 16637.980469\n",
      "Train Epoch: 451 [157440/225000 (70%)] Loss: 18311.439453\n",
      "Train Epoch: 451 [159936/225000 (71%)] Loss: 16806.273438\n",
      "Train Epoch: 451 [162432/225000 (72%)] Loss: 16852.105469\n",
      "Train Epoch: 451 [164928/225000 (73%)] Loss: 16949.597656\n",
      "Train Epoch: 451 [167424/225000 (74%)] Loss: 16295.632812\n",
      "Train Epoch: 451 [169920/225000 (76%)] Loss: 16754.390625\n",
      "Train Epoch: 451 [172416/225000 (77%)] Loss: 16591.751953\n",
      "Train Epoch: 451 [174912/225000 (78%)] Loss: 16222.877930\n",
      "Train Epoch: 451 [177408/225000 (79%)] Loss: 16695.367188\n",
      "Train Epoch: 451 [179904/225000 (80%)] Loss: 16493.623047\n",
      "Train Epoch: 451 [182400/225000 (81%)] Loss: 16370.809570\n",
      "Train Epoch: 451 [184896/225000 (82%)] Loss: 16617.685547\n",
      "Train Epoch: 451 [187392/225000 (83%)] Loss: 16340.297852\n",
      "Train Epoch: 451 [189888/225000 (84%)] Loss: 17474.308594\n",
      "Train Epoch: 451 [192384/225000 (86%)] Loss: 16156.949219\n",
      "Train Epoch: 451 [194880/225000 (87%)] Loss: 16757.542969\n",
      "Train Epoch: 451 [197376/225000 (88%)] Loss: 17909.074219\n",
      "Train Epoch: 451 [199872/225000 (89%)] Loss: 16944.109375\n",
      "Train Epoch: 451 [202368/225000 (90%)] Loss: 16616.886719\n",
      "Train Epoch: 451 [204864/225000 (91%)] Loss: 16570.210938\n",
      "Train Epoch: 451 [207360/225000 (92%)] Loss: 16613.652344\n",
      "Train Epoch: 451 [209856/225000 (93%)] Loss: 16693.896484\n",
      "Train Epoch: 451 [212352/225000 (94%)] Loss: 16782.808594\n",
      "Train Epoch: 451 [214848/225000 (95%)] Loss: 16697.000000\n",
      "Train Epoch: 451 [217344/225000 (97%)] Loss: 16225.904297\n",
      "Train Epoch: 451 [219840/225000 (98%)] Loss: 17094.597656\n",
      "Train Epoch: 451 [222336/225000 (99%)] Loss: 16160.238281\n",
      "Train Epoch: 451 [224832/225000 (100%)] Loss: 17623.363281\n",
      "    epoch          : 451\n",
      "    loss           : 16730.957715343695\n",
      "    val_loss       : 16659.5217831244\n",
      "Train Epoch: 452 [192/225000 (0%)] Loss: 16328.565430\n",
      "Train Epoch: 452 [2688/225000 (1%)] Loss: 16389.675781\n",
      "Train Epoch: 452 [5184/225000 (2%)] Loss: 16687.234375\n",
      "Train Epoch: 452 [7680/225000 (3%)] Loss: 16665.806641\n",
      "Train Epoch: 452 [10176/225000 (5%)] Loss: 16432.869141\n",
      "Train Epoch: 452 [12672/225000 (6%)] Loss: 16709.144531\n",
      "Train Epoch: 452 [15168/225000 (7%)] Loss: 16685.316406\n",
      "Train Epoch: 452 [17664/225000 (8%)] Loss: 16704.855469\n",
      "Train Epoch: 452 [20160/225000 (9%)] Loss: 16361.249023\n",
      "Train Epoch: 452 [22656/225000 (10%)] Loss: 16709.437500\n",
      "Train Epoch: 452 [25152/225000 (11%)] Loss: 17032.062500\n",
      "Train Epoch: 452 [27648/225000 (12%)] Loss: 16724.527344\n",
      "Train Epoch: 452 [30144/225000 (13%)] Loss: 16638.072266\n",
      "Train Epoch: 452 [32640/225000 (15%)] Loss: 16257.232422\n",
      "Train Epoch: 452 [35136/225000 (16%)] Loss: 16823.933594\n",
      "Train Epoch: 452 [37632/225000 (17%)] Loss: 16959.617188\n",
      "Train Epoch: 452 [40128/225000 (18%)] Loss: 16856.283203\n",
      "Train Epoch: 452 [42624/225000 (19%)] Loss: 17158.613281\n",
      "Train Epoch: 452 [45120/225000 (20%)] Loss: 16782.804688\n",
      "Train Epoch: 452 [47616/225000 (21%)] Loss: 16258.468750\n",
      "Train Epoch: 452 [50112/225000 (22%)] Loss: 16699.042969\n",
      "Train Epoch: 452 [52608/225000 (23%)] Loss: 16488.859375\n",
      "Train Epoch: 452 [55104/225000 (24%)] Loss: 16505.638672\n",
      "Train Epoch: 452 [57600/225000 (26%)] Loss: 16744.972656\n",
      "Train Epoch: 452 [60096/225000 (27%)] Loss: 16883.789062\n",
      "Train Epoch: 452 [62592/225000 (28%)] Loss: 16828.435547\n",
      "Train Epoch: 452 [65088/225000 (29%)] Loss: 16369.207031\n",
      "Train Epoch: 452 [67584/225000 (30%)] Loss: 16682.748047\n",
      "Train Epoch: 452 [70080/225000 (31%)] Loss: 16215.935547\n",
      "Train Epoch: 452 [72576/225000 (32%)] Loss: 16579.062500\n",
      "Train Epoch: 452 [75072/225000 (33%)] Loss: 16272.509766\n",
      "Train Epoch: 452 [77568/225000 (34%)] Loss: 16777.587891\n",
      "Train Epoch: 452 [80064/225000 (36%)] Loss: 16765.269531\n",
      "Train Epoch: 452 [82560/225000 (37%)] Loss: 16061.268555\n",
      "Train Epoch: 452 [85056/225000 (38%)] Loss: 16395.050781\n",
      "Train Epoch: 452 [87552/225000 (39%)] Loss: 16902.675781\n",
      "Train Epoch: 452 [90048/225000 (40%)] Loss: 16537.738281\n",
      "Train Epoch: 452 [92544/225000 (41%)] Loss: 16685.275391\n",
      "Train Epoch: 452 [95040/225000 (42%)] Loss: 16781.207031\n",
      "Train Epoch: 452 [97536/225000 (43%)] Loss: 16322.513672\n",
      "Train Epoch: 452 [100032/225000 (44%)] Loss: 16743.343750\n",
      "Train Epoch: 452 [102528/225000 (46%)] Loss: 16828.789062\n",
      "Train Epoch: 452 [105024/225000 (47%)] Loss: 16757.199219\n",
      "Train Epoch: 452 [107520/225000 (48%)] Loss: 16643.019531\n",
      "Train Epoch: 452 [110016/225000 (49%)] Loss: 16279.220703\n",
      "Train Epoch: 452 [112512/225000 (50%)] Loss: 16946.201172\n",
      "Train Epoch: 452 [115008/225000 (51%)] Loss: 16595.316406\n",
      "Train Epoch: 452 [117504/225000 (52%)] Loss: 16711.089844\n",
      "Train Epoch: 452 [120000/225000 (53%)] Loss: 16649.363281\n",
      "Train Epoch: 452 [122496/225000 (54%)] Loss: 16644.541016\n",
      "Train Epoch: 452 [124992/225000 (56%)] Loss: 16693.109375\n",
      "Train Epoch: 452 [127488/225000 (57%)] Loss: 16746.726562\n",
      "Train Epoch: 452 [129984/225000 (58%)] Loss: 16364.395508\n",
      "Train Epoch: 452 [132480/225000 (59%)] Loss: 16598.623047\n",
      "Train Epoch: 452 [134976/225000 (60%)] Loss: 18037.253906\n",
      "Train Epoch: 452 [137472/225000 (61%)] Loss: 16972.115234\n",
      "Train Epoch: 452 [139968/225000 (62%)] Loss: 15996.248047\n",
      "Train Epoch: 452 [142464/225000 (63%)] Loss: 16524.154297\n",
      "Train Epoch: 452 [144960/225000 (64%)] Loss: 16881.119141\n",
      "Train Epoch: 452 [147456/225000 (66%)] Loss: 16843.886719\n",
      "Train Epoch: 452 [149952/225000 (67%)] Loss: 16092.311523\n",
      "Train Epoch: 452 [152448/225000 (68%)] Loss: 16957.470703\n",
      "Train Epoch: 452 [154944/225000 (69%)] Loss: 16314.878906\n",
      "Train Epoch: 452 [157440/225000 (70%)] Loss: 16684.328125\n",
      "Train Epoch: 452 [159936/225000 (71%)] Loss: 16977.054688\n",
      "Train Epoch: 452 [162432/225000 (72%)] Loss: 16490.113281\n",
      "Train Epoch: 452 [164928/225000 (73%)] Loss: 16061.547852\n",
      "Train Epoch: 452 [167424/225000 (74%)] Loss: 17123.416016\n",
      "Train Epoch: 452 [169920/225000 (76%)] Loss: 17170.375000\n",
      "Train Epoch: 452 [172416/225000 (77%)] Loss: 16764.703125\n",
      "Train Epoch: 452 [174912/225000 (78%)] Loss: 16960.777344\n",
      "Train Epoch: 452 [177408/225000 (79%)] Loss: 17103.902344\n",
      "Train Epoch: 452 [179904/225000 (80%)] Loss: 16997.083984\n",
      "Train Epoch: 452 [182400/225000 (81%)] Loss: 16923.382812\n",
      "Train Epoch: 452 [184896/225000 (82%)] Loss: 16777.775391\n",
      "Train Epoch: 452 [187392/225000 (83%)] Loss: 16276.758789\n",
      "Train Epoch: 452 [189888/225000 (84%)] Loss: 16350.379883\n",
      "Train Epoch: 452 [192384/225000 (86%)] Loss: 16340.231445\n",
      "Train Epoch: 452 [194880/225000 (87%)] Loss: 16637.960938\n",
      "Train Epoch: 452 [197376/225000 (88%)] Loss: 17222.373047\n",
      "Train Epoch: 452 [199872/225000 (89%)] Loss: 16318.860352\n",
      "Train Epoch: 452 [202368/225000 (90%)] Loss: 16610.929688\n",
      "Train Epoch: 452 [204864/225000 (91%)] Loss: 17208.923828\n",
      "Train Epoch: 452 [207360/225000 (92%)] Loss: 16866.734375\n",
      "Train Epoch: 452 [209856/225000 (93%)] Loss: 16574.480469\n",
      "Train Epoch: 452 [212352/225000 (94%)] Loss: 16488.664062\n",
      "Train Epoch: 452 [214848/225000 (95%)] Loss: 16794.878906\n",
      "Train Epoch: 452 [217344/225000 (97%)] Loss: 16648.210938\n",
      "Train Epoch: 452 [219840/225000 (98%)] Loss: 16479.441406\n",
      "Train Epoch: 452 [222336/225000 (99%)] Loss: 17063.820312\n",
      "Train Epoch: 452 [224832/225000 (100%)] Loss: 16847.640625\n",
      "    epoch          : 452\n",
      "    loss           : 16693.683351275864\n",
      "    val_loss       : 16612.880573138937\n",
      "Train Epoch: 453 [192/225000 (0%)] Loss: 16650.300781\n",
      "Train Epoch: 453 [2688/225000 (1%)] Loss: 16307.638672\n",
      "Train Epoch: 453 [5184/225000 (2%)] Loss: 17130.171875\n",
      "Train Epoch: 453 [7680/225000 (3%)] Loss: 17363.343750\n",
      "Train Epoch: 453 [10176/225000 (5%)] Loss: 16404.960938\n",
      "Train Epoch: 453 [12672/225000 (6%)] Loss: 17279.812500\n",
      "Train Epoch: 453 [15168/225000 (7%)] Loss: 16485.078125\n",
      "Train Epoch: 453 [17664/225000 (8%)] Loss: 16932.873047\n",
      "Train Epoch: 453 [20160/225000 (9%)] Loss: 16490.511719\n",
      "Train Epoch: 453 [22656/225000 (10%)] Loss: 16509.421875\n",
      "Train Epoch: 453 [25152/225000 (11%)] Loss: 16454.072266\n",
      "Train Epoch: 453 [27648/225000 (12%)] Loss: 18590.058594\n",
      "Train Epoch: 453 [30144/225000 (13%)] Loss: 16503.050781\n",
      "Train Epoch: 453 [32640/225000 (15%)] Loss: 16275.310547\n",
      "Train Epoch: 453 [35136/225000 (16%)] Loss: 16816.406250\n",
      "Train Epoch: 453 [37632/225000 (17%)] Loss: 16595.765625\n",
      "Train Epoch: 453 [40128/225000 (18%)] Loss: 16479.144531\n",
      "Train Epoch: 453 [42624/225000 (19%)] Loss: 16200.878906\n",
      "Train Epoch: 453 [45120/225000 (20%)] Loss: 16540.406250\n",
      "Train Epoch: 453 [47616/225000 (21%)] Loss: 16467.630859\n",
      "Train Epoch: 453 [50112/225000 (22%)] Loss: 17054.431641\n",
      "Train Epoch: 453 [52608/225000 (23%)] Loss: 16539.212891\n",
      "Train Epoch: 453 [55104/225000 (24%)] Loss: 16576.033203\n",
      "Train Epoch: 453 [57600/225000 (26%)] Loss: 16732.582031\n",
      "Train Epoch: 453 [60096/225000 (27%)] Loss: 17093.628906\n",
      "Train Epoch: 453 [62592/225000 (28%)] Loss: 16325.701172\n",
      "Train Epoch: 453 [65088/225000 (29%)] Loss: 18601.189453\n",
      "Train Epoch: 453 [67584/225000 (30%)] Loss: 17061.335938\n",
      "Train Epoch: 453 [70080/225000 (31%)] Loss: 16547.667969\n",
      "Train Epoch: 453 [72576/225000 (32%)] Loss: 16812.042969\n",
      "Train Epoch: 453 [75072/225000 (33%)] Loss: 17002.863281\n",
      "Train Epoch: 453 [77568/225000 (34%)] Loss: 18166.263672\n",
      "Train Epoch: 453 [80064/225000 (36%)] Loss: 16536.722656\n",
      "Train Epoch: 453 [82560/225000 (37%)] Loss: 16578.703125\n",
      "Train Epoch: 453 [85056/225000 (38%)] Loss: 18105.296875\n",
      "Train Epoch: 453 [87552/225000 (39%)] Loss: 16423.160156\n",
      "Train Epoch: 453 [90048/225000 (40%)] Loss: 16741.285156\n",
      "Train Epoch: 453 [92544/225000 (41%)] Loss: 16225.663086\n",
      "Train Epoch: 453 [95040/225000 (42%)] Loss: 16659.246094\n",
      "Train Epoch: 453 [97536/225000 (43%)] Loss: 17952.656250\n",
      "Train Epoch: 453 [100032/225000 (44%)] Loss: 17049.431641\n",
      "Train Epoch: 453 [102528/225000 (46%)] Loss: 16997.183594\n",
      "Train Epoch: 453 [105024/225000 (47%)] Loss: 16381.625000\n",
      "Train Epoch: 453 [107520/225000 (48%)] Loss: 16645.101562\n",
      "Train Epoch: 453 [110016/225000 (49%)] Loss: 16587.187500\n",
      "Train Epoch: 453 [112512/225000 (50%)] Loss: 16736.304688\n",
      "Train Epoch: 453 [115008/225000 (51%)] Loss: 18070.777344\n",
      "Train Epoch: 453 [117504/225000 (52%)] Loss: 16955.611328\n",
      "Train Epoch: 453 [120000/225000 (53%)] Loss: 16456.800781\n",
      "Train Epoch: 453 [122496/225000 (54%)] Loss: 16830.925781\n",
      "Train Epoch: 453 [124992/225000 (56%)] Loss: 16642.826172\n",
      "Train Epoch: 453 [127488/225000 (57%)] Loss: 16327.361328\n",
      "Train Epoch: 453 [129984/225000 (58%)] Loss: 16681.460938\n",
      "Train Epoch: 453 [132480/225000 (59%)] Loss: 16609.496094\n",
      "Train Epoch: 453 [134976/225000 (60%)] Loss: 16528.398438\n",
      "Train Epoch: 453 [137472/225000 (61%)] Loss: 16761.113281\n",
      "Train Epoch: 453 [139968/225000 (62%)] Loss: 17132.292969\n",
      "Train Epoch: 453 [142464/225000 (63%)] Loss: 16435.898438\n",
      "Train Epoch: 453 [144960/225000 (64%)] Loss: 16298.431641\n",
      "Train Epoch: 453 [147456/225000 (66%)] Loss: 16700.070312\n",
      "Train Epoch: 453 [149952/225000 (67%)] Loss: 16616.726562\n",
      "Train Epoch: 453 [152448/225000 (68%)] Loss: 16608.865234\n",
      "Train Epoch: 453 [154944/225000 (69%)] Loss: 16512.183594\n",
      "Train Epoch: 453 [157440/225000 (70%)] Loss: 18106.378906\n",
      "Train Epoch: 453 [159936/225000 (71%)] Loss: 16525.218750\n",
      "Train Epoch: 453 [162432/225000 (72%)] Loss: 16179.965820\n",
      "Train Epoch: 453 [164928/225000 (73%)] Loss: 16876.095703\n",
      "Train Epoch: 453 [167424/225000 (74%)] Loss: 16915.830078\n",
      "Train Epoch: 453 [169920/225000 (76%)] Loss: 18288.664062\n",
      "Train Epoch: 453 [172416/225000 (77%)] Loss: 16501.925781\n",
      "Train Epoch: 453 [174912/225000 (78%)] Loss: 16805.191406\n",
      "Train Epoch: 453 [177408/225000 (79%)] Loss: 16417.664062\n",
      "Train Epoch: 453 [179904/225000 (80%)] Loss: 16496.121094\n",
      "Train Epoch: 453 [182400/225000 (81%)] Loss: 16301.055664\n",
      "Train Epoch: 453 [184896/225000 (82%)] Loss: 16035.668945\n",
      "Train Epoch: 453 [187392/225000 (83%)] Loss: 16055.083008\n",
      "Train Epoch: 453 [189888/225000 (84%)] Loss: 17063.929688\n",
      "Train Epoch: 453 [192384/225000 (86%)] Loss: 16360.976562\n",
      "Train Epoch: 453 [194880/225000 (87%)] Loss: 16727.705078\n",
      "Train Epoch: 453 [197376/225000 (88%)] Loss: 16714.695312\n",
      "Train Epoch: 453 [199872/225000 (89%)] Loss: 16753.292969\n",
      "Train Epoch: 453 [202368/225000 (90%)] Loss: 18160.666016\n",
      "Train Epoch: 453 [204864/225000 (91%)] Loss: 16830.337891\n",
      "Train Epoch: 453 [207360/225000 (92%)] Loss: 17282.353516\n",
      "Train Epoch: 453 [209856/225000 (93%)] Loss: 16665.371094\n",
      "Train Epoch: 453 [212352/225000 (94%)] Loss: 16804.064453\n",
      "Train Epoch: 453 [214848/225000 (95%)] Loss: 16541.933594\n",
      "Train Epoch: 453 [217344/225000 (97%)] Loss: 16517.056641\n",
      "Train Epoch: 453 [219840/225000 (98%)] Loss: 16886.851562\n",
      "Train Epoch: 453 [222336/225000 (99%)] Loss: 16857.937500\n",
      "Train Epoch: 453 [224832/225000 (100%)] Loss: 16574.734375\n",
      "    epoch          : 453\n",
      "    loss           : 16718.576597662915\n",
      "    val_loss       : 16596.349353237005\n",
      "Train Epoch: 454 [192/225000 (0%)] Loss: 16703.945312\n",
      "Train Epoch: 454 [2688/225000 (1%)] Loss: 16552.406250\n",
      "Train Epoch: 454 [5184/225000 (2%)] Loss: 16949.234375\n",
      "Train Epoch: 454 [7680/225000 (3%)] Loss: 16111.770508\n",
      "Train Epoch: 454 [10176/225000 (5%)] Loss: 16720.390625\n",
      "Train Epoch: 454 [12672/225000 (6%)] Loss: 16730.703125\n",
      "Train Epoch: 454 [15168/225000 (7%)] Loss: 18572.013672\n",
      "Train Epoch: 454 [17664/225000 (8%)] Loss: 16579.234375\n",
      "Train Epoch: 454 [20160/225000 (9%)] Loss: 16591.101562\n",
      "Train Epoch: 454 [22656/225000 (10%)] Loss: 16517.832031\n",
      "Train Epoch: 454 [25152/225000 (11%)] Loss: 18214.460938\n",
      "Train Epoch: 454 [27648/225000 (12%)] Loss: 16469.179688\n",
      "Train Epoch: 454 [30144/225000 (13%)] Loss: 16762.917969\n",
      "Train Epoch: 454 [32640/225000 (15%)] Loss: 16800.246094\n",
      "Train Epoch: 454 [35136/225000 (16%)] Loss: 16858.988281\n",
      "Train Epoch: 454 [37632/225000 (17%)] Loss: 16724.994141\n",
      "Train Epoch: 454 [40128/225000 (18%)] Loss: 16615.958984\n",
      "Train Epoch: 454 [42624/225000 (19%)] Loss: 16379.027344\n",
      "Train Epoch: 454 [45120/225000 (20%)] Loss: 16523.068359\n",
      "Train Epoch: 454 [47616/225000 (21%)] Loss: 16425.048828\n",
      "Train Epoch: 454 [50112/225000 (22%)] Loss: 16584.464844\n",
      "Train Epoch: 454 [52608/225000 (23%)] Loss: 18530.925781\n",
      "Train Epoch: 454 [55104/225000 (24%)] Loss: 16696.421875\n",
      "Train Epoch: 454 [57600/225000 (26%)] Loss: 16990.828125\n",
      "Train Epoch: 454 [60096/225000 (27%)] Loss: 16413.326172\n",
      "Train Epoch: 454 [62592/225000 (28%)] Loss: 16712.953125\n",
      "Train Epoch: 454 [65088/225000 (29%)] Loss: 16243.748047\n",
      "Train Epoch: 454 [67584/225000 (30%)] Loss: 16994.210938\n",
      "Train Epoch: 454 [70080/225000 (31%)] Loss: 16617.257812\n",
      "Train Epoch: 454 [72576/225000 (32%)] Loss: 16335.431641\n",
      "Train Epoch: 454 [75072/225000 (33%)] Loss: 16522.476562\n",
      "Train Epoch: 454 [77568/225000 (34%)] Loss: 16685.210938\n",
      "Train Epoch: 454 [80064/225000 (36%)] Loss: 16180.195312\n",
      "Train Epoch: 454 [82560/225000 (37%)] Loss: 16935.587891\n",
      "Train Epoch: 454 [85056/225000 (38%)] Loss: 17270.496094\n",
      "Train Epoch: 454 [87552/225000 (39%)] Loss: 16200.935547\n",
      "Train Epoch: 454 [90048/225000 (40%)] Loss: 16881.855469\n",
      "Train Epoch: 454 [92544/225000 (41%)] Loss: 16597.667969\n",
      "Train Epoch: 454 [95040/225000 (42%)] Loss: 16850.609375\n",
      "Train Epoch: 454 [97536/225000 (43%)] Loss: 16765.039062\n",
      "Train Epoch: 454 [100032/225000 (44%)] Loss: 16630.275391\n",
      "Train Epoch: 454 [102528/225000 (46%)] Loss: 16659.976562\n",
      "Train Epoch: 454 [105024/225000 (47%)] Loss: 17094.949219\n",
      "Train Epoch: 454 [107520/225000 (48%)] Loss: 16897.275391\n",
      "Train Epoch: 454 [110016/225000 (49%)] Loss: 16558.453125\n",
      "Train Epoch: 454 [112512/225000 (50%)] Loss: 16179.665039\n",
      "Train Epoch: 454 [115008/225000 (51%)] Loss: 16316.988281\n",
      "Train Epoch: 454 [117504/225000 (52%)] Loss: 16297.230469\n",
      "Train Epoch: 454 [120000/225000 (53%)] Loss: 16182.441406\n",
      "Train Epoch: 454 [122496/225000 (54%)] Loss: 16548.238281\n",
      "Train Epoch: 454 [124992/225000 (56%)] Loss: 16605.367188\n",
      "Train Epoch: 454 [127488/225000 (57%)] Loss: 16725.550781\n",
      "Train Epoch: 454 [129984/225000 (58%)] Loss: 17115.419922\n",
      "Train Epoch: 454 [132480/225000 (59%)] Loss: 18165.203125\n",
      "Train Epoch: 454 [134976/225000 (60%)] Loss: 17004.435547\n",
      "Train Epoch: 454 [137472/225000 (61%)] Loss: 16226.021484\n",
      "Train Epoch: 454 [139968/225000 (62%)] Loss: 16297.881836\n",
      "Train Epoch: 454 [142464/225000 (63%)] Loss: 16358.086914\n",
      "Train Epoch: 454 [144960/225000 (64%)] Loss: 16726.332031\n",
      "Train Epoch: 454 [147456/225000 (66%)] Loss: 17107.679688\n",
      "Train Epoch: 454 [149952/225000 (67%)] Loss: 16311.432617\n",
      "Train Epoch: 454 [152448/225000 (68%)] Loss: 16853.285156\n",
      "Train Epoch: 454 [154944/225000 (69%)] Loss: 16941.548828\n",
      "Train Epoch: 454 [157440/225000 (70%)] Loss: 16499.873047\n",
      "Train Epoch: 454 [159936/225000 (71%)] Loss: 16928.320312\n",
      "Train Epoch: 454 [162432/225000 (72%)] Loss: 16463.619141\n",
      "Train Epoch: 454 [164928/225000 (73%)] Loss: 17087.673828\n",
      "Train Epoch: 454 [167424/225000 (74%)] Loss: 16999.841797\n",
      "Train Epoch: 454 [169920/225000 (76%)] Loss: 16480.585938\n",
      "Train Epoch: 454 [172416/225000 (77%)] Loss: 16352.808594\n",
      "Train Epoch: 454 [174912/225000 (78%)] Loss: 18095.957031\n",
      "Train Epoch: 454 [177408/225000 (79%)] Loss: 16879.078125\n",
      "Train Epoch: 454 [179904/225000 (80%)] Loss: 16941.398438\n",
      "Train Epoch: 454 [182400/225000 (81%)] Loss: 16692.906250\n",
      "Train Epoch: 454 [184896/225000 (82%)] Loss: 16568.714844\n",
      "Train Epoch: 454 [187392/225000 (83%)] Loss: 16484.949219\n",
      "Train Epoch: 454 [189888/225000 (84%)] Loss: 16682.585938\n",
      "Train Epoch: 454 [192384/225000 (86%)] Loss: 16581.615234\n",
      "Train Epoch: 454 [194880/225000 (87%)] Loss: 18175.353516\n",
      "Train Epoch: 454 [197376/225000 (88%)] Loss: 16676.789062\n",
      "Train Epoch: 454 [199872/225000 (89%)] Loss: 16409.050781\n",
      "Train Epoch: 454 [202368/225000 (90%)] Loss: 16733.734375\n",
      "Train Epoch: 454 [204864/225000 (91%)] Loss: 16878.417969\n",
      "Train Epoch: 454 [207360/225000 (92%)] Loss: 16336.143555\n",
      "Train Epoch: 454 [209856/225000 (93%)] Loss: 16802.828125\n",
      "Train Epoch: 454 [212352/225000 (94%)] Loss: 17048.361328\n",
      "Train Epoch: 454 [214848/225000 (95%)] Loss: 16377.459961\n",
      "Train Epoch: 454 [217344/225000 (97%)] Loss: 16464.375000\n",
      "Train Epoch: 454 [219840/225000 (98%)] Loss: 16448.714844\n",
      "Train Epoch: 454 [222336/225000 (99%)] Loss: 16552.187500\n",
      "Train Epoch: 454 [224832/225000 (100%)] Loss: 16079.564453\n",
      "    epoch          : 454\n",
      "    loss           : 16728.188321579033\n",
      "    val_loss       : 16620.30387061152\n",
      "Train Epoch: 455 [192/225000 (0%)] Loss: 16758.244141\n",
      "Train Epoch: 455 [2688/225000 (1%)] Loss: 16719.953125\n",
      "Train Epoch: 455 [5184/225000 (2%)] Loss: 16333.833984\n",
      "Train Epoch: 455 [7680/225000 (3%)] Loss: 16545.949219\n",
      "Train Epoch: 455 [10176/225000 (5%)] Loss: 16477.859375\n",
      "Train Epoch: 455 [12672/225000 (6%)] Loss: 16535.421875\n",
      "Train Epoch: 455 [15168/225000 (7%)] Loss: 16610.097656\n",
      "Train Epoch: 455 [17664/225000 (8%)] Loss: 16801.847656\n",
      "Train Epoch: 455 [20160/225000 (9%)] Loss: 16926.037109\n",
      "Train Epoch: 455 [22656/225000 (10%)] Loss: 16879.179688\n",
      "Train Epoch: 455 [25152/225000 (11%)] Loss: 17819.074219\n",
      "Train Epoch: 455 [27648/225000 (12%)] Loss: 16246.535156\n",
      "Train Epoch: 455 [30144/225000 (13%)] Loss: 17118.164062\n",
      "Train Epoch: 455 [32640/225000 (15%)] Loss: 16861.632812\n",
      "Train Epoch: 455 [35136/225000 (16%)] Loss: 16573.460938\n",
      "Train Epoch: 455 [37632/225000 (17%)] Loss: 16938.462891\n",
      "Train Epoch: 455 [40128/225000 (18%)] Loss: 16614.921875\n",
      "Train Epoch: 455 [42624/225000 (19%)] Loss: 16469.750000\n",
      "Train Epoch: 455 [45120/225000 (20%)] Loss: 16785.093750\n",
      "Train Epoch: 455 [47616/225000 (21%)] Loss: 16592.832031\n",
      "Train Epoch: 455 [50112/225000 (22%)] Loss: 16659.718750\n",
      "Train Epoch: 455 [52608/225000 (23%)] Loss: 16164.720703\n",
      "Train Epoch: 455 [55104/225000 (24%)] Loss: 16723.718750\n",
      "Train Epoch: 455 [57600/225000 (26%)] Loss: 16812.761719\n",
      "Train Epoch: 455 [60096/225000 (27%)] Loss: 18268.921875\n",
      "Train Epoch: 455 [62592/225000 (28%)] Loss: 16885.089844\n",
      "Train Epoch: 455 [65088/225000 (29%)] Loss: 16999.943359\n",
      "Train Epoch: 455 [67584/225000 (30%)] Loss: 16963.601562\n",
      "Train Epoch: 455 [70080/225000 (31%)] Loss: 16472.089844\n",
      "Train Epoch: 455 [72576/225000 (32%)] Loss: 16781.292969\n",
      "Train Epoch: 455 [75072/225000 (33%)] Loss: 16344.516602\n",
      "Train Epoch: 455 [77568/225000 (34%)] Loss: 16256.305664\n",
      "Train Epoch: 455 [80064/225000 (36%)] Loss: 16514.701172\n",
      "Train Epoch: 455 [82560/225000 (37%)] Loss: 16428.882812\n",
      "Train Epoch: 455 [85056/225000 (38%)] Loss: 16978.394531\n",
      "Train Epoch: 455 [87552/225000 (39%)] Loss: 16426.152344\n",
      "Train Epoch: 455 [90048/225000 (40%)] Loss: 16233.078125\n",
      "Train Epoch: 455 [92544/225000 (41%)] Loss: 16857.859375\n",
      "Train Epoch: 455 [95040/225000 (42%)] Loss: 16607.167969\n",
      "Train Epoch: 455 [97536/225000 (43%)] Loss: 16759.289062\n",
      "Train Epoch: 455 [100032/225000 (44%)] Loss: 16546.218750\n",
      "Train Epoch: 455 [102528/225000 (46%)] Loss: 16324.583008\n",
      "Train Epoch: 455 [105024/225000 (47%)] Loss: 16722.839844\n",
      "Train Epoch: 455 [107520/225000 (48%)] Loss: 16657.851562\n",
      "Train Epoch: 455 [110016/225000 (49%)] Loss: 16837.054688\n",
      "Train Epoch: 455 [112512/225000 (50%)] Loss: 16860.441406\n",
      "Train Epoch: 455 [115008/225000 (51%)] Loss: 16744.480469\n",
      "Train Epoch: 455 [117504/225000 (52%)] Loss: 16216.336914\n",
      "Train Epoch: 455 [120000/225000 (53%)] Loss: 16564.349609\n",
      "Train Epoch: 455 [122496/225000 (54%)] Loss: 16629.041016\n",
      "Train Epoch: 455 [124992/225000 (56%)] Loss: 16661.406250\n",
      "Train Epoch: 455 [127488/225000 (57%)] Loss: 16652.205078\n",
      "Train Epoch: 455 [129984/225000 (58%)] Loss: 16554.851562\n",
      "Train Epoch: 455 [132480/225000 (59%)] Loss: 16521.062500\n",
      "Train Epoch: 455 [134976/225000 (60%)] Loss: 17094.931641\n",
      "Train Epoch: 455 [137472/225000 (61%)] Loss: 16717.679688\n",
      "Train Epoch: 455 [139968/225000 (62%)] Loss: 16373.853516\n",
      "Train Epoch: 455 [142464/225000 (63%)] Loss: 16899.253906\n",
      "Train Epoch: 455 [144960/225000 (64%)] Loss: 16685.333984\n",
      "Train Epoch: 455 [147456/225000 (66%)] Loss: 15969.879883\n",
      "Train Epoch: 455 [149952/225000 (67%)] Loss: 16635.642578\n",
      "Train Epoch: 455 [152448/225000 (68%)] Loss: 16821.375000\n",
      "Train Epoch: 455 [154944/225000 (69%)] Loss: 16465.427734\n",
      "Train Epoch: 455 [157440/225000 (70%)] Loss: 16917.281250\n",
      "Train Epoch: 455 [159936/225000 (71%)] Loss: 16521.927734\n",
      "Train Epoch: 455 [162432/225000 (72%)] Loss: 16570.001953\n",
      "Train Epoch: 455 [164928/225000 (73%)] Loss: 16862.201172\n",
      "Train Epoch: 455 [167424/225000 (74%)] Loss: 16722.220703\n",
      "Train Epoch: 455 [169920/225000 (76%)] Loss: 16897.925781\n",
      "Train Epoch: 455 [172416/225000 (77%)] Loss: 17033.195312\n",
      "Train Epoch: 455 [174912/225000 (78%)] Loss: 16827.935547\n",
      "Train Epoch: 455 [177408/225000 (79%)] Loss: 16732.828125\n",
      "Train Epoch: 455 [179904/225000 (80%)] Loss: 16199.771484\n",
      "Train Epoch: 455 [182400/225000 (81%)] Loss: 16612.917969\n",
      "Train Epoch: 455 [184896/225000 (82%)] Loss: 17050.755859\n",
      "Train Epoch: 455 [187392/225000 (83%)] Loss: 16723.707031\n",
      "Train Epoch: 455 [189888/225000 (84%)] Loss: 16329.542969\n",
      "Train Epoch: 455 [192384/225000 (86%)] Loss: 16428.296875\n",
      "Train Epoch: 455 [194880/225000 (87%)] Loss: 16735.578125\n",
      "Train Epoch: 455 [197376/225000 (88%)] Loss: 16647.716797\n",
      "Train Epoch: 455 [199872/225000 (89%)] Loss: 17278.169922\n",
      "Train Epoch: 455 [202368/225000 (90%)] Loss: 17008.515625\n",
      "Train Epoch: 455 [204864/225000 (91%)] Loss: 16135.425781\n",
      "Train Epoch: 455 [207360/225000 (92%)] Loss: 16905.839844\n",
      "Train Epoch: 455 [209856/225000 (93%)] Loss: 16527.400391\n",
      "Train Epoch: 455 [212352/225000 (94%)] Loss: 17006.027344\n",
      "Train Epoch: 455 [214848/225000 (95%)] Loss: 16291.600586\n",
      "Train Epoch: 455 [217344/225000 (97%)] Loss: 17048.597656\n",
      "Train Epoch: 455 [219840/225000 (98%)] Loss: 16271.994141\n",
      "Train Epoch: 455 [222336/225000 (99%)] Loss: 16880.582031\n",
      "Train Epoch: 455 [224832/225000 (100%)] Loss: 16485.335938\n",
      "    epoch          : 455\n",
      "    loss           : 16690.01249450059\n",
      "    val_loss       : 16636.191194215804\n",
      "Train Epoch: 456 [192/225000 (0%)] Loss: 16811.148438\n",
      "Train Epoch: 456 [2688/225000 (1%)] Loss: 16184.374023\n",
      "Train Epoch: 456 [5184/225000 (2%)] Loss: 16566.417969\n",
      "Train Epoch: 456 [7680/225000 (3%)] Loss: 16821.554688\n",
      "Train Epoch: 456 [10176/225000 (5%)] Loss: 16280.215820\n",
      "Train Epoch: 456 [12672/225000 (6%)] Loss: 16588.328125\n",
      "Train Epoch: 456 [15168/225000 (7%)] Loss: 16908.089844\n",
      "Train Epoch: 456 [17664/225000 (8%)] Loss: 16481.679688\n",
      "Train Epoch: 456 [20160/225000 (9%)] Loss: 16512.808594\n",
      "Train Epoch: 456 [22656/225000 (10%)] Loss: 16632.339844\n",
      "Train Epoch: 456 [25152/225000 (11%)] Loss: 16324.810547\n",
      "Train Epoch: 456 [27648/225000 (12%)] Loss: 16921.839844\n",
      "Train Epoch: 456 [30144/225000 (13%)] Loss: 16270.483398\n",
      "Train Epoch: 456 [32640/225000 (15%)] Loss: 17009.330078\n",
      "Train Epoch: 456 [35136/225000 (16%)] Loss: 16908.425781\n",
      "Train Epoch: 456 [37632/225000 (17%)] Loss: 16444.007812\n",
      "Train Epoch: 456 [40128/225000 (18%)] Loss: 16752.013672\n",
      "Train Epoch: 456 [42624/225000 (19%)] Loss: 16379.793945\n",
      "Train Epoch: 456 [45120/225000 (20%)] Loss: 16868.644531\n",
      "Train Epoch: 456 [47616/225000 (21%)] Loss: 16483.140625\n",
      "Train Epoch: 456 [50112/225000 (22%)] Loss: 16489.296875\n",
      "Train Epoch: 456 [52608/225000 (23%)] Loss: 16435.546875\n",
      "Train Epoch: 456 [55104/225000 (24%)] Loss: 17156.699219\n",
      "Train Epoch: 456 [57600/225000 (26%)] Loss: 16517.140625\n",
      "Train Epoch: 456 [60096/225000 (27%)] Loss: 16434.806641\n",
      "Train Epoch: 456 [62592/225000 (28%)] Loss: 17944.867188\n",
      "Train Epoch: 456 [65088/225000 (29%)] Loss: 16220.946289\n",
      "Train Epoch: 456 [67584/225000 (30%)] Loss: 16387.083984\n",
      "Train Epoch: 456 [70080/225000 (31%)] Loss: 16434.083984\n",
      "Train Epoch: 456 [72576/225000 (32%)] Loss: 16289.372070\n",
      "Train Epoch: 456 [75072/225000 (33%)] Loss: 17060.248047\n",
      "Train Epoch: 456 [77568/225000 (34%)] Loss: 16684.138672\n",
      "Train Epoch: 456 [80064/225000 (36%)] Loss: 16748.410156\n",
      "Train Epoch: 456 [82560/225000 (37%)] Loss: 16573.517578\n",
      "Train Epoch: 456 [85056/225000 (38%)] Loss: 16939.820312\n",
      "Train Epoch: 456 [87552/225000 (39%)] Loss: 16499.488281\n",
      "Train Epoch: 456 [90048/225000 (40%)] Loss: 16626.871094\n",
      "Train Epoch: 456 [92544/225000 (41%)] Loss: 16532.189453\n",
      "Train Epoch: 456 [95040/225000 (42%)] Loss: 17534.568359\n",
      "Train Epoch: 456 [97536/225000 (43%)] Loss: 16557.498047\n",
      "Train Epoch: 456 [100032/225000 (44%)] Loss: 16417.867188\n",
      "Train Epoch: 456 [102528/225000 (46%)] Loss: 16682.578125\n",
      "Train Epoch: 456 [105024/225000 (47%)] Loss: 16707.855469\n",
      "Train Epoch: 456 [107520/225000 (48%)] Loss: 25101.958984\n",
      "Train Epoch: 456 [110016/225000 (49%)] Loss: 16712.273438\n",
      "Train Epoch: 456 [112512/225000 (50%)] Loss: 16523.753906\n",
      "Train Epoch: 456 [115008/225000 (51%)] Loss: 16365.439453\n",
      "Train Epoch: 456 [117504/225000 (52%)] Loss: 16796.765625\n",
      "Train Epoch: 456 [120000/225000 (53%)] Loss: 16292.503906\n",
      "Train Epoch: 456 [122496/225000 (54%)] Loss: 16725.789062\n",
      "Train Epoch: 456 [124992/225000 (56%)] Loss: 16967.058594\n",
      "Train Epoch: 456 [127488/225000 (57%)] Loss: 16364.917969\n",
      "Train Epoch: 456 [129984/225000 (58%)] Loss: 16434.976562\n",
      "Train Epoch: 456 [132480/225000 (59%)] Loss: 16299.135742\n",
      "Train Epoch: 456 [134976/225000 (60%)] Loss: 16319.330078\n",
      "Train Epoch: 456 [137472/225000 (61%)] Loss: 16833.519531\n",
      "Train Epoch: 456 [139968/225000 (62%)] Loss: 16602.382812\n",
      "Train Epoch: 456 [142464/225000 (63%)] Loss: 16601.851562\n",
      "Train Epoch: 456 [144960/225000 (64%)] Loss: 16649.308594\n",
      "Train Epoch: 456 [147456/225000 (66%)] Loss: 16466.982422\n",
      "Train Epoch: 456 [149952/225000 (67%)] Loss: 16546.332031\n",
      "Train Epoch: 456 [152448/225000 (68%)] Loss: 16897.535156\n",
      "Train Epoch: 456 [154944/225000 (69%)] Loss: 16976.437500\n",
      "Train Epoch: 456 [157440/225000 (70%)] Loss: 17219.652344\n",
      "Train Epoch: 456 [159936/225000 (71%)] Loss: 16732.468750\n",
      "Train Epoch: 456 [162432/225000 (72%)] Loss: 17078.851562\n",
      "Train Epoch: 456 [164928/225000 (73%)] Loss: 16508.660156\n",
      "Train Epoch: 456 [167424/225000 (74%)] Loss: 16619.214844\n",
      "Train Epoch: 456 [169920/225000 (76%)] Loss: 16283.060547\n",
      "Train Epoch: 456 [172416/225000 (77%)] Loss: 16513.466797\n",
      "Train Epoch: 456 [174912/225000 (78%)] Loss: 17998.707031\n",
      "Train Epoch: 456 [177408/225000 (79%)] Loss: 16763.718750\n",
      "Train Epoch: 456 [179904/225000 (80%)] Loss: 16712.476562\n",
      "Train Epoch: 456 [182400/225000 (81%)] Loss: 16548.269531\n",
      "Train Epoch: 456 [184896/225000 (82%)] Loss: 16555.101562\n",
      "Train Epoch: 456 [187392/225000 (83%)] Loss: 16849.507812\n",
      "Train Epoch: 456 [189888/225000 (84%)] Loss: 16900.968750\n",
      "Train Epoch: 456 [192384/225000 (86%)] Loss: 16110.802734\n",
      "Train Epoch: 456 [194880/225000 (87%)] Loss: 16763.148438\n",
      "Train Epoch: 456 [197376/225000 (88%)] Loss: 16724.386719\n",
      "Train Epoch: 456 [199872/225000 (89%)] Loss: 19605.472656\n",
      "Train Epoch: 456 [202368/225000 (90%)] Loss: 16417.964844\n",
      "Train Epoch: 456 [204864/225000 (91%)] Loss: 17185.617188\n",
      "Train Epoch: 456 [207360/225000 (92%)] Loss: 16893.673828\n",
      "Train Epoch: 456 [209856/225000 (93%)] Loss: 16611.089844\n",
      "Train Epoch: 456 [212352/225000 (94%)] Loss: 16946.929688\n",
      "Train Epoch: 456 [214848/225000 (95%)] Loss: 16868.339844\n",
      "Train Epoch: 456 [217344/225000 (97%)] Loss: 16530.673828\n",
      "Train Epoch: 456 [219840/225000 (98%)] Loss: 17009.222656\n",
      "Train Epoch: 456 [222336/225000 (99%)] Loss: 16889.847656\n",
      "Train Epoch: 456 [224832/225000 (100%)] Loss: 16719.566406\n",
      "    epoch          : 456\n",
      "    loss           : 16703.63630129453\n",
      "    val_loss       : 16671.2616336191\n",
      "Train Epoch: 457 [192/225000 (0%)] Loss: 16447.599609\n",
      "Train Epoch: 457 [2688/225000 (1%)] Loss: 16680.119141\n",
      "Train Epoch: 457 [5184/225000 (2%)] Loss: 16598.789062\n",
      "Train Epoch: 457 [7680/225000 (3%)] Loss: 16895.904297\n",
      "Train Epoch: 457 [10176/225000 (5%)] Loss: 16679.523438\n",
      "Train Epoch: 457 [12672/225000 (6%)] Loss: 16669.990234\n",
      "Train Epoch: 457 [15168/225000 (7%)] Loss: 16517.644531\n",
      "Train Epoch: 457 [17664/225000 (8%)] Loss: 16880.402344\n",
      "Train Epoch: 457 [20160/225000 (9%)] Loss: 17003.882812\n",
      "Train Epoch: 457 [22656/225000 (10%)] Loss: 16689.476562\n",
      "Train Epoch: 457 [25152/225000 (11%)] Loss: 16746.478516\n",
      "Train Epoch: 457 [27648/225000 (12%)] Loss: 16275.961914\n",
      "Train Epoch: 457 [30144/225000 (13%)] Loss: 16191.802734\n",
      "Train Epoch: 457 [32640/225000 (15%)] Loss: 16589.755859\n",
      "Train Epoch: 457 [35136/225000 (16%)] Loss: 16971.332031\n",
      "Train Epoch: 457 [37632/225000 (17%)] Loss: 16761.695312\n",
      "Train Epoch: 457 [40128/225000 (18%)] Loss: 16888.738281\n",
      "Train Epoch: 457 [42624/225000 (19%)] Loss: 16786.779297\n",
      "Train Epoch: 457 [45120/225000 (20%)] Loss: 16786.466797\n",
      "Train Epoch: 457 [47616/225000 (21%)] Loss: 16804.746094\n",
      "Train Epoch: 457 [50112/225000 (22%)] Loss: 16530.789062\n",
      "Train Epoch: 457 [52608/225000 (23%)] Loss: 16849.166016\n",
      "Train Epoch: 457 [55104/225000 (24%)] Loss: 16649.267578\n",
      "Train Epoch: 457 [57600/225000 (26%)] Loss: 16560.453125\n",
      "Train Epoch: 457 [60096/225000 (27%)] Loss: 18166.939453\n",
      "Train Epoch: 457 [62592/225000 (28%)] Loss: 16390.275391\n",
      "Train Epoch: 457 [65088/225000 (29%)] Loss: 16525.699219\n",
      "Train Epoch: 457 [67584/225000 (30%)] Loss: 16232.014648\n",
      "Train Epoch: 457 [70080/225000 (31%)] Loss: 16759.558594\n",
      "Train Epoch: 457 [72576/225000 (32%)] Loss: 16101.064453\n",
      "Train Epoch: 457 [75072/225000 (33%)] Loss: 16644.443359\n",
      "Train Epoch: 457 [77568/225000 (34%)] Loss: 16571.199219\n",
      "Train Epoch: 457 [80064/225000 (36%)] Loss: 16122.158203\n",
      "Train Epoch: 457 [82560/225000 (37%)] Loss: 17178.359375\n",
      "Train Epoch: 457 [85056/225000 (38%)] Loss: 16609.296875\n",
      "Train Epoch: 457 [87552/225000 (39%)] Loss: 17224.609375\n",
      "Train Epoch: 457 [90048/225000 (40%)] Loss: 18199.214844\n",
      "Train Epoch: 457 [92544/225000 (41%)] Loss: 16713.300781\n",
      "Train Epoch: 457 [95040/225000 (42%)] Loss: 16708.144531\n",
      "Train Epoch: 457 [97536/225000 (43%)] Loss: 16049.467773\n",
      "Train Epoch: 457 [100032/225000 (44%)] Loss: 16503.550781\n",
      "Train Epoch: 457 [102528/225000 (46%)] Loss: 16576.777344\n",
      "Train Epoch: 457 [105024/225000 (47%)] Loss: 16503.601562\n",
      "Train Epoch: 457 [107520/225000 (48%)] Loss: 16868.212891\n",
      "Train Epoch: 457 [110016/225000 (49%)] Loss: 16643.261719\n",
      "Train Epoch: 457 [112512/225000 (50%)] Loss: 16542.410156\n",
      "Train Epoch: 457 [115008/225000 (51%)] Loss: 16484.707031\n",
      "Train Epoch: 457 [117504/225000 (52%)] Loss: 16577.888672\n",
      "Train Epoch: 457 [120000/225000 (53%)] Loss: 16534.693359\n",
      "Train Epoch: 457 [122496/225000 (54%)] Loss: 17082.644531\n",
      "Train Epoch: 457 [124992/225000 (56%)] Loss: 16947.748047\n",
      "Train Epoch: 457 [127488/225000 (57%)] Loss: 17089.910156\n",
      "Train Epoch: 457 [129984/225000 (58%)] Loss: 16734.255859\n",
      "Train Epoch: 457 [132480/225000 (59%)] Loss: 16396.416016\n",
      "Train Epoch: 457 [134976/225000 (60%)] Loss: 16574.728516\n",
      "Train Epoch: 457 [137472/225000 (61%)] Loss: 16152.222656\n",
      "Train Epoch: 457 [139968/225000 (62%)] Loss: 16634.605469\n",
      "Train Epoch: 457 [142464/225000 (63%)] Loss: 16912.416016\n",
      "Train Epoch: 457 [144960/225000 (64%)] Loss: 16664.871094\n",
      "Train Epoch: 457 [147456/225000 (66%)] Loss: 16851.203125\n",
      "Train Epoch: 457 [149952/225000 (67%)] Loss: 16705.574219\n",
      "Train Epoch: 457 [152448/225000 (68%)] Loss: 16453.773438\n",
      "Train Epoch: 457 [154944/225000 (69%)] Loss: 16689.441406\n",
      "Train Epoch: 457 [157440/225000 (70%)] Loss: 16818.066406\n",
      "Train Epoch: 457 [159936/225000 (71%)] Loss: 17135.457031\n",
      "Train Epoch: 457 [162432/225000 (72%)] Loss: 16965.410156\n",
      "Train Epoch: 457 [164928/225000 (73%)] Loss: 16439.175781\n",
      "Train Epoch: 457 [167424/225000 (74%)] Loss: 16994.244141\n",
      "Train Epoch: 457 [169920/225000 (76%)] Loss: 16550.355469\n",
      "Train Epoch: 457 [172416/225000 (77%)] Loss: 16882.917969\n",
      "Train Epoch: 457 [174912/225000 (78%)] Loss: 16468.441406\n",
      "Train Epoch: 457 [177408/225000 (79%)] Loss: 16511.701172\n",
      "Train Epoch: 457 [179904/225000 (80%)] Loss: 16621.316406\n",
      "Train Epoch: 457 [182400/225000 (81%)] Loss: 16958.839844\n",
      "Train Epoch: 457 [184896/225000 (82%)] Loss: 16498.960938\n",
      "Train Epoch: 457 [187392/225000 (83%)] Loss: 18368.765625\n",
      "Train Epoch: 457 [189888/225000 (84%)] Loss: 16875.593750\n",
      "Train Epoch: 457 [192384/225000 (86%)] Loss: 16449.652344\n",
      "Train Epoch: 457 [194880/225000 (87%)] Loss: 16575.734375\n",
      "Train Epoch: 457 [197376/225000 (88%)] Loss: 16494.974609\n",
      "Train Epoch: 457 [199872/225000 (89%)] Loss: 16852.609375\n",
      "Train Epoch: 457 [202368/225000 (90%)] Loss: 16431.517578\n",
      "Train Epoch: 457 [204864/225000 (91%)] Loss: 16694.156250\n",
      "Train Epoch: 457 [207360/225000 (92%)] Loss: 16910.335938\n",
      "Train Epoch: 457 [209856/225000 (93%)] Loss: 16567.722656\n",
      "Train Epoch: 457 [212352/225000 (94%)] Loss: 16730.539062\n",
      "Train Epoch: 457 [214848/225000 (95%)] Loss: 16692.767578\n",
      "Train Epoch: 457 [217344/225000 (97%)] Loss: 16766.062500\n",
      "Train Epoch: 457 [219840/225000 (98%)] Loss: 16310.812500\n",
      "Train Epoch: 457 [222336/225000 (99%)] Loss: 16399.203125\n",
      "Train Epoch: 457 [224832/225000 (100%)] Loss: 16694.132812\n",
      "    epoch          : 457\n",
      "    loss           : 16724.795071025757\n",
      "    val_loss       : 16681.10963691737\n",
      "Train Epoch: 458 [192/225000 (0%)] Loss: 16691.332031\n",
      "Train Epoch: 458 [2688/225000 (1%)] Loss: 17170.025391\n",
      "Train Epoch: 458 [5184/225000 (2%)] Loss: 16873.460938\n",
      "Train Epoch: 458 [7680/225000 (3%)] Loss: 16343.660156\n",
      "Train Epoch: 458 [10176/225000 (5%)] Loss: 16654.605469\n",
      "Train Epoch: 458 [12672/225000 (6%)] Loss: 16737.792969\n",
      "Train Epoch: 458 [15168/225000 (7%)] Loss: 16932.800781\n",
      "Train Epoch: 458 [17664/225000 (8%)] Loss: 18458.839844\n",
      "Train Epoch: 458 [20160/225000 (9%)] Loss: 16944.218750\n",
      "Train Epoch: 458 [22656/225000 (10%)] Loss: 16960.300781\n",
      "Train Epoch: 458 [25152/225000 (11%)] Loss: 16800.074219\n",
      "Train Epoch: 458 [27648/225000 (12%)] Loss: 16801.632812\n",
      "Train Epoch: 458 [30144/225000 (13%)] Loss: 16558.017578\n",
      "Train Epoch: 458 [32640/225000 (15%)] Loss: 16673.972656\n",
      "Train Epoch: 458 [35136/225000 (16%)] Loss: 16609.746094\n",
      "Train Epoch: 458 [37632/225000 (17%)] Loss: 16845.843750\n",
      "Train Epoch: 458 [40128/225000 (18%)] Loss: 16804.867188\n",
      "Train Epoch: 458 [42624/225000 (19%)] Loss: 16443.035156\n",
      "Train Epoch: 458 [45120/225000 (20%)] Loss: 16541.359375\n",
      "Train Epoch: 458 [47616/225000 (21%)] Loss: 16360.134766\n",
      "Train Epoch: 458 [50112/225000 (22%)] Loss: 16559.273438\n",
      "Train Epoch: 458 [52608/225000 (23%)] Loss: 16630.804688\n",
      "Train Epoch: 458 [55104/225000 (24%)] Loss: 16235.308594\n",
      "Train Epoch: 458 [57600/225000 (26%)] Loss: 16145.605469\n",
      "Train Epoch: 458 [60096/225000 (27%)] Loss: 16846.802734\n",
      "Train Epoch: 458 [62592/225000 (28%)] Loss: 16751.238281\n",
      "Train Epoch: 458 [65088/225000 (29%)] Loss: 17118.068359\n",
      "Train Epoch: 458 [67584/225000 (30%)] Loss: 16573.226562\n",
      "Train Epoch: 458 [70080/225000 (31%)] Loss: 16330.512695\n",
      "Train Epoch: 458 [72576/225000 (32%)] Loss: 16866.472656\n",
      "Train Epoch: 458 [75072/225000 (33%)] Loss: 16799.910156\n",
      "Train Epoch: 458 [77568/225000 (34%)] Loss: 16985.687500\n",
      "Train Epoch: 458 [80064/225000 (36%)] Loss: 16491.269531\n",
      "Train Epoch: 458 [82560/225000 (37%)] Loss: 16653.210938\n",
      "Train Epoch: 458 [85056/225000 (38%)] Loss: 16514.472656\n",
      "Train Epoch: 458 [87552/225000 (39%)] Loss: 16967.404297\n",
      "Train Epoch: 458 [90048/225000 (40%)] Loss: 16827.859375\n",
      "Train Epoch: 458 [92544/225000 (41%)] Loss: 16680.998047\n",
      "Train Epoch: 458 [95040/225000 (42%)] Loss: 17024.722656\n",
      "Train Epoch: 458 [97536/225000 (43%)] Loss: 16971.041016\n",
      "Train Epoch: 458 [100032/225000 (44%)] Loss: 16453.568359\n",
      "Train Epoch: 458 [102528/225000 (46%)] Loss: 16812.390625\n",
      "Train Epoch: 458 [105024/225000 (47%)] Loss: 17966.449219\n",
      "Train Epoch: 458 [107520/225000 (48%)] Loss: 16227.481445\n",
      "Train Epoch: 458 [110016/225000 (49%)] Loss: 16519.339844\n",
      "Train Epoch: 458 [112512/225000 (50%)] Loss: 16853.402344\n",
      "Train Epoch: 458 [115008/225000 (51%)] Loss: 16634.269531\n",
      "Train Epoch: 458 [117504/225000 (52%)] Loss: 16567.609375\n",
      "Train Epoch: 458 [120000/225000 (53%)] Loss: 16246.613281\n",
      "Train Epoch: 458 [122496/225000 (54%)] Loss: 17037.703125\n",
      "Train Epoch: 458 [124992/225000 (56%)] Loss: 16857.625000\n",
      "Train Epoch: 458 [127488/225000 (57%)] Loss: 15945.455078\n",
      "Train Epoch: 458 [129984/225000 (58%)] Loss: 16985.275391\n",
      "Train Epoch: 458 [132480/225000 (59%)] Loss: 16377.177734\n",
      "Train Epoch: 458 [134976/225000 (60%)] Loss: 16250.942383\n",
      "Train Epoch: 458 [137472/225000 (61%)] Loss: 16591.601562\n",
      "Train Epoch: 458 [139968/225000 (62%)] Loss: 16480.982422\n",
      "Train Epoch: 458 [142464/225000 (63%)] Loss: 16288.031250\n",
      "Train Epoch: 458 [144960/225000 (64%)] Loss: 16092.669922\n",
      "Train Epoch: 458 [147456/225000 (66%)] Loss: 16684.787109\n",
      "Train Epoch: 458 [149952/225000 (67%)] Loss: 16940.443359\n",
      "Train Epoch: 458 [152448/225000 (68%)] Loss: 16718.375000\n",
      "Train Epoch: 458 [154944/225000 (69%)] Loss: 16725.623047\n",
      "Train Epoch: 458 [157440/225000 (70%)] Loss: 16269.761719\n",
      "Train Epoch: 458 [159936/225000 (71%)] Loss: 16662.169922\n",
      "Train Epoch: 458 [162432/225000 (72%)] Loss: 16164.701172\n",
      "Train Epoch: 458 [164928/225000 (73%)] Loss: 16762.765625\n",
      "Train Epoch: 458 [167424/225000 (74%)] Loss: 16261.947266\n",
      "Train Epoch: 458 [169920/225000 (76%)] Loss: 16553.146484\n",
      "Train Epoch: 458 [172416/225000 (77%)] Loss: 16604.611328\n",
      "Train Epoch: 458 [174912/225000 (78%)] Loss: 16783.792969\n",
      "Train Epoch: 458 [177408/225000 (79%)] Loss: 15894.395508\n",
      "Train Epoch: 458 [179904/225000 (80%)] Loss: 16727.707031\n",
      "Train Epoch: 458 [182400/225000 (81%)] Loss: 16576.179688\n",
      "Train Epoch: 458 [184896/225000 (82%)] Loss: 16215.319336\n",
      "Train Epoch: 458 [187392/225000 (83%)] Loss: 16264.792969\n",
      "Train Epoch: 458 [189888/225000 (84%)] Loss: 16550.552734\n",
      "Train Epoch: 458 [192384/225000 (86%)] Loss: 16361.519531\n",
      "Train Epoch: 458 [194880/225000 (87%)] Loss: 16617.107422\n",
      "Train Epoch: 458 [197376/225000 (88%)] Loss: 16482.152344\n",
      "Train Epoch: 458 [199872/225000 (89%)] Loss: 16820.480469\n",
      "Train Epoch: 458 [202368/225000 (90%)] Loss: 16302.166992\n",
      "Train Epoch: 458 [204864/225000 (91%)] Loss: 16607.343750\n",
      "Train Epoch: 458 [207360/225000 (92%)] Loss: 16215.329102\n",
      "Train Epoch: 458 [209856/225000 (93%)] Loss: 16458.744141\n",
      "Train Epoch: 458 [212352/225000 (94%)] Loss: 16262.988281\n",
      "Train Epoch: 458 [214848/225000 (95%)] Loss: 16667.853516\n",
      "Train Epoch: 458 [217344/225000 (97%)] Loss: 16102.423828\n",
      "Train Epoch: 458 [219840/225000 (98%)] Loss: 16832.125000\n",
      "Train Epoch: 458 [222336/225000 (99%)] Loss: 16400.097656\n",
      "Train Epoch: 458 [224832/225000 (100%)] Loss: 16869.734375\n",
      "    epoch          : 458\n",
      "    loss           : 16730.559560313566\n",
      "    val_loss       : 16659.515595654495\n",
      "Train Epoch: 459 [192/225000 (0%)] Loss: 18363.351562\n",
      "Train Epoch: 459 [2688/225000 (1%)] Loss: 16993.546875\n",
      "Train Epoch: 459 [5184/225000 (2%)] Loss: 16801.703125\n",
      "Train Epoch: 459 [7680/225000 (3%)] Loss: 16166.448242\n",
      "Train Epoch: 459 [10176/225000 (5%)] Loss: 16775.304688\n",
      "Train Epoch: 459 [12672/225000 (6%)] Loss: 16285.697266\n",
      "Train Epoch: 459 [15168/225000 (7%)] Loss: 16598.292969\n",
      "Train Epoch: 459 [17664/225000 (8%)] Loss: 16634.705078\n",
      "Train Epoch: 459 [20160/225000 (9%)] Loss: 16819.693359\n",
      "Train Epoch: 459 [22656/225000 (10%)] Loss: 16504.769531\n",
      "Train Epoch: 459 [25152/225000 (11%)] Loss: 16545.609375\n",
      "Train Epoch: 459 [27648/225000 (12%)] Loss: 16648.843750\n",
      "Train Epoch: 459 [30144/225000 (13%)] Loss: 17069.410156\n",
      "Train Epoch: 459 [32640/225000 (15%)] Loss: 16364.096680\n",
      "Train Epoch: 459 [35136/225000 (16%)] Loss: 16702.558594\n",
      "Train Epoch: 459 [37632/225000 (17%)] Loss: 16435.011719\n",
      "Train Epoch: 459 [40128/225000 (18%)] Loss: 16657.302734\n",
      "Train Epoch: 459 [42624/225000 (19%)] Loss: 16967.804688\n",
      "Train Epoch: 459 [45120/225000 (20%)] Loss: 16422.761719\n",
      "Train Epoch: 459 [47616/225000 (21%)] Loss: 18154.705078\n",
      "Train Epoch: 459 [50112/225000 (22%)] Loss: 16219.663086\n",
      "Train Epoch: 459 [52608/225000 (23%)] Loss: 16739.796875\n",
      "Train Epoch: 459 [55104/225000 (24%)] Loss: 16393.937500\n",
      "Train Epoch: 459 [57600/225000 (26%)] Loss: 16796.806641\n",
      "Train Epoch: 459 [60096/225000 (27%)] Loss: 16348.947266\n",
      "Train Epoch: 459 [62592/225000 (28%)] Loss: 18574.953125\n",
      "Train Epoch: 459 [65088/225000 (29%)] Loss: 16438.509766\n",
      "Train Epoch: 459 [67584/225000 (30%)] Loss: 16801.445312\n",
      "Train Epoch: 459 [70080/225000 (31%)] Loss: 16718.433594\n",
      "Train Epoch: 459 [72576/225000 (32%)] Loss: 16527.785156\n",
      "Train Epoch: 459 [75072/225000 (33%)] Loss: 16104.134766\n",
      "Train Epoch: 459 [77568/225000 (34%)] Loss: 17191.496094\n",
      "Train Epoch: 459 [80064/225000 (36%)] Loss: 16583.287109\n",
      "Train Epoch: 459 [82560/225000 (37%)] Loss: 18441.058594\n",
      "Train Epoch: 459 [85056/225000 (38%)] Loss: 18628.285156\n",
      "Train Epoch: 459 [87552/225000 (39%)] Loss: 16610.705078\n",
      "Train Epoch: 459 [90048/225000 (40%)] Loss: 16435.007812\n",
      "Train Epoch: 459 [92544/225000 (41%)] Loss: 16492.632812\n",
      "Train Epoch: 459 [95040/225000 (42%)] Loss: 16519.230469\n",
      "Train Epoch: 459 [97536/225000 (43%)] Loss: 16220.913086\n",
      "Train Epoch: 459 [100032/225000 (44%)] Loss: 16711.097656\n",
      "Train Epoch: 459 [102528/225000 (46%)] Loss: 16896.927734\n",
      "Train Epoch: 459 [105024/225000 (47%)] Loss: 16277.270508\n",
      "Train Epoch: 459 [107520/225000 (48%)] Loss: 16784.884766\n",
      "Train Epoch: 459 [110016/225000 (49%)] Loss: 16770.945312\n",
      "Train Epoch: 459 [112512/225000 (50%)] Loss: 17006.517578\n",
      "Train Epoch: 459 [115008/225000 (51%)] Loss: 16617.056641\n",
      "Train Epoch: 459 [117504/225000 (52%)] Loss: 16831.597656\n",
      "Train Epoch: 459 [120000/225000 (53%)] Loss: 16900.953125\n",
      "Train Epoch: 459 [122496/225000 (54%)] Loss: 17045.125000\n",
      "Train Epoch: 459 [124992/225000 (56%)] Loss: 16716.910156\n",
      "Train Epoch: 459 [127488/225000 (57%)] Loss: 16669.330078\n",
      "Train Epoch: 459 [129984/225000 (58%)] Loss: 16782.691406\n",
      "Train Epoch: 459 [132480/225000 (59%)] Loss: 16940.119141\n",
      "Train Epoch: 459 [134976/225000 (60%)] Loss: 16603.523438\n",
      "Train Epoch: 459 [137472/225000 (61%)] Loss: 16897.349609\n",
      "Train Epoch: 459 [139968/225000 (62%)] Loss: 16440.941406\n",
      "Train Epoch: 459 [142464/225000 (63%)] Loss: 16637.847656\n",
      "Train Epoch: 459 [144960/225000 (64%)] Loss: 16585.466797\n",
      "Train Epoch: 459 [147456/225000 (66%)] Loss: 16520.671875\n",
      "Train Epoch: 459 [149952/225000 (67%)] Loss: 16491.513672\n",
      "Train Epoch: 459 [152448/225000 (68%)] Loss: 16595.839844\n",
      "Train Epoch: 459 [154944/225000 (69%)] Loss: 16425.292969\n",
      "Train Epoch: 459 [157440/225000 (70%)] Loss: 16317.845703\n",
      "Train Epoch: 459 [159936/225000 (71%)] Loss: 16720.279297\n",
      "Train Epoch: 459 [162432/225000 (72%)] Loss: 17090.273438\n",
      "Train Epoch: 459 [164928/225000 (73%)] Loss: 16365.596680\n",
      "Train Epoch: 459 [167424/225000 (74%)] Loss: 16780.019531\n",
      "Train Epoch: 459 [169920/225000 (76%)] Loss: 16688.992188\n",
      "Train Epoch: 459 [172416/225000 (77%)] Loss: 16891.453125\n",
      "Train Epoch: 459 [174912/225000 (78%)] Loss: 16193.107422\n",
      "Train Epoch: 459 [177408/225000 (79%)] Loss: 16268.128906\n",
      "Train Epoch: 459 [179904/225000 (80%)] Loss: 16267.896484\n",
      "Train Epoch: 459 [182400/225000 (81%)] Loss: 17037.332031\n",
      "Train Epoch: 459 [184896/225000 (82%)] Loss: 16536.601562\n",
      "Train Epoch: 459 [187392/225000 (83%)] Loss: 16361.116211\n",
      "Train Epoch: 459 [189888/225000 (84%)] Loss: 16167.219727\n",
      "Train Epoch: 459 [192384/225000 (86%)] Loss: 16630.984375\n",
      "Train Epoch: 459 [194880/225000 (87%)] Loss: 16806.501953\n",
      "Train Epoch: 459 [197376/225000 (88%)] Loss: 16494.773438\n",
      "Train Epoch: 459 [199872/225000 (89%)] Loss: 16780.167969\n",
      "Train Epoch: 459 [202368/225000 (90%)] Loss: 16927.953125\n",
      "Train Epoch: 459 [204864/225000 (91%)] Loss: 17046.601562\n",
      "Train Epoch: 459 [207360/225000 (92%)] Loss: 16489.148438\n",
      "Train Epoch: 459 [209856/225000 (93%)] Loss: 16440.132812\n",
      "Train Epoch: 459 [212352/225000 (94%)] Loss: 16536.562500\n",
      "Train Epoch: 459 [214848/225000 (95%)] Loss: 16545.355469\n",
      "Train Epoch: 459 [217344/225000 (97%)] Loss: 16971.623047\n",
      "Train Epoch: 459 [219840/225000 (98%)] Loss: 16884.238281\n",
      "Train Epoch: 459 [222336/225000 (99%)] Loss: 16837.865234\n",
      "Train Epoch: 459 [224832/225000 (100%)] Loss: 16708.402344\n",
      "    epoch          : 459\n",
      "    loss           : 16722.213501393184\n",
      "    val_loss       : 16592.797565304598\n",
      "Validation performance didn't improve for 75 epochs. Training stops.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MolecularVaeCategoryModel(\n",
       "  (_category): FreeCategory(\n",
       "    (generator_0): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(196, 50, num_layers=3, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=50, out_features=34, bias=True)\n",
       "        (1): LogSoftmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_0_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_1): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(196, 50, num_layers=3, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=50, out_features=34, bias=True)\n",
       "        (1): LogSoftmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_1_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=50, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(50, 196, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_2): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(196, 488, num_layers=3, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=488, out_features=34, bias=True)\n",
       "        (1): LogSoftmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_2_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_3): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(196, 488, num_layers=3, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=488, out_features=34, bias=True)\n",
       "        (1): LogSoftmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_3_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=488, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(488, 196, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_4): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(196, 501, num_layers=3, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=501, out_features=34, bias=True)\n",
       "        (1): LogSoftmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_4_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_5): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(196, 501, num_layers=3, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=501, out_features=34, bias=True)\n",
       "        (1): LogSoftmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_5_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=501, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(501, 196, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_6): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(292, 50, num_layers=3, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=50, out_features=34, bias=True)\n",
       "        (1): LogSoftmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_6_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_7): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(292, 50, num_layers=3, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=50, out_features=34, bias=True)\n",
       "        (1): LogSoftmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_7_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=50, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(50, 292, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_8): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(292, 488, num_layers=3, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=488, out_features=34, bias=True)\n",
       "        (1): LogSoftmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_8_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_9): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(292, 488, num_layers=3, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=488, out_features=34, bias=True)\n",
       "        (1): LogSoftmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_9_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=488, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(488, 292, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_10): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(292, 501, num_layers=3, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=501, out_features=34, bias=True)\n",
       "        (1): LogSoftmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_10_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_11): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(292, 501, num_layers=3, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=501, out_features=34, bias=True)\n",
       "        (1): LogSoftmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_11_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=501, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(501, 292, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_12): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(435, 50, num_layers=3, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=50, out_features=34, bias=True)\n",
       "        (1): LogSoftmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_12_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_13): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(435, 50, num_layers=3, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=50, out_features=34, bias=True)\n",
       "        (1): LogSoftmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_13_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=50, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(50, 435, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_14): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(435, 488, num_layers=3, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=488, out_features=34, bias=True)\n",
       "        (1): LogSoftmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_14_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_15): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(435, 488, num_layers=3, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=488, out_features=34, bias=True)\n",
       "        (1): LogSoftmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_15_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=488, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(488, 435, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_16): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(435, 501, num_layers=3, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=501, out_features=34, bias=True)\n",
       "        (1): LogSoftmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_16_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_17): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(435, 501, num_layers=3, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=501, out_features=34, bias=True)\n",
       "        (1): LogSoftmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_17_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=501, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(501, 435, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=4080, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_weights): Sequential(\n",
       "    (0): Linear(in_features=4080, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=44, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_xs, valid_ys = list(valid_data_loader)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, recons = model(observations=valid_xs, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reconstructions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1b90103b208f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mreconstructions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reconstructions' is not defined"
     ]
    }
   ],
   "source": [
    "(reconstructions == validation_data).all(dim=-1).to(dtype=torch.float).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:categorical_bpl] *",
   "language": "python",
   "name": "conda-env-categorical_bpl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
