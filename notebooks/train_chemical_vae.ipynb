{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eli/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [22:50:16] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='chemical_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-4,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 5,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer, log_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [256/225000 (0%)] Loss: 110166.968750\n",
      "Train Epoch: 1 [4352/225000 (2%)] Loss: 108390.539062\n",
      "Train Epoch: 1 [8448/225000 (4%)] Loss: 108199.859375\n",
      "Train Epoch: 1 [12544/225000 (6%)] Loss: 107913.734375\n",
      "Train Epoch: 1 [16640/225000 (7%)] Loss: 106272.039062\n",
      "Train Epoch: 1 [20736/225000 (9%)] Loss: 103633.375000\n",
      "Train Epoch: 1 [24832/225000 (11%)] Loss: 97388.984375\n",
      "Train Epoch: 1 [28928/225000 (13%)] Loss: 88351.898438\n",
      "Train Epoch: 1 [33024/225000 (15%)] Loss: 74670.343750\n",
      "Train Epoch: 1 [37120/225000 (16%)] Loss: 96051.171875\n",
      "Train Epoch: 1 [41216/225000 (18%)] Loss: 72077.375000\n",
      "Train Epoch: 1 [45312/225000 (20%)] Loss: 48737.968750\n",
      "Train Epoch: 1 [49408/225000 (22%)] Loss: 45598.078125\n",
      "Train Epoch: 1 [53504/225000 (24%)] Loss: 43244.671875\n",
      "Train Epoch: 1 [57600/225000 (26%)] Loss: 40929.460938\n",
      "Train Epoch: 1 [61696/225000 (27%)] Loss: 39422.781250\n",
      "Train Epoch: 1 [65792/225000 (29%)] Loss: 38276.609375\n",
      "Train Epoch: 1 [69888/225000 (31%)] Loss: 37793.140625\n",
      "Train Epoch: 1 [73984/225000 (33%)] Loss: 37379.347656\n",
      "Train Epoch: 1 [78080/225000 (35%)] Loss: 36901.972656\n",
      "Train Epoch: 1 [82176/225000 (37%)] Loss: 35394.273438\n",
      "Train Epoch: 1 [86272/225000 (38%)] Loss: 34997.429688\n",
      "Train Epoch: 1 [90368/225000 (40%)] Loss: 34797.875000\n",
      "Train Epoch: 1 [94464/225000 (42%)] Loss: 33663.414062\n",
      "Train Epoch: 1 [98560/225000 (44%)] Loss: 32897.714844\n",
      "Train Epoch: 1 [102656/225000 (46%)] Loss: 33066.472656\n",
      "Train Epoch: 1 [106752/225000 (47%)] Loss: 32241.195312\n",
      "Train Epoch: 1 [110848/225000 (49%)] Loss: 64288.015625\n",
      "Train Epoch: 1 [114944/225000 (51%)] Loss: 45409.734375\n",
      "Train Epoch: 1 [119040/225000 (53%)] Loss: 31142.687500\n",
      "Train Epoch: 1 [123136/225000 (55%)] Loss: 40459.414062\n",
      "Train Epoch: 1 [127232/225000 (57%)] Loss: 37401.726562\n",
      "Train Epoch: 1 [131328/225000 (58%)] Loss: 30806.458984\n",
      "Train Epoch: 1 [135424/225000 (60%)] Loss: 55447.765625\n",
      "Train Epoch: 1 [139520/225000 (62%)] Loss: 33057.250000\n",
      "Train Epoch: 1 [143616/225000 (64%)] Loss: 30305.685547\n",
      "Train Epoch: 1 [147712/225000 (66%)] Loss: 30040.699219\n",
      "Train Epoch: 1 [151808/225000 (67%)] Loss: 31880.556641\n",
      "Train Epoch: 1 [155904/225000 (69%)] Loss: 29953.281250\n",
      "Train Epoch: 1 [160000/225000 (71%)] Loss: 30343.603516\n",
      "Train Epoch: 1 [164096/225000 (73%)] Loss: 29922.378906\n",
      "Train Epoch: 1 [168192/225000 (75%)] Loss: 30543.546875\n",
      "Train Epoch: 1 [172288/225000 (77%)] Loss: 29606.152344\n",
      "Train Epoch: 1 [176384/225000 (78%)] Loss: 29722.000000\n",
      "Train Epoch: 1 [180480/225000 (80%)] Loss: 31083.345703\n",
      "Train Epoch: 1 [184576/225000 (82%)] Loss: 29291.058594\n",
      "Train Epoch: 1 [188672/225000 (84%)] Loss: 29165.269531\n",
      "Train Epoch: 1 [192768/225000 (86%)] Loss: 29698.544922\n",
      "Train Epoch: 1 [196864/225000 (87%)] Loss: 29180.804688\n",
      "Train Epoch: 1 [200960/225000 (89%)] Loss: 28926.740234\n",
      "Train Epoch: 1 [205056/225000 (91%)] Loss: 29229.197266\n",
      "Train Epoch: 1 [209152/225000 (93%)] Loss: 47387.160156\n",
      "Train Epoch: 1 [213248/225000 (95%)] Loss: 29270.167969\n",
      "Train Epoch: 1 [217344/225000 (97%)] Loss: 28990.332031\n",
      "Train Epoch: 1 [221440/225000 (98%)] Loss: 28818.890625\n",
      "    epoch          : 1\n",
      "    loss           : 46840.89711764079\n",
      "    val_loss       : 29067.46338922333\n",
      "Train Epoch: 2 [256/225000 (0%)] Loss: 28839.703125\n",
      "Train Epoch: 2 [4352/225000 (2%)] Loss: 29284.496094\n",
      "Train Epoch: 2 [8448/225000 (4%)] Loss: 29583.837891\n",
      "Train Epoch: 2 [12544/225000 (6%)] Loss: 28848.962891\n",
      "Train Epoch: 2 [16640/225000 (7%)] Loss: 28836.974609\n",
      "Train Epoch: 2 [20736/225000 (9%)] Loss: 28749.480469\n",
      "Train Epoch: 2 [24832/225000 (11%)] Loss: 28432.943359\n",
      "Train Epoch: 2 [28928/225000 (13%)] Loss: 28331.394531\n",
      "Train Epoch: 2 [33024/225000 (15%)] Loss: 28039.296875\n",
      "Train Epoch: 2 [37120/225000 (16%)] Loss: 30127.695312\n",
      "Train Epoch: 2 [41216/225000 (18%)] Loss: 27504.953125\n",
      "Train Epoch: 2 [45312/225000 (20%)] Loss: 27700.449219\n",
      "Train Epoch: 2 [49408/225000 (22%)] Loss: 27784.951172\n",
      "Train Epoch: 2 [53504/225000 (24%)] Loss: 28108.626953\n",
      "Train Epoch: 2 [57600/225000 (26%)] Loss: 27329.105469\n",
      "Train Epoch: 2 [61696/225000 (27%)] Loss: 28041.847656\n",
      "Train Epoch: 2 [65792/225000 (29%)] Loss: 28861.835938\n",
      "Train Epoch: 2 [69888/225000 (31%)] Loss: 27302.314453\n",
      "Train Epoch: 2 [73984/225000 (33%)] Loss: 27684.611328\n",
      "Train Epoch: 2 [78080/225000 (35%)] Loss: 27213.630859\n",
      "Train Epoch: 2 [82176/225000 (37%)] Loss: 26672.826172\n",
      "Train Epoch: 2 [86272/225000 (38%)] Loss: 26597.585938\n",
      "Train Epoch: 2 [90368/225000 (40%)] Loss: 26827.871094\n",
      "Train Epoch: 2 [94464/225000 (42%)] Loss: 26612.078125\n",
      "Train Epoch: 2 [98560/225000 (44%)] Loss: 26649.572266\n",
      "Train Epoch: 2 [102656/225000 (46%)] Loss: 26202.810547\n",
      "Train Epoch: 2 [106752/225000 (47%)] Loss: 26064.402344\n",
      "Train Epoch: 2 [110848/225000 (49%)] Loss: 27189.798828\n",
      "Train Epoch: 2 [114944/225000 (51%)] Loss: 27778.119141\n",
      "Train Epoch: 2 [119040/225000 (53%)] Loss: 24974.753906\n",
      "Train Epoch: 2 [123136/225000 (55%)] Loss: 25117.925781\n",
      "Train Epoch: 2 [127232/225000 (57%)] Loss: 24950.792969\n",
      "Train Epoch: 2 [131328/225000 (58%)] Loss: 24798.857422\n",
      "Train Epoch: 2 [135424/225000 (60%)] Loss: 24238.572266\n",
      "Train Epoch: 2 [139520/225000 (62%)] Loss: 24229.781250\n",
      "Train Epoch: 2 [143616/225000 (64%)] Loss: 23886.800781\n",
      "Train Epoch: 2 [147712/225000 (66%)] Loss: 23494.984375\n",
      "Train Epoch: 2 [151808/225000 (67%)] Loss: 23597.623047\n",
      "Train Epoch: 2 [155904/225000 (69%)] Loss: 25217.615234\n",
      "Train Epoch: 2 [160000/225000 (71%)] Loss: 22818.371094\n",
      "Train Epoch: 2 [164096/225000 (73%)] Loss: 22535.117188\n",
      "Train Epoch: 2 [168192/225000 (75%)] Loss: 22610.658203\n",
      "Train Epoch: 2 [172288/225000 (77%)] Loss: 21980.900391\n",
      "Train Epoch: 2 [176384/225000 (78%)] Loss: 22604.355469\n",
      "Train Epoch: 2 [180480/225000 (80%)] Loss: 22161.453125\n",
      "Train Epoch: 2 [184576/225000 (82%)] Loss: 21667.597656\n",
      "Train Epoch: 2 [188672/225000 (84%)] Loss: 21499.830078\n",
      "Train Epoch: 2 [192768/225000 (86%)] Loss: 21477.259766\n",
      "Train Epoch: 2 [196864/225000 (87%)] Loss: 23435.796875\n",
      "Train Epoch: 2 [200960/225000 (89%)] Loss: 21147.236328\n",
      "Train Epoch: 2 [205056/225000 (91%)] Loss: 20794.201172\n",
      "Train Epoch: 2 [209152/225000 (93%)] Loss: 23378.210938\n",
      "Train Epoch: 2 [213248/225000 (95%)] Loss: 21007.943359\n",
      "Train Epoch: 2 [217344/225000 (97%)] Loss: 22967.197266\n",
      "Train Epoch: 2 [221440/225000 (98%)] Loss: 20507.845703\n",
      "    epoch          : 2\n",
      "    loss           : 25603.330340319255\n",
      "    val_loss       : 21150.723949428724\n",
      "Train Epoch: 3 [256/225000 (0%)] Loss: 25833.748047\n",
      "Train Epoch: 3 [4352/225000 (2%)] Loss: 19597.857422\n",
      "Train Epoch: 3 [8448/225000 (4%)] Loss: 19419.726562\n",
      "Train Epoch: 3 [12544/225000 (6%)] Loss: 19443.566406\n",
      "Train Epoch: 3 [16640/225000 (7%)] Loss: 19257.320312\n",
      "Train Epoch: 3 [20736/225000 (9%)] Loss: 22068.876953\n",
      "Train Epoch: 3 [24832/225000 (11%)] Loss: 19182.095703\n",
      "Train Epoch: 3 [28928/225000 (13%)] Loss: 18769.449219\n",
      "Train Epoch: 3 [33024/225000 (15%)] Loss: 18821.587891\n",
      "Train Epoch: 3 [37120/225000 (16%)] Loss: 18632.791016\n",
      "Train Epoch: 3 [41216/225000 (18%)] Loss: 18684.710938\n",
      "Train Epoch: 3 [45312/225000 (20%)] Loss: 18505.673828\n",
      "Train Epoch: 3 [49408/225000 (22%)] Loss: 18262.294922\n",
      "Train Epoch: 3 [53504/225000 (24%)] Loss: 17971.769531\n",
      "Train Epoch: 3 [57600/225000 (26%)] Loss: 17981.994141\n",
      "Train Epoch: 3 [61696/225000 (27%)] Loss: 17822.169922\n",
      "Train Epoch: 3 [65792/225000 (29%)] Loss: 17938.912109\n",
      "Train Epoch: 3 [69888/225000 (31%)] Loss: 17992.937500\n",
      "Train Epoch: 3 [73984/225000 (33%)] Loss: 17685.023438\n",
      "Train Epoch: 3 [78080/225000 (35%)] Loss: 17602.281250\n",
      "Train Epoch: 3 [82176/225000 (37%)] Loss: 17541.300781\n",
      "Train Epoch: 3 [86272/225000 (38%)] Loss: 17418.433594\n",
      "Train Epoch: 3 [90368/225000 (40%)] Loss: 17357.244141\n",
      "Train Epoch: 3 [94464/225000 (42%)] Loss: 17077.513672\n",
      "Train Epoch: 3 [98560/225000 (44%)] Loss: 17140.638672\n",
      "Train Epoch: 3 [102656/225000 (46%)] Loss: 16981.580078\n",
      "Train Epoch: 3 [106752/225000 (47%)] Loss: 16683.544922\n",
      "Train Epoch: 3 [110848/225000 (49%)] Loss: 17039.353516\n",
      "Train Epoch: 3 [114944/225000 (51%)] Loss: 16599.964844\n",
      "Train Epoch: 3 [119040/225000 (53%)] Loss: 16682.601562\n",
      "Train Epoch: 3 [123136/225000 (55%)] Loss: 16316.498047\n",
      "Train Epoch: 3 [127232/225000 (57%)] Loss: 16714.562500\n",
      "Train Epoch: 3 [131328/225000 (58%)] Loss: 16254.175781\n",
      "Train Epoch: 3 [135424/225000 (60%)] Loss: 16276.775391\n",
      "Train Epoch: 3 [139520/225000 (62%)] Loss: 16222.521484\n",
      "Train Epoch: 3 [143616/225000 (64%)] Loss: 15877.998047\n",
      "Train Epoch: 3 [147712/225000 (66%)] Loss: 16130.574219\n",
      "Train Epoch: 3 [151808/225000 (67%)] Loss: 16074.583984\n",
      "Train Epoch: 3 [155904/225000 (69%)] Loss: 15841.865234\n",
      "Train Epoch: 3 [160000/225000 (71%)] Loss: 15662.423828\n",
      "Train Epoch: 3 [164096/225000 (73%)] Loss: 15534.812500\n",
      "Train Epoch: 3 [168192/225000 (75%)] Loss: 15884.169922\n",
      "Train Epoch: 3 [172288/225000 (77%)] Loss: 15621.310547\n",
      "Train Epoch: 3 [176384/225000 (78%)] Loss: 15561.634766\n",
      "Train Epoch: 3 [180480/225000 (80%)] Loss: 15570.814453\n",
      "Train Epoch: 3 [184576/225000 (82%)] Loss: 15225.240234\n",
      "Train Epoch: 3 [188672/225000 (84%)] Loss: 15545.974609\n",
      "Train Epoch: 3 [192768/225000 (86%)] Loss: 24819.970703\n",
      "Train Epoch: 3 [196864/225000 (87%)] Loss: 15467.751953\n",
      "Train Epoch: 3 [200960/225000 (89%)] Loss: 15093.400391\n",
      "Train Epoch: 3 [205056/225000 (91%)] Loss: 15333.521484\n",
      "Train Epoch: 3 [209152/225000 (93%)] Loss: 15386.757812\n",
      "Train Epoch: 3 [213248/225000 (95%)] Loss: 15254.810547\n",
      "Train Epoch: 3 [217344/225000 (97%)] Loss: 15184.951172\n",
      "Train Epoch: 3 [221440/225000 (98%)] Loss: 24670.531250\n",
      "    epoch          : 3\n",
      "    loss           : 17620.620618245164\n",
      "    val_loss       : 15117.381545021826\n",
      "Train Epoch: 4 [256/225000 (0%)] Loss: 14995.996094\n",
      "Train Epoch: 4 [4352/225000 (2%)] Loss: 14979.427734\n",
      "Train Epoch: 4 [8448/225000 (4%)] Loss: 15151.416016\n",
      "Train Epoch: 4 [12544/225000 (6%)] Loss: 14960.203125\n",
      "Train Epoch: 4 [16640/225000 (7%)] Loss: 14810.035156\n",
      "Train Epoch: 4 [20736/225000 (9%)] Loss: 14951.294922\n",
      "Train Epoch: 4 [24832/225000 (11%)] Loss: 14558.392578\n",
      "Train Epoch: 4 [28928/225000 (13%)] Loss: 18581.449219\n",
      "Train Epoch: 4 [33024/225000 (15%)] Loss: 14673.457031\n",
      "Train Epoch: 4 [37120/225000 (16%)] Loss: 18575.705078\n",
      "Train Epoch: 4 [41216/225000 (18%)] Loss: 14326.767578\n",
      "Train Epoch: 4 [45312/225000 (20%)] Loss: 14654.201172\n",
      "Train Epoch: 4 [49408/225000 (22%)] Loss: 14604.236328\n",
      "Train Epoch: 4 [53504/225000 (24%)] Loss: 14653.035156\n",
      "Train Epoch: 4 [57600/225000 (26%)] Loss: 14717.771484\n",
      "Train Epoch: 4 [61696/225000 (27%)] Loss: 14480.052734\n",
      "Train Epoch: 4 [65792/225000 (29%)] Loss: 14059.855469\n",
      "Train Epoch: 4 [69888/225000 (31%)] Loss: 14291.320312\n",
      "Train Epoch: 4 [73984/225000 (33%)] Loss: 18400.548828\n",
      "Train Epoch: 4 [78080/225000 (35%)] Loss: 14367.152344\n",
      "Train Epoch: 4 [82176/225000 (37%)] Loss: 14299.986328\n",
      "Train Epoch: 4 [86272/225000 (38%)] Loss: 14216.722656\n",
      "Train Epoch: 4 [90368/225000 (40%)] Loss: 14213.332031\n",
      "Train Epoch: 4 [94464/225000 (42%)] Loss: 14392.873047\n",
      "Train Epoch: 4 [98560/225000 (44%)] Loss: 14231.839844\n",
      "Train Epoch: 4 [102656/225000 (46%)] Loss: 14175.863281\n",
      "Train Epoch: 4 [106752/225000 (47%)] Loss: 13978.886719\n",
      "Train Epoch: 4 [110848/225000 (49%)] Loss: 18148.328125\n",
      "Train Epoch: 4 [114944/225000 (51%)] Loss: 14412.089844\n",
      "Train Epoch: 4 [119040/225000 (53%)] Loss: 14242.623047\n",
      "Train Epoch: 4 [123136/225000 (55%)] Loss: 14200.861328\n",
      "Train Epoch: 4 [127232/225000 (57%)] Loss: 14009.111328\n",
      "Train Epoch: 4 [131328/225000 (58%)] Loss: 14071.667969\n",
      "Train Epoch: 4 [135424/225000 (60%)] Loss: 13743.642578\n",
      "Train Epoch: 4 [139520/225000 (62%)] Loss: 13854.914062\n",
      "Train Epoch: 4 [143616/225000 (64%)] Loss: 14061.433594\n",
      "Train Epoch: 4 [147712/225000 (66%)] Loss: 13704.113281\n",
      "Train Epoch: 4 [151808/225000 (67%)] Loss: 13577.289062\n",
      "Train Epoch: 4 [155904/225000 (69%)] Loss: 14004.562500\n",
      "Train Epoch: 4 [160000/225000 (71%)] Loss: 13898.589844\n",
      "Train Epoch: 4 [164096/225000 (73%)] Loss: 13849.781250\n",
      "Train Epoch: 4 [168192/225000 (75%)] Loss: 13499.443359\n",
      "Train Epoch: 4 [172288/225000 (77%)] Loss: 13692.492188\n",
      "Train Epoch: 4 [176384/225000 (78%)] Loss: 13370.166016\n",
      "Train Epoch: 4 [180480/225000 (80%)] Loss: 17789.892578\n",
      "Train Epoch: 4 [184576/225000 (82%)] Loss: 13344.892578\n",
      "Train Epoch: 4 [188672/225000 (84%)] Loss: 13517.697266\n",
      "Train Epoch: 4 [192768/225000 (86%)] Loss: 13475.453125\n",
      "Train Epoch: 4 [196864/225000 (87%)] Loss: 17518.259766\n",
      "Train Epoch: 4 [200960/225000 (89%)] Loss: 13537.085938\n",
      "Train Epoch: 4 [205056/225000 (91%)] Loss: 13347.554688\n",
      "Train Epoch: 4 [209152/225000 (93%)] Loss: 13620.843750\n",
      "Train Epoch: 4 [213248/225000 (95%)] Loss: 13681.873047\n",
      "Train Epoch: 4 [217344/225000 (97%)] Loss: 17706.050781\n",
      "Train Epoch: 4 [221440/225000 (98%)] Loss: 13433.890625\n",
      "    epoch          : 4\n",
      "    loss           : 14600.439392020407\n",
      "    val_loss       : 13810.216006037532\n",
      "Train Epoch: 5 [256/225000 (0%)] Loss: 13332.615234\n",
      "Train Epoch: 5 [4352/225000 (2%)] Loss: 13421.642578\n",
      "Train Epoch: 5 [8448/225000 (4%)] Loss: 13088.177734\n",
      "Train Epoch: 5 [12544/225000 (6%)] Loss: 13002.880859\n",
      "Train Epoch: 5 [16640/225000 (7%)] Loss: 13331.240234\n",
      "Train Epoch: 5 [20736/225000 (9%)] Loss: 13317.527344\n",
      "Train Epoch: 5 [24832/225000 (11%)] Loss: 13034.875000\n",
      "Train Epoch: 5 [28928/225000 (13%)] Loss: 13370.832031\n",
      "Train Epoch: 5 [33024/225000 (15%)] Loss: 13263.541016\n",
      "Train Epoch: 5 [37120/225000 (16%)] Loss: 13391.757812\n",
      "Train Epoch: 5 [41216/225000 (18%)] Loss: 13390.408203\n",
      "Train Epoch: 5 [45312/225000 (20%)] Loss: 13351.382812\n",
      "Train Epoch: 5 [49408/225000 (22%)] Loss: 13144.353516\n",
      "Train Epoch: 5 [53504/225000 (24%)] Loss: 13054.580078\n",
      "Train Epoch: 5 [57600/225000 (26%)] Loss: 13157.240234\n",
      "Train Epoch: 5 [61696/225000 (27%)] Loss: 12761.412109\n",
      "Train Epoch: 5 [65792/225000 (29%)] Loss: 13232.958984\n",
      "Train Epoch: 5 [69888/225000 (31%)] Loss: 12926.566406\n",
      "Train Epoch: 5 [73984/225000 (33%)] Loss: 12839.757812\n",
      "Train Epoch: 5 [78080/225000 (35%)] Loss: 12902.039062\n",
      "Train Epoch: 5 [82176/225000 (37%)] Loss: 13114.591797\n",
      "Train Epoch: 5 [86272/225000 (38%)] Loss: 13189.974609\n",
      "Train Epoch: 5 [90368/225000 (40%)] Loss: 12926.183594\n",
      "Train Epoch: 5 [94464/225000 (42%)] Loss: 12547.728516\n",
      "Train Epoch: 5 [98560/225000 (44%)] Loss: 13163.630859\n",
      "Train Epoch: 5 [102656/225000 (46%)] Loss: 13116.970703\n",
      "Train Epoch: 5 [106752/225000 (47%)] Loss: 12829.080078\n",
      "Train Epoch: 5 [110848/225000 (49%)] Loss: 12628.945312\n",
      "Train Epoch: 5 [114944/225000 (51%)] Loss: 12903.820312\n",
      "Train Epoch: 5 [119040/225000 (53%)] Loss: 12776.552734\n",
      "Train Epoch: 5 [123136/225000 (55%)] Loss: 13009.955078\n",
      "Train Epoch: 5 [127232/225000 (57%)] Loss: 12693.931641\n",
      "Train Epoch: 5 [131328/225000 (58%)] Loss: 12839.556641\n",
      "Train Epoch: 5 [135424/225000 (60%)] Loss: 12745.216797\n",
      "Train Epoch: 5 [139520/225000 (62%)] Loss: 12739.564453\n",
      "Train Epoch: 5 [143616/225000 (64%)] Loss: 12497.171875\n",
      "Train Epoch: 5 [147712/225000 (66%)] Loss: 12703.478516\n",
      "Train Epoch: 5 [151808/225000 (67%)] Loss: 12687.611328\n",
      "Train Epoch: 5 [155904/225000 (69%)] Loss: 12848.472656\n",
      "Train Epoch: 5 [160000/225000 (71%)] Loss: 12660.572266\n",
      "Train Epoch: 5 [164096/225000 (73%)] Loss: 12651.103516\n",
      "Train Epoch: 5 [168192/225000 (75%)] Loss: 12716.339844\n",
      "Train Epoch: 5 [172288/225000 (77%)] Loss: 12844.298828\n",
      "Train Epoch: 5 [176384/225000 (78%)] Loss: 12471.507812\n",
      "Train Epoch: 5 [180480/225000 (80%)] Loss: 12404.996094\n",
      "Train Epoch: 5 [184576/225000 (82%)] Loss: 12726.863281\n",
      "Train Epoch: 5 [188672/225000 (84%)] Loss: 12917.138672\n",
      "Train Epoch: 5 [192768/225000 (86%)] Loss: 12461.792969\n",
      "Train Epoch: 5 [196864/225000 (87%)] Loss: 12908.011719\n",
      "Train Epoch: 5 [200960/225000 (89%)] Loss: 12300.597656\n",
      "Train Epoch: 5 [205056/225000 (91%)] Loss: 12578.021484\n",
      "Train Epoch: 5 [209152/225000 (93%)] Loss: 12499.203125\n",
      "Train Epoch: 5 [213248/225000 (95%)] Loss: 12347.021484\n",
      "Train Epoch: 5 [217344/225000 (97%)] Loss: 12617.564453\n",
      "Train Epoch: 5 [221440/225000 (98%)] Loss: 12568.337891\n",
      "    epoch          : 5\n",
      "    loss           : 13159.755035018487\n",
      "    val_loss       : 12715.9281560511\n",
      "Train Epoch: 6 [256/225000 (0%)] Loss: 12614.080078\n",
      "Train Epoch: 6 [4352/225000 (2%)] Loss: 12526.646484\n",
      "Train Epoch: 6 [8448/225000 (4%)] Loss: 12432.533203\n",
      "Train Epoch: 6 [12544/225000 (6%)] Loss: 12501.072266\n",
      "Train Epoch: 6 [16640/225000 (7%)] Loss: 12329.626953\n",
      "Train Epoch: 6 [20736/225000 (9%)] Loss: 12224.128906\n",
      "Train Epoch: 6 [24832/225000 (11%)] Loss: 12446.962891\n",
      "Train Epoch: 6 [28928/225000 (13%)] Loss: 12095.320312\n",
      "Train Epoch: 6 [33024/225000 (15%)] Loss: 12304.345703\n",
      "Train Epoch: 6 [37120/225000 (16%)] Loss: 12506.650391\n",
      "Train Epoch: 6 [41216/225000 (18%)] Loss: 12243.115234\n",
      "Train Epoch: 6 [45312/225000 (20%)] Loss: 12198.679688\n",
      "Train Epoch: 6 [49408/225000 (22%)] Loss: 12189.113281\n",
      "Train Epoch: 6 [53504/225000 (24%)] Loss: 12496.943359\n",
      "Train Epoch: 6 [57600/225000 (26%)] Loss: 11999.261719\n",
      "Train Epoch: 6 [61696/225000 (27%)] Loss: 12447.480469\n",
      "Train Epoch: 6 [65792/225000 (29%)] Loss: 12082.125000\n",
      "Train Epoch: 6 [69888/225000 (31%)] Loss: 12561.769531\n",
      "Train Epoch: 6 [73984/225000 (33%)] Loss: 12190.060547\n",
      "Train Epoch: 6 [78080/225000 (35%)] Loss: 12326.789062\n",
      "Train Epoch: 6 [82176/225000 (37%)] Loss: 11992.791016\n",
      "Train Epoch: 6 [86272/225000 (38%)] Loss: 11980.013672\n",
      "Train Epoch: 6 [90368/225000 (40%)] Loss: 12183.949219\n",
      "Train Epoch: 6 [94464/225000 (42%)] Loss: 12290.609375\n",
      "Train Epoch: 6 [98560/225000 (44%)] Loss: 12215.367188\n",
      "Train Epoch: 6 [102656/225000 (46%)] Loss: 12119.955078\n",
      "Train Epoch: 6 [106752/225000 (47%)] Loss: 12312.519531\n",
      "Train Epoch: 6 [110848/225000 (49%)] Loss: 12316.062500\n",
      "Train Epoch: 6 [114944/225000 (51%)] Loss: 12210.447266\n",
      "Train Epoch: 6 [119040/225000 (53%)] Loss: 11985.708984\n",
      "Train Epoch: 6 [123136/225000 (55%)] Loss: 12269.052734\n",
      "Train Epoch: 6 [127232/225000 (57%)] Loss: 11911.371094\n",
      "Train Epoch: 6 [131328/225000 (58%)] Loss: 12165.701172\n",
      "Train Epoch: 6 [135424/225000 (60%)] Loss: 11719.177734\n",
      "Train Epoch: 6 [139520/225000 (62%)] Loss: 12330.615234\n",
      "Train Epoch: 6 [143616/225000 (64%)] Loss: 11995.486328\n",
      "Train Epoch: 6 [147712/225000 (66%)] Loss: 11881.726562\n",
      "Train Epoch: 6 [151808/225000 (67%)] Loss: 11980.744141\n",
      "Train Epoch: 6 [155904/225000 (69%)] Loss: 12103.429688\n",
      "Train Epoch: 6 [160000/225000 (71%)] Loss: 12126.980469\n",
      "Train Epoch: 6 [164096/225000 (73%)] Loss: 11801.179688\n",
      "Train Epoch: 6 [168192/225000 (75%)] Loss: 12032.140625\n",
      "Train Epoch: 6 [172288/225000 (77%)] Loss: 11785.203125\n",
      "Train Epoch: 6 [176384/225000 (78%)] Loss: 12050.513672\n",
      "Train Epoch: 6 [180480/225000 (80%)] Loss: 12039.662109\n",
      "Train Epoch: 6 [184576/225000 (82%)] Loss: 11734.404297\n",
      "Train Epoch: 6 [188672/225000 (84%)] Loss: 11821.931641\n",
      "Train Epoch: 6 [192768/225000 (86%)] Loss: 11667.537109\n",
      "Train Epoch: 6 [196864/225000 (87%)] Loss: 12270.314453\n",
      "Train Epoch: 6 [200960/225000 (89%)] Loss: 11756.748047\n",
      "Train Epoch: 6 [205056/225000 (91%)] Loss: 11891.435547\n",
      "Train Epoch: 6 [209152/225000 (93%)] Loss: 11989.519531\n",
      "Train Epoch: 6 [213248/225000 (95%)] Loss: 11833.958984\n",
      "Train Epoch: 6 [217344/225000 (97%)] Loss: 11714.970703\n",
      "Train Epoch: 6 [221440/225000 (98%)] Loss: 11861.693359\n",
      "    epoch          : 6\n",
      "    loss           : 12339.539922408276\n",
      "    val_loss       : 12004.192953587795\n",
      "Train Epoch: 7 [256/225000 (0%)] Loss: 11750.666016\n",
      "Train Epoch: 7 [4352/225000 (2%)] Loss: 11667.101562\n",
      "Train Epoch: 7 [8448/225000 (4%)] Loss: 11654.253906\n",
      "Train Epoch: 7 [12544/225000 (6%)] Loss: 11575.896484\n",
      "Train Epoch: 7 [16640/225000 (7%)] Loss: 11857.441406\n",
      "Train Epoch: 7 [20736/225000 (9%)] Loss: 11926.005859\n",
      "Train Epoch: 7 [24832/225000 (11%)] Loss: 11667.515625\n",
      "Train Epoch: 7 [28928/225000 (13%)] Loss: 11893.443359\n",
      "Train Epoch: 7 [33024/225000 (15%)] Loss: 11967.136719\n",
      "Train Epoch: 7 [37120/225000 (16%)] Loss: 11705.291016\n",
      "Train Epoch: 7 [41216/225000 (18%)] Loss: 11814.326172\n",
      "Train Epoch: 7 [45312/225000 (20%)] Loss: 11726.115234\n",
      "Train Epoch: 7 [49408/225000 (22%)] Loss: 11680.982422\n",
      "Train Epoch: 7 [53504/225000 (24%)] Loss: 11878.011719\n",
      "Train Epoch: 7 [57600/225000 (26%)] Loss: 11882.318359\n",
      "Train Epoch: 7 [61696/225000 (27%)] Loss: 11739.085938\n",
      "Train Epoch: 7 [65792/225000 (29%)] Loss: 11901.160156\n",
      "Train Epoch: 7 [69888/225000 (31%)] Loss: 11749.693359\n",
      "Train Epoch: 7 [73984/225000 (33%)] Loss: 11733.535156\n",
      "Train Epoch: 7 [78080/225000 (35%)] Loss: 11756.234375\n",
      "Train Epoch: 7 [82176/225000 (37%)] Loss: 11795.646484\n",
      "Train Epoch: 7 [86272/225000 (38%)] Loss: 11716.796875\n",
      "Train Epoch: 7 [90368/225000 (40%)] Loss: 11397.007812\n",
      "Train Epoch: 7 [94464/225000 (42%)] Loss: 11620.582031\n",
      "Train Epoch: 7 [98560/225000 (44%)] Loss: 11694.886719\n",
      "Train Epoch: 7 [102656/225000 (46%)] Loss: 11919.423828\n",
      "Train Epoch: 7 [106752/225000 (47%)] Loss: 11729.019531\n",
      "Train Epoch: 7 [110848/225000 (49%)] Loss: 11748.775391\n",
      "Train Epoch: 7 [114944/225000 (51%)] Loss: 11642.177734\n",
      "Train Epoch: 7 [119040/225000 (53%)] Loss: 11569.302734\n",
      "Train Epoch: 7 [123136/225000 (55%)] Loss: 11446.195312\n",
      "Train Epoch: 7 [127232/225000 (57%)] Loss: 11603.259766\n",
      "Train Epoch: 7 [131328/225000 (58%)] Loss: 11638.986328\n",
      "Train Epoch: 7 [135424/225000 (60%)] Loss: 11484.314453\n",
      "Train Epoch: 7 [139520/225000 (62%)] Loss: 11620.884766\n",
      "Train Epoch: 7 [143616/225000 (64%)] Loss: 11389.455078\n",
      "Train Epoch: 7 [147712/225000 (66%)] Loss: 11343.275391\n",
      "Train Epoch: 7 [151808/225000 (67%)] Loss: 11503.957031\n",
      "Train Epoch: 7 [155904/225000 (69%)] Loss: 11568.876953\n",
      "Train Epoch: 7 [160000/225000 (71%)] Loss: 11499.117188\n",
      "Train Epoch: 7 [164096/225000 (73%)] Loss: 11625.705078\n",
      "Train Epoch: 7 [168192/225000 (75%)] Loss: 11408.056641\n",
      "Train Epoch: 7 [172288/225000 (77%)] Loss: 11737.429688\n",
      "Train Epoch: 7 [176384/225000 (78%)] Loss: 11625.453125\n",
      "Train Epoch: 7 [180480/225000 (80%)] Loss: 11440.623047\n",
      "Train Epoch: 7 [184576/225000 (82%)] Loss: 11442.755859\n",
      "Train Epoch: 7 [188672/225000 (84%)] Loss: 11382.650391\n",
      "Train Epoch: 7 [192768/225000 (86%)] Loss: 11116.996094\n",
      "Train Epoch: 7 [196864/225000 (87%)] Loss: 11404.726562\n",
      "Train Epoch: 7 [200960/225000 (89%)] Loss: 11352.207031\n",
      "Train Epoch: 7 [205056/225000 (91%)] Loss: 11395.542969\n",
      "Train Epoch: 7 [209152/225000 (93%)] Loss: 11410.789062\n",
      "Train Epoch: 7 [213248/225000 (95%)] Loss: 11539.269531\n",
      "Train Epoch: 7 [217344/225000 (97%)] Loss: 11314.574219\n",
      "Train Epoch: 7 [221440/225000 (98%)] Loss: 11528.673828\n",
      "    epoch          : 7\n",
      "    loss           : 11705.228980019909\n",
      "    val_loss       : 12332.287124070586\n",
      "Train Epoch: 8 [256/225000 (0%)] Loss: 11485.017578\n",
      "Train Epoch: 8 [4352/225000 (2%)] Loss: 11358.386719\n",
      "Train Epoch: 8 [8448/225000 (4%)] Loss: 11321.480469\n",
      "Train Epoch: 8 [12544/225000 (6%)] Loss: 11256.230469\n",
      "Train Epoch: 8 [16640/225000 (7%)] Loss: 11246.662109\n",
      "Train Epoch: 8 [20736/225000 (9%)] Loss: 11705.570312\n",
      "Train Epoch: 8 [24832/225000 (11%)] Loss: 11491.544922\n",
      "Train Epoch: 8 [28928/225000 (13%)] Loss: 11448.792969\n",
      "Train Epoch: 8 [33024/225000 (15%)] Loss: 11159.087891\n",
      "Train Epoch: 8 [37120/225000 (16%)] Loss: 11276.148438\n",
      "Train Epoch: 8 [41216/225000 (18%)] Loss: 11314.267578\n",
      "Train Epoch: 8 [45312/225000 (20%)] Loss: 11013.201172\n",
      "Train Epoch: 8 [49408/225000 (22%)] Loss: 11185.250000\n",
      "Train Epoch: 8 [53504/225000 (24%)] Loss: 11399.486328\n",
      "Train Epoch: 8 [57600/225000 (26%)] Loss: 11247.261719\n",
      "Train Epoch: 8 [61696/225000 (27%)] Loss: 11234.076172\n",
      "Train Epoch: 8 [65792/225000 (29%)] Loss: 11043.031250\n",
      "Train Epoch: 8 [69888/225000 (31%)] Loss: 11108.462891\n",
      "Train Epoch: 8 [73984/225000 (33%)] Loss: 11124.818359\n",
      "Train Epoch: 8 [78080/225000 (35%)] Loss: 11293.076172\n",
      "Train Epoch: 8 [82176/225000 (37%)] Loss: 11187.865234\n",
      "Train Epoch: 8 [86272/225000 (38%)] Loss: 11224.072266\n",
      "Train Epoch: 8 [90368/225000 (40%)] Loss: 10910.062500\n",
      "Train Epoch: 8 [94464/225000 (42%)] Loss: 11299.101562\n",
      "Train Epoch: 8 [98560/225000 (44%)] Loss: 11236.527344\n",
      "Train Epoch: 8 [102656/225000 (46%)] Loss: 11426.566406\n",
      "Train Epoch: 8 [106752/225000 (47%)] Loss: 11315.818359\n",
      "Train Epoch: 8 [110848/225000 (49%)] Loss: 11402.330078\n",
      "Train Epoch: 8 [114944/225000 (51%)] Loss: 11062.255859\n",
      "Train Epoch: 8 [119040/225000 (53%)] Loss: 11099.792969\n",
      "Train Epoch: 8 [123136/225000 (55%)] Loss: 11494.839844\n",
      "Train Epoch: 8 [127232/225000 (57%)] Loss: 11318.994141\n",
      "Train Epoch: 8 [131328/225000 (58%)] Loss: 11176.732422\n",
      "Train Epoch: 8 [135424/225000 (60%)] Loss: 11136.443359\n",
      "Train Epoch: 8 [139520/225000 (62%)] Loss: 11245.580078\n",
      "Train Epoch: 8 [143616/225000 (64%)] Loss: 11065.681641\n",
      "Train Epoch: 8 [147712/225000 (66%)] Loss: 11076.347656\n",
      "Train Epoch: 8 [151808/225000 (67%)] Loss: 10946.693359\n",
      "Train Epoch: 8 [155904/225000 (69%)] Loss: 11396.353516\n",
      "Train Epoch: 8 [160000/225000 (71%)] Loss: 11139.099609\n",
      "Train Epoch: 8 [164096/225000 (73%)] Loss: 11071.937500\n",
      "Train Epoch: 8 [168192/225000 (75%)] Loss: 11028.660156\n",
      "Train Epoch: 8 [172288/225000 (77%)] Loss: 10969.199219\n",
      "Train Epoch: 8 [176384/225000 (78%)] Loss: 11108.144531\n",
      "Train Epoch: 8 [180480/225000 (80%)] Loss: 11170.423828\n",
      "Train Epoch: 8 [184576/225000 (82%)] Loss: 11050.542969\n",
      "Train Epoch: 8 [188672/225000 (84%)] Loss: 11053.972656\n",
      "Train Epoch: 8 [192768/225000 (86%)] Loss: 11147.876953\n",
      "Train Epoch: 8 [196864/225000 (87%)] Loss: 11132.205078\n",
      "Train Epoch: 8 [200960/225000 (89%)] Loss: 10904.230469\n",
      "Train Epoch: 8 [205056/225000 (91%)] Loss: 10962.869141\n",
      "Train Epoch: 8 [209152/225000 (93%)] Loss: 11112.164062\n",
      "Train Epoch: 8 [213248/225000 (95%)] Loss: 10991.271484\n",
      "Train Epoch: 8 [217344/225000 (97%)] Loss: 10845.750000\n",
      "Train Epoch: 8 [221440/225000 (98%)] Loss: 11096.820312\n",
      "    epoch          : 8\n",
      "    loss           : 11369.204452636162\n",
      "    val_loss       : 11049.757506992135\n",
      "Train Epoch: 9 [256/225000 (0%)] Loss: 11079.548828\n",
      "Train Epoch: 9 [4352/225000 (2%)] Loss: 10968.310547\n",
      "Train Epoch: 9 [8448/225000 (4%)] Loss: 11082.980469\n",
      "Train Epoch: 9 [12544/225000 (6%)] Loss: 10925.794922\n",
      "Train Epoch: 9 [16640/225000 (7%)] Loss: 10773.296875\n",
      "Train Epoch: 9 [20736/225000 (9%)] Loss: 10827.693359\n",
      "Train Epoch: 9 [24832/225000 (11%)] Loss: 11029.083984\n",
      "Train Epoch: 9 [28928/225000 (13%)] Loss: 10776.224609\n",
      "Train Epoch: 9 [33024/225000 (15%)] Loss: 11094.406250\n",
      "Train Epoch: 9 [37120/225000 (16%)] Loss: 10885.205078\n",
      "Train Epoch: 9 [41216/225000 (18%)] Loss: 11044.214844\n",
      "Train Epoch: 9 [45312/225000 (20%)] Loss: 10848.593750\n",
      "Train Epoch: 9 [49408/225000 (22%)] Loss: 11134.132812\n",
      "Train Epoch: 9 [53504/225000 (24%)] Loss: 10734.587891\n",
      "Train Epoch: 9 [57600/225000 (26%)] Loss: 11199.214844\n",
      "Train Epoch: 9 [61696/225000 (27%)] Loss: 10783.798828\n",
      "Train Epoch: 9 [65792/225000 (29%)] Loss: 10687.058594\n",
      "Train Epoch: 9 [69888/225000 (31%)] Loss: 10800.242188\n",
      "Train Epoch: 9 [73984/225000 (33%)] Loss: 10899.861328\n",
      "Train Epoch: 9 [78080/225000 (35%)] Loss: 10975.009766\n",
      "Train Epoch: 9 [82176/225000 (37%)] Loss: 10759.486328\n",
      "Train Epoch: 9 [86272/225000 (38%)] Loss: 11073.701172\n",
      "Train Epoch: 9 [90368/225000 (40%)] Loss: 11006.589844\n",
      "Train Epoch: 9 [94464/225000 (42%)] Loss: 11076.447266\n",
      "Train Epoch: 9 [98560/225000 (44%)] Loss: 10938.281250\n",
      "Train Epoch: 9 [102656/225000 (46%)] Loss: 10631.962891\n",
      "Train Epoch: 9 [106752/225000 (47%)] Loss: 11042.144531\n",
      "Train Epoch: 9 [110848/225000 (49%)] Loss: 33896.140625\n",
      "Train Epoch: 9 [114944/225000 (51%)] Loss: 10524.775391\n",
      "Train Epoch: 9 [119040/225000 (53%)] Loss: 10618.400391\n",
      "Train Epoch: 9 [123136/225000 (55%)] Loss: 10881.765625\n",
      "Train Epoch: 9 [127232/225000 (57%)] Loss: 10740.025391\n",
      "Train Epoch: 9 [131328/225000 (58%)] Loss: 10772.902344\n",
      "Train Epoch: 9 [135424/225000 (60%)] Loss: 10763.570312\n",
      "Train Epoch: 9 [139520/225000 (62%)] Loss: 10808.033203\n",
      "Train Epoch: 9 [143616/225000 (64%)] Loss: 10835.115234\n",
      "Train Epoch: 9 [147712/225000 (66%)] Loss: 10864.732422\n",
      "Train Epoch: 9 [151808/225000 (67%)] Loss: 10936.292969\n",
      "Train Epoch: 9 [155904/225000 (69%)] Loss: 11112.585938\n",
      "Train Epoch: 9 [160000/225000 (71%)] Loss: 10780.060547\n",
      "Train Epoch: 9 [164096/225000 (73%)] Loss: 10765.650391\n",
      "Train Epoch: 9 [168192/225000 (75%)] Loss: 10755.681641\n",
      "Train Epoch: 9 [172288/225000 (77%)] Loss: 10812.849609\n",
      "Train Epoch: 9 [176384/225000 (78%)] Loss: 10872.109375\n",
      "Train Epoch: 9 [180480/225000 (80%)] Loss: 10963.123047\n",
      "Train Epoch: 9 [184576/225000 (82%)] Loss: 10770.404297\n",
      "Train Epoch: 9 [188672/225000 (84%)] Loss: 10773.892578\n",
      "Train Epoch: 9 [192768/225000 (86%)] Loss: 10702.966797\n",
      "Train Epoch: 9 [196864/225000 (87%)] Loss: 10868.755859\n",
      "Train Epoch: 9 [200960/225000 (89%)] Loss: 10885.125000\n",
      "Train Epoch: 9 [205056/225000 (91%)] Loss: 11106.859375\n",
      "Train Epoch: 9 [209152/225000 (93%)] Loss: 11032.441406\n",
      "Train Epoch: 9 [213248/225000 (95%)] Loss: 10666.775391\n",
      "Train Epoch: 9 [217344/225000 (97%)] Loss: 10888.587891\n",
      "Train Epoch: 9 [221440/225000 (98%)] Loss: 10614.103516\n",
      "    epoch          : 9\n",
      "    loss           : 10950.004686166809\n",
      "    val_loss       : 10769.690072498759\n",
      "Train Epoch: 10 [256/225000 (0%)] Loss: 10580.896484\n",
      "Train Epoch: 10 [4352/225000 (2%)] Loss: 10780.263672\n",
      "Train Epoch: 10 [8448/225000 (4%)] Loss: 10813.605469\n",
      "Train Epoch: 10 [12544/225000 (6%)] Loss: 10645.136719\n",
      "Train Epoch: 10 [16640/225000 (7%)] Loss: 10653.003906\n",
      "Train Epoch: 10 [20736/225000 (9%)] Loss: 10831.064453\n",
      "Train Epoch: 10 [24832/225000 (11%)] Loss: 10748.568359\n",
      "Train Epoch: 10 [28928/225000 (13%)] Loss: 10610.886719\n",
      "Train Epoch: 10 [33024/225000 (15%)] Loss: 10507.736328\n",
      "Train Epoch: 10 [37120/225000 (16%)] Loss: 10636.027344\n",
      "Train Epoch: 10 [41216/225000 (18%)] Loss: 10622.607422\n",
      "Train Epoch: 10 [45312/225000 (20%)] Loss: 10684.240234\n",
      "Train Epoch: 10 [49408/225000 (22%)] Loss: 10662.105469\n",
      "Train Epoch: 10 [53504/225000 (24%)] Loss: 10660.810547\n",
      "Train Epoch: 10 [57600/225000 (26%)] Loss: 10612.150391\n",
      "Train Epoch: 10 [61696/225000 (27%)] Loss: 10592.320312\n",
      "Train Epoch: 10 [65792/225000 (29%)] Loss: 10618.451172\n",
      "Train Epoch: 10 [69888/225000 (31%)] Loss: 10632.158203\n",
      "Train Epoch: 10 [73984/225000 (33%)] Loss: 10883.333984\n",
      "Train Epoch: 10 [78080/225000 (35%)] Loss: 10450.558594\n",
      "Train Epoch: 10 [82176/225000 (37%)] Loss: 15294.410156\n",
      "Train Epoch: 10 [86272/225000 (38%)] Loss: 10667.037109\n",
      "Train Epoch: 10 [90368/225000 (40%)] Loss: 10679.613281\n",
      "Train Epoch: 10 [94464/225000 (42%)] Loss: 10311.470703\n",
      "Train Epoch: 10 [98560/225000 (44%)] Loss: 10610.001953\n",
      "Train Epoch: 10 [102656/225000 (46%)] Loss: 10772.101562\n",
      "Train Epoch: 10 [106752/225000 (47%)] Loss: 10692.371094\n",
      "Train Epoch: 10 [110848/225000 (49%)] Loss: 10717.285156\n",
      "Train Epoch: 10 [114944/225000 (51%)] Loss: 10582.580078\n",
      "Train Epoch: 10 [119040/225000 (53%)] Loss: 10920.322266\n",
      "Train Epoch: 10 [123136/225000 (55%)] Loss: 10695.439453\n",
      "Train Epoch: 10 [127232/225000 (57%)] Loss: 10623.427734\n",
      "Train Epoch: 10 [131328/225000 (58%)] Loss: 10497.736328\n",
      "Train Epoch: 10 [135424/225000 (60%)] Loss: 10477.755859\n",
      "Train Epoch: 10 [139520/225000 (62%)] Loss: 10315.789062\n",
      "Train Epoch: 10 [143616/225000 (64%)] Loss: 10604.601562\n",
      "Train Epoch: 10 [147712/225000 (66%)] Loss: 10230.765625\n",
      "Train Epoch: 10 [151808/225000 (67%)] Loss: 10684.298828\n",
      "Train Epoch: 10 [155904/225000 (69%)] Loss: 10668.140625\n",
      "Train Epoch: 10 [160000/225000 (71%)] Loss: 10463.634766\n",
      "Train Epoch: 10 [164096/225000 (73%)] Loss: 10336.136719\n",
      "Train Epoch: 10 [168192/225000 (75%)] Loss: 10605.689453\n",
      "Train Epoch: 10 [172288/225000 (77%)] Loss: 10766.626953\n",
      "Train Epoch: 10 [176384/225000 (78%)] Loss: 10456.666016\n",
      "Train Epoch: 10 [180480/225000 (80%)] Loss: 10536.726562\n",
      "Train Epoch: 10 [184576/225000 (82%)] Loss: 10513.896484\n",
      "Train Epoch: 10 [188672/225000 (84%)] Loss: 10751.585938\n",
      "Train Epoch: 10 [192768/225000 (86%)] Loss: 10543.609375\n",
      "Train Epoch: 10 [196864/225000 (87%)] Loss: 10634.269531\n",
      "Train Epoch: 10 [200960/225000 (89%)] Loss: 10314.277344\n",
      "Train Epoch: 10 [205056/225000 (91%)] Loss: 10387.310547\n",
      "Train Epoch: 10 [209152/225000 (93%)] Loss: 10429.619141\n",
      "Train Epoch: 10 [213248/225000 (95%)] Loss: 10445.648438\n",
      "Train Epoch: 10 [217344/225000 (97%)] Loss: 10398.972656\n",
      "Train Epoch: 10 [221440/225000 (98%)] Loss: 10630.173828\n",
      "    epoch          : 10\n",
      "    loss           : 10766.617780770051\n",
      "    val_loss       : 10527.682923263434\n",
      "Train Epoch: 11 [256/225000 (0%)] Loss: 10497.904297\n",
      "Train Epoch: 11 [4352/225000 (2%)] Loss: 10491.900391\n",
      "Train Epoch: 11 [8448/225000 (4%)] Loss: 10663.677734\n",
      "Train Epoch: 11 [12544/225000 (6%)] Loss: 10349.613281\n",
      "Train Epoch: 11 [16640/225000 (7%)] Loss: 10455.957031\n",
      "Train Epoch: 11 [20736/225000 (9%)] Loss: 10510.958984\n",
      "Train Epoch: 11 [24832/225000 (11%)] Loss: 10291.535156\n",
      "Train Epoch: 11 [28928/225000 (13%)] Loss: 10294.527344\n",
      "Train Epoch: 11 [33024/225000 (15%)] Loss: 10292.072266\n",
      "Train Epoch: 11 [37120/225000 (16%)] Loss: 10334.683594\n",
      "Train Epoch: 11 [41216/225000 (18%)] Loss: 10562.791016\n",
      "Train Epoch: 11 [45312/225000 (20%)] Loss: 10481.751953\n",
      "Train Epoch: 11 [49408/225000 (22%)] Loss: 10178.171875\n",
      "Train Epoch: 11 [53504/225000 (24%)] Loss: 10622.072266\n",
      "Train Epoch: 11 [57600/225000 (26%)] Loss: 10276.339844\n",
      "Train Epoch: 11 [61696/225000 (27%)] Loss: 10282.451172\n",
      "Train Epoch: 11 [65792/225000 (29%)] Loss: 10586.974609\n",
      "Train Epoch: 11 [69888/225000 (31%)] Loss: 10285.005859\n",
      "Train Epoch: 11 [73984/225000 (33%)] Loss: 10475.451172\n",
      "Train Epoch: 11 [78080/225000 (35%)] Loss: 10373.314453\n",
      "Train Epoch: 11 [82176/225000 (37%)] Loss: 10218.154297\n",
      "Train Epoch: 11 [86272/225000 (38%)] Loss: 10324.519531\n",
      "Train Epoch: 11 [90368/225000 (40%)] Loss: 10412.240234\n",
      "Train Epoch: 11 [94464/225000 (42%)] Loss: 10432.316406\n",
      "Train Epoch: 11 [98560/225000 (44%)] Loss: 10293.644531\n",
      "Train Epoch: 11 [102656/225000 (46%)] Loss: 10494.207031\n",
      "Train Epoch: 11 [106752/225000 (47%)] Loss: 10206.812500\n",
      "Train Epoch: 11 [110848/225000 (49%)] Loss: 10305.533203\n",
      "Train Epoch: 11 [114944/225000 (51%)] Loss: 10575.757812\n",
      "Train Epoch: 11 [119040/225000 (53%)] Loss: 10447.804688\n",
      "Train Epoch: 11 [123136/225000 (55%)] Loss: 10437.013672\n",
      "Train Epoch: 11 [127232/225000 (57%)] Loss: 10410.484375\n",
      "Train Epoch: 11 [131328/225000 (58%)] Loss: 10365.244141\n",
      "Train Epoch: 11 [135424/225000 (60%)] Loss: 10340.601562\n",
      "Train Epoch: 11 [139520/225000 (62%)] Loss: 10513.419922\n",
      "Train Epoch: 11 [143616/225000 (64%)] Loss: 10380.091797\n",
      "Train Epoch: 11 [147712/225000 (66%)] Loss: 10366.218750\n",
      "Train Epoch: 11 [151808/225000 (67%)] Loss: 10250.394531\n",
      "Train Epoch: 11 [155904/225000 (69%)] Loss: 10348.185547\n",
      "Train Epoch: 11 [160000/225000 (71%)] Loss: 10353.863281\n",
      "Train Epoch: 11 [164096/225000 (73%)] Loss: 10285.234375\n",
      "Train Epoch: 11 [168192/225000 (75%)] Loss: 10292.154297\n",
      "Train Epoch: 11 [172288/225000 (77%)] Loss: 10423.982422\n",
      "Train Epoch: 11 [176384/225000 (78%)] Loss: 10184.322266\n",
      "Train Epoch: 11 [180480/225000 (80%)] Loss: 10238.138672\n",
      "Train Epoch: 11 [184576/225000 (82%)] Loss: 10052.900391\n",
      "Train Epoch: 11 [188672/225000 (84%)] Loss: 10205.988281\n",
      "Train Epoch: 11 [192768/225000 (86%)] Loss: 10539.664062\n",
      "Train Epoch: 11 [196864/225000 (87%)] Loss: 10264.062500\n",
      "Train Epoch: 11 [200960/225000 (89%)] Loss: 10308.496094\n",
      "Train Epoch: 11 [205056/225000 (91%)] Loss: 10183.841797\n",
      "Train Epoch: 11 [209152/225000 (93%)] Loss: 10176.755859\n",
      "Train Epoch: 11 [213248/225000 (95%)] Loss: 10406.031250\n",
      "Train Epoch: 11 [217344/225000 (97%)] Loss: 10347.335938\n",
      "Train Epoch: 11 [221440/225000 (98%)] Loss: 10220.187500\n",
      "    epoch          : 11\n",
      "    loss           : 10684.97982881826\n",
      "    val_loss       : 10524.764521817771\n",
      "Train Epoch: 12 [256/225000 (0%)] Loss: 9998.955078\n",
      "Train Epoch: 12 [4352/225000 (2%)] Loss: 10079.281250\n",
      "Train Epoch: 12 [8448/225000 (4%)] Loss: 10161.070312\n",
      "Train Epoch: 12 [12544/225000 (6%)] Loss: 10233.890625\n",
      "Train Epoch: 12 [16640/225000 (7%)] Loss: 10233.775391\n",
      "Train Epoch: 12 [20736/225000 (9%)] Loss: 10090.281250\n",
      "Train Epoch: 12 [24832/225000 (11%)] Loss: 10152.988281\n",
      "Train Epoch: 12 [28928/225000 (13%)] Loss: 10201.230469\n",
      "Train Epoch: 12 [33024/225000 (15%)] Loss: 10496.980469\n",
      "Train Epoch: 12 [37120/225000 (16%)] Loss: 10214.855469\n",
      "Train Epoch: 12 [41216/225000 (18%)] Loss: 10207.365234\n",
      "Train Epoch: 12 [45312/225000 (20%)] Loss: 10102.230469\n",
      "Train Epoch: 12 [49408/225000 (22%)] Loss: 10324.529297\n",
      "Train Epoch: 12 [53504/225000 (24%)] Loss: 20153.386719\n",
      "Train Epoch: 12 [57600/225000 (26%)] Loss: 9906.052734\n",
      "Train Epoch: 12 [61696/225000 (27%)] Loss: 10280.027344\n",
      "Train Epoch: 12 [65792/225000 (29%)] Loss: 10054.656250\n",
      "Train Epoch: 12 [69888/225000 (31%)] Loss: 10088.865234\n",
      "Train Epoch: 12 [73984/225000 (33%)] Loss: 10325.236328\n",
      "Train Epoch: 12 [78080/225000 (35%)] Loss: 10232.294922\n",
      "Train Epoch: 12 [82176/225000 (37%)] Loss: 10280.824219\n",
      "Train Epoch: 12 [86272/225000 (38%)] Loss: 10110.285156\n",
      "Train Epoch: 12 [90368/225000 (40%)] Loss: 10286.710938\n",
      "Train Epoch: 12 [94464/225000 (42%)] Loss: 10019.755859\n",
      "Train Epoch: 12 [98560/225000 (44%)] Loss: 9991.990234\n",
      "Train Epoch: 12 [102656/225000 (46%)] Loss: 10306.613281\n",
      "Train Epoch: 12 [106752/225000 (47%)] Loss: 10149.400391\n",
      "Train Epoch: 12 [110848/225000 (49%)] Loss: 10163.728516\n",
      "Train Epoch: 12 [114944/225000 (51%)] Loss: 10056.871094\n",
      "Train Epoch: 12 [119040/225000 (53%)] Loss: 10303.941406\n",
      "Train Epoch: 12 [123136/225000 (55%)] Loss: 10012.121094\n",
      "Train Epoch: 12 [127232/225000 (57%)] Loss: 10124.738281\n",
      "Train Epoch: 12 [131328/225000 (58%)] Loss: 10252.234375\n",
      "Train Epoch: 12 [135424/225000 (60%)] Loss: 10208.009766\n",
      "Train Epoch: 12 [139520/225000 (62%)] Loss: 10305.367188\n",
      "Train Epoch: 12 [143616/225000 (64%)] Loss: 10377.685547\n",
      "Train Epoch: 12 [147712/225000 (66%)] Loss: 10261.205078\n",
      "Train Epoch: 12 [151808/225000 (67%)] Loss: 9904.652344\n",
      "Train Epoch: 12 [155904/225000 (69%)] Loss: 10053.427734\n",
      "Train Epoch: 12 [160000/225000 (71%)] Loss: 10127.128906\n",
      "Train Epoch: 12 [164096/225000 (73%)] Loss: 10020.923828\n",
      "Train Epoch: 12 [168192/225000 (75%)] Loss: 10317.763672\n",
      "Train Epoch: 12 [172288/225000 (77%)] Loss: 10285.003906\n",
      "Train Epoch: 12 [176384/225000 (78%)] Loss: 10216.539062\n",
      "Train Epoch: 12 [180480/225000 (80%)] Loss: 10132.689453\n",
      "Train Epoch: 12 [184576/225000 (82%)] Loss: 10014.753906\n",
      "Train Epoch: 12 [188672/225000 (84%)] Loss: 10236.724609\n",
      "Train Epoch: 12 [192768/225000 (86%)] Loss: 10472.443359\n",
      "Train Epoch: 12 [196864/225000 (87%)] Loss: 9928.378906\n",
      "Train Epoch: 12 [200960/225000 (89%)] Loss: 9811.914062\n",
      "Train Epoch: 12 [205056/225000 (91%)] Loss: 9924.775391\n",
      "Train Epoch: 12 [209152/225000 (93%)] Loss: 10196.179688\n",
      "Train Epoch: 12 [213248/225000 (95%)] Loss: 10233.570312\n",
      "Train Epoch: 12 [217344/225000 (97%)] Loss: 10097.777344\n",
      "Train Epoch: 12 [221440/225000 (98%)] Loss: 10197.958984\n",
      "    epoch          : 12\n",
      "    loss           : 10392.2298377062\n",
      "    val_loss       : 10173.170214993004\n",
      "Train Epoch: 13 [256/225000 (0%)] Loss: 10003.498047\n",
      "Train Epoch: 13 [4352/225000 (2%)] Loss: 10102.703125\n",
      "Train Epoch: 13 [8448/225000 (4%)] Loss: 9876.400391\n",
      "Train Epoch: 13 [12544/225000 (6%)] Loss: 10096.367188\n",
      "Train Epoch: 13 [16640/225000 (7%)] Loss: 10293.433594\n",
      "Train Epoch: 13 [20736/225000 (9%)] Loss: 10171.404297\n",
      "Train Epoch: 13 [24832/225000 (11%)] Loss: 10339.279297\n",
      "Train Epoch: 13 [28928/225000 (13%)] Loss: 9992.921875\n",
      "Train Epoch: 13 [33024/225000 (15%)] Loss: 10131.900391\n",
      "Train Epoch: 13 [37120/225000 (16%)] Loss: 33914.343750\n",
      "Train Epoch: 13 [41216/225000 (18%)] Loss: 10118.433594\n",
      "Train Epoch: 13 [45312/225000 (20%)] Loss: 10167.210938\n",
      "Train Epoch: 13 [49408/225000 (22%)] Loss: 34968.429688\n",
      "Train Epoch: 13 [53504/225000 (24%)] Loss: 10012.150391\n",
      "Train Epoch: 13 [57600/225000 (26%)] Loss: 10176.373047\n",
      "Train Epoch: 13 [61696/225000 (27%)] Loss: 10040.496094\n",
      "Train Epoch: 13 [65792/225000 (29%)] Loss: 10020.318359\n",
      "Train Epoch: 13 [69888/225000 (31%)] Loss: 10288.791016\n",
      "Train Epoch: 13 [73984/225000 (33%)] Loss: 10084.734375\n",
      "Train Epoch: 13 [78080/225000 (35%)] Loss: 9985.304688\n",
      "Train Epoch: 13 [82176/225000 (37%)] Loss: 9921.996094\n",
      "Train Epoch: 13 [86272/225000 (38%)] Loss: 9907.123047\n",
      "Train Epoch: 13 [90368/225000 (40%)] Loss: 10081.542969\n",
      "Train Epoch: 13 [94464/225000 (42%)] Loss: 9952.339844\n",
      "Train Epoch: 13 [98560/225000 (44%)] Loss: 10336.361328\n",
      "Train Epoch: 13 [102656/225000 (46%)] Loss: 9565.511719\n",
      "Train Epoch: 13 [106752/225000 (47%)] Loss: 10148.949219\n",
      "Train Epoch: 13 [110848/225000 (49%)] Loss: 14410.001953\n",
      "Train Epoch: 13 [114944/225000 (51%)] Loss: 9934.574219\n",
      "Train Epoch: 13 [119040/225000 (53%)] Loss: 14475.226562\n",
      "Train Epoch: 13 [123136/225000 (55%)] Loss: 10268.089844\n",
      "Train Epoch: 13 [127232/225000 (57%)] Loss: 9832.169922\n",
      "Train Epoch: 13 [131328/225000 (58%)] Loss: 10199.863281\n",
      "Train Epoch: 13 [135424/225000 (60%)] Loss: 9931.595703\n",
      "Train Epoch: 13 [139520/225000 (62%)] Loss: 10035.841797\n",
      "Train Epoch: 13 [143616/225000 (64%)] Loss: 10040.187500\n",
      "Train Epoch: 13 [147712/225000 (66%)] Loss: 10002.835938\n",
      "Train Epoch: 13 [151808/225000 (67%)] Loss: 10177.210938\n",
      "Train Epoch: 13 [155904/225000 (69%)] Loss: 9724.695312\n",
      "Train Epoch: 13 [160000/225000 (71%)] Loss: 9866.048828\n",
      "Train Epoch: 13 [164096/225000 (73%)] Loss: 10151.072266\n",
      "Train Epoch: 13 [168192/225000 (75%)] Loss: 9827.404297\n",
      "Train Epoch: 13 [172288/225000 (77%)] Loss: 9972.720703\n",
      "Train Epoch: 13 [176384/225000 (78%)] Loss: 10237.646484\n",
      "Train Epoch: 13 [180480/225000 (80%)] Loss: 10129.779297\n",
      "Train Epoch: 13 [184576/225000 (82%)] Loss: 9950.421875\n",
      "Train Epoch: 13 [188672/225000 (84%)] Loss: 10016.222656\n",
      "Train Epoch: 13 [192768/225000 (86%)] Loss: 10024.005859\n",
      "Train Epoch: 13 [196864/225000 (87%)] Loss: 10254.242188\n",
      "Train Epoch: 13 [200960/225000 (89%)] Loss: 9789.416016\n",
      "Train Epoch: 13 [205056/225000 (91%)] Loss: 10016.191406\n",
      "Train Epoch: 13 [209152/225000 (93%)] Loss: 9889.675781\n",
      "Train Epoch: 13 [213248/225000 (95%)] Loss: 9896.269531\n",
      "Train Epoch: 13 [217344/225000 (97%)] Loss: 10165.708984\n",
      "Train Epoch: 13 [221440/225000 (98%)] Loss: 9916.486328\n",
      "    epoch          : 13\n",
      "    loss           : 10340.772127639719\n",
      "    val_loss       : 10247.723389921748\n",
      "Train Epoch: 14 [256/225000 (0%)] Loss: 9918.583984\n",
      "Train Epoch: 14 [4352/225000 (2%)] Loss: 10092.441406\n",
      "Train Epoch: 14 [8448/225000 (4%)] Loss: 9934.597656\n",
      "Train Epoch: 14 [12544/225000 (6%)] Loss: 10125.738281\n",
      "Train Epoch: 14 [16640/225000 (7%)] Loss: 9956.089844\n",
      "Train Epoch: 14 [20736/225000 (9%)] Loss: 9987.849609\n",
      "Train Epoch: 14 [24832/225000 (11%)] Loss: 9820.792969\n",
      "Train Epoch: 14 [28928/225000 (13%)] Loss: 10118.099609\n",
      "Train Epoch: 14 [33024/225000 (15%)] Loss: 9999.935547\n",
      "Train Epoch: 14 [37120/225000 (16%)] Loss: 9902.330078\n",
      "Train Epoch: 14 [41216/225000 (18%)] Loss: 9860.000000\n",
      "Train Epoch: 14 [45312/225000 (20%)] Loss: 9796.373047\n",
      "Train Epoch: 14 [49408/225000 (22%)] Loss: 9996.685547\n",
      "Train Epoch: 14 [53504/225000 (24%)] Loss: 9793.255859\n",
      "Train Epoch: 14 [57600/225000 (26%)] Loss: 9903.302734\n",
      "Train Epoch: 14 [61696/225000 (27%)] Loss: 9711.322266\n",
      "Train Epoch: 14 [65792/225000 (29%)] Loss: 10206.091797\n",
      "Train Epoch: 14 [69888/225000 (31%)] Loss: 10090.867188\n",
      "Train Epoch: 14 [73984/225000 (33%)] Loss: 10101.468750\n",
      "Train Epoch: 14 [78080/225000 (35%)] Loss: 10037.564453\n",
      "Train Epoch: 14 [82176/225000 (37%)] Loss: 9965.757812\n",
      "Train Epoch: 14 [86272/225000 (38%)] Loss: 9921.205078\n",
      "Train Epoch: 14 [90368/225000 (40%)] Loss: 10052.421875\n",
      "Train Epoch: 14 [94464/225000 (42%)] Loss: 9946.619141\n",
      "Train Epoch: 14 [98560/225000 (44%)] Loss: 9941.431641\n",
      "Train Epoch: 14 [102656/225000 (46%)] Loss: 10001.798828\n",
      "Train Epoch: 14 [106752/225000 (47%)] Loss: 10093.261719\n",
      "Train Epoch: 14 [110848/225000 (49%)] Loss: 9919.519531\n",
      "Train Epoch: 14 [114944/225000 (51%)] Loss: 9926.636719\n",
      "Train Epoch: 14 [119040/225000 (53%)] Loss: 10060.320312\n",
      "Train Epoch: 14 [123136/225000 (55%)] Loss: 9749.587891\n",
      "Train Epoch: 14 [127232/225000 (57%)] Loss: 9669.099609\n",
      "Train Epoch: 14 [131328/225000 (58%)] Loss: 9900.136719\n",
      "Train Epoch: 14 [135424/225000 (60%)] Loss: 9874.628906\n",
      "Train Epoch: 14 [139520/225000 (62%)] Loss: 9832.337891\n",
      "Train Epoch: 14 [143616/225000 (64%)] Loss: 9904.126953\n",
      "Train Epoch: 14 [147712/225000 (66%)] Loss: 9660.875000\n",
      "Train Epoch: 14 [151808/225000 (67%)] Loss: 9758.279297\n",
      "Train Epoch: 14 [155904/225000 (69%)] Loss: 9673.929688\n",
      "Train Epoch: 14 [160000/225000 (71%)] Loss: 9797.748047\n",
      "Train Epoch: 14 [164096/225000 (73%)] Loss: 10226.984375\n",
      "Train Epoch: 14 [168192/225000 (75%)] Loss: 9982.865234\n",
      "Train Epoch: 14 [172288/225000 (77%)] Loss: 9834.111328\n",
      "Train Epoch: 14 [176384/225000 (78%)] Loss: 9813.937500\n",
      "Train Epoch: 14 [180480/225000 (80%)] Loss: 10064.978516\n",
      "Train Epoch: 14 [184576/225000 (82%)] Loss: 9888.283203\n",
      "Train Epoch: 14 [188672/225000 (84%)] Loss: 9722.761719\n",
      "Train Epoch: 14 [192768/225000 (86%)] Loss: 9826.601562\n",
      "Train Epoch: 14 [196864/225000 (87%)] Loss: 9804.125000\n",
      "Train Epoch: 14 [200960/225000 (89%)] Loss: 9978.214844\n",
      "Train Epoch: 14 [205056/225000 (91%)] Loss: 9814.298828\n",
      "Train Epoch: 14 [209152/225000 (93%)] Loss: 9864.658203\n",
      "Train Epoch: 14 [213248/225000 (95%)] Loss: 9739.644531\n",
      "Train Epoch: 14 [217344/225000 (97%)] Loss: 9684.125000\n",
      "Train Epoch: 14 [221440/225000 (98%)] Loss: 9823.185547\n",
      "    epoch          : 14\n",
      "    loss           : 10058.7963339466\n",
      "    val_loss       : 10007.305038547029\n",
      "Train Epoch: 15 [256/225000 (0%)] Loss: 9863.279297\n",
      "Train Epoch: 15 [4352/225000 (2%)] Loss: 9618.023438\n",
      "Train Epoch: 15 [8448/225000 (4%)] Loss: 9943.957031\n",
      "Train Epoch: 15 [12544/225000 (6%)] Loss: 9790.347656\n",
      "Train Epoch: 15 [16640/225000 (7%)] Loss: 9823.705078\n",
      "Train Epoch: 15 [20736/225000 (9%)] Loss: 9831.546875\n",
      "Train Epoch: 15 [24832/225000 (11%)] Loss: 10054.976562\n",
      "Train Epoch: 15 [28928/225000 (13%)] Loss: 9727.275391\n",
      "Train Epoch: 15 [33024/225000 (15%)] Loss: 9599.451172\n",
      "Train Epoch: 15 [37120/225000 (16%)] Loss: 9649.564453\n",
      "Train Epoch: 15 [41216/225000 (18%)] Loss: 9956.113281\n",
      "Train Epoch: 15 [45312/225000 (20%)] Loss: 9899.720703\n",
      "Train Epoch: 15 [49408/225000 (22%)] Loss: 9808.585938\n",
      "Train Epoch: 15 [53504/225000 (24%)] Loss: 9691.492188\n",
      "Train Epoch: 15 [57600/225000 (26%)] Loss: 9669.191406\n",
      "Train Epoch: 15 [61696/225000 (27%)] Loss: 10022.800781\n",
      "Train Epoch: 15 [65792/225000 (29%)] Loss: 9654.044922\n",
      "Train Epoch: 15 [69888/225000 (31%)] Loss: 9834.693359\n",
      "Train Epoch: 15 [73984/225000 (33%)] Loss: 9686.417969\n",
      "Train Epoch: 15 [78080/225000 (35%)] Loss: 9885.558594\n",
      "Train Epoch: 15 [82176/225000 (37%)] Loss: 9782.462891\n",
      "Train Epoch: 15 [86272/225000 (38%)] Loss: 9392.789062\n",
      "Train Epoch: 15 [90368/225000 (40%)] Loss: 9690.533203\n",
      "Train Epoch: 15 [94464/225000 (42%)] Loss: 9801.486328\n",
      "Train Epoch: 15 [98560/225000 (44%)] Loss: 9736.582031\n",
      "Train Epoch: 15 [102656/225000 (46%)] Loss: 9801.443359\n",
      "Train Epoch: 15 [106752/225000 (47%)] Loss: 9755.960938\n",
      "Train Epoch: 15 [110848/225000 (49%)] Loss: 9561.935547\n",
      "Train Epoch: 15 [114944/225000 (51%)] Loss: 9758.783203\n",
      "Train Epoch: 15 [119040/225000 (53%)] Loss: 9716.501953\n",
      "Train Epoch: 15 [123136/225000 (55%)] Loss: 9769.679688\n",
      "Train Epoch: 15 [127232/225000 (57%)] Loss: 9896.703125\n",
      "Train Epoch: 15 [131328/225000 (58%)] Loss: 9861.988281\n",
      "Train Epoch: 15 [135424/225000 (60%)] Loss: 9747.529297\n",
      "Train Epoch: 15 [139520/225000 (62%)] Loss: 9650.980469\n",
      "Train Epoch: 15 [143616/225000 (64%)] Loss: 9791.615234\n",
      "Train Epoch: 15 [147712/225000 (66%)] Loss: 9867.013672\n",
      "Train Epoch: 15 [151808/225000 (67%)] Loss: 9667.255859\n",
      "Train Epoch: 15 [155904/225000 (69%)] Loss: 9886.349609\n",
      "Train Epoch: 15 [160000/225000 (71%)] Loss: 9767.945312\n",
      "Train Epoch: 15 [164096/225000 (73%)] Loss: 9869.517578\n",
      "Train Epoch: 15 [168192/225000 (75%)] Loss: 9787.576172\n",
      "Train Epoch: 15 [172288/225000 (77%)] Loss: 9745.556641\n",
      "Train Epoch: 15 [176384/225000 (78%)] Loss: 9756.373047\n",
      "Train Epoch: 15 [180480/225000 (80%)] Loss: 9774.503906\n",
      "Train Epoch: 15 [184576/225000 (82%)] Loss: 9828.371094\n",
      "Train Epoch: 15 [188672/225000 (84%)] Loss: 9815.154297\n",
      "Train Epoch: 15 [192768/225000 (86%)] Loss: 9934.828125\n",
      "Train Epoch: 15 [196864/225000 (87%)] Loss: 9588.843750\n",
      "Train Epoch: 15 [200960/225000 (89%)] Loss: 9610.703125\n",
      "Train Epoch: 15 [205056/225000 (91%)] Loss: 9730.105469\n",
      "Train Epoch: 15 [209152/225000 (93%)] Loss: 9586.494141\n",
      "Train Epoch: 15 [213248/225000 (95%)] Loss: 9928.882812\n",
      "Train Epoch: 15 [217344/225000 (97%)] Loss: 9897.210938\n",
      "Train Epoch: 15 [221440/225000 (98%)] Loss: 9714.261719\n",
      "    epoch          : 15\n",
      "    loss           : 9808.155622289178\n",
      "    val_loss       : 9668.102734735425\n",
      "Train Epoch: 16 [256/225000 (0%)] Loss: 9527.185547\n",
      "Train Epoch: 16 [4352/225000 (2%)] Loss: 9674.503906\n",
      "Train Epoch: 16 [8448/225000 (4%)] Loss: 9666.158203\n",
      "Train Epoch: 16 [12544/225000 (6%)] Loss: 9660.121094\n",
      "Train Epoch: 16 [16640/225000 (7%)] Loss: 9507.126953\n",
      "Train Epoch: 16 [20736/225000 (9%)] Loss: 9786.996094\n",
      "Train Epoch: 16 [24832/225000 (11%)] Loss: 9833.179688\n",
      "Train Epoch: 16 [28928/225000 (13%)] Loss: 9634.855469\n",
      "Train Epoch: 16 [33024/225000 (15%)] Loss: 10014.398438\n",
      "Train Epoch: 16 [37120/225000 (16%)] Loss: 9730.912109\n",
      "Train Epoch: 16 [41216/225000 (18%)] Loss: 9568.072266\n",
      "Train Epoch: 16 [45312/225000 (20%)] Loss: 9615.031250\n",
      "Train Epoch: 16 [49408/225000 (22%)] Loss: 9595.896484\n",
      "Train Epoch: 16 [53504/225000 (24%)] Loss: 9814.888672\n",
      "Train Epoch: 16 [57600/225000 (26%)] Loss: 9676.714844\n",
      "Train Epoch: 16 [61696/225000 (27%)] Loss: 9744.660156\n",
      "Train Epoch: 16 [65792/225000 (29%)] Loss: 9619.648438\n",
      "Train Epoch: 16 [69888/225000 (31%)] Loss: 9652.994141\n",
      "Train Epoch: 16 [73984/225000 (33%)] Loss: 9860.537109\n",
      "Train Epoch: 16 [78080/225000 (35%)] Loss: 9843.494141\n",
      "Train Epoch: 16 [82176/225000 (37%)] Loss: 9681.142578\n",
      "Train Epoch: 16 [86272/225000 (38%)] Loss: 9624.402344\n",
      "Train Epoch: 16 [90368/225000 (40%)] Loss: 9605.859375\n",
      "Train Epoch: 16 [94464/225000 (42%)] Loss: 9663.132812\n",
      "Train Epoch: 16 [98560/225000 (44%)] Loss: 9779.320312\n",
      "Train Epoch: 16 [102656/225000 (46%)] Loss: 9820.113281\n",
      "Train Epoch: 16 [106752/225000 (47%)] Loss: 9658.632812\n",
      "Train Epoch: 16 [110848/225000 (49%)] Loss: 9588.919922\n",
      "Train Epoch: 16 [114944/225000 (51%)] Loss: 9838.492188\n",
      "Train Epoch: 16 [119040/225000 (53%)] Loss: 9468.062500\n",
      "Train Epoch: 16 [123136/225000 (55%)] Loss: 9543.484375\n",
      "Train Epoch: 16 [127232/225000 (57%)] Loss: 9399.562500\n",
      "Train Epoch: 16 [131328/225000 (58%)] Loss: 9595.636719\n",
      "Train Epoch: 16 [135424/225000 (60%)] Loss: 9544.562500\n",
      "Train Epoch: 16 [139520/225000 (62%)] Loss: 9539.628906\n",
      "Train Epoch: 16 [143616/225000 (64%)] Loss: 9466.007812\n",
      "Train Epoch: 16 [147712/225000 (66%)] Loss: 31041.857422\n",
      "Train Epoch: 16 [151808/225000 (67%)] Loss: 9765.414062\n",
      "Train Epoch: 16 [155904/225000 (69%)] Loss: 9522.832031\n",
      "Train Epoch: 16 [160000/225000 (71%)] Loss: 9860.964844\n",
      "Train Epoch: 16 [164096/225000 (73%)] Loss: 9527.214844\n",
      "Train Epoch: 16 [168192/225000 (75%)] Loss: 9614.882812\n",
      "Train Epoch: 16 [172288/225000 (77%)] Loss: 9432.126953\n",
      "Train Epoch: 16 [176384/225000 (78%)] Loss: 9851.121094\n",
      "Train Epoch: 16 [180480/225000 (80%)] Loss: 9496.056641\n",
      "Train Epoch: 16 [184576/225000 (82%)] Loss: 9525.347656\n",
      "Train Epoch: 16 [188672/225000 (84%)] Loss: 9837.941406\n",
      "Train Epoch: 16 [192768/225000 (86%)] Loss: 9696.371094\n",
      "Train Epoch: 16 [196864/225000 (87%)] Loss: 9637.785156\n",
      "Train Epoch: 16 [200960/225000 (89%)] Loss: 9946.726562\n",
      "Train Epoch: 16 [205056/225000 (91%)] Loss: 9805.511719\n",
      "Train Epoch: 16 [209152/225000 (93%)] Loss: 9510.160156\n",
      "Train Epoch: 16 [213248/225000 (95%)] Loss: 9788.359375\n",
      "Train Epoch: 16 [217344/225000 (97%)] Loss: 9562.726562\n",
      "Train Epoch: 16 [221440/225000 (98%)] Loss: 9410.726562\n",
      "    epoch          : 16\n",
      "    loss           : 9763.809455880262\n",
      "    val_loss       : 9570.930593363484\n",
      "Train Epoch: 17 [256/225000 (0%)] Loss: 9521.292969\n",
      "Train Epoch: 17 [4352/225000 (2%)] Loss: 9585.798828\n",
      "Train Epoch: 17 [8448/225000 (4%)] Loss: 9599.609375\n",
      "Train Epoch: 17 [12544/225000 (6%)] Loss: 9502.613281\n",
      "Train Epoch: 17 [16640/225000 (7%)] Loss: 9821.271484\n",
      "Train Epoch: 17 [20736/225000 (9%)] Loss: 9453.697266\n",
      "Train Epoch: 17 [24832/225000 (11%)] Loss: 9572.873047\n",
      "Train Epoch: 17 [28928/225000 (13%)] Loss: 9444.908203\n",
      "Train Epoch: 17 [33024/225000 (15%)] Loss: 9685.044922\n",
      "Train Epoch: 17 [37120/225000 (16%)] Loss: 9556.750000\n",
      "Train Epoch: 17 [41216/225000 (18%)] Loss: 9477.779297\n",
      "Train Epoch: 17 [45312/225000 (20%)] Loss: 9692.332031\n",
      "Train Epoch: 17 [49408/225000 (22%)] Loss: 9694.578125\n",
      "Train Epoch: 17 [53504/225000 (24%)] Loss: 9556.632812\n",
      "Train Epoch: 17 [57600/225000 (26%)] Loss: 9310.595703\n",
      "Train Epoch: 17 [61696/225000 (27%)] Loss: 9675.609375\n",
      "Train Epoch: 17 [65792/225000 (29%)] Loss: 9690.996094\n",
      "Train Epoch: 17 [69888/225000 (31%)] Loss: 9638.787109\n",
      "Train Epoch: 17 [73984/225000 (33%)] Loss: 9756.306641\n",
      "Train Epoch: 17 [78080/225000 (35%)] Loss: 9611.076172\n",
      "Train Epoch: 17 [82176/225000 (37%)] Loss: 9757.789062\n",
      "Train Epoch: 17 [86272/225000 (38%)] Loss: 9508.056641\n",
      "Train Epoch: 17 [90368/225000 (40%)] Loss: 9586.642578\n",
      "Train Epoch: 17 [94464/225000 (42%)] Loss: 9416.486328\n",
      "Train Epoch: 17 [98560/225000 (44%)] Loss: 9759.095703\n",
      "Train Epoch: 17 [102656/225000 (46%)] Loss: 9606.941406\n",
      "Train Epoch: 17 [106752/225000 (47%)] Loss: 9615.593750\n",
      "Train Epoch: 17 [110848/225000 (49%)] Loss: 9515.015625\n",
      "Train Epoch: 17 [114944/225000 (51%)] Loss: 9452.490234\n",
      "Train Epoch: 17 [119040/225000 (53%)] Loss: 9645.031250\n",
      "Train Epoch: 17 [123136/225000 (55%)] Loss: 9519.845703\n",
      "Train Epoch: 17 [127232/225000 (57%)] Loss: 9567.275391\n",
      "Train Epoch: 17 [131328/225000 (58%)] Loss: 9681.000000\n",
      "Train Epoch: 17 [135424/225000 (60%)] Loss: 9528.039062\n",
      "Train Epoch: 17 [139520/225000 (62%)] Loss: 9715.630859\n",
      "Train Epoch: 17 [143616/225000 (64%)] Loss: 9575.619141\n",
      "Train Epoch: 17 [147712/225000 (66%)] Loss: 9549.878906\n",
      "Train Epoch: 17 [151808/225000 (67%)] Loss: 9456.867188\n",
      "Train Epoch: 17 [155904/225000 (69%)] Loss: 9582.015625\n",
      "Train Epoch: 17 [160000/225000 (71%)] Loss: 9771.490234\n",
      "Train Epoch: 17 [164096/225000 (73%)] Loss: 9343.974609\n",
      "Train Epoch: 17 [168192/225000 (75%)] Loss: 9633.333984\n",
      "Train Epoch: 17 [172288/225000 (77%)] Loss: 9394.982422\n",
      "Train Epoch: 17 [176384/225000 (78%)] Loss: 9622.384766\n",
      "Train Epoch: 17 [180480/225000 (80%)] Loss: 9505.236328\n",
      "Train Epoch: 17 [184576/225000 (82%)] Loss: 9626.738281\n",
      "Train Epoch: 17 [188672/225000 (84%)] Loss: 9690.687500\n",
      "Train Epoch: 17 [192768/225000 (86%)] Loss: 9675.437500\n",
      "Train Epoch: 17 [196864/225000 (87%)] Loss: 9677.769531\n",
      "Train Epoch: 17 [200960/225000 (89%)] Loss: 9389.447266\n",
      "Train Epoch: 17 [205056/225000 (91%)] Loss: 9556.937500\n",
      "Train Epoch: 17 [209152/225000 (93%)] Loss: 9496.886719\n",
      "Train Epoch: 17 [213248/225000 (95%)] Loss: 9416.810547\n",
      "Train Epoch: 17 [217344/225000 (97%)] Loss: 9721.681641\n",
      "Train Epoch: 17 [221440/225000 (98%)] Loss: 9555.443359\n",
      "    epoch          : 17\n",
      "    loss           : 9732.808091581343\n",
      "    val_loss       : 9473.622489948662\n",
      "Train Epoch: 18 [256/225000 (0%)] Loss: 9366.798828\n",
      "Train Epoch: 18 [4352/225000 (2%)] Loss: 9393.445312\n",
      "Train Epoch: 18 [8448/225000 (4%)] Loss: 9625.886719\n",
      "Train Epoch: 18 [12544/225000 (6%)] Loss: 9604.986328\n",
      "Train Epoch: 18 [16640/225000 (7%)] Loss: 9526.582031\n",
      "Train Epoch: 18 [20736/225000 (9%)] Loss: 9490.087891\n",
      "Train Epoch: 18 [24832/225000 (11%)] Loss: 9749.609375\n",
      "Train Epoch: 18 [28928/225000 (13%)] Loss: 14296.724609\n",
      "Train Epoch: 18 [33024/225000 (15%)] Loss: 9713.388672\n",
      "Train Epoch: 18 [37120/225000 (16%)] Loss: 9301.566406\n",
      "Train Epoch: 18 [41216/225000 (18%)] Loss: 9231.794922\n",
      "Train Epoch: 18 [45312/225000 (20%)] Loss: 9415.267578\n",
      "Train Epoch: 18 [49408/225000 (22%)] Loss: 9530.941406\n",
      "Train Epoch: 18 [53504/225000 (24%)] Loss: 9559.328125\n",
      "Train Epoch: 18 [57600/225000 (26%)] Loss: 9402.486328\n",
      "Train Epoch: 18 [61696/225000 (27%)] Loss: 9499.896484\n",
      "Train Epoch: 18 [65792/225000 (29%)] Loss: 9488.156250\n",
      "Train Epoch: 18 [69888/225000 (31%)] Loss: 9660.585938\n",
      "Train Epoch: 18 [73984/225000 (33%)] Loss: 9492.925781\n",
      "Train Epoch: 18 [78080/225000 (35%)] Loss: 9456.427734\n",
      "Train Epoch: 18 [82176/225000 (37%)] Loss: 9345.751953\n",
      "Train Epoch: 18 [86272/225000 (38%)] Loss: 9481.820312\n",
      "Train Epoch: 18 [90368/225000 (40%)] Loss: 9360.105469\n",
      "Train Epoch: 18 [94464/225000 (42%)] Loss: 9577.851562\n",
      "Train Epoch: 18 [98560/225000 (44%)] Loss: 9548.736328\n",
      "Train Epoch: 18 [102656/225000 (46%)] Loss: 9528.273438\n",
      "Train Epoch: 18 [106752/225000 (47%)] Loss: 9380.439453\n",
      "Train Epoch: 18 [110848/225000 (49%)] Loss: 9371.011719\n",
      "Train Epoch: 18 [114944/225000 (51%)] Loss: 9468.533203\n",
      "Train Epoch: 18 [119040/225000 (53%)] Loss: 9428.800781\n",
      "Train Epoch: 18 [123136/225000 (55%)] Loss: 9477.675781\n",
      "Train Epoch: 18 [127232/225000 (57%)] Loss: 9340.144531\n",
      "Train Epoch: 18 [131328/225000 (58%)] Loss: 9376.601562\n",
      "Train Epoch: 18 [135424/225000 (60%)] Loss: 9576.941406\n",
      "Train Epoch: 18 [139520/225000 (62%)] Loss: 9679.929688\n",
      "Train Epoch: 18 [143616/225000 (64%)] Loss: 9568.593750\n",
      "Train Epoch: 18 [147712/225000 (66%)] Loss: 9494.550781\n",
      "Train Epoch: 18 [151808/225000 (67%)] Loss: 9291.208984\n",
      "Train Epoch: 18 [155904/225000 (69%)] Loss: 9353.205078\n",
      "Train Epoch: 18 [160000/225000 (71%)] Loss: 14309.830078\n",
      "Train Epoch: 18 [164096/225000 (73%)] Loss: 14375.859375\n",
      "Train Epoch: 18 [168192/225000 (75%)] Loss: 9578.265625\n",
      "Train Epoch: 18 [172288/225000 (77%)] Loss: 9374.289062\n",
      "Train Epoch: 18 [176384/225000 (78%)] Loss: 14472.517578\n",
      "Train Epoch: 18 [180480/225000 (80%)] Loss: 9452.642578\n",
      "Train Epoch: 18 [184576/225000 (82%)] Loss: 9395.714844\n",
      "Train Epoch: 18 [188672/225000 (84%)] Loss: 9351.703125\n",
      "Train Epoch: 18 [192768/225000 (86%)] Loss: 9179.558594\n",
      "Train Epoch: 18 [196864/225000 (87%)] Loss: 9416.208984\n",
      "Train Epoch: 18 [200960/225000 (89%)] Loss: 9200.294922\n",
      "Train Epoch: 18 [205056/225000 (91%)] Loss: 9356.921875\n",
      "Train Epoch: 18 [209152/225000 (93%)] Loss: 9353.943359\n",
      "Train Epoch: 18 [213248/225000 (95%)] Loss: 9354.398438\n",
      "Train Epoch: 18 [217344/225000 (97%)] Loss: 9537.804688\n",
      "Train Epoch: 18 [221440/225000 (98%)] Loss: 9328.470703\n",
      "    epoch          : 18\n",
      "    loss           : 9628.320114743316\n",
      "    val_loss       : 9533.551314148368\n",
      "Train Epoch: 19 [256/225000 (0%)] Loss: 9384.140625\n",
      "Train Epoch: 19 [4352/225000 (2%)] Loss: 9473.910156\n",
      "Train Epoch: 19 [8448/225000 (4%)] Loss: 9490.490234\n",
      "Train Epoch: 19 [12544/225000 (6%)] Loss: 9361.636719\n",
      "Train Epoch: 19 [16640/225000 (7%)] Loss: 9225.632812\n",
      "Train Epoch: 19 [20736/225000 (9%)] Loss: 9365.289062\n",
      "Train Epoch: 19 [24832/225000 (11%)] Loss: 9308.460938\n",
      "Train Epoch: 19 [28928/225000 (13%)] Loss: 9252.382812\n",
      "Train Epoch: 19 [33024/225000 (15%)] Loss: 9332.753906\n",
      "Train Epoch: 19 [37120/225000 (16%)] Loss: 9320.708984\n",
      "Train Epoch: 19 [41216/225000 (18%)] Loss: 9386.113281\n",
      "Train Epoch: 19 [45312/225000 (20%)] Loss: 9531.025391\n",
      "Train Epoch: 19 [49408/225000 (22%)] Loss: 9419.529297\n",
      "Train Epoch: 19 [53504/225000 (24%)] Loss: 9322.976562\n",
      "Train Epoch: 19 [57600/225000 (26%)] Loss: 9404.873047\n",
      "Train Epoch: 19 [61696/225000 (27%)] Loss: 9380.558594\n",
      "Train Epoch: 19 [65792/225000 (29%)] Loss: 9280.625000\n",
      "Train Epoch: 19 [69888/225000 (31%)] Loss: 9364.603516\n",
      "Train Epoch: 19 [73984/225000 (33%)] Loss: 9460.000000\n",
      "Train Epoch: 19 [78080/225000 (35%)] Loss: 9323.681641\n",
      "Train Epoch: 19 [82176/225000 (37%)] Loss: 9328.515625\n",
      "Train Epoch: 19 [86272/225000 (38%)] Loss: 9475.640625\n",
      "Train Epoch: 19 [90368/225000 (40%)] Loss: 19493.802734\n",
      "Train Epoch: 19 [94464/225000 (42%)] Loss: 9427.466797\n",
      "Train Epoch: 19 [98560/225000 (44%)] Loss: 9582.623047\n",
      "Train Epoch: 19 [102656/225000 (46%)] Loss: 9483.445312\n",
      "Train Epoch: 19 [106752/225000 (47%)] Loss: 9453.574219\n",
      "Train Epoch: 19 [110848/225000 (49%)] Loss: 9214.626953\n",
      "Train Epoch: 19 [114944/225000 (51%)] Loss: 9512.587891\n",
      "Train Epoch: 19 [119040/225000 (53%)] Loss: 9342.003906\n",
      "Train Epoch: 19 [123136/225000 (55%)] Loss: 9320.470703\n",
      "Train Epoch: 19 [127232/225000 (57%)] Loss: 9240.628906\n",
      "Train Epoch: 19 [131328/225000 (58%)] Loss: 9254.189453\n",
      "Train Epoch: 19 [135424/225000 (60%)] Loss: 9246.128906\n",
      "Train Epoch: 19 [139520/225000 (62%)] Loss: 9364.298828\n",
      "Train Epoch: 19 [143616/225000 (64%)] Loss: 9245.478516\n",
      "Train Epoch: 19 [147712/225000 (66%)] Loss: 9368.625000\n",
      "Train Epoch: 19 [151808/225000 (67%)] Loss: 9412.494141\n",
      "Train Epoch: 19 [155904/225000 (69%)] Loss: 9440.300781\n",
      "Train Epoch: 19 [160000/225000 (71%)] Loss: 9384.746094\n",
      "Train Epoch: 19 [164096/225000 (73%)] Loss: 9176.115234\n",
      "Train Epoch: 19 [168192/225000 (75%)] Loss: 9363.441406\n",
      "Train Epoch: 19 [172288/225000 (77%)] Loss: 9431.119141\n",
      "Train Epoch: 19 [176384/225000 (78%)] Loss: 9219.212891\n",
      "Train Epoch: 19 [180480/225000 (80%)] Loss: 9343.414062\n",
      "Train Epoch: 19 [184576/225000 (82%)] Loss: 9504.128906\n",
      "Train Epoch: 19 [188672/225000 (84%)] Loss: 9288.550781\n",
      "Train Epoch: 19 [192768/225000 (86%)] Loss: 9487.400391\n",
      "Train Epoch: 19 [196864/225000 (87%)] Loss: 9219.992188\n",
      "Train Epoch: 19 [200960/225000 (89%)] Loss: 9381.583984\n",
      "Train Epoch: 19 [205056/225000 (91%)] Loss: 9035.027344\n",
      "Train Epoch: 19 [209152/225000 (93%)] Loss: 9104.199219\n",
      "Train Epoch: 19 [213248/225000 (95%)] Loss: 9348.324219\n",
      "Train Epoch: 19 [217344/225000 (97%)] Loss: 9111.525391\n",
      "Train Epoch: 19 [221440/225000 (98%)] Loss: 9183.732422\n",
      "    epoch          : 19\n",
      "    loss           : 9616.63191703996\n",
      "    val_loss       : 9547.064058885891\n",
      "Train Epoch: 20 [256/225000 (0%)] Loss: 9283.052734\n",
      "Train Epoch: 20 [4352/225000 (2%)] Loss: 9344.912109\n",
      "Train Epoch: 20 [8448/225000 (4%)] Loss: 9019.720703\n",
      "Train Epoch: 20 [12544/225000 (6%)] Loss: 9518.613281\n",
      "Train Epoch: 20 [16640/225000 (7%)] Loss: 9249.529297\n",
      "Train Epoch: 20 [20736/225000 (9%)] Loss: 9400.841797\n",
      "Train Epoch: 20 [24832/225000 (11%)] Loss: 9538.833984\n",
      "Train Epoch: 20 [28928/225000 (13%)] Loss: 9230.894531\n",
      "Train Epoch: 20 [33024/225000 (15%)] Loss: 9115.949219\n",
      "Train Epoch: 20 [37120/225000 (16%)] Loss: 9339.218750\n",
      "Train Epoch: 20 [41216/225000 (18%)] Loss: 9413.027344\n",
      "Train Epoch: 20 [45312/225000 (20%)] Loss: 9274.710938\n",
      "Train Epoch: 20 [49408/225000 (22%)] Loss: 19129.410156\n",
      "Train Epoch: 20 [53504/225000 (24%)] Loss: 9089.123047\n",
      "Train Epoch: 20 [57600/225000 (26%)] Loss: 9243.105469\n",
      "Train Epoch: 20 [61696/225000 (27%)] Loss: 9577.585938\n",
      "Train Epoch: 20 [65792/225000 (29%)] Loss: 9230.029297\n",
      "Train Epoch: 20 [69888/225000 (31%)] Loss: 9280.947266\n",
      "Train Epoch: 20 [73984/225000 (33%)] Loss: 9449.156250\n",
      "Train Epoch: 20 [78080/225000 (35%)] Loss: 9240.738281\n",
      "Train Epoch: 20 [82176/225000 (37%)] Loss: 9202.707031\n",
      "Train Epoch: 20 [86272/225000 (38%)] Loss: 9242.269531\n",
      "Train Epoch: 20 [90368/225000 (40%)] Loss: 9142.566406\n",
      "Train Epoch: 20 [94464/225000 (42%)] Loss: 9306.763672\n",
      "Train Epoch: 20 [98560/225000 (44%)] Loss: 9468.625000\n",
      "Train Epoch: 20 [102656/225000 (46%)] Loss: 9162.072266\n",
      "Train Epoch: 20 [106752/225000 (47%)] Loss: 9386.986328\n",
      "Train Epoch: 20 [110848/225000 (49%)] Loss: 9291.644531\n",
      "Train Epoch: 20 [114944/225000 (51%)] Loss: 9081.527344\n",
      "Train Epoch: 20 [119040/225000 (53%)] Loss: 9504.701172\n",
      "Train Epoch: 20 [123136/225000 (55%)] Loss: 9258.126953\n",
      "Train Epoch: 20 [127232/225000 (57%)] Loss: 9264.263672\n",
      "Train Epoch: 20 [131328/225000 (58%)] Loss: 9097.310547\n",
      "Train Epoch: 20 [135424/225000 (60%)] Loss: 19423.773438\n",
      "Train Epoch: 20 [139520/225000 (62%)] Loss: 9363.628906\n",
      "Train Epoch: 20 [143616/225000 (64%)] Loss: 9152.527344\n",
      "Train Epoch: 20 [147712/225000 (66%)] Loss: 9312.369141\n",
      "Train Epoch: 20 [151808/225000 (67%)] Loss: 9263.718750\n",
      "Train Epoch: 20 [155904/225000 (69%)] Loss: 9460.332031\n",
      "Train Epoch: 20 [160000/225000 (71%)] Loss: 9318.355469\n",
      "Train Epoch: 20 [164096/225000 (73%)] Loss: 9317.929688\n",
      "Train Epoch: 20 [168192/225000 (75%)] Loss: 9515.330078\n",
      "Train Epoch: 20 [172288/225000 (77%)] Loss: 9444.759766\n",
      "Train Epoch: 20 [176384/225000 (78%)] Loss: 9075.201172\n",
      "Train Epoch: 20 [180480/225000 (80%)] Loss: 9391.699219\n",
      "Train Epoch: 20 [184576/225000 (82%)] Loss: 9154.261719\n",
      "Train Epoch: 20 [188672/225000 (84%)] Loss: 9406.884766\n",
      "Train Epoch: 20 [192768/225000 (86%)] Loss: 9162.982422\n",
      "Train Epoch: 20 [196864/225000 (87%)] Loss: 9070.298828\n",
      "Train Epoch: 20 [200960/225000 (89%)] Loss: 9111.123047\n",
      "Train Epoch: 20 [205056/225000 (91%)] Loss: 9371.214844\n",
      "Train Epoch: 20 [209152/225000 (93%)] Loss: 9327.628906\n",
      "Train Epoch: 20 [213248/225000 (95%)] Loss: 9232.371094\n",
      "Train Epoch: 20 [217344/225000 (97%)] Loss: 9255.748047\n",
      "Train Epoch: 20 [221440/225000 (98%)] Loss: 9126.611328\n",
      "    epoch          : 20\n",
      "    loss           : 9551.00143651344\n",
      "    val_loss       : 9436.54021510786\n",
      "Train Epoch: 21 [256/225000 (0%)] Loss: 9137.507812\n",
      "Train Epoch: 21 [4352/225000 (2%)] Loss: 9398.865234\n",
      "Train Epoch: 21 [8448/225000 (4%)] Loss: 9281.027344\n",
      "Train Epoch: 21 [12544/225000 (6%)] Loss: 9307.080078\n",
      "Train Epoch: 21 [16640/225000 (7%)] Loss: 9292.244141\n",
      "Train Epoch: 21 [20736/225000 (9%)] Loss: 9255.496094\n",
      "Train Epoch: 21 [24832/225000 (11%)] Loss: 9248.638672\n",
      "Train Epoch: 21 [28928/225000 (13%)] Loss: 9435.904297\n",
      "Train Epoch: 21 [33024/225000 (15%)] Loss: 9096.705078\n",
      "Train Epoch: 21 [37120/225000 (16%)] Loss: 9126.824219\n",
      "Train Epoch: 21 [41216/225000 (18%)] Loss: 31773.316406\n",
      "Train Epoch: 21 [45312/225000 (20%)] Loss: 9274.832031\n",
      "Train Epoch: 21 [49408/225000 (22%)] Loss: 9349.101562\n",
      "Train Epoch: 21 [53504/225000 (24%)] Loss: 8931.359375\n",
      "Train Epoch: 21 [57600/225000 (26%)] Loss: 9334.892578\n",
      "Train Epoch: 21 [61696/225000 (27%)] Loss: 9221.513672\n",
      "Train Epoch: 21 [65792/225000 (29%)] Loss: 9324.501953\n",
      "Train Epoch: 21 [69888/225000 (31%)] Loss: 9350.201172\n",
      "Train Epoch: 21 [73984/225000 (33%)] Loss: 9244.820312\n",
      "Train Epoch: 21 [78080/225000 (35%)] Loss: 9344.082031\n",
      "Train Epoch: 21 [82176/225000 (37%)] Loss: 9185.332031\n",
      "Train Epoch: 21 [86272/225000 (38%)] Loss: 9433.316406\n",
      "Train Epoch: 21 [90368/225000 (40%)] Loss: 18816.326172\n",
      "Train Epoch: 21 [94464/225000 (42%)] Loss: 9307.398438\n",
      "Train Epoch: 21 [98560/225000 (44%)] Loss: 9399.500000\n",
      "Train Epoch: 21 [102656/225000 (46%)] Loss: 9198.455078\n",
      "Train Epoch: 21 [106752/225000 (47%)] Loss: 9184.312500\n",
      "Train Epoch: 21 [110848/225000 (49%)] Loss: 9180.623047\n",
      "Train Epoch: 21 [114944/225000 (51%)] Loss: 9414.357422\n",
      "Train Epoch: 21 [119040/225000 (53%)] Loss: 9298.048828\n",
      "Train Epoch: 21 [123136/225000 (55%)] Loss: 9162.468750\n",
      "Train Epoch: 21 [127232/225000 (57%)] Loss: 9235.707031\n",
      "Train Epoch: 21 [131328/225000 (58%)] Loss: 9233.896484\n",
      "Train Epoch: 21 [135424/225000 (60%)] Loss: 9284.226562\n",
      "Train Epoch: 21 [139520/225000 (62%)] Loss: 9186.337891\n",
      "Train Epoch: 21 [143616/225000 (64%)] Loss: 9281.771484\n",
      "Train Epoch: 21 [147712/225000 (66%)] Loss: 9326.933594\n",
      "Train Epoch: 21 [151808/225000 (67%)] Loss: 9276.423828\n",
      "Train Epoch: 21 [155904/225000 (69%)] Loss: 9209.816406\n",
      "Train Epoch: 21 [160000/225000 (71%)] Loss: 9310.070312\n",
      "Train Epoch: 21 [164096/225000 (73%)] Loss: 8987.355469\n",
      "Train Epoch: 21 [168192/225000 (75%)] Loss: 9138.783203\n",
      "Train Epoch: 21 [172288/225000 (77%)] Loss: 8961.037109\n",
      "Train Epoch: 21 [176384/225000 (78%)] Loss: 9086.595703\n",
      "Train Epoch: 21 [180480/225000 (80%)] Loss: 9161.337891\n",
      "Train Epoch: 21 [184576/225000 (82%)] Loss: 9219.941406\n",
      "Train Epoch: 21 [188672/225000 (84%)] Loss: 9276.576172\n",
      "Train Epoch: 21 [192768/225000 (86%)] Loss: 8999.269531\n",
      "Train Epoch: 21 [196864/225000 (87%)] Loss: 9203.623047\n",
      "Train Epoch: 21 [200960/225000 (89%)] Loss: 9552.349609\n",
      "Train Epoch: 21 [205056/225000 (91%)] Loss: 9127.675781\n",
      "Train Epoch: 21 [209152/225000 (93%)] Loss: 9127.531250\n",
      "Train Epoch: 21 [213248/225000 (95%)] Loss: 9189.322266\n",
      "Train Epoch: 21 [217344/225000 (97%)] Loss: 9013.220703\n",
      "Train Epoch: 21 [221440/225000 (98%)] Loss: 9215.490234\n",
      "    epoch          : 21\n",
      "    loss           : 9547.351480286547\n",
      "    val_loss       : 9237.305744914376\n",
      "Train Epoch: 22 [256/225000 (0%)] Loss: 9275.998047\n",
      "Train Epoch: 22 [4352/225000 (2%)] Loss: 9268.744141\n",
      "Train Epoch: 22 [8448/225000 (4%)] Loss: 9132.791016\n",
      "Train Epoch: 22 [12544/225000 (6%)] Loss: 9272.115234\n",
      "Train Epoch: 22 [16640/225000 (7%)] Loss: 9449.527344\n",
      "Train Epoch: 22 [20736/225000 (9%)] Loss: 9082.773438\n",
      "Train Epoch: 22 [24832/225000 (11%)] Loss: 9283.601562\n",
      "Train Epoch: 22 [28928/225000 (13%)] Loss: 9053.232422\n",
      "Train Epoch: 22 [33024/225000 (15%)] Loss: 9119.457031\n",
      "Train Epoch: 22 [37120/225000 (16%)] Loss: 9168.357422\n",
      "Train Epoch: 22 [41216/225000 (18%)] Loss: 9064.123047\n",
      "Train Epoch: 22 [45312/225000 (20%)] Loss: 9411.388672\n",
      "Train Epoch: 22 [49408/225000 (22%)] Loss: 9027.925781\n",
      "Train Epoch: 22 [53504/225000 (24%)] Loss: 13996.179688\n",
      "Train Epoch: 22 [57600/225000 (26%)] Loss: 9002.011719\n",
      "Train Epoch: 22 [61696/225000 (27%)] Loss: 9098.835938\n",
      "Train Epoch: 22 [65792/225000 (29%)] Loss: 9065.253906\n",
      "Train Epoch: 22 [69888/225000 (31%)] Loss: 9208.744141\n",
      "Train Epoch: 22 [73984/225000 (33%)] Loss: 9013.742188\n",
      "Train Epoch: 22 [78080/225000 (35%)] Loss: 9133.990234\n",
      "Train Epoch: 22 [82176/225000 (37%)] Loss: 9269.056641\n",
      "Train Epoch: 22 [86272/225000 (38%)] Loss: 9272.607422\n",
      "Train Epoch: 22 [90368/225000 (40%)] Loss: 9147.304688\n",
      "Train Epoch: 22 [94464/225000 (42%)] Loss: 9284.099609\n",
      "Train Epoch: 22 [98560/225000 (44%)] Loss: 9285.371094\n",
      "Train Epoch: 22 [102656/225000 (46%)] Loss: 9137.207031\n",
      "Train Epoch: 22 [106752/225000 (47%)] Loss: 9134.707031\n",
      "Train Epoch: 22 [110848/225000 (49%)] Loss: 9328.140625\n",
      "Train Epoch: 22 [114944/225000 (51%)] Loss: 9093.367188\n",
      "Train Epoch: 22 [119040/225000 (53%)] Loss: 9115.160156\n",
      "Train Epoch: 22 [123136/225000 (55%)] Loss: 9236.296875\n",
      "Train Epoch: 22 [127232/225000 (57%)] Loss: 9151.654297\n",
      "Train Epoch: 22 [131328/225000 (58%)] Loss: 9115.007812\n",
      "Train Epoch: 22 [135424/225000 (60%)] Loss: 9159.437500\n",
      "Train Epoch: 22 [139520/225000 (62%)] Loss: 9434.519531\n",
      "Train Epoch: 22 [143616/225000 (64%)] Loss: 9217.179688\n",
      "Train Epoch: 22 [147712/225000 (66%)] Loss: 9145.009766\n",
      "Train Epoch: 22 [151808/225000 (67%)] Loss: 9173.707031\n",
      "Train Epoch: 22 [155904/225000 (69%)] Loss: 8997.759766\n",
      "Train Epoch: 22 [160000/225000 (71%)] Loss: 9211.562500\n",
      "Train Epoch: 22 [164096/225000 (73%)] Loss: 9235.601562\n",
      "Train Epoch: 22 [168192/225000 (75%)] Loss: 9149.537109\n",
      "Train Epoch: 22 [172288/225000 (77%)] Loss: 9241.789062\n",
      "Train Epoch: 22 [176384/225000 (78%)] Loss: 9095.710938\n",
      "Train Epoch: 22 [180480/225000 (80%)] Loss: 9271.250000\n",
      "Train Epoch: 22 [184576/225000 (82%)] Loss: 9088.591797\n",
      "Train Epoch: 22 [188672/225000 (84%)] Loss: 9068.425781\n",
      "Train Epoch: 22 [192768/225000 (86%)] Loss: 9133.814453\n",
      "Train Epoch: 22 [196864/225000 (87%)] Loss: 9164.841797\n",
      "Train Epoch: 22 [200960/225000 (89%)] Loss: 9218.371094\n",
      "Train Epoch: 22 [205056/225000 (91%)] Loss: 9124.343750\n",
      "Train Epoch: 22 [209152/225000 (93%)] Loss: 9082.705078\n",
      "Train Epoch: 22 [213248/225000 (95%)] Loss: 9204.927734\n",
      "Train Epoch: 22 [217344/225000 (97%)] Loss: 9083.451172\n",
      "Train Epoch: 22 [221440/225000 (98%)] Loss: 9071.867188\n",
      "    epoch          : 22\n",
      "    loss           : 9324.002363081272\n",
      "    val_loss       : 9089.498845394777\n",
      "Train Epoch: 23 [256/225000 (0%)] Loss: 9279.353516\n",
      "Train Epoch: 23 [4352/225000 (2%)] Loss: 8888.091797\n",
      "Train Epoch: 23 [8448/225000 (4%)] Loss: 9317.656250\n",
      "Train Epoch: 23 [12544/225000 (6%)] Loss: 9124.976562\n",
      "Train Epoch: 23 [16640/225000 (7%)] Loss: 8918.652344\n",
      "Train Epoch: 23 [20736/225000 (9%)] Loss: 8974.507812\n",
      "Train Epoch: 23 [24832/225000 (11%)] Loss: 9151.857422\n",
      "Train Epoch: 23 [28928/225000 (13%)] Loss: 9084.945312\n",
      "Train Epoch: 23 [33024/225000 (15%)] Loss: 9129.958984\n",
      "Train Epoch: 23 [37120/225000 (16%)] Loss: 9266.384766\n",
      "Train Epoch: 23 [41216/225000 (18%)] Loss: 8981.914062\n",
      "Train Epoch: 23 [45312/225000 (20%)] Loss: 8907.388672\n",
      "Train Epoch: 23 [49408/225000 (22%)] Loss: 8909.802734\n",
      "Train Epoch: 23 [53504/225000 (24%)] Loss: 9245.712891\n",
      "Train Epoch: 23 [57600/225000 (26%)] Loss: 9109.740234\n",
      "Train Epoch: 23 [61696/225000 (27%)] Loss: 9331.398438\n",
      "Train Epoch: 23 [65792/225000 (29%)] Loss: 9230.681641\n",
      "Train Epoch: 23 [69888/225000 (31%)] Loss: 8931.173828\n",
      "Train Epoch: 23 [73984/225000 (33%)] Loss: 9260.599609\n",
      "Train Epoch: 23 [78080/225000 (35%)] Loss: 9192.408203\n",
      "Train Epoch: 23 [82176/225000 (37%)] Loss: 9118.339844\n",
      "Train Epoch: 23 [86272/225000 (38%)] Loss: 9248.234375\n",
      "Train Epoch: 23 [90368/225000 (40%)] Loss: 9044.078125\n",
      "Train Epoch: 23 [94464/225000 (42%)] Loss: 9256.662109\n",
      "Train Epoch: 23 [98560/225000 (44%)] Loss: 9120.046875\n",
      "Train Epoch: 23 [102656/225000 (46%)] Loss: 9078.841797\n",
      "Train Epoch: 23 [106752/225000 (47%)] Loss: 8997.837891\n",
      "Train Epoch: 23 [110848/225000 (49%)] Loss: 9108.119141\n",
      "Train Epoch: 23 [114944/225000 (51%)] Loss: 9152.009766\n",
      "Train Epoch: 23 [119040/225000 (53%)] Loss: 9243.339844\n",
      "Train Epoch: 23 [123136/225000 (55%)] Loss: 8993.667969\n",
      "Train Epoch: 23 [127232/225000 (57%)] Loss: 9183.648438\n",
      "Train Epoch: 23 [131328/225000 (58%)] Loss: 9039.726562\n",
      "Train Epoch: 23 [135424/225000 (60%)] Loss: 8976.488281\n",
      "Train Epoch: 23 [139520/225000 (62%)] Loss: 9000.931641\n",
      "Train Epoch: 23 [143616/225000 (64%)] Loss: 9053.724609\n",
      "Train Epoch: 23 [147712/225000 (66%)] Loss: 9195.208984\n",
      "Train Epoch: 23 [151808/225000 (67%)] Loss: 9306.716797\n",
      "Train Epoch: 23 [155904/225000 (69%)] Loss: 9066.396484\n",
      "Train Epoch: 23 [160000/225000 (71%)] Loss: 8999.154297\n",
      "Train Epoch: 23 [164096/225000 (73%)] Loss: 8918.251953\n",
      "Train Epoch: 23 [168192/225000 (75%)] Loss: 9129.121094\n",
      "Train Epoch: 23 [172288/225000 (77%)] Loss: 8922.320312\n",
      "Train Epoch: 23 [176384/225000 (78%)] Loss: 9086.947266\n",
      "Train Epoch: 23 [180480/225000 (80%)] Loss: 9107.646484\n",
      "Train Epoch: 23 [184576/225000 (82%)] Loss: 8897.160156\n",
      "Train Epoch: 23 [188672/225000 (84%)] Loss: 9023.630859\n",
      "Train Epoch: 23 [192768/225000 (86%)] Loss: 9142.455078\n",
      "Train Epoch: 23 [196864/225000 (87%)] Loss: 9349.105469\n",
      "Train Epoch: 23 [200960/225000 (89%)] Loss: 9041.533203\n",
      "Train Epoch: 23 [205056/225000 (91%)] Loss: 9197.751953\n",
      "Train Epoch: 23 [209152/225000 (93%)] Loss: 9138.501953\n",
      "Train Epoch: 23 [213248/225000 (95%)] Loss: 9018.103516\n",
      "Train Epoch: 23 [217344/225000 (97%)] Loss: 9094.599609\n",
      "Train Epoch: 23 [221440/225000 (98%)] Loss: 9170.921875\n",
      "    epoch          : 23\n",
      "    loss           : 9189.258269117961\n",
      "    val_loss       : 9040.029687546954\n",
      "Train Epoch: 24 [256/225000 (0%)] Loss: 9190.912109\n",
      "Train Epoch: 24 [4352/225000 (2%)] Loss: 8890.529297\n",
      "Train Epoch: 24 [8448/225000 (4%)] Loss: 9183.873047\n",
      "Train Epoch: 24 [12544/225000 (6%)] Loss: 9193.337891\n",
      "Train Epoch: 24 [16640/225000 (7%)] Loss: 8838.093750\n",
      "Train Epoch: 24 [20736/225000 (9%)] Loss: 9142.865234\n",
      "Train Epoch: 24 [24832/225000 (11%)] Loss: 9091.521484\n",
      "Train Epoch: 24 [28928/225000 (13%)] Loss: 9222.949219\n",
      "Train Epoch: 24 [33024/225000 (15%)] Loss: 8992.773438\n",
      "Train Epoch: 24 [37120/225000 (16%)] Loss: 8838.990234\n",
      "Train Epoch: 24 [41216/225000 (18%)] Loss: 9190.546875\n",
      "Train Epoch: 24 [45312/225000 (20%)] Loss: 8913.582031\n",
      "Train Epoch: 24 [49408/225000 (22%)] Loss: 9054.287109\n",
      "Train Epoch: 24 [53504/225000 (24%)] Loss: 9199.441406\n",
      "Train Epoch: 24 [57600/225000 (26%)] Loss: 9069.564453\n",
      "Train Epoch: 24 [61696/225000 (27%)] Loss: 9162.365234\n",
      "Train Epoch: 24 [65792/225000 (29%)] Loss: 9183.119141\n",
      "Train Epoch: 24 [69888/225000 (31%)] Loss: 9077.509766\n",
      "Train Epoch: 24 [73984/225000 (33%)] Loss: 8903.613281\n",
      "Train Epoch: 24 [78080/225000 (35%)] Loss: 8789.605469\n",
      "Train Epoch: 24 [82176/225000 (37%)] Loss: 8957.236328\n",
      "Train Epoch: 24 [86272/225000 (38%)] Loss: 9061.578125\n",
      "Train Epoch: 24 [90368/225000 (40%)] Loss: 9031.964844\n",
      "Train Epoch: 24 [94464/225000 (42%)] Loss: 9004.435547\n",
      "Train Epoch: 24 [98560/225000 (44%)] Loss: 9288.902344\n",
      "Train Epoch: 24 [102656/225000 (46%)] Loss: 9157.669922\n",
      "Train Epoch: 24 [106752/225000 (47%)] Loss: 8857.044922\n",
      "Train Epoch: 24 [110848/225000 (49%)] Loss: 8810.294922\n",
      "Train Epoch: 24 [114944/225000 (51%)] Loss: 9064.992188\n",
      "Train Epoch: 24 [119040/225000 (53%)] Loss: 8951.144531\n",
      "Train Epoch: 24 [123136/225000 (55%)] Loss: 9038.117188\n",
      "Train Epoch: 24 [127232/225000 (57%)] Loss: 8953.390625\n",
      "Train Epoch: 24 [131328/225000 (58%)] Loss: 9247.277344\n",
      "Train Epoch: 24 [135424/225000 (60%)] Loss: 8789.560547\n",
      "Train Epoch: 24 [139520/225000 (62%)] Loss: 9081.738281\n",
      "Train Epoch: 24 [143616/225000 (64%)] Loss: 8883.820312\n",
      "Train Epoch: 24 [147712/225000 (66%)] Loss: 8920.048828\n",
      "Train Epoch: 24 [151808/225000 (67%)] Loss: 9110.277344\n",
      "Train Epoch: 24 [155904/225000 (69%)] Loss: 8994.265625\n",
      "Train Epoch: 24 [160000/225000 (71%)] Loss: 18688.980469\n",
      "Train Epoch: 24 [164096/225000 (73%)] Loss: 9115.123047\n",
      "Train Epoch: 24 [168192/225000 (75%)] Loss: 9057.947266\n",
      "Train Epoch: 24 [172288/225000 (77%)] Loss: 9067.750000\n",
      "Train Epoch: 24 [176384/225000 (78%)] Loss: 8980.708984\n",
      "Train Epoch: 24 [180480/225000 (80%)] Loss: 8944.050781\n",
      "Train Epoch: 24 [184576/225000 (82%)] Loss: 9056.314453\n",
      "Train Epoch: 24 [188672/225000 (84%)] Loss: 9045.205078\n",
      "Train Epoch: 24 [192768/225000 (86%)] Loss: 8998.414062\n",
      "Train Epoch: 24 [196864/225000 (87%)] Loss: 9151.281250\n",
      "Train Epoch: 24 [200960/225000 (89%)] Loss: 8967.509766\n",
      "Train Epoch: 24 [205056/225000 (91%)] Loss: 9156.564453\n",
      "Train Epoch: 24 [209152/225000 (93%)] Loss: 9213.197266\n",
      "Train Epoch: 24 [213248/225000 (95%)] Loss: 9084.496094\n",
      "Train Epoch: 24 [217344/225000 (97%)] Loss: 9247.220703\n",
      "Train Epoch: 24 [221440/225000 (98%)] Loss: 8901.603516\n",
      "    epoch          : 24\n",
      "    loss           : 9330.669384154578\n",
      "    val_loss       : 9128.51290684087\n",
      "Train Epoch: 25 [256/225000 (0%)] Loss: 8905.876953\n",
      "Train Epoch: 25 [4352/225000 (2%)] Loss: 9163.746094\n",
      "Train Epoch: 25 [8448/225000 (4%)] Loss: 8873.794922\n",
      "Train Epoch: 25 [12544/225000 (6%)] Loss: 9017.322266\n",
      "Train Epoch: 25 [16640/225000 (7%)] Loss: 8825.712891\n",
      "Train Epoch: 25 [20736/225000 (9%)] Loss: 9018.792969\n",
      "Train Epoch: 25 [24832/225000 (11%)] Loss: 9150.265625\n",
      "Train Epoch: 25 [28928/225000 (13%)] Loss: 9059.953125\n",
      "Train Epoch: 25 [33024/225000 (15%)] Loss: 8859.880859\n",
      "Train Epoch: 25 [37120/225000 (16%)] Loss: 9194.996094\n",
      "Train Epoch: 25 [41216/225000 (18%)] Loss: 8981.562500\n",
      "Train Epoch: 25 [45312/225000 (20%)] Loss: 8921.080078\n",
      "Train Epoch: 25 [49408/225000 (22%)] Loss: 8984.441406\n",
      "Train Epoch: 25 [53504/225000 (24%)] Loss: 9114.558594\n",
      "Train Epoch: 25 [57600/225000 (26%)] Loss: 9166.804688\n",
      "Train Epoch: 25 [61696/225000 (27%)] Loss: 9206.701172\n",
      "Train Epoch: 25 [65792/225000 (29%)] Loss: 8913.005859\n",
      "Train Epoch: 25 [69888/225000 (31%)] Loss: 9183.218750\n",
      "Train Epoch: 25 [73984/225000 (33%)] Loss: 9026.431641\n",
      "Train Epoch: 25 [78080/225000 (35%)] Loss: 9038.314453\n",
      "Train Epoch: 25 [82176/225000 (37%)] Loss: 9139.777344\n",
      "Train Epoch: 25 [86272/225000 (38%)] Loss: 8992.917969\n",
      "Train Epoch: 25 [90368/225000 (40%)] Loss: 9077.435547\n",
      "Train Epoch: 25 [94464/225000 (42%)] Loss: 8910.964844\n",
      "Train Epoch: 25 [98560/225000 (44%)] Loss: 8871.212891\n",
      "Train Epoch: 25 [102656/225000 (46%)] Loss: 9155.794922\n",
      "Train Epoch: 25 [106752/225000 (47%)] Loss: 9114.556641\n",
      "Train Epoch: 25 [110848/225000 (49%)] Loss: 8871.902344\n",
      "Train Epoch: 25 [114944/225000 (51%)] Loss: 9017.417969\n",
      "Train Epoch: 25 [119040/225000 (53%)] Loss: 8822.882812\n",
      "Train Epoch: 25 [123136/225000 (55%)] Loss: 8997.285156\n",
      "Train Epoch: 25 [127232/225000 (57%)] Loss: 9027.312500\n",
      "Train Epoch: 25 [131328/225000 (58%)] Loss: 8844.306641\n",
      "Train Epoch: 25 [135424/225000 (60%)] Loss: 9215.335938\n",
      "Train Epoch: 25 [139520/225000 (62%)] Loss: 8920.802734\n",
      "Train Epoch: 25 [143616/225000 (64%)] Loss: 8978.599609\n",
      "Train Epoch: 25 [147712/225000 (66%)] Loss: 8984.246094\n",
      "Train Epoch: 25 [151808/225000 (67%)] Loss: 9070.251953\n",
      "Train Epoch: 25 [155904/225000 (69%)] Loss: 8989.978516\n",
      "Train Epoch: 25 [160000/225000 (71%)] Loss: 8879.994141\n",
      "Train Epoch: 25 [164096/225000 (73%)] Loss: 13858.363281\n",
      "Train Epoch: 25 [168192/225000 (75%)] Loss: 9083.945312\n",
      "Train Epoch: 25 [172288/225000 (77%)] Loss: 8840.167969\n",
      "Train Epoch: 25 [176384/225000 (78%)] Loss: 8924.156250\n",
      "Train Epoch: 25 [180480/225000 (80%)] Loss: 8894.687500\n",
      "Train Epoch: 25 [184576/225000 (82%)] Loss: 8926.841797\n",
      "Train Epoch: 25 [188672/225000 (84%)] Loss: 8964.294922\n",
      "Train Epoch: 25 [192768/225000 (86%)] Loss: 8966.564453\n",
      "Train Epoch: 25 [196864/225000 (87%)] Loss: 8957.898438\n",
      "Train Epoch: 25 [200960/225000 (89%)] Loss: 8750.574219\n",
      "Train Epoch: 25 [205056/225000 (91%)] Loss: 9052.656250\n",
      "Train Epoch: 25 [209152/225000 (93%)] Loss: 8844.054688\n",
      "Train Epoch: 25 [213248/225000 (95%)] Loss: 9020.861328\n",
      "Train Epoch: 25 [217344/225000 (97%)] Loss: 8971.771484\n",
      "Train Epoch: 25 [221440/225000 (98%)] Loss: 8873.683594\n",
      "    epoch          : 25\n",
      "    loss           : 9015.164899077432\n",
      "    val_loss       : 9173.504150590117\n",
      "Train Epoch: 26 [256/225000 (0%)] Loss: 8816.935547\n",
      "Train Epoch: 26 [4352/225000 (2%)] Loss: 8910.587891\n",
      "Train Epoch: 26 [8448/225000 (4%)] Loss: 9026.255859\n",
      "Train Epoch: 26 [12544/225000 (6%)] Loss: 8768.269531\n",
      "Train Epoch: 26 [16640/225000 (7%)] Loss: 8770.273438\n",
      "Train Epoch: 26 [20736/225000 (9%)] Loss: 8916.863281\n",
      "Train Epoch: 26 [24832/225000 (11%)] Loss: 8685.011719\n",
      "Train Epoch: 26 [28928/225000 (13%)] Loss: 8995.548828\n",
      "Train Epoch: 26 [33024/225000 (15%)] Loss: 9079.126953\n",
      "Train Epoch: 26 [37120/225000 (16%)] Loss: 8939.572266\n",
      "Train Epoch: 26 [41216/225000 (18%)] Loss: 9120.648438\n",
      "Train Epoch: 26 [45312/225000 (20%)] Loss: 13885.933594\n",
      "Train Epoch: 26 [49408/225000 (22%)] Loss: 8969.789062\n",
      "Train Epoch: 26 [53504/225000 (24%)] Loss: 8999.146484\n",
      "Train Epoch: 26 [57600/225000 (26%)] Loss: 8916.777344\n",
      "Train Epoch: 26 [61696/225000 (27%)] Loss: 8998.025391\n",
      "Train Epoch: 26 [65792/225000 (29%)] Loss: 9116.707031\n",
      "Train Epoch: 26 [69888/225000 (31%)] Loss: 8850.720703\n",
      "Train Epoch: 26 [73984/225000 (33%)] Loss: 8895.740234\n",
      "Train Epoch: 26 [78080/225000 (35%)] Loss: 9192.281250\n",
      "Train Epoch: 26 [82176/225000 (37%)] Loss: 8919.777344\n",
      "Train Epoch: 26 [86272/225000 (38%)] Loss: 8808.421875\n",
      "Train Epoch: 26 [90368/225000 (40%)] Loss: 9266.841797\n",
      "Train Epoch: 26 [94464/225000 (42%)] Loss: 8836.113281\n",
      "Train Epoch: 26 [98560/225000 (44%)] Loss: 8790.652344\n",
      "Train Epoch: 26 [102656/225000 (46%)] Loss: 8873.181641\n",
      "Train Epoch: 26 [106752/225000 (47%)] Loss: 8951.830078\n",
      "Train Epoch: 26 [110848/225000 (49%)] Loss: 9144.171875\n",
      "Train Epoch: 26 [114944/225000 (51%)] Loss: 8982.802734\n",
      "Train Epoch: 26 [119040/225000 (53%)] Loss: 8841.605469\n",
      "Train Epoch: 26 [123136/225000 (55%)] Loss: 8791.951172\n",
      "Train Epoch: 26 [127232/225000 (57%)] Loss: 8924.144531\n",
      "Train Epoch: 26 [131328/225000 (58%)] Loss: 8966.818359\n",
      "Train Epoch: 26 [135424/225000 (60%)] Loss: 8844.066406\n",
      "Train Epoch: 26 [139520/225000 (62%)] Loss: 8990.513672\n",
      "Train Epoch: 26 [143616/225000 (64%)] Loss: 9027.591797\n",
      "Train Epoch: 26 [147712/225000 (66%)] Loss: 8881.158203\n",
      "Train Epoch: 26 [151808/225000 (67%)] Loss: 8901.763672\n",
      "Train Epoch: 26 [155904/225000 (69%)] Loss: 8741.718750\n",
      "Train Epoch: 26 [160000/225000 (71%)] Loss: 8888.658203\n",
      "Train Epoch: 26 [164096/225000 (73%)] Loss: 8947.130859\n",
      "Train Epoch: 26 [168192/225000 (75%)] Loss: 13894.248047\n",
      "Train Epoch: 26 [172288/225000 (77%)] Loss: 8944.986328\n",
      "Train Epoch: 26 [176384/225000 (78%)] Loss: 8913.437500\n",
      "Train Epoch: 26 [180480/225000 (80%)] Loss: 9066.910156\n",
      "Train Epoch: 26 [184576/225000 (82%)] Loss: 8913.099609\n",
      "Train Epoch: 26 [188672/225000 (84%)] Loss: 9055.642578\n",
      "Train Epoch: 26 [192768/225000 (86%)] Loss: 8862.761719\n",
      "Train Epoch: 26 [196864/225000 (87%)] Loss: 9152.689453\n",
      "Train Epoch: 26 [200960/225000 (89%)] Loss: 9004.992188\n",
      "Train Epoch: 26 [205056/225000 (91%)] Loss: 8963.484375\n",
      "Train Epoch: 26 [209152/225000 (93%)] Loss: 8858.111328\n",
      "Train Epoch: 26 [213248/225000 (95%)] Loss: 8919.367188\n",
      "Train Epoch: 26 [217344/225000 (97%)] Loss: 9036.562500\n",
      "Train Epoch: 26 [221440/225000 (98%)] Loss: 9041.628906\n",
      "    epoch          : 26\n",
      "    loss           : 9104.758792395478\n",
      "    val_loss       : 8928.530603891733\n",
      "Train Epoch: 27 [256/225000 (0%)] Loss: 9012.292969\n",
      "Train Epoch: 27 [4352/225000 (2%)] Loss: 8979.123047\n",
      "Train Epoch: 27 [8448/225000 (4%)] Loss: 9041.210938\n",
      "Train Epoch: 27 [12544/225000 (6%)] Loss: 9009.046875\n",
      "Train Epoch: 27 [16640/225000 (7%)] Loss: 9032.142578\n",
      "Train Epoch: 27 [20736/225000 (9%)] Loss: 8938.982422\n",
      "Train Epoch: 27 [24832/225000 (11%)] Loss: 9061.060547\n",
      "Train Epoch: 27 [28928/225000 (13%)] Loss: 9055.345703\n",
      "Train Epoch: 27 [33024/225000 (15%)] Loss: 8849.226562\n",
      "Train Epoch: 27 [37120/225000 (16%)] Loss: 8937.501953\n",
      "Train Epoch: 27 [41216/225000 (18%)] Loss: 8896.240234\n",
      "Train Epoch: 27 [45312/225000 (20%)] Loss: 9048.666016\n",
      "Train Epoch: 27 [49408/225000 (22%)] Loss: 9087.328125\n",
      "Train Epoch: 27 [53504/225000 (24%)] Loss: 8974.861328\n",
      "Train Epoch: 27 [57600/225000 (26%)] Loss: 8708.048828\n",
      "Train Epoch: 27 [61696/225000 (27%)] Loss: 8824.947266\n",
      "Train Epoch: 27 [65792/225000 (29%)] Loss: 8871.179688\n",
      "Train Epoch: 27 [69888/225000 (31%)] Loss: 9073.906250\n",
      "Train Epoch: 27 [73984/225000 (33%)] Loss: 8722.216797\n",
      "Train Epoch: 27 [78080/225000 (35%)] Loss: 8794.621094\n",
      "Train Epoch: 27 [82176/225000 (37%)] Loss: 8859.041016\n",
      "Train Epoch: 27 [86272/225000 (38%)] Loss: 8790.447266\n",
      "Train Epoch: 27 [90368/225000 (40%)] Loss: 9082.867188\n",
      "Train Epoch: 27 [94464/225000 (42%)] Loss: 9045.837891\n",
      "Train Epoch: 27 [98560/225000 (44%)] Loss: 9022.634766\n",
      "Train Epoch: 27 [102656/225000 (46%)] Loss: 8886.859375\n",
      "Train Epoch: 27 [106752/225000 (47%)] Loss: 8999.003906\n",
      "Train Epoch: 27 [110848/225000 (49%)] Loss: 8968.699219\n",
      "Train Epoch: 27 [114944/225000 (51%)] Loss: 8879.863281\n",
      "Train Epoch: 27 [119040/225000 (53%)] Loss: 8726.707031\n",
      "Train Epoch: 27 [123136/225000 (55%)] Loss: 8928.396484\n",
      "Train Epoch: 27 [127232/225000 (57%)] Loss: 8903.306641\n",
      "Train Epoch: 27 [131328/225000 (58%)] Loss: 8908.322266\n",
      "Train Epoch: 27 [135424/225000 (60%)] Loss: 8795.556641\n",
      "Train Epoch: 27 [139520/225000 (62%)] Loss: 8952.933594\n",
      "Train Epoch: 27 [143616/225000 (64%)] Loss: 8996.142578\n",
      "Train Epoch: 27 [147712/225000 (66%)] Loss: 8960.060547\n",
      "Train Epoch: 27 [151808/225000 (67%)] Loss: 8916.011719\n",
      "Train Epoch: 27 [155904/225000 (69%)] Loss: 9011.927734\n",
      "Train Epoch: 27 [160000/225000 (71%)] Loss: 8875.349609\n",
      "Train Epoch: 27 [164096/225000 (73%)] Loss: 9098.056641\n",
      "Train Epoch: 27 [168192/225000 (75%)] Loss: 8857.802734\n",
      "Train Epoch: 27 [172288/225000 (77%)] Loss: 8820.689453\n",
      "Train Epoch: 27 [176384/225000 (78%)] Loss: 8763.927734\n",
      "Train Epoch: 27 [180480/225000 (80%)] Loss: 8946.980469\n",
      "Train Epoch: 27 [184576/225000 (82%)] Loss: 8751.128906\n",
      "Train Epoch: 27 [188672/225000 (84%)] Loss: 8795.716797\n",
      "Train Epoch: 27 [192768/225000 (86%)] Loss: 8773.130859\n",
      "Train Epoch: 27 [196864/225000 (87%)] Loss: 8827.570312\n",
      "Train Epoch: 27 [200960/225000 (89%)] Loss: 8930.318359\n",
      "Train Epoch: 27 [205056/225000 (91%)] Loss: 9011.697266\n",
      "Train Epoch: 27 [209152/225000 (93%)] Loss: 8856.031250\n",
      "Train Epoch: 27 [213248/225000 (95%)] Loss: 8961.783203\n",
      "Train Epoch: 27 [217344/225000 (97%)] Loss: 8872.804688\n",
      "Train Epoch: 27 [221440/225000 (98%)] Loss: 9014.335938\n",
      "    epoch          : 27\n",
      "    loss           : 8949.214965959187\n",
      "    val_loss       : 9104.675807747306\n",
      "Train Epoch: 28 [256/225000 (0%)] Loss: 8856.847656\n",
      "Train Epoch: 28 [4352/225000 (2%)] Loss: 8686.423828\n",
      "Train Epoch: 28 [8448/225000 (4%)] Loss: 8939.332031\n",
      "Train Epoch: 28 [12544/225000 (6%)] Loss: 9110.572266\n",
      "Train Epoch: 28 [16640/225000 (7%)] Loss: 8892.125000\n",
      "Train Epoch: 28 [20736/225000 (9%)] Loss: 8842.710938\n",
      "Train Epoch: 28 [24832/225000 (11%)] Loss: 8780.595703\n",
      "Train Epoch: 28 [28928/225000 (13%)] Loss: 8937.910156\n",
      "Train Epoch: 28 [33024/225000 (15%)] Loss: 9132.386719\n",
      "Train Epoch: 28 [37120/225000 (16%)] Loss: 9129.929688\n",
      "Train Epoch: 28 [41216/225000 (18%)] Loss: 8937.302734\n",
      "Train Epoch: 28 [45312/225000 (20%)] Loss: 8740.582031\n",
      "Train Epoch: 28 [49408/225000 (22%)] Loss: 9040.001953\n",
      "Train Epoch: 28 [53504/225000 (24%)] Loss: 8953.232422\n",
      "Train Epoch: 28 [57600/225000 (26%)] Loss: 8776.292969\n",
      "Train Epoch: 28 [61696/225000 (27%)] Loss: 8971.062500\n",
      "Train Epoch: 28 [65792/225000 (29%)] Loss: 8920.626953\n",
      "Train Epoch: 28 [69888/225000 (31%)] Loss: 8612.000000\n",
      "Train Epoch: 28 [73984/225000 (33%)] Loss: 8885.574219\n",
      "Train Epoch: 28 [78080/225000 (35%)] Loss: 8909.513672\n",
      "Train Epoch: 28 [82176/225000 (37%)] Loss: 8818.667969\n",
      "Train Epoch: 28 [86272/225000 (38%)] Loss: 9088.966797\n",
      "Train Epoch: 28 [90368/225000 (40%)] Loss: 8962.769531\n",
      "Train Epoch: 28 [94464/225000 (42%)] Loss: 8951.009766\n",
      "Train Epoch: 28 [98560/225000 (44%)] Loss: 9054.478516\n",
      "Train Epoch: 28 [102656/225000 (46%)] Loss: 8732.177734\n",
      "Train Epoch: 28 [106752/225000 (47%)] Loss: 8801.980469\n",
      "Train Epoch: 28 [110848/225000 (49%)] Loss: 8824.320312\n",
      "Train Epoch: 28 [114944/225000 (51%)] Loss: 8598.759766\n",
      "Train Epoch: 28 [119040/225000 (53%)] Loss: 8853.568359\n",
      "Train Epoch: 28 [123136/225000 (55%)] Loss: 8843.257812\n",
      "Train Epoch: 28 [127232/225000 (57%)] Loss: 8980.283203\n",
      "Train Epoch: 28 [131328/225000 (58%)] Loss: 8666.421875\n",
      "Train Epoch: 28 [135424/225000 (60%)] Loss: 8808.425781\n",
      "Train Epoch: 28 [139520/225000 (62%)] Loss: 8941.921875\n",
      "Train Epoch: 28 [143616/225000 (64%)] Loss: 8961.349609\n",
      "Train Epoch: 28 [147712/225000 (66%)] Loss: 9001.693359\n",
      "Train Epoch: 28 [151808/225000 (67%)] Loss: 8905.750000\n",
      "Train Epoch: 28 [155904/225000 (69%)] Loss: 8889.626953\n",
      "Train Epoch: 28 [160000/225000 (71%)] Loss: 8780.931641\n",
      "Train Epoch: 28 [164096/225000 (73%)] Loss: 8855.414062\n",
      "Train Epoch: 28 [168192/225000 (75%)] Loss: 8974.080078\n",
      "Train Epoch: 28 [172288/225000 (77%)] Loss: 8876.658203\n",
      "Train Epoch: 28 [176384/225000 (78%)] Loss: 8713.978516\n",
      "Train Epoch: 28 [180480/225000 (80%)] Loss: 9009.937500\n",
      "Train Epoch: 28 [184576/225000 (82%)] Loss: 8813.109375\n",
      "Train Epoch: 28 [188672/225000 (84%)] Loss: 8926.013672\n",
      "Train Epoch: 28 [192768/225000 (86%)] Loss: 8979.736328\n",
      "Train Epoch: 28 [196864/225000 (87%)] Loss: 8726.498047\n",
      "Train Epoch: 28 [200960/225000 (89%)] Loss: 8826.160156\n",
      "Train Epoch: 28 [205056/225000 (91%)] Loss: 8856.234375\n",
      "Train Epoch: 28 [209152/225000 (93%)] Loss: 8793.302734\n",
      "Train Epoch: 28 [213248/225000 (95%)] Loss: 8878.960938\n",
      "Train Epoch: 28 [217344/225000 (97%)] Loss: 8805.900391\n",
      "Train Epoch: 28 [221440/225000 (98%)] Loss: 8905.433594\n",
      "    epoch          : 28\n",
      "    loss           : 8893.417127728599\n",
      "    val_loss       : 8811.97674743679\n",
      "Train Epoch: 29 [256/225000 (0%)] Loss: 8857.119141\n",
      "Train Epoch: 29 [4352/225000 (2%)] Loss: 8991.142578\n",
      "Train Epoch: 29 [8448/225000 (4%)] Loss: 8742.234375\n",
      "Train Epoch: 29 [12544/225000 (6%)] Loss: 8776.191406\n",
      "Train Epoch: 29 [16640/225000 (7%)] Loss: 8920.921875\n",
      "Train Epoch: 29 [20736/225000 (9%)] Loss: 8593.046875\n",
      "Train Epoch: 29 [24832/225000 (11%)] Loss: 8695.449219\n",
      "Train Epoch: 29 [28928/225000 (13%)] Loss: 8792.464844\n",
      "Train Epoch: 29 [33024/225000 (15%)] Loss: 9040.785156\n",
      "Train Epoch: 29 [37120/225000 (16%)] Loss: 8690.607422\n",
      "Train Epoch: 29 [41216/225000 (18%)] Loss: 9184.271484\n",
      "Train Epoch: 29 [45312/225000 (20%)] Loss: 8851.988281\n",
      "Train Epoch: 29 [49408/225000 (22%)] Loss: 8862.511719\n",
      "Train Epoch: 29 [53504/225000 (24%)] Loss: 8762.429688\n",
      "Train Epoch: 29 [57600/225000 (26%)] Loss: 8675.474609\n",
      "Train Epoch: 29 [61696/225000 (27%)] Loss: 8796.470703\n",
      "Train Epoch: 29 [65792/225000 (29%)] Loss: 8906.537109\n",
      "Train Epoch: 29 [69888/225000 (31%)] Loss: 8843.427734\n",
      "Train Epoch: 29 [73984/225000 (33%)] Loss: 8820.976562\n",
      "Train Epoch: 29 [78080/225000 (35%)] Loss: 8664.863281\n",
      "Train Epoch: 29 [82176/225000 (37%)] Loss: 8878.011719\n",
      "Train Epoch: 29 [86272/225000 (38%)] Loss: 8934.917969\n",
      "Train Epoch: 29 [90368/225000 (40%)] Loss: 8835.933594\n",
      "Train Epoch: 29 [94464/225000 (42%)] Loss: 8744.962891\n",
      "Train Epoch: 29 [98560/225000 (44%)] Loss: 8794.808594\n",
      "Train Epoch: 29 [102656/225000 (46%)] Loss: 8738.070312\n",
      "Train Epoch: 29 [106752/225000 (47%)] Loss: 8893.783203\n",
      "Train Epoch: 29 [110848/225000 (49%)] Loss: 8761.966797\n",
      "Train Epoch: 29 [114944/225000 (51%)] Loss: 8837.822266\n",
      "Train Epoch: 29 [119040/225000 (53%)] Loss: 8905.597656\n",
      "Train Epoch: 29 [123136/225000 (55%)] Loss: 8827.615234\n",
      "Train Epoch: 29 [127232/225000 (57%)] Loss: 8797.828125\n",
      "Train Epoch: 29 [131328/225000 (58%)] Loss: 8756.517578\n",
      "Train Epoch: 29 [135424/225000 (60%)] Loss: 8868.761719\n",
      "Train Epoch: 29 [139520/225000 (62%)] Loss: 8712.205078\n",
      "Train Epoch: 29 [143616/225000 (64%)] Loss: 8829.269531\n",
      "Train Epoch: 29 [147712/225000 (66%)] Loss: 8763.773438\n",
      "Train Epoch: 29 [151808/225000 (67%)] Loss: 8763.755859\n",
      "Train Epoch: 29 [155904/225000 (69%)] Loss: 8623.337891\n",
      "Train Epoch: 29 [160000/225000 (71%)] Loss: 8680.820312\n",
      "Train Epoch: 29 [164096/225000 (73%)] Loss: 8657.591797\n",
      "Train Epoch: 29 [168192/225000 (75%)] Loss: 9020.287109\n",
      "Train Epoch: 29 [172288/225000 (77%)] Loss: 8941.335938\n",
      "Train Epoch: 29 [176384/225000 (78%)] Loss: 8502.287109\n",
      "Train Epoch: 29 [180480/225000 (80%)] Loss: 8803.154297\n",
      "Train Epoch: 29 [184576/225000 (82%)] Loss: 8897.707031\n",
      "Train Epoch: 29 [188672/225000 (84%)] Loss: 8616.005859\n",
      "Train Epoch: 29 [192768/225000 (86%)] Loss: 8908.585938\n",
      "Train Epoch: 29 [196864/225000 (87%)] Loss: 8778.685547\n",
      "Train Epoch: 29 [200960/225000 (89%)] Loss: 8634.224609\n",
      "Train Epoch: 29 [205056/225000 (91%)] Loss: 8865.871094\n",
      "Train Epoch: 29 [209152/225000 (93%)] Loss: 8634.972656\n",
      "Train Epoch: 29 [213248/225000 (95%)] Loss: 8810.960938\n",
      "Train Epoch: 29 [217344/225000 (97%)] Loss: 8990.052734\n",
      "Train Epoch: 29 [221440/225000 (98%)] Loss: 8997.310547\n",
      "    epoch          : 29\n",
      "    loss           : 8839.205486970279\n",
      "    val_loss       : 8772.412772080119\n",
      "Train Epoch: 30 [256/225000 (0%)] Loss: 9006.064453\n",
      "Train Epoch: 30 [4352/225000 (2%)] Loss: 8804.205078\n",
      "Train Epoch: 30 [8448/225000 (4%)] Loss: 8819.152344\n",
      "Train Epoch: 30 [12544/225000 (6%)] Loss: 8711.220703\n",
      "Train Epoch: 30 [16640/225000 (7%)] Loss: 8893.759766\n",
      "Train Epoch: 30 [20736/225000 (9%)] Loss: 8959.292969\n",
      "Train Epoch: 30 [24832/225000 (11%)] Loss: 8871.701172\n",
      "Train Epoch: 30 [28928/225000 (13%)] Loss: 8844.974609\n",
      "Train Epoch: 30 [33024/225000 (15%)] Loss: 8796.519531\n",
      "Train Epoch: 30 [37120/225000 (16%)] Loss: 8728.039062\n",
      "Train Epoch: 30 [41216/225000 (18%)] Loss: 8890.751953\n",
      "Train Epoch: 30 [45312/225000 (20%)] Loss: 8961.613281\n",
      "Train Epoch: 30 [49408/225000 (22%)] Loss: 8751.339844\n",
      "Train Epoch: 30 [53504/225000 (24%)] Loss: 8712.955078\n",
      "Train Epoch: 30 [57600/225000 (26%)] Loss: 8698.312500\n",
      "Train Epoch: 30 [61696/225000 (27%)] Loss: 8615.357422\n",
      "Train Epoch: 30 [65792/225000 (29%)] Loss: 8678.726562\n",
      "Train Epoch: 30 [69888/225000 (31%)] Loss: 8764.990234\n",
      "Train Epoch: 30 [73984/225000 (33%)] Loss: 8754.941406\n",
      "Train Epoch: 30 [78080/225000 (35%)] Loss: 8540.625000\n",
      "Train Epoch: 30 [82176/225000 (37%)] Loss: 8938.519531\n",
      "Train Epoch: 30 [86272/225000 (38%)] Loss: 8665.755859\n",
      "Train Epoch: 30 [90368/225000 (40%)] Loss: 8526.390625\n",
      "Train Epoch: 30 [94464/225000 (42%)] Loss: 8448.791016\n",
      "Train Epoch: 30 [98560/225000 (44%)] Loss: 8751.607422\n",
      "Train Epoch: 30 [102656/225000 (46%)] Loss: 8778.632812\n",
      "Train Epoch: 30 [106752/225000 (47%)] Loss: 8692.634766\n",
      "Train Epoch: 30 [110848/225000 (49%)] Loss: 9114.732422\n",
      "Train Epoch: 30 [114944/225000 (51%)] Loss: 8739.757812\n",
      "Train Epoch: 30 [119040/225000 (53%)] Loss: 8861.416016\n",
      "Train Epoch: 30 [123136/225000 (55%)] Loss: 8760.656250\n",
      "Train Epoch: 30 [127232/225000 (57%)] Loss: 8886.878906\n",
      "Train Epoch: 30 [131328/225000 (58%)] Loss: 8852.539062\n",
      "Train Epoch: 30 [135424/225000 (60%)] Loss: 8639.718750\n",
      "Train Epoch: 30 [139520/225000 (62%)] Loss: 8664.753906\n",
      "Train Epoch: 30 [143616/225000 (64%)] Loss: 8595.798828\n",
      "Train Epoch: 30 [147712/225000 (66%)] Loss: 8846.619141\n",
      "Train Epoch: 30 [151808/225000 (67%)] Loss: 8714.552734\n",
      "Train Epoch: 30 [155904/225000 (69%)] Loss: 8760.847656\n",
      "Train Epoch: 30 [160000/225000 (71%)] Loss: 8737.916016\n",
      "Train Epoch: 30 [164096/225000 (73%)] Loss: 8837.593750\n",
      "Train Epoch: 30 [168192/225000 (75%)] Loss: 8838.654297\n",
      "Train Epoch: 30 [172288/225000 (77%)] Loss: 8851.492188\n",
      "Train Epoch: 30 [176384/225000 (78%)] Loss: 8742.701172\n",
      "Train Epoch: 30 [180480/225000 (80%)] Loss: 8942.521484\n",
      "Train Epoch: 30 [184576/225000 (82%)] Loss: 8647.140625\n",
      "Train Epoch: 30 [188672/225000 (84%)] Loss: 8742.250000\n",
      "Train Epoch: 30 [192768/225000 (86%)] Loss: 8747.673828\n",
      "Train Epoch: 30 [196864/225000 (87%)] Loss: 8773.064453\n",
      "Train Epoch: 30 [200960/225000 (89%)] Loss: 8701.304688\n",
      "Train Epoch: 30 [205056/225000 (91%)] Loss: 8668.339844\n",
      "Train Epoch: 30 [209152/225000 (93%)] Loss: 8802.214844\n",
      "Train Epoch: 30 [213248/225000 (95%)] Loss: 8516.345703\n",
      "Train Epoch: 30 [217344/225000 (97%)] Loss: 8845.900391\n",
      "Train Epoch: 30 [221440/225000 (98%)] Loss: 8463.744141\n",
      "    epoch          : 30\n",
      "    loss           : 8821.463610548208\n",
      "    val_loss       : 8720.409666628253\n",
      "Train Epoch: 31 [256/225000 (0%)] Loss: 8624.250000\n",
      "Train Epoch: 31 [4352/225000 (2%)] Loss: 8720.316406\n",
      "Train Epoch: 31 [8448/225000 (4%)] Loss: 8711.171875\n",
      "Train Epoch: 31 [12544/225000 (6%)] Loss: 8866.544922\n",
      "Train Epoch: 31 [16640/225000 (7%)] Loss: 9070.734375\n",
      "Train Epoch: 31 [20736/225000 (9%)] Loss: 8758.976562\n",
      "Train Epoch: 31 [24832/225000 (11%)] Loss: 8817.740234\n",
      "Train Epoch: 31 [28928/225000 (13%)] Loss: 8740.837891\n",
      "Train Epoch: 31 [33024/225000 (15%)] Loss: 8563.318359\n",
      "Train Epoch: 31 [37120/225000 (16%)] Loss: 8628.023438\n",
      "Train Epoch: 31 [41216/225000 (18%)] Loss: 8634.023438\n",
      "Train Epoch: 31 [45312/225000 (20%)] Loss: 8931.500000\n",
      "Train Epoch: 31 [49408/225000 (22%)] Loss: 8728.107422\n",
      "Train Epoch: 31 [53504/225000 (24%)] Loss: 8702.117188\n",
      "Train Epoch: 31 [57600/225000 (26%)] Loss: 8706.533203\n",
      "Train Epoch: 31 [61696/225000 (27%)] Loss: 8871.652344\n",
      "Train Epoch: 31 [65792/225000 (29%)] Loss: 8680.037109\n",
      "Train Epoch: 31 [69888/225000 (31%)] Loss: 8788.220703\n",
      "Train Epoch: 31 [73984/225000 (33%)] Loss: 8861.626953\n",
      "Train Epoch: 31 [78080/225000 (35%)] Loss: 8483.416016\n",
      "Train Epoch: 31 [82176/225000 (37%)] Loss: 8732.406250\n",
      "Train Epoch: 31 [86272/225000 (38%)] Loss: 8820.910156\n",
      "Train Epoch: 31 [90368/225000 (40%)] Loss: 8646.822266\n",
      "Train Epoch: 31 [94464/225000 (42%)] Loss: 8644.220703\n",
      "Train Epoch: 31 [98560/225000 (44%)] Loss: 8776.750000\n",
      "Train Epoch: 31 [102656/225000 (46%)] Loss: 8842.568359\n",
      "Train Epoch: 31 [106752/225000 (47%)] Loss: 8634.312500\n",
      "Train Epoch: 31 [110848/225000 (49%)] Loss: 8726.230469\n",
      "Train Epoch: 31 [114944/225000 (51%)] Loss: 8715.339844\n",
      "Train Epoch: 31 [119040/225000 (53%)] Loss: 8658.080078\n",
      "Train Epoch: 31 [123136/225000 (55%)] Loss: 8477.048828\n",
      "Train Epoch: 31 [127232/225000 (57%)] Loss: 8935.849609\n",
      "Train Epoch: 31 [131328/225000 (58%)] Loss: 8513.037109\n",
      "Train Epoch: 31 [135424/225000 (60%)] Loss: 8731.816406\n",
      "Train Epoch: 31 [139520/225000 (62%)] Loss: 8863.654297\n",
      "Train Epoch: 31 [143616/225000 (64%)] Loss: 8765.421875\n",
      "Train Epoch: 31 [147712/225000 (66%)] Loss: 8787.236328\n",
      "Train Epoch: 31 [151808/225000 (67%)] Loss: 8613.349609\n",
      "Train Epoch: 31 [155904/225000 (69%)] Loss: 8857.257812\n",
      "Train Epoch: 31 [160000/225000 (71%)] Loss: 8685.234375\n",
      "Train Epoch: 31 [164096/225000 (73%)] Loss: 8720.738281\n",
      "Train Epoch: 31 [168192/225000 (75%)] Loss: 9019.103516\n",
      "Train Epoch: 31 [172288/225000 (77%)] Loss: 8614.322266\n",
      "Train Epoch: 31 [176384/225000 (78%)] Loss: 8823.277344\n",
      "Train Epoch: 31 [180480/225000 (80%)] Loss: 8442.783203\n",
      "Train Epoch: 31 [184576/225000 (82%)] Loss: 8857.287109\n",
      "Train Epoch: 31 [188672/225000 (84%)] Loss: 8635.208984\n",
      "Train Epoch: 31 [192768/225000 (86%)] Loss: 8891.199219\n",
      "Train Epoch: 31 [196864/225000 (87%)] Loss: 8856.703125\n",
      "Train Epoch: 31 [200960/225000 (89%)] Loss: 8715.179688\n",
      "Train Epoch: 31 [205056/225000 (91%)] Loss: 8645.166016\n",
      "Train Epoch: 31 [209152/225000 (93%)] Loss: 8774.541016\n",
      "Train Epoch: 31 [213248/225000 (95%)] Loss: 8622.976562\n",
      "Train Epoch: 31 [217344/225000 (97%)] Loss: 8800.931641\n",
      "Train Epoch: 31 [221440/225000 (98%)] Loss: 8706.195312\n",
      "    epoch          : 31\n",
      "    loss           : 8752.357782947596\n",
      "    val_loss       : 8666.11070267035\n",
      "Train Epoch: 32 [256/225000 (0%)] Loss: 8642.986328\n",
      "Train Epoch: 32 [4352/225000 (2%)] Loss: 8648.275391\n",
      "Train Epoch: 32 [8448/225000 (4%)] Loss: 8712.941406\n",
      "Train Epoch: 32 [12544/225000 (6%)] Loss: 8544.810547\n",
      "Train Epoch: 32 [16640/225000 (7%)] Loss: 8669.644531\n",
      "Train Epoch: 32 [20736/225000 (9%)] Loss: 8794.949219\n",
      "Train Epoch: 32 [24832/225000 (11%)] Loss: 8632.093750\n",
      "Train Epoch: 32 [28928/225000 (13%)] Loss: 8771.435547\n",
      "Train Epoch: 32 [33024/225000 (15%)] Loss: 8592.134766\n",
      "Train Epoch: 32 [37120/225000 (16%)] Loss: 8500.597656\n",
      "Train Epoch: 32 [41216/225000 (18%)] Loss: 8729.353516\n",
      "Train Epoch: 32 [45312/225000 (20%)] Loss: 8721.488281\n",
      "Train Epoch: 32 [49408/225000 (22%)] Loss: 8688.869141\n",
      "Train Epoch: 32 [53504/225000 (24%)] Loss: 8854.544922\n",
      "Train Epoch: 32 [57600/225000 (26%)] Loss: 8720.388672\n",
      "Train Epoch: 32 [61696/225000 (27%)] Loss: 8685.656250\n",
      "Train Epoch: 32 [65792/225000 (29%)] Loss: 8469.728516\n",
      "Train Epoch: 32 [69888/225000 (31%)] Loss: 8732.675781\n",
      "Train Epoch: 32 [73984/225000 (33%)] Loss: 8574.621094\n",
      "Train Epoch: 32 [78080/225000 (35%)] Loss: 8676.664062\n",
      "Train Epoch: 32 [82176/225000 (37%)] Loss: 8649.181641\n",
      "Train Epoch: 32 [86272/225000 (38%)] Loss: 8583.500000\n",
      "Train Epoch: 32 [90368/225000 (40%)] Loss: 8672.093750\n",
      "Train Epoch: 32 [94464/225000 (42%)] Loss: 8674.078125\n",
      "Train Epoch: 32 [98560/225000 (44%)] Loss: 9055.085938\n",
      "Train Epoch: 32 [102656/225000 (46%)] Loss: 8820.873047\n",
      "Train Epoch: 32 [106752/225000 (47%)] Loss: 8757.601562\n",
      "Train Epoch: 32 [110848/225000 (49%)] Loss: 8667.203125\n",
      "Train Epoch: 32 [114944/225000 (51%)] Loss: 8567.062500\n",
      "Train Epoch: 32 [119040/225000 (53%)] Loss: 8853.691406\n",
      "Train Epoch: 32 [123136/225000 (55%)] Loss: 8591.017578\n",
      "Train Epoch: 32 [127232/225000 (57%)] Loss: 8856.593750\n",
      "Train Epoch: 32 [131328/225000 (58%)] Loss: 8854.462891\n",
      "Train Epoch: 32 [135424/225000 (60%)] Loss: 8691.218750\n",
      "Train Epoch: 32 [139520/225000 (62%)] Loss: 8723.201172\n",
      "Train Epoch: 32 [143616/225000 (64%)] Loss: 8708.166016\n",
      "Train Epoch: 32 [147712/225000 (66%)] Loss: 8594.078125\n",
      "Train Epoch: 32 [151808/225000 (67%)] Loss: 8510.542969\n",
      "Train Epoch: 32 [155904/225000 (69%)] Loss: 8735.101562\n",
      "Train Epoch: 32 [160000/225000 (71%)] Loss: 8439.257812\n",
      "Train Epoch: 32 [164096/225000 (73%)] Loss: 8657.515625\n",
      "Train Epoch: 32 [168192/225000 (75%)] Loss: 8823.863281\n",
      "Train Epoch: 32 [172288/225000 (77%)] Loss: 8760.634766\n",
      "Train Epoch: 32 [176384/225000 (78%)] Loss: 8580.660156\n",
      "Train Epoch: 32 [180480/225000 (80%)] Loss: 8823.761719\n",
      "Train Epoch: 32 [184576/225000 (82%)] Loss: 8730.156250\n",
      "Train Epoch: 32 [188672/225000 (84%)] Loss: 8702.380859\n",
      "Train Epoch: 32 [192768/225000 (86%)] Loss: 8730.199219\n",
      "Train Epoch: 32 [196864/225000 (87%)] Loss: 8573.978516\n",
      "Train Epoch: 32 [200960/225000 (89%)] Loss: 8562.427734\n",
      "Train Epoch: 32 [205056/225000 (91%)] Loss: 8654.619141\n",
      "Train Epoch: 32 [209152/225000 (93%)] Loss: 8735.023438\n",
      "Train Epoch: 32 [213248/225000 (95%)] Loss: 8472.402344\n",
      "Train Epoch: 32 [217344/225000 (97%)] Loss: 8722.132812\n",
      "Train Epoch: 32 [221440/225000 (98%)] Loss: 8634.488281\n",
      "    epoch          : 32\n",
      "    loss           : 8708.681458422212\n",
      "    val_loss       : 8630.885413483698\n",
      "Train Epoch: 33 [256/225000 (0%)] Loss: 8584.984375\n",
      "Train Epoch: 33 [4352/225000 (2%)] Loss: 8629.332031\n",
      "Train Epoch: 33 [8448/225000 (4%)] Loss: 8759.669922\n",
      "Train Epoch: 33 [12544/225000 (6%)] Loss: 8674.318359\n",
      "Train Epoch: 33 [16640/225000 (7%)] Loss: 8632.017578\n",
      "Train Epoch: 33 [20736/225000 (9%)] Loss: 8926.632812\n",
      "Train Epoch: 33 [24832/225000 (11%)] Loss: 8758.115234\n",
      "Train Epoch: 33 [28928/225000 (13%)] Loss: 8575.378906\n",
      "Train Epoch: 33 [33024/225000 (15%)] Loss: 8659.970703\n",
      "Train Epoch: 33 [37120/225000 (16%)] Loss: 8654.908203\n",
      "Train Epoch: 33 [41216/225000 (18%)] Loss: 8602.748047\n",
      "Train Epoch: 33 [45312/225000 (20%)] Loss: 8709.310547\n",
      "Train Epoch: 33 [49408/225000 (22%)] Loss: 8741.445312\n",
      "Train Epoch: 33 [53504/225000 (24%)] Loss: 8591.593750\n",
      "Train Epoch: 33 [57600/225000 (26%)] Loss: 8747.091797\n",
      "Train Epoch: 33 [61696/225000 (27%)] Loss: 8700.408203\n",
      "Train Epoch: 33 [65792/225000 (29%)] Loss: 8543.466797\n",
      "Train Epoch: 33 [69888/225000 (31%)] Loss: 8555.941406\n",
      "Train Epoch: 33 [73984/225000 (33%)] Loss: 8673.085938\n",
      "Train Epoch: 33 [78080/225000 (35%)] Loss: 8738.496094\n",
      "Train Epoch: 33 [82176/225000 (37%)] Loss: 8447.960938\n",
      "Train Epoch: 33 [86272/225000 (38%)] Loss: 8674.382812\n",
      "Train Epoch: 33 [90368/225000 (40%)] Loss: 8829.671875\n",
      "Train Epoch: 33 [94464/225000 (42%)] Loss: 8598.796875\n",
      "Train Epoch: 33 [98560/225000 (44%)] Loss: 8702.419922\n",
      "Train Epoch: 33 [102656/225000 (46%)] Loss: 8481.355469\n",
      "Train Epoch: 33 [106752/225000 (47%)] Loss: 8678.517578\n",
      "Train Epoch: 33 [110848/225000 (49%)] Loss: 8623.212891\n",
      "Train Epoch: 33 [114944/225000 (51%)] Loss: 8385.642578\n",
      "Train Epoch: 33 [119040/225000 (53%)] Loss: 8649.035156\n",
      "Train Epoch: 33 [123136/225000 (55%)] Loss: 8603.236328\n",
      "Train Epoch: 33 [127232/225000 (57%)] Loss: 8785.380859\n",
      "Train Epoch: 33 [131328/225000 (58%)] Loss: 8710.890625\n",
      "Train Epoch: 33 [135424/225000 (60%)] Loss: 8791.738281\n",
      "Train Epoch: 33 [139520/225000 (62%)] Loss: 8448.398438\n",
      "Train Epoch: 33 [143616/225000 (64%)] Loss: 8770.542969\n",
      "Train Epoch: 33 [147712/225000 (66%)] Loss: 8569.291016\n",
      "Train Epoch: 33 [151808/225000 (67%)] Loss: 8570.468750\n",
      "Train Epoch: 33 [155904/225000 (69%)] Loss: 8681.968750\n",
      "Train Epoch: 33 [160000/225000 (71%)] Loss: 8576.597656\n",
      "Train Epoch: 33 [164096/225000 (73%)] Loss: 8564.980469\n",
      "Train Epoch: 33 [168192/225000 (75%)] Loss: 8674.789062\n",
      "Train Epoch: 33 [172288/225000 (77%)] Loss: 8627.349609\n",
      "Train Epoch: 33 [176384/225000 (78%)] Loss: 8623.271484\n",
      "Train Epoch: 33 [180480/225000 (80%)] Loss: 8490.201172\n",
      "Train Epoch: 33 [184576/225000 (82%)] Loss: 8542.292969\n",
      "Train Epoch: 33 [188672/225000 (84%)] Loss: 8749.333984\n",
      "Train Epoch: 33 [192768/225000 (86%)] Loss: 8663.859375\n",
      "Train Epoch: 33 [196864/225000 (87%)] Loss: 8557.269531\n",
      "Train Epoch: 33 [200960/225000 (89%)] Loss: 8808.291016\n",
      "Train Epoch: 33 [205056/225000 (91%)] Loss: 8660.527344\n",
      "Train Epoch: 33 [209152/225000 (93%)] Loss: 8674.554688\n",
      "Train Epoch: 33 [213248/225000 (95%)] Loss: 8618.228516\n",
      "Train Epoch: 33 [217344/225000 (97%)] Loss: 8833.666016\n",
      "Train Epoch: 33 [221440/225000 (98%)] Loss: 8897.695312\n",
      "    epoch          : 33\n",
      "    loss           : 8698.875004443971\n",
      "    val_loss       : 8658.943161149415\n",
      "Train Epoch: 34 [256/225000 (0%)] Loss: 8732.552734\n",
      "Train Epoch: 34 [4352/225000 (2%)] Loss: 8756.048828\n",
      "Train Epoch: 34 [8448/225000 (4%)] Loss: 8662.542969\n",
      "Train Epoch: 34 [12544/225000 (6%)] Loss: 8666.546875\n",
      "Train Epoch: 34 [16640/225000 (7%)] Loss: 8512.054688\n",
      "Train Epoch: 34 [20736/225000 (9%)] Loss: 8589.626953\n",
      "Train Epoch: 34 [24832/225000 (11%)] Loss: 8515.775391\n",
      "Train Epoch: 34 [28928/225000 (13%)] Loss: 8644.230469\n",
      "Train Epoch: 34 [33024/225000 (15%)] Loss: 8630.800781\n",
      "Train Epoch: 34 [37120/225000 (16%)] Loss: 8497.931641\n",
      "Train Epoch: 34 [41216/225000 (18%)] Loss: 8817.277344\n",
      "Train Epoch: 34 [45312/225000 (20%)] Loss: 8602.318359\n",
      "Train Epoch: 34 [49408/225000 (22%)] Loss: 8598.664062\n",
      "Train Epoch: 34 [53504/225000 (24%)] Loss: 8476.912109\n",
      "Train Epoch: 34 [57600/225000 (26%)] Loss: 8768.232422\n",
      "Train Epoch: 34 [61696/225000 (27%)] Loss: 8805.609375\n",
      "Train Epoch: 34 [65792/225000 (29%)] Loss: 8481.511719\n",
      "Train Epoch: 34 [69888/225000 (31%)] Loss: 8516.710938\n",
      "Train Epoch: 34 [73984/225000 (33%)] Loss: 8634.712891\n",
      "Train Epoch: 34 [78080/225000 (35%)] Loss: 8615.503906\n",
      "Train Epoch: 34 [82176/225000 (37%)] Loss: 8618.650391\n",
      "Train Epoch: 34 [86272/225000 (38%)] Loss: 8571.398438\n",
      "Train Epoch: 34 [90368/225000 (40%)] Loss: 8759.828125\n",
      "Train Epoch: 34 [94464/225000 (42%)] Loss: 8568.421875\n",
      "Train Epoch: 34 [98560/225000 (44%)] Loss: 8603.271484\n",
      "Train Epoch: 34 [102656/225000 (46%)] Loss: 8531.015625\n",
      "Train Epoch: 34 [106752/225000 (47%)] Loss: 8533.238281\n",
      "Train Epoch: 34 [110848/225000 (49%)] Loss: 8382.210938\n",
      "Train Epoch: 34 [114944/225000 (51%)] Loss: 8599.884766\n",
      "Train Epoch: 34 [119040/225000 (53%)] Loss: 8486.908203\n",
      "Train Epoch: 34 [123136/225000 (55%)] Loss: 8591.158203\n",
      "Train Epoch: 34 [127232/225000 (57%)] Loss: 8679.156250\n",
      "Train Epoch: 34 [131328/225000 (58%)] Loss: 8646.904297\n",
      "Train Epoch: 34 [135424/225000 (60%)] Loss: 8373.728516\n",
      "Train Epoch: 34 [139520/225000 (62%)] Loss: 8724.283203\n",
      "Train Epoch: 34 [143616/225000 (64%)] Loss: 8819.478516\n",
      "Train Epoch: 34 [147712/225000 (66%)] Loss: 8592.445312\n",
      "Train Epoch: 34 [151808/225000 (67%)] Loss: 8524.640625\n",
      "Train Epoch: 34 [155904/225000 (69%)] Loss: 8530.818359\n",
      "Train Epoch: 34 [160000/225000 (71%)] Loss: 8678.609375\n",
      "Train Epoch: 34 [164096/225000 (73%)] Loss: 8783.572266\n",
      "Train Epoch: 34 [168192/225000 (75%)] Loss: 8379.421875\n",
      "Train Epoch: 34 [172288/225000 (77%)] Loss: 8545.808594\n",
      "Train Epoch: 34 [176384/225000 (78%)] Loss: 8570.437500\n",
      "Train Epoch: 34 [180480/225000 (80%)] Loss: 8488.855469\n",
      "Train Epoch: 34 [184576/225000 (82%)] Loss: 8693.871094\n",
      "Train Epoch: 34 [188672/225000 (84%)] Loss: 8732.537109\n",
      "Train Epoch: 34 [192768/225000 (86%)] Loss: 8313.980469\n",
      "Train Epoch: 34 [196864/225000 (87%)] Loss: 8552.716797\n",
      "Train Epoch: 34 [200960/225000 (89%)] Loss: 8605.136719\n",
      "Train Epoch: 34 [205056/225000 (91%)] Loss: 8495.191406\n",
      "Train Epoch: 34 [209152/225000 (93%)] Loss: 8397.349609\n",
      "Train Epoch: 34 [213248/225000 (95%)] Loss: 8536.037109\n",
      "Train Epoch: 34 [217344/225000 (97%)] Loss: 8716.779297\n",
      "Train Epoch: 34 [221440/225000 (98%)] Loss: 8576.619141\n",
      "    epoch          : 34\n",
      "    loss           : 8638.767637007608\n",
      "    val_loss       : 8555.090932005522\n",
      "Train Epoch: 35 [256/225000 (0%)] Loss: 8592.437500\n",
      "Train Epoch: 35 [4352/225000 (2%)] Loss: 8601.689453\n",
      "Train Epoch: 35 [8448/225000 (4%)] Loss: 8727.359375\n",
      "Train Epoch: 35 [12544/225000 (6%)] Loss: 8557.228516\n",
      "Train Epoch: 35 [16640/225000 (7%)] Loss: 8622.341797\n",
      "Train Epoch: 35 [20736/225000 (9%)] Loss: 8459.763672\n",
      "Train Epoch: 35 [24832/225000 (11%)] Loss: 8601.681641\n",
      "Train Epoch: 35 [28928/225000 (13%)] Loss: 8535.722656\n",
      "Train Epoch: 35 [33024/225000 (15%)] Loss: 8491.970703\n",
      "Train Epoch: 35 [37120/225000 (16%)] Loss: 8560.705078\n",
      "Train Epoch: 35 [41216/225000 (18%)] Loss: 8340.166016\n",
      "Train Epoch: 35 [45312/225000 (20%)] Loss: 8677.355469\n",
      "Train Epoch: 35 [49408/225000 (22%)] Loss: 8590.708984\n",
      "Train Epoch: 35 [53504/225000 (24%)] Loss: 8535.503906\n",
      "Train Epoch: 35 [57600/225000 (26%)] Loss: 8626.574219\n",
      "Train Epoch: 35 [61696/225000 (27%)] Loss: 8523.992188\n",
      "Train Epoch: 35 [65792/225000 (29%)] Loss: 8636.357422\n",
      "Train Epoch: 35 [69888/225000 (31%)] Loss: 8626.080078\n",
      "Train Epoch: 35 [73984/225000 (33%)] Loss: 8516.632812\n",
      "Train Epoch: 35 [78080/225000 (35%)] Loss: 8494.939453\n",
      "Train Epoch: 35 [82176/225000 (37%)] Loss: 8556.689453\n",
      "Train Epoch: 35 [86272/225000 (38%)] Loss: 8456.378906\n",
      "Train Epoch: 35 [90368/225000 (40%)] Loss: 8716.371094\n",
      "Train Epoch: 35 [94464/225000 (42%)] Loss: 8429.271484\n",
      "Train Epoch: 35 [98560/225000 (44%)] Loss: 8505.794922\n",
      "Train Epoch: 35 [102656/225000 (46%)] Loss: 8390.763672\n",
      "Train Epoch: 35 [106752/225000 (47%)] Loss: 8593.423828\n",
      "Train Epoch: 35 [110848/225000 (49%)] Loss: 8418.066406\n",
      "Train Epoch: 35 [114944/225000 (51%)] Loss: 8881.201172\n",
      "Train Epoch: 35 [119040/225000 (53%)] Loss: 8348.646484\n",
      "Train Epoch: 35 [123136/225000 (55%)] Loss: 8647.367188\n",
      "Train Epoch: 35 [127232/225000 (57%)] Loss: 8432.515625\n",
      "Train Epoch: 35 [131328/225000 (58%)] Loss: 8590.380859\n",
      "Train Epoch: 35 [135424/225000 (60%)] Loss: 8598.232422\n",
      "Train Epoch: 35 [139520/225000 (62%)] Loss: 8563.460938\n",
      "Train Epoch: 35 [143616/225000 (64%)] Loss: 8587.818359\n",
      "Train Epoch: 35 [147712/225000 (66%)] Loss: 8636.181641\n",
      "Train Epoch: 35 [151808/225000 (67%)] Loss: 8579.392578\n",
      "Train Epoch: 35 [155904/225000 (69%)] Loss: 8595.775391\n",
      "Train Epoch: 35 [160000/225000 (71%)] Loss: 8438.519531\n",
      "Train Epoch: 35 [164096/225000 (73%)] Loss: 8618.314453\n",
      "Train Epoch: 35 [168192/225000 (75%)] Loss: 8480.068359\n",
      "Train Epoch: 35 [172288/225000 (77%)] Loss: 8597.339844\n",
      "Train Epoch: 35 [176384/225000 (78%)] Loss: 8568.285156\n",
      "Train Epoch: 35 [180480/225000 (80%)] Loss: 8473.220703\n",
      "Train Epoch: 35 [184576/225000 (82%)] Loss: 8694.871094\n",
      "Train Epoch: 35 [188672/225000 (84%)] Loss: 8568.505859\n",
      "Train Epoch: 35 [192768/225000 (86%)] Loss: 8559.873047\n",
      "Train Epoch: 35 [196864/225000 (87%)] Loss: 8702.021484\n",
      "Train Epoch: 35 [200960/225000 (89%)] Loss: 8356.150391\n",
      "Train Epoch: 35 [205056/225000 (91%)] Loss: 8668.929688\n",
      "Train Epoch: 35 [209152/225000 (93%)] Loss: 8668.832031\n",
      "Train Epoch: 35 [213248/225000 (95%)] Loss: 8746.982422\n",
      "Train Epoch: 35 [217344/225000 (97%)] Loss: 8492.001953\n",
      "Train Epoch: 35 [221440/225000 (98%)] Loss: 8822.482422\n",
      "    epoch          : 35\n",
      "    loss           : 8606.518825769695\n",
      "    val_loss       : 8532.610046353875\n",
      "Train Epoch: 36 [256/225000 (0%)] Loss: 8508.890625\n",
      "Train Epoch: 36 [4352/225000 (2%)] Loss: 8532.464844\n",
      "Train Epoch: 36 [8448/225000 (4%)] Loss: 8743.503906\n",
      "Train Epoch: 36 [12544/225000 (6%)] Loss: 8584.355469\n",
      "Train Epoch: 36 [16640/225000 (7%)] Loss: 8518.193359\n",
      "Train Epoch: 36 [20736/225000 (9%)] Loss: 8737.091797\n",
      "Train Epoch: 36 [24832/225000 (11%)] Loss: 8589.664062\n",
      "Train Epoch: 36 [28928/225000 (13%)] Loss: 8598.007812\n",
      "Train Epoch: 36 [33024/225000 (15%)] Loss: 8508.460938\n",
      "Train Epoch: 36 [37120/225000 (16%)] Loss: 8589.373047\n",
      "Train Epoch: 36 [41216/225000 (18%)] Loss: 8615.931641\n",
      "Train Epoch: 36 [45312/225000 (20%)] Loss: 8470.369141\n",
      "Train Epoch: 36 [49408/225000 (22%)] Loss: 8556.052734\n",
      "Train Epoch: 36 [53504/225000 (24%)] Loss: 8440.230469\n",
      "Train Epoch: 36 [57600/225000 (26%)] Loss: 8563.953125\n",
      "Train Epoch: 36 [61696/225000 (27%)] Loss: 8664.246094\n",
      "Train Epoch: 36 [65792/225000 (29%)] Loss: 8405.736328\n",
      "Train Epoch: 36 [69888/225000 (31%)] Loss: 8589.111328\n",
      "Train Epoch: 36 [73984/225000 (33%)] Loss: 8417.341797\n",
      "Train Epoch: 36 [78080/225000 (35%)] Loss: 8568.490234\n",
      "Train Epoch: 36 [82176/225000 (37%)] Loss: 8555.998047\n",
      "Train Epoch: 36 [86272/225000 (38%)] Loss: 8350.445312\n",
      "Train Epoch: 36 [90368/225000 (40%)] Loss: 8432.087891\n",
      "Train Epoch: 36 [94464/225000 (42%)] Loss: 8450.480469\n",
      "Train Epoch: 36 [98560/225000 (44%)] Loss: 8668.626953\n",
      "Train Epoch: 36 [102656/225000 (46%)] Loss: 8769.693359\n",
      "Train Epoch: 36 [106752/225000 (47%)] Loss: 8593.437500\n",
      "Train Epoch: 36 [110848/225000 (49%)] Loss: 8400.320312\n",
      "Train Epoch: 36 [114944/225000 (51%)] Loss: 8555.185547\n",
      "Train Epoch: 36 [119040/225000 (53%)] Loss: 8608.517578\n",
      "Train Epoch: 36 [123136/225000 (55%)] Loss: 8578.287109\n",
      "Train Epoch: 36 [127232/225000 (57%)] Loss: 8524.031250\n",
      "Train Epoch: 36 [131328/225000 (58%)] Loss: 8661.042969\n",
      "Train Epoch: 36 [135424/225000 (60%)] Loss: 8636.980469\n",
      "Train Epoch: 36 [139520/225000 (62%)] Loss: 8380.812500\n",
      "Train Epoch: 36 [143616/225000 (64%)] Loss: 8521.816406\n",
      "Train Epoch: 36 [147712/225000 (66%)] Loss: 8599.357422\n",
      "Train Epoch: 36 [151808/225000 (67%)] Loss: 8451.158203\n",
      "Train Epoch: 36 [155904/225000 (69%)] Loss: 8534.894531\n",
      "Train Epoch: 36 [160000/225000 (71%)] Loss: 8341.181641\n",
      "Train Epoch: 36 [164096/225000 (73%)] Loss: 8603.058594\n",
      "Train Epoch: 36 [168192/225000 (75%)] Loss: 8566.181641\n",
      "Train Epoch: 36 [172288/225000 (77%)] Loss: 8755.798828\n",
      "Train Epoch: 36 [176384/225000 (78%)] Loss: 8670.900391\n",
      "Train Epoch: 36 [180480/225000 (80%)] Loss: 8909.923828\n",
      "Train Epoch: 36 [184576/225000 (82%)] Loss: 8587.435547\n",
      "Train Epoch: 36 [188672/225000 (84%)] Loss: 8427.011719\n",
      "Train Epoch: 36 [192768/225000 (86%)] Loss: 8430.447266\n",
      "Train Epoch: 36 [196864/225000 (87%)] Loss: 8765.685547\n",
      "Train Epoch: 36 [200960/225000 (89%)] Loss: 8497.574219\n",
      "Train Epoch: 36 [205056/225000 (91%)] Loss: 8538.458984\n",
      "Train Epoch: 36 [209152/225000 (93%)] Loss: 8651.697266\n",
      "Train Epoch: 36 [213248/225000 (95%)] Loss: 8637.408203\n",
      "Train Epoch: 36 [217344/225000 (97%)] Loss: 8499.291016\n",
      "Train Epoch: 36 [221440/225000 (98%)] Loss: 8381.052734\n",
      "    epoch          : 36\n",
      "    loss           : 8566.707041248934\n",
      "    val_loss       : 8494.838084591895\n",
      "Train Epoch: 37 [256/225000 (0%)] Loss: 8511.675781\n",
      "Train Epoch: 37 [4352/225000 (2%)] Loss: 8710.396484\n",
      "Train Epoch: 37 [8448/225000 (4%)] Loss: 8455.943359\n",
      "Train Epoch: 37 [12544/225000 (6%)] Loss: 8335.724609\n",
      "Train Epoch: 37 [16640/225000 (7%)] Loss: 8563.580078\n",
      "Train Epoch: 37 [20736/225000 (9%)] Loss: 8696.640625\n",
      "Train Epoch: 37 [24832/225000 (11%)] Loss: 8337.312500\n",
      "Train Epoch: 37 [28928/225000 (13%)] Loss: 8357.875000\n",
      "Train Epoch: 37 [33024/225000 (15%)] Loss: 8601.904297\n",
      "Train Epoch: 37 [37120/225000 (16%)] Loss: 8342.322266\n",
      "Train Epoch: 37 [41216/225000 (18%)] Loss: 8476.982422\n",
      "Train Epoch: 37 [45312/225000 (20%)] Loss: 8437.392578\n",
      "Train Epoch: 37 [49408/225000 (22%)] Loss: 8680.792969\n",
      "Train Epoch: 37 [53504/225000 (24%)] Loss: 8452.205078\n",
      "Train Epoch: 37 [57600/225000 (26%)] Loss: 8717.474609\n",
      "Train Epoch: 37 [61696/225000 (27%)] Loss: 8410.781250\n",
      "Train Epoch: 37 [65792/225000 (29%)] Loss: 8662.025391\n",
      "Train Epoch: 37 [69888/225000 (31%)] Loss: 8472.537109\n",
      "Train Epoch: 37 [73984/225000 (33%)] Loss: 8454.718750\n",
      "Train Epoch: 37 [78080/225000 (35%)] Loss: 8593.533203\n",
      "Train Epoch: 37 [82176/225000 (37%)] Loss: 8424.865234\n",
      "Train Epoch: 37 [86272/225000 (38%)] Loss: 8676.931641\n",
      "Train Epoch: 37 [90368/225000 (40%)] Loss: 8296.042969\n",
      "Train Epoch: 37 [94464/225000 (42%)] Loss: 8489.992188\n",
      "Train Epoch: 37 [98560/225000 (44%)] Loss: 8530.371094\n",
      "Train Epoch: 37 [102656/225000 (46%)] Loss: 8359.345703\n",
      "Train Epoch: 37 [106752/225000 (47%)] Loss: 8385.318359\n",
      "Train Epoch: 37 [110848/225000 (49%)] Loss: 8510.281250\n",
      "Train Epoch: 37 [114944/225000 (51%)] Loss: 8496.982422\n",
      "Train Epoch: 37 [119040/225000 (53%)] Loss: 8702.433594\n",
      "Train Epoch: 37 [123136/225000 (55%)] Loss: 8542.566406\n",
      "Train Epoch: 37 [127232/225000 (57%)] Loss: 8417.966797\n",
      "Train Epoch: 37 [131328/225000 (58%)] Loss: 8440.634766\n",
      "Train Epoch: 37 [135424/225000 (60%)] Loss: 8407.669922\n",
      "Train Epoch: 37 [139520/225000 (62%)] Loss: 8689.695312\n",
      "Train Epoch: 37 [143616/225000 (64%)] Loss: 8572.628906\n",
      "Train Epoch: 37 [147712/225000 (66%)] Loss: 8470.513672\n",
      "Train Epoch: 37 [151808/225000 (67%)] Loss: 8476.822266\n",
      "Train Epoch: 37 [155904/225000 (69%)] Loss: 8607.082031\n",
      "Train Epoch: 37 [160000/225000 (71%)] Loss: 8572.421875\n",
      "Train Epoch: 37 [164096/225000 (73%)] Loss: 8557.033203\n",
      "Train Epoch: 37 [168192/225000 (75%)] Loss: 8476.984375\n",
      "Train Epoch: 37 [172288/225000 (77%)] Loss: 8445.734375\n",
      "Train Epoch: 37 [176384/225000 (78%)] Loss: 8292.507812\n",
      "Train Epoch: 37 [180480/225000 (80%)] Loss: 8579.728516\n",
      "Train Epoch: 37 [184576/225000 (82%)] Loss: 8473.605469\n",
      "Train Epoch: 37 [188672/225000 (84%)] Loss: 8364.062500\n",
      "Train Epoch: 37 [192768/225000 (86%)] Loss: 8616.220703\n",
      "Train Epoch: 37 [196864/225000 (87%)] Loss: 8484.443359\n",
      "Train Epoch: 37 [200960/225000 (89%)] Loss: 8501.277344\n",
      "Train Epoch: 37 [205056/225000 (91%)] Loss: 8447.035156\n",
      "Train Epoch: 37 [209152/225000 (93%)] Loss: 8480.044922\n",
      "Train Epoch: 37 [213248/225000 (95%)] Loss: 8550.767578\n",
      "Train Epoch: 37 [217344/225000 (97%)] Loss: 8491.085938\n",
      "Train Epoch: 37 [221440/225000 (98%)] Loss: 8600.650391\n",
      "    epoch          : 37\n",
      "    loss           : 8579.513508559086\n",
      "    val_loss       : 8470.933564957308\n",
      "Train Epoch: 38 [256/225000 (0%)] Loss: 8613.107422\n",
      "Train Epoch: 38 [4352/225000 (2%)] Loss: 8524.740234\n",
      "Train Epoch: 38 [8448/225000 (4%)] Loss: 8547.949219\n",
      "Train Epoch: 38 [12544/225000 (6%)] Loss: 8580.689453\n",
      "Train Epoch: 38 [16640/225000 (7%)] Loss: 8453.740234\n",
      "Train Epoch: 38 [20736/225000 (9%)] Loss: 8361.775391\n",
      "Train Epoch: 38 [24832/225000 (11%)] Loss: 8532.796875\n",
      "Train Epoch: 38 [28928/225000 (13%)] Loss: 8447.453125\n",
      "Train Epoch: 38 [33024/225000 (15%)] Loss: 8411.427734\n",
      "Train Epoch: 38 [37120/225000 (16%)] Loss: 8458.787109\n",
      "Train Epoch: 38 [41216/225000 (18%)] Loss: 8568.033203\n",
      "Train Epoch: 38 [45312/225000 (20%)] Loss: 8476.162109\n",
      "Train Epoch: 38 [49408/225000 (22%)] Loss: 8513.726562\n",
      "Train Epoch: 38 [53504/225000 (24%)] Loss: 8465.822266\n",
      "Train Epoch: 38 [57600/225000 (26%)] Loss: 8498.853516\n",
      "Train Epoch: 38 [61696/225000 (27%)] Loss: 8394.152344\n",
      "Train Epoch: 38 [65792/225000 (29%)] Loss: 8543.853516\n",
      "Train Epoch: 38 [69888/225000 (31%)] Loss: 8418.648438\n",
      "Train Epoch: 38 [73984/225000 (33%)] Loss: 8481.681641\n",
      "Train Epoch: 38 [78080/225000 (35%)] Loss: 8456.750000\n",
      "Train Epoch: 38 [82176/225000 (37%)] Loss: 8379.097656\n",
      "Train Epoch: 38 [86272/225000 (38%)] Loss: 8524.789062\n",
      "Train Epoch: 38 [90368/225000 (40%)] Loss: 8375.125000\n",
      "Train Epoch: 38 [94464/225000 (42%)] Loss: 8398.492188\n",
      "Train Epoch: 38 [98560/225000 (44%)] Loss: 8437.208984\n",
      "Train Epoch: 38 [102656/225000 (46%)] Loss: 8648.828125\n",
      "Train Epoch: 38 [106752/225000 (47%)] Loss: 8410.945312\n",
      "Train Epoch: 38 [110848/225000 (49%)] Loss: 8319.677734\n",
      "Train Epoch: 38 [114944/225000 (51%)] Loss: 8454.767578\n",
      "Train Epoch: 38 [119040/225000 (53%)] Loss: 8457.166016\n",
      "Train Epoch: 38 [123136/225000 (55%)] Loss: 8400.179688\n",
      "Train Epoch: 38 [127232/225000 (57%)] Loss: 8619.253906\n",
      "Train Epoch: 38 [131328/225000 (58%)] Loss: 8533.634766\n",
      "Train Epoch: 38 [135424/225000 (60%)] Loss: 8548.248047\n",
      "Train Epoch: 38 [139520/225000 (62%)] Loss: 8454.105469\n",
      "Train Epoch: 38 [143616/225000 (64%)] Loss: 8421.496094\n",
      "Train Epoch: 38 [147712/225000 (66%)] Loss: 8371.960938\n",
      "Train Epoch: 38 [151808/225000 (67%)] Loss: 8378.337891\n",
      "Train Epoch: 38 [155904/225000 (69%)] Loss: 8391.591797\n",
      "Train Epoch: 38 [160000/225000 (71%)] Loss: 8510.667969\n",
      "Train Epoch: 38 [164096/225000 (73%)] Loss: 8576.083984\n",
      "Train Epoch: 38 [168192/225000 (75%)] Loss: 8546.144531\n",
      "Train Epoch: 38 [172288/225000 (77%)] Loss: 8430.107422\n",
      "Train Epoch: 38 [176384/225000 (78%)] Loss: 8558.441406\n",
      "Train Epoch: 38 [180480/225000 (80%)] Loss: 8523.318359\n",
      "Train Epoch: 38 [184576/225000 (82%)] Loss: 8408.562500\n",
      "Train Epoch: 38 [188672/225000 (84%)] Loss: 8554.941406\n",
      "Train Epoch: 38 [192768/225000 (86%)] Loss: 8465.376953\n",
      "Train Epoch: 38 [196864/225000 (87%)] Loss: 8374.074219\n",
      "Train Epoch: 38 [200960/225000 (89%)] Loss: 8437.365234\n",
      "Train Epoch: 38 [205056/225000 (91%)] Loss: 8448.505859\n",
      "Train Epoch: 38 [209152/225000 (93%)] Loss: 8490.267578\n",
      "Train Epoch: 38 [213248/225000 (95%)] Loss: 8356.679688\n",
      "Train Epoch: 38 [217344/225000 (97%)] Loss: 8403.531250\n",
      "Train Epoch: 38 [221440/225000 (98%)] Loss: 8614.318359\n",
      "    epoch          : 38\n",
      "    loss           : 8508.848808349332\n",
      "    val_loss       : 8472.337505554058\n",
      "Train Epoch: 39 [256/225000 (0%)] Loss: 8618.693359\n",
      "Train Epoch: 39 [4352/225000 (2%)] Loss: 8288.908203\n",
      "Train Epoch: 39 [8448/225000 (4%)] Loss: 8314.472656\n",
      "Train Epoch: 39 [12544/225000 (6%)] Loss: 8390.244141\n",
      "Train Epoch: 39 [16640/225000 (7%)] Loss: 8596.527344\n",
      "Train Epoch: 39 [20736/225000 (9%)] Loss: 8466.478516\n",
      "Train Epoch: 39 [24832/225000 (11%)] Loss: 8475.398438\n",
      "Train Epoch: 39 [28928/225000 (13%)] Loss: 8486.718750\n",
      "Train Epoch: 39 [33024/225000 (15%)] Loss: 8590.076172\n",
      "Train Epoch: 39 [37120/225000 (16%)] Loss: 8361.558594\n",
      "Train Epoch: 39 [41216/225000 (18%)] Loss: 8301.847656\n",
      "Train Epoch: 39 [45312/225000 (20%)] Loss: 8599.777344\n",
      "Train Epoch: 39 [49408/225000 (22%)] Loss: 8374.734375\n",
      "Train Epoch: 39 [53504/225000 (24%)] Loss: 8438.955078\n",
      "Train Epoch: 39 [57600/225000 (26%)] Loss: 8445.289062\n",
      "Train Epoch: 39 [61696/225000 (27%)] Loss: 8290.515625\n",
      "Train Epoch: 39 [65792/225000 (29%)] Loss: 8341.277344\n",
      "Train Epoch: 39 [69888/225000 (31%)] Loss: 8505.292969\n",
      "Train Epoch: 39 [73984/225000 (33%)] Loss: 8450.037109\n",
      "Train Epoch: 39 [78080/225000 (35%)] Loss: 8258.898438\n",
      "Train Epoch: 39 [82176/225000 (37%)] Loss: 8480.632812\n",
      "Train Epoch: 39 [86272/225000 (38%)] Loss: 8424.998047\n",
      "Train Epoch: 39 [90368/225000 (40%)] Loss: 8521.677734\n",
      "Train Epoch: 39 [94464/225000 (42%)] Loss: 8414.621094\n",
      "Train Epoch: 39 [98560/225000 (44%)] Loss: 8602.322266\n",
      "Train Epoch: 39 [102656/225000 (46%)] Loss: 8525.134766\n",
      "Train Epoch: 39 [106752/225000 (47%)] Loss: 8642.609375\n",
      "Train Epoch: 39 [110848/225000 (49%)] Loss: 8527.283203\n",
      "Train Epoch: 39 [114944/225000 (51%)] Loss: 8276.863281\n",
      "Train Epoch: 39 [119040/225000 (53%)] Loss: 8309.130859\n",
      "Train Epoch: 39 [123136/225000 (55%)] Loss: 8495.414062\n",
      "Train Epoch: 39 [127232/225000 (57%)] Loss: 8555.570312\n",
      "Train Epoch: 39 [131328/225000 (58%)] Loss: 8293.039062\n",
      "Train Epoch: 39 [135424/225000 (60%)] Loss: 8571.683594\n",
      "Train Epoch: 39 [139520/225000 (62%)] Loss: 8303.806641\n",
      "Train Epoch: 39 [143616/225000 (64%)] Loss: 8324.582031\n",
      "Train Epoch: 39 [147712/225000 (66%)] Loss: 8344.423828\n",
      "Train Epoch: 39 [151808/225000 (67%)] Loss: 8776.833984\n",
      "Train Epoch: 39 [155904/225000 (69%)] Loss: 8363.675781\n",
      "Train Epoch: 39 [160000/225000 (71%)] Loss: 8333.781250\n",
      "Train Epoch: 39 [164096/225000 (73%)] Loss: 8420.988281\n",
      "Train Epoch: 39 [168192/225000 (75%)] Loss: 8357.625000\n",
      "Train Epoch: 39 [172288/225000 (77%)] Loss: 8343.314453\n",
      "Train Epoch: 39 [176384/225000 (78%)] Loss: 8422.865234\n",
      "Train Epoch: 39 [180480/225000 (80%)] Loss: 8438.718750\n",
      "Train Epoch: 39 [184576/225000 (82%)] Loss: 8515.488281\n",
      "Train Epoch: 39 [188672/225000 (84%)] Loss: 8488.070312\n",
      "Train Epoch: 39 [192768/225000 (86%)] Loss: 8500.044922\n",
      "Train Epoch: 39 [196864/225000 (87%)] Loss: 8706.021484\n",
      "Train Epoch: 39 [200960/225000 (89%)] Loss: 8550.804688\n",
      "Train Epoch: 39 [205056/225000 (91%)] Loss: 8649.324219\n",
      "Train Epoch: 39 [209152/225000 (93%)] Loss: 8716.064453\n",
      "Train Epoch: 39 [213248/225000 (95%)] Loss: 8379.689453\n",
      "Train Epoch: 39 [217344/225000 (97%)] Loss: 8619.089844\n",
      "Train Epoch: 39 [221440/225000 (98%)] Loss: 8568.847656\n",
      "    epoch          : 39\n",
      "    loss           : 8452.278568063852\n",
      "    val_loss       : 8423.354253612002\n",
      "Train Epoch: 40 [256/225000 (0%)] Loss: 8457.568359\n",
      "Train Epoch: 40 [4352/225000 (2%)] Loss: 8395.419922\n",
      "Train Epoch: 40 [8448/225000 (4%)] Loss: 8369.554688\n",
      "Train Epoch: 40 [12544/225000 (6%)] Loss: 8511.363281\n",
      "Train Epoch: 40 [16640/225000 (7%)] Loss: 8502.263672\n",
      "Train Epoch: 40 [20736/225000 (9%)] Loss: 8405.832031\n",
      "Train Epoch: 40 [24832/225000 (11%)] Loss: 8384.962891\n",
      "Train Epoch: 40 [28928/225000 (13%)] Loss: 8218.244141\n",
      "Train Epoch: 40 [33024/225000 (15%)] Loss: 8287.236328\n",
      "Train Epoch: 40 [37120/225000 (16%)] Loss: 8543.322266\n",
      "Train Epoch: 40 [41216/225000 (18%)] Loss: 8656.556641\n",
      "Train Epoch: 40 [45312/225000 (20%)] Loss: 8376.826172\n",
      "Train Epoch: 40 [49408/225000 (22%)] Loss: 8682.386719\n",
      "Train Epoch: 40 [53504/225000 (24%)] Loss: 8228.244141\n",
      "Train Epoch: 40 [57600/225000 (26%)] Loss: 8539.765625\n",
      "Train Epoch: 40 [61696/225000 (27%)] Loss: 8499.367188\n",
      "Train Epoch: 40 [65792/225000 (29%)] Loss: 8298.814453\n",
      "Train Epoch: 40 [69888/225000 (31%)] Loss: 8425.259766\n",
      "Train Epoch: 40 [73984/225000 (33%)] Loss: 8255.728516\n",
      "Train Epoch: 40 [78080/225000 (35%)] Loss: 8445.335938\n",
      "Train Epoch: 40 [82176/225000 (37%)] Loss: 8583.882812\n",
      "Train Epoch: 40 [86272/225000 (38%)] Loss: 8247.916016\n",
      "Train Epoch: 40 [90368/225000 (40%)] Loss: 8422.025391\n",
      "Train Epoch: 40 [94464/225000 (42%)] Loss: 8582.525391\n",
      "Train Epoch: 40 [98560/225000 (44%)] Loss: 8499.425781\n",
      "Train Epoch: 40 [102656/225000 (46%)] Loss: 8351.197266\n",
      "Train Epoch: 40 [106752/225000 (47%)] Loss: 8351.406250\n",
      "Train Epoch: 40 [110848/225000 (49%)] Loss: 8473.318359\n",
      "Train Epoch: 40 [114944/225000 (51%)] Loss: 8268.195312\n",
      "Train Epoch: 40 [119040/225000 (53%)] Loss: 8564.644531\n",
      "Train Epoch: 40 [123136/225000 (55%)] Loss: 8357.640625\n",
      "Train Epoch: 40 [127232/225000 (57%)] Loss: 8321.027344\n",
      "Train Epoch: 40 [131328/225000 (58%)] Loss: 8276.119141\n",
      "Train Epoch: 40 [135424/225000 (60%)] Loss: 8495.791016\n",
      "Train Epoch: 40 [139520/225000 (62%)] Loss: 8483.494141\n",
      "Train Epoch: 40 [143616/225000 (64%)] Loss: 8355.599609\n",
      "Train Epoch: 40 [147712/225000 (66%)] Loss: 8380.679688\n",
      "Train Epoch: 40 [151808/225000 (67%)] Loss: 8414.326172\n",
      "Train Epoch: 40 [155904/225000 (69%)] Loss: 8306.035156\n",
      "Train Epoch: 40 [160000/225000 (71%)] Loss: 8594.292969\n",
      "Train Epoch: 40 [164096/225000 (73%)] Loss: 8614.162109\n",
      "Train Epoch: 40 [168192/225000 (75%)] Loss: 8593.519531\n",
      "Train Epoch: 40 [172288/225000 (77%)] Loss: 8494.705078\n",
      "Train Epoch: 40 [176384/225000 (78%)] Loss: 8615.539062\n",
      "Train Epoch: 40 [180480/225000 (80%)] Loss: 8228.878906\n",
      "Train Epoch: 40 [184576/225000 (82%)] Loss: 8496.583984\n",
      "Train Epoch: 40 [188672/225000 (84%)] Loss: 8262.781250\n",
      "Train Epoch: 40 [192768/225000 (86%)] Loss: 8412.656250\n",
      "Train Epoch: 40 [196864/225000 (87%)] Loss: 8518.037109\n",
      "Train Epoch: 40 [200960/225000 (89%)] Loss: 8424.632812\n",
      "Train Epoch: 40 [205056/225000 (91%)] Loss: 8499.650391\n",
      "Train Epoch: 40 [209152/225000 (93%)] Loss: 8372.003906\n",
      "Train Epoch: 40 [213248/225000 (95%)] Loss: 8375.144531\n",
      "Train Epoch: 40 [217344/225000 (97%)] Loss: 8338.392578\n",
      "Train Epoch: 40 [221440/225000 (98%)] Loss: 8473.494141\n",
      "    epoch          : 40\n",
      "    loss           : 8429.828773819681\n",
      "    val_loss       : 8379.129146343592\n",
      "Train Epoch: 41 [256/225000 (0%)] Loss: 8281.537109\n",
      "Train Epoch: 41 [4352/225000 (2%)] Loss: 8412.048828\n",
      "Train Epoch: 41 [8448/225000 (4%)] Loss: 8417.876953\n",
      "Train Epoch: 41 [12544/225000 (6%)] Loss: 8381.527344\n",
      "Train Epoch: 41 [16640/225000 (7%)] Loss: 8582.470703\n",
      "Train Epoch: 41 [20736/225000 (9%)] Loss: 8302.330078\n",
      "Train Epoch: 41 [24832/225000 (11%)] Loss: 8473.035156\n",
      "Train Epoch: 41 [28928/225000 (13%)] Loss: 8280.152344\n",
      "Train Epoch: 41 [33024/225000 (15%)] Loss: 8415.017578\n",
      "Train Epoch: 41 [37120/225000 (16%)] Loss: 8546.935547\n",
      "Train Epoch: 41 [41216/225000 (18%)] Loss: 8404.849609\n",
      "Train Epoch: 41 [45312/225000 (20%)] Loss: 8458.183594\n",
      "Train Epoch: 41 [49408/225000 (22%)] Loss: 8431.609375\n",
      "Train Epoch: 41 [53504/225000 (24%)] Loss: 8440.197266\n",
      "Train Epoch: 41 [57600/225000 (26%)] Loss: 8276.269531\n",
      "Train Epoch: 41 [61696/225000 (27%)] Loss: 8528.714844\n",
      "Train Epoch: 41 [65792/225000 (29%)] Loss: 8426.449219\n",
      "Train Epoch: 41 [69888/225000 (31%)] Loss: 8194.623047\n",
      "Train Epoch: 41 [73984/225000 (33%)] Loss: 8403.714844\n",
      "Train Epoch: 41 [78080/225000 (35%)] Loss: 8422.917969\n",
      "Train Epoch: 41 [82176/225000 (37%)] Loss: 8469.003906\n",
      "Train Epoch: 41 [86272/225000 (38%)] Loss: 8371.812500\n",
      "Train Epoch: 41 [90368/225000 (40%)] Loss: 8236.876953\n",
      "Train Epoch: 41 [94464/225000 (42%)] Loss: 8501.296875\n",
      "Train Epoch: 41 [98560/225000 (44%)] Loss: 8531.791016\n",
      "Train Epoch: 41 [102656/225000 (46%)] Loss: 8357.511719\n",
      "Train Epoch: 41 [106752/225000 (47%)] Loss: 8225.876953\n",
      "Train Epoch: 41 [110848/225000 (49%)] Loss: 8568.244141\n",
      "Train Epoch: 41 [114944/225000 (51%)] Loss: 8453.742188\n",
      "Train Epoch: 41 [119040/225000 (53%)] Loss: 8345.308594\n",
      "Train Epoch: 41 [123136/225000 (55%)] Loss: 8453.224609\n",
      "Train Epoch: 41 [127232/225000 (57%)] Loss: 8259.451172\n",
      "Train Epoch: 41 [131328/225000 (58%)] Loss: 8355.363281\n",
      "Train Epoch: 41 [135424/225000 (60%)] Loss: 8679.589844\n",
      "Train Epoch: 41 [139520/225000 (62%)] Loss: 8565.675781\n",
      "Train Epoch: 41 [143616/225000 (64%)] Loss: 8378.271484\n",
      "Train Epoch: 41 [147712/225000 (66%)] Loss: 8463.218750\n",
      "Train Epoch: 41 [151808/225000 (67%)] Loss: 8453.900391\n",
      "Train Epoch: 41 [155904/225000 (69%)] Loss: 8367.490234\n",
      "Train Epoch: 41 [160000/225000 (71%)] Loss: 8289.119141\n",
      "Train Epoch: 41 [164096/225000 (73%)] Loss: 8474.593750\n",
      "Train Epoch: 41 [168192/225000 (75%)] Loss: 8431.003906\n",
      "Train Epoch: 41 [172288/225000 (77%)] Loss: 8369.857422\n",
      "Train Epoch: 41 [176384/225000 (78%)] Loss: 8324.183594\n",
      "Train Epoch: 41 [180480/225000 (80%)] Loss: 8324.826172\n",
      "Train Epoch: 41 [184576/225000 (82%)] Loss: 8508.980469\n",
      "Train Epoch: 41 [188672/225000 (84%)] Loss: 8272.638672\n",
      "Train Epoch: 41 [192768/225000 (86%)] Loss: 8427.511719\n",
      "Train Epoch: 41 [196864/225000 (87%)] Loss: 8615.769531\n",
      "Train Epoch: 41 [200960/225000 (89%)] Loss: 8319.527344\n",
      "Train Epoch: 41 [205056/225000 (91%)] Loss: 8343.845703\n",
      "Train Epoch: 41 [209152/225000 (93%)] Loss: 8577.865234\n",
      "Train Epoch: 41 [213248/225000 (95%)] Loss: 8325.867188\n",
      "Train Epoch: 41 [217344/225000 (97%)] Loss: 8632.480469\n",
      "Train Epoch: 41 [221440/225000 (98%)] Loss: 8286.531250\n",
      "    epoch          : 41\n",
      "    loss           : 8397.714150490614\n",
      "    val_loss       : 8358.675984557794\n",
      "Train Epoch: 42 [256/225000 (0%)] Loss: 8099.099609\n",
      "Train Epoch: 42 [4352/225000 (2%)] Loss: 8344.990234\n",
      "Train Epoch: 42 [8448/225000 (4%)] Loss: 8496.957031\n",
      "Train Epoch: 42 [12544/225000 (6%)] Loss: 8648.710938\n",
      "Train Epoch: 42 [16640/225000 (7%)] Loss: 8392.810547\n",
      "Train Epoch: 42 [20736/225000 (9%)] Loss: 8365.445312\n",
      "Train Epoch: 42 [24832/225000 (11%)] Loss: 8578.205078\n",
      "Train Epoch: 42 [28928/225000 (13%)] Loss: 8463.228516\n",
      "Train Epoch: 42 [33024/225000 (15%)] Loss: 8409.207031\n",
      "Train Epoch: 42 [37120/225000 (16%)] Loss: 8544.925781\n",
      "Train Epoch: 42 [41216/225000 (18%)] Loss: 8424.857422\n",
      "Train Epoch: 42 [45312/225000 (20%)] Loss: 8313.582031\n",
      "Train Epoch: 42 [49408/225000 (22%)] Loss: 8426.080078\n",
      "Train Epoch: 42 [53504/225000 (24%)] Loss: 8440.732422\n",
      "Train Epoch: 42 [57600/225000 (26%)] Loss: 8262.005859\n",
      "Train Epoch: 42 [61696/225000 (27%)] Loss: 8230.935547\n",
      "Train Epoch: 42 [65792/225000 (29%)] Loss: 8088.017578\n",
      "Train Epoch: 42 [69888/225000 (31%)] Loss: 8195.867188\n",
      "Train Epoch: 42 [73984/225000 (33%)] Loss: 8546.521484\n",
      "Train Epoch: 42 [78080/225000 (35%)] Loss: 8601.093750\n",
      "Train Epoch: 42 [82176/225000 (37%)] Loss: 8232.955078\n",
      "Train Epoch: 42 [86272/225000 (38%)] Loss: 8276.570312\n",
      "Train Epoch: 42 [90368/225000 (40%)] Loss: 8612.824219\n",
      "Train Epoch: 42 [94464/225000 (42%)] Loss: 8188.687500\n",
      "Train Epoch: 42 [98560/225000 (44%)] Loss: 8488.707031\n",
      "Train Epoch: 42 [102656/225000 (46%)] Loss: 8669.082031\n",
      "Train Epoch: 42 [106752/225000 (47%)] Loss: 8309.484375\n",
      "Train Epoch: 42 [110848/225000 (49%)] Loss: 8316.158203\n",
      "Train Epoch: 42 [114944/225000 (51%)] Loss: 8316.105469\n",
      "Train Epoch: 42 [119040/225000 (53%)] Loss: 8384.306641\n",
      "Train Epoch: 42 [123136/225000 (55%)] Loss: 8429.394531\n",
      "Train Epoch: 42 [127232/225000 (57%)] Loss: 8502.160156\n",
      "Train Epoch: 42 [131328/225000 (58%)] Loss: 8430.470703\n",
      "Train Epoch: 42 [135424/225000 (60%)] Loss: 8456.611328\n",
      "Train Epoch: 42 [139520/225000 (62%)] Loss: 8621.982422\n",
      "Train Epoch: 42 [143616/225000 (64%)] Loss: 8326.705078\n",
      "Train Epoch: 42 [147712/225000 (66%)] Loss: 8441.890625\n",
      "Train Epoch: 42 [151808/225000 (67%)] Loss: 8259.066406\n",
      "Train Epoch: 42 [155904/225000 (69%)] Loss: 8292.382812\n",
      "Train Epoch: 42 [160000/225000 (71%)] Loss: 8342.539062\n",
      "Train Epoch: 42 [164096/225000 (73%)] Loss: 8340.900391\n",
      "Train Epoch: 42 [168192/225000 (75%)] Loss: 8460.783203\n",
      "Train Epoch: 42 [172288/225000 (77%)] Loss: 8429.345703\n",
      "Train Epoch: 42 [176384/225000 (78%)] Loss: 8397.410156\n",
      "Train Epoch: 42 [180480/225000 (80%)] Loss: 8307.500000\n",
      "Train Epoch: 42 [184576/225000 (82%)] Loss: 8419.445312\n",
      "Train Epoch: 42 [188672/225000 (84%)] Loss: 8337.640625\n",
      "Train Epoch: 42 [192768/225000 (86%)] Loss: 8595.246094\n",
      "Train Epoch: 42 [196864/225000 (87%)] Loss: 8408.685547\n",
      "Train Epoch: 42 [200960/225000 (89%)] Loss: 8441.150391\n",
      "Train Epoch: 42 [205056/225000 (91%)] Loss: 8290.041016\n",
      "Train Epoch: 42 [209152/225000 (93%)] Loss: 8143.732422\n",
      "Train Epoch: 42 [213248/225000 (95%)] Loss: 8316.472656\n",
      "Train Epoch: 42 [217344/225000 (97%)] Loss: 8246.349609\n",
      "Train Epoch: 42 [221440/225000 (98%)] Loss: 8556.769531\n",
      "    epoch          : 42\n",
      "    loss           : 8372.34217572348\n",
      "    val_loss       : 8329.36477313662\n",
      "Train Epoch: 43 [256/225000 (0%)] Loss: 8394.910156\n",
      "Train Epoch: 43 [4352/225000 (2%)] Loss: 8500.863281\n",
      "Train Epoch: 43 [8448/225000 (4%)] Loss: 8352.888672\n",
      "Train Epoch: 43 [12544/225000 (6%)] Loss: 8445.830078\n",
      "Train Epoch: 43 [16640/225000 (7%)] Loss: 8529.898438\n",
      "Train Epoch: 43 [20736/225000 (9%)] Loss: 8216.861328\n",
      "Train Epoch: 43 [24832/225000 (11%)] Loss: 8520.851562\n",
      "Train Epoch: 43 [28928/225000 (13%)] Loss: 8209.554688\n",
      "Train Epoch: 43 [33024/225000 (15%)] Loss: 8324.464844\n",
      "Train Epoch: 43 [37120/225000 (16%)] Loss: 8577.324219\n",
      "Train Epoch: 43 [41216/225000 (18%)] Loss: 8467.130859\n",
      "Train Epoch: 43 [45312/225000 (20%)] Loss: 8396.667969\n",
      "Train Epoch: 43 [49408/225000 (22%)] Loss: 8430.525391\n",
      "Train Epoch: 43 [53504/225000 (24%)] Loss: 8179.345703\n",
      "Train Epoch: 43 [57600/225000 (26%)] Loss: 8374.330078\n",
      "Train Epoch: 43 [61696/225000 (27%)] Loss: 8326.974609\n",
      "Train Epoch: 43 [65792/225000 (29%)] Loss: 8288.597656\n",
      "Train Epoch: 43 [69888/225000 (31%)] Loss: 8360.125000\n",
      "Train Epoch: 43 [73984/225000 (33%)] Loss: 8320.361328\n",
      "Train Epoch: 43 [78080/225000 (35%)] Loss: 8429.726562\n",
      "Train Epoch: 43 [82176/225000 (37%)] Loss: 8041.755859\n",
      "Train Epoch: 43 [86272/225000 (38%)] Loss: 8273.560547\n",
      "Train Epoch: 43 [90368/225000 (40%)] Loss: 8309.050781\n",
      "Train Epoch: 43 [94464/225000 (42%)] Loss: 8188.980469\n",
      "Train Epoch: 43 [98560/225000 (44%)] Loss: 8333.277344\n",
      "Train Epoch: 43 [102656/225000 (46%)] Loss: 8337.533203\n",
      "Train Epoch: 43 [106752/225000 (47%)] Loss: 8349.958984\n",
      "Train Epoch: 43 [110848/225000 (49%)] Loss: 8285.253906\n",
      "Train Epoch: 43 [114944/225000 (51%)] Loss: 8409.111328\n",
      "Train Epoch: 43 [119040/225000 (53%)] Loss: 8211.187500\n",
      "Train Epoch: 43 [123136/225000 (55%)] Loss: 8433.953125\n",
      "Train Epoch: 43 [127232/225000 (57%)] Loss: 8623.042969\n",
      "Train Epoch: 43 [131328/225000 (58%)] Loss: 8410.523438\n",
      "Train Epoch: 43 [135424/225000 (60%)] Loss: 8409.304688\n",
      "Train Epoch: 43 [139520/225000 (62%)] Loss: 8304.878906\n",
      "Train Epoch: 43 [143616/225000 (64%)] Loss: 8349.609375\n",
      "Train Epoch: 43 [147712/225000 (66%)] Loss: 8278.484375\n",
      "Train Epoch: 43 [151808/225000 (67%)] Loss: 8382.539062\n",
      "Train Epoch: 43 [155904/225000 (69%)] Loss: 8252.085938\n",
      "Train Epoch: 43 [160000/225000 (71%)] Loss: 8399.271484\n",
      "Train Epoch: 43 [164096/225000 (73%)] Loss: 8327.058594\n",
      "Train Epoch: 43 [168192/225000 (75%)] Loss: 8248.703125\n",
      "Train Epoch: 43 [172288/225000 (77%)] Loss: 8190.351562\n",
      "Train Epoch: 43 [176384/225000 (78%)] Loss: 8488.271484\n",
      "Train Epoch: 43 [180480/225000 (80%)] Loss: 8410.185547\n",
      "Train Epoch: 43 [184576/225000 (82%)] Loss: 8346.873047\n",
      "Train Epoch: 43 [188672/225000 (84%)] Loss: 8385.519531\n",
      "Train Epoch: 43 [192768/225000 (86%)] Loss: 8371.603516\n",
      "Train Epoch: 43 [196864/225000 (87%)] Loss: 8097.222656\n",
      "Train Epoch: 43 [200960/225000 (89%)] Loss: 8212.785156\n",
      "Train Epoch: 43 [205056/225000 (91%)] Loss: 8345.816406\n",
      "Train Epoch: 43 [209152/225000 (93%)] Loss: 8470.144531\n",
      "Train Epoch: 43 [213248/225000 (95%)] Loss: 8344.292969\n",
      "Train Epoch: 43 [217344/225000 (97%)] Loss: 8312.052734\n",
      "Train Epoch: 43 [221440/225000 (98%)] Loss: 8477.689453\n",
      "    epoch          : 43\n",
      "    loss           : 8373.237133594639\n",
      "    val_loss       : 8551.424544121537\n",
      "Train Epoch: 44 [256/225000 (0%)] Loss: 8349.287109\n",
      "Train Epoch: 44 [4352/225000 (2%)] Loss: 8482.808594\n",
      "Train Epoch: 44 [8448/225000 (4%)] Loss: 8359.847656\n",
      "Train Epoch: 44 [12544/225000 (6%)] Loss: 8384.259766\n",
      "Train Epoch: 44 [16640/225000 (7%)] Loss: 8359.009766\n",
      "Train Epoch: 44 [20736/225000 (9%)] Loss: 8339.728516\n",
      "Train Epoch: 44 [24832/225000 (11%)] Loss: 8511.218750\n",
      "Train Epoch: 44 [28928/225000 (13%)] Loss: 8531.642578\n",
      "Train Epoch: 44 [33024/225000 (15%)] Loss: 8286.726562\n",
      "Train Epoch: 44 [37120/225000 (16%)] Loss: 8129.394531\n",
      "Train Epoch: 44 [41216/225000 (18%)] Loss: 8192.175781\n",
      "Train Epoch: 44 [45312/225000 (20%)] Loss: 8335.304688\n",
      "Train Epoch: 44 [49408/225000 (22%)] Loss: 8305.730469\n",
      "Train Epoch: 44 [53504/225000 (24%)] Loss: 8312.833984\n",
      "Train Epoch: 44 [57600/225000 (26%)] Loss: 8189.552734\n",
      "Train Epoch: 44 [61696/225000 (27%)] Loss: 8323.355469\n",
      "Train Epoch: 44 [65792/225000 (29%)] Loss: 8409.265625\n",
      "Train Epoch: 44 [69888/225000 (31%)] Loss: 8218.435547\n",
      "Train Epoch: 44 [73984/225000 (33%)] Loss: 8286.966797\n",
      "Train Epoch: 44 [78080/225000 (35%)] Loss: 8157.431641\n",
      "Train Epoch: 44 [82176/225000 (37%)] Loss: 8542.378906\n",
      "Train Epoch: 44 [86272/225000 (38%)] Loss: 8336.802734\n",
      "Train Epoch: 44 [90368/225000 (40%)] Loss: 8388.595703\n",
      "Train Epoch: 44 [94464/225000 (42%)] Loss: 8346.244141\n",
      "Train Epoch: 44 [98560/225000 (44%)] Loss: 8364.498047\n",
      "Train Epoch: 44 [102656/225000 (46%)] Loss: 8229.556641\n",
      "Train Epoch: 44 [106752/225000 (47%)] Loss: 8182.677734\n",
      "Train Epoch: 44 [110848/225000 (49%)] Loss: 8213.730469\n",
      "Train Epoch: 44 [114944/225000 (51%)] Loss: 8497.292969\n",
      "Train Epoch: 44 [119040/225000 (53%)] Loss: 8259.707031\n",
      "Train Epoch: 44 [123136/225000 (55%)] Loss: 8400.949219\n",
      "Train Epoch: 44 [127232/225000 (57%)] Loss: 8328.210938\n",
      "Train Epoch: 44 [131328/225000 (58%)] Loss: 8238.224609\n",
      "Train Epoch: 44 [135424/225000 (60%)] Loss: 8232.431641\n",
      "Train Epoch: 44 [139520/225000 (62%)] Loss: 8409.662109\n",
      "Train Epoch: 44 [143616/225000 (64%)] Loss: 8193.521484\n",
      "Train Epoch: 44 [147712/225000 (66%)] Loss: 8533.162109\n",
      "Train Epoch: 44 [151808/225000 (67%)] Loss: 8463.509766\n",
      "Train Epoch: 44 [155904/225000 (69%)] Loss: 8340.494141\n",
      "Train Epoch: 44 [160000/225000 (71%)] Loss: 8309.628906\n",
      "Train Epoch: 44 [164096/225000 (73%)] Loss: 8260.001953\n",
      "Train Epoch: 44 [168192/225000 (75%)] Loss: 8367.634766\n",
      "Train Epoch: 44 [172288/225000 (77%)] Loss: 8283.900391\n",
      "Train Epoch: 44 [176384/225000 (78%)] Loss: 8139.369141\n",
      "Train Epoch: 44 [180480/225000 (80%)] Loss: 8223.369141\n",
      "Train Epoch: 44 [184576/225000 (82%)] Loss: 8187.035156\n",
      "Train Epoch: 44 [188672/225000 (84%)] Loss: 8195.546875\n",
      "Train Epoch: 44 [192768/225000 (86%)] Loss: 8298.652344\n",
      "Train Epoch: 44 [196864/225000 (87%)] Loss: 8368.636719\n",
      "Train Epoch: 44 [200960/225000 (89%)] Loss: 8093.865234\n",
      "Train Epoch: 44 [205056/225000 (91%)] Loss: 8292.312500\n",
      "Train Epoch: 44 [209152/225000 (93%)] Loss: 8296.373047\n",
      "Train Epoch: 44 [213248/225000 (95%)] Loss: 8272.314453\n",
      "Train Epoch: 44 [217344/225000 (97%)] Loss: 8351.144531\n",
      "Train Epoch: 44 [221440/225000 (98%)] Loss: 8375.914062\n",
      "    epoch          : 44\n",
      "    loss           : 8346.039870191624\n",
      "    val_loss       : 8396.424831619068\n",
      "Train Epoch: 45 [256/225000 (0%)] Loss: 8273.927734\n",
      "Train Epoch: 45 [4352/225000 (2%)] Loss: 8524.998047\n",
      "Train Epoch: 45 [8448/225000 (4%)] Loss: 8251.113281\n",
      "Train Epoch: 45 [12544/225000 (6%)] Loss: 8017.560547\n",
      "Train Epoch: 45 [16640/225000 (7%)] Loss: 8336.921875\n",
      "Train Epoch: 45 [20736/225000 (9%)] Loss: 8304.572266\n",
      "Train Epoch: 45 [24832/225000 (11%)] Loss: 8266.507812\n",
      "Train Epoch: 45 [28928/225000 (13%)] Loss: 8051.779297\n",
      "Train Epoch: 45 [33024/225000 (15%)] Loss: 8422.845703\n",
      "Train Epoch: 45 [37120/225000 (16%)] Loss: 8507.707031\n",
      "Train Epoch: 45 [41216/225000 (18%)] Loss: 8288.351562\n",
      "Train Epoch: 45 [45312/225000 (20%)] Loss: 8260.357422\n",
      "Train Epoch: 45 [49408/225000 (22%)] Loss: 8290.960938\n",
      "Train Epoch: 45 [53504/225000 (24%)] Loss: 8357.972656\n",
      "Train Epoch: 45 [57600/225000 (26%)] Loss: 8116.289062\n",
      "Train Epoch: 45 [61696/225000 (27%)] Loss: 8260.160156\n",
      "Train Epoch: 45 [65792/225000 (29%)] Loss: 8476.783203\n",
      "Train Epoch: 45 [69888/225000 (31%)] Loss: 8240.794922\n",
      "Train Epoch: 45 [73984/225000 (33%)] Loss: 8420.195312\n",
      "Train Epoch: 45 [78080/225000 (35%)] Loss: 8191.851562\n",
      "Train Epoch: 45 [82176/225000 (37%)] Loss: 8374.058594\n",
      "Train Epoch: 45 [86272/225000 (38%)] Loss: 8341.724609\n",
      "Train Epoch: 45 [90368/225000 (40%)] Loss: 8320.455078\n",
      "Train Epoch: 45 [94464/225000 (42%)] Loss: 8449.005859\n",
      "Train Epoch: 45 [98560/225000 (44%)] Loss: 8232.000000\n",
      "Train Epoch: 45 [102656/225000 (46%)] Loss: 8391.195312\n",
      "Train Epoch: 45 [106752/225000 (47%)] Loss: 8316.777344\n",
      "Train Epoch: 45 [110848/225000 (49%)] Loss: 8393.810547\n",
      "Train Epoch: 45 [114944/225000 (51%)] Loss: 8234.033203\n",
      "Train Epoch: 45 [119040/225000 (53%)] Loss: 8334.822266\n",
      "Train Epoch: 45 [123136/225000 (55%)] Loss: 8280.365234\n",
      "Train Epoch: 45 [127232/225000 (57%)] Loss: 8311.669922\n",
      "Train Epoch: 45 [131328/225000 (58%)] Loss: 8223.259766\n",
      "Train Epoch: 45 [135424/225000 (60%)] Loss: 8283.953125\n",
      "Train Epoch: 45 [139520/225000 (62%)] Loss: 8291.482422\n",
      "Train Epoch: 45 [143616/225000 (64%)] Loss: 8180.125000\n",
      "Train Epoch: 45 [147712/225000 (66%)] Loss: 8050.906250\n",
      "Train Epoch: 45 [151808/225000 (67%)] Loss: 8489.757812\n",
      "Train Epoch: 45 [155904/225000 (69%)] Loss: 8194.000000\n",
      "Train Epoch: 45 [160000/225000 (71%)] Loss: 8267.634766\n",
      "Train Epoch: 45 [164096/225000 (73%)] Loss: 8307.339844\n",
      "Train Epoch: 45 [168192/225000 (75%)] Loss: 8087.146484\n",
      "Train Epoch: 45 [172288/225000 (77%)] Loss: 8333.509766\n",
      "Train Epoch: 45 [176384/225000 (78%)] Loss: 8541.533203\n",
      "Train Epoch: 45 [180480/225000 (80%)] Loss: 8324.121094\n",
      "Train Epoch: 45 [184576/225000 (82%)] Loss: 8292.068359\n",
      "Train Epoch: 45 [188672/225000 (84%)] Loss: 8198.423828\n",
      "Train Epoch: 45 [192768/225000 (86%)] Loss: 8266.593750\n",
      "Train Epoch: 45 [196864/225000 (87%)] Loss: 8300.458984\n",
      "Train Epoch: 45 [200960/225000 (89%)] Loss: 8072.402344\n",
      "Train Epoch: 45 [205056/225000 (91%)] Loss: 8389.621094\n",
      "Train Epoch: 45 [209152/225000 (93%)] Loss: 8331.306641\n",
      "Train Epoch: 45 [213248/225000 (95%)] Loss: 8300.046875\n",
      "Train Epoch: 45 [217344/225000 (97%)] Loss: 8317.380859\n",
      "Train Epoch: 45 [221440/225000 (98%)] Loss: 8456.546875\n",
      "    epoch          : 45\n",
      "    loss           : 8334.052714377132\n",
      "    val_loss       : 8263.152288760457\n",
      "Train Epoch: 46 [256/225000 (0%)] Loss: 8429.402344\n",
      "Train Epoch: 46 [4352/225000 (2%)] Loss: 8300.656250\n",
      "Train Epoch: 46 [8448/225000 (4%)] Loss: 8565.523438\n",
      "Train Epoch: 46 [12544/225000 (6%)] Loss: 8305.134766\n",
      "Train Epoch: 46 [16640/225000 (7%)] Loss: 8197.097656\n",
      "Train Epoch: 46 [20736/225000 (9%)] Loss: 8192.535156\n",
      "Train Epoch: 46 [24832/225000 (11%)] Loss: 8378.990234\n",
      "Train Epoch: 46 [28928/225000 (13%)] Loss: 8221.835938\n",
      "Train Epoch: 46 [33024/225000 (15%)] Loss: 8267.253906\n",
      "Train Epoch: 46 [37120/225000 (16%)] Loss: 8329.542969\n",
      "Train Epoch: 46 [41216/225000 (18%)] Loss: 8356.982422\n",
      "Train Epoch: 46 [45312/225000 (20%)] Loss: 8293.980469\n",
      "Train Epoch: 46 [49408/225000 (22%)] Loss: 8181.703125\n",
      "Train Epoch: 46 [53504/225000 (24%)] Loss: 8304.781250\n",
      "Train Epoch: 46 [57600/225000 (26%)] Loss: 8362.291016\n",
      "Train Epoch: 46 [61696/225000 (27%)] Loss: 8273.291016\n",
      "Train Epoch: 46 [65792/225000 (29%)] Loss: 8456.134766\n",
      "Train Epoch: 46 [69888/225000 (31%)] Loss: 8356.919922\n",
      "Train Epoch: 46 [73984/225000 (33%)] Loss: 8170.062500\n",
      "Train Epoch: 46 [78080/225000 (35%)] Loss: 8334.121094\n",
      "Train Epoch: 46 [82176/225000 (37%)] Loss: 8181.787109\n",
      "Train Epoch: 46 [86272/225000 (38%)] Loss: 8185.335938\n",
      "Train Epoch: 46 [90368/225000 (40%)] Loss: 8459.458984\n",
      "Train Epoch: 46 [94464/225000 (42%)] Loss: 8129.734375\n",
      "Train Epoch: 46 [98560/225000 (44%)] Loss: 8240.785156\n",
      "Train Epoch: 46 [102656/225000 (46%)] Loss: 8265.771484\n",
      "Train Epoch: 46 [106752/225000 (47%)] Loss: 8117.152344\n",
      "Train Epoch: 46 [110848/225000 (49%)] Loss: 8565.658203\n",
      "Train Epoch: 46 [114944/225000 (51%)] Loss: 8191.705078\n",
      "Train Epoch: 46 [119040/225000 (53%)] Loss: 8302.398438\n",
      "Train Epoch: 46 [123136/225000 (55%)] Loss: 8161.726562\n",
      "Train Epoch: 46 [127232/225000 (57%)] Loss: 8283.966797\n",
      "Train Epoch: 46 [131328/225000 (58%)] Loss: 8129.654297\n",
      "Train Epoch: 46 [135424/225000 (60%)] Loss: 8223.576172\n",
      "Train Epoch: 46 [139520/225000 (62%)] Loss: 7951.767578\n",
      "Train Epoch: 46 [143616/225000 (64%)] Loss: 8299.244141\n",
      "Train Epoch: 46 [147712/225000 (66%)] Loss: 8140.275391\n",
      "Train Epoch: 46 [151808/225000 (67%)] Loss: 8626.351562\n",
      "Train Epoch: 46 [155904/225000 (69%)] Loss: 8167.990234\n",
      "Train Epoch: 46 [160000/225000 (71%)] Loss: 8314.173828\n",
      "Train Epoch: 46 [164096/225000 (73%)] Loss: 8168.814453\n",
      "Train Epoch: 46 [168192/225000 (75%)] Loss: 8127.423828\n",
      "Train Epoch: 46 [172288/225000 (77%)] Loss: 8388.984375\n",
      "Train Epoch: 46 [176384/225000 (78%)] Loss: 8220.351562\n",
      "Train Epoch: 46 [180480/225000 (80%)] Loss: 8412.388672\n",
      "Train Epoch: 46 [184576/225000 (82%)] Loss: 8164.849609\n",
      "Train Epoch: 46 [188672/225000 (84%)] Loss: 8461.945312\n",
      "Train Epoch: 46 [192768/225000 (86%)] Loss: 8020.871094\n",
      "Train Epoch: 46 [196864/225000 (87%)] Loss: 8359.761719\n",
      "Train Epoch: 46 [200960/225000 (89%)] Loss: 8292.003906\n",
      "Train Epoch: 46 [205056/225000 (91%)] Loss: 8246.167969\n",
      "Train Epoch: 46 [209152/225000 (93%)] Loss: 8205.949219\n",
      "Train Epoch: 46 [213248/225000 (95%)] Loss: 8327.423828\n",
      "Train Epoch: 46 [217344/225000 (97%)] Loss: 8393.730469\n",
      "Train Epoch: 46 [221440/225000 (98%)] Loss: 8164.800781\n",
      "    epoch          : 46\n",
      "    loss           : 8280.819049301408\n",
      "    val_loss       : 8245.405664976763\n",
      "Train Epoch: 47 [256/225000 (0%)] Loss: 8132.519531\n",
      "Train Epoch: 47 [4352/225000 (2%)] Loss: 8356.494141\n",
      "Train Epoch: 47 [8448/225000 (4%)] Loss: 8213.730469\n",
      "Train Epoch: 47 [12544/225000 (6%)] Loss: 8328.390625\n",
      "Train Epoch: 47 [16640/225000 (7%)] Loss: 8316.833984\n",
      "Train Epoch: 47 [20736/225000 (9%)] Loss: 8329.945312\n",
      "Train Epoch: 47 [24832/225000 (11%)] Loss: 8106.718750\n",
      "Train Epoch: 47 [28928/225000 (13%)] Loss: 8235.361328\n",
      "Train Epoch: 47 [33024/225000 (15%)] Loss: 8321.804688\n",
      "Train Epoch: 47 [37120/225000 (16%)] Loss: 8287.781250\n",
      "Train Epoch: 47 [41216/225000 (18%)] Loss: 8107.138672\n",
      "Train Epoch: 47 [45312/225000 (20%)] Loss: 8334.382812\n",
      "Train Epoch: 47 [49408/225000 (22%)] Loss: 8156.921875\n",
      "Train Epoch: 47 [53504/225000 (24%)] Loss: 8225.396484\n",
      "Train Epoch: 47 [57600/225000 (26%)] Loss: 8280.955078\n",
      "Train Epoch: 47 [61696/225000 (27%)] Loss: 8104.013672\n",
      "Train Epoch: 47 [65792/225000 (29%)] Loss: 8168.544922\n",
      "Train Epoch: 47 [69888/225000 (31%)] Loss: 8367.826172\n",
      "Train Epoch: 47 [73984/225000 (33%)] Loss: 8145.150391\n",
      "Train Epoch: 47 [78080/225000 (35%)] Loss: 8320.455078\n",
      "Train Epoch: 47 [82176/225000 (37%)] Loss: 8243.970703\n",
      "Train Epoch: 47 [86272/225000 (38%)] Loss: 8053.347656\n",
      "Train Epoch: 47 [90368/225000 (40%)] Loss: 8385.324219\n",
      "Train Epoch: 47 [94464/225000 (42%)] Loss: 8194.052734\n",
      "Train Epoch: 47 [98560/225000 (44%)] Loss: 8237.390625\n",
      "Train Epoch: 47 [102656/225000 (46%)] Loss: 8230.792969\n",
      "Train Epoch: 47 [106752/225000 (47%)] Loss: 8178.474609\n",
      "Train Epoch: 47 [110848/225000 (49%)] Loss: 8177.525391\n",
      "Train Epoch: 47 [114944/225000 (51%)] Loss: 8068.544922\n",
      "Train Epoch: 47 [119040/225000 (53%)] Loss: 8134.625000\n",
      "Train Epoch: 47 [123136/225000 (55%)] Loss: 8431.068359\n",
      "Train Epoch: 47 [127232/225000 (57%)] Loss: 8202.634766\n",
      "Train Epoch: 47 [131328/225000 (58%)] Loss: 8129.779297\n",
      "Train Epoch: 47 [135424/225000 (60%)] Loss: 8161.414062\n",
      "Train Epoch: 47 [139520/225000 (62%)] Loss: 8239.150391\n",
      "Train Epoch: 47 [143616/225000 (64%)] Loss: 8358.478516\n",
      "Train Epoch: 47 [147712/225000 (66%)] Loss: 8284.369141\n",
      "Train Epoch: 47 [151808/225000 (67%)] Loss: 8202.523438\n",
      "Train Epoch: 47 [155904/225000 (69%)] Loss: 8180.011719\n",
      "Train Epoch: 47 [160000/225000 (71%)] Loss: 8283.724609\n",
      "Train Epoch: 47 [164096/225000 (73%)] Loss: 8252.949219\n",
      "Train Epoch: 47 [168192/225000 (75%)] Loss: 8166.294922\n",
      "Train Epoch: 47 [172288/225000 (77%)] Loss: 8254.742188\n",
      "Train Epoch: 47 [176384/225000 (78%)] Loss: 8306.650391\n",
      "Train Epoch: 47 [180480/225000 (80%)] Loss: 8265.083984\n",
      "Train Epoch: 47 [184576/225000 (82%)] Loss: 8333.373047\n",
      "Train Epoch: 47 [188672/225000 (84%)] Loss: 8447.558594\n",
      "Train Epoch: 47 [192768/225000 (86%)] Loss: 8213.972656\n",
      "Train Epoch: 47 [196864/225000 (87%)] Loss: 8326.990234\n",
      "Train Epoch: 47 [200960/225000 (89%)] Loss: 8103.843750\n",
      "Train Epoch: 47 [205056/225000 (91%)] Loss: 8099.748047\n",
      "Train Epoch: 47 [209152/225000 (93%)] Loss: 8207.830078\n",
      "Train Epoch: 47 [213248/225000 (95%)] Loss: 8286.466797\n",
      "Train Epoch: 47 [217344/225000 (97%)] Loss: 8252.101562\n",
      "Train Epoch: 47 [221440/225000 (98%)] Loss: 8289.412109\n",
      "    epoch          : 47\n",
      "    loss           : 8258.676837803967\n",
      "    val_loss       : 8219.946832277945\n",
      "Train Epoch: 48 [256/225000 (0%)] Loss: 8343.279297\n",
      "Train Epoch: 48 [4352/225000 (2%)] Loss: 8301.990234\n",
      "Train Epoch: 48 [8448/225000 (4%)] Loss: 8128.710938\n",
      "Train Epoch: 48 [12544/225000 (6%)] Loss: 8246.544922\n",
      "Train Epoch: 48 [16640/225000 (7%)] Loss: 8493.556641\n",
      "Train Epoch: 48 [20736/225000 (9%)] Loss: 8253.419922\n",
      "Train Epoch: 48 [24832/225000 (11%)] Loss: 8218.564453\n",
      "Train Epoch: 48 [28928/225000 (13%)] Loss: 8062.248047\n",
      "Train Epoch: 48 [33024/225000 (15%)] Loss: 8241.517578\n",
      "Train Epoch: 48 [37120/225000 (16%)] Loss: 8361.849609\n",
      "Train Epoch: 48 [41216/225000 (18%)] Loss: 8258.412109\n",
      "Train Epoch: 48 [45312/225000 (20%)] Loss: 8256.003906\n",
      "Train Epoch: 48 [49408/225000 (22%)] Loss: 8279.214844\n",
      "Train Epoch: 48 [53504/225000 (24%)] Loss: 8277.675781\n",
      "Train Epoch: 48 [57600/225000 (26%)] Loss: 8215.728516\n",
      "Train Epoch: 48 [61696/225000 (27%)] Loss: 8299.808594\n",
      "Train Epoch: 48 [65792/225000 (29%)] Loss: 8227.318359\n",
      "Train Epoch: 48 [69888/225000 (31%)] Loss: 8142.357422\n",
      "Train Epoch: 48 [73984/225000 (33%)] Loss: 8172.439453\n",
      "Train Epoch: 48 [78080/225000 (35%)] Loss: 8205.621094\n",
      "Train Epoch: 48 [82176/225000 (37%)] Loss: 8204.099609\n",
      "Train Epoch: 48 [86272/225000 (38%)] Loss: 8164.974609\n",
      "Train Epoch: 48 [90368/225000 (40%)] Loss: 8262.777344\n",
      "Train Epoch: 48 [94464/225000 (42%)] Loss: 8177.539062\n",
      "Train Epoch: 48 [98560/225000 (44%)] Loss: 8365.177734\n",
      "Train Epoch: 48 [102656/225000 (46%)] Loss: 8249.394531\n",
      "Train Epoch: 48 [106752/225000 (47%)] Loss: 8327.367188\n",
      "Train Epoch: 48 [110848/225000 (49%)] Loss: 8442.173828\n",
      "Train Epoch: 48 [114944/225000 (51%)] Loss: 8498.058594\n",
      "Train Epoch: 48 [119040/225000 (53%)] Loss: 8341.212891\n",
      "Train Epoch: 48 [123136/225000 (55%)] Loss: 8208.126953\n",
      "Train Epoch: 48 [127232/225000 (57%)] Loss: 8248.873047\n",
      "Train Epoch: 48 [131328/225000 (58%)] Loss: 7944.916016\n",
      "Train Epoch: 48 [135424/225000 (60%)] Loss: 8197.101562\n",
      "Train Epoch: 48 [139520/225000 (62%)] Loss: 8182.068359\n",
      "Train Epoch: 48 [143616/225000 (64%)] Loss: 8407.789062\n",
      "Train Epoch: 48 [147712/225000 (66%)] Loss: 8111.707031\n",
      "Train Epoch: 48 [151808/225000 (67%)] Loss: 8218.568359\n",
      "Train Epoch: 48 [155904/225000 (69%)] Loss: 8338.230469\n",
      "Train Epoch: 48 [160000/225000 (71%)] Loss: 8305.759766\n",
      "Train Epoch: 48 [164096/225000 (73%)] Loss: 8429.144531\n",
      "Train Epoch: 48 [168192/225000 (75%)] Loss: 8375.921875\n",
      "Train Epoch: 48 [172288/225000 (77%)] Loss: 8361.468750\n",
      "Train Epoch: 48 [176384/225000 (78%)] Loss: 8211.755859\n",
      "Train Epoch: 48 [180480/225000 (80%)] Loss: 8037.634766\n",
      "Train Epoch: 48 [184576/225000 (82%)] Loss: 8048.445312\n",
      "Train Epoch: 48 [188672/225000 (84%)] Loss: 8307.931641\n",
      "Train Epoch: 48 [192768/225000 (86%)] Loss: 8188.923828\n",
      "Train Epoch: 48 [196864/225000 (87%)] Loss: 8302.132812\n",
      "Train Epoch: 48 [200960/225000 (89%)] Loss: 8352.996094\n",
      "Train Epoch: 48 [205056/225000 (91%)] Loss: 8113.070312\n",
      "Train Epoch: 48 [209152/225000 (93%)] Loss: 8300.017578\n",
      "Train Epoch: 48 [213248/225000 (95%)] Loss: 8267.095703\n",
      "Train Epoch: 48 [217344/225000 (97%)] Loss: 8160.158203\n",
      "Train Epoch: 48 [221440/225000 (98%)] Loss: 8254.507812\n",
      "    epoch          : 48\n",
      "    loss           : 8313.455339208262\n",
      "    val_loss       : 8209.630007006988\n",
      "Train Epoch: 49 [256/225000 (0%)] Loss: 8302.158203\n",
      "Train Epoch: 49 [4352/225000 (2%)] Loss: 8145.068359\n",
      "Train Epoch: 49 [8448/225000 (4%)] Loss: 8171.628906\n",
      "Train Epoch: 49 [12544/225000 (6%)] Loss: 8369.330078\n",
      "Train Epoch: 49 [16640/225000 (7%)] Loss: 8265.095703\n",
      "Train Epoch: 49 [20736/225000 (9%)] Loss: 8238.845703\n",
      "Train Epoch: 49 [24832/225000 (11%)] Loss: 8149.271484\n",
      "Train Epoch: 49 [28928/225000 (13%)] Loss: 8148.753906\n",
      "Train Epoch: 49 [33024/225000 (15%)] Loss: 8262.382812\n",
      "Train Epoch: 49 [37120/225000 (16%)] Loss: 8360.308594\n",
      "Train Epoch: 49 [41216/225000 (18%)] Loss: 8163.304688\n",
      "Train Epoch: 49 [45312/225000 (20%)] Loss: 8019.287109\n",
      "Train Epoch: 49 [49408/225000 (22%)] Loss: 8151.443359\n",
      "Train Epoch: 49 [53504/225000 (24%)] Loss: 8179.521484\n",
      "Train Epoch: 49 [57600/225000 (26%)] Loss: 8113.255859\n",
      "Train Epoch: 49 [61696/225000 (27%)] Loss: 8406.126953\n",
      "Train Epoch: 49 [65792/225000 (29%)] Loss: 8117.296875\n",
      "Train Epoch: 49 [69888/225000 (31%)] Loss: 8134.441406\n",
      "Train Epoch: 49 [73984/225000 (33%)] Loss: 8238.113281\n",
      "Train Epoch: 49 [78080/225000 (35%)] Loss: 8290.650391\n",
      "Train Epoch: 49 [82176/225000 (37%)] Loss: 8084.792969\n",
      "Train Epoch: 49 [86272/225000 (38%)] Loss: 8107.033203\n",
      "Train Epoch: 49 [90368/225000 (40%)] Loss: 8149.080078\n",
      "Train Epoch: 49 [94464/225000 (42%)] Loss: 8250.900391\n",
      "Train Epoch: 49 [98560/225000 (44%)] Loss: 8208.523438\n",
      "Train Epoch: 49 [102656/225000 (46%)] Loss: 8110.958984\n",
      "Train Epoch: 49 [106752/225000 (47%)] Loss: 8285.572266\n",
      "Train Epoch: 49 [110848/225000 (49%)] Loss: 8347.699219\n",
      "Train Epoch: 49 [114944/225000 (51%)] Loss: 8501.939453\n",
      "Train Epoch: 49 [119040/225000 (53%)] Loss: 8285.736328\n",
      "Train Epoch: 49 [123136/225000 (55%)] Loss: 8284.019531\n",
      "Train Epoch: 49 [127232/225000 (57%)] Loss: 8176.740234\n",
      "Train Epoch: 49 [131328/225000 (58%)] Loss: 8338.896484\n",
      "Train Epoch: 49 [135424/225000 (60%)] Loss: 8188.572266\n",
      "Train Epoch: 49 [139520/225000 (62%)] Loss: 8119.392578\n",
      "Train Epoch: 49 [143616/225000 (64%)] Loss: 8159.107422\n",
      "Train Epoch: 49 [147712/225000 (66%)] Loss: 8246.457031\n",
      "Train Epoch: 49 [151808/225000 (67%)] Loss: 8082.990234\n",
      "Train Epoch: 49 [155904/225000 (69%)] Loss: 8263.384766\n",
      "Train Epoch: 49 [160000/225000 (71%)] Loss: 8154.609375\n",
      "Train Epoch: 49 [164096/225000 (73%)] Loss: 8305.257812\n",
      "Train Epoch: 49 [168192/225000 (75%)] Loss: 8106.093750\n",
      "Train Epoch: 49 [172288/225000 (77%)] Loss: 8130.498047\n",
      "Train Epoch: 49 [176384/225000 (78%)] Loss: 8164.490234\n",
      "Train Epoch: 49 [180480/225000 (80%)] Loss: 8354.169922\n",
      "Train Epoch: 49 [184576/225000 (82%)] Loss: 8105.416016\n",
      "Train Epoch: 49 [188672/225000 (84%)] Loss: 8214.544922\n",
      "Train Epoch: 49 [192768/225000 (86%)] Loss: 8244.746094\n",
      "Train Epoch: 49 [196864/225000 (87%)] Loss: 8129.533203\n",
      "Train Epoch: 49 [200960/225000 (89%)] Loss: 8114.662109\n",
      "Train Epoch: 49 [205056/225000 (91%)] Loss: 8347.300781\n",
      "Train Epoch: 49 [209152/225000 (93%)] Loss: 8119.367188\n",
      "Train Epoch: 49 [213248/225000 (95%)] Loss: 8147.042969\n",
      "Train Epoch: 49 [217344/225000 (97%)] Loss: 8268.804688\n",
      "Train Epoch: 49 [221440/225000 (98%)] Loss: 8201.470703\n",
      "    epoch          : 49\n",
      "    loss           : 8246.486650312856\n",
      "    val_loss       : 8193.73580311026\n",
      "Train Epoch: 50 [256/225000 (0%)] Loss: 8225.156250\n",
      "Train Epoch: 50 [4352/225000 (2%)] Loss: 8508.791016\n",
      "Train Epoch: 50 [8448/225000 (4%)] Loss: 8210.919922\n",
      "Train Epoch: 50 [12544/225000 (6%)] Loss: 8165.669922\n",
      "Train Epoch: 50 [16640/225000 (7%)] Loss: 8409.144531\n",
      "Train Epoch: 50 [20736/225000 (9%)] Loss: 8225.154297\n",
      "Train Epoch: 50 [24832/225000 (11%)] Loss: 8299.732422\n",
      "Train Epoch: 50 [28928/225000 (13%)] Loss: 8144.005859\n",
      "Train Epoch: 50 [33024/225000 (15%)] Loss: 8137.160156\n",
      "Train Epoch: 50 [37120/225000 (16%)] Loss: 8262.330078\n",
      "Train Epoch: 50 [41216/225000 (18%)] Loss: 8084.117188\n",
      "Train Epoch: 50 [45312/225000 (20%)] Loss: 8136.404297\n",
      "Train Epoch: 50 [49408/225000 (22%)] Loss: 8352.154297\n",
      "Train Epoch: 50 [53504/225000 (24%)] Loss: 8077.199219\n",
      "Train Epoch: 50 [57600/225000 (26%)] Loss: 8335.859375\n",
      "Train Epoch: 50 [61696/225000 (27%)] Loss: 8106.464844\n",
      "Train Epoch: 50 [65792/225000 (29%)] Loss: 8202.511719\n",
      "Train Epoch: 50 [69888/225000 (31%)] Loss: 8212.113281\n",
      "Train Epoch: 50 [73984/225000 (33%)] Loss: 8064.552734\n",
      "Train Epoch: 50 [78080/225000 (35%)] Loss: 8044.291016\n",
      "Train Epoch: 50 [82176/225000 (37%)] Loss: 8316.019531\n",
      "Train Epoch: 50 [86272/225000 (38%)] Loss: 8216.644531\n",
      "Train Epoch: 50 [90368/225000 (40%)] Loss: 8330.843750\n",
      "Train Epoch: 50 [94464/225000 (42%)] Loss: 8150.943359\n",
      "Train Epoch: 50 [98560/225000 (44%)] Loss: 8060.871094\n",
      "Train Epoch: 50 [102656/225000 (46%)] Loss: 8011.501953\n",
      "Train Epoch: 50 [106752/225000 (47%)] Loss: 8269.042969\n",
      "Train Epoch: 50 [110848/225000 (49%)] Loss: 8204.177734\n",
      "Train Epoch: 50 [114944/225000 (51%)] Loss: 8050.613281\n",
      "Train Epoch: 50 [119040/225000 (53%)] Loss: 8341.277344\n",
      "Train Epoch: 50 [123136/225000 (55%)] Loss: 8128.839844\n",
      "Train Epoch: 50 [127232/225000 (57%)] Loss: 8029.095703\n",
      "Train Epoch: 50 [131328/225000 (58%)] Loss: 8188.353516\n",
      "Train Epoch: 50 [135424/225000 (60%)] Loss: 8154.677734\n",
      "Train Epoch: 50 [139520/225000 (62%)] Loss: 8210.003906\n",
      "Train Epoch: 50 [143616/225000 (64%)] Loss: 8269.794922\n",
      "Train Epoch: 50 [147712/225000 (66%)] Loss: 8275.775391\n",
      "Train Epoch: 50 [151808/225000 (67%)] Loss: 8287.048828\n",
      "Train Epoch: 50 [155904/225000 (69%)] Loss: 8297.242188\n",
      "Train Epoch: 50 [160000/225000 (71%)] Loss: 8296.521484\n",
      "Train Epoch: 50 [164096/225000 (73%)] Loss: 8327.269531\n",
      "Train Epoch: 50 [168192/225000 (75%)] Loss: 8228.623047\n",
      "Train Epoch: 50 [172288/225000 (77%)] Loss: 8103.982422\n",
      "Train Epoch: 50 [176384/225000 (78%)] Loss: 8115.652344\n",
      "Train Epoch: 50 [180480/225000 (80%)] Loss: 7948.660156\n",
      "Train Epoch: 50 [184576/225000 (82%)] Loss: 8223.285156\n",
      "Train Epoch: 50 [188672/225000 (84%)] Loss: 8302.246094\n",
      "Train Epoch: 50 [192768/225000 (86%)] Loss: 8215.095703\n",
      "Train Epoch: 50 [196864/225000 (87%)] Loss: 8315.976562\n",
      "Train Epoch: 50 [200960/225000 (89%)] Loss: 8221.865234\n",
      "Train Epoch: 50 [205056/225000 (91%)] Loss: 8185.658203\n",
      "Train Epoch: 50 [209152/225000 (93%)] Loss: 8351.382812\n",
      "Train Epoch: 50 [213248/225000 (95%)] Loss: 8337.392578\n",
      "Train Epoch: 50 [217344/225000 (97%)] Loss: 8178.078125\n",
      "Train Epoch: 50 [221440/225000 (98%)] Loss: 8304.210938\n",
      "    epoch          : 50\n",
      "    loss           : 8240.165422354949\n",
      "    val_loss       : 8175.355606723805\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0827_225017/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [256/225000 (0%)] Loss: 8378.400391\n",
      "Train Epoch: 51 [4352/225000 (2%)] Loss: 8074.306641\n",
      "Train Epoch: 51 [8448/225000 (4%)] Loss: 8234.904297\n",
      "Train Epoch: 51 [12544/225000 (6%)] Loss: 8135.300781\n",
      "Train Epoch: 51 [16640/225000 (7%)] Loss: 8190.375000\n",
      "Train Epoch: 51 [20736/225000 (9%)] Loss: 8204.537109\n",
      "Train Epoch: 51 [24832/225000 (11%)] Loss: 8109.046875\n",
      "Train Epoch: 51 [28928/225000 (13%)] Loss: 8102.494141\n",
      "Train Epoch: 51 [33024/225000 (15%)] Loss: 8216.724609\n",
      "Train Epoch: 51 [37120/225000 (16%)] Loss: 8154.519531\n",
      "Train Epoch: 51 [41216/225000 (18%)] Loss: 8288.035156\n",
      "Train Epoch: 51 [45312/225000 (20%)] Loss: 8357.687500\n",
      "Train Epoch: 51 [49408/225000 (22%)] Loss: 8253.220703\n",
      "Train Epoch: 51 [53504/225000 (24%)] Loss: 8226.898438\n",
      "Train Epoch: 51 [57600/225000 (26%)] Loss: 8142.621094\n",
      "Train Epoch: 51 [61696/225000 (27%)] Loss: 8063.332031\n",
      "Train Epoch: 51 [65792/225000 (29%)] Loss: 8415.640625\n",
      "Train Epoch: 51 [69888/225000 (31%)] Loss: 8167.378906\n",
      "Train Epoch: 51 [73984/225000 (33%)] Loss: 8347.279297\n",
      "Train Epoch: 51 [78080/225000 (35%)] Loss: 8107.861328\n",
      "Train Epoch: 51 [82176/225000 (37%)] Loss: 8010.185547\n",
      "Train Epoch: 51 [86272/225000 (38%)] Loss: 8280.662109\n",
      "Train Epoch: 51 [90368/225000 (40%)] Loss: 8258.894531\n",
      "Train Epoch: 51 [94464/225000 (42%)] Loss: 8031.193359\n",
      "Train Epoch: 51 [98560/225000 (44%)] Loss: 8416.630859\n",
      "Train Epoch: 51 [102656/225000 (46%)] Loss: 8165.082031\n",
      "Train Epoch: 51 [106752/225000 (47%)] Loss: 8244.117188\n",
      "Train Epoch: 51 [110848/225000 (49%)] Loss: 8135.244141\n",
      "Train Epoch: 51 [114944/225000 (51%)] Loss: 7920.888672\n",
      "Train Epoch: 51 [119040/225000 (53%)] Loss: 8054.044922\n",
      "Train Epoch: 51 [123136/225000 (55%)] Loss: 8205.261719\n",
      "Train Epoch: 51 [127232/225000 (57%)] Loss: 8162.603516\n",
      "Train Epoch: 51 [131328/225000 (58%)] Loss: 8201.648438\n",
      "Train Epoch: 51 [135424/225000 (60%)] Loss: 8217.353516\n",
      "Train Epoch: 51 [139520/225000 (62%)] Loss: 8311.552734\n",
      "Train Epoch: 51 [143616/225000 (64%)] Loss: 8068.052734\n",
      "Train Epoch: 51 [147712/225000 (66%)] Loss: 8199.693359\n",
      "Train Epoch: 51 [151808/225000 (67%)] Loss: 8160.722656\n",
      "Train Epoch: 51 [155904/225000 (69%)] Loss: 8466.802734\n",
      "Train Epoch: 51 [160000/225000 (71%)] Loss: 8419.841797\n",
      "Train Epoch: 51 [164096/225000 (73%)] Loss: 8226.041016\n",
      "Train Epoch: 51 [168192/225000 (75%)] Loss: 8332.558594\n",
      "Train Epoch: 51 [172288/225000 (77%)] Loss: 8294.210938\n",
      "Train Epoch: 51 [176384/225000 (78%)] Loss: 8157.869141\n",
      "Train Epoch: 51 [180480/225000 (80%)] Loss: 8346.796875\n",
      "Train Epoch: 51 [184576/225000 (82%)] Loss: 8329.769531\n",
      "Train Epoch: 51 [188672/225000 (84%)] Loss: 8115.806641\n",
      "Train Epoch: 51 [192768/225000 (86%)] Loss: 8067.082031\n",
      "Train Epoch: 51 [196864/225000 (87%)] Loss: 8145.548828\n",
      "Train Epoch: 51 [200960/225000 (89%)] Loss: 8172.285156\n",
      "Train Epoch: 51 [205056/225000 (91%)] Loss: 8118.992188\n",
      "Train Epoch: 51 [209152/225000 (93%)] Loss: 8118.425781\n",
      "Train Epoch: 51 [213248/225000 (95%)] Loss: 8157.599609\n",
      "Train Epoch: 51 [217344/225000 (97%)] Loss: 8167.500000\n",
      "Train Epoch: 51 [221440/225000 (98%)] Loss: 7994.308594\n",
      "    epoch          : 51\n",
      "    loss           : 8182.488404570179\n",
      "    val_loss       : 8168.245032607293\n",
      "Train Epoch: 52 [256/225000 (0%)] Loss: 8300.427734\n",
      "Train Epoch: 52 [4352/225000 (2%)] Loss: 8150.542969\n",
      "Train Epoch: 52 [8448/225000 (4%)] Loss: 8120.316406\n",
      "Train Epoch: 52 [12544/225000 (6%)] Loss: 8018.986328\n",
      "Train Epoch: 52 [16640/225000 (7%)] Loss: 8095.093750\n",
      "Train Epoch: 52 [20736/225000 (9%)] Loss: 8440.619141\n",
      "Train Epoch: 52 [24832/225000 (11%)] Loss: 8264.666016\n",
      "Train Epoch: 52 [28928/225000 (13%)] Loss: 8051.417969\n",
      "Train Epoch: 52 [33024/225000 (15%)] Loss: 8087.228516\n",
      "Train Epoch: 52 [37120/225000 (16%)] Loss: 8169.958984\n",
      "Train Epoch: 52 [41216/225000 (18%)] Loss: 8051.332031\n",
      "Train Epoch: 52 [45312/225000 (20%)] Loss: 8092.693359\n",
      "Train Epoch: 52 [49408/225000 (22%)] Loss: 8297.900391\n",
      "Train Epoch: 52 [53504/225000 (24%)] Loss: 8127.291016\n",
      "Train Epoch: 52 [57600/225000 (26%)] Loss: 8110.189453\n",
      "Train Epoch: 52 [61696/225000 (27%)] Loss: 8095.947266\n",
      "Train Epoch: 52 [65792/225000 (29%)] Loss: 8185.125000\n",
      "Train Epoch: 52 [69888/225000 (31%)] Loss: 7993.777344\n",
      "Train Epoch: 52 [73984/225000 (33%)] Loss: 8187.378906\n",
      "Train Epoch: 52 [78080/225000 (35%)] Loss: 8099.101562\n",
      "Train Epoch: 52 [82176/225000 (37%)] Loss: 8312.166016\n",
      "Train Epoch: 52 [86272/225000 (38%)] Loss: 8142.916016\n",
      "Train Epoch: 52 [90368/225000 (40%)] Loss: 8230.867188\n",
      "Train Epoch: 52 [94464/225000 (42%)] Loss: 8221.076172\n",
      "Train Epoch: 52 [98560/225000 (44%)] Loss: 8314.621094\n",
      "Train Epoch: 52 [102656/225000 (46%)] Loss: 8394.638672\n",
      "Train Epoch: 52 [106752/225000 (47%)] Loss: 8187.220703\n",
      "Train Epoch: 52 [110848/225000 (49%)] Loss: 8030.531250\n",
      "Train Epoch: 52 [114944/225000 (51%)] Loss: 8120.099609\n",
      "Train Epoch: 52 [119040/225000 (53%)] Loss: 8023.705078\n",
      "Train Epoch: 52 [123136/225000 (55%)] Loss: 8058.794922\n",
      "Train Epoch: 52 [127232/225000 (57%)] Loss: 8171.412109\n",
      "Train Epoch: 52 [131328/225000 (58%)] Loss: 7972.511719\n",
      "Train Epoch: 52 [135424/225000 (60%)] Loss: 8183.640625\n",
      "Train Epoch: 52 [139520/225000 (62%)] Loss: 8322.496094\n",
      "Train Epoch: 52 [143616/225000 (64%)] Loss: 8308.134766\n",
      "Train Epoch: 52 [147712/225000 (66%)] Loss: 8286.927734\n",
      "Train Epoch: 52 [151808/225000 (67%)] Loss: 8248.515625\n",
      "Train Epoch: 52 [155904/225000 (69%)] Loss: 8141.644531\n",
      "Train Epoch: 52 [160000/225000 (71%)] Loss: 8101.783203\n",
      "Train Epoch: 52 [164096/225000 (73%)] Loss: 8280.517578\n",
      "Train Epoch: 52 [168192/225000 (75%)] Loss: 8164.740234\n",
      "Train Epoch: 52 [172288/225000 (77%)] Loss: 7993.246094\n",
      "Train Epoch: 52 [176384/225000 (78%)] Loss: 8087.685547\n",
      "Train Epoch: 52 [180480/225000 (80%)] Loss: 8128.390625\n",
      "Train Epoch: 52 [184576/225000 (82%)] Loss: 8015.273438\n",
      "Train Epoch: 52 [188672/225000 (84%)] Loss: 8196.455078\n",
      "Train Epoch: 52 [192768/225000 (86%)] Loss: 8121.015625\n",
      "Train Epoch: 52 [196864/225000 (87%)] Loss: 8191.548828\n",
      "Train Epoch: 52 [200960/225000 (89%)] Loss: 8145.908203\n",
      "Train Epoch: 52 [205056/225000 (91%)] Loss: 7978.769531\n",
      "Train Epoch: 52 [209152/225000 (93%)] Loss: 8033.855469\n",
      "Train Epoch: 52 [213248/225000 (95%)] Loss: 8134.908203\n",
      "Train Epoch: 52 [217344/225000 (97%)] Loss: 8154.316406\n",
      "Train Epoch: 52 [221440/225000 (98%)] Loss: 8099.675781\n",
      "    epoch          : 52\n",
      "    loss           : 8170.711478553399\n",
      "    val_loss       : 8150.556614827136\n",
      "Train Epoch: 53 [256/225000 (0%)] Loss: 8050.994141\n",
      "Train Epoch: 53 [4352/225000 (2%)] Loss: 8168.097656\n",
      "Train Epoch: 53 [8448/225000 (4%)] Loss: 8214.906250\n",
      "Train Epoch: 53 [12544/225000 (6%)] Loss: 8010.349609\n",
      "Train Epoch: 53 [16640/225000 (7%)] Loss: 8010.363281\n",
      "Train Epoch: 53 [20736/225000 (9%)] Loss: 7973.296875\n",
      "Train Epoch: 53 [24832/225000 (11%)] Loss: 8168.259766\n",
      "Train Epoch: 53 [28928/225000 (13%)] Loss: 8256.902344\n",
      "Train Epoch: 53 [33024/225000 (15%)] Loss: 8187.632812\n",
      "Train Epoch: 53 [37120/225000 (16%)] Loss: 8062.009766\n",
      "Train Epoch: 53 [41216/225000 (18%)] Loss: 8139.583984\n",
      "Train Epoch: 53 [45312/225000 (20%)] Loss: 8031.003906\n",
      "Train Epoch: 53 [49408/225000 (22%)] Loss: 7883.406250\n",
      "Train Epoch: 53 [53504/225000 (24%)] Loss: 8258.990234\n",
      "Train Epoch: 53 [57600/225000 (26%)] Loss: 8147.685547\n",
      "Train Epoch: 53 [61696/225000 (27%)] Loss: 8381.771484\n",
      "Train Epoch: 53 [65792/225000 (29%)] Loss: 8025.740234\n",
      "Train Epoch: 53 [69888/225000 (31%)] Loss: 8126.849609\n",
      "Train Epoch: 53 [73984/225000 (33%)] Loss: 8192.212891\n",
      "Train Epoch: 53 [78080/225000 (35%)] Loss: 8297.029297\n",
      "Train Epoch: 53 [82176/225000 (37%)] Loss: 8194.529297\n",
      "Train Epoch: 53 [86272/225000 (38%)] Loss: 8190.570312\n",
      "Train Epoch: 53 [90368/225000 (40%)] Loss: 8075.623047\n",
      "Train Epoch: 53 [94464/225000 (42%)] Loss: 8182.033203\n",
      "Train Epoch: 53 [98560/225000 (44%)] Loss: 8103.046875\n",
      "Train Epoch: 53 [102656/225000 (46%)] Loss: 8016.234375\n",
      "Train Epoch: 53 [106752/225000 (47%)] Loss: 8239.875000\n",
      "Train Epoch: 53 [110848/225000 (49%)] Loss: 8049.072266\n",
      "Train Epoch: 53 [114944/225000 (51%)] Loss: 8323.693359\n",
      "Train Epoch: 53 [119040/225000 (53%)] Loss: 8140.478516\n",
      "Train Epoch: 53 [123136/225000 (55%)] Loss: 8082.544922\n",
      "Train Epoch: 53 [127232/225000 (57%)] Loss: 8159.835938\n",
      "Train Epoch: 53 [131328/225000 (58%)] Loss: 8096.310547\n",
      "Train Epoch: 53 [135424/225000 (60%)] Loss: 8359.009766\n",
      "Train Epoch: 53 [139520/225000 (62%)] Loss: 7963.986328\n",
      "Train Epoch: 53 [143616/225000 (64%)] Loss: 8318.320312\n",
      "Train Epoch: 53 [147712/225000 (66%)] Loss: 8170.054688\n",
      "Train Epoch: 53 [151808/225000 (67%)] Loss: 8044.410156\n",
      "Train Epoch: 53 [155904/225000 (69%)] Loss: 8268.433594\n",
      "Train Epoch: 53 [160000/225000 (71%)] Loss: 8128.417969\n",
      "Train Epoch: 53 [164096/225000 (73%)] Loss: 8140.609375\n",
      "Train Epoch: 53 [168192/225000 (75%)] Loss: 8127.726562\n",
      "Train Epoch: 53 [172288/225000 (77%)] Loss: 8328.835938\n",
      "Train Epoch: 53 [176384/225000 (78%)] Loss: 8106.150391\n",
      "Train Epoch: 53 [180480/225000 (80%)] Loss: 8383.851562\n",
      "Train Epoch: 53 [184576/225000 (82%)] Loss: 8360.167969\n",
      "Train Epoch: 53 [188672/225000 (84%)] Loss: 8263.109375\n",
      "Train Epoch: 53 [192768/225000 (86%)] Loss: 8146.060547\n",
      "Train Epoch: 53 [196864/225000 (87%)] Loss: 8029.345703\n",
      "Train Epoch: 53 [200960/225000 (89%)] Loss: 8042.847656\n",
      "Train Epoch: 53 [205056/225000 (91%)] Loss: 8114.269531\n",
      "Train Epoch: 53 [209152/225000 (93%)] Loss: 8117.828125\n",
      "Train Epoch: 53 [213248/225000 (95%)] Loss: 8131.845703\n",
      "Train Epoch: 53 [217344/225000 (97%)] Loss: 8165.978516\n",
      "Train Epoch: 53 [221440/225000 (98%)] Loss: 8294.300781\n",
      "    epoch          : 53\n",
      "    loss           : 8179.07137016496\n",
      "    val_loss       : 8117.698344767094\n",
      "Train Epoch: 54 [256/225000 (0%)] Loss: 8032.492188\n",
      "Train Epoch: 54 [4352/225000 (2%)] Loss: 8131.085938\n",
      "Train Epoch: 54 [8448/225000 (4%)] Loss: 8157.199219\n",
      "Train Epoch: 54 [12544/225000 (6%)] Loss: 8507.902344\n",
      "Train Epoch: 54 [16640/225000 (7%)] Loss: 8100.873047\n",
      "Train Epoch: 54 [20736/225000 (9%)] Loss: 8250.865234\n",
      "Train Epoch: 54 [24832/225000 (11%)] Loss: 8223.623047\n",
      "Train Epoch: 54 [28928/225000 (13%)] Loss: 7923.730469\n",
      "Train Epoch: 54 [33024/225000 (15%)] Loss: 8182.576172\n",
      "Train Epoch: 54 [37120/225000 (16%)] Loss: 8105.468750\n",
      "Train Epoch: 54 [41216/225000 (18%)] Loss: 8127.791016\n",
      "Train Epoch: 54 [45312/225000 (20%)] Loss: 7912.951172\n",
      "Train Epoch: 54 [49408/225000 (22%)] Loss: 8167.943359\n",
      "Train Epoch: 54 [53504/225000 (24%)] Loss: 7971.464844\n",
      "Train Epoch: 54 [57600/225000 (26%)] Loss: 8080.246094\n",
      "Train Epoch: 54 [61696/225000 (27%)] Loss: 8230.728516\n",
      "Train Epoch: 54 [65792/225000 (29%)] Loss: 8265.441406\n",
      "Train Epoch: 54 [69888/225000 (31%)] Loss: 8064.429688\n",
      "Train Epoch: 54 [73984/225000 (33%)] Loss: 8113.492188\n",
      "Train Epoch: 54 [78080/225000 (35%)] Loss: 8061.068359\n",
      "Train Epoch: 54 [82176/225000 (37%)] Loss: 8081.412109\n",
      "Train Epoch: 54 [86272/225000 (38%)] Loss: 8036.839844\n",
      "Train Epoch: 54 [90368/225000 (40%)] Loss: 8068.109375\n",
      "Train Epoch: 54 [94464/225000 (42%)] Loss: 8055.955078\n",
      "Train Epoch: 54 [98560/225000 (44%)] Loss: 8117.087891\n",
      "Train Epoch: 54 [102656/225000 (46%)] Loss: 8144.363281\n",
      "Train Epoch: 54 [106752/225000 (47%)] Loss: 8079.744141\n",
      "Train Epoch: 54 [110848/225000 (49%)] Loss: 7986.851562\n",
      "Train Epoch: 54 [114944/225000 (51%)] Loss: 8190.810547\n",
      "Train Epoch: 54 [119040/225000 (53%)] Loss: 7940.490234\n",
      "Train Epoch: 54 [123136/225000 (55%)] Loss: 7974.294922\n",
      "Train Epoch: 54 [127232/225000 (57%)] Loss: 8097.839844\n",
      "Train Epoch: 54 [131328/225000 (58%)] Loss: 7917.259766\n",
      "Train Epoch: 54 [135424/225000 (60%)] Loss: 8207.300781\n",
      "Train Epoch: 54 [139520/225000 (62%)] Loss: 8076.134766\n",
      "Train Epoch: 54 [143616/225000 (64%)] Loss: 8166.447266\n",
      "Train Epoch: 54 [147712/225000 (66%)] Loss: 8048.783203\n",
      "Train Epoch: 54 [151808/225000 (67%)] Loss: 8121.882812\n",
      "Train Epoch: 54 [155904/225000 (69%)] Loss: 8296.869141\n",
      "Train Epoch: 54 [160000/225000 (71%)] Loss: 8326.394531\n",
      "Train Epoch: 54 [164096/225000 (73%)] Loss: 8130.019531\n",
      "Train Epoch: 54 [168192/225000 (75%)] Loss: 8060.509766\n",
      "Train Epoch: 54 [172288/225000 (77%)] Loss: 8168.404297\n",
      "Train Epoch: 54 [176384/225000 (78%)] Loss: 8031.406250\n",
      "Train Epoch: 54 [180480/225000 (80%)] Loss: 8279.042969\n",
      "Train Epoch: 54 [184576/225000 (82%)] Loss: 8192.242188\n",
      "Train Epoch: 54 [188672/225000 (84%)] Loss: 8171.939453\n",
      "Train Epoch: 54 [192768/225000 (86%)] Loss: 8020.230469\n",
      "Train Epoch: 54 [196864/225000 (87%)] Loss: 8041.845703\n",
      "Train Epoch: 54 [200960/225000 (89%)] Loss: 8356.210938\n",
      "Train Epoch: 54 [205056/225000 (91%)] Loss: 8178.330078\n",
      "Train Epoch: 54 [209152/225000 (93%)] Loss: 8157.728516\n",
      "Train Epoch: 54 [213248/225000 (95%)] Loss: 8023.736328\n",
      "Train Epoch: 54 [217344/225000 (97%)] Loss: 8094.177734\n",
      "Train Epoch: 54 [221440/225000 (98%)] Loss: 8096.367188\n",
      "    epoch          : 54\n",
      "    loss           : 8161.894826774033\n",
      "    val_loss       : 8096.8066182173025\n",
      "Train Epoch: 55 [256/225000 (0%)] Loss: 8018.138672\n",
      "Train Epoch: 55 [4352/225000 (2%)] Loss: 8079.722656\n",
      "Train Epoch: 55 [8448/225000 (4%)] Loss: 8173.232422\n",
      "Train Epoch: 55 [12544/225000 (6%)] Loss: 8050.283203\n",
      "Train Epoch: 55 [16640/225000 (7%)] Loss: 8165.423828\n",
      "Train Epoch: 55 [20736/225000 (9%)] Loss: 7936.933594\n",
      "Train Epoch: 55 [24832/225000 (11%)] Loss: 8134.388672\n",
      "Train Epoch: 55 [28928/225000 (13%)] Loss: 8240.404297\n",
      "Train Epoch: 55 [33024/225000 (15%)] Loss: 8130.007812\n",
      "Train Epoch: 55 [37120/225000 (16%)] Loss: 8061.621094\n",
      "Train Epoch: 55 [41216/225000 (18%)] Loss: 8181.439453\n",
      "Train Epoch: 55 [45312/225000 (20%)] Loss: 8145.378906\n",
      "Train Epoch: 55 [49408/225000 (22%)] Loss: 8048.339844\n",
      "Train Epoch: 55 [53504/225000 (24%)] Loss: 7968.310547\n",
      "Train Epoch: 55 [57600/225000 (26%)] Loss: 8320.888672\n",
      "Train Epoch: 55 [61696/225000 (27%)] Loss: 8059.830078\n",
      "Train Epoch: 55 [65792/225000 (29%)] Loss: 8185.294922\n",
      "Train Epoch: 55 [69888/225000 (31%)] Loss: 8163.062500\n",
      "Train Epoch: 55 [73984/225000 (33%)] Loss: 7999.437500\n",
      "Train Epoch: 55 [78080/225000 (35%)] Loss: 8262.632812\n",
      "Train Epoch: 55 [82176/225000 (37%)] Loss: 7989.519531\n",
      "Train Epoch: 55 [86272/225000 (38%)] Loss: 8058.121094\n",
      "Train Epoch: 55 [90368/225000 (40%)] Loss: 8039.929688\n",
      "Train Epoch: 55 [94464/225000 (42%)] Loss: 8302.037109\n",
      "Train Epoch: 55 [98560/225000 (44%)] Loss: 8001.318359\n",
      "Train Epoch: 55 [102656/225000 (46%)] Loss: 8195.074219\n",
      "Train Epoch: 55 [106752/225000 (47%)] Loss: 8194.722656\n",
      "Train Epoch: 55 [110848/225000 (49%)] Loss: 8223.371094\n",
      "Train Epoch: 55 [114944/225000 (51%)] Loss: 8015.935547\n",
      "Train Epoch: 55 [119040/225000 (53%)] Loss: 8035.314453\n",
      "Train Epoch: 55 [123136/225000 (55%)] Loss: 8210.355469\n",
      "Train Epoch: 55 [127232/225000 (57%)] Loss: 8200.994141\n",
      "Train Epoch: 55 [131328/225000 (58%)] Loss: 8089.113281\n",
      "Train Epoch: 55 [135424/225000 (60%)] Loss: 8226.126953\n",
      "Train Epoch: 55 [139520/225000 (62%)] Loss: 8022.210938\n",
      "Train Epoch: 55 [143616/225000 (64%)] Loss: 7918.722656\n",
      "Train Epoch: 55 [147712/225000 (66%)] Loss: 8079.400391\n",
      "Train Epoch: 55 [151808/225000 (67%)] Loss: 8152.503906\n",
      "Train Epoch: 55 [155904/225000 (69%)] Loss: 8095.445312\n",
      "Train Epoch: 55 [160000/225000 (71%)] Loss: 8186.867188\n",
      "Train Epoch: 55 [164096/225000 (73%)] Loss: 8091.720703\n",
      "Train Epoch: 55 [168192/225000 (75%)] Loss: 8074.259766\n",
      "Train Epoch: 55 [172288/225000 (77%)] Loss: 7921.912109\n",
      "Train Epoch: 55 [176384/225000 (78%)] Loss: 8164.595703\n",
      "Train Epoch: 55 [180480/225000 (80%)] Loss: 8067.015625\n",
      "Train Epoch: 55 [184576/225000 (82%)] Loss: 8104.732422\n",
      "Train Epoch: 55 [188672/225000 (84%)] Loss: 7922.851562\n",
      "Train Epoch: 55 [192768/225000 (86%)] Loss: 8162.029297\n",
      "Train Epoch: 55 [196864/225000 (87%)] Loss: 8153.990234\n",
      "Train Epoch: 55 [200960/225000 (89%)] Loss: 8231.062500\n",
      "Train Epoch: 55 [205056/225000 (91%)] Loss: 8242.933594\n",
      "Train Epoch: 55 [209152/225000 (93%)] Loss: 8038.533203\n",
      "Train Epoch: 55 [213248/225000 (95%)] Loss: 8140.054688\n",
      "Train Epoch: 55 [217344/225000 (97%)] Loss: 8108.634766\n",
      "Train Epoch: 55 [221440/225000 (98%)] Loss: 8029.568359\n",
      "    epoch          : 55\n",
      "    loss           : 8112.065410800626\n",
      "    val_loss       : 8081.026530335144\n",
      "Train Epoch: 56 [256/225000 (0%)] Loss: 8114.474609\n",
      "Train Epoch: 56 [4352/225000 (2%)] Loss: 8011.720703\n",
      "Train Epoch: 56 [8448/225000 (4%)] Loss: 8149.283203\n",
      "Train Epoch: 56 [12544/225000 (6%)] Loss: 7924.517578\n",
      "Train Epoch: 56 [16640/225000 (7%)] Loss: 8131.207031\n",
      "Train Epoch: 56 [20736/225000 (9%)] Loss: 8217.333984\n",
      "Train Epoch: 56 [24832/225000 (11%)] Loss: 8049.886719\n",
      "Train Epoch: 56 [28928/225000 (13%)] Loss: 8341.830078\n",
      "Train Epoch: 56 [33024/225000 (15%)] Loss: 8052.287109\n",
      "Train Epoch: 56 [37120/225000 (16%)] Loss: 7978.769531\n",
      "Train Epoch: 56 [41216/225000 (18%)] Loss: 8275.667969\n",
      "Train Epoch: 56 [45312/225000 (20%)] Loss: 7991.625000\n",
      "Train Epoch: 56 [49408/225000 (22%)] Loss: 8049.595703\n",
      "Train Epoch: 56 [53504/225000 (24%)] Loss: 7979.474609\n",
      "Train Epoch: 56 [57600/225000 (26%)] Loss: 8070.335938\n",
      "Train Epoch: 56 [61696/225000 (27%)] Loss: 8026.431641\n",
      "Train Epoch: 56 [65792/225000 (29%)] Loss: 8154.714844\n",
      "Train Epoch: 56 [69888/225000 (31%)] Loss: 7905.244141\n",
      "Train Epoch: 56 [73984/225000 (33%)] Loss: 8025.507812\n",
      "Train Epoch: 56 [78080/225000 (35%)] Loss: 8040.771484\n",
      "Train Epoch: 56 [82176/225000 (37%)] Loss: 8066.632812\n",
      "Train Epoch: 56 [86272/225000 (38%)] Loss: 8047.144531\n",
      "Train Epoch: 56 [90368/225000 (40%)] Loss: 8354.417969\n",
      "Train Epoch: 56 [94464/225000 (42%)] Loss: 8020.904297\n",
      "Train Epoch: 56 [98560/225000 (44%)] Loss: 8093.718750\n",
      "Train Epoch: 56 [102656/225000 (46%)] Loss: 8097.498047\n",
      "Train Epoch: 56 [106752/225000 (47%)] Loss: 8184.636719\n",
      "Train Epoch: 56 [110848/225000 (49%)] Loss: 8095.798828\n",
      "Train Epoch: 56 [114944/225000 (51%)] Loss: 8049.623047\n",
      "Train Epoch: 56 [119040/225000 (53%)] Loss: 8126.326172\n",
      "Train Epoch: 56 [123136/225000 (55%)] Loss: 8026.968750\n",
      "Train Epoch: 56 [127232/225000 (57%)] Loss: 8310.396484\n",
      "Train Epoch: 56 [131328/225000 (58%)] Loss: 8135.626953\n",
      "Train Epoch: 56 [135424/225000 (60%)] Loss: 8097.095703\n",
      "Train Epoch: 56 [139520/225000 (62%)] Loss: 7981.523438\n",
      "Train Epoch: 56 [143616/225000 (64%)] Loss: 8016.871094\n",
      "Train Epoch: 56 [147712/225000 (66%)] Loss: 8108.996094\n",
      "Train Epoch: 56 [151808/225000 (67%)] Loss: 8074.400391\n",
      "Train Epoch: 56 [155904/225000 (69%)] Loss: 8176.630859\n",
      "Train Epoch: 56 [160000/225000 (71%)] Loss: 8139.419922\n",
      "Train Epoch: 56 [164096/225000 (73%)] Loss: 8251.003906\n",
      "Train Epoch: 56 [168192/225000 (75%)] Loss: 7995.652344\n",
      "Train Epoch: 56 [172288/225000 (77%)] Loss: 8148.908203\n",
      "Train Epoch: 56 [176384/225000 (78%)] Loss: 8127.871094\n",
      "Train Epoch: 56 [180480/225000 (80%)] Loss: 8054.564453\n",
      "Train Epoch: 56 [184576/225000 (82%)] Loss: 7971.519531\n",
      "Train Epoch: 56 [188672/225000 (84%)] Loss: 8001.394531\n",
      "Train Epoch: 56 [192768/225000 (86%)] Loss: 8034.021484\n",
      "Train Epoch: 56 [196864/225000 (87%)] Loss: 7971.695312\n",
      "Train Epoch: 56 [200960/225000 (89%)] Loss: 7939.369141\n",
      "Train Epoch: 56 [205056/225000 (91%)] Loss: 8069.324219\n",
      "Train Epoch: 56 [209152/225000 (93%)] Loss: 7923.341797\n",
      "Train Epoch: 56 [213248/225000 (95%)] Loss: 8264.724609\n",
      "Train Epoch: 56 [217344/225000 (97%)] Loss: 8100.039062\n",
      "Train Epoch: 56 [221440/225000 (98%)] Loss: 7870.392578\n",
      "    epoch          : 56\n",
      "    loss           : 8123.445053638723\n",
      "    val_loss       : 8067.488564146416\n",
      "Train Epoch: 57 [256/225000 (0%)] Loss: 8177.136719\n",
      "Train Epoch: 57 [4352/225000 (2%)] Loss: 8097.179688\n",
      "Train Epoch: 57 [8448/225000 (4%)] Loss: 7990.027344\n",
      "Train Epoch: 57 [12544/225000 (6%)] Loss: 8084.878906\n",
      "Train Epoch: 57 [16640/225000 (7%)] Loss: 8091.271484\n",
      "Train Epoch: 57 [20736/225000 (9%)] Loss: 8110.796875\n",
      "Train Epoch: 57 [24832/225000 (11%)] Loss: 8106.599609\n",
      "Train Epoch: 57 [28928/225000 (13%)] Loss: 8033.255859\n",
      "Train Epoch: 57 [33024/225000 (15%)] Loss: 8025.609375\n",
      "Train Epoch: 57 [37120/225000 (16%)] Loss: 8125.710938\n",
      "Train Epoch: 57 [41216/225000 (18%)] Loss: 8272.171875\n",
      "Train Epoch: 57 [45312/225000 (20%)] Loss: 8264.333984\n",
      "Train Epoch: 57 [49408/225000 (22%)] Loss: 8285.804688\n",
      "Train Epoch: 57 [53504/225000 (24%)] Loss: 8034.306641\n",
      "Train Epoch: 57 [57600/225000 (26%)] Loss: 7950.259766\n",
      "Train Epoch: 57 [61696/225000 (27%)] Loss: 8226.705078\n",
      "Train Epoch: 57 [65792/225000 (29%)] Loss: 8272.322266\n",
      "Train Epoch: 57 [69888/225000 (31%)] Loss: 8107.267578\n",
      "Train Epoch: 57 [73984/225000 (33%)] Loss: 7990.126953\n",
      "Train Epoch: 57 [78080/225000 (35%)] Loss: 7986.740234\n",
      "Train Epoch: 57 [82176/225000 (37%)] Loss: 8055.066406\n",
      "Train Epoch: 57 [86272/225000 (38%)] Loss: 8004.232422\n",
      "Train Epoch: 57 [90368/225000 (40%)] Loss: 8145.781250\n",
      "Train Epoch: 57 [94464/225000 (42%)] Loss: 8259.369141\n",
      "Train Epoch: 57 [98560/225000 (44%)] Loss: 8056.162109\n",
      "Train Epoch: 57 [102656/225000 (46%)] Loss: 8252.164062\n",
      "Train Epoch: 57 [106752/225000 (47%)] Loss: 8030.183594\n",
      "Train Epoch: 57 [110848/225000 (49%)] Loss: 7937.091797\n",
      "Train Epoch: 57 [114944/225000 (51%)] Loss: 7956.560547\n",
      "Train Epoch: 57 [119040/225000 (53%)] Loss: 7938.619141\n",
      "Train Epoch: 57 [123136/225000 (55%)] Loss: 7997.970703\n",
      "Train Epoch: 57 [127232/225000 (57%)] Loss: 8130.140625\n",
      "Train Epoch: 57 [131328/225000 (58%)] Loss: 8098.400391\n",
      "Train Epoch: 57 [135424/225000 (60%)] Loss: 8017.763672\n",
      "Train Epoch: 57 [139520/225000 (62%)] Loss: 8180.691406\n",
      "Train Epoch: 57 [143616/225000 (64%)] Loss: 7925.226562\n",
      "Train Epoch: 57 [147712/225000 (66%)] Loss: 8063.058594\n",
      "Train Epoch: 57 [151808/225000 (67%)] Loss: 8106.789062\n",
      "Train Epoch: 57 [155904/225000 (69%)] Loss: 8190.625000\n",
      "Train Epoch: 57 [160000/225000 (71%)] Loss: 7924.617188\n",
      "Train Epoch: 57 [164096/225000 (73%)] Loss: 8029.505859\n",
      "Train Epoch: 57 [168192/225000 (75%)] Loss: 8123.802734\n",
      "Train Epoch: 57 [172288/225000 (77%)] Loss: 7983.263672\n",
      "Train Epoch: 57 [176384/225000 (78%)] Loss: 8049.023438\n",
      "Train Epoch: 57 [180480/225000 (80%)] Loss: 8168.720703\n",
      "Train Epoch: 57 [184576/225000 (82%)] Loss: 8122.814453\n",
      "Train Epoch: 57 [188672/225000 (84%)] Loss: 8052.708984\n",
      "Train Epoch: 57 [192768/225000 (86%)] Loss: 8051.287109\n",
      "Train Epoch: 57 [196864/225000 (87%)] Loss: 8122.439453\n",
      "Train Epoch: 57 [200960/225000 (89%)] Loss: 7950.037109\n",
      "Train Epoch: 57 [205056/225000 (91%)] Loss: 8020.820312\n",
      "Train Epoch: 57 [209152/225000 (93%)] Loss: 8083.085938\n",
      "Train Epoch: 57 [213248/225000 (95%)] Loss: 8338.443359\n",
      "Train Epoch: 57 [217344/225000 (97%)] Loss: 8039.300781\n",
      "Train Epoch: 57 [221440/225000 (98%)] Loss: 8149.339844\n",
      "    epoch          : 57\n",
      "    loss           : 8122.746969212173\n",
      "    val_loss       : 8073.676191917183\n",
      "Train Epoch: 58 [256/225000 (0%)] Loss: 7894.921875\n",
      "Train Epoch: 58 [4352/225000 (2%)] Loss: 8036.279297\n",
      "Train Epoch: 58 [8448/225000 (4%)] Loss: 8103.541016\n",
      "Train Epoch: 58 [12544/225000 (6%)] Loss: 8160.740234\n",
      "Train Epoch: 58 [16640/225000 (7%)] Loss: 8186.046875\n",
      "Train Epoch: 58 [20736/225000 (9%)] Loss: 8020.644531\n",
      "Train Epoch: 58 [24832/225000 (11%)] Loss: 8007.324219\n",
      "Train Epoch: 58 [28928/225000 (13%)] Loss: 8008.267578\n",
      "Train Epoch: 58 [33024/225000 (15%)] Loss: 7976.384766\n",
      "Train Epoch: 58 [37120/225000 (16%)] Loss: 7994.630859\n",
      "Train Epoch: 58 [41216/225000 (18%)] Loss: 7907.675781\n",
      "Train Epoch: 58 [45312/225000 (20%)] Loss: 8032.513672\n",
      "Train Epoch: 58 [49408/225000 (22%)] Loss: 8048.574219\n",
      "Train Epoch: 58 [53504/225000 (24%)] Loss: 7949.326172\n",
      "Train Epoch: 58 [57600/225000 (26%)] Loss: 8385.724609\n",
      "Train Epoch: 58 [61696/225000 (27%)] Loss: 8112.949219\n",
      "Train Epoch: 58 [65792/225000 (29%)] Loss: 8057.794922\n",
      "Train Epoch: 58 [69888/225000 (31%)] Loss: 7969.556641\n",
      "Train Epoch: 58 [73984/225000 (33%)] Loss: 7979.986328\n",
      "Train Epoch: 58 [78080/225000 (35%)] Loss: 7987.914062\n",
      "Train Epoch: 58 [82176/225000 (37%)] Loss: 8180.382812\n",
      "Train Epoch: 58 [86272/225000 (38%)] Loss: 8227.126953\n",
      "Train Epoch: 58 [90368/225000 (40%)] Loss: 8092.767578\n",
      "Train Epoch: 58 [94464/225000 (42%)] Loss: 7985.068359\n",
      "Train Epoch: 58 [98560/225000 (44%)] Loss: 7978.414062\n",
      "Train Epoch: 58 [102656/225000 (46%)] Loss: 7966.794922\n",
      "Train Epoch: 58 [106752/225000 (47%)] Loss: 8164.582031\n",
      "Train Epoch: 58 [110848/225000 (49%)] Loss: 8024.287109\n",
      "Train Epoch: 58 [114944/225000 (51%)] Loss: 7953.437500\n",
      "Train Epoch: 58 [119040/225000 (53%)] Loss: 8083.333984\n",
      "Train Epoch: 58 [123136/225000 (55%)] Loss: 8101.705078\n",
      "Train Epoch: 58 [127232/225000 (57%)] Loss: 8010.580078\n",
      "Train Epoch: 58 [131328/225000 (58%)] Loss: 8015.714844\n",
      "Train Epoch: 58 [135424/225000 (60%)] Loss: 7853.949219\n",
      "Train Epoch: 58 [139520/225000 (62%)] Loss: 8133.187500\n",
      "Train Epoch: 58 [143616/225000 (64%)] Loss: 8310.486328\n",
      "Train Epoch: 58 [147712/225000 (66%)] Loss: 8101.419922\n",
      "Train Epoch: 58 [151808/225000 (67%)] Loss: 7966.150391\n",
      "Train Epoch: 58 [155904/225000 (69%)] Loss: 7984.349609\n",
      "Train Epoch: 58 [160000/225000 (71%)] Loss: 8019.865234\n",
      "Train Epoch: 58 [164096/225000 (73%)] Loss: 7963.855469\n",
      "Train Epoch: 58 [168192/225000 (75%)] Loss: 7994.576172\n",
      "Train Epoch: 58 [172288/225000 (77%)] Loss: 8009.878906\n",
      "Train Epoch: 58 [176384/225000 (78%)] Loss: 8246.632812\n",
      "Train Epoch: 58 [180480/225000 (80%)] Loss: 8182.984375\n",
      "Train Epoch: 58 [184576/225000 (82%)] Loss: 7943.761719\n",
      "Train Epoch: 58 [188672/225000 (84%)] Loss: 8075.080078\n",
      "Train Epoch: 58 [192768/225000 (86%)] Loss: 8153.771484\n",
      "Train Epoch: 58 [196864/225000 (87%)] Loss: 8129.578125\n",
      "Train Epoch: 58 [200960/225000 (89%)] Loss: 7940.539062\n",
      "Train Epoch: 58 [205056/225000 (91%)] Loss: 8052.818359\n",
      "Train Epoch: 58 [209152/225000 (93%)] Loss: 7909.582031\n",
      "Train Epoch: 58 [213248/225000 (95%)] Loss: 8108.964844\n",
      "Train Epoch: 58 [217344/225000 (97%)] Loss: 8145.033203\n",
      "Train Epoch: 58 [221440/225000 (98%)] Loss: 7987.216797\n",
      "    epoch          : 58\n",
      "    loss           : 8078.53882808056\n",
      "    val_loss       : 8048.392968319508\n",
      "Train Epoch: 59 [256/225000 (0%)] Loss: 7992.943359\n",
      "Train Epoch: 59 [4352/225000 (2%)] Loss: 8050.070312\n",
      "Train Epoch: 59 [8448/225000 (4%)] Loss: 8046.414062\n",
      "Train Epoch: 59 [12544/225000 (6%)] Loss: 7890.392578\n",
      "Train Epoch: 59 [16640/225000 (7%)] Loss: 8001.675781\n",
      "Train Epoch: 59 [20736/225000 (9%)] Loss: 8003.623047\n",
      "Train Epoch: 59 [24832/225000 (11%)] Loss: 8203.009766\n",
      "Train Epoch: 59 [28928/225000 (13%)] Loss: 8015.216797\n",
      "Train Epoch: 59 [33024/225000 (15%)] Loss: 8216.378906\n",
      "Train Epoch: 59 [37120/225000 (16%)] Loss: 8010.689453\n",
      "Train Epoch: 59 [41216/225000 (18%)] Loss: 8150.083984\n",
      "Train Epoch: 59 [45312/225000 (20%)] Loss: 7803.343750\n",
      "Train Epoch: 59 [49408/225000 (22%)] Loss: 8241.609375\n",
      "Train Epoch: 59 [53504/225000 (24%)] Loss: 7966.285156\n",
      "Train Epoch: 59 [57600/225000 (26%)] Loss: 8126.148438\n",
      "Train Epoch: 59 [61696/225000 (27%)] Loss: 7862.001953\n",
      "Train Epoch: 59 [65792/225000 (29%)] Loss: 8178.707031\n",
      "Train Epoch: 59 [69888/225000 (31%)] Loss: 7970.130859\n",
      "Train Epoch: 59 [73984/225000 (33%)] Loss: 8010.630859\n",
      "Train Epoch: 59 [78080/225000 (35%)] Loss: 8029.070312\n",
      "Train Epoch: 59 [82176/225000 (37%)] Loss: 8110.257812\n",
      "Train Epoch: 59 [86272/225000 (38%)] Loss: 8033.830078\n",
      "Train Epoch: 59 [90368/225000 (40%)] Loss: 8112.541016\n",
      "Train Epoch: 59 [94464/225000 (42%)] Loss: 8006.650391\n",
      "Train Epoch: 59 [98560/225000 (44%)] Loss: 8118.906250\n",
      "Train Epoch: 59 [102656/225000 (46%)] Loss: 8096.281250\n",
      "Train Epoch: 59 [106752/225000 (47%)] Loss: 8120.748047\n",
      "Train Epoch: 59 [110848/225000 (49%)] Loss: 8157.693359\n",
      "Train Epoch: 59 [114944/225000 (51%)] Loss: 8149.675781\n",
      "Train Epoch: 59 [119040/225000 (53%)] Loss: 7995.361328\n",
      "Train Epoch: 59 [123136/225000 (55%)] Loss: 8225.507812\n",
      "Train Epoch: 59 [127232/225000 (57%)] Loss: 8087.404297\n",
      "Train Epoch: 59 [131328/225000 (58%)] Loss: 7951.259766\n",
      "Train Epoch: 59 [135424/225000 (60%)] Loss: 8106.750000\n",
      "Train Epoch: 59 [139520/225000 (62%)] Loss: 7962.343750\n",
      "Train Epoch: 59 [143616/225000 (64%)] Loss: 8126.953125\n",
      "Train Epoch: 59 [147712/225000 (66%)] Loss: 8185.937500\n",
      "Train Epoch: 59 [151808/225000 (67%)] Loss: 8109.583984\n",
      "Train Epoch: 59 [155904/225000 (69%)] Loss: 8137.109375\n",
      "Train Epoch: 59 [160000/225000 (71%)] Loss: 8000.998047\n",
      "Train Epoch: 59 [164096/225000 (73%)] Loss: 8024.626953\n",
      "Train Epoch: 59 [168192/225000 (75%)] Loss: 7979.599609\n",
      "Train Epoch: 59 [172288/225000 (77%)] Loss: 8124.005859\n",
      "Train Epoch: 59 [176384/225000 (78%)] Loss: 8089.208984\n",
      "Train Epoch: 59 [180480/225000 (80%)] Loss: 8092.935547\n",
      "Train Epoch: 59 [184576/225000 (82%)] Loss: 7870.187500\n",
      "Train Epoch: 59 [188672/225000 (84%)] Loss: 8144.482422\n",
      "Train Epoch: 59 [192768/225000 (86%)] Loss: 7980.365234\n",
      "Train Epoch: 59 [196864/225000 (87%)] Loss: 8024.552734\n",
      "Train Epoch: 59 [200960/225000 (89%)] Loss: 7992.914062\n",
      "Train Epoch: 59 [205056/225000 (91%)] Loss: 8345.328125\n",
      "Train Epoch: 59 [209152/225000 (93%)] Loss: 7930.994141\n",
      "Train Epoch: 59 [213248/225000 (95%)] Loss: 8186.703125\n",
      "Train Epoch: 59 [217344/225000 (97%)] Loss: 8067.912109\n",
      "Train Epoch: 59 [221440/225000 (98%)] Loss: 8025.033203\n",
      "    epoch          : 59\n",
      "    loss           : 8074.344554358646\n",
      "    val_loss       : 8167.240368667914\n",
      "Train Epoch: 60 [256/225000 (0%)] Loss: 8094.658203\n",
      "Train Epoch: 60 [4352/225000 (2%)] Loss: 7897.496094\n",
      "Train Epoch: 60 [8448/225000 (4%)] Loss: 8004.023438\n",
      "Train Epoch: 60 [12544/225000 (6%)] Loss: 8078.810547\n",
      "Train Epoch: 60 [16640/225000 (7%)] Loss: 8006.878906\n",
      "Train Epoch: 60 [20736/225000 (9%)] Loss: 7983.583984\n",
      "Train Epoch: 60 [24832/225000 (11%)] Loss: 8010.271484\n",
      "Train Epoch: 60 [28928/225000 (13%)] Loss: 7969.281250\n",
      "Train Epoch: 60 [33024/225000 (15%)] Loss: 8055.039062\n",
      "Train Epoch: 60 [37120/225000 (16%)] Loss: 7918.730469\n",
      "Train Epoch: 60 [41216/225000 (18%)] Loss: 7923.957031\n",
      "Train Epoch: 60 [45312/225000 (20%)] Loss: 8003.009766\n",
      "Train Epoch: 60 [49408/225000 (22%)] Loss: 8129.806641\n",
      "Train Epoch: 60 [53504/225000 (24%)] Loss: 8103.052734\n",
      "Train Epoch: 60 [57600/225000 (26%)] Loss: 7925.134766\n",
      "Train Epoch: 60 [61696/225000 (27%)] Loss: 8285.294922\n",
      "Train Epoch: 60 [65792/225000 (29%)] Loss: 8055.679688\n",
      "Train Epoch: 60 [69888/225000 (31%)] Loss: 7931.517578\n",
      "Train Epoch: 60 [73984/225000 (33%)] Loss: 8095.583984\n",
      "Train Epoch: 60 [78080/225000 (35%)] Loss: 8167.701172\n",
      "Train Epoch: 60 [82176/225000 (37%)] Loss: 8033.994141\n",
      "Train Epoch: 60 [86272/225000 (38%)] Loss: 8031.539062\n",
      "Train Epoch: 60 [90368/225000 (40%)] Loss: 8069.029297\n",
      "Train Epoch: 60 [94464/225000 (42%)] Loss: 7969.939453\n",
      "Train Epoch: 60 [98560/225000 (44%)] Loss: 7900.632812\n",
      "Train Epoch: 60 [102656/225000 (46%)] Loss: 7974.294922\n",
      "Train Epoch: 60 [106752/225000 (47%)] Loss: 8120.263672\n",
      "Train Epoch: 60 [110848/225000 (49%)] Loss: 8029.544922\n",
      "Train Epoch: 60 [114944/225000 (51%)] Loss: 8255.089844\n",
      "Train Epoch: 60 [119040/225000 (53%)] Loss: 8147.818359\n",
      "Train Epoch: 60 [123136/225000 (55%)] Loss: 8103.080078\n",
      "Train Epoch: 60 [127232/225000 (57%)] Loss: 7969.837891\n",
      "Train Epoch: 60 [131328/225000 (58%)] Loss: 7932.095703\n",
      "Train Epoch: 60 [135424/225000 (60%)] Loss: 8088.703125\n",
      "Train Epoch: 60 [139520/225000 (62%)] Loss: 7999.882812\n",
      "Train Epoch: 60 [143616/225000 (64%)] Loss: 7854.316406\n",
      "Train Epoch: 60 [147712/225000 (66%)] Loss: 8032.955078\n",
      "Train Epoch: 60 [151808/225000 (67%)] Loss: 7937.857422\n",
      "Train Epoch: 60 [155904/225000 (69%)] Loss: 7835.628906\n",
      "Train Epoch: 60 [160000/225000 (71%)] Loss: 7998.669922\n",
      "Train Epoch: 60 [164096/225000 (73%)] Loss: 7889.683594\n",
      "Train Epoch: 60 [168192/225000 (75%)] Loss: 8054.970703\n",
      "Train Epoch: 60 [172288/225000 (77%)] Loss: 7780.916016\n",
      "Train Epoch: 60 [176384/225000 (78%)] Loss: 8053.431641\n",
      "Train Epoch: 60 [180480/225000 (80%)] Loss: 7964.128906\n",
      "Train Epoch: 60 [184576/225000 (82%)] Loss: 8054.154297\n",
      "Train Epoch: 60 [188672/225000 (84%)] Loss: 7922.013672\n",
      "Train Epoch: 60 [192768/225000 (86%)] Loss: 7857.269531\n",
      "Train Epoch: 60 [196864/225000 (87%)] Loss: 7924.521484\n",
      "Train Epoch: 60 [200960/225000 (89%)] Loss: 7962.468750\n",
      "Train Epoch: 60 [205056/225000 (91%)] Loss: 7962.564453\n",
      "Train Epoch: 60 [209152/225000 (93%)] Loss: 7782.269531\n",
      "Train Epoch: 60 [213248/225000 (95%)] Loss: 7944.130859\n",
      "Train Epoch: 60 [217344/225000 (97%)] Loss: 8108.949219\n",
      "Train Epoch: 60 [221440/225000 (98%)] Loss: 7936.927734\n",
      "    epoch          : 60\n",
      "    loss           : 8036.604398864121\n",
      "    val_loss       : 8012.150722731133\n",
      "Train Epoch: 61 [256/225000 (0%)] Loss: 8065.162109\n",
      "Train Epoch: 61 [4352/225000 (2%)] Loss: 8134.394531\n",
      "Train Epoch: 61 [8448/225000 (4%)] Loss: 7889.261719\n",
      "Train Epoch: 61 [12544/225000 (6%)] Loss: 8095.195312\n",
      "Train Epoch: 61 [16640/225000 (7%)] Loss: 8156.732422\n",
      "Train Epoch: 61 [20736/225000 (9%)] Loss: 8122.076172\n",
      "Train Epoch: 61 [24832/225000 (11%)] Loss: 8078.195312\n",
      "Train Epoch: 61 [28928/225000 (13%)] Loss: 7663.263672\n",
      "Train Epoch: 61 [33024/225000 (15%)] Loss: 7990.505859\n",
      "Train Epoch: 61 [37120/225000 (16%)] Loss: 7911.531250\n",
      "Train Epoch: 61 [41216/225000 (18%)] Loss: 7972.486328\n",
      "Train Epoch: 61 [45312/225000 (20%)] Loss: 7965.343750\n",
      "Train Epoch: 61 [49408/225000 (22%)] Loss: 8022.228516\n",
      "Train Epoch: 61 [53504/225000 (24%)] Loss: 8008.029297\n",
      "Train Epoch: 61 [57600/225000 (26%)] Loss: 7830.863281\n",
      "Train Epoch: 61 [61696/225000 (27%)] Loss: 7967.507812\n",
      "Train Epoch: 61 [65792/225000 (29%)] Loss: 7870.208984\n",
      "Train Epoch: 61 [69888/225000 (31%)] Loss: 8132.609375\n",
      "Train Epoch: 61 [73984/225000 (33%)] Loss: 8257.970703\n",
      "Train Epoch: 61 [78080/225000 (35%)] Loss: 7891.914062\n",
      "Train Epoch: 61 [82176/225000 (37%)] Loss: 7994.353516\n",
      "Train Epoch: 61 [86272/225000 (38%)] Loss: 8148.957031\n",
      "Train Epoch: 61 [90368/225000 (40%)] Loss: 8007.671875\n",
      "Train Epoch: 61 [94464/225000 (42%)] Loss: 7984.876953\n",
      "Train Epoch: 61 [98560/225000 (44%)] Loss: 8084.576172\n",
      "Train Epoch: 61 [102656/225000 (46%)] Loss: 8122.513672\n",
      "Train Epoch: 61 [106752/225000 (47%)] Loss: 7978.214844\n",
      "Train Epoch: 61 [110848/225000 (49%)] Loss: 8302.900391\n",
      "Train Epoch: 61 [114944/225000 (51%)] Loss: 7897.939453\n",
      "Train Epoch: 61 [119040/225000 (53%)] Loss: 8173.640625\n",
      "Train Epoch: 61 [123136/225000 (55%)] Loss: 7939.210938\n",
      "Train Epoch: 61 [127232/225000 (57%)] Loss: 8021.480469\n",
      "Train Epoch: 61 [131328/225000 (58%)] Loss: 7901.804688\n",
      "Train Epoch: 61 [135424/225000 (60%)] Loss: 8089.341797\n",
      "Train Epoch: 61 [139520/225000 (62%)] Loss: 7891.251953\n",
      "Train Epoch: 61 [143616/225000 (64%)] Loss: 7892.240234\n",
      "Train Epoch: 61 [147712/225000 (66%)] Loss: 8115.025391\n",
      "Train Epoch: 61 [151808/225000 (67%)] Loss: 8031.796875\n",
      "Train Epoch: 61 [155904/225000 (69%)] Loss: 7963.255859\n",
      "Train Epoch: 61 [160000/225000 (71%)] Loss: 8074.953125\n",
      "Train Epoch: 61 [164096/225000 (73%)] Loss: 7943.177734\n",
      "Train Epoch: 61 [168192/225000 (75%)] Loss: 7995.583984\n",
      "Train Epoch: 61 [172288/225000 (77%)] Loss: 8032.533203\n",
      "Train Epoch: 61 [176384/225000 (78%)] Loss: 7941.513672\n",
      "Train Epoch: 61 [180480/225000 (80%)] Loss: 7908.250000\n",
      "Train Epoch: 61 [184576/225000 (82%)] Loss: 8125.753906\n",
      "Train Epoch: 61 [188672/225000 (84%)] Loss: 8016.480469\n",
      "Train Epoch: 61 [192768/225000 (86%)] Loss: 8075.630859\n",
      "Train Epoch: 61 [196864/225000 (87%)] Loss: 7921.525391\n",
      "Train Epoch: 61 [200960/225000 (89%)] Loss: 7886.597656\n",
      "Train Epoch: 61 [205056/225000 (91%)] Loss: 7993.390625\n",
      "Train Epoch: 61 [209152/225000 (93%)] Loss: 7849.324219\n",
      "Train Epoch: 61 [213248/225000 (95%)] Loss: 8034.449219\n",
      "Train Epoch: 61 [217344/225000 (97%)] Loss: 7752.298828\n",
      "Train Epoch: 61 [221440/225000 (98%)] Loss: 7999.681641\n",
      "    epoch          : 61\n",
      "    loss           : 8028.876863134599\n",
      "    val_loss       : 7993.174952970476\n",
      "Train Epoch: 62 [256/225000 (0%)] Loss: 8116.169922\n",
      "Train Epoch: 62 [4352/225000 (2%)] Loss: 8216.876953\n",
      "Train Epoch: 62 [8448/225000 (4%)] Loss: 8124.966797\n",
      "Train Epoch: 62 [12544/225000 (6%)] Loss: 8023.917969\n",
      "Train Epoch: 62 [16640/225000 (7%)] Loss: 8064.394531\n",
      "Train Epoch: 62 [20736/225000 (9%)] Loss: 8260.115234\n",
      "Train Epoch: 62 [24832/225000 (11%)] Loss: 7978.167969\n",
      "Train Epoch: 62 [28928/225000 (13%)] Loss: 8060.312500\n",
      "Train Epoch: 62 [33024/225000 (15%)] Loss: 7996.591797\n",
      "Train Epoch: 62 [37120/225000 (16%)] Loss: 7972.779297\n",
      "Train Epoch: 62 [41216/225000 (18%)] Loss: 7924.423828\n",
      "Train Epoch: 62 [45312/225000 (20%)] Loss: 7838.298828\n",
      "Train Epoch: 62 [49408/225000 (22%)] Loss: 8114.052734\n",
      "Train Epoch: 62 [53504/225000 (24%)] Loss: 8130.867188\n",
      "Train Epoch: 62 [57600/225000 (26%)] Loss: 7981.548828\n",
      "Train Epoch: 62 [61696/225000 (27%)] Loss: 8057.683594\n",
      "Train Epoch: 62 [65792/225000 (29%)] Loss: 8028.328125\n",
      "Train Epoch: 62 [69888/225000 (31%)] Loss: 7851.673828\n",
      "Train Epoch: 62 [73984/225000 (33%)] Loss: 8035.537109\n",
      "Train Epoch: 62 [78080/225000 (35%)] Loss: 8173.552734\n",
      "Train Epoch: 62 [82176/225000 (37%)] Loss: 8287.558594\n",
      "Train Epoch: 62 [86272/225000 (38%)] Loss: 7700.671875\n",
      "Train Epoch: 62 [90368/225000 (40%)] Loss: 7974.000000\n",
      "Train Epoch: 62 [94464/225000 (42%)] Loss: 8087.494141\n",
      "Train Epoch: 62 [98560/225000 (44%)] Loss: 8091.695312\n",
      "Train Epoch: 62 [102656/225000 (46%)] Loss: 7957.365234\n",
      "Train Epoch: 62 [106752/225000 (47%)] Loss: 7947.158203\n",
      "Train Epoch: 62 [110848/225000 (49%)] Loss: 7912.548828\n",
      "Train Epoch: 62 [114944/225000 (51%)] Loss: 7914.142578\n",
      "Train Epoch: 62 [119040/225000 (53%)] Loss: 7997.638672\n",
      "Train Epoch: 62 [123136/225000 (55%)] Loss: 8020.259766\n",
      "Train Epoch: 62 [127232/225000 (57%)] Loss: 8213.511719\n",
      "Train Epoch: 62 [131328/225000 (58%)] Loss: 8049.486328\n",
      "Train Epoch: 62 [135424/225000 (60%)] Loss: 7980.724609\n",
      "Train Epoch: 62 [139520/225000 (62%)] Loss: 7992.310547\n",
      "Train Epoch: 62 [143616/225000 (64%)] Loss: 7881.121094\n",
      "Train Epoch: 62 [147712/225000 (66%)] Loss: 7922.470703\n",
      "Train Epoch: 62 [151808/225000 (67%)] Loss: 7948.771484\n",
      "Train Epoch: 62 [155904/225000 (69%)] Loss: 8017.619141\n",
      "Train Epoch: 62 [160000/225000 (71%)] Loss: 8060.158203\n",
      "Train Epoch: 62 [164096/225000 (73%)] Loss: 7995.361328\n",
      "Train Epoch: 62 [168192/225000 (75%)] Loss: 7887.828125\n",
      "Train Epoch: 62 [172288/225000 (77%)] Loss: 7938.136719\n",
      "Train Epoch: 62 [176384/225000 (78%)] Loss: 7991.826172\n",
      "Train Epoch: 62 [180480/225000 (80%)] Loss: 8103.324219\n",
      "Train Epoch: 62 [184576/225000 (82%)] Loss: 7974.652344\n",
      "Train Epoch: 62 [188672/225000 (84%)] Loss: 8035.080078\n",
      "Train Epoch: 62 [192768/225000 (86%)] Loss: 7981.126953\n",
      "Train Epoch: 62 [196864/225000 (87%)] Loss: 7968.687500\n",
      "Train Epoch: 62 [200960/225000 (89%)] Loss: 7928.255859\n",
      "Train Epoch: 62 [205056/225000 (91%)] Loss: 7933.949219\n",
      "Train Epoch: 62 [209152/225000 (93%)] Loss: 7989.810547\n",
      "Train Epoch: 62 [213248/225000 (95%)] Loss: 8186.978516\n",
      "Train Epoch: 62 [217344/225000 (97%)] Loss: 8001.371094\n",
      "Train Epoch: 62 [221440/225000 (98%)] Loss: 8043.689453\n",
      "    epoch          : 62\n",
      "    loss           : 8034.802178878697\n",
      "    val_loss       : 7977.995826154339\n",
      "Train Epoch: 63 [256/225000 (0%)] Loss: 8044.343750\n",
      "Train Epoch: 63 [4352/225000 (2%)] Loss: 7998.757812\n",
      "Train Epoch: 63 [8448/225000 (4%)] Loss: 7992.562500\n",
      "Train Epoch: 63 [12544/225000 (6%)] Loss: 8066.566406\n",
      "Train Epoch: 63 [16640/225000 (7%)] Loss: 8215.789062\n",
      "Train Epoch: 63 [20736/225000 (9%)] Loss: 7889.281250\n",
      "Train Epoch: 63 [24832/225000 (11%)] Loss: 7926.529297\n",
      "Train Epoch: 63 [28928/225000 (13%)] Loss: 8011.886719\n",
      "Train Epoch: 63 [33024/225000 (15%)] Loss: 8228.611328\n",
      "Train Epoch: 63 [37120/225000 (16%)] Loss: 8032.812500\n",
      "Train Epoch: 63 [41216/225000 (18%)] Loss: 7895.392578\n",
      "Train Epoch: 63 [45312/225000 (20%)] Loss: 8136.658203\n",
      "Train Epoch: 63 [49408/225000 (22%)] Loss: 8022.798828\n",
      "Train Epoch: 63 [53504/225000 (24%)] Loss: 8070.974609\n",
      "Train Epoch: 63 [57600/225000 (26%)] Loss: 8052.750000\n",
      "Train Epoch: 63 [61696/225000 (27%)] Loss: 7872.791016\n",
      "Train Epoch: 63 [65792/225000 (29%)] Loss: 8019.177734\n",
      "Train Epoch: 63 [69888/225000 (31%)] Loss: 7947.755859\n",
      "Train Epoch: 63 [73984/225000 (33%)] Loss: 7871.742188\n",
      "Train Epoch: 63 [78080/225000 (35%)] Loss: 7878.625000\n",
      "Train Epoch: 63 [82176/225000 (37%)] Loss: 8144.947266\n",
      "Train Epoch: 63 [86272/225000 (38%)] Loss: 7992.980469\n",
      "Train Epoch: 63 [90368/225000 (40%)] Loss: 8024.441406\n",
      "Train Epoch: 63 [94464/225000 (42%)] Loss: 7876.628906\n",
      "Train Epoch: 63 [98560/225000 (44%)] Loss: 8098.494141\n",
      "Train Epoch: 63 [102656/225000 (46%)] Loss: 7918.955078\n",
      "Train Epoch: 63 [106752/225000 (47%)] Loss: 7923.736328\n",
      "Train Epoch: 63 [110848/225000 (49%)] Loss: 8002.287109\n",
      "Train Epoch: 63 [114944/225000 (51%)] Loss: 7974.771484\n",
      "Train Epoch: 63 [119040/225000 (53%)] Loss: 8036.244141\n",
      "Train Epoch: 63 [123136/225000 (55%)] Loss: 8042.492188\n",
      "Train Epoch: 63 [127232/225000 (57%)] Loss: 7871.431641\n",
      "Train Epoch: 63 [131328/225000 (58%)] Loss: 8069.425781\n",
      "Train Epoch: 63 [135424/225000 (60%)] Loss: 8073.218750\n",
      "Train Epoch: 63 [139520/225000 (62%)] Loss: 7950.248047\n",
      "Train Epoch: 63 [143616/225000 (64%)] Loss: 7915.279297\n",
      "Train Epoch: 63 [147712/225000 (66%)] Loss: 8265.837891\n",
      "Train Epoch: 63 [151808/225000 (67%)] Loss: 7995.640625\n",
      "Train Epoch: 63 [155904/225000 (69%)] Loss: 8107.925781\n",
      "Train Epoch: 63 [160000/225000 (71%)] Loss: 7860.181641\n",
      "Train Epoch: 63 [164096/225000 (73%)] Loss: 8074.722656\n",
      "Train Epoch: 63 [168192/225000 (75%)] Loss: 7897.656250\n",
      "Train Epoch: 63 [172288/225000 (77%)] Loss: 7949.931641\n",
      "Train Epoch: 63 [176384/225000 (78%)] Loss: 7796.812500\n",
      "Train Epoch: 63 [180480/225000 (80%)] Loss: 8029.326172\n",
      "Train Epoch: 63 [184576/225000 (82%)] Loss: 7932.386719\n",
      "Train Epoch: 63 [188672/225000 (84%)] Loss: 8059.001953\n",
      "Train Epoch: 63 [192768/225000 (86%)] Loss: 7925.142578\n",
      "Train Epoch: 63 [196864/225000 (87%)] Loss: 8134.232422\n",
      "Train Epoch: 63 [200960/225000 (89%)] Loss: 7931.814453\n",
      "Train Epoch: 63 [205056/225000 (91%)] Loss: 7830.218750\n",
      "Train Epoch: 63 [209152/225000 (93%)] Loss: 7975.570312\n",
      "Train Epoch: 63 [213248/225000 (95%)] Loss: 8284.207031\n",
      "Train Epoch: 63 [217344/225000 (97%)] Loss: 7799.326172\n",
      "Train Epoch: 63 [221440/225000 (98%)] Loss: 8132.056641\n",
      "    epoch          : 63\n",
      "    loss           : 7995.880533854167\n",
      "    val_loss       : 7967.416507365752\n",
      "Train Epoch: 64 [256/225000 (0%)] Loss: 8086.117188\n",
      "Train Epoch: 64 [4352/225000 (2%)] Loss: 8023.005859\n",
      "Train Epoch: 64 [8448/225000 (4%)] Loss: 8050.060547\n",
      "Train Epoch: 64 [12544/225000 (6%)] Loss: 8016.310547\n",
      "Train Epoch: 64 [16640/225000 (7%)] Loss: 7809.203125\n",
      "Train Epoch: 64 [20736/225000 (9%)] Loss: 7897.439453\n",
      "Train Epoch: 64 [24832/225000 (11%)] Loss: 7857.900391\n",
      "Train Epoch: 64 [28928/225000 (13%)] Loss: 7938.648438\n",
      "Train Epoch: 64 [33024/225000 (15%)] Loss: 7948.925781\n",
      "Train Epoch: 64 [37120/225000 (16%)] Loss: 7852.244141\n",
      "Train Epoch: 64 [41216/225000 (18%)] Loss: 8164.380859\n",
      "Train Epoch: 64 [45312/225000 (20%)] Loss: 7932.818359\n",
      "Train Epoch: 64 [49408/225000 (22%)] Loss: 8083.722656\n",
      "Train Epoch: 64 [53504/225000 (24%)] Loss: 8130.601562\n",
      "Train Epoch: 64 [57600/225000 (26%)] Loss: 8045.029297\n",
      "Train Epoch: 64 [61696/225000 (27%)] Loss: 8101.654297\n",
      "Train Epoch: 64 [65792/225000 (29%)] Loss: 7787.423828\n",
      "Train Epoch: 64 [69888/225000 (31%)] Loss: 8011.847656\n",
      "Train Epoch: 64 [73984/225000 (33%)] Loss: 8023.583984\n",
      "Train Epoch: 64 [78080/225000 (35%)] Loss: 8054.283203\n",
      "Train Epoch: 64 [82176/225000 (37%)] Loss: 8072.429688\n",
      "Train Epoch: 64 [86272/225000 (38%)] Loss: 8039.330078\n",
      "Train Epoch: 64 [90368/225000 (40%)] Loss: 7988.824219\n",
      "Train Epoch: 64 [94464/225000 (42%)] Loss: 7968.894531\n",
      "Train Epoch: 64 [98560/225000 (44%)] Loss: 8168.576172\n",
      "Train Epoch: 64 [102656/225000 (46%)] Loss: 8094.697266\n",
      "Train Epoch: 64 [106752/225000 (47%)] Loss: 7937.076172\n",
      "Train Epoch: 64 [110848/225000 (49%)] Loss: 8113.720703\n",
      "Train Epoch: 64 [114944/225000 (51%)] Loss: 8143.537109\n",
      "Train Epoch: 64 [119040/225000 (53%)] Loss: 7930.966797\n",
      "Train Epoch: 64 [123136/225000 (55%)] Loss: 8037.046875\n",
      "Train Epoch: 64 [127232/225000 (57%)] Loss: 8189.931641\n",
      "Train Epoch: 64 [131328/225000 (58%)] Loss: 7848.730469\n",
      "Train Epoch: 64 [135424/225000 (60%)] Loss: 7987.263672\n",
      "Train Epoch: 64 [139520/225000 (62%)] Loss: 7924.232422\n",
      "Train Epoch: 64 [143616/225000 (64%)] Loss: 8078.687500\n",
      "Train Epoch: 64 [147712/225000 (66%)] Loss: 7981.759766\n",
      "Train Epoch: 64 [151808/225000 (67%)] Loss: 8011.390625\n",
      "Train Epoch: 64 [155904/225000 (69%)] Loss: 7966.275391\n",
      "Train Epoch: 64 [160000/225000 (71%)] Loss: 8004.291016\n",
      "Train Epoch: 64 [164096/225000 (73%)] Loss: 7964.367188\n",
      "Train Epoch: 64 [168192/225000 (75%)] Loss: 7888.169922\n",
      "Train Epoch: 64 [172288/225000 (77%)] Loss: 7975.775391\n",
      "Train Epoch: 64 [176384/225000 (78%)] Loss: 7914.710938\n",
      "Train Epoch: 64 [180480/225000 (80%)] Loss: 7814.916016\n",
      "Train Epoch: 64 [184576/225000 (82%)] Loss: 7970.308594\n",
      "Train Epoch: 64 [188672/225000 (84%)] Loss: 8061.931641\n",
      "Train Epoch: 64 [192768/225000 (86%)] Loss: 7908.197266\n",
      "Train Epoch: 64 [196864/225000 (87%)] Loss: 8042.562500\n",
      "Train Epoch: 64 [200960/225000 (89%)] Loss: 7898.349609\n",
      "Train Epoch: 64 [205056/225000 (91%)] Loss: 7933.041016\n",
      "Train Epoch: 64 [209152/225000 (93%)] Loss: 7907.982422\n",
      "Train Epoch: 64 [213248/225000 (95%)] Loss: 7935.011719\n",
      "Train Epoch: 64 [217344/225000 (97%)] Loss: 8038.072266\n",
      "Train Epoch: 64 [221440/225000 (98%)] Loss: 8003.505859\n",
      "    epoch          : 64\n",
      "    loss           : 8005.386924283632\n",
      "    val_loss       : 7958.461486540278\n",
      "Train Epoch: 65 [256/225000 (0%)] Loss: 8087.738281\n",
      "Train Epoch: 65 [4352/225000 (2%)] Loss: 7874.183594\n",
      "Train Epoch: 65 [8448/225000 (4%)] Loss: 7932.587891\n",
      "Train Epoch: 65 [12544/225000 (6%)] Loss: 8059.916016\n",
      "Train Epoch: 65 [16640/225000 (7%)] Loss: 8050.191406\n",
      "Train Epoch: 65 [20736/225000 (9%)] Loss: 8113.074219\n",
      "Train Epoch: 65 [24832/225000 (11%)] Loss: 7947.087891\n",
      "Train Epoch: 65 [28928/225000 (13%)] Loss: 7936.664062\n",
      "Train Epoch: 65 [33024/225000 (15%)] Loss: 7962.451172\n",
      "Train Epoch: 65 [37120/225000 (16%)] Loss: 8098.064453\n",
      "Train Epoch: 65 [41216/225000 (18%)] Loss: 7886.351562\n",
      "Train Epoch: 65 [45312/225000 (20%)] Loss: 8036.283203\n",
      "Train Epoch: 65 [49408/225000 (22%)] Loss: 7984.587891\n",
      "Train Epoch: 65 [53504/225000 (24%)] Loss: 8019.992188\n",
      "Train Epoch: 65 [57600/225000 (26%)] Loss: 7961.904297\n",
      "Train Epoch: 65 [61696/225000 (27%)] Loss: 7836.451172\n",
      "Train Epoch: 65 [65792/225000 (29%)] Loss: 8030.716797\n",
      "Train Epoch: 65 [69888/225000 (31%)] Loss: 8184.978516\n",
      "Train Epoch: 65 [73984/225000 (33%)] Loss: 7944.691406\n",
      "Train Epoch: 65 [78080/225000 (35%)] Loss: 7881.783203\n",
      "Train Epoch: 65 [82176/225000 (37%)] Loss: 7909.382812\n",
      "Train Epoch: 65 [86272/225000 (38%)] Loss: 7955.023438\n",
      "Train Epoch: 65 [90368/225000 (40%)] Loss: 7900.941406\n",
      "Train Epoch: 65 [94464/225000 (42%)] Loss: 8075.193359\n",
      "Train Epoch: 65 [98560/225000 (44%)] Loss: 7849.744141\n",
      "Train Epoch: 65 [102656/225000 (46%)] Loss: 7995.824219\n",
      "Train Epoch: 65 [106752/225000 (47%)] Loss: 7954.580078\n",
      "Train Epoch: 65 [110848/225000 (49%)] Loss: 7897.064453\n",
      "Train Epoch: 65 [114944/225000 (51%)] Loss: 7972.361328\n",
      "Train Epoch: 65 [119040/225000 (53%)] Loss: 8007.132812\n",
      "Train Epoch: 65 [123136/225000 (55%)] Loss: 8216.591797\n",
      "Train Epoch: 65 [127232/225000 (57%)] Loss: 7955.333984\n",
      "Train Epoch: 65 [131328/225000 (58%)] Loss: 7922.890625\n",
      "Train Epoch: 65 [135424/225000 (60%)] Loss: 7973.156250\n",
      "Train Epoch: 65 [139520/225000 (62%)] Loss: 7916.875000\n",
      "Train Epoch: 65 [143616/225000 (64%)] Loss: 7865.357422\n",
      "Train Epoch: 65 [147712/225000 (66%)] Loss: 7912.509766\n",
      "Train Epoch: 65 [151808/225000 (67%)] Loss: 7888.339844\n",
      "Train Epoch: 65 [155904/225000 (69%)] Loss: 7772.767578\n",
      "Train Epoch: 65 [160000/225000 (71%)] Loss: 7929.998047\n",
      "Train Epoch: 65 [164096/225000 (73%)] Loss: 7922.837891\n",
      "Train Epoch: 65 [168192/225000 (75%)] Loss: 7942.873047\n",
      "Train Epoch: 65 [172288/225000 (77%)] Loss: 7831.470703\n",
      "Train Epoch: 65 [176384/225000 (78%)] Loss: 7965.792969\n",
      "Train Epoch: 65 [180480/225000 (80%)] Loss: 7867.152344\n",
      "Train Epoch: 65 [184576/225000 (82%)] Loss: 8035.443359\n",
      "Train Epoch: 65 [188672/225000 (84%)] Loss: 7974.794922\n",
      "Train Epoch: 65 [192768/225000 (86%)] Loss: 7864.271484\n",
      "Train Epoch: 65 [196864/225000 (87%)] Loss: 7959.984375\n",
      "Train Epoch: 65 [200960/225000 (89%)] Loss: 8033.017578\n",
      "Train Epoch: 65 [205056/225000 (91%)] Loss: 8140.576172\n",
      "Train Epoch: 65 [209152/225000 (93%)] Loss: 7889.242188\n",
      "Train Epoch: 65 [213248/225000 (95%)] Loss: 8009.484375\n",
      "Train Epoch: 65 [217344/225000 (97%)] Loss: 8051.539062\n",
      "Train Epoch: 65 [221440/225000 (98%)] Loss: 7865.347656\n",
      "    epoch          : 65\n",
      "    loss           : 7995.102467958974\n",
      "    val_loss       : 8000.4730315719335\n",
      "Train Epoch: 66 [256/225000 (0%)] Loss: 8080.113281\n",
      "Train Epoch: 66 [4352/225000 (2%)] Loss: 8150.517578\n",
      "Train Epoch: 66 [8448/225000 (4%)] Loss: 7845.888672\n",
      "Train Epoch: 66 [12544/225000 (6%)] Loss: 7963.523438\n",
      "Train Epoch: 66 [16640/225000 (7%)] Loss: 8177.550781\n",
      "Train Epoch: 66 [20736/225000 (9%)] Loss: 7786.068359\n",
      "Train Epoch: 66 [24832/225000 (11%)] Loss: 7841.867188\n",
      "Train Epoch: 66 [28928/225000 (13%)] Loss: 8189.574219\n",
      "Train Epoch: 66 [33024/225000 (15%)] Loss: 7997.240234\n",
      "Train Epoch: 66 [37120/225000 (16%)] Loss: 7888.404297\n",
      "Train Epoch: 66 [41216/225000 (18%)] Loss: 8132.443359\n",
      "Train Epoch: 66 [45312/225000 (20%)] Loss: 7991.513672\n",
      "Train Epoch: 66 [49408/225000 (22%)] Loss: 7765.773438\n",
      "Train Epoch: 66 [53504/225000 (24%)] Loss: 7996.570312\n",
      "Train Epoch: 66 [57600/225000 (26%)] Loss: 8025.568359\n",
      "Train Epoch: 66 [61696/225000 (27%)] Loss: 7799.400391\n",
      "Train Epoch: 66 [65792/225000 (29%)] Loss: 7868.968750\n",
      "Train Epoch: 66 [69888/225000 (31%)] Loss: 7786.830078\n",
      "Train Epoch: 66 [73984/225000 (33%)] Loss: 7875.820312\n",
      "Train Epoch: 66 [78080/225000 (35%)] Loss: 7951.636719\n",
      "Train Epoch: 66 [82176/225000 (37%)] Loss: 8102.646484\n",
      "Train Epoch: 66 [86272/225000 (38%)] Loss: 7862.068359\n",
      "Train Epoch: 66 [90368/225000 (40%)] Loss: 7955.453125\n",
      "Train Epoch: 66 [94464/225000 (42%)] Loss: 7996.126953\n",
      "Train Epoch: 66 [98560/225000 (44%)] Loss: 8123.853516\n",
      "Train Epoch: 66 [102656/225000 (46%)] Loss: 7907.013672\n",
      "Train Epoch: 66 [106752/225000 (47%)] Loss: 7852.736328\n",
      "Train Epoch: 66 [110848/225000 (49%)] Loss: 7948.550781\n",
      "Train Epoch: 66 [114944/225000 (51%)] Loss: 7894.082031\n",
      "Train Epoch: 66 [119040/225000 (53%)] Loss: 8037.599609\n",
      "Train Epoch: 66 [123136/225000 (55%)] Loss: 8042.578125\n",
      "Train Epoch: 66 [127232/225000 (57%)] Loss: 7946.074219\n",
      "Train Epoch: 66 [131328/225000 (58%)] Loss: 7924.537109\n",
      "Train Epoch: 66 [135424/225000 (60%)] Loss: 7940.125000\n",
      "Train Epoch: 66 [139520/225000 (62%)] Loss: 7988.384766\n",
      "Train Epoch: 66 [143616/225000 (64%)] Loss: 8103.406250\n",
      "Train Epoch: 66 [147712/225000 (66%)] Loss: 7870.474609\n",
      "Train Epoch: 66 [151808/225000 (67%)] Loss: 7919.275391\n",
      "Train Epoch: 66 [155904/225000 (69%)] Loss: 7811.812500\n",
      "Train Epoch: 66 [160000/225000 (71%)] Loss: 7837.603516\n",
      "Train Epoch: 66 [164096/225000 (73%)] Loss: 7943.238281\n",
      "Train Epoch: 66 [168192/225000 (75%)] Loss: 7956.552734\n",
      "Train Epoch: 66 [172288/225000 (77%)] Loss: 7999.091797\n",
      "Train Epoch: 66 [176384/225000 (78%)] Loss: 7980.443359\n",
      "Train Epoch: 66 [180480/225000 (80%)] Loss: 8125.767578\n",
      "Train Epoch: 66 [184576/225000 (82%)] Loss: 8159.814453\n",
      "Train Epoch: 66 [188672/225000 (84%)] Loss: 7981.341797\n",
      "Train Epoch: 66 [192768/225000 (86%)] Loss: 7927.947266\n",
      "Train Epoch: 66 [196864/225000 (87%)] Loss: 7912.912109\n",
      "Train Epoch: 66 [200960/225000 (89%)] Loss: 8061.492188\n",
      "Train Epoch: 66 [205056/225000 (91%)] Loss: 8026.148438\n",
      "Train Epoch: 66 [209152/225000 (93%)] Loss: 8024.343750\n",
      "Train Epoch: 66 [213248/225000 (95%)] Loss: 8097.023438\n",
      "Train Epoch: 66 [217344/225000 (97%)] Loss: 8051.056641\n",
      "Train Epoch: 66 [221440/225000 (98%)] Loss: 7894.279297\n",
      "    epoch          : 66\n",
      "    loss           : 7956.302898801906\n",
      "    val_loss       : 7928.552840585611\n",
      "Train Epoch: 67 [256/225000 (0%)] Loss: 7838.654297\n",
      "Train Epoch: 67 [4352/225000 (2%)] Loss: 7939.957031\n",
      "Train Epoch: 67 [8448/225000 (4%)] Loss: 7801.941406\n",
      "Train Epoch: 67 [12544/225000 (6%)] Loss: 8105.542969\n",
      "Train Epoch: 67 [16640/225000 (7%)] Loss: 7944.685547\n",
      "Train Epoch: 67 [20736/225000 (9%)] Loss: 7900.216797\n",
      "Train Epoch: 67 [24832/225000 (11%)] Loss: 7997.558594\n",
      "Train Epoch: 67 [28928/225000 (13%)] Loss: 7959.164062\n",
      "Train Epoch: 67 [33024/225000 (15%)] Loss: 7938.562500\n",
      "Train Epoch: 67 [37120/225000 (16%)] Loss: 7774.826172\n",
      "Train Epoch: 67 [41216/225000 (18%)] Loss: 7868.564453\n",
      "Train Epoch: 67 [45312/225000 (20%)] Loss: 8011.406250\n",
      "Train Epoch: 67 [49408/225000 (22%)] Loss: 7826.093750\n",
      "Train Epoch: 67 [53504/225000 (24%)] Loss: 7845.525391\n",
      "Train Epoch: 67 [57600/225000 (26%)] Loss: 8053.164062\n",
      "Train Epoch: 67 [61696/225000 (27%)] Loss: 8014.310547\n",
      "Train Epoch: 67 [65792/225000 (29%)] Loss: 7868.615234\n",
      "Train Epoch: 67 [69888/225000 (31%)] Loss: 7738.628906\n",
      "Train Epoch: 67 [73984/225000 (33%)] Loss: 7863.148438\n",
      "Train Epoch: 67 [78080/225000 (35%)] Loss: 7769.605469\n",
      "Train Epoch: 67 [82176/225000 (37%)] Loss: 8070.140625\n",
      "Train Epoch: 67 [86272/225000 (38%)] Loss: 7876.763672\n",
      "Train Epoch: 67 [90368/225000 (40%)] Loss: 7930.792969\n",
      "Train Epoch: 67 [94464/225000 (42%)] Loss: 7865.669922\n",
      "Train Epoch: 67 [98560/225000 (44%)] Loss: 7884.560547\n",
      "Train Epoch: 67 [102656/225000 (46%)] Loss: 8054.443359\n",
      "Train Epoch: 67 [106752/225000 (47%)] Loss: 7946.441406\n",
      "Train Epoch: 67 [110848/225000 (49%)] Loss: 7790.275391\n",
      "Train Epoch: 67 [114944/225000 (51%)] Loss: 7793.542969\n",
      "Train Epoch: 67 [119040/225000 (53%)] Loss: 7973.742188\n",
      "Train Epoch: 67 [123136/225000 (55%)] Loss: 8126.636719\n",
      "Train Epoch: 67 [127232/225000 (57%)] Loss: 7951.314453\n",
      "Train Epoch: 67 [131328/225000 (58%)] Loss: 7976.626953\n",
      "Train Epoch: 67 [135424/225000 (60%)] Loss: 7797.355469\n",
      "Train Epoch: 67 [139520/225000 (62%)] Loss: 8032.705078\n",
      "Train Epoch: 67 [143616/225000 (64%)] Loss: 7956.900391\n",
      "Train Epoch: 67 [147712/225000 (66%)] Loss: 7968.509766\n",
      "Train Epoch: 67 [151808/225000 (67%)] Loss: 8057.984375\n",
      "Train Epoch: 67 [155904/225000 (69%)] Loss: 7824.533203\n",
      "Train Epoch: 67 [160000/225000 (71%)] Loss: 8033.140625\n",
      "Train Epoch: 67 [164096/225000 (73%)] Loss: 7769.560547\n",
      "Train Epoch: 67 [168192/225000 (75%)] Loss: 8179.470703\n",
      "Train Epoch: 67 [172288/225000 (77%)] Loss: 7809.347656\n",
      "Train Epoch: 67 [176384/225000 (78%)] Loss: 7644.244141\n",
      "Train Epoch: 67 [180480/225000 (80%)] Loss: 7900.289062\n",
      "Train Epoch: 67 [184576/225000 (82%)] Loss: 7880.873047\n",
      "Train Epoch: 67 [188672/225000 (84%)] Loss: 7864.916016\n",
      "Train Epoch: 67 [192768/225000 (86%)] Loss: 7831.507812\n",
      "Train Epoch: 67 [196864/225000 (87%)] Loss: 8032.179688\n",
      "Train Epoch: 67 [200960/225000 (89%)] Loss: 7698.320312\n",
      "Train Epoch: 67 [205056/225000 (91%)] Loss: 7937.855469\n",
      "Train Epoch: 67 [209152/225000 (93%)] Loss: 8123.476562\n",
      "Train Epoch: 67 [213248/225000 (95%)] Loss: 8026.476562\n",
      "Train Epoch: 67 [217344/225000 (97%)] Loss: 7924.636719\n",
      "Train Epoch: 67 [221440/225000 (98%)] Loss: 7733.460938\n",
      "    epoch          : 67\n",
      "    loss           : 7972.89827529508\n",
      "    val_loss       : 7919.41092335448\n",
      "Train Epoch: 68 [256/225000 (0%)] Loss: 7888.871094\n",
      "Train Epoch: 68 [4352/225000 (2%)] Loss: 8021.162109\n",
      "Train Epoch: 68 [8448/225000 (4%)] Loss: 7816.796875\n",
      "Train Epoch: 68 [12544/225000 (6%)] Loss: 8029.404297\n",
      "Train Epoch: 68 [16640/225000 (7%)] Loss: 8172.216797\n",
      "Train Epoch: 68 [20736/225000 (9%)] Loss: 7859.642578\n",
      "Train Epoch: 68 [24832/225000 (11%)] Loss: 7853.419922\n",
      "Train Epoch: 68 [28928/225000 (13%)] Loss: 7955.867188\n",
      "Train Epoch: 68 [33024/225000 (15%)] Loss: 8119.451172\n",
      "Train Epoch: 68 [37120/225000 (16%)] Loss: 8084.722656\n",
      "Train Epoch: 68 [41216/225000 (18%)] Loss: 8159.615234\n",
      "Train Epoch: 68 [45312/225000 (20%)] Loss: 7947.759766\n",
      "Train Epoch: 68 [49408/225000 (22%)] Loss: 8158.503906\n",
      "Train Epoch: 68 [53504/225000 (24%)] Loss: 8051.861328\n",
      "Train Epoch: 68 [57600/225000 (26%)] Loss: 8251.152344\n",
      "Train Epoch: 68 [61696/225000 (27%)] Loss: 7938.787109\n",
      "Train Epoch: 68 [65792/225000 (29%)] Loss: 7926.732422\n",
      "Train Epoch: 68 [69888/225000 (31%)] Loss: 7915.798828\n",
      "Train Epoch: 68 [73984/225000 (33%)] Loss: 7967.638672\n",
      "Train Epoch: 68 [78080/225000 (35%)] Loss: 7979.396484\n",
      "Train Epoch: 68 [82176/225000 (37%)] Loss: 8005.302734\n",
      "Train Epoch: 68 [86272/225000 (38%)] Loss: 7826.607422\n",
      "Train Epoch: 68 [90368/225000 (40%)] Loss: 7927.439453\n",
      "Train Epoch: 68 [94464/225000 (42%)] Loss: 8058.966797\n",
      "Train Epoch: 68 [98560/225000 (44%)] Loss: 8018.970703\n",
      "Train Epoch: 68 [102656/225000 (46%)] Loss: 8029.279297\n",
      "Train Epoch: 68 [106752/225000 (47%)] Loss: 7837.496094\n",
      "Train Epoch: 68 [110848/225000 (49%)] Loss: 7950.677734\n",
      "Train Epoch: 68 [114944/225000 (51%)] Loss: 7823.765625\n",
      "Train Epoch: 68 [119040/225000 (53%)] Loss: 7738.466797\n",
      "Train Epoch: 68 [123136/225000 (55%)] Loss: 8001.488281\n",
      "Train Epoch: 68 [127232/225000 (57%)] Loss: 7942.646484\n",
      "Train Epoch: 68 [131328/225000 (58%)] Loss: 8130.476562\n",
      "Train Epoch: 68 [135424/225000 (60%)] Loss: 7743.632812\n",
      "Train Epoch: 68 [139520/225000 (62%)] Loss: 7806.402344\n",
      "Train Epoch: 68 [143616/225000 (64%)] Loss: 7917.773438\n",
      "Train Epoch: 68 [147712/225000 (66%)] Loss: 7747.939453\n",
      "Train Epoch: 68 [151808/225000 (67%)] Loss: 7931.642578\n",
      "Train Epoch: 68 [155904/225000 (69%)] Loss: 7844.548828\n",
      "Train Epoch: 68 [160000/225000 (71%)] Loss: 8074.628906\n",
      "Train Epoch: 68 [164096/225000 (73%)] Loss: 8139.472656\n",
      "Train Epoch: 68 [168192/225000 (75%)] Loss: 7904.957031\n",
      "Train Epoch: 68 [172288/225000 (77%)] Loss: 7918.023438\n",
      "Train Epoch: 68 [176384/225000 (78%)] Loss: 7887.070312\n",
      "Train Epoch: 68 [180480/225000 (80%)] Loss: 7956.679688\n",
      "Train Epoch: 68 [184576/225000 (82%)] Loss: 7899.033203\n",
      "Train Epoch: 68 [188672/225000 (84%)] Loss: 7846.417969\n",
      "Train Epoch: 68 [192768/225000 (86%)] Loss: 7637.359375\n",
      "Train Epoch: 68 [196864/225000 (87%)] Loss: 7896.644531\n",
      "Train Epoch: 68 [200960/225000 (89%)] Loss: 7878.683594\n",
      "Train Epoch: 68 [205056/225000 (91%)] Loss: 8034.533203\n",
      "Train Epoch: 68 [209152/225000 (93%)] Loss: 7749.083984\n",
      "Train Epoch: 68 [213248/225000 (95%)] Loss: 7997.021484\n",
      "Train Epoch: 68 [217344/225000 (97%)] Loss: 8003.816406\n",
      "Train Epoch: 68 [221440/225000 (98%)] Loss: 7903.177734\n",
      "    epoch          : 68\n",
      "    loss           : 7979.26525392847\n",
      "    val_loss       : 7905.792778737691\n",
      "Train Epoch: 69 [256/225000 (0%)] Loss: 7768.437500\n",
      "Train Epoch: 69 [4352/225000 (2%)] Loss: 7913.236328\n",
      "Train Epoch: 69 [8448/225000 (4%)] Loss: 7801.886719\n",
      "Train Epoch: 69 [12544/225000 (6%)] Loss: 7784.468750\n",
      "Train Epoch: 69 [16640/225000 (7%)] Loss: 7776.511719\n",
      "Train Epoch: 69 [20736/225000 (9%)] Loss: 7847.449219\n",
      "Train Epoch: 69 [24832/225000 (11%)] Loss: 7924.056641\n",
      "Train Epoch: 69 [28928/225000 (13%)] Loss: 8019.677734\n",
      "Train Epoch: 69 [33024/225000 (15%)] Loss: 7819.365234\n",
      "Train Epoch: 69 [37120/225000 (16%)] Loss: 7774.787109\n",
      "Train Epoch: 69 [41216/225000 (18%)] Loss: 8087.625000\n",
      "Train Epoch: 69 [45312/225000 (20%)] Loss: 7962.865234\n",
      "Train Epoch: 69 [49408/225000 (22%)] Loss: 8020.291016\n",
      "Train Epoch: 69 [53504/225000 (24%)] Loss: 7749.583984\n",
      "Train Epoch: 69 [57600/225000 (26%)] Loss: 7849.935547\n",
      "Train Epoch: 69 [61696/225000 (27%)] Loss: 7890.851562\n",
      "Train Epoch: 69 [65792/225000 (29%)] Loss: 7978.275391\n",
      "Train Epoch: 69 [69888/225000 (31%)] Loss: 7739.488281\n",
      "Train Epoch: 69 [73984/225000 (33%)] Loss: 8015.984375\n",
      "Train Epoch: 69 [78080/225000 (35%)] Loss: 7898.425781\n",
      "Train Epoch: 69 [82176/225000 (37%)] Loss: 8021.751953\n",
      "Train Epoch: 69 [86272/225000 (38%)] Loss: 7873.154297\n",
      "Train Epoch: 69 [90368/225000 (40%)] Loss: 7815.437500\n",
      "Train Epoch: 69 [94464/225000 (42%)] Loss: 7783.130859\n",
      "Train Epoch: 69 [98560/225000 (44%)] Loss: 7935.419922\n",
      "Train Epoch: 69 [102656/225000 (46%)] Loss: 7864.636719\n",
      "Train Epoch: 69 [106752/225000 (47%)] Loss: 7922.255859\n",
      "Train Epoch: 69 [110848/225000 (49%)] Loss: 7783.705078\n",
      "Train Epoch: 69 [114944/225000 (51%)] Loss: 7982.294922\n",
      "Train Epoch: 69 [119040/225000 (53%)] Loss: 7846.589844\n",
      "Train Epoch: 69 [123136/225000 (55%)] Loss: 8063.578125\n",
      "Train Epoch: 69 [127232/225000 (57%)] Loss: 7825.058594\n",
      "Train Epoch: 69 [131328/225000 (58%)] Loss: 7865.207031\n",
      "Train Epoch: 69 [135424/225000 (60%)] Loss: 7937.265625\n",
      "Train Epoch: 69 [139520/225000 (62%)] Loss: 7835.843750\n",
      "Train Epoch: 69 [143616/225000 (64%)] Loss: 7896.628906\n",
      "Train Epoch: 69 [147712/225000 (66%)] Loss: 7937.250000\n",
      "Train Epoch: 69 [151808/225000 (67%)] Loss: 7704.082031\n",
      "Train Epoch: 69 [155904/225000 (69%)] Loss: 7912.324219\n",
      "Train Epoch: 69 [160000/225000 (71%)] Loss: 8042.154297\n",
      "Train Epoch: 69 [164096/225000 (73%)] Loss: 7818.429688\n",
      "Train Epoch: 69 [168192/225000 (75%)] Loss: 7770.402344\n",
      "Train Epoch: 69 [172288/225000 (77%)] Loss: 8152.253906\n",
      "Train Epoch: 69 [176384/225000 (78%)] Loss: 7888.472656\n",
      "Train Epoch: 69 [180480/225000 (80%)] Loss: 7807.283203\n",
      "Train Epoch: 69 [184576/225000 (82%)] Loss: 7932.679688\n",
      "Train Epoch: 69 [188672/225000 (84%)] Loss: 8036.060547\n",
      "Train Epoch: 69 [192768/225000 (86%)] Loss: 8011.705078\n",
      "Train Epoch: 69 [196864/225000 (87%)] Loss: 7906.041016\n",
      "Train Epoch: 69 [200960/225000 (89%)] Loss: 8074.955078\n",
      "Train Epoch: 69 [205056/225000 (91%)] Loss: 8059.544922\n",
      "Train Epoch: 69 [209152/225000 (93%)] Loss: 8076.732422\n",
      "Train Epoch: 69 [213248/225000 (95%)] Loss: 7711.544922\n",
      "Train Epoch: 69 [217344/225000 (97%)] Loss: 7852.449219\n",
      "Train Epoch: 69 [221440/225000 (98%)] Loss: 8129.314453\n",
      "    epoch          : 69\n",
      "    loss           : 7929.30226109215\n",
      "    val_loss       : 7895.983596968407\n",
      "Train Epoch: 70 [256/225000 (0%)] Loss: 7812.691406\n",
      "Train Epoch: 70 [4352/225000 (2%)] Loss: 8009.169922\n",
      "Train Epoch: 70 [8448/225000 (4%)] Loss: 7945.281250\n",
      "Train Epoch: 70 [12544/225000 (6%)] Loss: 7767.757812\n",
      "Train Epoch: 70 [16640/225000 (7%)] Loss: 7928.562500\n",
      "Train Epoch: 70 [20736/225000 (9%)] Loss: 7985.669922\n",
      "Train Epoch: 70 [24832/225000 (11%)] Loss: 7841.628906\n",
      "Train Epoch: 70 [28928/225000 (13%)] Loss: 7904.326172\n",
      "Train Epoch: 70 [33024/225000 (15%)] Loss: 7823.037109\n",
      "Train Epoch: 70 [37120/225000 (16%)] Loss: 7739.886719\n",
      "Train Epoch: 70 [41216/225000 (18%)] Loss: 7974.654297\n",
      "Train Epoch: 70 [45312/225000 (20%)] Loss: 7920.464844\n",
      "Train Epoch: 70 [49408/225000 (22%)] Loss: 7798.087891\n",
      "Train Epoch: 70 [53504/225000 (24%)] Loss: 7857.703125\n",
      "Train Epoch: 70 [57600/225000 (26%)] Loss: 7972.394531\n",
      "Train Epoch: 70 [61696/225000 (27%)] Loss: 8033.353516\n",
      "Train Epoch: 70 [65792/225000 (29%)] Loss: 7931.416016\n",
      "Train Epoch: 70 [69888/225000 (31%)] Loss: 7950.058594\n",
      "Train Epoch: 70 [73984/225000 (33%)] Loss: 7919.353516\n",
      "Train Epoch: 70 [78080/225000 (35%)] Loss: 7845.943359\n",
      "Train Epoch: 70 [82176/225000 (37%)] Loss: 7787.890625\n",
      "Train Epoch: 70 [86272/225000 (38%)] Loss: 7757.976562\n",
      "Train Epoch: 70 [90368/225000 (40%)] Loss: 7974.101562\n",
      "Train Epoch: 70 [94464/225000 (42%)] Loss: 8126.595703\n",
      "Train Epoch: 70 [98560/225000 (44%)] Loss: 8031.980469\n",
      "Train Epoch: 70 [102656/225000 (46%)] Loss: 7876.130859\n",
      "Train Epoch: 70 [106752/225000 (47%)] Loss: 7908.173828\n",
      "Train Epoch: 70 [110848/225000 (49%)] Loss: 7765.500000\n",
      "Train Epoch: 70 [114944/225000 (51%)] Loss: 7840.330078\n",
      "Train Epoch: 70 [119040/225000 (53%)] Loss: 7977.046875\n",
      "Train Epoch: 70 [123136/225000 (55%)] Loss: 8034.013672\n",
      "Train Epoch: 70 [127232/225000 (57%)] Loss: 8060.238281\n",
      "Train Epoch: 70 [131328/225000 (58%)] Loss: 8017.054688\n",
      "Train Epoch: 70 [135424/225000 (60%)] Loss: 7795.044922\n",
      "Train Epoch: 70 [139520/225000 (62%)] Loss: 7957.886719\n",
      "Train Epoch: 70 [143616/225000 (64%)] Loss: 7770.851562\n",
      "Train Epoch: 70 [147712/225000 (66%)] Loss: 7844.884766\n",
      "Train Epoch: 70 [151808/225000 (67%)] Loss: 7916.437500\n",
      "Train Epoch: 70 [155904/225000 (69%)] Loss: 7878.361328\n",
      "Train Epoch: 70 [160000/225000 (71%)] Loss: 7905.222656\n",
      "Train Epoch: 70 [164096/225000 (73%)] Loss: 7759.375000\n",
      "Train Epoch: 70 [168192/225000 (75%)] Loss: 7908.966797\n",
      "Train Epoch: 70 [172288/225000 (77%)] Loss: 8021.638672\n",
      "Train Epoch: 70 [176384/225000 (78%)] Loss: 7714.355469\n",
      "Train Epoch: 70 [180480/225000 (80%)] Loss: 7890.945312\n",
      "Train Epoch: 70 [184576/225000 (82%)] Loss: 7883.791016\n",
      "Train Epoch: 70 [188672/225000 (84%)] Loss: 7740.201172\n",
      "Train Epoch: 70 [192768/225000 (86%)] Loss: 7914.437500\n",
      "Train Epoch: 70 [196864/225000 (87%)] Loss: 7994.250000\n",
      "Train Epoch: 70 [200960/225000 (89%)] Loss: 7926.097656\n",
      "Train Epoch: 70 [205056/225000 (91%)] Loss: 7958.714844\n",
      "Train Epoch: 70 [209152/225000 (93%)] Loss: 7896.031250\n",
      "Train Epoch: 70 [213248/225000 (95%)] Loss: 8027.027344\n",
      "Train Epoch: 70 [217344/225000 (97%)] Loss: 7840.644531\n",
      "Train Epoch: 70 [221440/225000 (98%)] Loss: 7934.410156\n",
      "    epoch          : 70\n",
      "    loss           : 7933.789910187358\n",
      "    val_loss       : 7903.50150352838\n",
      "Train Epoch: 71 [256/225000 (0%)] Loss: 7836.287109\n",
      "Train Epoch: 71 [4352/225000 (2%)] Loss: 8019.955078\n",
      "Train Epoch: 71 [8448/225000 (4%)] Loss: 7884.103516\n",
      "Train Epoch: 71 [12544/225000 (6%)] Loss: 7891.337891\n",
      "Train Epoch: 71 [16640/225000 (7%)] Loss: 7935.052734\n",
      "Train Epoch: 71 [20736/225000 (9%)] Loss: 8065.660156\n",
      "Train Epoch: 71 [24832/225000 (11%)] Loss: 7786.015625\n",
      "Train Epoch: 71 [28928/225000 (13%)] Loss: 7770.191406\n",
      "Train Epoch: 71 [33024/225000 (15%)] Loss: 7877.253906\n",
      "Train Epoch: 71 [37120/225000 (16%)] Loss: 7815.746094\n",
      "Train Epoch: 71 [41216/225000 (18%)] Loss: 7938.197266\n",
      "Train Epoch: 71 [45312/225000 (20%)] Loss: 8124.398438\n",
      "Train Epoch: 71 [49408/225000 (22%)] Loss: 7943.501953\n",
      "Train Epoch: 71 [53504/225000 (24%)] Loss: 7758.337891\n",
      "Train Epoch: 71 [57600/225000 (26%)] Loss: 7789.642578\n",
      "Train Epoch: 71 [61696/225000 (27%)] Loss: 7789.203125\n",
      "Train Epoch: 71 [65792/225000 (29%)] Loss: 8014.412109\n",
      "Train Epoch: 71 [69888/225000 (31%)] Loss: 7959.687500\n",
      "Train Epoch: 71 [73984/225000 (33%)] Loss: 7855.576172\n",
      "Train Epoch: 71 [78080/225000 (35%)] Loss: 7739.341797\n",
      "Train Epoch: 71 [82176/225000 (37%)] Loss: 8015.369141\n",
      "Train Epoch: 71 [86272/225000 (38%)] Loss: 8058.730469\n",
      "Train Epoch: 71 [90368/225000 (40%)] Loss: 7691.503906\n",
      "Train Epoch: 71 [94464/225000 (42%)] Loss: 8013.554688\n",
      "Train Epoch: 71 [98560/225000 (44%)] Loss: 7734.056641\n",
      "Train Epoch: 71 [102656/225000 (46%)] Loss: 7930.826172\n",
      "Train Epoch: 71 [106752/225000 (47%)] Loss: 7717.699219\n",
      "Train Epoch: 71 [110848/225000 (49%)] Loss: 7706.669922\n",
      "Train Epoch: 71 [114944/225000 (51%)] Loss: 7953.519531\n",
      "Train Epoch: 71 [119040/225000 (53%)] Loss: 7817.328125\n",
      "Train Epoch: 71 [123136/225000 (55%)] Loss: 7746.158203\n",
      "Train Epoch: 71 [127232/225000 (57%)] Loss: 8054.626953\n",
      "Train Epoch: 71 [131328/225000 (58%)] Loss: 8089.017578\n",
      "Train Epoch: 71 [135424/225000 (60%)] Loss: 7846.744141\n",
      "Train Epoch: 71 [139520/225000 (62%)] Loss: 7887.447266\n",
      "Train Epoch: 71 [143616/225000 (64%)] Loss: 7831.974609\n",
      "Train Epoch: 71 [147712/225000 (66%)] Loss: 7856.349609\n",
      "Train Epoch: 71 [151808/225000 (67%)] Loss: 7782.091797\n",
      "Train Epoch: 71 [155904/225000 (69%)] Loss: 7920.523438\n",
      "Train Epoch: 71 [160000/225000 (71%)] Loss: 7852.365234\n",
      "Train Epoch: 71 [164096/225000 (73%)] Loss: 7950.519531\n",
      "Train Epoch: 71 [168192/225000 (75%)] Loss: 8023.361328\n",
      "Train Epoch: 71 [172288/225000 (77%)] Loss: 7990.878906\n",
      "Train Epoch: 71 [176384/225000 (78%)] Loss: 8051.230469\n",
      "Train Epoch: 71 [180480/225000 (80%)] Loss: 7956.566406\n",
      "Train Epoch: 71 [184576/225000 (82%)] Loss: 8045.171875\n",
      "Train Epoch: 71 [188672/225000 (84%)] Loss: 7932.517578\n",
      "Train Epoch: 71 [192768/225000 (86%)] Loss: 8107.955078\n",
      "Train Epoch: 71 [196864/225000 (87%)] Loss: 7842.447266\n",
      "Train Epoch: 71 [200960/225000 (89%)] Loss: 7942.779297\n",
      "Train Epoch: 71 [205056/225000 (91%)] Loss: 7948.876953\n",
      "Train Epoch: 71 [209152/225000 (93%)] Loss: 7745.552734\n",
      "Train Epoch: 71 [213248/225000 (95%)] Loss: 7897.109375\n",
      "Train Epoch: 71 [217344/225000 (97%)] Loss: 7990.914062\n",
      "Train Epoch: 71 [221440/225000 (98%)] Loss: 7890.105469\n",
      "    epoch          : 71\n",
      "    loss           : 7920.2496411493885\n",
      "    val_loss       : 7869.906593084335\n",
      "Train Epoch: 72 [256/225000 (0%)] Loss: 8040.126953\n",
      "Train Epoch: 72 [4352/225000 (2%)] Loss: 8108.048828\n",
      "Train Epoch: 72 [8448/225000 (4%)] Loss: 7804.232422\n",
      "Train Epoch: 72 [12544/225000 (6%)] Loss: 7830.150391\n",
      "Train Epoch: 72 [16640/225000 (7%)] Loss: 7819.242188\n",
      "Train Epoch: 72 [20736/225000 (9%)] Loss: 7884.958984\n",
      "Train Epoch: 72 [24832/225000 (11%)] Loss: 7822.955078\n",
      "Train Epoch: 72 [28928/225000 (13%)] Loss: 8116.498047\n",
      "Train Epoch: 72 [33024/225000 (15%)] Loss: 7851.285156\n",
      "Train Epoch: 72 [37120/225000 (16%)] Loss: 7807.259766\n",
      "Train Epoch: 72 [41216/225000 (18%)] Loss: 7819.033203\n",
      "Train Epoch: 72 [45312/225000 (20%)] Loss: 7854.617188\n",
      "Train Epoch: 72 [49408/225000 (22%)] Loss: 7985.494141\n",
      "Train Epoch: 72 [53504/225000 (24%)] Loss: 8118.101562\n",
      "Train Epoch: 72 [57600/225000 (26%)] Loss: 7962.220703\n",
      "Train Epoch: 72 [61696/225000 (27%)] Loss: 7857.205078\n",
      "Train Epoch: 72 [65792/225000 (29%)] Loss: 7837.750000\n",
      "Train Epoch: 72 [69888/225000 (31%)] Loss: 7943.652344\n",
      "Train Epoch: 72 [73984/225000 (33%)] Loss: 7738.033203\n",
      "Train Epoch: 72 [78080/225000 (35%)] Loss: 8030.048828\n",
      "Train Epoch: 72 [82176/225000 (37%)] Loss: 7943.189453\n",
      "Train Epoch: 72 [86272/225000 (38%)] Loss: 8156.941406\n",
      "Train Epoch: 72 [90368/225000 (40%)] Loss: 7899.503906\n",
      "Train Epoch: 72 [94464/225000 (42%)] Loss: 7853.244141\n",
      "Train Epoch: 72 [98560/225000 (44%)] Loss: 7761.375000\n",
      "Train Epoch: 72 [102656/225000 (46%)] Loss: 7875.980469\n",
      "Train Epoch: 72 [106752/225000 (47%)] Loss: 7773.642578\n",
      "Train Epoch: 72 [110848/225000 (49%)] Loss: 7855.123047\n",
      "Train Epoch: 72 [114944/225000 (51%)] Loss: 8090.343750\n",
      "Train Epoch: 72 [119040/225000 (53%)] Loss: 7793.800781\n",
      "Train Epoch: 72 [123136/225000 (55%)] Loss: 7926.679688\n",
      "Train Epoch: 72 [127232/225000 (57%)] Loss: 7987.027344\n",
      "Train Epoch: 72 [131328/225000 (58%)] Loss: 7777.566406\n",
      "Train Epoch: 72 [135424/225000 (60%)] Loss: 7905.992188\n",
      "Train Epoch: 72 [139520/225000 (62%)] Loss: 7965.781250\n",
      "Train Epoch: 72 [143616/225000 (64%)] Loss: 7947.146484\n",
      "Train Epoch: 72 [147712/225000 (66%)] Loss: 7888.990234\n",
      "Train Epoch: 72 [151808/225000 (67%)] Loss: 7960.484375\n",
      "Train Epoch: 72 [155904/225000 (69%)] Loss: 7933.107422\n",
      "Train Epoch: 72 [160000/225000 (71%)] Loss: 7854.796875\n",
      "Train Epoch: 72 [164096/225000 (73%)] Loss: 7758.212891\n",
      "Train Epoch: 72 [168192/225000 (75%)] Loss: 7793.554688\n",
      "Train Epoch: 72 [172288/225000 (77%)] Loss: 7781.605469\n",
      "Train Epoch: 72 [176384/225000 (78%)] Loss: 7836.923828\n",
      "Train Epoch: 72 [180480/225000 (80%)] Loss: 7939.652344\n",
      "Train Epoch: 72 [184576/225000 (82%)] Loss: 7836.160156\n",
      "Train Epoch: 72 [188672/225000 (84%)] Loss: 7806.181641\n",
      "Train Epoch: 72 [192768/225000 (86%)] Loss: 7799.185547\n",
      "Train Epoch: 72 [196864/225000 (87%)] Loss: 7950.654297\n",
      "Train Epoch: 72 [200960/225000 (89%)] Loss: 7757.203125\n",
      "Train Epoch: 72 [205056/225000 (91%)] Loss: 7877.447266\n",
      "Train Epoch: 72 [209152/225000 (93%)] Loss: 7984.746094\n",
      "Train Epoch: 72 [213248/225000 (95%)] Loss: 8133.228516\n",
      "Train Epoch: 72 [217344/225000 (97%)] Loss: 7726.531250\n",
      "Train Epoch: 72 [221440/225000 (98%)] Loss: 8012.919922\n",
      "    epoch          : 72\n",
      "    loss           : 7884.69145735566\n",
      "    val_loss       : 8073.089048796771\n",
      "Train Epoch: 73 [256/225000 (0%)] Loss: 7763.966797\n",
      "Train Epoch: 73 [4352/225000 (2%)] Loss: 7759.822266\n",
      "Train Epoch: 73 [8448/225000 (4%)] Loss: 7761.613281\n",
      "Train Epoch: 73 [12544/225000 (6%)] Loss: 7855.289062\n",
      "Train Epoch: 73 [16640/225000 (7%)] Loss: 7768.576172\n",
      "Train Epoch: 73 [20736/225000 (9%)] Loss: 7947.929688\n",
      "Train Epoch: 73 [24832/225000 (11%)] Loss: 7897.171875\n",
      "Train Epoch: 73 [28928/225000 (13%)] Loss: 7958.978516\n",
      "Train Epoch: 73 [33024/225000 (15%)] Loss: 7557.349609\n",
      "Train Epoch: 73 [37120/225000 (16%)] Loss: 7926.996094\n",
      "Train Epoch: 73 [41216/225000 (18%)] Loss: 7987.064453\n",
      "Train Epoch: 73 [45312/225000 (20%)] Loss: 8039.128906\n",
      "Train Epoch: 73 [49408/225000 (22%)] Loss: 8014.488281\n",
      "Train Epoch: 73 [53504/225000 (24%)] Loss: 7969.087891\n",
      "Train Epoch: 73 [57600/225000 (26%)] Loss: 7843.433594\n",
      "Train Epoch: 73 [61696/225000 (27%)] Loss: 7724.128906\n",
      "Train Epoch: 73 [65792/225000 (29%)] Loss: 7944.550781\n",
      "Train Epoch: 73 [69888/225000 (31%)] Loss: 7769.113281\n",
      "Train Epoch: 73 [73984/225000 (33%)] Loss: 7829.724609\n",
      "Train Epoch: 73 [78080/225000 (35%)] Loss: 7885.664062\n",
      "Train Epoch: 73 [82176/225000 (37%)] Loss: 7779.539062\n",
      "Train Epoch: 73 [86272/225000 (38%)] Loss: 7886.240234\n",
      "Train Epoch: 73 [90368/225000 (40%)] Loss: 7728.666016\n",
      "Train Epoch: 73 [94464/225000 (42%)] Loss: 7972.261719\n",
      "Train Epoch: 73 [98560/225000 (44%)] Loss: 7993.037109\n",
      "Train Epoch: 73 [102656/225000 (46%)] Loss: 7702.904297\n",
      "Train Epoch: 73 [106752/225000 (47%)] Loss: 7716.775391\n",
      "Train Epoch: 73 [110848/225000 (49%)] Loss: 8050.361328\n",
      "Train Epoch: 73 [114944/225000 (51%)] Loss: 8059.021484\n",
      "Train Epoch: 73 [119040/225000 (53%)] Loss: 7867.794922\n",
      "Train Epoch: 73 [123136/225000 (55%)] Loss: 7980.212891\n",
      "Train Epoch: 73 [127232/225000 (57%)] Loss: 7688.964844\n",
      "Train Epoch: 73 [131328/225000 (58%)] Loss: 7897.021484\n",
      "Train Epoch: 73 [135424/225000 (60%)] Loss: 8023.576172\n",
      "Train Epoch: 73 [139520/225000 (62%)] Loss: 7841.328125\n",
      "Train Epoch: 73 [143616/225000 (64%)] Loss: 7838.646484\n",
      "Train Epoch: 73 [147712/225000 (66%)] Loss: 8047.175781\n",
      "Train Epoch: 73 [151808/225000 (67%)] Loss: 7900.042969\n",
      "Train Epoch: 73 [155904/225000 (69%)] Loss: 7852.984375\n",
      "Train Epoch: 73 [160000/225000 (71%)] Loss: 7955.615234\n",
      "Train Epoch: 73 [164096/225000 (73%)] Loss: 7970.113281\n",
      "Train Epoch: 73 [168192/225000 (75%)] Loss: 7786.095703\n",
      "Train Epoch: 73 [172288/225000 (77%)] Loss: 7797.681641\n",
      "Train Epoch: 73 [176384/225000 (78%)] Loss: 7933.847656\n",
      "Train Epoch: 73 [180480/225000 (80%)] Loss: 7941.955078\n",
      "Train Epoch: 73 [184576/225000 (82%)] Loss: 7884.494141\n",
      "Train Epoch: 73 [188672/225000 (84%)] Loss: 7813.345703\n",
      "Train Epoch: 73 [192768/225000 (86%)] Loss: 7826.189453\n",
      "Train Epoch: 73 [196864/225000 (87%)] Loss: 7838.001953\n",
      "Train Epoch: 73 [200960/225000 (89%)] Loss: 7917.933594\n",
      "Train Epoch: 73 [205056/225000 (91%)] Loss: 7717.156250\n",
      "Train Epoch: 73 [209152/225000 (93%)] Loss: 7925.695312\n",
      "Train Epoch: 73 [213248/225000 (95%)] Loss: 7880.570312\n",
      "Train Epoch: 73 [217344/225000 (97%)] Loss: 7953.703125\n",
      "Train Epoch: 73 [221440/225000 (98%)] Loss: 7983.755859\n",
      "    epoch          : 73\n",
      "    loss           : 7891.328477184656\n",
      "    val_loss       : 7848.987849252565\n",
      "Train Epoch: 74 [256/225000 (0%)] Loss: 7820.601562\n",
      "Train Epoch: 74 [4352/225000 (2%)] Loss: 7848.269531\n",
      "Train Epoch: 74 [8448/225000 (4%)] Loss: 7941.232422\n",
      "Train Epoch: 74 [12544/225000 (6%)] Loss: 7608.683594\n",
      "Train Epoch: 74 [16640/225000 (7%)] Loss: 7874.101562\n",
      "Train Epoch: 74 [20736/225000 (9%)] Loss: 7705.994141\n",
      "Train Epoch: 74 [24832/225000 (11%)] Loss: 7781.486328\n",
      "Train Epoch: 74 [28928/225000 (13%)] Loss: 7806.419922\n",
      "Train Epoch: 74 [33024/225000 (15%)] Loss: 7712.398438\n",
      "Train Epoch: 74 [37120/225000 (16%)] Loss: 7856.015625\n",
      "Train Epoch: 74 [41216/225000 (18%)] Loss: 7900.210938\n",
      "Train Epoch: 74 [45312/225000 (20%)] Loss: 7875.396484\n",
      "Train Epoch: 74 [49408/225000 (22%)] Loss: 8029.126953\n",
      "Train Epoch: 74 [53504/225000 (24%)] Loss: 8149.058594\n",
      "Train Epoch: 74 [57600/225000 (26%)] Loss: 7686.376953\n",
      "Train Epoch: 74 [61696/225000 (27%)] Loss: 7816.968750\n",
      "Train Epoch: 74 [65792/225000 (29%)] Loss: 7877.160156\n",
      "Train Epoch: 74 [69888/225000 (31%)] Loss: 7919.025391\n",
      "Train Epoch: 74 [73984/225000 (33%)] Loss: 7798.669922\n",
      "Train Epoch: 74 [78080/225000 (35%)] Loss: 7832.134766\n",
      "Train Epoch: 74 [82176/225000 (37%)] Loss: 7793.982422\n",
      "Train Epoch: 74 [86272/225000 (38%)] Loss: 7871.210938\n",
      "Train Epoch: 74 [90368/225000 (40%)] Loss: 7834.941406\n",
      "Train Epoch: 74 [94464/225000 (42%)] Loss: 7884.009766\n",
      "Train Epoch: 74 [98560/225000 (44%)] Loss: 8026.759766\n",
      "Train Epoch: 74 [102656/225000 (46%)] Loss: 7927.742188\n",
      "Train Epoch: 74 [106752/225000 (47%)] Loss: 7827.480469\n",
      "Train Epoch: 74 [110848/225000 (49%)] Loss: 7976.304688\n",
      "Train Epoch: 74 [114944/225000 (51%)] Loss: 7726.576172\n",
      "Train Epoch: 74 [119040/225000 (53%)] Loss: 7789.314453\n",
      "Train Epoch: 74 [123136/225000 (55%)] Loss: 8000.525391\n",
      "Train Epoch: 74 [127232/225000 (57%)] Loss: 7884.126953\n",
      "Train Epoch: 74 [131328/225000 (58%)] Loss: 7753.505859\n",
      "Train Epoch: 74 [135424/225000 (60%)] Loss: 7994.630859\n",
      "Train Epoch: 74 [139520/225000 (62%)] Loss: 7987.904297\n",
      "Train Epoch: 74 [143616/225000 (64%)] Loss: 7661.160156\n",
      "Train Epoch: 74 [147712/225000 (66%)] Loss: 7863.492188\n",
      "Train Epoch: 74 [151808/225000 (67%)] Loss: 7758.468750\n",
      "Train Epoch: 74 [155904/225000 (69%)] Loss: 7712.070312\n",
      "Train Epoch: 74 [160000/225000 (71%)] Loss: 8002.919922\n",
      "Train Epoch: 74 [164096/225000 (73%)] Loss: 7772.117188\n",
      "Train Epoch: 74 [168192/225000 (75%)] Loss: 8054.658203\n",
      "Train Epoch: 74 [172288/225000 (77%)] Loss: 7955.603516\n",
      "Train Epoch: 74 [176384/225000 (78%)] Loss: 7951.447266\n",
      "Train Epoch: 74 [180480/225000 (80%)] Loss: 7835.248047\n",
      "Train Epoch: 74 [184576/225000 (82%)] Loss: 7941.210938\n",
      "Train Epoch: 74 [188672/225000 (84%)] Loss: 7892.107422\n",
      "Train Epoch: 74 [192768/225000 (86%)] Loss: 7855.097656\n",
      "Train Epoch: 74 [196864/225000 (87%)] Loss: 7991.183594\n",
      "Train Epoch: 74 [200960/225000 (89%)] Loss: 7975.617188\n",
      "Train Epoch: 74 [205056/225000 (91%)] Loss: 7738.103516\n",
      "Train Epoch: 74 [209152/225000 (93%)] Loss: 7729.017578\n",
      "Train Epoch: 74 [213248/225000 (95%)] Loss: 7887.769531\n",
      "Train Epoch: 74 [217344/225000 (97%)] Loss: 7610.964844\n",
      "Train Epoch: 74 [221440/225000 (98%)] Loss: 7683.685547\n",
      "    epoch          : 74\n",
      "    loss           : 7863.121619249502\n",
      "    val_loss       : 7841.509603505232\n",
      "Train Epoch: 75 [256/225000 (0%)] Loss: 7761.617188\n",
      "Train Epoch: 75 [4352/225000 (2%)] Loss: 7964.736328\n",
      "Train Epoch: 75 [8448/225000 (4%)] Loss: 7834.197266\n",
      "Train Epoch: 75 [12544/225000 (6%)] Loss: 7921.201172\n",
      "Train Epoch: 75 [16640/225000 (7%)] Loss: 7969.222656\n",
      "Train Epoch: 75 [20736/225000 (9%)] Loss: 7709.388672\n",
      "Train Epoch: 75 [24832/225000 (11%)] Loss: 7748.423828\n",
      "Train Epoch: 75 [28928/225000 (13%)] Loss: 7759.884766\n",
      "Train Epoch: 75 [33024/225000 (15%)] Loss: 7693.912109\n",
      "Train Epoch: 75 [37120/225000 (16%)] Loss: 7715.050781\n",
      "Train Epoch: 75 [41216/225000 (18%)] Loss: 7805.970703\n",
      "Train Epoch: 75 [45312/225000 (20%)] Loss: 7921.894531\n",
      "Train Epoch: 75 [49408/225000 (22%)] Loss: 7923.341797\n",
      "Train Epoch: 75 [53504/225000 (24%)] Loss: 7881.046875\n",
      "Train Epoch: 75 [57600/225000 (26%)] Loss: 8179.582031\n",
      "Train Epoch: 75 [61696/225000 (27%)] Loss: 7927.250000\n",
      "Train Epoch: 75 [65792/225000 (29%)] Loss: 7987.210938\n",
      "Train Epoch: 75 [69888/225000 (31%)] Loss: 7680.421875\n",
      "Train Epoch: 75 [73984/225000 (33%)] Loss: 7710.306641\n",
      "Train Epoch: 75 [78080/225000 (35%)] Loss: 7814.560547\n",
      "Train Epoch: 75 [82176/225000 (37%)] Loss: 7615.171875\n",
      "Train Epoch: 75 [86272/225000 (38%)] Loss: 7820.837891\n",
      "Train Epoch: 75 [90368/225000 (40%)] Loss: 7939.728516\n",
      "Train Epoch: 75 [94464/225000 (42%)] Loss: 7633.705078\n",
      "Train Epoch: 75 [98560/225000 (44%)] Loss: 7957.539062\n",
      "Train Epoch: 75 [102656/225000 (46%)] Loss: 7751.726562\n",
      "Train Epoch: 75 [106752/225000 (47%)] Loss: 7950.423828\n",
      "Train Epoch: 75 [110848/225000 (49%)] Loss: 7847.544922\n",
      "Train Epoch: 75 [114944/225000 (51%)] Loss: 7795.433594\n",
      "Train Epoch: 75 [119040/225000 (53%)] Loss: 7802.814453\n",
      "Train Epoch: 75 [123136/225000 (55%)] Loss: 7664.644531\n",
      "Train Epoch: 75 [127232/225000 (57%)] Loss: 7518.140625\n",
      "Train Epoch: 75 [131328/225000 (58%)] Loss: 7733.828125\n",
      "Train Epoch: 75 [135424/225000 (60%)] Loss: 7871.333984\n",
      "Train Epoch: 75 [139520/225000 (62%)] Loss: 7851.884766\n",
      "Train Epoch: 75 [143616/225000 (64%)] Loss: 7941.121094\n",
      "Train Epoch: 75 [147712/225000 (66%)] Loss: 7825.906250\n",
      "Train Epoch: 75 [151808/225000 (67%)] Loss: 7815.623047\n",
      "Train Epoch: 75 [155904/225000 (69%)] Loss: 7897.261719\n",
      "Train Epoch: 75 [160000/225000 (71%)] Loss: 7807.187500\n",
      "Train Epoch: 75 [164096/225000 (73%)] Loss: 7764.560547\n",
      "Train Epoch: 75 [168192/225000 (75%)] Loss: 7901.796875\n",
      "Train Epoch: 75 [172288/225000 (77%)] Loss: 7869.253906\n",
      "Train Epoch: 75 [176384/225000 (78%)] Loss: 7819.210938\n",
      "Train Epoch: 75 [180480/225000 (80%)] Loss: 7921.453125\n",
      "Train Epoch: 75 [184576/225000 (82%)] Loss: 7750.998047\n",
      "Train Epoch: 75 [188672/225000 (84%)] Loss: 7877.861328\n",
      "Train Epoch: 75 [192768/225000 (86%)] Loss: 7816.058594\n",
      "Train Epoch: 75 [196864/225000 (87%)] Loss: 7826.560547\n",
      "Train Epoch: 75 [200960/225000 (89%)] Loss: 7792.859375\n",
      "Train Epoch: 75 [205056/225000 (91%)] Loss: 7907.619141\n",
      "Train Epoch: 75 [209152/225000 (93%)] Loss: 7858.595703\n",
      "Train Epoch: 75 [213248/225000 (95%)] Loss: 8000.656250\n",
      "Train Epoch: 75 [217344/225000 (97%)] Loss: 7817.164062\n",
      "Train Epoch: 75 [221440/225000 (98%)] Loss: 7839.503906\n",
      "    epoch          : 75\n",
      "    loss           : 7850.7633130243885\n",
      "    val_loss       : 7832.701155419252\n",
      "Train Epoch: 76 [256/225000 (0%)] Loss: 8030.310547\n",
      "Train Epoch: 76 [4352/225000 (2%)] Loss: 7886.035156\n",
      "Train Epoch: 76 [8448/225000 (4%)] Loss: 7832.253906\n",
      "Train Epoch: 76 [12544/225000 (6%)] Loss: 7952.042969\n",
      "Train Epoch: 76 [16640/225000 (7%)] Loss: 7781.853516\n",
      "Train Epoch: 76 [20736/225000 (9%)] Loss: 7851.007812\n",
      "Train Epoch: 76 [24832/225000 (11%)] Loss: 7979.492188\n",
      "Train Epoch: 76 [28928/225000 (13%)] Loss: 7803.197266\n",
      "Train Epoch: 76 [33024/225000 (15%)] Loss: 7928.546875\n",
      "Train Epoch: 76 [37120/225000 (16%)] Loss: 7811.867188\n",
      "Train Epoch: 76 [41216/225000 (18%)] Loss: 7928.970703\n",
      "Train Epoch: 76 [45312/225000 (20%)] Loss: 7890.169922\n",
      "Train Epoch: 76 [49408/225000 (22%)] Loss: 7871.312500\n",
      "Train Epoch: 76 [53504/225000 (24%)] Loss: 7673.318359\n",
      "Train Epoch: 76 [57600/225000 (26%)] Loss: 7991.402344\n",
      "Train Epoch: 76 [61696/225000 (27%)] Loss: 7708.445312\n",
      "Train Epoch: 76 [65792/225000 (29%)] Loss: 7832.402344\n",
      "Train Epoch: 76 [69888/225000 (31%)] Loss: 7914.384766\n",
      "Train Epoch: 76 [73984/225000 (33%)] Loss: 7855.287109\n",
      "Train Epoch: 76 [78080/225000 (35%)] Loss: 7874.322266\n",
      "Train Epoch: 76 [82176/225000 (37%)] Loss: 7713.279297\n",
      "Train Epoch: 76 [86272/225000 (38%)] Loss: 7699.369141\n",
      "Train Epoch: 76 [90368/225000 (40%)] Loss: 7693.890625\n",
      "Train Epoch: 76 [94464/225000 (42%)] Loss: 7835.228516\n",
      "Train Epoch: 76 [98560/225000 (44%)] Loss: 7934.822266\n",
      "Train Epoch: 76 [102656/225000 (46%)] Loss: 7926.208984\n",
      "Train Epoch: 76 [106752/225000 (47%)] Loss: 7673.562500\n",
      "Train Epoch: 76 [110848/225000 (49%)] Loss: 7773.699219\n",
      "Train Epoch: 76 [114944/225000 (51%)] Loss: 7896.287109\n",
      "Train Epoch: 76 [119040/225000 (53%)] Loss: 7906.515625\n",
      "Train Epoch: 76 [123136/225000 (55%)] Loss: 7862.687500\n",
      "Train Epoch: 76 [127232/225000 (57%)] Loss: 7818.421875\n",
      "Train Epoch: 76 [131328/225000 (58%)] Loss: 7872.441406\n",
      "Train Epoch: 76 [135424/225000 (60%)] Loss: 7667.273438\n",
      "Train Epoch: 76 [139520/225000 (62%)] Loss: 7902.027344\n",
      "Train Epoch: 76 [143616/225000 (64%)] Loss: 7919.892578\n",
      "Train Epoch: 76 [147712/225000 (66%)] Loss: 7836.113281\n",
      "Train Epoch: 76 [151808/225000 (67%)] Loss: 7823.558594\n",
      "Train Epoch: 76 [155904/225000 (69%)] Loss: 7713.482422\n",
      "Train Epoch: 76 [160000/225000 (71%)] Loss: 7976.619141\n",
      "Train Epoch: 76 [164096/225000 (73%)] Loss: 7874.388672\n",
      "Train Epoch: 76 [168192/225000 (75%)] Loss: 7982.912109\n",
      "Train Epoch: 76 [172288/225000 (77%)] Loss: 8024.187500\n",
      "Train Epoch: 76 [176384/225000 (78%)] Loss: 7887.976562\n",
      "Train Epoch: 76 [180480/225000 (80%)] Loss: 7888.671875\n",
      "Train Epoch: 76 [184576/225000 (82%)] Loss: 7885.884766\n",
      "Train Epoch: 76 [188672/225000 (84%)] Loss: 7690.072266\n",
      "Train Epoch: 76 [192768/225000 (86%)] Loss: 7869.486328\n",
      "Train Epoch: 76 [196864/225000 (87%)] Loss: 7537.970703\n",
      "Train Epoch: 76 [200960/225000 (89%)] Loss: 7872.718750\n",
      "Train Epoch: 76 [205056/225000 (91%)] Loss: 7829.242188\n",
      "Train Epoch: 76 [209152/225000 (93%)] Loss: 7800.763672\n",
      "Train Epoch: 76 [213248/225000 (95%)] Loss: 7771.484375\n",
      "Train Epoch: 76 [217344/225000 (97%)] Loss: 7890.988281\n",
      "Train Epoch: 76 [221440/225000 (98%)] Loss: 7630.804688\n",
      "    epoch          : 76\n",
      "    loss           : 7842.131415982295\n",
      "    val_loss       : 7814.720838224402\n",
      "Train Epoch: 77 [256/225000 (0%)] Loss: 7800.386719\n",
      "Train Epoch: 77 [4352/225000 (2%)] Loss: 7823.546875\n",
      "Train Epoch: 77 [8448/225000 (4%)] Loss: 7704.353516\n",
      "Train Epoch: 77 [12544/225000 (6%)] Loss: 7752.095703\n",
      "Train Epoch: 77 [16640/225000 (7%)] Loss: 7723.919922\n",
      "Train Epoch: 77 [20736/225000 (9%)] Loss: 7977.343750\n",
      "Train Epoch: 77 [24832/225000 (11%)] Loss: 7916.744141\n",
      "Train Epoch: 77 [28928/225000 (13%)] Loss: 7765.412109\n",
      "Train Epoch: 77 [33024/225000 (15%)] Loss: 7918.617188\n",
      "Train Epoch: 77 [37120/225000 (16%)] Loss: 7733.068359\n",
      "Train Epoch: 77 [41216/225000 (18%)] Loss: 7743.871094\n",
      "Train Epoch: 77 [45312/225000 (20%)] Loss: 7905.291016\n",
      "Train Epoch: 77 [49408/225000 (22%)] Loss: 7743.361328\n",
      "Train Epoch: 77 [53504/225000 (24%)] Loss: 7790.142578\n",
      "Train Epoch: 77 [57600/225000 (26%)] Loss: 7801.980469\n",
      "Train Epoch: 77 [61696/225000 (27%)] Loss: 7848.000000\n",
      "Train Epoch: 77 [65792/225000 (29%)] Loss: 7840.412109\n",
      "Train Epoch: 77 [69888/225000 (31%)] Loss: 7797.314453\n",
      "Train Epoch: 77 [73984/225000 (33%)] Loss: 7704.607422\n",
      "Train Epoch: 77 [78080/225000 (35%)] Loss: 7872.650391\n",
      "Train Epoch: 77 [82176/225000 (37%)] Loss: 7779.447266\n",
      "Train Epoch: 77 [86272/225000 (38%)] Loss: 7982.701172\n",
      "Train Epoch: 77 [90368/225000 (40%)] Loss: 7930.005859\n",
      "Train Epoch: 77 [94464/225000 (42%)] Loss: 7999.666016\n",
      "Train Epoch: 77 [98560/225000 (44%)] Loss: 7866.019531\n",
      "Train Epoch: 77 [102656/225000 (46%)] Loss: 7824.062500\n",
      "Train Epoch: 77 [106752/225000 (47%)] Loss: 8041.142578\n",
      "Train Epoch: 77 [110848/225000 (49%)] Loss: 7972.666016\n",
      "Train Epoch: 77 [114944/225000 (51%)] Loss: 7750.121094\n",
      "Train Epoch: 77 [119040/225000 (53%)] Loss: 7732.910156\n",
      "Train Epoch: 77 [123136/225000 (55%)] Loss: 7665.062500\n",
      "Train Epoch: 77 [127232/225000 (57%)] Loss: 8073.142578\n",
      "Train Epoch: 77 [131328/225000 (58%)] Loss: 7527.845703\n",
      "Train Epoch: 77 [135424/225000 (60%)] Loss: 7791.076172\n",
      "Train Epoch: 77 [139520/225000 (62%)] Loss: 7673.746094\n",
      "Train Epoch: 77 [143616/225000 (64%)] Loss: 7738.955078\n",
      "Train Epoch: 77 [147712/225000 (66%)] Loss: 7866.337891\n",
      "Train Epoch: 77 [151808/225000 (67%)] Loss: 7705.591797\n",
      "Train Epoch: 77 [155904/225000 (69%)] Loss: 7771.103516\n",
      "Train Epoch: 77 [160000/225000 (71%)] Loss: 7762.658203\n",
      "Train Epoch: 77 [164096/225000 (73%)] Loss: 7889.476562\n",
      "Train Epoch: 77 [168192/225000 (75%)] Loss: 7801.617188\n",
      "Train Epoch: 77 [172288/225000 (77%)] Loss: 7879.734375\n",
      "Train Epoch: 77 [176384/225000 (78%)] Loss: 7820.363281\n",
      "Train Epoch: 77 [180480/225000 (80%)] Loss: 7731.990234\n",
      "Train Epoch: 77 [184576/225000 (82%)] Loss: 7931.406250\n",
      "Train Epoch: 77 [188672/225000 (84%)] Loss: 7774.419922\n",
      "Train Epoch: 77 [192768/225000 (86%)] Loss: 7901.771484\n",
      "Train Epoch: 77 [196864/225000 (87%)] Loss: 7663.863281\n",
      "Train Epoch: 77 [200960/225000 (89%)] Loss: 7734.083984\n",
      "Train Epoch: 77 [205056/225000 (91%)] Loss: 7672.867188\n",
      "Train Epoch: 77 [209152/225000 (93%)] Loss: 7773.607422\n",
      "Train Epoch: 77 [213248/225000 (95%)] Loss: 8068.015625\n",
      "Train Epoch: 77 [217344/225000 (97%)] Loss: 7664.824219\n",
      "Train Epoch: 77 [221440/225000 (98%)] Loss: 7812.853516\n",
      "    epoch          : 77\n",
      "    loss           : 7830.507834719852\n",
      "    val_loss       : 7806.136778534675\n",
      "Train Epoch: 78 [256/225000 (0%)] Loss: 7780.162109\n",
      "Train Epoch: 78 [4352/225000 (2%)] Loss: 7733.367188\n",
      "Train Epoch: 78 [8448/225000 (4%)] Loss: 7878.660156\n",
      "Train Epoch: 78 [12544/225000 (6%)] Loss: 7695.222656\n",
      "Train Epoch: 78 [16640/225000 (7%)] Loss: 7748.091797\n",
      "Train Epoch: 78 [20736/225000 (9%)] Loss: 7781.007812\n",
      "Train Epoch: 78 [24832/225000 (11%)] Loss: 7698.078125\n",
      "Train Epoch: 78 [28928/225000 (13%)] Loss: 7797.255859\n",
      "Train Epoch: 78 [33024/225000 (15%)] Loss: 7637.349609\n",
      "Train Epoch: 78 [37120/225000 (16%)] Loss: 7791.146484\n",
      "Train Epoch: 78 [41216/225000 (18%)] Loss: 7945.080078\n",
      "Train Epoch: 78 [45312/225000 (20%)] Loss: 7922.908203\n",
      "Train Epoch: 78 [49408/225000 (22%)] Loss: 7850.017578\n",
      "Train Epoch: 78 [53504/225000 (24%)] Loss: 7798.367188\n",
      "Train Epoch: 78 [57600/225000 (26%)] Loss: 7753.960938\n",
      "Train Epoch: 78 [61696/225000 (27%)] Loss: 7716.238281\n",
      "Train Epoch: 78 [65792/225000 (29%)] Loss: 7824.285156\n",
      "Train Epoch: 78 [69888/225000 (31%)] Loss: 8036.160156\n",
      "Train Epoch: 78 [73984/225000 (33%)] Loss: 7951.583984\n",
      "Train Epoch: 78 [78080/225000 (35%)] Loss: 7749.052734\n",
      "Train Epoch: 78 [82176/225000 (37%)] Loss: 7746.943359\n",
      "Train Epoch: 78 [86272/225000 (38%)] Loss: 7662.138672\n",
      "Train Epoch: 78 [90368/225000 (40%)] Loss: 8030.560547\n",
      "Train Epoch: 78 [94464/225000 (42%)] Loss: 7910.833984\n",
      "Train Epoch: 78 [98560/225000 (44%)] Loss: 7551.015625\n",
      "Train Epoch: 78 [102656/225000 (46%)] Loss: 7754.400391\n",
      "Train Epoch: 78 [106752/225000 (47%)] Loss: 7866.179688\n",
      "Train Epoch: 78 [110848/225000 (49%)] Loss: 7777.556641\n",
      "Train Epoch: 78 [114944/225000 (51%)] Loss: 7797.097656\n",
      "Train Epoch: 78 [119040/225000 (53%)] Loss: 7781.097656\n",
      "Train Epoch: 78 [123136/225000 (55%)] Loss: 7900.242188\n",
      "Train Epoch: 78 [127232/225000 (57%)] Loss: 7733.755859\n",
      "Train Epoch: 78 [131328/225000 (58%)] Loss: 7598.673828\n",
      "Train Epoch: 78 [135424/225000 (60%)] Loss: 7784.443359\n",
      "Train Epoch: 78 [139520/225000 (62%)] Loss: 7802.103516\n",
      "Train Epoch: 78 [143616/225000 (64%)] Loss: 7722.835938\n",
      "Train Epoch: 78 [147712/225000 (66%)] Loss: 7840.544922\n",
      "Train Epoch: 78 [151808/225000 (67%)] Loss: 7895.308594\n",
      "Train Epoch: 78 [155904/225000 (69%)] Loss: 7976.482422\n",
      "Train Epoch: 78 [160000/225000 (71%)] Loss: 7714.281250\n",
      "Train Epoch: 78 [164096/225000 (73%)] Loss: 7751.042969\n",
      "Train Epoch: 78 [168192/225000 (75%)] Loss: 7791.296875\n",
      "Train Epoch: 78 [172288/225000 (77%)] Loss: 7624.277344\n",
      "Train Epoch: 78 [176384/225000 (78%)] Loss: 7823.060547\n",
      "Train Epoch: 78 [180480/225000 (80%)] Loss: 7847.873047\n",
      "Train Epoch: 78 [184576/225000 (82%)] Loss: 7819.023438\n",
      "Train Epoch: 78 [188672/225000 (84%)] Loss: 7778.500000\n",
      "Train Epoch: 78 [192768/225000 (86%)] Loss: 7681.093750\n",
      "Train Epoch: 78 [196864/225000 (87%)] Loss: 7823.580078\n",
      "Train Epoch: 78 [200960/225000 (89%)] Loss: 7828.152344\n",
      "Train Epoch: 78 [205056/225000 (91%)] Loss: 7772.722656\n",
      "Train Epoch: 78 [209152/225000 (93%)] Loss: 7809.394531\n",
      "Train Epoch: 78 [213248/225000 (95%)] Loss: 7722.197266\n",
      "Train Epoch: 78 [217344/225000 (97%)] Loss: 7892.804688\n",
      "Train Epoch: 78 [221440/225000 (98%)] Loss: 7796.740234\n",
      "    epoch          : 78\n",
      "    loss           : 7826.806228446743\n",
      "    val_loss       : 7796.244174224989\n",
      "Train Epoch: 79 [256/225000 (0%)] Loss: 7802.470703\n",
      "Train Epoch: 79 [4352/225000 (2%)] Loss: 7712.361328\n",
      "Train Epoch: 79 [8448/225000 (4%)] Loss: 7783.468750\n",
      "Train Epoch: 79 [12544/225000 (6%)] Loss: 7730.324219\n",
      "Train Epoch: 79 [16640/225000 (7%)] Loss: 7693.291016\n",
      "Train Epoch: 79 [20736/225000 (9%)] Loss: 7855.025391\n",
      "Train Epoch: 79 [24832/225000 (11%)] Loss: 7860.753906\n",
      "Train Epoch: 79 [28928/225000 (13%)] Loss: 7779.044922\n",
      "Train Epoch: 79 [33024/225000 (15%)] Loss: 7847.597656\n",
      "Train Epoch: 79 [37120/225000 (16%)] Loss: 7794.097656\n",
      "Train Epoch: 79 [41216/225000 (18%)] Loss: 7892.826172\n",
      "Train Epoch: 79 [45312/225000 (20%)] Loss: 7978.046875\n",
      "Train Epoch: 79 [49408/225000 (22%)] Loss: 8002.484375\n",
      "Train Epoch: 79 [53504/225000 (24%)] Loss: 7998.191406\n",
      "Train Epoch: 79 [57600/225000 (26%)] Loss: 7854.423828\n",
      "Train Epoch: 79 [61696/225000 (27%)] Loss: 7731.406250\n",
      "Train Epoch: 79 [65792/225000 (29%)] Loss: 8107.197266\n",
      "Train Epoch: 79 [69888/225000 (31%)] Loss: 7913.494141\n",
      "Train Epoch: 79 [73984/225000 (33%)] Loss: 7906.400391\n",
      "Train Epoch: 79 [78080/225000 (35%)] Loss: 7862.933594\n",
      "Train Epoch: 79 [82176/225000 (37%)] Loss: 7853.585938\n",
      "Train Epoch: 79 [86272/225000 (38%)] Loss: 7919.679688\n",
      "Train Epoch: 79 [90368/225000 (40%)] Loss: 7937.730469\n",
      "Train Epoch: 79 [94464/225000 (42%)] Loss: 7893.984375\n",
      "Train Epoch: 79 [98560/225000 (44%)] Loss: 7869.945312\n",
      "Train Epoch: 79 [102656/225000 (46%)] Loss: 7836.662109\n",
      "Train Epoch: 79 [106752/225000 (47%)] Loss: 7855.232422\n",
      "Train Epoch: 79 [110848/225000 (49%)] Loss: 7880.791016\n",
      "Train Epoch: 79 [114944/225000 (51%)] Loss: 7726.220703\n",
      "Train Epoch: 79 [119040/225000 (53%)] Loss: 7837.251953\n",
      "Train Epoch: 79 [123136/225000 (55%)] Loss: 7760.376953\n",
      "Train Epoch: 79 [127232/225000 (57%)] Loss: 7678.513672\n",
      "Train Epoch: 79 [131328/225000 (58%)] Loss: 7818.482422\n",
      "Train Epoch: 79 [135424/225000 (60%)] Loss: 7660.583984\n",
      "Train Epoch: 79 [139520/225000 (62%)] Loss: 7889.105469\n",
      "Train Epoch: 79 [143616/225000 (64%)] Loss: 7690.253906\n",
      "Train Epoch: 79 [147712/225000 (66%)] Loss: 7931.275391\n",
      "Train Epoch: 79 [151808/225000 (67%)] Loss: 7715.263672\n",
      "Train Epoch: 79 [155904/225000 (69%)] Loss: 7738.087891\n",
      "Train Epoch: 79 [160000/225000 (71%)] Loss: 7795.777344\n",
      "Train Epoch: 79 [164096/225000 (73%)] Loss: 7956.240234\n",
      "Train Epoch: 79 [168192/225000 (75%)] Loss: 7814.306641\n",
      "Train Epoch: 79 [172288/225000 (77%)] Loss: 7837.179688\n",
      "Train Epoch: 79 [176384/225000 (78%)] Loss: 7767.123047\n",
      "Train Epoch: 79 [180480/225000 (80%)] Loss: 7743.531250\n",
      "Train Epoch: 79 [184576/225000 (82%)] Loss: 7859.076172\n",
      "Train Epoch: 79 [188672/225000 (84%)] Loss: 7747.478516\n",
      "Train Epoch: 79 [192768/225000 (86%)] Loss: 7686.304688\n",
      "Train Epoch: 79 [196864/225000 (87%)] Loss: 7592.003906\n",
      "Train Epoch: 79 [200960/225000 (89%)] Loss: 7882.726562\n",
      "Train Epoch: 79 [205056/225000 (91%)] Loss: 7837.808594\n",
      "Train Epoch: 79 [209152/225000 (93%)] Loss: 7851.638672\n",
      "Train Epoch: 79 [213248/225000 (95%)] Loss: 7759.826172\n",
      "Train Epoch: 79 [217344/225000 (97%)] Loss: 7699.460938\n",
      "Train Epoch: 79 [221440/225000 (98%)] Loss: 7879.255859\n",
      "    epoch          : 79\n",
      "    loss           : 7809.439810864619\n",
      "    val_loss       : 7785.98678092324\n",
      "Train Epoch: 80 [256/225000 (0%)] Loss: 7992.433594\n",
      "Train Epoch: 80 [4352/225000 (2%)] Loss: 7975.611328\n",
      "Train Epoch: 80 [8448/225000 (4%)] Loss: 7890.783203\n",
      "Train Epoch: 80 [12544/225000 (6%)] Loss: 7788.226562\n",
      "Train Epoch: 80 [16640/225000 (7%)] Loss: 7777.789062\n",
      "Train Epoch: 80 [20736/225000 (9%)] Loss: 7924.314453\n",
      "Train Epoch: 80 [24832/225000 (11%)] Loss: 7678.042969\n",
      "Train Epoch: 80 [28928/225000 (13%)] Loss: 7863.115234\n",
      "Train Epoch: 80 [33024/225000 (15%)] Loss: 7551.919922\n",
      "Train Epoch: 80 [37120/225000 (16%)] Loss: 7947.203125\n",
      "Train Epoch: 80 [41216/225000 (18%)] Loss: 7781.601562\n",
      "Train Epoch: 80 [45312/225000 (20%)] Loss: 7839.023438\n",
      "Train Epoch: 80 [49408/225000 (22%)] Loss: 7873.025391\n",
      "Train Epoch: 80 [53504/225000 (24%)] Loss: 7726.861328\n",
      "Train Epoch: 80 [57600/225000 (26%)] Loss: 7690.771484\n",
      "Train Epoch: 80 [61696/225000 (27%)] Loss: 8141.683594\n",
      "Train Epoch: 80 [65792/225000 (29%)] Loss: 7755.066406\n",
      "Train Epoch: 80 [69888/225000 (31%)] Loss: 7648.378906\n",
      "Train Epoch: 80 [73984/225000 (33%)] Loss: 8098.978516\n",
      "Train Epoch: 80 [78080/225000 (35%)] Loss: 7727.044922\n",
      "Train Epoch: 80 [82176/225000 (37%)] Loss: 7707.279297\n",
      "Train Epoch: 80 [86272/225000 (38%)] Loss: 7794.492188\n",
      "Train Epoch: 80 [90368/225000 (40%)] Loss: 7589.890625\n",
      "Train Epoch: 80 [94464/225000 (42%)] Loss: 7676.056641\n",
      "Train Epoch: 80 [98560/225000 (44%)] Loss: 7911.482422\n",
      "Train Epoch: 80 [102656/225000 (46%)] Loss: 7732.037109\n",
      "Train Epoch: 80 [106752/225000 (47%)] Loss: 7891.833984\n",
      "Train Epoch: 80 [110848/225000 (49%)] Loss: 7808.750000\n",
      "Train Epoch: 80 [114944/225000 (51%)] Loss: 7919.513672\n",
      "Train Epoch: 80 [119040/225000 (53%)] Loss: 7756.296875\n",
      "Train Epoch: 80 [123136/225000 (55%)] Loss: 7826.816406\n",
      "Train Epoch: 80 [127232/225000 (57%)] Loss: 7611.142578\n",
      "Train Epoch: 80 [131328/225000 (58%)] Loss: 7690.394531\n",
      "Train Epoch: 80 [135424/225000 (60%)] Loss: 7830.998047\n",
      "Train Epoch: 80 [139520/225000 (62%)] Loss: 7943.191406\n",
      "Train Epoch: 80 [143616/225000 (64%)] Loss: 7870.111328\n",
      "Train Epoch: 80 [147712/225000 (66%)] Loss: 8066.458984\n",
      "Train Epoch: 80 [151808/225000 (67%)] Loss: 7760.722656\n",
      "Train Epoch: 80 [155904/225000 (69%)] Loss: 7604.134766\n",
      "Train Epoch: 80 [160000/225000 (71%)] Loss: 7828.281250\n",
      "Train Epoch: 80 [164096/225000 (73%)] Loss: 7624.105469\n",
      "Train Epoch: 80 [168192/225000 (75%)] Loss: 7920.392578\n",
      "Train Epoch: 80 [172288/225000 (77%)] Loss: 7839.458984\n",
      "Train Epoch: 80 [176384/225000 (78%)] Loss: 7909.480469\n",
      "Train Epoch: 80 [180480/225000 (80%)] Loss: 7858.746094\n",
      "Train Epoch: 80 [184576/225000 (82%)] Loss: 7682.773438\n",
      "Train Epoch: 80 [188672/225000 (84%)] Loss: 7786.388672\n",
      "Train Epoch: 80 [192768/225000 (86%)] Loss: 7844.359375\n",
      "Train Epoch: 80 [196864/225000 (87%)] Loss: 7777.662109\n",
      "Train Epoch: 80 [200960/225000 (89%)] Loss: 7781.496094\n",
      "Train Epoch: 80 [205056/225000 (91%)] Loss: 7875.384766\n",
      "Train Epoch: 80 [209152/225000 (93%)] Loss: 7872.406250\n",
      "Train Epoch: 80 [213248/225000 (95%)] Loss: 8051.089844\n",
      "Train Epoch: 80 [217344/225000 (97%)] Loss: 7803.185547\n",
      "Train Epoch: 80 [221440/225000 (98%)] Loss: 7643.539062\n",
      "    epoch          : 80\n",
      "    loss           : 7799.651878244098\n",
      "    val_loss       : 7786.849698659109\n",
      "Train Epoch: 81 [256/225000 (0%)] Loss: 7750.279297\n",
      "Train Epoch: 81 [4352/225000 (2%)] Loss: 7912.917969\n",
      "Train Epoch: 81 [8448/225000 (4%)] Loss: 7837.218750\n",
      "Train Epoch: 81 [12544/225000 (6%)] Loss: 7773.355469\n",
      "Train Epoch: 81 [16640/225000 (7%)] Loss: 7793.498047\n",
      "Train Epoch: 81 [20736/225000 (9%)] Loss: 7779.623047\n",
      "Train Epoch: 81 [24832/225000 (11%)] Loss: 7704.953125\n",
      "Train Epoch: 81 [28928/225000 (13%)] Loss: 7699.402344\n",
      "Train Epoch: 81 [33024/225000 (15%)] Loss: 7840.621094\n",
      "Train Epoch: 81 [37120/225000 (16%)] Loss: 7648.333984\n",
      "Train Epoch: 81 [41216/225000 (18%)] Loss: 7775.632812\n",
      "Train Epoch: 81 [45312/225000 (20%)] Loss: 8007.656250\n",
      "Train Epoch: 81 [49408/225000 (22%)] Loss: 7751.345703\n",
      "Train Epoch: 81 [53504/225000 (24%)] Loss: 7675.707031\n",
      "Train Epoch: 81 [57600/225000 (26%)] Loss: 7676.345703\n",
      "Train Epoch: 81 [61696/225000 (27%)] Loss: 7739.837891\n",
      "Train Epoch: 81 [65792/225000 (29%)] Loss: 7470.976562\n",
      "Train Epoch: 81 [69888/225000 (31%)] Loss: 7694.220703\n",
      "Train Epoch: 81 [73984/225000 (33%)] Loss: 7824.880859\n",
      "Train Epoch: 81 [78080/225000 (35%)] Loss: 7707.373047\n",
      "Train Epoch: 81 [82176/225000 (37%)] Loss: 7793.617188\n",
      "Train Epoch: 81 [86272/225000 (38%)] Loss: 7813.232422\n",
      "Train Epoch: 81 [90368/225000 (40%)] Loss: 7748.761719\n",
      "Train Epoch: 81 [94464/225000 (42%)] Loss: 7700.037109\n",
      "Train Epoch: 81 [98560/225000 (44%)] Loss: 7634.054688\n",
      "Train Epoch: 81 [102656/225000 (46%)] Loss: 7802.212891\n",
      "Train Epoch: 81 [106752/225000 (47%)] Loss: 7896.578125\n",
      "Train Epoch: 81 [110848/225000 (49%)] Loss: 7959.091797\n",
      "Train Epoch: 81 [114944/225000 (51%)] Loss: 7837.056641\n",
      "Train Epoch: 81 [119040/225000 (53%)] Loss: 7552.535156\n",
      "Train Epoch: 81 [123136/225000 (55%)] Loss: 7860.138672\n",
      "Train Epoch: 81 [127232/225000 (57%)] Loss: 7662.402344\n",
      "Train Epoch: 81 [131328/225000 (58%)] Loss: 7628.373047\n",
      "Train Epoch: 81 [135424/225000 (60%)] Loss: 7781.287109\n",
      "Train Epoch: 81 [139520/225000 (62%)] Loss: 7701.611328\n",
      "Train Epoch: 81 [143616/225000 (64%)] Loss: 8104.394531\n",
      "Train Epoch: 81 [147712/225000 (66%)] Loss: 7876.380859\n",
      "Train Epoch: 81 [151808/225000 (67%)] Loss: 7796.441406\n",
      "Train Epoch: 81 [155904/225000 (69%)] Loss: 7768.574219\n",
      "Train Epoch: 81 [160000/225000 (71%)] Loss: 7833.662109\n",
      "Train Epoch: 81 [164096/225000 (73%)] Loss: 7659.792969\n",
      "Train Epoch: 81 [168192/225000 (75%)] Loss: 7807.154297\n",
      "Train Epoch: 81 [172288/225000 (77%)] Loss: 7991.679688\n",
      "Train Epoch: 81 [176384/225000 (78%)] Loss: 7903.984375\n",
      "Train Epoch: 81 [180480/225000 (80%)] Loss: 7851.488281\n",
      "Train Epoch: 81 [184576/225000 (82%)] Loss: 7819.248047\n",
      "Train Epoch: 81 [188672/225000 (84%)] Loss: 7640.500000\n",
      "Train Epoch: 81 [192768/225000 (86%)] Loss: 7900.410156\n",
      "Train Epoch: 81 [196864/225000 (87%)] Loss: 7770.162109\n",
      "Train Epoch: 81 [200960/225000 (89%)] Loss: 7934.824219\n",
      "Train Epoch: 81 [205056/225000 (91%)] Loss: 7812.802734\n",
      "Train Epoch: 81 [209152/225000 (93%)] Loss: 7791.207031\n",
      "Train Epoch: 81 [213248/225000 (95%)] Loss: 7919.125000\n",
      "Train Epoch: 81 [217344/225000 (97%)] Loss: 7806.566406\n",
      "Train Epoch: 81 [221440/225000 (98%)] Loss: 7706.320312\n",
      "    epoch          : 81\n",
      "    loss           : 7806.213216145833\n",
      "    val_loss       : 7771.014257113544\n",
      "Train Epoch: 82 [256/225000 (0%)] Loss: 7692.589844\n",
      "Train Epoch: 82 [4352/225000 (2%)] Loss: 7948.859375\n",
      "Train Epoch: 82 [8448/225000 (4%)] Loss: 7773.458984\n",
      "Train Epoch: 82 [12544/225000 (6%)] Loss: 7614.730469\n",
      "Train Epoch: 82 [16640/225000 (7%)] Loss: 7771.802734\n",
      "Train Epoch: 82 [20736/225000 (9%)] Loss: 7663.675781\n",
      "Train Epoch: 82 [24832/225000 (11%)] Loss: 7992.031250\n",
      "Train Epoch: 82 [28928/225000 (13%)] Loss: 7802.921875\n",
      "Train Epoch: 82 [33024/225000 (15%)] Loss: 7785.748047\n",
      "Train Epoch: 82 [37120/225000 (16%)] Loss: 7633.177734\n",
      "Train Epoch: 82 [41216/225000 (18%)] Loss: 7825.212891\n",
      "Train Epoch: 82 [45312/225000 (20%)] Loss: 7549.525391\n",
      "Train Epoch: 82 [49408/225000 (22%)] Loss: 7809.697266\n",
      "Train Epoch: 82 [53504/225000 (24%)] Loss: 7786.820312\n",
      "Train Epoch: 82 [57600/225000 (26%)] Loss: 7664.767578\n",
      "Train Epoch: 82 [61696/225000 (27%)] Loss: 7641.957031\n",
      "Train Epoch: 82 [65792/225000 (29%)] Loss: 8019.437500\n",
      "Train Epoch: 82 [69888/225000 (31%)] Loss: 7604.873047\n",
      "Train Epoch: 82 [73984/225000 (33%)] Loss: 7653.978516\n",
      "Train Epoch: 82 [78080/225000 (35%)] Loss: 7707.357422\n",
      "Train Epoch: 82 [82176/225000 (37%)] Loss: 7618.570312\n",
      "Train Epoch: 82 [86272/225000 (38%)] Loss: 7723.001953\n",
      "Train Epoch: 82 [90368/225000 (40%)] Loss: 7782.750000\n",
      "Train Epoch: 82 [94464/225000 (42%)] Loss: 7841.802734\n",
      "Train Epoch: 82 [98560/225000 (44%)] Loss: 7722.562500\n",
      "Train Epoch: 82 [102656/225000 (46%)] Loss: 7743.390625\n",
      "Train Epoch: 82 [106752/225000 (47%)] Loss: 7535.156250\n",
      "Train Epoch: 82 [110848/225000 (49%)] Loss: 8044.078125\n",
      "Train Epoch: 82 [114944/225000 (51%)] Loss: 7946.875000\n",
      "Train Epoch: 82 [119040/225000 (53%)] Loss: 7988.667969\n",
      "Train Epoch: 82 [123136/225000 (55%)] Loss: 7633.691406\n",
      "Train Epoch: 82 [127232/225000 (57%)] Loss: 7761.894531\n",
      "Train Epoch: 82 [131328/225000 (58%)] Loss: 7621.730469\n",
      "Train Epoch: 82 [135424/225000 (60%)] Loss: 7741.484375\n",
      "Train Epoch: 82 [139520/225000 (62%)] Loss: 7702.984375\n",
      "Train Epoch: 82 [143616/225000 (64%)] Loss: 7860.830078\n",
      "Train Epoch: 82 [147712/225000 (66%)] Loss: 7565.121094\n",
      "Train Epoch: 82 [151808/225000 (67%)] Loss: 7833.863281\n",
      "Train Epoch: 82 [155904/225000 (69%)] Loss: 7712.292969\n",
      "Train Epoch: 82 [160000/225000 (71%)] Loss: 7806.587891\n",
      "Train Epoch: 82 [164096/225000 (73%)] Loss: 7732.529297\n",
      "Train Epoch: 82 [168192/225000 (75%)] Loss: 7844.320312\n",
      "Train Epoch: 82 [172288/225000 (77%)] Loss: 7712.958984\n",
      "Train Epoch: 82 [176384/225000 (78%)] Loss: 7868.675781\n",
      "Train Epoch: 82 [180480/225000 (80%)] Loss: 7680.939453\n",
      "Train Epoch: 82 [184576/225000 (82%)] Loss: 7711.621094\n",
      "Train Epoch: 82 [188672/225000 (84%)] Loss: 7885.691406\n",
      "Train Epoch: 82 [192768/225000 (86%)] Loss: 7636.007812\n",
      "Train Epoch: 82 [196864/225000 (87%)] Loss: 7840.458984\n",
      "Train Epoch: 82 [200960/225000 (89%)] Loss: 7856.322266\n",
      "Train Epoch: 82 [205056/225000 (91%)] Loss: 7746.091797\n",
      "Train Epoch: 82 [209152/225000 (93%)] Loss: 7636.744141\n",
      "Train Epoch: 82 [213248/225000 (95%)] Loss: 7888.837891\n",
      "Train Epoch: 82 [217344/225000 (97%)] Loss: 7782.294922\n",
      "Train Epoch: 82 [221440/225000 (98%)] Loss: 7672.099609\n",
      "    epoch          : 82\n",
      "    loss           : 7813.552006674843\n",
      "    val_loss       : 7908.144464057319\n",
      "Train Epoch: 83 [256/225000 (0%)] Loss: 7908.167969\n",
      "Train Epoch: 83 [4352/225000 (2%)] Loss: 7924.910156\n",
      "Train Epoch: 83 [8448/225000 (4%)] Loss: 7849.013672\n",
      "Train Epoch: 83 [12544/225000 (6%)] Loss: 7735.554688\n",
      "Train Epoch: 83 [16640/225000 (7%)] Loss: 7692.910156\n",
      "Train Epoch: 83 [20736/225000 (9%)] Loss: 7880.400391\n",
      "Train Epoch: 83 [24832/225000 (11%)] Loss: 7757.746094\n",
      "Train Epoch: 83 [28928/225000 (13%)] Loss: 7864.841797\n",
      "Train Epoch: 83 [33024/225000 (15%)] Loss: 7827.433594\n",
      "Train Epoch: 83 [37120/225000 (16%)] Loss: 7829.167969\n",
      "Train Epoch: 83 [41216/225000 (18%)] Loss: 7792.406250\n",
      "Train Epoch: 83 [45312/225000 (20%)] Loss: 7639.828125\n",
      "Train Epoch: 83 [49408/225000 (22%)] Loss: 7867.429688\n",
      "Train Epoch: 83 [53504/225000 (24%)] Loss: 7606.906250\n",
      "Train Epoch: 83 [57600/225000 (26%)] Loss: 7915.921875\n",
      "Train Epoch: 83 [61696/225000 (27%)] Loss: 7804.509766\n",
      "Train Epoch: 83 [65792/225000 (29%)] Loss: 7676.234375\n",
      "Train Epoch: 83 [69888/225000 (31%)] Loss: 7678.466797\n",
      "Train Epoch: 83 [73984/225000 (33%)] Loss: 7732.261719\n",
      "Train Epoch: 83 [78080/225000 (35%)] Loss: 7756.400391\n",
      "Train Epoch: 83 [82176/225000 (37%)] Loss: 7698.250000\n",
      "Train Epoch: 83 [86272/225000 (38%)] Loss: 7691.656250\n",
      "Train Epoch: 83 [90368/225000 (40%)] Loss: 7785.111328\n",
      "Train Epoch: 83 [94464/225000 (42%)] Loss: 7882.882812\n",
      "Train Epoch: 83 [98560/225000 (44%)] Loss: 7869.531250\n",
      "Train Epoch: 83 [102656/225000 (46%)] Loss: 7886.835938\n",
      "Train Epoch: 83 [106752/225000 (47%)] Loss: 7615.384766\n",
      "Train Epoch: 83 [110848/225000 (49%)] Loss: 7700.738281\n",
      "Train Epoch: 83 [114944/225000 (51%)] Loss: 7628.125000\n",
      "Train Epoch: 83 [119040/225000 (53%)] Loss: 7837.146484\n",
      "Train Epoch: 83 [123136/225000 (55%)] Loss: 7798.726562\n",
      "Train Epoch: 83 [127232/225000 (57%)] Loss: 7665.626953\n",
      "Train Epoch: 83 [131328/225000 (58%)] Loss: 7824.900391\n",
      "Train Epoch: 83 [135424/225000 (60%)] Loss: 7800.335938\n",
      "Train Epoch: 83 [139520/225000 (62%)] Loss: 7848.927734\n",
      "Train Epoch: 83 [143616/225000 (64%)] Loss: 7815.158203\n",
      "Train Epoch: 83 [147712/225000 (66%)] Loss: 7640.544922\n",
      "Train Epoch: 83 [151808/225000 (67%)] Loss: 7838.292969\n",
      "Train Epoch: 83 [155904/225000 (69%)] Loss: 7740.148438\n",
      "Train Epoch: 83 [160000/225000 (71%)] Loss: 7605.236328\n",
      "Train Epoch: 83 [164096/225000 (73%)] Loss: 7726.011719\n",
      "Train Epoch: 83 [168192/225000 (75%)] Loss: 7827.478516\n",
      "Train Epoch: 83 [172288/225000 (77%)] Loss: 7757.263672\n",
      "Train Epoch: 83 [176384/225000 (78%)] Loss: 7757.097656\n",
      "Train Epoch: 83 [180480/225000 (80%)] Loss: 7801.664062\n",
      "Train Epoch: 83 [184576/225000 (82%)] Loss: 7783.384766\n",
      "Train Epoch: 83 [188672/225000 (84%)] Loss: 7722.615234\n",
      "Train Epoch: 83 [192768/225000 (86%)] Loss: 7793.212891\n",
      "Train Epoch: 83 [196864/225000 (87%)] Loss: 7628.308594\n",
      "Train Epoch: 83 [200960/225000 (89%)] Loss: 7611.234375\n",
      "Train Epoch: 83 [205056/225000 (91%)] Loss: 7508.355469\n",
      "Train Epoch: 83 [209152/225000 (93%)] Loss: 7814.765625\n",
      "Train Epoch: 83 [213248/225000 (95%)] Loss: 7946.390625\n",
      "Train Epoch: 83 [217344/225000 (97%)] Loss: 7722.681641\n",
      "Train Epoch: 83 [221440/225000 (98%)] Loss: 7861.732422\n",
      "    epoch          : 83\n",
      "    loss           : 7778.9858537311575\n",
      "    val_loss       : 7762.704937065134\n",
      "Train Epoch: 84 [256/225000 (0%)] Loss: 7772.468750\n",
      "Train Epoch: 84 [4352/225000 (2%)] Loss: 7662.716797\n",
      "Train Epoch: 84 [8448/225000 (4%)] Loss: 7528.259766\n",
      "Train Epoch: 84 [12544/225000 (6%)] Loss: 7896.585938\n",
      "Train Epoch: 84 [16640/225000 (7%)] Loss: 7722.093750\n",
      "Train Epoch: 84 [20736/225000 (9%)] Loss: 7763.619141\n",
      "Train Epoch: 84 [24832/225000 (11%)] Loss: 7720.568359\n",
      "Train Epoch: 84 [28928/225000 (13%)] Loss: 7678.291016\n",
      "Train Epoch: 84 [33024/225000 (15%)] Loss: 7858.443359\n",
      "Train Epoch: 84 [37120/225000 (16%)] Loss: 7695.837891\n",
      "Train Epoch: 84 [41216/225000 (18%)] Loss: 7842.021484\n",
      "Train Epoch: 84 [45312/225000 (20%)] Loss: 7628.642578\n",
      "Train Epoch: 84 [49408/225000 (22%)] Loss: 7773.423828\n",
      "Train Epoch: 84 [53504/225000 (24%)] Loss: 7611.429688\n",
      "Train Epoch: 84 [57600/225000 (26%)] Loss: 7828.898438\n",
      "Train Epoch: 84 [61696/225000 (27%)] Loss: 7793.052734\n",
      "Train Epoch: 84 [65792/225000 (29%)] Loss: 7802.449219\n",
      "Train Epoch: 84 [69888/225000 (31%)] Loss: 7614.818359\n",
      "Train Epoch: 84 [73984/225000 (33%)] Loss: 7839.623047\n",
      "Train Epoch: 84 [78080/225000 (35%)] Loss: 7673.621094\n",
      "Train Epoch: 84 [82176/225000 (37%)] Loss: 7773.751953\n",
      "Train Epoch: 84 [86272/225000 (38%)] Loss: 7899.900391\n",
      "Train Epoch: 84 [90368/225000 (40%)] Loss: 7800.783203\n",
      "Train Epoch: 84 [94464/225000 (42%)] Loss: 7917.658203\n",
      "Train Epoch: 84 [98560/225000 (44%)] Loss: 7734.263672\n",
      "Train Epoch: 84 [102656/225000 (46%)] Loss: 7852.298828\n",
      "Train Epoch: 84 [106752/225000 (47%)] Loss: 7691.595703\n",
      "Train Epoch: 84 [110848/225000 (49%)] Loss: 7711.636719\n",
      "Train Epoch: 84 [114944/225000 (51%)] Loss: 7728.121094\n",
      "Train Epoch: 84 [119040/225000 (53%)] Loss: 7696.951172\n",
      "Train Epoch: 84 [123136/225000 (55%)] Loss: 7705.050781\n",
      "Train Epoch: 84 [127232/225000 (57%)] Loss: 7660.935547\n",
      "Train Epoch: 84 [131328/225000 (58%)] Loss: 7775.753906\n",
      "Train Epoch: 84 [135424/225000 (60%)] Loss: 7847.810547\n",
      "Train Epoch: 84 [139520/225000 (62%)] Loss: 7748.601562\n",
      "Train Epoch: 84 [143616/225000 (64%)] Loss: 7753.404297\n",
      "Train Epoch: 84 [147712/225000 (66%)] Loss: 7963.089844\n",
      "Train Epoch: 84 [151808/225000 (67%)] Loss: 7812.546875\n",
      "Train Epoch: 84 [155904/225000 (69%)] Loss: 7654.542969\n",
      "Train Epoch: 84 [160000/225000 (71%)] Loss: 7878.945312\n",
      "Train Epoch: 84 [164096/225000 (73%)] Loss: 7626.828125\n",
      "Train Epoch: 84 [168192/225000 (75%)] Loss: 7679.632812\n",
      "Train Epoch: 84 [172288/225000 (77%)] Loss: 7869.285156\n",
      "Train Epoch: 84 [176384/225000 (78%)] Loss: 7703.785156\n",
      "Train Epoch: 84 [180480/225000 (80%)] Loss: 7670.464844\n",
      "Train Epoch: 84 [184576/225000 (82%)] Loss: 7577.921875\n",
      "Train Epoch: 84 [188672/225000 (84%)] Loss: 7814.287109\n",
      "Train Epoch: 84 [192768/225000 (86%)] Loss: 7680.636719\n",
      "Train Epoch: 84 [196864/225000 (87%)] Loss: 7707.611328\n",
      "Train Epoch: 84 [200960/225000 (89%)] Loss: 7795.339844\n",
      "Train Epoch: 84 [205056/225000 (91%)] Loss: 7637.998047\n",
      "Train Epoch: 84 [209152/225000 (93%)] Loss: 7678.613281\n",
      "Train Epoch: 84 [213248/225000 (95%)] Loss: 7728.347656\n",
      "Train Epoch: 84 [217344/225000 (97%)] Loss: 7797.417969\n",
      "Train Epoch: 84 [221440/225000 (98%)] Loss: 7823.425781\n",
      "    epoch          : 84\n",
      "    loss           : 7763.056678398749\n",
      "    val_loss       : 7745.3642843462985\n",
      "Train Epoch: 85 [256/225000 (0%)] Loss: 7858.511719\n",
      "Train Epoch: 85 [4352/225000 (2%)] Loss: 7641.185547\n",
      "Train Epoch: 85 [8448/225000 (4%)] Loss: 7718.652344\n",
      "Train Epoch: 85 [12544/225000 (6%)] Loss: 7852.146484\n",
      "Train Epoch: 85 [16640/225000 (7%)] Loss: 7892.380859\n",
      "Train Epoch: 85 [20736/225000 (9%)] Loss: 7797.507812\n",
      "Train Epoch: 85 [24832/225000 (11%)] Loss: 7660.841797\n",
      "Train Epoch: 85 [28928/225000 (13%)] Loss: 8084.699219\n",
      "Train Epoch: 85 [33024/225000 (15%)] Loss: 7666.576172\n",
      "Train Epoch: 85 [37120/225000 (16%)] Loss: 7767.001953\n",
      "Train Epoch: 85 [41216/225000 (18%)] Loss: 7757.820312\n",
      "Train Epoch: 85 [45312/225000 (20%)] Loss: 7906.080078\n",
      "Train Epoch: 85 [49408/225000 (22%)] Loss: 7688.994141\n",
      "Train Epoch: 85 [53504/225000 (24%)] Loss: 7767.625000\n",
      "Train Epoch: 85 [57600/225000 (26%)] Loss: 7660.931641\n",
      "Train Epoch: 85 [61696/225000 (27%)] Loss: 7752.132812\n",
      "Train Epoch: 85 [65792/225000 (29%)] Loss: 7898.068359\n",
      "Train Epoch: 85 [69888/225000 (31%)] Loss: 7548.722656\n",
      "Train Epoch: 85 [73984/225000 (33%)] Loss: 7819.242188\n",
      "Train Epoch: 85 [78080/225000 (35%)] Loss: 7732.892578\n",
      "Train Epoch: 85 [82176/225000 (37%)] Loss: 7729.269531\n",
      "Train Epoch: 85 [86272/225000 (38%)] Loss: 7762.568359\n",
      "Train Epoch: 85 [90368/225000 (40%)] Loss: 7846.486328\n",
      "Train Epoch: 85 [94464/225000 (42%)] Loss: 7812.769531\n",
      "Train Epoch: 85 [98560/225000 (44%)] Loss: 7884.783203\n",
      "Train Epoch: 85 [102656/225000 (46%)] Loss: 7826.673828\n",
      "Train Epoch: 85 [106752/225000 (47%)] Loss: 7786.380859\n",
      "Train Epoch: 85 [110848/225000 (49%)] Loss: 7600.484375\n",
      "Train Epoch: 85 [114944/225000 (51%)] Loss: 7813.484375\n",
      "Train Epoch: 85 [119040/225000 (53%)] Loss: 7841.949219\n",
      "Train Epoch: 85 [123136/225000 (55%)] Loss: 7782.402344\n",
      "Train Epoch: 85 [127232/225000 (57%)] Loss: 7594.105469\n",
      "Train Epoch: 85 [131328/225000 (58%)] Loss: 7687.826172\n",
      "Train Epoch: 85 [135424/225000 (60%)] Loss: 7642.744141\n",
      "Train Epoch: 85 [139520/225000 (62%)] Loss: 7729.439453\n",
      "Train Epoch: 85 [143616/225000 (64%)] Loss: 7648.107422\n",
      "Train Epoch: 85 [147712/225000 (66%)] Loss: 7951.640625\n",
      "Train Epoch: 85 [151808/225000 (67%)] Loss: 7826.593750\n",
      "Train Epoch: 85 [155904/225000 (69%)] Loss: 7721.345703\n",
      "Train Epoch: 85 [160000/225000 (71%)] Loss: 7609.435547\n",
      "Train Epoch: 85 [164096/225000 (73%)] Loss: 7773.361328\n",
      "Train Epoch: 85 [168192/225000 (75%)] Loss: 7929.451172\n",
      "Train Epoch: 85 [172288/225000 (77%)] Loss: 7916.058594\n",
      "Train Epoch: 85 [176384/225000 (78%)] Loss: 7691.501953\n",
      "Train Epoch: 85 [180480/225000 (80%)] Loss: 7933.509766\n",
      "Train Epoch: 85 [184576/225000 (82%)] Loss: 7580.421875\n",
      "Train Epoch: 85 [188672/225000 (84%)] Loss: 7776.326172\n",
      "Train Epoch: 85 [192768/225000 (86%)] Loss: 7673.648438\n",
      "Train Epoch: 85 [196864/225000 (87%)] Loss: 7829.240234\n",
      "Train Epoch: 85 [200960/225000 (89%)] Loss: 7839.857422\n",
      "Train Epoch: 85 [205056/225000 (91%)] Loss: 8064.597656\n",
      "Train Epoch: 85 [209152/225000 (93%)] Loss: 7712.560547\n",
      "Train Epoch: 85 [213248/225000 (95%)] Loss: 7751.443359\n",
      "Train Epoch: 85 [217344/225000 (97%)] Loss: 7658.287109\n",
      "Train Epoch: 85 [221440/225000 (98%)] Loss: 7880.947266\n",
      "    epoch          : 85\n",
      "    loss           : 7802.515894971203\n",
      "    val_loss       : 7732.611919275352\n",
      "Train Epoch: 86 [256/225000 (0%)] Loss: 7626.107422\n",
      "Train Epoch: 86 [4352/225000 (2%)] Loss: 7564.902344\n",
      "Train Epoch: 86 [8448/225000 (4%)] Loss: 7690.433594\n",
      "Train Epoch: 86 [12544/225000 (6%)] Loss: 7760.726562\n",
      "Train Epoch: 86 [16640/225000 (7%)] Loss: 7712.658203\n",
      "Train Epoch: 86 [20736/225000 (9%)] Loss: 7655.980469\n",
      "Train Epoch: 86 [24832/225000 (11%)] Loss: 7742.277344\n",
      "Train Epoch: 86 [28928/225000 (13%)] Loss: 7912.324219\n",
      "Train Epoch: 86 [33024/225000 (15%)] Loss: 7848.041016\n",
      "Train Epoch: 86 [37120/225000 (16%)] Loss: 7621.439453\n",
      "Train Epoch: 86 [41216/225000 (18%)] Loss: 7630.318359\n",
      "Train Epoch: 86 [45312/225000 (20%)] Loss: 7742.419922\n",
      "Train Epoch: 86 [49408/225000 (22%)] Loss: 7604.287109\n",
      "Train Epoch: 86 [53504/225000 (24%)] Loss: 7933.138672\n",
      "Train Epoch: 86 [57600/225000 (26%)] Loss: 7724.167969\n",
      "Train Epoch: 86 [61696/225000 (27%)] Loss: 7624.326172\n",
      "Train Epoch: 86 [65792/225000 (29%)] Loss: 7661.869141\n",
      "Train Epoch: 86 [69888/225000 (31%)] Loss: 7534.216797\n",
      "Train Epoch: 86 [73984/225000 (33%)] Loss: 7779.037109\n",
      "Train Epoch: 86 [78080/225000 (35%)] Loss: 7835.160156\n",
      "Train Epoch: 86 [82176/225000 (37%)] Loss: 7694.519531\n",
      "Train Epoch: 86 [86272/225000 (38%)] Loss: 7818.529297\n",
      "Train Epoch: 86 [90368/225000 (40%)] Loss: 7675.544922\n",
      "Train Epoch: 86 [94464/225000 (42%)] Loss: 7683.541016\n",
      "Train Epoch: 86 [98560/225000 (44%)] Loss: 7562.167969\n",
      "Train Epoch: 86 [102656/225000 (46%)] Loss: 7967.832031\n",
      "Train Epoch: 86 [106752/225000 (47%)] Loss: 7957.253906\n",
      "Train Epoch: 86 [110848/225000 (49%)] Loss: 7763.646484\n",
      "Train Epoch: 86 [114944/225000 (51%)] Loss: 7799.123047\n",
      "Train Epoch: 86 [119040/225000 (53%)] Loss: 7701.189453\n",
      "Train Epoch: 86 [123136/225000 (55%)] Loss: 7723.525391\n",
      "Train Epoch: 86 [127232/225000 (57%)] Loss: 7892.472656\n",
      "Train Epoch: 86 [131328/225000 (58%)] Loss: 7625.970703\n",
      "Train Epoch: 86 [135424/225000 (60%)] Loss: 7831.189453\n",
      "Train Epoch: 86 [139520/225000 (62%)] Loss: 7595.984375\n",
      "Train Epoch: 86 [143616/225000 (64%)] Loss: 7831.443359\n",
      "Train Epoch: 86 [147712/225000 (66%)] Loss: 7686.494141\n",
      "Train Epoch: 86 [151808/225000 (67%)] Loss: 7701.404297\n",
      "Train Epoch: 86 [155904/225000 (69%)] Loss: 7665.939453\n",
      "Train Epoch: 86 [160000/225000 (71%)] Loss: 7640.302734\n",
      "Train Epoch: 86 [164096/225000 (73%)] Loss: 7694.781250\n",
      "Train Epoch: 86 [168192/225000 (75%)] Loss: 7942.699219\n",
      "Train Epoch: 86 [172288/225000 (77%)] Loss: 7672.505859\n",
      "Train Epoch: 86 [176384/225000 (78%)] Loss: 7747.644531\n",
      "Train Epoch: 86 [180480/225000 (80%)] Loss: 7711.199219\n",
      "Train Epoch: 86 [184576/225000 (82%)] Loss: 7706.667969\n",
      "Train Epoch: 86 [188672/225000 (84%)] Loss: 7545.912109\n",
      "Train Epoch: 86 [192768/225000 (86%)] Loss: 7929.515625\n",
      "Train Epoch: 86 [196864/225000 (87%)] Loss: 7603.832031\n",
      "Train Epoch: 86 [200960/225000 (89%)] Loss: 7760.810547\n",
      "Train Epoch: 86 [205056/225000 (91%)] Loss: 7672.228516\n",
      "Train Epoch: 86 [209152/225000 (93%)] Loss: 7831.935547\n",
      "Train Epoch: 86 [213248/225000 (95%)] Loss: 7980.699219\n",
      "Train Epoch: 86 [217344/225000 (97%)] Loss: 7980.603516\n",
      "Train Epoch: 86 [221440/225000 (98%)] Loss: 7769.564453\n",
      "    epoch          : 86\n",
      "    loss           : 7769.006327102887\n",
      "    val_loss       : 7846.768000798566\n",
      "Train Epoch: 87 [256/225000 (0%)] Loss: 7626.267578\n",
      "Train Epoch: 87 [4352/225000 (2%)] Loss: 7840.302734\n",
      "Train Epoch: 87 [8448/225000 (4%)] Loss: 7533.685547\n",
      "Train Epoch: 87 [12544/225000 (6%)] Loss: 7677.199219\n",
      "Train Epoch: 87 [16640/225000 (7%)] Loss: 7682.429688\n",
      "Train Epoch: 87 [20736/225000 (9%)] Loss: 7516.646484\n",
      "Train Epoch: 87 [24832/225000 (11%)] Loss: 7738.138672\n",
      "Train Epoch: 87 [28928/225000 (13%)] Loss: 7665.146484\n",
      "Train Epoch: 87 [33024/225000 (15%)] Loss: 7713.601562\n",
      "Train Epoch: 87 [37120/225000 (16%)] Loss: 7843.865234\n",
      "Train Epoch: 87 [41216/225000 (18%)] Loss: 7764.099609\n",
      "Train Epoch: 87 [45312/225000 (20%)] Loss: 7823.294922\n",
      "Train Epoch: 87 [49408/225000 (22%)] Loss: 7523.158203\n",
      "Train Epoch: 87 [53504/225000 (24%)] Loss: 7679.736328\n",
      "Train Epoch: 87 [57600/225000 (26%)] Loss: 7788.669922\n",
      "Train Epoch: 87 [61696/225000 (27%)] Loss: 7755.755859\n",
      "Train Epoch: 87 [65792/225000 (29%)] Loss: 7643.023438\n",
      "Train Epoch: 87 [69888/225000 (31%)] Loss: 7783.070312\n",
      "Train Epoch: 87 [73984/225000 (33%)] Loss: 7885.650391\n",
      "Train Epoch: 87 [78080/225000 (35%)] Loss: 7900.601562\n",
      "Train Epoch: 87 [82176/225000 (37%)] Loss: 7718.638672\n",
      "Train Epoch: 87 [86272/225000 (38%)] Loss: 7786.750000\n",
      "Train Epoch: 87 [90368/225000 (40%)] Loss: 7705.976562\n",
      "Train Epoch: 87 [94464/225000 (42%)] Loss: 7688.259766\n",
      "Train Epoch: 87 [98560/225000 (44%)] Loss: 7685.183594\n",
      "Train Epoch: 87 [102656/225000 (46%)] Loss: 7769.156250\n",
      "Train Epoch: 87 [106752/225000 (47%)] Loss: 7584.447266\n",
      "Train Epoch: 87 [110848/225000 (49%)] Loss: 7752.796875\n",
      "Train Epoch: 87 [114944/225000 (51%)] Loss: 7799.806641\n",
      "Train Epoch: 87 [119040/225000 (53%)] Loss: 7731.666016\n",
      "Train Epoch: 87 [123136/225000 (55%)] Loss: 7699.273438\n",
      "Train Epoch: 87 [127232/225000 (57%)] Loss: 7821.359375\n",
      "Train Epoch: 87 [131328/225000 (58%)] Loss: 7657.513672\n",
      "Train Epoch: 87 [135424/225000 (60%)] Loss: 7545.595703\n",
      "Train Epoch: 87 [139520/225000 (62%)] Loss: 7657.980469\n",
      "Train Epoch: 87 [143616/225000 (64%)] Loss: 7538.539062\n",
      "Train Epoch: 87 [147712/225000 (66%)] Loss: 7585.435547\n",
      "Train Epoch: 87 [151808/225000 (67%)] Loss: 7525.480469\n",
      "Train Epoch: 87 [155904/225000 (69%)] Loss: 7613.898438\n",
      "Train Epoch: 87 [160000/225000 (71%)] Loss: 7654.708984\n",
      "Train Epoch: 87 [164096/225000 (73%)] Loss: 7669.783203\n",
      "Train Epoch: 87 [168192/225000 (75%)] Loss: 7689.492188\n",
      "Train Epoch: 87 [172288/225000 (77%)] Loss: 7800.914062\n",
      "Train Epoch: 87 [176384/225000 (78%)] Loss: 7738.193359\n",
      "Train Epoch: 87 [180480/225000 (80%)] Loss: 7753.623047\n",
      "Train Epoch: 87 [184576/225000 (82%)] Loss: 7686.185547\n",
      "Train Epoch: 87 [188672/225000 (84%)] Loss: 7683.216797\n",
      "Train Epoch: 87 [192768/225000 (86%)] Loss: 7923.861328\n",
      "Train Epoch: 87 [196864/225000 (87%)] Loss: 7859.685547\n",
      "Train Epoch: 87 [200960/225000 (89%)] Loss: 7822.121094\n",
      "Train Epoch: 87 [205056/225000 (91%)] Loss: 7705.015625\n",
      "Train Epoch: 87 [209152/225000 (93%)] Loss: 7799.898438\n",
      "Train Epoch: 87 [213248/225000 (95%)] Loss: 8086.832031\n",
      "Train Epoch: 87 [217344/225000 (97%)] Loss: 7784.441406\n",
      "Train Epoch: 87 [221440/225000 (98%)] Loss: 7691.791016\n",
      "    epoch          : 87\n",
      "    loss           : 7763.355348762799\n",
      "    val_loss       : 7721.631749497385\n",
      "Train Epoch: 88 [256/225000 (0%)] Loss: 7621.140625\n",
      "Train Epoch: 88 [4352/225000 (2%)] Loss: 7693.324219\n",
      "Train Epoch: 88 [8448/225000 (4%)] Loss: 7717.962891\n",
      "Train Epoch: 88 [12544/225000 (6%)] Loss: 7787.755859\n",
      "Train Epoch: 88 [16640/225000 (7%)] Loss: 7804.812500\n",
      "Train Epoch: 88 [20736/225000 (9%)] Loss: 7602.488281\n",
      "Train Epoch: 88 [24832/225000 (11%)] Loss: 7710.314453\n",
      "Train Epoch: 88 [28928/225000 (13%)] Loss: 7778.732422\n",
      "Train Epoch: 88 [33024/225000 (15%)] Loss: 7622.681641\n",
      "Train Epoch: 88 [37120/225000 (16%)] Loss: 7730.525391\n",
      "Train Epoch: 88 [41216/225000 (18%)] Loss: 7704.300781\n",
      "Train Epoch: 88 [45312/225000 (20%)] Loss: 7792.542969\n",
      "Train Epoch: 88 [49408/225000 (22%)] Loss: 7845.007812\n",
      "Train Epoch: 88 [53504/225000 (24%)] Loss: 7546.082031\n",
      "Train Epoch: 88 [57600/225000 (26%)] Loss: 7749.630859\n",
      "Train Epoch: 88 [61696/225000 (27%)] Loss: 7663.476562\n",
      "Train Epoch: 88 [65792/225000 (29%)] Loss: 7721.101562\n",
      "Train Epoch: 88 [69888/225000 (31%)] Loss: 7702.890625\n",
      "Train Epoch: 88 [73984/225000 (33%)] Loss: 7779.003906\n",
      "Train Epoch: 88 [78080/225000 (35%)] Loss: 7846.841797\n",
      "Train Epoch: 88 [82176/225000 (37%)] Loss: 7712.636719\n",
      "Train Epoch: 88 [86272/225000 (38%)] Loss: 7625.845703\n",
      "Train Epoch: 88 [90368/225000 (40%)] Loss: 7777.349609\n",
      "Train Epoch: 88 [94464/225000 (42%)] Loss: 7804.710938\n",
      "Train Epoch: 88 [98560/225000 (44%)] Loss: 7704.962891\n",
      "Train Epoch: 88 [102656/225000 (46%)] Loss: 7729.277344\n",
      "Train Epoch: 88 [106752/225000 (47%)] Loss: 7932.488281\n",
      "Train Epoch: 88 [110848/225000 (49%)] Loss: 7706.939453\n",
      "Train Epoch: 88 [114944/225000 (51%)] Loss: 7718.230469\n",
      "Train Epoch: 88 [119040/225000 (53%)] Loss: 7628.460938\n",
      "Train Epoch: 88 [123136/225000 (55%)] Loss: 7624.537109\n",
      "Train Epoch: 88 [127232/225000 (57%)] Loss: 7605.039062\n",
      "Train Epoch: 88 [131328/225000 (58%)] Loss: 7763.933594\n",
      "Train Epoch: 88 [135424/225000 (60%)] Loss: 7903.320312\n",
      "Train Epoch: 88 [139520/225000 (62%)] Loss: 7813.564453\n",
      "Train Epoch: 88 [143616/225000 (64%)] Loss: 7800.945312\n",
      "Train Epoch: 88 [147712/225000 (66%)] Loss: 7758.962891\n",
      "Train Epoch: 88 [151808/225000 (67%)] Loss: 7740.613281\n",
      "Train Epoch: 88 [155904/225000 (69%)] Loss: 7628.052734\n",
      "Train Epoch: 88 [160000/225000 (71%)] Loss: 7521.298828\n",
      "Train Epoch: 88 [164096/225000 (73%)] Loss: 7816.650391\n",
      "Train Epoch: 88 [168192/225000 (75%)] Loss: 7490.193359\n",
      "Train Epoch: 88 [172288/225000 (77%)] Loss: 7630.361328\n",
      "Train Epoch: 88 [176384/225000 (78%)] Loss: 7587.505859\n",
      "Train Epoch: 88 [180480/225000 (80%)] Loss: 7535.859375\n",
      "Train Epoch: 88 [184576/225000 (82%)] Loss: 7765.628906\n",
      "Train Epoch: 88 [188672/225000 (84%)] Loss: 7796.486328\n",
      "Train Epoch: 88 [192768/225000 (86%)] Loss: 7557.658203\n",
      "Train Epoch: 88 [196864/225000 (87%)] Loss: 7856.937500\n",
      "Train Epoch: 88 [200960/225000 (89%)] Loss: 7829.396484\n",
      "Train Epoch: 88 [205056/225000 (91%)] Loss: 7830.365234\n",
      "Train Epoch: 88 [209152/225000 (93%)] Loss: 7758.150391\n",
      "Train Epoch: 88 [213248/225000 (95%)] Loss: 7890.425781\n",
      "Train Epoch: 88 [217344/225000 (97%)] Loss: 7635.644531\n",
      "Train Epoch: 88 [221440/225000 (98%)] Loss: 7632.298828\n",
      "    epoch          : 88\n",
      "    loss           : 7726.939284254124\n",
      "    val_loss       : 7712.410571459604\n",
      "Train Epoch: 89 [256/225000 (0%)] Loss: 7592.369141\n",
      "Train Epoch: 89 [4352/225000 (2%)] Loss: 7767.662109\n",
      "Train Epoch: 89 [8448/225000 (4%)] Loss: 7591.400391\n",
      "Train Epoch: 89 [12544/225000 (6%)] Loss: 7715.560547\n",
      "Train Epoch: 89 [16640/225000 (7%)] Loss: 7576.552734\n",
      "Train Epoch: 89 [20736/225000 (9%)] Loss: 7692.066406\n",
      "Train Epoch: 89 [24832/225000 (11%)] Loss: 7732.310547\n",
      "Train Epoch: 89 [28928/225000 (13%)] Loss: 7642.058594\n",
      "Train Epoch: 89 [33024/225000 (15%)] Loss: 7728.544922\n",
      "Train Epoch: 89 [37120/225000 (16%)] Loss: 7779.630859\n",
      "Train Epoch: 89 [41216/225000 (18%)] Loss: 7947.427734\n",
      "Train Epoch: 89 [45312/225000 (20%)] Loss: 7797.875000\n",
      "Train Epoch: 89 [49408/225000 (22%)] Loss: 7591.302734\n",
      "Train Epoch: 89 [53504/225000 (24%)] Loss: 7502.234375\n",
      "Train Epoch: 89 [57600/225000 (26%)] Loss: 7758.351562\n",
      "Train Epoch: 89 [61696/225000 (27%)] Loss: 7829.564453\n",
      "Train Epoch: 89 [65792/225000 (29%)] Loss: 7603.550781\n",
      "Train Epoch: 89 [69888/225000 (31%)] Loss: 7817.185547\n",
      "Train Epoch: 89 [73984/225000 (33%)] Loss: 7844.236328\n",
      "Train Epoch: 89 [78080/225000 (35%)] Loss: 7466.855469\n",
      "Train Epoch: 89 [82176/225000 (37%)] Loss: 7637.146484\n",
      "Train Epoch: 89 [86272/225000 (38%)] Loss: 7703.402344\n",
      "Train Epoch: 89 [90368/225000 (40%)] Loss: 7595.976562\n",
      "Train Epoch: 89 [94464/225000 (42%)] Loss: 7833.808594\n",
      "Train Epoch: 89 [98560/225000 (44%)] Loss: 7713.683594\n",
      "Train Epoch: 89 [102656/225000 (46%)] Loss: 7565.986328\n",
      "Train Epoch: 89 [106752/225000 (47%)] Loss: 7667.638672\n",
      "Train Epoch: 89 [110848/225000 (49%)] Loss: 7689.900391\n",
      "Train Epoch: 89 [114944/225000 (51%)] Loss: 7652.287109\n",
      "Train Epoch: 89 [119040/225000 (53%)] Loss: 7790.326172\n",
      "Train Epoch: 89 [123136/225000 (55%)] Loss: 7696.570312\n",
      "Train Epoch: 89 [127232/225000 (57%)] Loss: 7829.718750\n",
      "Train Epoch: 89 [131328/225000 (58%)] Loss: 7596.214844\n",
      "Train Epoch: 89 [135424/225000 (60%)] Loss: 7736.126953\n",
      "Train Epoch: 89 [139520/225000 (62%)] Loss: 7694.388672\n",
      "Train Epoch: 89 [143616/225000 (64%)] Loss: 7711.380859\n",
      "Train Epoch: 89 [147712/225000 (66%)] Loss: 7849.226562\n",
      "Train Epoch: 89 [151808/225000 (67%)] Loss: 7510.474609\n",
      "Train Epoch: 89 [155904/225000 (69%)] Loss: 7758.212891\n",
      "Train Epoch: 89 [160000/225000 (71%)] Loss: 7756.464844\n",
      "Train Epoch: 89 [164096/225000 (73%)] Loss: 8031.285156\n",
      "Train Epoch: 89 [168192/225000 (75%)] Loss: 7637.970703\n",
      "Train Epoch: 89 [172288/225000 (77%)] Loss: 7799.648438\n",
      "Train Epoch: 89 [176384/225000 (78%)] Loss: 7907.917969\n",
      "Train Epoch: 89 [180480/225000 (80%)] Loss: 7829.000000\n",
      "Train Epoch: 89 [184576/225000 (82%)] Loss: 7717.169922\n",
      "Train Epoch: 89 [188672/225000 (84%)] Loss: 7739.222656\n",
      "Train Epoch: 89 [192768/225000 (86%)] Loss: 7844.609375\n",
      "Train Epoch: 89 [196864/225000 (87%)] Loss: 7890.283203\n",
      "Train Epoch: 89 [200960/225000 (89%)] Loss: 7750.853516\n",
      "Train Epoch: 89 [205056/225000 (91%)] Loss: 7834.529297\n",
      "Train Epoch: 89 [209152/225000 (93%)] Loss: 7706.357422\n",
      "Train Epoch: 89 [213248/225000 (95%)] Loss: 7576.773438\n",
      "Train Epoch: 89 [217344/225000 (97%)] Loss: 7889.707031\n",
      "Train Epoch: 89 [221440/225000 (98%)] Loss: 7685.048828\n",
      "    epoch          : 89\n",
      "    loss           : 7717.9895922212745\n",
      "    val_loss       : 7706.5282317625015\n",
      "Train Epoch: 90 [256/225000 (0%)] Loss: 7651.486328\n",
      "Train Epoch: 90 [4352/225000 (2%)] Loss: 7896.078125\n",
      "Train Epoch: 90 [8448/225000 (4%)] Loss: 7804.486328\n",
      "Train Epoch: 90 [12544/225000 (6%)] Loss: 7654.339844\n",
      "Train Epoch: 90 [16640/225000 (7%)] Loss: 7637.072266\n",
      "Train Epoch: 90 [20736/225000 (9%)] Loss: 7751.453125\n",
      "Train Epoch: 90 [24832/225000 (11%)] Loss: 7937.521484\n",
      "Train Epoch: 90 [28928/225000 (13%)] Loss: 7727.736328\n",
      "Train Epoch: 90 [33024/225000 (15%)] Loss: 7571.232422\n",
      "Train Epoch: 90 [37120/225000 (16%)] Loss: 7644.507812\n",
      "Train Epoch: 90 [41216/225000 (18%)] Loss: 7747.183594\n",
      "Train Epoch: 90 [45312/225000 (20%)] Loss: 7897.197266\n",
      "Train Epoch: 90 [49408/225000 (22%)] Loss: 7900.873047\n",
      "Train Epoch: 90 [53504/225000 (24%)] Loss: 7676.339844\n",
      "Train Epoch: 90 [57600/225000 (26%)] Loss: 7802.414062\n",
      "Train Epoch: 90 [61696/225000 (27%)] Loss: 7816.925781\n",
      "Train Epoch: 90 [65792/225000 (29%)] Loss: 7602.207031\n",
      "Train Epoch: 90 [69888/225000 (31%)] Loss: 7739.789062\n",
      "Train Epoch: 90 [73984/225000 (33%)] Loss: 7674.587891\n",
      "Train Epoch: 90 [78080/225000 (35%)] Loss: 7720.390625\n",
      "Train Epoch: 90 [82176/225000 (37%)] Loss: 7555.423828\n",
      "Train Epoch: 90 [86272/225000 (38%)] Loss: 7740.689453\n",
      "Train Epoch: 90 [90368/225000 (40%)] Loss: 7812.781250\n",
      "Train Epoch: 90 [94464/225000 (42%)] Loss: 7783.662109\n",
      "Train Epoch: 90 [98560/225000 (44%)] Loss: 7587.230469\n",
      "Train Epoch: 90 [102656/225000 (46%)] Loss: 7580.294922\n",
      "Train Epoch: 90 [106752/225000 (47%)] Loss: 7631.746094\n",
      "Train Epoch: 90 [110848/225000 (49%)] Loss: 7612.916016\n",
      "Train Epoch: 90 [114944/225000 (51%)] Loss: 7729.082031\n",
      "Train Epoch: 90 [119040/225000 (53%)] Loss: 7899.035156\n",
      "Train Epoch: 90 [123136/225000 (55%)] Loss: 7771.687500\n",
      "Train Epoch: 90 [127232/225000 (57%)] Loss: 7541.537109\n",
      "Train Epoch: 90 [131328/225000 (58%)] Loss: 7571.001953\n",
      "Train Epoch: 90 [135424/225000 (60%)] Loss: 7684.875000\n",
      "Train Epoch: 90 [139520/225000 (62%)] Loss: 7644.640625\n",
      "Train Epoch: 90 [143616/225000 (64%)] Loss: 7610.183594\n",
      "Train Epoch: 90 [147712/225000 (66%)] Loss: 7742.646484\n",
      "Train Epoch: 90 [151808/225000 (67%)] Loss: 7616.300781\n",
      "Train Epoch: 90 [155904/225000 (69%)] Loss: 7838.404297\n",
      "Train Epoch: 90 [160000/225000 (71%)] Loss: 7591.103516\n",
      "Train Epoch: 90 [164096/225000 (73%)] Loss: 7672.757812\n",
      "Train Epoch: 90 [168192/225000 (75%)] Loss: 7844.623047\n",
      "Train Epoch: 90 [172288/225000 (77%)] Loss: 7645.142578\n",
      "Train Epoch: 90 [176384/225000 (78%)] Loss: 7650.464844\n",
      "Train Epoch: 90 [180480/225000 (80%)] Loss: 7711.314453\n",
      "Train Epoch: 90 [184576/225000 (82%)] Loss: 7653.794922\n",
      "Train Epoch: 90 [188672/225000 (84%)] Loss: 7923.554688\n",
      "Train Epoch: 90 [192768/225000 (86%)] Loss: 7643.960938\n",
      "Train Epoch: 90 [196864/225000 (87%)] Loss: 7783.494141\n",
      "Train Epoch: 90 [200960/225000 (89%)] Loss: 7596.384766\n",
      "Train Epoch: 90 [205056/225000 (91%)] Loss: 7849.943359\n",
      "Train Epoch: 90 [209152/225000 (93%)] Loss: 7666.046875\n",
      "Train Epoch: 90 [213248/225000 (95%)] Loss: 7574.746094\n",
      "Train Epoch: 90 [217344/225000 (97%)] Loss: 7830.978516\n",
      "Train Epoch: 90 [221440/225000 (98%)] Loss: 7754.236328\n",
      "    epoch          : 90\n",
      "    loss           : 7711.5024730695395\n",
      "    val_loss       : 7710.7070307804615\n",
      "Train Epoch: 91 [256/225000 (0%)] Loss: 7597.781250\n",
      "Train Epoch: 91 [4352/225000 (2%)] Loss: 7612.601562\n",
      "Train Epoch: 91 [8448/225000 (4%)] Loss: 7558.109375\n",
      "Train Epoch: 91 [12544/225000 (6%)] Loss: 7683.294922\n",
      "Train Epoch: 91 [16640/225000 (7%)] Loss: 7735.074219\n",
      "Train Epoch: 91 [20736/225000 (9%)] Loss: 7733.757812\n",
      "Train Epoch: 91 [24832/225000 (11%)] Loss: 7846.357422\n",
      "Train Epoch: 91 [28928/225000 (13%)] Loss: 7592.927734\n",
      "Train Epoch: 91 [33024/225000 (15%)] Loss: 7720.009766\n",
      "Train Epoch: 91 [37120/225000 (16%)] Loss: 7681.662109\n",
      "Train Epoch: 91 [41216/225000 (18%)] Loss: 7736.289062\n",
      "Train Epoch: 91 [45312/225000 (20%)] Loss: 7719.482422\n",
      "Train Epoch: 91 [49408/225000 (22%)] Loss: 7709.363281\n",
      "Train Epoch: 91 [53504/225000 (24%)] Loss: 7774.156250\n",
      "Train Epoch: 91 [57600/225000 (26%)] Loss: 7527.166016\n",
      "Train Epoch: 91 [61696/225000 (27%)] Loss: 7660.113281\n",
      "Train Epoch: 91 [65792/225000 (29%)] Loss: 7658.068359\n",
      "Train Epoch: 91 [69888/225000 (31%)] Loss: 7605.111328\n",
      "Train Epoch: 91 [73984/225000 (33%)] Loss: 7565.130859\n",
      "Train Epoch: 91 [78080/225000 (35%)] Loss: 7435.171875\n",
      "Train Epoch: 91 [82176/225000 (37%)] Loss: 7924.224609\n",
      "Train Epoch: 91 [86272/225000 (38%)] Loss: 7705.968750\n",
      "Train Epoch: 91 [90368/225000 (40%)] Loss: 7769.648438\n",
      "Train Epoch: 91 [94464/225000 (42%)] Loss: 7694.847656\n",
      "Train Epoch: 91 [98560/225000 (44%)] Loss: 7910.291016\n",
      "Train Epoch: 91 [102656/225000 (46%)] Loss: 7852.855469\n",
      "Train Epoch: 91 [106752/225000 (47%)] Loss: 7716.177734\n",
      "Train Epoch: 91 [110848/225000 (49%)] Loss: 7649.531250\n",
      "Train Epoch: 91 [114944/225000 (51%)] Loss: 7630.701172\n",
      "Train Epoch: 91 [119040/225000 (53%)] Loss: 7608.748047\n",
      "Train Epoch: 91 [123136/225000 (55%)] Loss: 7600.052734\n",
      "Train Epoch: 91 [127232/225000 (57%)] Loss: 7692.792969\n",
      "Train Epoch: 91 [131328/225000 (58%)] Loss: 7971.214844\n",
      "Train Epoch: 91 [135424/225000 (60%)] Loss: 7524.617188\n",
      "Train Epoch: 91 [139520/225000 (62%)] Loss: 7682.429688\n",
      "Train Epoch: 91 [143616/225000 (64%)] Loss: 7619.339844\n",
      "Train Epoch: 91 [147712/225000 (66%)] Loss: 7698.148438\n",
      "Train Epoch: 91 [151808/225000 (67%)] Loss: 7715.966797\n",
      "Train Epoch: 91 [155904/225000 (69%)] Loss: 7804.798828\n",
      "Train Epoch: 91 [160000/225000 (71%)] Loss: 7806.105469\n",
      "Train Epoch: 91 [164096/225000 (73%)] Loss: 7917.138672\n",
      "Train Epoch: 91 [168192/225000 (75%)] Loss: 7611.074219\n",
      "Train Epoch: 91 [172288/225000 (77%)] Loss: 7717.699219\n",
      "Train Epoch: 91 [176384/225000 (78%)] Loss: 7525.742188\n",
      "Train Epoch: 91 [180480/225000 (80%)] Loss: 7755.435547\n",
      "Train Epoch: 91 [184576/225000 (82%)] Loss: 7691.816406\n",
      "Train Epoch: 91 [188672/225000 (84%)] Loss: 17387.630859\n",
      "Train Epoch: 91 [192768/225000 (86%)] Loss: 7886.867188\n",
      "Train Epoch: 91 [196864/225000 (87%)] Loss: 7631.363281\n",
      "Train Epoch: 91 [200960/225000 (89%)] Loss: 7782.525391\n",
      "Train Epoch: 91 [205056/225000 (91%)] Loss: 7607.029297\n",
      "Train Epoch: 91 [209152/225000 (93%)] Loss: 7673.119141\n",
      "Train Epoch: 91 [213248/225000 (95%)] Loss: 7615.263672\n",
      "Train Epoch: 91 [217344/225000 (97%)] Loss: 7627.048828\n",
      "Train Epoch: 91 [221440/225000 (98%)] Loss: 7696.970703\n",
      "    epoch          : 91\n",
      "    loss           : 7736.185412444895\n",
      "    val_loss       : 7678.677896682097\n",
      "Train Epoch: 92 [256/225000 (0%)] Loss: 7719.550781\n",
      "Train Epoch: 92 [4352/225000 (2%)] Loss: 7790.080078\n",
      "Train Epoch: 92 [8448/225000 (4%)] Loss: 7658.511719\n",
      "Train Epoch: 92 [12544/225000 (6%)] Loss: 7663.101562\n",
      "Train Epoch: 92 [16640/225000 (7%)] Loss: 7594.771484\n",
      "Train Epoch: 92 [20736/225000 (9%)] Loss: 7568.244141\n",
      "Train Epoch: 92 [24832/225000 (11%)] Loss: 7599.720703\n",
      "Train Epoch: 92 [28928/225000 (13%)] Loss: 7628.382812\n",
      "Train Epoch: 92 [33024/225000 (15%)] Loss: 7862.337891\n",
      "Train Epoch: 92 [37120/225000 (16%)] Loss: 7810.935547\n",
      "Train Epoch: 92 [41216/225000 (18%)] Loss: 7550.855469\n",
      "Train Epoch: 92 [45312/225000 (20%)] Loss: 7614.873047\n",
      "Train Epoch: 92 [49408/225000 (22%)] Loss: 7745.498047\n",
      "Train Epoch: 92 [53504/225000 (24%)] Loss: 7650.890625\n",
      "Train Epoch: 92 [57600/225000 (26%)] Loss: 7716.718750\n",
      "Train Epoch: 92 [61696/225000 (27%)] Loss: 7647.966797\n",
      "Train Epoch: 92 [65792/225000 (29%)] Loss: 7869.169922\n",
      "Train Epoch: 92 [69888/225000 (31%)] Loss: 7733.671875\n",
      "Train Epoch: 92 [73984/225000 (33%)] Loss: 7786.634766\n",
      "Train Epoch: 92 [78080/225000 (35%)] Loss: 7804.851562\n",
      "Train Epoch: 92 [82176/225000 (37%)] Loss: 7671.884766\n",
      "Train Epoch: 92 [86272/225000 (38%)] Loss: 7607.962891\n",
      "Train Epoch: 92 [90368/225000 (40%)] Loss: 7812.458984\n",
      "Train Epoch: 92 [94464/225000 (42%)] Loss: 7681.750000\n",
      "Train Epoch: 92 [98560/225000 (44%)] Loss: 7768.638672\n",
      "Train Epoch: 92 [102656/225000 (46%)] Loss: 7719.621094\n",
      "Train Epoch: 92 [106752/225000 (47%)] Loss: 7634.488281\n",
      "Train Epoch: 92 [110848/225000 (49%)] Loss: 7757.591797\n",
      "Train Epoch: 92 [114944/225000 (51%)] Loss: 7820.121094\n",
      "Train Epoch: 92 [119040/225000 (53%)] Loss: 7825.613281\n",
      "Train Epoch: 92 [123136/225000 (55%)] Loss: 7794.894531\n",
      "Train Epoch: 92 [127232/225000 (57%)] Loss: 7902.386719\n",
      "Train Epoch: 92 [131328/225000 (58%)] Loss: 7623.744141\n",
      "Train Epoch: 92 [135424/225000 (60%)] Loss: 7732.824219\n",
      "Train Epoch: 92 [139520/225000 (62%)] Loss: 7727.269531\n",
      "Train Epoch: 92 [143616/225000 (64%)] Loss: 7800.882812\n",
      "Train Epoch: 92 [147712/225000 (66%)] Loss: 7891.839844\n",
      "Train Epoch: 92 [151808/225000 (67%)] Loss: 7811.304688\n",
      "Train Epoch: 92 [155904/225000 (69%)] Loss: 7672.675781\n",
      "Train Epoch: 92 [160000/225000 (71%)] Loss: 7903.277344\n",
      "Train Epoch: 92 [164096/225000 (73%)] Loss: 7595.878906\n",
      "Train Epoch: 92 [168192/225000 (75%)] Loss: 7608.052734\n",
      "Train Epoch: 92 [172288/225000 (77%)] Loss: 7733.896484\n",
      "Train Epoch: 92 [176384/225000 (78%)] Loss: 7595.759766\n",
      "Train Epoch: 92 [180480/225000 (80%)] Loss: 7610.541016\n",
      "Train Epoch: 92 [184576/225000 (82%)] Loss: 7496.677734\n",
      "Train Epoch: 92 [188672/225000 (84%)] Loss: 7773.867188\n",
      "Train Epoch: 92 [192768/225000 (86%)] Loss: 7719.472656\n",
      "Train Epoch: 92 [196864/225000 (87%)] Loss: 7860.865234\n",
      "Train Epoch: 92 [200960/225000 (89%)] Loss: 7621.253906\n",
      "Train Epoch: 92 [205056/225000 (91%)] Loss: 7637.607422\n",
      "Train Epoch: 92 [209152/225000 (93%)] Loss: 7753.691406\n",
      "Train Epoch: 92 [213248/225000 (95%)] Loss: 7693.125000\n",
      "Train Epoch: 92 [217344/225000 (97%)] Loss: 7699.000000\n",
      "Train Epoch: 92 [221440/225000 (98%)] Loss: 7680.925781\n",
      "    epoch          : 92\n",
      "    loss           : 7732.880916035623\n",
      "    val_loss       : 7683.549130191608\n",
      "Train Epoch: 93 [256/225000 (0%)] Loss: 7588.693359\n",
      "Train Epoch: 93 [4352/225000 (2%)] Loss: 7607.128906\n",
      "Train Epoch: 93 [8448/225000 (4%)] Loss: 7727.402344\n",
      "Train Epoch: 93 [12544/225000 (6%)] Loss: 7620.085938\n",
      "Train Epoch: 93 [16640/225000 (7%)] Loss: 7711.232422\n",
      "Train Epoch: 93 [20736/225000 (9%)] Loss: 7617.798828\n",
      "Train Epoch: 93 [24832/225000 (11%)] Loss: 7532.113281\n",
      "Train Epoch: 93 [28928/225000 (13%)] Loss: 7759.951172\n",
      "Train Epoch: 93 [33024/225000 (15%)] Loss: 7497.572266\n",
      "Train Epoch: 93 [37120/225000 (16%)] Loss: 7635.382812\n",
      "Train Epoch: 93 [41216/225000 (18%)] Loss: 7575.677734\n",
      "Train Epoch: 93 [45312/225000 (20%)] Loss: 7657.800781\n",
      "Train Epoch: 93 [49408/225000 (22%)] Loss: 7773.431641\n",
      "Train Epoch: 93 [53504/225000 (24%)] Loss: 7855.826172\n",
      "Train Epoch: 93 [57600/225000 (26%)] Loss: 7683.160156\n",
      "Train Epoch: 93 [61696/225000 (27%)] Loss: 7607.441406\n",
      "Train Epoch: 93 [65792/225000 (29%)] Loss: 7792.193359\n",
      "Train Epoch: 93 [69888/225000 (31%)] Loss: 7801.144531\n",
      "Train Epoch: 93 [73984/225000 (33%)] Loss: 7841.589844\n",
      "Train Epoch: 93 [78080/225000 (35%)] Loss: 7733.527344\n",
      "Train Epoch: 93 [82176/225000 (37%)] Loss: 7623.751953\n",
      "Train Epoch: 93 [86272/225000 (38%)] Loss: 7675.716797\n",
      "Train Epoch: 93 [90368/225000 (40%)] Loss: 7788.330078\n",
      "Train Epoch: 93 [94464/225000 (42%)] Loss: 7664.519531\n",
      "Train Epoch: 93 [98560/225000 (44%)] Loss: 7760.373047\n",
      "Train Epoch: 93 [102656/225000 (46%)] Loss: 7678.304688\n",
      "Train Epoch: 93 [106752/225000 (47%)] Loss: 7474.859375\n",
      "Train Epoch: 93 [110848/225000 (49%)] Loss: 7709.402344\n",
      "Train Epoch: 93 [114944/225000 (51%)] Loss: 7784.613281\n",
      "Train Epoch: 93 [119040/225000 (53%)] Loss: 7622.544922\n",
      "Train Epoch: 93 [123136/225000 (55%)] Loss: 7623.958984\n",
      "Train Epoch: 93 [127232/225000 (57%)] Loss: 7772.197266\n",
      "Train Epoch: 93 [131328/225000 (58%)] Loss: 7831.390625\n",
      "Train Epoch: 93 [135424/225000 (60%)] Loss: 7759.955078\n",
      "Train Epoch: 93 [139520/225000 (62%)] Loss: 7553.082031\n",
      "Train Epoch: 93 [143616/225000 (64%)] Loss: 7671.210938\n",
      "Train Epoch: 93 [147712/225000 (66%)] Loss: 7575.185547\n",
      "Train Epoch: 93 [151808/225000 (67%)] Loss: 7664.849609\n",
      "Train Epoch: 93 [155904/225000 (69%)] Loss: 7394.236328\n",
      "Train Epoch: 93 [160000/225000 (71%)] Loss: 7535.521484\n",
      "Train Epoch: 93 [164096/225000 (73%)] Loss: 7484.951172\n",
      "Train Epoch: 93 [168192/225000 (75%)] Loss: 7618.046875\n",
      "Train Epoch: 93 [172288/225000 (77%)] Loss: 7742.396484\n",
      "Train Epoch: 93 [176384/225000 (78%)] Loss: 7598.855469\n",
      "Train Epoch: 93 [180480/225000 (80%)] Loss: 7436.746094\n",
      "Train Epoch: 93 [184576/225000 (82%)] Loss: 7625.841797\n",
      "Train Epoch: 93 [188672/225000 (84%)] Loss: 7758.345703\n",
      "Train Epoch: 93 [192768/225000 (86%)] Loss: 7603.027344\n",
      "Train Epoch: 93 [196864/225000 (87%)] Loss: 7632.044922\n",
      "Train Epoch: 93 [200960/225000 (89%)] Loss: 7589.957031\n",
      "Train Epoch: 93 [205056/225000 (91%)] Loss: 7638.142578\n",
      "Train Epoch: 93 [209152/225000 (93%)] Loss: 7644.324219\n",
      "Train Epoch: 93 [213248/225000 (95%)] Loss: 7632.775391\n",
      "Train Epoch: 93 [217344/225000 (97%)] Loss: 7684.822266\n",
      "Train Epoch: 93 [221440/225000 (98%)] Loss: 7573.501953\n",
      "    epoch          : 93\n",
      "    loss           : 7713.5277026006115\n",
      "    val_loss       : 7663.601886148355\n",
      "Train Epoch: 94 [256/225000 (0%)] Loss: 7565.892578\n",
      "Train Epoch: 94 [4352/225000 (2%)] Loss: 7713.707031\n",
      "Train Epoch: 94 [8448/225000 (4%)] Loss: 7672.779297\n",
      "Train Epoch: 94 [12544/225000 (6%)] Loss: 7648.228516\n",
      "Train Epoch: 94 [16640/225000 (7%)] Loss: 7639.533203\n",
      "Train Epoch: 94 [20736/225000 (9%)] Loss: 7748.623047\n",
      "Train Epoch: 94 [24832/225000 (11%)] Loss: 7481.763672\n",
      "Train Epoch: 94 [28928/225000 (13%)] Loss: 7710.685547\n",
      "Train Epoch: 94 [33024/225000 (15%)] Loss: 7592.142578\n",
      "Train Epoch: 94 [37120/225000 (16%)] Loss: 7671.777344\n",
      "Train Epoch: 94 [41216/225000 (18%)] Loss: 7681.000000\n",
      "Train Epoch: 94 [45312/225000 (20%)] Loss: 7647.603516\n",
      "Train Epoch: 94 [49408/225000 (22%)] Loss: 7769.582031\n",
      "Train Epoch: 94 [53504/225000 (24%)] Loss: 7593.363281\n",
      "Train Epoch: 94 [57600/225000 (26%)] Loss: 7571.000000\n",
      "Train Epoch: 94 [61696/225000 (27%)] Loss: 7971.037109\n",
      "Train Epoch: 94 [65792/225000 (29%)] Loss: 7705.134766\n",
      "Train Epoch: 94 [69888/225000 (31%)] Loss: 7636.333984\n",
      "Train Epoch: 94 [73984/225000 (33%)] Loss: 7776.181641\n",
      "Train Epoch: 94 [78080/225000 (35%)] Loss: 7685.224609\n",
      "Train Epoch: 94 [82176/225000 (37%)] Loss: 7676.304688\n",
      "Train Epoch: 94 [86272/225000 (38%)] Loss: 7626.044922\n",
      "Train Epoch: 94 [90368/225000 (40%)] Loss: 7633.429688\n",
      "Train Epoch: 94 [94464/225000 (42%)] Loss: 7531.507812\n",
      "Train Epoch: 94 [98560/225000 (44%)] Loss: 7525.933594\n",
      "Train Epoch: 94 [102656/225000 (46%)] Loss: 7490.802734\n",
      "Train Epoch: 94 [106752/225000 (47%)] Loss: 7636.722656\n",
      "Train Epoch: 94 [110848/225000 (49%)] Loss: 7597.767578\n",
      "Train Epoch: 94 [114944/225000 (51%)] Loss: 7704.046875\n",
      "Train Epoch: 94 [119040/225000 (53%)] Loss: 7814.890625\n",
      "Train Epoch: 94 [123136/225000 (55%)] Loss: 7678.056641\n",
      "Train Epoch: 94 [127232/225000 (57%)] Loss: 7719.830078\n",
      "Train Epoch: 94 [131328/225000 (58%)] Loss: 7789.355469\n",
      "Train Epoch: 94 [135424/225000 (60%)] Loss: 7633.703125\n",
      "Train Epoch: 94 [139520/225000 (62%)] Loss: 7811.951172\n",
      "Train Epoch: 94 [143616/225000 (64%)] Loss: 7850.529297\n",
      "Train Epoch: 94 [147712/225000 (66%)] Loss: 7752.681641\n",
      "Train Epoch: 94 [151808/225000 (67%)] Loss: 7708.339844\n",
      "Train Epoch: 94 [155904/225000 (69%)] Loss: 7518.230469\n",
      "Train Epoch: 94 [160000/225000 (71%)] Loss: 7791.167969\n",
      "Train Epoch: 94 [164096/225000 (73%)] Loss: 7614.732422\n",
      "Train Epoch: 94 [168192/225000 (75%)] Loss: 7718.437500\n",
      "Train Epoch: 94 [172288/225000 (77%)] Loss: 7629.716797\n",
      "Train Epoch: 94 [176384/225000 (78%)] Loss: 7816.628906\n",
      "Train Epoch: 94 [180480/225000 (80%)] Loss: 7580.419922\n",
      "Train Epoch: 94 [184576/225000 (82%)] Loss: 7660.781250\n",
      "Train Epoch: 94 [188672/225000 (84%)] Loss: 7692.154297\n",
      "Train Epoch: 94 [192768/225000 (86%)] Loss: 7617.236328\n",
      "Train Epoch: 94 [196864/225000 (87%)] Loss: 7751.640625\n",
      "Train Epoch: 94 [200960/225000 (89%)] Loss: 7713.089844\n",
      "Train Epoch: 94 [205056/225000 (91%)] Loss: 7645.275391\n",
      "Train Epoch: 94 [209152/225000 (93%)] Loss: 7835.027344\n",
      "Train Epoch: 94 [213248/225000 (95%)] Loss: 7724.064453\n",
      "Train Epoch: 94 [217344/225000 (97%)] Loss: 7778.947266\n",
      "Train Epoch: 94 [221440/225000 (98%)] Loss: 7787.607422\n",
      "    epoch          : 94\n",
      "    loss           : 7712.348208413325\n",
      "    val_loss       : 7669.3504626519825\n",
      "Train Epoch: 95 [256/225000 (0%)] Loss: 7760.667969\n",
      "Train Epoch: 95 [4352/225000 (2%)] Loss: 7614.361328\n",
      "Train Epoch: 95 [8448/225000 (4%)] Loss: 7673.351562\n",
      "Train Epoch: 95 [12544/225000 (6%)] Loss: 7649.464844\n",
      "Train Epoch: 95 [16640/225000 (7%)] Loss: 7658.273438\n",
      "Train Epoch: 95 [20736/225000 (9%)] Loss: 7578.859375\n",
      "Train Epoch: 95 [24832/225000 (11%)] Loss: 7625.380859\n",
      "Train Epoch: 95 [28928/225000 (13%)] Loss: 7816.267578\n",
      "Train Epoch: 95 [33024/225000 (15%)] Loss: 7864.697266\n",
      "Train Epoch: 95 [37120/225000 (16%)] Loss: 7752.582031\n",
      "Train Epoch: 95 [41216/225000 (18%)] Loss: 7782.964844\n",
      "Train Epoch: 95 [45312/225000 (20%)] Loss: 7773.867188\n",
      "Train Epoch: 95 [49408/225000 (22%)] Loss: 7681.982422\n",
      "Train Epoch: 95 [53504/225000 (24%)] Loss: 7713.796875\n",
      "Train Epoch: 95 [57600/225000 (26%)] Loss: 7837.916016\n",
      "Train Epoch: 95 [61696/225000 (27%)] Loss: 7673.636719\n",
      "Train Epoch: 95 [65792/225000 (29%)] Loss: 7602.812500\n",
      "Train Epoch: 95 [69888/225000 (31%)] Loss: 7757.029297\n",
      "Train Epoch: 95 [73984/225000 (33%)] Loss: 7657.201172\n",
      "Train Epoch: 95 [78080/225000 (35%)] Loss: 7638.216797\n",
      "Train Epoch: 95 [82176/225000 (37%)] Loss: 7636.777344\n",
      "Train Epoch: 95 [86272/225000 (38%)] Loss: 7574.357422\n",
      "Train Epoch: 95 [90368/225000 (40%)] Loss: 7683.882812\n",
      "Train Epoch: 95 [94464/225000 (42%)] Loss: 7589.140625\n",
      "Train Epoch: 95 [98560/225000 (44%)] Loss: 7687.687500\n",
      "Train Epoch: 95 [102656/225000 (46%)] Loss: 7626.515625\n",
      "Train Epoch: 95 [106752/225000 (47%)] Loss: 7737.197266\n",
      "Train Epoch: 95 [110848/225000 (49%)] Loss: 7549.998047\n",
      "Train Epoch: 95 [114944/225000 (51%)] Loss: 7629.078125\n",
      "Train Epoch: 95 [119040/225000 (53%)] Loss: 7582.144531\n",
      "Train Epoch: 95 [123136/225000 (55%)] Loss: 7833.292969\n",
      "Train Epoch: 95 [127232/225000 (57%)] Loss: 7625.267578\n",
      "Train Epoch: 95 [131328/225000 (58%)] Loss: 7714.417969\n",
      "Train Epoch: 95 [135424/225000 (60%)] Loss: 7583.023438\n",
      "Train Epoch: 95 [139520/225000 (62%)] Loss: 7711.230469\n",
      "Train Epoch: 95 [143616/225000 (64%)] Loss: 7611.347656\n",
      "Train Epoch: 95 [147712/225000 (66%)] Loss: 7753.937500\n",
      "Train Epoch: 95 [151808/225000 (67%)] Loss: 7802.947266\n",
      "Train Epoch: 95 [155904/225000 (69%)] Loss: 7869.666016\n",
      "Train Epoch: 95 [160000/225000 (71%)] Loss: 7795.412109\n",
      "Train Epoch: 95 [164096/225000 (73%)] Loss: 7635.236328\n",
      "Train Epoch: 95 [168192/225000 (75%)] Loss: 7581.384766\n",
      "Train Epoch: 95 [172288/225000 (77%)] Loss: 7759.908203\n",
      "Train Epoch: 95 [176384/225000 (78%)] Loss: 7555.570312\n",
      "Train Epoch: 95 [180480/225000 (80%)] Loss: 7610.804688\n",
      "Train Epoch: 95 [184576/225000 (82%)] Loss: 7711.146484\n",
      "Train Epoch: 95 [188672/225000 (84%)] Loss: 7716.138672\n",
      "Train Epoch: 95 [192768/225000 (86%)] Loss: 7901.578125\n",
      "Train Epoch: 95 [196864/225000 (87%)] Loss: 7666.066406\n",
      "Train Epoch: 95 [200960/225000 (89%)] Loss: 7603.691406\n",
      "Train Epoch: 95 [205056/225000 (91%)] Loss: 7733.009766\n",
      "Train Epoch: 95 [209152/225000 (93%)] Loss: 7721.091797\n",
      "Train Epoch: 95 [213248/225000 (95%)] Loss: 7641.548828\n",
      "Train Epoch: 95 [217344/225000 (97%)] Loss: 7682.656250\n",
      "Train Epoch: 95 [221440/225000 (98%)] Loss: 7735.937500\n",
      "    epoch          : 95\n",
      "    loss           : 7670.827910578427\n",
      "    val_loss       : 7649.5987097584475\n",
      "Train Epoch: 96 [256/225000 (0%)] Loss: 7727.726562\n",
      "Train Epoch: 96 [4352/225000 (2%)] Loss: 7623.542969\n",
      "Train Epoch: 96 [8448/225000 (4%)] Loss: 7624.447266\n",
      "Train Epoch: 96 [12544/225000 (6%)] Loss: 7588.488281\n",
      "Train Epoch: 96 [16640/225000 (7%)] Loss: 7610.675781\n",
      "Train Epoch: 96 [20736/225000 (9%)] Loss: 7683.742188\n",
      "Train Epoch: 96 [24832/225000 (11%)] Loss: 7642.953125\n",
      "Train Epoch: 96 [28928/225000 (13%)] Loss: 7822.529297\n",
      "Train Epoch: 96 [33024/225000 (15%)] Loss: 7551.167969\n",
      "Train Epoch: 96 [37120/225000 (16%)] Loss: 7644.292969\n",
      "Train Epoch: 96 [41216/225000 (18%)] Loss: 7712.101562\n",
      "Train Epoch: 96 [45312/225000 (20%)] Loss: 7539.578125\n",
      "Train Epoch: 96 [49408/225000 (22%)] Loss: 7637.207031\n",
      "Train Epoch: 96 [53504/225000 (24%)] Loss: 7503.943359\n",
      "Train Epoch: 96 [57600/225000 (26%)] Loss: 7568.380859\n",
      "Train Epoch: 96 [61696/225000 (27%)] Loss: 7613.796875\n",
      "Train Epoch: 96 [65792/225000 (29%)] Loss: 7784.591797\n",
      "Train Epoch: 96 [69888/225000 (31%)] Loss: 7593.646484\n",
      "Train Epoch: 96 [73984/225000 (33%)] Loss: 7527.603516\n",
      "Train Epoch: 96 [78080/225000 (35%)] Loss: 7737.900391\n",
      "Train Epoch: 96 [82176/225000 (37%)] Loss: 7781.589844\n",
      "Train Epoch: 96 [86272/225000 (38%)] Loss: 7600.062500\n",
      "Train Epoch: 96 [90368/225000 (40%)] Loss: 7699.744141\n",
      "Train Epoch: 96 [94464/225000 (42%)] Loss: 7637.818359\n",
      "Train Epoch: 96 [98560/225000 (44%)] Loss: 7609.039062\n",
      "Train Epoch: 96 [102656/225000 (46%)] Loss: 7729.933594\n",
      "Train Epoch: 96 [106752/225000 (47%)] Loss: 7563.001953\n",
      "Train Epoch: 96 [110848/225000 (49%)] Loss: 7517.001953\n",
      "Train Epoch: 96 [114944/225000 (51%)] Loss: 7693.537109\n",
      "Train Epoch: 96 [119040/225000 (53%)] Loss: 7684.406250\n",
      "Train Epoch: 96 [123136/225000 (55%)] Loss: 7855.394531\n",
      "Train Epoch: 96 [127232/225000 (57%)] Loss: 7763.900391\n",
      "Train Epoch: 96 [131328/225000 (58%)] Loss: 7692.904297\n",
      "Train Epoch: 96 [135424/225000 (60%)] Loss: 7673.156250\n",
      "Train Epoch: 96 [139520/225000 (62%)] Loss: 7513.892578\n",
      "Train Epoch: 96 [143616/225000 (64%)] Loss: 7677.302734\n",
      "Train Epoch: 96 [147712/225000 (66%)] Loss: 7728.853516\n",
      "Train Epoch: 96 [151808/225000 (67%)] Loss: 7743.681641\n",
      "Train Epoch: 96 [155904/225000 (69%)] Loss: 7773.232422\n",
      "Train Epoch: 96 [160000/225000 (71%)] Loss: 7617.406250\n",
      "Train Epoch: 96 [164096/225000 (73%)] Loss: 7687.582031\n",
      "Train Epoch: 96 [168192/225000 (75%)] Loss: 7717.244141\n",
      "Train Epoch: 96 [172288/225000 (77%)] Loss: 7605.158203\n",
      "Train Epoch: 96 [176384/225000 (78%)] Loss: 7538.972656\n",
      "Train Epoch: 96 [180480/225000 (80%)] Loss: 7883.199219\n",
      "Train Epoch: 96 [184576/225000 (82%)] Loss: 7527.242188\n",
      "Train Epoch: 96 [188672/225000 (84%)] Loss: 7478.451172\n",
      "Train Epoch: 96 [192768/225000 (86%)] Loss: 7564.679688\n",
      "Train Epoch: 96 [196864/225000 (87%)] Loss: 7568.732422\n",
      "Train Epoch: 96 [200960/225000 (89%)] Loss: 7617.882812\n",
      "Train Epoch: 96 [205056/225000 (91%)] Loss: 7654.421875\n",
      "Train Epoch: 96 [209152/225000 (93%)] Loss: 7687.966797\n",
      "Train Epoch: 96 [213248/225000 (95%)] Loss: 7776.107422\n",
      "Train Epoch: 96 [217344/225000 (97%)] Loss: 7551.332031\n",
      "Train Epoch: 96 [221440/225000 (98%)] Loss: 7586.703125\n",
      "    epoch          : 96\n",
      "    loss           : 7663.380973807239\n",
      "    val_loss       : 7668.497417672556\n",
      "Train Epoch: 97 [256/225000 (0%)] Loss: 7728.279297\n",
      "Train Epoch: 97 [4352/225000 (2%)] Loss: 7644.376953\n",
      "Train Epoch: 97 [8448/225000 (4%)] Loss: 7676.753906\n",
      "Train Epoch: 97 [12544/225000 (6%)] Loss: 7586.322266\n",
      "Train Epoch: 97 [16640/225000 (7%)] Loss: 7700.242188\n",
      "Train Epoch: 97 [20736/225000 (9%)] Loss: 7574.941406\n",
      "Train Epoch: 97 [24832/225000 (11%)] Loss: 7741.779297\n",
      "Train Epoch: 97 [28928/225000 (13%)] Loss: 7559.976562\n",
      "Train Epoch: 97 [33024/225000 (15%)] Loss: 7740.603516\n",
      "Train Epoch: 97 [37120/225000 (16%)] Loss: 7670.976562\n",
      "Train Epoch: 97 [41216/225000 (18%)] Loss: 7596.814453\n",
      "Train Epoch: 97 [45312/225000 (20%)] Loss: 7629.781250\n",
      "Train Epoch: 97 [49408/225000 (22%)] Loss: 7679.988281\n",
      "Train Epoch: 97 [53504/225000 (24%)] Loss: 7745.544922\n",
      "Train Epoch: 97 [57600/225000 (26%)] Loss: 7800.869141\n",
      "Train Epoch: 97 [61696/225000 (27%)] Loss: 7774.105469\n",
      "Train Epoch: 97 [65792/225000 (29%)] Loss: 7769.437500\n",
      "Train Epoch: 97 [69888/225000 (31%)] Loss: 7606.353516\n",
      "Train Epoch: 97 [73984/225000 (33%)] Loss: 7650.425781\n",
      "Train Epoch: 97 [78080/225000 (35%)] Loss: 7364.720703\n",
      "Train Epoch: 97 [82176/225000 (37%)] Loss: 7770.580078\n",
      "Train Epoch: 97 [86272/225000 (38%)] Loss: 7515.392578\n",
      "Train Epoch: 97 [90368/225000 (40%)] Loss: 7632.585938\n",
      "Train Epoch: 97 [94464/225000 (42%)] Loss: 7603.019531\n",
      "Train Epoch: 97 [98560/225000 (44%)] Loss: 7902.228516\n",
      "Train Epoch: 97 [102656/225000 (46%)] Loss: 7626.451172\n",
      "Train Epoch: 97 [106752/225000 (47%)] Loss: 7508.994141\n",
      "Train Epoch: 97 [110848/225000 (49%)] Loss: 7457.222656\n",
      "Train Epoch: 97 [114944/225000 (51%)] Loss: 7487.763672\n",
      "Train Epoch: 97 [119040/225000 (53%)] Loss: 7846.250000\n",
      "Train Epoch: 97 [123136/225000 (55%)] Loss: 7591.826172\n",
      "Train Epoch: 97 [127232/225000 (57%)] Loss: 7916.666016\n",
      "Train Epoch: 97 [131328/225000 (58%)] Loss: 7697.634766\n",
      "Train Epoch: 97 [135424/225000 (60%)] Loss: 7637.363281\n",
      "Train Epoch: 97 [139520/225000 (62%)] Loss: 7631.943359\n",
      "Train Epoch: 97 [143616/225000 (64%)] Loss: 7605.746094\n",
      "Train Epoch: 97 [147712/225000 (66%)] Loss: 7707.193359\n",
      "Train Epoch: 97 [151808/225000 (67%)] Loss: 7758.035156\n",
      "Train Epoch: 97 [155904/225000 (69%)] Loss: 7563.312500\n",
      "Train Epoch: 97 [160000/225000 (71%)] Loss: 7646.638672\n",
      "Train Epoch: 97 [164096/225000 (73%)] Loss: 7512.341797\n",
      "Train Epoch: 97 [168192/225000 (75%)] Loss: 7633.785156\n",
      "Train Epoch: 97 [172288/225000 (77%)] Loss: 7690.794922\n",
      "Train Epoch: 97 [176384/225000 (78%)] Loss: 7714.617188\n",
      "Train Epoch: 97 [180480/225000 (80%)] Loss: 7648.939453\n",
      "Train Epoch: 97 [184576/225000 (82%)] Loss: 7527.695312\n",
      "Train Epoch: 97 [188672/225000 (84%)] Loss: 7745.375000\n",
      "Train Epoch: 97 [192768/225000 (86%)] Loss: 7754.349609\n",
      "Train Epoch: 97 [196864/225000 (87%)] Loss: 7584.550781\n",
      "Train Epoch: 97 [200960/225000 (89%)] Loss: 7890.603516\n",
      "Train Epoch: 97 [205056/225000 (91%)] Loss: 7698.792969\n",
      "Train Epoch: 97 [209152/225000 (93%)] Loss: 7845.771484\n",
      "Train Epoch: 97 [213248/225000 (95%)] Loss: 7550.503906\n",
      "Train Epoch: 97 [217344/225000 (97%)] Loss: 7659.433594\n",
      "Train Epoch: 97 [221440/225000 (98%)] Loss: 7594.960938\n",
      "    epoch          : 97\n",
      "    loss           : 7655.314817530574\n",
      "    val_loss       : 7634.728028856978\n",
      "Train Epoch: 98 [256/225000 (0%)] Loss: 7690.220703\n",
      "Train Epoch: 98 [4352/225000 (2%)] Loss: 7623.779297\n",
      "Train Epoch: 98 [8448/225000 (4%)] Loss: 7725.951172\n",
      "Train Epoch: 98 [12544/225000 (6%)] Loss: 7768.316406\n",
      "Train Epoch: 98 [16640/225000 (7%)] Loss: 7898.722656\n",
      "Train Epoch: 98 [20736/225000 (9%)] Loss: 7685.820312\n",
      "Train Epoch: 98 [24832/225000 (11%)] Loss: 7580.462891\n",
      "Train Epoch: 98 [28928/225000 (13%)] Loss: 7661.111328\n",
      "Train Epoch: 98 [33024/225000 (15%)] Loss: 7678.939453\n",
      "Train Epoch: 98 [37120/225000 (16%)] Loss: 7674.652344\n",
      "Train Epoch: 98 [41216/225000 (18%)] Loss: 7576.000000\n",
      "Train Epoch: 98 [45312/225000 (20%)] Loss: 7602.500000\n",
      "Train Epoch: 98 [49408/225000 (22%)] Loss: 7518.851562\n",
      "Train Epoch: 98 [53504/225000 (24%)] Loss: 7716.861328\n",
      "Train Epoch: 98 [57600/225000 (26%)] Loss: 7413.480469\n",
      "Train Epoch: 98 [61696/225000 (27%)] Loss: 7628.826172\n",
      "Train Epoch: 98 [65792/225000 (29%)] Loss: 7713.457031\n",
      "Train Epoch: 98 [69888/225000 (31%)] Loss: 7507.865234\n",
      "Train Epoch: 98 [73984/225000 (33%)] Loss: 7646.611328\n",
      "Train Epoch: 98 [78080/225000 (35%)] Loss: 7591.587891\n",
      "Train Epoch: 98 [82176/225000 (37%)] Loss: 7657.343750\n",
      "Train Epoch: 98 [86272/225000 (38%)] Loss: 7604.937500\n",
      "Train Epoch: 98 [90368/225000 (40%)] Loss: 7624.775391\n",
      "Train Epoch: 98 [94464/225000 (42%)] Loss: 7718.427734\n",
      "Train Epoch: 98 [98560/225000 (44%)] Loss: 7827.046875\n",
      "Train Epoch: 98 [102656/225000 (46%)] Loss: 7603.017578\n",
      "Train Epoch: 98 [106752/225000 (47%)] Loss: 7699.509766\n",
      "Train Epoch: 98 [110848/225000 (49%)] Loss: 7783.451172\n",
      "Train Epoch: 98 [114944/225000 (51%)] Loss: 7565.126953\n",
      "Train Epoch: 98 [119040/225000 (53%)] Loss: 7875.921875\n",
      "Train Epoch: 98 [123136/225000 (55%)] Loss: 7662.923828\n",
      "Train Epoch: 98 [127232/225000 (57%)] Loss: 7674.785156\n",
      "Train Epoch: 98 [131328/225000 (58%)] Loss: 7694.583984\n",
      "Train Epoch: 98 [135424/225000 (60%)] Loss: 7658.117188\n",
      "Train Epoch: 98 [139520/225000 (62%)] Loss: 7601.603516\n",
      "Train Epoch: 98 [143616/225000 (64%)] Loss: 7474.585938\n",
      "Train Epoch: 98 [147712/225000 (66%)] Loss: 7754.625000\n",
      "Train Epoch: 98 [151808/225000 (67%)] Loss: 7646.431641\n",
      "Train Epoch: 98 [155904/225000 (69%)] Loss: 7644.103516\n",
      "Train Epoch: 98 [160000/225000 (71%)] Loss: 7488.923828\n",
      "Train Epoch: 98 [164096/225000 (73%)] Loss: 7676.566406\n",
      "Train Epoch: 98 [168192/225000 (75%)] Loss: 7639.785156\n",
      "Train Epoch: 98 [172288/225000 (77%)] Loss: 7375.960938\n",
      "Train Epoch: 98 [176384/225000 (78%)] Loss: 7629.359375\n",
      "Train Epoch: 98 [180480/225000 (80%)] Loss: 7792.449219\n",
      "Train Epoch: 98 [184576/225000 (82%)] Loss: 7666.244141\n",
      "Train Epoch: 98 [188672/225000 (84%)] Loss: 7582.232422\n",
      "Train Epoch: 98 [192768/225000 (86%)] Loss: 7644.615234\n",
      "Train Epoch: 98 [196864/225000 (87%)] Loss: 7544.351562\n",
      "Train Epoch: 98 [200960/225000 (89%)] Loss: 7854.134766\n",
      "Train Epoch: 98 [205056/225000 (91%)] Loss: 7637.201172\n",
      "Train Epoch: 98 [209152/225000 (93%)] Loss: 7687.244141\n",
      "Train Epoch: 98 [213248/225000 (95%)] Loss: 7713.328125\n",
      "Train Epoch: 98 [217344/225000 (97%)] Loss: 7609.578125\n",
      "Train Epoch: 98 [221440/225000 (98%)] Loss: 7612.650391\n",
      "    epoch          : 98\n",
      "    loss           : 7685.93727002453\n",
      "    val_loss       : 7628.385380996125\n",
      "Train Epoch: 99 [256/225000 (0%)] Loss: 7524.750000\n",
      "Train Epoch: 99 [4352/225000 (2%)] Loss: 7627.951172\n",
      "Train Epoch: 99 [8448/225000 (4%)] Loss: 7575.154297\n",
      "Train Epoch: 99 [12544/225000 (6%)] Loss: 7564.035156\n",
      "Train Epoch: 99 [16640/225000 (7%)] Loss: 7603.728516\n",
      "Train Epoch: 99 [20736/225000 (9%)] Loss: 7610.148438\n",
      "Train Epoch: 99 [24832/225000 (11%)] Loss: 7632.503906\n",
      "Train Epoch: 99 [28928/225000 (13%)] Loss: 7547.765625\n",
      "Train Epoch: 99 [33024/225000 (15%)] Loss: 7784.724609\n",
      "Train Epoch: 99 [37120/225000 (16%)] Loss: 7613.056641\n",
      "Train Epoch: 99 [41216/225000 (18%)] Loss: 7791.119141\n",
      "Train Epoch: 99 [45312/225000 (20%)] Loss: 7718.156250\n",
      "Train Epoch: 99 [49408/225000 (22%)] Loss: 7488.689453\n",
      "Train Epoch: 99 [53504/225000 (24%)] Loss: 7554.964844\n",
      "Train Epoch: 99 [57600/225000 (26%)] Loss: 7613.863281\n",
      "Train Epoch: 99 [61696/225000 (27%)] Loss: 7391.746094\n",
      "Train Epoch: 99 [65792/225000 (29%)] Loss: 7450.619141\n",
      "Train Epoch: 99 [69888/225000 (31%)] Loss: 7551.867188\n",
      "Train Epoch: 99 [73984/225000 (33%)] Loss: 7730.853516\n",
      "Train Epoch: 99 [78080/225000 (35%)] Loss: 7794.556641\n",
      "Train Epoch: 99 [82176/225000 (37%)] Loss: 7701.634766\n",
      "Train Epoch: 99 [86272/225000 (38%)] Loss: 7914.347656\n",
      "Train Epoch: 99 [90368/225000 (40%)] Loss: 7658.833984\n",
      "Train Epoch: 99 [94464/225000 (42%)] Loss: 7662.757812\n",
      "Train Epoch: 99 [98560/225000 (44%)] Loss: 7767.750000\n",
      "Train Epoch: 99 [102656/225000 (46%)] Loss: 7726.111328\n",
      "Train Epoch: 99 [106752/225000 (47%)] Loss: 7594.304688\n",
      "Train Epoch: 99 [110848/225000 (49%)] Loss: 7550.005859\n",
      "Train Epoch: 99 [114944/225000 (51%)] Loss: 7637.236328\n",
      "Train Epoch: 99 [119040/225000 (53%)] Loss: 7626.296875\n",
      "Train Epoch: 99 [123136/225000 (55%)] Loss: 7502.496094\n",
      "Train Epoch: 99 [127232/225000 (57%)] Loss: 7748.503906\n",
      "Train Epoch: 99 [131328/225000 (58%)] Loss: 7564.984375\n",
      "Train Epoch: 99 [135424/225000 (60%)] Loss: 7704.765625\n",
      "Train Epoch: 99 [139520/225000 (62%)] Loss: 7499.791016\n",
      "Train Epoch: 99 [143616/225000 (64%)] Loss: 7693.195312\n",
      "Train Epoch: 99 [147712/225000 (66%)] Loss: 7677.378906\n",
      "Train Epoch: 99 [151808/225000 (67%)] Loss: 7588.123047\n",
      "Train Epoch: 99 [155904/225000 (69%)] Loss: 7638.158203\n",
      "Train Epoch: 99 [160000/225000 (71%)] Loss: 7798.435547\n",
      "Train Epoch: 99 [164096/225000 (73%)] Loss: 7620.402344\n",
      "Train Epoch: 99 [168192/225000 (75%)] Loss: 7676.652344\n",
      "Train Epoch: 99 [172288/225000 (77%)] Loss: 7703.058594\n",
      "Train Epoch: 99 [176384/225000 (78%)] Loss: 7507.273438\n",
      "Train Epoch: 99 [180480/225000 (80%)] Loss: 7445.474609\n",
      "Train Epoch: 99 [184576/225000 (82%)] Loss: 7819.582031\n",
      "Train Epoch: 99 [188672/225000 (84%)] Loss: 7575.011719\n",
      "Train Epoch: 99 [192768/225000 (86%)] Loss: 7604.187500\n",
      "Train Epoch: 99 [196864/225000 (87%)] Loss: 7566.125000\n",
      "Train Epoch: 99 [200960/225000 (89%)] Loss: 7596.689453\n",
      "Train Epoch: 99 [205056/225000 (91%)] Loss: 7683.230469\n",
      "Train Epoch: 99 [209152/225000 (93%)] Loss: 7498.443359\n",
      "Train Epoch: 99 [213248/225000 (95%)] Loss: 7656.814453\n",
      "Train Epoch: 99 [217344/225000 (97%)] Loss: 7757.511719\n",
      "Train Epoch: 99 [221440/225000 (98%)] Loss: 7600.753906\n",
      "    epoch          : 99\n",
      "    loss           : 7661.888237476891\n",
      "    val_loss       : 7617.490499571878\n",
      "Train Epoch: 100 [256/225000 (0%)] Loss: 7772.669922\n",
      "Train Epoch: 100 [4352/225000 (2%)] Loss: 7600.068359\n",
      "Train Epoch: 100 [8448/225000 (4%)] Loss: 7575.906250\n",
      "Train Epoch: 100 [12544/225000 (6%)] Loss: 7691.085938\n",
      "Train Epoch: 100 [16640/225000 (7%)] Loss: 7750.080078\n",
      "Train Epoch: 100 [20736/225000 (9%)] Loss: 7497.529297\n",
      "Train Epoch: 100 [24832/225000 (11%)] Loss: 7499.896484\n",
      "Train Epoch: 100 [28928/225000 (13%)] Loss: 7730.359375\n",
      "Train Epoch: 100 [33024/225000 (15%)] Loss: 7680.646484\n",
      "Train Epoch: 100 [37120/225000 (16%)] Loss: 7482.947266\n",
      "Train Epoch: 100 [41216/225000 (18%)] Loss: 7950.380859\n",
      "Train Epoch: 100 [45312/225000 (20%)] Loss: 7481.650391\n",
      "Train Epoch: 100 [49408/225000 (22%)] Loss: 7683.828125\n",
      "Train Epoch: 100 [53504/225000 (24%)] Loss: 7819.236328\n",
      "Train Epoch: 100 [57600/225000 (26%)] Loss: 7378.062500\n",
      "Train Epoch: 100 [61696/225000 (27%)] Loss: 7714.435547\n",
      "Train Epoch: 100 [65792/225000 (29%)] Loss: 7745.792969\n",
      "Train Epoch: 100 [69888/225000 (31%)] Loss: 7915.373047\n",
      "Train Epoch: 100 [73984/225000 (33%)] Loss: 7598.117188\n",
      "Train Epoch: 100 [78080/225000 (35%)] Loss: 7699.195312\n",
      "Train Epoch: 100 [82176/225000 (37%)] Loss: 7736.564453\n",
      "Train Epoch: 100 [86272/225000 (38%)] Loss: 7738.501953\n",
      "Train Epoch: 100 [90368/225000 (40%)] Loss: 7514.982422\n",
      "Train Epoch: 100 [94464/225000 (42%)] Loss: 7560.535156\n",
      "Train Epoch: 100 [98560/225000 (44%)] Loss: 7442.060547\n",
      "Train Epoch: 100 [102656/225000 (46%)] Loss: 7760.787109\n",
      "Train Epoch: 100 [106752/225000 (47%)] Loss: 7502.669922\n",
      "Train Epoch: 100 [110848/225000 (49%)] Loss: 7582.355469\n",
      "Train Epoch: 100 [114944/225000 (51%)] Loss: 7550.755859\n",
      "Train Epoch: 100 [119040/225000 (53%)] Loss: 7731.927734\n",
      "Train Epoch: 100 [123136/225000 (55%)] Loss: 7733.808594\n",
      "Train Epoch: 100 [127232/225000 (57%)] Loss: 7634.332031\n",
      "Train Epoch: 100 [131328/225000 (58%)] Loss: 7778.279297\n",
      "Train Epoch: 100 [135424/225000 (60%)] Loss: 7618.931641\n",
      "Train Epoch: 100 [139520/225000 (62%)] Loss: 7701.443359\n",
      "Train Epoch: 100 [143616/225000 (64%)] Loss: 7560.671875\n",
      "Train Epoch: 100 [147712/225000 (66%)] Loss: 7530.035156\n",
      "Train Epoch: 100 [151808/225000 (67%)] Loss: 7657.511719\n",
      "Train Epoch: 100 [155904/225000 (69%)] Loss: 7718.900391\n",
      "Train Epoch: 100 [160000/225000 (71%)] Loss: 7678.115234\n",
      "Train Epoch: 100 [164096/225000 (73%)] Loss: 7661.111328\n",
      "Train Epoch: 100 [168192/225000 (75%)] Loss: 7601.486328\n",
      "Train Epoch: 100 [172288/225000 (77%)] Loss: 7815.175781\n",
      "Train Epoch: 100 [176384/225000 (78%)] Loss: 7872.058594\n",
      "Train Epoch: 100 [180480/225000 (80%)] Loss: 7704.980469\n",
      "Train Epoch: 100 [184576/225000 (82%)] Loss: 7658.798828\n",
      "Train Epoch: 100 [188672/225000 (84%)] Loss: 7807.916016\n",
      "Train Epoch: 100 [192768/225000 (86%)] Loss: 7605.625000\n",
      "Train Epoch: 100 [196864/225000 (87%)] Loss: 7730.750000\n",
      "Train Epoch: 100 [200960/225000 (89%)] Loss: 7599.806641\n",
      "Train Epoch: 100 [205056/225000 (91%)] Loss: 7541.615234\n",
      "Train Epoch: 100 [209152/225000 (93%)] Loss: 7569.781250\n",
      "Train Epoch: 100 [213248/225000 (95%)] Loss: 7762.869141\n",
      "Train Epoch: 100 [217344/225000 (97%)] Loss: 7757.267578\n",
      "Train Epoch: 100 [221440/225000 (98%)] Loss: 7580.259766\n",
      "    epoch          : 100\n",
      "    loss           : 7658.816900641709\n",
      "    val_loss       : 7613.93193469972\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0827_225017/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 101 [256/225000 (0%)] Loss: 7644.005859\n",
      "Train Epoch: 101 [4352/225000 (2%)] Loss: 7729.130859\n",
      "Train Epoch: 101 [8448/225000 (4%)] Loss: 7766.947266\n",
      "Train Epoch: 101 [12544/225000 (6%)] Loss: 7499.544922\n",
      "Train Epoch: 101 [16640/225000 (7%)] Loss: 7698.033203\n",
      "Train Epoch: 101 [20736/225000 (9%)] Loss: 7633.685547\n",
      "Train Epoch: 101 [24832/225000 (11%)] Loss: 7576.164062\n",
      "Train Epoch: 101 [28928/225000 (13%)] Loss: 7529.683594\n",
      "Train Epoch: 101 [33024/225000 (15%)] Loss: 7702.917969\n",
      "Train Epoch: 101 [37120/225000 (16%)] Loss: 7782.984375\n",
      "Train Epoch: 101 [41216/225000 (18%)] Loss: 7717.425781\n",
      "Train Epoch: 101 [45312/225000 (20%)] Loss: 7599.808594\n",
      "Train Epoch: 101 [49408/225000 (22%)] Loss: 7643.917969\n",
      "Train Epoch: 101 [53504/225000 (24%)] Loss: 7521.746094\n",
      "Train Epoch: 101 [57600/225000 (26%)] Loss: 7485.273438\n",
      "Train Epoch: 101 [61696/225000 (27%)] Loss: 7635.205078\n",
      "Train Epoch: 101 [65792/225000 (29%)] Loss: 7581.914062\n",
      "Train Epoch: 101 [69888/225000 (31%)] Loss: 7651.351562\n",
      "Train Epoch: 101 [73984/225000 (33%)] Loss: 7574.046875\n",
      "Train Epoch: 101 [78080/225000 (35%)] Loss: 7684.785156\n",
      "Train Epoch: 101 [82176/225000 (37%)] Loss: 7581.445312\n",
      "Train Epoch: 101 [86272/225000 (38%)] Loss: 7469.140625\n",
      "Train Epoch: 101 [90368/225000 (40%)] Loss: 7688.105469\n",
      "Train Epoch: 101 [94464/225000 (42%)] Loss: 7535.542969\n",
      "Train Epoch: 101 [98560/225000 (44%)] Loss: 7618.093750\n",
      "Train Epoch: 101 [102656/225000 (46%)] Loss: 7706.181641\n",
      "Train Epoch: 101 [106752/225000 (47%)] Loss: 7737.082031\n",
      "Train Epoch: 101 [110848/225000 (49%)] Loss: 7542.876953\n",
      "Train Epoch: 101 [114944/225000 (51%)] Loss: 7546.810547\n",
      "Train Epoch: 101 [119040/225000 (53%)] Loss: 7664.824219\n",
      "Train Epoch: 101 [123136/225000 (55%)] Loss: 7685.789062\n",
      "Train Epoch: 101 [127232/225000 (57%)] Loss: 7715.718750\n",
      "Train Epoch: 101 [131328/225000 (58%)] Loss: 7531.888672\n",
      "Train Epoch: 101 [135424/225000 (60%)] Loss: 7473.398438\n",
      "Train Epoch: 101 [139520/225000 (62%)] Loss: 7608.074219\n",
      "Train Epoch: 101 [143616/225000 (64%)] Loss: 7756.029297\n",
      "Train Epoch: 101 [147712/225000 (66%)] Loss: 7687.781250\n",
      "Train Epoch: 101 [151808/225000 (67%)] Loss: 7522.964844\n",
      "Train Epoch: 101 [155904/225000 (69%)] Loss: 7604.156250\n",
      "Train Epoch: 101 [160000/225000 (71%)] Loss: 7411.056641\n",
      "Train Epoch: 101 [164096/225000 (73%)] Loss: 7674.042969\n",
      "Train Epoch: 101 [168192/225000 (75%)] Loss: 7561.312500\n",
      "Train Epoch: 101 [172288/225000 (77%)] Loss: 7805.027344\n",
      "Train Epoch: 101 [176384/225000 (78%)] Loss: 7555.050781\n",
      "Train Epoch: 101 [180480/225000 (80%)] Loss: 7621.023438\n",
      "Train Epoch: 101 [184576/225000 (82%)] Loss: 7602.355469\n",
      "Train Epoch: 101 [188672/225000 (84%)] Loss: 7711.294922\n",
      "Train Epoch: 101 [192768/225000 (86%)] Loss: 7670.521484\n",
      "Train Epoch: 101 [196864/225000 (87%)] Loss: 7650.322266\n",
      "Train Epoch: 101 [200960/225000 (89%)] Loss: 7715.619141\n",
      "Train Epoch: 101 [205056/225000 (91%)] Loss: 7439.318359\n",
      "Train Epoch: 101 [209152/225000 (93%)] Loss: 7683.587891\n",
      "Train Epoch: 101 [213248/225000 (95%)] Loss: 7634.062500\n",
      "Train Epoch: 101 [217344/225000 (97%)] Loss: 7651.763672\n",
      "Train Epoch: 101 [221440/225000 (98%)] Loss: 7761.673828\n",
      "    epoch          : 101\n",
      "    loss           : 7627.093924425839\n",
      "    val_loss       : 7617.32451317505\n",
      "Train Epoch: 102 [256/225000 (0%)] Loss: 7823.660156\n",
      "Train Epoch: 102 [4352/225000 (2%)] Loss: 7555.173828\n",
      "Train Epoch: 102 [8448/225000 (4%)] Loss: 7529.449219\n",
      "Train Epoch: 102 [12544/225000 (6%)] Loss: 7617.933594\n",
      "Train Epoch: 102 [16640/225000 (7%)] Loss: 7628.886719\n",
      "Train Epoch: 102 [20736/225000 (9%)] Loss: 7475.181641\n",
      "Train Epoch: 102 [24832/225000 (11%)] Loss: 7557.908203\n",
      "Train Epoch: 102 [28928/225000 (13%)] Loss: 7673.519531\n",
      "Train Epoch: 102 [33024/225000 (15%)] Loss: 7583.535156\n",
      "Train Epoch: 102 [37120/225000 (16%)] Loss: 7421.671875\n",
      "Train Epoch: 102 [41216/225000 (18%)] Loss: 7631.486328\n",
      "Train Epoch: 102 [45312/225000 (20%)] Loss: 7586.996094\n",
      "Train Epoch: 102 [49408/225000 (22%)] Loss: 7719.009766\n",
      "Train Epoch: 102 [53504/225000 (24%)] Loss: 7685.617188\n",
      "Train Epoch: 102 [57600/225000 (26%)] Loss: 7740.507812\n",
      "Train Epoch: 102 [61696/225000 (27%)] Loss: 7531.433594\n",
      "Train Epoch: 102 [65792/225000 (29%)] Loss: 7487.802734\n",
      "Train Epoch: 102 [69888/225000 (31%)] Loss: 7491.820312\n",
      "Train Epoch: 102 [73984/225000 (33%)] Loss: 7989.191406\n",
      "Train Epoch: 102 [78080/225000 (35%)] Loss: 7473.630859\n",
      "Train Epoch: 102 [82176/225000 (37%)] Loss: 7569.998047\n",
      "Train Epoch: 102 [86272/225000 (38%)] Loss: 7662.449219\n",
      "Train Epoch: 102 [90368/225000 (40%)] Loss: 7682.578125\n",
      "Train Epoch: 102 [94464/225000 (42%)] Loss: 7719.675781\n",
      "Train Epoch: 102 [98560/225000 (44%)] Loss: 7588.277344\n",
      "Train Epoch: 102 [102656/225000 (46%)] Loss: 7574.474609\n",
      "Train Epoch: 102 [106752/225000 (47%)] Loss: 7724.916016\n",
      "Train Epoch: 102 [110848/225000 (49%)] Loss: 7584.117188\n",
      "Train Epoch: 102 [114944/225000 (51%)] Loss: 7761.703125\n",
      "Train Epoch: 102 [119040/225000 (53%)] Loss: 7714.291016\n",
      "Train Epoch: 102 [123136/225000 (55%)] Loss: 7622.970703\n",
      "Train Epoch: 102 [127232/225000 (57%)] Loss: 7717.119141\n",
      "Train Epoch: 102 [131328/225000 (58%)] Loss: 7569.490234\n",
      "Train Epoch: 102 [135424/225000 (60%)] Loss: 7694.832031\n",
      "Train Epoch: 102 [139520/225000 (62%)] Loss: 7587.322266\n",
      "Train Epoch: 102 [143616/225000 (64%)] Loss: 7528.951172\n",
      "Train Epoch: 102 [147712/225000 (66%)] Loss: 7486.488281\n",
      "Train Epoch: 102 [151808/225000 (67%)] Loss: 7803.470703\n",
      "Train Epoch: 102 [155904/225000 (69%)] Loss: 7856.316406\n",
      "Train Epoch: 102 [160000/225000 (71%)] Loss: 7440.558594\n",
      "Train Epoch: 102 [164096/225000 (73%)] Loss: 7458.884766\n",
      "Train Epoch: 102 [168192/225000 (75%)] Loss: 7595.919922\n",
      "Train Epoch: 102 [172288/225000 (77%)] Loss: 7630.648438\n",
      "Train Epoch: 102 [176384/225000 (78%)] Loss: 7514.517578\n",
      "Train Epoch: 102 [180480/225000 (80%)] Loss: 7696.787109\n",
      "Train Epoch: 102 [184576/225000 (82%)] Loss: 7472.417969\n",
      "Train Epoch: 102 [188672/225000 (84%)] Loss: 7773.882812\n",
      "Train Epoch: 102 [192768/225000 (86%)] Loss: 7700.425781\n",
      "Train Epoch: 102 [196864/225000 (87%)] Loss: 7623.712891\n",
      "Train Epoch: 102 [200960/225000 (89%)] Loss: 7416.546875\n",
      "Train Epoch: 102 [205056/225000 (91%)] Loss: 7666.414062\n",
      "Train Epoch: 102 [209152/225000 (93%)] Loss: 7586.138672\n",
      "Train Epoch: 102 [213248/225000 (95%)] Loss: 7687.302734\n",
      "Train Epoch: 102 [217344/225000 (97%)] Loss: 7544.320312\n",
      "Train Epoch: 102 [221440/225000 (98%)] Loss: 7743.726562\n",
      "    epoch          : 102\n",
      "    loss           : 7618.350397068757\n",
      "    val_loss       : 7605.020998477936\n",
      "Train Epoch: 103 [256/225000 (0%)] Loss: 7586.310547\n",
      "Train Epoch: 103 [4352/225000 (2%)] Loss: 7784.837891\n",
      "Train Epoch: 103 [8448/225000 (4%)] Loss: 7678.166016\n",
      "Train Epoch: 103 [12544/225000 (6%)] Loss: 7773.730469\n",
      "Train Epoch: 103 [16640/225000 (7%)] Loss: 7527.771484\n",
      "Train Epoch: 103 [20736/225000 (9%)] Loss: 7553.216797\n",
      "Train Epoch: 103 [24832/225000 (11%)] Loss: 7606.677734\n",
      "Train Epoch: 103 [28928/225000 (13%)] Loss: 7788.023438\n",
      "Train Epoch: 103 [33024/225000 (15%)] Loss: 7513.697266\n",
      "Train Epoch: 103 [37120/225000 (16%)] Loss: 7521.816406\n",
      "Train Epoch: 103 [41216/225000 (18%)] Loss: 7537.412109\n",
      "Train Epoch: 103 [45312/225000 (20%)] Loss: 7634.402344\n",
      "Train Epoch: 103 [49408/225000 (22%)] Loss: 7630.585938\n",
      "Train Epoch: 103 [53504/225000 (24%)] Loss: 7642.814453\n",
      "Train Epoch: 103 [57600/225000 (26%)] Loss: 7920.017578\n",
      "Train Epoch: 103 [61696/225000 (27%)] Loss: 7765.607422\n",
      "Train Epoch: 103 [65792/225000 (29%)] Loss: 7577.599609\n",
      "Train Epoch: 103 [69888/225000 (31%)] Loss: 7520.583984\n",
      "Train Epoch: 103 [73984/225000 (33%)] Loss: 7633.062500\n",
      "Train Epoch: 103 [78080/225000 (35%)] Loss: 7634.953125\n",
      "Train Epoch: 103 [82176/225000 (37%)] Loss: 7516.697266\n",
      "Train Epoch: 103 [86272/225000 (38%)] Loss: 7578.693359\n",
      "Train Epoch: 103 [90368/225000 (40%)] Loss: 7766.021484\n",
      "Train Epoch: 103 [94464/225000 (42%)] Loss: 7596.007812\n",
      "Train Epoch: 103 [98560/225000 (44%)] Loss: 7564.314453\n",
      "Train Epoch: 103 [102656/225000 (46%)] Loss: 7638.841797\n",
      "Train Epoch: 103 [106752/225000 (47%)] Loss: 7648.755859\n",
      "Train Epoch: 103 [110848/225000 (49%)] Loss: 7477.087891\n",
      "Train Epoch: 103 [114944/225000 (51%)] Loss: 7583.777344\n",
      "Train Epoch: 103 [119040/225000 (53%)] Loss: 7612.898438\n",
      "Train Epoch: 103 [123136/225000 (55%)] Loss: 7685.941406\n",
      "Train Epoch: 103 [127232/225000 (57%)] Loss: 7604.634766\n",
      "Train Epoch: 103 [131328/225000 (58%)] Loss: 7633.978516\n",
      "Train Epoch: 103 [135424/225000 (60%)] Loss: 7791.283203\n",
      "Train Epoch: 103 [139520/225000 (62%)] Loss: 7592.791016\n",
      "Train Epoch: 103 [143616/225000 (64%)] Loss: 7655.105469\n",
      "Train Epoch: 103 [147712/225000 (66%)] Loss: 7475.332031\n",
      "Train Epoch: 103 [151808/225000 (67%)] Loss: 7476.935547\n",
      "Train Epoch: 103 [155904/225000 (69%)] Loss: 7632.240234\n",
      "Train Epoch: 103 [160000/225000 (71%)] Loss: 7551.304688\n",
      "Train Epoch: 103 [164096/225000 (73%)] Loss: 7554.789062\n",
      "Train Epoch: 103 [168192/225000 (75%)] Loss: 7653.074219\n",
      "Train Epoch: 103 [172288/225000 (77%)] Loss: 7570.441406\n",
      "Train Epoch: 103 [176384/225000 (78%)] Loss: 7551.587891\n",
      "Train Epoch: 103 [180480/225000 (80%)] Loss: 7804.800781\n",
      "Train Epoch: 103 [184576/225000 (82%)] Loss: 7620.876953\n",
      "Train Epoch: 103 [188672/225000 (84%)] Loss: 7692.210938\n",
      "Train Epoch: 103 [192768/225000 (86%)] Loss: 7517.572266\n",
      "Train Epoch: 103 [196864/225000 (87%)] Loss: 7632.097656\n",
      "Train Epoch: 103 [200960/225000 (89%)] Loss: 7442.367188\n",
      "Train Epoch: 103 [205056/225000 (91%)] Loss: 7579.033203\n",
      "Train Epoch: 103 [209152/225000 (93%)] Loss: 7628.908203\n",
      "Train Epoch: 103 [213248/225000 (95%)] Loss: 7693.052734\n",
      "Train Epoch: 103 [217344/225000 (97%)] Loss: 7569.416016\n",
      "Train Epoch: 103 [221440/225000 (98%)] Loss: 7590.675781\n",
      "    epoch          : 103\n",
      "    loss           : 7623.657202120663\n",
      "    val_loss       : 7602.903921458186\n",
      "Train Epoch: 104 [256/225000 (0%)] Loss: 7551.164062\n",
      "Train Epoch: 104 [4352/225000 (2%)] Loss: 7529.837891\n",
      "Train Epoch: 104 [8448/225000 (4%)] Loss: 7681.357422\n",
      "Train Epoch: 104 [12544/225000 (6%)] Loss: 7547.210938\n",
      "Train Epoch: 104 [16640/225000 (7%)] Loss: 7510.785156\n",
      "Train Epoch: 104 [20736/225000 (9%)] Loss: 7741.238281\n",
      "Train Epoch: 104 [24832/225000 (11%)] Loss: 7709.914062\n",
      "Train Epoch: 104 [28928/225000 (13%)] Loss: 7707.669922\n",
      "Train Epoch: 104 [33024/225000 (15%)] Loss: 7735.626953\n",
      "Train Epoch: 104 [37120/225000 (16%)] Loss: 7608.548828\n",
      "Train Epoch: 104 [41216/225000 (18%)] Loss: 7832.033203\n",
      "Train Epoch: 104 [45312/225000 (20%)] Loss: 7417.472656\n",
      "Train Epoch: 104 [49408/225000 (22%)] Loss: 7801.257812\n",
      "Train Epoch: 104 [53504/225000 (24%)] Loss: 7415.427734\n",
      "Train Epoch: 104 [57600/225000 (26%)] Loss: 7604.710938\n",
      "Train Epoch: 104 [61696/225000 (27%)] Loss: 7478.621094\n",
      "Train Epoch: 104 [65792/225000 (29%)] Loss: 7444.406250\n",
      "Train Epoch: 104 [69888/225000 (31%)] Loss: 7707.171875\n",
      "Train Epoch: 104 [73984/225000 (33%)] Loss: 7614.433594\n",
      "Train Epoch: 104 [78080/225000 (35%)] Loss: 7567.601562\n",
      "Train Epoch: 104 [82176/225000 (37%)] Loss: 7560.019531\n",
      "Train Epoch: 104 [86272/225000 (38%)] Loss: 7435.884766\n",
      "Train Epoch: 104 [90368/225000 (40%)] Loss: 7679.623047\n",
      "Train Epoch: 104 [94464/225000 (42%)] Loss: 7671.908203\n",
      "Train Epoch: 104 [98560/225000 (44%)] Loss: 7475.582031\n",
      "Train Epoch: 104 [102656/225000 (46%)] Loss: 7414.187500\n",
      "Train Epoch: 104 [106752/225000 (47%)] Loss: 7809.275391\n",
      "Train Epoch: 104 [110848/225000 (49%)] Loss: 7517.583984\n",
      "Train Epoch: 104 [114944/225000 (51%)] Loss: 7498.816406\n",
      "Train Epoch: 104 [119040/225000 (53%)] Loss: 7714.599609\n",
      "Train Epoch: 104 [123136/225000 (55%)] Loss: 7748.455078\n",
      "Train Epoch: 104 [127232/225000 (57%)] Loss: 7556.652344\n",
      "Train Epoch: 104 [131328/225000 (58%)] Loss: 7739.966797\n",
      "Train Epoch: 104 [135424/225000 (60%)] Loss: 7637.031250\n",
      "Train Epoch: 104 [139520/225000 (62%)] Loss: 7497.988281\n",
      "Train Epoch: 104 [143616/225000 (64%)] Loss: 7444.771484\n",
      "Train Epoch: 104 [147712/225000 (66%)] Loss: 7720.882812\n",
      "Train Epoch: 104 [151808/225000 (67%)] Loss: 7508.250000\n",
      "Train Epoch: 104 [155904/225000 (69%)] Loss: 7522.107422\n",
      "Train Epoch: 104 [160000/225000 (71%)] Loss: 7666.328125\n",
      "Train Epoch: 104 [164096/225000 (73%)] Loss: 7760.054688\n",
      "Train Epoch: 104 [168192/225000 (75%)] Loss: 7570.378906\n",
      "Train Epoch: 104 [172288/225000 (77%)] Loss: 7622.728516\n",
      "Train Epoch: 104 [176384/225000 (78%)] Loss: 7741.261719\n",
      "Train Epoch: 104 [180480/225000 (80%)] Loss: 7571.757812\n",
      "Train Epoch: 104 [184576/225000 (82%)] Loss: 7541.332031\n",
      "Train Epoch: 104 [188672/225000 (84%)] Loss: 7604.494141\n",
      "Train Epoch: 104 [192768/225000 (86%)] Loss: 7476.023438\n",
      "Train Epoch: 104 [196864/225000 (87%)] Loss: 7524.714844\n",
      "Train Epoch: 104 [200960/225000 (89%)] Loss: 7462.550781\n",
      "Train Epoch: 104 [205056/225000 (91%)] Loss: 7587.730469\n",
      "Train Epoch: 104 [209152/225000 (93%)] Loss: 7511.531250\n",
      "Train Epoch: 104 [213248/225000 (95%)] Loss: 7618.626953\n",
      "Train Epoch: 104 [217344/225000 (97%)] Loss: 7499.544922\n",
      "Train Epoch: 104 [221440/225000 (98%)] Loss: 7663.673828\n",
      "    epoch          : 104\n",
      "    loss           : 7642.414316917307\n",
      "    val_loss       : 7662.206950648707\n",
      "Train Epoch: 105 [256/225000 (0%)] Loss: 7622.251953\n",
      "Train Epoch: 105 [4352/225000 (2%)] Loss: 7527.947266\n",
      "Train Epoch: 105 [8448/225000 (4%)] Loss: 7529.320312\n",
      "Train Epoch: 105 [12544/225000 (6%)] Loss: 7449.578125\n",
      "Train Epoch: 105 [16640/225000 (7%)] Loss: 7635.173828\n",
      "Train Epoch: 105 [20736/225000 (9%)] Loss: 7649.529297\n",
      "Train Epoch: 105 [24832/225000 (11%)] Loss: 7631.132812\n",
      "Train Epoch: 105 [28928/225000 (13%)] Loss: 7664.267578\n",
      "Train Epoch: 105 [33024/225000 (15%)] Loss: 7650.023438\n",
      "Train Epoch: 105 [37120/225000 (16%)] Loss: 7569.496094\n",
      "Train Epoch: 105 [41216/225000 (18%)] Loss: 7711.167969\n",
      "Train Epoch: 105 [45312/225000 (20%)] Loss: 7607.902344\n",
      "Train Epoch: 105 [49408/225000 (22%)] Loss: 7713.574219\n",
      "Train Epoch: 105 [53504/225000 (24%)] Loss: 7596.251953\n",
      "Train Epoch: 105 [57600/225000 (26%)] Loss: 7624.947266\n",
      "Train Epoch: 105 [61696/225000 (27%)] Loss: 7579.654297\n",
      "Train Epoch: 105 [65792/225000 (29%)] Loss: 7555.083984\n",
      "Train Epoch: 105 [69888/225000 (31%)] Loss: 7709.234375\n",
      "Train Epoch: 105 [73984/225000 (33%)] Loss: 7588.564453\n",
      "Train Epoch: 105 [78080/225000 (35%)] Loss: 7610.925781\n",
      "Train Epoch: 105 [82176/225000 (37%)] Loss: 7403.529297\n",
      "Train Epoch: 105 [86272/225000 (38%)] Loss: 7612.638672\n",
      "Train Epoch: 105 [90368/225000 (40%)] Loss: 7745.886719\n",
      "Train Epoch: 105 [94464/225000 (42%)] Loss: 7531.359375\n",
      "Train Epoch: 105 [98560/225000 (44%)] Loss: 7559.826172\n",
      "Train Epoch: 105 [102656/225000 (46%)] Loss: 7758.408203\n",
      "Train Epoch: 105 [106752/225000 (47%)] Loss: 7531.693359\n",
      "Train Epoch: 105 [110848/225000 (49%)] Loss: 7566.039062\n",
      "Train Epoch: 105 [114944/225000 (51%)] Loss: 7695.455078\n",
      "Train Epoch: 105 [119040/225000 (53%)] Loss: 7383.394531\n",
      "Train Epoch: 105 [123136/225000 (55%)] Loss: 7485.212891\n",
      "Train Epoch: 105 [127232/225000 (57%)] Loss: 7810.730469\n",
      "Train Epoch: 105 [131328/225000 (58%)] Loss: 7578.132812\n",
      "Train Epoch: 105 [135424/225000 (60%)] Loss: 7575.476562\n",
      "Train Epoch: 105 [139520/225000 (62%)] Loss: 7426.767578\n",
      "Train Epoch: 105 [143616/225000 (64%)] Loss: 7661.083984\n",
      "Train Epoch: 105 [147712/225000 (66%)] Loss: 7653.664062\n",
      "Train Epoch: 105 [151808/225000 (67%)] Loss: 7552.285156\n",
      "Train Epoch: 105 [155904/225000 (69%)] Loss: 7578.751953\n",
      "Train Epoch: 105 [160000/225000 (71%)] Loss: 7640.074219\n",
      "Train Epoch: 105 [164096/225000 (73%)] Loss: 7732.021484\n",
      "Train Epoch: 105 [168192/225000 (75%)] Loss: 7509.521484\n",
      "Train Epoch: 105 [172288/225000 (77%)] Loss: 7495.941406\n",
      "Train Epoch: 105 [176384/225000 (78%)] Loss: 7607.529297\n",
      "Train Epoch: 105 [180480/225000 (80%)] Loss: 7551.507812\n",
      "Train Epoch: 105 [184576/225000 (82%)] Loss: 7599.175781\n",
      "Train Epoch: 105 [188672/225000 (84%)] Loss: 7693.091797\n",
      "Train Epoch: 105 [192768/225000 (86%)] Loss: 7694.251953\n",
      "Train Epoch: 105 [196864/225000 (87%)] Loss: 7562.111328\n",
      "Train Epoch: 105 [200960/225000 (89%)] Loss: 7620.921875\n",
      "Train Epoch: 105 [205056/225000 (91%)] Loss: 7468.894531\n",
      "Train Epoch: 105 [209152/225000 (93%)] Loss: 7811.273438\n",
      "Train Epoch: 105 [213248/225000 (95%)] Loss: 7569.699219\n",
      "Train Epoch: 105 [217344/225000 (97%)] Loss: 7669.775391\n",
      "Train Epoch: 105 [221440/225000 (98%)] Loss: 7495.167969\n",
      "    epoch          : 105\n",
      "    loss           : 7622.988211257466\n",
      "    val_loss       : 7581.685158972838\n",
      "Train Epoch: 106 [256/225000 (0%)] Loss: 7505.177734\n",
      "Train Epoch: 106 [4352/225000 (2%)] Loss: 7552.070312\n",
      "Train Epoch: 106 [8448/225000 (4%)] Loss: 7583.207031\n",
      "Train Epoch: 106 [12544/225000 (6%)] Loss: 7633.076172\n",
      "Train Epoch: 106 [16640/225000 (7%)] Loss: 7654.517578\n",
      "Train Epoch: 106 [20736/225000 (9%)] Loss: 7820.349609\n",
      "Train Epoch: 106 [24832/225000 (11%)] Loss: 7674.111328\n",
      "Train Epoch: 106 [28928/225000 (13%)] Loss: 7781.552734\n",
      "Train Epoch: 106 [33024/225000 (15%)] Loss: 7728.578125\n",
      "Train Epoch: 106 [37120/225000 (16%)] Loss: 7594.392578\n",
      "Train Epoch: 106 [41216/225000 (18%)] Loss: 7465.027344\n",
      "Train Epoch: 106 [45312/225000 (20%)] Loss: 7682.486328\n",
      "Train Epoch: 106 [49408/225000 (22%)] Loss: 7681.798828\n",
      "Train Epoch: 106 [53504/225000 (24%)] Loss: 7479.148438\n",
      "Train Epoch: 106 [57600/225000 (26%)] Loss: 7524.259766\n",
      "Train Epoch: 106 [61696/225000 (27%)] Loss: 7751.464844\n",
      "Train Epoch: 106 [65792/225000 (29%)] Loss: 7799.333984\n",
      "Train Epoch: 106 [69888/225000 (31%)] Loss: 7638.984375\n",
      "Train Epoch: 106 [73984/225000 (33%)] Loss: 7561.652344\n",
      "Train Epoch: 106 [78080/225000 (35%)] Loss: 7561.589844\n",
      "Train Epoch: 106 [82176/225000 (37%)] Loss: 7400.496094\n",
      "Train Epoch: 106 [86272/225000 (38%)] Loss: 7437.369141\n",
      "Train Epoch: 106 [90368/225000 (40%)] Loss: 7625.414062\n",
      "Train Epoch: 106 [94464/225000 (42%)] Loss: 7690.394531\n",
      "Train Epoch: 106 [98560/225000 (44%)] Loss: 7658.201172\n",
      "Train Epoch: 106 [102656/225000 (46%)] Loss: 7798.292969\n",
      "Train Epoch: 106 [106752/225000 (47%)] Loss: 7644.271484\n",
      "Train Epoch: 106 [110848/225000 (49%)] Loss: 7469.320312\n",
      "Train Epoch: 106 [114944/225000 (51%)] Loss: 7282.839844\n",
      "Train Epoch: 106 [119040/225000 (53%)] Loss: 7583.238281\n",
      "Train Epoch: 106 [123136/225000 (55%)] Loss: 7423.679688\n",
      "Train Epoch: 106 [127232/225000 (57%)] Loss: 7744.517578\n",
      "Train Epoch: 106 [131328/225000 (58%)] Loss: 7552.205078\n",
      "Train Epoch: 106 [135424/225000 (60%)] Loss: 7714.722656\n",
      "Train Epoch: 106 [139520/225000 (62%)] Loss: 7485.906250\n",
      "Train Epoch: 106 [143616/225000 (64%)] Loss: 7436.503906\n",
      "Train Epoch: 106 [147712/225000 (66%)] Loss: 7595.521484\n",
      "Train Epoch: 106 [151808/225000 (67%)] Loss: 7697.380859\n",
      "Train Epoch: 106 [155904/225000 (69%)] Loss: 7369.902344\n",
      "Train Epoch: 106 [160000/225000 (71%)] Loss: 7659.291016\n",
      "Train Epoch: 106 [164096/225000 (73%)] Loss: 7527.443359\n",
      "Train Epoch: 106 [168192/225000 (75%)] Loss: 7606.890625\n",
      "Train Epoch: 106 [172288/225000 (77%)] Loss: 7576.375000\n",
      "Train Epoch: 106 [176384/225000 (78%)] Loss: 7606.875000\n",
      "Train Epoch: 106 [180480/225000 (80%)] Loss: 7631.865234\n",
      "Train Epoch: 106 [184576/225000 (82%)] Loss: 7560.357422\n",
      "Train Epoch: 106 [188672/225000 (84%)] Loss: 7574.017578\n",
      "Train Epoch: 106 [192768/225000 (86%)] Loss: 7597.814453\n",
      "Train Epoch: 106 [196864/225000 (87%)] Loss: 7447.435547\n",
      "Train Epoch: 106 [200960/225000 (89%)] Loss: 17894.718750\n",
      "Train Epoch: 106 [205056/225000 (91%)] Loss: 7399.744141\n",
      "Train Epoch: 106 [209152/225000 (93%)] Loss: 7578.810547\n",
      "Train Epoch: 106 [213248/225000 (95%)] Loss: 7411.871094\n",
      "Train Epoch: 106 [217344/225000 (97%)] Loss: 7634.750000\n",
      "Train Epoch: 106 [221440/225000 (98%)] Loss: 7749.910156\n",
      "    epoch          : 106\n",
      "    loss           : 7603.296695019198\n",
      "    val_loss       : 7676.668348309945\n",
      "Train Epoch: 107 [256/225000 (0%)] Loss: 7783.828125\n",
      "Train Epoch: 107 [4352/225000 (2%)] Loss: 7643.515625\n",
      "Train Epoch: 107 [8448/225000 (4%)] Loss: 7501.462891\n",
      "Train Epoch: 107 [12544/225000 (6%)] Loss: 7852.589844\n",
      "Train Epoch: 107 [16640/225000 (7%)] Loss: 7858.703125\n",
      "Train Epoch: 107 [20736/225000 (9%)] Loss: 7874.701172\n",
      "Train Epoch: 107 [24832/225000 (11%)] Loss: 7432.843750\n",
      "Train Epoch: 107 [28928/225000 (13%)] Loss: 7492.712891\n",
      "Train Epoch: 107 [33024/225000 (15%)] Loss: 7696.472656\n",
      "Train Epoch: 107 [37120/225000 (16%)] Loss: 7564.619141\n",
      "Train Epoch: 107 [41216/225000 (18%)] Loss: 7636.179688\n",
      "Train Epoch: 107 [45312/225000 (20%)] Loss: 7520.357422\n",
      "Train Epoch: 107 [49408/225000 (22%)] Loss: 7433.212891\n",
      "Train Epoch: 107 [53504/225000 (24%)] Loss: 7601.806641\n",
      "Train Epoch: 107 [57600/225000 (26%)] Loss: 7626.621094\n",
      "Train Epoch: 107 [61696/225000 (27%)] Loss: 7477.130859\n",
      "Train Epoch: 107 [65792/225000 (29%)] Loss: 7718.388672\n",
      "Train Epoch: 107 [69888/225000 (31%)] Loss: 7577.150391\n",
      "Train Epoch: 107 [73984/225000 (33%)] Loss: 7428.097656\n",
      "Train Epoch: 107 [78080/225000 (35%)] Loss: 7633.710938\n",
      "Train Epoch: 107 [82176/225000 (37%)] Loss: 7497.755859\n",
      "Train Epoch: 107 [86272/225000 (38%)] Loss: 7559.056641\n",
      "Train Epoch: 107 [90368/225000 (40%)] Loss: 7536.343750\n",
      "Train Epoch: 107 [94464/225000 (42%)] Loss: 7499.980469\n",
      "Train Epoch: 107 [98560/225000 (44%)] Loss: 7525.103516\n",
      "Train Epoch: 107 [102656/225000 (46%)] Loss: 7656.402344\n",
      "Train Epoch: 107 [106752/225000 (47%)] Loss: 7463.134766\n",
      "Train Epoch: 107 [110848/225000 (49%)] Loss: 7509.470703\n",
      "Train Epoch: 107 [114944/225000 (51%)] Loss: 7477.859375\n",
      "Train Epoch: 107 [119040/225000 (53%)] Loss: 7748.794922\n",
      "Train Epoch: 107 [123136/225000 (55%)] Loss: 7588.982422\n",
      "Train Epoch: 107 [127232/225000 (57%)] Loss: 7626.332031\n",
      "Train Epoch: 107 [131328/225000 (58%)] Loss: 7519.787109\n",
      "Train Epoch: 107 [135424/225000 (60%)] Loss: 7441.947266\n",
      "Train Epoch: 107 [139520/225000 (62%)] Loss: 7626.636719\n",
      "Train Epoch: 107 [143616/225000 (64%)] Loss: 7649.621094\n",
      "Train Epoch: 107 [147712/225000 (66%)] Loss: 7628.539062\n",
      "Train Epoch: 107 [151808/225000 (67%)] Loss: 7648.679688\n",
      "Train Epoch: 107 [155904/225000 (69%)] Loss: 7483.951172\n",
      "Train Epoch: 107 [160000/225000 (71%)] Loss: 7769.576172\n",
      "Train Epoch: 107 [164096/225000 (73%)] Loss: 7584.246094\n",
      "Train Epoch: 107 [168192/225000 (75%)] Loss: 7490.107422\n",
      "Train Epoch: 107 [172288/225000 (77%)] Loss: 7644.912109\n",
      "Train Epoch: 107 [176384/225000 (78%)] Loss: 7383.193359\n",
      "Train Epoch: 107 [180480/225000 (80%)] Loss: 7705.367188\n",
      "Train Epoch: 107 [184576/225000 (82%)] Loss: 7664.429688\n",
      "Train Epoch: 107 [188672/225000 (84%)] Loss: 7732.400391\n",
      "Train Epoch: 107 [192768/225000 (86%)] Loss: 7518.697266\n",
      "Train Epoch: 107 [196864/225000 (87%)] Loss: 7765.162109\n",
      "Train Epoch: 107 [200960/225000 (89%)] Loss: 7668.185547\n",
      "Train Epoch: 107 [205056/225000 (91%)] Loss: 7698.228516\n",
      "Train Epoch: 107 [209152/225000 (93%)] Loss: 7657.496094\n",
      "Train Epoch: 107 [213248/225000 (95%)] Loss: 7534.410156\n",
      "Train Epoch: 107 [217344/225000 (97%)] Loss: 7568.361328\n",
      "Train Epoch: 107 [221440/225000 (98%)] Loss: 7447.896484\n",
      "    epoch          : 107\n",
      "    loss           : 7584.704622618032\n",
      "    val_loss       : 7574.951636409273\n",
      "Train Epoch: 108 [256/225000 (0%)] Loss: 7610.791016\n",
      "Train Epoch: 108 [4352/225000 (2%)] Loss: 7586.908203\n",
      "Train Epoch: 108 [8448/225000 (4%)] Loss: 7545.523438\n",
      "Train Epoch: 108 [12544/225000 (6%)] Loss: 7626.736328\n",
      "Train Epoch: 108 [16640/225000 (7%)] Loss: 7679.496094\n",
      "Train Epoch: 108 [20736/225000 (9%)] Loss: 7589.169922\n",
      "Train Epoch: 108 [24832/225000 (11%)] Loss: 7633.849609\n",
      "Train Epoch: 108 [28928/225000 (13%)] Loss: 7401.500000\n",
      "Train Epoch: 108 [33024/225000 (15%)] Loss: 7565.765625\n",
      "Train Epoch: 108 [37120/225000 (16%)] Loss: 7546.171875\n",
      "Train Epoch: 108 [41216/225000 (18%)] Loss: 7548.773438\n",
      "Train Epoch: 108 [45312/225000 (20%)] Loss: 7641.767578\n",
      "Train Epoch: 108 [49408/225000 (22%)] Loss: 7812.748047\n",
      "Train Epoch: 108 [53504/225000 (24%)] Loss: 7460.367188\n",
      "Train Epoch: 108 [57600/225000 (26%)] Loss: 7508.642578\n",
      "Train Epoch: 108 [61696/225000 (27%)] Loss: 7502.853516\n",
      "Train Epoch: 108 [65792/225000 (29%)] Loss: 7807.447266\n",
      "Train Epoch: 108 [69888/225000 (31%)] Loss: 7535.031250\n",
      "Train Epoch: 108 [73984/225000 (33%)] Loss: 7642.566406\n",
      "Train Epoch: 108 [78080/225000 (35%)] Loss: 7532.281250\n",
      "Train Epoch: 108 [82176/225000 (37%)] Loss: 7483.523438\n",
      "Train Epoch: 108 [86272/225000 (38%)] Loss: 7563.955078\n",
      "Train Epoch: 108 [90368/225000 (40%)] Loss: 7605.449219\n",
      "Train Epoch: 108 [94464/225000 (42%)] Loss: 7455.361328\n",
      "Train Epoch: 108 [98560/225000 (44%)] Loss: 7496.728516\n",
      "Train Epoch: 108 [102656/225000 (46%)] Loss: 7635.464844\n",
      "Train Epoch: 108 [106752/225000 (47%)] Loss: 7582.738281\n",
      "Train Epoch: 108 [110848/225000 (49%)] Loss: 7572.644531\n",
      "Train Epoch: 108 [114944/225000 (51%)] Loss: 7505.021484\n",
      "Train Epoch: 108 [119040/225000 (53%)] Loss: 7597.091797\n",
      "Train Epoch: 108 [123136/225000 (55%)] Loss: 7706.769531\n",
      "Train Epoch: 108 [127232/225000 (57%)] Loss: 7459.199219\n",
      "Train Epoch: 108 [131328/225000 (58%)] Loss: 7580.371094\n",
      "Train Epoch: 108 [135424/225000 (60%)] Loss: 7507.091797\n",
      "Train Epoch: 108 [139520/225000 (62%)] Loss: 7613.332031\n",
      "Train Epoch: 108 [143616/225000 (64%)] Loss: 7638.775391\n",
      "Train Epoch: 108 [147712/225000 (66%)] Loss: 7598.599609\n",
      "Train Epoch: 108 [151808/225000 (67%)] Loss: 7798.046875\n",
      "Train Epoch: 108 [155904/225000 (69%)] Loss: 7584.500000\n",
      "Train Epoch: 108 [160000/225000 (71%)] Loss: 7617.541016\n",
      "Train Epoch: 108 [164096/225000 (73%)] Loss: 7694.349609\n",
      "Train Epoch: 108 [168192/225000 (75%)] Loss: 7600.509766\n",
      "Train Epoch: 108 [172288/225000 (77%)] Loss: 7367.611328\n",
      "Train Epoch: 108 [176384/225000 (78%)] Loss: 7418.324219\n",
      "Train Epoch: 108 [180480/225000 (80%)] Loss: 7786.966797\n",
      "Train Epoch: 108 [184576/225000 (82%)] Loss: 7534.865234\n",
      "Train Epoch: 108 [188672/225000 (84%)] Loss: 7743.330078\n",
      "Train Epoch: 108 [192768/225000 (86%)] Loss: 7583.894531\n",
      "Train Epoch: 108 [196864/225000 (87%)] Loss: 7619.527344\n",
      "Train Epoch: 108 [200960/225000 (89%)] Loss: 7645.152344\n",
      "Train Epoch: 108 [205056/225000 (91%)] Loss: 7620.583984\n",
      "Train Epoch: 108 [209152/225000 (93%)] Loss: 7677.875000\n",
      "Train Epoch: 108 [213248/225000 (95%)] Loss: 7589.837891\n",
      "Train Epoch: 108 [217344/225000 (97%)] Loss: 7701.917969\n",
      "Train Epoch: 108 [221440/225000 (98%)] Loss: 7509.917969\n",
      "    epoch          : 108\n",
      "    loss           : 7609.678470963097\n",
      "    val_loss       : 7563.588861146752\n",
      "Train Epoch: 109 [256/225000 (0%)] Loss: 7551.636719\n",
      "Train Epoch: 109 [4352/225000 (2%)] Loss: 7396.875000\n",
      "Train Epoch: 109 [8448/225000 (4%)] Loss: 7413.179688\n",
      "Train Epoch: 109 [12544/225000 (6%)] Loss: 7659.421875\n",
      "Train Epoch: 109 [16640/225000 (7%)] Loss: 7554.279297\n",
      "Train Epoch: 109 [20736/225000 (9%)] Loss: 7499.244141\n",
      "Train Epoch: 109 [24832/225000 (11%)] Loss: 7617.925781\n",
      "Train Epoch: 109 [28928/225000 (13%)] Loss: 7812.703125\n",
      "Train Epoch: 109 [33024/225000 (15%)] Loss: 7558.490234\n",
      "Train Epoch: 109 [37120/225000 (16%)] Loss: 7647.832031\n",
      "Train Epoch: 109 [41216/225000 (18%)] Loss: 7558.468750\n",
      "Train Epoch: 109 [45312/225000 (20%)] Loss: 7693.167969\n",
      "Train Epoch: 109 [49408/225000 (22%)] Loss: 7652.957031\n",
      "Train Epoch: 109 [53504/225000 (24%)] Loss: 7556.685547\n",
      "Train Epoch: 109 [57600/225000 (26%)] Loss: 7672.132812\n",
      "Train Epoch: 109 [61696/225000 (27%)] Loss: 7619.470703\n",
      "Train Epoch: 109 [65792/225000 (29%)] Loss: 7574.416016\n",
      "Train Epoch: 109 [69888/225000 (31%)] Loss: 7574.726562\n",
      "Train Epoch: 109 [73984/225000 (33%)] Loss: 7561.041016\n",
      "Train Epoch: 109 [78080/225000 (35%)] Loss: 7510.412109\n",
      "Train Epoch: 109 [82176/225000 (37%)] Loss: 7671.412109\n",
      "Train Epoch: 109 [86272/225000 (38%)] Loss: 7627.740234\n",
      "Train Epoch: 109 [90368/225000 (40%)] Loss: 7394.427734\n",
      "Train Epoch: 109 [94464/225000 (42%)] Loss: 7431.513672\n",
      "Train Epoch: 109 [98560/225000 (44%)] Loss: 7681.968750\n",
      "Train Epoch: 109 [102656/225000 (46%)] Loss: 7601.181641\n",
      "Train Epoch: 109 [106752/225000 (47%)] Loss: 7585.355469\n",
      "Train Epoch: 109 [110848/225000 (49%)] Loss: 7581.001953\n",
      "Train Epoch: 109 [114944/225000 (51%)] Loss: 7652.490234\n",
      "Train Epoch: 109 [119040/225000 (53%)] Loss: 7545.093750\n",
      "Train Epoch: 109 [123136/225000 (55%)] Loss: 7595.658203\n",
      "Train Epoch: 109 [127232/225000 (57%)] Loss: 7575.984375\n",
      "Train Epoch: 109 [131328/225000 (58%)] Loss: 7405.949219\n",
      "Train Epoch: 109 [135424/225000 (60%)] Loss: 7391.136719\n",
      "Train Epoch: 109 [139520/225000 (62%)] Loss: 7568.718750\n",
      "Train Epoch: 109 [143616/225000 (64%)] Loss: 7683.162109\n",
      "Train Epoch: 109 [147712/225000 (66%)] Loss: 7565.160156\n",
      "Train Epoch: 109 [151808/225000 (67%)] Loss: 7607.638672\n",
      "Train Epoch: 109 [155904/225000 (69%)] Loss: 7557.462891\n",
      "Train Epoch: 109 [160000/225000 (71%)] Loss: 7602.722656\n",
      "Train Epoch: 109 [164096/225000 (73%)] Loss: 7623.644531\n",
      "Train Epoch: 109 [168192/225000 (75%)] Loss: 7513.291016\n",
      "Train Epoch: 109 [172288/225000 (77%)] Loss: 7512.818359\n",
      "Train Epoch: 109 [176384/225000 (78%)] Loss: 7536.511719\n",
      "Train Epoch: 109 [180480/225000 (80%)] Loss: 7624.605469\n",
      "Train Epoch: 109 [184576/225000 (82%)] Loss: 7614.730469\n",
      "Train Epoch: 109 [188672/225000 (84%)] Loss: 7562.871094\n",
      "Train Epoch: 109 [192768/225000 (86%)] Loss: 7645.593750\n",
      "Train Epoch: 109 [196864/225000 (87%)] Loss: 7661.388672\n",
      "Train Epoch: 109 [200960/225000 (89%)] Loss: 7686.277344\n",
      "Train Epoch: 109 [205056/225000 (91%)] Loss: 7580.888672\n",
      "Train Epoch: 109 [209152/225000 (93%)] Loss: 7553.308594\n",
      "Train Epoch: 109 [213248/225000 (95%)] Loss: 7638.623047\n",
      "Train Epoch: 109 [217344/225000 (97%)] Loss: 7675.916016\n",
      "Train Epoch: 109 [221440/225000 (98%)] Loss: 7616.017578\n",
      "    epoch          : 109\n",
      "    loss           : 7593.283238676763\n",
      "    val_loss       : 7562.584553123736\n",
      "Train Epoch: 110 [256/225000 (0%)] Loss: 7580.300781\n",
      "Train Epoch: 110 [4352/225000 (2%)] Loss: 7613.933594\n",
      "Train Epoch: 110 [8448/225000 (4%)] Loss: 7598.718750\n",
      "Train Epoch: 110 [12544/225000 (6%)] Loss: 7518.519531\n",
      "Train Epoch: 110 [16640/225000 (7%)] Loss: 7584.080078\n",
      "Train Epoch: 110 [20736/225000 (9%)] Loss: 7790.863281\n",
      "Train Epoch: 110 [24832/225000 (11%)] Loss: 7463.667969\n",
      "Train Epoch: 110 [28928/225000 (13%)] Loss: 7465.255859\n",
      "Train Epoch: 110 [33024/225000 (15%)] Loss: 7527.197266\n",
      "Train Epoch: 110 [37120/225000 (16%)] Loss: 7658.478516\n",
      "Train Epoch: 110 [41216/225000 (18%)] Loss: 7590.818359\n",
      "Train Epoch: 110 [45312/225000 (20%)] Loss: 7485.867188\n",
      "Train Epoch: 110 [49408/225000 (22%)] Loss: 7278.917969\n",
      "Train Epoch: 110 [53504/225000 (24%)] Loss: 7524.419922\n",
      "Train Epoch: 110 [57600/225000 (26%)] Loss: 7566.865234\n",
      "Train Epoch: 110 [61696/225000 (27%)] Loss: 7557.160156\n",
      "Train Epoch: 110 [65792/225000 (29%)] Loss: 7650.203125\n",
      "Train Epoch: 110 [69888/225000 (31%)] Loss: 7701.017578\n",
      "Train Epoch: 110 [73984/225000 (33%)] Loss: 7476.232422\n",
      "Train Epoch: 110 [78080/225000 (35%)] Loss: 7603.710938\n",
      "Train Epoch: 110 [82176/225000 (37%)] Loss: 7451.386719\n",
      "Train Epoch: 110 [86272/225000 (38%)] Loss: 7497.373047\n",
      "Train Epoch: 110 [90368/225000 (40%)] Loss: 7498.843750\n",
      "Train Epoch: 110 [94464/225000 (42%)] Loss: 7696.859375\n",
      "Train Epoch: 110 [98560/225000 (44%)] Loss: 7586.458984\n",
      "Train Epoch: 110 [102656/225000 (46%)] Loss: 7468.783203\n",
      "Train Epoch: 110 [106752/225000 (47%)] Loss: 7546.800781\n",
      "Train Epoch: 110 [110848/225000 (49%)] Loss: 7638.431641\n",
      "Train Epoch: 110 [114944/225000 (51%)] Loss: 7715.232422\n",
      "Train Epoch: 110 [119040/225000 (53%)] Loss: 7578.441406\n",
      "Train Epoch: 110 [123136/225000 (55%)] Loss: 7381.023438\n",
      "Train Epoch: 110 [127232/225000 (57%)] Loss: 7446.740234\n",
      "Train Epoch: 110 [131328/225000 (58%)] Loss: 7553.566406\n",
      "Train Epoch: 110 [135424/225000 (60%)] Loss: 7518.996094\n",
      "Train Epoch: 110 [139520/225000 (62%)] Loss: 7580.267578\n",
      "Train Epoch: 110 [143616/225000 (64%)] Loss: 7435.644531\n",
      "Train Epoch: 110 [147712/225000 (66%)] Loss: 7925.525391\n",
      "Train Epoch: 110 [151808/225000 (67%)] Loss: 7675.955078\n",
      "Train Epoch: 110 [155904/225000 (69%)] Loss: 7606.673828\n",
      "Train Epoch: 110 [160000/225000 (71%)] Loss: 7374.019531\n",
      "Train Epoch: 110 [164096/225000 (73%)] Loss: 7545.533203\n",
      "Train Epoch: 110 [168192/225000 (75%)] Loss: 7743.285156\n",
      "Train Epoch: 110 [172288/225000 (77%)] Loss: 7637.091797\n",
      "Train Epoch: 110 [176384/225000 (78%)] Loss: 7477.837891\n",
      "Train Epoch: 110 [180480/225000 (80%)] Loss: 7564.552734\n",
      "Train Epoch: 110 [184576/225000 (82%)] Loss: 7554.302734\n",
      "Train Epoch: 110 [188672/225000 (84%)] Loss: 7587.089844\n",
      "Train Epoch: 110 [192768/225000 (86%)] Loss: 7613.117188\n",
      "Train Epoch: 110 [196864/225000 (87%)] Loss: 7521.716797\n",
      "Train Epoch: 110 [200960/225000 (89%)] Loss: 7574.890625\n",
      "Train Epoch: 110 [205056/225000 (91%)] Loss: 7696.384766\n",
      "Train Epoch: 110 [209152/225000 (93%)] Loss: 7624.496094\n",
      "Train Epoch: 110 [213248/225000 (95%)] Loss: 7509.865234\n",
      "Train Epoch: 110 [217344/225000 (97%)] Loss: 7440.105469\n",
      "Train Epoch: 110 [221440/225000 (98%)] Loss: 7673.541016\n",
      "    epoch          : 110\n",
      "    loss           : 7565.064668657566\n",
      "    val_loss       : 7553.045077297153\n",
      "Train Epoch: 111 [256/225000 (0%)] Loss: 7688.746094\n",
      "Train Epoch: 111 [4352/225000 (2%)] Loss: 7564.453125\n",
      "Train Epoch: 111 [8448/225000 (4%)] Loss: 7704.027344\n",
      "Train Epoch: 111 [12544/225000 (6%)] Loss: 7619.267578\n",
      "Train Epoch: 111 [16640/225000 (7%)] Loss: 7432.074219\n",
      "Train Epoch: 111 [20736/225000 (9%)] Loss: 7505.599609\n",
      "Train Epoch: 111 [24832/225000 (11%)] Loss: 7584.962891\n",
      "Train Epoch: 111 [28928/225000 (13%)] Loss: 7524.517578\n",
      "Train Epoch: 111 [33024/225000 (15%)] Loss: 7581.148438\n",
      "Train Epoch: 111 [37120/225000 (16%)] Loss: 7807.742188\n",
      "Train Epoch: 111 [41216/225000 (18%)] Loss: 7536.388672\n",
      "Train Epoch: 111 [45312/225000 (20%)] Loss: 7582.074219\n",
      "Train Epoch: 111 [49408/225000 (22%)] Loss: 7478.392578\n",
      "Train Epoch: 111 [53504/225000 (24%)] Loss: 7612.068359\n",
      "Train Epoch: 111 [57600/225000 (26%)] Loss: 7536.906250\n",
      "Train Epoch: 111 [61696/225000 (27%)] Loss: 7636.800781\n",
      "Train Epoch: 111 [65792/225000 (29%)] Loss: 7610.564453\n",
      "Train Epoch: 111 [69888/225000 (31%)] Loss: 7434.064453\n",
      "Train Epoch: 111 [73984/225000 (33%)] Loss: 7507.736328\n",
      "Train Epoch: 111 [78080/225000 (35%)] Loss: 7672.078125\n",
      "Train Epoch: 111 [82176/225000 (37%)] Loss: 7739.621094\n",
      "Train Epoch: 111 [86272/225000 (38%)] Loss: 7671.687500\n",
      "Train Epoch: 111 [90368/225000 (40%)] Loss: 7510.416016\n",
      "Train Epoch: 111 [94464/225000 (42%)] Loss: 7627.701172\n",
      "Train Epoch: 111 [98560/225000 (44%)] Loss: 7641.767578\n",
      "Train Epoch: 111 [102656/225000 (46%)] Loss: 7591.292969\n",
      "Train Epoch: 111 [106752/225000 (47%)] Loss: 7413.138672\n",
      "Train Epoch: 111 [110848/225000 (49%)] Loss: 7575.460938\n",
      "Train Epoch: 111 [114944/225000 (51%)] Loss: 7587.296875\n",
      "Train Epoch: 111 [119040/225000 (53%)] Loss: 7364.945312\n",
      "Train Epoch: 111 [123136/225000 (55%)] Loss: 7588.033203\n",
      "Train Epoch: 111 [127232/225000 (57%)] Loss: 7537.494141\n",
      "Train Epoch: 111 [131328/225000 (58%)] Loss: 7415.531250\n",
      "Train Epoch: 111 [135424/225000 (60%)] Loss: 7638.230469\n",
      "Train Epoch: 111 [139520/225000 (62%)] Loss: 7491.117188\n",
      "Train Epoch: 111 [143616/225000 (64%)] Loss: 7665.304688\n",
      "Train Epoch: 111 [147712/225000 (66%)] Loss: 7577.628906\n",
      "Train Epoch: 111 [151808/225000 (67%)] Loss: 7611.046875\n",
      "Train Epoch: 111 [155904/225000 (69%)] Loss: 7538.000000\n",
      "Train Epoch: 111 [160000/225000 (71%)] Loss: 7453.160156\n",
      "Train Epoch: 111 [164096/225000 (73%)] Loss: 7460.130859\n",
      "Train Epoch: 111 [168192/225000 (75%)] Loss: 7562.658203\n",
      "Train Epoch: 111 [172288/225000 (77%)] Loss: 7561.029297\n",
      "Train Epoch: 111 [176384/225000 (78%)] Loss: 7550.251953\n",
      "Train Epoch: 111 [180480/225000 (80%)] Loss: 7682.242188\n",
      "Train Epoch: 111 [184576/225000 (82%)] Loss: 7459.318359\n",
      "Train Epoch: 111 [188672/225000 (84%)] Loss: 7574.224609\n",
      "Train Epoch: 111 [192768/225000 (86%)] Loss: 7659.751953\n",
      "Train Epoch: 111 [196864/225000 (87%)] Loss: 7444.011719\n",
      "Train Epoch: 111 [200960/225000 (89%)] Loss: 7394.630859\n",
      "Train Epoch: 111 [205056/225000 (91%)] Loss: 7513.707031\n",
      "Train Epoch: 111 [209152/225000 (93%)] Loss: 7551.802734\n",
      "Train Epoch: 111 [213248/225000 (95%)] Loss: 7498.941406\n",
      "Train Epoch: 111 [217344/225000 (97%)] Loss: 7549.130859\n",
      "Train Epoch: 111 [221440/225000 (98%)] Loss: 7604.486328\n",
      "    epoch          : 111\n",
      "    loss           : 7586.819427038894\n",
      "    val_loss       : 7548.875578318323\n",
      "Train Epoch: 112 [256/225000 (0%)] Loss: 7564.507812\n",
      "Train Epoch: 112 [4352/225000 (2%)] Loss: 7417.052734\n",
      "Train Epoch: 112 [8448/225000 (4%)] Loss: 7551.179688\n",
      "Train Epoch: 112 [12544/225000 (6%)] Loss: 7624.244141\n",
      "Train Epoch: 112 [16640/225000 (7%)] Loss: 7736.501953\n",
      "Train Epoch: 112 [20736/225000 (9%)] Loss: 7536.648438\n",
      "Train Epoch: 112 [24832/225000 (11%)] Loss: 7432.792969\n",
      "Train Epoch: 112 [28928/225000 (13%)] Loss: 7559.785156\n",
      "Train Epoch: 112 [33024/225000 (15%)] Loss: 7702.431641\n",
      "Train Epoch: 112 [37120/225000 (16%)] Loss: 7366.271484\n",
      "Train Epoch: 112 [41216/225000 (18%)] Loss: 7521.876953\n",
      "Train Epoch: 112 [45312/225000 (20%)] Loss: 7367.566406\n",
      "Train Epoch: 112 [49408/225000 (22%)] Loss: 7561.496094\n",
      "Train Epoch: 112 [53504/225000 (24%)] Loss: 7560.378906\n",
      "Train Epoch: 112 [57600/225000 (26%)] Loss: 7736.910156\n",
      "Train Epoch: 112 [61696/225000 (27%)] Loss: 7657.650391\n",
      "Train Epoch: 112 [65792/225000 (29%)] Loss: 7375.845703\n",
      "Train Epoch: 112 [69888/225000 (31%)] Loss: 7435.794922\n",
      "Train Epoch: 112 [73984/225000 (33%)] Loss: 7308.261719\n",
      "Train Epoch: 112 [78080/225000 (35%)] Loss: 7619.863281\n",
      "Train Epoch: 112 [82176/225000 (37%)] Loss: 7700.699219\n",
      "Train Epoch: 112 [86272/225000 (38%)] Loss: 7468.021484\n",
      "Train Epoch: 112 [90368/225000 (40%)] Loss: 7740.724609\n",
      "Train Epoch: 112 [94464/225000 (42%)] Loss: 7491.513672\n",
      "Train Epoch: 112 [98560/225000 (44%)] Loss: 7577.750000\n",
      "Train Epoch: 112 [102656/225000 (46%)] Loss: 7674.978516\n",
      "Train Epoch: 112 [106752/225000 (47%)] Loss: 7559.953125\n",
      "Train Epoch: 112 [110848/225000 (49%)] Loss: 7527.666016\n",
      "Train Epoch: 112 [114944/225000 (51%)] Loss: 7422.173828\n",
      "Train Epoch: 112 [119040/225000 (53%)] Loss: 7492.019531\n",
      "Train Epoch: 112 [123136/225000 (55%)] Loss: 7554.802734\n",
      "Train Epoch: 112 [127232/225000 (57%)] Loss: 7528.970703\n",
      "Train Epoch: 112 [131328/225000 (58%)] Loss: 7393.320312\n",
      "Train Epoch: 112 [135424/225000 (60%)] Loss: 7611.378906\n",
      "Train Epoch: 112 [139520/225000 (62%)] Loss: 7724.476562\n",
      "Train Epoch: 112 [143616/225000 (64%)] Loss: 7608.771484\n",
      "Train Epoch: 112 [147712/225000 (66%)] Loss: 7581.919922\n",
      "Train Epoch: 112 [151808/225000 (67%)] Loss: 7488.148438\n",
      "Train Epoch: 112 [155904/225000 (69%)] Loss: 7421.531250\n",
      "Train Epoch: 112 [160000/225000 (71%)] Loss: 7454.064453\n",
      "Train Epoch: 112 [164096/225000 (73%)] Loss: 7538.919922\n",
      "Train Epoch: 112 [168192/225000 (75%)] Loss: 7698.794922\n",
      "Train Epoch: 112 [172288/225000 (77%)] Loss: 7587.646484\n",
      "Train Epoch: 112 [176384/225000 (78%)] Loss: 7615.246094\n",
      "Train Epoch: 112 [180480/225000 (80%)] Loss: 7681.480469\n",
      "Train Epoch: 112 [184576/225000 (82%)] Loss: 7586.457031\n",
      "Train Epoch: 112 [188672/225000 (84%)] Loss: 7492.974609\n",
      "Train Epoch: 112 [192768/225000 (86%)] Loss: 7717.634766\n",
      "Train Epoch: 112 [196864/225000 (87%)] Loss: 7719.582031\n",
      "Train Epoch: 112 [200960/225000 (89%)] Loss: 7721.285156\n",
      "Train Epoch: 112 [205056/225000 (91%)] Loss: 7567.916016\n",
      "Train Epoch: 112 [209152/225000 (93%)] Loss: 7434.601562\n",
      "Train Epoch: 112 [213248/225000 (95%)] Loss: 7604.039062\n",
      "Train Epoch: 112 [217344/225000 (97%)] Loss: 7706.525391\n",
      "Train Epoch: 112 [221440/225000 (98%)] Loss: 7621.691406\n",
      "    epoch          : 112\n",
      "    loss           : 7566.172152748151\n",
      "    val_loss       : 7540.483395029088\n",
      "Train Epoch: 113 [256/225000 (0%)] Loss: 7498.791016\n",
      "Train Epoch: 113 [4352/225000 (2%)] Loss: 7601.847656\n",
      "Train Epoch: 113 [8448/225000 (4%)] Loss: 7450.644531\n",
      "Train Epoch: 113 [12544/225000 (6%)] Loss: 7647.187500\n",
      "Train Epoch: 113 [16640/225000 (7%)] Loss: 7670.878906\n",
      "Train Epoch: 113 [20736/225000 (9%)] Loss: 7355.187500\n",
      "Train Epoch: 113 [24832/225000 (11%)] Loss: 7562.626953\n",
      "Train Epoch: 113 [28928/225000 (13%)] Loss: 7555.941406\n",
      "Train Epoch: 113 [33024/225000 (15%)] Loss: 7580.244141\n",
      "Train Epoch: 113 [37120/225000 (16%)] Loss: 7684.630859\n",
      "Train Epoch: 113 [41216/225000 (18%)] Loss: 7552.525391\n",
      "Train Epoch: 113 [45312/225000 (20%)] Loss: 7458.597656\n",
      "Train Epoch: 113 [49408/225000 (22%)] Loss: 7514.003906\n",
      "Train Epoch: 113 [53504/225000 (24%)] Loss: 7709.919922\n",
      "Train Epoch: 113 [57600/225000 (26%)] Loss: 7571.990234\n",
      "Train Epoch: 113 [61696/225000 (27%)] Loss: 7518.503906\n",
      "Train Epoch: 113 [65792/225000 (29%)] Loss: 7903.890625\n",
      "Train Epoch: 113 [69888/225000 (31%)] Loss: 7709.503906\n",
      "Train Epoch: 113 [73984/225000 (33%)] Loss: 7557.671875\n",
      "Train Epoch: 113 [78080/225000 (35%)] Loss: 7479.857422\n",
      "Train Epoch: 113 [82176/225000 (37%)] Loss: 7665.050781\n",
      "Train Epoch: 113 [86272/225000 (38%)] Loss: 7458.697266\n",
      "Train Epoch: 113 [90368/225000 (40%)] Loss: 7683.710938\n",
      "Train Epoch: 113 [94464/225000 (42%)] Loss: 7457.843750\n",
      "Train Epoch: 113 [98560/225000 (44%)] Loss: 7686.949219\n",
      "Train Epoch: 113 [102656/225000 (46%)] Loss: 7454.142578\n",
      "Train Epoch: 113 [106752/225000 (47%)] Loss: 7515.992188\n",
      "Train Epoch: 113 [110848/225000 (49%)] Loss: 7680.412109\n",
      "Train Epoch: 113 [114944/225000 (51%)] Loss: 7466.033203\n",
      "Train Epoch: 113 [119040/225000 (53%)] Loss: 7536.941406\n",
      "Train Epoch: 113 [123136/225000 (55%)] Loss: 7542.576172\n",
      "Train Epoch: 113 [127232/225000 (57%)] Loss: 7518.093750\n",
      "Train Epoch: 113 [131328/225000 (58%)] Loss: 7467.509766\n",
      "Train Epoch: 113 [135424/225000 (60%)] Loss: 7516.292969\n",
      "Train Epoch: 113 [139520/225000 (62%)] Loss: 7523.626953\n",
      "Train Epoch: 113 [143616/225000 (64%)] Loss: 7629.101562\n",
      "Train Epoch: 113 [147712/225000 (66%)] Loss: 7536.677734\n",
      "Train Epoch: 113 [151808/225000 (67%)] Loss: 7463.238281\n",
      "Train Epoch: 113 [155904/225000 (69%)] Loss: 7526.466797\n",
      "Train Epoch: 113 [160000/225000 (71%)] Loss: 7411.746094\n",
      "Train Epoch: 113 [164096/225000 (73%)] Loss: 7416.718750\n",
      "Train Epoch: 113 [168192/225000 (75%)] Loss: 7768.259766\n",
      "Train Epoch: 113 [172288/225000 (77%)] Loss: 7677.656250\n",
      "Train Epoch: 113 [176384/225000 (78%)] Loss: 7377.699219\n",
      "Train Epoch: 113 [180480/225000 (80%)] Loss: 7544.601562\n",
      "Train Epoch: 113 [184576/225000 (82%)] Loss: 7667.908203\n",
      "Train Epoch: 113 [188672/225000 (84%)] Loss: 7693.275391\n",
      "Train Epoch: 113 [192768/225000 (86%)] Loss: 7478.494141\n",
      "Train Epoch: 113 [196864/225000 (87%)] Loss: 7483.589844\n",
      "Train Epoch: 113 [200960/225000 (89%)] Loss: 7536.949219\n",
      "Train Epoch: 113 [205056/225000 (91%)] Loss: 7566.734375\n",
      "Train Epoch: 113 [209152/225000 (93%)] Loss: 7519.994141\n",
      "Train Epoch: 113 [213248/225000 (95%)] Loss: 7584.019531\n",
      "Train Epoch: 113 [217344/225000 (97%)] Loss: 7549.501953\n",
      "Train Epoch: 113 [221440/225000 (98%)] Loss: 7531.027344\n",
      "    epoch          : 113\n",
      "    loss           : 7584.622592479024\n",
      "    val_loss       : 7537.2634897000935\n",
      "Train Epoch: 114 [256/225000 (0%)] Loss: 7563.511719\n",
      "Train Epoch: 114 [4352/225000 (2%)] Loss: 7652.033203\n",
      "Train Epoch: 114 [8448/225000 (4%)] Loss: 7542.160156\n",
      "Train Epoch: 114 [12544/225000 (6%)] Loss: 7363.671875\n",
      "Train Epoch: 114 [16640/225000 (7%)] Loss: 7417.308594\n",
      "Train Epoch: 114 [20736/225000 (9%)] Loss: 7581.816406\n",
      "Train Epoch: 114 [24832/225000 (11%)] Loss: 7481.423828\n",
      "Train Epoch: 114 [28928/225000 (13%)] Loss: 7633.470703\n",
      "Train Epoch: 114 [33024/225000 (15%)] Loss: 7587.195312\n",
      "Train Epoch: 114 [37120/225000 (16%)] Loss: 7618.513672\n",
      "Train Epoch: 114 [41216/225000 (18%)] Loss: 7666.105469\n",
      "Train Epoch: 114 [45312/225000 (20%)] Loss: 7640.248047\n",
      "Train Epoch: 114 [49408/225000 (22%)] Loss: 7572.205078\n",
      "Train Epoch: 114 [53504/225000 (24%)] Loss: 7671.724609\n",
      "Train Epoch: 114 [57600/225000 (26%)] Loss: 7358.824219\n",
      "Train Epoch: 114 [61696/225000 (27%)] Loss: 7417.029297\n",
      "Train Epoch: 114 [65792/225000 (29%)] Loss: 7512.750000\n",
      "Train Epoch: 114 [69888/225000 (31%)] Loss: 7553.613281\n",
      "Train Epoch: 114 [73984/225000 (33%)] Loss: 7671.738281\n",
      "Train Epoch: 114 [78080/225000 (35%)] Loss: 7490.800781\n",
      "Train Epoch: 114 [82176/225000 (37%)] Loss: 7711.521484\n",
      "Train Epoch: 114 [86272/225000 (38%)] Loss: 7603.882812\n",
      "Train Epoch: 114 [90368/225000 (40%)] Loss: 7547.212891\n",
      "Train Epoch: 114 [94464/225000 (42%)] Loss: 7681.500000\n",
      "Train Epoch: 114 [98560/225000 (44%)] Loss: 7594.703125\n",
      "Train Epoch: 114 [102656/225000 (46%)] Loss: 7543.259766\n",
      "Train Epoch: 114 [106752/225000 (47%)] Loss: 7424.556641\n",
      "Train Epoch: 114 [110848/225000 (49%)] Loss: 7640.558594\n",
      "Train Epoch: 114 [114944/225000 (51%)] Loss: 7588.304688\n",
      "Train Epoch: 114 [119040/225000 (53%)] Loss: 7522.630859\n",
      "Train Epoch: 114 [123136/225000 (55%)] Loss: 7652.615234\n",
      "Train Epoch: 114 [127232/225000 (57%)] Loss: 7659.791016\n",
      "Train Epoch: 114 [131328/225000 (58%)] Loss: 7818.062500\n",
      "Train Epoch: 114 [135424/225000 (60%)] Loss: 7426.777344\n",
      "Train Epoch: 114 [139520/225000 (62%)] Loss: 7395.849609\n",
      "Train Epoch: 114 [143616/225000 (64%)] Loss: 7553.812500\n",
      "Train Epoch: 114 [147712/225000 (66%)] Loss: 7497.417969\n",
      "Train Epoch: 114 [151808/225000 (67%)] Loss: 7390.736328\n",
      "Train Epoch: 114 [155904/225000 (69%)] Loss: 7566.425781\n",
      "Train Epoch: 114 [160000/225000 (71%)] Loss: 7669.882812\n",
      "Train Epoch: 114 [164096/225000 (73%)] Loss: 7709.488281\n",
      "Train Epoch: 114 [168192/225000 (75%)] Loss: 7417.083984\n",
      "Train Epoch: 114 [172288/225000 (77%)] Loss: 7558.886719\n",
      "Train Epoch: 114 [176384/225000 (78%)] Loss: 7422.509766\n",
      "Train Epoch: 114 [180480/225000 (80%)] Loss: 7347.697266\n",
      "Train Epoch: 114 [184576/225000 (82%)] Loss: 7446.003906\n",
      "Train Epoch: 114 [188672/225000 (84%)] Loss: 7453.943359\n",
      "Train Epoch: 114 [192768/225000 (86%)] Loss: 7401.117188\n",
      "Train Epoch: 114 [196864/225000 (87%)] Loss: 7527.812500\n",
      "Train Epoch: 114 [200960/225000 (89%)] Loss: 7533.417969\n",
      "Train Epoch: 114 [205056/225000 (91%)] Loss: 7541.400391\n",
      "Train Epoch: 114 [209152/225000 (93%)] Loss: 7549.371094\n",
      "Train Epoch: 114 [213248/225000 (95%)] Loss: 7586.056641\n",
      "Train Epoch: 114 [217344/225000 (97%)] Loss: 7713.933594\n",
      "Train Epoch: 114 [221440/225000 (98%)] Loss: 7351.562500\n",
      "    epoch          : 114\n",
      "    loss           : 7539.273227522398\n",
      "    val_loss       : 7537.60842591889\n",
      "Train Epoch: 115 [256/225000 (0%)] Loss: 7553.755859\n",
      "Train Epoch: 115 [4352/225000 (2%)] Loss: 7385.353516\n",
      "Train Epoch: 115 [8448/225000 (4%)] Loss: 7453.439453\n",
      "Train Epoch: 115 [12544/225000 (6%)] Loss: 7452.177734\n",
      "Train Epoch: 115 [16640/225000 (7%)] Loss: 7633.013672\n",
      "Train Epoch: 115 [20736/225000 (9%)] Loss: 7681.169922\n",
      "Train Epoch: 115 [24832/225000 (11%)] Loss: 7468.589844\n",
      "Train Epoch: 115 [28928/225000 (13%)] Loss: 7610.871094\n",
      "Train Epoch: 115 [33024/225000 (15%)] Loss: 7544.937500\n",
      "Train Epoch: 115 [37120/225000 (16%)] Loss: 7558.279297\n",
      "Train Epoch: 115 [41216/225000 (18%)] Loss: 7364.144531\n",
      "Train Epoch: 115 [45312/225000 (20%)] Loss: 7404.740234\n",
      "Train Epoch: 115 [49408/225000 (22%)] Loss: 7449.552734\n",
      "Train Epoch: 115 [53504/225000 (24%)] Loss: 7576.583984\n",
      "Train Epoch: 115 [57600/225000 (26%)] Loss: 7584.566406\n",
      "Train Epoch: 115 [61696/225000 (27%)] Loss: 7486.322266\n",
      "Train Epoch: 115 [65792/225000 (29%)] Loss: 7500.662109\n",
      "Train Epoch: 115 [69888/225000 (31%)] Loss: 7361.667969\n",
      "Train Epoch: 115 [73984/225000 (33%)] Loss: 7707.550781\n",
      "Train Epoch: 115 [78080/225000 (35%)] Loss: 7656.250000\n",
      "Train Epoch: 115 [82176/225000 (37%)] Loss: 7599.951172\n",
      "Train Epoch: 115 [86272/225000 (38%)] Loss: 7475.619141\n",
      "Train Epoch: 115 [90368/225000 (40%)] Loss: 7651.136719\n",
      "Train Epoch: 115 [94464/225000 (42%)] Loss: 7544.785156\n",
      "Train Epoch: 115 [98560/225000 (44%)] Loss: 7196.376953\n",
      "Train Epoch: 115 [102656/225000 (46%)] Loss: 7430.582031\n",
      "Train Epoch: 115 [106752/225000 (47%)] Loss: 7438.955078\n",
      "Train Epoch: 115 [110848/225000 (49%)] Loss: 7593.845703\n",
      "Train Epoch: 115 [114944/225000 (51%)] Loss: 7375.904297\n",
      "Train Epoch: 115 [119040/225000 (53%)] Loss: 7468.345703\n",
      "Train Epoch: 115 [123136/225000 (55%)] Loss: 7603.208984\n",
      "Train Epoch: 115 [127232/225000 (57%)] Loss: 7656.316406\n",
      "Train Epoch: 115 [131328/225000 (58%)] Loss: 7468.693359\n",
      "Train Epoch: 115 [135424/225000 (60%)] Loss: 7723.837891\n",
      "Train Epoch: 115 [139520/225000 (62%)] Loss: 7471.298828\n",
      "Train Epoch: 115 [143616/225000 (64%)] Loss: 7654.546875\n",
      "Train Epoch: 115 [147712/225000 (66%)] Loss: 7492.873047\n",
      "Train Epoch: 115 [151808/225000 (67%)] Loss: 7515.812500\n",
      "Train Epoch: 115 [155904/225000 (69%)] Loss: 7628.726562\n",
      "Train Epoch: 115 [160000/225000 (71%)] Loss: 7575.494141\n",
      "Train Epoch: 115 [164096/225000 (73%)] Loss: 7555.558594\n",
      "Train Epoch: 115 [168192/225000 (75%)] Loss: 7353.093750\n",
      "Train Epoch: 115 [172288/225000 (77%)] Loss: 7582.435547\n",
      "Train Epoch: 115 [176384/225000 (78%)] Loss: 7627.169922\n",
      "Train Epoch: 115 [180480/225000 (80%)] Loss: 7484.777344\n",
      "Train Epoch: 115 [184576/225000 (82%)] Loss: 7644.744141\n",
      "Train Epoch: 115 [188672/225000 (84%)] Loss: 7548.335938\n",
      "Train Epoch: 115 [192768/225000 (86%)] Loss: 7496.308594\n",
      "Train Epoch: 115 [196864/225000 (87%)] Loss: 7365.113281\n",
      "Train Epoch: 115 [200960/225000 (89%)] Loss: 7603.996094\n",
      "Train Epoch: 115 [205056/225000 (91%)] Loss: 7661.083984\n",
      "Train Epoch: 115 [209152/225000 (93%)] Loss: 7512.496094\n",
      "Train Epoch: 115 [213248/225000 (95%)] Loss: 7656.091797\n",
      "Train Epoch: 115 [217344/225000 (97%)] Loss: 7597.994141\n",
      "Train Epoch: 115 [221440/225000 (98%)] Loss: 7385.097656\n",
      "    epoch          : 115\n",
      "    loss           : 7534.807817166169\n",
      "    val_loss       : 7623.2102660573255\n",
      "Train Epoch: 116 [256/225000 (0%)] Loss: 7298.628906\n",
      "Train Epoch: 116 [4352/225000 (2%)] Loss: 7546.658203\n",
      "Train Epoch: 116 [8448/225000 (4%)] Loss: 7433.330078\n",
      "Train Epoch: 116 [12544/225000 (6%)] Loss: 7535.505859\n",
      "Train Epoch: 116 [16640/225000 (7%)] Loss: 7461.351562\n",
      "Train Epoch: 116 [20736/225000 (9%)] Loss: 7451.478516\n",
      "Train Epoch: 116 [24832/225000 (11%)] Loss: 7457.914062\n",
      "Train Epoch: 116 [28928/225000 (13%)] Loss: 7565.667969\n",
      "Train Epoch: 116 [33024/225000 (15%)] Loss: 7557.234375\n",
      "Train Epoch: 116 [37120/225000 (16%)] Loss: 7395.068359\n",
      "Train Epoch: 116 [41216/225000 (18%)] Loss: 7525.923828\n",
      "Train Epoch: 116 [45312/225000 (20%)] Loss: 7539.302734\n",
      "Train Epoch: 116 [49408/225000 (22%)] Loss: 7788.519531\n",
      "Train Epoch: 116 [53504/225000 (24%)] Loss: 7556.601562\n",
      "Train Epoch: 116 [57600/225000 (26%)] Loss: 7582.699219\n",
      "Train Epoch: 116 [61696/225000 (27%)] Loss: 7640.017578\n",
      "Train Epoch: 116 [65792/225000 (29%)] Loss: 7559.035156\n",
      "Train Epoch: 116 [69888/225000 (31%)] Loss: 7411.666016\n",
      "Train Epoch: 116 [73984/225000 (33%)] Loss: 7582.527344\n",
      "Train Epoch: 116 [78080/225000 (35%)] Loss: 7659.351562\n",
      "Train Epoch: 116 [82176/225000 (37%)] Loss: 7544.675781\n",
      "Train Epoch: 116 [86272/225000 (38%)] Loss: 7386.275391\n",
      "Train Epoch: 116 [90368/225000 (40%)] Loss: 7525.630859\n",
      "Train Epoch: 116 [94464/225000 (42%)] Loss: 7525.214844\n",
      "Train Epoch: 116 [98560/225000 (44%)] Loss: 7594.972656\n",
      "Train Epoch: 116 [102656/225000 (46%)] Loss: 7562.162109\n",
      "Train Epoch: 116 [106752/225000 (47%)] Loss: 7474.382812\n",
      "Train Epoch: 116 [110848/225000 (49%)] Loss: 7613.244141\n",
      "Train Epoch: 116 [114944/225000 (51%)] Loss: 7558.832031\n",
      "Train Epoch: 116 [119040/225000 (53%)] Loss: 7391.468750\n",
      "Train Epoch: 116 [123136/225000 (55%)] Loss: 7657.279297\n",
      "Train Epoch: 116 [127232/225000 (57%)] Loss: 7560.134766\n",
      "Train Epoch: 116 [131328/225000 (58%)] Loss: 7572.457031\n",
      "Train Epoch: 116 [135424/225000 (60%)] Loss: 7590.732422\n",
      "Train Epoch: 116 [139520/225000 (62%)] Loss: 7529.724609\n",
      "Train Epoch: 116 [143616/225000 (64%)] Loss: 7532.103516\n",
      "Train Epoch: 116 [147712/225000 (66%)] Loss: 7385.902344\n",
      "Train Epoch: 116 [151808/225000 (67%)] Loss: 7555.033203\n",
      "Train Epoch: 116 [155904/225000 (69%)] Loss: 7659.125000\n",
      "Train Epoch: 116 [160000/225000 (71%)] Loss: 7511.095703\n",
      "Train Epoch: 116 [164096/225000 (73%)] Loss: 7699.705078\n",
      "Train Epoch: 116 [168192/225000 (75%)] Loss: 7516.726562\n",
      "Train Epoch: 116 [172288/225000 (77%)] Loss: 7526.791016\n",
      "Train Epoch: 116 [176384/225000 (78%)] Loss: 7525.322266\n",
      "Train Epoch: 116 [180480/225000 (80%)] Loss: 7450.681641\n",
      "Train Epoch: 116 [184576/225000 (82%)] Loss: 7529.603516\n",
      "Train Epoch: 116 [188672/225000 (84%)] Loss: 7339.011719\n",
      "Train Epoch: 116 [192768/225000 (86%)] Loss: 7645.677734\n",
      "Train Epoch: 116 [196864/225000 (87%)] Loss: 7656.863281\n",
      "Train Epoch: 116 [200960/225000 (89%)] Loss: 7424.867188\n",
      "Train Epoch: 116 [205056/225000 (91%)] Loss: 7469.919922\n",
      "Train Epoch: 116 [209152/225000 (93%)] Loss: 7381.240234\n",
      "Train Epoch: 116 [213248/225000 (95%)] Loss: 7641.283203\n",
      "Train Epoch: 116 [217344/225000 (97%)] Loss: 7446.662109\n",
      "Train Epoch: 116 [221440/225000 (98%)] Loss: 7639.287109\n",
      "    epoch          : 116\n",
      "    loss           : 7529.56521193295\n",
      "    val_loss       : 7514.9285138054765\n",
      "Train Epoch: 117 [256/225000 (0%)] Loss: 7571.912109\n",
      "Train Epoch: 117 [4352/225000 (2%)] Loss: 7597.437500\n",
      "Train Epoch: 117 [8448/225000 (4%)] Loss: 7461.820312\n",
      "Train Epoch: 117 [12544/225000 (6%)] Loss: 7667.701172\n",
      "Train Epoch: 117 [16640/225000 (7%)] Loss: 7470.166016\n",
      "Train Epoch: 117 [20736/225000 (9%)] Loss: 7518.761719\n",
      "Train Epoch: 117 [24832/225000 (11%)] Loss: 7396.470703\n",
      "Train Epoch: 117 [28928/225000 (13%)] Loss: 7299.828125\n",
      "Train Epoch: 117 [33024/225000 (15%)] Loss: 7479.183594\n",
      "Train Epoch: 117 [37120/225000 (16%)] Loss: 7331.714844\n",
      "Train Epoch: 117 [41216/225000 (18%)] Loss: 7290.419922\n",
      "Train Epoch: 117 [45312/225000 (20%)] Loss: 7647.113281\n",
      "Train Epoch: 117 [49408/225000 (22%)] Loss: 7543.503906\n",
      "Train Epoch: 117 [53504/225000 (24%)] Loss: 7556.130859\n",
      "Train Epoch: 117 [57600/225000 (26%)] Loss: 7494.498047\n",
      "Train Epoch: 117 [61696/225000 (27%)] Loss: 17660.091797\n",
      "Train Epoch: 117 [65792/225000 (29%)] Loss: 7437.173828\n",
      "Train Epoch: 117 [69888/225000 (31%)] Loss: 7563.552734\n",
      "Train Epoch: 117 [73984/225000 (33%)] Loss: 7483.082031\n",
      "Train Epoch: 117 [78080/225000 (35%)] Loss: 7706.330078\n",
      "Train Epoch: 117 [82176/225000 (37%)] Loss: 7519.796875\n",
      "Train Epoch: 117 [86272/225000 (38%)] Loss: 7476.173828\n",
      "Train Epoch: 117 [90368/225000 (40%)] Loss: 7587.062500\n",
      "Train Epoch: 117 [94464/225000 (42%)] Loss: 7575.222656\n",
      "Train Epoch: 117 [98560/225000 (44%)] Loss: 7643.876953\n",
      "Train Epoch: 117 [102656/225000 (46%)] Loss: 7542.621094\n",
      "Train Epoch: 117 [106752/225000 (47%)] Loss: 7609.197266\n",
      "Train Epoch: 117 [110848/225000 (49%)] Loss: 7572.609375\n",
      "Train Epoch: 117 [114944/225000 (51%)] Loss: 7537.214844\n",
      "Train Epoch: 117 [119040/225000 (53%)] Loss: 7407.556641\n",
      "Train Epoch: 117 [123136/225000 (55%)] Loss: 7609.136719\n",
      "Train Epoch: 117 [127232/225000 (57%)] Loss: 7568.802734\n",
      "Train Epoch: 117 [131328/225000 (58%)] Loss: 7480.339844\n",
      "Train Epoch: 117 [135424/225000 (60%)] Loss: 7661.248047\n",
      "Train Epoch: 117 [139520/225000 (62%)] Loss: 7747.343750\n",
      "Train Epoch: 117 [143616/225000 (64%)] Loss: 7402.316406\n",
      "Train Epoch: 117 [147712/225000 (66%)] Loss: 7343.541016\n",
      "Train Epoch: 117 [151808/225000 (67%)] Loss: 7580.179688\n",
      "Train Epoch: 117 [155904/225000 (69%)] Loss: 7571.021484\n",
      "Train Epoch: 117 [160000/225000 (71%)] Loss: 7652.970703\n",
      "Train Epoch: 117 [164096/225000 (73%)] Loss: 7667.984375\n",
      "Train Epoch: 117 [168192/225000 (75%)] Loss: 7525.457031\n",
      "Train Epoch: 117 [172288/225000 (77%)] Loss: 7624.945312\n",
      "Train Epoch: 117 [176384/225000 (78%)] Loss: 7665.205078\n",
      "Train Epoch: 117 [180480/225000 (80%)] Loss: 7540.884766\n",
      "Train Epoch: 117 [184576/225000 (82%)] Loss: 7580.548828\n",
      "Train Epoch: 117 [188672/225000 (84%)] Loss: 7444.958984\n",
      "Train Epoch: 117 [192768/225000 (86%)] Loss: 7388.701172\n",
      "Train Epoch: 117 [196864/225000 (87%)] Loss: 7443.144531\n",
      "Train Epoch: 117 [200960/225000 (89%)] Loss: 7433.957031\n",
      "Train Epoch: 117 [205056/225000 (91%)] Loss: 7417.582031\n",
      "Train Epoch: 117 [209152/225000 (93%)] Loss: 7697.078125\n",
      "Train Epoch: 117 [213248/225000 (95%)] Loss: 7528.855469\n",
      "Train Epoch: 117 [217344/225000 (97%)] Loss: 7603.214844\n",
      "Train Epoch: 117 [221440/225000 (98%)] Loss: 7490.433594\n",
      "    epoch          : 117\n",
      "    loss           : 7546.364589999289\n",
      "    val_loss       : 7506.0410548132295\n",
      "Train Epoch: 118 [256/225000 (0%)] Loss: 7605.660156\n",
      "Train Epoch: 118 [4352/225000 (2%)] Loss: 7472.265625\n",
      "Train Epoch: 118 [8448/225000 (4%)] Loss: 7475.904297\n",
      "Train Epoch: 118 [12544/225000 (6%)] Loss: 7458.830078\n",
      "Train Epoch: 118 [16640/225000 (7%)] Loss: 7510.775391\n",
      "Train Epoch: 118 [20736/225000 (9%)] Loss: 7514.222656\n",
      "Train Epoch: 118 [24832/225000 (11%)] Loss: 7478.748047\n",
      "Train Epoch: 118 [28928/225000 (13%)] Loss: 7323.539062\n",
      "Train Epoch: 118 [33024/225000 (15%)] Loss: 7497.804688\n",
      "Train Epoch: 118 [37120/225000 (16%)] Loss: 7734.482422\n",
      "Train Epoch: 118 [41216/225000 (18%)] Loss: 7490.009766\n",
      "Train Epoch: 118 [45312/225000 (20%)] Loss: 7448.548828\n",
      "Train Epoch: 118 [49408/225000 (22%)] Loss: 7494.462891\n",
      "Train Epoch: 118 [53504/225000 (24%)] Loss: 7487.595703\n",
      "Train Epoch: 118 [57600/225000 (26%)] Loss: 7571.162109\n",
      "Train Epoch: 118 [61696/225000 (27%)] Loss: 7437.789062\n",
      "Train Epoch: 118 [65792/225000 (29%)] Loss: 7616.291016\n",
      "Train Epoch: 118 [69888/225000 (31%)] Loss: 7578.808594\n",
      "Train Epoch: 118 [73984/225000 (33%)] Loss: 7564.218750\n",
      "Train Epoch: 118 [78080/225000 (35%)] Loss: 7302.933594\n",
      "Train Epoch: 118 [82176/225000 (37%)] Loss: 7407.798828\n",
      "Train Epoch: 118 [86272/225000 (38%)] Loss: 7574.144531\n",
      "Train Epoch: 118 [90368/225000 (40%)] Loss: 7410.105469\n",
      "Train Epoch: 118 [94464/225000 (42%)] Loss: 7570.511719\n",
      "Train Epoch: 118 [98560/225000 (44%)] Loss: 7693.197266\n",
      "Train Epoch: 118 [102656/225000 (46%)] Loss: 7586.890625\n",
      "Train Epoch: 118 [106752/225000 (47%)] Loss: 7405.156250\n",
      "Train Epoch: 118 [110848/225000 (49%)] Loss: 7562.087891\n",
      "Train Epoch: 118 [114944/225000 (51%)] Loss: 7626.078125\n",
      "Train Epoch: 118 [119040/225000 (53%)] Loss: 7488.507812\n",
      "Train Epoch: 118 [123136/225000 (55%)] Loss: 7607.822266\n",
      "Train Epoch: 118 [127232/225000 (57%)] Loss: 7524.277344\n",
      "Train Epoch: 118 [131328/225000 (58%)] Loss: 7651.953125\n",
      "Train Epoch: 118 [135424/225000 (60%)] Loss: 7632.080078\n",
      "Train Epoch: 118 [139520/225000 (62%)] Loss: 7327.488281\n",
      "Train Epoch: 118 [143616/225000 (64%)] Loss: 7604.386719\n",
      "Train Epoch: 118 [147712/225000 (66%)] Loss: 7507.433594\n",
      "Train Epoch: 118 [151808/225000 (67%)] Loss: 7612.771484\n",
      "Train Epoch: 118 [155904/225000 (69%)] Loss: 7398.535156\n",
      "Train Epoch: 118 [160000/225000 (71%)] Loss: 7621.476562\n",
      "Train Epoch: 118 [164096/225000 (73%)] Loss: 7408.144531\n",
      "Train Epoch: 118 [168192/225000 (75%)] Loss: 7487.007812\n",
      "Train Epoch: 118 [172288/225000 (77%)] Loss: 7356.441406\n",
      "Train Epoch: 118 [176384/225000 (78%)] Loss: 7548.949219\n",
      "Train Epoch: 118 [180480/225000 (80%)] Loss: 7515.046875\n",
      "Train Epoch: 118 [184576/225000 (82%)] Loss: 7591.531250\n",
      "Train Epoch: 118 [188672/225000 (84%)] Loss: 7455.931641\n",
      "Train Epoch: 118 [192768/225000 (86%)] Loss: 7418.947266\n",
      "Train Epoch: 118 [196864/225000 (87%)] Loss: 7416.544922\n",
      "Train Epoch: 118 [200960/225000 (89%)] Loss: 7554.224609\n",
      "Train Epoch: 118 [205056/225000 (91%)] Loss: 7520.933594\n",
      "Train Epoch: 118 [209152/225000 (93%)] Loss: 7584.923828\n",
      "Train Epoch: 118 [213248/225000 (95%)] Loss: 7580.871094\n",
      "Train Epoch: 118 [217344/225000 (97%)] Loss: 7519.298828\n",
      "Train Epoch: 118 [221440/225000 (98%)] Loss: 7664.435547\n",
      "    epoch          : 118\n",
      "    loss           : 7516.530546741681\n",
      "    val_loss       : 7507.646864068752\n",
      "Train Epoch: 119 [256/225000 (0%)] Loss: 7391.845703\n",
      "Train Epoch: 119 [4352/225000 (2%)] Loss: 7609.787109\n",
      "Train Epoch: 119 [8448/225000 (4%)] Loss: 7422.234375\n",
      "Train Epoch: 119 [12544/225000 (6%)] Loss: 7593.863281\n",
      "Train Epoch: 119 [16640/225000 (7%)] Loss: 7566.005859\n",
      "Train Epoch: 119 [20736/225000 (9%)] Loss: 7295.201172\n",
      "Train Epoch: 119 [24832/225000 (11%)] Loss: 7422.271484\n",
      "Train Epoch: 119 [28928/225000 (13%)] Loss: 7529.208984\n",
      "Train Epoch: 119 [33024/225000 (15%)] Loss: 7610.361328\n",
      "Train Epoch: 119 [37120/225000 (16%)] Loss: 7565.876953\n",
      "Train Epoch: 119 [41216/225000 (18%)] Loss: 7635.144531\n",
      "Train Epoch: 119 [45312/225000 (20%)] Loss: 7500.810547\n",
      "Train Epoch: 119 [49408/225000 (22%)] Loss: 7500.943359\n",
      "Train Epoch: 119 [53504/225000 (24%)] Loss: 7455.986328\n",
      "Train Epoch: 119 [57600/225000 (26%)] Loss: 7528.740234\n",
      "Train Epoch: 119 [61696/225000 (27%)] Loss: 7511.230469\n",
      "Train Epoch: 119 [65792/225000 (29%)] Loss: 7455.212891\n",
      "Train Epoch: 119 [69888/225000 (31%)] Loss: 7657.640625\n",
      "Train Epoch: 119 [73984/225000 (33%)] Loss: 7562.968750\n",
      "Train Epoch: 119 [78080/225000 (35%)] Loss: 7316.337891\n",
      "Train Epoch: 119 [82176/225000 (37%)] Loss: 7549.601562\n",
      "Train Epoch: 119 [86272/225000 (38%)] Loss: 7362.826172\n",
      "Train Epoch: 119 [90368/225000 (40%)] Loss: 7385.402344\n",
      "Train Epoch: 119 [94464/225000 (42%)] Loss: 7577.160156\n",
      "Train Epoch: 119 [98560/225000 (44%)] Loss: 7598.037109\n",
      "Train Epoch: 119 [102656/225000 (46%)] Loss: 7315.220703\n",
      "Train Epoch: 119 [106752/225000 (47%)] Loss: 7524.992188\n",
      "Train Epoch: 119 [110848/225000 (49%)] Loss: 7424.464844\n",
      "Train Epoch: 119 [114944/225000 (51%)] Loss: 7349.796875\n",
      "Train Epoch: 119 [119040/225000 (53%)] Loss: 7382.890625\n",
      "Train Epoch: 119 [123136/225000 (55%)] Loss: 7499.662109\n",
      "Train Epoch: 119 [127232/225000 (57%)] Loss: 7337.349609\n",
      "Train Epoch: 119 [131328/225000 (58%)] Loss: 7715.343750\n",
      "Train Epoch: 119 [135424/225000 (60%)] Loss: 7529.046875\n",
      "Train Epoch: 119 [139520/225000 (62%)] Loss: 7565.544922\n",
      "Train Epoch: 119 [143616/225000 (64%)] Loss: 7639.708984\n",
      "Train Epoch: 119 [147712/225000 (66%)] Loss: 7417.253906\n",
      "Train Epoch: 119 [151808/225000 (67%)] Loss: 7667.259766\n",
      "Train Epoch: 119 [155904/225000 (69%)] Loss: 7488.304688\n",
      "Train Epoch: 119 [160000/225000 (71%)] Loss: 7676.210938\n",
      "Train Epoch: 119 [164096/225000 (73%)] Loss: 7467.285156\n",
      "Train Epoch: 119 [168192/225000 (75%)] Loss: 7419.375000\n",
      "Train Epoch: 119 [172288/225000 (77%)] Loss: 7356.328125\n",
      "Train Epoch: 119 [176384/225000 (78%)] Loss: 7557.011719\n",
      "Train Epoch: 119 [180480/225000 (80%)] Loss: 7392.669922\n",
      "Train Epoch: 119 [184576/225000 (82%)] Loss: 7542.027344\n",
      "Train Epoch: 119 [188672/225000 (84%)] Loss: 7421.230469\n",
      "Train Epoch: 119 [192768/225000 (86%)] Loss: 7369.287109\n",
      "Train Epoch: 119 [196864/225000 (87%)] Loss: 7533.072266\n",
      "Train Epoch: 119 [200960/225000 (89%)] Loss: 7523.460938\n",
      "Train Epoch: 119 [205056/225000 (91%)] Loss: 7470.275391\n",
      "Train Epoch: 119 [209152/225000 (93%)] Loss: 7562.507812\n",
      "Train Epoch: 119 [213248/225000 (95%)] Loss: 7543.166016\n",
      "Train Epoch: 119 [217344/225000 (97%)] Loss: 7368.914062\n",
      "Train Epoch: 119 [221440/225000 (98%)] Loss: 7402.912109\n",
      "    epoch          : 119\n",
      "    loss           : 7543.374852237984\n",
      "    val_loss       : 7499.671454245947\n",
      "Train Epoch: 120 [256/225000 (0%)] Loss: 7482.048828\n",
      "Train Epoch: 120 [4352/225000 (2%)] Loss: 7500.804688\n",
      "Train Epoch: 120 [8448/225000 (4%)] Loss: 7467.964844\n",
      "Train Epoch: 120 [12544/225000 (6%)] Loss: 7520.980469\n",
      "Train Epoch: 120 [16640/225000 (7%)] Loss: 7534.652344\n",
      "Train Epoch: 120 [20736/225000 (9%)] Loss: 7468.798828\n",
      "Train Epoch: 120 [24832/225000 (11%)] Loss: 7602.886719\n",
      "Train Epoch: 120 [28928/225000 (13%)] Loss: 7519.953125\n",
      "Train Epoch: 120 [33024/225000 (15%)] Loss: 7393.011719\n",
      "Train Epoch: 120 [37120/225000 (16%)] Loss: 7391.894531\n",
      "Train Epoch: 120 [41216/225000 (18%)] Loss: 7647.292969\n",
      "Train Epoch: 120 [45312/225000 (20%)] Loss: 7443.753906\n",
      "Train Epoch: 120 [49408/225000 (22%)] Loss: 7542.595703\n",
      "Train Epoch: 120 [53504/225000 (24%)] Loss: 7299.513672\n",
      "Train Epoch: 120 [57600/225000 (26%)] Loss: 7344.556641\n",
      "Train Epoch: 120 [61696/225000 (27%)] Loss: 7415.597656\n",
      "Train Epoch: 120 [65792/225000 (29%)] Loss: 7498.476562\n",
      "Train Epoch: 120 [69888/225000 (31%)] Loss: 7445.984375\n",
      "Train Epoch: 120 [73984/225000 (33%)] Loss: 7511.994141\n",
      "Train Epoch: 120 [78080/225000 (35%)] Loss: 7475.148438\n",
      "Train Epoch: 120 [82176/225000 (37%)] Loss: 7367.150391\n",
      "Train Epoch: 120 [86272/225000 (38%)] Loss: 7395.025391\n",
      "Train Epoch: 120 [90368/225000 (40%)] Loss: 7421.771484\n",
      "Train Epoch: 120 [94464/225000 (42%)] Loss: 7484.353516\n",
      "Train Epoch: 120 [98560/225000 (44%)] Loss: 7503.576172\n",
      "Train Epoch: 120 [102656/225000 (46%)] Loss: 7729.324219\n",
      "Train Epoch: 120 [106752/225000 (47%)] Loss: 7464.535156\n",
      "Train Epoch: 120 [110848/225000 (49%)] Loss: 7521.248047\n",
      "Train Epoch: 120 [114944/225000 (51%)] Loss: 7366.296875\n",
      "Train Epoch: 120 [119040/225000 (53%)] Loss: 7404.875000\n",
      "Train Epoch: 120 [123136/225000 (55%)] Loss: 7579.208984\n",
      "Train Epoch: 120 [127232/225000 (57%)] Loss: 7417.183594\n",
      "Train Epoch: 120 [131328/225000 (58%)] Loss: 7436.775391\n",
      "Train Epoch: 120 [135424/225000 (60%)] Loss: 7701.013672\n",
      "Train Epoch: 120 [139520/225000 (62%)] Loss: 7525.939453\n",
      "Train Epoch: 120 [143616/225000 (64%)] Loss: 7316.728516\n",
      "Train Epoch: 120 [147712/225000 (66%)] Loss: 7382.460938\n",
      "Train Epoch: 120 [151808/225000 (67%)] Loss: 7531.410156\n",
      "Train Epoch: 120 [155904/225000 (69%)] Loss: 7375.015625\n",
      "Train Epoch: 120 [160000/225000 (71%)] Loss: 7465.029297\n",
      "Train Epoch: 120 [164096/225000 (73%)] Loss: 7455.876953\n",
      "Train Epoch: 120 [168192/225000 (75%)] Loss: 7588.496094\n",
      "Train Epoch: 120 [172288/225000 (77%)] Loss: 7487.679688\n",
      "Train Epoch: 120 [176384/225000 (78%)] Loss: 7532.839844\n",
      "Train Epoch: 120 [180480/225000 (80%)] Loss: 7446.611328\n",
      "Train Epoch: 120 [184576/225000 (82%)] Loss: 7203.480469\n",
      "Train Epoch: 120 [188672/225000 (84%)] Loss: 7497.773438\n",
      "Train Epoch: 120 [192768/225000 (86%)] Loss: 7480.587891\n",
      "Train Epoch: 120 [196864/225000 (87%)] Loss: 7488.263672\n",
      "Train Epoch: 120 [200960/225000 (89%)] Loss: 7659.023438\n",
      "Train Epoch: 120 [205056/225000 (91%)] Loss: 7597.845703\n",
      "Train Epoch: 120 [209152/225000 (93%)] Loss: 7696.123047\n",
      "Train Epoch: 120 [213248/225000 (95%)] Loss: 7394.351562\n",
      "Train Epoch: 120 [217344/225000 (97%)] Loss: 7406.847656\n",
      "Train Epoch: 120 [221440/225000 (98%)] Loss: 7516.998047\n",
      "    epoch          : 120\n",
      "    loss           : 7541.938139931741\n",
      "    val_loss       : 7497.37204200151\n",
      "Train Epoch: 121 [256/225000 (0%)] Loss: 7436.601562\n",
      "Train Epoch: 121 [4352/225000 (2%)] Loss: 7566.072266\n",
      "Train Epoch: 121 [8448/225000 (4%)] Loss: 7473.582031\n",
      "Train Epoch: 121 [12544/225000 (6%)] Loss: 7531.287109\n",
      "Train Epoch: 121 [16640/225000 (7%)] Loss: 7573.062500\n",
      "Train Epoch: 121 [20736/225000 (9%)] Loss: 7324.470703\n",
      "Train Epoch: 121 [24832/225000 (11%)] Loss: 7486.925781\n",
      "Train Epoch: 121 [28928/225000 (13%)] Loss: 7460.357422\n",
      "Train Epoch: 121 [33024/225000 (15%)] Loss: 7344.712891\n",
      "Train Epoch: 121 [37120/225000 (16%)] Loss: 7491.267578\n",
      "Train Epoch: 121 [41216/225000 (18%)] Loss: 7465.123047\n",
      "Train Epoch: 121 [45312/225000 (20%)] Loss: 7636.828125\n",
      "Train Epoch: 121 [49408/225000 (22%)] Loss: 7487.339844\n",
      "Train Epoch: 121 [53504/225000 (24%)] Loss: 7653.121094\n",
      "Train Epoch: 121 [57600/225000 (26%)] Loss: 7401.757812\n",
      "Train Epoch: 121 [61696/225000 (27%)] Loss: 7443.080078\n",
      "Train Epoch: 121 [65792/225000 (29%)] Loss: 7488.996094\n",
      "Train Epoch: 121 [69888/225000 (31%)] Loss: 7580.115234\n",
      "Train Epoch: 121 [73984/225000 (33%)] Loss: 7466.550781\n",
      "Train Epoch: 121 [78080/225000 (35%)] Loss: 7315.570312\n",
      "Train Epoch: 121 [82176/225000 (37%)] Loss: 7575.214844\n",
      "Train Epoch: 121 [86272/225000 (38%)] Loss: 7525.910156\n",
      "Train Epoch: 121 [90368/225000 (40%)] Loss: 7537.929688\n",
      "Train Epoch: 121 [94464/225000 (42%)] Loss: 7461.542969\n",
      "Train Epoch: 121 [98560/225000 (44%)] Loss: 7458.923828\n",
      "Train Epoch: 121 [102656/225000 (46%)] Loss: 7467.576172\n",
      "Train Epoch: 121 [106752/225000 (47%)] Loss: 7414.468750\n",
      "Train Epoch: 121 [110848/225000 (49%)] Loss: 7533.996094\n",
      "Train Epoch: 121 [114944/225000 (51%)] Loss: 7672.916016\n",
      "Train Epoch: 121 [119040/225000 (53%)] Loss: 7548.828125\n",
      "Train Epoch: 121 [123136/225000 (55%)] Loss: 7626.359375\n",
      "Train Epoch: 121 [127232/225000 (57%)] Loss: 7531.033203\n",
      "Train Epoch: 121 [131328/225000 (58%)] Loss: 7474.685547\n",
      "Train Epoch: 121 [135424/225000 (60%)] Loss: 7495.748047\n",
      "Train Epoch: 121 [139520/225000 (62%)] Loss: 7611.048828\n",
      "Train Epoch: 121 [143616/225000 (64%)] Loss: 7519.152344\n",
      "Train Epoch: 121 [147712/225000 (66%)] Loss: 7485.521484\n",
      "Train Epoch: 121 [151808/225000 (67%)] Loss: 7386.736328\n",
      "Train Epoch: 121 [155904/225000 (69%)] Loss: 7425.339844\n",
      "Train Epoch: 121 [160000/225000 (71%)] Loss: 7581.371094\n",
      "Train Epoch: 121 [164096/225000 (73%)] Loss: 7650.378906\n",
      "Train Epoch: 121 [168192/225000 (75%)] Loss: 7452.917969\n",
      "Train Epoch: 121 [172288/225000 (77%)] Loss: 7393.259766\n",
      "Train Epoch: 121 [176384/225000 (78%)] Loss: 7523.878906\n",
      "Train Epoch: 121 [180480/225000 (80%)] Loss: 7605.333984\n",
      "Train Epoch: 121 [184576/225000 (82%)] Loss: 7462.476562\n",
      "Train Epoch: 121 [188672/225000 (84%)] Loss: 7419.757812\n",
      "Train Epoch: 121 [192768/225000 (86%)] Loss: 7363.103516\n",
      "Train Epoch: 121 [196864/225000 (87%)] Loss: 7435.162109\n",
      "Train Epoch: 121 [200960/225000 (89%)] Loss: 7469.125000\n",
      "Train Epoch: 121 [205056/225000 (91%)] Loss: 7538.457031\n",
      "Train Epoch: 121 [209152/225000 (93%)] Loss: 17907.093750\n",
      "Train Epoch: 121 [213248/225000 (95%)] Loss: 7470.476562\n",
      "Train Epoch: 121 [217344/225000 (97%)] Loss: 7419.142578\n",
      "Train Epoch: 121 [221440/225000 (98%)] Loss: 7533.691406\n",
      "    epoch          : 121\n",
      "    loss           : 7537.055324098763\n",
      "    val_loss       : 7483.614150577662\n",
      "Train Epoch: 122 [256/225000 (0%)] Loss: 7435.863281\n",
      "Train Epoch: 122 [4352/225000 (2%)] Loss: 7742.625000\n",
      "Train Epoch: 122 [8448/225000 (4%)] Loss: 7408.958984\n",
      "Train Epoch: 122 [12544/225000 (6%)] Loss: 7504.250000\n",
      "Train Epoch: 122 [16640/225000 (7%)] Loss: 7571.117188\n",
      "Train Epoch: 122 [20736/225000 (9%)] Loss: 7307.187500\n",
      "Train Epoch: 122 [24832/225000 (11%)] Loss: 7474.511719\n",
      "Train Epoch: 122 [28928/225000 (13%)] Loss: 7583.988281\n",
      "Train Epoch: 122 [33024/225000 (15%)] Loss: 7604.859375\n",
      "Train Epoch: 122 [37120/225000 (16%)] Loss: 7562.830078\n",
      "Train Epoch: 122 [41216/225000 (18%)] Loss: 7638.041016\n",
      "Train Epoch: 122 [45312/225000 (20%)] Loss: 7635.025391\n",
      "Train Epoch: 122 [49408/225000 (22%)] Loss: 7477.421875\n",
      "Train Epoch: 122 [53504/225000 (24%)] Loss: 7633.281250\n",
      "Train Epoch: 122 [57600/225000 (26%)] Loss: 7402.146484\n",
      "Train Epoch: 122 [61696/225000 (27%)] Loss: 7571.992188\n",
      "Train Epoch: 122 [65792/225000 (29%)] Loss: 7596.218750\n",
      "Train Epoch: 122 [69888/225000 (31%)] Loss: 7266.273438\n",
      "Train Epoch: 122 [73984/225000 (33%)] Loss: 7659.826172\n",
      "Train Epoch: 122 [78080/225000 (35%)] Loss: 7439.900391\n",
      "Train Epoch: 122 [82176/225000 (37%)] Loss: 7420.835938\n",
      "Train Epoch: 122 [86272/225000 (38%)] Loss: 7445.437500\n",
      "Train Epoch: 122 [90368/225000 (40%)] Loss: 7401.203125\n",
      "Train Epoch: 122 [94464/225000 (42%)] Loss: 7489.386719\n",
      "Train Epoch: 122 [98560/225000 (44%)] Loss: 7367.710938\n",
      "Train Epoch: 122 [102656/225000 (46%)] Loss: 7366.025391\n",
      "Train Epoch: 122 [106752/225000 (47%)] Loss: 7511.339844\n",
      "Train Epoch: 122 [110848/225000 (49%)] Loss: 7427.722656\n",
      "Train Epoch: 122 [114944/225000 (51%)] Loss: 7512.599609\n",
      "Train Epoch: 122 [119040/225000 (53%)] Loss: 7510.781250\n",
      "Train Epoch: 122 [123136/225000 (55%)] Loss: 7631.609375\n",
      "Train Epoch: 122 [127232/225000 (57%)] Loss: 7361.773438\n",
      "Train Epoch: 122 [131328/225000 (58%)] Loss: 7634.845703\n",
      "Train Epoch: 122 [135424/225000 (60%)] Loss: 7339.160156\n",
      "Train Epoch: 122 [139520/225000 (62%)] Loss: 7581.417969\n",
      "Train Epoch: 122 [143616/225000 (64%)] Loss: 7868.171875\n",
      "Train Epoch: 122 [147712/225000 (66%)] Loss: 7474.080078\n",
      "Train Epoch: 122 [151808/225000 (67%)] Loss: 7360.429688\n",
      "Train Epoch: 122 [155904/225000 (69%)] Loss: 7295.414062\n",
      "Train Epoch: 122 [160000/225000 (71%)] Loss: 7556.919922\n",
      "Train Epoch: 122 [164096/225000 (73%)] Loss: 7561.562500\n",
      "Train Epoch: 122 [168192/225000 (75%)] Loss: 7503.554688\n",
      "Train Epoch: 122 [172288/225000 (77%)] Loss: 7552.033203\n",
      "Train Epoch: 122 [176384/225000 (78%)] Loss: 7684.857422\n",
      "Train Epoch: 122 [180480/225000 (80%)] Loss: 7551.369141\n",
      "Train Epoch: 122 [184576/225000 (82%)] Loss: 7411.683594\n",
      "Train Epoch: 122 [188672/225000 (84%)] Loss: 7328.267578\n",
      "Train Epoch: 122 [192768/225000 (86%)] Loss: 7558.224609\n",
      "Train Epoch: 122 [196864/225000 (87%)] Loss: 7572.568359\n",
      "Train Epoch: 122 [200960/225000 (89%)] Loss: 7595.958984\n",
      "Train Epoch: 122 [205056/225000 (91%)] Loss: 7464.089844\n",
      "Train Epoch: 122 [209152/225000 (93%)] Loss: 7347.210938\n",
      "Train Epoch: 122 [213248/225000 (95%)] Loss: 7402.044922\n",
      "Train Epoch: 122 [217344/225000 (97%)] Loss: 7512.240234\n",
      "Train Epoch: 122 [221440/225000 (98%)] Loss: 7659.544922\n",
      "    epoch          : 122\n",
      "    loss           : 7494.793250942122\n",
      "    val_loss       : 7481.168538799091\n",
      "Train Epoch: 123 [256/225000 (0%)] Loss: 7480.898438\n",
      "Train Epoch: 123 [4352/225000 (2%)] Loss: 7437.685547\n",
      "Train Epoch: 123 [8448/225000 (4%)] Loss: 7467.078125\n",
      "Train Epoch: 123 [12544/225000 (6%)] Loss: 7561.066406\n",
      "Train Epoch: 123 [16640/225000 (7%)] Loss: 7457.310547\n",
      "Train Epoch: 123 [20736/225000 (9%)] Loss: 7406.085938\n",
      "Train Epoch: 123 [24832/225000 (11%)] Loss: 7424.511719\n",
      "Train Epoch: 123 [28928/225000 (13%)] Loss: 7423.576172\n",
      "Train Epoch: 123 [33024/225000 (15%)] Loss: 7363.814453\n",
      "Train Epoch: 123 [37120/225000 (16%)] Loss: 7371.566406\n",
      "Train Epoch: 123 [41216/225000 (18%)] Loss: 7408.691406\n",
      "Train Epoch: 123 [45312/225000 (20%)] Loss: 7829.392578\n",
      "Train Epoch: 123 [49408/225000 (22%)] Loss: 7383.410156\n",
      "Train Epoch: 123 [53504/225000 (24%)] Loss: 7421.994141\n",
      "Train Epoch: 123 [57600/225000 (26%)] Loss: 7498.636719\n",
      "Train Epoch: 123 [61696/225000 (27%)] Loss: 7571.423828\n",
      "Train Epoch: 123 [65792/225000 (29%)] Loss: 7404.775391\n",
      "Train Epoch: 123 [69888/225000 (31%)] Loss: 7707.429688\n",
      "Train Epoch: 123 [73984/225000 (33%)] Loss: 7620.111328\n",
      "Train Epoch: 123 [78080/225000 (35%)] Loss: 7484.667969\n",
      "Train Epoch: 123 [82176/225000 (37%)] Loss: 7527.478516\n",
      "Train Epoch: 123 [86272/225000 (38%)] Loss: 7529.347656\n",
      "Train Epoch: 123 [90368/225000 (40%)] Loss: 7370.408203\n",
      "Train Epoch: 123 [94464/225000 (42%)] Loss: 7297.875000\n",
      "Train Epoch: 123 [98560/225000 (44%)] Loss: 7549.417969\n",
      "Train Epoch: 123 [102656/225000 (46%)] Loss: 7691.070312\n",
      "Train Epoch: 123 [106752/225000 (47%)] Loss: 7516.583984\n",
      "Train Epoch: 123 [110848/225000 (49%)] Loss: 7717.992188\n",
      "Train Epoch: 123 [114944/225000 (51%)] Loss: 7538.640625\n",
      "Train Epoch: 123 [119040/225000 (53%)] Loss: 7538.748047\n",
      "Train Epoch: 123 [123136/225000 (55%)] Loss: 7403.419922\n",
      "Train Epoch: 123 [127232/225000 (57%)] Loss: 7577.308594\n",
      "Train Epoch: 123 [131328/225000 (58%)] Loss: 7874.244141\n",
      "Train Epoch: 123 [135424/225000 (60%)] Loss: 7484.603516\n",
      "Train Epoch: 123 [139520/225000 (62%)] Loss: 7549.367188\n",
      "Train Epoch: 123 [143616/225000 (64%)] Loss: 7611.402344\n",
      "Train Epoch: 123 [147712/225000 (66%)] Loss: 7462.408203\n",
      "Train Epoch: 123 [151808/225000 (67%)] Loss: 7595.943359\n",
      "Train Epoch: 123 [155904/225000 (69%)] Loss: 7410.261719\n",
      "Train Epoch: 123 [160000/225000 (71%)] Loss: 7417.042969\n",
      "Train Epoch: 123 [164096/225000 (73%)] Loss: 7553.021484\n",
      "Train Epoch: 123 [168192/225000 (75%)] Loss: 7641.724609\n",
      "Train Epoch: 123 [172288/225000 (77%)] Loss: 7468.212891\n",
      "Train Epoch: 123 [176384/225000 (78%)] Loss: 7438.964844\n",
      "Train Epoch: 123 [180480/225000 (80%)] Loss: 7534.734375\n",
      "Train Epoch: 123 [184576/225000 (82%)] Loss: 7567.482422\n",
      "Train Epoch: 123 [188672/225000 (84%)] Loss: 7175.513672\n",
      "Train Epoch: 123 [192768/225000 (86%)] Loss: 7556.919922\n",
      "Train Epoch: 123 [196864/225000 (87%)] Loss: 7486.355469\n",
      "Train Epoch: 123 [200960/225000 (89%)] Loss: 7488.072266\n",
      "Train Epoch: 123 [205056/225000 (91%)] Loss: 7576.638672\n",
      "Train Epoch: 123 [209152/225000 (93%)] Loss: 7413.025391\n",
      "Train Epoch: 123 [213248/225000 (95%)] Loss: 7467.873047\n",
      "Train Epoch: 123 [217344/225000 (97%)] Loss: 7290.443359\n",
      "Train Epoch: 123 [221440/225000 (98%)] Loss: 7404.666016\n",
      "    epoch          : 123\n",
      "    loss           : 7489.607389656215\n",
      "    val_loss       : 7483.057912198865\n",
      "Train Epoch: 124 [256/225000 (0%)] Loss: 7592.117188\n",
      "Train Epoch: 124 [4352/225000 (2%)] Loss: 7476.091797\n",
      "Train Epoch: 124 [8448/225000 (4%)] Loss: 7442.236328\n",
      "Train Epoch: 124 [12544/225000 (6%)] Loss: 7519.687500\n",
      "Train Epoch: 124 [16640/225000 (7%)] Loss: 7565.574219\n",
      "Train Epoch: 124 [20736/225000 (9%)] Loss: 7396.457031\n",
      "Train Epoch: 124 [24832/225000 (11%)] Loss: 7376.296875\n",
      "Train Epoch: 124 [28928/225000 (13%)] Loss: 7699.832031\n",
      "Train Epoch: 124 [33024/225000 (15%)] Loss: 7559.822266\n",
      "Train Epoch: 124 [37120/225000 (16%)] Loss: 7631.650391\n",
      "Train Epoch: 124 [41216/225000 (18%)] Loss: 7383.488281\n",
      "Train Epoch: 124 [45312/225000 (20%)] Loss: 7690.193359\n",
      "Train Epoch: 124 [49408/225000 (22%)] Loss: 7605.603516\n",
      "Train Epoch: 124 [53504/225000 (24%)] Loss: 7394.509766\n",
      "Train Epoch: 124 [57600/225000 (26%)] Loss: 7481.875000\n",
      "Train Epoch: 124 [61696/225000 (27%)] Loss: 7408.378906\n",
      "Train Epoch: 124 [65792/225000 (29%)] Loss: 7477.667969\n",
      "Train Epoch: 124 [69888/225000 (31%)] Loss: 7286.148438\n",
      "Train Epoch: 124 [73984/225000 (33%)] Loss: 7455.126953\n",
      "Train Epoch: 124 [78080/225000 (35%)] Loss: 7514.392578\n",
      "Train Epoch: 124 [82176/225000 (37%)] Loss: 7664.224609\n",
      "Train Epoch: 124 [86272/225000 (38%)] Loss: 7312.650391\n",
      "Train Epoch: 124 [90368/225000 (40%)] Loss: 7596.419922\n",
      "Train Epoch: 124 [94464/225000 (42%)] Loss: 7484.771484\n",
      "Train Epoch: 124 [98560/225000 (44%)] Loss: 7406.107422\n",
      "Train Epoch: 124 [102656/225000 (46%)] Loss: 7519.373047\n",
      "Train Epoch: 124 [106752/225000 (47%)] Loss: 7401.015625\n",
      "Train Epoch: 124 [110848/225000 (49%)] Loss: 7498.996094\n",
      "Train Epoch: 124 [114944/225000 (51%)] Loss: 7511.431641\n",
      "Train Epoch: 124 [119040/225000 (53%)] Loss: 7487.939453\n",
      "Train Epoch: 124 [123136/225000 (55%)] Loss: 7378.519531\n",
      "Train Epoch: 124 [127232/225000 (57%)] Loss: 7584.796875\n",
      "Train Epoch: 124 [131328/225000 (58%)] Loss: 7439.765625\n",
      "Train Epoch: 124 [135424/225000 (60%)] Loss: 7505.324219\n",
      "Train Epoch: 124 [139520/225000 (62%)] Loss: 7396.132812\n",
      "Train Epoch: 124 [143616/225000 (64%)] Loss: 7421.359375\n",
      "Train Epoch: 124 [147712/225000 (66%)] Loss: 7559.011719\n",
      "Train Epoch: 124 [151808/225000 (67%)] Loss: 7412.318359\n",
      "Train Epoch: 124 [155904/225000 (69%)] Loss: 7316.380859\n",
      "Train Epoch: 124 [160000/225000 (71%)] Loss: 7788.791016\n",
      "Train Epoch: 124 [164096/225000 (73%)] Loss: 7522.216797\n",
      "Train Epoch: 124 [168192/225000 (75%)] Loss: 7477.046875\n",
      "Train Epoch: 124 [172288/225000 (77%)] Loss: 7646.076172\n",
      "Train Epoch: 124 [176384/225000 (78%)] Loss: 7447.787109\n",
      "Train Epoch: 124 [180480/225000 (80%)] Loss: 7504.953125\n",
      "Train Epoch: 124 [184576/225000 (82%)] Loss: 7589.078125\n",
      "Train Epoch: 124 [188672/225000 (84%)] Loss: 7530.886719\n",
      "Train Epoch: 124 [192768/225000 (86%)] Loss: 7560.169922\n",
      "Train Epoch: 124 [196864/225000 (87%)] Loss: 7523.562500\n",
      "Train Epoch: 124 [200960/225000 (89%)] Loss: 7547.609375\n",
      "Train Epoch: 124 [205056/225000 (91%)] Loss: 7422.134766\n",
      "Train Epoch: 124 [209152/225000 (93%)] Loss: 7330.029297\n",
      "Train Epoch: 124 [213248/225000 (95%)] Loss: 7528.675781\n",
      "Train Epoch: 124 [217344/225000 (97%)] Loss: 7475.005859\n",
      "Train Epoch: 124 [221440/225000 (98%)] Loss: 7438.113281\n",
      "    epoch          : 124\n",
      "    loss           : 7483.205369205062\n",
      "    val_loss       : 7471.206774929348\n",
      "Train Epoch: 125 [256/225000 (0%)] Loss: 7370.562500\n",
      "Train Epoch: 125 [4352/225000 (2%)] Loss: 7421.791016\n",
      "Train Epoch: 125 [8448/225000 (4%)] Loss: 7462.007812\n",
      "Train Epoch: 125 [12544/225000 (6%)] Loss: 7360.275391\n",
      "Train Epoch: 125 [16640/225000 (7%)] Loss: 7552.908203\n",
      "Train Epoch: 125 [20736/225000 (9%)] Loss: 7386.382812\n",
      "Train Epoch: 125 [24832/225000 (11%)] Loss: 7436.609375\n",
      "Train Epoch: 125 [28928/225000 (13%)] Loss: 7432.521484\n",
      "Train Epoch: 125 [33024/225000 (15%)] Loss: 7465.525391\n",
      "Train Epoch: 125 [37120/225000 (16%)] Loss: 7578.166016\n",
      "Train Epoch: 125 [41216/225000 (18%)] Loss: 7361.863281\n",
      "Train Epoch: 125 [45312/225000 (20%)] Loss: 7487.382812\n",
      "Train Epoch: 125 [49408/225000 (22%)] Loss: 7536.316406\n",
      "Train Epoch: 125 [53504/225000 (24%)] Loss: 7455.970703\n",
      "Train Epoch: 125 [57600/225000 (26%)] Loss: 7679.728516\n",
      "Train Epoch: 125 [61696/225000 (27%)] Loss: 7319.244141\n",
      "Train Epoch: 125 [65792/225000 (29%)] Loss: 7396.812500\n",
      "Train Epoch: 125 [69888/225000 (31%)] Loss: 7377.945312\n",
      "Train Epoch: 125 [73984/225000 (33%)] Loss: 7581.591797\n",
      "Train Epoch: 125 [78080/225000 (35%)] Loss: 7621.228516\n",
      "Train Epoch: 125 [82176/225000 (37%)] Loss: 7466.914062\n",
      "Train Epoch: 125 [86272/225000 (38%)] Loss: 7503.687500\n",
      "Train Epoch: 125 [90368/225000 (40%)] Loss: 7509.203125\n",
      "Train Epoch: 125 [94464/225000 (42%)] Loss: 7661.173828\n",
      "Train Epoch: 125 [98560/225000 (44%)] Loss: 7349.574219\n",
      "Train Epoch: 125 [102656/225000 (46%)] Loss: 7412.781250\n",
      "Train Epoch: 125 [106752/225000 (47%)] Loss: 7280.027344\n",
      "Train Epoch: 125 [110848/225000 (49%)] Loss: 7274.650391\n",
      "Train Epoch: 125 [114944/225000 (51%)] Loss: 7567.136719\n",
      "Train Epoch: 125 [119040/225000 (53%)] Loss: 7441.810547\n",
      "Train Epoch: 125 [123136/225000 (55%)] Loss: 7626.015625\n",
      "Train Epoch: 125 [127232/225000 (57%)] Loss: 7511.199219\n",
      "Train Epoch: 125 [131328/225000 (58%)] Loss: 7498.968750\n",
      "Train Epoch: 125 [135424/225000 (60%)] Loss: 7480.238281\n",
      "Train Epoch: 125 [139520/225000 (62%)] Loss: 7521.728516\n",
      "Train Epoch: 125 [143616/225000 (64%)] Loss: 7340.265625\n",
      "Train Epoch: 125 [147712/225000 (66%)] Loss: 7561.253906\n",
      "Train Epoch: 125 [151808/225000 (67%)] Loss: 7445.986328\n",
      "Train Epoch: 125 [155904/225000 (69%)] Loss: 7291.667969\n",
      "Train Epoch: 125 [160000/225000 (71%)] Loss: 7349.035156\n",
      "Train Epoch: 125 [164096/225000 (73%)] Loss: 7462.583984\n",
      "Train Epoch: 125 [168192/225000 (75%)] Loss: 7396.603516\n",
      "Train Epoch: 125 [172288/225000 (77%)] Loss: 7582.009766\n",
      "Train Epoch: 125 [176384/225000 (78%)] Loss: 7388.234375\n",
      "Train Epoch: 125 [180480/225000 (80%)] Loss: 7550.931641\n",
      "Train Epoch: 125 [184576/225000 (82%)] Loss: 7454.335938\n",
      "Train Epoch: 125 [188672/225000 (84%)] Loss: 7499.412109\n",
      "Train Epoch: 125 [192768/225000 (86%)] Loss: 7641.640625\n",
      "Train Epoch: 125 [196864/225000 (87%)] Loss: 7464.478516\n",
      "Train Epoch: 125 [200960/225000 (89%)] Loss: 7409.173828\n",
      "Train Epoch: 125 [205056/225000 (91%)] Loss: 7625.914062\n",
      "Train Epoch: 125 [209152/225000 (93%)] Loss: 7315.568359\n",
      "Train Epoch: 125 [213248/225000 (95%)] Loss: 7407.107422\n",
      "Train Epoch: 125 [217344/225000 (97%)] Loss: 7595.382812\n",
      "Train Epoch: 125 [221440/225000 (98%)] Loss: 7669.691406\n",
      "    epoch          : 125\n",
      "    loss           : 7488.657318774886\n",
      "    val_loss       : 7468.558037538918\n",
      "Train Epoch: 126 [256/225000 (0%)] Loss: 7615.347656\n",
      "Train Epoch: 126 [4352/225000 (2%)] Loss: 7363.833984\n",
      "Train Epoch: 126 [8448/225000 (4%)] Loss: 7484.763672\n",
      "Train Epoch: 126 [12544/225000 (6%)] Loss: 7584.222656\n",
      "Train Epoch: 126 [16640/225000 (7%)] Loss: 7449.513672\n",
      "Train Epoch: 126 [20736/225000 (9%)] Loss: 7661.724609\n",
      "Train Epoch: 126 [24832/225000 (11%)] Loss: 7603.037109\n",
      "Train Epoch: 126 [28928/225000 (13%)] Loss: 7528.179688\n",
      "Train Epoch: 126 [33024/225000 (15%)] Loss: 7733.896484\n",
      "Train Epoch: 126 [37120/225000 (16%)] Loss: 7396.531250\n",
      "Train Epoch: 126 [41216/225000 (18%)] Loss: 7535.306641\n",
      "Train Epoch: 126 [45312/225000 (20%)] Loss: 7583.027344\n",
      "Train Epoch: 126 [49408/225000 (22%)] Loss: 7365.138672\n",
      "Train Epoch: 126 [53504/225000 (24%)] Loss: 7576.658203\n",
      "Train Epoch: 126 [57600/225000 (26%)] Loss: 7425.830078\n",
      "Train Epoch: 126 [61696/225000 (27%)] Loss: 7571.583984\n",
      "Train Epoch: 126 [65792/225000 (29%)] Loss: 7475.857422\n",
      "Train Epoch: 126 [69888/225000 (31%)] Loss: 7472.710938\n",
      "Train Epoch: 126 [73984/225000 (33%)] Loss: 7640.103516\n",
      "Train Epoch: 126 [78080/225000 (35%)] Loss: 7474.693359\n",
      "Train Epoch: 126 [82176/225000 (37%)] Loss: 7411.423828\n",
      "Train Epoch: 126 [86272/225000 (38%)] Loss: 7319.318359\n",
      "Train Epoch: 126 [90368/225000 (40%)] Loss: 7529.871094\n",
      "Train Epoch: 126 [94464/225000 (42%)] Loss: 7373.400391\n",
      "Train Epoch: 126 [98560/225000 (44%)] Loss: 7230.269531\n",
      "Train Epoch: 126 [102656/225000 (46%)] Loss: 7632.826172\n",
      "Train Epoch: 126 [106752/225000 (47%)] Loss: 7493.123047\n",
      "Train Epoch: 126 [110848/225000 (49%)] Loss: 7478.146484\n",
      "Train Epoch: 126 [114944/225000 (51%)] Loss: 7417.279297\n",
      "Train Epoch: 126 [119040/225000 (53%)] Loss: 7388.357422\n",
      "Train Epoch: 126 [123136/225000 (55%)] Loss: 7663.990234\n",
      "Train Epoch: 126 [127232/225000 (57%)] Loss: 7352.179688\n",
      "Train Epoch: 126 [131328/225000 (58%)] Loss: 7473.382812\n",
      "Train Epoch: 126 [135424/225000 (60%)] Loss: 7618.468750\n",
      "Train Epoch: 126 [139520/225000 (62%)] Loss: 7383.853516\n",
      "Train Epoch: 126 [143616/225000 (64%)] Loss: 7435.701172\n",
      "Train Epoch: 126 [147712/225000 (66%)] Loss: 7485.642578\n",
      "Train Epoch: 126 [151808/225000 (67%)] Loss: 7524.017578\n",
      "Train Epoch: 126 [155904/225000 (69%)] Loss: 7508.529297\n",
      "Train Epoch: 126 [160000/225000 (71%)] Loss: 7522.695312\n",
      "Train Epoch: 126 [164096/225000 (73%)] Loss: 7558.593750\n",
      "Train Epoch: 126 [168192/225000 (75%)] Loss: 7592.287109\n",
      "Train Epoch: 126 [172288/225000 (77%)] Loss: 7432.941406\n",
      "Train Epoch: 126 [176384/225000 (78%)] Loss: 7300.375000\n",
      "Train Epoch: 126 [180480/225000 (80%)] Loss: 7451.541016\n",
      "Train Epoch: 126 [184576/225000 (82%)] Loss: 7536.279297\n",
      "Train Epoch: 126 [188672/225000 (84%)] Loss: 7408.732422\n",
      "Train Epoch: 126 [192768/225000 (86%)] Loss: 7352.033203\n",
      "Train Epoch: 126 [196864/225000 (87%)] Loss: 7495.251953\n",
      "Train Epoch: 126 [200960/225000 (89%)] Loss: 7467.125000\n",
      "Train Epoch: 126 [205056/225000 (91%)] Loss: 7510.062500\n",
      "Train Epoch: 126 [209152/225000 (93%)] Loss: 7287.492188\n",
      "Train Epoch: 126 [213248/225000 (95%)] Loss: 7394.212891\n",
      "Train Epoch: 126 [217344/225000 (97%)] Loss: 7417.708984\n",
      "Train Epoch: 126 [221440/225000 (98%)] Loss: 7401.078125\n",
      "    epoch          : 126\n",
      "    loss           : 7504.454734828285\n",
      "    val_loss       : 7459.4223629138905\n",
      "Train Epoch: 127 [256/225000 (0%)] Loss: 7204.960938\n",
      "Train Epoch: 127 [4352/225000 (2%)] Loss: 7453.500000\n",
      "Train Epoch: 127 [8448/225000 (4%)] Loss: 7441.625000\n",
      "Train Epoch: 127 [12544/225000 (6%)] Loss: 7685.626953\n",
      "Train Epoch: 127 [16640/225000 (7%)] Loss: 7263.957031\n",
      "Train Epoch: 127 [20736/225000 (9%)] Loss: 7544.007812\n",
      "Train Epoch: 127 [24832/225000 (11%)] Loss: 7432.070312\n",
      "Train Epoch: 127 [28928/225000 (13%)] Loss: 7578.015625\n",
      "Train Epoch: 127 [33024/225000 (15%)] Loss: 7402.404297\n",
      "Train Epoch: 127 [37120/225000 (16%)] Loss: 7596.546875\n",
      "Train Epoch: 127 [41216/225000 (18%)] Loss: 7573.552734\n",
      "Train Epoch: 127 [45312/225000 (20%)] Loss: 7404.976562\n",
      "Train Epoch: 127 [49408/225000 (22%)] Loss: 7651.478516\n",
      "Train Epoch: 127 [53504/225000 (24%)] Loss: 7521.511719\n",
      "Train Epoch: 127 [57600/225000 (26%)] Loss: 7537.617188\n",
      "Train Epoch: 127 [61696/225000 (27%)] Loss: 7510.697266\n",
      "Train Epoch: 127 [65792/225000 (29%)] Loss: 7384.955078\n",
      "Train Epoch: 127 [69888/225000 (31%)] Loss: 7592.273438\n",
      "Train Epoch: 127 [73984/225000 (33%)] Loss: 7457.791016\n",
      "Train Epoch: 127 [78080/225000 (35%)] Loss: 7559.767578\n",
      "Train Epoch: 127 [82176/225000 (37%)] Loss: 7422.275391\n",
      "Train Epoch: 127 [86272/225000 (38%)] Loss: 7491.724609\n",
      "Train Epoch: 127 [90368/225000 (40%)] Loss: 7467.017578\n",
      "Train Epoch: 127 [94464/225000 (42%)] Loss: 7564.945312\n",
      "Train Epoch: 127 [98560/225000 (44%)] Loss: 7439.699219\n",
      "Train Epoch: 127 [102656/225000 (46%)] Loss: 7337.111328\n",
      "Train Epoch: 127 [106752/225000 (47%)] Loss: 7477.767578\n",
      "Train Epoch: 127 [110848/225000 (49%)] Loss: 7369.576172\n",
      "Train Epoch: 127 [114944/225000 (51%)] Loss: 7531.917969\n",
      "Train Epoch: 127 [119040/225000 (53%)] Loss: 7699.361328\n",
      "Train Epoch: 127 [123136/225000 (55%)] Loss: 7529.527344\n",
      "Train Epoch: 127 [127232/225000 (57%)] Loss: 7468.849609\n",
      "Train Epoch: 127 [131328/225000 (58%)] Loss: 7475.978516\n",
      "Train Epoch: 127 [135424/225000 (60%)] Loss: 7557.220703\n",
      "Train Epoch: 127 [139520/225000 (62%)] Loss: 7497.013672\n",
      "Train Epoch: 127 [143616/225000 (64%)] Loss: 7311.396484\n",
      "Train Epoch: 127 [147712/225000 (66%)] Loss: 7681.652344\n",
      "Train Epoch: 127 [151808/225000 (67%)] Loss: 7436.812500\n",
      "Train Epoch: 127 [155904/225000 (69%)] Loss: 7350.808594\n",
      "Train Epoch: 127 [160000/225000 (71%)] Loss: 7509.742188\n",
      "Train Epoch: 127 [164096/225000 (73%)] Loss: 7605.441406\n",
      "Train Epoch: 127 [168192/225000 (75%)] Loss: 7409.480469\n",
      "Train Epoch: 127 [172288/225000 (77%)] Loss: 7450.072266\n",
      "Train Epoch: 127 [176384/225000 (78%)] Loss: 7506.333984\n",
      "Train Epoch: 127 [180480/225000 (80%)] Loss: 7245.462891\n",
      "Train Epoch: 127 [184576/225000 (82%)] Loss: 7580.419922\n",
      "Train Epoch: 127 [188672/225000 (84%)] Loss: 7471.246094\n",
      "Train Epoch: 127 [192768/225000 (86%)] Loss: 7494.066406\n",
      "Train Epoch: 127 [196864/225000 (87%)] Loss: 7419.398438\n",
      "Train Epoch: 127 [200960/225000 (89%)] Loss: 7504.845703\n",
      "Train Epoch: 127 [205056/225000 (91%)] Loss: 7426.777344\n",
      "Train Epoch: 127 [209152/225000 (93%)] Loss: 7382.921875\n",
      "Train Epoch: 127 [213248/225000 (95%)] Loss: 7470.962891\n",
      "Train Epoch: 127 [217344/225000 (97%)] Loss: 7653.615234\n",
      "Train Epoch: 127 [221440/225000 (98%)] Loss: 7509.554688\n",
      "    epoch          : 127\n",
      "    loss           : 7479.016479353313\n",
      "    val_loss       : 7459.8431834809635\n",
      "Train Epoch: 128 [256/225000 (0%)] Loss: 7469.498047\n",
      "Train Epoch: 128 [4352/225000 (2%)] Loss: 7413.285156\n",
      "Train Epoch: 128 [8448/225000 (4%)] Loss: 7453.675781\n",
      "Train Epoch: 128 [12544/225000 (6%)] Loss: 7340.154297\n",
      "Train Epoch: 128 [16640/225000 (7%)] Loss: 7430.259766\n",
      "Train Epoch: 128 [20736/225000 (9%)] Loss: 7468.666016\n",
      "Train Epoch: 128 [24832/225000 (11%)] Loss: 7434.179688\n",
      "Train Epoch: 128 [28928/225000 (13%)] Loss: 7559.056641\n",
      "Train Epoch: 128 [33024/225000 (15%)] Loss: 7462.343750\n",
      "Train Epoch: 128 [37120/225000 (16%)] Loss: 7337.382812\n",
      "Train Epoch: 128 [41216/225000 (18%)] Loss: 7431.833984\n",
      "Train Epoch: 128 [45312/225000 (20%)] Loss: 7275.640625\n",
      "Train Epoch: 128 [49408/225000 (22%)] Loss: 7552.240234\n",
      "Train Epoch: 128 [53504/225000 (24%)] Loss: 7449.455078\n",
      "Train Epoch: 128 [57600/225000 (26%)] Loss: 7553.710938\n",
      "Train Epoch: 128 [61696/225000 (27%)] Loss: 7303.896484\n",
      "Train Epoch: 128 [65792/225000 (29%)] Loss: 7515.972656\n",
      "Train Epoch: 128 [69888/225000 (31%)] Loss: 7340.722656\n",
      "Train Epoch: 128 [73984/225000 (33%)] Loss: 7533.744141\n",
      "Train Epoch: 128 [78080/225000 (35%)] Loss: 7560.570312\n",
      "Train Epoch: 128 [82176/225000 (37%)] Loss: 7510.720703\n",
      "Train Epoch: 128 [86272/225000 (38%)] Loss: 7284.125000\n",
      "Train Epoch: 128 [90368/225000 (40%)] Loss: 7436.533203\n",
      "Train Epoch: 128 [94464/225000 (42%)] Loss: 7349.630859\n",
      "Train Epoch: 128 [98560/225000 (44%)] Loss: 7382.359375\n",
      "Train Epoch: 128 [102656/225000 (46%)] Loss: 7431.847656\n",
      "Train Epoch: 128 [106752/225000 (47%)] Loss: 7414.523438\n",
      "Train Epoch: 128 [110848/225000 (49%)] Loss: 7592.431641\n",
      "Train Epoch: 128 [114944/225000 (51%)] Loss: 7463.388672\n",
      "Train Epoch: 128 [119040/225000 (53%)] Loss: 7613.708984\n",
      "Train Epoch: 128 [123136/225000 (55%)] Loss: 7573.523438\n",
      "Train Epoch: 128 [127232/225000 (57%)] Loss: 7589.955078\n",
      "Train Epoch: 128 [131328/225000 (58%)] Loss: 7408.394531\n",
      "Train Epoch: 128 [135424/225000 (60%)] Loss: 7530.431641\n",
      "Train Epoch: 128 [139520/225000 (62%)] Loss: 7508.312500\n",
      "Train Epoch: 128 [143616/225000 (64%)] Loss: 7476.343750\n",
      "Train Epoch: 128 [147712/225000 (66%)] Loss: 7465.802734\n",
      "Train Epoch: 128 [151808/225000 (67%)] Loss: 7335.218750\n",
      "Train Epoch: 128 [155904/225000 (69%)] Loss: 7423.808594\n",
      "Train Epoch: 128 [160000/225000 (71%)] Loss: 7311.851562\n",
      "Train Epoch: 128 [164096/225000 (73%)] Loss: 7472.757812\n",
      "Train Epoch: 128 [168192/225000 (75%)] Loss: 7553.339844\n",
      "Train Epoch: 128 [172288/225000 (77%)] Loss: 7447.484375\n",
      "Train Epoch: 128 [176384/225000 (78%)] Loss: 7351.250000\n",
      "Train Epoch: 128 [180480/225000 (80%)] Loss: 7486.447266\n",
      "Train Epoch: 128 [184576/225000 (82%)] Loss: 7415.009766\n",
      "Train Epoch: 128 [188672/225000 (84%)] Loss: 7506.199219\n",
      "Train Epoch: 128 [192768/225000 (86%)] Loss: 7594.933594\n",
      "Train Epoch: 128 [196864/225000 (87%)] Loss: 7578.625000\n",
      "Train Epoch: 128 [200960/225000 (89%)] Loss: 7367.970703\n",
      "Train Epoch: 128 [205056/225000 (91%)] Loss: 7517.669922\n",
      "Train Epoch: 128 [209152/225000 (93%)] Loss: 7394.609375\n",
      "Train Epoch: 128 [213248/225000 (95%)] Loss: 7447.310547\n",
      "Train Epoch: 128 [217344/225000 (97%)] Loss: 7250.648438\n",
      "Train Epoch: 128 [221440/225000 (98%)] Loss: 7378.630859\n",
      "    epoch          : 128\n",
      "    loss           : 7486.152244871658\n",
      "    val_loss       : 7450.330001109717\n",
      "Train Epoch: 129 [256/225000 (0%)] Loss: 7377.607422\n",
      "Train Epoch: 129 [4352/225000 (2%)] Loss: 7271.781250\n",
      "Train Epoch: 129 [8448/225000 (4%)] Loss: 7383.205078\n",
      "Train Epoch: 129 [12544/225000 (6%)] Loss: 7434.619141\n",
      "Train Epoch: 129 [16640/225000 (7%)] Loss: 7439.574219\n",
      "Train Epoch: 129 [20736/225000 (9%)] Loss: 7607.966797\n",
      "Train Epoch: 129 [24832/225000 (11%)] Loss: 7568.941406\n",
      "Train Epoch: 129 [28928/225000 (13%)] Loss: 7455.626953\n",
      "Train Epoch: 129 [33024/225000 (15%)] Loss: 7588.394531\n",
      "Train Epoch: 129 [37120/225000 (16%)] Loss: 7403.345703\n",
      "Train Epoch: 129 [41216/225000 (18%)] Loss: 7624.138672\n",
      "Train Epoch: 129 [45312/225000 (20%)] Loss: 7493.316406\n",
      "Train Epoch: 129 [49408/225000 (22%)] Loss: 7364.707031\n",
      "Train Epoch: 129 [53504/225000 (24%)] Loss: 7509.003906\n",
      "Train Epoch: 129 [57600/225000 (26%)] Loss: 7505.291016\n",
      "Train Epoch: 129 [61696/225000 (27%)] Loss: 7509.474609\n",
      "Train Epoch: 129 [65792/225000 (29%)] Loss: 7472.449219\n",
      "Train Epoch: 129 [69888/225000 (31%)] Loss: 7407.689453\n",
      "Train Epoch: 129 [73984/225000 (33%)] Loss: 7206.673828\n",
      "Train Epoch: 129 [78080/225000 (35%)] Loss: 7492.085938\n",
      "Train Epoch: 129 [82176/225000 (37%)] Loss: 7526.621094\n",
      "Train Epoch: 129 [86272/225000 (38%)] Loss: 7504.134766\n",
      "Train Epoch: 129 [90368/225000 (40%)] Loss: 7449.810547\n",
      "Train Epoch: 129 [94464/225000 (42%)] Loss: 7352.351562\n",
      "Train Epoch: 129 [98560/225000 (44%)] Loss: 7347.320312\n",
      "Train Epoch: 129 [102656/225000 (46%)] Loss: 7562.650391\n",
      "Train Epoch: 129 [106752/225000 (47%)] Loss: 7611.974609\n",
      "Train Epoch: 129 [110848/225000 (49%)] Loss: 7532.658203\n",
      "Train Epoch: 129 [114944/225000 (51%)] Loss: 7704.628906\n",
      "Train Epoch: 129 [119040/225000 (53%)] Loss: 7403.056641\n",
      "Train Epoch: 129 [123136/225000 (55%)] Loss: 7586.777344\n",
      "Train Epoch: 129 [127232/225000 (57%)] Loss: 7572.599609\n",
      "Train Epoch: 129 [131328/225000 (58%)] Loss: 7573.525391\n",
      "Train Epoch: 129 [135424/225000 (60%)] Loss: 7278.275391\n",
      "Train Epoch: 129 [139520/225000 (62%)] Loss: 7496.996094\n",
      "Train Epoch: 129 [143616/225000 (64%)] Loss: 7386.943359\n",
      "Train Epoch: 129 [147712/225000 (66%)] Loss: 7535.316406\n",
      "Train Epoch: 129 [151808/225000 (67%)] Loss: 7378.044922\n",
      "Train Epoch: 129 [155904/225000 (69%)] Loss: 7498.402344\n",
      "Train Epoch: 129 [160000/225000 (71%)] Loss: 7417.699219\n",
      "Train Epoch: 129 [164096/225000 (73%)] Loss: 7471.835938\n",
      "Train Epoch: 129 [168192/225000 (75%)] Loss: 7468.638672\n",
      "Train Epoch: 129 [172288/225000 (77%)] Loss: 7415.744141\n",
      "Train Epoch: 129 [176384/225000 (78%)] Loss: 7663.259766\n",
      "Train Epoch: 129 [180480/225000 (80%)] Loss: 7558.439453\n",
      "Train Epoch: 129 [184576/225000 (82%)] Loss: 7448.462891\n",
      "Train Epoch: 129 [188672/225000 (84%)] Loss: 7517.712891\n",
      "Train Epoch: 129 [192768/225000 (86%)] Loss: 7482.089844\n",
      "Train Epoch: 129 [196864/225000 (87%)] Loss: 7447.593750\n",
      "Train Epoch: 129 [200960/225000 (89%)] Loss: 7440.597656\n",
      "Train Epoch: 129 [205056/225000 (91%)] Loss: 7573.042969\n",
      "Train Epoch: 129 [209152/225000 (93%)] Loss: 7386.720703\n",
      "Train Epoch: 129 [213248/225000 (95%)] Loss: 7340.197266\n",
      "Train Epoch: 129 [217344/225000 (97%)] Loss: 7495.703125\n",
      "Train Epoch: 129 [221440/225000 (98%)] Loss: 7482.974609\n",
      "    epoch          : 129\n",
      "    loss           : 7456.3316690664105\n",
      "    val_loss       : 7546.181976738025\n",
      "Train Epoch: 130 [256/225000 (0%)] Loss: 7509.941406\n",
      "Train Epoch: 130 [4352/225000 (2%)] Loss: 7474.025391\n",
      "Train Epoch: 130 [8448/225000 (4%)] Loss: 7603.408203\n",
      "Train Epoch: 130 [12544/225000 (6%)] Loss: 7618.343750\n",
      "Train Epoch: 130 [16640/225000 (7%)] Loss: 7419.130859\n",
      "Train Epoch: 130 [20736/225000 (9%)] Loss: 7494.240234\n",
      "Train Epoch: 130 [24832/225000 (11%)] Loss: 7501.953125\n",
      "Train Epoch: 130 [28928/225000 (13%)] Loss: 7597.304688\n",
      "Train Epoch: 130 [33024/225000 (15%)] Loss: 7492.083984\n",
      "Train Epoch: 130 [37120/225000 (16%)] Loss: 7476.886719\n",
      "Train Epoch: 130 [41216/225000 (18%)] Loss: 7351.160156\n",
      "Train Epoch: 130 [45312/225000 (20%)] Loss: 7487.580078\n",
      "Train Epoch: 130 [49408/225000 (22%)] Loss: 7409.410156\n",
      "Train Epoch: 130 [53504/225000 (24%)] Loss: 7503.396484\n",
      "Train Epoch: 130 [57600/225000 (26%)] Loss: 7522.775391\n",
      "Train Epoch: 130 [61696/225000 (27%)] Loss: 7307.408203\n",
      "Train Epoch: 130 [65792/225000 (29%)] Loss: 7417.906250\n",
      "Train Epoch: 130 [69888/225000 (31%)] Loss: 7530.005859\n",
      "Train Epoch: 130 [73984/225000 (33%)] Loss: 7311.056641\n",
      "Train Epoch: 130 [78080/225000 (35%)] Loss: 7601.037109\n",
      "Train Epoch: 130 [82176/225000 (37%)] Loss: 7567.541016\n",
      "Train Epoch: 130 [86272/225000 (38%)] Loss: 7399.355469\n",
      "Train Epoch: 130 [90368/225000 (40%)] Loss: 7403.537109\n",
      "Train Epoch: 130 [94464/225000 (42%)] Loss: 7474.742188\n",
      "Train Epoch: 130 [98560/225000 (44%)] Loss: 7463.818359\n",
      "Train Epoch: 130 [102656/225000 (46%)] Loss: 7505.238281\n",
      "Train Epoch: 130 [106752/225000 (47%)] Loss: 7311.828125\n",
      "Train Epoch: 130 [110848/225000 (49%)] Loss: 7499.480469\n",
      "Train Epoch: 130 [114944/225000 (51%)] Loss: 7686.605469\n",
      "Train Epoch: 130 [119040/225000 (53%)] Loss: 7382.263672\n",
      "Train Epoch: 130 [123136/225000 (55%)] Loss: 7428.208984\n",
      "Train Epoch: 130 [127232/225000 (57%)] Loss: 7372.046875\n",
      "Train Epoch: 130 [131328/225000 (58%)] Loss: 7381.996094\n",
      "Train Epoch: 130 [135424/225000 (60%)] Loss: 7475.943359\n",
      "Train Epoch: 130 [139520/225000 (62%)] Loss: 7501.044922\n",
      "Train Epoch: 130 [143616/225000 (64%)] Loss: 7351.574219\n",
      "Train Epoch: 130 [147712/225000 (66%)] Loss: 7118.421875\n",
      "Train Epoch: 130 [151808/225000 (67%)] Loss: 7458.125000\n",
      "Train Epoch: 130 [155904/225000 (69%)] Loss: 7457.845703\n",
      "Train Epoch: 130 [160000/225000 (71%)] Loss: 7515.568359\n",
      "Train Epoch: 130 [164096/225000 (73%)] Loss: 7452.398438\n",
      "Train Epoch: 130 [168192/225000 (75%)] Loss: 7590.226562\n",
      "Train Epoch: 130 [172288/225000 (77%)] Loss: 7405.556641\n",
      "Train Epoch: 130 [176384/225000 (78%)] Loss: 7510.406250\n",
      "Train Epoch: 130 [180480/225000 (80%)] Loss: 7397.833984\n",
      "Train Epoch: 130 [184576/225000 (82%)] Loss: 7323.812500\n",
      "Train Epoch: 130 [188672/225000 (84%)] Loss: 7280.724609\n",
      "Train Epoch: 130 [192768/225000 (86%)] Loss: 7500.976562\n",
      "Train Epoch: 130 [196864/225000 (87%)] Loss: 7308.671875\n",
      "Train Epoch: 130 [200960/225000 (89%)] Loss: 7452.871094\n",
      "Train Epoch: 130 [205056/225000 (91%)] Loss: 7401.953125\n",
      "Train Epoch: 130 [209152/225000 (93%)] Loss: 7540.324219\n",
      "Train Epoch: 130 [213248/225000 (95%)] Loss: 7545.644531\n",
      "Train Epoch: 130 [217344/225000 (97%)] Loss: 7341.439453\n",
      "Train Epoch: 130 [221440/225000 (98%)] Loss: 7327.400391\n",
      "    epoch          : 130\n",
      "    loss           : 7451.187358903939\n",
      "    val_loss       : 7439.568909255826\n",
      "Train Epoch: 131 [256/225000 (0%)] Loss: 7555.880859\n",
      "Train Epoch: 131 [4352/225000 (2%)] Loss: 7330.833984\n",
      "Train Epoch: 131 [8448/225000 (4%)] Loss: 7510.787109\n",
      "Train Epoch: 131 [12544/225000 (6%)] Loss: 7567.257812\n",
      "Train Epoch: 131 [16640/225000 (7%)] Loss: 7501.644531\n",
      "Train Epoch: 131 [20736/225000 (9%)] Loss: 7311.511719\n",
      "Train Epoch: 131 [24832/225000 (11%)] Loss: 7328.580078\n",
      "Train Epoch: 131 [28928/225000 (13%)] Loss: 7365.115234\n",
      "Train Epoch: 131 [33024/225000 (15%)] Loss: 7456.945312\n",
      "Train Epoch: 131 [37120/225000 (16%)] Loss: 7642.347656\n",
      "Train Epoch: 131 [41216/225000 (18%)] Loss: 7382.718750\n",
      "Train Epoch: 131 [45312/225000 (20%)] Loss: 7457.552734\n",
      "Train Epoch: 131 [49408/225000 (22%)] Loss: 7417.843750\n",
      "Train Epoch: 131 [53504/225000 (24%)] Loss: 7476.396484\n",
      "Train Epoch: 131 [57600/225000 (26%)] Loss: 7367.546875\n",
      "Train Epoch: 131 [61696/225000 (27%)] Loss: 7445.148438\n",
      "Train Epoch: 131 [65792/225000 (29%)] Loss: 7415.691406\n",
      "Train Epoch: 131 [69888/225000 (31%)] Loss: 7533.169922\n",
      "Train Epoch: 131 [73984/225000 (33%)] Loss: 7408.054688\n",
      "Train Epoch: 131 [78080/225000 (35%)] Loss: 7435.654297\n",
      "Train Epoch: 131 [82176/225000 (37%)] Loss: 7633.550781\n",
      "Train Epoch: 131 [86272/225000 (38%)] Loss: 7461.730469\n",
      "Train Epoch: 131 [90368/225000 (40%)] Loss: 7469.093750\n",
      "Train Epoch: 131 [94464/225000 (42%)] Loss: 7377.404297\n",
      "Train Epoch: 131 [98560/225000 (44%)] Loss: 7540.900391\n",
      "Train Epoch: 131 [102656/225000 (46%)] Loss: 7525.994141\n",
      "Train Epoch: 131 [106752/225000 (47%)] Loss: 7401.191406\n",
      "Train Epoch: 131 [110848/225000 (49%)] Loss: 7505.064453\n",
      "Train Epoch: 131 [114944/225000 (51%)] Loss: 7419.562500\n",
      "Train Epoch: 131 [119040/225000 (53%)] Loss: 7409.708984\n",
      "Train Epoch: 131 [123136/225000 (55%)] Loss: 7591.824219\n",
      "Train Epoch: 131 [127232/225000 (57%)] Loss: 7422.382812\n",
      "Train Epoch: 131 [131328/225000 (58%)] Loss: 7539.355469\n",
      "Train Epoch: 131 [135424/225000 (60%)] Loss: 7382.318359\n",
      "Train Epoch: 131 [139520/225000 (62%)] Loss: 7527.673828\n",
      "Train Epoch: 131 [143616/225000 (64%)] Loss: 7479.265625\n",
      "Train Epoch: 131 [147712/225000 (66%)] Loss: 7422.910156\n",
      "Train Epoch: 131 [151808/225000 (67%)] Loss: 7496.853516\n",
      "Train Epoch: 131 [155904/225000 (69%)] Loss: 7414.515625\n",
      "Train Epoch: 131 [160000/225000 (71%)] Loss: 7321.140625\n",
      "Train Epoch: 131 [164096/225000 (73%)] Loss: 7428.726562\n",
      "Train Epoch: 131 [168192/225000 (75%)] Loss: 7528.751953\n",
      "Train Epoch: 131 [172288/225000 (77%)] Loss: 7533.835938\n",
      "Train Epoch: 131 [176384/225000 (78%)] Loss: 7622.671875\n",
      "Train Epoch: 131 [180480/225000 (80%)] Loss: 7618.548828\n",
      "Train Epoch: 131 [184576/225000 (82%)] Loss: 7391.037109\n",
      "Train Epoch: 131 [188672/225000 (84%)] Loss: 7624.705078\n",
      "Train Epoch: 131 [192768/225000 (86%)] Loss: 7393.802734\n",
      "Train Epoch: 131 [196864/225000 (87%)] Loss: 7513.046875\n",
      "Train Epoch: 131 [200960/225000 (89%)] Loss: 7360.292969\n",
      "Train Epoch: 131 [205056/225000 (91%)] Loss: 7413.054688\n",
      "Train Epoch: 131 [209152/225000 (93%)] Loss: 7332.947266\n",
      "Train Epoch: 131 [213248/225000 (95%)] Loss: 7457.138672\n",
      "Train Epoch: 131 [217344/225000 (97%)] Loss: 7341.722656\n",
      "Train Epoch: 131 [221440/225000 (98%)] Loss: 7357.062500\n",
      "    epoch          : 131\n",
      "    loss           : 7452.3744100629265\n",
      "    val_loss       : 7437.493781612844\n",
      "Train Epoch: 132 [256/225000 (0%)] Loss: 7337.046875\n",
      "Train Epoch: 132 [4352/225000 (2%)] Loss: 7533.080078\n",
      "Train Epoch: 132 [8448/225000 (4%)] Loss: 7523.142578\n",
      "Train Epoch: 132 [12544/225000 (6%)] Loss: 7403.507812\n",
      "Train Epoch: 132 [16640/225000 (7%)] Loss: 7389.873047\n",
      "Train Epoch: 132 [20736/225000 (9%)] Loss: 7281.679688\n",
      "Train Epoch: 132 [24832/225000 (11%)] Loss: 7452.332031\n",
      "Train Epoch: 132 [28928/225000 (13%)] Loss: 7378.923828\n",
      "Train Epoch: 132 [33024/225000 (15%)] Loss: 7441.718750\n",
      "Train Epoch: 132 [37120/225000 (16%)] Loss: 7553.232422\n",
      "Train Epoch: 132 [41216/225000 (18%)] Loss: 7348.931641\n",
      "Train Epoch: 132 [45312/225000 (20%)] Loss: 7406.167969\n",
      "Train Epoch: 132 [49408/225000 (22%)] Loss: 7531.970703\n",
      "Train Epoch: 132 [53504/225000 (24%)] Loss: 7625.539062\n",
      "Train Epoch: 132 [57600/225000 (26%)] Loss: 7401.128906\n",
      "Train Epoch: 132 [61696/225000 (27%)] Loss: 7548.068359\n",
      "Train Epoch: 132 [65792/225000 (29%)] Loss: 7260.111328\n",
      "Train Epoch: 132 [69888/225000 (31%)] Loss: 7487.623047\n",
      "Train Epoch: 132 [73984/225000 (33%)] Loss: 7429.220703\n",
      "Train Epoch: 132 [78080/225000 (35%)] Loss: 7458.287109\n",
      "Train Epoch: 132 [82176/225000 (37%)] Loss: 7279.687500\n",
      "Train Epoch: 132 [86272/225000 (38%)] Loss: 7417.287109\n",
      "Train Epoch: 132 [90368/225000 (40%)] Loss: 7592.814453\n",
      "Train Epoch: 132 [94464/225000 (42%)] Loss: 7372.312500\n",
      "Train Epoch: 132 [98560/225000 (44%)] Loss: 7499.962891\n",
      "Train Epoch: 132 [102656/225000 (46%)] Loss: 7507.591797\n",
      "Train Epoch: 132 [106752/225000 (47%)] Loss: 7340.662109\n",
      "Train Epoch: 132 [110848/225000 (49%)] Loss: 7664.523438\n",
      "Train Epoch: 132 [114944/225000 (51%)] Loss: 7411.792969\n",
      "Train Epoch: 132 [119040/225000 (53%)] Loss: 7470.587891\n",
      "Train Epoch: 132 [123136/225000 (55%)] Loss: 7347.902344\n",
      "Train Epoch: 132 [127232/225000 (57%)] Loss: 7496.689453\n",
      "Train Epoch: 132 [131328/225000 (58%)] Loss: 7450.763672\n",
      "Train Epoch: 132 [135424/225000 (60%)] Loss: 7271.949219\n",
      "Train Epoch: 132 [139520/225000 (62%)] Loss: 7382.216797\n",
      "Train Epoch: 132 [143616/225000 (64%)] Loss: 7689.652344\n",
      "Train Epoch: 132 [147712/225000 (66%)] Loss: 7454.798828\n",
      "Train Epoch: 132 [151808/225000 (67%)] Loss: 7349.759766\n",
      "Train Epoch: 132 [155904/225000 (69%)] Loss: 7603.603516\n",
      "Train Epoch: 132 [160000/225000 (71%)] Loss: 7428.306641\n",
      "Train Epoch: 132 [164096/225000 (73%)] Loss: 7561.294922\n",
      "Train Epoch: 132 [168192/225000 (75%)] Loss: 7306.017578\n",
      "Train Epoch: 132 [172288/225000 (77%)] Loss: 7302.361328\n",
      "Train Epoch: 132 [176384/225000 (78%)] Loss: 7318.496094\n",
      "Train Epoch: 132 [180480/225000 (80%)] Loss: 7480.787109\n",
      "Train Epoch: 132 [184576/225000 (82%)] Loss: 7331.097656\n",
      "Train Epoch: 132 [188672/225000 (84%)] Loss: 7583.529297\n",
      "Train Epoch: 132 [192768/225000 (86%)] Loss: 7439.365234\n",
      "Train Epoch: 132 [196864/225000 (87%)] Loss: 7630.740234\n",
      "Train Epoch: 132 [200960/225000 (89%)] Loss: 7644.517578\n",
      "Train Epoch: 132 [205056/225000 (91%)] Loss: 7241.828125\n",
      "Train Epoch: 132 [209152/225000 (93%)] Loss: 7390.484375\n",
      "Train Epoch: 132 [213248/225000 (95%)] Loss: 7399.960938\n",
      "Train Epoch: 132 [217344/225000 (97%)] Loss: 7602.414062\n",
      "Train Epoch: 132 [221440/225000 (98%)] Loss: 7343.642578\n",
      "    epoch          : 132\n",
      "    loss           : 7453.112448005546\n",
      "    val_loss       : 7433.860636518926\n",
      "Train Epoch: 133 [256/225000 (0%)] Loss: 7286.134766\n",
      "Train Epoch: 133 [4352/225000 (2%)] Loss: 7548.751953\n",
      "Train Epoch: 133 [8448/225000 (4%)] Loss: 7518.136719\n",
      "Train Epoch: 133 [12544/225000 (6%)] Loss: 7319.689453\n",
      "Train Epoch: 133 [16640/225000 (7%)] Loss: 7457.130859\n",
      "Train Epoch: 133 [20736/225000 (9%)] Loss: 7333.699219\n",
      "Train Epoch: 133 [24832/225000 (11%)] Loss: 7440.632812\n",
      "Train Epoch: 133 [28928/225000 (13%)] Loss: 7385.289062\n",
      "Train Epoch: 133 [33024/225000 (15%)] Loss: 7592.015625\n",
      "Train Epoch: 133 [37120/225000 (16%)] Loss: 7271.535156\n",
      "Train Epoch: 133 [41216/225000 (18%)] Loss: 7295.777344\n",
      "Train Epoch: 133 [45312/225000 (20%)] Loss: 7258.900391\n",
      "Train Epoch: 133 [49408/225000 (22%)] Loss: 7571.773438\n",
      "Train Epoch: 133 [53504/225000 (24%)] Loss: 7404.810547\n",
      "Train Epoch: 133 [57600/225000 (26%)] Loss: 7519.013672\n",
      "Train Epoch: 133 [61696/225000 (27%)] Loss: 7465.181641\n",
      "Train Epoch: 133 [65792/225000 (29%)] Loss: 7428.880859\n",
      "Train Epoch: 133 [69888/225000 (31%)] Loss: 7525.101562\n",
      "Train Epoch: 133 [73984/225000 (33%)] Loss: 7237.007812\n",
      "Train Epoch: 133 [78080/225000 (35%)] Loss: 7268.533203\n",
      "Train Epoch: 133 [82176/225000 (37%)] Loss: 7505.015625\n",
      "Train Epoch: 133 [86272/225000 (38%)] Loss: 7564.007812\n",
      "Train Epoch: 133 [90368/225000 (40%)] Loss: 7483.763672\n",
      "Train Epoch: 133 [94464/225000 (42%)] Loss: 7378.265625\n",
      "Train Epoch: 133 [98560/225000 (44%)] Loss: 7339.656250\n",
      "Train Epoch: 133 [102656/225000 (46%)] Loss: 7508.603516\n",
      "Train Epoch: 133 [106752/225000 (47%)] Loss: 7363.576172\n",
      "Train Epoch: 133 [110848/225000 (49%)] Loss: 7586.851562\n",
      "Train Epoch: 133 [114944/225000 (51%)] Loss: 7422.152344\n",
      "Train Epoch: 133 [119040/225000 (53%)] Loss: 7414.708984\n",
      "Train Epoch: 133 [123136/225000 (55%)] Loss: 7316.914062\n",
      "Train Epoch: 133 [127232/225000 (57%)] Loss: 7380.457031\n",
      "Train Epoch: 133 [131328/225000 (58%)] Loss: 7353.908203\n",
      "Train Epoch: 133 [135424/225000 (60%)] Loss: 7450.320312\n",
      "Train Epoch: 133 [139520/225000 (62%)] Loss: 7391.382812\n",
      "Train Epoch: 133 [143616/225000 (64%)] Loss: 7584.373047\n",
      "Train Epoch: 133 [147712/225000 (66%)] Loss: 7362.351562\n",
      "Train Epoch: 133 [151808/225000 (67%)] Loss: 7351.509766\n",
      "Train Epoch: 133 [155904/225000 (69%)] Loss: 7251.121094\n",
      "Train Epoch: 133 [160000/225000 (71%)] Loss: 7445.007812\n",
      "Train Epoch: 133 [164096/225000 (73%)] Loss: 7485.984375\n",
      "Train Epoch: 133 [168192/225000 (75%)] Loss: 7572.617188\n",
      "Train Epoch: 133 [172288/225000 (77%)] Loss: 7425.587891\n",
      "Train Epoch: 133 [176384/225000 (78%)] Loss: 7391.119141\n",
      "Train Epoch: 133 [180480/225000 (80%)] Loss: 7464.861328\n",
      "Train Epoch: 133 [184576/225000 (82%)] Loss: 7398.519531\n",
      "Train Epoch: 133 [188672/225000 (84%)] Loss: 7428.361328\n",
      "Train Epoch: 133 [192768/225000 (86%)] Loss: 7555.373047\n",
      "Train Epoch: 133 [196864/225000 (87%)] Loss: 7434.142578\n",
      "Train Epoch: 133 [200960/225000 (89%)] Loss: 7662.552734\n",
      "Train Epoch: 133 [205056/225000 (91%)] Loss: 7380.843750\n",
      "Train Epoch: 133 [209152/225000 (93%)] Loss: 7449.960938\n",
      "Train Epoch: 133 [213248/225000 (95%)] Loss: 7464.138672\n",
      "Train Epoch: 133 [217344/225000 (97%)] Loss: 7508.285156\n",
      "Train Epoch: 133 [221440/225000 (98%)] Loss: 7311.710938\n",
      "    epoch          : 133\n",
      "    loss           : 7447.32480313211\n",
      "    val_loss       : 7425.303385291781\n",
      "Train Epoch: 134 [256/225000 (0%)] Loss: 7416.207031\n",
      "Train Epoch: 134 [4352/225000 (2%)] Loss: 7373.683594\n",
      "Train Epoch: 134 [8448/225000 (4%)] Loss: 7368.822266\n",
      "Train Epoch: 134 [12544/225000 (6%)] Loss: 7566.728516\n",
      "Train Epoch: 134 [16640/225000 (7%)] Loss: 7387.109375\n",
      "Train Epoch: 134 [20736/225000 (9%)] Loss: 7448.935547\n",
      "Train Epoch: 134 [24832/225000 (11%)] Loss: 7424.292969\n",
      "Train Epoch: 134 [28928/225000 (13%)] Loss: 7410.294922\n",
      "Train Epoch: 134 [33024/225000 (15%)] Loss: 7444.373047\n",
      "Train Epoch: 134 [37120/225000 (16%)] Loss: 7275.542969\n",
      "Train Epoch: 134 [41216/225000 (18%)] Loss: 7512.166016\n",
      "Train Epoch: 134 [45312/225000 (20%)] Loss: 7472.511719\n",
      "Train Epoch: 134 [49408/225000 (22%)] Loss: 7477.359375\n",
      "Train Epoch: 134 [53504/225000 (24%)] Loss: 7382.160156\n",
      "Train Epoch: 134 [57600/225000 (26%)] Loss: 7356.917969\n",
      "Train Epoch: 134 [61696/225000 (27%)] Loss: 7418.322266\n",
      "Train Epoch: 134 [65792/225000 (29%)] Loss: 7205.669922\n",
      "Train Epoch: 134 [69888/225000 (31%)] Loss: 7469.574219\n",
      "Train Epoch: 134 [73984/225000 (33%)] Loss: 7444.482422\n",
      "Train Epoch: 134 [78080/225000 (35%)] Loss: 7512.658203\n",
      "Train Epoch: 134 [82176/225000 (37%)] Loss: 7406.296875\n",
      "Train Epoch: 134 [86272/225000 (38%)] Loss: 7398.541016\n",
      "Train Epoch: 134 [90368/225000 (40%)] Loss: 7388.601562\n",
      "Train Epoch: 134 [94464/225000 (42%)] Loss: 7477.785156\n",
      "Train Epoch: 134 [98560/225000 (44%)] Loss: 7411.710938\n",
      "Train Epoch: 134 [102656/225000 (46%)] Loss: 7328.279297\n",
      "Train Epoch: 134 [106752/225000 (47%)] Loss: 7441.818359\n",
      "Train Epoch: 134 [110848/225000 (49%)] Loss: 7469.531250\n",
      "Train Epoch: 134 [114944/225000 (51%)] Loss: 7397.353516\n",
      "Train Epoch: 134 [119040/225000 (53%)] Loss: 7511.222656\n",
      "Train Epoch: 134 [123136/225000 (55%)] Loss: 7489.988281\n",
      "Train Epoch: 134 [127232/225000 (57%)] Loss: 7284.568359\n",
      "Train Epoch: 134 [131328/225000 (58%)] Loss: 7434.832031\n",
      "Train Epoch: 134 [135424/225000 (60%)] Loss: 7441.765625\n",
      "Train Epoch: 134 [139520/225000 (62%)] Loss: 7403.753906\n",
      "Train Epoch: 134 [143616/225000 (64%)] Loss: 7560.257812\n",
      "Train Epoch: 134 [147712/225000 (66%)] Loss: 7469.257812\n",
      "Train Epoch: 134 [151808/225000 (67%)] Loss: 7290.660156\n",
      "Train Epoch: 134 [155904/225000 (69%)] Loss: 7534.113281\n",
      "Train Epoch: 134 [160000/225000 (71%)] Loss: 7651.529297\n",
      "Train Epoch: 134 [164096/225000 (73%)] Loss: 7584.037109\n",
      "Train Epoch: 134 [168192/225000 (75%)] Loss: 7290.851562\n",
      "Train Epoch: 134 [172288/225000 (77%)] Loss: 7697.884766\n",
      "Train Epoch: 134 [176384/225000 (78%)] Loss: 7288.187500\n",
      "Train Epoch: 134 [180480/225000 (80%)] Loss: 7403.111328\n",
      "Train Epoch: 134 [184576/225000 (82%)] Loss: 7337.212891\n",
      "Train Epoch: 134 [188672/225000 (84%)] Loss: 7459.509766\n",
      "Train Epoch: 134 [192768/225000 (86%)] Loss: 7298.917969\n",
      "Train Epoch: 134 [196864/225000 (87%)] Loss: 7380.445312\n",
      "Train Epoch: 134 [200960/225000 (89%)] Loss: 7302.169922\n",
      "Train Epoch: 134 [205056/225000 (91%)] Loss: 7590.486328\n",
      "Train Epoch: 134 [209152/225000 (93%)] Loss: 7326.318359\n",
      "Train Epoch: 134 [213248/225000 (95%)] Loss: 7388.761719\n",
      "Train Epoch: 134 [217344/225000 (97%)] Loss: 7365.320312\n",
      "Train Epoch: 134 [221440/225000 (98%)] Loss: 7289.496094\n",
      "    epoch          : 134\n",
      "    loss           : 7430.822754461747\n",
      "    val_loss       : 7526.030261970296\n",
      "Train Epoch: 135 [256/225000 (0%)] Loss: 7345.324219\n",
      "Train Epoch: 135 [4352/225000 (2%)] Loss: 7448.884766\n",
      "Train Epoch: 135 [8448/225000 (4%)] Loss: 7615.017578\n",
      "Train Epoch: 135 [12544/225000 (6%)] Loss: 7304.302734\n",
      "Train Epoch: 135 [16640/225000 (7%)] Loss: 7506.908203\n",
      "Train Epoch: 135 [20736/225000 (9%)] Loss: 7502.544922\n",
      "Train Epoch: 135 [24832/225000 (11%)] Loss: 7435.306641\n",
      "Train Epoch: 135 [28928/225000 (13%)] Loss: 7366.419922\n",
      "Train Epoch: 135 [33024/225000 (15%)] Loss: 7538.363281\n",
      "Train Epoch: 135 [37120/225000 (16%)] Loss: 7365.230469\n",
      "Train Epoch: 135 [41216/225000 (18%)] Loss: 7507.587891\n",
      "Train Epoch: 135 [45312/225000 (20%)] Loss: 7488.697266\n",
      "Train Epoch: 135 [49408/225000 (22%)] Loss: 7485.753906\n",
      "Train Epoch: 135 [53504/225000 (24%)] Loss: 7493.232422\n",
      "Train Epoch: 135 [57600/225000 (26%)] Loss: 7500.894531\n",
      "Train Epoch: 135 [61696/225000 (27%)] Loss: 7236.023438\n",
      "Train Epoch: 135 [65792/225000 (29%)] Loss: 7592.263672\n",
      "Train Epoch: 135 [69888/225000 (31%)] Loss: 7366.921875\n",
      "Train Epoch: 135 [73984/225000 (33%)] Loss: 7303.121094\n",
      "Train Epoch: 135 [78080/225000 (35%)] Loss: 7444.326172\n",
      "Train Epoch: 135 [82176/225000 (37%)] Loss: 7418.583984\n",
      "Train Epoch: 135 [86272/225000 (38%)] Loss: 7519.335938\n",
      "Train Epoch: 135 [90368/225000 (40%)] Loss: 7353.232422\n",
      "Train Epoch: 135 [94464/225000 (42%)] Loss: 7413.013672\n",
      "Train Epoch: 135 [98560/225000 (44%)] Loss: 7410.279297\n",
      "Train Epoch: 135 [102656/225000 (46%)] Loss: 7574.837891\n",
      "Train Epoch: 135 [106752/225000 (47%)] Loss: 7371.849609\n",
      "Train Epoch: 135 [110848/225000 (49%)] Loss: 7628.527344\n",
      "Train Epoch: 135 [114944/225000 (51%)] Loss: 7190.601562\n",
      "Train Epoch: 135 [119040/225000 (53%)] Loss: 7293.986328\n",
      "Train Epoch: 135 [123136/225000 (55%)] Loss: 7524.980469\n",
      "Train Epoch: 135 [127232/225000 (57%)] Loss: 7238.275391\n",
      "Train Epoch: 135 [131328/225000 (58%)] Loss: 7267.900391\n",
      "Train Epoch: 135 [135424/225000 (60%)] Loss: 7362.843750\n",
      "Train Epoch: 135 [139520/225000 (62%)] Loss: 7400.796875\n",
      "Train Epoch: 135 [143616/225000 (64%)] Loss: 7359.632812\n",
      "Train Epoch: 135 [147712/225000 (66%)] Loss: 7605.576172\n",
      "Train Epoch: 135 [151808/225000 (67%)] Loss: 7309.677734\n",
      "Train Epoch: 135 [155904/225000 (69%)] Loss: 7384.455078\n",
      "Train Epoch: 135 [160000/225000 (71%)] Loss: 7284.240234\n",
      "Train Epoch: 135 [164096/225000 (73%)] Loss: 7385.287109\n",
      "Train Epoch: 135 [168192/225000 (75%)] Loss: 7386.988281\n",
      "Train Epoch: 135 [172288/225000 (77%)] Loss: 7471.001953\n",
      "Train Epoch: 135 [176384/225000 (78%)] Loss: 7323.095703\n",
      "Train Epoch: 135 [180480/225000 (80%)] Loss: 7395.632812\n",
      "Train Epoch: 135 [184576/225000 (82%)] Loss: 7524.751953\n",
      "Train Epoch: 135 [188672/225000 (84%)] Loss: 7443.949219\n",
      "Train Epoch: 135 [192768/225000 (86%)] Loss: 7422.232422\n",
      "Train Epoch: 135 [196864/225000 (87%)] Loss: 7277.613281\n",
      "Train Epoch: 135 [200960/225000 (89%)] Loss: 7297.722656\n",
      "Train Epoch: 135 [205056/225000 (91%)] Loss: 7258.359375\n",
      "Train Epoch: 135 [209152/225000 (93%)] Loss: 7331.404297\n",
      "Train Epoch: 135 [213248/225000 (95%)] Loss: 7471.736328\n",
      "Train Epoch: 135 [217344/225000 (97%)] Loss: 7330.972656\n",
      "Train Epoch: 135 [221440/225000 (98%)] Loss: 7272.076172\n",
      "    epoch          : 135\n",
      "    loss           : 7426.683224900455\n",
      "    val_loss       : 7419.418124638041\n",
      "Train Epoch: 136 [256/225000 (0%)] Loss: 7492.205078\n",
      "Train Epoch: 136 [4352/225000 (2%)] Loss: 7460.585938\n",
      "Train Epoch: 136 [8448/225000 (4%)] Loss: 7345.810547\n",
      "Train Epoch: 136 [12544/225000 (6%)] Loss: 7532.406250\n",
      "Train Epoch: 136 [16640/225000 (7%)] Loss: 7262.265625\n",
      "Train Epoch: 136 [20736/225000 (9%)] Loss: 7392.253906\n",
      "Train Epoch: 136 [24832/225000 (11%)] Loss: 7350.384766\n",
      "Train Epoch: 136 [28928/225000 (13%)] Loss: 7381.279297\n",
      "Train Epoch: 136 [33024/225000 (15%)] Loss: 7245.173828\n",
      "Train Epoch: 136 [37120/225000 (16%)] Loss: 7478.654297\n",
      "Train Epoch: 136 [41216/225000 (18%)] Loss: 7569.347656\n",
      "Train Epoch: 136 [45312/225000 (20%)] Loss: 7382.460938\n",
      "Train Epoch: 136 [49408/225000 (22%)] Loss: 7404.562500\n",
      "Train Epoch: 136 [53504/225000 (24%)] Loss: 7384.876953\n",
      "Train Epoch: 136 [57600/225000 (26%)] Loss: 7335.402344\n",
      "Train Epoch: 136 [61696/225000 (27%)] Loss: 7400.443359\n",
      "Train Epoch: 136 [65792/225000 (29%)] Loss: 7310.064453\n",
      "Train Epoch: 136 [69888/225000 (31%)] Loss: 7485.638672\n",
      "Train Epoch: 136 [73984/225000 (33%)] Loss: 7506.363281\n",
      "Train Epoch: 136 [78080/225000 (35%)] Loss: 7383.640625\n",
      "Train Epoch: 136 [82176/225000 (37%)] Loss: 7482.931641\n",
      "Train Epoch: 136 [86272/225000 (38%)] Loss: 7424.906250\n",
      "Train Epoch: 136 [90368/225000 (40%)] Loss: 7324.986328\n",
      "Train Epoch: 136 [94464/225000 (42%)] Loss: 7544.001953\n",
      "Train Epoch: 136 [98560/225000 (44%)] Loss: 7416.521484\n",
      "Train Epoch: 136 [102656/225000 (46%)] Loss: 7437.828125\n",
      "Train Epoch: 136 [106752/225000 (47%)] Loss: 7367.134766\n",
      "Train Epoch: 136 [110848/225000 (49%)] Loss: 7315.705078\n",
      "Train Epoch: 136 [114944/225000 (51%)] Loss: 7469.378906\n",
      "Train Epoch: 136 [119040/225000 (53%)] Loss: 7354.935547\n",
      "Train Epoch: 136 [123136/225000 (55%)] Loss: 7277.714844\n",
      "Train Epoch: 136 [127232/225000 (57%)] Loss: 7403.847656\n",
      "Train Epoch: 136 [131328/225000 (58%)] Loss: 7507.134766\n",
      "Train Epoch: 136 [135424/225000 (60%)] Loss: 7606.652344\n",
      "Train Epoch: 136 [139520/225000 (62%)] Loss: 7348.845703\n",
      "Train Epoch: 136 [143616/225000 (64%)] Loss: 7406.552734\n",
      "Train Epoch: 136 [147712/225000 (66%)] Loss: 7328.304688\n",
      "Train Epoch: 136 [151808/225000 (67%)] Loss: 7346.777344\n",
      "Train Epoch: 136 [155904/225000 (69%)] Loss: 7602.599609\n",
      "Train Epoch: 136 [160000/225000 (71%)] Loss: 7373.580078\n",
      "Train Epoch: 136 [164096/225000 (73%)] Loss: 7501.845703\n",
      "Train Epoch: 136 [168192/225000 (75%)] Loss: 7380.587891\n",
      "Train Epoch: 136 [172288/225000 (77%)] Loss: 7442.496094\n",
      "Train Epoch: 136 [176384/225000 (78%)] Loss: 7268.609375\n",
      "Train Epoch: 136 [180480/225000 (80%)] Loss: 7425.601562\n",
      "Train Epoch: 136 [184576/225000 (82%)] Loss: 7484.177734\n",
      "Train Epoch: 136 [188672/225000 (84%)] Loss: 7410.783203\n",
      "Train Epoch: 136 [192768/225000 (86%)] Loss: 7442.939453\n",
      "Train Epoch: 136 [196864/225000 (87%)] Loss: 7641.845703\n",
      "Train Epoch: 136 [200960/225000 (89%)] Loss: 7323.240234\n",
      "Train Epoch: 136 [205056/225000 (91%)] Loss: 7434.289062\n",
      "Train Epoch: 136 [209152/225000 (93%)] Loss: 7331.794922\n",
      "Train Epoch: 136 [213248/225000 (95%)] Loss: 7508.373047\n",
      "Train Epoch: 136 [217344/225000 (97%)] Loss: 7538.281250\n",
      "Train Epoch: 136 [221440/225000 (98%)] Loss: 7551.621094\n",
      "    epoch          : 136\n",
      "    loss           : 7432.705805825157\n",
      "    val_loss       : 7410.342461532476\n",
      "Train Epoch: 137 [256/225000 (0%)] Loss: 7387.351562\n",
      "Train Epoch: 137 [4352/225000 (2%)] Loss: 7442.390625\n",
      "Train Epoch: 137 [8448/225000 (4%)] Loss: 7543.427734\n",
      "Train Epoch: 137 [12544/225000 (6%)] Loss: 7293.478516\n",
      "Train Epoch: 137 [16640/225000 (7%)] Loss: 7562.371094\n",
      "Train Epoch: 137 [20736/225000 (9%)] Loss: 7361.941406\n",
      "Train Epoch: 137 [24832/225000 (11%)] Loss: 7343.550781\n",
      "Train Epoch: 137 [28928/225000 (13%)] Loss: 7429.421875\n",
      "Train Epoch: 137 [33024/225000 (15%)] Loss: 7466.611328\n",
      "Train Epoch: 137 [37120/225000 (16%)] Loss: 7352.242188\n",
      "Train Epoch: 137 [41216/225000 (18%)] Loss: 7354.337891\n",
      "Train Epoch: 137 [45312/225000 (20%)] Loss: 7501.826172\n",
      "Train Epoch: 137 [49408/225000 (22%)] Loss: 7417.990234\n",
      "Train Epoch: 137 [53504/225000 (24%)] Loss: 7297.455078\n",
      "Train Epoch: 137 [57600/225000 (26%)] Loss: 7429.679688\n",
      "Train Epoch: 137 [61696/225000 (27%)] Loss: 7512.687500\n",
      "Train Epoch: 137 [65792/225000 (29%)] Loss: 7485.226562\n",
      "Train Epoch: 137 [69888/225000 (31%)] Loss: 7460.580078\n",
      "Train Epoch: 137 [73984/225000 (33%)] Loss: 7526.925781\n",
      "Train Epoch: 137 [78080/225000 (35%)] Loss: 7410.849609\n",
      "Train Epoch: 137 [82176/225000 (37%)] Loss: 7375.244141\n",
      "Train Epoch: 137 [86272/225000 (38%)] Loss: 7464.199219\n",
      "Train Epoch: 137 [90368/225000 (40%)] Loss: 7523.515625\n",
      "Train Epoch: 137 [94464/225000 (42%)] Loss: 7450.708984\n",
      "Train Epoch: 137 [98560/225000 (44%)] Loss: 7482.494141\n",
      "Train Epoch: 137 [102656/225000 (46%)] Loss: 7452.919922\n",
      "Train Epoch: 137 [106752/225000 (47%)] Loss: 7553.833984\n",
      "Train Epoch: 137 [110848/225000 (49%)] Loss: 7474.908203\n",
      "Train Epoch: 137 [114944/225000 (51%)] Loss: 7295.929688\n",
      "Train Epoch: 137 [119040/225000 (53%)] Loss: 7512.556641\n",
      "Train Epoch: 137 [123136/225000 (55%)] Loss: 7168.021484\n",
      "Train Epoch: 137 [127232/225000 (57%)] Loss: 7575.013672\n",
      "Train Epoch: 137 [131328/225000 (58%)] Loss: 7406.181641\n",
      "Train Epoch: 137 [135424/225000 (60%)] Loss: 7410.716797\n",
      "Train Epoch: 137 [139520/225000 (62%)] Loss: 7352.591797\n",
      "Train Epoch: 137 [143616/225000 (64%)] Loss: 7420.248047\n",
      "Train Epoch: 137 [147712/225000 (66%)] Loss: 7387.978516\n",
      "Train Epoch: 137 [151808/225000 (67%)] Loss: 7341.804688\n",
      "Train Epoch: 137 [155904/225000 (69%)] Loss: 7577.507812\n",
      "Train Epoch: 137 [160000/225000 (71%)] Loss: 7539.296875\n",
      "Train Epoch: 137 [164096/225000 (73%)] Loss: 7536.457031\n",
      "Train Epoch: 137 [168192/225000 (75%)] Loss: 7527.087891\n",
      "Train Epoch: 137 [172288/225000 (77%)] Loss: 7423.974609\n",
      "Train Epoch: 137 [176384/225000 (78%)] Loss: 7499.433594\n",
      "Train Epoch: 137 [180480/225000 (80%)] Loss: 7429.642578\n",
      "Train Epoch: 137 [184576/225000 (82%)] Loss: 7564.132812\n",
      "Train Epoch: 137 [188672/225000 (84%)] Loss: 7440.630859\n",
      "Train Epoch: 137 [192768/225000 (86%)] Loss: 7425.812500\n",
      "Train Epoch: 137 [196864/225000 (87%)] Loss: 7281.933594\n",
      "Train Epoch: 137 [200960/225000 (89%)] Loss: 7287.156250\n",
      "Train Epoch: 137 [205056/225000 (91%)] Loss: 7481.369141\n",
      "Train Epoch: 137 [209152/225000 (93%)] Loss: 7427.142578\n",
      "Train Epoch: 137 [213248/225000 (95%)] Loss: 7458.789062\n",
      "Train Epoch: 137 [217344/225000 (97%)] Loss: 7495.802734\n",
      "Train Epoch: 137 [221440/225000 (98%)] Loss: 29938.529297\n",
      "    epoch          : 137\n",
      "    loss           : 7454.378360752631\n",
      "    val_loss       : 7406.986716375059\n",
      "Train Epoch: 138 [256/225000 (0%)] Loss: 7490.080078\n",
      "Train Epoch: 138 [4352/225000 (2%)] Loss: 7440.134766\n",
      "Train Epoch: 138 [8448/225000 (4%)] Loss: 7273.283203\n",
      "Train Epoch: 138 [12544/225000 (6%)] Loss: 7584.279297\n",
      "Train Epoch: 138 [16640/225000 (7%)] Loss: 7419.980469\n",
      "Train Epoch: 138 [20736/225000 (9%)] Loss: 7603.394531\n",
      "Train Epoch: 138 [24832/225000 (11%)] Loss: 7310.373047\n",
      "Train Epoch: 138 [28928/225000 (13%)] Loss: 7426.005859\n",
      "Train Epoch: 138 [33024/225000 (15%)] Loss: 7377.494141\n",
      "Train Epoch: 138 [37120/225000 (16%)] Loss: 7565.570312\n",
      "Train Epoch: 138 [41216/225000 (18%)] Loss: 7429.880859\n",
      "Train Epoch: 138 [45312/225000 (20%)] Loss: 7334.230469\n",
      "Train Epoch: 138 [49408/225000 (22%)] Loss: 7394.029297\n",
      "Train Epoch: 138 [53504/225000 (24%)] Loss: 7493.501953\n",
      "Train Epoch: 138 [57600/225000 (26%)] Loss: 7378.433594\n",
      "Train Epoch: 138 [61696/225000 (27%)] Loss: 7510.853516\n",
      "Train Epoch: 138 [65792/225000 (29%)] Loss: 7646.494141\n",
      "Train Epoch: 138 [69888/225000 (31%)] Loss: 7584.951172\n",
      "Train Epoch: 138 [73984/225000 (33%)] Loss: 7390.277344\n",
      "Train Epoch: 138 [78080/225000 (35%)] Loss: 7436.130859\n",
      "Train Epoch: 138 [82176/225000 (37%)] Loss: 7501.414062\n",
      "Train Epoch: 138 [86272/225000 (38%)] Loss: 7349.558594\n",
      "Train Epoch: 138 [90368/225000 (40%)] Loss: 7307.732422\n",
      "Train Epoch: 138 [94464/225000 (42%)] Loss: 7300.185547\n",
      "Train Epoch: 138 [98560/225000 (44%)] Loss: 7466.205078\n",
      "Train Epoch: 138 [102656/225000 (46%)] Loss: 7392.492188\n",
      "Train Epoch: 138 [106752/225000 (47%)] Loss: 7480.597656\n",
      "Train Epoch: 138 [110848/225000 (49%)] Loss: 7558.884766\n",
      "Train Epoch: 138 [114944/225000 (51%)] Loss: 7544.082031\n",
      "Train Epoch: 138 [119040/225000 (53%)] Loss: 7297.267578\n",
      "Train Epoch: 138 [123136/225000 (55%)] Loss: 7357.718750\n",
      "Train Epoch: 138 [127232/225000 (57%)] Loss: 7474.822266\n",
      "Train Epoch: 138 [131328/225000 (58%)] Loss: 7380.753906\n",
      "Train Epoch: 138 [135424/225000 (60%)] Loss: 7491.042969\n",
      "Train Epoch: 138 [139520/225000 (62%)] Loss: 7196.851562\n",
      "Train Epoch: 138 [143616/225000 (64%)] Loss: 7524.796875\n",
      "Train Epoch: 138 [147712/225000 (66%)] Loss: 7432.759766\n",
      "Train Epoch: 138 [151808/225000 (67%)] Loss: 7499.332031\n",
      "Train Epoch: 138 [155904/225000 (69%)] Loss: 7411.761719\n",
      "Train Epoch: 138 [160000/225000 (71%)] Loss: 7615.833984\n",
      "Train Epoch: 138 [164096/225000 (73%)] Loss: 7345.230469\n",
      "Train Epoch: 138 [168192/225000 (75%)] Loss: 7714.119141\n",
      "Train Epoch: 138 [172288/225000 (77%)] Loss: 7405.804688\n",
      "Train Epoch: 138 [176384/225000 (78%)] Loss: 7551.156250\n",
      "Train Epoch: 138 [180480/225000 (80%)] Loss: 7410.939453\n",
      "Train Epoch: 138 [184576/225000 (82%)] Loss: 7467.935547\n",
      "Train Epoch: 138 [188672/225000 (84%)] Loss: 7478.542969\n",
      "Train Epoch: 138 [192768/225000 (86%)] Loss: 7297.283203\n",
      "Train Epoch: 138 [196864/225000 (87%)] Loss: 7304.841797\n",
      "Train Epoch: 138 [200960/225000 (89%)] Loss: 7324.714844\n",
      "Train Epoch: 138 [205056/225000 (91%)] Loss: 7378.976562\n",
      "Train Epoch: 138 [209152/225000 (93%)] Loss: 7466.865234\n",
      "Train Epoch: 138 [213248/225000 (95%)] Loss: 7240.375000\n",
      "Train Epoch: 138 [217344/225000 (97%)] Loss: 7242.207031\n",
      "Train Epoch: 138 [221440/225000 (98%)] Loss: 7281.341797\n",
      "    epoch          : 138\n",
      "    loss           : 7429.217184611419\n",
      "    val_loss       : 7662.402994015995\n",
      "Train Epoch: 139 [256/225000 (0%)] Loss: 7284.291016\n",
      "Train Epoch: 139 [4352/225000 (2%)] Loss: 7434.785156\n",
      "Train Epoch: 139 [8448/225000 (4%)] Loss: 7427.199219\n",
      "Train Epoch: 139 [12544/225000 (6%)] Loss: 7650.455078\n",
      "Train Epoch: 139 [16640/225000 (7%)] Loss: 7506.013672\n",
      "Train Epoch: 139 [20736/225000 (9%)] Loss: 7258.267578\n",
      "Train Epoch: 139 [24832/225000 (11%)] Loss: 7346.480469\n",
      "Train Epoch: 139 [28928/225000 (13%)] Loss: 7617.972656\n",
      "Train Epoch: 139 [33024/225000 (15%)] Loss: 7310.984375\n",
      "Train Epoch: 139 [37120/225000 (16%)] Loss: 7629.835938\n",
      "Train Epoch: 139 [41216/225000 (18%)] Loss: 7422.119141\n",
      "Train Epoch: 139 [45312/225000 (20%)] Loss: 7393.027344\n",
      "Train Epoch: 139 [49408/225000 (22%)] Loss: 7371.484375\n",
      "Train Epoch: 139 [53504/225000 (24%)] Loss: 7382.078125\n",
      "Train Epoch: 139 [57600/225000 (26%)] Loss: 7301.328125\n",
      "Train Epoch: 139 [61696/225000 (27%)] Loss: 7366.000000\n",
      "Train Epoch: 139 [65792/225000 (29%)] Loss: 7565.980469\n",
      "Train Epoch: 139 [69888/225000 (31%)] Loss: 7431.886719\n",
      "Train Epoch: 139 [73984/225000 (33%)] Loss: 7265.736328\n",
      "Train Epoch: 139 [78080/225000 (35%)] Loss: 7120.435547\n",
      "Train Epoch: 139 [82176/225000 (37%)] Loss: 7319.205078\n",
      "Train Epoch: 139 [86272/225000 (38%)] Loss: 7296.433594\n",
      "Train Epoch: 139 [90368/225000 (40%)] Loss: 7305.275391\n",
      "Train Epoch: 139 [94464/225000 (42%)] Loss: 7333.753906\n",
      "Train Epoch: 139 [98560/225000 (44%)] Loss: 7397.712891\n",
      "Train Epoch: 139 [102656/225000 (46%)] Loss: 7347.177734\n",
      "Train Epoch: 139 [106752/225000 (47%)] Loss: 7342.876953\n",
      "Train Epoch: 139 [110848/225000 (49%)] Loss: 7384.031250\n",
      "Train Epoch: 139 [114944/225000 (51%)] Loss: 7201.687500\n",
      "Train Epoch: 139 [119040/225000 (53%)] Loss: 7484.363281\n",
      "Train Epoch: 139 [123136/225000 (55%)] Loss: 7359.134766\n",
      "Train Epoch: 139 [127232/225000 (57%)] Loss: 7488.386719\n",
      "Train Epoch: 139 [131328/225000 (58%)] Loss: 7358.341797\n",
      "Train Epoch: 139 [135424/225000 (60%)] Loss: 7530.992188\n",
      "Train Epoch: 139 [139520/225000 (62%)] Loss: 7289.021484\n",
      "Train Epoch: 139 [143616/225000 (64%)] Loss: 7633.265625\n",
      "Train Epoch: 139 [147712/225000 (66%)] Loss: 7502.404297\n",
      "Train Epoch: 139 [151808/225000 (67%)] Loss: 7301.431641\n",
      "Train Epoch: 139 [155904/225000 (69%)] Loss: 7320.271484\n",
      "Train Epoch: 139 [160000/225000 (71%)] Loss: 7501.031250\n",
      "Train Epoch: 139 [164096/225000 (73%)] Loss: 7378.089844\n",
      "Train Epoch: 139 [168192/225000 (75%)] Loss: 7377.240234\n",
      "Train Epoch: 139 [172288/225000 (77%)] Loss: 7299.888672\n",
      "Train Epoch: 139 [176384/225000 (78%)] Loss: 7289.814453\n",
      "Train Epoch: 139 [180480/225000 (80%)] Loss: 7363.146484\n",
      "Train Epoch: 139 [184576/225000 (82%)] Loss: 7425.054688\n",
      "Train Epoch: 139 [188672/225000 (84%)] Loss: 7340.835938\n",
      "Train Epoch: 139 [192768/225000 (86%)] Loss: 7474.218750\n",
      "Train Epoch: 139 [196864/225000 (87%)] Loss: 7413.908203\n",
      "Train Epoch: 139 [200960/225000 (89%)] Loss: 7336.636719\n",
      "Train Epoch: 139 [205056/225000 (91%)] Loss: 7474.718750\n",
      "Train Epoch: 139 [209152/225000 (93%)] Loss: 7552.095703\n",
      "Train Epoch: 139 [213248/225000 (95%)] Loss: 7415.423828\n",
      "Train Epoch: 139 [217344/225000 (97%)] Loss: 7419.029297\n",
      "Train Epoch: 139 [221440/225000 (98%)] Loss: 7401.802734\n",
      "    epoch          : 139\n",
      "    loss           : 7432.025248417946\n",
      "    val_loss       : 7403.452164084328\n",
      "Train Epoch: 140 [256/225000 (0%)] Loss: 7283.800781\n",
      "Train Epoch: 140 [4352/225000 (2%)] Loss: 7409.449219\n",
      "Train Epoch: 140 [8448/225000 (4%)] Loss: 7273.031250\n",
      "Train Epoch: 140 [12544/225000 (6%)] Loss: 7140.513672\n",
      "Train Epoch: 140 [16640/225000 (7%)] Loss: 7657.587891\n",
      "Train Epoch: 140 [20736/225000 (9%)] Loss: 7387.150391\n",
      "Train Epoch: 140 [24832/225000 (11%)] Loss: 7467.207031\n",
      "Train Epoch: 140 [28928/225000 (13%)] Loss: 7141.806641\n",
      "Train Epoch: 140 [33024/225000 (15%)] Loss: 7377.529297\n",
      "Train Epoch: 140 [37120/225000 (16%)] Loss: 7476.025391\n",
      "Train Epoch: 140 [41216/225000 (18%)] Loss: 7561.791016\n",
      "Train Epoch: 140 [45312/225000 (20%)] Loss: 7297.324219\n",
      "Train Epoch: 140 [49408/225000 (22%)] Loss: 7456.191406\n",
      "Train Epoch: 140 [53504/225000 (24%)] Loss: 7579.308594\n",
      "Train Epoch: 140 [57600/225000 (26%)] Loss: 7397.548828\n",
      "Train Epoch: 140 [61696/225000 (27%)] Loss: 7379.013672\n",
      "Train Epoch: 140 [65792/225000 (29%)] Loss: 7326.451172\n",
      "Train Epoch: 140 [69888/225000 (31%)] Loss: 7474.039062\n",
      "Train Epoch: 140 [73984/225000 (33%)] Loss: 7379.904297\n",
      "Train Epoch: 140 [78080/225000 (35%)] Loss: 7539.875000\n",
      "Train Epoch: 140 [82176/225000 (37%)] Loss: 7318.630859\n",
      "Train Epoch: 140 [86272/225000 (38%)] Loss: 7542.458984\n",
      "Train Epoch: 140 [90368/225000 (40%)] Loss: 7488.806641\n",
      "Train Epoch: 140 [94464/225000 (42%)] Loss: 7459.964844\n",
      "Train Epoch: 140 [98560/225000 (44%)] Loss: 7411.326172\n",
      "Train Epoch: 140 [102656/225000 (46%)] Loss: 7516.072266\n",
      "Train Epoch: 140 [106752/225000 (47%)] Loss: 7496.101562\n",
      "Train Epoch: 140 [110848/225000 (49%)] Loss: 7367.365234\n",
      "Train Epoch: 140 [114944/225000 (51%)] Loss: 7548.994141\n",
      "Train Epoch: 140 [119040/225000 (53%)] Loss: 7259.109375\n",
      "Train Epoch: 140 [123136/225000 (55%)] Loss: 7335.066406\n",
      "Train Epoch: 140 [127232/225000 (57%)] Loss: 7499.130859\n",
      "Train Epoch: 140 [131328/225000 (58%)] Loss: 7521.671875\n",
      "Train Epoch: 140 [135424/225000 (60%)] Loss: 7420.490234\n",
      "Train Epoch: 140 [139520/225000 (62%)] Loss: 7488.562500\n",
      "Train Epoch: 140 [143616/225000 (64%)] Loss: 7443.927734\n",
      "Train Epoch: 140 [147712/225000 (66%)] Loss: 7356.607422\n",
      "Train Epoch: 140 [151808/225000 (67%)] Loss: 7324.289062\n",
      "Train Epoch: 140 [155904/225000 (69%)] Loss: 7341.093750\n",
      "Train Epoch: 140 [160000/225000 (71%)] Loss: 7312.382812\n",
      "Train Epoch: 140 [164096/225000 (73%)] Loss: 7360.542969\n",
      "Train Epoch: 140 [168192/225000 (75%)] Loss: 7427.103516\n",
      "Train Epoch: 140 [172288/225000 (77%)] Loss: 7428.160156\n",
      "Train Epoch: 140 [176384/225000 (78%)] Loss: 7337.548828\n",
      "Train Epoch: 140 [180480/225000 (80%)] Loss: 7306.734375\n",
      "Train Epoch: 140 [184576/225000 (82%)] Loss: 7320.781250\n",
      "Train Epoch: 140 [188672/225000 (84%)] Loss: 7511.937500\n",
      "Train Epoch: 140 [192768/225000 (86%)] Loss: 7308.931641\n",
      "Train Epoch: 140 [196864/225000 (87%)] Loss: 7413.292969\n",
      "Train Epoch: 140 [200960/225000 (89%)] Loss: 7315.542969\n",
      "Train Epoch: 140 [205056/225000 (91%)] Loss: 7442.591797\n",
      "Train Epoch: 140 [209152/225000 (93%)] Loss: 7404.810547\n",
      "Train Epoch: 140 [213248/225000 (95%)] Loss: 7360.597656\n",
      "Train Epoch: 140 [217344/225000 (97%)] Loss: 7460.843750\n",
      "Train Epoch: 140 [221440/225000 (98%)] Loss: 7446.130859\n",
      "    epoch          : 140\n",
      "    loss           : 7409.071390162827\n",
      "    val_loss       : 7399.31709949216\n",
      "Train Epoch: 141 [256/225000 (0%)] Loss: 7401.541016\n",
      "Train Epoch: 141 [4352/225000 (2%)] Loss: 7467.166016\n",
      "Train Epoch: 141 [8448/225000 (4%)] Loss: 7396.714844\n",
      "Train Epoch: 141 [12544/225000 (6%)] Loss: 7369.863281\n",
      "Train Epoch: 141 [16640/225000 (7%)] Loss: 7420.437500\n",
      "Train Epoch: 141 [20736/225000 (9%)] Loss: 7492.869141\n",
      "Train Epoch: 141 [24832/225000 (11%)] Loss: 7357.203125\n",
      "Train Epoch: 141 [28928/225000 (13%)] Loss: 7356.023438\n",
      "Train Epoch: 141 [33024/225000 (15%)] Loss: 7315.941406\n",
      "Train Epoch: 141 [37120/225000 (16%)] Loss: 7491.875000\n",
      "Train Epoch: 141 [41216/225000 (18%)] Loss: 7360.115234\n",
      "Train Epoch: 141 [45312/225000 (20%)] Loss: 7544.349609\n",
      "Train Epoch: 141 [49408/225000 (22%)] Loss: 7378.302734\n",
      "Train Epoch: 141 [53504/225000 (24%)] Loss: 7343.044922\n",
      "Train Epoch: 141 [57600/225000 (26%)] Loss: 7391.181641\n",
      "Train Epoch: 141 [61696/225000 (27%)] Loss: 7303.445312\n",
      "Train Epoch: 141 [65792/225000 (29%)] Loss: 7475.171875\n",
      "Train Epoch: 141 [69888/225000 (31%)] Loss: 7412.238281\n",
      "Train Epoch: 141 [73984/225000 (33%)] Loss: 7541.746094\n",
      "Train Epoch: 141 [78080/225000 (35%)] Loss: 7206.558594\n",
      "Train Epoch: 141 [82176/225000 (37%)] Loss: 7494.414062\n",
      "Train Epoch: 141 [86272/225000 (38%)] Loss: 7323.255859\n",
      "Train Epoch: 141 [90368/225000 (40%)] Loss: 7533.462891\n",
      "Train Epoch: 141 [94464/225000 (42%)] Loss: 7394.978516\n",
      "Train Epoch: 141 [98560/225000 (44%)] Loss: 7401.097656\n",
      "Train Epoch: 141 [102656/225000 (46%)] Loss: 7383.398438\n",
      "Train Epoch: 141 [106752/225000 (47%)] Loss: 7314.341797\n",
      "Train Epoch: 141 [110848/225000 (49%)] Loss: 7294.193359\n",
      "Train Epoch: 141 [114944/225000 (51%)] Loss: 7367.761719\n",
      "Train Epoch: 141 [119040/225000 (53%)] Loss: 7296.478516\n",
      "Train Epoch: 141 [123136/225000 (55%)] Loss: 7491.177734\n",
      "Train Epoch: 141 [127232/225000 (57%)] Loss: 7250.951172\n",
      "Train Epoch: 141 [131328/225000 (58%)] Loss: 7280.863281\n",
      "Train Epoch: 141 [135424/225000 (60%)] Loss: 7582.693359\n",
      "Train Epoch: 141 [139520/225000 (62%)] Loss: 7397.021484\n",
      "Train Epoch: 141 [143616/225000 (64%)] Loss: 7251.931641\n",
      "Train Epoch: 141 [147712/225000 (66%)] Loss: 7530.876953\n",
      "Train Epoch: 141 [151808/225000 (67%)] Loss: 7440.861328\n",
      "Train Epoch: 141 [155904/225000 (69%)] Loss: 7431.105469\n",
      "Train Epoch: 141 [160000/225000 (71%)] Loss: 7327.507812\n",
      "Train Epoch: 141 [164096/225000 (73%)] Loss: 7419.986328\n",
      "Train Epoch: 141 [168192/225000 (75%)] Loss: 7480.572266\n",
      "Train Epoch: 141 [172288/225000 (77%)] Loss: 7514.759766\n",
      "Train Epoch: 141 [176384/225000 (78%)] Loss: 7523.576172\n",
      "Train Epoch: 141 [180480/225000 (80%)] Loss: 7370.455078\n",
      "Train Epoch: 141 [184576/225000 (82%)] Loss: 7271.005859\n",
      "Train Epoch: 141 [188672/225000 (84%)] Loss: 7670.888672\n",
      "Train Epoch: 141 [192768/225000 (86%)] Loss: 7480.583984\n",
      "Train Epoch: 141 [196864/225000 (87%)] Loss: 7279.000000\n",
      "Train Epoch: 141 [200960/225000 (89%)] Loss: 12511.968750\n",
      "Train Epoch: 141 [205056/225000 (91%)] Loss: 7552.939453\n",
      "Train Epoch: 141 [209152/225000 (93%)] Loss: 7453.312500\n",
      "Train Epoch: 141 [213248/225000 (95%)] Loss: 7193.160156\n",
      "Train Epoch: 141 [217344/225000 (97%)] Loss: 7365.650391\n",
      "Train Epoch: 141 [221440/225000 (98%)] Loss: 7360.960938\n",
      "    epoch          : 141\n",
      "    loss           : 7404.676792253271\n",
      "    val_loss       : 7401.388943888704\n",
      "Train Epoch: 142 [256/225000 (0%)] Loss: 7552.109375\n",
      "Train Epoch: 142 [4352/225000 (2%)] Loss: 7392.023438\n",
      "Train Epoch: 142 [8448/225000 (4%)] Loss: 7399.662109\n",
      "Train Epoch: 142 [12544/225000 (6%)] Loss: 7353.835938\n",
      "Train Epoch: 142 [16640/225000 (7%)] Loss: 7393.777344\n",
      "Train Epoch: 142 [20736/225000 (9%)] Loss: 7408.087891\n",
      "Train Epoch: 142 [24832/225000 (11%)] Loss: 7270.634766\n",
      "Train Epoch: 142 [28928/225000 (13%)] Loss: 7512.203125\n",
      "Train Epoch: 142 [33024/225000 (15%)] Loss: 7529.781250\n",
      "Train Epoch: 142 [37120/225000 (16%)] Loss: 7227.757812\n",
      "Train Epoch: 142 [41216/225000 (18%)] Loss: 7532.376953\n",
      "Train Epoch: 142 [45312/225000 (20%)] Loss: 7279.955078\n",
      "Train Epoch: 142 [49408/225000 (22%)] Loss: 7265.697266\n",
      "Train Epoch: 142 [53504/225000 (24%)] Loss: 7285.265625\n",
      "Train Epoch: 142 [57600/225000 (26%)] Loss: 7328.224609\n",
      "Train Epoch: 142 [61696/225000 (27%)] Loss: 7420.820312\n",
      "Train Epoch: 142 [65792/225000 (29%)] Loss: 7440.460938\n",
      "Train Epoch: 142 [69888/225000 (31%)] Loss: 7373.550781\n",
      "Train Epoch: 142 [73984/225000 (33%)] Loss: 7385.902344\n",
      "Train Epoch: 142 [78080/225000 (35%)] Loss: 7536.427734\n",
      "Train Epoch: 142 [82176/225000 (37%)] Loss: 7505.560547\n",
      "Train Epoch: 142 [86272/225000 (38%)] Loss: 7330.880859\n",
      "Train Epoch: 142 [90368/225000 (40%)] Loss: 7272.544922\n",
      "Train Epoch: 142 [94464/225000 (42%)] Loss: 7372.751953\n",
      "Train Epoch: 142 [98560/225000 (44%)] Loss: 7420.408203\n",
      "Train Epoch: 142 [102656/225000 (46%)] Loss: 7354.076172\n",
      "Train Epoch: 142 [106752/225000 (47%)] Loss: 7170.019531\n",
      "Train Epoch: 142 [110848/225000 (49%)] Loss: 7400.470703\n",
      "Train Epoch: 142 [114944/225000 (51%)] Loss: 7389.011719\n",
      "Train Epoch: 142 [119040/225000 (53%)] Loss: 7096.796875\n",
      "Train Epoch: 142 [123136/225000 (55%)] Loss: 7266.238281\n",
      "Train Epoch: 142 [127232/225000 (57%)] Loss: 7523.435547\n",
      "Train Epoch: 142 [131328/225000 (58%)] Loss: 7448.291016\n",
      "Train Epoch: 142 [135424/225000 (60%)] Loss: 7330.902344\n",
      "Train Epoch: 142 [139520/225000 (62%)] Loss: 7439.062500\n",
      "Train Epoch: 142 [143616/225000 (64%)] Loss: 7381.710938\n",
      "Train Epoch: 142 [147712/225000 (66%)] Loss: 7284.775391\n",
      "Train Epoch: 142 [151808/225000 (67%)] Loss: 7586.265625\n",
      "Train Epoch: 142 [155904/225000 (69%)] Loss: 7342.974609\n",
      "Train Epoch: 142 [160000/225000 (71%)] Loss: 7382.267578\n",
      "Train Epoch: 142 [164096/225000 (73%)] Loss: 7344.400391\n",
      "Train Epoch: 142 [168192/225000 (75%)] Loss: 7573.341797\n",
      "Train Epoch: 142 [172288/225000 (77%)] Loss: 7415.062500\n",
      "Train Epoch: 142 [176384/225000 (78%)] Loss: 7493.021484\n",
      "Train Epoch: 142 [180480/225000 (80%)] Loss: 7449.349609\n",
      "Train Epoch: 142 [184576/225000 (82%)] Loss: 7608.828125\n",
      "Train Epoch: 142 [188672/225000 (84%)] Loss: 7466.177734\n",
      "Train Epoch: 142 [192768/225000 (86%)] Loss: 7470.121094\n",
      "Train Epoch: 142 [196864/225000 (87%)] Loss: 7382.636719\n",
      "Train Epoch: 142 [200960/225000 (89%)] Loss: 7426.191406\n",
      "Train Epoch: 142 [205056/225000 (91%)] Loss: 7329.011719\n",
      "Train Epoch: 142 [209152/225000 (93%)] Loss: 7399.939453\n",
      "Train Epoch: 142 [213248/225000 (95%)] Loss: 7499.382812\n",
      "Train Epoch: 142 [217344/225000 (97%)] Loss: 7392.474609\n",
      "Train Epoch: 142 [221440/225000 (98%)] Loss: 7410.607422\n",
      "    epoch          : 142\n",
      "    loss           : 7418.351920239619\n",
      "    val_loss       : 7613.219827383148\n",
      "Train Epoch: 143 [256/225000 (0%)] Loss: 7300.230469\n",
      "Train Epoch: 143 [4352/225000 (2%)] Loss: 7401.910156\n",
      "Train Epoch: 143 [8448/225000 (4%)] Loss: 7411.304688\n",
      "Train Epoch: 143 [12544/225000 (6%)] Loss: 7353.435547\n",
      "Train Epoch: 143 [16640/225000 (7%)] Loss: 7330.117188\n",
      "Train Epoch: 143 [20736/225000 (9%)] Loss: 7494.312500\n",
      "Train Epoch: 143 [24832/225000 (11%)] Loss: 7418.666016\n",
      "Train Epoch: 143 [28928/225000 (13%)] Loss: 7451.416016\n",
      "Train Epoch: 143 [33024/225000 (15%)] Loss: 7204.298828\n",
      "Train Epoch: 143 [37120/225000 (16%)] Loss: 7555.660156\n",
      "Train Epoch: 143 [41216/225000 (18%)] Loss: 7350.763672\n",
      "Train Epoch: 143 [45312/225000 (20%)] Loss: 7403.722656\n",
      "Train Epoch: 143 [49408/225000 (22%)] Loss: 7309.859375\n",
      "Train Epoch: 143 [53504/225000 (24%)] Loss: 7345.019531\n",
      "Train Epoch: 143 [57600/225000 (26%)] Loss: 7391.482422\n",
      "Train Epoch: 143 [61696/225000 (27%)] Loss: 7425.556641\n",
      "Train Epoch: 143 [65792/225000 (29%)] Loss: 7460.876953\n",
      "Train Epoch: 143 [69888/225000 (31%)] Loss: 7506.150391\n",
      "Train Epoch: 143 [73984/225000 (33%)] Loss: 7338.214844\n",
      "Train Epoch: 143 [78080/225000 (35%)] Loss: 7333.732422\n",
      "Train Epoch: 143 [82176/225000 (37%)] Loss: 7477.335938\n",
      "Train Epoch: 143 [86272/225000 (38%)] Loss: 7346.857422\n",
      "Train Epoch: 143 [90368/225000 (40%)] Loss: 7326.603516\n",
      "Train Epoch: 143 [94464/225000 (42%)] Loss: 7488.007812\n",
      "Train Epoch: 143 [98560/225000 (44%)] Loss: 7328.830078\n",
      "Train Epoch: 143 [102656/225000 (46%)] Loss: 7399.324219\n",
      "Train Epoch: 143 [106752/225000 (47%)] Loss: 7284.261719\n",
      "Train Epoch: 143 [110848/225000 (49%)] Loss: 7512.212891\n",
      "Train Epoch: 143 [114944/225000 (51%)] Loss: 7418.140625\n",
      "Train Epoch: 143 [119040/225000 (53%)] Loss: 7506.970703\n",
      "Train Epoch: 143 [123136/225000 (55%)] Loss: 7294.460938\n",
      "Train Epoch: 143 [127232/225000 (57%)] Loss: 7510.531250\n",
      "Train Epoch: 143 [131328/225000 (58%)] Loss: 7392.908203\n",
      "Train Epoch: 143 [135424/225000 (60%)] Loss: 7276.972656\n",
      "Train Epoch: 143 [139520/225000 (62%)] Loss: 7445.619141\n",
      "Train Epoch: 143 [143616/225000 (64%)] Loss: 7469.435547\n",
      "Train Epoch: 143 [147712/225000 (66%)] Loss: 7484.265625\n",
      "Train Epoch: 143 [151808/225000 (67%)] Loss: 7512.062500\n",
      "Train Epoch: 143 [155904/225000 (69%)] Loss: 7478.152344\n",
      "Train Epoch: 143 [160000/225000 (71%)] Loss: 7288.726562\n",
      "Train Epoch: 143 [164096/225000 (73%)] Loss: 7535.228516\n",
      "Train Epoch: 143 [168192/225000 (75%)] Loss: 7539.140625\n",
      "Train Epoch: 143 [172288/225000 (77%)] Loss: 7349.128906\n",
      "Train Epoch: 143 [176384/225000 (78%)] Loss: 7460.373047\n",
      "Train Epoch: 143 [180480/225000 (80%)] Loss: 7412.041016\n",
      "Train Epoch: 143 [184576/225000 (82%)] Loss: 7523.146484\n",
      "Train Epoch: 143 [188672/225000 (84%)] Loss: 7333.917969\n",
      "Train Epoch: 143 [192768/225000 (86%)] Loss: 7380.273438\n",
      "Train Epoch: 143 [196864/225000 (87%)] Loss: 7309.615234\n",
      "Train Epoch: 143 [200960/225000 (89%)] Loss: 7424.451172\n",
      "Train Epoch: 143 [205056/225000 (91%)] Loss: 7418.060547\n",
      "Train Epoch: 143 [209152/225000 (93%)] Loss: 7354.427734\n",
      "Train Epoch: 143 [213248/225000 (95%)] Loss: 7290.541016\n",
      "Train Epoch: 143 [217344/225000 (97%)] Loss: 7515.939453\n",
      "Train Epoch: 143 [221440/225000 (98%)] Loss: 7587.361328\n",
      "    epoch          : 143\n",
      "    loss           : 7425.153975798137\n",
      "    val_loss       : 7389.571236724756\n",
      "Train Epoch: 144 [256/225000 (0%)] Loss: 7411.695312\n",
      "Train Epoch: 144 [4352/225000 (2%)] Loss: 7404.171875\n",
      "Train Epoch: 144 [8448/225000 (4%)] Loss: 7452.677734\n",
      "Train Epoch: 144 [12544/225000 (6%)] Loss: 7337.917969\n",
      "Train Epoch: 144 [16640/225000 (7%)] Loss: 7295.992188\n",
      "Train Epoch: 144 [20736/225000 (9%)] Loss: 7236.535156\n",
      "Train Epoch: 144 [24832/225000 (11%)] Loss: 7565.271484\n",
      "Train Epoch: 144 [28928/225000 (13%)] Loss: 7244.490234\n",
      "Train Epoch: 144 [33024/225000 (15%)] Loss: 7439.804688\n",
      "Train Epoch: 144 [37120/225000 (16%)] Loss: 7374.335938\n",
      "Train Epoch: 144 [41216/225000 (18%)] Loss: 7361.421875\n",
      "Train Epoch: 144 [45312/225000 (20%)] Loss: 7247.759766\n",
      "Train Epoch: 144 [49408/225000 (22%)] Loss: 7518.048828\n",
      "Train Epoch: 144 [53504/225000 (24%)] Loss: 7340.027344\n",
      "Train Epoch: 144 [57600/225000 (26%)] Loss: 7264.492188\n",
      "Train Epoch: 144 [61696/225000 (27%)] Loss: 7157.109375\n",
      "Train Epoch: 144 [65792/225000 (29%)] Loss: 7428.734375\n",
      "Train Epoch: 144 [69888/225000 (31%)] Loss: 7384.162109\n",
      "Train Epoch: 144 [73984/225000 (33%)] Loss: 7438.957031\n",
      "Train Epoch: 144 [78080/225000 (35%)] Loss: 7279.517578\n",
      "Train Epoch: 144 [82176/225000 (37%)] Loss: 7375.748047\n",
      "Train Epoch: 144 [86272/225000 (38%)] Loss: 7350.892578\n",
      "Train Epoch: 144 [90368/225000 (40%)] Loss: 7344.955078\n",
      "Train Epoch: 144 [94464/225000 (42%)] Loss: 7413.855469\n",
      "Train Epoch: 144 [98560/225000 (44%)] Loss: 7335.558594\n",
      "Train Epoch: 144 [102656/225000 (46%)] Loss: 7332.533203\n",
      "Train Epoch: 144 [106752/225000 (47%)] Loss: 7429.429688\n",
      "Train Epoch: 144 [110848/225000 (49%)] Loss: 7343.150391\n",
      "Train Epoch: 144 [114944/225000 (51%)] Loss: 7228.873047\n",
      "Train Epoch: 144 [119040/225000 (53%)] Loss: 7488.136719\n",
      "Train Epoch: 144 [123136/225000 (55%)] Loss: 7339.289062\n",
      "Train Epoch: 144 [127232/225000 (57%)] Loss: 7545.867188\n",
      "Train Epoch: 144 [131328/225000 (58%)] Loss: 7375.306641\n",
      "Train Epoch: 144 [135424/225000 (60%)] Loss: 7304.349609\n",
      "Train Epoch: 144 [139520/225000 (62%)] Loss: 7419.175781\n",
      "Train Epoch: 144 [143616/225000 (64%)] Loss: 7207.265625\n",
      "Train Epoch: 144 [147712/225000 (66%)] Loss: 7441.503906\n",
      "Train Epoch: 144 [151808/225000 (67%)] Loss: 7514.523438\n",
      "Train Epoch: 144 [155904/225000 (69%)] Loss: 7389.320312\n",
      "Train Epoch: 144 [160000/225000 (71%)] Loss: 7416.955078\n",
      "Train Epoch: 144 [164096/225000 (73%)] Loss: 7350.478516\n",
      "Train Epoch: 144 [168192/225000 (75%)] Loss: 7385.085938\n",
      "Train Epoch: 144 [172288/225000 (77%)] Loss: 7412.234375\n",
      "Train Epoch: 144 [176384/225000 (78%)] Loss: 7338.408203\n",
      "Train Epoch: 144 [180480/225000 (80%)] Loss: 7564.144531\n",
      "Train Epoch: 144 [184576/225000 (82%)] Loss: 7359.839844\n",
      "Train Epoch: 144 [188672/225000 (84%)] Loss: 7338.978516\n",
      "Train Epoch: 144 [192768/225000 (86%)] Loss: 7534.048828\n",
      "Train Epoch: 144 [196864/225000 (87%)] Loss: 7494.740234\n",
      "Train Epoch: 144 [200960/225000 (89%)] Loss: 7426.423828\n",
      "Train Epoch: 144 [205056/225000 (91%)] Loss: 7345.457031\n",
      "Train Epoch: 144 [209152/225000 (93%)] Loss: 7253.185547\n",
      "Train Epoch: 144 [213248/225000 (95%)] Loss: 7371.349609\n",
      "Train Epoch: 144 [217344/225000 (97%)] Loss: 7370.462891\n",
      "Train Epoch: 144 [221440/225000 (98%)] Loss: 7496.025391\n",
      "    epoch          : 144\n",
      "    loss           : 7419.706786831627\n",
      "    val_loss       : 7475.26179316944\n",
      "Train Epoch: 145 [256/225000 (0%)] Loss: 7660.269531\n",
      "Train Epoch: 145 [4352/225000 (2%)] Loss: 7452.859375\n",
      "Train Epoch: 145 [8448/225000 (4%)] Loss: 7575.039062\n",
      "Train Epoch: 145 [12544/225000 (6%)] Loss: 7535.775391\n",
      "Train Epoch: 145 [16640/225000 (7%)] Loss: 7350.279297\n",
      "Train Epoch: 145 [20736/225000 (9%)] Loss: 7320.121094\n",
      "Train Epoch: 145 [24832/225000 (11%)] Loss: 7338.541016\n",
      "Train Epoch: 145 [28928/225000 (13%)] Loss: 7302.955078\n",
      "Train Epoch: 145 [33024/225000 (15%)] Loss: 7462.521484\n",
      "Train Epoch: 145 [37120/225000 (16%)] Loss: 7400.068359\n",
      "Train Epoch: 145 [41216/225000 (18%)] Loss: 7436.697266\n",
      "Train Epoch: 145 [45312/225000 (20%)] Loss: 7490.660156\n",
      "Train Epoch: 145 [49408/225000 (22%)] Loss: 7323.759766\n",
      "Train Epoch: 145 [53504/225000 (24%)] Loss: 7359.275391\n",
      "Train Epoch: 145 [57600/225000 (26%)] Loss: 7343.673828\n",
      "Train Epoch: 145 [61696/225000 (27%)] Loss: 7261.498047\n",
      "Train Epoch: 145 [65792/225000 (29%)] Loss: 7486.574219\n",
      "Train Epoch: 145 [69888/225000 (31%)] Loss: 7370.845703\n",
      "Train Epoch: 145 [73984/225000 (33%)] Loss: 7516.541016\n",
      "Train Epoch: 145 [78080/225000 (35%)] Loss: 7189.617188\n",
      "Train Epoch: 145 [82176/225000 (37%)] Loss: 7211.845703\n",
      "Train Epoch: 145 [86272/225000 (38%)] Loss: 7448.783203\n",
      "Train Epoch: 145 [90368/225000 (40%)] Loss: 7355.822266\n",
      "Train Epoch: 145 [94464/225000 (42%)] Loss: 7346.724609\n",
      "Train Epoch: 145 [98560/225000 (44%)] Loss: 7473.810547\n",
      "Train Epoch: 145 [102656/225000 (46%)] Loss: 7362.640625\n",
      "Train Epoch: 145 [106752/225000 (47%)] Loss: 7429.083984\n",
      "Train Epoch: 145 [110848/225000 (49%)] Loss: 7404.208984\n",
      "Train Epoch: 145 [114944/225000 (51%)] Loss: 7378.863281\n",
      "Train Epoch: 145 [119040/225000 (53%)] Loss: 7232.216797\n",
      "Train Epoch: 145 [123136/225000 (55%)] Loss: 7416.396484\n",
      "Train Epoch: 145 [127232/225000 (57%)] Loss: 7306.820312\n",
      "Train Epoch: 145 [131328/225000 (58%)] Loss: 7405.798828\n",
      "Train Epoch: 145 [135424/225000 (60%)] Loss: 7367.183594\n",
      "Train Epoch: 145 [139520/225000 (62%)] Loss: 7436.820312\n",
      "Train Epoch: 145 [143616/225000 (64%)] Loss: 7379.662109\n",
      "Train Epoch: 145 [147712/225000 (66%)] Loss: 7515.943359\n",
      "Train Epoch: 145 [151808/225000 (67%)] Loss: 7198.253906\n",
      "Train Epoch: 145 [155904/225000 (69%)] Loss: 7377.433594\n",
      "Train Epoch: 145 [160000/225000 (71%)] Loss: 7703.648438\n",
      "Train Epoch: 145 [164096/225000 (73%)] Loss: 7337.562500\n",
      "Train Epoch: 145 [168192/225000 (75%)] Loss: 7449.970703\n",
      "Train Epoch: 145 [172288/225000 (77%)] Loss: 7462.912109\n",
      "Train Epoch: 145 [176384/225000 (78%)] Loss: 7217.441406\n",
      "Train Epoch: 145 [180480/225000 (80%)] Loss: 7323.275391\n",
      "Train Epoch: 145 [184576/225000 (82%)] Loss: 7332.396484\n",
      "Train Epoch: 145 [188672/225000 (84%)] Loss: 7350.832031\n",
      "Train Epoch: 145 [192768/225000 (86%)] Loss: 7288.298828\n",
      "Train Epoch: 145 [196864/225000 (87%)] Loss: 7374.804688\n",
      "Train Epoch: 145 [200960/225000 (89%)] Loss: 7496.490234\n",
      "Train Epoch: 145 [205056/225000 (91%)] Loss: 7396.939453\n",
      "Train Epoch: 145 [209152/225000 (93%)] Loss: 7210.591797\n",
      "Train Epoch: 145 [213248/225000 (95%)] Loss: 7322.578125\n",
      "Train Epoch: 145 [217344/225000 (97%)] Loss: 7363.011719\n",
      "Train Epoch: 145 [221440/225000 (98%)] Loss: 7256.992188\n",
      "    epoch          : 145\n",
      "    loss           : 7380.170669573024\n",
      "    val_loss       : 7381.161506908281\n",
      "Train Epoch: 146 [256/225000 (0%)] Loss: 7309.533203\n",
      "Train Epoch: 146 [4352/225000 (2%)] Loss: 7487.136719\n",
      "Train Epoch: 146 [8448/225000 (4%)] Loss: 7381.146484\n",
      "Train Epoch: 146 [12544/225000 (6%)] Loss: 7537.748047\n",
      "Train Epoch: 146 [16640/225000 (7%)] Loss: 7495.455078\n",
      "Train Epoch: 146 [20736/225000 (9%)] Loss: 7347.105469\n",
      "Train Epoch: 146 [24832/225000 (11%)] Loss: 7272.871094\n",
      "Train Epoch: 146 [28928/225000 (13%)] Loss: 7260.726562\n",
      "Train Epoch: 146 [33024/225000 (15%)] Loss: 7426.568359\n",
      "Train Epoch: 146 [37120/225000 (16%)] Loss: 7416.236328\n",
      "Train Epoch: 146 [41216/225000 (18%)] Loss: 7419.066406\n",
      "Train Epoch: 146 [45312/225000 (20%)] Loss: 7556.839844\n",
      "Train Epoch: 146 [49408/225000 (22%)] Loss: 7151.179688\n",
      "Train Epoch: 146 [53504/225000 (24%)] Loss: 7395.474609\n",
      "Train Epoch: 146 [57600/225000 (26%)] Loss: 7320.830078\n",
      "Train Epoch: 146 [61696/225000 (27%)] Loss: 17446.689453\n",
      "Train Epoch: 146 [65792/225000 (29%)] Loss: 7192.716797\n",
      "Train Epoch: 146 [69888/225000 (31%)] Loss: 7294.011719\n",
      "Train Epoch: 146 [73984/225000 (33%)] Loss: 7220.164062\n",
      "Train Epoch: 146 [78080/225000 (35%)] Loss: 7199.044922\n",
      "Train Epoch: 146 [82176/225000 (37%)] Loss: 7246.183594\n",
      "Train Epoch: 146 [86272/225000 (38%)] Loss: 7459.873047\n",
      "Train Epoch: 146 [90368/225000 (40%)] Loss: 7496.070312\n",
      "Train Epoch: 146 [94464/225000 (42%)] Loss: 7335.644531\n",
      "Train Epoch: 146 [98560/225000 (44%)] Loss: 7390.238281\n",
      "Train Epoch: 146 [102656/225000 (46%)] Loss: 7397.250000\n",
      "Train Epoch: 146 [106752/225000 (47%)] Loss: 7512.966797\n",
      "Train Epoch: 146 [110848/225000 (49%)] Loss: 7364.656250\n",
      "Train Epoch: 146 [114944/225000 (51%)] Loss: 7452.916016\n",
      "Train Epoch: 146 [119040/225000 (53%)] Loss: 7280.734375\n",
      "Train Epoch: 146 [123136/225000 (55%)] Loss: 7626.611328\n",
      "Train Epoch: 146 [127232/225000 (57%)] Loss: 7386.708984\n",
      "Train Epoch: 146 [131328/225000 (58%)] Loss: 7360.355469\n",
      "Train Epoch: 146 [135424/225000 (60%)] Loss: 7563.755859\n",
      "Train Epoch: 146 [139520/225000 (62%)] Loss: 7399.166016\n",
      "Train Epoch: 146 [143616/225000 (64%)] Loss: 7347.410156\n",
      "Train Epoch: 146 [147712/225000 (66%)] Loss: 7372.712891\n",
      "Train Epoch: 146 [151808/225000 (67%)] Loss: 7456.244141\n",
      "Train Epoch: 146 [155904/225000 (69%)] Loss: 7253.074219\n",
      "Train Epoch: 146 [160000/225000 (71%)] Loss: 7372.232422\n",
      "Train Epoch: 146 [164096/225000 (73%)] Loss: 7277.269531\n",
      "Train Epoch: 146 [168192/225000 (75%)] Loss: 7390.273438\n",
      "Train Epoch: 146 [172288/225000 (77%)] Loss: 7386.539062\n",
      "Train Epoch: 146 [176384/225000 (78%)] Loss: 7407.769531\n",
      "Train Epoch: 146 [180480/225000 (80%)] Loss: 7442.183594\n",
      "Train Epoch: 146 [184576/225000 (82%)] Loss: 7201.955078\n",
      "Train Epoch: 146 [188672/225000 (84%)] Loss: 7309.585938\n",
      "Train Epoch: 146 [192768/225000 (86%)] Loss: 7417.083984\n",
      "Train Epoch: 146 [196864/225000 (87%)] Loss: 7455.324219\n",
      "Train Epoch: 146 [200960/225000 (89%)] Loss: 7343.333984\n",
      "Train Epoch: 146 [205056/225000 (91%)] Loss: 7358.332031\n",
      "Train Epoch: 146 [209152/225000 (93%)] Loss: 7271.781250\n",
      "Train Epoch: 146 [213248/225000 (95%)] Loss: 7303.910156\n",
      "Train Epoch: 146 [217344/225000 (97%)] Loss: 7530.324219\n",
      "Train Epoch: 146 [221440/225000 (98%)] Loss: 7436.972656\n",
      "    epoch          : 146\n",
      "    loss           : 7386.681300661263\n",
      "    val_loss       : 7381.367362944447\n",
      "Train Epoch: 147 [256/225000 (0%)] Loss: 7549.970703\n",
      "Train Epoch: 147 [4352/225000 (2%)] Loss: 7235.027344\n",
      "Train Epoch: 147 [8448/225000 (4%)] Loss: 7252.990234\n",
      "Train Epoch: 147 [12544/225000 (6%)] Loss: 7174.197266\n",
      "Train Epoch: 147 [16640/225000 (7%)] Loss: 7298.800781\n",
      "Train Epoch: 147 [20736/225000 (9%)] Loss: 7405.294922\n",
      "Train Epoch: 147 [24832/225000 (11%)] Loss: 7299.947266\n",
      "Train Epoch: 147 [28928/225000 (13%)] Loss: 7323.126953\n",
      "Train Epoch: 147 [33024/225000 (15%)] Loss: 7246.457031\n",
      "Train Epoch: 147 [37120/225000 (16%)] Loss: 7303.169922\n",
      "Train Epoch: 147 [41216/225000 (18%)] Loss: 7236.925781\n",
      "Train Epoch: 147 [45312/225000 (20%)] Loss: 7469.781250\n",
      "Train Epoch: 147 [49408/225000 (22%)] Loss: 7245.632812\n",
      "Train Epoch: 147 [53504/225000 (24%)] Loss: 7449.550781\n",
      "Train Epoch: 147 [57600/225000 (26%)] Loss: 7296.560547\n",
      "Train Epoch: 147 [61696/225000 (27%)] Loss: 7366.128906\n",
      "Train Epoch: 147 [65792/225000 (29%)] Loss: 7415.427734\n",
      "Train Epoch: 147 [69888/225000 (31%)] Loss: 7292.917969\n",
      "Train Epoch: 147 [73984/225000 (33%)] Loss: 7292.341797\n",
      "Train Epoch: 147 [78080/225000 (35%)] Loss: 7374.587891\n",
      "Train Epoch: 147 [82176/225000 (37%)] Loss: 7290.619141\n",
      "Train Epoch: 147 [86272/225000 (38%)] Loss: 7385.050781\n",
      "Train Epoch: 147 [90368/225000 (40%)] Loss: 7250.080078\n",
      "Train Epoch: 147 [94464/225000 (42%)] Loss: 7353.111328\n",
      "Train Epoch: 147 [98560/225000 (44%)] Loss: 7121.371094\n",
      "Train Epoch: 147 [102656/225000 (46%)] Loss: 7253.185547\n",
      "Train Epoch: 147 [106752/225000 (47%)] Loss: 7455.343750\n",
      "Train Epoch: 147 [110848/225000 (49%)] Loss: 7368.505859\n",
      "Train Epoch: 147 [114944/225000 (51%)] Loss: 7304.738281\n",
      "Train Epoch: 147 [119040/225000 (53%)] Loss: 7446.837891\n",
      "Train Epoch: 147 [123136/225000 (55%)] Loss: 7426.197266\n",
      "Train Epoch: 147 [127232/225000 (57%)] Loss: 7465.306641\n",
      "Train Epoch: 147 [131328/225000 (58%)] Loss: 7357.337891\n",
      "Train Epoch: 147 [135424/225000 (60%)] Loss: 7351.750000\n",
      "Train Epoch: 147 [139520/225000 (62%)] Loss: 7335.908203\n",
      "Train Epoch: 147 [143616/225000 (64%)] Loss: 7378.716797\n",
      "Train Epoch: 147 [147712/225000 (66%)] Loss: 7339.796875\n",
      "Train Epoch: 147 [151808/225000 (67%)] Loss: 7281.609375\n",
      "Train Epoch: 147 [155904/225000 (69%)] Loss: 7378.132812\n",
      "Train Epoch: 147 [160000/225000 (71%)] Loss: 7468.621094\n",
      "Train Epoch: 147 [164096/225000 (73%)] Loss: 7312.382812\n",
      "Train Epoch: 147 [168192/225000 (75%)] Loss: 7350.701172\n",
      "Train Epoch: 147 [172288/225000 (77%)] Loss: 7409.968750\n",
      "Train Epoch: 147 [176384/225000 (78%)] Loss: 7375.687500\n",
      "Train Epoch: 147 [180480/225000 (80%)] Loss: 7387.955078\n",
      "Train Epoch: 147 [184576/225000 (82%)] Loss: 7474.886719\n",
      "Train Epoch: 147 [188672/225000 (84%)] Loss: 7448.625000\n",
      "Train Epoch: 147 [192768/225000 (86%)] Loss: 7396.123047\n",
      "Train Epoch: 147 [196864/225000 (87%)] Loss: 7332.255859\n",
      "Train Epoch: 147 [200960/225000 (89%)] Loss: 7427.882812\n",
      "Train Epoch: 147 [205056/225000 (91%)] Loss: 7284.359375\n",
      "Train Epoch: 147 [209152/225000 (93%)] Loss: 7402.603516\n",
      "Train Epoch: 147 [213248/225000 (95%)] Loss: 7255.332031\n",
      "Train Epoch: 147 [217344/225000 (97%)] Loss: 7396.902344\n",
      "Train Epoch: 147 [221440/225000 (98%)] Loss: 7283.800781\n",
      "    epoch          : 147\n",
      "    loss           : 7372.493598460609\n",
      "    val_loss       : 7365.877624476442\n",
      "Train Epoch: 148 [256/225000 (0%)] Loss: 7303.839844\n",
      "Train Epoch: 148 [4352/225000 (2%)] Loss: 7243.476562\n",
      "Train Epoch: 148 [8448/225000 (4%)] Loss: 7374.134766\n",
      "Train Epoch: 148 [12544/225000 (6%)] Loss: 7330.257812\n",
      "Train Epoch: 148 [16640/225000 (7%)] Loss: 7440.824219\n",
      "Train Epoch: 148 [20736/225000 (9%)] Loss: 7342.550781\n",
      "Train Epoch: 148 [24832/225000 (11%)] Loss: 7371.482422\n",
      "Train Epoch: 148 [28928/225000 (13%)] Loss: 7277.914062\n",
      "Train Epoch: 148 [33024/225000 (15%)] Loss: 7465.386719\n",
      "Train Epoch: 148 [37120/225000 (16%)] Loss: 7354.632812\n",
      "Train Epoch: 148 [41216/225000 (18%)] Loss: 7197.615234\n",
      "Train Epoch: 148 [45312/225000 (20%)] Loss: 7545.611328\n",
      "Train Epoch: 148 [49408/225000 (22%)] Loss: 7466.947266\n",
      "Train Epoch: 148 [53504/225000 (24%)] Loss: 7311.978516\n",
      "Train Epoch: 148 [57600/225000 (26%)] Loss: 7349.226562\n",
      "Train Epoch: 148 [61696/225000 (27%)] Loss: 7326.724609\n",
      "Train Epoch: 148 [65792/225000 (29%)] Loss: 7401.957031\n",
      "Train Epoch: 148 [69888/225000 (31%)] Loss: 7332.580078\n",
      "Train Epoch: 148 [73984/225000 (33%)] Loss: 7236.544922\n",
      "Train Epoch: 148 [78080/225000 (35%)] Loss: 7475.253906\n",
      "Train Epoch: 148 [82176/225000 (37%)] Loss: 7400.076172\n",
      "Train Epoch: 148 [86272/225000 (38%)] Loss: 7306.808594\n",
      "Train Epoch: 148 [90368/225000 (40%)] Loss: 7359.720703\n",
      "Train Epoch: 148 [94464/225000 (42%)] Loss: 7289.765625\n",
      "Train Epoch: 148 [98560/225000 (44%)] Loss: 7487.478516\n",
      "Train Epoch: 148 [102656/225000 (46%)] Loss: 7317.400391\n",
      "Train Epoch: 148 [106752/225000 (47%)] Loss: 7328.015625\n",
      "Train Epoch: 148 [110848/225000 (49%)] Loss: 7504.490234\n",
      "Train Epoch: 148 [114944/225000 (51%)] Loss: 7083.468750\n",
      "Train Epoch: 148 [119040/225000 (53%)] Loss: 7401.195312\n",
      "Train Epoch: 148 [123136/225000 (55%)] Loss: 7404.156250\n",
      "Train Epoch: 148 [127232/225000 (57%)] Loss: 7373.634766\n",
      "Train Epoch: 148 [131328/225000 (58%)] Loss: 7431.373047\n",
      "Train Epoch: 148 [135424/225000 (60%)] Loss: 7286.648438\n",
      "Train Epoch: 148 [139520/225000 (62%)] Loss: 7402.041016\n",
      "Train Epoch: 148 [143616/225000 (64%)] Loss: 7368.923828\n",
      "Train Epoch: 148 [147712/225000 (66%)] Loss: 7358.617188\n",
      "Train Epoch: 148 [151808/225000 (67%)] Loss: 7367.001953\n",
      "Train Epoch: 148 [155904/225000 (69%)] Loss: 7294.957031\n",
      "Train Epoch: 148 [160000/225000 (71%)] Loss: 7248.966797\n",
      "Train Epoch: 148 [164096/225000 (73%)] Loss: 7388.681641\n",
      "Train Epoch: 148 [168192/225000 (75%)] Loss: 7219.871094\n",
      "Train Epoch: 148 [172288/225000 (77%)] Loss: 7442.130859\n",
      "Train Epoch: 148 [176384/225000 (78%)] Loss: 7584.808594\n",
      "Train Epoch: 148 [180480/225000 (80%)] Loss: 7496.925781\n",
      "Train Epoch: 148 [184576/225000 (82%)] Loss: 7224.156250\n",
      "Train Epoch: 148 [188672/225000 (84%)] Loss: 7390.830078\n",
      "Train Epoch: 148 [192768/225000 (86%)] Loss: 7305.273438\n",
      "Train Epoch: 148 [196864/225000 (87%)] Loss: 7454.078125\n",
      "Train Epoch: 148 [200960/225000 (89%)] Loss: 7584.972656\n",
      "Train Epoch: 148 [205056/225000 (91%)] Loss: 7299.945312\n",
      "Train Epoch: 148 [209152/225000 (93%)] Loss: 7380.945312\n",
      "Train Epoch: 148 [213248/225000 (95%)] Loss: 7398.126953\n",
      "Train Epoch: 148 [217344/225000 (97%)] Loss: 7459.789062\n",
      "Train Epoch: 148 [221440/225000 (98%)] Loss: 7243.570312\n",
      "    epoch          : 148\n",
      "    loss           : 7379.221616360922\n",
      "    val_loss       : 7358.924546219865\n",
      "Train Epoch: 149 [256/225000 (0%)] Loss: 7343.238281\n",
      "Train Epoch: 149 [4352/225000 (2%)] Loss: 7373.673828\n",
      "Train Epoch: 149 [8448/225000 (4%)] Loss: 7261.783203\n",
      "Train Epoch: 149 [12544/225000 (6%)] Loss: 12624.957031\n",
      "Train Epoch: 149 [16640/225000 (7%)] Loss: 7257.576172\n",
      "Train Epoch: 149 [20736/225000 (9%)] Loss: 7139.423828\n",
      "Train Epoch: 149 [24832/225000 (11%)] Loss: 7369.669922\n",
      "Train Epoch: 149 [28928/225000 (13%)] Loss: 7311.759766\n",
      "Train Epoch: 149 [33024/225000 (15%)] Loss: 7213.359375\n",
      "Train Epoch: 149 [37120/225000 (16%)] Loss: 7380.339844\n",
      "Train Epoch: 149 [41216/225000 (18%)] Loss: 7208.996094\n",
      "Train Epoch: 149 [45312/225000 (20%)] Loss: 7338.898438\n",
      "Train Epoch: 149 [49408/225000 (22%)] Loss: 7264.593750\n",
      "Train Epoch: 149 [53504/225000 (24%)] Loss: 7303.906250\n",
      "Train Epoch: 149 [57600/225000 (26%)] Loss: 7430.726562\n",
      "Train Epoch: 149 [61696/225000 (27%)] Loss: 7267.513672\n",
      "Train Epoch: 149 [65792/225000 (29%)] Loss: 7402.580078\n",
      "Train Epoch: 149 [69888/225000 (31%)] Loss: 7338.414062\n",
      "Train Epoch: 149 [73984/225000 (33%)] Loss: 7223.849609\n",
      "Train Epoch: 149 [78080/225000 (35%)] Loss: 7556.921875\n",
      "Train Epoch: 149 [82176/225000 (37%)] Loss: 7348.173828\n",
      "Train Epoch: 149 [86272/225000 (38%)] Loss: 7520.269531\n",
      "Train Epoch: 149 [90368/225000 (40%)] Loss: 7248.142578\n",
      "Train Epoch: 149 [94464/225000 (42%)] Loss: 7469.595703\n",
      "Train Epoch: 149 [98560/225000 (44%)] Loss: 7367.253906\n",
      "Train Epoch: 149 [102656/225000 (46%)] Loss: 7328.976562\n",
      "Train Epoch: 149 [106752/225000 (47%)] Loss: 7342.232422\n",
      "Train Epoch: 149 [110848/225000 (49%)] Loss: 7413.386719\n",
      "Train Epoch: 149 [114944/225000 (51%)] Loss: 7309.728516\n",
      "Train Epoch: 149 [119040/225000 (53%)] Loss: 7346.960938\n",
      "Train Epoch: 149 [123136/225000 (55%)] Loss: 7340.976562\n",
      "Train Epoch: 149 [127232/225000 (57%)] Loss: 7393.025391\n",
      "Train Epoch: 149 [131328/225000 (58%)] Loss: 7354.974609\n",
      "Train Epoch: 149 [135424/225000 (60%)] Loss: 7217.933594\n",
      "Train Epoch: 149 [139520/225000 (62%)] Loss: 7502.056641\n",
      "Train Epoch: 149 [143616/225000 (64%)] Loss: 7304.492188\n",
      "Train Epoch: 149 [147712/225000 (66%)] Loss: 7456.541016\n",
      "Train Epoch: 149 [151808/225000 (67%)] Loss: 7316.777344\n",
      "Train Epoch: 149 [155904/225000 (69%)] Loss: 7318.285156\n",
      "Train Epoch: 149 [160000/225000 (71%)] Loss: 7414.189453\n",
      "Train Epoch: 149 [164096/225000 (73%)] Loss: 7291.281250\n",
      "Train Epoch: 149 [168192/225000 (75%)] Loss: 7359.859375\n",
      "Train Epoch: 149 [172288/225000 (77%)] Loss: 7301.093750\n",
      "Train Epoch: 149 [176384/225000 (78%)] Loss: 7537.138672\n",
      "Train Epoch: 149 [180480/225000 (80%)] Loss: 7366.787109\n",
      "Train Epoch: 149 [184576/225000 (82%)] Loss: 7376.496094\n",
      "Train Epoch: 149 [188672/225000 (84%)] Loss: 7204.056641\n",
      "Train Epoch: 149 [192768/225000 (86%)] Loss: 7228.804688\n",
      "Train Epoch: 149 [196864/225000 (87%)] Loss: 7330.347656\n",
      "Train Epoch: 149 [200960/225000 (89%)] Loss: 7444.675781\n",
      "Train Epoch: 149 [205056/225000 (91%)] Loss: 7426.468750\n",
      "Train Epoch: 149 [209152/225000 (93%)] Loss: 7445.767578\n",
      "Train Epoch: 149 [213248/225000 (95%)] Loss: 7170.380859\n",
      "Train Epoch: 149 [217344/225000 (97%)] Loss: 7227.613281\n",
      "Train Epoch: 149 [221440/225000 (98%)] Loss: 7290.187500\n",
      "    epoch          : 149\n",
      "    loss           : 7398.579164889078\n",
      "    val_loss       : 7357.907887764123\n",
      "Train Epoch: 150 [256/225000 (0%)] Loss: 7482.333984\n",
      "Train Epoch: 150 [4352/225000 (2%)] Loss: 7270.628906\n",
      "Train Epoch: 150 [8448/225000 (4%)] Loss: 7323.531250\n",
      "Train Epoch: 150 [12544/225000 (6%)] Loss: 7387.251953\n",
      "Train Epoch: 150 [16640/225000 (7%)] Loss: 7429.548828\n",
      "Train Epoch: 150 [20736/225000 (9%)] Loss: 7382.474609\n",
      "Train Epoch: 150 [24832/225000 (11%)] Loss: 7293.205078\n",
      "Train Epoch: 150 [28928/225000 (13%)] Loss: 7306.871094\n",
      "Train Epoch: 150 [33024/225000 (15%)] Loss: 7289.144531\n",
      "Train Epoch: 150 [37120/225000 (16%)] Loss: 7332.328125\n",
      "Train Epoch: 150 [41216/225000 (18%)] Loss: 7405.648438\n",
      "Train Epoch: 150 [45312/225000 (20%)] Loss: 7396.697266\n",
      "Train Epoch: 150 [49408/225000 (22%)] Loss: 7329.386719\n",
      "Train Epoch: 150 [53504/225000 (24%)] Loss: 7238.544922\n",
      "Train Epoch: 150 [57600/225000 (26%)] Loss: 7418.212891\n",
      "Train Epoch: 150 [61696/225000 (27%)] Loss: 7581.550781\n",
      "Train Epoch: 150 [65792/225000 (29%)] Loss: 7413.748047\n",
      "Train Epoch: 150 [69888/225000 (31%)] Loss: 7282.474609\n",
      "Train Epoch: 150 [73984/225000 (33%)] Loss: 7424.947266\n",
      "Train Epoch: 150 [78080/225000 (35%)] Loss: 7312.966797\n",
      "Train Epoch: 150 [82176/225000 (37%)] Loss: 7307.953125\n",
      "Train Epoch: 150 [86272/225000 (38%)] Loss: 7342.255859\n",
      "Train Epoch: 150 [90368/225000 (40%)] Loss: 7322.554688\n",
      "Train Epoch: 150 [94464/225000 (42%)] Loss: 7377.515625\n",
      "Train Epoch: 150 [98560/225000 (44%)] Loss: 7381.416016\n",
      "Train Epoch: 150 [102656/225000 (46%)] Loss: 7305.322266\n",
      "Train Epoch: 150 [106752/225000 (47%)] Loss: 7371.404297\n",
      "Train Epoch: 150 [110848/225000 (49%)] Loss: 7235.505859\n",
      "Train Epoch: 150 [114944/225000 (51%)] Loss: 7523.089844\n",
      "Train Epoch: 150 [119040/225000 (53%)] Loss: 7251.267578\n",
      "Train Epoch: 150 [123136/225000 (55%)] Loss: 7430.783203\n",
      "Train Epoch: 150 [127232/225000 (57%)] Loss: 7353.113281\n",
      "Train Epoch: 150 [131328/225000 (58%)] Loss: 7361.529297\n",
      "Train Epoch: 150 [135424/225000 (60%)] Loss: 7442.042969\n",
      "Train Epoch: 150 [139520/225000 (62%)] Loss: 7339.529297\n",
      "Train Epoch: 150 [143616/225000 (64%)] Loss: 7336.107422\n",
      "Train Epoch: 150 [147712/225000 (66%)] Loss: 7309.435547\n",
      "Train Epoch: 150 [151808/225000 (67%)] Loss: 7239.554688\n",
      "Train Epoch: 150 [155904/225000 (69%)] Loss: 7305.480469\n",
      "Train Epoch: 150 [160000/225000 (71%)] Loss: 7342.246094\n",
      "Train Epoch: 150 [164096/225000 (73%)] Loss: 7320.896484\n",
      "Train Epoch: 150 [168192/225000 (75%)] Loss: 7365.990234\n",
      "Train Epoch: 150 [172288/225000 (77%)] Loss: 7298.505859\n",
      "Train Epoch: 150 [176384/225000 (78%)] Loss: 7369.046875\n",
      "Train Epoch: 150 [180480/225000 (80%)] Loss: 7358.867188\n",
      "Train Epoch: 150 [184576/225000 (82%)] Loss: 7433.294922\n",
      "Train Epoch: 150 [188672/225000 (84%)] Loss: 7386.158203\n",
      "Train Epoch: 150 [192768/225000 (86%)] Loss: 7282.820312\n",
      "Train Epoch: 150 [196864/225000 (87%)] Loss: 7422.539062\n",
      "Train Epoch: 150 [200960/225000 (89%)] Loss: 7260.185547\n",
      "Train Epoch: 150 [205056/225000 (91%)] Loss: 7393.390625\n",
      "Train Epoch: 150 [209152/225000 (93%)] Loss: 7304.251953\n",
      "Train Epoch: 150 [213248/225000 (95%)] Loss: 7248.255859\n",
      "Train Epoch: 150 [217344/225000 (97%)] Loss: 7112.892578\n",
      "Train Epoch: 150 [221440/225000 (98%)] Loss: 7338.300781\n",
      "    epoch          : 150\n",
      "    loss           : 7363.674395842221\n",
      "    val_loss       : 7349.598738664267\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0827_225017/checkpoint-epoch150.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 151 [256/225000 (0%)] Loss: 7410.910156\n",
      "Train Epoch: 151 [4352/225000 (2%)] Loss: 7232.384766\n",
      "Train Epoch: 151 [8448/225000 (4%)] Loss: 7254.322266\n",
      "Train Epoch: 151 [12544/225000 (6%)] Loss: 7267.152344\n",
      "Train Epoch: 151 [16640/225000 (7%)] Loss: 7416.873047\n",
      "Train Epoch: 151 [20736/225000 (9%)] Loss: 7382.390625\n",
      "Train Epoch: 151 [24832/225000 (11%)] Loss: 7315.060547\n",
      "Train Epoch: 151 [28928/225000 (13%)] Loss: 7268.820312\n",
      "Train Epoch: 151 [33024/225000 (15%)] Loss: 7298.267578\n",
      "Train Epoch: 151 [37120/225000 (16%)] Loss: 7307.925781\n",
      "Train Epoch: 151 [41216/225000 (18%)] Loss: 7334.923828\n",
      "Train Epoch: 151 [45312/225000 (20%)] Loss: 7436.466797\n",
      "Train Epoch: 151 [49408/225000 (22%)] Loss: 7453.242188\n",
      "Train Epoch: 151 [53504/225000 (24%)] Loss: 7248.160156\n",
      "Train Epoch: 151 [57600/225000 (26%)] Loss: 7170.853516\n",
      "Train Epoch: 151 [61696/225000 (27%)] Loss: 7448.751953\n",
      "Train Epoch: 151 [65792/225000 (29%)] Loss: 7492.384766\n",
      "Train Epoch: 151 [69888/225000 (31%)] Loss: 7339.285156\n",
      "Train Epoch: 151 [73984/225000 (33%)] Loss: 7358.589844\n",
      "Train Epoch: 151 [78080/225000 (35%)] Loss: 7510.312500\n",
      "Train Epoch: 151 [82176/225000 (37%)] Loss: 7272.646484\n",
      "Train Epoch: 151 [86272/225000 (38%)] Loss: 7408.839844\n",
      "Train Epoch: 151 [90368/225000 (40%)] Loss: 7410.628906\n",
      "Train Epoch: 151 [94464/225000 (42%)] Loss: 7280.490234\n",
      "Train Epoch: 151 [98560/225000 (44%)] Loss: 7456.453125\n",
      "Train Epoch: 151 [102656/225000 (46%)] Loss: 7307.464844\n",
      "Train Epoch: 151 [106752/225000 (47%)] Loss: 7286.507812\n",
      "Train Epoch: 151 [110848/225000 (49%)] Loss: 7328.246094\n",
      "Train Epoch: 151 [114944/225000 (51%)] Loss: 7366.839844\n",
      "Train Epoch: 151 [119040/225000 (53%)] Loss: 7224.398438\n",
      "Train Epoch: 151 [123136/225000 (55%)] Loss: 7294.750000\n",
      "Train Epoch: 151 [127232/225000 (57%)] Loss: 7375.482422\n",
      "Train Epoch: 151 [131328/225000 (58%)] Loss: 7397.234375\n",
      "Train Epoch: 151 [135424/225000 (60%)] Loss: 7551.279297\n",
      "Train Epoch: 151 [139520/225000 (62%)] Loss: 7357.187500\n",
      "Train Epoch: 151 [143616/225000 (64%)] Loss: 7480.648438\n",
      "Train Epoch: 151 [147712/225000 (66%)] Loss: 7319.306641\n",
      "Train Epoch: 151 [151808/225000 (67%)] Loss: 7239.621094\n",
      "Train Epoch: 151 [155904/225000 (69%)] Loss: 7427.044922\n",
      "Train Epoch: 151 [160000/225000 (71%)] Loss: 7264.560547\n",
      "Train Epoch: 151 [164096/225000 (73%)] Loss: 7445.474609\n",
      "Train Epoch: 151 [168192/225000 (75%)] Loss: 7438.246094\n",
      "Train Epoch: 151 [172288/225000 (77%)] Loss: 7312.357422\n",
      "Train Epoch: 151 [176384/225000 (78%)] Loss: 7487.201172\n",
      "Train Epoch: 151 [180480/225000 (80%)] Loss: 7417.103516\n",
      "Train Epoch: 151 [184576/225000 (82%)] Loss: 7276.166016\n",
      "Train Epoch: 151 [188672/225000 (84%)] Loss: 7376.105469\n",
      "Train Epoch: 151 [192768/225000 (86%)] Loss: 7405.779297\n",
      "Train Epoch: 151 [196864/225000 (87%)] Loss: 7237.449219\n",
      "Train Epoch: 151 [200960/225000 (89%)] Loss: 7478.908203\n",
      "Train Epoch: 151 [205056/225000 (91%)] Loss: 7267.609375\n",
      "Train Epoch: 151 [209152/225000 (93%)] Loss: 7360.416016\n",
      "Train Epoch: 151 [213248/225000 (95%)] Loss: 7263.626953\n",
      "Train Epoch: 151 [217344/225000 (97%)] Loss: 7350.208984\n",
      "Train Epoch: 151 [221440/225000 (98%)] Loss: 7398.140625\n",
      "    epoch          : 151\n",
      "    loss           : 7360.0145384492325\n",
      "    val_loss       : 7343.47543805716\n",
      "Train Epoch: 152 [256/225000 (0%)] Loss: 7254.462891\n",
      "Train Epoch: 152 [4352/225000 (2%)] Loss: 7297.341797\n",
      "Train Epoch: 152 [8448/225000 (4%)] Loss: 7473.291016\n",
      "Train Epoch: 152 [12544/225000 (6%)] Loss: 7296.330078\n",
      "Train Epoch: 152 [16640/225000 (7%)] Loss: 7312.017578\n",
      "Train Epoch: 152 [20736/225000 (9%)] Loss: 7323.673828\n",
      "Train Epoch: 152 [24832/225000 (11%)] Loss: 7257.888672\n",
      "Train Epoch: 152 [28928/225000 (13%)] Loss: 7513.216797\n",
      "Train Epoch: 152 [33024/225000 (15%)] Loss: 7415.941406\n",
      "Train Epoch: 152 [37120/225000 (16%)] Loss: 7232.332031\n",
      "Train Epoch: 152 [41216/225000 (18%)] Loss: 7397.564453\n",
      "Train Epoch: 152 [45312/225000 (20%)] Loss: 7457.863281\n",
      "Train Epoch: 152 [49408/225000 (22%)] Loss: 7526.343750\n",
      "Train Epoch: 152 [53504/225000 (24%)] Loss: 7351.962891\n",
      "Train Epoch: 152 [57600/225000 (26%)] Loss: 7400.800781\n",
      "Train Epoch: 152 [61696/225000 (27%)] Loss: 7270.564453\n",
      "Train Epoch: 152 [65792/225000 (29%)] Loss: 7378.613281\n",
      "Train Epoch: 152 [69888/225000 (31%)] Loss: 7358.345703\n",
      "Train Epoch: 152 [73984/225000 (33%)] Loss: 7327.859375\n",
      "Train Epoch: 152 [78080/225000 (35%)] Loss: 7380.046875\n",
      "Train Epoch: 152 [82176/225000 (37%)] Loss: 7338.597656\n",
      "Train Epoch: 152 [86272/225000 (38%)] Loss: 7310.435547\n",
      "Train Epoch: 152 [90368/225000 (40%)] Loss: 7552.992188\n",
      "Train Epoch: 152 [94464/225000 (42%)] Loss: 7145.300781\n",
      "Train Epoch: 152 [98560/225000 (44%)] Loss: 7226.347656\n",
      "Train Epoch: 152 [102656/225000 (46%)] Loss: 7454.632812\n",
      "Train Epoch: 152 [106752/225000 (47%)] Loss: 7532.390625\n",
      "Train Epoch: 152 [110848/225000 (49%)] Loss: 7315.328125\n",
      "Train Epoch: 152 [114944/225000 (51%)] Loss: 7522.933594\n",
      "Train Epoch: 152 [119040/225000 (53%)] Loss: 7395.947266\n",
      "Train Epoch: 152 [123136/225000 (55%)] Loss: 7264.687500\n",
      "Train Epoch: 152 [127232/225000 (57%)] Loss: 7209.083984\n",
      "Train Epoch: 152 [131328/225000 (58%)] Loss: 7324.855469\n",
      "Train Epoch: 152 [135424/225000 (60%)] Loss: 7216.498047\n",
      "Train Epoch: 152 [139520/225000 (62%)] Loss: 7266.333984\n",
      "Train Epoch: 152 [143616/225000 (64%)] Loss: 7396.507812\n",
      "Train Epoch: 152 [147712/225000 (66%)] Loss: 7489.742188\n",
      "Train Epoch: 152 [151808/225000 (67%)] Loss: 7244.757812\n",
      "Train Epoch: 152 [155904/225000 (69%)] Loss: 7382.404297\n",
      "Train Epoch: 152 [160000/225000 (71%)] Loss: 7444.007812\n",
      "Train Epoch: 152 [164096/225000 (73%)] Loss: 7293.029297\n",
      "Train Epoch: 152 [168192/225000 (75%)] Loss: 7299.681641\n",
      "Train Epoch: 152 [172288/225000 (77%)] Loss: 7268.750000\n",
      "Train Epoch: 152 [176384/225000 (78%)] Loss: 7428.580078\n",
      "Train Epoch: 152 [180480/225000 (80%)] Loss: 7466.900391\n",
      "Train Epoch: 152 [184576/225000 (82%)] Loss: 7313.894531\n",
      "Train Epoch: 152 [188672/225000 (84%)] Loss: 7292.869141\n",
      "Train Epoch: 152 [192768/225000 (86%)] Loss: 7331.550781\n",
      "Train Epoch: 152 [196864/225000 (87%)] Loss: 7311.546875\n",
      "Train Epoch: 152 [200960/225000 (89%)] Loss: 7283.287109\n",
      "Train Epoch: 152 [205056/225000 (91%)] Loss: 7422.972656\n",
      "Train Epoch: 152 [209152/225000 (93%)] Loss: 7253.683594\n",
      "Train Epoch: 152 [213248/225000 (95%)] Loss: 7448.320312\n",
      "Train Epoch: 152 [217344/225000 (97%)] Loss: 7334.443359\n",
      "Train Epoch: 152 [221440/225000 (98%)] Loss: 7283.556641\n",
      "    epoch          : 152\n",
      "    loss           : 7361.475735921502\n",
      "    val_loss       : 7341.032851182685\n",
      "Train Epoch: 153 [256/225000 (0%)] Loss: 7297.544922\n",
      "Train Epoch: 153 [4352/225000 (2%)] Loss: 7356.144531\n",
      "Train Epoch: 153 [8448/225000 (4%)] Loss: 7353.138672\n",
      "Train Epoch: 153 [12544/225000 (6%)] Loss: 7344.519531\n",
      "Train Epoch: 153 [16640/225000 (7%)] Loss: 7380.882812\n",
      "Train Epoch: 153 [20736/225000 (9%)] Loss: 7398.406250\n",
      "Train Epoch: 153 [24832/225000 (11%)] Loss: 7318.228516\n",
      "Train Epoch: 153 [28928/225000 (13%)] Loss: 7341.050781\n",
      "Train Epoch: 153 [33024/225000 (15%)] Loss: 7329.421875\n",
      "Train Epoch: 153 [37120/225000 (16%)] Loss: 7297.507812\n",
      "Train Epoch: 153 [41216/225000 (18%)] Loss: 7390.123047\n",
      "Train Epoch: 153 [45312/225000 (20%)] Loss: 7303.714844\n",
      "Train Epoch: 153 [49408/225000 (22%)] Loss: 7394.044922\n",
      "Train Epoch: 153 [53504/225000 (24%)] Loss: 7362.281250\n",
      "Train Epoch: 153 [57600/225000 (26%)] Loss: 7264.968750\n",
      "Train Epoch: 153 [61696/225000 (27%)] Loss: 7338.955078\n",
      "Train Epoch: 153 [65792/225000 (29%)] Loss: 7265.917969\n",
      "Train Epoch: 153 [69888/225000 (31%)] Loss: 7461.222656\n",
      "Train Epoch: 153 [73984/225000 (33%)] Loss: 7418.126953\n",
      "Train Epoch: 153 [78080/225000 (35%)] Loss: 7321.269531\n",
      "Train Epoch: 153 [82176/225000 (37%)] Loss: 7369.416016\n",
      "Train Epoch: 153 [86272/225000 (38%)] Loss: 7182.666016\n",
      "Train Epoch: 153 [90368/225000 (40%)] Loss: 7409.539062\n",
      "Train Epoch: 153 [94464/225000 (42%)] Loss: 7239.601562\n",
      "Train Epoch: 153 [98560/225000 (44%)] Loss: 7401.736328\n",
      "Train Epoch: 153 [102656/225000 (46%)] Loss: 7184.794922\n",
      "Train Epoch: 153 [106752/225000 (47%)] Loss: 7358.353516\n",
      "Train Epoch: 153 [110848/225000 (49%)] Loss: 7579.425781\n",
      "Train Epoch: 153 [114944/225000 (51%)] Loss: 7374.484375\n",
      "Train Epoch: 153 [119040/225000 (53%)] Loss: 7475.849609\n",
      "Train Epoch: 153 [123136/225000 (55%)] Loss: 7411.746094\n",
      "Train Epoch: 153 [127232/225000 (57%)] Loss: 6986.931641\n",
      "Train Epoch: 153 [131328/225000 (58%)] Loss: 7271.291016\n",
      "Train Epoch: 153 [135424/225000 (60%)] Loss: 7348.654297\n",
      "Train Epoch: 153 [139520/225000 (62%)] Loss: 7562.142578\n",
      "Train Epoch: 153 [143616/225000 (64%)] Loss: 7461.080078\n",
      "Train Epoch: 153 [147712/225000 (66%)] Loss: 7271.638672\n",
      "Train Epoch: 153 [151808/225000 (67%)] Loss: 7469.349609\n",
      "Train Epoch: 153 [155904/225000 (69%)] Loss: 7302.957031\n",
      "Train Epoch: 153 [160000/225000 (71%)] Loss: 7427.230469\n",
      "Train Epoch: 153 [164096/225000 (73%)] Loss: 7322.099609\n",
      "Train Epoch: 153 [168192/225000 (75%)] Loss: 7210.181641\n",
      "Train Epoch: 153 [172288/225000 (77%)] Loss: 7201.048828\n",
      "Train Epoch: 153 [176384/225000 (78%)] Loss: 7247.128906\n",
      "Train Epoch: 153 [180480/225000 (80%)] Loss: 7269.507812\n",
      "Train Epoch: 153 [184576/225000 (82%)] Loss: 7296.005859\n",
      "Train Epoch: 153 [188672/225000 (84%)] Loss: 7387.648438\n",
      "Train Epoch: 153 [192768/225000 (86%)] Loss: 7361.798828\n",
      "Train Epoch: 153 [196864/225000 (87%)] Loss: 7337.492188\n",
      "Train Epoch: 153 [200960/225000 (89%)] Loss: 7427.023438\n",
      "Train Epoch: 153 [205056/225000 (91%)] Loss: 7408.062500\n",
      "Train Epoch: 153 [209152/225000 (93%)] Loss: 7168.335938\n",
      "Train Epoch: 153 [213248/225000 (95%)] Loss: 7275.490234\n",
      "Train Epoch: 153 [217344/225000 (97%)] Loss: 7342.240234\n",
      "Train Epoch: 153 [221440/225000 (98%)] Loss: 7251.669922\n",
      "    epoch          : 153\n",
      "    loss           : 7355.697601144767\n",
      "    val_loss       : 7521.223501760132\n",
      "Train Epoch: 154 [256/225000 (0%)] Loss: 7261.531250\n",
      "Train Epoch: 154 [4352/225000 (2%)] Loss: 7334.435547\n",
      "Train Epoch: 154 [8448/225000 (4%)] Loss: 7300.214844\n",
      "Train Epoch: 154 [12544/225000 (6%)] Loss: 7362.871094\n",
      "Train Epoch: 154 [16640/225000 (7%)] Loss: 7357.412109\n",
      "Train Epoch: 154 [20736/225000 (9%)] Loss: 7338.685547\n",
      "Train Epoch: 154 [24832/225000 (11%)] Loss: 7431.187500\n",
      "Train Epoch: 154 [28928/225000 (13%)] Loss: 7400.464844\n",
      "Train Epoch: 154 [33024/225000 (15%)] Loss: 7382.066406\n",
      "Train Epoch: 154 [37120/225000 (16%)] Loss: 7255.835938\n",
      "Train Epoch: 154 [41216/225000 (18%)] Loss: 7240.365234\n",
      "Train Epoch: 154 [45312/225000 (20%)] Loss: 7371.990234\n",
      "Train Epoch: 154 [49408/225000 (22%)] Loss: 7338.917969\n",
      "Train Epoch: 154 [53504/225000 (24%)] Loss: 7286.798828\n",
      "Train Epoch: 154 [57600/225000 (26%)] Loss: 7424.550781\n",
      "Train Epoch: 154 [61696/225000 (27%)] Loss: 7439.013672\n",
      "Train Epoch: 154 [65792/225000 (29%)] Loss: 7253.548828\n",
      "Train Epoch: 154 [69888/225000 (31%)] Loss: 7217.353516\n",
      "Train Epoch: 154 [73984/225000 (33%)] Loss: 7362.361328\n",
      "Train Epoch: 154 [78080/225000 (35%)] Loss: 7400.738281\n",
      "Train Epoch: 154 [82176/225000 (37%)] Loss: 7504.830078\n",
      "Train Epoch: 154 [86272/225000 (38%)] Loss: 7409.314453\n",
      "Train Epoch: 154 [90368/225000 (40%)] Loss: 7523.990234\n",
      "Train Epoch: 154 [94464/225000 (42%)] Loss: 7433.458984\n",
      "Train Epoch: 154 [98560/225000 (44%)] Loss: 7314.318359\n",
      "Train Epoch: 154 [102656/225000 (46%)] Loss: 7156.728516\n",
      "Train Epoch: 154 [106752/225000 (47%)] Loss: 7316.343750\n",
      "Train Epoch: 154 [110848/225000 (49%)] Loss: 7232.146484\n",
      "Train Epoch: 154 [114944/225000 (51%)] Loss: 7269.824219\n",
      "Train Epoch: 154 [119040/225000 (53%)] Loss: 7251.244141\n",
      "Train Epoch: 154 [123136/225000 (55%)] Loss: 7188.402344\n",
      "Train Epoch: 154 [127232/225000 (57%)] Loss: 7407.949219\n",
      "Train Epoch: 154 [131328/225000 (58%)] Loss: 7211.466797\n",
      "Train Epoch: 154 [135424/225000 (60%)] Loss: 7387.126953\n",
      "Train Epoch: 154 [139520/225000 (62%)] Loss: 7291.458984\n",
      "Train Epoch: 154 [143616/225000 (64%)] Loss: 7198.720703\n",
      "Train Epoch: 154 [147712/225000 (66%)] Loss: 7124.867188\n",
      "Train Epoch: 154 [151808/225000 (67%)] Loss: 7353.876953\n",
      "Train Epoch: 154 [155904/225000 (69%)] Loss: 7337.246094\n",
      "Train Epoch: 154 [160000/225000 (71%)] Loss: 7472.160156\n",
      "Train Epoch: 154 [164096/225000 (73%)] Loss: 7350.105469\n",
      "Train Epoch: 154 [168192/225000 (75%)] Loss: 7297.447266\n",
      "Train Epoch: 154 [172288/225000 (77%)] Loss: 7403.386719\n",
      "Train Epoch: 154 [176384/225000 (78%)] Loss: 7334.605469\n",
      "Train Epoch: 154 [180480/225000 (80%)] Loss: 7290.546875\n",
      "Train Epoch: 154 [184576/225000 (82%)] Loss: 7257.593750\n",
      "Train Epoch: 154 [188672/225000 (84%)] Loss: 7416.666016\n",
      "Train Epoch: 154 [192768/225000 (86%)] Loss: 7449.365234\n",
      "Train Epoch: 154 [196864/225000 (87%)] Loss: 7292.048828\n",
      "Train Epoch: 154 [200960/225000 (89%)] Loss: 7302.394531\n",
      "Train Epoch: 154 [205056/225000 (91%)] Loss: 7409.580078\n",
      "Train Epoch: 154 [209152/225000 (93%)] Loss: 7307.134766\n",
      "Train Epoch: 154 [213248/225000 (95%)] Loss: 7302.150391\n",
      "Train Epoch: 154 [217344/225000 (97%)] Loss: 7435.238281\n",
      "Train Epoch: 154 [221440/225000 (98%)] Loss: 7253.339844\n",
      "    epoch          : 154\n",
      "    loss           : 7352.2818643789105\n",
      "    val_loss       : 7363.986942082035\n",
      "Train Epoch: 155 [256/225000 (0%)] Loss: 7520.980469\n",
      "Train Epoch: 155 [4352/225000 (2%)] Loss: 7342.849609\n",
      "Train Epoch: 155 [8448/225000 (4%)] Loss: 7216.607422\n",
      "Train Epoch: 155 [12544/225000 (6%)] Loss: 7326.400391\n",
      "Train Epoch: 155 [16640/225000 (7%)] Loss: 7296.886719\n",
      "Train Epoch: 155 [20736/225000 (9%)] Loss: 7342.699219\n",
      "Train Epoch: 155 [24832/225000 (11%)] Loss: 7419.591797\n",
      "Train Epoch: 155 [28928/225000 (13%)] Loss: 7415.531250\n",
      "Train Epoch: 155 [33024/225000 (15%)] Loss: 7285.585938\n",
      "Train Epoch: 155 [37120/225000 (16%)] Loss: 7188.699219\n",
      "Train Epoch: 155 [41216/225000 (18%)] Loss: 7315.693359\n",
      "Train Epoch: 155 [45312/225000 (20%)] Loss: 7397.867188\n",
      "Train Epoch: 155 [49408/225000 (22%)] Loss: 7424.343750\n",
      "Train Epoch: 155 [53504/225000 (24%)] Loss: 7320.177734\n",
      "Train Epoch: 155 [57600/225000 (26%)] Loss: 7329.974609\n",
      "Train Epoch: 155 [61696/225000 (27%)] Loss: 7385.615234\n",
      "Train Epoch: 155 [65792/225000 (29%)] Loss: 7537.226562\n",
      "Train Epoch: 155 [69888/225000 (31%)] Loss: 7388.998047\n",
      "Train Epoch: 155 [73984/225000 (33%)] Loss: 7489.828125\n",
      "Train Epoch: 155 [78080/225000 (35%)] Loss: 7098.210938\n",
      "Train Epoch: 155 [82176/225000 (37%)] Loss: 7340.542969\n",
      "Train Epoch: 155 [86272/225000 (38%)] Loss: 7496.128906\n",
      "Train Epoch: 155 [90368/225000 (40%)] Loss: 7410.648438\n",
      "Train Epoch: 155 [94464/225000 (42%)] Loss: 7370.599609\n",
      "Train Epoch: 155 [98560/225000 (44%)] Loss: 7336.734375\n",
      "Train Epoch: 155 [102656/225000 (46%)] Loss: 7212.250000\n",
      "Train Epoch: 155 [106752/225000 (47%)] Loss: 7409.013672\n",
      "Train Epoch: 155 [110848/225000 (49%)] Loss: 7367.810547\n",
      "Train Epoch: 155 [114944/225000 (51%)] Loss: 7221.623047\n",
      "Train Epoch: 155 [119040/225000 (53%)] Loss: 7471.980469\n",
      "Train Epoch: 155 [123136/225000 (55%)] Loss: 7391.246094\n",
      "Train Epoch: 155 [127232/225000 (57%)] Loss: 7309.435547\n",
      "Train Epoch: 155 [131328/225000 (58%)] Loss: 7225.478516\n",
      "Train Epoch: 155 [135424/225000 (60%)] Loss: 7391.953125\n",
      "Train Epoch: 155 [139520/225000 (62%)] Loss: 7206.652344\n",
      "Train Epoch: 155 [143616/225000 (64%)] Loss: 7370.222656\n",
      "Train Epoch: 155 [147712/225000 (66%)] Loss: 7365.990234\n",
      "Train Epoch: 155 [151808/225000 (67%)] Loss: 7328.103516\n",
      "Train Epoch: 155 [155904/225000 (69%)] Loss: 6992.792969\n",
      "Train Epoch: 155 [160000/225000 (71%)] Loss: 7414.722656\n",
      "Train Epoch: 155 [164096/225000 (73%)] Loss: 7323.677734\n",
      "Train Epoch: 155 [168192/225000 (75%)] Loss: 7432.425781\n",
      "Train Epoch: 155 [172288/225000 (77%)] Loss: 7405.833984\n",
      "Train Epoch: 155 [176384/225000 (78%)] Loss: 7282.935547\n",
      "Train Epoch: 155 [180480/225000 (80%)] Loss: 7171.332031\n",
      "Train Epoch: 155 [184576/225000 (82%)] Loss: 7472.105469\n",
      "Train Epoch: 155 [188672/225000 (84%)] Loss: 7474.041016\n",
      "Train Epoch: 155 [192768/225000 (86%)] Loss: 7219.371094\n",
      "Train Epoch: 155 [196864/225000 (87%)] Loss: 7214.761719\n",
      "Train Epoch: 155 [200960/225000 (89%)] Loss: 7255.312500\n",
      "Train Epoch: 155 [205056/225000 (91%)] Loss: 7214.726562\n",
      "Train Epoch: 155 [209152/225000 (93%)] Loss: 7303.066406\n",
      "Train Epoch: 155 [213248/225000 (95%)] Loss: 7262.976562\n",
      "Train Epoch: 155 [217344/225000 (97%)] Loss: 7453.794922\n",
      "Train Epoch: 155 [221440/225000 (98%)] Loss: 7531.896484\n",
      "    epoch          : 155\n",
      "    loss           : 7336.861530325654\n",
      "    val_loss       : 7389.478435293752\n",
      "Train Epoch: 156 [256/225000 (0%)] Loss: 7341.304688\n",
      "Train Epoch: 156 [4352/225000 (2%)] Loss: 7437.312500\n",
      "Train Epoch: 156 [8448/225000 (4%)] Loss: 7300.763672\n",
      "Train Epoch: 156 [12544/225000 (6%)] Loss: 7327.875000\n",
      "Train Epoch: 156 [16640/225000 (7%)] Loss: 7561.123047\n",
      "Train Epoch: 156 [20736/225000 (9%)] Loss: 7474.621094\n",
      "Train Epoch: 156 [24832/225000 (11%)] Loss: 7251.773438\n",
      "Train Epoch: 156 [28928/225000 (13%)] Loss: 7302.546875\n",
      "Train Epoch: 156 [33024/225000 (15%)] Loss: 7368.962891\n",
      "Train Epoch: 156 [37120/225000 (16%)] Loss: 7415.958984\n",
      "Train Epoch: 156 [41216/225000 (18%)] Loss: 7535.248047\n",
      "Train Epoch: 156 [45312/225000 (20%)] Loss: 7447.435547\n",
      "Train Epoch: 156 [49408/225000 (22%)] Loss: 7385.136719\n",
      "Train Epoch: 156 [53504/225000 (24%)] Loss: 7326.992188\n",
      "Train Epoch: 156 [57600/225000 (26%)] Loss: 7302.361328\n",
      "Train Epoch: 156 [61696/225000 (27%)] Loss: 7260.925781\n",
      "Train Epoch: 156 [65792/225000 (29%)] Loss: 7332.751953\n",
      "Train Epoch: 156 [69888/225000 (31%)] Loss: 7403.160156\n",
      "Train Epoch: 156 [73984/225000 (33%)] Loss: 7272.361328\n",
      "Train Epoch: 156 [78080/225000 (35%)] Loss: 7264.013672\n",
      "Train Epoch: 156 [82176/225000 (37%)] Loss: 7351.164062\n",
      "Train Epoch: 156 [86272/225000 (38%)] Loss: 7317.080078\n",
      "Train Epoch: 156 [90368/225000 (40%)] Loss: 7300.556641\n",
      "Train Epoch: 156 [94464/225000 (42%)] Loss: 7257.667969\n",
      "Train Epoch: 156 [98560/225000 (44%)] Loss: 7369.156250\n",
      "Train Epoch: 156 [102656/225000 (46%)] Loss: 7484.394531\n",
      "Train Epoch: 156 [106752/225000 (47%)] Loss: 7460.177734\n",
      "Train Epoch: 156 [110848/225000 (49%)] Loss: 7354.154297\n",
      "Train Epoch: 156 [114944/225000 (51%)] Loss: 7298.638672\n",
      "Train Epoch: 156 [119040/225000 (53%)] Loss: 7407.166016\n",
      "Train Epoch: 156 [123136/225000 (55%)] Loss: 7405.097656\n",
      "Train Epoch: 156 [127232/225000 (57%)] Loss: 7261.900391\n",
      "Train Epoch: 156 [131328/225000 (58%)] Loss: 7390.943359\n",
      "Train Epoch: 156 [135424/225000 (60%)] Loss: 7338.269531\n",
      "Train Epoch: 156 [139520/225000 (62%)] Loss: 7368.255859\n",
      "Train Epoch: 156 [143616/225000 (64%)] Loss: 7447.650391\n",
      "Train Epoch: 156 [147712/225000 (66%)] Loss: 7227.216797\n",
      "Train Epoch: 156 [151808/225000 (67%)] Loss: 7390.804688\n",
      "Train Epoch: 156 [155904/225000 (69%)] Loss: 7531.355469\n",
      "Train Epoch: 156 [160000/225000 (71%)] Loss: 7378.841797\n",
      "Train Epoch: 156 [164096/225000 (73%)] Loss: 7254.083984\n",
      "Train Epoch: 156 [168192/225000 (75%)] Loss: 7315.878906\n",
      "Train Epoch: 156 [172288/225000 (77%)] Loss: 7490.085938\n",
      "Train Epoch: 156 [176384/225000 (78%)] Loss: 7391.310547\n",
      "Train Epoch: 156 [180480/225000 (80%)] Loss: 7235.703125\n",
      "Train Epoch: 156 [184576/225000 (82%)] Loss: 7232.181641\n",
      "Train Epoch: 156 [188672/225000 (84%)] Loss: 7211.117188\n",
      "Train Epoch: 156 [192768/225000 (86%)] Loss: 7489.085938\n",
      "Train Epoch: 156 [196864/225000 (87%)] Loss: 7176.937500\n",
      "Train Epoch: 156 [200960/225000 (89%)] Loss: 7244.570312\n",
      "Train Epoch: 156 [205056/225000 (91%)] Loss: 7219.234375\n",
      "Train Epoch: 156 [209152/225000 (93%)] Loss: 7378.812500\n",
      "Train Epoch: 156 [213248/225000 (95%)] Loss: 7331.396484\n",
      "Train Epoch: 156 [217344/225000 (97%)] Loss: 7351.779297\n",
      "Train Epoch: 156 [221440/225000 (98%)] Loss: 7272.904297\n",
      "    epoch          : 156\n",
      "    loss           : 7356.308103802261\n",
      "    val_loss       : 7328.148057521606\n",
      "Train Epoch: 157 [256/225000 (0%)] Loss: 7208.259766\n",
      "Train Epoch: 157 [4352/225000 (2%)] Loss: 7141.316406\n",
      "Train Epoch: 157 [8448/225000 (4%)] Loss: 7380.402344\n",
      "Train Epoch: 157 [12544/225000 (6%)] Loss: 7422.662109\n",
      "Train Epoch: 157 [16640/225000 (7%)] Loss: 7327.167969\n",
      "Train Epoch: 157 [20736/225000 (9%)] Loss: 7266.966797\n",
      "Train Epoch: 157 [24832/225000 (11%)] Loss: 7329.044922\n",
      "Train Epoch: 157 [28928/225000 (13%)] Loss: 7324.080078\n",
      "Train Epoch: 157 [33024/225000 (15%)] Loss: 7477.414062\n",
      "Train Epoch: 157 [37120/225000 (16%)] Loss: 7504.632812\n",
      "Train Epoch: 157 [41216/225000 (18%)] Loss: 7152.466797\n",
      "Train Epoch: 157 [45312/225000 (20%)] Loss: 7378.525391\n",
      "Train Epoch: 157 [49408/225000 (22%)] Loss: 7324.542969\n",
      "Train Epoch: 157 [53504/225000 (24%)] Loss: 7396.595703\n",
      "Train Epoch: 157 [57600/225000 (26%)] Loss: 7231.916016\n",
      "Train Epoch: 157 [61696/225000 (27%)] Loss: 7335.662109\n",
      "Train Epoch: 157 [65792/225000 (29%)] Loss: 7199.804688\n",
      "Train Epoch: 157 [69888/225000 (31%)] Loss: 7457.453125\n",
      "Train Epoch: 157 [73984/225000 (33%)] Loss: 7529.289062\n",
      "Train Epoch: 157 [78080/225000 (35%)] Loss: 7232.136719\n",
      "Train Epoch: 157 [82176/225000 (37%)] Loss: 7412.767578\n",
      "Train Epoch: 157 [86272/225000 (38%)] Loss: 7447.894531\n",
      "Train Epoch: 157 [90368/225000 (40%)] Loss: 7368.484375\n",
      "Train Epoch: 157 [94464/225000 (42%)] Loss: 7232.445312\n",
      "Train Epoch: 157 [98560/225000 (44%)] Loss: 7241.392578\n",
      "Train Epoch: 157 [102656/225000 (46%)] Loss: 7340.886719\n",
      "Train Epoch: 157 [106752/225000 (47%)] Loss: 7230.025391\n",
      "Train Epoch: 157 [110848/225000 (49%)] Loss: 7463.500000\n",
      "Train Epoch: 157 [114944/225000 (51%)] Loss: 7396.357422\n",
      "Train Epoch: 157 [119040/225000 (53%)] Loss: 7218.578125\n",
      "Train Epoch: 157 [123136/225000 (55%)] Loss: 7337.458984\n",
      "Train Epoch: 157 [127232/225000 (57%)] Loss: 7226.349609\n",
      "Train Epoch: 157 [131328/225000 (58%)] Loss: 7329.550781\n",
      "Train Epoch: 157 [135424/225000 (60%)] Loss: 7494.841797\n",
      "Train Epoch: 157 [139520/225000 (62%)] Loss: 7258.089844\n",
      "Train Epoch: 157 [143616/225000 (64%)] Loss: 7387.458984\n",
      "Train Epoch: 157 [147712/225000 (66%)] Loss: 7231.785156\n",
      "Train Epoch: 157 [151808/225000 (67%)] Loss: 7476.234375\n",
      "Train Epoch: 157 [155904/225000 (69%)] Loss: 7297.919922\n",
      "Train Epoch: 157 [160000/225000 (71%)] Loss: 7284.673828\n",
      "Train Epoch: 157 [164096/225000 (73%)] Loss: 7218.802734\n",
      "Train Epoch: 157 [168192/225000 (75%)] Loss: 7331.207031\n",
      "Train Epoch: 157 [172288/225000 (77%)] Loss: 7400.664062\n",
      "Train Epoch: 157 [176384/225000 (78%)] Loss: 7346.591797\n",
      "Train Epoch: 157 [180480/225000 (80%)] Loss: 7266.320312\n",
      "Train Epoch: 157 [184576/225000 (82%)] Loss: 7210.783203\n",
      "Train Epoch: 157 [188672/225000 (84%)] Loss: 7345.923828\n",
      "Train Epoch: 157 [192768/225000 (86%)] Loss: 7354.464844\n",
      "Train Epoch: 157 [196864/225000 (87%)] Loss: 7328.378906\n",
      "Train Epoch: 157 [200960/225000 (89%)] Loss: 7253.345703\n",
      "Train Epoch: 157 [205056/225000 (91%)] Loss: 7288.796875\n",
      "Train Epoch: 157 [209152/225000 (93%)] Loss: 7226.716797\n",
      "Train Epoch: 157 [213248/225000 (95%)] Loss: 7240.376953\n",
      "Train Epoch: 157 [217344/225000 (97%)] Loss: 7244.666016\n",
      "Train Epoch: 157 [221440/225000 (98%)] Loss: 7352.691406\n",
      "    epoch          : 157\n",
      "    loss           : 7350.350984783845\n",
      "    val_loss       : 7332.341479111691\n",
      "Train Epoch: 158 [256/225000 (0%)] Loss: 7472.753906\n",
      "Train Epoch: 158 [4352/225000 (2%)] Loss: 7230.285156\n",
      "Train Epoch: 158 [8448/225000 (4%)] Loss: 7402.433594\n",
      "Train Epoch: 158 [12544/225000 (6%)] Loss: 7269.589844\n",
      "Train Epoch: 158 [16640/225000 (7%)] Loss: 7337.404297\n",
      "Train Epoch: 158 [20736/225000 (9%)] Loss: 7217.187500\n",
      "Train Epoch: 158 [24832/225000 (11%)] Loss: 7502.042969\n",
      "Train Epoch: 158 [28928/225000 (13%)] Loss: 7397.416016\n",
      "Train Epoch: 158 [33024/225000 (15%)] Loss: 7391.453125\n",
      "Train Epoch: 158 [37120/225000 (16%)] Loss: 7320.525391\n",
      "Train Epoch: 158 [41216/225000 (18%)] Loss: 7277.945312\n",
      "Train Epoch: 158 [45312/225000 (20%)] Loss: 7508.333984\n",
      "Train Epoch: 158 [49408/225000 (22%)] Loss: 7363.669922\n",
      "Train Epoch: 158 [53504/225000 (24%)] Loss: 7266.208984\n",
      "Train Epoch: 158 [57600/225000 (26%)] Loss: 7401.748047\n",
      "Train Epoch: 158 [61696/225000 (27%)] Loss: 7270.367188\n",
      "Train Epoch: 158 [65792/225000 (29%)] Loss: 7308.990234\n",
      "Train Epoch: 158 [69888/225000 (31%)] Loss: 7291.445312\n",
      "Train Epoch: 158 [73984/225000 (33%)] Loss: 7589.556641\n",
      "Train Epoch: 158 [78080/225000 (35%)] Loss: 7481.437500\n",
      "Train Epoch: 158 [82176/225000 (37%)] Loss: 7298.611328\n",
      "Train Epoch: 158 [86272/225000 (38%)] Loss: 7358.177734\n",
      "Train Epoch: 158 [90368/225000 (40%)] Loss: 7248.181641\n",
      "Train Epoch: 158 [94464/225000 (42%)] Loss: 7454.158203\n",
      "Train Epoch: 158 [98560/225000 (44%)] Loss: 7178.943359\n",
      "Train Epoch: 158 [102656/225000 (46%)] Loss: 7379.927734\n",
      "Train Epoch: 158 [106752/225000 (47%)] Loss: 7214.138672\n",
      "Train Epoch: 158 [110848/225000 (49%)] Loss: 7151.580078\n",
      "Train Epoch: 158 [114944/225000 (51%)] Loss: 7378.699219\n",
      "Train Epoch: 158 [119040/225000 (53%)] Loss: 7200.041016\n",
      "Train Epoch: 158 [123136/225000 (55%)] Loss: 7390.492188\n",
      "Train Epoch: 158 [127232/225000 (57%)] Loss: 7297.978516\n",
      "Train Epoch: 158 [131328/225000 (58%)] Loss: 7191.164062\n",
      "Train Epoch: 158 [135424/225000 (60%)] Loss: 7346.173828\n",
      "Train Epoch: 158 [139520/225000 (62%)] Loss: 7396.335938\n",
      "Train Epoch: 158 [143616/225000 (64%)] Loss: 7441.953125\n",
      "Train Epoch: 158 [147712/225000 (66%)] Loss: 7048.013672\n",
      "Train Epoch: 158 [151808/225000 (67%)] Loss: 7190.275391\n",
      "Train Epoch: 158 [155904/225000 (69%)] Loss: 7334.779297\n",
      "Train Epoch: 158 [160000/225000 (71%)] Loss: 7206.945312\n",
      "Train Epoch: 158 [164096/225000 (73%)] Loss: 7247.417969\n",
      "Train Epoch: 158 [168192/225000 (75%)] Loss: 7264.431641\n",
      "Train Epoch: 158 [172288/225000 (77%)] Loss: 7423.324219\n",
      "Train Epoch: 158 [176384/225000 (78%)] Loss: 7260.351562\n",
      "Train Epoch: 158 [180480/225000 (80%)] Loss: 7302.414062\n",
      "Train Epoch: 158 [184576/225000 (82%)] Loss: 7341.425781\n",
      "Train Epoch: 158 [188672/225000 (84%)] Loss: 7232.072266\n",
      "Train Epoch: 158 [192768/225000 (86%)] Loss: 7342.250000\n",
      "Train Epoch: 158 [196864/225000 (87%)] Loss: 7213.193359\n",
      "Train Epoch: 158 [200960/225000 (89%)] Loss: 7189.013672\n",
      "Train Epoch: 158 [205056/225000 (91%)] Loss: 7315.425781\n",
      "Train Epoch: 158 [209152/225000 (93%)] Loss: 7214.132812\n",
      "Train Epoch: 158 [213248/225000 (95%)] Loss: 7278.410156\n",
      "Train Epoch: 158 [217344/225000 (97%)] Loss: 7316.988281\n",
      "Train Epoch: 158 [221440/225000 (98%)] Loss: 7338.388672\n",
      "    epoch          : 158\n",
      "    loss           : 7324.993465141496\n",
      "    val_loss       : 7318.4824253679535\n",
      "Train Epoch: 159 [256/225000 (0%)] Loss: 7461.833984\n",
      "Train Epoch: 159 [4352/225000 (2%)] Loss: 7353.914062\n",
      "Train Epoch: 159 [8448/225000 (4%)] Loss: 7286.751953\n",
      "Train Epoch: 159 [12544/225000 (6%)] Loss: 7439.998047\n",
      "Train Epoch: 159 [16640/225000 (7%)] Loss: 7300.738281\n",
      "Train Epoch: 159 [20736/225000 (9%)] Loss: 7271.599609\n",
      "Train Epoch: 159 [24832/225000 (11%)] Loss: 7254.199219\n",
      "Train Epoch: 159 [28928/225000 (13%)] Loss: 7207.656250\n",
      "Train Epoch: 159 [33024/225000 (15%)] Loss: 7373.429688\n",
      "Train Epoch: 159 [37120/225000 (16%)] Loss: 7310.427734\n",
      "Train Epoch: 159 [41216/225000 (18%)] Loss: 7444.195312\n",
      "Train Epoch: 159 [45312/225000 (20%)] Loss: 7277.015625\n",
      "Train Epoch: 159 [49408/225000 (22%)] Loss: 7435.919922\n",
      "Train Epoch: 159 [53504/225000 (24%)] Loss: 7469.517578\n",
      "Train Epoch: 159 [57600/225000 (26%)] Loss: 7370.189453\n",
      "Train Epoch: 159 [61696/225000 (27%)] Loss: 7271.025391\n",
      "Train Epoch: 159 [65792/225000 (29%)] Loss: 7287.871094\n",
      "Train Epoch: 159 [69888/225000 (31%)] Loss: 7334.056641\n",
      "Train Epoch: 159 [73984/225000 (33%)] Loss: 7327.439453\n",
      "Train Epoch: 159 [78080/225000 (35%)] Loss: 7436.183594\n",
      "Train Epoch: 159 [82176/225000 (37%)] Loss: 7272.009766\n",
      "Train Epoch: 159 [86272/225000 (38%)] Loss: 7417.351562\n",
      "Train Epoch: 159 [90368/225000 (40%)] Loss: 7360.640625\n",
      "Train Epoch: 159 [94464/225000 (42%)] Loss: 7315.908203\n",
      "Train Epoch: 159 [98560/225000 (44%)] Loss: 7186.193359\n",
      "Train Epoch: 159 [102656/225000 (46%)] Loss: 7414.628906\n",
      "Train Epoch: 159 [106752/225000 (47%)] Loss: 7197.583984\n",
      "Train Epoch: 159 [110848/225000 (49%)] Loss: 7331.960938\n",
      "Train Epoch: 159 [114944/225000 (51%)] Loss: 7297.574219\n",
      "Train Epoch: 159 [119040/225000 (53%)] Loss: 7279.423828\n",
      "Train Epoch: 159 [123136/225000 (55%)] Loss: 7272.400391\n",
      "Train Epoch: 159 [127232/225000 (57%)] Loss: 7357.951172\n",
      "Train Epoch: 159 [131328/225000 (58%)] Loss: 7286.029297\n",
      "Train Epoch: 159 [135424/225000 (60%)] Loss: 7260.265625\n",
      "Train Epoch: 159 [139520/225000 (62%)] Loss: 7237.216797\n",
      "Train Epoch: 159 [143616/225000 (64%)] Loss: 7615.064453\n",
      "Train Epoch: 159 [147712/225000 (66%)] Loss: 7361.546875\n",
      "Train Epoch: 159 [151808/225000 (67%)] Loss: 7631.138672\n",
      "Train Epoch: 159 [155904/225000 (69%)] Loss: 7255.927734\n",
      "Train Epoch: 159 [160000/225000 (71%)] Loss: 7215.546875\n",
      "Train Epoch: 159 [164096/225000 (73%)] Loss: 7299.017578\n",
      "Train Epoch: 159 [168192/225000 (75%)] Loss: 7220.281250\n",
      "Train Epoch: 159 [172288/225000 (77%)] Loss: 7312.699219\n",
      "Train Epoch: 159 [176384/225000 (78%)] Loss: 7334.728516\n",
      "Train Epoch: 159 [180480/225000 (80%)] Loss: 7462.183594\n",
      "Train Epoch: 159 [184576/225000 (82%)] Loss: 7337.486328\n",
      "Train Epoch: 159 [188672/225000 (84%)] Loss: 7179.699219\n",
      "Train Epoch: 159 [192768/225000 (86%)] Loss: 7163.886719\n",
      "Train Epoch: 159 [196864/225000 (87%)] Loss: 7248.185547\n",
      "Train Epoch: 159 [200960/225000 (89%)] Loss: 7256.599609\n",
      "Train Epoch: 159 [205056/225000 (91%)] Loss: 7242.443359\n",
      "Train Epoch: 159 [209152/225000 (93%)] Loss: 7304.833984\n",
      "Train Epoch: 159 [213248/225000 (95%)] Loss: 7417.791016\n",
      "Train Epoch: 159 [217344/225000 (97%)] Loss: 7244.894531\n",
      "Train Epoch: 159 [221440/225000 (98%)] Loss: 7282.404297\n",
      "    epoch          : 159\n",
      "    loss           : 7331.6046199516495\n",
      "    val_loss       : 7311.959893204728\n",
      "Train Epoch: 160 [256/225000 (0%)] Loss: 7319.570312\n",
      "Train Epoch: 160 [4352/225000 (2%)] Loss: 7426.378906\n",
      "Train Epoch: 160 [8448/225000 (4%)] Loss: 7503.195312\n",
      "Train Epoch: 160 [12544/225000 (6%)] Loss: 7215.408203\n",
      "Train Epoch: 160 [16640/225000 (7%)] Loss: 7388.429688\n",
      "Train Epoch: 160 [20736/225000 (9%)] Loss: 7381.423828\n",
      "Train Epoch: 160 [24832/225000 (11%)] Loss: 7475.878906\n",
      "Train Epoch: 160 [28928/225000 (13%)] Loss: 7227.228516\n",
      "Train Epoch: 160 [33024/225000 (15%)] Loss: 7329.869141\n",
      "Train Epoch: 160 [37120/225000 (16%)] Loss: 7196.271484\n",
      "Train Epoch: 160 [41216/225000 (18%)] Loss: 7233.904297\n",
      "Train Epoch: 160 [45312/225000 (20%)] Loss: 7245.107422\n",
      "Train Epoch: 160 [49408/225000 (22%)] Loss: 7381.921875\n",
      "Train Epoch: 160 [53504/225000 (24%)] Loss: 7298.802734\n",
      "Train Epoch: 160 [57600/225000 (26%)] Loss: 7162.142578\n",
      "Train Epoch: 160 [61696/225000 (27%)] Loss: 7290.841797\n",
      "Train Epoch: 160 [65792/225000 (29%)] Loss: 7503.542969\n",
      "Train Epoch: 160 [69888/225000 (31%)] Loss: 7253.423828\n",
      "Train Epoch: 160 [73984/225000 (33%)] Loss: 7404.585938\n",
      "Train Epoch: 160 [78080/225000 (35%)] Loss: 7401.818359\n",
      "Train Epoch: 160 [82176/225000 (37%)] Loss: 7337.169922\n",
      "Train Epoch: 160 [86272/225000 (38%)] Loss: 7196.636719\n",
      "Train Epoch: 160 [90368/225000 (40%)] Loss: 7310.582031\n",
      "Train Epoch: 160 [94464/225000 (42%)] Loss: 7400.216797\n",
      "Train Epoch: 160 [98560/225000 (44%)] Loss: 7248.285156\n",
      "Train Epoch: 160 [102656/225000 (46%)] Loss: 7284.748047\n",
      "Train Epoch: 160 [106752/225000 (47%)] Loss: 7244.027344\n",
      "Train Epoch: 160 [110848/225000 (49%)] Loss: 7384.548828\n",
      "Train Epoch: 160 [114944/225000 (51%)] Loss: 7490.542969\n",
      "Train Epoch: 160 [119040/225000 (53%)] Loss: 7251.582031\n",
      "Train Epoch: 160 [123136/225000 (55%)] Loss: 7138.763672\n",
      "Train Epoch: 160 [127232/225000 (57%)] Loss: 7340.517578\n",
      "Train Epoch: 160 [131328/225000 (58%)] Loss: 7483.783203\n",
      "Train Epoch: 160 [135424/225000 (60%)] Loss: 7256.492188\n",
      "Train Epoch: 160 [139520/225000 (62%)] Loss: 7494.253906\n",
      "Train Epoch: 160 [143616/225000 (64%)] Loss: 7319.380859\n",
      "Train Epoch: 160 [147712/225000 (66%)] Loss: 7301.429688\n",
      "Train Epoch: 160 [151808/225000 (67%)] Loss: 7222.882812\n",
      "Train Epoch: 160 [155904/225000 (69%)] Loss: 7463.001953\n",
      "Train Epoch: 160 [160000/225000 (71%)] Loss: 7413.083984\n",
      "Train Epoch: 160 [164096/225000 (73%)] Loss: 7395.312500\n",
      "Train Epoch: 160 [168192/225000 (75%)] Loss: 7433.759766\n",
      "Train Epoch: 160 [172288/225000 (77%)] Loss: 7327.775391\n",
      "Train Epoch: 160 [176384/225000 (78%)] Loss: 7357.009766\n",
      "Train Epoch: 160 [180480/225000 (80%)] Loss: 7391.138672\n",
      "Train Epoch: 160 [184576/225000 (82%)] Loss: 7366.962891\n",
      "Train Epoch: 160 [188672/225000 (84%)] Loss: 7283.560547\n",
      "Train Epoch: 160 [192768/225000 (86%)] Loss: 7354.728516\n",
      "Train Epoch: 160 [196864/225000 (87%)] Loss: 7264.986328\n",
      "Train Epoch: 160 [200960/225000 (89%)] Loss: 7285.886719\n",
      "Train Epoch: 160 [205056/225000 (91%)] Loss: 7362.423828\n",
      "Train Epoch: 160 [209152/225000 (93%)] Loss: 7209.261719\n",
      "Train Epoch: 160 [213248/225000 (95%)] Loss: 7442.189453\n",
      "Train Epoch: 160 [217344/225000 (97%)] Loss: 7382.144531\n",
      "Train Epoch: 160 [221440/225000 (98%)] Loss: 7140.000000\n",
      "    epoch          : 160\n",
      "    loss           : 7362.07421875\n",
      "    val_loss       : 7310.092762557828\n",
      "Train Epoch: 161 [256/225000 (0%)] Loss: 7344.474609\n",
      "Train Epoch: 161 [4352/225000 (2%)] Loss: 7206.785156\n",
      "Train Epoch: 161 [8448/225000 (4%)] Loss: 7318.316406\n",
      "Train Epoch: 161 [12544/225000 (6%)] Loss: 7325.089844\n",
      "Train Epoch: 161 [16640/225000 (7%)] Loss: 7312.214844\n",
      "Train Epoch: 161 [20736/225000 (9%)] Loss: 7260.082031\n",
      "Train Epoch: 161 [24832/225000 (11%)] Loss: 7209.775391\n",
      "Train Epoch: 161 [28928/225000 (13%)] Loss: 7299.304688\n",
      "Train Epoch: 161 [33024/225000 (15%)] Loss: 7246.992188\n",
      "Train Epoch: 161 [37120/225000 (16%)] Loss: 7403.490234\n",
      "Train Epoch: 161 [41216/225000 (18%)] Loss: 7173.962891\n",
      "Train Epoch: 161 [45312/225000 (20%)] Loss: 7398.160156\n",
      "Train Epoch: 161 [49408/225000 (22%)] Loss: 7230.996094\n",
      "Train Epoch: 161 [53504/225000 (24%)] Loss: 7294.408203\n",
      "Train Epoch: 161 [57600/225000 (26%)] Loss: 7407.091797\n",
      "Train Epoch: 161 [61696/225000 (27%)] Loss: 7347.478516\n",
      "Train Epoch: 161 [65792/225000 (29%)] Loss: 7329.626953\n",
      "Train Epoch: 161 [69888/225000 (31%)] Loss: 7190.937500\n",
      "Train Epoch: 161 [73984/225000 (33%)] Loss: 7345.484375\n",
      "Train Epoch: 161 [78080/225000 (35%)] Loss: 7412.332031\n",
      "Train Epoch: 161 [82176/225000 (37%)] Loss: 7431.029297\n",
      "Train Epoch: 161 [86272/225000 (38%)] Loss: 7313.365234\n",
      "Train Epoch: 161 [90368/225000 (40%)] Loss: 7383.888672\n",
      "Train Epoch: 161 [94464/225000 (42%)] Loss: 7177.457031\n",
      "Train Epoch: 161 [98560/225000 (44%)] Loss: 7251.093750\n",
      "Train Epoch: 161 [102656/225000 (46%)] Loss: 7331.236328\n",
      "Train Epoch: 161 [106752/225000 (47%)] Loss: 7457.900391\n",
      "Train Epoch: 161 [110848/225000 (49%)] Loss: 7326.662109\n",
      "Train Epoch: 161 [114944/225000 (51%)] Loss: 7249.470703\n",
      "Train Epoch: 161 [119040/225000 (53%)] Loss: 7401.562500\n",
      "Train Epoch: 161 [123136/225000 (55%)] Loss: 7262.904297\n",
      "Train Epoch: 161 [127232/225000 (57%)] Loss: 7280.812500\n",
      "Train Epoch: 161 [131328/225000 (58%)] Loss: 7357.878906\n",
      "Train Epoch: 161 [135424/225000 (60%)] Loss: 7405.492188\n",
      "Train Epoch: 161 [139520/225000 (62%)] Loss: 7385.044922\n",
      "Train Epoch: 161 [143616/225000 (64%)] Loss: 7273.013672\n",
      "Train Epoch: 161 [147712/225000 (66%)] Loss: 7445.736328\n",
      "Train Epoch: 161 [151808/225000 (67%)] Loss: 7250.007812\n",
      "Train Epoch: 161 [155904/225000 (69%)] Loss: 7251.875000\n",
      "Train Epoch: 161 [160000/225000 (71%)] Loss: 7260.066406\n",
      "Train Epoch: 161 [164096/225000 (73%)] Loss: 7471.576172\n",
      "Train Epoch: 161 [168192/225000 (75%)] Loss: 7210.089844\n",
      "Train Epoch: 161 [172288/225000 (77%)] Loss: 7449.939453\n",
      "Train Epoch: 161 [176384/225000 (78%)] Loss: 7439.742188\n",
      "Train Epoch: 161 [180480/225000 (80%)] Loss: 7340.527344\n",
      "Train Epoch: 161 [184576/225000 (82%)] Loss: 7331.712891\n",
      "Train Epoch: 161 [188672/225000 (84%)] Loss: 7255.142578\n",
      "Train Epoch: 161 [192768/225000 (86%)] Loss: 7337.095703\n",
      "Train Epoch: 161 [196864/225000 (87%)] Loss: 7516.091797\n",
      "Train Epoch: 161 [200960/225000 (89%)] Loss: 7422.011719\n",
      "Train Epoch: 161 [205056/225000 (91%)] Loss: 7258.804688\n",
      "Train Epoch: 161 [209152/225000 (93%)] Loss: 7347.882812\n",
      "Train Epoch: 161 [213248/225000 (95%)] Loss: 7645.244141\n",
      "Train Epoch: 161 [217344/225000 (97%)] Loss: 7490.912109\n",
      "Train Epoch: 161 [221440/225000 (98%)] Loss: 7246.957031\n",
      "    epoch          : 161\n",
      "    loss           : 7312.4121915884525\n",
      "    val_loss       : 7309.66336464517\n",
      "Train Epoch: 162 [256/225000 (0%)] Loss: 7377.695312\n",
      "Train Epoch: 162 [4352/225000 (2%)] Loss: 7420.626953\n",
      "Train Epoch: 162 [8448/225000 (4%)] Loss: 7449.304688\n",
      "Train Epoch: 162 [12544/225000 (6%)] Loss: 7451.181641\n",
      "Train Epoch: 162 [16640/225000 (7%)] Loss: 7389.544922\n",
      "Train Epoch: 162 [20736/225000 (9%)] Loss: 7268.214844\n",
      "Train Epoch: 162 [24832/225000 (11%)] Loss: 7301.298828\n",
      "Train Epoch: 162 [28928/225000 (13%)] Loss: 7372.652344\n",
      "Train Epoch: 162 [33024/225000 (15%)] Loss: 7272.001953\n",
      "Train Epoch: 162 [37120/225000 (16%)] Loss: 7368.023438\n",
      "Train Epoch: 162 [41216/225000 (18%)] Loss: 7295.363281\n",
      "Train Epoch: 162 [45312/225000 (20%)] Loss: 7462.894531\n",
      "Train Epoch: 162 [49408/225000 (22%)] Loss: 7479.621094\n",
      "Train Epoch: 162 [53504/225000 (24%)] Loss: 7312.648438\n",
      "Train Epoch: 162 [57600/225000 (26%)] Loss: 7290.140625\n",
      "Train Epoch: 162 [61696/225000 (27%)] Loss: 7276.718750\n",
      "Train Epoch: 162 [65792/225000 (29%)] Loss: 7398.478516\n",
      "Train Epoch: 162 [69888/225000 (31%)] Loss: 7430.980469\n",
      "Train Epoch: 162 [73984/225000 (33%)] Loss: 7371.171875\n",
      "Train Epoch: 162 [78080/225000 (35%)] Loss: 7411.255859\n",
      "Train Epoch: 162 [82176/225000 (37%)] Loss: 7327.162109\n",
      "Train Epoch: 162 [86272/225000 (38%)] Loss: 7220.238281\n",
      "Train Epoch: 162 [90368/225000 (40%)] Loss: 7259.363281\n",
      "Train Epoch: 162 [94464/225000 (42%)] Loss: 7456.441406\n",
      "Train Epoch: 162 [98560/225000 (44%)] Loss: 7437.785156\n",
      "Train Epoch: 162 [102656/225000 (46%)] Loss: 7276.341797\n",
      "Train Epoch: 162 [106752/225000 (47%)] Loss: 7200.070312\n",
      "Train Epoch: 162 [110848/225000 (49%)] Loss: 7313.453125\n",
      "Train Epoch: 162 [114944/225000 (51%)] Loss: 7452.867188\n",
      "Train Epoch: 162 [119040/225000 (53%)] Loss: 7454.017578\n",
      "Train Epoch: 162 [123136/225000 (55%)] Loss: 7277.123047\n",
      "Train Epoch: 162 [127232/225000 (57%)] Loss: 7195.445312\n",
      "Train Epoch: 162 [131328/225000 (58%)] Loss: 7206.058594\n",
      "Train Epoch: 162 [135424/225000 (60%)] Loss: 7482.074219\n",
      "Train Epoch: 162 [139520/225000 (62%)] Loss: 7314.822266\n",
      "Train Epoch: 162 [143616/225000 (64%)] Loss: 7359.759766\n",
      "Train Epoch: 162 [147712/225000 (66%)] Loss: 7434.085938\n",
      "Train Epoch: 162 [151808/225000 (67%)] Loss: 7368.460938\n",
      "Train Epoch: 162 [155904/225000 (69%)] Loss: 7403.955078\n",
      "Train Epoch: 162 [160000/225000 (71%)] Loss: 7399.037109\n",
      "Train Epoch: 162 [164096/225000 (73%)] Loss: 7235.357422\n",
      "Train Epoch: 162 [168192/225000 (75%)] Loss: 7316.951172\n",
      "Train Epoch: 162 [172288/225000 (77%)] Loss: 7378.716797\n",
      "Train Epoch: 162 [176384/225000 (78%)] Loss: 7256.566406\n",
      "Train Epoch: 162 [180480/225000 (80%)] Loss: 7315.425781\n",
      "Train Epoch: 162 [184576/225000 (82%)] Loss: 7116.765625\n",
      "Train Epoch: 162 [188672/225000 (84%)] Loss: 7401.378906\n",
      "Train Epoch: 162 [192768/225000 (86%)] Loss: 7230.414062\n",
      "Train Epoch: 162 [196864/225000 (87%)] Loss: 7190.226562\n",
      "Train Epoch: 162 [200960/225000 (89%)] Loss: 7363.689453\n",
      "Train Epoch: 162 [205056/225000 (91%)] Loss: 7198.121094\n",
      "Train Epoch: 162 [209152/225000 (93%)] Loss: 7350.195312\n",
      "Train Epoch: 162 [213248/225000 (95%)] Loss: 7238.697266\n",
      "Train Epoch: 162 [217344/225000 (97%)] Loss: 7377.216797\n",
      "Train Epoch: 162 [221440/225000 (98%)] Loss: 7440.585938\n",
      "    epoch          : 162\n",
      "    loss           : 7308.482695179181\n",
      "    val_loss       : 7310.291829291655\n",
      "Train Epoch: 163 [256/225000 (0%)] Loss: 7143.144531\n",
      "Train Epoch: 163 [4352/225000 (2%)] Loss: 7265.697266\n",
      "Train Epoch: 163 [8448/225000 (4%)] Loss: 7411.826172\n",
      "Train Epoch: 163 [12544/225000 (6%)] Loss: 7452.111328\n",
      "Train Epoch: 163 [16640/225000 (7%)] Loss: 7446.855469\n",
      "Train Epoch: 163 [20736/225000 (9%)] Loss: 7246.728516\n",
      "Train Epoch: 163 [24832/225000 (11%)] Loss: 7462.431641\n",
      "Train Epoch: 163 [28928/225000 (13%)] Loss: 7433.804688\n",
      "Train Epoch: 163 [33024/225000 (15%)] Loss: 7259.208984\n",
      "Train Epoch: 163 [37120/225000 (16%)] Loss: 7091.875000\n",
      "Train Epoch: 163 [41216/225000 (18%)] Loss: 7403.281250\n",
      "Train Epoch: 163 [45312/225000 (20%)] Loss: 7319.498047\n",
      "Train Epoch: 163 [49408/225000 (22%)] Loss: 7205.720703\n",
      "Train Epoch: 163 [53504/225000 (24%)] Loss: 7259.187500\n",
      "Train Epoch: 163 [57600/225000 (26%)] Loss: 7328.515625\n",
      "Train Epoch: 163 [61696/225000 (27%)] Loss: 7083.070312\n",
      "Train Epoch: 163 [65792/225000 (29%)] Loss: 7134.623047\n",
      "Train Epoch: 163 [69888/225000 (31%)] Loss: 7173.552734\n",
      "Train Epoch: 163 [73984/225000 (33%)] Loss: 7244.968750\n",
      "Train Epoch: 163 [78080/225000 (35%)] Loss: 7326.978516\n",
      "Train Epoch: 163 [82176/225000 (37%)] Loss: 7177.587891\n",
      "Train Epoch: 163 [86272/225000 (38%)] Loss: 7125.089844\n",
      "Train Epoch: 163 [90368/225000 (40%)] Loss: 7361.394531\n",
      "Train Epoch: 163 [94464/225000 (42%)] Loss: 7207.107422\n",
      "Train Epoch: 163 [98560/225000 (44%)] Loss: 7307.480469\n",
      "Train Epoch: 163 [102656/225000 (46%)] Loss: 7174.566406\n",
      "Train Epoch: 163 [106752/225000 (47%)] Loss: 7340.681641\n",
      "Train Epoch: 163 [110848/225000 (49%)] Loss: 7321.076172\n",
      "Train Epoch: 163 [114944/225000 (51%)] Loss: 7305.085938\n",
      "Train Epoch: 163 [119040/225000 (53%)] Loss: 7320.976562\n",
      "Train Epoch: 163 [123136/225000 (55%)] Loss: 7297.849609\n",
      "Train Epoch: 163 [127232/225000 (57%)] Loss: 7227.921875\n",
      "Train Epoch: 163 [131328/225000 (58%)] Loss: 7312.214844\n",
      "Train Epoch: 163 [135424/225000 (60%)] Loss: 7353.265625\n",
      "Train Epoch: 163 [139520/225000 (62%)] Loss: 7291.285156\n",
      "Train Epoch: 163 [143616/225000 (64%)] Loss: 7303.212891\n",
      "Train Epoch: 163 [147712/225000 (66%)] Loss: 7293.794922\n",
      "Train Epoch: 163 [151808/225000 (67%)] Loss: 7203.015625\n",
      "Train Epoch: 163 [155904/225000 (69%)] Loss: 7259.771484\n",
      "Train Epoch: 163 [160000/225000 (71%)] Loss: 7232.482422\n",
      "Train Epoch: 163 [164096/225000 (73%)] Loss: 7261.958984\n",
      "Train Epoch: 163 [168192/225000 (75%)] Loss: 7541.646484\n",
      "Train Epoch: 163 [172288/225000 (77%)] Loss: 7253.843750\n",
      "Train Epoch: 163 [176384/225000 (78%)] Loss: 7368.521484\n",
      "Train Epoch: 163 [180480/225000 (80%)] Loss: 7438.113281\n",
      "Train Epoch: 163 [184576/225000 (82%)] Loss: 7376.927734\n",
      "Train Epoch: 163 [188672/225000 (84%)] Loss: 7272.263672\n",
      "Train Epoch: 163 [192768/225000 (86%)] Loss: 7304.699219\n",
      "Train Epoch: 163 [196864/225000 (87%)] Loss: 7186.648438\n",
      "Train Epoch: 163 [200960/225000 (89%)] Loss: 7322.312500\n",
      "Train Epoch: 163 [205056/225000 (91%)] Loss: 7442.019531\n",
      "Train Epoch: 163 [209152/225000 (93%)] Loss: 7186.767578\n",
      "Train Epoch: 163 [213248/225000 (95%)] Loss: 7322.177734\n",
      "Train Epoch: 163 [217344/225000 (97%)] Loss: 7290.310547\n",
      "Train Epoch: 163 [221440/225000 (98%)] Loss: 7323.007812\n",
      "    epoch          : 163\n",
      "    loss           : 7305.195213621658\n",
      "    val_loss       : 7304.2327695087515\n",
      "Train Epoch: 164 [256/225000 (0%)] Loss: 7305.671875\n",
      "Train Epoch: 164 [4352/225000 (2%)] Loss: 7373.603516\n",
      "Train Epoch: 164 [8448/225000 (4%)] Loss: 7237.394531\n",
      "Train Epoch: 164 [12544/225000 (6%)] Loss: 7250.404297\n",
      "Train Epoch: 164 [16640/225000 (7%)] Loss: 7247.265625\n",
      "Train Epoch: 164 [20736/225000 (9%)] Loss: 7276.615234\n",
      "Train Epoch: 164 [24832/225000 (11%)] Loss: 7350.142578\n",
      "Train Epoch: 164 [28928/225000 (13%)] Loss: 7324.541016\n",
      "Train Epoch: 164 [33024/225000 (15%)] Loss: 7361.779297\n",
      "Train Epoch: 164 [37120/225000 (16%)] Loss: 7346.000000\n",
      "Train Epoch: 164 [41216/225000 (18%)] Loss: 7311.457031\n",
      "Train Epoch: 164 [45312/225000 (20%)] Loss: 7170.992188\n",
      "Train Epoch: 164 [49408/225000 (22%)] Loss: 7219.437500\n",
      "Train Epoch: 164 [53504/225000 (24%)] Loss: 7325.349609\n",
      "Train Epoch: 164 [57600/225000 (26%)] Loss: 7286.183594\n",
      "Train Epoch: 164 [61696/225000 (27%)] Loss: 7390.093750\n",
      "Train Epoch: 164 [65792/225000 (29%)] Loss: 7346.351562\n",
      "Train Epoch: 164 [69888/225000 (31%)] Loss: 7455.509766\n",
      "Train Epoch: 164 [73984/225000 (33%)] Loss: 7292.921875\n",
      "Train Epoch: 164 [78080/225000 (35%)] Loss: 7161.222656\n",
      "Train Epoch: 164 [82176/225000 (37%)] Loss: 7355.482422\n",
      "Train Epoch: 164 [86272/225000 (38%)] Loss: 7366.267578\n",
      "Train Epoch: 164 [90368/225000 (40%)] Loss: 7313.378906\n",
      "Train Epoch: 164 [94464/225000 (42%)] Loss: 7390.468750\n",
      "Train Epoch: 164 [98560/225000 (44%)] Loss: 7174.449219\n",
      "Train Epoch: 164 [102656/225000 (46%)] Loss: 7227.300781\n",
      "Train Epoch: 164 [106752/225000 (47%)] Loss: 7534.398438\n",
      "Train Epoch: 164 [110848/225000 (49%)] Loss: 7318.304688\n",
      "Train Epoch: 164 [114944/225000 (51%)] Loss: 7294.169922\n",
      "Train Epoch: 164 [119040/225000 (53%)] Loss: 7365.554688\n",
      "Train Epoch: 164 [123136/225000 (55%)] Loss: 7185.230469\n",
      "Train Epoch: 164 [127232/225000 (57%)] Loss: 7295.910156\n",
      "Train Epoch: 164 [131328/225000 (58%)] Loss: 7360.232422\n",
      "Train Epoch: 164 [135424/225000 (60%)] Loss: 7406.986328\n",
      "Train Epoch: 164 [139520/225000 (62%)] Loss: 7273.035156\n",
      "Train Epoch: 164 [143616/225000 (64%)] Loss: 7328.722656\n",
      "Train Epoch: 164 [147712/225000 (66%)] Loss: 7254.785156\n",
      "Train Epoch: 164 [151808/225000 (67%)] Loss: 7221.941406\n",
      "Train Epoch: 164 [155904/225000 (69%)] Loss: 7560.517578\n",
      "Train Epoch: 164 [160000/225000 (71%)] Loss: 7304.490234\n",
      "Train Epoch: 164 [164096/225000 (73%)] Loss: 7315.771484\n",
      "Train Epoch: 164 [168192/225000 (75%)] Loss: 7252.972656\n",
      "Train Epoch: 164 [172288/225000 (77%)] Loss: 7452.466797\n",
      "Train Epoch: 164 [176384/225000 (78%)] Loss: 7248.771484\n",
      "Train Epoch: 164 [180480/225000 (80%)] Loss: 7210.546875\n",
      "Train Epoch: 164 [184576/225000 (82%)] Loss: 7412.515625\n",
      "Train Epoch: 164 [188672/225000 (84%)] Loss: 7275.720703\n",
      "Train Epoch: 164 [192768/225000 (86%)] Loss: 7313.796875\n",
      "Train Epoch: 164 [196864/225000 (87%)] Loss: 7174.492188\n",
      "Train Epoch: 164 [200960/225000 (89%)] Loss: 7298.914062\n",
      "Train Epoch: 164 [205056/225000 (91%)] Loss: 7334.037109\n",
      "Train Epoch: 164 [209152/225000 (93%)] Loss: 7354.513672\n",
      "Train Epoch: 164 [213248/225000 (95%)] Loss: 7340.228516\n",
      "Train Epoch: 164 [217344/225000 (97%)] Loss: 7408.830078\n",
      "Train Epoch: 164 [221440/225000 (98%)] Loss: 7122.931641\n",
      "    epoch          : 164\n",
      "    loss           : 7301.178038786974\n",
      "    val_loss       : 7301.0999519228935\n",
      "Train Epoch: 165 [256/225000 (0%)] Loss: 7475.238281\n",
      "Train Epoch: 165 [4352/225000 (2%)] Loss: 7318.402344\n",
      "Train Epoch: 165 [8448/225000 (4%)] Loss: 7467.904297\n",
      "Train Epoch: 165 [12544/225000 (6%)] Loss: 7246.457031\n",
      "Train Epoch: 165 [16640/225000 (7%)] Loss: 7409.876953\n",
      "Train Epoch: 165 [20736/225000 (9%)] Loss: 7298.205078\n",
      "Train Epoch: 165 [24832/225000 (11%)] Loss: 7355.947266\n",
      "Train Epoch: 165 [28928/225000 (13%)] Loss: 7460.138672\n",
      "Train Epoch: 165 [33024/225000 (15%)] Loss: 7221.943359\n",
      "Train Epoch: 165 [37120/225000 (16%)] Loss: 7292.931641\n",
      "Train Epoch: 165 [41216/225000 (18%)] Loss: 7454.326172\n",
      "Train Epoch: 165 [45312/225000 (20%)] Loss: 7347.023438\n",
      "Train Epoch: 165 [49408/225000 (22%)] Loss: 7266.212891\n",
      "Train Epoch: 165 [53504/225000 (24%)] Loss: 7317.273438\n",
      "Train Epoch: 165 [57600/225000 (26%)] Loss: 7351.650391\n",
      "Train Epoch: 165 [61696/225000 (27%)] Loss: 7240.529297\n",
      "Train Epoch: 165 [65792/225000 (29%)] Loss: 7239.869141\n",
      "Train Epoch: 165 [69888/225000 (31%)] Loss: 7218.781250\n",
      "Train Epoch: 165 [73984/225000 (33%)] Loss: 7248.945312\n",
      "Train Epoch: 165 [78080/225000 (35%)] Loss: 7308.611328\n",
      "Train Epoch: 165 [82176/225000 (37%)] Loss: 7566.794922\n",
      "Train Epoch: 165 [86272/225000 (38%)] Loss: 7356.193359\n",
      "Train Epoch: 165 [90368/225000 (40%)] Loss: 7250.935547\n",
      "Train Epoch: 165 [94464/225000 (42%)] Loss: 7420.279297\n",
      "Train Epoch: 165 [98560/225000 (44%)] Loss: 7249.962891\n",
      "Train Epoch: 165 [102656/225000 (46%)] Loss: 7314.119141\n",
      "Train Epoch: 165 [106752/225000 (47%)] Loss: 7440.962891\n",
      "Train Epoch: 165 [110848/225000 (49%)] Loss: 7302.369141\n",
      "Train Epoch: 165 [114944/225000 (51%)] Loss: 7158.390625\n",
      "Train Epoch: 165 [119040/225000 (53%)] Loss: 7318.638672\n",
      "Train Epoch: 165 [123136/225000 (55%)] Loss: 7405.771484\n",
      "Train Epoch: 165 [127232/225000 (57%)] Loss: 7199.578125\n",
      "Train Epoch: 165 [131328/225000 (58%)] Loss: 7400.337891\n",
      "Train Epoch: 165 [135424/225000 (60%)] Loss: 7251.419922\n",
      "Train Epoch: 165 [139520/225000 (62%)] Loss: 7207.558594\n",
      "Train Epoch: 165 [143616/225000 (64%)] Loss: 7228.017578\n",
      "Train Epoch: 165 [147712/225000 (66%)] Loss: 7371.380859\n",
      "Train Epoch: 165 [151808/225000 (67%)] Loss: 7223.625000\n",
      "Train Epoch: 165 [155904/225000 (69%)] Loss: 7344.468750\n",
      "Train Epoch: 165 [160000/225000 (71%)] Loss: 7337.472656\n",
      "Train Epoch: 165 [164096/225000 (73%)] Loss: 7171.015625\n",
      "Train Epoch: 165 [168192/225000 (75%)] Loss: 7304.970703\n",
      "Train Epoch: 165 [172288/225000 (77%)] Loss: 7347.603516\n",
      "Train Epoch: 165 [176384/225000 (78%)] Loss: 7178.292969\n",
      "Train Epoch: 165 [180480/225000 (80%)] Loss: 7244.937500\n",
      "Train Epoch: 165 [184576/225000 (82%)] Loss: 7359.925781\n",
      "Train Epoch: 165 [188672/225000 (84%)] Loss: 7243.066406\n",
      "Train Epoch: 165 [192768/225000 (86%)] Loss: 7168.111328\n",
      "Train Epoch: 165 [196864/225000 (87%)] Loss: 7181.562500\n",
      "Train Epoch: 165 [200960/225000 (89%)] Loss: 7311.623047\n",
      "Train Epoch: 165 [205056/225000 (91%)] Loss: 7343.296875\n",
      "Train Epoch: 165 [209152/225000 (93%)] Loss: 7264.818359\n",
      "Train Epoch: 165 [213248/225000 (95%)] Loss: 7224.505859\n",
      "Train Epoch: 165 [217344/225000 (97%)] Loss: 7469.011719\n",
      "Train Epoch: 165 [221440/225000 (98%)] Loss: 7343.865234\n",
      "    epoch          : 165\n",
      "    loss           : 7296.621761456556\n",
      "    val_loss       : 7302.277634610935\n",
      "Train Epoch: 166 [256/225000 (0%)] Loss: 7329.890625\n",
      "Train Epoch: 166 [4352/225000 (2%)] Loss: 7155.027344\n",
      "Train Epoch: 166 [8448/225000 (4%)] Loss: 7316.570312\n",
      "Train Epoch: 166 [12544/225000 (6%)] Loss: 7278.636719\n",
      "Train Epoch: 166 [16640/225000 (7%)] Loss: 7308.417969\n",
      "Train Epoch: 166 [20736/225000 (9%)] Loss: 7223.228516\n",
      "Train Epoch: 166 [24832/225000 (11%)] Loss: 7200.941406\n",
      "Train Epoch: 166 [28928/225000 (13%)] Loss: 7214.771484\n",
      "Train Epoch: 166 [33024/225000 (15%)] Loss: 7185.517578\n",
      "Train Epoch: 166 [37120/225000 (16%)] Loss: 7303.933594\n",
      "Train Epoch: 166 [41216/225000 (18%)] Loss: 7210.451172\n",
      "Train Epoch: 166 [45312/225000 (20%)] Loss: 7299.908203\n",
      "Train Epoch: 166 [49408/225000 (22%)] Loss: 7188.171875\n",
      "Train Epoch: 166 [53504/225000 (24%)] Loss: 7171.193359\n",
      "Train Epoch: 166 [57600/225000 (26%)] Loss: 7301.326172\n",
      "Train Epoch: 166 [61696/225000 (27%)] Loss: 7222.623047\n",
      "Train Epoch: 166 [65792/225000 (29%)] Loss: 7429.966797\n",
      "Train Epoch: 166 [69888/225000 (31%)] Loss: 7271.845703\n",
      "Train Epoch: 166 [73984/225000 (33%)] Loss: 7370.533203\n",
      "Train Epoch: 166 [78080/225000 (35%)] Loss: 7387.376953\n",
      "Train Epoch: 166 [82176/225000 (37%)] Loss: 7452.810547\n",
      "Train Epoch: 166 [86272/225000 (38%)] Loss: 7325.136719\n",
      "Train Epoch: 166 [90368/225000 (40%)] Loss: 7330.609375\n",
      "Train Epoch: 166 [94464/225000 (42%)] Loss: 7506.031250\n",
      "Train Epoch: 166 [98560/225000 (44%)] Loss: 7201.488281\n",
      "Train Epoch: 166 [102656/225000 (46%)] Loss: 7378.179688\n",
      "Train Epoch: 166 [106752/225000 (47%)] Loss: 7201.816406\n",
      "Train Epoch: 166 [110848/225000 (49%)] Loss: 7276.207031\n",
      "Train Epoch: 166 [114944/225000 (51%)] Loss: 7163.316406\n",
      "Train Epoch: 166 [119040/225000 (53%)] Loss: 7341.832031\n",
      "Train Epoch: 166 [123136/225000 (55%)] Loss: 7327.044922\n",
      "Train Epoch: 166 [127232/225000 (57%)] Loss: 7246.736328\n",
      "Train Epoch: 166 [131328/225000 (58%)] Loss: 7275.773438\n",
      "Train Epoch: 166 [135424/225000 (60%)] Loss: 7292.429688\n",
      "Train Epoch: 166 [139520/225000 (62%)] Loss: 7379.173828\n",
      "Train Epoch: 166 [143616/225000 (64%)] Loss: 7377.109375\n",
      "Train Epoch: 166 [147712/225000 (66%)] Loss: 7287.316406\n",
      "Train Epoch: 166 [151808/225000 (67%)] Loss: 7229.736328\n",
      "Train Epoch: 166 [155904/225000 (69%)] Loss: 7260.476562\n",
      "Train Epoch: 166 [160000/225000 (71%)] Loss: 7289.554688\n",
      "Train Epoch: 166 [164096/225000 (73%)] Loss: 7190.650391\n",
      "Train Epoch: 166 [168192/225000 (75%)] Loss: 7396.431641\n",
      "Train Epoch: 166 [172288/225000 (77%)] Loss: 7413.800781\n",
      "Train Epoch: 166 [176384/225000 (78%)] Loss: 7336.996094\n",
      "Train Epoch: 166 [180480/225000 (80%)] Loss: 7146.248047\n",
      "Train Epoch: 166 [184576/225000 (82%)] Loss: 7246.763672\n",
      "Train Epoch: 166 [188672/225000 (84%)] Loss: 7121.800781\n",
      "Train Epoch: 166 [192768/225000 (86%)] Loss: 7419.539062\n",
      "Train Epoch: 166 [196864/225000 (87%)] Loss: 7269.429688\n",
      "Train Epoch: 166 [200960/225000 (89%)] Loss: 7097.386719\n",
      "Train Epoch: 166 [205056/225000 (91%)] Loss: 7083.869141\n",
      "Train Epoch: 166 [209152/225000 (93%)] Loss: 7296.033203\n",
      "Train Epoch: 166 [213248/225000 (95%)] Loss: 7300.605469\n",
      "Train Epoch: 166 [217344/225000 (97%)] Loss: 7138.703125\n",
      "Train Epoch: 166 [221440/225000 (98%)] Loss: 7266.388672\n",
      "    epoch          : 166\n",
      "    loss           : 7303.561030156783\n",
      "    val_loss       : 7285.30030898172\n",
      "Train Epoch: 167 [256/225000 (0%)] Loss: 7409.677734\n",
      "Train Epoch: 167 [4352/225000 (2%)] Loss: 7324.160156\n",
      "Train Epoch: 167 [8448/225000 (4%)] Loss: 7266.654297\n",
      "Train Epoch: 167 [12544/225000 (6%)] Loss: 7300.029297\n",
      "Train Epoch: 167 [16640/225000 (7%)] Loss: 7270.175781\n",
      "Train Epoch: 167 [20736/225000 (9%)] Loss: 7269.373047\n",
      "Train Epoch: 167 [24832/225000 (11%)] Loss: 7193.257812\n",
      "Train Epoch: 167 [28928/225000 (13%)] Loss: 7264.634766\n",
      "Train Epoch: 167 [33024/225000 (15%)] Loss: 7544.398438\n",
      "Train Epoch: 167 [37120/225000 (16%)] Loss: 7333.173828\n",
      "Train Epoch: 167 [41216/225000 (18%)] Loss: 7531.339844\n",
      "Train Epoch: 167 [45312/225000 (20%)] Loss: 7292.171875\n",
      "Train Epoch: 167 [49408/225000 (22%)] Loss: 7346.375000\n",
      "Train Epoch: 167 [53504/225000 (24%)] Loss: 7256.218750\n",
      "Train Epoch: 167 [57600/225000 (26%)] Loss: 7239.421875\n",
      "Train Epoch: 167 [61696/225000 (27%)] Loss: 7163.457031\n",
      "Train Epoch: 167 [65792/225000 (29%)] Loss: 7221.191406\n",
      "Train Epoch: 167 [69888/225000 (31%)] Loss: 7291.191406\n",
      "Train Epoch: 167 [73984/225000 (33%)] Loss: 7122.416016\n",
      "Train Epoch: 167 [78080/225000 (35%)] Loss: 7345.865234\n",
      "Train Epoch: 167 [82176/225000 (37%)] Loss: 7350.777344\n",
      "Train Epoch: 167 [86272/225000 (38%)] Loss: 7174.943359\n",
      "Train Epoch: 167 [90368/225000 (40%)] Loss: 7109.800781\n",
      "Train Epoch: 167 [94464/225000 (42%)] Loss: 7372.462891\n",
      "Train Epoch: 167 [98560/225000 (44%)] Loss: 7173.880859\n",
      "Train Epoch: 167 [102656/225000 (46%)] Loss: 7328.804688\n",
      "Train Epoch: 167 [106752/225000 (47%)] Loss: 7307.773438\n",
      "Train Epoch: 167 [110848/225000 (49%)] Loss: 7348.013672\n",
      "Train Epoch: 167 [114944/225000 (51%)] Loss: 7264.845703\n",
      "Train Epoch: 167 [119040/225000 (53%)] Loss: 7240.550781\n",
      "Train Epoch: 167 [123136/225000 (55%)] Loss: 7384.580078\n",
      "Train Epoch: 167 [127232/225000 (57%)] Loss: 7383.490234\n",
      "Train Epoch: 167 [131328/225000 (58%)] Loss: 7246.484375\n",
      "Train Epoch: 167 [135424/225000 (60%)] Loss: 7211.550781\n",
      "Train Epoch: 167 [139520/225000 (62%)] Loss: 7166.154297\n",
      "Train Epoch: 167 [143616/225000 (64%)] Loss: 7366.998047\n",
      "Train Epoch: 167 [147712/225000 (66%)] Loss: 7414.257812\n",
      "Train Epoch: 167 [151808/225000 (67%)] Loss: 7369.173828\n",
      "Train Epoch: 167 [155904/225000 (69%)] Loss: 7287.828125\n",
      "Train Epoch: 167 [160000/225000 (71%)] Loss: 7162.964844\n",
      "Train Epoch: 167 [164096/225000 (73%)] Loss: 7146.376953\n",
      "Train Epoch: 167 [168192/225000 (75%)] Loss: 7149.318359\n",
      "Train Epoch: 167 [172288/225000 (77%)] Loss: 7230.662109\n",
      "Train Epoch: 167 [176384/225000 (78%)] Loss: 7475.523438\n",
      "Train Epoch: 167 [180480/225000 (80%)] Loss: 7382.550781\n",
      "Train Epoch: 167 [184576/225000 (82%)] Loss: 7224.572266\n",
      "Train Epoch: 167 [188672/225000 (84%)] Loss: 7272.513672\n",
      "Train Epoch: 167 [192768/225000 (86%)] Loss: 7202.718750\n",
      "Train Epoch: 167 [196864/225000 (87%)] Loss: 7270.808594\n",
      "Train Epoch: 167 [200960/225000 (89%)] Loss: 7368.625000\n",
      "Train Epoch: 167 [205056/225000 (91%)] Loss: 7355.390625\n",
      "Train Epoch: 167 [209152/225000 (93%)] Loss: 7309.853516\n",
      "Train Epoch: 167 [213248/225000 (95%)] Loss: 7246.595703\n",
      "Train Epoch: 167 [217344/225000 (97%)] Loss: 7265.216797\n",
      "Train Epoch: 167 [221440/225000 (98%)] Loss: 7249.904297\n",
      "    epoch          : 167\n",
      "    loss           : 7312.288188148819\n",
      "    val_loss       : 7305.939102913652\n",
      "Train Epoch: 168 [256/225000 (0%)] Loss: 7248.746094\n",
      "Train Epoch: 168 [4352/225000 (2%)] Loss: 7276.546875\n",
      "Train Epoch: 168 [8448/225000 (4%)] Loss: 7326.634766\n",
      "Train Epoch: 168 [12544/225000 (6%)] Loss: 7285.691406\n",
      "Train Epoch: 168 [16640/225000 (7%)] Loss: 7286.925781\n",
      "Train Epoch: 168 [20736/225000 (9%)] Loss: 7245.310547\n",
      "Train Epoch: 168 [24832/225000 (11%)] Loss: 7358.816406\n",
      "Train Epoch: 168 [28928/225000 (13%)] Loss: 7252.031250\n",
      "Train Epoch: 168 [33024/225000 (15%)] Loss: 7436.996094\n",
      "Train Epoch: 168 [37120/225000 (16%)] Loss: 7340.033203\n",
      "Train Epoch: 168 [41216/225000 (18%)] Loss: 7381.021484\n",
      "Train Epoch: 168 [45312/225000 (20%)] Loss: 7297.947266\n",
      "Train Epoch: 168 [49408/225000 (22%)] Loss: 7366.230469\n",
      "Train Epoch: 168 [53504/225000 (24%)] Loss: 7153.205078\n",
      "Train Epoch: 168 [57600/225000 (26%)] Loss: 7302.740234\n",
      "Train Epoch: 168 [61696/225000 (27%)] Loss: 7324.652344\n",
      "Train Epoch: 168 [65792/225000 (29%)] Loss: 7345.343750\n",
      "Train Epoch: 168 [69888/225000 (31%)] Loss: 7249.281250\n",
      "Train Epoch: 168 [73984/225000 (33%)] Loss: 7174.585938\n",
      "Train Epoch: 168 [78080/225000 (35%)] Loss: 7188.853516\n",
      "Train Epoch: 168 [82176/225000 (37%)] Loss: 7200.283203\n",
      "Train Epoch: 168 [86272/225000 (38%)] Loss: 7387.656250\n",
      "Train Epoch: 168 [90368/225000 (40%)] Loss: 7206.701172\n",
      "Train Epoch: 168 [94464/225000 (42%)] Loss: 7240.781250\n",
      "Train Epoch: 168 [98560/225000 (44%)] Loss: 7117.714844\n",
      "Train Epoch: 168 [102656/225000 (46%)] Loss: 7378.998047\n",
      "Train Epoch: 168 [106752/225000 (47%)] Loss: 7364.664062\n",
      "Train Epoch: 168 [110848/225000 (49%)] Loss: 7336.890625\n",
      "Train Epoch: 168 [114944/225000 (51%)] Loss: 7533.914062\n",
      "Train Epoch: 168 [119040/225000 (53%)] Loss: 7228.910156\n",
      "Train Epoch: 168 [123136/225000 (55%)] Loss: 7094.820312\n",
      "Train Epoch: 168 [127232/225000 (57%)] Loss: 7168.761719\n",
      "Train Epoch: 168 [131328/225000 (58%)] Loss: 7317.957031\n",
      "Train Epoch: 168 [135424/225000 (60%)] Loss: 7328.085938\n",
      "Train Epoch: 168 [139520/225000 (62%)] Loss: 7354.921875\n",
      "Train Epoch: 168 [143616/225000 (64%)] Loss: 7181.875000\n",
      "Train Epoch: 168 [147712/225000 (66%)] Loss: 7135.916016\n",
      "Train Epoch: 168 [151808/225000 (67%)] Loss: 7240.910156\n",
      "Train Epoch: 168 [155904/225000 (69%)] Loss: 7334.050781\n",
      "Train Epoch: 168 [160000/225000 (71%)] Loss: 7435.894531\n",
      "Train Epoch: 168 [164096/225000 (73%)] Loss: 7189.929688\n",
      "Train Epoch: 168 [168192/225000 (75%)] Loss: 7278.111328\n",
      "Train Epoch: 168 [172288/225000 (77%)] Loss: 7435.611328\n",
      "Train Epoch: 168 [176384/225000 (78%)] Loss: 7335.082031\n",
      "Train Epoch: 168 [180480/225000 (80%)] Loss: 7321.294922\n",
      "Train Epoch: 168 [184576/225000 (82%)] Loss: 7495.080078\n",
      "Train Epoch: 168 [188672/225000 (84%)] Loss: 7230.312500\n",
      "Train Epoch: 168 [192768/225000 (86%)] Loss: 7220.152344\n",
      "Train Epoch: 168 [196864/225000 (87%)] Loss: 7422.570312\n",
      "Train Epoch: 168 [200960/225000 (89%)] Loss: 7330.667969\n",
      "Train Epoch: 168 [205056/225000 (91%)] Loss: 7455.087891\n",
      "Train Epoch: 168 [209152/225000 (93%)] Loss: 7250.947266\n",
      "Train Epoch: 168 [213248/225000 (95%)] Loss: 7249.246094\n",
      "Train Epoch: 168 [217344/225000 (97%)] Loss: 7279.988281\n",
      "Train Epoch: 168 [221440/225000 (98%)] Loss: 7257.488281\n",
      "    epoch          : 168\n",
      "    loss           : 7320.393510247795\n",
      "    val_loss       : 7279.319432837622\n",
      "Train Epoch: 169 [256/225000 (0%)] Loss: 7419.560547\n",
      "Train Epoch: 169 [4352/225000 (2%)] Loss: 7200.414062\n",
      "Train Epoch: 169 [8448/225000 (4%)] Loss: 7260.892578\n",
      "Train Epoch: 169 [12544/225000 (6%)] Loss: 7312.923828\n",
      "Train Epoch: 169 [16640/225000 (7%)] Loss: 7204.908203\n",
      "Train Epoch: 169 [20736/225000 (9%)] Loss: 7219.378906\n",
      "Train Epoch: 169 [24832/225000 (11%)] Loss: 7233.316406\n",
      "Train Epoch: 169 [28928/225000 (13%)] Loss: 7360.914062\n",
      "Train Epoch: 169 [33024/225000 (15%)] Loss: 7346.212891\n",
      "Train Epoch: 169 [37120/225000 (16%)] Loss: 7278.089844\n",
      "Train Epoch: 169 [41216/225000 (18%)] Loss: 7379.669922\n",
      "Train Epoch: 169 [45312/225000 (20%)] Loss: 7364.597656\n",
      "Train Epoch: 169 [49408/225000 (22%)] Loss: 7359.199219\n",
      "Train Epoch: 169 [53504/225000 (24%)] Loss: 7287.972656\n",
      "Train Epoch: 169 [57600/225000 (26%)] Loss: 7349.261719\n",
      "Train Epoch: 169 [61696/225000 (27%)] Loss: 7267.363281\n",
      "Train Epoch: 169 [65792/225000 (29%)] Loss: 7226.324219\n",
      "Train Epoch: 169 [69888/225000 (31%)] Loss: 7318.054688\n",
      "Train Epoch: 169 [73984/225000 (33%)] Loss: 7386.365234\n",
      "Train Epoch: 169 [78080/225000 (35%)] Loss: 7174.277344\n",
      "Train Epoch: 169 [82176/225000 (37%)] Loss: 7324.708984\n",
      "Train Epoch: 169 [86272/225000 (38%)] Loss: 7400.875000\n",
      "Train Epoch: 169 [90368/225000 (40%)] Loss: 7208.628906\n",
      "Train Epoch: 169 [94464/225000 (42%)] Loss: 7413.841797\n",
      "Train Epoch: 169 [98560/225000 (44%)] Loss: 7517.787109\n",
      "Train Epoch: 169 [102656/225000 (46%)] Loss: 7323.302734\n",
      "Train Epoch: 169 [106752/225000 (47%)] Loss: 7278.320312\n",
      "Train Epoch: 169 [110848/225000 (49%)] Loss: 7208.425781\n",
      "Train Epoch: 169 [114944/225000 (51%)] Loss: 7155.531250\n",
      "Train Epoch: 169 [119040/225000 (53%)] Loss: 7355.591797\n",
      "Train Epoch: 169 [123136/225000 (55%)] Loss: 7313.179688\n",
      "Train Epoch: 169 [127232/225000 (57%)] Loss: 7327.835938\n",
      "Train Epoch: 169 [131328/225000 (58%)] Loss: 7388.214844\n",
      "Train Epoch: 169 [135424/225000 (60%)] Loss: 7472.480469\n",
      "Train Epoch: 169 [139520/225000 (62%)] Loss: 7495.873047\n",
      "Train Epoch: 169 [143616/225000 (64%)] Loss: 7350.029297\n",
      "Train Epoch: 169 [147712/225000 (66%)] Loss: 7142.767578\n",
      "Train Epoch: 169 [151808/225000 (67%)] Loss: 7258.328125\n",
      "Train Epoch: 169 [155904/225000 (69%)] Loss: 7249.835938\n",
      "Train Epoch: 169 [160000/225000 (71%)] Loss: 7233.726562\n",
      "Train Epoch: 169 [164096/225000 (73%)] Loss: 7369.880859\n",
      "Train Epoch: 169 [168192/225000 (75%)] Loss: 7489.296875\n",
      "Train Epoch: 169 [172288/225000 (77%)] Loss: 7072.244141\n",
      "Train Epoch: 169 [176384/225000 (78%)] Loss: 7258.458984\n",
      "Train Epoch: 169 [180480/225000 (80%)] Loss: 7419.218750\n",
      "Train Epoch: 169 [184576/225000 (82%)] Loss: 7279.511719\n",
      "Train Epoch: 169 [188672/225000 (84%)] Loss: 7269.375000\n",
      "Train Epoch: 169 [192768/225000 (86%)] Loss: 7321.685547\n",
      "Train Epoch: 169 [196864/225000 (87%)] Loss: 7239.324219\n",
      "Train Epoch: 169 [200960/225000 (89%)] Loss: 7200.460938\n",
      "Train Epoch: 169 [205056/225000 (91%)] Loss: 7261.953125\n",
      "Train Epoch: 169 [209152/225000 (93%)] Loss: 7407.851562\n",
      "Train Epoch: 169 [213248/225000 (95%)] Loss: 7198.726562\n",
      "Train Epoch: 169 [217344/225000 (97%)] Loss: 7327.730469\n",
      "Train Epoch: 169 [221440/225000 (98%)] Loss: 7457.660156\n",
      "    epoch          : 169\n",
      "    loss           : 7292.990318810438\n",
      "    val_loss       : 7275.045990349078\n",
      "Train Epoch: 170 [256/225000 (0%)] Loss: 7338.039062\n",
      "Train Epoch: 170 [4352/225000 (2%)] Loss: 7275.720703\n",
      "Train Epoch: 170 [8448/225000 (4%)] Loss: 7198.595703\n",
      "Train Epoch: 170 [12544/225000 (6%)] Loss: 7259.373047\n",
      "Train Epoch: 170 [16640/225000 (7%)] Loss: 7372.640625\n",
      "Train Epoch: 170 [20736/225000 (9%)] Loss: 7216.519531\n",
      "Train Epoch: 170 [24832/225000 (11%)] Loss: 7158.050781\n",
      "Train Epoch: 170 [28928/225000 (13%)] Loss: 7275.708984\n",
      "Train Epoch: 170 [33024/225000 (15%)] Loss: 7104.046875\n",
      "Train Epoch: 170 [37120/225000 (16%)] Loss: 7086.548828\n",
      "Train Epoch: 170 [41216/225000 (18%)] Loss: 7354.658203\n",
      "Train Epoch: 170 [45312/225000 (20%)] Loss: 7545.828125\n",
      "Train Epoch: 170 [49408/225000 (22%)] Loss: 7251.244141\n",
      "Train Epoch: 170 [53504/225000 (24%)] Loss: 7297.562500\n",
      "Train Epoch: 170 [57600/225000 (26%)] Loss: 7268.015625\n",
      "Train Epoch: 170 [61696/225000 (27%)] Loss: 7263.943359\n",
      "Train Epoch: 170 [65792/225000 (29%)] Loss: 7203.138672\n",
      "Train Epoch: 170 [69888/225000 (31%)] Loss: 7282.046875\n",
      "Train Epoch: 170 [73984/225000 (33%)] Loss: 7157.173828\n",
      "Train Epoch: 170 [78080/225000 (35%)] Loss: 7157.554688\n",
      "Train Epoch: 170 [82176/225000 (37%)] Loss: 7305.160156\n",
      "Train Epoch: 170 [86272/225000 (38%)] Loss: 7265.134766\n",
      "Train Epoch: 170 [90368/225000 (40%)] Loss: 7353.779297\n",
      "Train Epoch: 170 [94464/225000 (42%)] Loss: 7246.765625\n",
      "Train Epoch: 170 [98560/225000 (44%)] Loss: 7372.974609\n",
      "Train Epoch: 170 [102656/225000 (46%)] Loss: 7257.267578\n",
      "Train Epoch: 170 [106752/225000 (47%)] Loss: 7276.898438\n",
      "Train Epoch: 170 [110848/225000 (49%)] Loss: 7179.308594\n",
      "Train Epoch: 170 [114944/225000 (51%)] Loss: 7165.693359\n",
      "Train Epoch: 170 [119040/225000 (53%)] Loss: 7272.621094\n",
      "Train Epoch: 170 [123136/225000 (55%)] Loss: 7252.062500\n",
      "Train Epoch: 170 [127232/225000 (57%)] Loss: 7259.654297\n",
      "Train Epoch: 170 [131328/225000 (58%)] Loss: 7055.806641\n",
      "Train Epoch: 170 [135424/225000 (60%)] Loss: 7252.564453\n",
      "Train Epoch: 170 [139520/225000 (62%)] Loss: 7206.783203\n",
      "Train Epoch: 170 [143616/225000 (64%)] Loss: 7278.160156\n",
      "Train Epoch: 170 [147712/225000 (66%)] Loss: 7499.318359\n",
      "Train Epoch: 170 [151808/225000 (67%)] Loss: 7295.095703\n",
      "Train Epoch: 170 [155904/225000 (69%)] Loss: 7318.128906\n",
      "Train Epoch: 170 [160000/225000 (71%)] Loss: 7401.017578\n",
      "Train Epoch: 170 [164096/225000 (73%)] Loss: 7290.933594\n",
      "Train Epoch: 170 [168192/225000 (75%)] Loss: 7335.400391\n",
      "Train Epoch: 170 [172288/225000 (77%)] Loss: 7196.482422\n",
      "Train Epoch: 170 [176384/225000 (78%)] Loss: 7242.070312\n",
      "Train Epoch: 170 [180480/225000 (80%)] Loss: 7427.335938\n",
      "Train Epoch: 170 [184576/225000 (82%)] Loss: 7326.556641\n",
      "Train Epoch: 170 [188672/225000 (84%)] Loss: 7257.443359\n",
      "Train Epoch: 170 [192768/225000 (86%)] Loss: 7359.072266\n",
      "Train Epoch: 170 [196864/225000 (87%)] Loss: 7299.212891\n",
      "Train Epoch: 170 [200960/225000 (89%)] Loss: 7253.302734\n",
      "Train Epoch: 170 [205056/225000 (91%)] Loss: 7308.804688\n",
      "Train Epoch: 170 [209152/225000 (93%)] Loss: 7179.925781\n",
      "Train Epoch: 170 [213248/225000 (95%)] Loss: 7207.689453\n",
      "Train Epoch: 170 [217344/225000 (97%)] Loss: 7068.738281\n",
      "Train Epoch: 170 [221440/225000 (98%)] Loss: 7448.447266\n",
      "    epoch          : 170\n",
      "    loss           : 7278.274941783988\n",
      "    val_loss       : 7272.670985011422\n",
      "Train Epoch: 171 [256/225000 (0%)] Loss: 7274.265625\n",
      "Train Epoch: 171 [4352/225000 (2%)] Loss: 7323.861328\n",
      "Train Epoch: 171 [8448/225000 (4%)] Loss: 7130.923828\n",
      "Train Epoch: 171 [12544/225000 (6%)] Loss: 7354.968750\n",
      "Train Epoch: 171 [16640/225000 (7%)] Loss: 7124.101562\n",
      "Train Epoch: 171 [20736/225000 (9%)] Loss: 7288.017578\n",
      "Train Epoch: 171 [24832/225000 (11%)] Loss: 7224.015625\n",
      "Train Epoch: 171 [28928/225000 (13%)] Loss: 7324.857422\n",
      "Train Epoch: 171 [33024/225000 (15%)] Loss: 7463.279297\n",
      "Train Epoch: 171 [37120/225000 (16%)] Loss: 7291.421875\n",
      "Train Epoch: 171 [41216/225000 (18%)] Loss: 7296.125000\n",
      "Train Epoch: 171 [45312/225000 (20%)] Loss: 7353.898438\n",
      "Train Epoch: 171 [49408/225000 (22%)] Loss: 7387.087891\n",
      "Train Epoch: 171 [53504/225000 (24%)] Loss: 7270.406250\n",
      "Train Epoch: 171 [57600/225000 (26%)] Loss: 7410.910156\n",
      "Train Epoch: 171 [61696/225000 (27%)] Loss: 7261.208984\n",
      "Train Epoch: 171 [65792/225000 (29%)] Loss: 7211.835938\n",
      "Train Epoch: 171 [69888/225000 (31%)] Loss: 7293.757812\n",
      "Train Epoch: 171 [73984/225000 (33%)] Loss: 7309.738281\n",
      "Train Epoch: 171 [78080/225000 (35%)] Loss: 7342.558594\n",
      "Train Epoch: 171 [82176/225000 (37%)] Loss: 7342.791016\n",
      "Train Epoch: 171 [86272/225000 (38%)] Loss: 7359.367188\n",
      "Train Epoch: 171 [90368/225000 (40%)] Loss: 7240.572266\n",
      "Train Epoch: 171 [94464/225000 (42%)] Loss: 7218.296875\n",
      "Train Epoch: 171 [98560/225000 (44%)] Loss: 7206.925781\n",
      "Train Epoch: 171 [102656/225000 (46%)] Loss: 7052.732422\n",
      "Train Epoch: 171 [106752/225000 (47%)] Loss: 7362.197266\n",
      "Train Epoch: 171 [110848/225000 (49%)] Loss: 7283.968750\n",
      "Train Epoch: 171 [114944/225000 (51%)] Loss: 7192.349609\n",
      "Train Epoch: 171 [119040/225000 (53%)] Loss: 7262.892578\n",
      "Train Epoch: 171 [123136/225000 (55%)] Loss: 7275.554688\n",
      "Train Epoch: 171 [127232/225000 (57%)] Loss: 7097.238281\n",
      "Train Epoch: 171 [131328/225000 (58%)] Loss: 7355.955078\n",
      "Train Epoch: 171 [135424/225000 (60%)] Loss: 7301.638672\n",
      "Train Epoch: 171 [139520/225000 (62%)] Loss: 7247.630859\n",
      "Train Epoch: 171 [143616/225000 (64%)] Loss: 7293.621094\n",
      "Train Epoch: 171 [147712/225000 (66%)] Loss: 7196.908203\n",
      "Train Epoch: 171 [151808/225000 (67%)] Loss: 7275.535156\n",
      "Train Epoch: 171 [155904/225000 (69%)] Loss: 7129.931641\n",
      "Train Epoch: 171 [160000/225000 (71%)] Loss: 7268.941406\n",
      "Train Epoch: 171 [164096/225000 (73%)] Loss: 7219.664062\n",
      "Train Epoch: 171 [168192/225000 (75%)] Loss: 7322.402344\n",
      "Train Epoch: 171 [172288/225000 (77%)] Loss: 7315.240234\n",
      "Train Epoch: 171 [176384/225000 (78%)] Loss: 7401.306641\n",
      "Train Epoch: 171 [180480/225000 (80%)] Loss: 7320.917969\n",
      "Train Epoch: 171 [184576/225000 (82%)] Loss: 7247.884766\n",
      "Train Epoch: 171 [188672/225000 (84%)] Loss: 7175.910156\n",
      "Train Epoch: 171 [192768/225000 (86%)] Loss: 7126.351562\n",
      "Train Epoch: 171 [196864/225000 (87%)] Loss: 7330.857422\n",
      "Train Epoch: 171 [200960/225000 (89%)] Loss: 7188.718750\n",
      "Train Epoch: 171 [205056/225000 (91%)] Loss: 7194.541016\n",
      "Train Epoch: 171 [209152/225000 (93%)] Loss: 7282.457031\n",
      "Train Epoch: 171 [213248/225000 (95%)] Loss: 7197.720703\n",
      "Train Epoch: 171 [217344/225000 (97%)] Loss: 7287.589844\n",
      "Train Epoch: 171 [221440/225000 (98%)] Loss: 7174.429688\n",
      "    epoch          : 171\n",
      "    loss           : 7274.6068319379265\n",
      "    val_loss       : 7371.310185410539\n",
      "Train Epoch: 172 [256/225000 (0%)] Loss: 7123.925781\n",
      "Train Epoch: 172 [4352/225000 (2%)] Loss: 7421.513672\n",
      "Train Epoch: 172 [8448/225000 (4%)] Loss: 7291.218750\n",
      "Train Epoch: 172 [12544/225000 (6%)] Loss: 7222.273438\n",
      "Train Epoch: 172 [16640/225000 (7%)] Loss: 7344.773438\n",
      "Train Epoch: 172 [20736/225000 (9%)] Loss: 7209.642578\n",
      "Train Epoch: 172 [24832/225000 (11%)] Loss: 7403.054688\n",
      "Train Epoch: 172 [28928/225000 (13%)] Loss: 7244.544922\n",
      "Train Epoch: 172 [33024/225000 (15%)] Loss: 7242.378906\n",
      "Train Epoch: 172 [37120/225000 (16%)] Loss: 7192.808594\n",
      "Train Epoch: 172 [41216/225000 (18%)] Loss: 7264.923828\n",
      "Train Epoch: 172 [45312/225000 (20%)] Loss: 7318.015625\n",
      "Train Epoch: 172 [49408/225000 (22%)] Loss: 7155.214844\n",
      "Train Epoch: 172 [53504/225000 (24%)] Loss: 7158.708984\n",
      "Train Epoch: 172 [57600/225000 (26%)] Loss: 7121.439453\n",
      "Train Epoch: 172 [61696/225000 (27%)] Loss: 7254.544922\n",
      "Train Epoch: 172 [65792/225000 (29%)] Loss: 7272.949219\n",
      "Train Epoch: 172 [69888/225000 (31%)] Loss: 7385.648438\n",
      "Train Epoch: 172 [73984/225000 (33%)] Loss: 7259.390625\n",
      "Train Epoch: 172 [78080/225000 (35%)] Loss: 7264.304688\n",
      "Train Epoch: 172 [82176/225000 (37%)] Loss: 7274.226562\n",
      "Train Epoch: 172 [86272/225000 (38%)] Loss: 7139.392578\n",
      "Train Epoch: 172 [90368/225000 (40%)] Loss: 7142.142578\n",
      "Train Epoch: 172 [94464/225000 (42%)] Loss: 7289.296875\n",
      "Train Epoch: 172 [98560/225000 (44%)] Loss: 7274.628906\n",
      "Train Epoch: 172 [102656/225000 (46%)] Loss: 7310.126953\n",
      "Train Epoch: 172 [106752/225000 (47%)] Loss: 7235.097656\n",
      "Train Epoch: 172 [110848/225000 (49%)] Loss: 7331.429688\n",
      "Train Epoch: 172 [114944/225000 (51%)] Loss: 7215.263672\n",
      "Train Epoch: 172 [119040/225000 (53%)] Loss: 7184.298828\n",
      "Train Epoch: 172 [123136/225000 (55%)] Loss: 7135.533203\n",
      "Train Epoch: 172 [127232/225000 (57%)] Loss: 7542.343750\n",
      "Train Epoch: 172 [131328/225000 (58%)] Loss: 7222.222656\n",
      "Train Epoch: 172 [135424/225000 (60%)] Loss: 7272.652344\n",
      "Train Epoch: 172 [139520/225000 (62%)] Loss: 7268.160156\n",
      "Train Epoch: 172 [143616/225000 (64%)] Loss: 7232.158203\n",
      "Train Epoch: 172 [147712/225000 (66%)] Loss: 7223.214844\n",
      "Train Epoch: 172 [151808/225000 (67%)] Loss: 7400.513672\n",
      "Train Epoch: 172 [155904/225000 (69%)] Loss: 7288.681641\n",
      "Train Epoch: 172 [160000/225000 (71%)] Loss: 7208.933594\n",
      "Train Epoch: 172 [164096/225000 (73%)] Loss: 7238.873047\n",
      "Train Epoch: 172 [168192/225000 (75%)] Loss: 7292.103516\n",
      "Train Epoch: 172 [172288/225000 (77%)] Loss: 7265.826172\n",
      "Train Epoch: 172 [176384/225000 (78%)] Loss: 7245.527344\n",
      "Train Epoch: 172 [180480/225000 (80%)] Loss: 7460.517578\n",
      "Train Epoch: 172 [184576/225000 (82%)] Loss: 7350.585938\n",
      "Train Epoch: 172 [188672/225000 (84%)] Loss: 7228.722656\n",
      "Train Epoch: 172 [192768/225000 (86%)] Loss: 7339.443359\n",
      "Train Epoch: 172 [196864/225000 (87%)] Loss: 7410.302734\n",
      "Train Epoch: 172 [200960/225000 (89%)] Loss: 7184.439453\n",
      "Train Epoch: 172 [205056/225000 (91%)] Loss: 7271.886719\n",
      "Train Epoch: 172 [209152/225000 (93%)] Loss: 7257.406250\n",
      "Train Epoch: 172 [213248/225000 (95%)] Loss: 7345.519531\n",
      "Train Epoch: 172 [217344/225000 (97%)] Loss: 7443.535156\n",
      "Train Epoch: 172 [221440/225000 (98%)] Loss: 7185.914062\n",
      "    epoch          : 172\n",
      "    loss           : 7270.977081333546\n",
      "    val_loss       : 7268.784507586031\n",
      "Train Epoch: 173 [256/225000 (0%)] Loss: 7241.419922\n",
      "Train Epoch: 173 [4352/225000 (2%)] Loss: 7291.310547\n",
      "Train Epoch: 173 [8448/225000 (4%)] Loss: 7006.664062\n",
      "Train Epoch: 173 [12544/225000 (6%)] Loss: 7376.318359\n",
      "Train Epoch: 173 [16640/225000 (7%)] Loss: 7228.800781\n",
      "Train Epoch: 173 [20736/225000 (9%)] Loss: 7228.105469\n",
      "Train Epoch: 173 [24832/225000 (11%)] Loss: 7121.605469\n",
      "Train Epoch: 173 [28928/225000 (13%)] Loss: 7249.234375\n",
      "Train Epoch: 173 [33024/225000 (15%)] Loss: 7314.171875\n",
      "Train Epoch: 173 [37120/225000 (16%)] Loss: 7221.078125\n",
      "Train Epoch: 173 [41216/225000 (18%)] Loss: 7402.025391\n",
      "Train Epoch: 173 [45312/225000 (20%)] Loss: 7402.375000\n",
      "Train Epoch: 173 [49408/225000 (22%)] Loss: 7251.681641\n",
      "Train Epoch: 173 [53504/225000 (24%)] Loss: 7298.314453\n",
      "Train Epoch: 173 [57600/225000 (26%)] Loss: 7174.343750\n",
      "Train Epoch: 173 [61696/225000 (27%)] Loss: 7304.685547\n",
      "Train Epoch: 173 [65792/225000 (29%)] Loss: 7252.949219\n",
      "Train Epoch: 173 [69888/225000 (31%)] Loss: 7498.976562\n",
      "Train Epoch: 173 [73984/225000 (33%)] Loss: 7359.509766\n",
      "Train Epoch: 173 [78080/225000 (35%)] Loss: 7466.001953\n",
      "Train Epoch: 173 [82176/225000 (37%)] Loss: 7178.904297\n",
      "Train Epoch: 173 [86272/225000 (38%)] Loss: 7214.238281\n",
      "Train Epoch: 173 [90368/225000 (40%)] Loss: 7282.806641\n",
      "Train Epoch: 173 [94464/225000 (42%)] Loss: 7264.300781\n",
      "Train Epoch: 173 [98560/225000 (44%)] Loss: 7329.517578\n",
      "Train Epoch: 173 [102656/225000 (46%)] Loss: 7222.916016\n",
      "Train Epoch: 173 [106752/225000 (47%)] Loss: 7255.658203\n",
      "Train Epoch: 173 [110848/225000 (49%)] Loss: 7177.843750\n",
      "Train Epoch: 173 [114944/225000 (51%)] Loss: 7067.773438\n",
      "Train Epoch: 173 [119040/225000 (53%)] Loss: 7195.777344\n",
      "Train Epoch: 173 [123136/225000 (55%)] Loss: 7315.142578\n",
      "Train Epoch: 173 [127232/225000 (57%)] Loss: 7281.914062\n",
      "Train Epoch: 173 [131328/225000 (58%)] Loss: 7424.849609\n",
      "Train Epoch: 173 [135424/225000 (60%)] Loss: 7158.072266\n",
      "Train Epoch: 173 [139520/225000 (62%)] Loss: 7254.912109\n",
      "Train Epoch: 173 [143616/225000 (64%)] Loss: 7259.757812\n",
      "Train Epoch: 173 [147712/225000 (66%)] Loss: 7237.671875\n",
      "Train Epoch: 173 [151808/225000 (67%)] Loss: 7428.462891\n",
      "Train Epoch: 173 [155904/225000 (69%)] Loss: 7248.470703\n",
      "Train Epoch: 173 [160000/225000 (71%)] Loss: 7294.656250\n",
      "Train Epoch: 173 [164096/225000 (73%)] Loss: 7354.314453\n",
      "Train Epoch: 173 [168192/225000 (75%)] Loss: 7170.929688\n",
      "Train Epoch: 173 [172288/225000 (77%)] Loss: 7143.933594\n",
      "Train Epoch: 173 [176384/225000 (78%)] Loss: 7265.953125\n",
      "Train Epoch: 173 [180480/225000 (80%)] Loss: 7339.035156\n",
      "Train Epoch: 173 [184576/225000 (82%)] Loss: 7445.763672\n",
      "Train Epoch: 173 [188672/225000 (84%)] Loss: 7325.572266\n",
      "Train Epoch: 173 [192768/225000 (86%)] Loss: 7191.722656\n",
      "Train Epoch: 173 [196864/225000 (87%)] Loss: 7365.021484\n",
      "Train Epoch: 173 [200960/225000 (89%)] Loss: 7319.960938\n",
      "Train Epoch: 173 [205056/225000 (91%)] Loss: 7230.664062\n",
      "Train Epoch: 173 [209152/225000 (93%)] Loss: 7301.687500\n",
      "Train Epoch: 173 [213248/225000 (95%)] Loss: 7212.712891\n",
      "Train Epoch: 173 [217344/225000 (97%)] Loss: 7271.900391\n",
      "Train Epoch: 173 [221440/225000 (98%)] Loss: 7329.279297\n",
      "    epoch          : 173\n",
      "    loss           : 7326.762905290102\n",
      "    val_loss       : 7258.831927876083\n",
      "Train Epoch: 174 [256/225000 (0%)] Loss: 7322.341797\n",
      "Train Epoch: 174 [4352/225000 (2%)] Loss: 7368.066406\n",
      "Train Epoch: 174 [8448/225000 (4%)] Loss: 7240.269531\n",
      "Train Epoch: 174 [12544/225000 (6%)] Loss: 7236.138672\n",
      "Train Epoch: 174 [16640/225000 (7%)] Loss: 7304.814453\n",
      "Train Epoch: 174 [20736/225000 (9%)] Loss: 7113.685547\n",
      "Train Epoch: 174 [24832/225000 (11%)] Loss: 7192.296875\n",
      "Train Epoch: 174 [28928/225000 (13%)] Loss: 7352.001953\n",
      "Train Epoch: 174 [33024/225000 (15%)] Loss: 7507.486328\n",
      "Train Epoch: 174 [37120/225000 (16%)] Loss: 7459.853516\n",
      "Train Epoch: 174 [41216/225000 (18%)] Loss: 7265.478516\n",
      "Train Epoch: 174 [45312/225000 (20%)] Loss: 7167.878906\n",
      "Train Epoch: 174 [49408/225000 (22%)] Loss: 7139.119141\n",
      "Train Epoch: 174 [53504/225000 (24%)] Loss: 7327.943359\n",
      "Train Epoch: 174 [57600/225000 (26%)] Loss: 7202.052734\n",
      "Train Epoch: 174 [61696/225000 (27%)] Loss: 7251.062500\n",
      "Train Epoch: 174 [65792/225000 (29%)] Loss: 7328.158203\n",
      "Train Epoch: 174 [69888/225000 (31%)] Loss: 7189.259766\n",
      "Train Epoch: 174 [73984/225000 (33%)] Loss: 7271.005859\n",
      "Train Epoch: 174 [78080/225000 (35%)] Loss: 7207.599609\n",
      "Train Epoch: 174 [82176/225000 (37%)] Loss: 7538.017578\n",
      "Train Epoch: 174 [86272/225000 (38%)] Loss: 7070.511719\n",
      "Train Epoch: 174 [90368/225000 (40%)] Loss: 7179.367188\n",
      "Train Epoch: 174 [94464/225000 (42%)] Loss: 7325.666016\n",
      "Train Epoch: 174 [98560/225000 (44%)] Loss: 7312.458984\n",
      "Train Epoch: 174 [102656/225000 (46%)] Loss: 7246.994141\n",
      "Train Epoch: 174 [106752/225000 (47%)] Loss: 7273.705078\n",
      "Train Epoch: 174 [110848/225000 (49%)] Loss: 7340.189453\n",
      "Train Epoch: 174 [114944/225000 (51%)] Loss: 7212.527344\n",
      "Train Epoch: 174 [119040/225000 (53%)] Loss: 7360.798828\n",
      "Train Epoch: 174 [123136/225000 (55%)] Loss: 7306.224609\n",
      "Train Epoch: 174 [127232/225000 (57%)] Loss: 7138.009766\n",
      "Train Epoch: 174 [131328/225000 (58%)] Loss: 7277.691406\n",
      "Train Epoch: 174 [135424/225000 (60%)] Loss: 7178.140625\n",
      "Train Epoch: 174 [139520/225000 (62%)] Loss: 7431.210938\n",
      "Train Epoch: 174 [143616/225000 (64%)] Loss: 7144.501953\n",
      "Train Epoch: 174 [147712/225000 (66%)] Loss: 7330.216797\n",
      "Train Epoch: 174 [151808/225000 (67%)] Loss: 7099.466797\n",
      "Train Epoch: 174 [155904/225000 (69%)] Loss: 7313.328125\n",
      "Train Epoch: 174 [160000/225000 (71%)] Loss: 7308.925781\n",
      "Train Epoch: 174 [164096/225000 (73%)] Loss: 7257.779297\n",
      "Train Epoch: 174 [168192/225000 (75%)] Loss: 7374.054688\n",
      "Train Epoch: 174 [172288/225000 (77%)] Loss: 7231.322266\n",
      "Train Epoch: 174 [176384/225000 (78%)] Loss: 7248.619141\n",
      "Train Epoch: 174 [180480/225000 (80%)] Loss: 7264.599609\n",
      "Train Epoch: 174 [184576/225000 (82%)] Loss: 7235.056641\n",
      "Train Epoch: 174 [188672/225000 (84%)] Loss: 7239.628906\n",
      "Train Epoch: 174 [192768/225000 (86%)] Loss: 7257.794922\n",
      "Train Epoch: 174 [196864/225000 (87%)] Loss: 7155.949219\n",
      "Train Epoch: 174 [200960/225000 (89%)] Loss: 7186.333984\n",
      "Train Epoch: 174 [205056/225000 (91%)] Loss: 7164.558594\n",
      "Train Epoch: 174 [209152/225000 (93%)] Loss: 7336.990234\n",
      "Train Epoch: 174 [213248/225000 (95%)] Loss: 7261.349609\n",
      "Train Epoch: 174 [217344/225000 (97%)] Loss: 7219.849609\n",
      "Train Epoch: 174 [221440/225000 (98%)] Loss: 7498.972656\n",
      "    epoch          : 174\n",
      "    loss           : 7273.535448441055\n",
      "    val_loss       : 7265.020357342399\n",
      "Train Epoch: 175 [256/225000 (0%)] Loss: 7365.373047\n",
      "Train Epoch: 175 [4352/225000 (2%)] Loss: 7266.230469\n",
      "Train Epoch: 175 [8448/225000 (4%)] Loss: 7334.033203\n",
      "Train Epoch: 175 [12544/225000 (6%)] Loss: 7301.970703\n",
      "Train Epoch: 175 [16640/225000 (7%)] Loss: 7386.042969\n",
      "Train Epoch: 175 [20736/225000 (9%)] Loss: 7202.113281\n",
      "Train Epoch: 175 [24832/225000 (11%)] Loss: 7408.078125\n",
      "Train Epoch: 175 [28928/225000 (13%)] Loss: 7234.074219\n",
      "Train Epoch: 175 [33024/225000 (15%)] Loss: 7159.314453\n",
      "Train Epoch: 175 [37120/225000 (16%)] Loss: 7035.275391\n",
      "Train Epoch: 175 [41216/225000 (18%)] Loss: 7318.906250\n",
      "Train Epoch: 175 [45312/225000 (20%)] Loss: 7075.386719\n",
      "Train Epoch: 175 [49408/225000 (22%)] Loss: 7321.123047\n",
      "Train Epoch: 175 [53504/225000 (24%)] Loss: 7207.029297\n",
      "Train Epoch: 175 [57600/225000 (26%)] Loss: 7150.033203\n",
      "Train Epoch: 175 [61696/225000 (27%)] Loss: 7163.359375\n",
      "Train Epoch: 175 [65792/225000 (29%)] Loss: 7150.435547\n",
      "Train Epoch: 175 [69888/225000 (31%)] Loss: 7331.429688\n",
      "Train Epoch: 175 [73984/225000 (33%)] Loss: 7245.541016\n",
      "Train Epoch: 175 [78080/225000 (35%)] Loss: 7164.033203\n",
      "Train Epoch: 175 [82176/225000 (37%)] Loss: 7257.193359\n",
      "Train Epoch: 175 [86272/225000 (38%)] Loss: 7316.587891\n",
      "Train Epoch: 175 [90368/225000 (40%)] Loss: 7233.410156\n",
      "Train Epoch: 175 [94464/225000 (42%)] Loss: 7189.980469\n",
      "Train Epoch: 175 [98560/225000 (44%)] Loss: 7289.949219\n",
      "Train Epoch: 175 [102656/225000 (46%)] Loss: 7321.341797\n",
      "Train Epoch: 175 [106752/225000 (47%)] Loss: 7228.587891\n",
      "Train Epoch: 175 [110848/225000 (49%)] Loss: 7346.460938\n",
      "Train Epoch: 175 [114944/225000 (51%)] Loss: 7237.169922\n",
      "Train Epoch: 175 [119040/225000 (53%)] Loss: 7298.039062\n",
      "Train Epoch: 175 [123136/225000 (55%)] Loss: 7166.056641\n",
      "Train Epoch: 175 [127232/225000 (57%)] Loss: 7134.369141\n",
      "Train Epoch: 175 [131328/225000 (58%)] Loss: 7161.007812\n",
      "Train Epoch: 175 [135424/225000 (60%)] Loss: 7220.361328\n",
      "Train Epoch: 175 [139520/225000 (62%)] Loss: 17251.183594\n",
      "Train Epoch: 175 [143616/225000 (64%)] Loss: 7243.402344\n",
      "Train Epoch: 175 [147712/225000 (66%)] Loss: 7334.595703\n",
      "Train Epoch: 175 [151808/225000 (67%)] Loss: 7276.466797\n",
      "Train Epoch: 175 [155904/225000 (69%)] Loss: 7234.406250\n",
      "Train Epoch: 175 [160000/225000 (71%)] Loss: 7244.373047\n",
      "Train Epoch: 175 [164096/225000 (73%)] Loss: 7362.626953\n",
      "Train Epoch: 175 [168192/225000 (75%)] Loss: 7299.773438\n",
      "Train Epoch: 175 [172288/225000 (77%)] Loss: 7433.751953\n",
      "Train Epoch: 175 [176384/225000 (78%)] Loss: 7173.324219\n",
      "Train Epoch: 175 [180480/225000 (80%)] Loss: 7227.113281\n",
      "Train Epoch: 175 [184576/225000 (82%)] Loss: 7299.607422\n",
      "Train Epoch: 175 [188672/225000 (84%)] Loss: 7252.205078\n",
      "Train Epoch: 175 [192768/225000 (86%)] Loss: 7099.037109\n",
      "Train Epoch: 175 [196864/225000 (87%)] Loss: 7174.714844\n",
      "Train Epoch: 175 [200960/225000 (89%)] Loss: 7404.941406\n",
      "Train Epoch: 175 [205056/225000 (91%)] Loss: 7356.451172\n",
      "Train Epoch: 175 [209152/225000 (93%)] Loss: 7251.574219\n",
      "Train Epoch: 175 [213248/225000 (95%)] Loss: 7434.478516\n",
      "Train Epoch: 175 [217344/225000 (97%)] Loss: 7245.404297\n",
      "Train Epoch: 175 [221440/225000 (98%)] Loss: 7455.982422\n",
      "    epoch          : 175\n",
      "    loss           : 7271.056507305887\n",
      "    val_loss       : 7256.843563045774\n",
      "Train Epoch: 176 [256/225000 (0%)] Loss: 7223.685547\n",
      "Train Epoch: 176 [4352/225000 (2%)] Loss: 7286.179688\n",
      "Train Epoch: 176 [8448/225000 (4%)] Loss: 7235.714844\n",
      "Train Epoch: 176 [12544/225000 (6%)] Loss: 7206.572266\n",
      "Train Epoch: 176 [16640/225000 (7%)] Loss: 7222.906250\n",
      "Train Epoch: 176 [20736/225000 (9%)] Loss: 7375.132812\n",
      "Train Epoch: 176 [24832/225000 (11%)] Loss: 7343.126953\n",
      "Train Epoch: 176 [28928/225000 (13%)] Loss: 7121.544922\n",
      "Train Epoch: 176 [33024/225000 (15%)] Loss: 7174.595703\n",
      "Train Epoch: 176 [37120/225000 (16%)] Loss: 7259.777344\n",
      "Train Epoch: 176 [41216/225000 (18%)] Loss: 7276.949219\n",
      "Train Epoch: 176 [45312/225000 (20%)] Loss: 7118.855469\n",
      "Train Epoch: 176 [49408/225000 (22%)] Loss: 7266.296875\n",
      "Train Epoch: 176 [53504/225000 (24%)] Loss: 7248.638672\n",
      "Train Epoch: 176 [57600/225000 (26%)] Loss: 7215.564453\n",
      "Train Epoch: 176 [61696/225000 (27%)] Loss: 7383.984375\n",
      "Train Epoch: 176 [65792/225000 (29%)] Loss: 7240.638672\n",
      "Train Epoch: 176 [69888/225000 (31%)] Loss: 7275.289062\n",
      "Train Epoch: 176 [73984/225000 (33%)] Loss: 7351.291016\n",
      "Train Epoch: 176 [78080/225000 (35%)] Loss: 7347.601562\n",
      "Train Epoch: 176 [82176/225000 (37%)] Loss: 7392.123047\n",
      "Train Epoch: 176 [86272/225000 (38%)] Loss: 7249.976562\n",
      "Train Epoch: 176 [90368/225000 (40%)] Loss: 7434.208984\n",
      "Train Epoch: 176 [94464/225000 (42%)] Loss: 7388.380859\n",
      "Train Epoch: 176 [98560/225000 (44%)] Loss: 7444.197266\n",
      "Train Epoch: 176 [102656/225000 (46%)] Loss: 7322.798828\n",
      "Train Epoch: 176 [106752/225000 (47%)] Loss: 7219.068359\n",
      "Train Epoch: 176 [110848/225000 (49%)] Loss: 7215.986328\n",
      "Train Epoch: 176 [114944/225000 (51%)] Loss: 7295.943359\n",
      "Train Epoch: 176 [119040/225000 (53%)] Loss: 7228.785156\n",
      "Train Epoch: 176 [123136/225000 (55%)] Loss: 7346.751953\n",
      "Train Epoch: 176 [127232/225000 (57%)] Loss: 7384.878906\n",
      "Train Epoch: 176 [131328/225000 (58%)] Loss: 7570.521484\n",
      "Train Epoch: 176 [135424/225000 (60%)] Loss: 7368.914062\n",
      "Train Epoch: 176 [139520/225000 (62%)] Loss: 7123.396484\n",
      "Train Epoch: 176 [143616/225000 (64%)] Loss: 7241.712891\n",
      "Train Epoch: 176 [147712/225000 (66%)] Loss: 7233.046875\n",
      "Train Epoch: 176 [151808/225000 (67%)] Loss: 7299.382812\n",
      "Train Epoch: 176 [155904/225000 (69%)] Loss: 7287.203125\n",
      "Train Epoch: 176 [160000/225000 (71%)] Loss: 7209.753906\n",
      "Train Epoch: 176 [164096/225000 (73%)] Loss: 7292.925781\n",
      "Train Epoch: 176 [168192/225000 (75%)] Loss: 7359.505859\n",
      "Train Epoch: 176 [172288/225000 (77%)] Loss: 7230.134766\n",
      "Train Epoch: 176 [176384/225000 (78%)] Loss: 7222.134766\n",
      "Train Epoch: 176 [180480/225000 (80%)] Loss: 7204.484375\n",
      "Train Epoch: 176 [184576/225000 (82%)] Loss: 7279.722656\n",
      "Train Epoch: 176 [188672/225000 (84%)] Loss: 7259.115234\n",
      "Train Epoch: 176 [192768/225000 (86%)] Loss: 7258.103516\n",
      "Train Epoch: 176 [196864/225000 (87%)] Loss: 7276.890625\n",
      "Train Epoch: 176 [200960/225000 (89%)] Loss: 7141.490234\n",
      "Train Epoch: 176 [205056/225000 (91%)] Loss: 7309.345703\n",
      "Train Epoch: 176 [209152/225000 (93%)] Loss: 7145.222656\n",
      "Train Epoch: 176 [213248/225000 (95%)] Loss: 7390.816406\n",
      "Train Epoch: 176 [217344/225000 (97%)] Loss: 7348.912109\n",
      "Train Epoch: 176 [221440/225000 (98%)] Loss: 7342.623047\n",
      "    epoch          : 176\n",
      "    loss           : 7267.45637465337\n",
      "    val_loss       : 7255.721179487754\n",
      "Train Epoch: 177 [256/225000 (0%)] Loss: 7176.480469\n",
      "Train Epoch: 177 [4352/225000 (2%)] Loss: 7175.162109\n",
      "Train Epoch: 177 [8448/225000 (4%)] Loss: 7350.357422\n",
      "Train Epoch: 177 [12544/225000 (6%)] Loss: 7387.609375\n",
      "Train Epoch: 177 [16640/225000 (7%)] Loss: 7214.265625\n",
      "Train Epoch: 177 [20736/225000 (9%)] Loss: 7362.642578\n",
      "Train Epoch: 177 [24832/225000 (11%)] Loss: 7187.394531\n",
      "Train Epoch: 177 [28928/225000 (13%)] Loss: 7157.269531\n",
      "Train Epoch: 177 [33024/225000 (15%)] Loss: 7369.050781\n",
      "Train Epoch: 177 [37120/225000 (16%)] Loss: 7253.236328\n",
      "Train Epoch: 177 [41216/225000 (18%)] Loss: 7232.582031\n",
      "Train Epoch: 177 [45312/225000 (20%)] Loss: 7243.783203\n",
      "Train Epoch: 177 [49408/225000 (22%)] Loss: 7148.488281\n",
      "Train Epoch: 177 [53504/225000 (24%)] Loss: 7247.064453\n",
      "Train Epoch: 177 [57600/225000 (26%)] Loss: 7441.669922\n",
      "Train Epoch: 177 [61696/225000 (27%)] Loss: 7212.353516\n",
      "Train Epoch: 177 [65792/225000 (29%)] Loss: 7264.794922\n",
      "Train Epoch: 177 [69888/225000 (31%)] Loss: 7130.841797\n",
      "Train Epoch: 177 [73984/225000 (33%)] Loss: 7488.296875\n",
      "Train Epoch: 177 [78080/225000 (35%)] Loss: 7348.597656\n",
      "Train Epoch: 177 [82176/225000 (37%)] Loss: 7317.074219\n",
      "Train Epoch: 177 [86272/225000 (38%)] Loss: 7208.812500\n",
      "Train Epoch: 177 [90368/225000 (40%)] Loss: 7462.058594\n",
      "Train Epoch: 177 [94464/225000 (42%)] Loss: 7303.201172\n",
      "Train Epoch: 177 [98560/225000 (44%)] Loss: 7344.324219\n",
      "Train Epoch: 177 [102656/225000 (46%)] Loss: 7245.535156\n",
      "Train Epoch: 177 [106752/225000 (47%)] Loss: 7268.681641\n",
      "Train Epoch: 177 [110848/225000 (49%)] Loss: 7436.068359\n",
      "Train Epoch: 177 [114944/225000 (51%)] Loss: 7317.025391\n",
      "Train Epoch: 177 [119040/225000 (53%)] Loss: 7209.960938\n",
      "Train Epoch: 177 [123136/225000 (55%)] Loss: 7267.494141\n",
      "Train Epoch: 177 [127232/225000 (57%)] Loss: 7195.113281\n",
      "Train Epoch: 177 [131328/225000 (58%)] Loss: 7312.281250\n",
      "Train Epoch: 177 [135424/225000 (60%)] Loss: 7199.712891\n",
      "Train Epoch: 177 [139520/225000 (62%)] Loss: 7083.763672\n",
      "Train Epoch: 177 [143616/225000 (64%)] Loss: 7302.355469\n",
      "Train Epoch: 177 [147712/225000 (66%)] Loss: 7196.585938\n",
      "Train Epoch: 177 [151808/225000 (67%)] Loss: 7210.767578\n",
      "Train Epoch: 177 [155904/225000 (69%)] Loss: 7421.462891\n",
      "Train Epoch: 177 [160000/225000 (71%)] Loss: 7180.611328\n",
      "Train Epoch: 177 [164096/225000 (73%)] Loss: 7371.017578\n",
      "Train Epoch: 177 [168192/225000 (75%)] Loss: 7281.394531\n",
      "Train Epoch: 177 [172288/225000 (77%)] Loss: 7391.433594\n",
      "Train Epoch: 177 [176384/225000 (78%)] Loss: 7215.072266\n",
      "Train Epoch: 177 [180480/225000 (80%)] Loss: 7165.691406\n",
      "Train Epoch: 177 [184576/225000 (82%)] Loss: 7325.927734\n",
      "Train Epoch: 177 [188672/225000 (84%)] Loss: 7217.328125\n",
      "Train Epoch: 177 [192768/225000 (86%)] Loss: 7246.464844\n",
      "Train Epoch: 177 [196864/225000 (87%)] Loss: 7396.318359\n",
      "Train Epoch: 177 [200960/225000 (89%)] Loss: 7232.128906\n",
      "Train Epoch: 177 [205056/225000 (91%)] Loss: 7186.724609\n",
      "Train Epoch: 177 [209152/225000 (93%)] Loss: 7178.421875\n",
      "Train Epoch: 177 [213248/225000 (95%)] Loss: 7160.255859\n",
      "Train Epoch: 177 [217344/225000 (97%)] Loss: 7186.207031\n",
      "Train Epoch: 177 [221440/225000 (98%)] Loss: 7487.949219\n",
      "    epoch          : 177\n",
      "    loss           : 7314.807497200299\n",
      "    val_loss       : 7253.822092345783\n",
      "Train Epoch: 178 [256/225000 (0%)] Loss: 7302.826172\n",
      "Train Epoch: 178 [4352/225000 (2%)] Loss: 7285.167969\n",
      "Train Epoch: 178 [8448/225000 (4%)] Loss: 7293.994141\n",
      "Train Epoch: 178 [12544/225000 (6%)] Loss: 7189.861328\n",
      "Train Epoch: 178 [16640/225000 (7%)] Loss: 7365.919922\n",
      "Train Epoch: 178 [20736/225000 (9%)] Loss: 7122.345703\n",
      "Train Epoch: 178 [24832/225000 (11%)] Loss: 7256.699219\n",
      "Train Epoch: 178 [28928/225000 (13%)] Loss: 7237.554688\n",
      "Train Epoch: 178 [33024/225000 (15%)] Loss: 7295.833984\n",
      "Train Epoch: 178 [37120/225000 (16%)] Loss: 7374.167969\n",
      "Train Epoch: 178 [41216/225000 (18%)] Loss: 7266.695312\n",
      "Train Epoch: 178 [45312/225000 (20%)] Loss: 7205.248047\n",
      "Train Epoch: 178 [49408/225000 (22%)] Loss: 7289.796875\n",
      "Train Epoch: 178 [53504/225000 (24%)] Loss: 7333.031250\n",
      "Train Epoch: 178 [57600/225000 (26%)] Loss: 7279.810547\n",
      "Train Epoch: 178 [61696/225000 (27%)] Loss: 7198.884766\n",
      "Train Epoch: 178 [65792/225000 (29%)] Loss: 7081.013672\n",
      "Train Epoch: 178 [69888/225000 (31%)] Loss: 7286.605469\n",
      "Train Epoch: 178 [73984/225000 (33%)] Loss: 7252.980469\n",
      "Train Epoch: 178 [78080/225000 (35%)] Loss: 7396.160156\n",
      "Train Epoch: 178 [82176/225000 (37%)] Loss: 7221.408203\n",
      "Train Epoch: 178 [86272/225000 (38%)] Loss: 7381.726562\n",
      "Train Epoch: 178 [90368/225000 (40%)] Loss: 7137.916016\n",
      "Train Epoch: 178 [94464/225000 (42%)] Loss: 7299.753906\n",
      "Train Epoch: 178 [98560/225000 (44%)] Loss: 7221.390625\n",
      "Train Epoch: 178 [102656/225000 (46%)] Loss: 7329.234375\n",
      "Train Epoch: 178 [106752/225000 (47%)] Loss: 7141.039062\n",
      "Train Epoch: 178 [110848/225000 (49%)] Loss: 7253.259766\n",
      "Train Epoch: 178 [114944/225000 (51%)] Loss: 7286.066406\n",
      "Train Epoch: 178 [119040/225000 (53%)] Loss: 7210.505859\n",
      "Train Epoch: 178 [123136/225000 (55%)] Loss: 7378.160156\n",
      "Train Epoch: 178 [127232/225000 (57%)] Loss: 7255.550781\n",
      "Train Epoch: 178 [131328/225000 (58%)] Loss: 7243.818359\n",
      "Train Epoch: 178 [135424/225000 (60%)] Loss: 7225.685547\n",
      "Train Epoch: 178 [139520/225000 (62%)] Loss: 7198.628906\n",
      "Train Epoch: 178 [143616/225000 (64%)] Loss: 7063.875000\n",
      "Train Epoch: 178 [147712/225000 (66%)] Loss: 7305.345703\n",
      "Train Epoch: 178 [151808/225000 (67%)] Loss: 7149.269531\n",
      "Train Epoch: 178 [155904/225000 (69%)] Loss: 7290.908203\n",
      "Train Epoch: 178 [160000/225000 (71%)] Loss: 7227.404297\n",
      "Train Epoch: 178 [164096/225000 (73%)] Loss: 7195.460938\n",
      "Train Epoch: 178 [168192/225000 (75%)] Loss: 7370.519531\n",
      "Train Epoch: 178 [172288/225000 (77%)] Loss: 7107.390625\n",
      "Train Epoch: 178 [176384/225000 (78%)] Loss: 7218.103516\n",
      "Train Epoch: 178 [180480/225000 (80%)] Loss: 7184.074219\n",
      "Train Epoch: 178 [184576/225000 (82%)] Loss: 7189.792969\n",
      "Train Epoch: 178 [188672/225000 (84%)] Loss: 7227.083984\n",
      "Train Epoch: 178 [192768/225000 (86%)] Loss: 7317.320312\n",
      "Train Epoch: 178 [196864/225000 (87%)] Loss: 7281.107422\n",
      "Train Epoch: 178 [200960/225000 (89%)] Loss: 7129.544922\n",
      "Train Epoch: 178 [205056/225000 (91%)] Loss: 7309.673828\n",
      "Train Epoch: 178 [209152/225000 (93%)] Loss: 7329.873047\n",
      "Train Epoch: 178 [213248/225000 (95%)] Loss: 7251.453125\n",
      "Train Epoch: 178 [217344/225000 (97%)] Loss: 7262.005859\n",
      "Train Epoch: 178 [221440/225000 (98%)] Loss: 7377.994141\n",
      "    epoch          : 178\n",
      "    loss           : 7260.923233743956\n",
      "    val_loss       : 7255.04310912867\n",
      "Train Epoch: 179 [256/225000 (0%)] Loss: 7241.560547\n",
      "Train Epoch: 179 [4352/225000 (2%)] Loss: 7193.898438\n",
      "Train Epoch: 179 [8448/225000 (4%)] Loss: 7259.199219\n",
      "Train Epoch: 179 [12544/225000 (6%)] Loss: 7211.791016\n",
      "Train Epoch: 179 [16640/225000 (7%)] Loss: 7338.691406\n",
      "Train Epoch: 179 [20736/225000 (9%)] Loss: 7103.353516\n",
      "Train Epoch: 179 [24832/225000 (11%)] Loss: 7238.046875\n",
      "Train Epoch: 179 [28928/225000 (13%)] Loss: 7516.404297\n",
      "Train Epoch: 179 [33024/225000 (15%)] Loss: 7244.449219\n",
      "Train Epoch: 179 [37120/225000 (16%)] Loss: 7303.738281\n",
      "Train Epoch: 179 [41216/225000 (18%)] Loss: 7330.789062\n",
      "Train Epoch: 179 [45312/225000 (20%)] Loss: 7215.289062\n",
      "Train Epoch: 179 [49408/225000 (22%)] Loss: 7175.226562\n",
      "Train Epoch: 179 [53504/225000 (24%)] Loss: 7156.916016\n",
      "Train Epoch: 179 [57600/225000 (26%)] Loss: 7167.490234\n",
      "Train Epoch: 179 [61696/225000 (27%)] Loss: 7129.228516\n",
      "Train Epoch: 179 [65792/225000 (29%)] Loss: 7267.267578\n",
      "Train Epoch: 179 [69888/225000 (31%)] Loss: 7264.986328\n",
      "Train Epoch: 179 [73984/225000 (33%)] Loss: 7249.199219\n",
      "Train Epoch: 179 [78080/225000 (35%)] Loss: 7159.705078\n",
      "Train Epoch: 179 [82176/225000 (37%)] Loss: 7135.574219\n",
      "Train Epoch: 179 [86272/225000 (38%)] Loss: 7188.541016\n",
      "Train Epoch: 179 [90368/225000 (40%)] Loss: 7440.150391\n",
      "Train Epoch: 179 [94464/225000 (42%)] Loss: 7391.820312\n",
      "Train Epoch: 179 [98560/225000 (44%)] Loss: 7135.681641\n",
      "Train Epoch: 179 [102656/225000 (46%)] Loss: 7352.785156\n",
      "Train Epoch: 179 [106752/225000 (47%)] Loss: 7326.375000\n",
      "Train Epoch: 179 [110848/225000 (49%)] Loss: 7362.755859\n",
      "Train Epoch: 179 [114944/225000 (51%)] Loss: 7184.806641\n",
      "Train Epoch: 179 [119040/225000 (53%)] Loss: 7183.914062\n",
      "Train Epoch: 179 [123136/225000 (55%)] Loss: 7360.910156\n",
      "Train Epoch: 179 [127232/225000 (57%)] Loss: 7223.316406\n",
      "Train Epoch: 179 [131328/225000 (58%)] Loss: 7249.046875\n",
      "Train Epoch: 179 [135424/225000 (60%)] Loss: 7311.638672\n",
      "Train Epoch: 179 [139520/225000 (62%)] Loss: 7372.359375\n",
      "Train Epoch: 179 [143616/225000 (64%)] Loss: 7328.830078\n",
      "Train Epoch: 179 [147712/225000 (66%)] Loss: 7089.662109\n",
      "Train Epoch: 179 [151808/225000 (67%)] Loss: 7218.992188\n",
      "Train Epoch: 179 [155904/225000 (69%)] Loss: 7147.722656\n",
      "Train Epoch: 179 [160000/225000 (71%)] Loss: 7193.160156\n",
      "Train Epoch: 179 [164096/225000 (73%)] Loss: 7193.185547\n",
      "Train Epoch: 179 [168192/225000 (75%)] Loss: 7248.875000\n",
      "Train Epoch: 179 [172288/225000 (77%)] Loss: 7211.048828\n",
      "Train Epoch: 179 [176384/225000 (78%)] Loss: 7176.232422\n",
      "Train Epoch: 179 [180480/225000 (80%)] Loss: 7302.175781\n",
      "Train Epoch: 179 [184576/225000 (82%)] Loss: 7195.904297\n",
      "Train Epoch: 179 [188672/225000 (84%)] Loss: 7222.593750\n",
      "Train Epoch: 179 [192768/225000 (86%)] Loss: 7365.810547\n",
      "Train Epoch: 179 [196864/225000 (87%)] Loss: 7119.779297\n",
      "Train Epoch: 179 [200960/225000 (89%)] Loss: 7227.808594\n",
      "Train Epoch: 179 [205056/225000 (91%)] Loss: 7221.779297\n",
      "Train Epoch: 179 [209152/225000 (93%)] Loss: 7179.121094\n",
      "Train Epoch: 179 [213248/225000 (95%)] Loss: 7343.955078\n",
      "Train Epoch: 179 [217344/225000 (97%)] Loss: 7160.708984\n",
      "Train Epoch: 179 [221440/225000 (98%)] Loss: 7321.539062\n",
      "    epoch          : 179\n",
      "    loss           : 7245.36584430994\n",
      "    val_loss       : 7247.53459943314\n",
      "Train Epoch: 180 [256/225000 (0%)] Loss: 7141.550781\n",
      "Train Epoch: 180 [4352/225000 (2%)] Loss: 7391.964844\n",
      "Train Epoch: 180 [8448/225000 (4%)] Loss: 7185.410156\n",
      "Train Epoch: 180 [12544/225000 (6%)] Loss: 7349.744141\n",
      "Train Epoch: 180 [16640/225000 (7%)] Loss: 7290.894531\n",
      "Train Epoch: 180 [20736/225000 (9%)] Loss: 7167.683594\n",
      "Train Epoch: 180 [24832/225000 (11%)] Loss: 7271.960938\n",
      "Train Epoch: 180 [28928/225000 (13%)] Loss: 7231.642578\n",
      "Train Epoch: 180 [33024/225000 (15%)] Loss: 7374.763672\n",
      "Train Epoch: 180 [37120/225000 (16%)] Loss: 7219.689453\n",
      "Train Epoch: 180 [41216/225000 (18%)] Loss: 7438.621094\n",
      "Train Epoch: 180 [45312/225000 (20%)] Loss: 7279.734375\n",
      "Train Epoch: 180 [49408/225000 (22%)] Loss: 7260.775391\n",
      "Train Epoch: 180 [53504/225000 (24%)] Loss: 7281.371094\n",
      "Train Epoch: 180 [57600/225000 (26%)] Loss: 7459.550781\n",
      "Train Epoch: 180 [61696/225000 (27%)] Loss: 7269.064453\n",
      "Train Epoch: 180 [65792/225000 (29%)] Loss: 7210.949219\n",
      "Train Epoch: 180 [69888/225000 (31%)] Loss: 7394.537109\n",
      "Train Epoch: 180 [73984/225000 (33%)] Loss: 7367.574219\n",
      "Train Epoch: 180 [78080/225000 (35%)] Loss: 7195.083984\n",
      "Train Epoch: 180 [82176/225000 (37%)] Loss: 7322.511719\n",
      "Train Epoch: 180 [86272/225000 (38%)] Loss: 7160.761719\n",
      "Train Epoch: 180 [90368/225000 (40%)] Loss: 7232.769531\n",
      "Train Epoch: 180 [94464/225000 (42%)] Loss: 7321.085938\n",
      "Train Epoch: 180 [98560/225000 (44%)] Loss: 7284.402344\n",
      "Train Epoch: 180 [102656/225000 (46%)] Loss: 7236.023438\n",
      "Train Epoch: 180 [106752/225000 (47%)] Loss: 7055.816406\n",
      "Train Epoch: 180 [110848/225000 (49%)] Loss: 7236.341797\n",
      "Train Epoch: 180 [114944/225000 (51%)] Loss: 7316.417969\n",
      "Train Epoch: 180 [119040/225000 (53%)] Loss: 7203.009766\n",
      "Train Epoch: 180 [123136/225000 (55%)] Loss: 7332.968750\n",
      "Train Epoch: 180 [127232/225000 (57%)] Loss: 7207.316406\n",
      "Train Epoch: 180 [131328/225000 (58%)] Loss: 7248.072266\n",
      "Train Epoch: 180 [135424/225000 (60%)] Loss: 7137.812500\n",
      "Train Epoch: 180 [139520/225000 (62%)] Loss: 7403.986328\n",
      "Train Epoch: 180 [143616/225000 (64%)] Loss: 7130.113281\n",
      "Train Epoch: 180 [147712/225000 (66%)] Loss: 7230.427734\n",
      "Train Epoch: 180 [151808/225000 (67%)] Loss: 7218.757812\n",
      "Train Epoch: 180 [155904/225000 (69%)] Loss: 7318.593750\n",
      "Train Epoch: 180 [160000/225000 (71%)] Loss: 7147.257812\n",
      "Train Epoch: 180 [164096/225000 (73%)] Loss: 7194.339844\n",
      "Train Epoch: 180 [168192/225000 (75%)] Loss: 7269.759766\n",
      "Train Epoch: 180 [172288/225000 (77%)] Loss: 7136.265625\n",
      "Train Epoch: 180 [176384/225000 (78%)] Loss: 7156.447266\n",
      "Train Epoch: 180 [180480/225000 (80%)] Loss: 7281.349609\n",
      "Train Epoch: 180 [184576/225000 (82%)] Loss: 7314.554688\n",
      "Train Epoch: 180 [188672/225000 (84%)] Loss: 7298.093750\n",
      "Train Epoch: 180 [192768/225000 (86%)] Loss: 7418.500000\n",
      "Train Epoch: 180 [196864/225000 (87%)] Loss: 7184.562500\n",
      "Train Epoch: 180 [200960/225000 (89%)] Loss: 7204.062500\n",
      "Train Epoch: 180 [205056/225000 (91%)] Loss: 7328.500000\n",
      "Train Epoch: 180 [209152/225000 (93%)] Loss: 7292.767578\n",
      "Train Epoch: 180 [213248/225000 (95%)] Loss: 7336.806641\n",
      "Train Epoch: 180 [217344/225000 (97%)] Loss: 7236.320312\n",
      "Train Epoch: 180 [221440/225000 (98%)] Loss: 7273.007812\n",
      "    epoch          : 180\n",
      "    loss           : 7264.247046981655\n",
      "    val_loss       : 7235.5997406001\n",
      "Train Epoch: 181 [256/225000 (0%)] Loss: 7222.955078\n",
      "Train Epoch: 181 [4352/225000 (2%)] Loss: 7311.242188\n",
      "Train Epoch: 181 [8448/225000 (4%)] Loss: 7324.503906\n",
      "Train Epoch: 181 [12544/225000 (6%)] Loss: 7229.498047\n",
      "Train Epoch: 181 [16640/225000 (7%)] Loss: 7094.248047\n",
      "Train Epoch: 181 [20736/225000 (9%)] Loss: 7185.392578\n",
      "Train Epoch: 181 [24832/225000 (11%)] Loss: 7205.707031\n",
      "Train Epoch: 181 [28928/225000 (13%)] Loss: 7273.603516\n",
      "Train Epoch: 181 [33024/225000 (15%)] Loss: 7332.000000\n",
      "Train Epoch: 181 [37120/225000 (16%)] Loss: 7260.126953\n",
      "Train Epoch: 181 [41216/225000 (18%)] Loss: 7246.349609\n",
      "Train Epoch: 181 [45312/225000 (20%)] Loss: 7347.388672\n",
      "Train Epoch: 181 [49408/225000 (22%)] Loss: 7328.701172\n",
      "Train Epoch: 181 [53504/225000 (24%)] Loss: 7248.806641\n",
      "Train Epoch: 181 [57600/225000 (26%)] Loss: 7231.068359\n",
      "Train Epoch: 181 [61696/225000 (27%)] Loss: 7080.513672\n",
      "Train Epoch: 181 [65792/225000 (29%)] Loss: 7180.292969\n",
      "Train Epoch: 181 [69888/225000 (31%)] Loss: 7201.908203\n",
      "Train Epoch: 181 [73984/225000 (33%)] Loss: 7297.019531\n",
      "Train Epoch: 181 [78080/225000 (35%)] Loss: 7230.789062\n",
      "Train Epoch: 181 [82176/225000 (37%)] Loss: 7200.572266\n",
      "Train Epoch: 181 [86272/225000 (38%)] Loss: 7445.197266\n",
      "Train Epoch: 181 [90368/225000 (40%)] Loss: 7434.058594\n",
      "Train Epoch: 181 [94464/225000 (42%)] Loss: 7261.195312\n",
      "Train Epoch: 181 [98560/225000 (44%)] Loss: 7224.080078\n",
      "Train Epoch: 181 [102656/225000 (46%)] Loss: 7188.619141\n",
      "Train Epoch: 181 [106752/225000 (47%)] Loss: 7248.609375\n",
      "Train Epoch: 181 [110848/225000 (49%)] Loss: 7233.328125\n",
      "Train Epoch: 181 [114944/225000 (51%)] Loss: 7302.335938\n",
      "Train Epoch: 181 [119040/225000 (53%)] Loss: 7162.392578\n",
      "Train Epoch: 181 [123136/225000 (55%)] Loss: 7277.083984\n",
      "Train Epoch: 181 [127232/225000 (57%)] Loss: 7179.005859\n",
      "Train Epoch: 181 [131328/225000 (58%)] Loss: 7167.208984\n",
      "Train Epoch: 181 [135424/225000 (60%)] Loss: 7259.564453\n",
      "Train Epoch: 181 [139520/225000 (62%)] Loss: 7230.859375\n",
      "Train Epoch: 181 [143616/225000 (64%)] Loss: 7234.214844\n",
      "Train Epoch: 181 [147712/225000 (66%)] Loss: 7361.568359\n",
      "Train Epoch: 181 [151808/225000 (67%)] Loss: 7122.037109\n",
      "Train Epoch: 181 [155904/225000 (69%)] Loss: 7144.923828\n",
      "Train Epoch: 181 [160000/225000 (71%)] Loss: 7236.935547\n",
      "Train Epoch: 181 [164096/225000 (73%)] Loss: 7199.697266\n",
      "Train Epoch: 181 [168192/225000 (75%)] Loss: 7175.535156\n",
      "Train Epoch: 181 [172288/225000 (77%)] Loss: 7142.845703\n",
      "Train Epoch: 181 [176384/225000 (78%)] Loss: 7089.162109\n",
      "Train Epoch: 181 [180480/225000 (80%)] Loss: 7355.708984\n",
      "Train Epoch: 181 [184576/225000 (82%)] Loss: 7323.144531\n",
      "Train Epoch: 181 [188672/225000 (84%)] Loss: 7063.492188\n",
      "Train Epoch: 181 [192768/225000 (86%)] Loss: 7203.976562\n",
      "Train Epoch: 181 [196864/225000 (87%)] Loss: 7218.134766\n",
      "Train Epoch: 181 [200960/225000 (89%)] Loss: 7234.656250\n",
      "Train Epoch: 181 [205056/225000 (91%)] Loss: 7118.529297\n",
      "Train Epoch: 181 [209152/225000 (93%)] Loss: 7284.037109\n",
      "Train Epoch: 181 [213248/225000 (95%)] Loss: 7188.605469\n",
      "Train Epoch: 181 [217344/225000 (97%)] Loss: 7315.234375\n",
      "Train Epoch: 181 [221440/225000 (98%)] Loss: 7207.140625\n",
      "    epoch          : 181\n",
      "    loss           : 7238.210929723052\n",
      "    val_loss       : 7332.519209995562\n",
      "Train Epoch: 182 [256/225000 (0%)] Loss: 7190.054688\n",
      "Train Epoch: 182 [4352/225000 (2%)] Loss: 7044.496094\n",
      "Train Epoch: 182 [8448/225000 (4%)] Loss: 7170.035156\n",
      "Train Epoch: 182 [12544/225000 (6%)] Loss: 7110.796875\n",
      "Train Epoch: 182 [16640/225000 (7%)] Loss: 7255.529297\n",
      "Train Epoch: 182 [20736/225000 (9%)] Loss: 7093.630859\n",
      "Train Epoch: 182 [24832/225000 (11%)] Loss: 7147.691406\n",
      "Train Epoch: 182 [28928/225000 (13%)] Loss: 7459.623047\n",
      "Train Epoch: 182 [33024/225000 (15%)] Loss: 7020.263672\n",
      "Train Epoch: 182 [37120/225000 (16%)] Loss: 7150.710938\n",
      "Train Epoch: 182 [41216/225000 (18%)] Loss: 7216.494141\n",
      "Train Epoch: 182 [45312/225000 (20%)] Loss: 7259.681641\n",
      "Train Epoch: 182 [49408/225000 (22%)] Loss: 7237.396484\n",
      "Train Epoch: 182 [53504/225000 (24%)] Loss: 7288.900391\n",
      "Train Epoch: 182 [57600/225000 (26%)] Loss: 7187.642578\n",
      "Train Epoch: 182 [61696/225000 (27%)] Loss: 7217.199219\n",
      "Train Epoch: 182 [65792/225000 (29%)] Loss: 7093.748047\n",
      "Train Epoch: 182 [69888/225000 (31%)] Loss: 7288.976562\n",
      "Train Epoch: 182 [73984/225000 (33%)] Loss: 7178.673828\n",
      "Train Epoch: 182 [78080/225000 (35%)] Loss: 7249.017578\n",
      "Train Epoch: 182 [82176/225000 (37%)] Loss: 7269.486328\n",
      "Train Epoch: 182 [86272/225000 (38%)] Loss: 7128.232422\n",
      "Train Epoch: 182 [90368/225000 (40%)] Loss: 7211.044922\n",
      "Train Epoch: 182 [94464/225000 (42%)] Loss: 7143.923828\n",
      "Train Epoch: 182 [98560/225000 (44%)] Loss: 7275.023438\n",
      "Train Epoch: 182 [102656/225000 (46%)] Loss: 7216.830078\n",
      "Train Epoch: 182 [106752/225000 (47%)] Loss: 7105.634766\n",
      "Train Epoch: 182 [110848/225000 (49%)] Loss: 7185.927734\n",
      "Train Epoch: 182 [114944/225000 (51%)] Loss: 7166.550781\n",
      "Train Epoch: 182 [119040/225000 (53%)] Loss: 7120.259766\n",
      "Train Epoch: 182 [123136/225000 (55%)] Loss: 7235.892578\n",
      "Train Epoch: 182 [127232/225000 (57%)] Loss: 7181.236328\n",
      "Train Epoch: 182 [131328/225000 (58%)] Loss: 7270.199219\n",
      "Train Epoch: 182 [135424/225000 (60%)] Loss: 7113.880859\n",
      "Train Epoch: 182 [139520/225000 (62%)] Loss: 7367.785156\n",
      "Train Epoch: 182 [143616/225000 (64%)] Loss: 7269.240234\n",
      "Train Epoch: 182 [147712/225000 (66%)] Loss: 7228.332031\n",
      "Train Epoch: 182 [151808/225000 (67%)] Loss: 7418.054688\n",
      "Train Epoch: 182 [155904/225000 (69%)] Loss: 7245.218750\n",
      "Train Epoch: 182 [160000/225000 (71%)] Loss: 7207.447266\n",
      "Train Epoch: 182 [164096/225000 (73%)] Loss: 7404.103516\n",
      "Train Epoch: 182 [168192/225000 (75%)] Loss: 7166.123047\n",
      "Train Epoch: 182 [172288/225000 (77%)] Loss: 7211.609375\n",
      "Train Epoch: 182 [176384/225000 (78%)] Loss: 7448.287109\n",
      "Train Epoch: 182 [180480/225000 (80%)] Loss: 7241.636719\n",
      "Train Epoch: 182 [184576/225000 (82%)] Loss: 7268.224609\n",
      "Train Epoch: 182 [188672/225000 (84%)] Loss: 7260.171875\n",
      "Train Epoch: 182 [192768/225000 (86%)] Loss: 7322.150391\n",
      "Train Epoch: 182 [196864/225000 (87%)] Loss: 7216.125000\n",
      "Train Epoch: 182 [200960/225000 (89%)] Loss: 7243.832031\n",
      "Train Epoch: 182 [205056/225000 (91%)] Loss: 7245.201172\n",
      "Train Epoch: 182 [209152/225000 (93%)] Loss: 7176.312500\n",
      "Train Epoch: 182 [213248/225000 (95%)] Loss: 7438.722656\n",
      "Train Epoch: 182 [217344/225000 (97%)] Loss: 7340.056641\n",
      "Train Epoch: 182 [221440/225000 (98%)] Loss: 7346.779297\n",
      "    epoch          : 182\n",
      "    loss           : 7235.348325067548\n",
      "    val_loss       : 7233.250589337276\n",
      "Train Epoch: 183 [256/225000 (0%)] Loss: 7212.005859\n",
      "Train Epoch: 183 [4352/225000 (2%)] Loss: 7408.660156\n",
      "Train Epoch: 183 [8448/225000 (4%)] Loss: 7110.328125\n",
      "Train Epoch: 183 [12544/225000 (6%)] Loss: 7179.998047\n",
      "Train Epoch: 183 [16640/225000 (7%)] Loss: 7322.941406\n",
      "Train Epoch: 183 [20736/225000 (9%)] Loss: 7360.158203\n",
      "Train Epoch: 183 [24832/225000 (11%)] Loss: 7224.320312\n",
      "Train Epoch: 183 [28928/225000 (13%)] Loss: 7136.890625\n",
      "Train Epoch: 183 [33024/225000 (15%)] Loss: 7222.980469\n",
      "Train Epoch: 183 [37120/225000 (16%)] Loss: 7292.792969\n",
      "Train Epoch: 183 [41216/225000 (18%)] Loss: 7343.755859\n",
      "Train Epoch: 183 [45312/225000 (20%)] Loss: 7415.310547\n",
      "Train Epoch: 183 [49408/225000 (22%)] Loss: 7313.695312\n",
      "Train Epoch: 183 [53504/225000 (24%)] Loss: 7158.931641\n",
      "Train Epoch: 183 [57600/225000 (26%)] Loss: 7162.171875\n",
      "Train Epoch: 183 [61696/225000 (27%)] Loss: 7079.115234\n",
      "Train Epoch: 183 [65792/225000 (29%)] Loss: 7237.929688\n",
      "Train Epoch: 183 [69888/225000 (31%)] Loss: 7266.806641\n",
      "Train Epoch: 183 [73984/225000 (33%)] Loss: 7310.849609\n",
      "Train Epoch: 183 [78080/225000 (35%)] Loss: 7239.751953\n",
      "Train Epoch: 183 [82176/225000 (37%)] Loss: 7281.455078\n",
      "Train Epoch: 183 [86272/225000 (38%)] Loss: 7120.447266\n",
      "Train Epoch: 183 [90368/225000 (40%)] Loss: 7171.382812\n",
      "Train Epoch: 183 [94464/225000 (42%)] Loss: 7181.117188\n",
      "Train Epoch: 183 [98560/225000 (44%)] Loss: 7176.513672\n",
      "Train Epoch: 183 [102656/225000 (46%)] Loss: 7237.201172\n",
      "Train Epoch: 183 [106752/225000 (47%)] Loss: 7253.564453\n",
      "Train Epoch: 183 [110848/225000 (49%)] Loss: 7319.648438\n",
      "Train Epoch: 183 [114944/225000 (51%)] Loss: 7390.220703\n",
      "Train Epoch: 183 [119040/225000 (53%)] Loss: 7502.025391\n",
      "Train Epoch: 183 [123136/225000 (55%)] Loss: 7259.652344\n",
      "Train Epoch: 183 [127232/225000 (57%)] Loss: 7171.294922\n",
      "Train Epoch: 183 [131328/225000 (58%)] Loss: 7091.917969\n",
      "Train Epoch: 183 [135424/225000 (60%)] Loss: 7258.480469\n",
      "Train Epoch: 183 [139520/225000 (62%)] Loss: 7068.322266\n",
      "Train Epoch: 183 [143616/225000 (64%)] Loss: 7179.332031\n",
      "Train Epoch: 183 [147712/225000 (66%)] Loss: 7279.396484\n",
      "Train Epoch: 183 [151808/225000 (67%)] Loss: 7318.550781\n",
      "Train Epoch: 183 [155904/225000 (69%)] Loss: 7395.353516\n",
      "Train Epoch: 183 [160000/225000 (71%)] Loss: 7268.457031\n",
      "Train Epoch: 183 [164096/225000 (73%)] Loss: 7289.917969\n",
      "Train Epoch: 183 [168192/225000 (75%)] Loss: 7375.898438\n",
      "Train Epoch: 183 [172288/225000 (77%)] Loss: 6942.916016\n",
      "Train Epoch: 183 [176384/225000 (78%)] Loss: 7133.824219\n",
      "Train Epoch: 183 [180480/225000 (80%)] Loss: 7268.337891\n",
      "Train Epoch: 183 [184576/225000 (82%)] Loss: 7248.304688\n",
      "Train Epoch: 183 [188672/225000 (84%)] Loss: 7259.169922\n",
      "Train Epoch: 183 [192768/225000 (86%)] Loss: 7324.533203\n",
      "Train Epoch: 183 [196864/225000 (87%)] Loss: 7111.720703\n",
      "Train Epoch: 183 [200960/225000 (89%)] Loss: 7119.628906\n",
      "Train Epoch: 183 [205056/225000 (91%)] Loss: 7203.033203\n",
      "Train Epoch: 183 [209152/225000 (93%)] Loss: 7277.734375\n",
      "Train Epoch: 183 [213248/225000 (95%)] Loss: 7272.191406\n",
      "Train Epoch: 183 [217344/225000 (97%)] Loss: 7164.644531\n",
      "Train Epoch: 183 [221440/225000 (98%)] Loss: 7173.873047\n",
      "    epoch          : 183\n",
      "    loss           : 7237.646175519056\n",
      "    val_loss       : 7235.879303843391\n",
      "Train Epoch: 184 [256/225000 (0%)] Loss: 7244.265625\n",
      "Train Epoch: 184 [4352/225000 (2%)] Loss: 7121.826172\n",
      "Train Epoch: 184 [8448/225000 (4%)] Loss: 7320.587891\n",
      "Train Epoch: 184 [12544/225000 (6%)] Loss: 7266.880859\n",
      "Train Epoch: 184 [16640/225000 (7%)] Loss: 7146.363281\n",
      "Train Epoch: 184 [20736/225000 (9%)] Loss: 7339.099609\n",
      "Train Epoch: 184 [24832/225000 (11%)] Loss: 7177.089844\n",
      "Train Epoch: 184 [28928/225000 (13%)] Loss: 7159.980469\n",
      "Train Epoch: 184 [33024/225000 (15%)] Loss: 7177.062500\n",
      "Train Epoch: 184 [37120/225000 (16%)] Loss: 7283.968750\n",
      "Train Epoch: 184 [41216/225000 (18%)] Loss: 7355.021484\n",
      "Train Epoch: 184 [45312/225000 (20%)] Loss: 7277.365234\n",
      "Train Epoch: 184 [49408/225000 (22%)] Loss: 7211.777344\n",
      "Train Epoch: 184 [53504/225000 (24%)] Loss: 7149.095703\n",
      "Train Epoch: 184 [57600/225000 (26%)] Loss: 7239.716797\n",
      "Train Epoch: 184 [61696/225000 (27%)] Loss: 7228.187500\n",
      "Train Epoch: 184 [65792/225000 (29%)] Loss: 7135.304688\n",
      "Train Epoch: 184 [69888/225000 (31%)] Loss: 7449.410156\n",
      "Train Epoch: 184 [73984/225000 (33%)] Loss: 7109.759766\n",
      "Train Epoch: 184 [78080/225000 (35%)] Loss: 7217.380859\n",
      "Train Epoch: 184 [82176/225000 (37%)] Loss: 7271.197266\n",
      "Train Epoch: 184 [86272/225000 (38%)] Loss: 7370.500000\n",
      "Train Epoch: 184 [90368/225000 (40%)] Loss: 7171.402344\n",
      "Train Epoch: 184 [94464/225000 (42%)] Loss: 7246.458984\n",
      "Train Epoch: 184 [98560/225000 (44%)] Loss: 7213.123047\n",
      "Train Epoch: 184 [102656/225000 (46%)] Loss: 7101.779297\n",
      "Train Epoch: 184 [106752/225000 (47%)] Loss: 7089.482422\n",
      "Train Epoch: 184 [110848/225000 (49%)] Loss: 7242.800781\n",
      "Train Epoch: 184 [114944/225000 (51%)] Loss: 7321.654297\n",
      "Train Epoch: 184 [119040/225000 (53%)] Loss: 7224.673828\n",
      "Train Epoch: 184 [123136/225000 (55%)] Loss: 7223.107422\n",
      "Train Epoch: 184 [127232/225000 (57%)] Loss: 7008.849609\n",
      "Train Epoch: 184 [131328/225000 (58%)] Loss: 7165.738281\n",
      "Train Epoch: 184 [135424/225000 (60%)] Loss: 7257.660156\n",
      "Train Epoch: 184 [139520/225000 (62%)] Loss: 7202.869141\n",
      "Train Epoch: 184 [143616/225000 (64%)] Loss: 7121.513672\n",
      "Train Epoch: 184 [147712/225000 (66%)] Loss: 7302.111328\n",
      "Train Epoch: 184 [151808/225000 (67%)] Loss: 7201.628906\n",
      "Train Epoch: 184 [155904/225000 (69%)] Loss: 7352.074219\n",
      "Train Epoch: 184 [160000/225000 (71%)] Loss: 7294.773438\n",
      "Train Epoch: 184 [164096/225000 (73%)] Loss: 7244.005859\n",
      "Train Epoch: 184 [168192/225000 (75%)] Loss: 7204.714844\n",
      "Train Epoch: 184 [172288/225000 (77%)] Loss: 7254.037109\n",
      "Train Epoch: 184 [176384/225000 (78%)] Loss: 7203.857422\n",
      "Train Epoch: 184 [180480/225000 (80%)] Loss: 7171.681641\n",
      "Train Epoch: 184 [184576/225000 (82%)] Loss: 7088.789062\n",
      "Train Epoch: 184 [188672/225000 (84%)] Loss: 7222.716797\n",
      "Train Epoch: 184 [192768/225000 (86%)] Loss: 7278.181641\n",
      "Train Epoch: 184 [196864/225000 (87%)] Loss: 7200.605469\n",
      "Train Epoch: 184 [200960/225000 (89%)] Loss: 7156.130859\n",
      "Train Epoch: 184 [205056/225000 (91%)] Loss: 7390.039062\n",
      "Train Epoch: 184 [209152/225000 (93%)] Loss: 7266.457031\n",
      "Train Epoch: 184 [213248/225000 (95%)] Loss: 7106.404297\n",
      "Train Epoch: 184 [217344/225000 (97%)] Loss: 7192.037109\n",
      "Train Epoch: 184 [221440/225000 (98%)] Loss: 7280.503906\n",
      "    epoch          : 184\n",
      "    loss           : 7239.07442761661\n",
      "    val_loss       : 7233.158455568917\n",
      "Train Epoch: 185 [256/225000 (0%)] Loss: 7144.375000\n",
      "Train Epoch: 185 [4352/225000 (2%)] Loss: 7244.263672\n",
      "Train Epoch: 185 [8448/225000 (4%)] Loss: 7287.494141\n",
      "Train Epoch: 185 [12544/225000 (6%)] Loss: 7191.468750\n",
      "Train Epoch: 185 [16640/225000 (7%)] Loss: 7254.457031\n",
      "Train Epoch: 185 [20736/225000 (9%)] Loss: 7104.425781\n",
      "Train Epoch: 185 [24832/225000 (11%)] Loss: 7252.080078\n",
      "Train Epoch: 185 [28928/225000 (13%)] Loss: 7149.201172\n",
      "Train Epoch: 185 [33024/225000 (15%)] Loss: 7505.148438\n",
      "Train Epoch: 185 [37120/225000 (16%)] Loss: 7299.365234\n",
      "Train Epoch: 185 [41216/225000 (18%)] Loss: 7366.054688\n",
      "Train Epoch: 185 [45312/225000 (20%)] Loss: 7230.117188\n",
      "Train Epoch: 185 [49408/225000 (22%)] Loss: 7091.742188\n",
      "Train Epoch: 185 [53504/225000 (24%)] Loss: 7228.175781\n",
      "Train Epoch: 185 [57600/225000 (26%)] Loss: 7120.371094\n",
      "Train Epoch: 185 [61696/225000 (27%)] Loss: 7056.529297\n",
      "Train Epoch: 185 [65792/225000 (29%)] Loss: 7320.425781\n",
      "Train Epoch: 185 [69888/225000 (31%)] Loss: 7367.464844\n",
      "Train Epoch: 185 [73984/225000 (33%)] Loss: 7070.152344\n",
      "Train Epoch: 185 [78080/225000 (35%)] Loss: 7221.433594\n",
      "Train Epoch: 185 [82176/225000 (37%)] Loss: 7273.130859\n",
      "Train Epoch: 185 [86272/225000 (38%)] Loss: 7083.935547\n",
      "Train Epoch: 185 [90368/225000 (40%)] Loss: 7355.007812\n",
      "Train Epoch: 185 [94464/225000 (42%)] Loss: 7303.912109\n",
      "Train Epoch: 185 [98560/225000 (44%)] Loss: 7136.072266\n",
      "Train Epoch: 185 [102656/225000 (46%)] Loss: 7186.300781\n",
      "Train Epoch: 185 [106752/225000 (47%)] Loss: 7202.146484\n",
      "Train Epoch: 185 [110848/225000 (49%)] Loss: 7344.267578\n",
      "Train Epoch: 185 [114944/225000 (51%)] Loss: 7289.791016\n",
      "Train Epoch: 185 [119040/225000 (53%)] Loss: 7374.935547\n",
      "Train Epoch: 185 [123136/225000 (55%)] Loss: 7163.529297\n",
      "Train Epoch: 185 [127232/225000 (57%)] Loss: 7221.912109\n",
      "Train Epoch: 185 [131328/225000 (58%)] Loss: 7210.216797\n",
      "Train Epoch: 185 [135424/225000 (60%)] Loss: 7285.361328\n",
      "Train Epoch: 185 [139520/225000 (62%)] Loss: 7073.882812\n",
      "Train Epoch: 185 [143616/225000 (64%)] Loss: 7207.832031\n",
      "Train Epoch: 185 [147712/225000 (66%)] Loss: 7354.820312\n",
      "Train Epoch: 185 [151808/225000 (67%)] Loss: 7151.914062\n",
      "Train Epoch: 185 [155904/225000 (69%)] Loss: 7074.625000\n",
      "Train Epoch: 185 [160000/225000 (71%)] Loss: 7042.802734\n",
      "Train Epoch: 185 [164096/225000 (73%)] Loss: 7291.591797\n",
      "Train Epoch: 185 [168192/225000 (75%)] Loss: 7368.521484\n",
      "Train Epoch: 185 [172288/225000 (77%)] Loss: 7318.082031\n",
      "Train Epoch: 185 [176384/225000 (78%)] Loss: 7177.835938\n",
      "Train Epoch: 185 [180480/225000 (80%)] Loss: 7260.367188\n",
      "Train Epoch: 185 [184576/225000 (82%)] Loss: 7186.222656\n",
      "Train Epoch: 185 [188672/225000 (84%)] Loss: 7208.048828\n",
      "Train Epoch: 185 [192768/225000 (86%)] Loss: 7154.181641\n",
      "Train Epoch: 185 [196864/225000 (87%)] Loss: 7144.041016\n",
      "Train Epoch: 185 [200960/225000 (89%)] Loss: 7306.898438\n",
      "Train Epoch: 185 [205056/225000 (91%)] Loss: 7204.033203\n",
      "Train Epoch: 185 [209152/225000 (93%)] Loss: 7340.517578\n",
      "Train Epoch: 185 [213248/225000 (95%)] Loss: 7230.892578\n",
      "Train Epoch: 185 [217344/225000 (97%)] Loss: 7324.158203\n",
      "Train Epoch: 185 [221440/225000 (98%)] Loss: 7142.177734\n",
      "    epoch          : 185\n",
      "    loss           : 7235.226048110424\n",
      "    val_loss       : 7225.677064540435\n",
      "Train Epoch: 186 [256/225000 (0%)] Loss: 7400.488281\n",
      "Train Epoch: 186 [4352/225000 (2%)] Loss: 7343.406250\n",
      "Train Epoch: 186 [8448/225000 (4%)] Loss: 7088.355469\n",
      "Train Epoch: 186 [12544/225000 (6%)] Loss: 7242.693359\n",
      "Train Epoch: 186 [16640/225000 (7%)] Loss: 7069.392578\n",
      "Train Epoch: 186 [20736/225000 (9%)] Loss: 7227.269531\n",
      "Train Epoch: 186 [24832/225000 (11%)] Loss: 7179.656250\n",
      "Train Epoch: 186 [28928/225000 (13%)] Loss: 7302.826172\n",
      "Train Epoch: 186 [33024/225000 (15%)] Loss: 7139.939453\n",
      "Train Epoch: 186 [37120/225000 (16%)] Loss: 7149.853516\n",
      "Train Epoch: 186 [41216/225000 (18%)] Loss: 7256.527344\n",
      "Train Epoch: 186 [45312/225000 (20%)] Loss: 7240.863281\n",
      "Train Epoch: 186 [49408/225000 (22%)] Loss: 16867.369141\n",
      "Train Epoch: 186 [53504/225000 (24%)] Loss: 7229.966797\n",
      "Train Epoch: 186 [57600/225000 (26%)] Loss: 7326.443359\n",
      "Train Epoch: 186 [61696/225000 (27%)] Loss: 7292.816406\n",
      "Train Epoch: 186 [65792/225000 (29%)] Loss: 7316.746094\n",
      "Train Epoch: 186 [69888/225000 (31%)] Loss: 7227.804688\n",
      "Train Epoch: 186 [73984/225000 (33%)] Loss: 7319.626953\n",
      "Train Epoch: 186 [78080/225000 (35%)] Loss: 7251.328125\n",
      "Train Epoch: 186 [82176/225000 (37%)] Loss: 7205.937500\n",
      "Train Epoch: 186 [86272/225000 (38%)] Loss: 7237.445312\n",
      "Train Epoch: 186 [90368/225000 (40%)] Loss: 7065.921875\n",
      "Train Epoch: 186 [94464/225000 (42%)] Loss: 7567.492188\n",
      "Train Epoch: 186 [98560/225000 (44%)] Loss: 7099.150391\n",
      "Train Epoch: 186 [102656/225000 (46%)] Loss: 7236.376953\n",
      "Train Epoch: 186 [106752/225000 (47%)] Loss: 7344.562500\n",
      "Train Epoch: 186 [110848/225000 (49%)] Loss: 6973.837891\n",
      "Train Epoch: 186 [114944/225000 (51%)] Loss: 7222.314453\n",
      "Train Epoch: 186 [119040/225000 (53%)] Loss: 7180.642578\n",
      "Train Epoch: 186 [123136/225000 (55%)] Loss: 7337.486328\n",
      "Train Epoch: 186 [127232/225000 (57%)] Loss: 7232.003906\n",
      "Train Epoch: 186 [131328/225000 (58%)] Loss: 7325.708984\n",
      "Train Epoch: 186 [135424/225000 (60%)] Loss: 7261.000000\n",
      "Train Epoch: 186 [139520/225000 (62%)] Loss: 7192.037109\n",
      "Train Epoch: 186 [143616/225000 (64%)] Loss: 7331.480469\n",
      "Train Epoch: 186 [147712/225000 (66%)] Loss: 7234.248047\n",
      "Train Epoch: 186 [151808/225000 (67%)] Loss: 7178.324219\n",
      "Train Epoch: 186 [155904/225000 (69%)] Loss: 7212.177734\n",
      "Train Epoch: 186 [160000/225000 (71%)] Loss: 7243.408203\n",
      "Train Epoch: 186 [164096/225000 (73%)] Loss: 7139.212891\n",
      "Train Epoch: 186 [168192/225000 (75%)] Loss: 7110.248047\n",
      "Train Epoch: 186 [172288/225000 (77%)] Loss: 7209.390625\n",
      "Train Epoch: 186 [176384/225000 (78%)] Loss: 7194.482422\n",
      "Train Epoch: 186 [180480/225000 (80%)] Loss: 7287.908203\n",
      "Train Epoch: 186 [184576/225000 (82%)] Loss: 7232.816406\n",
      "Train Epoch: 186 [188672/225000 (84%)] Loss: 7151.355469\n",
      "Train Epoch: 186 [192768/225000 (86%)] Loss: 7235.136719\n",
      "Train Epoch: 186 [196864/225000 (87%)] Loss: 7357.265625\n",
      "Train Epoch: 186 [200960/225000 (89%)] Loss: 7145.490234\n",
      "Train Epoch: 186 [205056/225000 (91%)] Loss: 7262.169922\n",
      "Train Epoch: 186 [209152/225000 (93%)] Loss: 7310.806641\n",
      "Train Epoch: 186 [213248/225000 (95%)] Loss: 7361.242188\n",
      "Train Epoch: 186 [217344/225000 (97%)] Loss: 7108.978516\n",
      "Train Epoch: 186 [221440/225000 (98%)] Loss: 7154.162109\n",
      "    epoch          : 186\n",
      "    loss           : 7253.277275979451\n",
      "    val_loss       : 7443.695519941194\n",
      "Train Epoch: 187 [256/225000 (0%)] Loss: 7195.412109\n",
      "Train Epoch: 187 [4352/225000 (2%)] Loss: 7238.853516\n",
      "Train Epoch: 187 [8448/225000 (4%)] Loss: 7125.052734\n",
      "Train Epoch: 187 [12544/225000 (6%)] Loss: 7160.757812\n",
      "Train Epoch: 187 [16640/225000 (7%)] Loss: 7261.121094\n",
      "Train Epoch: 187 [20736/225000 (9%)] Loss: 7235.386719\n",
      "Train Epoch: 187 [24832/225000 (11%)] Loss: 7166.457031\n",
      "Train Epoch: 187 [28928/225000 (13%)] Loss: 7222.244141\n",
      "Train Epoch: 187 [33024/225000 (15%)] Loss: 7154.175781\n",
      "Train Epoch: 187 [37120/225000 (16%)] Loss: 7120.109375\n",
      "Train Epoch: 187 [41216/225000 (18%)] Loss: 7120.240234\n",
      "Train Epoch: 187 [45312/225000 (20%)] Loss: 7211.990234\n",
      "Train Epoch: 187 [49408/225000 (22%)] Loss: 7047.583984\n",
      "Train Epoch: 187 [53504/225000 (24%)] Loss: 7253.921875\n",
      "Train Epoch: 187 [57600/225000 (26%)] Loss: 7030.638672\n",
      "Train Epoch: 187 [61696/225000 (27%)] Loss: 7175.005859\n",
      "Train Epoch: 187 [65792/225000 (29%)] Loss: 7121.490234\n",
      "Train Epoch: 187 [69888/225000 (31%)] Loss: 7181.125000\n",
      "Train Epoch: 187 [73984/225000 (33%)] Loss: 7362.136719\n",
      "Train Epoch: 187 [78080/225000 (35%)] Loss: 7286.050781\n",
      "Train Epoch: 187 [82176/225000 (37%)] Loss: 7220.187500\n",
      "Train Epoch: 187 [86272/225000 (38%)] Loss: 7121.251953\n",
      "Train Epoch: 187 [90368/225000 (40%)] Loss: 7368.755859\n",
      "Train Epoch: 187 [94464/225000 (42%)] Loss: 7290.332031\n",
      "Train Epoch: 187 [98560/225000 (44%)] Loss: 7073.427734\n",
      "Train Epoch: 187 [102656/225000 (46%)] Loss: 7231.896484\n",
      "Train Epoch: 187 [106752/225000 (47%)] Loss: 7209.410156\n",
      "Train Epoch: 187 [110848/225000 (49%)] Loss: 7202.812500\n",
      "Train Epoch: 187 [114944/225000 (51%)] Loss: 7298.556641\n",
      "Train Epoch: 187 [119040/225000 (53%)] Loss: 7243.669922\n",
      "Train Epoch: 187 [123136/225000 (55%)] Loss: 7159.037109\n",
      "Train Epoch: 187 [127232/225000 (57%)] Loss: 7277.960938\n",
      "Train Epoch: 187 [131328/225000 (58%)] Loss: 7065.236328\n",
      "Train Epoch: 187 [135424/225000 (60%)] Loss: 7162.660156\n",
      "Train Epoch: 187 [139520/225000 (62%)] Loss: 7293.443359\n",
      "Train Epoch: 187 [143616/225000 (64%)] Loss: 7331.085938\n",
      "Train Epoch: 187 [147712/225000 (66%)] Loss: 7255.980469\n",
      "Train Epoch: 187 [151808/225000 (67%)] Loss: 7313.683594\n",
      "Train Epoch: 187 [155904/225000 (69%)] Loss: 7287.056641\n",
      "Train Epoch: 187 [160000/225000 (71%)] Loss: 7189.232422\n",
      "Train Epoch: 187 [164096/225000 (73%)] Loss: 7276.142578\n",
      "Train Epoch: 187 [168192/225000 (75%)] Loss: 7163.787109\n",
      "Train Epoch: 187 [172288/225000 (77%)] Loss: 7170.488281\n",
      "Train Epoch: 187 [176384/225000 (78%)] Loss: 7230.871094\n",
      "Train Epoch: 187 [180480/225000 (80%)] Loss: 7148.414062\n",
      "Train Epoch: 187 [184576/225000 (82%)] Loss: 7154.535156\n",
      "Train Epoch: 187 [188672/225000 (84%)] Loss: 7249.427734\n",
      "Train Epoch: 187 [192768/225000 (86%)] Loss: 7249.556641\n",
      "Train Epoch: 187 [196864/225000 (87%)] Loss: 7303.207031\n",
      "Train Epoch: 187 [200960/225000 (89%)] Loss: 7094.011719\n",
      "Train Epoch: 187 [205056/225000 (91%)] Loss: 7303.958984\n",
      "Train Epoch: 187 [209152/225000 (93%)] Loss: 7274.033203\n",
      "Train Epoch: 187 [213248/225000 (95%)] Loss: 7238.615234\n",
      "Train Epoch: 187 [217344/225000 (97%)] Loss: 7086.710938\n",
      "Train Epoch: 187 [221440/225000 (98%)] Loss: 7122.958984\n",
      "    epoch          : 187\n",
      "    loss           : 7253.634137914178\n",
      "    val_loss       : 7404.8910447091475\n",
      "Train Epoch: 188 [256/225000 (0%)] Loss: 7260.087891\n",
      "Train Epoch: 188 [4352/225000 (2%)] Loss: 7236.703125\n",
      "Train Epoch: 188 [8448/225000 (4%)] Loss: 7059.218750\n",
      "Train Epoch: 188 [12544/225000 (6%)] Loss: 7324.835938\n",
      "Train Epoch: 188 [16640/225000 (7%)] Loss: 7236.667969\n",
      "Train Epoch: 188 [20736/225000 (9%)] Loss: 7248.439453\n",
      "Train Epoch: 188 [24832/225000 (11%)] Loss: 7277.392578\n",
      "Train Epoch: 188 [28928/225000 (13%)] Loss: 7139.841797\n",
      "Train Epoch: 188 [33024/225000 (15%)] Loss: 7191.175781\n",
      "Train Epoch: 188 [37120/225000 (16%)] Loss: 7393.929688\n",
      "Train Epoch: 188 [41216/225000 (18%)] Loss: 7215.726562\n",
      "Train Epoch: 188 [45312/225000 (20%)] Loss: 7297.386719\n",
      "Train Epoch: 188 [49408/225000 (22%)] Loss: 7232.117188\n",
      "Train Epoch: 188 [53504/225000 (24%)] Loss: 7195.472656\n",
      "Train Epoch: 188 [57600/225000 (26%)] Loss: 7219.439453\n",
      "Train Epoch: 188 [61696/225000 (27%)] Loss: 7297.019531\n",
      "Train Epoch: 188 [65792/225000 (29%)] Loss: 7154.601562\n",
      "Train Epoch: 188 [69888/225000 (31%)] Loss: 7226.683594\n",
      "Train Epoch: 188 [73984/225000 (33%)] Loss: 7275.378906\n",
      "Train Epoch: 188 [78080/225000 (35%)] Loss: 7146.914062\n",
      "Train Epoch: 188 [82176/225000 (37%)] Loss: 7091.949219\n",
      "Train Epoch: 188 [86272/225000 (38%)] Loss: 7334.919922\n",
      "Train Epoch: 188 [90368/225000 (40%)] Loss: 7181.119141\n",
      "Train Epoch: 188 [94464/225000 (42%)] Loss: 7137.751953\n",
      "Train Epoch: 188 [98560/225000 (44%)] Loss: 7177.244141\n",
      "Train Epoch: 188 [102656/225000 (46%)] Loss: 6957.757812\n",
      "Train Epoch: 188 [106752/225000 (47%)] Loss: 7134.484375\n",
      "Train Epoch: 188 [110848/225000 (49%)] Loss: 7183.410156\n",
      "Train Epoch: 188 [114944/225000 (51%)] Loss: 7192.628906\n",
      "Train Epoch: 188 [119040/225000 (53%)] Loss: 7242.898438\n",
      "Train Epoch: 188 [123136/225000 (55%)] Loss: 7351.003906\n",
      "Train Epoch: 188 [127232/225000 (57%)] Loss: 7107.794922\n",
      "Train Epoch: 188 [131328/225000 (58%)] Loss: 7219.005859\n",
      "Train Epoch: 188 [135424/225000 (60%)] Loss: 6991.425781\n",
      "Train Epoch: 188 [139520/225000 (62%)] Loss: 7159.007812\n",
      "Train Epoch: 188 [143616/225000 (64%)] Loss: 7172.951172\n",
      "Train Epoch: 188 [147712/225000 (66%)] Loss: 7215.833984\n",
      "Train Epoch: 188 [151808/225000 (67%)] Loss: 7066.519531\n",
      "Train Epoch: 188 [155904/225000 (69%)] Loss: 7227.841797\n",
      "Train Epoch: 188 [160000/225000 (71%)] Loss: 7117.142578\n",
      "Train Epoch: 188 [164096/225000 (73%)] Loss: 7044.582031\n",
      "Train Epoch: 188 [168192/225000 (75%)] Loss: 7156.255859\n",
      "Train Epoch: 188 [172288/225000 (77%)] Loss: 7083.033203\n",
      "Train Epoch: 188 [176384/225000 (78%)] Loss: 7207.005859\n",
      "Train Epoch: 188 [180480/225000 (80%)] Loss: 7188.531250\n",
      "Train Epoch: 188 [184576/225000 (82%)] Loss: 6939.960938\n",
      "Train Epoch: 188 [188672/225000 (84%)] Loss: 7150.285156\n",
      "Train Epoch: 188 [192768/225000 (86%)] Loss: 7342.958984\n",
      "Train Epoch: 188 [196864/225000 (87%)] Loss: 7239.595703\n",
      "Train Epoch: 188 [200960/225000 (89%)] Loss: 7234.935547\n",
      "Train Epoch: 188 [205056/225000 (91%)] Loss: 7053.623047\n",
      "Train Epoch: 188 [209152/225000 (93%)] Loss: 7173.210938\n",
      "Train Epoch: 188 [213248/225000 (95%)] Loss: 7110.335938\n",
      "Train Epoch: 188 [217344/225000 (97%)] Loss: 7292.529297\n",
      "Train Epoch: 188 [221440/225000 (98%)] Loss: 7245.373047\n",
      "    epoch          : 188\n",
      "    loss           : 7249.634215683661\n",
      "    val_loss       : 7217.377851133444\n",
      "Train Epoch: 189 [256/225000 (0%)] Loss: 7168.263672\n",
      "Train Epoch: 189 [4352/225000 (2%)] Loss: 7231.949219\n",
      "Train Epoch: 189 [8448/225000 (4%)] Loss: 7175.976562\n",
      "Train Epoch: 189 [12544/225000 (6%)] Loss: 7154.080078\n",
      "Train Epoch: 189 [16640/225000 (7%)] Loss: 7222.285156\n",
      "Train Epoch: 189 [20736/225000 (9%)] Loss: 7284.869141\n",
      "Train Epoch: 189 [24832/225000 (11%)] Loss: 7045.742188\n",
      "Train Epoch: 189 [28928/225000 (13%)] Loss: 7030.144531\n",
      "Train Epoch: 189 [33024/225000 (15%)] Loss: 7159.652344\n",
      "Train Epoch: 189 [37120/225000 (16%)] Loss: 7345.027344\n",
      "Train Epoch: 189 [41216/225000 (18%)] Loss: 7225.990234\n",
      "Train Epoch: 189 [45312/225000 (20%)] Loss: 7394.990234\n",
      "Train Epoch: 189 [49408/225000 (22%)] Loss: 7173.492188\n",
      "Train Epoch: 189 [53504/225000 (24%)] Loss: 7232.029297\n",
      "Train Epoch: 189 [57600/225000 (26%)] Loss: 7045.462891\n",
      "Train Epoch: 189 [61696/225000 (27%)] Loss: 7213.720703\n",
      "Train Epoch: 189 [65792/225000 (29%)] Loss: 7060.599609\n",
      "Train Epoch: 189 [69888/225000 (31%)] Loss: 7080.542969\n",
      "Train Epoch: 189 [73984/225000 (33%)] Loss: 7341.554688\n",
      "Train Epoch: 189 [78080/225000 (35%)] Loss: 7192.833984\n",
      "Train Epoch: 189 [82176/225000 (37%)] Loss: 6984.679688\n",
      "Train Epoch: 189 [86272/225000 (38%)] Loss: 7194.097656\n",
      "Train Epoch: 189 [90368/225000 (40%)] Loss: 7056.083984\n",
      "Train Epoch: 189 [94464/225000 (42%)] Loss: 7363.490234\n",
      "Train Epoch: 189 [98560/225000 (44%)] Loss: 7219.765625\n",
      "Train Epoch: 189 [102656/225000 (46%)] Loss: 7492.343750\n",
      "Train Epoch: 189 [106752/225000 (47%)] Loss: 7385.125000\n",
      "Train Epoch: 189 [110848/225000 (49%)] Loss: 7090.498047\n",
      "Train Epoch: 189 [114944/225000 (51%)] Loss: 7275.560547\n",
      "Train Epoch: 189 [119040/225000 (53%)] Loss: 7205.009766\n",
      "Train Epoch: 189 [123136/225000 (55%)] Loss: 7209.203125\n",
      "Train Epoch: 189 [127232/225000 (57%)] Loss: 7182.199219\n",
      "Train Epoch: 189 [131328/225000 (58%)] Loss: 7151.718750\n",
      "Train Epoch: 189 [135424/225000 (60%)] Loss: 7218.992188\n",
      "Train Epoch: 189 [139520/225000 (62%)] Loss: 7131.222656\n",
      "Train Epoch: 189 [143616/225000 (64%)] Loss: 7206.189453\n",
      "Train Epoch: 189 [147712/225000 (66%)] Loss: 7183.822266\n",
      "Train Epoch: 189 [151808/225000 (67%)] Loss: 7214.730469\n",
      "Train Epoch: 189 [155904/225000 (69%)] Loss: 7269.791016\n",
      "Train Epoch: 189 [160000/225000 (71%)] Loss: 7188.916016\n",
      "Train Epoch: 189 [164096/225000 (73%)] Loss: 7125.990234\n",
      "Train Epoch: 189 [168192/225000 (75%)] Loss: 7034.878906\n",
      "Train Epoch: 189 [172288/225000 (77%)] Loss: 7357.921875\n",
      "Train Epoch: 189 [176384/225000 (78%)] Loss: 7276.542969\n",
      "Train Epoch: 189 [180480/225000 (80%)] Loss: 7284.771484\n",
      "Train Epoch: 189 [184576/225000 (82%)] Loss: 7164.541016\n",
      "Train Epoch: 189 [188672/225000 (84%)] Loss: 7227.085938\n",
      "Train Epoch: 189 [192768/225000 (86%)] Loss: 7158.105469\n",
      "Train Epoch: 189 [196864/225000 (87%)] Loss: 7159.527344\n",
      "Train Epoch: 189 [200960/225000 (89%)] Loss: 7191.488281\n",
      "Train Epoch: 189 [205056/225000 (91%)] Loss: 7293.458984\n",
      "Train Epoch: 189 [209152/225000 (93%)] Loss: 7358.449219\n",
      "Train Epoch: 189 [213248/225000 (95%)] Loss: 7199.267578\n",
      "Train Epoch: 189 [217344/225000 (97%)] Loss: 7139.578125\n",
      "Train Epoch: 189 [221440/225000 (98%)] Loss: 7473.941406\n",
      "    epoch          : 189\n",
      "    loss           : 7212.648919670791\n",
      "    val_loss       : 7207.19490290418\n",
      "Train Epoch: 190 [256/225000 (0%)] Loss: 7250.332031\n",
      "Train Epoch: 190 [4352/225000 (2%)] Loss: 7129.044922\n",
      "Train Epoch: 190 [8448/225000 (4%)] Loss: 7421.226562\n",
      "Train Epoch: 190 [12544/225000 (6%)] Loss: 6978.101562\n",
      "Train Epoch: 190 [16640/225000 (7%)] Loss: 7181.835938\n",
      "Train Epoch: 190 [20736/225000 (9%)] Loss: 7272.529297\n",
      "Train Epoch: 190 [24832/225000 (11%)] Loss: 7121.466797\n",
      "Train Epoch: 190 [28928/225000 (13%)] Loss: 7126.892578\n",
      "Train Epoch: 190 [33024/225000 (15%)] Loss: 7390.253906\n",
      "Train Epoch: 190 [37120/225000 (16%)] Loss: 7172.267578\n",
      "Train Epoch: 190 [41216/225000 (18%)] Loss: 7375.658203\n",
      "Train Epoch: 190 [45312/225000 (20%)] Loss: 7098.693359\n",
      "Train Epoch: 190 [49408/225000 (22%)] Loss: 7053.525391\n",
      "Train Epoch: 190 [53504/225000 (24%)] Loss: 7252.957031\n",
      "Train Epoch: 190 [57600/225000 (26%)] Loss: 7161.375000\n",
      "Train Epoch: 190 [61696/225000 (27%)] Loss: 7238.117188\n",
      "Train Epoch: 190 [65792/225000 (29%)] Loss: 7019.017578\n",
      "Train Epoch: 190 [69888/225000 (31%)] Loss: 7171.994141\n",
      "Train Epoch: 190 [73984/225000 (33%)] Loss: 7266.650391\n",
      "Train Epoch: 190 [78080/225000 (35%)] Loss: 7283.560547\n",
      "Train Epoch: 190 [82176/225000 (37%)] Loss: 7134.367188\n",
      "Train Epoch: 190 [86272/225000 (38%)] Loss: 7152.748047\n",
      "Train Epoch: 190 [90368/225000 (40%)] Loss: 7277.812500\n",
      "Train Epoch: 190 [94464/225000 (42%)] Loss: 7279.783203\n",
      "Train Epoch: 190 [98560/225000 (44%)] Loss: 7223.339844\n",
      "Train Epoch: 190 [102656/225000 (46%)] Loss: 7129.582031\n",
      "Train Epoch: 190 [106752/225000 (47%)] Loss: 7268.789062\n",
      "Train Epoch: 190 [110848/225000 (49%)] Loss: 7158.138672\n",
      "Train Epoch: 190 [114944/225000 (51%)] Loss: 7327.652344\n",
      "Train Epoch: 190 [119040/225000 (53%)] Loss: 7274.595703\n",
      "Train Epoch: 190 [123136/225000 (55%)] Loss: 7292.544922\n",
      "Train Epoch: 190 [127232/225000 (57%)] Loss: 7288.998047\n",
      "Train Epoch: 190 [131328/225000 (58%)] Loss: 7192.974609\n",
      "Train Epoch: 190 [135424/225000 (60%)] Loss: 7149.910156\n",
      "Train Epoch: 190 [139520/225000 (62%)] Loss: 7127.064453\n",
      "Train Epoch: 190 [143616/225000 (64%)] Loss: 7131.925781\n",
      "Train Epoch: 190 [147712/225000 (66%)] Loss: 7187.347656\n",
      "Train Epoch: 190 [151808/225000 (67%)] Loss: 7028.976562\n",
      "Train Epoch: 190 [155904/225000 (69%)] Loss: 7321.564453\n",
      "Train Epoch: 190 [160000/225000 (71%)] Loss: 7309.515625\n",
      "Train Epoch: 190 [164096/225000 (73%)] Loss: 7110.144531\n",
      "Train Epoch: 190 [168192/225000 (75%)] Loss: 7274.996094\n",
      "Train Epoch: 190 [172288/225000 (77%)] Loss: 7190.115234\n",
      "Train Epoch: 190 [176384/225000 (78%)] Loss: 7122.275391\n",
      "Train Epoch: 190 [180480/225000 (80%)] Loss: 7159.078125\n",
      "Train Epoch: 190 [184576/225000 (82%)] Loss: 7255.802734\n",
      "Train Epoch: 190 [188672/225000 (84%)] Loss: 7164.595703\n",
      "Train Epoch: 190 [192768/225000 (86%)] Loss: 7156.275391\n",
      "Train Epoch: 190 [196864/225000 (87%)] Loss: 7207.919922\n",
      "Train Epoch: 190 [200960/225000 (89%)] Loss: 7241.431641\n",
      "Train Epoch: 190 [205056/225000 (91%)] Loss: 7297.525391\n",
      "Train Epoch: 190 [209152/225000 (93%)] Loss: 7107.357422\n",
      "Train Epoch: 190 [213248/225000 (95%)] Loss: 7148.259766\n",
      "Train Epoch: 190 [217344/225000 (97%)] Loss: 7099.603516\n",
      "Train Epoch: 190 [221440/225000 (98%)] Loss: 7184.033203\n",
      "    epoch          : 190\n",
      "    loss           : 7208.3367618565135\n",
      "    val_loss       : 7203.436597954254\n",
      "Train Epoch: 191 [256/225000 (0%)] Loss: 7262.083984\n",
      "Train Epoch: 191 [4352/225000 (2%)] Loss: 7332.437500\n",
      "Train Epoch: 191 [8448/225000 (4%)] Loss: 7212.074219\n",
      "Train Epoch: 191 [12544/225000 (6%)] Loss: 7009.029297\n",
      "Train Epoch: 191 [16640/225000 (7%)] Loss: 7304.544922\n",
      "Train Epoch: 191 [20736/225000 (9%)] Loss: 7259.259766\n",
      "Train Epoch: 191 [24832/225000 (11%)] Loss: 7217.070312\n",
      "Train Epoch: 191 [28928/225000 (13%)] Loss: 7441.126953\n",
      "Train Epoch: 191 [33024/225000 (15%)] Loss: 7119.505859\n",
      "Train Epoch: 191 [37120/225000 (16%)] Loss: 7301.640625\n",
      "Train Epoch: 191 [41216/225000 (18%)] Loss: 7044.140625\n",
      "Train Epoch: 191 [45312/225000 (20%)] Loss: 7313.226562\n",
      "Train Epoch: 191 [49408/225000 (22%)] Loss: 7192.968750\n",
      "Train Epoch: 191 [53504/225000 (24%)] Loss: 7182.958984\n",
      "Train Epoch: 191 [57600/225000 (26%)] Loss: 7219.828125\n",
      "Train Epoch: 191 [61696/225000 (27%)] Loss: 7263.294922\n",
      "Train Epoch: 191 [65792/225000 (29%)] Loss: 7138.089844\n",
      "Train Epoch: 191 [69888/225000 (31%)] Loss: 7231.931641\n",
      "Train Epoch: 191 [73984/225000 (33%)] Loss: 7332.687500\n",
      "Train Epoch: 191 [78080/225000 (35%)] Loss: 7191.777344\n",
      "Train Epoch: 191 [82176/225000 (37%)] Loss: 7264.003906\n",
      "Train Epoch: 191 [86272/225000 (38%)] Loss: 7428.898438\n",
      "Train Epoch: 191 [90368/225000 (40%)] Loss: 7127.324219\n",
      "Train Epoch: 191 [94464/225000 (42%)] Loss: 7329.259766\n",
      "Train Epoch: 191 [98560/225000 (44%)] Loss: 7257.935547\n",
      "Train Epoch: 191 [102656/225000 (46%)] Loss: 7121.279297\n",
      "Train Epoch: 191 [106752/225000 (47%)] Loss: 7223.734375\n",
      "Train Epoch: 191 [110848/225000 (49%)] Loss: 7100.781250\n",
      "Train Epoch: 191 [114944/225000 (51%)] Loss: 7363.244141\n",
      "Train Epoch: 191 [119040/225000 (53%)] Loss: 7157.453125\n",
      "Train Epoch: 191 [123136/225000 (55%)] Loss: 7121.492188\n",
      "Train Epoch: 191 [127232/225000 (57%)] Loss: 7154.765625\n",
      "Train Epoch: 191 [131328/225000 (58%)] Loss: 7339.164062\n",
      "Train Epoch: 191 [135424/225000 (60%)] Loss: 7016.433594\n",
      "Train Epoch: 191 [139520/225000 (62%)] Loss: 7262.984375\n",
      "Train Epoch: 191 [143616/225000 (64%)] Loss: 7220.632812\n",
      "Train Epoch: 191 [147712/225000 (66%)] Loss: 7178.853516\n",
      "Train Epoch: 191 [151808/225000 (67%)] Loss: 6981.691406\n",
      "Train Epoch: 191 [155904/225000 (69%)] Loss: 7390.218750\n",
      "Train Epoch: 191 [160000/225000 (71%)] Loss: 7049.697266\n",
      "Train Epoch: 191 [164096/225000 (73%)] Loss: 7242.691406\n",
      "Train Epoch: 191 [168192/225000 (75%)] Loss: 7204.171875\n",
      "Train Epoch: 191 [172288/225000 (77%)] Loss: 7164.251953\n",
      "Train Epoch: 191 [176384/225000 (78%)] Loss: 7269.810547\n",
      "Train Epoch: 191 [180480/225000 (80%)] Loss: 7213.562500\n",
      "Train Epoch: 191 [184576/225000 (82%)] Loss: 7324.501953\n",
      "Train Epoch: 191 [188672/225000 (84%)] Loss: 7033.939453\n",
      "Train Epoch: 191 [192768/225000 (86%)] Loss: 7274.812500\n",
      "Train Epoch: 191 [196864/225000 (87%)] Loss: 7225.443359\n",
      "Train Epoch: 191 [200960/225000 (89%)] Loss: 7253.326172\n",
      "Train Epoch: 191 [205056/225000 (91%)] Loss: 7186.115234\n",
      "Train Epoch: 191 [209152/225000 (93%)] Loss: 7260.750000\n",
      "Train Epoch: 191 [213248/225000 (95%)] Loss: 6862.578125\n",
      "Train Epoch: 191 [217344/225000 (97%)] Loss: 7314.490234\n",
      "Train Epoch: 191 [221440/225000 (98%)] Loss: 7410.542969\n",
      "    epoch          : 191\n",
      "    loss           : 7233.04958693295\n",
      "    val_loss       : 7199.455831316052\n",
      "Train Epoch: 192 [256/225000 (0%)] Loss: 7268.599609\n",
      "Train Epoch: 192 [4352/225000 (2%)] Loss: 7269.767578\n",
      "Train Epoch: 192 [8448/225000 (4%)] Loss: 7083.052734\n",
      "Train Epoch: 192 [12544/225000 (6%)] Loss: 7429.515625\n",
      "Train Epoch: 192 [16640/225000 (7%)] Loss: 7162.587891\n",
      "Train Epoch: 192 [20736/225000 (9%)] Loss: 7199.269531\n",
      "Train Epoch: 192 [24832/225000 (11%)] Loss: 7315.779297\n",
      "Train Epoch: 192 [28928/225000 (13%)] Loss: 7230.501953\n",
      "Train Epoch: 192 [33024/225000 (15%)] Loss: 7158.212891\n",
      "Train Epoch: 192 [37120/225000 (16%)] Loss: 7268.767578\n",
      "Train Epoch: 192 [41216/225000 (18%)] Loss: 7235.162109\n",
      "Train Epoch: 192 [45312/225000 (20%)] Loss: 7209.464844\n",
      "Train Epoch: 192 [49408/225000 (22%)] Loss: 7351.099609\n",
      "Train Epoch: 192 [53504/225000 (24%)] Loss: 7007.998047\n",
      "Train Epoch: 192 [57600/225000 (26%)] Loss: 7312.699219\n",
      "Train Epoch: 192 [61696/225000 (27%)] Loss: 7316.558594\n",
      "Train Epoch: 192 [65792/225000 (29%)] Loss: 7087.505859\n",
      "Train Epoch: 192 [69888/225000 (31%)] Loss: 7226.962891\n",
      "Train Epoch: 192 [73984/225000 (33%)] Loss: 7191.910156\n",
      "Train Epoch: 192 [78080/225000 (35%)] Loss: 7219.310547\n",
      "Train Epoch: 192 [82176/225000 (37%)] Loss: 7128.375000\n",
      "Train Epoch: 192 [86272/225000 (38%)] Loss: 7316.837891\n",
      "Train Epoch: 192 [90368/225000 (40%)] Loss: 7283.058594\n",
      "Train Epoch: 192 [94464/225000 (42%)] Loss: 7311.005859\n",
      "Train Epoch: 192 [98560/225000 (44%)] Loss: 7216.822266\n",
      "Train Epoch: 192 [102656/225000 (46%)] Loss: 7128.367188\n",
      "Train Epoch: 192 [106752/225000 (47%)] Loss: 7200.148438\n",
      "Train Epoch: 192 [110848/225000 (49%)] Loss: 7435.052734\n",
      "Train Epoch: 192 [114944/225000 (51%)] Loss: 7236.011719\n",
      "Train Epoch: 192 [119040/225000 (53%)] Loss: 7283.324219\n",
      "Train Epoch: 192 [123136/225000 (55%)] Loss: 7194.158203\n",
      "Train Epoch: 192 [127232/225000 (57%)] Loss: 7150.375000\n",
      "Train Epoch: 192 [131328/225000 (58%)] Loss: 7209.914062\n",
      "Train Epoch: 192 [135424/225000 (60%)] Loss: 7301.339844\n",
      "Train Epoch: 192 [139520/225000 (62%)] Loss: 7132.882812\n",
      "Train Epoch: 192 [143616/225000 (64%)] Loss: 7174.753906\n",
      "Train Epoch: 192 [147712/225000 (66%)] Loss: 7163.076172\n",
      "Train Epoch: 192 [151808/225000 (67%)] Loss: 7116.367188\n",
      "Train Epoch: 192 [155904/225000 (69%)] Loss: 7428.666016\n",
      "Train Epoch: 192 [160000/225000 (71%)] Loss: 7291.123047\n",
      "Train Epoch: 192 [164096/225000 (73%)] Loss: 7226.957031\n",
      "Train Epoch: 192 [168192/225000 (75%)] Loss: 7334.027344\n",
      "Train Epoch: 192 [172288/225000 (77%)] Loss: 7117.320312\n",
      "Train Epoch: 192 [176384/225000 (78%)] Loss: 7089.996094\n",
      "Train Epoch: 192 [180480/225000 (80%)] Loss: 7295.585938\n",
      "Train Epoch: 192 [184576/225000 (82%)] Loss: 7171.109375\n",
      "Train Epoch: 192 [188672/225000 (84%)] Loss: 7206.402344\n",
      "Train Epoch: 192 [192768/225000 (86%)] Loss: 7124.777344\n",
      "Train Epoch: 192 [196864/225000 (87%)] Loss: 7164.892578\n",
      "Train Epoch: 192 [200960/225000 (89%)] Loss: 7252.263672\n",
      "Train Epoch: 192 [205056/225000 (91%)] Loss: 7087.998047\n",
      "Train Epoch: 192 [209152/225000 (93%)] Loss: 7448.250000\n",
      "Train Epoch: 192 [213248/225000 (95%)] Loss: 7053.255859\n",
      "Train Epoch: 192 [217344/225000 (97%)] Loss: 7207.203125\n",
      "Train Epoch: 192 [221440/225000 (98%)] Loss: 7232.777344\n",
      "    epoch          : 192\n",
      "    loss           : 7226.998215745876\n",
      "    val_loss       : 7418.317347558177\n",
      "Train Epoch: 193 [256/225000 (0%)] Loss: 7190.458984\n",
      "Train Epoch: 193 [4352/225000 (2%)] Loss: 7294.832031\n",
      "Train Epoch: 193 [8448/225000 (4%)] Loss: 7216.958984\n",
      "Train Epoch: 193 [12544/225000 (6%)] Loss: 7142.156250\n",
      "Train Epoch: 193 [16640/225000 (7%)] Loss: 7134.224609\n",
      "Train Epoch: 193 [20736/225000 (9%)] Loss: 7015.591797\n",
      "Train Epoch: 193 [24832/225000 (11%)] Loss: 7207.841797\n",
      "Train Epoch: 193 [28928/225000 (13%)] Loss: 7250.533203\n",
      "Train Epoch: 193 [33024/225000 (15%)] Loss: 7186.593750\n",
      "Train Epoch: 193 [37120/225000 (16%)] Loss: 7300.865234\n",
      "Train Epoch: 193 [41216/225000 (18%)] Loss: 7077.175781\n",
      "Train Epoch: 193 [45312/225000 (20%)] Loss: 7210.107422\n",
      "Train Epoch: 193 [49408/225000 (22%)] Loss: 7255.222656\n",
      "Train Epoch: 193 [53504/225000 (24%)] Loss: 7257.039062\n",
      "Train Epoch: 193 [57600/225000 (26%)] Loss: 7072.248047\n",
      "Train Epoch: 193 [61696/225000 (27%)] Loss: 7018.291016\n",
      "Train Epoch: 193 [65792/225000 (29%)] Loss: 7159.292969\n",
      "Train Epoch: 193 [69888/225000 (31%)] Loss: 7065.716797\n",
      "Train Epoch: 193 [73984/225000 (33%)] Loss: 7112.882812\n",
      "Train Epoch: 193 [78080/225000 (35%)] Loss: 7077.888672\n",
      "Train Epoch: 193 [82176/225000 (37%)] Loss: 7451.402344\n",
      "Train Epoch: 193 [86272/225000 (38%)] Loss: 7238.167969\n",
      "Train Epoch: 193 [90368/225000 (40%)] Loss: 7144.255859\n",
      "Train Epoch: 193 [94464/225000 (42%)] Loss: 7251.488281\n",
      "Train Epoch: 193 [98560/225000 (44%)] Loss: 7080.886719\n",
      "Train Epoch: 193 [102656/225000 (46%)] Loss: 7214.267578\n",
      "Train Epoch: 193 [106752/225000 (47%)] Loss: 7394.031250\n",
      "Train Epoch: 193 [110848/225000 (49%)] Loss: 7172.664062\n",
      "Train Epoch: 193 [114944/225000 (51%)] Loss: 7205.636719\n",
      "Train Epoch: 193 [119040/225000 (53%)] Loss: 7088.468750\n",
      "Train Epoch: 193 [123136/225000 (55%)] Loss: 7227.105469\n",
      "Train Epoch: 193 [127232/225000 (57%)] Loss: 7112.982422\n",
      "Train Epoch: 193 [131328/225000 (58%)] Loss: 7255.142578\n",
      "Train Epoch: 193 [135424/225000 (60%)] Loss: 7149.953125\n",
      "Train Epoch: 193 [139520/225000 (62%)] Loss: 7363.486328\n",
      "Train Epoch: 193 [143616/225000 (64%)] Loss: 7135.769531\n",
      "Train Epoch: 193 [147712/225000 (66%)] Loss: 7195.101562\n",
      "Train Epoch: 193 [151808/225000 (67%)] Loss: 7401.953125\n",
      "Train Epoch: 193 [155904/225000 (69%)] Loss: 7134.917969\n",
      "Train Epoch: 193 [160000/225000 (71%)] Loss: 7069.875000\n",
      "Train Epoch: 193 [164096/225000 (73%)] Loss: 7038.544922\n",
      "Train Epoch: 193 [168192/225000 (75%)] Loss: 7300.064453\n",
      "Train Epoch: 193 [172288/225000 (77%)] Loss: 7196.867188\n",
      "Train Epoch: 193 [176384/225000 (78%)] Loss: 7126.093750\n",
      "Train Epoch: 193 [180480/225000 (80%)] Loss: 7188.408203\n",
      "Train Epoch: 193 [184576/225000 (82%)] Loss: 7512.476562\n",
      "Train Epoch: 193 [188672/225000 (84%)] Loss: 7251.960938\n",
      "Train Epoch: 193 [192768/225000 (86%)] Loss: 7159.220703\n",
      "Train Epoch: 193 [196864/225000 (87%)] Loss: 7165.605469\n",
      "Train Epoch: 193 [200960/225000 (89%)] Loss: 7232.777344\n",
      "Train Epoch: 193 [205056/225000 (91%)] Loss: 7320.519531\n",
      "Train Epoch: 193 [209152/225000 (93%)] Loss: 7418.777344\n",
      "Train Epoch: 193 [213248/225000 (95%)] Loss: 7096.880859\n",
      "Train Epoch: 193 [217344/225000 (97%)] Loss: 7231.970703\n",
      "Train Epoch: 193 [221440/225000 (98%)] Loss: 7151.300781\n",
      "    epoch          : 193\n",
      "    loss           : 7205.250257750285\n",
      "    val_loss       : 7296.503096584154\n",
      "Train Epoch: 194 [256/225000 (0%)] Loss: 7180.343750\n",
      "Train Epoch: 194 [4352/225000 (2%)] Loss: 7159.085938\n",
      "Train Epoch: 194 [8448/225000 (4%)] Loss: 6950.578125\n",
      "Train Epoch: 194 [12544/225000 (6%)] Loss: 7121.259766\n",
      "Train Epoch: 194 [16640/225000 (7%)] Loss: 7275.394531\n",
      "Train Epoch: 194 [20736/225000 (9%)] Loss: 7069.009766\n",
      "Train Epoch: 194 [24832/225000 (11%)] Loss: 7215.312500\n",
      "Train Epoch: 194 [28928/225000 (13%)] Loss: 7149.402344\n",
      "Train Epoch: 194 [33024/225000 (15%)] Loss: 7102.978516\n",
      "Train Epoch: 194 [37120/225000 (16%)] Loss: 7193.087891\n",
      "Train Epoch: 194 [41216/225000 (18%)] Loss: 7330.828125\n",
      "Train Epoch: 194 [45312/225000 (20%)] Loss: 7172.847656\n",
      "Train Epoch: 194 [49408/225000 (22%)] Loss: 7183.632812\n",
      "Train Epoch: 194 [53504/225000 (24%)] Loss: 7212.691406\n",
      "Train Epoch: 194 [57600/225000 (26%)] Loss: 7138.832031\n",
      "Train Epoch: 194 [61696/225000 (27%)] Loss: 7233.880859\n",
      "Train Epoch: 194 [65792/225000 (29%)] Loss: 7234.044922\n",
      "Train Epoch: 194 [69888/225000 (31%)] Loss: 7211.779297\n",
      "Train Epoch: 194 [73984/225000 (33%)] Loss: 7050.320312\n",
      "Train Epoch: 194 [78080/225000 (35%)] Loss: 7022.513672\n",
      "Train Epoch: 194 [82176/225000 (37%)] Loss: 7209.994141\n",
      "Train Epoch: 194 [86272/225000 (38%)] Loss: 7371.523438\n",
      "Train Epoch: 194 [90368/225000 (40%)] Loss: 7223.070312\n",
      "Train Epoch: 194 [94464/225000 (42%)] Loss: 7057.908203\n",
      "Train Epoch: 194 [98560/225000 (44%)] Loss: 7083.611328\n",
      "Train Epoch: 194 [102656/225000 (46%)] Loss: 7361.072266\n",
      "Train Epoch: 194 [106752/225000 (47%)] Loss: 7259.093750\n",
      "Train Epoch: 194 [110848/225000 (49%)] Loss: 7270.158203\n",
      "Train Epoch: 194 [114944/225000 (51%)] Loss: 7322.349609\n",
      "Train Epoch: 194 [119040/225000 (53%)] Loss: 7097.548828\n",
      "Train Epoch: 194 [123136/225000 (55%)] Loss: 7264.744141\n",
      "Train Epoch: 194 [127232/225000 (57%)] Loss: 7296.554688\n",
      "Train Epoch: 194 [131328/225000 (58%)] Loss: 7143.568359\n",
      "Train Epoch: 194 [135424/225000 (60%)] Loss: 7130.275391\n",
      "Train Epoch: 194 [139520/225000 (62%)] Loss: 7234.687500\n",
      "Train Epoch: 194 [143616/225000 (64%)] Loss: 7297.162109\n",
      "Train Epoch: 194 [147712/225000 (66%)] Loss: 7165.160156\n",
      "Train Epoch: 194 [151808/225000 (67%)] Loss: 7297.449219\n",
      "Train Epoch: 194 [155904/225000 (69%)] Loss: 7210.029297\n",
      "Train Epoch: 194 [160000/225000 (71%)] Loss: 7284.421875\n",
      "Train Epoch: 194 [164096/225000 (73%)] Loss: 7279.056641\n",
      "Train Epoch: 194 [168192/225000 (75%)] Loss: 7236.808594\n",
      "Train Epoch: 194 [172288/225000 (77%)] Loss: 7230.863281\n",
      "Train Epoch: 194 [176384/225000 (78%)] Loss: 7194.074219\n",
      "Train Epoch: 194 [180480/225000 (80%)] Loss: 7058.875000\n",
      "Train Epoch: 194 [184576/225000 (82%)] Loss: 7148.667969\n",
      "Train Epoch: 194 [188672/225000 (84%)] Loss: 7124.556641\n",
      "Train Epoch: 194 [192768/225000 (86%)] Loss: 7088.132812\n",
      "Train Epoch: 194 [196864/225000 (87%)] Loss: 7232.712891\n",
      "Train Epoch: 194 [200960/225000 (89%)] Loss: 7275.291016\n",
      "Train Epoch: 194 [205056/225000 (91%)] Loss: 7135.021484\n",
      "Train Epoch: 194 [209152/225000 (93%)] Loss: 7129.074219\n",
      "Train Epoch: 194 [213248/225000 (95%)] Loss: 7121.246094\n",
      "Train Epoch: 194 [217344/225000 (97%)] Loss: 7192.119141\n",
      "Train Epoch: 194 [221440/225000 (98%)] Loss: 7107.128906\n",
      "    epoch          : 194\n",
      "    loss           : 7196.095696459044\n",
      "    val_loss       : 7191.783621053306\n",
      "Train Epoch: 195 [256/225000 (0%)] Loss: 7172.802734\n",
      "Train Epoch: 195 [4352/225000 (2%)] Loss: 7171.572266\n",
      "Train Epoch: 195 [8448/225000 (4%)] Loss: 7263.761719\n",
      "Train Epoch: 195 [12544/225000 (6%)] Loss: 7385.021484\n",
      "Train Epoch: 195 [16640/225000 (7%)] Loss: 7216.029297\n",
      "Train Epoch: 195 [20736/225000 (9%)] Loss: 7129.835938\n",
      "Train Epoch: 195 [24832/225000 (11%)] Loss: 7161.919922\n",
      "Train Epoch: 195 [28928/225000 (13%)] Loss: 7442.074219\n",
      "Train Epoch: 195 [33024/225000 (15%)] Loss: 7122.673828\n",
      "Train Epoch: 195 [37120/225000 (16%)] Loss: 7087.878906\n",
      "Train Epoch: 195 [41216/225000 (18%)] Loss: 7232.353516\n",
      "Train Epoch: 195 [45312/225000 (20%)] Loss: 7114.462891\n",
      "Train Epoch: 195 [49408/225000 (22%)] Loss: 7186.992188\n",
      "Train Epoch: 195 [53504/225000 (24%)] Loss: 7195.867188\n",
      "Train Epoch: 195 [57600/225000 (26%)] Loss: 7189.773438\n",
      "Train Epoch: 195 [61696/225000 (27%)] Loss: 7319.912109\n",
      "Train Epoch: 195 [65792/225000 (29%)] Loss: 7038.236328\n",
      "Train Epoch: 195 [69888/225000 (31%)] Loss: 7322.474609\n",
      "Train Epoch: 195 [73984/225000 (33%)] Loss: 7178.580078\n",
      "Train Epoch: 195 [78080/225000 (35%)] Loss: 7210.115234\n",
      "Train Epoch: 195 [82176/225000 (37%)] Loss: 7166.531250\n",
      "Train Epoch: 195 [86272/225000 (38%)] Loss: 7314.046875\n",
      "Train Epoch: 195 [90368/225000 (40%)] Loss: 7140.234375\n",
      "Train Epoch: 195 [94464/225000 (42%)] Loss: 7141.675781\n",
      "Train Epoch: 195 [98560/225000 (44%)] Loss: 6941.037109\n",
      "Train Epoch: 195 [102656/225000 (46%)] Loss: 7348.611328\n",
      "Train Epoch: 195 [106752/225000 (47%)] Loss: 7255.265625\n",
      "Train Epoch: 195 [110848/225000 (49%)] Loss: 7268.097656\n",
      "Train Epoch: 195 [114944/225000 (51%)] Loss: 7071.982422\n",
      "Train Epoch: 195 [119040/225000 (53%)] Loss: 7213.765625\n",
      "Train Epoch: 195 [123136/225000 (55%)] Loss: 7336.867188\n",
      "Train Epoch: 195 [127232/225000 (57%)] Loss: 7075.068359\n",
      "Train Epoch: 195 [131328/225000 (58%)] Loss: 7172.111328\n",
      "Train Epoch: 195 [135424/225000 (60%)] Loss: 7012.470703\n",
      "Train Epoch: 195 [139520/225000 (62%)] Loss: 7109.830078\n",
      "Train Epoch: 195 [143616/225000 (64%)] Loss: 7192.568359\n",
      "Train Epoch: 195 [147712/225000 (66%)] Loss: 7204.019531\n",
      "Train Epoch: 195 [151808/225000 (67%)] Loss: 7175.373047\n",
      "Train Epoch: 195 [155904/225000 (69%)] Loss: 7189.507812\n",
      "Train Epoch: 195 [160000/225000 (71%)] Loss: 7212.222656\n",
      "Train Epoch: 195 [164096/225000 (73%)] Loss: 7308.400391\n",
      "Train Epoch: 195 [168192/225000 (75%)] Loss: 7112.128906\n",
      "Train Epoch: 195 [172288/225000 (77%)] Loss: 7172.648438\n",
      "Train Epoch: 195 [176384/225000 (78%)] Loss: 7108.748047\n",
      "Train Epoch: 195 [180480/225000 (80%)] Loss: 7269.175781\n",
      "Train Epoch: 195 [184576/225000 (82%)] Loss: 7195.490234\n",
      "Train Epoch: 195 [188672/225000 (84%)] Loss: 7277.937500\n",
      "Train Epoch: 195 [192768/225000 (86%)] Loss: 7278.695312\n",
      "Train Epoch: 195 [196864/225000 (87%)] Loss: 7210.798828\n",
      "Train Epoch: 195 [200960/225000 (89%)] Loss: 7233.041016\n",
      "Train Epoch: 195 [205056/225000 (91%)] Loss: 7168.718750\n",
      "Train Epoch: 195 [209152/225000 (93%)] Loss: 7252.322266\n",
      "Train Epoch: 195 [213248/225000 (95%)] Loss: 7135.593750\n",
      "Train Epoch: 195 [217344/225000 (97%)] Loss: 7182.164062\n",
      "Train Epoch: 195 [221440/225000 (98%)] Loss: 6945.955078\n",
      "    epoch          : 195\n",
      "    loss           : 7259.196064641994\n",
      "    val_loss       : 7292.518957249972\n",
      "Train Epoch: 196 [256/225000 (0%)] Loss: 7329.373047\n",
      "Train Epoch: 196 [4352/225000 (2%)] Loss: 7282.908203\n",
      "Train Epoch: 196 [8448/225000 (4%)] Loss: 7268.044922\n",
      "Train Epoch: 196 [12544/225000 (6%)] Loss: 7264.025391\n",
      "Train Epoch: 196 [16640/225000 (7%)] Loss: 7144.214844\n",
      "Train Epoch: 196 [20736/225000 (9%)] Loss: 7293.503906\n",
      "Train Epoch: 196 [24832/225000 (11%)] Loss: 7168.666016\n",
      "Train Epoch: 196 [28928/225000 (13%)] Loss: 7204.564453\n",
      "Train Epoch: 196 [33024/225000 (15%)] Loss: 7297.533203\n",
      "Train Epoch: 196 [37120/225000 (16%)] Loss: 7126.041016\n",
      "Train Epoch: 196 [41216/225000 (18%)] Loss: 7184.404297\n",
      "Train Epoch: 196 [45312/225000 (20%)] Loss: 7079.330078\n",
      "Train Epoch: 196 [49408/225000 (22%)] Loss: 7230.859375\n",
      "Train Epoch: 196 [53504/225000 (24%)] Loss: 7194.460938\n",
      "Train Epoch: 196 [57600/225000 (26%)] Loss: 7135.179688\n",
      "Train Epoch: 196 [61696/225000 (27%)] Loss: 7065.919922\n",
      "Train Epoch: 196 [65792/225000 (29%)] Loss: 7220.332031\n",
      "Train Epoch: 196 [69888/225000 (31%)] Loss: 7257.708984\n",
      "Train Epoch: 196 [73984/225000 (33%)] Loss: 7214.052734\n",
      "Train Epoch: 196 [78080/225000 (35%)] Loss: 7230.031250\n",
      "Train Epoch: 196 [82176/225000 (37%)] Loss: 7054.615234\n",
      "Train Epoch: 196 [86272/225000 (38%)] Loss: 7247.884766\n",
      "Train Epoch: 196 [90368/225000 (40%)] Loss: 7116.267578\n",
      "Train Epoch: 196 [94464/225000 (42%)] Loss: 7242.021484\n",
      "Train Epoch: 196 [98560/225000 (44%)] Loss: 7260.296875\n",
      "Train Epoch: 196 [102656/225000 (46%)] Loss: 7100.558594\n",
      "Train Epoch: 196 [106752/225000 (47%)] Loss: 7216.056641\n",
      "Train Epoch: 196 [110848/225000 (49%)] Loss: 7042.105469\n",
      "Train Epoch: 196 [114944/225000 (51%)] Loss: 7139.720703\n",
      "Train Epoch: 196 [119040/225000 (53%)] Loss: 7315.091797\n",
      "Train Epoch: 196 [123136/225000 (55%)] Loss: 7255.320312\n",
      "Train Epoch: 196 [127232/225000 (57%)] Loss: 7179.222656\n",
      "Train Epoch: 196 [131328/225000 (58%)] Loss: 7228.861328\n",
      "Train Epoch: 196 [135424/225000 (60%)] Loss: 7142.626953\n",
      "Train Epoch: 196 [139520/225000 (62%)] Loss: 7207.460938\n",
      "Train Epoch: 196 [143616/225000 (64%)] Loss: 7084.710938\n",
      "Train Epoch: 196 [147712/225000 (66%)] Loss: 7275.232422\n",
      "Train Epoch: 196 [151808/225000 (67%)] Loss: 7134.207031\n",
      "Train Epoch: 196 [155904/225000 (69%)] Loss: 7175.457031\n",
      "Train Epoch: 196 [160000/225000 (71%)] Loss: 7301.740234\n",
      "Train Epoch: 196 [164096/225000 (73%)] Loss: 7224.597656\n",
      "Train Epoch: 196 [168192/225000 (75%)] Loss: 7114.884766\n",
      "Train Epoch: 196 [172288/225000 (77%)] Loss: 7207.773438\n",
      "Train Epoch: 196 [176384/225000 (78%)] Loss: 7157.701172\n",
      "Train Epoch: 196 [180480/225000 (80%)] Loss: 7135.117188\n",
      "Train Epoch: 196 [184576/225000 (82%)] Loss: 7276.755859\n",
      "Train Epoch: 196 [188672/225000 (84%)] Loss: 7209.994141\n",
      "Train Epoch: 196 [192768/225000 (86%)] Loss: 7260.232422\n",
      "Train Epoch: 196 [196864/225000 (87%)] Loss: 7257.009766\n",
      "Train Epoch: 196 [200960/225000 (89%)] Loss: 7369.857422\n",
      "Train Epoch: 196 [205056/225000 (91%)] Loss: 7079.593750\n",
      "Train Epoch: 196 [209152/225000 (93%)] Loss: 7266.632812\n",
      "Train Epoch: 196 [213248/225000 (95%)] Loss: 7234.033203\n",
      "Train Epoch: 196 [217344/225000 (97%)] Loss: 7188.302734\n",
      "Train Epoch: 196 [221440/225000 (98%)] Loss: 7198.837891\n",
      "    epoch          : 196\n",
      "    loss           : 7215.404676834471\n",
      "    val_loss       : 7310.118564162935\n",
      "Train Epoch: 197 [256/225000 (0%)] Loss: 7123.636719\n",
      "Train Epoch: 197 [4352/225000 (2%)] Loss: 7103.837891\n",
      "Train Epoch: 197 [8448/225000 (4%)] Loss: 7291.326172\n",
      "Train Epoch: 197 [12544/225000 (6%)] Loss: 7159.449219\n",
      "Train Epoch: 197 [16640/225000 (7%)] Loss: 7190.517578\n",
      "Train Epoch: 197 [20736/225000 (9%)] Loss: 7262.046875\n",
      "Train Epoch: 197 [24832/225000 (11%)] Loss: 7117.345703\n",
      "Train Epoch: 197 [28928/225000 (13%)] Loss: 7191.089844\n",
      "Train Epoch: 197 [33024/225000 (15%)] Loss: 7254.892578\n",
      "Train Epoch: 197 [37120/225000 (16%)] Loss: 7161.740234\n",
      "Train Epoch: 197 [41216/225000 (18%)] Loss: 7152.822266\n",
      "Train Epoch: 197 [45312/225000 (20%)] Loss: 16395.351562\n",
      "Train Epoch: 197 [49408/225000 (22%)] Loss: 7186.103516\n",
      "Train Epoch: 197 [53504/225000 (24%)] Loss: 7237.347656\n",
      "Train Epoch: 197 [57600/225000 (26%)] Loss: 7091.382812\n",
      "Train Epoch: 197 [61696/225000 (27%)] Loss: 7231.197266\n",
      "Train Epoch: 197 [65792/225000 (29%)] Loss: 7188.304688\n",
      "Train Epoch: 197 [69888/225000 (31%)] Loss: 7196.150391\n",
      "Train Epoch: 197 [73984/225000 (33%)] Loss: 7201.832031\n",
      "Train Epoch: 197 [78080/225000 (35%)] Loss: 7347.839844\n",
      "Train Epoch: 197 [82176/225000 (37%)] Loss: 7220.736328\n",
      "Train Epoch: 197 [86272/225000 (38%)] Loss: 7090.166016\n",
      "Train Epoch: 197 [90368/225000 (40%)] Loss: 7156.470703\n",
      "Train Epoch: 197 [94464/225000 (42%)] Loss: 7242.298828\n",
      "Train Epoch: 197 [98560/225000 (44%)] Loss: 7248.546875\n",
      "Train Epoch: 197 [102656/225000 (46%)] Loss: 7056.775391\n",
      "Train Epoch: 197 [106752/225000 (47%)] Loss: 7241.482422\n",
      "Train Epoch: 197 [110848/225000 (49%)] Loss: 7011.330078\n",
      "Train Epoch: 197 [114944/225000 (51%)] Loss: 7099.019531\n",
      "Train Epoch: 197 [119040/225000 (53%)] Loss: 7168.535156\n",
      "Train Epoch: 197 [123136/225000 (55%)] Loss: 7188.281250\n",
      "Train Epoch: 197 [127232/225000 (57%)] Loss: 7204.720703\n",
      "Train Epoch: 197 [131328/225000 (58%)] Loss: 7126.302734\n",
      "Train Epoch: 197 [135424/225000 (60%)] Loss: 7140.539062\n",
      "Train Epoch: 197 [139520/225000 (62%)] Loss: 7179.242188\n",
      "Train Epoch: 197 [143616/225000 (64%)] Loss: 7281.158203\n",
      "Train Epoch: 197 [147712/225000 (66%)] Loss: 7115.552734\n",
      "Train Epoch: 197 [151808/225000 (67%)] Loss: 7273.037109\n",
      "Train Epoch: 197 [155904/225000 (69%)] Loss: 7159.679688\n",
      "Train Epoch: 197 [160000/225000 (71%)] Loss: 7471.755859\n",
      "Train Epoch: 197 [164096/225000 (73%)] Loss: 7143.933594\n",
      "Train Epoch: 197 [168192/225000 (75%)] Loss: 7151.292969\n",
      "Train Epoch: 197 [172288/225000 (77%)] Loss: 7161.929688\n",
      "Train Epoch: 197 [176384/225000 (78%)] Loss: 7186.226562\n",
      "Train Epoch: 197 [180480/225000 (80%)] Loss: 7140.636719\n",
      "Train Epoch: 197 [184576/225000 (82%)] Loss: 7202.896484\n",
      "Train Epoch: 197 [188672/225000 (84%)] Loss: 7138.812500\n",
      "Train Epoch: 197 [192768/225000 (86%)] Loss: 7266.125000\n",
      "Train Epoch: 197 [196864/225000 (87%)] Loss: 7182.601562\n",
      "Train Epoch: 197 [200960/225000 (89%)] Loss: 7062.509766\n",
      "Train Epoch: 197 [205056/225000 (91%)] Loss: 7131.865234\n",
      "Train Epoch: 197 [209152/225000 (93%)] Loss: 7473.708984\n",
      "Train Epoch: 197 [213248/225000 (95%)] Loss: 7146.332031\n",
      "Train Epoch: 197 [217344/225000 (97%)] Loss: 7150.544922\n",
      "Train Epoch: 197 [221440/225000 (98%)] Loss: 7150.496094\n",
      "    epoch          : 197\n",
      "    loss           : 7196.45050639043\n",
      "    val_loss       : 7187.0791716526965\n",
      "Train Epoch: 198 [256/225000 (0%)] Loss: 7381.828125\n",
      "Train Epoch: 198 [4352/225000 (2%)] Loss: 7187.884766\n",
      "Train Epoch: 198 [8448/225000 (4%)] Loss: 7224.402344\n",
      "Train Epoch: 198 [12544/225000 (6%)] Loss: 7172.955078\n",
      "Train Epoch: 198 [16640/225000 (7%)] Loss: 7245.828125\n",
      "Train Epoch: 198 [20736/225000 (9%)] Loss: 7391.591797\n",
      "Train Epoch: 198 [24832/225000 (11%)] Loss: 7091.148438\n",
      "Train Epoch: 198 [28928/225000 (13%)] Loss: 7246.742188\n",
      "Train Epoch: 198 [33024/225000 (15%)] Loss: 6962.773438\n",
      "Train Epoch: 198 [37120/225000 (16%)] Loss: 7119.109375\n",
      "Train Epoch: 198 [41216/225000 (18%)] Loss: 7268.355469\n",
      "Train Epoch: 198 [45312/225000 (20%)] Loss: 7089.425781\n",
      "Train Epoch: 198 [49408/225000 (22%)] Loss: 7035.859375\n",
      "Train Epoch: 198 [53504/225000 (24%)] Loss: 7177.636719\n",
      "Train Epoch: 198 [57600/225000 (26%)] Loss: 7142.849609\n",
      "Train Epoch: 198 [61696/225000 (27%)] Loss: 7324.240234\n",
      "Train Epoch: 198 [65792/225000 (29%)] Loss: 7137.337891\n",
      "Train Epoch: 198 [69888/225000 (31%)] Loss: 7309.294922\n",
      "Train Epoch: 198 [73984/225000 (33%)] Loss: 7084.501953\n",
      "Train Epoch: 198 [78080/225000 (35%)] Loss: 7173.320312\n",
      "Train Epoch: 198 [82176/225000 (37%)] Loss: 7255.156250\n",
      "Train Epoch: 198 [86272/225000 (38%)] Loss: 7345.242188\n",
      "Train Epoch: 198 [90368/225000 (40%)] Loss: 7324.693359\n",
      "Train Epoch: 198 [94464/225000 (42%)] Loss: 7324.808594\n",
      "Train Epoch: 198 [98560/225000 (44%)] Loss: 7110.269531\n",
      "Train Epoch: 198 [102656/225000 (46%)] Loss: 7094.177734\n",
      "Train Epoch: 198 [106752/225000 (47%)] Loss: 7189.455078\n",
      "Train Epoch: 198 [110848/225000 (49%)] Loss: 7087.380859\n",
      "Train Epoch: 198 [114944/225000 (51%)] Loss: 7115.375000\n",
      "Train Epoch: 198 [119040/225000 (53%)] Loss: 7121.957031\n",
      "Train Epoch: 198 [123136/225000 (55%)] Loss: 7314.171875\n",
      "Train Epoch: 198 [127232/225000 (57%)] Loss: 7302.412109\n",
      "Train Epoch: 198 [131328/225000 (58%)] Loss: 7091.880859\n",
      "Train Epoch: 198 [135424/225000 (60%)] Loss: 7163.617188\n",
      "Train Epoch: 198 [139520/225000 (62%)] Loss: 7333.322266\n",
      "Train Epoch: 198 [143616/225000 (64%)] Loss: 7160.521484\n",
      "Train Epoch: 198 [147712/225000 (66%)] Loss: 7176.023438\n",
      "Train Epoch: 198 [151808/225000 (67%)] Loss: 7149.431641\n",
      "Train Epoch: 198 [155904/225000 (69%)] Loss: 7370.177734\n",
      "Train Epoch: 198 [160000/225000 (71%)] Loss: 6920.453125\n",
      "Train Epoch: 198 [164096/225000 (73%)] Loss: 7211.720703\n",
      "Train Epoch: 198 [168192/225000 (75%)] Loss: 7033.681641\n",
      "Train Epoch: 198 [172288/225000 (77%)] Loss: 7223.521484\n",
      "Train Epoch: 198 [176384/225000 (78%)] Loss: 7230.257812\n",
      "Train Epoch: 198 [180480/225000 (80%)] Loss: 7202.775391\n",
      "Train Epoch: 198 [184576/225000 (82%)] Loss: 7098.621094\n",
      "Train Epoch: 198 [188672/225000 (84%)] Loss: 7196.835938\n",
      "Train Epoch: 198 [192768/225000 (86%)] Loss: 7289.400391\n",
      "Train Epoch: 198 [196864/225000 (87%)] Loss: 7142.519531\n",
      "Train Epoch: 198 [200960/225000 (89%)] Loss: 7197.089844\n",
      "Train Epoch: 198 [205056/225000 (91%)] Loss: 7191.156250\n",
      "Train Epoch: 198 [209152/225000 (93%)] Loss: 7182.244141\n",
      "Train Epoch: 198 [213248/225000 (95%)] Loss: 7257.171875\n",
      "Train Epoch: 198 [217344/225000 (97%)] Loss: 7104.841797\n",
      "Train Epoch: 198 [221440/225000 (98%)] Loss: 7140.796875\n",
      "    epoch          : 198\n",
      "    loss           : 7189.34770735566\n",
      "    val_loss       : 7181.274998156392\n",
      "Train Epoch: 199 [256/225000 (0%)] Loss: 7374.810547\n",
      "Train Epoch: 199 [4352/225000 (2%)] Loss: 7297.630859\n",
      "Train Epoch: 199 [8448/225000 (4%)] Loss: 7138.236328\n",
      "Train Epoch: 199 [12544/225000 (6%)] Loss: 7071.962891\n",
      "Train Epoch: 199 [16640/225000 (7%)] Loss: 7202.089844\n",
      "Train Epoch: 199 [20736/225000 (9%)] Loss: 7194.929688\n",
      "Train Epoch: 199 [24832/225000 (11%)] Loss: 7148.070312\n",
      "Train Epoch: 199 [28928/225000 (13%)] Loss: 7169.869141\n",
      "Train Epoch: 199 [33024/225000 (15%)] Loss: 7206.144531\n",
      "Train Epoch: 199 [37120/225000 (16%)] Loss: 7160.869141\n",
      "Train Epoch: 199 [41216/225000 (18%)] Loss: 7209.111328\n",
      "Train Epoch: 199 [45312/225000 (20%)] Loss: 7047.228516\n",
      "Train Epoch: 199 [49408/225000 (22%)] Loss: 7208.292969\n",
      "Train Epoch: 199 [53504/225000 (24%)] Loss: 7106.171875\n",
      "Train Epoch: 199 [57600/225000 (26%)] Loss: 7058.416016\n",
      "Train Epoch: 199 [61696/225000 (27%)] Loss: 7224.414062\n",
      "Train Epoch: 199 [65792/225000 (29%)] Loss: 7079.917969\n",
      "Train Epoch: 199 [69888/225000 (31%)] Loss: 7088.177734\n",
      "Train Epoch: 199 [73984/225000 (33%)] Loss: 7044.576172\n",
      "Train Epoch: 199 [78080/225000 (35%)] Loss: 7145.070312\n",
      "Train Epoch: 199 [82176/225000 (37%)] Loss: 7040.947266\n",
      "Train Epoch: 199 [86272/225000 (38%)] Loss: 7204.996094\n",
      "Train Epoch: 199 [90368/225000 (40%)] Loss: 7083.419922\n",
      "Train Epoch: 199 [94464/225000 (42%)] Loss: 7040.888672\n",
      "Train Epoch: 199 [98560/225000 (44%)] Loss: 7253.369141\n",
      "Train Epoch: 199 [102656/225000 (46%)] Loss: 7091.525391\n",
      "Train Epoch: 199 [106752/225000 (47%)] Loss: 7151.750000\n",
      "Train Epoch: 199 [110848/225000 (49%)] Loss: 7247.763672\n",
      "Train Epoch: 199 [114944/225000 (51%)] Loss: 7185.292969\n",
      "Train Epoch: 199 [119040/225000 (53%)] Loss: 7128.652344\n",
      "Train Epoch: 199 [123136/225000 (55%)] Loss: 7258.947266\n",
      "Train Epoch: 199 [127232/225000 (57%)] Loss: 7113.914062\n",
      "Train Epoch: 199 [131328/225000 (58%)] Loss: 7251.425781\n",
      "Train Epoch: 199 [135424/225000 (60%)] Loss: 7218.349609\n",
      "Train Epoch: 199 [139520/225000 (62%)] Loss: 7390.603516\n",
      "Train Epoch: 199 [143616/225000 (64%)] Loss: 7272.828125\n",
      "Train Epoch: 199 [147712/225000 (66%)] Loss: 7130.980469\n",
      "Train Epoch: 199 [151808/225000 (67%)] Loss: 7315.279297\n",
      "Train Epoch: 199 [155904/225000 (69%)] Loss: 7242.181641\n",
      "Train Epoch: 199 [160000/225000 (71%)] Loss: 7253.910156\n",
      "Train Epoch: 199 [164096/225000 (73%)] Loss: 7121.130859\n",
      "Train Epoch: 199 [168192/225000 (75%)] Loss: 7336.371094\n",
      "Train Epoch: 199 [172288/225000 (77%)] Loss: 7191.841797\n",
      "Train Epoch: 199 [176384/225000 (78%)] Loss: 7194.740234\n",
      "Train Epoch: 199 [180480/225000 (80%)] Loss: 7154.791016\n",
      "Train Epoch: 199 [184576/225000 (82%)] Loss: 7180.955078\n",
      "Train Epoch: 199 [188672/225000 (84%)] Loss: 7186.904297\n",
      "Train Epoch: 199 [192768/225000 (86%)] Loss: 7121.236328\n",
      "Train Epoch: 199 [196864/225000 (87%)] Loss: 7163.470703\n",
      "Train Epoch: 199 [200960/225000 (89%)] Loss: 7237.595703\n",
      "Train Epoch: 199 [205056/225000 (91%)] Loss: 7202.601562\n",
      "Train Epoch: 199 [209152/225000 (93%)] Loss: 7119.279297\n",
      "Train Epoch: 199 [213248/225000 (95%)] Loss: 7119.751953\n",
      "Train Epoch: 199 [217344/225000 (97%)] Loss: 7115.531250\n",
      "Train Epoch: 199 [221440/225000 (98%)] Loss: 7153.953125\n",
      "    epoch          : 199\n",
      "    loss           : 7226.4124682256115\n",
      "    val_loss       : 7179.36736117577\n",
      "Train Epoch: 200 [256/225000 (0%)] Loss: 7195.755859\n",
      "Train Epoch: 200 [4352/225000 (2%)] Loss: 7242.902344\n",
      "Train Epoch: 200 [8448/225000 (4%)] Loss: 7144.164062\n",
      "Train Epoch: 200 [12544/225000 (6%)] Loss: 7254.277344\n",
      "Train Epoch: 200 [16640/225000 (7%)] Loss: 7074.910156\n",
      "Train Epoch: 200 [20736/225000 (9%)] Loss: 7152.996094\n",
      "Train Epoch: 200 [24832/225000 (11%)] Loss: 7118.443359\n",
      "Train Epoch: 200 [28928/225000 (13%)] Loss: 7120.998047\n",
      "Train Epoch: 200 [33024/225000 (15%)] Loss: 7160.333984\n",
      "Train Epoch: 200 [37120/225000 (16%)] Loss: 7244.875000\n",
      "Train Epoch: 200 [41216/225000 (18%)] Loss: 7153.082031\n",
      "Train Epoch: 200 [45312/225000 (20%)] Loss: 7202.539062\n",
      "Train Epoch: 200 [49408/225000 (22%)] Loss: 7108.017578\n",
      "Train Epoch: 200 [53504/225000 (24%)] Loss: 7042.804688\n",
      "Train Epoch: 200 [57600/225000 (26%)] Loss: 7387.511719\n",
      "Train Epoch: 200 [61696/225000 (27%)] Loss: 7042.250000\n",
      "Train Epoch: 200 [65792/225000 (29%)] Loss: 7323.212891\n",
      "Train Epoch: 200 [69888/225000 (31%)] Loss: 7240.472656\n",
      "Train Epoch: 200 [73984/225000 (33%)] Loss: 7297.912109\n",
      "Train Epoch: 200 [78080/225000 (35%)] Loss: 7065.349609\n",
      "Train Epoch: 200 [82176/225000 (37%)] Loss: 7286.890625\n",
      "Train Epoch: 200 [86272/225000 (38%)] Loss: 7171.742188\n",
      "Train Epoch: 200 [90368/225000 (40%)] Loss: 7184.404297\n",
      "Train Epoch: 200 [94464/225000 (42%)] Loss: 7199.626953\n",
      "Train Epoch: 200 [98560/225000 (44%)] Loss: 7355.833984\n",
      "Train Epoch: 200 [102656/225000 (46%)] Loss: 7129.736328\n",
      "Train Epoch: 200 [106752/225000 (47%)] Loss: 7273.976562\n",
      "Train Epoch: 200 [110848/225000 (49%)] Loss: 7218.845703\n",
      "Train Epoch: 200 [114944/225000 (51%)] Loss: 7290.404297\n",
      "Train Epoch: 200 [119040/225000 (53%)] Loss: 7134.802734\n",
      "Train Epoch: 200 [123136/225000 (55%)] Loss: 7307.990234\n",
      "Train Epoch: 200 [127232/225000 (57%)] Loss: 7286.078125\n",
      "Train Epoch: 200 [131328/225000 (58%)] Loss: 7080.125000\n",
      "Train Epoch: 200 [135424/225000 (60%)] Loss: 7277.687500\n",
      "Train Epoch: 200 [139520/225000 (62%)] Loss: 7178.644531\n",
      "Train Epoch: 200 [143616/225000 (64%)] Loss: 7066.041016\n",
      "Train Epoch: 200 [147712/225000 (66%)] Loss: 7018.312500\n",
      "Train Epoch: 200 [151808/225000 (67%)] Loss: 7108.238281\n",
      "Train Epoch: 200 [155904/225000 (69%)] Loss: 7361.031250\n",
      "Train Epoch: 200 [160000/225000 (71%)] Loss: 7198.488281\n",
      "Train Epoch: 200 [164096/225000 (73%)] Loss: 7269.957031\n",
      "Train Epoch: 200 [168192/225000 (75%)] Loss: 7284.609375\n",
      "Train Epoch: 200 [172288/225000 (77%)] Loss: 7096.218750\n",
      "Train Epoch: 200 [176384/225000 (78%)] Loss: 7280.380859\n",
      "Train Epoch: 200 [180480/225000 (80%)] Loss: 7302.669922\n",
      "Train Epoch: 200 [184576/225000 (82%)] Loss: 7012.126953\n",
      "Train Epoch: 200 [188672/225000 (84%)] Loss: 7287.820312\n",
      "Train Epoch: 200 [192768/225000 (86%)] Loss: 7095.468750\n",
      "Train Epoch: 200 [196864/225000 (87%)] Loss: 7135.482422\n",
      "Train Epoch: 200 [200960/225000 (89%)] Loss: 6992.091797\n",
      "Train Epoch: 200 [205056/225000 (91%)] Loss: 7229.730469\n",
      "Train Epoch: 200 [209152/225000 (93%)] Loss: 7121.953125\n",
      "Train Epoch: 200 [213248/225000 (95%)] Loss: 7047.597656\n",
      "Train Epoch: 200 [217344/225000 (97%)] Loss: 7172.222656\n",
      "Train Epoch: 200 [221440/225000 (98%)] Loss: 7210.783203\n",
      "    epoch          : 200\n",
      "    loss           : 7220.243750666596\n",
      "    val_loss       : 7179.022457313781\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0827_225017/checkpoint-epoch200.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 201 [256/225000 (0%)] Loss: 6962.425781\n",
      "Train Epoch: 201 [4352/225000 (2%)] Loss: 7210.888672\n",
      "Train Epoch: 201 [8448/225000 (4%)] Loss: 7232.066406\n",
      "Train Epoch: 201 [12544/225000 (6%)] Loss: 7290.423828\n",
      "Train Epoch: 201 [16640/225000 (7%)] Loss: 7077.541016\n",
      "Train Epoch: 201 [20736/225000 (9%)] Loss: 7070.277344\n",
      "Train Epoch: 201 [24832/225000 (11%)] Loss: 7144.771484\n",
      "Train Epoch: 201 [28928/225000 (13%)] Loss: 7000.339844\n",
      "Train Epoch: 201 [33024/225000 (15%)] Loss: 7280.466797\n",
      "Train Epoch: 201 [37120/225000 (16%)] Loss: 6979.691406\n",
      "Train Epoch: 201 [41216/225000 (18%)] Loss: 7087.248047\n",
      "Train Epoch: 201 [45312/225000 (20%)] Loss: 7190.291016\n",
      "Train Epoch: 201 [49408/225000 (22%)] Loss: 7285.660156\n",
      "Train Epoch: 201 [53504/225000 (24%)] Loss: 7150.664062\n",
      "Train Epoch: 201 [57600/225000 (26%)] Loss: 7124.464844\n",
      "Train Epoch: 201 [61696/225000 (27%)] Loss: 6980.472656\n",
      "Train Epoch: 201 [65792/225000 (29%)] Loss: 7220.660156\n",
      "Train Epoch: 201 [69888/225000 (31%)] Loss: 7349.789062\n",
      "Train Epoch: 201 [73984/225000 (33%)] Loss: 7086.861328\n",
      "Train Epoch: 201 [78080/225000 (35%)] Loss: 7167.000000\n",
      "Train Epoch: 201 [82176/225000 (37%)] Loss: 7103.996094\n",
      "Train Epoch: 201 [86272/225000 (38%)] Loss: 7005.261719\n",
      "Train Epoch: 201 [90368/225000 (40%)] Loss: 7091.501953\n",
      "Train Epoch: 201 [94464/225000 (42%)] Loss: 7256.179688\n",
      "Train Epoch: 201 [98560/225000 (44%)] Loss: 7216.410156\n",
      "Train Epoch: 201 [102656/225000 (46%)] Loss: 7095.703125\n",
      "Train Epoch: 201 [106752/225000 (47%)] Loss: 7272.851562\n",
      "Train Epoch: 201 [110848/225000 (49%)] Loss: 7315.003906\n",
      "Train Epoch: 201 [114944/225000 (51%)] Loss: 7168.892578\n",
      "Train Epoch: 201 [119040/225000 (53%)] Loss: 7187.978516\n",
      "Train Epoch: 201 [123136/225000 (55%)] Loss: 7205.759766\n",
      "Train Epoch: 201 [127232/225000 (57%)] Loss: 7057.580078\n",
      "Train Epoch: 201 [131328/225000 (58%)] Loss: 7156.623047\n",
      "Train Epoch: 201 [135424/225000 (60%)] Loss: 7225.955078\n",
      "Train Epoch: 201 [139520/225000 (62%)] Loss: 7164.740234\n",
      "Train Epoch: 201 [143616/225000 (64%)] Loss: 7097.505859\n",
      "Train Epoch: 201 [147712/225000 (66%)] Loss: 7228.585938\n",
      "Train Epoch: 201 [151808/225000 (67%)] Loss: 6991.738281\n",
      "Train Epoch: 201 [155904/225000 (69%)] Loss: 7245.851562\n",
      "Train Epoch: 201 [160000/225000 (71%)] Loss: 7020.869141\n",
      "Train Epoch: 201 [164096/225000 (73%)] Loss: 7086.658203\n",
      "Train Epoch: 201 [168192/225000 (75%)] Loss: 7094.357422\n",
      "Train Epoch: 201 [172288/225000 (77%)] Loss: 7170.509766\n",
      "Train Epoch: 201 [176384/225000 (78%)] Loss: 7252.253906\n",
      "Train Epoch: 201 [180480/225000 (80%)] Loss: 7063.195312\n",
      "Train Epoch: 201 [184576/225000 (82%)] Loss: 7180.564453\n",
      "Train Epoch: 201 [188672/225000 (84%)] Loss: 7304.027344\n",
      "Train Epoch: 201 [192768/225000 (86%)] Loss: 7115.671875\n",
      "Train Epoch: 201 [196864/225000 (87%)] Loss: 7378.033203\n",
      "Train Epoch: 201 [200960/225000 (89%)] Loss: 7147.253906\n",
      "Train Epoch: 201 [205056/225000 (91%)] Loss: 7111.017578\n",
      "Train Epoch: 201 [209152/225000 (93%)] Loss: 7145.654297\n",
      "Train Epoch: 201 [213248/225000 (95%)] Loss: 7126.767578\n",
      "Train Epoch: 201 [217344/225000 (97%)] Loss: 7193.257812\n",
      "Train Epoch: 201 [221440/225000 (98%)] Loss: 7172.970703\n",
      "    epoch          : 201\n",
      "    loss           : 7174.16427136661\n",
      "    val_loss       : 7170.162960726388\n",
      "Train Epoch: 202 [256/225000 (0%)] Loss: 7229.718750\n",
      "Train Epoch: 202 [4352/225000 (2%)] Loss: 7275.986328\n",
      "Train Epoch: 202 [8448/225000 (4%)] Loss: 7212.937500\n",
      "Train Epoch: 202 [12544/225000 (6%)] Loss: 7247.126953\n",
      "Train Epoch: 202 [16640/225000 (7%)] Loss: 6995.097656\n",
      "Train Epoch: 202 [20736/225000 (9%)] Loss: 7433.642578\n",
      "Train Epoch: 202 [24832/225000 (11%)] Loss: 7184.498047\n",
      "Train Epoch: 202 [28928/225000 (13%)] Loss: 7158.300781\n",
      "Train Epoch: 202 [33024/225000 (15%)] Loss: 7168.083984\n",
      "Train Epoch: 202 [37120/225000 (16%)] Loss: 7323.734375\n",
      "Train Epoch: 202 [41216/225000 (18%)] Loss: 7208.220703\n",
      "Train Epoch: 202 [45312/225000 (20%)] Loss: 7047.980469\n",
      "Train Epoch: 202 [49408/225000 (22%)] Loss: 7015.433594\n",
      "Train Epoch: 202 [53504/225000 (24%)] Loss: 7197.177734\n",
      "Train Epoch: 202 [57600/225000 (26%)] Loss: 7143.839844\n",
      "Train Epoch: 202 [61696/225000 (27%)] Loss: 7100.783203\n",
      "Train Epoch: 202 [65792/225000 (29%)] Loss: 7146.964844\n",
      "Train Epoch: 202 [69888/225000 (31%)] Loss: 7117.412109\n",
      "Train Epoch: 202 [73984/225000 (33%)] Loss: 7146.953125\n",
      "Train Epoch: 202 [78080/225000 (35%)] Loss: 7119.964844\n",
      "Train Epoch: 202 [82176/225000 (37%)] Loss: 7293.957031\n",
      "Train Epoch: 202 [86272/225000 (38%)] Loss: 7253.171875\n",
      "Train Epoch: 202 [90368/225000 (40%)] Loss: 7189.849609\n",
      "Train Epoch: 202 [94464/225000 (42%)] Loss: 7154.681641\n",
      "Train Epoch: 202 [98560/225000 (44%)] Loss: 7262.623047\n",
      "Train Epoch: 202 [102656/225000 (46%)] Loss: 7271.837891\n",
      "Train Epoch: 202 [106752/225000 (47%)] Loss: 7196.650391\n",
      "Train Epoch: 202 [110848/225000 (49%)] Loss: 7094.271484\n",
      "Train Epoch: 202 [114944/225000 (51%)] Loss: 7209.533203\n",
      "Train Epoch: 202 [119040/225000 (53%)] Loss: 7202.345703\n",
      "Train Epoch: 202 [123136/225000 (55%)] Loss: 7064.496094\n",
      "Train Epoch: 202 [127232/225000 (57%)] Loss: 7203.466797\n",
      "Train Epoch: 202 [131328/225000 (58%)] Loss: 7031.636719\n",
      "Train Epoch: 202 [135424/225000 (60%)] Loss: 7128.800781\n",
      "Train Epoch: 202 [139520/225000 (62%)] Loss: 7114.994141\n",
      "Train Epoch: 202 [143616/225000 (64%)] Loss: 7088.210938\n",
      "Train Epoch: 202 [147712/225000 (66%)] Loss: 7216.029297\n",
      "Train Epoch: 202 [151808/225000 (67%)] Loss: 7177.587891\n",
      "Train Epoch: 202 [155904/225000 (69%)] Loss: 7080.900391\n",
      "Train Epoch: 202 [160000/225000 (71%)] Loss: 7194.275391\n",
      "Train Epoch: 202 [164096/225000 (73%)] Loss: 7115.333984\n",
      "Train Epoch: 202 [168192/225000 (75%)] Loss: 7103.138672\n",
      "Train Epoch: 202 [172288/225000 (77%)] Loss: 7120.103516\n",
      "Train Epoch: 202 [176384/225000 (78%)] Loss: 7066.583984\n",
      "Train Epoch: 202 [180480/225000 (80%)] Loss: 7131.181641\n",
      "Train Epoch: 202 [184576/225000 (82%)] Loss: 7184.843750\n",
      "Train Epoch: 202 [188672/225000 (84%)] Loss: 7038.375000\n",
      "Train Epoch: 202 [192768/225000 (86%)] Loss: 7012.417969\n",
      "Train Epoch: 202 [196864/225000 (87%)] Loss: 7321.568359\n",
      "Train Epoch: 202 [200960/225000 (89%)] Loss: 7053.644531\n",
      "Train Epoch: 202 [205056/225000 (91%)] Loss: 7115.962891\n",
      "Train Epoch: 202 [209152/225000 (93%)] Loss: 7502.660156\n",
      "Train Epoch: 202 [213248/225000 (95%)] Loss: 7335.746094\n",
      "Train Epoch: 202 [217344/225000 (97%)] Loss: 7158.445312\n",
      "Train Epoch: 202 [221440/225000 (98%)] Loss: 7192.046875\n",
      "    epoch          : 202\n",
      "    loss           : 7171.303405414534\n",
      "    val_loss       : 7263.873371238611\n",
      "Train Epoch: 203 [256/225000 (0%)] Loss: 7112.685547\n",
      "Train Epoch: 203 [4352/225000 (2%)] Loss: 7010.679688\n",
      "Train Epoch: 203 [8448/225000 (4%)] Loss: 7209.742188\n",
      "Train Epoch: 203 [12544/225000 (6%)] Loss: 7113.013672\n",
      "Train Epoch: 203 [16640/225000 (7%)] Loss: 7132.894531\n",
      "Train Epoch: 203 [20736/225000 (9%)] Loss: 7220.464844\n",
      "Train Epoch: 203 [24832/225000 (11%)] Loss: 7424.513672\n",
      "Train Epoch: 203 [28928/225000 (13%)] Loss: 7031.921875\n",
      "Train Epoch: 203 [33024/225000 (15%)] Loss: 7304.261719\n",
      "Train Epoch: 203 [37120/225000 (16%)] Loss: 7189.121094\n",
      "Train Epoch: 203 [41216/225000 (18%)] Loss: 7021.669922\n",
      "Train Epoch: 203 [45312/225000 (20%)] Loss: 7226.273438\n",
      "Train Epoch: 203 [49408/225000 (22%)] Loss: 7270.085938\n",
      "Train Epoch: 203 [53504/225000 (24%)] Loss: 7191.123047\n",
      "Train Epoch: 203 [57600/225000 (26%)] Loss: 7257.373047\n",
      "Train Epoch: 203 [61696/225000 (27%)] Loss: 6967.628906\n",
      "Train Epoch: 203 [65792/225000 (29%)] Loss: 7305.818359\n",
      "Train Epoch: 203 [69888/225000 (31%)] Loss: 7181.369141\n",
      "Train Epoch: 203 [73984/225000 (33%)] Loss: 7152.716797\n",
      "Train Epoch: 203 [78080/225000 (35%)] Loss: 7029.404297\n",
      "Train Epoch: 203 [82176/225000 (37%)] Loss: 7083.673828\n",
      "Train Epoch: 203 [86272/225000 (38%)] Loss: 7243.417969\n",
      "Train Epoch: 203 [90368/225000 (40%)] Loss: 7266.744141\n",
      "Train Epoch: 203 [94464/225000 (42%)] Loss: 7287.333984\n",
      "Train Epoch: 203 [98560/225000 (44%)] Loss: 7234.765625\n",
      "Train Epoch: 203 [102656/225000 (46%)] Loss: 7385.818359\n",
      "Train Epoch: 203 [106752/225000 (47%)] Loss: 7089.712891\n",
      "Train Epoch: 203 [110848/225000 (49%)] Loss: 7137.447266\n",
      "Train Epoch: 203 [114944/225000 (51%)] Loss: 7105.097656\n",
      "Train Epoch: 203 [119040/225000 (53%)] Loss: 7366.724609\n",
      "Train Epoch: 203 [123136/225000 (55%)] Loss: 7223.951172\n",
      "Train Epoch: 203 [127232/225000 (57%)] Loss: 7196.056641\n",
      "Train Epoch: 203 [131328/225000 (58%)] Loss: 7051.792969\n",
      "Train Epoch: 203 [135424/225000 (60%)] Loss: 7014.203125\n",
      "Train Epoch: 203 [139520/225000 (62%)] Loss: 7283.263672\n",
      "Train Epoch: 203 [143616/225000 (64%)] Loss: 7196.078125\n",
      "Train Epoch: 203 [147712/225000 (66%)] Loss: 7213.263672\n",
      "Train Epoch: 203 [151808/225000 (67%)] Loss: 7223.542969\n",
      "Train Epoch: 203 [155904/225000 (69%)] Loss: 7151.193359\n",
      "Train Epoch: 203 [160000/225000 (71%)] Loss: 7388.525391\n",
      "Train Epoch: 203 [164096/225000 (73%)] Loss: 7189.228516\n",
      "Train Epoch: 203 [168192/225000 (75%)] Loss: 7108.125000\n",
      "Train Epoch: 203 [172288/225000 (77%)] Loss: 7345.214844\n",
      "Train Epoch: 203 [176384/225000 (78%)] Loss: 7055.453125\n",
      "Train Epoch: 203 [180480/225000 (80%)] Loss: 6998.335938\n",
      "Train Epoch: 203 [184576/225000 (82%)] Loss: 7058.667969\n",
      "Train Epoch: 203 [188672/225000 (84%)] Loss: 7121.966797\n",
      "Train Epoch: 203 [192768/225000 (86%)] Loss: 7477.257812\n",
      "Train Epoch: 203 [196864/225000 (87%)] Loss: 7000.322266\n",
      "Train Epoch: 203 [200960/225000 (89%)] Loss: 6945.101562\n",
      "Train Epoch: 203 [205056/225000 (91%)] Loss: 7064.783203\n",
      "Train Epoch: 203 [209152/225000 (93%)] Loss: 7174.837891\n",
      "Train Epoch: 203 [213248/225000 (95%)] Loss: 7136.599609\n",
      "Train Epoch: 203 [217344/225000 (97%)] Loss: 7146.373047\n",
      "Train Epoch: 203 [221440/225000 (98%)] Loss: 7193.289062\n",
      "    epoch          : 203\n",
      "    loss           : 7168.123130199446\n",
      "    val_loss       : 7171.835332072511\n",
      "Train Epoch: 204 [256/225000 (0%)] Loss: 7127.326172\n",
      "Train Epoch: 204 [4352/225000 (2%)] Loss: 7047.605469\n",
      "Train Epoch: 204 [8448/225000 (4%)] Loss: 7045.001953\n",
      "Train Epoch: 204 [12544/225000 (6%)] Loss: 7190.427734\n",
      "Train Epoch: 204 [16640/225000 (7%)] Loss: 7234.841797\n",
      "Train Epoch: 204 [20736/225000 (9%)] Loss: 7188.345703\n",
      "Train Epoch: 204 [24832/225000 (11%)] Loss: 7015.617188\n",
      "Train Epoch: 204 [28928/225000 (13%)] Loss: 7238.623047\n",
      "Train Epoch: 204 [33024/225000 (15%)] Loss: 7148.796875\n",
      "Train Epoch: 204 [37120/225000 (16%)] Loss: 7108.791016\n",
      "Train Epoch: 204 [41216/225000 (18%)] Loss: 6998.767578\n",
      "Train Epoch: 204 [45312/225000 (20%)] Loss: 7426.351562\n",
      "Train Epoch: 204 [49408/225000 (22%)] Loss: 7192.000000\n",
      "Train Epoch: 204 [53504/225000 (24%)] Loss: 7169.646484\n",
      "Train Epoch: 204 [57600/225000 (26%)] Loss: 7082.152344\n",
      "Train Epoch: 204 [61696/225000 (27%)] Loss: 7118.150391\n",
      "Train Epoch: 204 [65792/225000 (29%)] Loss: 7205.714844\n",
      "Train Epoch: 204 [69888/225000 (31%)] Loss: 7237.603516\n",
      "Train Epoch: 204 [73984/225000 (33%)] Loss: 7109.697266\n",
      "Train Epoch: 204 [78080/225000 (35%)] Loss: 7041.330078\n",
      "Train Epoch: 204 [82176/225000 (37%)] Loss: 7228.310547\n",
      "Train Epoch: 204 [86272/225000 (38%)] Loss: 7327.429688\n",
      "Train Epoch: 204 [90368/225000 (40%)] Loss: 7390.318359\n",
      "Train Epoch: 204 [94464/225000 (42%)] Loss: 7121.941406\n",
      "Train Epoch: 204 [98560/225000 (44%)] Loss: 7178.791016\n",
      "Train Epoch: 204 [102656/225000 (46%)] Loss: 7037.544922\n",
      "Train Epoch: 204 [106752/225000 (47%)] Loss: 7267.917969\n",
      "Train Epoch: 204 [110848/225000 (49%)] Loss: 7226.113281\n",
      "Train Epoch: 204 [114944/225000 (51%)] Loss: 7119.601562\n",
      "Train Epoch: 204 [119040/225000 (53%)] Loss: 7053.039062\n",
      "Train Epoch: 204 [123136/225000 (55%)] Loss: 7210.900391\n",
      "Train Epoch: 204 [127232/225000 (57%)] Loss: 7138.003906\n",
      "Train Epoch: 204 [131328/225000 (58%)] Loss: 7073.066406\n",
      "Train Epoch: 204 [135424/225000 (60%)] Loss: 7344.835938\n",
      "Train Epoch: 204 [139520/225000 (62%)] Loss: 7306.214844\n",
      "Train Epoch: 204 [143616/225000 (64%)] Loss: 7217.085938\n",
      "Train Epoch: 204 [147712/225000 (66%)] Loss: 7120.613281\n",
      "Train Epoch: 204 [151808/225000 (67%)] Loss: 7161.273438\n",
      "Train Epoch: 204 [155904/225000 (69%)] Loss: 7293.378906\n",
      "Train Epoch: 204 [160000/225000 (71%)] Loss: 7134.875000\n",
      "Train Epoch: 204 [164096/225000 (73%)] Loss: 7127.166016\n",
      "Train Epoch: 204 [168192/225000 (75%)] Loss: 7067.955078\n",
      "Train Epoch: 204 [172288/225000 (77%)] Loss: 7252.087891\n",
      "Train Epoch: 204 [176384/225000 (78%)] Loss: 7059.453125\n",
      "Train Epoch: 204 [180480/225000 (80%)] Loss: 7245.611328\n",
      "Train Epoch: 204 [184576/225000 (82%)] Loss: 7117.871094\n",
      "Train Epoch: 204 [188672/225000 (84%)] Loss: 7290.371094\n",
      "Train Epoch: 204 [192768/225000 (86%)] Loss: 7178.830078\n",
      "Train Epoch: 204 [196864/225000 (87%)] Loss: 7093.341797\n",
      "Train Epoch: 204 [200960/225000 (89%)] Loss: 7178.324219\n",
      "Train Epoch: 204 [205056/225000 (91%)] Loss: 7176.251953\n",
      "Train Epoch: 204 [209152/225000 (93%)] Loss: 7073.615234\n",
      "Train Epoch: 204 [213248/225000 (95%)] Loss: 7119.964844\n",
      "Train Epoch: 204 [217344/225000 (97%)] Loss: 7218.751953\n",
      "Train Epoch: 204 [221440/225000 (98%)] Loss: 7103.019531\n",
      "    epoch          : 204\n",
      "    loss           : 7164.1673299292515\n",
      "    val_loss       : 7160.2993986691745\n",
      "Train Epoch: 205 [256/225000 (0%)] Loss: 7287.722656\n",
      "Train Epoch: 205 [4352/225000 (2%)] Loss: 7136.179688\n",
      "Train Epoch: 205 [8448/225000 (4%)] Loss: 7206.617188\n",
      "Train Epoch: 205 [12544/225000 (6%)] Loss: 7158.654297\n",
      "Train Epoch: 205 [16640/225000 (7%)] Loss: 7123.820312\n",
      "Train Epoch: 205 [20736/225000 (9%)] Loss: 7044.365234\n",
      "Train Epoch: 205 [24832/225000 (11%)] Loss: 7341.099609\n",
      "Train Epoch: 205 [28928/225000 (13%)] Loss: 7100.083984\n",
      "Train Epoch: 205 [33024/225000 (15%)] Loss: 7275.001953\n",
      "Train Epoch: 205 [37120/225000 (16%)] Loss: 7039.324219\n",
      "Train Epoch: 205 [41216/225000 (18%)] Loss: 7085.162109\n",
      "Train Epoch: 205 [45312/225000 (20%)] Loss: 7162.558594\n",
      "Train Epoch: 205 [49408/225000 (22%)] Loss: 7293.917969\n",
      "Train Epoch: 205 [53504/225000 (24%)] Loss: 7258.046875\n",
      "Train Epoch: 205 [57600/225000 (26%)] Loss: 7128.382812\n",
      "Train Epoch: 205 [61696/225000 (27%)] Loss: 6995.046875\n",
      "Train Epoch: 205 [65792/225000 (29%)] Loss: 7175.472656\n",
      "Train Epoch: 205 [69888/225000 (31%)] Loss: 7216.031250\n",
      "Train Epoch: 205 [73984/225000 (33%)] Loss: 7284.496094\n",
      "Train Epoch: 205 [78080/225000 (35%)] Loss: 7024.984375\n",
      "Train Epoch: 205 [82176/225000 (37%)] Loss: 7059.039062\n",
      "Train Epoch: 205 [86272/225000 (38%)] Loss: 6994.230469\n",
      "Train Epoch: 205 [90368/225000 (40%)] Loss: 7127.171875\n",
      "Train Epoch: 205 [94464/225000 (42%)] Loss: 7295.671875\n",
      "Train Epoch: 205 [98560/225000 (44%)] Loss: 7168.037109\n",
      "Train Epoch: 205 [102656/225000 (46%)] Loss: 7222.666016\n",
      "Train Epoch: 205 [106752/225000 (47%)] Loss: 7168.421875\n",
      "Train Epoch: 205 [110848/225000 (49%)] Loss: 7151.839844\n",
      "Train Epoch: 205 [114944/225000 (51%)] Loss: 7028.941406\n",
      "Train Epoch: 205 [119040/225000 (53%)] Loss: 7235.521484\n",
      "Train Epoch: 205 [123136/225000 (55%)] Loss: 7081.833984\n",
      "Train Epoch: 205 [127232/225000 (57%)] Loss: 7162.673828\n",
      "Train Epoch: 205 [131328/225000 (58%)] Loss: 7078.005859\n",
      "Train Epoch: 205 [135424/225000 (60%)] Loss: 7211.025391\n",
      "Train Epoch: 205 [139520/225000 (62%)] Loss: 7175.404297\n",
      "Train Epoch: 205 [143616/225000 (64%)] Loss: 7280.726562\n",
      "Train Epoch: 205 [147712/225000 (66%)] Loss: 7217.539062\n",
      "Train Epoch: 205 [151808/225000 (67%)] Loss: 7147.781250\n",
      "Train Epoch: 205 [155904/225000 (69%)] Loss: 7123.177734\n",
      "Train Epoch: 205 [160000/225000 (71%)] Loss: 7098.527344\n",
      "Train Epoch: 205 [164096/225000 (73%)] Loss: 7134.048828\n",
      "Train Epoch: 205 [168192/225000 (75%)] Loss: 7124.769531\n",
      "Train Epoch: 205 [172288/225000 (77%)] Loss: 7143.951172\n",
      "Train Epoch: 205 [176384/225000 (78%)] Loss: 7184.628906\n",
      "Train Epoch: 205 [180480/225000 (80%)] Loss: 7288.240234\n",
      "Train Epoch: 205 [184576/225000 (82%)] Loss: 7191.277344\n",
      "Train Epoch: 205 [188672/225000 (84%)] Loss: 7102.884766\n",
      "Train Epoch: 205 [192768/225000 (86%)] Loss: 7174.125000\n",
      "Train Epoch: 205 [196864/225000 (87%)] Loss: 7276.560547\n",
      "Train Epoch: 205 [200960/225000 (89%)] Loss: 7186.412109\n",
      "Train Epoch: 205 [205056/225000 (91%)] Loss: 7162.494141\n",
      "Train Epoch: 205 [209152/225000 (93%)] Loss: 7064.648438\n",
      "Train Epoch: 205 [213248/225000 (95%)] Loss: 7188.978516\n",
      "Train Epoch: 205 [217344/225000 (97%)] Loss: 7172.117188\n",
      "Train Epoch: 205 [221440/225000 (98%)] Loss: 7262.011719\n",
      "    epoch          : 205\n",
      "    loss           : 7162.703110557096\n",
      "    val_loss       : 7160.253253428304\n",
      "Train Epoch: 206 [256/225000 (0%)] Loss: 7190.007812\n",
      "Train Epoch: 206 [4352/225000 (2%)] Loss: 7052.566406\n",
      "Train Epoch: 206 [8448/225000 (4%)] Loss: 7139.429688\n",
      "Train Epoch: 206 [12544/225000 (6%)] Loss: 16981.189453\n",
      "Train Epoch: 206 [16640/225000 (7%)] Loss: 7192.492188\n",
      "Train Epoch: 206 [20736/225000 (9%)] Loss: 7204.650391\n",
      "Train Epoch: 206 [24832/225000 (11%)] Loss: 7309.925781\n",
      "Train Epoch: 206 [28928/225000 (13%)] Loss: 7095.582031\n",
      "Train Epoch: 206 [33024/225000 (15%)] Loss: 7286.843750\n",
      "Train Epoch: 206 [37120/225000 (16%)] Loss: 7286.451172\n",
      "Train Epoch: 206 [41216/225000 (18%)] Loss: 7169.482422\n",
      "Train Epoch: 206 [45312/225000 (20%)] Loss: 7194.373047\n",
      "Train Epoch: 206 [49408/225000 (22%)] Loss: 7117.996094\n",
      "Train Epoch: 206 [53504/225000 (24%)] Loss: 7189.505859\n",
      "Train Epoch: 206 [57600/225000 (26%)] Loss: 7070.478516\n",
      "Train Epoch: 206 [61696/225000 (27%)] Loss: 7090.441406\n",
      "Train Epoch: 206 [65792/225000 (29%)] Loss: 7065.833984\n",
      "Train Epoch: 206 [69888/225000 (31%)] Loss: 7224.060547\n",
      "Train Epoch: 206 [73984/225000 (33%)] Loss: 7179.548828\n",
      "Train Epoch: 206 [78080/225000 (35%)] Loss: 7193.281250\n",
      "Train Epoch: 206 [82176/225000 (37%)] Loss: 7241.167969\n",
      "Train Epoch: 206 [86272/225000 (38%)] Loss: 7084.605469\n",
      "Train Epoch: 206 [90368/225000 (40%)] Loss: 7004.626953\n",
      "Train Epoch: 206 [94464/225000 (42%)] Loss: 7089.994141\n",
      "Train Epoch: 206 [98560/225000 (44%)] Loss: 7144.964844\n",
      "Train Epoch: 206 [102656/225000 (46%)] Loss: 7213.238281\n",
      "Train Epoch: 206 [106752/225000 (47%)] Loss: 6935.882812\n",
      "Train Epoch: 206 [110848/225000 (49%)] Loss: 7171.673828\n",
      "Train Epoch: 206 [114944/225000 (51%)] Loss: 7059.371094\n",
      "Train Epoch: 206 [119040/225000 (53%)] Loss: 7335.332031\n",
      "Train Epoch: 206 [123136/225000 (55%)] Loss: 7071.222656\n",
      "Train Epoch: 206 [127232/225000 (57%)] Loss: 7087.921875\n",
      "Train Epoch: 206 [131328/225000 (58%)] Loss: 7154.419922\n",
      "Train Epoch: 206 [135424/225000 (60%)] Loss: 7119.664062\n",
      "Train Epoch: 206 [139520/225000 (62%)] Loss: 7138.429688\n",
      "Train Epoch: 206 [143616/225000 (64%)] Loss: 7065.748047\n",
      "Train Epoch: 206 [147712/225000 (66%)] Loss: 7146.578125\n",
      "Train Epoch: 206 [151808/225000 (67%)] Loss: 7125.750000\n",
      "Train Epoch: 206 [155904/225000 (69%)] Loss: 7166.583984\n",
      "Train Epoch: 206 [160000/225000 (71%)] Loss: 7103.199219\n",
      "Train Epoch: 206 [164096/225000 (73%)] Loss: 7085.541016\n",
      "Train Epoch: 206 [168192/225000 (75%)] Loss: 7193.708984\n",
      "Train Epoch: 206 [172288/225000 (77%)] Loss: 7209.000000\n",
      "Train Epoch: 206 [176384/225000 (78%)] Loss: 7162.542969\n",
      "Train Epoch: 206 [180480/225000 (80%)] Loss: 7109.937500\n",
      "Train Epoch: 206 [184576/225000 (82%)] Loss: 7334.181641\n",
      "Train Epoch: 206 [188672/225000 (84%)] Loss: 7115.712891\n",
      "Train Epoch: 206 [192768/225000 (86%)] Loss: 7136.947266\n",
      "Train Epoch: 206 [196864/225000 (87%)] Loss: 7261.033203\n",
      "Train Epoch: 206 [200960/225000 (89%)] Loss: 7162.087891\n",
      "Train Epoch: 206 [205056/225000 (91%)] Loss: 7278.410156\n",
      "Train Epoch: 206 [209152/225000 (93%)] Loss: 7244.197266\n",
      "Train Epoch: 206 [213248/225000 (95%)] Loss: 7126.445312\n",
      "Train Epoch: 206 [217344/225000 (97%)] Loss: 7063.283203\n",
      "Train Epoch: 206 [221440/225000 (98%)] Loss: 7118.408203\n",
      "    epoch          : 206\n",
      "    loss           : 7179.910458439988\n",
      "    val_loss       : 7161.861142601286\n",
      "Train Epoch: 207 [256/225000 (0%)] Loss: 7205.773438\n",
      "Train Epoch: 207 [4352/225000 (2%)] Loss: 7204.171875\n",
      "Train Epoch: 207 [8448/225000 (4%)] Loss: 7038.064453\n",
      "Train Epoch: 207 [12544/225000 (6%)] Loss: 7249.410156\n",
      "Train Epoch: 207 [16640/225000 (7%)] Loss: 7255.970703\n",
      "Train Epoch: 207 [20736/225000 (9%)] Loss: 7095.326172\n",
      "Train Epoch: 207 [24832/225000 (11%)] Loss: 7088.275391\n",
      "Train Epoch: 207 [28928/225000 (13%)] Loss: 7092.898438\n",
      "Train Epoch: 207 [33024/225000 (15%)] Loss: 7246.566406\n",
      "Train Epoch: 207 [37120/225000 (16%)] Loss: 7013.552734\n",
      "Train Epoch: 207 [41216/225000 (18%)] Loss: 7125.121094\n",
      "Train Epoch: 207 [45312/225000 (20%)] Loss: 7182.712891\n",
      "Train Epoch: 207 [49408/225000 (22%)] Loss: 7396.781250\n",
      "Train Epoch: 207 [53504/225000 (24%)] Loss: 7217.500000\n",
      "Train Epoch: 207 [57600/225000 (26%)] Loss: 7147.972656\n",
      "Train Epoch: 207 [61696/225000 (27%)] Loss: 7244.828125\n",
      "Train Epoch: 207 [65792/225000 (29%)] Loss: 7105.314453\n",
      "Train Epoch: 207 [69888/225000 (31%)] Loss: 7301.191406\n",
      "Train Epoch: 207 [73984/225000 (33%)] Loss: 7214.685547\n",
      "Train Epoch: 207 [78080/225000 (35%)] Loss: 7114.343750\n",
      "Train Epoch: 207 [82176/225000 (37%)] Loss: 7065.439453\n",
      "Train Epoch: 207 [86272/225000 (38%)] Loss: 7060.798828\n",
      "Train Epoch: 207 [90368/225000 (40%)] Loss: 7216.466797\n",
      "Train Epoch: 207 [94464/225000 (42%)] Loss: 7157.013672\n",
      "Train Epoch: 207 [98560/225000 (44%)] Loss: 7167.107422\n",
      "Train Epoch: 207 [102656/225000 (46%)] Loss: 7273.396484\n",
      "Train Epoch: 207 [106752/225000 (47%)] Loss: 7048.673828\n",
      "Train Epoch: 207 [110848/225000 (49%)] Loss: 7055.144531\n",
      "Train Epoch: 207 [114944/225000 (51%)] Loss: 7217.994141\n",
      "Train Epoch: 207 [119040/225000 (53%)] Loss: 7182.445312\n",
      "Train Epoch: 207 [123136/225000 (55%)] Loss: 7108.255859\n",
      "Train Epoch: 207 [127232/225000 (57%)] Loss: 7184.095703\n",
      "Train Epoch: 207 [131328/225000 (58%)] Loss: 7255.900391\n",
      "Train Epoch: 207 [135424/225000 (60%)] Loss: 7166.589844\n",
      "Train Epoch: 207 [139520/225000 (62%)] Loss: 7218.542969\n",
      "Train Epoch: 207 [143616/225000 (64%)] Loss: 7290.708984\n",
      "Train Epoch: 207 [147712/225000 (66%)] Loss: 7303.394531\n",
      "Train Epoch: 207 [151808/225000 (67%)] Loss: 7032.117188\n",
      "Train Epoch: 207 [155904/225000 (69%)] Loss: 6962.400391\n",
      "Train Epoch: 207 [160000/225000 (71%)] Loss: 7253.775391\n",
      "Train Epoch: 207 [164096/225000 (73%)] Loss: 7103.244141\n",
      "Train Epoch: 207 [168192/225000 (75%)] Loss: 7102.074219\n",
      "Train Epoch: 207 [172288/225000 (77%)] Loss: 7167.820312\n",
      "Train Epoch: 207 [176384/225000 (78%)] Loss: 7160.109375\n",
      "Train Epoch: 207 [180480/225000 (80%)] Loss: 7157.285156\n",
      "Train Epoch: 207 [184576/225000 (82%)] Loss: 7348.130859\n",
      "Train Epoch: 207 [188672/225000 (84%)] Loss: 7264.541016\n",
      "Train Epoch: 207 [192768/225000 (86%)] Loss: 7200.142578\n",
      "Train Epoch: 207 [196864/225000 (87%)] Loss: 7114.660156\n",
      "Train Epoch: 207 [200960/225000 (89%)] Loss: 7213.402344\n",
      "Train Epoch: 207 [205056/225000 (91%)] Loss: 7071.880859\n",
      "Train Epoch: 207 [209152/225000 (93%)] Loss: 7155.095703\n",
      "Train Epoch: 207 [213248/225000 (95%)] Loss: 7210.306641\n",
      "Train Epoch: 207 [217344/225000 (97%)] Loss: 7142.853516\n",
      "Train Epoch: 207 [221440/225000 (98%)] Loss: 6998.570312\n",
      "    epoch          : 207\n",
      "    loss           : 7167.248390171715\n",
      "    val_loss       : 7154.870770584564\n",
      "Train Epoch: 208 [256/225000 (0%)] Loss: 7170.601562\n",
      "Train Epoch: 208 [4352/225000 (2%)] Loss: 7162.966797\n",
      "Train Epoch: 208 [8448/225000 (4%)] Loss: 7020.460938\n",
      "Train Epoch: 208 [12544/225000 (6%)] Loss: 7114.531250\n",
      "Train Epoch: 208 [16640/225000 (7%)] Loss: 7143.802734\n",
      "Train Epoch: 208 [20736/225000 (9%)] Loss: 7123.451172\n",
      "Train Epoch: 208 [24832/225000 (11%)] Loss: 7291.257812\n",
      "Train Epoch: 208 [28928/225000 (13%)] Loss: 7211.740234\n",
      "Train Epoch: 208 [33024/225000 (15%)] Loss: 7082.990234\n",
      "Train Epoch: 208 [37120/225000 (16%)] Loss: 7064.259766\n",
      "Train Epoch: 208 [41216/225000 (18%)] Loss: 7071.021484\n",
      "Train Epoch: 208 [45312/225000 (20%)] Loss: 7252.898438\n",
      "Train Epoch: 208 [49408/225000 (22%)] Loss: 7354.757812\n",
      "Train Epoch: 208 [53504/225000 (24%)] Loss: 7106.564453\n",
      "Train Epoch: 208 [57600/225000 (26%)] Loss: 7110.058594\n",
      "Train Epoch: 208 [61696/225000 (27%)] Loss: 7086.566406\n",
      "Train Epoch: 208 [65792/225000 (29%)] Loss: 7124.462891\n",
      "Train Epoch: 208 [69888/225000 (31%)] Loss: 7179.666016\n",
      "Train Epoch: 208 [73984/225000 (33%)] Loss: 7192.740234\n",
      "Train Epoch: 208 [78080/225000 (35%)] Loss: 7047.689453\n",
      "Train Epoch: 208 [82176/225000 (37%)] Loss: 7047.109375\n",
      "Train Epoch: 208 [86272/225000 (38%)] Loss: 7205.001953\n",
      "Train Epoch: 208 [90368/225000 (40%)] Loss: 7099.976562\n",
      "Train Epoch: 208 [94464/225000 (42%)] Loss: 7167.562500\n",
      "Train Epoch: 208 [98560/225000 (44%)] Loss: 7350.578125\n",
      "Train Epoch: 208 [102656/225000 (46%)] Loss: 6926.021484\n",
      "Train Epoch: 208 [106752/225000 (47%)] Loss: 7165.320312\n",
      "Train Epoch: 208 [110848/225000 (49%)] Loss: 7194.414062\n",
      "Train Epoch: 208 [114944/225000 (51%)] Loss: 7188.302734\n",
      "Train Epoch: 208 [119040/225000 (53%)] Loss: 7098.992188\n",
      "Train Epoch: 208 [123136/225000 (55%)] Loss: 7182.460938\n",
      "Train Epoch: 208 [127232/225000 (57%)] Loss: 7234.619141\n",
      "Train Epoch: 208 [131328/225000 (58%)] Loss: 7152.949219\n",
      "Train Epoch: 208 [135424/225000 (60%)] Loss: 7199.904297\n",
      "Train Epoch: 208 [139520/225000 (62%)] Loss: 7001.902344\n",
      "Train Epoch: 208 [143616/225000 (64%)] Loss: 7223.835938\n",
      "Train Epoch: 208 [147712/225000 (66%)] Loss: 7367.427734\n",
      "Train Epoch: 208 [151808/225000 (67%)] Loss: 6955.179688\n",
      "Train Epoch: 208 [155904/225000 (69%)] Loss: 7034.240234\n",
      "Train Epoch: 208 [160000/225000 (71%)] Loss: 7230.828125\n",
      "Train Epoch: 208 [164096/225000 (73%)] Loss: 7130.578125\n",
      "Train Epoch: 208 [168192/225000 (75%)] Loss: 7166.154297\n",
      "Train Epoch: 208 [172288/225000 (77%)] Loss: 7237.076172\n",
      "Train Epoch: 208 [176384/225000 (78%)] Loss: 7020.064453\n",
      "Train Epoch: 208 [180480/225000 (80%)] Loss: 7193.603516\n",
      "Train Epoch: 208 [184576/225000 (82%)] Loss: 7056.378906\n",
      "Train Epoch: 208 [188672/225000 (84%)] Loss: 7146.332031\n",
      "Train Epoch: 208 [192768/225000 (86%)] Loss: 7190.529297\n",
      "Train Epoch: 208 [196864/225000 (87%)] Loss: 7081.005859\n",
      "Train Epoch: 208 [200960/225000 (89%)] Loss: 7239.539062\n",
      "Train Epoch: 208 [205056/225000 (91%)] Loss: 7052.648438\n",
      "Train Epoch: 208 [209152/225000 (93%)] Loss: 7210.082031\n",
      "Train Epoch: 208 [213248/225000 (95%)] Loss: 7184.107422\n",
      "Train Epoch: 208 [217344/225000 (97%)] Loss: 7021.298828\n",
      "Train Epoch: 208 [221440/225000 (98%)] Loss: 7170.269531\n",
      "    epoch          : 208\n",
      "    loss           : 7186.388854077787\n",
      "    val_loss       : 7153.876481494125\n",
      "Train Epoch: 209 [256/225000 (0%)] Loss: 7164.476562\n",
      "Train Epoch: 209 [4352/225000 (2%)] Loss: 7123.070312\n",
      "Train Epoch: 209 [8448/225000 (4%)] Loss: 7070.101562\n",
      "Train Epoch: 209 [12544/225000 (6%)] Loss: 7120.693359\n",
      "Train Epoch: 209 [16640/225000 (7%)] Loss: 7148.964844\n",
      "Train Epoch: 209 [20736/225000 (9%)] Loss: 7139.455078\n",
      "Train Epoch: 209 [24832/225000 (11%)] Loss: 7056.080078\n",
      "Train Epoch: 209 [28928/225000 (13%)] Loss: 7193.158203\n",
      "Train Epoch: 209 [33024/225000 (15%)] Loss: 7282.671875\n",
      "Train Epoch: 209 [37120/225000 (16%)] Loss: 7219.677734\n",
      "Train Epoch: 209 [41216/225000 (18%)] Loss: 7090.500000\n",
      "Train Epoch: 209 [45312/225000 (20%)] Loss: 7091.554688\n",
      "Train Epoch: 209 [49408/225000 (22%)] Loss: 7065.035156\n",
      "Train Epoch: 209 [53504/225000 (24%)] Loss: 7122.839844\n",
      "Train Epoch: 209 [57600/225000 (26%)] Loss: 7150.732422\n",
      "Train Epoch: 209 [61696/225000 (27%)] Loss: 7260.507812\n",
      "Train Epoch: 209 [65792/225000 (29%)] Loss: 7039.234375\n",
      "Train Epoch: 209 [69888/225000 (31%)] Loss: 7213.894531\n",
      "Train Epoch: 209 [73984/225000 (33%)] Loss: 7106.425781\n",
      "Train Epoch: 209 [78080/225000 (35%)] Loss: 7464.138672\n",
      "Train Epoch: 209 [82176/225000 (37%)] Loss: 7043.939453\n",
      "Train Epoch: 209 [86272/225000 (38%)] Loss: 7079.345703\n",
      "Train Epoch: 209 [90368/225000 (40%)] Loss: 7131.570312\n",
      "Train Epoch: 209 [94464/225000 (42%)] Loss: 7256.050781\n",
      "Train Epoch: 209 [98560/225000 (44%)] Loss: 7093.060547\n",
      "Train Epoch: 209 [102656/225000 (46%)] Loss: 7314.910156\n",
      "Train Epoch: 209 [106752/225000 (47%)] Loss: 7224.255859\n",
      "Train Epoch: 209 [110848/225000 (49%)] Loss: 7102.007812\n",
      "Train Epoch: 209 [114944/225000 (51%)] Loss: 7321.683594\n",
      "Train Epoch: 209 [119040/225000 (53%)] Loss: 7171.363281\n",
      "Train Epoch: 209 [123136/225000 (55%)] Loss: 7306.083984\n",
      "Train Epoch: 209 [127232/225000 (57%)] Loss: 7195.191406\n",
      "Train Epoch: 209 [131328/225000 (58%)] Loss: 7014.691406\n",
      "Train Epoch: 209 [135424/225000 (60%)] Loss: 7052.484375\n",
      "Train Epoch: 209 [139520/225000 (62%)] Loss: 7197.628906\n",
      "Train Epoch: 209 [143616/225000 (64%)] Loss: 6984.060547\n",
      "Train Epoch: 209 [147712/225000 (66%)] Loss: 7279.154297\n",
      "Train Epoch: 209 [151808/225000 (67%)] Loss: 7105.667969\n",
      "Train Epoch: 209 [155904/225000 (69%)] Loss: 7132.160156\n",
      "Train Epoch: 209 [160000/225000 (71%)] Loss: 7088.218750\n",
      "Train Epoch: 209 [164096/225000 (73%)] Loss: 7171.298828\n",
      "Train Epoch: 209 [168192/225000 (75%)] Loss: 7022.509766\n",
      "Train Epoch: 209 [172288/225000 (77%)] Loss: 7073.404297\n",
      "Train Epoch: 209 [176384/225000 (78%)] Loss: 7121.171875\n",
      "Train Epoch: 209 [180480/225000 (80%)] Loss: 7152.867188\n",
      "Train Epoch: 209 [184576/225000 (82%)] Loss: 7222.248047\n",
      "Train Epoch: 209 [188672/225000 (84%)] Loss: 7240.523438\n",
      "Train Epoch: 209 [192768/225000 (86%)] Loss: 7049.937500\n",
      "Train Epoch: 209 [196864/225000 (87%)] Loss: 6992.361328\n",
      "Train Epoch: 209 [200960/225000 (89%)] Loss: 7225.113281\n",
      "Train Epoch: 209 [205056/225000 (91%)] Loss: 7336.230469\n",
      "Train Epoch: 209 [209152/225000 (93%)] Loss: 6987.693359\n",
      "Train Epoch: 209 [213248/225000 (95%)] Loss: 7165.892578\n",
      "Train Epoch: 209 [217344/225000 (97%)] Loss: 7029.291016\n",
      "Train Epoch: 209 [221440/225000 (98%)] Loss: 7215.208984\n",
      "    epoch          : 209\n",
      "    loss           : 7151.075893015856\n",
      "    val_loss       : 7162.680499186321\n",
      "Train Epoch: 210 [256/225000 (0%)] Loss: 7097.560547\n",
      "Train Epoch: 210 [4352/225000 (2%)] Loss: 7263.193359\n",
      "Train Epoch: 210 [8448/225000 (4%)] Loss: 7101.945312\n",
      "Train Epoch: 210 [12544/225000 (6%)] Loss: 7183.134766\n",
      "Train Epoch: 210 [16640/225000 (7%)] Loss: 7112.703125\n",
      "Train Epoch: 210 [20736/225000 (9%)] Loss: 7034.046875\n",
      "Train Epoch: 210 [24832/225000 (11%)] Loss: 7279.082031\n",
      "Train Epoch: 210 [28928/225000 (13%)] Loss: 7197.734375\n",
      "Train Epoch: 210 [33024/225000 (15%)] Loss: 7212.300781\n",
      "Train Epoch: 210 [37120/225000 (16%)] Loss: 7098.611328\n",
      "Train Epoch: 210 [41216/225000 (18%)] Loss: 7206.941406\n",
      "Train Epoch: 210 [45312/225000 (20%)] Loss: 7053.681641\n",
      "Train Epoch: 210 [49408/225000 (22%)] Loss: 7080.572266\n",
      "Train Epoch: 210 [53504/225000 (24%)] Loss: 7165.732422\n",
      "Train Epoch: 210 [57600/225000 (26%)] Loss: 7155.267578\n",
      "Train Epoch: 210 [61696/225000 (27%)] Loss: 7071.955078\n",
      "Train Epoch: 210 [65792/225000 (29%)] Loss: 7137.925781\n",
      "Train Epoch: 210 [69888/225000 (31%)] Loss: 7145.613281\n",
      "Train Epoch: 210 [73984/225000 (33%)] Loss: 7074.593750\n",
      "Train Epoch: 210 [78080/225000 (35%)] Loss: 7082.001953\n",
      "Train Epoch: 210 [82176/225000 (37%)] Loss: 7187.775391\n",
      "Train Epoch: 210 [86272/225000 (38%)] Loss: 7055.751953\n",
      "Train Epoch: 210 [90368/225000 (40%)] Loss: 7059.064453\n",
      "Train Epoch: 210 [94464/225000 (42%)] Loss: 7103.349609\n",
      "Train Epoch: 210 [98560/225000 (44%)] Loss: 7124.197266\n",
      "Train Epoch: 210 [102656/225000 (46%)] Loss: 7292.496094\n",
      "Train Epoch: 210 [106752/225000 (47%)] Loss: 7138.224609\n",
      "Train Epoch: 210 [110848/225000 (49%)] Loss: 7311.400391\n",
      "Train Epoch: 210 [114944/225000 (51%)] Loss: 7105.158203\n",
      "Train Epoch: 210 [119040/225000 (53%)] Loss: 7212.017578\n",
      "Train Epoch: 210 [123136/225000 (55%)] Loss: 7106.761719\n",
      "Train Epoch: 210 [127232/225000 (57%)] Loss: 7161.587891\n",
      "Train Epoch: 210 [131328/225000 (58%)] Loss: 7227.402344\n",
      "Train Epoch: 210 [135424/225000 (60%)] Loss: 7143.644531\n",
      "Train Epoch: 210 [139520/225000 (62%)] Loss: 7251.765625\n",
      "Train Epoch: 210 [143616/225000 (64%)] Loss: 7223.009766\n",
      "Train Epoch: 210 [147712/225000 (66%)] Loss: 7075.453125\n",
      "Train Epoch: 210 [151808/225000 (67%)] Loss: 7075.183594\n",
      "Train Epoch: 210 [155904/225000 (69%)] Loss: 7180.169922\n",
      "Train Epoch: 210 [160000/225000 (71%)] Loss: 7065.925781\n",
      "Train Epoch: 210 [164096/225000 (73%)] Loss: 7204.865234\n",
      "Train Epoch: 210 [168192/225000 (75%)] Loss: 7118.021484\n",
      "Train Epoch: 210 [172288/225000 (77%)] Loss: 7195.791016\n",
      "Train Epoch: 210 [176384/225000 (78%)] Loss: 7266.781250\n",
      "Train Epoch: 210 [180480/225000 (80%)] Loss: 7235.232422\n",
      "Train Epoch: 210 [184576/225000 (82%)] Loss: 7266.253906\n",
      "Train Epoch: 210 [188672/225000 (84%)] Loss: 7227.638672\n",
      "Train Epoch: 210 [192768/225000 (86%)] Loss: 7262.927734\n",
      "Train Epoch: 210 [196864/225000 (87%)] Loss: 7182.365234\n",
      "Train Epoch: 210 [200960/225000 (89%)] Loss: 7158.126953\n",
      "Train Epoch: 210 [205056/225000 (91%)] Loss: 7087.054688\n",
      "Train Epoch: 210 [209152/225000 (93%)] Loss: 7050.714844\n",
      "Train Epoch: 210 [213248/225000 (95%)] Loss: 7368.703125\n",
      "Train Epoch: 210 [217344/225000 (97%)] Loss: 6970.044922\n",
      "Train Epoch: 210 [221440/225000 (98%)] Loss: 7140.560547\n",
      "    epoch          : 210\n",
      "    loss           : 7165.698427723265\n",
      "    val_loss       : 7146.601346149737\n",
      "Train Epoch: 211 [256/225000 (0%)] Loss: 7079.507812\n",
      "Train Epoch: 211 [4352/225000 (2%)] Loss: 7137.673828\n",
      "Train Epoch: 211 [8448/225000 (4%)] Loss: 7218.576172\n",
      "Train Epoch: 211 [12544/225000 (6%)] Loss: 7051.101562\n",
      "Train Epoch: 211 [16640/225000 (7%)] Loss: 7224.982422\n",
      "Train Epoch: 211 [20736/225000 (9%)] Loss: 7234.837891\n",
      "Train Epoch: 211 [24832/225000 (11%)] Loss: 6979.585938\n",
      "Train Epoch: 211 [28928/225000 (13%)] Loss: 7297.820312\n",
      "Train Epoch: 211 [33024/225000 (15%)] Loss: 7281.767578\n",
      "Train Epoch: 211 [37120/225000 (16%)] Loss: 7237.171875\n",
      "Train Epoch: 211 [41216/225000 (18%)] Loss: 6978.800781\n",
      "Train Epoch: 211 [45312/225000 (20%)] Loss: 7215.464844\n",
      "Train Epoch: 211 [49408/225000 (22%)] Loss: 7139.369141\n",
      "Train Epoch: 211 [53504/225000 (24%)] Loss: 7118.808594\n",
      "Train Epoch: 211 [57600/225000 (26%)] Loss: 7245.687500\n",
      "Train Epoch: 211 [61696/225000 (27%)] Loss: 7183.033203\n",
      "Train Epoch: 211 [65792/225000 (29%)] Loss: 7185.417969\n",
      "Train Epoch: 211 [69888/225000 (31%)] Loss: 6993.500000\n",
      "Train Epoch: 211 [73984/225000 (33%)] Loss: 7247.589844\n",
      "Train Epoch: 211 [78080/225000 (35%)] Loss: 7159.134766\n",
      "Train Epoch: 211 [82176/225000 (37%)] Loss: 7143.132812\n",
      "Train Epoch: 211 [86272/225000 (38%)] Loss: 7023.398438\n",
      "Train Epoch: 211 [90368/225000 (40%)] Loss: 7081.416016\n",
      "Train Epoch: 211 [94464/225000 (42%)] Loss: 7174.287109\n",
      "Train Epoch: 211 [98560/225000 (44%)] Loss: 7117.599609\n",
      "Train Epoch: 211 [102656/225000 (46%)] Loss: 7240.316406\n",
      "Train Epoch: 211 [106752/225000 (47%)] Loss: 7065.308594\n",
      "Train Epoch: 211 [110848/225000 (49%)] Loss: 7213.550781\n",
      "Train Epoch: 211 [114944/225000 (51%)] Loss: 7195.859375\n",
      "Train Epoch: 211 [119040/225000 (53%)] Loss: 7146.384766\n",
      "Train Epoch: 211 [123136/225000 (55%)] Loss: 7069.103516\n",
      "Train Epoch: 211 [127232/225000 (57%)] Loss: 7010.728516\n",
      "Train Epoch: 211 [131328/225000 (58%)] Loss: 7166.976562\n",
      "Train Epoch: 211 [135424/225000 (60%)] Loss: 7130.250000\n",
      "Train Epoch: 211 [139520/225000 (62%)] Loss: 7214.072266\n",
      "Train Epoch: 211 [143616/225000 (64%)] Loss: 7326.953125\n",
      "Train Epoch: 211 [147712/225000 (66%)] Loss: 7176.103516\n",
      "Train Epoch: 211 [151808/225000 (67%)] Loss: 7040.632812\n",
      "Train Epoch: 211 [155904/225000 (69%)] Loss: 7192.638672\n",
      "Train Epoch: 211 [160000/225000 (71%)] Loss: 7336.464844\n",
      "Train Epoch: 211 [164096/225000 (73%)] Loss: 7247.507812\n",
      "Train Epoch: 211 [168192/225000 (75%)] Loss: 6993.152344\n",
      "Train Epoch: 211 [172288/225000 (77%)] Loss: 7293.648438\n",
      "Train Epoch: 211 [176384/225000 (78%)] Loss: 7138.744141\n",
      "Train Epoch: 211 [180480/225000 (80%)] Loss: 7189.595703\n",
      "Train Epoch: 211 [184576/225000 (82%)] Loss: 7121.683594\n",
      "Train Epoch: 211 [188672/225000 (84%)] Loss: 7106.490234\n",
      "Train Epoch: 211 [192768/225000 (86%)] Loss: 7369.328125\n",
      "Train Epoch: 211 [196864/225000 (87%)] Loss: 7247.175781\n",
      "Train Epoch: 211 [200960/225000 (89%)] Loss: 7277.056641\n",
      "Train Epoch: 211 [205056/225000 (91%)] Loss: 7194.855469\n",
      "Train Epoch: 211 [209152/225000 (93%)] Loss: 7296.826172\n",
      "Train Epoch: 211 [213248/225000 (95%)] Loss: 6981.216797\n",
      "Train Epoch: 211 [217344/225000 (97%)] Loss: 7137.953125\n",
      "Train Epoch: 211 [221440/225000 (98%)] Loss: 7191.519531\n",
      "    epoch          : 211\n",
      "    loss           : 7184.991228713382\n",
      "    val_loss       : 7197.21112988433\n",
      "Train Epoch: 212 [256/225000 (0%)] Loss: 7108.197266\n",
      "Train Epoch: 212 [4352/225000 (2%)] Loss: 7312.197266\n",
      "Train Epoch: 212 [8448/225000 (4%)] Loss: 7029.740234\n",
      "Train Epoch: 212 [12544/225000 (6%)] Loss: 7119.687500\n",
      "Train Epoch: 212 [16640/225000 (7%)] Loss: 7144.931641\n",
      "Train Epoch: 212 [20736/225000 (9%)] Loss: 7025.765625\n",
      "Train Epoch: 212 [24832/225000 (11%)] Loss: 7024.179688\n",
      "Train Epoch: 212 [28928/225000 (13%)] Loss: 7250.214844\n",
      "Train Epoch: 212 [33024/225000 (15%)] Loss: 7094.578125\n",
      "Train Epoch: 212 [37120/225000 (16%)] Loss: 7327.111328\n",
      "Train Epoch: 212 [41216/225000 (18%)] Loss: 7088.742188\n",
      "Train Epoch: 212 [45312/225000 (20%)] Loss: 7188.978516\n",
      "Train Epoch: 212 [49408/225000 (22%)] Loss: 7026.003906\n",
      "Train Epoch: 212 [53504/225000 (24%)] Loss: 7360.486328\n",
      "Train Epoch: 212 [57600/225000 (26%)] Loss: 7068.146484\n",
      "Train Epoch: 212 [61696/225000 (27%)] Loss: 7109.853516\n",
      "Train Epoch: 212 [65792/225000 (29%)] Loss: 7136.080078\n",
      "Train Epoch: 212 [69888/225000 (31%)] Loss: 7127.908203\n",
      "Train Epoch: 212 [73984/225000 (33%)] Loss: 7201.974609\n",
      "Train Epoch: 212 [78080/225000 (35%)] Loss: 7353.207031\n",
      "Train Epoch: 212 [82176/225000 (37%)] Loss: 6974.546875\n",
      "Train Epoch: 212 [86272/225000 (38%)] Loss: 7195.496094\n",
      "Train Epoch: 212 [90368/225000 (40%)] Loss: 7030.082031\n",
      "Train Epoch: 212 [94464/225000 (42%)] Loss: 7081.794922\n",
      "Train Epoch: 212 [98560/225000 (44%)] Loss: 7163.431641\n",
      "Train Epoch: 212 [102656/225000 (46%)] Loss: 7038.814453\n",
      "Train Epoch: 212 [106752/225000 (47%)] Loss: 7252.509766\n",
      "Train Epoch: 212 [110848/225000 (49%)] Loss: 7124.210938\n",
      "Train Epoch: 212 [114944/225000 (51%)] Loss: 7109.824219\n",
      "Train Epoch: 212 [119040/225000 (53%)] Loss: 7058.425781\n",
      "Train Epoch: 212 [123136/225000 (55%)] Loss: 7196.169922\n",
      "Train Epoch: 212 [127232/225000 (57%)] Loss: 7140.863281\n",
      "Train Epoch: 212 [131328/225000 (58%)] Loss: 7041.070312\n",
      "Train Epoch: 212 [135424/225000 (60%)] Loss: 7207.082031\n",
      "Train Epoch: 212 [139520/225000 (62%)] Loss: 7243.650391\n",
      "Train Epoch: 212 [143616/225000 (64%)] Loss: 7042.535156\n",
      "Train Epoch: 212 [147712/225000 (66%)] Loss: 7229.628906\n",
      "Train Epoch: 212 [151808/225000 (67%)] Loss: 7134.234375\n",
      "Train Epoch: 212 [155904/225000 (69%)] Loss: 7134.701172\n",
      "Train Epoch: 212 [160000/225000 (71%)] Loss: 7351.464844\n",
      "Train Epoch: 212 [164096/225000 (73%)] Loss: 7354.880859\n",
      "Train Epoch: 212 [168192/225000 (75%)] Loss: 7129.062500\n",
      "Train Epoch: 212 [172288/225000 (77%)] Loss: 6959.664062\n",
      "Train Epoch: 212 [176384/225000 (78%)] Loss: 7042.941406\n",
      "Train Epoch: 212 [180480/225000 (80%)] Loss: 7088.386719\n",
      "Train Epoch: 212 [184576/225000 (82%)] Loss: 7209.369141\n",
      "Train Epoch: 212 [188672/225000 (84%)] Loss: 7078.398438\n",
      "Train Epoch: 212 [192768/225000 (86%)] Loss: 7127.857422\n",
      "Train Epoch: 212 [196864/225000 (87%)] Loss: 7098.476562\n",
      "Train Epoch: 212 [200960/225000 (89%)] Loss: 7141.335938\n",
      "Train Epoch: 212 [205056/225000 (91%)] Loss: 7024.716797\n",
      "Train Epoch: 212 [209152/225000 (93%)] Loss: 6929.990234\n",
      "Train Epoch: 212 [213248/225000 (95%)] Loss: 7132.814453\n",
      "Train Epoch: 212 [217344/225000 (97%)] Loss: 7110.103516\n",
      "Train Epoch: 212 [221440/225000 (98%)] Loss: 7194.435547\n",
      "    epoch          : 212\n",
      "    loss           : 7141.6900886127705\n",
      "    val_loss       : 7139.072186409211\n",
      "Train Epoch: 213 [256/225000 (0%)] Loss: 7242.697266\n",
      "Train Epoch: 213 [4352/225000 (2%)] Loss: 7232.812500\n",
      "Train Epoch: 213 [8448/225000 (4%)] Loss: 7155.429688\n",
      "Train Epoch: 213 [12544/225000 (6%)] Loss: 7019.910156\n",
      "Train Epoch: 213 [16640/225000 (7%)] Loss: 7172.730469\n",
      "Train Epoch: 213 [20736/225000 (9%)] Loss: 7169.099609\n",
      "Train Epoch: 213 [24832/225000 (11%)] Loss: 7061.117188\n",
      "Train Epoch: 213 [28928/225000 (13%)] Loss: 7043.421875\n",
      "Train Epoch: 213 [33024/225000 (15%)] Loss: 7085.089844\n",
      "Train Epoch: 213 [37120/225000 (16%)] Loss: 6987.632812\n",
      "Train Epoch: 213 [41216/225000 (18%)] Loss: 7189.832031\n",
      "Train Epoch: 213 [45312/225000 (20%)] Loss: 7221.802734\n",
      "Train Epoch: 213 [49408/225000 (22%)] Loss: 7069.992188\n",
      "Train Epoch: 213 [53504/225000 (24%)] Loss: 7164.775391\n",
      "Train Epoch: 213 [57600/225000 (26%)] Loss: 7176.673828\n",
      "Train Epoch: 213 [61696/225000 (27%)] Loss: 7219.037109\n",
      "Train Epoch: 213 [65792/225000 (29%)] Loss: 7117.511719\n",
      "Train Epoch: 213 [69888/225000 (31%)] Loss: 7040.035156\n",
      "Train Epoch: 213 [73984/225000 (33%)] Loss: 7090.425781\n",
      "Train Epoch: 213 [78080/225000 (35%)] Loss: 7072.150391\n",
      "Train Epoch: 213 [82176/225000 (37%)] Loss: 7141.167969\n",
      "Train Epoch: 213 [86272/225000 (38%)] Loss: 7089.548828\n",
      "Train Epoch: 213 [90368/225000 (40%)] Loss: 7028.097656\n",
      "Train Epoch: 213 [94464/225000 (42%)] Loss: 7107.218750\n",
      "Train Epoch: 213 [98560/225000 (44%)] Loss: 7207.566406\n",
      "Train Epoch: 213 [102656/225000 (46%)] Loss: 7175.283203\n",
      "Train Epoch: 213 [106752/225000 (47%)] Loss: 7160.490234\n",
      "Train Epoch: 213 [110848/225000 (49%)] Loss: 7094.453125\n",
      "Train Epoch: 213 [114944/225000 (51%)] Loss: 7193.632812\n",
      "Train Epoch: 213 [119040/225000 (53%)] Loss: 7095.318359\n",
      "Train Epoch: 213 [123136/225000 (55%)] Loss: 7229.984375\n",
      "Train Epoch: 213 [127232/225000 (57%)] Loss: 7104.781250\n",
      "Train Epoch: 213 [131328/225000 (58%)] Loss: 7287.599609\n",
      "Train Epoch: 213 [135424/225000 (60%)] Loss: 7057.205078\n",
      "Train Epoch: 213 [139520/225000 (62%)] Loss: 6986.408203\n",
      "Train Epoch: 213 [143616/225000 (64%)] Loss: 7194.699219\n",
      "Train Epoch: 213 [147712/225000 (66%)] Loss: 7224.033203\n",
      "Train Epoch: 213 [151808/225000 (67%)] Loss: 7209.017578\n",
      "Train Epoch: 213 [155904/225000 (69%)] Loss: 7173.095703\n",
      "Train Epoch: 213 [160000/225000 (71%)] Loss: 7270.917969\n",
      "Train Epoch: 213 [164096/225000 (73%)] Loss: 7074.720703\n",
      "Train Epoch: 213 [168192/225000 (75%)] Loss: 7251.138672\n",
      "Train Epoch: 213 [172288/225000 (77%)] Loss: 7053.904297\n",
      "Train Epoch: 213 [176384/225000 (78%)] Loss: 7016.646484\n",
      "Train Epoch: 213 [180480/225000 (80%)] Loss: 7256.160156\n",
      "Train Epoch: 213 [184576/225000 (82%)] Loss: 7292.001953\n",
      "Train Epoch: 213 [188672/225000 (84%)] Loss: 7148.738281\n",
      "Train Epoch: 213 [192768/225000 (86%)] Loss: 7157.568359\n",
      "Train Epoch: 213 [196864/225000 (87%)] Loss: 7111.681641\n",
      "Train Epoch: 213 [200960/225000 (89%)] Loss: 7116.386719\n",
      "Train Epoch: 213 [205056/225000 (91%)] Loss: 7038.123047\n",
      "Train Epoch: 213 [209152/225000 (93%)] Loss: 7091.398438\n",
      "Train Epoch: 213 [213248/225000 (95%)] Loss: 7233.767578\n",
      "Train Epoch: 213 [217344/225000 (97%)] Loss: 7209.697266\n",
      "Train Epoch: 213 [221440/225000 (98%)] Loss: 7172.859375\n",
      "    epoch          : 213\n",
      "    loss           : 7148.461117480802\n",
      "    val_loss       : 7138.48018106879\n",
      "Train Epoch: 214 [256/225000 (0%)] Loss: 7093.011719\n",
      "Train Epoch: 214 [4352/225000 (2%)] Loss: 7028.205078\n",
      "Train Epoch: 214 [8448/225000 (4%)] Loss: 7286.447266\n",
      "Train Epoch: 214 [12544/225000 (6%)] Loss: 7133.226562\n",
      "Train Epoch: 214 [16640/225000 (7%)] Loss: 6972.806641\n",
      "Train Epoch: 214 [20736/225000 (9%)] Loss: 7105.933594\n",
      "Train Epoch: 214 [24832/225000 (11%)] Loss: 7097.794922\n",
      "Train Epoch: 214 [28928/225000 (13%)] Loss: 7118.728516\n",
      "Train Epoch: 214 [33024/225000 (15%)] Loss: 7042.861328\n",
      "Train Epoch: 214 [37120/225000 (16%)] Loss: 7170.722656\n",
      "Train Epoch: 214 [41216/225000 (18%)] Loss: 7174.966797\n",
      "Train Epoch: 214 [45312/225000 (20%)] Loss: 7095.183594\n",
      "Train Epoch: 214 [49408/225000 (22%)] Loss: 7199.826172\n",
      "Train Epoch: 214 [53504/225000 (24%)] Loss: 7094.712891\n",
      "Train Epoch: 214 [57600/225000 (26%)] Loss: 7116.810547\n",
      "Train Epoch: 214 [61696/225000 (27%)] Loss: 7156.777344\n",
      "Train Epoch: 214 [65792/225000 (29%)] Loss: 7080.777344\n",
      "Train Epoch: 214 [69888/225000 (31%)] Loss: 7070.552734\n",
      "Train Epoch: 214 [73984/225000 (33%)] Loss: 7082.933594\n",
      "Train Epoch: 214 [78080/225000 (35%)] Loss: 7086.300781\n",
      "Train Epoch: 214 [82176/225000 (37%)] Loss: 7056.720703\n",
      "Train Epoch: 214 [86272/225000 (38%)] Loss: 7080.566406\n",
      "Train Epoch: 214 [90368/225000 (40%)] Loss: 7226.148438\n",
      "Train Epoch: 214 [94464/225000 (42%)] Loss: 7201.841797\n",
      "Train Epoch: 214 [98560/225000 (44%)] Loss: 7255.818359\n",
      "Train Epoch: 214 [102656/225000 (46%)] Loss: 7241.712891\n",
      "Train Epoch: 214 [106752/225000 (47%)] Loss: 7175.300781\n",
      "Train Epoch: 214 [110848/225000 (49%)] Loss: 7133.832031\n",
      "Train Epoch: 214 [114944/225000 (51%)] Loss: 7167.525391\n",
      "Train Epoch: 214 [119040/225000 (53%)] Loss: 7133.121094\n",
      "Train Epoch: 214 [123136/225000 (55%)] Loss: 7176.312500\n",
      "Train Epoch: 214 [127232/225000 (57%)] Loss: 7390.865234\n",
      "Train Epoch: 214 [131328/225000 (58%)] Loss: 7099.621094\n",
      "Train Epoch: 214 [135424/225000 (60%)] Loss: 7135.470703\n",
      "Train Epoch: 214 [139520/225000 (62%)] Loss: 6941.767578\n",
      "Train Epoch: 214 [143616/225000 (64%)] Loss: 7199.970703\n",
      "Train Epoch: 214 [147712/225000 (66%)] Loss: 7107.830078\n",
      "Train Epoch: 214 [151808/225000 (67%)] Loss: 7191.261719\n",
      "Train Epoch: 214 [155904/225000 (69%)] Loss: 7274.734375\n",
      "Train Epoch: 214 [160000/225000 (71%)] Loss: 7062.789062\n",
      "Train Epoch: 214 [164096/225000 (73%)] Loss: 7156.632812\n",
      "Train Epoch: 214 [168192/225000 (75%)] Loss: 7128.062500\n",
      "Train Epoch: 214 [172288/225000 (77%)] Loss: 7172.927734\n",
      "Train Epoch: 214 [176384/225000 (78%)] Loss: 7140.695312\n",
      "Train Epoch: 214 [180480/225000 (80%)] Loss: 7098.937500\n",
      "Train Epoch: 214 [184576/225000 (82%)] Loss: 7243.576172\n",
      "Train Epoch: 214 [188672/225000 (84%)] Loss: 7200.527344\n",
      "Train Epoch: 214 [192768/225000 (86%)] Loss: 7020.546875\n",
      "Train Epoch: 214 [196864/225000 (87%)] Loss: 7131.693359\n",
      "Train Epoch: 214 [200960/225000 (89%)] Loss: 6955.773438\n",
      "Train Epoch: 214 [205056/225000 (91%)] Loss: 7296.308594\n",
      "Train Epoch: 214 [209152/225000 (93%)] Loss: 7204.947266\n",
      "Train Epoch: 214 [213248/225000 (95%)] Loss: 7196.203125\n",
      "Train Epoch: 214 [217344/225000 (97%)] Loss: 7106.425781\n",
      "Train Epoch: 214 [221440/225000 (98%)] Loss: 6979.781250\n",
      "    epoch          : 214\n",
      "    loss           : 7159.991860868174\n",
      "    val_loss       : 7136.960805447734\n",
      "Train Epoch: 215 [256/225000 (0%)] Loss: 7140.011719\n",
      "Train Epoch: 215 [4352/225000 (2%)] Loss: 7062.597656\n",
      "Train Epoch: 215 [8448/225000 (4%)] Loss: 6944.382812\n",
      "Train Epoch: 215 [12544/225000 (6%)] Loss: 7142.742188\n",
      "Train Epoch: 215 [16640/225000 (7%)] Loss: 7056.015625\n",
      "Train Epoch: 215 [20736/225000 (9%)] Loss: 7228.228516\n",
      "Train Epoch: 215 [24832/225000 (11%)] Loss: 7105.603516\n",
      "Train Epoch: 215 [28928/225000 (13%)] Loss: 7073.607422\n",
      "Train Epoch: 215 [33024/225000 (15%)] Loss: 7253.974609\n",
      "Train Epoch: 215 [37120/225000 (16%)] Loss: 7025.767578\n",
      "Train Epoch: 215 [41216/225000 (18%)] Loss: 7079.292969\n",
      "Train Epoch: 215 [45312/225000 (20%)] Loss: 7250.947266\n",
      "Train Epoch: 215 [49408/225000 (22%)] Loss: 7156.203125\n",
      "Train Epoch: 215 [53504/225000 (24%)] Loss: 7078.250000\n",
      "Train Epoch: 215 [57600/225000 (26%)] Loss: 7238.767578\n",
      "Train Epoch: 215 [61696/225000 (27%)] Loss: 7126.250000\n",
      "Train Epoch: 215 [65792/225000 (29%)] Loss: 7119.492188\n",
      "Train Epoch: 215 [69888/225000 (31%)] Loss: 7179.875000\n",
      "Train Epoch: 215 [73984/225000 (33%)] Loss: 7021.683594\n",
      "Train Epoch: 215 [78080/225000 (35%)] Loss: 7195.724609\n",
      "Train Epoch: 215 [82176/225000 (37%)] Loss: 7191.916016\n",
      "Train Epoch: 215 [86272/225000 (38%)] Loss: 7180.216797\n",
      "Train Epoch: 215 [90368/225000 (40%)] Loss: 7113.726562\n",
      "Train Epoch: 215 [94464/225000 (42%)] Loss: 7107.949219\n",
      "Train Epoch: 215 [98560/225000 (44%)] Loss: 7048.687500\n",
      "Train Epoch: 215 [102656/225000 (46%)] Loss: 7147.519531\n",
      "Train Epoch: 215 [106752/225000 (47%)] Loss: 7251.451172\n",
      "Train Epoch: 215 [110848/225000 (49%)] Loss: 7278.039062\n",
      "Train Epoch: 215 [114944/225000 (51%)] Loss: 7079.843750\n",
      "Train Epoch: 215 [119040/225000 (53%)] Loss: 7069.837891\n",
      "Train Epoch: 215 [123136/225000 (55%)] Loss: 7126.462891\n",
      "Train Epoch: 215 [127232/225000 (57%)] Loss: 7195.572266\n",
      "Train Epoch: 215 [131328/225000 (58%)] Loss: 7148.587891\n",
      "Train Epoch: 215 [135424/225000 (60%)] Loss: 7276.207031\n",
      "Train Epoch: 215 [139520/225000 (62%)] Loss: 7119.154297\n",
      "Train Epoch: 215 [143616/225000 (64%)] Loss: 7168.240234\n",
      "Train Epoch: 215 [147712/225000 (66%)] Loss: 7051.173828\n",
      "Train Epoch: 215 [151808/225000 (67%)] Loss: 7104.373047\n",
      "Train Epoch: 215 [155904/225000 (69%)] Loss: 7212.869141\n",
      "Train Epoch: 215 [160000/225000 (71%)] Loss: 7282.763672\n",
      "Train Epoch: 215 [164096/225000 (73%)] Loss: 7187.570312\n",
      "Train Epoch: 215 [168192/225000 (75%)] Loss: 7116.638672\n",
      "Train Epoch: 215 [172288/225000 (77%)] Loss: 7115.960938\n",
      "Train Epoch: 215 [176384/225000 (78%)] Loss: 7041.205078\n",
      "Train Epoch: 215 [180480/225000 (80%)] Loss: 7278.388672\n",
      "Train Epoch: 215 [184576/225000 (82%)] Loss: 6935.607422\n",
      "Train Epoch: 215 [188672/225000 (84%)] Loss: 7206.693359\n",
      "Train Epoch: 215 [192768/225000 (86%)] Loss: 7128.931641\n",
      "Train Epoch: 215 [196864/225000 (87%)] Loss: 7287.720703\n",
      "Train Epoch: 215 [200960/225000 (89%)] Loss: 7133.220703\n",
      "Train Epoch: 215 [205056/225000 (91%)] Loss: 7083.064453\n",
      "Train Epoch: 215 [209152/225000 (93%)] Loss: 7111.123047\n",
      "Train Epoch: 215 [213248/225000 (95%)] Loss: 7096.089844\n",
      "Train Epoch: 215 [217344/225000 (97%)] Loss: 7221.218750\n",
      "Train Epoch: 215 [221440/225000 (98%)] Loss: 7251.019531\n",
      "    epoch          : 215\n",
      "    loss           : 7133.6370320499145\n",
      "    val_loss       : 7134.539776483361\n",
      "Train Epoch: 216 [256/225000 (0%)] Loss: 7173.824219\n",
      "Train Epoch: 216 [4352/225000 (2%)] Loss: 7167.554688\n",
      "Train Epoch: 216 [8448/225000 (4%)] Loss: 7070.951172\n",
      "Train Epoch: 216 [12544/225000 (6%)] Loss: 6963.496094\n",
      "Train Epoch: 216 [16640/225000 (7%)] Loss: 7070.753906\n",
      "Train Epoch: 216 [20736/225000 (9%)] Loss: 7086.460938\n",
      "Train Epoch: 216 [24832/225000 (11%)] Loss: 7225.933594\n",
      "Train Epoch: 216 [28928/225000 (13%)] Loss: 7157.232422\n",
      "Train Epoch: 216 [33024/225000 (15%)] Loss: 7049.789062\n",
      "Train Epoch: 216 [37120/225000 (16%)] Loss: 7168.746094\n",
      "Train Epoch: 216 [41216/225000 (18%)] Loss: 7250.619141\n",
      "Train Epoch: 216 [45312/225000 (20%)] Loss: 7308.417969\n",
      "Train Epoch: 216 [49408/225000 (22%)] Loss: 7092.789062\n",
      "Train Epoch: 216 [53504/225000 (24%)] Loss: 7117.533203\n",
      "Train Epoch: 216 [57600/225000 (26%)] Loss: 7353.974609\n",
      "Train Epoch: 216 [61696/225000 (27%)] Loss: 7153.990234\n",
      "Train Epoch: 216 [65792/225000 (29%)] Loss: 7148.117188\n",
      "Train Epoch: 216 [69888/225000 (31%)] Loss: 7215.652344\n",
      "Train Epoch: 216 [73984/225000 (33%)] Loss: 7146.630859\n",
      "Train Epoch: 216 [78080/225000 (35%)] Loss: 7127.906250\n",
      "Train Epoch: 216 [82176/225000 (37%)] Loss: 7091.396484\n",
      "Train Epoch: 216 [86272/225000 (38%)] Loss: 7117.669922\n",
      "Train Epoch: 216 [90368/225000 (40%)] Loss: 7281.306641\n",
      "Train Epoch: 216 [94464/225000 (42%)] Loss: 7061.974609\n",
      "Train Epoch: 216 [98560/225000 (44%)] Loss: 7334.560547\n",
      "Train Epoch: 216 [102656/225000 (46%)] Loss: 7185.912109\n",
      "Train Epoch: 216 [106752/225000 (47%)] Loss: 16825.744141\n",
      "Train Epoch: 216 [110848/225000 (49%)] Loss: 7075.507812\n",
      "Train Epoch: 216 [114944/225000 (51%)] Loss: 7130.929688\n",
      "Train Epoch: 216 [119040/225000 (53%)] Loss: 7142.406250\n",
      "Train Epoch: 216 [123136/225000 (55%)] Loss: 24204.970703\n",
      "Train Epoch: 216 [127232/225000 (57%)] Loss: 7090.009766\n",
      "Train Epoch: 216 [131328/225000 (58%)] Loss: 7161.574219\n",
      "Train Epoch: 216 [135424/225000 (60%)] Loss: 7188.373047\n",
      "Train Epoch: 216 [139520/225000 (62%)] Loss: 7059.000000\n",
      "Train Epoch: 216 [143616/225000 (64%)] Loss: 7169.066406\n",
      "Train Epoch: 216 [147712/225000 (66%)] Loss: 7189.906250\n",
      "Train Epoch: 216 [151808/225000 (67%)] Loss: 7299.273438\n",
      "Train Epoch: 216 [155904/225000 (69%)] Loss: 7073.216797\n",
      "Train Epoch: 216 [160000/225000 (71%)] Loss: 7129.521484\n",
      "Train Epoch: 216 [164096/225000 (73%)] Loss: 6929.138672\n",
      "Train Epoch: 216 [168192/225000 (75%)] Loss: 6972.906250\n",
      "Train Epoch: 216 [172288/225000 (77%)] Loss: 7214.365234\n",
      "Train Epoch: 216 [176384/225000 (78%)] Loss: 7131.728516\n",
      "Train Epoch: 216 [180480/225000 (80%)] Loss: 7198.697266\n",
      "Train Epoch: 216 [184576/225000 (82%)] Loss: 7162.205078\n",
      "Train Epoch: 216 [188672/225000 (84%)] Loss: 7312.744141\n",
      "Train Epoch: 216 [192768/225000 (86%)] Loss: 7115.464844\n",
      "Train Epoch: 216 [196864/225000 (87%)] Loss: 7101.810547\n",
      "Train Epoch: 216 [200960/225000 (89%)] Loss: 7116.660156\n",
      "Train Epoch: 216 [205056/225000 (91%)] Loss: 6971.164062\n",
      "Train Epoch: 216 [209152/225000 (93%)] Loss: 7089.673828\n",
      "Train Epoch: 216 [213248/225000 (95%)] Loss: 7094.021484\n",
      "Train Epoch: 216 [217344/225000 (97%)] Loss: 7077.773438\n",
      "Train Epoch: 216 [221440/225000 (98%)] Loss: 7121.527344\n",
      "    epoch          : 216\n",
      "    loss           : 7184.640981628626\n",
      "    val_loss       : 7131.165601218233\n",
      "Train Epoch: 217 [256/225000 (0%)] Loss: 7054.015625\n",
      "Train Epoch: 217 [4352/225000 (2%)] Loss: 7126.853516\n",
      "Train Epoch: 217 [8448/225000 (4%)] Loss: 7123.560547\n",
      "Train Epoch: 217 [12544/225000 (6%)] Loss: 7181.117188\n",
      "Train Epoch: 217 [16640/225000 (7%)] Loss: 7191.142578\n",
      "Train Epoch: 217 [20736/225000 (9%)] Loss: 7360.943359\n",
      "Train Epoch: 217 [24832/225000 (11%)] Loss: 7124.656250\n",
      "Train Epoch: 217 [28928/225000 (13%)] Loss: 7043.166016\n",
      "Train Epoch: 217 [33024/225000 (15%)] Loss: 7029.603516\n",
      "Train Epoch: 217 [37120/225000 (16%)] Loss: 7185.712891\n",
      "Train Epoch: 217 [41216/225000 (18%)] Loss: 7163.623047\n",
      "Train Epoch: 217 [45312/225000 (20%)] Loss: 7091.650391\n",
      "Train Epoch: 217 [49408/225000 (22%)] Loss: 7253.001953\n",
      "Train Epoch: 217 [53504/225000 (24%)] Loss: 7139.728516\n",
      "Train Epoch: 217 [57600/225000 (26%)] Loss: 7117.648438\n",
      "Train Epoch: 217 [61696/225000 (27%)] Loss: 6996.240234\n",
      "Train Epoch: 217 [65792/225000 (29%)] Loss: 7147.507812\n",
      "Train Epoch: 217 [69888/225000 (31%)] Loss: 7038.847656\n",
      "Train Epoch: 217 [73984/225000 (33%)] Loss: 7125.433594\n",
      "Train Epoch: 217 [78080/225000 (35%)] Loss: 7019.460938\n",
      "Train Epoch: 217 [82176/225000 (37%)] Loss: 7043.554688\n",
      "Train Epoch: 217 [86272/225000 (38%)] Loss: 6977.931641\n",
      "Train Epoch: 217 [90368/225000 (40%)] Loss: 7218.976562\n",
      "Train Epoch: 217 [94464/225000 (42%)] Loss: 6989.423828\n",
      "Train Epoch: 217 [98560/225000 (44%)] Loss: 6973.009766\n",
      "Train Epoch: 217 [102656/225000 (46%)] Loss: 7126.369141\n",
      "Train Epoch: 217 [106752/225000 (47%)] Loss: 7184.939453\n",
      "Train Epoch: 217 [110848/225000 (49%)] Loss: 7182.642578\n",
      "Train Epoch: 217 [114944/225000 (51%)] Loss: 7006.871094\n",
      "Train Epoch: 217 [119040/225000 (53%)] Loss: 7174.781250\n",
      "Train Epoch: 217 [123136/225000 (55%)] Loss: 7225.511719\n",
      "Train Epoch: 217 [127232/225000 (57%)] Loss: 7104.957031\n",
      "Train Epoch: 217 [131328/225000 (58%)] Loss: 7053.808594\n",
      "Train Epoch: 217 [135424/225000 (60%)] Loss: 6946.910156\n",
      "Train Epoch: 217 [139520/225000 (62%)] Loss: 7173.199219\n",
      "Train Epoch: 217 [143616/225000 (64%)] Loss: 7024.261719\n",
      "Train Epoch: 217 [147712/225000 (66%)] Loss: 7057.576172\n",
      "Train Epoch: 217 [151808/225000 (67%)] Loss: 7133.708984\n",
      "Train Epoch: 217 [155904/225000 (69%)] Loss: 7153.230469\n",
      "Train Epoch: 217 [160000/225000 (71%)] Loss: 7154.099609\n",
      "Train Epoch: 217 [164096/225000 (73%)] Loss: 7260.857422\n",
      "Train Epoch: 217 [168192/225000 (75%)] Loss: 7156.392578\n",
      "Train Epoch: 217 [172288/225000 (77%)] Loss: 6996.453125\n",
      "Train Epoch: 217 [176384/225000 (78%)] Loss: 6977.972656\n",
      "Train Epoch: 217 [180480/225000 (80%)] Loss: 7107.074219\n",
      "Train Epoch: 217 [184576/225000 (82%)] Loss: 7236.373047\n",
      "Train Epoch: 217 [188672/225000 (84%)] Loss: 7006.765625\n",
      "Train Epoch: 217 [192768/225000 (86%)] Loss: 7114.173828\n",
      "Train Epoch: 217 [196864/225000 (87%)] Loss: 7002.195312\n",
      "Train Epoch: 217 [200960/225000 (89%)] Loss: 7057.023438\n",
      "Train Epoch: 217 [205056/225000 (91%)] Loss: 6968.285156\n",
      "Train Epoch: 217 [209152/225000 (93%)] Loss: 7172.693359\n",
      "Train Epoch: 217 [213248/225000 (95%)] Loss: 7125.308594\n",
      "Train Epoch: 217 [217344/225000 (97%)] Loss: 7165.431641\n",
      "Train Epoch: 217 [221440/225000 (98%)] Loss: 7047.687500\n",
      "    epoch          : 217\n",
      "    loss           : 7145.874494498365\n",
      "    val_loss       : 7129.755259701184\n",
      "Train Epoch: 218 [256/225000 (0%)] Loss: 7045.875000\n",
      "Train Epoch: 218 [4352/225000 (2%)] Loss: 7175.714844\n",
      "Train Epoch: 218 [8448/225000 (4%)] Loss: 7238.035156\n",
      "Train Epoch: 218 [12544/225000 (6%)] Loss: 7235.437500\n",
      "Train Epoch: 218 [16640/225000 (7%)] Loss: 7121.716797\n",
      "Train Epoch: 218 [20736/225000 (9%)] Loss: 7144.644531\n",
      "Train Epoch: 218 [24832/225000 (11%)] Loss: 7069.048828\n",
      "Train Epoch: 218 [28928/225000 (13%)] Loss: 7080.671875\n",
      "Train Epoch: 218 [33024/225000 (15%)] Loss: 7178.031250\n",
      "Train Epoch: 218 [37120/225000 (16%)] Loss: 7089.113281\n",
      "Train Epoch: 218 [41216/225000 (18%)] Loss: 7271.875000\n",
      "Train Epoch: 218 [45312/225000 (20%)] Loss: 7188.208984\n",
      "Train Epoch: 218 [49408/225000 (22%)] Loss: 7216.371094\n",
      "Train Epoch: 218 [53504/225000 (24%)] Loss: 7069.474609\n",
      "Train Epoch: 218 [57600/225000 (26%)] Loss: 7096.265625\n",
      "Train Epoch: 218 [61696/225000 (27%)] Loss: 7287.322266\n",
      "Train Epoch: 218 [65792/225000 (29%)] Loss: 7032.523438\n",
      "Train Epoch: 218 [69888/225000 (31%)] Loss: 7140.203125\n",
      "Train Epoch: 218 [73984/225000 (33%)] Loss: 7124.207031\n",
      "Train Epoch: 218 [78080/225000 (35%)] Loss: 7102.871094\n",
      "Train Epoch: 218 [82176/225000 (37%)] Loss: 7194.566406\n",
      "Train Epoch: 218 [86272/225000 (38%)] Loss: 7091.994141\n",
      "Train Epoch: 218 [90368/225000 (40%)] Loss: 7257.162109\n",
      "Train Epoch: 218 [94464/225000 (42%)] Loss: 7173.910156\n",
      "Train Epoch: 218 [98560/225000 (44%)] Loss: 7265.072266\n",
      "Train Epoch: 218 [102656/225000 (46%)] Loss: 7258.617188\n",
      "Train Epoch: 218 [106752/225000 (47%)] Loss: 7162.814453\n",
      "Train Epoch: 218 [110848/225000 (49%)] Loss: 7219.746094\n",
      "Train Epoch: 218 [114944/225000 (51%)] Loss: 7092.095703\n",
      "Train Epoch: 218 [119040/225000 (53%)] Loss: 7101.238281\n",
      "Train Epoch: 218 [123136/225000 (55%)] Loss: 7229.013672\n",
      "Train Epoch: 218 [127232/225000 (57%)] Loss: 7319.765625\n",
      "Train Epoch: 218 [131328/225000 (58%)] Loss: 6968.828125\n",
      "Train Epoch: 218 [135424/225000 (60%)] Loss: 7120.294922\n",
      "Train Epoch: 218 [139520/225000 (62%)] Loss: 7205.484375\n",
      "Train Epoch: 218 [143616/225000 (64%)] Loss: 7269.193359\n",
      "Train Epoch: 218 [147712/225000 (66%)] Loss: 7202.576172\n",
      "Train Epoch: 218 [151808/225000 (67%)] Loss: 7090.611328\n",
      "Train Epoch: 218 [155904/225000 (69%)] Loss: 7071.207031\n",
      "Train Epoch: 218 [160000/225000 (71%)] Loss: 7181.019531\n",
      "Train Epoch: 218 [164096/225000 (73%)] Loss: 7217.169922\n",
      "Train Epoch: 218 [168192/225000 (75%)] Loss: 7187.218750\n",
      "Train Epoch: 218 [172288/225000 (77%)] Loss: 7158.640625\n",
      "Train Epoch: 218 [176384/225000 (78%)] Loss: 7181.232422\n",
      "Train Epoch: 218 [180480/225000 (80%)] Loss: 7172.111328\n",
      "Train Epoch: 218 [184576/225000 (82%)] Loss: 7316.833984\n",
      "Train Epoch: 218 [188672/225000 (84%)] Loss: 6943.966797\n",
      "Train Epoch: 218 [192768/225000 (86%)] Loss: 7080.560547\n",
      "Train Epoch: 218 [196864/225000 (87%)] Loss: 7208.339844\n",
      "Train Epoch: 218 [200960/225000 (89%)] Loss: 6968.906250\n",
      "Train Epoch: 218 [205056/225000 (91%)] Loss: 7107.296875\n",
      "Train Epoch: 218 [209152/225000 (93%)] Loss: 7224.099609\n",
      "Train Epoch: 218 [213248/225000 (95%)] Loss: 7230.525391\n",
      "Train Epoch: 218 [217344/225000 (97%)] Loss: 7167.320312\n",
      "Train Epoch: 218 [221440/225000 (98%)] Loss: 6998.599609\n",
      "    epoch          : 218\n",
      "    loss           : 7160.115181047355\n",
      "    val_loss       : 7123.740078315443\n",
      "Train Epoch: 219 [256/225000 (0%)] Loss: 7017.714844\n",
      "Train Epoch: 219 [4352/225000 (2%)] Loss: 7159.828125\n",
      "Train Epoch: 219 [8448/225000 (4%)] Loss: 7047.005859\n",
      "Train Epoch: 219 [12544/225000 (6%)] Loss: 7010.431641\n",
      "Train Epoch: 219 [16640/225000 (7%)] Loss: 7008.673828\n",
      "Train Epoch: 219 [20736/225000 (9%)] Loss: 7141.748047\n",
      "Train Epoch: 219 [24832/225000 (11%)] Loss: 7190.939453\n",
      "Train Epoch: 219 [28928/225000 (13%)] Loss: 7127.134766\n",
      "Train Epoch: 219 [33024/225000 (15%)] Loss: 7214.419922\n",
      "Train Epoch: 219 [37120/225000 (16%)] Loss: 7040.001953\n",
      "Train Epoch: 219 [41216/225000 (18%)] Loss: 7027.326172\n",
      "Train Epoch: 219 [45312/225000 (20%)] Loss: 7032.568359\n",
      "Train Epoch: 219 [49408/225000 (22%)] Loss: 7183.671875\n",
      "Train Epoch: 219 [53504/225000 (24%)] Loss: 7037.115234\n",
      "Train Epoch: 219 [57600/225000 (26%)] Loss: 7057.082031\n",
      "Train Epoch: 219 [61696/225000 (27%)] Loss: 7192.814453\n",
      "Train Epoch: 219 [65792/225000 (29%)] Loss: 7082.062500\n",
      "Train Epoch: 219 [69888/225000 (31%)] Loss: 6916.791016\n",
      "Train Epoch: 219 [73984/225000 (33%)] Loss: 7023.072266\n",
      "Train Epoch: 219 [78080/225000 (35%)] Loss: 7025.417969\n",
      "Train Epoch: 219 [82176/225000 (37%)] Loss: 7178.058594\n",
      "Train Epoch: 219 [86272/225000 (38%)] Loss: 7306.316406\n",
      "Train Epoch: 219 [90368/225000 (40%)] Loss: 7092.699219\n",
      "Train Epoch: 219 [94464/225000 (42%)] Loss: 7005.085938\n",
      "Train Epoch: 219 [98560/225000 (44%)] Loss: 6984.978516\n",
      "Train Epoch: 219 [102656/225000 (46%)] Loss: 7156.521484\n",
      "Train Epoch: 219 [106752/225000 (47%)] Loss: 7150.691406\n",
      "Train Epoch: 219 [110848/225000 (49%)] Loss: 7060.355469\n",
      "Train Epoch: 219 [114944/225000 (51%)] Loss: 7162.597656\n",
      "Train Epoch: 219 [119040/225000 (53%)] Loss: 7110.972656\n",
      "Train Epoch: 219 [123136/225000 (55%)] Loss: 7219.054688\n",
      "Train Epoch: 219 [127232/225000 (57%)] Loss: 7129.533203\n",
      "Train Epoch: 219 [131328/225000 (58%)] Loss: 7274.042969\n",
      "Train Epoch: 219 [135424/225000 (60%)] Loss: 7169.501953\n",
      "Train Epoch: 219 [139520/225000 (62%)] Loss: 7099.623047\n",
      "Train Epoch: 219 [143616/225000 (64%)] Loss: 7249.462891\n",
      "Train Epoch: 219 [147712/225000 (66%)] Loss: 7285.828125\n",
      "Train Epoch: 219 [151808/225000 (67%)] Loss: 6995.345703\n",
      "Train Epoch: 219 [155904/225000 (69%)] Loss: 7318.765625\n",
      "Train Epoch: 219 [160000/225000 (71%)] Loss: 7296.292969\n",
      "Train Epoch: 219 [164096/225000 (73%)] Loss: 7222.906250\n",
      "Train Epoch: 219 [168192/225000 (75%)] Loss: 6969.492188\n",
      "Train Epoch: 219 [172288/225000 (77%)] Loss: 7036.433594\n",
      "Train Epoch: 219 [176384/225000 (78%)] Loss: 6981.099609\n",
      "Train Epoch: 219 [180480/225000 (80%)] Loss: 7218.615234\n",
      "Train Epoch: 219 [184576/225000 (82%)] Loss: 7160.640625\n",
      "Train Epoch: 219 [188672/225000 (84%)] Loss: 7010.837891\n",
      "Train Epoch: 219 [192768/225000 (86%)] Loss: 7280.894531\n",
      "Train Epoch: 219 [196864/225000 (87%)] Loss: 7253.292969\n",
      "Train Epoch: 219 [200960/225000 (89%)] Loss: 7250.361328\n",
      "Train Epoch: 219 [205056/225000 (91%)] Loss: 7160.376953\n",
      "Train Epoch: 219 [209152/225000 (93%)] Loss: 7277.738281\n",
      "Train Epoch: 219 [213248/225000 (95%)] Loss: 6920.455078\n",
      "Train Epoch: 219 [217344/225000 (97%)] Loss: 7236.560547\n",
      "Train Epoch: 219 [221440/225000 (98%)] Loss: 7077.365234\n",
      "    epoch          : 219\n",
      "    loss           : 7128.691049621374\n",
      "    val_loss       : 7122.895006139668\n",
      "Train Epoch: 220 [256/225000 (0%)] Loss: 7300.072266\n",
      "Train Epoch: 220 [4352/225000 (2%)] Loss: 7207.359375\n",
      "Train Epoch: 220 [8448/225000 (4%)] Loss: 7074.058594\n",
      "Train Epoch: 220 [12544/225000 (6%)] Loss: 7326.894531\n",
      "Train Epoch: 220 [16640/225000 (7%)] Loss: 7093.349609\n",
      "Train Epoch: 220 [20736/225000 (9%)] Loss: 7005.447266\n",
      "Train Epoch: 220 [24832/225000 (11%)] Loss: 7202.320312\n",
      "Train Epoch: 220 [28928/225000 (13%)] Loss: 7161.626953\n",
      "Train Epoch: 220 [33024/225000 (15%)] Loss: 7181.824219\n",
      "Train Epoch: 220 [37120/225000 (16%)] Loss: 7232.437500\n",
      "Train Epoch: 220 [41216/225000 (18%)] Loss: 7054.273438\n",
      "Train Epoch: 220 [45312/225000 (20%)] Loss: 7178.691406\n",
      "Train Epoch: 220 [49408/225000 (22%)] Loss: 7090.160156\n",
      "Train Epoch: 220 [53504/225000 (24%)] Loss: 7068.060547\n",
      "Train Epoch: 220 [57600/225000 (26%)] Loss: 7076.177734\n",
      "Train Epoch: 220 [61696/225000 (27%)] Loss: 7081.119141\n",
      "Train Epoch: 220 [65792/225000 (29%)] Loss: 7066.751953\n",
      "Train Epoch: 220 [69888/225000 (31%)] Loss: 6980.136719\n",
      "Train Epoch: 220 [73984/225000 (33%)] Loss: 7219.271484\n",
      "Train Epoch: 220 [78080/225000 (35%)] Loss: 7144.634766\n",
      "Train Epoch: 220 [82176/225000 (37%)] Loss: 7112.316406\n",
      "Train Epoch: 220 [86272/225000 (38%)] Loss: 7123.957031\n",
      "Train Epoch: 220 [90368/225000 (40%)] Loss: 7126.708984\n",
      "Train Epoch: 220 [94464/225000 (42%)] Loss: 6976.332031\n",
      "Train Epoch: 220 [98560/225000 (44%)] Loss: 7137.646484\n",
      "Train Epoch: 220 [102656/225000 (46%)] Loss: 7037.429688\n",
      "Train Epoch: 220 [106752/225000 (47%)] Loss: 7114.244141\n",
      "Train Epoch: 220 [110848/225000 (49%)] Loss: 7063.517578\n",
      "Train Epoch: 220 [114944/225000 (51%)] Loss: 7233.724609\n",
      "Train Epoch: 220 [119040/225000 (53%)] Loss: 7144.857422\n",
      "Train Epoch: 220 [123136/225000 (55%)] Loss: 7176.535156\n",
      "Train Epoch: 220 [127232/225000 (57%)] Loss: 7205.919922\n",
      "Train Epoch: 220 [131328/225000 (58%)] Loss: 7144.970703\n",
      "Train Epoch: 220 [135424/225000 (60%)] Loss: 6848.451172\n",
      "Train Epoch: 220 [139520/225000 (62%)] Loss: 7102.490234\n",
      "Train Epoch: 220 [143616/225000 (64%)] Loss: 6967.763672\n",
      "Train Epoch: 220 [147712/225000 (66%)] Loss: 7310.515625\n",
      "Train Epoch: 220 [151808/225000 (67%)] Loss: 7206.123047\n",
      "Train Epoch: 220 [155904/225000 (69%)] Loss: 6975.210938\n",
      "Train Epoch: 220 [160000/225000 (71%)] Loss: 7128.533203\n",
      "Train Epoch: 220 [164096/225000 (73%)] Loss: 7016.019531\n",
      "Train Epoch: 220 [168192/225000 (75%)] Loss: 7113.947266\n",
      "Train Epoch: 220 [172288/225000 (77%)] Loss: 7164.886719\n",
      "Train Epoch: 220 [176384/225000 (78%)] Loss: 7048.435547\n",
      "Train Epoch: 220 [180480/225000 (80%)] Loss: 7027.443359\n",
      "Train Epoch: 220 [184576/225000 (82%)] Loss: 7002.785156\n",
      "Train Epoch: 220 [188672/225000 (84%)] Loss: 7119.835938\n",
      "Train Epoch: 220 [192768/225000 (86%)] Loss: 7187.539062\n",
      "Train Epoch: 220 [196864/225000 (87%)] Loss: 7236.654297\n",
      "Train Epoch: 220 [200960/225000 (89%)] Loss: 7163.806641\n",
      "Train Epoch: 220 [205056/225000 (91%)] Loss: 7205.142578\n",
      "Train Epoch: 220 [209152/225000 (93%)] Loss: 7144.371094\n",
      "Train Epoch: 220 [213248/225000 (95%)] Loss: 7158.316406\n",
      "Train Epoch: 220 [217344/225000 (97%)] Loss: 7025.369141\n",
      "Train Epoch: 220 [221440/225000 (98%)] Loss: 7036.541016\n",
      "    epoch          : 220\n",
      "    loss           : 7157.9314917519905\n",
      "    val_loss       : 7124.501212020918\n",
      "Train Epoch: 221 [256/225000 (0%)] Loss: 7201.722656\n",
      "Train Epoch: 221 [4352/225000 (2%)] Loss: 7076.097656\n",
      "Train Epoch: 221 [8448/225000 (4%)] Loss: 7073.871094\n",
      "Train Epoch: 221 [12544/225000 (6%)] Loss: 7015.236328\n",
      "Train Epoch: 221 [16640/225000 (7%)] Loss: 7098.207031\n",
      "Train Epoch: 221 [20736/225000 (9%)] Loss: 7260.601562\n",
      "Train Epoch: 221 [24832/225000 (11%)] Loss: 7150.667969\n",
      "Train Epoch: 221 [28928/225000 (13%)] Loss: 7178.763672\n",
      "Train Epoch: 221 [33024/225000 (15%)] Loss: 7146.650391\n",
      "Train Epoch: 221 [37120/225000 (16%)] Loss: 7145.925781\n",
      "Train Epoch: 221 [41216/225000 (18%)] Loss: 7081.013672\n",
      "Train Epoch: 221 [45312/225000 (20%)] Loss: 7192.373047\n",
      "Train Epoch: 221 [49408/225000 (22%)] Loss: 7202.496094\n",
      "Train Epoch: 221 [53504/225000 (24%)] Loss: 7195.732422\n",
      "Train Epoch: 221 [57600/225000 (26%)] Loss: 7011.103516\n",
      "Train Epoch: 221 [61696/225000 (27%)] Loss: 7184.947266\n",
      "Train Epoch: 221 [65792/225000 (29%)] Loss: 7314.910156\n",
      "Train Epoch: 221 [69888/225000 (31%)] Loss: 7185.677734\n",
      "Train Epoch: 221 [73984/225000 (33%)] Loss: 7021.572266\n",
      "Train Epoch: 221 [78080/225000 (35%)] Loss: 7155.830078\n",
      "Train Epoch: 221 [82176/225000 (37%)] Loss: 7339.943359\n",
      "Train Epoch: 221 [86272/225000 (38%)] Loss: 6996.980469\n",
      "Train Epoch: 221 [90368/225000 (40%)] Loss: 7133.398438\n",
      "Train Epoch: 221 [94464/225000 (42%)] Loss: 7061.935547\n",
      "Train Epoch: 221 [98560/225000 (44%)] Loss: 6949.056641\n",
      "Train Epoch: 221 [102656/225000 (46%)] Loss: 7093.744141\n",
      "Train Epoch: 221 [106752/225000 (47%)] Loss: 7312.367188\n",
      "Train Epoch: 221 [110848/225000 (49%)] Loss: 7151.388672\n",
      "Train Epoch: 221 [114944/225000 (51%)] Loss: 6953.492188\n",
      "Train Epoch: 221 [119040/225000 (53%)] Loss: 7124.894531\n",
      "Train Epoch: 221 [123136/225000 (55%)] Loss: 7085.158203\n",
      "Train Epoch: 221 [127232/225000 (57%)] Loss: 7149.566406\n",
      "Train Epoch: 221 [131328/225000 (58%)] Loss: 7005.105469\n",
      "Train Epoch: 221 [135424/225000 (60%)] Loss: 7095.691406\n",
      "Train Epoch: 221 [139520/225000 (62%)] Loss: 7153.767578\n",
      "Train Epoch: 221 [143616/225000 (64%)] Loss: 7032.736328\n",
      "Train Epoch: 221 [147712/225000 (66%)] Loss: 7167.220703\n",
      "Train Epoch: 221 [151808/225000 (67%)] Loss: 7218.300781\n",
      "Train Epoch: 221 [155904/225000 (69%)] Loss: 7022.585938\n",
      "Train Epoch: 221 [160000/225000 (71%)] Loss: 7139.455078\n",
      "Train Epoch: 221 [164096/225000 (73%)] Loss: 7258.578125\n",
      "Train Epoch: 221 [168192/225000 (75%)] Loss: 6993.320312\n",
      "Train Epoch: 221 [172288/225000 (77%)] Loss: 6970.421875\n",
      "Train Epoch: 221 [176384/225000 (78%)] Loss: 7191.929688\n",
      "Train Epoch: 221 [180480/225000 (80%)] Loss: 6998.447266\n",
      "Train Epoch: 221 [184576/225000 (82%)] Loss: 7076.652344\n",
      "Train Epoch: 221 [188672/225000 (84%)] Loss: 7176.861328\n",
      "Train Epoch: 221 [192768/225000 (86%)] Loss: 7035.701172\n",
      "Train Epoch: 221 [196864/225000 (87%)] Loss: 7081.902344\n",
      "Train Epoch: 221 [200960/225000 (89%)] Loss: 7166.718750\n",
      "Train Epoch: 221 [205056/225000 (91%)] Loss: 7183.730469\n",
      "Train Epoch: 221 [209152/225000 (93%)] Loss: 7083.621094\n",
      "Train Epoch: 221 [213248/225000 (95%)] Loss: 7225.042969\n",
      "Train Epoch: 221 [217344/225000 (97%)] Loss: 7043.761719\n",
      "Train Epoch: 221 [221440/225000 (98%)] Loss: 7267.691406\n",
      "    epoch          : 221\n",
      "    loss           : 7128.420338497227\n",
      "    val_loss       : 7122.25343648025\n",
      "Train Epoch: 222 [256/225000 (0%)] Loss: 7255.720703\n",
      "Train Epoch: 222 [4352/225000 (2%)] Loss: 7112.941406\n",
      "Train Epoch: 222 [8448/225000 (4%)] Loss: 6981.558594\n",
      "Train Epoch: 222 [12544/225000 (6%)] Loss: 7131.693359\n",
      "Train Epoch: 222 [16640/225000 (7%)] Loss: 7027.404297\n",
      "Train Epoch: 222 [20736/225000 (9%)] Loss: 7258.083984\n",
      "Train Epoch: 222 [24832/225000 (11%)] Loss: 27179.332031\n",
      "Train Epoch: 222 [28928/225000 (13%)] Loss: 7006.113281\n",
      "Train Epoch: 222 [33024/225000 (15%)] Loss: 7223.689453\n",
      "Train Epoch: 222 [37120/225000 (16%)] Loss: 7005.308594\n",
      "Train Epoch: 222 [41216/225000 (18%)] Loss: 7304.363281\n",
      "Train Epoch: 222 [45312/225000 (20%)] Loss: 7253.835938\n",
      "Train Epoch: 222 [49408/225000 (22%)] Loss: 7156.613281\n",
      "Train Epoch: 222 [53504/225000 (24%)] Loss: 7136.345703\n",
      "Train Epoch: 222 [57600/225000 (26%)] Loss: 7388.832031\n",
      "Train Epoch: 222 [61696/225000 (27%)] Loss: 7154.349609\n",
      "Train Epoch: 222 [65792/225000 (29%)] Loss: 6979.169922\n",
      "Train Epoch: 222 [69888/225000 (31%)] Loss: 7040.816406\n",
      "Train Epoch: 222 [73984/225000 (33%)] Loss: 7047.851562\n",
      "Train Epoch: 222 [78080/225000 (35%)] Loss: 6975.603516\n",
      "Train Epoch: 222 [82176/225000 (37%)] Loss: 6995.509766\n",
      "Train Epoch: 222 [86272/225000 (38%)] Loss: 7113.732422\n",
      "Train Epoch: 222 [90368/225000 (40%)] Loss: 7147.021484\n",
      "Train Epoch: 222 [94464/225000 (42%)] Loss: 6991.078125\n",
      "Train Epoch: 222 [98560/225000 (44%)] Loss: 7186.882812\n",
      "Train Epoch: 222 [102656/225000 (46%)] Loss: 7303.777344\n",
      "Train Epoch: 222 [106752/225000 (47%)] Loss: 7232.769531\n",
      "Train Epoch: 222 [110848/225000 (49%)] Loss: 7382.320312\n",
      "Train Epoch: 222 [114944/225000 (51%)] Loss: 7157.419922\n",
      "Train Epoch: 222 [119040/225000 (53%)] Loss: 7303.990234\n",
      "Train Epoch: 222 [123136/225000 (55%)] Loss: 7139.685547\n",
      "Train Epoch: 222 [127232/225000 (57%)] Loss: 6971.074219\n",
      "Train Epoch: 222 [131328/225000 (58%)] Loss: 7181.611328\n",
      "Train Epoch: 222 [135424/225000 (60%)] Loss: 7153.701172\n",
      "Train Epoch: 222 [139520/225000 (62%)] Loss: 7219.238281\n",
      "Train Epoch: 222 [143616/225000 (64%)] Loss: 7019.609375\n",
      "Train Epoch: 222 [147712/225000 (66%)] Loss: 7248.273438\n",
      "Train Epoch: 222 [151808/225000 (67%)] Loss: 6970.964844\n",
      "Train Epoch: 222 [155904/225000 (69%)] Loss: 7164.667969\n",
      "Train Epoch: 222 [160000/225000 (71%)] Loss: 7187.365234\n",
      "Train Epoch: 222 [164096/225000 (73%)] Loss: 7082.880859\n",
      "Train Epoch: 222 [168192/225000 (75%)] Loss: 7154.746094\n",
      "Train Epoch: 222 [172288/225000 (77%)] Loss: 6969.832031\n",
      "Train Epoch: 222 [176384/225000 (78%)] Loss: 7130.304688\n",
      "Train Epoch: 222 [180480/225000 (80%)] Loss: 7098.853516\n",
      "Train Epoch: 222 [184576/225000 (82%)] Loss: 7109.746094\n",
      "Train Epoch: 222 [188672/225000 (84%)] Loss: 7143.134766\n",
      "Train Epoch: 222 [192768/225000 (86%)] Loss: 7172.625000\n",
      "Train Epoch: 222 [196864/225000 (87%)] Loss: 7075.085938\n",
      "Train Epoch: 222 [200960/225000 (89%)] Loss: 7319.330078\n",
      "Train Epoch: 222 [205056/225000 (91%)] Loss: 7085.683594\n",
      "Train Epoch: 222 [209152/225000 (93%)] Loss: 7218.685547\n",
      "Train Epoch: 222 [213248/225000 (95%)] Loss: 6957.357422\n",
      "Train Epoch: 222 [217344/225000 (97%)] Loss: 7121.042969\n",
      "Train Epoch: 222 [221440/225000 (98%)] Loss: 7168.419922\n",
      "    epoch          : 222\n",
      "    loss           : 7154.862542439918\n",
      "    val_loss       : 7200.321225253903\n",
      "Train Epoch: 223 [256/225000 (0%)] Loss: 7089.740234\n",
      "Train Epoch: 223 [4352/225000 (2%)] Loss: 7234.843750\n",
      "Train Epoch: 223 [8448/225000 (4%)] Loss: 7152.189453\n",
      "Train Epoch: 223 [12544/225000 (6%)] Loss: 6959.539062\n",
      "Train Epoch: 223 [16640/225000 (7%)] Loss: 7229.292969\n",
      "Train Epoch: 223 [20736/225000 (9%)] Loss: 7099.019531\n",
      "Train Epoch: 223 [24832/225000 (11%)] Loss: 7148.136719\n",
      "Train Epoch: 223 [28928/225000 (13%)] Loss: 7132.378906\n",
      "Train Epoch: 223 [33024/225000 (15%)] Loss: 7208.068359\n",
      "Train Epoch: 223 [37120/225000 (16%)] Loss: 7036.597656\n",
      "Train Epoch: 223 [41216/225000 (18%)] Loss: 7105.335938\n",
      "Train Epoch: 223 [45312/225000 (20%)] Loss: 7079.949219\n",
      "Train Epoch: 223 [49408/225000 (22%)] Loss: 7015.908203\n",
      "Train Epoch: 223 [53504/225000 (24%)] Loss: 7041.402344\n",
      "Train Epoch: 223 [57600/225000 (26%)] Loss: 7020.732422\n",
      "Train Epoch: 223 [61696/225000 (27%)] Loss: 7041.976562\n",
      "Train Epoch: 223 [65792/225000 (29%)] Loss: 7023.787109\n",
      "Train Epoch: 223 [69888/225000 (31%)] Loss: 7094.302734\n",
      "Train Epoch: 223 [73984/225000 (33%)] Loss: 7113.595703\n",
      "Train Epoch: 223 [78080/225000 (35%)] Loss: 7113.125000\n",
      "Train Epoch: 223 [82176/225000 (37%)] Loss: 7178.009766\n",
      "Train Epoch: 223 [86272/225000 (38%)] Loss: 7082.343750\n",
      "Train Epoch: 223 [90368/225000 (40%)] Loss: 7080.789062\n",
      "Train Epoch: 223 [94464/225000 (42%)] Loss: 7198.437500\n",
      "Train Epoch: 223 [98560/225000 (44%)] Loss: 7236.160156\n",
      "Train Epoch: 223 [102656/225000 (46%)] Loss: 7234.585938\n",
      "Train Epoch: 223 [106752/225000 (47%)] Loss: 7172.730469\n",
      "Train Epoch: 223 [110848/225000 (49%)] Loss: 7278.630859\n",
      "Train Epoch: 223 [114944/225000 (51%)] Loss: 7109.228516\n",
      "Train Epoch: 223 [119040/225000 (53%)] Loss: 7146.617188\n",
      "Train Epoch: 223 [123136/225000 (55%)] Loss: 7263.294922\n",
      "Train Epoch: 223 [127232/225000 (57%)] Loss: 7065.599609\n",
      "Train Epoch: 223 [131328/225000 (58%)] Loss: 7165.789062\n",
      "Train Epoch: 223 [135424/225000 (60%)] Loss: 6975.876953\n",
      "Train Epoch: 223 [139520/225000 (62%)] Loss: 6959.845703\n",
      "Train Epoch: 223 [143616/225000 (64%)] Loss: 7033.978516\n",
      "Train Epoch: 223 [147712/225000 (66%)] Loss: 7057.179688\n",
      "Train Epoch: 223 [151808/225000 (67%)] Loss: 7136.187500\n",
      "Train Epoch: 223 [155904/225000 (69%)] Loss: 7074.958984\n",
      "Train Epoch: 223 [160000/225000 (71%)] Loss: 7026.515625\n",
      "Train Epoch: 223 [164096/225000 (73%)] Loss: 7247.044922\n",
      "Train Epoch: 223 [168192/225000 (75%)] Loss: 7174.968750\n",
      "Train Epoch: 223 [172288/225000 (77%)] Loss: 7061.417969\n",
      "Train Epoch: 223 [176384/225000 (78%)] Loss: 7307.744141\n",
      "Train Epoch: 223 [180480/225000 (80%)] Loss: 7198.369141\n",
      "Train Epoch: 223 [184576/225000 (82%)] Loss: 7174.478516\n",
      "Train Epoch: 223 [188672/225000 (84%)] Loss: 7016.351562\n",
      "Train Epoch: 223 [192768/225000 (86%)] Loss: 7047.861328\n",
      "Train Epoch: 223 [196864/225000 (87%)] Loss: 6938.867188\n",
      "Train Epoch: 223 [200960/225000 (89%)] Loss: 7131.302734\n",
      "Train Epoch: 223 [205056/225000 (91%)] Loss: 7064.306641\n",
      "Train Epoch: 223 [209152/225000 (93%)] Loss: 7259.236328\n",
      "Train Epoch: 223 [213248/225000 (95%)] Loss: 7032.416016\n",
      "Train Epoch: 223 [217344/225000 (97%)] Loss: 6897.201172\n",
      "Train Epoch: 223 [221440/225000 (98%)] Loss: 7078.519531\n",
      "    epoch          : 223\n",
      "    loss           : 7152.432506088239\n",
      "    val_loss       : 7116.617467484912\n",
      "Train Epoch: 224 [256/225000 (0%)] Loss: 7080.453125\n",
      "Train Epoch: 224 [4352/225000 (2%)] Loss: 7205.951172\n",
      "Train Epoch: 224 [8448/225000 (4%)] Loss: 7026.037109\n",
      "Train Epoch: 224 [12544/225000 (6%)] Loss: 7122.808594\n",
      "Train Epoch: 224 [16640/225000 (7%)] Loss: 7091.648438\n",
      "Train Epoch: 224 [20736/225000 (9%)] Loss: 7278.925781\n",
      "Train Epoch: 224 [24832/225000 (11%)] Loss: 7272.916016\n",
      "Train Epoch: 224 [28928/225000 (13%)] Loss: 7141.802734\n",
      "Train Epoch: 224 [33024/225000 (15%)] Loss: 6967.892578\n",
      "Train Epoch: 224 [37120/225000 (16%)] Loss: 7077.083984\n",
      "Train Epoch: 224 [41216/225000 (18%)] Loss: 7070.875000\n",
      "Train Epoch: 224 [45312/225000 (20%)] Loss: 6932.324219\n",
      "Train Epoch: 224 [49408/225000 (22%)] Loss: 6900.460938\n",
      "Train Epoch: 224 [53504/225000 (24%)] Loss: 7120.000000\n",
      "Train Epoch: 224 [57600/225000 (26%)] Loss: 7005.007812\n",
      "Train Epoch: 224 [61696/225000 (27%)] Loss: 7104.597656\n",
      "Train Epoch: 224 [65792/225000 (29%)] Loss: 7288.228516\n",
      "Train Epoch: 224 [69888/225000 (31%)] Loss: 7060.074219\n",
      "Train Epoch: 224 [73984/225000 (33%)] Loss: 7101.121094\n",
      "Train Epoch: 224 [78080/225000 (35%)] Loss: 7003.970703\n",
      "Train Epoch: 224 [82176/225000 (37%)] Loss: 7206.271484\n",
      "Train Epoch: 224 [86272/225000 (38%)] Loss: 7224.154297\n",
      "Train Epoch: 224 [90368/225000 (40%)] Loss: 6902.472656\n",
      "Train Epoch: 224 [94464/225000 (42%)] Loss: 7056.103516\n",
      "Train Epoch: 224 [98560/225000 (44%)] Loss: 7139.828125\n",
      "Train Epoch: 224 [102656/225000 (46%)] Loss: 7152.679688\n",
      "Train Epoch: 224 [106752/225000 (47%)] Loss: 6967.371094\n",
      "Train Epoch: 224 [110848/225000 (49%)] Loss: 7113.718750\n",
      "Train Epoch: 224 [114944/225000 (51%)] Loss: 7250.554688\n",
      "Train Epoch: 224 [119040/225000 (53%)] Loss: 7174.664062\n",
      "Train Epoch: 224 [123136/225000 (55%)] Loss: 7108.126953\n",
      "Train Epoch: 224 [127232/225000 (57%)] Loss: 7017.826172\n",
      "Train Epoch: 224 [131328/225000 (58%)] Loss: 7089.037109\n",
      "Train Epoch: 224 [135424/225000 (60%)] Loss: 7103.798828\n",
      "Train Epoch: 224 [139520/225000 (62%)] Loss: 7324.550781\n",
      "Train Epoch: 224 [143616/225000 (64%)] Loss: 7034.660156\n",
      "Train Epoch: 224 [147712/225000 (66%)] Loss: 7128.109375\n",
      "Train Epoch: 224 [151808/225000 (67%)] Loss: 7242.548828\n",
      "Train Epoch: 224 [155904/225000 (69%)] Loss: 6945.414062\n",
      "Train Epoch: 224 [160000/225000 (71%)] Loss: 7344.869141\n",
      "Train Epoch: 224 [164096/225000 (73%)] Loss: 7054.660156\n",
      "Train Epoch: 224 [168192/225000 (75%)] Loss: 7186.970703\n",
      "Train Epoch: 224 [172288/225000 (77%)] Loss: 7194.343750\n",
      "Train Epoch: 224 [176384/225000 (78%)] Loss: 7036.292969\n",
      "Train Epoch: 224 [180480/225000 (80%)] Loss: 7164.210938\n",
      "Train Epoch: 224 [184576/225000 (82%)] Loss: 7053.302734\n",
      "Train Epoch: 224 [188672/225000 (84%)] Loss: 7136.693359\n",
      "Train Epoch: 224 [192768/225000 (86%)] Loss: 7235.503906\n",
      "Train Epoch: 224 [196864/225000 (87%)] Loss: 7244.003906\n",
      "Train Epoch: 224 [200960/225000 (89%)] Loss: 7211.849609\n",
      "Train Epoch: 224 [205056/225000 (91%)] Loss: 7169.294922\n",
      "Train Epoch: 224 [209152/225000 (93%)] Loss: 7008.912109\n",
      "Train Epoch: 224 [213248/225000 (95%)] Loss: 7161.923828\n",
      "Train Epoch: 224 [217344/225000 (97%)] Loss: 7090.691406\n",
      "Train Epoch: 224 [221440/225000 (98%)] Loss: 6975.806641\n",
      "    epoch          : 224\n",
      "    loss           : 7109.28504959471\n",
      "    val_loss       : 7115.232764155281\n",
      "Train Epoch: 225 [256/225000 (0%)] Loss: 7004.332031\n",
      "Train Epoch: 225 [4352/225000 (2%)] Loss: 6957.568359\n",
      "Train Epoch: 225 [8448/225000 (4%)] Loss: 7175.191406\n",
      "Train Epoch: 225 [12544/225000 (6%)] Loss: 7081.757812\n",
      "Train Epoch: 225 [16640/225000 (7%)] Loss: 7161.908203\n",
      "Train Epoch: 225 [20736/225000 (9%)] Loss: 7103.957031\n",
      "Train Epoch: 225 [24832/225000 (11%)] Loss: 7118.210938\n",
      "Train Epoch: 225 [28928/225000 (13%)] Loss: 7105.230469\n",
      "Train Epoch: 225 [33024/225000 (15%)] Loss: 7004.998047\n",
      "Train Epoch: 225 [37120/225000 (16%)] Loss: 6959.031250\n",
      "Train Epoch: 225 [41216/225000 (18%)] Loss: 6988.025391\n",
      "Train Epoch: 225 [45312/225000 (20%)] Loss: 7036.669922\n",
      "Train Epoch: 225 [49408/225000 (22%)] Loss: 6951.685547\n",
      "Train Epoch: 225 [53504/225000 (24%)] Loss: 6997.111328\n",
      "Train Epoch: 225 [57600/225000 (26%)] Loss: 7086.302734\n",
      "Train Epoch: 225 [61696/225000 (27%)] Loss: 7000.179688\n",
      "Train Epoch: 225 [65792/225000 (29%)] Loss: 6949.689453\n",
      "Train Epoch: 225 [69888/225000 (31%)] Loss: 7060.681641\n",
      "Train Epoch: 225 [73984/225000 (33%)] Loss: 7055.648438\n",
      "Train Epoch: 225 [78080/225000 (35%)] Loss: 7262.671875\n",
      "Train Epoch: 225 [82176/225000 (37%)] Loss: 7012.123047\n",
      "Train Epoch: 225 [86272/225000 (38%)] Loss: 7066.265625\n",
      "Train Epoch: 225 [90368/225000 (40%)] Loss: 7173.261719\n",
      "Train Epoch: 225 [94464/225000 (42%)] Loss: 7067.763672\n",
      "Train Epoch: 225 [98560/225000 (44%)] Loss: 7119.207031\n",
      "Train Epoch: 225 [102656/225000 (46%)] Loss: 7102.326172\n",
      "Train Epoch: 225 [106752/225000 (47%)] Loss: 7091.208984\n",
      "Train Epoch: 225 [110848/225000 (49%)] Loss: 6971.787109\n",
      "Train Epoch: 225 [114944/225000 (51%)] Loss: 6902.228516\n",
      "Train Epoch: 225 [119040/225000 (53%)] Loss: 7099.669922\n",
      "Train Epoch: 225 [123136/225000 (55%)] Loss: 7162.515625\n",
      "Train Epoch: 225 [127232/225000 (57%)] Loss: 7232.472656\n",
      "Train Epoch: 225 [131328/225000 (58%)] Loss: 7148.087891\n",
      "Train Epoch: 225 [135424/225000 (60%)] Loss: 7103.908203\n",
      "Train Epoch: 225 [139520/225000 (62%)] Loss: 7055.873047\n",
      "Train Epoch: 225 [143616/225000 (64%)] Loss: 7191.423828\n",
      "Train Epoch: 225 [147712/225000 (66%)] Loss: 6969.759766\n",
      "Train Epoch: 225 [151808/225000 (67%)] Loss: 6995.751953\n",
      "Train Epoch: 225 [155904/225000 (69%)] Loss: 7067.056641\n",
      "Train Epoch: 225 [160000/225000 (71%)] Loss: 7057.746094\n",
      "Train Epoch: 225 [164096/225000 (73%)] Loss: 6909.509766\n",
      "Train Epoch: 225 [168192/225000 (75%)] Loss: 7161.978516\n",
      "Train Epoch: 225 [172288/225000 (77%)] Loss: 7134.556641\n",
      "Train Epoch: 225 [176384/225000 (78%)] Loss: 7187.267578\n",
      "Train Epoch: 225 [180480/225000 (80%)] Loss: 6949.873047\n",
      "Train Epoch: 225 [184576/225000 (82%)] Loss: 6997.742188\n",
      "Train Epoch: 225 [188672/225000 (84%)] Loss: 7077.250000\n",
      "Train Epoch: 225 [192768/225000 (86%)] Loss: 7216.035156\n",
      "Train Epoch: 225 [196864/225000 (87%)] Loss: 7228.056641\n",
      "Train Epoch: 225 [200960/225000 (89%)] Loss: 7227.224609\n",
      "Train Epoch: 225 [205056/225000 (91%)] Loss: 7010.080078\n",
      "Train Epoch: 225 [209152/225000 (93%)] Loss: 7285.578125\n",
      "Train Epoch: 225 [213248/225000 (95%)] Loss: 7042.732422\n",
      "Train Epoch: 225 [217344/225000 (97%)] Loss: 7156.205078\n",
      "Train Epoch: 225 [221440/225000 (98%)] Loss: 6994.439453\n",
      "    epoch          : 225\n",
      "    loss           : 7106.338375017775\n",
      "    val_loss       : 7119.2608611462065\n",
      "Train Epoch: 226 [256/225000 (0%)] Loss: 7150.517578\n",
      "Train Epoch: 226 [4352/225000 (2%)] Loss: 7063.326172\n",
      "Train Epoch: 226 [8448/225000 (4%)] Loss: 7097.462891\n",
      "Train Epoch: 226 [12544/225000 (6%)] Loss: 7089.390625\n",
      "Train Epoch: 226 [16640/225000 (7%)] Loss: 7015.457031\n",
      "Train Epoch: 226 [20736/225000 (9%)] Loss: 7093.267578\n",
      "Train Epoch: 226 [24832/225000 (11%)] Loss: 7022.046875\n",
      "Train Epoch: 226 [28928/225000 (13%)] Loss: 7053.691406\n",
      "Train Epoch: 226 [33024/225000 (15%)] Loss: 7072.117188\n",
      "Train Epoch: 226 [37120/225000 (16%)] Loss: 7056.923828\n",
      "Train Epoch: 226 [41216/225000 (18%)] Loss: 7055.382812\n",
      "Train Epoch: 226 [45312/225000 (20%)] Loss: 7128.333984\n",
      "Train Epoch: 226 [49408/225000 (22%)] Loss: 7114.542969\n",
      "Train Epoch: 226 [53504/225000 (24%)] Loss: 7092.287109\n",
      "Train Epoch: 226 [57600/225000 (26%)] Loss: 7166.013672\n",
      "Train Epoch: 226 [61696/225000 (27%)] Loss: 7057.929688\n",
      "Train Epoch: 226 [65792/225000 (29%)] Loss: 7017.591797\n",
      "Train Epoch: 226 [69888/225000 (31%)] Loss: 7128.123047\n",
      "Train Epoch: 226 [73984/225000 (33%)] Loss: 7223.646484\n",
      "Train Epoch: 226 [78080/225000 (35%)] Loss: 7136.535156\n",
      "Train Epoch: 226 [82176/225000 (37%)] Loss: 7054.851562\n",
      "Train Epoch: 226 [86272/225000 (38%)] Loss: 7181.037109\n",
      "Train Epoch: 226 [90368/225000 (40%)] Loss: 7227.109375\n",
      "Train Epoch: 226 [94464/225000 (42%)] Loss: 7253.974609\n",
      "Train Epoch: 226 [98560/225000 (44%)] Loss: 6963.734375\n",
      "Train Epoch: 226 [102656/225000 (46%)] Loss: 7026.796875\n",
      "Train Epoch: 226 [106752/225000 (47%)] Loss: 7152.244141\n",
      "Train Epoch: 226 [110848/225000 (49%)] Loss: 7213.000000\n",
      "Train Epoch: 226 [114944/225000 (51%)] Loss: 7073.013672\n",
      "Train Epoch: 226 [119040/225000 (53%)] Loss: 6958.054688\n",
      "Train Epoch: 226 [123136/225000 (55%)] Loss: 6992.855469\n",
      "Train Epoch: 226 [127232/225000 (57%)] Loss: 7111.656250\n",
      "Train Epoch: 226 [131328/225000 (58%)] Loss: 7148.287109\n",
      "Train Epoch: 226 [135424/225000 (60%)] Loss: 7197.847656\n",
      "Train Epoch: 226 [139520/225000 (62%)] Loss: 6995.441406\n",
      "Train Epoch: 226 [143616/225000 (64%)] Loss: 7288.244141\n",
      "Train Epoch: 226 [147712/225000 (66%)] Loss: 6997.867188\n",
      "Train Epoch: 226 [151808/225000 (67%)] Loss: 7212.613281\n",
      "Train Epoch: 226 [155904/225000 (69%)] Loss: 7116.607422\n",
      "Train Epoch: 226 [160000/225000 (71%)] Loss: 7008.259766\n",
      "Train Epoch: 226 [164096/225000 (73%)] Loss: 7094.761719\n",
      "Train Epoch: 226 [168192/225000 (75%)] Loss: 7180.349609\n",
      "Train Epoch: 226 [172288/225000 (77%)] Loss: 7021.312500\n",
      "Train Epoch: 226 [176384/225000 (78%)] Loss: 6902.666016\n",
      "Train Epoch: 226 [180480/225000 (80%)] Loss: 6959.082031\n",
      "Train Epoch: 226 [184576/225000 (82%)] Loss: 7082.585938\n",
      "Train Epoch: 226 [188672/225000 (84%)] Loss: 7072.500000\n",
      "Train Epoch: 226 [192768/225000 (86%)] Loss: 7036.093750\n",
      "Train Epoch: 226 [196864/225000 (87%)] Loss: 7072.910156\n",
      "Train Epoch: 226 [200960/225000 (89%)] Loss: 7120.992188\n",
      "Train Epoch: 226 [205056/225000 (91%)] Loss: 7153.416016\n",
      "Train Epoch: 226 [209152/225000 (93%)] Loss: 7033.335938\n",
      "Train Epoch: 226 [213248/225000 (95%)] Loss: 7118.707031\n",
      "Train Epoch: 226 [217344/225000 (97%)] Loss: 7121.283203\n",
      "Train Epoch: 226 [221440/225000 (98%)] Loss: 7051.492188\n",
      "    epoch          : 226\n",
      "    loss           : 7114.209592087955\n",
      "    val_loss       : 7105.0210392061545\n",
      "Train Epoch: 227 [256/225000 (0%)] Loss: 7138.363281\n",
      "Train Epoch: 227 [4352/225000 (2%)] Loss: 7067.000000\n",
      "Train Epoch: 227 [8448/225000 (4%)] Loss: 7334.050781\n",
      "Train Epoch: 227 [12544/225000 (6%)] Loss: 7060.335938\n",
      "Train Epoch: 227 [16640/225000 (7%)] Loss: 7059.117188\n",
      "Train Epoch: 227 [20736/225000 (9%)] Loss: 7338.750000\n",
      "Train Epoch: 227 [24832/225000 (11%)] Loss: 7185.435547\n",
      "Train Epoch: 227 [28928/225000 (13%)] Loss: 7025.937500\n",
      "Train Epoch: 227 [33024/225000 (15%)] Loss: 7192.783203\n",
      "Train Epoch: 227 [37120/225000 (16%)] Loss: 7156.003906\n",
      "Train Epoch: 227 [41216/225000 (18%)] Loss: 7021.683594\n",
      "Train Epoch: 227 [45312/225000 (20%)] Loss: 7193.490234\n",
      "Train Epoch: 227 [49408/225000 (22%)] Loss: 7067.902344\n",
      "Train Epoch: 227 [53504/225000 (24%)] Loss: 7094.046875\n",
      "Train Epoch: 227 [57600/225000 (26%)] Loss: 7136.832031\n",
      "Train Epoch: 227 [61696/225000 (27%)] Loss: 7175.400391\n",
      "Train Epoch: 227 [65792/225000 (29%)] Loss: 6905.277344\n",
      "Train Epoch: 227 [69888/225000 (31%)] Loss: 7097.601562\n",
      "Train Epoch: 227 [73984/225000 (33%)] Loss: 7207.367188\n",
      "Train Epoch: 227 [78080/225000 (35%)] Loss: 7188.753906\n",
      "Train Epoch: 227 [82176/225000 (37%)] Loss: 6936.501953\n",
      "Train Epoch: 227 [86272/225000 (38%)] Loss: 7158.419922\n",
      "Train Epoch: 227 [90368/225000 (40%)] Loss: 7155.328125\n",
      "Train Epoch: 227 [94464/225000 (42%)] Loss: 7001.324219\n",
      "Train Epoch: 227 [98560/225000 (44%)] Loss: 7115.318359\n",
      "Train Epoch: 227 [102656/225000 (46%)] Loss: 7074.798828\n",
      "Train Epoch: 227 [106752/225000 (47%)] Loss: 7176.638672\n",
      "Train Epoch: 227 [110848/225000 (49%)] Loss: 6966.105469\n",
      "Train Epoch: 227 [114944/225000 (51%)] Loss: 6866.351562\n",
      "Train Epoch: 227 [119040/225000 (53%)] Loss: 7109.482422\n",
      "Train Epoch: 227 [123136/225000 (55%)] Loss: 7138.625000\n",
      "Train Epoch: 227 [127232/225000 (57%)] Loss: 7099.466797\n",
      "Train Epoch: 227 [131328/225000 (58%)] Loss: 7031.806641\n",
      "Train Epoch: 227 [135424/225000 (60%)] Loss: 7183.078125\n",
      "Train Epoch: 227 [139520/225000 (62%)] Loss: 7175.148438\n",
      "Train Epoch: 227 [143616/225000 (64%)] Loss: 6965.462891\n",
      "Train Epoch: 227 [147712/225000 (66%)] Loss: 7156.419922\n",
      "Train Epoch: 227 [151808/225000 (67%)] Loss: 7075.917969\n",
      "Train Epoch: 227 [155904/225000 (69%)] Loss: 7043.189453\n",
      "Train Epoch: 227 [160000/225000 (71%)] Loss: 7215.207031\n",
      "Train Epoch: 227 [164096/225000 (73%)] Loss: 7051.664062\n",
      "Train Epoch: 227 [168192/225000 (75%)] Loss: 7128.361328\n",
      "Train Epoch: 227 [172288/225000 (77%)] Loss: 7157.156250\n",
      "Train Epoch: 227 [176384/225000 (78%)] Loss: 7013.902344\n",
      "Train Epoch: 227 [180480/225000 (80%)] Loss: 7169.007812\n",
      "Train Epoch: 227 [184576/225000 (82%)] Loss: 7260.388672\n",
      "Train Epoch: 227 [188672/225000 (84%)] Loss: 7213.750000\n",
      "Train Epoch: 227 [192768/225000 (86%)] Loss: 7118.123047\n",
      "Train Epoch: 227 [196864/225000 (87%)] Loss: 7035.332031\n",
      "Train Epoch: 227 [200960/225000 (89%)] Loss: 7231.712891\n",
      "Train Epoch: 227 [205056/225000 (91%)] Loss: 6971.367188\n",
      "Train Epoch: 227 [209152/225000 (93%)] Loss: 7066.660156\n",
      "Train Epoch: 227 [213248/225000 (95%)] Loss: 7021.082031\n",
      "Train Epoch: 227 [217344/225000 (97%)] Loss: 7104.570312\n",
      "Train Epoch: 227 [221440/225000 (98%)] Loss: 7215.781250\n",
      "    epoch          : 227\n",
      "    loss           : 7107.128678496516\n",
      "    val_loss       : 7103.635341846213\n",
      "Train Epoch: 228 [256/225000 (0%)] Loss: 7090.476562\n",
      "Train Epoch: 228 [4352/225000 (2%)] Loss: 7073.181641\n",
      "Train Epoch: 228 [8448/225000 (4%)] Loss: 6950.234375\n",
      "Train Epoch: 228 [12544/225000 (6%)] Loss: 7076.437500\n",
      "Train Epoch: 228 [16640/225000 (7%)] Loss: 7370.623047\n",
      "Train Epoch: 228 [20736/225000 (9%)] Loss: 7095.927734\n",
      "Train Epoch: 228 [24832/225000 (11%)] Loss: 7188.943359\n",
      "Train Epoch: 228 [28928/225000 (13%)] Loss: 7097.664062\n",
      "Train Epoch: 228 [33024/225000 (15%)] Loss: 7146.281250\n",
      "Train Epoch: 228 [37120/225000 (16%)] Loss: 7056.974609\n",
      "Train Epoch: 228 [41216/225000 (18%)] Loss: 7127.898438\n",
      "Train Epoch: 228 [45312/225000 (20%)] Loss: 7162.291016\n",
      "Train Epoch: 228 [49408/225000 (22%)] Loss: 7237.894531\n",
      "Train Epoch: 228 [53504/225000 (24%)] Loss: 7039.271484\n",
      "Train Epoch: 228 [57600/225000 (26%)] Loss: 7178.804688\n",
      "Train Epoch: 228 [61696/225000 (27%)] Loss: 7185.736328\n",
      "Train Epoch: 228 [65792/225000 (29%)] Loss: 7186.134766\n",
      "Train Epoch: 228 [69888/225000 (31%)] Loss: 6980.773438\n",
      "Train Epoch: 228 [73984/225000 (33%)] Loss: 7172.746094\n",
      "Train Epoch: 228 [78080/225000 (35%)] Loss: 7188.222656\n",
      "Train Epoch: 228 [82176/225000 (37%)] Loss: 7072.990234\n",
      "Train Epoch: 228 [86272/225000 (38%)] Loss: 7128.382812\n",
      "Train Epoch: 228 [90368/225000 (40%)] Loss: 7075.611328\n",
      "Train Epoch: 228 [94464/225000 (42%)] Loss: 7050.238281\n",
      "Train Epoch: 228 [98560/225000 (44%)] Loss: 7129.978516\n",
      "Train Epoch: 228 [102656/225000 (46%)] Loss: 7200.449219\n",
      "Train Epoch: 228 [106752/225000 (47%)] Loss: 7173.007812\n",
      "Train Epoch: 228 [110848/225000 (49%)] Loss: 7080.849609\n",
      "Train Epoch: 228 [114944/225000 (51%)] Loss: 7143.607422\n",
      "Train Epoch: 228 [119040/225000 (53%)] Loss: 7115.480469\n",
      "Train Epoch: 228 [123136/225000 (55%)] Loss: 7102.814453\n",
      "Train Epoch: 228 [127232/225000 (57%)] Loss: 7145.937500\n",
      "Train Epoch: 228 [131328/225000 (58%)] Loss: 7182.771484\n",
      "Train Epoch: 228 [135424/225000 (60%)] Loss: 7174.091797\n",
      "Train Epoch: 228 [139520/225000 (62%)] Loss: 7292.982422\n",
      "Train Epoch: 228 [143616/225000 (64%)] Loss: 7158.087891\n",
      "Train Epoch: 228 [147712/225000 (66%)] Loss: 7042.974609\n",
      "Train Epoch: 228 [151808/225000 (67%)] Loss: 6933.947266\n",
      "Train Epoch: 228 [155904/225000 (69%)] Loss: 7183.156250\n",
      "Train Epoch: 228 [160000/225000 (71%)] Loss: 7186.892578\n",
      "Train Epoch: 228 [164096/225000 (73%)] Loss: 7082.015625\n",
      "Train Epoch: 228 [168192/225000 (75%)] Loss: 7022.281250\n",
      "Train Epoch: 228 [172288/225000 (77%)] Loss: 7107.705078\n",
      "Train Epoch: 228 [176384/225000 (78%)] Loss: 6975.220703\n",
      "Train Epoch: 228 [180480/225000 (80%)] Loss: 7110.363281\n",
      "Train Epoch: 228 [184576/225000 (82%)] Loss: 7061.978516\n",
      "Train Epoch: 228 [188672/225000 (84%)] Loss: 6899.314453\n",
      "Train Epoch: 228 [192768/225000 (86%)] Loss: 7084.056641\n",
      "Train Epoch: 228 [196864/225000 (87%)] Loss: 7066.951172\n",
      "Train Epoch: 228 [200960/225000 (89%)] Loss: 7046.583984\n",
      "Train Epoch: 228 [205056/225000 (91%)] Loss: 7105.472656\n",
      "Train Epoch: 228 [209152/225000 (93%)] Loss: 7140.019531\n",
      "Train Epoch: 228 [213248/225000 (95%)] Loss: 7149.970703\n",
      "Train Epoch: 228 [217344/225000 (97%)] Loss: 7035.927734\n",
      "Train Epoch: 228 [221440/225000 (98%)] Loss: 7096.058594\n",
      "    epoch          : 228\n",
      "    loss           : 7110.030224553825\n",
      "    val_loss       : 7201.511054338241\n",
      "Train Epoch: 229 [256/225000 (0%)] Loss: 7184.666016\n",
      "Train Epoch: 229 [4352/225000 (2%)] Loss: 7187.240234\n",
      "Train Epoch: 229 [8448/225000 (4%)] Loss: 7102.972656\n",
      "Train Epoch: 229 [12544/225000 (6%)] Loss: 7065.714844\n",
      "Train Epoch: 229 [16640/225000 (7%)] Loss: 7147.927734\n",
      "Train Epoch: 229 [20736/225000 (9%)] Loss: 7097.130859\n",
      "Train Epoch: 229 [24832/225000 (11%)] Loss: 7054.937500\n",
      "Train Epoch: 229 [28928/225000 (13%)] Loss: 7108.001953\n",
      "Train Epoch: 229 [33024/225000 (15%)] Loss: 7153.044922\n",
      "Train Epoch: 229 [37120/225000 (16%)] Loss: 6982.417969\n",
      "Train Epoch: 229 [41216/225000 (18%)] Loss: 7282.654297\n",
      "Train Epoch: 229 [45312/225000 (20%)] Loss: 7241.343750\n",
      "Train Epoch: 229 [49408/225000 (22%)] Loss: 7035.507812\n",
      "Train Epoch: 229 [53504/225000 (24%)] Loss: 7100.988281\n",
      "Train Epoch: 229 [57600/225000 (26%)] Loss: 7064.265625\n",
      "Train Epoch: 229 [61696/225000 (27%)] Loss: 7042.880859\n",
      "Train Epoch: 229 [65792/225000 (29%)] Loss: 7031.375000\n",
      "Train Epoch: 229 [69888/225000 (31%)] Loss: 7021.343750\n",
      "Train Epoch: 229 [73984/225000 (33%)] Loss: 7129.666016\n",
      "Train Epoch: 229 [78080/225000 (35%)] Loss: 7171.941406\n",
      "Train Epoch: 229 [82176/225000 (37%)] Loss: 7129.101562\n",
      "Train Epoch: 229 [86272/225000 (38%)] Loss: 7244.246094\n",
      "Train Epoch: 229 [90368/225000 (40%)] Loss: 7084.527344\n",
      "Train Epoch: 229 [94464/225000 (42%)] Loss: 7079.257812\n",
      "Train Epoch: 229 [98560/225000 (44%)] Loss: 7044.375000\n",
      "Train Epoch: 229 [102656/225000 (46%)] Loss: 7239.537109\n",
      "Train Epoch: 229 [106752/225000 (47%)] Loss: 7111.804688\n",
      "Train Epoch: 229 [110848/225000 (49%)] Loss: 7122.554688\n",
      "Train Epoch: 229 [114944/225000 (51%)] Loss: 7102.242188\n",
      "Train Epoch: 229 [119040/225000 (53%)] Loss: 7090.039062\n",
      "Train Epoch: 229 [123136/225000 (55%)] Loss: 7132.998047\n",
      "Train Epoch: 229 [127232/225000 (57%)] Loss: 7180.595703\n",
      "Train Epoch: 229 [131328/225000 (58%)] Loss: 7202.216797\n",
      "Train Epoch: 229 [135424/225000 (60%)] Loss: 7228.933594\n",
      "Train Epoch: 229 [139520/225000 (62%)] Loss: 7338.527344\n",
      "Train Epoch: 229 [143616/225000 (64%)] Loss: 7107.785156\n",
      "Train Epoch: 229 [147712/225000 (66%)] Loss: 7012.955078\n",
      "Train Epoch: 229 [151808/225000 (67%)] Loss: 7057.558594\n",
      "Train Epoch: 229 [155904/225000 (69%)] Loss: 7064.078125\n",
      "Train Epoch: 229 [160000/225000 (71%)] Loss: 7090.019531\n",
      "Train Epoch: 229 [164096/225000 (73%)] Loss: 7011.078125\n",
      "Train Epoch: 229 [168192/225000 (75%)] Loss: 7006.759766\n",
      "Train Epoch: 229 [172288/225000 (77%)] Loss: 6973.449219\n",
      "Train Epoch: 229 [176384/225000 (78%)] Loss: 7179.955078\n",
      "Train Epoch: 229 [180480/225000 (80%)] Loss: 7100.751953\n",
      "Train Epoch: 229 [184576/225000 (82%)] Loss: 6990.064453\n",
      "Train Epoch: 229 [188672/225000 (84%)] Loss: 7154.574219\n",
      "Train Epoch: 229 [192768/225000 (86%)] Loss: 7206.431641\n",
      "Train Epoch: 229 [196864/225000 (87%)] Loss: 7112.933594\n",
      "Train Epoch: 229 [200960/225000 (89%)] Loss: 6984.000000\n",
      "Train Epoch: 229 [205056/225000 (91%)] Loss: 7260.265625\n",
      "Train Epoch: 229 [209152/225000 (93%)] Loss: 6942.148438\n",
      "Train Epoch: 229 [213248/225000 (95%)] Loss: 7022.968750\n",
      "Train Epoch: 229 [217344/225000 (97%)] Loss: 7264.880859\n",
      "Train Epoch: 229 [221440/225000 (98%)] Loss: 7109.146484\n",
      "    epoch          : 229\n",
      "    loss           : 7106.551707817833\n",
      "    val_loss       : 7100.348102549509\n",
      "Train Epoch: 230 [256/225000 (0%)] Loss: 7177.902344\n",
      "Train Epoch: 230 [4352/225000 (2%)] Loss: 7005.542969\n",
      "Train Epoch: 230 [8448/225000 (4%)] Loss: 7252.683594\n",
      "Train Epoch: 230 [12544/225000 (6%)] Loss: 7189.166016\n",
      "Train Epoch: 230 [16640/225000 (7%)] Loss: 7267.712891\n",
      "Train Epoch: 230 [20736/225000 (9%)] Loss: 6882.031250\n",
      "Train Epoch: 230 [24832/225000 (11%)] Loss: 7038.628906\n",
      "Train Epoch: 230 [28928/225000 (13%)] Loss: 7250.244141\n",
      "Train Epoch: 230 [33024/225000 (15%)] Loss: 7213.171875\n",
      "Train Epoch: 230 [37120/225000 (16%)] Loss: 7069.429688\n",
      "Train Epoch: 230 [41216/225000 (18%)] Loss: 7117.064453\n",
      "Train Epoch: 230 [45312/225000 (20%)] Loss: 7188.189453\n",
      "Train Epoch: 230 [49408/225000 (22%)] Loss: 6952.046875\n",
      "Train Epoch: 230 [53504/225000 (24%)] Loss: 7164.691406\n",
      "Train Epoch: 230 [57600/225000 (26%)] Loss: 7139.781250\n",
      "Train Epoch: 230 [61696/225000 (27%)] Loss: 6931.447266\n",
      "Train Epoch: 230 [65792/225000 (29%)] Loss: 7062.498047\n",
      "Train Epoch: 230 [69888/225000 (31%)] Loss: 7052.074219\n",
      "Train Epoch: 230 [73984/225000 (33%)] Loss: 7238.962891\n",
      "Train Epoch: 230 [78080/225000 (35%)] Loss: 7019.925781\n",
      "Train Epoch: 230 [82176/225000 (37%)] Loss: 7164.384766\n",
      "Train Epoch: 230 [86272/225000 (38%)] Loss: 7042.136719\n",
      "Train Epoch: 230 [90368/225000 (40%)] Loss: 7113.726562\n",
      "Train Epoch: 230 [94464/225000 (42%)] Loss: 7062.511719\n",
      "Train Epoch: 230 [98560/225000 (44%)] Loss: 7061.890625\n",
      "Train Epoch: 230 [102656/225000 (46%)] Loss: 7082.664062\n",
      "Train Epoch: 230 [106752/225000 (47%)] Loss: 7138.648438\n",
      "Train Epoch: 230 [110848/225000 (49%)] Loss: 6943.908203\n",
      "Train Epoch: 230 [114944/225000 (51%)] Loss: 6976.210938\n",
      "Train Epoch: 230 [119040/225000 (53%)] Loss: 7084.154297\n",
      "Train Epoch: 230 [123136/225000 (55%)] Loss: 7074.306641\n",
      "Train Epoch: 230 [127232/225000 (57%)] Loss: 7295.595703\n",
      "Train Epoch: 230 [131328/225000 (58%)] Loss: 7072.988281\n",
      "Train Epoch: 230 [135424/225000 (60%)] Loss: 7080.082031\n",
      "Train Epoch: 230 [139520/225000 (62%)] Loss: 6932.269531\n",
      "Train Epoch: 230 [143616/225000 (64%)] Loss: 7035.220703\n",
      "Train Epoch: 230 [147712/225000 (66%)] Loss: 7209.546875\n",
      "Train Epoch: 230 [151808/225000 (67%)] Loss: 7132.025391\n",
      "Train Epoch: 230 [155904/225000 (69%)] Loss: 7071.900391\n",
      "Train Epoch: 230 [160000/225000 (71%)] Loss: 7091.466797\n",
      "Train Epoch: 230 [164096/225000 (73%)] Loss: 7097.007812\n",
      "Train Epoch: 230 [168192/225000 (75%)] Loss: 7075.675781\n",
      "Train Epoch: 230 [172288/225000 (77%)] Loss: 7096.488281\n",
      "Train Epoch: 230 [176384/225000 (78%)] Loss: 7198.992188\n",
      "Train Epoch: 230 [180480/225000 (80%)] Loss: 7378.207031\n",
      "Train Epoch: 230 [184576/225000 (82%)] Loss: 7168.160156\n",
      "Train Epoch: 230 [188672/225000 (84%)] Loss: 7220.318359\n",
      "Train Epoch: 230 [192768/225000 (86%)] Loss: 7144.728516\n",
      "Train Epoch: 230 [196864/225000 (87%)] Loss: 6907.154297\n",
      "Train Epoch: 230 [200960/225000 (89%)] Loss: 7169.425781\n",
      "Train Epoch: 230 [205056/225000 (91%)] Loss: 7059.511719\n",
      "Train Epoch: 230 [209152/225000 (93%)] Loss: 6987.869141\n",
      "Train Epoch: 230 [213248/225000 (95%)] Loss: 7052.050781\n",
      "Train Epoch: 230 [217344/225000 (97%)] Loss: 7115.375000\n",
      "Train Epoch: 230 [221440/225000 (98%)] Loss: 7096.966797\n",
      "    epoch          : 230\n",
      "    loss           : 7137.632828053896\n",
      "    val_loss       : 7096.299648980705\n",
      "Train Epoch: 231 [256/225000 (0%)] Loss: 7077.296875\n",
      "Train Epoch: 231 [4352/225000 (2%)] Loss: 7132.757812\n",
      "Train Epoch: 231 [8448/225000 (4%)] Loss: 7017.597656\n",
      "Train Epoch: 231 [12544/225000 (6%)] Loss: 7224.585938\n",
      "Train Epoch: 231 [16640/225000 (7%)] Loss: 7058.576172\n",
      "Train Epoch: 231 [20736/225000 (9%)] Loss: 7176.960938\n",
      "Train Epoch: 231 [24832/225000 (11%)] Loss: 7122.453125\n",
      "Train Epoch: 231 [28928/225000 (13%)] Loss: 7073.755859\n",
      "Train Epoch: 231 [33024/225000 (15%)] Loss: 7023.566406\n",
      "Train Epoch: 231 [37120/225000 (16%)] Loss: 7002.738281\n",
      "Train Epoch: 231 [41216/225000 (18%)] Loss: 15678.783203\n",
      "Train Epoch: 231 [45312/225000 (20%)] Loss: 6919.222656\n",
      "Train Epoch: 231 [49408/225000 (22%)] Loss: 7136.835938\n",
      "Train Epoch: 231 [53504/225000 (24%)] Loss: 7105.208984\n",
      "Train Epoch: 231 [57600/225000 (26%)] Loss: 7074.416016\n",
      "Train Epoch: 231 [61696/225000 (27%)] Loss: 7031.896484\n",
      "Train Epoch: 231 [65792/225000 (29%)] Loss: 6997.662109\n",
      "Train Epoch: 231 [69888/225000 (31%)] Loss: 7138.558594\n",
      "Train Epoch: 231 [73984/225000 (33%)] Loss: 7134.828125\n",
      "Train Epoch: 231 [78080/225000 (35%)] Loss: 7066.238281\n",
      "Train Epoch: 231 [82176/225000 (37%)] Loss: 7005.646484\n",
      "Train Epoch: 231 [86272/225000 (38%)] Loss: 7114.708984\n",
      "Train Epoch: 231 [90368/225000 (40%)] Loss: 6951.072266\n",
      "Train Epoch: 231 [94464/225000 (42%)] Loss: 7037.947266\n",
      "Train Epoch: 231 [98560/225000 (44%)] Loss: 7244.630859\n",
      "Train Epoch: 231 [102656/225000 (46%)] Loss: 6996.173828\n",
      "Train Epoch: 231 [106752/225000 (47%)] Loss: 7296.289062\n",
      "Train Epoch: 231 [110848/225000 (49%)] Loss: 7095.947266\n",
      "Train Epoch: 231 [114944/225000 (51%)] Loss: 7167.396484\n",
      "Train Epoch: 231 [119040/225000 (53%)] Loss: 6966.234375\n",
      "Train Epoch: 231 [123136/225000 (55%)] Loss: 7215.843750\n",
      "Train Epoch: 231 [127232/225000 (57%)] Loss: 6949.601562\n",
      "Train Epoch: 231 [131328/225000 (58%)] Loss: 6897.560547\n",
      "Train Epoch: 231 [135424/225000 (60%)] Loss: 6964.158203\n",
      "Train Epoch: 231 [139520/225000 (62%)] Loss: 7058.109375\n",
      "Train Epoch: 231 [143616/225000 (64%)] Loss: 6986.255859\n",
      "Train Epoch: 231 [147712/225000 (66%)] Loss: 7045.130859\n",
      "Train Epoch: 231 [151808/225000 (67%)] Loss: 7060.019531\n",
      "Train Epoch: 231 [155904/225000 (69%)] Loss: 6959.478516\n",
      "Train Epoch: 231 [160000/225000 (71%)] Loss: 7182.117188\n",
      "Train Epoch: 231 [164096/225000 (73%)] Loss: 7102.767578\n",
      "Train Epoch: 231 [168192/225000 (75%)] Loss: 7111.617188\n",
      "Train Epoch: 231 [172288/225000 (77%)] Loss: 7123.757812\n",
      "Train Epoch: 231 [176384/225000 (78%)] Loss: 7043.585938\n",
      "Train Epoch: 231 [180480/225000 (80%)] Loss: 6913.357422\n",
      "Train Epoch: 231 [184576/225000 (82%)] Loss: 6946.226562\n",
      "Train Epoch: 231 [188672/225000 (84%)] Loss: 7039.986328\n",
      "Train Epoch: 231 [192768/225000 (86%)] Loss: 7188.710938\n",
      "Train Epoch: 231 [196864/225000 (87%)] Loss: 7148.666016\n",
      "Train Epoch: 231 [200960/225000 (89%)] Loss: 7255.119141\n",
      "Train Epoch: 231 [205056/225000 (91%)] Loss: 7020.945312\n",
      "Train Epoch: 231 [209152/225000 (93%)] Loss: 7185.369141\n",
      "Train Epoch: 231 [213248/225000 (95%)] Loss: 7153.298828\n",
      "Train Epoch: 231 [217344/225000 (97%)] Loss: 6980.185547\n",
      "Train Epoch: 231 [221440/225000 (98%)] Loss: 7044.689453\n",
      "    epoch          : 231\n",
      "    loss           : 7100.697172301621\n",
      "    val_loss       : 7094.05482835429\n",
      "Train Epoch: 232 [256/225000 (0%)] Loss: 7136.771484\n",
      "Train Epoch: 232 [4352/225000 (2%)] Loss: 7054.304688\n",
      "Train Epoch: 232 [8448/225000 (4%)] Loss: 7096.250000\n",
      "Train Epoch: 232 [12544/225000 (6%)] Loss: 6936.892578\n",
      "Train Epoch: 232 [16640/225000 (7%)] Loss: 7091.310547\n",
      "Train Epoch: 232 [20736/225000 (9%)] Loss: 7391.755859\n",
      "Train Epoch: 232 [24832/225000 (11%)] Loss: 7121.117188\n",
      "Train Epoch: 232 [28928/225000 (13%)] Loss: 7123.648438\n",
      "Train Epoch: 232 [33024/225000 (15%)] Loss: 6851.404297\n",
      "Train Epoch: 232 [37120/225000 (16%)] Loss: 7095.490234\n",
      "Train Epoch: 232 [41216/225000 (18%)] Loss: 7024.636719\n",
      "Train Epoch: 232 [45312/225000 (20%)] Loss: 6997.841797\n",
      "Train Epoch: 232 [49408/225000 (22%)] Loss: 7184.833984\n",
      "Train Epoch: 232 [53504/225000 (24%)] Loss: 7208.078125\n",
      "Train Epoch: 232 [57600/225000 (26%)] Loss: 7002.236328\n",
      "Train Epoch: 232 [61696/225000 (27%)] Loss: 7156.273438\n",
      "Train Epoch: 232 [65792/225000 (29%)] Loss: 7080.140625\n",
      "Train Epoch: 232 [69888/225000 (31%)] Loss: 7031.345703\n",
      "Train Epoch: 232 [73984/225000 (33%)] Loss: 7070.304688\n",
      "Train Epoch: 232 [78080/225000 (35%)] Loss: 7087.386719\n",
      "Train Epoch: 232 [82176/225000 (37%)] Loss: 7000.560547\n",
      "Train Epoch: 232 [86272/225000 (38%)] Loss: 7078.042969\n",
      "Train Epoch: 232 [90368/225000 (40%)] Loss: 7232.529297\n",
      "Train Epoch: 232 [94464/225000 (42%)] Loss: 6950.386719\n",
      "Train Epoch: 232 [98560/225000 (44%)] Loss: 7116.091797\n",
      "Train Epoch: 232 [102656/225000 (46%)] Loss: 7111.564453\n",
      "Train Epoch: 232 [106752/225000 (47%)] Loss: 7163.919922\n",
      "Train Epoch: 232 [110848/225000 (49%)] Loss: 7144.492188\n",
      "Train Epoch: 232 [114944/225000 (51%)] Loss: 6912.136719\n",
      "Train Epoch: 232 [119040/225000 (53%)] Loss: 7071.097656\n",
      "Train Epoch: 232 [123136/225000 (55%)] Loss: 7158.863281\n",
      "Train Epoch: 232 [127232/225000 (57%)] Loss: 7202.042969\n",
      "Train Epoch: 232 [131328/225000 (58%)] Loss: 7051.767578\n",
      "Train Epoch: 232 [135424/225000 (60%)] Loss: 7158.718750\n",
      "Train Epoch: 232 [139520/225000 (62%)] Loss: 6986.091797\n",
      "Train Epoch: 232 [143616/225000 (64%)] Loss: 7053.400391\n",
      "Train Epoch: 232 [147712/225000 (66%)] Loss: 7185.957031\n",
      "Train Epoch: 232 [151808/225000 (67%)] Loss: 7306.183594\n",
      "Train Epoch: 232 [155904/225000 (69%)] Loss: 7067.986328\n",
      "Train Epoch: 232 [160000/225000 (71%)] Loss: 7169.011719\n",
      "Train Epoch: 232 [164096/225000 (73%)] Loss: 7263.082031\n",
      "Train Epoch: 232 [168192/225000 (75%)] Loss: 6989.613281\n",
      "Train Epoch: 232 [172288/225000 (77%)] Loss: 7139.570312\n",
      "Train Epoch: 232 [176384/225000 (78%)] Loss: 7072.132812\n",
      "Train Epoch: 232 [180480/225000 (80%)] Loss: 7035.744141\n",
      "Train Epoch: 232 [184576/225000 (82%)] Loss: 6999.125000\n",
      "Train Epoch: 232 [188672/225000 (84%)] Loss: 7138.109375\n",
      "Train Epoch: 232 [192768/225000 (86%)] Loss: 7092.058594\n",
      "Train Epoch: 232 [196864/225000 (87%)] Loss: 7019.716797\n",
      "Train Epoch: 232 [200960/225000 (89%)] Loss: 6911.625000\n",
      "Train Epoch: 232 [205056/225000 (91%)] Loss: 6972.248047\n",
      "Train Epoch: 232 [209152/225000 (93%)] Loss: 7070.015625\n",
      "Train Epoch: 232 [213248/225000 (95%)] Loss: 7134.626953\n",
      "Train Epoch: 232 [217344/225000 (97%)] Loss: 7009.814453\n",
      "Train Epoch: 232 [221440/225000 (98%)] Loss: 7035.095703\n",
      "    epoch          : 232\n",
      "    loss           : 7096.001836470777\n",
      "    val_loss       : 7092.081570111975\n",
      "Train Epoch: 233 [256/225000 (0%)] Loss: 7089.134766\n",
      "Train Epoch: 233 [4352/225000 (2%)] Loss: 7130.523438\n",
      "Train Epoch: 233 [8448/225000 (4%)] Loss: 7167.867188\n",
      "Train Epoch: 233 [12544/225000 (6%)] Loss: 6993.585938\n",
      "Train Epoch: 233 [16640/225000 (7%)] Loss: 7119.560547\n",
      "Train Epoch: 233 [20736/225000 (9%)] Loss: 7217.890625\n",
      "Train Epoch: 233 [24832/225000 (11%)] Loss: 7010.169922\n",
      "Train Epoch: 233 [28928/225000 (13%)] Loss: 7023.537109\n",
      "Train Epoch: 233 [33024/225000 (15%)] Loss: 7076.810547\n",
      "Train Epoch: 233 [37120/225000 (16%)] Loss: 6959.078125\n",
      "Train Epoch: 233 [41216/225000 (18%)] Loss: 6822.339844\n",
      "Train Epoch: 233 [45312/225000 (20%)] Loss: 6956.333984\n",
      "Train Epoch: 233 [49408/225000 (22%)] Loss: 7219.408203\n",
      "Train Epoch: 233 [53504/225000 (24%)] Loss: 7190.533203\n",
      "Train Epoch: 233 [57600/225000 (26%)] Loss: 7161.052734\n",
      "Train Epoch: 233 [61696/225000 (27%)] Loss: 7095.767578\n",
      "Train Epoch: 233 [65792/225000 (29%)] Loss: 7012.804688\n",
      "Train Epoch: 233 [69888/225000 (31%)] Loss: 7075.236328\n",
      "Train Epoch: 233 [73984/225000 (33%)] Loss: 7046.958984\n",
      "Train Epoch: 233 [78080/225000 (35%)] Loss: 7077.736328\n",
      "Train Epoch: 233 [82176/225000 (37%)] Loss: 7067.431641\n",
      "Train Epoch: 233 [86272/225000 (38%)] Loss: 6967.035156\n",
      "Train Epoch: 233 [90368/225000 (40%)] Loss: 7108.521484\n",
      "Train Epoch: 233 [94464/225000 (42%)] Loss: 7122.289062\n",
      "Train Epoch: 233 [98560/225000 (44%)] Loss: 7027.876953\n",
      "Train Epoch: 233 [102656/225000 (46%)] Loss: 7147.787109\n",
      "Train Epoch: 233 [106752/225000 (47%)] Loss: 7068.750000\n",
      "Train Epoch: 233 [110848/225000 (49%)] Loss: 7198.730469\n",
      "Train Epoch: 233 [114944/225000 (51%)] Loss: 7206.369141\n",
      "Train Epoch: 233 [119040/225000 (53%)] Loss: 7173.855469\n",
      "Train Epoch: 233 [123136/225000 (55%)] Loss: 7105.822266\n",
      "Train Epoch: 233 [127232/225000 (57%)] Loss: 6960.972656\n",
      "Train Epoch: 233 [131328/225000 (58%)] Loss: 7178.632812\n",
      "Train Epoch: 233 [135424/225000 (60%)] Loss: 7028.345703\n",
      "Train Epoch: 233 [139520/225000 (62%)] Loss: 7079.400391\n",
      "Train Epoch: 233 [143616/225000 (64%)] Loss: 7008.611328\n",
      "Train Epoch: 233 [147712/225000 (66%)] Loss: 7078.908203\n",
      "Train Epoch: 233 [151808/225000 (67%)] Loss: 7119.443359\n",
      "Train Epoch: 233 [155904/225000 (69%)] Loss: 7238.904297\n",
      "Train Epoch: 233 [160000/225000 (71%)] Loss: 7084.060547\n",
      "Train Epoch: 233 [164096/225000 (73%)] Loss: 7120.917969\n",
      "Train Epoch: 233 [168192/225000 (75%)] Loss: 7087.023438\n",
      "Train Epoch: 233 [172288/225000 (77%)] Loss: 7013.951172\n",
      "Train Epoch: 233 [176384/225000 (78%)] Loss: 7143.640625\n",
      "Train Epoch: 233 [180480/225000 (80%)] Loss: 7119.744141\n",
      "Train Epoch: 233 [184576/225000 (82%)] Loss: 7008.572266\n",
      "Train Epoch: 233 [188672/225000 (84%)] Loss: 7173.232422\n",
      "Train Epoch: 233 [192768/225000 (86%)] Loss: 7068.052734\n",
      "Train Epoch: 233 [196864/225000 (87%)] Loss: 6966.853516\n",
      "Train Epoch: 233 [200960/225000 (89%)] Loss: 7070.287109\n",
      "Train Epoch: 233 [205056/225000 (91%)] Loss: 7120.917969\n",
      "Train Epoch: 233 [209152/225000 (93%)] Loss: 6955.560547\n",
      "Train Epoch: 233 [213248/225000 (95%)] Loss: 7018.232422\n",
      "Train Epoch: 233 [217344/225000 (97%)] Loss: 7093.941406\n",
      "Train Epoch: 233 [221440/225000 (98%)] Loss: 7163.890625\n",
      "    epoch          : 233\n",
      "    loss           : 7086.643570241396\n",
      "    val_loss       : 7091.243727019855\n",
      "Train Epoch: 234 [256/225000 (0%)] Loss: 7031.498047\n",
      "Train Epoch: 234 [4352/225000 (2%)] Loss: 7216.017578\n",
      "Train Epoch: 234 [8448/225000 (4%)] Loss: 7139.160156\n",
      "Train Epoch: 234 [12544/225000 (6%)] Loss: 6946.367188\n",
      "Train Epoch: 234 [16640/225000 (7%)] Loss: 7139.087891\n",
      "Train Epoch: 234 [20736/225000 (9%)] Loss: 7013.837891\n",
      "Train Epoch: 234 [24832/225000 (11%)] Loss: 7095.996094\n",
      "Train Epoch: 234 [28928/225000 (13%)] Loss: 7132.736328\n",
      "Train Epoch: 234 [33024/225000 (15%)] Loss: 7077.042969\n",
      "Train Epoch: 234 [37120/225000 (16%)] Loss: 7043.625000\n",
      "Train Epoch: 234 [41216/225000 (18%)] Loss: 7059.179688\n",
      "Train Epoch: 234 [45312/225000 (20%)] Loss: 7105.917969\n",
      "Train Epoch: 234 [49408/225000 (22%)] Loss: 7165.972656\n",
      "Train Epoch: 234 [53504/225000 (24%)] Loss: 6988.798828\n",
      "Train Epoch: 234 [57600/225000 (26%)] Loss: 7051.390625\n",
      "Train Epoch: 234 [61696/225000 (27%)] Loss: 6954.886719\n",
      "Train Epoch: 234 [65792/225000 (29%)] Loss: 7119.152344\n",
      "Train Epoch: 234 [69888/225000 (31%)] Loss: 6992.583984\n",
      "Train Epoch: 234 [73984/225000 (33%)] Loss: 7071.896484\n",
      "Train Epoch: 234 [78080/225000 (35%)] Loss: 7078.925781\n",
      "Train Epoch: 234 [82176/225000 (37%)] Loss: 7160.894531\n",
      "Train Epoch: 234 [86272/225000 (38%)] Loss: 7097.906250\n",
      "Train Epoch: 234 [90368/225000 (40%)] Loss: 7212.208984\n",
      "Train Epoch: 234 [94464/225000 (42%)] Loss: 7117.464844\n",
      "Train Epoch: 234 [98560/225000 (44%)] Loss: 7052.482422\n",
      "Train Epoch: 234 [102656/225000 (46%)] Loss: 6954.492188\n",
      "Train Epoch: 234 [106752/225000 (47%)] Loss: 7026.169922\n",
      "Train Epoch: 234 [110848/225000 (49%)] Loss: 7142.943359\n",
      "Train Epoch: 234 [114944/225000 (51%)] Loss: 7078.181641\n",
      "Train Epoch: 234 [119040/225000 (53%)] Loss: 6940.802734\n",
      "Train Epoch: 234 [123136/225000 (55%)] Loss: 7016.343750\n",
      "Train Epoch: 234 [127232/225000 (57%)] Loss: 7069.523438\n",
      "Train Epoch: 234 [131328/225000 (58%)] Loss: 7063.271484\n",
      "Train Epoch: 234 [135424/225000 (60%)] Loss: 7235.138672\n",
      "Train Epoch: 234 [139520/225000 (62%)] Loss: 6970.152344\n",
      "Train Epoch: 234 [143616/225000 (64%)] Loss: 7217.345703\n",
      "Train Epoch: 234 [147712/225000 (66%)] Loss: 6984.107422\n",
      "Train Epoch: 234 [151808/225000 (67%)] Loss: 7101.371094\n",
      "Train Epoch: 234 [155904/225000 (69%)] Loss: 6975.873047\n",
      "Train Epoch: 234 [160000/225000 (71%)] Loss: 7099.205078\n",
      "Train Epoch: 234 [164096/225000 (73%)] Loss: 7188.787109\n",
      "Train Epoch: 234 [168192/225000 (75%)] Loss: 7048.117188\n",
      "Train Epoch: 234 [172288/225000 (77%)] Loss: 7044.382812\n",
      "Train Epoch: 234 [176384/225000 (78%)] Loss: 7028.958984\n",
      "Train Epoch: 234 [180480/225000 (80%)] Loss: 6975.406250\n",
      "Train Epoch: 234 [184576/225000 (82%)] Loss: 6995.492188\n",
      "Train Epoch: 234 [188672/225000 (84%)] Loss: 7138.966797\n",
      "Train Epoch: 234 [192768/225000 (86%)] Loss: 7013.185547\n",
      "Train Epoch: 234 [196864/225000 (87%)] Loss: 7146.566406\n",
      "Train Epoch: 234 [200960/225000 (89%)] Loss: 7112.187500\n",
      "Train Epoch: 234 [205056/225000 (91%)] Loss: 7034.191406\n",
      "Train Epoch: 234 [209152/225000 (93%)] Loss: 7114.814453\n",
      "Train Epoch: 234 [213248/225000 (95%)] Loss: 7187.666016\n",
      "Train Epoch: 234 [217344/225000 (97%)] Loss: 6958.574219\n",
      "Train Epoch: 234 [221440/225000 (98%)] Loss: 7155.363281\n",
      "    epoch          : 234\n",
      "    loss           : 7084.135781072241\n",
      "    val_loss       : 7085.173164732602\n",
      "Train Epoch: 235 [256/225000 (0%)] Loss: 7264.359375\n",
      "Train Epoch: 235 [4352/225000 (2%)] Loss: 7139.109375\n",
      "Train Epoch: 235 [8448/225000 (4%)] Loss: 7031.951172\n",
      "Train Epoch: 235 [12544/225000 (6%)] Loss: 6939.029297\n",
      "Train Epoch: 235 [16640/225000 (7%)] Loss: 7057.242188\n",
      "Train Epoch: 235 [20736/225000 (9%)] Loss: 7037.626953\n",
      "Train Epoch: 235 [24832/225000 (11%)] Loss: 6984.572266\n",
      "Train Epoch: 235 [28928/225000 (13%)] Loss: 7125.669922\n",
      "Train Epoch: 235 [33024/225000 (15%)] Loss: 7158.695312\n",
      "Train Epoch: 235 [37120/225000 (16%)] Loss: 7033.919922\n",
      "Train Epoch: 235 [41216/225000 (18%)] Loss: 7115.216797\n",
      "Train Epoch: 235 [45312/225000 (20%)] Loss: 6898.332031\n",
      "Train Epoch: 235 [49408/225000 (22%)] Loss: 7205.531250\n",
      "Train Epoch: 235 [53504/225000 (24%)] Loss: 7048.826172\n",
      "Train Epoch: 235 [57600/225000 (26%)] Loss: 7110.498047\n",
      "Train Epoch: 235 [61696/225000 (27%)] Loss: 7097.378906\n",
      "Train Epoch: 235 [65792/225000 (29%)] Loss: 7148.394531\n",
      "Train Epoch: 235 [69888/225000 (31%)] Loss: 7113.482422\n",
      "Train Epoch: 235 [73984/225000 (33%)] Loss: 7050.789062\n",
      "Train Epoch: 235 [78080/225000 (35%)] Loss: 7027.560547\n",
      "Train Epoch: 235 [82176/225000 (37%)] Loss: 7021.365234\n",
      "Train Epoch: 235 [86272/225000 (38%)] Loss: 7134.736328\n",
      "Train Epoch: 235 [90368/225000 (40%)] Loss: 6995.156250\n",
      "Train Epoch: 235 [94464/225000 (42%)] Loss: 7035.634766\n",
      "Train Epoch: 235 [98560/225000 (44%)] Loss: 7070.583984\n",
      "Train Epoch: 235 [102656/225000 (46%)] Loss: 7127.048828\n",
      "Train Epoch: 235 [106752/225000 (47%)] Loss: 7137.560547\n",
      "Train Epoch: 235 [110848/225000 (49%)] Loss: 7142.480469\n",
      "Train Epoch: 235 [114944/225000 (51%)] Loss: 6947.970703\n",
      "Train Epoch: 235 [119040/225000 (53%)] Loss: 6946.117188\n",
      "Train Epoch: 235 [123136/225000 (55%)] Loss: 7144.054688\n",
      "Train Epoch: 235 [127232/225000 (57%)] Loss: 7110.597656\n",
      "Train Epoch: 235 [131328/225000 (58%)] Loss: 7102.636719\n",
      "Train Epoch: 235 [135424/225000 (60%)] Loss: 7173.968750\n",
      "Train Epoch: 235 [139520/225000 (62%)] Loss: 7148.996094\n",
      "Train Epoch: 235 [143616/225000 (64%)] Loss: 7064.144531\n",
      "Train Epoch: 235 [147712/225000 (66%)] Loss: 7161.474609\n",
      "Train Epoch: 235 [151808/225000 (67%)] Loss: 7072.585938\n",
      "Train Epoch: 235 [155904/225000 (69%)] Loss: 7007.441406\n",
      "Train Epoch: 235 [160000/225000 (71%)] Loss: 7064.390625\n",
      "Train Epoch: 235 [164096/225000 (73%)] Loss: 7009.400391\n",
      "Train Epoch: 235 [168192/225000 (75%)] Loss: 6948.433594\n",
      "Train Epoch: 235 [172288/225000 (77%)] Loss: 6982.443359\n",
      "Train Epoch: 235 [176384/225000 (78%)] Loss: 7076.748047\n",
      "Train Epoch: 235 [180480/225000 (80%)] Loss: 7058.611328\n",
      "Train Epoch: 235 [184576/225000 (82%)] Loss: 7005.617188\n",
      "Train Epoch: 235 [188672/225000 (84%)] Loss: 6998.289062\n",
      "Train Epoch: 235 [192768/225000 (86%)] Loss: 7096.988281\n",
      "Train Epoch: 235 [196864/225000 (87%)] Loss: 7119.181641\n",
      "Train Epoch: 235 [200960/225000 (89%)] Loss: 7187.732422\n",
      "Train Epoch: 235 [205056/225000 (91%)] Loss: 7238.265625\n",
      "Train Epoch: 235 [209152/225000 (93%)] Loss: 7177.832031\n",
      "Train Epoch: 235 [213248/225000 (95%)] Loss: 7127.478516\n",
      "Train Epoch: 235 [217344/225000 (97%)] Loss: 7173.357422\n",
      "Train Epoch: 235 [221440/225000 (98%)] Loss: 7223.906250\n",
      "    epoch          : 235\n",
      "    loss           : 7092.086719638794\n",
      "    val_loss       : 7145.768878486691\n",
      "Train Epoch: 236 [256/225000 (0%)] Loss: 7065.867188\n",
      "Train Epoch: 236 [4352/225000 (2%)] Loss: 7214.730469\n",
      "Train Epoch: 236 [8448/225000 (4%)] Loss: 6972.314453\n",
      "Train Epoch: 236 [12544/225000 (6%)] Loss: 7154.539062\n",
      "Train Epoch: 236 [16640/225000 (7%)] Loss: 7167.458984\n",
      "Train Epoch: 236 [20736/225000 (9%)] Loss: 7127.724609\n",
      "Train Epoch: 236 [24832/225000 (11%)] Loss: 7194.201172\n",
      "Train Epoch: 236 [28928/225000 (13%)] Loss: 7201.689453\n",
      "Train Epoch: 236 [33024/225000 (15%)] Loss: 6950.076172\n",
      "Train Epoch: 236 [37120/225000 (16%)] Loss: 6989.550781\n",
      "Train Epoch: 236 [41216/225000 (18%)] Loss: 7116.033203\n",
      "Train Epoch: 236 [45312/225000 (20%)] Loss: 7098.736328\n",
      "Train Epoch: 236 [49408/225000 (22%)] Loss: 7069.994141\n",
      "Train Epoch: 236 [53504/225000 (24%)] Loss: 7179.933594\n",
      "Train Epoch: 236 [57600/225000 (26%)] Loss: 7053.164062\n",
      "Train Epoch: 236 [61696/225000 (27%)] Loss: 7045.074219\n",
      "Train Epoch: 236 [65792/225000 (29%)] Loss: 6997.402344\n",
      "Train Epoch: 236 [69888/225000 (31%)] Loss: 7077.791016\n",
      "Train Epoch: 236 [73984/225000 (33%)] Loss: 7140.019531\n",
      "Train Epoch: 236 [78080/225000 (35%)] Loss: 6969.429688\n",
      "Train Epoch: 236 [82176/225000 (37%)] Loss: 6989.667969\n",
      "Train Epoch: 236 [86272/225000 (38%)] Loss: 7022.263672\n",
      "Train Epoch: 236 [90368/225000 (40%)] Loss: 7004.123047\n",
      "Train Epoch: 236 [94464/225000 (42%)] Loss: 7063.933594\n",
      "Train Epoch: 236 [98560/225000 (44%)] Loss: 7141.945312\n",
      "Train Epoch: 236 [102656/225000 (46%)] Loss: 7051.417969\n",
      "Train Epoch: 236 [106752/225000 (47%)] Loss: 7265.066406\n",
      "Train Epoch: 236 [110848/225000 (49%)] Loss: 7229.873047\n",
      "Train Epoch: 236 [114944/225000 (51%)] Loss: 7030.750000\n",
      "Train Epoch: 236 [119040/225000 (53%)] Loss: 7188.318359\n",
      "Train Epoch: 236 [123136/225000 (55%)] Loss: 7105.115234\n",
      "Train Epoch: 236 [127232/225000 (57%)] Loss: 7234.623047\n",
      "Train Epoch: 236 [131328/225000 (58%)] Loss: 7056.529297\n",
      "Train Epoch: 236 [135424/225000 (60%)] Loss: 6948.470703\n",
      "Train Epoch: 236 [139520/225000 (62%)] Loss: 6989.859375\n",
      "Train Epoch: 236 [143616/225000 (64%)] Loss: 7100.726562\n",
      "Train Epoch: 236 [147712/225000 (66%)] Loss: 7054.871094\n",
      "Train Epoch: 236 [151808/225000 (67%)] Loss: 7240.257812\n",
      "Train Epoch: 236 [155904/225000 (69%)] Loss: 6999.083984\n",
      "Train Epoch: 236 [160000/225000 (71%)] Loss: 7027.308594\n",
      "Train Epoch: 236 [164096/225000 (73%)] Loss: 7088.386719\n",
      "Train Epoch: 236 [168192/225000 (75%)] Loss: 7075.533203\n",
      "Train Epoch: 236 [172288/225000 (77%)] Loss: 7080.259766\n",
      "Train Epoch: 236 [176384/225000 (78%)] Loss: 7267.847656\n",
      "Train Epoch: 236 [180480/225000 (80%)] Loss: 7032.279297\n",
      "Train Epoch: 236 [184576/225000 (82%)] Loss: 7132.636719\n",
      "Train Epoch: 236 [188672/225000 (84%)] Loss: 7088.882812\n",
      "Train Epoch: 236 [192768/225000 (86%)] Loss: 7153.201172\n",
      "Train Epoch: 236 [196864/225000 (87%)] Loss: 7215.164062\n",
      "Train Epoch: 236 [200960/225000 (89%)] Loss: 7115.583984\n",
      "Train Epoch: 236 [205056/225000 (91%)] Loss: 7033.404297\n",
      "Train Epoch: 236 [209152/225000 (93%)] Loss: 7237.169922\n",
      "Train Epoch: 236 [213248/225000 (95%)] Loss: 7130.216797\n",
      "Train Epoch: 236 [217344/225000 (97%)] Loss: 6979.363281\n",
      "Train Epoch: 236 [221440/225000 (98%)] Loss: 7322.761719\n",
      "    epoch          : 236\n",
      "    loss           : 7079.39009172355\n",
      "    val_loss       : 7079.4841456267295\n",
      "Train Epoch: 237 [256/225000 (0%)] Loss: 6935.712891\n",
      "Train Epoch: 237 [4352/225000 (2%)] Loss: 7103.091797\n",
      "Train Epoch: 237 [8448/225000 (4%)] Loss: 7163.902344\n",
      "Train Epoch: 237 [12544/225000 (6%)] Loss: 6945.734375\n",
      "Train Epoch: 237 [16640/225000 (7%)] Loss: 6983.121094\n",
      "Train Epoch: 237 [20736/225000 (9%)] Loss: 7053.431641\n",
      "Train Epoch: 237 [24832/225000 (11%)] Loss: 7076.251953\n",
      "Train Epoch: 237 [28928/225000 (13%)] Loss: 7033.447266\n",
      "Train Epoch: 237 [33024/225000 (15%)] Loss: 7168.404297\n",
      "Train Epoch: 237 [37120/225000 (16%)] Loss: 7053.857422\n",
      "Train Epoch: 237 [41216/225000 (18%)] Loss: 6947.181641\n",
      "Train Epoch: 237 [45312/225000 (20%)] Loss: 7009.201172\n",
      "Train Epoch: 237 [49408/225000 (22%)] Loss: 6961.492188\n",
      "Train Epoch: 237 [53504/225000 (24%)] Loss: 7033.755859\n",
      "Train Epoch: 237 [57600/225000 (26%)] Loss: 7137.259766\n",
      "Train Epoch: 237 [61696/225000 (27%)] Loss: 7133.847656\n",
      "Train Epoch: 237 [65792/225000 (29%)] Loss: 6967.410156\n",
      "Train Epoch: 237 [69888/225000 (31%)] Loss: 7031.785156\n",
      "Train Epoch: 237 [73984/225000 (33%)] Loss: 7140.742188\n",
      "Train Epoch: 237 [78080/225000 (35%)] Loss: 7102.714844\n",
      "Train Epoch: 237 [82176/225000 (37%)] Loss: 7008.277344\n",
      "Train Epoch: 237 [86272/225000 (38%)] Loss: 7013.859375\n",
      "Train Epoch: 237 [90368/225000 (40%)] Loss: 6862.058594\n",
      "Train Epoch: 237 [94464/225000 (42%)] Loss: 7062.121094\n",
      "Train Epoch: 237 [98560/225000 (44%)] Loss: 7030.490234\n",
      "Train Epoch: 237 [102656/225000 (46%)] Loss: 7090.208984\n",
      "Train Epoch: 237 [106752/225000 (47%)] Loss: 7080.291016\n",
      "Train Epoch: 237 [110848/225000 (49%)] Loss: 7102.744141\n",
      "Train Epoch: 237 [114944/225000 (51%)] Loss: 7045.646484\n",
      "Train Epoch: 237 [119040/225000 (53%)] Loss: 7085.199219\n",
      "Train Epoch: 237 [123136/225000 (55%)] Loss: 7061.386719\n",
      "Train Epoch: 237 [127232/225000 (57%)] Loss: 7046.703125\n",
      "Train Epoch: 237 [131328/225000 (58%)] Loss: 6989.869141\n",
      "Train Epoch: 237 [135424/225000 (60%)] Loss: 7211.562500\n",
      "Train Epoch: 237 [139520/225000 (62%)] Loss: 7154.947266\n",
      "Train Epoch: 237 [143616/225000 (64%)] Loss: 7096.357422\n",
      "Train Epoch: 237 [147712/225000 (66%)] Loss: 6997.289062\n",
      "Train Epoch: 237 [151808/225000 (67%)] Loss: 7151.859375\n",
      "Train Epoch: 237 [155904/225000 (69%)] Loss: 7133.447266\n",
      "Train Epoch: 237 [160000/225000 (71%)] Loss: 7085.285156\n",
      "Train Epoch: 237 [164096/225000 (73%)] Loss: 7026.009766\n",
      "Train Epoch: 237 [168192/225000 (75%)] Loss: 6982.738281\n",
      "Train Epoch: 237 [172288/225000 (77%)] Loss: 7223.437500\n",
      "Train Epoch: 237 [176384/225000 (78%)] Loss: 7172.937500\n",
      "Train Epoch: 237 [180480/225000 (80%)] Loss: 7160.669922\n",
      "Train Epoch: 237 [184576/225000 (82%)] Loss: 7001.423828\n",
      "Train Epoch: 237 [188672/225000 (84%)] Loss: 6952.976562\n",
      "Train Epoch: 237 [192768/225000 (86%)] Loss: 7091.328125\n",
      "Train Epoch: 237 [196864/225000 (87%)] Loss: 7048.121094\n",
      "Train Epoch: 237 [200960/225000 (89%)] Loss: 7225.101562\n",
      "Train Epoch: 237 [205056/225000 (91%)] Loss: 7158.777344\n",
      "Train Epoch: 237 [209152/225000 (93%)] Loss: 7040.529297\n",
      "Train Epoch: 237 [213248/225000 (95%)] Loss: 6997.076172\n",
      "Train Epoch: 237 [217344/225000 (97%)] Loss: 7046.101562\n",
      "Train Epoch: 237 [221440/225000 (98%)] Loss: 7022.550781\n",
      "    epoch          : 237\n",
      "    loss           : 7094.328168328711\n",
      "    val_loss       : 7080.3373606636815\n",
      "Train Epoch: 238 [256/225000 (0%)] Loss: 7078.599609\n",
      "Train Epoch: 238 [4352/225000 (2%)] Loss: 7103.253906\n",
      "Train Epoch: 238 [8448/225000 (4%)] Loss: 7122.515625\n",
      "Train Epoch: 238 [12544/225000 (6%)] Loss: 7226.097656\n",
      "Train Epoch: 238 [16640/225000 (7%)] Loss: 7043.246094\n",
      "Train Epoch: 238 [20736/225000 (9%)] Loss: 7197.623047\n",
      "Train Epoch: 238 [24832/225000 (11%)] Loss: 7144.111328\n",
      "Train Epoch: 238 [28928/225000 (13%)] Loss: 7054.539062\n",
      "Train Epoch: 238 [33024/225000 (15%)] Loss: 7135.574219\n",
      "Train Epoch: 238 [37120/225000 (16%)] Loss: 7093.388672\n",
      "Train Epoch: 238 [41216/225000 (18%)] Loss: 7102.566406\n",
      "Train Epoch: 238 [45312/225000 (20%)] Loss: 7103.515625\n",
      "Train Epoch: 238 [49408/225000 (22%)] Loss: 7251.878906\n",
      "Train Epoch: 238 [53504/225000 (24%)] Loss: 7057.769531\n",
      "Train Epoch: 238 [57600/225000 (26%)] Loss: 7000.914062\n",
      "Train Epoch: 238 [61696/225000 (27%)] Loss: 6960.431641\n",
      "Train Epoch: 238 [65792/225000 (29%)] Loss: 7210.437500\n",
      "Train Epoch: 238 [69888/225000 (31%)] Loss: 7025.970703\n",
      "Train Epoch: 238 [73984/225000 (33%)] Loss: 6954.697266\n",
      "Train Epoch: 238 [78080/225000 (35%)] Loss: 7140.568359\n",
      "Train Epoch: 238 [82176/225000 (37%)] Loss: 7005.759766\n",
      "Train Epoch: 238 [86272/225000 (38%)] Loss: 7059.947266\n",
      "Train Epoch: 238 [90368/225000 (40%)] Loss: 7097.958984\n",
      "Train Epoch: 238 [94464/225000 (42%)] Loss: 7027.216797\n",
      "Train Epoch: 238 [98560/225000 (44%)] Loss: 6951.365234\n",
      "Train Epoch: 238 [102656/225000 (46%)] Loss: 6980.128906\n",
      "Train Epoch: 238 [106752/225000 (47%)] Loss: 7024.722656\n",
      "Train Epoch: 238 [110848/225000 (49%)] Loss: 7112.859375\n",
      "Train Epoch: 238 [114944/225000 (51%)] Loss: 7060.417969\n",
      "Train Epoch: 238 [119040/225000 (53%)] Loss: 7026.033203\n",
      "Train Epoch: 238 [123136/225000 (55%)] Loss: 7062.857422\n",
      "Train Epoch: 238 [127232/225000 (57%)] Loss: 7308.771484\n",
      "Train Epoch: 238 [131328/225000 (58%)] Loss: 6959.568359\n",
      "Train Epoch: 238 [135424/225000 (60%)] Loss: 7188.394531\n",
      "Train Epoch: 238 [139520/225000 (62%)] Loss: 7177.992188\n",
      "Train Epoch: 238 [143616/225000 (64%)] Loss: 7031.398438\n",
      "Train Epoch: 238 [147712/225000 (66%)] Loss: 7171.009766\n",
      "Train Epoch: 238 [151808/225000 (67%)] Loss: 7101.615234\n",
      "Train Epoch: 238 [155904/225000 (69%)] Loss: 6937.531250\n",
      "Train Epoch: 238 [160000/225000 (71%)] Loss: 7038.769531\n",
      "Train Epoch: 238 [164096/225000 (73%)] Loss: 7016.714844\n",
      "Train Epoch: 238 [168192/225000 (75%)] Loss: 6977.699219\n",
      "Train Epoch: 238 [172288/225000 (77%)] Loss: 7049.460938\n",
      "Train Epoch: 238 [176384/225000 (78%)] Loss: 7166.925781\n",
      "Train Epoch: 238 [180480/225000 (80%)] Loss: 6893.433594\n",
      "Train Epoch: 238 [184576/225000 (82%)] Loss: 7090.076172\n",
      "Train Epoch: 238 [188672/225000 (84%)] Loss: 6984.593750\n",
      "Train Epoch: 238 [192768/225000 (86%)] Loss: 6989.646484\n",
      "Train Epoch: 238 [196864/225000 (87%)] Loss: 7145.259766\n",
      "Train Epoch: 238 [200960/225000 (89%)] Loss: 7019.500000\n",
      "Train Epoch: 238 [205056/225000 (91%)] Loss: 6915.230469\n",
      "Train Epoch: 238 [209152/225000 (93%)] Loss: 7096.240234\n",
      "Train Epoch: 238 [213248/225000 (95%)] Loss: 7007.976562\n",
      "Train Epoch: 238 [217344/225000 (97%)] Loss: 6992.386719\n",
      "Train Epoch: 238 [221440/225000 (98%)] Loss: 7160.330078\n",
      "    epoch          : 238\n",
      "    loss           : 7084.634568979309\n",
      "    val_loss       : 7076.440287591243\n",
      "Train Epoch: 239 [256/225000 (0%)] Loss: 7015.205078\n",
      "Train Epoch: 239 [4352/225000 (2%)] Loss: 7038.035156\n",
      "Train Epoch: 239 [8448/225000 (4%)] Loss: 7046.542969\n",
      "Train Epoch: 239 [12544/225000 (6%)] Loss: 7085.583984\n",
      "Train Epoch: 239 [16640/225000 (7%)] Loss: 6944.810547\n",
      "Train Epoch: 239 [20736/225000 (9%)] Loss: 7040.060547\n",
      "Train Epoch: 239 [24832/225000 (11%)] Loss: 6965.787109\n",
      "Train Epoch: 239 [28928/225000 (13%)] Loss: 7184.466797\n",
      "Train Epoch: 239 [33024/225000 (15%)] Loss: 7132.462891\n",
      "Train Epoch: 239 [37120/225000 (16%)] Loss: 7161.335938\n",
      "Train Epoch: 239 [41216/225000 (18%)] Loss: 6952.339844\n",
      "Train Epoch: 239 [45312/225000 (20%)] Loss: 6895.058594\n",
      "Train Epoch: 239 [49408/225000 (22%)] Loss: 6969.230469\n",
      "Train Epoch: 239 [53504/225000 (24%)] Loss: 7185.349609\n",
      "Train Epoch: 239 [57600/225000 (26%)] Loss: 7062.855469\n",
      "Train Epoch: 239 [61696/225000 (27%)] Loss: 7128.337891\n",
      "Train Epoch: 239 [65792/225000 (29%)] Loss: 7126.960938\n",
      "Train Epoch: 239 [69888/225000 (31%)] Loss: 6983.021484\n",
      "Train Epoch: 239 [73984/225000 (33%)] Loss: 7132.015625\n",
      "Train Epoch: 239 [78080/225000 (35%)] Loss: 7045.714844\n",
      "Train Epoch: 239 [82176/225000 (37%)] Loss: 7221.054688\n",
      "Train Epoch: 239 [86272/225000 (38%)] Loss: 7116.712891\n",
      "Train Epoch: 239 [90368/225000 (40%)] Loss: 7069.777344\n",
      "Train Epoch: 239 [94464/225000 (42%)] Loss: 7139.986328\n",
      "Train Epoch: 239 [98560/225000 (44%)] Loss: 7050.267578\n",
      "Train Epoch: 239 [102656/225000 (46%)] Loss: 7040.783203\n",
      "Train Epoch: 239 [106752/225000 (47%)] Loss: 7000.855469\n",
      "Train Epoch: 239 [110848/225000 (49%)] Loss: 7157.625000\n",
      "Train Epoch: 239 [114944/225000 (51%)] Loss: 6894.470703\n",
      "Train Epoch: 239 [119040/225000 (53%)] Loss: 7092.085938\n",
      "Train Epoch: 239 [123136/225000 (55%)] Loss: 7048.226562\n",
      "Train Epoch: 239 [127232/225000 (57%)] Loss: 7053.087891\n",
      "Train Epoch: 239 [131328/225000 (58%)] Loss: 7185.392578\n",
      "Train Epoch: 239 [135424/225000 (60%)] Loss: 7092.041016\n",
      "Train Epoch: 239 [139520/225000 (62%)] Loss: 7062.412109\n",
      "Train Epoch: 239 [143616/225000 (64%)] Loss: 7035.371094\n",
      "Train Epoch: 239 [147712/225000 (66%)] Loss: 7249.111328\n",
      "Train Epoch: 239 [151808/225000 (67%)] Loss: 6996.205078\n",
      "Train Epoch: 239 [155904/225000 (69%)] Loss: 6982.332031\n",
      "Train Epoch: 239 [160000/225000 (71%)] Loss: 6874.138672\n",
      "Train Epoch: 239 [164096/225000 (73%)] Loss: 7162.369141\n",
      "Train Epoch: 239 [168192/225000 (75%)] Loss: 7238.810547\n",
      "Train Epoch: 239 [172288/225000 (77%)] Loss: 7099.416016\n",
      "Train Epoch: 239 [176384/225000 (78%)] Loss: 7211.105469\n",
      "Train Epoch: 239 [180480/225000 (80%)] Loss: 7085.429688\n",
      "Train Epoch: 239 [184576/225000 (82%)] Loss: 7086.507812\n",
      "Train Epoch: 239 [188672/225000 (84%)] Loss: 7140.501953\n",
      "Train Epoch: 239 [192768/225000 (86%)] Loss: 6990.753906\n",
      "Train Epoch: 239 [196864/225000 (87%)] Loss: 7055.007812\n",
      "Train Epoch: 239 [200960/225000 (89%)] Loss: 7034.832031\n",
      "Train Epoch: 239 [205056/225000 (91%)] Loss: 7148.845703\n",
      "Train Epoch: 239 [209152/225000 (93%)] Loss: 7189.109375\n",
      "Train Epoch: 239 [213248/225000 (95%)] Loss: 6949.183594\n",
      "Train Epoch: 239 [217344/225000 (97%)] Loss: 7199.236328\n",
      "Train Epoch: 239 [221440/225000 (98%)] Loss: 7017.937500\n",
      "    epoch          : 239\n",
      "    loss           : 7091.177734375\n",
      "    val_loss       : 7072.464581005427\n",
      "Train Epoch: 240 [256/225000 (0%)] Loss: 7102.244141\n",
      "Train Epoch: 240 [4352/225000 (2%)] Loss: 7137.908203\n",
      "Train Epoch: 240 [8448/225000 (4%)] Loss: 6934.003906\n",
      "Train Epoch: 240 [12544/225000 (6%)] Loss: 6890.578125\n",
      "Train Epoch: 240 [16640/225000 (7%)] Loss: 6978.560547\n",
      "Train Epoch: 240 [20736/225000 (9%)] Loss: 7086.986328\n",
      "Train Epoch: 240 [24832/225000 (11%)] Loss: 6905.625000\n",
      "Train Epoch: 240 [28928/225000 (13%)] Loss: 7287.755859\n",
      "Train Epoch: 240 [33024/225000 (15%)] Loss: 7011.582031\n",
      "Train Epoch: 240 [37120/225000 (16%)] Loss: 7013.857422\n",
      "Train Epoch: 240 [41216/225000 (18%)] Loss: 7091.435547\n",
      "Train Epoch: 240 [45312/225000 (20%)] Loss: 7035.103516\n",
      "Train Epoch: 240 [49408/225000 (22%)] Loss: 6964.324219\n",
      "Train Epoch: 240 [53504/225000 (24%)] Loss: 7213.443359\n",
      "Train Epoch: 240 [57600/225000 (26%)] Loss: 6922.080078\n",
      "Train Epoch: 240 [61696/225000 (27%)] Loss: 6997.458984\n",
      "Train Epoch: 240 [65792/225000 (29%)] Loss: 7055.152344\n",
      "Train Epoch: 240 [69888/225000 (31%)] Loss: 7084.199219\n",
      "Train Epoch: 240 [73984/225000 (33%)] Loss: 7066.980469\n",
      "Train Epoch: 240 [78080/225000 (35%)] Loss: 7225.750000\n",
      "Train Epoch: 240 [82176/225000 (37%)] Loss: 7214.181641\n",
      "Train Epoch: 240 [86272/225000 (38%)] Loss: 7031.591797\n",
      "Train Epoch: 240 [90368/225000 (40%)] Loss: 7013.361328\n",
      "Train Epoch: 240 [94464/225000 (42%)] Loss: 7091.617188\n",
      "Train Epoch: 240 [98560/225000 (44%)] Loss: 7187.298828\n",
      "Train Epoch: 240 [102656/225000 (46%)] Loss: 7114.314453\n",
      "Train Epoch: 240 [106752/225000 (47%)] Loss: 7111.361328\n",
      "Train Epoch: 240 [110848/225000 (49%)] Loss: 7016.824219\n",
      "Train Epoch: 240 [114944/225000 (51%)] Loss: 7041.744141\n",
      "Train Epoch: 240 [119040/225000 (53%)] Loss: 7294.148438\n",
      "Train Epoch: 240 [123136/225000 (55%)] Loss: 7205.847656\n",
      "Train Epoch: 240 [127232/225000 (57%)] Loss: 7063.048828\n",
      "Train Epoch: 240 [131328/225000 (58%)] Loss: 7070.869141\n",
      "Train Epoch: 240 [135424/225000 (60%)] Loss: 7112.185547\n",
      "Train Epoch: 240 [139520/225000 (62%)] Loss: 6961.755859\n",
      "Train Epoch: 240 [143616/225000 (64%)] Loss: 6984.683594\n",
      "Train Epoch: 240 [147712/225000 (66%)] Loss: 7113.841797\n",
      "Train Epoch: 240 [151808/225000 (67%)] Loss: 6980.441406\n",
      "Train Epoch: 240 [155904/225000 (69%)] Loss: 7156.248047\n",
      "Train Epoch: 240 [160000/225000 (71%)] Loss: 7212.339844\n",
      "Train Epoch: 240 [164096/225000 (73%)] Loss: 7147.314453\n",
      "Train Epoch: 240 [168192/225000 (75%)] Loss: 7102.462891\n",
      "Train Epoch: 240 [172288/225000 (77%)] Loss: 7023.755859\n",
      "Train Epoch: 240 [176384/225000 (78%)] Loss: 7121.826172\n",
      "Train Epoch: 240 [180480/225000 (80%)] Loss: 7178.855469\n",
      "Train Epoch: 240 [184576/225000 (82%)] Loss: 7184.380859\n",
      "Train Epoch: 240 [188672/225000 (84%)] Loss: 7202.960938\n",
      "Train Epoch: 240 [192768/225000 (86%)] Loss: 6971.445312\n",
      "Train Epoch: 240 [196864/225000 (87%)] Loss: 7262.714844\n",
      "Train Epoch: 240 [200960/225000 (89%)] Loss: 7072.900391\n",
      "Train Epoch: 240 [205056/225000 (91%)] Loss: 6998.371094\n",
      "Train Epoch: 240 [209152/225000 (93%)] Loss: 7013.460938\n",
      "Train Epoch: 240 [213248/225000 (95%)] Loss: 7244.734375\n",
      "Train Epoch: 240 [217344/225000 (97%)] Loss: 7239.875000\n",
      "Train Epoch: 240 [221440/225000 (98%)] Loss: 6953.406250\n",
      "    epoch          : 240\n",
      "    loss           : 7111.985563762088\n",
      "    val_loss       : 7074.694212194609\n",
      "Train Epoch: 241 [256/225000 (0%)] Loss: 6988.628906\n",
      "Train Epoch: 241 [4352/225000 (2%)] Loss: 6888.501953\n",
      "Train Epoch: 241 [8448/225000 (4%)] Loss: 6964.957031\n",
      "Train Epoch: 241 [12544/225000 (6%)] Loss: 7047.609375\n",
      "Train Epoch: 241 [16640/225000 (7%)] Loss: 6980.744141\n",
      "Train Epoch: 241 [20736/225000 (9%)] Loss: 7062.230469\n",
      "Train Epoch: 241 [24832/225000 (11%)] Loss: 7100.615234\n",
      "Train Epoch: 241 [28928/225000 (13%)] Loss: 7000.068359\n",
      "Train Epoch: 241 [33024/225000 (15%)] Loss: 7122.560547\n",
      "Train Epoch: 241 [37120/225000 (16%)] Loss: 6963.681641\n",
      "Train Epoch: 241 [41216/225000 (18%)] Loss: 7143.855469\n",
      "Train Epoch: 241 [45312/225000 (20%)] Loss: 7046.746094\n",
      "Train Epoch: 241 [49408/225000 (22%)] Loss: 7126.591797\n",
      "Train Epoch: 241 [53504/225000 (24%)] Loss: 7066.003906\n",
      "Train Epoch: 241 [57600/225000 (26%)] Loss: 7067.253906\n",
      "Train Epoch: 241 [61696/225000 (27%)] Loss: 6999.964844\n",
      "Train Epoch: 241 [65792/225000 (29%)] Loss: 7022.986328\n",
      "Train Epoch: 241 [69888/225000 (31%)] Loss: 7030.865234\n",
      "Train Epoch: 241 [73984/225000 (33%)] Loss: 7097.011719\n",
      "Train Epoch: 241 [78080/225000 (35%)] Loss: 7143.230469\n",
      "Train Epoch: 241 [82176/225000 (37%)] Loss: 6990.470703\n",
      "Train Epoch: 241 [86272/225000 (38%)] Loss: 6959.466797\n",
      "Train Epoch: 241 [90368/225000 (40%)] Loss: 6997.068359\n",
      "Train Epoch: 241 [94464/225000 (42%)] Loss: 7061.500000\n",
      "Train Epoch: 241 [98560/225000 (44%)] Loss: 6998.507812\n",
      "Train Epoch: 241 [102656/225000 (46%)] Loss: 7072.785156\n",
      "Train Epoch: 241 [106752/225000 (47%)] Loss: 7010.527344\n",
      "Train Epoch: 241 [110848/225000 (49%)] Loss: 6994.023438\n",
      "Train Epoch: 241 [114944/225000 (51%)] Loss: 7002.337891\n",
      "Train Epoch: 241 [119040/225000 (53%)] Loss: 7283.810547\n",
      "Train Epoch: 241 [123136/225000 (55%)] Loss: 6936.582031\n",
      "Train Epoch: 241 [127232/225000 (57%)] Loss: 7109.955078\n",
      "Train Epoch: 241 [131328/225000 (58%)] Loss: 7180.833984\n",
      "Train Epoch: 241 [135424/225000 (60%)] Loss: 7207.000000\n",
      "Train Epoch: 241 [139520/225000 (62%)] Loss: 6976.773438\n",
      "Train Epoch: 241 [143616/225000 (64%)] Loss: 7007.513672\n",
      "Train Epoch: 241 [147712/225000 (66%)] Loss: 7258.751953\n",
      "Train Epoch: 241 [151808/225000 (67%)] Loss: 7135.861328\n",
      "Train Epoch: 241 [155904/225000 (69%)] Loss: 7208.925781\n",
      "Train Epoch: 241 [160000/225000 (71%)] Loss: 6969.501953\n",
      "Train Epoch: 241 [164096/225000 (73%)] Loss: 7005.576172\n",
      "Train Epoch: 241 [168192/225000 (75%)] Loss: 7048.560547\n",
      "Train Epoch: 241 [172288/225000 (77%)] Loss: 7187.791016\n",
      "Train Epoch: 241 [176384/225000 (78%)] Loss: 7197.515625\n",
      "Train Epoch: 241 [180480/225000 (80%)] Loss: 7242.021484\n",
      "Train Epoch: 241 [184576/225000 (82%)] Loss: 7183.203125\n",
      "Train Epoch: 241 [188672/225000 (84%)] Loss: 7128.460938\n",
      "Train Epoch: 241 [192768/225000 (86%)] Loss: 7110.998047\n",
      "Train Epoch: 241 [196864/225000 (87%)] Loss: 7268.166016\n",
      "Train Epoch: 241 [200960/225000 (89%)] Loss: 7001.455078\n",
      "Train Epoch: 241 [205056/225000 (91%)] Loss: 6999.328125\n",
      "Train Epoch: 241 [209152/225000 (93%)] Loss: 7225.519531\n",
      "Train Epoch: 241 [213248/225000 (95%)] Loss: 6976.972656\n",
      "Train Epoch: 241 [217344/225000 (97%)] Loss: 7129.937500\n",
      "Train Epoch: 241 [221440/225000 (98%)] Loss: 7008.224609\n",
      "    epoch          : 241\n",
      "    loss           : 7066.877022006542\n",
      "    val_loss       : 7069.938416787556\n",
      "Train Epoch: 242 [256/225000 (0%)] Loss: 7122.824219\n",
      "Train Epoch: 242 [4352/225000 (2%)] Loss: 6952.626953\n",
      "Train Epoch: 242 [8448/225000 (4%)] Loss: 7028.349609\n",
      "Train Epoch: 242 [12544/225000 (6%)] Loss: 7019.787109\n",
      "Train Epoch: 242 [16640/225000 (7%)] Loss: 7140.275391\n",
      "Train Epoch: 242 [20736/225000 (9%)] Loss: 6915.613281\n",
      "Train Epoch: 242 [24832/225000 (11%)] Loss: 7023.757812\n",
      "Train Epoch: 242 [28928/225000 (13%)] Loss: 7125.195312\n",
      "Train Epoch: 242 [33024/225000 (15%)] Loss: 7022.888672\n",
      "Train Epoch: 242 [37120/225000 (16%)] Loss: 7100.386719\n",
      "Train Epoch: 242 [41216/225000 (18%)] Loss: 7204.091797\n",
      "Train Epoch: 242 [45312/225000 (20%)] Loss: 7073.626953\n",
      "Train Epoch: 242 [49408/225000 (22%)] Loss: 7029.943359\n",
      "Train Epoch: 242 [53504/225000 (24%)] Loss: 7135.115234\n",
      "Train Epoch: 242 [57600/225000 (26%)] Loss: 7264.412109\n",
      "Train Epoch: 242 [61696/225000 (27%)] Loss: 7063.216797\n",
      "Train Epoch: 242 [65792/225000 (29%)] Loss: 7082.486328\n",
      "Train Epoch: 242 [69888/225000 (31%)] Loss: 6978.222656\n",
      "Train Epoch: 242 [73984/225000 (33%)] Loss: 6980.677734\n",
      "Train Epoch: 242 [78080/225000 (35%)] Loss: 7162.785156\n",
      "Train Epoch: 242 [82176/225000 (37%)] Loss: 7076.167969\n",
      "Train Epoch: 242 [86272/225000 (38%)] Loss: 7023.583984\n",
      "Train Epoch: 242 [90368/225000 (40%)] Loss: 7129.431641\n",
      "Train Epoch: 242 [94464/225000 (42%)] Loss: 7018.943359\n",
      "Train Epoch: 242 [98560/225000 (44%)] Loss: 7107.992188\n",
      "Train Epoch: 242 [102656/225000 (46%)] Loss: 7079.214844\n",
      "Train Epoch: 242 [106752/225000 (47%)] Loss: 7193.263672\n",
      "Train Epoch: 242 [110848/225000 (49%)] Loss: 6992.261719\n",
      "Train Epoch: 242 [114944/225000 (51%)] Loss: 7151.238281\n",
      "Train Epoch: 242 [119040/225000 (53%)] Loss: 6920.486328\n",
      "Train Epoch: 242 [123136/225000 (55%)] Loss: 7018.173828\n",
      "Train Epoch: 242 [127232/225000 (57%)] Loss: 7180.974609\n",
      "Train Epoch: 242 [131328/225000 (58%)] Loss: 7084.656250\n",
      "Train Epoch: 242 [135424/225000 (60%)] Loss: 6985.560547\n",
      "Train Epoch: 242 [139520/225000 (62%)] Loss: 6950.128906\n",
      "Train Epoch: 242 [143616/225000 (64%)] Loss: 7139.306641\n",
      "Train Epoch: 242 [147712/225000 (66%)] Loss: 7079.330078\n",
      "Train Epoch: 242 [151808/225000 (67%)] Loss: 7070.496094\n",
      "Train Epoch: 242 [155904/225000 (69%)] Loss: 7212.447266\n",
      "Train Epoch: 242 [160000/225000 (71%)] Loss: 7111.666016\n",
      "Train Epoch: 242 [164096/225000 (73%)] Loss: 7119.908203\n",
      "Train Epoch: 242 [168192/225000 (75%)] Loss: 6985.701172\n",
      "Train Epoch: 242 [172288/225000 (77%)] Loss: 7035.619141\n",
      "Train Epoch: 242 [176384/225000 (78%)] Loss: 7032.576172\n",
      "Train Epoch: 242 [180480/225000 (80%)] Loss: 7161.634766\n",
      "Train Epoch: 242 [184576/225000 (82%)] Loss: 7051.570312\n",
      "Train Epoch: 242 [188672/225000 (84%)] Loss: 7093.492188\n",
      "Train Epoch: 242 [192768/225000 (86%)] Loss: 6996.382812\n",
      "Train Epoch: 242 [196864/225000 (87%)] Loss: 7053.394531\n",
      "Train Epoch: 242 [200960/225000 (89%)] Loss: 6964.750000\n",
      "Train Epoch: 242 [205056/225000 (91%)] Loss: 7209.914062\n",
      "Train Epoch: 242 [209152/225000 (93%)] Loss: 7009.771484\n",
      "Train Epoch: 242 [213248/225000 (95%)] Loss: 15443.177734\n",
      "Train Epoch: 242 [217344/225000 (97%)] Loss: 7050.761719\n",
      "Train Epoch: 242 [221440/225000 (98%)] Loss: 6953.923828\n",
      "    epoch          : 242\n",
      "    loss           : 7085.3267440361915\n",
      "    val_loss       : 7066.4210808155485\n",
      "Train Epoch: 243 [256/225000 (0%)] Loss: 7009.005859\n",
      "Train Epoch: 243 [4352/225000 (2%)] Loss: 7048.050781\n",
      "Train Epoch: 243 [8448/225000 (4%)] Loss: 7061.921875\n",
      "Train Epoch: 243 [12544/225000 (6%)] Loss: 7104.882812\n",
      "Train Epoch: 243 [16640/225000 (7%)] Loss: 7127.093750\n",
      "Train Epoch: 243 [20736/225000 (9%)] Loss: 7127.390625\n",
      "Train Epoch: 243 [24832/225000 (11%)] Loss: 6923.236328\n",
      "Train Epoch: 243 [28928/225000 (13%)] Loss: 6934.003906\n",
      "Train Epoch: 243 [33024/225000 (15%)] Loss: 7084.914062\n",
      "Train Epoch: 243 [37120/225000 (16%)] Loss: 7019.884766\n",
      "Train Epoch: 243 [41216/225000 (18%)] Loss: 7104.027344\n",
      "Train Epoch: 243 [45312/225000 (20%)] Loss: 7138.068359\n",
      "Train Epoch: 243 [49408/225000 (22%)] Loss: 7157.476562\n",
      "Train Epoch: 243 [53504/225000 (24%)] Loss: 7213.611328\n",
      "Train Epoch: 243 [57600/225000 (26%)] Loss: 7197.841797\n",
      "Train Epoch: 243 [61696/225000 (27%)] Loss: 7143.951172\n",
      "Train Epoch: 243 [65792/225000 (29%)] Loss: 6976.017578\n",
      "Train Epoch: 243 [69888/225000 (31%)] Loss: 7149.468750\n",
      "Train Epoch: 243 [73984/225000 (33%)] Loss: 7046.373047\n",
      "Train Epoch: 243 [78080/225000 (35%)] Loss: 7028.580078\n",
      "Train Epoch: 243 [82176/225000 (37%)] Loss: 7058.013672\n",
      "Train Epoch: 243 [86272/225000 (38%)] Loss: 6894.894531\n",
      "Train Epoch: 243 [90368/225000 (40%)] Loss: 7077.085938\n",
      "Train Epoch: 243 [94464/225000 (42%)] Loss: 7028.582031\n",
      "Train Epoch: 243 [98560/225000 (44%)] Loss: 7151.890625\n",
      "Train Epoch: 243 [102656/225000 (46%)] Loss: 7179.207031\n",
      "Train Epoch: 243 [106752/225000 (47%)] Loss: 7022.279297\n",
      "Train Epoch: 243 [110848/225000 (49%)] Loss: 7036.453125\n",
      "Train Epoch: 243 [114944/225000 (51%)] Loss: 7175.359375\n",
      "Train Epoch: 243 [119040/225000 (53%)] Loss: 6992.515625\n",
      "Train Epoch: 243 [123136/225000 (55%)] Loss: 6983.449219\n",
      "Train Epoch: 243 [127232/225000 (57%)] Loss: 7215.744141\n",
      "Train Epoch: 243 [131328/225000 (58%)] Loss: 7158.121094\n",
      "Train Epoch: 243 [135424/225000 (60%)] Loss: 6927.248047\n",
      "Train Epoch: 243 [139520/225000 (62%)] Loss: 7114.382812\n",
      "Train Epoch: 243 [143616/225000 (64%)] Loss: 6982.812500\n",
      "Train Epoch: 243 [147712/225000 (66%)] Loss: 7047.015625\n",
      "Train Epoch: 243 [151808/225000 (67%)] Loss: 7114.943359\n",
      "Train Epoch: 243 [155904/225000 (69%)] Loss: 6941.767578\n",
      "Train Epoch: 243 [160000/225000 (71%)] Loss: 7100.943359\n",
      "Train Epoch: 243 [164096/225000 (73%)] Loss: 6972.853516\n",
      "Train Epoch: 243 [168192/225000 (75%)] Loss: 7079.927734\n",
      "Train Epoch: 243 [172288/225000 (77%)] Loss: 6941.416016\n",
      "Train Epoch: 243 [176384/225000 (78%)] Loss: 7104.021484\n",
      "Train Epoch: 243 [180480/225000 (80%)] Loss: 6847.496094\n",
      "Train Epoch: 243 [184576/225000 (82%)] Loss: 7076.685547\n",
      "Train Epoch: 243 [188672/225000 (84%)] Loss: 6937.642578\n",
      "Train Epoch: 243 [192768/225000 (86%)] Loss: 7126.095703\n",
      "Train Epoch: 243 [196864/225000 (87%)] Loss: 6983.720703\n",
      "Train Epoch: 243 [200960/225000 (89%)] Loss: 7207.986328\n",
      "Train Epoch: 243 [205056/225000 (91%)] Loss: 7131.287109\n",
      "Train Epoch: 243 [209152/225000 (93%)] Loss: 7011.744141\n",
      "Train Epoch: 243 [213248/225000 (95%)] Loss: 7051.906250\n",
      "Train Epoch: 243 [217344/225000 (97%)] Loss: 7060.277344\n",
      "Train Epoch: 243 [221440/225000 (98%)] Loss: 7135.427734\n",
      "    epoch          : 243\n",
      "    loss           : 7073.550915680105\n",
      "    val_loss       : 7075.722919212312\n",
      "Train Epoch: 244 [256/225000 (0%)] Loss: 6980.197266\n",
      "Train Epoch: 244 [4352/225000 (2%)] Loss: 7021.156250\n",
      "Train Epoch: 244 [8448/225000 (4%)] Loss: 7191.976562\n",
      "Train Epoch: 244 [12544/225000 (6%)] Loss: 6956.703125\n",
      "Train Epoch: 244 [16640/225000 (7%)] Loss: 7123.755859\n",
      "Train Epoch: 244 [20736/225000 (9%)] Loss: 7112.658203\n",
      "Train Epoch: 244 [24832/225000 (11%)] Loss: 6946.416016\n",
      "Train Epoch: 244 [28928/225000 (13%)] Loss: 7113.701172\n",
      "Train Epoch: 244 [33024/225000 (15%)] Loss: 7020.078125\n",
      "Train Epoch: 244 [37120/225000 (16%)] Loss: 7085.662109\n",
      "Train Epoch: 244 [41216/225000 (18%)] Loss: 6953.177734\n",
      "Train Epoch: 244 [45312/225000 (20%)] Loss: 7050.658203\n",
      "Train Epoch: 244 [49408/225000 (22%)] Loss: 6966.597656\n",
      "Train Epoch: 244 [53504/225000 (24%)] Loss: 7154.791016\n",
      "Train Epoch: 244 [57600/225000 (26%)] Loss: 7085.439453\n",
      "Train Epoch: 244 [61696/225000 (27%)] Loss: 7108.154297\n",
      "Train Epoch: 244 [65792/225000 (29%)] Loss: 7112.328125\n",
      "Train Epoch: 244 [69888/225000 (31%)] Loss: 6980.751953\n",
      "Train Epoch: 244 [73984/225000 (33%)] Loss: 6925.171875\n",
      "Train Epoch: 244 [78080/225000 (35%)] Loss: 7026.416016\n",
      "Train Epoch: 244 [82176/225000 (37%)] Loss: 7183.476562\n",
      "Train Epoch: 244 [86272/225000 (38%)] Loss: 7036.376953\n",
      "Train Epoch: 244 [90368/225000 (40%)] Loss: 7142.273438\n",
      "Train Epoch: 244 [94464/225000 (42%)] Loss: 6999.275391\n",
      "Train Epoch: 244 [98560/225000 (44%)] Loss: 7106.970703\n",
      "Train Epoch: 244 [102656/225000 (46%)] Loss: 6968.074219\n",
      "Train Epoch: 244 [106752/225000 (47%)] Loss: 6977.560547\n",
      "Train Epoch: 244 [110848/225000 (49%)] Loss: 7146.105469\n",
      "Train Epoch: 244 [114944/225000 (51%)] Loss: 7044.058594\n",
      "Train Epoch: 244 [119040/225000 (53%)] Loss: 7191.675781\n",
      "Train Epoch: 244 [123136/225000 (55%)] Loss: 7060.628906\n",
      "Train Epoch: 244 [127232/225000 (57%)] Loss: 6987.078125\n",
      "Train Epoch: 244 [131328/225000 (58%)] Loss: 7020.070312\n",
      "Train Epoch: 244 [135424/225000 (60%)] Loss: 6925.478516\n",
      "Train Epoch: 244 [139520/225000 (62%)] Loss: 6995.646484\n",
      "Train Epoch: 244 [143616/225000 (64%)] Loss: 7158.203125\n",
      "Train Epoch: 244 [147712/225000 (66%)] Loss: 6970.312500\n",
      "Train Epoch: 244 [151808/225000 (67%)] Loss: 7145.796875\n",
      "Train Epoch: 244 [155904/225000 (69%)] Loss: 7025.242188\n",
      "Train Epoch: 244 [160000/225000 (71%)] Loss: 7114.201172\n",
      "Train Epoch: 244 [164096/225000 (73%)] Loss: 7060.650391\n",
      "Train Epoch: 244 [168192/225000 (75%)] Loss: 7029.324219\n",
      "Train Epoch: 244 [172288/225000 (77%)] Loss: 7141.412109\n",
      "Train Epoch: 244 [176384/225000 (78%)] Loss: 7129.478516\n",
      "Train Epoch: 244 [180480/225000 (80%)] Loss: 7083.125000\n",
      "Train Epoch: 244 [184576/225000 (82%)] Loss: 6995.976562\n",
      "Train Epoch: 244 [188672/225000 (84%)] Loss: 7040.535156\n",
      "Train Epoch: 244 [192768/225000 (86%)] Loss: 7013.957031\n",
      "Train Epoch: 244 [196864/225000 (87%)] Loss: 7171.398438\n",
      "Train Epoch: 244 [200960/225000 (89%)] Loss: 7084.583984\n",
      "Train Epoch: 244 [205056/225000 (91%)] Loss: 7007.705078\n",
      "Train Epoch: 244 [209152/225000 (93%)] Loss: 7052.949219\n",
      "Train Epoch: 244 [213248/225000 (95%)] Loss: 6965.468750\n",
      "Train Epoch: 244 [217344/225000 (97%)] Loss: 7032.259766\n",
      "Train Epoch: 244 [221440/225000 (98%)] Loss: 7169.320312\n",
      "    epoch          : 244\n",
      "    loss           : 7065.508463541667\n",
      "    val_loss       : 7061.305432468044\n",
      "Train Epoch: 245 [256/225000 (0%)] Loss: 7137.085938\n",
      "Train Epoch: 245 [4352/225000 (2%)] Loss: 6972.566406\n",
      "Train Epoch: 245 [8448/225000 (4%)] Loss: 7102.798828\n",
      "Train Epoch: 245 [12544/225000 (6%)] Loss: 7077.685547\n",
      "Train Epoch: 245 [16640/225000 (7%)] Loss: 7063.304688\n",
      "Train Epoch: 245 [20736/225000 (9%)] Loss: 7201.777344\n",
      "Train Epoch: 245 [24832/225000 (11%)] Loss: 7161.224609\n",
      "Train Epoch: 245 [28928/225000 (13%)] Loss: 7127.009766\n",
      "Train Epoch: 245 [33024/225000 (15%)] Loss: 7266.431641\n",
      "Train Epoch: 245 [37120/225000 (16%)] Loss: 6991.203125\n",
      "Train Epoch: 245 [41216/225000 (18%)] Loss: 7080.585938\n",
      "Train Epoch: 245 [45312/225000 (20%)] Loss: 7041.025391\n",
      "Train Epoch: 245 [49408/225000 (22%)] Loss: 6981.337891\n",
      "Train Epoch: 245 [53504/225000 (24%)] Loss: 7106.314453\n",
      "Train Epoch: 245 [57600/225000 (26%)] Loss: 6928.433594\n",
      "Train Epoch: 245 [61696/225000 (27%)] Loss: 7050.134766\n",
      "Train Epoch: 245 [65792/225000 (29%)] Loss: 6994.048828\n",
      "Train Epoch: 245 [69888/225000 (31%)] Loss: 7192.529297\n",
      "Train Epoch: 245 [73984/225000 (33%)] Loss: 7093.396484\n",
      "Train Epoch: 245 [78080/225000 (35%)] Loss: 6938.203125\n",
      "Train Epoch: 245 [82176/225000 (37%)] Loss: 7031.052734\n",
      "Train Epoch: 245 [86272/225000 (38%)] Loss: 6976.509766\n",
      "Train Epoch: 245 [90368/225000 (40%)] Loss: 7064.724609\n",
      "Train Epoch: 245 [94464/225000 (42%)] Loss: 7077.289062\n",
      "Train Epoch: 245 [98560/225000 (44%)] Loss: 7097.994141\n",
      "Train Epoch: 245 [102656/225000 (46%)] Loss: 7079.314453\n",
      "Train Epoch: 245 [106752/225000 (47%)] Loss: 6952.007812\n",
      "Train Epoch: 245 [110848/225000 (49%)] Loss: 7116.564453\n",
      "Train Epoch: 245 [114944/225000 (51%)] Loss: 7044.003906\n",
      "Train Epoch: 245 [119040/225000 (53%)] Loss: 7042.710938\n",
      "Train Epoch: 245 [123136/225000 (55%)] Loss: 7180.832031\n",
      "Train Epoch: 245 [127232/225000 (57%)] Loss: 7137.503906\n",
      "Train Epoch: 245 [131328/225000 (58%)] Loss: 7142.115234\n",
      "Train Epoch: 245 [135424/225000 (60%)] Loss: 6831.896484\n",
      "Train Epoch: 245 [139520/225000 (62%)] Loss: 7197.958984\n",
      "Train Epoch: 245 [143616/225000 (64%)] Loss: 6869.253906\n",
      "Train Epoch: 245 [147712/225000 (66%)] Loss: 7169.667969\n",
      "Train Epoch: 245 [151808/225000 (67%)] Loss: 7056.623047\n",
      "Train Epoch: 245 [155904/225000 (69%)] Loss: 7122.300781\n",
      "Train Epoch: 245 [160000/225000 (71%)] Loss: 7117.900391\n",
      "Train Epoch: 245 [164096/225000 (73%)] Loss: 7108.859375\n",
      "Train Epoch: 245 [168192/225000 (75%)] Loss: 7224.228516\n",
      "Train Epoch: 245 [172288/225000 (77%)] Loss: 7143.427734\n",
      "Train Epoch: 245 [176384/225000 (78%)] Loss: 7069.886719\n",
      "Train Epoch: 245 [180480/225000 (80%)] Loss: 6999.650391\n",
      "Train Epoch: 245 [184576/225000 (82%)] Loss: 7080.335938\n",
      "Train Epoch: 245 [188672/225000 (84%)] Loss: 7061.845703\n",
      "Train Epoch: 245 [192768/225000 (86%)] Loss: 6885.408203\n",
      "Train Epoch: 245 [196864/225000 (87%)] Loss: 7172.425781\n",
      "Train Epoch: 245 [200960/225000 (89%)] Loss: 7043.474609\n",
      "Train Epoch: 245 [205056/225000 (91%)] Loss: 7151.751953\n",
      "Train Epoch: 245 [209152/225000 (93%)] Loss: 7069.937500\n",
      "Train Epoch: 245 [213248/225000 (95%)] Loss: 6950.568359\n",
      "Train Epoch: 245 [217344/225000 (97%)] Loss: 6916.134766\n",
      "Train Epoch: 245 [221440/225000 (98%)] Loss: 7203.667969\n",
      "    epoch          : 245\n",
      "    loss           : 7095.850945899104\n",
      "    val_loss       : 7060.763940037513\n",
      "Train Epoch: 246 [256/225000 (0%)] Loss: 6966.132812\n",
      "Train Epoch: 246 [4352/225000 (2%)] Loss: 7048.798828\n",
      "Train Epoch: 246 [8448/225000 (4%)] Loss: 7056.388672\n",
      "Train Epoch: 246 [12544/225000 (6%)] Loss: 7077.917969\n",
      "Train Epoch: 246 [16640/225000 (7%)] Loss: 6989.347656\n",
      "Train Epoch: 246 [20736/225000 (9%)] Loss: 7055.089844\n",
      "Train Epoch: 246 [24832/225000 (11%)] Loss: 7263.554688\n",
      "Train Epoch: 246 [28928/225000 (13%)] Loss: 7002.880859\n",
      "Train Epoch: 246 [33024/225000 (15%)] Loss: 7156.683594\n",
      "Train Epoch: 246 [37120/225000 (16%)] Loss: 7077.419922\n",
      "Train Epoch: 246 [41216/225000 (18%)] Loss: 6860.238281\n",
      "Train Epoch: 246 [45312/225000 (20%)] Loss: 7137.791016\n",
      "Train Epoch: 246 [49408/225000 (22%)] Loss: 6950.349609\n",
      "Train Epoch: 246 [53504/225000 (24%)] Loss: 7280.980469\n",
      "Train Epoch: 246 [57600/225000 (26%)] Loss: 6936.873047\n",
      "Train Epoch: 246 [61696/225000 (27%)] Loss: 7225.064453\n",
      "Train Epoch: 246 [65792/225000 (29%)] Loss: 7095.916016\n",
      "Train Epoch: 246 [69888/225000 (31%)] Loss: 6907.833984\n",
      "Train Epoch: 246 [73984/225000 (33%)] Loss: 6982.173828\n",
      "Train Epoch: 246 [78080/225000 (35%)] Loss: 6982.572266\n",
      "Train Epoch: 246 [82176/225000 (37%)] Loss: 7037.654297\n",
      "Train Epoch: 246 [86272/225000 (38%)] Loss: 7116.900391\n",
      "Train Epoch: 246 [90368/225000 (40%)] Loss: 7109.378906\n",
      "Train Epoch: 246 [94464/225000 (42%)] Loss: 6983.919922\n",
      "Train Epoch: 246 [98560/225000 (44%)] Loss: 7082.888672\n",
      "Train Epoch: 246 [102656/225000 (46%)] Loss: 6874.234375\n",
      "Train Epoch: 246 [106752/225000 (47%)] Loss: 6904.359375\n",
      "Train Epoch: 246 [110848/225000 (49%)] Loss: 7103.179688\n",
      "Train Epoch: 246 [114944/225000 (51%)] Loss: 7336.484375\n",
      "Train Epoch: 246 [119040/225000 (53%)] Loss: 7072.171875\n",
      "Train Epoch: 246 [123136/225000 (55%)] Loss: 7138.351562\n",
      "Train Epoch: 246 [127232/225000 (57%)] Loss: 7031.322266\n",
      "Train Epoch: 246 [131328/225000 (58%)] Loss: 7043.394531\n",
      "Train Epoch: 246 [135424/225000 (60%)] Loss: 6944.816406\n",
      "Train Epoch: 246 [139520/225000 (62%)] Loss: 7101.818359\n",
      "Train Epoch: 246 [143616/225000 (64%)] Loss: 7109.007812\n",
      "Train Epoch: 246 [147712/225000 (66%)] Loss: 7055.105469\n",
      "Train Epoch: 246 [151808/225000 (67%)] Loss: 6931.523438\n",
      "Train Epoch: 246 [155904/225000 (69%)] Loss: 6980.562500\n",
      "Train Epoch: 246 [160000/225000 (71%)] Loss: 7108.265625\n",
      "Train Epoch: 246 [164096/225000 (73%)] Loss: 7061.445312\n",
      "Train Epoch: 246 [168192/225000 (75%)] Loss: 7056.550781\n",
      "Train Epoch: 246 [172288/225000 (77%)] Loss: 7062.957031\n",
      "Train Epoch: 246 [176384/225000 (78%)] Loss: 6854.634766\n",
      "Train Epoch: 246 [180480/225000 (80%)] Loss: 7039.861328\n",
      "Train Epoch: 246 [184576/225000 (82%)] Loss: 7167.529297\n",
      "Train Epoch: 246 [188672/225000 (84%)] Loss: 7186.835938\n",
      "Train Epoch: 246 [192768/225000 (86%)] Loss: 7097.277344\n",
      "Train Epoch: 246 [196864/225000 (87%)] Loss: 6996.652344\n",
      "Train Epoch: 246 [200960/225000 (89%)] Loss: 7175.509766\n",
      "Train Epoch: 246 [205056/225000 (91%)] Loss: 7124.076172\n",
      "Train Epoch: 246 [209152/225000 (93%)] Loss: 7111.615234\n",
      "Train Epoch: 246 [213248/225000 (95%)] Loss: 7072.978516\n",
      "Train Epoch: 246 [217344/225000 (97%)] Loss: 6973.537109\n",
      "Train Epoch: 246 [221440/225000 (98%)] Loss: 7225.125000\n",
      "    epoch          : 246\n",
      "    loss           : 7085.6345400935015\n",
      "    val_loss       : 7055.120093939256\n",
      "Train Epoch: 247 [256/225000 (0%)] Loss: 6861.728516\n",
      "Train Epoch: 247 [4352/225000 (2%)] Loss: 6920.658203\n",
      "Train Epoch: 247 [8448/225000 (4%)] Loss: 7051.291016\n",
      "Train Epoch: 247 [12544/225000 (6%)] Loss: 7221.546875\n",
      "Train Epoch: 247 [16640/225000 (7%)] Loss: 7106.009766\n",
      "Train Epoch: 247 [20736/225000 (9%)] Loss: 7024.289062\n",
      "Train Epoch: 247 [24832/225000 (11%)] Loss: 7188.794922\n",
      "Train Epoch: 247 [28928/225000 (13%)] Loss: 7119.539062\n",
      "Train Epoch: 247 [33024/225000 (15%)] Loss: 7133.316406\n",
      "Train Epoch: 247 [37120/225000 (16%)] Loss: 7030.181641\n",
      "Train Epoch: 247 [41216/225000 (18%)] Loss: 7147.773438\n",
      "Train Epoch: 247 [45312/225000 (20%)] Loss: 7147.724609\n",
      "Train Epoch: 247 [49408/225000 (22%)] Loss: 6800.736328\n",
      "Train Epoch: 247 [53504/225000 (24%)] Loss: 7009.738281\n",
      "Train Epoch: 247 [57600/225000 (26%)] Loss: 6966.326172\n",
      "Train Epoch: 247 [61696/225000 (27%)] Loss: 7000.964844\n",
      "Train Epoch: 247 [65792/225000 (29%)] Loss: 7150.777344\n",
      "Train Epoch: 247 [69888/225000 (31%)] Loss: 6997.320312\n",
      "Train Epoch: 247 [73984/225000 (33%)] Loss: 7093.738281\n",
      "Train Epoch: 247 [78080/225000 (35%)] Loss: 7053.494141\n",
      "Train Epoch: 247 [82176/225000 (37%)] Loss: 7056.529297\n",
      "Train Epoch: 247 [86272/225000 (38%)] Loss: 7122.830078\n",
      "Train Epoch: 247 [90368/225000 (40%)] Loss: 6954.775391\n",
      "Train Epoch: 247 [94464/225000 (42%)] Loss: 6960.869141\n",
      "Train Epoch: 247 [98560/225000 (44%)] Loss: 6909.388672\n",
      "Train Epoch: 247 [102656/225000 (46%)] Loss: 7090.414062\n",
      "Train Epoch: 247 [106752/225000 (47%)] Loss: 7079.796875\n",
      "Train Epoch: 247 [110848/225000 (49%)] Loss: 7092.195312\n",
      "Train Epoch: 247 [114944/225000 (51%)] Loss: 6934.978516\n",
      "Train Epoch: 247 [119040/225000 (53%)] Loss: 6980.535156\n",
      "Train Epoch: 247 [123136/225000 (55%)] Loss: 7077.410156\n",
      "Train Epoch: 247 [127232/225000 (57%)] Loss: 6864.294922\n",
      "Train Epoch: 247 [131328/225000 (58%)] Loss: 7039.015625\n",
      "Train Epoch: 247 [135424/225000 (60%)] Loss: 6913.658203\n",
      "Train Epoch: 247 [139520/225000 (62%)] Loss: 7022.867188\n",
      "Train Epoch: 247 [143616/225000 (64%)] Loss: 6971.632812\n",
      "Train Epoch: 247 [147712/225000 (66%)] Loss: 7037.351562\n",
      "Train Epoch: 247 [151808/225000 (67%)] Loss: 7221.275391\n",
      "Train Epoch: 247 [155904/225000 (69%)] Loss: 7194.357422\n",
      "Train Epoch: 247 [160000/225000 (71%)] Loss: 7167.824219\n",
      "Train Epoch: 247 [164096/225000 (73%)] Loss: 7136.722656\n",
      "Train Epoch: 247 [168192/225000 (75%)] Loss: 6950.533203\n",
      "Train Epoch: 247 [172288/225000 (77%)] Loss: 6906.984375\n",
      "Train Epoch: 247 [176384/225000 (78%)] Loss: 7226.039062\n",
      "Train Epoch: 247 [180480/225000 (80%)] Loss: 7130.289062\n",
      "Train Epoch: 247 [184576/225000 (82%)] Loss: 7119.896484\n",
      "Train Epoch: 247 [188672/225000 (84%)] Loss: 6878.939453\n",
      "Train Epoch: 247 [192768/225000 (86%)] Loss: 7115.417969\n",
      "Train Epoch: 247 [196864/225000 (87%)] Loss: 7109.730469\n",
      "Train Epoch: 247 [200960/225000 (89%)] Loss: 7039.523438\n",
      "Train Epoch: 247 [205056/225000 (91%)] Loss: 7058.730469\n",
      "Train Epoch: 247 [209152/225000 (93%)] Loss: 6894.917969\n",
      "Train Epoch: 247 [213248/225000 (95%)] Loss: 7153.521484\n",
      "Train Epoch: 247 [217344/225000 (97%)] Loss: 6945.597656\n",
      "Train Epoch: 247 [221440/225000 (98%)] Loss: 7152.406250\n",
      "    epoch          : 247\n",
      "    loss           : 7053.352392411476\n",
      "    val_loss       : 7054.987685816629\n",
      "Train Epoch: 248 [256/225000 (0%)] Loss: 6951.625000\n",
      "Train Epoch: 248 [4352/225000 (2%)] Loss: 7036.771484\n",
      "Train Epoch: 248 [8448/225000 (4%)] Loss: 7065.205078\n",
      "Train Epoch: 248 [12544/225000 (6%)] Loss: 6986.955078\n",
      "Train Epoch: 248 [16640/225000 (7%)] Loss: 6948.048828\n",
      "Train Epoch: 248 [20736/225000 (9%)] Loss: 6991.888672\n",
      "Train Epoch: 248 [24832/225000 (11%)] Loss: 6897.193359\n",
      "Train Epoch: 248 [28928/225000 (13%)] Loss: 7042.179688\n",
      "Train Epoch: 248 [33024/225000 (15%)] Loss: 6901.304688\n",
      "Train Epoch: 248 [37120/225000 (16%)] Loss: 7000.785156\n",
      "Train Epoch: 248 [41216/225000 (18%)] Loss: 6975.107422\n",
      "Train Epoch: 248 [45312/225000 (20%)] Loss: 7078.240234\n",
      "Train Epoch: 248 [49408/225000 (22%)] Loss: 7044.417969\n",
      "Train Epoch: 248 [53504/225000 (24%)] Loss: 6993.689453\n",
      "Train Epoch: 248 [57600/225000 (26%)] Loss: 6994.039062\n",
      "Train Epoch: 248 [61696/225000 (27%)] Loss: 7020.212891\n",
      "Train Epoch: 248 [65792/225000 (29%)] Loss: 7060.216797\n",
      "Train Epoch: 248 [69888/225000 (31%)] Loss: 7106.980469\n",
      "Train Epoch: 248 [73984/225000 (33%)] Loss: 7061.429688\n",
      "Train Epoch: 248 [78080/225000 (35%)] Loss: 6957.978516\n",
      "Train Epoch: 248 [82176/225000 (37%)] Loss: 7278.361328\n",
      "Train Epoch: 248 [86272/225000 (38%)] Loss: 7149.076172\n",
      "Train Epoch: 248 [90368/225000 (40%)] Loss: 7136.113281\n",
      "Train Epoch: 248 [94464/225000 (42%)] Loss: 7097.291016\n",
      "Train Epoch: 248 [98560/225000 (44%)] Loss: 7088.625000\n",
      "Train Epoch: 248 [102656/225000 (46%)] Loss: 7206.955078\n",
      "Train Epoch: 248 [106752/225000 (47%)] Loss: 6957.312500\n",
      "Train Epoch: 248 [110848/225000 (49%)] Loss: 6930.683594\n",
      "Train Epoch: 248 [114944/225000 (51%)] Loss: 7193.791016\n",
      "Train Epoch: 248 [119040/225000 (53%)] Loss: 7160.431641\n",
      "Train Epoch: 248 [123136/225000 (55%)] Loss: 7163.496094\n",
      "Train Epoch: 248 [127232/225000 (57%)] Loss: 6964.001953\n",
      "Train Epoch: 248 [131328/225000 (58%)] Loss: 7032.994141\n",
      "Train Epoch: 248 [135424/225000 (60%)] Loss: 6989.730469\n",
      "Train Epoch: 248 [139520/225000 (62%)] Loss: 7019.671875\n",
      "Train Epoch: 248 [143616/225000 (64%)] Loss: 7134.242188\n",
      "Train Epoch: 248 [147712/225000 (66%)] Loss: 6901.105469\n",
      "Train Epoch: 248 [151808/225000 (67%)] Loss: 6961.626953\n",
      "Train Epoch: 248 [155904/225000 (69%)] Loss: 7099.488281\n",
      "Train Epoch: 248 [160000/225000 (71%)] Loss: 7039.363281\n",
      "Train Epoch: 248 [164096/225000 (73%)] Loss: 7172.544922\n",
      "Train Epoch: 248 [168192/225000 (75%)] Loss: 7142.869141\n",
      "Train Epoch: 248 [172288/225000 (77%)] Loss: 7037.148438\n",
      "Train Epoch: 248 [176384/225000 (78%)] Loss: 6973.757812\n",
      "Train Epoch: 248 [180480/225000 (80%)] Loss: 7015.359375\n",
      "Train Epoch: 248 [184576/225000 (82%)] Loss: 7008.308594\n",
      "Train Epoch: 248 [188672/225000 (84%)] Loss: 7187.439453\n",
      "Train Epoch: 248 [192768/225000 (86%)] Loss: 7090.753906\n",
      "Train Epoch: 248 [196864/225000 (87%)] Loss: 6907.832031\n",
      "Train Epoch: 248 [200960/225000 (89%)] Loss: 7111.207031\n",
      "Train Epoch: 248 [205056/225000 (91%)] Loss: 7132.523438\n",
      "Train Epoch: 248 [209152/225000 (93%)] Loss: 6959.943359\n",
      "Train Epoch: 248 [213248/225000 (95%)] Loss: 7057.310547\n",
      "Train Epoch: 248 [217344/225000 (97%)] Loss: 7085.595703\n",
      "Train Epoch: 248 [221440/225000 (98%)] Loss: 6973.259766\n",
      "    epoch          : 248\n",
      "    loss           : 7050.6821427936575\n",
      "    val_loss       : 7058.548947711381\n",
      "Train Epoch: 249 [256/225000 (0%)] Loss: 6956.878906\n",
      "Train Epoch: 249 [4352/225000 (2%)] Loss: 7122.425781\n",
      "Train Epoch: 249 [8448/225000 (4%)] Loss: 6974.640625\n",
      "Train Epoch: 249 [12544/225000 (6%)] Loss: 7014.484375\n",
      "Train Epoch: 249 [16640/225000 (7%)] Loss: 7052.183594\n",
      "Train Epoch: 249 [20736/225000 (9%)] Loss: 7110.113281\n",
      "Train Epoch: 249 [24832/225000 (11%)] Loss: 6968.990234\n",
      "Train Epoch: 249 [28928/225000 (13%)] Loss: 7081.970703\n",
      "Train Epoch: 249 [33024/225000 (15%)] Loss: 7017.312500\n",
      "Train Epoch: 249 [37120/225000 (16%)] Loss: 7100.697266\n",
      "Train Epoch: 249 [41216/225000 (18%)] Loss: 7000.578125\n",
      "Train Epoch: 249 [45312/225000 (20%)] Loss: 7108.119141\n",
      "Train Epoch: 249 [49408/225000 (22%)] Loss: 7029.171875\n",
      "Train Epoch: 249 [53504/225000 (24%)] Loss: 6994.238281\n",
      "Train Epoch: 249 [57600/225000 (26%)] Loss: 7222.593750\n",
      "Train Epoch: 249 [61696/225000 (27%)] Loss: 7044.806641\n",
      "Train Epoch: 249 [65792/225000 (29%)] Loss: 7096.144531\n",
      "Train Epoch: 249 [69888/225000 (31%)] Loss: 6894.152344\n",
      "Train Epoch: 249 [73984/225000 (33%)] Loss: 7027.060547\n",
      "Train Epoch: 249 [78080/225000 (35%)] Loss: 7054.472656\n",
      "Train Epoch: 249 [82176/225000 (37%)] Loss: 7101.697266\n",
      "Train Epoch: 249 [86272/225000 (38%)] Loss: 7141.654297\n",
      "Train Epoch: 249 [90368/225000 (40%)] Loss: 7182.539062\n",
      "Train Epoch: 249 [94464/225000 (42%)] Loss: 6926.498047\n",
      "Train Epoch: 249 [98560/225000 (44%)] Loss: 7025.195312\n",
      "Train Epoch: 249 [102656/225000 (46%)] Loss: 6988.445312\n",
      "Train Epoch: 249 [106752/225000 (47%)] Loss: 6975.982422\n",
      "Train Epoch: 249 [110848/225000 (49%)] Loss: 6980.345703\n",
      "Train Epoch: 249 [114944/225000 (51%)] Loss: 6970.357422\n",
      "Train Epoch: 249 [119040/225000 (53%)] Loss: 7074.339844\n",
      "Train Epoch: 249 [123136/225000 (55%)] Loss: 6959.111328\n",
      "Train Epoch: 249 [127232/225000 (57%)] Loss: 6912.654297\n",
      "Train Epoch: 249 [131328/225000 (58%)] Loss: 7234.705078\n",
      "Train Epoch: 249 [135424/225000 (60%)] Loss: 7005.925781\n",
      "Train Epoch: 249 [139520/225000 (62%)] Loss: 7014.861328\n",
      "Train Epoch: 249 [143616/225000 (64%)] Loss: 7084.058594\n",
      "Train Epoch: 249 [147712/225000 (66%)] Loss: 7026.927734\n",
      "Train Epoch: 249 [151808/225000 (67%)] Loss: 7099.421875\n",
      "Train Epoch: 249 [155904/225000 (69%)] Loss: 7139.716797\n",
      "Train Epoch: 249 [160000/225000 (71%)] Loss: 6964.164062\n",
      "Train Epoch: 249 [164096/225000 (73%)] Loss: 7062.703125\n",
      "Train Epoch: 249 [168192/225000 (75%)] Loss: 7009.839844\n",
      "Train Epoch: 249 [172288/225000 (77%)] Loss: 6849.558594\n",
      "Train Epoch: 249 [176384/225000 (78%)] Loss: 7231.689453\n",
      "Train Epoch: 249 [180480/225000 (80%)] Loss: 7089.736328\n",
      "Train Epoch: 249 [184576/225000 (82%)] Loss: 7085.134766\n",
      "Train Epoch: 249 [188672/225000 (84%)] Loss: 7043.703125\n",
      "Train Epoch: 249 [192768/225000 (86%)] Loss: 7245.460938\n",
      "Train Epoch: 249 [196864/225000 (87%)] Loss: 7162.029297\n",
      "Train Epoch: 249 [200960/225000 (89%)] Loss: 7025.257812\n",
      "Train Epoch: 249 [205056/225000 (91%)] Loss: 7025.244141\n",
      "Train Epoch: 249 [209152/225000 (93%)] Loss: 6849.333984\n",
      "Train Epoch: 249 [213248/225000 (95%)] Loss: 6926.218750\n",
      "Train Epoch: 249 [217344/225000 (97%)] Loss: 7023.009766\n",
      "Train Epoch: 249 [221440/225000 (98%)] Loss: 6904.960938\n",
      "    epoch          : 249\n",
      "    loss           : 7075.488723425057\n",
      "    val_loss       : 7050.594047772641\n",
      "Train Epoch: 250 [256/225000 (0%)] Loss: 6969.060547\n",
      "Train Epoch: 250 [4352/225000 (2%)] Loss: 6881.542969\n",
      "Train Epoch: 250 [8448/225000 (4%)] Loss: 7055.783203\n",
      "Train Epoch: 250 [12544/225000 (6%)] Loss: 6970.240234\n",
      "Train Epoch: 250 [16640/225000 (7%)] Loss: 6993.896484\n",
      "Train Epoch: 250 [20736/225000 (9%)] Loss: 7145.566406\n",
      "Train Epoch: 250 [24832/225000 (11%)] Loss: 7132.035156\n",
      "Train Epoch: 250 [28928/225000 (13%)] Loss: 6946.753906\n",
      "Train Epoch: 250 [33024/225000 (15%)] Loss: 7084.423828\n",
      "Train Epoch: 250 [37120/225000 (16%)] Loss: 7257.626953\n",
      "Train Epoch: 250 [41216/225000 (18%)] Loss: 6953.716797\n",
      "Train Epoch: 250 [45312/225000 (20%)] Loss: 7026.041016\n",
      "Train Epoch: 250 [49408/225000 (22%)] Loss: 7133.984375\n",
      "Train Epoch: 250 [53504/225000 (24%)] Loss: 6910.531250\n",
      "Train Epoch: 250 [57600/225000 (26%)] Loss: 7078.095703\n",
      "Train Epoch: 250 [61696/225000 (27%)] Loss: 7168.976562\n",
      "Train Epoch: 250 [65792/225000 (29%)] Loss: 7030.921875\n",
      "Train Epoch: 250 [69888/225000 (31%)] Loss: 6947.982422\n",
      "Train Epoch: 250 [73984/225000 (33%)] Loss: 7062.300781\n",
      "Train Epoch: 250 [78080/225000 (35%)] Loss: 7072.675781\n",
      "Train Epoch: 250 [82176/225000 (37%)] Loss: 12466.896484\n",
      "Train Epoch: 250 [86272/225000 (38%)] Loss: 7065.126953\n",
      "Train Epoch: 250 [90368/225000 (40%)] Loss: 7093.546875\n",
      "Train Epoch: 250 [94464/225000 (42%)] Loss: 7091.376953\n",
      "Train Epoch: 250 [98560/225000 (44%)] Loss: 7117.322266\n",
      "Train Epoch: 250 [102656/225000 (46%)] Loss: 7015.044922\n",
      "Train Epoch: 250 [106752/225000 (47%)] Loss: 7206.408203\n",
      "Train Epoch: 250 [110848/225000 (49%)] Loss: 7207.656250\n",
      "Train Epoch: 250 [114944/225000 (51%)] Loss: 6901.953125\n",
      "Train Epoch: 250 [119040/225000 (53%)] Loss: 7128.412109\n",
      "Train Epoch: 250 [123136/225000 (55%)] Loss: 6917.216797\n",
      "Train Epoch: 250 [127232/225000 (57%)] Loss: 6888.642578\n",
      "Train Epoch: 250 [131328/225000 (58%)] Loss: 6948.746094\n",
      "Train Epoch: 250 [135424/225000 (60%)] Loss: 6927.898438\n",
      "Train Epoch: 250 [139520/225000 (62%)] Loss: 7076.242188\n",
      "Train Epoch: 250 [143616/225000 (64%)] Loss: 7036.605469\n",
      "Train Epoch: 250 [147712/225000 (66%)] Loss: 7003.550781\n",
      "Train Epoch: 250 [151808/225000 (67%)] Loss: 6983.380859\n",
      "Train Epoch: 250 [155904/225000 (69%)] Loss: 7013.972656\n",
      "Train Epoch: 250 [160000/225000 (71%)] Loss: 6933.007812\n",
      "Train Epoch: 250 [164096/225000 (73%)] Loss: 6969.941406\n",
      "Train Epoch: 250 [168192/225000 (75%)] Loss: 7125.068359\n",
      "Train Epoch: 250 [172288/225000 (77%)] Loss: 7066.808594\n",
      "Train Epoch: 250 [176384/225000 (78%)] Loss: 6988.509766\n",
      "Train Epoch: 250 [180480/225000 (80%)] Loss: 7284.183594\n",
      "Train Epoch: 250 [184576/225000 (82%)] Loss: 6936.011719\n",
      "Train Epoch: 250 [188672/225000 (84%)] Loss: 6950.708984\n",
      "Train Epoch: 250 [192768/225000 (86%)] Loss: 7230.830078\n",
      "Train Epoch: 250 [196864/225000 (87%)] Loss: 7143.283203\n",
      "Train Epoch: 250 [200960/225000 (89%)] Loss: 7161.339844\n",
      "Train Epoch: 250 [205056/225000 (91%)] Loss: 6974.921875\n",
      "Train Epoch: 250 [209152/225000 (93%)] Loss: 7109.482422\n",
      "Train Epoch: 250 [213248/225000 (95%)] Loss: 7185.337891\n",
      "Train Epoch: 250 [217344/225000 (97%)] Loss: 7105.603516\n",
      "Train Epoch: 250 [221440/225000 (98%)] Loss: 7158.654297\n",
      "    epoch          : 250\n",
      "    loss           : 7072.511696530148\n",
      "    val_loss       : 7051.35439722392\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0827_225017/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [256/225000 (0%)] Loss: 6980.832031\n",
      "Train Epoch: 251 [4352/225000 (2%)] Loss: 7019.009766\n",
      "Train Epoch: 251 [8448/225000 (4%)] Loss: 7059.498047\n",
      "Train Epoch: 251 [12544/225000 (6%)] Loss: 7066.720703\n",
      "Train Epoch: 251 [16640/225000 (7%)] Loss: 7160.734375\n",
      "Train Epoch: 251 [20736/225000 (9%)] Loss: 6832.398438\n",
      "Train Epoch: 251 [24832/225000 (11%)] Loss: 7108.179688\n",
      "Train Epoch: 251 [28928/225000 (13%)] Loss: 7047.830078\n",
      "Train Epoch: 251 [33024/225000 (15%)] Loss: 7061.826172\n",
      "Train Epoch: 251 [37120/225000 (16%)] Loss: 7079.480469\n",
      "Train Epoch: 251 [41216/225000 (18%)] Loss: 7216.562500\n",
      "Train Epoch: 251 [45312/225000 (20%)] Loss: 6923.876953\n",
      "Train Epoch: 251 [49408/225000 (22%)] Loss: 7226.539062\n",
      "Train Epoch: 251 [53504/225000 (24%)] Loss: 6938.931641\n",
      "Train Epoch: 251 [57600/225000 (26%)] Loss: 7211.804688\n",
      "Train Epoch: 251 [61696/225000 (27%)] Loss: 6905.869141\n",
      "Train Epoch: 251 [65792/225000 (29%)] Loss: 7106.257812\n",
      "Train Epoch: 251 [69888/225000 (31%)] Loss: 6949.279297\n",
      "Train Epoch: 251 [73984/225000 (33%)] Loss: 7031.947266\n",
      "Train Epoch: 251 [78080/225000 (35%)] Loss: 7147.824219\n",
      "Train Epoch: 251 [82176/225000 (37%)] Loss: 7134.726562\n",
      "Train Epoch: 251 [86272/225000 (38%)] Loss: 6936.095703\n",
      "Train Epoch: 251 [90368/225000 (40%)] Loss: 6959.529297\n",
      "Train Epoch: 251 [94464/225000 (42%)] Loss: 7085.501953\n",
      "Train Epoch: 251 [98560/225000 (44%)] Loss: 6943.660156\n",
      "Train Epoch: 251 [102656/225000 (46%)] Loss: 6981.562500\n",
      "Train Epoch: 251 [106752/225000 (47%)] Loss: 6919.453125\n",
      "Train Epoch: 251 [110848/225000 (49%)] Loss: 6857.269531\n",
      "Train Epoch: 251 [114944/225000 (51%)] Loss: 7072.488281\n",
      "Train Epoch: 251 [119040/225000 (53%)] Loss: 7069.554688\n",
      "Train Epoch: 251 [123136/225000 (55%)] Loss: 7141.343750\n",
      "Train Epoch: 251 [127232/225000 (57%)] Loss: 6975.736328\n",
      "Train Epoch: 251 [131328/225000 (58%)] Loss: 7023.757812\n",
      "Train Epoch: 251 [135424/225000 (60%)] Loss: 7006.636719\n",
      "Train Epoch: 251 [139520/225000 (62%)] Loss: 7049.328125\n",
      "Train Epoch: 251 [143616/225000 (64%)] Loss: 7156.666016\n",
      "Train Epoch: 251 [147712/225000 (66%)] Loss: 7273.451172\n",
      "Train Epoch: 251 [151808/225000 (67%)] Loss: 6957.458984\n",
      "Train Epoch: 251 [155904/225000 (69%)] Loss: 7025.244141\n",
      "Train Epoch: 251 [160000/225000 (71%)] Loss: 7004.267578\n",
      "Train Epoch: 251 [164096/225000 (73%)] Loss: 7027.462891\n",
      "Train Epoch: 251 [168192/225000 (75%)] Loss: 7034.824219\n",
      "Train Epoch: 251 [172288/225000 (77%)] Loss: 7138.707031\n",
      "Train Epoch: 251 [176384/225000 (78%)] Loss: 7030.910156\n",
      "Train Epoch: 251 [180480/225000 (80%)] Loss: 6922.462891\n",
      "Train Epoch: 251 [184576/225000 (82%)] Loss: 6976.951172\n",
      "Train Epoch: 251 [188672/225000 (84%)] Loss: 7045.757812\n",
      "Train Epoch: 251 [192768/225000 (86%)] Loss: 6904.273438\n",
      "Train Epoch: 251 [196864/225000 (87%)] Loss: 7110.822266\n",
      "Train Epoch: 251 [200960/225000 (89%)] Loss: 7035.984375\n",
      "Train Epoch: 251 [205056/225000 (91%)] Loss: 7010.142578\n",
      "Train Epoch: 251 [209152/225000 (93%)] Loss: 6871.248047\n",
      "Train Epoch: 251 [213248/225000 (95%)] Loss: 6995.705078\n",
      "Train Epoch: 251 [217344/225000 (97%)] Loss: 7103.710938\n",
      "Train Epoch: 251 [221440/225000 (98%)] Loss: 6943.654297\n",
      "    epoch          : 251\n",
      "    loss           : 7076.966348033988\n",
      "    val_loss       : 7049.800328933463\n",
      "Train Epoch: 252 [256/225000 (0%)] Loss: 7102.839844\n",
      "Train Epoch: 252 [4352/225000 (2%)] Loss: 7189.324219\n",
      "Train Epoch: 252 [8448/225000 (4%)] Loss: 7018.458984\n",
      "Train Epoch: 252 [12544/225000 (6%)] Loss: 7239.601562\n",
      "Train Epoch: 252 [16640/225000 (7%)] Loss: 7055.521484\n",
      "Train Epoch: 252 [20736/225000 (9%)] Loss: 6981.353516\n",
      "Train Epoch: 252 [24832/225000 (11%)] Loss: 7128.875000\n",
      "Train Epoch: 252 [28928/225000 (13%)] Loss: 6929.525391\n",
      "Train Epoch: 252 [33024/225000 (15%)] Loss: 6991.394531\n",
      "Train Epoch: 252 [37120/225000 (16%)] Loss: 6987.283203\n",
      "Train Epoch: 252 [41216/225000 (18%)] Loss: 6971.648438\n",
      "Train Epoch: 252 [45312/225000 (20%)] Loss: 6939.751953\n",
      "Train Epoch: 252 [49408/225000 (22%)] Loss: 6988.550781\n",
      "Train Epoch: 252 [53504/225000 (24%)] Loss: 7121.787109\n",
      "Train Epoch: 252 [57600/225000 (26%)] Loss: 6970.580078\n",
      "Train Epoch: 252 [61696/225000 (27%)] Loss: 7144.542969\n",
      "Train Epoch: 252 [65792/225000 (29%)] Loss: 7106.089844\n",
      "Train Epoch: 252 [69888/225000 (31%)] Loss: 7038.162109\n",
      "Train Epoch: 252 [73984/225000 (33%)] Loss: 7062.562500\n",
      "Train Epoch: 252 [78080/225000 (35%)] Loss: 6964.562500\n",
      "Train Epoch: 252 [82176/225000 (37%)] Loss: 7078.287109\n",
      "Train Epoch: 252 [86272/225000 (38%)] Loss: 7061.544922\n",
      "Train Epoch: 252 [90368/225000 (40%)] Loss: 6986.945312\n",
      "Train Epoch: 252 [94464/225000 (42%)] Loss: 6965.833984\n",
      "Train Epoch: 252 [98560/225000 (44%)] Loss: 6983.263672\n",
      "Train Epoch: 252 [102656/225000 (46%)] Loss: 7008.189453\n",
      "Train Epoch: 252 [106752/225000 (47%)] Loss: 6961.828125\n",
      "Train Epoch: 252 [110848/225000 (49%)] Loss: 7130.443359\n",
      "Train Epoch: 252 [114944/225000 (51%)] Loss: 6976.835938\n",
      "Train Epoch: 252 [119040/225000 (53%)] Loss: 7116.945312\n",
      "Train Epoch: 252 [123136/225000 (55%)] Loss: 6986.382812\n",
      "Train Epoch: 252 [127232/225000 (57%)] Loss: 7203.978516\n",
      "Train Epoch: 252 [131328/225000 (58%)] Loss: 7151.613281\n",
      "Train Epoch: 252 [135424/225000 (60%)] Loss: 6965.556641\n",
      "Train Epoch: 252 [139520/225000 (62%)] Loss: 6893.316406\n",
      "Train Epoch: 252 [143616/225000 (64%)] Loss: 7161.718750\n",
      "Train Epoch: 252 [147712/225000 (66%)] Loss: 6980.228516\n",
      "Train Epoch: 252 [151808/225000 (67%)] Loss: 7144.720703\n",
      "Train Epoch: 252 [155904/225000 (69%)] Loss: 7219.662109\n",
      "Train Epoch: 252 [160000/225000 (71%)] Loss: 6881.755859\n",
      "Train Epoch: 252 [164096/225000 (73%)] Loss: 7119.027344\n",
      "Train Epoch: 252 [168192/225000 (75%)] Loss: 6920.609375\n",
      "Train Epoch: 252 [172288/225000 (77%)] Loss: 7089.833984\n",
      "Train Epoch: 252 [176384/225000 (78%)] Loss: 7076.830078\n",
      "Train Epoch: 252 [180480/225000 (80%)] Loss: 7080.957031\n",
      "Train Epoch: 252 [184576/225000 (82%)] Loss: 7011.785156\n",
      "Train Epoch: 252 [188672/225000 (84%)] Loss: 7033.826172\n",
      "Train Epoch: 252 [192768/225000 (86%)] Loss: 7116.689453\n",
      "Train Epoch: 252 [196864/225000 (87%)] Loss: 7037.695312\n",
      "Train Epoch: 252 [200960/225000 (89%)] Loss: 6911.580078\n",
      "Train Epoch: 252 [205056/225000 (91%)] Loss: 6976.382812\n",
      "Train Epoch: 252 [209152/225000 (93%)] Loss: 7007.986328\n",
      "Train Epoch: 252 [213248/225000 (95%)] Loss: 7061.335938\n",
      "Train Epoch: 252 [217344/225000 (97%)] Loss: 7107.527344\n",
      "Train Epoch: 252 [221440/225000 (98%)] Loss: 6872.308594\n",
      "    epoch          : 252\n",
      "    loss           : 7041.370672683803\n",
      "    val_loss       : 7041.818901364901\n",
      "Train Epoch: 253 [256/225000 (0%)] Loss: 7004.955078\n",
      "Train Epoch: 253 [4352/225000 (2%)] Loss: 7006.902344\n",
      "Train Epoch: 253 [8448/225000 (4%)] Loss: 7202.255859\n",
      "Train Epoch: 253 [12544/225000 (6%)] Loss: 7059.695312\n",
      "Train Epoch: 253 [16640/225000 (7%)] Loss: 6956.527344\n",
      "Train Epoch: 253 [20736/225000 (9%)] Loss: 7062.783203\n",
      "Train Epoch: 253 [24832/225000 (11%)] Loss: 7073.648438\n",
      "Train Epoch: 253 [28928/225000 (13%)] Loss: 6884.771484\n",
      "Train Epoch: 253 [33024/225000 (15%)] Loss: 7096.250000\n",
      "Train Epoch: 253 [37120/225000 (16%)] Loss: 7093.496094\n",
      "Train Epoch: 253 [41216/225000 (18%)] Loss: 6983.496094\n",
      "Train Epoch: 253 [45312/225000 (20%)] Loss: 7109.583984\n",
      "Train Epoch: 253 [49408/225000 (22%)] Loss: 6929.886719\n",
      "Train Epoch: 253 [53504/225000 (24%)] Loss: 7198.271484\n",
      "Train Epoch: 253 [57600/225000 (26%)] Loss: 7006.160156\n",
      "Train Epoch: 253 [61696/225000 (27%)] Loss: 6792.533203\n",
      "Train Epoch: 253 [65792/225000 (29%)] Loss: 6947.746094\n",
      "Train Epoch: 253 [69888/225000 (31%)] Loss: 7021.541016\n",
      "Train Epoch: 253 [73984/225000 (33%)] Loss: 6937.339844\n",
      "Train Epoch: 253 [78080/225000 (35%)] Loss: 7096.402344\n",
      "Train Epoch: 253 [82176/225000 (37%)] Loss: 7124.251953\n",
      "Train Epoch: 253 [86272/225000 (38%)] Loss: 6961.777344\n",
      "Train Epoch: 253 [90368/225000 (40%)] Loss: 7020.562500\n",
      "Train Epoch: 253 [94464/225000 (42%)] Loss: 6974.304688\n",
      "Train Epoch: 253 [98560/225000 (44%)] Loss: 7059.468750\n",
      "Train Epoch: 253 [102656/225000 (46%)] Loss: 6957.466797\n",
      "Train Epoch: 253 [106752/225000 (47%)] Loss: 7003.976562\n",
      "Train Epoch: 253 [110848/225000 (49%)] Loss: 7045.423828\n",
      "Train Epoch: 253 [114944/225000 (51%)] Loss: 7109.296875\n",
      "Train Epoch: 253 [119040/225000 (53%)] Loss: 7076.968750\n",
      "Train Epoch: 253 [123136/225000 (55%)] Loss: 7250.363281\n",
      "Train Epoch: 253 [127232/225000 (57%)] Loss: 6880.179688\n",
      "Train Epoch: 253 [131328/225000 (58%)] Loss: 7070.136719\n",
      "Train Epoch: 253 [135424/225000 (60%)] Loss: 6986.580078\n",
      "Train Epoch: 253 [139520/225000 (62%)] Loss: 7114.468750\n",
      "Train Epoch: 253 [143616/225000 (64%)] Loss: 7048.308594\n",
      "Train Epoch: 253 [147712/225000 (66%)] Loss: 7175.230469\n",
      "Train Epoch: 253 [151808/225000 (67%)] Loss: 6999.755859\n",
      "Train Epoch: 253 [155904/225000 (69%)] Loss: 7107.689453\n",
      "Train Epoch: 253 [160000/225000 (71%)] Loss: 6898.003906\n",
      "Train Epoch: 253 [164096/225000 (73%)] Loss: 6964.070312\n",
      "Train Epoch: 253 [168192/225000 (75%)] Loss: 7031.517578\n",
      "Train Epoch: 253 [172288/225000 (77%)] Loss: 7050.740234\n",
      "Train Epoch: 253 [176384/225000 (78%)] Loss: 7061.283203\n",
      "Train Epoch: 253 [180480/225000 (80%)] Loss: 6988.087891\n",
      "Train Epoch: 253 [184576/225000 (82%)] Loss: 6942.355469\n",
      "Train Epoch: 253 [188672/225000 (84%)] Loss: 7026.816406\n",
      "Train Epoch: 253 [192768/225000 (86%)] Loss: 7025.890625\n",
      "Train Epoch: 253 [196864/225000 (87%)] Loss: 6960.740234\n",
      "Train Epoch: 253 [200960/225000 (89%)] Loss: 7124.261719\n",
      "Train Epoch: 253 [205056/225000 (91%)] Loss: 7126.675781\n",
      "Train Epoch: 253 [209152/225000 (93%)] Loss: 7130.398438\n",
      "Train Epoch: 253 [213248/225000 (95%)] Loss: 6922.039062\n",
      "Train Epoch: 253 [217344/225000 (97%)] Loss: 7046.353516\n",
      "Train Epoch: 253 [221440/225000 (98%)] Loss: 7057.955078\n",
      "    epoch          : 253\n",
      "    loss           : 7072.618749555603\n",
      "    val_loss       : 7097.277896095296\n",
      "Train Epoch: 254 [256/225000 (0%)] Loss: 7101.058594\n",
      "Train Epoch: 254 [4352/225000 (2%)] Loss: 7058.177734\n",
      "Train Epoch: 254 [8448/225000 (4%)] Loss: 7042.457031\n",
      "Train Epoch: 254 [12544/225000 (6%)] Loss: 6933.890625\n",
      "Train Epoch: 254 [16640/225000 (7%)] Loss: 7023.007812\n",
      "Train Epoch: 254 [20736/225000 (9%)] Loss: 7180.839844\n",
      "Train Epoch: 254 [24832/225000 (11%)] Loss: 7031.121094\n",
      "Train Epoch: 254 [28928/225000 (13%)] Loss: 7175.009766\n",
      "Train Epoch: 254 [33024/225000 (15%)] Loss: 7068.523438\n",
      "Train Epoch: 254 [37120/225000 (16%)] Loss: 7032.177734\n",
      "Train Epoch: 254 [41216/225000 (18%)] Loss: 7036.775391\n",
      "Train Epoch: 254 [45312/225000 (20%)] Loss: 7001.291016\n",
      "Train Epoch: 254 [49408/225000 (22%)] Loss: 7007.210938\n",
      "Train Epoch: 254 [53504/225000 (24%)] Loss: 7037.386719\n",
      "Train Epoch: 254 [57600/225000 (26%)] Loss: 7082.412109\n",
      "Train Epoch: 254 [61696/225000 (27%)] Loss: 7062.238281\n",
      "Train Epoch: 254 [65792/225000 (29%)] Loss: 7009.398438\n",
      "Train Epoch: 254 [69888/225000 (31%)] Loss: 6984.615234\n",
      "Train Epoch: 254 [73984/225000 (33%)] Loss: 7075.412109\n",
      "Train Epoch: 254 [78080/225000 (35%)] Loss: 7165.402344\n",
      "Train Epoch: 254 [82176/225000 (37%)] Loss: 7067.845703\n",
      "Train Epoch: 254 [86272/225000 (38%)] Loss: 7077.111328\n",
      "Train Epoch: 254 [90368/225000 (40%)] Loss: 6904.351562\n",
      "Train Epoch: 254 [94464/225000 (42%)] Loss: 7000.234375\n",
      "Train Epoch: 254 [98560/225000 (44%)] Loss: 7166.683594\n",
      "Train Epoch: 254 [102656/225000 (46%)] Loss: 7127.617188\n",
      "Train Epoch: 254 [106752/225000 (47%)] Loss: 6956.304688\n",
      "Train Epoch: 254 [110848/225000 (49%)] Loss: 6796.498047\n",
      "Train Epoch: 254 [114944/225000 (51%)] Loss: 7062.712891\n",
      "Train Epoch: 254 [119040/225000 (53%)] Loss: 7174.062500\n",
      "Train Epoch: 254 [123136/225000 (55%)] Loss: 7043.679688\n",
      "Train Epoch: 254 [127232/225000 (57%)] Loss: 7200.007812\n",
      "Train Epoch: 254 [131328/225000 (58%)] Loss: 7041.271484\n",
      "Train Epoch: 254 [135424/225000 (60%)] Loss: 7078.451172\n",
      "Train Epoch: 254 [139520/225000 (62%)] Loss: 7013.587891\n",
      "Train Epoch: 254 [143616/225000 (64%)] Loss: 7017.599609\n",
      "Train Epoch: 254 [147712/225000 (66%)] Loss: 7081.585938\n",
      "Train Epoch: 254 [151808/225000 (67%)] Loss: 6881.294922\n",
      "Train Epoch: 254 [155904/225000 (69%)] Loss: 7068.900391\n",
      "Train Epoch: 254 [160000/225000 (71%)] Loss: 7023.646484\n",
      "Train Epoch: 254 [164096/225000 (73%)] Loss: 6987.337891\n",
      "Train Epoch: 254 [168192/225000 (75%)] Loss: 7073.265625\n",
      "Train Epoch: 254 [172288/225000 (77%)] Loss: 6969.414062\n",
      "Train Epoch: 254 [176384/225000 (78%)] Loss: 7002.015625\n",
      "Train Epoch: 254 [180480/225000 (80%)] Loss: 6974.253906\n",
      "Train Epoch: 254 [184576/225000 (82%)] Loss: 6916.015625\n",
      "Train Epoch: 254 [188672/225000 (84%)] Loss: 6949.828125\n",
      "Train Epoch: 254 [192768/225000 (86%)] Loss: 7086.298828\n",
      "Train Epoch: 254 [196864/225000 (87%)] Loss: 7014.767578\n",
      "Train Epoch: 254 [200960/225000 (89%)] Loss: 6961.888672\n",
      "Train Epoch: 254 [205056/225000 (91%)] Loss: 7147.507812\n",
      "Train Epoch: 254 [209152/225000 (93%)] Loss: 7058.099609\n",
      "Train Epoch: 254 [213248/225000 (95%)] Loss: 6952.279297\n",
      "Train Epoch: 254 [217344/225000 (97%)] Loss: 7065.007812\n",
      "Train Epoch: 254 [221440/225000 (98%)] Loss: 7137.302734\n",
      "    epoch          : 254\n",
      "    loss           : 7036.8763176372295\n",
      "    val_loss       : 7041.656312424309\n",
      "Train Epoch: 255 [256/225000 (0%)] Loss: 7124.765625\n",
      "Train Epoch: 255 [4352/225000 (2%)] Loss: 7052.410156\n",
      "Train Epoch: 255 [8448/225000 (4%)] Loss: 6923.824219\n",
      "Train Epoch: 255 [12544/225000 (6%)] Loss: 7083.183594\n",
      "Train Epoch: 255 [16640/225000 (7%)] Loss: 7086.945312\n",
      "Train Epoch: 255 [20736/225000 (9%)] Loss: 7119.167969\n",
      "Train Epoch: 255 [24832/225000 (11%)] Loss: 7064.498047\n",
      "Train Epoch: 255 [28928/225000 (13%)] Loss: 7063.828125\n",
      "Train Epoch: 255 [33024/225000 (15%)] Loss: 7242.000000\n",
      "Train Epoch: 255 [37120/225000 (16%)] Loss: 7111.800781\n",
      "Train Epoch: 255 [41216/225000 (18%)] Loss: 7140.880859\n",
      "Train Epoch: 255 [45312/225000 (20%)] Loss: 7023.791016\n",
      "Train Epoch: 255 [49408/225000 (22%)] Loss: 7068.619141\n",
      "Train Epoch: 255 [53504/225000 (24%)] Loss: 7033.380859\n",
      "Train Epoch: 255 [57600/225000 (26%)] Loss: 7012.779297\n",
      "Train Epoch: 255 [61696/225000 (27%)] Loss: 7059.435547\n",
      "Train Epoch: 255 [65792/225000 (29%)] Loss: 7049.837891\n",
      "Train Epoch: 255 [69888/225000 (31%)] Loss: 7083.498047\n",
      "Train Epoch: 255 [73984/225000 (33%)] Loss: 7095.132812\n",
      "Train Epoch: 255 [78080/225000 (35%)] Loss: 7043.875000\n",
      "Train Epoch: 255 [82176/225000 (37%)] Loss: 7013.433594\n",
      "Train Epoch: 255 [86272/225000 (38%)] Loss: 7047.515625\n",
      "Train Epoch: 255 [90368/225000 (40%)] Loss: 6865.031250\n",
      "Train Epoch: 255 [94464/225000 (42%)] Loss: 7124.042969\n",
      "Train Epoch: 255 [98560/225000 (44%)] Loss: 7057.910156\n",
      "Train Epoch: 255 [102656/225000 (46%)] Loss: 6849.275391\n",
      "Train Epoch: 255 [106752/225000 (47%)] Loss: 7011.873047\n",
      "Train Epoch: 255 [110848/225000 (49%)] Loss: 6994.687500\n",
      "Train Epoch: 255 [114944/225000 (51%)] Loss: 6911.835938\n",
      "Train Epoch: 255 [119040/225000 (53%)] Loss: 6918.566406\n",
      "Train Epoch: 255 [123136/225000 (55%)] Loss: 7069.783203\n",
      "Train Epoch: 255 [127232/225000 (57%)] Loss: 7115.390625\n",
      "Train Epoch: 255 [131328/225000 (58%)] Loss: 7023.330078\n",
      "Train Epoch: 255 [135424/225000 (60%)] Loss: 6984.572266\n",
      "Train Epoch: 255 [139520/225000 (62%)] Loss: 6996.240234\n",
      "Train Epoch: 255 [143616/225000 (64%)] Loss: 7007.300781\n",
      "Train Epoch: 255 [147712/225000 (66%)] Loss: 7058.919922\n",
      "Train Epoch: 255 [151808/225000 (67%)] Loss: 7060.865234\n",
      "Train Epoch: 255 [155904/225000 (69%)] Loss: 7021.968750\n",
      "Train Epoch: 255 [160000/225000 (71%)] Loss: 7107.839844\n",
      "Train Epoch: 255 [164096/225000 (73%)] Loss: 6926.396484\n",
      "Train Epoch: 255 [168192/225000 (75%)] Loss: 7183.802734\n",
      "Train Epoch: 255 [172288/225000 (77%)] Loss: 7048.375000\n",
      "Train Epoch: 255 [176384/225000 (78%)] Loss: 7022.183594\n",
      "Train Epoch: 255 [180480/225000 (80%)] Loss: 7370.884766\n",
      "Train Epoch: 255 [184576/225000 (82%)] Loss: 6900.156250\n",
      "Train Epoch: 255 [188672/225000 (84%)] Loss: 6955.742188\n",
      "Train Epoch: 255 [192768/225000 (86%)] Loss: 6917.888672\n",
      "Train Epoch: 255 [196864/225000 (87%)] Loss: 6881.353516\n",
      "Train Epoch: 255 [200960/225000 (89%)] Loss: 7156.310547\n",
      "Train Epoch: 255 [205056/225000 (91%)] Loss: 7070.001953\n",
      "Train Epoch: 255 [209152/225000 (93%)] Loss: 7040.105469\n",
      "Train Epoch: 255 [213248/225000 (95%)] Loss: 7024.154297\n",
      "Train Epoch: 255 [217344/225000 (97%)] Loss: 7086.601562\n",
      "Train Epoch: 255 [221440/225000 (98%)] Loss: 6982.466797\n",
      "    epoch          : 255\n",
      "    loss           : 7059.7095043195395\n",
      "    val_loss       : 7038.6959898909745\n",
      "Train Epoch: 256 [256/225000 (0%)] Loss: 7150.968750\n",
      "Train Epoch: 256 [4352/225000 (2%)] Loss: 7077.291016\n",
      "Train Epoch: 256 [8448/225000 (4%)] Loss: 6844.080078\n",
      "Train Epoch: 256 [12544/225000 (6%)] Loss: 7087.357422\n",
      "Train Epoch: 256 [16640/225000 (7%)] Loss: 6965.916016\n",
      "Train Epoch: 256 [20736/225000 (9%)] Loss: 7061.855469\n",
      "Train Epoch: 256 [24832/225000 (11%)] Loss: 7177.089844\n",
      "Train Epoch: 256 [28928/225000 (13%)] Loss: 7264.236328\n",
      "Train Epoch: 256 [33024/225000 (15%)] Loss: 7267.349609\n",
      "Train Epoch: 256 [37120/225000 (16%)] Loss: 6991.917969\n",
      "Train Epoch: 256 [41216/225000 (18%)] Loss: 7031.343750\n",
      "Train Epoch: 256 [45312/225000 (20%)] Loss: 7135.339844\n",
      "Train Epoch: 256 [49408/225000 (22%)] Loss: 7078.488281\n",
      "Train Epoch: 256 [53504/225000 (24%)] Loss: 7091.005859\n",
      "Train Epoch: 256 [57600/225000 (26%)] Loss: 7027.525391\n",
      "Train Epoch: 256 [61696/225000 (27%)] Loss: 7033.939453\n",
      "Train Epoch: 256 [65792/225000 (29%)] Loss: 6880.218750\n",
      "Train Epoch: 256 [69888/225000 (31%)] Loss: 7030.580078\n",
      "Train Epoch: 256 [73984/225000 (33%)] Loss: 6904.050781\n",
      "Train Epoch: 256 [78080/225000 (35%)] Loss: 6967.574219\n",
      "Train Epoch: 256 [82176/225000 (37%)] Loss: 7052.162109\n",
      "Train Epoch: 256 [86272/225000 (38%)] Loss: 7082.398438\n",
      "Train Epoch: 256 [90368/225000 (40%)] Loss: 7024.941406\n",
      "Train Epoch: 256 [94464/225000 (42%)] Loss: 7087.716797\n",
      "Train Epoch: 256 [98560/225000 (44%)] Loss: 7079.289062\n",
      "Train Epoch: 256 [102656/225000 (46%)] Loss: 6967.710938\n",
      "Train Epoch: 256 [106752/225000 (47%)] Loss: 6999.960938\n",
      "Train Epoch: 256 [110848/225000 (49%)] Loss: 7104.859375\n",
      "Train Epoch: 256 [114944/225000 (51%)] Loss: 7186.201172\n",
      "Train Epoch: 256 [119040/225000 (53%)] Loss: 6923.865234\n",
      "Train Epoch: 256 [123136/225000 (55%)] Loss: 7082.287109\n",
      "Train Epoch: 256 [127232/225000 (57%)] Loss: 7075.062500\n",
      "Train Epoch: 256 [131328/225000 (58%)] Loss: 7138.380859\n",
      "Train Epoch: 256 [135424/225000 (60%)] Loss: 7029.900391\n",
      "Train Epoch: 256 [139520/225000 (62%)] Loss: 6989.628906\n",
      "Train Epoch: 256 [143616/225000 (64%)] Loss: 7155.515625\n",
      "Train Epoch: 256 [147712/225000 (66%)] Loss: 7084.962891\n",
      "Train Epoch: 256 [151808/225000 (67%)] Loss: 7108.179688\n",
      "Train Epoch: 256 [155904/225000 (69%)] Loss: 7073.107422\n",
      "Train Epoch: 256 [160000/225000 (71%)] Loss: 6979.191406\n",
      "Train Epoch: 256 [164096/225000 (73%)] Loss: 6966.173828\n",
      "Train Epoch: 256 [168192/225000 (75%)] Loss: 7022.343750\n",
      "Train Epoch: 256 [172288/225000 (77%)] Loss: 6944.683594\n",
      "Train Epoch: 256 [176384/225000 (78%)] Loss: 6946.873047\n",
      "Train Epoch: 256 [180480/225000 (80%)] Loss: 7218.082031\n",
      "Train Epoch: 256 [184576/225000 (82%)] Loss: 6880.109375\n",
      "Train Epoch: 256 [188672/225000 (84%)] Loss: 6961.580078\n",
      "Train Epoch: 256 [192768/225000 (86%)] Loss: 7020.445312\n",
      "Train Epoch: 256 [196864/225000 (87%)] Loss: 7158.966797\n",
      "Train Epoch: 256 [200960/225000 (89%)] Loss: 7144.992188\n",
      "Train Epoch: 256 [205056/225000 (91%)] Loss: 6973.662109\n",
      "Train Epoch: 256 [209152/225000 (93%)] Loss: 7080.105469\n",
      "Train Epoch: 256 [213248/225000 (95%)] Loss: 6861.894531\n",
      "Train Epoch: 256 [217344/225000 (97%)] Loss: 7083.171875\n",
      "Train Epoch: 256 [221440/225000 (98%)] Loss: 7075.480469\n",
      "    epoch          : 256\n",
      "    loss           : 7033.389212928399\n",
      "    val_loss       : 7035.545241331568\n",
      "Train Epoch: 257 [256/225000 (0%)] Loss: 7048.445312\n",
      "Train Epoch: 257 [4352/225000 (2%)] Loss: 6977.664062\n",
      "Train Epoch: 257 [8448/225000 (4%)] Loss: 6937.503906\n",
      "Train Epoch: 257 [12544/225000 (6%)] Loss: 7046.660156\n",
      "Train Epoch: 257 [16640/225000 (7%)] Loss: 6992.462891\n",
      "Train Epoch: 257 [20736/225000 (9%)] Loss: 7206.683594\n",
      "Train Epoch: 257 [24832/225000 (11%)] Loss: 7187.746094\n",
      "Train Epoch: 257 [28928/225000 (13%)] Loss: 6883.482422\n",
      "Train Epoch: 257 [33024/225000 (15%)] Loss: 6995.763672\n",
      "Train Epoch: 257 [37120/225000 (16%)] Loss: 7128.003906\n",
      "Train Epoch: 257 [41216/225000 (18%)] Loss: 6996.443359\n",
      "Train Epoch: 257 [45312/225000 (20%)] Loss: 6978.240234\n",
      "Train Epoch: 257 [49408/225000 (22%)] Loss: 7171.763672\n",
      "Train Epoch: 257 [53504/225000 (24%)] Loss: 7212.783203\n",
      "Train Epoch: 257 [57600/225000 (26%)] Loss: 7083.468750\n",
      "Train Epoch: 257 [61696/225000 (27%)] Loss: 6943.613281\n",
      "Train Epoch: 257 [65792/225000 (29%)] Loss: 7147.195312\n",
      "Train Epoch: 257 [69888/225000 (31%)] Loss: 7059.384766\n",
      "Train Epoch: 257 [73984/225000 (33%)] Loss: 6965.666016\n",
      "Train Epoch: 257 [78080/225000 (35%)] Loss: 7215.468750\n",
      "Train Epoch: 257 [82176/225000 (37%)] Loss: 7082.728516\n",
      "Train Epoch: 257 [86272/225000 (38%)] Loss: 7156.892578\n",
      "Train Epoch: 257 [90368/225000 (40%)] Loss: 7080.605469\n",
      "Train Epoch: 257 [94464/225000 (42%)] Loss: 7036.187500\n",
      "Train Epoch: 257 [98560/225000 (44%)] Loss: 7086.304688\n",
      "Train Epoch: 257 [102656/225000 (46%)] Loss: 6945.763672\n",
      "Train Epoch: 257 [106752/225000 (47%)] Loss: 7077.746094\n",
      "Train Epoch: 257 [110848/225000 (49%)] Loss: 7067.066406\n",
      "Train Epoch: 257 [114944/225000 (51%)] Loss: 6996.681641\n",
      "Train Epoch: 257 [119040/225000 (53%)] Loss: 6987.054688\n",
      "Train Epoch: 257 [123136/225000 (55%)] Loss: 6934.750000\n",
      "Train Epoch: 257 [127232/225000 (57%)] Loss: 6884.199219\n",
      "Train Epoch: 257 [131328/225000 (58%)] Loss: 7097.177734\n",
      "Train Epoch: 257 [135424/225000 (60%)] Loss: 7097.289062\n",
      "Train Epoch: 257 [139520/225000 (62%)] Loss: 7003.708984\n",
      "Train Epoch: 257 [143616/225000 (64%)] Loss: 7086.322266\n",
      "Train Epoch: 257 [147712/225000 (66%)] Loss: 6968.865234\n",
      "Train Epoch: 257 [151808/225000 (67%)] Loss: 6990.427734\n",
      "Train Epoch: 257 [155904/225000 (69%)] Loss: 7142.908203\n",
      "Train Epoch: 257 [160000/225000 (71%)] Loss: 7028.070312\n",
      "Train Epoch: 257 [164096/225000 (73%)] Loss: 7129.777344\n",
      "Train Epoch: 257 [168192/225000 (75%)] Loss: 7059.054688\n",
      "Train Epoch: 257 [172288/225000 (77%)] Loss: 7044.919922\n",
      "Train Epoch: 257 [176384/225000 (78%)] Loss: 7168.169922\n",
      "Train Epoch: 257 [180480/225000 (80%)] Loss: 7195.052734\n",
      "Train Epoch: 257 [184576/225000 (82%)] Loss: 6968.566406\n",
      "Train Epoch: 257 [188672/225000 (84%)] Loss: 7093.335938\n",
      "Train Epoch: 257 [192768/225000 (86%)] Loss: 7005.816406\n",
      "Train Epoch: 257 [196864/225000 (87%)] Loss: 6918.617188\n",
      "Train Epoch: 257 [200960/225000 (89%)] Loss: 7056.435547\n",
      "Train Epoch: 257 [205056/225000 (91%)] Loss: 7049.724609\n",
      "Train Epoch: 257 [209152/225000 (93%)] Loss: 7166.900391\n",
      "Train Epoch: 257 [213248/225000 (95%)] Loss: 7109.558594\n",
      "Train Epoch: 257 [217344/225000 (97%)] Loss: 7117.322266\n",
      "Train Epoch: 257 [221440/225000 (98%)] Loss: 6906.472656\n",
      "    epoch          : 257\n",
      "    loss           : 7041.058059362557\n",
      "    val_loss       : 7034.903998353044\n",
      "Train Epoch: 258 [256/225000 (0%)] Loss: 7135.673828\n",
      "Train Epoch: 258 [4352/225000 (2%)] Loss: 6979.457031\n",
      "Train Epoch: 258 [8448/225000 (4%)] Loss: 7053.931641\n",
      "Train Epoch: 258 [12544/225000 (6%)] Loss: 7022.158203\n",
      "Train Epoch: 258 [16640/225000 (7%)] Loss: 7184.414062\n",
      "Train Epoch: 258 [20736/225000 (9%)] Loss: 7109.957031\n",
      "Train Epoch: 258 [24832/225000 (11%)] Loss: 7138.248047\n",
      "Train Epoch: 258 [28928/225000 (13%)] Loss: 7194.863281\n",
      "Train Epoch: 258 [33024/225000 (15%)] Loss: 7097.398438\n",
      "Train Epoch: 258 [37120/225000 (16%)] Loss: 7028.939453\n",
      "Train Epoch: 258 [41216/225000 (18%)] Loss: 6937.037109\n",
      "Train Epoch: 258 [45312/225000 (20%)] Loss: 7027.968750\n",
      "Train Epoch: 258 [49408/225000 (22%)] Loss: 6898.855469\n",
      "Train Epoch: 258 [53504/225000 (24%)] Loss: 6940.990234\n",
      "Train Epoch: 258 [57600/225000 (26%)] Loss: 7044.503906\n",
      "Train Epoch: 258 [61696/225000 (27%)] Loss: 7198.773438\n",
      "Train Epoch: 258 [65792/225000 (29%)] Loss: 7085.783203\n",
      "Train Epoch: 258 [69888/225000 (31%)] Loss: 7018.244141\n",
      "Train Epoch: 258 [73984/225000 (33%)] Loss: 7101.263672\n",
      "Train Epoch: 258 [78080/225000 (35%)] Loss: 7023.228516\n",
      "Train Epoch: 258 [82176/225000 (37%)] Loss: 7003.392578\n",
      "Train Epoch: 258 [86272/225000 (38%)] Loss: 7083.312500\n",
      "Train Epoch: 258 [90368/225000 (40%)] Loss: 6928.154297\n",
      "Train Epoch: 258 [94464/225000 (42%)] Loss: 6979.203125\n",
      "Train Epoch: 258 [98560/225000 (44%)] Loss: 7123.064453\n",
      "Train Epoch: 258 [102656/225000 (46%)] Loss: 7001.593750\n",
      "Train Epoch: 258 [106752/225000 (47%)] Loss: 6973.148438\n",
      "Train Epoch: 258 [110848/225000 (49%)] Loss: 6920.789062\n",
      "Train Epoch: 258 [114944/225000 (51%)] Loss: 7205.773438\n",
      "Train Epoch: 258 [119040/225000 (53%)] Loss: 6950.623047\n",
      "Train Epoch: 258 [123136/225000 (55%)] Loss: 7087.894531\n",
      "Train Epoch: 258 [127232/225000 (57%)] Loss: 7095.228516\n",
      "Train Epoch: 258 [131328/225000 (58%)] Loss: 7025.175781\n",
      "Train Epoch: 258 [135424/225000 (60%)] Loss: 7041.128906\n",
      "Train Epoch: 258 [139520/225000 (62%)] Loss: 7139.400391\n",
      "Train Epoch: 258 [143616/225000 (64%)] Loss: 7088.982422\n",
      "Train Epoch: 258 [147712/225000 (66%)] Loss: 6958.025391\n",
      "Train Epoch: 258 [151808/225000 (67%)] Loss: 6952.345703\n",
      "Train Epoch: 258 [155904/225000 (69%)] Loss: 7226.519531\n",
      "Train Epoch: 258 [160000/225000 (71%)] Loss: 6998.751953\n",
      "Train Epoch: 258 [164096/225000 (73%)] Loss: 6923.724609\n",
      "Train Epoch: 258 [168192/225000 (75%)] Loss: 6899.232422\n",
      "Train Epoch: 258 [172288/225000 (77%)] Loss: 7009.072266\n",
      "Train Epoch: 258 [176384/225000 (78%)] Loss: 6974.203125\n",
      "Train Epoch: 258 [180480/225000 (80%)] Loss: 7038.308594\n",
      "Train Epoch: 258 [184576/225000 (82%)] Loss: 6964.832031\n",
      "Train Epoch: 258 [188672/225000 (84%)] Loss: 7035.880859\n",
      "Train Epoch: 258 [192768/225000 (86%)] Loss: 7054.669922\n",
      "Train Epoch: 258 [196864/225000 (87%)] Loss: 6965.302734\n",
      "Train Epoch: 258 [200960/225000 (89%)] Loss: 7004.173828\n",
      "Train Epoch: 258 [205056/225000 (91%)] Loss: 7100.863281\n",
      "Train Epoch: 258 [209152/225000 (93%)] Loss: 6919.355469\n",
      "Train Epoch: 258 [213248/225000 (95%)] Loss: 7124.861328\n",
      "Train Epoch: 258 [217344/225000 (97%)] Loss: 6973.429688\n",
      "Train Epoch: 258 [221440/225000 (98%)] Loss: 6916.792969\n",
      "    epoch          : 258\n",
      "    loss           : 7053.352997902446\n",
      "    val_loss       : 7138.546205923265\n",
      "Train Epoch: 259 [256/225000 (0%)] Loss: 7102.605469\n",
      "Train Epoch: 259 [4352/225000 (2%)] Loss: 6978.529297\n",
      "Train Epoch: 259 [8448/225000 (4%)] Loss: 7009.525391\n",
      "Train Epoch: 259 [12544/225000 (6%)] Loss: 7024.744141\n",
      "Train Epoch: 259 [16640/225000 (7%)] Loss: 7197.236328\n",
      "Train Epoch: 259 [20736/225000 (9%)] Loss: 7071.173828\n",
      "Train Epoch: 259 [24832/225000 (11%)] Loss: 6941.492188\n",
      "Train Epoch: 259 [28928/225000 (13%)] Loss: 7037.773438\n",
      "Train Epoch: 259 [33024/225000 (15%)] Loss: 7130.582031\n",
      "Train Epoch: 259 [37120/225000 (16%)] Loss: 7182.650391\n",
      "Train Epoch: 259 [41216/225000 (18%)] Loss: 7053.146484\n",
      "Train Epoch: 259 [45312/225000 (20%)] Loss: 7055.750000\n",
      "Train Epoch: 259 [49408/225000 (22%)] Loss: 6949.396484\n",
      "Train Epoch: 259 [53504/225000 (24%)] Loss: 7057.003906\n",
      "Train Epoch: 259 [57600/225000 (26%)] Loss: 7058.500000\n",
      "Train Epoch: 259 [61696/225000 (27%)] Loss: 7228.841797\n",
      "Train Epoch: 259 [65792/225000 (29%)] Loss: 6991.244141\n",
      "Train Epoch: 259 [69888/225000 (31%)] Loss: 6950.664062\n",
      "Train Epoch: 259 [73984/225000 (33%)] Loss: 6889.535156\n",
      "Train Epoch: 259 [78080/225000 (35%)] Loss: 6975.933594\n",
      "Train Epoch: 259 [82176/225000 (37%)] Loss: 7024.308594\n",
      "Train Epoch: 259 [86272/225000 (38%)] Loss: 7123.205078\n",
      "Train Epoch: 259 [90368/225000 (40%)] Loss: 6907.314453\n",
      "Train Epoch: 259 [94464/225000 (42%)] Loss: 7060.162109\n",
      "Train Epoch: 259 [98560/225000 (44%)] Loss: 6836.990234\n",
      "Train Epoch: 259 [102656/225000 (46%)] Loss: 6940.902344\n",
      "Train Epoch: 259 [106752/225000 (47%)] Loss: 6891.755859\n",
      "Train Epoch: 259 [110848/225000 (49%)] Loss: 7135.843750\n",
      "Train Epoch: 259 [114944/225000 (51%)] Loss: 7010.330078\n",
      "Train Epoch: 259 [119040/225000 (53%)] Loss: 7055.785156\n",
      "Train Epoch: 259 [123136/225000 (55%)] Loss: 7031.128906\n",
      "Train Epoch: 259 [127232/225000 (57%)] Loss: 7195.562500\n",
      "Train Epoch: 259 [131328/225000 (58%)] Loss: 6970.769531\n",
      "Train Epoch: 259 [135424/225000 (60%)] Loss: 6995.658203\n",
      "Train Epoch: 259 [139520/225000 (62%)] Loss: 7184.328125\n",
      "Train Epoch: 259 [143616/225000 (64%)] Loss: 7057.470703\n",
      "Train Epoch: 259 [147712/225000 (66%)] Loss: 6859.505859\n",
      "Train Epoch: 259 [151808/225000 (67%)] Loss: 6968.958984\n",
      "Train Epoch: 259 [155904/225000 (69%)] Loss: 6924.406250\n",
      "Train Epoch: 259 [160000/225000 (71%)] Loss: 7244.410156\n",
      "Train Epoch: 259 [164096/225000 (73%)] Loss: 6946.335938\n",
      "Train Epoch: 259 [168192/225000 (75%)] Loss: 7095.236328\n",
      "Train Epoch: 259 [172288/225000 (77%)] Loss: 7069.408203\n",
      "Train Epoch: 259 [176384/225000 (78%)] Loss: 25949.156250\n",
      "Train Epoch: 259 [180480/225000 (80%)] Loss: 7004.517578\n",
      "Train Epoch: 259 [184576/225000 (82%)] Loss: 7007.273438\n",
      "Train Epoch: 259 [188672/225000 (84%)] Loss: 6978.917969\n",
      "Train Epoch: 259 [192768/225000 (86%)] Loss: 7047.464844\n",
      "Train Epoch: 259 [196864/225000 (87%)] Loss: 6950.033203\n",
      "Train Epoch: 259 [200960/225000 (89%)] Loss: 7017.863281\n",
      "Train Epoch: 259 [205056/225000 (91%)] Loss: 7022.189453\n",
      "Train Epoch: 259 [209152/225000 (93%)] Loss: 7212.095703\n",
      "Train Epoch: 259 [213248/225000 (95%)] Loss: 6947.960938\n",
      "Train Epoch: 259 [217344/225000 (97%)] Loss: 6932.888672\n",
      "Train Epoch: 259 [221440/225000 (98%)] Loss: 7258.945312\n",
      "    epoch          : 259\n",
      "    loss           : 7059.295580693615\n",
      "    val_loss       : 7029.750222956648\n",
      "Train Epoch: 260 [256/225000 (0%)] Loss: 7199.062500\n",
      "Train Epoch: 260 [4352/225000 (2%)] Loss: 7042.259766\n",
      "Train Epoch: 260 [8448/225000 (4%)] Loss: 7100.630859\n",
      "Train Epoch: 260 [12544/225000 (6%)] Loss: 6922.339844\n",
      "Train Epoch: 260 [16640/225000 (7%)] Loss: 7065.472656\n",
      "Train Epoch: 260 [20736/225000 (9%)] Loss: 6888.359375\n",
      "Train Epoch: 260 [24832/225000 (11%)] Loss: 6912.484375\n",
      "Train Epoch: 260 [28928/225000 (13%)] Loss: 6982.402344\n",
      "Train Epoch: 260 [33024/225000 (15%)] Loss: 7106.468750\n",
      "Train Epoch: 260 [37120/225000 (16%)] Loss: 7040.775391\n",
      "Train Epoch: 260 [41216/225000 (18%)] Loss: 7107.839844\n",
      "Train Epoch: 260 [45312/225000 (20%)] Loss: 7071.082031\n",
      "Train Epoch: 260 [49408/225000 (22%)] Loss: 6864.201172\n",
      "Train Epoch: 260 [53504/225000 (24%)] Loss: 7116.611328\n",
      "Train Epoch: 260 [57600/225000 (26%)] Loss: 7097.511719\n",
      "Train Epoch: 260 [61696/225000 (27%)] Loss: 6917.900391\n",
      "Train Epoch: 260 [65792/225000 (29%)] Loss: 6898.675781\n",
      "Train Epoch: 260 [69888/225000 (31%)] Loss: 7077.083984\n",
      "Train Epoch: 260 [73984/225000 (33%)] Loss: 7051.419922\n",
      "Train Epoch: 260 [78080/225000 (35%)] Loss: 7109.701172\n",
      "Train Epoch: 260 [82176/225000 (37%)] Loss: 6900.935547\n",
      "Train Epoch: 260 [86272/225000 (38%)] Loss: 7170.408203\n",
      "Train Epoch: 260 [90368/225000 (40%)] Loss: 7062.208984\n",
      "Train Epoch: 260 [94464/225000 (42%)] Loss: 7147.605469\n",
      "Train Epoch: 260 [98560/225000 (44%)] Loss: 6946.611328\n",
      "Train Epoch: 260 [102656/225000 (46%)] Loss: 7184.291016\n",
      "Train Epoch: 260 [106752/225000 (47%)] Loss: 7039.601562\n",
      "Train Epoch: 260 [110848/225000 (49%)] Loss: 6953.701172\n",
      "Train Epoch: 260 [114944/225000 (51%)] Loss: 7105.861328\n",
      "Train Epoch: 260 [119040/225000 (53%)] Loss: 7181.556641\n",
      "Train Epoch: 260 [123136/225000 (55%)] Loss: 6943.619141\n",
      "Train Epoch: 260 [127232/225000 (57%)] Loss: 7098.228516\n",
      "Train Epoch: 260 [131328/225000 (58%)] Loss: 6880.949219\n",
      "Train Epoch: 260 [135424/225000 (60%)] Loss: 7146.451172\n",
      "Train Epoch: 260 [139520/225000 (62%)] Loss: 7089.660156\n",
      "Train Epoch: 260 [143616/225000 (64%)] Loss: 6983.601562\n",
      "Train Epoch: 260 [147712/225000 (66%)] Loss: 6911.634766\n",
      "Train Epoch: 260 [151808/225000 (67%)] Loss: 7086.673828\n",
      "Train Epoch: 260 [155904/225000 (69%)] Loss: 7098.052734\n",
      "Train Epoch: 260 [160000/225000 (71%)] Loss: 6881.585938\n",
      "Train Epoch: 260 [164096/225000 (73%)] Loss: 6981.332031\n",
      "Train Epoch: 260 [168192/225000 (75%)] Loss: 6967.646484\n",
      "Train Epoch: 260 [172288/225000 (77%)] Loss: 7007.738281\n",
      "Train Epoch: 260 [176384/225000 (78%)] Loss: 7032.992188\n",
      "Train Epoch: 260 [180480/225000 (80%)] Loss: 6912.927734\n",
      "Train Epoch: 260 [184576/225000 (82%)] Loss: 6989.832031\n",
      "Train Epoch: 260 [188672/225000 (84%)] Loss: 7057.535156\n",
      "Train Epoch: 260 [192768/225000 (86%)] Loss: 7025.750000\n",
      "Train Epoch: 260 [196864/225000 (87%)] Loss: 6920.142578\n",
      "Train Epoch: 260 [200960/225000 (89%)] Loss: 7068.960938\n",
      "Train Epoch: 260 [205056/225000 (91%)] Loss: 6966.916016\n",
      "Train Epoch: 260 [209152/225000 (93%)] Loss: 6899.519531\n",
      "Train Epoch: 260 [213248/225000 (95%)] Loss: 6936.277344\n",
      "Train Epoch: 260 [217344/225000 (97%)] Loss: 7158.248047\n",
      "Train Epoch: 260 [221440/225000 (98%)] Loss: 6917.480469\n",
      "    epoch          : 260\n",
      "    loss           : 7035.780167893203\n",
      "    val_loss       : 7029.172224925489\n",
      "Train Epoch: 261 [256/225000 (0%)] Loss: 7040.710938\n",
      "Train Epoch: 261 [4352/225000 (2%)] Loss: 7038.945312\n",
      "Train Epoch: 261 [8448/225000 (4%)] Loss: 6915.044922\n",
      "Train Epoch: 261 [12544/225000 (6%)] Loss: 7093.228516\n",
      "Train Epoch: 261 [16640/225000 (7%)] Loss: 6980.501953\n",
      "Train Epoch: 261 [20736/225000 (9%)] Loss: 6851.289062\n",
      "Train Epoch: 261 [24832/225000 (11%)] Loss: 6953.179688\n",
      "Train Epoch: 261 [28928/225000 (13%)] Loss: 7086.970703\n",
      "Train Epoch: 261 [33024/225000 (15%)] Loss: 7168.742188\n",
      "Train Epoch: 261 [37120/225000 (16%)] Loss: 7028.150391\n",
      "Train Epoch: 261 [41216/225000 (18%)] Loss: 7075.917969\n",
      "Train Epoch: 261 [45312/225000 (20%)] Loss: 7012.886719\n",
      "Train Epoch: 261 [49408/225000 (22%)] Loss: 7059.408203\n",
      "Train Epoch: 261 [53504/225000 (24%)] Loss: 7002.943359\n",
      "Train Epoch: 261 [57600/225000 (26%)] Loss: 7012.041016\n",
      "Train Epoch: 261 [61696/225000 (27%)] Loss: 7027.701172\n",
      "Train Epoch: 261 [65792/225000 (29%)] Loss: 7088.990234\n",
      "Train Epoch: 261 [69888/225000 (31%)] Loss: 6854.984375\n",
      "Train Epoch: 261 [73984/225000 (33%)] Loss: 6933.246094\n",
      "Train Epoch: 261 [78080/225000 (35%)] Loss: 7009.021484\n",
      "Train Epoch: 261 [82176/225000 (37%)] Loss: 7033.615234\n",
      "Train Epoch: 261 [86272/225000 (38%)] Loss: 6960.322266\n",
      "Train Epoch: 261 [90368/225000 (40%)] Loss: 7028.214844\n",
      "Train Epoch: 261 [94464/225000 (42%)] Loss: 7055.876953\n",
      "Train Epoch: 261 [98560/225000 (44%)] Loss: 7023.794922\n",
      "Train Epoch: 261 [102656/225000 (46%)] Loss: 7028.519531\n",
      "Train Epoch: 261 [106752/225000 (47%)] Loss: 7009.886719\n",
      "Train Epoch: 261 [110848/225000 (49%)] Loss: 7042.816406\n",
      "Train Epoch: 261 [114944/225000 (51%)] Loss: 6815.048828\n",
      "Train Epoch: 261 [119040/225000 (53%)] Loss: 7085.605469\n",
      "Train Epoch: 261 [123136/225000 (55%)] Loss: 6958.769531\n",
      "Train Epoch: 261 [127232/225000 (57%)] Loss: 7191.419922\n",
      "Train Epoch: 261 [131328/225000 (58%)] Loss: 7031.908203\n",
      "Train Epoch: 261 [135424/225000 (60%)] Loss: 7078.806641\n",
      "Train Epoch: 261 [139520/225000 (62%)] Loss: 7044.349609\n",
      "Train Epoch: 261 [143616/225000 (64%)] Loss: 6971.236328\n",
      "Train Epoch: 261 [147712/225000 (66%)] Loss: 7249.115234\n",
      "Train Epoch: 261 [151808/225000 (67%)] Loss: 7210.648438\n",
      "Train Epoch: 261 [155904/225000 (69%)] Loss: 6978.365234\n",
      "Train Epoch: 261 [160000/225000 (71%)] Loss: 7008.349609\n",
      "Train Epoch: 261 [164096/225000 (73%)] Loss: 6931.896484\n",
      "Train Epoch: 261 [168192/225000 (75%)] Loss: 7099.937500\n",
      "Train Epoch: 261 [172288/225000 (77%)] Loss: 7018.556641\n",
      "Train Epoch: 261 [176384/225000 (78%)] Loss: 7028.539062\n",
      "Train Epoch: 261 [180480/225000 (80%)] Loss: 6994.722656\n",
      "Train Epoch: 261 [184576/225000 (82%)] Loss: 7016.679688\n",
      "Train Epoch: 261 [188672/225000 (84%)] Loss: 7068.578125\n",
      "Train Epoch: 261 [192768/225000 (86%)] Loss: 7110.572266\n",
      "Train Epoch: 261 [196864/225000 (87%)] Loss: 6857.494141\n",
      "Train Epoch: 261 [200960/225000 (89%)] Loss: 7055.908203\n",
      "Train Epoch: 261 [205056/225000 (91%)] Loss: 7096.287109\n",
      "Train Epoch: 261 [209152/225000 (93%)] Loss: 7010.619141\n",
      "Train Epoch: 261 [213248/225000 (95%)] Loss: 7234.208984\n",
      "Train Epoch: 261 [217344/225000 (97%)] Loss: 7162.177734\n",
      "Train Epoch: 261 [221440/225000 (98%)] Loss: 7110.179688\n",
      "    epoch          : 261\n",
      "    loss           : 7022.097849562713\n",
      "    val_loss       : 7026.860402238613\n",
      "Train Epoch: 262 [256/225000 (0%)] Loss: 7073.283203\n",
      "Train Epoch: 262 [4352/225000 (2%)] Loss: 6971.455078\n",
      "Train Epoch: 262 [8448/225000 (4%)] Loss: 7081.382812\n",
      "Train Epoch: 262 [12544/225000 (6%)] Loss: 6925.998047\n",
      "Train Epoch: 262 [16640/225000 (7%)] Loss: 7090.310547\n",
      "Train Epoch: 262 [20736/225000 (9%)] Loss: 7150.003906\n",
      "Train Epoch: 262 [24832/225000 (11%)] Loss: 7020.845703\n",
      "Train Epoch: 262 [28928/225000 (13%)] Loss: 7098.824219\n",
      "Train Epoch: 262 [33024/225000 (15%)] Loss: 6945.916016\n",
      "Train Epoch: 262 [37120/225000 (16%)] Loss: 7044.900391\n",
      "Train Epoch: 262 [41216/225000 (18%)] Loss: 6965.761719\n",
      "Train Epoch: 262 [45312/225000 (20%)] Loss: 7048.039062\n",
      "Train Epoch: 262 [49408/225000 (22%)] Loss: 7149.560547\n",
      "Train Epoch: 262 [53504/225000 (24%)] Loss: 7123.574219\n",
      "Train Epoch: 262 [57600/225000 (26%)] Loss: 6956.914062\n",
      "Train Epoch: 262 [61696/225000 (27%)] Loss: 6981.488281\n",
      "Train Epoch: 262 [65792/225000 (29%)] Loss: 6930.126953\n",
      "Train Epoch: 262 [69888/225000 (31%)] Loss: 7046.496094\n",
      "Train Epoch: 262 [73984/225000 (33%)] Loss: 7103.488281\n",
      "Train Epoch: 262 [78080/225000 (35%)] Loss: 6975.550781\n",
      "Train Epoch: 262 [82176/225000 (37%)] Loss: 7076.472656\n",
      "Train Epoch: 262 [86272/225000 (38%)] Loss: 7187.699219\n",
      "Train Epoch: 262 [90368/225000 (40%)] Loss: 7053.927734\n",
      "Train Epoch: 262 [94464/225000 (42%)] Loss: 7040.890625\n",
      "Train Epoch: 262 [98560/225000 (44%)] Loss: 7065.109375\n",
      "Train Epoch: 262 [102656/225000 (46%)] Loss: 7029.882812\n",
      "Train Epoch: 262 [106752/225000 (47%)] Loss: 7159.519531\n",
      "Train Epoch: 262 [110848/225000 (49%)] Loss: 7011.259766\n",
      "Train Epoch: 262 [114944/225000 (51%)] Loss: 7078.089844\n",
      "Train Epoch: 262 [119040/225000 (53%)] Loss: 6925.648438\n",
      "Train Epoch: 262 [123136/225000 (55%)] Loss: 6837.455078\n",
      "Train Epoch: 262 [127232/225000 (57%)] Loss: 6978.714844\n",
      "Train Epoch: 262 [131328/225000 (58%)] Loss: 6999.007812\n",
      "Train Epoch: 262 [135424/225000 (60%)] Loss: 7046.177734\n",
      "Train Epoch: 262 [139520/225000 (62%)] Loss: 6998.785156\n",
      "Train Epoch: 262 [143616/225000 (64%)] Loss: 6956.636719\n",
      "Train Epoch: 262 [147712/225000 (66%)] Loss: 6951.728516\n",
      "Train Epoch: 262 [151808/225000 (67%)] Loss: 7029.771484\n",
      "Train Epoch: 262 [155904/225000 (69%)] Loss: 7054.705078\n",
      "Train Epoch: 262 [160000/225000 (71%)] Loss: 7048.761719\n",
      "Train Epoch: 262 [164096/225000 (73%)] Loss: 6914.212891\n",
      "Train Epoch: 262 [168192/225000 (75%)] Loss: 6920.009766\n",
      "Train Epoch: 262 [172288/225000 (77%)] Loss: 7195.642578\n",
      "Train Epoch: 262 [176384/225000 (78%)] Loss: 7178.886719\n",
      "Train Epoch: 262 [180480/225000 (80%)] Loss: 6967.078125\n",
      "Train Epoch: 262 [184576/225000 (82%)] Loss: 6916.212891\n",
      "Train Epoch: 262 [188672/225000 (84%)] Loss: 7089.228516\n",
      "Train Epoch: 262 [192768/225000 (86%)] Loss: 7130.431641\n",
      "Train Epoch: 262 [196864/225000 (87%)] Loss: 7163.583984\n",
      "Train Epoch: 262 [200960/225000 (89%)] Loss: 7106.906250\n",
      "Train Epoch: 262 [205056/225000 (91%)] Loss: 7114.197266\n",
      "Train Epoch: 262 [209152/225000 (93%)] Loss: 6914.316406\n",
      "Train Epoch: 262 [213248/225000 (95%)] Loss: 7084.054688\n",
      "Train Epoch: 262 [217344/225000 (97%)] Loss: 7049.447266\n",
      "Train Epoch: 262 [221440/225000 (98%)] Loss: 7046.980469\n",
      "    epoch          : 262\n",
      "    loss           : 7031.514252924133\n",
      "    val_loss       : 7028.239914238453\n",
      "Train Epoch: 263 [256/225000 (0%)] Loss: 6932.511719\n",
      "Train Epoch: 263 [4352/225000 (2%)] Loss: 7003.496094\n",
      "Train Epoch: 263 [8448/225000 (4%)] Loss: 7041.636719\n",
      "Train Epoch: 263 [12544/225000 (6%)] Loss: 6974.664062\n",
      "Train Epoch: 263 [16640/225000 (7%)] Loss: 6955.259766\n",
      "Train Epoch: 263 [20736/225000 (9%)] Loss: 7114.832031\n",
      "Train Epoch: 263 [24832/225000 (11%)] Loss: 7081.146484\n",
      "Train Epoch: 263 [28928/225000 (13%)] Loss: 6900.638672\n",
      "Train Epoch: 263 [33024/225000 (15%)] Loss: 7107.583984\n",
      "Train Epoch: 263 [37120/225000 (16%)] Loss: 6902.251953\n",
      "Train Epoch: 263 [41216/225000 (18%)] Loss: 6949.173828\n",
      "Train Epoch: 263 [45312/225000 (20%)] Loss: 7108.367188\n",
      "Train Epoch: 263 [49408/225000 (22%)] Loss: 6901.513672\n",
      "Train Epoch: 263 [53504/225000 (24%)] Loss: 7101.380859\n",
      "Train Epoch: 263 [57600/225000 (26%)] Loss: 6799.957031\n",
      "Train Epoch: 263 [61696/225000 (27%)] Loss: 7132.796875\n",
      "Train Epoch: 263 [65792/225000 (29%)] Loss: 6919.234375\n",
      "Train Epoch: 263 [69888/225000 (31%)] Loss: 6997.455078\n",
      "Train Epoch: 263 [73984/225000 (33%)] Loss: 6900.982422\n",
      "Train Epoch: 263 [78080/225000 (35%)] Loss: 6993.699219\n",
      "Train Epoch: 263 [82176/225000 (37%)] Loss: 7095.226562\n",
      "Train Epoch: 263 [86272/225000 (38%)] Loss: 6897.035156\n",
      "Train Epoch: 263 [90368/225000 (40%)] Loss: 6929.468750\n",
      "Train Epoch: 263 [94464/225000 (42%)] Loss: 6982.169922\n",
      "Train Epoch: 263 [98560/225000 (44%)] Loss: 6998.867188\n",
      "Train Epoch: 263 [102656/225000 (46%)] Loss: 6984.806641\n",
      "Train Epoch: 263 [106752/225000 (47%)] Loss: 6987.115234\n",
      "Train Epoch: 263 [110848/225000 (49%)] Loss: 7094.441406\n",
      "Train Epoch: 263 [114944/225000 (51%)] Loss: 7094.523438\n",
      "Train Epoch: 263 [119040/225000 (53%)] Loss: 7083.738281\n",
      "Train Epoch: 263 [123136/225000 (55%)] Loss: 6862.878906\n",
      "Train Epoch: 263 [127232/225000 (57%)] Loss: 6989.908203\n",
      "Train Epoch: 263 [131328/225000 (58%)] Loss: 7025.525391\n",
      "Train Epoch: 263 [135424/225000 (60%)] Loss: 6952.197266\n",
      "Train Epoch: 263 [139520/225000 (62%)] Loss: 7000.378906\n",
      "Train Epoch: 263 [143616/225000 (64%)] Loss: 6785.847656\n",
      "Train Epoch: 263 [147712/225000 (66%)] Loss: 7053.472656\n",
      "Train Epoch: 263 [151808/225000 (67%)] Loss: 6950.611328\n",
      "Train Epoch: 263 [155904/225000 (69%)] Loss: 6879.298828\n",
      "Train Epoch: 263 [160000/225000 (71%)] Loss: 7000.115234\n",
      "Train Epoch: 263 [164096/225000 (73%)] Loss: 6918.095703\n",
      "Train Epoch: 263 [168192/225000 (75%)] Loss: 7005.128906\n",
      "Train Epoch: 263 [172288/225000 (77%)] Loss: 7053.962891\n",
      "Train Epoch: 263 [176384/225000 (78%)] Loss: 6950.810547\n",
      "Train Epoch: 263 [180480/225000 (80%)] Loss: 6933.472656\n",
      "Train Epoch: 263 [184576/225000 (82%)] Loss: 6896.660156\n",
      "Train Epoch: 263 [188672/225000 (84%)] Loss: 7024.765625\n",
      "Train Epoch: 263 [192768/225000 (86%)] Loss: 6898.757812\n",
      "Train Epoch: 263 [196864/225000 (87%)] Loss: 7230.570312\n",
      "Train Epoch: 263 [200960/225000 (89%)] Loss: 6978.847656\n",
      "Train Epoch: 263 [205056/225000 (91%)] Loss: 6954.916016\n",
      "Train Epoch: 263 [209152/225000 (93%)] Loss: 7046.498047\n",
      "Train Epoch: 263 [213248/225000 (95%)] Loss: 6949.531250\n",
      "Train Epoch: 263 [217344/225000 (97%)] Loss: 6872.339844\n",
      "Train Epoch: 263 [221440/225000 (98%)] Loss: 7064.800781\n",
      "    epoch          : 263\n",
      "    loss           : 7030.022309842506\n",
      "    val_loss       : 7022.857161573002\n",
      "Train Epoch: 264 [256/225000 (0%)] Loss: 6899.496094\n",
      "Train Epoch: 264 [4352/225000 (2%)] Loss: 7027.587891\n",
      "Train Epoch: 264 [8448/225000 (4%)] Loss: 7013.613281\n",
      "Train Epoch: 264 [12544/225000 (6%)] Loss: 7036.529297\n",
      "Train Epoch: 264 [16640/225000 (7%)] Loss: 6996.287109\n",
      "Train Epoch: 264 [20736/225000 (9%)] Loss: 7104.582031\n",
      "Train Epoch: 264 [24832/225000 (11%)] Loss: 6989.753906\n",
      "Train Epoch: 264 [28928/225000 (13%)] Loss: 6873.119141\n",
      "Train Epoch: 264 [33024/225000 (15%)] Loss: 6931.523438\n",
      "Train Epoch: 264 [37120/225000 (16%)] Loss: 7002.199219\n",
      "Train Epoch: 264 [41216/225000 (18%)] Loss: 7142.787109\n",
      "Train Epoch: 264 [45312/225000 (20%)] Loss: 7055.089844\n",
      "Train Epoch: 264 [49408/225000 (22%)] Loss: 7004.576172\n",
      "Train Epoch: 264 [53504/225000 (24%)] Loss: 7014.275391\n",
      "Train Epoch: 264 [57600/225000 (26%)] Loss: 6838.402344\n",
      "Train Epoch: 264 [61696/225000 (27%)] Loss: 6974.103516\n",
      "Train Epoch: 264 [65792/225000 (29%)] Loss: 7026.167969\n",
      "Train Epoch: 264 [69888/225000 (31%)] Loss: 7011.857422\n",
      "Train Epoch: 264 [73984/225000 (33%)] Loss: 6978.654297\n",
      "Train Epoch: 264 [78080/225000 (35%)] Loss: 7019.718750\n",
      "Train Epoch: 264 [82176/225000 (37%)] Loss: 6873.433594\n",
      "Train Epoch: 264 [86272/225000 (38%)] Loss: 6957.181641\n",
      "Train Epoch: 264 [90368/225000 (40%)] Loss: 6986.107422\n",
      "Train Epoch: 264 [94464/225000 (42%)] Loss: 7228.476562\n",
      "Train Epoch: 264 [98560/225000 (44%)] Loss: 7178.595703\n",
      "Train Epoch: 264 [102656/225000 (46%)] Loss: 6992.115234\n",
      "Train Epoch: 264 [106752/225000 (47%)] Loss: 6922.091797\n",
      "Train Epoch: 264 [110848/225000 (49%)] Loss: 7083.566406\n",
      "Train Epoch: 264 [114944/225000 (51%)] Loss: 7072.130859\n",
      "Train Epoch: 264 [119040/225000 (53%)] Loss: 7114.603516\n",
      "Train Epoch: 264 [123136/225000 (55%)] Loss: 7031.460938\n",
      "Train Epoch: 264 [127232/225000 (57%)] Loss: 7142.761719\n",
      "Train Epoch: 264 [131328/225000 (58%)] Loss: 7083.933594\n",
      "Train Epoch: 264 [135424/225000 (60%)] Loss: 6927.244141\n",
      "Train Epoch: 264 [139520/225000 (62%)] Loss: 7063.455078\n",
      "Train Epoch: 264 [143616/225000 (64%)] Loss: 6978.464844\n",
      "Train Epoch: 264 [147712/225000 (66%)] Loss: 7111.447266\n",
      "Train Epoch: 264 [151808/225000 (67%)] Loss: 6921.564453\n",
      "Train Epoch: 264 [155904/225000 (69%)] Loss: 7164.261719\n",
      "Train Epoch: 264 [160000/225000 (71%)] Loss: 7140.812500\n",
      "Train Epoch: 264 [164096/225000 (73%)] Loss: 7086.574219\n",
      "Train Epoch: 264 [168192/225000 (75%)] Loss: 7124.876953\n",
      "Train Epoch: 264 [172288/225000 (77%)] Loss: 6979.074219\n",
      "Train Epoch: 264 [176384/225000 (78%)] Loss: 6912.156250\n",
      "Train Epoch: 264 [180480/225000 (80%)] Loss: 7048.927734\n",
      "Train Epoch: 264 [184576/225000 (82%)] Loss: 6851.119141\n",
      "Train Epoch: 264 [188672/225000 (84%)] Loss: 7110.593750\n",
      "Train Epoch: 264 [192768/225000 (86%)] Loss: 7225.105469\n",
      "Train Epoch: 264 [196864/225000 (87%)] Loss: 7115.408203\n",
      "Train Epoch: 264 [200960/225000 (89%)] Loss: 6862.251953\n",
      "Train Epoch: 264 [205056/225000 (91%)] Loss: 7006.855469\n",
      "Train Epoch: 264 [209152/225000 (93%)] Loss: 6963.007812\n",
      "Train Epoch: 264 [213248/225000 (95%)] Loss: 7014.189453\n",
      "Train Epoch: 264 [217344/225000 (97%)] Loss: 7098.593750\n",
      "Train Epoch: 264 [221440/225000 (98%)] Loss: 6969.111328\n",
      "    epoch          : 264\n",
      "    loss           : 7043.390009510097\n",
      "    val_loss       : 7022.96418360788\n",
      "Train Epoch: 265 [256/225000 (0%)] Loss: 6969.275391\n",
      "Train Epoch: 265 [4352/225000 (2%)] Loss: 7233.751953\n",
      "Train Epoch: 265 [8448/225000 (4%)] Loss: 7082.718750\n",
      "Train Epoch: 265 [12544/225000 (6%)] Loss: 7159.769531\n",
      "Train Epoch: 265 [16640/225000 (7%)] Loss: 6983.353516\n",
      "Train Epoch: 265 [20736/225000 (9%)] Loss: 7049.585938\n",
      "Train Epoch: 265 [24832/225000 (11%)] Loss: 7041.652344\n",
      "Train Epoch: 265 [28928/225000 (13%)] Loss: 7143.156250\n",
      "Train Epoch: 265 [33024/225000 (15%)] Loss: 6802.675781\n",
      "Train Epoch: 265 [37120/225000 (16%)] Loss: 6934.820312\n",
      "Train Epoch: 265 [41216/225000 (18%)] Loss: 6949.693359\n",
      "Train Epoch: 265 [45312/225000 (20%)] Loss: 6961.958984\n",
      "Train Epoch: 265 [49408/225000 (22%)] Loss: 7015.244141\n",
      "Train Epoch: 265 [53504/225000 (24%)] Loss: 7152.943359\n",
      "Train Epoch: 265 [57600/225000 (26%)] Loss: 6995.187500\n",
      "Train Epoch: 265 [61696/225000 (27%)] Loss: 6850.070312\n",
      "Train Epoch: 265 [65792/225000 (29%)] Loss: 6930.625000\n",
      "Train Epoch: 265 [69888/225000 (31%)] Loss: 7051.296875\n",
      "Train Epoch: 265 [73984/225000 (33%)] Loss: 6899.246094\n",
      "Train Epoch: 265 [78080/225000 (35%)] Loss: 7081.453125\n",
      "Train Epoch: 265 [82176/225000 (37%)] Loss: 6908.238281\n",
      "Train Epoch: 265 [86272/225000 (38%)] Loss: 6969.621094\n",
      "Train Epoch: 265 [90368/225000 (40%)] Loss: 7068.423828\n",
      "Train Epoch: 265 [94464/225000 (42%)] Loss: 7071.576172\n",
      "Train Epoch: 265 [98560/225000 (44%)] Loss: 6910.443359\n",
      "Train Epoch: 265 [102656/225000 (46%)] Loss: 7135.667969\n",
      "Train Epoch: 265 [106752/225000 (47%)] Loss: 6984.023438\n",
      "Train Epoch: 265 [110848/225000 (49%)] Loss: 6916.146484\n",
      "Train Epoch: 265 [114944/225000 (51%)] Loss: 6985.138672\n",
      "Train Epoch: 265 [119040/225000 (53%)] Loss: 7015.259766\n",
      "Train Epoch: 265 [123136/225000 (55%)] Loss: 7009.994141\n",
      "Train Epoch: 265 [127232/225000 (57%)] Loss: 7053.621094\n",
      "Train Epoch: 265 [131328/225000 (58%)] Loss: 6880.232422\n",
      "Train Epoch: 265 [135424/225000 (60%)] Loss: 7129.800781\n",
      "Train Epoch: 265 [139520/225000 (62%)] Loss: 6947.908203\n",
      "Train Epoch: 265 [143616/225000 (64%)] Loss: 7189.921875\n",
      "Train Epoch: 265 [147712/225000 (66%)] Loss: 6992.441406\n",
      "Train Epoch: 265 [151808/225000 (67%)] Loss: 6865.322266\n",
      "Train Epoch: 265 [155904/225000 (69%)] Loss: 7031.695312\n",
      "Train Epoch: 265 [160000/225000 (71%)] Loss: 7005.843750\n",
      "Train Epoch: 265 [164096/225000 (73%)] Loss: 6873.654297\n",
      "Train Epoch: 265 [168192/225000 (75%)] Loss: 6905.425781\n",
      "Train Epoch: 265 [172288/225000 (77%)] Loss: 6965.734375\n",
      "Train Epoch: 265 [176384/225000 (78%)] Loss: 7123.611328\n",
      "Train Epoch: 265 [180480/225000 (80%)] Loss: 7018.980469\n",
      "Train Epoch: 265 [184576/225000 (82%)] Loss: 6895.390625\n",
      "Train Epoch: 265 [188672/225000 (84%)] Loss: 6886.384766\n",
      "Train Epoch: 265 [192768/225000 (86%)] Loss: 6912.546875\n",
      "Train Epoch: 265 [196864/225000 (87%)] Loss: 6977.832031\n",
      "Train Epoch: 265 [200960/225000 (89%)] Loss: 7012.921875\n",
      "Train Epoch: 265 [205056/225000 (91%)] Loss: 6971.998047\n",
      "Train Epoch: 265 [209152/225000 (93%)] Loss: 7025.455078\n",
      "Train Epoch: 265 [213248/225000 (95%)] Loss: 6976.658203\n",
      "Train Epoch: 265 [217344/225000 (97%)] Loss: 7094.310547\n",
      "Train Epoch: 265 [221440/225000 (98%)] Loss: 7013.185547\n",
      "    epoch          : 265\n",
      "    loss           : 7014.437105597625\n",
      "    val_loss       : 7019.093828889789\n",
      "Train Epoch: 266 [256/225000 (0%)] Loss: 6890.982422\n",
      "Train Epoch: 266 [4352/225000 (2%)] Loss: 7131.052734\n",
      "Train Epoch: 266 [8448/225000 (4%)] Loss: 7069.761719\n",
      "Train Epoch: 266 [12544/225000 (6%)] Loss: 7106.039062\n",
      "Train Epoch: 266 [16640/225000 (7%)] Loss: 6939.205078\n",
      "Train Epoch: 266 [20736/225000 (9%)] Loss: 6978.261719\n",
      "Train Epoch: 266 [24832/225000 (11%)] Loss: 7105.074219\n",
      "Train Epoch: 266 [28928/225000 (13%)] Loss: 7062.519531\n",
      "Train Epoch: 266 [33024/225000 (15%)] Loss: 6977.236328\n",
      "Train Epoch: 266 [37120/225000 (16%)] Loss: 6983.679688\n",
      "Train Epoch: 266 [41216/225000 (18%)] Loss: 6822.423828\n",
      "Train Epoch: 266 [45312/225000 (20%)] Loss: 6809.525391\n",
      "Train Epoch: 266 [49408/225000 (22%)] Loss: 6973.894531\n",
      "Train Epoch: 266 [53504/225000 (24%)] Loss: 6868.519531\n",
      "Train Epoch: 266 [57600/225000 (26%)] Loss: 7180.484375\n",
      "Train Epoch: 266 [61696/225000 (27%)] Loss: 7215.685547\n",
      "Train Epoch: 266 [65792/225000 (29%)] Loss: 7112.498047\n",
      "Train Epoch: 266 [69888/225000 (31%)] Loss: 6970.146484\n",
      "Train Epoch: 266 [73984/225000 (33%)] Loss: 6885.017578\n",
      "Train Epoch: 266 [78080/225000 (35%)] Loss: 7176.894531\n",
      "Train Epoch: 266 [82176/225000 (37%)] Loss: 7128.322266\n",
      "Train Epoch: 266 [86272/225000 (38%)] Loss: 6927.917969\n",
      "Train Epoch: 266 [90368/225000 (40%)] Loss: 7034.808594\n",
      "Train Epoch: 266 [94464/225000 (42%)] Loss: 7011.634766\n",
      "Train Epoch: 266 [98560/225000 (44%)] Loss: 7043.617188\n",
      "Train Epoch: 266 [102656/225000 (46%)] Loss: 7008.113281\n",
      "Train Epoch: 266 [106752/225000 (47%)] Loss: 6991.835938\n",
      "Train Epoch: 266 [110848/225000 (49%)] Loss: 6988.250000\n",
      "Train Epoch: 266 [114944/225000 (51%)] Loss: 6920.705078\n",
      "Train Epoch: 266 [119040/225000 (53%)] Loss: 6871.072266\n",
      "Train Epoch: 266 [123136/225000 (55%)] Loss: 6976.333984\n",
      "Train Epoch: 266 [127232/225000 (57%)] Loss: 6815.462891\n",
      "Train Epoch: 266 [131328/225000 (58%)] Loss: 6897.671875\n",
      "Train Epoch: 266 [135424/225000 (60%)] Loss: 6951.970703\n",
      "Train Epoch: 266 [139520/225000 (62%)] Loss: 6960.904297\n",
      "Train Epoch: 266 [143616/225000 (64%)] Loss: 6856.929688\n",
      "Train Epoch: 266 [147712/225000 (66%)] Loss: 6988.080078\n",
      "Train Epoch: 266 [151808/225000 (67%)] Loss: 6964.640625\n",
      "Train Epoch: 266 [155904/225000 (69%)] Loss: 6815.431641\n",
      "Train Epoch: 266 [160000/225000 (71%)] Loss: 7007.986328\n",
      "Train Epoch: 266 [164096/225000 (73%)] Loss: 7088.050781\n",
      "Train Epoch: 266 [168192/225000 (75%)] Loss: 6939.775391\n",
      "Train Epoch: 266 [172288/225000 (77%)] Loss: 7169.615234\n",
      "Train Epoch: 266 [176384/225000 (78%)] Loss: 7059.388672\n",
      "Train Epoch: 266 [180480/225000 (80%)] Loss: 6895.273438\n",
      "Train Epoch: 266 [184576/225000 (82%)] Loss: 7010.980469\n",
      "Train Epoch: 266 [188672/225000 (84%)] Loss: 7019.923828\n",
      "Train Epoch: 266 [192768/225000 (86%)] Loss: 6941.271484\n",
      "Train Epoch: 266 [196864/225000 (87%)] Loss: 6978.707031\n",
      "Train Epoch: 266 [200960/225000 (89%)] Loss: 7184.894531\n",
      "Train Epoch: 266 [205056/225000 (91%)] Loss: 7122.242188\n",
      "Train Epoch: 266 [209152/225000 (93%)] Loss: 6889.119141\n",
      "Train Epoch: 266 [213248/225000 (95%)] Loss: 7077.050781\n",
      "Train Epoch: 266 [217344/225000 (97%)] Loss: 6918.445312\n",
      "Train Epoch: 266 [221440/225000 (98%)] Loss: 7023.398438\n",
      "    epoch          : 266\n",
      "    loss           : 7059.149156312215\n",
      "    val_loss       : 7072.434722542154\n",
      "Train Epoch: 267 [256/225000 (0%)] Loss: 7063.935547\n",
      "Train Epoch: 267 [4352/225000 (2%)] Loss: 6983.994141\n",
      "Train Epoch: 267 [8448/225000 (4%)] Loss: 7057.759766\n",
      "Train Epoch: 267 [12544/225000 (6%)] Loss: 7085.269531\n",
      "Train Epoch: 267 [16640/225000 (7%)] Loss: 6910.890625\n",
      "Train Epoch: 267 [20736/225000 (9%)] Loss: 7149.925781\n",
      "Train Epoch: 267 [24832/225000 (11%)] Loss: 6866.296875\n",
      "Train Epoch: 267 [28928/225000 (13%)] Loss: 6913.875000\n",
      "Train Epoch: 267 [33024/225000 (15%)] Loss: 7174.687500\n",
      "Train Epoch: 267 [37120/225000 (16%)] Loss: 7025.080078\n",
      "Train Epoch: 267 [41216/225000 (18%)] Loss: 7041.992188\n",
      "Train Epoch: 267 [45312/225000 (20%)] Loss: 7093.330078\n",
      "Train Epoch: 267 [49408/225000 (22%)] Loss: 7150.593750\n",
      "Train Epoch: 267 [53504/225000 (24%)] Loss: 7013.767578\n",
      "Train Epoch: 267 [57600/225000 (26%)] Loss: 6982.669922\n",
      "Train Epoch: 267 [61696/225000 (27%)] Loss: 7035.572266\n",
      "Train Epoch: 267 [65792/225000 (29%)] Loss: 7072.978516\n",
      "Train Epoch: 267 [69888/225000 (31%)] Loss: 6964.544922\n",
      "Train Epoch: 267 [73984/225000 (33%)] Loss: 7046.175781\n",
      "Train Epoch: 267 [78080/225000 (35%)] Loss: 7081.498047\n",
      "Train Epoch: 267 [82176/225000 (37%)] Loss: 6924.564453\n",
      "Train Epoch: 267 [86272/225000 (38%)] Loss: 6975.156250\n",
      "Train Epoch: 267 [90368/225000 (40%)] Loss: 7054.003906\n",
      "Train Epoch: 267 [94464/225000 (42%)] Loss: 6920.601562\n",
      "Train Epoch: 267 [98560/225000 (44%)] Loss: 6996.216797\n",
      "Train Epoch: 267 [102656/225000 (46%)] Loss: 6995.941406\n",
      "Train Epoch: 267 [106752/225000 (47%)] Loss: 6919.964844\n",
      "Train Epoch: 267 [110848/225000 (49%)] Loss: 7084.171875\n",
      "Train Epoch: 267 [114944/225000 (51%)] Loss: 7089.042969\n",
      "Train Epoch: 267 [119040/225000 (53%)] Loss: 7204.212891\n",
      "Train Epoch: 267 [123136/225000 (55%)] Loss: 6927.253906\n",
      "Train Epoch: 267 [127232/225000 (57%)] Loss: 6877.824219\n",
      "Train Epoch: 267 [131328/225000 (58%)] Loss: 6943.533203\n",
      "Train Epoch: 267 [135424/225000 (60%)] Loss: 7029.167969\n",
      "Train Epoch: 267 [139520/225000 (62%)] Loss: 7009.132812\n",
      "Train Epoch: 267 [143616/225000 (64%)] Loss: 6995.822266\n",
      "Train Epoch: 267 [147712/225000 (66%)] Loss: 6930.285156\n",
      "Train Epoch: 267 [151808/225000 (67%)] Loss: 6927.683594\n",
      "Train Epoch: 267 [155904/225000 (69%)] Loss: 6930.236328\n",
      "Train Epoch: 267 [160000/225000 (71%)] Loss: 7040.769531\n",
      "Train Epoch: 267 [164096/225000 (73%)] Loss: 6951.634766\n",
      "Train Epoch: 267 [168192/225000 (75%)] Loss: 7079.191406\n",
      "Train Epoch: 267 [172288/225000 (77%)] Loss: 7235.990234\n",
      "Train Epoch: 267 [176384/225000 (78%)] Loss: 6998.566406\n",
      "Train Epoch: 267 [180480/225000 (80%)] Loss: 7170.939453\n",
      "Train Epoch: 267 [184576/225000 (82%)] Loss: 6954.201172\n",
      "Train Epoch: 267 [188672/225000 (84%)] Loss: 7075.914062\n",
      "Train Epoch: 267 [192768/225000 (86%)] Loss: 7044.099609\n",
      "Train Epoch: 267 [196864/225000 (87%)] Loss: 7003.697266\n",
      "Train Epoch: 267 [200960/225000 (89%)] Loss: 7008.226562\n",
      "Train Epoch: 267 [205056/225000 (91%)] Loss: 7024.312500\n",
      "Train Epoch: 267 [209152/225000 (93%)] Loss: 7045.812500\n",
      "Train Epoch: 267 [213248/225000 (95%)] Loss: 6938.005859\n",
      "Train Epoch: 267 [217344/225000 (97%)] Loss: 6877.394531\n",
      "Train Epoch: 267 [221440/225000 (98%)] Loss: 6945.785156\n",
      "    epoch          : 267\n",
      "    loss           : 7031.243999528939\n",
      "    val_loss       : 7020.643578069551\n",
      "Train Epoch: 268 [256/225000 (0%)] Loss: 7017.578125\n",
      "Train Epoch: 268 [4352/225000 (2%)] Loss: 6840.322266\n",
      "Train Epoch: 268 [8448/225000 (4%)] Loss: 7066.462891\n",
      "Train Epoch: 268 [12544/225000 (6%)] Loss: 7062.955078\n",
      "Train Epoch: 268 [16640/225000 (7%)] Loss: 7009.253906\n",
      "Train Epoch: 268 [20736/225000 (9%)] Loss: 6952.998047\n",
      "Train Epoch: 268 [24832/225000 (11%)] Loss: 7103.935547\n",
      "Train Epoch: 268 [28928/225000 (13%)] Loss: 7087.187500\n",
      "Train Epoch: 268 [33024/225000 (15%)] Loss: 6836.568359\n",
      "Train Epoch: 268 [37120/225000 (16%)] Loss: 7128.615234\n",
      "Train Epoch: 268 [41216/225000 (18%)] Loss: 6964.800781\n",
      "Train Epoch: 268 [45312/225000 (20%)] Loss: 6983.265625\n",
      "Train Epoch: 268 [49408/225000 (22%)] Loss: 7049.529297\n",
      "Train Epoch: 268 [53504/225000 (24%)] Loss: 7018.953125\n",
      "Train Epoch: 268 [57600/225000 (26%)] Loss: 6984.919922\n",
      "Train Epoch: 268 [61696/225000 (27%)] Loss: 7043.046875\n",
      "Train Epoch: 268 [65792/225000 (29%)] Loss: 7022.837891\n",
      "Train Epoch: 268 [69888/225000 (31%)] Loss: 6999.708984\n",
      "Train Epoch: 268 [73984/225000 (33%)] Loss: 6978.490234\n",
      "Train Epoch: 268 [78080/225000 (35%)] Loss: 6876.080078\n",
      "Train Epoch: 268 [82176/225000 (37%)] Loss: 6809.826172\n",
      "Train Epoch: 268 [86272/225000 (38%)] Loss: 6956.958984\n",
      "Train Epoch: 268 [90368/225000 (40%)] Loss: 7001.847656\n",
      "Train Epoch: 268 [94464/225000 (42%)] Loss: 7135.158203\n",
      "Train Epoch: 268 [98560/225000 (44%)] Loss: 7171.246094\n",
      "Train Epoch: 268 [102656/225000 (46%)] Loss: 7151.041016\n",
      "Train Epoch: 268 [106752/225000 (47%)] Loss: 7094.650391\n",
      "Train Epoch: 268 [110848/225000 (49%)] Loss: 7023.232422\n",
      "Train Epoch: 268 [114944/225000 (51%)] Loss: 7138.544922\n",
      "Train Epoch: 268 [119040/225000 (53%)] Loss: 7013.488281\n",
      "Train Epoch: 268 [123136/225000 (55%)] Loss: 7021.742188\n",
      "Train Epoch: 268 [127232/225000 (57%)] Loss: 7086.933594\n",
      "Train Epoch: 268 [131328/225000 (58%)] Loss: 7125.406250\n",
      "Train Epoch: 268 [135424/225000 (60%)] Loss: 6969.220703\n",
      "Train Epoch: 268 [139520/225000 (62%)] Loss: 6932.679688\n",
      "Train Epoch: 268 [143616/225000 (64%)] Loss: 7051.705078\n",
      "Train Epoch: 268 [147712/225000 (66%)] Loss: 7114.097656\n",
      "Train Epoch: 268 [151808/225000 (67%)] Loss: 7012.472656\n",
      "Train Epoch: 268 [155904/225000 (69%)] Loss: 6936.265625\n",
      "Train Epoch: 268 [160000/225000 (71%)] Loss: 6878.714844\n",
      "Train Epoch: 268 [164096/225000 (73%)] Loss: 7198.861328\n",
      "Train Epoch: 268 [168192/225000 (75%)] Loss: 7116.294922\n",
      "Train Epoch: 268 [172288/225000 (77%)] Loss: 7102.978516\n",
      "Train Epoch: 268 [176384/225000 (78%)] Loss: 7004.941406\n",
      "Train Epoch: 268 [180480/225000 (80%)] Loss: 7046.628906\n",
      "Train Epoch: 268 [184576/225000 (82%)] Loss: 6908.748047\n",
      "Train Epoch: 268 [188672/225000 (84%)] Loss: 7004.281250\n",
      "Train Epoch: 268 [192768/225000 (86%)] Loss: 6998.392578\n",
      "Train Epoch: 268 [196864/225000 (87%)] Loss: 7026.080078\n",
      "Train Epoch: 268 [200960/225000 (89%)] Loss: 6852.314453\n",
      "Train Epoch: 268 [205056/225000 (91%)] Loss: 6764.148438\n",
      "Train Epoch: 268 [209152/225000 (93%)] Loss: 7044.119141\n",
      "Train Epoch: 268 [213248/225000 (95%)] Loss: 7023.898438\n",
      "Train Epoch: 268 [217344/225000 (97%)] Loss: 7008.982422\n",
      "Train Epoch: 268 [221440/225000 (98%)] Loss: 6955.060547\n",
      "    epoch          : 268\n",
      "    loss           : 7008.623563486562\n",
      "    val_loss       : 7016.10184659155\n",
      "Train Epoch: 269 [256/225000 (0%)] Loss: 6885.998047\n",
      "Train Epoch: 269 [4352/225000 (2%)] Loss: 6969.886719\n",
      "Train Epoch: 269 [8448/225000 (4%)] Loss: 7084.740234\n",
      "Train Epoch: 269 [12544/225000 (6%)] Loss: 7151.843750\n",
      "Train Epoch: 269 [16640/225000 (7%)] Loss: 6934.472656\n",
      "Train Epoch: 269 [20736/225000 (9%)] Loss: 7157.919922\n",
      "Train Epoch: 269 [24832/225000 (11%)] Loss: 6974.009766\n",
      "Train Epoch: 269 [28928/225000 (13%)] Loss: 6922.763672\n",
      "Train Epoch: 269 [33024/225000 (15%)] Loss: 7165.902344\n",
      "Train Epoch: 269 [37120/225000 (16%)] Loss: 7055.783203\n",
      "Train Epoch: 269 [41216/225000 (18%)] Loss: 6994.416016\n",
      "Train Epoch: 269 [45312/225000 (20%)] Loss: 7088.683594\n",
      "Train Epoch: 269 [49408/225000 (22%)] Loss: 7072.380859\n",
      "Train Epoch: 269 [53504/225000 (24%)] Loss: 6872.005859\n",
      "Train Epoch: 269 [57600/225000 (26%)] Loss: 6939.130859\n",
      "Train Epoch: 269 [61696/225000 (27%)] Loss: 7057.833984\n",
      "Train Epoch: 269 [65792/225000 (29%)] Loss: 6912.593750\n",
      "Train Epoch: 269 [69888/225000 (31%)] Loss: 7006.175781\n",
      "Train Epoch: 269 [73984/225000 (33%)] Loss: 6982.388672\n",
      "Train Epoch: 269 [78080/225000 (35%)] Loss: 6945.337891\n",
      "Train Epoch: 269 [82176/225000 (37%)] Loss: 6833.197266\n",
      "Train Epoch: 269 [86272/225000 (38%)] Loss: 7009.095703\n",
      "Train Epoch: 269 [90368/225000 (40%)] Loss: 7075.808594\n",
      "Train Epoch: 269 [94464/225000 (42%)] Loss: 6966.808594\n",
      "Train Epoch: 269 [98560/225000 (44%)] Loss: 6973.488281\n",
      "Train Epoch: 269 [102656/225000 (46%)] Loss: 6909.591797\n",
      "Train Epoch: 269 [106752/225000 (47%)] Loss: 6870.951172\n",
      "Train Epoch: 269 [110848/225000 (49%)] Loss: 6913.753906\n",
      "Train Epoch: 269 [114944/225000 (51%)] Loss: 7071.937500\n",
      "Train Epoch: 269 [119040/225000 (53%)] Loss: 7011.689453\n",
      "Train Epoch: 269 [123136/225000 (55%)] Loss: 7010.919922\n",
      "Train Epoch: 269 [127232/225000 (57%)] Loss: 6911.351562\n",
      "Train Epoch: 269 [131328/225000 (58%)] Loss: 7083.947266\n",
      "Train Epoch: 269 [135424/225000 (60%)] Loss: 7018.224609\n",
      "Train Epoch: 269 [139520/225000 (62%)] Loss: 7106.503906\n",
      "Train Epoch: 269 [143616/225000 (64%)] Loss: 7102.996094\n",
      "Train Epoch: 269 [147712/225000 (66%)] Loss: 6949.105469\n",
      "Train Epoch: 269 [151808/225000 (67%)] Loss: 7001.750000\n",
      "Train Epoch: 269 [155904/225000 (69%)] Loss: 6999.812500\n",
      "Train Epoch: 269 [160000/225000 (71%)] Loss: 7029.634766\n",
      "Train Epoch: 269 [164096/225000 (73%)] Loss: 6946.255859\n",
      "Train Epoch: 269 [168192/225000 (75%)] Loss: 7211.431641\n",
      "Train Epoch: 269 [172288/225000 (77%)] Loss: 6995.193359\n",
      "Train Epoch: 269 [176384/225000 (78%)] Loss: 7109.095703\n",
      "Train Epoch: 269 [180480/225000 (80%)] Loss: 7010.669922\n",
      "Train Epoch: 269 [184576/225000 (82%)] Loss: 7167.154297\n",
      "Train Epoch: 269 [188672/225000 (84%)] Loss: 6977.857422\n",
      "Train Epoch: 269 [192768/225000 (86%)] Loss: 6996.808594\n",
      "Train Epoch: 269 [196864/225000 (87%)] Loss: 7215.457031\n",
      "Train Epoch: 269 [200960/225000 (89%)] Loss: 7089.396484\n",
      "Train Epoch: 269 [205056/225000 (91%)] Loss: 6883.939453\n",
      "Train Epoch: 269 [209152/225000 (93%)] Loss: 6914.216797\n",
      "Train Epoch: 269 [213248/225000 (95%)] Loss: 6971.023438\n",
      "Train Epoch: 269 [217344/225000 (97%)] Loss: 7009.884766\n",
      "Train Epoch: 269 [221440/225000 (98%)] Loss: 7019.119141\n",
      "    epoch          : 269\n",
      "    loss           : 7026.73503159663\n",
      "    val_loss       : 7010.297887734005\n",
      "Train Epoch: 270 [256/225000 (0%)] Loss: 6869.052734\n",
      "Train Epoch: 270 [4352/225000 (2%)] Loss: 7154.484375\n",
      "Train Epoch: 270 [8448/225000 (4%)] Loss: 7081.595703\n",
      "Train Epoch: 270 [12544/225000 (6%)] Loss: 6942.962891\n",
      "Train Epoch: 270 [16640/225000 (7%)] Loss: 6953.546875\n",
      "Train Epoch: 270 [20736/225000 (9%)] Loss: 7122.234375\n",
      "Train Epoch: 270 [24832/225000 (11%)] Loss: 7127.664062\n",
      "Train Epoch: 270 [28928/225000 (13%)] Loss: 6922.832031\n",
      "Train Epoch: 270 [33024/225000 (15%)] Loss: 7023.884766\n",
      "Train Epoch: 270 [37120/225000 (16%)] Loss: 6981.585938\n",
      "Train Epoch: 270 [41216/225000 (18%)] Loss: 7059.029297\n",
      "Train Epoch: 270 [45312/225000 (20%)] Loss: 6892.267578\n",
      "Train Epoch: 270 [49408/225000 (22%)] Loss: 6865.617188\n",
      "Train Epoch: 270 [53504/225000 (24%)] Loss: 7229.925781\n",
      "Train Epoch: 270 [57600/225000 (26%)] Loss: 6936.285156\n",
      "Train Epoch: 270 [61696/225000 (27%)] Loss: 6996.437500\n",
      "Train Epoch: 270 [65792/225000 (29%)] Loss: 6879.076172\n",
      "Train Epoch: 270 [69888/225000 (31%)] Loss: 7172.724609\n",
      "Train Epoch: 270 [73984/225000 (33%)] Loss: 7054.261719\n",
      "Train Epoch: 270 [78080/225000 (35%)] Loss: 6913.929688\n",
      "Train Epoch: 270 [82176/225000 (37%)] Loss: 7161.863281\n",
      "Train Epoch: 270 [86272/225000 (38%)] Loss: 6943.439453\n",
      "Train Epoch: 270 [90368/225000 (40%)] Loss: 7089.814453\n",
      "Train Epoch: 270 [94464/225000 (42%)] Loss: 6767.152344\n",
      "Train Epoch: 270 [98560/225000 (44%)] Loss: 7014.857422\n",
      "Train Epoch: 270 [102656/225000 (46%)] Loss: 7268.171875\n",
      "Train Epoch: 270 [106752/225000 (47%)] Loss: 6939.103516\n",
      "Train Epoch: 270 [110848/225000 (49%)] Loss: 7001.646484\n",
      "Train Epoch: 270 [114944/225000 (51%)] Loss: 6964.240234\n",
      "Train Epoch: 270 [119040/225000 (53%)] Loss: 6885.544922\n",
      "Train Epoch: 270 [123136/225000 (55%)] Loss: 6822.029297\n",
      "Train Epoch: 270 [127232/225000 (57%)] Loss: 6921.722656\n",
      "Train Epoch: 270 [131328/225000 (58%)] Loss: 7030.931641\n",
      "Train Epoch: 270 [135424/225000 (60%)] Loss: 6968.871094\n",
      "Train Epoch: 270 [139520/225000 (62%)] Loss: 6919.619141\n",
      "Train Epoch: 270 [143616/225000 (64%)] Loss: 7087.423828\n",
      "Train Epoch: 270 [147712/225000 (66%)] Loss: 7032.818359\n",
      "Train Epoch: 270 [151808/225000 (67%)] Loss: 7071.248047\n",
      "Train Epoch: 270 [155904/225000 (69%)] Loss: 6970.408203\n",
      "Train Epoch: 270 [160000/225000 (71%)] Loss: 6921.203125\n",
      "Train Epoch: 270 [164096/225000 (73%)] Loss: 7113.789062\n",
      "Train Epoch: 270 [168192/225000 (75%)] Loss: 6961.123047\n",
      "Train Epoch: 270 [172288/225000 (77%)] Loss: 7179.757812\n",
      "Train Epoch: 270 [176384/225000 (78%)] Loss: 6993.060547\n",
      "Train Epoch: 270 [180480/225000 (80%)] Loss: 6920.656250\n",
      "Train Epoch: 270 [184576/225000 (82%)] Loss: 6979.851562\n",
      "Train Epoch: 270 [188672/225000 (84%)] Loss: 7068.181641\n",
      "Train Epoch: 270 [192768/225000 (86%)] Loss: 7039.583984\n",
      "Train Epoch: 270 [196864/225000 (87%)] Loss: 7018.390625\n",
      "Train Epoch: 270 [200960/225000 (89%)] Loss: 6966.248047\n",
      "Train Epoch: 270 [205056/225000 (91%)] Loss: 6995.767578\n",
      "Train Epoch: 270 [209152/225000 (93%)] Loss: 6979.005859\n",
      "Train Epoch: 270 [213248/225000 (95%)] Loss: 6962.015625\n",
      "Train Epoch: 270 [217344/225000 (97%)] Loss: 6992.636719\n",
      "Train Epoch: 270 [221440/225000 (98%)] Loss: 7020.779297\n",
      "    epoch          : 270\n",
      "    loss           : 7012.769624573379\n",
      "    val_loss       : 7011.301857087077\n",
      "Train Epoch: 271 [256/225000 (0%)] Loss: 6971.166016\n",
      "Train Epoch: 271 [4352/225000 (2%)] Loss: 6904.169922\n",
      "Train Epoch: 271 [8448/225000 (4%)] Loss: 7013.527344\n",
      "Train Epoch: 271 [12544/225000 (6%)] Loss: 6914.148438\n",
      "Train Epoch: 271 [16640/225000 (7%)] Loss: 6995.447266\n",
      "Train Epoch: 271 [20736/225000 (9%)] Loss: 7025.736328\n",
      "Train Epoch: 271 [24832/225000 (11%)] Loss: 6959.945312\n",
      "Train Epoch: 271 [28928/225000 (13%)] Loss: 7069.767578\n",
      "Train Epoch: 271 [33024/225000 (15%)] Loss: 7162.152344\n",
      "Train Epoch: 271 [37120/225000 (16%)] Loss: 7116.888672\n",
      "Train Epoch: 271 [41216/225000 (18%)] Loss: 7055.068359\n",
      "Train Epoch: 271 [45312/225000 (20%)] Loss: 7077.062500\n",
      "Train Epoch: 271 [49408/225000 (22%)] Loss: 6952.806641\n",
      "Train Epoch: 271 [53504/225000 (24%)] Loss: 7000.419922\n",
      "Train Epoch: 271 [57600/225000 (26%)] Loss: 6844.550781\n",
      "Train Epoch: 271 [61696/225000 (27%)] Loss: 7162.945312\n",
      "Train Epoch: 271 [65792/225000 (29%)] Loss: 6910.742188\n",
      "Train Epoch: 271 [69888/225000 (31%)] Loss: 7182.574219\n",
      "Train Epoch: 271 [73984/225000 (33%)] Loss: 7059.617188\n",
      "Train Epoch: 271 [78080/225000 (35%)] Loss: 6954.921875\n",
      "Train Epoch: 271 [82176/225000 (37%)] Loss: 7168.808594\n",
      "Train Epoch: 271 [86272/225000 (38%)] Loss: 7043.457031\n",
      "Train Epoch: 271 [90368/225000 (40%)] Loss: 7110.435547\n",
      "Train Epoch: 271 [94464/225000 (42%)] Loss: 7057.339844\n",
      "Train Epoch: 271 [98560/225000 (44%)] Loss: 7053.712891\n",
      "Train Epoch: 271 [102656/225000 (46%)] Loss: 6928.658203\n",
      "Train Epoch: 271 [106752/225000 (47%)] Loss: 7019.640625\n",
      "Train Epoch: 271 [110848/225000 (49%)] Loss: 7118.628906\n",
      "Train Epoch: 271 [114944/225000 (51%)] Loss: 6766.105469\n",
      "Train Epoch: 271 [119040/225000 (53%)] Loss: 7108.980469\n",
      "Train Epoch: 271 [123136/225000 (55%)] Loss: 6895.480469\n",
      "Train Epoch: 271 [127232/225000 (57%)] Loss: 6995.707031\n",
      "Train Epoch: 271 [131328/225000 (58%)] Loss: 6956.400391\n",
      "Train Epoch: 271 [135424/225000 (60%)] Loss: 6995.070312\n",
      "Train Epoch: 271 [139520/225000 (62%)] Loss: 6996.939453\n",
      "Train Epoch: 271 [143616/225000 (64%)] Loss: 7004.199219\n",
      "Train Epoch: 271 [147712/225000 (66%)] Loss: 7033.007812\n",
      "Train Epoch: 271 [151808/225000 (67%)] Loss: 6965.304688\n",
      "Train Epoch: 271 [155904/225000 (69%)] Loss: 7016.884766\n",
      "Train Epoch: 271 [160000/225000 (71%)] Loss: 7102.921875\n",
      "Train Epoch: 271 [164096/225000 (73%)] Loss: 6979.281250\n",
      "Train Epoch: 271 [168192/225000 (75%)] Loss: 6931.986328\n",
      "Train Epoch: 271 [172288/225000 (77%)] Loss: 7110.773438\n",
      "Train Epoch: 271 [176384/225000 (78%)] Loss: 7041.699219\n",
      "Train Epoch: 271 [180480/225000 (80%)] Loss: 6995.347656\n",
      "Train Epoch: 271 [184576/225000 (82%)] Loss: 7029.574219\n",
      "Train Epoch: 271 [188672/225000 (84%)] Loss: 7057.115234\n",
      "Train Epoch: 271 [192768/225000 (86%)] Loss: 6920.953125\n",
      "Train Epoch: 271 [196864/225000 (87%)] Loss: 7152.425781\n",
      "Train Epoch: 271 [200960/225000 (89%)] Loss: 7046.019531\n",
      "Train Epoch: 271 [205056/225000 (91%)] Loss: 6987.558594\n",
      "Train Epoch: 271 [209152/225000 (93%)] Loss: 6878.339844\n",
      "Train Epoch: 271 [213248/225000 (95%)] Loss: 7087.277344\n",
      "Train Epoch: 271 [217344/225000 (97%)] Loss: 6906.355469\n",
      "Train Epoch: 271 [221440/225000 (98%)] Loss: 6915.173828\n",
      "    epoch          : 271\n",
      "    loss           : 7021.739753315202\n",
      "    val_loss       : 7007.6823675425685\n",
      "Train Epoch: 272 [256/225000 (0%)] Loss: 7082.054688\n",
      "Train Epoch: 272 [4352/225000 (2%)] Loss: 7024.339844\n",
      "Train Epoch: 272 [8448/225000 (4%)] Loss: 6969.013672\n",
      "Train Epoch: 272 [12544/225000 (6%)] Loss: 7157.015625\n",
      "Train Epoch: 272 [16640/225000 (7%)] Loss: 6930.167969\n",
      "Train Epoch: 272 [20736/225000 (9%)] Loss: 7044.906250\n",
      "Train Epoch: 272 [24832/225000 (11%)] Loss: 7140.667969\n",
      "Train Epoch: 272 [28928/225000 (13%)] Loss: 7001.781250\n",
      "Train Epoch: 272 [33024/225000 (15%)] Loss: 6969.074219\n",
      "Train Epoch: 272 [37120/225000 (16%)] Loss: 6878.277344\n",
      "Train Epoch: 272 [41216/225000 (18%)] Loss: 6903.824219\n",
      "Train Epoch: 272 [45312/225000 (20%)] Loss: 7060.052734\n",
      "Train Epoch: 272 [49408/225000 (22%)] Loss: 6963.154297\n",
      "Train Epoch: 272 [53504/225000 (24%)] Loss: 7040.957031\n",
      "Train Epoch: 272 [57600/225000 (26%)] Loss: 7155.250000\n",
      "Train Epoch: 272 [61696/225000 (27%)] Loss: 6851.058594\n",
      "Train Epoch: 272 [65792/225000 (29%)] Loss: 6942.111328\n",
      "Train Epoch: 272 [69888/225000 (31%)] Loss: 6713.294922\n",
      "Train Epoch: 272 [73984/225000 (33%)] Loss: 6922.359375\n",
      "Train Epoch: 272 [78080/225000 (35%)] Loss: 6940.132812\n",
      "Train Epoch: 272 [82176/225000 (37%)] Loss: 7000.718750\n",
      "Train Epoch: 272 [86272/225000 (38%)] Loss: 7094.138672\n",
      "Train Epoch: 272 [90368/225000 (40%)] Loss: 7141.039062\n",
      "Train Epoch: 272 [94464/225000 (42%)] Loss: 7008.457031\n",
      "Train Epoch: 272 [98560/225000 (44%)] Loss: 6901.574219\n",
      "Train Epoch: 272 [102656/225000 (46%)] Loss: 6919.638672\n",
      "Train Epoch: 272 [106752/225000 (47%)] Loss: 7036.533203\n",
      "Train Epoch: 272 [110848/225000 (49%)] Loss: 7071.570312\n",
      "Train Epoch: 272 [114944/225000 (51%)] Loss: 7000.226562\n",
      "Train Epoch: 272 [119040/225000 (53%)] Loss: 6919.542969\n",
      "Train Epoch: 272 [123136/225000 (55%)] Loss: 6839.033203\n",
      "Train Epoch: 272 [127232/225000 (57%)] Loss: 7279.957031\n",
      "Train Epoch: 272 [131328/225000 (58%)] Loss: 7034.599609\n",
      "Train Epoch: 272 [135424/225000 (60%)] Loss: 6997.814453\n",
      "Train Epoch: 272 [139520/225000 (62%)] Loss: 7047.689453\n",
      "Train Epoch: 272 [143616/225000 (64%)] Loss: 7108.732422\n",
      "Train Epoch: 272 [147712/225000 (66%)] Loss: 7084.804688\n",
      "Train Epoch: 272 [151808/225000 (67%)] Loss: 7007.333984\n",
      "Train Epoch: 272 [155904/225000 (69%)] Loss: 7046.974609\n",
      "Train Epoch: 272 [160000/225000 (71%)] Loss: 7039.738281\n",
      "Train Epoch: 272 [164096/225000 (73%)] Loss: 6859.787109\n",
      "Train Epoch: 272 [168192/225000 (75%)] Loss: 7142.777344\n",
      "Train Epoch: 272 [172288/225000 (77%)] Loss: 7098.808594\n",
      "Train Epoch: 272 [176384/225000 (78%)] Loss: 7185.330078\n",
      "Train Epoch: 272 [180480/225000 (80%)] Loss: 7049.685547\n",
      "Train Epoch: 272 [184576/225000 (82%)] Loss: 6945.390625\n",
      "Train Epoch: 272 [188672/225000 (84%)] Loss: 6967.505859\n",
      "Train Epoch: 272 [192768/225000 (86%)] Loss: 6989.179688\n",
      "Train Epoch: 272 [196864/225000 (87%)] Loss: 7004.166016\n",
      "Train Epoch: 272 [200960/225000 (89%)] Loss: 6896.359375\n",
      "Train Epoch: 272 [205056/225000 (91%)] Loss: 7052.476562\n",
      "Train Epoch: 272 [209152/225000 (93%)] Loss: 6785.476562\n",
      "Train Epoch: 272 [213248/225000 (95%)] Loss: 6987.021484\n",
      "Train Epoch: 272 [217344/225000 (97%)] Loss: 7065.095703\n",
      "Train Epoch: 272 [221440/225000 (98%)] Loss: 7062.537109\n",
      "    epoch          : 272\n",
      "    loss           : 7018.462475113765\n",
      "    val_loss       : 7007.589435988543\n",
      "Train Epoch: 273 [256/225000 (0%)] Loss: 6933.978516\n",
      "Train Epoch: 273 [4352/225000 (2%)] Loss: 6967.542969\n",
      "Train Epoch: 273 [8448/225000 (4%)] Loss: 7086.529297\n",
      "Train Epoch: 273 [12544/225000 (6%)] Loss: 7024.958984\n",
      "Train Epoch: 273 [16640/225000 (7%)] Loss: 6880.671875\n",
      "Train Epoch: 273 [20736/225000 (9%)] Loss: 7067.521484\n",
      "Train Epoch: 273 [24832/225000 (11%)] Loss: 7034.853516\n",
      "Train Epoch: 273 [28928/225000 (13%)] Loss: 7011.746094\n",
      "Train Epoch: 273 [33024/225000 (15%)] Loss: 7013.146484\n",
      "Train Epoch: 273 [37120/225000 (16%)] Loss: 7071.576172\n",
      "Train Epoch: 273 [41216/225000 (18%)] Loss: 7130.011719\n",
      "Train Epoch: 273 [45312/225000 (20%)] Loss: 7019.343750\n",
      "Train Epoch: 273 [49408/225000 (22%)] Loss: 6836.882812\n",
      "Train Epoch: 273 [53504/225000 (24%)] Loss: 6936.451172\n",
      "Train Epoch: 273 [57600/225000 (26%)] Loss: 6934.826172\n",
      "Train Epoch: 273 [61696/225000 (27%)] Loss: 6944.324219\n",
      "Train Epoch: 273 [65792/225000 (29%)] Loss: 7008.441406\n",
      "Train Epoch: 273 [69888/225000 (31%)] Loss: 7018.503906\n",
      "Train Epoch: 273 [73984/225000 (33%)] Loss: 6948.843750\n",
      "Train Epoch: 273 [78080/225000 (35%)] Loss: 6869.910156\n",
      "Train Epoch: 273 [82176/225000 (37%)] Loss: 6919.751953\n",
      "Train Epoch: 273 [86272/225000 (38%)] Loss: 6783.474609\n",
      "Train Epoch: 273 [90368/225000 (40%)] Loss: 7028.810547\n",
      "Train Epoch: 273 [94464/225000 (42%)] Loss: 6894.251953\n",
      "Train Epoch: 273 [98560/225000 (44%)] Loss: 7054.402344\n",
      "Train Epoch: 273 [102656/225000 (46%)] Loss: 7013.785156\n",
      "Train Epoch: 273 [106752/225000 (47%)] Loss: 6950.083984\n",
      "Train Epoch: 273 [110848/225000 (49%)] Loss: 7030.556641\n",
      "Train Epoch: 273 [114944/225000 (51%)] Loss: 6977.244141\n",
      "Train Epoch: 273 [119040/225000 (53%)] Loss: 7042.878906\n",
      "Train Epoch: 273 [123136/225000 (55%)] Loss: 7011.697266\n",
      "Train Epoch: 273 [127232/225000 (57%)] Loss: 6950.888672\n",
      "Train Epoch: 273 [131328/225000 (58%)] Loss: 6963.980469\n",
      "Train Epoch: 273 [135424/225000 (60%)] Loss: 6950.785156\n",
      "Train Epoch: 273 [139520/225000 (62%)] Loss: 7038.896484\n",
      "Train Epoch: 273 [143616/225000 (64%)] Loss: 6904.091797\n",
      "Train Epoch: 273 [147712/225000 (66%)] Loss: 7138.281250\n",
      "Train Epoch: 273 [151808/225000 (67%)] Loss: 7090.244141\n",
      "Train Epoch: 273 [155904/225000 (69%)] Loss: 6992.048828\n",
      "Train Epoch: 273 [160000/225000 (71%)] Loss: 6861.113281\n",
      "Train Epoch: 273 [164096/225000 (73%)] Loss: 6934.222656\n",
      "Train Epoch: 273 [168192/225000 (75%)] Loss: 6955.654297\n",
      "Train Epoch: 273 [172288/225000 (77%)] Loss: 6979.667969\n",
      "Train Epoch: 273 [176384/225000 (78%)] Loss: 6941.341797\n",
      "Train Epoch: 273 [180480/225000 (80%)] Loss: 6848.550781\n",
      "Train Epoch: 273 [184576/225000 (82%)] Loss: 6905.318359\n",
      "Train Epoch: 273 [188672/225000 (84%)] Loss: 7154.312500\n",
      "Train Epoch: 273 [192768/225000 (86%)] Loss: 7074.488281\n",
      "Train Epoch: 273 [196864/225000 (87%)] Loss: 7039.408203\n",
      "Train Epoch: 273 [200960/225000 (89%)] Loss: 7050.761719\n",
      "Train Epoch: 273 [205056/225000 (91%)] Loss: 7053.474609\n",
      "Train Epoch: 273 [209152/225000 (93%)] Loss: 6939.330078\n",
      "Train Epoch: 273 [213248/225000 (95%)] Loss: 6929.052734\n",
      "Train Epoch: 273 [217344/225000 (97%)] Loss: 7008.097656\n",
      "Train Epoch: 273 [221440/225000 (98%)] Loss: 7059.101562\n",
      "    epoch          : 273\n",
      "    loss           : 7024.518395815558\n",
      "    val_loss       : 7009.236213207245\n",
      "Train Epoch: 274 [256/225000 (0%)] Loss: 6944.605469\n",
      "Train Epoch: 274 [4352/225000 (2%)] Loss: 6951.257812\n",
      "Train Epoch: 274 [8448/225000 (4%)] Loss: 6974.152344\n",
      "Train Epoch: 274 [12544/225000 (6%)] Loss: 6897.412109\n",
      "Train Epoch: 274 [16640/225000 (7%)] Loss: 6950.929688\n",
      "Train Epoch: 274 [20736/225000 (9%)] Loss: 6915.679688\n",
      "Train Epoch: 274 [24832/225000 (11%)] Loss: 6940.539062\n",
      "Train Epoch: 274 [28928/225000 (13%)] Loss: 7062.750000\n",
      "Train Epoch: 274 [33024/225000 (15%)] Loss: 6933.062500\n",
      "Train Epoch: 274 [37120/225000 (16%)] Loss: 7000.125000\n",
      "Train Epoch: 274 [41216/225000 (18%)] Loss: 6955.132812\n",
      "Train Epoch: 274 [45312/225000 (20%)] Loss: 6874.560547\n",
      "Train Epoch: 274 [49408/225000 (22%)] Loss: 6752.701172\n",
      "Train Epoch: 274 [53504/225000 (24%)] Loss: 7014.769531\n",
      "Train Epoch: 274 [57600/225000 (26%)] Loss: 7039.394531\n",
      "Train Epoch: 274 [61696/225000 (27%)] Loss: 6925.353516\n",
      "Train Epoch: 274 [65792/225000 (29%)] Loss: 6964.837891\n",
      "Train Epoch: 274 [69888/225000 (31%)] Loss: 7113.935547\n",
      "Train Epoch: 274 [73984/225000 (33%)] Loss: 7158.212891\n",
      "Train Epoch: 274 [78080/225000 (35%)] Loss: 6973.410156\n",
      "Train Epoch: 274 [82176/225000 (37%)] Loss: 7049.667969\n",
      "Train Epoch: 274 [86272/225000 (38%)] Loss: 6992.542969\n",
      "Train Epoch: 274 [90368/225000 (40%)] Loss: 6937.468750\n",
      "Train Epoch: 274 [94464/225000 (42%)] Loss: 7104.119141\n",
      "Train Epoch: 274 [98560/225000 (44%)] Loss: 7009.289062\n",
      "Train Epoch: 274 [102656/225000 (46%)] Loss: 6974.812500\n",
      "Train Epoch: 274 [106752/225000 (47%)] Loss: 6909.046875\n",
      "Train Epoch: 274 [110848/225000 (49%)] Loss: 6927.878906\n",
      "Train Epoch: 274 [114944/225000 (51%)] Loss: 6986.605469\n",
      "Train Epoch: 274 [119040/225000 (53%)] Loss: 6945.089844\n",
      "Train Epoch: 274 [123136/225000 (55%)] Loss: 6969.660156\n",
      "Train Epoch: 274 [127232/225000 (57%)] Loss: 7254.851562\n",
      "Train Epoch: 274 [131328/225000 (58%)] Loss: 7094.544922\n",
      "Train Epoch: 274 [135424/225000 (60%)] Loss: 6862.298828\n",
      "Train Epoch: 274 [139520/225000 (62%)] Loss: 7194.326172\n",
      "Train Epoch: 274 [143616/225000 (64%)] Loss: 7122.837891\n",
      "Train Epoch: 274 [147712/225000 (66%)] Loss: 7087.933594\n",
      "Train Epoch: 274 [151808/225000 (67%)] Loss: 6984.873047\n",
      "Train Epoch: 274 [155904/225000 (69%)] Loss: 6905.548828\n",
      "Train Epoch: 274 [160000/225000 (71%)] Loss: 7045.386719\n",
      "Train Epoch: 274 [164096/225000 (73%)] Loss: 7081.041016\n",
      "Train Epoch: 274 [168192/225000 (75%)] Loss: 6999.564453\n",
      "Train Epoch: 274 [172288/225000 (77%)] Loss: 7056.513672\n",
      "Train Epoch: 274 [176384/225000 (78%)] Loss: 7062.287109\n",
      "Train Epoch: 274 [180480/225000 (80%)] Loss: 6960.031250\n",
      "Train Epoch: 274 [184576/225000 (82%)] Loss: 7059.761719\n",
      "Train Epoch: 274 [188672/225000 (84%)] Loss: 7035.861328\n",
      "Train Epoch: 274 [192768/225000 (86%)] Loss: 6851.455078\n",
      "Train Epoch: 274 [196864/225000 (87%)] Loss: 6958.625000\n",
      "Train Epoch: 274 [200960/225000 (89%)] Loss: 6996.771484\n",
      "Train Epoch: 274 [205056/225000 (91%)] Loss: 7059.583984\n",
      "Train Epoch: 274 [209152/225000 (93%)] Loss: 6807.783203\n",
      "Train Epoch: 274 [213248/225000 (95%)] Loss: 6953.917969\n",
      "Train Epoch: 274 [217344/225000 (97%)] Loss: 7056.552734\n",
      "Train Epoch: 274 [221440/225000 (98%)] Loss: 7010.953125\n",
      "    epoch          : 274\n",
      "    loss           : 6996.028211435225\n",
      "    val_loss       : 7003.097989672301\n",
      "Train Epoch: 275 [256/225000 (0%)] Loss: 7144.935547\n",
      "Train Epoch: 275 [4352/225000 (2%)] Loss: 6911.972656\n",
      "Train Epoch: 275 [8448/225000 (4%)] Loss: 6895.658203\n",
      "Train Epoch: 275 [12544/225000 (6%)] Loss: 7108.683594\n",
      "Train Epoch: 275 [16640/225000 (7%)] Loss: 7099.367188\n",
      "Train Epoch: 275 [20736/225000 (9%)] Loss: 7061.341797\n",
      "Train Epoch: 275 [24832/225000 (11%)] Loss: 6847.921875\n",
      "Train Epoch: 275 [28928/225000 (13%)] Loss: 7103.888672\n",
      "Train Epoch: 275 [33024/225000 (15%)] Loss: 6966.591797\n",
      "Train Epoch: 275 [37120/225000 (16%)] Loss: 6851.085938\n",
      "Train Epoch: 275 [41216/225000 (18%)] Loss: 7096.882812\n",
      "Train Epoch: 275 [45312/225000 (20%)] Loss: 7053.355469\n",
      "Train Epoch: 275 [49408/225000 (22%)] Loss: 7023.777344\n",
      "Train Epoch: 275 [53504/225000 (24%)] Loss: 7069.234375\n",
      "Train Epoch: 275 [57600/225000 (26%)] Loss: 7021.015625\n",
      "Train Epoch: 275 [61696/225000 (27%)] Loss: 6977.982422\n",
      "Train Epoch: 275 [65792/225000 (29%)] Loss: 7013.544922\n",
      "Train Epoch: 275 [69888/225000 (31%)] Loss: 6782.998047\n",
      "Train Epoch: 275 [73984/225000 (33%)] Loss: 6924.902344\n",
      "Train Epoch: 275 [78080/225000 (35%)] Loss: 7059.058594\n",
      "Train Epoch: 275 [82176/225000 (37%)] Loss: 6946.144531\n",
      "Train Epoch: 275 [86272/225000 (38%)] Loss: 6911.833984\n",
      "Train Epoch: 275 [90368/225000 (40%)] Loss: 6982.076172\n",
      "Train Epoch: 275 [94464/225000 (42%)] Loss: 7007.646484\n",
      "Train Epoch: 275 [98560/225000 (44%)] Loss: 7072.679688\n",
      "Train Epoch: 275 [102656/225000 (46%)] Loss: 7077.964844\n",
      "Train Epoch: 275 [106752/225000 (47%)] Loss: 7022.101562\n",
      "Train Epoch: 275 [110848/225000 (49%)] Loss: 6981.873047\n",
      "Train Epoch: 275 [114944/225000 (51%)] Loss: 7191.361328\n",
      "Train Epoch: 275 [119040/225000 (53%)] Loss: 6899.453125\n",
      "Train Epoch: 275 [123136/225000 (55%)] Loss: 7122.083984\n",
      "Train Epoch: 275 [127232/225000 (57%)] Loss: 6897.068359\n",
      "Train Epoch: 275 [131328/225000 (58%)] Loss: 6925.191406\n",
      "Train Epoch: 275 [135424/225000 (60%)] Loss: 6966.162109\n",
      "Train Epoch: 275 [139520/225000 (62%)] Loss: 7061.478516\n",
      "Train Epoch: 275 [143616/225000 (64%)] Loss: 7070.951172\n",
      "Train Epoch: 275 [147712/225000 (66%)] Loss: 6827.066406\n",
      "Train Epoch: 275 [151808/225000 (67%)] Loss: 6999.794922\n",
      "Train Epoch: 275 [155904/225000 (69%)] Loss: 7190.484375\n",
      "Train Epoch: 275 [160000/225000 (71%)] Loss: 7133.142578\n",
      "Train Epoch: 275 [164096/225000 (73%)] Loss: 6938.791016\n",
      "Train Epoch: 275 [168192/225000 (75%)] Loss: 7070.765625\n",
      "Train Epoch: 275 [172288/225000 (77%)] Loss: 7070.767578\n",
      "Train Epoch: 275 [176384/225000 (78%)] Loss: 7074.410156\n",
      "Train Epoch: 275 [180480/225000 (80%)] Loss: 6900.724609\n",
      "Train Epoch: 275 [184576/225000 (82%)] Loss: 7109.941406\n",
      "Train Epoch: 275 [188672/225000 (84%)] Loss: 6863.982422\n",
      "Train Epoch: 275 [192768/225000 (86%)] Loss: 7012.076172\n",
      "Train Epoch: 275 [196864/225000 (87%)] Loss: 7060.072266\n",
      "Train Epoch: 275 [200960/225000 (89%)] Loss: 7077.236328\n",
      "Train Epoch: 275 [205056/225000 (91%)] Loss: 6997.203125\n",
      "Train Epoch: 275 [209152/225000 (93%)] Loss: 6987.953125\n",
      "Train Epoch: 275 [213248/225000 (95%)] Loss: 6846.857422\n",
      "Train Epoch: 275 [217344/225000 (97%)] Loss: 6926.708984\n",
      "Train Epoch: 275 [221440/225000 (98%)] Loss: 6742.515625\n",
      "    epoch          : 275\n",
      "    loss           : 7026.400859463879\n",
      "    val_loss       : 7052.8795601299835\n",
      "Train Epoch: 276 [256/225000 (0%)] Loss: 7014.535156\n",
      "Train Epoch: 276 [4352/225000 (2%)] Loss: 6849.316406\n",
      "Train Epoch: 276 [8448/225000 (4%)] Loss: 6974.960938\n",
      "Train Epoch: 276 [12544/225000 (6%)] Loss: 7056.669922\n",
      "Train Epoch: 276 [16640/225000 (7%)] Loss: 7045.654297\n",
      "Train Epoch: 276 [20736/225000 (9%)] Loss: 7093.865234\n",
      "Train Epoch: 276 [24832/225000 (11%)] Loss: 6827.818359\n",
      "Train Epoch: 276 [28928/225000 (13%)] Loss: 6915.343750\n",
      "Train Epoch: 276 [33024/225000 (15%)] Loss: 7116.230469\n",
      "Train Epoch: 276 [37120/225000 (16%)] Loss: 7224.148438\n",
      "Train Epoch: 276 [41216/225000 (18%)] Loss: 7036.101562\n",
      "Train Epoch: 276 [45312/225000 (20%)] Loss: 6977.666016\n",
      "Train Epoch: 276 [49408/225000 (22%)] Loss: 7089.130859\n",
      "Train Epoch: 276 [53504/225000 (24%)] Loss: 7003.490234\n",
      "Train Epoch: 276 [57600/225000 (26%)] Loss: 6959.957031\n",
      "Train Epoch: 276 [61696/225000 (27%)] Loss: 6963.853516\n",
      "Train Epoch: 276 [65792/225000 (29%)] Loss: 7183.109375\n",
      "Train Epoch: 276 [69888/225000 (31%)] Loss: 6898.914062\n",
      "Train Epoch: 276 [73984/225000 (33%)] Loss: 6928.683594\n",
      "Train Epoch: 276 [78080/225000 (35%)] Loss: 6865.156250\n",
      "Train Epoch: 276 [82176/225000 (37%)] Loss: 7001.648438\n",
      "Train Epoch: 276 [86272/225000 (38%)] Loss: 7036.107422\n",
      "Train Epoch: 276 [90368/225000 (40%)] Loss: 7122.152344\n",
      "Train Epoch: 276 [94464/225000 (42%)] Loss: 6923.054688\n",
      "Train Epoch: 276 [98560/225000 (44%)] Loss: 7062.789062\n",
      "Train Epoch: 276 [102656/225000 (46%)] Loss: 7057.703125\n",
      "Train Epoch: 276 [106752/225000 (47%)] Loss: 6947.759766\n",
      "Train Epoch: 276 [110848/225000 (49%)] Loss: 6979.396484\n",
      "Train Epoch: 276 [114944/225000 (51%)] Loss: 6944.376953\n",
      "Train Epoch: 276 [119040/225000 (53%)] Loss: 6826.623047\n",
      "Train Epoch: 276 [123136/225000 (55%)] Loss: 6895.144531\n",
      "Train Epoch: 276 [127232/225000 (57%)] Loss: 6971.230469\n",
      "Train Epoch: 276 [131328/225000 (58%)] Loss: 7069.572266\n",
      "Train Epoch: 276 [135424/225000 (60%)] Loss: 6845.351562\n",
      "Train Epoch: 276 [139520/225000 (62%)] Loss: 6865.060547\n",
      "Train Epoch: 276 [143616/225000 (64%)] Loss: 6948.638672\n",
      "Train Epoch: 276 [147712/225000 (66%)] Loss: 7066.484375\n",
      "Train Epoch: 276 [151808/225000 (67%)] Loss: 6998.296875\n",
      "Train Epoch: 276 [155904/225000 (69%)] Loss: 6925.009766\n",
      "Train Epoch: 276 [160000/225000 (71%)] Loss: 6910.208984\n",
      "Train Epoch: 276 [164096/225000 (73%)] Loss: 7120.046875\n",
      "Train Epoch: 276 [168192/225000 (75%)] Loss: 7166.103516\n",
      "Train Epoch: 276 [172288/225000 (77%)] Loss: 6927.908203\n",
      "Train Epoch: 276 [176384/225000 (78%)] Loss: 6935.158203\n",
      "Train Epoch: 276 [180480/225000 (80%)] Loss: 7077.757812\n",
      "Train Epoch: 276 [184576/225000 (82%)] Loss: 7016.125000\n",
      "Train Epoch: 276 [188672/225000 (84%)] Loss: 6965.835938\n",
      "Train Epoch: 276 [192768/225000 (86%)] Loss: 6993.580078\n",
      "Train Epoch: 276 [196864/225000 (87%)] Loss: 7105.035156\n",
      "Train Epoch: 276 [200960/225000 (89%)] Loss: 6888.667969\n",
      "Train Epoch: 276 [205056/225000 (91%)] Loss: 7015.980469\n",
      "Train Epoch: 276 [209152/225000 (93%)] Loss: 7095.765625\n",
      "Train Epoch: 276 [213248/225000 (95%)] Loss: 7120.406250\n",
      "Train Epoch: 276 [217344/225000 (97%)] Loss: 6857.689453\n",
      "Train Epoch: 276 [221440/225000 (98%)] Loss: 6910.789062\n",
      "    epoch          : 276\n",
      "    loss           : 6991.937874404508\n",
      "    val_loss       : 7001.200263567117\n",
      "Train Epoch: 277 [256/225000 (0%)] Loss: 6895.167969\n",
      "Train Epoch: 277 [4352/225000 (2%)] Loss: 6948.371094\n",
      "Train Epoch: 277 [8448/225000 (4%)] Loss: 6999.447266\n",
      "Train Epoch: 277 [12544/225000 (6%)] Loss: 7005.484375\n",
      "Train Epoch: 277 [16640/225000 (7%)] Loss: 7055.771484\n",
      "Train Epoch: 277 [20736/225000 (9%)] Loss: 6812.080078\n",
      "Train Epoch: 277 [24832/225000 (11%)] Loss: 6851.175781\n",
      "Train Epoch: 277 [28928/225000 (13%)] Loss: 6944.533203\n",
      "Train Epoch: 277 [33024/225000 (15%)] Loss: 6972.675781\n",
      "Train Epoch: 277 [37120/225000 (16%)] Loss: 6873.308594\n",
      "Train Epoch: 277 [41216/225000 (18%)] Loss: 7190.984375\n",
      "Train Epoch: 277 [45312/225000 (20%)] Loss: 6923.541016\n",
      "Train Epoch: 277 [49408/225000 (22%)] Loss: 6853.255859\n",
      "Train Epoch: 277 [53504/225000 (24%)] Loss: 7097.683594\n",
      "Train Epoch: 277 [57600/225000 (26%)] Loss: 6912.529297\n",
      "Train Epoch: 277 [61696/225000 (27%)] Loss: 6951.353516\n",
      "Train Epoch: 277 [65792/225000 (29%)] Loss: 7012.232422\n",
      "Train Epoch: 277 [69888/225000 (31%)] Loss: 7124.685547\n",
      "Train Epoch: 277 [73984/225000 (33%)] Loss: 6939.978516\n",
      "Train Epoch: 277 [78080/225000 (35%)] Loss: 6920.652344\n",
      "Train Epoch: 277 [82176/225000 (37%)] Loss: 7004.035156\n",
      "Train Epoch: 277 [86272/225000 (38%)] Loss: 6942.955078\n",
      "Train Epoch: 277 [90368/225000 (40%)] Loss: 7179.408203\n",
      "Train Epoch: 277 [94464/225000 (42%)] Loss: 7060.457031\n",
      "Train Epoch: 277 [98560/225000 (44%)] Loss: 7114.533203\n",
      "Train Epoch: 277 [102656/225000 (46%)] Loss: 7051.244141\n",
      "Train Epoch: 277 [106752/225000 (47%)] Loss: 7121.822266\n",
      "Train Epoch: 277 [110848/225000 (49%)] Loss: 7140.376953\n",
      "Train Epoch: 277 [114944/225000 (51%)] Loss: 7052.230469\n",
      "Train Epoch: 277 [119040/225000 (53%)] Loss: 6929.632812\n",
      "Train Epoch: 277 [123136/225000 (55%)] Loss: 7113.949219\n",
      "Train Epoch: 277 [127232/225000 (57%)] Loss: 6930.773438\n",
      "Train Epoch: 277 [131328/225000 (58%)] Loss: 7034.716797\n",
      "Train Epoch: 277 [135424/225000 (60%)] Loss: 6963.111328\n",
      "Train Epoch: 277 [139520/225000 (62%)] Loss: 6932.216797\n",
      "Train Epoch: 277 [143616/225000 (64%)] Loss: 7124.525391\n",
      "Train Epoch: 277 [147712/225000 (66%)] Loss: 7084.753906\n",
      "Train Epoch: 277 [151808/225000 (67%)] Loss: 7058.613281\n",
      "Train Epoch: 277 [155904/225000 (69%)] Loss: 7036.292969\n",
      "Train Epoch: 277 [160000/225000 (71%)] Loss: 6997.958984\n",
      "Train Epoch: 277 [164096/225000 (73%)] Loss: 7094.619141\n",
      "Train Epoch: 277 [168192/225000 (75%)] Loss: 6822.701172\n",
      "Train Epoch: 277 [172288/225000 (77%)] Loss: 6900.025391\n",
      "Train Epoch: 277 [176384/225000 (78%)] Loss: 7071.947266\n",
      "Train Epoch: 277 [180480/225000 (80%)] Loss: 6998.408203\n",
      "Train Epoch: 277 [184576/225000 (82%)] Loss: 6986.810547\n",
      "Train Epoch: 277 [188672/225000 (84%)] Loss: 7025.257812\n",
      "Train Epoch: 277 [192768/225000 (86%)] Loss: 6995.859375\n",
      "Train Epoch: 277 [196864/225000 (87%)] Loss: 7039.595703\n",
      "Train Epoch: 277 [200960/225000 (89%)] Loss: 6932.744141\n",
      "Train Epoch: 277 [205056/225000 (91%)] Loss: 7038.609375\n",
      "Train Epoch: 277 [209152/225000 (93%)] Loss: 6843.525391\n",
      "Train Epoch: 277 [213248/225000 (95%)] Loss: 7127.076172\n",
      "Train Epoch: 277 [217344/225000 (97%)] Loss: 7040.501953\n",
      "Train Epoch: 277 [221440/225000 (98%)] Loss: 7011.105469\n",
      "    epoch          : 277\n",
      "    loss           : 7036.619487254693\n",
      "    val_loss       : 6996.303344274053\n",
      "Train Epoch: 278 [256/225000 (0%)] Loss: 7072.541016\n",
      "Train Epoch: 278 [4352/225000 (2%)] Loss: 6901.931641\n",
      "Train Epoch: 278 [8448/225000 (4%)] Loss: 6987.654297\n",
      "Train Epoch: 278 [12544/225000 (6%)] Loss: 7188.515625\n",
      "Train Epoch: 278 [16640/225000 (7%)] Loss: 7084.320312\n",
      "Train Epoch: 278 [20736/225000 (9%)] Loss: 7122.412109\n",
      "Train Epoch: 278 [24832/225000 (11%)] Loss: 6986.658203\n",
      "Train Epoch: 278 [28928/225000 (13%)] Loss: 6932.833984\n",
      "Train Epoch: 278 [33024/225000 (15%)] Loss: 7063.861328\n",
      "Train Epoch: 278 [37120/225000 (16%)] Loss: 7045.246094\n",
      "Train Epoch: 278 [41216/225000 (18%)] Loss: 7245.658203\n",
      "Train Epoch: 278 [45312/225000 (20%)] Loss: 6957.992188\n",
      "Train Epoch: 278 [49408/225000 (22%)] Loss: 6861.021484\n",
      "Train Epoch: 278 [53504/225000 (24%)] Loss: 6926.076172\n",
      "Train Epoch: 278 [57600/225000 (26%)] Loss: 7097.251953\n",
      "Train Epoch: 278 [61696/225000 (27%)] Loss: 6936.519531\n",
      "Train Epoch: 278 [65792/225000 (29%)] Loss: 6991.117188\n",
      "Train Epoch: 278 [69888/225000 (31%)] Loss: 7021.722656\n",
      "Train Epoch: 278 [73984/225000 (33%)] Loss: 7071.140625\n",
      "Train Epoch: 278 [78080/225000 (35%)] Loss: 7106.460938\n",
      "Train Epoch: 278 [82176/225000 (37%)] Loss: 6957.380859\n",
      "Train Epoch: 278 [86272/225000 (38%)] Loss: 7095.871094\n",
      "Train Epoch: 278 [90368/225000 (40%)] Loss: 7035.667969\n",
      "Train Epoch: 278 [94464/225000 (42%)] Loss: 6925.443359\n",
      "Train Epoch: 278 [98560/225000 (44%)] Loss: 7223.945312\n",
      "Train Epoch: 278 [102656/225000 (46%)] Loss: 6941.197266\n",
      "Train Epoch: 278 [106752/225000 (47%)] Loss: 7060.763672\n",
      "Train Epoch: 278 [110848/225000 (49%)] Loss: 6989.531250\n",
      "Train Epoch: 278 [114944/225000 (51%)] Loss: 6981.675781\n",
      "Train Epoch: 278 [119040/225000 (53%)] Loss: 6775.275391\n",
      "Train Epoch: 278 [123136/225000 (55%)] Loss: 6960.037109\n",
      "Train Epoch: 278 [127232/225000 (57%)] Loss: 6953.925781\n",
      "Train Epoch: 278 [131328/225000 (58%)] Loss: 6909.453125\n",
      "Train Epoch: 278 [135424/225000 (60%)] Loss: 7108.906250\n",
      "Train Epoch: 278 [139520/225000 (62%)] Loss: 6995.476562\n",
      "Train Epoch: 278 [143616/225000 (64%)] Loss: 7114.328125\n",
      "Train Epoch: 278 [147712/225000 (66%)] Loss: 7049.318359\n",
      "Train Epoch: 278 [151808/225000 (67%)] Loss: 7040.125000\n",
      "Train Epoch: 278 [155904/225000 (69%)] Loss: 7089.353516\n",
      "Train Epoch: 278 [160000/225000 (71%)] Loss: 6940.707031\n",
      "Train Epoch: 278 [164096/225000 (73%)] Loss: 6882.125000\n",
      "Train Epoch: 278 [168192/225000 (75%)] Loss: 7032.031250\n",
      "Train Epoch: 278 [172288/225000 (77%)] Loss: 6969.912109\n",
      "Train Epoch: 278 [176384/225000 (78%)] Loss: 6869.824219\n",
      "Train Epoch: 278 [180480/225000 (80%)] Loss: 6824.455078\n",
      "Train Epoch: 278 [184576/225000 (82%)] Loss: 7007.931641\n",
      "Train Epoch: 278 [188672/225000 (84%)] Loss: 6937.513672\n",
      "Train Epoch: 278 [192768/225000 (86%)] Loss: 6935.216797\n",
      "Train Epoch: 278 [196864/225000 (87%)] Loss: 7051.837891\n",
      "Train Epoch: 278 [200960/225000 (89%)] Loss: 6908.873047\n",
      "Train Epoch: 278 [205056/225000 (91%)] Loss: 6906.796875\n",
      "Train Epoch: 278 [209152/225000 (93%)] Loss: 7027.960938\n",
      "Train Epoch: 278 [213248/225000 (95%)] Loss: 6968.925781\n",
      "Train Epoch: 278 [217344/225000 (97%)] Loss: 6940.080078\n",
      "Train Epoch: 278 [221440/225000 (98%)] Loss: 7051.308594\n",
      "    epoch          : 278\n",
      "    loss           : 7050.654081342434\n",
      "    val_loss       : 6995.144406172694\n",
      "Train Epoch: 279 [256/225000 (0%)] Loss: 6937.976562\n",
      "Train Epoch: 279 [4352/225000 (2%)] Loss: 6987.394531\n",
      "Train Epoch: 279 [8448/225000 (4%)] Loss: 7047.205078\n",
      "Train Epoch: 279 [12544/225000 (6%)] Loss: 7072.921875\n",
      "Train Epoch: 279 [16640/225000 (7%)] Loss: 6919.220703\n",
      "Train Epoch: 279 [20736/225000 (9%)] Loss: 7008.953125\n",
      "Train Epoch: 279 [24832/225000 (11%)] Loss: 6823.005859\n",
      "Train Epoch: 279 [28928/225000 (13%)] Loss: 6818.277344\n",
      "Train Epoch: 279 [33024/225000 (15%)] Loss: 6916.216797\n",
      "Train Epoch: 279 [37120/225000 (16%)] Loss: 6902.212891\n",
      "Train Epoch: 279 [41216/225000 (18%)] Loss: 6994.494141\n",
      "Train Epoch: 279 [45312/225000 (20%)] Loss: 7145.613281\n",
      "Train Epoch: 279 [49408/225000 (22%)] Loss: 6899.230469\n",
      "Train Epoch: 279 [53504/225000 (24%)] Loss: 6895.728516\n",
      "Train Epoch: 279 [57600/225000 (26%)] Loss: 7058.419922\n",
      "Train Epoch: 279 [61696/225000 (27%)] Loss: 6947.998047\n",
      "Train Epoch: 279 [65792/225000 (29%)] Loss: 7103.531250\n",
      "Train Epoch: 279 [69888/225000 (31%)] Loss: 6959.343750\n",
      "Train Epoch: 279 [73984/225000 (33%)] Loss: 6826.501953\n",
      "Train Epoch: 279 [78080/225000 (35%)] Loss: 7006.826172\n",
      "Train Epoch: 279 [82176/225000 (37%)] Loss: 6994.658203\n",
      "Train Epoch: 279 [86272/225000 (38%)] Loss: 7196.445312\n",
      "Train Epoch: 279 [90368/225000 (40%)] Loss: 7096.183594\n",
      "Train Epoch: 279 [94464/225000 (42%)] Loss: 6788.416016\n",
      "Train Epoch: 279 [98560/225000 (44%)] Loss: 7136.148438\n",
      "Train Epoch: 279 [102656/225000 (46%)] Loss: 7053.917969\n",
      "Train Epoch: 279 [106752/225000 (47%)] Loss: 6920.658203\n",
      "Train Epoch: 279 [110848/225000 (49%)] Loss: 7014.248047\n",
      "Train Epoch: 279 [114944/225000 (51%)] Loss: 6966.492188\n",
      "Train Epoch: 279 [119040/225000 (53%)] Loss: 7014.875000\n",
      "Train Epoch: 279 [123136/225000 (55%)] Loss: 6952.556641\n",
      "Train Epoch: 279 [127232/225000 (57%)] Loss: 6967.753906\n",
      "Train Epoch: 279 [131328/225000 (58%)] Loss: 6868.875000\n",
      "Train Epoch: 279 [135424/225000 (60%)] Loss: 7006.816406\n",
      "Train Epoch: 279 [139520/225000 (62%)] Loss: 6896.138672\n",
      "Train Epoch: 279 [143616/225000 (64%)] Loss: 6919.638672\n",
      "Train Epoch: 279 [147712/225000 (66%)] Loss: 6969.025391\n",
      "Train Epoch: 279 [151808/225000 (67%)] Loss: 7045.998047\n",
      "Train Epoch: 279 [155904/225000 (69%)] Loss: 6886.689453\n",
      "Train Epoch: 279 [160000/225000 (71%)] Loss: 6878.001953\n",
      "Train Epoch: 279 [164096/225000 (73%)] Loss: 6910.333984\n",
      "Train Epoch: 279 [168192/225000 (75%)] Loss: 7007.996094\n",
      "Train Epoch: 279 [172288/225000 (77%)] Loss: 6913.109375\n",
      "Train Epoch: 279 [176384/225000 (78%)] Loss: 6980.994141\n",
      "Train Epoch: 279 [180480/225000 (80%)] Loss: 7077.292969\n",
      "Train Epoch: 279 [184576/225000 (82%)] Loss: 7006.011719\n",
      "Train Epoch: 279 [188672/225000 (84%)] Loss: 6944.544922\n",
      "Train Epoch: 279 [192768/225000 (86%)] Loss: 6961.748047\n",
      "Train Epoch: 279 [196864/225000 (87%)] Loss: 7052.644531\n",
      "Train Epoch: 279 [200960/225000 (89%)] Loss: 6947.804688\n",
      "Train Epoch: 279 [205056/225000 (91%)] Loss: 7011.552734\n",
      "Train Epoch: 279 [209152/225000 (93%)] Loss: 7029.947266\n",
      "Train Epoch: 279 [213248/225000 (95%)] Loss: 6959.853516\n",
      "Train Epoch: 279 [217344/225000 (97%)] Loss: 7033.738281\n",
      "Train Epoch: 279 [221440/225000 (98%)] Loss: 7080.279297\n",
      "    epoch          : 279\n",
      "    loss           : 6997.338339466012\n",
      "    val_loss       : 6994.428339539742\n",
      "Train Epoch: 280 [256/225000 (0%)] Loss: 7029.837891\n",
      "Train Epoch: 280 [4352/225000 (2%)] Loss: 6932.119141\n",
      "Train Epoch: 280 [8448/225000 (4%)] Loss: 7102.132812\n",
      "Train Epoch: 280 [12544/225000 (6%)] Loss: 7002.400391\n",
      "Train Epoch: 280 [16640/225000 (7%)] Loss: 6884.128906\n",
      "Train Epoch: 280 [20736/225000 (9%)] Loss: 6997.974609\n",
      "Train Epoch: 280 [24832/225000 (11%)] Loss: 6993.636719\n",
      "Train Epoch: 280 [28928/225000 (13%)] Loss: 6926.296875\n",
      "Train Epoch: 280 [33024/225000 (15%)] Loss: 7043.716797\n",
      "Train Epoch: 280 [37120/225000 (16%)] Loss: 6985.384766\n",
      "Train Epoch: 280 [41216/225000 (18%)] Loss: 6817.103516\n",
      "Train Epoch: 280 [45312/225000 (20%)] Loss: 6932.992188\n",
      "Train Epoch: 280 [49408/225000 (22%)] Loss: 6995.978516\n",
      "Train Epoch: 280 [53504/225000 (24%)] Loss: 6883.199219\n",
      "Train Epoch: 280 [57600/225000 (26%)] Loss: 7184.531250\n",
      "Train Epoch: 280 [61696/225000 (27%)] Loss: 6922.371094\n",
      "Train Epoch: 280 [65792/225000 (29%)] Loss: 7066.318359\n",
      "Train Epoch: 280 [69888/225000 (31%)] Loss: 6969.056641\n",
      "Train Epoch: 280 [73984/225000 (33%)] Loss: 6983.716797\n",
      "Train Epoch: 280 [78080/225000 (35%)] Loss: 6970.697266\n",
      "Train Epoch: 280 [82176/225000 (37%)] Loss: 6945.638672\n",
      "Train Epoch: 280 [86272/225000 (38%)] Loss: 6897.802734\n",
      "Train Epoch: 280 [90368/225000 (40%)] Loss: 6874.757812\n",
      "Train Epoch: 280 [94464/225000 (42%)] Loss: 6806.044922\n",
      "Train Epoch: 280 [98560/225000 (44%)] Loss: 6925.505859\n",
      "Train Epoch: 280 [102656/225000 (46%)] Loss: 6958.410156\n",
      "Train Epoch: 280 [106752/225000 (47%)] Loss: 6874.660156\n",
      "Train Epoch: 280 [110848/225000 (49%)] Loss: 6906.236328\n",
      "Train Epoch: 280 [114944/225000 (51%)] Loss: 7127.242188\n",
      "Train Epoch: 280 [119040/225000 (53%)] Loss: 6987.367188\n",
      "Train Epoch: 280 [123136/225000 (55%)] Loss: 7022.298828\n",
      "Train Epoch: 280 [127232/225000 (57%)] Loss: 7024.322266\n",
      "Train Epoch: 280 [131328/225000 (58%)] Loss: 7012.728516\n",
      "Train Epoch: 280 [135424/225000 (60%)] Loss: 6926.064453\n",
      "Train Epoch: 280 [139520/225000 (62%)] Loss: 6999.289062\n",
      "Train Epoch: 280 [143616/225000 (64%)] Loss: 7021.078125\n",
      "Train Epoch: 280 [147712/225000 (66%)] Loss: 7179.013672\n",
      "Train Epoch: 280 [151808/225000 (67%)] Loss: 7082.962891\n",
      "Train Epoch: 280 [155904/225000 (69%)] Loss: 6986.500000\n",
      "Train Epoch: 280 [160000/225000 (71%)] Loss: 6982.095703\n",
      "Train Epoch: 280 [164096/225000 (73%)] Loss: 6909.361328\n",
      "Train Epoch: 280 [168192/225000 (75%)] Loss: 6949.589844\n",
      "Train Epoch: 280 [172288/225000 (77%)] Loss: 6954.074219\n",
      "Train Epoch: 280 [176384/225000 (78%)] Loss: 6882.974609\n",
      "Train Epoch: 280 [180480/225000 (80%)] Loss: 6855.187500\n",
      "Train Epoch: 280 [184576/225000 (82%)] Loss: 7043.191406\n",
      "Train Epoch: 280 [188672/225000 (84%)] Loss: 6835.033203\n",
      "Train Epoch: 280 [192768/225000 (86%)] Loss: 7197.238281\n",
      "Train Epoch: 280 [196864/225000 (87%)] Loss: 6969.818359\n",
      "Train Epoch: 280 [200960/225000 (89%)] Loss: 6933.648438\n",
      "Train Epoch: 280 [205056/225000 (91%)] Loss: 6935.199219\n",
      "Train Epoch: 280 [209152/225000 (93%)] Loss: 6886.851562\n",
      "Train Epoch: 280 [213248/225000 (95%)] Loss: 7025.658203\n",
      "Train Epoch: 280 [217344/225000 (97%)] Loss: 6938.281250\n",
      "Train Epoch: 280 [221440/225000 (98%)] Loss: 6885.361328\n",
      "    epoch          : 280\n",
      "    loss           : 6993.6081517971415\n",
      "    val_loss       : 7068.177120955623\n",
      "Train Epoch: 281 [256/225000 (0%)] Loss: 7007.886719\n",
      "Train Epoch: 281 [4352/225000 (2%)] Loss: 6856.482422\n",
      "Train Epoch: 281 [8448/225000 (4%)] Loss: 7099.898438\n",
      "Train Epoch: 281 [12544/225000 (6%)] Loss: 6937.392578\n",
      "Train Epoch: 281 [16640/225000 (7%)] Loss: 6941.521484\n",
      "Train Epoch: 281 [20736/225000 (9%)] Loss: 6856.509766\n",
      "Train Epoch: 281 [24832/225000 (11%)] Loss: 6948.746094\n",
      "Train Epoch: 281 [28928/225000 (13%)] Loss: 6954.357422\n",
      "Train Epoch: 281 [33024/225000 (15%)] Loss: 7116.638672\n",
      "Train Epoch: 281 [37120/225000 (16%)] Loss: 6895.542969\n",
      "Train Epoch: 281 [41216/225000 (18%)] Loss: 6948.855469\n",
      "Train Epoch: 281 [45312/225000 (20%)] Loss: 6986.263672\n",
      "Train Epoch: 281 [49408/225000 (22%)] Loss: 6934.974609\n",
      "Train Epoch: 281 [53504/225000 (24%)] Loss: 7112.714844\n",
      "Train Epoch: 281 [57600/225000 (26%)] Loss: 7006.154297\n",
      "Train Epoch: 281 [61696/225000 (27%)] Loss: 7095.333984\n",
      "Train Epoch: 281 [65792/225000 (29%)] Loss: 6951.099609\n",
      "Train Epoch: 281 [69888/225000 (31%)] Loss: 7005.031250\n",
      "Train Epoch: 281 [73984/225000 (33%)] Loss: 7015.242188\n",
      "Train Epoch: 281 [78080/225000 (35%)] Loss: 6803.425781\n",
      "Train Epoch: 281 [82176/225000 (37%)] Loss: 7025.816406\n",
      "Train Epoch: 281 [86272/225000 (38%)] Loss: 6948.482422\n",
      "Train Epoch: 281 [90368/225000 (40%)] Loss: 7064.412109\n",
      "Train Epoch: 281 [94464/225000 (42%)] Loss: 7238.804688\n",
      "Train Epoch: 281 [98560/225000 (44%)] Loss: 6897.306641\n",
      "Train Epoch: 281 [102656/225000 (46%)] Loss: 7154.779297\n",
      "Train Epoch: 281 [106752/225000 (47%)] Loss: 7019.214844\n",
      "Train Epoch: 281 [110848/225000 (49%)] Loss: 6977.283203\n",
      "Train Epoch: 281 [114944/225000 (51%)] Loss: 7186.626953\n",
      "Train Epoch: 281 [119040/225000 (53%)] Loss: 6935.121094\n",
      "Train Epoch: 281 [123136/225000 (55%)] Loss: 6818.375000\n",
      "Train Epoch: 281 [127232/225000 (57%)] Loss: 6997.718750\n",
      "Train Epoch: 281 [131328/225000 (58%)] Loss: 7008.117188\n",
      "Train Epoch: 281 [135424/225000 (60%)] Loss: 7061.076172\n",
      "Train Epoch: 281 [139520/225000 (62%)] Loss: 6989.943359\n",
      "Train Epoch: 281 [143616/225000 (64%)] Loss: 6893.921875\n",
      "Train Epoch: 281 [147712/225000 (66%)] Loss: 7040.666016\n",
      "Train Epoch: 281 [151808/225000 (67%)] Loss: 6989.603516\n",
      "Train Epoch: 281 [155904/225000 (69%)] Loss: 6895.923828\n",
      "Train Epoch: 281 [160000/225000 (71%)] Loss: 6892.347656\n",
      "Train Epoch: 281 [164096/225000 (73%)] Loss: 6872.833984\n",
      "Train Epoch: 281 [168192/225000 (75%)] Loss: 7050.904297\n",
      "Train Epoch: 281 [172288/225000 (77%)] Loss: 7073.917969\n",
      "Train Epoch: 281 [176384/225000 (78%)] Loss: 7008.820312\n",
      "Train Epoch: 281 [180480/225000 (80%)] Loss: 6904.755859\n",
      "Train Epoch: 281 [184576/225000 (82%)] Loss: 7011.724609\n",
      "Train Epoch: 281 [188672/225000 (84%)] Loss: 7042.097656\n",
      "Train Epoch: 281 [192768/225000 (86%)] Loss: 6938.531250\n",
      "Train Epoch: 281 [196864/225000 (87%)] Loss: 6994.058594\n",
      "Train Epoch: 281 [200960/225000 (89%)] Loss: 7148.201172\n",
      "Train Epoch: 281 [205056/225000 (91%)] Loss: 6983.400391\n",
      "Train Epoch: 281 [209152/225000 (93%)] Loss: 7012.949219\n",
      "Train Epoch: 281 [213248/225000 (95%)] Loss: 6946.275391\n",
      "Train Epoch: 281 [217344/225000 (97%)] Loss: 6920.515625\n",
      "Train Epoch: 281 [221440/225000 (98%)] Loss: 6998.802734\n",
      "    epoch          : 281\n",
      "    loss           : 7000.125223309514\n",
      "    val_loss       : 6992.554189935023\n",
      "Train Epoch: 282 [256/225000 (0%)] Loss: 7050.214844\n",
      "Train Epoch: 282 [4352/225000 (2%)] Loss: 7027.009766\n",
      "Train Epoch: 282 [8448/225000 (4%)] Loss: 6987.218750\n",
      "Train Epoch: 282 [12544/225000 (6%)] Loss: 6869.298828\n",
      "Train Epoch: 282 [16640/225000 (7%)] Loss: 7021.156250\n",
      "Train Epoch: 282 [20736/225000 (9%)] Loss: 7163.832031\n",
      "Train Epoch: 282 [24832/225000 (11%)] Loss: 7047.234375\n",
      "Train Epoch: 282 [28928/225000 (13%)] Loss: 7005.490234\n",
      "Train Epoch: 282 [33024/225000 (15%)] Loss: 6947.558594\n",
      "Train Epoch: 282 [37120/225000 (16%)] Loss: 6835.412109\n",
      "Train Epoch: 282 [41216/225000 (18%)] Loss: 6969.593750\n",
      "Train Epoch: 282 [45312/225000 (20%)] Loss: 6954.248047\n",
      "Train Epoch: 282 [49408/225000 (22%)] Loss: 6998.216797\n",
      "Train Epoch: 282 [53504/225000 (24%)] Loss: 7083.136719\n",
      "Train Epoch: 282 [57600/225000 (26%)] Loss: 6837.781250\n",
      "Train Epoch: 282 [61696/225000 (27%)] Loss: 7056.349609\n",
      "Train Epoch: 282 [65792/225000 (29%)] Loss: 6994.535156\n",
      "Train Epoch: 282 [69888/225000 (31%)] Loss: 7056.042969\n",
      "Train Epoch: 282 [73984/225000 (33%)] Loss: 6973.162109\n",
      "Train Epoch: 282 [78080/225000 (35%)] Loss: 6941.917969\n",
      "Train Epoch: 282 [82176/225000 (37%)] Loss: 6810.722656\n",
      "Train Epoch: 282 [86272/225000 (38%)] Loss: 7084.109375\n",
      "Train Epoch: 282 [90368/225000 (40%)] Loss: 7003.175781\n",
      "Train Epoch: 282 [94464/225000 (42%)] Loss: 6923.833984\n",
      "Train Epoch: 282 [98560/225000 (44%)] Loss: 6991.921875\n",
      "Train Epoch: 282 [102656/225000 (46%)] Loss: 7044.595703\n",
      "Train Epoch: 282 [106752/225000 (47%)] Loss: 6799.912109\n",
      "Train Epoch: 282 [110848/225000 (49%)] Loss: 7112.066406\n",
      "Train Epoch: 282 [114944/225000 (51%)] Loss: 6974.203125\n",
      "Train Epoch: 282 [119040/225000 (53%)] Loss: 6969.177734\n",
      "Train Epoch: 282 [123136/225000 (55%)] Loss: 6982.080078\n",
      "Train Epoch: 282 [127232/225000 (57%)] Loss: 6896.123047\n",
      "Train Epoch: 282 [131328/225000 (58%)] Loss: 6936.863281\n",
      "Train Epoch: 282 [135424/225000 (60%)] Loss: 7063.126953\n",
      "Train Epoch: 282 [139520/225000 (62%)] Loss: 7026.136719\n",
      "Train Epoch: 282 [143616/225000 (64%)] Loss: 7102.263672\n",
      "Train Epoch: 282 [147712/225000 (66%)] Loss: 7100.623047\n",
      "Train Epoch: 282 [151808/225000 (67%)] Loss: 6842.439453\n",
      "Train Epoch: 282 [155904/225000 (69%)] Loss: 7084.765625\n",
      "Train Epoch: 282 [160000/225000 (71%)] Loss: 7014.250000\n",
      "Train Epoch: 282 [164096/225000 (73%)] Loss: 7130.355469\n",
      "Train Epoch: 282 [168192/225000 (75%)] Loss: 6978.402344\n",
      "Train Epoch: 282 [172288/225000 (77%)] Loss: 6888.951172\n",
      "Train Epoch: 282 [176384/225000 (78%)] Loss: 6858.945312\n",
      "Train Epoch: 282 [180480/225000 (80%)] Loss: 6968.253906\n",
      "Train Epoch: 282 [184576/225000 (82%)] Loss: 6928.160156\n",
      "Train Epoch: 282 [188672/225000 (84%)] Loss: 6900.257812\n",
      "Train Epoch: 282 [192768/225000 (86%)] Loss: 6866.810547\n",
      "Train Epoch: 282 [196864/225000 (87%)] Loss: 7108.738281\n",
      "Train Epoch: 282 [200960/225000 (89%)] Loss: 6875.013672\n",
      "Train Epoch: 282 [205056/225000 (91%)] Loss: 6922.027344\n",
      "Train Epoch: 282 [209152/225000 (93%)] Loss: 6996.216797\n",
      "Train Epoch: 282 [213248/225000 (95%)] Loss: 7051.345703\n",
      "Train Epoch: 282 [217344/225000 (97%)] Loss: 7224.992188\n",
      "Train Epoch: 282 [221440/225000 (98%)] Loss: 6873.070312\n",
      "    epoch          : 282\n",
      "    loss           : 6980.963680540743\n",
      "    val_loss       : 6995.022174664906\n",
      "Train Epoch: 283 [256/225000 (0%)] Loss: 6679.755859\n",
      "Train Epoch: 283 [4352/225000 (2%)] Loss: 7107.134766\n",
      "Train Epoch: 283 [8448/225000 (4%)] Loss: 6975.367188\n",
      "Train Epoch: 283 [12544/225000 (6%)] Loss: 6982.152344\n",
      "Train Epoch: 283 [16640/225000 (7%)] Loss: 7078.904297\n",
      "Train Epoch: 283 [20736/225000 (9%)] Loss: 6966.425781\n",
      "Train Epoch: 283 [24832/225000 (11%)] Loss: 7058.007812\n",
      "Train Epoch: 283 [28928/225000 (13%)] Loss: 7014.144531\n",
      "Train Epoch: 283 [33024/225000 (15%)] Loss: 6932.623047\n",
      "Train Epoch: 283 [37120/225000 (16%)] Loss: 7042.658203\n",
      "Train Epoch: 283 [41216/225000 (18%)] Loss: 7174.572266\n",
      "Train Epoch: 283 [45312/225000 (20%)] Loss: 6785.796875\n",
      "Train Epoch: 283 [49408/225000 (22%)] Loss: 7004.945312\n",
      "Train Epoch: 283 [53504/225000 (24%)] Loss: 6946.183594\n",
      "Train Epoch: 283 [57600/225000 (26%)] Loss: 7077.031250\n",
      "Train Epoch: 283 [61696/225000 (27%)] Loss: 7014.207031\n",
      "Train Epoch: 283 [65792/225000 (29%)] Loss: 6932.255859\n",
      "Train Epoch: 283 [69888/225000 (31%)] Loss: 7149.675781\n",
      "Train Epoch: 283 [73984/225000 (33%)] Loss: 6956.082031\n",
      "Train Epoch: 283 [78080/225000 (35%)] Loss: 7053.142578\n",
      "Train Epoch: 283 [82176/225000 (37%)] Loss: 7074.560547\n",
      "Train Epoch: 283 [86272/225000 (38%)] Loss: 6890.525391\n",
      "Train Epoch: 283 [90368/225000 (40%)] Loss: 6845.382812\n",
      "Train Epoch: 283 [94464/225000 (42%)] Loss: 6866.919922\n",
      "Train Epoch: 283 [98560/225000 (44%)] Loss: 6910.962891\n",
      "Train Epoch: 283 [102656/225000 (46%)] Loss: 6860.404297\n",
      "Train Epoch: 283 [106752/225000 (47%)] Loss: 6956.476562\n",
      "Train Epoch: 283 [110848/225000 (49%)] Loss: 6969.513672\n",
      "Train Epoch: 283 [114944/225000 (51%)] Loss: 6946.998047\n",
      "Train Epoch: 283 [119040/225000 (53%)] Loss: 6954.197266\n",
      "Train Epoch: 283 [123136/225000 (55%)] Loss: 6955.291016\n",
      "Train Epoch: 283 [127232/225000 (57%)] Loss: 7087.671875\n",
      "Train Epoch: 283 [131328/225000 (58%)] Loss: 6778.724609\n",
      "Train Epoch: 283 [135424/225000 (60%)] Loss: 6970.570312\n",
      "Train Epoch: 283 [139520/225000 (62%)] Loss: 7015.451172\n",
      "Train Epoch: 283 [143616/225000 (64%)] Loss: 7128.423828\n",
      "Train Epoch: 283 [147712/225000 (66%)] Loss: 6927.039062\n",
      "Train Epoch: 283 [151808/225000 (67%)] Loss: 6942.179688\n",
      "Train Epoch: 283 [155904/225000 (69%)] Loss: 6816.449219\n",
      "Train Epoch: 283 [160000/225000 (71%)] Loss: 7245.810547\n",
      "Train Epoch: 283 [164096/225000 (73%)] Loss: 7066.611328\n",
      "Train Epoch: 283 [168192/225000 (75%)] Loss: 6964.544922\n",
      "Train Epoch: 283 [172288/225000 (77%)] Loss: 7049.585938\n",
      "Train Epoch: 283 [176384/225000 (78%)] Loss: 6978.195312\n",
      "Train Epoch: 283 [180480/225000 (80%)] Loss: 6999.689453\n",
      "Train Epoch: 283 [184576/225000 (82%)] Loss: 6902.943359\n",
      "Train Epoch: 283 [188672/225000 (84%)] Loss: 6792.523438\n",
      "Train Epoch: 283 [192768/225000 (86%)] Loss: 6875.875000\n",
      "Train Epoch: 283 [196864/225000 (87%)] Loss: 6815.898438\n",
      "Train Epoch: 283 [200960/225000 (89%)] Loss: 6982.417969\n",
      "Train Epoch: 283 [205056/225000 (91%)] Loss: 7037.726562\n",
      "Train Epoch: 283 [209152/225000 (93%)] Loss: 6945.136719\n",
      "Train Epoch: 283 [213248/225000 (95%)] Loss: 7076.957031\n",
      "Train Epoch: 283 [217344/225000 (97%)] Loss: 7100.404297\n",
      "Train Epoch: 283 [221440/225000 (98%)] Loss: 6941.728516\n",
      "    epoch          : 283\n",
      "    loss           : 6987.541683331556\n",
      "    val_loss       : 7065.619223183515\n",
      "Train Epoch: 284 [256/225000 (0%)] Loss: 6980.947266\n",
      "Train Epoch: 284 [4352/225000 (2%)] Loss: 7099.134766\n",
      "Train Epoch: 284 [8448/225000 (4%)] Loss: 7011.589844\n",
      "Train Epoch: 284 [12544/225000 (6%)] Loss: 7032.962891\n",
      "Train Epoch: 284 [16640/225000 (7%)] Loss: 6949.998047\n",
      "Train Epoch: 284 [20736/225000 (9%)] Loss: 7000.416016\n",
      "Train Epoch: 284 [24832/225000 (11%)] Loss: 6963.972656\n",
      "Train Epoch: 284 [28928/225000 (13%)] Loss: 7003.708984\n",
      "Train Epoch: 284 [33024/225000 (15%)] Loss: 7016.808594\n",
      "Train Epoch: 284 [37120/225000 (16%)] Loss: 6782.072266\n",
      "Train Epoch: 284 [41216/225000 (18%)] Loss: 7085.039062\n",
      "Train Epoch: 284 [45312/225000 (20%)] Loss: 6907.400391\n",
      "Train Epoch: 284 [49408/225000 (22%)] Loss: 6862.521484\n",
      "Train Epoch: 284 [53504/225000 (24%)] Loss: 6975.337891\n",
      "Train Epoch: 284 [57600/225000 (26%)] Loss: 6918.582031\n",
      "Train Epoch: 284 [61696/225000 (27%)] Loss: 6950.386719\n",
      "Train Epoch: 284 [65792/225000 (29%)] Loss: 6908.384766\n",
      "Train Epoch: 284 [69888/225000 (31%)] Loss: 7008.380859\n",
      "Train Epoch: 284 [73984/225000 (33%)] Loss: 7018.804688\n",
      "Train Epoch: 284 [78080/225000 (35%)] Loss: 6847.570312\n",
      "Train Epoch: 284 [82176/225000 (37%)] Loss: 6939.224609\n",
      "Train Epoch: 284 [86272/225000 (38%)] Loss: 6907.433594\n",
      "Train Epoch: 284 [90368/225000 (40%)] Loss: 6966.226562\n",
      "Train Epoch: 284 [94464/225000 (42%)] Loss: 7007.902344\n",
      "Train Epoch: 284 [98560/225000 (44%)] Loss: 6963.480469\n",
      "Train Epoch: 284 [102656/225000 (46%)] Loss: 7105.732422\n",
      "Train Epoch: 284 [106752/225000 (47%)] Loss: 6838.613281\n",
      "Train Epoch: 284 [110848/225000 (49%)] Loss: 6888.607422\n",
      "Train Epoch: 284 [114944/225000 (51%)] Loss: 6829.140625\n",
      "Train Epoch: 284 [119040/225000 (53%)] Loss: 6871.933594\n",
      "Train Epoch: 284 [123136/225000 (55%)] Loss: 6957.146484\n",
      "Train Epoch: 284 [127232/225000 (57%)] Loss: 7028.429688\n",
      "Train Epoch: 284 [131328/225000 (58%)] Loss: 12367.521484\n",
      "Train Epoch: 284 [135424/225000 (60%)] Loss: 7091.142578\n",
      "Train Epoch: 284 [139520/225000 (62%)] Loss: 7025.064453\n",
      "Train Epoch: 284 [143616/225000 (64%)] Loss: 6929.310547\n",
      "Train Epoch: 284 [147712/225000 (66%)] Loss: 6879.160156\n",
      "Train Epoch: 284 [151808/225000 (67%)] Loss: 6894.585938\n",
      "Train Epoch: 284 [155904/225000 (69%)] Loss: 6885.966797\n",
      "Train Epoch: 284 [160000/225000 (71%)] Loss: 6921.894531\n",
      "Train Epoch: 284 [164096/225000 (73%)] Loss: 7105.621094\n",
      "Train Epoch: 284 [168192/225000 (75%)] Loss: 6955.156250\n",
      "Train Epoch: 284 [172288/225000 (77%)] Loss: 6831.181641\n",
      "Train Epoch: 284 [176384/225000 (78%)] Loss: 6924.214844\n",
      "Train Epoch: 284 [180480/225000 (80%)] Loss: 6900.103516\n",
      "Train Epoch: 284 [184576/225000 (82%)] Loss: 6991.220703\n",
      "Train Epoch: 284 [188672/225000 (84%)] Loss: 6989.347656\n",
      "Train Epoch: 284 [192768/225000 (86%)] Loss: 7083.177734\n",
      "Train Epoch: 284 [196864/225000 (87%)] Loss: 6969.103516\n",
      "Train Epoch: 284 [200960/225000 (89%)] Loss: 6970.500000\n",
      "Train Epoch: 284 [205056/225000 (91%)] Loss: 7003.800781\n",
      "Train Epoch: 284 [209152/225000 (93%)] Loss: 6936.576172\n",
      "Train Epoch: 284 [213248/225000 (95%)] Loss: 7223.765625\n",
      "Train Epoch: 284 [217344/225000 (97%)] Loss: 7031.796875\n",
      "Train Epoch: 284 [221440/225000 (98%)] Loss: 6965.474609\n",
      "    epoch          : 284\n",
      "    loss           : 7013.293504248436\n",
      "    val_loss       : 6986.616618117508\n",
      "Train Epoch: 285 [256/225000 (0%)] Loss: 7006.140625\n",
      "Train Epoch: 285 [4352/225000 (2%)] Loss: 7009.435547\n",
      "Train Epoch: 285 [8448/225000 (4%)] Loss: 6946.166016\n",
      "Train Epoch: 285 [12544/225000 (6%)] Loss: 6967.923828\n",
      "Train Epoch: 285 [16640/225000 (7%)] Loss: 6890.517578\n",
      "Train Epoch: 285 [20736/225000 (9%)] Loss: 6980.259766\n",
      "Train Epoch: 285 [24832/225000 (11%)] Loss: 6943.158203\n",
      "Train Epoch: 285 [28928/225000 (13%)] Loss: 7000.814453\n",
      "Train Epoch: 285 [33024/225000 (15%)] Loss: 6829.990234\n",
      "Train Epoch: 285 [37120/225000 (16%)] Loss: 6979.269531\n",
      "Train Epoch: 285 [41216/225000 (18%)] Loss: 6940.833984\n",
      "Train Epoch: 285 [45312/225000 (20%)] Loss: 7033.914062\n",
      "Train Epoch: 285 [49408/225000 (22%)] Loss: 6954.933594\n",
      "Train Epoch: 285 [53504/225000 (24%)] Loss: 7110.394531\n",
      "Train Epoch: 285 [57600/225000 (26%)] Loss: 6822.505859\n",
      "Train Epoch: 285 [61696/225000 (27%)] Loss: 7048.171875\n",
      "Train Epoch: 285 [65792/225000 (29%)] Loss: 7082.712891\n",
      "Train Epoch: 285 [69888/225000 (31%)] Loss: 6974.025391\n",
      "Train Epoch: 285 [73984/225000 (33%)] Loss: 6937.023438\n",
      "Train Epoch: 285 [78080/225000 (35%)] Loss: 6883.208984\n",
      "Train Epoch: 285 [82176/225000 (37%)] Loss: 6878.763672\n",
      "Train Epoch: 285 [86272/225000 (38%)] Loss: 6993.865234\n",
      "Train Epoch: 285 [90368/225000 (40%)] Loss: 6997.314453\n",
      "Train Epoch: 285 [94464/225000 (42%)] Loss: 6967.376953\n",
      "Train Epoch: 285 [98560/225000 (44%)] Loss: 6935.039062\n",
      "Train Epoch: 285 [102656/225000 (46%)] Loss: 6852.353516\n",
      "Train Epoch: 285 [106752/225000 (47%)] Loss: 6803.798828\n",
      "Train Epoch: 285 [110848/225000 (49%)] Loss: 6875.962891\n",
      "Train Epoch: 285 [114944/225000 (51%)] Loss: 7076.099609\n",
      "Train Epoch: 285 [119040/225000 (53%)] Loss: 7075.822266\n",
      "Train Epoch: 285 [123136/225000 (55%)] Loss: 7063.283203\n",
      "Train Epoch: 285 [127232/225000 (57%)] Loss: 7131.378906\n",
      "Train Epoch: 285 [131328/225000 (58%)] Loss: 7017.527344\n",
      "Train Epoch: 285 [135424/225000 (60%)] Loss: 6970.105469\n",
      "Train Epoch: 285 [139520/225000 (62%)] Loss: 6904.335938\n",
      "Train Epoch: 285 [143616/225000 (64%)] Loss: 7085.203125\n",
      "Train Epoch: 285 [147712/225000 (66%)] Loss: 6941.107422\n",
      "Train Epoch: 285 [151808/225000 (67%)] Loss: 6941.628906\n",
      "Train Epoch: 285 [155904/225000 (69%)] Loss: 6887.101562\n",
      "Train Epoch: 285 [160000/225000 (71%)] Loss: 7072.300781\n",
      "Train Epoch: 285 [164096/225000 (73%)] Loss: 6984.638672\n",
      "Train Epoch: 285 [168192/225000 (75%)] Loss: 6939.806641\n",
      "Train Epoch: 285 [172288/225000 (77%)] Loss: 6920.375000\n",
      "Train Epoch: 285 [176384/225000 (78%)] Loss: 6944.107422\n",
      "Train Epoch: 285 [180480/225000 (80%)] Loss: 6868.105469\n",
      "Train Epoch: 285 [184576/225000 (82%)] Loss: 7073.308594\n",
      "Train Epoch: 285 [188672/225000 (84%)] Loss: 6815.175781\n",
      "Train Epoch: 285 [192768/225000 (86%)] Loss: 6893.919922\n",
      "Train Epoch: 285 [196864/225000 (87%)] Loss: 6924.203125\n",
      "Train Epoch: 285 [200960/225000 (89%)] Loss: 7008.263672\n",
      "Train Epoch: 285 [205056/225000 (91%)] Loss: 6893.517578\n",
      "Train Epoch: 285 [209152/225000 (93%)] Loss: 7119.117188\n",
      "Train Epoch: 285 [213248/225000 (95%)] Loss: 7163.542969\n",
      "Train Epoch: 285 [217344/225000 (97%)] Loss: 6865.119141\n",
      "Train Epoch: 285 [221440/225000 (98%)] Loss: 6926.126953\n",
      "    epoch          : 285\n",
      "    loss           : 6994.804076454067\n",
      "    val_loss       : 6990.769176011183\n",
      "Train Epoch: 286 [256/225000 (0%)] Loss: 6785.419922\n",
      "Train Epoch: 286 [4352/225000 (2%)] Loss: 6952.279297\n",
      "Train Epoch: 286 [8448/225000 (4%)] Loss: 6832.271484\n",
      "Train Epoch: 286 [12544/225000 (6%)] Loss: 6991.763672\n",
      "Train Epoch: 286 [16640/225000 (7%)] Loss: 7018.312500\n",
      "Train Epoch: 286 [20736/225000 (9%)] Loss: 6983.117188\n",
      "Train Epoch: 286 [24832/225000 (11%)] Loss: 7074.078125\n",
      "Train Epoch: 286 [28928/225000 (13%)] Loss: 6930.371094\n",
      "Train Epoch: 286 [33024/225000 (15%)] Loss: 7059.777344\n",
      "Train Epoch: 286 [37120/225000 (16%)] Loss: 7039.939453\n",
      "Train Epoch: 286 [41216/225000 (18%)] Loss: 7114.558594\n",
      "Train Epoch: 286 [45312/225000 (20%)] Loss: 6882.292969\n",
      "Train Epoch: 286 [49408/225000 (22%)] Loss: 6746.658203\n",
      "Train Epoch: 286 [53504/225000 (24%)] Loss: 6933.175781\n",
      "Train Epoch: 286 [57600/225000 (26%)] Loss: 7230.173828\n",
      "Train Epoch: 286 [61696/225000 (27%)] Loss: 7021.832031\n",
      "Train Epoch: 286 [65792/225000 (29%)] Loss: 7000.425781\n",
      "Train Epoch: 286 [69888/225000 (31%)] Loss: 6901.384766\n",
      "Train Epoch: 286 [73984/225000 (33%)] Loss: 7006.376953\n",
      "Train Epoch: 286 [78080/225000 (35%)] Loss: 7034.431641\n",
      "Train Epoch: 286 [82176/225000 (37%)] Loss: 6797.101562\n",
      "Train Epoch: 286 [86272/225000 (38%)] Loss: 7072.261719\n",
      "Train Epoch: 286 [90368/225000 (40%)] Loss: 7124.068359\n",
      "Train Epoch: 286 [94464/225000 (42%)] Loss: 6882.886719\n",
      "Train Epoch: 286 [98560/225000 (44%)] Loss: 7055.427734\n",
      "Train Epoch: 286 [102656/225000 (46%)] Loss: 7095.669922\n",
      "Train Epoch: 286 [106752/225000 (47%)] Loss: 6984.703125\n",
      "Train Epoch: 286 [110848/225000 (49%)] Loss: 7010.875000\n",
      "Train Epoch: 286 [114944/225000 (51%)] Loss: 7072.994141\n",
      "Train Epoch: 286 [119040/225000 (53%)] Loss: 7155.939453\n",
      "Train Epoch: 286 [123136/225000 (55%)] Loss: 7054.841797\n",
      "Train Epoch: 286 [127232/225000 (57%)] Loss: 6940.703125\n",
      "Train Epoch: 286 [131328/225000 (58%)] Loss: 6876.841797\n",
      "Train Epoch: 286 [135424/225000 (60%)] Loss: 7154.427734\n",
      "Train Epoch: 286 [139520/225000 (62%)] Loss: 6982.304688\n",
      "Train Epoch: 286 [143616/225000 (64%)] Loss: 6948.060547\n",
      "Train Epoch: 286 [147712/225000 (66%)] Loss: 6993.292969\n",
      "Train Epoch: 286 [151808/225000 (67%)] Loss: 6972.648438\n",
      "Train Epoch: 286 [155904/225000 (69%)] Loss: 7154.988281\n",
      "Train Epoch: 286 [160000/225000 (71%)] Loss: 7229.591797\n",
      "Train Epoch: 286 [164096/225000 (73%)] Loss: 6816.304688\n",
      "Train Epoch: 286 [168192/225000 (75%)] Loss: 7004.642578\n",
      "Train Epoch: 286 [172288/225000 (77%)] Loss: 6991.666016\n",
      "Train Epoch: 286 [176384/225000 (78%)] Loss: 6804.808594\n",
      "Train Epoch: 286 [180480/225000 (80%)] Loss: 6916.255859\n",
      "Train Epoch: 286 [184576/225000 (82%)] Loss: 6953.037109\n",
      "Train Epoch: 286 [188672/225000 (84%)] Loss: 6924.000000\n",
      "Train Epoch: 286 [192768/225000 (86%)] Loss: 6988.000000\n",
      "Train Epoch: 286 [196864/225000 (87%)] Loss: 7140.251953\n",
      "Train Epoch: 286 [200960/225000 (89%)] Loss: 6881.566406\n",
      "Train Epoch: 286 [205056/225000 (91%)] Loss: 7045.263672\n",
      "Train Epoch: 286 [209152/225000 (93%)] Loss: 6940.847656\n",
      "Train Epoch: 286 [213248/225000 (95%)] Loss: 7064.357422\n",
      "Train Epoch: 286 [217344/225000 (97%)] Loss: 7000.625000\n",
      "Train Epoch: 286 [221440/225000 (98%)] Loss: 6960.917969\n",
      "    epoch          : 286\n",
      "    loss           : 7024.871479264434\n",
      "    val_loss       : 6981.255235971236\n",
      "Train Epoch: 287 [256/225000 (0%)] Loss: 6900.765625\n",
      "Train Epoch: 287 [4352/225000 (2%)] Loss: 6911.683594\n",
      "Train Epoch: 287 [8448/225000 (4%)] Loss: 6788.017578\n",
      "Train Epoch: 287 [12544/225000 (6%)] Loss: 7179.447266\n",
      "Train Epoch: 287 [16640/225000 (7%)] Loss: 6990.933594\n",
      "Train Epoch: 287 [20736/225000 (9%)] Loss: 6987.888672\n",
      "Train Epoch: 287 [24832/225000 (11%)] Loss: 7066.636719\n",
      "Train Epoch: 287 [28928/225000 (13%)] Loss: 7052.783203\n",
      "Train Epoch: 287 [33024/225000 (15%)] Loss: 6775.554688\n",
      "Train Epoch: 287 [37120/225000 (16%)] Loss: 7158.726562\n",
      "Train Epoch: 287 [41216/225000 (18%)] Loss: 6993.533203\n",
      "Train Epoch: 287 [45312/225000 (20%)] Loss: 6975.376953\n",
      "Train Epoch: 287 [49408/225000 (22%)] Loss: 7078.724609\n",
      "Train Epoch: 287 [53504/225000 (24%)] Loss: 6910.646484\n",
      "Train Epoch: 287 [57600/225000 (26%)] Loss: 6931.404297\n",
      "Train Epoch: 287 [61696/225000 (27%)] Loss: 7192.339844\n",
      "Train Epoch: 287 [65792/225000 (29%)] Loss: 6987.498047\n",
      "Train Epoch: 287 [69888/225000 (31%)] Loss: 7073.630859\n",
      "Train Epoch: 287 [73984/225000 (33%)] Loss: 7142.855469\n",
      "Train Epoch: 287 [78080/225000 (35%)] Loss: 6841.578125\n",
      "Train Epoch: 287 [82176/225000 (37%)] Loss: 6881.222656\n",
      "Train Epoch: 287 [86272/225000 (38%)] Loss: 7046.542969\n",
      "Train Epoch: 287 [90368/225000 (40%)] Loss: 6904.929688\n",
      "Train Epoch: 287 [94464/225000 (42%)] Loss: 7090.953125\n",
      "Train Epoch: 287 [98560/225000 (44%)] Loss: 7048.289062\n",
      "Train Epoch: 287 [102656/225000 (46%)] Loss: 6904.638672\n",
      "Train Epoch: 287 [106752/225000 (47%)] Loss: 6986.341797\n",
      "Train Epoch: 287 [110848/225000 (49%)] Loss: 6915.738281\n",
      "Train Epoch: 287 [114944/225000 (51%)] Loss: 6903.886719\n",
      "Train Epoch: 287 [119040/225000 (53%)] Loss: 7033.724609\n",
      "Train Epoch: 287 [123136/225000 (55%)] Loss: 6960.410156\n",
      "Train Epoch: 287 [127232/225000 (57%)] Loss: 6965.601562\n",
      "Train Epoch: 287 [131328/225000 (58%)] Loss: 7012.978516\n",
      "Train Epoch: 287 [135424/225000 (60%)] Loss: 6989.886719\n",
      "Train Epoch: 287 [139520/225000 (62%)] Loss: 6884.222656\n",
      "Train Epoch: 287 [143616/225000 (64%)] Loss: 7045.070312\n",
      "Train Epoch: 287 [147712/225000 (66%)] Loss: 6906.410156\n",
      "Train Epoch: 287 [151808/225000 (67%)] Loss: 6906.160156\n",
      "Train Epoch: 287 [155904/225000 (69%)] Loss: 6956.285156\n",
      "Train Epoch: 287 [160000/225000 (71%)] Loss: 7040.033203\n",
      "Train Epoch: 287 [164096/225000 (73%)] Loss: 6850.419922\n",
      "Train Epoch: 287 [168192/225000 (75%)] Loss: 6998.929688\n",
      "Train Epoch: 287 [172288/225000 (77%)] Loss: 6988.837891\n",
      "Train Epoch: 287 [176384/225000 (78%)] Loss: 6975.623047\n",
      "Train Epoch: 287 [180480/225000 (80%)] Loss: 7002.587891\n",
      "Train Epoch: 287 [184576/225000 (82%)] Loss: 6933.041016\n",
      "Train Epoch: 287 [188672/225000 (84%)] Loss: 6856.707031\n",
      "Train Epoch: 287 [192768/225000 (86%)] Loss: 6986.337891\n",
      "Train Epoch: 287 [196864/225000 (87%)] Loss: 6984.203125\n",
      "Train Epoch: 287 [200960/225000 (89%)] Loss: 7167.046875\n",
      "Train Epoch: 287 [205056/225000 (91%)] Loss: 7177.994141\n",
      "Train Epoch: 287 [209152/225000 (93%)] Loss: 6962.193359\n",
      "Train Epoch: 287 [213248/225000 (95%)] Loss: 6878.914062\n",
      "Train Epoch: 287 [217344/225000 (97%)] Loss: 6888.925781\n",
      "Train Epoch: 287 [221440/225000 (98%)] Loss: 7028.287109\n",
      "    epoch          : 287\n",
      "    loss           : 6989.630219443259\n",
      "    val_loss       : 6978.26203726749\n",
      "Train Epoch: 288 [256/225000 (0%)] Loss: 7078.593750\n",
      "Train Epoch: 288 [4352/225000 (2%)] Loss: 7048.845703\n",
      "Train Epoch: 288 [8448/225000 (4%)] Loss: 6917.800781\n",
      "Train Epoch: 288 [12544/225000 (6%)] Loss: 6879.603516\n",
      "Train Epoch: 288 [16640/225000 (7%)] Loss: 6936.199219\n",
      "Train Epoch: 288 [20736/225000 (9%)] Loss: 6938.652344\n",
      "Train Epoch: 288 [24832/225000 (11%)] Loss: 7115.376953\n",
      "Train Epoch: 288 [28928/225000 (13%)] Loss: 6877.210938\n",
      "Train Epoch: 288 [33024/225000 (15%)] Loss: 6859.384766\n",
      "Train Epoch: 288 [37120/225000 (16%)] Loss: 6839.435547\n",
      "Train Epoch: 288 [41216/225000 (18%)] Loss: 6834.437500\n",
      "Train Epoch: 288 [45312/225000 (20%)] Loss: 6908.710938\n",
      "Train Epoch: 288 [49408/225000 (22%)] Loss: 6972.572266\n",
      "Train Epoch: 288 [53504/225000 (24%)] Loss: 7057.509766\n",
      "Train Epoch: 288 [57600/225000 (26%)] Loss: 7069.275391\n",
      "Train Epoch: 288 [61696/225000 (27%)] Loss: 7052.570312\n",
      "Train Epoch: 288 [65792/225000 (29%)] Loss: 7080.728516\n",
      "Train Epoch: 288 [69888/225000 (31%)] Loss: 6887.554688\n",
      "Train Epoch: 288 [73984/225000 (33%)] Loss: 7056.021484\n",
      "Train Epoch: 288 [78080/225000 (35%)] Loss: 7142.994141\n",
      "Train Epoch: 288 [82176/225000 (37%)] Loss: 6914.642578\n",
      "Train Epoch: 288 [86272/225000 (38%)] Loss: 7072.947266\n",
      "Train Epoch: 288 [90368/225000 (40%)] Loss: 7024.648438\n",
      "Train Epoch: 288 [94464/225000 (42%)] Loss: 7053.128906\n",
      "Train Epoch: 288 [98560/225000 (44%)] Loss: 6952.666016\n",
      "Train Epoch: 288 [102656/225000 (46%)] Loss: 7054.449219\n",
      "Train Epoch: 288 [106752/225000 (47%)] Loss: 7059.554688\n",
      "Train Epoch: 288 [110848/225000 (49%)] Loss: 6839.312500\n",
      "Train Epoch: 288 [114944/225000 (51%)] Loss: 6947.126953\n",
      "Train Epoch: 288 [119040/225000 (53%)] Loss: 6973.414062\n",
      "Train Epoch: 288 [123136/225000 (55%)] Loss: 7036.865234\n",
      "Train Epoch: 288 [127232/225000 (57%)] Loss: 6838.779297\n",
      "Train Epoch: 288 [131328/225000 (58%)] Loss: 6986.919922\n",
      "Train Epoch: 288 [135424/225000 (60%)] Loss: 7045.529297\n",
      "Train Epoch: 288 [139520/225000 (62%)] Loss: 6953.613281\n",
      "Train Epoch: 288 [143616/225000 (64%)] Loss: 6887.539062\n",
      "Train Epoch: 288 [147712/225000 (66%)] Loss: 6930.531250\n",
      "Train Epoch: 288 [151808/225000 (67%)] Loss: 6925.330078\n",
      "Train Epoch: 288 [155904/225000 (69%)] Loss: 6960.113281\n",
      "Train Epoch: 288 [160000/225000 (71%)] Loss: 6999.320312\n",
      "Train Epoch: 288 [164096/225000 (73%)] Loss: 6986.849609\n",
      "Train Epoch: 288 [168192/225000 (75%)] Loss: 6872.843750\n",
      "Train Epoch: 288 [172288/225000 (77%)] Loss: 7025.875000\n",
      "Train Epoch: 288 [176384/225000 (78%)] Loss: 6952.341797\n",
      "Train Epoch: 288 [180480/225000 (80%)] Loss: 6971.037109\n",
      "Train Epoch: 288 [184576/225000 (82%)] Loss: 6970.531250\n",
      "Train Epoch: 288 [188672/225000 (84%)] Loss: 6900.730469\n",
      "Train Epoch: 288 [192768/225000 (86%)] Loss: 6969.353516\n",
      "Train Epoch: 288 [196864/225000 (87%)] Loss: 6989.224609\n",
      "Train Epoch: 288 [200960/225000 (89%)] Loss: 7064.296875\n",
      "Train Epoch: 288 [205056/225000 (91%)] Loss: 6984.828125\n",
      "Train Epoch: 288 [209152/225000 (93%)] Loss: 6970.480469\n",
      "Train Epoch: 288 [213248/225000 (95%)] Loss: 6942.556641\n",
      "Train Epoch: 288 [217344/225000 (97%)] Loss: 6820.890625\n",
      "Train Epoch: 288 [221440/225000 (98%)] Loss: 6928.035156\n",
      "    epoch          : 288\n",
      "    loss           : 6995.547686024602\n",
      "    val_loss       : 6976.210540391961\n",
      "Train Epoch: 289 [256/225000 (0%)] Loss: 6927.818359\n",
      "Train Epoch: 289 [4352/225000 (2%)] Loss: 7136.705078\n",
      "Train Epoch: 289 [8448/225000 (4%)] Loss: 6993.507812\n",
      "Train Epoch: 289 [12544/225000 (6%)] Loss: 6909.324219\n",
      "Train Epoch: 289 [16640/225000 (7%)] Loss: 6972.476562\n",
      "Train Epoch: 289 [20736/225000 (9%)] Loss: 6909.476562\n",
      "Train Epoch: 289 [24832/225000 (11%)] Loss: 6834.593750\n",
      "Train Epoch: 289 [28928/225000 (13%)] Loss: 6842.691406\n",
      "Train Epoch: 289 [33024/225000 (15%)] Loss: 6863.722656\n",
      "Train Epoch: 289 [37120/225000 (16%)] Loss: 6900.820312\n",
      "Train Epoch: 289 [41216/225000 (18%)] Loss: 6955.607422\n",
      "Train Epoch: 289 [45312/225000 (20%)] Loss: 7175.554688\n",
      "Train Epoch: 289 [49408/225000 (22%)] Loss: 7042.648438\n",
      "Train Epoch: 289 [53504/225000 (24%)] Loss: 6839.558594\n",
      "Train Epoch: 289 [57600/225000 (26%)] Loss: 6877.722656\n",
      "Train Epoch: 289 [61696/225000 (27%)] Loss: 6951.738281\n",
      "Train Epoch: 289 [65792/225000 (29%)] Loss: 7048.035156\n",
      "Train Epoch: 289 [69888/225000 (31%)] Loss: 7010.599609\n",
      "Train Epoch: 289 [73984/225000 (33%)] Loss: 7082.738281\n",
      "Train Epoch: 289 [78080/225000 (35%)] Loss: 6858.011719\n",
      "Train Epoch: 289 [82176/225000 (37%)] Loss: 7011.546875\n",
      "Train Epoch: 289 [86272/225000 (38%)] Loss: 7049.750000\n",
      "Train Epoch: 289 [90368/225000 (40%)] Loss: 7033.705078\n",
      "Train Epoch: 289 [94464/225000 (42%)] Loss: 7119.546875\n",
      "Train Epoch: 289 [98560/225000 (44%)] Loss: 6848.269531\n",
      "Train Epoch: 289 [102656/225000 (46%)] Loss: 7080.207031\n",
      "Train Epoch: 289 [106752/225000 (47%)] Loss: 6946.894531\n",
      "Train Epoch: 289 [110848/225000 (49%)] Loss: 7050.250000\n",
      "Train Epoch: 289 [114944/225000 (51%)] Loss: 6922.847656\n",
      "Train Epoch: 289 [119040/225000 (53%)] Loss: 6866.673828\n",
      "Train Epoch: 289 [123136/225000 (55%)] Loss: 7089.896484\n",
      "Train Epoch: 289 [127232/225000 (57%)] Loss: 7097.871094\n",
      "Train Epoch: 289 [131328/225000 (58%)] Loss: 6898.021484\n",
      "Train Epoch: 289 [135424/225000 (60%)] Loss: 7104.085938\n",
      "Train Epoch: 289 [139520/225000 (62%)] Loss: 6934.300781\n",
      "Train Epoch: 289 [143616/225000 (64%)] Loss: 6940.816406\n",
      "Train Epoch: 289 [147712/225000 (66%)] Loss: 7126.146484\n",
      "Train Epoch: 289 [151808/225000 (67%)] Loss: 6993.320312\n",
      "Train Epoch: 289 [155904/225000 (69%)] Loss: 7059.238281\n",
      "Train Epoch: 289 [160000/225000 (71%)] Loss: 6997.773438\n",
      "Train Epoch: 289 [164096/225000 (73%)] Loss: 7061.134766\n",
      "Train Epoch: 289 [168192/225000 (75%)] Loss: 6885.056641\n",
      "Train Epoch: 289 [172288/225000 (77%)] Loss: 6984.443359\n",
      "Train Epoch: 289 [176384/225000 (78%)] Loss: 6889.632812\n",
      "Train Epoch: 289 [180480/225000 (80%)] Loss: 6826.107422\n",
      "Train Epoch: 289 [184576/225000 (82%)] Loss: 6940.281250\n",
      "Train Epoch: 289 [188672/225000 (84%)] Loss: 6991.355469\n",
      "Train Epoch: 289 [192768/225000 (86%)] Loss: 6995.710938\n",
      "Train Epoch: 289 [196864/225000 (87%)] Loss: 6831.697266\n",
      "Train Epoch: 289 [200960/225000 (89%)] Loss: 6962.423828\n",
      "Train Epoch: 289 [205056/225000 (91%)] Loss: 6841.316406\n",
      "Train Epoch: 289 [209152/225000 (93%)] Loss: 6861.736328\n",
      "Train Epoch: 289 [213248/225000 (95%)] Loss: 6984.443359\n",
      "Train Epoch: 289 [217344/225000 (97%)] Loss: 6958.708984\n",
      "Train Epoch: 289 [221440/225000 (98%)] Loss: 6877.330078\n",
      "    epoch          : 289\n",
      "    loss           : 7028.879142891425\n",
      "    val_loss       : 6981.880874356445\n",
      "Train Epoch: 290 [256/225000 (0%)] Loss: 6959.886719\n",
      "Train Epoch: 290 [4352/225000 (2%)] Loss: 7102.998047\n",
      "Train Epoch: 290 [8448/225000 (4%)] Loss: 7021.722656\n",
      "Train Epoch: 290 [12544/225000 (6%)] Loss: 6844.447266\n",
      "Train Epoch: 290 [16640/225000 (7%)] Loss: 7113.662109\n",
      "Train Epoch: 290 [20736/225000 (9%)] Loss: 7040.560547\n",
      "Train Epoch: 290 [24832/225000 (11%)] Loss: 6982.687500\n",
      "Train Epoch: 290 [28928/225000 (13%)] Loss: 6895.419922\n",
      "Train Epoch: 290 [33024/225000 (15%)] Loss: 7034.439453\n",
      "Train Epoch: 290 [37120/225000 (16%)] Loss: 6964.103516\n",
      "Train Epoch: 290 [41216/225000 (18%)] Loss: 7143.292969\n",
      "Train Epoch: 290 [45312/225000 (20%)] Loss: 7066.953125\n",
      "Train Epoch: 290 [49408/225000 (22%)] Loss: 6985.808594\n",
      "Train Epoch: 290 [53504/225000 (24%)] Loss: 6987.808594\n",
      "Train Epoch: 290 [57600/225000 (26%)] Loss: 6974.189453\n",
      "Train Epoch: 290 [61696/225000 (27%)] Loss: 7024.406250\n",
      "Train Epoch: 290 [65792/225000 (29%)] Loss: 6856.689453\n",
      "Train Epoch: 290 [69888/225000 (31%)] Loss: 6920.820312\n",
      "Train Epoch: 290 [73984/225000 (33%)] Loss: 6954.617188\n",
      "Train Epoch: 290 [78080/225000 (35%)] Loss: 6956.654297\n",
      "Train Epoch: 290 [82176/225000 (37%)] Loss: 7042.652344\n",
      "Train Epoch: 290 [86272/225000 (38%)] Loss: 6883.232422\n",
      "Train Epoch: 290 [90368/225000 (40%)] Loss: 6783.720703\n",
      "Train Epoch: 290 [94464/225000 (42%)] Loss: 6902.023438\n",
      "Train Epoch: 290 [98560/225000 (44%)] Loss: 7065.679688\n",
      "Train Epoch: 290 [102656/225000 (46%)] Loss: 7007.253906\n",
      "Train Epoch: 290 [106752/225000 (47%)] Loss: 7081.503906\n",
      "Train Epoch: 290 [110848/225000 (49%)] Loss: 7016.300781\n",
      "Train Epoch: 290 [114944/225000 (51%)] Loss: 6915.666016\n",
      "Train Epoch: 290 [119040/225000 (53%)] Loss: 7031.201172\n",
      "Train Epoch: 290 [123136/225000 (55%)] Loss: 6997.376953\n",
      "Train Epoch: 290 [127232/225000 (57%)] Loss: 7029.804688\n",
      "Train Epoch: 290 [131328/225000 (58%)] Loss: 6923.326172\n",
      "Train Epoch: 290 [135424/225000 (60%)] Loss: 7003.400391\n",
      "Train Epoch: 290 [139520/225000 (62%)] Loss: 6850.062500\n",
      "Train Epoch: 290 [143616/225000 (64%)] Loss: 7008.912109\n",
      "Train Epoch: 290 [147712/225000 (66%)] Loss: 7170.945312\n",
      "Train Epoch: 290 [151808/225000 (67%)] Loss: 6930.183594\n",
      "Train Epoch: 290 [155904/225000 (69%)] Loss: 6905.406250\n",
      "Train Epoch: 290 [160000/225000 (71%)] Loss: 6810.640625\n",
      "Train Epoch: 290 [164096/225000 (73%)] Loss: 6862.779297\n",
      "Train Epoch: 290 [168192/225000 (75%)] Loss: 7042.146484\n",
      "Train Epoch: 290 [172288/225000 (77%)] Loss: 6972.923828\n",
      "Train Epoch: 290 [176384/225000 (78%)] Loss: 7057.884766\n",
      "Train Epoch: 290 [180480/225000 (80%)] Loss: 7009.750000\n",
      "Train Epoch: 290 [184576/225000 (82%)] Loss: 7167.638672\n",
      "Train Epoch: 290 [188672/225000 (84%)] Loss: 6950.333984\n",
      "Train Epoch: 290 [192768/225000 (86%)] Loss: 7025.822266\n",
      "Train Epoch: 290 [196864/225000 (87%)] Loss: 6962.173828\n",
      "Train Epoch: 290 [200960/225000 (89%)] Loss: 6998.125000\n",
      "Train Epoch: 290 [205056/225000 (91%)] Loss: 7117.986328\n",
      "Train Epoch: 290 [209152/225000 (93%)] Loss: 6977.919922\n",
      "Train Epoch: 290 [213248/225000 (95%)] Loss: 6851.099609\n",
      "Train Epoch: 290 [217344/225000 (97%)] Loss: 6889.085938\n",
      "Train Epoch: 290 [221440/225000 (98%)] Loss: 6911.640625\n",
      "    epoch          : 290\n",
      "    loss           : 6985.3140664995735\n",
      "    val_loss       : 6977.716838780714\n",
      "Train Epoch: 291 [256/225000 (0%)] Loss: 6921.240234\n",
      "Train Epoch: 291 [4352/225000 (2%)] Loss: 6816.136719\n",
      "Train Epoch: 291 [8448/225000 (4%)] Loss: 7063.994141\n",
      "Train Epoch: 291 [12544/225000 (6%)] Loss: 6940.421875\n",
      "Train Epoch: 291 [16640/225000 (7%)] Loss: 7007.171875\n",
      "Train Epoch: 291 [20736/225000 (9%)] Loss: 6889.455078\n",
      "Train Epoch: 291 [24832/225000 (11%)] Loss: 6925.968750\n",
      "Train Epoch: 291 [28928/225000 (13%)] Loss: 6963.826172\n",
      "Train Epoch: 291 [33024/225000 (15%)] Loss: 6907.306641\n",
      "Train Epoch: 291 [37120/225000 (16%)] Loss: 6900.029297\n",
      "Train Epoch: 291 [41216/225000 (18%)] Loss: 6850.822266\n",
      "Train Epoch: 291 [45312/225000 (20%)] Loss: 7022.865234\n",
      "Train Epoch: 291 [49408/225000 (22%)] Loss: 7052.068359\n",
      "Train Epoch: 291 [53504/225000 (24%)] Loss: 6864.447266\n",
      "Train Epoch: 291 [57600/225000 (26%)] Loss: 6848.587891\n",
      "Train Epoch: 291 [61696/225000 (27%)] Loss: 6903.343750\n",
      "Train Epoch: 291 [65792/225000 (29%)] Loss: 7011.263672\n",
      "Train Epoch: 291 [69888/225000 (31%)] Loss: 7022.117188\n",
      "Train Epoch: 291 [73984/225000 (33%)] Loss: 7064.218750\n",
      "Train Epoch: 291 [78080/225000 (35%)] Loss: 6977.089844\n",
      "Train Epoch: 291 [82176/225000 (37%)] Loss: 6936.757812\n",
      "Train Epoch: 291 [86272/225000 (38%)] Loss: 6902.455078\n",
      "Train Epoch: 291 [90368/225000 (40%)] Loss: 6971.580078\n",
      "Train Epoch: 291 [94464/225000 (42%)] Loss: 6975.455078\n",
      "Train Epoch: 291 [98560/225000 (44%)] Loss: 7123.683594\n",
      "Train Epoch: 291 [102656/225000 (46%)] Loss: 7027.169922\n",
      "Train Epoch: 291 [106752/225000 (47%)] Loss: 6982.453125\n",
      "Train Epoch: 291 [110848/225000 (49%)] Loss: 7149.126953\n",
      "Train Epoch: 291 [114944/225000 (51%)] Loss: 6973.535156\n",
      "Train Epoch: 291 [119040/225000 (53%)] Loss: 6837.027344\n",
      "Train Epoch: 291 [123136/225000 (55%)] Loss: 6956.876953\n",
      "Train Epoch: 291 [127232/225000 (57%)] Loss: 6893.773438\n",
      "Train Epoch: 291 [131328/225000 (58%)] Loss: 6985.869141\n",
      "Train Epoch: 291 [135424/225000 (60%)] Loss: 6952.890625\n",
      "Train Epoch: 291 [139520/225000 (62%)] Loss: 7005.482422\n",
      "Train Epoch: 291 [143616/225000 (64%)] Loss: 6789.947266\n",
      "Train Epoch: 291 [147712/225000 (66%)] Loss: 6969.195312\n",
      "Train Epoch: 291 [151808/225000 (67%)] Loss: 6973.072266\n",
      "Train Epoch: 291 [155904/225000 (69%)] Loss: 6931.507812\n",
      "Train Epoch: 291 [160000/225000 (71%)] Loss: 6994.617188\n",
      "Train Epoch: 291 [164096/225000 (73%)] Loss: 6994.333984\n",
      "Train Epoch: 291 [168192/225000 (75%)] Loss: 6983.009766\n",
      "Train Epoch: 291 [172288/225000 (77%)] Loss: 7030.753906\n",
      "Train Epoch: 291 [176384/225000 (78%)] Loss: 6866.347656\n",
      "Train Epoch: 291 [180480/225000 (80%)] Loss: 7160.896484\n",
      "Train Epoch: 291 [184576/225000 (82%)] Loss: 6996.693359\n",
      "Train Epoch: 291 [188672/225000 (84%)] Loss: 7091.992188\n",
      "Train Epoch: 291 [192768/225000 (86%)] Loss: 6936.322266\n",
      "Train Epoch: 291 [196864/225000 (87%)] Loss: 7078.156250\n",
      "Train Epoch: 291 [200960/225000 (89%)] Loss: 6852.103516\n",
      "Train Epoch: 291 [205056/225000 (91%)] Loss: 6765.062500\n",
      "Train Epoch: 291 [209152/225000 (93%)] Loss: 6985.507812\n",
      "Train Epoch: 291 [213248/225000 (95%)] Loss: 6874.904297\n",
      "Train Epoch: 291 [217344/225000 (97%)] Loss: 7010.539062\n",
      "Train Epoch: 291 [221440/225000 (98%)] Loss: 7031.822266\n",
      "    epoch          : 291\n",
      "    loss           : 6981.602761261021\n",
      "    val_loss       : 6977.154254249164\n",
      "Train Epoch: 292 [256/225000 (0%)] Loss: 6921.970703\n",
      "Train Epoch: 292 [4352/225000 (2%)] Loss: 7037.072266\n",
      "Train Epoch: 292 [8448/225000 (4%)] Loss: 6755.947266\n",
      "Train Epoch: 292 [12544/225000 (6%)] Loss: 6855.119141\n",
      "Train Epoch: 292 [16640/225000 (7%)] Loss: 7082.392578\n",
      "Train Epoch: 292 [20736/225000 (9%)] Loss: 6957.689453\n",
      "Train Epoch: 292 [24832/225000 (11%)] Loss: 6980.416016\n",
      "Train Epoch: 292 [28928/225000 (13%)] Loss: 6914.857422\n",
      "Train Epoch: 292 [33024/225000 (15%)] Loss: 6973.042969\n",
      "Train Epoch: 292 [37120/225000 (16%)] Loss: 7052.392578\n",
      "Train Epoch: 292 [41216/225000 (18%)] Loss: 7056.142578\n",
      "Train Epoch: 292 [45312/225000 (20%)] Loss: 6944.539062\n",
      "Train Epoch: 292 [49408/225000 (22%)] Loss: 6996.765625\n",
      "Train Epoch: 292 [53504/225000 (24%)] Loss: 7094.361328\n",
      "Train Epoch: 292 [57600/225000 (26%)] Loss: 6995.791016\n",
      "Train Epoch: 292 [61696/225000 (27%)] Loss: 7110.615234\n",
      "Train Epoch: 292 [65792/225000 (29%)] Loss: 6897.468750\n",
      "Train Epoch: 292 [69888/225000 (31%)] Loss: 6923.623047\n",
      "Train Epoch: 292 [73984/225000 (33%)] Loss: 7171.884766\n",
      "Train Epoch: 292 [78080/225000 (35%)] Loss: 6849.472656\n",
      "Train Epoch: 292 [82176/225000 (37%)] Loss: 7002.611328\n",
      "Train Epoch: 292 [86272/225000 (38%)] Loss: 6960.201172\n",
      "Train Epoch: 292 [90368/225000 (40%)] Loss: 6816.757812\n",
      "Train Epoch: 292 [94464/225000 (42%)] Loss: 6926.066406\n",
      "Train Epoch: 292 [98560/225000 (44%)] Loss: 7009.664062\n",
      "Train Epoch: 292 [102656/225000 (46%)] Loss: 7040.607422\n",
      "Train Epoch: 292 [106752/225000 (47%)] Loss: 7033.611328\n",
      "Train Epoch: 292 [110848/225000 (49%)] Loss: 6998.164062\n",
      "Train Epoch: 292 [114944/225000 (51%)] Loss: 6928.207031\n",
      "Train Epoch: 292 [119040/225000 (53%)] Loss: 6889.544922\n",
      "Train Epoch: 292 [123136/225000 (55%)] Loss: 6975.445312\n",
      "Train Epoch: 292 [127232/225000 (57%)] Loss: 6981.353516\n",
      "Train Epoch: 292 [131328/225000 (58%)] Loss: 6809.634766\n",
      "Train Epoch: 292 [135424/225000 (60%)] Loss: 6886.355469\n",
      "Train Epoch: 292 [139520/225000 (62%)] Loss: 6869.212891\n",
      "Train Epoch: 292 [143616/225000 (64%)] Loss: 7009.900391\n",
      "Train Epoch: 292 [147712/225000 (66%)] Loss: 6944.994141\n",
      "Train Epoch: 292 [151808/225000 (67%)] Loss: 6971.894531\n",
      "Train Epoch: 292 [155904/225000 (69%)] Loss: 7121.347656\n",
      "Train Epoch: 292 [160000/225000 (71%)] Loss: 6976.058594\n",
      "Train Epoch: 292 [164096/225000 (73%)] Loss: 7025.751953\n",
      "Train Epoch: 292 [168192/225000 (75%)] Loss: 6928.550781\n",
      "Train Epoch: 292 [172288/225000 (77%)] Loss: 6847.048828\n",
      "Train Epoch: 292 [176384/225000 (78%)] Loss: 6965.824219\n",
      "Train Epoch: 292 [180480/225000 (80%)] Loss: 6861.736328\n",
      "Train Epoch: 292 [184576/225000 (82%)] Loss: 7009.867188\n",
      "Train Epoch: 292 [188672/225000 (84%)] Loss: 6982.072266\n",
      "Train Epoch: 292 [192768/225000 (86%)] Loss: 6865.705078\n",
      "Train Epoch: 292 [196864/225000 (87%)] Loss: 7026.169922\n",
      "Train Epoch: 292 [200960/225000 (89%)] Loss: 6794.390625\n",
      "Train Epoch: 292 [205056/225000 (91%)] Loss: 6970.986328\n",
      "Train Epoch: 292 [209152/225000 (93%)] Loss: 6934.179688\n",
      "Train Epoch: 292 [213248/225000 (95%)] Loss: 7077.677734\n",
      "Train Epoch: 292 [217344/225000 (97%)] Loss: 6969.900391\n",
      "Train Epoch: 292 [221440/225000 (98%)] Loss: 6884.222656\n",
      "    epoch          : 292\n",
      "    loss           : 6962.870484926052\n",
      "    val_loss       : 6973.438773768289\n",
      "Train Epoch: 293 [256/225000 (0%)] Loss: 6900.767578\n",
      "Train Epoch: 293 [4352/225000 (2%)] Loss: 7103.585938\n",
      "Train Epoch: 293 [8448/225000 (4%)] Loss: 6903.107422\n",
      "Train Epoch: 293 [12544/225000 (6%)] Loss: 6925.826172\n",
      "Train Epoch: 293 [16640/225000 (7%)] Loss: 6903.740234\n",
      "Train Epoch: 293 [20736/225000 (9%)] Loss: 6950.916016\n",
      "Train Epoch: 293 [24832/225000 (11%)] Loss: 6977.492188\n",
      "Train Epoch: 293 [28928/225000 (13%)] Loss: 6801.007812\n",
      "Train Epoch: 293 [33024/225000 (15%)] Loss: 6944.724609\n",
      "Train Epoch: 293 [37120/225000 (16%)] Loss: 7173.503906\n",
      "Train Epoch: 293 [41216/225000 (18%)] Loss: 7030.330078\n",
      "Train Epoch: 293 [45312/225000 (20%)] Loss: 6977.326172\n",
      "Train Epoch: 293 [49408/225000 (22%)] Loss: 6880.927734\n",
      "Train Epoch: 293 [53504/225000 (24%)] Loss: 6986.523438\n",
      "Train Epoch: 293 [57600/225000 (26%)] Loss: 6803.515625\n",
      "Train Epoch: 293 [61696/225000 (27%)] Loss: 6951.148438\n",
      "Train Epoch: 293 [65792/225000 (29%)] Loss: 6958.480469\n",
      "Train Epoch: 293 [69888/225000 (31%)] Loss: 6835.613281\n",
      "Train Epoch: 293 [73984/225000 (33%)] Loss: 7131.082031\n",
      "Train Epoch: 293 [78080/225000 (35%)] Loss: 6979.742188\n",
      "Train Epoch: 293 [82176/225000 (37%)] Loss: 6938.078125\n",
      "Train Epoch: 293 [86272/225000 (38%)] Loss: 6952.853516\n",
      "Train Epoch: 293 [90368/225000 (40%)] Loss: 7045.228516\n",
      "Train Epoch: 293 [94464/225000 (42%)] Loss: 6963.279297\n",
      "Train Epoch: 293 [98560/225000 (44%)] Loss: 6879.193359\n",
      "Train Epoch: 293 [102656/225000 (46%)] Loss: 6907.042969\n",
      "Train Epoch: 293 [106752/225000 (47%)] Loss: 7082.792969\n",
      "Train Epoch: 293 [110848/225000 (49%)] Loss: 6897.050781\n",
      "Train Epoch: 293 [114944/225000 (51%)] Loss: 7026.292969\n",
      "Train Epoch: 293 [119040/225000 (53%)] Loss: 7077.789062\n",
      "Train Epoch: 293 [123136/225000 (55%)] Loss: 7066.759766\n",
      "Train Epoch: 293 [127232/225000 (57%)] Loss: 6876.498047\n",
      "Train Epoch: 293 [131328/225000 (58%)] Loss: 7034.259766\n",
      "Train Epoch: 293 [135424/225000 (60%)] Loss: 6902.052734\n",
      "Train Epoch: 293 [139520/225000 (62%)] Loss: 7020.197266\n",
      "Train Epoch: 293 [143616/225000 (64%)] Loss: 6842.683594\n",
      "Train Epoch: 293 [147712/225000 (66%)] Loss: 6850.816406\n",
      "Train Epoch: 293 [151808/225000 (67%)] Loss: 6928.906250\n",
      "Train Epoch: 293 [155904/225000 (69%)] Loss: 7024.312500\n",
      "Train Epoch: 293 [160000/225000 (71%)] Loss: 6990.843750\n",
      "Train Epoch: 293 [164096/225000 (73%)] Loss: 6924.677734\n",
      "Train Epoch: 293 [168192/225000 (75%)] Loss: 7142.816406\n",
      "Train Epoch: 293 [172288/225000 (77%)] Loss: 6761.480469\n",
      "Train Epoch: 293 [176384/225000 (78%)] Loss: 7046.312500\n",
      "Train Epoch: 293 [180480/225000 (80%)] Loss: 6880.576172\n",
      "Train Epoch: 293 [184576/225000 (82%)] Loss: 7257.914062\n",
      "Train Epoch: 293 [188672/225000 (84%)] Loss: 7053.130859\n",
      "Train Epoch: 293 [192768/225000 (86%)] Loss: 6866.261719\n",
      "Train Epoch: 293 [196864/225000 (87%)] Loss: 6903.167969\n",
      "Train Epoch: 293 [200960/225000 (89%)] Loss: 7054.996094\n",
      "Train Epoch: 293 [205056/225000 (91%)] Loss: 6854.560547\n",
      "Train Epoch: 293 [209152/225000 (93%)] Loss: 6942.792969\n",
      "Train Epoch: 293 [213248/225000 (95%)] Loss: 6808.791016\n",
      "Train Epoch: 293 [217344/225000 (97%)] Loss: 6926.615234\n",
      "Train Epoch: 293 [221440/225000 (98%)] Loss: 7076.181641\n",
      "    epoch          : 293\n",
      "    loss           : 6961.421782787614\n",
      "    val_loss       : 6968.022315724772\n",
      "Train Epoch: 294 [256/225000 (0%)] Loss: 6819.451172\n",
      "Train Epoch: 294 [4352/225000 (2%)] Loss: 7079.595703\n",
      "Train Epoch: 294 [8448/225000 (4%)] Loss: 7044.101562\n",
      "Train Epoch: 294 [12544/225000 (6%)] Loss: 6842.535156\n",
      "Train Epoch: 294 [16640/225000 (7%)] Loss: 7099.128906\n",
      "Train Epoch: 294 [20736/225000 (9%)] Loss: 6881.621094\n",
      "Train Epoch: 294 [24832/225000 (11%)] Loss: 6842.296875\n",
      "Train Epoch: 294 [28928/225000 (13%)] Loss: 6932.419922\n",
      "Train Epoch: 294 [33024/225000 (15%)] Loss: 6953.810547\n",
      "Train Epoch: 294 [37120/225000 (16%)] Loss: 6828.783203\n",
      "Train Epoch: 294 [41216/225000 (18%)] Loss: 6941.814453\n",
      "Train Epoch: 294 [45312/225000 (20%)] Loss: 7005.744141\n",
      "Train Epoch: 294 [49408/225000 (22%)] Loss: 6939.744141\n",
      "Train Epoch: 294 [53504/225000 (24%)] Loss: 6894.513672\n",
      "Train Epoch: 294 [57600/225000 (26%)] Loss: 7076.439453\n",
      "Train Epoch: 294 [61696/225000 (27%)] Loss: 7091.972656\n",
      "Train Epoch: 294 [65792/225000 (29%)] Loss: 7134.298828\n",
      "Train Epoch: 294 [69888/225000 (31%)] Loss: 7086.908203\n",
      "Train Epoch: 294 [73984/225000 (33%)] Loss: 6985.179688\n",
      "Train Epoch: 294 [78080/225000 (35%)] Loss: 6937.070312\n",
      "Train Epoch: 294 [82176/225000 (37%)] Loss: 6739.171875\n",
      "Train Epoch: 294 [86272/225000 (38%)] Loss: 7097.167969\n",
      "Train Epoch: 294 [90368/225000 (40%)] Loss: 6825.677734\n",
      "Train Epoch: 294 [94464/225000 (42%)] Loss: 6963.761719\n",
      "Train Epoch: 294 [98560/225000 (44%)] Loss: 7066.513672\n",
      "Train Epoch: 294 [102656/225000 (46%)] Loss: 6825.683594\n",
      "Train Epoch: 294 [106752/225000 (47%)] Loss: 6832.068359\n",
      "Train Epoch: 294 [110848/225000 (49%)] Loss: 6904.173828\n",
      "Train Epoch: 294 [114944/225000 (51%)] Loss: 6950.212891\n",
      "Train Epoch: 294 [119040/225000 (53%)] Loss: 6869.343750\n",
      "Train Epoch: 294 [123136/225000 (55%)] Loss: 7002.486328\n",
      "Train Epoch: 294 [127232/225000 (57%)] Loss: 6882.457031\n",
      "Train Epoch: 294 [131328/225000 (58%)] Loss: 7056.312500\n",
      "Train Epoch: 294 [135424/225000 (60%)] Loss: 6943.812500\n",
      "Train Epoch: 294 [139520/225000 (62%)] Loss: 6957.484375\n",
      "Train Epoch: 294 [143616/225000 (64%)] Loss: 7112.306641\n",
      "Train Epoch: 294 [147712/225000 (66%)] Loss: 6950.052734\n",
      "Train Epoch: 294 [151808/225000 (67%)] Loss: 7148.228516\n",
      "Train Epoch: 294 [155904/225000 (69%)] Loss: 6869.140625\n",
      "Train Epoch: 294 [160000/225000 (71%)] Loss: 6908.804688\n",
      "Train Epoch: 294 [164096/225000 (73%)] Loss: 7162.964844\n",
      "Train Epoch: 294 [168192/225000 (75%)] Loss: 6997.382812\n",
      "Train Epoch: 294 [172288/225000 (77%)] Loss: 7062.187500\n",
      "Train Epoch: 294 [176384/225000 (78%)] Loss: 6911.076172\n",
      "Train Epoch: 294 [180480/225000 (80%)] Loss: 7046.132812\n",
      "Train Epoch: 294 [184576/225000 (82%)] Loss: 6902.988281\n",
      "Train Epoch: 294 [188672/225000 (84%)] Loss: 6986.730469\n",
      "Train Epoch: 294 [192768/225000 (86%)] Loss: 7036.960938\n",
      "Train Epoch: 294 [196864/225000 (87%)] Loss: 6906.232422\n",
      "Train Epoch: 294 [200960/225000 (89%)] Loss: 7012.148438\n",
      "Train Epoch: 294 [205056/225000 (91%)] Loss: 7086.066406\n",
      "Train Epoch: 294 [209152/225000 (93%)] Loss: 7005.337891\n",
      "Train Epoch: 294 [213248/225000 (95%)] Loss: 7045.675781\n",
      "Train Epoch: 294 [217344/225000 (97%)] Loss: 6885.001953\n",
      "Train Epoch: 294 [221440/225000 (98%)] Loss: 6892.373047\n",
      "    epoch          : 294\n",
      "    loss           : 6967.681247333618\n",
      "    val_loss       : 6966.104260739015\n",
      "Train Epoch: 295 [256/225000 (0%)] Loss: 6923.917969\n",
      "Train Epoch: 295 [4352/225000 (2%)] Loss: 6898.384766\n",
      "Train Epoch: 295 [8448/225000 (4%)] Loss: 6972.359375\n",
      "Train Epoch: 295 [12544/225000 (6%)] Loss: 7007.376953\n",
      "Train Epoch: 295 [16640/225000 (7%)] Loss: 6890.566406\n",
      "Train Epoch: 295 [20736/225000 (9%)] Loss: 7072.523438\n",
      "Train Epoch: 295 [24832/225000 (11%)] Loss: 6857.927734\n",
      "Train Epoch: 295 [28928/225000 (13%)] Loss: 6820.259766\n",
      "Train Epoch: 295 [33024/225000 (15%)] Loss: 6811.244141\n",
      "Train Epoch: 295 [37120/225000 (16%)] Loss: 6887.189453\n",
      "Train Epoch: 295 [41216/225000 (18%)] Loss: 6950.875000\n",
      "Train Epoch: 295 [45312/225000 (20%)] Loss: 6954.541016\n",
      "Train Epoch: 295 [49408/225000 (22%)] Loss: 6854.378906\n",
      "Train Epoch: 295 [53504/225000 (24%)] Loss: 6835.785156\n",
      "Train Epoch: 295 [57600/225000 (26%)] Loss: 6909.667969\n",
      "Train Epoch: 295 [61696/225000 (27%)] Loss: 6928.630859\n",
      "Train Epoch: 295 [65792/225000 (29%)] Loss: 6852.123047\n",
      "Train Epoch: 295 [69888/225000 (31%)] Loss: 6918.003906\n",
      "Train Epoch: 295 [73984/225000 (33%)] Loss: 6844.500000\n",
      "Train Epoch: 295 [78080/225000 (35%)] Loss: 7022.656250\n",
      "Train Epoch: 295 [82176/225000 (37%)] Loss: 6961.593750\n",
      "Train Epoch: 295 [86272/225000 (38%)] Loss: 6846.917969\n",
      "Train Epoch: 295 [90368/225000 (40%)] Loss: 6850.253906\n",
      "Train Epoch: 295 [94464/225000 (42%)] Loss: 7008.611328\n",
      "Train Epoch: 295 [98560/225000 (44%)] Loss: 6849.863281\n",
      "Train Epoch: 295 [102656/225000 (46%)] Loss: 6869.955078\n",
      "Train Epoch: 295 [106752/225000 (47%)] Loss: 7108.429688\n",
      "Train Epoch: 295 [110848/225000 (49%)] Loss: 6901.091797\n",
      "Train Epoch: 295 [114944/225000 (51%)] Loss: 7025.736328\n",
      "Train Epoch: 295 [119040/225000 (53%)] Loss: 6872.498047\n",
      "Train Epoch: 295 [123136/225000 (55%)] Loss: 6895.769531\n",
      "Train Epoch: 295 [127232/225000 (57%)] Loss: 6888.716797\n",
      "Train Epoch: 295 [131328/225000 (58%)] Loss: 6953.082031\n",
      "Train Epoch: 295 [135424/225000 (60%)] Loss: 6909.066406\n",
      "Train Epoch: 295 [139520/225000 (62%)] Loss: 6980.974609\n",
      "Train Epoch: 295 [143616/225000 (64%)] Loss: 6934.548828\n",
      "Train Epoch: 295 [147712/225000 (66%)] Loss: 6809.960938\n",
      "Train Epoch: 295 [151808/225000 (67%)] Loss: 6853.236328\n",
      "Train Epoch: 295 [155904/225000 (69%)] Loss: 7027.718750\n",
      "Train Epoch: 295 [160000/225000 (71%)] Loss: 7056.779297\n",
      "Train Epoch: 295 [164096/225000 (73%)] Loss: 6827.988281\n",
      "Train Epoch: 295 [168192/225000 (75%)] Loss: 7087.015625\n",
      "Train Epoch: 295 [172288/225000 (77%)] Loss: 6972.142578\n",
      "Train Epoch: 295 [176384/225000 (78%)] Loss: 7083.255859\n",
      "Train Epoch: 295 [180480/225000 (80%)] Loss: 7089.205078\n",
      "Train Epoch: 295 [184576/225000 (82%)] Loss: 7003.523438\n",
      "Train Epoch: 295 [188672/225000 (84%)] Loss: 7057.910156\n",
      "Train Epoch: 295 [192768/225000 (86%)] Loss: 6822.945312\n",
      "Train Epoch: 295 [196864/225000 (87%)] Loss: 6967.189453\n",
      "Train Epoch: 295 [200960/225000 (89%)] Loss: 6966.791016\n",
      "Train Epoch: 295 [205056/225000 (91%)] Loss: 7053.062500\n",
      "Train Epoch: 295 [209152/225000 (93%)] Loss: 6956.041016\n",
      "Train Epoch: 295 [213248/225000 (95%)] Loss: 6984.458984\n",
      "Train Epoch: 295 [217344/225000 (97%)] Loss: 6911.107422\n",
      "Train Epoch: 295 [221440/225000 (98%)] Loss: 6959.599609\n",
      "    epoch          : 295\n",
      "    loss           : 6990.449609819397\n",
      "    val_loss       : 6967.072682688431\n",
      "Train Epoch: 296 [256/225000 (0%)] Loss: 6891.009766\n",
      "Train Epoch: 296 [4352/225000 (2%)] Loss: 6947.892578\n",
      "Train Epoch: 296 [8448/225000 (4%)] Loss: 6944.855469\n",
      "Train Epoch: 296 [12544/225000 (6%)] Loss: 6930.632812\n",
      "Train Epoch: 296 [16640/225000 (7%)] Loss: 6931.050781\n",
      "Train Epoch: 296 [20736/225000 (9%)] Loss: 6946.890625\n",
      "Train Epoch: 296 [24832/225000 (11%)] Loss: 6967.812500\n",
      "Train Epoch: 296 [28928/225000 (13%)] Loss: 6877.091797\n",
      "Train Epoch: 296 [33024/225000 (15%)] Loss: 6921.222656\n",
      "Train Epoch: 296 [37120/225000 (16%)] Loss: 6871.517578\n",
      "Train Epoch: 296 [41216/225000 (18%)] Loss: 6926.214844\n",
      "Train Epoch: 296 [45312/225000 (20%)] Loss: 6829.390625\n",
      "Train Epoch: 296 [49408/225000 (22%)] Loss: 6884.550781\n",
      "Train Epoch: 296 [53504/225000 (24%)] Loss: 7071.511719\n",
      "Train Epoch: 296 [57600/225000 (26%)] Loss: 6980.330078\n",
      "Train Epoch: 296 [61696/225000 (27%)] Loss: 6852.347656\n",
      "Train Epoch: 296 [65792/225000 (29%)] Loss: 6898.867188\n",
      "Train Epoch: 296 [69888/225000 (31%)] Loss: 7053.625000\n",
      "Train Epoch: 296 [73984/225000 (33%)] Loss: 6924.472656\n",
      "Train Epoch: 296 [78080/225000 (35%)] Loss: 7061.705078\n",
      "Train Epoch: 296 [82176/225000 (37%)] Loss: 6952.250000\n",
      "Train Epoch: 296 [86272/225000 (38%)] Loss: 7050.718750\n",
      "Train Epoch: 296 [90368/225000 (40%)] Loss: 6804.761719\n",
      "Train Epoch: 296 [94464/225000 (42%)] Loss: 6794.873047\n",
      "Train Epoch: 296 [98560/225000 (44%)] Loss: 6954.906250\n",
      "Train Epoch: 296 [102656/225000 (46%)] Loss: 6946.228516\n",
      "Train Epoch: 296 [106752/225000 (47%)] Loss: 6951.669922\n",
      "Train Epoch: 296 [110848/225000 (49%)] Loss: 6831.365234\n",
      "Train Epoch: 296 [114944/225000 (51%)] Loss: 7209.710938\n",
      "Train Epoch: 296 [119040/225000 (53%)] Loss: 6935.826172\n",
      "Train Epoch: 296 [123136/225000 (55%)] Loss: 6731.302734\n",
      "Train Epoch: 296 [127232/225000 (57%)] Loss: 6932.457031\n",
      "Train Epoch: 296 [131328/225000 (58%)] Loss: 7082.382812\n",
      "Train Epoch: 296 [135424/225000 (60%)] Loss: 6980.576172\n",
      "Train Epoch: 296 [139520/225000 (62%)] Loss: 6818.867188\n",
      "Train Epoch: 296 [143616/225000 (64%)] Loss: 6911.736328\n",
      "Train Epoch: 296 [147712/225000 (66%)] Loss: 6900.410156\n",
      "Train Epoch: 296 [151808/225000 (67%)] Loss: 7192.197266\n",
      "Train Epoch: 296 [155904/225000 (69%)] Loss: 7018.712891\n",
      "Train Epoch: 296 [160000/225000 (71%)] Loss: 6956.007812\n",
      "Train Epoch: 296 [164096/225000 (73%)] Loss: 7097.160156\n",
      "Train Epoch: 296 [168192/225000 (75%)] Loss: 6795.326172\n",
      "Train Epoch: 296 [172288/225000 (77%)] Loss: 6936.099609\n",
      "Train Epoch: 296 [176384/225000 (78%)] Loss: 6837.447266\n",
      "Train Epoch: 296 [180480/225000 (80%)] Loss: 6884.041016\n",
      "Train Epoch: 296 [184576/225000 (82%)] Loss: 6838.568359\n",
      "Train Epoch: 296 [188672/225000 (84%)] Loss: 6898.613281\n",
      "Train Epoch: 296 [192768/225000 (86%)] Loss: 7122.863281\n",
      "Train Epoch: 296 [196864/225000 (87%)] Loss: 7071.326172\n",
      "Train Epoch: 296 [200960/225000 (89%)] Loss: 7152.474609\n",
      "Train Epoch: 296 [205056/225000 (91%)] Loss: 6986.267578\n",
      "Train Epoch: 296 [209152/225000 (93%)] Loss: 7009.427734\n",
      "Train Epoch: 296 [213248/225000 (95%)] Loss: 7040.052734\n",
      "Train Epoch: 296 [217344/225000 (97%)] Loss: 6944.076172\n",
      "Train Epoch: 296 [221440/225000 (98%)] Loss: 7053.349609\n",
      "    epoch          : 296\n",
      "    loss           : 6996.708046697241\n",
      "    val_loss       : 6966.67010907251\n",
      "Train Epoch: 297 [256/225000 (0%)] Loss: 7057.349609\n",
      "Train Epoch: 297 [4352/225000 (2%)] Loss: 6914.708984\n",
      "Train Epoch: 297 [8448/225000 (4%)] Loss: 6995.667969\n",
      "Train Epoch: 297 [12544/225000 (6%)] Loss: 6865.537109\n",
      "Train Epoch: 297 [16640/225000 (7%)] Loss: 7060.130859\n",
      "Train Epoch: 297 [20736/225000 (9%)] Loss: 7042.597656\n",
      "Train Epoch: 297 [24832/225000 (11%)] Loss: 7112.089844\n",
      "Train Epoch: 297 [28928/225000 (13%)] Loss: 6886.087891\n",
      "Train Epoch: 297 [33024/225000 (15%)] Loss: 7139.306641\n",
      "Train Epoch: 297 [37120/225000 (16%)] Loss: 6906.562500\n",
      "Train Epoch: 297 [41216/225000 (18%)] Loss: 6941.281250\n",
      "Train Epoch: 297 [45312/225000 (20%)] Loss: 6949.087891\n",
      "Train Epoch: 297 [49408/225000 (22%)] Loss: 6870.390625\n",
      "Train Epoch: 297 [53504/225000 (24%)] Loss: 7010.898438\n",
      "Train Epoch: 297 [57600/225000 (26%)] Loss: 6965.824219\n",
      "Train Epoch: 297 [61696/225000 (27%)] Loss: 6812.373047\n",
      "Train Epoch: 297 [65792/225000 (29%)] Loss: 6978.548828\n",
      "Train Epoch: 297 [69888/225000 (31%)] Loss: 6909.349609\n",
      "Train Epoch: 297 [73984/225000 (33%)] Loss: 6902.873047\n",
      "Train Epoch: 297 [78080/225000 (35%)] Loss: 6980.830078\n",
      "Train Epoch: 297 [82176/225000 (37%)] Loss: 6862.279297\n",
      "Train Epoch: 297 [86272/225000 (38%)] Loss: 6886.431641\n",
      "Train Epoch: 297 [90368/225000 (40%)] Loss: 6964.593750\n",
      "Train Epoch: 297 [94464/225000 (42%)] Loss: 6892.056641\n",
      "Train Epoch: 297 [98560/225000 (44%)] Loss: 6860.160156\n",
      "Train Epoch: 297 [102656/225000 (46%)] Loss: 6952.876953\n",
      "Train Epoch: 297 [106752/225000 (47%)] Loss: 7060.484375\n",
      "Train Epoch: 297 [110848/225000 (49%)] Loss: 6862.814453\n",
      "Train Epoch: 297 [114944/225000 (51%)] Loss: 6991.822266\n",
      "Train Epoch: 297 [119040/225000 (53%)] Loss: 6939.320312\n",
      "Train Epoch: 297 [123136/225000 (55%)] Loss: 6918.656250\n",
      "Train Epoch: 297 [127232/225000 (57%)] Loss: 6954.640625\n",
      "Train Epoch: 297 [131328/225000 (58%)] Loss: 7040.537109\n",
      "Train Epoch: 297 [135424/225000 (60%)] Loss: 6949.968750\n",
      "Train Epoch: 297 [139520/225000 (62%)] Loss: 6901.781250\n",
      "Train Epoch: 297 [143616/225000 (64%)] Loss: 6981.234375\n",
      "Train Epoch: 297 [147712/225000 (66%)] Loss: 7003.884766\n",
      "Train Epoch: 297 [151808/225000 (67%)] Loss: 6854.738281\n",
      "Train Epoch: 297 [155904/225000 (69%)] Loss: 7012.277344\n",
      "Train Epoch: 297 [160000/225000 (71%)] Loss: 6942.599609\n",
      "Train Epoch: 297 [164096/225000 (73%)] Loss: 6971.980469\n",
      "Train Epoch: 297 [168192/225000 (75%)] Loss: 7092.128906\n",
      "Train Epoch: 297 [172288/225000 (77%)] Loss: 6998.828125\n",
      "Train Epoch: 297 [176384/225000 (78%)] Loss: 6950.146484\n",
      "Train Epoch: 297 [180480/225000 (80%)] Loss: 6846.376953\n",
      "Train Epoch: 297 [184576/225000 (82%)] Loss: 7079.056641\n",
      "Train Epoch: 297 [188672/225000 (84%)] Loss: 6909.138672\n",
      "Train Epoch: 297 [192768/225000 (86%)] Loss: 6937.939453\n",
      "Train Epoch: 297 [196864/225000 (87%)] Loss: 6960.464844\n",
      "Train Epoch: 297 [200960/225000 (89%)] Loss: 6868.136719\n",
      "Train Epoch: 297 [205056/225000 (91%)] Loss: 6981.425781\n",
      "Train Epoch: 297 [209152/225000 (93%)] Loss: 6871.001953\n",
      "Train Epoch: 297 [213248/225000 (95%)] Loss: 6989.593750\n",
      "Train Epoch: 297 [217344/225000 (97%)] Loss: 7046.837891\n",
      "Train Epoch: 297 [221440/225000 (98%)] Loss: 7057.335938\n",
      "    epoch          : 297\n",
      "    loss           : 6986.570171403939\n",
      "    val_loss       : 6961.825404125817\n",
      "Train Epoch: 298 [256/225000 (0%)] Loss: 7121.230469\n",
      "Train Epoch: 298 [4352/225000 (2%)] Loss: 7079.435547\n",
      "Train Epoch: 298 [8448/225000 (4%)] Loss: 6984.332031\n",
      "Train Epoch: 298 [12544/225000 (6%)] Loss: 6859.445312\n",
      "Train Epoch: 298 [16640/225000 (7%)] Loss: 7001.613281\n",
      "Train Epoch: 298 [20736/225000 (9%)] Loss: 6911.035156\n",
      "Train Epoch: 298 [24832/225000 (11%)] Loss: 7121.351562\n",
      "Train Epoch: 298 [28928/225000 (13%)] Loss: 7188.423828\n",
      "Train Epoch: 298 [33024/225000 (15%)] Loss: 6876.476562\n",
      "Train Epoch: 298 [37120/225000 (16%)] Loss: 7047.695312\n",
      "Train Epoch: 298 [41216/225000 (18%)] Loss: 6871.400391\n",
      "Train Epoch: 298 [45312/225000 (20%)] Loss: 6891.804688\n",
      "Train Epoch: 298 [49408/225000 (22%)] Loss: 6842.912109\n",
      "Train Epoch: 298 [53504/225000 (24%)] Loss: 6841.792969\n",
      "Train Epoch: 298 [57600/225000 (26%)] Loss: 6967.966797\n",
      "Train Epoch: 298 [61696/225000 (27%)] Loss: 7075.017578\n",
      "Train Epoch: 298 [65792/225000 (29%)] Loss: 7013.904297\n",
      "Train Epoch: 298 [69888/225000 (31%)] Loss: 6845.826172\n",
      "Train Epoch: 298 [73984/225000 (33%)] Loss: 6959.113281\n",
      "Train Epoch: 298 [78080/225000 (35%)] Loss: 7029.871094\n",
      "Train Epoch: 298 [82176/225000 (37%)] Loss: 6747.833984\n",
      "Train Epoch: 298 [86272/225000 (38%)] Loss: 6914.037109\n",
      "Train Epoch: 298 [90368/225000 (40%)] Loss: 6950.476562\n",
      "Train Epoch: 298 [94464/225000 (42%)] Loss: 7164.570312\n",
      "Train Epoch: 298 [98560/225000 (44%)] Loss: 7037.507812\n",
      "Train Epoch: 298 [102656/225000 (46%)] Loss: 7036.931641\n",
      "Train Epoch: 298 [106752/225000 (47%)] Loss: 6897.816406\n",
      "Train Epoch: 298 [110848/225000 (49%)] Loss: 6995.162109\n",
      "Train Epoch: 298 [114944/225000 (51%)] Loss: 6859.564453\n",
      "Train Epoch: 298 [119040/225000 (53%)] Loss: 6943.072266\n",
      "Train Epoch: 298 [123136/225000 (55%)] Loss: 6952.960938\n",
      "Train Epoch: 298 [127232/225000 (57%)] Loss: 6950.748047\n",
      "Train Epoch: 298 [131328/225000 (58%)] Loss: 6877.396484\n",
      "Train Epoch: 298 [135424/225000 (60%)] Loss: 6927.085938\n",
      "Train Epoch: 298 [139520/225000 (62%)] Loss: 6884.808594\n",
      "Train Epoch: 298 [143616/225000 (64%)] Loss: 6973.775391\n",
      "Train Epoch: 298 [147712/225000 (66%)] Loss: 6919.488281\n",
      "Train Epoch: 298 [151808/225000 (67%)] Loss: 6874.378906\n",
      "Train Epoch: 298 [155904/225000 (69%)] Loss: 6950.525391\n",
      "Train Epoch: 298 [160000/225000 (71%)] Loss: 7076.824219\n",
      "Train Epoch: 298 [164096/225000 (73%)] Loss: 6832.117188\n",
      "Train Epoch: 298 [168192/225000 (75%)] Loss: 7018.396484\n",
      "Train Epoch: 298 [172288/225000 (77%)] Loss: 7028.281250\n",
      "Train Epoch: 298 [176384/225000 (78%)] Loss: 6998.296875\n",
      "Train Epoch: 298 [180480/225000 (80%)] Loss: 6906.373047\n",
      "Train Epoch: 298 [184576/225000 (82%)] Loss: 6895.740234\n",
      "Train Epoch: 298 [188672/225000 (84%)] Loss: 6838.943359\n",
      "Train Epoch: 298 [192768/225000 (86%)] Loss: 6966.205078\n",
      "Train Epoch: 298 [196864/225000 (87%)] Loss: 7018.816406\n",
      "Train Epoch: 298 [200960/225000 (89%)] Loss: 7046.769531\n",
      "Train Epoch: 298 [205056/225000 (91%)] Loss: 7056.681641\n",
      "Train Epoch: 298 [209152/225000 (93%)] Loss: 6929.902344\n",
      "Train Epoch: 298 [213248/225000 (95%)] Loss: 7047.718750\n",
      "Train Epoch: 298 [217344/225000 (97%)] Loss: 6835.234375\n",
      "Train Epoch: 298 [221440/225000 (98%)] Loss: 6894.128906\n",
      "    epoch          : 298\n",
      "    loss           : 6980.362616876422\n",
      "    val_loss       : 7105.0049046910535\n",
      "Train Epoch: 299 [256/225000 (0%)] Loss: 6938.134766\n",
      "Train Epoch: 299 [4352/225000 (2%)] Loss: 6830.667969\n",
      "Train Epoch: 299 [8448/225000 (4%)] Loss: 7060.498047\n",
      "Train Epoch: 299 [12544/225000 (6%)] Loss: 6960.470703\n",
      "Train Epoch: 299 [16640/225000 (7%)] Loss: 7007.515625\n",
      "Train Epoch: 299 [20736/225000 (9%)] Loss: 7059.947266\n",
      "Train Epoch: 299 [24832/225000 (11%)] Loss: 6951.056641\n",
      "Train Epoch: 299 [28928/225000 (13%)] Loss: 6888.201172\n",
      "Train Epoch: 299 [33024/225000 (15%)] Loss: 6934.363281\n",
      "Train Epoch: 299 [37120/225000 (16%)] Loss: 6959.785156\n",
      "Train Epoch: 299 [41216/225000 (18%)] Loss: 6986.994141\n",
      "Train Epoch: 299 [45312/225000 (20%)] Loss: 7056.343750\n",
      "Train Epoch: 299 [49408/225000 (22%)] Loss: 6883.691406\n",
      "Train Epoch: 299 [53504/225000 (24%)] Loss: 7001.121094\n",
      "Train Epoch: 299 [57600/225000 (26%)] Loss: 7002.291016\n",
      "Train Epoch: 299 [61696/225000 (27%)] Loss: 7023.701172\n",
      "Train Epoch: 299 [65792/225000 (29%)] Loss: 6844.955078\n",
      "Train Epoch: 299 [69888/225000 (31%)] Loss: 6847.562500\n",
      "Train Epoch: 299 [73984/225000 (33%)] Loss: 7039.933594\n",
      "Train Epoch: 299 [78080/225000 (35%)] Loss: 6928.482422\n",
      "Train Epoch: 299 [82176/225000 (37%)] Loss: 6911.009766\n",
      "Train Epoch: 299 [86272/225000 (38%)] Loss: 6760.437500\n",
      "Train Epoch: 299 [90368/225000 (40%)] Loss: 6993.142578\n",
      "Train Epoch: 299 [94464/225000 (42%)] Loss: 7033.318359\n",
      "Train Epoch: 299 [98560/225000 (44%)] Loss: 6802.982422\n",
      "Train Epoch: 299 [102656/225000 (46%)] Loss: 6794.832031\n",
      "Train Epoch: 299 [106752/225000 (47%)] Loss: 7133.318359\n",
      "Train Epoch: 299 [110848/225000 (49%)] Loss: 6931.798828\n",
      "Train Epoch: 299 [114944/225000 (51%)] Loss: 6998.832031\n",
      "Train Epoch: 299 [119040/225000 (53%)] Loss: 6940.750000\n",
      "Train Epoch: 299 [123136/225000 (55%)] Loss: 7028.267578\n",
      "Train Epoch: 299 [127232/225000 (57%)] Loss: 6946.390625\n",
      "Train Epoch: 299 [131328/225000 (58%)] Loss: 6805.103516\n",
      "Train Epoch: 299 [135424/225000 (60%)] Loss: 6864.144531\n",
      "Train Epoch: 299 [139520/225000 (62%)] Loss: 6901.835938\n",
      "Train Epoch: 299 [143616/225000 (64%)] Loss: 6884.996094\n",
      "Train Epoch: 299 [147712/225000 (66%)] Loss: 6871.960938\n",
      "Train Epoch: 299 [151808/225000 (67%)] Loss: 6840.806641\n",
      "Train Epoch: 299 [155904/225000 (69%)] Loss: 7010.500000\n",
      "Train Epoch: 299 [160000/225000 (71%)] Loss: 7048.679688\n",
      "Train Epoch: 299 [164096/225000 (73%)] Loss: 6936.599609\n",
      "Train Epoch: 299 [168192/225000 (75%)] Loss: 7096.974609\n",
      "Train Epoch: 299 [172288/225000 (77%)] Loss: 6814.580078\n",
      "Train Epoch: 299 [176384/225000 (78%)] Loss: 7055.185547\n",
      "Train Epoch: 299 [180480/225000 (80%)] Loss: 7031.328125\n",
      "Train Epoch: 299 [184576/225000 (82%)] Loss: 6906.371094\n",
      "Train Epoch: 299 [188672/225000 (84%)] Loss: 6999.044922\n",
      "Train Epoch: 299 [192768/225000 (86%)] Loss: 6862.160156\n",
      "Train Epoch: 299 [196864/225000 (87%)] Loss: 6998.910156\n",
      "Train Epoch: 299 [200960/225000 (89%)] Loss: 6978.984375\n",
      "Train Epoch: 299 [205056/225000 (91%)] Loss: 7082.378906\n",
      "Train Epoch: 299 [209152/225000 (93%)] Loss: 6980.562500\n",
      "Train Epoch: 299 [213248/225000 (95%)] Loss: 7019.839844\n",
      "Train Epoch: 299 [217344/225000 (97%)] Loss: 6998.964844\n",
      "Train Epoch: 299 [221440/225000 (98%)] Loss: 6954.746094\n",
      "    epoch          : 299\n",
      "    loss           : 6969.6352889025175\n",
      "    val_loss       : 6963.855847078927\n",
      "Train Epoch: 300 [256/225000 (0%)] Loss: 6856.121094\n",
      "Train Epoch: 300 [4352/225000 (2%)] Loss: 7048.005859\n",
      "Train Epoch: 300 [8448/225000 (4%)] Loss: 7094.712891\n",
      "Train Epoch: 300 [12544/225000 (6%)] Loss: 7066.714844\n",
      "Train Epoch: 300 [16640/225000 (7%)] Loss: 6889.115234\n",
      "Train Epoch: 300 [20736/225000 (9%)] Loss: 6935.857422\n",
      "Train Epoch: 300 [24832/225000 (11%)] Loss: 7037.050781\n",
      "Train Epoch: 300 [28928/225000 (13%)] Loss: 7168.496094\n",
      "Train Epoch: 300 [33024/225000 (15%)] Loss: 7072.101562\n",
      "Train Epoch: 300 [37120/225000 (16%)] Loss: 6982.812500\n",
      "Train Epoch: 300 [41216/225000 (18%)] Loss: 7004.484375\n",
      "Train Epoch: 300 [45312/225000 (20%)] Loss: 6925.089844\n",
      "Train Epoch: 300 [49408/225000 (22%)] Loss: 6952.912109\n",
      "Train Epoch: 300 [53504/225000 (24%)] Loss: 6833.529297\n",
      "Train Epoch: 300 [57600/225000 (26%)] Loss: 6925.445312\n",
      "Train Epoch: 300 [61696/225000 (27%)] Loss: 7100.126953\n",
      "Train Epoch: 300 [65792/225000 (29%)] Loss: 6891.435547\n",
      "Train Epoch: 300 [69888/225000 (31%)] Loss: 6927.960938\n",
      "Train Epoch: 300 [73984/225000 (33%)] Loss: 6954.044922\n",
      "Train Epoch: 300 [78080/225000 (35%)] Loss: 6915.207031\n",
      "Train Epoch: 300 [82176/225000 (37%)] Loss: 6990.900391\n",
      "Train Epoch: 300 [86272/225000 (38%)] Loss: 6857.546875\n",
      "Train Epoch: 300 [90368/225000 (40%)] Loss: 6929.568359\n",
      "Train Epoch: 300 [94464/225000 (42%)] Loss: 6922.857422\n",
      "Train Epoch: 300 [98560/225000 (44%)] Loss: 7038.617188\n",
      "Train Epoch: 300 [102656/225000 (46%)] Loss: 6814.251953\n",
      "Train Epoch: 300 [106752/225000 (47%)] Loss: 6813.509766\n",
      "Train Epoch: 300 [110848/225000 (49%)] Loss: 6938.550781\n",
      "Train Epoch: 300 [114944/225000 (51%)] Loss: 7083.410156\n",
      "Train Epoch: 300 [119040/225000 (53%)] Loss: 6903.976562\n",
      "Train Epoch: 300 [123136/225000 (55%)] Loss: 6843.476562\n",
      "Train Epoch: 300 [127232/225000 (57%)] Loss: 6866.822266\n",
      "Train Epoch: 300 [131328/225000 (58%)] Loss: 6847.960938\n",
      "Train Epoch: 300 [135424/225000 (60%)] Loss: 6990.212891\n",
      "Train Epoch: 300 [139520/225000 (62%)] Loss: 6985.564453\n",
      "Train Epoch: 300 [143616/225000 (64%)] Loss: 6988.783203\n",
      "Train Epoch: 300 [147712/225000 (66%)] Loss: 7076.488281\n",
      "Train Epoch: 300 [151808/225000 (67%)] Loss: 6917.275391\n",
      "Train Epoch: 300 [155904/225000 (69%)] Loss: 7000.650391\n",
      "Train Epoch: 300 [160000/225000 (71%)] Loss: 6963.949219\n",
      "Train Epoch: 300 [164096/225000 (73%)] Loss: 7096.699219\n",
      "Train Epoch: 300 [168192/225000 (75%)] Loss: 6938.240234\n",
      "Train Epoch: 300 [172288/225000 (77%)] Loss: 6972.339844\n",
      "Train Epoch: 300 [176384/225000 (78%)] Loss: 7065.445312\n",
      "Train Epoch: 300 [180480/225000 (80%)] Loss: 6942.806641\n",
      "Train Epoch: 300 [184576/225000 (82%)] Loss: 7057.988281\n",
      "Train Epoch: 300 [188672/225000 (84%)] Loss: 6847.619141\n",
      "Train Epoch: 300 [192768/225000 (86%)] Loss: 6841.093750\n",
      "Train Epoch: 300 [196864/225000 (87%)] Loss: 6902.554688\n",
      "Train Epoch: 300 [200960/225000 (89%)] Loss: 6894.076172\n",
      "Train Epoch: 300 [205056/225000 (91%)] Loss: 7002.583984\n",
      "Train Epoch: 300 [209152/225000 (93%)] Loss: 6840.789062\n",
      "Train Epoch: 300 [213248/225000 (95%)] Loss: 6886.380859\n",
      "Train Epoch: 300 [217344/225000 (97%)] Loss: 7053.818359\n",
      "Train Epoch: 300 [221440/225000 (98%)] Loss: 6939.734375\n",
      "    epoch          : 300\n",
      "    loss           : 6963.083402214875\n",
      "    val_loss       : 6956.501855053464\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0827_225017/checkpoint-epoch300.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 301 [256/225000 (0%)] Loss: 7051.066406\n",
      "Train Epoch: 301 [4352/225000 (2%)] Loss: 6799.339844\n",
      "Train Epoch: 301 [8448/225000 (4%)] Loss: 6909.408203\n",
      "Train Epoch: 301 [12544/225000 (6%)] Loss: 6937.794922\n",
      "Train Epoch: 301 [16640/225000 (7%)] Loss: 6910.523438\n",
      "Train Epoch: 301 [20736/225000 (9%)] Loss: 6835.742188\n",
      "Train Epoch: 301 [24832/225000 (11%)] Loss: 7005.218750\n",
      "Train Epoch: 301 [28928/225000 (13%)] Loss: 6921.507812\n",
      "Train Epoch: 301 [33024/225000 (15%)] Loss: 7008.701172\n",
      "Train Epoch: 301 [37120/225000 (16%)] Loss: 6878.404297\n",
      "Train Epoch: 301 [41216/225000 (18%)] Loss: 6894.164062\n",
      "Train Epoch: 301 [45312/225000 (20%)] Loss: 6984.714844\n",
      "Train Epoch: 301 [49408/225000 (22%)] Loss: 6955.152344\n",
      "Train Epoch: 301 [53504/225000 (24%)] Loss: 7130.439453\n",
      "Train Epoch: 301 [57600/225000 (26%)] Loss: 7057.023438\n",
      "Train Epoch: 301 [61696/225000 (27%)] Loss: 6980.558594\n",
      "Train Epoch: 301 [65792/225000 (29%)] Loss: 6879.287109\n",
      "Train Epoch: 301 [69888/225000 (31%)] Loss: 7030.904297\n",
      "Train Epoch: 301 [73984/225000 (33%)] Loss: 6713.337891\n",
      "Train Epoch: 301 [78080/225000 (35%)] Loss: 6940.033203\n",
      "Train Epoch: 301 [82176/225000 (37%)] Loss: 6898.183594\n",
      "Train Epoch: 301 [86272/225000 (38%)] Loss: 6903.853516\n",
      "Train Epoch: 301 [90368/225000 (40%)] Loss: 6987.326172\n",
      "Train Epoch: 301 [94464/225000 (42%)] Loss: 7098.105469\n",
      "Train Epoch: 301 [98560/225000 (44%)] Loss: 6979.626953\n",
      "Train Epoch: 301 [102656/225000 (46%)] Loss: 6893.464844\n",
      "Train Epoch: 301 [106752/225000 (47%)] Loss: 6838.929688\n",
      "Train Epoch: 301 [110848/225000 (49%)] Loss: 6875.623047\n",
      "Train Epoch: 301 [114944/225000 (51%)] Loss: 7077.332031\n",
      "Train Epoch: 301 [119040/225000 (53%)] Loss: 6993.089844\n",
      "Train Epoch: 301 [123136/225000 (55%)] Loss: 6969.316406\n",
      "Train Epoch: 301 [127232/225000 (57%)] Loss: 6930.015625\n",
      "Train Epoch: 301 [131328/225000 (58%)] Loss: 6970.716797\n",
      "Train Epoch: 301 [135424/225000 (60%)] Loss: 6826.712891\n",
      "Train Epoch: 301 [139520/225000 (62%)] Loss: 6990.310547\n",
      "Train Epoch: 301 [143616/225000 (64%)] Loss: 6899.648438\n",
      "Train Epoch: 301 [147712/225000 (66%)] Loss: 7089.529297\n",
      "Train Epoch: 301 [151808/225000 (67%)] Loss: 7034.662109\n",
      "Train Epoch: 301 [155904/225000 (69%)] Loss: 7002.101562\n",
      "Train Epoch: 301 [160000/225000 (71%)] Loss: 6944.488281\n",
      "Train Epoch: 301 [164096/225000 (73%)] Loss: 6970.109375\n",
      "Train Epoch: 301 [168192/225000 (75%)] Loss: 6882.511719\n",
      "Train Epoch: 301 [172288/225000 (77%)] Loss: 7071.054688\n",
      "Train Epoch: 301 [176384/225000 (78%)] Loss: 6957.724609\n",
      "Train Epoch: 301 [180480/225000 (80%)] Loss: 7087.556641\n",
      "Train Epoch: 301 [184576/225000 (82%)] Loss: 6912.746094\n",
      "Train Epoch: 301 [188672/225000 (84%)] Loss: 6823.656250\n",
      "Train Epoch: 301 [192768/225000 (86%)] Loss: 6934.564453\n",
      "Train Epoch: 301 [196864/225000 (87%)] Loss: 6920.689453\n",
      "Train Epoch: 301 [200960/225000 (89%)] Loss: 6864.753906\n",
      "Train Epoch: 301 [205056/225000 (91%)] Loss: 7107.736328\n",
      "Train Epoch: 301 [209152/225000 (93%)] Loss: 6871.710938\n",
      "Train Epoch: 301 [213248/225000 (95%)] Loss: 6811.669922\n",
      "Train Epoch: 301 [217344/225000 (97%)] Loss: 7032.451172\n",
      "Train Epoch: 301 [221440/225000 (98%)] Loss: 6995.136719\n",
      "    epoch          : 301\n",
      "    loss           : 6947.528450298635\n",
      "    val_loss       : 6955.777084508721\n",
      "Train Epoch: 302 [256/225000 (0%)] Loss: 6948.966797\n",
      "Train Epoch: 302 [4352/225000 (2%)] Loss: 6825.138672\n",
      "Train Epoch: 302 [8448/225000 (4%)] Loss: 7100.441406\n",
      "Train Epoch: 302 [12544/225000 (6%)] Loss: 6999.740234\n",
      "Train Epoch: 302 [16640/225000 (7%)] Loss: 6920.970703\n",
      "Train Epoch: 302 [20736/225000 (9%)] Loss: 6994.820312\n",
      "Train Epoch: 302 [24832/225000 (11%)] Loss: 6976.912109\n",
      "Train Epoch: 302 [28928/225000 (13%)] Loss: 7037.361328\n",
      "Train Epoch: 302 [33024/225000 (15%)] Loss: 6887.058594\n",
      "Train Epoch: 302 [37120/225000 (16%)] Loss: 6956.412109\n",
      "Train Epoch: 302 [41216/225000 (18%)] Loss: 6899.394531\n",
      "Train Epoch: 302 [45312/225000 (20%)] Loss: 6948.626953\n",
      "Train Epoch: 302 [49408/225000 (22%)] Loss: 6879.861328\n",
      "Train Epoch: 302 [53504/225000 (24%)] Loss: 6970.308594\n",
      "Train Epoch: 302 [57600/225000 (26%)] Loss: 6849.611328\n",
      "Train Epoch: 302 [61696/225000 (27%)] Loss: 6811.162109\n",
      "Train Epoch: 302 [65792/225000 (29%)] Loss: 6961.212891\n",
      "Train Epoch: 302 [69888/225000 (31%)] Loss: 6808.455078\n",
      "Train Epoch: 302 [73984/225000 (33%)] Loss: 7160.355469\n",
      "Train Epoch: 302 [78080/225000 (35%)] Loss: 6916.031250\n",
      "Train Epoch: 302 [82176/225000 (37%)] Loss: 6780.548828\n",
      "Train Epoch: 302 [86272/225000 (38%)] Loss: 6821.271484\n",
      "Train Epoch: 302 [90368/225000 (40%)] Loss: 6970.535156\n",
      "Train Epoch: 302 [94464/225000 (42%)] Loss: 7052.724609\n",
      "Train Epoch: 302 [98560/225000 (44%)] Loss: 7121.496094\n",
      "Train Epoch: 302 [102656/225000 (46%)] Loss: 6964.275391\n",
      "Train Epoch: 302 [106752/225000 (47%)] Loss: 7081.578125\n",
      "Train Epoch: 302 [110848/225000 (49%)] Loss: 6863.625000\n",
      "Train Epoch: 302 [114944/225000 (51%)] Loss: 6878.412109\n",
      "Train Epoch: 302 [119040/225000 (53%)] Loss: 6891.273438\n",
      "Train Epoch: 302 [123136/225000 (55%)] Loss: 7086.416016\n",
      "Train Epoch: 302 [127232/225000 (57%)] Loss: 6982.601562\n",
      "Train Epoch: 302 [131328/225000 (58%)] Loss: 6849.351562\n",
      "Train Epoch: 302 [135424/225000 (60%)] Loss: 7012.652344\n",
      "Train Epoch: 302 [139520/225000 (62%)] Loss: 6939.087891\n",
      "Train Epoch: 302 [143616/225000 (64%)] Loss: 6995.742188\n",
      "Train Epoch: 302 [147712/225000 (66%)] Loss: 6780.324219\n",
      "Train Epoch: 302 [151808/225000 (67%)] Loss: 6888.308594\n",
      "Train Epoch: 302 [155904/225000 (69%)] Loss: 6901.146484\n",
      "Train Epoch: 302 [160000/225000 (71%)] Loss: 6934.394531\n",
      "Train Epoch: 302 [164096/225000 (73%)] Loss: 7061.445312\n",
      "Train Epoch: 302 [168192/225000 (75%)] Loss: 6971.730469\n",
      "Train Epoch: 302 [172288/225000 (77%)] Loss: 6813.345703\n",
      "Train Epoch: 302 [176384/225000 (78%)] Loss: 6808.798828\n",
      "Train Epoch: 302 [180480/225000 (80%)] Loss: 6910.189453\n",
      "Train Epoch: 302 [184576/225000 (82%)] Loss: 7155.183594\n",
      "Train Epoch: 302 [188672/225000 (84%)] Loss: 6812.630859\n",
      "Train Epoch: 302 [192768/225000 (86%)] Loss: 6985.082031\n",
      "Train Epoch: 302 [196864/225000 (87%)] Loss: 6894.218750\n",
      "Train Epoch: 302 [200960/225000 (89%)] Loss: 6922.228516\n",
      "Train Epoch: 302 [205056/225000 (91%)] Loss: 6902.974609\n",
      "Train Epoch: 302 [209152/225000 (93%)] Loss: 6937.921875\n",
      "Train Epoch: 302 [213248/225000 (95%)] Loss: 6953.921875\n",
      "Train Epoch: 302 [217344/225000 (97%)] Loss: 6998.707031\n",
      "Train Epoch: 302 [221440/225000 (98%)] Loss: 6873.195312\n",
      "    epoch          : 302\n",
      "    loss           : 6965.51644824552\n",
      "    val_loss       : 6955.7181332451955\n",
      "Train Epoch: 303 [256/225000 (0%)] Loss: 6899.259766\n",
      "Train Epoch: 303 [4352/225000 (2%)] Loss: 6876.546875\n",
      "Train Epoch: 303 [8448/225000 (4%)] Loss: 6858.984375\n",
      "Train Epoch: 303 [12544/225000 (6%)] Loss: 7094.490234\n",
      "Train Epoch: 303 [16640/225000 (7%)] Loss: 6955.845703\n",
      "Train Epoch: 303 [20736/225000 (9%)] Loss: 6996.351562\n",
      "Train Epoch: 303 [24832/225000 (11%)] Loss: 7056.376953\n",
      "Train Epoch: 303 [28928/225000 (13%)] Loss: 6927.597656\n",
      "Train Epoch: 303 [33024/225000 (15%)] Loss: 6938.720703\n",
      "Train Epoch: 303 [37120/225000 (16%)] Loss: 7029.300781\n",
      "Train Epoch: 303 [41216/225000 (18%)] Loss: 7048.878906\n",
      "Train Epoch: 303 [45312/225000 (20%)] Loss: 7021.039062\n",
      "Train Epoch: 303 [49408/225000 (22%)] Loss: 6885.986328\n",
      "Train Epoch: 303 [53504/225000 (24%)] Loss: 6945.861328\n",
      "Train Epoch: 303 [57600/225000 (26%)] Loss: 6977.769531\n",
      "Train Epoch: 303 [61696/225000 (27%)] Loss: 6928.076172\n",
      "Train Epoch: 303 [65792/225000 (29%)] Loss: 6846.496094\n",
      "Train Epoch: 303 [69888/225000 (31%)] Loss: 6991.302734\n",
      "Train Epoch: 303 [73984/225000 (33%)] Loss: 6893.121094\n",
      "Train Epoch: 303 [78080/225000 (35%)] Loss: 6995.419922\n",
      "Train Epoch: 303 [82176/225000 (37%)] Loss: 6992.146484\n",
      "Train Epoch: 303 [86272/225000 (38%)] Loss: 7024.742188\n",
      "Train Epoch: 303 [90368/225000 (40%)] Loss: 7036.201172\n",
      "Train Epoch: 303 [94464/225000 (42%)] Loss: 7096.197266\n",
      "Train Epoch: 303 [98560/225000 (44%)] Loss: 7050.746094\n",
      "Train Epoch: 303 [102656/225000 (46%)] Loss: 7060.695312\n",
      "Train Epoch: 303 [106752/225000 (47%)] Loss: 6876.103516\n",
      "Train Epoch: 303 [110848/225000 (49%)] Loss: 6849.703125\n",
      "Train Epoch: 303 [114944/225000 (51%)] Loss: 6812.886719\n",
      "Train Epoch: 303 [119040/225000 (53%)] Loss: 6801.625000\n",
      "Train Epoch: 303 [123136/225000 (55%)] Loss: 6799.419922\n",
      "Train Epoch: 303 [127232/225000 (57%)] Loss: 6990.955078\n",
      "Train Epoch: 303 [131328/225000 (58%)] Loss: 6862.818359\n",
      "Train Epoch: 303 [135424/225000 (60%)] Loss: 6865.751953\n",
      "Train Epoch: 303 [139520/225000 (62%)] Loss: 6760.083984\n",
      "Train Epoch: 303 [143616/225000 (64%)] Loss: 7089.253906\n",
      "Train Epoch: 303 [147712/225000 (66%)] Loss: 6995.326172\n",
      "Train Epoch: 303 [151808/225000 (67%)] Loss: 6925.708984\n",
      "Train Epoch: 303 [155904/225000 (69%)] Loss: 6947.750000\n",
      "Train Epoch: 303 [160000/225000 (71%)] Loss: 6890.863281\n",
      "Train Epoch: 303 [164096/225000 (73%)] Loss: 6891.617188\n",
      "Train Epoch: 303 [168192/225000 (75%)] Loss: 6811.511719\n",
      "Train Epoch: 303 [172288/225000 (77%)] Loss: 6854.824219\n",
      "Train Epoch: 303 [176384/225000 (78%)] Loss: 6915.597656\n",
      "Train Epoch: 303 [180480/225000 (80%)] Loss: 6918.146484\n",
      "Train Epoch: 303 [184576/225000 (82%)] Loss: 6873.001953\n",
      "Train Epoch: 303 [188672/225000 (84%)] Loss: 7003.142578\n",
      "Train Epoch: 303 [192768/225000 (86%)] Loss: 7046.896484\n",
      "Train Epoch: 303 [196864/225000 (87%)] Loss: 7044.654297\n",
      "Train Epoch: 303 [200960/225000 (89%)] Loss: 6938.539062\n",
      "Train Epoch: 303 [205056/225000 (91%)] Loss: 6762.328125\n",
      "Train Epoch: 303 [209152/225000 (93%)] Loss: 6990.880859\n",
      "Train Epoch: 303 [213248/225000 (95%)] Loss: 7217.683594\n",
      "Train Epoch: 303 [217344/225000 (97%)] Loss: 7061.283203\n",
      "Train Epoch: 303 [221440/225000 (98%)] Loss: 6870.921875\n",
      "    epoch          : 303\n",
      "    loss           : 6952.176436735637\n",
      "    val_loss       : 6955.01390660539\n",
      "Train Epoch: 304 [256/225000 (0%)] Loss: 7000.205078\n",
      "Train Epoch: 304 [4352/225000 (2%)] Loss: 6991.654297\n",
      "Train Epoch: 304 [8448/225000 (4%)] Loss: 6920.568359\n",
      "Train Epoch: 304 [12544/225000 (6%)] Loss: 7177.453125\n",
      "Train Epoch: 304 [16640/225000 (7%)] Loss: 6962.792969\n",
      "Train Epoch: 304 [20736/225000 (9%)] Loss: 6976.296875\n",
      "Train Epoch: 304 [24832/225000 (11%)] Loss: 7004.763672\n",
      "Train Epoch: 304 [28928/225000 (13%)] Loss: 6915.601562\n",
      "Train Epoch: 304 [33024/225000 (15%)] Loss: 6878.646484\n",
      "Train Epoch: 304 [37120/225000 (16%)] Loss: 6883.720703\n",
      "Train Epoch: 304 [41216/225000 (18%)] Loss: 6884.474609\n",
      "Train Epoch: 304 [45312/225000 (20%)] Loss: 7113.781250\n",
      "Train Epoch: 304 [49408/225000 (22%)] Loss: 7110.552734\n",
      "Train Epoch: 304 [53504/225000 (24%)] Loss: 6962.062500\n",
      "Train Epoch: 304 [57600/225000 (26%)] Loss: 6845.882812\n",
      "Train Epoch: 304 [61696/225000 (27%)] Loss: 6985.650391\n",
      "Train Epoch: 304 [65792/225000 (29%)] Loss: 7125.908203\n",
      "Train Epoch: 304 [69888/225000 (31%)] Loss: 7015.257812\n",
      "Train Epoch: 304 [73984/225000 (33%)] Loss: 6842.175781\n",
      "Train Epoch: 304 [78080/225000 (35%)] Loss: 6926.392578\n",
      "Train Epoch: 304 [82176/225000 (37%)] Loss: 6901.257812\n",
      "Train Epoch: 304 [86272/225000 (38%)] Loss: 6918.503906\n",
      "Train Epoch: 304 [90368/225000 (40%)] Loss: 6850.312500\n",
      "Train Epoch: 304 [94464/225000 (42%)] Loss: 7007.853516\n",
      "Train Epoch: 304 [98560/225000 (44%)] Loss: 6860.302734\n",
      "Train Epoch: 304 [102656/225000 (46%)] Loss: 7065.025391\n",
      "Train Epoch: 304 [106752/225000 (47%)] Loss: 6983.203125\n",
      "Train Epoch: 304 [110848/225000 (49%)] Loss: 6992.794922\n",
      "Train Epoch: 304 [114944/225000 (51%)] Loss: 6876.169922\n",
      "Train Epoch: 304 [119040/225000 (53%)] Loss: 7097.562500\n",
      "Train Epoch: 304 [123136/225000 (55%)] Loss: 6893.861328\n",
      "Train Epoch: 304 [127232/225000 (57%)] Loss: 6743.615234\n",
      "Train Epoch: 304 [131328/225000 (58%)] Loss: 6900.585938\n",
      "Train Epoch: 304 [135424/225000 (60%)] Loss: 6931.835938\n",
      "Train Epoch: 304 [139520/225000 (62%)] Loss: 6967.583984\n",
      "Train Epoch: 304 [143616/225000 (64%)] Loss: 6915.744141\n",
      "Train Epoch: 304 [147712/225000 (66%)] Loss: 6851.812500\n",
      "Train Epoch: 304 [151808/225000 (67%)] Loss: 7017.759766\n",
      "Train Epoch: 304 [155904/225000 (69%)] Loss: 6852.220703\n",
      "Train Epoch: 304 [160000/225000 (71%)] Loss: 7001.894531\n",
      "Train Epoch: 304 [164096/225000 (73%)] Loss: 6870.890625\n",
      "Train Epoch: 304 [168192/225000 (75%)] Loss: 6956.253906\n",
      "Train Epoch: 304 [172288/225000 (77%)] Loss: 7014.880859\n",
      "Train Epoch: 304 [176384/225000 (78%)] Loss: 7006.240234\n",
      "Train Epoch: 304 [180480/225000 (80%)] Loss: 6927.798828\n",
      "Train Epoch: 304 [184576/225000 (82%)] Loss: 7015.953125\n",
      "Train Epoch: 304 [188672/225000 (84%)] Loss: 6855.843750\n",
      "Train Epoch: 304 [192768/225000 (86%)] Loss: 7107.847656\n",
      "Train Epoch: 304 [196864/225000 (87%)] Loss: 6834.169922\n",
      "Train Epoch: 304 [200960/225000 (89%)] Loss: 6953.914062\n",
      "Train Epoch: 304 [205056/225000 (91%)] Loss: 7030.949219\n",
      "Train Epoch: 304 [209152/225000 (93%)] Loss: 6851.460938\n",
      "Train Epoch: 304 [213248/225000 (95%)] Loss: 6937.841797\n",
      "Train Epoch: 304 [217344/225000 (97%)] Loss: 6888.421875\n",
      "Train Epoch: 304 [221440/225000 (98%)] Loss: 6916.496094\n",
      "    epoch          : 304\n",
      "    loss           : 6941.484582755617\n",
      "    val_loss       : 7033.077495623608\n",
      "Train Epoch: 305 [256/225000 (0%)] Loss: 7020.087891\n",
      "Train Epoch: 305 [4352/225000 (2%)] Loss: 6967.375000\n",
      "Train Epoch: 305 [8448/225000 (4%)] Loss: 6780.275391\n",
      "Train Epoch: 305 [12544/225000 (6%)] Loss: 6858.267578\n",
      "Train Epoch: 305 [16640/225000 (7%)] Loss: 6887.820312\n",
      "Train Epoch: 305 [20736/225000 (9%)] Loss: 7029.550781\n",
      "Train Epoch: 305 [24832/225000 (11%)] Loss: 6954.380859\n",
      "Train Epoch: 305 [28928/225000 (13%)] Loss: 6865.898438\n",
      "Train Epoch: 305 [33024/225000 (15%)] Loss: 6907.021484\n",
      "Train Epoch: 305 [37120/225000 (16%)] Loss: 6789.000000\n",
      "Train Epoch: 305 [41216/225000 (18%)] Loss: 6901.476562\n",
      "Train Epoch: 305 [45312/225000 (20%)] Loss: 6942.326172\n",
      "Train Epoch: 305 [49408/225000 (22%)] Loss: 6841.214844\n",
      "Train Epoch: 305 [53504/225000 (24%)] Loss: 6893.396484\n",
      "Train Epoch: 305 [57600/225000 (26%)] Loss: 6887.513672\n",
      "Train Epoch: 305 [61696/225000 (27%)] Loss: 6837.128906\n",
      "Train Epoch: 305 [65792/225000 (29%)] Loss: 6918.089844\n",
      "Train Epoch: 305 [69888/225000 (31%)] Loss: 7133.083984\n",
      "Train Epoch: 305 [73984/225000 (33%)] Loss: 6872.695312\n",
      "Train Epoch: 305 [78080/225000 (35%)] Loss: 6793.101562\n",
      "Train Epoch: 305 [82176/225000 (37%)] Loss: 7036.146484\n",
      "Train Epoch: 305 [86272/225000 (38%)] Loss: 6961.544922\n",
      "Train Epoch: 305 [90368/225000 (40%)] Loss: 7037.357422\n",
      "Train Epoch: 305 [94464/225000 (42%)] Loss: 6949.683594\n",
      "Train Epoch: 305 [98560/225000 (44%)] Loss: 6838.437500\n",
      "Train Epoch: 305 [102656/225000 (46%)] Loss: 6892.062500\n",
      "Train Epoch: 305 [106752/225000 (47%)] Loss: 6932.755859\n",
      "Train Epoch: 305 [110848/225000 (49%)] Loss: 6984.031250\n",
      "Train Epoch: 305 [114944/225000 (51%)] Loss: 6907.013672\n",
      "Train Epoch: 305 [119040/225000 (53%)] Loss: 6811.800781\n",
      "Train Epoch: 305 [123136/225000 (55%)] Loss: 7051.894531\n",
      "Train Epoch: 305 [127232/225000 (57%)] Loss: 6752.560547\n",
      "Train Epoch: 305 [131328/225000 (58%)] Loss: 7104.617188\n",
      "Train Epoch: 305 [135424/225000 (60%)] Loss: 6932.199219\n",
      "Train Epoch: 305 [139520/225000 (62%)] Loss: 6984.181641\n",
      "Train Epoch: 305 [143616/225000 (64%)] Loss: 6780.095703\n",
      "Train Epoch: 305 [147712/225000 (66%)] Loss: 6815.021484\n",
      "Train Epoch: 305 [151808/225000 (67%)] Loss: 7031.630859\n",
      "Train Epoch: 305 [155904/225000 (69%)] Loss: 6903.822266\n",
      "Train Epoch: 305 [160000/225000 (71%)] Loss: 6810.929688\n",
      "Train Epoch: 305 [164096/225000 (73%)] Loss: 6898.820312\n",
      "Train Epoch: 305 [168192/225000 (75%)] Loss: 7007.857422\n",
      "Train Epoch: 305 [172288/225000 (77%)] Loss: 7019.921875\n",
      "Train Epoch: 305 [176384/225000 (78%)] Loss: 6798.585938\n",
      "Train Epoch: 305 [180480/225000 (80%)] Loss: 6918.458984\n",
      "Train Epoch: 305 [184576/225000 (82%)] Loss: 6945.443359\n",
      "Train Epoch: 305 [188672/225000 (84%)] Loss: 7024.644531\n",
      "Train Epoch: 305 [192768/225000 (86%)] Loss: 6889.181641\n",
      "Train Epoch: 305 [196864/225000 (87%)] Loss: 7006.888672\n",
      "Train Epoch: 305 [200960/225000 (89%)] Loss: 7053.253906\n",
      "Train Epoch: 305 [205056/225000 (91%)] Loss: 6930.453125\n",
      "Train Epoch: 305 [209152/225000 (93%)] Loss: 7163.804688\n",
      "Train Epoch: 305 [213248/225000 (95%)] Loss: 6973.382812\n",
      "Train Epoch: 305 [217344/225000 (97%)] Loss: 6925.060547\n",
      "Train Epoch: 305 [221440/225000 (98%)] Loss: 6942.558594\n",
      "    epoch          : 305\n",
      "    loss           : 6949.818092736775\n",
      "    val_loss       : 6952.073898882282\n",
      "Train Epoch: 306 [256/225000 (0%)] Loss: 6748.433594\n",
      "Train Epoch: 306 [4352/225000 (2%)] Loss: 6788.753906\n",
      "Train Epoch: 306 [8448/225000 (4%)] Loss: 6921.619141\n",
      "Train Epoch: 306 [12544/225000 (6%)] Loss: 7028.187500\n",
      "Train Epoch: 306 [16640/225000 (7%)] Loss: 7005.078125\n",
      "Train Epoch: 306 [20736/225000 (9%)] Loss: 7052.222656\n",
      "Train Epoch: 306 [24832/225000 (11%)] Loss: 6866.326172\n",
      "Train Epoch: 306 [28928/225000 (13%)] Loss: 7006.074219\n",
      "Train Epoch: 306 [33024/225000 (15%)] Loss: 6941.712891\n",
      "Train Epoch: 306 [37120/225000 (16%)] Loss: 6957.867188\n",
      "Train Epoch: 306 [41216/225000 (18%)] Loss: 6949.289062\n",
      "Train Epoch: 306 [45312/225000 (20%)] Loss: 7054.125000\n",
      "Train Epoch: 306 [49408/225000 (22%)] Loss: 6737.408203\n",
      "Train Epoch: 306 [53504/225000 (24%)] Loss: 6887.458984\n",
      "Train Epoch: 306 [57600/225000 (26%)] Loss: 6852.054688\n",
      "Train Epoch: 306 [61696/225000 (27%)] Loss: 7038.300781\n",
      "Train Epoch: 306 [65792/225000 (29%)] Loss: 6997.818359\n",
      "Train Epoch: 306 [69888/225000 (31%)] Loss: 6951.710938\n",
      "Train Epoch: 306 [73984/225000 (33%)] Loss: 6983.826172\n",
      "Train Epoch: 306 [78080/225000 (35%)] Loss: 7049.726562\n",
      "Train Epoch: 306 [82176/225000 (37%)] Loss: 6829.039062\n",
      "Train Epoch: 306 [86272/225000 (38%)] Loss: 7010.857422\n",
      "Train Epoch: 306 [90368/225000 (40%)] Loss: 6870.048828\n",
      "Train Epoch: 306 [94464/225000 (42%)] Loss: 6987.619141\n",
      "Train Epoch: 306 [98560/225000 (44%)] Loss: 7081.123047\n",
      "Train Epoch: 306 [102656/225000 (46%)] Loss: 6921.572266\n",
      "Train Epoch: 306 [106752/225000 (47%)] Loss: 6961.357422\n",
      "Train Epoch: 306 [110848/225000 (49%)] Loss: 7092.412109\n",
      "Train Epoch: 306 [114944/225000 (51%)] Loss: 6966.767578\n",
      "Train Epoch: 306 [119040/225000 (53%)] Loss: 6917.728516\n",
      "Train Epoch: 306 [123136/225000 (55%)] Loss: 6913.183594\n",
      "Train Epoch: 306 [127232/225000 (57%)] Loss: 6824.062500\n",
      "Train Epoch: 306 [131328/225000 (58%)] Loss: 6745.644531\n",
      "Train Epoch: 306 [135424/225000 (60%)] Loss: 6785.736328\n",
      "Train Epoch: 306 [139520/225000 (62%)] Loss: 6834.197266\n",
      "Train Epoch: 306 [143616/225000 (64%)] Loss: 6949.998047\n",
      "Train Epoch: 306 [147712/225000 (66%)] Loss: 6795.951172\n",
      "Train Epoch: 306 [151808/225000 (67%)] Loss: 6982.281250\n",
      "Train Epoch: 306 [155904/225000 (69%)] Loss: 6981.300781\n",
      "Train Epoch: 306 [160000/225000 (71%)] Loss: 6970.755859\n",
      "Train Epoch: 306 [164096/225000 (73%)] Loss: 7094.642578\n",
      "Train Epoch: 306 [168192/225000 (75%)] Loss: 7044.628906\n",
      "Train Epoch: 306 [172288/225000 (77%)] Loss: 7104.707031\n",
      "Train Epoch: 306 [176384/225000 (78%)] Loss: 14296.796875\n",
      "Train Epoch: 306 [180480/225000 (80%)] Loss: 6821.269531\n",
      "Train Epoch: 306 [184576/225000 (82%)] Loss: 7043.759766\n",
      "Train Epoch: 306 [188672/225000 (84%)] Loss: 6940.207031\n",
      "Train Epoch: 306 [192768/225000 (86%)] Loss: 6888.837891\n",
      "Train Epoch: 306 [196864/225000 (87%)] Loss: 7040.580078\n",
      "Train Epoch: 306 [200960/225000 (89%)] Loss: 6917.773438\n",
      "Train Epoch: 306 [205056/225000 (91%)] Loss: 6850.230469\n",
      "Train Epoch: 306 [209152/225000 (93%)] Loss: 7070.544922\n",
      "Train Epoch: 306 [213248/225000 (95%)] Loss: 6932.533203\n",
      "Train Epoch: 306 [217344/225000 (97%)] Loss: 7015.117188\n",
      "Train Epoch: 306 [221440/225000 (98%)] Loss: 7019.812500\n",
      "    epoch          : 306\n",
      "    loss           : 6973.833381106016\n",
      "    val_loss       : 6948.53675555453\n",
      "Train Epoch: 307 [256/225000 (0%)] Loss: 6798.816406\n",
      "Train Epoch: 307 [4352/225000 (2%)] Loss: 7005.830078\n",
      "Train Epoch: 307 [8448/225000 (4%)] Loss: 6949.214844\n",
      "Train Epoch: 307 [12544/225000 (6%)] Loss: 7106.675781\n",
      "Train Epoch: 307 [16640/225000 (7%)] Loss: 6775.259766\n",
      "Train Epoch: 307 [20736/225000 (9%)] Loss: 6990.197266\n",
      "Train Epoch: 307 [24832/225000 (11%)] Loss: 7039.476562\n",
      "Train Epoch: 307 [28928/225000 (13%)] Loss: 7033.699219\n",
      "Train Epoch: 307 [33024/225000 (15%)] Loss: 6902.007812\n",
      "Train Epoch: 307 [37120/225000 (16%)] Loss: 6996.322266\n",
      "Train Epoch: 307 [41216/225000 (18%)] Loss: 6982.289062\n",
      "Train Epoch: 307 [45312/225000 (20%)] Loss: 7087.779297\n",
      "Train Epoch: 307 [49408/225000 (22%)] Loss: 6994.324219\n",
      "Train Epoch: 307 [53504/225000 (24%)] Loss: 6997.455078\n",
      "Train Epoch: 307 [57600/225000 (26%)] Loss: 6971.035156\n",
      "Train Epoch: 307 [61696/225000 (27%)] Loss: 6940.863281\n",
      "Train Epoch: 307 [65792/225000 (29%)] Loss: 6897.205078\n",
      "Train Epoch: 307 [69888/225000 (31%)] Loss: 7076.216797\n",
      "Train Epoch: 307 [73984/225000 (33%)] Loss: 6716.464844\n",
      "Train Epoch: 307 [78080/225000 (35%)] Loss: 7005.103516\n",
      "Train Epoch: 307 [82176/225000 (37%)] Loss: 7163.099609\n",
      "Train Epoch: 307 [86272/225000 (38%)] Loss: 7030.896484\n",
      "Train Epoch: 307 [90368/225000 (40%)] Loss: 6893.343750\n",
      "Train Epoch: 307 [94464/225000 (42%)] Loss: 6863.261719\n",
      "Train Epoch: 307 [98560/225000 (44%)] Loss: 6982.890625\n",
      "Train Epoch: 307 [102656/225000 (46%)] Loss: 6920.324219\n",
      "Train Epoch: 307 [106752/225000 (47%)] Loss: 6815.189453\n",
      "Train Epoch: 307 [110848/225000 (49%)] Loss: 7006.316406\n",
      "Train Epoch: 307 [114944/225000 (51%)] Loss: 6832.419922\n",
      "Train Epoch: 307 [119040/225000 (53%)] Loss: 7076.679688\n",
      "Train Epoch: 307 [123136/225000 (55%)] Loss: 6941.148438\n",
      "Train Epoch: 307 [127232/225000 (57%)] Loss: 6968.292969\n",
      "Train Epoch: 307 [131328/225000 (58%)] Loss: 6878.214844\n",
      "Train Epoch: 307 [135424/225000 (60%)] Loss: 7093.927734\n",
      "Train Epoch: 307 [139520/225000 (62%)] Loss: 6972.917969\n",
      "Train Epoch: 307 [143616/225000 (64%)] Loss: 6899.855469\n",
      "Train Epoch: 307 [147712/225000 (66%)] Loss: 6884.234375\n",
      "Train Epoch: 307 [151808/225000 (67%)] Loss: 6840.580078\n",
      "Train Epoch: 307 [155904/225000 (69%)] Loss: 6841.076172\n",
      "Train Epoch: 307 [160000/225000 (71%)] Loss: 6858.222656\n",
      "Train Epoch: 307 [164096/225000 (73%)] Loss: 7007.214844\n",
      "Train Epoch: 307 [168192/225000 (75%)] Loss: 6912.328125\n",
      "Train Epoch: 307 [172288/225000 (77%)] Loss: 6944.537109\n",
      "Train Epoch: 307 [176384/225000 (78%)] Loss: 7112.214844\n",
      "Train Epoch: 307 [180480/225000 (80%)] Loss: 6961.240234\n",
      "Train Epoch: 307 [184576/225000 (82%)] Loss: 7069.470703\n",
      "Train Epoch: 307 [188672/225000 (84%)] Loss: 6886.185547\n",
      "Train Epoch: 307 [192768/225000 (86%)] Loss: 6776.656250\n",
      "Train Epoch: 307 [196864/225000 (87%)] Loss: 6810.589844\n",
      "Train Epoch: 307 [200960/225000 (89%)] Loss: 6947.281250\n",
      "Train Epoch: 307 [205056/225000 (91%)] Loss: 7070.289062\n",
      "Train Epoch: 307 [209152/225000 (93%)] Loss: 7001.404297\n",
      "Train Epoch: 307 [213248/225000 (95%)] Loss: 7022.552734\n",
      "Train Epoch: 307 [217344/225000 (97%)] Loss: 6968.312500\n",
      "Train Epoch: 307 [221440/225000 (98%)] Loss: 7008.113281\n",
      "    epoch          : 307\n",
      "    loss           : 6967.557145015643\n",
      "    val_loss       : 7034.14260786346\n",
      "Train Epoch: 308 [256/225000 (0%)] Loss: 7029.554688\n",
      "Train Epoch: 308 [4352/225000 (2%)] Loss: 6939.138672\n",
      "Train Epoch: 308 [8448/225000 (4%)] Loss: 6938.292969\n",
      "Train Epoch: 308 [12544/225000 (6%)] Loss: 6899.994141\n",
      "Train Epoch: 308 [16640/225000 (7%)] Loss: 7037.730469\n",
      "Train Epoch: 308 [20736/225000 (9%)] Loss: 6938.144531\n",
      "Train Epoch: 308 [24832/225000 (11%)] Loss: 6849.654297\n",
      "Train Epoch: 308 [28928/225000 (13%)] Loss: 7122.089844\n",
      "Train Epoch: 308 [33024/225000 (15%)] Loss: 6910.289062\n",
      "Train Epoch: 308 [37120/225000 (16%)] Loss: 7100.511719\n",
      "Train Epoch: 308 [41216/225000 (18%)] Loss: 6913.222656\n",
      "Train Epoch: 308 [45312/225000 (20%)] Loss: 6915.375000\n",
      "Train Epoch: 308 [49408/225000 (22%)] Loss: 6848.642578\n",
      "Train Epoch: 308 [53504/225000 (24%)] Loss: 6883.646484\n",
      "Train Epoch: 308 [57600/225000 (26%)] Loss: 6792.646484\n",
      "Train Epoch: 308 [61696/225000 (27%)] Loss: 6807.724609\n",
      "Train Epoch: 308 [65792/225000 (29%)] Loss: 6927.289062\n",
      "Train Epoch: 308 [69888/225000 (31%)] Loss: 6808.691406\n",
      "Train Epoch: 308 [73984/225000 (33%)] Loss: 6946.833984\n",
      "Train Epoch: 308 [78080/225000 (35%)] Loss: 6750.150391\n",
      "Train Epoch: 308 [82176/225000 (37%)] Loss: 6888.705078\n",
      "Train Epoch: 308 [86272/225000 (38%)] Loss: 7072.439453\n",
      "Train Epoch: 308 [90368/225000 (40%)] Loss: 6769.943359\n",
      "Train Epoch: 308 [94464/225000 (42%)] Loss: 6953.576172\n",
      "Train Epoch: 308 [98560/225000 (44%)] Loss: 6938.837891\n",
      "Train Epoch: 308 [102656/225000 (46%)] Loss: 7012.888672\n",
      "Train Epoch: 308 [106752/225000 (47%)] Loss: 6898.363281\n",
      "Train Epoch: 308 [110848/225000 (49%)] Loss: 6919.648438\n",
      "Train Epoch: 308 [114944/225000 (51%)] Loss: 6814.107422\n",
      "Train Epoch: 308 [119040/225000 (53%)] Loss: 7138.054688\n",
      "Train Epoch: 308 [123136/225000 (55%)] Loss: 6984.800781\n",
      "Train Epoch: 308 [127232/225000 (57%)] Loss: 6851.267578\n",
      "Train Epoch: 308 [131328/225000 (58%)] Loss: 6876.968750\n",
      "Train Epoch: 308 [135424/225000 (60%)] Loss: 6956.937500\n",
      "Train Epoch: 308 [139520/225000 (62%)] Loss: 6995.115234\n",
      "Train Epoch: 308 [143616/225000 (64%)] Loss: 6984.980469\n",
      "Train Epoch: 308 [147712/225000 (66%)] Loss: 6926.796875\n",
      "Train Epoch: 308 [151808/225000 (67%)] Loss: 6815.605469\n",
      "Train Epoch: 308 [155904/225000 (69%)] Loss: 6957.435547\n",
      "Train Epoch: 308 [160000/225000 (71%)] Loss: 6875.980469\n",
      "Train Epoch: 308 [164096/225000 (73%)] Loss: 6866.871094\n",
      "Train Epoch: 308 [168192/225000 (75%)] Loss: 6969.562500\n",
      "Train Epoch: 308 [172288/225000 (77%)] Loss: 6909.730469\n",
      "Train Epoch: 308 [176384/225000 (78%)] Loss: 6926.003906\n",
      "Train Epoch: 308 [180480/225000 (80%)] Loss: 7005.062500\n",
      "Train Epoch: 308 [184576/225000 (82%)] Loss: 6873.789062\n",
      "Train Epoch: 308 [188672/225000 (84%)] Loss: 6981.875000\n",
      "Train Epoch: 308 [192768/225000 (86%)] Loss: 6816.193359\n",
      "Train Epoch: 308 [196864/225000 (87%)] Loss: 6790.660156\n",
      "Train Epoch: 308 [200960/225000 (89%)] Loss: 6983.130859\n",
      "Train Epoch: 308 [205056/225000 (91%)] Loss: 6875.021484\n",
      "Train Epoch: 308 [209152/225000 (93%)] Loss: 7004.394531\n",
      "Train Epoch: 308 [213248/225000 (95%)] Loss: 7012.732422\n",
      "Train Epoch: 308 [217344/225000 (97%)] Loss: 7000.796875\n",
      "Train Epoch: 308 [221440/225000 (98%)] Loss: 6873.898438\n",
      "    epoch          : 308\n",
      "    loss           : 6935.900925012443\n",
      "    val_loss       : 6944.6846937756145\n",
      "Train Epoch: 309 [256/225000 (0%)] Loss: 7026.306641\n",
      "Train Epoch: 309 [4352/225000 (2%)] Loss: 7014.871094\n",
      "Train Epoch: 309 [8448/225000 (4%)] Loss: 6987.273438\n",
      "Train Epoch: 309 [12544/225000 (6%)] Loss: 7005.294922\n",
      "Train Epoch: 309 [16640/225000 (7%)] Loss: 7045.343750\n",
      "Train Epoch: 309 [20736/225000 (9%)] Loss: 6865.212891\n",
      "Train Epoch: 309 [24832/225000 (11%)] Loss: 7093.730469\n",
      "Train Epoch: 309 [28928/225000 (13%)] Loss: 6985.345703\n",
      "Train Epoch: 309 [33024/225000 (15%)] Loss: 6950.101562\n",
      "Train Epoch: 309 [37120/225000 (16%)] Loss: 6955.849609\n",
      "Train Epoch: 309 [41216/225000 (18%)] Loss: 7029.800781\n",
      "Train Epoch: 309 [45312/225000 (20%)] Loss: 7048.873047\n",
      "Train Epoch: 309 [49408/225000 (22%)] Loss: 6950.892578\n",
      "Train Epoch: 309 [53504/225000 (24%)] Loss: 6868.832031\n",
      "Train Epoch: 309 [57600/225000 (26%)] Loss: 7005.976562\n",
      "Train Epoch: 309 [61696/225000 (27%)] Loss: 6972.501953\n",
      "Train Epoch: 309 [65792/225000 (29%)] Loss: 7004.466797\n",
      "Train Epoch: 309 [69888/225000 (31%)] Loss: 6807.267578\n",
      "Train Epoch: 309 [73984/225000 (33%)] Loss: 6988.746094\n",
      "Train Epoch: 309 [78080/225000 (35%)] Loss: 6940.705078\n",
      "Train Epoch: 309 [82176/225000 (37%)] Loss: 6710.986328\n",
      "Train Epoch: 309 [86272/225000 (38%)] Loss: 6892.638672\n",
      "Train Epoch: 309 [90368/225000 (40%)] Loss: 6945.904297\n",
      "Train Epoch: 309 [94464/225000 (42%)] Loss: 6853.248047\n",
      "Train Epoch: 309 [98560/225000 (44%)] Loss: 6794.248047\n",
      "Train Epoch: 309 [102656/225000 (46%)] Loss: 6825.705078\n",
      "Train Epoch: 309 [106752/225000 (47%)] Loss: 6872.916016\n",
      "Train Epoch: 309 [110848/225000 (49%)] Loss: 6966.957031\n",
      "Train Epoch: 309 [114944/225000 (51%)] Loss: 6930.089844\n",
      "Train Epoch: 309 [119040/225000 (53%)] Loss: 6952.277344\n",
      "Train Epoch: 309 [123136/225000 (55%)] Loss: 6825.847656\n",
      "Train Epoch: 309 [127232/225000 (57%)] Loss: 6955.996094\n",
      "Train Epoch: 309 [131328/225000 (58%)] Loss: 6896.361328\n",
      "Train Epoch: 309 [135424/225000 (60%)] Loss: 6949.777344\n",
      "Train Epoch: 309 [139520/225000 (62%)] Loss: 6927.083984\n",
      "Train Epoch: 309 [143616/225000 (64%)] Loss: 6851.458984\n",
      "Train Epoch: 309 [147712/225000 (66%)] Loss: 6992.798828\n",
      "Train Epoch: 309 [151808/225000 (67%)] Loss: 6798.671875\n",
      "Train Epoch: 309 [155904/225000 (69%)] Loss: 6999.527344\n",
      "Train Epoch: 309 [160000/225000 (71%)] Loss: 6863.482422\n",
      "Train Epoch: 309 [164096/225000 (73%)] Loss: 7025.716797\n",
      "Train Epoch: 309 [168192/225000 (75%)] Loss: 6864.464844\n",
      "Train Epoch: 309 [172288/225000 (77%)] Loss: 6901.160156\n",
      "Train Epoch: 309 [176384/225000 (78%)] Loss: 6861.929688\n",
      "Train Epoch: 309 [180480/225000 (80%)] Loss: 6922.087891\n",
      "Train Epoch: 309 [184576/225000 (82%)] Loss: 6986.445312\n",
      "Train Epoch: 309 [188672/225000 (84%)] Loss: 6986.335938\n",
      "Train Epoch: 309 [192768/225000 (86%)] Loss: 6834.560547\n",
      "Train Epoch: 309 [196864/225000 (87%)] Loss: 7013.662109\n",
      "Train Epoch: 309 [200960/225000 (89%)] Loss: 6867.703125\n",
      "Train Epoch: 309 [205056/225000 (91%)] Loss: 6992.988281\n",
      "Train Epoch: 309 [209152/225000 (93%)] Loss: 6974.511719\n",
      "Train Epoch: 309 [213248/225000 (95%)] Loss: 6985.080078\n",
      "Train Epoch: 309 [217344/225000 (97%)] Loss: 6907.062500\n",
      "Train Epoch: 309 [221440/225000 (98%)] Loss: 7055.242188\n",
      "    epoch          : 309\n",
      "    loss           : 6952.960685304679\n",
      "    val_loss       : 7079.168672041017\n",
      "Train Epoch: 310 [256/225000 (0%)] Loss: 6939.886719\n",
      "Train Epoch: 310 [4352/225000 (2%)] Loss: 6944.677734\n",
      "Train Epoch: 310 [8448/225000 (4%)] Loss: 6876.792969\n",
      "Train Epoch: 310 [12544/225000 (6%)] Loss: 7097.291016\n",
      "Train Epoch: 310 [16640/225000 (7%)] Loss: 6888.511719\n",
      "Train Epoch: 310 [20736/225000 (9%)] Loss: 6927.962891\n",
      "Train Epoch: 310 [24832/225000 (11%)] Loss: 6947.322266\n",
      "Train Epoch: 310 [28928/225000 (13%)] Loss: 6928.929688\n",
      "Train Epoch: 310 [33024/225000 (15%)] Loss: 6939.964844\n",
      "Train Epoch: 310 [37120/225000 (16%)] Loss: 7135.583984\n",
      "Train Epoch: 310 [41216/225000 (18%)] Loss: 6935.830078\n",
      "Train Epoch: 310 [45312/225000 (20%)] Loss: 6890.144531\n",
      "Train Epoch: 310 [49408/225000 (22%)] Loss: 6975.175781\n",
      "Train Epoch: 310 [53504/225000 (24%)] Loss: 6869.267578\n",
      "Train Epoch: 310 [57600/225000 (26%)] Loss: 7028.482422\n",
      "Train Epoch: 310 [61696/225000 (27%)] Loss: 6939.623047\n",
      "Train Epoch: 310 [65792/225000 (29%)] Loss: 7176.439453\n",
      "Train Epoch: 310 [69888/225000 (31%)] Loss: 6944.273438\n",
      "Train Epoch: 310 [73984/225000 (33%)] Loss: 6894.078125\n",
      "Train Epoch: 310 [78080/225000 (35%)] Loss: 6927.537109\n",
      "Train Epoch: 310 [82176/225000 (37%)] Loss: 6763.615234\n",
      "Train Epoch: 310 [86272/225000 (38%)] Loss: 7051.224609\n",
      "Train Epoch: 310 [90368/225000 (40%)] Loss: 6927.361328\n",
      "Train Epoch: 310 [94464/225000 (42%)] Loss: 6852.601562\n",
      "Train Epoch: 310 [98560/225000 (44%)] Loss: 6783.542969\n",
      "Train Epoch: 310 [102656/225000 (46%)] Loss: 6765.017578\n",
      "Train Epoch: 310 [106752/225000 (47%)] Loss: 7019.957031\n",
      "Train Epoch: 310 [110848/225000 (49%)] Loss: 6840.845703\n",
      "Train Epoch: 310 [114944/225000 (51%)] Loss: 6835.603516\n",
      "Train Epoch: 310 [119040/225000 (53%)] Loss: 6976.238281\n",
      "Train Epoch: 310 [123136/225000 (55%)] Loss: 6977.333984\n",
      "Train Epoch: 310 [127232/225000 (57%)] Loss: 6965.589844\n",
      "Train Epoch: 310 [131328/225000 (58%)] Loss: 6867.701172\n",
      "Train Epoch: 310 [135424/225000 (60%)] Loss: 6878.542969\n",
      "Train Epoch: 310 [139520/225000 (62%)] Loss: 6937.365234\n",
      "Train Epoch: 310 [143616/225000 (64%)] Loss: 6759.113281\n",
      "Train Epoch: 310 [147712/225000 (66%)] Loss: 6919.765625\n",
      "Train Epoch: 310 [151808/225000 (67%)] Loss: 6899.560547\n",
      "Train Epoch: 310 [155904/225000 (69%)] Loss: 6928.337891\n",
      "Train Epoch: 310 [160000/225000 (71%)] Loss: 6949.691406\n",
      "Train Epoch: 310 [164096/225000 (73%)] Loss: 6917.615234\n",
      "Train Epoch: 310 [168192/225000 (75%)] Loss: 6786.716797\n",
      "Train Epoch: 310 [172288/225000 (77%)] Loss: 6934.232422\n",
      "Train Epoch: 310 [176384/225000 (78%)] Loss: 6918.898438\n",
      "Train Epoch: 310 [180480/225000 (80%)] Loss: 6829.341797\n",
      "Train Epoch: 310 [184576/225000 (82%)] Loss: 6822.220703\n",
      "Train Epoch: 310 [188672/225000 (84%)] Loss: 6878.296875\n",
      "Train Epoch: 310 [192768/225000 (86%)] Loss: 6823.109375\n",
      "Train Epoch: 310 [196864/225000 (87%)] Loss: 6866.070312\n",
      "Train Epoch: 310 [200960/225000 (89%)] Loss: 6957.302734\n",
      "Train Epoch: 310 [205056/225000 (91%)] Loss: 6860.732422\n",
      "Train Epoch: 310 [209152/225000 (93%)] Loss: 6969.037109\n",
      "Train Epoch: 310 [213248/225000 (95%)] Loss: 6876.970703\n",
      "Train Epoch: 310 [217344/225000 (97%)] Loss: 7050.138672\n",
      "Train Epoch: 310 [221440/225000 (98%)] Loss: 7019.060547\n",
      "    epoch          : 310\n",
      "    loss           : 6948.070821334613\n",
      "    val_loss       : 6940.734817188613\n",
      "Train Epoch: 311 [256/225000 (0%)] Loss: 6915.951172\n",
      "Train Epoch: 311 [4352/225000 (2%)] Loss: 6727.189453\n",
      "Train Epoch: 311 [8448/225000 (4%)] Loss: 6849.656250\n",
      "Train Epoch: 311 [12544/225000 (6%)] Loss: 6965.869141\n",
      "Train Epoch: 311 [16640/225000 (7%)] Loss: 6866.222656\n",
      "Train Epoch: 311 [20736/225000 (9%)] Loss: 7034.238281\n",
      "Train Epoch: 311 [24832/225000 (11%)] Loss: 6811.644531\n",
      "Train Epoch: 311 [28928/225000 (13%)] Loss: 6938.716797\n",
      "Train Epoch: 311 [33024/225000 (15%)] Loss: 6817.080078\n",
      "Train Epoch: 311 [37120/225000 (16%)] Loss: 6823.318359\n",
      "Train Epoch: 311 [41216/225000 (18%)] Loss: 7008.968750\n",
      "Train Epoch: 311 [45312/225000 (20%)] Loss: 7031.408203\n",
      "Train Epoch: 311 [49408/225000 (22%)] Loss: 7107.119141\n",
      "Train Epoch: 311 [53504/225000 (24%)] Loss: 6990.720703\n",
      "Train Epoch: 311 [57600/225000 (26%)] Loss: 6891.853516\n",
      "Train Epoch: 311 [61696/225000 (27%)] Loss: 6870.341797\n",
      "Train Epoch: 311 [65792/225000 (29%)] Loss: 6753.685547\n",
      "Train Epoch: 311 [69888/225000 (31%)] Loss: 6911.140625\n",
      "Train Epoch: 311 [73984/225000 (33%)] Loss: 6812.849609\n",
      "Train Epoch: 311 [78080/225000 (35%)] Loss: 6773.945312\n",
      "Train Epoch: 311 [82176/225000 (37%)] Loss: 6932.488281\n",
      "Train Epoch: 311 [86272/225000 (38%)] Loss: 7021.677734\n",
      "Train Epoch: 311 [90368/225000 (40%)] Loss: 7010.957031\n",
      "Train Epoch: 311 [94464/225000 (42%)] Loss: 6973.992188\n",
      "Train Epoch: 311 [98560/225000 (44%)] Loss: 6862.558594\n",
      "Train Epoch: 311 [102656/225000 (46%)] Loss: 6904.152344\n",
      "Train Epoch: 311 [106752/225000 (47%)] Loss: 6979.117188\n",
      "Train Epoch: 311 [110848/225000 (49%)] Loss: 7051.990234\n",
      "Train Epoch: 311 [114944/225000 (51%)] Loss: 7092.154297\n",
      "Train Epoch: 311 [119040/225000 (53%)] Loss: 6834.529297\n",
      "Train Epoch: 311 [123136/225000 (55%)] Loss: 6883.742188\n",
      "Train Epoch: 311 [127232/225000 (57%)] Loss: 6890.400391\n",
      "Train Epoch: 311 [131328/225000 (58%)] Loss: 6847.988281\n",
      "Train Epoch: 311 [135424/225000 (60%)] Loss: 7104.878906\n",
      "Train Epoch: 311 [139520/225000 (62%)] Loss: 7124.822266\n",
      "Train Epoch: 311 [143616/225000 (64%)] Loss: 6965.339844\n",
      "Train Epoch: 311 [147712/225000 (66%)] Loss: 6937.451172\n",
      "Train Epoch: 311 [151808/225000 (67%)] Loss: 6983.572266\n",
      "Train Epoch: 311 [155904/225000 (69%)] Loss: 6872.152344\n",
      "Train Epoch: 311 [160000/225000 (71%)] Loss: 6917.914062\n",
      "Train Epoch: 311 [164096/225000 (73%)] Loss: 6710.978516\n",
      "Train Epoch: 311 [168192/225000 (75%)] Loss: 6987.916016\n",
      "Train Epoch: 311 [172288/225000 (77%)] Loss: 6840.402344\n",
      "Train Epoch: 311 [176384/225000 (78%)] Loss: 6879.248047\n",
      "Train Epoch: 311 [180480/225000 (80%)] Loss: 6994.890625\n",
      "Train Epoch: 311 [184576/225000 (82%)] Loss: 6984.162109\n",
      "Train Epoch: 311 [188672/225000 (84%)] Loss: 6907.814453\n",
      "Train Epoch: 311 [192768/225000 (86%)] Loss: 6895.027344\n",
      "Train Epoch: 311 [196864/225000 (87%)] Loss: 6818.417969\n",
      "Train Epoch: 311 [200960/225000 (89%)] Loss: 6941.146484\n",
      "Train Epoch: 311 [205056/225000 (91%)] Loss: 6781.009766\n",
      "Train Epoch: 311 [209152/225000 (93%)] Loss: 7027.429688\n",
      "Train Epoch: 311 [213248/225000 (95%)] Loss: 6856.970703\n",
      "Train Epoch: 311 [217344/225000 (97%)] Loss: 7009.072266\n",
      "Train Epoch: 311 [221440/225000 (98%)] Loss: 7096.054688\n",
      "    epoch          : 311\n",
      "    loss           : 6938.8358619525025\n",
      "    val_loss       : 7076.381882879199\n",
      "Train Epoch: 312 [256/225000 (0%)] Loss: 6950.634766\n",
      "Train Epoch: 312 [4352/225000 (2%)] Loss: 6911.576172\n",
      "Train Epoch: 312 [8448/225000 (4%)] Loss: 6853.410156\n",
      "Train Epoch: 312 [12544/225000 (6%)] Loss: 6961.318359\n",
      "Train Epoch: 312 [16640/225000 (7%)] Loss: 6861.466797\n",
      "Train Epoch: 312 [20736/225000 (9%)] Loss: 6955.492188\n",
      "Train Epoch: 312 [24832/225000 (11%)] Loss: 6999.228516\n",
      "Train Epoch: 312 [28928/225000 (13%)] Loss: 6784.482422\n",
      "Train Epoch: 312 [33024/225000 (15%)] Loss: 6937.193359\n",
      "Train Epoch: 312 [37120/225000 (16%)] Loss: 6775.816406\n",
      "Train Epoch: 312 [41216/225000 (18%)] Loss: 7006.093750\n",
      "Train Epoch: 312 [45312/225000 (20%)] Loss: 6897.828125\n",
      "Train Epoch: 312 [49408/225000 (22%)] Loss: 6804.492188\n",
      "Train Epoch: 312 [53504/225000 (24%)] Loss: 6986.796875\n",
      "Train Epoch: 312 [57600/225000 (26%)] Loss: 6817.980469\n",
      "Train Epoch: 312 [61696/225000 (27%)] Loss: 6906.060547\n",
      "Train Epoch: 312 [65792/225000 (29%)] Loss: 7013.626953\n",
      "Train Epoch: 312 [69888/225000 (31%)] Loss: 7025.515625\n",
      "Train Epoch: 312 [73984/225000 (33%)] Loss: 6978.039062\n",
      "Train Epoch: 312 [78080/225000 (35%)] Loss: 6858.898438\n",
      "Train Epoch: 312 [82176/225000 (37%)] Loss: 6977.816406\n",
      "Train Epoch: 312 [86272/225000 (38%)] Loss: 6892.601562\n",
      "Train Epoch: 312 [90368/225000 (40%)] Loss: 6845.126953\n",
      "Train Epoch: 312 [94464/225000 (42%)] Loss: 6951.847656\n",
      "Train Epoch: 312 [98560/225000 (44%)] Loss: 7026.689453\n",
      "Train Epoch: 312 [102656/225000 (46%)] Loss: 6877.634766\n",
      "Train Epoch: 312 [106752/225000 (47%)] Loss: 6960.851562\n",
      "Train Epoch: 312 [110848/225000 (49%)] Loss: 6926.375000\n",
      "Train Epoch: 312 [114944/225000 (51%)] Loss: 6993.832031\n",
      "Train Epoch: 312 [119040/225000 (53%)] Loss: 6996.205078\n",
      "Train Epoch: 312 [123136/225000 (55%)] Loss: 6921.212891\n",
      "Train Epoch: 312 [127232/225000 (57%)] Loss: 6957.927734\n",
      "Train Epoch: 312 [131328/225000 (58%)] Loss: 6871.183594\n",
      "Train Epoch: 312 [135424/225000 (60%)] Loss: 6865.962891\n",
      "Train Epoch: 312 [139520/225000 (62%)] Loss: 6920.035156\n",
      "Train Epoch: 312 [143616/225000 (64%)] Loss: 6943.703125\n",
      "Train Epoch: 312 [147712/225000 (66%)] Loss: 6894.025391\n",
      "Train Epoch: 312 [151808/225000 (67%)] Loss: 6847.027344\n",
      "Train Epoch: 312 [155904/225000 (69%)] Loss: 6948.798828\n",
      "Train Epoch: 312 [160000/225000 (71%)] Loss: 6929.039062\n",
      "Train Epoch: 312 [164096/225000 (73%)] Loss: 6835.638672\n",
      "Train Epoch: 312 [168192/225000 (75%)] Loss: 6949.472656\n",
      "Train Epoch: 312 [172288/225000 (77%)] Loss: 6811.507812\n",
      "Train Epoch: 312 [176384/225000 (78%)] Loss: 6983.794922\n",
      "Train Epoch: 312 [180480/225000 (80%)] Loss: 6885.341797\n",
      "Train Epoch: 312 [184576/225000 (82%)] Loss: 6959.794922\n",
      "Train Epoch: 312 [188672/225000 (84%)] Loss: 6974.986328\n",
      "Train Epoch: 312 [192768/225000 (86%)] Loss: 6796.404297\n",
      "Train Epoch: 312 [196864/225000 (87%)] Loss: 6775.421875\n",
      "Train Epoch: 312 [200960/225000 (89%)] Loss: 6902.187500\n",
      "Train Epoch: 312 [205056/225000 (91%)] Loss: 6994.056641\n",
      "Train Epoch: 312 [209152/225000 (93%)] Loss: 7009.687500\n",
      "Train Epoch: 312 [213248/225000 (95%)] Loss: 6956.115234\n",
      "Train Epoch: 312 [217344/225000 (97%)] Loss: 6947.515625\n",
      "Train Epoch: 312 [221440/225000 (98%)] Loss: 6896.980469\n",
      "    epoch          : 312\n",
      "    loss           : 6935.131643735779\n",
      "    val_loss       : 7013.35488097765\n",
      "Train Epoch: 313 [256/225000 (0%)] Loss: 6927.164062\n",
      "Train Epoch: 313 [4352/225000 (2%)] Loss: 6738.378906\n",
      "Train Epoch: 313 [8448/225000 (4%)] Loss: 7004.072266\n",
      "Train Epoch: 313 [12544/225000 (6%)] Loss: 7066.978516\n",
      "Train Epoch: 313 [16640/225000 (7%)] Loss: 6872.863281\n",
      "Train Epoch: 313 [20736/225000 (9%)] Loss: 7008.154297\n",
      "Train Epoch: 313 [24832/225000 (11%)] Loss: 6943.507812\n",
      "Train Epoch: 313 [28928/225000 (13%)] Loss: 6904.404297\n",
      "Train Epoch: 313 [33024/225000 (15%)] Loss: 6850.898438\n",
      "Train Epoch: 313 [37120/225000 (16%)] Loss: 6884.867188\n",
      "Train Epoch: 313 [41216/225000 (18%)] Loss: 6860.703125\n",
      "Train Epoch: 313 [45312/225000 (20%)] Loss: 7033.093750\n",
      "Train Epoch: 313 [49408/225000 (22%)] Loss: 6883.828125\n",
      "Train Epoch: 313 [53504/225000 (24%)] Loss: 6911.300781\n",
      "Train Epoch: 313 [57600/225000 (26%)] Loss: 6902.884766\n",
      "Train Epoch: 313 [61696/225000 (27%)] Loss: 6917.943359\n",
      "Train Epoch: 313 [65792/225000 (29%)] Loss: 6980.611328\n",
      "Train Epoch: 313 [69888/225000 (31%)] Loss: 7071.949219\n",
      "Train Epoch: 313 [73984/225000 (33%)] Loss: 6857.917969\n",
      "Train Epoch: 313 [78080/225000 (35%)] Loss: 7053.205078\n",
      "Train Epoch: 313 [82176/225000 (37%)] Loss: 6797.546875\n",
      "Train Epoch: 313 [86272/225000 (38%)] Loss: 6976.644531\n",
      "Train Epoch: 313 [90368/225000 (40%)] Loss: 6899.070312\n",
      "Train Epoch: 313 [94464/225000 (42%)] Loss: 6981.388672\n",
      "Train Epoch: 313 [98560/225000 (44%)] Loss: 6875.763672\n",
      "Train Epoch: 313 [102656/225000 (46%)] Loss: 6795.636719\n",
      "Train Epoch: 313 [106752/225000 (47%)] Loss: 6869.888672\n",
      "Train Epoch: 313 [110848/225000 (49%)] Loss: 7022.150391\n",
      "Train Epoch: 313 [114944/225000 (51%)] Loss: 6802.685547\n",
      "Train Epoch: 313 [119040/225000 (53%)] Loss: 7015.546875\n",
      "Train Epoch: 313 [123136/225000 (55%)] Loss: 6844.783203\n",
      "Train Epoch: 313 [127232/225000 (57%)] Loss: 6860.412109\n",
      "Train Epoch: 313 [131328/225000 (58%)] Loss: 6881.117188\n",
      "Train Epoch: 313 [135424/225000 (60%)] Loss: 6957.818359\n",
      "Train Epoch: 313 [139520/225000 (62%)] Loss: 7008.140625\n",
      "Train Epoch: 313 [143616/225000 (64%)] Loss: 6952.783203\n",
      "Train Epoch: 313 [147712/225000 (66%)] Loss: 6954.652344\n",
      "Train Epoch: 313 [151808/225000 (67%)] Loss: 6952.574219\n",
      "Train Epoch: 313 [155904/225000 (69%)] Loss: 6982.527344\n",
      "Train Epoch: 313 [160000/225000 (71%)] Loss: 6979.812500\n",
      "Train Epoch: 313 [164096/225000 (73%)] Loss: 6831.779297\n",
      "Train Epoch: 313 [168192/225000 (75%)] Loss: 6833.492188\n",
      "Train Epoch: 313 [172288/225000 (77%)] Loss: 6846.902344\n",
      "Train Epoch: 313 [176384/225000 (78%)] Loss: 6870.248047\n",
      "Train Epoch: 313 [180480/225000 (80%)] Loss: 6847.615234\n",
      "Train Epoch: 313 [184576/225000 (82%)] Loss: 6968.314453\n",
      "Train Epoch: 313 [188672/225000 (84%)] Loss: 7014.449219\n",
      "Train Epoch: 313 [192768/225000 (86%)] Loss: 6934.542969\n",
      "Train Epoch: 313 [196864/225000 (87%)] Loss: 6971.183594\n",
      "Train Epoch: 313 [200960/225000 (89%)] Loss: 6799.068359\n",
      "Train Epoch: 313 [205056/225000 (91%)] Loss: 7012.664062\n",
      "Train Epoch: 313 [209152/225000 (93%)] Loss: 7018.568359\n",
      "Train Epoch: 313 [213248/225000 (95%)] Loss: 6916.789062\n",
      "Train Epoch: 313 [217344/225000 (97%)] Loss: 7033.826172\n",
      "Train Epoch: 313 [221440/225000 (98%)] Loss: 7015.103516\n",
      "    epoch          : 313\n",
      "    loss           : 6958.219397708689\n",
      "    val_loss       : 6933.149311632526\n",
      "Train Epoch: 314 [256/225000 (0%)] Loss: 7162.792969\n",
      "Train Epoch: 314 [4352/225000 (2%)] Loss: 7045.259766\n",
      "Train Epoch: 314 [8448/225000 (4%)] Loss: 6689.832031\n",
      "Train Epoch: 314 [12544/225000 (6%)] Loss: 6828.746094\n",
      "Train Epoch: 314 [16640/225000 (7%)] Loss: 7076.816406\n",
      "Train Epoch: 314 [20736/225000 (9%)] Loss: 6931.500000\n",
      "Train Epoch: 314 [24832/225000 (11%)] Loss: 6857.111328\n",
      "Train Epoch: 314 [28928/225000 (13%)] Loss: 7006.306641\n",
      "Train Epoch: 314 [33024/225000 (15%)] Loss: 6977.246094\n",
      "Train Epoch: 314 [37120/225000 (16%)] Loss: 7012.943359\n",
      "Train Epoch: 314 [41216/225000 (18%)] Loss: 7028.341797\n",
      "Train Epoch: 314 [45312/225000 (20%)] Loss: 7009.988281\n",
      "Train Epoch: 314 [49408/225000 (22%)] Loss: 6908.589844\n",
      "Train Epoch: 314 [53504/225000 (24%)] Loss: 6997.314453\n",
      "Train Epoch: 314 [57600/225000 (26%)] Loss: 6775.316406\n",
      "Train Epoch: 314 [61696/225000 (27%)] Loss: 7019.017578\n",
      "Train Epoch: 314 [65792/225000 (29%)] Loss: 7046.535156\n",
      "Train Epoch: 314 [69888/225000 (31%)] Loss: 6984.763672\n",
      "Train Epoch: 314 [73984/225000 (33%)] Loss: 6813.919922\n",
      "Train Epoch: 314 [78080/225000 (35%)] Loss: 6908.500000\n",
      "Train Epoch: 314 [82176/225000 (37%)] Loss: 6927.263672\n",
      "Train Epoch: 314 [86272/225000 (38%)] Loss: 6942.970703\n",
      "Train Epoch: 314 [90368/225000 (40%)] Loss: 6906.392578\n",
      "Train Epoch: 314 [94464/225000 (42%)] Loss: 6894.007812\n",
      "Train Epoch: 314 [98560/225000 (44%)] Loss: 6874.626953\n",
      "Train Epoch: 314 [102656/225000 (46%)] Loss: 6813.214844\n",
      "Train Epoch: 314 [106752/225000 (47%)] Loss: 6877.517578\n",
      "Train Epoch: 314 [110848/225000 (49%)] Loss: 6986.537109\n",
      "Train Epoch: 314 [114944/225000 (51%)] Loss: 6820.693359\n",
      "Train Epoch: 314 [119040/225000 (53%)] Loss: 6771.464844\n",
      "Train Epoch: 314 [123136/225000 (55%)] Loss: 7058.894531\n",
      "Train Epoch: 314 [127232/225000 (57%)] Loss: 6864.431641\n",
      "Train Epoch: 314 [131328/225000 (58%)] Loss: 6819.738281\n",
      "Train Epoch: 314 [135424/225000 (60%)] Loss: 6813.826172\n",
      "Train Epoch: 314 [139520/225000 (62%)] Loss: 6933.263672\n",
      "Train Epoch: 314 [143616/225000 (64%)] Loss: 6782.824219\n",
      "Train Epoch: 314 [147712/225000 (66%)] Loss: 7051.742188\n",
      "Train Epoch: 314 [151808/225000 (67%)] Loss: 6855.343750\n",
      "Train Epoch: 314 [155904/225000 (69%)] Loss: 6917.634766\n",
      "Train Epoch: 314 [160000/225000 (71%)] Loss: 6850.609375\n",
      "Train Epoch: 314 [164096/225000 (73%)] Loss: 6975.593750\n",
      "Train Epoch: 314 [168192/225000 (75%)] Loss: 7057.169922\n",
      "Train Epoch: 314 [172288/225000 (77%)] Loss: 6901.962891\n",
      "Train Epoch: 314 [176384/225000 (78%)] Loss: 7013.197266\n",
      "Train Epoch: 314 [180480/225000 (80%)] Loss: 6868.998047\n",
      "Train Epoch: 314 [184576/225000 (82%)] Loss: 6943.984375\n",
      "Train Epoch: 314 [188672/225000 (84%)] Loss: 6903.556641\n",
      "Train Epoch: 314 [192768/225000 (86%)] Loss: 6961.781250\n",
      "Train Epoch: 314 [196864/225000 (87%)] Loss: 7059.972656\n",
      "Train Epoch: 314 [200960/225000 (89%)] Loss: 6912.337891\n",
      "Train Epoch: 314 [205056/225000 (91%)] Loss: 6879.509766\n",
      "Train Epoch: 314 [209152/225000 (93%)] Loss: 6918.093750\n",
      "Train Epoch: 314 [213248/225000 (95%)] Loss: 6844.457031\n",
      "Train Epoch: 314 [217344/225000 (97%)] Loss: 6946.900391\n",
      "Train Epoch: 314 [221440/225000 (98%)] Loss: 6939.699219\n",
      "    epoch          : 314\n",
      "    loss           : 6925.308635967719\n",
      "    val_loss       : 6934.368961762409\n",
      "Train Epoch: 315 [256/225000 (0%)] Loss: 6887.576172\n",
      "Train Epoch: 315 [4352/225000 (2%)] Loss: 6849.884766\n",
      "Train Epoch: 315 [8448/225000 (4%)] Loss: 6962.519531\n",
      "Train Epoch: 315 [12544/225000 (6%)] Loss: 7035.919922\n",
      "Train Epoch: 315 [16640/225000 (7%)] Loss: 6981.208984\n",
      "Train Epoch: 315 [20736/225000 (9%)] Loss: 6861.771484\n",
      "Train Epoch: 315 [24832/225000 (11%)] Loss: 7025.031250\n",
      "Train Epoch: 315 [28928/225000 (13%)] Loss: 7034.263672\n",
      "Train Epoch: 315 [33024/225000 (15%)] Loss: 6857.046875\n",
      "Train Epoch: 315 [37120/225000 (16%)] Loss: 7049.794922\n",
      "Train Epoch: 315 [41216/225000 (18%)] Loss: 6808.451172\n",
      "Train Epoch: 315 [45312/225000 (20%)] Loss: 6838.996094\n",
      "Train Epoch: 315 [49408/225000 (22%)] Loss: 6857.810547\n",
      "Train Epoch: 315 [53504/225000 (24%)] Loss: 6777.742188\n",
      "Train Epoch: 315 [57600/225000 (26%)] Loss: 6922.978516\n",
      "Train Epoch: 315 [61696/225000 (27%)] Loss: 6902.097656\n",
      "Train Epoch: 315 [65792/225000 (29%)] Loss: 7187.242188\n",
      "Train Epoch: 315 [69888/225000 (31%)] Loss: 6850.457031\n",
      "Train Epoch: 315 [73984/225000 (33%)] Loss: 6838.359375\n",
      "Train Epoch: 315 [78080/225000 (35%)] Loss: 6705.187500\n",
      "Train Epoch: 315 [82176/225000 (37%)] Loss: 6756.798828\n",
      "Train Epoch: 315 [86272/225000 (38%)] Loss: 7039.695312\n",
      "Train Epoch: 315 [90368/225000 (40%)] Loss: 6919.382812\n",
      "Train Epoch: 315 [94464/225000 (42%)] Loss: 7106.937500\n",
      "Train Epoch: 315 [98560/225000 (44%)] Loss: 6736.269531\n",
      "Train Epoch: 315 [102656/225000 (46%)] Loss: 6751.732422\n",
      "Train Epoch: 315 [106752/225000 (47%)] Loss: 6935.654297\n",
      "Train Epoch: 315 [110848/225000 (49%)] Loss: 6923.644531\n",
      "Train Epoch: 315 [114944/225000 (51%)] Loss: 7051.468750\n",
      "Train Epoch: 315 [119040/225000 (53%)] Loss: 6937.376953\n",
      "Train Epoch: 315 [123136/225000 (55%)] Loss: 6908.482422\n",
      "Train Epoch: 315 [127232/225000 (57%)] Loss: 6947.582031\n",
      "Train Epoch: 315 [131328/225000 (58%)] Loss: 7001.140625\n",
      "Train Epoch: 315 [135424/225000 (60%)] Loss: 6994.525391\n",
      "Train Epoch: 315 [139520/225000 (62%)] Loss: 7033.951172\n",
      "Train Epoch: 315 [143616/225000 (64%)] Loss: 6987.976562\n",
      "Train Epoch: 315 [147712/225000 (66%)] Loss: 6906.445312\n",
      "Train Epoch: 315 [151808/225000 (67%)] Loss: 7007.835938\n",
      "Train Epoch: 315 [155904/225000 (69%)] Loss: 6967.718750\n",
      "Train Epoch: 315 [160000/225000 (71%)] Loss: 6841.736328\n",
      "Train Epoch: 315 [164096/225000 (73%)] Loss: 6937.937500\n",
      "Train Epoch: 315 [168192/225000 (75%)] Loss: 6935.662109\n",
      "Train Epoch: 315 [172288/225000 (77%)] Loss: 6875.755859\n",
      "Train Epoch: 315 [176384/225000 (78%)] Loss: 6836.710938\n",
      "Train Epoch: 315 [180480/225000 (80%)] Loss: 6979.009766\n",
      "Train Epoch: 315 [184576/225000 (82%)] Loss: 6729.941406\n",
      "Train Epoch: 315 [188672/225000 (84%)] Loss: 6899.537109\n",
      "Train Epoch: 315 [192768/225000 (86%)] Loss: 6974.398438\n",
      "Train Epoch: 315 [196864/225000 (87%)] Loss: 7154.218750\n",
      "Train Epoch: 315 [200960/225000 (89%)] Loss: 6803.714844\n",
      "Train Epoch: 315 [205056/225000 (91%)] Loss: 6837.371094\n",
      "Train Epoch: 315 [209152/225000 (93%)] Loss: 6833.427734\n",
      "Train Epoch: 315 [213248/225000 (95%)] Loss: 6797.529297\n",
      "Train Epoch: 315 [217344/225000 (97%)] Loss: 7117.761719\n",
      "Train Epoch: 315 [221440/225000 (98%)] Loss: 6749.261719\n",
      "    epoch          : 315\n",
      "    loss           : 6938.720079858149\n",
      "    val_loss       : 6934.515758077709\n",
      "Train Epoch: 316 [256/225000 (0%)] Loss: 6946.337891\n",
      "Train Epoch: 316 [4352/225000 (2%)] Loss: 6938.785156\n",
      "Train Epoch: 316 [8448/225000 (4%)] Loss: 6836.275391\n",
      "Train Epoch: 316 [12544/225000 (6%)] Loss: 6861.017578\n",
      "Train Epoch: 316 [16640/225000 (7%)] Loss: 6911.482422\n",
      "Train Epoch: 316 [20736/225000 (9%)] Loss: 6736.988281\n",
      "Train Epoch: 316 [24832/225000 (11%)] Loss: 6919.205078\n",
      "Train Epoch: 316 [28928/225000 (13%)] Loss: 6794.855469\n",
      "Train Epoch: 316 [33024/225000 (15%)] Loss: 6867.484375\n",
      "Train Epoch: 316 [37120/225000 (16%)] Loss: 6908.779297\n",
      "Train Epoch: 316 [41216/225000 (18%)] Loss: 6801.699219\n",
      "Train Epoch: 316 [45312/225000 (20%)] Loss: 6789.148438\n",
      "Train Epoch: 316 [49408/225000 (22%)] Loss: 7081.525391\n",
      "Train Epoch: 316 [53504/225000 (24%)] Loss: 6958.326172\n",
      "Train Epoch: 316 [57600/225000 (26%)] Loss: 6938.152344\n",
      "Train Epoch: 316 [61696/225000 (27%)] Loss: 6891.716797\n",
      "Train Epoch: 316 [65792/225000 (29%)] Loss: 6824.832031\n",
      "Train Epoch: 316 [69888/225000 (31%)] Loss: 6910.216797\n",
      "Train Epoch: 316 [73984/225000 (33%)] Loss: 6837.281250\n",
      "Train Epoch: 316 [78080/225000 (35%)] Loss: 6828.271484\n",
      "Train Epoch: 316 [82176/225000 (37%)] Loss: 6905.875000\n",
      "Train Epoch: 316 [86272/225000 (38%)] Loss: 6875.876953\n",
      "Train Epoch: 316 [90368/225000 (40%)] Loss: 6894.582031\n",
      "Train Epoch: 316 [94464/225000 (42%)] Loss: 6824.306641\n",
      "Train Epoch: 316 [98560/225000 (44%)] Loss: 6892.292969\n",
      "Train Epoch: 316 [102656/225000 (46%)] Loss: 7036.814453\n",
      "Train Epoch: 316 [106752/225000 (47%)] Loss: 6885.750000\n",
      "Train Epoch: 316 [110848/225000 (49%)] Loss: 7010.197266\n",
      "Train Epoch: 316 [114944/225000 (51%)] Loss: 6943.480469\n",
      "Train Epoch: 316 [119040/225000 (53%)] Loss: 7045.501953\n",
      "Train Epoch: 316 [123136/225000 (55%)] Loss: 6973.869141\n",
      "Train Epoch: 316 [127232/225000 (57%)] Loss: 7056.187500\n",
      "Train Epoch: 316 [131328/225000 (58%)] Loss: 6990.755859\n",
      "Train Epoch: 316 [135424/225000 (60%)] Loss: 6846.689453\n",
      "Train Epoch: 316 [139520/225000 (62%)] Loss: 6888.437500\n",
      "Train Epoch: 316 [143616/225000 (64%)] Loss: 6963.363281\n",
      "Train Epoch: 316 [147712/225000 (66%)] Loss: 6986.585938\n",
      "Train Epoch: 316 [151808/225000 (67%)] Loss: 6826.421875\n",
      "Train Epoch: 316 [155904/225000 (69%)] Loss: 6833.919922\n",
      "Train Epoch: 316 [160000/225000 (71%)] Loss: 7015.458984\n",
      "Train Epoch: 316 [164096/225000 (73%)] Loss: 6917.556641\n",
      "Train Epoch: 316 [168192/225000 (75%)] Loss: 7131.947266\n",
      "Train Epoch: 316 [172288/225000 (77%)] Loss: 6945.728516\n",
      "Train Epoch: 316 [176384/225000 (78%)] Loss: 7024.353516\n",
      "Train Epoch: 316 [180480/225000 (80%)] Loss: 6989.835938\n",
      "Train Epoch: 316 [184576/225000 (82%)] Loss: 6979.976562\n",
      "Train Epoch: 316 [188672/225000 (84%)] Loss: 6985.162109\n",
      "Train Epoch: 316 [192768/225000 (86%)] Loss: 6813.908203\n",
      "Train Epoch: 316 [196864/225000 (87%)] Loss: 7009.566406\n",
      "Train Epoch: 316 [200960/225000 (89%)] Loss: 6776.445312\n",
      "Train Epoch: 316 [205056/225000 (91%)] Loss: 6939.099609\n",
      "Train Epoch: 316 [209152/225000 (93%)] Loss: 6924.289062\n",
      "Train Epoch: 316 [213248/225000 (95%)] Loss: 6997.482422\n",
      "Train Epoch: 316 [217344/225000 (97%)] Loss: 6951.400391\n",
      "Train Epoch: 316 [221440/225000 (98%)] Loss: 6998.765625\n",
      "    epoch          : 316\n",
      "    loss           : 6945.38551221203\n",
      "    val_loss       : 6933.313452637925\n",
      "Train Epoch: 317 [256/225000 (0%)] Loss: 6759.318359\n",
      "Train Epoch: 317 [4352/225000 (2%)] Loss: 7062.400391\n",
      "Train Epoch: 317 [8448/225000 (4%)] Loss: 6901.566406\n",
      "Train Epoch: 317 [12544/225000 (6%)] Loss: 6967.966797\n",
      "Train Epoch: 317 [16640/225000 (7%)] Loss: 6949.810547\n",
      "Train Epoch: 317 [20736/225000 (9%)] Loss: 7002.433594\n",
      "Train Epoch: 317 [24832/225000 (11%)] Loss: 6759.914062\n",
      "Train Epoch: 317 [28928/225000 (13%)] Loss: 6756.064453\n",
      "Train Epoch: 317 [33024/225000 (15%)] Loss: 7034.556641\n",
      "Train Epoch: 317 [37120/225000 (16%)] Loss: 7091.470703\n",
      "Train Epoch: 317 [41216/225000 (18%)] Loss: 7122.716797\n",
      "Train Epoch: 317 [45312/225000 (20%)] Loss: 6955.859375\n",
      "Train Epoch: 317 [49408/225000 (22%)] Loss: 6968.541016\n",
      "Train Epoch: 317 [53504/225000 (24%)] Loss: 6888.841797\n",
      "Train Epoch: 317 [57600/225000 (26%)] Loss: 6780.837891\n",
      "Train Epoch: 317 [61696/225000 (27%)] Loss: 7058.552734\n",
      "Train Epoch: 317 [65792/225000 (29%)] Loss: 7051.595703\n",
      "Train Epoch: 317 [69888/225000 (31%)] Loss: 7017.769531\n",
      "Train Epoch: 317 [73984/225000 (33%)] Loss: 6995.099609\n",
      "Train Epoch: 317 [78080/225000 (35%)] Loss: 6982.353516\n",
      "Train Epoch: 317 [82176/225000 (37%)] Loss: 6778.837891\n",
      "Train Epoch: 317 [86272/225000 (38%)] Loss: 6905.150391\n",
      "Train Epoch: 317 [90368/225000 (40%)] Loss: 6881.566406\n",
      "Train Epoch: 317 [94464/225000 (42%)] Loss: 6781.689453\n",
      "Train Epoch: 317 [98560/225000 (44%)] Loss: 6810.376953\n",
      "Train Epoch: 317 [102656/225000 (46%)] Loss: 6874.052734\n",
      "Train Epoch: 317 [106752/225000 (47%)] Loss: 6905.523438\n",
      "Train Epoch: 317 [110848/225000 (49%)] Loss: 6971.111328\n",
      "Train Epoch: 317 [114944/225000 (51%)] Loss: 6980.453125\n",
      "Train Epoch: 317 [119040/225000 (53%)] Loss: 6922.183594\n",
      "Train Epoch: 317 [123136/225000 (55%)] Loss: 6954.984375\n",
      "Train Epoch: 317 [127232/225000 (57%)] Loss: 7051.511719\n",
      "Train Epoch: 317 [131328/225000 (58%)] Loss: 6872.238281\n",
      "Train Epoch: 317 [135424/225000 (60%)] Loss: 7054.195312\n",
      "Train Epoch: 317 [139520/225000 (62%)] Loss: 6933.414062\n",
      "Train Epoch: 317 [143616/225000 (64%)] Loss: 6838.636719\n",
      "Train Epoch: 317 [147712/225000 (66%)] Loss: 6864.730469\n",
      "Train Epoch: 317 [151808/225000 (67%)] Loss: 6702.060547\n",
      "Train Epoch: 317 [155904/225000 (69%)] Loss: 7011.642578\n",
      "Train Epoch: 317 [160000/225000 (71%)] Loss: 7123.312500\n",
      "Train Epoch: 317 [164096/225000 (73%)] Loss: 6936.144531\n",
      "Train Epoch: 317 [168192/225000 (75%)] Loss: 6954.333984\n",
      "Train Epoch: 317 [172288/225000 (77%)] Loss: 7031.464844\n",
      "Train Epoch: 317 [176384/225000 (78%)] Loss: 6748.902344\n",
      "Train Epoch: 317 [180480/225000 (80%)] Loss: 7027.248047\n",
      "Train Epoch: 317 [184576/225000 (82%)] Loss: 7003.744141\n",
      "Train Epoch: 317 [188672/225000 (84%)] Loss: 6902.697266\n",
      "Train Epoch: 317 [192768/225000 (86%)] Loss: 6971.386719\n",
      "Train Epoch: 317 [196864/225000 (87%)] Loss: 6828.898438\n",
      "Train Epoch: 317 [200960/225000 (89%)] Loss: 6874.453125\n",
      "Train Epoch: 317 [205056/225000 (91%)] Loss: 6882.052734\n",
      "Train Epoch: 317 [209152/225000 (93%)] Loss: 6893.882812\n",
      "Train Epoch: 317 [213248/225000 (95%)] Loss: 6899.318359\n",
      "Train Epoch: 317 [217344/225000 (97%)] Loss: 12361.705078\n",
      "Train Epoch: 317 [221440/225000 (98%)] Loss: 7047.759766\n",
      "    epoch          : 317\n",
      "    loss           : 6963.17728220101\n",
      "    val_loss       : 6933.327062375692\n",
      "Train Epoch: 318 [256/225000 (0%)] Loss: 6840.099609\n",
      "Train Epoch: 318 [4352/225000 (2%)] Loss: 7049.242188\n",
      "Train Epoch: 318 [8448/225000 (4%)] Loss: 6913.837891\n",
      "Train Epoch: 318 [12544/225000 (6%)] Loss: 6941.980469\n",
      "Train Epoch: 318 [16640/225000 (7%)] Loss: 6900.728516\n",
      "Train Epoch: 318 [20736/225000 (9%)] Loss: 6896.486328\n",
      "Train Epoch: 318 [24832/225000 (11%)] Loss: 6840.660156\n",
      "Train Epoch: 318 [28928/225000 (13%)] Loss: 6815.451172\n",
      "Train Epoch: 318 [33024/225000 (15%)] Loss: 6920.451172\n",
      "Train Epoch: 318 [37120/225000 (16%)] Loss: 6885.962891\n",
      "Train Epoch: 318 [41216/225000 (18%)] Loss: 7022.062500\n",
      "Train Epoch: 318 [45312/225000 (20%)] Loss: 6946.396484\n",
      "Train Epoch: 318 [49408/225000 (22%)] Loss: 6986.976562\n",
      "Train Epoch: 318 [53504/225000 (24%)] Loss: 6904.541016\n",
      "Train Epoch: 318 [57600/225000 (26%)] Loss: 6968.416016\n",
      "Train Epoch: 318 [61696/225000 (27%)] Loss: 6911.732422\n",
      "Train Epoch: 318 [65792/225000 (29%)] Loss: 7020.130859\n",
      "Train Epoch: 318 [69888/225000 (31%)] Loss: 7005.552734\n",
      "Train Epoch: 318 [73984/225000 (33%)] Loss: 6874.992188\n",
      "Train Epoch: 318 [78080/225000 (35%)] Loss: 6933.496094\n",
      "Train Epoch: 318 [82176/225000 (37%)] Loss: 7016.226562\n",
      "Train Epoch: 318 [86272/225000 (38%)] Loss: 6898.912109\n",
      "Train Epoch: 318 [90368/225000 (40%)] Loss: 6851.982422\n",
      "Train Epoch: 318 [94464/225000 (42%)] Loss: 7170.855469\n",
      "Train Epoch: 318 [98560/225000 (44%)] Loss: 6817.416016\n",
      "Train Epoch: 318 [102656/225000 (46%)] Loss: 6943.539062\n",
      "Train Epoch: 318 [106752/225000 (47%)] Loss: 7063.449219\n",
      "Train Epoch: 318 [110848/225000 (49%)] Loss: 6965.617188\n",
      "Train Epoch: 318 [114944/225000 (51%)] Loss: 6998.964844\n",
      "Train Epoch: 318 [119040/225000 (53%)] Loss: 6850.226562\n",
      "Train Epoch: 318 [123136/225000 (55%)] Loss: 6803.427734\n",
      "Train Epoch: 318 [127232/225000 (57%)] Loss: 6886.410156\n",
      "Train Epoch: 318 [131328/225000 (58%)] Loss: 7173.804688\n",
      "Train Epoch: 318 [135424/225000 (60%)] Loss: 6869.244141\n",
      "Train Epoch: 318 [139520/225000 (62%)] Loss: 6890.937500\n",
      "Train Epoch: 318 [143616/225000 (64%)] Loss: 6992.343750\n",
      "Train Epoch: 318 [147712/225000 (66%)] Loss: 6756.968750\n",
      "Train Epoch: 318 [151808/225000 (67%)] Loss: 7100.720703\n",
      "Train Epoch: 318 [155904/225000 (69%)] Loss: 7132.994141\n",
      "Train Epoch: 318 [160000/225000 (71%)] Loss: 6880.160156\n",
      "Train Epoch: 318 [164096/225000 (73%)] Loss: 6950.039062\n",
      "Train Epoch: 318 [168192/225000 (75%)] Loss: 6965.607422\n",
      "Train Epoch: 318 [172288/225000 (77%)] Loss: 6861.222656\n",
      "Train Epoch: 318 [176384/225000 (78%)] Loss: 6936.392578\n",
      "Train Epoch: 318 [180480/225000 (80%)] Loss: 7013.199219\n",
      "Train Epoch: 318 [184576/225000 (82%)] Loss: 6791.294922\n",
      "Train Epoch: 318 [188672/225000 (84%)] Loss: 7065.833984\n",
      "Train Epoch: 318 [192768/225000 (86%)] Loss: 6764.208984\n",
      "Train Epoch: 318 [196864/225000 (87%)] Loss: 6825.902344\n",
      "Train Epoch: 318 [200960/225000 (89%)] Loss: 6993.974609\n",
      "Train Epoch: 318 [205056/225000 (91%)] Loss: 6924.140625\n",
      "Train Epoch: 318 [209152/225000 (93%)] Loss: 7123.419922\n",
      "Train Epoch: 318 [213248/225000 (95%)] Loss: 6922.865234\n",
      "Train Epoch: 318 [217344/225000 (97%)] Loss: 6988.707031\n",
      "Train Epoch: 318 [221440/225000 (98%)] Loss: 6947.302734\n",
      "    epoch          : 318\n",
      "    loss           : 6933.729132225896\n",
      "    val_loss       : 6928.537788814428\n",
      "Train Epoch: 319 [256/225000 (0%)] Loss: 6838.193359\n",
      "Train Epoch: 319 [4352/225000 (2%)] Loss: 6869.835938\n",
      "Train Epoch: 319 [8448/225000 (4%)] Loss: 6926.298828\n",
      "Train Epoch: 319 [12544/225000 (6%)] Loss: 6853.363281\n",
      "Train Epoch: 319 [16640/225000 (7%)] Loss: 7046.087891\n",
      "Train Epoch: 319 [20736/225000 (9%)] Loss: 6850.435547\n",
      "Train Epoch: 319 [24832/225000 (11%)] Loss: 6917.808594\n",
      "Train Epoch: 319 [28928/225000 (13%)] Loss: 6915.628906\n",
      "Train Epoch: 319 [33024/225000 (15%)] Loss: 6888.236328\n",
      "Train Epoch: 319 [37120/225000 (16%)] Loss: 7027.082031\n",
      "Train Epoch: 319 [41216/225000 (18%)] Loss: 6961.378906\n",
      "Train Epoch: 319 [45312/225000 (20%)] Loss: 7029.169922\n",
      "Train Epoch: 319 [49408/225000 (22%)] Loss: 6788.083984\n",
      "Train Epoch: 319 [53504/225000 (24%)] Loss: 7025.718750\n",
      "Train Epoch: 319 [57600/225000 (26%)] Loss: 6991.705078\n",
      "Train Epoch: 319 [61696/225000 (27%)] Loss: 6942.628906\n",
      "Train Epoch: 319 [65792/225000 (29%)] Loss: 6838.003906\n",
      "Train Epoch: 319 [69888/225000 (31%)] Loss: 6908.144531\n",
      "Train Epoch: 319 [73984/225000 (33%)] Loss: 6858.798828\n",
      "Train Epoch: 319 [78080/225000 (35%)] Loss: 7003.255859\n",
      "Train Epoch: 319 [82176/225000 (37%)] Loss: 6945.904297\n",
      "Train Epoch: 319 [86272/225000 (38%)] Loss: 7193.226562\n",
      "Train Epoch: 319 [90368/225000 (40%)] Loss: 6973.021484\n",
      "Train Epoch: 319 [94464/225000 (42%)] Loss: 6956.144531\n",
      "Train Epoch: 319 [98560/225000 (44%)] Loss: 7109.558594\n",
      "Train Epoch: 319 [102656/225000 (46%)] Loss: 6933.726562\n",
      "Train Epoch: 319 [106752/225000 (47%)] Loss: 6948.617188\n",
      "Train Epoch: 319 [110848/225000 (49%)] Loss: 6949.595703\n",
      "Train Epoch: 319 [114944/225000 (51%)] Loss: 6889.171875\n",
      "Train Epoch: 319 [119040/225000 (53%)] Loss: 6897.068359\n",
      "Train Epoch: 319 [123136/225000 (55%)] Loss: 6873.328125\n",
      "Train Epoch: 319 [127232/225000 (57%)] Loss: 6854.130859\n",
      "Train Epoch: 319 [131328/225000 (58%)] Loss: 6845.218750\n",
      "Train Epoch: 319 [135424/225000 (60%)] Loss: 6825.111328\n",
      "Train Epoch: 319 [139520/225000 (62%)] Loss: 6930.964844\n",
      "Train Epoch: 319 [143616/225000 (64%)] Loss: 7151.103516\n",
      "Train Epoch: 319 [147712/225000 (66%)] Loss: 6874.740234\n",
      "Train Epoch: 319 [151808/225000 (67%)] Loss: 6924.167969\n",
      "Train Epoch: 319 [155904/225000 (69%)] Loss: 6876.312500\n",
      "Train Epoch: 319 [160000/225000 (71%)] Loss: 6752.263672\n",
      "Train Epoch: 319 [164096/225000 (73%)] Loss: 6926.621094\n",
      "Train Epoch: 319 [168192/225000 (75%)] Loss: 6812.322266\n",
      "Train Epoch: 319 [172288/225000 (77%)] Loss: 6972.134766\n",
      "Train Epoch: 319 [176384/225000 (78%)] Loss: 6981.724609\n",
      "Train Epoch: 319 [180480/225000 (80%)] Loss: 6918.835938\n",
      "Train Epoch: 319 [184576/225000 (82%)] Loss: 6988.480469\n",
      "Train Epoch: 319 [188672/225000 (84%)] Loss: 6889.580078\n",
      "Train Epoch: 319 [192768/225000 (86%)] Loss: 6950.158203\n",
      "Train Epoch: 319 [196864/225000 (87%)] Loss: 6957.556641\n",
      "Train Epoch: 319 [200960/225000 (89%)] Loss: 7072.691406\n",
      "Train Epoch: 319 [205056/225000 (91%)] Loss: 6734.964844\n",
      "Train Epoch: 319 [209152/225000 (93%)] Loss: 6948.832031\n",
      "Train Epoch: 319 [213248/225000 (95%)] Loss: 6927.447266\n",
      "Train Epoch: 319 [217344/225000 (97%)] Loss: 7022.169922\n",
      "Train Epoch: 319 [221440/225000 (98%)] Loss: 6880.421875\n",
      "    epoch          : 319\n",
      "    loss           : 6940.820458040032\n",
      "    val_loss       : 6929.9362633909495\n",
      "Train Epoch: 320 [256/225000 (0%)] Loss: 6928.509766\n",
      "Train Epoch: 320 [4352/225000 (2%)] Loss: 6865.804688\n",
      "Train Epoch: 320 [8448/225000 (4%)] Loss: 6856.173828\n",
      "Train Epoch: 320 [12544/225000 (6%)] Loss: 7066.550781\n",
      "Train Epoch: 320 [16640/225000 (7%)] Loss: 6860.455078\n",
      "Train Epoch: 320 [20736/225000 (9%)] Loss: 6951.017578\n",
      "Train Epoch: 320 [24832/225000 (11%)] Loss: 6981.453125\n",
      "Train Epoch: 320 [28928/225000 (13%)] Loss: 6934.455078\n",
      "Train Epoch: 320 [33024/225000 (15%)] Loss: 7001.886719\n",
      "Train Epoch: 320 [37120/225000 (16%)] Loss: 6811.208984\n",
      "Train Epoch: 320 [41216/225000 (18%)] Loss: 7025.488281\n",
      "Train Epoch: 320 [45312/225000 (20%)] Loss: 6991.910156\n",
      "Train Epoch: 320 [49408/225000 (22%)] Loss: 6949.806641\n",
      "Train Epoch: 320 [53504/225000 (24%)] Loss: 6908.472656\n",
      "Train Epoch: 320 [57600/225000 (26%)] Loss: 6959.263672\n",
      "Train Epoch: 320 [61696/225000 (27%)] Loss: 6918.378906\n",
      "Train Epoch: 320 [65792/225000 (29%)] Loss: 6996.234375\n",
      "Train Epoch: 320 [69888/225000 (31%)] Loss: 6968.810547\n",
      "Train Epoch: 320 [73984/225000 (33%)] Loss: 6934.072266\n",
      "Train Epoch: 320 [78080/225000 (35%)] Loss: 7012.818359\n",
      "Train Epoch: 320 [82176/225000 (37%)] Loss: 6926.130859\n",
      "Train Epoch: 320 [86272/225000 (38%)] Loss: 6867.367188\n",
      "Train Epoch: 320 [90368/225000 (40%)] Loss: 6989.154297\n",
      "Train Epoch: 320 [94464/225000 (42%)] Loss: 6899.529297\n",
      "Train Epoch: 320 [98560/225000 (44%)] Loss: 6985.904297\n",
      "Train Epoch: 320 [102656/225000 (46%)] Loss: 6867.271484\n",
      "Train Epoch: 320 [106752/225000 (47%)] Loss: 6919.740234\n",
      "Train Epoch: 320 [110848/225000 (49%)] Loss: 6810.843750\n",
      "Train Epoch: 320 [114944/225000 (51%)] Loss: 6821.857422\n",
      "Train Epoch: 320 [119040/225000 (53%)] Loss: 6983.837891\n",
      "Train Epoch: 320 [123136/225000 (55%)] Loss: 6973.373047\n",
      "Train Epoch: 320 [127232/225000 (57%)] Loss: 6885.423828\n",
      "Train Epoch: 320 [131328/225000 (58%)] Loss: 7039.947266\n",
      "Train Epoch: 320 [135424/225000 (60%)] Loss: 6996.183594\n",
      "Train Epoch: 320 [139520/225000 (62%)] Loss: 6904.714844\n",
      "Train Epoch: 320 [143616/225000 (64%)] Loss: 7105.888672\n",
      "Train Epoch: 320 [147712/225000 (66%)] Loss: 6865.808594\n",
      "Train Epoch: 320 [151808/225000 (67%)] Loss: 6971.060547\n",
      "Train Epoch: 320 [155904/225000 (69%)] Loss: 6836.984375\n",
      "Train Epoch: 320 [160000/225000 (71%)] Loss: 6808.460938\n",
      "Train Epoch: 320 [164096/225000 (73%)] Loss: 7127.646484\n",
      "Train Epoch: 320 [168192/225000 (75%)] Loss: 6857.357422\n",
      "Train Epoch: 320 [172288/225000 (77%)] Loss: 6775.634766\n",
      "Train Epoch: 320 [176384/225000 (78%)] Loss: 6832.845703\n",
      "Train Epoch: 320 [180480/225000 (80%)] Loss: 6891.625000\n",
      "Train Epoch: 320 [184576/225000 (82%)] Loss: 7156.054688\n",
      "Train Epoch: 320 [188672/225000 (84%)] Loss: 6764.226562\n",
      "Train Epoch: 320 [192768/225000 (86%)] Loss: 6988.478516\n",
      "Train Epoch: 320 [196864/225000 (87%)] Loss: 6894.722656\n",
      "Train Epoch: 320 [200960/225000 (89%)] Loss: 6747.878906\n",
      "Train Epoch: 320 [205056/225000 (91%)] Loss: 6954.605469\n",
      "Train Epoch: 320 [209152/225000 (93%)] Loss: 6934.929688\n",
      "Train Epoch: 320 [213248/225000 (95%)] Loss: 7152.388672\n",
      "Train Epoch: 320 [217344/225000 (97%)] Loss: 6827.083984\n",
      "Train Epoch: 320 [221440/225000 (98%)] Loss: 6848.046875\n",
      "    epoch          : 320\n",
      "    loss           : 6930.468510025597\n",
      "    val_loss       : 6926.743618235296\n",
      "Train Epoch: 321 [256/225000 (0%)] Loss: 6922.708984\n",
      "Train Epoch: 321 [4352/225000 (2%)] Loss: 6828.119141\n",
      "Train Epoch: 321 [8448/225000 (4%)] Loss: 7018.152344\n",
      "Train Epoch: 321 [12544/225000 (6%)] Loss: 6841.894531\n",
      "Train Epoch: 321 [16640/225000 (7%)] Loss: 6702.570312\n",
      "Train Epoch: 321 [20736/225000 (9%)] Loss: 6786.841797\n",
      "Train Epoch: 321 [24832/225000 (11%)] Loss: 6864.363281\n",
      "Train Epoch: 321 [28928/225000 (13%)] Loss: 6872.380859\n",
      "Train Epoch: 321 [33024/225000 (15%)] Loss: 6968.275391\n",
      "Train Epoch: 321 [37120/225000 (16%)] Loss: 7089.888672\n",
      "Train Epoch: 321 [41216/225000 (18%)] Loss: 6764.546875\n",
      "Train Epoch: 321 [45312/225000 (20%)] Loss: 6892.455078\n",
      "Train Epoch: 321 [49408/225000 (22%)] Loss: 6827.009766\n",
      "Train Epoch: 321 [53504/225000 (24%)] Loss: 6938.810547\n",
      "Train Epoch: 321 [57600/225000 (26%)] Loss: 6895.718750\n",
      "Train Epoch: 321 [61696/225000 (27%)] Loss: 6996.027344\n",
      "Train Epoch: 321 [65792/225000 (29%)] Loss: 7009.220703\n",
      "Train Epoch: 321 [69888/225000 (31%)] Loss: 6818.160156\n",
      "Train Epoch: 321 [73984/225000 (33%)] Loss: 6790.970703\n",
      "Train Epoch: 321 [78080/225000 (35%)] Loss: 6844.119141\n",
      "Train Epoch: 321 [82176/225000 (37%)] Loss: 6962.537109\n",
      "Train Epoch: 321 [86272/225000 (38%)] Loss: 6915.656250\n",
      "Train Epoch: 321 [90368/225000 (40%)] Loss: 6967.302734\n",
      "Train Epoch: 321 [94464/225000 (42%)] Loss: 6880.500000\n",
      "Train Epoch: 321 [98560/225000 (44%)] Loss: 6827.412109\n",
      "Train Epoch: 321 [102656/225000 (46%)] Loss: 7110.902344\n",
      "Train Epoch: 321 [106752/225000 (47%)] Loss: 7012.589844\n",
      "Train Epoch: 321 [110848/225000 (49%)] Loss: 6912.021484\n",
      "Train Epoch: 321 [114944/225000 (51%)] Loss: 7047.185547\n",
      "Train Epoch: 321 [119040/225000 (53%)] Loss: 7008.968750\n",
      "Train Epoch: 321 [123136/225000 (55%)] Loss: 6872.097656\n",
      "Train Epoch: 321 [127232/225000 (57%)] Loss: 6978.318359\n",
      "Train Epoch: 321 [131328/225000 (58%)] Loss: 6818.404297\n",
      "Train Epoch: 321 [135424/225000 (60%)] Loss: 6752.562500\n",
      "Train Epoch: 321 [139520/225000 (62%)] Loss: 6841.408203\n",
      "Train Epoch: 321 [143616/225000 (64%)] Loss: 6914.232422\n",
      "Train Epoch: 321 [147712/225000 (66%)] Loss: 6868.027344\n",
      "Train Epoch: 321 [151808/225000 (67%)] Loss: 6976.925781\n",
      "Train Epoch: 321 [155904/225000 (69%)] Loss: 6730.658203\n",
      "Train Epoch: 321 [160000/225000 (71%)] Loss: 6957.230469\n",
      "Train Epoch: 321 [164096/225000 (73%)] Loss: 6975.724609\n",
      "Train Epoch: 321 [168192/225000 (75%)] Loss: 7078.699219\n",
      "Train Epoch: 321 [172288/225000 (77%)] Loss: 6874.013672\n",
      "Train Epoch: 321 [176384/225000 (78%)] Loss: 6921.533203\n",
      "Train Epoch: 321 [180480/225000 (80%)] Loss: 6679.806641\n",
      "Train Epoch: 321 [184576/225000 (82%)] Loss: 7047.515625\n",
      "Train Epoch: 321 [188672/225000 (84%)] Loss: 6899.892578\n",
      "Train Epoch: 321 [192768/225000 (86%)] Loss: 6922.552734\n",
      "Train Epoch: 321 [196864/225000 (87%)] Loss: 6904.562500\n",
      "Train Epoch: 321 [200960/225000 (89%)] Loss: 6886.457031\n",
      "Train Epoch: 321 [205056/225000 (91%)] Loss: 6883.998047\n",
      "Train Epoch: 321 [209152/225000 (93%)] Loss: 6771.318359\n",
      "Train Epoch: 321 [213248/225000 (95%)] Loss: 6755.400391\n",
      "Train Epoch: 321 [217344/225000 (97%)] Loss: 6867.322266\n",
      "Train Epoch: 321 [221440/225000 (98%)] Loss: 6863.005859\n",
      "    epoch          : 321\n",
      "    loss           : 6928.683571530148\n",
      "    val_loss       : 6926.8790815947\n",
      "Train Epoch: 322 [256/225000 (0%)] Loss: 6965.148438\n",
      "Train Epoch: 322 [4352/225000 (2%)] Loss: 6792.316406\n",
      "Train Epoch: 322 [8448/225000 (4%)] Loss: 6888.574219\n",
      "Train Epoch: 322 [12544/225000 (6%)] Loss: 6956.888672\n",
      "Train Epoch: 322 [16640/225000 (7%)] Loss: 6923.302734\n",
      "Train Epoch: 322 [20736/225000 (9%)] Loss: 7104.892578\n",
      "Train Epoch: 322 [24832/225000 (11%)] Loss: 7199.767578\n",
      "Train Epoch: 322 [28928/225000 (13%)] Loss: 6997.505859\n",
      "Train Epoch: 322 [33024/225000 (15%)] Loss: 7030.226562\n",
      "Train Epoch: 322 [37120/225000 (16%)] Loss: 6938.314453\n",
      "Train Epoch: 322 [41216/225000 (18%)] Loss: 6940.156250\n",
      "Train Epoch: 322 [45312/225000 (20%)] Loss: 6905.310547\n",
      "Train Epoch: 322 [49408/225000 (22%)] Loss: 6769.656250\n",
      "Train Epoch: 322 [53504/225000 (24%)] Loss: 6839.294922\n",
      "Train Epoch: 322 [57600/225000 (26%)] Loss: 6906.943359\n",
      "Train Epoch: 322 [61696/225000 (27%)] Loss: 6944.447266\n",
      "Train Epoch: 322 [65792/225000 (29%)] Loss: 6843.177734\n",
      "Train Epoch: 322 [69888/225000 (31%)] Loss: 6927.453125\n",
      "Train Epoch: 322 [73984/225000 (33%)] Loss: 6755.630859\n",
      "Train Epoch: 322 [78080/225000 (35%)] Loss: 6943.460938\n",
      "Train Epoch: 322 [82176/225000 (37%)] Loss: 6808.195312\n",
      "Train Epoch: 322 [86272/225000 (38%)] Loss: 6925.029297\n",
      "Train Epoch: 322 [90368/225000 (40%)] Loss: 7036.390625\n",
      "Train Epoch: 322 [94464/225000 (42%)] Loss: 6873.742188\n",
      "Train Epoch: 322 [98560/225000 (44%)] Loss: 6951.687500\n",
      "Train Epoch: 322 [102656/225000 (46%)] Loss: 6805.603516\n",
      "Train Epoch: 322 [106752/225000 (47%)] Loss: 6990.781250\n",
      "Train Epoch: 322 [110848/225000 (49%)] Loss: 6837.798828\n",
      "Train Epoch: 322 [114944/225000 (51%)] Loss: 7038.787109\n",
      "Train Epoch: 322 [119040/225000 (53%)] Loss: 7083.306641\n",
      "Train Epoch: 322 [123136/225000 (55%)] Loss: 6890.488281\n",
      "Train Epoch: 322 [127232/225000 (57%)] Loss: 6933.429688\n",
      "Train Epoch: 322 [131328/225000 (58%)] Loss: 6872.980469\n",
      "Train Epoch: 322 [135424/225000 (60%)] Loss: 6790.664062\n",
      "Train Epoch: 322 [139520/225000 (62%)] Loss: 6764.066406\n",
      "Train Epoch: 322 [143616/225000 (64%)] Loss: 6958.251953\n",
      "Train Epoch: 322 [147712/225000 (66%)] Loss: 6922.068359\n",
      "Train Epoch: 322 [151808/225000 (67%)] Loss: 6705.121094\n",
      "Train Epoch: 322 [155904/225000 (69%)] Loss: 6806.359375\n",
      "Train Epoch: 322 [160000/225000 (71%)] Loss: 6895.562500\n",
      "Train Epoch: 322 [164096/225000 (73%)] Loss: 6977.484375\n",
      "Train Epoch: 322 [168192/225000 (75%)] Loss: 6941.726562\n",
      "Train Epoch: 322 [172288/225000 (77%)] Loss: 6813.390625\n",
      "Train Epoch: 322 [176384/225000 (78%)] Loss: 6866.271484\n",
      "Train Epoch: 322 [180480/225000 (80%)] Loss: 6854.236328\n",
      "Train Epoch: 322 [184576/225000 (82%)] Loss: 6795.728516\n",
      "Train Epoch: 322 [188672/225000 (84%)] Loss: 6790.691406\n",
      "Train Epoch: 322 [192768/225000 (86%)] Loss: 6916.117188\n",
      "Train Epoch: 322 [196864/225000 (87%)] Loss: 6917.849609\n",
      "Train Epoch: 322 [200960/225000 (89%)] Loss: 6917.164062\n",
      "Train Epoch: 322 [205056/225000 (91%)] Loss: 7067.236328\n",
      "Train Epoch: 322 [209152/225000 (93%)] Loss: 6902.541016\n",
      "Train Epoch: 322 [213248/225000 (95%)] Loss: 7019.308594\n",
      "Train Epoch: 322 [217344/225000 (97%)] Loss: 6766.751953\n",
      "Train Epoch: 322 [221440/225000 (98%)] Loss: 6937.865234\n",
      "    epoch          : 322\n",
      "    loss           : 6947.004231770833\n",
      "    val_loss       : 6932.002145161434\n",
      "Train Epoch: 323 [256/225000 (0%)] Loss: 6849.865234\n",
      "Train Epoch: 323 [4352/225000 (2%)] Loss: 6955.191406\n",
      "Train Epoch: 323 [8448/225000 (4%)] Loss: 6833.126953\n",
      "Train Epoch: 323 [12544/225000 (6%)] Loss: 6922.593750\n",
      "Train Epoch: 323 [16640/225000 (7%)] Loss: 6750.445312\n",
      "Train Epoch: 323 [20736/225000 (9%)] Loss: 6942.220703\n",
      "Train Epoch: 323 [24832/225000 (11%)] Loss: 6867.378906\n",
      "Train Epoch: 323 [28928/225000 (13%)] Loss: 6964.691406\n",
      "Train Epoch: 323 [33024/225000 (15%)] Loss: 6995.173828\n",
      "Train Epoch: 323 [37120/225000 (16%)] Loss: 6965.958984\n",
      "Train Epoch: 323 [41216/225000 (18%)] Loss: 6825.552734\n",
      "Train Epoch: 323 [45312/225000 (20%)] Loss: 6955.017578\n",
      "Train Epoch: 323 [49408/225000 (22%)] Loss: 7021.437500\n",
      "Train Epoch: 323 [53504/225000 (24%)] Loss: 7126.642578\n",
      "Train Epoch: 323 [57600/225000 (26%)] Loss: 6882.583984\n",
      "Train Epoch: 323 [61696/225000 (27%)] Loss: 7001.962891\n",
      "Train Epoch: 323 [65792/225000 (29%)] Loss: 6810.783203\n",
      "Train Epoch: 323 [69888/225000 (31%)] Loss: 6903.949219\n",
      "Train Epoch: 323 [73984/225000 (33%)] Loss: 7144.707031\n",
      "Train Epoch: 323 [78080/225000 (35%)] Loss: 6952.726562\n",
      "Train Epoch: 323 [82176/225000 (37%)] Loss: 6836.531250\n",
      "Train Epoch: 323 [86272/225000 (38%)] Loss: 6827.115234\n",
      "Train Epoch: 323 [90368/225000 (40%)] Loss: 7007.812500\n",
      "Train Epoch: 323 [94464/225000 (42%)] Loss: 6770.949219\n",
      "Train Epoch: 323 [98560/225000 (44%)] Loss: 6958.042969\n",
      "Train Epoch: 323 [102656/225000 (46%)] Loss: 6835.972656\n",
      "Train Epoch: 323 [106752/225000 (47%)] Loss: 6904.308594\n",
      "Train Epoch: 323 [110848/225000 (49%)] Loss: 6667.787109\n",
      "Train Epoch: 323 [114944/225000 (51%)] Loss: 6919.796875\n",
      "Train Epoch: 323 [119040/225000 (53%)] Loss: 6937.902344\n",
      "Train Epoch: 323 [123136/225000 (55%)] Loss: 6857.310547\n",
      "Train Epoch: 323 [127232/225000 (57%)] Loss: 7012.855469\n",
      "Train Epoch: 323 [131328/225000 (58%)] Loss: 6759.921875\n",
      "Train Epoch: 323 [135424/225000 (60%)] Loss: 6925.828125\n",
      "Train Epoch: 323 [139520/225000 (62%)] Loss: 6905.628906\n",
      "Train Epoch: 323 [143616/225000 (64%)] Loss: 6751.867188\n",
      "Train Epoch: 323 [147712/225000 (66%)] Loss: 6979.796875\n",
      "Train Epoch: 323 [151808/225000 (67%)] Loss: 6908.728516\n",
      "Train Epoch: 323 [155904/225000 (69%)] Loss: 7024.203125\n",
      "Train Epoch: 323 [160000/225000 (71%)] Loss: 7006.640625\n",
      "Train Epoch: 323 [164096/225000 (73%)] Loss: 6923.734375\n",
      "Train Epoch: 323 [168192/225000 (75%)] Loss: 7018.525391\n",
      "Train Epoch: 323 [172288/225000 (77%)] Loss: 6888.630859\n",
      "Train Epoch: 323 [176384/225000 (78%)] Loss: 6978.818359\n",
      "Train Epoch: 323 [180480/225000 (80%)] Loss: 6950.759766\n",
      "Train Epoch: 323 [184576/225000 (82%)] Loss: 6716.464844\n",
      "Train Epoch: 323 [188672/225000 (84%)] Loss: 6794.908203\n",
      "Train Epoch: 323 [192768/225000 (86%)] Loss: 6998.937500\n",
      "Train Epoch: 323 [196864/225000 (87%)] Loss: 6871.820312\n",
      "Train Epoch: 323 [200960/225000 (89%)] Loss: 6786.673828\n",
      "Train Epoch: 323 [205056/225000 (91%)] Loss: 6786.611328\n",
      "Train Epoch: 323 [209152/225000 (93%)] Loss: 6778.804688\n",
      "Train Epoch: 323 [213248/225000 (95%)] Loss: 6941.265625\n",
      "Train Epoch: 323 [217344/225000 (97%)] Loss: 6715.023438\n",
      "Train Epoch: 323 [221440/225000 (98%)] Loss: 6943.302734\n",
      "    epoch          : 323\n",
      "    loss           : 6910.875391069397\n",
      "    val_loss       : 6924.247682975263\n",
      "Train Epoch: 324 [256/225000 (0%)] Loss: 7054.822266\n",
      "Train Epoch: 324 [4352/225000 (2%)] Loss: 6968.158203\n",
      "Train Epoch: 324 [8448/225000 (4%)] Loss: 6773.447266\n",
      "Train Epoch: 324 [12544/225000 (6%)] Loss: 6909.400391\n",
      "Train Epoch: 324 [16640/225000 (7%)] Loss: 6891.429688\n",
      "Train Epoch: 324 [20736/225000 (9%)] Loss: 6910.058594\n",
      "Train Epoch: 324 [24832/225000 (11%)] Loss: 6958.398438\n",
      "Train Epoch: 324 [28928/225000 (13%)] Loss: 7030.507812\n",
      "Train Epoch: 324 [33024/225000 (15%)] Loss: 6854.865234\n",
      "Train Epoch: 324 [37120/225000 (16%)] Loss: 6980.589844\n",
      "Train Epoch: 324 [41216/225000 (18%)] Loss: 6922.894531\n",
      "Train Epoch: 324 [45312/225000 (20%)] Loss: 7023.890625\n",
      "Train Epoch: 324 [49408/225000 (22%)] Loss: 6922.056641\n",
      "Train Epoch: 324 [53504/225000 (24%)] Loss: 7030.458984\n",
      "Train Epoch: 324 [57600/225000 (26%)] Loss: 6890.835938\n",
      "Train Epoch: 324 [61696/225000 (27%)] Loss: 6886.718750\n",
      "Train Epoch: 324 [65792/225000 (29%)] Loss: 6961.578125\n",
      "Train Epoch: 324 [69888/225000 (31%)] Loss: 7038.212891\n",
      "Train Epoch: 324 [73984/225000 (33%)] Loss: 6930.822266\n",
      "Train Epoch: 324 [78080/225000 (35%)] Loss: 6877.750000\n",
      "Train Epoch: 324 [82176/225000 (37%)] Loss: 6974.121094\n",
      "Train Epoch: 324 [86272/225000 (38%)] Loss: 6691.890625\n",
      "Train Epoch: 324 [90368/225000 (40%)] Loss: 6855.968750\n",
      "Train Epoch: 324 [94464/225000 (42%)] Loss: 6765.556641\n",
      "Train Epoch: 324 [98560/225000 (44%)] Loss: 6959.611328\n",
      "Train Epoch: 324 [102656/225000 (46%)] Loss: 6818.236328\n",
      "Train Epoch: 324 [106752/225000 (47%)] Loss: 6889.330078\n",
      "Train Epoch: 324 [110848/225000 (49%)] Loss: 6879.144531\n",
      "Train Epoch: 324 [114944/225000 (51%)] Loss: 6961.515625\n",
      "Train Epoch: 324 [119040/225000 (53%)] Loss: 7123.791016\n",
      "Train Epoch: 324 [123136/225000 (55%)] Loss: 6893.552734\n",
      "Train Epoch: 324 [127232/225000 (57%)] Loss: 6899.724609\n",
      "Train Epoch: 324 [131328/225000 (58%)] Loss: 6967.181641\n",
      "Train Epoch: 324 [135424/225000 (60%)] Loss: 7052.570312\n",
      "Train Epoch: 324 [139520/225000 (62%)] Loss: 6975.806641\n",
      "Train Epoch: 324 [143616/225000 (64%)] Loss: 6881.792969\n",
      "Train Epoch: 324 [147712/225000 (66%)] Loss: 6871.781250\n",
      "Train Epoch: 324 [151808/225000 (67%)] Loss: 6887.080078\n",
      "Train Epoch: 324 [155904/225000 (69%)] Loss: 6919.556641\n",
      "Train Epoch: 324 [160000/225000 (71%)] Loss: 6799.642578\n",
      "Train Epoch: 324 [164096/225000 (73%)] Loss: 6892.513672\n",
      "Train Epoch: 324 [168192/225000 (75%)] Loss: 7005.125000\n",
      "Train Epoch: 324 [172288/225000 (77%)] Loss: 7074.736328\n",
      "Train Epoch: 324 [176384/225000 (78%)] Loss: 6894.603516\n",
      "Train Epoch: 324 [180480/225000 (80%)] Loss: 6834.480469\n",
      "Train Epoch: 324 [184576/225000 (82%)] Loss: 6971.550781\n",
      "Train Epoch: 324 [188672/225000 (84%)] Loss: 6958.119141\n",
      "Train Epoch: 324 [192768/225000 (86%)] Loss: 7002.826172\n",
      "Train Epoch: 324 [196864/225000 (87%)] Loss: 6807.828125\n",
      "Train Epoch: 324 [200960/225000 (89%)] Loss: 6773.824219\n",
      "Train Epoch: 324 [205056/225000 (91%)] Loss: 6997.410156\n",
      "Train Epoch: 324 [209152/225000 (93%)] Loss: 6900.818359\n",
      "Train Epoch: 324 [213248/225000 (95%)] Loss: 6970.330078\n",
      "Train Epoch: 324 [217344/225000 (97%)] Loss: 6980.773438\n",
      "Train Epoch: 324 [221440/225000 (98%)] Loss: 6864.882812\n",
      "    epoch          : 324\n",
      "    loss           : 6911.247714688211\n",
      "    val_loss       : 7000.530114431771\n",
      "Train Epoch: 325 [256/225000 (0%)] Loss: 7014.179688\n",
      "Train Epoch: 325 [4352/225000 (2%)] Loss: 7004.355469\n",
      "Train Epoch: 325 [8448/225000 (4%)] Loss: 6928.500000\n",
      "Train Epoch: 325 [12544/225000 (6%)] Loss: 6885.519531\n",
      "Train Epoch: 325 [16640/225000 (7%)] Loss: 6895.335938\n",
      "Train Epoch: 325 [20736/225000 (9%)] Loss: 6880.613281\n",
      "Train Epoch: 325 [24832/225000 (11%)] Loss: 7001.970703\n",
      "Train Epoch: 325 [28928/225000 (13%)] Loss: 6800.425781\n",
      "Train Epoch: 325 [33024/225000 (15%)] Loss: 6805.488281\n",
      "Train Epoch: 325 [37120/225000 (16%)] Loss: 6865.746094\n",
      "Train Epoch: 325 [41216/225000 (18%)] Loss: 6820.945312\n",
      "Train Epoch: 325 [45312/225000 (20%)] Loss: 6830.144531\n",
      "Train Epoch: 325 [49408/225000 (22%)] Loss: 6954.558594\n",
      "Train Epoch: 325 [53504/225000 (24%)] Loss: 7091.861328\n",
      "Train Epoch: 325 [57600/225000 (26%)] Loss: 6905.599609\n",
      "Train Epoch: 325 [61696/225000 (27%)] Loss: 6952.763672\n",
      "Train Epoch: 325 [65792/225000 (29%)] Loss: 6846.193359\n",
      "Train Epoch: 325 [69888/225000 (31%)] Loss: 6944.251953\n",
      "Train Epoch: 325 [73984/225000 (33%)] Loss: 7154.205078\n",
      "Train Epoch: 325 [78080/225000 (35%)] Loss: 6997.281250\n",
      "Train Epoch: 325 [82176/225000 (37%)] Loss: 6902.843750\n",
      "Train Epoch: 325 [86272/225000 (38%)] Loss: 6822.632812\n",
      "Train Epoch: 325 [90368/225000 (40%)] Loss: 6873.998047\n",
      "Train Epoch: 325 [94464/225000 (42%)] Loss: 6950.222656\n",
      "Train Epoch: 325 [98560/225000 (44%)] Loss: 6715.816406\n",
      "Train Epoch: 325 [102656/225000 (46%)] Loss: 6894.052734\n",
      "Train Epoch: 325 [106752/225000 (47%)] Loss: 6838.115234\n",
      "Train Epoch: 325 [110848/225000 (49%)] Loss: 6991.039062\n",
      "Train Epoch: 325 [114944/225000 (51%)] Loss: 6972.929688\n",
      "Train Epoch: 325 [119040/225000 (53%)] Loss: 7094.257812\n",
      "Train Epoch: 325 [123136/225000 (55%)] Loss: 6969.066406\n",
      "Train Epoch: 325 [127232/225000 (57%)] Loss: 6858.410156\n",
      "Train Epoch: 325 [131328/225000 (58%)] Loss: 6781.615234\n",
      "Train Epoch: 325 [135424/225000 (60%)] Loss: 6823.316406\n",
      "Train Epoch: 325 [139520/225000 (62%)] Loss: 6829.818359\n",
      "Train Epoch: 325 [143616/225000 (64%)] Loss: 6909.642578\n",
      "Train Epoch: 325 [147712/225000 (66%)] Loss: 6784.695312\n",
      "Train Epoch: 325 [151808/225000 (67%)] Loss: 6909.326172\n",
      "Train Epoch: 325 [155904/225000 (69%)] Loss: 6797.271484\n",
      "Train Epoch: 325 [160000/225000 (71%)] Loss: 6966.527344\n",
      "Train Epoch: 325 [164096/225000 (73%)] Loss: 6902.216797\n",
      "Train Epoch: 325 [168192/225000 (75%)] Loss: 7048.927734\n",
      "Train Epoch: 325 [172288/225000 (77%)] Loss: 6941.388672\n",
      "Train Epoch: 325 [176384/225000 (78%)] Loss: 6951.294922\n",
      "Train Epoch: 325 [180480/225000 (80%)] Loss: 7036.230469\n",
      "Train Epoch: 325 [184576/225000 (82%)] Loss: 6749.871094\n",
      "Train Epoch: 325 [188672/225000 (84%)] Loss: 6840.351562\n",
      "Train Epoch: 325 [192768/225000 (86%)] Loss: 6833.826172\n",
      "Train Epoch: 325 [196864/225000 (87%)] Loss: 6977.865234\n",
      "Train Epoch: 325 [200960/225000 (89%)] Loss: 6799.259766\n",
      "Train Epoch: 325 [205056/225000 (91%)] Loss: 6926.400391\n",
      "Train Epoch: 325 [209152/225000 (93%)] Loss: 6788.490234\n",
      "Train Epoch: 325 [213248/225000 (95%)] Loss: 7028.000000\n",
      "Train Epoch: 325 [217344/225000 (97%)] Loss: 7022.515625\n",
      "Train Epoch: 325 [221440/225000 (98%)] Loss: 6790.552734\n",
      "    epoch          : 325\n",
      "    loss           : 6907.706613516781\n",
      "    val_loss       : 6935.17438428256\n",
      "Train Epoch: 326 [256/225000 (0%)] Loss: 7099.208984\n",
      "Train Epoch: 326 [4352/225000 (2%)] Loss: 6908.105469\n",
      "Train Epoch: 326 [8448/225000 (4%)] Loss: 6834.230469\n",
      "Train Epoch: 326 [12544/225000 (6%)] Loss: 6992.988281\n",
      "Train Epoch: 326 [16640/225000 (7%)] Loss: 6641.277344\n",
      "Train Epoch: 326 [20736/225000 (9%)] Loss: 6830.283203\n",
      "Train Epoch: 326 [24832/225000 (11%)] Loss: 6907.835938\n",
      "Train Epoch: 326 [28928/225000 (13%)] Loss: 6862.822266\n",
      "Train Epoch: 326 [33024/225000 (15%)] Loss: 7001.210938\n",
      "Train Epoch: 326 [37120/225000 (16%)] Loss: 6906.445312\n",
      "Train Epoch: 326 [41216/225000 (18%)] Loss: 6898.750000\n",
      "Train Epoch: 326 [45312/225000 (20%)] Loss: 6909.537109\n",
      "Train Epoch: 326 [49408/225000 (22%)] Loss: 6893.603516\n",
      "Train Epoch: 326 [53504/225000 (24%)] Loss: 7036.132812\n",
      "Train Epoch: 326 [57600/225000 (26%)] Loss: 6686.060547\n",
      "Train Epoch: 326 [61696/225000 (27%)] Loss: 6962.453125\n",
      "Train Epoch: 326 [65792/225000 (29%)] Loss: 6796.500000\n",
      "Train Epoch: 326 [69888/225000 (31%)] Loss: 6940.835938\n",
      "Train Epoch: 326 [73984/225000 (33%)] Loss: 6984.343750\n",
      "Train Epoch: 326 [78080/225000 (35%)] Loss: 6870.894531\n",
      "Train Epoch: 326 [82176/225000 (37%)] Loss: 6904.849609\n",
      "Train Epoch: 326 [86272/225000 (38%)] Loss: 7059.500000\n",
      "Train Epoch: 326 [90368/225000 (40%)] Loss: 6815.808594\n",
      "Train Epoch: 326 [94464/225000 (42%)] Loss: 6984.527344\n",
      "Train Epoch: 326 [98560/225000 (44%)] Loss: 6921.365234\n",
      "Train Epoch: 326 [102656/225000 (46%)] Loss: 6911.380859\n",
      "Train Epoch: 326 [106752/225000 (47%)] Loss: 6800.722656\n",
      "Train Epoch: 326 [110848/225000 (49%)] Loss: 6882.375000\n",
      "Train Epoch: 326 [114944/225000 (51%)] Loss: 6924.798828\n",
      "Train Epoch: 326 [119040/225000 (53%)] Loss: 6881.896484\n",
      "Train Epoch: 326 [123136/225000 (55%)] Loss: 6919.373047\n",
      "Train Epoch: 326 [127232/225000 (57%)] Loss: 6876.466797\n",
      "Train Epoch: 326 [131328/225000 (58%)] Loss: 7056.576172\n",
      "Train Epoch: 326 [135424/225000 (60%)] Loss: 6756.103516\n",
      "Train Epoch: 326 [139520/225000 (62%)] Loss: 6955.890625\n",
      "Train Epoch: 326 [143616/225000 (64%)] Loss: 6795.765625\n",
      "Train Epoch: 326 [147712/225000 (66%)] Loss: 6955.931641\n",
      "Train Epoch: 326 [151808/225000 (67%)] Loss: 6919.068359\n",
      "Train Epoch: 326 [155904/225000 (69%)] Loss: 6909.998047\n",
      "Train Epoch: 326 [160000/225000 (71%)] Loss: 6929.681641\n",
      "Train Epoch: 326 [164096/225000 (73%)] Loss: 6817.007812\n",
      "Train Epoch: 326 [168192/225000 (75%)] Loss: 7190.033203\n",
      "Train Epoch: 326 [172288/225000 (77%)] Loss: 6861.646484\n",
      "Train Epoch: 326 [176384/225000 (78%)] Loss: 6833.742188\n",
      "Train Epoch: 326 [180480/225000 (80%)] Loss: 6941.589844\n",
      "Train Epoch: 326 [184576/225000 (82%)] Loss: 6903.242188\n",
      "Train Epoch: 326 [188672/225000 (84%)] Loss: 6961.240234\n",
      "Train Epoch: 326 [192768/225000 (86%)] Loss: 7037.486328\n",
      "Train Epoch: 326 [196864/225000 (87%)] Loss: 6875.908203\n",
      "Train Epoch: 326 [200960/225000 (89%)] Loss: 6898.083984\n",
      "Train Epoch: 326 [205056/225000 (91%)] Loss: 6918.890625\n",
      "Train Epoch: 326 [209152/225000 (93%)] Loss: 6958.888672\n",
      "Train Epoch: 326 [213248/225000 (95%)] Loss: 6849.189453\n",
      "Train Epoch: 326 [217344/225000 (97%)] Loss: 6920.964844\n",
      "Train Epoch: 326 [221440/225000 (98%)] Loss: 6863.507812\n",
      "    epoch          : 326\n",
      "    loss           : 6915.520790004622\n",
      "    val_loss       : 6915.958487382957\n",
      "Train Epoch: 327 [256/225000 (0%)] Loss: 6922.326172\n",
      "Train Epoch: 327 [4352/225000 (2%)] Loss: 6740.470703\n",
      "Train Epoch: 327 [8448/225000 (4%)] Loss: 6909.912109\n",
      "Train Epoch: 327 [12544/225000 (6%)] Loss: 6913.542969\n",
      "Train Epoch: 327 [16640/225000 (7%)] Loss: 6947.294922\n",
      "Train Epoch: 327 [20736/225000 (9%)] Loss: 6901.451172\n",
      "Train Epoch: 327 [24832/225000 (11%)] Loss: 6947.181641\n",
      "Train Epoch: 327 [28928/225000 (13%)] Loss: 6857.597656\n",
      "Train Epoch: 327 [33024/225000 (15%)] Loss: 6831.708984\n",
      "Train Epoch: 327 [37120/225000 (16%)] Loss: 6989.728516\n",
      "Train Epoch: 327 [41216/225000 (18%)] Loss: 6889.011719\n",
      "Train Epoch: 327 [45312/225000 (20%)] Loss: 6878.593750\n",
      "Train Epoch: 327 [49408/225000 (22%)] Loss: 6897.500000\n",
      "Train Epoch: 327 [53504/225000 (24%)] Loss: 6914.376953\n",
      "Train Epoch: 327 [57600/225000 (26%)] Loss: 6955.572266\n",
      "Train Epoch: 327 [61696/225000 (27%)] Loss: 6847.988281\n",
      "Train Epoch: 327 [65792/225000 (29%)] Loss: 6895.371094\n",
      "Train Epoch: 327 [69888/225000 (31%)] Loss: 6867.476562\n",
      "Train Epoch: 327 [73984/225000 (33%)] Loss: 6892.554688\n",
      "Train Epoch: 327 [78080/225000 (35%)] Loss: 6839.984375\n",
      "Train Epoch: 327 [82176/225000 (37%)] Loss: 6967.449219\n",
      "Train Epoch: 327 [86272/225000 (38%)] Loss: 6989.064453\n",
      "Train Epoch: 327 [90368/225000 (40%)] Loss: 6872.464844\n",
      "Train Epoch: 327 [94464/225000 (42%)] Loss: 6758.986328\n",
      "Train Epoch: 327 [98560/225000 (44%)] Loss: 6963.507812\n",
      "Train Epoch: 327 [102656/225000 (46%)] Loss: 6906.839844\n",
      "Train Epoch: 327 [106752/225000 (47%)] Loss: 6881.734375\n",
      "Train Epoch: 327 [110848/225000 (49%)] Loss: 6919.941406\n",
      "Train Epoch: 327 [114944/225000 (51%)] Loss: 6880.431641\n",
      "Train Epoch: 327 [119040/225000 (53%)] Loss: 6892.382812\n",
      "Train Epoch: 327 [123136/225000 (55%)] Loss: 6958.156250\n",
      "Train Epoch: 327 [127232/225000 (57%)] Loss: 6963.990234\n",
      "Train Epoch: 327 [131328/225000 (58%)] Loss: 6856.460938\n",
      "Train Epoch: 327 [135424/225000 (60%)] Loss: 6913.728516\n",
      "Train Epoch: 327 [139520/225000 (62%)] Loss: 6834.445312\n",
      "Train Epoch: 327 [143616/225000 (64%)] Loss: 7079.169922\n",
      "Train Epoch: 327 [147712/225000 (66%)] Loss: 6865.851562\n",
      "Train Epoch: 327 [151808/225000 (67%)] Loss: 6985.753906\n",
      "Train Epoch: 327 [155904/225000 (69%)] Loss: 6905.318359\n",
      "Train Epoch: 327 [160000/225000 (71%)] Loss: 6853.773438\n",
      "Train Epoch: 327 [164096/225000 (73%)] Loss: 6964.511719\n",
      "Train Epoch: 327 [168192/225000 (75%)] Loss: 7018.246094\n",
      "Train Epoch: 327 [172288/225000 (77%)] Loss: 6885.041016\n",
      "Train Epoch: 327 [176384/225000 (78%)] Loss: 6966.494141\n",
      "Train Epoch: 327 [180480/225000 (80%)] Loss: 6803.853516\n",
      "Train Epoch: 327 [184576/225000 (82%)] Loss: 6973.521484\n",
      "Train Epoch: 327 [188672/225000 (84%)] Loss: 6904.939453\n",
      "Train Epoch: 327 [192768/225000 (86%)] Loss: 6923.917969\n",
      "Train Epoch: 327 [196864/225000 (87%)] Loss: 6989.019531\n",
      "Train Epoch: 327 [200960/225000 (89%)] Loss: 6875.769531\n",
      "Train Epoch: 327 [205056/225000 (91%)] Loss: 6860.675781\n",
      "Train Epoch: 327 [209152/225000 (93%)] Loss: 6847.375000\n",
      "Train Epoch: 327 [213248/225000 (95%)] Loss: 6889.671875\n",
      "Train Epoch: 327 [217344/225000 (97%)] Loss: 6740.712891\n",
      "Train Epoch: 327 [221440/225000 (98%)] Loss: 7013.683594\n",
      "    epoch          : 327\n",
      "    loss           : 6911.256757057025\n",
      "    val_loss       : 6992.166082372471\n",
      "Train Epoch: 328 [256/225000 (0%)] Loss: 6922.332031\n",
      "Train Epoch: 328 [4352/225000 (2%)] Loss: 6990.927734\n",
      "Train Epoch: 328 [8448/225000 (4%)] Loss: 7014.210938\n",
      "Train Epoch: 328 [12544/225000 (6%)] Loss: 6784.408203\n",
      "Train Epoch: 328 [16640/225000 (7%)] Loss: 6885.576172\n",
      "Train Epoch: 328 [20736/225000 (9%)] Loss: 6795.144531\n",
      "Train Epoch: 328 [24832/225000 (11%)] Loss: 6953.378906\n",
      "Train Epoch: 328 [28928/225000 (13%)] Loss: 6785.642578\n",
      "Train Epoch: 328 [33024/225000 (15%)] Loss: 6895.468750\n",
      "Train Epoch: 328 [37120/225000 (16%)] Loss: 6935.373047\n",
      "Train Epoch: 328 [41216/225000 (18%)] Loss: 7035.791016\n",
      "Train Epoch: 328 [45312/225000 (20%)] Loss: 6903.710938\n",
      "Train Epoch: 328 [49408/225000 (22%)] Loss: 7072.335938\n",
      "Train Epoch: 328 [53504/225000 (24%)] Loss: 6877.136719\n",
      "Train Epoch: 328 [57600/225000 (26%)] Loss: 6842.007812\n",
      "Train Epoch: 328 [61696/225000 (27%)] Loss: 6927.056641\n",
      "Train Epoch: 328 [65792/225000 (29%)] Loss: 6702.275391\n",
      "Train Epoch: 328 [69888/225000 (31%)] Loss: 6773.359375\n",
      "Train Epoch: 328 [73984/225000 (33%)] Loss: 6868.660156\n",
      "Train Epoch: 328 [78080/225000 (35%)] Loss: 6997.359375\n",
      "Train Epoch: 328 [82176/225000 (37%)] Loss: 6837.916016\n",
      "Train Epoch: 328 [86272/225000 (38%)] Loss: 6880.107422\n",
      "Train Epoch: 328 [90368/225000 (40%)] Loss: 6854.269531\n",
      "Train Epoch: 328 [94464/225000 (42%)] Loss: 6974.308594\n",
      "Train Epoch: 328 [98560/225000 (44%)] Loss: 6890.167969\n",
      "Train Epoch: 328 [102656/225000 (46%)] Loss: 6985.865234\n",
      "Train Epoch: 328 [106752/225000 (47%)] Loss: 6800.216797\n",
      "Train Epoch: 328 [110848/225000 (49%)] Loss: 7164.044922\n",
      "Train Epoch: 328 [114944/225000 (51%)] Loss: 6956.373047\n",
      "Train Epoch: 328 [119040/225000 (53%)] Loss: 6971.005859\n",
      "Train Epoch: 328 [123136/225000 (55%)] Loss: 6792.699219\n",
      "Train Epoch: 328 [127232/225000 (57%)] Loss: 6939.667969\n",
      "Train Epoch: 328 [131328/225000 (58%)] Loss: 6815.687500\n",
      "Train Epoch: 328 [135424/225000 (60%)] Loss: 6910.539062\n",
      "Train Epoch: 328 [139520/225000 (62%)] Loss: 6789.427734\n",
      "Train Epoch: 328 [143616/225000 (64%)] Loss: 6964.939453\n",
      "Train Epoch: 328 [147712/225000 (66%)] Loss: 6992.785156\n",
      "Train Epoch: 328 [151808/225000 (67%)] Loss: 6855.250000\n",
      "Train Epoch: 328 [155904/225000 (69%)] Loss: 6883.121094\n",
      "Train Epoch: 328 [160000/225000 (71%)] Loss: 6961.277344\n",
      "Train Epoch: 328 [164096/225000 (73%)] Loss: 6875.888672\n",
      "Train Epoch: 328 [168192/225000 (75%)] Loss: 6903.845703\n",
      "Train Epoch: 328 [172288/225000 (77%)] Loss: 19321.900391\n",
      "Train Epoch: 328 [176384/225000 (78%)] Loss: 6913.251953\n",
      "Train Epoch: 328 [180480/225000 (80%)] Loss: 6980.599609\n",
      "Train Epoch: 328 [184576/225000 (82%)] Loss: 6927.343750\n",
      "Train Epoch: 328 [188672/225000 (84%)] Loss: 6781.611328\n",
      "Train Epoch: 328 [192768/225000 (86%)] Loss: 6906.130859\n",
      "Train Epoch: 328 [196864/225000 (87%)] Loss: 6818.435547\n",
      "Train Epoch: 328 [200960/225000 (89%)] Loss: 6742.330078\n",
      "Train Epoch: 328 [205056/225000 (91%)] Loss: 6925.271484\n",
      "Train Epoch: 328 [209152/225000 (93%)] Loss: 7033.085938\n",
      "Train Epoch: 328 [213248/225000 (95%)] Loss: 6838.724609\n",
      "Train Epoch: 328 [217344/225000 (97%)] Loss: 6841.404297\n",
      "Train Epoch: 328 [221440/225000 (98%)] Loss: 6963.320312\n",
      "    epoch          : 328\n",
      "    loss           : 6917.438514336249\n",
      "    val_loss       : 6913.8055787621715\n",
      "Train Epoch: 329 [256/225000 (0%)] Loss: 6943.005859\n",
      "Train Epoch: 329 [4352/225000 (2%)] Loss: 6872.140625\n",
      "Train Epoch: 329 [8448/225000 (4%)] Loss: 6862.556641\n",
      "Train Epoch: 329 [12544/225000 (6%)] Loss: 6912.880859\n",
      "Train Epoch: 329 [16640/225000 (7%)] Loss: 6840.595703\n",
      "Train Epoch: 329 [20736/225000 (9%)] Loss: 6876.376953\n",
      "Train Epoch: 329 [24832/225000 (11%)] Loss: 6832.753906\n",
      "Train Epoch: 329 [28928/225000 (13%)] Loss: 7064.638672\n",
      "Train Epoch: 329 [33024/225000 (15%)] Loss: 6854.566406\n",
      "Train Epoch: 329 [37120/225000 (16%)] Loss: 6717.689453\n",
      "Train Epoch: 329 [41216/225000 (18%)] Loss: 6815.091797\n",
      "Train Epoch: 329 [45312/225000 (20%)] Loss: 6994.439453\n",
      "Train Epoch: 329 [49408/225000 (22%)] Loss: 6988.884766\n",
      "Train Epoch: 329 [53504/225000 (24%)] Loss: 6893.224609\n",
      "Train Epoch: 329 [57600/225000 (26%)] Loss: 6857.066406\n",
      "Train Epoch: 329 [61696/225000 (27%)] Loss: 6841.146484\n",
      "Train Epoch: 329 [65792/225000 (29%)] Loss: 6797.966797\n",
      "Train Epoch: 329 [69888/225000 (31%)] Loss: 6828.335938\n",
      "Train Epoch: 329 [73984/225000 (33%)] Loss: 6970.267578\n",
      "Train Epoch: 329 [78080/225000 (35%)] Loss: 7011.994141\n",
      "Train Epoch: 329 [82176/225000 (37%)] Loss: 6841.912109\n",
      "Train Epoch: 329 [86272/225000 (38%)] Loss: 6867.919922\n",
      "Train Epoch: 329 [90368/225000 (40%)] Loss: 6904.291016\n",
      "Train Epoch: 329 [94464/225000 (42%)] Loss: 6860.574219\n",
      "Train Epoch: 329 [98560/225000 (44%)] Loss: 6908.341797\n",
      "Train Epoch: 329 [102656/225000 (46%)] Loss: 6795.730469\n",
      "Train Epoch: 329 [106752/225000 (47%)] Loss: 6966.498047\n",
      "Train Epoch: 329 [110848/225000 (49%)] Loss: 6886.656250\n",
      "Train Epoch: 329 [114944/225000 (51%)] Loss: 7006.947266\n",
      "Train Epoch: 329 [119040/225000 (53%)] Loss: 6878.412109\n",
      "Train Epoch: 329 [123136/225000 (55%)] Loss: 7019.626953\n",
      "Train Epoch: 329 [127232/225000 (57%)] Loss: 6869.369141\n",
      "Train Epoch: 329 [131328/225000 (58%)] Loss: 6925.607422\n",
      "Train Epoch: 329 [135424/225000 (60%)] Loss: 6906.888672\n",
      "Train Epoch: 329 [139520/225000 (62%)] Loss: 6973.488281\n",
      "Train Epoch: 329 [143616/225000 (64%)] Loss: 6937.197266\n",
      "Train Epoch: 329 [147712/225000 (66%)] Loss: 6907.363281\n",
      "Train Epoch: 329 [151808/225000 (67%)] Loss: 6836.630859\n",
      "Train Epoch: 329 [155904/225000 (69%)] Loss: 7013.783203\n",
      "Train Epoch: 329 [160000/225000 (71%)] Loss: 6955.220703\n",
      "Train Epoch: 329 [164096/225000 (73%)] Loss: 6961.111328\n",
      "Train Epoch: 329 [168192/225000 (75%)] Loss: 6958.339844\n",
      "Train Epoch: 329 [172288/225000 (77%)] Loss: 6892.634766\n",
      "Train Epoch: 329 [176384/225000 (78%)] Loss: 6814.464844\n",
      "Train Epoch: 329 [180480/225000 (80%)] Loss: 6875.839844\n",
      "Train Epoch: 329 [184576/225000 (82%)] Loss: 6855.046875\n",
      "Train Epoch: 329 [188672/225000 (84%)] Loss: 6917.306641\n",
      "Train Epoch: 329 [192768/225000 (86%)] Loss: 7007.253906\n",
      "Train Epoch: 329 [196864/225000 (87%)] Loss: 6902.539062\n",
      "Train Epoch: 329 [200960/225000 (89%)] Loss: 6864.111328\n",
      "Train Epoch: 329 [205056/225000 (91%)] Loss: 6878.847656\n",
      "Train Epoch: 329 [209152/225000 (93%)] Loss: 6961.621094\n",
      "Train Epoch: 329 [213248/225000 (95%)] Loss: 6827.220703\n",
      "Train Epoch: 329 [217344/225000 (97%)] Loss: 6806.619141\n",
      "Train Epoch: 329 [221440/225000 (98%)] Loss: 7010.572266\n",
      "    epoch          : 329\n",
      "    loss           : 6907.982236339235\n",
      "    val_loss       : 6915.608086576267\n",
      "Train Epoch: 330 [256/225000 (0%)] Loss: 6747.394531\n",
      "Train Epoch: 330 [4352/225000 (2%)] Loss: 6918.871094\n",
      "Train Epoch: 330 [8448/225000 (4%)] Loss: 6862.994141\n",
      "Train Epoch: 330 [12544/225000 (6%)] Loss: 6730.000000\n",
      "Train Epoch: 330 [16640/225000 (7%)] Loss: 6818.337891\n",
      "Train Epoch: 330 [20736/225000 (9%)] Loss: 6851.654297\n",
      "Train Epoch: 330 [24832/225000 (11%)] Loss: 6798.808594\n",
      "Train Epoch: 330 [28928/225000 (13%)] Loss: 6869.767578\n",
      "Train Epoch: 330 [33024/225000 (15%)] Loss: 6794.107422\n",
      "Train Epoch: 330 [37120/225000 (16%)] Loss: 6692.416016\n",
      "Train Epoch: 330 [41216/225000 (18%)] Loss: 6973.419922\n",
      "Train Epoch: 330 [45312/225000 (20%)] Loss: 6957.177734\n",
      "Train Epoch: 330 [49408/225000 (22%)] Loss: 6879.978516\n",
      "Train Epoch: 330 [53504/225000 (24%)] Loss: 6892.710938\n",
      "Train Epoch: 330 [57600/225000 (26%)] Loss: 6872.242188\n",
      "Train Epoch: 330 [61696/225000 (27%)] Loss: 6933.283203\n",
      "Train Epoch: 330 [65792/225000 (29%)] Loss: 6880.917969\n",
      "Train Epoch: 330 [69888/225000 (31%)] Loss: 7081.070312\n",
      "Train Epoch: 330 [73984/225000 (33%)] Loss: 7013.177734\n",
      "Train Epoch: 330 [78080/225000 (35%)] Loss: 6800.439453\n",
      "Train Epoch: 330 [82176/225000 (37%)] Loss: 6691.369141\n",
      "Train Epoch: 330 [86272/225000 (38%)] Loss: 6924.996094\n",
      "Train Epoch: 330 [90368/225000 (40%)] Loss: 6849.787109\n",
      "Train Epoch: 330 [94464/225000 (42%)] Loss: 6963.001953\n",
      "Train Epoch: 330 [98560/225000 (44%)] Loss: 6915.437500\n",
      "Train Epoch: 330 [102656/225000 (46%)] Loss: 7007.025391\n",
      "Train Epoch: 330 [106752/225000 (47%)] Loss: 6797.751953\n",
      "Train Epoch: 330 [110848/225000 (49%)] Loss: 6852.396484\n",
      "Train Epoch: 330 [114944/225000 (51%)] Loss: 6930.794922\n",
      "Train Epoch: 330 [119040/225000 (53%)] Loss: 6858.531250\n",
      "Train Epoch: 330 [123136/225000 (55%)] Loss: 7007.970703\n",
      "Train Epoch: 330 [127232/225000 (57%)] Loss: 6845.203125\n",
      "Train Epoch: 330 [131328/225000 (58%)] Loss: 7022.974609\n",
      "Train Epoch: 330 [135424/225000 (60%)] Loss: 6811.179688\n",
      "Train Epoch: 330 [139520/225000 (62%)] Loss: 6884.056641\n",
      "Train Epoch: 330 [143616/225000 (64%)] Loss: 7036.312500\n",
      "Train Epoch: 330 [147712/225000 (66%)] Loss: 6797.974609\n",
      "Train Epoch: 330 [151808/225000 (67%)] Loss: 6882.128906\n",
      "Train Epoch: 330 [155904/225000 (69%)] Loss: 6785.349609\n",
      "Train Epoch: 330 [160000/225000 (71%)] Loss: 6754.876953\n",
      "Train Epoch: 330 [164096/225000 (73%)] Loss: 6769.900391\n",
      "Train Epoch: 330 [168192/225000 (75%)] Loss: 6866.875000\n",
      "Train Epoch: 330 [172288/225000 (77%)] Loss: 7019.560547\n",
      "Train Epoch: 330 [176384/225000 (78%)] Loss: 6934.470703\n",
      "Train Epoch: 330 [180480/225000 (80%)] Loss: 6958.216797\n",
      "Train Epoch: 330 [184576/225000 (82%)] Loss: 6842.335938\n",
      "Train Epoch: 330 [188672/225000 (84%)] Loss: 6947.533203\n",
      "Train Epoch: 330 [192768/225000 (86%)] Loss: 6841.812500\n",
      "Train Epoch: 330 [196864/225000 (87%)] Loss: 6779.800781\n",
      "Train Epoch: 330 [200960/225000 (89%)] Loss: 6802.462891\n",
      "Train Epoch: 330 [205056/225000 (91%)] Loss: 6875.701172\n",
      "Train Epoch: 330 [209152/225000 (93%)] Loss: 6850.001953\n",
      "Train Epoch: 330 [213248/225000 (95%)] Loss: 6862.207031\n",
      "Train Epoch: 330 [217344/225000 (97%)] Loss: 6934.394531\n",
      "Train Epoch: 330 [221440/225000 (98%)] Loss: 6893.373047\n",
      "    epoch          : 330\n",
      "    loss           : 6900.20975207089\n",
      "    val_loss       : 6911.58697098372\n",
      "Train Epoch: 331 [256/225000 (0%)] Loss: 6850.150391\n",
      "Train Epoch: 331 [4352/225000 (2%)] Loss: 6787.746094\n",
      "Train Epoch: 331 [8448/225000 (4%)] Loss: 6882.925781\n",
      "Train Epoch: 331 [12544/225000 (6%)] Loss: 6961.978516\n",
      "Train Epoch: 331 [16640/225000 (7%)] Loss: 6819.855469\n",
      "Train Epoch: 331 [20736/225000 (9%)] Loss: 6737.466797\n",
      "Train Epoch: 331 [24832/225000 (11%)] Loss: 7088.648438\n",
      "Train Epoch: 331 [28928/225000 (13%)] Loss: 6947.402344\n",
      "Train Epoch: 331 [33024/225000 (15%)] Loss: 6919.466797\n",
      "Train Epoch: 331 [37120/225000 (16%)] Loss: 6848.638672\n",
      "Train Epoch: 331 [41216/225000 (18%)] Loss: 7006.291016\n",
      "Train Epoch: 331 [45312/225000 (20%)] Loss: 6961.291016\n",
      "Train Epoch: 331 [49408/225000 (22%)] Loss: 6964.945312\n",
      "Train Epoch: 331 [53504/225000 (24%)] Loss: 6953.783203\n",
      "Train Epoch: 331 [57600/225000 (26%)] Loss: 6823.394531\n",
      "Train Epoch: 331 [61696/225000 (27%)] Loss: 6693.578125\n",
      "Train Epoch: 331 [65792/225000 (29%)] Loss: 6770.751953\n",
      "Train Epoch: 331 [69888/225000 (31%)] Loss: 6803.882812\n",
      "Train Epoch: 331 [73984/225000 (33%)] Loss: 6716.957031\n",
      "Train Epoch: 331 [78080/225000 (35%)] Loss: 6894.246094\n",
      "Train Epoch: 331 [82176/225000 (37%)] Loss: 6888.300781\n",
      "Train Epoch: 331 [86272/225000 (38%)] Loss: 6951.697266\n",
      "Train Epoch: 331 [90368/225000 (40%)] Loss: 6843.570312\n",
      "Train Epoch: 331 [94464/225000 (42%)] Loss: 6813.173828\n",
      "Train Epoch: 331 [98560/225000 (44%)] Loss: 7011.892578\n",
      "Train Epoch: 331 [102656/225000 (46%)] Loss: 6846.003906\n",
      "Train Epoch: 331 [106752/225000 (47%)] Loss: 6897.361328\n",
      "Train Epoch: 331 [110848/225000 (49%)] Loss: 6979.283203\n",
      "Train Epoch: 331 [114944/225000 (51%)] Loss: 6808.603516\n",
      "Train Epoch: 331 [119040/225000 (53%)] Loss: 6941.027344\n",
      "Train Epoch: 331 [123136/225000 (55%)] Loss: 6831.134766\n",
      "Train Epoch: 331 [127232/225000 (57%)] Loss: 6862.371094\n",
      "Train Epoch: 331 [131328/225000 (58%)] Loss: 6874.326172\n",
      "Train Epoch: 331 [135424/225000 (60%)] Loss: 7096.119141\n",
      "Train Epoch: 331 [139520/225000 (62%)] Loss: 6940.000000\n",
      "Train Epoch: 331 [143616/225000 (64%)] Loss: 6797.136719\n",
      "Train Epoch: 331 [147712/225000 (66%)] Loss: 6846.134766\n",
      "Train Epoch: 331 [151808/225000 (67%)] Loss: 7012.507812\n",
      "Train Epoch: 331 [155904/225000 (69%)] Loss: 6892.818359\n",
      "Train Epoch: 331 [160000/225000 (71%)] Loss: 6927.730469\n",
      "Train Epoch: 331 [164096/225000 (73%)] Loss: 7033.232422\n",
      "Train Epoch: 331 [168192/225000 (75%)] Loss: 7029.115234\n",
      "Train Epoch: 331 [172288/225000 (77%)] Loss: 6795.587891\n",
      "Train Epoch: 331 [176384/225000 (78%)] Loss: 6864.431641\n",
      "Train Epoch: 331 [180480/225000 (80%)] Loss: 6850.546875\n",
      "Train Epoch: 331 [184576/225000 (82%)] Loss: 6696.470703\n",
      "Train Epoch: 331 [188672/225000 (84%)] Loss: 7005.568359\n",
      "Train Epoch: 331 [192768/225000 (86%)] Loss: 6904.996094\n",
      "Train Epoch: 331 [196864/225000 (87%)] Loss: 6766.576172\n",
      "Train Epoch: 331 [200960/225000 (89%)] Loss: 7004.175781\n",
      "Train Epoch: 331 [205056/225000 (91%)] Loss: 6826.904297\n",
      "Train Epoch: 331 [209152/225000 (93%)] Loss: 6843.833984\n",
      "Train Epoch: 331 [213248/225000 (95%)] Loss: 6822.855469\n",
      "Train Epoch: 331 [217344/225000 (97%)] Loss: 6857.855469\n",
      "Train Epoch: 331 [221440/225000 (98%)] Loss: 6903.878906\n",
      "    epoch          : 331\n",
      "    loss           : 6912.757158125355\n",
      "    val_loss       : 6910.274638927713\n",
      "Train Epoch: 332 [256/225000 (0%)] Loss: 6878.990234\n",
      "Train Epoch: 332 [4352/225000 (2%)] Loss: 7062.062500\n",
      "Train Epoch: 332 [8448/225000 (4%)] Loss: 6908.359375\n",
      "Train Epoch: 332 [12544/225000 (6%)] Loss: 6978.902344\n",
      "Train Epoch: 332 [16640/225000 (7%)] Loss: 6798.351562\n",
      "Train Epoch: 332 [20736/225000 (9%)] Loss: 6719.857422\n",
      "Train Epoch: 332 [24832/225000 (11%)] Loss: 6921.490234\n",
      "Train Epoch: 332 [28928/225000 (13%)] Loss: 6819.425781\n",
      "Train Epoch: 332 [33024/225000 (15%)] Loss: 6918.062500\n",
      "Train Epoch: 332 [37120/225000 (16%)] Loss: 7003.080078\n",
      "Train Epoch: 332 [41216/225000 (18%)] Loss: 6911.035156\n",
      "Train Epoch: 332 [45312/225000 (20%)] Loss: 6994.931641\n",
      "Train Epoch: 332 [49408/225000 (22%)] Loss: 6906.189453\n",
      "Train Epoch: 332 [53504/225000 (24%)] Loss: 6899.958984\n",
      "Train Epoch: 332 [57600/225000 (26%)] Loss: 6861.962891\n",
      "Train Epoch: 332 [61696/225000 (27%)] Loss: 6907.957031\n",
      "Train Epoch: 332 [65792/225000 (29%)] Loss: 6833.531250\n",
      "Train Epoch: 332 [69888/225000 (31%)] Loss: 6910.748047\n",
      "Train Epoch: 332 [73984/225000 (33%)] Loss: 6950.988281\n",
      "Train Epoch: 332 [78080/225000 (35%)] Loss: 6826.933594\n",
      "Train Epoch: 332 [82176/225000 (37%)] Loss: 6944.753906\n",
      "Train Epoch: 332 [86272/225000 (38%)] Loss: 6827.029297\n",
      "Train Epoch: 332 [90368/225000 (40%)] Loss: 6897.425781\n",
      "Train Epoch: 332 [94464/225000 (42%)] Loss: 6880.269531\n",
      "Train Epoch: 332 [98560/225000 (44%)] Loss: 6970.113281\n",
      "Train Epoch: 332 [102656/225000 (46%)] Loss: 6924.283203\n",
      "Train Epoch: 332 [106752/225000 (47%)] Loss: 6929.255859\n",
      "Train Epoch: 332 [110848/225000 (49%)] Loss: 6933.355469\n",
      "Train Epoch: 332 [114944/225000 (51%)] Loss: 6860.441406\n",
      "Train Epoch: 332 [119040/225000 (53%)] Loss: 6748.933594\n",
      "Train Epoch: 332 [123136/225000 (55%)] Loss: 6916.423828\n",
      "Train Epoch: 332 [127232/225000 (57%)] Loss: 6863.421875\n",
      "Train Epoch: 332 [131328/225000 (58%)] Loss: 7015.820312\n",
      "Train Epoch: 332 [135424/225000 (60%)] Loss: 6892.230469\n",
      "Train Epoch: 332 [139520/225000 (62%)] Loss: 6990.847656\n",
      "Train Epoch: 332 [143616/225000 (64%)] Loss: 6932.970703\n",
      "Train Epoch: 332 [147712/225000 (66%)] Loss: 6935.939453\n",
      "Train Epoch: 332 [151808/225000 (67%)] Loss: 6941.808594\n",
      "Train Epoch: 332 [155904/225000 (69%)] Loss: 6993.916016\n",
      "Train Epoch: 332 [160000/225000 (71%)] Loss: 6911.099609\n",
      "Train Epoch: 332 [164096/225000 (73%)] Loss: 6871.103516\n",
      "Train Epoch: 332 [168192/225000 (75%)] Loss: 6812.406250\n",
      "Train Epoch: 332 [172288/225000 (77%)] Loss: 6870.357422\n",
      "Train Epoch: 332 [176384/225000 (78%)] Loss: 6966.988281\n",
      "Train Epoch: 332 [180480/225000 (80%)] Loss: 6928.785156\n",
      "Train Epoch: 332 [184576/225000 (82%)] Loss: 7053.960938\n",
      "Train Epoch: 332 [188672/225000 (84%)] Loss: 6936.343750\n",
      "Train Epoch: 332 [192768/225000 (86%)] Loss: 6893.064453\n",
      "Train Epoch: 332 [196864/225000 (87%)] Loss: 6988.041016\n",
      "Train Epoch: 332 [200960/225000 (89%)] Loss: 6987.292969\n",
      "Train Epoch: 332 [205056/225000 (91%)] Loss: 6886.392578\n",
      "Train Epoch: 332 [209152/225000 (93%)] Loss: 6840.623047\n",
      "Train Epoch: 332 [213248/225000 (95%)] Loss: 6911.050781\n",
      "Train Epoch: 332 [217344/225000 (97%)] Loss: 7006.597656\n",
      "Train Epoch: 332 [221440/225000 (98%)] Loss: 6962.097656\n",
      "    epoch          : 332\n",
      "    loss           : 6905.809952493956\n",
      "    val_loss       : 6909.202664534048\n",
      "Train Epoch: 333 [256/225000 (0%)] Loss: 6929.117188\n",
      "Train Epoch: 333 [4352/225000 (2%)] Loss: 6858.664062\n",
      "Train Epoch: 333 [8448/225000 (4%)] Loss: 6754.304688\n",
      "Train Epoch: 333 [12544/225000 (6%)] Loss: 6762.707031\n",
      "Train Epoch: 333 [16640/225000 (7%)] Loss: 6945.142578\n",
      "Train Epoch: 333 [20736/225000 (9%)] Loss: 6987.753906\n",
      "Train Epoch: 333 [24832/225000 (11%)] Loss: 6891.285156\n",
      "Train Epoch: 333 [28928/225000 (13%)] Loss: 6938.332031\n",
      "Train Epoch: 333 [33024/225000 (15%)] Loss: 7016.367188\n",
      "Train Epoch: 333 [37120/225000 (16%)] Loss: 7047.923828\n",
      "Train Epoch: 333 [41216/225000 (18%)] Loss: 6840.861328\n",
      "Train Epoch: 333 [45312/225000 (20%)] Loss: 6987.826172\n",
      "Train Epoch: 333 [49408/225000 (22%)] Loss: 7030.501953\n",
      "Train Epoch: 333 [53504/225000 (24%)] Loss: 7047.832031\n",
      "Train Epoch: 333 [57600/225000 (26%)] Loss: 6847.423828\n",
      "Train Epoch: 333 [61696/225000 (27%)] Loss: 6912.046875\n",
      "Train Epoch: 333 [65792/225000 (29%)] Loss: 6874.005859\n",
      "Train Epoch: 333 [69888/225000 (31%)] Loss: 6962.072266\n",
      "Train Epoch: 333 [73984/225000 (33%)] Loss: 6870.669922\n",
      "Train Epoch: 333 [78080/225000 (35%)] Loss: 6858.250000\n",
      "Train Epoch: 333 [82176/225000 (37%)] Loss: 6829.181641\n",
      "Train Epoch: 333 [86272/225000 (38%)] Loss: 6935.693359\n",
      "Train Epoch: 333 [90368/225000 (40%)] Loss: 6966.910156\n",
      "Train Epoch: 333 [94464/225000 (42%)] Loss: 6964.312500\n",
      "Train Epoch: 333 [98560/225000 (44%)] Loss: 6774.732422\n",
      "Train Epoch: 333 [102656/225000 (46%)] Loss: 6769.326172\n",
      "Train Epoch: 333 [106752/225000 (47%)] Loss: 6980.625000\n",
      "Train Epoch: 333 [110848/225000 (49%)] Loss: 7039.986328\n",
      "Train Epoch: 333 [114944/225000 (51%)] Loss: 6772.380859\n",
      "Train Epoch: 333 [119040/225000 (53%)] Loss: 6910.832031\n",
      "Train Epoch: 333 [123136/225000 (55%)] Loss: 6933.933594\n",
      "Train Epoch: 333 [127232/225000 (57%)] Loss: 6817.529297\n",
      "Train Epoch: 333 [131328/225000 (58%)] Loss: 6967.431641\n",
      "Train Epoch: 333 [135424/225000 (60%)] Loss: 6883.822266\n",
      "Train Epoch: 333 [139520/225000 (62%)] Loss: 6661.662109\n",
      "Train Epoch: 333 [143616/225000 (64%)] Loss: 7029.859375\n",
      "Train Epoch: 333 [147712/225000 (66%)] Loss: 6871.167969\n",
      "Train Epoch: 333 [151808/225000 (67%)] Loss: 6911.134766\n",
      "Train Epoch: 333 [155904/225000 (69%)] Loss: 6893.468750\n",
      "Train Epoch: 333 [160000/225000 (71%)] Loss: 6791.519531\n",
      "Train Epoch: 333 [164096/225000 (73%)] Loss: 6921.986328\n",
      "Train Epoch: 333 [168192/225000 (75%)] Loss: 6982.232422\n",
      "Train Epoch: 333 [172288/225000 (77%)] Loss: 6897.378906\n",
      "Train Epoch: 333 [176384/225000 (78%)] Loss: 6888.310547\n",
      "Train Epoch: 333 [180480/225000 (80%)] Loss: 6847.359375\n",
      "Train Epoch: 333 [184576/225000 (82%)] Loss: 6907.826172\n",
      "Train Epoch: 333 [188672/225000 (84%)] Loss: 6780.632812\n",
      "Train Epoch: 333 [192768/225000 (86%)] Loss: 6830.306641\n",
      "Train Epoch: 333 [196864/225000 (87%)] Loss: 6711.105469\n",
      "Train Epoch: 333 [200960/225000 (89%)] Loss: 6860.369141\n",
      "Train Epoch: 333 [205056/225000 (91%)] Loss: 6939.740234\n",
      "Train Epoch: 333 [209152/225000 (93%)] Loss: 6826.097656\n",
      "Train Epoch: 333 [213248/225000 (95%)] Loss: 6874.404297\n",
      "Train Epoch: 333 [217344/225000 (97%)] Loss: 6794.843750\n",
      "Train Epoch: 333 [221440/225000 (98%)] Loss: 6860.667969\n",
      "    epoch          : 333\n",
      "    loss           : 6925.968851100327\n",
      "    val_loss       : 6960.520190384923\n",
      "Train Epoch: 334 [256/225000 (0%)] Loss: 6850.136719\n",
      "Train Epoch: 334 [4352/225000 (2%)] Loss: 6892.462891\n",
      "Train Epoch: 334 [8448/225000 (4%)] Loss: 6819.876953\n",
      "Train Epoch: 334 [12544/225000 (6%)] Loss: 6874.542969\n",
      "Train Epoch: 334 [16640/225000 (7%)] Loss: 6730.810547\n",
      "Train Epoch: 334 [20736/225000 (9%)] Loss: 6996.509766\n",
      "Train Epoch: 334 [24832/225000 (11%)] Loss: 6997.201172\n",
      "Train Epoch: 334 [28928/225000 (13%)] Loss: 6758.832031\n",
      "Train Epoch: 334 [33024/225000 (15%)] Loss: 6723.152344\n",
      "Train Epoch: 334 [37120/225000 (16%)] Loss: 6940.052734\n",
      "Train Epoch: 334 [41216/225000 (18%)] Loss: 6760.552734\n",
      "Train Epoch: 334 [45312/225000 (20%)] Loss: 6864.564453\n",
      "Train Epoch: 334 [49408/225000 (22%)] Loss: 6874.183594\n",
      "Train Epoch: 334 [53504/225000 (24%)] Loss: 7083.757812\n",
      "Train Epoch: 334 [57600/225000 (26%)] Loss: 6853.710938\n",
      "Train Epoch: 334 [61696/225000 (27%)] Loss: 6855.828125\n",
      "Train Epoch: 334 [65792/225000 (29%)] Loss: 6901.019531\n",
      "Train Epoch: 334 [69888/225000 (31%)] Loss: 6846.171875\n",
      "Train Epoch: 334 [73984/225000 (33%)] Loss: 6865.921875\n",
      "Train Epoch: 334 [78080/225000 (35%)] Loss: 6920.207031\n",
      "Train Epoch: 334 [82176/225000 (37%)] Loss: 6815.423828\n",
      "Train Epoch: 334 [86272/225000 (38%)] Loss: 6987.503906\n",
      "Train Epoch: 334 [90368/225000 (40%)] Loss: 6791.861328\n",
      "Train Epoch: 334 [94464/225000 (42%)] Loss: 6891.107422\n",
      "Train Epoch: 334 [98560/225000 (44%)] Loss: 6814.923828\n",
      "Train Epoch: 334 [102656/225000 (46%)] Loss: 6843.867188\n",
      "Train Epoch: 334 [106752/225000 (47%)] Loss: 7040.121094\n",
      "Train Epoch: 334 [110848/225000 (49%)] Loss: 6846.859375\n",
      "Train Epoch: 334 [114944/225000 (51%)] Loss: 6931.447266\n",
      "Train Epoch: 334 [119040/225000 (53%)] Loss: 6953.763672\n",
      "Train Epoch: 334 [123136/225000 (55%)] Loss: 6839.048828\n",
      "Train Epoch: 334 [127232/225000 (57%)] Loss: 7106.179688\n",
      "Train Epoch: 334 [131328/225000 (58%)] Loss: 6968.462891\n",
      "Train Epoch: 334 [135424/225000 (60%)] Loss: 6831.683594\n",
      "Train Epoch: 334 [139520/225000 (62%)] Loss: 6976.558594\n",
      "Train Epoch: 334 [143616/225000 (64%)] Loss: 6915.111328\n",
      "Train Epoch: 334 [147712/225000 (66%)] Loss: 7017.800781\n",
      "Train Epoch: 334 [151808/225000 (67%)] Loss: 6938.863281\n",
      "Train Epoch: 334 [155904/225000 (69%)] Loss: 6908.410156\n",
      "Train Epoch: 334 [160000/225000 (71%)] Loss: 7027.476562\n",
      "Train Epoch: 334 [164096/225000 (73%)] Loss: 6894.484375\n",
      "Train Epoch: 334 [168192/225000 (75%)] Loss: 6957.839844\n",
      "Train Epoch: 334 [172288/225000 (77%)] Loss: 6728.271484\n",
      "Train Epoch: 334 [176384/225000 (78%)] Loss: 6805.728516\n",
      "Train Epoch: 334 [180480/225000 (80%)] Loss: 6861.468750\n",
      "Train Epoch: 334 [184576/225000 (82%)] Loss: 7001.232422\n",
      "Train Epoch: 334 [188672/225000 (84%)] Loss: 6844.814453\n",
      "Train Epoch: 334 [192768/225000 (86%)] Loss: 6838.056641\n",
      "Train Epoch: 334 [196864/225000 (87%)] Loss: 6946.363281\n",
      "Train Epoch: 334 [200960/225000 (89%)] Loss: 6765.115234\n",
      "Train Epoch: 334 [205056/225000 (91%)] Loss: 6944.027344\n",
      "Train Epoch: 334 [209152/225000 (93%)] Loss: 6905.447266\n",
      "Train Epoch: 334 [213248/225000 (95%)] Loss: 6831.740234\n",
      "Train Epoch: 334 [217344/225000 (97%)] Loss: 6726.058594\n",
      "Train Epoch: 334 [221440/225000 (98%)] Loss: 6876.005859\n",
      "    epoch          : 334\n",
      "    loss           : 6900.643900206201\n",
      "    val_loss       : 7027.055929222885\n",
      "Train Epoch: 335 [256/225000 (0%)] Loss: 6797.947266\n",
      "Train Epoch: 335 [4352/225000 (2%)] Loss: 7011.292969\n",
      "Train Epoch: 335 [8448/225000 (4%)] Loss: 6887.308594\n",
      "Train Epoch: 335 [12544/225000 (6%)] Loss: 7009.406250\n",
      "Train Epoch: 335 [16640/225000 (7%)] Loss: 7098.365234\n",
      "Train Epoch: 335 [20736/225000 (9%)] Loss: 6880.271484\n",
      "Train Epoch: 335 [24832/225000 (11%)] Loss: 6886.623047\n",
      "Train Epoch: 335 [28928/225000 (13%)] Loss: 6885.218750\n",
      "Train Epoch: 335 [33024/225000 (15%)] Loss: 6859.339844\n",
      "Train Epoch: 335 [37120/225000 (16%)] Loss: 6900.505859\n",
      "Train Epoch: 335 [41216/225000 (18%)] Loss: 6818.423828\n",
      "Train Epoch: 335 [45312/225000 (20%)] Loss: 6917.039062\n",
      "Train Epoch: 335 [49408/225000 (22%)] Loss: 6995.494141\n",
      "Train Epoch: 335 [53504/225000 (24%)] Loss: 6949.486328\n",
      "Train Epoch: 335 [57600/225000 (26%)] Loss: 6781.964844\n",
      "Train Epoch: 335 [61696/225000 (27%)] Loss: 6831.482422\n",
      "Train Epoch: 335 [65792/225000 (29%)] Loss: 6943.753906\n",
      "Train Epoch: 335 [69888/225000 (31%)] Loss: 6948.642578\n",
      "Train Epoch: 335 [73984/225000 (33%)] Loss: 6984.533203\n",
      "Train Epoch: 335 [78080/225000 (35%)] Loss: 6964.351562\n",
      "Train Epoch: 335 [82176/225000 (37%)] Loss: 6914.082031\n",
      "Train Epoch: 335 [86272/225000 (38%)] Loss: 6908.175781\n",
      "Train Epoch: 335 [90368/225000 (40%)] Loss: 6696.173828\n",
      "Train Epoch: 335 [94464/225000 (42%)] Loss: 6971.189453\n",
      "Train Epoch: 335 [98560/225000 (44%)] Loss: 6790.896484\n",
      "Train Epoch: 335 [102656/225000 (46%)] Loss: 6847.900391\n",
      "Train Epoch: 335 [106752/225000 (47%)] Loss: 6781.757812\n",
      "Train Epoch: 335 [110848/225000 (49%)] Loss: 6983.126953\n",
      "Train Epoch: 335 [114944/225000 (51%)] Loss: 7004.273438\n",
      "Train Epoch: 335 [119040/225000 (53%)] Loss: 6833.279297\n",
      "Train Epoch: 335 [123136/225000 (55%)] Loss: 6804.722656\n",
      "Train Epoch: 335 [127232/225000 (57%)] Loss: 6919.730469\n",
      "Train Epoch: 335 [131328/225000 (58%)] Loss: 6855.441406\n",
      "Train Epoch: 335 [135424/225000 (60%)] Loss: 6948.832031\n",
      "Train Epoch: 335 [139520/225000 (62%)] Loss: 6843.390625\n",
      "Train Epoch: 335 [143616/225000 (64%)] Loss: 6844.718750\n",
      "Train Epoch: 335 [147712/225000 (66%)] Loss: 6938.167969\n",
      "Train Epoch: 335 [151808/225000 (67%)] Loss: 6988.515625\n",
      "Train Epoch: 335 [155904/225000 (69%)] Loss: 7010.203125\n",
      "Train Epoch: 335 [160000/225000 (71%)] Loss: 6964.228516\n",
      "Train Epoch: 335 [164096/225000 (73%)] Loss: 6832.234375\n",
      "Train Epoch: 335 [168192/225000 (75%)] Loss: 6801.859375\n",
      "Train Epoch: 335 [172288/225000 (77%)] Loss: 6884.886719\n",
      "Train Epoch: 335 [176384/225000 (78%)] Loss: 6832.349609\n",
      "Train Epoch: 335 [180480/225000 (80%)] Loss: 6802.169922\n",
      "Train Epoch: 335 [184576/225000 (82%)] Loss: 6916.718750\n",
      "Train Epoch: 335 [188672/225000 (84%)] Loss: 6742.574219\n",
      "Train Epoch: 335 [192768/225000 (86%)] Loss: 6813.130859\n",
      "Train Epoch: 335 [196864/225000 (87%)] Loss: 6812.419922\n",
      "Train Epoch: 335 [200960/225000 (89%)] Loss: 7034.792969\n",
      "Train Epoch: 335 [205056/225000 (91%)] Loss: 6779.425781\n",
      "Train Epoch: 335 [209152/225000 (93%)] Loss: 6770.568359\n",
      "Train Epoch: 335 [213248/225000 (95%)] Loss: 6976.390625\n",
      "Train Epoch: 335 [217344/225000 (97%)] Loss: 6910.394531\n",
      "Train Epoch: 335 [221440/225000 (98%)] Loss: 7001.439453\n",
      "    epoch          : 335\n",
      "    loss           : 6898.235893726891\n",
      "    val_loss       : 6903.7132874824565\n",
      "Train Epoch: 336 [256/225000 (0%)] Loss: 6793.423828\n",
      "Train Epoch: 336 [4352/225000 (2%)] Loss: 6753.115234\n",
      "Train Epoch: 336 [8448/225000 (4%)] Loss: 6919.892578\n",
      "Train Epoch: 336 [12544/225000 (6%)] Loss: 6716.857422\n",
      "Train Epoch: 336 [16640/225000 (7%)] Loss: 6929.480469\n",
      "Train Epoch: 336 [20736/225000 (9%)] Loss: 6736.048828\n",
      "Train Epoch: 336 [24832/225000 (11%)] Loss: 6949.988281\n",
      "Train Epoch: 336 [28928/225000 (13%)] Loss: 6872.308594\n",
      "Train Epoch: 336 [33024/225000 (15%)] Loss: 6966.414062\n",
      "Train Epoch: 336 [37120/225000 (16%)] Loss: 6941.265625\n",
      "Train Epoch: 336 [41216/225000 (18%)] Loss: 7002.507812\n",
      "Train Epoch: 336 [45312/225000 (20%)] Loss: 7013.490234\n",
      "Train Epoch: 336 [49408/225000 (22%)] Loss: 6888.847656\n",
      "Train Epoch: 336 [53504/225000 (24%)] Loss: 6847.126953\n",
      "Train Epoch: 336 [57600/225000 (26%)] Loss: 6927.228516\n",
      "Train Epoch: 336 [61696/225000 (27%)] Loss: 6812.554688\n",
      "Train Epoch: 336 [65792/225000 (29%)] Loss: 6860.402344\n",
      "Train Epoch: 336 [69888/225000 (31%)] Loss: 6941.869141\n",
      "Train Epoch: 336 [73984/225000 (33%)] Loss: 6832.410156\n",
      "Train Epoch: 336 [78080/225000 (35%)] Loss: 6883.296875\n",
      "Train Epoch: 336 [82176/225000 (37%)] Loss: 6898.818359\n",
      "Train Epoch: 336 [86272/225000 (38%)] Loss: 6912.626953\n",
      "Train Epoch: 336 [90368/225000 (40%)] Loss: 6882.937500\n",
      "Train Epoch: 336 [94464/225000 (42%)] Loss: 6823.314453\n",
      "Train Epoch: 336 [98560/225000 (44%)] Loss: 7008.322266\n",
      "Train Epoch: 336 [102656/225000 (46%)] Loss: 6982.494141\n",
      "Train Epoch: 336 [106752/225000 (47%)] Loss: 6985.246094\n",
      "Train Epoch: 336 [110848/225000 (49%)] Loss: 6777.740234\n",
      "Train Epoch: 336 [114944/225000 (51%)] Loss: 6811.736328\n",
      "Train Epoch: 336 [119040/225000 (53%)] Loss: 6858.792969\n",
      "Train Epoch: 336 [123136/225000 (55%)] Loss: 6830.912109\n",
      "Train Epoch: 336 [127232/225000 (57%)] Loss: 6900.523438\n",
      "Train Epoch: 336 [131328/225000 (58%)] Loss: 6958.142578\n",
      "Train Epoch: 336 [135424/225000 (60%)] Loss: 6926.433594\n",
      "Train Epoch: 336 [139520/225000 (62%)] Loss: 6965.373047\n",
      "Train Epoch: 336 [143616/225000 (64%)] Loss: 6785.552734\n",
      "Train Epoch: 336 [147712/225000 (66%)] Loss: 6811.435547\n",
      "Train Epoch: 336 [151808/225000 (67%)] Loss: 6999.152344\n",
      "Train Epoch: 336 [155904/225000 (69%)] Loss: 6819.029297\n",
      "Train Epoch: 336 [160000/225000 (71%)] Loss: 6920.878906\n",
      "Train Epoch: 336 [164096/225000 (73%)] Loss: 6815.156250\n",
      "Train Epoch: 336 [168192/225000 (75%)] Loss: 6859.900391\n",
      "Train Epoch: 336 [172288/225000 (77%)] Loss: 6759.027344\n",
      "Train Epoch: 336 [176384/225000 (78%)] Loss: 6901.335938\n",
      "Train Epoch: 336 [180480/225000 (80%)] Loss: 6784.337891\n",
      "Train Epoch: 336 [184576/225000 (82%)] Loss: 6877.435547\n",
      "Train Epoch: 336 [188672/225000 (84%)] Loss: 6947.183594\n",
      "Train Epoch: 336 [192768/225000 (86%)] Loss: 6889.005859\n",
      "Train Epoch: 336 [196864/225000 (87%)] Loss: 6967.513672\n",
      "Train Epoch: 336 [200960/225000 (89%)] Loss: 6813.640625\n",
      "Train Epoch: 336 [205056/225000 (91%)] Loss: 6888.816406\n",
      "Train Epoch: 336 [209152/225000 (93%)] Loss: 6896.474609\n",
      "Train Epoch: 336 [213248/225000 (95%)] Loss: 6984.312500\n",
      "Train Epoch: 336 [217344/225000 (97%)] Loss: 7027.681641\n",
      "Train Epoch: 336 [221440/225000 (98%)] Loss: 6903.666016\n",
      "    epoch          : 336\n",
      "    loss           : 6906.843502248649\n",
      "    val_loss       : 6901.812723232775\n",
      "Train Epoch: 337 [256/225000 (0%)] Loss: 6917.666016\n",
      "Train Epoch: 337 [4352/225000 (2%)] Loss: 6832.757812\n",
      "Train Epoch: 337 [8448/225000 (4%)] Loss: 6849.046875\n",
      "Train Epoch: 337 [12544/225000 (6%)] Loss: 6802.992188\n",
      "Train Epoch: 337 [16640/225000 (7%)] Loss: 6897.007812\n",
      "Train Epoch: 337 [20736/225000 (9%)] Loss: 6789.531250\n",
      "Train Epoch: 337 [24832/225000 (11%)] Loss: 6916.867188\n",
      "Train Epoch: 337 [28928/225000 (13%)] Loss: 6958.628906\n",
      "Train Epoch: 337 [33024/225000 (15%)] Loss: 6718.437500\n",
      "Train Epoch: 337 [37120/225000 (16%)] Loss: 7024.351562\n",
      "Train Epoch: 337 [41216/225000 (18%)] Loss: 6934.878906\n",
      "Train Epoch: 337 [45312/225000 (20%)] Loss: 6835.664062\n",
      "Train Epoch: 337 [49408/225000 (22%)] Loss: 6903.615234\n",
      "Train Epoch: 337 [53504/225000 (24%)] Loss: 6828.166016\n",
      "Train Epoch: 337 [57600/225000 (26%)] Loss: 6843.123047\n",
      "Train Epoch: 337 [61696/225000 (27%)] Loss: 6987.587891\n",
      "Train Epoch: 337 [65792/225000 (29%)] Loss: 6820.392578\n",
      "Train Epoch: 337 [69888/225000 (31%)] Loss: 6958.822266\n",
      "Train Epoch: 337 [73984/225000 (33%)] Loss: 7063.181641\n",
      "Train Epoch: 337 [78080/225000 (35%)] Loss: 6792.455078\n",
      "Train Epoch: 337 [82176/225000 (37%)] Loss: 6831.931641\n",
      "Train Epoch: 337 [86272/225000 (38%)] Loss: 6903.595703\n",
      "Train Epoch: 337 [90368/225000 (40%)] Loss: 6682.888672\n",
      "Train Epoch: 337 [94464/225000 (42%)] Loss: 6963.380859\n",
      "Train Epoch: 337 [98560/225000 (44%)] Loss: 6882.269531\n",
      "Train Epoch: 337 [102656/225000 (46%)] Loss: 7006.125000\n",
      "Train Epoch: 337 [106752/225000 (47%)] Loss: 6784.027344\n",
      "Train Epoch: 337 [110848/225000 (49%)] Loss: 6937.189453\n",
      "Train Epoch: 337 [114944/225000 (51%)] Loss: 6824.390625\n",
      "Train Epoch: 337 [119040/225000 (53%)] Loss: 6931.986328\n",
      "Train Epoch: 337 [123136/225000 (55%)] Loss: 6848.404297\n",
      "Train Epoch: 337 [127232/225000 (57%)] Loss: 7035.667969\n",
      "Train Epoch: 337 [131328/225000 (58%)] Loss: 6789.955078\n",
      "Train Epoch: 337 [135424/225000 (60%)] Loss: 6893.693359\n",
      "Train Epoch: 337 [139520/225000 (62%)] Loss: 6895.761719\n",
      "Train Epoch: 337 [143616/225000 (64%)] Loss: 6925.875000\n",
      "Train Epoch: 337 [147712/225000 (66%)] Loss: 6776.228516\n",
      "Train Epoch: 337 [151808/225000 (67%)] Loss: 6902.343750\n",
      "Train Epoch: 337 [155904/225000 (69%)] Loss: 6786.982422\n",
      "Train Epoch: 337 [160000/225000 (71%)] Loss: 6973.015625\n",
      "Train Epoch: 337 [164096/225000 (73%)] Loss: 6975.671875\n",
      "Train Epoch: 337 [168192/225000 (75%)] Loss: 6885.384766\n",
      "Train Epoch: 337 [172288/225000 (77%)] Loss: 6811.285156\n",
      "Train Epoch: 337 [176384/225000 (78%)] Loss: 6921.945312\n",
      "Train Epoch: 337 [180480/225000 (80%)] Loss: 6871.146484\n",
      "Train Epoch: 337 [184576/225000 (82%)] Loss: 6868.744141\n",
      "Train Epoch: 337 [188672/225000 (84%)] Loss: 6936.197266\n",
      "Train Epoch: 337 [192768/225000 (86%)] Loss: 6871.427734\n",
      "Train Epoch: 337 [196864/225000 (87%)] Loss: 6880.263672\n",
      "Train Epoch: 337 [200960/225000 (89%)] Loss: 6840.423828\n",
      "Train Epoch: 337 [205056/225000 (91%)] Loss: 6911.619141\n",
      "Train Epoch: 337 [209152/225000 (93%)] Loss: 6886.242188\n",
      "Train Epoch: 337 [213248/225000 (95%)] Loss: 6816.582031\n",
      "Train Epoch: 337 [217344/225000 (97%)] Loss: 6765.085938\n",
      "Train Epoch: 337 [221440/225000 (98%)] Loss: 6897.083984\n",
      "    epoch          : 337\n",
      "    loss           : 6896.029555736277\n",
      "    val_loss       : 6903.944807694883\n",
      "Train Epoch: 338 [256/225000 (0%)] Loss: 6822.890625\n",
      "Train Epoch: 338 [4352/225000 (2%)] Loss: 6895.232422\n",
      "Train Epoch: 338 [8448/225000 (4%)] Loss: 6906.605469\n",
      "Train Epoch: 338 [12544/225000 (6%)] Loss: 6975.341797\n",
      "Train Epoch: 338 [16640/225000 (7%)] Loss: 6970.050781\n",
      "Train Epoch: 338 [20736/225000 (9%)] Loss: 6847.861328\n",
      "Train Epoch: 338 [24832/225000 (11%)] Loss: 6923.656250\n",
      "Train Epoch: 338 [28928/225000 (13%)] Loss: 6743.267578\n",
      "Train Epoch: 338 [33024/225000 (15%)] Loss: 6823.066406\n",
      "Train Epoch: 338 [37120/225000 (16%)] Loss: 6917.101562\n",
      "Train Epoch: 338 [41216/225000 (18%)] Loss: 6815.292969\n",
      "Train Epoch: 338 [45312/225000 (20%)] Loss: 6845.515625\n",
      "Train Epoch: 338 [49408/225000 (22%)] Loss: 6880.832031\n",
      "Train Epoch: 338 [53504/225000 (24%)] Loss: 6930.597656\n",
      "Train Epoch: 338 [57600/225000 (26%)] Loss: 6996.886719\n",
      "Train Epoch: 338 [61696/225000 (27%)] Loss: 6928.789062\n",
      "Train Epoch: 338 [65792/225000 (29%)] Loss: 6871.937500\n",
      "Train Epoch: 338 [69888/225000 (31%)] Loss: 6939.347656\n",
      "Train Epoch: 338 [73984/225000 (33%)] Loss: 7005.730469\n",
      "Train Epoch: 338 [78080/225000 (35%)] Loss: 6878.345703\n",
      "Train Epoch: 338 [82176/225000 (37%)] Loss: 6889.142578\n",
      "Train Epoch: 338 [86272/225000 (38%)] Loss: 6869.921875\n",
      "Train Epoch: 338 [90368/225000 (40%)] Loss: 6962.242188\n",
      "Train Epoch: 338 [94464/225000 (42%)] Loss: 7048.900391\n",
      "Train Epoch: 338 [98560/225000 (44%)] Loss: 6824.148438\n",
      "Train Epoch: 338 [102656/225000 (46%)] Loss: 6802.181641\n",
      "Train Epoch: 338 [106752/225000 (47%)] Loss: 6914.753906\n",
      "Train Epoch: 338 [110848/225000 (49%)] Loss: 7008.158203\n",
      "Train Epoch: 338 [114944/225000 (51%)] Loss: 6583.185547\n",
      "Train Epoch: 338 [119040/225000 (53%)] Loss: 6938.226562\n",
      "Train Epoch: 338 [123136/225000 (55%)] Loss: 6848.621094\n",
      "Train Epoch: 338 [127232/225000 (57%)] Loss: 6829.232422\n",
      "Train Epoch: 338 [131328/225000 (58%)] Loss: 6878.945312\n",
      "Train Epoch: 338 [135424/225000 (60%)] Loss: 7123.654297\n",
      "Train Epoch: 338 [139520/225000 (62%)] Loss: 6940.464844\n",
      "Train Epoch: 338 [143616/225000 (64%)] Loss: 6998.044922\n",
      "Train Epoch: 338 [147712/225000 (66%)] Loss: 6913.882812\n",
      "Train Epoch: 338 [151808/225000 (67%)] Loss: 6880.607422\n",
      "Train Epoch: 338 [155904/225000 (69%)] Loss: 6958.298828\n",
      "Train Epoch: 338 [160000/225000 (71%)] Loss: 6902.326172\n",
      "Train Epoch: 338 [164096/225000 (73%)] Loss: 6815.294922\n",
      "Train Epoch: 338 [168192/225000 (75%)] Loss: 6897.335938\n",
      "Train Epoch: 338 [172288/225000 (77%)] Loss: 6852.617188\n",
      "Train Epoch: 338 [176384/225000 (78%)] Loss: 7008.355469\n",
      "Train Epoch: 338 [180480/225000 (80%)] Loss: 6983.626953\n",
      "Train Epoch: 338 [184576/225000 (82%)] Loss: 6777.287109\n",
      "Train Epoch: 338 [188672/225000 (84%)] Loss: 6854.792969\n",
      "Train Epoch: 338 [192768/225000 (86%)] Loss: 6699.873047\n",
      "Train Epoch: 338 [196864/225000 (87%)] Loss: 6815.181641\n",
      "Train Epoch: 338 [200960/225000 (89%)] Loss: 7010.652344\n",
      "Train Epoch: 338 [205056/225000 (91%)] Loss: 6989.984375\n",
      "Train Epoch: 338 [209152/225000 (93%)] Loss: 6906.958984\n",
      "Train Epoch: 338 [213248/225000 (95%)] Loss: 6797.566406\n",
      "Train Epoch: 338 [217344/225000 (97%)] Loss: 6818.476562\n",
      "Train Epoch: 338 [221440/225000 (98%)] Loss: 6944.437500\n",
      "    epoch          : 338\n",
      "    loss           : 6888.464823752133\n",
      "    val_loss       : 6975.793243327919\n",
      "Train Epoch: 339 [256/225000 (0%)] Loss: 6829.144531\n",
      "Train Epoch: 339 [4352/225000 (2%)] Loss: 6886.625000\n",
      "Train Epoch: 339 [8448/225000 (4%)] Loss: 6998.162109\n",
      "Train Epoch: 339 [12544/225000 (6%)] Loss: 7100.986328\n",
      "Train Epoch: 339 [16640/225000 (7%)] Loss: 6876.482422\n",
      "Train Epoch: 339 [20736/225000 (9%)] Loss: 6724.109375\n",
      "Train Epoch: 339 [24832/225000 (11%)] Loss: 6893.166016\n",
      "Train Epoch: 339 [28928/225000 (13%)] Loss: 6925.894531\n",
      "Train Epoch: 339 [33024/225000 (15%)] Loss: 6801.982422\n",
      "Train Epoch: 339 [37120/225000 (16%)] Loss: 6793.771484\n",
      "Train Epoch: 339 [41216/225000 (18%)] Loss: 6836.851562\n",
      "Train Epoch: 339 [45312/225000 (20%)] Loss: 6761.125000\n",
      "Train Epoch: 339 [49408/225000 (22%)] Loss: 6860.253906\n",
      "Train Epoch: 339 [53504/225000 (24%)] Loss: 6906.267578\n",
      "Train Epoch: 339 [57600/225000 (26%)] Loss: 6804.412109\n",
      "Train Epoch: 339 [61696/225000 (27%)] Loss: 6933.542969\n",
      "Train Epoch: 339 [65792/225000 (29%)] Loss: 6939.890625\n",
      "Train Epoch: 339 [69888/225000 (31%)] Loss: 6862.212891\n",
      "Train Epoch: 339 [73984/225000 (33%)] Loss: 6930.400391\n",
      "Train Epoch: 339 [78080/225000 (35%)] Loss: 6837.302734\n",
      "Train Epoch: 339 [82176/225000 (37%)] Loss: 6802.333984\n",
      "Train Epoch: 339 [86272/225000 (38%)] Loss: 7160.951172\n",
      "Train Epoch: 339 [90368/225000 (40%)] Loss: 6854.263672\n",
      "Train Epoch: 339 [94464/225000 (42%)] Loss: 6862.298828\n",
      "Train Epoch: 339 [98560/225000 (44%)] Loss: 6813.849609\n",
      "Train Epoch: 339 [102656/225000 (46%)] Loss: 7024.400391\n",
      "Train Epoch: 339 [106752/225000 (47%)] Loss: 6879.488281\n",
      "Train Epoch: 339 [110848/225000 (49%)] Loss: 6943.029297\n",
      "Train Epoch: 339 [114944/225000 (51%)] Loss: 6884.492188\n",
      "Train Epoch: 339 [119040/225000 (53%)] Loss: 6809.017578\n",
      "Train Epoch: 339 [123136/225000 (55%)] Loss: 6922.023438\n",
      "Train Epoch: 339 [127232/225000 (57%)] Loss: 6993.865234\n",
      "Train Epoch: 339 [131328/225000 (58%)] Loss: 6863.365234\n",
      "Train Epoch: 339 [135424/225000 (60%)] Loss: 6848.458984\n",
      "Train Epoch: 339 [139520/225000 (62%)] Loss: 6900.273438\n",
      "Train Epoch: 339 [143616/225000 (64%)] Loss: 6882.943359\n",
      "Train Epoch: 339 [147712/225000 (66%)] Loss: 6778.574219\n",
      "Train Epoch: 339 [151808/225000 (67%)] Loss: 6986.113281\n",
      "Train Epoch: 339 [155904/225000 (69%)] Loss: 6877.425781\n",
      "Train Epoch: 339 [160000/225000 (71%)] Loss: 6856.048828\n",
      "Train Epoch: 339 [164096/225000 (73%)] Loss: 6827.681641\n",
      "Train Epoch: 339 [168192/225000 (75%)] Loss: 6762.099609\n",
      "Train Epoch: 339 [172288/225000 (77%)] Loss: 7080.861328\n",
      "Train Epoch: 339 [176384/225000 (78%)] Loss: 6860.726562\n",
      "Train Epoch: 339 [180480/225000 (80%)] Loss: 6984.863281\n",
      "Train Epoch: 339 [184576/225000 (82%)] Loss: 6891.187500\n",
      "Train Epoch: 339 [188672/225000 (84%)] Loss: 6929.599609\n",
      "Train Epoch: 339 [192768/225000 (86%)] Loss: 6991.777344\n",
      "Train Epoch: 339 [196864/225000 (87%)] Loss: 7002.632812\n",
      "Train Epoch: 339 [200960/225000 (89%)] Loss: 6922.757812\n",
      "Train Epoch: 339 [205056/225000 (91%)] Loss: 6893.103516\n",
      "Train Epoch: 339 [209152/225000 (93%)] Loss: 7004.927734\n",
      "Train Epoch: 339 [213248/225000 (95%)] Loss: 6906.710938\n",
      "Train Epoch: 339 [217344/225000 (97%)] Loss: 6960.734375\n",
      "Train Epoch: 339 [221440/225000 (98%)] Loss: 6819.851562\n",
      "    epoch          : 339\n",
      "    loss           : 6934.179177554394\n",
      "    val_loss       : 6901.747020523158\n",
      "Train Epoch: 340 [256/225000 (0%)] Loss: 6832.646484\n",
      "Train Epoch: 340 [4352/225000 (2%)] Loss: 6823.861328\n",
      "Train Epoch: 340 [8448/225000 (4%)] Loss: 6956.208984\n",
      "Train Epoch: 340 [12544/225000 (6%)] Loss: 6883.134766\n",
      "Train Epoch: 340 [16640/225000 (7%)] Loss: 6922.322266\n",
      "Train Epoch: 340 [20736/225000 (9%)] Loss: 7028.593750\n",
      "Train Epoch: 340 [24832/225000 (11%)] Loss: 6830.855469\n",
      "Train Epoch: 340 [28928/225000 (13%)] Loss: 6991.337891\n",
      "Train Epoch: 340 [33024/225000 (15%)] Loss: 6944.814453\n",
      "Train Epoch: 340 [37120/225000 (16%)] Loss: 6904.490234\n",
      "Train Epoch: 340 [41216/225000 (18%)] Loss: 7042.537109\n",
      "Train Epoch: 340 [45312/225000 (20%)] Loss: 7035.492188\n",
      "Train Epoch: 340 [49408/225000 (22%)] Loss: 6898.128906\n",
      "Train Epoch: 340 [53504/225000 (24%)] Loss: 6906.046875\n",
      "Train Epoch: 340 [57600/225000 (26%)] Loss: 6921.025391\n",
      "Train Epoch: 340 [61696/225000 (27%)] Loss: 7092.832031\n",
      "Train Epoch: 340 [65792/225000 (29%)] Loss: 6956.712891\n",
      "Train Epoch: 340 [69888/225000 (31%)] Loss: 6855.703125\n",
      "Train Epoch: 340 [73984/225000 (33%)] Loss: 6938.632812\n",
      "Train Epoch: 340 [78080/225000 (35%)] Loss: 6956.683594\n",
      "Train Epoch: 340 [82176/225000 (37%)] Loss: 6850.322266\n",
      "Train Epoch: 340 [86272/225000 (38%)] Loss: 6899.876953\n",
      "Train Epoch: 340 [90368/225000 (40%)] Loss: 6964.261719\n",
      "Train Epoch: 340 [94464/225000 (42%)] Loss: 6979.416016\n",
      "Train Epoch: 340 [98560/225000 (44%)] Loss: 6848.583984\n",
      "Train Epoch: 340 [102656/225000 (46%)] Loss: 6953.345703\n",
      "Train Epoch: 340 [106752/225000 (47%)] Loss: 6965.775391\n",
      "Train Epoch: 340 [110848/225000 (49%)] Loss: 6851.662109\n",
      "Train Epoch: 340 [114944/225000 (51%)] Loss: 7037.753906\n",
      "Train Epoch: 340 [119040/225000 (53%)] Loss: 6921.101562\n",
      "Train Epoch: 340 [123136/225000 (55%)] Loss: 6861.138672\n",
      "Train Epoch: 340 [127232/225000 (57%)] Loss: 6789.839844\n",
      "Train Epoch: 340 [131328/225000 (58%)] Loss: 6950.929688\n",
      "Train Epoch: 340 [135424/225000 (60%)] Loss: 6995.816406\n",
      "Train Epoch: 340 [139520/225000 (62%)] Loss: 6822.955078\n",
      "Train Epoch: 340 [143616/225000 (64%)] Loss: 6876.175781\n",
      "Train Epoch: 340 [147712/225000 (66%)] Loss: 6864.294922\n",
      "Train Epoch: 340 [151808/225000 (67%)] Loss: 6758.136719\n",
      "Train Epoch: 340 [155904/225000 (69%)] Loss: 7081.062500\n",
      "Train Epoch: 340 [160000/225000 (71%)] Loss: 6981.576172\n",
      "Train Epoch: 340 [164096/225000 (73%)] Loss: 6840.486328\n",
      "Train Epoch: 340 [168192/225000 (75%)] Loss: 6806.837891\n",
      "Train Epoch: 340 [172288/225000 (77%)] Loss: 6834.167969\n",
      "Train Epoch: 340 [176384/225000 (78%)] Loss: 6899.298828\n",
      "Train Epoch: 340 [180480/225000 (80%)] Loss: 6870.175781\n",
      "Train Epoch: 340 [184576/225000 (82%)] Loss: 6702.972656\n",
      "Train Epoch: 340 [188672/225000 (84%)] Loss: 6801.609375\n",
      "Train Epoch: 340 [192768/225000 (86%)] Loss: 6918.636719\n",
      "Train Epoch: 340 [196864/225000 (87%)] Loss: 6792.205078\n",
      "Train Epoch: 340 [200960/225000 (89%)] Loss: 6816.126953\n",
      "Train Epoch: 340 [205056/225000 (91%)] Loss: 6907.214844\n",
      "Train Epoch: 340 [209152/225000 (93%)] Loss: 6884.867188\n",
      "Train Epoch: 340 [213248/225000 (95%)] Loss: 6886.019531\n",
      "Train Epoch: 340 [217344/225000 (97%)] Loss: 6878.240234\n",
      "Train Epoch: 340 [221440/225000 (98%)] Loss: 6893.150391\n",
      "    epoch          : 340\n",
      "    loss           : 6917.87387012052\n",
      "    val_loss       : 6894.2714176518575\n",
      "Train Epoch: 341 [256/225000 (0%)] Loss: 6746.177734\n",
      "Train Epoch: 341 [4352/225000 (2%)] Loss: 6910.253906\n",
      "Train Epoch: 341 [8448/225000 (4%)] Loss: 6931.714844\n",
      "Train Epoch: 341 [12544/225000 (6%)] Loss: 6901.476562\n",
      "Train Epoch: 341 [16640/225000 (7%)] Loss: 6736.617188\n",
      "Train Epoch: 341 [20736/225000 (9%)] Loss: 6835.050781\n",
      "Train Epoch: 341 [24832/225000 (11%)] Loss: 6926.025391\n",
      "Train Epoch: 341 [28928/225000 (13%)] Loss: 6825.509766\n",
      "Train Epoch: 341 [33024/225000 (15%)] Loss: 6748.875000\n",
      "Train Epoch: 341 [37120/225000 (16%)] Loss: 6887.216797\n",
      "Train Epoch: 341 [41216/225000 (18%)] Loss: 6776.292969\n",
      "Train Epoch: 341 [45312/225000 (20%)] Loss: 6858.775391\n",
      "Train Epoch: 341 [49408/225000 (22%)] Loss: 6932.716797\n",
      "Train Epoch: 341 [53504/225000 (24%)] Loss: 6927.591797\n",
      "Train Epoch: 341 [57600/225000 (26%)] Loss: 6634.169922\n",
      "Train Epoch: 341 [61696/225000 (27%)] Loss: 6710.097656\n",
      "Train Epoch: 341 [65792/225000 (29%)] Loss: 6786.605469\n",
      "Train Epoch: 341 [69888/225000 (31%)] Loss: 6878.728516\n",
      "Train Epoch: 341 [73984/225000 (33%)] Loss: 6903.117188\n",
      "Train Epoch: 341 [78080/225000 (35%)] Loss: 6928.693359\n",
      "Train Epoch: 341 [82176/225000 (37%)] Loss: 6943.656250\n",
      "Train Epoch: 341 [86272/225000 (38%)] Loss: 6955.769531\n",
      "Train Epoch: 341 [90368/225000 (40%)] Loss: 7001.085938\n",
      "Train Epoch: 341 [94464/225000 (42%)] Loss: 6733.619141\n",
      "Train Epoch: 341 [98560/225000 (44%)] Loss: 6800.447266\n",
      "Train Epoch: 341 [102656/225000 (46%)] Loss: 6956.742188\n",
      "Train Epoch: 341 [106752/225000 (47%)] Loss: 6978.687500\n",
      "Train Epoch: 341 [110848/225000 (49%)] Loss: 6971.964844\n",
      "Train Epoch: 341 [114944/225000 (51%)] Loss: 6803.488281\n",
      "Train Epoch: 341 [119040/225000 (53%)] Loss: 6732.345703\n",
      "Train Epoch: 341 [123136/225000 (55%)] Loss: 6819.597656\n",
      "Train Epoch: 341 [127232/225000 (57%)] Loss: 6822.060547\n",
      "Train Epoch: 341 [131328/225000 (58%)] Loss: 6944.277344\n",
      "Train Epoch: 341 [135424/225000 (60%)] Loss: 6942.472656\n",
      "Train Epoch: 341 [139520/225000 (62%)] Loss: 7019.378906\n",
      "Train Epoch: 341 [143616/225000 (64%)] Loss: 7022.925781\n",
      "Train Epoch: 341 [147712/225000 (66%)] Loss: 6837.923828\n",
      "Train Epoch: 341 [151808/225000 (67%)] Loss: 6870.033203\n",
      "Train Epoch: 341 [155904/225000 (69%)] Loss: 6987.453125\n",
      "Train Epoch: 341 [160000/225000 (71%)] Loss: 6823.003906\n",
      "Train Epoch: 341 [164096/225000 (73%)] Loss: 7128.406250\n",
      "Train Epoch: 341 [168192/225000 (75%)] Loss: 6924.310547\n",
      "Train Epoch: 341 [172288/225000 (77%)] Loss: 6772.886719\n",
      "Train Epoch: 341 [176384/225000 (78%)] Loss: 7005.115234\n",
      "Train Epoch: 341 [180480/225000 (80%)] Loss: 6898.785156\n",
      "Train Epoch: 341 [184576/225000 (82%)] Loss: 6976.000000\n",
      "Train Epoch: 341 [188672/225000 (84%)] Loss: 6891.863281\n",
      "Train Epoch: 341 [192768/225000 (86%)] Loss: 6883.138672\n",
      "Train Epoch: 341 [196864/225000 (87%)] Loss: 6973.732422\n",
      "Train Epoch: 341 [200960/225000 (89%)] Loss: 6790.763672\n",
      "Train Epoch: 341 [205056/225000 (91%)] Loss: 6958.302734\n",
      "Train Epoch: 341 [209152/225000 (93%)] Loss: 6906.384766\n",
      "Train Epoch: 341 [213248/225000 (95%)] Loss: 6806.066406\n",
      "Train Epoch: 341 [217344/225000 (97%)] Loss: 6913.638672\n",
      "Train Epoch: 341 [221440/225000 (98%)] Loss: 6835.488281\n",
      "    epoch          : 341\n",
      "    loss           : 6905.0133374662255\n",
      "    val_loss       : 6894.936059026694\n",
      "Train Epoch: 342 [256/225000 (0%)] Loss: 7033.062500\n",
      "Train Epoch: 342 [4352/225000 (2%)] Loss: 7061.166016\n",
      "Train Epoch: 342 [8448/225000 (4%)] Loss: 6883.671875\n",
      "Train Epoch: 342 [12544/225000 (6%)] Loss: 6812.535156\n",
      "Train Epoch: 342 [16640/225000 (7%)] Loss: 6907.652344\n",
      "Train Epoch: 342 [20736/225000 (9%)] Loss: 6786.441406\n",
      "Train Epoch: 342 [24832/225000 (11%)] Loss: 6788.736328\n",
      "Train Epoch: 342 [28928/225000 (13%)] Loss: 6605.888672\n",
      "Train Epoch: 342 [33024/225000 (15%)] Loss: 6811.015625\n",
      "Train Epoch: 342 [37120/225000 (16%)] Loss: 6880.619141\n",
      "Train Epoch: 342 [41216/225000 (18%)] Loss: 6753.037109\n",
      "Train Epoch: 342 [45312/225000 (20%)] Loss: 6972.255859\n",
      "Train Epoch: 342 [49408/225000 (22%)] Loss: 6823.886719\n",
      "Train Epoch: 342 [53504/225000 (24%)] Loss: 6810.470703\n",
      "Train Epoch: 342 [57600/225000 (26%)] Loss: 6988.529297\n",
      "Train Epoch: 342 [61696/225000 (27%)] Loss: 6827.130859\n",
      "Train Epoch: 342 [65792/225000 (29%)] Loss: 6766.078125\n",
      "Train Epoch: 342 [69888/225000 (31%)] Loss: 6926.732422\n",
      "Train Epoch: 342 [73984/225000 (33%)] Loss: 6878.921875\n",
      "Train Epoch: 342 [78080/225000 (35%)] Loss: 6890.892578\n",
      "Train Epoch: 342 [82176/225000 (37%)] Loss: 6842.287109\n",
      "Train Epoch: 342 [86272/225000 (38%)] Loss: 6789.597656\n",
      "Train Epoch: 342 [90368/225000 (40%)] Loss: 6933.216797\n",
      "Train Epoch: 342 [94464/225000 (42%)] Loss: 6878.652344\n",
      "Train Epoch: 342 [98560/225000 (44%)] Loss: 6899.728516\n",
      "Train Epoch: 342 [102656/225000 (46%)] Loss: 6870.005859\n",
      "Train Epoch: 342 [106752/225000 (47%)] Loss: 7014.849609\n",
      "Train Epoch: 342 [110848/225000 (49%)] Loss: 6831.257812\n",
      "Train Epoch: 342 [114944/225000 (51%)] Loss: 6977.162109\n",
      "Train Epoch: 342 [119040/225000 (53%)] Loss: 6784.195312\n",
      "Train Epoch: 342 [123136/225000 (55%)] Loss: 6892.554688\n",
      "Train Epoch: 342 [127232/225000 (57%)] Loss: 6990.794922\n",
      "Train Epoch: 342 [131328/225000 (58%)] Loss: 6784.781250\n",
      "Train Epoch: 342 [135424/225000 (60%)] Loss: 6867.263672\n",
      "Train Epoch: 342 [139520/225000 (62%)] Loss: 6970.437500\n",
      "Train Epoch: 342 [143616/225000 (64%)] Loss: 6886.037109\n",
      "Train Epoch: 342 [147712/225000 (66%)] Loss: 6993.435547\n",
      "Train Epoch: 342 [151808/225000 (67%)] Loss: 6888.207031\n",
      "Train Epoch: 342 [155904/225000 (69%)] Loss: 6779.392578\n",
      "Train Epoch: 342 [160000/225000 (71%)] Loss: 6956.193359\n",
      "Train Epoch: 342 [164096/225000 (73%)] Loss: 6908.058594\n",
      "Train Epoch: 342 [168192/225000 (75%)] Loss: 6938.597656\n",
      "Train Epoch: 342 [172288/225000 (77%)] Loss: 6684.484375\n",
      "Train Epoch: 342 [176384/225000 (78%)] Loss: 6824.474609\n",
      "Train Epoch: 342 [180480/225000 (80%)] Loss: 6782.583984\n",
      "Train Epoch: 342 [184576/225000 (82%)] Loss: 6844.814453\n",
      "Train Epoch: 342 [188672/225000 (84%)] Loss: 6879.503906\n",
      "Train Epoch: 342 [192768/225000 (86%)] Loss: 6842.750000\n",
      "Train Epoch: 342 [196864/225000 (87%)] Loss: 6817.025391\n",
      "Train Epoch: 342 [200960/225000 (89%)] Loss: 6871.857422\n",
      "Train Epoch: 342 [205056/225000 (91%)] Loss: 6994.642578\n",
      "Train Epoch: 342 [209152/225000 (93%)] Loss: 6959.029297\n",
      "Train Epoch: 342 [213248/225000 (95%)] Loss: 6838.595703\n",
      "Train Epoch: 342 [217344/225000 (97%)] Loss: 6900.470703\n",
      "Train Epoch: 342 [221440/225000 (98%)] Loss: 6824.707031\n",
      "    epoch          : 342\n",
      "    loss           : 6891.0019131292665\n",
      "    val_loss       : 6897.137816871916\n",
      "Train Epoch: 343 [256/225000 (0%)] Loss: 6929.505859\n",
      "Train Epoch: 343 [4352/225000 (2%)] Loss: 6879.662109\n",
      "Train Epoch: 343 [8448/225000 (4%)] Loss: 6900.746094\n",
      "Train Epoch: 343 [12544/225000 (6%)] Loss: 6833.087891\n",
      "Train Epoch: 343 [16640/225000 (7%)] Loss: 6956.548828\n",
      "Train Epoch: 343 [20736/225000 (9%)] Loss: 6960.736328\n",
      "Train Epoch: 343 [24832/225000 (11%)] Loss: 6870.806641\n",
      "Train Epoch: 343 [28928/225000 (13%)] Loss: 6981.105469\n",
      "Train Epoch: 343 [33024/225000 (15%)] Loss: 6832.837891\n",
      "Train Epoch: 343 [37120/225000 (16%)] Loss: 6956.808594\n",
      "Train Epoch: 343 [41216/225000 (18%)] Loss: 6979.105469\n",
      "Train Epoch: 343 [45312/225000 (20%)] Loss: 6751.753906\n",
      "Train Epoch: 343 [49408/225000 (22%)] Loss: 6887.779297\n",
      "Train Epoch: 343 [53504/225000 (24%)] Loss: 6925.449219\n",
      "Train Epoch: 343 [57600/225000 (26%)] Loss: 6826.279297\n",
      "Train Epoch: 343 [61696/225000 (27%)] Loss: 6940.935547\n",
      "Train Epoch: 343 [65792/225000 (29%)] Loss: 6884.541016\n",
      "Train Epoch: 343 [69888/225000 (31%)] Loss: 6875.134766\n",
      "Train Epoch: 343 [73984/225000 (33%)] Loss: 6844.417969\n",
      "Train Epoch: 343 [78080/225000 (35%)] Loss: 6987.740234\n",
      "Train Epoch: 343 [82176/225000 (37%)] Loss: 6911.767578\n",
      "Train Epoch: 343 [86272/225000 (38%)] Loss: 6879.923828\n",
      "Train Epoch: 343 [90368/225000 (40%)] Loss: 6912.863281\n",
      "Train Epoch: 343 [94464/225000 (42%)] Loss: 6985.017578\n",
      "Train Epoch: 343 [98560/225000 (44%)] Loss: 6861.814453\n",
      "Train Epoch: 343 [102656/225000 (46%)] Loss: 6719.152344\n",
      "Train Epoch: 343 [106752/225000 (47%)] Loss: 6720.060547\n",
      "Train Epoch: 343 [110848/225000 (49%)] Loss: 6759.011719\n",
      "Train Epoch: 343 [114944/225000 (51%)] Loss: 6771.125000\n",
      "Train Epoch: 343 [119040/225000 (53%)] Loss: 6787.564453\n",
      "Train Epoch: 343 [123136/225000 (55%)] Loss: 6788.570312\n",
      "Train Epoch: 343 [127232/225000 (57%)] Loss: 6828.361328\n",
      "Train Epoch: 343 [131328/225000 (58%)] Loss: 6874.322266\n",
      "Train Epoch: 343 [135424/225000 (60%)] Loss: 6927.658203\n",
      "Train Epoch: 343 [139520/225000 (62%)] Loss: 6903.896484\n",
      "Train Epoch: 343 [143616/225000 (64%)] Loss: 6839.046875\n",
      "Train Epoch: 343 [147712/225000 (66%)] Loss: 7077.007812\n",
      "Train Epoch: 343 [151808/225000 (67%)] Loss: 6961.097656\n",
      "Train Epoch: 343 [155904/225000 (69%)] Loss: 6790.613281\n",
      "Train Epoch: 343 [160000/225000 (71%)] Loss: 6882.519531\n",
      "Train Epoch: 343 [164096/225000 (73%)] Loss: 6960.357422\n",
      "Train Epoch: 343 [168192/225000 (75%)] Loss: 6841.640625\n",
      "Train Epoch: 343 [172288/225000 (77%)] Loss: 6651.886719\n",
      "Train Epoch: 343 [176384/225000 (78%)] Loss: 6803.402344\n",
      "Train Epoch: 343 [180480/225000 (80%)] Loss: 6790.380859\n",
      "Train Epoch: 343 [184576/225000 (82%)] Loss: 7006.910156\n",
      "Train Epoch: 343 [188672/225000 (84%)] Loss: 6768.460938\n",
      "Train Epoch: 343 [192768/225000 (86%)] Loss: 6960.232422\n",
      "Train Epoch: 343 [196864/225000 (87%)] Loss: 6739.878906\n",
      "Train Epoch: 343 [200960/225000 (89%)] Loss: 6752.708984\n",
      "Train Epoch: 343 [205056/225000 (91%)] Loss: 6957.048828\n",
      "Train Epoch: 343 [209152/225000 (93%)] Loss: 7016.941406\n",
      "Train Epoch: 343 [213248/225000 (95%)] Loss: 6828.738281\n",
      "Train Epoch: 343 [217344/225000 (97%)] Loss: 6918.488281\n",
      "Train Epoch: 343 [221440/225000 (98%)] Loss: 6935.011719\n",
      "    epoch          : 343\n",
      "    loss           : 6891.800851242534\n",
      "    val_loss       : 6894.524594282618\n",
      "Train Epoch: 344 [256/225000 (0%)] Loss: 6891.162109\n",
      "Train Epoch: 344 [4352/225000 (2%)] Loss: 6833.699219\n",
      "Train Epoch: 344 [8448/225000 (4%)] Loss: 6707.794922\n",
      "Train Epoch: 344 [12544/225000 (6%)] Loss: 6899.240234\n",
      "Train Epoch: 344 [16640/225000 (7%)] Loss: 6855.839844\n",
      "Train Epoch: 344 [20736/225000 (9%)] Loss: 7085.458984\n",
      "Train Epoch: 344 [24832/225000 (11%)] Loss: 6991.919922\n",
      "Train Epoch: 344 [28928/225000 (13%)] Loss: 6795.560547\n",
      "Train Epoch: 344 [33024/225000 (15%)] Loss: 6964.294922\n",
      "Train Epoch: 344 [37120/225000 (16%)] Loss: 6936.675781\n",
      "Train Epoch: 344 [41216/225000 (18%)] Loss: 6821.623047\n",
      "Train Epoch: 344 [45312/225000 (20%)] Loss: 6841.978516\n",
      "Train Epoch: 344 [49408/225000 (22%)] Loss: 6848.074219\n",
      "Train Epoch: 344 [53504/225000 (24%)] Loss: 6902.394531\n",
      "Train Epoch: 344 [57600/225000 (26%)] Loss: 6850.078125\n",
      "Train Epoch: 344 [61696/225000 (27%)] Loss: 6840.396484\n",
      "Train Epoch: 344 [65792/225000 (29%)] Loss: 6959.433594\n",
      "Train Epoch: 344 [69888/225000 (31%)] Loss: 6872.814453\n",
      "Train Epoch: 344 [73984/225000 (33%)] Loss: 6936.572266\n",
      "Train Epoch: 344 [78080/225000 (35%)] Loss: 6852.212891\n",
      "Train Epoch: 344 [82176/225000 (37%)] Loss: 6762.718750\n",
      "Train Epoch: 344 [86272/225000 (38%)] Loss: 6836.451172\n",
      "Train Epoch: 344 [90368/225000 (40%)] Loss: 6857.763672\n",
      "Train Epoch: 344 [94464/225000 (42%)] Loss: 6961.957031\n",
      "Train Epoch: 344 [98560/225000 (44%)] Loss: 6927.224609\n",
      "Train Epoch: 344 [102656/225000 (46%)] Loss: 6843.386719\n",
      "Train Epoch: 344 [106752/225000 (47%)] Loss: 6951.833984\n",
      "Train Epoch: 344 [110848/225000 (49%)] Loss: 6879.816406\n",
      "Train Epoch: 344 [114944/225000 (51%)] Loss: 6756.091797\n",
      "Train Epoch: 344 [119040/225000 (53%)] Loss: 6985.953125\n",
      "Train Epoch: 344 [123136/225000 (55%)] Loss: 6890.794922\n",
      "Train Epoch: 344 [127232/225000 (57%)] Loss: 6776.236328\n",
      "Train Epoch: 344 [131328/225000 (58%)] Loss: 6842.734375\n",
      "Train Epoch: 344 [135424/225000 (60%)] Loss: 6851.275391\n",
      "Train Epoch: 344 [139520/225000 (62%)] Loss: 6870.337891\n",
      "Train Epoch: 344 [143616/225000 (64%)] Loss: 6957.111328\n",
      "Train Epoch: 344 [147712/225000 (66%)] Loss: 7082.853516\n",
      "Train Epoch: 344 [151808/225000 (67%)] Loss: 6861.195312\n",
      "Train Epoch: 344 [155904/225000 (69%)] Loss: 6771.695312\n",
      "Train Epoch: 344 [160000/225000 (71%)] Loss: 6888.714844\n",
      "Train Epoch: 344 [164096/225000 (73%)] Loss: 6771.207031\n",
      "Train Epoch: 344 [168192/225000 (75%)] Loss: 6920.755859\n",
      "Train Epoch: 344 [172288/225000 (77%)] Loss: 6868.687500\n",
      "Train Epoch: 344 [176384/225000 (78%)] Loss: 7123.849609\n",
      "Train Epoch: 344 [180480/225000 (80%)] Loss: 6841.677734\n",
      "Train Epoch: 344 [184576/225000 (82%)] Loss: 6960.498047\n",
      "Train Epoch: 344 [188672/225000 (84%)] Loss: 6803.275391\n",
      "Train Epoch: 344 [192768/225000 (86%)] Loss: 6865.853516\n",
      "Train Epoch: 344 [196864/225000 (87%)] Loss: 6847.179688\n",
      "Train Epoch: 344 [200960/225000 (89%)] Loss: 6936.583984\n",
      "Train Epoch: 344 [205056/225000 (91%)] Loss: 7129.427734\n",
      "Train Epoch: 344 [209152/225000 (93%)] Loss: 6825.900391\n",
      "Train Epoch: 344 [213248/225000 (95%)] Loss: 6969.650391\n",
      "Train Epoch: 344 [217344/225000 (97%)] Loss: 6809.992188\n",
      "Train Epoch: 344 [221440/225000 (98%)] Loss: 6858.822266\n",
      "    epoch          : 344\n",
      "    loss           : 6879.079894811221\n",
      "    val_loss       : 7019.174339620435\n",
      "Train Epoch: 345 [256/225000 (0%)] Loss: 6724.228516\n",
      "Train Epoch: 345 [4352/225000 (2%)] Loss: 6823.783203\n",
      "Train Epoch: 345 [8448/225000 (4%)] Loss: 6948.201172\n",
      "Train Epoch: 345 [12544/225000 (6%)] Loss: 6885.351562\n",
      "Train Epoch: 345 [16640/225000 (7%)] Loss: 6910.679688\n",
      "Train Epoch: 345 [20736/225000 (9%)] Loss: 6827.773438\n",
      "Train Epoch: 345 [24832/225000 (11%)] Loss: 6866.517578\n",
      "Train Epoch: 345 [28928/225000 (13%)] Loss: 6872.943359\n",
      "Train Epoch: 345 [33024/225000 (15%)] Loss: 6940.853516\n",
      "Train Epoch: 345 [37120/225000 (16%)] Loss: 6804.775391\n",
      "Train Epoch: 345 [41216/225000 (18%)] Loss: 7057.507812\n",
      "Train Epoch: 345 [45312/225000 (20%)] Loss: 6962.507812\n",
      "Train Epoch: 345 [49408/225000 (22%)] Loss: 6888.638672\n",
      "Train Epoch: 345 [53504/225000 (24%)] Loss: 6883.693359\n",
      "Train Epoch: 345 [57600/225000 (26%)] Loss: 6798.820312\n",
      "Train Epoch: 345 [61696/225000 (27%)] Loss: 6850.234375\n",
      "Train Epoch: 345 [65792/225000 (29%)] Loss: 7015.523438\n",
      "Train Epoch: 345 [69888/225000 (31%)] Loss: 6914.908203\n",
      "Train Epoch: 345 [73984/225000 (33%)] Loss: 6844.906250\n",
      "Train Epoch: 345 [78080/225000 (35%)] Loss: 6809.595703\n",
      "Train Epoch: 345 [82176/225000 (37%)] Loss: 6882.115234\n",
      "Train Epoch: 345 [86272/225000 (38%)] Loss: 6933.285156\n",
      "Train Epoch: 345 [90368/225000 (40%)] Loss: 6878.519531\n",
      "Train Epoch: 345 [94464/225000 (42%)] Loss: 6829.169922\n",
      "Train Epoch: 345 [98560/225000 (44%)] Loss: 6879.285156\n",
      "Train Epoch: 345 [102656/225000 (46%)] Loss: 6936.740234\n",
      "Train Epoch: 345 [106752/225000 (47%)] Loss: 6857.417969\n",
      "Train Epoch: 345 [110848/225000 (49%)] Loss: 6828.671875\n",
      "Train Epoch: 345 [114944/225000 (51%)] Loss: 6763.146484\n",
      "Train Epoch: 345 [119040/225000 (53%)] Loss: 6887.066406\n",
      "Train Epoch: 345 [123136/225000 (55%)] Loss: 6852.277344\n",
      "Train Epoch: 345 [127232/225000 (57%)] Loss: 6930.441406\n",
      "Train Epoch: 345 [131328/225000 (58%)] Loss: 7004.871094\n",
      "Train Epoch: 345 [135424/225000 (60%)] Loss: 6848.708984\n",
      "Train Epoch: 345 [139520/225000 (62%)] Loss: 6796.845703\n",
      "Train Epoch: 345 [143616/225000 (64%)] Loss: 6820.880859\n",
      "Train Epoch: 345 [147712/225000 (66%)] Loss: 7063.251953\n",
      "Train Epoch: 345 [151808/225000 (67%)] Loss: 6750.251953\n",
      "Train Epoch: 345 [155904/225000 (69%)] Loss: 6933.662109\n",
      "Train Epoch: 345 [160000/225000 (71%)] Loss: 6996.082031\n",
      "Train Epoch: 345 [164096/225000 (73%)] Loss: 6850.605469\n",
      "Train Epoch: 345 [168192/225000 (75%)] Loss: 7000.230469\n",
      "Train Epoch: 345 [172288/225000 (77%)] Loss: 7080.988281\n",
      "Train Epoch: 345 [176384/225000 (78%)] Loss: 6996.628906\n",
      "Train Epoch: 345 [180480/225000 (80%)] Loss: 6991.826172\n",
      "Train Epoch: 345 [184576/225000 (82%)] Loss: 6840.013672\n",
      "Train Epoch: 345 [188672/225000 (84%)] Loss: 6911.416016\n",
      "Train Epoch: 345 [192768/225000 (86%)] Loss: 6834.056641\n",
      "Train Epoch: 345 [196864/225000 (87%)] Loss: 6941.742188\n",
      "Train Epoch: 345 [200960/225000 (89%)] Loss: 6821.054688\n",
      "Train Epoch: 345 [205056/225000 (91%)] Loss: 6789.240234\n",
      "Train Epoch: 345 [209152/225000 (93%)] Loss: 6793.326172\n",
      "Train Epoch: 345 [213248/225000 (95%)] Loss: 6980.478516\n",
      "Train Epoch: 345 [217344/225000 (97%)] Loss: 6880.294922\n",
      "Train Epoch: 345 [221440/225000 (98%)] Loss: 6787.138672\n",
      "    epoch          : 345\n",
      "    loss           : 6901.723277294866\n",
      "    val_loss       : 6890.039440210985\n",
      "Train Epoch: 346 [256/225000 (0%)] Loss: 6996.408203\n",
      "Train Epoch: 346 [4352/225000 (2%)] Loss: 6908.316406\n",
      "Train Epoch: 346 [8448/225000 (4%)] Loss: 6812.123047\n",
      "Train Epoch: 346 [12544/225000 (6%)] Loss: 6813.671875\n",
      "Train Epoch: 346 [16640/225000 (7%)] Loss: 6820.304688\n",
      "Train Epoch: 346 [20736/225000 (9%)] Loss: 6792.974609\n",
      "Train Epoch: 346 [24832/225000 (11%)] Loss: 6884.890625\n",
      "Train Epoch: 346 [28928/225000 (13%)] Loss: 6810.759766\n",
      "Train Epoch: 346 [33024/225000 (15%)] Loss: 6891.179688\n",
      "Train Epoch: 346 [37120/225000 (16%)] Loss: 6874.556641\n",
      "Train Epoch: 346 [41216/225000 (18%)] Loss: 6906.998047\n",
      "Train Epoch: 346 [45312/225000 (20%)] Loss: 6793.722656\n",
      "Train Epoch: 346 [49408/225000 (22%)] Loss: 6920.867188\n",
      "Train Epoch: 346 [53504/225000 (24%)] Loss: 6918.785156\n",
      "Train Epoch: 346 [57600/225000 (26%)] Loss: 6624.628906\n",
      "Train Epoch: 346 [61696/225000 (27%)] Loss: 6968.246094\n",
      "Train Epoch: 346 [65792/225000 (29%)] Loss: 6841.646484\n",
      "Train Epoch: 346 [69888/225000 (31%)] Loss: 6984.511719\n",
      "Train Epoch: 346 [73984/225000 (33%)] Loss: 6671.683594\n",
      "Train Epoch: 346 [78080/225000 (35%)] Loss: 6892.958984\n",
      "Train Epoch: 346 [82176/225000 (37%)] Loss: 6801.294922\n",
      "Train Epoch: 346 [86272/225000 (38%)] Loss: 6877.107422\n",
      "Train Epoch: 346 [90368/225000 (40%)] Loss: 6837.234375\n",
      "Train Epoch: 346 [94464/225000 (42%)] Loss: 6953.917969\n",
      "Train Epoch: 346 [98560/225000 (44%)] Loss: 6871.009766\n",
      "Train Epoch: 346 [102656/225000 (46%)] Loss: 6808.726562\n",
      "Train Epoch: 346 [106752/225000 (47%)] Loss: 6851.994141\n",
      "Train Epoch: 346 [110848/225000 (49%)] Loss: 7018.119141\n",
      "Train Epoch: 346 [114944/225000 (51%)] Loss: 6847.115234\n",
      "Train Epoch: 346 [119040/225000 (53%)] Loss: 6760.595703\n",
      "Train Epoch: 346 [123136/225000 (55%)] Loss: 6788.900391\n",
      "Train Epoch: 346 [127232/225000 (57%)] Loss: 6775.593750\n",
      "Train Epoch: 346 [131328/225000 (58%)] Loss: 6829.041016\n",
      "Train Epoch: 346 [135424/225000 (60%)] Loss: 6876.458984\n",
      "Train Epoch: 346 [139520/225000 (62%)] Loss: 6919.498047\n",
      "Train Epoch: 346 [143616/225000 (64%)] Loss: 7074.064453\n",
      "Train Epoch: 346 [147712/225000 (66%)] Loss: 6910.638672\n",
      "Train Epoch: 346 [151808/225000 (67%)] Loss: 7066.576172\n",
      "Train Epoch: 346 [155904/225000 (69%)] Loss: 7094.880859\n",
      "Train Epoch: 346 [160000/225000 (71%)] Loss: 6932.572266\n",
      "Train Epoch: 346 [164096/225000 (73%)] Loss: 6826.048828\n",
      "Train Epoch: 346 [168192/225000 (75%)] Loss: 6781.277344\n",
      "Train Epoch: 346 [172288/225000 (77%)] Loss: 6797.917969\n",
      "Train Epoch: 346 [176384/225000 (78%)] Loss: 6822.267578\n",
      "Train Epoch: 346 [180480/225000 (80%)] Loss: 6877.744141\n",
      "Train Epoch: 346 [184576/225000 (82%)] Loss: 6952.687500\n",
      "Train Epoch: 346 [188672/225000 (84%)] Loss: 6726.451172\n",
      "Train Epoch: 346 [192768/225000 (86%)] Loss: 6922.472656\n",
      "Train Epoch: 346 [196864/225000 (87%)] Loss: 6707.367188\n",
      "Train Epoch: 346 [200960/225000 (89%)] Loss: 6925.980469\n",
      "Train Epoch: 346 [205056/225000 (91%)] Loss: 6869.441406\n",
      "Train Epoch: 346 [209152/225000 (93%)] Loss: 6879.066406\n",
      "Train Epoch: 346 [213248/225000 (95%)] Loss: 6903.201172\n",
      "Train Epoch: 346 [217344/225000 (97%)] Loss: 6988.906250\n",
      "Train Epoch: 346 [221440/225000 (98%)] Loss: 6881.015625\n",
      "    epoch          : 346\n",
      "    loss           : 6876.6475364849975\n",
      "    val_loss       : 6985.36517446625\n",
      "Train Epoch: 347 [256/225000 (0%)] Loss: 6783.734375\n",
      "Train Epoch: 347 [4352/225000 (2%)] Loss: 6837.218750\n",
      "Train Epoch: 347 [8448/225000 (4%)] Loss: 6697.740234\n",
      "Train Epoch: 347 [12544/225000 (6%)] Loss: 6891.255859\n",
      "Train Epoch: 347 [16640/225000 (7%)] Loss: 6741.824219\n",
      "Train Epoch: 347 [20736/225000 (9%)] Loss: 6755.361328\n",
      "Train Epoch: 347 [24832/225000 (11%)] Loss: 7017.078125\n",
      "Train Epoch: 347 [28928/225000 (13%)] Loss: 6900.544922\n",
      "Train Epoch: 347 [33024/225000 (15%)] Loss: 6745.392578\n",
      "Train Epoch: 347 [37120/225000 (16%)] Loss: 7015.494141\n",
      "Train Epoch: 347 [41216/225000 (18%)] Loss: 6902.156250\n",
      "Train Epoch: 347 [45312/225000 (20%)] Loss: 6908.638672\n",
      "Train Epoch: 347 [49408/225000 (22%)] Loss: 7066.105469\n",
      "Train Epoch: 347 [53504/225000 (24%)] Loss: 6838.564453\n",
      "Train Epoch: 347 [57600/225000 (26%)] Loss: 7022.667969\n",
      "Train Epoch: 347 [61696/225000 (27%)] Loss: 6869.822266\n",
      "Train Epoch: 347 [65792/225000 (29%)] Loss: 6731.837891\n",
      "Train Epoch: 347 [69888/225000 (31%)] Loss: 6844.787109\n",
      "Train Epoch: 347 [73984/225000 (33%)] Loss: 6888.013672\n",
      "Train Epoch: 347 [78080/225000 (35%)] Loss: 6942.525391\n",
      "Train Epoch: 347 [82176/225000 (37%)] Loss: 6890.070312\n",
      "Train Epoch: 347 [86272/225000 (38%)] Loss: 6893.667969\n",
      "Train Epoch: 347 [90368/225000 (40%)] Loss: 6850.400391\n",
      "Train Epoch: 347 [94464/225000 (42%)] Loss: 6891.789062\n",
      "Train Epoch: 347 [98560/225000 (44%)] Loss: 6823.259766\n",
      "Train Epoch: 347 [102656/225000 (46%)] Loss: 6874.648438\n",
      "Train Epoch: 347 [106752/225000 (47%)] Loss: 6954.287109\n",
      "Train Epoch: 347 [110848/225000 (49%)] Loss: 6882.375000\n",
      "Train Epoch: 347 [114944/225000 (51%)] Loss: 6917.384766\n",
      "Train Epoch: 347 [119040/225000 (53%)] Loss: 6726.302734\n",
      "Train Epoch: 347 [123136/225000 (55%)] Loss: 6902.154297\n",
      "Train Epoch: 347 [127232/225000 (57%)] Loss: 6772.103516\n",
      "Train Epoch: 347 [131328/225000 (58%)] Loss: 6953.298828\n",
      "Train Epoch: 347 [135424/225000 (60%)] Loss: 6889.857422\n",
      "Train Epoch: 347 [139520/225000 (62%)] Loss: 6755.421875\n",
      "Train Epoch: 347 [143616/225000 (64%)] Loss: 6848.115234\n",
      "Train Epoch: 347 [147712/225000 (66%)] Loss: 6824.347656\n",
      "Train Epoch: 347 [151808/225000 (67%)] Loss: 7007.076172\n",
      "Train Epoch: 347 [155904/225000 (69%)] Loss: 6890.013672\n",
      "Train Epoch: 347 [160000/225000 (71%)] Loss: 6934.355469\n",
      "Train Epoch: 347 [164096/225000 (73%)] Loss: 6828.375000\n",
      "Train Epoch: 347 [168192/225000 (75%)] Loss: 6767.664062\n",
      "Train Epoch: 347 [172288/225000 (77%)] Loss: 7040.966797\n",
      "Train Epoch: 347 [176384/225000 (78%)] Loss: 6906.244141\n",
      "Train Epoch: 347 [180480/225000 (80%)] Loss: 6878.453125\n",
      "Train Epoch: 347 [184576/225000 (82%)] Loss: 6847.890625\n",
      "Train Epoch: 347 [188672/225000 (84%)] Loss: 7025.921875\n",
      "Train Epoch: 347 [192768/225000 (86%)] Loss: 6745.246094\n",
      "Train Epoch: 347 [196864/225000 (87%)] Loss: 6870.121094\n",
      "Train Epoch: 347 [200960/225000 (89%)] Loss: 6979.955078\n",
      "Train Epoch: 347 [205056/225000 (91%)] Loss: 6835.152344\n",
      "Train Epoch: 347 [209152/225000 (93%)] Loss: 7101.761719\n",
      "Train Epoch: 347 [213248/225000 (95%)] Loss: 6801.150391\n",
      "Train Epoch: 347 [217344/225000 (97%)] Loss: 6786.431641\n",
      "Train Epoch: 347 [221440/225000 (98%)] Loss: 6792.031250\n",
      "    epoch          : 347\n",
      "    loss           : 6887.594447703356\n",
      "    val_loss       : 6885.535425422143\n",
      "Train Epoch: 348 [256/225000 (0%)] Loss: 6928.701172\n",
      "Train Epoch: 348 [4352/225000 (2%)] Loss: 6863.417969\n",
      "Train Epoch: 348 [8448/225000 (4%)] Loss: 7003.789062\n",
      "Train Epoch: 348 [12544/225000 (6%)] Loss: 6793.130859\n",
      "Train Epoch: 348 [16640/225000 (7%)] Loss: 6853.101562\n",
      "Train Epoch: 348 [20736/225000 (9%)] Loss: 6728.240234\n",
      "Train Epoch: 348 [24832/225000 (11%)] Loss: 6932.714844\n",
      "Train Epoch: 348 [28928/225000 (13%)] Loss: 6949.500000\n",
      "Train Epoch: 348 [33024/225000 (15%)] Loss: 6770.142578\n",
      "Train Epoch: 348 [37120/225000 (16%)] Loss: 6888.250000\n",
      "Train Epoch: 348 [41216/225000 (18%)] Loss: 6803.035156\n",
      "Train Epoch: 348 [45312/225000 (20%)] Loss: 7038.416016\n",
      "Train Epoch: 348 [49408/225000 (22%)] Loss: 6955.582031\n",
      "Train Epoch: 348 [53504/225000 (24%)] Loss: 6947.431641\n",
      "Train Epoch: 348 [57600/225000 (26%)] Loss: 6893.525391\n",
      "Train Epoch: 348 [61696/225000 (27%)] Loss: 6929.609375\n",
      "Train Epoch: 348 [65792/225000 (29%)] Loss: 6770.697266\n",
      "Train Epoch: 348 [69888/225000 (31%)] Loss: 6889.927734\n",
      "Train Epoch: 348 [73984/225000 (33%)] Loss: 6962.531250\n",
      "Train Epoch: 348 [78080/225000 (35%)] Loss: 6790.990234\n",
      "Train Epoch: 348 [82176/225000 (37%)] Loss: 6871.382812\n",
      "Train Epoch: 348 [86272/225000 (38%)] Loss: 6845.142578\n",
      "Train Epoch: 348 [90368/225000 (40%)] Loss: 6830.611328\n",
      "Train Epoch: 348 [94464/225000 (42%)] Loss: 6889.851562\n",
      "Train Epoch: 348 [98560/225000 (44%)] Loss: 6736.982422\n",
      "Train Epoch: 348 [102656/225000 (46%)] Loss: 6735.480469\n",
      "Train Epoch: 348 [106752/225000 (47%)] Loss: 6822.132812\n",
      "Train Epoch: 348 [110848/225000 (49%)] Loss: 6782.951172\n",
      "Train Epoch: 348 [114944/225000 (51%)] Loss: 6961.603516\n",
      "Train Epoch: 348 [119040/225000 (53%)] Loss: 6746.640625\n",
      "Train Epoch: 348 [123136/225000 (55%)] Loss: 6959.937500\n",
      "Train Epoch: 348 [127232/225000 (57%)] Loss: 6804.642578\n",
      "Train Epoch: 348 [131328/225000 (58%)] Loss: 6856.564453\n",
      "Train Epoch: 348 [135424/225000 (60%)] Loss: 6911.767578\n",
      "Train Epoch: 348 [139520/225000 (62%)] Loss: 6956.291016\n",
      "Train Epoch: 348 [143616/225000 (64%)] Loss: 6795.191406\n",
      "Train Epoch: 348 [147712/225000 (66%)] Loss: 6762.457031\n",
      "Train Epoch: 348 [151808/225000 (67%)] Loss: 6961.775391\n",
      "Train Epoch: 348 [155904/225000 (69%)] Loss: 6936.132812\n",
      "Train Epoch: 348 [160000/225000 (71%)] Loss: 6720.330078\n",
      "Train Epoch: 348 [164096/225000 (73%)] Loss: 6973.396484\n",
      "Train Epoch: 348 [168192/225000 (75%)] Loss: 6960.078125\n",
      "Train Epoch: 348 [172288/225000 (77%)] Loss: 6987.312500\n",
      "Train Epoch: 348 [176384/225000 (78%)] Loss: 7121.400391\n",
      "Train Epoch: 348 [180480/225000 (80%)] Loss: 6807.986328\n",
      "Train Epoch: 348 [184576/225000 (82%)] Loss: 6927.123047\n",
      "Train Epoch: 348 [188672/225000 (84%)] Loss: 6952.992188\n",
      "Train Epoch: 348 [192768/225000 (86%)] Loss: 6861.742188\n",
      "Train Epoch: 348 [196864/225000 (87%)] Loss: 6880.296875\n",
      "Train Epoch: 348 [200960/225000 (89%)] Loss: 7123.679688\n",
      "Train Epoch: 348 [205056/225000 (91%)] Loss: 6861.296875\n",
      "Train Epoch: 348 [209152/225000 (93%)] Loss: 6902.880859\n",
      "Train Epoch: 348 [213248/225000 (95%)] Loss: 6937.619141\n",
      "Train Epoch: 348 [217344/225000 (97%)] Loss: 7035.611328\n",
      "Train Epoch: 348 [221440/225000 (98%)] Loss: 6881.492188\n",
      "    epoch          : 348\n",
      "    loss           : 6872.612635763297\n",
      "    val_loss       : 6883.66142903542\n",
      "Train Epoch: 349 [256/225000 (0%)] Loss: 7052.806641\n",
      "Train Epoch: 349 [4352/225000 (2%)] Loss: 6840.369141\n",
      "Train Epoch: 349 [8448/225000 (4%)] Loss: 6686.498047\n",
      "Train Epoch: 349 [12544/225000 (6%)] Loss: 7017.746094\n",
      "Train Epoch: 349 [16640/225000 (7%)] Loss: 6989.921875\n",
      "Train Epoch: 349 [20736/225000 (9%)] Loss: 6923.222656\n",
      "Train Epoch: 349 [24832/225000 (11%)] Loss: 6714.189453\n",
      "Train Epoch: 349 [28928/225000 (13%)] Loss: 6832.791016\n",
      "Train Epoch: 349 [33024/225000 (15%)] Loss: 6833.533203\n",
      "Train Epoch: 349 [37120/225000 (16%)] Loss: 6925.259766\n",
      "Train Epoch: 349 [41216/225000 (18%)] Loss: 6755.437500\n",
      "Train Epoch: 349 [45312/225000 (20%)] Loss: 6679.369141\n",
      "Train Epoch: 349 [49408/225000 (22%)] Loss: 6707.291016\n",
      "Train Epoch: 349 [53504/225000 (24%)] Loss: 6977.443359\n",
      "Train Epoch: 349 [57600/225000 (26%)] Loss: 6807.292969\n",
      "Train Epoch: 349 [61696/225000 (27%)] Loss: 6958.769531\n",
      "Train Epoch: 349 [65792/225000 (29%)] Loss: 6823.187500\n",
      "Train Epoch: 349 [69888/225000 (31%)] Loss: 6836.414062\n",
      "Train Epoch: 349 [73984/225000 (33%)] Loss: 6911.548828\n",
      "Train Epoch: 349 [78080/225000 (35%)] Loss: 6827.970703\n",
      "Train Epoch: 349 [82176/225000 (37%)] Loss: 7173.941406\n",
      "Train Epoch: 349 [86272/225000 (38%)] Loss: 7068.054688\n",
      "Train Epoch: 349 [90368/225000 (40%)] Loss: 6757.873047\n",
      "Train Epoch: 349 [94464/225000 (42%)] Loss: 6847.962891\n",
      "Train Epoch: 349 [98560/225000 (44%)] Loss: 6978.248047\n",
      "Train Epoch: 349 [102656/225000 (46%)] Loss: 6743.894531\n",
      "Train Epoch: 349 [106752/225000 (47%)] Loss: 6818.025391\n",
      "Train Epoch: 349 [110848/225000 (49%)] Loss: 6954.179688\n",
      "Train Epoch: 349 [114944/225000 (51%)] Loss: 6915.970703\n",
      "Train Epoch: 349 [119040/225000 (53%)] Loss: 6907.142578\n",
      "Train Epoch: 349 [123136/225000 (55%)] Loss: 6933.505859\n",
      "Train Epoch: 349 [127232/225000 (57%)] Loss: 6807.376953\n",
      "Train Epoch: 349 [131328/225000 (58%)] Loss: 6930.472656\n",
      "Train Epoch: 349 [135424/225000 (60%)] Loss: 6896.621094\n",
      "Train Epoch: 349 [139520/225000 (62%)] Loss: 6653.072266\n",
      "Train Epoch: 349 [143616/225000 (64%)] Loss: 6941.855469\n",
      "Train Epoch: 349 [147712/225000 (66%)] Loss: 6993.546875\n",
      "Train Epoch: 349 [151808/225000 (67%)] Loss: 6869.099609\n",
      "Train Epoch: 349 [155904/225000 (69%)] Loss: 7011.763672\n",
      "Train Epoch: 349 [160000/225000 (71%)] Loss: 6780.712891\n",
      "Train Epoch: 349 [164096/225000 (73%)] Loss: 6713.541016\n",
      "Train Epoch: 349 [168192/225000 (75%)] Loss: 6898.181641\n",
      "Train Epoch: 349 [172288/225000 (77%)] Loss: 7004.814453\n",
      "Train Epoch: 349 [176384/225000 (78%)] Loss: 6979.212891\n",
      "Train Epoch: 349 [180480/225000 (80%)] Loss: 6958.218750\n",
      "Train Epoch: 349 [184576/225000 (82%)] Loss: 6887.894531\n",
      "Train Epoch: 349 [188672/225000 (84%)] Loss: 6969.800781\n",
      "Train Epoch: 349 [192768/225000 (86%)] Loss: 7066.023438\n",
      "Train Epoch: 349 [196864/225000 (87%)] Loss: 6836.947266\n",
      "Train Epoch: 349 [200960/225000 (89%)] Loss: 6993.138672\n",
      "Train Epoch: 349 [205056/225000 (91%)] Loss: 6942.343750\n",
      "Train Epoch: 349 [209152/225000 (93%)] Loss: 6862.552734\n",
      "Train Epoch: 349 [213248/225000 (95%)] Loss: 6833.863281\n",
      "Train Epoch: 349 [217344/225000 (97%)] Loss: 6825.472656\n",
      "Train Epoch: 349 [221440/225000 (98%)] Loss: 6871.410156\n",
      "    epoch          : 349\n",
      "    loss           : 6871.859582755617\n",
      "    val_loss       : 6885.826052755726\n",
      "Train Epoch: 350 [256/225000 (0%)] Loss: 6902.267578\n",
      "Train Epoch: 350 [4352/225000 (2%)] Loss: 6752.593750\n",
      "Train Epoch: 350 [8448/225000 (4%)] Loss: 6880.181641\n",
      "Train Epoch: 350 [12544/225000 (6%)] Loss: 6887.998047\n",
      "Train Epoch: 350 [16640/225000 (7%)] Loss: 6877.052734\n",
      "Train Epoch: 350 [20736/225000 (9%)] Loss: 6807.273438\n",
      "Train Epoch: 350 [24832/225000 (11%)] Loss: 7008.667969\n",
      "Train Epoch: 350 [28928/225000 (13%)] Loss: 6795.099609\n",
      "Train Epoch: 350 [33024/225000 (15%)] Loss: 6759.837891\n",
      "Train Epoch: 350 [37120/225000 (16%)] Loss: 6815.787109\n",
      "Train Epoch: 350 [41216/225000 (18%)] Loss: 6945.056641\n",
      "Train Epoch: 350 [45312/225000 (20%)] Loss: 6880.091797\n",
      "Train Epoch: 350 [49408/225000 (22%)] Loss: 6824.242188\n",
      "Train Epoch: 350 [53504/225000 (24%)] Loss: 6688.437500\n",
      "Train Epoch: 350 [57600/225000 (26%)] Loss: 6721.177734\n",
      "Train Epoch: 350 [61696/225000 (27%)] Loss: 6964.843750\n",
      "Train Epoch: 350 [65792/225000 (29%)] Loss: 6757.027344\n",
      "Train Epoch: 350 [69888/225000 (31%)] Loss: 6835.880859\n",
      "Train Epoch: 350 [73984/225000 (33%)] Loss: 6920.646484\n",
      "Train Epoch: 350 [78080/225000 (35%)] Loss: 6833.064453\n",
      "Train Epoch: 350 [82176/225000 (37%)] Loss: 6834.091797\n",
      "Train Epoch: 350 [86272/225000 (38%)] Loss: 6835.335938\n",
      "Train Epoch: 350 [90368/225000 (40%)] Loss: 6964.289062\n",
      "Train Epoch: 350 [94464/225000 (42%)] Loss: 6861.275391\n",
      "Train Epoch: 350 [98560/225000 (44%)] Loss: 6942.613281\n",
      "Train Epoch: 350 [102656/225000 (46%)] Loss: 6988.794922\n",
      "Train Epoch: 350 [106752/225000 (47%)] Loss: 6881.042969\n",
      "Train Epoch: 350 [110848/225000 (49%)] Loss: 6898.031250\n",
      "Train Epoch: 350 [114944/225000 (51%)] Loss: 6750.345703\n",
      "Train Epoch: 350 [119040/225000 (53%)] Loss: 6917.085938\n",
      "Train Epoch: 350 [123136/225000 (55%)] Loss: 6893.611328\n",
      "Train Epoch: 350 [127232/225000 (57%)] Loss: 6738.207031\n",
      "Train Epoch: 350 [131328/225000 (58%)] Loss: 6827.289062\n",
      "Train Epoch: 350 [135424/225000 (60%)] Loss: 6845.271484\n",
      "Train Epoch: 350 [139520/225000 (62%)] Loss: 7017.238281\n",
      "Train Epoch: 350 [143616/225000 (64%)] Loss: 6876.935547\n",
      "Train Epoch: 350 [147712/225000 (66%)] Loss: 6830.027344\n",
      "Train Epoch: 350 [151808/225000 (67%)] Loss: 7014.279297\n",
      "Train Epoch: 350 [155904/225000 (69%)] Loss: 6809.763672\n",
      "Train Epoch: 350 [160000/225000 (71%)] Loss: 6699.484375\n",
      "Train Epoch: 350 [164096/225000 (73%)] Loss: 7077.591797\n",
      "Train Epoch: 350 [168192/225000 (75%)] Loss: 6941.488281\n",
      "Train Epoch: 350 [172288/225000 (77%)] Loss: 6970.775391\n",
      "Train Epoch: 350 [176384/225000 (78%)] Loss: 6839.470703\n",
      "Train Epoch: 350 [180480/225000 (80%)] Loss: 6843.291016\n",
      "Train Epoch: 350 [184576/225000 (82%)] Loss: 6864.859375\n",
      "Train Epoch: 350 [188672/225000 (84%)] Loss: 6872.244141\n",
      "Train Epoch: 350 [192768/225000 (86%)] Loss: 6979.750000\n",
      "Train Epoch: 350 [196864/225000 (87%)] Loss: 6890.041016\n",
      "Train Epoch: 350 [200960/225000 (89%)] Loss: 6879.460938\n",
      "Train Epoch: 350 [205056/225000 (91%)] Loss: 7017.910156\n",
      "Train Epoch: 350 [209152/225000 (93%)] Loss: 6893.021484\n",
      "Train Epoch: 350 [213248/225000 (95%)] Loss: 6759.531250\n",
      "Train Epoch: 350 [217344/225000 (97%)] Loss: 6827.779297\n",
      "Train Epoch: 350 [221440/225000 (98%)] Loss: 6841.601562\n",
      "    epoch          : 350\n",
      "    loss           : 6878.613505670506\n",
      "    val_loss       : 6889.569142482718\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0827_225017/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [256/225000 (0%)] Loss: 6779.705078\n",
      "Train Epoch: 351 [4352/225000 (2%)] Loss: 6864.462891\n",
      "Train Epoch: 351 [8448/225000 (4%)] Loss: 6736.757812\n",
      "Train Epoch: 351 [12544/225000 (6%)] Loss: 6943.957031\n",
      "Train Epoch: 351 [16640/225000 (7%)] Loss: 6782.857422\n",
      "Train Epoch: 351 [20736/225000 (9%)] Loss: 6996.806641\n",
      "Train Epoch: 351 [24832/225000 (11%)] Loss: 6874.660156\n",
      "Train Epoch: 351 [28928/225000 (13%)] Loss: 7023.751953\n",
      "Train Epoch: 351 [33024/225000 (15%)] Loss: 6718.960938\n",
      "Train Epoch: 351 [37120/225000 (16%)] Loss: 6765.724609\n",
      "Train Epoch: 351 [41216/225000 (18%)] Loss: 6720.697266\n",
      "Train Epoch: 351 [45312/225000 (20%)] Loss: 6788.333984\n",
      "Train Epoch: 351 [49408/225000 (22%)] Loss: 6862.974609\n",
      "Train Epoch: 351 [53504/225000 (24%)] Loss: 6738.634766\n",
      "Train Epoch: 351 [57600/225000 (26%)] Loss: 6960.417969\n",
      "Train Epoch: 351 [61696/225000 (27%)] Loss: 7024.355469\n",
      "Train Epoch: 351 [65792/225000 (29%)] Loss: 6931.474609\n",
      "Train Epoch: 351 [69888/225000 (31%)] Loss: 6848.873047\n",
      "Train Epoch: 351 [73984/225000 (33%)] Loss: 7036.515625\n",
      "Train Epoch: 351 [78080/225000 (35%)] Loss: 6923.398438\n",
      "Train Epoch: 351 [82176/225000 (37%)] Loss: 6800.148438\n",
      "Train Epoch: 351 [86272/225000 (38%)] Loss: 6878.001953\n",
      "Train Epoch: 351 [90368/225000 (40%)] Loss: 6974.156250\n",
      "Train Epoch: 351 [94464/225000 (42%)] Loss: 6814.042969\n",
      "Train Epoch: 351 [98560/225000 (44%)] Loss: 6780.912109\n",
      "Train Epoch: 351 [102656/225000 (46%)] Loss: 6810.041016\n",
      "Train Epoch: 351 [106752/225000 (47%)] Loss: 6945.445312\n",
      "Train Epoch: 351 [110848/225000 (49%)] Loss: 6818.867188\n",
      "Train Epoch: 351 [114944/225000 (51%)] Loss: 6914.517578\n",
      "Train Epoch: 351 [119040/225000 (53%)] Loss: 7023.931641\n",
      "Train Epoch: 351 [123136/225000 (55%)] Loss: 6782.117188\n",
      "Train Epoch: 351 [127232/225000 (57%)] Loss: 6920.904297\n",
      "Train Epoch: 351 [131328/225000 (58%)] Loss: 6984.324219\n",
      "Train Epoch: 351 [135424/225000 (60%)] Loss: 7018.843750\n",
      "Train Epoch: 351 [139520/225000 (62%)] Loss: 6910.998047\n",
      "Train Epoch: 351 [143616/225000 (64%)] Loss: 6886.058594\n",
      "Train Epoch: 351 [147712/225000 (66%)] Loss: 6881.457031\n",
      "Train Epoch: 351 [151808/225000 (67%)] Loss: 6867.162109\n",
      "Train Epoch: 351 [155904/225000 (69%)] Loss: 6977.669922\n",
      "Train Epoch: 351 [160000/225000 (71%)] Loss: 6928.621094\n",
      "Train Epoch: 351 [164096/225000 (73%)] Loss: 6934.814453\n",
      "Train Epoch: 351 [168192/225000 (75%)] Loss: 6782.939453\n",
      "Train Epoch: 351 [172288/225000 (77%)] Loss: 6846.919922\n",
      "Train Epoch: 351 [176384/225000 (78%)] Loss: 6911.070312\n",
      "Train Epoch: 351 [180480/225000 (80%)] Loss: 6929.314453\n",
      "Train Epoch: 351 [184576/225000 (82%)] Loss: 6849.535156\n",
      "Train Epoch: 351 [188672/225000 (84%)] Loss: 6946.634766\n",
      "Train Epoch: 351 [192768/225000 (86%)] Loss: 6664.177734\n",
      "Train Epoch: 351 [196864/225000 (87%)] Loss: 6784.533203\n",
      "Train Epoch: 351 [200960/225000 (89%)] Loss: 6837.205078\n",
      "Train Epoch: 351 [205056/225000 (91%)] Loss: 6881.841797\n",
      "Train Epoch: 351 [209152/225000 (93%)] Loss: 6807.039062\n",
      "Train Epoch: 351 [213248/225000 (95%)] Loss: 6993.646484\n",
      "Train Epoch: 351 [217344/225000 (97%)] Loss: 6916.488281\n",
      "Train Epoch: 351 [221440/225000 (98%)] Loss: 6879.142578\n",
      "    epoch          : 351\n",
      "    loss           : 6876.777428185438\n",
      "    val_loss       : 6883.4272198969\n",
      "Train Epoch: 352 [256/225000 (0%)] Loss: 7014.421875\n",
      "Train Epoch: 352 [4352/225000 (2%)] Loss: 6837.675781\n",
      "Train Epoch: 352 [8448/225000 (4%)] Loss: 6815.728516\n",
      "Train Epoch: 352 [12544/225000 (6%)] Loss: 6757.390625\n",
      "Train Epoch: 352 [16640/225000 (7%)] Loss: 6818.867188\n",
      "Train Epoch: 352 [20736/225000 (9%)] Loss: 7051.164062\n",
      "Train Epoch: 352 [24832/225000 (11%)] Loss: 6979.503906\n",
      "Train Epoch: 352 [28928/225000 (13%)] Loss: 6755.990234\n",
      "Train Epoch: 352 [33024/225000 (15%)] Loss: 6881.986328\n",
      "Train Epoch: 352 [37120/225000 (16%)] Loss: 6887.753906\n",
      "Train Epoch: 352 [41216/225000 (18%)] Loss: 6806.513672\n",
      "Train Epoch: 352 [45312/225000 (20%)] Loss: 6855.386719\n",
      "Train Epoch: 352 [49408/225000 (22%)] Loss: 6925.671875\n",
      "Train Epoch: 352 [53504/225000 (24%)] Loss: 6693.736328\n",
      "Train Epoch: 352 [57600/225000 (26%)] Loss: 6934.294922\n",
      "Train Epoch: 352 [61696/225000 (27%)] Loss: 6674.339844\n",
      "Train Epoch: 352 [65792/225000 (29%)] Loss: 6768.949219\n",
      "Train Epoch: 352 [69888/225000 (31%)] Loss: 7068.988281\n",
      "Train Epoch: 352 [73984/225000 (33%)] Loss: 6804.257812\n",
      "Train Epoch: 352 [78080/225000 (35%)] Loss: 6936.037109\n",
      "Train Epoch: 352 [82176/225000 (37%)] Loss: 6759.814453\n",
      "Train Epoch: 352 [86272/225000 (38%)] Loss: 6898.216797\n",
      "Train Epoch: 352 [90368/225000 (40%)] Loss: 6866.585938\n",
      "Train Epoch: 352 [94464/225000 (42%)] Loss: 6815.435547\n",
      "Train Epoch: 352 [98560/225000 (44%)] Loss: 6823.630859\n",
      "Train Epoch: 352 [102656/225000 (46%)] Loss: 6761.552734\n",
      "Train Epoch: 352 [106752/225000 (47%)] Loss: 6947.154297\n",
      "Train Epoch: 352 [110848/225000 (49%)] Loss: 7084.544922\n",
      "Train Epoch: 352 [114944/225000 (51%)] Loss: 6864.087891\n",
      "Train Epoch: 352 [119040/225000 (53%)] Loss: 6747.750000\n",
      "Train Epoch: 352 [123136/225000 (55%)] Loss: 6901.357422\n",
      "Train Epoch: 352 [127232/225000 (57%)] Loss: 6994.628906\n",
      "Train Epoch: 352 [131328/225000 (58%)] Loss: 6939.798828\n",
      "Train Epoch: 352 [135424/225000 (60%)] Loss: 6977.660156\n",
      "Train Epoch: 352 [139520/225000 (62%)] Loss: 6846.408203\n",
      "Train Epoch: 352 [143616/225000 (64%)] Loss: 6938.572266\n",
      "Train Epoch: 352 [147712/225000 (66%)] Loss: 6870.486328\n",
      "Train Epoch: 352 [151808/225000 (67%)] Loss: 6871.708984\n",
      "Train Epoch: 352 [155904/225000 (69%)] Loss: 7061.699219\n",
      "Train Epoch: 352 [160000/225000 (71%)] Loss: 6941.910156\n",
      "Train Epoch: 352 [164096/225000 (73%)] Loss: 6788.398438\n",
      "Train Epoch: 352 [168192/225000 (75%)] Loss: 6806.146484\n",
      "Train Epoch: 352 [172288/225000 (77%)] Loss: 6897.173828\n",
      "Train Epoch: 352 [176384/225000 (78%)] Loss: 6704.976562\n",
      "Train Epoch: 352 [180480/225000 (80%)] Loss: 6842.728516\n",
      "Train Epoch: 352 [184576/225000 (82%)] Loss: 6858.179688\n",
      "Train Epoch: 352 [188672/225000 (84%)] Loss: 6860.066406\n",
      "Train Epoch: 352 [192768/225000 (86%)] Loss: 6926.435547\n",
      "Train Epoch: 352 [196864/225000 (87%)] Loss: 6936.201172\n",
      "Train Epoch: 352 [200960/225000 (89%)] Loss: 6911.792969\n",
      "Train Epoch: 352 [205056/225000 (91%)] Loss: 6892.632812\n",
      "Train Epoch: 352 [209152/225000 (93%)] Loss: 6729.773438\n",
      "Train Epoch: 352 [213248/225000 (95%)] Loss: 6866.306641\n",
      "Train Epoch: 352 [217344/225000 (97%)] Loss: 6750.103516\n",
      "Train Epoch: 352 [221440/225000 (98%)] Loss: 7067.669922\n",
      "    epoch          : 352\n",
      "    loss           : 6889.049112539107\n",
      "    val_loss       : 6999.716741462144\n",
      "Train Epoch: 353 [256/225000 (0%)] Loss: 6877.701172\n",
      "Train Epoch: 353 [4352/225000 (2%)] Loss: 6951.125000\n",
      "Train Epoch: 353 [8448/225000 (4%)] Loss: 7051.513672\n",
      "Train Epoch: 353 [12544/225000 (6%)] Loss: 6869.121094\n",
      "Train Epoch: 353 [16640/225000 (7%)] Loss: 6976.177734\n",
      "Train Epoch: 353 [20736/225000 (9%)] Loss: 6804.226562\n",
      "Train Epoch: 353 [24832/225000 (11%)] Loss: 6852.552734\n",
      "Train Epoch: 353 [28928/225000 (13%)] Loss: 6873.562500\n",
      "Train Epoch: 353 [33024/225000 (15%)] Loss: 6911.101562\n",
      "Train Epoch: 353 [37120/225000 (16%)] Loss: 6795.964844\n",
      "Train Epoch: 353 [41216/225000 (18%)] Loss: 6920.392578\n",
      "Train Epoch: 353 [45312/225000 (20%)] Loss: 6860.304688\n",
      "Train Epoch: 353 [49408/225000 (22%)] Loss: 6958.693359\n",
      "Train Epoch: 353 [53504/225000 (24%)] Loss: 6878.919922\n",
      "Train Epoch: 353 [57600/225000 (26%)] Loss: 6813.281250\n",
      "Train Epoch: 353 [61696/225000 (27%)] Loss: 6954.957031\n",
      "Train Epoch: 353 [65792/225000 (29%)] Loss: 6919.289062\n",
      "Train Epoch: 353 [69888/225000 (31%)] Loss: 6932.566406\n",
      "Train Epoch: 353 [73984/225000 (33%)] Loss: 6929.023438\n",
      "Train Epoch: 353 [78080/225000 (35%)] Loss: 6855.201172\n",
      "Train Epoch: 353 [82176/225000 (37%)] Loss: 6802.535156\n",
      "Train Epoch: 353 [86272/225000 (38%)] Loss: 6924.005859\n",
      "Train Epoch: 353 [90368/225000 (40%)] Loss: 6783.005859\n",
      "Train Epoch: 353 [94464/225000 (42%)] Loss: 6920.287109\n",
      "Train Epoch: 353 [98560/225000 (44%)] Loss: 6907.751953\n",
      "Train Epoch: 353 [102656/225000 (46%)] Loss: 6858.357422\n",
      "Train Epoch: 353 [106752/225000 (47%)] Loss: 6921.800781\n",
      "Train Epoch: 353 [110848/225000 (49%)] Loss: 6917.289062\n",
      "Train Epoch: 353 [114944/225000 (51%)] Loss: 6757.984375\n",
      "Train Epoch: 353 [119040/225000 (53%)] Loss: 6747.369141\n",
      "Train Epoch: 353 [123136/225000 (55%)] Loss: 6843.857422\n",
      "Train Epoch: 353 [127232/225000 (57%)] Loss: 7069.964844\n",
      "Train Epoch: 353 [131328/225000 (58%)] Loss: 6765.349609\n",
      "Train Epoch: 353 [135424/225000 (60%)] Loss: 6866.271484\n",
      "Train Epoch: 353 [139520/225000 (62%)] Loss: 6779.902344\n",
      "Train Epoch: 353 [143616/225000 (64%)] Loss: 6813.009766\n",
      "Train Epoch: 353 [147712/225000 (66%)] Loss: 6828.945312\n",
      "Train Epoch: 353 [151808/225000 (67%)] Loss: 6824.769531\n",
      "Train Epoch: 353 [155904/225000 (69%)] Loss: 6916.845703\n",
      "Train Epoch: 353 [160000/225000 (71%)] Loss: 6874.521484\n",
      "Train Epoch: 353 [164096/225000 (73%)] Loss: 6862.082031\n",
      "Train Epoch: 353 [168192/225000 (75%)] Loss: 6956.570312\n",
      "Train Epoch: 353 [172288/225000 (77%)] Loss: 6935.748047\n",
      "Train Epoch: 353 [176384/225000 (78%)] Loss: 6900.404297\n",
      "Train Epoch: 353 [180480/225000 (80%)] Loss: 6934.623047\n",
      "Train Epoch: 353 [184576/225000 (82%)] Loss: 6739.011719\n",
      "Train Epoch: 353 [188672/225000 (84%)] Loss: 6775.255859\n",
      "Train Epoch: 353 [192768/225000 (86%)] Loss: 6926.027344\n",
      "Train Epoch: 353 [196864/225000 (87%)] Loss: 6983.812500\n",
      "Train Epoch: 353 [200960/225000 (89%)] Loss: 6936.970703\n",
      "Train Epoch: 353 [205056/225000 (91%)] Loss: 6795.458984\n",
      "Train Epoch: 353 [209152/225000 (93%)] Loss: 7059.111328\n",
      "Train Epoch: 353 [213248/225000 (95%)] Loss: 6812.781250\n",
      "Train Epoch: 353 [217344/225000 (97%)] Loss: 6678.041016\n",
      "Train Epoch: 353 [221440/225000 (98%)] Loss: 6849.750000\n",
      "    epoch          : 353\n",
      "    loss           : 6879.840223709471\n",
      "    val_loss       : 6880.328458384592\n",
      "Train Epoch: 354 [256/225000 (0%)] Loss: 6918.785156\n",
      "Train Epoch: 354 [4352/225000 (2%)] Loss: 6852.195312\n",
      "Train Epoch: 354 [8448/225000 (4%)] Loss: 6928.478516\n",
      "Train Epoch: 354 [12544/225000 (6%)] Loss: 6757.228516\n",
      "Train Epoch: 354 [16640/225000 (7%)] Loss: 6923.519531\n",
      "Train Epoch: 354 [20736/225000 (9%)] Loss: 6878.400391\n",
      "Train Epoch: 354 [24832/225000 (11%)] Loss: 6913.222656\n",
      "Train Epoch: 354 [28928/225000 (13%)] Loss: 7011.177734\n",
      "Train Epoch: 354 [33024/225000 (15%)] Loss: 6734.072266\n",
      "Train Epoch: 354 [37120/225000 (16%)] Loss: 6783.593750\n",
      "Train Epoch: 354 [41216/225000 (18%)] Loss: 7011.925781\n",
      "Train Epoch: 354 [45312/225000 (20%)] Loss: 6821.660156\n",
      "Train Epoch: 354 [49408/225000 (22%)] Loss: 6858.337891\n",
      "Train Epoch: 354 [53504/225000 (24%)] Loss: 6906.355469\n",
      "Train Epoch: 354 [57600/225000 (26%)] Loss: 6862.050781\n",
      "Train Epoch: 354 [61696/225000 (27%)] Loss: 6940.955078\n",
      "Train Epoch: 354 [65792/225000 (29%)] Loss: 6788.607422\n",
      "Train Epoch: 354 [69888/225000 (31%)] Loss: 6856.519531\n",
      "Train Epoch: 354 [73984/225000 (33%)] Loss: 6976.890625\n",
      "Train Epoch: 354 [78080/225000 (35%)] Loss: 6857.980469\n",
      "Train Epoch: 354 [82176/225000 (37%)] Loss: 6787.154297\n",
      "Train Epoch: 354 [86272/225000 (38%)] Loss: 6729.937500\n",
      "Train Epoch: 354 [90368/225000 (40%)] Loss: 6904.583984\n",
      "Train Epoch: 354 [94464/225000 (42%)] Loss: 6963.269531\n",
      "Train Epoch: 354 [98560/225000 (44%)] Loss: 6731.650391\n",
      "Train Epoch: 354 [102656/225000 (46%)] Loss: 6794.738281\n",
      "Train Epoch: 354 [106752/225000 (47%)] Loss: 6805.548828\n",
      "Train Epoch: 354 [110848/225000 (49%)] Loss: 6748.287109\n",
      "Train Epoch: 354 [114944/225000 (51%)] Loss: 6906.185547\n",
      "Train Epoch: 354 [119040/225000 (53%)] Loss: 6901.732422\n",
      "Train Epoch: 354 [123136/225000 (55%)] Loss: 6939.810547\n",
      "Train Epoch: 354 [127232/225000 (57%)] Loss: 6921.246094\n",
      "Train Epoch: 354 [131328/225000 (58%)] Loss: 6980.601562\n",
      "Train Epoch: 354 [135424/225000 (60%)] Loss: 6857.003906\n",
      "Train Epoch: 354 [139520/225000 (62%)] Loss: 6971.492188\n",
      "Train Epoch: 354 [143616/225000 (64%)] Loss: 7005.123047\n",
      "Train Epoch: 354 [147712/225000 (66%)] Loss: 6677.142578\n",
      "Train Epoch: 354 [151808/225000 (67%)] Loss: 6782.199219\n",
      "Train Epoch: 354 [155904/225000 (69%)] Loss: 6896.373047\n",
      "Train Epoch: 354 [160000/225000 (71%)] Loss: 6928.992188\n",
      "Train Epoch: 354 [164096/225000 (73%)] Loss: 6989.843750\n",
      "Train Epoch: 354 [168192/225000 (75%)] Loss: 7059.675781\n",
      "Train Epoch: 354 [172288/225000 (77%)] Loss: 6817.324219\n",
      "Train Epoch: 354 [176384/225000 (78%)] Loss: 6886.384766\n",
      "Train Epoch: 354 [180480/225000 (80%)] Loss: 6746.478516\n",
      "Train Epoch: 354 [184576/225000 (82%)] Loss: 6673.324219\n",
      "Train Epoch: 354 [188672/225000 (84%)] Loss: 6758.181641\n",
      "Train Epoch: 354 [192768/225000 (86%)] Loss: 6858.634766\n",
      "Train Epoch: 354 [196864/225000 (87%)] Loss: 7003.599609\n",
      "Train Epoch: 354 [200960/225000 (89%)] Loss: 7004.738281\n",
      "Train Epoch: 354 [205056/225000 (91%)] Loss: 6697.585938\n",
      "Train Epoch: 354 [209152/225000 (93%)] Loss: 6757.720703\n",
      "Train Epoch: 354 [213248/225000 (95%)] Loss: 6741.341797\n",
      "Train Epoch: 354 [217344/225000 (97%)] Loss: 6846.005859\n",
      "Train Epoch: 354 [221440/225000 (98%)] Loss: 6822.287109\n",
      "    epoch          : 354\n",
      "    loss           : 6885.583943268273\n",
      "    val_loss       : 6887.637617626969\n",
      "Train Epoch: 355 [256/225000 (0%)] Loss: 7074.447266\n",
      "Train Epoch: 355 [4352/225000 (2%)] Loss: 6976.628906\n",
      "Train Epoch: 355 [8448/225000 (4%)] Loss: 6991.457031\n",
      "Train Epoch: 355 [12544/225000 (6%)] Loss: 6944.218750\n",
      "Train Epoch: 355 [16640/225000 (7%)] Loss: 6691.953125\n",
      "Train Epoch: 355 [20736/225000 (9%)] Loss: 7007.382812\n",
      "Train Epoch: 355 [24832/225000 (11%)] Loss: 6825.019531\n",
      "Train Epoch: 355 [28928/225000 (13%)] Loss: 6994.585938\n",
      "Train Epoch: 355 [33024/225000 (15%)] Loss: 6865.916016\n",
      "Train Epoch: 355 [37120/225000 (16%)] Loss: 6776.373047\n",
      "Train Epoch: 355 [41216/225000 (18%)] Loss: 6822.505859\n",
      "Train Epoch: 355 [45312/225000 (20%)] Loss: 6990.935547\n",
      "Train Epoch: 355 [49408/225000 (22%)] Loss: 6811.990234\n",
      "Train Epoch: 355 [53504/225000 (24%)] Loss: 6892.779297\n",
      "Train Epoch: 355 [57600/225000 (26%)] Loss: 6944.906250\n",
      "Train Epoch: 355 [61696/225000 (27%)] Loss: 6820.095703\n",
      "Train Epoch: 355 [65792/225000 (29%)] Loss: 6793.949219\n",
      "Train Epoch: 355 [69888/225000 (31%)] Loss: 6969.009766\n",
      "Train Epoch: 355 [73984/225000 (33%)] Loss: 6845.339844\n",
      "Train Epoch: 355 [78080/225000 (35%)] Loss: 6803.671875\n",
      "Train Epoch: 355 [82176/225000 (37%)] Loss: 6817.824219\n",
      "Train Epoch: 355 [86272/225000 (38%)] Loss: 6869.542969\n",
      "Train Epoch: 355 [90368/225000 (40%)] Loss: 6916.093750\n",
      "Train Epoch: 355 [94464/225000 (42%)] Loss: 6710.539062\n",
      "Train Epoch: 355 [98560/225000 (44%)] Loss: 6992.222656\n",
      "Train Epoch: 355 [102656/225000 (46%)] Loss: 6932.011719\n",
      "Train Epoch: 355 [106752/225000 (47%)] Loss: 6726.720703\n",
      "Train Epoch: 355 [110848/225000 (49%)] Loss: 6911.400391\n",
      "Train Epoch: 355 [114944/225000 (51%)] Loss: 6836.423828\n",
      "Train Epoch: 355 [119040/225000 (53%)] Loss: 6980.511719\n",
      "Train Epoch: 355 [123136/225000 (55%)] Loss: 6917.089844\n",
      "Train Epoch: 355 [127232/225000 (57%)] Loss: 6972.947266\n",
      "Train Epoch: 355 [131328/225000 (58%)] Loss: 6889.845703\n",
      "Train Epoch: 355 [135424/225000 (60%)] Loss: 6881.708984\n",
      "Train Epoch: 355 [139520/225000 (62%)] Loss: 6829.830078\n",
      "Train Epoch: 355 [143616/225000 (64%)] Loss: 7034.714844\n",
      "Train Epoch: 355 [147712/225000 (66%)] Loss: 6800.691406\n",
      "Train Epoch: 355 [151808/225000 (67%)] Loss: 7035.486328\n",
      "Train Epoch: 355 [155904/225000 (69%)] Loss: 6801.128906\n",
      "Train Epoch: 355 [160000/225000 (71%)] Loss: 6798.123047\n",
      "Train Epoch: 355 [164096/225000 (73%)] Loss: 6787.679688\n",
      "Train Epoch: 355 [168192/225000 (75%)] Loss: 6792.728516\n",
      "Train Epoch: 355 [172288/225000 (77%)] Loss: 6826.574219\n",
      "Train Epoch: 355 [176384/225000 (78%)] Loss: 6779.248047\n",
      "Train Epoch: 355 [180480/225000 (80%)] Loss: 6939.833984\n",
      "Train Epoch: 355 [184576/225000 (82%)] Loss: 7031.617188\n",
      "Train Epoch: 355 [188672/225000 (84%)] Loss: 6767.250000\n",
      "Train Epoch: 355 [192768/225000 (86%)] Loss: 6767.345703\n",
      "Train Epoch: 355 [196864/225000 (87%)] Loss: 7012.554688\n",
      "Train Epoch: 355 [200960/225000 (89%)] Loss: 6747.751953\n",
      "Train Epoch: 355 [205056/225000 (91%)] Loss: 6833.402344\n",
      "Train Epoch: 355 [209152/225000 (93%)] Loss: 6978.757812\n",
      "Train Epoch: 355 [213248/225000 (95%)] Loss: 6788.982422\n",
      "Train Epoch: 355 [217344/225000 (97%)] Loss: 6693.509766\n",
      "Train Epoch: 355 [221440/225000 (98%)] Loss: 6830.144531\n",
      "    epoch          : 355\n",
      "    loss           : 6875.47341950192\n",
      "    val_loss       : 6947.758592671277\n",
      "Train Epoch: 356 [256/225000 (0%)] Loss: 6881.705078\n",
      "Train Epoch: 356 [4352/225000 (2%)] Loss: 6849.988281\n",
      "Train Epoch: 356 [8448/225000 (4%)] Loss: 6892.146484\n",
      "Train Epoch: 356 [12544/225000 (6%)] Loss: 6863.253906\n",
      "Train Epoch: 356 [16640/225000 (7%)] Loss: 6740.208984\n",
      "Train Epoch: 356 [20736/225000 (9%)] Loss: 6829.681641\n",
      "Train Epoch: 356 [24832/225000 (11%)] Loss: 6872.451172\n",
      "Train Epoch: 356 [28928/225000 (13%)] Loss: 6955.570312\n",
      "Train Epoch: 356 [33024/225000 (15%)] Loss: 6975.222656\n",
      "Train Epoch: 356 [37120/225000 (16%)] Loss: 6999.373047\n",
      "Train Epoch: 356 [41216/225000 (18%)] Loss: 6897.177734\n",
      "Train Epoch: 356 [45312/225000 (20%)] Loss: 6784.138672\n",
      "Train Epoch: 356 [49408/225000 (22%)] Loss: 6743.154297\n",
      "Train Epoch: 356 [53504/225000 (24%)] Loss: 6838.664062\n",
      "Train Epoch: 356 [57600/225000 (26%)] Loss: 6773.708984\n",
      "Train Epoch: 356 [61696/225000 (27%)] Loss: 6781.488281\n",
      "Train Epoch: 356 [65792/225000 (29%)] Loss: 6863.277344\n",
      "Train Epoch: 356 [69888/225000 (31%)] Loss: 6821.234375\n",
      "Train Epoch: 356 [73984/225000 (33%)] Loss: 6844.062500\n",
      "Train Epoch: 356 [78080/225000 (35%)] Loss: 6872.873047\n",
      "Train Epoch: 356 [82176/225000 (37%)] Loss: 6713.445312\n",
      "Train Epoch: 356 [86272/225000 (38%)] Loss: 6840.179688\n",
      "Train Epoch: 356 [90368/225000 (40%)] Loss: 6880.058594\n",
      "Train Epoch: 356 [94464/225000 (42%)] Loss: 6955.111328\n",
      "Train Epoch: 356 [98560/225000 (44%)] Loss: 6755.642578\n",
      "Train Epoch: 356 [102656/225000 (46%)] Loss: 6821.830078\n",
      "Train Epoch: 356 [106752/225000 (47%)] Loss: 6874.031250\n",
      "Train Epoch: 356 [110848/225000 (49%)] Loss: 6769.521484\n",
      "Train Epoch: 356 [114944/225000 (51%)] Loss: 6699.595703\n",
      "Train Epoch: 356 [119040/225000 (53%)] Loss: 6844.404297\n",
      "Train Epoch: 356 [123136/225000 (55%)] Loss: 7003.873047\n",
      "Train Epoch: 356 [127232/225000 (57%)] Loss: 6959.166016\n",
      "Train Epoch: 356 [131328/225000 (58%)] Loss: 6771.599609\n",
      "Train Epoch: 356 [135424/225000 (60%)] Loss: 6865.603516\n",
      "Train Epoch: 356 [139520/225000 (62%)] Loss: 6876.255859\n",
      "Train Epoch: 356 [143616/225000 (64%)] Loss: 6846.361328\n",
      "Train Epoch: 356 [147712/225000 (66%)] Loss: 6912.798828\n",
      "Train Epoch: 356 [151808/225000 (67%)] Loss: 6976.220703\n",
      "Train Epoch: 356 [155904/225000 (69%)] Loss: 6928.425781\n",
      "Train Epoch: 356 [160000/225000 (71%)] Loss: 6788.423828\n",
      "Train Epoch: 356 [164096/225000 (73%)] Loss: 6831.929688\n",
      "Train Epoch: 356 [168192/225000 (75%)] Loss: 6905.902344\n",
      "Train Epoch: 356 [172288/225000 (77%)] Loss: 6756.873047\n",
      "Train Epoch: 356 [176384/225000 (78%)] Loss: 6857.128906\n",
      "Train Epoch: 356 [180480/225000 (80%)] Loss: 6839.429688\n",
      "Train Epoch: 356 [184576/225000 (82%)] Loss: 6900.207031\n",
      "Train Epoch: 356 [188672/225000 (84%)] Loss: 6808.664062\n",
      "Train Epoch: 356 [192768/225000 (86%)] Loss: 7050.417969\n",
      "Train Epoch: 356 [196864/225000 (87%)] Loss: 6863.707031\n",
      "Train Epoch: 356 [200960/225000 (89%)] Loss: 6895.439453\n",
      "Train Epoch: 356 [205056/225000 (91%)] Loss: 6910.148438\n",
      "Train Epoch: 356 [209152/225000 (93%)] Loss: 6901.707031\n",
      "Train Epoch: 356 [213248/225000 (95%)] Loss: 6715.457031\n",
      "Train Epoch: 356 [217344/225000 (97%)] Loss: 6873.404297\n",
      "Train Epoch: 356 [221440/225000 (98%)] Loss: 6708.107422\n",
      "    epoch          : 356\n",
      "    loss           : 6880.886069930319\n",
      "    val_loss       : 6879.42222230775\n",
      "Train Epoch: 357 [256/225000 (0%)] Loss: 6916.867188\n",
      "Train Epoch: 357 [4352/225000 (2%)] Loss: 6877.916016\n",
      "Train Epoch: 357 [8448/225000 (4%)] Loss: 6907.613281\n",
      "Train Epoch: 357 [12544/225000 (6%)] Loss: 6940.179688\n",
      "Train Epoch: 357 [16640/225000 (7%)] Loss: 6789.703125\n",
      "Train Epoch: 357 [20736/225000 (9%)] Loss: 6916.492188\n",
      "Train Epoch: 357 [24832/225000 (11%)] Loss: 6857.234375\n",
      "Train Epoch: 357 [28928/225000 (13%)] Loss: 6943.423828\n",
      "Train Epoch: 357 [33024/225000 (15%)] Loss: 6742.820312\n",
      "Train Epoch: 357 [37120/225000 (16%)] Loss: 6975.386719\n",
      "Train Epoch: 357 [41216/225000 (18%)] Loss: 6773.162109\n",
      "Train Epoch: 357 [45312/225000 (20%)] Loss: 6811.636719\n",
      "Train Epoch: 357 [49408/225000 (22%)] Loss: 6775.318359\n",
      "Train Epoch: 357 [53504/225000 (24%)] Loss: 6825.476562\n",
      "Train Epoch: 357 [57600/225000 (26%)] Loss: 6832.564453\n",
      "Train Epoch: 357 [61696/225000 (27%)] Loss: 6962.712891\n",
      "Train Epoch: 357 [65792/225000 (29%)] Loss: 6859.601562\n",
      "Train Epoch: 357 [69888/225000 (31%)] Loss: 7027.660156\n",
      "Train Epoch: 357 [73984/225000 (33%)] Loss: 6884.097656\n",
      "Train Epoch: 357 [78080/225000 (35%)] Loss: 6979.423828\n",
      "Train Epoch: 357 [82176/225000 (37%)] Loss: 7098.564453\n",
      "Train Epoch: 357 [86272/225000 (38%)] Loss: 7018.035156\n",
      "Train Epoch: 357 [90368/225000 (40%)] Loss: 6728.429688\n",
      "Train Epoch: 357 [94464/225000 (42%)] Loss: 6884.865234\n",
      "Train Epoch: 357 [98560/225000 (44%)] Loss: 6926.597656\n",
      "Train Epoch: 357 [102656/225000 (46%)] Loss: 6925.656250\n",
      "Train Epoch: 357 [106752/225000 (47%)] Loss: 6828.181641\n",
      "Train Epoch: 357 [110848/225000 (49%)] Loss: 6807.369141\n",
      "Train Epoch: 357 [114944/225000 (51%)] Loss: 6974.550781\n",
      "Train Epoch: 357 [119040/225000 (53%)] Loss: 6798.392578\n",
      "Train Epoch: 357 [123136/225000 (55%)] Loss: 6700.806641\n",
      "Train Epoch: 357 [127232/225000 (57%)] Loss: 6829.841797\n",
      "Train Epoch: 357 [131328/225000 (58%)] Loss: 6811.671875\n",
      "Train Epoch: 357 [135424/225000 (60%)] Loss: 6784.195312\n",
      "Train Epoch: 357 [139520/225000 (62%)] Loss: 6800.650391\n",
      "Train Epoch: 357 [143616/225000 (64%)] Loss: 6783.873047\n",
      "Train Epoch: 357 [147712/225000 (66%)] Loss: 6890.212891\n",
      "Train Epoch: 357 [151808/225000 (67%)] Loss: 6925.816406\n",
      "Train Epoch: 357 [155904/225000 (69%)] Loss: 6719.986328\n",
      "Train Epoch: 357 [160000/225000 (71%)] Loss: 6887.167969\n",
      "Train Epoch: 357 [164096/225000 (73%)] Loss: 6926.396484\n",
      "Train Epoch: 357 [168192/225000 (75%)] Loss: 6841.625000\n",
      "Train Epoch: 357 [172288/225000 (77%)] Loss: 6907.486328\n",
      "Train Epoch: 357 [176384/225000 (78%)] Loss: 6889.019531\n",
      "Train Epoch: 357 [180480/225000 (80%)] Loss: 7114.744141\n",
      "Train Epoch: 357 [184576/225000 (82%)] Loss: 6917.521484\n",
      "Train Epoch: 357 [188672/225000 (84%)] Loss: 6807.085938\n",
      "Train Epoch: 357 [192768/225000 (86%)] Loss: 6835.572266\n",
      "Train Epoch: 357 [196864/225000 (87%)] Loss: 6948.896484\n",
      "Train Epoch: 357 [200960/225000 (89%)] Loss: 6866.238281\n",
      "Train Epoch: 357 [205056/225000 (91%)] Loss: 6947.074219\n",
      "Train Epoch: 357 [209152/225000 (93%)] Loss: 6914.283203\n",
      "Train Epoch: 357 [213248/225000 (95%)] Loss: 6885.142578\n",
      "Train Epoch: 357 [217344/225000 (97%)] Loss: 6871.666016\n",
      "Train Epoch: 357 [221440/225000 (98%)] Loss: 7066.021484\n",
      "    epoch          : 357\n",
      "    loss           : 6890.405427865472\n",
      "    val_loss       : 6878.988403563597\n",
      "Train Epoch: 358 [256/225000 (0%)] Loss: 6756.453125\n",
      "Train Epoch: 358 [4352/225000 (2%)] Loss: 6873.371094\n",
      "Train Epoch: 358 [8448/225000 (4%)] Loss: 6815.898438\n",
      "Train Epoch: 358 [12544/225000 (6%)] Loss: 6750.978516\n",
      "Train Epoch: 358 [16640/225000 (7%)] Loss: 6992.382812\n",
      "Train Epoch: 358 [20736/225000 (9%)] Loss: 6883.404297\n",
      "Train Epoch: 358 [24832/225000 (11%)] Loss: 6774.261719\n",
      "Train Epoch: 358 [28928/225000 (13%)] Loss: 6923.939453\n",
      "Train Epoch: 358 [33024/225000 (15%)] Loss: 6721.396484\n",
      "Train Epoch: 358 [37120/225000 (16%)] Loss: 6964.714844\n",
      "Train Epoch: 358 [41216/225000 (18%)] Loss: 6881.095703\n",
      "Train Epoch: 358 [45312/225000 (20%)] Loss: 6854.460938\n",
      "Train Epoch: 358 [49408/225000 (22%)] Loss: 6943.746094\n",
      "Train Epoch: 358 [53504/225000 (24%)] Loss: 6997.017578\n",
      "Train Epoch: 358 [57600/225000 (26%)] Loss: 6893.019531\n",
      "Train Epoch: 358 [61696/225000 (27%)] Loss: 6526.429688\n",
      "Train Epoch: 358 [65792/225000 (29%)] Loss: 6923.667969\n",
      "Train Epoch: 358 [69888/225000 (31%)] Loss: 6885.216797\n",
      "Train Epoch: 358 [73984/225000 (33%)] Loss: 6974.392578\n",
      "Train Epoch: 358 [78080/225000 (35%)] Loss: 6920.433594\n",
      "Train Epoch: 358 [82176/225000 (37%)] Loss: 6920.587891\n",
      "Train Epoch: 358 [86272/225000 (38%)] Loss: 6712.720703\n",
      "Train Epoch: 358 [90368/225000 (40%)] Loss: 6814.441406\n",
      "Train Epoch: 358 [94464/225000 (42%)] Loss: 7056.269531\n",
      "Train Epoch: 358 [98560/225000 (44%)] Loss: 6698.525391\n",
      "Train Epoch: 358 [102656/225000 (46%)] Loss: 6877.439453\n",
      "Train Epoch: 358 [106752/225000 (47%)] Loss: 7027.718750\n",
      "Train Epoch: 358 [110848/225000 (49%)] Loss: 6825.041016\n",
      "Train Epoch: 358 [114944/225000 (51%)] Loss: 6998.689453\n",
      "Train Epoch: 358 [119040/225000 (53%)] Loss: 6848.298828\n",
      "Train Epoch: 358 [123136/225000 (55%)] Loss: 6788.318359\n",
      "Train Epoch: 358 [127232/225000 (57%)] Loss: 6765.705078\n",
      "Train Epoch: 358 [131328/225000 (58%)] Loss: 6958.976562\n",
      "Train Epoch: 358 [135424/225000 (60%)] Loss: 6965.132812\n",
      "Train Epoch: 358 [139520/225000 (62%)] Loss: 6756.398438\n",
      "Train Epoch: 358 [143616/225000 (64%)] Loss: 6651.083984\n",
      "Train Epoch: 358 [147712/225000 (66%)] Loss: 6833.730469\n",
      "Train Epoch: 358 [151808/225000 (67%)] Loss: 6706.226562\n",
      "Train Epoch: 358 [155904/225000 (69%)] Loss: 6886.044922\n",
      "Train Epoch: 358 [160000/225000 (71%)] Loss: 6713.660156\n",
      "Train Epoch: 358 [164096/225000 (73%)] Loss: 6762.603516\n",
      "Train Epoch: 358 [168192/225000 (75%)] Loss: 6795.394531\n",
      "Train Epoch: 358 [172288/225000 (77%)] Loss: 6870.535156\n",
      "Train Epoch: 358 [176384/225000 (78%)] Loss: 7067.304688\n",
      "Train Epoch: 358 [180480/225000 (80%)] Loss: 6883.494141\n",
      "Train Epoch: 358 [184576/225000 (82%)] Loss: 6918.779297\n",
      "Train Epoch: 358 [188672/225000 (84%)] Loss: 6726.560547\n",
      "Train Epoch: 358 [192768/225000 (86%)] Loss: 6861.878906\n",
      "Train Epoch: 358 [196864/225000 (87%)] Loss: 6956.400391\n",
      "Train Epoch: 358 [200960/225000 (89%)] Loss: 6792.367188\n",
      "Train Epoch: 358 [205056/225000 (91%)] Loss: 6763.292969\n",
      "Train Epoch: 358 [209152/225000 (93%)] Loss: 6893.785156\n",
      "Train Epoch: 358 [213248/225000 (95%)] Loss: 6723.466797\n",
      "Train Epoch: 358 [217344/225000 (97%)] Loss: 6811.859375\n",
      "Train Epoch: 358 [221440/225000 (98%)] Loss: 6823.738281\n",
      "    epoch          : 358\n",
      "    loss           : 6912.459067699446\n",
      "    val_loss       : 6879.546609739868\n",
      "Train Epoch: 359 [256/225000 (0%)] Loss: 6719.507812\n",
      "Train Epoch: 359 [4352/225000 (2%)] Loss: 6885.517578\n",
      "Train Epoch: 359 [8448/225000 (4%)] Loss: 6869.355469\n",
      "Train Epoch: 359 [12544/225000 (6%)] Loss: 6869.148438\n",
      "Train Epoch: 359 [16640/225000 (7%)] Loss: 6791.443359\n",
      "Train Epoch: 359 [20736/225000 (9%)] Loss: 6928.720703\n",
      "Train Epoch: 359 [24832/225000 (11%)] Loss: 6744.828125\n",
      "Train Epoch: 359 [28928/225000 (13%)] Loss: 6849.136719\n",
      "Train Epoch: 359 [33024/225000 (15%)] Loss: 6872.005859\n",
      "Train Epoch: 359 [37120/225000 (16%)] Loss: 6833.289062\n",
      "Train Epoch: 359 [41216/225000 (18%)] Loss: 6851.121094\n",
      "Train Epoch: 359 [45312/225000 (20%)] Loss: 7029.718750\n",
      "Train Epoch: 359 [49408/225000 (22%)] Loss: 6744.375000\n",
      "Train Epoch: 359 [53504/225000 (24%)] Loss: 6922.333984\n",
      "Train Epoch: 359 [57600/225000 (26%)] Loss: 6914.501953\n",
      "Train Epoch: 359 [61696/225000 (27%)] Loss: 6917.677734\n",
      "Train Epoch: 359 [65792/225000 (29%)] Loss: 6838.117188\n",
      "Train Epoch: 359 [69888/225000 (31%)] Loss: 6932.373047\n",
      "Train Epoch: 359 [73984/225000 (33%)] Loss: 6927.246094\n",
      "Train Epoch: 359 [78080/225000 (35%)] Loss: 6890.306641\n",
      "Train Epoch: 359 [82176/225000 (37%)] Loss: 6972.554688\n",
      "Train Epoch: 359 [86272/225000 (38%)] Loss: 6762.765625\n",
      "Train Epoch: 359 [90368/225000 (40%)] Loss: 6941.890625\n",
      "Train Epoch: 359 [94464/225000 (42%)] Loss: 6846.636719\n",
      "Train Epoch: 359 [98560/225000 (44%)] Loss: 6869.386719\n",
      "Train Epoch: 359 [102656/225000 (46%)] Loss: 7022.976562\n",
      "Train Epoch: 359 [106752/225000 (47%)] Loss: 6898.166016\n",
      "Train Epoch: 359 [110848/225000 (49%)] Loss: 6733.640625\n",
      "Train Epoch: 359 [114944/225000 (51%)] Loss: 6756.876953\n",
      "Train Epoch: 359 [119040/225000 (53%)] Loss: 6751.630859\n",
      "Train Epoch: 359 [123136/225000 (55%)] Loss: 7092.484375\n",
      "Train Epoch: 359 [127232/225000 (57%)] Loss: 6845.798828\n",
      "Train Epoch: 359 [131328/225000 (58%)] Loss: 6907.365234\n",
      "Train Epoch: 359 [135424/225000 (60%)] Loss: 6775.013672\n",
      "Train Epoch: 359 [139520/225000 (62%)] Loss: 6953.326172\n",
      "Train Epoch: 359 [143616/225000 (64%)] Loss: 6748.160156\n",
      "Train Epoch: 359 [147712/225000 (66%)] Loss: 6807.822266\n",
      "Train Epoch: 359 [151808/225000 (67%)] Loss: 7022.966797\n",
      "Train Epoch: 359 [155904/225000 (69%)] Loss: 6891.373047\n",
      "Train Epoch: 359 [160000/225000 (71%)] Loss: 6916.871094\n",
      "Train Epoch: 359 [164096/225000 (73%)] Loss: 6878.802734\n",
      "Train Epoch: 359 [168192/225000 (75%)] Loss: 6771.207031\n",
      "Train Epoch: 359 [172288/225000 (77%)] Loss: 6790.060547\n",
      "Train Epoch: 359 [176384/225000 (78%)] Loss: 6740.427734\n",
      "Train Epoch: 359 [180480/225000 (80%)] Loss: 6875.146484\n",
      "Train Epoch: 359 [184576/225000 (82%)] Loss: 6839.744141\n",
      "Train Epoch: 359 [188672/225000 (84%)] Loss: 6889.757812\n",
      "Train Epoch: 359 [192768/225000 (86%)] Loss: 6904.267578\n",
      "Train Epoch: 359 [196864/225000 (87%)] Loss: 6887.335938\n",
      "Train Epoch: 359 [200960/225000 (89%)] Loss: 6871.189453\n",
      "Train Epoch: 359 [205056/225000 (91%)] Loss: 6990.275391\n",
      "Train Epoch: 359 [209152/225000 (93%)] Loss: 6781.166016\n",
      "Train Epoch: 359 [213248/225000 (95%)] Loss: 6871.500000\n",
      "Train Epoch: 359 [217344/225000 (97%)] Loss: 6817.343750\n",
      "Train Epoch: 359 [221440/225000 (98%)] Loss: 6788.609375\n",
      "    epoch          : 359\n",
      "    loss           : 6865.357394100185\n",
      "    val_loss       : 6876.9143712617915\n",
      "Train Epoch: 360 [256/225000 (0%)] Loss: 6705.353516\n",
      "Train Epoch: 360 [4352/225000 (2%)] Loss: 6849.878906\n",
      "Train Epoch: 360 [8448/225000 (4%)] Loss: 6865.128906\n",
      "Train Epoch: 360 [12544/225000 (6%)] Loss: 6952.882812\n",
      "Train Epoch: 360 [16640/225000 (7%)] Loss: 6929.544922\n",
      "Train Epoch: 360 [20736/225000 (9%)] Loss: 6733.373047\n",
      "Train Epoch: 360 [24832/225000 (11%)] Loss: 6869.480469\n",
      "Train Epoch: 360 [28928/225000 (13%)] Loss: 6780.564453\n",
      "Train Epoch: 360 [33024/225000 (15%)] Loss: 6904.603516\n",
      "Train Epoch: 360 [37120/225000 (16%)] Loss: 6819.558594\n",
      "Train Epoch: 360 [41216/225000 (18%)] Loss: 6815.892578\n",
      "Train Epoch: 360 [45312/225000 (20%)] Loss: 6828.574219\n",
      "Train Epoch: 360 [49408/225000 (22%)] Loss: 7132.902344\n",
      "Train Epoch: 360 [53504/225000 (24%)] Loss: 6903.818359\n",
      "Train Epoch: 360 [57600/225000 (26%)] Loss: 6813.919922\n",
      "Train Epoch: 360 [61696/225000 (27%)] Loss: 6860.861328\n",
      "Train Epoch: 360 [65792/225000 (29%)] Loss: 6932.935547\n",
      "Train Epoch: 360 [69888/225000 (31%)] Loss: 6916.908203\n",
      "Train Epoch: 360 [73984/225000 (33%)] Loss: 6941.732422\n",
      "Train Epoch: 360 [78080/225000 (35%)] Loss: 6962.144531\n",
      "Train Epoch: 360 [82176/225000 (37%)] Loss: 6996.017578\n",
      "Train Epoch: 360 [86272/225000 (38%)] Loss: 6846.429688\n",
      "Train Epoch: 360 [90368/225000 (40%)] Loss: 6836.220703\n",
      "Train Epoch: 360 [94464/225000 (42%)] Loss: 6695.312500\n",
      "Train Epoch: 360 [98560/225000 (44%)] Loss: 6956.515625\n",
      "Train Epoch: 360 [102656/225000 (46%)] Loss: 6846.773438\n",
      "Train Epoch: 360 [106752/225000 (47%)] Loss: 6865.365234\n",
      "Train Epoch: 360 [110848/225000 (49%)] Loss: 6821.878906\n",
      "Train Epoch: 360 [114944/225000 (51%)] Loss: 6807.931641\n",
      "Train Epoch: 360 [119040/225000 (53%)] Loss: 6952.767578\n",
      "Train Epoch: 360 [123136/225000 (55%)] Loss: 6866.458984\n",
      "Train Epoch: 360 [127232/225000 (57%)] Loss: 6932.332031\n",
      "Train Epoch: 360 [131328/225000 (58%)] Loss: 6877.757812\n",
      "Train Epoch: 360 [135424/225000 (60%)] Loss: 6944.714844\n",
      "Train Epoch: 360 [139520/225000 (62%)] Loss: 6938.681641\n",
      "Train Epoch: 360 [143616/225000 (64%)] Loss: 6702.292969\n",
      "Train Epoch: 360 [147712/225000 (66%)] Loss: 6965.291016\n",
      "Train Epoch: 360 [151808/225000 (67%)] Loss: 6860.441406\n",
      "Train Epoch: 360 [155904/225000 (69%)] Loss: 6878.693359\n",
      "Train Epoch: 360 [160000/225000 (71%)] Loss: 6908.248047\n",
      "Train Epoch: 360 [164096/225000 (73%)] Loss: 6747.144531\n",
      "Train Epoch: 360 [168192/225000 (75%)] Loss: 6934.132812\n",
      "Train Epoch: 360 [172288/225000 (77%)] Loss: 6880.312500\n",
      "Train Epoch: 360 [176384/225000 (78%)] Loss: 6872.671875\n",
      "Train Epoch: 360 [180480/225000 (80%)] Loss: 6765.802734\n",
      "Train Epoch: 360 [184576/225000 (82%)] Loss: 6691.398438\n",
      "Train Epoch: 360 [188672/225000 (84%)] Loss: 6773.521484\n",
      "Train Epoch: 360 [192768/225000 (86%)] Loss: 6883.455078\n",
      "Train Epoch: 360 [196864/225000 (87%)] Loss: 6906.382812\n",
      "Train Epoch: 360 [200960/225000 (89%)] Loss: 6842.234375\n",
      "Train Epoch: 360 [205056/225000 (91%)] Loss: 6750.578125\n",
      "Train Epoch: 360 [209152/225000 (93%)] Loss: 6697.162109\n",
      "Train Epoch: 360 [213248/225000 (95%)] Loss: 6715.675781\n",
      "Train Epoch: 360 [217344/225000 (97%)] Loss: 6677.591797\n",
      "Train Epoch: 360 [221440/225000 (98%)] Loss: 6676.949219\n",
      "    epoch          : 360\n",
      "    loss           : 6886.484053923137\n",
      "    val_loss       : 6881.492622609041\n",
      "Train Epoch: 361 [256/225000 (0%)] Loss: 6724.066406\n",
      "Train Epoch: 361 [4352/225000 (2%)] Loss: 6841.025391\n",
      "Train Epoch: 361 [8448/225000 (4%)] Loss: 6948.035156\n",
      "Train Epoch: 361 [12544/225000 (6%)] Loss: 6830.396484\n",
      "Train Epoch: 361 [16640/225000 (7%)] Loss: 6848.039062\n",
      "Train Epoch: 361 [20736/225000 (9%)] Loss: 6735.003906\n",
      "Train Epoch: 361 [24832/225000 (11%)] Loss: 6792.376953\n",
      "Train Epoch: 361 [28928/225000 (13%)] Loss: 6857.421875\n",
      "Train Epoch: 361 [33024/225000 (15%)] Loss: 6868.187500\n",
      "Train Epoch: 361 [37120/225000 (16%)] Loss: 6759.207031\n",
      "Train Epoch: 361 [41216/225000 (18%)] Loss: 6799.070312\n",
      "Train Epoch: 361 [45312/225000 (20%)] Loss: 6672.468750\n",
      "Train Epoch: 361 [49408/225000 (22%)] Loss: 6796.841797\n",
      "Train Epoch: 361 [53504/225000 (24%)] Loss: 6955.000000\n",
      "Train Epoch: 361 [57600/225000 (26%)] Loss: 6927.724609\n",
      "Train Epoch: 361 [61696/225000 (27%)] Loss: 6899.250000\n",
      "Train Epoch: 361 [65792/225000 (29%)] Loss: 6843.179688\n",
      "Train Epoch: 361 [69888/225000 (31%)] Loss: 6797.216797\n",
      "Train Epoch: 361 [73984/225000 (33%)] Loss: 6778.660156\n",
      "Train Epoch: 361 [78080/225000 (35%)] Loss: 6662.248047\n",
      "Train Epoch: 361 [82176/225000 (37%)] Loss: 7093.802734\n",
      "Train Epoch: 361 [86272/225000 (38%)] Loss: 6875.968750\n",
      "Train Epoch: 361 [90368/225000 (40%)] Loss: 6760.748047\n",
      "Train Epoch: 361 [94464/225000 (42%)] Loss: 6914.017578\n",
      "Train Epoch: 361 [98560/225000 (44%)] Loss: 6722.427734\n",
      "Train Epoch: 361 [102656/225000 (46%)] Loss: 6880.271484\n",
      "Train Epoch: 361 [106752/225000 (47%)] Loss: 6857.509766\n",
      "Train Epoch: 361 [110848/225000 (49%)] Loss: 6793.162109\n",
      "Train Epoch: 361 [114944/225000 (51%)] Loss: 6868.136719\n",
      "Train Epoch: 361 [119040/225000 (53%)] Loss: 7006.537109\n",
      "Train Epoch: 361 [123136/225000 (55%)] Loss: 6816.099609\n",
      "Train Epoch: 361 [127232/225000 (57%)] Loss: 6814.925781\n",
      "Train Epoch: 361 [131328/225000 (58%)] Loss: 6755.939453\n",
      "Train Epoch: 361 [135424/225000 (60%)] Loss: 6910.955078\n",
      "Train Epoch: 361 [139520/225000 (62%)] Loss: 6840.884766\n",
      "Train Epoch: 361 [143616/225000 (64%)] Loss: 6719.050781\n",
      "Train Epoch: 361 [147712/225000 (66%)] Loss: 6965.300781\n",
      "Train Epoch: 361 [151808/225000 (67%)] Loss: 6786.371094\n",
      "Train Epoch: 361 [155904/225000 (69%)] Loss: 7100.767578\n",
      "Train Epoch: 361 [160000/225000 (71%)] Loss: 6987.382812\n",
      "Train Epoch: 361 [164096/225000 (73%)] Loss: 7058.970703\n",
      "Train Epoch: 361 [168192/225000 (75%)] Loss: 6630.173828\n",
      "Train Epoch: 361 [172288/225000 (77%)] Loss: 6975.537109\n",
      "Train Epoch: 361 [176384/225000 (78%)] Loss: 6871.664062\n",
      "Train Epoch: 361 [180480/225000 (80%)] Loss: 6703.673828\n",
      "Train Epoch: 361 [184576/225000 (82%)] Loss: 6875.048828\n",
      "Train Epoch: 361 [188672/225000 (84%)] Loss: 6907.130859\n",
      "Train Epoch: 361 [192768/225000 (86%)] Loss: 6943.753906\n",
      "Train Epoch: 361 [196864/225000 (87%)] Loss: 6803.878906\n",
      "Train Epoch: 361 [200960/225000 (89%)] Loss: 6702.316406\n",
      "Train Epoch: 361 [205056/225000 (91%)] Loss: 6804.619141\n",
      "Train Epoch: 361 [209152/225000 (93%)] Loss: 6765.013672\n",
      "Train Epoch: 361 [213248/225000 (95%)] Loss: 6920.755859\n",
      "Train Epoch: 361 [217344/225000 (97%)] Loss: 6905.380859\n",
      "Train Epoch: 361 [221440/225000 (98%)] Loss: 6777.849609\n",
      "    epoch          : 361\n",
      "    loss           : 6862.716170275171\n",
      "    val_loss       : 6869.805231155181\n",
      "Train Epoch: 362 [256/225000 (0%)] Loss: 6762.255859\n",
      "Train Epoch: 362 [4352/225000 (2%)] Loss: 6741.816406\n",
      "Train Epoch: 362 [8448/225000 (4%)] Loss: 6877.703125\n",
      "Train Epoch: 362 [12544/225000 (6%)] Loss: 6803.402344\n",
      "Train Epoch: 362 [16640/225000 (7%)] Loss: 6878.814453\n",
      "Train Epoch: 362 [20736/225000 (9%)] Loss: 7066.664062\n",
      "Train Epoch: 362 [24832/225000 (11%)] Loss: 6939.363281\n",
      "Train Epoch: 362 [28928/225000 (13%)] Loss: 6861.900391\n",
      "Train Epoch: 362 [33024/225000 (15%)] Loss: 6931.683594\n",
      "Train Epoch: 362 [37120/225000 (16%)] Loss: 6940.693359\n",
      "Train Epoch: 362 [41216/225000 (18%)] Loss: 6782.947266\n",
      "Train Epoch: 362 [45312/225000 (20%)] Loss: 6806.746094\n",
      "Train Epoch: 362 [49408/225000 (22%)] Loss: 6741.966797\n",
      "Train Epoch: 362 [53504/225000 (24%)] Loss: 7119.080078\n",
      "Train Epoch: 362 [57600/225000 (26%)] Loss: 6937.175781\n",
      "Train Epoch: 362 [61696/225000 (27%)] Loss: 6929.593750\n",
      "Train Epoch: 362 [65792/225000 (29%)] Loss: 6893.816406\n",
      "Train Epoch: 362 [69888/225000 (31%)] Loss: 7044.570312\n",
      "Train Epoch: 362 [73984/225000 (33%)] Loss: 6776.984375\n",
      "Train Epoch: 362 [78080/225000 (35%)] Loss: 6794.980469\n",
      "Train Epoch: 362 [82176/225000 (37%)] Loss: 7023.771484\n",
      "Train Epoch: 362 [86272/225000 (38%)] Loss: 6836.419922\n",
      "Train Epoch: 362 [90368/225000 (40%)] Loss: 6807.736328\n",
      "Train Epoch: 362 [94464/225000 (42%)] Loss: 6958.185547\n",
      "Train Epoch: 362 [98560/225000 (44%)] Loss: 6891.746094\n",
      "Train Epoch: 362 [102656/225000 (46%)] Loss: 6843.187500\n",
      "Train Epoch: 362 [106752/225000 (47%)] Loss: 6860.445312\n",
      "Train Epoch: 362 [110848/225000 (49%)] Loss: 6736.894531\n",
      "Train Epoch: 362 [114944/225000 (51%)] Loss: 6726.625000\n",
      "Train Epoch: 362 [119040/225000 (53%)] Loss: 6907.736328\n",
      "Train Epoch: 362 [123136/225000 (55%)] Loss: 6820.310547\n",
      "Train Epoch: 362 [127232/225000 (57%)] Loss: 6716.685547\n",
      "Train Epoch: 362 [131328/225000 (58%)] Loss: 6853.628906\n",
      "Train Epoch: 362 [135424/225000 (60%)] Loss: 6931.580078\n",
      "Train Epoch: 362 [139520/225000 (62%)] Loss: 6691.638672\n",
      "Train Epoch: 362 [143616/225000 (64%)] Loss: 6820.365234\n",
      "Train Epoch: 362 [147712/225000 (66%)] Loss: 6809.916016\n",
      "Train Epoch: 362 [151808/225000 (67%)] Loss: 6803.603516\n",
      "Train Epoch: 362 [155904/225000 (69%)] Loss: 7103.142578\n",
      "Train Epoch: 362 [160000/225000 (71%)] Loss: 6749.636719\n",
      "Train Epoch: 362 [164096/225000 (73%)] Loss: 6830.810547\n",
      "Train Epoch: 362 [168192/225000 (75%)] Loss: 6870.250000\n",
      "Train Epoch: 362 [172288/225000 (77%)] Loss: 6678.033203\n",
      "Train Epoch: 362 [176384/225000 (78%)] Loss: 6857.919922\n",
      "Train Epoch: 362 [180480/225000 (80%)] Loss: 6882.013672\n",
      "Train Epoch: 362 [184576/225000 (82%)] Loss: 6891.410156\n",
      "Train Epoch: 362 [188672/225000 (84%)] Loss: 6825.787109\n",
      "Train Epoch: 362 [192768/225000 (86%)] Loss: 6839.289062\n",
      "Train Epoch: 362 [196864/225000 (87%)] Loss: 6898.056641\n",
      "Train Epoch: 362 [200960/225000 (89%)] Loss: 6758.248047\n",
      "Train Epoch: 362 [205056/225000 (91%)] Loss: 6911.822266\n",
      "Train Epoch: 362 [209152/225000 (93%)] Loss: 6864.210938\n",
      "Train Epoch: 362 [213248/225000 (95%)] Loss: 6763.015625\n",
      "Train Epoch: 362 [217344/225000 (97%)] Loss: 6871.802734\n",
      "Train Epoch: 362 [221440/225000 (98%)] Loss: 6816.359375\n",
      "    epoch          : 362\n",
      "    loss           : 6888.544721896331\n",
      "    val_loss       : 6867.390795050836\n",
      "Train Epoch: 363 [256/225000 (0%)] Loss: 6918.837891\n",
      "Train Epoch: 363 [4352/225000 (2%)] Loss: 6936.859375\n",
      "Train Epoch: 363 [8448/225000 (4%)] Loss: 6796.373047\n",
      "Train Epoch: 363 [12544/225000 (6%)] Loss: 6742.898438\n",
      "Train Epoch: 363 [16640/225000 (7%)] Loss: 6839.328125\n",
      "Train Epoch: 363 [20736/225000 (9%)] Loss: 6843.494141\n",
      "Train Epoch: 363 [24832/225000 (11%)] Loss: 6872.201172\n",
      "Train Epoch: 363 [28928/225000 (13%)] Loss: 6865.589844\n",
      "Train Epoch: 363 [33024/225000 (15%)] Loss: 6996.660156\n",
      "Train Epoch: 363 [37120/225000 (16%)] Loss: 6857.939453\n",
      "Train Epoch: 363 [41216/225000 (18%)] Loss: 7019.056641\n",
      "Train Epoch: 363 [45312/225000 (20%)] Loss: 6917.341797\n",
      "Train Epoch: 363 [49408/225000 (22%)] Loss: 6880.703125\n",
      "Train Epoch: 363 [53504/225000 (24%)] Loss: 7023.380859\n",
      "Train Epoch: 363 [57600/225000 (26%)] Loss: 6805.472656\n",
      "Train Epoch: 363 [61696/225000 (27%)] Loss: 6883.267578\n",
      "Train Epoch: 363 [65792/225000 (29%)] Loss: 6895.771484\n",
      "Train Epoch: 363 [69888/225000 (31%)] Loss: 6787.794922\n",
      "Train Epoch: 363 [73984/225000 (33%)] Loss: 7024.195312\n",
      "Train Epoch: 363 [78080/225000 (35%)] Loss: 6980.960938\n",
      "Train Epoch: 363 [82176/225000 (37%)] Loss: 6808.101562\n",
      "Train Epoch: 363 [86272/225000 (38%)] Loss: 6920.597656\n",
      "Train Epoch: 363 [90368/225000 (40%)] Loss: 6818.996094\n",
      "Train Epoch: 363 [94464/225000 (42%)] Loss: 6855.259766\n",
      "Train Epoch: 363 [98560/225000 (44%)] Loss: 6842.123047\n",
      "Train Epoch: 363 [102656/225000 (46%)] Loss: 6712.914062\n",
      "Train Epoch: 363 [106752/225000 (47%)] Loss: 6836.714844\n",
      "Train Epoch: 363 [110848/225000 (49%)] Loss: 6809.062500\n",
      "Train Epoch: 363 [114944/225000 (51%)] Loss: 6847.445312\n",
      "Train Epoch: 363 [119040/225000 (53%)] Loss: 6848.718750\n",
      "Train Epoch: 363 [123136/225000 (55%)] Loss: 6831.750000\n",
      "Train Epoch: 363 [127232/225000 (57%)] Loss: 6868.125000\n",
      "Train Epoch: 363 [131328/225000 (58%)] Loss: 6813.666016\n",
      "Train Epoch: 363 [135424/225000 (60%)] Loss: 6897.654297\n",
      "Train Epoch: 363 [139520/225000 (62%)] Loss: 6711.406250\n",
      "Train Epoch: 363 [143616/225000 (64%)] Loss: 6765.169922\n",
      "Train Epoch: 363 [147712/225000 (66%)] Loss: 6851.343750\n",
      "Train Epoch: 363 [151808/225000 (67%)] Loss: 6883.880859\n",
      "Train Epoch: 363 [155904/225000 (69%)] Loss: 6868.609375\n",
      "Train Epoch: 363 [160000/225000 (71%)] Loss: 6709.437500\n",
      "Train Epoch: 363 [164096/225000 (73%)] Loss: 6875.775391\n",
      "Train Epoch: 363 [168192/225000 (75%)] Loss: 6756.390625\n",
      "Train Epoch: 363 [172288/225000 (77%)] Loss: 6887.287109\n",
      "Train Epoch: 363 [176384/225000 (78%)] Loss: 6896.615234\n",
      "Train Epoch: 363 [180480/225000 (80%)] Loss: 6960.486328\n",
      "Train Epoch: 363 [184576/225000 (82%)] Loss: 6836.582031\n",
      "Train Epoch: 363 [188672/225000 (84%)] Loss: 6798.416016\n",
      "Train Epoch: 363 [192768/225000 (86%)] Loss: 6723.605469\n",
      "Train Epoch: 363 [196864/225000 (87%)] Loss: 6699.000000\n",
      "Train Epoch: 363 [200960/225000 (89%)] Loss: 6870.984375\n",
      "Train Epoch: 363 [205056/225000 (91%)] Loss: 6741.414062\n",
      "Train Epoch: 363 [209152/225000 (93%)] Loss: 6815.281250\n",
      "Train Epoch: 363 [213248/225000 (95%)] Loss: 6727.500000\n",
      "Train Epoch: 363 [217344/225000 (97%)] Loss: 6794.455078\n",
      "Train Epoch: 363 [221440/225000 (98%)] Loss: 6948.341797\n",
      "    epoch          : 363\n",
      "    loss           : 6879.606793053185\n",
      "    val_loss       : 6866.460255464729\n",
      "Train Epoch: 364 [256/225000 (0%)] Loss: 6781.535156\n",
      "Train Epoch: 364 [4352/225000 (2%)] Loss: 6903.240234\n",
      "Train Epoch: 364 [8448/225000 (4%)] Loss: 6806.625000\n",
      "Train Epoch: 364 [12544/225000 (6%)] Loss: 6801.158203\n",
      "Train Epoch: 364 [16640/225000 (7%)] Loss: 6810.640625\n",
      "Train Epoch: 364 [20736/225000 (9%)] Loss: 6879.970703\n",
      "Train Epoch: 364 [24832/225000 (11%)] Loss: 6724.171875\n",
      "Train Epoch: 364 [28928/225000 (13%)] Loss: 6922.261719\n",
      "Train Epoch: 364 [33024/225000 (15%)] Loss: 6867.582031\n",
      "Train Epoch: 364 [37120/225000 (16%)] Loss: 6828.283203\n",
      "Train Epoch: 364 [41216/225000 (18%)] Loss: 6852.818359\n",
      "Train Epoch: 364 [45312/225000 (20%)] Loss: 6622.730469\n",
      "Train Epoch: 364 [49408/225000 (22%)] Loss: 6782.701172\n",
      "Train Epoch: 364 [53504/225000 (24%)] Loss: 6762.406250\n",
      "Train Epoch: 364 [57600/225000 (26%)] Loss: 6892.953125\n",
      "Train Epoch: 364 [61696/225000 (27%)] Loss: 6804.345703\n",
      "Train Epoch: 364 [65792/225000 (29%)] Loss: 6750.363281\n",
      "Train Epoch: 364 [69888/225000 (31%)] Loss: 6749.367188\n",
      "Train Epoch: 364 [73984/225000 (33%)] Loss: 6819.757812\n",
      "Train Epoch: 364 [78080/225000 (35%)] Loss: 6786.880859\n",
      "Train Epoch: 364 [82176/225000 (37%)] Loss: 6711.207031\n",
      "Train Epoch: 364 [86272/225000 (38%)] Loss: 6755.240234\n",
      "Train Epoch: 364 [90368/225000 (40%)] Loss: 6848.640625\n",
      "Train Epoch: 364 [94464/225000 (42%)] Loss: 6907.724609\n",
      "Train Epoch: 364 [98560/225000 (44%)] Loss: 6791.542969\n",
      "Train Epoch: 364 [102656/225000 (46%)] Loss: 6707.787109\n",
      "Train Epoch: 364 [106752/225000 (47%)] Loss: 6987.560547\n",
      "Train Epoch: 364 [110848/225000 (49%)] Loss: 7004.443359\n",
      "Train Epoch: 364 [114944/225000 (51%)] Loss: 7000.679688\n",
      "Train Epoch: 364 [119040/225000 (53%)] Loss: 6797.283203\n",
      "Train Epoch: 364 [123136/225000 (55%)] Loss: 6932.330078\n",
      "Train Epoch: 364 [127232/225000 (57%)] Loss: 6882.416016\n",
      "Train Epoch: 364 [131328/225000 (58%)] Loss: 6893.707031\n",
      "Train Epoch: 364 [135424/225000 (60%)] Loss: 6847.193359\n",
      "Train Epoch: 364 [139520/225000 (62%)] Loss: 6772.742188\n",
      "Train Epoch: 364 [143616/225000 (64%)] Loss: 6840.763672\n",
      "Train Epoch: 364 [147712/225000 (66%)] Loss: 6822.982422\n",
      "Train Epoch: 364 [151808/225000 (67%)] Loss: 6877.312500\n",
      "Train Epoch: 364 [155904/225000 (69%)] Loss: 6961.869141\n",
      "Train Epoch: 364 [160000/225000 (71%)] Loss: 6949.181641\n",
      "Train Epoch: 364 [164096/225000 (73%)] Loss: 6861.033203\n",
      "Train Epoch: 364 [168192/225000 (75%)] Loss: 6931.152344\n",
      "Train Epoch: 364 [172288/225000 (77%)] Loss: 6839.306641\n",
      "Train Epoch: 364 [176384/225000 (78%)] Loss: 6857.300781\n",
      "Train Epoch: 364 [180480/225000 (80%)] Loss: 6868.613281\n",
      "Train Epoch: 364 [184576/225000 (82%)] Loss: 6811.626953\n",
      "Train Epoch: 364 [188672/225000 (84%)] Loss: 6840.617188\n",
      "Train Epoch: 364 [192768/225000 (86%)] Loss: 6829.328125\n",
      "Train Epoch: 364 [196864/225000 (87%)] Loss: 6878.746094\n",
      "Train Epoch: 364 [200960/225000 (89%)] Loss: 6791.005859\n",
      "Train Epoch: 364 [205056/225000 (91%)] Loss: 6861.414062\n",
      "Train Epoch: 364 [209152/225000 (93%)] Loss: 6845.310547\n",
      "Train Epoch: 364 [213248/225000 (95%)] Loss: 6758.519531\n",
      "Train Epoch: 364 [217344/225000 (97%)] Loss: 6828.964844\n",
      "Train Epoch: 364 [221440/225000 (98%)] Loss: 6682.943359\n",
      "    epoch          : 364\n",
      "    loss           : 6859.375488836747\n",
      "    val_loss       : 6862.041082946622\n",
      "Train Epoch: 365 [256/225000 (0%)] Loss: 6787.996094\n",
      "Train Epoch: 365 [4352/225000 (2%)] Loss: 6867.007812\n",
      "Train Epoch: 365 [8448/225000 (4%)] Loss: 6921.310547\n",
      "Train Epoch: 365 [12544/225000 (6%)] Loss: 6855.958984\n",
      "Train Epoch: 365 [16640/225000 (7%)] Loss: 6754.511719\n",
      "Train Epoch: 365 [20736/225000 (9%)] Loss: 6913.554688\n",
      "Train Epoch: 365 [24832/225000 (11%)] Loss: 6874.140625\n",
      "Train Epoch: 365 [28928/225000 (13%)] Loss: 6805.835938\n",
      "Train Epoch: 365 [33024/225000 (15%)] Loss: 6741.751953\n",
      "Train Epoch: 365 [37120/225000 (16%)] Loss: 6842.904297\n",
      "Train Epoch: 365 [41216/225000 (18%)] Loss: 6713.794922\n",
      "Train Epoch: 365 [45312/225000 (20%)] Loss: 6915.250000\n",
      "Train Epoch: 365 [49408/225000 (22%)] Loss: 6990.269531\n",
      "Train Epoch: 365 [53504/225000 (24%)] Loss: 6689.990234\n",
      "Train Epoch: 365 [57600/225000 (26%)] Loss: 6847.708984\n",
      "Train Epoch: 365 [61696/225000 (27%)] Loss: 6791.697266\n",
      "Train Epoch: 365 [65792/225000 (29%)] Loss: 6732.826172\n",
      "Train Epoch: 365 [69888/225000 (31%)] Loss: 6763.076172\n",
      "Train Epoch: 365 [73984/225000 (33%)] Loss: 6700.720703\n",
      "Train Epoch: 365 [78080/225000 (35%)] Loss: 6838.824219\n",
      "Train Epoch: 365 [82176/225000 (37%)] Loss: 6858.960938\n",
      "Train Epoch: 365 [86272/225000 (38%)] Loss: 7132.978516\n",
      "Train Epoch: 365 [90368/225000 (40%)] Loss: 6724.080078\n",
      "Train Epoch: 365 [94464/225000 (42%)] Loss: 6714.816406\n",
      "Train Epoch: 365 [98560/225000 (44%)] Loss: 6813.414062\n",
      "Train Epoch: 365 [102656/225000 (46%)] Loss: 6726.457031\n",
      "Train Epoch: 365 [106752/225000 (47%)] Loss: 6795.263672\n",
      "Train Epoch: 365 [110848/225000 (49%)] Loss: 6927.234375\n",
      "Train Epoch: 365 [114944/225000 (51%)] Loss: 6894.072266\n",
      "Train Epoch: 365 [119040/225000 (53%)] Loss: 6776.330078\n",
      "Train Epoch: 365 [123136/225000 (55%)] Loss: 6890.914062\n",
      "Train Epoch: 365 [127232/225000 (57%)] Loss: 6688.998047\n",
      "Train Epoch: 365 [131328/225000 (58%)] Loss: 6888.767578\n",
      "Train Epoch: 365 [135424/225000 (60%)] Loss: 6707.900391\n",
      "Train Epoch: 365 [139520/225000 (62%)] Loss: 6966.759766\n",
      "Train Epoch: 365 [143616/225000 (64%)] Loss: 6710.779297\n",
      "Train Epoch: 365 [147712/225000 (66%)] Loss: 6830.427734\n",
      "Train Epoch: 365 [151808/225000 (67%)] Loss: 6825.373047\n",
      "Train Epoch: 365 [155904/225000 (69%)] Loss: 7006.662109\n",
      "Train Epoch: 365 [160000/225000 (71%)] Loss: 6901.753906\n",
      "Train Epoch: 365 [164096/225000 (73%)] Loss: 6948.634766\n",
      "Train Epoch: 365 [168192/225000 (75%)] Loss: 6844.826172\n",
      "Train Epoch: 365 [172288/225000 (77%)] Loss: 6797.056641\n",
      "Train Epoch: 365 [176384/225000 (78%)] Loss: 6797.087891\n",
      "Train Epoch: 365 [180480/225000 (80%)] Loss: 6765.083984\n",
      "Train Epoch: 365 [184576/225000 (82%)] Loss: 6911.798828\n",
      "Train Epoch: 365 [188672/225000 (84%)] Loss: 6854.451172\n",
      "Train Epoch: 365 [192768/225000 (86%)] Loss: 6832.705078\n",
      "Train Epoch: 365 [196864/225000 (87%)] Loss: 6841.894531\n",
      "Train Epoch: 365 [200960/225000 (89%)] Loss: 6883.492188\n",
      "Train Epoch: 365 [205056/225000 (91%)] Loss: 6872.515625\n",
      "Train Epoch: 365 [209152/225000 (93%)] Loss: 6825.824219\n",
      "Train Epoch: 365 [213248/225000 (95%)] Loss: 6767.222656\n",
      "Train Epoch: 365 [217344/225000 (97%)] Loss: 6732.396484\n",
      "Train Epoch: 365 [221440/225000 (98%)] Loss: 6815.482422\n",
      "    epoch          : 365\n",
      "    loss           : 6856.575674150313\n",
      "    val_loss       : 6862.91565648147\n",
      "Train Epoch: 366 [256/225000 (0%)] Loss: 7010.460938\n",
      "Train Epoch: 366 [4352/225000 (2%)] Loss: 6841.806641\n",
      "Train Epoch: 366 [8448/225000 (4%)] Loss: 6768.185547\n",
      "Train Epoch: 366 [12544/225000 (6%)] Loss: 6892.912109\n",
      "Train Epoch: 366 [16640/225000 (7%)] Loss: 6900.390625\n",
      "Train Epoch: 366 [20736/225000 (9%)] Loss: 6608.927734\n",
      "Train Epoch: 366 [24832/225000 (11%)] Loss: 6982.687500\n",
      "Train Epoch: 366 [28928/225000 (13%)] Loss: 6775.886719\n",
      "Train Epoch: 366 [33024/225000 (15%)] Loss: 6868.478516\n",
      "Train Epoch: 366 [37120/225000 (16%)] Loss: 6796.888672\n",
      "Train Epoch: 366 [41216/225000 (18%)] Loss: 6772.636719\n",
      "Train Epoch: 366 [45312/225000 (20%)] Loss: 6957.888672\n",
      "Train Epoch: 366 [49408/225000 (22%)] Loss: 6958.570312\n",
      "Train Epoch: 366 [53504/225000 (24%)] Loss: 6975.005859\n",
      "Train Epoch: 366 [57600/225000 (26%)] Loss: 6793.720703\n",
      "Train Epoch: 366 [61696/225000 (27%)] Loss: 6809.171875\n",
      "Train Epoch: 366 [65792/225000 (29%)] Loss: 6855.417969\n",
      "Train Epoch: 366 [69888/225000 (31%)] Loss: 6969.126953\n",
      "Train Epoch: 366 [73984/225000 (33%)] Loss: 6930.123047\n",
      "Train Epoch: 366 [78080/225000 (35%)] Loss: 6774.402344\n",
      "Train Epoch: 366 [82176/225000 (37%)] Loss: 6889.257812\n",
      "Train Epoch: 366 [86272/225000 (38%)] Loss: 6836.207031\n",
      "Train Epoch: 366 [90368/225000 (40%)] Loss: 6908.259766\n",
      "Train Epoch: 366 [94464/225000 (42%)] Loss: 6921.683594\n",
      "Train Epoch: 366 [98560/225000 (44%)] Loss: 6744.027344\n",
      "Train Epoch: 366 [102656/225000 (46%)] Loss: 6826.757812\n",
      "Train Epoch: 366 [106752/225000 (47%)] Loss: 6909.773438\n",
      "Train Epoch: 366 [110848/225000 (49%)] Loss: 6825.769531\n",
      "Train Epoch: 366 [114944/225000 (51%)] Loss: 6925.216797\n",
      "Train Epoch: 366 [119040/225000 (53%)] Loss: 6749.138672\n",
      "Train Epoch: 366 [123136/225000 (55%)] Loss: 6921.931641\n",
      "Train Epoch: 366 [127232/225000 (57%)] Loss: 6886.408203\n",
      "Train Epoch: 366 [131328/225000 (58%)] Loss: 7025.472656\n",
      "Train Epoch: 366 [135424/225000 (60%)] Loss: 6879.701172\n",
      "Train Epoch: 366 [139520/225000 (62%)] Loss: 6875.062500\n",
      "Train Epoch: 366 [143616/225000 (64%)] Loss: 6945.615234\n",
      "Train Epoch: 366 [147712/225000 (66%)] Loss: 6751.132812\n",
      "Train Epoch: 366 [151808/225000 (67%)] Loss: 6877.892578\n",
      "Train Epoch: 366 [155904/225000 (69%)] Loss: 6774.232422\n",
      "Train Epoch: 366 [160000/225000 (71%)] Loss: 6959.080078\n",
      "Train Epoch: 366 [164096/225000 (73%)] Loss: 6715.664062\n",
      "Train Epoch: 366 [168192/225000 (75%)] Loss: 6831.451172\n",
      "Train Epoch: 366 [172288/225000 (77%)] Loss: 6727.589844\n",
      "Train Epoch: 366 [176384/225000 (78%)] Loss: 6839.658203\n",
      "Train Epoch: 366 [180480/225000 (80%)] Loss: 6697.199219\n",
      "Train Epoch: 366 [184576/225000 (82%)] Loss: 6703.517578\n",
      "Train Epoch: 366 [188672/225000 (84%)] Loss: 6922.841797\n",
      "Train Epoch: 366 [192768/225000 (86%)] Loss: 6795.027344\n",
      "Train Epoch: 366 [196864/225000 (87%)] Loss: 6900.113281\n",
      "Train Epoch: 366 [200960/225000 (89%)] Loss: 6737.710938\n",
      "Train Epoch: 366 [205056/225000 (91%)] Loss: 6929.560547\n",
      "Train Epoch: 366 [209152/225000 (93%)] Loss: 6724.353516\n",
      "Train Epoch: 366 [213248/225000 (95%)] Loss: 6714.679688\n",
      "Train Epoch: 366 [217344/225000 (97%)] Loss: 6778.054688\n",
      "Train Epoch: 366 [221440/225000 (98%)] Loss: 6706.658203\n",
      "    epoch          : 366\n",
      "    loss           : 6855.685671306172\n",
      "    val_loss       : 6862.739827507613\n",
      "Train Epoch: 367 [256/225000 (0%)] Loss: 6746.390625\n",
      "Train Epoch: 367 [4352/225000 (2%)] Loss: 7016.236328\n",
      "Train Epoch: 367 [8448/225000 (4%)] Loss: 6805.023438\n",
      "Train Epoch: 367 [12544/225000 (6%)] Loss: 6676.621094\n",
      "Train Epoch: 367 [16640/225000 (7%)] Loss: 6982.257812\n",
      "Train Epoch: 367 [20736/225000 (9%)] Loss: 6846.138672\n",
      "Train Epoch: 367 [24832/225000 (11%)] Loss: 6830.728516\n",
      "Train Epoch: 367 [28928/225000 (13%)] Loss: 6781.917969\n",
      "Train Epoch: 367 [33024/225000 (15%)] Loss: 6854.048828\n",
      "Train Epoch: 367 [37120/225000 (16%)] Loss: 6919.974609\n",
      "Train Epoch: 367 [41216/225000 (18%)] Loss: 7012.359375\n",
      "Train Epoch: 367 [45312/225000 (20%)] Loss: 6792.945312\n",
      "Train Epoch: 367 [49408/225000 (22%)] Loss: 6910.498047\n",
      "Train Epoch: 367 [53504/225000 (24%)] Loss: 6922.927734\n",
      "Train Epoch: 367 [57600/225000 (26%)] Loss: 6931.433594\n",
      "Train Epoch: 367 [61696/225000 (27%)] Loss: 6875.166016\n",
      "Train Epoch: 367 [65792/225000 (29%)] Loss: 6770.787109\n",
      "Train Epoch: 367 [69888/225000 (31%)] Loss: 6853.960938\n",
      "Train Epoch: 367 [73984/225000 (33%)] Loss: 7042.179688\n",
      "Train Epoch: 367 [78080/225000 (35%)] Loss: 6859.058594\n",
      "Train Epoch: 367 [82176/225000 (37%)] Loss: 6764.390625\n",
      "Train Epoch: 367 [86272/225000 (38%)] Loss: 6904.355469\n",
      "Train Epoch: 367 [90368/225000 (40%)] Loss: 6829.339844\n",
      "Train Epoch: 367 [94464/225000 (42%)] Loss: 6951.031250\n",
      "Train Epoch: 367 [98560/225000 (44%)] Loss: 6749.955078\n",
      "Train Epoch: 367 [102656/225000 (46%)] Loss: 7005.212891\n",
      "Train Epoch: 367 [106752/225000 (47%)] Loss: 6774.994141\n",
      "Train Epoch: 367 [110848/225000 (49%)] Loss: 6809.453125\n",
      "Train Epoch: 367 [114944/225000 (51%)] Loss: 6794.757812\n",
      "Train Epoch: 367 [119040/225000 (53%)] Loss: 6787.083984\n",
      "Train Epoch: 367 [123136/225000 (55%)] Loss: 6952.519531\n",
      "Train Epoch: 367 [127232/225000 (57%)] Loss: 6897.373047\n",
      "Train Epoch: 367 [131328/225000 (58%)] Loss: 6739.873047\n",
      "Train Epoch: 367 [135424/225000 (60%)] Loss: 6842.324219\n",
      "Train Epoch: 367 [139520/225000 (62%)] Loss: 6751.876953\n",
      "Train Epoch: 367 [143616/225000 (64%)] Loss: 6849.591797\n",
      "Train Epoch: 367 [147712/225000 (66%)] Loss: 6841.591797\n",
      "Train Epoch: 367 [151808/225000 (67%)] Loss: 6925.468750\n",
      "Train Epoch: 367 [155904/225000 (69%)] Loss: 6883.080078\n",
      "Train Epoch: 367 [160000/225000 (71%)] Loss: 6886.996094\n",
      "Train Epoch: 367 [164096/225000 (73%)] Loss: 6745.603516\n",
      "Train Epoch: 367 [168192/225000 (75%)] Loss: 6774.925781\n",
      "Train Epoch: 367 [172288/225000 (77%)] Loss: 6766.140625\n",
      "Train Epoch: 367 [176384/225000 (78%)] Loss: 6935.669922\n",
      "Train Epoch: 367 [180480/225000 (80%)] Loss: 6917.392578\n",
      "Train Epoch: 367 [184576/225000 (82%)] Loss: 6821.697266\n",
      "Train Epoch: 367 [188672/225000 (84%)] Loss: 6920.343750\n",
      "Train Epoch: 367 [192768/225000 (86%)] Loss: 7072.162109\n",
      "Train Epoch: 367 [196864/225000 (87%)] Loss: 6970.632812\n",
      "Train Epoch: 367 [200960/225000 (89%)] Loss: 6841.636719\n",
      "Train Epoch: 367 [205056/225000 (91%)] Loss: 6598.574219\n",
      "Train Epoch: 367 [209152/225000 (93%)] Loss: 6775.429688\n",
      "Train Epoch: 367 [213248/225000 (95%)] Loss: 6867.339844\n",
      "Train Epoch: 367 [217344/225000 (97%)] Loss: 6884.621094\n",
      "Train Epoch: 367 [221440/225000 (98%)] Loss: 6987.667969\n",
      "    epoch          : 367\n",
      "    loss           : 6846.374247858006\n",
      "    val_loss       : 6915.988878678302\n",
      "Train Epoch: 368 [256/225000 (0%)] Loss: 6846.125000\n",
      "Train Epoch: 368 [4352/225000 (2%)] Loss: 6921.457031\n",
      "Train Epoch: 368 [8448/225000 (4%)] Loss: 6759.871094\n",
      "Train Epoch: 368 [12544/225000 (6%)] Loss: 6892.000000\n",
      "Train Epoch: 368 [16640/225000 (7%)] Loss: 6741.080078\n",
      "Train Epoch: 368 [20736/225000 (9%)] Loss: 6777.412109\n",
      "Train Epoch: 368 [24832/225000 (11%)] Loss: 6904.548828\n",
      "Train Epoch: 368 [28928/225000 (13%)] Loss: 6686.623047\n",
      "Train Epoch: 368 [33024/225000 (15%)] Loss: 6956.181641\n",
      "Train Epoch: 368 [37120/225000 (16%)] Loss: 6777.886719\n",
      "Train Epoch: 368 [41216/225000 (18%)] Loss: 6752.992188\n",
      "Train Epoch: 368 [45312/225000 (20%)] Loss: 6704.476562\n",
      "Train Epoch: 368 [49408/225000 (22%)] Loss: 6760.804688\n",
      "Train Epoch: 368 [53504/225000 (24%)] Loss: 6875.759766\n",
      "Train Epoch: 368 [57600/225000 (26%)] Loss: 6870.716797\n",
      "Train Epoch: 368 [61696/225000 (27%)] Loss: 6772.181641\n",
      "Train Epoch: 368 [65792/225000 (29%)] Loss: 6905.654297\n",
      "Train Epoch: 368 [69888/225000 (31%)] Loss: 7013.541016\n",
      "Train Epoch: 368 [73984/225000 (33%)] Loss: 6836.464844\n",
      "Train Epoch: 368 [78080/225000 (35%)] Loss: 6911.279297\n",
      "Train Epoch: 368 [82176/225000 (37%)] Loss: 6823.091797\n",
      "Train Epoch: 368 [86272/225000 (38%)] Loss: 6769.574219\n",
      "Train Epoch: 368 [90368/225000 (40%)] Loss: 6960.177734\n",
      "Train Epoch: 368 [94464/225000 (42%)] Loss: 6943.568359\n",
      "Train Epoch: 368 [98560/225000 (44%)] Loss: 6875.638672\n",
      "Train Epoch: 368 [102656/225000 (46%)] Loss: 6794.947266\n",
      "Train Epoch: 368 [106752/225000 (47%)] Loss: 6777.072266\n",
      "Train Epoch: 368 [110848/225000 (49%)] Loss: 6959.566406\n",
      "Train Epoch: 368 [114944/225000 (51%)] Loss: 6933.267578\n",
      "Train Epoch: 368 [119040/225000 (53%)] Loss: 6838.494141\n",
      "Train Epoch: 368 [123136/225000 (55%)] Loss: 6782.851562\n",
      "Train Epoch: 368 [127232/225000 (57%)] Loss: 6877.828125\n",
      "Train Epoch: 368 [131328/225000 (58%)] Loss: 6928.517578\n",
      "Train Epoch: 368 [135424/225000 (60%)] Loss: 6555.367188\n",
      "Train Epoch: 368 [139520/225000 (62%)] Loss: 6884.416016\n",
      "Train Epoch: 368 [143616/225000 (64%)] Loss: 6990.759766\n",
      "Train Epoch: 368 [147712/225000 (66%)] Loss: 6748.189453\n",
      "Train Epoch: 368 [151808/225000 (67%)] Loss: 6825.234375\n",
      "Train Epoch: 368 [155904/225000 (69%)] Loss: 6913.548828\n",
      "Train Epoch: 368 [160000/225000 (71%)] Loss: 6856.447266\n",
      "Train Epoch: 368 [164096/225000 (73%)] Loss: 6931.166016\n",
      "Train Epoch: 368 [168192/225000 (75%)] Loss: 6878.035156\n",
      "Train Epoch: 368 [172288/225000 (77%)] Loss: 6847.185547\n",
      "Train Epoch: 368 [176384/225000 (78%)] Loss: 6756.271484\n",
      "Train Epoch: 368 [180480/225000 (80%)] Loss: 6985.964844\n",
      "Train Epoch: 368 [184576/225000 (82%)] Loss: 6842.109375\n",
      "Train Epoch: 368 [188672/225000 (84%)] Loss: 6889.158203\n",
      "Train Epoch: 368 [192768/225000 (86%)] Loss: 6889.275391\n",
      "Train Epoch: 368 [196864/225000 (87%)] Loss: 7112.546875\n",
      "Train Epoch: 368 [200960/225000 (89%)] Loss: 6783.052734\n",
      "Train Epoch: 368 [205056/225000 (91%)] Loss: 6824.583984\n",
      "Train Epoch: 368 [209152/225000 (93%)] Loss: 6893.195312\n",
      "Train Epoch: 368 [213248/225000 (95%)] Loss: 6850.140625\n",
      "Train Epoch: 368 [217344/225000 (97%)] Loss: 6906.562500\n",
      "Train Epoch: 368 [221440/225000 (98%)] Loss: 6709.865234\n",
      "    epoch          : 368\n",
      "    loss           : 6855.725913680319\n",
      "    val_loss       : 6862.823108764327\n",
      "Train Epoch: 369 [256/225000 (0%)] Loss: 6801.958984\n",
      "Train Epoch: 369 [4352/225000 (2%)] Loss: 6820.671875\n",
      "Train Epoch: 369 [8448/225000 (4%)] Loss: 6785.929688\n",
      "Train Epoch: 369 [12544/225000 (6%)] Loss: 6812.781250\n",
      "Train Epoch: 369 [16640/225000 (7%)] Loss: 6692.187500\n",
      "Train Epoch: 369 [20736/225000 (9%)] Loss: 6915.621094\n",
      "Train Epoch: 369 [24832/225000 (11%)] Loss: 6765.572266\n",
      "Train Epoch: 369 [28928/225000 (13%)] Loss: 6790.419922\n",
      "Train Epoch: 369 [33024/225000 (15%)] Loss: 6882.429688\n",
      "Train Epoch: 369 [37120/225000 (16%)] Loss: 6909.447266\n",
      "Train Epoch: 369 [41216/225000 (18%)] Loss: 6818.484375\n",
      "Train Epoch: 369 [45312/225000 (20%)] Loss: 6844.337891\n",
      "Train Epoch: 369 [49408/225000 (22%)] Loss: 6903.337891\n",
      "Train Epoch: 369 [53504/225000 (24%)] Loss: 6874.623047\n",
      "Train Epoch: 369 [57600/225000 (26%)] Loss: 6787.119141\n",
      "Train Epoch: 369 [61696/225000 (27%)] Loss: 6785.761719\n",
      "Train Epoch: 369 [65792/225000 (29%)] Loss: 6889.359375\n",
      "Train Epoch: 369 [69888/225000 (31%)] Loss: 6732.785156\n",
      "Train Epoch: 369 [73984/225000 (33%)] Loss: 7027.130859\n",
      "Train Epoch: 369 [78080/225000 (35%)] Loss: 6844.871094\n",
      "Train Epoch: 369 [82176/225000 (37%)] Loss: 6840.914062\n",
      "Train Epoch: 369 [86272/225000 (38%)] Loss: 6838.488281\n",
      "Train Epoch: 369 [90368/225000 (40%)] Loss: 7015.654297\n",
      "Train Epoch: 369 [94464/225000 (42%)] Loss: 6835.660156\n",
      "Train Epoch: 369 [98560/225000 (44%)] Loss: 6656.378906\n",
      "Train Epoch: 369 [102656/225000 (46%)] Loss: 6867.228516\n",
      "Train Epoch: 369 [106752/225000 (47%)] Loss: 6821.189453\n",
      "Train Epoch: 369 [110848/225000 (49%)] Loss: 6765.351562\n",
      "Train Epoch: 369 [114944/225000 (51%)] Loss: 6883.912109\n",
      "Train Epoch: 369 [119040/225000 (53%)] Loss: 6719.533203\n",
      "Train Epoch: 369 [123136/225000 (55%)] Loss: 6933.916016\n",
      "Train Epoch: 369 [127232/225000 (57%)] Loss: 6687.673828\n",
      "Train Epoch: 369 [131328/225000 (58%)] Loss: 6930.332031\n",
      "Train Epoch: 369 [135424/225000 (60%)] Loss: 6883.576172\n",
      "Train Epoch: 369 [139520/225000 (62%)] Loss: 6745.960938\n",
      "Train Epoch: 369 [143616/225000 (64%)] Loss: 6902.201172\n",
      "Train Epoch: 369 [147712/225000 (66%)] Loss: 6928.671875\n",
      "Train Epoch: 369 [151808/225000 (67%)] Loss: 6837.529297\n",
      "Train Epoch: 369 [155904/225000 (69%)] Loss: 6807.910156\n",
      "Train Epoch: 369 [160000/225000 (71%)] Loss: 6870.476562\n",
      "Train Epoch: 369 [164096/225000 (73%)] Loss: 6804.544922\n",
      "Train Epoch: 369 [168192/225000 (75%)] Loss: 6752.078125\n",
      "Train Epoch: 369 [172288/225000 (77%)] Loss: 6916.123047\n",
      "Train Epoch: 369 [176384/225000 (78%)] Loss: 6873.785156\n",
      "Train Epoch: 369 [180480/225000 (80%)] Loss: 6876.730469\n",
      "Train Epoch: 369 [184576/225000 (82%)] Loss: 6745.853516\n",
      "Train Epoch: 369 [188672/225000 (84%)] Loss: 6770.195312\n",
      "Train Epoch: 369 [192768/225000 (86%)] Loss: 6851.271484\n",
      "Train Epoch: 369 [196864/225000 (87%)] Loss: 6822.601562\n",
      "Train Epoch: 369 [200960/225000 (89%)] Loss: 6825.767578\n",
      "Train Epoch: 369 [205056/225000 (91%)] Loss: 6849.154297\n",
      "Train Epoch: 369 [209152/225000 (93%)] Loss: 6718.173828\n",
      "Train Epoch: 369 [213248/225000 (95%)] Loss: 6743.126953\n",
      "Train Epoch: 369 [217344/225000 (97%)] Loss: 6959.615234\n",
      "Train Epoch: 369 [221440/225000 (98%)] Loss: 6854.287109\n",
      "    epoch          : 369\n",
      "    loss           : 6858.854669946317\n",
      "    val_loss       : 6855.825609853073\n",
      "Train Epoch: 370 [256/225000 (0%)] Loss: 6938.808594\n",
      "Train Epoch: 370 [4352/225000 (2%)] Loss: 6908.732422\n",
      "Train Epoch: 370 [8448/225000 (4%)] Loss: 6921.369141\n",
      "Train Epoch: 370 [12544/225000 (6%)] Loss: 6781.611328\n",
      "Train Epoch: 370 [16640/225000 (7%)] Loss: 6741.949219\n",
      "Train Epoch: 370 [20736/225000 (9%)] Loss: 6967.177734\n",
      "Train Epoch: 370 [24832/225000 (11%)] Loss: 6839.957031\n",
      "Train Epoch: 370 [28928/225000 (13%)] Loss: 6771.654297\n",
      "Train Epoch: 370 [33024/225000 (15%)] Loss: 6837.609375\n",
      "Train Epoch: 370 [37120/225000 (16%)] Loss: 6740.710938\n",
      "Train Epoch: 370 [41216/225000 (18%)] Loss: 6733.189453\n",
      "Train Epoch: 370 [45312/225000 (20%)] Loss: 6808.806641\n",
      "Train Epoch: 370 [49408/225000 (22%)] Loss: 6881.171875\n",
      "Train Epoch: 370 [53504/225000 (24%)] Loss: 6941.652344\n",
      "Train Epoch: 370 [57600/225000 (26%)] Loss: 6769.148438\n",
      "Train Epoch: 370 [61696/225000 (27%)] Loss: 6845.919922\n",
      "Train Epoch: 370 [65792/225000 (29%)] Loss: 6781.875000\n",
      "Train Epoch: 370 [69888/225000 (31%)] Loss: 6710.054688\n",
      "Train Epoch: 370 [73984/225000 (33%)] Loss: 6990.515625\n",
      "Train Epoch: 370 [78080/225000 (35%)] Loss: 7009.406250\n",
      "Train Epoch: 370 [82176/225000 (37%)] Loss: 6788.097656\n",
      "Train Epoch: 370 [86272/225000 (38%)] Loss: 6850.226562\n",
      "Train Epoch: 370 [90368/225000 (40%)] Loss: 6760.560547\n",
      "Train Epoch: 370 [94464/225000 (42%)] Loss: 6876.152344\n",
      "Train Epoch: 370 [98560/225000 (44%)] Loss: 6857.326172\n",
      "Train Epoch: 370 [102656/225000 (46%)] Loss: 6939.796875\n",
      "Train Epoch: 370 [106752/225000 (47%)] Loss: 6944.929688\n",
      "Train Epoch: 370 [110848/225000 (49%)] Loss: 6742.509766\n",
      "Train Epoch: 370 [114944/225000 (51%)] Loss: 6990.171875\n",
      "Train Epoch: 370 [119040/225000 (53%)] Loss: 6925.835938\n",
      "Train Epoch: 370 [123136/225000 (55%)] Loss: 6774.318359\n",
      "Train Epoch: 370 [127232/225000 (57%)] Loss: 6859.560547\n",
      "Train Epoch: 370 [131328/225000 (58%)] Loss: 6706.556641\n",
      "Train Epoch: 370 [135424/225000 (60%)] Loss: 6948.847656\n",
      "Train Epoch: 370 [139520/225000 (62%)] Loss: 6766.001953\n",
      "Train Epoch: 370 [143616/225000 (64%)] Loss: 6717.507812\n",
      "Train Epoch: 370 [147712/225000 (66%)] Loss: 6940.507812\n",
      "Train Epoch: 370 [151808/225000 (67%)] Loss: 6851.806641\n",
      "Train Epoch: 370 [155904/225000 (69%)] Loss: 6911.322266\n",
      "Train Epoch: 370 [160000/225000 (71%)] Loss: 6819.472656\n",
      "Train Epoch: 370 [164096/225000 (73%)] Loss: 6950.550781\n",
      "Train Epoch: 370 [168192/225000 (75%)] Loss: 6862.457031\n",
      "Train Epoch: 370 [172288/225000 (77%)] Loss: 6969.089844\n",
      "Train Epoch: 370 [176384/225000 (78%)] Loss: 6728.892578\n",
      "Train Epoch: 370 [180480/225000 (80%)] Loss: 6799.189453\n",
      "Train Epoch: 370 [184576/225000 (82%)] Loss: 6875.451172\n",
      "Train Epoch: 370 [188672/225000 (84%)] Loss: 6766.914062\n",
      "Train Epoch: 370 [192768/225000 (86%)] Loss: 6798.193359\n",
      "Train Epoch: 370 [196864/225000 (87%)] Loss: 6876.734375\n",
      "Train Epoch: 370 [200960/225000 (89%)] Loss: 6907.033203\n",
      "Train Epoch: 370 [205056/225000 (91%)] Loss: 7027.986328\n",
      "Train Epoch: 370 [209152/225000 (93%)] Loss: 6760.029297\n",
      "Train Epoch: 370 [213248/225000 (95%)] Loss: 6836.902344\n",
      "Train Epoch: 370 [217344/225000 (97%)] Loss: 6926.689453\n",
      "Train Epoch: 370 [221440/225000 (98%)] Loss: 6939.779297\n",
      "    epoch          : 370\n",
      "    loss           : 6841.907203231655\n",
      "    val_loss       : 6865.451631137303\n",
      "Train Epoch: 371 [256/225000 (0%)] Loss: 6888.998047\n",
      "Train Epoch: 371 [4352/225000 (2%)] Loss: 6732.896484\n",
      "Train Epoch: 371 [8448/225000 (4%)] Loss: 6794.789062\n",
      "Train Epoch: 371 [12544/225000 (6%)] Loss: 6946.123047\n",
      "Train Epoch: 371 [16640/225000 (7%)] Loss: 6895.451172\n",
      "Train Epoch: 371 [20736/225000 (9%)] Loss: 6934.548828\n",
      "Train Epoch: 371 [24832/225000 (11%)] Loss: 6827.894531\n",
      "Train Epoch: 371 [28928/225000 (13%)] Loss: 6809.369141\n",
      "Train Epoch: 371 [33024/225000 (15%)] Loss: 6866.558594\n",
      "Train Epoch: 371 [37120/225000 (16%)] Loss: 6865.787109\n",
      "Train Epoch: 371 [41216/225000 (18%)] Loss: 6801.064453\n",
      "Train Epoch: 371 [45312/225000 (20%)] Loss: 6730.521484\n",
      "Train Epoch: 371 [49408/225000 (22%)] Loss: 6666.449219\n",
      "Train Epoch: 371 [53504/225000 (24%)] Loss: 6853.123047\n",
      "Train Epoch: 371 [57600/225000 (26%)] Loss: 6971.173828\n",
      "Train Epoch: 371 [61696/225000 (27%)] Loss: 6658.753906\n",
      "Train Epoch: 371 [65792/225000 (29%)] Loss: 6758.042969\n",
      "Train Epoch: 371 [69888/225000 (31%)] Loss: 6863.111328\n",
      "Train Epoch: 371 [73984/225000 (33%)] Loss: 6832.708984\n",
      "Train Epoch: 371 [78080/225000 (35%)] Loss: 6623.876953\n",
      "Train Epoch: 371 [82176/225000 (37%)] Loss: 6792.304688\n",
      "Train Epoch: 371 [86272/225000 (38%)] Loss: 6803.695312\n",
      "Train Epoch: 371 [90368/225000 (40%)] Loss: 6721.347656\n",
      "Train Epoch: 371 [94464/225000 (42%)] Loss: 6846.322266\n",
      "Train Epoch: 371 [98560/225000 (44%)] Loss: 6797.529297\n",
      "Train Epoch: 371 [102656/225000 (46%)] Loss: 6672.689453\n",
      "Train Epoch: 371 [106752/225000 (47%)] Loss: 6937.136719\n",
      "Train Epoch: 371 [110848/225000 (49%)] Loss: 6812.722656\n",
      "Train Epoch: 371 [114944/225000 (51%)] Loss: 6768.755859\n",
      "Train Epoch: 371 [119040/225000 (53%)] Loss: 6834.251953\n",
      "Train Epoch: 371 [123136/225000 (55%)] Loss: 6890.007812\n",
      "Train Epoch: 371 [127232/225000 (57%)] Loss: 6702.921875\n",
      "Train Epoch: 371 [131328/225000 (58%)] Loss: 6787.228516\n",
      "Train Epoch: 371 [135424/225000 (60%)] Loss: 6905.371094\n",
      "Train Epoch: 371 [139520/225000 (62%)] Loss: 6817.744141\n",
      "Train Epoch: 371 [143616/225000 (64%)] Loss: 6911.886719\n",
      "Train Epoch: 371 [147712/225000 (66%)] Loss: 6676.767578\n",
      "Train Epoch: 371 [151808/225000 (67%)] Loss: 6834.240234\n",
      "Train Epoch: 371 [155904/225000 (69%)] Loss: 6933.074219\n",
      "Train Epoch: 371 [160000/225000 (71%)] Loss: 6884.570312\n",
      "Train Epoch: 371 [164096/225000 (73%)] Loss: 6995.109375\n",
      "Train Epoch: 371 [168192/225000 (75%)] Loss: 7067.269531\n",
      "Train Epoch: 371 [172288/225000 (77%)] Loss: 6758.710938\n",
      "Train Epoch: 371 [176384/225000 (78%)] Loss: 6834.310547\n",
      "Train Epoch: 371 [180480/225000 (80%)] Loss: 6812.283203\n",
      "Train Epoch: 371 [184576/225000 (82%)] Loss: 6831.070312\n",
      "Train Epoch: 371 [188672/225000 (84%)] Loss: 6958.253906\n",
      "Train Epoch: 371 [192768/225000 (86%)] Loss: 6825.976562\n",
      "Train Epoch: 371 [196864/225000 (87%)] Loss: 6816.722656\n",
      "Train Epoch: 371 [200960/225000 (89%)] Loss: 6966.738281\n",
      "Train Epoch: 371 [205056/225000 (91%)] Loss: 6775.769531\n",
      "Train Epoch: 371 [209152/225000 (93%)] Loss: 6691.771484\n",
      "Train Epoch: 371 [213248/225000 (95%)] Loss: 6800.470703\n",
      "Train Epoch: 371 [217344/225000 (97%)] Loss: 6802.791016\n",
      "Train Epoch: 371 [221440/225000 (98%)] Loss: 6769.054688\n",
      "    epoch          : 371\n",
      "    loss           : 6859.792547683803\n",
      "    val_loss       : 6859.454864334087\n",
      "Train Epoch: 372 [256/225000 (0%)] Loss: 6777.824219\n",
      "Train Epoch: 372 [4352/225000 (2%)] Loss: 7035.308594\n",
      "Train Epoch: 372 [8448/225000 (4%)] Loss: 6755.798828\n",
      "Train Epoch: 372 [12544/225000 (6%)] Loss: 6917.683594\n",
      "Train Epoch: 372 [16640/225000 (7%)] Loss: 6721.869141\n",
      "Train Epoch: 372 [20736/225000 (9%)] Loss: 6807.650391\n",
      "Train Epoch: 372 [24832/225000 (11%)] Loss: 6874.330078\n",
      "Train Epoch: 372 [28928/225000 (13%)] Loss: 6874.669922\n",
      "Train Epoch: 372 [33024/225000 (15%)] Loss: 6769.015625\n",
      "Train Epoch: 372 [37120/225000 (16%)] Loss: 6968.791016\n",
      "Train Epoch: 372 [41216/225000 (18%)] Loss: 6775.103516\n",
      "Train Epoch: 372 [45312/225000 (20%)] Loss: 6779.441406\n",
      "Train Epoch: 372 [49408/225000 (22%)] Loss: 6804.648438\n",
      "Train Epoch: 372 [53504/225000 (24%)] Loss: 6904.601562\n",
      "Train Epoch: 372 [57600/225000 (26%)] Loss: 6844.224609\n",
      "Train Epoch: 372 [61696/225000 (27%)] Loss: 6792.599609\n",
      "Train Epoch: 372 [65792/225000 (29%)] Loss: 6766.466797\n",
      "Train Epoch: 372 [69888/225000 (31%)] Loss: 6977.646484\n",
      "Train Epoch: 372 [73984/225000 (33%)] Loss: 6834.742188\n",
      "Train Epoch: 372 [78080/225000 (35%)] Loss: 6773.966797\n",
      "Train Epoch: 372 [82176/225000 (37%)] Loss: 6997.410156\n",
      "Train Epoch: 372 [86272/225000 (38%)] Loss: 6735.876953\n",
      "Train Epoch: 372 [90368/225000 (40%)] Loss: 6776.154297\n",
      "Train Epoch: 372 [94464/225000 (42%)] Loss: 6750.427734\n",
      "Train Epoch: 372 [98560/225000 (44%)] Loss: 6845.894531\n",
      "Train Epoch: 372 [102656/225000 (46%)] Loss: 6829.074219\n",
      "Train Epoch: 372 [106752/225000 (47%)] Loss: 6744.187500\n",
      "Train Epoch: 372 [110848/225000 (49%)] Loss: 6860.445312\n",
      "Train Epoch: 372 [114944/225000 (51%)] Loss: 6837.994141\n",
      "Train Epoch: 372 [119040/225000 (53%)] Loss: 6854.162109\n",
      "Train Epoch: 372 [123136/225000 (55%)] Loss: 6844.964844\n",
      "Train Epoch: 372 [127232/225000 (57%)] Loss: 6956.738281\n",
      "Train Epoch: 372 [131328/225000 (58%)] Loss: 6755.074219\n",
      "Train Epoch: 372 [135424/225000 (60%)] Loss: 6866.470703\n",
      "Train Epoch: 372 [139520/225000 (62%)] Loss: 6826.085938\n",
      "Train Epoch: 372 [143616/225000 (64%)] Loss: 6774.404297\n",
      "Train Epoch: 372 [147712/225000 (66%)] Loss: 6815.292969\n",
      "Train Epoch: 372 [151808/225000 (67%)] Loss: 6687.644531\n",
      "Train Epoch: 372 [155904/225000 (69%)] Loss: 6907.164062\n",
      "Train Epoch: 372 [160000/225000 (71%)] Loss: 6968.060547\n",
      "Train Epoch: 372 [164096/225000 (73%)] Loss: 6886.630859\n",
      "Train Epoch: 372 [168192/225000 (75%)] Loss: 6795.164062\n",
      "Train Epoch: 372 [172288/225000 (77%)] Loss: 6851.433594\n",
      "Train Epoch: 372 [176384/225000 (78%)] Loss: 6999.755859\n",
      "Train Epoch: 372 [180480/225000 (80%)] Loss: 6915.285156\n",
      "Train Epoch: 372 [184576/225000 (82%)] Loss: 6824.617188\n",
      "Train Epoch: 372 [188672/225000 (84%)] Loss: 6864.220703\n",
      "Train Epoch: 372 [192768/225000 (86%)] Loss: 6834.785156\n",
      "Train Epoch: 372 [196864/225000 (87%)] Loss: 6862.441406\n",
      "Train Epoch: 372 [200960/225000 (89%)] Loss: 6898.865234\n",
      "Train Epoch: 372 [205056/225000 (91%)] Loss: 6905.701172\n",
      "Train Epoch: 372 [209152/225000 (93%)] Loss: 6725.042969\n",
      "Train Epoch: 372 [213248/225000 (95%)] Loss: 6891.468750\n",
      "Train Epoch: 372 [217344/225000 (97%)] Loss: 6902.031250\n",
      "Train Epoch: 372 [221440/225000 (98%)] Loss: 6824.402344\n",
      "    epoch          : 372\n",
      "    loss           : 6852.040234597199\n",
      "    val_loss       : 6850.241923334647\n",
      "Train Epoch: 373 [256/225000 (0%)] Loss: 6954.449219\n",
      "Train Epoch: 373 [4352/225000 (2%)] Loss: 6854.046875\n",
      "Train Epoch: 373 [8448/225000 (4%)] Loss: 6731.828125\n",
      "Train Epoch: 373 [12544/225000 (6%)] Loss: 6729.394531\n",
      "Train Epoch: 373 [16640/225000 (7%)] Loss: 6871.226562\n",
      "Train Epoch: 373 [20736/225000 (9%)] Loss: 6872.990234\n",
      "Train Epoch: 373 [24832/225000 (11%)] Loss: 6864.425781\n",
      "Train Epoch: 373 [28928/225000 (13%)] Loss: 6810.679688\n",
      "Train Epoch: 373 [33024/225000 (15%)] Loss: 6672.837891\n",
      "Train Epoch: 373 [37120/225000 (16%)] Loss: 6684.113281\n",
      "Train Epoch: 373 [41216/225000 (18%)] Loss: 6982.341797\n",
      "Train Epoch: 373 [45312/225000 (20%)] Loss: 6732.392578\n",
      "Train Epoch: 373 [49408/225000 (22%)] Loss: 6775.937500\n",
      "Train Epoch: 373 [53504/225000 (24%)] Loss: 6797.857422\n",
      "Train Epoch: 373 [57600/225000 (26%)] Loss: 6938.835938\n",
      "Train Epoch: 373 [61696/225000 (27%)] Loss: 6728.009766\n",
      "Train Epoch: 373 [65792/225000 (29%)] Loss: 6928.480469\n",
      "Train Epoch: 373 [69888/225000 (31%)] Loss: 6782.431641\n",
      "Train Epoch: 373 [73984/225000 (33%)] Loss: 7046.423828\n",
      "Train Epoch: 373 [78080/225000 (35%)] Loss: 6826.900391\n",
      "Train Epoch: 373 [82176/225000 (37%)] Loss: 6870.427734\n",
      "Train Epoch: 373 [86272/225000 (38%)] Loss: 6711.117188\n",
      "Train Epoch: 373 [90368/225000 (40%)] Loss: 6769.585938\n",
      "Train Epoch: 373 [94464/225000 (42%)] Loss: 6765.208984\n",
      "Train Epoch: 373 [98560/225000 (44%)] Loss: 6809.197266\n",
      "Train Epoch: 373 [102656/225000 (46%)] Loss: 6825.363281\n",
      "Train Epoch: 373 [106752/225000 (47%)] Loss: 6787.857422\n",
      "Train Epoch: 373 [110848/225000 (49%)] Loss: 6829.595703\n",
      "Train Epoch: 373 [114944/225000 (51%)] Loss: 6838.728516\n",
      "Train Epoch: 373 [119040/225000 (53%)] Loss: 6960.695312\n",
      "Train Epoch: 373 [123136/225000 (55%)] Loss: 6823.080078\n",
      "Train Epoch: 373 [127232/225000 (57%)] Loss: 6759.554688\n",
      "Train Epoch: 373 [131328/225000 (58%)] Loss: 6705.570312\n",
      "Train Epoch: 373 [135424/225000 (60%)] Loss: 6859.355469\n",
      "Train Epoch: 373 [139520/225000 (62%)] Loss: 6766.593750\n",
      "Train Epoch: 373 [143616/225000 (64%)] Loss: 6847.462891\n",
      "Train Epoch: 373 [147712/225000 (66%)] Loss: 7040.695312\n",
      "Train Epoch: 373 [151808/225000 (67%)] Loss: 6831.519531\n",
      "Train Epoch: 373 [155904/225000 (69%)] Loss: 6758.507812\n",
      "Train Epoch: 373 [160000/225000 (71%)] Loss: 6696.537109\n",
      "Train Epoch: 373 [164096/225000 (73%)] Loss: 6841.318359\n",
      "Train Epoch: 373 [168192/225000 (75%)] Loss: 6756.035156\n",
      "Train Epoch: 373 [172288/225000 (77%)] Loss: 6776.281250\n",
      "Train Epoch: 373 [176384/225000 (78%)] Loss: 6762.021484\n",
      "Train Epoch: 373 [180480/225000 (80%)] Loss: 6828.910156\n",
      "Train Epoch: 373 [184576/225000 (82%)] Loss: 6683.255859\n",
      "Train Epoch: 373 [188672/225000 (84%)] Loss: 6765.119141\n",
      "Train Epoch: 373 [192768/225000 (86%)] Loss: 6806.726562\n",
      "Train Epoch: 373 [196864/225000 (87%)] Loss: 6790.449219\n",
      "Train Epoch: 373 [200960/225000 (89%)] Loss: 6834.982422\n",
      "Train Epoch: 373 [205056/225000 (91%)] Loss: 6754.257812\n",
      "Train Epoch: 373 [209152/225000 (93%)] Loss: 6849.390625\n",
      "Train Epoch: 373 [213248/225000 (95%)] Loss: 6732.886719\n",
      "Train Epoch: 373 [217344/225000 (97%)] Loss: 6953.642578\n",
      "Train Epoch: 373 [221440/225000 (98%)] Loss: 6932.716797\n",
      "    epoch          : 373\n",
      "    loss           : 6863.829592621231\n",
      "    val_loss       : 6856.7970064343235\n",
      "Train Epoch: 374 [256/225000 (0%)] Loss: 6792.490234\n",
      "Train Epoch: 374 [4352/225000 (2%)] Loss: 6859.335938\n",
      "Train Epoch: 374 [8448/225000 (4%)] Loss: 6636.617188\n",
      "Train Epoch: 374 [12544/225000 (6%)] Loss: 6827.744141\n",
      "Train Epoch: 374 [16640/225000 (7%)] Loss: 6659.154297\n",
      "Train Epoch: 374 [20736/225000 (9%)] Loss: 6765.923828\n",
      "Train Epoch: 374 [24832/225000 (11%)] Loss: 6752.615234\n",
      "Train Epoch: 374 [28928/225000 (13%)] Loss: 6715.130859\n",
      "Train Epoch: 374 [33024/225000 (15%)] Loss: 6931.517578\n",
      "Train Epoch: 374 [37120/225000 (16%)] Loss: 6774.433594\n",
      "Train Epoch: 374 [41216/225000 (18%)] Loss: 6933.871094\n",
      "Train Epoch: 374 [45312/225000 (20%)] Loss: 6731.304688\n",
      "Train Epoch: 374 [49408/225000 (22%)] Loss: 6892.441406\n",
      "Train Epoch: 374 [53504/225000 (24%)] Loss: 6786.337891\n",
      "Train Epoch: 374 [57600/225000 (26%)] Loss: 6726.744141\n",
      "Train Epoch: 374 [61696/225000 (27%)] Loss: 6877.964844\n",
      "Train Epoch: 374 [65792/225000 (29%)] Loss: 6839.990234\n",
      "Train Epoch: 374 [69888/225000 (31%)] Loss: 6940.644531\n",
      "Train Epoch: 374 [73984/225000 (33%)] Loss: 6850.484375\n",
      "Train Epoch: 374 [78080/225000 (35%)] Loss: 6804.355469\n",
      "Train Epoch: 374 [82176/225000 (37%)] Loss: 6903.068359\n",
      "Train Epoch: 374 [86272/225000 (38%)] Loss: 6713.267578\n",
      "Train Epoch: 374 [90368/225000 (40%)] Loss: 6861.705078\n",
      "Train Epoch: 374 [94464/225000 (42%)] Loss: 6805.648438\n",
      "Train Epoch: 374 [98560/225000 (44%)] Loss: 6927.763672\n",
      "Train Epoch: 374 [102656/225000 (46%)] Loss: 6878.650391\n",
      "Train Epoch: 374 [106752/225000 (47%)] Loss: 6893.849609\n",
      "Train Epoch: 374 [110848/225000 (49%)] Loss: 6937.796875\n",
      "Train Epoch: 374 [114944/225000 (51%)] Loss: 6800.830078\n",
      "Train Epoch: 374 [119040/225000 (53%)] Loss: 6839.378906\n",
      "Train Epoch: 374 [123136/225000 (55%)] Loss: 6992.949219\n",
      "Train Epoch: 374 [127232/225000 (57%)] Loss: 6802.169922\n",
      "Train Epoch: 374 [131328/225000 (58%)] Loss: 7069.621094\n",
      "Train Epoch: 374 [135424/225000 (60%)] Loss: 6686.304688\n",
      "Train Epoch: 374 [139520/225000 (62%)] Loss: 6861.484375\n",
      "Train Epoch: 374 [143616/225000 (64%)] Loss: 6745.535156\n",
      "Train Epoch: 374 [147712/225000 (66%)] Loss: 6797.460938\n",
      "Train Epoch: 374 [151808/225000 (67%)] Loss: 6679.628906\n",
      "Train Epoch: 374 [155904/225000 (69%)] Loss: 6932.271484\n",
      "Train Epoch: 374 [160000/225000 (71%)] Loss: 6852.548828\n",
      "Train Epoch: 374 [164096/225000 (73%)] Loss: 6958.714844\n",
      "Train Epoch: 374 [168192/225000 (75%)] Loss: 6628.738281\n",
      "Train Epoch: 374 [172288/225000 (77%)] Loss: 6872.214844\n",
      "Train Epoch: 374 [176384/225000 (78%)] Loss: 6868.353516\n",
      "Train Epoch: 374 [180480/225000 (80%)] Loss: 6744.660156\n",
      "Train Epoch: 374 [184576/225000 (82%)] Loss: 6772.058594\n",
      "Train Epoch: 374 [188672/225000 (84%)] Loss: 6925.871094\n",
      "Train Epoch: 374 [192768/225000 (86%)] Loss: 6733.562500\n",
      "Train Epoch: 374 [196864/225000 (87%)] Loss: 6830.792969\n",
      "Train Epoch: 374 [200960/225000 (89%)] Loss: 6886.328125\n",
      "Train Epoch: 374 [205056/225000 (91%)] Loss: 6818.656250\n",
      "Train Epoch: 374 [209152/225000 (93%)] Loss: 6868.652344\n",
      "Train Epoch: 374 [213248/225000 (95%)] Loss: 6692.550781\n",
      "Train Epoch: 374 [217344/225000 (97%)] Loss: 6887.162109\n",
      "Train Epoch: 374 [221440/225000 (98%)] Loss: 6785.005859\n",
      "    epoch          : 374\n",
      "    loss           : 6872.5089379355095\n",
      "    val_loss       : 6950.249207936988\n",
      "Train Epoch: 375 [256/225000 (0%)] Loss: 6745.091797\n",
      "Train Epoch: 375 [4352/225000 (2%)] Loss: 6831.021484\n",
      "Train Epoch: 375 [8448/225000 (4%)] Loss: 6800.908203\n",
      "Train Epoch: 375 [12544/225000 (6%)] Loss: 6818.693359\n",
      "Train Epoch: 375 [16640/225000 (7%)] Loss: 6783.201172\n",
      "Train Epoch: 375 [20736/225000 (9%)] Loss: 6701.240234\n",
      "Train Epoch: 375 [24832/225000 (11%)] Loss: 6951.712891\n",
      "Train Epoch: 375 [28928/225000 (13%)] Loss: 6851.787109\n",
      "Train Epoch: 375 [33024/225000 (15%)] Loss: 6721.925781\n",
      "Train Epoch: 375 [37120/225000 (16%)] Loss: 6823.441406\n",
      "Train Epoch: 375 [41216/225000 (18%)] Loss: 6805.039062\n",
      "Train Epoch: 375 [45312/225000 (20%)] Loss: 6832.152344\n",
      "Train Epoch: 375 [49408/225000 (22%)] Loss: 6733.560547\n",
      "Train Epoch: 375 [53504/225000 (24%)] Loss: 6780.337891\n",
      "Train Epoch: 375 [57600/225000 (26%)] Loss: 6977.142578\n",
      "Train Epoch: 375 [61696/225000 (27%)] Loss: 6910.230469\n",
      "Train Epoch: 375 [65792/225000 (29%)] Loss: 6800.810547\n",
      "Train Epoch: 375 [69888/225000 (31%)] Loss: 6854.123047\n",
      "Train Epoch: 375 [73984/225000 (33%)] Loss: 6976.595703\n",
      "Train Epoch: 375 [78080/225000 (35%)] Loss: 6836.419922\n",
      "Train Epoch: 375 [82176/225000 (37%)] Loss: 6778.445312\n",
      "Train Epoch: 375 [86272/225000 (38%)] Loss: 6839.628906\n",
      "Train Epoch: 375 [90368/225000 (40%)] Loss: 6750.783203\n",
      "Train Epoch: 375 [94464/225000 (42%)] Loss: 6863.546875\n",
      "Train Epoch: 375 [98560/225000 (44%)] Loss: 6921.287109\n",
      "Train Epoch: 375 [102656/225000 (46%)] Loss: 6735.513672\n",
      "Train Epoch: 375 [106752/225000 (47%)] Loss: 6713.220703\n",
      "Train Epoch: 375 [110848/225000 (49%)] Loss: 6866.541016\n",
      "Train Epoch: 375 [114944/225000 (51%)] Loss: 6965.666016\n",
      "Train Epoch: 375 [119040/225000 (53%)] Loss: 6816.533203\n",
      "Train Epoch: 375 [123136/225000 (55%)] Loss: 6712.113281\n",
      "Train Epoch: 375 [127232/225000 (57%)] Loss: 6856.201172\n",
      "Train Epoch: 375 [131328/225000 (58%)] Loss: 6830.455078\n",
      "Train Epoch: 375 [135424/225000 (60%)] Loss: 6928.023438\n",
      "Train Epoch: 375 [139520/225000 (62%)] Loss: 6865.015625\n",
      "Train Epoch: 375 [143616/225000 (64%)] Loss: 6741.828125\n",
      "Train Epoch: 375 [147712/225000 (66%)] Loss: 6879.230469\n",
      "Train Epoch: 375 [151808/225000 (67%)] Loss: 6959.728516\n",
      "Train Epoch: 375 [155904/225000 (69%)] Loss: 6901.183594\n",
      "Train Epoch: 375 [160000/225000 (71%)] Loss: 6738.894531\n",
      "Train Epoch: 375 [164096/225000 (73%)] Loss: 6698.550781\n",
      "Train Epoch: 375 [168192/225000 (75%)] Loss: 6746.207031\n",
      "Train Epoch: 375 [172288/225000 (77%)] Loss: 6860.398438\n",
      "Train Epoch: 375 [176384/225000 (78%)] Loss: 6952.830078\n",
      "Train Epoch: 375 [180480/225000 (80%)] Loss: 6747.283203\n",
      "Train Epoch: 375 [184576/225000 (82%)] Loss: 6760.541016\n",
      "Train Epoch: 375 [188672/225000 (84%)] Loss: 6877.607422\n",
      "Train Epoch: 375 [192768/225000 (86%)] Loss: 6801.152344\n",
      "Train Epoch: 375 [196864/225000 (87%)] Loss: 6692.558594\n",
      "Train Epoch: 375 [200960/225000 (89%)] Loss: 6906.218750\n",
      "Train Epoch: 375 [205056/225000 (91%)] Loss: 6799.896484\n",
      "Train Epoch: 375 [209152/225000 (93%)] Loss: 6763.642578\n",
      "Train Epoch: 375 [213248/225000 (95%)] Loss: 6940.812500\n",
      "Train Epoch: 375 [217344/225000 (97%)] Loss: 6797.140625\n",
      "Train Epoch: 375 [221440/225000 (98%)] Loss: 6897.164062\n",
      "    epoch          : 375\n",
      "    loss           : 6854.296469487699\n",
      "    val_loss       : 6849.749779754756\n",
      "Train Epoch: 376 [256/225000 (0%)] Loss: 6842.441406\n",
      "Train Epoch: 376 [4352/225000 (2%)] Loss: 6672.525391\n",
      "Train Epoch: 376 [8448/225000 (4%)] Loss: 6793.751953\n",
      "Train Epoch: 376 [12544/225000 (6%)] Loss: 6810.419922\n",
      "Train Epoch: 376 [16640/225000 (7%)] Loss: 6928.226562\n",
      "Train Epoch: 376 [20736/225000 (9%)] Loss: 6759.578125\n",
      "Train Epoch: 376 [24832/225000 (11%)] Loss: 6799.701172\n",
      "Train Epoch: 376 [28928/225000 (13%)] Loss: 6837.074219\n",
      "Train Epoch: 376 [33024/225000 (15%)] Loss: 6879.406250\n",
      "Train Epoch: 376 [37120/225000 (16%)] Loss: 6866.527344\n",
      "Train Epoch: 376 [41216/225000 (18%)] Loss: 6884.681641\n",
      "Train Epoch: 376 [45312/225000 (20%)] Loss: 6935.525391\n",
      "Train Epoch: 376 [49408/225000 (22%)] Loss: 6847.912109\n",
      "Train Epoch: 376 [53504/225000 (24%)] Loss: 6885.152344\n",
      "Train Epoch: 376 [57600/225000 (26%)] Loss: 6675.712891\n",
      "Train Epoch: 376 [61696/225000 (27%)] Loss: 6725.115234\n",
      "Train Epoch: 376 [65792/225000 (29%)] Loss: 6840.023438\n",
      "Train Epoch: 376 [69888/225000 (31%)] Loss: 6879.097656\n",
      "Train Epoch: 376 [73984/225000 (33%)] Loss: 6950.054688\n",
      "Train Epoch: 376 [78080/225000 (35%)] Loss: 6883.818359\n",
      "Train Epoch: 376 [82176/225000 (37%)] Loss: 6857.410156\n",
      "Train Epoch: 376 [86272/225000 (38%)] Loss: 6956.931641\n",
      "Train Epoch: 376 [90368/225000 (40%)] Loss: 6867.185547\n",
      "Train Epoch: 376 [94464/225000 (42%)] Loss: 6860.060547\n",
      "Train Epoch: 376 [98560/225000 (44%)] Loss: 7001.333984\n",
      "Train Epoch: 376 [102656/225000 (46%)] Loss: 6702.712891\n",
      "Train Epoch: 376 [106752/225000 (47%)] Loss: 6751.833984\n",
      "Train Epoch: 376 [110848/225000 (49%)] Loss: 6954.500000\n",
      "Train Epoch: 376 [114944/225000 (51%)] Loss: 6828.982422\n",
      "Train Epoch: 376 [119040/225000 (53%)] Loss: 6848.726562\n",
      "Train Epoch: 376 [123136/225000 (55%)] Loss: 6832.005859\n",
      "Train Epoch: 376 [127232/225000 (57%)] Loss: 6730.591797\n",
      "Train Epoch: 376 [131328/225000 (58%)] Loss: 6755.677734\n",
      "Train Epoch: 376 [135424/225000 (60%)] Loss: 6739.205078\n",
      "Train Epoch: 376 [139520/225000 (62%)] Loss: 6855.062500\n",
      "Train Epoch: 376 [143616/225000 (64%)] Loss: 6748.253906\n",
      "Train Epoch: 376 [147712/225000 (66%)] Loss: 6951.822266\n",
      "Train Epoch: 376 [151808/225000 (67%)] Loss: 6891.929688\n",
      "Train Epoch: 376 [155904/225000 (69%)] Loss: 6861.441406\n",
      "Train Epoch: 376 [160000/225000 (71%)] Loss: 6681.177734\n",
      "Train Epoch: 376 [164096/225000 (73%)] Loss: 6838.939453\n",
      "Train Epoch: 376 [168192/225000 (75%)] Loss: 6794.638672\n",
      "Train Epoch: 376 [172288/225000 (77%)] Loss: 6858.603516\n",
      "Train Epoch: 376 [176384/225000 (78%)] Loss: 6853.871094\n",
      "Train Epoch: 376 [180480/225000 (80%)] Loss: 6860.812500\n",
      "Train Epoch: 376 [184576/225000 (82%)] Loss: 6677.269531\n",
      "Train Epoch: 376 [188672/225000 (84%)] Loss: 6661.144531\n",
      "Train Epoch: 376 [192768/225000 (86%)] Loss: 6654.019531\n",
      "Train Epoch: 376 [196864/225000 (87%)] Loss: 7001.210938\n",
      "Train Epoch: 376 [200960/225000 (89%)] Loss: 6980.552734\n",
      "Train Epoch: 376 [205056/225000 (91%)] Loss: 6885.230469\n",
      "Train Epoch: 376 [209152/225000 (93%)] Loss: 6862.541016\n",
      "Train Epoch: 376 [213248/225000 (95%)] Loss: 6775.292969\n",
      "Train Epoch: 376 [217344/225000 (97%)] Loss: 6801.458984\n",
      "Train Epoch: 376 [221440/225000 (98%)] Loss: 7087.843750\n",
      "    epoch          : 376\n",
      "    loss           : 6839.526743813994\n",
      "    val_loss       : 6848.69391662004\n",
      "Train Epoch: 377 [256/225000 (0%)] Loss: 6682.585938\n",
      "Train Epoch: 377 [4352/225000 (2%)] Loss: 6794.441406\n",
      "Train Epoch: 377 [8448/225000 (4%)] Loss: 6879.396484\n",
      "Train Epoch: 377 [12544/225000 (6%)] Loss: 6787.796875\n",
      "Train Epoch: 377 [16640/225000 (7%)] Loss: 6960.501953\n",
      "Train Epoch: 377 [20736/225000 (9%)] Loss: 6869.580078\n",
      "Train Epoch: 377 [24832/225000 (11%)] Loss: 6805.220703\n",
      "Train Epoch: 377 [28928/225000 (13%)] Loss: 6795.642578\n",
      "Train Epoch: 377 [33024/225000 (15%)] Loss: 6846.203125\n",
      "Train Epoch: 377 [37120/225000 (16%)] Loss: 6885.289062\n",
      "Train Epoch: 377 [41216/225000 (18%)] Loss: 6747.234375\n",
      "Train Epoch: 377 [45312/225000 (20%)] Loss: 6919.412109\n",
      "Train Epoch: 377 [49408/225000 (22%)] Loss: 6849.550781\n",
      "Train Epoch: 377 [53504/225000 (24%)] Loss: 6839.945312\n",
      "Train Epoch: 377 [57600/225000 (26%)] Loss: 6854.769531\n",
      "Train Epoch: 377 [61696/225000 (27%)] Loss: 6901.707031\n",
      "Train Epoch: 377 [65792/225000 (29%)] Loss: 6982.996094\n",
      "Train Epoch: 377 [69888/225000 (31%)] Loss: 6848.509766\n",
      "Train Epoch: 377 [73984/225000 (33%)] Loss: 6866.103516\n",
      "Train Epoch: 377 [78080/225000 (35%)] Loss: 6930.957031\n",
      "Train Epoch: 377 [82176/225000 (37%)] Loss: 6996.130859\n",
      "Train Epoch: 377 [86272/225000 (38%)] Loss: 7016.273438\n",
      "Train Epoch: 377 [90368/225000 (40%)] Loss: 6948.148438\n",
      "Train Epoch: 377 [94464/225000 (42%)] Loss: 6692.404297\n",
      "Train Epoch: 377 [98560/225000 (44%)] Loss: 6832.076172\n",
      "Train Epoch: 377 [102656/225000 (46%)] Loss: 6838.175781\n",
      "Train Epoch: 377 [106752/225000 (47%)] Loss: 6848.962891\n",
      "Train Epoch: 377 [110848/225000 (49%)] Loss: 6835.880859\n",
      "Train Epoch: 377 [114944/225000 (51%)] Loss: 6918.029297\n",
      "Train Epoch: 377 [119040/225000 (53%)] Loss: 6768.947266\n",
      "Train Epoch: 377 [123136/225000 (55%)] Loss: 6745.683594\n",
      "Train Epoch: 377 [127232/225000 (57%)] Loss: 6977.246094\n",
      "Train Epoch: 377 [131328/225000 (58%)] Loss: 6940.785156\n",
      "Train Epoch: 377 [135424/225000 (60%)] Loss: 6638.193359\n",
      "Train Epoch: 377 [139520/225000 (62%)] Loss: 6801.957031\n",
      "Train Epoch: 377 [143616/225000 (64%)] Loss: 6726.888672\n",
      "Train Epoch: 377 [147712/225000 (66%)] Loss: 7035.742188\n",
      "Train Epoch: 377 [151808/225000 (67%)] Loss: 6860.873047\n",
      "Train Epoch: 377 [155904/225000 (69%)] Loss: 6855.091797\n",
      "Train Epoch: 377 [160000/225000 (71%)] Loss: 6840.447266\n",
      "Train Epoch: 377 [164096/225000 (73%)] Loss: 6896.287109\n",
      "Train Epoch: 377 [168192/225000 (75%)] Loss: 6770.824219\n",
      "Train Epoch: 377 [172288/225000 (77%)] Loss: 6816.453125\n",
      "Train Epoch: 377 [176384/225000 (78%)] Loss: 6873.476562\n",
      "Train Epoch: 377 [180480/225000 (80%)] Loss: 6876.388672\n",
      "Train Epoch: 377 [184576/225000 (82%)] Loss: 6913.787109\n",
      "Train Epoch: 377 [188672/225000 (84%)] Loss: 6786.207031\n",
      "Train Epoch: 377 [192768/225000 (86%)] Loss: 6872.654297\n",
      "Train Epoch: 377 [196864/225000 (87%)] Loss: 6729.824219\n",
      "Train Epoch: 377 [200960/225000 (89%)] Loss: 6851.683594\n",
      "Train Epoch: 377 [205056/225000 (91%)] Loss: 6693.931641\n",
      "Train Epoch: 377 [209152/225000 (93%)] Loss: 6771.667969\n",
      "Train Epoch: 377 [213248/225000 (95%)] Loss: 6825.613281\n",
      "Train Epoch: 377 [217344/225000 (97%)] Loss: 6898.878906\n",
      "Train Epoch: 377 [221440/225000 (98%)] Loss: 6793.224609\n",
      "    epoch          : 377\n",
      "    loss           : 6831.822784458546\n",
      "    val_loss       : 6848.502690356605\n",
      "Train Epoch: 378 [256/225000 (0%)] Loss: 6840.804688\n",
      "Train Epoch: 378 [4352/225000 (2%)] Loss: 6681.619141\n",
      "Train Epoch: 378 [8448/225000 (4%)] Loss: 7138.896484\n",
      "Train Epoch: 378 [12544/225000 (6%)] Loss: 6768.599609\n",
      "Train Epoch: 378 [16640/225000 (7%)] Loss: 6772.970703\n",
      "Train Epoch: 378 [20736/225000 (9%)] Loss: 6925.027344\n",
      "Train Epoch: 378 [24832/225000 (11%)] Loss: 6811.896484\n",
      "Train Epoch: 378 [28928/225000 (13%)] Loss: 6785.703125\n",
      "Train Epoch: 378 [33024/225000 (15%)] Loss: 6832.773438\n",
      "Train Epoch: 378 [37120/225000 (16%)] Loss: 6733.525391\n",
      "Train Epoch: 378 [41216/225000 (18%)] Loss: 6870.867188\n",
      "Train Epoch: 378 [45312/225000 (20%)] Loss: 6916.158203\n",
      "Train Epoch: 378 [49408/225000 (22%)] Loss: 6697.259766\n",
      "Train Epoch: 378 [53504/225000 (24%)] Loss: 6726.349609\n",
      "Train Epoch: 378 [57600/225000 (26%)] Loss: 6785.195312\n",
      "Train Epoch: 378 [61696/225000 (27%)] Loss: 6718.574219\n",
      "Train Epoch: 378 [65792/225000 (29%)] Loss: 6779.820312\n",
      "Train Epoch: 378 [69888/225000 (31%)] Loss: 6751.191406\n",
      "Train Epoch: 378 [73984/225000 (33%)] Loss: 6696.412109\n",
      "Train Epoch: 378 [78080/225000 (35%)] Loss: 6904.910156\n",
      "Train Epoch: 378 [82176/225000 (37%)] Loss: 6742.685547\n",
      "Train Epoch: 378 [86272/225000 (38%)] Loss: 6761.578125\n",
      "Train Epoch: 378 [90368/225000 (40%)] Loss: 6934.066406\n",
      "Train Epoch: 378 [94464/225000 (42%)] Loss: 6821.781250\n",
      "Train Epoch: 378 [98560/225000 (44%)] Loss: 6796.074219\n",
      "Train Epoch: 378 [102656/225000 (46%)] Loss: 6810.388672\n",
      "Train Epoch: 378 [106752/225000 (47%)] Loss: 6822.554688\n",
      "Train Epoch: 378 [110848/225000 (49%)] Loss: 6799.656250\n",
      "Train Epoch: 378 [114944/225000 (51%)] Loss: 6912.750000\n",
      "Train Epoch: 378 [119040/225000 (53%)] Loss: 6782.148438\n",
      "Train Epoch: 378 [123136/225000 (55%)] Loss: 6709.869141\n",
      "Train Epoch: 378 [127232/225000 (57%)] Loss: 6911.380859\n",
      "Train Epoch: 378 [131328/225000 (58%)] Loss: 6783.005859\n",
      "Train Epoch: 378 [135424/225000 (60%)] Loss: 6688.039062\n",
      "Train Epoch: 378 [139520/225000 (62%)] Loss: 6936.498047\n",
      "Train Epoch: 378 [143616/225000 (64%)] Loss: 6877.308594\n",
      "Train Epoch: 378 [147712/225000 (66%)] Loss: 6778.447266\n",
      "Train Epoch: 378 [151808/225000 (67%)] Loss: 6802.826172\n",
      "Train Epoch: 378 [155904/225000 (69%)] Loss: 6930.802734\n",
      "Train Epoch: 378 [160000/225000 (71%)] Loss: 6798.132812\n",
      "Train Epoch: 378 [164096/225000 (73%)] Loss: 6905.484375\n",
      "Train Epoch: 378 [168192/225000 (75%)] Loss: 6937.037109\n",
      "Train Epoch: 378 [172288/225000 (77%)] Loss: 6921.121094\n",
      "Train Epoch: 378 [176384/225000 (78%)] Loss: 6588.251953\n",
      "Train Epoch: 378 [180480/225000 (80%)] Loss: 6938.107422\n",
      "Train Epoch: 378 [184576/225000 (82%)] Loss: 6748.185547\n",
      "Train Epoch: 378 [188672/225000 (84%)] Loss: 6819.482422\n",
      "Train Epoch: 378 [192768/225000 (86%)] Loss: 6959.683594\n",
      "Train Epoch: 378 [196864/225000 (87%)] Loss: 6914.816406\n",
      "Train Epoch: 378 [200960/225000 (89%)] Loss: 6783.546875\n",
      "Train Epoch: 378 [205056/225000 (91%)] Loss: 6825.542969\n",
      "Train Epoch: 378 [209152/225000 (93%)] Loss: 6730.806641\n",
      "Train Epoch: 378 [213248/225000 (95%)] Loss: 6882.798828\n",
      "Train Epoch: 378 [217344/225000 (97%)] Loss: 6754.037109\n",
      "Train Epoch: 378 [221440/225000 (98%)] Loss: 6898.681641\n",
      "    epoch          : 378\n",
      "    loss           : 6855.615950965231\n",
      "    val_loss       : 6847.977359035794\n",
      "Train Epoch: 379 [256/225000 (0%)] Loss: 6749.554688\n",
      "Train Epoch: 379 [4352/225000 (2%)] Loss: 6732.914062\n",
      "Train Epoch: 379 [8448/225000 (4%)] Loss: 6738.337891\n",
      "Train Epoch: 379 [12544/225000 (6%)] Loss: 6747.617188\n",
      "Train Epoch: 379 [16640/225000 (7%)] Loss: 6814.642578\n",
      "Train Epoch: 379 [20736/225000 (9%)] Loss: 6832.546875\n",
      "Train Epoch: 379 [24832/225000 (11%)] Loss: 6756.054688\n",
      "Train Epoch: 379 [28928/225000 (13%)] Loss: 6727.179688\n",
      "Train Epoch: 379 [33024/225000 (15%)] Loss: 6806.968750\n",
      "Train Epoch: 379 [37120/225000 (16%)] Loss: 6803.498047\n",
      "Train Epoch: 379 [41216/225000 (18%)] Loss: 6799.914062\n",
      "Train Epoch: 379 [45312/225000 (20%)] Loss: 6932.146484\n",
      "Train Epoch: 379 [49408/225000 (22%)] Loss: 6755.490234\n",
      "Train Epoch: 379 [53504/225000 (24%)] Loss: 6851.318359\n",
      "Train Epoch: 379 [57600/225000 (26%)] Loss: 6674.173828\n",
      "Train Epoch: 379 [61696/225000 (27%)] Loss: 6749.242188\n",
      "Train Epoch: 379 [65792/225000 (29%)] Loss: 6671.042969\n",
      "Train Epoch: 379 [69888/225000 (31%)] Loss: 6853.189453\n",
      "Train Epoch: 379 [73984/225000 (33%)] Loss: 6925.072266\n",
      "Train Epoch: 379 [78080/225000 (35%)] Loss: 6764.615234\n",
      "Train Epoch: 379 [82176/225000 (37%)] Loss: 6806.337891\n",
      "Train Epoch: 379 [86272/225000 (38%)] Loss: 6914.513672\n",
      "Train Epoch: 379 [90368/225000 (40%)] Loss: 6826.208984\n",
      "Train Epoch: 379 [94464/225000 (42%)] Loss: 6889.958984\n",
      "Train Epoch: 379 [98560/225000 (44%)] Loss: 6785.505859\n",
      "Train Epoch: 379 [102656/225000 (46%)] Loss: 6750.646484\n",
      "Train Epoch: 379 [106752/225000 (47%)] Loss: 6946.691406\n",
      "Train Epoch: 379 [110848/225000 (49%)] Loss: 6816.335938\n",
      "Train Epoch: 379 [114944/225000 (51%)] Loss: 6885.482422\n",
      "Train Epoch: 379 [119040/225000 (53%)] Loss: 6906.078125\n",
      "Train Epoch: 379 [123136/225000 (55%)] Loss: 6739.328125\n",
      "Train Epoch: 379 [127232/225000 (57%)] Loss: 6771.677734\n",
      "Train Epoch: 379 [131328/225000 (58%)] Loss: 6784.498047\n",
      "Train Epoch: 379 [135424/225000 (60%)] Loss: 6901.378906\n",
      "Train Epoch: 379 [139520/225000 (62%)] Loss: 6817.500000\n",
      "Train Epoch: 379 [143616/225000 (64%)] Loss: 6827.724609\n",
      "Train Epoch: 379 [147712/225000 (66%)] Loss: 6742.306641\n",
      "Train Epoch: 379 [151808/225000 (67%)] Loss: 6954.724609\n",
      "Train Epoch: 379 [155904/225000 (69%)] Loss: 6741.982422\n",
      "Train Epoch: 379 [160000/225000 (71%)] Loss: 6773.433594\n",
      "Train Epoch: 379 [164096/225000 (73%)] Loss: 6793.500000\n",
      "Train Epoch: 379 [168192/225000 (75%)] Loss: 6725.332031\n",
      "Train Epoch: 379 [172288/225000 (77%)] Loss: 6734.185547\n",
      "Train Epoch: 379 [176384/225000 (78%)] Loss: 6869.648438\n",
      "Train Epoch: 379 [180480/225000 (80%)] Loss: 6955.474609\n",
      "Train Epoch: 379 [184576/225000 (82%)] Loss: 7014.833984\n",
      "Train Epoch: 379 [188672/225000 (84%)] Loss: 6837.298828\n",
      "Train Epoch: 379 [192768/225000 (86%)] Loss: 6724.365234\n",
      "Train Epoch: 379 [196864/225000 (87%)] Loss: 6886.140625\n",
      "Train Epoch: 379 [200960/225000 (89%)] Loss: 6823.666016\n",
      "Train Epoch: 379 [205056/225000 (91%)] Loss: 6764.728516\n",
      "Train Epoch: 379 [209152/225000 (93%)] Loss: 6924.757812\n",
      "Train Epoch: 379 [213248/225000 (95%)] Loss: 6828.875000\n",
      "Train Epoch: 379 [217344/225000 (97%)] Loss: 6781.078125\n",
      "Train Epoch: 379 [221440/225000 (98%)] Loss: 6813.035156\n",
      "    epoch          : 379\n",
      "    loss           : 6835.439469789889\n",
      "    val_loss       : 6957.459639274344\n",
      "Train Epoch: 380 [256/225000 (0%)] Loss: 6828.210938\n",
      "Train Epoch: 380 [4352/225000 (2%)] Loss: 6707.277344\n",
      "Train Epoch: 380 [8448/225000 (4%)] Loss: 7009.023438\n",
      "Train Epoch: 380 [12544/225000 (6%)] Loss: 6811.283203\n",
      "Train Epoch: 380 [16640/225000 (7%)] Loss: 6727.787109\n",
      "Train Epoch: 380 [20736/225000 (9%)] Loss: 6916.746094\n",
      "Train Epoch: 380 [24832/225000 (11%)] Loss: 6756.730469\n",
      "Train Epoch: 380 [28928/225000 (13%)] Loss: 6765.712891\n",
      "Train Epoch: 380 [33024/225000 (15%)] Loss: 6900.082031\n",
      "Train Epoch: 380 [37120/225000 (16%)] Loss: 6667.240234\n",
      "Train Epoch: 380 [41216/225000 (18%)] Loss: 6691.970703\n",
      "Train Epoch: 380 [45312/225000 (20%)] Loss: 6878.406250\n",
      "Train Epoch: 380 [49408/225000 (22%)] Loss: 6716.773438\n",
      "Train Epoch: 380 [53504/225000 (24%)] Loss: 6785.705078\n",
      "Train Epoch: 380 [57600/225000 (26%)] Loss: 6840.894531\n",
      "Train Epoch: 380 [61696/225000 (27%)] Loss: 6846.660156\n",
      "Train Epoch: 380 [65792/225000 (29%)] Loss: 6939.675781\n",
      "Train Epoch: 380 [69888/225000 (31%)] Loss: 6837.625000\n",
      "Train Epoch: 380 [73984/225000 (33%)] Loss: 6941.929688\n",
      "Train Epoch: 380 [78080/225000 (35%)] Loss: 6853.361328\n",
      "Train Epoch: 380 [82176/225000 (37%)] Loss: 6857.441406\n",
      "Train Epoch: 380 [86272/225000 (38%)] Loss: 6869.558594\n",
      "Train Epoch: 380 [90368/225000 (40%)] Loss: 6852.681641\n",
      "Train Epoch: 380 [94464/225000 (42%)] Loss: 6887.789062\n",
      "Train Epoch: 380 [98560/225000 (44%)] Loss: 6932.474609\n",
      "Train Epoch: 380 [102656/225000 (46%)] Loss: 6772.404297\n",
      "Train Epoch: 380 [106752/225000 (47%)] Loss: 6698.230469\n",
      "Train Epoch: 380 [110848/225000 (49%)] Loss: 6939.826172\n",
      "Train Epoch: 380 [114944/225000 (51%)] Loss: 6767.033203\n",
      "Train Epoch: 380 [119040/225000 (53%)] Loss: 6839.000000\n",
      "Train Epoch: 380 [123136/225000 (55%)] Loss: 6663.730469\n",
      "Train Epoch: 380 [127232/225000 (57%)] Loss: 6799.029297\n",
      "Train Epoch: 380 [131328/225000 (58%)] Loss: 6758.742188\n",
      "Train Epoch: 380 [135424/225000 (60%)] Loss: 6883.718750\n",
      "Train Epoch: 380 [139520/225000 (62%)] Loss: 6808.095703\n",
      "Train Epoch: 380 [143616/225000 (64%)] Loss: 6808.583984\n",
      "Train Epoch: 380 [147712/225000 (66%)] Loss: 6874.980469\n",
      "Train Epoch: 380 [151808/225000 (67%)] Loss: 6974.941406\n",
      "Train Epoch: 380 [155904/225000 (69%)] Loss: 6811.339844\n",
      "Train Epoch: 380 [160000/225000 (71%)] Loss: 6810.425781\n",
      "Train Epoch: 380 [164096/225000 (73%)] Loss: 6793.505859\n",
      "Train Epoch: 380 [168192/225000 (75%)] Loss: 6798.419922\n",
      "Train Epoch: 380 [172288/225000 (77%)] Loss: 6785.234375\n",
      "Train Epoch: 380 [176384/225000 (78%)] Loss: 6921.251953\n",
      "Train Epoch: 380 [180480/225000 (80%)] Loss: 6853.675781\n",
      "Train Epoch: 380 [184576/225000 (82%)] Loss: 6682.742188\n",
      "Train Epoch: 380 [188672/225000 (84%)] Loss: 6844.242188\n",
      "Train Epoch: 380 [192768/225000 (86%)] Loss: 6862.917969\n",
      "Train Epoch: 380 [196864/225000 (87%)] Loss: 6789.943359\n",
      "Train Epoch: 380 [200960/225000 (89%)] Loss: 6826.404297\n",
      "Train Epoch: 380 [205056/225000 (91%)] Loss: 6928.587891\n",
      "Train Epoch: 380 [209152/225000 (93%)] Loss: 6785.822266\n",
      "Train Epoch: 380 [213248/225000 (95%)] Loss: 6800.232422\n",
      "Train Epoch: 380 [217344/225000 (97%)] Loss: 6854.873047\n",
      "Train Epoch: 380 [221440/225000 (98%)] Loss: 6749.402344\n",
      "    epoch          : 380\n",
      "    loss           : 6828.672887114263\n",
      "    val_loss       : 6842.035833777213\n",
      "Train Epoch: 381 [256/225000 (0%)] Loss: 6902.041016\n",
      "Train Epoch: 381 [4352/225000 (2%)] Loss: 6719.421875\n",
      "Train Epoch: 381 [8448/225000 (4%)] Loss: 6904.468750\n",
      "Train Epoch: 381 [12544/225000 (6%)] Loss: 6831.255859\n",
      "Train Epoch: 381 [16640/225000 (7%)] Loss: 6836.732422\n",
      "Train Epoch: 381 [20736/225000 (9%)] Loss: 6813.714844\n",
      "Train Epoch: 381 [24832/225000 (11%)] Loss: 6867.152344\n",
      "Train Epoch: 381 [28928/225000 (13%)] Loss: 6902.259766\n",
      "Train Epoch: 381 [33024/225000 (15%)] Loss: 6946.996094\n",
      "Train Epoch: 381 [37120/225000 (16%)] Loss: 6653.615234\n",
      "Train Epoch: 381 [41216/225000 (18%)] Loss: 6827.449219\n",
      "Train Epoch: 381 [45312/225000 (20%)] Loss: 6913.261719\n",
      "Train Epoch: 381 [49408/225000 (22%)] Loss: 6872.806641\n",
      "Train Epoch: 381 [53504/225000 (24%)] Loss: 6829.753906\n",
      "Train Epoch: 381 [57600/225000 (26%)] Loss: 6746.162109\n",
      "Train Epoch: 381 [61696/225000 (27%)] Loss: 7068.994141\n",
      "Train Epoch: 381 [65792/225000 (29%)] Loss: 6870.822266\n",
      "Train Epoch: 381 [69888/225000 (31%)] Loss: 6771.289062\n",
      "Train Epoch: 381 [73984/225000 (33%)] Loss: 6786.708984\n",
      "Train Epoch: 381 [78080/225000 (35%)] Loss: 6917.000000\n",
      "Train Epoch: 381 [82176/225000 (37%)] Loss: 6967.089844\n",
      "Train Epoch: 381 [86272/225000 (38%)] Loss: 6881.232422\n",
      "Train Epoch: 381 [90368/225000 (40%)] Loss: 6774.416016\n",
      "Train Epoch: 381 [94464/225000 (42%)] Loss: 6738.011719\n",
      "Train Epoch: 381 [98560/225000 (44%)] Loss: 6833.658203\n",
      "Train Epoch: 381 [102656/225000 (46%)] Loss: 6878.486328\n",
      "Train Epoch: 381 [106752/225000 (47%)] Loss: 6793.990234\n",
      "Train Epoch: 381 [110848/225000 (49%)] Loss: 6756.087891\n",
      "Train Epoch: 381 [114944/225000 (51%)] Loss: 6859.837891\n",
      "Train Epoch: 381 [119040/225000 (53%)] Loss: 6846.656250\n",
      "Train Epoch: 381 [123136/225000 (55%)] Loss: 6852.449219\n",
      "Train Epoch: 381 [127232/225000 (57%)] Loss: 6832.582031\n",
      "Train Epoch: 381 [131328/225000 (58%)] Loss: 6783.904297\n",
      "Train Epoch: 381 [135424/225000 (60%)] Loss: 6944.753906\n",
      "Train Epoch: 381 [139520/225000 (62%)] Loss: 6845.451172\n",
      "Train Epoch: 381 [143616/225000 (64%)] Loss: 6875.703125\n",
      "Train Epoch: 381 [147712/225000 (66%)] Loss: 6748.691406\n",
      "Train Epoch: 381 [151808/225000 (67%)] Loss: 6831.328125\n",
      "Train Epoch: 381 [155904/225000 (69%)] Loss: 6754.021484\n",
      "Train Epoch: 381 [160000/225000 (71%)] Loss: 6848.357422\n",
      "Train Epoch: 381 [164096/225000 (73%)] Loss: 6803.279297\n",
      "Train Epoch: 381 [168192/225000 (75%)] Loss: 6742.427734\n",
      "Train Epoch: 381 [172288/225000 (77%)] Loss: 6921.613281\n",
      "Train Epoch: 381 [176384/225000 (78%)] Loss: 6785.976562\n",
      "Train Epoch: 381 [180480/225000 (80%)] Loss: 6790.277344\n",
      "Train Epoch: 381 [184576/225000 (82%)] Loss: 6824.761719\n",
      "Train Epoch: 381 [188672/225000 (84%)] Loss: 6775.744141\n",
      "Train Epoch: 381 [192768/225000 (86%)] Loss: 6814.398438\n",
      "Train Epoch: 381 [196864/225000 (87%)] Loss: 6798.123047\n",
      "Train Epoch: 381 [200960/225000 (89%)] Loss: 6896.677734\n",
      "Train Epoch: 381 [205056/225000 (91%)] Loss: 6973.255859\n",
      "Train Epoch: 381 [209152/225000 (93%)] Loss: 6845.927734\n",
      "Train Epoch: 381 [213248/225000 (95%)] Loss: 6952.525391\n",
      "Train Epoch: 381 [217344/225000 (97%)] Loss: 6794.357422\n",
      "Train Epoch: 381 [221440/225000 (98%)] Loss: 6807.050781\n",
      "    epoch          : 381\n",
      "    loss           : 6846.020788893629\n",
      "    val_loss       : 6911.164532685766\n",
      "Train Epoch: 382 [256/225000 (0%)] Loss: 6873.279297\n",
      "Train Epoch: 382 [4352/225000 (2%)] Loss: 6777.521484\n",
      "Train Epoch: 382 [8448/225000 (4%)] Loss: 6867.982422\n",
      "Train Epoch: 382 [12544/225000 (6%)] Loss: 6809.646484\n",
      "Train Epoch: 382 [16640/225000 (7%)] Loss: 6844.314453\n",
      "Train Epoch: 382 [20736/225000 (9%)] Loss: 6739.259766\n",
      "Train Epoch: 382 [24832/225000 (11%)] Loss: 6951.718750\n",
      "Train Epoch: 382 [28928/225000 (13%)] Loss: 6815.898438\n",
      "Train Epoch: 382 [33024/225000 (15%)] Loss: 6741.945312\n",
      "Train Epoch: 382 [37120/225000 (16%)] Loss: 6790.320312\n",
      "Train Epoch: 382 [41216/225000 (18%)] Loss: 6881.574219\n",
      "Train Epoch: 382 [45312/225000 (20%)] Loss: 6845.361328\n",
      "Train Epoch: 382 [49408/225000 (22%)] Loss: 6791.722656\n",
      "Train Epoch: 382 [53504/225000 (24%)] Loss: 6854.000000\n",
      "Train Epoch: 382 [57600/225000 (26%)] Loss: 6859.554688\n",
      "Train Epoch: 382 [61696/225000 (27%)] Loss: 6897.439453\n",
      "Train Epoch: 382 [65792/225000 (29%)] Loss: 6797.339844\n",
      "Train Epoch: 382 [69888/225000 (31%)] Loss: 6781.892578\n",
      "Train Epoch: 382 [73984/225000 (33%)] Loss: 6856.496094\n",
      "Train Epoch: 382 [78080/225000 (35%)] Loss: 6763.953125\n",
      "Train Epoch: 382 [82176/225000 (37%)] Loss: 6907.357422\n",
      "Train Epoch: 382 [86272/225000 (38%)] Loss: 6820.324219\n",
      "Train Epoch: 382 [90368/225000 (40%)] Loss: 6749.679688\n",
      "Train Epoch: 382 [94464/225000 (42%)] Loss: 6889.949219\n",
      "Train Epoch: 382 [98560/225000 (44%)] Loss: 6747.332031\n",
      "Train Epoch: 382 [102656/225000 (46%)] Loss: 6956.787109\n",
      "Train Epoch: 382 [106752/225000 (47%)] Loss: 6770.406250\n",
      "Train Epoch: 382 [110848/225000 (49%)] Loss: 6842.207031\n",
      "Train Epoch: 382 [114944/225000 (51%)] Loss: 6883.457031\n",
      "Train Epoch: 382 [119040/225000 (53%)] Loss: 6956.623047\n",
      "Train Epoch: 382 [123136/225000 (55%)] Loss: 6864.070312\n",
      "Train Epoch: 382 [127232/225000 (57%)] Loss: 6982.951172\n",
      "Train Epoch: 382 [131328/225000 (58%)] Loss: 6782.800781\n",
      "Train Epoch: 382 [135424/225000 (60%)] Loss: 6864.386719\n",
      "Train Epoch: 382 [139520/225000 (62%)] Loss: 6902.431641\n",
      "Train Epoch: 382 [143616/225000 (64%)] Loss: 6919.892578\n",
      "Train Epoch: 382 [147712/225000 (66%)] Loss: 6815.728516\n",
      "Train Epoch: 382 [151808/225000 (67%)] Loss: 7033.511719\n",
      "Train Epoch: 382 [155904/225000 (69%)] Loss: 6699.380859\n",
      "Train Epoch: 382 [160000/225000 (71%)] Loss: 6753.923828\n",
      "Train Epoch: 382 [164096/225000 (73%)] Loss: 6850.552734\n",
      "Train Epoch: 382 [168192/225000 (75%)] Loss: 6674.615234\n",
      "Train Epoch: 382 [172288/225000 (77%)] Loss: 6778.398438\n",
      "Train Epoch: 382 [176384/225000 (78%)] Loss: 6842.105469\n",
      "Train Epoch: 382 [180480/225000 (80%)] Loss: 6800.277344\n",
      "Train Epoch: 382 [184576/225000 (82%)] Loss: 6709.119141\n",
      "Train Epoch: 382 [188672/225000 (84%)] Loss: 6819.587891\n",
      "Train Epoch: 382 [192768/225000 (86%)] Loss: 6851.302734\n",
      "Train Epoch: 382 [196864/225000 (87%)] Loss: 6959.402344\n",
      "Train Epoch: 382 [200960/225000 (89%)] Loss: 6803.541016\n",
      "Train Epoch: 382 [205056/225000 (91%)] Loss: 6913.183594\n",
      "Train Epoch: 382 [209152/225000 (93%)] Loss: 6871.791016\n",
      "Train Epoch: 382 [213248/225000 (95%)] Loss: 6790.259766\n",
      "Train Epoch: 382 [217344/225000 (97%)] Loss: 6800.458984\n",
      "Train Epoch: 382 [221440/225000 (98%)] Loss: 13616.320312\n",
      "    epoch          : 382\n",
      "    loss           : 6838.740822090088\n",
      "    val_loss       : 6895.558013893512\n",
      "Train Epoch: 383 [256/225000 (0%)] Loss: 6801.640625\n",
      "Train Epoch: 383 [4352/225000 (2%)] Loss: 6757.677734\n",
      "Train Epoch: 383 [8448/225000 (4%)] Loss: 6781.843750\n",
      "Train Epoch: 383 [12544/225000 (6%)] Loss: 6897.851562\n",
      "Train Epoch: 383 [16640/225000 (7%)] Loss: 6771.677734\n",
      "Train Epoch: 383 [20736/225000 (9%)] Loss: 6954.751953\n",
      "Train Epoch: 383 [24832/225000 (11%)] Loss: 6792.742188\n",
      "Train Epoch: 383 [28928/225000 (13%)] Loss: 6858.552734\n",
      "Train Epoch: 383 [33024/225000 (15%)] Loss: 6896.443359\n",
      "Train Epoch: 383 [37120/225000 (16%)] Loss: 6948.847656\n",
      "Train Epoch: 383 [41216/225000 (18%)] Loss: 6820.671875\n",
      "Train Epoch: 383 [45312/225000 (20%)] Loss: 6768.142578\n",
      "Train Epoch: 383 [49408/225000 (22%)] Loss: 6769.193359\n",
      "Train Epoch: 383 [53504/225000 (24%)] Loss: 6846.091797\n",
      "Train Epoch: 383 [57600/225000 (26%)] Loss: 6864.771484\n",
      "Train Epoch: 383 [61696/225000 (27%)] Loss: 6830.542969\n",
      "Train Epoch: 383 [65792/225000 (29%)] Loss: 6892.484375\n",
      "Train Epoch: 383 [69888/225000 (31%)] Loss: 6822.849609\n",
      "Train Epoch: 383 [73984/225000 (33%)] Loss: 6803.607422\n",
      "Train Epoch: 383 [78080/225000 (35%)] Loss: 6796.529297\n",
      "Train Epoch: 383 [82176/225000 (37%)] Loss: 6945.044922\n",
      "Train Epoch: 383 [86272/225000 (38%)] Loss: 6846.175781\n",
      "Train Epoch: 383 [90368/225000 (40%)] Loss: 6711.484375\n",
      "Train Epoch: 383 [94464/225000 (42%)] Loss: 6782.328125\n",
      "Train Epoch: 383 [98560/225000 (44%)] Loss: 6737.708984\n",
      "Train Epoch: 383 [102656/225000 (46%)] Loss: 6950.960938\n",
      "Train Epoch: 383 [106752/225000 (47%)] Loss: 6846.882812\n",
      "Train Epoch: 383 [110848/225000 (49%)] Loss: 6777.701172\n",
      "Train Epoch: 383 [114944/225000 (51%)] Loss: 6795.289062\n",
      "Train Epoch: 383 [119040/225000 (53%)] Loss: 6777.291016\n",
      "Train Epoch: 383 [123136/225000 (55%)] Loss: 6842.333984\n",
      "Train Epoch: 383 [127232/225000 (57%)] Loss: 6904.351562\n",
      "Train Epoch: 383 [131328/225000 (58%)] Loss: 6868.378906\n",
      "Train Epoch: 383 [135424/225000 (60%)] Loss: 6867.320312\n",
      "Train Epoch: 383 [139520/225000 (62%)] Loss: 6755.041016\n",
      "Train Epoch: 383 [143616/225000 (64%)] Loss: 6725.285156\n",
      "Train Epoch: 383 [147712/225000 (66%)] Loss: 6737.207031\n",
      "Train Epoch: 383 [151808/225000 (67%)] Loss: 6928.171875\n",
      "Train Epoch: 383 [155904/225000 (69%)] Loss: 6842.884766\n",
      "Train Epoch: 383 [160000/225000 (71%)] Loss: 6733.333984\n",
      "Train Epoch: 383 [164096/225000 (73%)] Loss: 6762.492188\n",
      "Train Epoch: 383 [168192/225000 (75%)] Loss: 6908.576172\n",
      "Train Epoch: 383 [172288/225000 (77%)] Loss: 6854.757812\n",
      "Train Epoch: 383 [176384/225000 (78%)] Loss: 6893.294922\n",
      "Train Epoch: 383 [180480/225000 (80%)] Loss: 6884.482422\n",
      "Train Epoch: 383 [184576/225000 (82%)] Loss: 6780.751953\n",
      "Train Epoch: 383 [188672/225000 (84%)] Loss: 6772.500000\n",
      "Train Epoch: 383 [192768/225000 (86%)] Loss: 6842.546875\n",
      "Train Epoch: 383 [196864/225000 (87%)] Loss: 6828.294922\n",
      "Train Epoch: 383 [200960/225000 (89%)] Loss: 6670.353516\n",
      "Train Epoch: 383 [205056/225000 (91%)] Loss: 6969.031250\n",
      "Train Epoch: 383 [209152/225000 (93%)] Loss: 6807.378906\n",
      "Train Epoch: 383 [213248/225000 (95%)] Loss: 6883.919922\n",
      "Train Epoch: 383 [217344/225000 (97%)] Loss: 6753.001953\n",
      "Train Epoch: 383 [221440/225000 (98%)] Loss: 6689.658203\n",
      "    epoch          : 383\n",
      "    loss           : 6852.277478180105\n",
      "    val_loss       : 6840.834167373424\n",
      "Train Epoch: 384 [256/225000 (0%)] Loss: 6890.259766\n",
      "Train Epoch: 384 [4352/225000 (2%)] Loss: 6859.123047\n",
      "Train Epoch: 384 [8448/225000 (4%)] Loss: 6743.529297\n",
      "Train Epoch: 384 [12544/225000 (6%)] Loss: 6892.371094\n",
      "Train Epoch: 384 [16640/225000 (7%)] Loss: 6847.167969\n",
      "Train Epoch: 384 [20736/225000 (9%)] Loss: 6753.101562\n",
      "Train Epoch: 384 [24832/225000 (11%)] Loss: 6846.816406\n",
      "Train Epoch: 384 [28928/225000 (13%)] Loss: 6723.535156\n",
      "Train Epoch: 384 [33024/225000 (15%)] Loss: 6849.517578\n",
      "Train Epoch: 384 [37120/225000 (16%)] Loss: 6916.898438\n",
      "Train Epoch: 384 [41216/225000 (18%)] Loss: 6813.736328\n",
      "Train Epoch: 384 [45312/225000 (20%)] Loss: 6822.386719\n",
      "Train Epoch: 384 [49408/225000 (22%)] Loss: 6759.593750\n",
      "Train Epoch: 384 [53504/225000 (24%)] Loss: 6738.296875\n",
      "Train Epoch: 384 [57600/225000 (26%)] Loss: 6822.021484\n",
      "Train Epoch: 384 [61696/225000 (27%)] Loss: 6950.646484\n",
      "Train Epoch: 384 [65792/225000 (29%)] Loss: 6789.222656\n",
      "Train Epoch: 384 [69888/225000 (31%)] Loss: 6713.988281\n",
      "Train Epoch: 384 [73984/225000 (33%)] Loss: 6863.357422\n",
      "Train Epoch: 384 [78080/225000 (35%)] Loss: 6689.730469\n",
      "Train Epoch: 384 [82176/225000 (37%)] Loss: 6656.029297\n",
      "Train Epoch: 384 [86272/225000 (38%)] Loss: 6744.568359\n",
      "Train Epoch: 384 [90368/225000 (40%)] Loss: 6785.359375\n",
      "Train Epoch: 384 [94464/225000 (42%)] Loss: 6722.685547\n",
      "Train Epoch: 384 [98560/225000 (44%)] Loss: 6873.253906\n",
      "Train Epoch: 384 [102656/225000 (46%)] Loss: 6790.925781\n",
      "Train Epoch: 384 [106752/225000 (47%)] Loss: 6891.851562\n",
      "Train Epoch: 384 [110848/225000 (49%)] Loss: 6766.671875\n",
      "Train Epoch: 384 [114944/225000 (51%)] Loss: 6832.042969\n",
      "Train Epoch: 384 [119040/225000 (53%)] Loss: 6891.742188\n",
      "Train Epoch: 384 [123136/225000 (55%)] Loss: 6835.259766\n",
      "Train Epoch: 384 [127232/225000 (57%)] Loss: 6876.755859\n",
      "Train Epoch: 384 [131328/225000 (58%)] Loss: 6880.884766\n",
      "Train Epoch: 384 [135424/225000 (60%)] Loss: 6735.953125\n",
      "Train Epoch: 384 [139520/225000 (62%)] Loss: 6942.128906\n",
      "Train Epoch: 384 [143616/225000 (64%)] Loss: 6868.640625\n",
      "Train Epoch: 384 [147712/225000 (66%)] Loss: 6747.582031\n",
      "Train Epoch: 384 [151808/225000 (67%)] Loss: 6735.367188\n",
      "Train Epoch: 384 [155904/225000 (69%)] Loss: 6849.121094\n",
      "Train Epoch: 384 [160000/225000 (71%)] Loss: 6716.097656\n",
      "Train Epoch: 384 [164096/225000 (73%)] Loss: 6873.332031\n",
      "Train Epoch: 384 [168192/225000 (75%)] Loss: 6877.025391\n",
      "Train Epoch: 384 [172288/225000 (77%)] Loss: 6932.728516\n",
      "Train Epoch: 384 [176384/225000 (78%)] Loss: 6810.798828\n",
      "Train Epoch: 384 [180480/225000 (80%)] Loss: 6816.195312\n",
      "Train Epoch: 384 [184576/225000 (82%)] Loss: 6929.189453\n",
      "Train Epoch: 384 [188672/225000 (84%)] Loss: 6960.806641\n",
      "Train Epoch: 384 [192768/225000 (86%)] Loss: 6948.267578\n",
      "Train Epoch: 384 [196864/225000 (87%)] Loss: 7073.222656\n",
      "Train Epoch: 384 [200960/225000 (89%)] Loss: 6774.812500\n",
      "Train Epoch: 384 [205056/225000 (91%)] Loss: 6761.673828\n",
      "Train Epoch: 384 [209152/225000 (93%)] Loss: 6806.896484\n",
      "Train Epoch: 384 [213248/225000 (95%)] Loss: 6759.175781\n",
      "Train Epoch: 384 [217344/225000 (97%)] Loss: 6772.500000\n",
      "Train Epoch: 384 [221440/225000 (98%)] Loss: 6779.058594\n",
      "    epoch          : 384\n",
      "    loss           : 6833.92013962955\n",
      "    val_loss       : 6838.251586111224\n",
      "Train Epoch: 385 [256/225000 (0%)] Loss: 6901.496094\n",
      "Train Epoch: 385 [4352/225000 (2%)] Loss: 6738.425781\n",
      "Train Epoch: 385 [8448/225000 (4%)] Loss: 6938.992188\n",
      "Train Epoch: 385 [12544/225000 (6%)] Loss: 6689.984375\n",
      "Train Epoch: 385 [16640/225000 (7%)] Loss: 6754.244141\n",
      "Train Epoch: 385 [20736/225000 (9%)] Loss: 6846.882812\n",
      "Train Epoch: 385 [24832/225000 (11%)] Loss: 6772.263672\n",
      "Train Epoch: 385 [28928/225000 (13%)] Loss: 6985.751953\n",
      "Train Epoch: 385 [33024/225000 (15%)] Loss: 6780.160156\n",
      "Train Epoch: 385 [37120/225000 (16%)] Loss: 6737.750000\n",
      "Train Epoch: 385 [41216/225000 (18%)] Loss: 6814.396484\n",
      "Train Epoch: 385 [45312/225000 (20%)] Loss: 6861.242188\n",
      "Train Epoch: 385 [49408/225000 (22%)] Loss: 6784.808594\n",
      "Train Epoch: 385 [53504/225000 (24%)] Loss: 6843.236328\n",
      "Train Epoch: 385 [57600/225000 (26%)] Loss: 6665.296875\n",
      "Train Epoch: 385 [61696/225000 (27%)] Loss: 6898.052734\n",
      "Train Epoch: 385 [65792/225000 (29%)] Loss: 7008.166016\n",
      "Train Epoch: 385 [69888/225000 (31%)] Loss: 6842.277344\n",
      "Train Epoch: 385 [73984/225000 (33%)] Loss: 6879.296875\n",
      "Train Epoch: 385 [78080/225000 (35%)] Loss: 7000.949219\n",
      "Train Epoch: 385 [82176/225000 (37%)] Loss: 6828.220703\n",
      "Train Epoch: 385 [86272/225000 (38%)] Loss: 7026.248047\n",
      "Train Epoch: 385 [90368/225000 (40%)] Loss: 6832.212891\n",
      "Train Epoch: 385 [94464/225000 (42%)] Loss: 6788.675781\n",
      "Train Epoch: 385 [98560/225000 (44%)] Loss: 6705.003906\n",
      "Train Epoch: 385 [102656/225000 (46%)] Loss: 6857.574219\n",
      "Train Epoch: 385 [106752/225000 (47%)] Loss: 6771.224609\n",
      "Train Epoch: 385 [110848/225000 (49%)] Loss: 6789.044922\n",
      "Train Epoch: 385 [114944/225000 (51%)] Loss: 6790.136719\n",
      "Train Epoch: 385 [119040/225000 (53%)] Loss: 6893.076172\n",
      "Train Epoch: 385 [123136/225000 (55%)] Loss: 6913.119141\n",
      "Train Epoch: 385 [127232/225000 (57%)] Loss: 6800.875000\n",
      "Train Epoch: 385 [131328/225000 (58%)] Loss: 6866.992188\n",
      "Train Epoch: 385 [135424/225000 (60%)] Loss: 6871.695312\n",
      "Train Epoch: 385 [139520/225000 (62%)] Loss: 6930.810547\n",
      "Train Epoch: 385 [143616/225000 (64%)] Loss: 6829.134766\n",
      "Train Epoch: 385 [147712/225000 (66%)] Loss: 6626.841797\n",
      "Train Epoch: 385 [151808/225000 (67%)] Loss: 6850.726562\n",
      "Train Epoch: 385 [155904/225000 (69%)] Loss: 6751.968750\n",
      "Train Epoch: 385 [160000/225000 (71%)] Loss: 6892.625000\n",
      "Train Epoch: 385 [164096/225000 (73%)] Loss: 6849.976562\n",
      "Train Epoch: 385 [168192/225000 (75%)] Loss: 6755.093750\n",
      "Train Epoch: 385 [172288/225000 (77%)] Loss: 6853.105469\n",
      "Train Epoch: 385 [176384/225000 (78%)] Loss: 6847.808594\n",
      "Train Epoch: 385 [180480/225000 (80%)] Loss: 6684.837891\n",
      "Train Epoch: 385 [184576/225000 (82%)] Loss: 6660.828125\n",
      "Train Epoch: 385 [188672/225000 (84%)] Loss: 6894.363281\n",
      "Train Epoch: 385 [192768/225000 (86%)] Loss: 6852.548828\n",
      "Train Epoch: 385 [196864/225000 (87%)] Loss: 6838.789062\n",
      "Train Epoch: 385 [200960/225000 (89%)] Loss: 6885.083984\n",
      "Train Epoch: 385 [205056/225000 (91%)] Loss: 6752.759766\n",
      "Train Epoch: 385 [209152/225000 (93%)] Loss: 6833.046875\n",
      "Train Epoch: 385 [213248/225000 (95%)] Loss: 6826.509766\n",
      "Train Epoch: 385 [217344/225000 (97%)] Loss: 6829.164062\n",
      "Train Epoch: 385 [221440/225000 (98%)] Loss: 6834.132812\n",
      "    epoch          : 385\n",
      "    loss           : 6841.201180762941\n",
      "    val_loss       : 6840.9531174192625\n",
      "Train Epoch: 386 [256/225000 (0%)] Loss: 6784.154297\n",
      "Train Epoch: 386 [4352/225000 (2%)] Loss: 6841.652344\n",
      "Train Epoch: 386 [8448/225000 (4%)] Loss: 6733.539062\n",
      "Train Epoch: 386 [12544/225000 (6%)] Loss: 6815.636719\n",
      "Train Epoch: 386 [16640/225000 (7%)] Loss: 6926.150391\n",
      "Train Epoch: 386 [20736/225000 (9%)] Loss: 6955.296875\n",
      "Train Epoch: 386 [24832/225000 (11%)] Loss: 6865.197266\n",
      "Train Epoch: 386 [28928/225000 (13%)] Loss: 6755.183594\n",
      "Train Epoch: 386 [33024/225000 (15%)] Loss: 6857.099609\n",
      "Train Epoch: 386 [37120/225000 (16%)] Loss: 6909.593750\n",
      "Train Epoch: 386 [41216/225000 (18%)] Loss: 6888.191406\n",
      "Train Epoch: 386 [45312/225000 (20%)] Loss: 6753.734375\n",
      "Train Epoch: 386 [49408/225000 (22%)] Loss: 6861.076172\n",
      "Train Epoch: 386 [53504/225000 (24%)] Loss: 6763.988281\n",
      "Train Epoch: 386 [57600/225000 (26%)] Loss: 6879.087891\n",
      "Train Epoch: 386 [61696/225000 (27%)] Loss: 6787.216797\n",
      "Train Epoch: 386 [65792/225000 (29%)] Loss: 6840.740234\n",
      "Train Epoch: 386 [69888/225000 (31%)] Loss: 6891.439453\n",
      "Train Epoch: 386 [73984/225000 (33%)] Loss: 6848.566406\n",
      "Train Epoch: 386 [78080/225000 (35%)] Loss: 6675.367188\n",
      "Train Epoch: 386 [82176/225000 (37%)] Loss: 7049.855469\n",
      "Train Epoch: 386 [86272/225000 (38%)] Loss: 6686.035156\n",
      "Train Epoch: 386 [90368/225000 (40%)] Loss: 6822.869141\n",
      "Train Epoch: 386 [94464/225000 (42%)] Loss: 7051.830078\n",
      "Train Epoch: 386 [98560/225000 (44%)] Loss: 6802.865234\n",
      "Train Epoch: 386 [102656/225000 (46%)] Loss: 6777.220703\n",
      "Train Epoch: 386 [106752/225000 (47%)] Loss: 6731.728516\n",
      "Train Epoch: 386 [110848/225000 (49%)] Loss: 6966.119141\n",
      "Train Epoch: 386 [114944/225000 (51%)] Loss: 6705.091797\n",
      "Train Epoch: 386 [119040/225000 (53%)] Loss: 6948.496094\n",
      "Train Epoch: 386 [123136/225000 (55%)] Loss: 6655.236328\n",
      "Train Epoch: 386 [127232/225000 (57%)] Loss: 7031.363281\n",
      "Train Epoch: 386 [131328/225000 (58%)] Loss: 6865.572266\n",
      "Train Epoch: 386 [135424/225000 (60%)] Loss: 6784.185547\n",
      "Train Epoch: 386 [139520/225000 (62%)] Loss: 6615.207031\n",
      "Train Epoch: 386 [143616/225000 (64%)] Loss: 6754.345703\n",
      "Train Epoch: 386 [147712/225000 (66%)] Loss: 6730.544922\n",
      "Train Epoch: 386 [151808/225000 (67%)] Loss: 6708.753906\n",
      "Train Epoch: 386 [155904/225000 (69%)] Loss: 6808.937500\n",
      "Train Epoch: 386 [160000/225000 (71%)] Loss: 6635.904297\n",
      "Train Epoch: 386 [164096/225000 (73%)] Loss: 6698.406250\n",
      "Train Epoch: 386 [168192/225000 (75%)] Loss: 6836.718750\n",
      "Train Epoch: 386 [172288/225000 (77%)] Loss: 6759.152344\n",
      "Train Epoch: 386 [176384/225000 (78%)] Loss: 6843.964844\n",
      "Train Epoch: 386 [180480/225000 (80%)] Loss: 6836.625000\n",
      "Train Epoch: 386 [184576/225000 (82%)] Loss: 6765.941406\n",
      "Train Epoch: 386 [188672/225000 (84%)] Loss: 6720.986328\n",
      "Train Epoch: 386 [192768/225000 (86%)] Loss: 6969.494141\n",
      "Train Epoch: 386 [196864/225000 (87%)] Loss: 6874.724609\n",
      "Train Epoch: 386 [200960/225000 (89%)] Loss: 6786.433594\n",
      "Train Epoch: 386 [205056/225000 (91%)] Loss: 17648.945312\n",
      "Train Epoch: 386 [209152/225000 (93%)] Loss: 6884.880859\n",
      "Train Epoch: 386 [213248/225000 (95%)] Loss: 6876.593750\n",
      "Train Epoch: 386 [217344/225000 (97%)] Loss: 6884.095703\n",
      "Train Epoch: 386 [221440/225000 (98%)] Loss: 6833.271484\n",
      "    epoch          : 386\n",
      "    loss           : 6845.661030601181\n",
      "    val_loss       : 6900.81620877373\n",
      "Train Epoch: 387 [256/225000 (0%)] Loss: 6737.667969\n",
      "Train Epoch: 387 [4352/225000 (2%)] Loss: 6793.312500\n",
      "Train Epoch: 387 [8448/225000 (4%)] Loss: 6788.929688\n",
      "Train Epoch: 387 [12544/225000 (6%)] Loss: 6837.416016\n",
      "Train Epoch: 387 [16640/225000 (7%)] Loss: 6983.837891\n",
      "Train Epoch: 387 [20736/225000 (9%)] Loss: 6932.316406\n",
      "Train Epoch: 387 [24832/225000 (11%)] Loss: 6875.089844\n",
      "Train Epoch: 387 [28928/225000 (13%)] Loss: 6774.765625\n",
      "Train Epoch: 387 [33024/225000 (15%)] Loss: 6653.300781\n",
      "Train Epoch: 387 [37120/225000 (16%)] Loss: 6683.773438\n",
      "Train Epoch: 387 [41216/225000 (18%)] Loss: 6851.771484\n",
      "Train Epoch: 387 [45312/225000 (20%)] Loss: 6804.697266\n",
      "Train Epoch: 387 [49408/225000 (22%)] Loss: 6774.312500\n",
      "Train Epoch: 387 [53504/225000 (24%)] Loss: 6800.111328\n",
      "Train Epoch: 387 [57600/225000 (26%)] Loss: 6934.341797\n",
      "Train Epoch: 387 [61696/225000 (27%)] Loss: 7007.417969\n",
      "Train Epoch: 387 [65792/225000 (29%)] Loss: 6953.994141\n",
      "Train Epoch: 387 [69888/225000 (31%)] Loss: 6846.226562\n",
      "Train Epoch: 387 [73984/225000 (33%)] Loss: 6713.283203\n",
      "Train Epoch: 387 [78080/225000 (35%)] Loss: 6913.240234\n",
      "Train Epoch: 387 [82176/225000 (37%)] Loss: 6837.345703\n",
      "Train Epoch: 387 [86272/225000 (38%)] Loss: 6842.769531\n",
      "Train Epoch: 387 [90368/225000 (40%)] Loss: 6647.140625\n",
      "Train Epoch: 387 [94464/225000 (42%)] Loss: 6847.484375\n",
      "Train Epoch: 387 [98560/225000 (44%)] Loss: 6958.494141\n",
      "Train Epoch: 387 [102656/225000 (46%)] Loss: 6648.609375\n",
      "Train Epoch: 387 [106752/225000 (47%)] Loss: 6872.359375\n",
      "Train Epoch: 387 [110848/225000 (49%)] Loss: 6776.992188\n",
      "Train Epoch: 387 [114944/225000 (51%)] Loss: 6708.042969\n",
      "Train Epoch: 387 [119040/225000 (53%)] Loss: 7057.023438\n",
      "Train Epoch: 387 [123136/225000 (55%)] Loss: 6688.722656\n",
      "Train Epoch: 387 [127232/225000 (57%)] Loss: 6888.037109\n",
      "Train Epoch: 387 [131328/225000 (58%)] Loss: 6944.691406\n",
      "Train Epoch: 387 [135424/225000 (60%)] Loss: 6845.988281\n",
      "Train Epoch: 387 [139520/225000 (62%)] Loss: 6815.894531\n",
      "Train Epoch: 387 [143616/225000 (64%)] Loss: 6762.398438\n",
      "Train Epoch: 387 [147712/225000 (66%)] Loss: 6755.673828\n",
      "Train Epoch: 387 [151808/225000 (67%)] Loss: 6779.103516\n",
      "Train Epoch: 387 [155904/225000 (69%)] Loss: 6871.042969\n",
      "Train Epoch: 387 [160000/225000 (71%)] Loss: 6870.816406\n",
      "Train Epoch: 387 [164096/225000 (73%)] Loss: 6799.259766\n",
      "Train Epoch: 387 [168192/225000 (75%)] Loss: 6873.556641\n",
      "Train Epoch: 387 [172288/225000 (77%)] Loss: 6895.251953\n",
      "Train Epoch: 387 [176384/225000 (78%)] Loss: 6818.138672\n",
      "Train Epoch: 387 [180480/225000 (80%)] Loss: 6860.636719\n",
      "Train Epoch: 387 [184576/225000 (82%)] Loss: 6757.593750\n",
      "Train Epoch: 387 [188672/225000 (84%)] Loss: 6939.669922\n",
      "Train Epoch: 387 [192768/225000 (86%)] Loss: 6810.978516\n",
      "Train Epoch: 387 [196864/225000 (87%)] Loss: 6749.066406\n",
      "Train Epoch: 387 [200960/225000 (89%)] Loss: 7030.720703\n",
      "Train Epoch: 387 [205056/225000 (91%)] Loss: 6915.558594\n",
      "Train Epoch: 387 [209152/225000 (93%)] Loss: 6708.638672\n",
      "Train Epoch: 387 [213248/225000 (95%)] Loss: 6737.916016\n",
      "Train Epoch: 387 [217344/225000 (97%)] Loss: 6801.339844\n",
      "Train Epoch: 387 [221440/225000 (98%)] Loss: 6820.791016\n",
      "    epoch          : 387\n",
      "    loss           : 6824.765998293516\n",
      "    val_loss       : 6838.144294905419\n",
      "Train Epoch: 388 [256/225000 (0%)] Loss: 6837.595703\n",
      "Train Epoch: 388 [4352/225000 (2%)] Loss: 6777.796875\n",
      "Train Epoch: 388 [8448/225000 (4%)] Loss: 6800.376953\n",
      "Train Epoch: 388 [12544/225000 (6%)] Loss: 6751.746094\n",
      "Train Epoch: 388 [16640/225000 (7%)] Loss: 6754.613281\n",
      "Train Epoch: 388 [20736/225000 (9%)] Loss: 6737.765625\n",
      "Train Epoch: 388 [24832/225000 (11%)] Loss: 6945.470703\n",
      "Train Epoch: 388 [28928/225000 (13%)] Loss: 6655.437500\n",
      "Train Epoch: 388 [33024/225000 (15%)] Loss: 6717.244141\n",
      "Train Epoch: 388 [37120/225000 (16%)] Loss: 6887.216797\n",
      "Train Epoch: 388 [41216/225000 (18%)] Loss: 6670.527344\n",
      "Train Epoch: 388 [45312/225000 (20%)] Loss: 6962.732422\n",
      "Train Epoch: 388 [49408/225000 (22%)] Loss: 6896.195312\n",
      "Train Epoch: 388 [53504/225000 (24%)] Loss: 6949.632812\n",
      "Train Epoch: 388 [57600/225000 (26%)] Loss: 6953.062500\n",
      "Train Epoch: 388 [61696/225000 (27%)] Loss: 6884.937500\n",
      "Train Epoch: 388 [65792/225000 (29%)] Loss: 6788.587891\n",
      "Train Epoch: 388 [69888/225000 (31%)] Loss: 6786.449219\n",
      "Train Epoch: 388 [73984/225000 (33%)] Loss: 6801.623047\n",
      "Train Epoch: 388 [78080/225000 (35%)] Loss: 6775.029297\n",
      "Train Epoch: 388 [82176/225000 (37%)] Loss: 6924.628906\n",
      "Train Epoch: 388 [86272/225000 (38%)] Loss: 6906.195312\n",
      "Train Epoch: 388 [90368/225000 (40%)] Loss: 6929.667969\n",
      "Train Epoch: 388 [94464/225000 (42%)] Loss: 6855.771484\n",
      "Train Epoch: 388 [98560/225000 (44%)] Loss: 6801.890625\n",
      "Train Epoch: 388 [102656/225000 (46%)] Loss: 6709.929688\n",
      "Train Epoch: 388 [106752/225000 (47%)] Loss: 6782.101562\n",
      "Train Epoch: 388 [110848/225000 (49%)] Loss: 6867.191406\n",
      "Train Epoch: 388 [114944/225000 (51%)] Loss: 6800.294922\n",
      "Train Epoch: 388 [119040/225000 (53%)] Loss: 6876.693359\n",
      "Train Epoch: 388 [123136/225000 (55%)] Loss: 6845.246094\n",
      "Train Epoch: 388 [127232/225000 (57%)] Loss: 6918.333984\n",
      "Train Epoch: 388 [131328/225000 (58%)] Loss: 6901.205078\n",
      "Train Epoch: 388 [135424/225000 (60%)] Loss: 6831.708984\n",
      "Train Epoch: 388 [139520/225000 (62%)] Loss: 6880.240234\n",
      "Train Epoch: 388 [143616/225000 (64%)] Loss: 6842.652344\n",
      "Train Epoch: 388 [147712/225000 (66%)] Loss: 6812.470703\n",
      "Train Epoch: 388 [151808/225000 (67%)] Loss: 7009.599609\n",
      "Train Epoch: 388 [155904/225000 (69%)] Loss: 6793.308594\n",
      "Train Epoch: 388 [160000/225000 (71%)] Loss: 7001.574219\n",
      "Train Epoch: 388 [164096/225000 (73%)] Loss: 6863.968750\n",
      "Train Epoch: 388 [168192/225000 (75%)] Loss: 6930.175781\n",
      "Train Epoch: 388 [172288/225000 (77%)] Loss: 6750.677734\n",
      "Train Epoch: 388 [176384/225000 (78%)] Loss: 6801.271484\n",
      "Train Epoch: 388 [180480/225000 (80%)] Loss: 6771.744141\n",
      "Train Epoch: 388 [184576/225000 (82%)] Loss: 6893.400391\n",
      "Train Epoch: 388 [188672/225000 (84%)] Loss: 6757.113281\n",
      "Train Epoch: 388 [192768/225000 (86%)] Loss: 6723.269531\n",
      "Train Epoch: 388 [196864/225000 (87%)] Loss: 6829.753906\n",
      "Train Epoch: 388 [200960/225000 (89%)] Loss: 6824.615234\n",
      "Train Epoch: 388 [205056/225000 (91%)] Loss: 6851.218750\n",
      "Train Epoch: 388 [209152/225000 (93%)] Loss: 6810.197266\n",
      "Train Epoch: 388 [213248/225000 (95%)] Loss: 6885.216797\n",
      "Train Epoch: 388 [217344/225000 (97%)] Loss: 6678.707031\n",
      "Train Epoch: 388 [221440/225000 (98%)] Loss: 6740.195312\n",
      "    epoch          : 388\n",
      "    loss           : 6855.0748820125855\n",
      "    val_loss       : 6837.524726405436\n",
      "Train Epoch: 389 [256/225000 (0%)] Loss: 6778.998047\n",
      "Train Epoch: 389 [4352/225000 (2%)] Loss: 6752.699219\n",
      "Train Epoch: 389 [8448/225000 (4%)] Loss: 6775.755859\n",
      "Train Epoch: 389 [12544/225000 (6%)] Loss: 6770.910156\n",
      "Train Epoch: 389 [16640/225000 (7%)] Loss: 6714.013672\n",
      "Train Epoch: 389 [20736/225000 (9%)] Loss: 6728.996094\n",
      "Train Epoch: 389 [24832/225000 (11%)] Loss: 6917.591797\n",
      "Train Epoch: 389 [28928/225000 (13%)] Loss: 6936.265625\n",
      "Train Epoch: 389 [33024/225000 (15%)] Loss: 6835.453125\n",
      "Train Epoch: 389 [37120/225000 (16%)] Loss: 6926.298828\n",
      "Train Epoch: 389 [41216/225000 (18%)] Loss: 6890.228516\n",
      "Train Epoch: 389 [45312/225000 (20%)] Loss: 6683.837891\n",
      "Train Epoch: 389 [49408/225000 (22%)] Loss: 6736.218750\n",
      "Train Epoch: 389 [53504/225000 (24%)] Loss: 6766.355469\n",
      "Train Epoch: 389 [57600/225000 (26%)] Loss: 6781.632812\n",
      "Train Epoch: 389 [61696/225000 (27%)] Loss: 6864.337891\n",
      "Train Epoch: 389 [65792/225000 (29%)] Loss: 6830.580078\n",
      "Train Epoch: 389 [69888/225000 (31%)] Loss: 6787.156250\n",
      "Train Epoch: 389 [73984/225000 (33%)] Loss: 6843.734375\n",
      "Train Epoch: 389 [78080/225000 (35%)] Loss: 6743.193359\n",
      "Train Epoch: 389 [82176/225000 (37%)] Loss: 6631.455078\n",
      "Train Epoch: 389 [86272/225000 (38%)] Loss: 6851.628906\n",
      "Train Epoch: 389 [90368/225000 (40%)] Loss: 6936.306641\n",
      "Train Epoch: 389 [94464/225000 (42%)] Loss: 6767.193359\n",
      "Train Epoch: 389 [98560/225000 (44%)] Loss: 6900.490234\n",
      "Train Epoch: 389 [102656/225000 (46%)] Loss: 6734.697266\n",
      "Train Epoch: 389 [106752/225000 (47%)] Loss: 6816.380859\n",
      "Train Epoch: 389 [110848/225000 (49%)] Loss: 6741.003906\n",
      "Train Epoch: 389 [114944/225000 (51%)] Loss: 6830.103516\n",
      "Train Epoch: 389 [119040/225000 (53%)] Loss: 6800.714844\n",
      "Train Epoch: 389 [123136/225000 (55%)] Loss: 6888.621094\n",
      "Train Epoch: 389 [127232/225000 (57%)] Loss: 6788.929688\n",
      "Train Epoch: 389 [131328/225000 (58%)] Loss: 6763.822266\n",
      "Train Epoch: 389 [135424/225000 (60%)] Loss: 6856.169922\n",
      "Train Epoch: 389 [139520/225000 (62%)] Loss: 6761.851562\n",
      "Train Epoch: 389 [143616/225000 (64%)] Loss: 6747.494141\n",
      "Train Epoch: 389 [147712/225000 (66%)] Loss: 6860.937500\n",
      "Train Epoch: 389 [151808/225000 (67%)] Loss: 6803.855469\n",
      "Train Epoch: 389 [155904/225000 (69%)] Loss: 6979.058594\n",
      "Train Epoch: 389 [160000/225000 (71%)] Loss: 6912.320312\n",
      "Train Epoch: 389 [164096/225000 (73%)] Loss: 6897.746094\n",
      "Train Epoch: 389 [168192/225000 (75%)] Loss: 6848.460938\n",
      "Train Epoch: 389 [172288/225000 (77%)] Loss: 6692.623047\n",
      "Train Epoch: 389 [176384/225000 (78%)] Loss: 6704.341797\n",
      "Train Epoch: 389 [180480/225000 (80%)] Loss: 6790.958984\n",
      "Train Epoch: 389 [184576/225000 (82%)] Loss: 7052.382812\n",
      "Train Epoch: 389 [188672/225000 (84%)] Loss: 6961.138672\n",
      "Train Epoch: 389 [192768/225000 (86%)] Loss: 6839.837891\n",
      "Train Epoch: 389 [196864/225000 (87%)] Loss: 6843.841797\n",
      "Train Epoch: 389 [200960/225000 (89%)] Loss: 6879.589844\n",
      "Train Epoch: 389 [205056/225000 (91%)] Loss: 6808.169922\n",
      "Train Epoch: 389 [209152/225000 (93%)] Loss: 6614.386719\n",
      "Train Epoch: 389 [213248/225000 (95%)] Loss: 6808.455078\n",
      "Train Epoch: 389 [217344/225000 (97%)] Loss: 6824.244141\n",
      "Train Epoch: 389 [221440/225000 (98%)] Loss: 6809.929688\n",
      "    epoch          : 389\n",
      "    loss           : 6834.199760914391\n",
      "    val_loss       : 6834.216131502269\n",
      "Train Epoch: 390 [256/225000 (0%)] Loss: 6816.693359\n",
      "Train Epoch: 390 [4352/225000 (2%)] Loss: 6772.902344\n",
      "Train Epoch: 390 [8448/225000 (4%)] Loss: 6822.878906\n",
      "Train Epoch: 390 [12544/225000 (6%)] Loss: 6905.298828\n",
      "Train Epoch: 390 [16640/225000 (7%)] Loss: 6866.212891\n",
      "Train Epoch: 390 [20736/225000 (9%)] Loss: 6740.414062\n",
      "Train Epoch: 390 [24832/225000 (11%)] Loss: 6797.453125\n",
      "Train Epoch: 390 [28928/225000 (13%)] Loss: 6837.236328\n",
      "Train Epoch: 390 [33024/225000 (15%)] Loss: 6706.580078\n",
      "Train Epoch: 390 [37120/225000 (16%)] Loss: 6806.375000\n",
      "Train Epoch: 390 [41216/225000 (18%)] Loss: 6875.361328\n",
      "Train Epoch: 390 [45312/225000 (20%)] Loss: 6685.765625\n",
      "Train Epoch: 390 [49408/225000 (22%)] Loss: 6762.521484\n",
      "Train Epoch: 390 [53504/225000 (24%)] Loss: 6844.205078\n",
      "Train Epoch: 390 [57600/225000 (26%)] Loss: 6799.103516\n",
      "Train Epoch: 390 [61696/225000 (27%)] Loss: 6849.076172\n",
      "Train Epoch: 390 [65792/225000 (29%)] Loss: 6754.125000\n",
      "Train Epoch: 390 [69888/225000 (31%)] Loss: 6916.582031\n",
      "Train Epoch: 390 [73984/225000 (33%)] Loss: 6846.166016\n",
      "Train Epoch: 390 [78080/225000 (35%)] Loss: 6755.375000\n",
      "Train Epoch: 390 [82176/225000 (37%)] Loss: 6687.185547\n",
      "Train Epoch: 390 [86272/225000 (38%)] Loss: 6681.511719\n",
      "Train Epoch: 390 [90368/225000 (40%)] Loss: 6725.437500\n",
      "Train Epoch: 390 [94464/225000 (42%)] Loss: 6756.984375\n",
      "Train Epoch: 390 [98560/225000 (44%)] Loss: 6974.783203\n",
      "Train Epoch: 390 [102656/225000 (46%)] Loss: 6845.351562\n",
      "Train Epoch: 390 [106752/225000 (47%)] Loss: 6778.509766\n",
      "Train Epoch: 390 [110848/225000 (49%)] Loss: 6759.060547\n",
      "Train Epoch: 390 [114944/225000 (51%)] Loss: 6789.117188\n",
      "Train Epoch: 390 [119040/225000 (53%)] Loss: 6865.208984\n",
      "Train Epoch: 390 [123136/225000 (55%)] Loss: 6828.523438\n",
      "Train Epoch: 390 [127232/225000 (57%)] Loss: 6879.476562\n",
      "Train Epoch: 390 [131328/225000 (58%)] Loss: 6666.527344\n",
      "Train Epoch: 390 [135424/225000 (60%)] Loss: 6846.328125\n",
      "Train Epoch: 390 [139520/225000 (62%)] Loss: 6784.654297\n",
      "Train Epoch: 390 [143616/225000 (64%)] Loss: 6873.042969\n",
      "Train Epoch: 390 [147712/225000 (66%)] Loss: 6798.634766\n",
      "Train Epoch: 390 [151808/225000 (67%)] Loss: 6898.875000\n",
      "Train Epoch: 390 [155904/225000 (69%)] Loss: 6865.388672\n",
      "Train Epoch: 390 [160000/225000 (71%)] Loss: 6860.537109\n",
      "Train Epoch: 390 [164096/225000 (73%)] Loss: 6661.757812\n",
      "Train Epoch: 390 [168192/225000 (75%)] Loss: 6862.939453\n",
      "Train Epoch: 390 [172288/225000 (77%)] Loss: 6887.914062\n",
      "Train Epoch: 390 [176384/225000 (78%)] Loss: 6850.582031\n",
      "Train Epoch: 390 [180480/225000 (80%)] Loss: 6836.605469\n",
      "Train Epoch: 390 [184576/225000 (82%)] Loss: 6776.767578\n",
      "Train Epoch: 390 [188672/225000 (84%)] Loss: 6667.468750\n",
      "Train Epoch: 390 [192768/225000 (86%)] Loss: 6829.234375\n",
      "Train Epoch: 390 [196864/225000 (87%)] Loss: 6741.400391\n",
      "Train Epoch: 390 [200960/225000 (89%)] Loss: 6768.695312\n",
      "Train Epoch: 390 [205056/225000 (91%)] Loss: 6647.962891\n",
      "Train Epoch: 390 [209152/225000 (93%)] Loss: 6826.880859\n",
      "Train Epoch: 390 [213248/225000 (95%)] Loss: 6760.617188\n",
      "Train Epoch: 390 [217344/225000 (97%)] Loss: 6927.953125\n",
      "Train Epoch: 390 [221440/225000 (98%)] Loss: 6936.199219\n",
      "    epoch          : 390\n",
      "    loss           : 6850.7745873773465\n",
      "    val_loss       : 6832.0141372644175\n",
      "Train Epoch: 391 [256/225000 (0%)] Loss: 6705.244141\n",
      "Train Epoch: 391 [4352/225000 (2%)] Loss: 6836.332031\n",
      "Train Epoch: 391 [8448/225000 (4%)] Loss: 6876.267578\n",
      "Train Epoch: 391 [12544/225000 (6%)] Loss: 6911.632812\n",
      "Train Epoch: 391 [16640/225000 (7%)] Loss: 6612.093750\n",
      "Train Epoch: 391 [20736/225000 (9%)] Loss: 6883.083984\n",
      "Train Epoch: 391 [24832/225000 (11%)] Loss: 6708.330078\n",
      "Train Epoch: 391 [28928/225000 (13%)] Loss: 6998.048828\n",
      "Train Epoch: 391 [33024/225000 (15%)] Loss: 6743.626953\n",
      "Train Epoch: 391 [37120/225000 (16%)] Loss: 6681.595703\n",
      "Train Epoch: 391 [41216/225000 (18%)] Loss: 6828.337891\n",
      "Train Epoch: 391 [45312/225000 (20%)] Loss: 6775.775391\n",
      "Train Epoch: 391 [49408/225000 (22%)] Loss: 6719.767578\n",
      "Train Epoch: 391 [53504/225000 (24%)] Loss: 6894.171875\n",
      "Train Epoch: 391 [57600/225000 (26%)] Loss: 6877.638672\n",
      "Train Epoch: 391 [61696/225000 (27%)] Loss: 6786.246094\n",
      "Train Epoch: 391 [65792/225000 (29%)] Loss: 6831.714844\n",
      "Train Epoch: 391 [69888/225000 (31%)] Loss: 6756.714844\n",
      "Train Epoch: 391 [73984/225000 (33%)] Loss: 6812.154297\n",
      "Train Epoch: 391 [78080/225000 (35%)] Loss: 6743.728516\n",
      "Train Epoch: 391 [82176/225000 (37%)] Loss: 6917.087891\n",
      "Train Epoch: 391 [86272/225000 (38%)] Loss: 6872.007812\n",
      "Train Epoch: 391 [90368/225000 (40%)] Loss: 7022.593750\n",
      "Train Epoch: 391 [94464/225000 (42%)] Loss: 6695.300781\n",
      "Train Epoch: 391 [98560/225000 (44%)] Loss: 6832.240234\n",
      "Train Epoch: 391 [102656/225000 (46%)] Loss: 6889.722656\n",
      "Train Epoch: 391 [106752/225000 (47%)] Loss: 6759.835938\n",
      "Train Epoch: 391 [110848/225000 (49%)] Loss: 6897.113281\n",
      "Train Epoch: 391 [114944/225000 (51%)] Loss: 6695.791016\n",
      "Train Epoch: 391 [119040/225000 (53%)] Loss: 6729.386719\n",
      "Train Epoch: 391 [123136/225000 (55%)] Loss: 6858.908203\n",
      "Train Epoch: 391 [127232/225000 (57%)] Loss: 6817.062500\n",
      "Train Epoch: 391 [131328/225000 (58%)] Loss: 6935.294922\n",
      "Train Epoch: 391 [135424/225000 (60%)] Loss: 6887.515625\n",
      "Train Epoch: 391 [139520/225000 (62%)] Loss: 6939.921875\n",
      "Train Epoch: 391 [143616/225000 (64%)] Loss: 6754.085938\n",
      "Train Epoch: 391 [147712/225000 (66%)] Loss: 6932.953125\n",
      "Train Epoch: 391 [151808/225000 (67%)] Loss: 6858.667969\n",
      "Train Epoch: 391 [155904/225000 (69%)] Loss: 6705.935547\n",
      "Train Epoch: 391 [160000/225000 (71%)] Loss: 6738.363281\n",
      "Train Epoch: 391 [164096/225000 (73%)] Loss: 6836.000000\n",
      "Train Epoch: 391 [168192/225000 (75%)] Loss: 6886.921875\n",
      "Train Epoch: 391 [172288/225000 (77%)] Loss: 6872.066406\n",
      "Train Epoch: 391 [176384/225000 (78%)] Loss: 6881.146484\n",
      "Train Epoch: 391 [180480/225000 (80%)] Loss: 6860.337891\n",
      "Train Epoch: 391 [184576/225000 (82%)] Loss: 6835.871094\n",
      "Train Epoch: 391 [188672/225000 (84%)] Loss: 6722.375000\n",
      "Train Epoch: 391 [192768/225000 (86%)] Loss: 6776.343750\n",
      "Train Epoch: 391 [196864/225000 (87%)] Loss: 6790.742188\n",
      "Train Epoch: 391 [200960/225000 (89%)] Loss: 6842.865234\n",
      "Train Epoch: 391 [205056/225000 (91%)] Loss: 6777.779297\n",
      "Train Epoch: 391 [209152/225000 (93%)] Loss: 6917.695312\n",
      "Train Epoch: 391 [213248/225000 (95%)] Loss: 6816.292969\n",
      "Train Epoch: 391 [217344/225000 (97%)] Loss: 6783.794922\n",
      "Train Epoch: 391 [221440/225000 (98%)] Loss: 6820.013672\n",
      "    epoch          : 391\n",
      "    loss           : 6848.756580409201\n",
      "    val_loss       : 6899.825108053733\n",
      "Train Epoch: 392 [256/225000 (0%)] Loss: 6765.535156\n",
      "Train Epoch: 392 [4352/225000 (2%)] Loss: 6731.859375\n",
      "Train Epoch: 392 [8448/225000 (4%)] Loss: 6827.087891\n",
      "Train Epoch: 392 [12544/225000 (6%)] Loss: 6873.587891\n",
      "Train Epoch: 392 [16640/225000 (7%)] Loss: 6793.402344\n",
      "Train Epoch: 392 [20736/225000 (9%)] Loss: 6692.992188\n",
      "Train Epoch: 392 [24832/225000 (11%)] Loss: 6797.880859\n",
      "Train Epoch: 392 [28928/225000 (13%)] Loss: 6618.496094\n",
      "Train Epoch: 392 [33024/225000 (15%)] Loss: 6867.060547\n",
      "Train Epoch: 392 [37120/225000 (16%)] Loss: 6804.777344\n",
      "Train Epoch: 392 [41216/225000 (18%)] Loss: 6796.427734\n",
      "Train Epoch: 392 [45312/225000 (20%)] Loss: 6881.896484\n",
      "Train Epoch: 392 [49408/225000 (22%)] Loss: 6822.548828\n",
      "Train Epoch: 392 [53504/225000 (24%)] Loss: 6850.525391\n",
      "Train Epoch: 392 [57600/225000 (26%)] Loss: 6845.580078\n",
      "Train Epoch: 392 [61696/225000 (27%)] Loss: 6777.880859\n",
      "Train Epoch: 392 [65792/225000 (29%)] Loss: 6804.013672\n",
      "Train Epoch: 392 [69888/225000 (31%)] Loss: 6726.191406\n",
      "Train Epoch: 392 [73984/225000 (33%)] Loss: 6806.626953\n",
      "Train Epoch: 392 [78080/225000 (35%)] Loss: 6680.369141\n",
      "Train Epoch: 392 [82176/225000 (37%)] Loss: 6835.009766\n",
      "Train Epoch: 392 [86272/225000 (38%)] Loss: 6910.244141\n",
      "Train Epoch: 392 [90368/225000 (40%)] Loss: 6708.455078\n",
      "Train Epoch: 392 [94464/225000 (42%)] Loss: 6831.207031\n",
      "Train Epoch: 392 [98560/225000 (44%)] Loss: 6858.701172\n",
      "Train Epoch: 392 [102656/225000 (46%)] Loss: 6682.011719\n",
      "Train Epoch: 392 [106752/225000 (47%)] Loss: 6827.871094\n",
      "Train Epoch: 392 [110848/225000 (49%)] Loss: 6653.609375\n",
      "Train Epoch: 392 [114944/225000 (51%)] Loss: 6833.138672\n",
      "Train Epoch: 392 [119040/225000 (53%)] Loss: 6771.855469\n",
      "Train Epoch: 392 [123136/225000 (55%)] Loss: 6789.894531\n",
      "Train Epoch: 392 [127232/225000 (57%)] Loss: 6694.876953\n",
      "Train Epoch: 392 [131328/225000 (58%)] Loss: 6943.021484\n",
      "Train Epoch: 392 [135424/225000 (60%)] Loss: 6784.330078\n",
      "Train Epoch: 392 [139520/225000 (62%)] Loss: 6749.189453\n",
      "Train Epoch: 392 [143616/225000 (64%)] Loss: 6820.941406\n",
      "Train Epoch: 392 [147712/225000 (66%)] Loss: 6837.488281\n",
      "Train Epoch: 392 [151808/225000 (67%)] Loss: 6781.451172\n",
      "Train Epoch: 392 [155904/225000 (69%)] Loss: 7025.464844\n",
      "Train Epoch: 392 [160000/225000 (71%)] Loss: 6808.523438\n",
      "Train Epoch: 392 [164096/225000 (73%)] Loss: 6712.093750\n",
      "Train Epoch: 392 [168192/225000 (75%)] Loss: 6813.630859\n",
      "Train Epoch: 392 [172288/225000 (77%)] Loss: 6847.382812\n",
      "Train Epoch: 392 [176384/225000 (78%)] Loss: 6674.810547\n",
      "Train Epoch: 392 [180480/225000 (80%)] Loss: 6845.048828\n",
      "Train Epoch: 392 [184576/225000 (82%)] Loss: 6730.777344\n",
      "Train Epoch: 392 [188672/225000 (84%)] Loss: 6554.308594\n",
      "Train Epoch: 392 [192768/225000 (86%)] Loss: 6758.679688\n",
      "Train Epoch: 392 [196864/225000 (87%)] Loss: 6737.279297\n",
      "Train Epoch: 392 [200960/225000 (89%)] Loss: 6870.228516\n",
      "Train Epoch: 392 [205056/225000 (91%)] Loss: 6840.298828\n",
      "Train Epoch: 392 [209152/225000 (93%)] Loss: 6894.060547\n",
      "Train Epoch: 392 [213248/225000 (95%)] Loss: 6760.900391\n",
      "Train Epoch: 392 [217344/225000 (97%)] Loss: 6885.087891\n",
      "Train Epoch: 392 [221440/225000 (98%)] Loss: 12292.123047\n",
      "    epoch          : 392\n",
      "    loss           : 6818.173880341653\n",
      "    val_loss       : 6834.136897425262\n",
      "Train Epoch: 393 [256/225000 (0%)] Loss: 6939.378906\n",
      "Train Epoch: 393 [4352/225000 (2%)] Loss: 6811.232422\n",
      "Train Epoch: 393 [8448/225000 (4%)] Loss: 6840.886719\n",
      "Train Epoch: 393 [12544/225000 (6%)] Loss: 6858.923828\n",
      "Train Epoch: 393 [16640/225000 (7%)] Loss: 6684.222656\n",
      "Train Epoch: 393 [20736/225000 (9%)] Loss: 6779.179688\n",
      "Train Epoch: 393 [24832/225000 (11%)] Loss: 6907.814453\n",
      "Train Epoch: 393 [28928/225000 (13%)] Loss: 6848.726562\n",
      "Train Epoch: 393 [33024/225000 (15%)] Loss: 6750.697266\n",
      "Train Epoch: 393 [37120/225000 (16%)] Loss: 6832.265625\n",
      "Train Epoch: 393 [41216/225000 (18%)] Loss: 6741.058594\n",
      "Train Epoch: 393 [45312/225000 (20%)] Loss: 6708.281250\n",
      "Train Epoch: 393 [49408/225000 (22%)] Loss: 6771.390625\n",
      "Train Epoch: 393 [53504/225000 (24%)] Loss: 6748.121094\n",
      "Train Epoch: 393 [57600/225000 (26%)] Loss: 6730.382812\n",
      "Train Epoch: 393 [61696/225000 (27%)] Loss: 6726.779297\n",
      "Train Epoch: 393 [65792/225000 (29%)] Loss: 6789.312500\n",
      "Train Epoch: 393 [69888/225000 (31%)] Loss: 6818.914062\n",
      "Train Epoch: 393 [73984/225000 (33%)] Loss: 6759.402344\n",
      "Train Epoch: 393 [78080/225000 (35%)] Loss: 6925.951172\n",
      "Train Epoch: 393 [82176/225000 (37%)] Loss: 6738.873047\n",
      "Train Epoch: 393 [86272/225000 (38%)] Loss: 6669.074219\n",
      "Train Epoch: 393 [90368/225000 (40%)] Loss: 6958.007812\n",
      "Train Epoch: 393 [94464/225000 (42%)] Loss: 6781.423828\n",
      "Train Epoch: 393 [98560/225000 (44%)] Loss: 6691.074219\n",
      "Train Epoch: 393 [102656/225000 (46%)] Loss: 6706.236328\n",
      "Train Epoch: 393 [106752/225000 (47%)] Loss: 6788.328125\n",
      "Train Epoch: 393 [110848/225000 (49%)] Loss: 6986.562500\n",
      "Train Epoch: 393 [114944/225000 (51%)] Loss: 6888.027344\n",
      "Train Epoch: 393 [119040/225000 (53%)] Loss: 6907.505859\n",
      "Train Epoch: 393 [123136/225000 (55%)] Loss: 6983.974609\n",
      "Train Epoch: 393 [127232/225000 (57%)] Loss: 6880.445312\n",
      "Train Epoch: 393 [131328/225000 (58%)] Loss: 6706.138672\n",
      "Train Epoch: 393 [135424/225000 (60%)] Loss: 6779.574219\n",
      "Train Epoch: 393 [139520/225000 (62%)] Loss: 6803.679688\n",
      "Train Epoch: 393 [143616/225000 (64%)] Loss: 6737.667969\n",
      "Train Epoch: 393 [147712/225000 (66%)] Loss: 6952.394531\n",
      "Train Epoch: 393 [151808/225000 (67%)] Loss: 6654.205078\n",
      "Train Epoch: 393 [155904/225000 (69%)] Loss: 6912.843750\n",
      "Train Epoch: 393 [160000/225000 (71%)] Loss: 6911.125000\n",
      "Train Epoch: 393 [164096/225000 (73%)] Loss: 6744.480469\n",
      "Train Epoch: 393 [168192/225000 (75%)] Loss: 6947.830078\n",
      "Train Epoch: 393 [172288/225000 (77%)] Loss: 7011.259766\n",
      "Train Epoch: 393 [176384/225000 (78%)] Loss: 6705.312500\n",
      "Train Epoch: 393 [180480/225000 (80%)] Loss: 6817.978516\n",
      "Train Epoch: 393 [184576/225000 (82%)] Loss: 6702.498047\n",
      "Train Epoch: 393 [188672/225000 (84%)] Loss: 6741.681641\n",
      "Train Epoch: 393 [192768/225000 (86%)] Loss: 6772.289062\n",
      "Train Epoch: 393 [196864/225000 (87%)] Loss: 6737.814453\n",
      "Train Epoch: 393 [200960/225000 (89%)] Loss: 6764.929688\n",
      "Train Epoch: 393 [205056/225000 (91%)] Loss: 6756.591797\n",
      "Train Epoch: 393 [209152/225000 (93%)] Loss: 6846.412109\n",
      "Train Epoch: 393 [213248/225000 (95%)] Loss: 6950.255859\n",
      "Train Epoch: 393 [217344/225000 (97%)] Loss: 6896.261719\n",
      "Train Epoch: 393 [221440/225000 (98%)] Loss: 6921.042969\n",
      "    epoch          : 393\n",
      "    loss           : 6852.312015607225\n",
      "    val_loss       : 6829.028513500885\n",
      "Train Epoch: 394 [256/225000 (0%)] Loss: 6937.861328\n",
      "Train Epoch: 394 [4352/225000 (2%)] Loss: 6726.839844\n",
      "Train Epoch: 394 [8448/225000 (4%)] Loss: 6968.964844\n",
      "Train Epoch: 394 [12544/225000 (6%)] Loss: 6842.462891\n",
      "Train Epoch: 394 [16640/225000 (7%)] Loss: 6676.501953\n",
      "Train Epoch: 394 [20736/225000 (9%)] Loss: 6879.593750\n",
      "Train Epoch: 394 [24832/225000 (11%)] Loss: 6816.814453\n",
      "Train Epoch: 394 [28928/225000 (13%)] Loss: 6815.978516\n",
      "Train Epoch: 394 [33024/225000 (15%)] Loss: 6883.884766\n",
      "Train Epoch: 394 [37120/225000 (16%)] Loss: 6983.802734\n",
      "Train Epoch: 394 [41216/225000 (18%)] Loss: 6825.445312\n",
      "Train Epoch: 394 [45312/225000 (20%)] Loss: 6853.240234\n",
      "Train Epoch: 394 [49408/225000 (22%)] Loss: 6633.125000\n",
      "Train Epoch: 394 [53504/225000 (24%)] Loss: 6658.919922\n",
      "Train Epoch: 394 [57600/225000 (26%)] Loss: 6658.464844\n",
      "Train Epoch: 394 [61696/225000 (27%)] Loss: 6750.398438\n",
      "Train Epoch: 394 [65792/225000 (29%)] Loss: 6735.634766\n",
      "Train Epoch: 394 [69888/225000 (31%)] Loss: 6726.906250\n",
      "Train Epoch: 394 [73984/225000 (33%)] Loss: 6847.980469\n",
      "Train Epoch: 394 [78080/225000 (35%)] Loss: 6769.550781\n",
      "Train Epoch: 394 [82176/225000 (37%)] Loss: 6681.101562\n",
      "Train Epoch: 394 [86272/225000 (38%)] Loss: 6862.193359\n",
      "Train Epoch: 394 [90368/225000 (40%)] Loss: 6672.677734\n",
      "Train Epoch: 394 [94464/225000 (42%)] Loss: 6743.244141\n",
      "Train Epoch: 394 [98560/225000 (44%)] Loss: 6784.398438\n",
      "Train Epoch: 394 [102656/225000 (46%)] Loss: 6789.845703\n",
      "Train Epoch: 394 [106752/225000 (47%)] Loss: 6778.804688\n",
      "Train Epoch: 394 [110848/225000 (49%)] Loss: 6839.585938\n",
      "Train Epoch: 394 [114944/225000 (51%)] Loss: 6733.222656\n",
      "Train Epoch: 394 [119040/225000 (53%)] Loss: 6834.158203\n",
      "Train Epoch: 394 [123136/225000 (55%)] Loss: 6938.857422\n",
      "Train Epoch: 394 [127232/225000 (57%)] Loss: 6857.486328\n",
      "Train Epoch: 394 [131328/225000 (58%)] Loss: 6691.892578\n",
      "Train Epoch: 394 [135424/225000 (60%)] Loss: 6815.277344\n",
      "Train Epoch: 394 [139520/225000 (62%)] Loss: 6944.097656\n",
      "Train Epoch: 394 [143616/225000 (64%)] Loss: 7037.408203\n",
      "Train Epoch: 394 [147712/225000 (66%)] Loss: 6834.962891\n",
      "Train Epoch: 394 [151808/225000 (67%)] Loss: 6754.949219\n",
      "Train Epoch: 394 [155904/225000 (69%)] Loss: 6821.730469\n",
      "Train Epoch: 394 [160000/225000 (71%)] Loss: 6867.642578\n",
      "Train Epoch: 394 [164096/225000 (73%)] Loss: 6604.695312\n",
      "Train Epoch: 394 [168192/225000 (75%)] Loss: 6884.160156\n",
      "Train Epoch: 394 [172288/225000 (77%)] Loss: 7004.349609\n",
      "Train Epoch: 394 [176384/225000 (78%)] Loss: 6818.960938\n",
      "Train Epoch: 394 [180480/225000 (80%)] Loss: 6811.837891\n",
      "Train Epoch: 394 [184576/225000 (82%)] Loss: 6656.962891\n",
      "Train Epoch: 394 [188672/225000 (84%)] Loss: 6960.814453\n",
      "Train Epoch: 394 [192768/225000 (86%)] Loss: 6757.718750\n",
      "Train Epoch: 394 [196864/225000 (87%)] Loss: 6683.117188\n",
      "Train Epoch: 394 [200960/225000 (89%)] Loss: 6755.890625\n",
      "Train Epoch: 394 [205056/225000 (91%)] Loss: 6880.128906\n",
      "Train Epoch: 394 [209152/225000 (93%)] Loss: 6978.142578\n",
      "Train Epoch: 394 [213248/225000 (95%)] Loss: 6699.794922\n",
      "Train Epoch: 394 [217344/225000 (97%)] Loss: 6798.367188\n",
      "Train Epoch: 394 [221440/225000 (98%)] Loss: 6735.875000\n",
      "    epoch          : 394\n",
      "    loss           : 6840.633556865046\n",
      "    val_loss       : 6829.960379257494\n",
      "Train Epoch: 395 [256/225000 (0%)] Loss: 6953.972656\n",
      "Train Epoch: 395 [4352/225000 (2%)] Loss: 6795.996094\n",
      "Train Epoch: 395 [8448/225000 (4%)] Loss: 6933.480469\n",
      "Train Epoch: 395 [12544/225000 (6%)] Loss: 6796.630859\n",
      "Train Epoch: 395 [16640/225000 (7%)] Loss: 6953.890625\n",
      "Train Epoch: 395 [20736/225000 (9%)] Loss: 6673.593750\n",
      "Train Epoch: 395 [24832/225000 (11%)] Loss: 6745.615234\n",
      "Train Epoch: 395 [28928/225000 (13%)] Loss: 6785.416016\n",
      "Train Epoch: 395 [33024/225000 (15%)] Loss: 6693.916016\n",
      "Train Epoch: 395 [37120/225000 (16%)] Loss: 6790.142578\n",
      "Train Epoch: 395 [41216/225000 (18%)] Loss: 6766.365234\n",
      "Train Epoch: 395 [45312/225000 (20%)] Loss: 6840.091797\n",
      "Train Epoch: 395 [49408/225000 (22%)] Loss: 6884.435547\n",
      "Train Epoch: 395 [53504/225000 (24%)] Loss: 6847.964844\n",
      "Train Epoch: 395 [57600/225000 (26%)] Loss: 6706.517578\n",
      "Train Epoch: 395 [61696/225000 (27%)] Loss: 6969.160156\n",
      "Train Epoch: 395 [65792/225000 (29%)] Loss: 6617.822266\n",
      "Train Epoch: 395 [69888/225000 (31%)] Loss: 6758.417969\n",
      "Train Epoch: 395 [73984/225000 (33%)] Loss: 6766.677734\n",
      "Train Epoch: 395 [78080/225000 (35%)] Loss: 6824.845703\n",
      "Train Epoch: 395 [82176/225000 (37%)] Loss: 6838.193359\n",
      "Train Epoch: 395 [86272/225000 (38%)] Loss: 6814.966797\n",
      "Train Epoch: 395 [90368/225000 (40%)] Loss: 6858.933594\n",
      "Train Epoch: 395 [94464/225000 (42%)] Loss: 6735.271484\n",
      "Train Epoch: 395 [98560/225000 (44%)] Loss: 6810.882812\n",
      "Train Epoch: 395 [102656/225000 (46%)] Loss: 6788.105469\n",
      "Train Epoch: 395 [106752/225000 (47%)] Loss: 6868.607422\n",
      "Train Epoch: 395 [110848/225000 (49%)] Loss: 6919.294922\n",
      "Train Epoch: 395 [114944/225000 (51%)] Loss: 6869.222656\n",
      "Train Epoch: 395 [119040/225000 (53%)] Loss: 6870.732422\n",
      "Train Epoch: 395 [123136/225000 (55%)] Loss: 6785.539062\n",
      "Train Epoch: 395 [127232/225000 (57%)] Loss: 6779.386719\n",
      "Train Epoch: 395 [131328/225000 (58%)] Loss: 6811.771484\n",
      "Train Epoch: 395 [135424/225000 (60%)] Loss: 6734.904297\n",
      "Train Epoch: 395 [139520/225000 (62%)] Loss: 6933.892578\n",
      "Train Epoch: 395 [143616/225000 (64%)] Loss: 6901.777344\n",
      "Train Epoch: 395 [147712/225000 (66%)] Loss: 6792.617188\n",
      "Train Epoch: 395 [151808/225000 (67%)] Loss: 6755.179688\n",
      "Train Epoch: 395 [155904/225000 (69%)] Loss: 6769.804688\n",
      "Train Epoch: 395 [160000/225000 (71%)] Loss: 6942.583984\n",
      "Train Epoch: 395 [164096/225000 (73%)] Loss: 6867.339844\n",
      "Train Epoch: 395 [168192/225000 (75%)] Loss: 6707.982422\n",
      "Train Epoch: 395 [172288/225000 (77%)] Loss: 6731.779297\n",
      "Train Epoch: 395 [176384/225000 (78%)] Loss: 6765.236328\n",
      "Train Epoch: 395 [180480/225000 (80%)] Loss: 6818.947266\n",
      "Train Epoch: 395 [184576/225000 (82%)] Loss: 6904.720703\n",
      "Train Epoch: 395 [188672/225000 (84%)] Loss: 6794.498047\n",
      "Train Epoch: 395 [192768/225000 (86%)] Loss: 6908.705078\n",
      "Train Epoch: 395 [196864/225000 (87%)] Loss: 6686.691406\n",
      "Train Epoch: 395 [200960/225000 (89%)] Loss: 6788.314453\n",
      "Train Epoch: 395 [205056/225000 (91%)] Loss: 6818.775391\n",
      "Train Epoch: 395 [209152/225000 (93%)] Loss: 6779.796875\n",
      "Train Epoch: 395 [213248/225000 (95%)] Loss: 6737.007812\n",
      "Train Epoch: 395 [217344/225000 (97%)] Loss: 6823.126953\n",
      "Train Epoch: 395 [221440/225000 (98%)] Loss: 6729.449219\n",
      "    epoch          : 395\n",
      "    loss           : 6819.359879390643\n",
      "    val_loss       : 6893.740938828916\n",
      "Train Epoch: 396 [256/225000 (0%)] Loss: 6937.312500\n",
      "Train Epoch: 396 [4352/225000 (2%)] Loss: 6903.812500\n",
      "Train Epoch: 396 [8448/225000 (4%)] Loss: 6783.781250\n",
      "Train Epoch: 396 [12544/225000 (6%)] Loss: 6939.960938\n",
      "Train Epoch: 396 [16640/225000 (7%)] Loss: 6803.302734\n",
      "Train Epoch: 396 [20736/225000 (9%)] Loss: 6675.367188\n",
      "Train Epoch: 396 [24832/225000 (11%)] Loss: 6881.697266\n",
      "Train Epoch: 396 [28928/225000 (13%)] Loss: 6688.818359\n",
      "Train Epoch: 396 [33024/225000 (15%)] Loss: 6984.349609\n",
      "Train Epoch: 396 [37120/225000 (16%)] Loss: 6866.955078\n",
      "Train Epoch: 396 [41216/225000 (18%)] Loss: 6952.710938\n",
      "Train Epoch: 396 [45312/225000 (20%)] Loss: 6782.474609\n",
      "Train Epoch: 396 [49408/225000 (22%)] Loss: 6821.384766\n",
      "Train Epoch: 396 [53504/225000 (24%)] Loss: 6873.195312\n",
      "Train Epoch: 396 [57600/225000 (26%)] Loss: 6832.226562\n",
      "Train Epoch: 396 [61696/225000 (27%)] Loss: 6809.443359\n",
      "Train Epoch: 396 [65792/225000 (29%)] Loss: 6984.804688\n",
      "Train Epoch: 396 [69888/225000 (31%)] Loss: 6887.712891\n",
      "Train Epoch: 396 [73984/225000 (33%)] Loss: 6854.066406\n",
      "Train Epoch: 396 [78080/225000 (35%)] Loss: 6930.734375\n",
      "Train Epoch: 396 [82176/225000 (37%)] Loss: 6913.068359\n",
      "Train Epoch: 396 [86272/225000 (38%)] Loss: 6856.675781\n",
      "Train Epoch: 396 [90368/225000 (40%)] Loss: 6638.695312\n",
      "Train Epoch: 396 [94464/225000 (42%)] Loss: 7017.242188\n",
      "Train Epoch: 396 [98560/225000 (44%)] Loss: 6705.919922\n",
      "Train Epoch: 396 [102656/225000 (46%)] Loss: 6754.681641\n",
      "Train Epoch: 396 [106752/225000 (47%)] Loss: 6826.195312\n",
      "Train Epoch: 396 [110848/225000 (49%)] Loss: 6839.468750\n",
      "Train Epoch: 396 [114944/225000 (51%)] Loss: 6787.669922\n",
      "Train Epoch: 396 [119040/225000 (53%)] Loss: 6749.789062\n",
      "Train Epoch: 396 [123136/225000 (55%)] Loss: 6790.529297\n",
      "Train Epoch: 396 [127232/225000 (57%)] Loss: 6855.744141\n",
      "Train Epoch: 396 [131328/225000 (58%)] Loss: 6815.974609\n",
      "Train Epoch: 396 [135424/225000 (60%)] Loss: 6889.525391\n",
      "Train Epoch: 396 [139520/225000 (62%)] Loss: 6907.201172\n",
      "Train Epoch: 396 [143616/225000 (64%)] Loss: 6850.302734\n",
      "Train Epoch: 396 [147712/225000 (66%)] Loss: 6902.408203\n",
      "Train Epoch: 396 [151808/225000 (67%)] Loss: 6846.445312\n",
      "Train Epoch: 396 [155904/225000 (69%)] Loss: 6775.144531\n",
      "Train Epoch: 396 [160000/225000 (71%)] Loss: 6938.367188\n",
      "Train Epoch: 396 [164096/225000 (73%)] Loss: 6810.912109\n",
      "Train Epoch: 396 [168192/225000 (75%)] Loss: 6689.552734\n",
      "Train Epoch: 396 [172288/225000 (77%)] Loss: 6874.539062\n",
      "Train Epoch: 396 [176384/225000 (78%)] Loss: 6911.203125\n",
      "Train Epoch: 396 [180480/225000 (80%)] Loss: 6694.761719\n",
      "Train Epoch: 396 [184576/225000 (82%)] Loss: 6786.576172\n",
      "Train Epoch: 396 [188672/225000 (84%)] Loss: 6681.080078\n",
      "Train Epoch: 396 [192768/225000 (86%)] Loss: 6829.810547\n",
      "Train Epoch: 396 [196864/225000 (87%)] Loss: 6847.365234\n",
      "Train Epoch: 396 [200960/225000 (89%)] Loss: 6883.529297\n",
      "Train Epoch: 396 [205056/225000 (91%)] Loss: 6909.419922\n",
      "Train Epoch: 396 [209152/225000 (93%)] Loss: 6802.164062\n",
      "Train Epoch: 396 [213248/225000 (95%)] Loss: 6876.974609\n",
      "Train Epoch: 396 [217344/225000 (97%)] Loss: 6763.792969\n",
      "Train Epoch: 396 [221440/225000 (98%)] Loss: 6892.119141\n",
      "    epoch          : 396\n",
      "    loss           : 6825.115177714377\n",
      "    val_loss       : 6827.413503551969\n",
      "Train Epoch: 397 [256/225000 (0%)] Loss: 6868.888672\n",
      "Train Epoch: 397 [4352/225000 (2%)] Loss: 6674.970703\n",
      "Train Epoch: 397 [8448/225000 (4%)] Loss: 6824.136719\n",
      "Train Epoch: 397 [12544/225000 (6%)] Loss: 6642.662109\n",
      "Train Epoch: 397 [16640/225000 (7%)] Loss: 6813.126953\n",
      "Train Epoch: 397 [20736/225000 (9%)] Loss: 6895.759766\n",
      "Train Epoch: 397 [24832/225000 (11%)] Loss: 6781.220703\n",
      "Train Epoch: 397 [28928/225000 (13%)] Loss: 6916.289062\n",
      "Train Epoch: 397 [33024/225000 (15%)] Loss: 6830.693359\n",
      "Train Epoch: 397 [37120/225000 (16%)] Loss: 6829.457031\n",
      "Train Epoch: 397 [41216/225000 (18%)] Loss: 6764.292969\n",
      "Train Epoch: 397 [45312/225000 (20%)] Loss: 6734.234375\n",
      "Train Epoch: 397 [49408/225000 (22%)] Loss: 6754.707031\n",
      "Train Epoch: 397 [53504/225000 (24%)] Loss: 6787.916016\n",
      "Train Epoch: 397 [57600/225000 (26%)] Loss: 6949.863281\n",
      "Train Epoch: 397 [61696/225000 (27%)] Loss: 6653.789062\n",
      "Train Epoch: 397 [65792/225000 (29%)] Loss: 6872.835938\n",
      "Train Epoch: 397 [69888/225000 (31%)] Loss: 6879.070312\n",
      "Train Epoch: 397 [73984/225000 (33%)] Loss: 6761.765625\n",
      "Train Epoch: 397 [78080/225000 (35%)] Loss: 6769.539062\n",
      "Train Epoch: 397 [82176/225000 (37%)] Loss: 6592.373047\n",
      "Train Epoch: 397 [86272/225000 (38%)] Loss: 6762.400391\n",
      "Train Epoch: 397 [90368/225000 (40%)] Loss: 6828.136719\n",
      "Train Epoch: 397 [94464/225000 (42%)] Loss: 6784.658203\n",
      "Train Epoch: 397 [98560/225000 (44%)] Loss: 6775.134766\n",
      "Train Epoch: 397 [102656/225000 (46%)] Loss: 6724.230469\n",
      "Train Epoch: 397 [106752/225000 (47%)] Loss: 6721.128906\n",
      "Train Epoch: 397 [110848/225000 (49%)] Loss: 6872.433594\n",
      "Train Epoch: 397 [114944/225000 (51%)] Loss: 6740.578125\n",
      "Train Epoch: 397 [119040/225000 (53%)] Loss: 6725.876953\n",
      "Train Epoch: 397 [123136/225000 (55%)] Loss: 6876.113281\n",
      "Train Epoch: 397 [127232/225000 (57%)] Loss: 6780.066406\n",
      "Train Epoch: 397 [131328/225000 (58%)] Loss: 6636.496094\n",
      "Train Epoch: 397 [135424/225000 (60%)] Loss: 6739.667969\n",
      "Train Epoch: 397 [139520/225000 (62%)] Loss: 6872.521484\n",
      "Train Epoch: 397 [143616/225000 (64%)] Loss: 6789.726562\n",
      "Train Epoch: 397 [147712/225000 (66%)] Loss: 6833.091797\n",
      "Train Epoch: 397 [151808/225000 (67%)] Loss: 6745.384766\n",
      "Train Epoch: 397 [155904/225000 (69%)] Loss: 6721.455078\n",
      "Train Epoch: 397 [160000/225000 (71%)] Loss: 6749.761719\n",
      "Train Epoch: 397 [164096/225000 (73%)] Loss: 6750.392578\n",
      "Train Epoch: 397 [168192/225000 (75%)] Loss: 6879.789062\n",
      "Train Epoch: 397 [172288/225000 (77%)] Loss: 6735.701172\n",
      "Train Epoch: 397 [176384/225000 (78%)] Loss: 6780.451172\n",
      "Train Epoch: 397 [180480/225000 (80%)] Loss: 6812.662109\n",
      "Train Epoch: 397 [184576/225000 (82%)] Loss: 6866.488281\n",
      "Train Epoch: 397 [188672/225000 (84%)] Loss: 6864.654297\n",
      "Train Epoch: 397 [192768/225000 (86%)] Loss: 6790.867188\n",
      "Train Epoch: 397 [196864/225000 (87%)] Loss: 6820.142578\n",
      "Train Epoch: 397 [200960/225000 (89%)] Loss: 6747.527344\n",
      "Train Epoch: 397 [205056/225000 (91%)] Loss: 6929.380859\n",
      "Train Epoch: 397 [209152/225000 (93%)] Loss: 6761.326172\n",
      "Train Epoch: 397 [213248/225000 (95%)] Loss: 6779.841797\n",
      "Train Epoch: 397 [217344/225000 (97%)] Loss: 6922.265625\n",
      "Train Epoch: 397 [221440/225000 (98%)] Loss: 6656.925781\n",
      "    epoch          : 397\n",
      "    loss           : 6812.31596296395\n",
      "    val_loss       : 6823.340898921295\n",
      "Train Epoch: 398 [256/225000 (0%)] Loss: 6818.644531\n",
      "Train Epoch: 398 [4352/225000 (2%)] Loss: 6753.892578\n",
      "Train Epoch: 398 [8448/225000 (4%)] Loss: 6885.871094\n",
      "Train Epoch: 398 [12544/225000 (6%)] Loss: 6757.888672\n",
      "Train Epoch: 398 [16640/225000 (7%)] Loss: 6863.240234\n",
      "Train Epoch: 398 [20736/225000 (9%)] Loss: 6804.880859\n",
      "Train Epoch: 398 [24832/225000 (11%)] Loss: 6795.671875\n",
      "Train Epoch: 398 [28928/225000 (13%)] Loss: 6819.498047\n",
      "Train Epoch: 398 [33024/225000 (15%)] Loss: 6634.585938\n",
      "Train Epoch: 398 [37120/225000 (16%)] Loss: 6722.718750\n",
      "Train Epoch: 398 [41216/225000 (18%)] Loss: 6739.632812\n",
      "Train Epoch: 398 [45312/225000 (20%)] Loss: 6886.992188\n",
      "Train Epoch: 398 [49408/225000 (22%)] Loss: 6714.882812\n",
      "Train Epoch: 398 [53504/225000 (24%)] Loss: 6726.306641\n",
      "Train Epoch: 398 [57600/225000 (26%)] Loss: 6889.130859\n",
      "Train Epoch: 398 [61696/225000 (27%)] Loss: 6704.710938\n",
      "Train Epoch: 398 [65792/225000 (29%)] Loss: 6813.832031\n",
      "Train Epoch: 398 [69888/225000 (31%)] Loss: 6766.646484\n",
      "Train Epoch: 398 [73984/225000 (33%)] Loss: 6859.683594\n",
      "Train Epoch: 398 [78080/225000 (35%)] Loss: 17595.144531\n",
      "Train Epoch: 398 [82176/225000 (37%)] Loss: 6880.806641\n",
      "Train Epoch: 398 [86272/225000 (38%)] Loss: 6751.281250\n",
      "Train Epoch: 398 [90368/225000 (40%)] Loss: 6742.412109\n",
      "Train Epoch: 398 [94464/225000 (42%)] Loss: 6743.341797\n",
      "Train Epoch: 398 [98560/225000 (44%)] Loss: 6709.853516\n",
      "Train Epoch: 398 [102656/225000 (46%)] Loss: 6830.646484\n",
      "Train Epoch: 398 [106752/225000 (47%)] Loss: 6871.845703\n",
      "Train Epoch: 398 [110848/225000 (49%)] Loss: 6637.537109\n",
      "Train Epoch: 398 [114944/225000 (51%)] Loss: 6966.431641\n",
      "Train Epoch: 398 [119040/225000 (53%)] Loss: 6773.667969\n",
      "Train Epoch: 398 [123136/225000 (55%)] Loss: 6755.943359\n",
      "Train Epoch: 398 [127232/225000 (57%)] Loss: 6747.431641\n",
      "Train Epoch: 398 [131328/225000 (58%)] Loss: 6796.720703\n",
      "Train Epoch: 398 [135424/225000 (60%)] Loss: 6791.669922\n",
      "Train Epoch: 398 [139520/225000 (62%)] Loss: 6771.414062\n",
      "Train Epoch: 398 [143616/225000 (64%)] Loss: 6785.695312\n",
      "Train Epoch: 398 [147712/225000 (66%)] Loss: 6766.052734\n",
      "Train Epoch: 398 [151808/225000 (67%)] Loss: 6859.322266\n",
      "Train Epoch: 398 [155904/225000 (69%)] Loss: 6925.990234\n",
      "Train Epoch: 398 [160000/225000 (71%)] Loss: 6758.677734\n",
      "Train Epoch: 398 [164096/225000 (73%)] Loss: 6824.859375\n",
      "Train Epoch: 398 [168192/225000 (75%)] Loss: 6901.082031\n",
      "Train Epoch: 398 [172288/225000 (77%)] Loss: 6716.253906\n",
      "Train Epoch: 398 [176384/225000 (78%)] Loss: 6787.816406\n",
      "Train Epoch: 398 [180480/225000 (80%)] Loss: 6948.138672\n",
      "Train Epoch: 398 [184576/225000 (82%)] Loss: 6709.867188\n",
      "Train Epoch: 398 [188672/225000 (84%)] Loss: 6700.896484\n",
      "Train Epoch: 398 [192768/225000 (86%)] Loss: 6698.761719\n",
      "Train Epoch: 398 [196864/225000 (87%)] Loss: 6728.273438\n",
      "Train Epoch: 398 [200960/225000 (89%)] Loss: 6698.724609\n",
      "Train Epoch: 398 [205056/225000 (91%)] Loss: 6777.392578\n",
      "Train Epoch: 398 [209152/225000 (93%)] Loss: 6796.101562\n",
      "Train Epoch: 398 [213248/225000 (95%)] Loss: 6996.064453\n",
      "Train Epoch: 398 [217344/225000 (97%)] Loss: 6695.912109\n",
      "Train Epoch: 398 [221440/225000 (98%)] Loss: 6735.970703\n",
      "    epoch          : 398\n",
      "    loss           : 6831.432130572739\n",
      "    val_loss       : 6823.170190193216\n",
      "Train Epoch: 399 [256/225000 (0%)] Loss: 6837.304688\n",
      "Train Epoch: 399 [4352/225000 (2%)] Loss: 6992.484375\n",
      "Train Epoch: 399 [8448/225000 (4%)] Loss: 6833.937500\n",
      "Train Epoch: 399 [12544/225000 (6%)] Loss: 6652.089844\n",
      "Train Epoch: 399 [16640/225000 (7%)] Loss: 6904.591797\n",
      "Train Epoch: 399 [20736/225000 (9%)] Loss: 6824.646484\n",
      "Train Epoch: 399 [24832/225000 (11%)] Loss: 6980.830078\n",
      "Train Epoch: 399 [28928/225000 (13%)] Loss: 6883.724609\n",
      "Train Epoch: 399 [33024/225000 (15%)] Loss: 6802.947266\n",
      "Train Epoch: 399 [37120/225000 (16%)] Loss: 6813.076172\n",
      "Train Epoch: 399 [41216/225000 (18%)] Loss: 6878.537109\n",
      "Train Epoch: 399 [45312/225000 (20%)] Loss: 6804.693359\n",
      "Train Epoch: 399 [49408/225000 (22%)] Loss: 6748.876953\n",
      "Train Epoch: 399 [53504/225000 (24%)] Loss: 6738.367188\n",
      "Train Epoch: 399 [57600/225000 (26%)] Loss: 6786.529297\n",
      "Train Epoch: 399 [61696/225000 (27%)] Loss: 6907.296875\n",
      "Train Epoch: 399 [65792/225000 (29%)] Loss: 6746.681641\n",
      "Train Epoch: 399 [69888/225000 (31%)] Loss: 6737.384766\n",
      "Train Epoch: 399 [73984/225000 (33%)] Loss: 6781.710938\n",
      "Train Epoch: 399 [78080/225000 (35%)] Loss: 6715.767578\n",
      "Train Epoch: 399 [82176/225000 (37%)] Loss: 6769.351562\n",
      "Train Epoch: 399 [86272/225000 (38%)] Loss: 6877.162109\n",
      "Train Epoch: 399 [90368/225000 (40%)] Loss: 6998.824219\n",
      "Train Epoch: 399 [94464/225000 (42%)] Loss: 6891.205078\n",
      "Train Epoch: 399 [98560/225000 (44%)] Loss: 6929.720703\n",
      "Train Epoch: 399 [102656/225000 (46%)] Loss: 6822.835938\n",
      "Train Epoch: 399 [106752/225000 (47%)] Loss: 6950.628906\n",
      "Train Epoch: 399 [110848/225000 (49%)] Loss: 6788.380859\n",
      "Train Epoch: 399 [114944/225000 (51%)] Loss: 6757.363281\n",
      "Train Epoch: 399 [119040/225000 (53%)] Loss: 6614.343750\n",
      "Train Epoch: 399 [123136/225000 (55%)] Loss: 6803.119141\n",
      "Train Epoch: 399 [127232/225000 (57%)] Loss: 6758.035156\n",
      "Train Epoch: 399 [131328/225000 (58%)] Loss: 6897.904297\n",
      "Train Epoch: 399 [135424/225000 (60%)] Loss: 6754.140625\n",
      "Train Epoch: 399 [139520/225000 (62%)] Loss: 6830.769531\n",
      "Train Epoch: 399 [143616/225000 (64%)] Loss: 6953.089844\n",
      "Train Epoch: 399 [147712/225000 (66%)] Loss: 6849.697266\n",
      "Train Epoch: 399 [151808/225000 (67%)] Loss: 6857.339844\n",
      "Train Epoch: 399 [155904/225000 (69%)] Loss: 6825.007812\n",
      "Train Epoch: 399 [160000/225000 (71%)] Loss: 6832.666016\n",
      "Train Epoch: 399 [164096/225000 (73%)] Loss: 6824.724609\n",
      "Train Epoch: 399 [168192/225000 (75%)] Loss: 6831.974609\n",
      "Train Epoch: 399 [172288/225000 (77%)] Loss: 6758.457031\n",
      "Train Epoch: 399 [176384/225000 (78%)] Loss: 6658.580078\n",
      "Train Epoch: 399 [180480/225000 (80%)] Loss: 6711.134766\n",
      "Train Epoch: 399 [184576/225000 (82%)] Loss: 7044.857422\n",
      "Train Epoch: 399 [188672/225000 (84%)] Loss: 6792.228516\n",
      "Train Epoch: 399 [192768/225000 (86%)] Loss: 6780.740234\n",
      "Train Epoch: 399 [196864/225000 (87%)] Loss: 6719.234375\n",
      "Train Epoch: 399 [200960/225000 (89%)] Loss: 6813.802734\n",
      "Train Epoch: 399 [205056/225000 (91%)] Loss: 6821.197266\n",
      "Train Epoch: 399 [209152/225000 (93%)] Loss: 6967.699219\n",
      "Train Epoch: 399 [213248/225000 (95%)] Loss: 6811.535156\n",
      "Train Epoch: 399 [217344/225000 (97%)] Loss: 6822.951172\n",
      "Train Epoch: 399 [221440/225000 (98%)] Loss: 6968.859375\n",
      "    epoch          : 399\n",
      "    loss           : 6823.631558189349\n",
      "    val_loss       : 6821.286755807546\n",
      "Train Epoch: 400 [256/225000 (0%)] Loss: 6687.966797\n",
      "Train Epoch: 400 [4352/225000 (2%)] Loss: 6799.490234\n",
      "Train Epoch: 400 [8448/225000 (4%)] Loss: 6798.601562\n",
      "Train Epoch: 400 [12544/225000 (6%)] Loss: 6799.121094\n",
      "Train Epoch: 400 [16640/225000 (7%)] Loss: 6820.308594\n",
      "Train Epoch: 400 [20736/225000 (9%)] Loss: 6793.986328\n",
      "Train Epoch: 400 [24832/225000 (11%)] Loss: 6807.943359\n",
      "Train Epoch: 400 [28928/225000 (13%)] Loss: 6810.234375\n",
      "Train Epoch: 400 [33024/225000 (15%)] Loss: 6794.519531\n",
      "Train Epoch: 400 [37120/225000 (16%)] Loss: 6868.531250\n",
      "Train Epoch: 400 [41216/225000 (18%)] Loss: 6778.072266\n",
      "Train Epoch: 400 [45312/225000 (20%)] Loss: 6999.355469\n",
      "Train Epoch: 400 [49408/225000 (22%)] Loss: 6759.322266\n",
      "Train Epoch: 400 [53504/225000 (24%)] Loss: 6748.658203\n",
      "Train Epoch: 400 [57600/225000 (26%)] Loss: 6912.298828\n",
      "Train Epoch: 400 [61696/225000 (27%)] Loss: 6818.798828\n",
      "Train Epoch: 400 [65792/225000 (29%)] Loss: 6925.462891\n",
      "Train Epoch: 400 [69888/225000 (31%)] Loss: 6654.261719\n",
      "Train Epoch: 400 [73984/225000 (33%)] Loss: 6931.859375\n",
      "Train Epoch: 400 [78080/225000 (35%)] Loss: 6882.738281\n",
      "Train Epoch: 400 [82176/225000 (37%)] Loss: 6708.851562\n",
      "Train Epoch: 400 [86272/225000 (38%)] Loss: 6736.486328\n",
      "Train Epoch: 400 [90368/225000 (40%)] Loss: 6718.988281\n",
      "Train Epoch: 400 [94464/225000 (42%)] Loss: 6759.158203\n",
      "Train Epoch: 400 [98560/225000 (44%)] Loss: 6639.423828\n",
      "Train Epoch: 400 [102656/225000 (46%)] Loss: 6634.005859\n",
      "Train Epoch: 400 [106752/225000 (47%)] Loss: 6799.328125\n",
      "Train Epoch: 400 [110848/225000 (49%)] Loss: 6769.798828\n",
      "Train Epoch: 400 [114944/225000 (51%)] Loss: 6800.873047\n",
      "Train Epoch: 400 [119040/225000 (53%)] Loss: 6665.535156\n",
      "Train Epoch: 400 [123136/225000 (55%)] Loss: 6818.730469\n",
      "Train Epoch: 400 [127232/225000 (57%)] Loss: 6848.171875\n",
      "Train Epoch: 400 [131328/225000 (58%)] Loss: 6757.810547\n",
      "Train Epoch: 400 [135424/225000 (60%)] Loss: 6805.951172\n",
      "Train Epoch: 400 [139520/225000 (62%)] Loss: 6873.750000\n",
      "Train Epoch: 400 [143616/225000 (64%)] Loss: 6641.660156\n",
      "Train Epoch: 400 [147712/225000 (66%)] Loss: 6711.570312\n",
      "Train Epoch: 400 [151808/225000 (67%)] Loss: 6679.048828\n",
      "Train Epoch: 400 [155904/225000 (69%)] Loss: 6642.679688\n",
      "Train Epoch: 400 [160000/225000 (71%)] Loss: 6786.718750\n",
      "Train Epoch: 400 [164096/225000 (73%)] Loss: 6904.693359\n",
      "Train Epoch: 400 [168192/225000 (75%)] Loss: 6877.566406\n",
      "Train Epoch: 400 [172288/225000 (77%)] Loss: 6885.933594\n",
      "Train Epoch: 400 [176384/225000 (78%)] Loss: 6818.111328\n",
      "Train Epoch: 400 [180480/225000 (80%)] Loss: 6672.271484\n",
      "Train Epoch: 400 [184576/225000 (82%)] Loss: 6668.259766\n",
      "Train Epoch: 400 [188672/225000 (84%)] Loss: 6789.423828\n",
      "Train Epoch: 400 [192768/225000 (86%)] Loss: 6888.955078\n",
      "Train Epoch: 400 [196864/225000 (87%)] Loss: 6779.525391\n",
      "Train Epoch: 400 [200960/225000 (89%)] Loss: 6918.814453\n",
      "Train Epoch: 400 [205056/225000 (91%)] Loss: 6905.917969\n",
      "Train Epoch: 400 [209152/225000 (93%)] Loss: 6810.216797\n",
      "Train Epoch: 400 [213248/225000 (95%)] Loss: 6759.724609\n",
      "Train Epoch: 400 [217344/225000 (97%)] Loss: 6938.126953\n",
      "Train Epoch: 400 [221440/225000 (98%)] Loss: 6709.048828\n",
      "    epoch          : 400\n",
      "    loss           : 6809.064375355518\n",
      "    val_loss       : 6825.728859962249\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0827_225017/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [256/225000 (0%)] Loss: 6913.408203\n",
      "Train Epoch: 401 [4352/225000 (2%)] Loss: 6904.281250\n",
      "Train Epoch: 401 [8448/225000 (4%)] Loss: 6822.939453\n",
      "Train Epoch: 401 [12544/225000 (6%)] Loss: 6590.712891\n",
      "Train Epoch: 401 [16640/225000 (7%)] Loss: 6850.060547\n",
      "Train Epoch: 401 [20736/225000 (9%)] Loss: 6767.824219\n",
      "Train Epoch: 401 [24832/225000 (11%)] Loss: 6784.722656\n",
      "Train Epoch: 401 [28928/225000 (13%)] Loss: 6738.494141\n",
      "Train Epoch: 401 [33024/225000 (15%)] Loss: 6748.863281\n",
      "Train Epoch: 401 [37120/225000 (16%)] Loss: 6837.253906\n",
      "Train Epoch: 401 [41216/225000 (18%)] Loss: 6957.445312\n",
      "Train Epoch: 401 [45312/225000 (20%)] Loss: 6780.041016\n",
      "Train Epoch: 401 [49408/225000 (22%)] Loss: 6772.259766\n",
      "Train Epoch: 401 [53504/225000 (24%)] Loss: 6835.593750\n",
      "Train Epoch: 401 [57600/225000 (26%)] Loss: 6909.560547\n",
      "Train Epoch: 401 [61696/225000 (27%)] Loss: 6756.548828\n",
      "Train Epoch: 401 [65792/225000 (29%)] Loss: 6840.201172\n",
      "Train Epoch: 401 [69888/225000 (31%)] Loss: 6831.140625\n",
      "Train Epoch: 401 [73984/225000 (33%)] Loss: 6761.806641\n",
      "Train Epoch: 401 [78080/225000 (35%)] Loss: 6844.091797\n",
      "Train Epoch: 401 [82176/225000 (37%)] Loss: 6805.529297\n",
      "Train Epoch: 401 [86272/225000 (38%)] Loss: 6924.912109\n",
      "Train Epoch: 401 [90368/225000 (40%)] Loss: 6842.671875\n",
      "Train Epoch: 401 [94464/225000 (42%)] Loss: 6776.558594\n",
      "Train Epoch: 401 [98560/225000 (44%)] Loss: 6859.158203\n",
      "Train Epoch: 401 [102656/225000 (46%)] Loss: 6805.955078\n",
      "Train Epoch: 401 [106752/225000 (47%)] Loss: 6791.455078\n",
      "Train Epoch: 401 [110848/225000 (49%)] Loss: 6750.285156\n",
      "Train Epoch: 401 [114944/225000 (51%)] Loss: 6972.414062\n",
      "Train Epoch: 401 [119040/225000 (53%)] Loss: 6647.976562\n",
      "Train Epoch: 401 [123136/225000 (55%)] Loss: 6798.125000\n",
      "Train Epoch: 401 [127232/225000 (57%)] Loss: 6584.974609\n",
      "Train Epoch: 401 [131328/225000 (58%)] Loss: 6760.964844\n",
      "Train Epoch: 401 [135424/225000 (60%)] Loss: 6867.578125\n",
      "Train Epoch: 401 [139520/225000 (62%)] Loss: 6784.828125\n",
      "Train Epoch: 401 [143616/225000 (64%)] Loss: 6668.902344\n",
      "Train Epoch: 401 [147712/225000 (66%)] Loss: 6722.089844\n",
      "Train Epoch: 401 [151808/225000 (67%)] Loss: 6657.472656\n",
      "Train Epoch: 401 [155904/225000 (69%)] Loss: 6837.814453\n",
      "Train Epoch: 401 [160000/225000 (71%)] Loss: 6921.750000\n",
      "Train Epoch: 401 [164096/225000 (73%)] Loss: 6734.021484\n",
      "Train Epoch: 401 [168192/225000 (75%)] Loss: 6840.146484\n",
      "Train Epoch: 401 [172288/225000 (77%)] Loss: 6765.986328\n",
      "Train Epoch: 401 [176384/225000 (78%)] Loss: 6742.847656\n",
      "Train Epoch: 401 [180480/225000 (80%)] Loss: 6919.726562\n",
      "Train Epoch: 401 [184576/225000 (82%)] Loss: 6781.074219\n",
      "Train Epoch: 401 [188672/225000 (84%)] Loss: 6673.992188\n",
      "Train Epoch: 401 [192768/225000 (86%)] Loss: 6955.519531\n",
      "Train Epoch: 401 [196864/225000 (87%)] Loss: 6786.777344\n",
      "Train Epoch: 401 [200960/225000 (89%)] Loss: 6849.533203\n",
      "Train Epoch: 401 [205056/225000 (91%)] Loss: 6839.300781\n",
      "Train Epoch: 401 [209152/225000 (93%)] Loss: 6820.605469\n",
      "Train Epoch: 401 [213248/225000 (95%)] Loss: 6834.523438\n",
      "Train Epoch: 401 [217344/225000 (97%)] Loss: 6863.865234\n",
      "Train Epoch: 401 [221440/225000 (98%)] Loss: 6694.464844\n",
      "    epoch          : 401\n",
      "    loss           : 6827.128076338524\n",
      "    val_loss       : 6815.671628584667\n",
      "Train Epoch: 402 [256/225000 (0%)] Loss: 6663.960938\n",
      "Train Epoch: 402 [4352/225000 (2%)] Loss: 6885.894531\n",
      "Train Epoch: 402 [8448/225000 (4%)] Loss: 6842.800781\n",
      "Train Epoch: 402 [12544/225000 (6%)] Loss: 6884.798828\n",
      "Train Epoch: 402 [16640/225000 (7%)] Loss: 6787.738281\n",
      "Train Epoch: 402 [20736/225000 (9%)] Loss: 6692.523438\n",
      "Train Epoch: 402 [24832/225000 (11%)] Loss: 6799.806641\n",
      "Train Epoch: 402 [28928/225000 (13%)] Loss: 6828.429688\n",
      "Train Epoch: 402 [33024/225000 (15%)] Loss: 6860.531250\n",
      "Train Epoch: 402 [37120/225000 (16%)] Loss: 6787.560547\n",
      "Train Epoch: 402 [41216/225000 (18%)] Loss: 6816.248047\n",
      "Train Epoch: 402 [45312/225000 (20%)] Loss: 6820.800781\n",
      "Train Epoch: 402 [49408/225000 (22%)] Loss: 6710.050781\n",
      "Train Epoch: 402 [53504/225000 (24%)] Loss: 6860.123047\n",
      "Train Epoch: 402 [57600/225000 (26%)] Loss: 6815.447266\n",
      "Train Epoch: 402 [61696/225000 (27%)] Loss: 6692.208984\n",
      "Train Epoch: 402 [65792/225000 (29%)] Loss: 6823.763672\n",
      "Train Epoch: 402 [69888/225000 (31%)] Loss: 6829.603516\n",
      "Train Epoch: 402 [73984/225000 (33%)] Loss: 6780.130859\n",
      "Train Epoch: 402 [78080/225000 (35%)] Loss: 6672.576172\n",
      "Train Epoch: 402 [82176/225000 (37%)] Loss: 6864.429688\n",
      "Train Epoch: 402 [86272/225000 (38%)] Loss: 6816.277344\n",
      "Train Epoch: 402 [90368/225000 (40%)] Loss: 6828.701172\n",
      "Train Epoch: 402 [94464/225000 (42%)] Loss: 6625.259766\n",
      "Train Epoch: 402 [98560/225000 (44%)] Loss: 6830.357422\n",
      "Train Epoch: 402 [102656/225000 (46%)] Loss: 6835.027344\n",
      "Train Epoch: 402 [106752/225000 (47%)] Loss: 6703.132812\n",
      "Train Epoch: 402 [110848/225000 (49%)] Loss: 6911.958984\n",
      "Train Epoch: 402 [114944/225000 (51%)] Loss: 6887.185547\n",
      "Train Epoch: 402 [119040/225000 (53%)] Loss: 6841.613281\n",
      "Train Epoch: 402 [123136/225000 (55%)] Loss: 6842.882812\n",
      "Train Epoch: 402 [127232/225000 (57%)] Loss: 6897.285156\n",
      "Train Epoch: 402 [131328/225000 (58%)] Loss: 6778.587891\n",
      "Train Epoch: 402 [135424/225000 (60%)] Loss: 6786.033203\n",
      "Train Epoch: 402 [139520/225000 (62%)] Loss: 6820.343750\n",
      "Train Epoch: 402 [143616/225000 (64%)] Loss: 6880.482422\n",
      "Train Epoch: 402 [147712/225000 (66%)] Loss: 6822.427734\n",
      "Train Epoch: 402 [151808/225000 (67%)] Loss: 6890.894531\n",
      "Train Epoch: 402 [155904/225000 (69%)] Loss: 6746.439453\n",
      "Train Epoch: 402 [160000/225000 (71%)] Loss: 6663.548828\n",
      "Train Epoch: 402 [164096/225000 (73%)] Loss: 6822.988281\n",
      "Train Epoch: 402 [168192/225000 (75%)] Loss: 6824.451172\n",
      "Train Epoch: 402 [172288/225000 (77%)] Loss: 6884.718750\n",
      "Train Epoch: 402 [176384/225000 (78%)] Loss: 6889.968750\n",
      "Train Epoch: 402 [180480/225000 (80%)] Loss: 6761.419922\n",
      "Train Epoch: 402 [184576/225000 (82%)] Loss: 6734.996094\n",
      "Train Epoch: 402 [188672/225000 (84%)] Loss: 6806.423828\n",
      "Train Epoch: 402 [192768/225000 (86%)] Loss: 6869.412109\n",
      "Train Epoch: 402 [196864/225000 (87%)] Loss: 6942.005859\n",
      "Train Epoch: 402 [200960/225000 (89%)] Loss: 6775.548828\n",
      "Train Epoch: 402 [205056/225000 (91%)] Loss: 6854.363281\n",
      "Train Epoch: 402 [209152/225000 (93%)] Loss: 6748.033203\n",
      "Train Epoch: 402 [213248/225000 (95%)] Loss: 6584.611328\n",
      "Train Epoch: 402 [217344/225000 (97%)] Loss: 6789.941406\n",
      "Train Epoch: 402 [221440/225000 (98%)] Loss: 6840.201172\n",
      "    epoch          : 402\n",
      "    loss           : 6813.584518762443\n",
      "    val_loss       : 6814.365318106145\n",
      "Train Epoch: 403 [256/225000 (0%)] Loss: 6754.255859\n",
      "Train Epoch: 403 [4352/225000 (2%)] Loss: 6818.330078\n",
      "Train Epoch: 403 [8448/225000 (4%)] Loss: 6873.835938\n",
      "Train Epoch: 403 [12544/225000 (6%)] Loss: 6847.630859\n",
      "Train Epoch: 403 [16640/225000 (7%)] Loss: 6674.142578\n",
      "Train Epoch: 403 [20736/225000 (9%)] Loss: 6758.886719\n",
      "Train Epoch: 403 [24832/225000 (11%)] Loss: 6655.064453\n",
      "Train Epoch: 403 [28928/225000 (13%)] Loss: 6773.146484\n",
      "Train Epoch: 403 [33024/225000 (15%)] Loss: 6732.570312\n",
      "Train Epoch: 403 [37120/225000 (16%)] Loss: 6856.509766\n",
      "Train Epoch: 403 [41216/225000 (18%)] Loss: 6944.251953\n",
      "Train Epoch: 403 [45312/225000 (20%)] Loss: 6691.095703\n",
      "Train Epoch: 403 [49408/225000 (22%)] Loss: 6868.667969\n",
      "Train Epoch: 403 [53504/225000 (24%)] Loss: 6839.001953\n",
      "Train Epoch: 403 [57600/225000 (26%)] Loss: 6875.119141\n",
      "Train Epoch: 403 [61696/225000 (27%)] Loss: 6972.435547\n",
      "Train Epoch: 403 [65792/225000 (29%)] Loss: 6785.089844\n",
      "Train Epoch: 403 [69888/225000 (31%)] Loss: 6799.964844\n",
      "Train Epoch: 403 [73984/225000 (33%)] Loss: 6766.078125\n",
      "Train Epoch: 403 [78080/225000 (35%)] Loss: 6840.914062\n",
      "Train Epoch: 403 [82176/225000 (37%)] Loss: 6804.679688\n",
      "Train Epoch: 403 [86272/225000 (38%)] Loss: 6707.527344\n",
      "Train Epoch: 403 [90368/225000 (40%)] Loss: 6772.089844\n",
      "Train Epoch: 403 [94464/225000 (42%)] Loss: 6796.095703\n",
      "Train Epoch: 403 [98560/225000 (44%)] Loss: 6702.886719\n",
      "Train Epoch: 403 [102656/225000 (46%)] Loss: 6857.507812\n",
      "Train Epoch: 403 [106752/225000 (47%)] Loss: 6678.511719\n",
      "Train Epoch: 403 [110848/225000 (49%)] Loss: 6827.240234\n",
      "Train Epoch: 403 [114944/225000 (51%)] Loss: 6679.054688\n",
      "Train Epoch: 403 [119040/225000 (53%)] Loss: 6763.990234\n",
      "Train Epoch: 403 [123136/225000 (55%)] Loss: 6959.792969\n",
      "Train Epoch: 403 [127232/225000 (57%)] Loss: 6753.498047\n",
      "Train Epoch: 403 [131328/225000 (58%)] Loss: 6876.828125\n",
      "Train Epoch: 403 [135424/225000 (60%)] Loss: 6767.027344\n",
      "Train Epoch: 403 [139520/225000 (62%)] Loss: 6740.611328\n",
      "Train Epoch: 403 [143616/225000 (64%)] Loss: 6788.248047\n",
      "Train Epoch: 403 [147712/225000 (66%)] Loss: 6886.005859\n",
      "Train Epoch: 403 [151808/225000 (67%)] Loss: 6810.111328\n",
      "Train Epoch: 403 [155904/225000 (69%)] Loss: 6922.478516\n",
      "Train Epoch: 403 [160000/225000 (71%)] Loss: 6758.068359\n",
      "Train Epoch: 403 [164096/225000 (73%)] Loss: 6869.525391\n",
      "Train Epoch: 403 [168192/225000 (75%)] Loss: 6807.271484\n",
      "Train Epoch: 403 [172288/225000 (77%)] Loss: 6829.898438\n",
      "Train Epoch: 403 [176384/225000 (78%)] Loss: 6791.115234\n",
      "Train Epoch: 403 [180480/225000 (80%)] Loss: 6799.601562\n",
      "Train Epoch: 403 [184576/225000 (82%)] Loss: 6818.257812\n",
      "Train Epoch: 403 [188672/225000 (84%)] Loss: 6884.732422\n",
      "Train Epoch: 403 [192768/225000 (86%)] Loss: 6661.152344\n",
      "Train Epoch: 403 [196864/225000 (87%)] Loss: 6812.796875\n",
      "Train Epoch: 403 [200960/225000 (89%)] Loss: 6886.968750\n",
      "Train Epoch: 403 [205056/225000 (91%)] Loss: 6643.888672\n",
      "Train Epoch: 403 [209152/225000 (93%)] Loss: 6832.437500\n",
      "Train Epoch: 403 [213248/225000 (95%)] Loss: 6846.199219\n",
      "Train Epoch: 403 [217344/225000 (97%)] Loss: 6908.087891\n",
      "Train Epoch: 403 [221440/225000 (98%)] Loss: 6620.101562\n",
      "    epoch          : 403\n",
      "    loss           : 6808.24934340337\n",
      "    val_loss       : 6913.729676901078\n",
      "Train Epoch: 404 [256/225000 (0%)] Loss: 6770.136719\n",
      "Train Epoch: 404 [4352/225000 (2%)] Loss: 6839.480469\n",
      "Train Epoch: 404 [8448/225000 (4%)] Loss: 6849.494141\n",
      "Train Epoch: 404 [12544/225000 (6%)] Loss: 6795.857422\n",
      "Train Epoch: 404 [16640/225000 (7%)] Loss: 6911.808594\n",
      "Train Epoch: 404 [20736/225000 (9%)] Loss: 6820.853516\n",
      "Train Epoch: 404 [24832/225000 (11%)] Loss: 6775.126953\n",
      "Train Epoch: 404 [28928/225000 (13%)] Loss: 6875.750000\n",
      "Train Epoch: 404 [33024/225000 (15%)] Loss: 7123.779297\n",
      "Train Epoch: 404 [37120/225000 (16%)] Loss: 6664.267578\n",
      "Train Epoch: 404 [41216/225000 (18%)] Loss: 6658.794922\n",
      "Train Epoch: 404 [45312/225000 (20%)] Loss: 6783.703125\n",
      "Train Epoch: 404 [49408/225000 (22%)] Loss: 6839.673828\n",
      "Train Epoch: 404 [53504/225000 (24%)] Loss: 6739.343750\n",
      "Train Epoch: 404 [57600/225000 (26%)] Loss: 6871.810547\n",
      "Train Epoch: 404 [61696/225000 (27%)] Loss: 6660.464844\n",
      "Train Epoch: 404 [65792/225000 (29%)] Loss: 6893.982422\n",
      "Train Epoch: 404 [69888/225000 (31%)] Loss: 6830.320312\n",
      "Train Epoch: 404 [73984/225000 (33%)] Loss: 6844.398438\n",
      "Train Epoch: 404 [78080/225000 (35%)] Loss: 6799.689453\n",
      "Train Epoch: 404 [82176/225000 (37%)] Loss: 6781.410156\n",
      "Train Epoch: 404 [86272/225000 (38%)] Loss: 6793.449219\n",
      "Train Epoch: 404 [90368/225000 (40%)] Loss: 6890.611328\n",
      "Train Epoch: 404 [94464/225000 (42%)] Loss: 6884.230469\n",
      "Train Epoch: 404 [98560/225000 (44%)] Loss: 6901.236328\n",
      "Train Epoch: 404 [102656/225000 (46%)] Loss: 6838.343750\n",
      "Train Epoch: 404 [106752/225000 (47%)] Loss: 6882.353516\n",
      "Train Epoch: 404 [110848/225000 (49%)] Loss: 6783.667969\n",
      "Train Epoch: 404 [114944/225000 (51%)] Loss: 6767.082031\n",
      "Train Epoch: 404 [119040/225000 (53%)] Loss: 6753.000000\n",
      "Train Epoch: 404 [123136/225000 (55%)] Loss: 6681.480469\n",
      "Train Epoch: 404 [127232/225000 (57%)] Loss: 6891.130859\n",
      "Train Epoch: 404 [131328/225000 (58%)] Loss: 6855.371094\n",
      "Train Epoch: 404 [135424/225000 (60%)] Loss: 6836.257812\n",
      "Train Epoch: 404 [139520/225000 (62%)] Loss: 6855.234375\n",
      "Train Epoch: 404 [143616/225000 (64%)] Loss: 6712.419922\n",
      "Train Epoch: 404 [147712/225000 (66%)] Loss: 6833.724609\n",
      "Train Epoch: 404 [151808/225000 (67%)] Loss: 6774.046875\n",
      "Train Epoch: 404 [155904/225000 (69%)] Loss: 6765.154297\n",
      "Train Epoch: 404 [160000/225000 (71%)] Loss: 6944.957031\n",
      "Train Epoch: 404 [164096/225000 (73%)] Loss: 6914.613281\n",
      "Train Epoch: 404 [168192/225000 (75%)] Loss: 6782.630859\n",
      "Train Epoch: 404 [172288/225000 (77%)] Loss: 6844.330078\n",
      "Train Epoch: 404 [176384/225000 (78%)] Loss: 6776.300781\n",
      "Train Epoch: 404 [180480/225000 (80%)] Loss: 6996.076172\n",
      "Train Epoch: 404 [184576/225000 (82%)] Loss: 6780.984375\n",
      "Train Epoch: 404 [188672/225000 (84%)] Loss: 6975.277344\n",
      "Train Epoch: 404 [192768/225000 (86%)] Loss: 6771.195312\n",
      "Train Epoch: 404 [196864/225000 (87%)] Loss: 6627.800781\n",
      "Train Epoch: 404 [200960/225000 (89%)] Loss: 6771.441406\n",
      "Train Epoch: 404 [205056/225000 (91%)] Loss: 6907.169922\n",
      "Train Epoch: 404 [209152/225000 (93%)] Loss: 6743.626953\n",
      "Train Epoch: 404 [213248/225000 (95%)] Loss: 6775.015625\n",
      "Train Epoch: 404 [217344/225000 (97%)] Loss: 6858.187500\n",
      "Train Epoch: 404 [221440/225000 (98%)] Loss: 6911.343750\n",
      "    epoch          : 404\n",
      "    loss           : 6807.041736659201\n",
      "    val_loss       : 6816.8274943512315\n",
      "Train Epoch: 405 [256/225000 (0%)] Loss: 6704.826172\n",
      "Train Epoch: 405 [4352/225000 (2%)] Loss: 6827.785156\n",
      "Train Epoch: 405 [8448/225000 (4%)] Loss: 6771.669922\n",
      "Train Epoch: 405 [12544/225000 (6%)] Loss: 6819.525391\n",
      "Train Epoch: 405 [16640/225000 (7%)] Loss: 6761.115234\n",
      "Train Epoch: 405 [20736/225000 (9%)] Loss: 6868.269531\n",
      "Train Epoch: 405 [24832/225000 (11%)] Loss: 6812.818359\n",
      "Train Epoch: 405 [28928/225000 (13%)] Loss: 6667.939453\n",
      "Train Epoch: 405 [33024/225000 (15%)] Loss: 6752.498047\n",
      "Train Epoch: 405 [37120/225000 (16%)] Loss: 6750.984375\n",
      "Train Epoch: 405 [41216/225000 (18%)] Loss: 6778.478516\n",
      "Train Epoch: 405 [45312/225000 (20%)] Loss: 6666.390625\n",
      "Train Epoch: 405 [49408/225000 (22%)] Loss: 6812.042969\n",
      "Train Epoch: 405 [53504/225000 (24%)] Loss: 6773.035156\n",
      "Train Epoch: 405 [57600/225000 (26%)] Loss: 6654.726562\n",
      "Train Epoch: 405 [61696/225000 (27%)] Loss: 6695.281250\n",
      "Train Epoch: 405 [65792/225000 (29%)] Loss: 6756.093750\n",
      "Train Epoch: 405 [69888/225000 (31%)] Loss: 6703.378906\n",
      "Train Epoch: 405 [73984/225000 (33%)] Loss: 6839.679688\n",
      "Train Epoch: 405 [78080/225000 (35%)] Loss: 6771.740234\n",
      "Train Epoch: 405 [82176/225000 (37%)] Loss: 6883.861328\n",
      "Train Epoch: 405 [86272/225000 (38%)] Loss: 6867.050781\n",
      "Train Epoch: 405 [90368/225000 (40%)] Loss: 6939.550781\n",
      "Train Epoch: 405 [94464/225000 (42%)] Loss: 6862.798828\n",
      "Train Epoch: 405 [98560/225000 (44%)] Loss: 6822.000000\n",
      "Train Epoch: 405 [102656/225000 (46%)] Loss: 6880.478516\n",
      "Train Epoch: 405 [106752/225000 (47%)] Loss: 6893.203125\n",
      "Train Epoch: 405 [110848/225000 (49%)] Loss: 6687.468750\n",
      "Train Epoch: 405 [114944/225000 (51%)] Loss: 6773.785156\n",
      "Train Epoch: 405 [119040/225000 (53%)] Loss: 6735.021484\n",
      "Train Epoch: 405 [123136/225000 (55%)] Loss: 6914.759766\n",
      "Train Epoch: 405 [127232/225000 (57%)] Loss: 6969.652344\n",
      "Train Epoch: 405 [131328/225000 (58%)] Loss: 6891.333984\n",
      "Train Epoch: 405 [135424/225000 (60%)] Loss: 7042.486328\n",
      "Train Epoch: 405 [139520/225000 (62%)] Loss: 6781.371094\n",
      "Train Epoch: 405 [143616/225000 (64%)] Loss: 6749.919922\n",
      "Train Epoch: 405 [147712/225000 (66%)] Loss: 6780.318359\n",
      "Train Epoch: 405 [151808/225000 (67%)] Loss: 6787.123047\n",
      "Train Epoch: 405 [155904/225000 (69%)] Loss: 6704.181641\n",
      "Train Epoch: 405 [160000/225000 (71%)] Loss: 6848.271484\n",
      "Train Epoch: 405 [164096/225000 (73%)] Loss: 6903.621094\n",
      "Train Epoch: 405 [168192/225000 (75%)] Loss: 6775.878906\n",
      "Train Epoch: 405 [172288/225000 (77%)] Loss: 6790.039062\n",
      "Train Epoch: 405 [176384/225000 (78%)] Loss: 6772.185547\n",
      "Train Epoch: 405 [180480/225000 (80%)] Loss: 6824.152344\n",
      "Train Epoch: 405 [184576/225000 (82%)] Loss: 6622.876953\n",
      "Train Epoch: 405 [188672/225000 (84%)] Loss: 6817.572266\n",
      "Train Epoch: 405 [192768/225000 (86%)] Loss: 6678.421875\n",
      "Train Epoch: 405 [196864/225000 (87%)] Loss: 6831.300781\n",
      "Train Epoch: 405 [200960/225000 (89%)] Loss: 6830.033203\n",
      "Train Epoch: 405 [205056/225000 (91%)] Loss: 6873.798828\n",
      "Train Epoch: 405 [209152/225000 (93%)] Loss: 6672.724609\n",
      "Train Epoch: 405 [213248/225000 (95%)] Loss: 6747.142578\n",
      "Train Epoch: 405 [217344/225000 (97%)] Loss: 6912.445312\n",
      "Train Epoch: 405 [221440/225000 (98%)] Loss: 6769.515625\n",
      "    epoch          : 405\n",
      "    loss           : 6814.438073272185\n",
      "    val_loss       : 6809.734182087743\n",
      "Train Epoch: 406 [256/225000 (0%)] Loss: 6755.964844\n",
      "Train Epoch: 406 [4352/225000 (2%)] Loss: 6596.375000\n",
      "Train Epoch: 406 [8448/225000 (4%)] Loss: 6762.878906\n",
      "Train Epoch: 406 [12544/225000 (6%)] Loss: 6732.369141\n",
      "Train Epoch: 406 [16640/225000 (7%)] Loss: 6859.035156\n",
      "Train Epoch: 406 [20736/225000 (9%)] Loss: 6803.298828\n",
      "Train Epoch: 406 [24832/225000 (11%)] Loss: 6699.074219\n",
      "Train Epoch: 406 [28928/225000 (13%)] Loss: 6899.089844\n",
      "Train Epoch: 406 [33024/225000 (15%)] Loss: 6712.835938\n",
      "Train Epoch: 406 [37120/225000 (16%)] Loss: 6833.851562\n",
      "Train Epoch: 406 [41216/225000 (18%)] Loss: 6742.490234\n",
      "Train Epoch: 406 [45312/225000 (20%)] Loss: 6735.791016\n",
      "Train Epoch: 406 [49408/225000 (22%)] Loss: 6938.218750\n",
      "Train Epoch: 406 [53504/225000 (24%)] Loss: 6745.820312\n",
      "Train Epoch: 406 [57600/225000 (26%)] Loss: 6806.769531\n",
      "Train Epoch: 406 [61696/225000 (27%)] Loss: 6890.119141\n",
      "Train Epoch: 406 [65792/225000 (29%)] Loss: 6848.443359\n",
      "Train Epoch: 406 [69888/225000 (31%)] Loss: 6817.789062\n",
      "Train Epoch: 406 [73984/225000 (33%)] Loss: 6757.492188\n",
      "Train Epoch: 406 [78080/225000 (35%)] Loss: 6693.154297\n",
      "Train Epoch: 406 [82176/225000 (37%)] Loss: 6896.568359\n",
      "Train Epoch: 406 [86272/225000 (38%)] Loss: 6865.699219\n",
      "Train Epoch: 406 [90368/225000 (40%)] Loss: 6617.236328\n",
      "Train Epoch: 406 [94464/225000 (42%)] Loss: 6851.763672\n",
      "Train Epoch: 406 [98560/225000 (44%)] Loss: 6940.638672\n",
      "Train Epoch: 406 [102656/225000 (46%)] Loss: 6831.810547\n",
      "Train Epoch: 406 [106752/225000 (47%)] Loss: 6849.816406\n",
      "Train Epoch: 406 [110848/225000 (49%)] Loss: 6868.652344\n",
      "Train Epoch: 406 [114944/225000 (51%)] Loss: 6718.529297\n",
      "Train Epoch: 406 [119040/225000 (53%)] Loss: 6721.996094\n",
      "Train Epoch: 406 [123136/225000 (55%)] Loss: 6955.169922\n",
      "Train Epoch: 406 [127232/225000 (57%)] Loss: 6678.230469\n",
      "Train Epoch: 406 [131328/225000 (58%)] Loss: 6624.042969\n",
      "Train Epoch: 406 [135424/225000 (60%)] Loss: 6734.130859\n",
      "Train Epoch: 406 [139520/225000 (62%)] Loss: 6755.253906\n",
      "Train Epoch: 406 [143616/225000 (64%)] Loss: 6599.781250\n",
      "Train Epoch: 406 [147712/225000 (66%)] Loss: 6835.464844\n",
      "Train Epoch: 406 [151808/225000 (67%)] Loss: 7011.066406\n",
      "Train Epoch: 406 [155904/225000 (69%)] Loss: 6765.986328\n",
      "Train Epoch: 406 [160000/225000 (71%)] Loss: 6817.998047\n",
      "Train Epoch: 406 [164096/225000 (73%)] Loss: 6802.980469\n",
      "Train Epoch: 406 [168192/225000 (75%)] Loss: 6945.000000\n",
      "Train Epoch: 406 [172288/225000 (77%)] Loss: 6721.246094\n",
      "Train Epoch: 406 [176384/225000 (78%)] Loss: 6866.916016\n",
      "Train Epoch: 406 [180480/225000 (80%)] Loss: 6813.501953\n",
      "Train Epoch: 406 [184576/225000 (82%)] Loss: 6758.941406\n",
      "Train Epoch: 406 [188672/225000 (84%)] Loss: 6738.796875\n",
      "Train Epoch: 406 [192768/225000 (86%)] Loss: 6723.529297\n",
      "Train Epoch: 406 [196864/225000 (87%)] Loss: 6817.451172\n",
      "Train Epoch: 406 [200960/225000 (89%)] Loss: 6812.562500\n",
      "Train Epoch: 406 [205056/225000 (91%)] Loss: 6816.035156\n",
      "Train Epoch: 406 [209152/225000 (93%)] Loss: 6829.406250\n",
      "Train Epoch: 406 [213248/225000 (95%)] Loss: 6835.523438\n",
      "Train Epoch: 406 [217344/225000 (97%)] Loss: 6752.712891\n",
      "Train Epoch: 406 [221440/225000 (98%)] Loss: 6733.312500\n",
      "    epoch          : 406\n",
      "    loss           : 6819.530400090657\n",
      "    val_loss       : 6817.127256644016\n",
      "Train Epoch: 407 [256/225000 (0%)] Loss: 6766.355469\n",
      "Train Epoch: 407 [4352/225000 (2%)] Loss: 6684.740234\n",
      "Train Epoch: 407 [8448/225000 (4%)] Loss: 6759.035156\n",
      "Train Epoch: 407 [12544/225000 (6%)] Loss: 6905.955078\n",
      "Train Epoch: 407 [16640/225000 (7%)] Loss: 6801.447266\n",
      "Train Epoch: 407 [20736/225000 (9%)] Loss: 6804.230469\n",
      "Train Epoch: 407 [24832/225000 (11%)] Loss: 6631.841797\n",
      "Train Epoch: 407 [28928/225000 (13%)] Loss: 6684.474609\n",
      "Train Epoch: 407 [33024/225000 (15%)] Loss: 6726.998047\n",
      "Train Epoch: 407 [37120/225000 (16%)] Loss: 6785.462891\n",
      "Train Epoch: 407 [41216/225000 (18%)] Loss: 6871.230469\n",
      "Train Epoch: 407 [45312/225000 (20%)] Loss: 6681.187500\n",
      "Train Epoch: 407 [49408/225000 (22%)] Loss: 6719.449219\n",
      "Train Epoch: 407 [53504/225000 (24%)] Loss: 6786.148438\n",
      "Train Epoch: 407 [57600/225000 (26%)] Loss: 6732.400391\n",
      "Train Epoch: 407 [61696/225000 (27%)] Loss: 6874.828125\n",
      "Train Epoch: 407 [65792/225000 (29%)] Loss: 6692.554688\n",
      "Train Epoch: 407 [69888/225000 (31%)] Loss: 6833.781250\n",
      "Train Epoch: 407 [73984/225000 (33%)] Loss: 6731.613281\n",
      "Train Epoch: 407 [78080/225000 (35%)] Loss: 6725.728516\n",
      "Train Epoch: 407 [82176/225000 (37%)] Loss: 6850.025391\n",
      "Train Epoch: 407 [86272/225000 (38%)] Loss: 6867.029297\n",
      "Train Epoch: 407 [90368/225000 (40%)] Loss: 6843.275391\n",
      "Train Epoch: 407 [94464/225000 (42%)] Loss: 6811.226562\n",
      "Train Epoch: 407 [98560/225000 (44%)] Loss: 6713.644531\n",
      "Train Epoch: 407 [102656/225000 (46%)] Loss: 6713.019531\n",
      "Train Epoch: 407 [106752/225000 (47%)] Loss: 6885.521484\n",
      "Train Epoch: 407 [110848/225000 (49%)] Loss: 6800.710938\n",
      "Train Epoch: 407 [114944/225000 (51%)] Loss: 6801.990234\n",
      "Train Epoch: 407 [119040/225000 (53%)] Loss: 6832.044922\n",
      "Train Epoch: 407 [123136/225000 (55%)] Loss: 6664.660156\n",
      "Train Epoch: 407 [127232/225000 (57%)] Loss: 6876.154297\n",
      "Train Epoch: 407 [131328/225000 (58%)] Loss: 6803.931641\n",
      "Train Epoch: 407 [135424/225000 (60%)] Loss: 6771.828125\n",
      "Train Epoch: 407 [139520/225000 (62%)] Loss: 6879.671875\n",
      "Train Epoch: 407 [143616/225000 (64%)] Loss: 6771.093750\n",
      "Train Epoch: 407 [147712/225000 (66%)] Loss: 6808.404297\n",
      "Train Epoch: 407 [151808/225000 (67%)] Loss: 6705.429688\n",
      "Train Epoch: 407 [155904/225000 (69%)] Loss: 6815.064453\n",
      "Train Epoch: 407 [160000/225000 (71%)] Loss: 6808.902344\n",
      "Train Epoch: 407 [164096/225000 (73%)] Loss: 6870.423828\n",
      "Train Epoch: 407 [168192/225000 (75%)] Loss: 6940.291016\n",
      "Train Epoch: 407 [172288/225000 (77%)] Loss: 6791.681641\n",
      "Train Epoch: 407 [176384/225000 (78%)] Loss: 6649.353516\n",
      "Train Epoch: 407 [180480/225000 (80%)] Loss: 6667.800781\n",
      "Train Epoch: 407 [184576/225000 (82%)] Loss: 6588.964844\n",
      "Train Epoch: 407 [188672/225000 (84%)] Loss: 6795.039062\n",
      "Train Epoch: 407 [192768/225000 (86%)] Loss: 6753.794922\n",
      "Train Epoch: 407 [196864/225000 (87%)] Loss: 6848.402344\n",
      "Train Epoch: 407 [200960/225000 (89%)] Loss: 6813.898438\n",
      "Train Epoch: 407 [205056/225000 (91%)] Loss: 6764.345703\n",
      "Train Epoch: 407 [209152/225000 (93%)] Loss: 6716.429688\n",
      "Train Epoch: 407 [213248/225000 (95%)] Loss: 6595.964844\n",
      "Train Epoch: 407 [217344/225000 (97%)] Loss: 6873.957031\n",
      "Train Epoch: 407 [221440/225000 (98%)] Loss: 6685.365234\n",
      "    epoch          : 407\n",
      "    loss           : 6791.803385416667\n",
      "    val_loss       : 6810.970294181181\n",
      "Train Epoch: 408 [256/225000 (0%)] Loss: 6783.062500\n",
      "Train Epoch: 408 [4352/225000 (2%)] Loss: 6789.123047\n",
      "Train Epoch: 408 [8448/225000 (4%)] Loss: 6689.000000\n",
      "Train Epoch: 408 [12544/225000 (6%)] Loss: 6799.181641\n",
      "Train Epoch: 408 [16640/225000 (7%)] Loss: 6761.429688\n",
      "Train Epoch: 408 [20736/225000 (9%)] Loss: 6792.619141\n",
      "Train Epoch: 408 [24832/225000 (11%)] Loss: 6807.027344\n",
      "Train Epoch: 408 [28928/225000 (13%)] Loss: 6766.886719\n",
      "Train Epoch: 408 [33024/225000 (15%)] Loss: 6822.404297\n",
      "Train Epoch: 408 [37120/225000 (16%)] Loss: 6790.648438\n",
      "Train Epoch: 408 [41216/225000 (18%)] Loss: 6799.099609\n",
      "Train Epoch: 408 [45312/225000 (20%)] Loss: 6778.792969\n",
      "Train Epoch: 408 [49408/225000 (22%)] Loss: 6753.572266\n",
      "Train Epoch: 408 [53504/225000 (24%)] Loss: 6723.339844\n",
      "Train Epoch: 408 [57600/225000 (26%)] Loss: 6842.810547\n",
      "Train Epoch: 408 [61696/225000 (27%)] Loss: 6843.908203\n",
      "Train Epoch: 408 [65792/225000 (29%)] Loss: 6853.253906\n",
      "Train Epoch: 408 [69888/225000 (31%)] Loss: 6800.892578\n",
      "Train Epoch: 408 [73984/225000 (33%)] Loss: 6793.207031\n",
      "Train Epoch: 408 [78080/225000 (35%)] Loss: 6793.392578\n",
      "Train Epoch: 408 [82176/225000 (37%)] Loss: 6723.890625\n",
      "Train Epoch: 408 [86272/225000 (38%)] Loss: 6680.937500\n",
      "Train Epoch: 408 [90368/225000 (40%)] Loss: 6713.902344\n",
      "Train Epoch: 408 [94464/225000 (42%)] Loss: 6918.486328\n",
      "Train Epoch: 408 [98560/225000 (44%)] Loss: 6791.738281\n",
      "Train Epoch: 408 [102656/225000 (46%)] Loss: 6695.322266\n",
      "Train Epoch: 408 [106752/225000 (47%)] Loss: 6771.728516\n",
      "Train Epoch: 408 [110848/225000 (49%)] Loss: 6800.525391\n",
      "Train Epoch: 408 [114944/225000 (51%)] Loss: 6854.113281\n",
      "Train Epoch: 408 [119040/225000 (53%)] Loss: 6727.367188\n",
      "Train Epoch: 408 [123136/225000 (55%)] Loss: 6716.087891\n",
      "Train Epoch: 408 [127232/225000 (57%)] Loss: 6593.802734\n",
      "Train Epoch: 408 [131328/225000 (58%)] Loss: 6760.587891\n",
      "Train Epoch: 408 [135424/225000 (60%)] Loss: 6803.750000\n",
      "Train Epoch: 408 [139520/225000 (62%)] Loss: 6871.246094\n",
      "Train Epoch: 408 [143616/225000 (64%)] Loss: 6776.789062\n",
      "Train Epoch: 408 [147712/225000 (66%)] Loss: 6839.292969\n",
      "Train Epoch: 408 [151808/225000 (67%)] Loss: 6739.058594\n",
      "Train Epoch: 408 [155904/225000 (69%)] Loss: 6893.183594\n",
      "Train Epoch: 408 [160000/225000 (71%)] Loss: 6780.146484\n",
      "Train Epoch: 408 [164096/225000 (73%)] Loss: 6725.740234\n",
      "Train Epoch: 408 [168192/225000 (75%)] Loss: 6777.123047\n",
      "Train Epoch: 408 [172288/225000 (77%)] Loss: 6646.246094\n",
      "Train Epoch: 408 [176384/225000 (78%)] Loss: 6640.052734\n",
      "Train Epoch: 408 [180480/225000 (80%)] Loss: 6751.855469\n",
      "Train Epoch: 408 [184576/225000 (82%)] Loss: 6817.113281\n",
      "Train Epoch: 408 [188672/225000 (84%)] Loss: 6885.498047\n",
      "Train Epoch: 408 [192768/225000 (86%)] Loss: 6882.500000\n",
      "Train Epoch: 408 [196864/225000 (87%)] Loss: 6637.337891\n",
      "Train Epoch: 408 [200960/225000 (89%)] Loss: 6745.000000\n",
      "Train Epoch: 408 [205056/225000 (91%)] Loss: 6841.728516\n",
      "Train Epoch: 408 [209152/225000 (93%)] Loss: 6828.273438\n",
      "Train Epoch: 408 [213248/225000 (95%)] Loss: 6814.087891\n",
      "Train Epoch: 408 [217344/225000 (97%)] Loss: 6890.091797\n",
      "Train Epoch: 408 [221440/225000 (98%)] Loss: 6913.845703\n",
      "    epoch          : 408\n",
      "    loss           : 6790.870133852389\n",
      "    val_loss       : 6815.278779003085\n",
      "Train Epoch: 409 [256/225000 (0%)] Loss: 6754.800781\n",
      "Train Epoch: 409 [4352/225000 (2%)] Loss: 6762.621094\n",
      "Train Epoch: 409 [8448/225000 (4%)] Loss: 6815.763672\n",
      "Train Epoch: 409 [12544/225000 (6%)] Loss: 6768.527344\n",
      "Train Epoch: 409 [16640/225000 (7%)] Loss: 6881.896484\n",
      "Train Epoch: 409 [20736/225000 (9%)] Loss: 6965.273438\n",
      "Train Epoch: 409 [24832/225000 (11%)] Loss: 6742.179688\n",
      "Train Epoch: 409 [28928/225000 (13%)] Loss: 6912.357422\n",
      "Train Epoch: 409 [33024/225000 (15%)] Loss: 6779.085938\n",
      "Train Epoch: 409 [37120/225000 (16%)] Loss: 6692.267578\n",
      "Train Epoch: 409 [41216/225000 (18%)] Loss: 6790.802734\n",
      "Train Epoch: 409 [45312/225000 (20%)] Loss: 6850.044922\n",
      "Train Epoch: 409 [49408/225000 (22%)] Loss: 6778.587891\n",
      "Train Epoch: 409 [53504/225000 (24%)] Loss: 6804.080078\n",
      "Train Epoch: 409 [57600/225000 (26%)] Loss: 6710.494141\n",
      "Train Epoch: 409 [61696/225000 (27%)] Loss: 6674.587891\n",
      "Train Epoch: 409 [65792/225000 (29%)] Loss: 6963.724609\n",
      "Train Epoch: 409 [69888/225000 (31%)] Loss: 6727.548828\n",
      "Train Epoch: 409 [73984/225000 (33%)] Loss: 6794.785156\n",
      "Train Epoch: 409 [78080/225000 (35%)] Loss: 6859.662109\n",
      "Train Epoch: 409 [82176/225000 (37%)] Loss: 6806.878906\n",
      "Train Epoch: 409 [86272/225000 (38%)] Loss: 6736.818359\n",
      "Train Epoch: 409 [90368/225000 (40%)] Loss: 6664.689453\n",
      "Train Epoch: 409 [94464/225000 (42%)] Loss: 6806.072266\n",
      "Train Epoch: 409 [98560/225000 (44%)] Loss: 6792.972656\n",
      "Train Epoch: 409 [102656/225000 (46%)] Loss: 6772.359375\n",
      "Train Epoch: 409 [106752/225000 (47%)] Loss: 6811.294922\n",
      "Train Epoch: 409 [110848/225000 (49%)] Loss: 6649.128906\n",
      "Train Epoch: 409 [114944/225000 (51%)] Loss: 6836.037109\n",
      "Train Epoch: 409 [119040/225000 (53%)] Loss: 6952.972656\n",
      "Train Epoch: 409 [123136/225000 (55%)] Loss: 6745.257812\n",
      "Train Epoch: 409 [127232/225000 (57%)] Loss: 6797.195312\n",
      "Train Epoch: 409 [131328/225000 (58%)] Loss: 6937.728516\n",
      "Train Epoch: 409 [135424/225000 (60%)] Loss: 6973.500000\n",
      "Train Epoch: 409 [139520/225000 (62%)] Loss: 6869.494141\n",
      "Train Epoch: 409 [143616/225000 (64%)] Loss: 6637.775391\n",
      "Train Epoch: 409 [147712/225000 (66%)] Loss: 6849.953125\n",
      "Train Epoch: 409 [151808/225000 (67%)] Loss: 6744.292969\n",
      "Train Epoch: 409 [155904/225000 (69%)] Loss: 6785.333984\n",
      "Train Epoch: 409 [160000/225000 (71%)] Loss: 6640.589844\n",
      "Train Epoch: 409 [164096/225000 (73%)] Loss: 6830.376953\n",
      "Train Epoch: 409 [168192/225000 (75%)] Loss: 6805.587891\n",
      "Train Epoch: 409 [172288/225000 (77%)] Loss: 6851.841797\n",
      "Train Epoch: 409 [176384/225000 (78%)] Loss: 6956.486328\n",
      "Train Epoch: 409 [180480/225000 (80%)] Loss: 6657.439453\n",
      "Train Epoch: 409 [184576/225000 (82%)] Loss: 6658.519531\n",
      "Train Epoch: 409 [188672/225000 (84%)] Loss: 6851.015625\n",
      "Train Epoch: 409 [192768/225000 (86%)] Loss: 6884.642578\n",
      "Train Epoch: 409 [196864/225000 (87%)] Loss: 6701.320312\n",
      "Train Epoch: 409 [200960/225000 (89%)] Loss: 6769.021484\n",
      "Train Epoch: 409 [205056/225000 (91%)] Loss: 6986.564453\n",
      "Train Epoch: 409 [209152/225000 (93%)] Loss: 6678.843750\n",
      "Train Epoch: 409 [213248/225000 (95%)] Loss: 6765.392578\n",
      "Train Epoch: 409 [217344/225000 (97%)] Loss: 6848.683594\n",
      "Train Epoch: 409 [221440/225000 (98%)] Loss: 6859.675781\n",
      "    epoch          : 409\n",
      "    loss           : 6795.71701018558\n",
      "    val_loss       : 6805.0887334760355\n",
      "Train Epoch: 410 [256/225000 (0%)] Loss: 6754.644531\n",
      "Train Epoch: 410 [4352/225000 (2%)] Loss: 6807.281250\n",
      "Train Epoch: 410 [8448/225000 (4%)] Loss: 6634.867188\n",
      "Train Epoch: 410 [12544/225000 (6%)] Loss: 6775.349609\n",
      "Train Epoch: 410 [16640/225000 (7%)] Loss: 6838.281250\n",
      "Train Epoch: 410 [20736/225000 (9%)] Loss: 6771.835938\n",
      "Train Epoch: 410 [24832/225000 (11%)] Loss: 6677.355469\n",
      "Train Epoch: 410 [28928/225000 (13%)] Loss: 6713.580078\n",
      "Train Epoch: 410 [33024/225000 (15%)] Loss: 6757.703125\n",
      "Train Epoch: 410 [37120/225000 (16%)] Loss: 6755.218750\n",
      "Train Epoch: 410 [41216/225000 (18%)] Loss: 6791.255859\n",
      "Train Epoch: 410 [45312/225000 (20%)] Loss: 6729.677734\n",
      "Train Epoch: 410 [49408/225000 (22%)] Loss: 6797.511719\n",
      "Train Epoch: 410 [53504/225000 (24%)] Loss: 6840.658203\n",
      "Train Epoch: 410 [57600/225000 (26%)] Loss: 6729.181641\n",
      "Train Epoch: 410 [61696/225000 (27%)] Loss: 6617.787109\n",
      "Train Epoch: 410 [65792/225000 (29%)] Loss: 6787.917969\n",
      "Train Epoch: 410 [69888/225000 (31%)] Loss: 6815.046875\n",
      "Train Epoch: 410 [73984/225000 (33%)] Loss: 6862.892578\n",
      "Train Epoch: 410 [78080/225000 (35%)] Loss: 6722.500000\n",
      "Train Epoch: 410 [82176/225000 (37%)] Loss: 6857.058594\n",
      "Train Epoch: 410 [86272/225000 (38%)] Loss: 6815.308594\n",
      "Train Epoch: 410 [90368/225000 (40%)] Loss: 6863.394531\n",
      "Train Epoch: 410 [94464/225000 (42%)] Loss: 6653.234375\n",
      "Train Epoch: 410 [98560/225000 (44%)] Loss: 6720.958984\n",
      "Train Epoch: 410 [102656/225000 (46%)] Loss: 6887.875000\n",
      "Train Epoch: 410 [106752/225000 (47%)] Loss: 6807.306641\n",
      "Train Epoch: 410 [110848/225000 (49%)] Loss: 6856.017578\n",
      "Train Epoch: 410 [114944/225000 (51%)] Loss: 6882.673828\n",
      "Train Epoch: 410 [119040/225000 (53%)] Loss: 6617.013672\n",
      "Train Epoch: 410 [123136/225000 (55%)] Loss: 6658.632812\n",
      "Train Epoch: 410 [127232/225000 (57%)] Loss: 6699.214844\n",
      "Train Epoch: 410 [131328/225000 (58%)] Loss: 6768.865234\n",
      "Train Epoch: 410 [135424/225000 (60%)] Loss: 6965.792969\n",
      "Train Epoch: 410 [139520/225000 (62%)] Loss: 6782.652344\n",
      "Train Epoch: 410 [143616/225000 (64%)] Loss: 6795.941406\n",
      "Train Epoch: 410 [147712/225000 (66%)] Loss: 6796.583984\n",
      "Train Epoch: 410 [151808/225000 (67%)] Loss: 6706.589844\n",
      "Train Epoch: 410 [155904/225000 (69%)] Loss: 6826.923828\n",
      "Train Epoch: 410 [160000/225000 (71%)] Loss: 6737.394531\n",
      "Train Epoch: 410 [164096/225000 (73%)] Loss: 6943.056641\n",
      "Train Epoch: 410 [168192/225000 (75%)] Loss: 6982.833984\n",
      "Train Epoch: 410 [172288/225000 (77%)] Loss: 6767.402344\n",
      "Train Epoch: 410 [176384/225000 (78%)] Loss: 6724.113281\n",
      "Train Epoch: 410 [180480/225000 (80%)] Loss: 6808.042969\n",
      "Train Epoch: 410 [184576/225000 (82%)] Loss: 6709.517578\n",
      "Train Epoch: 410 [188672/225000 (84%)] Loss: 6842.529297\n",
      "Train Epoch: 410 [192768/225000 (86%)] Loss: 6814.193359\n",
      "Train Epoch: 410 [196864/225000 (87%)] Loss: 6697.123047\n",
      "Train Epoch: 410 [200960/225000 (89%)] Loss: 6765.527344\n",
      "Train Epoch: 410 [205056/225000 (91%)] Loss: 6811.113281\n",
      "Train Epoch: 410 [209152/225000 (93%)] Loss: 6679.015625\n",
      "Train Epoch: 410 [213248/225000 (95%)] Loss: 6953.871094\n",
      "Train Epoch: 410 [217344/225000 (97%)] Loss: 6786.832031\n",
      "Train Epoch: 410 [221440/225000 (98%)] Loss: 6865.070312\n",
      "    epoch          : 410\n",
      "    loss           : 6801.692138394127\n",
      "    val_loss       : 6804.560720468054\n",
      "Train Epoch: 411 [256/225000 (0%)] Loss: 6739.884766\n",
      "Train Epoch: 411 [4352/225000 (2%)] Loss: 6702.080078\n",
      "Train Epoch: 411 [8448/225000 (4%)] Loss: 6776.460938\n",
      "Train Epoch: 411 [12544/225000 (6%)] Loss: 6906.767578\n",
      "Train Epoch: 411 [16640/225000 (7%)] Loss: 6862.927734\n",
      "Train Epoch: 411 [20736/225000 (9%)] Loss: 6840.669922\n",
      "Train Epoch: 411 [24832/225000 (11%)] Loss: 6623.753906\n",
      "Train Epoch: 411 [28928/225000 (13%)] Loss: 6728.925781\n",
      "Train Epoch: 411 [33024/225000 (15%)] Loss: 6732.294922\n",
      "Train Epoch: 411 [37120/225000 (16%)] Loss: 6801.554688\n",
      "Train Epoch: 411 [41216/225000 (18%)] Loss: 6800.162109\n",
      "Train Epoch: 411 [45312/225000 (20%)] Loss: 6762.681641\n",
      "Train Epoch: 411 [49408/225000 (22%)] Loss: 6917.994141\n",
      "Train Epoch: 411 [53504/225000 (24%)] Loss: 6746.416016\n",
      "Train Epoch: 411 [57600/225000 (26%)] Loss: 6811.373047\n",
      "Train Epoch: 411 [61696/225000 (27%)] Loss: 6808.080078\n",
      "Train Epoch: 411 [65792/225000 (29%)] Loss: 6747.601562\n",
      "Train Epoch: 411 [69888/225000 (31%)] Loss: 6834.662109\n",
      "Train Epoch: 411 [73984/225000 (33%)] Loss: 6870.560547\n",
      "Train Epoch: 411 [78080/225000 (35%)] Loss: 6837.533203\n",
      "Train Epoch: 411 [82176/225000 (37%)] Loss: 6674.095703\n",
      "Train Epoch: 411 [86272/225000 (38%)] Loss: 6801.234375\n",
      "Train Epoch: 411 [90368/225000 (40%)] Loss: 7021.755859\n",
      "Train Epoch: 411 [94464/225000 (42%)] Loss: 6820.734375\n",
      "Train Epoch: 411 [98560/225000 (44%)] Loss: 6712.013672\n",
      "Train Epoch: 411 [102656/225000 (46%)] Loss: 6676.427734\n",
      "Train Epoch: 411 [106752/225000 (47%)] Loss: 6604.123047\n",
      "Train Epoch: 411 [110848/225000 (49%)] Loss: 6684.519531\n",
      "Train Epoch: 411 [114944/225000 (51%)] Loss: 6784.761719\n",
      "Train Epoch: 411 [119040/225000 (53%)] Loss: 6795.054688\n",
      "Train Epoch: 411 [123136/225000 (55%)] Loss: 6667.337891\n",
      "Train Epoch: 411 [127232/225000 (57%)] Loss: 6751.212891\n",
      "Train Epoch: 411 [131328/225000 (58%)] Loss: 6751.843750\n",
      "Train Epoch: 411 [135424/225000 (60%)] Loss: 6703.519531\n",
      "Train Epoch: 411 [139520/225000 (62%)] Loss: 6901.408203\n",
      "Train Epoch: 411 [143616/225000 (64%)] Loss: 6886.810547\n",
      "Train Epoch: 411 [147712/225000 (66%)] Loss: 6686.148438\n",
      "Train Epoch: 411 [151808/225000 (67%)] Loss: 6744.605469\n",
      "Train Epoch: 411 [155904/225000 (69%)] Loss: 6613.691406\n",
      "Train Epoch: 411 [160000/225000 (71%)] Loss: 6816.400391\n",
      "Train Epoch: 411 [164096/225000 (73%)] Loss: 7008.273438\n",
      "Train Epoch: 411 [168192/225000 (75%)] Loss: 6779.009766\n",
      "Train Epoch: 411 [172288/225000 (77%)] Loss: 6853.193359\n",
      "Train Epoch: 411 [176384/225000 (78%)] Loss: 6887.726562\n",
      "Train Epoch: 411 [180480/225000 (80%)] Loss: 6787.783203\n",
      "Train Epoch: 411 [184576/225000 (82%)] Loss: 6716.328125\n",
      "Train Epoch: 411 [188672/225000 (84%)] Loss: 6930.042969\n",
      "Train Epoch: 411 [192768/225000 (86%)] Loss: 6900.378906\n",
      "Train Epoch: 411 [196864/225000 (87%)] Loss: 6643.380859\n",
      "Train Epoch: 411 [200960/225000 (89%)] Loss: 6782.564453\n",
      "Train Epoch: 411 [205056/225000 (91%)] Loss: 6783.664062\n",
      "Train Epoch: 411 [209152/225000 (93%)] Loss: 6676.746094\n",
      "Train Epoch: 411 [213248/225000 (95%)] Loss: 6895.021484\n",
      "Train Epoch: 411 [217344/225000 (97%)] Loss: 6888.800781\n",
      "Train Epoch: 411 [221440/225000 (98%)] Loss: 6691.591797\n",
      "    epoch          : 411\n",
      "    loss           : 6797.312131150455\n",
      "    val_loss       : 6803.900168334952\n",
      "Train Epoch: 412 [256/225000 (0%)] Loss: 6696.207031\n",
      "Train Epoch: 412 [4352/225000 (2%)] Loss: 6651.523438\n",
      "Train Epoch: 412 [8448/225000 (4%)] Loss: 6801.527344\n",
      "Train Epoch: 412 [12544/225000 (6%)] Loss: 6819.871094\n",
      "Train Epoch: 412 [16640/225000 (7%)] Loss: 6888.546875\n",
      "Train Epoch: 412 [20736/225000 (9%)] Loss: 6739.957031\n",
      "Train Epoch: 412 [24832/225000 (11%)] Loss: 6927.519531\n",
      "Train Epoch: 412 [28928/225000 (13%)] Loss: 6981.447266\n",
      "Train Epoch: 412 [33024/225000 (15%)] Loss: 6701.125000\n",
      "Train Epoch: 412 [37120/225000 (16%)] Loss: 6702.945312\n",
      "Train Epoch: 412 [41216/225000 (18%)] Loss: 6797.304688\n",
      "Train Epoch: 412 [45312/225000 (20%)] Loss: 6727.351562\n",
      "Train Epoch: 412 [49408/225000 (22%)] Loss: 6610.933594\n",
      "Train Epoch: 412 [53504/225000 (24%)] Loss: 6869.712891\n",
      "Train Epoch: 412 [57600/225000 (26%)] Loss: 6713.638672\n",
      "Train Epoch: 412 [61696/225000 (27%)] Loss: 6795.074219\n",
      "Train Epoch: 412 [65792/225000 (29%)] Loss: 6724.439453\n",
      "Train Epoch: 412 [69888/225000 (31%)] Loss: 6748.800781\n",
      "Train Epoch: 412 [73984/225000 (33%)] Loss: 6894.691406\n",
      "Train Epoch: 412 [78080/225000 (35%)] Loss: 6614.355469\n",
      "Train Epoch: 412 [82176/225000 (37%)] Loss: 7019.726562\n",
      "Train Epoch: 412 [86272/225000 (38%)] Loss: 6857.285156\n",
      "Train Epoch: 412 [90368/225000 (40%)] Loss: 6660.503906\n",
      "Train Epoch: 412 [94464/225000 (42%)] Loss: 6661.529297\n",
      "Train Epoch: 412 [98560/225000 (44%)] Loss: 6828.332031\n",
      "Train Epoch: 412 [102656/225000 (46%)] Loss: 6675.177734\n",
      "Train Epoch: 412 [106752/225000 (47%)] Loss: 6747.939453\n",
      "Train Epoch: 412 [110848/225000 (49%)] Loss: 6759.271484\n",
      "Train Epoch: 412 [114944/225000 (51%)] Loss: 6716.058594\n",
      "Train Epoch: 412 [119040/225000 (53%)] Loss: 6754.865234\n",
      "Train Epoch: 412 [123136/225000 (55%)] Loss: 6899.994141\n",
      "Train Epoch: 412 [127232/225000 (57%)] Loss: 6721.894531\n",
      "Train Epoch: 412 [131328/225000 (58%)] Loss: 6728.476562\n",
      "Train Epoch: 412 [135424/225000 (60%)] Loss: 6862.333984\n",
      "Train Epoch: 412 [139520/225000 (62%)] Loss: 6652.443359\n",
      "Train Epoch: 412 [143616/225000 (64%)] Loss: 6804.128906\n",
      "Train Epoch: 412 [147712/225000 (66%)] Loss: 6870.130859\n",
      "Train Epoch: 412 [151808/225000 (67%)] Loss: 6866.593750\n",
      "Train Epoch: 412 [155904/225000 (69%)] Loss: 6736.775391\n",
      "Train Epoch: 412 [160000/225000 (71%)] Loss: 6738.892578\n",
      "Train Epoch: 412 [164096/225000 (73%)] Loss: 6865.695312\n",
      "Train Epoch: 412 [168192/225000 (75%)] Loss: 6803.675781\n",
      "Train Epoch: 412 [172288/225000 (77%)] Loss: 6844.943359\n",
      "Train Epoch: 412 [176384/225000 (78%)] Loss: 6764.591797\n",
      "Train Epoch: 412 [180480/225000 (80%)] Loss: 6761.384766\n",
      "Train Epoch: 412 [184576/225000 (82%)] Loss: 6804.558594\n",
      "Train Epoch: 412 [188672/225000 (84%)] Loss: 6806.634766\n",
      "Train Epoch: 412 [192768/225000 (86%)] Loss: 6762.931641\n",
      "Train Epoch: 412 [196864/225000 (87%)] Loss: 6757.574219\n",
      "Train Epoch: 412 [200960/225000 (89%)] Loss: 6771.509766\n",
      "Train Epoch: 412 [205056/225000 (91%)] Loss: 6746.535156\n",
      "Train Epoch: 412 [209152/225000 (93%)] Loss: 6725.458984\n",
      "Train Epoch: 412 [213248/225000 (95%)] Loss: 6859.367188\n",
      "Train Epoch: 412 [217344/225000 (97%)] Loss: 6861.443359\n",
      "Train Epoch: 412 [221440/225000 (98%)] Loss: 6798.751953\n",
      "    epoch          : 412\n",
      "    loss           : 6785.736969167733\n",
      "    val_loss       : 6804.634741625007\n",
      "Train Epoch: 413 [256/225000 (0%)] Loss: 6780.306641\n",
      "Train Epoch: 413 [4352/225000 (2%)] Loss: 6712.339844\n",
      "Train Epoch: 413 [8448/225000 (4%)] Loss: 6888.384766\n",
      "Train Epoch: 413 [12544/225000 (6%)] Loss: 6927.097656\n",
      "Train Epoch: 413 [16640/225000 (7%)] Loss: 6794.953125\n",
      "Train Epoch: 413 [20736/225000 (9%)] Loss: 6827.056641\n",
      "Train Epoch: 413 [24832/225000 (11%)] Loss: 6685.384766\n",
      "Train Epoch: 413 [28928/225000 (13%)] Loss: 6827.843750\n",
      "Train Epoch: 413 [33024/225000 (15%)] Loss: 6694.611328\n",
      "Train Epoch: 413 [37120/225000 (16%)] Loss: 6756.728516\n",
      "Train Epoch: 413 [41216/225000 (18%)] Loss: 6849.746094\n",
      "Train Epoch: 413 [45312/225000 (20%)] Loss: 6746.322266\n",
      "Train Epoch: 413 [49408/225000 (22%)] Loss: 6694.365234\n",
      "Train Epoch: 413 [53504/225000 (24%)] Loss: 6777.548828\n",
      "Train Epoch: 413 [57600/225000 (26%)] Loss: 6834.640625\n",
      "Train Epoch: 413 [61696/225000 (27%)] Loss: 6699.158203\n",
      "Train Epoch: 413 [65792/225000 (29%)] Loss: 6777.044922\n",
      "Train Epoch: 413 [69888/225000 (31%)] Loss: 6845.140625\n",
      "Train Epoch: 413 [73984/225000 (33%)] Loss: 6828.443359\n",
      "Train Epoch: 413 [78080/225000 (35%)] Loss: 6791.015625\n",
      "Train Epoch: 413 [82176/225000 (37%)] Loss: 6896.863281\n",
      "Train Epoch: 413 [86272/225000 (38%)] Loss: 6875.025391\n",
      "Train Epoch: 413 [90368/225000 (40%)] Loss: 6699.171875\n",
      "Train Epoch: 413 [94464/225000 (42%)] Loss: 6984.296875\n",
      "Train Epoch: 413 [98560/225000 (44%)] Loss: 6783.984375\n",
      "Train Epoch: 413 [102656/225000 (46%)] Loss: 6792.494141\n",
      "Train Epoch: 413 [106752/225000 (47%)] Loss: 6836.427734\n",
      "Train Epoch: 413 [110848/225000 (49%)] Loss: 6661.121094\n",
      "Train Epoch: 413 [114944/225000 (51%)] Loss: 6807.507812\n",
      "Train Epoch: 413 [119040/225000 (53%)] Loss: 6673.839844\n",
      "Train Epoch: 413 [123136/225000 (55%)] Loss: 6611.251953\n",
      "Train Epoch: 413 [127232/225000 (57%)] Loss: 6727.287109\n",
      "Train Epoch: 413 [131328/225000 (58%)] Loss: 6869.037109\n",
      "Train Epoch: 413 [135424/225000 (60%)] Loss: 7073.003906\n",
      "Train Epoch: 413 [139520/225000 (62%)] Loss: 6805.097656\n",
      "Train Epoch: 413 [143616/225000 (64%)] Loss: 6725.353516\n",
      "Train Epoch: 413 [147712/225000 (66%)] Loss: 6782.529297\n",
      "Train Epoch: 413 [151808/225000 (67%)] Loss: 6865.675781\n",
      "Train Epoch: 413 [155904/225000 (69%)] Loss: 6674.955078\n",
      "Train Epoch: 413 [160000/225000 (71%)] Loss: 6701.062500\n",
      "Train Epoch: 413 [164096/225000 (73%)] Loss: 6680.705078\n",
      "Train Epoch: 413 [168192/225000 (75%)] Loss: 6792.181641\n",
      "Train Epoch: 413 [172288/225000 (77%)] Loss: 6767.701172\n",
      "Train Epoch: 413 [176384/225000 (78%)] Loss: 6641.656250\n",
      "Train Epoch: 413 [180480/225000 (80%)] Loss: 6637.832031\n",
      "Train Epoch: 413 [184576/225000 (82%)] Loss: 6799.199219\n",
      "Train Epoch: 413 [188672/225000 (84%)] Loss: 6847.068359\n",
      "Train Epoch: 413 [192768/225000 (86%)] Loss: 6829.136719\n",
      "Train Epoch: 413 [196864/225000 (87%)] Loss: 6897.085938\n",
      "Train Epoch: 413 [200960/225000 (89%)] Loss: 6588.669922\n",
      "Train Epoch: 413 [205056/225000 (91%)] Loss: 6721.982422\n",
      "Train Epoch: 413 [209152/225000 (93%)] Loss: 6841.728516\n",
      "Train Epoch: 413 [213248/225000 (95%)] Loss: 6840.085938\n",
      "Train Epoch: 413 [217344/225000 (97%)] Loss: 6669.113281\n",
      "Train Epoch: 413 [221440/225000 (98%)] Loss: 6773.470703\n",
      "    epoch          : 413\n",
      "    loss           : 6797.931646179963\n",
      "    val_loss       : 6801.0330505346765\n",
      "Train Epoch: 414 [256/225000 (0%)] Loss: 6650.921875\n",
      "Train Epoch: 414 [4352/225000 (2%)] Loss: 6892.638672\n",
      "Train Epoch: 414 [8448/225000 (4%)] Loss: 6763.271484\n",
      "Train Epoch: 414 [12544/225000 (6%)] Loss: 6822.509766\n",
      "Train Epoch: 414 [16640/225000 (7%)] Loss: 6817.007812\n",
      "Train Epoch: 414 [20736/225000 (9%)] Loss: 6702.236328\n",
      "Train Epoch: 414 [24832/225000 (11%)] Loss: 6835.648438\n",
      "Train Epoch: 414 [28928/225000 (13%)] Loss: 6839.466797\n",
      "Train Epoch: 414 [33024/225000 (15%)] Loss: 6676.193359\n",
      "Train Epoch: 414 [37120/225000 (16%)] Loss: 6559.839844\n",
      "Train Epoch: 414 [41216/225000 (18%)] Loss: 6891.683594\n",
      "Train Epoch: 414 [45312/225000 (20%)] Loss: 6758.570312\n",
      "Train Epoch: 414 [49408/225000 (22%)] Loss: 6894.318359\n",
      "Train Epoch: 414 [53504/225000 (24%)] Loss: 6673.056641\n",
      "Train Epoch: 414 [57600/225000 (26%)] Loss: 6728.347656\n",
      "Train Epoch: 414 [61696/225000 (27%)] Loss: 6804.984375\n",
      "Train Epoch: 414 [65792/225000 (29%)] Loss: 6823.939453\n",
      "Train Epoch: 414 [69888/225000 (31%)] Loss: 6805.738281\n",
      "Train Epoch: 414 [73984/225000 (33%)] Loss: 6773.740234\n",
      "Train Epoch: 414 [78080/225000 (35%)] Loss: 6790.410156\n",
      "Train Epoch: 414 [82176/225000 (37%)] Loss: 6843.072266\n",
      "Train Epoch: 414 [86272/225000 (38%)] Loss: 6800.869141\n",
      "Train Epoch: 414 [90368/225000 (40%)] Loss: 6785.503906\n",
      "Train Epoch: 414 [94464/225000 (42%)] Loss: 6816.773438\n",
      "Train Epoch: 414 [98560/225000 (44%)] Loss: 6701.941406\n",
      "Train Epoch: 414 [102656/225000 (46%)] Loss: 6744.308594\n",
      "Train Epoch: 414 [106752/225000 (47%)] Loss: 6589.322266\n",
      "Train Epoch: 414 [110848/225000 (49%)] Loss: 6682.253906\n",
      "Train Epoch: 414 [114944/225000 (51%)] Loss: 6656.693359\n",
      "Train Epoch: 414 [119040/225000 (53%)] Loss: 6786.654297\n",
      "Train Epoch: 414 [123136/225000 (55%)] Loss: 6768.580078\n",
      "Train Epoch: 414 [127232/225000 (57%)] Loss: 6802.681641\n",
      "Train Epoch: 414 [131328/225000 (58%)] Loss: 6694.275391\n",
      "Train Epoch: 414 [135424/225000 (60%)] Loss: 6867.535156\n",
      "Train Epoch: 414 [139520/225000 (62%)] Loss: 6661.281250\n",
      "Train Epoch: 414 [143616/225000 (64%)] Loss: 6726.726562\n",
      "Train Epoch: 414 [147712/225000 (66%)] Loss: 6744.341797\n",
      "Train Epoch: 414 [151808/225000 (67%)] Loss: 6644.212891\n",
      "Train Epoch: 414 [155904/225000 (69%)] Loss: 6730.863281\n",
      "Train Epoch: 414 [160000/225000 (71%)] Loss: 6865.845703\n",
      "Train Epoch: 414 [164096/225000 (73%)] Loss: 6688.351562\n",
      "Train Epoch: 414 [168192/225000 (75%)] Loss: 6674.652344\n",
      "Train Epoch: 414 [172288/225000 (77%)] Loss: 6731.861328\n",
      "Train Epoch: 414 [176384/225000 (78%)] Loss: 6694.144531\n",
      "Train Epoch: 414 [180480/225000 (80%)] Loss: 6777.978516\n",
      "Train Epoch: 414 [184576/225000 (82%)] Loss: 6753.691406\n",
      "Train Epoch: 414 [188672/225000 (84%)] Loss: 6679.591797\n",
      "Train Epoch: 414 [192768/225000 (86%)] Loss: 6783.470703\n",
      "Train Epoch: 414 [196864/225000 (87%)] Loss: 6794.154297\n",
      "Train Epoch: 414 [200960/225000 (89%)] Loss: 6864.722656\n",
      "Train Epoch: 414 [205056/225000 (91%)] Loss: 6859.068359\n",
      "Train Epoch: 414 [209152/225000 (93%)] Loss: 6749.195312\n",
      "Train Epoch: 414 [213248/225000 (95%)] Loss: 6611.025391\n",
      "Train Epoch: 414 [217344/225000 (97%)] Loss: 6841.277344\n",
      "Train Epoch: 414 [221440/225000 (98%)] Loss: 6812.117188\n",
      "    epoch          : 414\n",
      "    loss           : 6804.342421252844\n",
      "    val_loss       : 6810.104546150383\n",
      "Train Epoch: 415 [256/225000 (0%)] Loss: 6841.572266\n",
      "Train Epoch: 415 [4352/225000 (2%)] Loss: 6745.568359\n",
      "Train Epoch: 415 [8448/225000 (4%)] Loss: 6803.484375\n",
      "Train Epoch: 415 [12544/225000 (6%)] Loss: 6749.347656\n",
      "Train Epoch: 415 [16640/225000 (7%)] Loss: 6817.755859\n",
      "Train Epoch: 415 [20736/225000 (9%)] Loss: 6821.228516\n",
      "Train Epoch: 415 [24832/225000 (11%)] Loss: 6782.921875\n",
      "Train Epoch: 415 [28928/225000 (13%)] Loss: 6669.976562\n",
      "Train Epoch: 415 [33024/225000 (15%)] Loss: 6774.361328\n",
      "Train Epoch: 415 [37120/225000 (16%)] Loss: 6842.890625\n",
      "Train Epoch: 415 [41216/225000 (18%)] Loss: 6698.615234\n",
      "Train Epoch: 415 [45312/225000 (20%)] Loss: 6778.166016\n",
      "Train Epoch: 415 [49408/225000 (22%)] Loss: 6819.789062\n",
      "Train Epoch: 415 [53504/225000 (24%)] Loss: 6739.521484\n",
      "Train Epoch: 415 [57600/225000 (26%)] Loss: 6676.841797\n",
      "Train Epoch: 415 [61696/225000 (27%)] Loss: 6925.250000\n",
      "Train Epoch: 415 [65792/225000 (29%)] Loss: 6746.787109\n",
      "Train Epoch: 415 [69888/225000 (31%)] Loss: 6830.955078\n",
      "Train Epoch: 415 [73984/225000 (33%)] Loss: 6871.167969\n",
      "Train Epoch: 415 [78080/225000 (35%)] Loss: 6735.216797\n",
      "Train Epoch: 415 [82176/225000 (37%)] Loss: 6763.675781\n",
      "Train Epoch: 415 [86272/225000 (38%)] Loss: 6858.013672\n",
      "Train Epoch: 415 [90368/225000 (40%)] Loss: 6683.810547\n",
      "Train Epoch: 415 [94464/225000 (42%)] Loss: 6703.738281\n",
      "Train Epoch: 415 [98560/225000 (44%)] Loss: 6710.654297\n",
      "Train Epoch: 415 [102656/225000 (46%)] Loss: 6885.746094\n",
      "Train Epoch: 415 [106752/225000 (47%)] Loss: 6860.904297\n",
      "Train Epoch: 415 [110848/225000 (49%)] Loss: 6786.062500\n",
      "Train Epoch: 415 [114944/225000 (51%)] Loss: 6711.195312\n",
      "Train Epoch: 415 [119040/225000 (53%)] Loss: 6812.265625\n",
      "Train Epoch: 415 [123136/225000 (55%)] Loss: 6622.062500\n",
      "Train Epoch: 415 [127232/225000 (57%)] Loss: 6769.871094\n",
      "Train Epoch: 415 [131328/225000 (58%)] Loss: 6781.052734\n",
      "Train Epoch: 415 [135424/225000 (60%)] Loss: 6774.498047\n",
      "Train Epoch: 415 [139520/225000 (62%)] Loss: 6817.388672\n",
      "Train Epoch: 415 [143616/225000 (64%)] Loss: 6714.503906\n",
      "Train Epoch: 415 [147712/225000 (66%)] Loss: 6785.382812\n",
      "Train Epoch: 415 [151808/225000 (67%)] Loss: 6799.468750\n",
      "Train Epoch: 415 [155904/225000 (69%)] Loss: 6807.701172\n",
      "Train Epoch: 415 [160000/225000 (71%)] Loss: 7029.849609\n",
      "Train Epoch: 415 [164096/225000 (73%)] Loss: 6898.326172\n",
      "Train Epoch: 415 [168192/225000 (75%)] Loss: 6727.273438\n",
      "Train Epoch: 415 [172288/225000 (77%)] Loss: 6743.808594\n",
      "Train Epoch: 415 [176384/225000 (78%)] Loss: 6888.113281\n",
      "Train Epoch: 415 [180480/225000 (80%)] Loss: 6784.548828\n",
      "Train Epoch: 415 [184576/225000 (82%)] Loss: 6934.585938\n",
      "Train Epoch: 415 [188672/225000 (84%)] Loss: 6799.164062\n",
      "Train Epoch: 415 [192768/225000 (86%)] Loss: 6813.255859\n",
      "Train Epoch: 415 [196864/225000 (87%)] Loss: 6804.296875\n",
      "Train Epoch: 415 [200960/225000 (89%)] Loss: 6704.181641\n",
      "Train Epoch: 415 [205056/225000 (91%)] Loss: 6714.941406\n",
      "Train Epoch: 415 [209152/225000 (93%)] Loss: 6754.183594\n",
      "Train Epoch: 415 [213248/225000 (95%)] Loss: 6830.214844\n",
      "Train Epoch: 415 [217344/225000 (97%)] Loss: 6641.539062\n",
      "Train Epoch: 415 [221440/225000 (98%)] Loss: 6821.998047\n",
      "    epoch          : 415\n",
      "    loss           : 6781.885879950583\n",
      "    val_loss       : 6797.806253849244\n",
      "Train Epoch: 416 [256/225000 (0%)] Loss: 6826.052734\n",
      "Train Epoch: 416 [4352/225000 (2%)] Loss: 6769.220703\n",
      "Train Epoch: 416 [8448/225000 (4%)] Loss: 6909.976562\n",
      "Train Epoch: 416 [12544/225000 (6%)] Loss: 6792.423828\n",
      "Train Epoch: 416 [16640/225000 (7%)] Loss: 6902.533203\n",
      "Train Epoch: 416 [20736/225000 (9%)] Loss: 6542.894531\n",
      "Train Epoch: 416 [24832/225000 (11%)] Loss: 6776.035156\n",
      "Train Epoch: 416 [28928/225000 (13%)] Loss: 6823.773438\n",
      "Train Epoch: 416 [33024/225000 (15%)] Loss: 6809.849609\n",
      "Train Epoch: 416 [37120/225000 (16%)] Loss: 6795.035156\n",
      "Train Epoch: 416 [41216/225000 (18%)] Loss: 6786.556641\n",
      "Train Epoch: 416 [45312/225000 (20%)] Loss: 6819.544922\n",
      "Train Epoch: 416 [49408/225000 (22%)] Loss: 6747.699219\n",
      "Train Epoch: 416 [53504/225000 (24%)] Loss: 6873.873047\n",
      "Train Epoch: 416 [57600/225000 (26%)] Loss: 6732.718750\n",
      "Train Epoch: 416 [61696/225000 (27%)] Loss: 6699.769531\n",
      "Train Epoch: 416 [65792/225000 (29%)] Loss: 6630.435547\n",
      "Train Epoch: 416 [69888/225000 (31%)] Loss: 6756.638672\n",
      "Train Epoch: 416 [73984/225000 (33%)] Loss: 6895.080078\n",
      "Train Epoch: 416 [78080/225000 (35%)] Loss: 6831.746094\n",
      "Train Epoch: 416 [82176/225000 (37%)] Loss: 6782.621094\n",
      "Train Epoch: 416 [86272/225000 (38%)] Loss: 6596.994141\n",
      "Train Epoch: 416 [90368/225000 (40%)] Loss: 6779.080078\n",
      "Train Epoch: 416 [94464/225000 (42%)] Loss: 6667.976562\n",
      "Train Epoch: 416 [98560/225000 (44%)] Loss: 6835.615234\n",
      "Train Epoch: 416 [102656/225000 (46%)] Loss: 6714.607422\n",
      "Train Epoch: 416 [106752/225000 (47%)] Loss: 6853.988281\n",
      "Train Epoch: 416 [110848/225000 (49%)] Loss: 6685.250000\n",
      "Train Epoch: 416 [114944/225000 (51%)] Loss: 6713.759766\n",
      "Train Epoch: 416 [119040/225000 (53%)] Loss: 6741.275391\n",
      "Train Epoch: 416 [123136/225000 (55%)] Loss: 6858.302734\n",
      "Train Epoch: 416 [127232/225000 (57%)] Loss: 6676.744141\n",
      "Train Epoch: 416 [131328/225000 (58%)] Loss: 6921.878906\n",
      "Train Epoch: 416 [135424/225000 (60%)] Loss: 6680.183594\n",
      "Train Epoch: 416 [139520/225000 (62%)] Loss: 6708.494141\n",
      "Train Epoch: 416 [143616/225000 (64%)] Loss: 6881.636719\n",
      "Train Epoch: 416 [147712/225000 (66%)] Loss: 6764.130859\n",
      "Train Epoch: 416 [151808/225000 (67%)] Loss: 7079.988281\n",
      "Train Epoch: 416 [155904/225000 (69%)] Loss: 6916.837891\n",
      "Train Epoch: 416 [160000/225000 (71%)] Loss: 6723.398438\n",
      "Train Epoch: 416 [164096/225000 (73%)] Loss: 6831.060547\n",
      "Train Epoch: 416 [168192/225000 (75%)] Loss: 6901.539062\n",
      "Train Epoch: 416 [172288/225000 (77%)] Loss: 6793.328125\n",
      "Train Epoch: 416 [176384/225000 (78%)] Loss: 6960.791016\n",
      "Train Epoch: 416 [180480/225000 (80%)] Loss: 6831.195312\n",
      "Train Epoch: 416 [184576/225000 (82%)] Loss: 6702.841797\n",
      "Train Epoch: 416 [188672/225000 (84%)] Loss: 6708.267578\n",
      "Train Epoch: 416 [192768/225000 (86%)] Loss: 6758.689453\n",
      "Train Epoch: 416 [196864/225000 (87%)] Loss: 6692.519531\n",
      "Train Epoch: 416 [200960/225000 (89%)] Loss: 6872.279297\n",
      "Train Epoch: 416 [205056/225000 (91%)] Loss: 6887.460938\n",
      "Train Epoch: 416 [209152/225000 (93%)] Loss: 6849.478516\n",
      "Train Epoch: 416 [213248/225000 (95%)] Loss: 6772.714844\n",
      "Train Epoch: 416 [217344/225000 (97%)] Loss: 6778.300781\n",
      "Train Epoch: 416 [221440/225000 (98%)] Loss: 6731.054688\n",
      "    epoch          : 416\n",
      "    loss           : 6798.132553638723\n",
      "    val_loss       : 6798.35367723752\n",
      "Train Epoch: 417 [256/225000 (0%)] Loss: 6782.228516\n",
      "Train Epoch: 417 [4352/225000 (2%)] Loss: 6849.794922\n",
      "Train Epoch: 417 [8448/225000 (4%)] Loss: 6882.830078\n",
      "Train Epoch: 417 [12544/225000 (6%)] Loss: 6765.837891\n",
      "Train Epoch: 417 [16640/225000 (7%)] Loss: 6771.732422\n",
      "Train Epoch: 417 [20736/225000 (9%)] Loss: 6659.570312\n",
      "Train Epoch: 417 [24832/225000 (11%)] Loss: 6739.884766\n",
      "Train Epoch: 417 [28928/225000 (13%)] Loss: 6774.359375\n",
      "Train Epoch: 417 [33024/225000 (15%)] Loss: 6786.250000\n",
      "Train Epoch: 417 [37120/225000 (16%)] Loss: 6717.011719\n",
      "Train Epoch: 417 [41216/225000 (18%)] Loss: 6837.832031\n",
      "Train Epoch: 417 [45312/225000 (20%)] Loss: 6801.679688\n",
      "Train Epoch: 417 [49408/225000 (22%)] Loss: 6811.406250\n",
      "Train Epoch: 417 [53504/225000 (24%)] Loss: 6790.472656\n",
      "Train Epoch: 417 [57600/225000 (26%)] Loss: 6825.013672\n",
      "Train Epoch: 417 [61696/225000 (27%)] Loss: 6877.044922\n",
      "Train Epoch: 417 [65792/225000 (29%)] Loss: 6696.513672\n",
      "Train Epoch: 417 [69888/225000 (31%)] Loss: 6653.804688\n",
      "Train Epoch: 417 [73984/225000 (33%)] Loss: 6991.390625\n",
      "Train Epoch: 417 [78080/225000 (35%)] Loss: 6825.339844\n",
      "Train Epoch: 417 [82176/225000 (37%)] Loss: 6641.849609\n",
      "Train Epoch: 417 [86272/225000 (38%)] Loss: 6897.330078\n",
      "Train Epoch: 417 [90368/225000 (40%)] Loss: 6809.689453\n",
      "Train Epoch: 417 [94464/225000 (42%)] Loss: 6781.792969\n",
      "Train Epoch: 417 [98560/225000 (44%)] Loss: 6736.935547\n",
      "Train Epoch: 417 [102656/225000 (46%)] Loss: 6707.416016\n",
      "Train Epoch: 417 [106752/225000 (47%)] Loss: 6852.447266\n",
      "Train Epoch: 417 [110848/225000 (49%)] Loss: 6745.150391\n",
      "Train Epoch: 417 [114944/225000 (51%)] Loss: 6590.980469\n",
      "Train Epoch: 417 [119040/225000 (53%)] Loss: 6868.255859\n",
      "Train Epoch: 417 [123136/225000 (55%)] Loss: 6913.361328\n",
      "Train Epoch: 417 [127232/225000 (57%)] Loss: 7067.382812\n",
      "Train Epoch: 417 [131328/225000 (58%)] Loss: 6867.072266\n",
      "Train Epoch: 417 [135424/225000 (60%)] Loss: 6653.824219\n",
      "Train Epoch: 417 [139520/225000 (62%)] Loss: 6651.462891\n",
      "Train Epoch: 417 [143616/225000 (64%)] Loss: 6753.398438\n",
      "Train Epoch: 417 [147712/225000 (66%)] Loss: 7018.201172\n",
      "Train Epoch: 417 [151808/225000 (67%)] Loss: 6749.855469\n",
      "Train Epoch: 417 [155904/225000 (69%)] Loss: 6706.886719\n",
      "Train Epoch: 417 [160000/225000 (71%)] Loss: 6704.097656\n",
      "Train Epoch: 417 [164096/225000 (73%)] Loss: 6931.988281\n",
      "Train Epoch: 417 [168192/225000 (75%)] Loss: 6736.146484\n",
      "Train Epoch: 417 [172288/225000 (77%)] Loss: 6628.425781\n",
      "Train Epoch: 417 [176384/225000 (78%)] Loss: 6855.648438\n",
      "Train Epoch: 417 [180480/225000 (80%)] Loss: 6725.904297\n",
      "Train Epoch: 417 [184576/225000 (82%)] Loss: 6853.876953\n",
      "Train Epoch: 417 [188672/225000 (84%)] Loss: 6789.361328\n",
      "Train Epoch: 417 [192768/225000 (86%)] Loss: 6857.296875\n",
      "Train Epoch: 417 [196864/225000 (87%)] Loss: 6802.185547\n",
      "Train Epoch: 417 [200960/225000 (89%)] Loss: 6882.914062\n",
      "Train Epoch: 417 [205056/225000 (91%)] Loss: 6822.324219\n",
      "Train Epoch: 417 [209152/225000 (93%)] Loss: 6895.787109\n",
      "Train Epoch: 417 [213248/225000 (95%)] Loss: 6906.015625\n",
      "Train Epoch: 417 [217344/225000 (97%)] Loss: 6822.527344\n",
      "Train Epoch: 417 [221440/225000 (98%)] Loss: 6734.031250\n",
      "    epoch          : 417\n",
      "    loss           : 6779.026240534343\n",
      "    val_loss       : 6797.525794279819\n",
      "Train Epoch: 418 [256/225000 (0%)] Loss: 6872.792969\n",
      "Train Epoch: 418 [4352/225000 (2%)] Loss: 6936.052734\n",
      "Train Epoch: 418 [8448/225000 (4%)] Loss: 6483.796875\n",
      "Train Epoch: 418 [12544/225000 (6%)] Loss: 6762.269531\n",
      "Train Epoch: 418 [16640/225000 (7%)] Loss: 6731.753906\n",
      "Train Epoch: 418 [20736/225000 (9%)] Loss: 6603.822266\n",
      "Train Epoch: 418 [24832/225000 (11%)] Loss: 6854.435547\n",
      "Train Epoch: 418 [28928/225000 (13%)] Loss: 6835.787109\n",
      "Train Epoch: 418 [33024/225000 (15%)] Loss: 6670.058594\n",
      "Train Epoch: 418 [37120/225000 (16%)] Loss: 6735.478516\n",
      "Train Epoch: 418 [41216/225000 (18%)] Loss: 6726.464844\n",
      "Train Epoch: 418 [45312/225000 (20%)] Loss: 6769.607422\n",
      "Train Epoch: 418 [49408/225000 (22%)] Loss: 6683.097656\n",
      "Train Epoch: 418 [53504/225000 (24%)] Loss: 6762.248047\n",
      "Train Epoch: 418 [57600/225000 (26%)] Loss: 6606.501953\n",
      "Train Epoch: 418 [61696/225000 (27%)] Loss: 6707.892578\n",
      "Train Epoch: 418 [65792/225000 (29%)] Loss: 6919.001953\n",
      "Train Epoch: 418 [69888/225000 (31%)] Loss: 6725.638672\n",
      "Train Epoch: 418 [73984/225000 (33%)] Loss: 7027.427734\n",
      "Train Epoch: 418 [78080/225000 (35%)] Loss: 6596.173828\n",
      "Train Epoch: 418 [82176/225000 (37%)] Loss: 6826.175781\n",
      "Train Epoch: 418 [86272/225000 (38%)] Loss: 6745.173828\n",
      "Train Epoch: 418 [90368/225000 (40%)] Loss: 6840.955078\n",
      "Train Epoch: 418 [94464/225000 (42%)] Loss: 6809.687500\n",
      "Train Epoch: 418 [98560/225000 (44%)] Loss: 6759.882812\n",
      "Train Epoch: 418 [102656/225000 (46%)] Loss: 6739.863281\n",
      "Train Epoch: 418 [106752/225000 (47%)] Loss: 6846.443359\n",
      "Train Epoch: 418 [110848/225000 (49%)] Loss: 6843.322266\n",
      "Train Epoch: 418 [114944/225000 (51%)] Loss: 6708.558594\n",
      "Train Epoch: 418 [119040/225000 (53%)] Loss: 6855.296875\n",
      "Train Epoch: 418 [123136/225000 (55%)] Loss: 6684.898438\n",
      "Train Epoch: 418 [127232/225000 (57%)] Loss: 6755.164062\n",
      "Train Epoch: 418 [131328/225000 (58%)] Loss: 6865.201172\n",
      "Train Epoch: 418 [135424/225000 (60%)] Loss: 6698.412109\n",
      "Train Epoch: 418 [139520/225000 (62%)] Loss: 6670.574219\n",
      "Train Epoch: 418 [143616/225000 (64%)] Loss: 6873.134766\n",
      "Train Epoch: 418 [147712/225000 (66%)] Loss: 6830.222656\n",
      "Train Epoch: 418 [151808/225000 (67%)] Loss: 6806.382812\n",
      "Train Epoch: 418 [155904/225000 (69%)] Loss: 6930.259766\n",
      "Train Epoch: 418 [160000/225000 (71%)] Loss: 6784.201172\n",
      "Train Epoch: 418 [164096/225000 (73%)] Loss: 6825.589844\n",
      "Train Epoch: 418 [168192/225000 (75%)] Loss: 6656.082031\n",
      "Train Epoch: 418 [172288/225000 (77%)] Loss: 6919.521484\n",
      "Train Epoch: 418 [176384/225000 (78%)] Loss: 6763.160156\n",
      "Train Epoch: 418 [180480/225000 (80%)] Loss: 6751.826172\n",
      "Train Epoch: 418 [184576/225000 (82%)] Loss: 6865.865234\n",
      "Train Epoch: 418 [188672/225000 (84%)] Loss: 6773.882812\n",
      "Train Epoch: 418 [192768/225000 (86%)] Loss: 6704.162109\n",
      "Train Epoch: 418 [196864/225000 (87%)] Loss: 6755.214844\n",
      "Train Epoch: 418 [200960/225000 (89%)] Loss: 6852.925781\n",
      "Train Epoch: 418 [205056/225000 (91%)] Loss: 6879.031250\n",
      "Train Epoch: 418 [209152/225000 (93%)] Loss: 6823.660156\n",
      "Train Epoch: 418 [213248/225000 (95%)] Loss: 6775.539062\n",
      "Train Epoch: 418 [217344/225000 (97%)] Loss: 6694.255859\n",
      "Train Epoch: 418 [221440/225000 (98%)] Loss: 6833.589844\n",
      "    epoch          : 418\n",
      "    loss           : 6800.714002728598\n",
      "    val_loss       : 6795.555926709759\n",
      "Train Epoch: 419 [256/225000 (0%)] Loss: 6801.554688\n",
      "Train Epoch: 419 [4352/225000 (2%)] Loss: 6675.785156\n",
      "Train Epoch: 419 [8448/225000 (4%)] Loss: 6797.300781\n",
      "Train Epoch: 419 [12544/225000 (6%)] Loss: 6895.275391\n",
      "Train Epoch: 419 [16640/225000 (7%)] Loss: 6647.437500\n",
      "Train Epoch: 419 [20736/225000 (9%)] Loss: 6894.363281\n",
      "Train Epoch: 419 [24832/225000 (11%)] Loss: 6828.919922\n",
      "Train Epoch: 419 [28928/225000 (13%)] Loss: 6571.880859\n",
      "Train Epoch: 419 [33024/225000 (15%)] Loss: 6756.304688\n",
      "Train Epoch: 419 [37120/225000 (16%)] Loss: 6861.021484\n",
      "Train Epoch: 419 [41216/225000 (18%)] Loss: 6818.119141\n",
      "Train Epoch: 419 [45312/225000 (20%)] Loss: 6983.640625\n",
      "Train Epoch: 419 [49408/225000 (22%)] Loss: 6844.482422\n",
      "Train Epoch: 419 [53504/225000 (24%)] Loss: 6812.830078\n",
      "Train Epoch: 419 [57600/225000 (26%)] Loss: 6761.617188\n",
      "Train Epoch: 419 [61696/225000 (27%)] Loss: 6756.974609\n",
      "Train Epoch: 419 [65792/225000 (29%)] Loss: 6760.824219\n",
      "Train Epoch: 419 [69888/225000 (31%)] Loss: 6838.960938\n",
      "Train Epoch: 419 [73984/225000 (33%)] Loss: 6870.179688\n",
      "Train Epoch: 419 [78080/225000 (35%)] Loss: 6856.306641\n",
      "Train Epoch: 419 [82176/225000 (37%)] Loss: 6768.259766\n",
      "Train Epoch: 419 [86272/225000 (38%)] Loss: 6705.126953\n",
      "Train Epoch: 419 [90368/225000 (40%)] Loss: 6852.865234\n",
      "Train Epoch: 419 [94464/225000 (42%)] Loss: 6692.128906\n",
      "Train Epoch: 419 [98560/225000 (44%)] Loss: 6998.865234\n",
      "Train Epoch: 419 [102656/225000 (46%)] Loss: 6951.050781\n",
      "Train Epoch: 419 [106752/225000 (47%)] Loss: 6793.484375\n",
      "Train Epoch: 419 [110848/225000 (49%)] Loss: 6717.861328\n",
      "Train Epoch: 419 [114944/225000 (51%)] Loss: 6688.656250\n",
      "Train Epoch: 419 [119040/225000 (53%)] Loss: 6720.238281\n",
      "Train Epoch: 419 [123136/225000 (55%)] Loss: 6756.246094\n",
      "Train Epoch: 419 [127232/225000 (57%)] Loss: 6838.068359\n",
      "Train Epoch: 419 [131328/225000 (58%)] Loss: 6887.761719\n",
      "Train Epoch: 419 [135424/225000 (60%)] Loss: 6828.447266\n",
      "Train Epoch: 419 [139520/225000 (62%)] Loss: 6712.357422\n",
      "Train Epoch: 419 [143616/225000 (64%)] Loss: 6708.359375\n",
      "Train Epoch: 419 [147712/225000 (66%)] Loss: 6780.453125\n",
      "Train Epoch: 419 [151808/225000 (67%)] Loss: 6691.236328\n",
      "Train Epoch: 419 [155904/225000 (69%)] Loss: 6762.923828\n",
      "Train Epoch: 419 [160000/225000 (71%)] Loss: 6784.062500\n",
      "Train Epoch: 419 [164096/225000 (73%)] Loss: 6785.187500\n",
      "Train Epoch: 419 [168192/225000 (75%)] Loss: 6790.542969\n",
      "Train Epoch: 419 [172288/225000 (77%)] Loss: 6583.060547\n",
      "Train Epoch: 419 [176384/225000 (78%)] Loss: 6786.578125\n",
      "Train Epoch: 419 [180480/225000 (80%)] Loss: 6739.951172\n",
      "Train Epoch: 419 [184576/225000 (82%)] Loss: 6672.490234\n",
      "Train Epoch: 419 [188672/225000 (84%)] Loss: 6844.458984\n",
      "Train Epoch: 419 [192768/225000 (86%)] Loss: 6714.417969\n",
      "Train Epoch: 419 [196864/225000 (87%)] Loss: 6743.880859\n",
      "Train Epoch: 419 [200960/225000 (89%)] Loss: 6917.263672\n",
      "Train Epoch: 419 [205056/225000 (91%)] Loss: 6915.349609\n",
      "Train Epoch: 419 [209152/225000 (93%)] Loss: 6737.941406\n",
      "Train Epoch: 419 [213248/225000 (95%)] Loss: 6873.177734\n",
      "Train Epoch: 419 [217344/225000 (97%)] Loss: 6850.902344\n",
      "Train Epoch: 419 [221440/225000 (98%)] Loss: 6749.212891\n",
      "    epoch          : 419\n",
      "    loss           : 6783.052965461462\n",
      "    val_loss       : 6800.142900121456\n",
      "Train Epoch: 420 [256/225000 (0%)] Loss: 6786.728516\n",
      "Train Epoch: 420 [4352/225000 (2%)] Loss: 6915.906250\n",
      "Train Epoch: 420 [8448/225000 (4%)] Loss: 6736.896484\n",
      "Train Epoch: 420 [12544/225000 (6%)] Loss: 6786.888672\n",
      "Train Epoch: 420 [16640/225000 (7%)] Loss: 6809.791016\n",
      "Train Epoch: 420 [20736/225000 (9%)] Loss: 6716.916016\n",
      "Train Epoch: 420 [24832/225000 (11%)] Loss: 6811.984375\n",
      "Train Epoch: 420 [28928/225000 (13%)] Loss: 6869.619141\n",
      "Train Epoch: 420 [33024/225000 (15%)] Loss: 6686.992188\n",
      "Train Epoch: 420 [37120/225000 (16%)] Loss: 6803.867188\n",
      "Train Epoch: 420 [41216/225000 (18%)] Loss: 6599.224609\n",
      "Train Epoch: 420 [45312/225000 (20%)] Loss: 6812.408203\n",
      "Train Epoch: 420 [49408/225000 (22%)] Loss: 6794.076172\n",
      "Train Epoch: 420 [53504/225000 (24%)] Loss: 6811.683594\n",
      "Train Epoch: 420 [57600/225000 (26%)] Loss: 6729.593750\n",
      "Train Epoch: 420 [61696/225000 (27%)] Loss: 6808.742188\n",
      "Train Epoch: 420 [65792/225000 (29%)] Loss: 6886.820312\n",
      "Train Epoch: 420 [69888/225000 (31%)] Loss: 6895.169922\n",
      "Train Epoch: 420 [73984/225000 (33%)] Loss: 6671.115234\n",
      "Train Epoch: 420 [78080/225000 (35%)] Loss: 6803.646484\n",
      "Train Epoch: 420 [82176/225000 (37%)] Loss: 6895.400391\n",
      "Train Epoch: 420 [86272/225000 (38%)] Loss: 6835.458984\n",
      "Train Epoch: 420 [90368/225000 (40%)] Loss: 6900.921875\n",
      "Train Epoch: 420 [94464/225000 (42%)] Loss: 6623.240234\n",
      "Train Epoch: 420 [98560/225000 (44%)] Loss: 6695.353516\n",
      "Train Epoch: 420 [102656/225000 (46%)] Loss: 6784.947266\n",
      "Train Epoch: 420 [106752/225000 (47%)] Loss: 6747.443359\n",
      "Train Epoch: 420 [110848/225000 (49%)] Loss: 6633.107422\n",
      "Train Epoch: 420 [114944/225000 (51%)] Loss: 6572.052734\n",
      "Train Epoch: 420 [119040/225000 (53%)] Loss: 6742.058594\n",
      "Train Epoch: 420 [123136/225000 (55%)] Loss: 6772.757812\n",
      "Train Epoch: 420 [127232/225000 (57%)] Loss: 6946.613281\n",
      "Train Epoch: 420 [131328/225000 (58%)] Loss: 6617.201172\n",
      "Train Epoch: 420 [135424/225000 (60%)] Loss: 6882.673828\n",
      "Train Epoch: 420 [139520/225000 (62%)] Loss: 6720.884766\n",
      "Train Epoch: 420 [143616/225000 (64%)] Loss: 6860.166016\n",
      "Train Epoch: 420 [147712/225000 (66%)] Loss: 6757.617188\n",
      "Train Epoch: 420 [151808/225000 (67%)] Loss: 6800.541016\n",
      "Train Epoch: 420 [155904/225000 (69%)] Loss: 6812.623047\n",
      "Train Epoch: 420 [160000/225000 (71%)] Loss: 6741.880859\n",
      "Train Epoch: 420 [164096/225000 (73%)] Loss: 6786.136719\n",
      "Train Epoch: 420 [168192/225000 (75%)] Loss: 6662.201172\n",
      "Train Epoch: 420 [172288/225000 (77%)] Loss: 6691.412109\n",
      "Train Epoch: 420 [176384/225000 (78%)] Loss: 6973.570312\n",
      "Train Epoch: 420 [180480/225000 (80%)] Loss: 6776.783203\n",
      "Train Epoch: 420 [184576/225000 (82%)] Loss: 6779.667969\n",
      "Train Epoch: 420 [188672/225000 (84%)] Loss: 6830.050781\n",
      "Train Epoch: 420 [192768/225000 (86%)] Loss: 6764.263672\n",
      "Train Epoch: 420 [196864/225000 (87%)] Loss: 6802.630859\n",
      "Train Epoch: 420 [200960/225000 (89%)] Loss: 6763.408203\n",
      "Train Epoch: 420 [205056/225000 (91%)] Loss: 6986.175781\n",
      "Train Epoch: 420 [209152/225000 (93%)] Loss: 6860.736328\n",
      "Train Epoch: 420 [213248/225000 (95%)] Loss: 6735.056641\n",
      "Train Epoch: 420 [217344/225000 (97%)] Loss: 6754.535156\n",
      "Train Epoch: 420 [221440/225000 (98%)] Loss: 6789.285156\n",
      "    epoch          : 420\n",
      "    loss           : 6781.53755266105\n",
      "    val_loss       : 6794.780567663057\n",
      "Train Epoch: 421 [256/225000 (0%)] Loss: 6756.576172\n",
      "Train Epoch: 421 [4352/225000 (2%)] Loss: 6729.916016\n",
      "Train Epoch: 421 [8448/225000 (4%)] Loss: 6844.369141\n",
      "Train Epoch: 421 [12544/225000 (6%)] Loss: 6917.500000\n",
      "Train Epoch: 421 [16640/225000 (7%)] Loss: 6834.312500\n",
      "Train Epoch: 421 [20736/225000 (9%)] Loss: 6677.894531\n",
      "Train Epoch: 421 [24832/225000 (11%)] Loss: 6625.271484\n",
      "Train Epoch: 421 [28928/225000 (13%)] Loss: 6699.240234\n",
      "Train Epoch: 421 [33024/225000 (15%)] Loss: 6787.146484\n",
      "Train Epoch: 421 [37120/225000 (16%)] Loss: 6799.773438\n",
      "Train Epoch: 421 [41216/225000 (18%)] Loss: 6826.072266\n",
      "Train Epoch: 421 [45312/225000 (20%)] Loss: 6849.765625\n",
      "Train Epoch: 421 [49408/225000 (22%)] Loss: 6631.234375\n",
      "Train Epoch: 421 [53504/225000 (24%)] Loss: 6621.392578\n",
      "Train Epoch: 421 [57600/225000 (26%)] Loss: 6671.927734\n",
      "Train Epoch: 421 [61696/225000 (27%)] Loss: 6757.716797\n",
      "Train Epoch: 421 [65792/225000 (29%)] Loss: 6764.251953\n",
      "Train Epoch: 421 [69888/225000 (31%)] Loss: 6939.611328\n",
      "Train Epoch: 421 [73984/225000 (33%)] Loss: 6796.539062\n",
      "Train Epoch: 421 [78080/225000 (35%)] Loss: 6899.912109\n",
      "Train Epoch: 421 [82176/225000 (37%)] Loss: 6673.462891\n",
      "Train Epoch: 421 [86272/225000 (38%)] Loss: 6771.994141\n",
      "Train Epoch: 421 [90368/225000 (40%)] Loss: 6768.466797\n",
      "Train Epoch: 421 [94464/225000 (42%)] Loss: 6835.732422\n",
      "Train Epoch: 421 [98560/225000 (44%)] Loss: 6908.107422\n",
      "Train Epoch: 421 [102656/225000 (46%)] Loss: 6721.751953\n",
      "Train Epoch: 421 [106752/225000 (47%)] Loss: 6925.935547\n",
      "Train Epoch: 421 [110848/225000 (49%)] Loss: 6860.494141\n",
      "Train Epoch: 421 [114944/225000 (51%)] Loss: 6680.988281\n",
      "Train Epoch: 421 [119040/225000 (53%)] Loss: 6792.513672\n",
      "Train Epoch: 421 [123136/225000 (55%)] Loss: 6614.904297\n",
      "Train Epoch: 421 [127232/225000 (57%)] Loss: 6703.335938\n",
      "Train Epoch: 421 [131328/225000 (58%)] Loss: 6839.638672\n",
      "Train Epoch: 421 [135424/225000 (60%)] Loss: 6821.583984\n",
      "Train Epoch: 421 [139520/225000 (62%)] Loss: 6821.341797\n",
      "Train Epoch: 421 [143616/225000 (64%)] Loss: 6738.882812\n",
      "Train Epoch: 421 [147712/225000 (66%)] Loss: 6591.470703\n",
      "Train Epoch: 421 [151808/225000 (67%)] Loss: 6692.169922\n",
      "Train Epoch: 421 [155904/225000 (69%)] Loss: 6726.527344\n",
      "Train Epoch: 421 [160000/225000 (71%)] Loss: 6855.916016\n",
      "Train Epoch: 421 [164096/225000 (73%)] Loss: 6672.031250\n",
      "Train Epoch: 421 [168192/225000 (75%)] Loss: 6693.574219\n",
      "Train Epoch: 421 [172288/225000 (77%)] Loss: 6777.066406\n",
      "Train Epoch: 421 [176384/225000 (78%)] Loss: 6771.404297\n",
      "Train Epoch: 421 [180480/225000 (80%)] Loss: 6807.005859\n",
      "Train Epoch: 421 [184576/225000 (82%)] Loss: 6723.955078\n",
      "Train Epoch: 421 [188672/225000 (84%)] Loss: 6615.363281\n",
      "Train Epoch: 421 [192768/225000 (86%)] Loss: 6724.076172\n",
      "Train Epoch: 421 [196864/225000 (87%)] Loss: 6795.892578\n",
      "Train Epoch: 421 [200960/225000 (89%)] Loss: 6758.181641\n",
      "Train Epoch: 421 [205056/225000 (91%)] Loss: 6765.205078\n",
      "Train Epoch: 421 [209152/225000 (93%)] Loss: 6748.140625\n",
      "Train Epoch: 421 [213248/225000 (95%)] Loss: 6835.925781\n",
      "Train Epoch: 421 [217344/225000 (97%)] Loss: 6770.359375\n",
      "Train Epoch: 421 [221440/225000 (98%)] Loss: 6906.996094\n",
      "    epoch          : 421\n",
      "    loss           : 6796.807168346488\n",
      "    val_loss       : 6889.016543531904\n",
      "Train Epoch: 422 [256/225000 (0%)] Loss: 6794.759766\n",
      "Train Epoch: 422 [4352/225000 (2%)] Loss: 6723.449219\n",
      "Train Epoch: 422 [8448/225000 (4%)] Loss: 6873.919922\n",
      "Train Epoch: 422 [12544/225000 (6%)] Loss: 6711.810547\n",
      "Train Epoch: 422 [16640/225000 (7%)] Loss: 6545.419922\n",
      "Train Epoch: 422 [20736/225000 (9%)] Loss: 6775.027344\n",
      "Train Epoch: 422 [24832/225000 (11%)] Loss: 6680.312500\n",
      "Train Epoch: 422 [28928/225000 (13%)] Loss: 6690.794922\n",
      "Train Epoch: 422 [33024/225000 (15%)] Loss: 6716.792969\n",
      "Train Epoch: 422 [37120/225000 (16%)] Loss: 6761.394531\n",
      "Train Epoch: 422 [41216/225000 (18%)] Loss: 6688.064453\n",
      "Train Epoch: 422 [45312/225000 (20%)] Loss: 6774.255859\n",
      "Train Epoch: 422 [49408/225000 (22%)] Loss: 6723.822266\n",
      "Train Epoch: 422 [53504/225000 (24%)] Loss: 6704.404297\n",
      "Train Epoch: 422 [57600/225000 (26%)] Loss: 6783.554688\n",
      "Train Epoch: 422 [61696/225000 (27%)] Loss: 6853.521484\n",
      "Train Epoch: 422 [65792/225000 (29%)] Loss: 6859.964844\n",
      "Train Epoch: 422 [69888/225000 (31%)] Loss: 6700.451172\n",
      "Train Epoch: 422 [73984/225000 (33%)] Loss: 6749.867188\n",
      "Train Epoch: 422 [78080/225000 (35%)] Loss: 6655.472656\n",
      "Train Epoch: 422 [82176/225000 (37%)] Loss: 6845.771484\n",
      "Train Epoch: 422 [86272/225000 (38%)] Loss: 6812.292969\n",
      "Train Epoch: 422 [90368/225000 (40%)] Loss: 6672.226562\n",
      "Train Epoch: 422 [94464/225000 (42%)] Loss: 6878.734375\n",
      "Train Epoch: 422 [98560/225000 (44%)] Loss: 6869.742188\n",
      "Train Epoch: 422 [102656/225000 (46%)] Loss: 6785.236328\n",
      "Train Epoch: 422 [106752/225000 (47%)] Loss: 6943.330078\n",
      "Train Epoch: 422 [110848/225000 (49%)] Loss: 6916.751953\n",
      "Train Epoch: 422 [114944/225000 (51%)] Loss: 6713.855469\n",
      "Train Epoch: 422 [119040/225000 (53%)] Loss: 6754.738281\n",
      "Train Epoch: 422 [123136/225000 (55%)] Loss: 6697.291016\n",
      "Train Epoch: 422 [127232/225000 (57%)] Loss: 6857.736328\n",
      "Train Epoch: 422 [131328/225000 (58%)] Loss: 6790.072266\n",
      "Train Epoch: 422 [135424/225000 (60%)] Loss: 6700.730469\n",
      "Train Epoch: 422 [139520/225000 (62%)] Loss: 6690.480469\n",
      "Train Epoch: 422 [143616/225000 (64%)] Loss: 6771.160156\n",
      "Train Epoch: 422 [147712/225000 (66%)] Loss: 6740.755859\n",
      "Train Epoch: 422 [151808/225000 (67%)] Loss: 6724.572266\n",
      "Train Epoch: 422 [155904/225000 (69%)] Loss: 6716.353516\n",
      "Train Epoch: 422 [160000/225000 (71%)] Loss: 6707.958984\n",
      "Train Epoch: 422 [164096/225000 (73%)] Loss: 6882.492188\n",
      "Train Epoch: 422 [168192/225000 (75%)] Loss: 6679.419922\n",
      "Train Epoch: 422 [172288/225000 (77%)] Loss: 7018.115234\n",
      "Train Epoch: 422 [176384/225000 (78%)] Loss: 6846.351562\n",
      "Train Epoch: 422 [180480/225000 (80%)] Loss: 6853.755859\n",
      "Train Epoch: 422 [184576/225000 (82%)] Loss: 6970.742188\n",
      "Train Epoch: 422 [188672/225000 (84%)] Loss: 6837.228516\n",
      "Train Epoch: 422 [192768/225000 (86%)] Loss: 6953.402344\n",
      "Train Epoch: 422 [196864/225000 (87%)] Loss: 6869.068359\n",
      "Train Epoch: 422 [200960/225000 (89%)] Loss: 6770.390625\n",
      "Train Epoch: 422 [205056/225000 (91%)] Loss: 6800.800781\n",
      "Train Epoch: 422 [209152/225000 (93%)] Loss: 6753.308594\n",
      "Train Epoch: 422 [213248/225000 (95%)] Loss: 6940.990234\n",
      "Train Epoch: 422 [217344/225000 (97%)] Loss: 6665.304688\n",
      "Train Epoch: 422 [221440/225000 (98%)] Loss: 6746.064453\n",
      "    epoch          : 422\n",
      "    loss           : 6784.029409085253\n",
      "    val_loss       : 6796.4370826701725\n",
      "Train Epoch: 423 [256/225000 (0%)] Loss: 6842.554688\n",
      "Train Epoch: 423 [4352/225000 (2%)] Loss: 6734.949219\n",
      "Train Epoch: 423 [8448/225000 (4%)] Loss: 6762.033203\n",
      "Train Epoch: 423 [12544/225000 (6%)] Loss: 6781.898438\n",
      "Train Epoch: 423 [16640/225000 (7%)] Loss: 6654.455078\n",
      "Train Epoch: 423 [20736/225000 (9%)] Loss: 6780.990234\n",
      "Train Epoch: 423 [24832/225000 (11%)] Loss: 6843.476562\n",
      "Train Epoch: 423 [28928/225000 (13%)] Loss: 6878.888672\n",
      "Train Epoch: 423 [33024/225000 (15%)] Loss: 6801.609375\n",
      "Train Epoch: 423 [37120/225000 (16%)] Loss: 6705.421875\n",
      "Train Epoch: 423 [41216/225000 (18%)] Loss: 6729.300781\n",
      "Train Epoch: 423 [45312/225000 (20%)] Loss: 6716.205078\n",
      "Train Epoch: 423 [49408/225000 (22%)] Loss: 6881.425781\n",
      "Train Epoch: 423 [53504/225000 (24%)] Loss: 6613.087891\n",
      "Train Epoch: 423 [57600/225000 (26%)] Loss: 6835.681641\n",
      "Train Epoch: 423 [61696/225000 (27%)] Loss: 6805.906250\n",
      "Train Epoch: 423 [65792/225000 (29%)] Loss: 6806.068359\n",
      "Train Epoch: 423 [69888/225000 (31%)] Loss: 6775.078125\n",
      "Train Epoch: 423 [73984/225000 (33%)] Loss: 6747.742188\n",
      "Train Epoch: 423 [78080/225000 (35%)] Loss: 6712.839844\n",
      "Train Epoch: 423 [82176/225000 (37%)] Loss: 6765.898438\n",
      "Train Epoch: 423 [86272/225000 (38%)] Loss: 6823.359375\n",
      "Train Epoch: 423 [90368/225000 (40%)] Loss: 6828.259766\n",
      "Train Epoch: 423 [94464/225000 (42%)] Loss: 6732.109375\n",
      "Train Epoch: 423 [98560/225000 (44%)] Loss: 6659.648438\n",
      "Train Epoch: 423 [102656/225000 (46%)] Loss: 6733.970703\n",
      "Train Epoch: 423 [106752/225000 (47%)] Loss: 6767.505859\n",
      "Train Epoch: 423 [110848/225000 (49%)] Loss: 6689.949219\n",
      "Train Epoch: 423 [114944/225000 (51%)] Loss: 6820.216797\n",
      "Train Epoch: 423 [119040/225000 (53%)] Loss: 6762.066406\n",
      "Train Epoch: 423 [123136/225000 (55%)] Loss: 6860.988281\n",
      "Train Epoch: 423 [127232/225000 (57%)] Loss: 6940.474609\n",
      "Train Epoch: 423 [131328/225000 (58%)] Loss: 6716.068359\n",
      "Train Epoch: 423 [135424/225000 (60%)] Loss: 6744.671875\n",
      "Train Epoch: 423 [139520/225000 (62%)] Loss: 6729.326172\n",
      "Train Epoch: 423 [143616/225000 (64%)] Loss: 6929.257812\n",
      "Train Epoch: 423 [147712/225000 (66%)] Loss: 6606.001953\n",
      "Train Epoch: 423 [151808/225000 (67%)] Loss: 6768.179688\n",
      "Train Epoch: 423 [155904/225000 (69%)] Loss: 6813.130859\n",
      "Train Epoch: 423 [160000/225000 (71%)] Loss: 6671.597656\n",
      "Train Epoch: 423 [164096/225000 (73%)] Loss: 6844.667969\n",
      "Train Epoch: 423 [168192/225000 (75%)] Loss: 6770.224609\n",
      "Train Epoch: 423 [172288/225000 (77%)] Loss: 6708.175781\n",
      "Train Epoch: 423 [176384/225000 (78%)] Loss: 6869.181641\n",
      "Train Epoch: 423 [180480/225000 (80%)] Loss: 6825.636719\n",
      "Train Epoch: 423 [184576/225000 (82%)] Loss: 6718.953125\n",
      "Train Epoch: 423 [188672/225000 (84%)] Loss: 6677.058594\n",
      "Train Epoch: 423 [192768/225000 (86%)] Loss: 6821.833984\n",
      "Train Epoch: 423 [196864/225000 (87%)] Loss: 6808.458984\n",
      "Train Epoch: 423 [200960/225000 (89%)] Loss: 6722.681641\n",
      "Train Epoch: 423 [205056/225000 (91%)] Loss: 6808.044922\n",
      "Train Epoch: 423 [209152/225000 (93%)] Loss: 6754.750000\n",
      "Train Epoch: 423 [213248/225000 (95%)] Loss: 6710.236328\n",
      "Train Epoch: 423 [217344/225000 (97%)] Loss: 6758.843750\n",
      "Train Epoch: 423 [221440/225000 (98%)] Loss: 6897.509766\n",
      "    epoch          : 423\n",
      "    loss           : 6788.377026450512\n",
      "    val_loss       : 6791.1504335585905\n",
      "Train Epoch: 424 [256/225000 (0%)] Loss: 6884.044922\n",
      "Train Epoch: 424 [4352/225000 (2%)] Loss: 6659.330078\n",
      "Train Epoch: 424 [8448/225000 (4%)] Loss: 6852.224609\n",
      "Train Epoch: 424 [12544/225000 (6%)] Loss: 6673.152344\n",
      "Train Epoch: 424 [16640/225000 (7%)] Loss: 6686.826172\n",
      "Train Epoch: 424 [20736/225000 (9%)] Loss: 6778.427734\n",
      "Train Epoch: 424 [24832/225000 (11%)] Loss: 6778.626953\n",
      "Train Epoch: 424 [28928/225000 (13%)] Loss: 7021.484375\n",
      "Train Epoch: 424 [33024/225000 (15%)] Loss: 6669.455078\n",
      "Train Epoch: 424 [37120/225000 (16%)] Loss: 6676.363281\n",
      "Train Epoch: 424 [41216/225000 (18%)] Loss: 6676.208984\n",
      "Train Epoch: 424 [45312/225000 (20%)] Loss: 6701.806641\n",
      "Train Epoch: 424 [49408/225000 (22%)] Loss: 6691.248047\n",
      "Train Epoch: 424 [53504/225000 (24%)] Loss: 6732.875000\n",
      "Train Epoch: 424 [57600/225000 (26%)] Loss: 6725.306641\n",
      "Train Epoch: 424 [61696/225000 (27%)] Loss: 6777.689453\n",
      "Train Epoch: 424 [65792/225000 (29%)] Loss: 6779.089844\n",
      "Train Epoch: 424 [69888/225000 (31%)] Loss: 6632.792969\n",
      "Train Epoch: 424 [73984/225000 (33%)] Loss: 6836.769531\n",
      "Train Epoch: 424 [78080/225000 (35%)] Loss: 6808.306641\n",
      "Train Epoch: 424 [82176/225000 (37%)] Loss: 6686.939453\n",
      "Train Epoch: 424 [86272/225000 (38%)] Loss: 6733.791016\n",
      "Train Epoch: 424 [90368/225000 (40%)] Loss: 6812.107422\n",
      "Train Epoch: 424 [94464/225000 (42%)] Loss: 6765.373047\n",
      "Train Epoch: 424 [98560/225000 (44%)] Loss: 6783.748047\n",
      "Train Epoch: 424 [102656/225000 (46%)] Loss: 6758.271484\n",
      "Train Epoch: 424 [106752/225000 (47%)] Loss: 6771.603516\n",
      "Train Epoch: 424 [110848/225000 (49%)] Loss: 6577.843750\n",
      "Train Epoch: 424 [114944/225000 (51%)] Loss: 6744.417969\n",
      "Train Epoch: 424 [119040/225000 (53%)] Loss: 6959.644531\n",
      "Train Epoch: 424 [123136/225000 (55%)] Loss: 6851.875000\n",
      "Train Epoch: 424 [127232/225000 (57%)] Loss: 6759.326172\n",
      "Train Epoch: 424 [131328/225000 (58%)] Loss: 6810.894531\n",
      "Train Epoch: 424 [135424/225000 (60%)] Loss: 6720.396484\n",
      "Train Epoch: 424 [139520/225000 (62%)] Loss: 6851.455078\n",
      "Train Epoch: 424 [143616/225000 (64%)] Loss: 6997.628906\n",
      "Train Epoch: 424 [147712/225000 (66%)] Loss: 6798.613281\n",
      "Train Epoch: 424 [151808/225000 (67%)] Loss: 6663.738281\n",
      "Train Epoch: 424 [155904/225000 (69%)] Loss: 7034.648438\n",
      "Train Epoch: 424 [160000/225000 (71%)] Loss: 6716.917969\n",
      "Train Epoch: 424 [164096/225000 (73%)] Loss: 6681.636719\n",
      "Train Epoch: 424 [168192/225000 (75%)] Loss: 6771.263672\n",
      "Train Epoch: 424 [172288/225000 (77%)] Loss: 6736.361328\n",
      "Train Epoch: 424 [176384/225000 (78%)] Loss: 6834.800781\n",
      "Train Epoch: 424 [180480/225000 (80%)] Loss: 6910.248047\n",
      "Train Epoch: 424 [184576/225000 (82%)] Loss: 6720.164062\n",
      "Train Epoch: 424 [188672/225000 (84%)] Loss: 6735.982422\n",
      "Train Epoch: 424 [192768/225000 (86%)] Loss: 6692.974609\n",
      "Train Epoch: 424 [196864/225000 (87%)] Loss: 6649.503906\n",
      "Train Epoch: 424 [200960/225000 (89%)] Loss: 6734.498047\n",
      "Train Epoch: 424 [205056/225000 (91%)] Loss: 6690.976562\n",
      "Train Epoch: 424 [209152/225000 (93%)] Loss: 6874.207031\n",
      "Train Epoch: 424 [213248/225000 (95%)] Loss: 6653.150391\n",
      "Train Epoch: 424 [217344/225000 (97%)] Loss: 6864.064453\n",
      "Train Epoch: 424 [221440/225000 (98%)] Loss: 6746.337891\n",
      "    epoch          : 424\n",
      "    loss           : 6805.206786831627\n",
      "    val_loss       : 6794.494741683104\n",
      "Train Epoch: 425 [256/225000 (0%)] Loss: 6737.925781\n",
      "Train Epoch: 425 [4352/225000 (2%)] Loss: 6579.421875\n",
      "Train Epoch: 425 [8448/225000 (4%)] Loss: 6557.869141\n",
      "Train Epoch: 425 [12544/225000 (6%)] Loss: 6849.773438\n",
      "Train Epoch: 425 [16640/225000 (7%)] Loss: 6635.111328\n",
      "Train Epoch: 425 [20736/225000 (9%)] Loss: 6675.857422\n",
      "Train Epoch: 425 [24832/225000 (11%)] Loss: 6740.511719\n",
      "Train Epoch: 425 [28928/225000 (13%)] Loss: 6722.181641\n",
      "Train Epoch: 425 [33024/225000 (15%)] Loss: 6876.816406\n",
      "Train Epoch: 425 [37120/225000 (16%)] Loss: 6618.306641\n",
      "Train Epoch: 425 [41216/225000 (18%)] Loss: 6819.222656\n",
      "Train Epoch: 425 [45312/225000 (20%)] Loss: 6725.876953\n",
      "Train Epoch: 425 [49408/225000 (22%)] Loss: 6652.214844\n",
      "Train Epoch: 425 [53504/225000 (24%)] Loss: 6788.421875\n",
      "Train Epoch: 425 [57600/225000 (26%)] Loss: 6749.449219\n",
      "Train Epoch: 425 [61696/225000 (27%)] Loss: 6739.296875\n",
      "Train Epoch: 425 [65792/225000 (29%)] Loss: 6791.558594\n",
      "Train Epoch: 425 [69888/225000 (31%)] Loss: 6861.832031\n",
      "Train Epoch: 425 [73984/225000 (33%)] Loss: 6804.294922\n",
      "Train Epoch: 425 [78080/225000 (35%)] Loss: 6855.298828\n",
      "Train Epoch: 425 [82176/225000 (37%)] Loss: 6816.005859\n",
      "Train Epoch: 425 [86272/225000 (38%)] Loss: 6743.736328\n",
      "Train Epoch: 425 [90368/225000 (40%)] Loss: 6866.263672\n",
      "Train Epoch: 425 [94464/225000 (42%)] Loss: 6829.132812\n",
      "Train Epoch: 425 [98560/225000 (44%)] Loss: 6764.175781\n",
      "Train Epoch: 425 [102656/225000 (46%)] Loss: 6636.005859\n",
      "Train Epoch: 425 [106752/225000 (47%)] Loss: 6800.898438\n",
      "Train Epoch: 425 [110848/225000 (49%)] Loss: 6707.300781\n",
      "Train Epoch: 425 [114944/225000 (51%)] Loss: 6841.941406\n",
      "Train Epoch: 425 [119040/225000 (53%)] Loss: 6771.781250\n",
      "Train Epoch: 425 [123136/225000 (55%)] Loss: 6728.316406\n",
      "Train Epoch: 425 [127232/225000 (57%)] Loss: 6780.107422\n",
      "Train Epoch: 425 [131328/225000 (58%)] Loss: 6848.730469\n",
      "Train Epoch: 425 [135424/225000 (60%)] Loss: 6793.115234\n",
      "Train Epoch: 425 [139520/225000 (62%)] Loss: 6734.621094\n",
      "Train Epoch: 425 [143616/225000 (64%)] Loss: 6647.611328\n",
      "Train Epoch: 425 [147712/225000 (66%)] Loss: 6812.802734\n",
      "Train Epoch: 425 [151808/225000 (67%)] Loss: 6766.378906\n",
      "Train Epoch: 425 [155904/225000 (69%)] Loss: 6875.070312\n",
      "Train Epoch: 425 [160000/225000 (71%)] Loss: 6771.609375\n",
      "Train Epoch: 425 [164096/225000 (73%)] Loss: 6867.451172\n",
      "Train Epoch: 425 [168192/225000 (75%)] Loss: 6774.763672\n",
      "Train Epoch: 425 [172288/225000 (77%)] Loss: 6751.759766\n",
      "Train Epoch: 425 [176384/225000 (78%)] Loss: 6935.371094\n",
      "Train Epoch: 425 [180480/225000 (80%)] Loss: 6591.607422\n",
      "Train Epoch: 425 [184576/225000 (82%)] Loss: 6608.468750\n",
      "Train Epoch: 425 [188672/225000 (84%)] Loss: 6685.312500\n",
      "Train Epoch: 425 [192768/225000 (86%)] Loss: 6914.333984\n",
      "Train Epoch: 425 [196864/225000 (87%)] Loss: 6842.875000\n",
      "Train Epoch: 425 [200960/225000 (89%)] Loss: 6857.339844\n",
      "Train Epoch: 425 [205056/225000 (91%)] Loss: 6654.654297\n",
      "Train Epoch: 425 [209152/225000 (93%)] Loss: 6766.980469\n",
      "Train Epoch: 425 [213248/225000 (95%)] Loss: 6841.941406\n",
      "Train Epoch: 425 [217344/225000 (97%)] Loss: 6813.798828\n",
      "Train Epoch: 425 [221440/225000 (98%)] Loss: 6868.707031\n",
      "    epoch          : 425\n",
      "    loss           : 6799.1999942228385\n",
      "    val_loss       : 6785.438409844223\n",
      "Train Epoch: 426 [256/225000 (0%)] Loss: 6714.912109\n",
      "Train Epoch: 426 [4352/225000 (2%)] Loss: 6672.316406\n",
      "Train Epoch: 426 [8448/225000 (4%)] Loss: 6780.505859\n",
      "Train Epoch: 426 [12544/225000 (6%)] Loss: 6690.115234\n",
      "Train Epoch: 426 [16640/225000 (7%)] Loss: 6684.359375\n",
      "Train Epoch: 426 [20736/225000 (9%)] Loss: 6847.566406\n",
      "Train Epoch: 426 [24832/225000 (11%)] Loss: 6799.980469\n",
      "Train Epoch: 426 [28928/225000 (13%)] Loss: 6870.685547\n",
      "Train Epoch: 426 [33024/225000 (15%)] Loss: 6819.312500\n",
      "Train Epoch: 426 [37120/225000 (16%)] Loss: 6704.089844\n",
      "Train Epoch: 426 [41216/225000 (18%)] Loss: 6798.753906\n",
      "Train Epoch: 426 [45312/225000 (20%)] Loss: 6848.394531\n",
      "Train Epoch: 426 [49408/225000 (22%)] Loss: 6621.105469\n",
      "Train Epoch: 426 [53504/225000 (24%)] Loss: 6869.060547\n",
      "Train Epoch: 426 [57600/225000 (26%)] Loss: 6699.689453\n",
      "Train Epoch: 426 [61696/225000 (27%)] Loss: 6785.400391\n",
      "Train Epoch: 426 [65792/225000 (29%)] Loss: 6707.939453\n",
      "Train Epoch: 426 [69888/225000 (31%)] Loss: 6951.613281\n",
      "Train Epoch: 426 [73984/225000 (33%)] Loss: 6861.408203\n",
      "Train Epoch: 426 [78080/225000 (35%)] Loss: 6765.857422\n",
      "Train Epoch: 426 [82176/225000 (37%)] Loss: 6685.369141\n",
      "Train Epoch: 426 [86272/225000 (38%)] Loss: 6714.425781\n",
      "Train Epoch: 426 [90368/225000 (40%)] Loss: 6714.513672\n",
      "Train Epoch: 426 [94464/225000 (42%)] Loss: 6847.275391\n",
      "Train Epoch: 426 [98560/225000 (44%)] Loss: 6702.753906\n",
      "Train Epoch: 426 [102656/225000 (46%)] Loss: 6714.990234\n",
      "Train Epoch: 426 [106752/225000 (47%)] Loss: 6874.638672\n",
      "Train Epoch: 426 [110848/225000 (49%)] Loss: 6654.236328\n",
      "Train Epoch: 426 [114944/225000 (51%)] Loss: 6667.425781\n",
      "Train Epoch: 426 [119040/225000 (53%)] Loss: 6986.660156\n",
      "Train Epoch: 426 [123136/225000 (55%)] Loss: 6612.478516\n",
      "Train Epoch: 426 [127232/225000 (57%)] Loss: 6845.878906\n",
      "Train Epoch: 426 [131328/225000 (58%)] Loss: 6612.019531\n",
      "Train Epoch: 426 [135424/225000 (60%)] Loss: 6632.470703\n",
      "Train Epoch: 426 [139520/225000 (62%)] Loss: 6712.546875\n",
      "Train Epoch: 426 [143616/225000 (64%)] Loss: 6743.679688\n",
      "Train Epoch: 426 [147712/225000 (66%)] Loss: 6783.314453\n",
      "Train Epoch: 426 [151808/225000 (67%)] Loss: 6791.650391\n",
      "Train Epoch: 426 [155904/225000 (69%)] Loss: 6740.777344\n",
      "Train Epoch: 426 [160000/225000 (71%)] Loss: 6770.646484\n",
      "Train Epoch: 426 [164096/225000 (73%)] Loss: 6774.214844\n",
      "Train Epoch: 426 [168192/225000 (75%)] Loss: 6656.218750\n",
      "Train Epoch: 426 [172288/225000 (77%)] Loss: 6615.406250\n",
      "Train Epoch: 426 [176384/225000 (78%)] Loss: 6621.400391\n",
      "Train Epoch: 426 [180480/225000 (80%)] Loss: 6810.953125\n",
      "Train Epoch: 426 [184576/225000 (82%)] Loss: 6677.296875\n",
      "Train Epoch: 426 [188672/225000 (84%)] Loss: 6897.074219\n",
      "Train Epoch: 426 [192768/225000 (86%)] Loss: 6795.093750\n",
      "Train Epoch: 426 [196864/225000 (87%)] Loss: 6617.228516\n",
      "Train Epoch: 426 [200960/225000 (89%)] Loss: 6821.732422\n",
      "Train Epoch: 426 [205056/225000 (91%)] Loss: 6610.130859\n",
      "Train Epoch: 426 [209152/225000 (93%)] Loss: 6842.771484\n",
      "Train Epoch: 426 [213248/225000 (95%)] Loss: 6934.271484\n",
      "Train Epoch: 426 [217344/225000 (97%)] Loss: 6682.390625\n",
      "Train Epoch: 426 [221440/225000 (98%)] Loss: 6646.382812\n",
      "    epoch          : 426\n",
      "    loss           : 6788.274008550199\n",
      "    val_loss       : 6789.9982663782275\n",
      "Train Epoch: 427 [256/225000 (0%)] Loss: 6796.511719\n",
      "Train Epoch: 427 [4352/225000 (2%)] Loss: 6885.953125\n",
      "Train Epoch: 427 [8448/225000 (4%)] Loss: 6826.003906\n",
      "Train Epoch: 427 [12544/225000 (6%)] Loss: 6800.066406\n",
      "Train Epoch: 427 [16640/225000 (7%)] Loss: 6742.892578\n",
      "Train Epoch: 427 [20736/225000 (9%)] Loss: 6856.589844\n",
      "Train Epoch: 427 [24832/225000 (11%)] Loss: 6738.121094\n",
      "Train Epoch: 427 [28928/225000 (13%)] Loss: 6846.380859\n",
      "Train Epoch: 427 [33024/225000 (15%)] Loss: 6646.707031\n",
      "Train Epoch: 427 [37120/225000 (16%)] Loss: 6695.191406\n",
      "Train Epoch: 427 [41216/225000 (18%)] Loss: 6940.689453\n",
      "Train Epoch: 427 [45312/225000 (20%)] Loss: 6839.199219\n",
      "Train Epoch: 427 [49408/225000 (22%)] Loss: 6788.722656\n",
      "Train Epoch: 427 [53504/225000 (24%)] Loss: 6712.050781\n",
      "Train Epoch: 427 [57600/225000 (26%)] Loss: 6724.437500\n",
      "Train Epoch: 427 [61696/225000 (27%)] Loss: 6640.695312\n",
      "Train Epoch: 427 [65792/225000 (29%)] Loss: 6793.056641\n",
      "Train Epoch: 427 [69888/225000 (31%)] Loss: 6781.060547\n",
      "Train Epoch: 427 [73984/225000 (33%)] Loss: 6641.695312\n",
      "Train Epoch: 427 [78080/225000 (35%)] Loss: 6809.847656\n",
      "Train Epoch: 427 [82176/225000 (37%)] Loss: 6856.111328\n",
      "Train Epoch: 427 [86272/225000 (38%)] Loss: 6696.167969\n",
      "Train Epoch: 427 [90368/225000 (40%)] Loss: 6773.972656\n",
      "Train Epoch: 427 [94464/225000 (42%)] Loss: 6824.281250\n",
      "Train Epoch: 427 [98560/225000 (44%)] Loss: 6718.708984\n",
      "Train Epoch: 427 [102656/225000 (46%)] Loss: 6791.566406\n",
      "Train Epoch: 427 [106752/225000 (47%)] Loss: 6741.707031\n",
      "Train Epoch: 427 [110848/225000 (49%)] Loss: 6653.455078\n",
      "Train Epoch: 427 [114944/225000 (51%)] Loss: 6737.472656\n",
      "Train Epoch: 427 [119040/225000 (53%)] Loss: 6672.587891\n",
      "Train Epoch: 427 [123136/225000 (55%)] Loss: 6806.267578\n",
      "Train Epoch: 427 [127232/225000 (57%)] Loss: 6690.376953\n",
      "Train Epoch: 427 [131328/225000 (58%)] Loss: 6752.232422\n",
      "Train Epoch: 427 [135424/225000 (60%)] Loss: 6876.783203\n",
      "Train Epoch: 427 [139520/225000 (62%)] Loss: 6720.390625\n",
      "Train Epoch: 427 [143616/225000 (64%)] Loss: 6735.810547\n",
      "Train Epoch: 427 [147712/225000 (66%)] Loss: 6785.070312\n",
      "Train Epoch: 427 [151808/225000 (67%)] Loss: 6772.000000\n",
      "Train Epoch: 427 [155904/225000 (69%)] Loss: 6915.609375\n",
      "Train Epoch: 427 [160000/225000 (71%)] Loss: 6763.132812\n",
      "Train Epoch: 427 [164096/225000 (73%)] Loss: 6619.675781\n",
      "Train Epoch: 427 [168192/225000 (75%)] Loss: 6815.380859\n",
      "Train Epoch: 427 [172288/225000 (77%)] Loss: 6799.261719\n",
      "Train Epoch: 427 [176384/225000 (78%)] Loss: 6866.783203\n",
      "Train Epoch: 427 [180480/225000 (80%)] Loss: 6730.791016\n",
      "Train Epoch: 427 [184576/225000 (82%)] Loss: 6769.908203\n",
      "Train Epoch: 427 [188672/225000 (84%)] Loss: 6850.628906\n",
      "Train Epoch: 427 [192768/225000 (86%)] Loss: 6697.283203\n",
      "Train Epoch: 427 [196864/225000 (87%)] Loss: 6768.695312\n",
      "Train Epoch: 427 [200960/225000 (89%)] Loss: 6658.701172\n",
      "Train Epoch: 427 [205056/225000 (91%)] Loss: 6848.568359\n",
      "Train Epoch: 427 [209152/225000 (93%)] Loss: 6677.359375\n",
      "Train Epoch: 427 [213248/225000 (95%)] Loss: 6712.859375\n",
      "Train Epoch: 427 [217344/225000 (97%)] Loss: 6885.816406\n",
      "Train Epoch: 427 [221440/225000 (98%)] Loss: 7002.947266\n",
      "    epoch          : 427\n",
      "    loss           : 6766.481777499289\n",
      "    val_loss       : 6787.454562286941\n",
      "Train Epoch: 428 [256/225000 (0%)] Loss: 6694.701172\n",
      "Train Epoch: 428 [4352/225000 (2%)] Loss: 6872.779297\n",
      "Train Epoch: 428 [8448/225000 (4%)] Loss: 6880.404297\n",
      "Train Epoch: 428 [12544/225000 (6%)] Loss: 6690.931641\n",
      "Train Epoch: 428 [16640/225000 (7%)] Loss: 6729.234375\n",
      "Train Epoch: 428 [20736/225000 (9%)] Loss: 6817.273438\n",
      "Train Epoch: 428 [24832/225000 (11%)] Loss: 6852.794922\n",
      "Train Epoch: 428 [28928/225000 (13%)] Loss: 6635.000000\n",
      "Train Epoch: 428 [33024/225000 (15%)] Loss: 6828.488281\n",
      "Train Epoch: 428 [37120/225000 (16%)] Loss: 6755.320312\n",
      "Train Epoch: 428 [41216/225000 (18%)] Loss: 6713.167969\n",
      "Train Epoch: 428 [45312/225000 (20%)] Loss: 6732.800781\n",
      "Train Epoch: 428 [49408/225000 (22%)] Loss: 6641.982422\n",
      "Train Epoch: 428 [53504/225000 (24%)] Loss: 6790.138672\n",
      "Train Epoch: 428 [57600/225000 (26%)] Loss: 6732.750000\n",
      "Train Epoch: 428 [61696/225000 (27%)] Loss: 6824.498047\n",
      "Train Epoch: 428 [65792/225000 (29%)] Loss: 6811.371094\n",
      "Train Epoch: 428 [69888/225000 (31%)] Loss: 6636.064453\n",
      "Train Epoch: 428 [73984/225000 (33%)] Loss: 6661.257812\n",
      "Train Epoch: 428 [78080/225000 (35%)] Loss: 6959.820312\n",
      "Train Epoch: 428 [82176/225000 (37%)] Loss: 6670.523438\n",
      "Train Epoch: 428 [86272/225000 (38%)] Loss: 6757.611328\n",
      "Train Epoch: 428 [90368/225000 (40%)] Loss: 6787.583984\n",
      "Train Epoch: 428 [94464/225000 (42%)] Loss: 6577.888672\n",
      "Train Epoch: 428 [98560/225000 (44%)] Loss: 6874.771484\n",
      "Train Epoch: 428 [102656/225000 (46%)] Loss: 6831.333984\n",
      "Train Epoch: 428 [106752/225000 (47%)] Loss: 6807.714844\n",
      "Train Epoch: 428 [110848/225000 (49%)] Loss: 6803.658203\n",
      "Train Epoch: 428 [114944/225000 (51%)] Loss: 6910.632812\n",
      "Train Epoch: 428 [119040/225000 (53%)] Loss: 6889.500000\n",
      "Train Epoch: 428 [123136/225000 (55%)] Loss: 6788.388672\n",
      "Train Epoch: 428 [127232/225000 (57%)] Loss: 6860.283203\n",
      "Train Epoch: 428 [131328/225000 (58%)] Loss: 6815.857422\n",
      "Train Epoch: 428 [135424/225000 (60%)] Loss: 6828.300781\n",
      "Train Epoch: 428 [139520/225000 (62%)] Loss: 6727.285156\n",
      "Train Epoch: 428 [143616/225000 (64%)] Loss: 6846.535156\n",
      "Train Epoch: 428 [147712/225000 (66%)] Loss: 6783.703125\n",
      "Train Epoch: 428 [151808/225000 (67%)] Loss: 6759.580078\n",
      "Train Epoch: 428 [155904/225000 (69%)] Loss: 6768.015625\n",
      "Train Epoch: 428 [160000/225000 (71%)] Loss: 6721.880859\n",
      "Train Epoch: 428 [164096/225000 (73%)] Loss: 6786.828125\n",
      "Train Epoch: 428 [168192/225000 (75%)] Loss: 6932.589844\n",
      "Train Epoch: 428 [172288/225000 (77%)] Loss: 6849.865234\n",
      "Train Epoch: 428 [176384/225000 (78%)] Loss: 6829.294922\n",
      "Train Epoch: 428 [180480/225000 (80%)] Loss: 6909.476562\n",
      "Train Epoch: 428 [184576/225000 (82%)] Loss: 6763.484375\n",
      "Train Epoch: 428 [188672/225000 (84%)] Loss: 6628.437500\n",
      "Train Epoch: 428 [192768/225000 (86%)] Loss: 6907.789062\n",
      "Train Epoch: 428 [196864/225000 (87%)] Loss: 6706.101562\n",
      "Train Epoch: 428 [200960/225000 (89%)] Loss: 6683.925781\n",
      "Train Epoch: 428 [205056/225000 (91%)] Loss: 6596.306641\n",
      "Train Epoch: 428 [209152/225000 (93%)] Loss: 6884.166016\n",
      "Train Epoch: 428 [213248/225000 (95%)] Loss: 6673.773438\n",
      "Train Epoch: 428 [217344/225000 (97%)] Loss: 6782.160156\n",
      "Train Epoch: 428 [221440/225000 (98%)] Loss: 6830.464844\n",
      "    epoch          : 428\n",
      "    loss           : 6796.023607481869\n",
      "    val_loss       : 6783.050791988568\n",
      "Train Epoch: 429 [256/225000 (0%)] Loss: 6672.138672\n",
      "Train Epoch: 429 [4352/225000 (2%)] Loss: 6782.017578\n",
      "Train Epoch: 429 [8448/225000 (4%)] Loss: 6813.625000\n",
      "Train Epoch: 429 [12544/225000 (6%)] Loss: 6757.250000\n",
      "Train Epoch: 429 [16640/225000 (7%)] Loss: 6660.283203\n",
      "Train Epoch: 429 [20736/225000 (9%)] Loss: 6782.787109\n",
      "Train Epoch: 429 [24832/225000 (11%)] Loss: 6647.816406\n",
      "Train Epoch: 429 [28928/225000 (13%)] Loss: 6833.703125\n",
      "Train Epoch: 429 [33024/225000 (15%)] Loss: 6865.367188\n",
      "Train Epoch: 429 [37120/225000 (16%)] Loss: 6746.484375\n",
      "Train Epoch: 429 [41216/225000 (18%)] Loss: 6840.419922\n",
      "Train Epoch: 429 [45312/225000 (20%)] Loss: 6698.857422\n",
      "Train Epoch: 429 [49408/225000 (22%)] Loss: 6741.740234\n",
      "Train Epoch: 429 [53504/225000 (24%)] Loss: 6829.644531\n",
      "Train Epoch: 429 [57600/225000 (26%)] Loss: 6800.763672\n",
      "Train Epoch: 429 [61696/225000 (27%)] Loss: 6771.363281\n",
      "Train Epoch: 429 [65792/225000 (29%)] Loss: 6984.386719\n",
      "Train Epoch: 429 [69888/225000 (31%)] Loss: 6693.615234\n",
      "Train Epoch: 429 [73984/225000 (33%)] Loss: 6843.695312\n",
      "Train Epoch: 429 [78080/225000 (35%)] Loss: 6783.912109\n",
      "Train Epoch: 429 [82176/225000 (37%)] Loss: 6813.669922\n",
      "Train Epoch: 429 [86272/225000 (38%)] Loss: 6662.152344\n",
      "Train Epoch: 429 [90368/225000 (40%)] Loss: 6746.421875\n",
      "Train Epoch: 429 [94464/225000 (42%)] Loss: 6703.839844\n",
      "Train Epoch: 429 [98560/225000 (44%)] Loss: 6805.027344\n",
      "Train Epoch: 429 [102656/225000 (46%)] Loss: 6771.558594\n",
      "Train Epoch: 429 [106752/225000 (47%)] Loss: 6771.562500\n",
      "Train Epoch: 429 [110848/225000 (49%)] Loss: 6718.462891\n",
      "Train Epoch: 429 [114944/225000 (51%)] Loss: 6606.417969\n",
      "Train Epoch: 429 [119040/225000 (53%)] Loss: 6743.771484\n",
      "Train Epoch: 429 [123136/225000 (55%)] Loss: 6747.349609\n",
      "Train Epoch: 429 [127232/225000 (57%)] Loss: 6774.968750\n",
      "Train Epoch: 429 [131328/225000 (58%)] Loss: 6803.708984\n",
      "Train Epoch: 429 [135424/225000 (60%)] Loss: 6757.671875\n",
      "Train Epoch: 429 [139520/225000 (62%)] Loss: 6711.937500\n",
      "Train Epoch: 429 [143616/225000 (64%)] Loss: 6690.765625\n",
      "Train Epoch: 429 [147712/225000 (66%)] Loss: 6731.757812\n",
      "Train Epoch: 429 [151808/225000 (67%)] Loss: 6786.396484\n",
      "Train Epoch: 429 [155904/225000 (69%)] Loss: 6838.242188\n",
      "Train Epoch: 429 [160000/225000 (71%)] Loss: 6709.394531\n",
      "Train Epoch: 429 [164096/225000 (73%)] Loss: 6717.451172\n",
      "Train Epoch: 429 [168192/225000 (75%)] Loss: 6719.558594\n",
      "Train Epoch: 429 [172288/225000 (77%)] Loss: 6649.080078\n",
      "Train Epoch: 429 [176384/225000 (78%)] Loss: 6838.978516\n",
      "Train Epoch: 429 [180480/225000 (80%)] Loss: 6685.046875\n",
      "Train Epoch: 429 [184576/225000 (82%)] Loss: 6784.238281\n",
      "Train Epoch: 429 [188672/225000 (84%)] Loss: 6714.982422\n",
      "Train Epoch: 429 [192768/225000 (86%)] Loss: 6680.972656\n",
      "Train Epoch: 429 [196864/225000 (87%)] Loss: 6766.400391\n",
      "Train Epoch: 429 [200960/225000 (89%)] Loss: 6843.474609\n",
      "Train Epoch: 429 [205056/225000 (91%)] Loss: 6756.679688\n",
      "Train Epoch: 429 [209152/225000 (93%)] Loss: 6950.699219\n",
      "Train Epoch: 429 [213248/225000 (95%)] Loss: 6810.291016\n",
      "Train Epoch: 429 [217344/225000 (97%)] Loss: 6588.714844\n",
      "Train Epoch: 429 [221440/225000 (98%)] Loss: 6786.392578\n",
      "    epoch          : 429\n",
      "    loss           : 6813.8214379355095\n",
      "    val_loss       : 6837.480963266626\n",
      "Train Epoch: 430 [256/225000 (0%)] Loss: 6708.367188\n",
      "Train Epoch: 430 [4352/225000 (2%)] Loss: 6844.949219\n",
      "Train Epoch: 430 [8448/225000 (4%)] Loss: 6849.287109\n",
      "Train Epoch: 430 [12544/225000 (6%)] Loss: 6820.013672\n",
      "Train Epoch: 430 [16640/225000 (7%)] Loss: 6900.957031\n",
      "Train Epoch: 430 [20736/225000 (9%)] Loss: 6804.718750\n",
      "Train Epoch: 430 [24832/225000 (11%)] Loss: 6819.304688\n",
      "Train Epoch: 430 [28928/225000 (13%)] Loss: 6696.148438\n",
      "Train Epoch: 430 [33024/225000 (15%)] Loss: 6921.140625\n",
      "Train Epoch: 430 [37120/225000 (16%)] Loss: 6589.347656\n",
      "Train Epoch: 430 [41216/225000 (18%)] Loss: 6748.449219\n",
      "Train Epoch: 430 [45312/225000 (20%)] Loss: 6712.656250\n",
      "Train Epoch: 430 [49408/225000 (22%)] Loss: 6719.607422\n",
      "Train Epoch: 430 [53504/225000 (24%)] Loss: 6750.683594\n",
      "Train Epoch: 430 [57600/225000 (26%)] Loss: 6772.484375\n",
      "Train Epoch: 430 [61696/225000 (27%)] Loss: 6747.917969\n",
      "Train Epoch: 430 [65792/225000 (29%)] Loss: 6644.474609\n",
      "Train Epoch: 430 [69888/225000 (31%)] Loss: 6668.664062\n",
      "Train Epoch: 430 [73984/225000 (33%)] Loss: 6725.507812\n",
      "Train Epoch: 430 [78080/225000 (35%)] Loss: 6767.396484\n",
      "Train Epoch: 430 [82176/225000 (37%)] Loss: 6728.931641\n",
      "Train Epoch: 430 [86272/225000 (38%)] Loss: 6666.443359\n",
      "Train Epoch: 430 [90368/225000 (40%)] Loss: 6777.093750\n",
      "Train Epoch: 430 [94464/225000 (42%)] Loss: 6750.013672\n",
      "Train Epoch: 430 [98560/225000 (44%)] Loss: 6725.912109\n",
      "Train Epoch: 430 [102656/225000 (46%)] Loss: 6640.673828\n",
      "Train Epoch: 430 [106752/225000 (47%)] Loss: 6718.322266\n",
      "Train Epoch: 430 [110848/225000 (49%)] Loss: 6881.923828\n",
      "Train Epoch: 430 [114944/225000 (51%)] Loss: 6821.716797\n",
      "Train Epoch: 430 [119040/225000 (53%)] Loss: 6689.705078\n",
      "Train Epoch: 430 [123136/225000 (55%)] Loss: 6870.828125\n",
      "Train Epoch: 430 [127232/225000 (57%)] Loss: 6728.052734\n",
      "Train Epoch: 430 [131328/225000 (58%)] Loss: 6763.136719\n",
      "Train Epoch: 430 [135424/225000 (60%)] Loss: 6928.230469\n",
      "Train Epoch: 430 [139520/225000 (62%)] Loss: 6734.650391\n",
      "Train Epoch: 430 [143616/225000 (64%)] Loss: 6758.732422\n",
      "Train Epoch: 430 [147712/225000 (66%)] Loss: 6828.984375\n",
      "Train Epoch: 430 [151808/225000 (67%)] Loss: 6790.236328\n",
      "Train Epoch: 430 [155904/225000 (69%)] Loss: 6883.894531\n",
      "Train Epoch: 430 [160000/225000 (71%)] Loss: 6700.082031\n",
      "Train Epoch: 430 [164096/225000 (73%)] Loss: 6771.554688\n",
      "Train Epoch: 430 [168192/225000 (75%)] Loss: 6853.009766\n",
      "Train Epoch: 430 [172288/225000 (77%)] Loss: 6823.605469\n",
      "Train Epoch: 430 [176384/225000 (78%)] Loss: 6732.183594\n",
      "Train Epoch: 430 [180480/225000 (80%)] Loss: 6802.755859\n",
      "Train Epoch: 430 [184576/225000 (82%)] Loss: 6618.117188\n",
      "Train Epoch: 430 [188672/225000 (84%)] Loss: 6749.720703\n",
      "Train Epoch: 430 [192768/225000 (86%)] Loss: 6700.439453\n",
      "Train Epoch: 430 [196864/225000 (87%)] Loss: 6920.031250\n",
      "Train Epoch: 430 [200960/225000 (89%)] Loss: 6727.250000\n",
      "Train Epoch: 430 [205056/225000 (91%)] Loss: 6842.169922\n",
      "Train Epoch: 430 [209152/225000 (93%)] Loss: 6749.208984\n",
      "Train Epoch: 430 [213248/225000 (95%)] Loss: 6615.443359\n",
      "Train Epoch: 430 [217344/225000 (97%)] Loss: 6834.109375\n",
      "Train Epoch: 430 [221440/225000 (98%)] Loss: 6908.187500\n",
      "    epoch          : 430\n",
      "    loss           : 6795.153185882395\n",
      "    val_loss       : 6782.6801707331015\n",
      "Train Epoch: 431 [256/225000 (0%)] Loss: 6862.769531\n",
      "Train Epoch: 431 [4352/225000 (2%)] Loss: 6840.203125\n",
      "Train Epoch: 431 [8448/225000 (4%)] Loss: 6995.982422\n",
      "Train Epoch: 431 [12544/225000 (6%)] Loss: 6726.755859\n",
      "Train Epoch: 431 [16640/225000 (7%)] Loss: 6666.634766\n",
      "Train Epoch: 431 [20736/225000 (9%)] Loss: 6642.035156\n",
      "Train Epoch: 431 [24832/225000 (11%)] Loss: 6731.046875\n",
      "Train Epoch: 431 [28928/225000 (13%)] Loss: 6692.488281\n",
      "Train Epoch: 431 [33024/225000 (15%)] Loss: 6818.847656\n",
      "Train Epoch: 431 [37120/225000 (16%)] Loss: 6660.812500\n",
      "Train Epoch: 431 [41216/225000 (18%)] Loss: 6780.580078\n",
      "Train Epoch: 431 [45312/225000 (20%)] Loss: 6828.363281\n",
      "Train Epoch: 431 [49408/225000 (22%)] Loss: 6803.781250\n",
      "Train Epoch: 431 [53504/225000 (24%)] Loss: 6888.673828\n",
      "Train Epoch: 431 [57600/225000 (26%)] Loss: 6878.109375\n",
      "Train Epoch: 431 [61696/225000 (27%)] Loss: 6756.373047\n",
      "Train Epoch: 431 [65792/225000 (29%)] Loss: 6882.259766\n",
      "Train Epoch: 431 [69888/225000 (31%)] Loss: 6662.472656\n",
      "Train Epoch: 431 [73984/225000 (33%)] Loss: 6722.750000\n",
      "Train Epoch: 431 [78080/225000 (35%)] Loss: 6756.763672\n",
      "Train Epoch: 431 [82176/225000 (37%)] Loss: 6747.472656\n",
      "Train Epoch: 431 [86272/225000 (38%)] Loss: 6838.869141\n",
      "Train Epoch: 431 [90368/225000 (40%)] Loss: 6725.634766\n",
      "Train Epoch: 431 [94464/225000 (42%)] Loss: 6718.125000\n",
      "Train Epoch: 431 [98560/225000 (44%)] Loss: 6725.261719\n",
      "Train Epoch: 431 [102656/225000 (46%)] Loss: 6739.757812\n",
      "Train Epoch: 431 [106752/225000 (47%)] Loss: 6765.177734\n",
      "Train Epoch: 431 [110848/225000 (49%)] Loss: 6830.017578\n",
      "Train Epoch: 431 [114944/225000 (51%)] Loss: 6654.082031\n",
      "Train Epoch: 431 [119040/225000 (53%)] Loss: 6683.890625\n",
      "Train Epoch: 431 [123136/225000 (55%)] Loss: 6666.384766\n",
      "Train Epoch: 431 [127232/225000 (57%)] Loss: 6687.285156\n",
      "Train Epoch: 431 [131328/225000 (58%)] Loss: 6610.021484\n",
      "Train Epoch: 431 [135424/225000 (60%)] Loss: 6622.318359\n",
      "Train Epoch: 431 [139520/225000 (62%)] Loss: 6907.224609\n",
      "Train Epoch: 431 [143616/225000 (64%)] Loss: 6781.269531\n",
      "Train Epoch: 431 [147712/225000 (66%)] Loss: 6782.478516\n",
      "Train Epoch: 431 [151808/225000 (67%)] Loss: 6739.164062\n",
      "Train Epoch: 431 [155904/225000 (69%)] Loss: 6850.062500\n",
      "Train Epoch: 431 [160000/225000 (71%)] Loss: 6779.537109\n",
      "Train Epoch: 431 [164096/225000 (73%)] Loss: 6851.460938\n",
      "Train Epoch: 431 [168192/225000 (75%)] Loss: 6663.095703\n",
      "Train Epoch: 431 [172288/225000 (77%)] Loss: 6642.265625\n",
      "Train Epoch: 431 [176384/225000 (78%)] Loss: 6653.753906\n",
      "Train Epoch: 431 [180480/225000 (80%)] Loss: 6792.767578\n",
      "Train Epoch: 431 [184576/225000 (82%)] Loss: 6827.697266\n",
      "Train Epoch: 431 [188672/225000 (84%)] Loss: 6680.341797\n",
      "Train Epoch: 431 [192768/225000 (86%)] Loss: 6667.615234\n",
      "Train Epoch: 431 [196864/225000 (87%)] Loss: 6818.681641\n",
      "Train Epoch: 431 [200960/225000 (89%)] Loss: 6757.791016\n",
      "Train Epoch: 431 [205056/225000 (91%)] Loss: 6839.128906\n",
      "Train Epoch: 431 [209152/225000 (93%)] Loss: 6624.119141\n",
      "Train Epoch: 431 [213248/225000 (95%)] Loss: 6707.841797\n",
      "Train Epoch: 431 [217344/225000 (97%)] Loss: 6705.332031\n",
      "Train Epoch: 431 [221440/225000 (98%)] Loss: 6828.537109\n",
      "    epoch          : 431\n",
      "    loss           : 6779.606916373365\n",
      "    val_loss       : 6781.0767032175645\n",
      "Train Epoch: 432 [256/225000 (0%)] Loss: 6901.351562\n",
      "Train Epoch: 432 [4352/225000 (2%)] Loss: 6801.146484\n",
      "Train Epoch: 432 [8448/225000 (4%)] Loss: 6708.603516\n",
      "Train Epoch: 432 [12544/225000 (6%)] Loss: 6672.400391\n",
      "Train Epoch: 432 [16640/225000 (7%)] Loss: 6686.380859\n",
      "Train Epoch: 432 [20736/225000 (9%)] Loss: 6651.255859\n",
      "Train Epoch: 432 [24832/225000 (11%)] Loss: 6697.304688\n",
      "Train Epoch: 432 [28928/225000 (13%)] Loss: 6612.763672\n",
      "Train Epoch: 432 [33024/225000 (15%)] Loss: 6753.101562\n",
      "Train Epoch: 432 [37120/225000 (16%)] Loss: 6700.966797\n",
      "Train Epoch: 432 [41216/225000 (18%)] Loss: 6809.349609\n",
      "Train Epoch: 432 [45312/225000 (20%)] Loss: 6863.099609\n",
      "Train Epoch: 432 [49408/225000 (22%)] Loss: 6781.904297\n",
      "Train Epoch: 432 [53504/225000 (24%)] Loss: 6645.837891\n",
      "Train Epoch: 432 [57600/225000 (26%)] Loss: 6720.183594\n",
      "Train Epoch: 432 [61696/225000 (27%)] Loss: 6687.085938\n",
      "Train Epoch: 432 [65792/225000 (29%)] Loss: 6806.197266\n",
      "Train Epoch: 432 [69888/225000 (31%)] Loss: 6740.843750\n",
      "Train Epoch: 432 [73984/225000 (33%)] Loss: 7041.923828\n",
      "Train Epoch: 432 [78080/225000 (35%)] Loss: 6578.369141\n",
      "Train Epoch: 432 [82176/225000 (37%)] Loss: 6727.734375\n",
      "Train Epoch: 432 [86272/225000 (38%)] Loss: 6681.585938\n",
      "Train Epoch: 432 [90368/225000 (40%)] Loss: 6713.062500\n",
      "Train Epoch: 432 [94464/225000 (42%)] Loss: 6854.628906\n",
      "Train Epoch: 432 [98560/225000 (44%)] Loss: 6804.548828\n",
      "Train Epoch: 432 [102656/225000 (46%)] Loss: 6764.384766\n",
      "Train Epoch: 432 [106752/225000 (47%)] Loss: 6837.996094\n",
      "Train Epoch: 432 [110848/225000 (49%)] Loss: 6725.126953\n",
      "Train Epoch: 432 [114944/225000 (51%)] Loss: 6846.105469\n",
      "Train Epoch: 432 [119040/225000 (53%)] Loss: 6882.347656\n",
      "Train Epoch: 432 [123136/225000 (55%)] Loss: 6706.337891\n",
      "Train Epoch: 432 [127232/225000 (57%)] Loss: 6727.919922\n",
      "Train Epoch: 432 [131328/225000 (58%)] Loss: 6697.287109\n",
      "Train Epoch: 432 [135424/225000 (60%)] Loss: 6916.500000\n",
      "Train Epoch: 432 [139520/225000 (62%)] Loss: 6686.087891\n",
      "Train Epoch: 432 [143616/225000 (64%)] Loss: 6878.482422\n",
      "Train Epoch: 432 [147712/225000 (66%)] Loss: 6800.931641\n",
      "Train Epoch: 432 [151808/225000 (67%)] Loss: 6772.736328\n",
      "Train Epoch: 432 [155904/225000 (69%)] Loss: 6836.716797\n",
      "Train Epoch: 432 [160000/225000 (71%)] Loss: 6935.087891\n",
      "Train Epoch: 432 [164096/225000 (73%)] Loss: 6974.312500\n",
      "Train Epoch: 432 [168192/225000 (75%)] Loss: 6846.705078\n",
      "Train Epoch: 432 [172288/225000 (77%)] Loss: 6710.876953\n",
      "Train Epoch: 432 [176384/225000 (78%)] Loss: 6749.716797\n",
      "Train Epoch: 432 [180480/225000 (80%)] Loss: 6719.667969\n",
      "Train Epoch: 432 [184576/225000 (82%)] Loss: 6670.656250\n",
      "Train Epoch: 432 [188672/225000 (84%)] Loss: 6770.525391\n",
      "Train Epoch: 432 [192768/225000 (86%)] Loss: 6770.279297\n",
      "Train Epoch: 432 [196864/225000 (87%)] Loss: 6725.031250\n",
      "Train Epoch: 432 [200960/225000 (89%)] Loss: 6908.134766\n",
      "Train Epoch: 432 [205056/225000 (91%)] Loss: 6824.720703\n",
      "Train Epoch: 432 [209152/225000 (93%)] Loss: 6658.255859\n",
      "Train Epoch: 432 [213248/225000 (95%)] Loss: 6653.574219\n",
      "Train Epoch: 432 [217344/225000 (97%)] Loss: 6737.548828\n",
      "Train Epoch: 432 [221440/225000 (98%)] Loss: 6841.820312\n",
      "    epoch          : 432\n",
      "    loss           : 6780.039903521402\n",
      "    val_loss       : 6779.08492720857\n",
      "Train Epoch: 433 [256/225000 (0%)] Loss: 6870.056641\n",
      "Train Epoch: 433 [4352/225000 (2%)] Loss: 6644.179688\n",
      "Train Epoch: 433 [8448/225000 (4%)] Loss: 6665.203125\n",
      "Train Epoch: 433 [12544/225000 (6%)] Loss: 6823.015625\n",
      "Train Epoch: 433 [16640/225000 (7%)] Loss: 6760.431641\n",
      "Train Epoch: 433 [20736/225000 (9%)] Loss: 6762.458984\n",
      "Train Epoch: 433 [24832/225000 (11%)] Loss: 6775.203125\n",
      "Train Epoch: 433 [28928/225000 (13%)] Loss: 6722.794922\n",
      "Train Epoch: 433 [33024/225000 (15%)] Loss: 6807.189453\n",
      "Train Epoch: 433 [37120/225000 (16%)] Loss: 6710.890625\n",
      "Train Epoch: 433 [41216/225000 (18%)] Loss: 6651.751953\n",
      "Train Epoch: 433 [45312/225000 (20%)] Loss: 6760.457031\n",
      "Train Epoch: 433 [49408/225000 (22%)] Loss: 6828.763672\n",
      "Train Epoch: 433 [53504/225000 (24%)] Loss: 6758.613281\n",
      "Train Epoch: 433 [57600/225000 (26%)] Loss: 6666.105469\n",
      "Train Epoch: 433 [61696/225000 (27%)] Loss: 6810.525391\n",
      "Train Epoch: 433 [65792/225000 (29%)] Loss: 6745.470703\n",
      "Train Epoch: 433 [69888/225000 (31%)] Loss: 6719.582031\n",
      "Train Epoch: 433 [73984/225000 (33%)] Loss: 6728.369141\n",
      "Train Epoch: 433 [78080/225000 (35%)] Loss: 6724.320312\n",
      "Train Epoch: 433 [82176/225000 (37%)] Loss: 6727.226562\n",
      "Train Epoch: 433 [86272/225000 (38%)] Loss: 6753.962891\n",
      "Train Epoch: 433 [90368/225000 (40%)] Loss: 6821.947266\n",
      "Train Epoch: 433 [94464/225000 (42%)] Loss: 6730.880859\n",
      "Train Epoch: 433 [98560/225000 (44%)] Loss: 6757.027344\n",
      "Train Epoch: 433 [102656/225000 (46%)] Loss: 6751.986328\n",
      "Train Epoch: 433 [106752/225000 (47%)] Loss: 6980.945312\n",
      "Train Epoch: 433 [110848/225000 (49%)] Loss: 6814.035156\n",
      "Train Epoch: 433 [114944/225000 (51%)] Loss: 6773.658203\n",
      "Train Epoch: 433 [119040/225000 (53%)] Loss: 6698.919922\n",
      "Train Epoch: 433 [123136/225000 (55%)] Loss: 6676.054688\n",
      "Train Epoch: 433 [127232/225000 (57%)] Loss: 6846.873047\n",
      "Train Epoch: 433 [131328/225000 (58%)] Loss: 6897.998047\n",
      "Train Epoch: 433 [135424/225000 (60%)] Loss: 6665.935547\n",
      "Train Epoch: 433 [139520/225000 (62%)] Loss: 6765.650391\n",
      "Train Epoch: 433 [143616/225000 (64%)] Loss: 6740.314453\n",
      "Train Epoch: 433 [147712/225000 (66%)] Loss: 6707.857422\n",
      "Train Epoch: 433 [151808/225000 (67%)] Loss: 6773.357422\n",
      "Train Epoch: 433 [155904/225000 (69%)] Loss: 6721.808594\n",
      "Train Epoch: 433 [160000/225000 (71%)] Loss: 6917.720703\n",
      "Train Epoch: 433 [164096/225000 (73%)] Loss: 6769.339844\n",
      "Train Epoch: 433 [168192/225000 (75%)] Loss: 6707.058594\n",
      "Train Epoch: 433 [172288/225000 (77%)] Loss: 6889.000000\n",
      "Train Epoch: 433 [176384/225000 (78%)] Loss: 6778.373047\n",
      "Train Epoch: 433 [180480/225000 (80%)] Loss: 6752.341797\n",
      "Train Epoch: 433 [184576/225000 (82%)] Loss: 6667.730469\n",
      "Train Epoch: 433 [188672/225000 (84%)] Loss: 6845.068359\n",
      "Train Epoch: 433 [192768/225000 (86%)] Loss: 6745.300781\n",
      "Train Epoch: 433 [196864/225000 (87%)] Loss: 6604.371094\n",
      "Train Epoch: 433 [200960/225000 (89%)] Loss: 6780.865234\n",
      "Train Epoch: 433 [205056/225000 (91%)] Loss: 6784.683594\n",
      "Train Epoch: 433 [209152/225000 (93%)] Loss: 6615.169922\n",
      "Train Epoch: 433 [213248/225000 (95%)] Loss: 6863.947266\n",
      "Train Epoch: 433 [217344/225000 (97%)] Loss: 6823.724609\n",
      "Train Epoch: 433 [221440/225000 (98%)] Loss: 6668.523438\n",
      "    epoch          : 433\n",
      "    loss           : 6759.527897024317\n",
      "    val_loss       : 6834.392702404333\n",
      "Train Epoch: 434 [256/225000 (0%)] Loss: 6781.722656\n",
      "Train Epoch: 434 [4352/225000 (2%)] Loss: 6628.253906\n",
      "Train Epoch: 434 [8448/225000 (4%)] Loss: 6643.222656\n",
      "Train Epoch: 434 [12544/225000 (6%)] Loss: 6698.099609\n",
      "Train Epoch: 434 [16640/225000 (7%)] Loss: 6717.263672\n",
      "Train Epoch: 434 [20736/225000 (9%)] Loss: 6779.076172\n",
      "Train Epoch: 434 [24832/225000 (11%)] Loss: 6758.972656\n",
      "Train Epoch: 434 [28928/225000 (13%)] Loss: 6801.417969\n",
      "Train Epoch: 434 [33024/225000 (15%)] Loss: 6888.640625\n",
      "Train Epoch: 434 [37120/225000 (16%)] Loss: 6772.810547\n",
      "Train Epoch: 434 [41216/225000 (18%)] Loss: 6664.968750\n",
      "Train Epoch: 434 [45312/225000 (20%)] Loss: 6732.833984\n",
      "Train Epoch: 434 [49408/225000 (22%)] Loss: 6955.775391\n",
      "Train Epoch: 434 [53504/225000 (24%)] Loss: 6836.195312\n",
      "Train Epoch: 434 [57600/225000 (26%)] Loss: 6718.179688\n",
      "Train Epoch: 434 [61696/225000 (27%)] Loss: 6976.589844\n",
      "Train Epoch: 434 [65792/225000 (29%)] Loss: 6700.039062\n",
      "Train Epoch: 434 [69888/225000 (31%)] Loss: 6794.220703\n",
      "Train Epoch: 434 [73984/225000 (33%)] Loss: 6758.388672\n",
      "Train Epoch: 434 [78080/225000 (35%)] Loss: 6805.623047\n",
      "Train Epoch: 434 [82176/225000 (37%)] Loss: 6709.214844\n",
      "Train Epoch: 434 [86272/225000 (38%)] Loss: 6796.650391\n",
      "Train Epoch: 434 [90368/225000 (40%)] Loss: 6730.529297\n",
      "Train Epoch: 434 [94464/225000 (42%)] Loss: 6679.685547\n",
      "Train Epoch: 434 [98560/225000 (44%)] Loss: 6826.472656\n",
      "Train Epoch: 434 [102656/225000 (46%)] Loss: 6799.917969\n",
      "Train Epoch: 434 [106752/225000 (47%)] Loss: 6704.789062\n",
      "Train Epoch: 434 [110848/225000 (49%)] Loss: 6790.277344\n",
      "Train Epoch: 434 [114944/225000 (51%)] Loss: 6716.814453\n",
      "Train Epoch: 434 [119040/225000 (53%)] Loss: 6863.455078\n",
      "Train Epoch: 434 [123136/225000 (55%)] Loss: 6779.201172\n",
      "Train Epoch: 434 [127232/225000 (57%)] Loss: 6734.121094\n",
      "Train Epoch: 434 [131328/225000 (58%)] Loss: 6947.990234\n",
      "Train Epoch: 434 [135424/225000 (60%)] Loss: 6795.322266\n",
      "Train Epoch: 434 [139520/225000 (62%)] Loss: 6636.089844\n",
      "Train Epoch: 434 [143616/225000 (64%)] Loss: 6600.083984\n",
      "Train Epoch: 434 [147712/225000 (66%)] Loss: 6628.431641\n",
      "Train Epoch: 434 [151808/225000 (67%)] Loss: 6638.384766\n",
      "Train Epoch: 434 [155904/225000 (69%)] Loss: 6736.990234\n",
      "Train Epoch: 434 [160000/225000 (71%)] Loss: 6707.255859\n",
      "Train Epoch: 434 [164096/225000 (73%)] Loss: 6688.830078\n",
      "Train Epoch: 434 [168192/225000 (75%)] Loss: 6758.380859\n",
      "Train Epoch: 434 [172288/225000 (77%)] Loss: 6677.433594\n",
      "Train Epoch: 434 [176384/225000 (78%)] Loss: 6857.265625\n",
      "Train Epoch: 434 [180480/225000 (80%)] Loss: 6891.671875\n",
      "Train Epoch: 434 [184576/225000 (82%)] Loss: 6630.330078\n",
      "Train Epoch: 434 [188672/225000 (84%)] Loss: 6667.361328\n",
      "Train Epoch: 434 [192768/225000 (86%)] Loss: 6734.878906\n",
      "Train Epoch: 434 [196864/225000 (87%)] Loss: 6826.951172\n",
      "Train Epoch: 434 [200960/225000 (89%)] Loss: 6884.648438\n",
      "Train Epoch: 434 [205056/225000 (91%)] Loss: 6808.365234\n",
      "Train Epoch: 434 [209152/225000 (93%)] Loss: 6790.212891\n",
      "Train Epoch: 434 [213248/225000 (95%)] Loss: 6769.695312\n",
      "Train Epoch: 434 [217344/225000 (97%)] Loss: 6737.945312\n",
      "Train Epoch: 434 [221440/225000 (98%)] Loss: 6800.513672\n",
      "    epoch          : 434\n",
      "    loss           : 6769.719547692691\n",
      "    val_loss       : 6784.115779119486\n",
      "Train Epoch: 435 [256/225000 (0%)] Loss: 6828.744141\n",
      "Train Epoch: 435 [4352/225000 (2%)] Loss: 6795.654297\n",
      "Train Epoch: 435 [8448/225000 (4%)] Loss: 6727.929688\n",
      "Train Epoch: 435 [12544/225000 (6%)] Loss: 6847.394531\n",
      "Train Epoch: 435 [16640/225000 (7%)] Loss: 6596.804688\n",
      "Train Epoch: 435 [20736/225000 (9%)] Loss: 6749.994141\n",
      "Train Epoch: 435 [24832/225000 (11%)] Loss: 6766.492188\n",
      "Train Epoch: 435 [28928/225000 (13%)] Loss: 6688.914062\n",
      "Train Epoch: 435 [33024/225000 (15%)] Loss: 6762.552734\n",
      "Train Epoch: 435 [37120/225000 (16%)] Loss: 6790.080078\n",
      "Train Epoch: 435 [41216/225000 (18%)] Loss: 6785.269531\n",
      "Train Epoch: 435 [45312/225000 (20%)] Loss: 6747.488281\n",
      "Train Epoch: 435 [49408/225000 (22%)] Loss: 6665.953125\n",
      "Train Epoch: 435 [53504/225000 (24%)] Loss: 6624.734375\n",
      "Train Epoch: 435 [57600/225000 (26%)] Loss: 6629.779297\n",
      "Train Epoch: 435 [61696/225000 (27%)] Loss: 6745.755859\n",
      "Train Epoch: 435 [65792/225000 (29%)] Loss: 6846.013672\n",
      "Train Epoch: 435 [69888/225000 (31%)] Loss: 6825.515625\n",
      "Train Epoch: 435 [73984/225000 (33%)] Loss: 6661.398438\n",
      "Train Epoch: 435 [78080/225000 (35%)] Loss: 6865.544922\n",
      "Train Epoch: 435 [82176/225000 (37%)] Loss: 6921.921875\n",
      "Train Epoch: 435 [86272/225000 (38%)] Loss: 6707.470703\n",
      "Train Epoch: 435 [90368/225000 (40%)] Loss: 6867.730469\n",
      "Train Epoch: 435 [94464/225000 (42%)] Loss: 6841.867188\n",
      "Train Epoch: 435 [98560/225000 (44%)] Loss: 6765.304688\n",
      "Train Epoch: 435 [102656/225000 (46%)] Loss: 6715.724609\n",
      "Train Epoch: 435 [106752/225000 (47%)] Loss: 6846.687500\n",
      "Train Epoch: 435 [110848/225000 (49%)] Loss: 6648.990234\n",
      "Train Epoch: 435 [114944/225000 (51%)] Loss: 6816.369141\n",
      "Train Epoch: 435 [119040/225000 (53%)] Loss: 6718.777344\n",
      "Train Epoch: 435 [123136/225000 (55%)] Loss: 6680.857422\n",
      "Train Epoch: 435 [127232/225000 (57%)] Loss: 6694.541016\n",
      "Train Epoch: 435 [131328/225000 (58%)] Loss: 6805.195312\n",
      "Train Epoch: 435 [135424/225000 (60%)] Loss: 6872.923828\n",
      "Train Epoch: 435 [139520/225000 (62%)] Loss: 6826.007812\n",
      "Train Epoch: 435 [143616/225000 (64%)] Loss: 6594.730469\n",
      "Train Epoch: 435 [147712/225000 (66%)] Loss: 6777.070312\n",
      "Train Epoch: 435 [151808/225000 (67%)] Loss: 7002.216797\n",
      "Train Epoch: 435 [155904/225000 (69%)] Loss: 17048.876953\n",
      "Train Epoch: 435 [160000/225000 (71%)] Loss: 6786.289062\n",
      "Train Epoch: 435 [164096/225000 (73%)] Loss: 6947.300781\n",
      "Train Epoch: 435 [168192/225000 (75%)] Loss: 6846.980469\n",
      "Train Epoch: 435 [172288/225000 (77%)] Loss: 6790.382812\n",
      "Train Epoch: 435 [176384/225000 (78%)] Loss: 6619.951172\n",
      "Train Epoch: 435 [180480/225000 (80%)] Loss: 6814.574219\n",
      "Train Epoch: 435 [184576/225000 (82%)] Loss: 6798.550781\n",
      "Train Epoch: 435 [188672/225000 (84%)] Loss: 6774.732422\n",
      "Train Epoch: 435 [192768/225000 (86%)] Loss: 6744.261719\n",
      "Train Epoch: 435 [196864/225000 (87%)] Loss: 6627.490234\n",
      "Train Epoch: 435 [200960/225000 (89%)] Loss: 6768.746094\n",
      "Train Epoch: 435 [205056/225000 (91%)] Loss: 6777.000000\n",
      "Train Epoch: 435 [209152/225000 (93%)] Loss: 6792.142578\n",
      "Train Epoch: 435 [213248/225000 (95%)] Loss: 6980.361328\n",
      "Train Epoch: 435 [217344/225000 (97%)] Loss: 6844.541016\n",
      "Train Epoch: 435 [221440/225000 (98%)] Loss: 6656.878906\n",
      "    epoch          : 435\n",
      "    loss           : 6768.4837117374145\n",
      "    val_loss       : 6777.57416855559\n",
      "Train Epoch: 436 [256/225000 (0%)] Loss: 6836.683594\n",
      "Train Epoch: 436 [4352/225000 (2%)] Loss: 6807.169922\n",
      "Train Epoch: 436 [8448/225000 (4%)] Loss: 6677.984375\n",
      "Train Epoch: 436 [12544/225000 (6%)] Loss: 6642.500000\n",
      "Train Epoch: 436 [16640/225000 (7%)] Loss: 6872.886719\n",
      "Train Epoch: 436 [20736/225000 (9%)] Loss: 6688.078125\n",
      "Train Epoch: 436 [24832/225000 (11%)] Loss: 6692.136719\n",
      "Train Epoch: 436 [28928/225000 (13%)] Loss: 6671.437500\n",
      "Train Epoch: 436 [33024/225000 (15%)] Loss: 6685.968750\n",
      "Train Epoch: 436 [37120/225000 (16%)] Loss: 6690.587891\n",
      "Train Epoch: 436 [41216/225000 (18%)] Loss: 6776.216797\n",
      "Train Epoch: 436 [45312/225000 (20%)] Loss: 6854.816406\n",
      "Train Epoch: 436 [49408/225000 (22%)] Loss: 6562.750000\n",
      "Train Epoch: 436 [53504/225000 (24%)] Loss: 6694.703125\n",
      "Train Epoch: 436 [57600/225000 (26%)] Loss: 6678.996094\n",
      "Train Epoch: 436 [61696/225000 (27%)] Loss: 6708.707031\n",
      "Train Epoch: 436 [65792/225000 (29%)] Loss: 6787.308594\n",
      "Train Epoch: 436 [69888/225000 (31%)] Loss: 6903.669922\n",
      "Train Epoch: 436 [73984/225000 (33%)] Loss: 6598.496094\n",
      "Train Epoch: 436 [78080/225000 (35%)] Loss: 6721.509766\n",
      "Train Epoch: 436 [82176/225000 (37%)] Loss: 6792.775391\n",
      "Train Epoch: 436 [86272/225000 (38%)] Loss: 6753.906250\n",
      "Train Epoch: 436 [90368/225000 (40%)] Loss: 6869.167969\n",
      "Train Epoch: 436 [94464/225000 (42%)] Loss: 6765.373047\n",
      "Train Epoch: 436 [98560/225000 (44%)] Loss: 6852.949219\n",
      "Train Epoch: 436 [102656/225000 (46%)] Loss: 6649.986328\n",
      "Train Epoch: 436 [106752/225000 (47%)] Loss: 6668.513672\n",
      "Train Epoch: 436 [110848/225000 (49%)] Loss: 6883.746094\n",
      "Train Epoch: 436 [114944/225000 (51%)] Loss: 6595.564453\n",
      "Train Epoch: 436 [119040/225000 (53%)] Loss: 6749.705078\n",
      "Train Epoch: 436 [123136/225000 (55%)] Loss: 6809.566406\n",
      "Train Epoch: 436 [127232/225000 (57%)] Loss: 6745.617188\n",
      "Train Epoch: 436 [131328/225000 (58%)] Loss: 6758.386719\n",
      "Train Epoch: 436 [135424/225000 (60%)] Loss: 6852.666016\n",
      "Train Epoch: 436 [139520/225000 (62%)] Loss: 6794.367188\n",
      "Train Epoch: 436 [143616/225000 (64%)] Loss: 6795.447266\n",
      "Train Epoch: 436 [147712/225000 (66%)] Loss: 6843.642578\n",
      "Train Epoch: 436 [151808/225000 (67%)] Loss: 6689.835938\n",
      "Train Epoch: 436 [155904/225000 (69%)] Loss: 6721.228516\n",
      "Train Epoch: 436 [160000/225000 (71%)] Loss: 6772.345703\n",
      "Train Epoch: 436 [164096/225000 (73%)] Loss: 6588.162109\n",
      "Train Epoch: 436 [168192/225000 (75%)] Loss: 6752.761719\n",
      "Train Epoch: 436 [172288/225000 (77%)] Loss: 6858.482422\n",
      "Train Epoch: 436 [176384/225000 (78%)] Loss: 6713.263672\n",
      "Train Epoch: 436 [180480/225000 (80%)] Loss: 6811.701172\n",
      "Train Epoch: 436 [184576/225000 (82%)] Loss: 6628.218750\n",
      "Train Epoch: 436 [188672/225000 (84%)] Loss: 6673.921875\n",
      "Train Epoch: 436 [192768/225000 (86%)] Loss: 6714.951172\n",
      "Train Epoch: 436 [196864/225000 (87%)] Loss: 6735.572266\n",
      "Train Epoch: 436 [200960/225000 (89%)] Loss: 6676.984375\n",
      "Train Epoch: 436 [205056/225000 (91%)] Loss: 6594.658203\n",
      "Train Epoch: 436 [209152/225000 (93%)] Loss: 6728.033203\n",
      "Train Epoch: 436 [213248/225000 (95%)] Loss: 6759.308594\n",
      "Train Epoch: 436 [217344/225000 (97%)] Loss: 6781.488281\n",
      "Train Epoch: 436 [221440/225000 (98%)] Loss: 6736.900391\n",
      "    epoch          : 436\n",
      "    loss           : 6783.935156916596\n",
      "    val_loss       : 6779.5997092261605\n",
      "Train Epoch: 437 [256/225000 (0%)] Loss: 6884.826172\n",
      "Train Epoch: 437 [4352/225000 (2%)] Loss: 6687.990234\n",
      "Train Epoch: 437 [8448/225000 (4%)] Loss: 6787.337891\n",
      "Train Epoch: 437 [12544/225000 (6%)] Loss: 6707.019531\n",
      "Train Epoch: 437 [16640/225000 (7%)] Loss: 6720.791016\n",
      "Train Epoch: 437 [20736/225000 (9%)] Loss: 6622.277344\n",
      "Train Epoch: 437 [24832/225000 (11%)] Loss: 6617.886719\n",
      "Train Epoch: 437 [28928/225000 (13%)] Loss: 6636.726562\n",
      "Train Epoch: 437 [33024/225000 (15%)] Loss: 6551.550781\n",
      "Train Epoch: 437 [37120/225000 (16%)] Loss: 6787.962891\n",
      "Train Epoch: 437 [41216/225000 (18%)] Loss: 7004.638672\n",
      "Train Epoch: 437 [45312/225000 (20%)] Loss: 6790.199219\n",
      "Train Epoch: 437 [49408/225000 (22%)] Loss: 6829.580078\n",
      "Train Epoch: 437 [53504/225000 (24%)] Loss: 6770.058594\n",
      "Train Epoch: 437 [57600/225000 (26%)] Loss: 6749.000000\n",
      "Train Epoch: 437 [61696/225000 (27%)] Loss: 6831.130859\n",
      "Train Epoch: 437 [65792/225000 (29%)] Loss: 6780.597656\n",
      "Train Epoch: 437 [69888/225000 (31%)] Loss: 6758.990234\n",
      "Train Epoch: 437 [73984/225000 (33%)] Loss: 6820.917969\n",
      "Train Epoch: 437 [78080/225000 (35%)] Loss: 6705.570312\n",
      "Train Epoch: 437 [82176/225000 (37%)] Loss: 6542.568359\n",
      "Train Epoch: 437 [86272/225000 (38%)] Loss: 6841.367188\n",
      "Train Epoch: 437 [90368/225000 (40%)] Loss: 6687.976562\n",
      "Train Epoch: 437 [94464/225000 (42%)] Loss: 6688.537109\n",
      "Train Epoch: 437 [98560/225000 (44%)] Loss: 6687.046875\n",
      "Train Epoch: 437 [102656/225000 (46%)] Loss: 6783.402344\n",
      "Train Epoch: 437 [106752/225000 (47%)] Loss: 6899.958984\n",
      "Train Epoch: 437 [110848/225000 (49%)] Loss: 6743.437500\n",
      "Train Epoch: 437 [114944/225000 (51%)] Loss: 6718.001953\n",
      "Train Epoch: 437 [119040/225000 (53%)] Loss: 6800.083984\n",
      "Train Epoch: 437 [123136/225000 (55%)] Loss: 6606.648438\n",
      "Train Epoch: 437 [127232/225000 (57%)] Loss: 6787.898438\n",
      "Train Epoch: 437 [131328/225000 (58%)] Loss: 6869.445312\n",
      "Train Epoch: 437 [135424/225000 (60%)] Loss: 6598.246094\n",
      "Train Epoch: 437 [139520/225000 (62%)] Loss: 6731.681641\n",
      "Train Epoch: 437 [143616/225000 (64%)] Loss: 6851.220703\n",
      "Train Epoch: 437 [147712/225000 (66%)] Loss: 6754.498047\n",
      "Train Epoch: 437 [151808/225000 (67%)] Loss: 6692.160156\n",
      "Train Epoch: 437 [155904/225000 (69%)] Loss: 6712.015625\n",
      "Train Epoch: 437 [160000/225000 (71%)] Loss: 6690.107422\n",
      "Train Epoch: 437 [164096/225000 (73%)] Loss: 6711.183594\n",
      "Train Epoch: 437 [168192/225000 (75%)] Loss: 6860.740234\n",
      "Train Epoch: 437 [172288/225000 (77%)] Loss: 6770.597656\n",
      "Train Epoch: 437 [176384/225000 (78%)] Loss: 6657.292969\n",
      "Train Epoch: 437 [180480/225000 (80%)] Loss: 6720.062500\n",
      "Train Epoch: 437 [184576/225000 (82%)] Loss: 6769.656250\n",
      "Train Epoch: 437 [188672/225000 (84%)] Loss: 6668.949219\n",
      "Train Epoch: 437 [192768/225000 (86%)] Loss: 6777.267578\n",
      "Train Epoch: 437 [196864/225000 (87%)] Loss: 6677.384766\n",
      "Train Epoch: 437 [200960/225000 (89%)] Loss: 6795.136719\n",
      "Train Epoch: 437 [205056/225000 (91%)] Loss: 6803.824219\n",
      "Train Epoch: 437 [209152/225000 (93%)] Loss: 6615.326172\n",
      "Train Epoch: 437 [213248/225000 (95%)] Loss: 6874.486328\n",
      "Train Epoch: 437 [217344/225000 (97%)] Loss: 6719.146484\n",
      "Train Epoch: 437 [221440/225000 (98%)] Loss: 6750.285156\n",
      "    epoch          : 437\n",
      "    loss           : 6786.287891513794\n",
      "    val_loss       : 6780.65094859746\n",
      "Train Epoch: 438 [256/225000 (0%)] Loss: 6762.820312\n",
      "Train Epoch: 438 [4352/225000 (2%)] Loss: 6793.800781\n",
      "Train Epoch: 438 [8448/225000 (4%)] Loss: 6765.615234\n",
      "Train Epoch: 438 [12544/225000 (6%)] Loss: 6805.171875\n",
      "Train Epoch: 438 [16640/225000 (7%)] Loss: 6744.605469\n",
      "Train Epoch: 438 [20736/225000 (9%)] Loss: 6775.306641\n",
      "Train Epoch: 438 [24832/225000 (11%)] Loss: 6715.333984\n",
      "Train Epoch: 438 [28928/225000 (13%)] Loss: 6737.343750\n",
      "Train Epoch: 438 [33024/225000 (15%)] Loss: 6697.687500\n",
      "Train Epoch: 438 [37120/225000 (16%)] Loss: 6803.875000\n",
      "Train Epoch: 438 [41216/225000 (18%)] Loss: 6663.333984\n",
      "Train Epoch: 438 [45312/225000 (20%)] Loss: 6787.230469\n",
      "Train Epoch: 438 [49408/225000 (22%)] Loss: 6621.007812\n",
      "Train Epoch: 438 [53504/225000 (24%)] Loss: 6628.455078\n",
      "Train Epoch: 438 [57600/225000 (26%)] Loss: 6814.761719\n",
      "Train Epoch: 438 [61696/225000 (27%)] Loss: 6640.523438\n",
      "Train Epoch: 438 [65792/225000 (29%)] Loss: 6653.876953\n",
      "Train Epoch: 438 [69888/225000 (31%)] Loss: 6808.333984\n",
      "Train Epoch: 438 [73984/225000 (33%)] Loss: 6775.986328\n",
      "Train Epoch: 438 [78080/225000 (35%)] Loss: 6666.902344\n",
      "Train Epoch: 438 [82176/225000 (37%)] Loss: 6830.365234\n",
      "Train Epoch: 438 [86272/225000 (38%)] Loss: 6710.330078\n",
      "Train Epoch: 438 [90368/225000 (40%)] Loss: 6770.923828\n",
      "Train Epoch: 438 [94464/225000 (42%)] Loss: 6763.974609\n",
      "Train Epoch: 438 [98560/225000 (44%)] Loss: 6675.837891\n",
      "Train Epoch: 438 [102656/225000 (46%)] Loss: 7012.271484\n",
      "Train Epoch: 438 [106752/225000 (47%)] Loss: 6809.945312\n",
      "Train Epoch: 438 [110848/225000 (49%)] Loss: 6678.857422\n",
      "Train Epoch: 438 [114944/225000 (51%)] Loss: 6756.421875\n",
      "Train Epoch: 438 [119040/225000 (53%)] Loss: 6861.748047\n",
      "Train Epoch: 438 [123136/225000 (55%)] Loss: 6773.546875\n",
      "Train Epoch: 438 [127232/225000 (57%)] Loss: 6722.283203\n",
      "Train Epoch: 438 [131328/225000 (58%)] Loss: 6667.431641\n",
      "Train Epoch: 438 [135424/225000 (60%)] Loss: 6872.066406\n",
      "Train Epoch: 438 [139520/225000 (62%)] Loss: 6822.005859\n",
      "Train Epoch: 438 [143616/225000 (64%)] Loss: 6762.228516\n",
      "Train Epoch: 438 [147712/225000 (66%)] Loss: 6903.462891\n",
      "Train Epoch: 438 [151808/225000 (67%)] Loss: 6705.728516\n",
      "Train Epoch: 438 [155904/225000 (69%)] Loss: 6762.333984\n",
      "Train Epoch: 438 [160000/225000 (71%)] Loss: 6777.914062\n",
      "Train Epoch: 438 [164096/225000 (73%)] Loss: 6760.833984\n",
      "Train Epoch: 438 [168192/225000 (75%)] Loss: 6657.769531\n",
      "Train Epoch: 438 [172288/225000 (77%)] Loss: 6805.896484\n",
      "Train Epoch: 438 [176384/225000 (78%)] Loss: 6736.775391\n",
      "Train Epoch: 438 [180480/225000 (80%)] Loss: 6811.132812\n",
      "Train Epoch: 438 [184576/225000 (82%)] Loss: 6790.943359\n",
      "Train Epoch: 438 [188672/225000 (84%)] Loss: 6700.175781\n",
      "Train Epoch: 438 [192768/225000 (86%)] Loss: 6888.351562\n",
      "Train Epoch: 438 [196864/225000 (87%)] Loss: 6855.244141\n",
      "Train Epoch: 438 [200960/225000 (89%)] Loss: 6861.203125\n",
      "Train Epoch: 438 [205056/225000 (91%)] Loss: 6847.765625\n",
      "Train Epoch: 438 [209152/225000 (93%)] Loss: 6802.273438\n",
      "Train Epoch: 438 [213248/225000 (95%)] Loss: 6722.669922\n",
      "Train Epoch: 438 [217344/225000 (97%)] Loss: 6699.402344\n",
      "Train Epoch: 438 [221440/225000 (98%)] Loss: 6802.179688\n",
      "    epoch          : 438\n",
      "    loss           : 6764.983080693615\n",
      "    val_loss       : 6775.140394641428\n",
      "Train Epoch: 439 [256/225000 (0%)] Loss: 6832.810547\n",
      "Train Epoch: 439 [4352/225000 (2%)] Loss: 6758.486328\n",
      "Train Epoch: 439 [8448/225000 (4%)] Loss: 6968.189453\n",
      "Train Epoch: 439 [12544/225000 (6%)] Loss: 6755.230469\n",
      "Train Epoch: 439 [16640/225000 (7%)] Loss: 6832.494141\n",
      "Train Epoch: 439 [20736/225000 (9%)] Loss: 6847.724609\n",
      "Train Epoch: 439 [24832/225000 (11%)] Loss: 6767.238281\n",
      "Train Epoch: 439 [28928/225000 (13%)] Loss: 6735.193359\n",
      "Train Epoch: 439 [33024/225000 (15%)] Loss: 6905.007812\n",
      "Train Epoch: 439 [37120/225000 (16%)] Loss: 6749.435547\n",
      "Train Epoch: 439 [41216/225000 (18%)] Loss: 6754.371094\n",
      "Train Epoch: 439 [45312/225000 (20%)] Loss: 6701.474609\n",
      "Train Epoch: 439 [49408/225000 (22%)] Loss: 6789.529297\n",
      "Train Epoch: 439 [53504/225000 (24%)] Loss: 6760.646484\n",
      "Train Epoch: 439 [57600/225000 (26%)] Loss: 6605.466797\n",
      "Train Epoch: 439 [61696/225000 (27%)] Loss: 6652.603516\n",
      "Train Epoch: 439 [65792/225000 (29%)] Loss: 6829.658203\n",
      "Train Epoch: 439 [69888/225000 (31%)] Loss: 6776.210938\n",
      "Train Epoch: 439 [73984/225000 (33%)] Loss: 6758.064453\n",
      "Train Epoch: 439 [78080/225000 (35%)] Loss: 6711.287109\n",
      "Train Epoch: 439 [82176/225000 (37%)] Loss: 6770.376953\n",
      "Train Epoch: 439 [86272/225000 (38%)] Loss: 6789.033203\n",
      "Train Epoch: 439 [90368/225000 (40%)] Loss: 6831.650391\n",
      "Train Epoch: 439 [94464/225000 (42%)] Loss: 6573.400391\n",
      "Train Epoch: 439 [98560/225000 (44%)] Loss: 6681.378906\n",
      "Train Epoch: 439 [102656/225000 (46%)] Loss: 6880.000000\n",
      "Train Epoch: 439 [106752/225000 (47%)] Loss: 6719.554688\n",
      "Train Epoch: 439 [110848/225000 (49%)] Loss: 6764.144531\n",
      "Train Epoch: 439 [114944/225000 (51%)] Loss: 6812.787109\n",
      "Train Epoch: 439 [119040/225000 (53%)] Loss: 6744.751953\n",
      "Train Epoch: 439 [123136/225000 (55%)] Loss: 6720.755859\n",
      "Train Epoch: 439 [127232/225000 (57%)] Loss: 6781.177734\n",
      "Train Epoch: 439 [131328/225000 (58%)] Loss: 6836.191406\n",
      "Train Epoch: 439 [135424/225000 (60%)] Loss: 6682.013672\n",
      "Train Epoch: 439 [139520/225000 (62%)] Loss: 6718.595703\n",
      "Train Epoch: 439 [143616/225000 (64%)] Loss: 6630.703125\n",
      "Train Epoch: 439 [147712/225000 (66%)] Loss: 6661.990234\n",
      "Train Epoch: 439 [151808/225000 (67%)] Loss: 6679.714844\n",
      "Train Epoch: 439 [155904/225000 (69%)] Loss: 6768.548828\n",
      "Train Epoch: 439 [160000/225000 (71%)] Loss: 6766.630859\n",
      "Train Epoch: 439 [164096/225000 (73%)] Loss: 6734.000000\n",
      "Train Epoch: 439 [168192/225000 (75%)] Loss: 6782.222656\n",
      "Train Epoch: 439 [172288/225000 (77%)] Loss: 6788.363281\n",
      "Train Epoch: 439 [176384/225000 (78%)] Loss: 6606.900391\n",
      "Train Epoch: 439 [180480/225000 (80%)] Loss: 6766.933594\n",
      "Train Epoch: 439 [184576/225000 (82%)] Loss: 6908.779297\n",
      "Train Epoch: 439 [188672/225000 (84%)] Loss: 6672.748047\n",
      "Train Epoch: 439 [192768/225000 (86%)] Loss: 6843.382812\n",
      "Train Epoch: 439 [196864/225000 (87%)] Loss: 6885.060547\n",
      "Train Epoch: 439 [200960/225000 (89%)] Loss: 6766.867188\n",
      "Train Epoch: 439 [205056/225000 (91%)] Loss: 6612.734375\n",
      "Train Epoch: 439 [209152/225000 (93%)] Loss: 6718.998047\n",
      "Train Epoch: 439 [213248/225000 (95%)] Loss: 6698.687500\n",
      "Train Epoch: 439 [217344/225000 (97%)] Loss: 6796.800781\n",
      "Train Epoch: 439 [221440/225000 (98%)] Loss: 6662.736328\n",
      "    epoch          : 439\n",
      "    loss           : 6771.2650561717865\n",
      "    val_loss       : 6775.55151445525\n",
      "Train Epoch: 440 [256/225000 (0%)] Loss: 6961.539062\n",
      "Train Epoch: 440 [4352/225000 (2%)] Loss: 6714.791016\n",
      "Train Epoch: 440 [8448/225000 (4%)] Loss: 6733.363281\n",
      "Train Epoch: 440 [12544/225000 (6%)] Loss: 6745.222656\n",
      "Train Epoch: 440 [16640/225000 (7%)] Loss: 6891.833984\n",
      "Train Epoch: 440 [20736/225000 (9%)] Loss: 6885.421875\n",
      "Train Epoch: 440 [24832/225000 (11%)] Loss: 6703.039062\n",
      "Train Epoch: 440 [28928/225000 (13%)] Loss: 6695.169922\n",
      "Train Epoch: 440 [33024/225000 (15%)] Loss: 6883.824219\n",
      "Train Epoch: 440 [37120/225000 (16%)] Loss: 6733.712891\n",
      "Train Epoch: 440 [41216/225000 (18%)] Loss: 6725.722656\n",
      "Train Epoch: 440 [45312/225000 (20%)] Loss: 6840.390625\n",
      "Train Epoch: 440 [49408/225000 (22%)] Loss: 6809.167969\n",
      "Train Epoch: 440 [53504/225000 (24%)] Loss: 6667.712891\n",
      "Train Epoch: 440 [57600/225000 (26%)] Loss: 6764.072266\n",
      "Train Epoch: 440 [61696/225000 (27%)] Loss: 6823.259766\n",
      "Train Epoch: 440 [65792/225000 (29%)] Loss: 6916.876953\n",
      "Train Epoch: 440 [69888/225000 (31%)] Loss: 6758.515625\n",
      "Train Epoch: 440 [73984/225000 (33%)] Loss: 6693.072266\n",
      "Train Epoch: 440 [78080/225000 (35%)] Loss: 6803.699219\n",
      "Train Epoch: 440 [82176/225000 (37%)] Loss: 6857.587891\n",
      "Train Epoch: 440 [86272/225000 (38%)] Loss: 6614.546875\n",
      "Train Epoch: 440 [90368/225000 (40%)] Loss: 6625.462891\n",
      "Train Epoch: 440 [94464/225000 (42%)] Loss: 6808.890625\n",
      "Train Epoch: 440 [98560/225000 (44%)] Loss: 6711.505859\n",
      "Train Epoch: 440 [102656/225000 (46%)] Loss: 6670.652344\n",
      "Train Epoch: 440 [106752/225000 (47%)] Loss: 6864.037109\n",
      "Train Epoch: 440 [110848/225000 (49%)] Loss: 6648.005859\n",
      "Train Epoch: 440 [114944/225000 (51%)] Loss: 6781.503906\n",
      "Train Epoch: 440 [119040/225000 (53%)] Loss: 6731.656250\n",
      "Train Epoch: 440 [123136/225000 (55%)] Loss: 6775.619141\n",
      "Train Epoch: 440 [127232/225000 (57%)] Loss: 6670.265625\n",
      "Train Epoch: 440 [131328/225000 (58%)] Loss: 6843.748047\n",
      "Train Epoch: 440 [135424/225000 (60%)] Loss: 6623.236328\n",
      "Train Epoch: 440 [139520/225000 (62%)] Loss: 6790.966797\n",
      "Train Epoch: 440 [143616/225000 (64%)] Loss: 6616.367188\n",
      "Train Epoch: 440 [147712/225000 (66%)] Loss: 6671.074219\n",
      "Train Epoch: 440 [151808/225000 (67%)] Loss: 6709.164062\n",
      "Train Epoch: 440 [155904/225000 (69%)] Loss: 7044.451172\n",
      "Train Epoch: 440 [160000/225000 (71%)] Loss: 6792.083984\n",
      "Train Epoch: 440 [164096/225000 (73%)] Loss: 6849.705078\n",
      "Train Epoch: 440 [168192/225000 (75%)] Loss: 6820.980469\n",
      "Train Epoch: 440 [172288/225000 (77%)] Loss: 6783.691406\n",
      "Train Epoch: 440 [176384/225000 (78%)] Loss: 6734.714844\n",
      "Train Epoch: 440 [180480/225000 (80%)] Loss: 6643.687500\n",
      "Train Epoch: 440 [184576/225000 (82%)] Loss: 6851.785156\n",
      "Train Epoch: 440 [188672/225000 (84%)] Loss: 6667.439453\n",
      "Train Epoch: 440 [192768/225000 (86%)] Loss: 6856.367188\n",
      "Train Epoch: 440 [196864/225000 (87%)] Loss: 6750.373047\n",
      "Train Epoch: 440 [200960/225000 (89%)] Loss: 6714.283203\n",
      "Train Epoch: 440 [205056/225000 (91%)] Loss: 6713.539062\n",
      "Train Epoch: 440 [209152/225000 (93%)] Loss: 6922.513672\n",
      "Train Epoch: 440 [213248/225000 (95%)] Loss: 6839.593750\n",
      "Train Epoch: 440 [217344/225000 (97%)] Loss: 6723.353516\n",
      "Train Epoch: 440 [221440/225000 (98%)] Loss: 6691.949219\n",
      "    epoch          : 440\n",
      "    loss           : 6767.979167777659\n",
      "    val_loss       : 6767.990253998309\n",
      "Train Epoch: 441 [256/225000 (0%)] Loss: 6517.593750\n",
      "Train Epoch: 441 [4352/225000 (2%)] Loss: 6646.326172\n",
      "Train Epoch: 441 [8448/225000 (4%)] Loss: 6775.822266\n",
      "Train Epoch: 441 [12544/225000 (6%)] Loss: 6847.279297\n",
      "Train Epoch: 441 [16640/225000 (7%)] Loss: 6618.736328\n",
      "Train Epoch: 441 [20736/225000 (9%)] Loss: 6625.388672\n",
      "Train Epoch: 441 [24832/225000 (11%)] Loss: 6828.302734\n",
      "Train Epoch: 441 [28928/225000 (13%)] Loss: 6710.162109\n",
      "Train Epoch: 441 [33024/225000 (15%)] Loss: 6630.015625\n",
      "Train Epoch: 441 [37120/225000 (16%)] Loss: 6735.761719\n",
      "Train Epoch: 441 [41216/225000 (18%)] Loss: 6780.005859\n",
      "Train Epoch: 441 [45312/225000 (20%)] Loss: 6802.347656\n",
      "Train Epoch: 441 [49408/225000 (22%)] Loss: 6694.572266\n",
      "Train Epoch: 441 [53504/225000 (24%)] Loss: 6712.000000\n",
      "Train Epoch: 441 [57600/225000 (26%)] Loss: 6793.589844\n",
      "Train Epoch: 441 [61696/225000 (27%)] Loss: 6794.750000\n",
      "Train Epoch: 441 [65792/225000 (29%)] Loss: 6851.830078\n",
      "Train Epoch: 441 [69888/225000 (31%)] Loss: 6691.619141\n",
      "Train Epoch: 441 [73984/225000 (33%)] Loss: 6626.785156\n",
      "Train Epoch: 441 [78080/225000 (35%)] Loss: 6759.775391\n",
      "Train Epoch: 441 [82176/225000 (37%)] Loss: 6783.464844\n",
      "Train Epoch: 441 [86272/225000 (38%)] Loss: 6657.449219\n",
      "Train Epoch: 441 [90368/225000 (40%)] Loss: 6613.388672\n",
      "Train Epoch: 441 [94464/225000 (42%)] Loss: 6897.669922\n",
      "Train Epoch: 441 [98560/225000 (44%)] Loss: 6903.312500\n",
      "Train Epoch: 441 [102656/225000 (46%)] Loss: 6690.142578\n",
      "Train Epoch: 441 [106752/225000 (47%)] Loss: 6669.419922\n",
      "Train Epoch: 441 [110848/225000 (49%)] Loss: 6767.210938\n",
      "Train Epoch: 441 [114944/225000 (51%)] Loss: 6616.228516\n",
      "Train Epoch: 441 [119040/225000 (53%)] Loss: 6802.070312\n",
      "Train Epoch: 441 [123136/225000 (55%)] Loss: 6812.283203\n",
      "Train Epoch: 441 [127232/225000 (57%)] Loss: 6858.755859\n",
      "Train Epoch: 441 [131328/225000 (58%)] Loss: 6536.191406\n",
      "Train Epoch: 441 [135424/225000 (60%)] Loss: 6790.054688\n",
      "Train Epoch: 441 [139520/225000 (62%)] Loss: 6868.408203\n",
      "Train Epoch: 441 [143616/225000 (64%)] Loss: 6864.947266\n",
      "Train Epoch: 441 [147712/225000 (66%)] Loss: 6864.576172\n",
      "Train Epoch: 441 [151808/225000 (67%)] Loss: 6837.058594\n",
      "Train Epoch: 441 [155904/225000 (69%)] Loss: 6655.763672\n",
      "Train Epoch: 441 [160000/225000 (71%)] Loss: 6847.656250\n",
      "Train Epoch: 441 [164096/225000 (73%)] Loss: 6788.677734\n",
      "Train Epoch: 441 [168192/225000 (75%)] Loss: 6743.226562\n",
      "Train Epoch: 441 [172288/225000 (77%)] Loss: 6875.771484\n",
      "Train Epoch: 441 [176384/225000 (78%)] Loss: 6703.914062\n",
      "Train Epoch: 441 [180480/225000 (80%)] Loss: 6726.990234\n",
      "Train Epoch: 441 [184576/225000 (82%)] Loss: 6610.175781\n",
      "Train Epoch: 441 [188672/225000 (84%)] Loss: 6879.339844\n",
      "Train Epoch: 441 [192768/225000 (86%)] Loss: 6780.367188\n",
      "Train Epoch: 441 [196864/225000 (87%)] Loss: 6744.507812\n",
      "Train Epoch: 441 [200960/225000 (89%)] Loss: 6740.591797\n",
      "Train Epoch: 441 [205056/225000 (91%)] Loss: 6789.103516\n",
      "Train Epoch: 441 [209152/225000 (93%)] Loss: 6778.025391\n",
      "Train Epoch: 441 [213248/225000 (95%)] Loss: 6639.277344\n",
      "Train Epoch: 441 [217344/225000 (97%)] Loss: 6895.144531\n",
      "Train Epoch: 441 [221440/225000 (98%)] Loss: 6827.042969\n",
      "    epoch          : 441\n",
      "    loss           : 6782.645755563851\n",
      "    val_loss       : 6769.333131298727\n",
      "Train Epoch: 442 [256/225000 (0%)] Loss: 6610.681641\n",
      "Train Epoch: 442 [4352/225000 (2%)] Loss: 6664.015625\n",
      "Train Epoch: 442 [8448/225000 (4%)] Loss: 6637.220703\n",
      "Train Epoch: 442 [12544/225000 (6%)] Loss: 6881.541016\n",
      "Train Epoch: 442 [16640/225000 (7%)] Loss: 6730.593750\n",
      "Train Epoch: 442 [20736/225000 (9%)] Loss: 6647.361328\n",
      "Train Epoch: 442 [24832/225000 (11%)] Loss: 6562.224609\n",
      "Train Epoch: 442 [28928/225000 (13%)] Loss: 6786.615234\n",
      "Train Epoch: 442 [33024/225000 (15%)] Loss: 6788.619141\n",
      "Train Epoch: 442 [37120/225000 (16%)] Loss: 6763.054688\n",
      "Train Epoch: 442 [41216/225000 (18%)] Loss: 6810.056641\n",
      "Train Epoch: 442 [45312/225000 (20%)] Loss: 6518.873047\n",
      "Train Epoch: 442 [49408/225000 (22%)] Loss: 6849.960938\n",
      "Train Epoch: 442 [53504/225000 (24%)] Loss: 6724.917969\n",
      "Train Epoch: 442 [57600/225000 (26%)] Loss: 6706.417969\n",
      "Train Epoch: 442 [61696/225000 (27%)] Loss: 6694.320312\n",
      "Train Epoch: 442 [65792/225000 (29%)] Loss: 6743.886719\n",
      "Train Epoch: 442 [69888/225000 (31%)] Loss: 6829.136719\n",
      "Train Epoch: 442 [73984/225000 (33%)] Loss: 6682.408203\n",
      "Train Epoch: 442 [78080/225000 (35%)] Loss: 6653.753906\n",
      "Train Epoch: 442 [82176/225000 (37%)] Loss: 6807.953125\n",
      "Train Epoch: 442 [86272/225000 (38%)] Loss: 6655.882812\n",
      "Train Epoch: 442 [90368/225000 (40%)] Loss: 6739.496094\n",
      "Train Epoch: 442 [94464/225000 (42%)] Loss: 6759.513672\n",
      "Train Epoch: 442 [98560/225000 (44%)] Loss: 6814.695312\n",
      "Train Epoch: 442 [102656/225000 (46%)] Loss: 6659.867188\n",
      "Train Epoch: 442 [106752/225000 (47%)] Loss: 6638.246094\n",
      "Train Epoch: 442 [110848/225000 (49%)] Loss: 6742.191406\n",
      "Train Epoch: 442 [114944/225000 (51%)] Loss: 6790.302734\n",
      "Train Epoch: 442 [119040/225000 (53%)] Loss: 6718.406250\n",
      "Train Epoch: 442 [123136/225000 (55%)] Loss: 6642.281250\n",
      "Train Epoch: 442 [127232/225000 (57%)] Loss: 6701.607422\n",
      "Train Epoch: 442 [131328/225000 (58%)] Loss: 6754.291016\n",
      "Train Epoch: 442 [135424/225000 (60%)] Loss: 6813.990234\n",
      "Train Epoch: 442 [139520/225000 (62%)] Loss: 6900.939453\n",
      "Train Epoch: 442 [143616/225000 (64%)] Loss: 6737.677734\n",
      "Train Epoch: 442 [147712/225000 (66%)] Loss: 6553.980469\n",
      "Train Epoch: 442 [151808/225000 (67%)] Loss: 6598.740234\n",
      "Train Epoch: 442 [155904/225000 (69%)] Loss: 6805.640625\n",
      "Train Epoch: 442 [160000/225000 (71%)] Loss: 6697.482422\n",
      "Train Epoch: 442 [164096/225000 (73%)] Loss: 6832.109375\n",
      "Train Epoch: 442 [168192/225000 (75%)] Loss: 6779.621094\n",
      "Train Epoch: 442 [172288/225000 (77%)] Loss: 6718.751953\n",
      "Train Epoch: 442 [176384/225000 (78%)] Loss: 6735.500000\n",
      "Train Epoch: 442 [180480/225000 (80%)] Loss: 6575.660156\n",
      "Train Epoch: 442 [184576/225000 (82%)] Loss: 6807.537109\n",
      "Train Epoch: 442 [188672/225000 (84%)] Loss: 6873.453125\n",
      "Train Epoch: 442 [192768/225000 (86%)] Loss: 6665.931641\n",
      "Train Epoch: 442 [196864/225000 (87%)] Loss: 6723.515625\n",
      "Train Epoch: 442 [200960/225000 (89%)] Loss: 6883.894531\n",
      "Train Epoch: 442 [205056/225000 (91%)] Loss: 6906.277344\n",
      "Train Epoch: 442 [209152/225000 (93%)] Loss: 6831.085938\n",
      "Train Epoch: 442 [213248/225000 (95%)] Loss: 6664.886719\n",
      "Train Epoch: 442 [217344/225000 (97%)] Loss: 6921.457031\n",
      "Train Epoch: 442 [221440/225000 (98%)] Loss: 6853.613281\n",
      "    epoch          : 442\n",
      "    loss           : 6771.576890687215\n",
      "    val_loss       : 6766.842645036931\n",
      "Train Epoch: 443 [256/225000 (0%)] Loss: 6741.283203\n",
      "Train Epoch: 443 [4352/225000 (2%)] Loss: 6815.771484\n",
      "Train Epoch: 443 [8448/225000 (4%)] Loss: 6901.041016\n",
      "Train Epoch: 443 [12544/225000 (6%)] Loss: 6561.302734\n",
      "Train Epoch: 443 [16640/225000 (7%)] Loss: 6856.496094\n",
      "Train Epoch: 443 [20736/225000 (9%)] Loss: 6760.357422\n",
      "Train Epoch: 443 [24832/225000 (11%)] Loss: 6712.144531\n",
      "Train Epoch: 443 [28928/225000 (13%)] Loss: 6570.835938\n",
      "Train Epoch: 443 [33024/225000 (15%)] Loss: 6756.132812\n",
      "Train Epoch: 443 [37120/225000 (16%)] Loss: 6861.328125\n",
      "Train Epoch: 443 [41216/225000 (18%)] Loss: 6884.746094\n",
      "Train Epoch: 443 [45312/225000 (20%)] Loss: 6788.771484\n",
      "Train Epoch: 443 [49408/225000 (22%)] Loss: 6587.013672\n",
      "Train Epoch: 443 [53504/225000 (24%)] Loss: 6766.203125\n",
      "Train Epoch: 443 [57600/225000 (26%)] Loss: 6769.128906\n",
      "Train Epoch: 443 [61696/225000 (27%)] Loss: 6706.291016\n",
      "Train Epoch: 443 [65792/225000 (29%)] Loss: 6661.742188\n",
      "Train Epoch: 443 [69888/225000 (31%)] Loss: 6757.394531\n",
      "Train Epoch: 443 [73984/225000 (33%)] Loss: 6714.175781\n",
      "Train Epoch: 443 [78080/225000 (35%)] Loss: 6821.970703\n",
      "Train Epoch: 443 [82176/225000 (37%)] Loss: 6756.089844\n",
      "Train Epoch: 443 [86272/225000 (38%)] Loss: 6594.470703\n",
      "Train Epoch: 443 [90368/225000 (40%)] Loss: 6611.195312\n",
      "Train Epoch: 443 [94464/225000 (42%)] Loss: 6781.681641\n",
      "Train Epoch: 443 [98560/225000 (44%)] Loss: 6630.128906\n",
      "Train Epoch: 443 [102656/225000 (46%)] Loss: 6806.013672\n",
      "Train Epoch: 443 [106752/225000 (47%)] Loss: 6752.609375\n",
      "Train Epoch: 443 [110848/225000 (49%)] Loss: 6814.099609\n",
      "Train Epoch: 443 [114944/225000 (51%)] Loss: 6739.837891\n",
      "Train Epoch: 443 [119040/225000 (53%)] Loss: 6891.578125\n",
      "Train Epoch: 443 [123136/225000 (55%)] Loss: 6626.775391\n",
      "Train Epoch: 443 [127232/225000 (57%)] Loss: 6893.863281\n",
      "Train Epoch: 443 [131328/225000 (58%)] Loss: 6743.607422\n",
      "Train Epoch: 443 [135424/225000 (60%)] Loss: 6697.617188\n",
      "Train Epoch: 443 [139520/225000 (62%)] Loss: 6731.566406\n",
      "Train Epoch: 443 [143616/225000 (64%)] Loss: 6791.083984\n",
      "Train Epoch: 443 [147712/225000 (66%)] Loss: 6762.439453\n",
      "Train Epoch: 443 [151808/225000 (67%)] Loss: 6723.294922\n",
      "Train Epoch: 443 [155904/225000 (69%)] Loss: 6624.923828\n",
      "Train Epoch: 443 [160000/225000 (71%)] Loss: 6725.630859\n",
      "Train Epoch: 443 [164096/225000 (73%)] Loss: 6733.558594\n",
      "Train Epoch: 443 [168192/225000 (75%)] Loss: 6745.767578\n",
      "Train Epoch: 443 [172288/225000 (77%)] Loss: 6789.259766\n",
      "Train Epoch: 443 [176384/225000 (78%)] Loss: 6737.820312\n",
      "Train Epoch: 443 [180480/225000 (80%)] Loss: 6723.812500\n",
      "Train Epoch: 443 [184576/225000 (82%)] Loss: 6676.291016\n",
      "Train Epoch: 443 [188672/225000 (84%)] Loss: 6723.662109\n",
      "Train Epoch: 443 [192768/225000 (86%)] Loss: 6668.921875\n",
      "Train Epoch: 443 [196864/225000 (87%)] Loss: 6684.052734\n",
      "Train Epoch: 443 [200960/225000 (89%)] Loss: 6834.296875\n",
      "Train Epoch: 443 [205056/225000 (91%)] Loss: 6723.441406\n",
      "Train Epoch: 443 [209152/225000 (93%)] Loss: 6894.583984\n",
      "Train Epoch: 443 [213248/225000 (95%)] Loss: 6762.062500\n",
      "Train Epoch: 443 [217344/225000 (97%)] Loss: 6813.478516\n",
      "Train Epoch: 443 [221440/225000 (98%)] Loss: 6693.833984\n",
      "    epoch          : 443\n",
      "    loss           : 6747.318752666382\n",
      "    val_loss       : 6767.82821013003\n",
      "Train Epoch: 444 [256/225000 (0%)] Loss: 6654.201172\n",
      "Train Epoch: 444 [4352/225000 (2%)] Loss: 6693.396484\n",
      "Train Epoch: 444 [8448/225000 (4%)] Loss: 6840.080078\n",
      "Train Epoch: 444 [12544/225000 (6%)] Loss: 6760.837891\n",
      "Train Epoch: 444 [16640/225000 (7%)] Loss: 6691.359375\n",
      "Train Epoch: 444 [20736/225000 (9%)] Loss: 6643.527344\n",
      "Train Epoch: 444 [24832/225000 (11%)] Loss: 6681.457031\n",
      "Train Epoch: 444 [28928/225000 (13%)] Loss: 6727.080078\n",
      "Train Epoch: 444 [33024/225000 (15%)] Loss: 6779.357422\n",
      "Train Epoch: 444 [37120/225000 (16%)] Loss: 6652.650391\n",
      "Train Epoch: 444 [41216/225000 (18%)] Loss: 6866.179688\n",
      "Train Epoch: 444 [45312/225000 (20%)] Loss: 6738.291016\n",
      "Train Epoch: 444 [49408/225000 (22%)] Loss: 6807.312500\n",
      "Train Epoch: 444 [53504/225000 (24%)] Loss: 6790.234375\n",
      "Train Epoch: 444 [57600/225000 (26%)] Loss: 6780.322266\n",
      "Train Epoch: 444 [61696/225000 (27%)] Loss: 7014.363281\n",
      "Train Epoch: 444 [65792/225000 (29%)] Loss: 6833.275391\n",
      "Train Epoch: 444 [69888/225000 (31%)] Loss: 6870.636719\n",
      "Train Epoch: 444 [73984/225000 (33%)] Loss: 6717.134766\n",
      "Train Epoch: 444 [78080/225000 (35%)] Loss: 6721.960938\n",
      "Train Epoch: 444 [82176/225000 (37%)] Loss: 6671.847656\n",
      "Train Epoch: 444 [86272/225000 (38%)] Loss: 6780.496094\n",
      "Train Epoch: 444 [90368/225000 (40%)] Loss: 6693.253906\n",
      "Train Epoch: 444 [94464/225000 (42%)] Loss: 6803.414062\n",
      "Train Epoch: 444 [98560/225000 (44%)] Loss: 6614.785156\n",
      "Train Epoch: 444 [102656/225000 (46%)] Loss: 6774.837891\n",
      "Train Epoch: 444 [106752/225000 (47%)] Loss: 6760.314453\n",
      "Train Epoch: 444 [110848/225000 (49%)] Loss: 6852.910156\n",
      "Train Epoch: 444 [114944/225000 (51%)] Loss: 6682.853516\n",
      "Train Epoch: 444 [119040/225000 (53%)] Loss: 6734.177734\n",
      "Train Epoch: 444 [123136/225000 (55%)] Loss: 6644.242188\n",
      "Train Epoch: 444 [127232/225000 (57%)] Loss: 6672.490234\n",
      "Train Epoch: 444 [131328/225000 (58%)] Loss: 6688.816406\n",
      "Train Epoch: 444 [135424/225000 (60%)] Loss: 6822.681641\n",
      "Train Epoch: 444 [139520/225000 (62%)] Loss: 6882.308594\n",
      "Train Epoch: 444 [143616/225000 (64%)] Loss: 6905.230469\n",
      "Train Epoch: 444 [147712/225000 (66%)] Loss: 6820.814453\n",
      "Train Epoch: 444 [151808/225000 (67%)] Loss: 6671.033203\n",
      "Train Epoch: 444 [155904/225000 (69%)] Loss: 6600.910156\n",
      "Train Epoch: 444 [160000/225000 (71%)] Loss: 6649.472656\n",
      "Train Epoch: 444 [164096/225000 (73%)] Loss: 6659.332031\n",
      "Train Epoch: 444 [168192/225000 (75%)] Loss: 6780.251953\n",
      "Train Epoch: 444 [172288/225000 (77%)] Loss: 6740.464844\n",
      "Train Epoch: 444 [176384/225000 (78%)] Loss: 6881.058594\n",
      "Train Epoch: 444 [180480/225000 (80%)] Loss: 6789.054688\n",
      "Train Epoch: 444 [184576/225000 (82%)] Loss: 6856.550781\n",
      "Train Epoch: 444 [188672/225000 (84%)] Loss: 6805.634766\n",
      "Train Epoch: 444 [192768/225000 (86%)] Loss: 6769.296875\n",
      "Train Epoch: 444 [196864/225000 (87%)] Loss: 6649.369141\n",
      "Train Epoch: 444 [200960/225000 (89%)] Loss: 6834.857422\n",
      "Train Epoch: 444 [205056/225000 (91%)] Loss: 6752.152344\n",
      "Train Epoch: 444 [209152/225000 (93%)] Loss: 6850.771484\n",
      "Train Epoch: 444 [213248/225000 (95%)] Loss: 6665.390625\n",
      "Train Epoch: 444 [217344/225000 (97%)] Loss: 6687.972656\n",
      "Train Epoch: 444 [221440/225000 (98%)] Loss: 6772.437500\n",
      "    epoch          : 444\n",
      "    loss           : 6807.643744667235\n",
      "    val_loss       : 6891.785179627185\n",
      "Train Epoch: 445 [256/225000 (0%)] Loss: 6773.464844\n",
      "Train Epoch: 445 [4352/225000 (2%)] Loss: 6700.666016\n",
      "Train Epoch: 445 [8448/225000 (4%)] Loss: 6705.808594\n",
      "Train Epoch: 445 [12544/225000 (6%)] Loss: 6676.246094\n",
      "Train Epoch: 445 [16640/225000 (7%)] Loss: 6865.982422\n",
      "Train Epoch: 445 [20736/225000 (9%)] Loss: 6783.476562\n",
      "Train Epoch: 445 [24832/225000 (11%)] Loss: 6687.421875\n",
      "Train Epoch: 445 [28928/225000 (13%)] Loss: 6658.205078\n",
      "Train Epoch: 445 [33024/225000 (15%)] Loss: 6678.875000\n",
      "Train Epoch: 445 [37120/225000 (16%)] Loss: 6815.330078\n",
      "Train Epoch: 445 [41216/225000 (18%)] Loss: 6754.968750\n",
      "Train Epoch: 445 [45312/225000 (20%)] Loss: 6745.296875\n",
      "Train Epoch: 445 [49408/225000 (22%)] Loss: 6739.720703\n",
      "Train Epoch: 445 [53504/225000 (24%)] Loss: 6545.875000\n",
      "Train Epoch: 445 [57600/225000 (26%)] Loss: 6935.625000\n",
      "Train Epoch: 445 [61696/225000 (27%)] Loss: 6726.316406\n",
      "Train Epoch: 445 [65792/225000 (29%)] Loss: 6708.501953\n",
      "Train Epoch: 445 [69888/225000 (31%)] Loss: 6838.890625\n",
      "Train Epoch: 445 [73984/225000 (33%)] Loss: 6622.175781\n",
      "Train Epoch: 445 [78080/225000 (35%)] Loss: 6859.394531\n",
      "Train Epoch: 445 [82176/225000 (37%)] Loss: 6754.166016\n",
      "Train Epoch: 445 [86272/225000 (38%)] Loss: 6742.171875\n",
      "Train Epoch: 445 [90368/225000 (40%)] Loss: 6830.251953\n",
      "Train Epoch: 445 [94464/225000 (42%)] Loss: 6746.941406\n",
      "Train Epoch: 445 [98560/225000 (44%)] Loss: 6943.322266\n",
      "Train Epoch: 445 [102656/225000 (46%)] Loss: 6677.601562\n",
      "Train Epoch: 445 [106752/225000 (47%)] Loss: 6984.273438\n",
      "Train Epoch: 445 [110848/225000 (49%)] Loss: 6595.693359\n",
      "Train Epoch: 445 [114944/225000 (51%)] Loss: 6799.273438\n",
      "Train Epoch: 445 [119040/225000 (53%)] Loss: 6591.527344\n",
      "Train Epoch: 445 [123136/225000 (55%)] Loss: 6746.226562\n",
      "Train Epoch: 445 [127232/225000 (57%)] Loss: 6618.876953\n",
      "Train Epoch: 445 [131328/225000 (58%)] Loss: 6708.179688\n",
      "Train Epoch: 445 [135424/225000 (60%)] Loss: 6590.152344\n",
      "Train Epoch: 445 [139520/225000 (62%)] Loss: 6605.863281\n",
      "Train Epoch: 445 [143616/225000 (64%)] Loss: 6624.986328\n",
      "Train Epoch: 445 [147712/225000 (66%)] Loss: 6781.818359\n",
      "Train Epoch: 445 [151808/225000 (67%)] Loss: 6708.759766\n",
      "Train Epoch: 445 [155904/225000 (69%)] Loss: 6752.126953\n",
      "Train Epoch: 445 [160000/225000 (71%)] Loss: 6655.548828\n",
      "Train Epoch: 445 [164096/225000 (73%)] Loss: 6837.121094\n",
      "Train Epoch: 445 [168192/225000 (75%)] Loss: 6625.847656\n",
      "Train Epoch: 445 [172288/225000 (77%)] Loss: 6715.603516\n",
      "Train Epoch: 445 [176384/225000 (78%)] Loss: 6921.660156\n",
      "Train Epoch: 445 [180480/225000 (80%)] Loss: 6810.625000\n",
      "Train Epoch: 445 [184576/225000 (82%)] Loss: 6732.666016\n",
      "Train Epoch: 445 [188672/225000 (84%)] Loss: 6733.763672\n",
      "Train Epoch: 445 [192768/225000 (86%)] Loss: 6763.591797\n",
      "Train Epoch: 445 [196864/225000 (87%)] Loss: 6816.636719\n",
      "Train Epoch: 445 [200960/225000 (89%)] Loss: 6777.443359\n",
      "Train Epoch: 445 [205056/225000 (91%)] Loss: 6774.210938\n",
      "Train Epoch: 445 [209152/225000 (93%)] Loss: 6688.857422\n",
      "Train Epoch: 445 [213248/225000 (95%)] Loss: 6674.722656\n",
      "Train Epoch: 445 [217344/225000 (97%)] Loss: 6927.125000\n",
      "Train Epoch: 445 [221440/225000 (98%)] Loss: 6709.769531\n",
      "    epoch          : 445\n",
      "    loss           : 6756.291153388083\n",
      "    val_loss       : 6764.276285950018\n",
      "Train Epoch: 446 [256/225000 (0%)] Loss: 6783.957031\n",
      "Train Epoch: 446 [4352/225000 (2%)] Loss: 6614.458984\n",
      "Train Epoch: 446 [8448/225000 (4%)] Loss: 6589.167969\n",
      "Train Epoch: 446 [12544/225000 (6%)] Loss: 6787.816406\n",
      "Train Epoch: 446 [16640/225000 (7%)] Loss: 6773.955078\n",
      "Train Epoch: 446 [20736/225000 (9%)] Loss: 6619.730469\n",
      "Train Epoch: 446 [24832/225000 (11%)] Loss: 6787.820312\n",
      "Train Epoch: 446 [28928/225000 (13%)] Loss: 6853.664062\n",
      "Train Epoch: 446 [33024/225000 (15%)] Loss: 6807.349609\n",
      "Train Epoch: 446 [37120/225000 (16%)] Loss: 6672.386719\n",
      "Train Epoch: 446 [41216/225000 (18%)] Loss: 6756.898438\n",
      "Train Epoch: 446 [45312/225000 (20%)] Loss: 6747.863281\n",
      "Train Epoch: 446 [49408/225000 (22%)] Loss: 6618.724609\n",
      "Train Epoch: 446 [53504/225000 (24%)] Loss: 6788.800781\n",
      "Train Epoch: 446 [57600/225000 (26%)] Loss: 6690.449219\n",
      "Train Epoch: 446 [61696/225000 (27%)] Loss: 6843.228516\n",
      "Train Epoch: 446 [65792/225000 (29%)] Loss: 6745.248047\n",
      "Train Epoch: 446 [69888/225000 (31%)] Loss: 6783.472656\n",
      "Train Epoch: 446 [73984/225000 (33%)] Loss: 6572.298828\n",
      "Train Epoch: 446 [78080/225000 (35%)] Loss: 6657.763672\n",
      "Train Epoch: 446 [82176/225000 (37%)] Loss: 6755.312500\n",
      "Train Epoch: 446 [86272/225000 (38%)] Loss: 6718.332031\n",
      "Train Epoch: 446 [90368/225000 (40%)] Loss: 6699.818359\n",
      "Train Epoch: 446 [94464/225000 (42%)] Loss: 6741.343750\n",
      "Train Epoch: 446 [98560/225000 (44%)] Loss: 6909.552734\n",
      "Train Epoch: 446 [102656/225000 (46%)] Loss: 6600.271484\n",
      "Train Epoch: 446 [106752/225000 (47%)] Loss: 6695.591797\n",
      "Train Epoch: 446 [110848/225000 (49%)] Loss: 6933.480469\n",
      "Train Epoch: 446 [114944/225000 (51%)] Loss: 6763.292969\n",
      "Train Epoch: 446 [119040/225000 (53%)] Loss: 6721.537109\n",
      "Train Epoch: 446 [123136/225000 (55%)] Loss: 6871.707031\n",
      "Train Epoch: 446 [127232/225000 (57%)] Loss: 6820.640625\n",
      "Train Epoch: 446 [131328/225000 (58%)] Loss: 6804.089844\n",
      "Train Epoch: 446 [135424/225000 (60%)] Loss: 6860.800781\n",
      "Train Epoch: 446 [139520/225000 (62%)] Loss: 6786.443359\n",
      "Train Epoch: 446 [143616/225000 (64%)] Loss: 6791.871094\n",
      "Train Epoch: 446 [147712/225000 (66%)] Loss: 6522.242188\n",
      "Train Epoch: 446 [151808/225000 (67%)] Loss: 6569.613281\n",
      "Train Epoch: 446 [155904/225000 (69%)] Loss: 6761.814453\n",
      "Train Epoch: 446 [160000/225000 (71%)] Loss: 6640.677734\n",
      "Train Epoch: 446 [164096/225000 (73%)] Loss: 6880.691406\n",
      "Train Epoch: 446 [168192/225000 (75%)] Loss: 6773.189453\n",
      "Train Epoch: 446 [172288/225000 (77%)] Loss: 6893.998047\n",
      "Train Epoch: 446 [176384/225000 (78%)] Loss: 6714.025391\n",
      "Train Epoch: 446 [180480/225000 (80%)] Loss: 6807.546875\n",
      "Train Epoch: 446 [184576/225000 (82%)] Loss: 6739.955078\n",
      "Train Epoch: 446 [188672/225000 (84%)] Loss: 6699.562500\n",
      "Train Epoch: 446 [192768/225000 (86%)] Loss: 6738.677734\n",
      "Train Epoch: 446 [196864/225000 (87%)] Loss: 6720.333984\n",
      "Train Epoch: 446 [200960/225000 (89%)] Loss: 6750.902344\n",
      "Train Epoch: 446 [205056/225000 (91%)] Loss: 6629.634766\n",
      "Train Epoch: 446 [209152/225000 (93%)] Loss: 6849.189453\n",
      "Train Epoch: 446 [213248/225000 (95%)] Loss: 6905.546875\n",
      "Train Epoch: 446 [217344/225000 (97%)] Loss: 6856.173828\n",
      "Train Epoch: 446 [221440/225000 (98%)] Loss: 6609.017578\n",
      "    epoch          : 446\n",
      "    loss           : 6757.532606521971\n",
      "    val_loss       : 6866.355994841274\n",
      "Train Epoch: 447 [256/225000 (0%)] Loss: 6748.890625\n",
      "Train Epoch: 447 [4352/225000 (2%)] Loss: 6768.507812\n",
      "Train Epoch: 447 [8448/225000 (4%)] Loss: 6695.373047\n",
      "Train Epoch: 447 [12544/225000 (6%)] Loss: 6669.894531\n",
      "Train Epoch: 447 [16640/225000 (7%)] Loss: 6794.925781\n",
      "Train Epoch: 447 [20736/225000 (9%)] Loss: 6699.068359\n",
      "Train Epoch: 447 [24832/225000 (11%)] Loss: 6719.927734\n",
      "Train Epoch: 447 [28928/225000 (13%)] Loss: 6686.712891\n",
      "Train Epoch: 447 [33024/225000 (15%)] Loss: 6850.214844\n",
      "Train Epoch: 447 [37120/225000 (16%)] Loss: 6587.312500\n",
      "Train Epoch: 447 [41216/225000 (18%)] Loss: 6797.234375\n",
      "Train Epoch: 447 [45312/225000 (20%)] Loss: 6706.576172\n",
      "Train Epoch: 447 [49408/225000 (22%)] Loss: 6839.427734\n",
      "Train Epoch: 447 [53504/225000 (24%)] Loss: 6798.152344\n",
      "Train Epoch: 447 [57600/225000 (26%)] Loss: 6641.933594\n",
      "Train Epoch: 447 [61696/225000 (27%)] Loss: 6840.302734\n",
      "Train Epoch: 447 [65792/225000 (29%)] Loss: 6790.023438\n",
      "Train Epoch: 447 [69888/225000 (31%)] Loss: 6755.855469\n",
      "Train Epoch: 447 [73984/225000 (33%)] Loss: 6807.603516\n",
      "Train Epoch: 447 [78080/225000 (35%)] Loss: 6589.111328\n",
      "Train Epoch: 447 [82176/225000 (37%)] Loss: 6628.654297\n",
      "Train Epoch: 447 [86272/225000 (38%)] Loss: 6924.726562\n",
      "Train Epoch: 447 [90368/225000 (40%)] Loss: 6781.375000\n",
      "Train Epoch: 447 [94464/225000 (42%)] Loss: 6728.310547\n",
      "Train Epoch: 447 [98560/225000 (44%)] Loss: 6658.177734\n",
      "Train Epoch: 447 [102656/225000 (46%)] Loss: 6907.533203\n",
      "Train Epoch: 447 [106752/225000 (47%)] Loss: 6693.722656\n",
      "Train Epoch: 447 [110848/225000 (49%)] Loss: 6657.017578\n",
      "Train Epoch: 447 [114944/225000 (51%)] Loss: 6870.748047\n",
      "Train Epoch: 447 [119040/225000 (53%)] Loss: 6892.853516\n",
      "Train Epoch: 447 [123136/225000 (55%)] Loss: 6858.181641\n",
      "Train Epoch: 447 [127232/225000 (57%)] Loss: 6746.076172\n",
      "Train Epoch: 447 [131328/225000 (58%)] Loss: 6880.949219\n",
      "Train Epoch: 447 [135424/225000 (60%)] Loss: 6742.123047\n",
      "Train Epoch: 447 [139520/225000 (62%)] Loss: 6939.255859\n",
      "Train Epoch: 447 [143616/225000 (64%)] Loss: 6717.109375\n",
      "Train Epoch: 447 [147712/225000 (66%)] Loss: 6592.232422\n",
      "Train Epoch: 447 [151808/225000 (67%)] Loss: 6580.605469\n",
      "Train Epoch: 447 [155904/225000 (69%)] Loss: 6767.814453\n",
      "Train Epoch: 447 [160000/225000 (71%)] Loss: 6674.250000\n",
      "Train Epoch: 447 [164096/225000 (73%)] Loss: 6703.011719\n",
      "Train Epoch: 447 [168192/225000 (75%)] Loss: 6801.552734\n",
      "Train Epoch: 447 [172288/225000 (77%)] Loss: 6633.734375\n",
      "Train Epoch: 447 [176384/225000 (78%)] Loss: 6687.214844\n",
      "Train Epoch: 447 [180480/225000 (80%)] Loss: 6834.998047\n",
      "Train Epoch: 447 [184576/225000 (82%)] Loss: 6775.099609\n",
      "Train Epoch: 447 [188672/225000 (84%)] Loss: 6831.074219\n",
      "Train Epoch: 447 [192768/225000 (86%)] Loss: 6714.765625\n",
      "Train Epoch: 447 [196864/225000 (87%)] Loss: 6873.783203\n",
      "Train Epoch: 447 [200960/225000 (89%)] Loss: 6669.146484\n",
      "Train Epoch: 447 [205056/225000 (91%)] Loss: 6600.947266\n",
      "Train Epoch: 447 [209152/225000 (93%)] Loss: 6868.904297\n",
      "Train Epoch: 447 [213248/225000 (95%)] Loss: 6641.380859\n",
      "Train Epoch: 447 [217344/225000 (97%)] Loss: 6921.011719\n",
      "Train Epoch: 447 [221440/225000 (98%)] Loss: 6711.226562\n",
      "    epoch          : 447\n",
      "    loss           : 6754.781567743885\n",
      "    val_loss       : 6764.702514721423\n",
      "Train Epoch: 448 [256/225000 (0%)] Loss: 6687.437500\n",
      "Train Epoch: 448 [4352/225000 (2%)] Loss: 6695.605469\n",
      "Train Epoch: 448 [8448/225000 (4%)] Loss: 6748.804688\n",
      "Train Epoch: 448 [12544/225000 (6%)] Loss: 6616.248047\n",
      "Train Epoch: 448 [16640/225000 (7%)] Loss: 6852.972656\n",
      "Train Epoch: 448 [20736/225000 (9%)] Loss: 6896.912109\n",
      "Train Epoch: 448 [24832/225000 (11%)] Loss: 6831.343750\n",
      "Train Epoch: 448 [28928/225000 (13%)] Loss: 6878.357422\n",
      "Train Epoch: 448 [33024/225000 (15%)] Loss: 6796.089844\n",
      "Train Epoch: 448 [37120/225000 (16%)] Loss: 6661.234375\n",
      "Train Epoch: 448 [41216/225000 (18%)] Loss: 6661.867188\n",
      "Train Epoch: 448 [45312/225000 (20%)] Loss: 6593.785156\n",
      "Train Epoch: 448 [49408/225000 (22%)] Loss: 6589.056641\n",
      "Train Epoch: 448 [53504/225000 (24%)] Loss: 6668.296875\n",
      "Train Epoch: 448 [57600/225000 (26%)] Loss: 6775.017578\n",
      "Train Epoch: 448 [61696/225000 (27%)] Loss: 6801.695312\n",
      "Train Epoch: 448 [65792/225000 (29%)] Loss: 6599.939453\n",
      "Train Epoch: 448 [69888/225000 (31%)] Loss: 6753.314453\n",
      "Train Epoch: 448 [73984/225000 (33%)] Loss: 6827.191406\n",
      "Train Epoch: 448 [78080/225000 (35%)] Loss: 6595.894531\n",
      "Train Epoch: 448 [82176/225000 (37%)] Loss: 6737.564453\n",
      "Train Epoch: 448 [86272/225000 (38%)] Loss: 6697.398438\n",
      "Train Epoch: 448 [90368/225000 (40%)] Loss: 6668.716797\n",
      "Train Epoch: 448 [94464/225000 (42%)] Loss: 6806.732422\n",
      "Train Epoch: 448 [98560/225000 (44%)] Loss: 6665.105469\n",
      "Train Epoch: 448 [102656/225000 (46%)] Loss: 6788.503906\n",
      "Train Epoch: 448 [106752/225000 (47%)] Loss: 6744.826172\n",
      "Train Epoch: 448 [110848/225000 (49%)] Loss: 6616.281250\n",
      "Train Epoch: 448 [114944/225000 (51%)] Loss: 6866.722656\n",
      "Train Epoch: 448 [119040/225000 (53%)] Loss: 6784.785156\n",
      "Train Epoch: 448 [123136/225000 (55%)] Loss: 6711.636719\n",
      "Train Epoch: 448 [127232/225000 (57%)] Loss: 6818.257812\n",
      "Train Epoch: 448 [131328/225000 (58%)] Loss: 6692.832031\n",
      "Train Epoch: 448 [135424/225000 (60%)] Loss: 6872.546875\n",
      "Train Epoch: 448 [139520/225000 (62%)] Loss: 6724.681641\n",
      "Train Epoch: 448 [143616/225000 (64%)] Loss: 6761.023438\n",
      "Train Epoch: 448 [147712/225000 (66%)] Loss: 6663.751953\n",
      "Train Epoch: 448 [151808/225000 (67%)] Loss: 6643.544922\n",
      "Train Epoch: 448 [155904/225000 (69%)] Loss: 6703.062500\n",
      "Train Epoch: 448 [160000/225000 (71%)] Loss: 6761.167969\n",
      "Train Epoch: 448 [164096/225000 (73%)] Loss: 6694.234375\n",
      "Train Epoch: 448 [168192/225000 (75%)] Loss: 6696.855469\n",
      "Train Epoch: 448 [172288/225000 (77%)] Loss: 6723.361328\n",
      "Train Epoch: 448 [176384/225000 (78%)] Loss: 6731.564453\n",
      "Train Epoch: 448 [180480/225000 (80%)] Loss: 6822.433594\n",
      "Train Epoch: 448 [184576/225000 (82%)] Loss: 6808.935547\n",
      "Train Epoch: 448 [188672/225000 (84%)] Loss: 6875.066406\n",
      "Train Epoch: 448 [192768/225000 (86%)] Loss: 6814.033203\n",
      "Train Epoch: 448 [196864/225000 (87%)] Loss: 6746.328125\n",
      "Train Epoch: 448 [200960/225000 (89%)] Loss: 6717.869141\n",
      "Train Epoch: 448 [205056/225000 (91%)] Loss: 6855.914062\n",
      "Train Epoch: 448 [209152/225000 (93%)] Loss: 6832.625000\n",
      "Train Epoch: 448 [213248/225000 (95%)] Loss: 6849.595703\n",
      "Train Epoch: 448 [217344/225000 (97%)] Loss: 6974.121094\n",
      "Train Epoch: 448 [221440/225000 (98%)] Loss: 6531.332031\n",
      "    epoch          : 448\n",
      "    loss           : 6754.585641975967\n",
      "    val_loss       : 6763.010207653046\n",
      "Train Epoch: 449 [256/225000 (0%)] Loss: 6753.167969\n",
      "Train Epoch: 449 [4352/225000 (2%)] Loss: 6875.484375\n",
      "Train Epoch: 449 [8448/225000 (4%)] Loss: 6800.212891\n",
      "Train Epoch: 449 [12544/225000 (6%)] Loss: 6689.369141\n",
      "Train Epoch: 449 [16640/225000 (7%)] Loss: 6719.787109\n",
      "Train Epoch: 449 [20736/225000 (9%)] Loss: 6639.031250\n",
      "Train Epoch: 449 [24832/225000 (11%)] Loss: 6697.572266\n",
      "Train Epoch: 449 [28928/225000 (13%)] Loss: 6798.330078\n",
      "Train Epoch: 449 [33024/225000 (15%)] Loss: 6601.835938\n",
      "Train Epoch: 449 [37120/225000 (16%)] Loss: 6536.144531\n",
      "Train Epoch: 449 [41216/225000 (18%)] Loss: 6765.103516\n",
      "Train Epoch: 449 [45312/225000 (20%)] Loss: 6717.091797\n",
      "Train Epoch: 449 [49408/225000 (22%)] Loss: 6713.226562\n",
      "Train Epoch: 449 [53504/225000 (24%)] Loss: 6653.925781\n",
      "Train Epoch: 449 [57600/225000 (26%)] Loss: 6780.099609\n",
      "Train Epoch: 449 [61696/225000 (27%)] Loss: 6778.224609\n",
      "Train Epoch: 449 [65792/225000 (29%)] Loss: 6559.644531\n",
      "Train Epoch: 449 [69888/225000 (31%)] Loss: 6649.091797\n",
      "Train Epoch: 449 [73984/225000 (33%)] Loss: 6670.250000\n",
      "Train Epoch: 449 [78080/225000 (35%)] Loss: 6782.554688\n",
      "Train Epoch: 449 [82176/225000 (37%)] Loss: 6727.566406\n",
      "Train Epoch: 449 [86272/225000 (38%)] Loss: 6679.535156\n",
      "Train Epoch: 449 [90368/225000 (40%)] Loss: 6704.087891\n",
      "Train Epoch: 449 [94464/225000 (42%)] Loss: 6786.646484\n",
      "Train Epoch: 449 [98560/225000 (44%)] Loss: 6687.990234\n",
      "Train Epoch: 449 [102656/225000 (46%)] Loss: 6688.916016\n",
      "Train Epoch: 449 [106752/225000 (47%)] Loss: 6842.779297\n",
      "Train Epoch: 449 [110848/225000 (49%)] Loss: 6901.507812\n",
      "Train Epoch: 449 [114944/225000 (51%)] Loss: 6758.648438\n",
      "Train Epoch: 449 [119040/225000 (53%)] Loss: 6636.843750\n",
      "Train Epoch: 449 [123136/225000 (55%)] Loss: 6911.064453\n",
      "Train Epoch: 449 [127232/225000 (57%)] Loss: 6706.966797\n",
      "Train Epoch: 449 [131328/225000 (58%)] Loss: 6831.445312\n",
      "Train Epoch: 449 [135424/225000 (60%)] Loss: 6645.734375\n",
      "Train Epoch: 449 [139520/225000 (62%)] Loss: 6704.031250\n",
      "Train Epoch: 449 [143616/225000 (64%)] Loss: 6823.904297\n",
      "Train Epoch: 449 [147712/225000 (66%)] Loss: 6739.867188\n",
      "Train Epoch: 449 [151808/225000 (67%)] Loss: 6800.878906\n",
      "Train Epoch: 449 [155904/225000 (69%)] Loss: 6703.107422\n",
      "Train Epoch: 449 [160000/225000 (71%)] Loss: 6658.794922\n",
      "Train Epoch: 449 [164096/225000 (73%)] Loss: 6814.593750\n",
      "Train Epoch: 449 [168192/225000 (75%)] Loss: 6581.974609\n",
      "Train Epoch: 449 [172288/225000 (77%)] Loss: 6666.824219\n",
      "Train Epoch: 449 [176384/225000 (78%)] Loss: 6802.306641\n",
      "Train Epoch: 449 [180480/225000 (80%)] Loss: 6663.664062\n",
      "Train Epoch: 449 [184576/225000 (82%)] Loss: 6803.865234\n",
      "Train Epoch: 449 [188672/225000 (84%)] Loss: 6661.478516\n",
      "Train Epoch: 449 [192768/225000 (86%)] Loss: 6690.361328\n",
      "Train Epoch: 449 [196864/225000 (87%)] Loss: 6757.765625\n",
      "Train Epoch: 449 [200960/225000 (89%)] Loss: 6814.681641\n",
      "Train Epoch: 449 [205056/225000 (91%)] Loss: 6696.658203\n",
      "Train Epoch: 449 [209152/225000 (93%)] Loss: 6821.318359\n",
      "Train Epoch: 449 [213248/225000 (95%)] Loss: 6745.937500\n",
      "Train Epoch: 449 [217344/225000 (97%)] Loss: 6787.169922\n",
      "Train Epoch: 449 [221440/225000 (98%)] Loss: 6788.283203\n",
      "    epoch          : 449\n",
      "    loss           : 6766.410047372725\n",
      "    val_loss       : 6760.767825140028\n",
      "Train Epoch: 450 [256/225000 (0%)] Loss: 6681.949219\n",
      "Train Epoch: 450 [4352/225000 (2%)] Loss: 6736.970703\n",
      "Train Epoch: 450 [8448/225000 (4%)] Loss: 6783.152344\n",
      "Train Epoch: 450 [12544/225000 (6%)] Loss: 6606.570312\n",
      "Train Epoch: 450 [16640/225000 (7%)] Loss: 6572.916016\n",
      "Train Epoch: 450 [20736/225000 (9%)] Loss: 6923.722656\n",
      "Train Epoch: 450 [24832/225000 (11%)] Loss: 6542.996094\n",
      "Train Epoch: 450 [28928/225000 (13%)] Loss: 6735.673828\n",
      "Train Epoch: 450 [33024/225000 (15%)] Loss: 6647.277344\n",
      "Train Epoch: 450 [37120/225000 (16%)] Loss: 6721.009766\n",
      "Train Epoch: 450 [41216/225000 (18%)] Loss: 6716.273438\n",
      "Train Epoch: 450 [45312/225000 (20%)] Loss: 6878.419922\n",
      "Train Epoch: 450 [49408/225000 (22%)] Loss: 6853.509766\n",
      "Train Epoch: 450 [53504/225000 (24%)] Loss: 6548.228516\n",
      "Train Epoch: 450 [57600/225000 (26%)] Loss: 6914.888672\n",
      "Train Epoch: 450 [61696/225000 (27%)] Loss: 6796.287109\n",
      "Train Epoch: 450 [65792/225000 (29%)] Loss: 6814.458984\n",
      "Train Epoch: 450 [69888/225000 (31%)] Loss: 6693.835938\n",
      "Train Epoch: 450 [73984/225000 (33%)] Loss: 6789.066406\n",
      "Train Epoch: 450 [78080/225000 (35%)] Loss: 6709.597656\n",
      "Train Epoch: 450 [82176/225000 (37%)] Loss: 6602.460938\n",
      "Train Epoch: 450 [86272/225000 (38%)] Loss: 6787.605469\n",
      "Train Epoch: 450 [90368/225000 (40%)] Loss: 6616.615234\n",
      "Train Epoch: 450 [94464/225000 (42%)] Loss: 6772.769531\n",
      "Train Epoch: 450 [98560/225000 (44%)] Loss: 6673.998047\n",
      "Train Epoch: 450 [102656/225000 (46%)] Loss: 6735.941406\n",
      "Train Epoch: 450 [106752/225000 (47%)] Loss: 6810.251953\n",
      "Train Epoch: 450 [110848/225000 (49%)] Loss: 6716.449219\n",
      "Train Epoch: 450 [114944/225000 (51%)] Loss: 6659.410156\n",
      "Train Epoch: 450 [119040/225000 (53%)] Loss: 6690.816406\n",
      "Train Epoch: 450 [123136/225000 (55%)] Loss: 6747.859375\n",
      "Train Epoch: 450 [127232/225000 (57%)] Loss: 6766.644531\n",
      "Train Epoch: 450 [131328/225000 (58%)] Loss: 6700.636719\n",
      "Train Epoch: 450 [135424/225000 (60%)] Loss: 6805.363281\n",
      "Train Epoch: 450 [139520/225000 (62%)] Loss: 6732.527344\n",
      "Train Epoch: 450 [143616/225000 (64%)] Loss: 6636.597656\n",
      "Train Epoch: 450 [147712/225000 (66%)] Loss: 6810.976562\n",
      "Train Epoch: 450 [151808/225000 (67%)] Loss: 6838.359375\n",
      "Train Epoch: 450 [155904/225000 (69%)] Loss: 6740.671875\n",
      "Train Epoch: 450 [160000/225000 (71%)] Loss: 6789.769531\n",
      "Train Epoch: 450 [164096/225000 (73%)] Loss: 6672.623047\n",
      "Train Epoch: 450 [168192/225000 (75%)] Loss: 6686.076172\n",
      "Train Epoch: 450 [172288/225000 (77%)] Loss: 6734.033203\n",
      "Train Epoch: 450 [176384/225000 (78%)] Loss: 6677.279297\n",
      "Train Epoch: 450 [180480/225000 (80%)] Loss: 6761.015625\n",
      "Train Epoch: 450 [184576/225000 (82%)] Loss: 6685.595703\n",
      "Train Epoch: 450 [188672/225000 (84%)] Loss: 6819.863281\n",
      "Train Epoch: 450 [192768/225000 (86%)] Loss: 6700.972656\n",
      "Train Epoch: 450 [196864/225000 (87%)] Loss: 6669.945312\n",
      "Train Epoch: 450 [200960/225000 (89%)] Loss: 6651.140625\n",
      "Train Epoch: 450 [205056/225000 (91%)] Loss: 6675.976562\n",
      "Train Epoch: 450 [209152/225000 (93%)] Loss: 6687.480469\n",
      "Train Epoch: 450 [213248/225000 (95%)] Loss: 6855.744141\n",
      "Train Epoch: 450 [217344/225000 (97%)] Loss: 6725.357422\n",
      "Train Epoch: 450 [221440/225000 (98%)] Loss: 6817.681641\n",
      "    epoch          : 450\n",
      "    loss           : 6751.671737236917\n",
      "    val_loss       : 6767.551693738723\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0827_225017/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [256/225000 (0%)] Loss: 6652.357422\n",
      "Train Epoch: 451 [4352/225000 (2%)] Loss: 6744.703125\n",
      "Train Epoch: 451 [8448/225000 (4%)] Loss: 6815.062500\n",
      "Train Epoch: 451 [12544/225000 (6%)] Loss: 6864.271484\n",
      "Train Epoch: 451 [16640/225000 (7%)] Loss: 6664.138672\n",
      "Train Epoch: 451 [20736/225000 (9%)] Loss: 6637.621094\n",
      "Train Epoch: 451 [24832/225000 (11%)] Loss: 6776.250000\n",
      "Train Epoch: 451 [28928/225000 (13%)] Loss: 6757.207031\n",
      "Train Epoch: 451 [33024/225000 (15%)] Loss: 6856.865234\n",
      "Train Epoch: 451 [37120/225000 (16%)] Loss: 6724.076172\n",
      "Train Epoch: 451 [41216/225000 (18%)] Loss: 6745.376953\n",
      "Train Epoch: 451 [45312/225000 (20%)] Loss: 6776.593750\n",
      "Train Epoch: 451 [49408/225000 (22%)] Loss: 6887.103516\n",
      "Train Epoch: 451 [53504/225000 (24%)] Loss: 6669.937500\n",
      "Train Epoch: 451 [57600/225000 (26%)] Loss: 6642.705078\n",
      "Train Epoch: 451 [61696/225000 (27%)] Loss: 6736.021484\n",
      "Train Epoch: 451 [65792/225000 (29%)] Loss: 6880.283203\n",
      "Train Epoch: 451 [69888/225000 (31%)] Loss: 6761.160156\n",
      "Train Epoch: 451 [73984/225000 (33%)] Loss: 6724.654297\n",
      "Train Epoch: 451 [78080/225000 (35%)] Loss: 6732.560547\n",
      "Train Epoch: 451 [82176/225000 (37%)] Loss: 6813.369141\n",
      "Train Epoch: 451 [86272/225000 (38%)] Loss: 6911.617188\n",
      "Train Epoch: 451 [90368/225000 (40%)] Loss: 6620.451172\n",
      "Train Epoch: 451 [94464/225000 (42%)] Loss: 6720.103516\n",
      "Train Epoch: 451 [98560/225000 (44%)] Loss: 6684.218750\n",
      "Train Epoch: 451 [102656/225000 (46%)] Loss: 6892.080078\n",
      "Train Epoch: 451 [106752/225000 (47%)] Loss: 6662.103516\n",
      "Train Epoch: 451 [110848/225000 (49%)] Loss: 6790.087891\n",
      "Train Epoch: 451 [114944/225000 (51%)] Loss: 6658.962891\n",
      "Train Epoch: 451 [119040/225000 (53%)] Loss: 6682.619141\n",
      "Train Epoch: 451 [123136/225000 (55%)] Loss: 6769.962891\n",
      "Train Epoch: 451 [127232/225000 (57%)] Loss: 6641.986328\n",
      "Train Epoch: 451 [131328/225000 (58%)] Loss: 6792.876953\n",
      "Train Epoch: 451 [135424/225000 (60%)] Loss: 6726.554688\n",
      "Train Epoch: 451 [139520/225000 (62%)] Loss: 6664.976562\n",
      "Train Epoch: 451 [143616/225000 (64%)] Loss: 6811.275391\n",
      "Train Epoch: 451 [147712/225000 (66%)] Loss: 6865.169922\n",
      "Train Epoch: 451 [151808/225000 (67%)] Loss: 6615.226562\n",
      "Train Epoch: 451 [155904/225000 (69%)] Loss: 6840.144531\n",
      "Train Epoch: 451 [160000/225000 (71%)] Loss: 6920.957031\n",
      "Train Epoch: 451 [164096/225000 (73%)] Loss: 6666.441406\n",
      "Train Epoch: 451 [168192/225000 (75%)] Loss: 6610.400391\n",
      "Train Epoch: 451 [172288/225000 (77%)] Loss: 6805.337891\n",
      "Train Epoch: 451 [176384/225000 (78%)] Loss: 6766.474609\n",
      "Train Epoch: 451 [180480/225000 (80%)] Loss: 6746.888672\n",
      "Train Epoch: 451 [184576/225000 (82%)] Loss: 6582.494141\n",
      "Train Epoch: 451 [188672/225000 (84%)] Loss: 6764.990234\n",
      "Train Epoch: 451 [192768/225000 (86%)] Loss: 6786.958984\n",
      "Train Epoch: 451 [196864/225000 (87%)] Loss: 6808.439453\n",
      "Train Epoch: 451 [200960/225000 (89%)] Loss: 6901.193359\n",
      "Train Epoch: 451 [205056/225000 (91%)] Loss: 6798.181641\n",
      "Train Epoch: 451 [209152/225000 (93%)] Loss: 6841.486328\n",
      "Train Epoch: 451 [213248/225000 (95%)] Loss: 6701.091797\n",
      "Train Epoch: 451 [217344/225000 (97%)] Loss: 6750.292969\n",
      "Train Epoch: 451 [221440/225000 (98%)] Loss: 6864.695312\n",
      "    epoch          : 451\n",
      "    loss           : 6738.149420728456\n",
      "    val_loss       : 6759.04091535539\n",
      "Train Epoch: 452 [256/225000 (0%)] Loss: 6770.771484\n",
      "Train Epoch: 452 [4352/225000 (2%)] Loss: 6755.037109\n",
      "Train Epoch: 452 [8448/225000 (4%)] Loss: 6877.183594\n",
      "Train Epoch: 452 [12544/225000 (6%)] Loss: 6715.546875\n",
      "Train Epoch: 452 [16640/225000 (7%)] Loss: 6779.230469\n",
      "Train Epoch: 452 [20736/225000 (9%)] Loss: 6719.748047\n",
      "Train Epoch: 452 [24832/225000 (11%)] Loss: 6658.248047\n",
      "Train Epoch: 452 [28928/225000 (13%)] Loss: 6823.222656\n",
      "Train Epoch: 452 [33024/225000 (15%)] Loss: 6765.404297\n",
      "Train Epoch: 452 [37120/225000 (16%)] Loss: 6806.968750\n",
      "Train Epoch: 452 [41216/225000 (18%)] Loss: 6971.529297\n",
      "Train Epoch: 452 [45312/225000 (20%)] Loss: 6800.968750\n",
      "Train Epoch: 452 [49408/225000 (22%)] Loss: 6753.271484\n",
      "Train Epoch: 452 [53504/225000 (24%)] Loss: 6869.259766\n",
      "Train Epoch: 452 [57600/225000 (26%)] Loss: 6771.662109\n",
      "Train Epoch: 452 [61696/225000 (27%)] Loss: 6650.908203\n",
      "Train Epoch: 452 [65792/225000 (29%)] Loss: 6769.707031\n",
      "Train Epoch: 452 [69888/225000 (31%)] Loss: 6915.246094\n",
      "Train Epoch: 452 [73984/225000 (33%)] Loss: 6845.054688\n",
      "Train Epoch: 452 [78080/225000 (35%)] Loss: 6845.466797\n",
      "Train Epoch: 452 [82176/225000 (37%)] Loss: 6743.613281\n",
      "Train Epoch: 452 [86272/225000 (38%)] Loss: 6872.771484\n",
      "Train Epoch: 452 [90368/225000 (40%)] Loss: 6855.576172\n",
      "Train Epoch: 452 [94464/225000 (42%)] Loss: 6624.451172\n",
      "Train Epoch: 452 [98560/225000 (44%)] Loss: 6740.988281\n",
      "Train Epoch: 452 [102656/225000 (46%)] Loss: 6794.933594\n",
      "Train Epoch: 452 [106752/225000 (47%)] Loss: 6574.433594\n",
      "Train Epoch: 452 [110848/225000 (49%)] Loss: 6701.687500\n",
      "Train Epoch: 452 [114944/225000 (51%)] Loss: 6840.757812\n",
      "Train Epoch: 452 [119040/225000 (53%)] Loss: 6686.582031\n",
      "Train Epoch: 452 [123136/225000 (55%)] Loss: 6782.968750\n",
      "Train Epoch: 452 [127232/225000 (57%)] Loss: 6667.927734\n",
      "Train Epoch: 452 [131328/225000 (58%)] Loss: 6731.271484\n",
      "Train Epoch: 452 [135424/225000 (60%)] Loss: 6691.796875\n",
      "Train Epoch: 452 [139520/225000 (62%)] Loss: 7001.332031\n",
      "Train Epoch: 452 [143616/225000 (64%)] Loss: 6778.546875\n",
      "Train Epoch: 452 [147712/225000 (66%)] Loss: 6717.148438\n",
      "Train Epoch: 452 [151808/225000 (67%)] Loss: 6817.400391\n",
      "Train Epoch: 452 [155904/225000 (69%)] Loss: 6646.529297\n",
      "Train Epoch: 452 [160000/225000 (71%)] Loss: 6851.480469\n",
      "Train Epoch: 452 [164096/225000 (73%)] Loss: 6607.689453\n",
      "Train Epoch: 452 [168192/225000 (75%)] Loss: 6751.496094\n",
      "Train Epoch: 452 [172288/225000 (77%)] Loss: 6772.576172\n",
      "Train Epoch: 452 [176384/225000 (78%)] Loss: 6778.173828\n",
      "Train Epoch: 452 [180480/225000 (80%)] Loss: 6810.056641\n",
      "Train Epoch: 452 [184576/225000 (82%)] Loss: 6680.119141\n",
      "Train Epoch: 452 [188672/225000 (84%)] Loss: 6814.947266\n",
      "Train Epoch: 452 [192768/225000 (86%)] Loss: 6751.511719\n",
      "Train Epoch: 452 [196864/225000 (87%)] Loss: 6659.611328\n",
      "Train Epoch: 452 [200960/225000 (89%)] Loss: 6776.843750\n",
      "Train Epoch: 452 [205056/225000 (91%)] Loss: 6850.748047\n",
      "Train Epoch: 452 [209152/225000 (93%)] Loss: 6794.556641\n",
      "Train Epoch: 452 [213248/225000 (95%)] Loss: 6726.931641\n",
      "Train Epoch: 452 [217344/225000 (97%)] Loss: 6714.552734\n",
      "Train Epoch: 452 [221440/225000 (98%)] Loss: 6741.091797\n",
      "    epoch          : 452\n",
      "    loss           : 6736.895271171075\n",
      "    val_loss       : 6762.904870154906\n",
      "Train Epoch: 453 [256/225000 (0%)] Loss: 6598.390625\n",
      "Train Epoch: 453 [4352/225000 (2%)] Loss: 6695.472656\n",
      "Train Epoch: 453 [8448/225000 (4%)] Loss: 6762.490234\n",
      "Train Epoch: 453 [12544/225000 (6%)] Loss: 6777.148438\n",
      "Train Epoch: 453 [16640/225000 (7%)] Loss: 6669.921875\n",
      "Train Epoch: 453 [20736/225000 (9%)] Loss: 6797.769531\n",
      "Train Epoch: 453 [24832/225000 (11%)] Loss: 6627.238281\n",
      "Train Epoch: 453 [28928/225000 (13%)] Loss: 6746.535156\n",
      "Train Epoch: 453 [33024/225000 (15%)] Loss: 6729.917969\n",
      "Train Epoch: 453 [37120/225000 (16%)] Loss: 6814.392578\n",
      "Train Epoch: 453 [41216/225000 (18%)] Loss: 6827.568359\n",
      "Train Epoch: 453 [45312/225000 (20%)] Loss: 6653.054688\n",
      "Train Epoch: 453 [49408/225000 (22%)] Loss: 6826.160156\n",
      "Train Epoch: 453 [53504/225000 (24%)] Loss: 6924.035156\n",
      "Train Epoch: 453 [57600/225000 (26%)] Loss: 6595.865234\n",
      "Train Epoch: 453 [61696/225000 (27%)] Loss: 6600.214844\n",
      "Train Epoch: 453 [65792/225000 (29%)] Loss: 6723.675781\n",
      "Train Epoch: 453 [69888/225000 (31%)] Loss: 6672.917969\n",
      "Train Epoch: 453 [73984/225000 (33%)] Loss: 6732.714844\n",
      "Train Epoch: 453 [78080/225000 (35%)] Loss: 6821.537109\n",
      "Train Epoch: 453 [82176/225000 (37%)] Loss: 6711.671875\n",
      "Train Epoch: 453 [86272/225000 (38%)] Loss: 6637.796875\n",
      "Train Epoch: 453 [90368/225000 (40%)] Loss: 6641.699219\n",
      "Train Epoch: 453 [94464/225000 (42%)] Loss: 6764.416016\n",
      "Train Epoch: 453 [98560/225000 (44%)] Loss: 6760.865234\n",
      "Train Epoch: 453 [102656/225000 (46%)] Loss: 6769.873047\n",
      "Train Epoch: 453 [106752/225000 (47%)] Loss: 6800.748047\n",
      "Train Epoch: 453 [110848/225000 (49%)] Loss: 6774.564453\n",
      "Train Epoch: 453 [114944/225000 (51%)] Loss: 6909.146484\n",
      "Train Epoch: 453 [119040/225000 (53%)] Loss: 6670.476562\n",
      "Train Epoch: 453 [123136/225000 (55%)] Loss: 6789.949219\n",
      "Train Epoch: 453 [127232/225000 (57%)] Loss: 6598.375000\n",
      "Train Epoch: 453 [131328/225000 (58%)] Loss: 6707.138672\n",
      "Train Epoch: 453 [135424/225000 (60%)] Loss: 6730.599609\n",
      "Train Epoch: 453 [139520/225000 (62%)] Loss: 6696.857422\n",
      "Train Epoch: 453 [143616/225000 (64%)] Loss: 6670.246094\n",
      "Train Epoch: 453 [147712/225000 (66%)] Loss: 6878.560547\n",
      "Train Epoch: 453 [151808/225000 (67%)] Loss: 6588.605469\n",
      "Train Epoch: 453 [155904/225000 (69%)] Loss: 6788.509766\n",
      "Train Epoch: 453 [160000/225000 (71%)] Loss: 6728.072266\n",
      "Train Epoch: 453 [164096/225000 (73%)] Loss: 6698.863281\n",
      "Train Epoch: 453 [168192/225000 (75%)] Loss: 6860.207031\n",
      "Train Epoch: 453 [172288/225000 (77%)] Loss: 6642.691406\n",
      "Train Epoch: 453 [176384/225000 (78%)] Loss: 6890.599609\n",
      "Train Epoch: 453 [180480/225000 (80%)] Loss: 6742.712891\n",
      "Train Epoch: 453 [184576/225000 (82%)] Loss: 6698.673828\n",
      "Train Epoch: 453 [188672/225000 (84%)] Loss: 6823.423828\n",
      "Train Epoch: 453 [192768/225000 (86%)] Loss: 6739.888672\n",
      "Train Epoch: 453 [196864/225000 (87%)] Loss: 6820.128906\n",
      "Train Epoch: 453 [200960/225000 (89%)] Loss: 6624.177734\n",
      "Train Epoch: 453 [205056/225000 (91%)] Loss: 6870.867188\n",
      "Train Epoch: 453 [209152/225000 (93%)] Loss: 6782.167969\n",
      "Train Epoch: 453 [213248/225000 (95%)] Loss: 6646.097656\n",
      "Train Epoch: 453 [217344/225000 (97%)] Loss: 6639.255859\n",
      "Train Epoch: 453 [221440/225000 (98%)] Loss: 6790.451172\n",
      "    epoch          : 453\n",
      "    loss           : 6749.618226278086\n",
      "    val_loss       : 6760.663745804709\n",
      "Train Epoch: 454 [256/225000 (0%)] Loss: 6878.177734\n",
      "Train Epoch: 454 [4352/225000 (2%)] Loss: 6635.855469\n",
      "Train Epoch: 454 [8448/225000 (4%)] Loss: 6807.263672\n",
      "Train Epoch: 454 [12544/225000 (6%)] Loss: 6735.552734\n",
      "Train Epoch: 454 [16640/225000 (7%)] Loss: 6678.513672\n",
      "Train Epoch: 454 [20736/225000 (9%)] Loss: 6825.390625\n",
      "Train Epoch: 454 [24832/225000 (11%)] Loss: 6618.398438\n",
      "Train Epoch: 454 [28928/225000 (13%)] Loss: 6760.972656\n",
      "Train Epoch: 454 [33024/225000 (15%)] Loss: 6866.220703\n",
      "Train Epoch: 454 [37120/225000 (16%)] Loss: 6760.777344\n",
      "Train Epoch: 454 [41216/225000 (18%)] Loss: 6800.714844\n",
      "Train Epoch: 454 [45312/225000 (20%)] Loss: 6877.253906\n",
      "Train Epoch: 454 [49408/225000 (22%)] Loss: 6775.017578\n",
      "Train Epoch: 454 [53504/225000 (24%)] Loss: 6743.527344\n",
      "Train Epoch: 454 [57600/225000 (26%)] Loss: 6646.175781\n",
      "Train Epoch: 454 [61696/225000 (27%)] Loss: 6724.837891\n",
      "Train Epoch: 454 [65792/225000 (29%)] Loss: 6732.970703\n",
      "Train Epoch: 454 [69888/225000 (31%)] Loss: 6703.962891\n",
      "Train Epoch: 454 [73984/225000 (33%)] Loss: 6684.822266\n",
      "Train Epoch: 454 [78080/225000 (35%)] Loss: 6781.726562\n",
      "Train Epoch: 454 [82176/225000 (37%)] Loss: 6733.654297\n",
      "Train Epoch: 454 [86272/225000 (38%)] Loss: 6739.662109\n",
      "Train Epoch: 454 [90368/225000 (40%)] Loss: 6703.847656\n",
      "Train Epoch: 454 [94464/225000 (42%)] Loss: 6686.652344\n",
      "Train Epoch: 454 [98560/225000 (44%)] Loss: 6728.013672\n",
      "Train Epoch: 454 [102656/225000 (46%)] Loss: 6832.279297\n",
      "Train Epoch: 454 [106752/225000 (47%)] Loss: 6768.078125\n",
      "Train Epoch: 454 [110848/225000 (49%)] Loss: 6772.703125\n",
      "Train Epoch: 454 [114944/225000 (51%)] Loss: 6736.666016\n",
      "Train Epoch: 454 [119040/225000 (53%)] Loss: 6807.771484\n",
      "Train Epoch: 454 [123136/225000 (55%)] Loss: 6735.683594\n",
      "Train Epoch: 454 [127232/225000 (57%)] Loss: 6648.226562\n",
      "Train Epoch: 454 [131328/225000 (58%)] Loss: 6797.130859\n",
      "Train Epoch: 454 [135424/225000 (60%)] Loss: 6732.556641\n",
      "Train Epoch: 454 [139520/225000 (62%)] Loss: 6663.333984\n",
      "Train Epoch: 454 [143616/225000 (64%)] Loss: 6786.011719\n",
      "Train Epoch: 454 [147712/225000 (66%)] Loss: 6677.515625\n",
      "Train Epoch: 454 [151808/225000 (67%)] Loss: 6732.728516\n",
      "Train Epoch: 454 [155904/225000 (69%)] Loss: 6743.160156\n",
      "Train Epoch: 454 [160000/225000 (71%)] Loss: 6744.312500\n",
      "Train Epoch: 454 [164096/225000 (73%)] Loss: 6705.486328\n",
      "Train Epoch: 454 [168192/225000 (75%)] Loss: 6662.134766\n",
      "Train Epoch: 454 [172288/225000 (77%)] Loss: 6776.412109\n",
      "Train Epoch: 454 [176384/225000 (78%)] Loss: 6834.171875\n",
      "Train Epoch: 454 [180480/225000 (80%)] Loss: 6808.556641\n",
      "Train Epoch: 454 [184576/225000 (82%)] Loss: 6788.080078\n",
      "Train Epoch: 454 [188672/225000 (84%)] Loss: 6828.857422\n",
      "Train Epoch: 454 [192768/225000 (86%)] Loss: 6603.113281\n",
      "Train Epoch: 454 [196864/225000 (87%)] Loss: 6642.498047\n",
      "Train Epoch: 454 [200960/225000 (89%)] Loss: 6690.986328\n",
      "Train Epoch: 454 [205056/225000 (91%)] Loss: 6783.146484\n",
      "Train Epoch: 454 [209152/225000 (93%)] Loss: 6798.560547\n",
      "Train Epoch: 454 [213248/225000 (95%)] Loss: 6878.482422\n",
      "Train Epoch: 454 [217344/225000 (97%)] Loss: 6742.841797\n",
      "Train Epoch: 454 [221440/225000 (98%)] Loss: 6814.533203\n",
      "    epoch          : 454\n",
      "    loss           : 6759.913527001564\n",
      "    val_loss       : 7009.051447873213\n",
      "Train Epoch: 455 [256/225000 (0%)] Loss: 6720.494141\n",
      "Train Epoch: 455 [4352/225000 (2%)] Loss: 6851.257812\n",
      "Train Epoch: 455 [8448/225000 (4%)] Loss: 6733.406250\n",
      "Train Epoch: 455 [12544/225000 (6%)] Loss: 6674.859375\n",
      "Train Epoch: 455 [16640/225000 (7%)] Loss: 6772.968750\n",
      "Train Epoch: 455 [20736/225000 (9%)] Loss: 6701.107422\n",
      "Train Epoch: 455 [24832/225000 (11%)] Loss: 6753.925781\n",
      "Train Epoch: 455 [28928/225000 (13%)] Loss: 6704.414062\n",
      "Train Epoch: 455 [33024/225000 (15%)] Loss: 6811.238281\n",
      "Train Epoch: 455 [37120/225000 (16%)] Loss: 6806.517578\n",
      "Train Epoch: 455 [41216/225000 (18%)] Loss: 6629.863281\n",
      "Train Epoch: 455 [45312/225000 (20%)] Loss: 6877.845703\n",
      "Train Epoch: 455 [49408/225000 (22%)] Loss: 6741.642578\n",
      "Train Epoch: 455 [53504/225000 (24%)] Loss: 6830.128906\n",
      "Train Epoch: 455 [57600/225000 (26%)] Loss: 6679.970703\n",
      "Train Epoch: 455 [61696/225000 (27%)] Loss: 6642.847656\n",
      "Train Epoch: 455 [65792/225000 (29%)] Loss: 6705.400391\n",
      "Train Epoch: 455 [69888/225000 (31%)] Loss: 6686.462891\n",
      "Train Epoch: 455 [73984/225000 (33%)] Loss: 6642.230469\n",
      "Train Epoch: 455 [78080/225000 (35%)] Loss: 6777.181641\n",
      "Train Epoch: 455 [82176/225000 (37%)] Loss: 6833.134766\n",
      "Train Epoch: 455 [86272/225000 (38%)] Loss: 6723.732422\n",
      "Train Epoch: 455 [90368/225000 (40%)] Loss: 6750.013672\n",
      "Train Epoch: 455 [94464/225000 (42%)] Loss: 6768.894531\n",
      "Train Epoch: 455 [98560/225000 (44%)] Loss: 6833.056641\n",
      "Train Epoch: 455 [102656/225000 (46%)] Loss: 6691.142578\n",
      "Train Epoch: 455 [106752/225000 (47%)] Loss: 12989.460938\n",
      "Train Epoch: 455 [110848/225000 (49%)] Loss: 6585.861328\n",
      "Train Epoch: 455 [114944/225000 (51%)] Loss: 6811.861328\n",
      "Train Epoch: 455 [119040/225000 (53%)] Loss: 6722.632812\n",
      "Train Epoch: 455 [123136/225000 (55%)] Loss: 6772.113281\n",
      "Train Epoch: 455 [127232/225000 (57%)] Loss: 6746.302734\n",
      "Train Epoch: 455 [131328/225000 (58%)] Loss: 6746.806641\n",
      "Train Epoch: 455 [135424/225000 (60%)] Loss: 6805.193359\n",
      "Train Epoch: 455 [139520/225000 (62%)] Loss: 6915.955078\n",
      "Train Epoch: 455 [143616/225000 (64%)] Loss: 6730.529297\n",
      "Train Epoch: 455 [147712/225000 (66%)] Loss: 6823.839844\n",
      "Train Epoch: 455 [151808/225000 (67%)] Loss: 6684.148438\n",
      "Train Epoch: 455 [155904/225000 (69%)] Loss: 6679.992188\n",
      "Train Epoch: 455 [160000/225000 (71%)] Loss: 6777.390625\n",
      "Train Epoch: 455 [164096/225000 (73%)] Loss: 6752.533203\n",
      "Train Epoch: 455 [168192/225000 (75%)] Loss: 6650.099609\n",
      "Train Epoch: 455 [172288/225000 (77%)] Loss: 6804.802734\n",
      "Train Epoch: 455 [176384/225000 (78%)] Loss: 6656.376953\n",
      "Train Epoch: 455 [180480/225000 (80%)] Loss: 6736.089844\n",
      "Train Epoch: 455 [184576/225000 (82%)] Loss: 6794.419922\n",
      "Train Epoch: 455 [188672/225000 (84%)] Loss: 6856.361328\n",
      "Train Epoch: 455 [192768/225000 (86%)] Loss: 6620.101562\n",
      "Train Epoch: 455 [196864/225000 (87%)] Loss: 6730.462891\n",
      "Train Epoch: 455 [200960/225000 (89%)] Loss: 6676.806641\n",
      "Train Epoch: 455 [205056/225000 (91%)] Loss: 6848.845703\n",
      "Train Epoch: 455 [209152/225000 (93%)] Loss: 6730.447266\n",
      "Train Epoch: 455 [213248/225000 (95%)] Loss: 6757.564453\n",
      "Train Epoch: 455 [217344/225000 (97%)] Loss: 6787.542969\n",
      "Train Epoch: 455 [221440/225000 (98%)] Loss: 6808.722656\n",
      "    epoch          : 455\n",
      "    loss           : 6758.704149335182\n",
      "    val_loss       : 6753.762834558682\n",
      "Train Epoch: 456 [256/225000 (0%)] Loss: 6607.330078\n",
      "Train Epoch: 456 [4352/225000 (2%)] Loss: 6691.630859\n",
      "Train Epoch: 456 [8448/225000 (4%)] Loss: 6752.955078\n",
      "Train Epoch: 456 [12544/225000 (6%)] Loss: 6639.306641\n",
      "Train Epoch: 456 [16640/225000 (7%)] Loss: 6576.623047\n",
      "Train Epoch: 456 [20736/225000 (9%)] Loss: 6791.904297\n",
      "Train Epoch: 456 [24832/225000 (11%)] Loss: 6596.798828\n",
      "Train Epoch: 456 [28928/225000 (13%)] Loss: 6735.230469\n",
      "Train Epoch: 456 [33024/225000 (15%)] Loss: 6601.072266\n",
      "Train Epoch: 456 [37120/225000 (16%)] Loss: 6853.011719\n",
      "Train Epoch: 456 [41216/225000 (18%)] Loss: 6756.806641\n",
      "Train Epoch: 456 [45312/225000 (20%)] Loss: 6686.380859\n",
      "Train Epoch: 456 [49408/225000 (22%)] Loss: 6799.855469\n",
      "Train Epoch: 456 [53504/225000 (24%)] Loss: 6873.896484\n",
      "Train Epoch: 456 [57600/225000 (26%)] Loss: 6781.724609\n",
      "Train Epoch: 456 [61696/225000 (27%)] Loss: 6667.148438\n",
      "Train Epoch: 456 [65792/225000 (29%)] Loss: 6715.757812\n",
      "Train Epoch: 456 [69888/225000 (31%)] Loss: 6722.281250\n",
      "Train Epoch: 456 [73984/225000 (33%)] Loss: 6847.111328\n",
      "Train Epoch: 456 [78080/225000 (35%)] Loss: 6800.033203\n",
      "Train Epoch: 456 [82176/225000 (37%)] Loss: 6875.130859\n",
      "Train Epoch: 456 [86272/225000 (38%)] Loss: 6633.248047\n",
      "Train Epoch: 456 [90368/225000 (40%)] Loss: 6896.576172\n",
      "Train Epoch: 456 [94464/225000 (42%)] Loss: 6791.287109\n",
      "Train Epoch: 456 [98560/225000 (44%)] Loss: 6716.613281\n",
      "Train Epoch: 456 [102656/225000 (46%)] Loss: 6685.156250\n",
      "Train Epoch: 456 [106752/225000 (47%)] Loss: 6673.380859\n",
      "Train Epoch: 456 [110848/225000 (49%)] Loss: 6633.638672\n",
      "Train Epoch: 456 [114944/225000 (51%)] Loss: 6774.572266\n",
      "Train Epoch: 456 [119040/225000 (53%)] Loss: 6812.714844\n",
      "Train Epoch: 456 [123136/225000 (55%)] Loss: 6730.878906\n",
      "Train Epoch: 456 [127232/225000 (57%)] Loss: 6745.041016\n",
      "Train Epoch: 456 [131328/225000 (58%)] Loss: 6745.539062\n",
      "Train Epoch: 456 [135424/225000 (60%)] Loss: 6782.693359\n",
      "Train Epoch: 456 [139520/225000 (62%)] Loss: 6792.714844\n",
      "Train Epoch: 456 [143616/225000 (64%)] Loss: 6861.898438\n",
      "Train Epoch: 456 [147712/225000 (66%)] Loss: 6601.716797\n",
      "Train Epoch: 456 [151808/225000 (67%)] Loss: 6637.697266\n",
      "Train Epoch: 456 [155904/225000 (69%)] Loss: 6741.988281\n",
      "Train Epoch: 456 [160000/225000 (71%)] Loss: 6657.640625\n",
      "Train Epoch: 456 [164096/225000 (73%)] Loss: 6766.039062\n",
      "Train Epoch: 456 [168192/225000 (75%)] Loss: 6625.490234\n",
      "Train Epoch: 456 [172288/225000 (77%)] Loss: 6930.474609\n",
      "Train Epoch: 456 [176384/225000 (78%)] Loss: 6659.378906\n",
      "Train Epoch: 456 [180480/225000 (80%)] Loss: 6652.496094\n",
      "Train Epoch: 456 [184576/225000 (82%)] Loss: 6796.265625\n",
      "Train Epoch: 456 [188672/225000 (84%)] Loss: 6841.039062\n",
      "Train Epoch: 456 [192768/225000 (86%)] Loss: 6873.847656\n",
      "Train Epoch: 456 [196864/225000 (87%)] Loss: 6772.052734\n",
      "Train Epoch: 456 [200960/225000 (89%)] Loss: 6735.001953\n",
      "Train Epoch: 456 [205056/225000 (91%)] Loss: 6731.818359\n",
      "Train Epoch: 456 [209152/225000 (93%)] Loss: 6701.759766\n",
      "Train Epoch: 456 [213248/225000 (95%)] Loss: 6665.427734\n",
      "Train Epoch: 456 [217344/225000 (97%)] Loss: 6803.242188\n",
      "Train Epoch: 456 [221440/225000 (98%)] Loss: 6737.603516\n",
      "    epoch          : 456\n",
      "    loss           : 6732.836590763652\n",
      "    val_loss       : 6859.216638251227\n",
      "Train Epoch: 457 [256/225000 (0%)] Loss: 6895.929688\n",
      "Train Epoch: 457 [4352/225000 (2%)] Loss: 6685.662109\n",
      "Train Epoch: 457 [8448/225000 (4%)] Loss: 6807.283203\n",
      "Train Epoch: 457 [12544/225000 (6%)] Loss: 6740.781250\n",
      "Train Epoch: 457 [16640/225000 (7%)] Loss: 6635.246094\n",
      "Train Epoch: 457 [20736/225000 (9%)] Loss: 6697.615234\n",
      "Train Epoch: 457 [24832/225000 (11%)] Loss: 6643.037109\n",
      "Train Epoch: 457 [28928/225000 (13%)] Loss: 6920.828125\n",
      "Train Epoch: 457 [33024/225000 (15%)] Loss: 6790.107422\n",
      "Train Epoch: 457 [37120/225000 (16%)] Loss: 6704.308594\n",
      "Train Epoch: 457 [41216/225000 (18%)] Loss: 6685.699219\n",
      "Train Epoch: 457 [45312/225000 (20%)] Loss: 6876.468750\n",
      "Train Epoch: 457 [49408/225000 (22%)] Loss: 6808.503906\n",
      "Train Epoch: 457 [53504/225000 (24%)] Loss: 6724.052734\n",
      "Train Epoch: 457 [57600/225000 (26%)] Loss: 6683.816406\n",
      "Train Epoch: 457 [61696/225000 (27%)] Loss: 6788.380859\n",
      "Train Epoch: 457 [65792/225000 (29%)] Loss: 6762.679688\n",
      "Train Epoch: 457 [69888/225000 (31%)] Loss: 6775.234375\n",
      "Train Epoch: 457 [73984/225000 (33%)] Loss: 6887.658203\n",
      "Train Epoch: 457 [78080/225000 (35%)] Loss: 6669.359375\n",
      "Train Epoch: 457 [82176/225000 (37%)] Loss: 6745.357422\n",
      "Train Epoch: 457 [86272/225000 (38%)] Loss: 6570.300781\n",
      "Train Epoch: 457 [90368/225000 (40%)] Loss: 6706.923828\n",
      "Train Epoch: 457 [94464/225000 (42%)] Loss: 6728.359375\n",
      "Train Epoch: 457 [98560/225000 (44%)] Loss: 6747.310547\n",
      "Train Epoch: 457 [102656/225000 (46%)] Loss: 6930.855469\n",
      "Train Epoch: 457 [106752/225000 (47%)] Loss: 6771.832031\n",
      "Train Epoch: 457 [110848/225000 (49%)] Loss: 6583.792969\n",
      "Train Epoch: 457 [114944/225000 (51%)] Loss: 6728.906250\n",
      "Train Epoch: 457 [119040/225000 (53%)] Loss: 6786.230469\n",
      "Train Epoch: 457 [123136/225000 (55%)] Loss: 6614.669922\n",
      "Train Epoch: 457 [127232/225000 (57%)] Loss: 6724.525391\n",
      "Train Epoch: 457 [131328/225000 (58%)] Loss: 6698.494141\n",
      "Train Epoch: 457 [135424/225000 (60%)] Loss: 6738.259766\n",
      "Train Epoch: 457 [139520/225000 (62%)] Loss: 6677.988281\n",
      "Train Epoch: 457 [143616/225000 (64%)] Loss: 6620.859375\n",
      "Train Epoch: 457 [147712/225000 (66%)] Loss: 6952.080078\n",
      "Train Epoch: 457 [151808/225000 (67%)] Loss: 6683.917969\n",
      "Train Epoch: 457 [155904/225000 (69%)] Loss: 6795.560547\n",
      "Train Epoch: 457 [160000/225000 (71%)] Loss: 6632.361328\n",
      "Train Epoch: 457 [164096/225000 (73%)] Loss: 6714.435547\n",
      "Train Epoch: 457 [168192/225000 (75%)] Loss: 6709.880859\n",
      "Train Epoch: 457 [172288/225000 (77%)] Loss: 6844.515625\n",
      "Train Epoch: 457 [176384/225000 (78%)] Loss: 6762.406250\n",
      "Train Epoch: 457 [180480/225000 (80%)] Loss: 6727.585938\n",
      "Train Epoch: 457 [184576/225000 (82%)] Loss: 6704.628906\n",
      "Train Epoch: 457 [188672/225000 (84%)] Loss: 6707.386719\n",
      "Train Epoch: 457 [192768/225000 (86%)] Loss: 6690.167969\n",
      "Train Epoch: 457 [196864/225000 (87%)] Loss: 6777.988281\n",
      "Train Epoch: 457 [200960/225000 (89%)] Loss: 6674.572266\n",
      "Train Epoch: 457 [205056/225000 (91%)] Loss: 6707.593750\n",
      "Train Epoch: 457 [209152/225000 (93%)] Loss: 6734.138672\n",
      "Train Epoch: 457 [213248/225000 (95%)] Loss: 6651.095703\n",
      "Train Epoch: 457 [217344/225000 (97%)] Loss: 6883.708984\n",
      "Train Epoch: 457 [221440/225000 (98%)] Loss: 6716.529297\n",
      "    epoch          : 457\n",
      "    loss           : 6752.016291595563\n",
      "    val_loss       : 6753.725551520075\n",
      "Train Epoch: 458 [256/225000 (0%)] Loss: 6881.523438\n",
      "Train Epoch: 458 [4352/225000 (2%)] Loss: 6734.632812\n",
      "Train Epoch: 458 [8448/225000 (4%)] Loss: 6630.769531\n",
      "Train Epoch: 458 [12544/225000 (6%)] Loss: 6704.365234\n",
      "Train Epoch: 458 [16640/225000 (7%)] Loss: 6689.285156\n",
      "Train Epoch: 458 [20736/225000 (9%)] Loss: 6827.943359\n",
      "Train Epoch: 458 [24832/225000 (11%)] Loss: 6703.724609\n",
      "Train Epoch: 458 [28928/225000 (13%)] Loss: 6722.939453\n",
      "Train Epoch: 458 [33024/225000 (15%)] Loss: 6794.912109\n",
      "Train Epoch: 458 [37120/225000 (16%)] Loss: 6703.978516\n",
      "Train Epoch: 458 [41216/225000 (18%)] Loss: 6655.351562\n",
      "Train Epoch: 458 [45312/225000 (20%)] Loss: 6617.791016\n",
      "Train Epoch: 458 [49408/225000 (22%)] Loss: 6692.515625\n",
      "Train Epoch: 458 [53504/225000 (24%)] Loss: 6570.462891\n",
      "Train Epoch: 458 [57600/225000 (26%)] Loss: 6624.533203\n",
      "Train Epoch: 458 [61696/225000 (27%)] Loss: 6685.896484\n",
      "Train Epoch: 458 [65792/225000 (29%)] Loss: 6646.417969\n",
      "Train Epoch: 458 [69888/225000 (31%)] Loss: 6881.941406\n",
      "Train Epoch: 458 [73984/225000 (33%)] Loss: 6620.785156\n",
      "Train Epoch: 458 [78080/225000 (35%)] Loss: 6589.367188\n",
      "Train Epoch: 458 [82176/225000 (37%)] Loss: 6916.806641\n",
      "Train Epoch: 458 [86272/225000 (38%)] Loss: 6661.417969\n",
      "Train Epoch: 458 [90368/225000 (40%)] Loss: 6870.375000\n",
      "Train Epoch: 458 [94464/225000 (42%)] Loss: 6652.152344\n",
      "Train Epoch: 458 [98560/225000 (44%)] Loss: 6833.589844\n",
      "Train Epoch: 458 [102656/225000 (46%)] Loss: 6802.667969\n",
      "Train Epoch: 458 [106752/225000 (47%)] Loss: 6799.595703\n",
      "Train Epoch: 458 [110848/225000 (49%)] Loss: 6711.302734\n",
      "Train Epoch: 458 [114944/225000 (51%)] Loss: 6680.130859\n",
      "Train Epoch: 458 [119040/225000 (53%)] Loss: 6789.470703\n",
      "Train Epoch: 458 [123136/225000 (55%)] Loss: 6660.562500\n",
      "Train Epoch: 458 [127232/225000 (57%)] Loss: 6650.304688\n",
      "Train Epoch: 458 [131328/225000 (58%)] Loss: 6752.445312\n",
      "Train Epoch: 458 [135424/225000 (60%)] Loss: 6752.574219\n",
      "Train Epoch: 458 [139520/225000 (62%)] Loss: 6503.414062\n",
      "Train Epoch: 458 [143616/225000 (64%)] Loss: 6830.974609\n",
      "Train Epoch: 458 [147712/225000 (66%)] Loss: 6754.083984\n",
      "Train Epoch: 458 [151808/225000 (67%)] Loss: 6736.007812\n",
      "Train Epoch: 458 [155904/225000 (69%)] Loss: 6666.960938\n",
      "Train Epoch: 458 [160000/225000 (71%)] Loss: 6623.957031\n",
      "Train Epoch: 458 [164096/225000 (73%)] Loss: 6797.363281\n",
      "Train Epoch: 458 [168192/225000 (75%)] Loss: 6843.060547\n",
      "Train Epoch: 458 [172288/225000 (77%)] Loss: 6572.939453\n",
      "Train Epoch: 458 [176384/225000 (78%)] Loss: 6802.320312\n",
      "Train Epoch: 458 [180480/225000 (80%)] Loss: 6624.880859\n",
      "Train Epoch: 458 [184576/225000 (82%)] Loss: 6698.431641\n",
      "Train Epoch: 458 [188672/225000 (84%)] Loss: 6841.062500\n",
      "Train Epoch: 458 [192768/225000 (86%)] Loss: 6697.916016\n",
      "Train Epoch: 458 [196864/225000 (87%)] Loss: 6840.724609\n",
      "Train Epoch: 458 [200960/225000 (89%)] Loss: 6746.251953\n",
      "Train Epoch: 458 [205056/225000 (91%)] Loss: 6787.066406\n",
      "Train Epoch: 458 [209152/225000 (93%)] Loss: 6748.349609\n",
      "Train Epoch: 458 [213248/225000 (95%)] Loss: 6677.398438\n",
      "Train Epoch: 458 [217344/225000 (97%)] Loss: 6806.728516\n",
      "Train Epoch: 458 [221440/225000 (98%)] Loss: 6681.980469\n",
      "    epoch          : 458\n",
      "    loss           : 6741.681852824588\n",
      "    val_loss       : 6754.269167824667\n",
      "Train Epoch: 459 [256/225000 (0%)] Loss: 6725.865234\n",
      "Train Epoch: 459 [4352/225000 (2%)] Loss: 6872.312500\n",
      "Train Epoch: 459 [8448/225000 (4%)] Loss: 6619.228516\n",
      "Train Epoch: 459 [12544/225000 (6%)] Loss: 6630.318359\n",
      "Train Epoch: 459 [16640/225000 (7%)] Loss: 6767.853516\n",
      "Train Epoch: 459 [20736/225000 (9%)] Loss: 6672.945312\n",
      "Train Epoch: 459 [24832/225000 (11%)] Loss: 6730.378906\n",
      "Train Epoch: 459 [28928/225000 (13%)] Loss: 6649.839844\n",
      "Train Epoch: 459 [33024/225000 (15%)] Loss: 6749.636719\n",
      "Train Epoch: 459 [37120/225000 (16%)] Loss: 6754.851562\n",
      "Train Epoch: 459 [41216/225000 (18%)] Loss: 6734.125000\n",
      "Train Epoch: 459 [45312/225000 (20%)] Loss: 6762.300781\n",
      "Train Epoch: 459 [49408/225000 (22%)] Loss: 6763.552734\n",
      "Train Epoch: 459 [53504/225000 (24%)] Loss: 6817.837891\n",
      "Train Epoch: 459 [57600/225000 (26%)] Loss: 6816.964844\n",
      "Train Epoch: 459 [61696/225000 (27%)] Loss: 6647.404297\n",
      "Train Epoch: 459 [65792/225000 (29%)] Loss: 6742.958984\n",
      "Train Epoch: 459 [69888/225000 (31%)] Loss: 6767.771484\n",
      "Train Epoch: 459 [73984/225000 (33%)] Loss: 6779.439453\n",
      "Train Epoch: 459 [78080/225000 (35%)] Loss: 6786.765625\n",
      "Train Epoch: 459 [82176/225000 (37%)] Loss: 6638.222656\n",
      "Train Epoch: 459 [86272/225000 (38%)] Loss: 6753.939453\n",
      "Train Epoch: 459 [90368/225000 (40%)] Loss: 6691.753906\n",
      "Train Epoch: 459 [94464/225000 (42%)] Loss: 6581.623047\n",
      "Train Epoch: 459 [98560/225000 (44%)] Loss: 6869.812500\n",
      "Train Epoch: 459 [102656/225000 (46%)] Loss: 6760.958984\n",
      "Train Epoch: 459 [106752/225000 (47%)] Loss: 6724.095703\n",
      "Train Epoch: 459 [110848/225000 (49%)] Loss: 6701.984375\n",
      "Train Epoch: 459 [114944/225000 (51%)] Loss: 6743.490234\n",
      "Train Epoch: 459 [119040/225000 (53%)] Loss: 6715.458984\n",
      "Train Epoch: 459 [123136/225000 (55%)] Loss: 6639.134766\n",
      "Train Epoch: 459 [127232/225000 (57%)] Loss: 6581.978516\n",
      "Train Epoch: 459 [131328/225000 (58%)] Loss: 6652.726562\n",
      "Train Epoch: 459 [135424/225000 (60%)] Loss: 6785.716797\n",
      "Train Epoch: 459 [139520/225000 (62%)] Loss: 6805.621094\n",
      "Train Epoch: 459 [143616/225000 (64%)] Loss: 6685.738281\n",
      "Train Epoch: 459 [147712/225000 (66%)] Loss: 6755.511719\n",
      "Train Epoch: 459 [151808/225000 (67%)] Loss: 6700.736328\n",
      "Train Epoch: 459 [155904/225000 (69%)] Loss: 6805.130859\n",
      "Train Epoch: 459 [160000/225000 (71%)] Loss: 6583.777344\n",
      "Train Epoch: 459 [164096/225000 (73%)] Loss: 6776.027344\n",
      "Train Epoch: 459 [168192/225000 (75%)] Loss: 6669.982422\n",
      "Train Epoch: 459 [172288/225000 (77%)] Loss: 6633.746094\n",
      "Train Epoch: 459 [176384/225000 (78%)] Loss: 6675.023438\n",
      "Train Epoch: 459 [180480/225000 (80%)] Loss: 6897.017578\n",
      "Train Epoch: 459 [184576/225000 (82%)] Loss: 6747.970703\n",
      "Train Epoch: 459 [188672/225000 (84%)] Loss: 6749.884766\n",
      "Train Epoch: 459 [192768/225000 (86%)] Loss: 6770.195312\n",
      "Train Epoch: 459 [196864/225000 (87%)] Loss: 6940.791016\n",
      "Train Epoch: 459 [200960/225000 (89%)] Loss: 6746.919922\n",
      "Train Epoch: 459 [205056/225000 (91%)] Loss: 6734.117188\n",
      "Train Epoch: 459 [209152/225000 (93%)] Loss: 6663.810547\n",
      "Train Epoch: 459 [213248/225000 (95%)] Loss: 6764.816406\n",
      "Train Epoch: 459 [217344/225000 (97%)] Loss: 6593.839844\n",
      "Train Epoch: 459 [221440/225000 (98%)] Loss: 6861.021484\n",
      "    epoch          : 459\n",
      "    loss           : 6749.0618645122295\n",
      "    val_loss       : 6748.726698430217\n",
      "Train Epoch: 460 [256/225000 (0%)] Loss: 6751.128906\n",
      "Train Epoch: 460 [4352/225000 (2%)] Loss: 6744.537109\n",
      "Train Epoch: 460 [8448/225000 (4%)] Loss: 6720.208984\n",
      "Train Epoch: 460 [12544/225000 (6%)] Loss: 6644.929688\n",
      "Train Epoch: 460 [16640/225000 (7%)] Loss: 6854.927734\n",
      "Train Epoch: 460 [20736/225000 (9%)] Loss: 6852.750000\n",
      "Train Epoch: 460 [24832/225000 (11%)] Loss: 6613.775391\n",
      "Train Epoch: 460 [28928/225000 (13%)] Loss: 6746.759766\n",
      "Train Epoch: 460 [33024/225000 (15%)] Loss: 6714.986328\n",
      "Train Epoch: 460 [37120/225000 (16%)] Loss: 6707.113281\n",
      "Train Epoch: 460 [41216/225000 (18%)] Loss: 6881.378906\n",
      "Train Epoch: 460 [45312/225000 (20%)] Loss: 6713.011719\n",
      "Train Epoch: 460 [49408/225000 (22%)] Loss: 6762.953125\n",
      "Train Epoch: 460 [53504/225000 (24%)] Loss: 6521.691406\n",
      "Train Epoch: 460 [57600/225000 (26%)] Loss: 6547.046875\n",
      "Train Epoch: 460 [61696/225000 (27%)] Loss: 6734.158203\n",
      "Train Epoch: 460 [65792/225000 (29%)] Loss: 6781.962891\n",
      "Train Epoch: 460 [69888/225000 (31%)] Loss: 6610.351562\n",
      "Train Epoch: 460 [73984/225000 (33%)] Loss: 6720.210938\n",
      "Train Epoch: 460 [78080/225000 (35%)] Loss: 6802.925781\n",
      "Train Epoch: 460 [82176/225000 (37%)] Loss: 6782.914062\n",
      "Train Epoch: 460 [86272/225000 (38%)] Loss: 6831.082031\n",
      "Train Epoch: 460 [90368/225000 (40%)] Loss: 6714.509766\n",
      "Train Epoch: 460 [94464/225000 (42%)] Loss: 6650.451172\n",
      "Train Epoch: 460 [98560/225000 (44%)] Loss: 6773.251953\n",
      "Train Epoch: 460 [102656/225000 (46%)] Loss: 6673.664062\n",
      "Train Epoch: 460 [106752/225000 (47%)] Loss: 6601.896484\n",
      "Train Epoch: 460 [110848/225000 (49%)] Loss: 6746.740234\n",
      "Train Epoch: 460 [114944/225000 (51%)] Loss: 6759.937500\n",
      "Train Epoch: 460 [119040/225000 (53%)] Loss: 6655.398438\n",
      "Train Epoch: 460 [123136/225000 (55%)] Loss: 6624.359375\n",
      "Train Epoch: 460 [127232/225000 (57%)] Loss: 6687.484375\n",
      "Train Epoch: 460 [131328/225000 (58%)] Loss: 6711.341797\n",
      "Train Epoch: 460 [135424/225000 (60%)] Loss: 6781.166016\n",
      "Train Epoch: 460 [139520/225000 (62%)] Loss: 6834.871094\n",
      "Train Epoch: 460 [143616/225000 (64%)] Loss: 6746.783203\n",
      "Train Epoch: 460 [147712/225000 (66%)] Loss: 6565.798828\n",
      "Train Epoch: 460 [151808/225000 (67%)] Loss: 6746.876953\n",
      "Train Epoch: 460 [155904/225000 (69%)] Loss: 6749.238281\n",
      "Train Epoch: 460 [160000/225000 (71%)] Loss: 6743.056641\n",
      "Train Epoch: 460 [164096/225000 (73%)] Loss: 6668.189453\n",
      "Train Epoch: 460 [168192/225000 (75%)] Loss: 6848.496094\n",
      "Train Epoch: 460 [172288/225000 (77%)] Loss: 6725.078125\n",
      "Train Epoch: 460 [176384/225000 (78%)] Loss: 6774.869141\n",
      "Train Epoch: 460 [180480/225000 (80%)] Loss: 6750.783203\n",
      "Train Epoch: 460 [184576/225000 (82%)] Loss: 6740.009766\n",
      "Train Epoch: 460 [188672/225000 (84%)] Loss: 6864.908203\n",
      "Train Epoch: 460 [192768/225000 (86%)] Loss: 6675.736328\n",
      "Train Epoch: 460 [196864/225000 (87%)] Loss: 6730.003906\n",
      "Train Epoch: 460 [200960/225000 (89%)] Loss: 6573.044922\n",
      "Train Epoch: 460 [205056/225000 (91%)] Loss: 6709.410156\n",
      "Train Epoch: 460 [209152/225000 (93%)] Loss: 6740.712891\n",
      "Train Epoch: 460 [213248/225000 (95%)] Loss: 6788.376953\n",
      "Train Epoch: 460 [217344/225000 (97%)] Loss: 6627.878906\n",
      "Train Epoch: 460 [221440/225000 (98%)] Loss: 6811.984375\n",
      "    epoch          : 460\n",
      "    loss           : 6740.121919217506\n",
      "    val_loss       : 6751.263748572797\n",
      "Train Epoch: 461 [256/225000 (0%)] Loss: 6695.908203\n",
      "Train Epoch: 461 [4352/225000 (2%)] Loss: 6715.251953\n",
      "Train Epoch: 461 [8448/225000 (4%)] Loss: 6818.876953\n",
      "Train Epoch: 461 [12544/225000 (6%)] Loss: 6701.125000\n",
      "Train Epoch: 461 [16640/225000 (7%)] Loss: 6754.005859\n",
      "Train Epoch: 461 [20736/225000 (9%)] Loss: 6760.957031\n",
      "Train Epoch: 461 [24832/225000 (11%)] Loss: 6624.441406\n",
      "Train Epoch: 461 [28928/225000 (13%)] Loss: 6783.345703\n",
      "Train Epoch: 461 [33024/225000 (15%)] Loss: 6614.847656\n",
      "Train Epoch: 461 [37120/225000 (16%)] Loss: 6684.468750\n",
      "Train Epoch: 461 [41216/225000 (18%)] Loss: 6612.417969\n",
      "Train Epoch: 461 [45312/225000 (20%)] Loss: 6697.312500\n",
      "Train Epoch: 461 [49408/225000 (22%)] Loss: 6642.382812\n",
      "Train Epoch: 461 [53504/225000 (24%)] Loss: 6608.517578\n",
      "Train Epoch: 461 [57600/225000 (26%)] Loss: 6665.267578\n",
      "Train Epoch: 461 [61696/225000 (27%)] Loss: 6707.197266\n",
      "Train Epoch: 461 [65792/225000 (29%)] Loss: 6493.128906\n",
      "Train Epoch: 461 [69888/225000 (31%)] Loss: 6482.144531\n",
      "Train Epoch: 461 [73984/225000 (33%)] Loss: 6827.500000\n",
      "Train Epoch: 461 [78080/225000 (35%)] Loss: 6780.970703\n",
      "Train Epoch: 461 [82176/225000 (37%)] Loss: 6605.974609\n",
      "Train Epoch: 461 [86272/225000 (38%)] Loss: 6700.460938\n",
      "Train Epoch: 461 [90368/225000 (40%)] Loss: 6844.486328\n",
      "Train Epoch: 461 [94464/225000 (42%)] Loss: 6769.994141\n",
      "Train Epoch: 461 [98560/225000 (44%)] Loss: 6593.154297\n",
      "Train Epoch: 461 [102656/225000 (46%)] Loss: 6941.099609\n",
      "Train Epoch: 461 [106752/225000 (47%)] Loss: 6888.287109\n",
      "Train Epoch: 461 [110848/225000 (49%)] Loss: 6713.619141\n",
      "Train Epoch: 461 [114944/225000 (51%)] Loss: 6659.810547\n",
      "Train Epoch: 461 [119040/225000 (53%)] Loss: 6688.753906\n",
      "Train Epoch: 461 [123136/225000 (55%)] Loss: 6737.468750\n",
      "Train Epoch: 461 [127232/225000 (57%)] Loss: 6786.908203\n",
      "Train Epoch: 461 [131328/225000 (58%)] Loss: 6917.980469\n",
      "Train Epoch: 461 [135424/225000 (60%)] Loss: 6844.080078\n",
      "Train Epoch: 461 [139520/225000 (62%)] Loss: 6644.744141\n",
      "Train Epoch: 461 [143616/225000 (64%)] Loss: 6695.099609\n",
      "Train Epoch: 461 [147712/225000 (66%)] Loss: 6831.119141\n",
      "Train Epoch: 461 [151808/225000 (67%)] Loss: 6695.316406\n",
      "Train Epoch: 461 [155904/225000 (69%)] Loss: 6750.638672\n",
      "Train Epoch: 461 [160000/225000 (71%)] Loss: 6744.753906\n",
      "Train Epoch: 461 [164096/225000 (73%)] Loss: 6620.140625\n",
      "Train Epoch: 461 [168192/225000 (75%)] Loss: 6689.410156\n",
      "Train Epoch: 461 [172288/225000 (77%)] Loss: 6630.781250\n",
      "Train Epoch: 461 [176384/225000 (78%)] Loss: 6824.050781\n",
      "Train Epoch: 461 [180480/225000 (80%)] Loss: 6685.146484\n",
      "Train Epoch: 461 [184576/225000 (82%)] Loss: 6666.878906\n",
      "Train Epoch: 461 [188672/225000 (84%)] Loss: 6725.921875\n",
      "Train Epoch: 461 [192768/225000 (86%)] Loss: 6670.130859\n",
      "Train Epoch: 461 [196864/225000 (87%)] Loss: 6680.560547\n",
      "Train Epoch: 461 [200960/225000 (89%)] Loss: 6755.986328\n",
      "Train Epoch: 461 [205056/225000 (91%)] Loss: 6939.736328\n",
      "Train Epoch: 461 [209152/225000 (93%)] Loss: 6747.250000\n",
      "Train Epoch: 461 [213248/225000 (95%)] Loss: 6666.957031\n",
      "Train Epoch: 461 [217344/225000 (97%)] Loss: 6660.947266\n",
      "Train Epoch: 461 [221440/225000 (98%)] Loss: 6785.119141\n",
      "    epoch          : 461\n",
      "    loss           : 6738.390275037329\n",
      "    val_loss       : 6750.844320406719\n",
      "Train Epoch: 462 [256/225000 (0%)] Loss: 6765.142578\n",
      "Train Epoch: 462 [4352/225000 (2%)] Loss: 6728.275391\n",
      "Train Epoch: 462 [8448/225000 (4%)] Loss: 6656.419922\n",
      "Train Epoch: 462 [12544/225000 (6%)] Loss: 6755.080078\n",
      "Train Epoch: 462 [16640/225000 (7%)] Loss: 6570.882812\n",
      "Train Epoch: 462 [20736/225000 (9%)] Loss: 13006.066406\n",
      "Train Epoch: 462 [24832/225000 (11%)] Loss: 6639.705078\n",
      "Train Epoch: 462 [28928/225000 (13%)] Loss: 6589.525391\n",
      "Train Epoch: 462 [33024/225000 (15%)] Loss: 6552.816406\n",
      "Train Epoch: 462 [37120/225000 (16%)] Loss: 6759.681641\n",
      "Train Epoch: 462 [41216/225000 (18%)] Loss: 6899.835938\n",
      "Train Epoch: 462 [45312/225000 (20%)] Loss: 6682.349609\n",
      "Train Epoch: 462 [49408/225000 (22%)] Loss: 6665.943359\n",
      "Train Epoch: 462 [53504/225000 (24%)] Loss: 6776.970703\n",
      "Train Epoch: 462 [57600/225000 (26%)] Loss: 6669.943359\n",
      "Train Epoch: 462 [61696/225000 (27%)] Loss: 6768.722656\n",
      "Train Epoch: 462 [65792/225000 (29%)] Loss: 6648.035156\n",
      "Train Epoch: 462 [69888/225000 (31%)] Loss: 6701.867188\n",
      "Train Epoch: 462 [73984/225000 (33%)] Loss: 6735.189453\n",
      "Train Epoch: 462 [78080/225000 (35%)] Loss: 6804.132812\n",
      "Train Epoch: 462 [82176/225000 (37%)] Loss: 6687.613281\n",
      "Train Epoch: 462 [86272/225000 (38%)] Loss: 6820.068359\n",
      "Train Epoch: 462 [90368/225000 (40%)] Loss: 6758.503906\n",
      "Train Epoch: 462 [94464/225000 (42%)] Loss: 6665.937500\n",
      "Train Epoch: 462 [98560/225000 (44%)] Loss: 6574.361328\n",
      "Train Epoch: 462 [102656/225000 (46%)] Loss: 6737.853516\n",
      "Train Epoch: 462 [106752/225000 (47%)] Loss: 6651.904297\n",
      "Train Epoch: 462 [110848/225000 (49%)] Loss: 6625.812500\n",
      "Train Epoch: 462 [114944/225000 (51%)] Loss: 6650.486328\n",
      "Train Epoch: 462 [119040/225000 (53%)] Loss: 6734.636719\n",
      "Train Epoch: 462 [123136/225000 (55%)] Loss: 6578.193359\n",
      "Train Epoch: 462 [127232/225000 (57%)] Loss: 6776.146484\n",
      "Train Epoch: 462 [131328/225000 (58%)] Loss: 6768.419922\n",
      "Train Epoch: 462 [135424/225000 (60%)] Loss: 6720.888672\n",
      "Train Epoch: 462 [139520/225000 (62%)] Loss: 6749.281250\n",
      "Train Epoch: 462 [143616/225000 (64%)] Loss: 6623.679688\n",
      "Train Epoch: 462 [147712/225000 (66%)] Loss: 6636.451172\n",
      "Train Epoch: 462 [151808/225000 (67%)] Loss: 6776.052734\n",
      "Train Epoch: 462 [155904/225000 (69%)] Loss: 6749.763672\n",
      "Train Epoch: 462 [160000/225000 (71%)] Loss: 6886.416016\n",
      "Train Epoch: 462 [164096/225000 (73%)] Loss: 6759.376953\n",
      "Train Epoch: 462 [168192/225000 (75%)] Loss: 6856.761719\n",
      "Train Epoch: 462 [172288/225000 (77%)] Loss: 6794.162109\n",
      "Train Epoch: 462 [176384/225000 (78%)] Loss: 6620.351562\n",
      "Train Epoch: 462 [180480/225000 (80%)] Loss: 6737.402344\n",
      "Train Epoch: 462 [184576/225000 (82%)] Loss: 6807.679688\n",
      "Train Epoch: 462 [188672/225000 (84%)] Loss: 6595.968750\n",
      "Train Epoch: 462 [192768/225000 (86%)] Loss: 6778.234375\n",
      "Train Epoch: 462 [196864/225000 (87%)] Loss: 6724.570312\n",
      "Train Epoch: 462 [200960/225000 (89%)] Loss: 6681.199219\n",
      "Train Epoch: 462 [205056/225000 (91%)] Loss: 6643.115234\n",
      "Train Epoch: 462 [209152/225000 (93%)] Loss: 6709.876953\n",
      "Train Epoch: 462 [213248/225000 (95%)] Loss: 6704.744141\n",
      "Train Epoch: 462 [217344/225000 (97%)] Loss: 6747.041016\n",
      "Train Epoch: 462 [221440/225000 (98%)] Loss: 6766.462891\n",
      "    epoch          : 462\n",
      "    loss           : 6750.788898073094\n",
      "    val_loss       : 6747.76492813412\n",
      "Train Epoch: 463 [256/225000 (0%)] Loss: 6611.453125\n",
      "Train Epoch: 463 [4352/225000 (2%)] Loss: 6636.109375\n",
      "Train Epoch: 463 [8448/225000 (4%)] Loss: 6682.996094\n",
      "Train Epoch: 463 [12544/225000 (6%)] Loss: 6711.195312\n",
      "Train Epoch: 463 [16640/225000 (7%)] Loss: 6890.162109\n",
      "Train Epoch: 463 [20736/225000 (9%)] Loss: 6612.283203\n",
      "Train Epoch: 463 [24832/225000 (11%)] Loss: 6554.371094\n",
      "Train Epoch: 463 [28928/225000 (13%)] Loss: 6669.238281\n",
      "Train Epoch: 463 [33024/225000 (15%)] Loss: 6705.154297\n",
      "Train Epoch: 463 [37120/225000 (16%)] Loss: 6759.048828\n",
      "Train Epoch: 463 [41216/225000 (18%)] Loss: 6482.884766\n",
      "Train Epoch: 463 [45312/225000 (20%)] Loss: 6682.265625\n",
      "Train Epoch: 463 [49408/225000 (22%)] Loss: 6700.281250\n",
      "Train Epoch: 463 [53504/225000 (24%)] Loss: 6791.482422\n",
      "Train Epoch: 463 [57600/225000 (26%)] Loss: 6840.087891\n",
      "Train Epoch: 463 [61696/225000 (27%)] Loss: 6776.595703\n",
      "Train Epoch: 463 [65792/225000 (29%)] Loss: 6694.158203\n",
      "Train Epoch: 463 [69888/225000 (31%)] Loss: 6666.609375\n",
      "Train Epoch: 463 [73984/225000 (33%)] Loss: 6619.453125\n",
      "Train Epoch: 463 [78080/225000 (35%)] Loss: 6682.833984\n",
      "Train Epoch: 463 [82176/225000 (37%)] Loss: 6645.431641\n",
      "Train Epoch: 463 [86272/225000 (38%)] Loss: 6870.958984\n",
      "Train Epoch: 463 [90368/225000 (40%)] Loss: 6940.302734\n",
      "Train Epoch: 463 [94464/225000 (42%)] Loss: 6707.089844\n",
      "Train Epoch: 463 [98560/225000 (44%)] Loss: 6695.208984\n",
      "Train Epoch: 463 [102656/225000 (46%)] Loss: 6806.003906\n",
      "Train Epoch: 463 [106752/225000 (47%)] Loss: 6633.357422\n",
      "Train Epoch: 463 [110848/225000 (49%)] Loss: 6581.330078\n",
      "Train Epoch: 463 [114944/225000 (51%)] Loss: 6787.748047\n",
      "Train Epoch: 463 [119040/225000 (53%)] Loss: 6646.214844\n",
      "Train Epoch: 463 [123136/225000 (55%)] Loss: 6677.226562\n",
      "Train Epoch: 463 [127232/225000 (57%)] Loss: 6900.025391\n",
      "Train Epoch: 463 [131328/225000 (58%)] Loss: 6834.990234\n",
      "Train Epoch: 463 [135424/225000 (60%)] Loss: 6674.195312\n",
      "Train Epoch: 463 [139520/225000 (62%)] Loss: 6614.970703\n",
      "Train Epoch: 463 [143616/225000 (64%)] Loss: 6782.839844\n",
      "Train Epoch: 463 [147712/225000 (66%)] Loss: 6759.880859\n",
      "Train Epoch: 463 [151808/225000 (67%)] Loss: 6692.576172\n",
      "Train Epoch: 463 [155904/225000 (69%)] Loss: 6810.656250\n",
      "Train Epoch: 463 [160000/225000 (71%)] Loss: 6643.066406\n",
      "Train Epoch: 463 [164096/225000 (73%)] Loss: 6595.806641\n",
      "Train Epoch: 463 [168192/225000 (75%)] Loss: 6717.054688\n",
      "Train Epoch: 463 [172288/225000 (77%)] Loss: 6741.355469\n",
      "Train Epoch: 463 [176384/225000 (78%)] Loss: 6721.941406\n",
      "Train Epoch: 463 [180480/225000 (80%)] Loss: 6656.046875\n",
      "Train Epoch: 463 [184576/225000 (82%)] Loss: 6659.708984\n",
      "Train Epoch: 463 [188672/225000 (84%)] Loss: 6606.193359\n",
      "Train Epoch: 463 [192768/225000 (86%)] Loss: 6690.166016\n",
      "Train Epoch: 463 [196864/225000 (87%)] Loss: 6631.269531\n",
      "Train Epoch: 463 [200960/225000 (89%)] Loss: 6869.435547\n",
      "Train Epoch: 463 [205056/225000 (91%)] Loss: 6706.441406\n",
      "Train Epoch: 463 [209152/225000 (93%)] Loss: 6742.062500\n",
      "Train Epoch: 463 [213248/225000 (95%)] Loss: 6588.990234\n",
      "Train Epoch: 463 [217344/225000 (97%)] Loss: 6722.296875\n",
      "Train Epoch: 463 [221440/225000 (98%)] Loss: 6648.679688\n",
      "    epoch          : 463\n",
      "    loss           : 6732.025521722127\n",
      "    val_loss       : 6745.431105282842\n",
      "Train Epoch: 464 [256/225000 (0%)] Loss: 6670.302734\n",
      "Train Epoch: 464 [4352/225000 (2%)] Loss: 6658.292969\n",
      "Train Epoch: 464 [8448/225000 (4%)] Loss: 6672.822266\n",
      "Train Epoch: 464 [12544/225000 (6%)] Loss: 6854.238281\n",
      "Train Epoch: 464 [16640/225000 (7%)] Loss: 6612.367188\n",
      "Train Epoch: 464 [20736/225000 (9%)] Loss: 6729.693359\n",
      "Train Epoch: 464 [24832/225000 (11%)] Loss: 6634.363281\n",
      "Train Epoch: 464 [28928/225000 (13%)] Loss: 6813.539062\n",
      "Train Epoch: 464 [33024/225000 (15%)] Loss: 6544.609375\n",
      "Train Epoch: 464 [37120/225000 (16%)] Loss: 6744.371094\n",
      "Train Epoch: 464 [41216/225000 (18%)] Loss: 6864.857422\n",
      "Train Epoch: 464 [45312/225000 (20%)] Loss: 6634.310547\n",
      "Train Epoch: 464 [49408/225000 (22%)] Loss: 6644.332031\n",
      "Train Epoch: 464 [53504/225000 (24%)] Loss: 6687.695312\n",
      "Train Epoch: 464 [57600/225000 (26%)] Loss: 6685.289062\n",
      "Train Epoch: 464 [61696/225000 (27%)] Loss: 6825.203125\n",
      "Train Epoch: 464 [65792/225000 (29%)] Loss: 6797.462891\n",
      "Train Epoch: 464 [69888/225000 (31%)] Loss: 6731.685547\n",
      "Train Epoch: 464 [73984/225000 (33%)] Loss: 6685.158203\n",
      "Train Epoch: 464 [78080/225000 (35%)] Loss: 6739.048828\n",
      "Train Epoch: 464 [82176/225000 (37%)] Loss: 6666.576172\n",
      "Train Epoch: 464 [86272/225000 (38%)] Loss: 6978.962891\n",
      "Train Epoch: 464 [90368/225000 (40%)] Loss: 6673.066406\n",
      "Train Epoch: 464 [94464/225000 (42%)] Loss: 6747.416016\n",
      "Train Epoch: 464 [98560/225000 (44%)] Loss: 6580.212891\n",
      "Train Epoch: 464 [102656/225000 (46%)] Loss: 6703.267578\n",
      "Train Epoch: 464 [106752/225000 (47%)] Loss: 6763.433594\n",
      "Train Epoch: 464 [110848/225000 (49%)] Loss: 6878.986328\n",
      "Train Epoch: 464 [114944/225000 (51%)] Loss: 6716.111328\n",
      "Train Epoch: 464 [119040/225000 (53%)] Loss: 6731.660156\n",
      "Train Epoch: 464 [123136/225000 (55%)] Loss: 6665.677734\n",
      "Train Epoch: 464 [127232/225000 (57%)] Loss: 6680.812500\n",
      "Train Epoch: 464 [131328/225000 (58%)] Loss: 6684.500000\n",
      "Train Epoch: 464 [135424/225000 (60%)] Loss: 6581.447266\n",
      "Train Epoch: 464 [139520/225000 (62%)] Loss: 6647.332031\n",
      "Train Epoch: 464 [143616/225000 (64%)] Loss: 6804.632812\n",
      "Train Epoch: 464 [147712/225000 (66%)] Loss: 6506.136719\n",
      "Train Epoch: 464 [151808/225000 (67%)] Loss: 6671.757812\n",
      "Train Epoch: 464 [155904/225000 (69%)] Loss: 6569.242188\n",
      "Train Epoch: 464 [160000/225000 (71%)] Loss: 6700.761719\n",
      "Train Epoch: 464 [164096/225000 (73%)] Loss: 6763.179688\n",
      "Train Epoch: 464 [168192/225000 (75%)] Loss: 6824.345703\n",
      "Train Epoch: 464 [172288/225000 (77%)] Loss: 6724.173828\n",
      "Train Epoch: 464 [176384/225000 (78%)] Loss: 6779.130859\n",
      "Train Epoch: 464 [180480/225000 (80%)] Loss: 6901.210938\n",
      "Train Epoch: 464 [184576/225000 (82%)] Loss: 6819.244141\n",
      "Train Epoch: 464 [188672/225000 (84%)] Loss: 6650.869141\n",
      "Train Epoch: 464 [192768/225000 (86%)] Loss: 6690.910156\n",
      "Train Epoch: 464 [196864/225000 (87%)] Loss: 6746.843750\n",
      "Train Epoch: 464 [200960/225000 (89%)] Loss: 6766.804688\n",
      "Train Epoch: 464 [205056/225000 (91%)] Loss: 6635.486328\n",
      "Train Epoch: 464 [209152/225000 (93%)] Loss: 6708.718750\n",
      "Train Epoch: 464 [213248/225000 (95%)] Loss: 6568.240234\n",
      "Train Epoch: 464 [217344/225000 (97%)] Loss: 6609.636719\n",
      "Train Epoch: 464 [221440/225000 (98%)] Loss: 6682.292969\n",
      "    epoch          : 464\n",
      "    loss           : 6731.956357988482\n",
      "    val_loss       : 6749.05492191412\n",
      "Train Epoch: 465 [256/225000 (0%)] Loss: 6731.255859\n",
      "Train Epoch: 465 [4352/225000 (2%)] Loss: 6621.351562\n",
      "Train Epoch: 465 [8448/225000 (4%)] Loss: 6601.511719\n",
      "Train Epoch: 465 [12544/225000 (6%)] Loss: 6742.974609\n",
      "Train Epoch: 465 [16640/225000 (7%)] Loss: 6741.724609\n",
      "Train Epoch: 465 [20736/225000 (9%)] Loss: 6657.015625\n",
      "Train Epoch: 465 [24832/225000 (11%)] Loss: 6630.687500\n",
      "Train Epoch: 465 [28928/225000 (13%)] Loss: 6815.294922\n",
      "Train Epoch: 465 [33024/225000 (15%)] Loss: 6598.382812\n",
      "Train Epoch: 465 [37120/225000 (16%)] Loss: 6693.595703\n",
      "Train Epoch: 465 [41216/225000 (18%)] Loss: 6709.736328\n",
      "Train Epoch: 465 [45312/225000 (20%)] Loss: 6795.675781\n",
      "Train Epoch: 465 [49408/225000 (22%)] Loss: 6704.345703\n",
      "Train Epoch: 465 [53504/225000 (24%)] Loss: 6689.824219\n",
      "Train Epoch: 465 [57600/225000 (26%)] Loss: 6797.255859\n",
      "Train Epoch: 465 [61696/225000 (27%)] Loss: 6834.572266\n",
      "Train Epoch: 465 [65792/225000 (29%)] Loss: 6845.623047\n",
      "Train Epoch: 465 [69888/225000 (31%)] Loss: 6797.796875\n",
      "Train Epoch: 465 [73984/225000 (33%)] Loss: 6736.160156\n",
      "Train Epoch: 465 [78080/225000 (35%)] Loss: 6708.435547\n",
      "Train Epoch: 465 [82176/225000 (37%)] Loss: 6829.607422\n",
      "Train Epoch: 465 [86272/225000 (38%)] Loss: 6667.652344\n",
      "Train Epoch: 465 [90368/225000 (40%)] Loss: 6747.824219\n",
      "Train Epoch: 465 [94464/225000 (42%)] Loss: 6700.970703\n",
      "Train Epoch: 465 [98560/225000 (44%)] Loss: 6516.294922\n",
      "Train Epoch: 465 [102656/225000 (46%)] Loss: 6667.761719\n",
      "Train Epoch: 465 [106752/225000 (47%)] Loss: 6669.345703\n",
      "Train Epoch: 465 [110848/225000 (49%)] Loss: 6578.458984\n",
      "Train Epoch: 465 [114944/225000 (51%)] Loss: 6771.511719\n",
      "Train Epoch: 465 [119040/225000 (53%)] Loss: 6832.232422\n",
      "Train Epoch: 465 [123136/225000 (55%)] Loss: 6633.326172\n",
      "Train Epoch: 465 [127232/225000 (57%)] Loss: 6601.238281\n",
      "Train Epoch: 465 [131328/225000 (58%)] Loss: 6900.781250\n",
      "Train Epoch: 465 [135424/225000 (60%)] Loss: 6905.210938\n",
      "Train Epoch: 465 [139520/225000 (62%)] Loss: 6648.742188\n",
      "Train Epoch: 465 [143616/225000 (64%)] Loss: 6730.224609\n",
      "Train Epoch: 465 [147712/225000 (66%)] Loss: 6717.960938\n",
      "Train Epoch: 465 [151808/225000 (67%)] Loss: 6824.064453\n",
      "Train Epoch: 465 [155904/225000 (69%)] Loss: 6660.904297\n",
      "Train Epoch: 465 [160000/225000 (71%)] Loss: 6620.318359\n",
      "Train Epoch: 465 [164096/225000 (73%)] Loss: 6592.619141\n",
      "Train Epoch: 465 [168192/225000 (75%)] Loss: 6709.904297\n",
      "Train Epoch: 465 [172288/225000 (77%)] Loss: 6632.472656\n",
      "Train Epoch: 465 [176384/225000 (78%)] Loss: 6678.246094\n",
      "Train Epoch: 465 [180480/225000 (80%)] Loss: 6722.656250\n",
      "Train Epoch: 465 [184576/225000 (82%)] Loss: 6558.603516\n",
      "Train Epoch: 465 [188672/225000 (84%)] Loss: 6702.365234\n",
      "Train Epoch: 465 [192768/225000 (86%)] Loss: 6612.507812\n",
      "Train Epoch: 465 [196864/225000 (87%)] Loss: 6720.185547\n",
      "Train Epoch: 465 [200960/225000 (89%)] Loss: 6728.470703\n",
      "Train Epoch: 465 [205056/225000 (91%)] Loss: 6771.558594\n",
      "Train Epoch: 465 [209152/225000 (93%)] Loss: 6814.294922\n",
      "Train Epoch: 465 [213248/225000 (95%)] Loss: 6871.171875\n",
      "Train Epoch: 465 [217344/225000 (97%)] Loss: 6639.060547\n",
      "Train Epoch: 465 [221440/225000 (98%)] Loss: 6792.285156\n",
      "    epoch          : 465\n",
      "    loss           : 6747.81733170684\n",
      "    val_loss       : 6744.969484742807\n",
      "Train Epoch: 466 [256/225000 (0%)] Loss: 6689.503906\n",
      "Train Epoch: 466 [4352/225000 (2%)] Loss: 6672.580078\n",
      "Train Epoch: 466 [8448/225000 (4%)] Loss: 6904.845703\n",
      "Train Epoch: 466 [12544/225000 (6%)] Loss: 6802.023438\n",
      "Train Epoch: 466 [16640/225000 (7%)] Loss: 6803.052734\n",
      "Train Epoch: 466 [20736/225000 (9%)] Loss: 6792.232422\n",
      "Train Epoch: 466 [24832/225000 (11%)] Loss: 6707.880859\n",
      "Train Epoch: 466 [28928/225000 (13%)] Loss: 6656.978516\n",
      "Train Epoch: 466 [33024/225000 (15%)] Loss: 6585.837891\n",
      "Train Epoch: 466 [37120/225000 (16%)] Loss: 6733.115234\n",
      "Train Epoch: 466 [41216/225000 (18%)] Loss: 6659.210938\n",
      "Train Epoch: 466 [45312/225000 (20%)] Loss: 6723.822266\n",
      "Train Epoch: 466 [49408/225000 (22%)] Loss: 6726.058594\n",
      "Train Epoch: 466 [53504/225000 (24%)] Loss: 6699.417969\n",
      "Train Epoch: 466 [57600/225000 (26%)] Loss: 6681.464844\n",
      "Train Epoch: 466 [61696/225000 (27%)] Loss: 6661.552734\n",
      "Train Epoch: 466 [65792/225000 (29%)] Loss: 6608.351562\n",
      "Train Epoch: 466 [69888/225000 (31%)] Loss: 6562.240234\n",
      "Train Epoch: 466 [73984/225000 (33%)] Loss: 6565.734375\n",
      "Train Epoch: 466 [78080/225000 (35%)] Loss: 6719.781250\n",
      "Train Epoch: 466 [82176/225000 (37%)] Loss: 6844.113281\n",
      "Train Epoch: 466 [86272/225000 (38%)] Loss: 6660.414062\n",
      "Train Epoch: 466 [90368/225000 (40%)] Loss: 6708.152344\n",
      "Train Epoch: 466 [94464/225000 (42%)] Loss: 6712.410156\n",
      "Train Epoch: 466 [98560/225000 (44%)] Loss: 6738.605469\n",
      "Train Epoch: 466 [102656/225000 (46%)] Loss: 6722.751953\n",
      "Train Epoch: 466 [106752/225000 (47%)] Loss: 6739.746094\n",
      "Train Epoch: 466 [110848/225000 (49%)] Loss: 6744.203125\n",
      "Train Epoch: 466 [114944/225000 (51%)] Loss: 6768.912109\n",
      "Train Epoch: 466 [119040/225000 (53%)] Loss: 6685.789062\n",
      "Train Epoch: 466 [123136/225000 (55%)] Loss: 6787.070312\n",
      "Train Epoch: 466 [127232/225000 (57%)] Loss: 6501.230469\n",
      "Train Epoch: 466 [131328/225000 (58%)] Loss: 6651.044922\n",
      "Train Epoch: 466 [135424/225000 (60%)] Loss: 6715.125000\n",
      "Train Epoch: 466 [139520/225000 (62%)] Loss: 6673.535156\n",
      "Train Epoch: 466 [143616/225000 (64%)] Loss: 6686.261719\n",
      "Train Epoch: 466 [147712/225000 (66%)] Loss: 6817.099609\n",
      "Train Epoch: 466 [151808/225000 (67%)] Loss: 6707.707031\n",
      "Train Epoch: 466 [155904/225000 (69%)] Loss: 6679.265625\n",
      "Train Epoch: 466 [160000/225000 (71%)] Loss: 6589.843750\n",
      "Train Epoch: 466 [164096/225000 (73%)] Loss: 6894.681641\n",
      "Train Epoch: 466 [168192/225000 (75%)] Loss: 6857.894531\n",
      "Train Epoch: 466 [172288/225000 (77%)] Loss: 6787.546875\n",
      "Train Epoch: 466 [176384/225000 (78%)] Loss: 6689.279297\n",
      "Train Epoch: 466 [180480/225000 (80%)] Loss: 6734.580078\n",
      "Train Epoch: 466 [184576/225000 (82%)] Loss: 6679.490234\n",
      "Train Epoch: 466 [188672/225000 (84%)] Loss: 6666.384766\n",
      "Train Epoch: 466 [192768/225000 (86%)] Loss: 6984.541016\n",
      "Train Epoch: 466 [196864/225000 (87%)] Loss: 6867.947266\n",
      "Train Epoch: 466 [200960/225000 (89%)] Loss: 6795.693359\n",
      "Train Epoch: 466 [205056/225000 (91%)] Loss: 6692.324219\n",
      "Train Epoch: 466 [209152/225000 (93%)] Loss: 6760.578125\n",
      "Train Epoch: 466 [213248/225000 (95%)] Loss: 6701.832031\n",
      "Train Epoch: 466 [217344/225000 (97%)] Loss: 6721.333984\n",
      "Train Epoch: 466 [221440/225000 (98%)] Loss: 6719.503906\n",
      "    epoch          : 466\n",
      "    loss           : 6777.799309184798\n",
      "    val_loss       : 6743.759843651129\n",
      "Train Epoch: 467 [256/225000 (0%)] Loss: 6689.726562\n",
      "Train Epoch: 467 [4352/225000 (2%)] Loss: 6673.955078\n",
      "Train Epoch: 467 [8448/225000 (4%)] Loss: 6755.179688\n",
      "Train Epoch: 467 [12544/225000 (6%)] Loss: 6811.990234\n",
      "Train Epoch: 467 [16640/225000 (7%)] Loss: 6909.035156\n",
      "Train Epoch: 467 [20736/225000 (9%)] Loss: 6876.640625\n",
      "Train Epoch: 467 [24832/225000 (11%)] Loss: 6909.550781\n",
      "Train Epoch: 467 [28928/225000 (13%)] Loss: 6796.921875\n",
      "Train Epoch: 467 [33024/225000 (15%)] Loss: 6717.925781\n",
      "Train Epoch: 467 [37120/225000 (16%)] Loss: 6613.523438\n",
      "Train Epoch: 467 [41216/225000 (18%)] Loss: 6663.394531\n",
      "Train Epoch: 467 [45312/225000 (20%)] Loss: 6639.792969\n",
      "Train Epoch: 467 [49408/225000 (22%)] Loss: 6741.824219\n",
      "Train Epoch: 467 [53504/225000 (24%)] Loss: 6814.480469\n",
      "Train Epoch: 467 [57600/225000 (26%)] Loss: 6764.980469\n",
      "Train Epoch: 467 [61696/225000 (27%)] Loss: 6651.400391\n",
      "Train Epoch: 467 [65792/225000 (29%)] Loss: 6726.207031\n",
      "Train Epoch: 467 [69888/225000 (31%)] Loss: 6768.611328\n",
      "Train Epoch: 467 [73984/225000 (33%)] Loss: 6849.226562\n",
      "Train Epoch: 467 [78080/225000 (35%)] Loss: 6579.095703\n",
      "Train Epoch: 467 [82176/225000 (37%)] Loss: 6794.941406\n",
      "Train Epoch: 467 [86272/225000 (38%)] Loss: 6609.023438\n",
      "Train Epoch: 467 [90368/225000 (40%)] Loss: 6770.714844\n",
      "Train Epoch: 467 [94464/225000 (42%)] Loss: 6720.626953\n",
      "Train Epoch: 467 [98560/225000 (44%)] Loss: 6754.500000\n",
      "Train Epoch: 467 [102656/225000 (46%)] Loss: 6762.906250\n",
      "Train Epoch: 467 [106752/225000 (47%)] Loss: 6770.460938\n",
      "Train Epoch: 467 [110848/225000 (49%)] Loss: 6754.277344\n",
      "Train Epoch: 467 [114944/225000 (51%)] Loss: 6808.210938\n",
      "Train Epoch: 467 [119040/225000 (53%)] Loss: 6653.003906\n",
      "Train Epoch: 467 [123136/225000 (55%)] Loss: 6539.308594\n",
      "Train Epoch: 467 [127232/225000 (57%)] Loss: 6689.328125\n",
      "Train Epoch: 467 [131328/225000 (58%)] Loss: 6847.589844\n",
      "Train Epoch: 467 [135424/225000 (60%)] Loss: 6799.388672\n",
      "Train Epoch: 467 [139520/225000 (62%)] Loss: 6789.505859\n",
      "Train Epoch: 467 [143616/225000 (64%)] Loss: 6701.023438\n",
      "Train Epoch: 467 [147712/225000 (66%)] Loss: 6612.853516\n",
      "Train Epoch: 467 [151808/225000 (67%)] Loss: 6717.839844\n",
      "Train Epoch: 467 [155904/225000 (69%)] Loss: 6618.937500\n",
      "Train Epoch: 467 [160000/225000 (71%)] Loss: 6665.259766\n",
      "Train Epoch: 467 [164096/225000 (73%)] Loss: 6631.912109\n",
      "Train Epoch: 467 [168192/225000 (75%)] Loss: 6923.166016\n",
      "Train Epoch: 467 [172288/225000 (77%)] Loss: 6711.037109\n",
      "Train Epoch: 467 [176384/225000 (78%)] Loss: 6827.029297\n",
      "Train Epoch: 467 [180480/225000 (80%)] Loss: 6659.449219\n",
      "Train Epoch: 467 [184576/225000 (82%)] Loss: 6701.837891\n",
      "Train Epoch: 467 [188672/225000 (84%)] Loss: 6644.769531\n",
      "Train Epoch: 467 [192768/225000 (86%)] Loss: 6661.814453\n",
      "Train Epoch: 467 [196864/225000 (87%)] Loss: 6704.146484\n",
      "Train Epoch: 467 [200960/225000 (89%)] Loss: 6724.291016\n",
      "Train Epoch: 467 [205056/225000 (91%)] Loss: 6624.283203\n",
      "Train Epoch: 467 [209152/225000 (93%)] Loss: 6785.837891\n",
      "Train Epoch: 467 [213248/225000 (95%)] Loss: 6731.300781\n",
      "Train Epoch: 467 [217344/225000 (97%)] Loss: 6772.505859\n",
      "Train Epoch: 467 [221440/225000 (98%)] Loss: 6782.431641\n",
      "    epoch          : 467\n",
      "    loss           : 6748.5839443792665\n",
      "    val_loss       : 6745.009772125555\n",
      "Train Epoch: 468 [256/225000 (0%)] Loss: 6641.392578\n",
      "Train Epoch: 468 [4352/225000 (2%)] Loss: 6780.535156\n",
      "Train Epoch: 468 [8448/225000 (4%)] Loss: 6788.101562\n",
      "Train Epoch: 468 [12544/225000 (6%)] Loss: 6717.857422\n",
      "Train Epoch: 468 [16640/225000 (7%)] Loss: 6758.607422\n",
      "Train Epoch: 468 [20736/225000 (9%)] Loss: 6702.394531\n",
      "Train Epoch: 468 [24832/225000 (11%)] Loss: 6781.324219\n",
      "Train Epoch: 468 [28928/225000 (13%)] Loss: 6888.699219\n",
      "Train Epoch: 468 [33024/225000 (15%)] Loss: 6878.117188\n",
      "Train Epoch: 468 [37120/225000 (16%)] Loss: 6812.357422\n",
      "Train Epoch: 468 [41216/225000 (18%)] Loss: 6772.861328\n",
      "Train Epoch: 468 [45312/225000 (20%)] Loss: 6820.746094\n",
      "Train Epoch: 468 [49408/225000 (22%)] Loss: 6854.521484\n",
      "Train Epoch: 468 [53504/225000 (24%)] Loss: 6729.718750\n",
      "Train Epoch: 468 [57600/225000 (26%)] Loss: 6641.292969\n",
      "Train Epoch: 468 [61696/225000 (27%)] Loss: 6699.720703\n",
      "Train Epoch: 468 [65792/225000 (29%)] Loss: 6682.091797\n",
      "Train Epoch: 468 [69888/225000 (31%)] Loss: 6829.042969\n",
      "Train Epoch: 468 [73984/225000 (33%)] Loss: 6835.873047\n",
      "Train Epoch: 468 [78080/225000 (35%)] Loss: 6677.636719\n",
      "Train Epoch: 468 [82176/225000 (37%)] Loss: 6799.371094\n",
      "Train Epoch: 468 [86272/225000 (38%)] Loss: 6704.642578\n",
      "Train Epoch: 468 [90368/225000 (40%)] Loss: 6776.445312\n",
      "Train Epoch: 468 [94464/225000 (42%)] Loss: 6646.068359\n",
      "Train Epoch: 468 [98560/225000 (44%)] Loss: 6861.501953\n",
      "Train Epoch: 468 [102656/225000 (46%)] Loss: 6830.117188\n",
      "Train Epoch: 468 [106752/225000 (47%)] Loss: 6622.439453\n",
      "Train Epoch: 468 [110848/225000 (49%)] Loss: 6685.439453\n",
      "Train Epoch: 468 [114944/225000 (51%)] Loss: 6639.505859\n",
      "Train Epoch: 468 [119040/225000 (53%)] Loss: 6741.562500\n",
      "Train Epoch: 468 [123136/225000 (55%)] Loss: 6719.818359\n",
      "Train Epoch: 468 [127232/225000 (57%)] Loss: 6811.859375\n",
      "Train Epoch: 468 [131328/225000 (58%)] Loss: 6872.783203\n",
      "Train Epoch: 468 [135424/225000 (60%)] Loss: 6731.082031\n",
      "Train Epoch: 468 [139520/225000 (62%)] Loss: 6798.208984\n",
      "Train Epoch: 468 [143616/225000 (64%)] Loss: 6793.552734\n",
      "Train Epoch: 468 [147712/225000 (66%)] Loss: 6726.779297\n",
      "Train Epoch: 468 [151808/225000 (67%)] Loss: 12006.169922\n",
      "Train Epoch: 468 [155904/225000 (69%)] Loss: 6887.685547\n",
      "Train Epoch: 468 [160000/225000 (71%)] Loss: 6717.798828\n",
      "Train Epoch: 468 [164096/225000 (73%)] Loss: 6847.314453\n",
      "Train Epoch: 468 [168192/225000 (75%)] Loss: 6723.357422\n",
      "Train Epoch: 468 [172288/225000 (77%)] Loss: 6756.011719\n",
      "Train Epoch: 468 [176384/225000 (78%)] Loss: 6641.396484\n",
      "Train Epoch: 468 [180480/225000 (80%)] Loss: 6764.917969\n",
      "Train Epoch: 468 [184576/225000 (82%)] Loss: 6620.857422\n",
      "Train Epoch: 468 [188672/225000 (84%)] Loss: 6833.253906\n",
      "Train Epoch: 468 [192768/225000 (86%)] Loss: 6662.943359\n",
      "Train Epoch: 468 [196864/225000 (87%)] Loss: 6831.648438\n",
      "Train Epoch: 468 [200960/225000 (89%)] Loss: 6855.099609\n",
      "Train Epoch: 468 [205056/225000 (91%)] Loss: 6598.341797\n",
      "Train Epoch: 468 [209152/225000 (93%)] Loss: 6755.720703\n",
      "Train Epoch: 468 [213248/225000 (95%)] Loss: 6761.324219\n",
      "Train Epoch: 468 [217344/225000 (97%)] Loss: 6724.421875\n",
      "Train Epoch: 468 [221440/225000 (98%)] Loss: 6771.529297\n",
      "    epoch          : 468\n",
      "    loss           : 6749.886764300697\n",
      "    val_loss       : 6899.769651803435\n",
      "Train Epoch: 469 [256/225000 (0%)] Loss: 6509.923828\n",
      "Train Epoch: 469 [4352/225000 (2%)] Loss: 6664.117188\n",
      "Train Epoch: 469 [8448/225000 (4%)] Loss: 6552.875000\n",
      "Train Epoch: 469 [12544/225000 (6%)] Loss: 6785.363281\n",
      "Train Epoch: 469 [16640/225000 (7%)] Loss: 6905.099609\n",
      "Train Epoch: 469 [20736/225000 (9%)] Loss: 6581.375000\n",
      "Train Epoch: 469 [24832/225000 (11%)] Loss: 6642.656250\n",
      "Train Epoch: 469 [28928/225000 (13%)] Loss: 6697.582031\n",
      "Train Epoch: 469 [33024/225000 (15%)] Loss: 6766.509766\n",
      "Train Epoch: 469 [37120/225000 (16%)] Loss: 6721.082031\n",
      "Train Epoch: 469 [41216/225000 (18%)] Loss: 6743.705078\n",
      "Train Epoch: 469 [45312/225000 (20%)] Loss: 6617.185547\n",
      "Train Epoch: 469 [49408/225000 (22%)] Loss: 6793.189453\n",
      "Train Epoch: 469 [53504/225000 (24%)] Loss: 6685.455078\n",
      "Train Epoch: 469 [57600/225000 (26%)] Loss: 7005.224609\n",
      "Train Epoch: 469 [61696/225000 (27%)] Loss: 6685.570312\n",
      "Train Epoch: 469 [65792/225000 (29%)] Loss: 6677.476562\n",
      "Train Epoch: 469 [69888/225000 (31%)] Loss: 6850.501953\n",
      "Train Epoch: 469 [73984/225000 (33%)] Loss: 6735.300781\n",
      "Train Epoch: 469 [78080/225000 (35%)] Loss: 6803.648438\n",
      "Train Epoch: 469 [82176/225000 (37%)] Loss: 6797.453125\n",
      "Train Epoch: 469 [86272/225000 (38%)] Loss: 6680.201172\n",
      "Train Epoch: 469 [90368/225000 (40%)] Loss: 6637.542969\n",
      "Train Epoch: 469 [94464/225000 (42%)] Loss: 6650.085938\n",
      "Train Epoch: 469 [98560/225000 (44%)] Loss: 6837.324219\n",
      "Train Epoch: 469 [102656/225000 (46%)] Loss: 6684.988281\n",
      "Train Epoch: 469 [106752/225000 (47%)] Loss: 6848.443359\n",
      "Train Epoch: 469 [110848/225000 (49%)] Loss: 6642.689453\n",
      "Train Epoch: 469 [114944/225000 (51%)] Loss: 6709.812500\n",
      "Train Epoch: 469 [119040/225000 (53%)] Loss: 6609.179688\n",
      "Train Epoch: 469 [123136/225000 (55%)] Loss: 6600.000000\n",
      "Train Epoch: 469 [127232/225000 (57%)] Loss: 6825.277344\n",
      "Train Epoch: 469 [131328/225000 (58%)] Loss: 6812.697266\n",
      "Train Epoch: 469 [135424/225000 (60%)] Loss: 6621.304688\n",
      "Train Epoch: 469 [139520/225000 (62%)] Loss: 6887.916016\n",
      "Train Epoch: 469 [143616/225000 (64%)] Loss: 6855.515625\n",
      "Train Epoch: 469 [147712/225000 (66%)] Loss: 6712.169922\n",
      "Train Epoch: 469 [151808/225000 (67%)] Loss: 6937.728516\n",
      "Train Epoch: 469 [155904/225000 (69%)] Loss: 6503.113281\n",
      "Train Epoch: 469 [160000/225000 (71%)] Loss: 6574.695312\n",
      "Train Epoch: 469 [164096/225000 (73%)] Loss: 6684.574219\n",
      "Train Epoch: 469 [168192/225000 (75%)] Loss: 6825.898438\n",
      "Train Epoch: 469 [172288/225000 (77%)] Loss: 6757.986328\n",
      "Train Epoch: 469 [176384/225000 (78%)] Loss: 6747.458984\n",
      "Train Epoch: 469 [180480/225000 (80%)] Loss: 6767.457031\n",
      "Train Epoch: 469 [184576/225000 (82%)] Loss: 6733.701172\n",
      "Train Epoch: 469 [188672/225000 (84%)] Loss: 6611.781250\n",
      "Train Epoch: 469 [192768/225000 (86%)] Loss: 6759.621094\n",
      "Train Epoch: 469 [196864/225000 (87%)] Loss: 6701.824219\n",
      "Train Epoch: 469 [200960/225000 (89%)] Loss: 6724.888672\n",
      "Train Epoch: 469 [205056/225000 (91%)] Loss: 6704.453125\n",
      "Train Epoch: 469 [209152/225000 (93%)] Loss: 6668.617188\n",
      "Train Epoch: 469 [213248/225000 (95%)] Loss: 6593.035156\n",
      "Train Epoch: 469 [217344/225000 (97%)] Loss: 6860.099609\n",
      "Train Epoch: 469 [221440/225000 (98%)] Loss: 6880.642578\n",
      "    epoch          : 469\n",
      "    loss           : 6718.693904872369\n",
      "    val_loss       : 6744.297328048823\n",
      "Train Epoch: 470 [256/225000 (0%)] Loss: 6785.617188\n",
      "Train Epoch: 470 [4352/225000 (2%)] Loss: 6832.371094\n",
      "Train Epoch: 470 [8448/225000 (4%)] Loss: 6707.964844\n",
      "Train Epoch: 470 [12544/225000 (6%)] Loss: 6672.039062\n",
      "Train Epoch: 470 [16640/225000 (7%)] Loss: 6604.480469\n",
      "Train Epoch: 470 [20736/225000 (9%)] Loss: 6633.871094\n",
      "Train Epoch: 470 [24832/225000 (11%)] Loss: 6901.214844\n",
      "Train Epoch: 470 [28928/225000 (13%)] Loss: 6547.154297\n",
      "Train Epoch: 470 [33024/225000 (15%)] Loss: 6719.023438\n",
      "Train Epoch: 470 [37120/225000 (16%)] Loss: 6713.246094\n",
      "Train Epoch: 470 [41216/225000 (18%)] Loss: 6658.757812\n",
      "Train Epoch: 470 [45312/225000 (20%)] Loss: 6730.136719\n",
      "Train Epoch: 470 [49408/225000 (22%)] Loss: 6624.224609\n",
      "Train Epoch: 470 [53504/225000 (24%)] Loss: 6740.933594\n",
      "Train Epoch: 470 [57600/225000 (26%)] Loss: 6654.570312\n",
      "Train Epoch: 470 [61696/225000 (27%)] Loss: 6653.867188\n",
      "Train Epoch: 470 [65792/225000 (29%)] Loss: 6631.519531\n",
      "Train Epoch: 470 [69888/225000 (31%)] Loss: 6834.824219\n",
      "Train Epoch: 470 [73984/225000 (33%)] Loss: 6600.884766\n",
      "Train Epoch: 470 [78080/225000 (35%)] Loss: 6690.933594\n",
      "Train Epoch: 470 [82176/225000 (37%)] Loss: 6646.894531\n",
      "Train Epoch: 470 [86272/225000 (38%)] Loss: 6870.751953\n",
      "Train Epoch: 470 [90368/225000 (40%)] Loss: 6822.007812\n",
      "Train Epoch: 470 [94464/225000 (42%)] Loss: 6791.373047\n",
      "Train Epoch: 470 [98560/225000 (44%)] Loss: 6666.457031\n",
      "Train Epoch: 470 [102656/225000 (46%)] Loss: 6551.896484\n",
      "Train Epoch: 470 [106752/225000 (47%)] Loss: 6617.210938\n",
      "Train Epoch: 470 [110848/225000 (49%)] Loss: 6666.087891\n",
      "Train Epoch: 470 [114944/225000 (51%)] Loss: 6688.525391\n",
      "Train Epoch: 470 [119040/225000 (53%)] Loss: 6688.503906\n",
      "Train Epoch: 470 [123136/225000 (55%)] Loss: 6692.390625\n",
      "Train Epoch: 470 [127232/225000 (57%)] Loss: 6932.195312\n",
      "Train Epoch: 470 [131328/225000 (58%)] Loss: 6731.917969\n",
      "Train Epoch: 470 [135424/225000 (60%)] Loss: 6714.201172\n",
      "Train Epoch: 470 [139520/225000 (62%)] Loss: 6799.611328\n",
      "Train Epoch: 470 [143616/225000 (64%)] Loss: 6916.660156\n",
      "Train Epoch: 470 [147712/225000 (66%)] Loss: 6682.181641\n",
      "Train Epoch: 470 [151808/225000 (67%)] Loss: 6685.101562\n",
      "Train Epoch: 470 [155904/225000 (69%)] Loss: 6735.152344\n",
      "Train Epoch: 470 [160000/225000 (71%)] Loss: 6686.759766\n",
      "Train Epoch: 470 [164096/225000 (73%)] Loss: 6652.246094\n",
      "Train Epoch: 470 [168192/225000 (75%)] Loss: 6632.628906\n",
      "Train Epoch: 470 [172288/225000 (77%)] Loss: 6751.539062\n",
      "Train Epoch: 470 [176384/225000 (78%)] Loss: 6738.681641\n",
      "Train Epoch: 470 [180480/225000 (80%)] Loss: 6772.257812\n",
      "Train Epoch: 470 [184576/225000 (82%)] Loss: 6664.513672\n",
      "Train Epoch: 470 [188672/225000 (84%)] Loss: 6775.320312\n",
      "Train Epoch: 470 [192768/225000 (86%)] Loss: 6882.341797\n",
      "Train Epoch: 470 [196864/225000 (87%)] Loss: 6687.566406\n",
      "Train Epoch: 470 [200960/225000 (89%)] Loss: 6696.507812\n",
      "Train Epoch: 470 [205056/225000 (91%)] Loss: 6711.091797\n",
      "Train Epoch: 470 [209152/225000 (93%)] Loss: 6616.466797\n",
      "Train Epoch: 470 [213248/225000 (95%)] Loss: 6602.126953\n",
      "Train Epoch: 470 [217344/225000 (97%)] Loss: 6735.541016\n",
      "Train Epoch: 470 [221440/225000 (98%)] Loss: 6781.816406\n",
      "    epoch          : 470\n",
      "    loss           : 6724.552992125285\n",
      "    val_loss       : 6741.609967455572\n",
      "Train Epoch: 471 [256/225000 (0%)] Loss: 6705.904297\n",
      "Train Epoch: 471 [4352/225000 (2%)] Loss: 6839.357422\n",
      "Train Epoch: 471 [8448/225000 (4%)] Loss: 6892.382812\n",
      "Train Epoch: 471 [12544/225000 (6%)] Loss: 6638.205078\n",
      "Train Epoch: 471 [16640/225000 (7%)] Loss: 6728.660156\n",
      "Train Epoch: 471 [20736/225000 (9%)] Loss: 6792.990234\n",
      "Train Epoch: 471 [24832/225000 (11%)] Loss: 6762.244141\n",
      "Train Epoch: 471 [28928/225000 (13%)] Loss: 6709.300781\n",
      "Train Epoch: 471 [33024/225000 (15%)] Loss: 6790.744141\n",
      "Train Epoch: 471 [37120/225000 (16%)] Loss: 6632.392578\n",
      "Train Epoch: 471 [41216/225000 (18%)] Loss: 6682.644531\n",
      "Train Epoch: 471 [45312/225000 (20%)] Loss: 6746.736328\n",
      "Train Epoch: 471 [49408/225000 (22%)] Loss: 6813.750000\n",
      "Train Epoch: 471 [53504/225000 (24%)] Loss: 6682.394531\n",
      "Train Epoch: 471 [57600/225000 (26%)] Loss: 6781.904297\n",
      "Train Epoch: 471 [61696/225000 (27%)] Loss: 6712.945312\n",
      "Train Epoch: 471 [65792/225000 (29%)] Loss: 6580.078125\n",
      "Train Epoch: 471 [69888/225000 (31%)] Loss: 6727.464844\n",
      "Train Epoch: 471 [73984/225000 (33%)] Loss: 6627.558594\n",
      "Train Epoch: 471 [78080/225000 (35%)] Loss: 12807.875000\n",
      "Train Epoch: 471 [82176/225000 (37%)] Loss: 6695.753906\n",
      "Train Epoch: 471 [86272/225000 (38%)] Loss: 6566.699219\n",
      "Train Epoch: 471 [90368/225000 (40%)] Loss: 6771.091797\n",
      "Train Epoch: 471 [94464/225000 (42%)] Loss: 6898.994141\n",
      "Train Epoch: 471 [98560/225000 (44%)] Loss: 6699.000000\n",
      "Train Epoch: 471 [102656/225000 (46%)] Loss: 6749.335938\n",
      "Train Epoch: 471 [106752/225000 (47%)] Loss: 6855.552734\n",
      "Train Epoch: 471 [110848/225000 (49%)] Loss: 6916.203125\n",
      "Train Epoch: 471 [114944/225000 (51%)] Loss: 6837.792969\n",
      "Train Epoch: 471 [119040/225000 (53%)] Loss: 6704.720703\n",
      "Train Epoch: 471 [123136/225000 (55%)] Loss: 6838.734375\n",
      "Train Epoch: 471 [127232/225000 (57%)] Loss: 6830.583984\n",
      "Train Epoch: 471 [131328/225000 (58%)] Loss: 6951.873047\n",
      "Train Epoch: 471 [135424/225000 (60%)] Loss: 6780.558594\n",
      "Train Epoch: 471 [139520/225000 (62%)] Loss: 6837.484375\n",
      "Train Epoch: 471 [143616/225000 (64%)] Loss: 6772.501953\n",
      "Train Epoch: 471 [147712/225000 (66%)] Loss: 6815.451172\n",
      "Train Epoch: 471 [151808/225000 (67%)] Loss: 6689.238281\n",
      "Train Epoch: 471 [155904/225000 (69%)] Loss: 6628.326172\n",
      "Train Epoch: 471 [160000/225000 (71%)] Loss: 6754.294922\n",
      "Train Epoch: 471 [164096/225000 (73%)] Loss: 6782.216797\n",
      "Train Epoch: 471 [168192/225000 (75%)] Loss: 6662.623047\n",
      "Train Epoch: 471 [172288/225000 (77%)] Loss: 6668.175781\n",
      "Train Epoch: 471 [176384/225000 (78%)] Loss: 6799.818359\n",
      "Train Epoch: 471 [180480/225000 (80%)] Loss: 6878.048828\n",
      "Train Epoch: 471 [184576/225000 (82%)] Loss: 6753.496094\n",
      "Train Epoch: 471 [188672/225000 (84%)] Loss: 6740.623047\n",
      "Train Epoch: 471 [192768/225000 (86%)] Loss: 6708.937500\n",
      "Train Epoch: 471 [196864/225000 (87%)] Loss: 6591.742188\n",
      "Train Epoch: 471 [200960/225000 (89%)] Loss: 6469.824219\n",
      "Train Epoch: 471 [205056/225000 (91%)] Loss: 6801.708984\n",
      "Train Epoch: 471 [209152/225000 (93%)] Loss: 6732.218750\n",
      "Train Epoch: 471 [213248/225000 (95%)] Loss: 6706.373047\n",
      "Train Epoch: 471 [217344/225000 (97%)] Loss: 6645.664062\n",
      "Train Epoch: 471 [221440/225000 (98%)] Loss: 6899.005859\n",
      "    epoch          : 471\n",
      "    loss           : 6723.124632261448\n",
      "    val_loss       : 6739.799357117438\n",
      "Train Epoch: 472 [256/225000 (0%)] Loss: 6702.779297\n",
      "Train Epoch: 472 [4352/225000 (2%)] Loss: 6718.025391\n",
      "Train Epoch: 472 [8448/225000 (4%)] Loss: 6826.570312\n",
      "Train Epoch: 472 [12544/225000 (6%)] Loss: 6734.257812\n",
      "Train Epoch: 472 [16640/225000 (7%)] Loss: 6757.162109\n",
      "Train Epoch: 472 [20736/225000 (9%)] Loss: 6735.734375\n",
      "Train Epoch: 472 [24832/225000 (11%)] Loss: 6821.218750\n",
      "Train Epoch: 472 [28928/225000 (13%)] Loss: 6722.638672\n",
      "Train Epoch: 472 [33024/225000 (15%)] Loss: 6753.349609\n",
      "Train Epoch: 472 [37120/225000 (16%)] Loss: 6652.437500\n",
      "Train Epoch: 472 [41216/225000 (18%)] Loss: 6752.582031\n",
      "Train Epoch: 472 [45312/225000 (20%)] Loss: 6570.271484\n",
      "Train Epoch: 472 [49408/225000 (22%)] Loss: 6538.367188\n",
      "Train Epoch: 472 [53504/225000 (24%)] Loss: 6540.259766\n",
      "Train Epoch: 472 [57600/225000 (26%)] Loss: 6740.951172\n",
      "Train Epoch: 472 [61696/225000 (27%)] Loss: 6705.939453\n",
      "Train Epoch: 472 [65792/225000 (29%)] Loss: 6824.421875\n",
      "Train Epoch: 472 [69888/225000 (31%)] Loss: 6772.101562\n",
      "Train Epoch: 472 [73984/225000 (33%)] Loss: 6670.441406\n",
      "Train Epoch: 472 [78080/225000 (35%)] Loss: 6792.513672\n",
      "Train Epoch: 472 [82176/225000 (37%)] Loss: 6602.933594\n",
      "Train Epoch: 472 [86272/225000 (38%)] Loss: 6612.439453\n",
      "Train Epoch: 472 [90368/225000 (40%)] Loss: 6810.621094\n",
      "Train Epoch: 472 [94464/225000 (42%)] Loss: 6820.685547\n",
      "Train Epoch: 472 [98560/225000 (44%)] Loss: 6815.605469\n",
      "Train Epoch: 472 [102656/225000 (46%)] Loss: 6615.544922\n",
      "Train Epoch: 472 [106752/225000 (47%)] Loss: 6807.445312\n",
      "Train Epoch: 472 [110848/225000 (49%)] Loss: 6636.615234\n",
      "Train Epoch: 472 [114944/225000 (51%)] Loss: 6781.429688\n",
      "Train Epoch: 472 [119040/225000 (53%)] Loss: 6697.867188\n",
      "Train Epoch: 472 [123136/225000 (55%)] Loss: 6754.876953\n",
      "Train Epoch: 472 [127232/225000 (57%)] Loss: 6759.787109\n",
      "Train Epoch: 472 [131328/225000 (58%)] Loss: 6631.753906\n",
      "Train Epoch: 472 [135424/225000 (60%)] Loss: 6843.425781\n",
      "Train Epoch: 472 [139520/225000 (62%)] Loss: 6667.162109\n",
      "Train Epoch: 472 [143616/225000 (64%)] Loss: 6669.361328\n",
      "Train Epoch: 472 [147712/225000 (66%)] Loss: 6766.490234\n",
      "Train Epoch: 472 [151808/225000 (67%)] Loss: 6829.113281\n",
      "Train Epoch: 472 [155904/225000 (69%)] Loss: 6710.750000\n",
      "Train Epoch: 472 [160000/225000 (71%)] Loss: 6832.267578\n",
      "Train Epoch: 472 [164096/225000 (73%)] Loss: 6908.125000\n",
      "Train Epoch: 472 [168192/225000 (75%)] Loss: 6733.673828\n",
      "Train Epoch: 472 [172288/225000 (77%)] Loss: 6618.919922\n",
      "Train Epoch: 472 [176384/225000 (78%)] Loss: 6730.597656\n",
      "Train Epoch: 472 [180480/225000 (80%)] Loss: 6609.738281\n",
      "Train Epoch: 472 [184576/225000 (82%)] Loss: 6731.082031\n",
      "Train Epoch: 472 [188672/225000 (84%)] Loss: 6686.472656\n",
      "Train Epoch: 472 [192768/225000 (86%)] Loss: 6861.404297\n",
      "Train Epoch: 472 [196864/225000 (87%)] Loss: 6816.431641\n",
      "Train Epoch: 472 [200960/225000 (89%)] Loss: 6835.509766\n",
      "Train Epoch: 472 [205056/225000 (91%)] Loss: 6716.888672\n",
      "Train Epoch: 472 [209152/225000 (93%)] Loss: 6599.677734\n",
      "Train Epoch: 472 [213248/225000 (95%)] Loss: 6891.539062\n",
      "Train Epoch: 472 [217344/225000 (97%)] Loss: 6670.306641\n",
      "Train Epoch: 472 [221440/225000 (98%)] Loss: 6569.275391\n",
      "    epoch          : 472\n",
      "    loss           : 6742.749206751279\n",
      "    val_loss       : 6737.470655436418\n",
      "Train Epoch: 473 [256/225000 (0%)] Loss: 6784.869141\n",
      "Train Epoch: 473 [4352/225000 (2%)] Loss: 6804.636719\n",
      "Train Epoch: 473 [8448/225000 (4%)] Loss: 6679.005859\n",
      "Train Epoch: 473 [12544/225000 (6%)] Loss: 6664.277344\n",
      "Train Epoch: 473 [16640/225000 (7%)] Loss: 6568.599609\n",
      "Train Epoch: 473 [20736/225000 (9%)] Loss: 6682.699219\n",
      "Train Epoch: 473 [24832/225000 (11%)] Loss: 6727.580078\n",
      "Train Epoch: 473 [28928/225000 (13%)] Loss: 6642.105469\n",
      "Train Epoch: 473 [33024/225000 (15%)] Loss: 6665.441406\n",
      "Train Epoch: 473 [37120/225000 (16%)] Loss: 6831.242188\n",
      "Train Epoch: 473 [41216/225000 (18%)] Loss: 6820.066406\n",
      "Train Epoch: 473 [45312/225000 (20%)] Loss: 6772.439453\n",
      "Train Epoch: 473 [49408/225000 (22%)] Loss: 6664.857422\n",
      "Train Epoch: 473 [53504/225000 (24%)] Loss: 6621.947266\n",
      "Train Epoch: 473 [57600/225000 (26%)] Loss: 6788.638672\n",
      "Train Epoch: 473 [61696/225000 (27%)] Loss: 6680.460938\n",
      "Train Epoch: 473 [65792/225000 (29%)] Loss: 6944.277344\n",
      "Train Epoch: 473 [69888/225000 (31%)] Loss: 6623.865234\n",
      "Train Epoch: 473 [73984/225000 (33%)] Loss: 6733.640625\n",
      "Train Epoch: 473 [78080/225000 (35%)] Loss: 6677.994141\n",
      "Train Epoch: 473 [82176/225000 (37%)] Loss: 6874.193359\n",
      "Train Epoch: 473 [86272/225000 (38%)] Loss: 6674.800781\n",
      "Train Epoch: 473 [90368/225000 (40%)] Loss: 6698.396484\n",
      "Train Epoch: 473 [94464/225000 (42%)] Loss: 6775.376953\n",
      "Train Epoch: 473 [98560/225000 (44%)] Loss: 6759.048828\n",
      "Train Epoch: 473 [102656/225000 (46%)] Loss: 6743.943359\n",
      "Train Epoch: 473 [106752/225000 (47%)] Loss: 6682.445312\n",
      "Train Epoch: 473 [110848/225000 (49%)] Loss: 6536.542969\n",
      "Train Epoch: 473 [114944/225000 (51%)] Loss: 6708.330078\n",
      "Train Epoch: 473 [119040/225000 (53%)] Loss: 6709.117188\n",
      "Train Epoch: 473 [123136/225000 (55%)] Loss: 6892.205078\n",
      "Train Epoch: 473 [127232/225000 (57%)] Loss: 16782.404297\n",
      "Train Epoch: 473 [131328/225000 (58%)] Loss: 6711.582031\n",
      "Train Epoch: 473 [135424/225000 (60%)] Loss: 6652.517578\n",
      "Train Epoch: 473 [139520/225000 (62%)] Loss: 6796.500000\n",
      "Train Epoch: 473 [143616/225000 (64%)] Loss: 6600.685547\n",
      "Train Epoch: 473 [147712/225000 (66%)] Loss: 6849.128906\n",
      "Train Epoch: 473 [151808/225000 (67%)] Loss: 6741.845703\n",
      "Train Epoch: 473 [155904/225000 (69%)] Loss: 6714.955078\n",
      "Train Epoch: 473 [160000/225000 (71%)] Loss: 6681.333984\n",
      "Train Epoch: 473 [164096/225000 (73%)] Loss: 6738.029297\n",
      "Train Epoch: 473 [168192/225000 (75%)] Loss: 6766.396484\n",
      "Train Epoch: 473 [172288/225000 (77%)] Loss: 6812.423828\n",
      "Train Epoch: 473 [176384/225000 (78%)] Loss: 6755.783203\n",
      "Train Epoch: 473 [180480/225000 (80%)] Loss: 6749.103516\n",
      "Train Epoch: 473 [184576/225000 (82%)] Loss: 6672.203125\n",
      "Train Epoch: 473 [188672/225000 (84%)] Loss: 6648.974609\n",
      "Train Epoch: 473 [192768/225000 (86%)] Loss: 6741.857422\n",
      "Train Epoch: 473 [196864/225000 (87%)] Loss: 6827.656250\n",
      "Train Epoch: 473 [200960/225000 (89%)] Loss: 6785.798828\n",
      "Train Epoch: 473 [205056/225000 (91%)] Loss: 6637.328125\n",
      "Train Epoch: 473 [209152/225000 (93%)] Loss: 6711.498047\n",
      "Train Epoch: 473 [213248/225000 (95%)] Loss: 6715.929688\n",
      "Train Epoch: 473 [217344/225000 (97%)] Loss: 6666.619141\n",
      "Train Epoch: 473 [221440/225000 (98%)] Loss: 6524.712891\n",
      "    epoch          : 473\n",
      "    loss           : 6747.743212946175\n",
      "    val_loss       : 6739.883374365008\n",
      "Train Epoch: 474 [256/225000 (0%)] Loss: 6812.324219\n",
      "Train Epoch: 474 [4352/225000 (2%)] Loss: 6742.142578\n",
      "Train Epoch: 474 [8448/225000 (4%)] Loss: 6737.177734\n",
      "Train Epoch: 474 [12544/225000 (6%)] Loss: 6641.626953\n",
      "Train Epoch: 474 [16640/225000 (7%)] Loss: 6820.732422\n",
      "Train Epoch: 474 [20736/225000 (9%)] Loss: 6721.687500\n",
      "Train Epoch: 474 [24832/225000 (11%)] Loss: 6793.921875\n",
      "Train Epoch: 474 [28928/225000 (13%)] Loss: 6611.513672\n",
      "Train Epoch: 474 [33024/225000 (15%)] Loss: 6659.847656\n",
      "Train Epoch: 474 [37120/225000 (16%)] Loss: 6812.332031\n",
      "Train Epoch: 474 [41216/225000 (18%)] Loss: 6783.154297\n",
      "Train Epoch: 474 [45312/225000 (20%)] Loss: 6727.681641\n",
      "Train Epoch: 474 [49408/225000 (22%)] Loss: 6719.638672\n",
      "Train Epoch: 474 [53504/225000 (24%)] Loss: 6661.365234\n",
      "Train Epoch: 474 [57600/225000 (26%)] Loss: 6712.507812\n",
      "Train Epoch: 474 [61696/225000 (27%)] Loss: 6737.804688\n",
      "Train Epoch: 474 [65792/225000 (29%)] Loss: 6684.517578\n",
      "Train Epoch: 474 [69888/225000 (31%)] Loss: 6685.740234\n",
      "Train Epoch: 474 [73984/225000 (33%)] Loss: 6841.435547\n",
      "Train Epoch: 474 [78080/225000 (35%)] Loss: 6637.957031\n",
      "Train Epoch: 474 [82176/225000 (37%)] Loss: 12772.060547\n",
      "Train Epoch: 474 [86272/225000 (38%)] Loss: 6784.001953\n",
      "Train Epoch: 474 [90368/225000 (40%)] Loss: 6684.753906\n",
      "Train Epoch: 474 [94464/225000 (42%)] Loss: 6655.201172\n",
      "Train Epoch: 474 [98560/225000 (44%)] Loss: 6696.250000\n",
      "Train Epoch: 474 [102656/225000 (46%)] Loss: 6621.822266\n",
      "Train Epoch: 474 [106752/225000 (47%)] Loss: 6682.220703\n",
      "Train Epoch: 474 [110848/225000 (49%)] Loss: 6711.267578\n",
      "Train Epoch: 474 [114944/225000 (51%)] Loss: 6713.832031\n",
      "Train Epoch: 474 [119040/225000 (53%)] Loss: 6647.666016\n",
      "Train Epoch: 474 [123136/225000 (55%)] Loss: 6660.361328\n",
      "Train Epoch: 474 [127232/225000 (57%)] Loss: 6706.443359\n",
      "Train Epoch: 474 [131328/225000 (58%)] Loss: 6699.173828\n",
      "Train Epoch: 474 [135424/225000 (60%)] Loss: 6772.232422\n",
      "Train Epoch: 474 [139520/225000 (62%)] Loss: 6709.066406\n",
      "Train Epoch: 474 [143616/225000 (64%)] Loss: 6753.390625\n",
      "Train Epoch: 474 [147712/225000 (66%)] Loss: 6629.746094\n",
      "Train Epoch: 474 [151808/225000 (67%)] Loss: 6633.794922\n",
      "Train Epoch: 474 [155904/225000 (69%)] Loss: 6600.548828\n",
      "Train Epoch: 474 [160000/225000 (71%)] Loss: 6684.617188\n",
      "Train Epoch: 474 [164096/225000 (73%)] Loss: 6685.792969\n",
      "Train Epoch: 474 [168192/225000 (75%)] Loss: 6844.812500\n",
      "Train Epoch: 474 [172288/225000 (77%)] Loss: 6667.933594\n",
      "Train Epoch: 474 [176384/225000 (78%)] Loss: 6751.929688\n",
      "Train Epoch: 474 [180480/225000 (80%)] Loss: 6737.011719\n",
      "Train Epoch: 474 [184576/225000 (82%)] Loss: 6680.025391\n",
      "Train Epoch: 474 [188672/225000 (84%)] Loss: 6690.916016\n",
      "Train Epoch: 474 [192768/225000 (86%)] Loss: 6651.714844\n",
      "Train Epoch: 474 [196864/225000 (87%)] Loss: 6832.302734\n",
      "Train Epoch: 474 [200960/225000 (89%)] Loss: 6571.876953\n",
      "Train Epoch: 474 [205056/225000 (91%)] Loss: 6652.718750\n",
      "Train Epoch: 474 [209152/225000 (93%)] Loss: 6708.160156\n",
      "Train Epoch: 474 [213248/225000 (95%)] Loss: 6695.880859\n",
      "Train Epoch: 474 [217344/225000 (97%)] Loss: 6719.492188\n",
      "Train Epoch: 474 [221440/225000 (98%)] Loss: 6656.144531\n",
      "    epoch          : 474\n",
      "    loss           : 6749.032281001138\n",
      "    val_loss       : 6924.066263437271\n",
      "Train Epoch: 475 [256/225000 (0%)] Loss: 6753.732422\n",
      "Train Epoch: 475 [4352/225000 (2%)] Loss: 6743.046875\n",
      "Train Epoch: 475 [8448/225000 (4%)] Loss: 6714.980469\n",
      "Train Epoch: 475 [12544/225000 (6%)] Loss: 6728.316406\n",
      "Train Epoch: 475 [16640/225000 (7%)] Loss: 6693.685547\n",
      "Train Epoch: 475 [20736/225000 (9%)] Loss: 6742.599609\n",
      "Train Epoch: 475 [24832/225000 (11%)] Loss: 6644.546875\n",
      "Train Epoch: 475 [28928/225000 (13%)] Loss: 6886.398438\n",
      "Train Epoch: 475 [33024/225000 (15%)] Loss: 6761.982422\n",
      "Train Epoch: 475 [37120/225000 (16%)] Loss: 6759.107422\n",
      "Train Epoch: 475 [41216/225000 (18%)] Loss: 6658.203125\n",
      "Train Epoch: 475 [45312/225000 (20%)] Loss: 6750.871094\n",
      "Train Epoch: 475 [49408/225000 (22%)] Loss: 6680.019531\n",
      "Train Epoch: 475 [53504/225000 (24%)] Loss: 6673.960938\n",
      "Train Epoch: 475 [57600/225000 (26%)] Loss: 6621.359375\n",
      "Train Epoch: 475 [61696/225000 (27%)] Loss: 6789.970703\n",
      "Train Epoch: 475 [65792/225000 (29%)] Loss: 6800.062500\n",
      "Train Epoch: 475 [69888/225000 (31%)] Loss: 6528.767578\n",
      "Train Epoch: 475 [73984/225000 (33%)] Loss: 6726.492188\n",
      "Train Epoch: 475 [78080/225000 (35%)] Loss: 6765.861328\n",
      "Train Epoch: 475 [82176/225000 (37%)] Loss: 6723.597656\n",
      "Train Epoch: 475 [86272/225000 (38%)] Loss: 6845.417969\n",
      "Train Epoch: 475 [90368/225000 (40%)] Loss: 6745.009766\n",
      "Train Epoch: 475 [94464/225000 (42%)] Loss: 6752.654297\n",
      "Train Epoch: 475 [98560/225000 (44%)] Loss: 6796.035156\n",
      "Train Epoch: 475 [102656/225000 (46%)] Loss: 6790.744141\n",
      "Train Epoch: 475 [106752/225000 (47%)] Loss: 6708.691406\n",
      "Train Epoch: 475 [110848/225000 (49%)] Loss: 6656.166016\n",
      "Train Epoch: 475 [114944/225000 (51%)] Loss: 6714.794922\n",
      "Train Epoch: 475 [119040/225000 (53%)] Loss: 6721.457031\n",
      "Train Epoch: 475 [123136/225000 (55%)] Loss: 6648.455078\n",
      "Train Epoch: 475 [127232/225000 (57%)] Loss: 6650.662109\n",
      "Train Epoch: 475 [131328/225000 (58%)] Loss: 6704.207031\n",
      "Train Epoch: 475 [135424/225000 (60%)] Loss: 6708.679688\n",
      "Train Epoch: 475 [139520/225000 (62%)] Loss: 6779.791016\n",
      "Train Epoch: 475 [143616/225000 (64%)] Loss: 6778.107422\n",
      "Train Epoch: 475 [147712/225000 (66%)] Loss: 6756.386719\n",
      "Train Epoch: 475 [151808/225000 (67%)] Loss: 6706.769531\n",
      "Train Epoch: 475 [155904/225000 (69%)] Loss: 6579.570312\n",
      "Train Epoch: 475 [160000/225000 (71%)] Loss: 6719.666016\n",
      "Train Epoch: 475 [164096/225000 (73%)] Loss: 6844.951172\n",
      "Train Epoch: 475 [168192/225000 (75%)] Loss: 6639.591797\n",
      "Train Epoch: 475 [172288/225000 (77%)] Loss: 6761.583984\n",
      "Train Epoch: 475 [176384/225000 (78%)] Loss: 6675.716797\n",
      "Train Epoch: 475 [180480/225000 (80%)] Loss: 6610.511719\n",
      "Train Epoch: 475 [184576/225000 (82%)] Loss: 6606.273438\n",
      "Train Epoch: 475 [188672/225000 (84%)] Loss: 6669.283203\n",
      "Train Epoch: 475 [192768/225000 (86%)] Loss: 6657.955078\n",
      "Train Epoch: 475 [196864/225000 (87%)] Loss: 6650.304688\n",
      "Train Epoch: 475 [200960/225000 (89%)] Loss: 6648.417969\n",
      "Train Epoch: 475 [205056/225000 (91%)] Loss: 6637.173828\n",
      "Train Epoch: 475 [209152/225000 (93%)] Loss: 6557.041016\n",
      "Train Epoch: 475 [213248/225000 (95%)] Loss: 6658.054688\n",
      "Train Epoch: 475 [217344/225000 (97%)] Loss: 6644.109375\n",
      "Train Epoch: 475 [221440/225000 (98%)] Loss: 6671.466797\n",
      "    epoch          : 475\n",
      "    loss           : 6725.279014682878\n",
      "    val_loss       : 6743.042006945124\n",
      "Train Epoch: 476 [256/225000 (0%)] Loss: 6791.951172\n",
      "Train Epoch: 476 [4352/225000 (2%)] Loss: 6793.607422\n",
      "Train Epoch: 476 [8448/225000 (4%)] Loss: 6678.912109\n",
      "Train Epoch: 476 [12544/225000 (6%)] Loss: 6781.187500\n",
      "Train Epoch: 476 [16640/225000 (7%)] Loss: 6620.949219\n",
      "Train Epoch: 476 [20736/225000 (9%)] Loss: 6605.535156\n",
      "Train Epoch: 476 [24832/225000 (11%)] Loss: 6789.255859\n",
      "Train Epoch: 476 [28928/225000 (13%)] Loss: 6755.845703\n",
      "Train Epoch: 476 [33024/225000 (15%)] Loss: 6610.246094\n",
      "Train Epoch: 476 [37120/225000 (16%)] Loss: 6599.005859\n",
      "Train Epoch: 476 [41216/225000 (18%)] Loss: 6654.089844\n",
      "Train Epoch: 476 [45312/225000 (20%)] Loss: 6682.984375\n",
      "Train Epoch: 476 [49408/225000 (22%)] Loss: 6681.041016\n",
      "Train Epoch: 476 [53504/225000 (24%)] Loss: 6695.503906\n",
      "Train Epoch: 476 [57600/225000 (26%)] Loss: 6737.351562\n",
      "Train Epoch: 476 [61696/225000 (27%)] Loss: 6753.767578\n",
      "Train Epoch: 476 [65792/225000 (29%)] Loss: 6730.519531\n",
      "Train Epoch: 476 [69888/225000 (31%)] Loss: 6746.283203\n",
      "Train Epoch: 476 [73984/225000 (33%)] Loss: 6770.382812\n",
      "Train Epoch: 476 [78080/225000 (35%)] Loss: 6596.044922\n",
      "Train Epoch: 476 [82176/225000 (37%)] Loss: 6817.251953\n",
      "Train Epoch: 476 [86272/225000 (38%)] Loss: 6711.857422\n",
      "Train Epoch: 476 [90368/225000 (40%)] Loss: 6593.876953\n",
      "Train Epoch: 476 [94464/225000 (42%)] Loss: 6695.912109\n",
      "Train Epoch: 476 [98560/225000 (44%)] Loss: 6804.218750\n",
      "Train Epoch: 476 [102656/225000 (46%)] Loss: 6821.064453\n",
      "Train Epoch: 476 [106752/225000 (47%)] Loss: 6780.933594\n",
      "Train Epoch: 476 [110848/225000 (49%)] Loss: 6601.712891\n",
      "Train Epoch: 476 [114944/225000 (51%)] Loss: 6729.761719\n",
      "Train Epoch: 476 [119040/225000 (53%)] Loss: 6779.878906\n",
      "Train Epoch: 476 [123136/225000 (55%)] Loss: 6608.812500\n",
      "Train Epoch: 476 [127232/225000 (57%)] Loss: 6679.650391\n",
      "Train Epoch: 476 [131328/225000 (58%)] Loss: 6681.613281\n",
      "Train Epoch: 476 [135424/225000 (60%)] Loss: 6750.066406\n",
      "Train Epoch: 476 [139520/225000 (62%)] Loss: 6761.644531\n",
      "Train Epoch: 476 [143616/225000 (64%)] Loss: 6896.289062\n",
      "Train Epoch: 476 [147712/225000 (66%)] Loss: 6742.718750\n",
      "Train Epoch: 476 [151808/225000 (67%)] Loss: 6714.898438\n",
      "Train Epoch: 476 [155904/225000 (69%)] Loss: 6683.207031\n",
      "Train Epoch: 476 [160000/225000 (71%)] Loss: 6700.353516\n",
      "Train Epoch: 476 [164096/225000 (73%)] Loss: 6573.966797\n",
      "Train Epoch: 476 [168192/225000 (75%)] Loss: 6786.837891\n",
      "Train Epoch: 476 [172288/225000 (77%)] Loss: 6625.587891\n",
      "Train Epoch: 476 [176384/225000 (78%)] Loss: 6793.212891\n",
      "Train Epoch: 476 [180480/225000 (80%)] Loss: 6652.816406\n",
      "Train Epoch: 476 [184576/225000 (82%)] Loss: 6722.146484\n",
      "Train Epoch: 476 [188672/225000 (84%)] Loss: 6643.751953\n",
      "Train Epoch: 476 [192768/225000 (86%)] Loss: 6716.304688\n",
      "Train Epoch: 476 [196864/225000 (87%)] Loss: 6787.107422\n",
      "Train Epoch: 476 [200960/225000 (89%)] Loss: 6823.957031\n",
      "Train Epoch: 476 [205056/225000 (91%)] Loss: 6616.779297\n",
      "Train Epoch: 476 [209152/225000 (93%)] Loss: 6755.781250\n",
      "Train Epoch: 476 [213248/225000 (95%)] Loss: 6718.107422\n",
      "Train Epoch: 476 [217344/225000 (97%)] Loss: 6655.433594\n",
      "Train Epoch: 477 [4352/225000 (2%)] Loss: 6631.542969\n",
      "Train Epoch: 477 [8448/225000 (4%)] Loss: 6718.427734\n",
      "Train Epoch: 477 [12544/225000 (6%)] Loss: 6644.162109\n",
      "Train Epoch: 477 [16640/225000 (7%)] Loss: 6595.296875\n",
      "Train Epoch: 477 [20736/225000 (9%)] Loss: 6890.238281\n",
      "Train Epoch: 477 [24832/225000 (11%)] Loss: 6705.691406\n",
      "Train Epoch: 477 [28928/225000 (13%)] Loss: 6700.144531\n",
      "Train Epoch: 477 [33024/225000 (15%)] Loss: 6761.759766\n",
      "Train Epoch: 477 [37120/225000 (16%)] Loss: 6631.740234\n",
      "Train Epoch: 477 [41216/225000 (18%)] Loss: 6696.851562\n",
      "Train Epoch: 477 [45312/225000 (20%)] Loss: 6834.259766\n",
      "Train Epoch: 477 [49408/225000 (22%)] Loss: 6696.384766\n",
      "Train Epoch: 477 [53504/225000 (24%)] Loss: 6639.289062\n",
      "Train Epoch: 477 [57600/225000 (26%)] Loss: 6858.613281\n",
      "Train Epoch: 477 [61696/225000 (27%)] Loss: 6585.173828\n",
      "Train Epoch: 477 [65792/225000 (29%)] Loss: 6915.478516\n",
      "Train Epoch: 477 [69888/225000 (31%)] Loss: 6523.238281\n",
      "Train Epoch: 477 [73984/225000 (33%)] Loss: 6773.820312\n",
      "Train Epoch: 477 [78080/225000 (35%)] Loss: 6683.423828\n",
      "Train Epoch: 477 [82176/225000 (37%)] Loss: 6630.687500\n",
      "Train Epoch: 477 [86272/225000 (38%)] Loss: 6694.320312\n",
      "Train Epoch: 477 [90368/225000 (40%)] Loss: 6655.585938\n",
      "Train Epoch: 477 [94464/225000 (42%)] Loss: 6778.720703\n",
      "Train Epoch: 477 [98560/225000 (44%)] Loss: 6621.083984\n",
      "Train Epoch: 477 [102656/225000 (46%)] Loss: 6797.511719\n",
      "Train Epoch: 477 [106752/225000 (47%)] Loss: 6550.808594\n",
      "Train Epoch: 477 [110848/225000 (49%)] Loss: 6690.039062\n",
      "Train Epoch: 477 [114944/225000 (51%)] Loss: 6702.351562\n",
      "Train Epoch: 477 [119040/225000 (53%)] Loss: 12332.898438\n",
      "Train Epoch: 477 [123136/225000 (55%)] Loss: 6796.769531\n",
      "Train Epoch: 477 [127232/225000 (57%)] Loss: 6738.544922\n",
      "Train Epoch: 477 [131328/225000 (58%)] Loss: 6822.144531\n",
      "Train Epoch: 477 [135424/225000 (60%)] Loss: 6695.267578\n",
      "Train Epoch: 477 [139520/225000 (62%)] Loss: 6711.386719\n",
      "Train Epoch: 477 [143616/225000 (64%)] Loss: 6806.732422\n",
      "Train Epoch: 477 [147712/225000 (66%)] Loss: 6659.464844\n",
      "Train Epoch: 477 [151808/225000 (67%)] Loss: 6798.724609\n",
      "Train Epoch: 477 [155904/225000 (69%)] Loss: 6778.978516\n",
      "Train Epoch: 477 [160000/225000 (71%)] Loss: 6624.623047\n",
      "Train Epoch: 477 [164096/225000 (73%)] Loss: 6643.031250\n",
      "Train Epoch: 477 [168192/225000 (75%)] Loss: 6701.537109\n",
      "Train Epoch: 477 [172288/225000 (77%)] Loss: 6746.931641\n",
      "Train Epoch: 477 [176384/225000 (78%)] Loss: 6662.238281\n",
      "Train Epoch: 477 [180480/225000 (80%)] Loss: 6740.974609\n",
      "Train Epoch: 477 [184576/225000 (82%)] Loss: 6828.013672\n",
      "Train Epoch: 477 [188672/225000 (84%)] Loss: 6629.800781\n",
      "Train Epoch: 477 [192768/225000 (86%)] Loss: 6676.187500\n",
      "Train Epoch: 477 [196864/225000 (87%)] Loss: 6779.929688\n",
      "Train Epoch: 477 [200960/225000 (89%)] Loss: 6638.019531\n",
      "Train Epoch: 477 [205056/225000 (91%)] Loss: 6768.236328\n",
      "Train Epoch: 477 [209152/225000 (93%)] Loss: 6816.509766\n",
      "Train Epoch: 477 [213248/225000 (95%)] Loss: 6710.123047\n",
      "Train Epoch: 477 [217344/225000 (97%)] Loss: 6445.287109\n",
      "Train Epoch: 477 [221440/225000 (98%)] Loss: 6755.455078\n",
      "    epoch          : 477\n",
      "    loss           : 6750.270954431528\n",
      "    val_loss       : 6845.769952804459\n",
      "Train Epoch: 478 [256/225000 (0%)] Loss: 6904.046875\n",
      "Train Epoch: 478 [4352/225000 (2%)] Loss: 6661.658203\n",
      "Train Epoch: 478 [8448/225000 (4%)] Loss: 6672.353516\n",
      "Train Epoch: 478 [12544/225000 (6%)] Loss: 6806.734375\n",
      "Train Epoch: 478 [16640/225000 (7%)] Loss: 6793.085938\n",
      "Train Epoch: 478 [20736/225000 (9%)] Loss: 6793.552734\n",
      "Train Epoch: 478 [24832/225000 (11%)] Loss: 6606.742188\n",
      "Train Epoch: 478 [28928/225000 (13%)] Loss: 6801.845703\n",
      "Train Epoch: 478 [33024/225000 (15%)] Loss: 6781.730469\n",
      "Train Epoch: 478 [37120/225000 (16%)] Loss: 6677.005859\n",
      "Train Epoch: 478 [41216/225000 (18%)] Loss: 6631.693359\n",
      "Train Epoch: 478 [45312/225000 (20%)] Loss: 6698.658203\n",
      "Train Epoch: 478 [49408/225000 (22%)] Loss: 6752.558594\n",
      "Train Epoch: 478 [53504/225000 (24%)] Loss: 6735.345703\n",
      "Train Epoch: 478 [57600/225000 (26%)] Loss: 6652.062500\n",
      "Train Epoch: 478 [61696/225000 (27%)] Loss: 6894.166016\n",
      "Train Epoch: 478 [65792/225000 (29%)] Loss: 6788.236328\n",
      "Train Epoch: 478 [69888/225000 (31%)] Loss: 6536.185547\n",
      "Train Epoch: 478 [73984/225000 (33%)] Loss: 6724.013672\n",
      "Train Epoch: 478 [78080/225000 (35%)] Loss: 6620.771484\n",
      "Train Epoch: 478 [82176/225000 (37%)] Loss: 6696.154297\n",
      "Train Epoch: 478 [86272/225000 (38%)] Loss: 6761.203125\n",
      "Train Epoch: 478 [90368/225000 (40%)] Loss: 6687.894531\n",
      "Train Epoch: 478 [94464/225000 (42%)] Loss: 6760.357422\n",
      "Train Epoch: 478 [98560/225000 (44%)] Loss: 6602.455078\n",
      "Train Epoch: 478 [102656/225000 (46%)] Loss: 6584.412109\n",
      "Train Epoch: 478 [106752/225000 (47%)] Loss: 6799.757812\n",
      "Train Epoch: 478 [110848/225000 (49%)] Loss: 6768.808594\n",
      "Train Epoch: 478 [114944/225000 (51%)] Loss: 6820.216797\n",
      "Train Epoch: 478 [119040/225000 (53%)] Loss: 6691.830078\n",
      "Train Epoch: 478 [123136/225000 (55%)] Loss: 6772.962891\n",
      "Train Epoch: 478 [127232/225000 (57%)] Loss: 6650.013672\n",
      "Train Epoch: 478 [131328/225000 (58%)] Loss: 6631.464844\n",
      "Train Epoch: 478 [135424/225000 (60%)] Loss: 6741.546875\n",
      "Train Epoch: 478 [139520/225000 (62%)] Loss: 6604.603516\n",
      "Train Epoch: 478 [143616/225000 (64%)] Loss: 6732.957031\n",
      "Train Epoch: 478 [147712/225000 (66%)] Loss: 6718.273438\n",
      "Train Epoch: 478 [151808/225000 (67%)] Loss: 6650.748047\n",
      "Train Epoch: 478 [155904/225000 (69%)] Loss: 6808.554688\n",
      "Train Epoch: 478 [160000/225000 (71%)] Loss: 6686.544922\n",
      "Train Epoch: 478 [164096/225000 (73%)] Loss: 6696.158203\n",
      "Train Epoch: 478 [168192/225000 (75%)] Loss: 6754.242188\n",
      "Train Epoch: 478 [172288/225000 (77%)] Loss: 6643.130859\n",
      "Train Epoch: 478 [176384/225000 (78%)] Loss: 6721.136719\n",
      "Train Epoch: 478 [180480/225000 (80%)] Loss: 6714.521484\n",
      "Train Epoch: 478 [184576/225000 (82%)] Loss: 6653.724609\n",
      "Train Epoch: 478 [188672/225000 (84%)] Loss: 6748.792969\n",
      "Train Epoch: 478 [192768/225000 (86%)] Loss: 6733.537109\n",
      "Train Epoch: 478 [196864/225000 (87%)] Loss: 6782.029297\n",
      "Train Epoch: 478 [200960/225000 (89%)] Loss: 6802.337891\n",
      "Train Epoch: 478 [205056/225000 (91%)] Loss: 6638.658203\n",
      "Train Epoch: 478 [209152/225000 (93%)] Loss: 6728.078125\n",
      "Train Epoch: 478 [213248/225000 (95%)] Loss: 6666.607422\n",
      "Train Epoch: 478 [217344/225000 (97%)] Loss: 6717.250000\n",
      "Train Epoch: 478 [221440/225000 (98%)] Loss: 6628.513672\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "    epoch          : 478\n",
      "    loss           : 6719.612760194468\n",
      "    val_loss       : 6794.344517912184\n",
      "Train Epoch: 479 [256/225000 (0%)] Loss: 6650.363281\n",
      "Train Epoch: 479 [4352/225000 (2%)] Loss: 6641.052734\n",
      "Train Epoch: 479 [8448/225000 (4%)] Loss: 6834.208984\n",
      "Train Epoch: 479 [12544/225000 (6%)] Loss: 6770.408203\n",
      "Train Epoch: 479 [16640/225000 (7%)] Loss: 6780.978516\n",
      "Train Epoch: 479 [20736/225000 (9%)] Loss: 6630.765625\n",
      "Train Epoch: 479 [24832/225000 (11%)] Loss: 6629.269531\n",
      "Train Epoch: 479 [28928/225000 (13%)] Loss: 6628.271484\n",
      "Train Epoch: 479 [33024/225000 (15%)] Loss: 6790.208984\n",
      "Train Epoch: 479 [37120/225000 (16%)] Loss: 6537.861328\n",
      "Train Epoch: 479 [41216/225000 (18%)] Loss: 6692.521484\n",
      "Train Epoch: 479 [45312/225000 (20%)] Loss: 6746.339844\n",
      "Train Epoch: 479 [49408/225000 (22%)] Loss: 6567.875000\n",
      "Train Epoch: 479 [53504/225000 (24%)] Loss: 6647.814453\n",
      "Train Epoch: 479 [57600/225000 (26%)] Loss: 6804.085938\n",
      "Train Epoch: 479 [61696/225000 (27%)] Loss: 6747.919922\n",
      "Train Epoch: 479 [65792/225000 (29%)] Loss: 6804.597656\n",
      "Train Epoch: 479 [69888/225000 (31%)] Loss: 6618.904297\n",
      "Train Epoch: 479 [73984/225000 (33%)] Loss: 6707.613281\n",
      "Train Epoch: 479 [78080/225000 (35%)] Loss: 6739.296875\n",
      "Train Epoch: 479 [82176/225000 (37%)] Loss: 6692.525391\n",
      "Train Epoch: 479 [86272/225000 (38%)] Loss: 6662.003906\n",
      "Train Epoch: 479 [90368/225000 (40%)] Loss: 6687.283203\n",
      "Train Epoch: 479 [94464/225000 (42%)] Loss: 6795.574219\n",
      "Train Epoch: 479 [98560/225000 (44%)] Loss: 6689.810547\n",
      "Train Epoch: 479 [102656/225000 (46%)] Loss: 6744.240234\n",
      "Train Epoch: 479 [106752/225000 (47%)] Loss: 6619.330078\n",
      "Train Epoch: 479 [110848/225000 (49%)] Loss: 6728.679688\n",
      "Train Epoch: 479 [114944/225000 (51%)] Loss: 6780.064453\n",
      "Train Epoch: 479 [119040/225000 (53%)] Loss: 6606.865234\n",
      "Train Epoch: 479 [123136/225000 (55%)] Loss: 6623.414062\n",
      "Train Epoch: 479 [127232/225000 (57%)] Loss: 6631.410156\n",
      "Train Epoch: 479 [131328/225000 (58%)] Loss: 6787.529297\n",
      "Train Epoch: 479 [135424/225000 (60%)] Loss: 6743.207031\n",
      "Train Epoch: 479 [139520/225000 (62%)] Loss: 6680.623047\n",
      "Train Epoch: 479 [143616/225000 (64%)] Loss: 6523.900391\n",
      "Train Epoch: 479 [147712/225000 (66%)] Loss: 6619.917969\n",
      "Train Epoch: 479 [151808/225000 (67%)] Loss: 6652.482422\n",
      "Train Epoch: 479 [155904/225000 (69%)] Loss: 6775.677734\n",
      "Train Epoch: 479 [160000/225000 (71%)] Loss: 6624.273438\n",
      "Train Epoch: 479 [164096/225000 (73%)] Loss: 6840.347656\n",
      "Train Epoch: 479 [168192/225000 (75%)] Loss: 6447.628906\n",
      "Train Epoch: 479 [172288/225000 (77%)] Loss: 6697.791016\n",
      "Train Epoch: 479 [176384/225000 (78%)] Loss: 6694.947266\n",
      "Train Epoch: 479 [180480/225000 (80%)] Loss: 6719.072266\n",
      "Train Epoch: 479 [184576/225000 (82%)] Loss: 6582.503906\n",
      "Train Epoch: 479 [188672/225000 (84%)] Loss: 6751.017578\n",
      "Train Epoch: 479 [192768/225000 (86%)] Loss: 6788.755859\n",
      "Train Epoch: 479 [196864/225000 (87%)] Loss: 6587.417969\n",
      "Train Epoch: 479 [200960/225000 (89%)] Loss: 6651.035156\n",
      "Train Epoch: 479 [205056/225000 (91%)] Loss: 6707.556641\n",
      "Train Epoch: 479 [209152/225000 (93%)] Loss: 6739.552734\n",
      "Train Epoch: 479 [213248/225000 (95%)] Loss: 6711.666016\n",
      "Train Epoch: 479 [217344/225000 (97%)] Loss: 6676.837891\n",
      "Train Epoch: 479 [221440/225000 (98%)] Loss: 6781.923828\n",
      "    epoch          : 479\n",
      "    loss           : 6726.577900579494\n",
      "    val_loss       : 6721.847902470706\n",
      "Train Epoch: 480 [256/225000 (0%)] Loss: 6807.333984\n",
      "Train Epoch: 480 [4352/225000 (2%)] Loss: 6636.068359\n",
      "Train Epoch: 480 [8448/225000 (4%)] Loss: 6717.390625\n",
      "Train Epoch: 480 [12544/225000 (6%)] Loss: 6659.837891\n",
      "Train Epoch: 480 [16640/225000 (7%)] Loss: 6720.000000\n",
      "Train Epoch: 480 [20736/225000 (9%)] Loss: 6734.871094\n",
      "Train Epoch: 480 [24832/225000 (11%)] Loss: 6739.230469\n",
      "Train Epoch: 480 [28928/225000 (13%)] Loss: 6664.791016\n",
      "Train Epoch: 480 [33024/225000 (15%)] Loss: 6744.207031\n",
      "Train Epoch: 480 [37120/225000 (16%)] Loss: 6786.263672\n",
      "Train Epoch: 480 [41216/225000 (18%)] Loss: 6597.667969\n",
      "Train Epoch: 480 [45312/225000 (20%)] Loss: 6611.259766\n",
      "Train Epoch: 480 [49408/225000 (22%)] Loss: 6662.390625\n",
      "Train Epoch: 480 [53504/225000 (24%)] Loss: 6735.722656\n",
      "Train Epoch: 480 [57600/225000 (26%)] Loss: 6784.175781\n",
      "Train Epoch: 480 [61696/225000 (27%)] Loss: 6692.281250\n",
      "Train Epoch: 480 [65792/225000 (29%)] Loss: 6633.076172\n",
      "Train Epoch: 480 [69888/225000 (31%)] Loss: 6733.796875\n",
      "Train Epoch: 480 [73984/225000 (33%)] Loss: 6680.146484\n",
      "Train Epoch: 480 [78080/225000 (35%)] Loss: 6732.429688\n",
      "Train Epoch: 480 [82176/225000 (37%)] Loss: 6722.802734\n",
      "Train Epoch: 480 [86272/225000 (38%)] Loss: 6536.587891\n",
      "Train Epoch: 480 [90368/225000 (40%)] Loss: 6656.408203\n",
      "Train Epoch: 480 [94464/225000 (42%)] Loss: 6672.544922\n",
      "Train Epoch: 480 [98560/225000 (44%)] Loss: 6685.785156\n",
      "Train Epoch: 480 [102656/225000 (46%)] Loss: 6643.957031\n",
      "Train Epoch: 480 [106752/225000 (47%)] Loss: 6896.935547\n",
      "Train Epoch: 480 [110848/225000 (49%)] Loss: 6755.765625\n",
      "Train Epoch: 480 [114944/225000 (51%)] Loss: 6799.335938\n",
      "Train Epoch: 480 [119040/225000 (53%)] Loss: 6649.158203\n",
      "Train Epoch: 480 [123136/225000 (55%)] Loss: 6664.859375\n",
      "Train Epoch: 480 [127232/225000 (57%)] Loss: 6657.710938\n",
      "Train Epoch: 480 [131328/225000 (58%)] Loss: 6699.433594\n",
      "Train Epoch: 480 [135424/225000 (60%)] Loss: 6675.035156\n",
      "Train Epoch: 480 [139520/225000 (62%)] Loss: 6612.859375\n",
      "Train Epoch: 480 [143616/225000 (64%)] Loss: 6818.722656\n",
      "Train Epoch: 480 [147712/225000 (66%)] Loss: 6588.273438\n",
      "Train Epoch: 480 [151808/225000 (67%)] Loss: 6900.617188\n",
      "Train Epoch: 480 [155904/225000 (69%)] Loss: 6673.611328\n",
      "Train Epoch: 480 [160000/225000 (71%)] Loss: 6690.853516\n",
      "Train Epoch: 480 [164096/225000 (73%)] Loss: 6552.375000\n",
      "Train Epoch: 480 [168192/225000 (75%)] Loss: 6628.849609\n",
      "Train Epoch: 480 [172288/225000 (77%)] Loss: 6778.798828\n",
      "Train Epoch: 480 [176384/225000 (78%)] Loss: 6788.636719\n",
      "Train Epoch: 480 [180480/225000 (80%)] Loss: 6702.123047\n",
      "Train Epoch: 480 [184576/225000 (82%)] Loss: 6781.958984\n",
      "Train Epoch: 480 [188672/225000 (84%)] Loss: 6736.226562\n",
      "Train Epoch: 480 [192768/225000 (86%)] Loss: 6718.966797\n",
      "Train Epoch: 480 [196864/225000 (87%)] Loss: 6656.447266\n",
      "Train Epoch: 480 [200960/225000 (89%)] Loss: 6647.222656\n",
      "Train Epoch: 480 [205056/225000 (91%)] Loss: 6834.955078\n",
      "Train Epoch: 480 [209152/225000 (93%)] Loss: 6745.099609\n",
      "Train Epoch: 480 [213248/225000 (95%)] Loss: 6704.613281\n",
      "Train Epoch: 480 [217344/225000 (97%)] Loss: 6674.529297\n",
      "Train Epoch: 480 [221440/225000 (98%)] Loss: 6776.591797\n",
      "    epoch          : 480\n",
      "    loss           : 6700.3156485530435\n",
      "    val_loss       : 6721.326039907884\n",
      "Train Epoch: 481 [256/225000 (0%)] Loss: 6692.818359\n",
      "Train Epoch: 481 [4352/225000 (2%)] Loss: 6727.734375\n",
      "Train Epoch: 481 [8448/225000 (4%)] Loss: 6626.660156\n",
      "Train Epoch: 481 [12544/225000 (6%)] Loss: 6705.449219\n",
      "Train Epoch: 481 [16640/225000 (7%)] Loss: 6783.390625\n",
      "Train Epoch: 481 [20736/225000 (9%)] Loss: 6616.976562\n",
      "Train Epoch: 481 [24832/225000 (11%)] Loss: 6568.546875\n",
      "Train Epoch: 481 [28928/225000 (13%)] Loss: 6779.882812\n",
      "Train Epoch: 481 [33024/225000 (15%)] Loss: 6613.750000\n",
      "Train Epoch: 481 [37120/225000 (16%)] Loss: 6668.826172\n",
      "Train Epoch: 481 [41216/225000 (18%)] Loss: 6719.595703\n",
      "Train Epoch: 481 [45312/225000 (20%)] Loss: 6683.548828\n",
      "Train Epoch: 481 [49408/225000 (22%)] Loss: 6537.484375\n",
      "Train Epoch: 481 [53504/225000 (24%)] Loss: 6570.300781\n",
      "Train Epoch: 481 [57600/225000 (26%)] Loss: 6716.296875\n",
      "Train Epoch: 481 [61696/225000 (27%)] Loss: 6639.654297\n",
      "Train Epoch: 481 [65792/225000 (29%)] Loss: 6627.857422\n",
      "Train Epoch: 481 [69888/225000 (31%)] Loss: 6589.404297\n",
      "Train Epoch: 481 [73984/225000 (33%)] Loss: 6711.257812\n",
      "Train Epoch: 481 [78080/225000 (35%)] Loss: 6519.365234\n",
      "Train Epoch: 481 [82176/225000 (37%)] Loss: 6795.218750\n",
      "Train Epoch: 481 [86272/225000 (38%)] Loss: 6601.550781\n",
      "Train Epoch: 481 [90368/225000 (40%)] Loss: 6674.519531\n",
      "Train Epoch: 481 [94464/225000 (42%)] Loss: 6723.023438\n",
      "Train Epoch: 481 [98560/225000 (44%)] Loss: 6622.439453\n",
      "Train Epoch: 481 [102656/225000 (46%)] Loss: 6870.083984\n",
      "Train Epoch: 481 [106752/225000 (47%)] Loss: 6767.996094\n",
      "Train Epoch: 481 [110848/225000 (49%)] Loss: 6697.406250\n",
      "Train Epoch: 481 [114944/225000 (51%)] Loss: 6718.425781\n",
      "Train Epoch: 481 [119040/225000 (53%)] Loss: 6730.804688\n",
      "Train Epoch: 481 [123136/225000 (55%)] Loss: 6697.054688\n",
      "Train Epoch: 481 [127232/225000 (57%)] Loss: 6637.718750\n",
      "Train Epoch: 481 [131328/225000 (58%)] Loss: 6711.316406\n",
      "Train Epoch: 481 [135424/225000 (60%)] Loss: 6675.056641\n",
      "Train Epoch: 481 [139520/225000 (62%)] Loss: 6648.863281\n",
      "Train Epoch: 481 [143616/225000 (64%)] Loss: 6629.398438\n",
      "Train Epoch: 481 [147712/225000 (66%)] Loss: 6693.679688\n",
      "Train Epoch: 481 [151808/225000 (67%)] Loss: 6481.212891\n",
      "Train Epoch: 481 [155904/225000 (69%)] Loss: 6768.218750\n",
      "Train Epoch: 481 [160000/225000 (71%)] Loss: 6713.744141\n",
      "Train Epoch: 481 [164096/225000 (73%)] Loss: 6642.441406\n",
      "Train Epoch: 481 [168192/225000 (75%)] Loss: 6650.316406\n",
      "Train Epoch: 481 [172288/225000 (77%)] Loss: 6650.826172\n",
      "Train Epoch: 481 [176384/225000 (78%)] Loss: 6619.007812\n",
      "Train Epoch: 481 [180480/225000 (80%)] Loss: 6652.333984\n",
      "Train Epoch: 481 [184576/225000 (82%)] Loss: 6694.941406\n",
      "Train Epoch: 481 [188672/225000 (84%)] Loss: 6855.623047\n",
      "Train Epoch: 481 [192768/225000 (86%)] Loss: 6761.007812\n",
      "Train Epoch: 481 [196864/225000 (87%)] Loss: 6693.365234\n",
      "Train Epoch: 481 [200960/225000 (89%)] Loss: 6742.371094\n",
      "Train Epoch: 481 [205056/225000 (91%)] Loss: 6650.236328\n",
      "Train Epoch: 481 [209152/225000 (93%)] Loss: 6602.904297\n",
      "Train Epoch: 481 [213248/225000 (95%)] Loss: 6701.591797\n",
      "Train Epoch: 481 [217344/225000 (97%)] Loss: 6768.091797\n",
      "Train Epoch: 481 [221440/225000 (98%)] Loss: 6762.253906\n",
      "    epoch          : 481\n",
      "    loss           : 6727.167418808661\n",
      "    val_loss       : 6721.523969594313\n",
      "Train Epoch: 482 [256/225000 (0%)] Loss: 6712.644531\n",
      "Train Epoch: 482 [4352/225000 (2%)] Loss: 6569.886719\n",
      "Train Epoch: 482 [8448/225000 (4%)] Loss: 6653.583984\n",
      "Train Epoch: 482 [12544/225000 (6%)] Loss: 6624.712891\n",
      "Train Epoch: 482 [16640/225000 (7%)] Loss: 6707.367188\n",
      "Train Epoch: 482 [20736/225000 (9%)] Loss: 6737.742188\n",
      "Train Epoch: 482 [24832/225000 (11%)] Loss: 6612.425781\n",
      "Train Epoch: 482 [28928/225000 (13%)] Loss: 6699.216797\n",
      "Train Epoch: 482 [33024/225000 (15%)] Loss: 6587.275391\n",
      "Train Epoch: 482 [37120/225000 (16%)] Loss: 6724.554688\n",
      "Train Epoch: 482 [41216/225000 (18%)] Loss: 6677.847656\n",
      "Train Epoch: 482 [45312/225000 (20%)] Loss: 6731.486328\n",
      "Train Epoch: 482 [49408/225000 (22%)] Loss: 6663.550781\n",
      "Train Epoch: 482 [53504/225000 (24%)] Loss: 6663.755859\n",
      "Train Epoch: 482 [57600/225000 (26%)] Loss: 6756.193359\n",
      "Train Epoch: 482 [61696/225000 (27%)] Loss: 6609.970703\n",
      "Train Epoch: 482 [65792/225000 (29%)] Loss: 6867.746094\n",
      "Train Epoch: 482 [69888/225000 (31%)] Loss: 6473.406250\n",
      "Train Epoch: 482 [73984/225000 (33%)] Loss: 6812.347656\n",
      "Train Epoch: 482 [78080/225000 (35%)] Loss: 6709.736328\n",
      "Train Epoch: 482 [82176/225000 (37%)] Loss: 6608.869141\n",
      "Train Epoch: 482 [86272/225000 (38%)] Loss: 6883.423828\n",
      "Train Epoch: 482 [90368/225000 (40%)] Loss: 6669.814453\n",
      "Train Epoch: 482 [94464/225000 (42%)] Loss: 6647.613281\n",
      "Train Epoch: 482 [98560/225000 (44%)] Loss: 6542.101562\n",
      "Train Epoch: 482 [102656/225000 (46%)] Loss: 6840.677734\n",
      "Train Epoch: 482 [106752/225000 (47%)] Loss: 6683.472656\n",
      "Train Epoch: 482 [110848/225000 (49%)] Loss: 6759.476562\n",
      "Train Epoch: 482 [114944/225000 (51%)] Loss: 6658.599609\n",
      "Train Epoch: 482 [119040/225000 (53%)] Loss: 6497.001953\n",
      "Train Epoch: 482 [123136/225000 (55%)] Loss: 6723.757812\n",
      "Train Epoch: 482 [127232/225000 (57%)] Loss: 6824.421875\n",
      "Train Epoch: 482 [131328/225000 (58%)] Loss: 6761.679688\n",
      "Train Epoch: 482 [135424/225000 (60%)] Loss: 6743.126953\n",
      "Train Epoch: 482 [139520/225000 (62%)] Loss: 6757.416016\n",
      "Train Epoch: 482 [143616/225000 (64%)] Loss: 6563.869141\n",
      "Train Epoch: 482 [147712/225000 (66%)] Loss: 6713.183594\n",
      "Train Epoch: 482 [151808/225000 (67%)] Loss: 6731.412109\n",
      "Train Epoch: 482 [155904/225000 (69%)] Loss: 6757.166016\n",
      "Train Epoch: 482 [160000/225000 (71%)] Loss: 6776.501953\n",
      "Train Epoch: 482 [164096/225000 (73%)] Loss: 6653.093750\n",
      "Train Epoch: 482 [168192/225000 (75%)] Loss: 6682.646484\n",
      "Train Epoch: 482 [172288/225000 (77%)] Loss: 6589.802734\n",
      "Train Epoch: 482 [176384/225000 (78%)] Loss: 6754.027344\n",
      "Train Epoch: 482 [180480/225000 (80%)] Loss: 6622.048828\n",
      "Train Epoch: 482 [184576/225000 (82%)] Loss: 6757.429688\n",
      "Train Epoch: 482 [188672/225000 (84%)] Loss: 6652.988281\n",
      "Train Epoch: 482 [192768/225000 (86%)] Loss: 6791.890625\n",
      "Train Epoch: 482 [196864/225000 (87%)] Loss: 6570.437500\n",
      "Train Epoch: 482 [200960/225000 (89%)] Loss: 6757.353516\n",
      "Train Epoch: 482 [205056/225000 (91%)] Loss: 6697.509766\n",
      "Train Epoch: 482 [209152/225000 (93%)] Loss: 6683.283203\n",
      "Train Epoch: 482 [213248/225000 (95%)] Loss: 6652.039062\n",
      "Train Epoch: 482 [217344/225000 (97%)] Loss: 6690.222656\n",
      "Train Epoch: 482 [221440/225000 (98%)] Loss: 6787.814453\n",
      "    epoch          : 482\n",
      "    loss           : 6704.127974127205\n",
      "    val_loss       : 6720.929939476811\n",
      "Train Epoch: 483 [256/225000 (0%)] Loss: 6676.203125\n",
      "Train Epoch: 483 [4352/225000 (2%)] Loss: 6663.927734\n",
      "Train Epoch: 483 [8448/225000 (4%)] Loss: 6757.556641\n",
      "Train Epoch: 483 [12544/225000 (6%)] Loss: 6939.509766\n",
      "Train Epoch: 483 [16640/225000 (7%)] Loss: 6774.935547\n",
      "Train Epoch: 483 [20736/225000 (9%)] Loss: 6609.142578\n",
      "Train Epoch: 483 [24832/225000 (11%)] Loss: 6643.351562\n",
      "Train Epoch: 483 [28928/225000 (13%)] Loss: 6554.560547\n",
      "Train Epoch: 483 [33024/225000 (15%)] Loss: 6752.150391\n",
      "Train Epoch: 483 [37120/225000 (16%)] Loss: 6628.626953\n",
      "Train Epoch: 483 [41216/225000 (18%)] Loss: 6822.728516\n",
      "Train Epoch: 483 [45312/225000 (20%)] Loss: 6656.306641\n",
      "Train Epoch: 483 [49408/225000 (22%)] Loss: 6595.816406\n",
      "Train Epoch: 483 [53504/225000 (24%)] Loss: 6697.853516\n",
      "Train Epoch: 483 [57600/225000 (26%)] Loss: 6577.974609\n",
      "Train Epoch: 483 [61696/225000 (27%)] Loss: 6614.005859\n",
      "Train Epoch: 483 [65792/225000 (29%)] Loss: 6742.050781\n",
      "Train Epoch: 483 [69888/225000 (31%)] Loss: 6685.044922\n",
      "Train Epoch: 483 [73984/225000 (33%)] Loss: 6661.617188\n",
      "Train Epoch: 483 [78080/225000 (35%)] Loss: 6805.128906\n",
      "Train Epoch: 483 [82176/225000 (37%)] Loss: 6698.937500\n",
      "Train Epoch: 483 [86272/225000 (38%)] Loss: 6588.505859\n",
      "Train Epoch: 483 [90368/225000 (40%)] Loss: 6862.949219\n",
      "Train Epoch: 483 [94464/225000 (42%)] Loss: 6682.289062\n",
      "Train Epoch: 483 [98560/225000 (44%)] Loss: 6705.806641\n",
      "Train Epoch: 483 [102656/225000 (46%)] Loss: 6636.894531\n",
      "Train Epoch: 483 [106752/225000 (47%)] Loss: 6609.195312\n",
      "Train Epoch: 483 [110848/225000 (49%)] Loss: 6857.058594\n",
      "Train Epoch: 483 [114944/225000 (51%)] Loss: 6678.763672\n",
      "Train Epoch: 483 [119040/225000 (53%)] Loss: 6642.832031\n",
      "Train Epoch: 483 [123136/225000 (55%)] Loss: 6698.050781\n",
      "Train Epoch: 483 [127232/225000 (57%)] Loss: 6678.951172\n",
      "Train Epoch: 483 [131328/225000 (58%)] Loss: 6774.626953\n",
      "Train Epoch: 483 [135424/225000 (60%)] Loss: 6717.611328\n",
      "Train Epoch: 483 [139520/225000 (62%)] Loss: 6667.419922\n",
      "Train Epoch: 483 [143616/225000 (64%)] Loss: 6796.683594\n",
      "Train Epoch: 483 [147712/225000 (66%)] Loss: 6640.496094\n",
      "Train Epoch: 483 [151808/225000 (67%)] Loss: 6880.398438\n",
      "Train Epoch: 483 [155904/225000 (69%)] Loss: 6753.626953\n",
      "Train Epoch: 483 [160000/225000 (71%)] Loss: 6565.689453\n",
      "Train Epoch: 483 [164096/225000 (73%)] Loss: 6546.837891\n",
      "Train Epoch: 483 [168192/225000 (75%)] Loss: 6608.007812\n",
      "Train Epoch: 483 [172288/225000 (77%)] Loss: 6748.396484\n",
      "Train Epoch: 483 [176384/225000 (78%)] Loss: 6626.208984\n",
      "Train Epoch: 483 [180480/225000 (80%)] Loss: 6769.328125\n",
      "Train Epoch: 483 [184576/225000 (82%)] Loss: 6709.812500\n",
      "Train Epoch: 483 [188672/225000 (84%)] Loss: 6730.125000\n",
      "Train Epoch: 483 [192768/225000 (86%)] Loss: 6694.951172\n",
      "Train Epoch: 483 [196864/225000 (87%)] Loss: 6794.509766\n",
      "Train Epoch: 483 [200960/225000 (89%)] Loss: 6692.728516\n",
      "Train Epoch: 483 [205056/225000 (91%)] Loss: 6842.927734\n",
      "Train Epoch: 483 [209152/225000 (93%)] Loss: 6683.314453\n",
      "Train Epoch: 483 [213248/225000 (95%)] Loss: 6868.724609\n",
      "Train Epoch: 483 [217344/225000 (97%)] Loss: 6716.763672\n",
      "Train Epoch: 483 [221440/225000 (98%)] Loss: 6686.443359\n",
      "    epoch          : 483\n",
      "    loss           : 6727.50773584151\n",
      "    val_loss       : 6720.621258241789\n",
      "Train Epoch: 484 [256/225000 (0%)] Loss: 6572.621094\n",
      "Train Epoch: 484 [4352/225000 (2%)] Loss: 6684.080078\n",
      "Train Epoch: 484 [8448/225000 (4%)] Loss: 6764.259766\n",
      "Train Epoch: 484 [12544/225000 (6%)] Loss: 6517.500000\n",
      "Train Epoch: 484 [16640/225000 (7%)] Loss: 6775.947266\n",
      "Train Epoch: 484 [20736/225000 (9%)] Loss: 6742.882812\n",
      "Train Epoch: 484 [24832/225000 (11%)] Loss: 6558.087891\n",
      "Train Epoch: 484 [28928/225000 (13%)] Loss: 6633.423828\n",
      "Train Epoch: 484 [33024/225000 (15%)] Loss: 6718.541016\n",
      "Train Epoch: 484 [37120/225000 (16%)] Loss: 6584.494141\n",
      "Train Epoch: 484 [41216/225000 (18%)] Loss: 6514.470703\n",
      "Train Epoch: 484 [45312/225000 (20%)] Loss: 6546.511719\n",
      "Train Epoch: 484 [49408/225000 (22%)] Loss: 6758.085938\n",
      "Train Epoch: 484 [53504/225000 (24%)] Loss: 6728.529297\n",
      "Train Epoch: 484 [57600/225000 (26%)] Loss: 6570.478516\n",
      "Train Epoch: 484 [61696/225000 (27%)] Loss: 6650.386719\n",
      "Train Epoch: 484 [65792/225000 (29%)] Loss: 6705.353516\n",
      "Train Epoch: 484 [69888/225000 (31%)] Loss: 6617.478516\n",
      "Train Epoch: 484 [73984/225000 (33%)] Loss: 6735.777344\n",
      "Train Epoch: 484 [78080/225000 (35%)] Loss: 6700.425781\n",
      "Train Epoch: 484 [82176/225000 (37%)] Loss: 6628.111328\n",
      "Train Epoch: 484 [86272/225000 (38%)] Loss: 6602.664062\n",
      "Train Epoch: 484 [90368/225000 (40%)] Loss: 6644.398438\n",
      "Train Epoch: 484 [94464/225000 (42%)] Loss: 6710.851562\n",
      "Train Epoch: 484 [98560/225000 (44%)] Loss: 6725.496094\n",
      "Train Epoch: 484 [102656/225000 (46%)] Loss: 6744.650391\n",
      "Train Epoch: 484 [106752/225000 (47%)] Loss: 6648.779297\n",
      "Train Epoch: 484 [110848/225000 (49%)] Loss: 6632.195312\n",
      "Train Epoch: 484 [114944/225000 (51%)] Loss: 6736.398438\n",
      "Train Epoch: 484 [119040/225000 (53%)] Loss: 6721.097656\n",
      "Train Epoch: 484 [123136/225000 (55%)] Loss: 6738.794922\n",
      "Train Epoch: 484 [127232/225000 (57%)] Loss: 6813.640625\n",
      "Train Epoch: 484 [131328/225000 (58%)] Loss: 6799.275391\n",
      "Train Epoch: 484 [135424/225000 (60%)] Loss: 6825.923828\n",
      "Train Epoch: 484 [139520/225000 (62%)] Loss: 6660.289062\n",
      "Train Epoch: 484 [143616/225000 (64%)] Loss: 6725.138672\n",
      "Train Epoch: 484 [147712/225000 (66%)] Loss: 6626.152344\n",
      "Train Epoch: 484 [151808/225000 (67%)] Loss: 6683.384766\n",
      "Train Epoch: 484 [155904/225000 (69%)] Loss: 6694.935547\n",
      "Train Epoch: 484 [160000/225000 (71%)] Loss: 6715.259766\n",
      "Train Epoch: 484 [164096/225000 (73%)] Loss: 6659.970703\n",
      "Train Epoch: 484 [168192/225000 (75%)] Loss: 6595.527344\n",
      "Train Epoch: 484 [172288/225000 (77%)] Loss: 6650.058594\n",
      "Train Epoch: 484 [176384/225000 (78%)] Loss: 6793.828125\n",
      "Train Epoch: 484 [180480/225000 (80%)] Loss: 6741.304688\n",
      "Train Epoch: 484 [184576/225000 (82%)] Loss: 6749.814453\n",
      "Train Epoch: 484 [188672/225000 (84%)] Loss: 6636.445312\n",
      "Train Epoch: 484 [192768/225000 (86%)] Loss: 6746.710938\n",
      "Train Epoch: 484 [196864/225000 (87%)] Loss: 6839.884766\n",
      "Train Epoch: 484 [200960/225000 (89%)] Loss: 6674.367188\n",
      "Train Epoch: 484 [205056/225000 (91%)] Loss: 6650.021484\n",
      "Train Epoch: 484 [209152/225000 (93%)] Loss: 6632.167969\n",
      "Train Epoch: 484 [213248/225000 (95%)] Loss: 6651.349609\n",
      "Train Epoch: 484 [217344/225000 (97%)] Loss: 6763.308594\n",
      "Train Epoch: 484 [221440/225000 (98%)] Loss: 6695.664062\n",
      "    epoch          : 484\n",
      "    loss           : 6713.640656107793\n",
      "    val_loss       : 6720.984110384571\n",
      "Train Epoch: 485 [256/225000 (0%)] Loss: 6720.910156\n",
      "Train Epoch: 485 [4352/225000 (2%)] Loss: 6559.345703\n",
      "Train Epoch: 485 [8448/225000 (4%)] Loss: 6879.332031\n",
      "Train Epoch: 485 [12544/225000 (6%)] Loss: 6702.417969\n",
      "Train Epoch: 485 [16640/225000 (7%)] Loss: 6622.988281\n",
      "Train Epoch: 485 [20736/225000 (9%)] Loss: 6869.007812\n",
      "Train Epoch: 485 [24832/225000 (11%)] Loss: 6793.519531\n",
      "Train Epoch: 485 [28928/225000 (13%)] Loss: 6688.998047\n",
      "Train Epoch: 485 [33024/225000 (15%)] Loss: 6608.636719\n",
      "Train Epoch: 485 [37120/225000 (16%)] Loss: 6805.289062\n",
      "Train Epoch: 485 [41216/225000 (18%)] Loss: 6872.939453\n",
      "Train Epoch: 485 [45312/225000 (20%)] Loss: 6711.669922\n",
      "Train Epoch: 485 [49408/225000 (22%)] Loss: 6677.369141\n",
      "Train Epoch: 485 [53504/225000 (24%)] Loss: 6792.275391\n",
      "Train Epoch: 485 [57600/225000 (26%)] Loss: 6778.253906\n",
      "Train Epoch: 485 [61696/225000 (27%)] Loss: 6626.050781\n",
      "Train Epoch: 485 [65792/225000 (29%)] Loss: 6672.484375\n",
      "Train Epoch: 485 [69888/225000 (31%)] Loss: 6677.042969\n",
      "Train Epoch: 485 [73984/225000 (33%)] Loss: 6746.630859\n",
      "Train Epoch: 485 [78080/225000 (35%)] Loss: 6680.376953\n",
      "Train Epoch: 485 [82176/225000 (37%)] Loss: 6662.519531\n",
      "Train Epoch: 485 [86272/225000 (38%)] Loss: 6614.906250\n",
      "Train Epoch: 485 [90368/225000 (40%)] Loss: 6726.671875\n",
      "Train Epoch: 485 [94464/225000 (42%)] Loss: 6750.578125\n",
      "Train Epoch: 485 [98560/225000 (44%)] Loss: 6750.632812\n",
      "Train Epoch: 485 [102656/225000 (46%)] Loss: 6679.736328\n",
      "Train Epoch: 485 [106752/225000 (47%)] Loss: 6624.753906\n",
      "Train Epoch: 485 [110848/225000 (49%)] Loss: 6788.666016\n",
      "Train Epoch: 485 [114944/225000 (51%)] Loss: 6772.103516\n",
      "Train Epoch: 485 [119040/225000 (53%)] Loss: 6563.466797\n",
      "Train Epoch: 485 [123136/225000 (55%)] Loss: 6668.431641\n",
      "Train Epoch: 485 [127232/225000 (57%)] Loss: 6636.283203\n",
      "Train Epoch: 485 [131328/225000 (58%)] Loss: 6839.945312\n",
      "Train Epoch: 485 [135424/225000 (60%)] Loss: 6837.564453\n",
      "Train Epoch: 485 [139520/225000 (62%)] Loss: 6761.109375\n",
      "Train Epoch: 485 [143616/225000 (64%)] Loss: 6724.125000\n",
      "Train Epoch: 485 [147712/225000 (66%)] Loss: 6716.822266\n",
      "Train Epoch: 485 [151808/225000 (67%)] Loss: 6530.980469\n",
      "Train Epoch: 485 [155904/225000 (69%)] Loss: 6573.058594\n",
      "Train Epoch: 485 [160000/225000 (71%)] Loss: 6579.396484\n",
      "Train Epoch: 485 [164096/225000 (73%)] Loss: 6666.128906\n",
      "Train Epoch: 485 [168192/225000 (75%)] Loss: 6720.453125\n",
      "Train Epoch: 485 [172288/225000 (77%)] Loss: 6654.794922\n",
      "Train Epoch: 485 [176384/225000 (78%)] Loss: 6667.156250\n",
      "Train Epoch: 485 [180480/225000 (80%)] Loss: 6871.726562\n",
      "Train Epoch: 485 [184576/225000 (82%)] Loss: 6791.082031\n",
      "Train Epoch: 485 [188672/225000 (84%)] Loss: 6668.011719\n",
      "Train Epoch: 485 [192768/225000 (86%)] Loss: 6578.195312\n",
      "Train Epoch: 485 [196864/225000 (87%)] Loss: 6611.673828\n",
      "Train Epoch: 485 [200960/225000 (89%)] Loss: 6820.390625\n",
      "Train Epoch: 485 [205056/225000 (91%)] Loss: 6797.353516\n",
      "Train Epoch: 485 [209152/225000 (93%)] Loss: 6604.597656\n",
      "Train Epoch: 485 [213248/225000 (95%)] Loss: 6679.257812\n",
      "Train Epoch: 485 [217344/225000 (97%)] Loss: 6702.068359\n",
      "Train Epoch: 485 [221440/225000 (98%)] Loss: 6547.076172\n",
      "    epoch          : 485\n",
      "    loss           : 6692.495979317761\n",
      "    val_loss       : 6720.463237626212\n",
      "Train Epoch: 486 [256/225000 (0%)] Loss: 6564.011719\n",
      "Train Epoch: 486 [4352/225000 (2%)] Loss: 6703.960938\n",
      "Train Epoch: 486 [8448/225000 (4%)] Loss: 6766.326172\n",
      "Train Epoch: 486 [12544/225000 (6%)] Loss: 6551.390625\n",
      "Train Epoch: 486 [16640/225000 (7%)] Loss: 6623.949219\n",
      "Train Epoch: 486 [20736/225000 (9%)] Loss: 6594.121094\n",
      "Train Epoch: 486 [24832/225000 (11%)] Loss: 6763.949219\n",
      "Train Epoch: 486 [28928/225000 (13%)] Loss: 6664.525391\n",
      "Train Epoch: 486 [33024/225000 (15%)] Loss: 6690.341797\n",
      "Train Epoch: 486 [37120/225000 (16%)] Loss: 6558.316406\n",
      "Train Epoch: 486 [41216/225000 (18%)] Loss: 6644.695312\n",
      "Train Epoch: 486 [45312/225000 (20%)] Loss: 6688.148438\n",
      "Train Epoch: 486 [49408/225000 (22%)] Loss: 6640.861328\n",
      "Train Epoch: 486 [53504/225000 (24%)] Loss: 6726.158203\n",
      "Train Epoch: 486 [57600/225000 (26%)] Loss: 6691.410156\n",
      "Train Epoch: 486 [61696/225000 (27%)] Loss: 6774.433594\n",
      "Train Epoch: 486 [65792/225000 (29%)] Loss: 6678.474609\n",
      "Train Epoch: 486 [69888/225000 (31%)] Loss: 6683.304688\n",
      "Train Epoch: 486 [73984/225000 (33%)] Loss: 6650.089844\n",
      "Train Epoch: 486 [78080/225000 (35%)] Loss: 6586.851562\n",
      "Train Epoch: 486 [82176/225000 (37%)] Loss: 6712.457031\n",
      "Train Epoch: 486 [86272/225000 (38%)] Loss: 6641.986328\n",
      "Train Epoch: 486 [90368/225000 (40%)] Loss: 6990.257812\n",
      "Train Epoch: 486 [94464/225000 (42%)] Loss: 6719.599609\n",
      "Train Epoch: 486 [98560/225000 (44%)] Loss: 6627.332031\n",
      "Train Epoch: 486 [102656/225000 (46%)] Loss: 6718.878906\n",
      "Train Epoch: 486 [106752/225000 (47%)] Loss: 6646.123047\n",
      "Train Epoch: 486 [110848/225000 (49%)] Loss: 6588.923828\n",
      "Train Epoch: 486 [114944/225000 (51%)] Loss: 6656.820312\n",
      "Train Epoch: 486 [119040/225000 (53%)] Loss: 6759.572266\n",
      "Train Epoch: 486 [123136/225000 (55%)] Loss: 6646.095703\n",
      "Train Epoch: 486 [127232/225000 (57%)] Loss: 6737.640625\n",
      "Train Epoch: 486 [131328/225000 (58%)] Loss: 6701.978516\n",
      "Train Epoch: 486 [135424/225000 (60%)] Loss: 6783.511719\n",
      "Train Epoch: 486 [139520/225000 (62%)] Loss: 6734.828125\n",
      "Train Epoch: 486 [143616/225000 (64%)] Loss: 6549.134766\n",
      "Train Epoch: 486 [147712/225000 (66%)] Loss: 6645.578125\n",
      "Train Epoch: 486 [151808/225000 (67%)] Loss: 6762.552734\n",
      "Train Epoch: 486 [155904/225000 (69%)] Loss: 6780.542969\n",
      "Train Epoch: 486 [160000/225000 (71%)] Loss: 6706.533203\n",
      "Train Epoch: 486 [164096/225000 (73%)] Loss: 6631.212891\n",
      "Train Epoch: 486 [168192/225000 (75%)] Loss: 6725.568359\n",
      "Train Epoch: 486 [172288/225000 (77%)] Loss: 6717.593750\n",
      "Train Epoch: 486 [176384/225000 (78%)] Loss: 6707.689453\n",
      "Train Epoch: 486 [180480/225000 (80%)] Loss: 6749.865234\n",
      "Train Epoch: 486 [184576/225000 (82%)] Loss: 6759.984375\n",
      "Train Epoch: 486 [188672/225000 (84%)] Loss: 6841.062500\n",
      "Train Epoch: 486 [192768/225000 (86%)] Loss: 6857.318359\n",
      "Train Epoch: 486 [196864/225000 (87%)] Loss: 6830.322266\n",
      "Train Epoch: 486 [200960/225000 (89%)] Loss: 6647.453125\n",
      "Train Epoch: 486 [205056/225000 (91%)] Loss: 6736.287109\n",
      "Train Epoch: 486 [209152/225000 (93%)] Loss: 6661.220703\n",
      "Train Epoch: 486 [213248/225000 (95%)] Loss: 6578.587891\n",
      "Train Epoch: 486 [217344/225000 (97%)] Loss: 6767.953125\n",
      "Train Epoch: 486 [221440/225000 (98%)] Loss: 6606.031250\n",
      "    epoch          : 486\n",
      "    loss           : 6725.212481779721\n",
      "    val_loss       : 6720.759981041052\n",
      "Train Epoch: 487 [256/225000 (0%)] Loss: 6673.833984\n",
      "Train Epoch: 487 [4352/225000 (2%)] Loss: 6616.185547\n",
      "Train Epoch: 487 [8448/225000 (4%)] Loss: 6743.992188\n",
      "Train Epoch: 487 [12544/225000 (6%)] Loss: 6778.191406\n",
      "Train Epoch: 487 [16640/225000 (7%)] Loss: 6731.570312\n",
      "Train Epoch: 487 [20736/225000 (9%)] Loss: 6565.037109\n",
      "Train Epoch: 487 [24832/225000 (11%)] Loss: 6697.416016\n",
      "Train Epoch: 487 [28928/225000 (13%)] Loss: 6620.828125\n",
      "Train Epoch: 487 [33024/225000 (15%)] Loss: 6590.601562\n",
      "Train Epoch: 487 [37120/225000 (16%)] Loss: 6608.285156\n",
      "Train Epoch: 487 [41216/225000 (18%)] Loss: 6518.693359\n",
      "Train Epoch: 487 [45312/225000 (20%)] Loss: 6713.189453\n",
      "Train Epoch: 487 [49408/225000 (22%)] Loss: 6822.796875\n",
      "Train Epoch: 487 [53504/225000 (24%)] Loss: 6564.083984\n",
      "Train Epoch: 487 [57600/225000 (26%)] Loss: 6645.595703\n",
      "Train Epoch: 487 [61696/225000 (27%)] Loss: 6615.992188\n",
      "Train Epoch: 487 [65792/225000 (29%)] Loss: 6675.740234\n",
      "Train Epoch: 487 [69888/225000 (31%)] Loss: 6640.158203\n",
      "Train Epoch: 487 [73984/225000 (33%)] Loss: 6534.416016\n",
      "Train Epoch: 487 [78080/225000 (35%)] Loss: 6676.792969\n",
      "Train Epoch: 487 [82176/225000 (37%)] Loss: 6576.955078\n",
      "Train Epoch: 487 [86272/225000 (38%)] Loss: 6653.882812\n",
      "Train Epoch: 487 [90368/225000 (40%)] Loss: 6677.568359\n",
      "Train Epoch: 487 [94464/225000 (42%)] Loss: 6650.177734\n",
      "Train Epoch: 487 [98560/225000 (44%)] Loss: 6755.648438\n",
      "Train Epoch: 487 [102656/225000 (46%)] Loss: 6670.267578\n",
      "Train Epoch: 487 [106752/225000 (47%)] Loss: 6642.294922\n",
      "Train Epoch: 487 [110848/225000 (49%)] Loss: 6696.675781\n",
      "Train Epoch: 487 [114944/225000 (51%)] Loss: 6726.443359\n",
      "Train Epoch: 487 [119040/225000 (53%)] Loss: 6790.195312\n",
      "Train Epoch: 487 [123136/225000 (55%)] Loss: 6674.953125\n",
      "Train Epoch: 487 [127232/225000 (57%)] Loss: 6520.121094\n",
      "Train Epoch: 487 [131328/225000 (58%)] Loss: 6643.923828\n",
      "Train Epoch: 487 [135424/225000 (60%)] Loss: 6673.355469\n",
      "Train Epoch: 487 [139520/225000 (62%)] Loss: 6756.599609\n",
      "Train Epoch: 487 [143616/225000 (64%)] Loss: 6615.812500\n",
      "Train Epoch: 487 [147712/225000 (66%)] Loss: 6762.230469\n",
      "Train Epoch: 487 [151808/225000 (67%)] Loss: 6733.308594\n",
      "Train Epoch: 487 [155904/225000 (69%)] Loss: 6732.173828\n",
      "Train Epoch: 487 [160000/225000 (71%)] Loss: 6749.974609\n",
      "Train Epoch: 487 [164096/225000 (73%)] Loss: 6731.232422\n",
      "Train Epoch: 487 [168192/225000 (75%)] Loss: 6848.447266\n",
      "Train Epoch: 487 [172288/225000 (77%)] Loss: 6659.773438\n",
      "Train Epoch: 487 [176384/225000 (78%)] Loss: 6598.759766\n",
      "Train Epoch: 487 [180480/225000 (80%)] Loss: 6717.630859\n",
      "Train Epoch: 487 [184576/225000 (82%)] Loss: 6615.693359\n",
      "Train Epoch: 487 [188672/225000 (84%)] Loss: 6665.519531\n",
      "Train Epoch: 487 [192768/225000 (86%)] Loss: 6642.197266\n",
      "Train Epoch: 487 [196864/225000 (87%)] Loss: 6677.716797\n",
      "Train Epoch: 487 [200960/225000 (89%)] Loss: 6713.130859\n",
      "Train Epoch: 487 [205056/225000 (91%)] Loss: 6747.277344\n",
      "Train Epoch: 487 [209152/225000 (93%)] Loss: 6699.890625\n",
      "Train Epoch: 487 [213248/225000 (95%)] Loss: 6613.542969\n",
      "Train Epoch: 487 [217344/225000 (97%)] Loss: 6658.619141\n",
      "Train Epoch: 487 [221440/225000 (98%)] Loss: 6683.578125\n",
      "    epoch          : 487\n",
      "    loss           : 6727.043079849261\n",
      "    val_loss       : 6720.291580793809\n",
      "Train Epoch: 488 [256/225000 (0%)] Loss: 6622.587891\n",
      "Train Epoch: 488 [4352/225000 (2%)] Loss: 6668.986328\n",
      "Train Epoch: 488 [8448/225000 (4%)] Loss: 6796.705078\n",
      "Train Epoch: 488 [12544/225000 (6%)] Loss: 6789.466797\n",
      "Train Epoch: 488 [16640/225000 (7%)] Loss: 6777.494141\n",
      "Train Epoch: 488 [20736/225000 (9%)] Loss: 6788.974609\n",
      "Train Epoch: 488 [24832/225000 (11%)] Loss: 6680.892578\n",
      "Train Epoch: 488 [28928/225000 (13%)] Loss: 6730.267578\n",
      "Train Epoch: 488 [33024/225000 (15%)] Loss: 6821.638672\n",
      "Train Epoch: 488 [37120/225000 (16%)] Loss: 6693.587891\n",
      "Train Epoch: 488 [41216/225000 (18%)] Loss: 6928.880859\n",
      "Train Epoch: 488 [45312/225000 (20%)] Loss: 6702.732422\n",
      "Train Epoch: 488 [49408/225000 (22%)] Loss: 6637.263672\n",
      "Train Epoch: 488 [53504/225000 (24%)] Loss: 6603.375000\n",
      "Train Epoch: 488 [57600/225000 (26%)] Loss: 6680.974609\n",
      "Train Epoch: 488 [61696/225000 (27%)] Loss: 6665.509766\n",
      "Train Epoch: 488 [65792/225000 (29%)] Loss: 6669.623047\n",
      "Train Epoch: 488 [69888/225000 (31%)] Loss: 6668.513672\n",
      "Train Epoch: 488 [73984/225000 (33%)] Loss: 6594.201172\n",
      "Train Epoch: 488 [78080/225000 (35%)] Loss: 6748.742188\n",
      "Train Epoch: 488 [82176/225000 (37%)] Loss: 6629.185547\n",
      "Train Epoch: 488 [86272/225000 (38%)] Loss: 6881.673828\n",
      "Train Epoch: 488 [90368/225000 (40%)] Loss: 6720.443359\n",
      "Train Epoch: 488 [94464/225000 (42%)] Loss: 6657.482422\n",
      "Train Epoch: 488 [98560/225000 (44%)] Loss: 6655.990234\n",
      "Train Epoch: 488 [102656/225000 (46%)] Loss: 6618.351562\n",
      "Train Epoch: 488 [106752/225000 (47%)] Loss: 6727.585938\n",
      "Train Epoch: 488 [110848/225000 (49%)] Loss: 6609.722656\n",
      "Train Epoch: 488 [114944/225000 (51%)] Loss: 6639.998047\n",
      "Train Epoch: 488 [119040/225000 (53%)] Loss: 6676.617188\n",
      "Train Epoch: 488 [123136/225000 (55%)] Loss: 6602.292969\n",
      "Train Epoch: 488 [127232/225000 (57%)] Loss: 6708.582031\n",
      "Train Epoch: 488 [131328/225000 (58%)] Loss: 6764.361328\n",
      "Train Epoch: 488 [135424/225000 (60%)] Loss: 6784.998047\n",
      "Train Epoch: 488 [139520/225000 (62%)] Loss: 6706.984375\n",
      "Train Epoch: 488 [143616/225000 (64%)] Loss: 6723.808594\n",
      "Train Epoch: 488 [147712/225000 (66%)] Loss: 6788.330078\n",
      "Train Epoch: 488 [151808/225000 (67%)] Loss: 6644.830078\n",
      "Train Epoch: 488 [155904/225000 (69%)] Loss: 6625.740234\n",
      "Train Epoch: 488 [160000/225000 (71%)] Loss: 6714.998047\n",
      "Train Epoch: 488 [164096/225000 (73%)] Loss: 6757.361328\n",
      "Train Epoch: 488 [168192/225000 (75%)] Loss: 6663.287109\n",
      "Train Epoch: 488 [172288/225000 (77%)] Loss: 6656.814453\n",
      "Train Epoch: 488 [176384/225000 (78%)] Loss: 6690.531250\n",
      "Train Epoch: 488 [180480/225000 (80%)] Loss: 6860.904297\n",
      "Train Epoch: 488 [184576/225000 (82%)] Loss: 6695.246094\n",
      "Train Epoch: 488 [188672/225000 (84%)] Loss: 6658.804688\n",
      "Train Epoch: 488 [192768/225000 (86%)] Loss: 6617.091797\n",
      "Train Epoch: 488 [196864/225000 (87%)] Loss: 6628.890625\n",
      "Train Epoch: 488 [200960/225000 (89%)] Loss: 6613.166016\n",
      "Train Epoch: 488 [205056/225000 (91%)] Loss: 6805.419922\n",
      "Train Epoch: 488 [209152/225000 (93%)] Loss: 6633.501953\n",
      "Train Epoch: 488 [213248/225000 (95%)] Loss: 6851.882812\n",
      "Train Epoch: 488 [217344/225000 (97%)] Loss: 6704.259766\n",
      "Train Epoch: 488 [221440/225000 (98%)] Loss: 6690.115234\n",
      "    epoch          : 488\n",
      "    loss           : 6740.245158294227\n",
      "    val_loss       : 6720.255061412344\n",
      "Train Epoch: 489 [256/225000 (0%)] Loss: 6713.039062\n",
      "Train Epoch: 489 [4352/225000 (2%)] Loss: 6648.574219\n",
      "Train Epoch: 489 [8448/225000 (4%)] Loss: 6638.347656\n",
      "Train Epoch: 489 [12544/225000 (6%)] Loss: 6660.978516\n",
      "Train Epoch: 489 [16640/225000 (7%)] Loss: 6676.751953\n",
      "Train Epoch: 489 [20736/225000 (9%)] Loss: 6616.894531\n",
      "Train Epoch: 489 [24832/225000 (11%)] Loss: 6651.263672\n",
      "Train Epoch: 489 [28928/225000 (13%)] Loss: 6668.365234\n",
      "Train Epoch: 489 [33024/225000 (15%)] Loss: 6620.730469\n",
      "Train Epoch: 489 [37120/225000 (16%)] Loss: 6838.585938\n",
      "Train Epoch: 489 [41216/225000 (18%)] Loss: 6749.166016\n",
      "Train Epoch: 489 [45312/225000 (20%)] Loss: 6586.882812\n",
      "Train Epoch: 489 [49408/225000 (22%)] Loss: 6626.349609\n",
      "Train Epoch: 489 [53504/225000 (24%)] Loss: 6849.320312\n",
      "Train Epoch: 489 [57600/225000 (26%)] Loss: 6694.660156\n",
      "Train Epoch: 489 [61696/225000 (27%)] Loss: 6733.873047\n",
      "Train Epoch: 489 [65792/225000 (29%)] Loss: 6763.640625\n",
      "Train Epoch: 489 [69888/225000 (31%)] Loss: 6787.757812\n",
      "Train Epoch: 489 [73984/225000 (33%)] Loss: 6800.521484\n",
      "Train Epoch: 489 [78080/225000 (35%)] Loss: 6788.650391\n",
      "Train Epoch: 489 [82176/225000 (37%)] Loss: 6672.562500\n",
      "Train Epoch: 489 [86272/225000 (38%)] Loss: 6766.244141\n",
      "Train Epoch: 489 [90368/225000 (40%)] Loss: 6742.943359\n",
      "Train Epoch: 489 [94464/225000 (42%)] Loss: 6881.568359\n",
      "Train Epoch: 489 [98560/225000 (44%)] Loss: 6573.669922\n",
      "Train Epoch: 489 [102656/225000 (46%)] Loss: 6675.484375\n",
      "Train Epoch: 489 [106752/225000 (47%)] Loss: 6604.029297\n",
      "Train Epoch: 489 [110848/225000 (49%)] Loss: 6646.412109\n",
      "Train Epoch: 489 [114944/225000 (51%)] Loss: 6673.224609\n",
      "Train Epoch: 489 [119040/225000 (53%)] Loss: 6775.925781\n",
      "Train Epoch: 489 [123136/225000 (55%)] Loss: 6627.976562\n",
      "Train Epoch: 489 [127232/225000 (57%)] Loss: 6755.296875\n",
      "Train Epoch: 489 [131328/225000 (58%)] Loss: 6692.552734\n",
      "Train Epoch: 489 [135424/225000 (60%)] Loss: 6670.527344\n",
      "Train Epoch: 489 [139520/225000 (62%)] Loss: 6820.708984\n",
      "Train Epoch: 489 [143616/225000 (64%)] Loss: 6739.658203\n",
      "Train Epoch: 489 [147712/225000 (66%)] Loss: 6606.853516\n",
      "Train Epoch: 489 [151808/225000 (67%)] Loss: 6712.443359\n",
      "Train Epoch: 489 [155904/225000 (69%)] Loss: 6695.990234\n",
      "Train Epoch: 489 [160000/225000 (71%)] Loss: 6777.505859\n",
      "Train Epoch: 489 [164096/225000 (73%)] Loss: 6755.644531\n",
      "Train Epoch: 489 [168192/225000 (75%)] Loss: 6607.597656\n",
      "Train Epoch: 489 [172288/225000 (77%)] Loss: 6695.593750\n",
      "Train Epoch: 489 [176384/225000 (78%)] Loss: 6660.099609\n",
      "Train Epoch: 489 [180480/225000 (80%)] Loss: 6798.171875\n",
      "Train Epoch: 489 [184576/225000 (82%)] Loss: 6545.234375\n",
      "Train Epoch: 489 [188672/225000 (84%)] Loss: 6696.095703\n",
      "Train Epoch: 489 [192768/225000 (86%)] Loss: 6636.261719\n",
      "Train Epoch: 489 [196864/225000 (87%)] Loss: 6702.046875\n",
      "Train Epoch: 489 [200960/225000 (89%)] Loss: 6668.203125\n",
      "Train Epoch: 489 [205056/225000 (91%)] Loss: 6772.609375\n",
      "Train Epoch: 489 [209152/225000 (93%)] Loss: 6629.677734\n",
      "Train Epoch: 489 [213248/225000 (95%)] Loss: 6737.945312\n",
      "Train Epoch: 489 [217344/225000 (97%)] Loss: 6617.384766\n",
      "Train Epoch: 489 [221440/225000 (98%)] Loss: 6627.306641\n",
      "    epoch          : 489\n",
      "    loss           : 6720.5620922657135\n",
      "    val_loss       : 6918.494993915363\n",
      "Train Epoch: 490 [256/225000 (0%)] Loss: 6611.896484\n",
      "Train Epoch: 490 [4352/225000 (2%)] Loss: 6781.203125\n",
      "Train Epoch: 490 [8448/225000 (4%)] Loss: 6738.597656\n",
      "Train Epoch: 490 [12544/225000 (6%)] Loss: 6683.054688\n",
      "Train Epoch: 490 [16640/225000 (7%)] Loss: 6617.857422\n",
      "Train Epoch: 490 [20736/225000 (9%)] Loss: 6675.378906\n",
      "Train Epoch: 490 [24832/225000 (11%)] Loss: 6635.058594\n",
      "Train Epoch: 490 [28928/225000 (13%)] Loss: 6625.222656\n",
      "Train Epoch: 490 [33024/225000 (15%)] Loss: 6624.009766\n",
      "Train Epoch: 490 [37120/225000 (16%)] Loss: 6541.810547\n",
      "Train Epoch: 490 [41216/225000 (18%)] Loss: 6569.603516\n",
      "Train Epoch: 490 [45312/225000 (20%)] Loss: 6739.699219\n",
      "Train Epoch: 490 [49408/225000 (22%)] Loss: 6796.843750\n",
      "Train Epoch: 490 [53504/225000 (24%)] Loss: 6656.291016\n",
      "Train Epoch: 490 [57600/225000 (26%)] Loss: 6724.271484\n",
      "Train Epoch: 490 [61696/225000 (27%)] Loss: 6622.218750\n",
      "Train Epoch: 490 [65792/225000 (29%)] Loss: 6881.611328\n",
      "Train Epoch: 490 [69888/225000 (31%)] Loss: 6764.191406\n",
      "Train Epoch: 490 [73984/225000 (33%)] Loss: 6614.207031\n",
      "Train Epoch: 490 [78080/225000 (35%)] Loss: 6712.871094\n",
      "Train Epoch: 490 [82176/225000 (37%)] Loss: 6662.291016\n",
      "Train Epoch: 490 [86272/225000 (38%)] Loss: 6692.847656\n",
      "Train Epoch: 490 [90368/225000 (40%)] Loss: 6743.134766\n",
      "Train Epoch: 490 [94464/225000 (42%)] Loss: 6585.130859\n",
      "Train Epoch: 490 [98560/225000 (44%)] Loss: 6682.128906\n",
      "Train Epoch: 490 [102656/225000 (46%)] Loss: 6706.732422\n",
      "Train Epoch: 490 [106752/225000 (47%)] Loss: 6770.169922\n",
      "Train Epoch: 490 [110848/225000 (49%)] Loss: 6545.519531\n",
      "Train Epoch: 490 [114944/225000 (51%)] Loss: 6694.046875\n",
      "Train Epoch: 490 [119040/225000 (53%)] Loss: 6575.761719\n",
      "Train Epoch: 490 [123136/225000 (55%)] Loss: 6796.861328\n",
      "Train Epoch: 490 [127232/225000 (57%)] Loss: 6730.753906\n",
      "Train Epoch: 490 [131328/225000 (58%)] Loss: 6816.386719\n",
      "Train Epoch: 490 [135424/225000 (60%)] Loss: 6677.150391\n",
      "Train Epoch: 490 [139520/225000 (62%)] Loss: 6612.041016\n",
      "Train Epoch: 490 [143616/225000 (64%)] Loss: 6571.259766\n",
      "Train Epoch: 490 [147712/225000 (66%)] Loss: 6671.808594\n",
      "Train Epoch: 490 [151808/225000 (67%)] Loss: 6851.251953\n",
      "Train Epoch: 490 [155904/225000 (69%)] Loss: 6703.179688\n",
      "Train Epoch: 490 [160000/225000 (71%)] Loss: 6790.583984\n",
      "Train Epoch: 490 [164096/225000 (73%)] Loss: 6495.099609\n",
      "Train Epoch: 490 [168192/225000 (75%)] Loss: 6615.125000\n",
      "Train Epoch: 490 [172288/225000 (77%)] Loss: 6791.455078\n",
      "Train Epoch: 490 [176384/225000 (78%)] Loss: 6579.453125\n",
      "Train Epoch: 490 [180480/225000 (80%)] Loss: 6720.900391\n",
      "Train Epoch: 490 [184576/225000 (82%)] Loss: 6666.783203\n",
      "Train Epoch: 490 [188672/225000 (84%)] Loss: 6644.154297\n",
      "Train Epoch: 490 [192768/225000 (86%)] Loss: 6711.464844\n",
      "Train Epoch: 490 [196864/225000 (87%)] Loss: 6789.894531\n",
      "Train Epoch: 490 [200960/225000 (89%)] Loss: 6631.445312\n",
      "Train Epoch: 490 [205056/225000 (91%)] Loss: 6708.921875\n",
      "Train Epoch: 490 [209152/225000 (93%)] Loss: 6678.177734\n",
      "Train Epoch: 490 [213248/225000 (95%)] Loss: 6823.888672\n",
      "Train Epoch: 490 [217344/225000 (97%)] Loss: 6671.871094\n",
      "Train Epoch: 490 [221440/225000 (98%)] Loss: 6600.652344\n",
      "    epoch          : 490\n",
      "    loss           : 6702.989173377062\n",
      "    val_loss       : 6818.918910160357\n",
      "Train Epoch: 491 [256/225000 (0%)] Loss: 6585.511719\n",
      "Train Epoch: 491 [4352/225000 (2%)] Loss: 6710.177734\n",
      "Train Epoch: 491 [8448/225000 (4%)] Loss: 6712.634766\n",
      "Train Epoch: 491 [12544/225000 (6%)] Loss: 6661.072266\n",
      "Train Epoch: 491 [16640/225000 (7%)] Loss: 6628.941406\n",
      "Train Epoch: 491 [20736/225000 (9%)] Loss: 6761.621094\n",
      "Train Epoch: 491 [24832/225000 (11%)] Loss: 6790.494141\n",
      "Train Epoch: 491 [28928/225000 (13%)] Loss: 6774.740234\n",
      "Train Epoch: 491 [33024/225000 (15%)] Loss: 6603.564453\n",
      "Train Epoch: 491 [37120/225000 (16%)] Loss: 6483.582031\n",
      "Train Epoch: 491 [41216/225000 (18%)] Loss: 6640.853516\n",
      "Train Epoch: 491 [45312/225000 (20%)] Loss: 6491.144531\n",
      "Train Epoch: 491 [49408/225000 (22%)] Loss: 6700.912109\n",
      "Train Epoch: 491 [53504/225000 (24%)] Loss: 6673.126953\n",
      "Train Epoch: 491 [57600/225000 (26%)] Loss: 6671.205078\n",
      "Train Epoch: 491 [61696/225000 (27%)] Loss: 6593.001953\n",
      "Train Epoch: 491 [65792/225000 (29%)] Loss: 6616.380859\n",
      "Train Epoch: 491 [69888/225000 (31%)] Loss: 6716.269531\n",
      "Train Epoch: 491 [73984/225000 (33%)] Loss: 6600.187500\n",
      "Train Epoch: 491 [78080/225000 (35%)] Loss: 6789.736328\n",
      "Train Epoch: 491 [82176/225000 (37%)] Loss: 6841.558594\n",
      "Train Epoch: 491 [86272/225000 (38%)] Loss: 6589.068359\n",
      "Train Epoch: 491 [90368/225000 (40%)] Loss: 6637.687500\n",
      "Train Epoch: 491 [94464/225000 (42%)] Loss: 6720.265625\n",
      "Train Epoch: 491 [98560/225000 (44%)] Loss: 6640.496094\n",
      "Train Epoch: 491 [102656/225000 (46%)] Loss: 6614.283203\n",
      "Train Epoch: 491 [106752/225000 (47%)] Loss: 6656.542969\n",
      "Train Epoch: 491 [110848/225000 (49%)] Loss: 6774.828125\n",
      "Train Epoch: 491 [114944/225000 (51%)] Loss: 6660.685547\n",
      "Train Epoch: 491 [119040/225000 (53%)] Loss: 6772.111328\n",
      "Train Epoch: 491 [123136/225000 (55%)] Loss: 6879.392578\n",
      "Train Epoch: 491 [127232/225000 (57%)] Loss: 6618.708984\n",
      "Train Epoch: 491 [131328/225000 (58%)] Loss: 6730.785156\n",
      "Train Epoch: 491 [135424/225000 (60%)] Loss: 6720.093750\n",
      "Train Epoch: 491 [139520/225000 (62%)] Loss: 6784.746094\n",
      "Train Epoch: 491 [143616/225000 (64%)] Loss: 6800.111328\n",
      "Train Epoch: 491 [147712/225000 (66%)] Loss: 6811.593750\n",
      "Train Epoch: 491 [151808/225000 (67%)] Loss: 6681.005859\n",
      "Train Epoch: 491 [155904/225000 (69%)] Loss: 6678.513672\n",
      "Train Epoch: 491 [160000/225000 (71%)] Loss: 6653.607422\n",
      "Train Epoch: 491 [164096/225000 (73%)] Loss: 6712.117188\n",
      "Train Epoch: 491 [168192/225000 (75%)] Loss: 6690.720703\n",
      "Train Epoch: 491 [172288/225000 (77%)] Loss: 6737.023438\n",
      "Train Epoch: 491 [176384/225000 (78%)] Loss: 6793.195312\n",
      "Train Epoch: 491 [180480/225000 (80%)] Loss: 6638.394531\n",
      "Train Epoch: 491 [184576/225000 (82%)] Loss: 6784.726562\n",
      "Train Epoch: 491 [188672/225000 (84%)] Loss: 6725.908203\n",
      "Train Epoch: 491 [192768/225000 (86%)] Loss: 6691.970703\n",
      "Train Epoch: 491 [196864/225000 (87%)] Loss: 6722.800781\n",
      "Train Epoch: 491 [200960/225000 (89%)] Loss: 6635.113281\n",
      "Train Epoch: 491 [205056/225000 (91%)] Loss: 6644.271484\n",
      "Train Epoch: 491 [209152/225000 (93%)] Loss: 6622.316406\n",
      "Train Epoch: 491 [213248/225000 (95%)] Loss: 6803.263672\n",
      "Train Epoch: 491 [217344/225000 (97%)] Loss: 6576.292969\n",
      "Train Epoch: 491 [221440/225000 (98%)] Loss: 6763.820312\n",
      "    epoch          : 491\n",
      "    loss           : 6711.664171377275\n",
      "    val_loss       : 6719.85683951086\n",
      "Train Epoch: 492 [256/225000 (0%)] Loss: 6629.722656\n",
      "Train Epoch: 492 [4352/225000 (2%)] Loss: 6726.189453\n",
      "Train Epoch: 492 [8448/225000 (4%)] Loss: 6686.066406\n",
      "Train Epoch: 492 [12544/225000 (6%)] Loss: 6701.347656\n",
      "Train Epoch: 492 [16640/225000 (7%)] Loss: 6522.496094\n",
      "Train Epoch: 492 [20736/225000 (9%)] Loss: 6761.105469\n",
      "Train Epoch: 492 [24832/225000 (11%)] Loss: 6810.449219\n",
      "Train Epoch: 492 [28928/225000 (13%)] Loss: 6601.914062\n",
      "Train Epoch: 492 [33024/225000 (15%)] Loss: 6770.507812\n",
      "Train Epoch: 492 [37120/225000 (16%)] Loss: 6683.111328\n",
      "Train Epoch: 492 [41216/225000 (18%)] Loss: 6733.746094\n",
      "Train Epoch: 492 [45312/225000 (20%)] Loss: 6684.216797\n",
      "Train Epoch: 492 [49408/225000 (22%)] Loss: 6632.269531\n",
      "Train Epoch: 492 [53504/225000 (24%)] Loss: 6652.949219\n",
      "Train Epoch: 492 [57600/225000 (26%)] Loss: 6637.626953\n",
      "Train Epoch: 492 [61696/225000 (27%)] Loss: 6706.890625\n",
      "Train Epoch: 492 [65792/225000 (29%)] Loss: 6744.734375\n",
      "Train Epoch: 492 [69888/225000 (31%)] Loss: 6782.466797\n",
      "Train Epoch: 492 [73984/225000 (33%)] Loss: 6819.378906\n",
      "Train Epoch: 492 [78080/225000 (35%)] Loss: 6871.224609\n",
      "Train Epoch: 492 [82176/225000 (37%)] Loss: 6634.361328\n",
      "Train Epoch: 492 [86272/225000 (38%)] Loss: 6590.115234\n",
      "Train Epoch: 492 [90368/225000 (40%)] Loss: 6660.931641\n",
      "Train Epoch: 492 [94464/225000 (42%)] Loss: 6621.630859\n",
      "Train Epoch: 492 [98560/225000 (44%)] Loss: 6630.935547\n",
      "Train Epoch: 492 [102656/225000 (46%)] Loss: 6810.373047\n",
      "Train Epoch: 492 [106752/225000 (47%)] Loss: 6820.333984\n",
      "Train Epoch: 492 [110848/225000 (49%)] Loss: 6651.650391\n",
      "Train Epoch: 492 [114944/225000 (51%)] Loss: 6630.640625\n",
      "Train Epoch: 492 [119040/225000 (53%)] Loss: 6654.820312\n",
      "Train Epoch: 492 [123136/225000 (55%)] Loss: 6768.894531\n",
      "Train Epoch: 492 [127232/225000 (57%)] Loss: 6551.201172\n",
      "Train Epoch: 492 [131328/225000 (58%)] Loss: 6565.970703\n",
      "Train Epoch: 492 [135424/225000 (60%)] Loss: 6731.005859\n",
      "Train Epoch: 492 [139520/225000 (62%)] Loss: 6659.503906\n",
      "Train Epoch: 492 [143616/225000 (64%)] Loss: 6651.996094\n",
      "Train Epoch: 492 [147712/225000 (66%)] Loss: 6580.837891\n",
      "Train Epoch: 492 [151808/225000 (67%)] Loss: 6811.490234\n",
      "Train Epoch: 492 [155904/225000 (69%)] Loss: 6838.554688\n",
      "Train Epoch: 492 [160000/225000 (71%)] Loss: 6743.351562\n",
      "Train Epoch: 492 [164096/225000 (73%)] Loss: 6591.359375\n",
      "Train Epoch: 492 [168192/225000 (75%)] Loss: 6659.845703\n",
      "Train Epoch: 492 [172288/225000 (77%)] Loss: 6683.642578\n",
      "Train Epoch: 492 [176384/225000 (78%)] Loss: 6657.931641\n",
      "Train Epoch: 492 [180480/225000 (80%)] Loss: 6588.814453\n",
      "Train Epoch: 492 [184576/225000 (82%)] Loss: 6679.601562\n",
      "Train Epoch: 492 [188672/225000 (84%)] Loss: 6691.292969\n",
      "Train Epoch: 492 [192768/225000 (86%)] Loss: 6711.306641\n",
      "Train Epoch: 492 [196864/225000 (87%)] Loss: 6706.541016\n",
      "Train Epoch: 492 [200960/225000 (89%)] Loss: 6598.636719\n",
      "Train Epoch: 492 [205056/225000 (91%)] Loss: 6777.404297\n",
      "Train Epoch: 492 [209152/225000 (93%)] Loss: 6641.816406\n",
      "Train Epoch: 492 [213248/225000 (95%)] Loss: 6821.986328\n",
      "Train Epoch: 492 [217344/225000 (97%)] Loss: 6722.537109\n",
      "Train Epoch: 492 [221440/225000 (98%)] Loss: 6763.068359\n",
      "    epoch          : 492\n",
      "    loss           : 6697.856843047853\n",
      "    val_loss       : 6719.829983520264\n",
      "Train Epoch: 493 [256/225000 (0%)] Loss: 6787.769531\n",
      "Train Epoch: 493 [4352/225000 (2%)] Loss: 6782.544922\n",
      "Train Epoch: 493 [8448/225000 (4%)] Loss: 6702.720703\n",
      "Train Epoch: 493 [12544/225000 (6%)] Loss: 6764.578125\n",
      "Train Epoch: 493 [16640/225000 (7%)] Loss: 6581.115234\n",
      "Train Epoch: 493 [20736/225000 (9%)] Loss: 6746.126953\n",
      "Train Epoch: 493 [24832/225000 (11%)] Loss: 6808.732422\n",
      "Train Epoch: 493 [28928/225000 (13%)] Loss: 6655.431641\n",
      "Train Epoch: 493 [33024/225000 (15%)] Loss: 6682.062500\n",
      "Train Epoch: 493 [37120/225000 (16%)] Loss: 6590.751953\n",
      "Train Epoch: 493 [41216/225000 (18%)] Loss: 6825.529297\n",
      "Train Epoch: 493 [45312/225000 (20%)] Loss: 6499.990234\n",
      "Train Epoch: 493 [49408/225000 (22%)] Loss: 6641.501953\n",
      "Train Epoch: 493 [53504/225000 (24%)] Loss: 6657.906250\n",
      "Train Epoch: 493 [57600/225000 (26%)] Loss: 6726.375000\n",
      "Train Epoch: 493 [61696/225000 (27%)] Loss: 6691.167969\n",
      "Train Epoch: 493 [65792/225000 (29%)] Loss: 6601.503906\n",
      "Train Epoch: 493 [69888/225000 (31%)] Loss: 6731.308594\n",
      "Train Epoch: 493 [73984/225000 (33%)] Loss: 6731.998047\n",
      "Train Epoch: 493 [78080/225000 (35%)] Loss: 6556.238281\n",
      "Train Epoch: 493 [82176/225000 (37%)] Loss: 6762.708984\n",
      "Train Epoch: 493 [86272/225000 (38%)] Loss: 6701.539062\n",
      "Train Epoch: 493 [90368/225000 (40%)] Loss: 6719.351562\n",
      "Train Epoch: 493 [94464/225000 (42%)] Loss: 6553.515625\n",
      "Train Epoch: 493 [98560/225000 (44%)] Loss: 6514.802734\n",
      "Train Epoch: 493 [102656/225000 (46%)] Loss: 6779.568359\n",
      "Train Epoch: 493 [106752/225000 (47%)] Loss: 6708.414062\n",
      "Train Epoch: 493 [110848/225000 (49%)] Loss: 6697.662109\n",
      "Train Epoch: 493 [114944/225000 (51%)] Loss: 6668.472656\n",
      "Train Epoch: 493 [119040/225000 (53%)] Loss: 6629.265625\n",
      "Train Epoch: 493 [123136/225000 (55%)] Loss: 6682.339844\n",
      "Train Epoch: 493 [127232/225000 (57%)] Loss: 6661.828125\n",
      "Train Epoch: 493 [131328/225000 (58%)] Loss: 6686.833984\n",
      "Train Epoch: 493 [135424/225000 (60%)] Loss: 6648.625000\n",
      "Train Epoch: 493 [139520/225000 (62%)] Loss: 6753.677734\n",
      "Train Epoch: 493 [143616/225000 (64%)] Loss: 6734.318359\n",
      "Train Epoch: 493 [147712/225000 (66%)] Loss: 6724.857422\n",
      "Train Epoch: 493 [151808/225000 (67%)] Loss: 6752.386719\n",
      "Train Epoch: 493 [155904/225000 (69%)] Loss: 6706.005859\n",
      "Train Epoch: 493 [160000/225000 (71%)] Loss: 6606.974609\n",
      "Train Epoch: 493 [164096/225000 (73%)] Loss: 6757.027344\n",
      "Train Epoch: 493 [168192/225000 (75%)] Loss: 6755.800781\n",
      "Train Epoch: 493 [172288/225000 (77%)] Loss: 6615.939453\n",
      "Train Epoch: 493 [176384/225000 (78%)] Loss: 6714.208984\n",
      "Train Epoch: 493 [180480/225000 (80%)] Loss: 6691.152344\n",
      "Train Epoch: 493 [184576/225000 (82%)] Loss: 6592.515625\n",
      "Train Epoch: 493 [188672/225000 (84%)] Loss: 6613.435547\n",
      "Train Epoch: 493 [192768/225000 (86%)] Loss: 6680.521484\n",
      "Train Epoch: 493 [196864/225000 (87%)] Loss: 6562.570312\n",
      "Train Epoch: 493 [200960/225000 (89%)] Loss: 6629.621094\n",
      "Train Epoch: 493 [205056/225000 (91%)] Loss: 6698.804688\n",
      "Train Epoch: 493 [209152/225000 (93%)] Loss: 6609.533203\n",
      "Train Epoch: 493 [213248/225000 (95%)] Loss: 6711.525391\n",
      "Train Epoch: 493 [217344/225000 (97%)] Loss: 6744.158203\n",
      "Train Epoch: 493 [221440/225000 (98%)] Loss: 6740.644531\n",
      "    epoch          : 493\n",
      "    loss           : 6691.461710750853\n",
      "    val_loss       : 6719.844211077204\n",
      "Train Epoch: 494 [256/225000 (0%)] Loss: 6593.046875\n",
      "Train Epoch: 494 [4352/225000 (2%)] Loss: 6616.201172\n",
      "Train Epoch: 494 [8448/225000 (4%)] Loss: 6776.728516\n",
      "Train Epoch: 494 [12544/225000 (6%)] Loss: 6612.765625\n",
      "Train Epoch: 494 [16640/225000 (7%)] Loss: 6655.173828\n",
      "Train Epoch: 494 [20736/225000 (9%)] Loss: 6698.099609\n",
      "Train Epoch: 494 [24832/225000 (11%)] Loss: 6740.755859\n",
      "Train Epoch: 494 [28928/225000 (13%)] Loss: 6670.152344\n",
      "Train Epoch: 494 [33024/225000 (15%)] Loss: 6583.832031\n",
      "Train Epoch: 494 [37120/225000 (16%)] Loss: 6779.371094\n",
      "Train Epoch: 494 [41216/225000 (18%)] Loss: 6706.253906\n",
      "Train Epoch: 494 [45312/225000 (20%)] Loss: 6645.679688\n",
      "Train Epoch: 494 [49408/225000 (22%)] Loss: 6800.789062\n",
      "Train Epoch: 494 [53504/225000 (24%)] Loss: 6668.195312\n",
      "Train Epoch: 494 [57600/225000 (26%)] Loss: 6673.701172\n",
      "Train Epoch: 494 [61696/225000 (27%)] Loss: 6765.953125\n",
      "Train Epoch: 494 [65792/225000 (29%)] Loss: 6602.013672\n",
      "Train Epoch: 494 [69888/225000 (31%)] Loss: 6731.015625\n",
      "Train Epoch: 494 [73984/225000 (33%)] Loss: 6896.470703\n",
      "Train Epoch: 494 [78080/225000 (35%)] Loss: 6701.123047\n",
      "Train Epoch: 494 [82176/225000 (37%)] Loss: 6815.333984\n",
      "Train Epoch: 494 [86272/225000 (38%)] Loss: 6642.013672\n",
      "Train Epoch: 494 [90368/225000 (40%)] Loss: 6792.289062\n",
      "Train Epoch: 494 [94464/225000 (42%)] Loss: 6733.054688\n",
      "Train Epoch: 494 [98560/225000 (44%)] Loss: 6490.617188\n",
      "Train Epoch: 494 [102656/225000 (46%)] Loss: 6623.212891\n",
      "Train Epoch: 494 [106752/225000 (47%)] Loss: 6644.287109\n",
      "Train Epoch: 494 [110848/225000 (49%)] Loss: 6757.080078\n",
      "Train Epoch: 494 [114944/225000 (51%)] Loss: 6697.630859\n",
      "Train Epoch: 494 [119040/225000 (53%)] Loss: 6739.945312\n",
      "Train Epoch: 494 [123136/225000 (55%)] Loss: 6817.363281\n",
      "Train Epoch: 494 [127232/225000 (57%)] Loss: 6829.779297\n",
      "Train Epoch: 494 [131328/225000 (58%)] Loss: 6914.365234\n",
      "Train Epoch: 494 [135424/225000 (60%)] Loss: 6773.417969\n",
      "Train Epoch: 494 [139520/225000 (62%)] Loss: 6749.994141\n",
      "Train Epoch: 494 [143616/225000 (64%)] Loss: 6737.669922\n",
      "Train Epoch: 494 [147712/225000 (66%)] Loss: 6733.064453\n",
      "Train Epoch: 494 [151808/225000 (67%)] Loss: 6829.398438\n",
      "Train Epoch: 494 [155904/225000 (69%)] Loss: 6553.138672\n",
      "Train Epoch: 494 [160000/225000 (71%)] Loss: 6690.955078\n",
      "Train Epoch: 494 [164096/225000 (73%)] Loss: 6623.185547\n",
      "Train Epoch: 494 [168192/225000 (75%)] Loss: 6795.197266\n",
      "Train Epoch: 494 [172288/225000 (77%)] Loss: 6657.578125\n",
      "Train Epoch: 494 [176384/225000 (78%)] Loss: 6724.507812\n",
      "Train Epoch: 494 [180480/225000 (80%)] Loss: 6883.009766\n",
      "Train Epoch: 494 [184576/225000 (82%)] Loss: 6639.246094\n",
      "Train Epoch: 494 [188672/225000 (84%)] Loss: 6557.185547\n",
      "Train Epoch: 494 [192768/225000 (86%)] Loss: 6538.109375\n",
      "Train Epoch: 494 [196864/225000 (87%)] Loss: 6726.933594\n",
      "Train Epoch: 494 [200960/225000 (89%)] Loss: 6738.197266\n",
      "Train Epoch: 494 [205056/225000 (91%)] Loss: 6750.378906\n",
      "Train Epoch: 494 [209152/225000 (93%)] Loss: 6624.310547\n",
      "Train Epoch: 494 [213248/225000 (95%)] Loss: 6536.630859\n",
      "Train Epoch: 494 [217344/225000 (97%)] Loss: 6688.468750\n",
      "Train Epoch: 494 [221440/225000 (98%)] Loss: 6628.707031\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "    epoch          : 494\n",
      "    loss           : 6714.632406987699\n",
      "    val_loss       : 6782.919592093448\n",
      "Train Epoch: 495 [256/225000 (0%)] Loss: 6616.789062\n",
      "Train Epoch: 495 [4352/225000 (2%)] Loss: 6635.966797\n",
      "Train Epoch: 495 [8448/225000 (4%)] Loss: 6648.117188\n",
      "Train Epoch: 495 [12544/225000 (6%)] Loss: 6794.421875\n",
      "Train Epoch: 495 [16640/225000 (7%)] Loss: 6767.910156\n",
      "Train Epoch: 495 [20736/225000 (9%)] Loss: 6662.845703\n",
      "Train Epoch: 495 [24832/225000 (11%)] Loss: 6699.416016\n",
      "Train Epoch: 495 [28928/225000 (13%)] Loss: 6685.845703\n",
      "Train Epoch: 495 [33024/225000 (15%)] Loss: 6741.531250\n",
      "Train Epoch: 495 [37120/225000 (16%)] Loss: 6616.318359\n",
      "Train Epoch: 495 [41216/225000 (18%)] Loss: 6571.980469\n",
      "Train Epoch: 495 [45312/225000 (20%)] Loss: 6810.419922\n",
      "Train Epoch: 495 [49408/225000 (22%)] Loss: 6808.070312\n",
      "Train Epoch: 495 [53504/225000 (24%)] Loss: 6672.238281\n",
      "Train Epoch: 495 [57600/225000 (26%)] Loss: 6805.380859\n",
      "Train Epoch: 495 [61696/225000 (27%)] Loss: 6656.935547\n",
      "Train Epoch: 495 [65792/225000 (29%)] Loss: 6697.375000\n",
      "Train Epoch: 495 [69888/225000 (31%)] Loss: 6834.457031\n",
      "Train Epoch: 495 [73984/225000 (33%)] Loss: 6932.751953\n",
      "Train Epoch: 495 [78080/225000 (35%)] Loss: 6717.376953\n",
      "Train Epoch: 495 [82176/225000 (37%)] Loss: 6738.353516\n",
      "Train Epoch: 495 [86272/225000 (38%)] Loss: 6718.982422\n",
      "Train Epoch: 495 [90368/225000 (40%)] Loss: 6706.681641\n",
      "Train Epoch: 495 [94464/225000 (42%)] Loss: 6716.986328\n",
      "Train Epoch: 495 [98560/225000 (44%)] Loss: 6750.447266\n",
      "Train Epoch: 495 [102656/225000 (46%)] Loss: 6723.431641\n",
      "Train Epoch: 495 [106752/225000 (47%)] Loss: 6652.160156\n",
      "Train Epoch: 495 [110848/225000 (49%)] Loss: 6719.007812\n",
      "Train Epoch: 495 [114944/225000 (51%)] Loss: 6725.630859\n",
      "Train Epoch: 495 [119040/225000 (53%)] Loss: 6536.304688\n",
      "Train Epoch: 495 [123136/225000 (55%)] Loss: 6781.242188\n",
      "Train Epoch: 495 [127232/225000 (57%)] Loss: 6630.687500\n",
      "Train Epoch: 495 [131328/225000 (58%)] Loss: 6716.802734\n",
      "Train Epoch: 495 [135424/225000 (60%)] Loss: 6660.070312\n",
      "Train Epoch: 495 [139520/225000 (62%)] Loss: 6599.025391\n",
      "Train Epoch: 495 [143616/225000 (64%)] Loss: 6744.970703\n",
      "Train Epoch: 495 [147712/225000 (66%)] Loss: 6652.718750\n",
      "Train Epoch: 495 [151808/225000 (67%)] Loss: 6711.767578\n",
      "Train Epoch: 495 [155904/225000 (69%)] Loss: 6559.812500\n",
      "Train Epoch: 495 [160000/225000 (71%)] Loss: 6607.187500\n",
      "Train Epoch: 495 [164096/225000 (73%)] Loss: 6661.623047\n",
      "Train Epoch: 495 [168192/225000 (75%)] Loss: 6686.277344\n",
      "Train Epoch: 495 [172288/225000 (77%)] Loss: 6703.195312\n",
      "Train Epoch: 495 [176384/225000 (78%)] Loss: 6574.683594\n",
      "Train Epoch: 495 [180480/225000 (80%)] Loss: 6588.992188\n",
      "Train Epoch: 495 [184576/225000 (82%)] Loss: 6682.925781\n",
      "Train Epoch: 495 [188672/225000 (84%)] Loss: 6602.820312\n",
      "Train Epoch: 495 [192768/225000 (86%)] Loss: 6789.720703\n",
      "Train Epoch: 495 [196864/225000 (87%)] Loss: 6710.228516\n",
      "Train Epoch: 495 [200960/225000 (89%)] Loss: 6671.304688\n",
      "Train Epoch: 495 [205056/225000 (91%)] Loss: 6563.998047\n",
      "Train Epoch: 495 [209152/225000 (93%)] Loss: 6746.066406\n",
      "Train Epoch: 495 [213248/225000 (95%)] Loss: 6711.033203\n",
      "Train Epoch: 495 [217344/225000 (97%)] Loss: 6675.642578\n",
      "Train Epoch: 495 [221440/225000 (98%)] Loss: 6601.441406\n",
      "    epoch          : 495\n",
      "    loss           : 6712.871528148109\n",
      "    val_loss       : 6782.329564571381\n",
      "Train Epoch: 496 [256/225000 (0%)] Loss: 6684.212891\n",
      "Train Epoch: 496 [4352/225000 (2%)] Loss: 6522.957031\n",
      "Train Epoch: 496 [8448/225000 (4%)] Loss: 6656.976562\n",
      "Train Epoch: 496 [12544/225000 (6%)] Loss: 6701.392578\n",
      "Train Epoch: 496 [16640/225000 (7%)] Loss: 6667.660156\n",
      "Train Epoch: 496 [20736/225000 (9%)] Loss: 6546.818359\n",
      "Train Epoch: 496 [24832/225000 (11%)] Loss: 6700.445312\n",
      "Train Epoch: 496 [28928/225000 (13%)] Loss: 6667.287109\n",
      "Train Epoch: 496 [33024/225000 (15%)] Loss: 6671.748047\n",
      "Train Epoch: 496 [37120/225000 (16%)] Loss: 6657.937500\n",
      "Train Epoch: 496 [41216/225000 (18%)] Loss: 6719.781250\n",
      "Train Epoch: 496 [45312/225000 (20%)] Loss: 6630.505859\n",
      "Train Epoch: 496 [49408/225000 (22%)] Loss: 6746.554688\n",
      "Train Epoch: 496 [53504/225000 (24%)] Loss: 6736.923828\n",
      "Train Epoch: 496 [57600/225000 (26%)] Loss: 6607.291016\n",
      "Train Epoch: 496 [61696/225000 (27%)] Loss: 6614.808594\n",
      "Train Epoch: 496 [65792/225000 (29%)] Loss: 6678.703125\n",
      "Train Epoch: 496 [69888/225000 (31%)] Loss: 6752.523438\n",
      "Train Epoch: 496 [73984/225000 (33%)] Loss: 6707.689453\n",
      "Train Epoch: 496 [78080/225000 (35%)] Loss: 6706.886719\n",
      "Train Epoch: 496 [82176/225000 (37%)] Loss: 6707.699219\n",
      "Train Epoch: 496 [86272/225000 (38%)] Loss: 6732.574219\n",
      "Train Epoch: 496 [90368/225000 (40%)] Loss: 6676.013672\n",
      "Train Epoch: 496 [94464/225000 (42%)] Loss: 6637.677734\n",
      "Train Epoch: 496 [98560/225000 (44%)] Loss: 6561.718750\n",
      "Train Epoch: 496 [102656/225000 (46%)] Loss: 6696.492188\n",
      "Train Epoch: 496 [106752/225000 (47%)] Loss: 6665.796875\n",
      "Train Epoch: 496 [110848/225000 (49%)] Loss: 6692.113281\n",
      "Train Epoch: 496 [114944/225000 (51%)] Loss: 6699.292969\n",
      "Train Epoch: 496 [119040/225000 (53%)] Loss: 6652.576172\n",
      "Train Epoch: 496 [123136/225000 (55%)] Loss: 6676.226562\n",
      "Train Epoch: 496 [127232/225000 (57%)] Loss: 6711.943359\n",
      "Train Epoch: 496 [131328/225000 (58%)] Loss: 6696.652344\n",
      "Train Epoch: 496 [135424/225000 (60%)] Loss: 6732.833984\n",
      "Train Epoch: 496 [139520/225000 (62%)] Loss: 6457.500000\n",
      "Train Epoch: 496 [143616/225000 (64%)] Loss: 6724.650391\n",
      "Train Epoch: 496 [147712/225000 (66%)] Loss: 6741.671875\n",
      "Train Epoch: 496 [151808/225000 (67%)] Loss: 6696.828125\n",
      "Train Epoch: 496 [155904/225000 (69%)] Loss: 6728.761719\n",
      "Train Epoch: 496 [160000/225000 (71%)] Loss: 6742.451172\n",
      "Train Epoch: 496 [164096/225000 (73%)] Loss: 6700.478516\n",
      "Train Epoch: 496 [168192/225000 (75%)] Loss: 6674.201172\n",
      "Train Epoch: 496 [172288/225000 (77%)] Loss: 6656.005859\n",
      "Train Epoch: 496 [176384/225000 (78%)] Loss: 6680.158203\n",
      "Train Epoch: 496 [180480/225000 (80%)] Loss: 6617.568359\n",
      "Train Epoch: 496 [184576/225000 (82%)] Loss: 6778.585938\n",
      "Train Epoch: 496 [188672/225000 (84%)] Loss: 6880.890625\n",
      "Train Epoch: 496 [192768/225000 (86%)] Loss: 6602.357422\n",
      "Train Epoch: 496 [196864/225000 (87%)] Loss: 6768.902344\n",
      "Train Epoch: 496 [200960/225000 (89%)] Loss: 6755.714844\n",
      "Train Epoch: 496 [205056/225000 (91%)] Loss: 6654.689453\n",
      "Train Epoch: 496 [209152/225000 (93%)] Loss: 6669.056641\n",
      "Train Epoch: 496 [213248/225000 (95%)] Loss: 6698.505859\n",
      "Train Epoch: 496 [217344/225000 (97%)] Loss: 6752.300781\n",
      "Train Epoch: 496 [221440/225000 (98%)] Loss: 6704.445312\n",
      "    epoch          : 496\n",
      "    loss           : 6689.757729175554\n",
      "    val_loss       : 6873.085964455897\n",
      "Train Epoch: 497 [256/225000 (0%)] Loss: 6714.390625\n",
      "Train Epoch: 497 [4352/225000 (2%)] Loss: 6760.115234\n",
      "Train Epoch: 497 [8448/225000 (4%)] Loss: 6698.562500\n",
      "Train Epoch: 497 [12544/225000 (6%)] Loss: 6709.449219\n",
      "Train Epoch: 497 [16640/225000 (7%)] Loss: 6768.750000\n",
      "Train Epoch: 497 [20736/225000 (9%)] Loss: 6688.173828\n",
      "Train Epoch: 497 [24832/225000 (11%)] Loss: 6706.623047\n",
      "Train Epoch: 497 [28928/225000 (13%)] Loss: 6667.822266\n",
      "Train Epoch: 497 [33024/225000 (15%)] Loss: 6701.753906\n",
      "Train Epoch: 497 [37120/225000 (16%)] Loss: 6630.070312\n",
      "Train Epoch: 497 [41216/225000 (18%)] Loss: 6526.375000\n",
      "Train Epoch: 497 [45312/225000 (20%)] Loss: 6747.716797\n",
      "Train Epoch: 497 [49408/225000 (22%)] Loss: 6633.054688\n",
      "Train Epoch: 497 [53504/225000 (24%)] Loss: 6710.822266\n",
      "Train Epoch: 497 [57600/225000 (26%)] Loss: 6776.548828\n",
      "Train Epoch: 497 [61696/225000 (27%)] Loss: 6650.167969\n",
      "Train Epoch: 497 [65792/225000 (29%)] Loss: 6666.996094\n",
      "Train Epoch: 497 [69888/225000 (31%)] Loss: 6734.890625\n",
      "Train Epoch: 497 [73984/225000 (33%)] Loss: 6725.494141\n",
      "Train Epoch: 497 [78080/225000 (35%)] Loss: 6720.173828\n",
      "Train Epoch: 497 [82176/225000 (37%)] Loss: 6687.123047\n",
      "Train Epoch: 497 [86272/225000 (38%)] Loss: 6518.712891\n",
      "Train Epoch: 497 [90368/225000 (40%)] Loss: 6726.189453\n",
      "Train Epoch: 497 [94464/225000 (42%)] Loss: 6604.714844\n",
      "Train Epoch: 497 [98560/225000 (44%)] Loss: 6739.998047\n",
      "Train Epoch: 497 [102656/225000 (46%)] Loss: 6820.326172\n",
      "Train Epoch: 497 [106752/225000 (47%)] Loss: 6620.281250\n",
      "Train Epoch: 497 [110848/225000 (49%)] Loss: 6659.351562\n",
      "Train Epoch: 497 [114944/225000 (51%)] Loss: 6743.615234\n",
      "Train Epoch: 497 [119040/225000 (53%)] Loss: 6719.609375\n",
      "Train Epoch: 497 [123136/225000 (55%)] Loss: 6710.988281\n",
      "Train Epoch: 497 [127232/225000 (57%)] Loss: 6649.183594\n",
      "Train Epoch: 497 [131328/225000 (58%)] Loss: 6516.769531\n",
      "Train Epoch: 497 [135424/225000 (60%)] Loss: 6687.242188\n",
      "Train Epoch: 497 [139520/225000 (62%)] Loss: 6644.171875\n",
      "Train Epoch: 497 [143616/225000 (64%)] Loss: 6772.062500\n",
      "Train Epoch: 497 [147712/225000 (66%)] Loss: 6636.964844\n",
      "Train Epoch: 497 [151808/225000 (67%)] Loss: 6722.125000\n",
      "Train Epoch: 497 [155904/225000 (69%)] Loss: 6699.042969\n",
      "Train Epoch: 497 [160000/225000 (71%)] Loss: 6693.875000\n",
      "Train Epoch: 497 [164096/225000 (73%)] Loss: 6728.402344\n",
      "Train Epoch: 497 [168192/225000 (75%)] Loss: 6646.556641\n",
      "Train Epoch: 497 [172288/225000 (77%)] Loss: 6654.671875\n",
      "Train Epoch: 497 [176384/225000 (78%)] Loss: 6792.521484\n",
      "Train Epoch: 497 [180480/225000 (80%)] Loss: 6598.615234\n",
      "Train Epoch: 497 [184576/225000 (82%)] Loss: 6704.433594\n",
      "Train Epoch: 497 [188672/225000 (84%)] Loss: 6494.126953\n",
      "Train Epoch: 497 [192768/225000 (86%)] Loss: 6658.705078\n",
      "Train Epoch: 497 [196864/225000 (87%)] Loss: 6616.548828\n",
      "Train Epoch: 497 [200960/225000 (89%)] Loss: 6723.908203\n",
      "Train Epoch: 497 [205056/225000 (91%)] Loss: 6751.244141\n",
      "Train Epoch: 497 [209152/225000 (93%)] Loss: 6677.517578\n",
      "Train Epoch: 497 [213248/225000 (95%)] Loss: 6750.554688\n",
      "Train Epoch: 497 [217344/225000 (97%)] Loss: 6515.455078\n",
      "Train Epoch: 497 [221440/225000 (98%)] Loss: 6627.685547\n",
      "    epoch          : 497\n",
      "    loss           : 6724.245949320962\n",
      "    val_loss       : 6718.717983559686\n",
      "Train Epoch: 498 [256/225000 (0%)] Loss: 6870.869141\n",
      "Train Epoch: 498 [4352/225000 (2%)] Loss: 6888.718750\n",
      "Train Epoch: 498 [8448/225000 (4%)] Loss: 6601.109375\n",
      "Train Epoch: 498 [12544/225000 (6%)] Loss: 6584.625000\n",
      "Train Epoch: 498 [16640/225000 (7%)] Loss: 6859.593750\n",
      "Train Epoch: 498 [20736/225000 (9%)] Loss: 6662.005859\n",
      "Train Epoch: 498 [24832/225000 (11%)] Loss: 6662.277344\n",
      "Train Epoch: 498 [28928/225000 (13%)] Loss: 6679.626953\n",
      "Train Epoch: 498 [33024/225000 (15%)] Loss: 6708.716797\n",
      "Train Epoch: 498 [37120/225000 (16%)] Loss: 6804.691406\n",
      "Train Epoch: 498 [41216/225000 (18%)] Loss: 6669.089844\n",
      "Train Epoch: 498 [45312/225000 (20%)] Loss: 6695.560547\n",
      "Train Epoch: 498 [49408/225000 (22%)] Loss: 6698.103516\n",
      "Train Epoch: 498 [53504/225000 (24%)] Loss: 6783.837891\n",
      "Train Epoch: 498 [57600/225000 (26%)] Loss: 6583.605469\n",
      "Train Epoch: 498 [61696/225000 (27%)] Loss: 6714.218750\n",
      "Train Epoch: 498 [65792/225000 (29%)] Loss: 6730.042969\n",
      "Train Epoch: 498 [69888/225000 (31%)] Loss: 6579.564453\n",
      "Train Epoch: 498 [73984/225000 (33%)] Loss: 6665.460938\n",
      "Train Epoch: 498 [78080/225000 (35%)] Loss: 6676.722656\n",
      "Train Epoch: 498 [82176/225000 (37%)] Loss: 6664.341797\n",
      "Train Epoch: 498 [86272/225000 (38%)] Loss: 6642.527344\n",
      "Train Epoch: 498 [90368/225000 (40%)] Loss: 6744.119141\n",
      "Train Epoch: 498 [94464/225000 (42%)] Loss: 6827.472656\n",
      "Train Epoch: 498 [98560/225000 (44%)] Loss: 6547.281250\n",
      "Train Epoch: 498 [102656/225000 (46%)] Loss: 6703.486328\n",
      "Train Epoch: 498 [106752/225000 (47%)] Loss: 6797.009766\n",
      "Train Epoch: 498 [110848/225000 (49%)] Loss: 6821.318359\n",
      "Train Epoch: 498 [114944/225000 (51%)] Loss: 6833.404297\n",
      "Train Epoch: 498 [119040/225000 (53%)] Loss: 6767.294922\n",
      "Train Epoch: 498 [123136/225000 (55%)] Loss: 6677.167969\n",
      "Train Epoch: 498 [127232/225000 (57%)] Loss: 6853.552734\n",
      "Train Epoch: 498 [131328/225000 (58%)] Loss: 6749.712891\n",
      "Train Epoch: 498 [135424/225000 (60%)] Loss: 6597.140625\n",
      "Train Epoch: 498 [139520/225000 (62%)] Loss: 6614.925781\n",
      "Train Epoch: 498 [143616/225000 (64%)] Loss: 6744.724609\n",
      "Train Epoch: 498 [147712/225000 (66%)] Loss: 6657.386719\n",
      "Train Epoch: 498 [151808/225000 (67%)] Loss: 6884.488281\n",
      "Train Epoch: 498 [155904/225000 (69%)] Loss: 6854.232422\n",
      "Train Epoch: 498 [160000/225000 (71%)] Loss: 6623.845703\n",
      "Train Epoch: 498 [164096/225000 (73%)] Loss: 6525.273438\n",
      "Train Epoch: 498 [168192/225000 (75%)] Loss: 6668.091797\n",
      "Train Epoch: 498 [172288/225000 (77%)] Loss: 6834.191406\n",
      "Train Epoch: 498 [176384/225000 (78%)] Loss: 6742.429688\n",
      "Train Epoch: 498 [180480/225000 (80%)] Loss: 6714.234375\n",
      "Train Epoch: 498 [184576/225000 (82%)] Loss: 6645.136719\n",
      "Train Epoch: 498 [188672/225000 (84%)] Loss: 6813.460938\n",
      "Train Epoch: 498 [192768/225000 (86%)] Loss: 6683.580078\n",
      "Train Epoch: 498 [196864/225000 (87%)] Loss: 6610.417969\n",
      "Train Epoch: 498 [200960/225000 (89%)] Loss: 6751.023438\n",
      "Train Epoch: 498 [205056/225000 (91%)] Loss: 6750.843750\n",
      "Train Epoch: 498 [209152/225000 (93%)] Loss: 6674.687500\n",
      "Train Epoch: 498 [213248/225000 (95%)] Loss: 6683.537109\n",
      "Train Epoch: 498 [217344/225000 (97%)] Loss: 6840.958984\n",
      "Train Epoch: 498 [221440/225000 (98%)] Loss: 6748.671875\n",
      "    epoch          : 498\n",
      "    loss           : 6718.422543817548\n",
      "    val_loss       : 6780.829620260365\n",
      "Train Epoch: 499 [256/225000 (0%)] Loss: 6738.115234\n",
      "Train Epoch: 499 [4352/225000 (2%)] Loss: 6694.447266\n",
      "Train Epoch: 499 [8448/225000 (4%)] Loss: 6710.671875\n",
      "Train Epoch: 499 [12544/225000 (6%)] Loss: 6696.070312\n",
      "Train Epoch: 499 [16640/225000 (7%)] Loss: 6697.072266\n",
      "Train Epoch: 499 [20736/225000 (9%)] Loss: 6736.744141\n",
      "Train Epoch: 499 [24832/225000 (11%)] Loss: 6594.085938\n",
      "Train Epoch: 499 [28928/225000 (13%)] Loss: 6552.048828\n",
      "Train Epoch: 499 [33024/225000 (15%)] Loss: 6732.150391\n",
      "Train Epoch: 499 [37120/225000 (16%)] Loss: 6484.974609\n",
      "Train Epoch: 499 [41216/225000 (18%)] Loss: 6716.341797\n",
      "Train Epoch: 499 [45312/225000 (20%)] Loss: 6724.376953\n",
      "Train Epoch: 499 [49408/225000 (22%)] Loss: 6612.068359\n",
      "Train Epoch: 499 [53504/225000 (24%)] Loss: 6757.361328\n",
      "Train Epoch: 499 [57600/225000 (26%)] Loss: 6662.958984\n",
      "Train Epoch: 499 [61696/225000 (27%)] Loss: 6756.025391\n",
      "Train Epoch: 499 [65792/225000 (29%)] Loss: 6668.781250\n",
      "Train Epoch: 499 [69888/225000 (31%)] Loss: 6759.988281\n",
      "Train Epoch: 499 [73984/225000 (33%)] Loss: 6651.562500\n",
      "Train Epoch: 499 [78080/225000 (35%)] Loss: 6603.232422\n",
      "Train Epoch: 499 [82176/225000 (37%)] Loss: 6673.531250\n",
      "Train Epoch: 499 [86272/225000 (38%)] Loss: 11876.070312\n",
      "Train Epoch: 499 [90368/225000 (40%)] Loss: 6637.841797\n",
      "Train Epoch: 499 [94464/225000 (42%)] Loss: 6602.013672\n",
      "Train Epoch: 499 [98560/225000 (44%)] Loss: 6685.541016\n",
      "Train Epoch: 499 [102656/225000 (46%)] Loss: 6723.597656\n",
      "Train Epoch: 499 [106752/225000 (47%)] Loss: 6713.544922\n",
      "Train Epoch: 499 [110848/225000 (49%)] Loss: 6704.453125\n",
      "Train Epoch: 499 [114944/225000 (51%)] Loss: 6729.892578\n",
      "Train Epoch: 499 [119040/225000 (53%)] Loss: 6597.310547\n",
      "Train Epoch: 499 [123136/225000 (55%)] Loss: 6798.285156\n",
      "Train Epoch: 499 [127232/225000 (57%)] Loss: 6567.017578\n",
      "Train Epoch: 499 [131328/225000 (58%)] Loss: 6695.326172\n",
      "Train Epoch: 499 [135424/225000 (60%)] Loss: 6512.853516\n",
      "Train Epoch: 499 [139520/225000 (62%)] Loss: 6658.984375\n",
      "Train Epoch: 499 [143616/225000 (64%)] Loss: 6650.666016\n",
      "Train Epoch: 499 [147712/225000 (66%)] Loss: 6835.751953\n",
      "Train Epoch: 499 [151808/225000 (67%)] Loss: 6743.640625\n",
      "Train Epoch: 499 [155904/225000 (69%)] Loss: 6641.394531\n",
      "Train Epoch: 499 [160000/225000 (71%)] Loss: 6560.712891\n",
      "Train Epoch: 499 [164096/225000 (73%)] Loss: 6718.328125\n",
      "Train Epoch: 499 [168192/225000 (75%)] Loss: 6680.214844\n",
      "Train Epoch: 499 [172288/225000 (77%)] Loss: 6836.601562\n",
      "Train Epoch: 499 [176384/225000 (78%)] Loss: 6643.560547\n",
      "Train Epoch: 499 [180480/225000 (80%)] Loss: 6730.978516\n",
      "Train Epoch: 499 [184576/225000 (82%)] Loss: 6744.875000\n",
      "Train Epoch: 499 [188672/225000 (84%)] Loss: 6672.652344\n",
      "Train Epoch: 499 [192768/225000 (86%)] Loss: 6725.429688\n",
      "Train Epoch: 499 [196864/225000 (87%)] Loss: 6595.402344\n",
      "Train Epoch: 499 [200960/225000 (89%)] Loss: 6784.238281\n",
      "Train Epoch: 499 [205056/225000 (91%)] Loss: 6706.203125\n",
      "Train Epoch: 499 [209152/225000 (93%)] Loss: 6636.492188\n",
      "Train Epoch: 499 [213248/225000 (95%)] Loss: 6581.955078\n",
      "Train Epoch: 499 [217344/225000 (97%)] Loss: 6608.812500\n",
      "Train Epoch: 499 [221440/225000 (98%)] Loss: 6774.378906\n",
      "    epoch          : 499\n",
      "    loss           : 6717.955962475114\n",
      "    val_loss       : 6718.725407151544\n",
      "Train Epoch: 500 [256/225000 (0%)] Loss: 6794.927734\n",
      "Train Epoch: 500 [4352/225000 (2%)] Loss: 6624.054688\n",
      "Train Epoch: 500 [8448/225000 (4%)] Loss: 6708.779297\n",
      "Train Epoch: 500 [12544/225000 (6%)] Loss: 6713.480469\n",
      "Train Epoch: 500 [16640/225000 (7%)] Loss: 6629.716797\n",
      "Train Epoch: 500 [20736/225000 (9%)] Loss: 6687.753906\n",
      "Train Epoch: 500 [24832/225000 (11%)] Loss: 6696.716797\n",
      "Train Epoch: 500 [28928/225000 (13%)] Loss: 6614.896484\n",
      "Train Epoch: 500 [33024/225000 (15%)] Loss: 6699.408203\n",
      "Train Epoch: 500 [37120/225000 (16%)] Loss: 6595.166016\n",
      "Train Epoch: 500 [41216/225000 (18%)] Loss: 6593.460938\n",
      "Train Epoch: 500 [45312/225000 (20%)] Loss: 6685.734375\n",
      "Train Epoch: 500 [49408/225000 (22%)] Loss: 6636.419922\n",
      "Train Epoch: 500 [53504/225000 (24%)] Loss: 6626.457031\n",
      "Train Epoch: 500 [57600/225000 (26%)] Loss: 6783.396484\n",
      "Train Epoch: 500 [61696/225000 (27%)] Loss: 6641.402344\n",
      "Train Epoch: 500 [65792/225000 (29%)] Loss: 6629.015625\n",
      "Train Epoch: 500 [69888/225000 (31%)] Loss: 6611.425781\n",
      "Train Epoch: 500 [73984/225000 (33%)] Loss: 6617.986328\n",
      "Train Epoch: 500 [78080/225000 (35%)] Loss: 6820.505859\n",
      "Train Epoch: 500 [82176/225000 (37%)] Loss: 6695.724609\n",
      "Train Epoch: 500 [86272/225000 (38%)] Loss: 6832.412109\n",
      "Train Epoch: 500 [90368/225000 (40%)] Loss: 6709.535156\n",
      "Train Epoch: 500 [94464/225000 (42%)] Loss: 6809.287109\n",
      "Train Epoch: 500 [98560/225000 (44%)] Loss: 6816.541016\n",
      "Train Epoch: 500 [102656/225000 (46%)] Loss: 6729.658203\n",
      "Train Epoch: 500 [106752/225000 (47%)] Loss: 6552.779297\n",
      "Train Epoch: 500 [110848/225000 (49%)] Loss: 6662.027344\n",
      "Train Epoch: 500 [114944/225000 (51%)] Loss: 6699.025391\n",
      "Train Epoch: 500 [119040/225000 (53%)] Loss: 6527.283203\n",
      "Train Epoch: 500 [123136/225000 (55%)] Loss: 6812.919922\n",
      "Train Epoch: 500 [127232/225000 (57%)] Loss: 6794.917969\n",
      "Train Epoch: 500 [131328/225000 (58%)] Loss: 6639.707031\n",
      "Train Epoch: 500 [135424/225000 (60%)] Loss: 6544.103516\n",
      "Train Epoch: 500 [139520/225000 (62%)] Loss: 6809.125000\n",
      "Train Epoch: 500 [143616/225000 (64%)] Loss: 6681.599609\n",
      "Train Epoch: 500 [147712/225000 (66%)] Loss: 6705.796875\n",
      "Train Epoch: 500 [151808/225000 (67%)] Loss: 6789.427734\n",
      "Train Epoch: 500 [155904/225000 (69%)] Loss: 6622.701172\n",
      "Train Epoch: 500 [160000/225000 (71%)] Loss: 6800.308594\n",
      "Train Epoch: 500 [164096/225000 (73%)] Loss: 6729.707031\n",
      "Train Epoch: 500 [168192/225000 (75%)] Loss: 6801.027344\n",
      "Train Epoch: 500 [172288/225000 (77%)] Loss: 6608.800781\n",
      "Train Epoch: 500 [176384/225000 (78%)] Loss: 6701.878906\n",
      "Train Epoch: 500 [180480/225000 (80%)] Loss: 6546.300781\n",
      "Train Epoch: 500 [184576/225000 (82%)] Loss: 6708.216797\n",
      "Train Epoch: 500 [188672/225000 (84%)] Loss: 6714.705078\n",
      "Train Epoch: 500 [192768/225000 (86%)] Loss: 6680.072266\n",
      "Train Epoch: 500 [196864/225000 (87%)] Loss: 6693.732422\n",
      "Train Epoch: 500 [200960/225000 (89%)] Loss: 6604.703125\n",
      "Train Epoch: 500 [205056/225000 (91%)] Loss: 6678.617188\n",
      "Train Epoch: 500 [209152/225000 (93%)] Loss: 6789.091797\n",
      "Train Epoch: 500 [213248/225000 (95%)] Loss: 6690.875000\n",
      "Train Epoch: 500 [217344/225000 (97%)] Loss: 6720.691406\n",
      "Train Epoch: 500 [221440/225000 (98%)] Loss: 6695.937500\n",
      "    epoch          : 500\n",
      "    loss           : 6702.688331022468\n",
      "    val_loss       : 6718.696543039107\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0827_225017/checkpoint-epoch500.pth ...\n",
      "Saving current best: model_best.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MolecularVaeCategoryModel(\n",
       "  (_category): FreeCategory(\n",
       "    (generator_0): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 64)\n",
       "      (recurrence2): GRUCell(64, 64)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_0_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_1): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 64)\n",
       "      (recurrence2): GRUCell(64, 64)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_1_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=64, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(64, 196, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_2): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 128)\n",
       "      (recurrence2): GRUCell(128, 128)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_2_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_3): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 128)\n",
       "      (recurrence2): GRUCell(128, 128)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_3_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=128, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(128, 196, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_4): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 256)\n",
       "      (recurrence2): GRUCell(256, 256)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_4_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_5): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 256)\n",
       "      (recurrence2): GRUCell(256, 256)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_5_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=256, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(256, 196, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_6): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 64)\n",
       "      (recurrence2): GRUCell(64, 64)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_6_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_7): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 64)\n",
       "      (recurrence2): GRUCell(64, 64)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_7_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=64, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(64, 292, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_8): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 128)\n",
       "      (recurrence2): GRUCell(128, 128)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_8_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_9): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 128)\n",
       "      (recurrence2): GRUCell(128, 128)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_9_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=128, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(128, 292, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_10): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 256)\n",
       "      (recurrence2): GRUCell(256, 256)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_10_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_11): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 256)\n",
       "      (recurrence2): GRUCell(256, 256)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_11_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=256, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(256, 292, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_12): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 64)\n",
       "      (recurrence2): GRUCell(64, 64)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_12_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_13): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 64)\n",
       "      (recurrence2): GRUCell(64, 64)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_13_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=64, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(64, 435, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_14): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 128)\n",
       "      (recurrence2): GRUCell(128, 128)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_14_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_15): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 128)\n",
       "      (recurrence2): GRUCell(128, 128)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_15_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=128, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(128, 435, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_16): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 256)\n",
       "      (recurrence2): GRUCell(256, 256)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_16_dagger): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_17): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 256)\n",
       "      (recurrence2): GRUCell(256, 256)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_17_dagger): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=256, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(256, 435, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=4080, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_weights): Sequential(\n",
       "    (0): Linear(in_features=4080, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=42, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_xs, valid_ys = list(valid_data_loader)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, recons = model(observations=valid_xs, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reconstructions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1b90103b208f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mreconstructions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reconstructions' is not defined"
     ]
    }
   ],
   "source": [
    "(reconstructions == validation_data).all(dim=-1).to(dtype=torch.float).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:categorical_bpl] *",
   "language": "python",
   "name": "conda-env-categorical_bpl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
